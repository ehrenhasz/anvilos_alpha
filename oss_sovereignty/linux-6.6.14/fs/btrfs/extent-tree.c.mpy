{
  "module_name": "extent-tree.c",
  "hash_id": "e692031b49377bbd3189bc65bbc422c49b65440ecb2b77ba99df67c2750dd963",
  "original_prompt": "Ingested from linux-6.6.14/fs/btrfs/extent-tree.c",
  "human_readable_source": "\n \n\n#include <linux/sched.h>\n#include <linux/sched/signal.h>\n#include <linux/pagemap.h>\n#include <linux/writeback.h>\n#include <linux/blkdev.h>\n#include <linux/sort.h>\n#include <linux/rcupdate.h>\n#include <linux/kthread.h>\n#include <linux/slab.h>\n#include <linux/ratelimit.h>\n#include <linux/percpu_counter.h>\n#include <linux/lockdep.h>\n#include <linux/crc32c.h>\n#include \"ctree.h\"\n#include \"extent-tree.h\"\n#include \"tree-log.h\"\n#include \"disk-io.h\"\n#include \"print-tree.h\"\n#include \"volumes.h\"\n#include \"raid56.h\"\n#include \"locking.h\"\n#include \"free-space-cache.h\"\n#include \"free-space-tree.h\"\n#include \"sysfs.h\"\n#include \"qgroup.h\"\n#include \"ref-verify.h\"\n#include \"space-info.h\"\n#include \"block-rsv.h\"\n#include \"delalloc-space.h\"\n#include \"discard.h\"\n#include \"rcu-string.h\"\n#include \"zoned.h\"\n#include \"dev-replace.h\"\n#include \"fs.h\"\n#include \"accessors.h\"\n#include \"root-tree.h\"\n#include \"file-item.h\"\n#include \"orphan.h\"\n#include \"tree-checker.h\"\n\n#undef SCRAMBLE_DELAYED_REFS\n\n\nstatic int __btrfs_free_extent(struct btrfs_trans_handle *trans,\n\t\t\t       struct btrfs_delayed_ref_node *node, u64 parent,\n\t\t\t       u64 root_objectid, u64 owner_objectid,\n\t\t\t       u64 owner_offset, int refs_to_drop,\n\t\t\t       struct btrfs_delayed_extent_op *extra_op);\nstatic void __run_delayed_extent_op(struct btrfs_delayed_extent_op *extent_op,\n\t\t\t\t    struct extent_buffer *leaf,\n\t\t\t\t    struct btrfs_extent_item *ei);\nstatic int alloc_reserved_file_extent(struct btrfs_trans_handle *trans,\n\t\t\t\t      u64 parent, u64 root_objectid,\n\t\t\t\t      u64 flags, u64 owner, u64 offset,\n\t\t\t\t      struct btrfs_key *ins, int ref_mod);\nstatic int alloc_reserved_tree_block(struct btrfs_trans_handle *trans,\n\t\t\t\t     struct btrfs_delayed_ref_node *node,\n\t\t\t\t     struct btrfs_delayed_extent_op *extent_op);\nstatic int find_next_key(struct btrfs_path *path, int level,\n\t\t\t struct btrfs_key *key);\n\nstatic int block_group_bits(struct btrfs_block_group *cache, u64 bits)\n{\n\treturn (cache->flags & bits) == bits;\n}\n\n \nint btrfs_lookup_data_extent(struct btrfs_fs_info *fs_info, u64 start, u64 len)\n{\n\tstruct btrfs_root *root = btrfs_extent_root(fs_info, start);\n\tint ret;\n\tstruct btrfs_key key;\n\tstruct btrfs_path *path;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = start;\n\tkey.offset = len;\n\tkey.type = BTRFS_EXTENT_ITEM_KEY;\n\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n \nint btrfs_lookup_extent_info(struct btrfs_trans_handle *trans,\n\t\t\t     struct btrfs_fs_info *fs_info, u64 bytenr,\n\t\t\t     u64 offset, int metadata, u64 *refs, u64 *flags)\n{\n\tstruct btrfs_root *extent_root;\n\tstruct btrfs_delayed_ref_head *head;\n\tstruct btrfs_delayed_ref_root *delayed_refs;\n\tstruct btrfs_path *path;\n\tstruct btrfs_extent_item *ei;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_key key;\n\tu32 item_size;\n\tu64 num_refs;\n\tu64 extent_flags;\n\tint ret;\n\n\t \n\tif (metadata && !btrfs_fs_incompat(fs_info, SKINNY_METADATA)) {\n\t\toffset = fs_info->nodesize;\n\t\tmetadata = 0;\n\t}\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tif (!trans) {\n\t\tpath->skip_locking = 1;\n\t\tpath->search_commit_root = 1;\n\t}\n\nsearch_again:\n\tkey.objectid = bytenr;\n\tkey.offset = offset;\n\tif (metadata)\n\t\tkey.type = BTRFS_METADATA_ITEM_KEY;\n\telse\n\t\tkey.type = BTRFS_EXTENT_ITEM_KEY;\n\n\textent_root = btrfs_extent_root(fs_info, bytenr);\n\tret = btrfs_search_slot(NULL, extent_root, &key, path, 0, 0);\n\tif (ret < 0)\n\t\tgoto out_free;\n\n\tif (ret > 0 && metadata && key.type == BTRFS_METADATA_ITEM_KEY) {\n\t\tif (path->slots[0]) {\n\t\t\tpath->slots[0]--;\n\t\t\tbtrfs_item_key_to_cpu(path->nodes[0], &key,\n\t\t\t\t\t      path->slots[0]);\n\t\t\tif (key.objectid == bytenr &&\n\t\t\t    key.type == BTRFS_EXTENT_ITEM_KEY &&\n\t\t\t    key.offset == fs_info->nodesize)\n\t\t\t\tret = 0;\n\t\t}\n\t}\n\n\tif (ret == 0) {\n\t\tleaf = path->nodes[0];\n\t\titem_size = btrfs_item_size(leaf, path->slots[0]);\n\t\tif (item_size >= sizeof(*ei)) {\n\t\t\tei = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t\t    struct btrfs_extent_item);\n\t\t\tnum_refs = btrfs_extent_refs(leaf, ei);\n\t\t\textent_flags = btrfs_extent_flags(leaf, ei);\n\t\t} else {\n\t\t\tret = -EUCLEAN;\n\t\t\tbtrfs_err(fs_info,\n\t\t\t\"unexpected extent item size, has %u expect >= %zu\",\n\t\t\t\t  item_size, sizeof(*ei));\n\t\t\tif (trans)\n\t\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\telse\n\t\t\t\tbtrfs_handle_fs_error(fs_info, ret, NULL);\n\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tBUG_ON(num_refs == 0);\n\t} else {\n\t\tnum_refs = 0;\n\t\textent_flags = 0;\n\t\tret = 0;\n\t}\n\n\tif (!trans)\n\t\tgoto out;\n\n\tdelayed_refs = &trans->transaction->delayed_refs;\n\tspin_lock(&delayed_refs->lock);\n\thead = btrfs_find_delayed_ref_head(delayed_refs, bytenr);\n\tif (head) {\n\t\tif (!mutex_trylock(&head->mutex)) {\n\t\t\trefcount_inc(&head->refs);\n\t\t\tspin_unlock(&delayed_refs->lock);\n\n\t\t\tbtrfs_release_path(path);\n\n\t\t\t \n\t\t\tmutex_lock(&head->mutex);\n\t\t\tmutex_unlock(&head->mutex);\n\t\t\tbtrfs_put_delayed_ref_head(head);\n\t\t\tgoto search_again;\n\t\t}\n\t\tspin_lock(&head->lock);\n\t\tif (head->extent_op && head->extent_op->update_flags)\n\t\t\textent_flags |= head->extent_op->flags_to_set;\n\t\telse\n\t\t\tBUG_ON(num_refs == 0);\n\n\t\tnum_refs += head->ref_mod;\n\t\tspin_unlock(&head->lock);\n\t\tmutex_unlock(&head->mutex);\n\t}\n\tspin_unlock(&delayed_refs->lock);\nout:\n\tWARN_ON(num_refs == 0);\n\tif (refs)\n\t\t*refs = num_refs;\n\tif (flags)\n\t\t*flags = extent_flags;\nout_free:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n \n\n \nint btrfs_get_extent_inline_ref_type(const struct extent_buffer *eb,\n\t\t\t\t     struct btrfs_extent_inline_ref *iref,\n\t\t\t\t     enum btrfs_inline_ref_type is_data)\n{\n\tint type = btrfs_extent_inline_ref_type(eb, iref);\n\tu64 offset = btrfs_extent_inline_ref_offset(eb, iref);\n\n\tif (type == BTRFS_TREE_BLOCK_REF_KEY ||\n\t    type == BTRFS_SHARED_BLOCK_REF_KEY ||\n\t    type == BTRFS_SHARED_DATA_REF_KEY ||\n\t    type == BTRFS_EXTENT_DATA_REF_KEY) {\n\t\tif (is_data == BTRFS_REF_TYPE_BLOCK) {\n\t\t\tif (type == BTRFS_TREE_BLOCK_REF_KEY)\n\t\t\t\treturn type;\n\t\t\tif (type == BTRFS_SHARED_BLOCK_REF_KEY) {\n\t\t\t\tASSERT(eb->fs_info);\n\t\t\t\t \n\t\t\t\tif (offset &&\n\t\t\t\t    IS_ALIGNED(offset, eb->fs_info->sectorsize))\n\t\t\t\t\treturn type;\n\t\t\t}\n\t\t} else if (is_data == BTRFS_REF_TYPE_DATA) {\n\t\t\tif (type == BTRFS_EXTENT_DATA_REF_KEY)\n\t\t\t\treturn type;\n\t\t\tif (type == BTRFS_SHARED_DATA_REF_KEY) {\n\t\t\t\tASSERT(eb->fs_info);\n\t\t\t\t \n\t\t\t\tif (offset &&\n\t\t\t\t    IS_ALIGNED(offset, eb->fs_info->sectorsize))\n\t\t\t\t\treturn type;\n\t\t\t}\n\t\t} else {\n\t\t\tASSERT(is_data == BTRFS_REF_TYPE_ANY);\n\t\t\treturn type;\n\t\t}\n\t}\n\n\tWARN_ON(1);\n\tbtrfs_print_leaf(eb);\n\tbtrfs_err(eb->fs_info,\n\t\t  \"eb %llu iref 0x%lx invalid extent inline ref type %d\",\n\t\t  eb->start, (unsigned long)iref, type);\n\n\treturn BTRFS_REF_TYPE_INVALID;\n}\n\nu64 hash_extent_data_ref(u64 root_objectid, u64 owner, u64 offset)\n{\n\tu32 high_crc = ~(u32)0;\n\tu32 low_crc = ~(u32)0;\n\t__le64 lenum;\n\n\tlenum = cpu_to_le64(root_objectid);\n\thigh_crc = btrfs_crc32c(high_crc, &lenum, sizeof(lenum));\n\tlenum = cpu_to_le64(owner);\n\tlow_crc = btrfs_crc32c(low_crc, &lenum, sizeof(lenum));\n\tlenum = cpu_to_le64(offset);\n\tlow_crc = btrfs_crc32c(low_crc, &lenum, sizeof(lenum));\n\n\treturn ((u64)high_crc << 31) ^ (u64)low_crc;\n}\n\nstatic u64 hash_extent_data_ref_item(struct extent_buffer *leaf,\n\t\t\t\t     struct btrfs_extent_data_ref *ref)\n{\n\treturn hash_extent_data_ref(btrfs_extent_data_ref_root(leaf, ref),\n\t\t\t\t    btrfs_extent_data_ref_objectid(leaf, ref),\n\t\t\t\t    btrfs_extent_data_ref_offset(leaf, ref));\n}\n\nstatic int match_extent_data_ref(struct extent_buffer *leaf,\n\t\t\t\t struct btrfs_extent_data_ref *ref,\n\t\t\t\t u64 root_objectid, u64 owner, u64 offset)\n{\n\tif (btrfs_extent_data_ref_root(leaf, ref) != root_objectid ||\n\t    btrfs_extent_data_ref_objectid(leaf, ref) != owner ||\n\t    btrfs_extent_data_ref_offset(leaf, ref) != offset)\n\t\treturn 0;\n\treturn 1;\n}\n\nstatic noinline int lookup_extent_data_ref(struct btrfs_trans_handle *trans,\n\t\t\t\t\t   struct btrfs_path *path,\n\t\t\t\t\t   u64 bytenr, u64 parent,\n\t\t\t\t\t   u64 root_objectid,\n\t\t\t\t\t   u64 owner, u64 offset)\n{\n\tstruct btrfs_root *root = btrfs_extent_root(trans->fs_info, bytenr);\n\tstruct btrfs_key key;\n\tstruct btrfs_extent_data_ref *ref;\n\tstruct extent_buffer *leaf;\n\tu32 nritems;\n\tint ret;\n\tint recow;\n\tint err = -ENOENT;\n\n\tkey.objectid = bytenr;\n\tif (parent) {\n\t\tkey.type = BTRFS_SHARED_DATA_REF_KEY;\n\t\tkey.offset = parent;\n\t} else {\n\t\tkey.type = BTRFS_EXTENT_DATA_REF_KEY;\n\t\tkey.offset = hash_extent_data_ref(root_objectid,\n\t\t\t\t\t\t  owner, offset);\n\t}\nagain:\n\trecow = 0;\n\tret = btrfs_search_slot(trans, root, &key, path, -1, 1);\n\tif (ret < 0) {\n\t\terr = ret;\n\t\tgoto fail;\n\t}\n\n\tif (parent) {\n\t\tif (!ret)\n\t\t\treturn 0;\n\t\tgoto fail;\n\t}\n\n\tleaf = path->nodes[0];\n\tnritems = btrfs_header_nritems(leaf);\n\twhile (1) {\n\t\tif (path->slots[0] >= nritems) {\n\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\tif (ret < 0)\n\t\t\t\terr = ret;\n\t\t\tif (ret)\n\t\t\t\tgoto fail;\n\n\t\t\tleaf = path->nodes[0];\n\t\t\tnritems = btrfs_header_nritems(leaf);\n\t\t\trecow = 1;\n\t\t}\n\n\t\tbtrfs_item_key_to_cpu(leaf, &key, path->slots[0]);\n\t\tif (key.objectid != bytenr ||\n\t\t    key.type != BTRFS_EXTENT_DATA_REF_KEY)\n\t\t\tgoto fail;\n\n\t\tref = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t     struct btrfs_extent_data_ref);\n\n\t\tif (match_extent_data_ref(leaf, ref, root_objectid,\n\t\t\t\t\t  owner, offset)) {\n\t\t\tif (recow) {\n\t\t\t\tbtrfs_release_path(path);\n\t\t\t\tgoto again;\n\t\t\t}\n\t\t\terr = 0;\n\t\t\tbreak;\n\t\t}\n\t\tpath->slots[0]++;\n\t}\nfail:\n\treturn err;\n}\n\nstatic noinline int insert_extent_data_ref(struct btrfs_trans_handle *trans,\n\t\t\t\t\t   struct btrfs_path *path,\n\t\t\t\t\t   u64 bytenr, u64 parent,\n\t\t\t\t\t   u64 root_objectid, u64 owner,\n\t\t\t\t\t   u64 offset, int refs_to_add)\n{\n\tstruct btrfs_root *root = btrfs_extent_root(trans->fs_info, bytenr);\n\tstruct btrfs_key key;\n\tstruct extent_buffer *leaf;\n\tu32 size;\n\tu32 num_refs;\n\tint ret;\n\n\tkey.objectid = bytenr;\n\tif (parent) {\n\t\tkey.type = BTRFS_SHARED_DATA_REF_KEY;\n\t\tkey.offset = parent;\n\t\tsize = sizeof(struct btrfs_shared_data_ref);\n\t} else {\n\t\tkey.type = BTRFS_EXTENT_DATA_REF_KEY;\n\t\tkey.offset = hash_extent_data_ref(root_objectid,\n\t\t\t\t\t\t  owner, offset);\n\t\tsize = sizeof(struct btrfs_extent_data_ref);\n\t}\n\n\tret = btrfs_insert_empty_item(trans, root, path, &key, size);\n\tif (ret && ret != -EEXIST)\n\t\tgoto fail;\n\n\tleaf = path->nodes[0];\n\tif (parent) {\n\t\tstruct btrfs_shared_data_ref *ref;\n\t\tref = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t     struct btrfs_shared_data_ref);\n\t\tif (ret == 0) {\n\t\t\tbtrfs_set_shared_data_ref_count(leaf, ref, refs_to_add);\n\t\t} else {\n\t\t\tnum_refs = btrfs_shared_data_ref_count(leaf, ref);\n\t\t\tnum_refs += refs_to_add;\n\t\t\tbtrfs_set_shared_data_ref_count(leaf, ref, num_refs);\n\t\t}\n\t} else {\n\t\tstruct btrfs_extent_data_ref *ref;\n\t\twhile (ret == -EEXIST) {\n\t\t\tref = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t\t     struct btrfs_extent_data_ref);\n\t\t\tif (match_extent_data_ref(leaf, ref, root_objectid,\n\t\t\t\t\t\t  owner, offset))\n\t\t\t\tbreak;\n\t\t\tbtrfs_release_path(path);\n\t\t\tkey.offset++;\n\t\t\tret = btrfs_insert_empty_item(trans, root, path, &key,\n\t\t\t\t\t\t      size);\n\t\t\tif (ret && ret != -EEXIST)\n\t\t\t\tgoto fail;\n\n\t\t\tleaf = path->nodes[0];\n\t\t}\n\t\tref = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t     struct btrfs_extent_data_ref);\n\t\tif (ret == 0) {\n\t\t\tbtrfs_set_extent_data_ref_root(leaf, ref,\n\t\t\t\t\t\t       root_objectid);\n\t\t\tbtrfs_set_extent_data_ref_objectid(leaf, ref, owner);\n\t\t\tbtrfs_set_extent_data_ref_offset(leaf, ref, offset);\n\t\t\tbtrfs_set_extent_data_ref_count(leaf, ref, refs_to_add);\n\t\t} else {\n\t\t\tnum_refs = btrfs_extent_data_ref_count(leaf, ref);\n\t\t\tnum_refs += refs_to_add;\n\t\t\tbtrfs_set_extent_data_ref_count(leaf, ref, num_refs);\n\t\t}\n\t}\n\tbtrfs_mark_buffer_dirty(trans, leaf);\n\tret = 0;\nfail:\n\tbtrfs_release_path(path);\n\treturn ret;\n}\n\nstatic noinline int remove_extent_data_ref(struct btrfs_trans_handle *trans,\n\t\t\t\t\t   struct btrfs_root *root,\n\t\t\t\t\t   struct btrfs_path *path,\n\t\t\t\t\t   int refs_to_drop)\n{\n\tstruct btrfs_key key;\n\tstruct btrfs_extent_data_ref *ref1 = NULL;\n\tstruct btrfs_shared_data_ref *ref2 = NULL;\n\tstruct extent_buffer *leaf;\n\tu32 num_refs = 0;\n\tint ret = 0;\n\n\tleaf = path->nodes[0];\n\tbtrfs_item_key_to_cpu(leaf, &key, path->slots[0]);\n\n\tif (key.type == BTRFS_EXTENT_DATA_REF_KEY) {\n\t\tref1 = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t      struct btrfs_extent_data_ref);\n\t\tnum_refs = btrfs_extent_data_ref_count(leaf, ref1);\n\t} else if (key.type == BTRFS_SHARED_DATA_REF_KEY) {\n\t\tref2 = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t      struct btrfs_shared_data_ref);\n\t\tnum_refs = btrfs_shared_data_ref_count(leaf, ref2);\n\t} else {\n\t\tbtrfs_err(trans->fs_info,\n\t\t\t  \"unrecognized backref key (%llu %u %llu)\",\n\t\t\t  key.objectid, key.type, key.offset);\n\t\tbtrfs_abort_transaction(trans, -EUCLEAN);\n\t\treturn -EUCLEAN;\n\t}\n\n\tBUG_ON(num_refs < refs_to_drop);\n\tnum_refs -= refs_to_drop;\n\n\tif (num_refs == 0) {\n\t\tret = btrfs_del_item(trans, root, path);\n\t} else {\n\t\tif (key.type == BTRFS_EXTENT_DATA_REF_KEY)\n\t\t\tbtrfs_set_extent_data_ref_count(leaf, ref1, num_refs);\n\t\telse if (key.type == BTRFS_SHARED_DATA_REF_KEY)\n\t\t\tbtrfs_set_shared_data_ref_count(leaf, ref2, num_refs);\n\t\tbtrfs_mark_buffer_dirty(trans, leaf);\n\t}\n\treturn ret;\n}\n\nstatic noinline u32 extent_data_ref_count(struct btrfs_path *path,\n\t\t\t\t\t  struct btrfs_extent_inline_ref *iref)\n{\n\tstruct btrfs_key key;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_extent_data_ref *ref1;\n\tstruct btrfs_shared_data_ref *ref2;\n\tu32 num_refs = 0;\n\tint type;\n\n\tleaf = path->nodes[0];\n\tbtrfs_item_key_to_cpu(leaf, &key, path->slots[0]);\n\n\tif (iref) {\n\t\t \n\t\ttype = btrfs_get_extent_inline_ref_type(leaf, iref, BTRFS_REF_TYPE_DATA);\n\t\tASSERT(type != BTRFS_REF_TYPE_INVALID);\n\t\tif (type == BTRFS_EXTENT_DATA_REF_KEY) {\n\t\t\tref1 = (struct btrfs_extent_data_ref *)(&iref->offset);\n\t\t\tnum_refs = btrfs_extent_data_ref_count(leaf, ref1);\n\t\t} else {\n\t\t\tref2 = (struct btrfs_shared_data_ref *)(iref + 1);\n\t\t\tnum_refs = btrfs_shared_data_ref_count(leaf, ref2);\n\t\t}\n\t} else if (key.type == BTRFS_EXTENT_DATA_REF_KEY) {\n\t\tref1 = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t      struct btrfs_extent_data_ref);\n\t\tnum_refs = btrfs_extent_data_ref_count(leaf, ref1);\n\t} else if (key.type == BTRFS_SHARED_DATA_REF_KEY) {\n\t\tref2 = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t      struct btrfs_shared_data_ref);\n\t\tnum_refs = btrfs_shared_data_ref_count(leaf, ref2);\n\t} else {\n\t\tWARN_ON(1);\n\t}\n\treturn num_refs;\n}\n\nstatic noinline int lookup_tree_block_ref(struct btrfs_trans_handle *trans,\n\t\t\t\t\t  struct btrfs_path *path,\n\t\t\t\t\t  u64 bytenr, u64 parent,\n\t\t\t\t\t  u64 root_objectid)\n{\n\tstruct btrfs_root *root = btrfs_extent_root(trans->fs_info, bytenr);\n\tstruct btrfs_key key;\n\tint ret;\n\n\tkey.objectid = bytenr;\n\tif (parent) {\n\t\tkey.type = BTRFS_SHARED_BLOCK_REF_KEY;\n\t\tkey.offset = parent;\n\t} else {\n\t\tkey.type = BTRFS_TREE_BLOCK_REF_KEY;\n\t\tkey.offset = root_objectid;\n\t}\n\n\tret = btrfs_search_slot(trans, root, &key, path, -1, 1);\n\tif (ret > 0)\n\t\tret = -ENOENT;\n\treturn ret;\n}\n\nstatic noinline int insert_tree_block_ref(struct btrfs_trans_handle *trans,\n\t\t\t\t\t  struct btrfs_path *path,\n\t\t\t\t\t  u64 bytenr, u64 parent,\n\t\t\t\t\t  u64 root_objectid)\n{\n\tstruct btrfs_root *root = btrfs_extent_root(trans->fs_info, bytenr);\n\tstruct btrfs_key key;\n\tint ret;\n\n\tkey.objectid = bytenr;\n\tif (parent) {\n\t\tkey.type = BTRFS_SHARED_BLOCK_REF_KEY;\n\t\tkey.offset = parent;\n\t} else {\n\t\tkey.type = BTRFS_TREE_BLOCK_REF_KEY;\n\t\tkey.offset = root_objectid;\n\t}\n\n\tret = btrfs_insert_empty_item(trans, root, path, &key, 0);\n\tbtrfs_release_path(path);\n\treturn ret;\n}\n\nstatic inline int extent_ref_type(u64 parent, u64 owner)\n{\n\tint type;\n\tif (owner < BTRFS_FIRST_FREE_OBJECTID) {\n\t\tif (parent > 0)\n\t\t\ttype = BTRFS_SHARED_BLOCK_REF_KEY;\n\t\telse\n\t\t\ttype = BTRFS_TREE_BLOCK_REF_KEY;\n\t} else {\n\t\tif (parent > 0)\n\t\t\ttype = BTRFS_SHARED_DATA_REF_KEY;\n\t\telse\n\t\t\ttype = BTRFS_EXTENT_DATA_REF_KEY;\n\t}\n\treturn type;\n}\n\nstatic int find_next_key(struct btrfs_path *path, int level,\n\t\t\t struct btrfs_key *key)\n\n{\n\tfor (; level < BTRFS_MAX_LEVEL; level++) {\n\t\tif (!path->nodes[level])\n\t\t\tbreak;\n\t\tif (path->slots[level] + 1 >=\n\t\t    btrfs_header_nritems(path->nodes[level]))\n\t\t\tcontinue;\n\t\tif (level == 0)\n\t\t\tbtrfs_item_key_to_cpu(path->nodes[level], key,\n\t\t\t\t\t      path->slots[level] + 1);\n\t\telse\n\t\t\tbtrfs_node_key_to_cpu(path->nodes[level], key,\n\t\t\t\t\t      path->slots[level] + 1);\n\t\treturn 0;\n\t}\n\treturn 1;\n}\n\n \nstatic noinline_for_stack\nint lookup_inline_extent_backref(struct btrfs_trans_handle *trans,\n\t\t\t\t struct btrfs_path *path,\n\t\t\t\t struct btrfs_extent_inline_ref **ref_ret,\n\t\t\t\t u64 bytenr, u64 num_bytes,\n\t\t\t\t u64 parent, u64 root_objectid,\n\t\t\t\t u64 owner, u64 offset, int insert)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_root *root = btrfs_extent_root(fs_info, bytenr);\n\tstruct btrfs_key key;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_extent_item *ei;\n\tstruct btrfs_extent_inline_ref *iref;\n\tu64 flags;\n\tu64 item_size;\n\tunsigned long ptr;\n\tunsigned long end;\n\tint extra_size;\n\tint type;\n\tint want;\n\tint ret;\n\tint err = 0;\n\tbool skinny_metadata = btrfs_fs_incompat(fs_info, SKINNY_METADATA);\n\tint needed;\n\n\tkey.objectid = bytenr;\n\tkey.type = BTRFS_EXTENT_ITEM_KEY;\n\tkey.offset = num_bytes;\n\n\twant = extent_ref_type(parent, owner);\n\tif (insert) {\n\t\textra_size = btrfs_extent_inline_ref_size(want);\n\t\tpath->search_for_extension = 1;\n\t\tpath->keep_locks = 1;\n\t} else\n\t\textra_size = -1;\n\n\t \n\tif (skinny_metadata && owner < BTRFS_FIRST_FREE_OBJECTID) {\n\t\tkey.type = BTRFS_METADATA_ITEM_KEY;\n\t\tkey.offset = owner;\n\t}\n\nagain:\n\tret = btrfs_search_slot(trans, root, &key, path, extra_size, 1);\n\tif (ret < 0) {\n\t\terr = ret;\n\t\tgoto out;\n\t}\n\n\t \n\tif (ret > 0 && skinny_metadata) {\n\t\tskinny_metadata = false;\n\t\tif (path->slots[0]) {\n\t\t\tpath->slots[0]--;\n\t\t\tbtrfs_item_key_to_cpu(path->nodes[0], &key,\n\t\t\t\t\t      path->slots[0]);\n\t\t\tif (key.objectid == bytenr &&\n\t\t\t    key.type == BTRFS_EXTENT_ITEM_KEY &&\n\t\t\t    key.offset == num_bytes)\n\t\t\t\tret = 0;\n\t\t}\n\t\tif (ret) {\n\t\t\tkey.objectid = bytenr;\n\t\t\tkey.type = BTRFS_EXTENT_ITEM_KEY;\n\t\t\tkey.offset = num_bytes;\n\t\t\tbtrfs_release_path(path);\n\t\t\tgoto again;\n\t\t}\n\t}\n\n\tif (ret && !insert) {\n\t\terr = -ENOENT;\n\t\tgoto out;\n\t} else if (WARN_ON(ret)) {\n\t\tbtrfs_print_leaf(path->nodes[0]);\n\t\tbtrfs_err(fs_info,\n\"extent item not found for insert, bytenr %llu num_bytes %llu parent %llu root_objectid %llu owner %llu offset %llu\",\n\t\t\t  bytenr, num_bytes, parent, root_objectid, owner,\n\t\t\t  offset);\n\t\terr = -EIO;\n\t\tgoto out;\n\t}\n\n\tleaf = path->nodes[0];\n\titem_size = btrfs_item_size(leaf, path->slots[0]);\n\tif (unlikely(item_size < sizeof(*ei))) {\n\t\terr = -EUCLEAN;\n\t\tbtrfs_err(fs_info,\n\t\t\t  \"unexpected extent item size, has %llu expect >= %zu\",\n\t\t\t  item_size, sizeof(*ei));\n\t\tbtrfs_abort_transaction(trans, err);\n\t\tgoto out;\n\t}\n\n\tei = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_extent_item);\n\tflags = btrfs_extent_flags(leaf, ei);\n\n\tptr = (unsigned long)(ei + 1);\n\tend = (unsigned long)ei + item_size;\n\n\tif (flags & BTRFS_EXTENT_FLAG_TREE_BLOCK && !skinny_metadata) {\n\t\tptr += sizeof(struct btrfs_tree_block_info);\n\t\tBUG_ON(ptr > end);\n\t}\n\n\tif (owner >= BTRFS_FIRST_FREE_OBJECTID)\n\t\tneeded = BTRFS_REF_TYPE_DATA;\n\telse\n\t\tneeded = BTRFS_REF_TYPE_BLOCK;\n\n\terr = -ENOENT;\n\twhile (1) {\n\t\tif (ptr >= end) {\n\t\t\tif (ptr > end) {\n\t\t\t\terr = -EUCLEAN;\n\t\t\t\tbtrfs_print_leaf(path->nodes[0]);\n\t\t\t\tbtrfs_crit(fs_info,\n\"overrun extent record at slot %d while looking for inline extent for root %llu owner %llu offset %llu parent %llu\",\n\t\t\t\t\tpath->slots[0], root_objectid, owner, offset, parent);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\tiref = (struct btrfs_extent_inline_ref *)ptr;\n\t\ttype = btrfs_get_extent_inline_ref_type(leaf, iref, needed);\n\t\tif (type == BTRFS_REF_TYPE_INVALID) {\n\t\t\terr = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (want < type)\n\t\t\tbreak;\n\t\tif (want > type) {\n\t\t\tptr += btrfs_extent_inline_ref_size(type);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (type == BTRFS_EXTENT_DATA_REF_KEY) {\n\t\t\tstruct btrfs_extent_data_ref *dref;\n\t\t\tdref = (struct btrfs_extent_data_ref *)(&iref->offset);\n\t\t\tif (match_extent_data_ref(leaf, dref, root_objectid,\n\t\t\t\t\t\t  owner, offset)) {\n\t\t\t\terr = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (hash_extent_data_ref_item(leaf, dref) <\n\t\t\t    hash_extent_data_ref(root_objectid, owner, offset))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tu64 ref_offset;\n\t\t\tref_offset = btrfs_extent_inline_ref_offset(leaf, iref);\n\t\t\tif (parent > 0) {\n\t\t\t\tif (parent == ref_offset) {\n\t\t\t\t\terr = 0;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (ref_offset < parent)\n\t\t\t\t\tbreak;\n\t\t\t} else {\n\t\t\t\tif (root_objectid == ref_offset) {\n\t\t\t\t\terr = 0;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (ref_offset < root_objectid)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tptr += btrfs_extent_inline_ref_size(type);\n\t}\n\tif (err == -ENOENT && insert) {\n\t\tif (item_size + extra_size >=\n\t\t    BTRFS_MAX_EXTENT_ITEM_SIZE(root)) {\n\t\t\terr = -EAGAIN;\n\t\t\tgoto out;\n\t\t}\n\t\t \n\t\tif (find_next_key(path, 0, &key) == 0 &&\n\t\t    key.objectid == bytenr &&\n\t\t    key.type < BTRFS_BLOCK_GROUP_ITEM_KEY) {\n\t\t\terr = -EAGAIN;\n\t\t\tgoto out;\n\t\t}\n\t}\n\t*ref_ret = (struct btrfs_extent_inline_ref *)ptr;\nout:\n\tif (insert) {\n\t\tpath->keep_locks = 0;\n\t\tpath->search_for_extension = 0;\n\t\tbtrfs_unlock_up_safe(path, 1);\n\t}\n\treturn err;\n}\n\n \nstatic noinline_for_stack\nvoid setup_inline_extent_backref(struct btrfs_trans_handle *trans,\n\t\t\t\t struct btrfs_path *path,\n\t\t\t\t struct btrfs_extent_inline_ref *iref,\n\t\t\t\t u64 parent, u64 root_objectid,\n\t\t\t\t u64 owner, u64 offset, int refs_to_add,\n\t\t\t\t struct btrfs_delayed_extent_op *extent_op)\n{\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_extent_item *ei;\n\tunsigned long ptr;\n\tunsigned long end;\n\tunsigned long item_offset;\n\tu64 refs;\n\tint size;\n\tint type;\n\n\tleaf = path->nodes[0];\n\tei = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_extent_item);\n\titem_offset = (unsigned long)iref - (unsigned long)ei;\n\n\ttype = extent_ref_type(parent, owner);\n\tsize = btrfs_extent_inline_ref_size(type);\n\n\tbtrfs_extend_item(trans, path, size);\n\n\tei = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_extent_item);\n\trefs = btrfs_extent_refs(leaf, ei);\n\trefs += refs_to_add;\n\tbtrfs_set_extent_refs(leaf, ei, refs);\n\tif (extent_op)\n\t\t__run_delayed_extent_op(extent_op, leaf, ei);\n\n\tptr = (unsigned long)ei + item_offset;\n\tend = (unsigned long)ei + btrfs_item_size(leaf, path->slots[0]);\n\tif (ptr < end - size)\n\t\tmemmove_extent_buffer(leaf, ptr + size, ptr,\n\t\t\t\t      end - size - ptr);\n\n\tiref = (struct btrfs_extent_inline_ref *)ptr;\n\tbtrfs_set_extent_inline_ref_type(leaf, iref, type);\n\tif (type == BTRFS_EXTENT_DATA_REF_KEY) {\n\t\tstruct btrfs_extent_data_ref *dref;\n\t\tdref = (struct btrfs_extent_data_ref *)(&iref->offset);\n\t\tbtrfs_set_extent_data_ref_root(leaf, dref, root_objectid);\n\t\tbtrfs_set_extent_data_ref_objectid(leaf, dref, owner);\n\t\tbtrfs_set_extent_data_ref_offset(leaf, dref, offset);\n\t\tbtrfs_set_extent_data_ref_count(leaf, dref, refs_to_add);\n\t} else if (type == BTRFS_SHARED_DATA_REF_KEY) {\n\t\tstruct btrfs_shared_data_ref *sref;\n\t\tsref = (struct btrfs_shared_data_ref *)(iref + 1);\n\t\tbtrfs_set_shared_data_ref_count(leaf, sref, refs_to_add);\n\t\tbtrfs_set_extent_inline_ref_offset(leaf, iref, parent);\n\t} else if (type == BTRFS_SHARED_BLOCK_REF_KEY) {\n\t\tbtrfs_set_extent_inline_ref_offset(leaf, iref, parent);\n\t} else {\n\t\tbtrfs_set_extent_inline_ref_offset(leaf, iref, root_objectid);\n\t}\n\tbtrfs_mark_buffer_dirty(trans, leaf);\n}\n\nstatic int lookup_extent_backref(struct btrfs_trans_handle *trans,\n\t\t\t\t struct btrfs_path *path,\n\t\t\t\t struct btrfs_extent_inline_ref **ref_ret,\n\t\t\t\t u64 bytenr, u64 num_bytes, u64 parent,\n\t\t\t\t u64 root_objectid, u64 owner, u64 offset)\n{\n\tint ret;\n\n\tret = lookup_inline_extent_backref(trans, path, ref_ret, bytenr,\n\t\t\t\t\t   num_bytes, parent, root_objectid,\n\t\t\t\t\t   owner, offset, 0);\n\tif (ret != -ENOENT)\n\t\treturn ret;\n\n\tbtrfs_release_path(path);\n\t*ref_ret = NULL;\n\n\tif (owner < BTRFS_FIRST_FREE_OBJECTID) {\n\t\tret = lookup_tree_block_ref(trans, path, bytenr, parent,\n\t\t\t\t\t    root_objectid);\n\t} else {\n\t\tret = lookup_extent_data_ref(trans, path, bytenr, parent,\n\t\t\t\t\t     root_objectid, owner, offset);\n\t}\n\treturn ret;\n}\n\n \nstatic noinline_for_stack int update_inline_extent_backref(\n\t\t\t\t  struct btrfs_trans_handle *trans,\n\t\t\t\t  struct btrfs_path *path,\n\t\t\t\t  struct btrfs_extent_inline_ref *iref,\n\t\t\t\t  int refs_to_mod,\n\t\t\t\t  struct btrfs_delayed_extent_op *extent_op)\n{\n\tstruct extent_buffer *leaf = path->nodes[0];\n\tstruct btrfs_fs_info *fs_info = leaf->fs_info;\n\tstruct btrfs_extent_item *ei;\n\tstruct btrfs_extent_data_ref *dref = NULL;\n\tstruct btrfs_shared_data_ref *sref = NULL;\n\tunsigned long ptr;\n\tunsigned long end;\n\tu32 item_size;\n\tint size;\n\tint type;\n\tu64 refs;\n\n\tei = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_extent_item);\n\trefs = btrfs_extent_refs(leaf, ei);\n\tif (unlikely(refs_to_mod < 0 && refs + refs_to_mod <= 0)) {\n\t\tstruct btrfs_key key;\n\t\tu32 extent_size;\n\n\t\tbtrfs_item_key_to_cpu(leaf, &key, path->slots[0]);\n\t\tif (key.type == BTRFS_METADATA_ITEM_KEY)\n\t\t\textent_size = fs_info->nodesize;\n\t\telse\n\t\t\textent_size = key.offset;\n\t\tbtrfs_print_leaf(leaf);\n\t\tbtrfs_err(fs_info,\n\t\"invalid refs_to_mod for extent %llu num_bytes %u, has %d expect >= -%llu\",\n\t\t\t  key.objectid, extent_size, refs_to_mod, refs);\n\t\treturn -EUCLEAN;\n\t}\n\trefs += refs_to_mod;\n\tbtrfs_set_extent_refs(leaf, ei, refs);\n\tif (extent_op)\n\t\t__run_delayed_extent_op(extent_op, leaf, ei);\n\n\ttype = btrfs_get_extent_inline_ref_type(leaf, iref, BTRFS_REF_TYPE_ANY);\n\t \n\tif (unlikely(type == BTRFS_REF_TYPE_INVALID))\n\t\treturn -EUCLEAN;\n\n\tif (type == BTRFS_EXTENT_DATA_REF_KEY) {\n\t\tdref = (struct btrfs_extent_data_ref *)(&iref->offset);\n\t\trefs = btrfs_extent_data_ref_count(leaf, dref);\n\t} else if (type == BTRFS_SHARED_DATA_REF_KEY) {\n\t\tsref = (struct btrfs_shared_data_ref *)(iref + 1);\n\t\trefs = btrfs_shared_data_ref_count(leaf, sref);\n\t} else {\n\t\trefs = 1;\n\t\t \n\t\tif (unlikely(refs_to_mod != -1)) {\n\t\t\tstruct btrfs_key key;\n\n\t\t\tbtrfs_item_key_to_cpu(leaf, &key, path->slots[0]);\n\n\t\t\tbtrfs_print_leaf(leaf);\n\t\t\tbtrfs_err(fs_info,\n\t\t\t\"invalid refs_to_mod for tree block %llu, has %d expect -1\",\n\t\t\t\t  key.objectid, refs_to_mod);\n\t\t\treturn -EUCLEAN;\n\t\t}\n\t}\n\n\tif (unlikely(refs_to_mod < 0 && refs < -refs_to_mod)) {\n\t\tstruct btrfs_key key;\n\t\tu32 extent_size;\n\n\t\tbtrfs_item_key_to_cpu(leaf, &key, path->slots[0]);\n\t\tif (key.type == BTRFS_METADATA_ITEM_KEY)\n\t\t\textent_size = fs_info->nodesize;\n\t\telse\n\t\t\textent_size = key.offset;\n\t\tbtrfs_print_leaf(leaf);\n\t\tbtrfs_err(fs_info,\n\"invalid refs_to_mod for backref entry, iref %lu extent %llu num_bytes %u, has %d expect >= -%llu\",\n\t\t\t  (unsigned long)iref, key.objectid, extent_size,\n\t\t\t  refs_to_mod, refs);\n\t\treturn -EUCLEAN;\n\t}\n\trefs += refs_to_mod;\n\n\tif (refs > 0) {\n\t\tif (type == BTRFS_EXTENT_DATA_REF_KEY)\n\t\t\tbtrfs_set_extent_data_ref_count(leaf, dref, refs);\n\t\telse\n\t\t\tbtrfs_set_shared_data_ref_count(leaf, sref, refs);\n\t} else {\n\t\tsize =  btrfs_extent_inline_ref_size(type);\n\t\titem_size = btrfs_item_size(leaf, path->slots[0]);\n\t\tptr = (unsigned long)iref;\n\t\tend = (unsigned long)ei + item_size;\n\t\tif (ptr + size < end)\n\t\t\tmemmove_extent_buffer(leaf, ptr, ptr + size,\n\t\t\t\t\t      end - ptr - size);\n\t\titem_size -= size;\n\t\tbtrfs_truncate_item(trans, path, item_size, 1);\n\t}\n\tbtrfs_mark_buffer_dirty(trans, leaf);\n\treturn 0;\n}\n\nstatic noinline_for_stack\nint insert_inline_extent_backref(struct btrfs_trans_handle *trans,\n\t\t\t\t struct btrfs_path *path,\n\t\t\t\t u64 bytenr, u64 num_bytes, u64 parent,\n\t\t\t\t u64 root_objectid, u64 owner,\n\t\t\t\t u64 offset, int refs_to_add,\n\t\t\t\t struct btrfs_delayed_extent_op *extent_op)\n{\n\tstruct btrfs_extent_inline_ref *iref;\n\tint ret;\n\n\tret = lookup_inline_extent_backref(trans, path, &iref, bytenr,\n\t\t\t\t\t   num_bytes, parent, root_objectid,\n\t\t\t\t\t   owner, offset, 1);\n\tif (ret == 0) {\n\t\t \n\t\tif (owner < BTRFS_FIRST_FREE_OBJECTID) {\n\t\t\tbtrfs_print_leaf(path->nodes[0]);\n\t\t\tbtrfs_crit(trans->fs_info,\n\"adding refs to an existing tree ref, bytenr %llu num_bytes %llu root_objectid %llu slot %u\",\n\t\t\t\t   bytenr, num_bytes, root_objectid, path->slots[0]);\n\t\t\treturn -EUCLEAN;\n\t\t}\n\t\tret = update_inline_extent_backref(trans, path, iref,\n\t\t\t\t\t\t   refs_to_add, extent_op);\n\t} else if (ret == -ENOENT) {\n\t\tsetup_inline_extent_backref(trans, path, iref, parent,\n\t\t\t\t\t    root_objectid, owner, offset,\n\t\t\t\t\t    refs_to_add, extent_op);\n\t\tret = 0;\n\t}\n\treturn ret;\n}\n\nstatic int remove_extent_backref(struct btrfs_trans_handle *trans,\n\t\t\t\t struct btrfs_root *root,\n\t\t\t\t struct btrfs_path *path,\n\t\t\t\t struct btrfs_extent_inline_ref *iref,\n\t\t\t\t int refs_to_drop, int is_data)\n{\n\tint ret = 0;\n\n\tBUG_ON(!is_data && refs_to_drop != 1);\n\tif (iref)\n\t\tret = update_inline_extent_backref(trans, path, iref,\n\t\t\t\t\t\t   -refs_to_drop, NULL);\n\telse if (is_data)\n\t\tret = remove_extent_data_ref(trans, root, path, refs_to_drop);\n\telse\n\t\tret = btrfs_del_item(trans, root, path);\n\treturn ret;\n}\n\nstatic int btrfs_issue_discard(struct block_device *bdev, u64 start, u64 len,\n\t\t\t       u64 *discarded_bytes)\n{\n\tint j, ret = 0;\n\tu64 bytes_left, end;\n\tu64 aligned_start = ALIGN(start, 1 << SECTOR_SHIFT);\n\n\tif (WARN_ON(start != aligned_start)) {\n\t\tlen -= aligned_start - start;\n\t\tlen = round_down(len, 1 << SECTOR_SHIFT);\n\t\tstart = aligned_start;\n\t}\n\n\t*discarded_bytes = 0;\n\n\tif (!len)\n\t\treturn 0;\n\n\tend = start + len;\n\tbytes_left = len;\n\n\t \n\tfor (j = 0; j < BTRFS_SUPER_MIRROR_MAX; j++) {\n\t\tu64 sb_start = btrfs_sb_offset(j);\n\t\tu64 sb_end = sb_start + BTRFS_SUPER_INFO_SIZE;\n\t\tu64 size = sb_start - start;\n\n\t\tif (!in_range(sb_start, start, bytes_left) &&\n\t\t    !in_range(sb_end, start, bytes_left) &&\n\t\t    !in_range(start, sb_start, BTRFS_SUPER_INFO_SIZE))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (sb_start <= start) {\n\t\t\tstart += sb_end - start;\n\t\t\tif (start > end) {\n\t\t\t\tbytes_left = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbytes_left = end - start;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (size) {\n\t\t\tret = blkdev_issue_discard(bdev, start >> SECTOR_SHIFT,\n\t\t\t\t\t\t   size >> SECTOR_SHIFT,\n\t\t\t\t\t\t   GFP_NOFS);\n\t\t\tif (!ret)\n\t\t\t\t*discarded_bytes += size;\n\t\t\telse if (ret != -EOPNOTSUPP)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tstart = sb_end;\n\t\tif (start > end) {\n\t\t\tbytes_left = 0;\n\t\t\tbreak;\n\t\t}\n\t\tbytes_left = end - start;\n\t}\n\n\tif (bytes_left) {\n\t\tret = blkdev_issue_discard(bdev, start >> SECTOR_SHIFT,\n\t\t\t\t\t   bytes_left >> SECTOR_SHIFT,\n\t\t\t\t\t   GFP_NOFS);\n\t\tif (!ret)\n\t\t\t*discarded_bytes += bytes_left;\n\t}\n\treturn ret;\n}\n\nstatic int do_discard_extent(struct btrfs_discard_stripe *stripe, u64 *bytes)\n{\n\tstruct btrfs_device *dev = stripe->dev;\n\tstruct btrfs_fs_info *fs_info = dev->fs_info;\n\tstruct btrfs_dev_replace *dev_replace = &fs_info->dev_replace;\n\tu64 phys = stripe->physical;\n\tu64 len = stripe->length;\n\tu64 discarded = 0;\n\tint ret = 0;\n\n\t \n\tif (btrfs_can_zone_reset(dev, phys, len)) {\n\t\tu64 src_disc;\n\n\t\tret = btrfs_reset_device_zone(dev, phys, len, &discarded);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tif (!btrfs_dev_replace_is_ongoing(dev_replace) ||\n\t\t    dev != dev_replace->srcdev)\n\t\t\tgoto out;\n\n\t\tsrc_disc = discarded;\n\n\t\t \n\t\tret = btrfs_reset_device_zone(dev_replace->tgtdev, phys, len,\n\t\t\t\t\t      &discarded);\n\t\tdiscarded += src_disc;\n\t} else if (bdev_max_discard_sectors(stripe->dev->bdev)) {\n\t\tret = btrfs_issue_discard(dev->bdev, phys, len, &discarded);\n\t} else {\n\t\tret = 0;\n\t\t*bytes = 0;\n\t}\n\nout:\n\t*bytes = discarded;\n\treturn ret;\n}\n\nint btrfs_discard_extent(struct btrfs_fs_info *fs_info, u64 bytenr,\n\t\t\t u64 num_bytes, u64 *actual_bytes)\n{\n\tint ret = 0;\n\tu64 discarded_bytes = 0;\n\tu64 end = bytenr + num_bytes;\n\tu64 cur = bytenr;\n\n\t \n\tbtrfs_bio_counter_inc_blocked(fs_info);\n\twhile (cur < end) {\n\t\tstruct btrfs_discard_stripe *stripes;\n\t\tunsigned int num_stripes;\n\t\tint i;\n\n\t\tnum_bytes = end - cur;\n\t\tstripes = btrfs_map_discard(fs_info, cur, &num_bytes, &num_stripes);\n\t\tif (IS_ERR(stripes)) {\n\t\t\tret = PTR_ERR(stripes);\n\t\t\tif (ret == -EOPNOTSUPP)\n\t\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tfor (i = 0; i < num_stripes; i++) {\n\t\t\tstruct btrfs_discard_stripe *stripe = stripes + i;\n\t\t\tu64 bytes;\n\n\t\t\tif (!stripe->dev->bdev) {\n\t\t\t\tASSERT(btrfs_test_opt(fs_info, DEGRADED));\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (!test_bit(BTRFS_DEV_STATE_WRITEABLE,\n\t\t\t\t\t&stripe->dev->dev_state))\n\t\t\t\tcontinue;\n\n\t\t\tret = do_discard_extent(stripe, &bytes);\n\t\t\tif (ret) {\n\t\t\t\t \n\t\t\t\tif (ret != -EOPNOTSUPP)\n\t\t\t\t\tbreak;\n\t\t\t\tret = 0;\n\t\t\t} else {\n\t\t\t\tdiscarded_bytes += bytes;\n\t\t\t}\n\t\t}\n\t\tkfree(stripes);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tcur += num_bytes;\n\t}\n\tbtrfs_bio_counter_dec(fs_info);\n\tif (actual_bytes)\n\t\t*actual_bytes = discarded_bytes;\n\treturn ret;\n}\n\n \nint btrfs_inc_extent_ref(struct btrfs_trans_handle *trans,\n\t\t\t struct btrfs_ref *generic_ref)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tint ret;\n\n\tASSERT(generic_ref->type != BTRFS_REF_NOT_SET &&\n\t       generic_ref->action);\n\tBUG_ON(generic_ref->type == BTRFS_REF_METADATA &&\n\t       generic_ref->tree_ref.owning_root == BTRFS_TREE_LOG_OBJECTID);\n\n\tif (generic_ref->type == BTRFS_REF_METADATA)\n\t\tret = btrfs_add_delayed_tree_ref(trans, generic_ref, NULL);\n\telse\n\t\tret = btrfs_add_delayed_data_ref(trans, generic_ref, 0);\n\n\tbtrfs_ref_tree_mod(fs_info, generic_ref);\n\n\treturn ret;\n}\n\n \nstatic int __btrfs_inc_extent_ref(struct btrfs_trans_handle *trans,\n\t\t\t\t  struct btrfs_delayed_ref_node *node,\n\t\t\t\t  u64 parent, u64 root_objectid,\n\t\t\t\t  u64 owner, u64 offset, int refs_to_add,\n\t\t\t\t  struct btrfs_delayed_extent_op *extent_op)\n{\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_extent_item *item;\n\tstruct btrfs_key key;\n\tu64 bytenr = node->bytenr;\n\tu64 num_bytes = node->num_bytes;\n\tu64 refs;\n\tint ret;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\t \n\tret = insert_inline_extent_backref(trans, path, bytenr, num_bytes,\n\t\t\t\t\t   parent, root_objectid, owner,\n\t\t\t\t\t   offset, refs_to_add, extent_op);\n\tif ((ret < 0 && ret != -EAGAIN) || !ret)\n\t\tgoto out;\n\n\t \n\tleaf = path->nodes[0];\n\tbtrfs_item_key_to_cpu(leaf, &key, path->slots[0]);\n\titem = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_extent_item);\n\trefs = btrfs_extent_refs(leaf, item);\n\tbtrfs_set_extent_refs(leaf, item, refs + refs_to_add);\n\tif (extent_op)\n\t\t__run_delayed_extent_op(extent_op, leaf, item);\n\n\tbtrfs_mark_buffer_dirty(trans, leaf);\n\tbtrfs_release_path(path);\n\n\t \n\tif (owner < BTRFS_FIRST_FREE_OBJECTID)\n\t\tret = insert_tree_block_ref(trans, path, bytenr, parent,\n\t\t\t\t\t    root_objectid);\n\telse\n\t\tret = insert_extent_data_ref(trans, path, bytenr, parent,\n\t\t\t\t\t     root_objectid, owner, offset,\n\t\t\t\t\t     refs_to_add);\n\n\tif (ret)\n\t\tbtrfs_abort_transaction(trans, ret);\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nstatic int run_delayed_data_ref(struct btrfs_trans_handle *trans,\n\t\t\t\tstruct btrfs_delayed_ref_node *node,\n\t\t\t\tstruct btrfs_delayed_extent_op *extent_op,\n\t\t\t\tbool insert_reserved)\n{\n\tint ret = 0;\n\tstruct btrfs_delayed_data_ref *ref;\n\tstruct btrfs_key ins;\n\tu64 parent = 0;\n\tu64 ref_root = 0;\n\tu64 flags = 0;\n\n\tins.objectid = node->bytenr;\n\tins.offset = node->num_bytes;\n\tins.type = BTRFS_EXTENT_ITEM_KEY;\n\n\tref = btrfs_delayed_node_to_data_ref(node);\n\ttrace_run_delayed_data_ref(trans->fs_info, node, ref, node->action);\n\n\tif (node->type == BTRFS_SHARED_DATA_REF_KEY)\n\t\tparent = ref->parent;\n\tref_root = ref->root;\n\n\tif (node->action == BTRFS_ADD_DELAYED_REF && insert_reserved) {\n\t\tif (extent_op)\n\t\t\tflags |= extent_op->flags_to_set;\n\t\tret = alloc_reserved_file_extent(trans, parent, ref_root,\n\t\t\t\t\t\t flags, ref->objectid,\n\t\t\t\t\t\t ref->offset, &ins,\n\t\t\t\t\t\t node->ref_mod);\n\t} else if (node->action == BTRFS_ADD_DELAYED_REF) {\n\t\tret = __btrfs_inc_extent_ref(trans, node, parent, ref_root,\n\t\t\t\t\t     ref->objectid, ref->offset,\n\t\t\t\t\t     node->ref_mod, extent_op);\n\t} else if (node->action == BTRFS_DROP_DELAYED_REF) {\n\t\tret = __btrfs_free_extent(trans, node, parent,\n\t\t\t\t\t  ref_root, ref->objectid,\n\t\t\t\t\t  ref->offset, node->ref_mod,\n\t\t\t\t\t  extent_op);\n\t} else {\n\t\tBUG();\n\t}\n\treturn ret;\n}\n\nstatic void __run_delayed_extent_op(struct btrfs_delayed_extent_op *extent_op,\n\t\t\t\t    struct extent_buffer *leaf,\n\t\t\t\t    struct btrfs_extent_item *ei)\n{\n\tu64 flags = btrfs_extent_flags(leaf, ei);\n\tif (extent_op->update_flags) {\n\t\tflags |= extent_op->flags_to_set;\n\t\tbtrfs_set_extent_flags(leaf, ei, flags);\n\t}\n\n\tif (extent_op->update_key) {\n\t\tstruct btrfs_tree_block_info *bi;\n\t\tBUG_ON(!(flags & BTRFS_EXTENT_FLAG_TREE_BLOCK));\n\t\tbi = (struct btrfs_tree_block_info *)(ei + 1);\n\t\tbtrfs_set_tree_block_key(leaf, bi, &extent_op->key);\n\t}\n}\n\nstatic int run_delayed_extent_op(struct btrfs_trans_handle *trans,\n\t\t\t\t struct btrfs_delayed_ref_head *head,\n\t\t\t\t struct btrfs_delayed_extent_op *extent_op)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_root *root;\n\tstruct btrfs_key key;\n\tstruct btrfs_path *path;\n\tstruct btrfs_extent_item *ei;\n\tstruct extent_buffer *leaf;\n\tu32 item_size;\n\tint ret;\n\tint err = 0;\n\tint metadata = 1;\n\n\tif (TRANS_ABORTED(trans))\n\t\treturn 0;\n\n\tif (!btrfs_fs_incompat(fs_info, SKINNY_METADATA))\n\t\tmetadata = 0;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = head->bytenr;\n\n\tif (metadata) {\n\t\tkey.type = BTRFS_METADATA_ITEM_KEY;\n\t\tkey.offset = extent_op->level;\n\t} else {\n\t\tkey.type = BTRFS_EXTENT_ITEM_KEY;\n\t\tkey.offset = head->num_bytes;\n\t}\n\n\troot = btrfs_extent_root(fs_info, key.objectid);\nagain:\n\tret = btrfs_search_slot(trans, root, &key, path, 0, 1);\n\tif (ret < 0) {\n\t\terr = ret;\n\t\tgoto out;\n\t}\n\tif (ret > 0) {\n\t\tif (metadata) {\n\t\t\tif (path->slots[0] > 0) {\n\t\t\t\tpath->slots[0]--;\n\t\t\t\tbtrfs_item_key_to_cpu(path->nodes[0], &key,\n\t\t\t\t\t\t      path->slots[0]);\n\t\t\t\tif (key.objectid == head->bytenr &&\n\t\t\t\t    key.type == BTRFS_EXTENT_ITEM_KEY &&\n\t\t\t\t    key.offset == head->num_bytes)\n\t\t\t\t\tret = 0;\n\t\t\t}\n\t\t\tif (ret > 0) {\n\t\t\t\tbtrfs_release_path(path);\n\t\t\t\tmetadata = 0;\n\n\t\t\t\tkey.objectid = head->bytenr;\n\t\t\t\tkey.offset = head->num_bytes;\n\t\t\t\tkey.type = BTRFS_EXTENT_ITEM_KEY;\n\t\t\t\tgoto again;\n\t\t\t}\n\t\t} else {\n\t\t\terr = -EUCLEAN;\n\t\t\tbtrfs_err(fs_info,\n\t\t  \"missing extent item for extent %llu num_bytes %llu level %d\",\n\t\t\t\t  head->bytenr, head->num_bytes, extent_op->level);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tleaf = path->nodes[0];\n\titem_size = btrfs_item_size(leaf, path->slots[0]);\n\n\tif (unlikely(item_size < sizeof(*ei))) {\n\t\terr = -EUCLEAN;\n\t\tbtrfs_err(fs_info,\n\t\t\t  \"unexpected extent item size, has %u expect >= %zu\",\n\t\t\t  item_size, sizeof(*ei));\n\t\tbtrfs_abort_transaction(trans, err);\n\t\tgoto out;\n\t}\n\n\tei = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_extent_item);\n\t__run_delayed_extent_op(extent_op, leaf, ei);\n\n\tbtrfs_mark_buffer_dirty(trans, leaf);\nout:\n\tbtrfs_free_path(path);\n\treturn err;\n}\n\nstatic int run_delayed_tree_ref(struct btrfs_trans_handle *trans,\n\t\t\t\tstruct btrfs_delayed_ref_node *node,\n\t\t\t\tstruct btrfs_delayed_extent_op *extent_op,\n\t\t\t\tbool insert_reserved)\n{\n\tint ret = 0;\n\tstruct btrfs_delayed_tree_ref *ref;\n\tu64 parent = 0;\n\tu64 ref_root = 0;\n\n\tref = btrfs_delayed_node_to_tree_ref(node);\n\ttrace_run_delayed_tree_ref(trans->fs_info, node, ref, node->action);\n\n\tif (node->type == BTRFS_SHARED_BLOCK_REF_KEY)\n\t\tparent = ref->parent;\n\tref_root = ref->root;\n\n\tif (unlikely(node->ref_mod != 1)) {\n\t\tbtrfs_err(trans->fs_info,\n\t\"btree block %llu has %d references rather than 1: action %d ref_root %llu parent %llu\",\n\t\t\t  node->bytenr, node->ref_mod, node->action, ref_root,\n\t\t\t  parent);\n\t\treturn -EUCLEAN;\n\t}\n\tif (node->action == BTRFS_ADD_DELAYED_REF && insert_reserved) {\n\t\tBUG_ON(!extent_op || !extent_op->update_flags);\n\t\tret = alloc_reserved_tree_block(trans, node, extent_op);\n\t} else if (node->action == BTRFS_ADD_DELAYED_REF) {\n\t\tret = __btrfs_inc_extent_ref(trans, node, parent, ref_root,\n\t\t\t\t\t     ref->level, 0, 1, extent_op);\n\t} else if (node->action == BTRFS_DROP_DELAYED_REF) {\n\t\tret = __btrfs_free_extent(trans, node, parent, ref_root,\n\t\t\t\t\t  ref->level, 0, 1, extent_op);\n\t} else {\n\t\tBUG();\n\t}\n\treturn ret;\n}\n\n \nstatic int run_one_delayed_ref(struct btrfs_trans_handle *trans,\n\t\t\t       struct btrfs_delayed_ref_node *node,\n\t\t\t       struct btrfs_delayed_extent_op *extent_op,\n\t\t\t       bool insert_reserved)\n{\n\tint ret = 0;\n\n\tif (TRANS_ABORTED(trans)) {\n\t\tif (insert_reserved)\n\t\t\tbtrfs_pin_extent(trans, node->bytenr, node->num_bytes, 1);\n\t\treturn 0;\n\t}\n\n\tif (node->type == BTRFS_TREE_BLOCK_REF_KEY ||\n\t    node->type == BTRFS_SHARED_BLOCK_REF_KEY)\n\t\tret = run_delayed_tree_ref(trans, node, extent_op,\n\t\t\t\t\t   insert_reserved);\n\telse if (node->type == BTRFS_EXTENT_DATA_REF_KEY ||\n\t\t node->type == BTRFS_SHARED_DATA_REF_KEY)\n\t\tret = run_delayed_data_ref(trans, node, extent_op,\n\t\t\t\t\t   insert_reserved);\n\telse\n\t\tBUG();\n\tif (ret && insert_reserved)\n\t\tbtrfs_pin_extent(trans, node->bytenr, node->num_bytes, 1);\n\tif (ret < 0)\n\t\tbtrfs_err(trans->fs_info,\n\"failed to run delayed ref for logical %llu num_bytes %llu type %u action %u ref_mod %d: %d\",\n\t\t\t  node->bytenr, node->num_bytes, node->type,\n\t\t\t  node->action, node->ref_mod, ret);\n\treturn ret;\n}\n\nstatic inline struct btrfs_delayed_ref_node *\nselect_delayed_ref(struct btrfs_delayed_ref_head *head)\n{\n\tstruct btrfs_delayed_ref_node *ref;\n\n\tif (RB_EMPTY_ROOT(&head->ref_tree.rb_root))\n\t\treturn NULL;\n\n\t \n\tif (!list_empty(&head->ref_add_list))\n\t\treturn list_first_entry(&head->ref_add_list,\n\t\t\t\tstruct btrfs_delayed_ref_node, add_list);\n\n\tref = rb_entry(rb_first_cached(&head->ref_tree),\n\t\t       struct btrfs_delayed_ref_node, ref_node);\n\tASSERT(list_empty(&ref->add_list));\n\treturn ref;\n}\n\nstatic void unselect_delayed_ref_head(struct btrfs_delayed_ref_root *delayed_refs,\n\t\t\t\t      struct btrfs_delayed_ref_head *head)\n{\n\tspin_lock(&delayed_refs->lock);\n\thead->processing = false;\n\tdelayed_refs->num_heads_ready++;\n\tspin_unlock(&delayed_refs->lock);\n\tbtrfs_delayed_ref_unlock(head);\n}\n\nstatic struct btrfs_delayed_extent_op *cleanup_extent_op(\n\t\t\t\tstruct btrfs_delayed_ref_head *head)\n{\n\tstruct btrfs_delayed_extent_op *extent_op = head->extent_op;\n\n\tif (!extent_op)\n\t\treturn NULL;\n\n\tif (head->must_insert_reserved) {\n\t\thead->extent_op = NULL;\n\t\tbtrfs_free_delayed_extent_op(extent_op);\n\t\treturn NULL;\n\t}\n\treturn extent_op;\n}\n\nstatic int run_and_cleanup_extent_op(struct btrfs_trans_handle *trans,\n\t\t\t\t     struct btrfs_delayed_ref_head *head)\n{\n\tstruct btrfs_delayed_extent_op *extent_op;\n\tint ret;\n\n\textent_op = cleanup_extent_op(head);\n\tif (!extent_op)\n\t\treturn 0;\n\thead->extent_op = NULL;\n\tspin_unlock(&head->lock);\n\tret = run_delayed_extent_op(trans, head, extent_op);\n\tbtrfs_free_delayed_extent_op(extent_op);\n\treturn ret ? ret : 1;\n}\n\nvoid btrfs_cleanup_ref_head_accounting(struct btrfs_fs_info *fs_info,\n\t\t\t\t  struct btrfs_delayed_ref_root *delayed_refs,\n\t\t\t\t  struct btrfs_delayed_ref_head *head)\n{\n\tint nr_items = 1;\t \n\n\t \n\tif (head->total_ref_mod < 0 && head->is_data) {\n\t\tspin_lock(&delayed_refs->lock);\n\t\tdelayed_refs->pending_csums -= head->num_bytes;\n\t\tspin_unlock(&delayed_refs->lock);\n\t\tnr_items += btrfs_csum_bytes_to_leaves(fs_info, head->num_bytes);\n\t}\n\n\tbtrfs_delayed_refs_rsv_release(fs_info, nr_items);\n}\n\nstatic int cleanup_ref_head(struct btrfs_trans_handle *trans,\n\t\t\t    struct btrfs_delayed_ref_head *head)\n{\n\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_delayed_ref_root *delayed_refs;\n\tint ret;\n\n\tdelayed_refs = &trans->transaction->delayed_refs;\n\n\tret = run_and_cleanup_extent_op(trans, head);\n\tif (ret < 0) {\n\t\tunselect_delayed_ref_head(delayed_refs, head);\n\t\tbtrfs_debug(fs_info, \"run_delayed_extent_op returned %d\", ret);\n\t\treturn ret;\n\t} else if (ret) {\n\t\treturn ret;\n\t}\n\n\t \n\tspin_unlock(&head->lock);\n\tspin_lock(&delayed_refs->lock);\n\tspin_lock(&head->lock);\n\tif (!RB_EMPTY_ROOT(&head->ref_tree.rb_root) || head->extent_op) {\n\t\tspin_unlock(&head->lock);\n\t\tspin_unlock(&delayed_refs->lock);\n\t\treturn 1;\n\t}\n\tbtrfs_delete_ref_head(delayed_refs, head);\n\tspin_unlock(&head->lock);\n\tspin_unlock(&delayed_refs->lock);\n\n\tif (head->must_insert_reserved) {\n\t\tbtrfs_pin_extent(trans, head->bytenr, head->num_bytes, 1);\n\t\tif (head->is_data) {\n\t\t\tstruct btrfs_root *csum_root;\n\n\t\t\tcsum_root = btrfs_csum_root(fs_info, head->bytenr);\n\t\t\tret = btrfs_del_csums(trans, csum_root, head->bytenr,\n\t\t\t\t\t      head->num_bytes);\n\t\t}\n\t}\n\n\tbtrfs_cleanup_ref_head_accounting(fs_info, delayed_refs, head);\n\n\ttrace_run_delayed_ref_head(fs_info, head, 0);\n\tbtrfs_delayed_ref_unlock(head);\n\tbtrfs_put_delayed_ref_head(head);\n\treturn ret;\n}\n\nstatic struct btrfs_delayed_ref_head *btrfs_obtain_ref_head(\n\t\t\t\t\tstruct btrfs_trans_handle *trans)\n{\n\tstruct btrfs_delayed_ref_root *delayed_refs =\n\t\t&trans->transaction->delayed_refs;\n\tstruct btrfs_delayed_ref_head *head = NULL;\n\tint ret;\n\n\tspin_lock(&delayed_refs->lock);\n\thead = btrfs_select_ref_head(delayed_refs);\n\tif (!head) {\n\t\tspin_unlock(&delayed_refs->lock);\n\t\treturn head;\n\t}\n\n\t \n\tret = btrfs_delayed_ref_lock(delayed_refs, head);\n\tspin_unlock(&delayed_refs->lock);\n\n\t \n\tif (ret == -EAGAIN)\n\t\thead = ERR_PTR(-EAGAIN);\n\n\treturn head;\n}\n\nstatic int btrfs_run_delayed_refs_for_head(struct btrfs_trans_handle *trans,\n\t\t\t\t\t   struct btrfs_delayed_ref_head *locked_ref)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_delayed_ref_root *delayed_refs;\n\tstruct btrfs_delayed_extent_op *extent_op;\n\tstruct btrfs_delayed_ref_node *ref;\n\tbool must_insert_reserved;\n\tint ret;\n\n\tdelayed_refs = &trans->transaction->delayed_refs;\n\n\tlockdep_assert_held(&locked_ref->mutex);\n\tlockdep_assert_held(&locked_ref->lock);\n\n\twhile ((ref = select_delayed_ref(locked_ref))) {\n\t\tif (ref->seq &&\n\t\t    btrfs_check_delayed_seq(fs_info, ref->seq)) {\n\t\t\tspin_unlock(&locked_ref->lock);\n\t\t\tunselect_delayed_ref_head(delayed_refs, locked_ref);\n\t\t\treturn -EAGAIN;\n\t\t}\n\n\t\trb_erase_cached(&ref->ref_node, &locked_ref->ref_tree);\n\t\tRB_CLEAR_NODE(&ref->ref_node);\n\t\tif (!list_empty(&ref->add_list))\n\t\t\tlist_del(&ref->add_list);\n\t\t \n\t\tswitch (ref->action) {\n\t\tcase BTRFS_ADD_DELAYED_REF:\n\t\tcase BTRFS_ADD_DELAYED_EXTENT:\n\t\t\tlocked_ref->ref_mod -= ref->ref_mod;\n\t\t\tbreak;\n\t\tcase BTRFS_DROP_DELAYED_REF:\n\t\t\tlocked_ref->ref_mod += ref->ref_mod;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tWARN_ON(1);\n\t\t}\n\t\tatomic_dec(&delayed_refs->num_entries);\n\n\t\t \n\t\tmust_insert_reserved = locked_ref->must_insert_reserved;\n\t\tlocked_ref->must_insert_reserved = false;\n\n\t\textent_op = locked_ref->extent_op;\n\t\tlocked_ref->extent_op = NULL;\n\t\tspin_unlock(&locked_ref->lock);\n\n\t\tret = run_one_delayed_ref(trans, ref, extent_op,\n\t\t\t\t\t  must_insert_reserved);\n\n\t\tbtrfs_free_delayed_extent_op(extent_op);\n\t\tif (ret) {\n\t\t\tunselect_delayed_ref_head(delayed_refs, locked_ref);\n\t\t\tbtrfs_put_delayed_ref(ref);\n\t\t\treturn ret;\n\t\t}\n\n\t\tbtrfs_put_delayed_ref(ref);\n\t\tcond_resched();\n\n\t\tspin_lock(&locked_ref->lock);\n\t\tbtrfs_merge_delayed_refs(fs_info, delayed_refs, locked_ref);\n\t}\n\n\treturn 0;\n}\n\n \nstatic noinline int __btrfs_run_delayed_refs(struct btrfs_trans_handle *trans,\n\t\t\t\t\t     unsigned long nr)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_delayed_ref_root *delayed_refs;\n\tstruct btrfs_delayed_ref_head *locked_ref = NULL;\n\tint ret;\n\tunsigned long count = 0;\n\n\tdelayed_refs = &trans->transaction->delayed_refs;\n\tdo {\n\t\tif (!locked_ref) {\n\t\t\tlocked_ref = btrfs_obtain_ref_head(trans);\n\t\t\tif (IS_ERR_OR_NULL(locked_ref)) {\n\t\t\t\tif (PTR_ERR(locked_ref) == -EAGAIN) {\n\t\t\t\t\tcontinue;\n\t\t\t\t} else {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tcount++;\n\t\t}\n\t\t \n\t\tspin_lock(&locked_ref->lock);\n\t\tbtrfs_merge_delayed_refs(fs_info, delayed_refs, locked_ref);\n\n\t\tret = btrfs_run_delayed_refs_for_head(trans, locked_ref);\n\t\tif (ret < 0 && ret != -EAGAIN) {\n\t\t\t \n\t\t\treturn ret;\n\t\t} else if (!ret) {\n\t\t\t \n\t\t\tret = cleanup_ref_head(trans, locked_ref);\n\t\t\tif (ret > 0 ) {\n\t\t\t\t \n\t\t\t\tret = 0;\n\t\t\t\tcontinue;\n\t\t\t} else if (ret) {\n\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\n\t\t \n\n\t\tlocked_ref = NULL;\n\t\tcond_resched();\n\t} while ((nr != -1 && count < nr) || locked_ref);\n\n\treturn 0;\n}\n\n#ifdef SCRAMBLE_DELAYED_REFS\n \nstatic u64 find_middle(struct rb_root *root)\n{\n\tstruct rb_node *n = root->rb_node;\n\tstruct btrfs_delayed_ref_node *entry;\n\tint alt = 1;\n\tu64 middle;\n\tu64 first = 0, last = 0;\n\n\tn = rb_first(root);\n\tif (n) {\n\t\tentry = rb_entry(n, struct btrfs_delayed_ref_node, rb_node);\n\t\tfirst = entry->bytenr;\n\t}\n\tn = rb_last(root);\n\tif (n) {\n\t\tentry = rb_entry(n, struct btrfs_delayed_ref_node, rb_node);\n\t\tlast = entry->bytenr;\n\t}\n\tn = root->rb_node;\n\n\twhile (n) {\n\t\tentry = rb_entry(n, struct btrfs_delayed_ref_node, rb_node);\n\t\tWARN_ON(!entry->in_tree);\n\n\t\tmiddle = entry->bytenr;\n\n\t\tif (alt)\n\t\t\tn = n->rb_left;\n\t\telse\n\t\t\tn = n->rb_right;\n\n\t\talt = 1 - alt;\n\t}\n\treturn middle;\n}\n#endif\n\n \nint btrfs_run_delayed_refs(struct btrfs_trans_handle *trans,\n\t\t\t   unsigned long count)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct rb_node *node;\n\tstruct btrfs_delayed_ref_root *delayed_refs;\n\tstruct btrfs_delayed_ref_head *head;\n\tint ret;\n\tint run_all = count == (unsigned long)-1;\n\n\t \n\tif (TRANS_ABORTED(trans))\n\t\treturn 0;\n\n\tif (test_bit(BTRFS_FS_CREATING_FREE_SPACE_TREE, &fs_info->flags))\n\t\treturn 0;\n\n\tdelayed_refs = &trans->transaction->delayed_refs;\n\tif (count == 0)\n\t\tcount = delayed_refs->num_heads_ready;\n\nagain:\n#ifdef SCRAMBLE_DELAYED_REFS\n\tdelayed_refs->run_delayed_start = find_middle(&delayed_refs->root);\n#endif\n\tret = __btrfs_run_delayed_refs(trans, count);\n\tif (ret < 0) {\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\treturn ret;\n\t}\n\n\tif (run_all) {\n\t\tbtrfs_create_pending_block_groups(trans);\n\n\t\tspin_lock(&delayed_refs->lock);\n\t\tnode = rb_first_cached(&delayed_refs->href_root);\n\t\tif (!node) {\n\t\t\tspin_unlock(&delayed_refs->lock);\n\t\t\tgoto out;\n\t\t}\n\t\thead = rb_entry(node, struct btrfs_delayed_ref_head,\n\t\t\t\thref_node);\n\t\trefcount_inc(&head->refs);\n\t\tspin_unlock(&delayed_refs->lock);\n\n\t\t \n\t\tmutex_lock(&head->mutex);\n\t\tmutex_unlock(&head->mutex);\n\n\t\tbtrfs_put_delayed_ref_head(head);\n\t\tcond_resched();\n\t\tgoto again;\n\t}\nout:\n\treturn 0;\n}\n\nint btrfs_set_disk_extent_flags(struct btrfs_trans_handle *trans,\n\t\t\t\tstruct extent_buffer *eb, u64 flags)\n{\n\tstruct btrfs_delayed_extent_op *extent_op;\n\tint level = btrfs_header_level(eb);\n\tint ret;\n\n\textent_op = btrfs_alloc_delayed_extent_op();\n\tif (!extent_op)\n\t\treturn -ENOMEM;\n\n\textent_op->flags_to_set = flags;\n\textent_op->update_flags = true;\n\textent_op->update_key = false;\n\textent_op->level = level;\n\n\tret = btrfs_add_delayed_extent_op(trans, eb->start, eb->len, extent_op);\n\tif (ret)\n\t\tbtrfs_free_delayed_extent_op(extent_op);\n\treturn ret;\n}\n\nstatic noinline int check_delayed_ref(struct btrfs_root *root,\n\t\t\t\t      struct btrfs_path *path,\n\t\t\t\t      u64 objectid, u64 offset, u64 bytenr)\n{\n\tstruct btrfs_delayed_ref_head *head;\n\tstruct btrfs_delayed_ref_node *ref;\n\tstruct btrfs_delayed_data_ref *data_ref;\n\tstruct btrfs_delayed_ref_root *delayed_refs;\n\tstruct btrfs_transaction *cur_trans;\n\tstruct rb_node *node;\n\tint ret = 0;\n\n\tspin_lock(&root->fs_info->trans_lock);\n\tcur_trans = root->fs_info->running_transaction;\n\tif (cur_trans)\n\t\trefcount_inc(&cur_trans->use_count);\n\tspin_unlock(&root->fs_info->trans_lock);\n\tif (!cur_trans)\n\t\treturn 0;\n\n\tdelayed_refs = &cur_trans->delayed_refs;\n\tspin_lock(&delayed_refs->lock);\n\thead = btrfs_find_delayed_ref_head(delayed_refs, bytenr);\n\tif (!head) {\n\t\tspin_unlock(&delayed_refs->lock);\n\t\tbtrfs_put_transaction(cur_trans);\n\t\treturn 0;\n\t}\n\n\tif (!mutex_trylock(&head->mutex)) {\n\t\tif (path->nowait) {\n\t\t\tspin_unlock(&delayed_refs->lock);\n\t\t\tbtrfs_put_transaction(cur_trans);\n\t\t\treturn -EAGAIN;\n\t\t}\n\n\t\trefcount_inc(&head->refs);\n\t\tspin_unlock(&delayed_refs->lock);\n\n\t\tbtrfs_release_path(path);\n\n\t\t \n\t\tmutex_lock(&head->mutex);\n\t\tmutex_unlock(&head->mutex);\n\t\tbtrfs_put_delayed_ref_head(head);\n\t\tbtrfs_put_transaction(cur_trans);\n\t\treturn -EAGAIN;\n\t}\n\tspin_unlock(&delayed_refs->lock);\n\n\tspin_lock(&head->lock);\n\t \n\tfor (node = rb_first_cached(&head->ref_tree); node;\n\t     node = rb_next(node)) {\n\t\tref = rb_entry(node, struct btrfs_delayed_ref_node, ref_node);\n\t\t \n\t\tif (ref->type != BTRFS_EXTENT_DATA_REF_KEY) {\n\t\t\tret = 1;\n\t\t\tbreak;\n\t\t}\n\n\t\tdata_ref = btrfs_delayed_node_to_data_ref(ref);\n\n\t\t \n\t\tif (data_ref->root != root->root_key.objectid ||\n\t\t    data_ref->objectid != objectid ||\n\t\t    data_ref->offset != offset) {\n\t\t\tret = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&head->lock);\n\tmutex_unlock(&head->mutex);\n\tbtrfs_put_transaction(cur_trans);\n\treturn ret;\n}\n\nstatic noinline int check_committed_ref(struct btrfs_root *root,\n\t\t\t\t\tstruct btrfs_path *path,\n\t\t\t\t\tu64 objectid, u64 offset, u64 bytenr,\n\t\t\t\t\tbool strict)\n{\n\tstruct btrfs_fs_info *fs_info = root->fs_info;\n\tstruct btrfs_root *extent_root = btrfs_extent_root(fs_info, bytenr);\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_extent_data_ref *ref;\n\tstruct btrfs_extent_inline_ref *iref;\n\tstruct btrfs_extent_item *ei;\n\tstruct btrfs_key key;\n\tu32 item_size;\n\tint type;\n\tint ret;\n\n\tkey.objectid = bytenr;\n\tkey.offset = (u64)-1;\n\tkey.type = BTRFS_EXTENT_ITEM_KEY;\n\n\tret = btrfs_search_slot(NULL, extent_root, &key, path, 0, 0);\n\tif (ret < 0)\n\t\tgoto out;\n\tBUG_ON(ret == 0);  \n\n\tret = -ENOENT;\n\tif (path->slots[0] == 0)\n\t\tgoto out;\n\n\tpath->slots[0]--;\n\tleaf = path->nodes[0];\n\tbtrfs_item_key_to_cpu(leaf, &key, path->slots[0]);\n\n\tif (key.objectid != bytenr || key.type != BTRFS_EXTENT_ITEM_KEY)\n\t\tgoto out;\n\n\tret = 1;\n\titem_size = btrfs_item_size(leaf, path->slots[0]);\n\tei = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_extent_item);\n\n\t \n\tif (item_size != sizeof(*ei) +\n\t    btrfs_extent_inline_ref_size(BTRFS_EXTENT_DATA_REF_KEY))\n\t\tgoto out;\n\n\t \n\tif (!strict &&\n\t    (btrfs_extent_generation(leaf, ei) <=\n\t     btrfs_root_last_snapshot(&root->root_item)))\n\t\tgoto out;\n\n\tiref = (struct btrfs_extent_inline_ref *)(ei + 1);\n\n\t \n\ttype = btrfs_get_extent_inline_ref_type(leaf, iref, BTRFS_REF_TYPE_DATA);\n\tif (type != BTRFS_EXTENT_DATA_REF_KEY)\n\t\tgoto out;\n\n\tref = (struct btrfs_extent_data_ref *)(&iref->offset);\n\tif (btrfs_extent_refs(leaf, ei) !=\n\t    btrfs_extent_data_ref_count(leaf, ref) ||\n\t    btrfs_extent_data_ref_root(leaf, ref) !=\n\t    root->root_key.objectid ||\n\t    btrfs_extent_data_ref_objectid(leaf, ref) != objectid ||\n\t    btrfs_extent_data_ref_offset(leaf, ref) != offset)\n\t\tgoto out;\n\n\tret = 0;\nout:\n\treturn ret;\n}\n\nint btrfs_cross_ref_exist(struct btrfs_root *root, u64 objectid, u64 offset,\n\t\t\t  u64 bytenr, bool strict, struct btrfs_path *path)\n{\n\tint ret;\n\n\tdo {\n\t\tret = check_committed_ref(root, path, objectid,\n\t\t\t\t\t  offset, bytenr, strict);\n\t\tif (ret && ret != -ENOENT)\n\t\t\tgoto out;\n\n\t\tret = check_delayed_ref(root, path, objectid, offset, bytenr);\n\t} while (ret == -EAGAIN);\n\nout:\n\tbtrfs_release_path(path);\n\tif (btrfs_is_data_reloc_root(root))\n\t\tWARN_ON(ret > 0);\n\treturn ret;\n}\n\nstatic int __btrfs_mod_ref(struct btrfs_trans_handle *trans,\n\t\t\t   struct btrfs_root *root,\n\t\t\t   struct extent_buffer *buf,\n\t\t\t   int full_backref, int inc)\n{\n\tstruct btrfs_fs_info *fs_info = root->fs_info;\n\tu64 bytenr;\n\tu64 num_bytes;\n\tu64 parent;\n\tu64 ref_root;\n\tu32 nritems;\n\tstruct btrfs_key key;\n\tstruct btrfs_file_extent_item *fi;\n\tstruct btrfs_ref generic_ref = { 0 };\n\tbool for_reloc = btrfs_header_flag(buf, BTRFS_HEADER_FLAG_RELOC);\n\tint i;\n\tint action;\n\tint level;\n\tint ret = 0;\n\n\tif (btrfs_is_testing(fs_info))\n\t\treturn 0;\n\n\tref_root = btrfs_header_owner(buf);\n\tnritems = btrfs_header_nritems(buf);\n\tlevel = btrfs_header_level(buf);\n\n\tif (!test_bit(BTRFS_ROOT_SHAREABLE, &root->state) && level == 0)\n\t\treturn 0;\n\n\tif (full_backref)\n\t\tparent = buf->start;\n\telse\n\t\tparent = 0;\n\tif (inc)\n\t\taction = BTRFS_ADD_DELAYED_REF;\n\telse\n\t\taction = BTRFS_DROP_DELAYED_REF;\n\n\tfor (i = 0; i < nritems; i++) {\n\t\tif (level == 0) {\n\t\t\tbtrfs_item_key_to_cpu(buf, &key, i);\n\t\t\tif (key.type != BTRFS_EXTENT_DATA_KEY)\n\t\t\t\tcontinue;\n\t\t\tfi = btrfs_item_ptr(buf, i,\n\t\t\t\t\t    struct btrfs_file_extent_item);\n\t\t\tif (btrfs_file_extent_type(buf, fi) ==\n\t\t\t    BTRFS_FILE_EXTENT_INLINE)\n\t\t\t\tcontinue;\n\t\t\tbytenr = btrfs_file_extent_disk_bytenr(buf, fi);\n\t\t\tif (bytenr == 0)\n\t\t\t\tcontinue;\n\n\t\t\tnum_bytes = btrfs_file_extent_disk_num_bytes(buf, fi);\n\t\t\tkey.offset -= btrfs_file_extent_offset(buf, fi);\n\t\t\tbtrfs_init_generic_ref(&generic_ref, action, bytenr,\n\t\t\t\t\t       num_bytes, parent);\n\t\t\tbtrfs_init_data_ref(&generic_ref, ref_root, key.objectid,\n\t\t\t\t\t    key.offset, root->root_key.objectid,\n\t\t\t\t\t    for_reloc);\n\t\t\tif (inc)\n\t\t\t\tret = btrfs_inc_extent_ref(trans, &generic_ref);\n\t\t\telse\n\t\t\t\tret = btrfs_free_extent(trans, &generic_ref);\n\t\t\tif (ret)\n\t\t\t\tgoto fail;\n\t\t} else {\n\t\t\tbytenr = btrfs_node_blockptr(buf, i);\n\t\t\tnum_bytes = fs_info->nodesize;\n\t\t\tbtrfs_init_generic_ref(&generic_ref, action, bytenr,\n\t\t\t\t\t       num_bytes, parent);\n\t\t\tbtrfs_init_tree_ref(&generic_ref, level - 1, ref_root,\n\t\t\t\t\t    root->root_key.objectid, for_reloc);\n\t\t\tif (inc)\n\t\t\t\tret = btrfs_inc_extent_ref(trans, &generic_ref);\n\t\t\telse\n\t\t\t\tret = btrfs_free_extent(trans, &generic_ref);\n\t\t\tif (ret)\n\t\t\t\tgoto fail;\n\t\t}\n\t}\n\treturn 0;\nfail:\n\treturn ret;\n}\n\nint btrfs_inc_ref(struct btrfs_trans_handle *trans, struct btrfs_root *root,\n\t\t  struct extent_buffer *buf, int full_backref)\n{\n\treturn __btrfs_mod_ref(trans, root, buf, full_backref, 1);\n}\n\nint btrfs_dec_ref(struct btrfs_trans_handle *trans, struct btrfs_root *root,\n\t\t  struct extent_buffer *buf, int full_backref)\n{\n\treturn __btrfs_mod_ref(trans, root, buf, full_backref, 0);\n}\n\nstatic u64 get_alloc_profile_by_root(struct btrfs_root *root, int data)\n{\n\tstruct btrfs_fs_info *fs_info = root->fs_info;\n\tu64 flags;\n\tu64 ret;\n\n\tif (data)\n\t\tflags = BTRFS_BLOCK_GROUP_DATA;\n\telse if (root == fs_info->chunk_root)\n\t\tflags = BTRFS_BLOCK_GROUP_SYSTEM;\n\telse\n\t\tflags = BTRFS_BLOCK_GROUP_METADATA;\n\n\tret = btrfs_get_alloc_profile(fs_info, flags);\n\treturn ret;\n}\n\nstatic u64 first_logical_byte(struct btrfs_fs_info *fs_info)\n{\n\tstruct rb_node *leftmost;\n\tu64 bytenr = 0;\n\n\tread_lock(&fs_info->block_group_cache_lock);\n\t \n\tleftmost = rb_first_cached(&fs_info->block_group_cache_tree);\n\tif (leftmost) {\n\t\tstruct btrfs_block_group *bg;\n\n\t\tbg = rb_entry(leftmost, struct btrfs_block_group, cache_node);\n\t\tbytenr = bg->start;\n\t}\n\tread_unlock(&fs_info->block_group_cache_lock);\n\n\treturn bytenr;\n}\n\nstatic int pin_down_extent(struct btrfs_trans_handle *trans,\n\t\t\t   struct btrfs_block_group *cache,\n\t\t\t   u64 bytenr, u64 num_bytes, int reserved)\n{\n\tstruct btrfs_fs_info *fs_info = cache->fs_info;\n\n\tspin_lock(&cache->space_info->lock);\n\tspin_lock(&cache->lock);\n\tcache->pinned += num_bytes;\n\tbtrfs_space_info_update_bytes_pinned(fs_info, cache->space_info,\n\t\t\t\t\t     num_bytes);\n\tif (reserved) {\n\t\tcache->reserved -= num_bytes;\n\t\tcache->space_info->bytes_reserved -= num_bytes;\n\t}\n\tspin_unlock(&cache->lock);\n\tspin_unlock(&cache->space_info->lock);\n\n\tset_extent_bit(&trans->transaction->pinned_extents, bytenr,\n\t\t       bytenr + num_bytes - 1, EXTENT_DIRTY, NULL);\n\treturn 0;\n}\n\nint btrfs_pin_extent(struct btrfs_trans_handle *trans,\n\t\t     u64 bytenr, u64 num_bytes, int reserved)\n{\n\tstruct btrfs_block_group *cache;\n\n\tcache = btrfs_lookup_block_group(trans->fs_info, bytenr);\n\tBUG_ON(!cache);  \n\n\tpin_down_extent(trans, cache, bytenr, num_bytes, reserved);\n\n\tbtrfs_put_block_group(cache);\n\treturn 0;\n}\n\n \nint btrfs_pin_extent_for_log_replay(struct btrfs_trans_handle *trans,\n\t\t\t\t    u64 bytenr, u64 num_bytes)\n{\n\tstruct btrfs_block_group *cache;\n\tint ret;\n\n\tcache = btrfs_lookup_block_group(trans->fs_info, bytenr);\n\tif (!cache)\n\t\treturn -EINVAL;\n\n\t \n\tret = btrfs_cache_block_group(cache, true);\n\tif (ret)\n\t\tgoto out;\n\n\tpin_down_extent(trans, cache, bytenr, num_bytes, 0);\n\n\t \n\tret = btrfs_remove_free_space(cache, bytenr, num_bytes);\nout:\n\tbtrfs_put_block_group(cache);\n\treturn ret;\n}\n\nstatic int __exclude_logged_extent(struct btrfs_fs_info *fs_info,\n\t\t\t\t   u64 start, u64 num_bytes)\n{\n\tint ret;\n\tstruct btrfs_block_group *block_group;\n\n\tblock_group = btrfs_lookup_block_group(fs_info, start);\n\tif (!block_group)\n\t\treturn -EINVAL;\n\n\tret = btrfs_cache_block_group(block_group, true);\n\tif (ret)\n\t\tgoto out;\n\n\tret = btrfs_remove_free_space(block_group, start, num_bytes);\nout:\n\tbtrfs_put_block_group(block_group);\n\treturn ret;\n}\n\nint btrfs_exclude_logged_extents(struct extent_buffer *eb)\n{\n\tstruct btrfs_fs_info *fs_info = eb->fs_info;\n\tstruct btrfs_file_extent_item *item;\n\tstruct btrfs_key key;\n\tint found_type;\n\tint i;\n\tint ret = 0;\n\n\tif (!btrfs_fs_incompat(fs_info, MIXED_GROUPS))\n\t\treturn 0;\n\n\tfor (i = 0; i < btrfs_header_nritems(eb); i++) {\n\t\tbtrfs_item_key_to_cpu(eb, &key, i);\n\t\tif (key.type != BTRFS_EXTENT_DATA_KEY)\n\t\t\tcontinue;\n\t\titem = btrfs_item_ptr(eb, i, struct btrfs_file_extent_item);\n\t\tfound_type = btrfs_file_extent_type(eb, item);\n\t\tif (found_type == BTRFS_FILE_EXTENT_INLINE)\n\t\t\tcontinue;\n\t\tif (btrfs_file_extent_disk_bytenr(eb, item) == 0)\n\t\t\tcontinue;\n\t\tkey.objectid = btrfs_file_extent_disk_bytenr(eb, item);\n\t\tkey.offset = btrfs_file_extent_disk_num_bytes(eb, item);\n\t\tret = __exclude_logged_extent(fs_info, key.objectid, key.offset);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic void\nbtrfs_inc_block_group_reservations(struct btrfs_block_group *bg)\n{\n\tatomic_inc(&bg->reservations);\n}\n\n \nstatic struct btrfs_free_cluster *\nfetch_cluster_info(struct btrfs_fs_info *fs_info,\n\t\t   struct btrfs_space_info *space_info, u64 *empty_cluster)\n{\n\tstruct btrfs_free_cluster *ret = NULL;\n\n\t*empty_cluster = 0;\n\tif (btrfs_mixed_space_info(space_info))\n\t\treturn ret;\n\n\tif (space_info->flags & BTRFS_BLOCK_GROUP_METADATA) {\n\t\tret = &fs_info->meta_alloc_cluster;\n\t\tif (btrfs_test_opt(fs_info, SSD))\n\t\t\t*empty_cluster = SZ_2M;\n\t\telse\n\t\t\t*empty_cluster = SZ_64K;\n\t} else if ((space_info->flags & BTRFS_BLOCK_GROUP_DATA) &&\n\t\t   btrfs_test_opt(fs_info, SSD_SPREAD)) {\n\t\t*empty_cluster = SZ_2M;\n\t\tret = &fs_info->data_alloc_cluster;\n\t}\n\n\treturn ret;\n}\n\nstatic int unpin_extent_range(struct btrfs_fs_info *fs_info,\n\t\t\t      u64 start, u64 end,\n\t\t\t      const bool return_free_space)\n{\n\tstruct btrfs_block_group *cache = NULL;\n\tstruct btrfs_space_info *space_info;\n\tstruct btrfs_block_rsv *global_rsv = &fs_info->global_block_rsv;\n\tstruct btrfs_free_cluster *cluster = NULL;\n\tu64 len;\n\tu64 total_unpinned = 0;\n\tu64 empty_cluster = 0;\n\tbool readonly;\n\n\twhile (start <= end) {\n\t\treadonly = false;\n\t\tif (!cache ||\n\t\t    start >= cache->start + cache->length) {\n\t\t\tif (cache)\n\t\t\t\tbtrfs_put_block_group(cache);\n\t\t\ttotal_unpinned = 0;\n\t\t\tcache = btrfs_lookup_block_group(fs_info, start);\n\t\t\tBUG_ON(!cache);  \n\n\t\t\tcluster = fetch_cluster_info(fs_info,\n\t\t\t\t\t\t     cache->space_info,\n\t\t\t\t\t\t     &empty_cluster);\n\t\t\tempty_cluster <<= 1;\n\t\t}\n\n\t\tlen = cache->start + cache->length - start;\n\t\tlen = min(len, end + 1 - start);\n\n\t\tif (return_free_space)\n\t\t\tbtrfs_add_free_space(cache, start, len);\n\n\t\tstart += len;\n\t\ttotal_unpinned += len;\n\t\tspace_info = cache->space_info;\n\n\t\t \n\t\tif (cluster && cluster->fragmented &&\n\t\t    total_unpinned > empty_cluster) {\n\t\t\tspin_lock(&cluster->lock);\n\t\t\tcluster->fragmented = 0;\n\t\t\tspin_unlock(&cluster->lock);\n\t\t}\n\n\t\tspin_lock(&space_info->lock);\n\t\tspin_lock(&cache->lock);\n\t\tcache->pinned -= len;\n\t\tbtrfs_space_info_update_bytes_pinned(fs_info, space_info, -len);\n\t\tspace_info->max_extent_size = 0;\n\t\tif (cache->ro) {\n\t\t\tspace_info->bytes_readonly += len;\n\t\t\treadonly = true;\n\t\t} else if (btrfs_is_zoned(fs_info)) {\n\t\t\t \n\t\t\tspace_info->bytes_zone_unusable += len;\n\t\t\treadonly = true;\n\t\t}\n\t\tspin_unlock(&cache->lock);\n\t\tif (!readonly && return_free_space &&\n\t\t    global_rsv->space_info == space_info) {\n\t\t\tspin_lock(&global_rsv->lock);\n\t\t\tif (!global_rsv->full) {\n\t\t\t\tu64 to_add = min(len, global_rsv->size -\n\t\t\t\t\t\t      global_rsv->reserved);\n\n\t\t\t\tglobal_rsv->reserved += to_add;\n\t\t\t\tbtrfs_space_info_update_bytes_may_use(fs_info,\n\t\t\t\t\t\tspace_info, to_add);\n\t\t\t\tif (global_rsv->reserved >= global_rsv->size)\n\t\t\t\t\tglobal_rsv->full = 1;\n\t\t\t\tlen -= to_add;\n\t\t\t}\n\t\t\tspin_unlock(&global_rsv->lock);\n\t\t}\n\t\t \n\t\tif (!readonly && return_free_space && len)\n\t\t\tbtrfs_try_granting_tickets(fs_info, space_info);\n\t\tspin_unlock(&space_info->lock);\n\t}\n\n\tif (cache)\n\t\tbtrfs_put_block_group(cache);\n\treturn 0;\n}\n\nint btrfs_finish_extent_commit(struct btrfs_trans_handle *trans)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_block_group *block_group, *tmp;\n\tstruct list_head *deleted_bgs;\n\tstruct extent_io_tree *unpin;\n\tu64 start;\n\tu64 end;\n\tint ret;\n\n\tunpin = &trans->transaction->pinned_extents;\n\n\twhile (!TRANS_ABORTED(trans)) {\n\t\tstruct extent_state *cached_state = NULL;\n\n\t\tmutex_lock(&fs_info->unused_bg_unpin_mutex);\n\t\tif (!find_first_extent_bit(unpin, 0, &start, &end,\n\t\t\t\t\t   EXTENT_DIRTY, &cached_state)) {\n\t\t\tmutex_unlock(&fs_info->unused_bg_unpin_mutex);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (btrfs_test_opt(fs_info, DISCARD_SYNC))\n\t\t\tret = btrfs_discard_extent(fs_info, start,\n\t\t\t\t\t\t   end + 1 - start, NULL);\n\n\t\tclear_extent_dirty(unpin, start, end, &cached_state);\n\t\tunpin_extent_range(fs_info, start, end, true);\n\t\tmutex_unlock(&fs_info->unused_bg_unpin_mutex);\n\t\tfree_extent_state(cached_state);\n\t\tcond_resched();\n\t}\n\n\tif (btrfs_test_opt(fs_info, DISCARD_ASYNC)) {\n\t\tbtrfs_discard_calc_delay(&fs_info->discard_ctl);\n\t\tbtrfs_discard_schedule_work(&fs_info->discard_ctl, true);\n\t}\n\n\t \n\tdeleted_bgs = &trans->transaction->deleted_bgs;\n\tlist_for_each_entry_safe(block_group, tmp, deleted_bgs, bg_list) {\n\t\tu64 trimmed = 0;\n\n\t\tret = -EROFS;\n\t\tif (!TRANS_ABORTED(trans))\n\t\t\tret = btrfs_discard_extent(fs_info,\n\t\t\t\t\t\t   block_group->start,\n\t\t\t\t\t\t   block_group->length,\n\t\t\t\t\t\t   &trimmed);\n\n\t\tlist_del_init(&block_group->bg_list);\n\t\tbtrfs_unfreeze_block_group(block_group);\n\t\tbtrfs_put_block_group(block_group);\n\n\t\tif (ret) {\n\t\t\tconst char *errstr = btrfs_decode_error(ret);\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t   \"discard failed while removing blockgroup: errno=%d %s\",\n\t\t\t\t   ret, errstr);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int do_free_extent_accounting(struct btrfs_trans_handle *trans,\n\t\t\t\t     u64 bytenr, u64 num_bytes, bool is_data)\n{\n\tint ret;\n\n\tif (is_data) {\n\t\tstruct btrfs_root *csum_root;\n\n\t\tcsum_root = btrfs_csum_root(trans->fs_info, bytenr);\n\t\tret = btrfs_del_csums(trans, csum_root, bytenr, num_bytes);\n\t\tif (ret) {\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tret = add_to_free_space_tree(trans, bytenr, num_bytes);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\treturn ret;\n\t}\n\n\tret = btrfs_update_block_group(trans, bytenr, num_bytes, false);\n\tif (ret)\n\t\tbtrfs_abort_transaction(trans, ret);\n\n\treturn ret;\n}\n\n#define abort_and_dump(trans, path, fmt, args...)\t\\\n({\t\t\t\t\t\t\t\\\n\tbtrfs_abort_transaction(trans, -EUCLEAN);\t\\\n\tbtrfs_print_leaf(path->nodes[0]);\t\t\\\n\tbtrfs_crit(trans->fs_info, fmt, ##args);\t\\\n})\n\n \nstatic int __btrfs_free_extent(struct btrfs_trans_handle *trans,\n\t\t\t       struct btrfs_delayed_ref_node *node, u64 parent,\n\t\t\t       u64 root_objectid, u64 owner_objectid,\n\t\t\t       u64 owner_offset, int refs_to_drop,\n\t\t\t       struct btrfs_delayed_extent_op *extent_op)\n{\n\tstruct btrfs_fs_info *info = trans->fs_info;\n\tstruct btrfs_key key;\n\tstruct btrfs_path *path;\n\tstruct btrfs_root *extent_root;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_extent_item *ei;\n\tstruct btrfs_extent_inline_ref *iref;\n\tint ret;\n\tint is_data;\n\tint extent_slot = 0;\n\tint found_extent = 0;\n\tint num_to_del = 1;\n\tu32 item_size;\n\tu64 refs;\n\tu64 bytenr = node->bytenr;\n\tu64 num_bytes = node->num_bytes;\n\tbool skinny_metadata = btrfs_fs_incompat(info, SKINNY_METADATA);\n\n\textent_root = btrfs_extent_root(info, bytenr);\n\tASSERT(extent_root);\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tis_data = owner_objectid >= BTRFS_FIRST_FREE_OBJECTID;\n\n\tif (!is_data && refs_to_drop != 1) {\n\t\tbtrfs_crit(info,\n\"invalid refs_to_drop, dropping more than 1 refs for tree block %llu refs_to_drop %u\",\n\t\t\t   node->bytenr, refs_to_drop);\n\t\tret = -EINVAL;\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tgoto out;\n\t}\n\n\tif (is_data)\n\t\tskinny_metadata = false;\n\n\tret = lookup_extent_backref(trans, path, &iref, bytenr, num_bytes,\n\t\t\t\t    parent, root_objectid, owner_objectid,\n\t\t\t\t    owner_offset);\n\tif (ret == 0) {\n\t\t \n\t\textent_slot = path->slots[0];\n\t\twhile (extent_slot >= 0) {\n\t\t\tbtrfs_item_key_to_cpu(path->nodes[0], &key,\n\t\t\t\t\t      extent_slot);\n\t\t\tif (key.objectid != bytenr)\n\t\t\t\tbreak;\n\t\t\tif (key.type == BTRFS_EXTENT_ITEM_KEY &&\n\t\t\t    key.offset == num_bytes) {\n\t\t\t\tfound_extent = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (key.type == BTRFS_METADATA_ITEM_KEY &&\n\t\t\t    key.offset == owner_objectid) {\n\t\t\t\tfound_extent = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (path->slots[0] - extent_slot > 5)\n\t\t\t\tbreak;\n\t\t\textent_slot--;\n\t\t}\n\n\t\tif (!found_extent) {\n\t\t\tif (iref) {\n\t\t\t\tabort_and_dump(trans, path,\n\"invalid iref slot %u, no EXTENT/METADATA_ITEM found but has inline extent ref\",\n\t\t\t\t\t   path->slots[0]);\n\t\t\t\tret = -EUCLEAN;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\t \n\t\t\tret = remove_extent_backref(trans, extent_root, path,\n\t\t\t\t\t\t    NULL, refs_to_drop, is_data);\n\t\t\tif (ret) {\n\t\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tbtrfs_release_path(path);\n\n\t\t\t \n\t\t\tkey.objectid = bytenr;\n\t\t\tkey.type = BTRFS_EXTENT_ITEM_KEY;\n\t\t\tkey.offset = num_bytes;\n\n\t\t\tif (!is_data && skinny_metadata) {\n\t\t\t\tkey.type = BTRFS_METADATA_ITEM_KEY;\n\t\t\t\tkey.offset = owner_objectid;\n\t\t\t}\n\n\t\t\tret = btrfs_search_slot(trans, extent_root,\n\t\t\t\t\t\t&key, path, -1, 1);\n\t\t\tif (ret > 0 && skinny_metadata && path->slots[0]) {\n\t\t\t\t \n\t\t\t\tpath->slots[0]--;\n\t\t\t\tbtrfs_item_key_to_cpu(path->nodes[0], &key,\n\t\t\t\t\t\t      path->slots[0]);\n\t\t\t\tif (key.objectid == bytenr &&\n\t\t\t\t    key.type == BTRFS_EXTENT_ITEM_KEY &&\n\t\t\t\t    key.offset == num_bytes)\n\t\t\t\t\tret = 0;\n\t\t\t}\n\n\t\t\tif (ret > 0 && skinny_metadata) {\n\t\t\t\tskinny_metadata = false;\n\t\t\t\tkey.objectid = bytenr;\n\t\t\t\tkey.type = BTRFS_EXTENT_ITEM_KEY;\n\t\t\t\tkey.offset = num_bytes;\n\t\t\t\tbtrfs_release_path(path);\n\t\t\t\tret = btrfs_search_slot(trans, extent_root,\n\t\t\t\t\t\t\t&key, path, -1, 1);\n\t\t\t}\n\n\t\t\tif (ret) {\n\t\t\t\tif (ret > 0)\n\t\t\t\t\tbtrfs_print_leaf(path->nodes[0]);\n\t\t\t\tbtrfs_err(info,\n\t\t\t\"umm, got %d back from search, was looking for %llu, slot %d\",\n\t\t\t\t\t  ret, bytenr, path->slots[0]);\n\t\t\t}\n\t\t\tif (ret < 0) {\n\t\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\textent_slot = path->slots[0];\n\t\t}\n\t} else if (WARN_ON(ret == -ENOENT)) {\n\t\tabort_and_dump(trans, path,\n\"unable to find ref byte nr %llu parent %llu root %llu owner %llu offset %llu slot %d\",\n\t\t\t       bytenr, parent, root_objectid, owner_objectid,\n\t\t\t       owner_offset, path->slots[0]);\n\t\tgoto out;\n\t} else {\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tgoto out;\n\t}\n\n\tleaf = path->nodes[0];\n\titem_size = btrfs_item_size(leaf, extent_slot);\n\tif (unlikely(item_size < sizeof(*ei))) {\n\t\tret = -EUCLEAN;\n\t\tbtrfs_err(trans->fs_info,\n\t\t\t  \"unexpected extent item size, has %u expect >= %zu\",\n\t\t\t  item_size, sizeof(*ei));\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tgoto out;\n\t}\n\tei = btrfs_item_ptr(leaf, extent_slot,\n\t\t\t    struct btrfs_extent_item);\n\tif (owner_objectid < BTRFS_FIRST_FREE_OBJECTID &&\n\t    key.type == BTRFS_EXTENT_ITEM_KEY) {\n\t\tstruct btrfs_tree_block_info *bi;\n\n\t\tif (item_size < sizeof(*ei) + sizeof(*bi)) {\n\t\t\tabort_and_dump(trans, path,\n\"invalid extent item size for key (%llu, %u, %llu) slot %u owner %llu, has %u expect >= %zu\",\n\t\t\t\t       key.objectid, key.type, key.offset,\n\t\t\t\t       path->slots[0], owner_objectid, item_size,\n\t\t\t\t       sizeof(*ei) + sizeof(*bi));\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\t\tbi = (struct btrfs_tree_block_info *)(ei + 1);\n\t\tWARN_ON(owner_objectid != btrfs_tree_block_level(leaf, bi));\n\t}\n\n\trefs = btrfs_extent_refs(leaf, ei);\n\tif (refs < refs_to_drop) {\n\t\tabort_and_dump(trans, path,\n\t\t\"trying to drop %d refs but we only have %llu for bytenr %llu slot %u\",\n\t\t\t       refs_to_drop, refs, bytenr, path->slots[0]);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\trefs -= refs_to_drop;\n\n\tif (refs > 0) {\n\t\tif (extent_op)\n\t\t\t__run_delayed_extent_op(extent_op, leaf, ei);\n\t\t \n\t\tif (iref) {\n\t\t\tif (!found_extent) {\n\t\t\t\tabort_and_dump(trans, path,\n\"invalid iref, got inlined extent ref but no EXTENT/METADATA_ITEM found, slot %u\",\n\t\t\t\t\t       path->slots[0]);\n\t\t\t\tret = -EUCLEAN;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t} else {\n\t\t\tbtrfs_set_extent_refs(leaf, ei, refs);\n\t\t\tbtrfs_mark_buffer_dirty(trans, leaf);\n\t\t}\n\t\tif (found_extent) {\n\t\t\tret = remove_extent_backref(trans, extent_root, path,\n\t\t\t\t\t\t    iref, refs_to_drop, is_data);\n\t\t\tif (ret) {\n\t\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t} else {\n\t\t \n\t\tif (found_extent) {\n\t\t\tif (is_data && refs_to_drop !=\n\t\t\t    extent_data_ref_count(path, iref)) {\n\t\t\t\tabort_and_dump(trans, path,\n\t\t\"invalid refs_to_drop, current refs %u refs_to_drop %u slot %u\",\n\t\t\t\t\t       extent_data_ref_count(path, iref),\n\t\t\t\t\t       refs_to_drop, path->slots[0]);\n\t\t\t\tret = -EUCLEAN;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (iref) {\n\t\t\t\tif (path->slots[0] != extent_slot) {\n\t\t\t\t\tabort_and_dump(trans, path,\n\"invalid iref, extent item key (%llu %u %llu) slot %u doesn't have wanted iref\",\n\t\t\t\t\t\t       key.objectid, key.type,\n\t\t\t\t\t\t       key.offset, path->slots[0]);\n\t\t\t\t\tret = -EUCLEAN;\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tif (path->slots[0] != extent_slot + 1) {\n\t\t\t\t\tabort_and_dump(trans, path,\n\t\"invalid SHARED_* item slot %u, previous item is not EXTENT/METADATA_ITEM\",\n\t\t\t\t\t\t       path->slots[0]);\n\t\t\t\t\tret = -EUCLEAN;\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t\tpath->slots[0] = extent_slot;\n\t\t\t\tnum_to_del = 2;\n\t\t\t}\n\t\t}\n\n\t\tret = btrfs_del_items(trans, extent_root, path, path->slots[0],\n\t\t\t\t      num_to_del);\n\t\tif (ret) {\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tgoto out;\n\t\t}\n\t\tbtrfs_release_path(path);\n\n\t\tret = do_free_extent_accounting(trans, bytenr, num_bytes, is_data);\n\t}\n\tbtrfs_release_path(path);\n\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n \nstatic noinline int check_ref_cleanup(struct btrfs_trans_handle *trans,\n\t\t\t\t      u64 bytenr)\n{\n\tstruct btrfs_delayed_ref_head *head;\n\tstruct btrfs_delayed_ref_root *delayed_refs;\n\tint ret = 0;\n\n\tdelayed_refs = &trans->transaction->delayed_refs;\n\tspin_lock(&delayed_refs->lock);\n\thead = btrfs_find_delayed_ref_head(delayed_refs, bytenr);\n\tif (!head)\n\t\tgoto out_delayed_unlock;\n\n\tspin_lock(&head->lock);\n\tif (!RB_EMPTY_ROOT(&head->ref_tree.rb_root))\n\t\tgoto out;\n\n\tif (cleanup_extent_op(head) != NULL)\n\t\tgoto out;\n\n\t \n\tif (!mutex_trylock(&head->mutex))\n\t\tgoto out;\n\n\tbtrfs_delete_ref_head(delayed_refs, head);\n\thead->processing = false;\n\n\tspin_unlock(&head->lock);\n\tspin_unlock(&delayed_refs->lock);\n\n\tBUG_ON(head->extent_op);\n\tif (head->must_insert_reserved)\n\t\tret = 1;\n\n\tbtrfs_cleanup_ref_head_accounting(trans->fs_info, delayed_refs, head);\n\tmutex_unlock(&head->mutex);\n\tbtrfs_put_delayed_ref_head(head);\n\treturn ret;\nout:\n\tspin_unlock(&head->lock);\n\nout_delayed_unlock:\n\tspin_unlock(&delayed_refs->lock);\n\treturn 0;\n}\n\nvoid btrfs_free_tree_block(struct btrfs_trans_handle *trans,\n\t\t\t   u64 root_id,\n\t\t\t   struct extent_buffer *buf,\n\t\t\t   u64 parent, int last_ref)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_ref generic_ref = { 0 };\n\tint ret;\n\n\tbtrfs_init_generic_ref(&generic_ref, BTRFS_DROP_DELAYED_REF,\n\t\t\t       buf->start, buf->len, parent);\n\tbtrfs_init_tree_ref(&generic_ref, btrfs_header_level(buf),\n\t\t\t    root_id, 0, false);\n\n\tif (root_id != BTRFS_TREE_LOG_OBJECTID) {\n\t\tbtrfs_ref_tree_mod(fs_info, &generic_ref);\n\t\tret = btrfs_add_delayed_tree_ref(trans, &generic_ref, NULL);\n\t\tBUG_ON(ret);  \n\t}\n\n\tif (last_ref && btrfs_header_generation(buf) == trans->transid) {\n\t\tstruct btrfs_block_group *cache;\n\t\tbool must_pin = false;\n\n\t\tif (root_id != BTRFS_TREE_LOG_OBJECTID) {\n\t\t\tret = check_ref_cleanup(trans, buf->start);\n\t\t\tif (!ret) {\n\t\t\t\tbtrfs_redirty_list_add(trans->transaction, buf);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\tcache = btrfs_lookup_block_group(fs_info, buf->start);\n\n\t\tif (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN)) {\n\t\t\tpin_down_extent(trans, cache, buf->start, buf->len, 1);\n\t\t\tbtrfs_put_block_group(cache);\n\t\t\tgoto out;\n\t\t}\n\n\t\t \n\t\tif (test_bit(BTRFS_FS_TREE_MOD_LOG_USERS, &fs_info->flags))\n\t\t\tmust_pin = true;\n\n\t\tif (must_pin || btrfs_is_zoned(fs_info)) {\n\t\t\tbtrfs_redirty_list_add(trans->transaction, buf);\n\t\t\tpin_down_extent(trans, cache, buf->start, buf->len, 1);\n\t\t\tbtrfs_put_block_group(cache);\n\t\t\tgoto out;\n\t\t}\n\n\t\tWARN_ON(test_bit(EXTENT_BUFFER_DIRTY, &buf->bflags));\n\n\t\tbtrfs_add_free_space(cache, buf->start, buf->len);\n\t\tbtrfs_free_reserved_bytes(cache, buf->len, 0);\n\t\tbtrfs_put_block_group(cache);\n\t\ttrace_btrfs_reserved_extent_free(fs_info, buf->start, buf->len);\n\t}\nout:\n\tif (last_ref) {\n\t\t \n\t\tclear_bit(EXTENT_BUFFER_CORRUPT, &buf->bflags);\n\t}\n}\n\n \nint btrfs_free_extent(struct btrfs_trans_handle *trans, struct btrfs_ref *ref)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tint ret;\n\n\tif (btrfs_is_testing(fs_info))\n\t\treturn 0;\n\n\t \n\tif ((ref->type == BTRFS_REF_METADATA &&\n\t     ref->tree_ref.owning_root == BTRFS_TREE_LOG_OBJECTID) ||\n\t    (ref->type == BTRFS_REF_DATA &&\n\t     ref->data_ref.owning_root == BTRFS_TREE_LOG_OBJECTID)) {\n\t\t \n\t\tbtrfs_pin_extent(trans, ref->bytenr, ref->len, 1);\n\t\tret = 0;\n\t} else if (ref->type == BTRFS_REF_METADATA) {\n\t\tret = btrfs_add_delayed_tree_ref(trans, ref, NULL);\n\t} else {\n\t\tret = btrfs_add_delayed_data_ref(trans, ref, 0);\n\t}\n\n\tif (!((ref->type == BTRFS_REF_METADATA &&\n\t       ref->tree_ref.owning_root == BTRFS_TREE_LOG_OBJECTID) ||\n\t      (ref->type == BTRFS_REF_DATA &&\n\t       ref->data_ref.owning_root == BTRFS_TREE_LOG_OBJECTID)))\n\t\tbtrfs_ref_tree_mod(fs_info, ref);\n\n\treturn ret;\n}\n\nenum btrfs_loop_type {\n\t \n\tLOOP_CACHING_NOWAIT,\n\n\t \n\tLOOP_CACHING_WAIT,\n\n\t \n\tLOOP_UNSET_SIZE_CLASS,\n\n\t \n\tLOOP_ALLOC_CHUNK,\n\n\t \n\tLOOP_WRONG_SIZE_CLASS,\n\n\t \n\tLOOP_NO_EMPTY_SIZE,\n};\n\nstatic inline void\nbtrfs_lock_block_group(struct btrfs_block_group *cache,\n\t\t       int delalloc)\n{\n\tif (delalloc)\n\t\tdown_read(&cache->data_rwsem);\n}\n\nstatic inline void btrfs_grab_block_group(struct btrfs_block_group *cache,\n\t\t       int delalloc)\n{\n\tbtrfs_get_block_group(cache);\n\tif (delalloc)\n\t\tdown_read(&cache->data_rwsem);\n}\n\nstatic struct btrfs_block_group *btrfs_lock_cluster(\n\t\t   struct btrfs_block_group *block_group,\n\t\t   struct btrfs_free_cluster *cluster,\n\t\t   int delalloc)\n\t__acquires(&cluster->refill_lock)\n{\n\tstruct btrfs_block_group *used_bg = NULL;\n\n\tspin_lock(&cluster->refill_lock);\n\twhile (1) {\n\t\tused_bg = cluster->block_group;\n\t\tif (!used_bg)\n\t\t\treturn NULL;\n\n\t\tif (used_bg == block_group)\n\t\t\treturn used_bg;\n\n\t\tbtrfs_get_block_group(used_bg);\n\n\t\tif (!delalloc)\n\t\t\treturn used_bg;\n\n\t\tif (down_read_trylock(&used_bg->data_rwsem))\n\t\t\treturn used_bg;\n\n\t\tspin_unlock(&cluster->refill_lock);\n\n\t\t \n\t\tdown_read_nested(&used_bg->data_rwsem, SINGLE_DEPTH_NESTING);\n\n\t\tspin_lock(&cluster->refill_lock);\n\t\tif (used_bg == cluster->block_group)\n\t\t\treturn used_bg;\n\n\t\tup_read(&used_bg->data_rwsem);\n\t\tbtrfs_put_block_group(used_bg);\n\t}\n}\n\nstatic inline void\nbtrfs_release_block_group(struct btrfs_block_group *cache,\n\t\t\t int delalloc)\n{\n\tif (delalloc)\n\t\tup_read(&cache->data_rwsem);\n\tbtrfs_put_block_group(cache);\n}\n\n \nstatic int find_free_extent_clustered(struct btrfs_block_group *bg,\n\t\t\t\t      struct find_free_extent_ctl *ffe_ctl,\n\t\t\t\t      struct btrfs_block_group **cluster_bg_ret)\n{\n\tstruct btrfs_block_group *cluster_bg;\n\tstruct btrfs_free_cluster *last_ptr = ffe_ctl->last_ptr;\n\tu64 aligned_cluster;\n\tu64 offset;\n\tint ret;\n\n\tcluster_bg = btrfs_lock_cluster(bg, last_ptr, ffe_ctl->delalloc);\n\tif (!cluster_bg)\n\t\tgoto refill_cluster;\n\tif (cluster_bg != bg && (cluster_bg->ro ||\n\t    !block_group_bits(cluster_bg, ffe_ctl->flags)))\n\t\tgoto release_cluster;\n\n\toffset = btrfs_alloc_from_cluster(cluster_bg, last_ptr,\n\t\t\tffe_ctl->num_bytes, cluster_bg->start,\n\t\t\t&ffe_ctl->max_extent_size);\n\tif (offset) {\n\t\t \n\t\tspin_unlock(&last_ptr->refill_lock);\n\t\ttrace_btrfs_reserve_extent_cluster(cluster_bg, ffe_ctl);\n\t\t*cluster_bg_ret = cluster_bg;\n\t\tffe_ctl->found_offset = offset;\n\t\treturn 0;\n\t}\n\tWARN_ON(last_ptr->block_group != cluster_bg);\n\nrelease_cluster:\n\t \n\tif (ffe_ctl->loop >= LOOP_NO_EMPTY_SIZE && cluster_bg != bg) {\n\t\tspin_unlock(&last_ptr->refill_lock);\n\t\tbtrfs_release_block_group(cluster_bg, ffe_ctl->delalloc);\n\t\treturn -ENOENT;\n\t}\n\n\t \n\tbtrfs_return_cluster_to_free_space(NULL, last_ptr);\n\n\tif (cluster_bg != bg)\n\t\tbtrfs_release_block_group(cluster_bg, ffe_ctl->delalloc);\n\nrefill_cluster:\n\tif (ffe_ctl->loop >= LOOP_NO_EMPTY_SIZE) {\n\t\tspin_unlock(&last_ptr->refill_lock);\n\t\treturn -ENOENT;\n\t}\n\n\taligned_cluster = max_t(u64,\n\t\t\tffe_ctl->empty_cluster + ffe_ctl->empty_size,\n\t\t\tbg->full_stripe_len);\n\tret = btrfs_find_space_cluster(bg, last_ptr, ffe_ctl->search_start,\n\t\t\tffe_ctl->num_bytes, aligned_cluster);\n\tif (ret == 0) {\n\t\t \n\t\toffset = btrfs_alloc_from_cluster(bg, last_ptr,\n\t\t\t\tffe_ctl->num_bytes, ffe_ctl->search_start,\n\t\t\t\t&ffe_ctl->max_extent_size);\n\t\tif (offset) {\n\t\t\t \n\t\t\tspin_unlock(&last_ptr->refill_lock);\n\t\t\tffe_ctl->found_offset = offset;\n\t\t\ttrace_btrfs_reserve_extent_cluster(bg, ffe_ctl);\n\t\t\treturn 0;\n\t\t}\n\t}\n\t \n\tbtrfs_return_cluster_to_free_space(NULL, last_ptr);\n\tspin_unlock(&last_ptr->refill_lock);\n\treturn 1;\n}\n\n \nstatic int find_free_extent_unclustered(struct btrfs_block_group *bg,\n\t\t\t\t\tstruct find_free_extent_ctl *ffe_ctl)\n{\n\tstruct btrfs_free_cluster *last_ptr = ffe_ctl->last_ptr;\n\tu64 offset;\n\n\t \n\tif (unlikely(last_ptr)) {\n\t\tspin_lock(&last_ptr->lock);\n\t\tlast_ptr->fragmented = 1;\n\t\tspin_unlock(&last_ptr->lock);\n\t}\n\tif (ffe_ctl->cached) {\n\t\tstruct btrfs_free_space_ctl *free_space_ctl;\n\n\t\tfree_space_ctl = bg->free_space_ctl;\n\t\tspin_lock(&free_space_ctl->tree_lock);\n\t\tif (free_space_ctl->free_space <\n\t\t    ffe_ctl->num_bytes + ffe_ctl->empty_cluster +\n\t\t    ffe_ctl->empty_size) {\n\t\t\tffe_ctl->total_free_space = max_t(u64,\n\t\t\t\t\tffe_ctl->total_free_space,\n\t\t\t\t\tfree_space_ctl->free_space);\n\t\t\tspin_unlock(&free_space_ctl->tree_lock);\n\t\t\treturn 1;\n\t\t}\n\t\tspin_unlock(&free_space_ctl->tree_lock);\n\t}\n\n\toffset = btrfs_find_space_for_alloc(bg, ffe_ctl->search_start,\n\t\t\tffe_ctl->num_bytes, ffe_ctl->empty_size,\n\t\t\t&ffe_ctl->max_extent_size);\n\tif (!offset)\n\t\treturn 1;\n\tffe_ctl->found_offset = offset;\n\treturn 0;\n}\n\nstatic int do_allocation_clustered(struct btrfs_block_group *block_group,\n\t\t\t\t   struct find_free_extent_ctl *ffe_ctl,\n\t\t\t\t   struct btrfs_block_group **bg_ret)\n{\n\tint ret;\n\n\t \n\tif (ffe_ctl->last_ptr && ffe_ctl->use_cluster) {\n\t\tret = find_free_extent_clustered(block_group, ffe_ctl, bg_ret);\n\t\tif (ret >= 0)\n\t\t\treturn ret;\n\t\t \n\t}\n\n\treturn find_free_extent_unclustered(block_group, ffe_ctl);\n}\n\n \n\n \nstatic int do_allocation_zoned(struct btrfs_block_group *block_group,\n\t\t\t       struct find_free_extent_ctl *ffe_ctl,\n\t\t\t       struct btrfs_block_group **bg_ret)\n{\n\tstruct btrfs_fs_info *fs_info = block_group->fs_info;\n\tstruct btrfs_space_info *space_info = block_group->space_info;\n\tstruct btrfs_free_space_ctl *ctl = block_group->free_space_ctl;\n\tu64 start = block_group->start;\n\tu64 num_bytes = ffe_ctl->num_bytes;\n\tu64 avail;\n\tu64 bytenr = block_group->start;\n\tu64 log_bytenr;\n\tu64 data_reloc_bytenr;\n\tint ret = 0;\n\tbool skip = false;\n\n\tASSERT(btrfs_is_zoned(block_group->fs_info));\n\n\t \n\tspin_lock(&fs_info->treelog_bg_lock);\n\tlog_bytenr = fs_info->treelog_bg;\n\tif (log_bytenr && ((ffe_ctl->for_treelog && bytenr != log_bytenr) ||\n\t\t\t   (!ffe_ctl->for_treelog && bytenr == log_bytenr)))\n\t\tskip = true;\n\tspin_unlock(&fs_info->treelog_bg_lock);\n\tif (skip)\n\t\treturn 1;\n\n\t \n\tspin_lock(&fs_info->relocation_bg_lock);\n\tdata_reloc_bytenr = fs_info->data_reloc_bg;\n\tif (data_reloc_bytenr &&\n\t    ((ffe_ctl->for_data_reloc && bytenr != data_reloc_bytenr) ||\n\t     (!ffe_ctl->for_data_reloc && bytenr == data_reloc_bytenr)))\n\t\tskip = true;\n\tspin_unlock(&fs_info->relocation_bg_lock);\n\tif (skip)\n\t\treturn 1;\n\n\t \n\tspin_lock(&block_group->lock);\n\tif (block_group->ro || btrfs_zoned_bg_is_full(block_group)) {\n\t\tret = 1;\n\t\t \n\t}\n\tspin_unlock(&block_group->lock);\n\n\t \n\tif (!ret && (block_group->flags & BTRFS_BLOCK_GROUP_DATA) &&\n\t    !btrfs_zone_activate(block_group)) {\n\t\tret = 1;\n\t\t \n\t}\n\n\tspin_lock(&space_info->lock);\n\tspin_lock(&block_group->lock);\n\tspin_lock(&fs_info->treelog_bg_lock);\n\tspin_lock(&fs_info->relocation_bg_lock);\n\n\tif (ret)\n\t\tgoto out;\n\n\tASSERT(!ffe_ctl->for_treelog ||\n\t       block_group->start == fs_info->treelog_bg ||\n\t       fs_info->treelog_bg == 0);\n\tASSERT(!ffe_ctl->for_data_reloc ||\n\t       block_group->start == fs_info->data_reloc_bg ||\n\t       fs_info->data_reloc_bg == 0);\n\n\tif (block_group->ro ||\n\t    (!ffe_ctl->for_data_reloc &&\n\t     test_bit(BLOCK_GROUP_FLAG_ZONED_DATA_RELOC, &block_group->runtime_flags))) {\n\t\tret = 1;\n\t\tgoto out;\n\t}\n\n\t \n\tif (ffe_ctl->for_treelog && !fs_info->treelog_bg &&\n\t    (block_group->used || block_group->reserved)) {\n\t\tret = 1;\n\t\tgoto out;\n\t}\n\n\t \n\tif (ffe_ctl->for_data_reloc && !fs_info->data_reloc_bg &&\n\t    (block_group->used || block_group->reserved)) {\n\t\tret = 1;\n\t\tgoto out;\n\t}\n\n\tWARN_ON_ONCE(block_group->alloc_offset > block_group->zone_capacity);\n\tavail = block_group->zone_capacity - block_group->alloc_offset;\n\tif (avail < num_bytes) {\n\t\tif (ffe_ctl->max_extent_size < avail) {\n\t\t\t \n\t\t\tffe_ctl->max_extent_size = avail;\n\t\t\tffe_ctl->total_free_space = avail;\n\t\t}\n\t\tret = 1;\n\t\tgoto out;\n\t}\n\n\tif (ffe_ctl->for_treelog && !fs_info->treelog_bg)\n\t\tfs_info->treelog_bg = block_group->start;\n\n\tif (ffe_ctl->for_data_reloc) {\n\t\tif (!fs_info->data_reloc_bg)\n\t\t\tfs_info->data_reloc_bg = block_group->start;\n\t\t \n\t\tset_bit(BLOCK_GROUP_FLAG_ZONED_DATA_RELOC, &block_group->runtime_flags);\n\t}\n\n\tffe_ctl->found_offset = start + block_group->alloc_offset;\n\tblock_group->alloc_offset += num_bytes;\n\tspin_lock(&ctl->tree_lock);\n\tctl->free_space -= num_bytes;\n\tspin_unlock(&ctl->tree_lock);\n\n\t \n\n\tffe_ctl->search_start = ffe_ctl->found_offset;\n\nout:\n\tif (ret && ffe_ctl->for_treelog)\n\t\tfs_info->treelog_bg = 0;\n\tif (ret && ffe_ctl->for_data_reloc)\n\t\tfs_info->data_reloc_bg = 0;\n\tspin_unlock(&fs_info->relocation_bg_lock);\n\tspin_unlock(&fs_info->treelog_bg_lock);\n\tspin_unlock(&block_group->lock);\n\tspin_unlock(&space_info->lock);\n\treturn ret;\n}\n\nstatic int do_allocation(struct btrfs_block_group *block_group,\n\t\t\t struct find_free_extent_ctl *ffe_ctl,\n\t\t\t struct btrfs_block_group **bg_ret)\n{\n\tswitch (ffe_ctl->policy) {\n\tcase BTRFS_EXTENT_ALLOC_CLUSTERED:\n\t\treturn do_allocation_clustered(block_group, ffe_ctl, bg_ret);\n\tcase BTRFS_EXTENT_ALLOC_ZONED:\n\t\treturn do_allocation_zoned(block_group, ffe_ctl, bg_ret);\n\tdefault:\n\t\tBUG();\n\t}\n}\n\nstatic void release_block_group(struct btrfs_block_group *block_group,\n\t\t\t\tstruct find_free_extent_ctl *ffe_ctl,\n\t\t\t\tint delalloc)\n{\n\tswitch (ffe_ctl->policy) {\n\tcase BTRFS_EXTENT_ALLOC_CLUSTERED:\n\t\tffe_ctl->retry_uncached = false;\n\t\tbreak;\n\tcase BTRFS_EXTENT_ALLOC_ZONED:\n\t\t \n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\tBUG_ON(btrfs_bg_flags_to_raid_index(block_group->flags) !=\n\t       ffe_ctl->index);\n\tbtrfs_release_block_group(block_group, delalloc);\n}\n\nstatic void found_extent_clustered(struct find_free_extent_ctl *ffe_ctl,\n\t\t\t\t   struct btrfs_key *ins)\n{\n\tstruct btrfs_free_cluster *last_ptr = ffe_ctl->last_ptr;\n\n\tif (!ffe_ctl->use_cluster && last_ptr) {\n\t\tspin_lock(&last_ptr->lock);\n\t\tlast_ptr->window_start = ins->objectid;\n\t\tspin_unlock(&last_ptr->lock);\n\t}\n}\n\nstatic void found_extent(struct find_free_extent_ctl *ffe_ctl,\n\t\t\t struct btrfs_key *ins)\n{\n\tswitch (ffe_ctl->policy) {\n\tcase BTRFS_EXTENT_ALLOC_CLUSTERED:\n\t\tfound_extent_clustered(ffe_ctl, ins);\n\t\tbreak;\n\tcase BTRFS_EXTENT_ALLOC_ZONED:\n\t\t \n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n}\n\nstatic int can_allocate_chunk_zoned(struct btrfs_fs_info *fs_info,\n\t\t\t\t    struct find_free_extent_ctl *ffe_ctl)\n{\n\t \n\tif (!(ffe_ctl->flags & BTRFS_BLOCK_GROUP_DATA))\n\t\treturn 0;\n\n\t \n\tif (btrfs_can_activate_zone(fs_info->fs_devices, ffe_ctl->flags))\n\t\treturn 0;\n\n\t \n\tif (ffe_ctl->flags & BTRFS_BLOCK_GROUP_DATA) {\n\t\tint ret = btrfs_zone_finish_one_bg(fs_info);\n\n\t\tif (ret == 1)\n\t\t\treturn 0;\n\t\telse if (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\t \n\tif (ffe_ctl->max_extent_size >= ffe_ctl->min_alloc_size)\n\t\treturn -ENOSPC;\n\n\t \n\tif (ffe_ctl->flags & BTRFS_BLOCK_GROUP_DATA)\n\t\treturn -EAGAIN;\n\n\t \n\treturn 0;\n}\n\nstatic int can_allocate_chunk(struct btrfs_fs_info *fs_info,\n\t\t\t      struct find_free_extent_ctl *ffe_ctl)\n{\n\tswitch (ffe_ctl->policy) {\n\tcase BTRFS_EXTENT_ALLOC_CLUSTERED:\n\t\treturn 0;\n\tcase BTRFS_EXTENT_ALLOC_ZONED:\n\t\treturn can_allocate_chunk_zoned(fs_info, ffe_ctl);\n\tdefault:\n\t\tBUG();\n\t}\n}\n\n \nstatic int find_free_extent_update_loop(struct btrfs_fs_info *fs_info,\n\t\t\t\t\tstruct btrfs_key *ins,\n\t\t\t\t\tstruct find_free_extent_ctl *ffe_ctl,\n\t\t\t\t\tbool full_search)\n{\n\tstruct btrfs_root *root = fs_info->chunk_root;\n\tint ret;\n\n\tif ((ffe_ctl->loop == LOOP_CACHING_NOWAIT) &&\n\t    ffe_ctl->have_caching_bg && !ffe_ctl->orig_have_caching_bg)\n\t\tffe_ctl->orig_have_caching_bg = true;\n\n\tif (ins->objectid) {\n\t\tfound_extent(ffe_ctl, ins);\n\t\treturn 0;\n\t}\n\n\tif (ffe_ctl->loop >= LOOP_CACHING_WAIT && ffe_ctl->have_caching_bg)\n\t\treturn 1;\n\n\tffe_ctl->index++;\n\tif (ffe_ctl->index < BTRFS_NR_RAID_TYPES)\n\t\treturn 1;\n\n\t \n\tif (ffe_ctl->loop < LOOP_NO_EMPTY_SIZE) {\n\t\tffe_ctl->index = 0;\n\t\t \n\t\tif (ffe_ctl->loop == LOOP_CACHING_NOWAIT &&\n\t\t    (!ffe_ctl->orig_have_caching_bg && full_search))\n\t\t\tffe_ctl->loop++;\n\t\tffe_ctl->loop++;\n\n\t\tif (ffe_ctl->loop == LOOP_ALLOC_CHUNK) {\n\t\t\tstruct btrfs_trans_handle *trans;\n\t\t\tint exist = 0;\n\n\t\t\t \n\t\t\tret = can_allocate_chunk(fs_info, ffe_ctl);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\n\t\t\ttrans = current->journal_info;\n\t\t\tif (trans)\n\t\t\t\texist = 1;\n\t\t\telse\n\t\t\t\ttrans = btrfs_join_transaction(root);\n\n\t\t\tif (IS_ERR(trans)) {\n\t\t\t\tret = PTR_ERR(trans);\n\t\t\t\treturn ret;\n\t\t\t}\n\n\t\t\tret = btrfs_chunk_alloc(trans, ffe_ctl->flags,\n\t\t\t\t\t\tCHUNK_ALLOC_FORCE_FOR_EXTENT);\n\n\t\t\t \n\t\t\tif (ret == -ENOSPC) {\n\t\t\t\tret = 0;\n\t\t\t\tffe_ctl->loop++;\n\t\t\t}\n\t\t\telse if (ret < 0)\n\t\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\telse\n\t\t\t\tret = 0;\n\t\t\tif (!exist)\n\t\t\t\tbtrfs_end_transaction(trans);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tif (ffe_ctl->loop == LOOP_NO_EMPTY_SIZE) {\n\t\t\tif (ffe_ctl->policy != BTRFS_EXTENT_ALLOC_CLUSTERED)\n\t\t\t\treturn -ENOSPC;\n\n\t\t\t \n\t\t\tif (ffe_ctl->empty_size == 0 &&\n\t\t\t    ffe_ctl->empty_cluster == 0)\n\t\t\t\treturn -ENOSPC;\n\t\t\tffe_ctl->empty_size = 0;\n\t\t\tffe_ctl->empty_cluster = 0;\n\t\t}\n\t\treturn 1;\n\t}\n\treturn -ENOSPC;\n}\n\nstatic bool find_free_extent_check_size_class(struct find_free_extent_ctl *ffe_ctl,\n\t\t\t\t\t      struct btrfs_block_group *bg)\n{\n\tif (ffe_ctl->policy == BTRFS_EXTENT_ALLOC_ZONED)\n\t\treturn true;\n\tif (!btrfs_block_group_should_use_size_class(bg))\n\t\treturn true;\n\tif (ffe_ctl->loop >= LOOP_WRONG_SIZE_CLASS)\n\t\treturn true;\n\tif (ffe_ctl->loop >= LOOP_UNSET_SIZE_CLASS &&\n\t    bg->size_class == BTRFS_BG_SZ_NONE)\n\t\treturn true;\n\treturn ffe_ctl->size_class == bg->size_class;\n}\n\nstatic int prepare_allocation_clustered(struct btrfs_fs_info *fs_info,\n\t\t\t\t\tstruct find_free_extent_ctl *ffe_ctl,\n\t\t\t\t\tstruct btrfs_space_info *space_info,\n\t\t\t\t\tstruct btrfs_key *ins)\n{\n\t \n\tif (space_info->max_extent_size) {\n\t\tspin_lock(&space_info->lock);\n\t\tif (space_info->max_extent_size &&\n\t\t    ffe_ctl->num_bytes > space_info->max_extent_size) {\n\t\t\tins->offset = space_info->max_extent_size;\n\t\t\tspin_unlock(&space_info->lock);\n\t\t\treturn -ENOSPC;\n\t\t} else if (space_info->max_extent_size) {\n\t\t\tffe_ctl->use_cluster = false;\n\t\t}\n\t\tspin_unlock(&space_info->lock);\n\t}\n\n\tffe_ctl->last_ptr = fetch_cluster_info(fs_info, space_info,\n\t\t\t\t\t       &ffe_ctl->empty_cluster);\n\tif (ffe_ctl->last_ptr) {\n\t\tstruct btrfs_free_cluster *last_ptr = ffe_ctl->last_ptr;\n\n\t\tspin_lock(&last_ptr->lock);\n\t\tif (last_ptr->block_group)\n\t\t\tffe_ctl->hint_byte = last_ptr->window_start;\n\t\tif (last_ptr->fragmented) {\n\t\t\t \n\t\t\tffe_ctl->hint_byte = last_ptr->window_start;\n\t\t\tffe_ctl->use_cluster = false;\n\t\t}\n\t\tspin_unlock(&last_ptr->lock);\n\t}\n\n\treturn 0;\n}\n\nstatic int prepare_allocation(struct btrfs_fs_info *fs_info,\n\t\t\t      struct find_free_extent_ctl *ffe_ctl,\n\t\t\t      struct btrfs_space_info *space_info,\n\t\t\t      struct btrfs_key *ins)\n{\n\tswitch (ffe_ctl->policy) {\n\tcase BTRFS_EXTENT_ALLOC_CLUSTERED:\n\t\treturn prepare_allocation_clustered(fs_info, ffe_ctl,\n\t\t\t\t\t\t    space_info, ins);\n\tcase BTRFS_EXTENT_ALLOC_ZONED:\n\t\tif (ffe_ctl->for_treelog) {\n\t\t\tspin_lock(&fs_info->treelog_bg_lock);\n\t\t\tif (fs_info->treelog_bg)\n\t\t\t\tffe_ctl->hint_byte = fs_info->treelog_bg;\n\t\t\tspin_unlock(&fs_info->treelog_bg_lock);\n\t\t}\n\t\tif (ffe_ctl->for_data_reloc) {\n\t\t\tspin_lock(&fs_info->relocation_bg_lock);\n\t\t\tif (fs_info->data_reloc_bg)\n\t\t\t\tffe_ctl->hint_byte = fs_info->data_reloc_bg;\n\t\t\tspin_unlock(&fs_info->relocation_bg_lock);\n\t\t}\n\t\treturn 0;\n\tdefault:\n\t\tBUG();\n\t}\n}\n\n \nstatic noinline int find_free_extent(struct btrfs_root *root,\n\t\t\t\t     struct btrfs_key *ins,\n\t\t\t\t     struct find_free_extent_ctl *ffe_ctl)\n{\n\tstruct btrfs_fs_info *fs_info = root->fs_info;\n\tint ret = 0;\n\tint cache_block_group_error = 0;\n\tstruct btrfs_block_group *block_group = NULL;\n\tstruct btrfs_space_info *space_info;\n\tbool full_search = false;\n\n\tWARN_ON(ffe_ctl->num_bytes < fs_info->sectorsize);\n\n\tffe_ctl->search_start = 0;\n\t \n\tffe_ctl->empty_cluster = 0;\n\tffe_ctl->last_ptr = NULL;\n\tffe_ctl->use_cluster = true;\n\tffe_ctl->have_caching_bg = false;\n\tffe_ctl->orig_have_caching_bg = false;\n\tffe_ctl->index = btrfs_bg_flags_to_raid_index(ffe_ctl->flags);\n\tffe_ctl->loop = 0;\n\tffe_ctl->retry_uncached = false;\n\tffe_ctl->cached = 0;\n\tffe_ctl->max_extent_size = 0;\n\tffe_ctl->total_free_space = 0;\n\tffe_ctl->found_offset = 0;\n\tffe_ctl->policy = BTRFS_EXTENT_ALLOC_CLUSTERED;\n\tffe_ctl->size_class = btrfs_calc_block_group_size_class(ffe_ctl->num_bytes);\n\n\tif (btrfs_is_zoned(fs_info))\n\t\tffe_ctl->policy = BTRFS_EXTENT_ALLOC_ZONED;\n\n\tins->type = BTRFS_EXTENT_ITEM_KEY;\n\tins->objectid = 0;\n\tins->offset = 0;\n\n\ttrace_find_free_extent(root, ffe_ctl);\n\n\tspace_info = btrfs_find_space_info(fs_info, ffe_ctl->flags);\n\tif (!space_info) {\n\t\tbtrfs_err(fs_info, \"No space info for %llu\", ffe_ctl->flags);\n\t\treturn -ENOSPC;\n\t}\n\n\tret = prepare_allocation(fs_info, ffe_ctl, space_info, ins);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tffe_ctl->search_start = max(ffe_ctl->search_start,\n\t\t\t\t    first_logical_byte(fs_info));\n\tffe_ctl->search_start = max(ffe_ctl->search_start, ffe_ctl->hint_byte);\n\tif (ffe_ctl->search_start == ffe_ctl->hint_byte) {\n\t\tblock_group = btrfs_lookup_block_group(fs_info,\n\t\t\t\t\t\t       ffe_ctl->search_start);\n\t\t \n\t\tif (block_group && block_group_bits(block_group, ffe_ctl->flags) &&\n\t\t    block_group->cached != BTRFS_CACHE_NO) {\n\t\t\tdown_read(&space_info->groups_sem);\n\t\t\tif (list_empty(&block_group->list) ||\n\t\t\t    block_group->ro) {\n\t\t\t\t \n\t\t\t\tbtrfs_put_block_group(block_group);\n\t\t\t\tup_read(&space_info->groups_sem);\n\t\t\t} else {\n\t\t\t\tffe_ctl->index = btrfs_bg_flags_to_raid_index(\n\t\t\t\t\t\t\tblock_group->flags);\n\t\t\t\tbtrfs_lock_block_group(block_group,\n\t\t\t\t\t\t       ffe_ctl->delalloc);\n\t\t\t\tffe_ctl->hinted = true;\n\t\t\t\tgoto have_block_group;\n\t\t\t}\n\t\t} else if (block_group) {\n\t\t\tbtrfs_put_block_group(block_group);\n\t\t}\n\t}\nsearch:\n\ttrace_find_free_extent_search_loop(root, ffe_ctl);\n\tffe_ctl->have_caching_bg = false;\n\tif (ffe_ctl->index == btrfs_bg_flags_to_raid_index(ffe_ctl->flags) ||\n\t    ffe_ctl->index == 0)\n\t\tfull_search = true;\n\tdown_read(&space_info->groups_sem);\n\tlist_for_each_entry(block_group,\n\t\t\t    &space_info->block_groups[ffe_ctl->index], list) {\n\t\tstruct btrfs_block_group *bg_ret;\n\n\t\tffe_ctl->hinted = false;\n\t\t \n\t\tif (unlikely(block_group->ro)) {\n\t\t\tif (ffe_ctl->for_treelog)\n\t\t\t\tbtrfs_clear_treelog_bg(block_group);\n\t\t\tif (ffe_ctl->for_data_reloc)\n\t\t\t\tbtrfs_clear_data_reloc_bg(block_group);\n\t\t\tcontinue;\n\t\t}\n\n\t\tbtrfs_grab_block_group(block_group, ffe_ctl->delalloc);\n\t\tffe_ctl->search_start = block_group->start;\n\n\t\t \n\t\tif (!block_group_bits(block_group, ffe_ctl->flags)) {\n\t\t\tu64 extra = BTRFS_BLOCK_GROUP_DUP |\n\t\t\t\tBTRFS_BLOCK_GROUP_RAID1_MASK |\n\t\t\t\tBTRFS_BLOCK_GROUP_RAID56_MASK |\n\t\t\t\tBTRFS_BLOCK_GROUP_RAID10;\n\n\t\t\t \n\t\t\tif ((ffe_ctl->flags & extra) && !(block_group->flags & extra))\n\t\t\t\tgoto loop;\n\n\t\t\t \n\t\t\tbtrfs_release_block_group(block_group, ffe_ctl->delalloc);\n\t\t\tcontinue;\n\t\t}\n\nhave_block_group:\n\t\ttrace_find_free_extent_have_block_group(root, ffe_ctl, block_group);\n\t\tffe_ctl->cached = btrfs_block_group_done(block_group);\n\t\tif (unlikely(!ffe_ctl->cached)) {\n\t\t\tffe_ctl->have_caching_bg = true;\n\t\t\tret = btrfs_cache_block_group(block_group, false);\n\n\t\t\t \n\t\t\tif (ret < 0) {\n\t\t\t\tif (!cache_block_group_error)\n\t\t\t\t\tcache_block_group_error = ret;\n\t\t\t\tret = 0;\n\t\t\t\tgoto loop;\n\t\t\t}\n\t\t\tret = 0;\n\t\t}\n\n\t\tif (unlikely(block_group->cached == BTRFS_CACHE_ERROR)) {\n\t\t\tif (!cache_block_group_error)\n\t\t\t\tcache_block_group_error = -EIO;\n\t\t\tgoto loop;\n\t\t}\n\n\t\tif (!find_free_extent_check_size_class(ffe_ctl, block_group))\n\t\t\tgoto loop;\n\n\t\tbg_ret = NULL;\n\t\tret = do_allocation(block_group, ffe_ctl, &bg_ret);\n\t\tif (ret > 0)\n\t\t\tgoto loop;\n\n\t\tif (bg_ret && bg_ret != block_group) {\n\t\t\tbtrfs_release_block_group(block_group, ffe_ctl->delalloc);\n\t\t\tblock_group = bg_ret;\n\t\t}\n\n\t\t \n\t\tffe_ctl->search_start = round_up(ffe_ctl->found_offset,\n\t\t\t\t\t\t fs_info->stripesize);\n\n\t\t \n\t\tif (ffe_ctl->search_start + ffe_ctl->num_bytes >\n\t\t    block_group->start + block_group->length) {\n\t\t\tbtrfs_add_free_space_unused(block_group,\n\t\t\t\t\t    ffe_ctl->found_offset,\n\t\t\t\t\t    ffe_ctl->num_bytes);\n\t\t\tgoto loop;\n\t\t}\n\n\t\tif (ffe_ctl->found_offset < ffe_ctl->search_start)\n\t\t\tbtrfs_add_free_space_unused(block_group,\n\t\t\t\t\tffe_ctl->found_offset,\n\t\t\t\t\tffe_ctl->search_start - ffe_ctl->found_offset);\n\n\t\tret = btrfs_add_reserved_bytes(block_group, ffe_ctl->ram_bytes,\n\t\t\t\t\t       ffe_ctl->num_bytes,\n\t\t\t\t\t       ffe_ctl->delalloc,\n\t\t\t\t\t       ffe_ctl->loop >= LOOP_WRONG_SIZE_CLASS);\n\t\tif (ret == -EAGAIN) {\n\t\t\tbtrfs_add_free_space_unused(block_group,\n\t\t\t\t\tffe_ctl->found_offset,\n\t\t\t\t\tffe_ctl->num_bytes);\n\t\t\tgoto loop;\n\t\t}\n\t\tbtrfs_inc_block_group_reservations(block_group);\n\n\t\t \n\t\tins->objectid = ffe_ctl->search_start;\n\t\tins->offset = ffe_ctl->num_bytes;\n\n\t\ttrace_btrfs_reserve_extent(block_group, ffe_ctl);\n\t\tbtrfs_release_block_group(block_group, ffe_ctl->delalloc);\n\t\tbreak;\nloop:\n\t\tif (!ffe_ctl->cached && ffe_ctl->loop > LOOP_CACHING_NOWAIT &&\n\t\t    !ffe_ctl->retry_uncached) {\n\t\t\tffe_ctl->retry_uncached = true;\n\t\t\tbtrfs_wait_block_group_cache_progress(block_group,\n\t\t\t\t\t\tffe_ctl->num_bytes +\n\t\t\t\t\t\tffe_ctl->empty_cluster +\n\t\t\t\t\t\tffe_ctl->empty_size);\n\t\t\tgoto have_block_group;\n\t\t}\n\t\trelease_block_group(block_group, ffe_ctl, ffe_ctl->delalloc);\n\t\tcond_resched();\n\t}\n\tup_read(&space_info->groups_sem);\n\n\tret = find_free_extent_update_loop(fs_info, ins, ffe_ctl, full_search);\n\tif (ret > 0)\n\t\tgoto search;\n\n\tif (ret == -ENOSPC && !cache_block_group_error) {\n\t\t \n\t\tif (!ffe_ctl->max_extent_size)\n\t\t\tffe_ctl->max_extent_size = ffe_ctl->total_free_space;\n\t\tspin_lock(&space_info->lock);\n\t\tspace_info->max_extent_size = ffe_ctl->max_extent_size;\n\t\tspin_unlock(&space_info->lock);\n\t\tins->offset = ffe_ctl->max_extent_size;\n\t} else if (ret == -ENOSPC) {\n\t\tret = cache_block_group_error;\n\t}\n\treturn ret;\n}\n\n \nint btrfs_reserve_extent(struct btrfs_root *root, u64 ram_bytes,\n\t\t\t u64 num_bytes, u64 min_alloc_size,\n\t\t\t u64 empty_size, u64 hint_byte,\n\t\t\t struct btrfs_key *ins, int is_data, int delalloc)\n{\n\tstruct btrfs_fs_info *fs_info = root->fs_info;\n\tstruct find_free_extent_ctl ffe_ctl = {};\n\tbool final_tried = num_bytes == min_alloc_size;\n\tu64 flags;\n\tint ret;\n\tbool for_treelog = (root->root_key.objectid == BTRFS_TREE_LOG_OBJECTID);\n\tbool for_data_reloc = (btrfs_is_data_reloc_root(root) && is_data);\n\n\tflags = get_alloc_profile_by_root(root, is_data);\nagain:\n\tWARN_ON(num_bytes < fs_info->sectorsize);\n\n\tffe_ctl.ram_bytes = ram_bytes;\n\tffe_ctl.num_bytes = num_bytes;\n\tffe_ctl.min_alloc_size = min_alloc_size;\n\tffe_ctl.empty_size = empty_size;\n\tffe_ctl.flags = flags;\n\tffe_ctl.delalloc = delalloc;\n\tffe_ctl.hint_byte = hint_byte;\n\tffe_ctl.for_treelog = for_treelog;\n\tffe_ctl.for_data_reloc = for_data_reloc;\n\n\tret = find_free_extent(root, ins, &ffe_ctl);\n\tif (!ret && !is_data) {\n\t\tbtrfs_dec_block_group_reservations(fs_info, ins->objectid);\n\t} else if (ret == -ENOSPC) {\n\t\tif (!final_tried && ins->offset) {\n\t\t\tnum_bytes = min(num_bytes >> 1, ins->offset);\n\t\t\tnum_bytes = round_down(num_bytes,\n\t\t\t\t\t       fs_info->sectorsize);\n\t\t\tnum_bytes = max(num_bytes, min_alloc_size);\n\t\t\tram_bytes = num_bytes;\n\t\t\tif (num_bytes == min_alloc_size)\n\t\t\t\tfinal_tried = true;\n\t\t\tgoto again;\n\t\t} else if (btrfs_test_opt(fs_info, ENOSPC_DEBUG)) {\n\t\t\tstruct btrfs_space_info *sinfo;\n\n\t\t\tsinfo = btrfs_find_space_info(fs_info, flags);\n\t\t\tbtrfs_err(fs_info,\n\t\"allocation failed flags %llu, wanted %llu tree-log %d, relocation: %d\",\n\t\t\t\t  flags, num_bytes, for_treelog, for_data_reloc);\n\t\t\tif (sinfo)\n\t\t\t\tbtrfs_dump_space_info(fs_info, sinfo,\n\t\t\t\t\t\t      num_bytes, 1);\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nint btrfs_free_reserved_extent(struct btrfs_fs_info *fs_info,\n\t\t\t       u64 start, u64 len, int delalloc)\n{\n\tstruct btrfs_block_group *cache;\n\n\tcache = btrfs_lookup_block_group(fs_info, start);\n\tif (!cache) {\n\t\tbtrfs_err(fs_info, \"Unable to find block group for %llu\",\n\t\t\t  start);\n\t\treturn -ENOSPC;\n\t}\n\n\tbtrfs_add_free_space(cache, start, len);\n\tbtrfs_free_reserved_bytes(cache, len, delalloc);\n\ttrace_btrfs_reserved_extent_free(fs_info, start, len);\n\n\tbtrfs_put_block_group(cache);\n\treturn 0;\n}\n\nint btrfs_pin_reserved_extent(struct btrfs_trans_handle *trans, u64 start,\n\t\t\t      u64 len)\n{\n\tstruct btrfs_block_group *cache;\n\tint ret = 0;\n\n\tcache = btrfs_lookup_block_group(trans->fs_info, start);\n\tif (!cache) {\n\t\tbtrfs_err(trans->fs_info, \"unable to find block group for %llu\",\n\t\t\t  start);\n\t\treturn -ENOSPC;\n\t}\n\n\tret = pin_down_extent(trans, cache, start, len, 1);\n\tbtrfs_put_block_group(cache);\n\treturn ret;\n}\n\nstatic int alloc_reserved_extent(struct btrfs_trans_handle *trans, u64 bytenr,\n\t\t\t\t u64 num_bytes)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tint ret;\n\n\tret = remove_from_free_space_tree(trans, bytenr, num_bytes);\n\tif (ret)\n\t\treturn ret;\n\n\tret = btrfs_update_block_group(trans, bytenr, num_bytes, true);\n\tif (ret) {\n\t\tASSERT(!ret);\n\t\tbtrfs_err(fs_info, \"update block group failed for %llu %llu\",\n\t\t\t  bytenr, num_bytes);\n\t\treturn ret;\n\t}\n\n\ttrace_btrfs_reserved_extent_alloc(fs_info, bytenr, num_bytes);\n\treturn 0;\n}\n\nstatic int alloc_reserved_file_extent(struct btrfs_trans_handle *trans,\n\t\t\t\t      u64 parent, u64 root_objectid,\n\t\t\t\t      u64 flags, u64 owner, u64 offset,\n\t\t\t\t      struct btrfs_key *ins, int ref_mod)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_root *extent_root;\n\tint ret;\n\tstruct btrfs_extent_item *extent_item;\n\tstruct btrfs_extent_inline_ref *iref;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tint type;\n\tu32 size;\n\n\tif (parent > 0)\n\t\ttype = BTRFS_SHARED_DATA_REF_KEY;\n\telse\n\t\ttype = BTRFS_EXTENT_DATA_REF_KEY;\n\n\tsize = sizeof(*extent_item) + btrfs_extent_inline_ref_size(type);\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\textent_root = btrfs_extent_root(fs_info, ins->objectid);\n\tret = btrfs_insert_empty_item(trans, extent_root, path, ins, size);\n\tif (ret) {\n\t\tbtrfs_free_path(path);\n\t\treturn ret;\n\t}\n\n\tleaf = path->nodes[0];\n\textent_item = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t     struct btrfs_extent_item);\n\tbtrfs_set_extent_refs(leaf, extent_item, ref_mod);\n\tbtrfs_set_extent_generation(leaf, extent_item, trans->transid);\n\tbtrfs_set_extent_flags(leaf, extent_item,\n\t\t\t       flags | BTRFS_EXTENT_FLAG_DATA);\n\n\tiref = (struct btrfs_extent_inline_ref *)(extent_item + 1);\n\tbtrfs_set_extent_inline_ref_type(leaf, iref, type);\n\tif (parent > 0) {\n\t\tstruct btrfs_shared_data_ref *ref;\n\t\tref = (struct btrfs_shared_data_ref *)(iref + 1);\n\t\tbtrfs_set_extent_inline_ref_offset(leaf, iref, parent);\n\t\tbtrfs_set_shared_data_ref_count(leaf, ref, ref_mod);\n\t} else {\n\t\tstruct btrfs_extent_data_ref *ref;\n\t\tref = (struct btrfs_extent_data_ref *)(&iref->offset);\n\t\tbtrfs_set_extent_data_ref_root(leaf, ref, root_objectid);\n\t\tbtrfs_set_extent_data_ref_objectid(leaf, ref, owner);\n\t\tbtrfs_set_extent_data_ref_offset(leaf, ref, offset);\n\t\tbtrfs_set_extent_data_ref_count(leaf, ref, ref_mod);\n\t}\n\n\tbtrfs_mark_buffer_dirty(trans, path->nodes[0]);\n\tbtrfs_free_path(path);\n\n\treturn alloc_reserved_extent(trans, ins->objectid, ins->offset);\n}\n\nstatic int alloc_reserved_tree_block(struct btrfs_trans_handle *trans,\n\t\t\t\t     struct btrfs_delayed_ref_node *node,\n\t\t\t\t     struct btrfs_delayed_extent_op *extent_op)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_root *extent_root;\n\tint ret;\n\tstruct btrfs_extent_item *extent_item;\n\tstruct btrfs_key extent_key;\n\tstruct btrfs_tree_block_info *block_info;\n\tstruct btrfs_extent_inline_ref *iref;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_delayed_tree_ref *ref;\n\tu32 size = sizeof(*extent_item) + sizeof(*iref);\n\tu64 flags = extent_op->flags_to_set;\n\tbool skinny_metadata = btrfs_fs_incompat(fs_info, SKINNY_METADATA);\n\n\tref = btrfs_delayed_node_to_tree_ref(node);\n\n\textent_key.objectid = node->bytenr;\n\tif (skinny_metadata) {\n\t\textent_key.offset = ref->level;\n\t\textent_key.type = BTRFS_METADATA_ITEM_KEY;\n\t} else {\n\t\textent_key.offset = node->num_bytes;\n\t\textent_key.type = BTRFS_EXTENT_ITEM_KEY;\n\t\tsize += sizeof(*block_info);\n\t}\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\textent_root = btrfs_extent_root(fs_info, extent_key.objectid);\n\tret = btrfs_insert_empty_item(trans, extent_root, path, &extent_key,\n\t\t\t\t      size);\n\tif (ret) {\n\t\tbtrfs_free_path(path);\n\t\treturn ret;\n\t}\n\n\tleaf = path->nodes[0];\n\textent_item = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t     struct btrfs_extent_item);\n\tbtrfs_set_extent_refs(leaf, extent_item, 1);\n\tbtrfs_set_extent_generation(leaf, extent_item, trans->transid);\n\tbtrfs_set_extent_flags(leaf, extent_item,\n\t\t\t       flags | BTRFS_EXTENT_FLAG_TREE_BLOCK);\n\n\tif (skinny_metadata) {\n\t\tiref = (struct btrfs_extent_inline_ref *)(extent_item + 1);\n\t} else {\n\t\tblock_info = (struct btrfs_tree_block_info *)(extent_item + 1);\n\t\tbtrfs_set_tree_block_key(leaf, block_info, &extent_op->key);\n\t\tbtrfs_set_tree_block_level(leaf, block_info, ref->level);\n\t\tiref = (struct btrfs_extent_inline_ref *)(block_info + 1);\n\t}\n\n\tif (node->type == BTRFS_SHARED_BLOCK_REF_KEY) {\n\t\tbtrfs_set_extent_inline_ref_type(leaf, iref,\n\t\t\t\t\t\t BTRFS_SHARED_BLOCK_REF_KEY);\n\t\tbtrfs_set_extent_inline_ref_offset(leaf, iref, ref->parent);\n\t} else {\n\t\tbtrfs_set_extent_inline_ref_type(leaf, iref,\n\t\t\t\t\t\t BTRFS_TREE_BLOCK_REF_KEY);\n\t\tbtrfs_set_extent_inline_ref_offset(leaf, iref, ref->root);\n\t}\n\n\tbtrfs_mark_buffer_dirty(trans, leaf);\n\tbtrfs_free_path(path);\n\n\treturn alloc_reserved_extent(trans, node->bytenr, fs_info->nodesize);\n}\n\nint btrfs_alloc_reserved_file_extent(struct btrfs_trans_handle *trans,\n\t\t\t\t     struct btrfs_root *root, u64 owner,\n\t\t\t\t     u64 offset, u64 ram_bytes,\n\t\t\t\t     struct btrfs_key *ins)\n{\n\tstruct btrfs_ref generic_ref = { 0 };\n\n\tBUG_ON(root->root_key.objectid == BTRFS_TREE_LOG_OBJECTID);\n\n\tbtrfs_init_generic_ref(&generic_ref, BTRFS_ADD_DELAYED_EXTENT,\n\t\t\t       ins->objectid, ins->offset, 0);\n\tbtrfs_init_data_ref(&generic_ref, root->root_key.objectid, owner,\n\t\t\t    offset, 0, false);\n\tbtrfs_ref_tree_mod(root->fs_info, &generic_ref);\n\n\treturn btrfs_add_delayed_data_ref(trans, &generic_ref, ram_bytes);\n}\n\n \nint btrfs_alloc_logged_file_extent(struct btrfs_trans_handle *trans,\n\t\t\t\t   u64 root_objectid, u64 owner, u64 offset,\n\t\t\t\t   struct btrfs_key *ins)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tint ret;\n\tstruct btrfs_block_group *block_group;\n\tstruct btrfs_space_info *space_info;\n\n\t \n\tif (!btrfs_fs_incompat(fs_info, MIXED_GROUPS)) {\n\t\tret = __exclude_logged_extent(fs_info, ins->objectid,\n\t\t\t\t\t      ins->offset);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tblock_group = btrfs_lookup_block_group(fs_info, ins->objectid);\n\tif (!block_group)\n\t\treturn -EINVAL;\n\n\tspace_info = block_group->space_info;\n\tspin_lock(&space_info->lock);\n\tspin_lock(&block_group->lock);\n\tspace_info->bytes_reserved += ins->offset;\n\tblock_group->reserved += ins->offset;\n\tspin_unlock(&block_group->lock);\n\tspin_unlock(&space_info->lock);\n\n\tret = alloc_reserved_file_extent(trans, 0, root_objectid, 0, owner,\n\t\t\t\t\t offset, ins, 1);\n\tif (ret)\n\t\tbtrfs_pin_extent(trans, ins->objectid, ins->offset, 1);\n\tbtrfs_put_block_group(block_group);\n\treturn ret;\n}\n\nstatic struct extent_buffer *\nbtrfs_init_new_buffer(struct btrfs_trans_handle *trans, struct btrfs_root *root,\n\t\t      u64 bytenr, int level, u64 owner,\n\t\t      enum btrfs_lock_nesting nest)\n{\n\tstruct btrfs_fs_info *fs_info = root->fs_info;\n\tstruct extent_buffer *buf;\n\tu64 lockdep_owner = owner;\n\n\tbuf = btrfs_find_create_tree_block(fs_info, bytenr, owner, level);\n\tif (IS_ERR(buf))\n\t\treturn buf;\n\n\t \n\tif (buf->lock_owner == current->pid) {\n\t\tbtrfs_err_rl(fs_info,\n\"tree block %llu owner %llu already locked by pid=%d, extent tree corruption detected\",\n\t\t\tbuf->start, btrfs_header_owner(buf), current->pid);\n\t\tfree_extent_buffer(buf);\n\t\treturn ERR_PTR(-EUCLEAN);\n\t}\n\n\t \n\tif (lockdep_owner == BTRFS_TREE_RELOC_OBJECTID &&\n\t    !test_bit(BTRFS_ROOT_RESET_LOCKDEP_CLASS, &root->state))\n\t\tlockdep_owner = BTRFS_FS_TREE_OBJECTID;\n\n\t \n\tbtrfs_set_header_generation(buf, trans->transid);\n\n\t \n\tbtrfs_set_buffer_lockdep_class(lockdep_owner, buf, level);\n\n\t__btrfs_tree_lock(buf, nest);\n\tbtrfs_clear_buffer_dirty(trans, buf);\n\tclear_bit(EXTENT_BUFFER_STALE, &buf->bflags);\n\tclear_bit(EXTENT_BUFFER_NO_CHECK, &buf->bflags);\n\n\tset_extent_buffer_uptodate(buf);\n\n\tmemzero_extent_buffer(buf, 0, sizeof(struct btrfs_header));\n\tbtrfs_set_header_level(buf, level);\n\tbtrfs_set_header_bytenr(buf, buf->start);\n\tbtrfs_set_header_generation(buf, trans->transid);\n\tbtrfs_set_header_backref_rev(buf, BTRFS_MIXED_BACKREF_REV);\n\tbtrfs_set_header_owner(buf, owner);\n\twrite_extent_buffer_fsid(buf, fs_info->fs_devices->metadata_uuid);\n\twrite_extent_buffer_chunk_tree_uuid(buf, fs_info->chunk_tree_uuid);\n\tif (root->root_key.objectid == BTRFS_TREE_LOG_OBJECTID) {\n\t\tbuf->log_index = root->log_transid % 2;\n\t\t \n\t\tif (buf->log_index == 0)\n\t\t\tset_extent_bit(&root->dirty_log_pages, buf->start,\n\t\t\t\t       buf->start + buf->len - 1,\n\t\t\t\t       EXTENT_DIRTY, NULL);\n\t\telse\n\t\t\tset_extent_bit(&root->dirty_log_pages, buf->start,\n\t\t\t\t       buf->start + buf->len - 1,\n\t\t\t\t       EXTENT_NEW, NULL);\n\t} else {\n\t\tbuf->log_index = -1;\n\t\tset_extent_bit(&trans->transaction->dirty_pages, buf->start,\n\t\t\t       buf->start + buf->len - 1, EXTENT_DIRTY, NULL);\n\t}\n\t \n\treturn buf;\n}\n\n \nstruct extent_buffer *btrfs_alloc_tree_block(struct btrfs_trans_handle *trans,\n\t\t\t\t\t     struct btrfs_root *root,\n\t\t\t\t\t     u64 parent, u64 root_objectid,\n\t\t\t\t\t     const struct btrfs_disk_key *key,\n\t\t\t\t\t     int level, u64 hint,\n\t\t\t\t\t     u64 empty_size,\n\t\t\t\t\t     enum btrfs_lock_nesting nest)\n{\n\tstruct btrfs_fs_info *fs_info = root->fs_info;\n\tstruct btrfs_key ins;\n\tstruct btrfs_block_rsv *block_rsv;\n\tstruct extent_buffer *buf;\n\tstruct btrfs_delayed_extent_op *extent_op;\n\tstruct btrfs_ref generic_ref = { 0 };\n\tu64 flags = 0;\n\tint ret;\n\tu32 blocksize = fs_info->nodesize;\n\tbool skinny_metadata = btrfs_fs_incompat(fs_info, SKINNY_METADATA);\n\n#ifdef CONFIG_BTRFS_FS_RUN_SANITY_TESTS\n\tif (btrfs_is_testing(fs_info)) {\n\t\tbuf = btrfs_init_new_buffer(trans, root, root->alloc_bytenr,\n\t\t\t\t\t    level, root_objectid, nest);\n\t\tif (!IS_ERR(buf))\n\t\t\troot->alloc_bytenr += blocksize;\n\t\treturn buf;\n\t}\n#endif\n\n\tblock_rsv = btrfs_use_block_rsv(trans, root, blocksize);\n\tif (IS_ERR(block_rsv))\n\t\treturn ERR_CAST(block_rsv);\n\n\tret = btrfs_reserve_extent(root, blocksize, blocksize, blocksize,\n\t\t\t\t   empty_size, hint, &ins, 0, 0);\n\tif (ret)\n\t\tgoto out_unuse;\n\n\tbuf = btrfs_init_new_buffer(trans, root, ins.objectid, level,\n\t\t\t\t    root_objectid, nest);\n\tif (IS_ERR(buf)) {\n\t\tret = PTR_ERR(buf);\n\t\tgoto out_free_reserved;\n\t}\n\n\tif (root_objectid == BTRFS_TREE_RELOC_OBJECTID) {\n\t\tif (parent == 0)\n\t\t\tparent = ins.objectid;\n\t\tflags |= BTRFS_BLOCK_FLAG_FULL_BACKREF;\n\t} else\n\t\tBUG_ON(parent > 0);\n\n\tif (root_objectid != BTRFS_TREE_LOG_OBJECTID) {\n\t\textent_op = btrfs_alloc_delayed_extent_op();\n\t\tif (!extent_op) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out_free_buf;\n\t\t}\n\t\tif (key)\n\t\t\tmemcpy(&extent_op->key, key, sizeof(extent_op->key));\n\t\telse\n\t\t\tmemset(&extent_op->key, 0, sizeof(extent_op->key));\n\t\textent_op->flags_to_set = flags;\n\t\textent_op->update_key = skinny_metadata ? false : true;\n\t\textent_op->update_flags = true;\n\t\textent_op->level = level;\n\n\t\tbtrfs_init_generic_ref(&generic_ref, BTRFS_ADD_DELAYED_EXTENT,\n\t\t\t\t       ins.objectid, ins.offset, parent);\n\t\tbtrfs_init_tree_ref(&generic_ref, level, root_objectid,\n\t\t\t\t    root->root_key.objectid, false);\n\t\tbtrfs_ref_tree_mod(fs_info, &generic_ref);\n\t\tret = btrfs_add_delayed_tree_ref(trans, &generic_ref, extent_op);\n\t\tif (ret)\n\t\t\tgoto out_free_delayed;\n\t}\n\treturn buf;\n\nout_free_delayed:\n\tbtrfs_free_delayed_extent_op(extent_op);\nout_free_buf:\n\tbtrfs_tree_unlock(buf);\n\tfree_extent_buffer(buf);\nout_free_reserved:\n\tbtrfs_free_reserved_extent(fs_info, ins.objectid, ins.offset, 0);\nout_unuse:\n\tbtrfs_unuse_block_rsv(fs_info, block_rsv, blocksize);\n\treturn ERR_PTR(ret);\n}\n\nstruct walk_control {\n\tu64 refs[BTRFS_MAX_LEVEL];\n\tu64 flags[BTRFS_MAX_LEVEL];\n\tstruct btrfs_key update_progress;\n\tstruct btrfs_key drop_progress;\n\tint drop_level;\n\tint stage;\n\tint level;\n\tint shared_level;\n\tint update_ref;\n\tint keep_locks;\n\tint reada_slot;\n\tint reada_count;\n\tint restarted;\n};\n\n#define DROP_REFERENCE\t1\n#define UPDATE_BACKREF\t2\n\nstatic noinline void reada_walk_down(struct btrfs_trans_handle *trans,\n\t\t\t\t     struct btrfs_root *root,\n\t\t\t\t     struct walk_control *wc,\n\t\t\t\t     struct btrfs_path *path)\n{\n\tstruct btrfs_fs_info *fs_info = root->fs_info;\n\tu64 bytenr;\n\tu64 generation;\n\tu64 refs;\n\tu64 flags;\n\tu32 nritems;\n\tstruct btrfs_key key;\n\tstruct extent_buffer *eb;\n\tint ret;\n\tint slot;\n\tint nread = 0;\n\n\tif (path->slots[wc->level] < wc->reada_slot) {\n\t\twc->reada_count = wc->reada_count * 2 / 3;\n\t\twc->reada_count = max(wc->reada_count, 2);\n\t} else {\n\t\twc->reada_count = wc->reada_count * 3 / 2;\n\t\twc->reada_count = min_t(int, wc->reada_count,\n\t\t\t\t\tBTRFS_NODEPTRS_PER_BLOCK(fs_info));\n\t}\n\n\teb = path->nodes[wc->level];\n\tnritems = btrfs_header_nritems(eb);\n\n\tfor (slot = path->slots[wc->level]; slot < nritems; slot++) {\n\t\tif (nread >= wc->reada_count)\n\t\t\tbreak;\n\n\t\tcond_resched();\n\t\tbytenr = btrfs_node_blockptr(eb, slot);\n\t\tgeneration = btrfs_node_ptr_generation(eb, slot);\n\n\t\tif (slot == path->slots[wc->level])\n\t\t\tgoto reada;\n\n\t\tif (wc->stage == UPDATE_BACKREF &&\n\t\t    generation <= root->root_key.offset)\n\t\t\tcontinue;\n\n\t\t \n\t\tret = btrfs_lookup_extent_info(trans, fs_info, bytenr,\n\t\t\t\t\t       wc->level - 1, 1, &refs,\n\t\t\t\t\t       &flags);\n\t\t \n\t\tif (ret < 0)\n\t\t\tcontinue;\n\t\tBUG_ON(refs == 0);\n\n\t\tif (wc->stage == DROP_REFERENCE) {\n\t\t\tif (refs == 1)\n\t\t\t\tgoto reada;\n\n\t\t\tif (wc->level == 1 &&\n\t\t\t    (flags & BTRFS_BLOCK_FLAG_FULL_BACKREF))\n\t\t\t\tcontinue;\n\t\t\tif (!wc->update_ref ||\n\t\t\t    generation <= root->root_key.offset)\n\t\t\t\tcontinue;\n\t\t\tbtrfs_node_key_to_cpu(eb, &key, slot);\n\t\t\tret = btrfs_comp_cpu_keys(&key,\n\t\t\t\t\t\t  &wc->update_progress);\n\t\t\tif (ret < 0)\n\t\t\t\tcontinue;\n\t\t} else {\n\t\t\tif (wc->level == 1 &&\n\t\t\t    (flags & BTRFS_BLOCK_FLAG_FULL_BACKREF))\n\t\t\t\tcontinue;\n\t\t}\nreada:\n\t\tbtrfs_readahead_node_child(eb, slot);\n\t\tnread++;\n\t}\n\twc->reada_slot = slot;\n}\n\n \nstatic noinline int walk_down_proc(struct btrfs_trans_handle *trans,\n\t\t\t\t   struct btrfs_root *root,\n\t\t\t\t   struct btrfs_path *path,\n\t\t\t\t   struct walk_control *wc, int lookup_info)\n{\n\tstruct btrfs_fs_info *fs_info = root->fs_info;\n\tint level = wc->level;\n\tstruct extent_buffer *eb = path->nodes[level];\n\tu64 flag = BTRFS_BLOCK_FLAG_FULL_BACKREF;\n\tint ret;\n\n\tif (wc->stage == UPDATE_BACKREF &&\n\t    btrfs_header_owner(eb) != root->root_key.objectid)\n\t\treturn 1;\n\n\t \n\tif (lookup_info &&\n\t    ((wc->stage == DROP_REFERENCE && wc->refs[level] != 1) ||\n\t     (wc->stage == UPDATE_BACKREF && !(wc->flags[level] & flag)))) {\n\t\tBUG_ON(!path->locks[level]);\n\t\tret = btrfs_lookup_extent_info(trans, fs_info,\n\t\t\t\t\t       eb->start, level, 1,\n\t\t\t\t\t       &wc->refs[level],\n\t\t\t\t\t       &wc->flags[level]);\n\t\tBUG_ON(ret == -ENOMEM);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tBUG_ON(wc->refs[level] == 0);\n\t}\n\n\tif (wc->stage == DROP_REFERENCE) {\n\t\tif (wc->refs[level] > 1)\n\t\t\treturn 1;\n\n\t\tif (path->locks[level] && !wc->keep_locks) {\n\t\t\tbtrfs_tree_unlock_rw(eb, path->locks[level]);\n\t\t\tpath->locks[level] = 0;\n\t\t}\n\t\treturn 0;\n\t}\n\n\t \n\tif (!(wc->flags[level] & flag)) {\n\t\tBUG_ON(!path->locks[level]);\n\t\tret = btrfs_inc_ref(trans, root, eb, 1);\n\t\tBUG_ON(ret);  \n\t\tret = btrfs_dec_ref(trans, root, eb, 0);\n\t\tBUG_ON(ret);  \n\t\tret = btrfs_set_disk_extent_flags(trans, eb, flag);\n\t\tBUG_ON(ret);  \n\t\twc->flags[level] |= flag;\n\t}\n\n\t \n\tif (path->locks[level] && level > 0) {\n\t\tbtrfs_tree_unlock_rw(eb, path->locks[level]);\n\t\tpath->locks[level] = 0;\n\t}\n\treturn 0;\n}\n\n \nstatic int check_ref_exists(struct btrfs_trans_handle *trans,\n\t\t\t    struct btrfs_root *root, u64 bytenr, u64 parent,\n\t\t\t    int level)\n{\n\tstruct btrfs_path *path;\n\tstruct btrfs_extent_inline_ref *iref;\n\tint ret;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tret = lookup_extent_backref(trans, path, &iref, bytenr,\n\t\t\t\t    root->fs_info->nodesize, parent,\n\t\t\t\t    root->root_key.objectid, level, 0);\n\tbtrfs_free_path(path);\n\tif (ret == -ENOENT)\n\t\treturn 0;\n\tif (ret < 0)\n\t\treturn ret;\n\treturn 1;\n}\n\n \nstatic noinline int do_walk_down(struct btrfs_trans_handle *trans,\n\t\t\t\t struct btrfs_root *root,\n\t\t\t\t struct btrfs_path *path,\n\t\t\t\t struct walk_control *wc, int *lookup_info)\n{\n\tstruct btrfs_fs_info *fs_info = root->fs_info;\n\tu64 bytenr;\n\tu64 generation;\n\tu64 parent;\n\tstruct btrfs_tree_parent_check check = { 0 };\n\tstruct btrfs_key key;\n\tstruct btrfs_ref ref = { 0 };\n\tstruct extent_buffer *next;\n\tint level = wc->level;\n\tint reada = 0;\n\tint ret = 0;\n\tbool need_account = false;\n\n\tgeneration = btrfs_node_ptr_generation(path->nodes[level],\n\t\t\t\t\t       path->slots[level]);\n\t \n\tif (wc->stage == UPDATE_BACKREF &&\n\t    generation <= root->root_key.offset) {\n\t\t*lookup_info = 1;\n\t\treturn 1;\n\t}\n\n\tbytenr = btrfs_node_blockptr(path->nodes[level], path->slots[level]);\n\n\tcheck.level = level - 1;\n\tcheck.transid = generation;\n\tcheck.owner_root = root->root_key.objectid;\n\tcheck.has_first_key = true;\n\tbtrfs_node_key_to_cpu(path->nodes[level], &check.first_key,\n\t\t\t      path->slots[level]);\n\n\tnext = find_extent_buffer(fs_info, bytenr);\n\tif (!next) {\n\t\tnext = btrfs_find_create_tree_block(fs_info, bytenr,\n\t\t\t\troot->root_key.objectid, level - 1);\n\t\tif (IS_ERR(next))\n\t\t\treturn PTR_ERR(next);\n\t\treada = 1;\n\t}\n\tbtrfs_tree_lock(next);\n\n\tret = btrfs_lookup_extent_info(trans, fs_info, bytenr, level - 1, 1,\n\t\t\t\t       &wc->refs[level - 1],\n\t\t\t\t       &wc->flags[level - 1]);\n\tif (ret < 0)\n\t\tgoto out_unlock;\n\n\tif (unlikely(wc->refs[level - 1] == 0)) {\n\t\tbtrfs_err(fs_info, \"Missing references.\");\n\t\tret = -EIO;\n\t\tgoto out_unlock;\n\t}\n\t*lookup_info = 0;\n\n\tif (wc->stage == DROP_REFERENCE) {\n\t\tif (wc->refs[level - 1] > 1) {\n\t\t\tneed_account = true;\n\t\t\tif (level == 1 &&\n\t\t\t    (wc->flags[0] & BTRFS_BLOCK_FLAG_FULL_BACKREF))\n\t\t\t\tgoto skip;\n\n\t\t\tif (!wc->update_ref ||\n\t\t\t    generation <= root->root_key.offset)\n\t\t\t\tgoto skip;\n\n\t\t\tbtrfs_node_key_to_cpu(path->nodes[level], &key,\n\t\t\t\t\t      path->slots[level]);\n\t\t\tret = btrfs_comp_cpu_keys(&key, &wc->update_progress);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto skip;\n\n\t\t\twc->stage = UPDATE_BACKREF;\n\t\t\twc->shared_level = level - 1;\n\t\t}\n\t} else {\n\t\tif (level == 1 &&\n\t\t    (wc->flags[0] & BTRFS_BLOCK_FLAG_FULL_BACKREF))\n\t\t\tgoto skip;\n\t}\n\n\tif (!btrfs_buffer_uptodate(next, generation, 0)) {\n\t\tbtrfs_tree_unlock(next);\n\t\tfree_extent_buffer(next);\n\t\tnext = NULL;\n\t\t*lookup_info = 1;\n\t}\n\n\tif (!next) {\n\t\tif (reada && level == 1)\n\t\t\treada_walk_down(trans, root, wc, path);\n\t\tnext = read_tree_block(fs_info, bytenr, &check);\n\t\tif (IS_ERR(next)) {\n\t\t\treturn PTR_ERR(next);\n\t\t} else if (!extent_buffer_uptodate(next)) {\n\t\t\tfree_extent_buffer(next);\n\t\t\treturn -EIO;\n\t\t}\n\t\tbtrfs_tree_lock(next);\n\t}\n\n\tlevel--;\n\tASSERT(level == btrfs_header_level(next));\n\tif (level != btrfs_header_level(next)) {\n\t\tbtrfs_err(root->fs_info, \"mismatched level\");\n\t\tret = -EIO;\n\t\tgoto out_unlock;\n\t}\n\tpath->nodes[level] = next;\n\tpath->slots[level] = 0;\n\tpath->locks[level] = BTRFS_WRITE_LOCK;\n\twc->level = level;\n\tif (wc->level == 1)\n\t\twc->reada_slot = 0;\n\treturn 0;\nskip:\n\twc->refs[level - 1] = 0;\n\twc->flags[level - 1] = 0;\n\tif (wc->stage == DROP_REFERENCE) {\n\t\tif (wc->flags[level] & BTRFS_BLOCK_FLAG_FULL_BACKREF) {\n\t\t\tparent = path->nodes[level]->start;\n\t\t} else {\n\t\t\tASSERT(root->root_key.objectid ==\n\t\t\t       btrfs_header_owner(path->nodes[level]));\n\t\t\tif (root->root_key.objectid !=\n\t\t\t    btrfs_header_owner(path->nodes[level])) {\n\t\t\t\tbtrfs_err(root->fs_info,\n\t\t\t\t\t\t\"mismatched block owner\");\n\t\t\t\tret = -EIO;\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t\tparent = 0;\n\t\t}\n\n\t\t \n\t\tif (wc->restarted) {\n\t\t\tret = check_ref_exists(trans, root, bytenr, parent,\n\t\t\t\t\t       level - 1);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out_unlock;\n\t\t\tif (ret == 0)\n\t\t\t\tgoto no_delete;\n\t\t\tret = 0;\n\t\t\twc->restarted = 0;\n\t\t}\n\n\t\t \n\t\tif (root->root_key.objectid != BTRFS_TREE_RELOC_OBJECTID &&\n\t\t    need_account) {\n\t\t\tret = btrfs_qgroup_trace_subtree(trans, next,\n\t\t\t\t\t\t\t generation, level - 1);\n\t\t\tif (ret) {\n\t\t\t\tbtrfs_err_rl(fs_info,\n\t\t\t\t\t     \"Error %d accounting shared subtree. Quota is out of sync, rescan required.\",\n\t\t\t\t\t     ret);\n\t\t\t}\n\t\t}\n\n\t\t \n\t\twc->drop_level = level;\n\t\tfind_next_key(path, level, &wc->drop_progress);\n\n\t\tbtrfs_init_generic_ref(&ref, BTRFS_DROP_DELAYED_REF, bytenr,\n\t\t\t\t       fs_info->nodesize, parent);\n\t\tbtrfs_init_tree_ref(&ref, level - 1, root->root_key.objectid,\n\t\t\t\t    0, false);\n\t\tret = btrfs_free_extent(trans, &ref);\n\t\tif (ret)\n\t\t\tgoto out_unlock;\n\t}\nno_delete:\n\t*lookup_info = 1;\n\tret = 1;\n\nout_unlock:\n\tbtrfs_tree_unlock(next);\n\tfree_extent_buffer(next);\n\n\treturn ret;\n}\n\n \nstatic noinline int walk_up_proc(struct btrfs_trans_handle *trans,\n\t\t\t\t struct btrfs_root *root,\n\t\t\t\t struct btrfs_path *path,\n\t\t\t\t struct walk_control *wc)\n{\n\tstruct btrfs_fs_info *fs_info = root->fs_info;\n\tint ret;\n\tint level = wc->level;\n\tstruct extent_buffer *eb = path->nodes[level];\n\tu64 parent = 0;\n\n\tif (wc->stage == UPDATE_BACKREF) {\n\t\tBUG_ON(wc->shared_level < level);\n\t\tif (level < wc->shared_level)\n\t\t\tgoto out;\n\n\t\tret = find_next_key(path, level + 1, &wc->update_progress);\n\t\tif (ret > 0)\n\t\t\twc->update_ref = 0;\n\n\t\twc->stage = DROP_REFERENCE;\n\t\twc->shared_level = -1;\n\t\tpath->slots[level] = 0;\n\n\t\t \n\t\tif (!path->locks[level]) {\n\t\t\tBUG_ON(level == 0);\n\t\t\tbtrfs_tree_lock(eb);\n\t\t\tpath->locks[level] = BTRFS_WRITE_LOCK;\n\n\t\t\tret = btrfs_lookup_extent_info(trans, fs_info,\n\t\t\t\t\t\t       eb->start, level, 1,\n\t\t\t\t\t\t       &wc->refs[level],\n\t\t\t\t\t\t       &wc->flags[level]);\n\t\t\tif (ret < 0) {\n\t\t\t\tbtrfs_tree_unlock_rw(eb, path->locks[level]);\n\t\t\t\tpath->locks[level] = 0;\n\t\t\t\treturn ret;\n\t\t\t}\n\t\t\tBUG_ON(wc->refs[level] == 0);\n\t\t\tif (wc->refs[level] == 1) {\n\t\t\t\tbtrfs_tree_unlock_rw(eb, path->locks[level]);\n\t\t\t\tpath->locks[level] = 0;\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tBUG_ON(wc->refs[level] > 1 && !path->locks[level]);\n\n\tif (wc->refs[level] == 1) {\n\t\tif (level == 0) {\n\t\t\tif (wc->flags[level] & BTRFS_BLOCK_FLAG_FULL_BACKREF)\n\t\t\t\tret = btrfs_dec_ref(trans, root, eb, 1);\n\t\t\telse\n\t\t\t\tret = btrfs_dec_ref(trans, root, eb, 0);\n\t\t\tBUG_ON(ret);  \n\t\t\tif (is_fstree(root->root_key.objectid)) {\n\t\t\t\tret = btrfs_qgroup_trace_leaf_items(trans, eb);\n\t\t\t\tif (ret) {\n\t\t\t\t\tbtrfs_err_rl(fs_info,\n\t\"error %d accounting leaf items, quota is out of sync, rescan required\",\n\t\t\t\t\t     ret);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t \n\t\tif (!path->locks[level]) {\n\t\t\tbtrfs_tree_lock(eb);\n\t\t\tpath->locks[level] = BTRFS_WRITE_LOCK;\n\t\t}\n\t\tbtrfs_clear_buffer_dirty(trans, eb);\n\t}\n\n\tif (eb == root->node) {\n\t\tif (wc->flags[level] & BTRFS_BLOCK_FLAG_FULL_BACKREF)\n\t\t\tparent = eb->start;\n\t\telse if (root->root_key.objectid != btrfs_header_owner(eb))\n\t\t\tgoto owner_mismatch;\n\t} else {\n\t\tif (wc->flags[level + 1] & BTRFS_BLOCK_FLAG_FULL_BACKREF)\n\t\t\tparent = path->nodes[level + 1]->start;\n\t\telse if (root->root_key.objectid !=\n\t\t\t btrfs_header_owner(path->nodes[level + 1]))\n\t\t\tgoto owner_mismatch;\n\t}\n\n\tbtrfs_free_tree_block(trans, btrfs_root_id(root), eb, parent,\n\t\t\t      wc->refs[level] == 1);\nout:\n\twc->refs[level] = 0;\n\twc->flags[level] = 0;\n\treturn 0;\n\nowner_mismatch:\n\tbtrfs_err_rl(fs_info, \"unexpected tree owner, have %llu expect %llu\",\n\t\t     btrfs_header_owner(eb), root->root_key.objectid);\n\treturn -EUCLEAN;\n}\n\nstatic noinline int walk_down_tree(struct btrfs_trans_handle *trans,\n\t\t\t\t   struct btrfs_root *root,\n\t\t\t\t   struct btrfs_path *path,\n\t\t\t\t   struct walk_control *wc)\n{\n\tint level = wc->level;\n\tint lookup_info = 1;\n\tint ret = 0;\n\n\twhile (level >= 0) {\n\t\tret = walk_down_proc(trans, root, path, wc, lookup_info);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tif (level == 0)\n\t\t\tbreak;\n\n\t\tif (path->slots[level] >=\n\t\t    btrfs_header_nritems(path->nodes[level]))\n\t\t\tbreak;\n\n\t\tret = do_walk_down(trans, root, path, wc, &lookup_info);\n\t\tif (ret > 0) {\n\t\t\tpath->slots[level]++;\n\t\t\tcontinue;\n\t\t} else if (ret < 0)\n\t\t\tbreak;\n\t\tlevel = wc->level;\n\t}\n\treturn (ret == 1) ? 0 : ret;\n}\n\nstatic noinline int walk_up_tree(struct btrfs_trans_handle *trans,\n\t\t\t\t struct btrfs_root *root,\n\t\t\t\t struct btrfs_path *path,\n\t\t\t\t struct walk_control *wc, int max_level)\n{\n\tint level = wc->level;\n\tint ret;\n\n\tpath->slots[level] = btrfs_header_nritems(path->nodes[level]);\n\twhile (level < max_level && path->nodes[level]) {\n\t\twc->level = level;\n\t\tif (path->slots[level] + 1 <\n\t\t    btrfs_header_nritems(path->nodes[level])) {\n\t\t\tpath->slots[level]++;\n\t\t\treturn 0;\n\t\t} else {\n\t\t\tret = walk_up_proc(trans, root, path, wc);\n\t\t\tif (ret > 0)\n\t\t\t\treturn 0;\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\n\t\t\tif (path->locks[level]) {\n\t\t\t\tbtrfs_tree_unlock_rw(path->nodes[level],\n\t\t\t\t\t\t     path->locks[level]);\n\t\t\t\tpath->locks[level] = 0;\n\t\t\t}\n\t\t\tfree_extent_buffer(path->nodes[level]);\n\t\t\tpath->nodes[level] = NULL;\n\t\t\tlevel++;\n\t\t}\n\t}\n\treturn 1;\n}\n\n \nint btrfs_drop_snapshot(struct btrfs_root *root, int update_ref, int for_reloc)\n{\n\tconst bool is_reloc_root = (root->root_key.objectid ==\n\t\t\t\t    BTRFS_TREE_RELOC_OBJECTID);\n\tstruct btrfs_fs_info *fs_info = root->fs_info;\n\tstruct btrfs_path *path;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_root *tree_root = fs_info->tree_root;\n\tstruct btrfs_root_item *root_item = &root->root_item;\n\tstruct walk_control *wc;\n\tstruct btrfs_key key;\n\tint err = 0;\n\tint ret;\n\tint level;\n\tbool root_dropped = false;\n\tbool unfinished_drop = false;\n\n\tbtrfs_debug(fs_info, \"Drop subvolume %llu\", root->root_key.objectid);\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\twc = kzalloc(sizeof(*wc), GFP_NOFS);\n\tif (!wc) {\n\t\tbtrfs_free_path(path);\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\t \n\tif (for_reloc)\n\t\ttrans = btrfs_join_transaction(tree_root);\n\telse\n\t\ttrans = btrfs_start_transaction(tree_root, 0);\n\tif (IS_ERR(trans)) {\n\t\terr = PTR_ERR(trans);\n\t\tgoto out_free;\n\t}\n\n\terr = btrfs_run_delayed_items(trans);\n\tif (err)\n\t\tgoto out_end_trans;\n\n\t \n\tset_bit(BTRFS_ROOT_DELETING, &root->state);\n\tunfinished_drop = test_bit(BTRFS_ROOT_UNFINISHED_DROP, &root->state);\n\n\tif (btrfs_disk_key_objectid(&root_item->drop_progress) == 0) {\n\t\tlevel = btrfs_header_level(root->node);\n\t\tpath->nodes[level] = btrfs_lock_root_node(root);\n\t\tpath->slots[level] = 0;\n\t\tpath->locks[level] = BTRFS_WRITE_LOCK;\n\t\tmemset(&wc->update_progress, 0,\n\t\t       sizeof(wc->update_progress));\n\t} else {\n\t\tbtrfs_disk_key_to_cpu(&key, &root_item->drop_progress);\n\t\tmemcpy(&wc->update_progress, &key,\n\t\t       sizeof(wc->update_progress));\n\n\t\tlevel = btrfs_root_drop_level(root_item);\n\t\tBUG_ON(level == 0);\n\t\tpath->lowest_level = level;\n\t\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\t\tpath->lowest_level = 0;\n\t\tif (ret < 0) {\n\t\t\terr = ret;\n\t\t\tgoto out_end_trans;\n\t\t}\n\t\tWARN_ON(ret > 0);\n\n\t\t \n\t\tbtrfs_unlock_up_safe(path, 0);\n\n\t\tlevel = btrfs_header_level(root->node);\n\t\twhile (1) {\n\t\t\tbtrfs_tree_lock(path->nodes[level]);\n\t\t\tpath->locks[level] = BTRFS_WRITE_LOCK;\n\n\t\t\tret = btrfs_lookup_extent_info(trans, fs_info,\n\t\t\t\t\t\tpath->nodes[level]->start,\n\t\t\t\t\t\tlevel, 1, &wc->refs[level],\n\t\t\t\t\t\t&wc->flags[level]);\n\t\t\tif (ret < 0) {\n\t\t\t\terr = ret;\n\t\t\t\tgoto out_end_trans;\n\t\t\t}\n\t\t\tBUG_ON(wc->refs[level] == 0);\n\n\t\t\tif (level == btrfs_root_drop_level(root_item))\n\t\t\t\tbreak;\n\n\t\t\tbtrfs_tree_unlock(path->nodes[level]);\n\t\t\tpath->locks[level] = 0;\n\t\t\tWARN_ON(wc->refs[level] != 1);\n\t\t\tlevel--;\n\t\t}\n\t}\n\n\twc->restarted = test_bit(BTRFS_ROOT_DEAD_TREE, &root->state);\n\twc->level = level;\n\twc->shared_level = -1;\n\twc->stage = DROP_REFERENCE;\n\twc->update_ref = update_ref;\n\twc->keep_locks = 0;\n\twc->reada_count = BTRFS_NODEPTRS_PER_BLOCK(fs_info);\n\n\twhile (1) {\n\n\t\tret = walk_down_tree(trans, root, path, wc);\n\t\tif (ret < 0) {\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\terr = ret;\n\t\t\tbreak;\n\t\t}\n\n\t\tret = walk_up_tree(trans, root, path, wc, BTRFS_MAX_LEVEL);\n\t\tif (ret < 0) {\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\terr = ret;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (ret > 0) {\n\t\t\tBUG_ON(wc->stage != DROP_REFERENCE);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (wc->stage == DROP_REFERENCE) {\n\t\t\twc->drop_level = wc->level;\n\t\t\tbtrfs_node_key_to_cpu(path->nodes[wc->drop_level],\n\t\t\t\t\t      &wc->drop_progress,\n\t\t\t\t\t      path->slots[wc->drop_level]);\n\t\t}\n\t\tbtrfs_cpu_key_to_disk(&root_item->drop_progress,\n\t\t\t\t      &wc->drop_progress);\n\t\tbtrfs_set_root_drop_level(root_item, wc->drop_level);\n\n\t\tBUG_ON(wc->level == 0);\n\t\tif (btrfs_should_end_transaction(trans) ||\n\t\t    (!for_reloc && btrfs_need_cleaner_sleep(fs_info))) {\n\t\t\tret = btrfs_update_root(trans, tree_root,\n\t\t\t\t\t\t&root->root_key,\n\t\t\t\t\t\troot_item);\n\t\t\tif (ret) {\n\t\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\t\terr = ret;\n\t\t\t\tgoto out_end_trans;\n\t\t\t}\n\n\t\t\tif (!is_reloc_root)\n\t\t\t\tbtrfs_set_last_root_drop_gen(fs_info, trans->transid);\n\n\t\t\tbtrfs_end_transaction_throttle(trans);\n\t\t\tif (!for_reloc && btrfs_need_cleaner_sleep(fs_info)) {\n\t\t\t\tbtrfs_debug(fs_info,\n\t\t\t\t\t    \"drop snapshot early exit\");\n\t\t\t\terr = -EAGAIN;\n\t\t\t\tgoto out_free;\n\t\t\t}\n\n\t\t        \n\t\t\tif (for_reloc)\n\t\t\t\ttrans = btrfs_join_transaction(tree_root);\n\t\t\telse\n\t\t\t\ttrans = btrfs_start_transaction(tree_root, 0);\n\t\t\tif (IS_ERR(trans)) {\n\t\t\t\terr = PTR_ERR(trans);\n\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t}\n\t}\n\tbtrfs_release_path(path);\n\tif (err)\n\t\tgoto out_end_trans;\n\n\tret = btrfs_del_root(trans, &root->root_key);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\terr = ret;\n\t\tgoto out_end_trans;\n\t}\n\n\tif (!is_reloc_root) {\n\t\tret = btrfs_find_root(tree_root, &root->root_key, path,\n\t\t\t\t      NULL, NULL);\n\t\tif (ret < 0) {\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\terr = ret;\n\t\t\tgoto out_end_trans;\n\t\t} else if (ret > 0) {\n\t\t\t \n\t\t\tbtrfs_del_orphan_item(trans, tree_root,\n\t\t\t\t\t      root->root_key.objectid);\n\t\t}\n\t}\n\n\t \n\tbtrfs_qgroup_convert_reserved_meta(root, INT_MAX);\n\tbtrfs_qgroup_free_meta_all_pertrans(root);\n\n\tif (test_bit(BTRFS_ROOT_IN_RADIX, &root->state))\n\t\tbtrfs_add_dropped_root(trans, root);\n\telse\n\t\tbtrfs_put_root(root);\n\troot_dropped = true;\nout_end_trans:\n\tif (!is_reloc_root)\n\t\tbtrfs_set_last_root_drop_gen(fs_info, trans->transid);\n\n\tbtrfs_end_transaction_throttle(trans);\nout_free:\n\tkfree(wc);\n\tbtrfs_free_path(path);\nout:\n\t \n\tif (!err && unfinished_drop)\n\t\tbtrfs_maybe_wake_unfinished_drop(fs_info);\n\n\t \n\tif (!for_reloc && !root_dropped)\n\t\tbtrfs_add_dead_root(root);\n\treturn err;\n}\n\n \nint btrfs_drop_subtree(struct btrfs_trans_handle *trans,\n\t\t\tstruct btrfs_root *root,\n\t\t\tstruct extent_buffer *node,\n\t\t\tstruct extent_buffer *parent)\n{\n\tstruct btrfs_fs_info *fs_info = root->fs_info;\n\tstruct btrfs_path *path;\n\tstruct walk_control *wc;\n\tint level;\n\tint parent_level;\n\tint ret = 0;\n\tint wret;\n\n\tBUG_ON(root->root_key.objectid != BTRFS_TREE_RELOC_OBJECTID);\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\twc = kzalloc(sizeof(*wc), GFP_NOFS);\n\tif (!wc) {\n\t\tbtrfs_free_path(path);\n\t\treturn -ENOMEM;\n\t}\n\n\tbtrfs_assert_tree_write_locked(parent);\n\tparent_level = btrfs_header_level(parent);\n\tatomic_inc(&parent->refs);\n\tpath->nodes[parent_level] = parent;\n\tpath->slots[parent_level] = btrfs_header_nritems(parent);\n\n\tbtrfs_assert_tree_write_locked(node);\n\tlevel = btrfs_header_level(node);\n\tpath->nodes[level] = node;\n\tpath->slots[level] = 0;\n\tpath->locks[level] = BTRFS_WRITE_LOCK;\n\n\twc->refs[parent_level] = 1;\n\twc->flags[parent_level] = BTRFS_BLOCK_FLAG_FULL_BACKREF;\n\twc->level = level;\n\twc->shared_level = -1;\n\twc->stage = DROP_REFERENCE;\n\twc->update_ref = 0;\n\twc->keep_locks = 1;\n\twc->reada_count = BTRFS_NODEPTRS_PER_BLOCK(fs_info);\n\n\twhile (1) {\n\t\twret = walk_down_tree(trans, root, path, wc);\n\t\tif (wret < 0) {\n\t\t\tret = wret;\n\t\t\tbreak;\n\t\t}\n\n\t\twret = walk_up_tree(trans, root, path, wc, parent_level);\n\t\tif (wret < 0)\n\t\t\tret = wret;\n\t\tif (wret != 0)\n\t\t\tbreak;\n\t}\n\n\tkfree(wc);\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nint btrfs_error_unpin_extent_range(struct btrfs_fs_info *fs_info,\n\t\t\t\t   u64 start, u64 end)\n{\n\treturn unpin_extent_range(fs_info, start, end, false);\n}\n\n \nstatic int btrfs_trim_free_extents(struct btrfs_device *device, u64 *trimmed)\n{\n\tu64 start = BTRFS_DEVICE_RANGE_RESERVED, len = 0, end = 0;\n\tint ret;\n\n\t*trimmed = 0;\n\n\t \n\tif (!bdev_max_discard_sectors(device->bdev))\n\t\treturn 0;\n\n\t \n\tif (!test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state))\n\t\treturn 0;\n\n\t \n\tif (device->total_bytes <= device->bytes_used)\n\t\treturn 0;\n\n\tret = 0;\n\n\twhile (1) {\n\t\tstruct btrfs_fs_info *fs_info = device->fs_info;\n\t\tu64 bytes;\n\n\t\tret = mutex_lock_interruptible(&fs_info->chunk_mutex);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tfind_first_clear_extent_bit(&device->alloc_state, start,\n\t\t\t\t\t    &start, &end,\n\t\t\t\t\t    CHUNK_TRIMMED | CHUNK_ALLOCATED);\n\n\t\t \n\t\tif (start > device->total_bytes) {\n\t\t\tWARN_ON(IS_ENABLED(CONFIG_BTRFS_DEBUG));\n\t\t\tbtrfs_warn_in_rcu(fs_info,\n\"ignoring attempt to trim beyond device size: offset %llu length %llu device %s device size %llu\",\n\t\t\t\t\t  start, end - start + 1,\n\t\t\t\t\t  btrfs_dev_name(device),\n\t\t\t\t\t  device->total_bytes);\n\t\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tstart = max_t(u64, start, BTRFS_DEVICE_RANGE_RESERVED);\n\n\t\t \n\t\tend = min(end, device->total_bytes - 1);\n\n\t\tlen = end - start + 1;\n\n\t\t \n\t\tif (!len) {\n\t\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tret = btrfs_issue_discard(device->bdev, start, len,\n\t\t\t\t\t  &bytes);\n\t\tif (!ret)\n\t\t\tset_extent_bit(&device->alloc_state, start,\n\t\t\t\t       start + bytes - 1, CHUNK_TRIMMED, NULL);\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tstart += len;\n\t\t*trimmed += bytes;\n\n\t\tif (fatal_signal_pending(current)) {\n\t\t\tret = -ERESTARTSYS;\n\t\t\tbreak;\n\t\t}\n\n\t\tcond_resched();\n\t}\n\n\treturn ret;\n}\n\n \nint btrfs_trim_fs(struct btrfs_fs_info *fs_info, struct fstrim_range *range)\n{\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tstruct btrfs_block_group *cache = NULL;\n\tstruct btrfs_device *device;\n\tu64 group_trimmed;\n\tu64 range_end = U64_MAX;\n\tu64 start;\n\tu64 end;\n\tu64 trimmed = 0;\n\tu64 bg_failed = 0;\n\tu64 dev_failed = 0;\n\tint bg_ret = 0;\n\tint dev_ret = 0;\n\tint ret = 0;\n\n\tif (range->start == U64_MAX)\n\t\treturn -EINVAL;\n\n\t \n\tif (range->len != U64_MAX &&\n\t    check_add_overflow(range->start, range->len, &range_end))\n\t\treturn -EINVAL;\n\n\tcache = btrfs_lookup_first_block_group(fs_info, range->start);\n\tfor (; cache; cache = btrfs_next_block_group(cache)) {\n\t\tif (cache->start >= range_end) {\n\t\t\tbtrfs_put_block_group(cache);\n\t\t\tbreak;\n\t\t}\n\n\t\tstart = max(range->start, cache->start);\n\t\tend = min(range_end, cache->start + cache->length);\n\n\t\tif (end - start >= range->minlen) {\n\t\t\tif (!btrfs_block_group_done(cache)) {\n\t\t\t\tret = btrfs_cache_block_group(cache, true);\n\t\t\t\tif (ret) {\n\t\t\t\t\tbg_failed++;\n\t\t\t\t\tbg_ret = ret;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t}\n\t\t\tret = btrfs_trim_block_group(cache,\n\t\t\t\t\t\t     &group_trimmed,\n\t\t\t\t\t\t     start,\n\t\t\t\t\t\t     end,\n\t\t\t\t\t\t     range->minlen);\n\n\t\t\ttrimmed += group_trimmed;\n\t\t\tif (ret) {\n\t\t\t\tbg_failed++;\n\t\t\t\tbg_ret = ret;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (bg_failed)\n\t\tbtrfs_warn(fs_info,\n\t\t\t\"failed to trim %llu block group(s), last error %d\",\n\t\t\tbg_failed, bg_ret);\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_for_each_entry(device, &fs_devices->devices, dev_list) {\n\t\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state))\n\t\t\tcontinue;\n\n\t\tret = btrfs_trim_free_extents(device, &group_trimmed);\n\t\tif (ret) {\n\t\t\tdev_failed++;\n\t\t\tdev_ret = ret;\n\t\t\tbreak;\n\t\t}\n\n\t\ttrimmed += group_trimmed;\n\t}\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\tif (dev_failed)\n\t\tbtrfs_warn(fs_info,\n\t\t\t\"failed to trim %llu device(s), last error %d\",\n\t\t\tdev_failed, dev_ret);\n\trange->len = trimmed;\n\tif (bg_ret)\n\t\treturn bg_ret;\n\treturn dev_ret;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}