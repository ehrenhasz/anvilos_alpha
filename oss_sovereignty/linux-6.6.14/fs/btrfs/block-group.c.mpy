{
  "module_name": "block-group.c",
  "hash_id": "bb6eab59b7bf0b2d848da5b92fd144090ce1aedd865d93d5c89133a1887d7b33",
  "original_prompt": "Ingested from linux-6.6.14/fs/btrfs/block-group.c",
  "human_readable_source": "\n\n#include <linux/sizes.h>\n#include <linux/list_sort.h>\n#include \"misc.h\"\n#include \"ctree.h\"\n#include \"block-group.h\"\n#include \"space-info.h\"\n#include \"disk-io.h\"\n#include \"free-space-cache.h\"\n#include \"free-space-tree.h\"\n#include \"volumes.h\"\n#include \"transaction.h\"\n#include \"ref-verify.h\"\n#include \"sysfs.h\"\n#include \"tree-log.h\"\n#include \"delalloc-space.h\"\n#include \"discard.h\"\n#include \"raid56.h\"\n#include \"zoned.h\"\n#include \"fs.h\"\n#include \"accessors.h\"\n#include \"extent-tree.h\"\n\n#ifdef CONFIG_BTRFS_DEBUG\nint btrfs_should_fragment_free_space(struct btrfs_block_group *block_group)\n{\n\tstruct btrfs_fs_info *fs_info = block_group->fs_info;\n\n\treturn (btrfs_test_opt(fs_info, FRAGMENT_METADATA) &&\n\t\tblock_group->flags & BTRFS_BLOCK_GROUP_METADATA) ||\n\t       (btrfs_test_opt(fs_info, FRAGMENT_DATA) &&\n\t\tblock_group->flags &  BTRFS_BLOCK_GROUP_DATA);\n}\n#endif\n\n \nstatic u64 get_restripe_target(struct btrfs_fs_info *fs_info, u64 flags)\n{\n\tstruct btrfs_balance_control *bctl = fs_info->balance_ctl;\n\tu64 target = 0;\n\n\tif (!bctl)\n\t\treturn 0;\n\n\tif (flags & BTRFS_BLOCK_GROUP_DATA &&\n\t    bctl->data.flags & BTRFS_BALANCE_ARGS_CONVERT) {\n\t\ttarget = BTRFS_BLOCK_GROUP_DATA | bctl->data.target;\n\t} else if (flags & BTRFS_BLOCK_GROUP_SYSTEM &&\n\t\t   bctl->sys.flags & BTRFS_BALANCE_ARGS_CONVERT) {\n\t\ttarget = BTRFS_BLOCK_GROUP_SYSTEM | bctl->sys.target;\n\t} else if (flags & BTRFS_BLOCK_GROUP_METADATA &&\n\t\t   bctl->meta.flags & BTRFS_BALANCE_ARGS_CONVERT) {\n\t\ttarget = BTRFS_BLOCK_GROUP_METADATA | bctl->meta.target;\n\t}\n\n\treturn target;\n}\n\n \nstatic u64 btrfs_reduce_alloc_profile(struct btrfs_fs_info *fs_info, u64 flags)\n{\n\tu64 num_devices = fs_info->fs_devices->rw_devices;\n\tu64 target;\n\tu64 raid_type;\n\tu64 allowed = 0;\n\n\t \n\tspin_lock(&fs_info->balance_lock);\n\ttarget = get_restripe_target(fs_info, flags);\n\tif (target) {\n\t\tspin_unlock(&fs_info->balance_lock);\n\t\treturn extended_to_chunk(target);\n\t}\n\tspin_unlock(&fs_info->balance_lock);\n\n\t \n\tfor (raid_type = 0; raid_type < BTRFS_NR_RAID_TYPES; raid_type++) {\n\t\tif (num_devices >= btrfs_raid_array[raid_type].devs_min)\n\t\t\tallowed |= btrfs_raid_array[raid_type].bg_flag;\n\t}\n\tallowed &= flags;\n\n\t \n\tif (allowed & BTRFS_BLOCK_GROUP_RAID1C4)\n\t\tallowed = BTRFS_BLOCK_GROUP_RAID1C4;\n\telse if (allowed & BTRFS_BLOCK_GROUP_RAID6)\n\t\tallowed = BTRFS_BLOCK_GROUP_RAID6;\n\telse if (allowed & BTRFS_BLOCK_GROUP_RAID1C3)\n\t\tallowed = BTRFS_BLOCK_GROUP_RAID1C3;\n\telse if (allowed & BTRFS_BLOCK_GROUP_RAID5)\n\t\tallowed = BTRFS_BLOCK_GROUP_RAID5;\n\telse if (allowed & BTRFS_BLOCK_GROUP_RAID10)\n\t\tallowed = BTRFS_BLOCK_GROUP_RAID10;\n\telse if (allowed & BTRFS_BLOCK_GROUP_RAID1)\n\t\tallowed = BTRFS_BLOCK_GROUP_RAID1;\n\telse if (allowed & BTRFS_BLOCK_GROUP_DUP)\n\t\tallowed = BTRFS_BLOCK_GROUP_DUP;\n\telse if (allowed & BTRFS_BLOCK_GROUP_RAID0)\n\t\tallowed = BTRFS_BLOCK_GROUP_RAID0;\n\n\tflags &= ~BTRFS_BLOCK_GROUP_PROFILE_MASK;\n\n\treturn extended_to_chunk(flags | allowed);\n}\n\nu64 btrfs_get_alloc_profile(struct btrfs_fs_info *fs_info, u64 orig_flags)\n{\n\tunsigned seq;\n\tu64 flags;\n\n\tdo {\n\t\tflags = orig_flags;\n\t\tseq = read_seqbegin(&fs_info->profiles_lock);\n\n\t\tif (flags & BTRFS_BLOCK_GROUP_DATA)\n\t\t\tflags |= fs_info->avail_data_alloc_bits;\n\t\telse if (flags & BTRFS_BLOCK_GROUP_SYSTEM)\n\t\t\tflags |= fs_info->avail_system_alloc_bits;\n\t\telse if (flags & BTRFS_BLOCK_GROUP_METADATA)\n\t\t\tflags |= fs_info->avail_metadata_alloc_bits;\n\t} while (read_seqretry(&fs_info->profiles_lock, seq));\n\n\treturn btrfs_reduce_alloc_profile(fs_info, flags);\n}\n\nvoid btrfs_get_block_group(struct btrfs_block_group *cache)\n{\n\trefcount_inc(&cache->refs);\n}\n\nvoid btrfs_put_block_group(struct btrfs_block_group *cache)\n{\n\tif (refcount_dec_and_test(&cache->refs)) {\n\t\tWARN_ON(cache->pinned > 0);\n\t\t \n\t\tif (!(cache->flags & BTRFS_BLOCK_GROUP_METADATA) ||\n\t\t    !BTRFS_FS_LOG_CLEANUP_ERROR(cache->fs_info))\n\t\t\tWARN_ON(cache->reserved > 0);\n\n\t\t \n\t\tif (WARN_ON(!list_empty(&cache->discard_list)))\n\t\t\tbtrfs_discard_cancel_work(&cache->fs_info->discard_ctl,\n\t\t\t\t\t\t  cache);\n\n\t\tkfree(cache->free_space_ctl);\n\t\tkfree(cache->physical_map);\n\t\tkfree(cache);\n\t}\n}\n\n \nstatic int btrfs_add_block_group_cache(struct btrfs_fs_info *info,\n\t\t\t\t       struct btrfs_block_group *block_group)\n{\n\tstruct rb_node **p;\n\tstruct rb_node *parent = NULL;\n\tstruct btrfs_block_group *cache;\n\tbool leftmost = true;\n\n\tASSERT(block_group->length != 0);\n\n\twrite_lock(&info->block_group_cache_lock);\n\tp = &info->block_group_cache_tree.rb_root.rb_node;\n\n\twhile (*p) {\n\t\tparent = *p;\n\t\tcache = rb_entry(parent, struct btrfs_block_group, cache_node);\n\t\tif (block_group->start < cache->start) {\n\t\t\tp = &(*p)->rb_left;\n\t\t} else if (block_group->start > cache->start) {\n\t\t\tp = &(*p)->rb_right;\n\t\t\tleftmost = false;\n\t\t} else {\n\t\t\twrite_unlock(&info->block_group_cache_lock);\n\t\t\treturn -EEXIST;\n\t\t}\n\t}\n\n\trb_link_node(&block_group->cache_node, parent, p);\n\trb_insert_color_cached(&block_group->cache_node,\n\t\t\t       &info->block_group_cache_tree, leftmost);\n\n\twrite_unlock(&info->block_group_cache_lock);\n\n\treturn 0;\n}\n\n \nstatic struct btrfs_block_group *block_group_cache_tree_search(\n\t\tstruct btrfs_fs_info *info, u64 bytenr, int contains)\n{\n\tstruct btrfs_block_group *cache, *ret = NULL;\n\tstruct rb_node *n;\n\tu64 end, start;\n\n\tread_lock(&info->block_group_cache_lock);\n\tn = info->block_group_cache_tree.rb_root.rb_node;\n\n\twhile (n) {\n\t\tcache = rb_entry(n, struct btrfs_block_group, cache_node);\n\t\tend = cache->start + cache->length - 1;\n\t\tstart = cache->start;\n\n\t\tif (bytenr < start) {\n\t\t\tif (!contains && (!ret || start < ret->start))\n\t\t\t\tret = cache;\n\t\t\tn = n->rb_left;\n\t\t} else if (bytenr > start) {\n\t\t\tif (contains && bytenr <= end) {\n\t\t\t\tret = cache;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tn = n->rb_right;\n\t\t} else {\n\t\t\tret = cache;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (ret)\n\t\tbtrfs_get_block_group(ret);\n\tread_unlock(&info->block_group_cache_lock);\n\n\treturn ret;\n}\n\n \nstruct btrfs_block_group *btrfs_lookup_first_block_group(\n\t\tstruct btrfs_fs_info *info, u64 bytenr)\n{\n\treturn block_group_cache_tree_search(info, bytenr, 0);\n}\n\n \nstruct btrfs_block_group *btrfs_lookup_block_group(\n\t\tstruct btrfs_fs_info *info, u64 bytenr)\n{\n\treturn block_group_cache_tree_search(info, bytenr, 1);\n}\n\nstruct btrfs_block_group *btrfs_next_block_group(\n\t\tstruct btrfs_block_group *cache)\n{\n\tstruct btrfs_fs_info *fs_info = cache->fs_info;\n\tstruct rb_node *node;\n\n\tread_lock(&fs_info->block_group_cache_lock);\n\n\t \n\tif (RB_EMPTY_NODE(&cache->cache_node)) {\n\t\tconst u64 next_bytenr = cache->start + cache->length;\n\n\t\tread_unlock(&fs_info->block_group_cache_lock);\n\t\tbtrfs_put_block_group(cache);\n\t\treturn btrfs_lookup_first_block_group(fs_info, next_bytenr);\n\t}\n\tnode = rb_next(&cache->cache_node);\n\tbtrfs_put_block_group(cache);\n\tif (node) {\n\t\tcache = rb_entry(node, struct btrfs_block_group, cache_node);\n\t\tbtrfs_get_block_group(cache);\n\t} else\n\t\tcache = NULL;\n\tread_unlock(&fs_info->block_group_cache_lock);\n\treturn cache;\n}\n\n \nstruct btrfs_block_group *btrfs_inc_nocow_writers(struct btrfs_fs_info *fs_info,\n\t\t\t\t\t\t  u64 bytenr)\n{\n\tstruct btrfs_block_group *bg;\n\tbool can_nocow = true;\n\n\tbg = btrfs_lookup_block_group(fs_info, bytenr);\n\tif (!bg)\n\t\treturn NULL;\n\n\tspin_lock(&bg->lock);\n\tif (bg->ro)\n\t\tcan_nocow = false;\n\telse\n\t\tatomic_inc(&bg->nocow_writers);\n\tspin_unlock(&bg->lock);\n\n\tif (!can_nocow) {\n\t\tbtrfs_put_block_group(bg);\n\t\treturn NULL;\n\t}\n\n\t \n\treturn bg;\n}\n\n \nvoid btrfs_dec_nocow_writers(struct btrfs_block_group *bg)\n{\n\tif (atomic_dec_and_test(&bg->nocow_writers))\n\t\twake_up_var(&bg->nocow_writers);\n\n\t \n\tbtrfs_put_block_group(bg);\n}\n\nvoid btrfs_wait_nocow_writers(struct btrfs_block_group *bg)\n{\n\twait_var_event(&bg->nocow_writers, !atomic_read(&bg->nocow_writers));\n}\n\nvoid btrfs_dec_block_group_reservations(struct btrfs_fs_info *fs_info,\n\t\t\t\t\tconst u64 start)\n{\n\tstruct btrfs_block_group *bg;\n\n\tbg = btrfs_lookup_block_group(fs_info, start);\n\tASSERT(bg);\n\tif (atomic_dec_and_test(&bg->reservations))\n\t\twake_up_var(&bg->reservations);\n\tbtrfs_put_block_group(bg);\n}\n\nvoid btrfs_wait_block_group_reservations(struct btrfs_block_group *bg)\n{\n\tstruct btrfs_space_info *space_info = bg->space_info;\n\n\tASSERT(bg->ro);\n\n\tif (!(bg->flags & BTRFS_BLOCK_GROUP_DATA))\n\t\treturn;\n\n\t \n\tdown_write(&space_info->groups_sem);\n\tup_write(&space_info->groups_sem);\n\n\twait_var_event(&bg->reservations, !atomic_read(&bg->reservations));\n}\n\nstruct btrfs_caching_control *btrfs_get_caching_control(\n\t\tstruct btrfs_block_group *cache)\n{\n\tstruct btrfs_caching_control *ctl;\n\n\tspin_lock(&cache->lock);\n\tif (!cache->caching_ctl) {\n\t\tspin_unlock(&cache->lock);\n\t\treturn NULL;\n\t}\n\n\tctl = cache->caching_ctl;\n\trefcount_inc(&ctl->count);\n\tspin_unlock(&cache->lock);\n\treturn ctl;\n}\n\nvoid btrfs_put_caching_control(struct btrfs_caching_control *ctl)\n{\n\tif (refcount_dec_and_test(&ctl->count))\n\t\tkfree(ctl);\n}\n\n \nvoid btrfs_wait_block_group_cache_progress(struct btrfs_block_group *cache,\n\t\t\t\t\t   u64 num_bytes)\n{\n\tstruct btrfs_caching_control *caching_ctl;\n\tint progress;\n\n\tcaching_ctl = btrfs_get_caching_control(cache);\n\tif (!caching_ctl)\n\t\treturn;\n\n\t \n\tprogress = atomic_read(&caching_ctl->progress);\n\n\twait_event(caching_ctl->wait, btrfs_block_group_done(cache) ||\n\t\t   (progress != atomic_read(&caching_ctl->progress) &&\n\t\t    (cache->free_space_ctl->free_space >= num_bytes)));\n\n\tbtrfs_put_caching_control(caching_ctl);\n}\n\nstatic int btrfs_caching_ctl_wait_done(struct btrfs_block_group *cache,\n\t\t\t\t       struct btrfs_caching_control *caching_ctl)\n{\n\twait_event(caching_ctl->wait, btrfs_block_group_done(cache));\n\treturn cache->cached == BTRFS_CACHE_ERROR ? -EIO : 0;\n}\n\nstatic int btrfs_wait_block_group_cache_done(struct btrfs_block_group *cache)\n{\n\tstruct btrfs_caching_control *caching_ctl;\n\tint ret;\n\n\tcaching_ctl = btrfs_get_caching_control(cache);\n\tif (!caching_ctl)\n\t\treturn (cache->cached == BTRFS_CACHE_ERROR) ? -EIO : 0;\n\tret = btrfs_caching_ctl_wait_done(cache, caching_ctl);\n\tbtrfs_put_caching_control(caching_ctl);\n\treturn ret;\n}\n\n#ifdef CONFIG_BTRFS_DEBUG\nstatic void fragment_free_space(struct btrfs_block_group *block_group)\n{\n\tstruct btrfs_fs_info *fs_info = block_group->fs_info;\n\tu64 start = block_group->start;\n\tu64 len = block_group->length;\n\tu64 chunk = block_group->flags & BTRFS_BLOCK_GROUP_METADATA ?\n\t\tfs_info->nodesize : fs_info->sectorsize;\n\tu64 step = chunk << 1;\n\n\twhile (len > chunk) {\n\t\tbtrfs_remove_free_space(block_group, start, chunk);\n\t\tstart += step;\n\t\tif (len < step)\n\t\t\tlen = 0;\n\t\telse\n\t\t\tlen -= step;\n\t}\n}\n#endif\n\n \nint btrfs_add_new_free_space(struct btrfs_block_group *block_group, u64 start,\n\t\t\t     u64 end, u64 *total_added_ret)\n{\n\tstruct btrfs_fs_info *info = block_group->fs_info;\n\tu64 extent_start, extent_end, size;\n\tint ret;\n\n\tif (total_added_ret)\n\t\t*total_added_ret = 0;\n\n\twhile (start < end) {\n\t\tif (!find_first_extent_bit(&info->excluded_extents, start,\n\t\t\t\t\t   &extent_start, &extent_end,\n\t\t\t\t\t   EXTENT_DIRTY | EXTENT_UPTODATE,\n\t\t\t\t\t   NULL))\n\t\t\tbreak;\n\n\t\tif (extent_start <= start) {\n\t\t\tstart = extent_end + 1;\n\t\t} else if (extent_start > start && extent_start < end) {\n\t\t\tsize = extent_start - start;\n\t\t\tret = btrfs_add_free_space_async_trimmed(block_group,\n\t\t\t\t\t\t\t\t start, size);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t\tif (total_added_ret)\n\t\t\t\t*total_added_ret += size;\n\t\t\tstart = extent_end + 1;\n\t\t} else {\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (start < end) {\n\t\tsize = end - start;\n\t\tret = btrfs_add_free_space_async_trimmed(block_group, start,\n\t\t\t\t\t\t\t size);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tif (total_added_ret)\n\t\t\t*total_added_ret += size;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int sample_block_group_extent_item(struct btrfs_caching_control *caching_ctl,\n\t\t\t\t\t  struct btrfs_block_group *block_group,\n\t\t\t\t\t  int index, int max_index,\n\t\t\t\t\t  struct btrfs_key *found_key)\n{\n\tstruct btrfs_fs_info *fs_info = block_group->fs_info;\n\tstruct btrfs_root *extent_root;\n\tu64 search_offset;\n\tu64 search_end = block_group->start + block_group->length;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key search_key;\n\tint ret = 0;\n\n\tASSERT(index >= 0);\n\tASSERT(index <= max_index);\n\tASSERT(max_index > 0);\n\tlockdep_assert_held(&caching_ctl->mutex);\n\tlockdep_assert_held_read(&fs_info->commit_root_sem);\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\textent_root = btrfs_extent_root(fs_info, max_t(u64, block_group->start,\n\t\t\t\t\t\t       BTRFS_SUPER_INFO_OFFSET));\n\n\tpath->skip_locking = 1;\n\tpath->search_commit_root = 1;\n\tpath->reada = READA_FORWARD;\n\n\tsearch_offset = index * div_u64(block_group->length, max_index);\n\tsearch_key.objectid = block_group->start + search_offset;\n\tsearch_key.type = BTRFS_EXTENT_ITEM_KEY;\n\tsearch_key.offset = 0;\n\n\tbtrfs_for_each_slot(extent_root, &search_key, found_key, path, ret) {\n\t\t \n\t\tif (found_key->type == BTRFS_EXTENT_ITEM_KEY &&\n\t\t    found_key->objectid >= block_group->start &&\n\t\t    found_key->objectid + found_key->offset <= search_end)\n\t\t\tbreak;\n\n\t\t \n\t\tif (found_key->objectid >= search_end) {\n\t\t\tret = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tlockdep_assert_held(&caching_ctl->mutex);\n\tlockdep_assert_held_read(&fs_info->commit_root_sem);\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n \nstatic int load_block_group_size_class(struct btrfs_caching_control *caching_ctl,\n\t\t\t\t       struct btrfs_block_group *block_group)\n{\n\tstruct btrfs_fs_info *fs_info = block_group->fs_info;\n\tstruct btrfs_key key;\n\tint i;\n\tu64 min_size = block_group->length;\n\tenum btrfs_block_group_size_class size_class = BTRFS_BG_SZ_NONE;\n\tint ret;\n\n\tif (!btrfs_block_group_should_use_size_class(block_group))\n\t\treturn 0;\n\n\tlockdep_assert_held(&caching_ctl->mutex);\n\tlockdep_assert_held_read(&fs_info->commit_root_sem);\n\tfor (i = 0; i < 5; ++i) {\n\t\tret = sample_block_group_extent_item(caching_ctl, block_group, i, 5, &key);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\tif (ret > 0)\n\t\t\tcontinue;\n\t\tmin_size = min_t(u64, min_size, key.offset);\n\t\tsize_class = btrfs_calc_block_group_size_class(min_size);\n\t}\n\tif (size_class != BTRFS_BG_SZ_NONE) {\n\t\tspin_lock(&block_group->lock);\n\t\tblock_group->size_class = size_class;\n\t\tspin_unlock(&block_group->lock);\n\t}\nout:\n\treturn ret;\n}\n\nstatic int load_extent_tree_free(struct btrfs_caching_control *caching_ctl)\n{\n\tstruct btrfs_block_group *block_group = caching_ctl->block_group;\n\tstruct btrfs_fs_info *fs_info = block_group->fs_info;\n\tstruct btrfs_root *extent_root;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_key key;\n\tu64 total_found = 0;\n\tu64 last = 0;\n\tu32 nritems;\n\tint ret;\n\tbool wakeup = true;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tlast = max_t(u64, block_group->start, BTRFS_SUPER_INFO_OFFSET);\n\textent_root = btrfs_extent_root(fs_info, last);\n\n#ifdef CONFIG_BTRFS_DEBUG\n\t \n\tif (btrfs_should_fragment_free_space(block_group))\n\t\twakeup = false;\n#endif\n\t \n\tpath->skip_locking = 1;\n\tpath->search_commit_root = 1;\n\tpath->reada = READA_FORWARD;\n\n\tkey.objectid = last;\n\tkey.offset = 0;\n\tkey.type = BTRFS_EXTENT_ITEM_KEY;\n\nnext:\n\tret = btrfs_search_slot(NULL, extent_root, &key, path, 0, 0);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tleaf = path->nodes[0];\n\tnritems = btrfs_header_nritems(leaf);\n\n\twhile (1) {\n\t\tif (btrfs_fs_closing(fs_info) > 1) {\n\t\t\tlast = (u64)-1;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (path->slots[0] < nritems) {\n\t\t\tbtrfs_item_key_to_cpu(leaf, &key, path->slots[0]);\n\t\t} else {\n\t\t\tret = btrfs_find_next_key(extent_root, path, &key, 0, 0);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\n\t\t\tif (need_resched() ||\n\t\t\t    rwsem_is_contended(&fs_info->commit_root_sem)) {\n\t\t\t\tbtrfs_release_path(path);\n\t\t\t\tup_read(&fs_info->commit_root_sem);\n\t\t\t\tmutex_unlock(&caching_ctl->mutex);\n\t\t\t\tcond_resched();\n\t\t\t\tmutex_lock(&caching_ctl->mutex);\n\t\t\t\tdown_read(&fs_info->commit_root_sem);\n\t\t\t\tgoto next;\n\t\t\t}\n\n\t\t\tret = btrfs_next_leaf(extent_root, path);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out;\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t\tleaf = path->nodes[0];\n\t\t\tnritems = btrfs_header_nritems(leaf);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (key.objectid < last) {\n\t\t\tkey.objectid = last;\n\t\t\tkey.offset = 0;\n\t\t\tkey.type = BTRFS_EXTENT_ITEM_KEY;\n\t\t\tbtrfs_release_path(path);\n\t\t\tgoto next;\n\t\t}\n\n\t\tif (key.objectid < block_group->start) {\n\t\t\tpath->slots[0]++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (key.objectid >= block_group->start + block_group->length)\n\t\t\tbreak;\n\n\t\tif (key.type == BTRFS_EXTENT_ITEM_KEY ||\n\t\t    key.type == BTRFS_METADATA_ITEM_KEY) {\n\t\t\tu64 space_added;\n\n\t\t\tret = btrfs_add_new_free_space(block_group, last,\n\t\t\t\t\t\t       key.objectid, &space_added);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t\ttotal_found += space_added;\n\t\t\tif (key.type == BTRFS_METADATA_ITEM_KEY)\n\t\t\t\tlast = key.objectid +\n\t\t\t\t\tfs_info->nodesize;\n\t\t\telse\n\t\t\t\tlast = key.objectid + key.offset;\n\n\t\t\tif (total_found > CACHING_CTL_WAKE_UP) {\n\t\t\t\ttotal_found = 0;\n\t\t\t\tif (wakeup) {\n\t\t\t\t\tatomic_inc(&caching_ctl->progress);\n\t\t\t\t\twake_up(&caching_ctl->wait);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tpath->slots[0]++;\n\t}\n\n\tret = btrfs_add_new_free_space(block_group, last,\n\t\t\t\t       block_group->start + block_group->length,\n\t\t\t\t       NULL);\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nstatic inline void btrfs_free_excluded_extents(const struct btrfs_block_group *bg)\n{\n\tclear_extent_bits(&bg->fs_info->excluded_extents, bg->start,\n\t\t\t  bg->start + bg->length - 1, EXTENT_UPTODATE);\n}\n\nstatic noinline void caching_thread(struct btrfs_work *work)\n{\n\tstruct btrfs_block_group *block_group;\n\tstruct btrfs_fs_info *fs_info;\n\tstruct btrfs_caching_control *caching_ctl;\n\tint ret;\n\n\tcaching_ctl = container_of(work, struct btrfs_caching_control, work);\n\tblock_group = caching_ctl->block_group;\n\tfs_info = block_group->fs_info;\n\n\tmutex_lock(&caching_ctl->mutex);\n\tdown_read(&fs_info->commit_root_sem);\n\n\tload_block_group_size_class(caching_ctl, block_group);\n\tif (btrfs_test_opt(fs_info, SPACE_CACHE)) {\n\t\tret = load_free_space_cache(block_group);\n\t\tif (ret == 1) {\n\t\t\tret = 0;\n\t\t\tgoto done;\n\t\t}\n\n\t\t \n\t\tspin_lock(&block_group->lock);\n\t\tblock_group->cached = BTRFS_CACHE_STARTED;\n\t\tspin_unlock(&block_group->lock);\n\t\twake_up(&caching_ctl->wait);\n\t}\n\n\t \n\tif (btrfs_fs_compat_ro(fs_info, FREE_SPACE_TREE) &&\n\t    !(test_bit(BTRFS_FS_FREE_SPACE_TREE_UNTRUSTED, &fs_info->flags)))\n\t\tret = load_free_space_tree(caching_ctl);\n\telse\n\t\tret = load_extent_tree_free(caching_ctl);\ndone:\n\tspin_lock(&block_group->lock);\n\tblock_group->caching_ctl = NULL;\n\tblock_group->cached = ret ? BTRFS_CACHE_ERROR : BTRFS_CACHE_FINISHED;\n\tspin_unlock(&block_group->lock);\n\n#ifdef CONFIG_BTRFS_DEBUG\n\tif (btrfs_should_fragment_free_space(block_group)) {\n\t\tu64 bytes_used;\n\n\t\tspin_lock(&block_group->space_info->lock);\n\t\tspin_lock(&block_group->lock);\n\t\tbytes_used = block_group->length - block_group->used;\n\t\tblock_group->space_info->bytes_used += bytes_used >> 1;\n\t\tspin_unlock(&block_group->lock);\n\t\tspin_unlock(&block_group->space_info->lock);\n\t\tfragment_free_space(block_group);\n\t}\n#endif\n\n\tup_read(&fs_info->commit_root_sem);\n\tbtrfs_free_excluded_extents(block_group);\n\tmutex_unlock(&caching_ctl->mutex);\n\n\twake_up(&caching_ctl->wait);\n\n\tbtrfs_put_caching_control(caching_ctl);\n\tbtrfs_put_block_group(block_group);\n}\n\nint btrfs_cache_block_group(struct btrfs_block_group *cache, bool wait)\n{\n\tstruct btrfs_fs_info *fs_info = cache->fs_info;\n\tstruct btrfs_caching_control *caching_ctl = NULL;\n\tint ret = 0;\n\n\t \n\tif (btrfs_is_zoned(fs_info))\n\t\treturn 0;\n\n\tcaching_ctl = kzalloc(sizeof(*caching_ctl), GFP_NOFS);\n\tif (!caching_ctl)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&caching_ctl->list);\n\tmutex_init(&caching_ctl->mutex);\n\tinit_waitqueue_head(&caching_ctl->wait);\n\tcaching_ctl->block_group = cache;\n\trefcount_set(&caching_ctl->count, 2);\n\tatomic_set(&caching_ctl->progress, 0);\n\tbtrfs_init_work(&caching_ctl->work, caching_thread, NULL, NULL);\n\n\tspin_lock(&cache->lock);\n\tif (cache->cached != BTRFS_CACHE_NO) {\n\t\tkfree(caching_ctl);\n\n\t\tcaching_ctl = cache->caching_ctl;\n\t\tif (caching_ctl)\n\t\t\trefcount_inc(&caching_ctl->count);\n\t\tspin_unlock(&cache->lock);\n\t\tgoto out;\n\t}\n\tWARN_ON(cache->caching_ctl);\n\tcache->caching_ctl = caching_ctl;\n\tcache->cached = BTRFS_CACHE_STARTED;\n\tspin_unlock(&cache->lock);\n\n\twrite_lock(&fs_info->block_group_cache_lock);\n\trefcount_inc(&caching_ctl->count);\n\tlist_add_tail(&caching_ctl->list, &fs_info->caching_block_groups);\n\twrite_unlock(&fs_info->block_group_cache_lock);\n\n\tbtrfs_get_block_group(cache);\n\n\tbtrfs_queue_work(fs_info->caching_workers, &caching_ctl->work);\nout:\n\tif (wait && caching_ctl)\n\t\tret = btrfs_caching_ctl_wait_done(cache, caching_ctl);\n\tif (caching_ctl)\n\t\tbtrfs_put_caching_control(caching_ctl);\n\n\treturn ret;\n}\n\nstatic void clear_avail_alloc_bits(struct btrfs_fs_info *fs_info, u64 flags)\n{\n\tu64 extra_flags = chunk_to_extended(flags) &\n\t\t\t\tBTRFS_EXTENDED_PROFILE_MASK;\n\n\twrite_seqlock(&fs_info->profiles_lock);\n\tif (flags & BTRFS_BLOCK_GROUP_DATA)\n\t\tfs_info->avail_data_alloc_bits &= ~extra_flags;\n\tif (flags & BTRFS_BLOCK_GROUP_METADATA)\n\t\tfs_info->avail_metadata_alloc_bits &= ~extra_flags;\n\tif (flags & BTRFS_BLOCK_GROUP_SYSTEM)\n\t\tfs_info->avail_system_alloc_bits &= ~extra_flags;\n\twrite_sequnlock(&fs_info->profiles_lock);\n}\n\n \nstatic void clear_incompat_bg_bits(struct btrfs_fs_info *fs_info, u64 flags)\n{\n\tbool found_raid56 = false;\n\tbool found_raid1c34 = false;\n\n\tif ((flags & BTRFS_BLOCK_GROUP_RAID56_MASK) ||\n\t    (flags & BTRFS_BLOCK_GROUP_RAID1C3) ||\n\t    (flags & BTRFS_BLOCK_GROUP_RAID1C4)) {\n\t\tstruct list_head *head = &fs_info->space_info;\n\t\tstruct btrfs_space_info *sinfo;\n\n\t\tlist_for_each_entry_rcu(sinfo, head, list) {\n\t\t\tdown_read(&sinfo->groups_sem);\n\t\t\tif (!list_empty(&sinfo->block_groups[BTRFS_RAID_RAID5]))\n\t\t\t\tfound_raid56 = true;\n\t\t\tif (!list_empty(&sinfo->block_groups[BTRFS_RAID_RAID6]))\n\t\t\t\tfound_raid56 = true;\n\t\t\tif (!list_empty(&sinfo->block_groups[BTRFS_RAID_RAID1C3]))\n\t\t\t\tfound_raid1c34 = true;\n\t\t\tif (!list_empty(&sinfo->block_groups[BTRFS_RAID_RAID1C4]))\n\t\t\t\tfound_raid1c34 = true;\n\t\t\tup_read(&sinfo->groups_sem);\n\t\t}\n\t\tif (!found_raid56)\n\t\t\tbtrfs_clear_fs_incompat(fs_info, RAID56);\n\t\tif (!found_raid1c34)\n\t\t\tbtrfs_clear_fs_incompat(fs_info, RAID1C34);\n\t}\n}\n\nstatic int remove_block_group_item(struct btrfs_trans_handle *trans,\n\t\t\t\t   struct btrfs_path *path,\n\t\t\t\t   struct btrfs_block_group *block_group)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_root *root;\n\tstruct btrfs_key key;\n\tint ret;\n\n\troot = btrfs_block_group_root(fs_info);\n\tkey.objectid = block_group->start;\n\tkey.type = BTRFS_BLOCK_GROUP_ITEM_KEY;\n\tkey.offset = block_group->length;\n\n\tret = btrfs_search_slot(trans, root, &key, path, -1, 1);\n\tif (ret > 0)\n\t\tret = -ENOENT;\n\tif (ret < 0)\n\t\treturn ret;\n\n\tret = btrfs_del_item(trans, root, path);\n\treturn ret;\n}\n\nint btrfs_remove_block_group(struct btrfs_trans_handle *trans,\n\t\t\t     u64 group_start, struct extent_map *em)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_path *path;\n\tstruct btrfs_block_group *block_group;\n\tstruct btrfs_free_cluster *cluster;\n\tstruct inode *inode;\n\tstruct kobject *kobj = NULL;\n\tint ret;\n\tint index;\n\tint factor;\n\tstruct btrfs_caching_control *caching_ctl = NULL;\n\tbool remove_em;\n\tbool remove_rsv = false;\n\n\tblock_group = btrfs_lookup_block_group(fs_info, group_start);\n\tBUG_ON(!block_group);\n\tBUG_ON(!block_group->ro);\n\n\ttrace_btrfs_remove_block_group(block_group);\n\t \n\tbtrfs_free_excluded_extents(block_group);\n\tbtrfs_free_ref_tree_range(fs_info, block_group->start,\n\t\t\t\t  block_group->length);\n\n\tindex = btrfs_bg_flags_to_raid_index(block_group->flags);\n\tfactor = btrfs_bg_type_to_factor(block_group->flags);\n\n\t \n\tcluster = &fs_info->data_alloc_cluster;\n\tspin_lock(&cluster->refill_lock);\n\tbtrfs_return_cluster_to_free_space(block_group, cluster);\n\tspin_unlock(&cluster->refill_lock);\n\n\t \n\tcluster = &fs_info->meta_alloc_cluster;\n\tspin_lock(&cluster->refill_lock);\n\tbtrfs_return_cluster_to_free_space(block_group, cluster);\n\tspin_unlock(&cluster->refill_lock);\n\n\tbtrfs_clear_treelog_bg(block_group);\n\tbtrfs_clear_data_reloc_bg(block_group);\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\t \n\tinode = lookup_free_space_inode(block_group, path);\n\n\tmutex_lock(&trans->transaction->cache_write_mutex);\n\t \n\tspin_lock(&trans->transaction->dirty_bgs_lock);\n\tif (!list_empty(&block_group->io_list)) {\n\t\tlist_del_init(&block_group->io_list);\n\n\t\tWARN_ON(!IS_ERR(inode) && inode != block_group->io_ctl.inode);\n\n\t\tspin_unlock(&trans->transaction->dirty_bgs_lock);\n\t\tbtrfs_wait_cache_io(trans, block_group, path);\n\t\tbtrfs_put_block_group(block_group);\n\t\tspin_lock(&trans->transaction->dirty_bgs_lock);\n\t}\n\n\tif (!list_empty(&block_group->dirty_list)) {\n\t\tlist_del_init(&block_group->dirty_list);\n\t\tremove_rsv = true;\n\t\tbtrfs_put_block_group(block_group);\n\t}\n\tspin_unlock(&trans->transaction->dirty_bgs_lock);\n\tmutex_unlock(&trans->transaction->cache_write_mutex);\n\n\tret = btrfs_remove_free_space_inode(trans, inode, block_group);\n\tif (ret)\n\t\tgoto out;\n\n\twrite_lock(&fs_info->block_group_cache_lock);\n\trb_erase_cached(&block_group->cache_node,\n\t\t\t&fs_info->block_group_cache_tree);\n\tRB_CLEAR_NODE(&block_group->cache_node);\n\n\t \n\tbtrfs_put_block_group(block_group);\n\n\twrite_unlock(&fs_info->block_group_cache_lock);\n\n\tdown_write(&block_group->space_info->groups_sem);\n\t \n\tlist_del_init(&block_group->list);\n\tif (list_empty(&block_group->space_info->block_groups[index])) {\n\t\tkobj = block_group->space_info->block_group_kobjs[index];\n\t\tblock_group->space_info->block_group_kobjs[index] = NULL;\n\t\tclear_avail_alloc_bits(fs_info, block_group->flags);\n\t}\n\tup_write(&block_group->space_info->groups_sem);\n\tclear_incompat_bg_bits(fs_info, block_group->flags);\n\tif (kobj) {\n\t\tkobject_del(kobj);\n\t\tkobject_put(kobj);\n\t}\n\n\tif (block_group->cached == BTRFS_CACHE_STARTED)\n\t\tbtrfs_wait_block_group_cache_done(block_group);\n\n\twrite_lock(&fs_info->block_group_cache_lock);\n\tcaching_ctl = btrfs_get_caching_control(block_group);\n\tif (!caching_ctl) {\n\t\tstruct btrfs_caching_control *ctl;\n\n\t\tlist_for_each_entry(ctl, &fs_info->caching_block_groups, list) {\n\t\t\tif (ctl->block_group == block_group) {\n\t\t\t\tcaching_ctl = ctl;\n\t\t\t\trefcount_inc(&caching_ctl->count);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\tif (caching_ctl)\n\t\tlist_del_init(&caching_ctl->list);\n\twrite_unlock(&fs_info->block_group_cache_lock);\n\n\tif (caching_ctl) {\n\t\t \n\t\tbtrfs_put_caching_control(caching_ctl);\n\t\tbtrfs_put_caching_control(caching_ctl);\n\t}\n\n\tspin_lock(&trans->transaction->dirty_bgs_lock);\n\tWARN_ON(!list_empty(&block_group->dirty_list));\n\tWARN_ON(!list_empty(&block_group->io_list));\n\tspin_unlock(&trans->transaction->dirty_bgs_lock);\n\n\tbtrfs_remove_free_space_cache(block_group);\n\n\tspin_lock(&block_group->space_info->lock);\n\tlist_del_init(&block_group->ro_list);\n\n\tif (btrfs_test_opt(fs_info, ENOSPC_DEBUG)) {\n\t\tWARN_ON(block_group->space_info->total_bytes\n\t\t\t< block_group->length);\n\t\tWARN_ON(block_group->space_info->bytes_readonly\n\t\t\t< block_group->length - block_group->zone_unusable);\n\t\tWARN_ON(block_group->space_info->bytes_zone_unusable\n\t\t\t< block_group->zone_unusable);\n\t\tWARN_ON(block_group->space_info->disk_total\n\t\t\t< block_group->length * factor);\n\t}\n\tblock_group->space_info->total_bytes -= block_group->length;\n\tblock_group->space_info->bytes_readonly -=\n\t\t(block_group->length - block_group->zone_unusable);\n\tblock_group->space_info->bytes_zone_unusable -=\n\t\tblock_group->zone_unusable;\n\tblock_group->space_info->disk_total -= block_group->length * factor;\n\n\tspin_unlock(&block_group->space_info->lock);\n\n\t \n\tret = remove_block_group_free_space(trans, block_group);\n\tif (ret)\n\t\tgoto out;\n\n\tret = remove_block_group_item(trans, path, block_group);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tspin_lock(&block_group->lock);\n\tset_bit(BLOCK_GROUP_FLAG_REMOVED, &block_group->runtime_flags);\n\n\t \n\tremove_em = (atomic_read(&block_group->frozen) == 0);\n\tspin_unlock(&block_group->lock);\n\n\tif (remove_em) {\n\t\tstruct extent_map_tree *em_tree;\n\n\t\tem_tree = &fs_info->mapping_tree;\n\t\twrite_lock(&em_tree->lock);\n\t\tremove_extent_mapping(em_tree, em);\n\t\twrite_unlock(&em_tree->lock);\n\t\t \n\t\tfree_extent_map(em);\n\t}\n\nout:\n\t \n\tbtrfs_put_block_group(block_group);\n\tif (remove_rsv)\n\t\tbtrfs_delayed_refs_rsv_release(fs_info, 1);\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nstruct btrfs_trans_handle *btrfs_start_trans_remove_block_group(\n\t\tstruct btrfs_fs_info *fs_info, const u64 chunk_offset)\n{\n\tstruct btrfs_root *root = btrfs_block_group_root(fs_info);\n\tstruct extent_map_tree *em_tree = &fs_info->mapping_tree;\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tunsigned int num_items;\n\n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, chunk_offset, 1);\n\tread_unlock(&em_tree->lock);\n\tASSERT(em && em->start == chunk_offset);\n\n\t \n\tmap = em->map_lookup;\n\tnum_items = 3 + map->num_stripes;\n\tfree_extent_map(em);\n\n\treturn btrfs_start_transaction_fallback_global_rsv(root, num_items);\n}\n\n \nstatic int inc_block_group_ro(struct btrfs_block_group *cache, int force)\n{\n\tstruct btrfs_space_info *sinfo = cache->space_info;\n\tu64 num_bytes;\n\tint ret = -ENOSPC;\n\n\tspin_lock(&sinfo->lock);\n\tspin_lock(&cache->lock);\n\n\tif (cache->swap_extents) {\n\t\tret = -ETXTBSY;\n\t\tgoto out;\n\t}\n\n\tif (cache->ro) {\n\t\tcache->ro++;\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\tnum_bytes = cache->length - cache->reserved - cache->pinned -\n\t\t    cache->bytes_super - cache->zone_unusable - cache->used;\n\n\t \n\tif (force) {\n\t\tret = 0;\n\t} else if (sinfo->flags & BTRFS_BLOCK_GROUP_DATA) {\n\t\tu64 sinfo_used = btrfs_space_info_used(sinfo, true);\n\n\t\t \n\t\tif (sinfo_used + num_bytes <= sinfo->total_bytes)\n\t\t\tret = 0;\n\t} else {\n\t\t \n\t\tif (btrfs_can_overcommit(cache->fs_info, sinfo, num_bytes,\n\t\t\t\t\t BTRFS_RESERVE_NO_FLUSH))\n\t\t\tret = 0;\n\t}\n\n\tif (!ret) {\n\t\tsinfo->bytes_readonly += num_bytes;\n\t\tif (btrfs_is_zoned(cache->fs_info)) {\n\t\t\t \n\t\t\tsinfo->bytes_readonly += cache->zone_unusable;\n\t\t\tsinfo->bytes_zone_unusable -= cache->zone_unusable;\n\t\t\tcache->zone_unusable = 0;\n\t\t}\n\t\tcache->ro++;\n\t\tlist_add_tail(&cache->ro_list, &sinfo->ro_bgs);\n\t}\nout:\n\tspin_unlock(&cache->lock);\n\tspin_unlock(&sinfo->lock);\n\tif (ret == -ENOSPC && btrfs_test_opt(cache->fs_info, ENOSPC_DEBUG)) {\n\t\tbtrfs_info(cache->fs_info,\n\t\t\t\"unable to make block group %llu ro\", cache->start);\n\t\tbtrfs_dump_space_info(cache->fs_info, cache->space_info, 0, 0);\n\t}\n\treturn ret;\n}\n\nstatic bool clean_pinned_extents(struct btrfs_trans_handle *trans,\n\t\t\t\t struct btrfs_block_group *bg)\n{\n\tstruct btrfs_fs_info *fs_info = bg->fs_info;\n\tstruct btrfs_transaction *prev_trans = NULL;\n\tconst u64 start = bg->start;\n\tconst u64 end = start + bg->length - 1;\n\tint ret;\n\n\tspin_lock(&fs_info->trans_lock);\n\tif (trans->transaction->list.prev != &fs_info->trans_list) {\n\t\tprev_trans = list_last_entry(&trans->transaction->list,\n\t\t\t\t\t     struct btrfs_transaction, list);\n\t\trefcount_inc(&prev_trans->use_count);\n\t}\n\tspin_unlock(&fs_info->trans_lock);\n\n\t \n\tmutex_lock(&fs_info->unused_bg_unpin_mutex);\n\tif (prev_trans) {\n\t\tret = clear_extent_bits(&prev_trans->pinned_extents, start, end,\n\t\t\t\t\tEXTENT_DIRTY);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tret = clear_extent_bits(&trans->transaction->pinned_extents, start, end,\n\t\t\t\tEXTENT_DIRTY);\nout:\n\tmutex_unlock(&fs_info->unused_bg_unpin_mutex);\n\tif (prev_trans)\n\t\tbtrfs_put_transaction(prev_trans);\n\n\treturn ret == 0;\n}\n\n \nvoid btrfs_delete_unused_bgs(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_block_group *block_group;\n\tstruct btrfs_space_info *space_info;\n\tstruct btrfs_trans_handle *trans;\n\tconst bool async_trim_enabled = btrfs_test_opt(fs_info, DISCARD_ASYNC);\n\tint ret = 0;\n\n\tif (!test_bit(BTRFS_FS_OPEN, &fs_info->flags))\n\t\treturn;\n\n\tif (btrfs_fs_closing(fs_info))\n\t\treturn;\n\n\t \n\tif (!mutex_trylock(&fs_info->reclaim_bgs_lock))\n\t\treturn;\n\n\tspin_lock(&fs_info->unused_bgs_lock);\n\twhile (!list_empty(&fs_info->unused_bgs)) {\n\t\tint trimming;\n\n\t\tblock_group = list_first_entry(&fs_info->unused_bgs,\n\t\t\t\t\t       struct btrfs_block_group,\n\t\t\t\t\t       bg_list);\n\t\tlist_del_init(&block_group->bg_list);\n\n\t\tspace_info = block_group->space_info;\n\n\t\tif (ret || btrfs_mixed_space_info(space_info)) {\n\t\t\tbtrfs_put_block_group(block_group);\n\t\t\tcontinue;\n\t\t}\n\t\tspin_unlock(&fs_info->unused_bgs_lock);\n\n\t\tbtrfs_discard_cancel_work(&fs_info->discard_ctl, block_group);\n\n\t\t \n\t\tdown_write(&space_info->groups_sem);\n\n\t\t \n\t\tif (btrfs_test_opt(fs_info, DISCARD_ASYNC) &&\n\t\t    !btrfs_is_free_space_trimmed(block_group)) {\n\t\t\ttrace_btrfs_skip_unused_block_group(block_group);\n\t\t\tup_write(&space_info->groups_sem);\n\t\t\t \n\t\t\tbtrfs_discard_queue_work(&fs_info->discard_ctl,\n\t\t\t\t\t\t block_group);\n\t\t\tgoto next;\n\t\t}\n\n\t\tspin_lock(&block_group->lock);\n\t\tif (block_group->reserved || block_group->pinned ||\n\t\t    block_group->used || block_group->ro ||\n\t\t    list_is_singular(&block_group->list)) {\n\t\t\t \n\t\t\ttrace_btrfs_skip_unused_block_group(block_group);\n\t\t\tspin_unlock(&block_group->lock);\n\t\t\tup_write(&space_info->groups_sem);\n\t\t\tgoto next;\n\t\t}\n\t\tspin_unlock(&block_group->lock);\n\n\t\t \n\t\tret = inc_block_group_ro(block_group, 0);\n\t\tup_write(&space_info->groups_sem);\n\t\tif (ret < 0) {\n\t\t\tret = 0;\n\t\t\tgoto next;\n\t\t}\n\n\t\tret = btrfs_zone_finish(block_group);\n\t\tif (ret < 0) {\n\t\t\tbtrfs_dec_block_group_ro(block_group);\n\t\t\tif (ret == -EAGAIN)\n\t\t\t\tret = 0;\n\t\t\tgoto next;\n\t\t}\n\n\t\t \n\t\ttrans = btrfs_start_trans_remove_block_group(fs_info,\n\t\t\t\t\t\t     block_group->start);\n\t\tif (IS_ERR(trans)) {\n\t\t\tbtrfs_dec_block_group_ro(block_group);\n\t\t\tret = PTR_ERR(trans);\n\t\t\tgoto next;\n\t\t}\n\n\t\t \n\t\tif (!clean_pinned_extents(trans, block_group)) {\n\t\t\tbtrfs_dec_block_group_ro(block_group);\n\t\t\tgoto end_trans;\n\t\t}\n\n\t\t \n\t\tspin_lock(&fs_info->discard_ctl.lock);\n\t\tif (!list_empty(&block_group->discard_list)) {\n\t\t\tspin_unlock(&fs_info->discard_ctl.lock);\n\t\t\tbtrfs_dec_block_group_ro(block_group);\n\t\t\tbtrfs_discard_queue_work(&fs_info->discard_ctl,\n\t\t\t\t\t\t block_group);\n\t\t\tgoto end_trans;\n\t\t}\n\t\tspin_unlock(&fs_info->discard_ctl.lock);\n\n\t\t \n\t\tspin_lock(&space_info->lock);\n\t\tspin_lock(&block_group->lock);\n\n\t\tbtrfs_space_info_update_bytes_pinned(fs_info, space_info,\n\t\t\t\t\t\t     -block_group->pinned);\n\t\tspace_info->bytes_readonly += block_group->pinned;\n\t\tblock_group->pinned = 0;\n\n\t\tspin_unlock(&block_group->lock);\n\t\tspin_unlock(&space_info->lock);\n\n\t\t \n\t\tif (!async_trim_enabled && btrfs_test_opt(fs_info, DISCARD_ASYNC))\n\t\t\tgoto flip_async;\n\n\t\t \n\t\ttrimming = btrfs_test_opt(fs_info, DISCARD_SYNC) ||\n\t\t\t\tbtrfs_is_zoned(fs_info);\n\n\t\t \n\t\tif (trimming)\n\t\t\tbtrfs_freeze_block_group(block_group);\n\n\t\t \n\t\tret = btrfs_remove_chunk(trans, block_group->start);\n\n\t\tif (ret) {\n\t\t\tif (trimming)\n\t\t\t\tbtrfs_unfreeze_block_group(block_group);\n\t\t\tgoto end_trans;\n\t\t}\n\n\t\t \n\t\tif (trimming) {\n\t\t\tspin_lock(&fs_info->unused_bgs_lock);\n\t\t\t \n\t\t\tlist_move(&block_group->bg_list,\n\t\t\t\t  &trans->transaction->deleted_bgs);\n\t\t\tspin_unlock(&fs_info->unused_bgs_lock);\n\t\t\tbtrfs_get_block_group(block_group);\n\t\t}\nend_trans:\n\t\tbtrfs_end_transaction(trans);\nnext:\n\t\tbtrfs_put_block_group(block_group);\n\t\tspin_lock(&fs_info->unused_bgs_lock);\n\t}\n\tspin_unlock(&fs_info->unused_bgs_lock);\n\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\treturn;\n\nflip_async:\n\tbtrfs_end_transaction(trans);\n\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\tbtrfs_put_block_group(block_group);\n\tbtrfs_discard_punt_unused_bgs_list(fs_info);\n}\n\nvoid btrfs_mark_bg_unused(struct btrfs_block_group *bg)\n{\n\tstruct btrfs_fs_info *fs_info = bg->fs_info;\n\n\tspin_lock(&fs_info->unused_bgs_lock);\n\tif (list_empty(&bg->bg_list)) {\n\t\tbtrfs_get_block_group(bg);\n\t\ttrace_btrfs_add_unused_block_group(bg);\n\t\tlist_add_tail(&bg->bg_list, &fs_info->unused_bgs);\n\t} else if (!test_bit(BLOCK_GROUP_FLAG_NEW, &bg->runtime_flags)) {\n\t\t \n\t\ttrace_btrfs_add_unused_block_group(bg);\n\t\tlist_move_tail(&bg->bg_list, &fs_info->unused_bgs);\n\t}\n\tspin_unlock(&fs_info->unused_bgs_lock);\n}\n\n \nstatic int reclaim_bgs_cmp(void *unused, const struct list_head *a,\n\t\t\t   const struct list_head *b)\n{\n\tconst struct btrfs_block_group *bg1, *bg2;\n\n\tbg1 = list_entry(a, struct btrfs_block_group, bg_list);\n\tbg2 = list_entry(b, struct btrfs_block_group, bg_list);\n\n\treturn bg1->used > bg2->used;\n}\n\nstatic inline bool btrfs_should_reclaim(struct btrfs_fs_info *fs_info)\n{\n\tif (btrfs_is_zoned(fs_info))\n\t\treturn btrfs_zoned_should_reclaim(fs_info);\n\treturn true;\n}\n\nstatic bool should_reclaim_block_group(struct btrfs_block_group *bg, u64 bytes_freed)\n{\n\tconst struct btrfs_space_info *space_info = bg->space_info;\n\tconst int reclaim_thresh = READ_ONCE(space_info->bg_reclaim_threshold);\n\tconst u64 new_val = bg->used;\n\tconst u64 old_val = new_val + bytes_freed;\n\tu64 thresh;\n\n\tif (reclaim_thresh == 0)\n\t\treturn false;\n\n\tthresh = mult_perc(bg->length, reclaim_thresh);\n\n\t \n\tif (old_val < thresh)\n\t\treturn false;\n\tif (new_val >= thresh)\n\t\treturn false;\n\treturn true;\n}\n\nvoid btrfs_reclaim_bgs_work(struct work_struct *work)\n{\n\tstruct btrfs_fs_info *fs_info =\n\t\tcontainer_of(work, struct btrfs_fs_info, reclaim_bgs_work);\n\tstruct btrfs_block_group *bg;\n\tstruct btrfs_space_info *space_info;\n\n\tif (!test_bit(BTRFS_FS_OPEN, &fs_info->flags))\n\t\treturn;\n\n\tif (btrfs_fs_closing(fs_info))\n\t\treturn;\n\n\tif (!btrfs_should_reclaim(fs_info))\n\t\treturn;\n\n\tsb_start_write(fs_info->sb);\n\n\tif (!btrfs_exclop_start(fs_info, BTRFS_EXCLOP_BALANCE)) {\n\t\tsb_end_write(fs_info->sb);\n\t\treturn;\n\t}\n\n\t \n\tif (!mutex_trylock(&fs_info->reclaim_bgs_lock)) {\n\t\tbtrfs_exclop_finish(fs_info);\n\t\tsb_end_write(fs_info->sb);\n\t\treturn;\n\t}\n\n\tspin_lock(&fs_info->unused_bgs_lock);\n\t \n\tlist_sort(NULL, &fs_info->reclaim_bgs, reclaim_bgs_cmp);\n\twhile (!list_empty(&fs_info->reclaim_bgs)) {\n\t\tu64 zone_unusable;\n\t\tint ret = 0;\n\n\t\tbg = list_first_entry(&fs_info->reclaim_bgs,\n\t\t\t\t      struct btrfs_block_group,\n\t\t\t\t      bg_list);\n\t\tlist_del_init(&bg->bg_list);\n\n\t\tspace_info = bg->space_info;\n\t\tspin_unlock(&fs_info->unused_bgs_lock);\n\n\t\t \n\t\tdown_write(&space_info->groups_sem);\n\n\t\tspin_lock(&bg->lock);\n\t\tif (bg->reserved || bg->pinned || bg->ro) {\n\t\t\t \n\t\t\tspin_unlock(&bg->lock);\n\t\t\tup_write(&space_info->groups_sem);\n\t\t\tgoto next;\n\t\t}\n\t\tif (bg->used == 0) {\n\t\t\t \n\t\t\tif (!btrfs_test_opt(fs_info, DISCARD_ASYNC))\n\t\t\t\tbtrfs_mark_bg_unused(bg);\n\t\t\tspin_unlock(&bg->lock);\n\t\t\tup_write(&space_info->groups_sem);\n\t\t\tgoto next;\n\n\t\t}\n\t\t \n\t\tif (!should_reclaim_block_group(bg, bg->length)) {\n\t\t\tspin_unlock(&bg->lock);\n\t\t\tup_write(&space_info->groups_sem);\n\t\t\tgoto next;\n\t\t}\n\t\tspin_unlock(&bg->lock);\n\n\t\t \n\t\tif (btrfs_need_cleaner_sleep(fs_info)) {\n\t\t\tup_write(&space_info->groups_sem);\n\t\t\tgoto next;\n\t\t}\n\n\t\t \n\t\tzone_unusable = bg->zone_unusable;\n\t\tret = inc_block_group_ro(bg, 0);\n\t\tup_write(&space_info->groups_sem);\n\t\tif (ret < 0)\n\t\t\tgoto next;\n\n\t\tbtrfs_info(fs_info,\n\t\t\t\"reclaiming chunk %llu with %llu%% used %llu%% unusable\",\n\t\t\t\tbg->start,\n\t\t\t\tdiv64_u64(bg->used * 100, bg->length),\n\t\t\t\tdiv64_u64(zone_unusable * 100, bg->length));\n\t\ttrace_btrfs_reclaim_block_group(bg);\n\t\tret = btrfs_relocate_chunk(fs_info, bg->start);\n\t\tif (ret) {\n\t\t\tbtrfs_dec_block_group_ro(bg);\n\t\t\tbtrfs_err(fs_info, \"error relocating chunk %llu\",\n\t\t\t\t  bg->start);\n\t\t}\n\nnext:\n\t\tif (ret)\n\t\t\tbtrfs_mark_bg_to_reclaim(bg);\n\t\tbtrfs_put_block_group(bg);\n\n\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\t \n\t\tbtrfs_delete_unused_bgs(fs_info);\n\t\t \n\t\tif (!mutex_trylock(&fs_info->reclaim_bgs_lock))\n\t\t\tgoto end;\n\t\tspin_lock(&fs_info->unused_bgs_lock);\n\t}\n\tspin_unlock(&fs_info->unused_bgs_lock);\n\tmutex_unlock(&fs_info->reclaim_bgs_lock);\nend:\n\tbtrfs_exclop_finish(fs_info);\n\tsb_end_write(fs_info->sb);\n}\n\nvoid btrfs_reclaim_bgs(struct btrfs_fs_info *fs_info)\n{\n\tspin_lock(&fs_info->unused_bgs_lock);\n\tif (!list_empty(&fs_info->reclaim_bgs))\n\t\tqueue_work(system_unbound_wq, &fs_info->reclaim_bgs_work);\n\tspin_unlock(&fs_info->unused_bgs_lock);\n}\n\nvoid btrfs_mark_bg_to_reclaim(struct btrfs_block_group *bg)\n{\n\tstruct btrfs_fs_info *fs_info = bg->fs_info;\n\n\tspin_lock(&fs_info->unused_bgs_lock);\n\tif (list_empty(&bg->bg_list)) {\n\t\tbtrfs_get_block_group(bg);\n\t\ttrace_btrfs_add_reclaim_block_group(bg);\n\t\tlist_add_tail(&bg->bg_list, &fs_info->reclaim_bgs);\n\t}\n\tspin_unlock(&fs_info->unused_bgs_lock);\n}\n\nstatic int read_bg_from_eb(struct btrfs_fs_info *fs_info, struct btrfs_key *key,\n\t\t\t   struct btrfs_path *path)\n{\n\tstruct extent_map_tree *em_tree;\n\tstruct extent_map *em;\n\tstruct btrfs_block_group_item bg;\n\tstruct extent_buffer *leaf;\n\tint slot;\n\tu64 flags;\n\tint ret = 0;\n\n\tslot = path->slots[0];\n\tleaf = path->nodes[0];\n\n\tem_tree = &fs_info->mapping_tree;\n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, key->objectid, key->offset);\n\tread_unlock(&em_tree->lock);\n\tif (!em) {\n\t\tbtrfs_err(fs_info,\n\t\t\t  \"logical %llu len %llu found bg but no related chunk\",\n\t\t\t  key->objectid, key->offset);\n\t\treturn -ENOENT;\n\t}\n\n\tif (em->start != key->objectid || em->len != key->offset) {\n\t\tbtrfs_err(fs_info,\n\t\t\t\"block group %llu len %llu mismatch with chunk %llu len %llu\",\n\t\t\tkey->objectid, key->offset, em->start, em->len);\n\t\tret = -EUCLEAN;\n\t\tgoto out_free_em;\n\t}\n\n\tread_extent_buffer(leaf, &bg, btrfs_item_ptr_offset(leaf, slot),\n\t\t\t   sizeof(bg));\n\tflags = btrfs_stack_block_group_flags(&bg) &\n\t\tBTRFS_BLOCK_GROUP_TYPE_MASK;\n\n\tif (flags != (em->map_lookup->type & BTRFS_BLOCK_GROUP_TYPE_MASK)) {\n\t\tbtrfs_err(fs_info,\n\"block group %llu len %llu type flags 0x%llx mismatch with chunk type flags 0x%llx\",\n\t\t\t  key->objectid, key->offset, flags,\n\t\t\t  (BTRFS_BLOCK_GROUP_TYPE_MASK & em->map_lookup->type));\n\t\tret = -EUCLEAN;\n\t}\n\nout_free_em:\n\tfree_extent_map(em);\n\treturn ret;\n}\n\nstatic int find_first_block_group(struct btrfs_fs_info *fs_info,\n\t\t\t\t  struct btrfs_path *path,\n\t\t\t\t  struct btrfs_key *key)\n{\n\tstruct btrfs_root *root = btrfs_block_group_root(fs_info);\n\tint ret;\n\tstruct btrfs_key found_key;\n\n\tbtrfs_for_each_slot(root, key, &found_key, path, ret) {\n\t\tif (found_key.objectid >= key->objectid &&\n\t\t    found_key.type == BTRFS_BLOCK_GROUP_ITEM_KEY) {\n\t\t\treturn read_bg_from_eb(fs_info, &found_key, path);\n\t\t}\n\t}\n\treturn ret;\n}\n\nstatic void set_avail_alloc_bits(struct btrfs_fs_info *fs_info, u64 flags)\n{\n\tu64 extra_flags = chunk_to_extended(flags) &\n\t\t\t\tBTRFS_EXTENDED_PROFILE_MASK;\n\n\twrite_seqlock(&fs_info->profiles_lock);\n\tif (flags & BTRFS_BLOCK_GROUP_DATA)\n\t\tfs_info->avail_data_alloc_bits |= extra_flags;\n\tif (flags & BTRFS_BLOCK_GROUP_METADATA)\n\t\tfs_info->avail_metadata_alloc_bits |= extra_flags;\n\tif (flags & BTRFS_BLOCK_GROUP_SYSTEM)\n\t\tfs_info->avail_system_alloc_bits |= extra_flags;\n\twrite_sequnlock(&fs_info->profiles_lock);\n}\n\n \nint btrfs_rmap_block(struct btrfs_fs_info *fs_info, u64 chunk_start,\n\t\t     u64 physical, u64 **logical, int *naddrs, int *stripe_len)\n{\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tu64 *buf;\n\tu64 bytenr;\n\tu64 data_stripe_length;\n\tu64 io_stripe_size;\n\tint i, nr = 0;\n\tint ret = 0;\n\n\tem = btrfs_get_chunk_map(fs_info, chunk_start, 1);\n\tif (IS_ERR(em))\n\t\treturn -EIO;\n\n\tmap = em->map_lookup;\n\tdata_stripe_length = em->orig_block_len;\n\tio_stripe_size = BTRFS_STRIPE_LEN;\n\tchunk_start = em->start;\n\n\t \n\tif (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK)\n\t\tio_stripe_size = btrfs_stripe_nr_to_offset(nr_data_stripes(map));\n\n\tbuf = kcalloc(map->num_stripes, sizeof(u64), GFP_NOFS);\n\tif (!buf) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tbool already_inserted = false;\n\t\tu32 stripe_nr;\n\t\tu32 offset;\n\t\tint j;\n\n\t\tif (!in_range(physical, map->stripes[i].physical,\n\t\t\t      data_stripe_length))\n\t\t\tcontinue;\n\n\t\tstripe_nr = (physical - map->stripes[i].physical) >>\n\t\t\t    BTRFS_STRIPE_LEN_SHIFT;\n\t\toffset = (physical - map->stripes[i].physical) &\n\t\t\t BTRFS_STRIPE_LEN_MASK;\n\n\t\tif (map->type & (BTRFS_BLOCK_GROUP_RAID0 |\n\t\t\t\t BTRFS_BLOCK_GROUP_RAID10))\n\t\t\tstripe_nr = div_u64(stripe_nr * map->num_stripes + i,\n\t\t\t\t\t    map->sub_stripes);\n\t\t \n\t\tbytenr = chunk_start + stripe_nr * io_stripe_size + offset;\n\n\t\t \n\t\tfor (j = 0; j < nr; j++) {\n\t\t\tif (buf[j] == bytenr) {\n\t\t\t\talready_inserted = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (!already_inserted)\n\t\t\tbuf[nr++] = bytenr;\n\t}\n\n\t*logical = buf;\n\t*naddrs = nr;\n\t*stripe_len = io_stripe_size;\nout:\n\tfree_extent_map(em);\n\treturn ret;\n}\n\nstatic int exclude_super_stripes(struct btrfs_block_group *cache)\n{\n\tstruct btrfs_fs_info *fs_info = cache->fs_info;\n\tconst bool zoned = btrfs_is_zoned(fs_info);\n\tu64 bytenr;\n\tu64 *logical;\n\tint stripe_len;\n\tint i, nr, ret;\n\n\tif (cache->start < BTRFS_SUPER_INFO_OFFSET) {\n\t\tstripe_len = BTRFS_SUPER_INFO_OFFSET - cache->start;\n\t\tcache->bytes_super += stripe_len;\n\t\tret = set_extent_bit(&fs_info->excluded_extents, cache->start,\n\t\t\t\t     cache->start + stripe_len - 1,\n\t\t\t\t     EXTENT_UPTODATE, NULL);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tfor (i = 0; i < BTRFS_SUPER_MIRROR_MAX; i++) {\n\t\tbytenr = btrfs_sb_offset(i);\n\t\tret = btrfs_rmap_block(fs_info, cache->start,\n\t\t\t\t       bytenr, &logical, &nr, &stripe_len);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\t \n\t\tif (zoned && nr) {\n\t\t\tkfree(logical);\n\t\t\tbtrfs_err(fs_info,\n\t\t\t\"zoned: block group %llu must not contain super block\",\n\t\t\t\t  cache->start);\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\twhile (nr--) {\n\t\t\tu64 len = min_t(u64, stripe_len,\n\t\t\t\tcache->start + cache->length - logical[nr]);\n\n\t\t\tcache->bytes_super += len;\n\t\t\tret = set_extent_bit(&fs_info->excluded_extents, logical[nr],\n\t\t\t\t\t     logical[nr] + len - 1,\n\t\t\t\t\t     EXTENT_UPTODATE, NULL);\n\t\t\tif (ret) {\n\t\t\t\tkfree(logical);\n\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\n\t\tkfree(logical);\n\t}\n\treturn 0;\n}\n\nstatic struct btrfs_block_group *btrfs_create_block_group_cache(\n\t\tstruct btrfs_fs_info *fs_info, u64 start)\n{\n\tstruct btrfs_block_group *cache;\n\n\tcache = kzalloc(sizeof(*cache), GFP_NOFS);\n\tif (!cache)\n\t\treturn NULL;\n\n\tcache->free_space_ctl = kzalloc(sizeof(*cache->free_space_ctl),\n\t\t\t\t\tGFP_NOFS);\n\tif (!cache->free_space_ctl) {\n\t\tkfree(cache);\n\t\treturn NULL;\n\t}\n\n\tcache->start = start;\n\n\tcache->fs_info = fs_info;\n\tcache->full_stripe_len = btrfs_full_stripe_len(fs_info, start);\n\n\tcache->discard_index = BTRFS_DISCARD_INDEX_UNUSED;\n\n\trefcount_set(&cache->refs, 1);\n\tspin_lock_init(&cache->lock);\n\tinit_rwsem(&cache->data_rwsem);\n\tINIT_LIST_HEAD(&cache->list);\n\tINIT_LIST_HEAD(&cache->cluster_list);\n\tINIT_LIST_HEAD(&cache->bg_list);\n\tINIT_LIST_HEAD(&cache->ro_list);\n\tINIT_LIST_HEAD(&cache->discard_list);\n\tINIT_LIST_HEAD(&cache->dirty_list);\n\tINIT_LIST_HEAD(&cache->io_list);\n\tINIT_LIST_HEAD(&cache->active_bg_list);\n\tbtrfs_init_free_space_ctl(cache, cache->free_space_ctl);\n\tatomic_set(&cache->frozen, 0);\n\tmutex_init(&cache->free_space_lock);\n\n\treturn cache;\n}\n\n \nstatic int check_chunk_block_group_mappings(struct btrfs_fs_info *fs_info)\n{\n\tstruct extent_map_tree *map_tree = &fs_info->mapping_tree;\n\tstruct extent_map *em;\n\tstruct btrfs_block_group *bg;\n\tu64 start = 0;\n\tint ret = 0;\n\n\twhile (1) {\n\t\tread_lock(&map_tree->lock);\n\t\t \n\t\tem = lookup_extent_mapping(map_tree, start, 1);\n\t\tread_unlock(&map_tree->lock);\n\t\tif (!em)\n\t\t\tbreak;\n\n\t\tbg = btrfs_lookup_block_group(fs_info, em->start);\n\t\tif (!bg) {\n\t\t\tbtrfs_err(fs_info,\n\t\"chunk start=%llu len=%llu doesn't have corresponding block group\",\n\t\t\t\t     em->start, em->len);\n\t\t\tret = -EUCLEAN;\n\t\t\tfree_extent_map(em);\n\t\t\tbreak;\n\t\t}\n\t\tif (bg->start != em->start || bg->length != em->len ||\n\t\t    (bg->flags & BTRFS_BLOCK_GROUP_TYPE_MASK) !=\n\t\t    (em->map_lookup->type & BTRFS_BLOCK_GROUP_TYPE_MASK)) {\n\t\t\tbtrfs_err(fs_info,\n\"chunk start=%llu len=%llu flags=0x%llx doesn't match block group start=%llu len=%llu flags=0x%llx\",\n\t\t\t\tem->start, em->len,\n\t\t\t\tem->map_lookup->type & BTRFS_BLOCK_GROUP_TYPE_MASK,\n\t\t\t\tbg->start, bg->length,\n\t\t\t\tbg->flags & BTRFS_BLOCK_GROUP_TYPE_MASK);\n\t\t\tret = -EUCLEAN;\n\t\t\tfree_extent_map(em);\n\t\t\tbtrfs_put_block_group(bg);\n\t\t\tbreak;\n\t\t}\n\t\tstart = em->start + em->len;\n\t\tfree_extent_map(em);\n\t\tbtrfs_put_block_group(bg);\n\t}\n\treturn ret;\n}\n\nstatic int read_one_block_group(struct btrfs_fs_info *info,\n\t\t\t\tstruct btrfs_block_group_item *bgi,\n\t\t\t\tconst struct btrfs_key *key,\n\t\t\t\tint need_clear)\n{\n\tstruct btrfs_block_group *cache;\n\tconst bool mixed = btrfs_fs_incompat(info, MIXED_GROUPS);\n\tint ret;\n\n\tASSERT(key->type == BTRFS_BLOCK_GROUP_ITEM_KEY);\n\n\tcache = btrfs_create_block_group_cache(info, key->objectid);\n\tif (!cache)\n\t\treturn -ENOMEM;\n\n\tcache->length = key->offset;\n\tcache->used = btrfs_stack_block_group_used(bgi);\n\tcache->commit_used = cache->used;\n\tcache->flags = btrfs_stack_block_group_flags(bgi);\n\tcache->global_root_id = btrfs_stack_block_group_chunk_objectid(bgi);\n\n\tset_free_space_tree_thresholds(cache);\n\n\tif (need_clear) {\n\t\t \n\t\tif (btrfs_test_opt(info, SPACE_CACHE))\n\t\t\tcache->disk_cache_state = BTRFS_DC_CLEAR;\n\t}\n\tif (!mixed && ((cache->flags & BTRFS_BLOCK_GROUP_METADATA) &&\n\t    (cache->flags & BTRFS_BLOCK_GROUP_DATA))) {\n\t\t\tbtrfs_err(info,\n\"bg %llu is a mixed block group but filesystem hasn't enabled mixed block groups\",\n\t\t\t\t  cache->start);\n\t\t\tret = -EINVAL;\n\t\t\tgoto error;\n\t}\n\n\tret = btrfs_load_block_group_zone_info(cache, false);\n\tif (ret) {\n\t\tbtrfs_err(info, \"zoned: failed to load zone info of bg %llu\",\n\t\t\t  cache->start);\n\t\tgoto error;\n\t}\n\n\t \n\tret = exclude_super_stripes(cache);\n\tif (ret) {\n\t\t \n\t\tbtrfs_free_excluded_extents(cache);\n\t\tgoto error;\n\t}\n\n\t \n\tif (btrfs_is_zoned(info)) {\n\t\tbtrfs_calc_zone_unusable(cache);\n\t\t \n\t\tbtrfs_free_excluded_extents(cache);\n\t} else if (cache->length == cache->used) {\n\t\tcache->cached = BTRFS_CACHE_FINISHED;\n\t\tbtrfs_free_excluded_extents(cache);\n\t} else if (cache->used == 0) {\n\t\tcache->cached = BTRFS_CACHE_FINISHED;\n\t\tret = btrfs_add_new_free_space(cache, cache->start,\n\t\t\t\t\t       cache->start + cache->length, NULL);\n\t\tbtrfs_free_excluded_extents(cache);\n\t\tif (ret)\n\t\t\tgoto error;\n\t}\n\n\tret = btrfs_add_block_group_cache(info, cache);\n\tif (ret) {\n\t\tbtrfs_remove_free_space_cache(cache);\n\t\tgoto error;\n\t}\n\ttrace_btrfs_add_block_group(info, cache, 0);\n\tbtrfs_add_bg_to_space_info(info, cache);\n\n\tset_avail_alloc_bits(info, cache->flags);\n\tif (btrfs_chunk_writeable(info, cache->start)) {\n\t\tif (cache->used == 0) {\n\t\t\tASSERT(list_empty(&cache->bg_list));\n\t\t\tif (btrfs_test_opt(info, DISCARD_ASYNC))\n\t\t\t\tbtrfs_discard_queue_work(&info->discard_ctl, cache);\n\t\t\telse\n\t\t\t\tbtrfs_mark_bg_unused(cache);\n\t\t}\n\t} else {\n\t\tinc_block_group_ro(cache, 1);\n\t}\n\n\treturn 0;\nerror:\n\tbtrfs_put_block_group(cache);\n\treturn ret;\n}\n\nstatic int fill_dummy_bgs(struct btrfs_fs_info *fs_info)\n{\n\tstruct extent_map_tree *em_tree = &fs_info->mapping_tree;\n\tstruct rb_node *node;\n\tint ret = 0;\n\n\tfor (node = rb_first_cached(&em_tree->map); node; node = rb_next(node)) {\n\t\tstruct extent_map *em;\n\t\tstruct map_lookup *map;\n\t\tstruct btrfs_block_group *bg;\n\n\t\tem = rb_entry(node, struct extent_map, rb_node);\n\t\tmap = em->map_lookup;\n\t\tbg = btrfs_create_block_group_cache(fs_info, em->start);\n\t\tif (!bg) {\n\t\t\tret = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tbg->length = em->len;\n\t\tbg->flags = map->type;\n\t\tbg->cached = BTRFS_CACHE_FINISHED;\n\t\tbg->used = em->len;\n\t\tbg->flags = map->type;\n\t\tret = btrfs_add_block_group_cache(fs_info, bg);\n\t\t \n\t\tif (ret == -EEXIST) {\n\t\t\tret = 0;\n\t\t\tbtrfs_put_block_group(bg);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (ret) {\n\t\t\tbtrfs_remove_free_space_cache(bg);\n\t\t\tbtrfs_put_block_group(bg);\n\t\t\tbreak;\n\t\t}\n\n\t\tbtrfs_add_bg_to_space_info(fs_info, bg);\n\n\t\tset_avail_alloc_bits(fs_info, bg->flags);\n\t}\n\tif (!ret)\n\t\tbtrfs_init_global_block_rsv(fs_info);\n\treturn ret;\n}\n\nint btrfs_read_block_groups(struct btrfs_fs_info *info)\n{\n\tstruct btrfs_root *root = btrfs_block_group_root(info);\n\tstruct btrfs_path *path;\n\tint ret;\n\tstruct btrfs_block_group *cache;\n\tstruct btrfs_space_info *space_info;\n\tstruct btrfs_key key;\n\tint need_clear = 0;\n\tu64 cache_gen;\n\n\t \n\tif (!root || (btrfs_super_compat_ro_flags(info->super_copy) &\n\t\t      ~BTRFS_FEATURE_COMPAT_RO_SUPP))\n\t\treturn fill_dummy_bgs(info);\n\n\tkey.objectid = 0;\n\tkey.offset = 0;\n\tkey.type = BTRFS_BLOCK_GROUP_ITEM_KEY;\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tcache_gen = btrfs_super_cache_generation(info->super_copy);\n\tif (btrfs_test_opt(info, SPACE_CACHE) &&\n\t    btrfs_super_generation(info->super_copy) != cache_gen)\n\t\tneed_clear = 1;\n\tif (btrfs_test_opt(info, CLEAR_CACHE))\n\t\tneed_clear = 1;\n\n\twhile (1) {\n\t\tstruct btrfs_block_group_item bgi;\n\t\tstruct extent_buffer *leaf;\n\t\tint slot;\n\n\t\tret = find_first_block_group(info, path, &key);\n\t\tif (ret > 0)\n\t\t\tbreak;\n\t\tif (ret != 0)\n\t\t\tgoto error;\n\n\t\tleaf = path->nodes[0];\n\t\tslot = path->slots[0];\n\n\t\tread_extent_buffer(leaf, &bgi, btrfs_item_ptr_offset(leaf, slot),\n\t\t\t\t   sizeof(bgi));\n\n\t\tbtrfs_item_key_to_cpu(leaf, &key, slot);\n\t\tbtrfs_release_path(path);\n\t\tret = read_one_block_group(info, &bgi, &key, need_clear);\n\t\tif (ret < 0)\n\t\t\tgoto error;\n\t\tkey.objectid += key.offset;\n\t\tkey.offset = 0;\n\t}\n\tbtrfs_release_path(path);\n\n\tlist_for_each_entry(space_info, &info->space_info, list) {\n\t\tint i;\n\n\t\tfor (i = 0; i < BTRFS_NR_RAID_TYPES; i++) {\n\t\t\tif (list_empty(&space_info->block_groups[i]))\n\t\t\t\tcontinue;\n\t\t\tcache = list_first_entry(&space_info->block_groups[i],\n\t\t\t\t\t\t struct btrfs_block_group,\n\t\t\t\t\t\t list);\n\t\t\tbtrfs_sysfs_add_block_group_type(cache);\n\t\t}\n\n\t\tif (!(btrfs_get_alloc_profile(info, space_info->flags) &\n\t\t      (BTRFS_BLOCK_GROUP_RAID10 |\n\t\t       BTRFS_BLOCK_GROUP_RAID1_MASK |\n\t\t       BTRFS_BLOCK_GROUP_RAID56_MASK |\n\t\t       BTRFS_BLOCK_GROUP_DUP)))\n\t\t\tcontinue;\n\t\t \n\t\tlist_for_each_entry(cache,\n\t\t\t\t&space_info->block_groups[BTRFS_RAID_RAID0],\n\t\t\t\tlist)\n\t\t\tinc_block_group_ro(cache, 1);\n\t\tlist_for_each_entry(cache,\n\t\t\t\t&space_info->block_groups[BTRFS_RAID_SINGLE],\n\t\t\t\tlist)\n\t\t\tinc_block_group_ro(cache, 1);\n\t}\n\n\tbtrfs_init_global_block_rsv(info);\n\tret = check_chunk_block_group_mappings(info);\nerror:\n\tbtrfs_free_path(path);\n\t \n\tif (ret && btrfs_test_opt(info, IGNOREBADROOTS))\n\t\tret = fill_dummy_bgs(info);\n\treturn ret;\n}\n\n \nstatic int insert_block_group_item(struct btrfs_trans_handle *trans,\n\t\t\t\t   struct btrfs_block_group *block_group)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_block_group_item bgi;\n\tstruct btrfs_root *root = btrfs_block_group_root(fs_info);\n\tstruct btrfs_key key;\n\tu64 old_commit_used;\n\tint ret;\n\n\tspin_lock(&block_group->lock);\n\tbtrfs_set_stack_block_group_used(&bgi, block_group->used);\n\tbtrfs_set_stack_block_group_chunk_objectid(&bgi,\n\t\t\t\t\t\t   block_group->global_root_id);\n\tbtrfs_set_stack_block_group_flags(&bgi, block_group->flags);\n\told_commit_used = block_group->commit_used;\n\tblock_group->commit_used = block_group->used;\n\tkey.objectid = block_group->start;\n\tkey.type = BTRFS_BLOCK_GROUP_ITEM_KEY;\n\tkey.offset = block_group->length;\n\tspin_unlock(&block_group->lock);\n\n\tret = btrfs_insert_item(trans, root, &key, &bgi, sizeof(bgi));\n\tif (ret < 0) {\n\t\tspin_lock(&block_group->lock);\n\t\tblock_group->commit_used = old_commit_used;\n\t\tspin_unlock(&block_group->lock);\n\t}\n\n\treturn ret;\n}\n\nstatic int insert_dev_extent(struct btrfs_trans_handle *trans,\n\t\t\t    struct btrfs_device *device, u64 chunk_offset,\n\t\t\t    u64 start, u64 num_bytes)\n{\n\tstruct btrfs_fs_info *fs_info = device->fs_info;\n\tstruct btrfs_root *root = fs_info->dev_root;\n\tstruct btrfs_path *path;\n\tstruct btrfs_dev_extent *extent;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_key key;\n\tint ret;\n\n\tWARN_ON(!test_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state));\n\tWARN_ON(test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state));\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = device->devid;\n\tkey.type = BTRFS_DEV_EXTENT_KEY;\n\tkey.offset = start;\n\tret = btrfs_insert_empty_item(trans, root, path, &key, sizeof(*extent));\n\tif (ret)\n\t\tgoto out;\n\n\tleaf = path->nodes[0];\n\textent = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_dev_extent);\n\tbtrfs_set_dev_extent_chunk_tree(leaf, extent, BTRFS_CHUNK_TREE_OBJECTID);\n\tbtrfs_set_dev_extent_chunk_objectid(leaf, extent,\n\t\t\t\t\t    BTRFS_FIRST_CHUNK_TREE_OBJECTID);\n\tbtrfs_set_dev_extent_chunk_offset(leaf, extent, chunk_offset);\n\n\tbtrfs_set_dev_extent_length(leaf, extent, num_bytes);\n\tbtrfs_mark_buffer_dirty(trans, leaf);\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n \nstatic int insert_dev_extents(struct btrfs_trans_handle *trans,\n\t\t\t\t   u64 chunk_offset, u64 chunk_size)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_device *device;\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tu64 dev_offset;\n\tu64 stripe_size;\n\tint i;\n\tint ret = 0;\n\n\tem = btrfs_get_chunk_map(fs_info, chunk_offset, chunk_size);\n\tif (IS_ERR(em))\n\t\treturn PTR_ERR(em);\n\n\tmap = em->map_lookup;\n\tstripe_size = em->orig_block_len;\n\n\t \n\tmutex_lock(&fs_info->fs_devices->device_list_mutex);\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tdevice = map->stripes[i].dev;\n\t\tdev_offset = map->stripes[i].physical;\n\n\t\tret = insert_dev_extent(trans, device, chunk_offset, dev_offset,\n\t\t\t\t       stripe_size);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\n\tfree_extent_map(em);\n\treturn ret;\n}\n\n \nvoid btrfs_create_pending_block_groups(struct btrfs_trans_handle *trans)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_block_group *block_group;\n\tint ret = 0;\n\n\twhile (!list_empty(&trans->new_bgs)) {\n\t\tint index;\n\n\t\tblock_group = list_first_entry(&trans->new_bgs,\n\t\t\t\t\t       struct btrfs_block_group,\n\t\t\t\t\t       bg_list);\n\t\tif (ret)\n\t\t\tgoto next;\n\n\t\tindex = btrfs_bg_flags_to_raid_index(block_group->flags);\n\n\t\tret = insert_block_group_item(trans, block_group);\n\t\tif (ret)\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\tif (!test_bit(BLOCK_GROUP_FLAG_CHUNK_ITEM_INSERTED,\n\t\t\t      &block_group->runtime_flags)) {\n\t\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\t\tret = btrfs_chunk_alloc_add_chunk_item(trans, block_group);\n\t\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t\t\tif (ret)\n\t\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t}\n\t\tret = insert_dev_extents(trans, block_group->start,\n\t\t\t\t\t block_group->length);\n\t\tif (ret)\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\tadd_block_group_free_space(trans, block_group);\n\n\t\t \n\t\tif (block_group->space_info->block_group_kobjs[index] == NULL)\n\t\t\tbtrfs_sysfs_add_block_group_type(block_group);\n\n\t\t \nnext:\n\t\tbtrfs_delayed_refs_rsv_release(fs_info, 1);\n\t\tlist_del_init(&block_group->bg_list);\n\t\tclear_bit(BLOCK_GROUP_FLAG_NEW, &block_group->runtime_flags);\n\t}\n\tbtrfs_trans_release_chunk_metadata(trans);\n}\n\n \nstatic u64 calculate_global_root_id(struct btrfs_fs_info *fs_info, u64 offset)\n{\n\tu64 div = SZ_1G;\n\tu64 index;\n\n\tif (!btrfs_fs_incompat(fs_info, EXTENT_TREE_V2))\n\t\treturn BTRFS_FIRST_CHUNK_TREE_OBJECTID;\n\n\t \n\tif (btrfs_super_total_bytes(fs_info->super_copy) <= (SZ_1G * 10ULL))\n\t\tdiv = SZ_128M;\n\n\toffset = div64_u64(offset, div);\n\tdiv64_u64_rem(offset, fs_info->nr_global_roots, &index);\n\treturn index;\n}\n\nstruct btrfs_block_group *btrfs_make_block_group(struct btrfs_trans_handle *trans,\n\t\t\t\t\t\t u64 type,\n\t\t\t\t\t\t u64 chunk_offset, u64 size)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_block_group *cache;\n\tint ret;\n\n\tbtrfs_set_log_full_commit(trans);\n\n\tcache = btrfs_create_block_group_cache(fs_info, chunk_offset);\n\tif (!cache)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\t \n\tset_bit(BLOCK_GROUP_FLAG_NEW, &cache->runtime_flags);\n\n\tcache->length = size;\n\tset_free_space_tree_thresholds(cache);\n\tcache->flags = type;\n\tcache->cached = BTRFS_CACHE_FINISHED;\n\tcache->global_root_id = calculate_global_root_id(fs_info, cache->start);\n\n\tif (btrfs_fs_compat_ro(fs_info, FREE_SPACE_TREE))\n\t\tset_bit(BLOCK_GROUP_FLAG_NEEDS_FREE_SPACE, &cache->runtime_flags);\n\n\tret = btrfs_load_block_group_zone_info(cache, true);\n\tif (ret) {\n\t\tbtrfs_put_block_group(cache);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\tret = exclude_super_stripes(cache);\n\tif (ret) {\n\t\t \n\t\tbtrfs_free_excluded_extents(cache);\n\t\tbtrfs_put_block_group(cache);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\tret = btrfs_add_new_free_space(cache, chunk_offset, chunk_offset + size, NULL);\n\tbtrfs_free_excluded_extents(cache);\n\tif (ret) {\n\t\tbtrfs_put_block_group(cache);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\t \n\tcache->space_info = btrfs_find_space_info(fs_info, cache->flags);\n\tASSERT(cache->space_info);\n\n\tret = btrfs_add_block_group_cache(fs_info, cache);\n\tif (ret) {\n\t\tbtrfs_remove_free_space_cache(cache);\n\t\tbtrfs_put_block_group(cache);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\t \n\ttrace_btrfs_add_block_group(fs_info, cache, 1);\n\tbtrfs_add_bg_to_space_info(fs_info, cache);\n\tbtrfs_update_global_block_rsv(fs_info);\n\n#ifdef CONFIG_BTRFS_DEBUG\n\tif (btrfs_should_fragment_free_space(cache)) {\n\t\tcache->space_info->bytes_used += size >> 1;\n\t\tfragment_free_space(cache);\n\t}\n#endif\n\n\tlist_add_tail(&cache->bg_list, &trans->new_bgs);\n\ttrans->delayed_ref_updates++;\n\tbtrfs_update_delayed_refs_rsv(trans);\n\n\tset_avail_alloc_bits(fs_info, type);\n\treturn cache;\n}\n\n \nint btrfs_inc_block_group_ro(struct btrfs_block_group *cache,\n\t\t\t     bool do_chunk_alloc)\n{\n\tstruct btrfs_fs_info *fs_info = cache->fs_info;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_root *root = btrfs_block_group_root(fs_info);\n\tu64 alloc_flags;\n\tint ret;\n\tbool dirty_bg_running;\n\n\t \n\tif (sb_rdonly(fs_info->sb)) {\n\t\tmutex_lock(&fs_info->ro_block_group_mutex);\n\t\tret = inc_block_group_ro(cache, 0);\n\t\tmutex_unlock(&fs_info->ro_block_group_mutex);\n\t\treturn ret;\n\t}\n\n\tdo {\n\t\ttrans = btrfs_join_transaction(root);\n\t\tif (IS_ERR(trans))\n\t\t\treturn PTR_ERR(trans);\n\n\t\tdirty_bg_running = false;\n\n\t\t \n\t\tmutex_lock(&fs_info->ro_block_group_mutex);\n\t\tif (test_bit(BTRFS_TRANS_DIRTY_BG_RUN, &trans->transaction->flags)) {\n\t\t\tu64 transid = trans->transid;\n\n\t\t\tmutex_unlock(&fs_info->ro_block_group_mutex);\n\t\t\tbtrfs_end_transaction(trans);\n\n\t\t\tret = btrfs_wait_for_commit(fs_info, transid);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t\tdirty_bg_running = true;\n\t\t}\n\t} while (dirty_bg_running);\n\n\tif (do_chunk_alloc) {\n\t\t \n\t\talloc_flags = btrfs_get_alloc_profile(fs_info, cache->flags);\n\t\tif (alloc_flags != cache->flags) {\n\t\t\tret = btrfs_chunk_alloc(trans, alloc_flags,\n\t\t\t\t\t\tCHUNK_ALLOC_FORCE);\n\t\t\t \n\t\t\tif (ret == -ENOSPC)\n\t\t\t\tret = 0;\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\n\tret = inc_block_group_ro(cache, 0);\n\tif (!ret)\n\t\tgoto out;\n\tif (ret == -ETXTBSY)\n\t\tgoto unlock_out;\n\n\t \n\tif (!do_chunk_alloc && ret == -ENOSPC &&\n\t    (cache->flags & BTRFS_BLOCK_GROUP_SYSTEM))\n\t\tgoto unlock_out;\n\n\talloc_flags = btrfs_get_alloc_profile(fs_info, cache->space_info->flags);\n\tret = btrfs_chunk_alloc(trans, alloc_flags, CHUNK_ALLOC_FORCE);\n\tif (ret < 0)\n\t\tgoto out;\n\t \n\tret = btrfs_zoned_activate_one_bg(fs_info, cache->space_info, true);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tret = inc_block_group_ro(cache, 0);\n\tif (ret == -ETXTBSY)\n\t\tgoto unlock_out;\nout:\n\tif (cache->flags & BTRFS_BLOCK_GROUP_SYSTEM) {\n\t\talloc_flags = btrfs_get_alloc_profile(fs_info, cache->flags);\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tcheck_system_chunk(trans, alloc_flags);\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\nunlock_out:\n\tmutex_unlock(&fs_info->ro_block_group_mutex);\n\n\tbtrfs_end_transaction(trans);\n\treturn ret;\n}\n\nvoid btrfs_dec_block_group_ro(struct btrfs_block_group *cache)\n{\n\tstruct btrfs_space_info *sinfo = cache->space_info;\n\tu64 num_bytes;\n\n\tBUG_ON(!cache->ro);\n\n\tspin_lock(&sinfo->lock);\n\tspin_lock(&cache->lock);\n\tif (!--cache->ro) {\n\t\tif (btrfs_is_zoned(cache->fs_info)) {\n\t\t\t \n\t\t\tcache->zone_unusable =\n\t\t\t\t(cache->alloc_offset - cache->used) +\n\t\t\t\t(cache->length - cache->zone_capacity);\n\t\t\tsinfo->bytes_zone_unusable += cache->zone_unusable;\n\t\t\tsinfo->bytes_readonly -= cache->zone_unusable;\n\t\t}\n\t\tnum_bytes = cache->length - cache->reserved -\n\t\t\t    cache->pinned - cache->bytes_super -\n\t\t\t    cache->zone_unusable - cache->used;\n\t\tsinfo->bytes_readonly -= num_bytes;\n\t\tlist_del_init(&cache->ro_list);\n\t}\n\tspin_unlock(&cache->lock);\n\tspin_unlock(&sinfo->lock);\n}\n\nstatic int update_block_group_item(struct btrfs_trans_handle *trans,\n\t\t\t\t   struct btrfs_path *path,\n\t\t\t\t   struct btrfs_block_group *cache)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tint ret;\n\tstruct btrfs_root *root = btrfs_block_group_root(fs_info);\n\tunsigned long bi;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_block_group_item bgi;\n\tstruct btrfs_key key;\n\tu64 old_commit_used;\n\tu64 used;\n\n\t \n\tspin_lock(&cache->lock);\n\told_commit_used = cache->commit_used;\n\tused = cache->used;\n\t \n\tif (cache->commit_used == used) {\n\t\tspin_unlock(&cache->lock);\n\t\treturn 0;\n\t}\n\tcache->commit_used = used;\n\tspin_unlock(&cache->lock);\n\n\tkey.objectid = cache->start;\n\tkey.type = BTRFS_BLOCK_GROUP_ITEM_KEY;\n\tkey.offset = cache->length;\n\n\tret = btrfs_search_slot(trans, root, &key, path, 0, 1);\n\tif (ret) {\n\t\tif (ret > 0)\n\t\t\tret = -ENOENT;\n\t\tgoto fail;\n\t}\n\n\tleaf = path->nodes[0];\n\tbi = btrfs_item_ptr_offset(leaf, path->slots[0]);\n\tbtrfs_set_stack_block_group_used(&bgi, used);\n\tbtrfs_set_stack_block_group_chunk_objectid(&bgi,\n\t\t\t\t\t\t   cache->global_root_id);\n\tbtrfs_set_stack_block_group_flags(&bgi, cache->flags);\n\twrite_extent_buffer(leaf, &bgi, bi, sizeof(bgi));\n\tbtrfs_mark_buffer_dirty(trans, leaf);\nfail:\n\tbtrfs_release_path(path);\n\t \n\tif (ret < 0 && ret != -ENOENT) {\n\t\tspin_lock(&cache->lock);\n\t\tcache->commit_used = old_commit_used;\n\t\tspin_unlock(&cache->lock);\n\t}\n\treturn ret;\n\n}\n\nstatic int cache_save_setup(struct btrfs_block_group *block_group,\n\t\t\t    struct btrfs_trans_handle *trans,\n\t\t\t    struct btrfs_path *path)\n{\n\tstruct btrfs_fs_info *fs_info = block_group->fs_info;\n\tstruct btrfs_root *root = fs_info->tree_root;\n\tstruct inode *inode = NULL;\n\tstruct extent_changeset *data_reserved = NULL;\n\tu64 alloc_hint = 0;\n\tint dcs = BTRFS_DC_ERROR;\n\tu64 cache_size = 0;\n\tint retries = 0;\n\tint ret = 0;\n\n\tif (!btrfs_test_opt(fs_info, SPACE_CACHE))\n\t\treturn 0;\n\n\t \n\tif (block_group->length < (100 * SZ_1M)) {\n\t\tspin_lock(&block_group->lock);\n\t\tblock_group->disk_cache_state = BTRFS_DC_WRITTEN;\n\t\tspin_unlock(&block_group->lock);\n\t\treturn 0;\n\t}\n\n\tif (TRANS_ABORTED(trans))\n\t\treturn 0;\nagain:\n\tinode = lookup_free_space_inode(block_group, path);\n\tif (IS_ERR(inode) && PTR_ERR(inode) != -ENOENT) {\n\t\tret = PTR_ERR(inode);\n\t\tbtrfs_release_path(path);\n\t\tgoto out;\n\t}\n\n\tif (IS_ERR(inode)) {\n\t\tBUG_ON(retries);\n\t\tretries++;\n\n\t\tif (block_group->ro)\n\t\t\tgoto out_free;\n\n\t\tret = create_free_space_inode(trans, block_group, path);\n\t\tif (ret)\n\t\t\tgoto out_free;\n\t\tgoto again;\n\t}\n\n\t \n\tBTRFS_I(inode)->generation = 0;\n\tret = btrfs_update_inode(trans, root, BTRFS_I(inode));\n\tif (ret) {\n\t\t \n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tgoto out_put;\n\t}\n\tWARN_ON(ret);\n\n\t \n\tif (block_group->cache_generation == trans->transid &&\n\t    i_size_read(inode)) {\n\t\tdcs = BTRFS_DC_SETUP;\n\t\tgoto out_put;\n\t}\n\n\tif (i_size_read(inode) > 0) {\n\t\tret = btrfs_check_trunc_cache_free_space(fs_info,\n\t\t\t\t\t&fs_info->global_block_rsv);\n\t\tif (ret)\n\t\t\tgoto out_put;\n\n\t\tret = btrfs_truncate_free_space_cache(trans, NULL, inode);\n\t\tif (ret)\n\t\t\tgoto out_put;\n\t}\n\n\tspin_lock(&block_group->lock);\n\tif (block_group->cached != BTRFS_CACHE_FINISHED ||\n\t    !btrfs_test_opt(fs_info, SPACE_CACHE)) {\n\t\t \n\t\tdcs = BTRFS_DC_WRITTEN;\n\t\tspin_unlock(&block_group->lock);\n\t\tgoto out_put;\n\t}\n\tspin_unlock(&block_group->lock);\n\n\t \n\tif (test_bit(BTRFS_TRANS_CACHE_ENOSPC, &trans->transaction->flags)) {\n\t\tret = -ENOSPC;\n\t\tgoto out_put;\n\t}\n\n\t \n\tcache_size = div_u64(block_group->length, SZ_256M);\n\tif (!cache_size)\n\t\tcache_size = 1;\n\n\tcache_size *= 16;\n\tcache_size *= fs_info->sectorsize;\n\n\tret = btrfs_check_data_free_space(BTRFS_I(inode), &data_reserved, 0,\n\t\t\t\t\t  cache_size, false);\n\tif (ret)\n\t\tgoto out_put;\n\n\tret = btrfs_prealloc_file_range_trans(inode, trans, 0, 0, cache_size,\n\t\t\t\t\t      cache_size, cache_size,\n\t\t\t\t\t      &alloc_hint);\n\t \n\tif (!ret)\n\t\tdcs = BTRFS_DC_SETUP;\n\telse if (ret == -ENOSPC)\n\t\tset_bit(BTRFS_TRANS_CACHE_ENOSPC, &trans->transaction->flags);\n\nout_put:\n\tiput(inode);\nout_free:\n\tbtrfs_release_path(path);\nout:\n\tspin_lock(&block_group->lock);\n\tif (!ret && dcs == BTRFS_DC_SETUP)\n\t\tblock_group->cache_generation = trans->transid;\n\tblock_group->disk_cache_state = dcs;\n\tspin_unlock(&block_group->lock);\n\n\textent_changeset_free(data_reserved);\n\treturn ret;\n}\n\nint btrfs_setup_space_cache(struct btrfs_trans_handle *trans)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_block_group *cache, *tmp;\n\tstruct btrfs_transaction *cur_trans = trans->transaction;\n\tstruct btrfs_path *path;\n\n\tif (list_empty(&cur_trans->dirty_bgs) ||\n\t    !btrfs_test_opt(fs_info, SPACE_CACHE))\n\t\treturn 0;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\t \n\tlist_for_each_entry_safe(cache, tmp, &cur_trans->dirty_bgs,\n\t\t\t\t dirty_list) {\n\t\tif (cache->disk_cache_state == BTRFS_DC_CLEAR)\n\t\t\tcache_save_setup(cache, trans, path);\n\t}\n\n\tbtrfs_free_path(path);\n\treturn 0;\n}\n\n \nint btrfs_start_dirty_block_groups(struct btrfs_trans_handle *trans)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_block_group *cache;\n\tstruct btrfs_transaction *cur_trans = trans->transaction;\n\tint ret = 0;\n\tint should_put;\n\tstruct btrfs_path *path = NULL;\n\tLIST_HEAD(dirty);\n\tstruct list_head *io = &cur_trans->io_bgs;\n\tint loops = 0;\n\n\tspin_lock(&cur_trans->dirty_bgs_lock);\n\tif (list_empty(&cur_trans->dirty_bgs)) {\n\t\tspin_unlock(&cur_trans->dirty_bgs_lock);\n\t\treturn 0;\n\t}\n\tlist_splice_init(&cur_trans->dirty_bgs, &dirty);\n\tspin_unlock(&cur_trans->dirty_bgs_lock);\n\nagain:\n\t \n\tbtrfs_create_pending_block_groups(trans);\n\n\tif (!path) {\n\t\tpath = btrfs_alloc_path();\n\t\tif (!path) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t \n\tmutex_lock(&trans->transaction->cache_write_mutex);\n\twhile (!list_empty(&dirty)) {\n\t\tbool drop_reserve = true;\n\n\t\tcache = list_first_entry(&dirty, struct btrfs_block_group,\n\t\t\t\t\t dirty_list);\n\t\t \n\t\tif (!list_empty(&cache->io_list)) {\n\t\t\tlist_del_init(&cache->io_list);\n\t\t\tbtrfs_wait_cache_io(trans, cache, path);\n\t\t\tbtrfs_put_block_group(cache);\n\t\t}\n\n\n\t\t \n\t\tspin_lock(&cur_trans->dirty_bgs_lock);\n\t\tlist_del_init(&cache->dirty_list);\n\t\tspin_unlock(&cur_trans->dirty_bgs_lock);\n\n\t\tshould_put = 1;\n\n\t\tcache_save_setup(cache, trans, path);\n\n\t\tif (cache->disk_cache_state == BTRFS_DC_SETUP) {\n\t\t\tcache->io_ctl.inode = NULL;\n\t\t\tret = btrfs_write_out_cache(trans, cache, path);\n\t\t\tif (ret == 0 && cache->io_ctl.inode) {\n\t\t\t\tshould_put = 0;\n\n\t\t\t\t \n\t\t\t\tlist_add_tail(&cache->io_list, io);\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tret = 0;\n\t\t\t}\n\t\t}\n\t\tif (!ret) {\n\t\t\tret = update_block_group_item(trans, path, cache);\n\t\t\t \n\t\t\tif (ret == -ENOENT) {\n\t\t\t\tret = 0;\n\t\t\t\tspin_lock(&cur_trans->dirty_bgs_lock);\n\t\t\t\tif (list_empty(&cache->dirty_list)) {\n\t\t\t\t\tlist_add_tail(&cache->dirty_list,\n\t\t\t\t\t\t      &cur_trans->dirty_bgs);\n\t\t\t\t\tbtrfs_get_block_group(cache);\n\t\t\t\t\tdrop_reserve = false;\n\t\t\t\t}\n\t\t\t\tspin_unlock(&cur_trans->dirty_bgs_lock);\n\t\t\t} else if (ret) {\n\t\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (should_put)\n\t\t\tbtrfs_put_block_group(cache);\n\t\tif (drop_reserve)\n\t\t\tbtrfs_delayed_refs_rsv_release(fs_info, 1);\n\t\t \n\t\tmutex_unlock(&trans->transaction->cache_write_mutex);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\tmutex_lock(&trans->transaction->cache_write_mutex);\n\t}\n\tmutex_unlock(&trans->transaction->cache_write_mutex);\n\n\t \n\tif (!ret)\n\t\tret = btrfs_run_delayed_refs(trans, 0);\n\tif (!ret && loops == 0) {\n\t\tloops++;\n\t\tspin_lock(&cur_trans->dirty_bgs_lock);\n\t\tlist_splice_init(&cur_trans->dirty_bgs, &dirty);\n\t\t \n\t\tif (!list_empty(&dirty)) {\n\t\t\tspin_unlock(&cur_trans->dirty_bgs_lock);\n\t\t\tgoto again;\n\t\t}\n\t\tspin_unlock(&cur_trans->dirty_bgs_lock);\n\t}\nout:\n\tif (ret < 0) {\n\t\tspin_lock(&cur_trans->dirty_bgs_lock);\n\t\tlist_splice_init(&dirty, &cur_trans->dirty_bgs);\n\t\tspin_unlock(&cur_trans->dirty_bgs_lock);\n\t\tbtrfs_cleanup_dirty_bgs(cur_trans, fs_info);\n\t}\n\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nint btrfs_write_dirty_block_groups(struct btrfs_trans_handle *trans)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_block_group *cache;\n\tstruct btrfs_transaction *cur_trans = trans->transaction;\n\tint ret = 0;\n\tint should_put;\n\tstruct btrfs_path *path;\n\tstruct list_head *io = &cur_trans->io_bgs;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\t \n\tspin_lock(&cur_trans->dirty_bgs_lock);\n\twhile (!list_empty(&cur_trans->dirty_bgs)) {\n\t\tcache = list_first_entry(&cur_trans->dirty_bgs,\n\t\t\t\t\t struct btrfs_block_group,\n\t\t\t\t\t dirty_list);\n\n\t\t \n\t\tif (!list_empty(&cache->io_list)) {\n\t\t\tspin_unlock(&cur_trans->dirty_bgs_lock);\n\t\t\tlist_del_init(&cache->io_list);\n\t\t\tbtrfs_wait_cache_io(trans, cache, path);\n\t\t\tbtrfs_put_block_group(cache);\n\t\t\tspin_lock(&cur_trans->dirty_bgs_lock);\n\t\t}\n\n\t\t \n\t\tlist_del_init(&cache->dirty_list);\n\t\tspin_unlock(&cur_trans->dirty_bgs_lock);\n\t\tshould_put = 1;\n\n\t\tcache_save_setup(cache, trans, path);\n\n\t\tif (!ret)\n\t\t\tret = btrfs_run_delayed_refs(trans,\n\t\t\t\t\t\t     (unsigned long) -1);\n\n\t\tif (!ret && cache->disk_cache_state == BTRFS_DC_SETUP) {\n\t\t\tcache->io_ctl.inode = NULL;\n\t\t\tret = btrfs_write_out_cache(trans, cache, path);\n\t\t\tif (ret == 0 && cache->io_ctl.inode) {\n\t\t\t\tshould_put = 0;\n\t\t\t\tlist_add_tail(&cache->io_list, io);\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tret = 0;\n\t\t\t}\n\t\t}\n\t\tif (!ret) {\n\t\t\tret = update_block_group_item(trans, path, cache);\n\t\t\t \n\t\t\tif (ret == -ENOENT) {\n\t\t\t\twait_event(cur_trans->writer_wait,\n\t\t\t\t   atomic_read(&cur_trans->num_writers) == 1);\n\t\t\t\tret = update_block_group_item(trans, path, cache);\n\t\t\t}\n\t\t\tif (ret)\n\t\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t}\n\n\t\t \n\t\tif (should_put)\n\t\t\tbtrfs_put_block_group(cache);\n\t\tbtrfs_delayed_refs_rsv_release(fs_info, 1);\n\t\tspin_lock(&cur_trans->dirty_bgs_lock);\n\t}\n\tspin_unlock(&cur_trans->dirty_bgs_lock);\n\n\t \n\twhile (!list_empty(io)) {\n\t\tcache = list_first_entry(io, struct btrfs_block_group,\n\t\t\t\t\t io_list);\n\t\tlist_del_init(&cache->io_list);\n\t\tbtrfs_wait_cache_io(trans, cache, path);\n\t\tbtrfs_put_block_group(cache);\n\t}\n\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nint btrfs_update_block_group(struct btrfs_trans_handle *trans,\n\t\t\t     u64 bytenr, u64 num_bytes, bool alloc)\n{\n\tstruct btrfs_fs_info *info = trans->fs_info;\n\tstruct btrfs_block_group *cache = NULL;\n\tu64 total = num_bytes;\n\tu64 old_val;\n\tu64 byte_in_group;\n\tint factor;\n\tint ret = 0;\n\n\t \n\tspin_lock(&info->delalloc_root_lock);\n\told_val = btrfs_super_bytes_used(info->super_copy);\n\tif (alloc)\n\t\told_val += num_bytes;\n\telse\n\t\told_val -= num_bytes;\n\tbtrfs_set_super_bytes_used(info->super_copy, old_val);\n\tspin_unlock(&info->delalloc_root_lock);\n\n\twhile (total) {\n\t\tstruct btrfs_space_info *space_info;\n\t\tbool reclaim = false;\n\n\t\tcache = btrfs_lookup_block_group(info, bytenr);\n\t\tif (!cache) {\n\t\t\tret = -ENOENT;\n\t\t\tbreak;\n\t\t}\n\t\tspace_info = cache->space_info;\n\t\tfactor = btrfs_bg_type_to_factor(cache->flags);\n\n\t\t \n\t\tif (!alloc && !btrfs_block_group_done(cache))\n\t\t\tbtrfs_cache_block_group(cache, true);\n\n\t\tbyte_in_group = bytenr - cache->start;\n\t\tWARN_ON(byte_in_group > cache->length);\n\n\t\tspin_lock(&space_info->lock);\n\t\tspin_lock(&cache->lock);\n\n\t\tif (btrfs_test_opt(info, SPACE_CACHE) &&\n\t\t    cache->disk_cache_state < BTRFS_DC_CLEAR)\n\t\t\tcache->disk_cache_state = BTRFS_DC_CLEAR;\n\n\t\told_val = cache->used;\n\t\tnum_bytes = min(total, cache->length - byte_in_group);\n\t\tif (alloc) {\n\t\t\told_val += num_bytes;\n\t\t\tcache->used = old_val;\n\t\t\tcache->reserved -= num_bytes;\n\t\t\tspace_info->bytes_reserved -= num_bytes;\n\t\t\tspace_info->bytes_used += num_bytes;\n\t\t\tspace_info->disk_used += num_bytes * factor;\n\t\t\tspin_unlock(&cache->lock);\n\t\t\tspin_unlock(&space_info->lock);\n\t\t} else {\n\t\t\told_val -= num_bytes;\n\t\t\tcache->used = old_val;\n\t\t\tcache->pinned += num_bytes;\n\t\t\tbtrfs_space_info_update_bytes_pinned(info, space_info,\n\t\t\t\t\t\t\t     num_bytes);\n\t\t\tspace_info->bytes_used -= num_bytes;\n\t\t\tspace_info->disk_used -= num_bytes * factor;\n\n\t\t\treclaim = should_reclaim_block_group(cache, num_bytes);\n\n\t\t\tspin_unlock(&cache->lock);\n\t\t\tspin_unlock(&space_info->lock);\n\n\t\t\tset_extent_bit(&trans->transaction->pinned_extents,\n\t\t\t\t       bytenr, bytenr + num_bytes - 1,\n\t\t\t\t       EXTENT_DIRTY, NULL);\n\t\t}\n\n\t\tspin_lock(&trans->transaction->dirty_bgs_lock);\n\t\tif (list_empty(&cache->dirty_list)) {\n\t\t\tlist_add_tail(&cache->dirty_list,\n\t\t\t\t      &trans->transaction->dirty_bgs);\n\t\t\ttrans->delayed_ref_updates++;\n\t\t\tbtrfs_get_block_group(cache);\n\t\t}\n\t\tspin_unlock(&trans->transaction->dirty_bgs_lock);\n\n\t\t \n\t\tif (!alloc && old_val == 0) {\n\t\t\tif (!btrfs_test_opt(info, DISCARD_ASYNC))\n\t\t\t\tbtrfs_mark_bg_unused(cache);\n\t\t} else if (!alloc && reclaim) {\n\t\t\tbtrfs_mark_bg_to_reclaim(cache);\n\t\t}\n\n\t\tbtrfs_put_block_group(cache);\n\t\ttotal -= num_bytes;\n\t\tbytenr += num_bytes;\n\t}\n\n\t \n\tbtrfs_update_delayed_refs_rsv(trans);\n\treturn ret;\n}\n\n \nint btrfs_add_reserved_bytes(struct btrfs_block_group *cache,\n\t\t\t     u64 ram_bytes, u64 num_bytes, int delalloc,\n\t\t\t     bool force_wrong_size_class)\n{\n\tstruct btrfs_space_info *space_info = cache->space_info;\n\tenum btrfs_block_group_size_class size_class;\n\tint ret = 0;\n\n\tspin_lock(&space_info->lock);\n\tspin_lock(&cache->lock);\n\tif (cache->ro) {\n\t\tret = -EAGAIN;\n\t\tgoto out;\n\t}\n\n\tif (btrfs_block_group_should_use_size_class(cache)) {\n\t\tsize_class = btrfs_calc_block_group_size_class(num_bytes);\n\t\tret = btrfs_use_block_group_size_class(cache, size_class, force_wrong_size_class);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\tcache->reserved += num_bytes;\n\tspace_info->bytes_reserved += num_bytes;\n\ttrace_btrfs_space_reservation(cache->fs_info, \"space_info\",\n\t\t\t\t      space_info->flags, num_bytes, 1);\n\tbtrfs_space_info_update_bytes_may_use(cache->fs_info,\n\t\t\t\t\t      space_info, -ram_bytes);\n\tif (delalloc)\n\t\tcache->delalloc_bytes += num_bytes;\n\n\t \n\tif (num_bytes < ram_bytes)\n\t\tbtrfs_try_granting_tickets(cache->fs_info, space_info);\nout:\n\tspin_unlock(&cache->lock);\n\tspin_unlock(&space_info->lock);\n\treturn ret;\n}\n\n \nvoid btrfs_free_reserved_bytes(struct btrfs_block_group *cache,\n\t\t\t       u64 num_bytes, int delalloc)\n{\n\tstruct btrfs_space_info *space_info = cache->space_info;\n\n\tspin_lock(&space_info->lock);\n\tspin_lock(&cache->lock);\n\tif (cache->ro)\n\t\tspace_info->bytes_readonly += num_bytes;\n\tcache->reserved -= num_bytes;\n\tspace_info->bytes_reserved -= num_bytes;\n\tspace_info->max_extent_size = 0;\n\n\tif (delalloc)\n\t\tcache->delalloc_bytes -= num_bytes;\n\tspin_unlock(&cache->lock);\n\n\tbtrfs_try_granting_tickets(cache->fs_info, space_info);\n\tspin_unlock(&space_info->lock);\n}\n\nstatic void force_metadata_allocation(struct btrfs_fs_info *info)\n{\n\tstruct list_head *head = &info->space_info;\n\tstruct btrfs_space_info *found;\n\n\tlist_for_each_entry(found, head, list) {\n\t\tif (found->flags & BTRFS_BLOCK_GROUP_METADATA)\n\t\t\tfound->force_alloc = CHUNK_ALLOC_FORCE;\n\t}\n}\n\nstatic int should_alloc_chunk(struct btrfs_fs_info *fs_info,\n\t\t\t      struct btrfs_space_info *sinfo, int force)\n{\n\tu64 bytes_used = btrfs_space_info_used(sinfo, false);\n\tu64 thresh;\n\n\tif (force == CHUNK_ALLOC_FORCE)\n\t\treturn 1;\n\n\t \n\tif (force == CHUNK_ALLOC_LIMITED) {\n\t\tthresh = btrfs_super_total_bytes(fs_info->super_copy);\n\t\tthresh = max_t(u64, SZ_64M, mult_perc(thresh, 1));\n\n\t\tif (sinfo->total_bytes - bytes_used < thresh)\n\t\t\treturn 1;\n\t}\n\n\tif (bytes_used + SZ_2M < mult_perc(sinfo->total_bytes, 80))\n\t\treturn 0;\n\treturn 1;\n}\n\nint btrfs_force_chunk_alloc(struct btrfs_trans_handle *trans, u64 type)\n{\n\tu64 alloc_flags = btrfs_get_alloc_profile(trans->fs_info, type);\n\n\treturn btrfs_chunk_alloc(trans, alloc_flags, CHUNK_ALLOC_FORCE);\n}\n\nstatic struct btrfs_block_group *do_chunk_alloc(struct btrfs_trans_handle *trans, u64 flags)\n{\n\tstruct btrfs_block_group *bg;\n\tint ret;\n\n\t \n\tcheck_system_chunk(trans, flags);\n\n\tbg = btrfs_create_chunk(trans, flags);\n\tif (IS_ERR(bg)) {\n\t\tret = PTR_ERR(bg);\n\t\tgoto out;\n\t}\n\n\tret = btrfs_chunk_alloc_add_chunk_item(trans, bg);\n\t \n\tif (ret == -ENOSPC) {\n\t\tconst u64 sys_flags = btrfs_system_alloc_profile(trans->fs_info);\n\t\tstruct btrfs_block_group *sys_bg;\n\n\t\tsys_bg = btrfs_create_chunk(trans, sys_flags);\n\t\tif (IS_ERR(sys_bg)) {\n\t\t\tret = PTR_ERR(sys_bg);\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tgoto out;\n\t\t}\n\n\t\tret = btrfs_chunk_alloc_add_chunk_item(trans, sys_bg);\n\t\tif (ret) {\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tgoto out;\n\t\t}\n\n\t\tret = btrfs_chunk_alloc_add_chunk_item(trans, bg);\n\t\tif (ret) {\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tgoto out;\n\t\t}\n\t} else if (ret) {\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tgoto out;\n\t}\nout:\n\tbtrfs_trans_release_chunk_metadata(trans);\n\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\n\tbtrfs_get_block_group(bg);\n\treturn bg;\n}\n\n \nint btrfs_chunk_alloc(struct btrfs_trans_handle *trans, u64 flags,\n\t\t      enum btrfs_chunk_alloc_enum force)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_space_info *space_info;\n\tstruct btrfs_block_group *ret_bg;\n\tbool wait_for_alloc = false;\n\tbool should_alloc = false;\n\tbool from_extent_allocation = false;\n\tint ret = 0;\n\n\tif (force == CHUNK_ALLOC_FORCE_FOR_EXTENT) {\n\t\tfrom_extent_allocation = true;\n\t\tforce = CHUNK_ALLOC_FORCE;\n\t}\n\n\t \n\tif (trans->allocating_chunk)\n\t\treturn -ENOSPC;\n\t \n\tif (flags & BTRFS_BLOCK_GROUP_SYSTEM)\n\t\treturn -ENOSPC;\n\n\tspace_info = btrfs_find_space_info(fs_info, flags);\n\tASSERT(space_info);\n\n\tdo {\n\t\tspin_lock(&space_info->lock);\n\t\tif (force < space_info->force_alloc)\n\t\t\tforce = space_info->force_alloc;\n\t\tshould_alloc = should_alloc_chunk(fs_info, space_info, force);\n\t\tif (space_info->full) {\n\t\t\t \n\t\t\tif (should_alloc)\n\t\t\t\tret = -ENOSPC;\n\t\t\telse\n\t\t\t\tret = 0;\n\t\t\tspin_unlock(&space_info->lock);\n\t\t\treturn ret;\n\t\t} else if (!should_alloc) {\n\t\t\tspin_unlock(&space_info->lock);\n\t\t\treturn 0;\n\t\t} else if (space_info->chunk_alloc) {\n\t\t\t \n\t\t\twait_for_alloc = true;\n\t\t\tforce = CHUNK_ALLOC_NO_FORCE;\n\t\t\tspin_unlock(&space_info->lock);\n\t\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t\t} else {\n\t\t\t \n\t\t\tspace_info->chunk_alloc = 1;\n\t\t\twait_for_alloc = false;\n\t\t\tspin_unlock(&space_info->lock);\n\t\t}\n\n\t\tcond_resched();\n\t} while (wait_for_alloc);\n\n\tmutex_lock(&fs_info->chunk_mutex);\n\ttrans->allocating_chunk = true;\n\n\t \n\tif (btrfs_mixed_space_info(space_info))\n\t\tflags |= (BTRFS_BLOCK_GROUP_DATA | BTRFS_BLOCK_GROUP_METADATA);\n\n\t \n\tif (flags & BTRFS_BLOCK_GROUP_DATA && fs_info->metadata_ratio) {\n\t\tfs_info->data_chunk_allocations++;\n\t\tif (!(fs_info->data_chunk_allocations %\n\t\t      fs_info->metadata_ratio))\n\t\t\tforce_metadata_allocation(fs_info);\n\t}\n\n\tret_bg = do_chunk_alloc(trans, flags);\n\ttrans->allocating_chunk = false;\n\n\tif (IS_ERR(ret_bg)) {\n\t\tret = PTR_ERR(ret_bg);\n\t} else if (from_extent_allocation && (flags & BTRFS_BLOCK_GROUP_DATA)) {\n\t\t \n\t\tbtrfs_zone_activate(ret_bg);\n\t}\n\n\tif (!ret)\n\t\tbtrfs_put_block_group(ret_bg);\n\n\tspin_lock(&space_info->lock);\n\tif (ret < 0) {\n\t\tif (ret == -ENOSPC)\n\t\t\tspace_info->full = 1;\n\t\telse\n\t\t\tgoto out;\n\t} else {\n\t\tret = 1;\n\t\tspace_info->max_extent_size = 0;\n\t}\n\n\tspace_info->force_alloc = CHUNK_ALLOC_NO_FORCE;\nout:\n\tspace_info->chunk_alloc = 0;\n\tspin_unlock(&space_info->lock);\n\tmutex_unlock(&fs_info->chunk_mutex);\n\n\treturn ret;\n}\n\nstatic u64 get_profile_num_devs(struct btrfs_fs_info *fs_info, u64 type)\n{\n\tu64 num_dev;\n\n\tnum_dev = btrfs_raid_array[btrfs_bg_flags_to_raid_index(type)].devs_max;\n\tif (!num_dev)\n\t\tnum_dev = fs_info->fs_devices->rw_devices;\n\n\treturn num_dev;\n}\n\nstatic void reserve_chunk_space(struct btrfs_trans_handle *trans,\n\t\t\t\tu64 bytes,\n\t\t\t\tu64 type)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_space_info *info;\n\tu64 left;\n\tint ret = 0;\n\n\t \n\tlockdep_assert_held(&fs_info->chunk_mutex);\n\n\tinfo = btrfs_find_space_info(fs_info, BTRFS_BLOCK_GROUP_SYSTEM);\n\tspin_lock(&info->lock);\n\tleft = info->total_bytes - btrfs_space_info_used(info, true);\n\tspin_unlock(&info->lock);\n\n\tif (left < bytes && btrfs_test_opt(fs_info, ENOSPC_DEBUG)) {\n\t\tbtrfs_info(fs_info, \"left=%llu, need=%llu, flags=%llu\",\n\t\t\t   left, bytes, type);\n\t\tbtrfs_dump_space_info(fs_info, info, 0, 0);\n\t}\n\n\tif (left < bytes) {\n\t\tu64 flags = btrfs_system_alloc_profile(fs_info);\n\t\tstruct btrfs_block_group *bg;\n\n\t\t \n\t\tbg = btrfs_create_chunk(trans, flags);\n\t\tif (IS_ERR(bg)) {\n\t\t\tret = PTR_ERR(bg);\n\t\t} else {\n\t\t\t \n\t\t\tret = btrfs_zoned_activate_one_bg(fs_info, info, true);\n\t\t\tif (ret < 0)\n\t\t\t\treturn;\n\n\t\t\t \n\t\t\tbtrfs_chunk_alloc_add_chunk_item(trans, bg);\n\t\t}\n\t}\n\n\tif (!ret) {\n\t\tret = btrfs_block_rsv_add(fs_info,\n\t\t\t\t\t  &fs_info->chunk_block_rsv,\n\t\t\t\t\t  bytes, BTRFS_RESERVE_NO_FLUSH);\n\t\tif (!ret)\n\t\t\ttrans->chunk_bytes_reserved += bytes;\n\t}\n}\n\n \nvoid check_system_chunk(struct btrfs_trans_handle *trans, u64 type)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tconst u64 num_devs = get_profile_num_devs(fs_info, type);\n\tu64 bytes;\n\n\t \n\tbytes = btrfs_calc_metadata_size(fs_info, num_devs) +\n\t\tbtrfs_calc_insert_metadata_size(fs_info, 1);\n\n\treserve_chunk_space(trans, bytes, type);\n}\n\n \nvoid btrfs_reserve_chunk_metadata(struct btrfs_trans_handle *trans,\n\t\t\t\t  bool is_item_insertion)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tu64 bytes;\n\n\tif (is_item_insertion)\n\t\tbytes = btrfs_calc_insert_metadata_size(fs_info, 1);\n\telse\n\t\tbytes = btrfs_calc_metadata_size(fs_info, 1);\n\n\tmutex_lock(&fs_info->chunk_mutex);\n\treserve_chunk_space(trans, bytes, BTRFS_BLOCK_GROUP_SYSTEM);\n\tmutex_unlock(&fs_info->chunk_mutex);\n}\n\nvoid btrfs_put_block_group_cache(struct btrfs_fs_info *info)\n{\n\tstruct btrfs_block_group *block_group;\n\n\tblock_group = btrfs_lookup_first_block_group(info, 0);\n\twhile (block_group) {\n\t\tbtrfs_wait_block_group_cache_done(block_group);\n\t\tspin_lock(&block_group->lock);\n\t\tif (test_and_clear_bit(BLOCK_GROUP_FLAG_IREF,\n\t\t\t\t       &block_group->runtime_flags)) {\n\t\t\tstruct inode *inode = block_group->inode;\n\n\t\t\tblock_group->inode = NULL;\n\t\t\tspin_unlock(&block_group->lock);\n\n\t\t\tASSERT(block_group->io_ctl.inode == NULL);\n\t\t\tiput(inode);\n\t\t} else {\n\t\t\tspin_unlock(&block_group->lock);\n\t\t}\n\t\tblock_group = btrfs_next_block_group(block_group);\n\t}\n}\n\n \nint btrfs_free_block_groups(struct btrfs_fs_info *info)\n{\n\tstruct btrfs_block_group *block_group;\n\tstruct btrfs_space_info *space_info;\n\tstruct btrfs_caching_control *caching_ctl;\n\tstruct rb_node *n;\n\n\tif (btrfs_is_zoned(info)) {\n\t\tif (info->active_meta_bg) {\n\t\t\tbtrfs_put_block_group(info->active_meta_bg);\n\t\t\tinfo->active_meta_bg = NULL;\n\t\t}\n\t\tif (info->active_system_bg) {\n\t\t\tbtrfs_put_block_group(info->active_system_bg);\n\t\t\tinfo->active_system_bg = NULL;\n\t\t}\n\t}\n\n\twrite_lock(&info->block_group_cache_lock);\n\twhile (!list_empty(&info->caching_block_groups)) {\n\t\tcaching_ctl = list_entry(info->caching_block_groups.next,\n\t\t\t\t\t struct btrfs_caching_control, list);\n\t\tlist_del(&caching_ctl->list);\n\t\tbtrfs_put_caching_control(caching_ctl);\n\t}\n\twrite_unlock(&info->block_group_cache_lock);\n\n\tspin_lock(&info->unused_bgs_lock);\n\twhile (!list_empty(&info->unused_bgs)) {\n\t\tblock_group = list_first_entry(&info->unused_bgs,\n\t\t\t\t\t       struct btrfs_block_group,\n\t\t\t\t\t       bg_list);\n\t\tlist_del_init(&block_group->bg_list);\n\t\tbtrfs_put_block_group(block_group);\n\t}\n\n\twhile (!list_empty(&info->reclaim_bgs)) {\n\t\tblock_group = list_first_entry(&info->reclaim_bgs,\n\t\t\t\t\t       struct btrfs_block_group,\n\t\t\t\t\t       bg_list);\n\t\tlist_del_init(&block_group->bg_list);\n\t\tbtrfs_put_block_group(block_group);\n\t}\n\tspin_unlock(&info->unused_bgs_lock);\n\n\tspin_lock(&info->zone_active_bgs_lock);\n\twhile (!list_empty(&info->zone_active_bgs)) {\n\t\tblock_group = list_first_entry(&info->zone_active_bgs,\n\t\t\t\t\t       struct btrfs_block_group,\n\t\t\t\t\t       active_bg_list);\n\t\tlist_del_init(&block_group->active_bg_list);\n\t\tbtrfs_put_block_group(block_group);\n\t}\n\tspin_unlock(&info->zone_active_bgs_lock);\n\n\twrite_lock(&info->block_group_cache_lock);\n\twhile ((n = rb_last(&info->block_group_cache_tree.rb_root)) != NULL) {\n\t\tblock_group = rb_entry(n, struct btrfs_block_group,\n\t\t\t\t       cache_node);\n\t\trb_erase_cached(&block_group->cache_node,\n\t\t\t\t&info->block_group_cache_tree);\n\t\tRB_CLEAR_NODE(&block_group->cache_node);\n\t\twrite_unlock(&info->block_group_cache_lock);\n\n\t\tdown_write(&block_group->space_info->groups_sem);\n\t\tlist_del(&block_group->list);\n\t\tup_write(&block_group->space_info->groups_sem);\n\n\t\t \n\t\tif (block_group->cached == BTRFS_CACHE_NO ||\n\t\t    block_group->cached == BTRFS_CACHE_ERROR)\n\t\t\tbtrfs_free_excluded_extents(block_group);\n\n\t\tbtrfs_remove_free_space_cache(block_group);\n\t\tASSERT(block_group->cached != BTRFS_CACHE_STARTED);\n\t\tASSERT(list_empty(&block_group->dirty_list));\n\t\tASSERT(list_empty(&block_group->io_list));\n\t\tASSERT(list_empty(&block_group->bg_list));\n\t\tASSERT(refcount_read(&block_group->refs) == 1);\n\t\tASSERT(block_group->swap_extents == 0);\n\t\tbtrfs_put_block_group(block_group);\n\n\t\twrite_lock(&info->block_group_cache_lock);\n\t}\n\twrite_unlock(&info->block_group_cache_lock);\n\n\tbtrfs_release_global_block_rsv(info);\n\n\twhile (!list_empty(&info->space_info)) {\n\t\tspace_info = list_entry(info->space_info.next,\n\t\t\t\t\tstruct btrfs_space_info,\n\t\t\t\t\tlist);\n\n\t\t \n\t\tif (WARN_ON(space_info->bytes_pinned > 0 ||\n\t\t\t    space_info->bytes_may_use > 0))\n\t\t\tbtrfs_dump_space_info(info, space_info, 0, 0);\n\n\t\t \n\t\tif (!(space_info->flags & BTRFS_BLOCK_GROUP_METADATA) ||\n\t\t    !BTRFS_FS_LOG_CLEANUP_ERROR(info)) {\n\t\t\tif (WARN_ON(space_info->bytes_reserved > 0))\n\t\t\t\tbtrfs_dump_space_info(info, space_info, 0, 0);\n\t\t}\n\n\t\tWARN_ON(space_info->reclaim_size > 0);\n\t\tlist_del(&space_info->list);\n\t\tbtrfs_sysfs_remove_space_info(space_info);\n\t}\n\treturn 0;\n}\n\nvoid btrfs_freeze_block_group(struct btrfs_block_group *cache)\n{\n\tatomic_inc(&cache->frozen);\n}\n\nvoid btrfs_unfreeze_block_group(struct btrfs_block_group *block_group)\n{\n\tstruct btrfs_fs_info *fs_info = block_group->fs_info;\n\tstruct extent_map_tree *em_tree;\n\tstruct extent_map *em;\n\tbool cleanup;\n\n\tspin_lock(&block_group->lock);\n\tcleanup = (atomic_dec_and_test(&block_group->frozen) &&\n\t\t   test_bit(BLOCK_GROUP_FLAG_REMOVED, &block_group->runtime_flags));\n\tspin_unlock(&block_group->lock);\n\n\tif (cleanup) {\n\t\tem_tree = &fs_info->mapping_tree;\n\t\twrite_lock(&em_tree->lock);\n\t\tem = lookup_extent_mapping(em_tree, block_group->start,\n\t\t\t\t\t   1);\n\t\tBUG_ON(!em);  \n\t\tremove_extent_mapping(em_tree, em);\n\t\twrite_unlock(&em_tree->lock);\n\n\t\t \n\t\tfree_extent_map(em);\n\t\tfree_extent_map(em);\n\n\t\t \n\t\tbtrfs_remove_free_space_cache(block_group);\n\t}\n}\n\nbool btrfs_inc_block_group_swap_extents(struct btrfs_block_group *bg)\n{\n\tbool ret = true;\n\n\tspin_lock(&bg->lock);\n\tif (bg->ro)\n\t\tret = false;\n\telse\n\t\tbg->swap_extents++;\n\tspin_unlock(&bg->lock);\n\n\treturn ret;\n}\n\nvoid btrfs_dec_block_group_swap_extents(struct btrfs_block_group *bg, int amount)\n{\n\tspin_lock(&bg->lock);\n\tASSERT(!bg->ro);\n\tASSERT(bg->swap_extents >= amount);\n\tbg->swap_extents -= amount;\n\tspin_unlock(&bg->lock);\n}\n\nenum btrfs_block_group_size_class btrfs_calc_block_group_size_class(u64 size)\n{\n\tif (size <= SZ_128K)\n\t\treturn BTRFS_BG_SZ_SMALL;\n\tif (size <= SZ_8M)\n\t\treturn BTRFS_BG_SZ_MEDIUM;\n\treturn BTRFS_BG_SZ_LARGE;\n}\n\n \nint btrfs_use_block_group_size_class(struct btrfs_block_group *bg,\n\t\t\t\t     enum btrfs_block_group_size_class size_class,\n\t\t\t\t     bool force_wrong_size_class)\n{\n\tASSERT(size_class != BTRFS_BG_SZ_NONE);\n\n\t \n\tif (bg->size_class == size_class)\n\t\treturn 0;\n\t \n\tif (bg->size_class != BTRFS_BG_SZ_NONE) {\n\t\tif (force_wrong_size_class)\n\t\t\treturn 0;\n\t\treturn -EAGAIN;\n\t}\n\t \n\tbg->size_class = size_class;\n\n\treturn 0;\n}\n\nbool btrfs_block_group_should_use_size_class(struct btrfs_block_group *bg)\n{\n\tif (btrfs_is_zoned(bg->fs_info))\n\t\treturn false;\n\tif (!btrfs_is_block_group_data_only(bg))\n\t\treturn false;\n\treturn true;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}