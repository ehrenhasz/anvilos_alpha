{
  "module_name": "async-thread.c",
  "hash_id": "78bf98b4daff76770057586ce87f1a6b20a7e97bf029d60881159a696a5894eb",
  "original_prompt": "Ingested from linux-6.6.14/fs/btrfs/async-thread.c",
  "human_readable_source": "\n \n\n#include <linux/kthread.h>\n#include <linux/slab.h>\n#include <linux/list.h>\n#include <linux/spinlock.h>\n#include <linux/freezer.h>\n#include \"async-thread.h\"\n#include \"ctree.h\"\n\nenum {\n\tWORK_DONE_BIT,\n\tWORK_ORDER_DONE_BIT,\n};\n\n#define NO_THRESHOLD (-1)\n#define DFT_THRESHOLD (32)\n\nstruct btrfs_workqueue {\n\tstruct workqueue_struct *normal_wq;\n\n\t \n\tstruct btrfs_fs_info *fs_info;\n\n\t \n\tstruct list_head ordered_list;\n\n\t \n\tspinlock_t list_lock;\n\n\t \n\tatomic_t pending;\n\n\t \n\tint limit_active;\n\n\t \n\tint current_active;\n\n\t \n\tint thresh;\n\tunsigned int count;\n\tspinlock_t thres_lock;\n};\n\nstruct btrfs_fs_info * __pure btrfs_workqueue_owner(const struct btrfs_workqueue *wq)\n{\n\treturn wq->fs_info;\n}\n\nstruct btrfs_fs_info * __pure btrfs_work_owner(const struct btrfs_work *work)\n{\n\treturn work->wq->fs_info;\n}\n\nbool btrfs_workqueue_normal_congested(const struct btrfs_workqueue *wq)\n{\n\t \n\tif (wq->thresh == NO_THRESHOLD)\n\t\treturn false;\n\n\treturn atomic_read(&wq->pending) > wq->thresh * 2;\n}\n\nstatic void btrfs_init_workqueue(struct btrfs_workqueue *wq,\n\t\t\t\t struct btrfs_fs_info *fs_info)\n{\n\twq->fs_info = fs_info;\n\tatomic_set(&wq->pending, 0);\n\tINIT_LIST_HEAD(&wq->ordered_list);\n\tspin_lock_init(&wq->list_lock);\n\tspin_lock_init(&wq->thres_lock);\n}\n\nstruct btrfs_workqueue *btrfs_alloc_workqueue(struct btrfs_fs_info *fs_info,\n\t\t\t\t\t      const char *name, unsigned int flags,\n\t\t\t\t\t      int limit_active, int thresh)\n{\n\tstruct btrfs_workqueue *ret = kzalloc(sizeof(*ret), GFP_KERNEL);\n\n\tif (!ret)\n\t\treturn NULL;\n\n\tbtrfs_init_workqueue(ret, fs_info);\n\n\tret->limit_active = limit_active;\n\tif (thresh == 0)\n\t\tthresh = DFT_THRESHOLD;\n\t \n\tif (thresh < DFT_THRESHOLD) {\n\t\tret->current_active = limit_active;\n\t\tret->thresh = NO_THRESHOLD;\n\t} else {\n\t\t \n\t\tret->current_active = 1;\n\t\tret->thresh = thresh;\n\t}\n\n\tret->normal_wq = alloc_workqueue(\"btrfs-%s\", flags, ret->current_active,\n\t\t\t\t\t name);\n\tif (!ret->normal_wq) {\n\t\tkfree(ret);\n\t\treturn NULL;\n\t}\n\n\ttrace_btrfs_workqueue_alloc(ret, name);\n\treturn ret;\n}\n\nstruct btrfs_workqueue *btrfs_alloc_ordered_workqueue(\n\t\t\t\tstruct btrfs_fs_info *fs_info, const char *name,\n\t\t\t\tunsigned int flags)\n{\n\tstruct btrfs_workqueue *ret;\n\n\tret = kzalloc(sizeof(*ret), GFP_KERNEL);\n\tif (!ret)\n\t\treturn NULL;\n\n\tbtrfs_init_workqueue(ret, fs_info);\n\n\t \n\tret->limit_active = 1;\n\tret->current_active = 1;\n\tret->thresh = NO_THRESHOLD;\n\n\tret->normal_wq = alloc_ordered_workqueue(\"btrfs-%s\", flags, name);\n\tif (!ret->normal_wq) {\n\t\tkfree(ret);\n\t\treturn NULL;\n\t}\n\n\ttrace_btrfs_workqueue_alloc(ret, name);\n\treturn ret;\n}\n\n \nstatic inline void thresh_queue_hook(struct btrfs_workqueue *wq)\n{\n\tif (wq->thresh == NO_THRESHOLD)\n\t\treturn;\n\tatomic_inc(&wq->pending);\n}\n\n \nstatic inline void thresh_exec_hook(struct btrfs_workqueue *wq)\n{\n\tint new_current_active;\n\tlong pending;\n\tint need_change = 0;\n\n\tif (wq->thresh == NO_THRESHOLD)\n\t\treturn;\n\n\tatomic_dec(&wq->pending);\n\tspin_lock(&wq->thres_lock);\n\t \n\twq->count++;\n\twq->count %= (wq->thresh / 4);\n\tif (!wq->count)\n\t\tgoto  out;\n\tnew_current_active = wq->current_active;\n\n\t \n\tpending = atomic_read(&wq->pending);\n\tif (pending > wq->thresh)\n\t\tnew_current_active++;\n\tif (pending < wq->thresh / 2)\n\t\tnew_current_active--;\n\tnew_current_active = clamp_val(new_current_active, 1, wq->limit_active);\n\tif (new_current_active != wq->current_active)  {\n\t\tneed_change = 1;\n\t\twq->current_active = new_current_active;\n\t}\nout:\n\tspin_unlock(&wq->thres_lock);\n\n\tif (need_change) {\n\t\tworkqueue_set_max_active(wq->normal_wq, wq->current_active);\n\t}\n}\n\nstatic void run_ordered_work(struct btrfs_workqueue *wq,\n\t\t\t     struct btrfs_work *self)\n{\n\tstruct list_head *list = &wq->ordered_list;\n\tstruct btrfs_work *work;\n\tspinlock_t *lock = &wq->list_lock;\n\tunsigned long flags;\n\tbool free_self = false;\n\n\twhile (1) {\n\t\tspin_lock_irqsave(lock, flags);\n\t\tif (list_empty(list))\n\t\t\tbreak;\n\t\twork = list_entry(list->next, struct btrfs_work,\n\t\t\t\t  ordered_list);\n\t\tif (!test_bit(WORK_DONE_BIT, &work->flags))\n\t\t\tbreak;\n\t\t \n\t\tsmp_rmb();\n\n\t\t \n\t\tif (test_and_set_bit(WORK_ORDER_DONE_BIT, &work->flags))\n\t\t\tbreak;\n\t\ttrace_btrfs_ordered_sched(work);\n\t\tspin_unlock_irqrestore(lock, flags);\n\t\twork->ordered_func(work);\n\n\t\t \n\t\tspin_lock_irqsave(lock, flags);\n\t\tlist_del(&work->ordered_list);\n\t\tspin_unlock_irqrestore(lock, flags);\n\n\t\tif (work == self) {\n\t\t\t \n\t\t\tfree_self = true;\n\t\t} else {\n\t\t\t \n\t\t\twork->ordered_free(work);\n\t\t\t \n\t\t\ttrace_btrfs_all_work_done(wq->fs_info, work);\n\t\t}\n\t}\n\tspin_unlock_irqrestore(lock, flags);\n\n\tif (free_self) {\n\t\tself->ordered_free(self);\n\t\t \n\t\ttrace_btrfs_all_work_done(wq->fs_info, self);\n\t}\n}\n\nstatic void btrfs_work_helper(struct work_struct *normal_work)\n{\n\tstruct btrfs_work *work = container_of(normal_work, struct btrfs_work,\n\t\t\t\t\t       normal_work);\n\tstruct btrfs_workqueue *wq = work->wq;\n\tint need_order = 0;\n\n\t \n\tif (work->ordered_func)\n\t\tneed_order = 1;\n\n\ttrace_btrfs_work_sched(work);\n\tthresh_exec_hook(wq);\n\twork->func(work);\n\tif (need_order) {\n\t\t \n\t\tsmp_mb__before_atomic();\n\t\tset_bit(WORK_DONE_BIT, &work->flags);\n\t\trun_ordered_work(wq, work);\n\t} else {\n\t\t \n\t\ttrace_btrfs_all_work_done(wq->fs_info, work);\n\t}\n}\n\nvoid btrfs_init_work(struct btrfs_work *work, btrfs_func_t func,\n\t\t     btrfs_func_t ordered_func, btrfs_func_t ordered_free)\n{\n\twork->func = func;\n\twork->ordered_func = ordered_func;\n\twork->ordered_free = ordered_free;\n\tINIT_WORK(&work->normal_work, btrfs_work_helper);\n\tINIT_LIST_HEAD(&work->ordered_list);\n\twork->flags = 0;\n}\n\nvoid btrfs_queue_work(struct btrfs_workqueue *wq, struct btrfs_work *work)\n{\n\tunsigned long flags;\n\n\twork->wq = wq;\n\tthresh_queue_hook(wq);\n\tif (work->ordered_func) {\n\t\tspin_lock_irqsave(&wq->list_lock, flags);\n\t\tlist_add_tail(&work->ordered_list, &wq->ordered_list);\n\t\tspin_unlock_irqrestore(&wq->list_lock, flags);\n\t}\n\ttrace_btrfs_work_queued(work);\n\tqueue_work(wq->normal_wq, &work->normal_work);\n}\n\nvoid btrfs_destroy_workqueue(struct btrfs_workqueue *wq)\n{\n\tif (!wq)\n\t\treturn;\n\tdestroy_workqueue(wq->normal_wq);\n\ttrace_btrfs_workqueue_destroy(wq);\n\tkfree(wq);\n}\n\nvoid btrfs_workqueue_set_max(struct btrfs_workqueue *wq, int limit_active)\n{\n\tif (wq)\n\t\twq->limit_active = limit_active;\n}\n\nvoid btrfs_flush_workqueue(struct btrfs_workqueue *wq)\n{\n\tflush_workqueue(wq->normal_wq);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}