{
  "module_name": "extent_io.c",
  "hash_id": "fa123f5818e2e3a497fdf8eb1a66e6f6df4bb42ca11c8796d820d5e826ebde21",
  "original_prompt": "Ingested from linux-6.6.14/fs/btrfs/extent_io.c",
  "human_readable_source": "\n\n#include <linux/bitops.h>\n#include <linux/slab.h>\n#include <linux/bio.h>\n#include <linux/mm.h>\n#include <linux/pagemap.h>\n#include <linux/page-flags.h>\n#include <linux/sched/mm.h>\n#include <linux/spinlock.h>\n#include <linux/blkdev.h>\n#include <linux/swap.h>\n#include <linux/writeback.h>\n#include <linux/pagevec.h>\n#include <linux/prefetch.h>\n#include <linux/fsverity.h>\n#include \"misc.h\"\n#include \"extent_io.h\"\n#include \"extent-io-tree.h\"\n#include \"extent_map.h\"\n#include \"ctree.h\"\n#include \"btrfs_inode.h\"\n#include \"bio.h\"\n#include \"check-integrity.h\"\n#include \"locking.h\"\n#include \"rcu-string.h\"\n#include \"backref.h\"\n#include \"disk-io.h\"\n#include \"subpage.h\"\n#include \"zoned.h\"\n#include \"block-group.h\"\n#include \"compression.h\"\n#include \"fs.h\"\n#include \"accessors.h\"\n#include \"file-item.h\"\n#include \"file.h\"\n#include \"dev-replace.h\"\n#include \"super.h\"\n#include \"transaction.h\"\n\nstatic struct kmem_cache *extent_buffer_cache;\n\n#ifdef CONFIG_BTRFS_DEBUG\nstatic inline void btrfs_leak_debug_add_eb(struct extent_buffer *eb)\n{\n\tstruct btrfs_fs_info *fs_info = eb->fs_info;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&fs_info->eb_leak_lock, flags);\n\tlist_add(&eb->leak_list, &fs_info->allocated_ebs);\n\tspin_unlock_irqrestore(&fs_info->eb_leak_lock, flags);\n}\n\nstatic inline void btrfs_leak_debug_del_eb(struct extent_buffer *eb)\n{\n\tstruct btrfs_fs_info *fs_info = eb->fs_info;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&fs_info->eb_leak_lock, flags);\n\tlist_del(&eb->leak_list);\n\tspin_unlock_irqrestore(&fs_info->eb_leak_lock, flags);\n}\n\nvoid btrfs_extent_buffer_leak_debug_check(struct btrfs_fs_info *fs_info)\n{\n\tstruct extent_buffer *eb;\n\tunsigned long flags;\n\n\t \n\tif (!fs_info->allocated_ebs.next)\n\t\treturn;\n\n\tWARN_ON(!list_empty(&fs_info->allocated_ebs));\n\tspin_lock_irqsave(&fs_info->eb_leak_lock, flags);\n\twhile (!list_empty(&fs_info->allocated_ebs)) {\n\t\teb = list_first_entry(&fs_info->allocated_ebs,\n\t\t\t\t      struct extent_buffer, leak_list);\n\t\tpr_err(\n\t\"BTRFS: buffer leak start %llu len %lu refs %d bflags %lu owner %llu\\n\",\n\t\t       eb->start, eb->len, atomic_read(&eb->refs), eb->bflags,\n\t\t       btrfs_header_owner(eb));\n\t\tlist_del(&eb->leak_list);\n\t\tkmem_cache_free(extent_buffer_cache, eb);\n\t}\n\tspin_unlock_irqrestore(&fs_info->eb_leak_lock, flags);\n}\n#else\n#define btrfs_leak_debug_add_eb(eb)\t\t\tdo {} while (0)\n#define btrfs_leak_debug_del_eb(eb)\t\t\tdo {} while (0)\n#endif\n\n \nstruct btrfs_bio_ctrl {\n\tstruct btrfs_bio *bbio;\n\tenum btrfs_compression_type compress_type;\n\tu32 len_to_oe_boundary;\n\tblk_opf_t opf;\n\tbtrfs_bio_end_io_t end_io_func;\n\tstruct writeback_control *wbc;\n};\n\nstatic void submit_one_bio(struct btrfs_bio_ctrl *bio_ctrl)\n{\n\tstruct btrfs_bio *bbio = bio_ctrl->bbio;\n\n\tif (!bbio)\n\t\treturn;\n\n\t \n\tASSERT(bbio->bio.bi_iter.bi_size);\n\n\tif (btrfs_op(&bbio->bio) == BTRFS_MAP_READ &&\n\t    bio_ctrl->compress_type != BTRFS_COMPRESS_NONE)\n\t\tbtrfs_submit_compressed_read(bbio);\n\telse\n\t\tbtrfs_submit_bio(bbio, 0);\n\n\t \n\tbio_ctrl->bbio = NULL;\n}\n\n \nstatic void submit_write_bio(struct btrfs_bio_ctrl *bio_ctrl, int ret)\n{\n\tstruct btrfs_bio *bbio = bio_ctrl->bbio;\n\n\tif (!bbio)\n\t\treturn;\n\n\tif (ret) {\n\t\tASSERT(ret < 0);\n\t\tbtrfs_bio_end_io(bbio, errno_to_blk_status(ret));\n\t\t \n\t\tbio_ctrl->bbio = NULL;\n\t} else {\n\t\tsubmit_one_bio(bio_ctrl);\n\t}\n}\n\nint __init extent_buffer_init_cachep(void)\n{\n\textent_buffer_cache = kmem_cache_create(\"btrfs_extent_buffer\",\n\t\t\tsizeof(struct extent_buffer), 0,\n\t\t\tSLAB_MEM_SPREAD, NULL);\n\tif (!extent_buffer_cache)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nvoid __cold extent_buffer_free_cachep(void)\n{\n\t \n\trcu_barrier();\n\tkmem_cache_destroy(extent_buffer_cache);\n}\n\nvoid extent_range_clear_dirty_for_io(struct inode *inode, u64 start, u64 end)\n{\n\tunsigned long index = start >> PAGE_SHIFT;\n\tunsigned long end_index = end >> PAGE_SHIFT;\n\tstruct page *page;\n\n\twhile (index <= end_index) {\n\t\tpage = find_get_page(inode->i_mapping, index);\n\t\tBUG_ON(!page);  \n\t\tclear_page_dirty_for_io(page);\n\t\tput_page(page);\n\t\tindex++;\n\t}\n}\n\nstatic void process_one_page(struct btrfs_fs_info *fs_info,\n\t\t\t     struct page *page, struct page *locked_page,\n\t\t\t     unsigned long page_ops, u64 start, u64 end)\n{\n\tu32 len;\n\n\tASSERT(end + 1 - start != 0 && end + 1 - start < U32_MAX);\n\tlen = end + 1 - start;\n\n\tif (page_ops & PAGE_SET_ORDERED)\n\t\tbtrfs_page_clamp_set_ordered(fs_info, page, start, len);\n\tif (page_ops & PAGE_START_WRITEBACK) {\n\t\tbtrfs_page_clamp_clear_dirty(fs_info, page, start, len);\n\t\tbtrfs_page_clamp_set_writeback(fs_info, page, start, len);\n\t}\n\tif (page_ops & PAGE_END_WRITEBACK)\n\t\tbtrfs_page_clamp_clear_writeback(fs_info, page, start, len);\n\n\tif (page != locked_page && (page_ops & PAGE_UNLOCK))\n\t\tbtrfs_page_end_writer_lock(fs_info, page, start, len);\n}\n\nstatic void __process_pages_contig(struct address_space *mapping,\n\t\t\t\t   struct page *locked_page, u64 start, u64 end,\n\t\t\t\t   unsigned long page_ops)\n{\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(mapping->host->i_sb);\n\tpgoff_t start_index = start >> PAGE_SHIFT;\n\tpgoff_t end_index = end >> PAGE_SHIFT;\n\tpgoff_t index = start_index;\n\tstruct folio_batch fbatch;\n\tint i;\n\n\tfolio_batch_init(&fbatch);\n\twhile (index <= end_index) {\n\t\tint found_folios;\n\n\t\tfound_folios = filemap_get_folios_contig(mapping, &index,\n\t\t\t\tend_index, &fbatch);\n\t\tfor (i = 0; i < found_folios; i++) {\n\t\t\tstruct folio *folio = fbatch.folios[i];\n\n\t\t\tprocess_one_page(fs_info, &folio->page, locked_page,\n\t\t\t\t\t page_ops, start, end);\n\t\t}\n\t\tfolio_batch_release(&fbatch);\n\t\tcond_resched();\n\t}\n}\n\nstatic noinline void __unlock_for_delalloc(struct inode *inode,\n\t\t\t\t\t   struct page *locked_page,\n\t\t\t\t\t   u64 start, u64 end)\n{\n\tunsigned long index = start >> PAGE_SHIFT;\n\tunsigned long end_index = end >> PAGE_SHIFT;\n\n\tASSERT(locked_page);\n\tif (index == locked_page->index && end_index == index)\n\t\treturn;\n\n\t__process_pages_contig(inode->i_mapping, locked_page, start, end,\n\t\t\t       PAGE_UNLOCK);\n}\n\nstatic noinline int lock_delalloc_pages(struct inode *inode,\n\t\t\t\t\tstruct page *locked_page,\n\t\t\t\t\tu64 start,\n\t\t\t\t\tu64 end)\n{\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct address_space *mapping = inode->i_mapping;\n\tpgoff_t start_index = start >> PAGE_SHIFT;\n\tpgoff_t end_index = end >> PAGE_SHIFT;\n\tpgoff_t index = start_index;\n\tu64 processed_end = start;\n\tstruct folio_batch fbatch;\n\n\tif (index == locked_page->index && index == end_index)\n\t\treturn 0;\n\n\tfolio_batch_init(&fbatch);\n\twhile (index <= end_index) {\n\t\tunsigned int found_folios, i;\n\n\t\tfound_folios = filemap_get_folios_contig(mapping, &index,\n\t\t\t\tend_index, &fbatch);\n\t\tif (found_folios == 0)\n\t\t\tgoto out;\n\n\t\tfor (i = 0; i < found_folios; i++) {\n\t\t\tstruct page *page = &fbatch.folios[i]->page;\n\t\t\tu32 len = end + 1 - start;\n\n\t\t\tif (page == locked_page)\n\t\t\t\tcontinue;\n\n\t\t\tif (btrfs_page_start_writer_lock(fs_info, page, start,\n\t\t\t\t\t\t\t len))\n\t\t\t\tgoto out;\n\n\t\t\tif (!PageDirty(page) || page->mapping != mapping) {\n\t\t\t\tbtrfs_page_end_writer_lock(fs_info, page, start,\n\t\t\t\t\t\t\t   len);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tprocessed_end = page_offset(page) + PAGE_SIZE - 1;\n\t\t}\n\t\tfolio_batch_release(&fbatch);\n\t\tcond_resched();\n\t}\n\n\treturn 0;\nout:\n\tfolio_batch_release(&fbatch);\n\tif (processed_end > start)\n\t\t__unlock_for_delalloc(inode, locked_page, start, processed_end);\n\treturn -EAGAIN;\n}\n\n \nEXPORT_FOR_TESTS\nnoinline_for_stack bool find_lock_delalloc_range(struct inode *inode,\n\t\t\t\t    struct page *locked_page, u64 *start,\n\t\t\t\t    u64 *end)\n{\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct extent_io_tree *tree = &BTRFS_I(inode)->io_tree;\n\tconst u64 orig_start = *start;\n\tconst u64 orig_end = *end;\n\t \n\tu64 max_bytes = fs_info ? fs_info->max_extent_size : BTRFS_MAX_EXTENT_SIZE;\n\tu64 delalloc_start;\n\tu64 delalloc_end;\n\tbool found;\n\tstruct extent_state *cached_state = NULL;\n\tint ret;\n\tint loops = 0;\n\n\t \n\tASSERT(orig_end > orig_start);\n\n\t \n\tASSERT(!(orig_start >= page_offset(locked_page) + PAGE_SIZE ||\n\t\t orig_end <= page_offset(locked_page)));\nagain:\n\t \n\tdelalloc_start = *start;\n\tdelalloc_end = 0;\n\tfound = btrfs_find_delalloc_range(tree, &delalloc_start, &delalloc_end,\n\t\t\t\t\t  max_bytes, &cached_state);\n\tif (!found || delalloc_end <= *start || delalloc_start > orig_end) {\n\t\t*start = delalloc_start;\n\n\t\t \n\t\t*end = min(delalloc_end, orig_end);\n\t\tfree_extent_state(cached_state);\n\t\treturn false;\n\t}\n\n\t \n\tif (delalloc_start < *start)\n\t\tdelalloc_start = *start;\n\n\t \n\tif (delalloc_end + 1 - delalloc_start > max_bytes)\n\t\tdelalloc_end = delalloc_start + max_bytes - 1;\n\n\t \n\tret = lock_delalloc_pages(inode, locked_page,\n\t\t\t\t  delalloc_start, delalloc_end);\n\tASSERT(!ret || ret == -EAGAIN);\n\tif (ret == -EAGAIN) {\n\t\t \n\t\tfree_extent_state(cached_state);\n\t\tcached_state = NULL;\n\t\tif (!loops) {\n\t\t\tmax_bytes = PAGE_SIZE;\n\t\t\tloops = 1;\n\t\t\tgoto again;\n\t\t} else {\n\t\t\tfound = false;\n\t\t\tgoto out_failed;\n\t\t}\n\t}\n\n\t \n\tlock_extent(tree, delalloc_start, delalloc_end, &cached_state);\n\n\t \n\tret = test_range_bit(tree, delalloc_start, delalloc_end,\n\t\t\t     EXTENT_DELALLOC, 1, cached_state);\n\tif (!ret) {\n\t\tunlock_extent(tree, delalloc_start, delalloc_end,\n\t\t\t      &cached_state);\n\t\t__unlock_for_delalloc(inode, locked_page,\n\t\t\t      delalloc_start, delalloc_end);\n\t\tcond_resched();\n\t\tgoto again;\n\t}\n\tfree_extent_state(cached_state);\n\t*start = delalloc_start;\n\t*end = delalloc_end;\nout_failed:\n\treturn found;\n}\n\nvoid extent_clear_unlock_delalloc(struct btrfs_inode *inode, u64 start, u64 end,\n\t\t\t\t  struct page *locked_page,\n\t\t\t\t  u32 clear_bits, unsigned long page_ops)\n{\n\tclear_extent_bit(&inode->io_tree, start, end, clear_bits, NULL);\n\n\t__process_pages_contig(inode->vfs_inode.i_mapping, locked_page,\n\t\t\t       start, end, page_ops);\n}\n\nstatic bool btrfs_verify_page(struct page *page, u64 start)\n{\n\tif (!fsverity_active(page->mapping->host) ||\n\t    PageUptodate(page) ||\n\t    start >= i_size_read(page->mapping->host))\n\t\treturn true;\n\treturn fsverity_verify_page(page);\n}\n\nstatic void end_page_read(struct page *page, bool uptodate, u64 start, u32 len)\n{\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(page->mapping->host->i_sb);\n\n\tASSERT(page_offset(page) <= start &&\n\t       start + len <= page_offset(page) + PAGE_SIZE);\n\n\tif (uptodate && btrfs_verify_page(page, start))\n\t\tbtrfs_page_set_uptodate(fs_info, page, start, len);\n\telse\n\t\tbtrfs_page_clear_uptodate(fs_info, page, start, len);\n\n\tif (!btrfs_is_subpage(fs_info, page))\n\t\tunlock_page(page);\n\telse\n\t\tbtrfs_subpage_end_reader(fs_info, page, start, len);\n}\n\n \nstatic void end_bio_extent_writepage(struct btrfs_bio *bbio)\n{\n\tstruct bio *bio = &bbio->bio;\n\tint error = blk_status_to_errno(bio->bi_status);\n\tstruct bio_vec *bvec;\n\tstruct bvec_iter_all iter_all;\n\n\tASSERT(!bio_flagged(bio, BIO_CLONED));\n\tbio_for_each_segment_all(bvec, bio, iter_all) {\n\t\tstruct page *page = bvec->bv_page;\n\t\tstruct inode *inode = page->mapping->host;\n\t\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\t\tconst u32 sectorsize = fs_info->sectorsize;\n\t\tu64 start = page_offset(page) + bvec->bv_offset;\n\t\tu32 len = bvec->bv_len;\n\n\t\t \n\t\tif (!IS_ALIGNED(bvec->bv_offset, sectorsize))\n\t\t\tbtrfs_err(fs_info,\n\t\t\"partial page write in btrfs with offset %u and length %u\",\n\t\t\t\t  bvec->bv_offset, bvec->bv_len);\n\t\telse if (!IS_ALIGNED(bvec->bv_len, sectorsize))\n\t\t\tbtrfs_info(fs_info,\n\t\t\"incomplete page write with offset %u and length %u\",\n\t\t\t\t   bvec->bv_offset, bvec->bv_len);\n\n\t\tbtrfs_finish_ordered_extent(bbio->ordered, page, start, len, !error);\n\t\tif (error)\n\t\t\tmapping_set_error(page->mapping, error);\n\t\tbtrfs_page_clear_writeback(fs_info, page, start, len);\n\t}\n\n\tbio_put(bio);\n}\n\n \nstruct processed_extent {\n\tstruct btrfs_inode *inode;\n\t \n\tu64 start;\n\t \n\tu64 end;\n\tbool uptodate;\n};\n\n \nstatic void endio_readpage_release_extent(struct processed_extent *processed,\n\t\t\t      struct btrfs_inode *inode, u64 start, u64 end,\n\t\t\t      bool uptodate)\n{\n\tstruct extent_state *cached = NULL;\n\tstruct extent_io_tree *tree;\n\n\t \n\tif (!processed->inode)\n\t\tgoto update;\n\n\t \n\tif (processed->inode == inode && processed->uptodate == uptodate &&\n\t    processed->end + 1 >= start && end >= processed->end) {\n\t\tprocessed->end = end;\n\t\treturn;\n\t}\n\n\ttree = &processed->inode->io_tree;\n\t \n\tunlock_extent(tree, processed->start, processed->end, &cached);\n\nupdate:\n\t \n\tprocessed->inode = inode;\n\tprocessed->start = start;\n\tprocessed->end = end;\n\tprocessed->uptodate = uptodate;\n}\n\nstatic void begin_page_read(struct btrfs_fs_info *fs_info, struct page *page)\n{\n\tASSERT(PageLocked(page));\n\tif (!btrfs_is_subpage(fs_info, page))\n\t\treturn;\n\n\tASSERT(PagePrivate(page));\n\tbtrfs_subpage_start_reader(fs_info, page, page_offset(page), PAGE_SIZE);\n}\n\n \nstatic void end_bio_extent_readpage(struct btrfs_bio *bbio)\n{\n\tstruct bio *bio = &bbio->bio;\n\tstruct bio_vec *bvec;\n\tstruct processed_extent processed = { 0 };\n\t \n\tu32 bio_offset = 0;\n\tstruct bvec_iter_all iter_all;\n\n\tASSERT(!bio_flagged(bio, BIO_CLONED));\n\tbio_for_each_segment_all(bvec, bio, iter_all) {\n\t\tbool uptodate = !bio->bi_status;\n\t\tstruct page *page = bvec->bv_page;\n\t\tstruct inode *inode = page->mapping->host;\n\t\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\t\tconst u32 sectorsize = fs_info->sectorsize;\n\t\tu64 start;\n\t\tu64 end;\n\t\tu32 len;\n\n\t\tbtrfs_debug(fs_info,\n\t\t\t\"end_bio_extent_readpage: bi_sector=%llu, err=%d, mirror=%u\",\n\t\t\tbio->bi_iter.bi_sector, bio->bi_status,\n\t\t\tbbio->mirror_num);\n\n\t\t \n\t\tif (!IS_ALIGNED(bvec->bv_offset, sectorsize))\n\t\t\tbtrfs_err(fs_info,\n\t\t\"partial page read in btrfs with offset %u and length %u\",\n\t\t\t\t  bvec->bv_offset, bvec->bv_len);\n\t\telse if (!IS_ALIGNED(bvec->bv_offset + bvec->bv_len,\n\t\t\t\t     sectorsize))\n\t\t\tbtrfs_info(fs_info,\n\t\t\"incomplete page read with offset %u and length %u\",\n\t\t\t\t   bvec->bv_offset, bvec->bv_len);\n\n\t\tstart = page_offset(page) + bvec->bv_offset;\n\t\tend = start + bvec->bv_len - 1;\n\t\tlen = bvec->bv_len;\n\n\t\tif (likely(uptodate)) {\n\t\t\tloff_t i_size = i_size_read(inode);\n\t\t\tpgoff_t end_index = i_size >> PAGE_SHIFT;\n\n\t\t\t \n\t\t\tif (page->index == end_index && i_size <= end) {\n\t\t\t\tu32 zero_start = max(offset_in_page(i_size),\n\t\t\t\t\t\t     offset_in_page(start));\n\n\t\t\t\tzero_user_segment(page, zero_start,\n\t\t\t\t\t\t  offset_in_page(end) + 1);\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tend_page_read(page, uptodate, start, len);\n\t\tendio_readpage_release_extent(&processed, BTRFS_I(inode),\n\t\t\t\t\t      start, end, uptodate);\n\n\t\tASSERT(bio_offset + len > bio_offset);\n\t\tbio_offset += len;\n\n\t}\n\t \n\tendio_readpage_release_extent(&processed, NULL, 0, 0, false);\n\tbio_put(bio);\n}\n\n \nint btrfs_alloc_page_array(unsigned int nr_pages, struct page **page_array)\n{\n\tunsigned int allocated;\n\n\tfor (allocated = 0; allocated < nr_pages;) {\n\t\tunsigned int last = allocated;\n\n\t\tallocated = alloc_pages_bulk_array(GFP_NOFS, nr_pages, page_array);\n\n\t\tif (allocated == nr_pages)\n\t\t\treturn 0;\n\n\t\t \n\t\tif (allocated == last) {\n\t\t\tfor (int i = 0; i < allocated; i++) {\n\t\t\t\t__free_page(page_array[i]);\n\t\t\t\tpage_array[i] = NULL;\n\t\t\t}\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tmemalloc_retry_wait(GFP_NOFS);\n\t}\n\treturn 0;\n}\n\nstatic bool btrfs_bio_is_contig(struct btrfs_bio_ctrl *bio_ctrl,\n\t\t\t\tstruct page *page, u64 disk_bytenr,\n\t\t\t\tunsigned int pg_offset)\n{\n\tstruct bio *bio = &bio_ctrl->bbio->bio;\n\tstruct bio_vec *bvec = bio_last_bvec_all(bio);\n\tconst sector_t sector = disk_bytenr >> SECTOR_SHIFT;\n\n\tif (bio_ctrl->compress_type != BTRFS_COMPRESS_NONE) {\n\t\t \n\t\treturn bio->bi_iter.bi_sector == sector;\n\t}\n\n\t \n\treturn bio_end_sector(bio) == sector &&\n\t\tpage_offset(bvec->bv_page) + bvec->bv_offset + bvec->bv_len ==\n\t\tpage_offset(page) + pg_offset;\n}\n\nstatic void alloc_new_bio(struct btrfs_inode *inode,\n\t\t\t  struct btrfs_bio_ctrl *bio_ctrl,\n\t\t\t  u64 disk_bytenr, u64 file_offset)\n{\n\tstruct btrfs_fs_info *fs_info = inode->root->fs_info;\n\tstruct btrfs_bio *bbio;\n\n\tbbio = btrfs_bio_alloc(BIO_MAX_VECS, bio_ctrl->opf, fs_info,\n\t\t\t       bio_ctrl->end_io_func, NULL);\n\tbbio->bio.bi_iter.bi_sector = disk_bytenr >> SECTOR_SHIFT;\n\tbbio->inode = inode;\n\tbbio->file_offset = file_offset;\n\tbio_ctrl->bbio = bbio;\n\tbio_ctrl->len_to_oe_boundary = U32_MAX;\n\n\t \n\tif (bio_ctrl->wbc) {\n\t\tstruct btrfs_ordered_extent *ordered;\n\n\t\tordered = btrfs_lookup_ordered_extent(inode, file_offset);\n\t\tif (ordered) {\n\t\t\tbio_ctrl->len_to_oe_boundary = min_t(u32, U32_MAX,\n\t\t\t\t\tordered->file_offset +\n\t\t\t\t\tordered->disk_num_bytes - file_offset);\n\t\t\tbbio->ordered = ordered;\n\t\t}\n\n\t\t \n\t\tbio_set_dev(&bbio->bio, fs_info->fs_devices->latest_dev->bdev);\n\t\twbc_init_bio(bio_ctrl->wbc, &bbio->bio);\n\t}\n}\n\n \nstatic void submit_extent_page(struct btrfs_bio_ctrl *bio_ctrl,\n\t\t\t       u64 disk_bytenr, struct page *page,\n\t\t\t       size_t size, unsigned long pg_offset)\n{\n\tstruct btrfs_inode *inode = BTRFS_I(page->mapping->host);\n\n\tASSERT(pg_offset + size <= PAGE_SIZE);\n\tASSERT(bio_ctrl->end_io_func);\n\n\tif (bio_ctrl->bbio &&\n\t    !btrfs_bio_is_contig(bio_ctrl, page, disk_bytenr, pg_offset))\n\t\tsubmit_one_bio(bio_ctrl);\n\n\tdo {\n\t\tu32 len = size;\n\n\t\t \n\t\tif (!bio_ctrl->bbio) {\n\t\t\talloc_new_bio(inode, bio_ctrl, disk_bytenr,\n\t\t\t\t      page_offset(page) + pg_offset);\n\t\t}\n\n\t\t \n\t\tif (len > bio_ctrl->len_to_oe_boundary) {\n\t\t\tASSERT(bio_ctrl->compress_type == BTRFS_COMPRESS_NONE);\n\t\t\tASSERT(is_data_inode(&inode->vfs_inode));\n\t\t\tlen = bio_ctrl->len_to_oe_boundary;\n\t\t}\n\n\t\tif (bio_add_page(&bio_ctrl->bbio->bio, page, len, pg_offset) != len) {\n\t\t\t \n\t\t\tsubmit_one_bio(bio_ctrl);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (bio_ctrl->wbc)\n\t\t\twbc_account_cgroup_owner(bio_ctrl->wbc, page, len);\n\n\t\tsize -= len;\n\t\tpg_offset += len;\n\t\tdisk_bytenr += len;\n\n\t\t \n\t\tif (bio_ctrl->len_to_oe_boundary != U32_MAX)\n\t\t\tbio_ctrl->len_to_oe_boundary -= len;\n\n\t\t \n\t\tif (bio_ctrl->len_to_oe_boundary == 0)\n\t\t\tsubmit_one_bio(bio_ctrl);\n\t} while (size);\n}\n\nstatic int attach_extent_buffer_page(struct extent_buffer *eb,\n\t\t\t\t     struct page *page,\n\t\t\t\t     struct btrfs_subpage *prealloc)\n{\n\tstruct btrfs_fs_info *fs_info = eb->fs_info;\n\tint ret = 0;\n\n\t \n\tif (page->mapping)\n\t\tlockdep_assert_held(&page->mapping->private_lock);\n\n\tif (fs_info->nodesize >= PAGE_SIZE) {\n\t\tif (!PagePrivate(page))\n\t\t\tattach_page_private(page, eb);\n\t\telse\n\t\t\tWARN_ON(page->private != (unsigned long)eb);\n\t\treturn 0;\n\t}\n\n\t \n\tif (PagePrivate(page)) {\n\t\tbtrfs_free_subpage(prealloc);\n\t\treturn 0;\n\t}\n\n\tif (prealloc)\n\t\t \n\t\tattach_page_private(page, prealloc);\n\telse\n\t\t \n\t\tret = btrfs_attach_subpage(fs_info, page,\n\t\t\t\t\t   BTRFS_SUBPAGE_METADATA);\n\treturn ret;\n}\n\nint set_page_extent_mapped(struct page *page)\n{\n\tstruct btrfs_fs_info *fs_info;\n\n\tASSERT(page->mapping);\n\n\tif (PagePrivate(page))\n\t\treturn 0;\n\n\tfs_info = btrfs_sb(page->mapping->host->i_sb);\n\n\tif (btrfs_is_subpage(fs_info, page))\n\t\treturn btrfs_attach_subpage(fs_info, page, BTRFS_SUBPAGE_DATA);\n\n\tattach_page_private(page, (void *)EXTENT_PAGE_PRIVATE);\n\treturn 0;\n}\n\nvoid clear_page_extent_mapped(struct page *page)\n{\n\tstruct btrfs_fs_info *fs_info;\n\n\tASSERT(page->mapping);\n\n\tif (!PagePrivate(page))\n\t\treturn;\n\n\tfs_info = btrfs_sb(page->mapping->host->i_sb);\n\tif (btrfs_is_subpage(fs_info, page))\n\t\treturn btrfs_detach_subpage(fs_info, page);\n\n\tdetach_page_private(page);\n}\n\nstatic struct extent_map *\n__get_extent_map(struct inode *inode, struct page *page, size_t pg_offset,\n\t\t u64 start, u64 len, struct extent_map **em_cached)\n{\n\tstruct extent_map *em;\n\n\tif (em_cached && *em_cached) {\n\t\tem = *em_cached;\n\t\tif (extent_map_in_tree(em) && start >= em->start &&\n\t\t    start < extent_map_end(em)) {\n\t\t\trefcount_inc(&em->refs);\n\t\t\treturn em;\n\t\t}\n\n\t\tfree_extent_map(em);\n\t\t*em_cached = NULL;\n\t}\n\n\tem = btrfs_get_extent(BTRFS_I(inode), page, pg_offset, start, len);\n\tif (em_cached && !IS_ERR(em)) {\n\t\tBUG_ON(*em_cached);\n\t\trefcount_inc(&em->refs);\n\t\t*em_cached = em;\n\t}\n\treturn em;\n}\n \nstatic int btrfs_do_readpage(struct page *page, struct extent_map **em_cached,\n\t\t      struct btrfs_bio_ctrl *bio_ctrl, u64 *prev_em_start)\n{\n\tstruct inode *inode = page->mapping->host;\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tu64 start = page_offset(page);\n\tconst u64 end = start + PAGE_SIZE - 1;\n\tu64 cur = start;\n\tu64 extent_offset;\n\tu64 last_byte = i_size_read(inode);\n\tu64 block_start;\n\tstruct extent_map *em;\n\tint ret = 0;\n\tsize_t pg_offset = 0;\n\tsize_t iosize;\n\tsize_t blocksize = inode->i_sb->s_blocksize;\n\tstruct extent_io_tree *tree = &BTRFS_I(inode)->io_tree;\n\n\tret = set_page_extent_mapped(page);\n\tif (ret < 0) {\n\t\tunlock_extent(tree, start, end, NULL);\n\t\tunlock_page(page);\n\t\treturn ret;\n\t}\n\n\tif (page->index == last_byte >> PAGE_SHIFT) {\n\t\tsize_t zero_offset = offset_in_page(last_byte);\n\n\t\tif (zero_offset) {\n\t\t\tiosize = PAGE_SIZE - zero_offset;\n\t\t\tmemzero_page(page, zero_offset, iosize);\n\t\t}\n\t}\n\tbio_ctrl->end_io_func = end_bio_extent_readpage;\n\tbegin_page_read(fs_info, page);\n\twhile (cur <= end) {\n\t\tenum btrfs_compression_type compress_type = BTRFS_COMPRESS_NONE;\n\t\tbool force_bio_submit = false;\n\t\tu64 disk_bytenr;\n\n\t\tASSERT(IS_ALIGNED(cur, fs_info->sectorsize));\n\t\tif (cur >= last_byte) {\n\t\t\tiosize = PAGE_SIZE - pg_offset;\n\t\t\tmemzero_page(page, pg_offset, iosize);\n\t\t\tunlock_extent(tree, cur, cur + iosize - 1, NULL);\n\t\t\tend_page_read(page, true, cur, iosize);\n\t\t\tbreak;\n\t\t}\n\t\tem = __get_extent_map(inode, page, pg_offset, cur,\n\t\t\t\t      end - cur + 1, em_cached);\n\t\tif (IS_ERR(em)) {\n\t\t\tunlock_extent(tree, cur, end, NULL);\n\t\t\tend_page_read(page, false, cur, end + 1 - cur);\n\t\t\treturn PTR_ERR(em);\n\t\t}\n\t\textent_offset = cur - em->start;\n\t\tBUG_ON(extent_map_end(em) <= cur);\n\t\tBUG_ON(end < cur);\n\n\t\tif (test_bit(EXTENT_FLAG_COMPRESSED, &em->flags))\n\t\t\tcompress_type = em->compress_type;\n\n\t\tiosize = min(extent_map_end(em) - cur, end - cur + 1);\n\t\tiosize = ALIGN(iosize, blocksize);\n\t\tif (compress_type != BTRFS_COMPRESS_NONE)\n\t\t\tdisk_bytenr = em->block_start;\n\t\telse\n\t\t\tdisk_bytenr = em->block_start + extent_offset;\n\t\tblock_start = em->block_start;\n\t\tif (test_bit(EXTENT_FLAG_PREALLOC, &em->flags))\n\t\t\tblock_start = EXTENT_MAP_HOLE;\n\n\t\t \n\t\tif (test_bit(EXTENT_FLAG_COMPRESSED, &em->flags) &&\n\t\t    prev_em_start && *prev_em_start != (u64)-1 &&\n\t\t    *prev_em_start != em->start)\n\t\t\tforce_bio_submit = true;\n\n\t\tif (prev_em_start)\n\t\t\t*prev_em_start = em->start;\n\n\t\tfree_extent_map(em);\n\t\tem = NULL;\n\n\t\t \n\t\tif (block_start == EXTENT_MAP_HOLE) {\n\t\t\tmemzero_page(page, pg_offset, iosize);\n\n\t\t\tunlock_extent(tree, cur, cur + iosize - 1, NULL);\n\t\t\tend_page_read(page, true, cur, iosize);\n\t\t\tcur = cur + iosize;\n\t\t\tpg_offset += iosize;\n\t\t\tcontinue;\n\t\t}\n\t\t \n\t\tif (block_start == EXTENT_MAP_INLINE) {\n\t\t\tunlock_extent(tree, cur, cur + iosize - 1, NULL);\n\t\t\tend_page_read(page, true, cur, iosize);\n\t\t\tcur = cur + iosize;\n\t\t\tpg_offset += iosize;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (bio_ctrl->compress_type != compress_type) {\n\t\t\tsubmit_one_bio(bio_ctrl);\n\t\t\tbio_ctrl->compress_type = compress_type;\n\t\t}\n\n\t\tif (force_bio_submit)\n\t\t\tsubmit_one_bio(bio_ctrl);\n\t\tsubmit_extent_page(bio_ctrl, disk_bytenr, page, iosize,\n\t\t\t\t   pg_offset);\n\t\tcur = cur + iosize;\n\t\tpg_offset += iosize;\n\t}\n\n\treturn 0;\n}\n\nint btrfs_read_folio(struct file *file, struct folio *folio)\n{\n\tstruct page *page = &folio->page;\n\tstruct btrfs_inode *inode = BTRFS_I(page->mapping->host);\n\tu64 start = page_offset(page);\n\tu64 end = start + PAGE_SIZE - 1;\n\tstruct btrfs_bio_ctrl bio_ctrl = { .opf = REQ_OP_READ };\n\tint ret;\n\n\tbtrfs_lock_and_flush_ordered_range(inode, start, end, NULL);\n\n\tret = btrfs_do_readpage(page, NULL, &bio_ctrl, NULL);\n\t \n\tsubmit_one_bio(&bio_ctrl);\n\treturn ret;\n}\n\nstatic inline void contiguous_readpages(struct page *pages[], int nr_pages,\n\t\t\t\t\tu64 start, u64 end,\n\t\t\t\t\tstruct extent_map **em_cached,\n\t\t\t\t\tstruct btrfs_bio_ctrl *bio_ctrl,\n\t\t\t\t\tu64 *prev_em_start)\n{\n\tstruct btrfs_inode *inode = BTRFS_I(pages[0]->mapping->host);\n\tint index;\n\n\tbtrfs_lock_and_flush_ordered_range(inode, start, end, NULL);\n\n\tfor (index = 0; index < nr_pages; index++) {\n\t\tbtrfs_do_readpage(pages[index], em_cached, bio_ctrl,\n\t\t\t\t  prev_em_start);\n\t\tput_page(pages[index]);\n\t}\n}\n\n \nstatic noinline_for_stack int writepage_delalloc(struct btrfs_inode *inode,\n\t\tstruct page *page, struct writeback_control *wbc)\n{\n\tconst u64 page_start = page_offset(page);\n\tconst u64 page_end = page_start + PAGE_SIZE - 1;\n\tu64 delalloc_start = page_start;\n\tu64 delalloc_end = page_end;\n\tu64 delalloc_to_write = 0;\n\tint ret = 0;\n\n\twhile (delalloc_start < page_end) {\n\t\tdelalloc_end = page_end;\n\t\tif (!find_lock_delalloc_range(&inode->vfs_inode, page,\n\t\t\t\t\t      &delalloc_start, &delalloc_end)) {\n\t\t\tdelalloc_start = delalloc_end + 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\tret = btrfs_run_delalloc_range(inode, page, delalloc_start,\n\t\t\t\t\t       delalloc_end, wbc);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tdelalloc_start = delalloc_end + 1;\n\t}\n\n\t \n\tdelalloc_to_write +=\n\t\tDIV_ROUND_UP(delalloc_end + 1 - page_start, PAGE_SIZE);\n\n\t \n\tif (ret == 1) {\n\t\twbc->nr_to_write -= delalloc_to_write;\n\t\treturn 1;\n\t}\n\n\tif (wbc->nr_to_write < delalloc_to_write) {\n\t\tint thresh = 8192;\n\n\t\tif (delalloc_to_write < thresh * 2)\n\t\t\tthresh = delalloc_to_write;\n\t\twbc->nr_to_write = min_t(u64, delalloc_to_write,\n\t\t\t\t\t thresh);\n\t}\n\n\treturn 0;\n}\n\n \nstatic void find_next_dirty_byte(struct btrfs_fs_info *fs_info,\n\t\t\t\t struct page *page, u64 *start, u64 *end)\n{\n\tstruct btrfs_subpage *subpage = (struct btrfs_subpage *)page->private;\n\tstruct btrfs_subpage_info *spi = fs_info->subpage_info;\n\tu64 orig_start = *start;\n\t \n\tunsigned long flags;\n\tint range_start_bit;\n\tint range_end_bit;\n\n\t \n\tif (!btrfs_is_subpage(fs_info, page)) {\n\t\t*start = page_offset(page);\n\t\t*end = page_offset(page) + PAGE_SIZE;\n\t\treturn;\n\t}\n\n\trange_start_bit = spi->dirty_offset +\n\t\t\t  (offset_in_page(orig_start) >> fs_info->sectorsize_bits);\n\n\t \n\tspin_lock_irqsave(&subpage->lock, flags);\n\tbitmap_next_set_region(subpage->bitmaps, &range_start_bit, &range_end_bit,\n\t\t\t       spi->dirty_offset + spi->bitmap_nr_bits);\n\tspin_unlock_irqrestore(&subpage->lock, flags);\n\n\trange_start_bit -= spi->dirty_offset;\n\trange_end_bit -= spi->dirty_offset;\n\n\t*start = page_offset(page) + range_start_bit * fs_info->sectorsize;\n\t*end = page_offset(page) + range_end_bit * fs_info->sectorsize;\n}\n\n \nstatic noinline_for_stack int __extent_writepage_io(struct btrfs_inode *inode,\n\t\t\t\t struct page *page,\n\t\t\t\t struct btrfs_bio_ctrl *bio_ctrl,\n\t\t\t\t loff_t i_size,\n\t\t\t\t int *nr_ret)\n{\n\tstruct btrfs_fs_info *fs_info = inode->root->fs_info;\n\tu64 cur = page_offset(page);\n\tu64 end = cur + PAGE_SIZE - 1;\n\tu64 extent_offset;\n\tu64 block_start;\n\tstruct extent_map *em;\n\tint ret = 0;\n\tint nr = 0;\n\n\tret = btrfs_writepage_cow_fixup(page);\n\tif (ret) {\n\t\t \n\t\tredirty_page_for_writepage(bio_ctrl->wbc, page);\n\t\tunlock_page(page);\n\t\treturn 1;\n\t}\n\n\tbio_ctrl->end_io_func = end_bio_extent_writepage;\n\twhile (cur <= end) {\n\t\tu32 len = end - cur + 1;\n\t\tu64 disk_bytenr;\n\t\tu64 em_end;\n\t\tu64 dirty_range_start = cur;\n\t\tu64 dirty_range_end;\n\t\tu32 iosize;\n\n\t\tif (cur >= i_size) {\n\t\t\tbtrfs_mark_ordered_io_finished(inode, page, cur, len,\n\t\t\t\t\t\t       true);\n\t\t\t \n\t\t\tbtrfs_page_clear_dirty(fs_info, page, cur, len);\n\t\t\tbreak;\n\t\t}\n\n\t\tfind_next_dirty_byte(fs_info, page, &dirty_range_start,\n\t\t\t\t     &dirty_range_end);\n\t\tif (cur < dirty_range_start) {\n\t\t\tcur = dirty_range_start;\n\t\t\tcontinue;\n\t\t}\n\n\t\tem = btrfs_get_extent(inode, NULL, 0, cur, len);\n\t\tif (IS_ERR(em)) {\n\t\t\tret = PTR_ERR_OR_ZERO(em);\n\t\t\tgoto out_error;\n\t\t}\n\n\t\textent_offset = cur - em->start;\n\t\tem_end = extent_map_end(em);\n\t\tASSERT(cur <= em_end);\n\t\tASSERT(cur < end);\n\t\tASSERT(IS_ALIGNED(em->start, fs_info->sectorsize));\n\t\tASSERT(IS_ALIGNED(em->len, fs_info->sectorsize));\n\n\t\tblock_start = em->block_start;\n\t\tdisk_bytenr = em->block_start + extent_offset;\n\n\t\tASSERT(!test_bit(EXTENT_FLAG_COMPRESSED, &em->flags));\n\t\tASSERT(block_start != EXTENT_MAP_HOLE);\n\t\tASSERT(block_start != EXTENT_MAP_INLINE);\n\n\t\t \n\t\tiosize = min(min(em_end, end + 1), dirty_range_end) - cur;\n\t\tfree_extent_map(em);\n\t\tem = NULL;\n\n\t\tbtrfs_set_range_writeback(inode, cur, cur + iosize - 1);\n\t\tif (!PageWriteback(page)) {\n\t\t\tbtrfs_err(inode->root->fs_info,\n\t\t\t\t   \"page %lu not writeback, cur %llu end %llu\",\n\t\t\t       page->index, cur, end);\n\t\t}\n\n\t\t \n\t\tbtrfs_page_clear_dirty(fs_info, page, cur, iosize);\n\n\t\tsubmit_extent_page(bio_ctrl, disk_bytenr, page, iosize,\n\t\t\t\t   cur - page_offset(page));\n\t\tcur += iosize;\n\t\tnr++;\n\t}\n\n\tbtrfs_page_assert_not_dirty(fs_info, page);\n\t*nr_ret = nr;\n\treturn 0;\n\nout_error:\n\t \n\t*nr_ret = nr;\n\treturn ret;\n}\n\n \nstatic int __extent_writepage(struct page *page, struct btrfs_bio_ctrl *bio_ctrl)\n{\n\tstruct folio *folio = page_folio(page);\n\tstruct inode *inode = page->mapping->host;\n\tconst u64 page_start = page_offset(page);\n\tint ret;\n\tint nr = 0;\n\tsize_t pg_offset;\n\tloff_t i_size = i_size_read(inode);\n\tunsigned long end_index = i_size >> PAGE_SHIFT;\n\n\ttrace___extent_writepage(page, inode, bio_ctrl->wbc);\n\n\tWARN_ON(!PageLocked(page));\n\n\tpg_offset = offset_in_page(i_size);\n\tif (page->index > end_index ||\n\t   (page->index == end_index && !pg_offset)) {\n\t\tfolio_invalidate(folio, 0, folio_size(folio));\n\t\tfolio_unlock(folio);\n\t\treturn 0;\n\t}\n\n\tif (page->index == end_index)\n\t\tmemzero_page(page, pg_offset, PAGE_SIZE - pg_offset);\n\n\tret = set_page_extent_mapped(page);\n\tif (ret < 0)\n\t\tgoto done;\n\n\tret = writepage_delalloc(BTRFS_I(inode), page, bio_ctrl->wbc);\n\tif (ret == 1)\n\t\treturn 0;\n\tif (ret)\n\t\tgoto done;\n\n\tret = __extent_writepage_io(BTRFS_I(inode), page, bio_ctrl, i_size, &nr);\n\tif (ret == 1)\n\t\treturn 0;\n\n\tbio_ctrl->wbc->nr_to_write--;\n\ndone:\n\tif (nr == 0) {\n\t\t \n\t\tset_page_writeback(page);\n\t\tend_page_writeback(page);\n\t}\n\tif (ret) {\n\t\tbtrfs_mark_ordered_io_finished(BTRFS_I(inode), page, page_start,\n\t\t\t\t\t       PAGE_SIZE, !ret);\n\t\tmapping_set_error(page->mapping, ret);\n\t}\n\tunlock_page(page);\n\tASSERT(ret <= 0);\n\treturn ret;\n}\n\nvoid wait_on_extent_buffer_writeback(struct extent_buffer *eb)\n{\n\twait_on_bit_io(&eb->bflags, EXTENT_BUFFER_WRITEBACK,\n\t\t       TASK_UNINTERRUPTIBLE);\n}\n\n \nstatic noinline_for_stack bool lock_extent_buffer_for_io(struct extent_buffer *eb,\n\t\t\t  struct writeback_control *wbc)\n{\n\tstruct btrfs_fs_info *fs_info = eb->fs_info;\n\tbool ret = false;\n\n\tbtrfs_tree_lock(eb);\n\twhile (test_bit(EXTENT_BUFFER_WRITEBACK, &eb->bflags)) {\n\t\tbtrfs_tree_unlock(eb);\n\t\tif (wbc->sync_mode != WB_SYNC_ALL)\n\t\t\treturn false;\n\t\twait_on_extent_buffer_writeback(eb);\n\t\tbtrfs_tree_lock(eb);\n\t}\n\n\t \n\tspin_lock(&eb->refs_lock);\n\tif (test_and_clear_bit(EXTENT_BUFFER_DIRTY, &eb->bflags)) {\n\t\tset_bit(EXTENT_BUFFER_WRITEBACK, &eb->bflags);\n\t\tspin_unlock(&eb->refs_lock);\n\t\tbtrfs_set_header_flag(eb, BTRFS_HEADER_FLAG_WRITTEN);\n\t\tpercpu_counter_add_batch(&fs_info->dirty_metadata_bytes,\n\t\t\t\t\t -eb->len,\n\t\t\t\t\t fs_info->dirty_metadata_batch);\n\t\tret = true;\n\t} else {\n\t\tspin_unlock(&eb->refs_lock);\n\t}\n\tbtrfs_tree_unlock(eb);\n\treturn ret;\n}\n\nstatic void set_btree_ioerr(struct extent_buffer *eb)\n{\n\tstruct btrfs_fs_info *fs_info = eb->fs_info;\n\n\tset_bit(EXTENT_BUFFER_WRITE_ERR, &eb->bflags);\n\n\t \n\tclear_bit(EXTENT_BUFFER_UPTODATE, &eb->bflags);\n\n\t \n\tmapping_set_error(eb->fs_info->btree_inode->i_mapping, -EIO);\n\n\t \n\tswitch (eb->log_index) {\n\tcase -1:\n\t\tset_bit(BTRFS_FS_BTREE_ERR, &fs_info->flags);\n\t\tbreak;\n\tcase 0:\n\t\tset_bit(BTRFS_FS_LOG1_ERR, &fs_info->flags);\n\t\tbreak;\n\tcase 1:\n\t\tset_bit(BTRFS_FS_LOG2_ERR, &fs_info->flags);\n\t\tbreak;\n\tdefault:\n\t\tBUG();  \n\t}\n}\n\n \nstatic struct extent_buffer *find_extent_buffer_nolock(\n\t\tstruct btrfs_fs_info *fs_info, u64 start)\n{\n\tstruct extent_buffer *eb;\n\n\trcu_read_lock();\n\teb = radix_tree_lookup(&fs_info->buffer_radix,\n\t\t\t       start >> fs_info->sectorsize_bits);\n\tif (eb && atomic_inc_not_zero(&eb->refs)) {\n\t\trcu_read_unlock();\n\t\treturn eb;\n\t}\n\trcu_read_unlock();\n\treturn NULL;\n}\n\nstatic void extent_buffer_write_end_io(struct btrfs_bio *bbio)\n{\n\tstruct extent_buffer *eb = bbio->private;\n\tstruct btrfs_fs_info *fs_info = eb->fs_info;\n\tbool uptodate = !bbio->bio.bi_status;\n\tstruct bvec_iter_all iter_all;\n\tstruct bio_vec *bvec;\n\tu32 bio_offset = 0;\n\n\tif (!uptodate)\n\t\tset_btree_ioerr(eb);\n\n\tbio_for_each_segment_all(bvec, &bbio->bio, iter_all) {\n\t\tu64 start = eb->start + bio_offset;\n\t\tstruct page *page = bvec->bv_page;\n\t\tu32 len = bvec->bv_len;\n\n\t\tbtrfs_page_clear_writeback(fs_info, page, start, len);\n\t\tbio_offset += len;\n\t}\n\n\tclear_bit(EXTENT_BUFFER_WRITEBACK, &eb->bflags);\n\tsmp_mb__after_atomic();\n\twake_up_bit(&eb->bflags, EXTENT_BUFFER_WRITEBACK);\n\n\tbio_put(&bbio->bio);\n}\n\nstatic void prepare_eb_write(struct extent_buffer *eb)\n{\n\tu32 nritems;\n\tunsigned long start;\n\tunsigned long end;\n\n\tclear_bit(EXTENT_BUFFER_WRITE_ERR, &eb->bflags);\n\n\t \n\tnritems = btrfs_header_nritems(eb);\n\tif (btrfs_header_level(eb) > 0) {\n\t\tend = btrfs_node_key_ptr_offset(eb, nritems);\n\t\tmemzero_extent_buffer(eb, end, eb->len - end);\n\t} else {\n\t\t \n\t\tstart = btrfs_item_nr_offset(eb, nritems);\n\t\tend = btrfs_item_nr_offset(eb, 0);\n\t\tif (nritems == 0)\n\t\t\tend += BTRFS_LEAF_DATA_SIZE(eb->fs_info);\n\t\telse\n\t\t\tend += btrfs_item_offset(eb, nritems - 1);\n\t\tmemzero_extent_buffer(eb, start, end - start);\n\t}\n}\n\nstatic noinline_for_stack void write_one_eb(struct extent_buffer *eb,\n\t\t\t\t\t    struct writeback_control *wbc)\n{\n\tstruct btrfs_fs_info *fs_info = eb->fs_info;\n\tstruct btrfs_bio *bbio;\n\n\tprepare_eb_write(eb);\n\n\tbbio = btrfs_bio_alloc(INLINE_EXTENT_BUFFER_PAGES,\n\t\t\t       REQ_OP_WRITE | REQ_META | wbc_to_write_flags(wbc),\n\t\t\t       eb->fs_info, extent_buffer_write_end_io, eb);\n\tbbio->bio.bi_iter.bi_sector = eb->start >> SECTOR_SHIFT;\n\tbio_set_dev(&bbio->bio, fs_info->fs_devices->latest_dev->bdev);\n\twbc_init_bio(wbc, &bbio->bio);\n\tbbio->inode = BTRFS_I(eb->fs_info->btree_inode);\n\tbbio->file_offset = eb->start;\n\tif (fs_info->nodesize < PAGE_SIZE) {\n\t\tstruct page *p = eb->pages[0];\n\n\t\tlock_page(p);\n\t\tbtrfs_subpage_set_writeback(fs_info, p, eb->start, eb->len);\n\t\tif (btrfs_subpage_clear_and_test_dirty(fs_info, p, eb->start,\n\t\t\t\t\t\t       eb->len)) {\n\t\t\tclear_page_dirty_for_io(p);\n\t\t\twbc->nr_to_write--;\n\t\t}\n\t\t__bio_add_page(&bbio->bio, p, eb->len, eb->start - page_offset(p));\n\t\twbc_account_cgroup_owner(wbc, p, eb->len);\n\t\tunlock_page(p);\n\t} else {\n\t\tfor (int i = 0; i < num_extent_pages(eb); i++) {\n\t\t\tstruct page *p = eb->pages[i];\n\n\t\t\tlock_page(p);\n\t\t\tclear_page_dirty_for_io(p);\n\t\t\tset_page_writeback(p);\n\t\t\t__bio_add_page(&bbio->bio, p, PAGE_SIZE, 0);\n\t\t\twbc_account_cgroup_owner(wbc, p, PAGE_SIZE);\n\t\t\twbc->nr_to_write--;\n\t\t\tunlock_page(p);\n\t\t}\n\t}\n\tbtrfs_submit_bio(bbio, 0);\n}\n\n \nstatic int submit_eb_subpage(struct page *page, struct writeback_control *wbc)\n{\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(page->mapping->host->i_sb);\n\tint submitted = 0;\n\tu64 page_start = page_offset(page);\n\tint bit_start = 0;\n\tint sectors_per_node = fs_info->nodesize >> fs_info->sectorsize_bits;\n\n\t \n\twhile (bit_start < fs_info->subpage_info->bitmap_nr_bits) {\n\t\tstruct btrfs_subpage *subpage = (struct btrfs_subpage *)page->private;\n\t\tstruct extent_buffer *eb;\n\t\tunsigned long flags;\n\t\tu64 start;\n\n\t\t \n\t\tspin_lock(&page->mapping->private_lock);\n\t\tif (!PagePrivate(page)) {\n\t\t\tspin_unlock(&page->mapping->private_lock);\n\t\t\tbreak;\n\t\t}\n\t\tspin_lock_irqsave(&subpage->lock, flags);\n\t\tif (!test_bit(bit_start + fs_info->subpage_info->dirty_offset,\n\t\t\t      subpage->bitmaps)) {\n\t\t\tspin_unlock_irqrestore(&subpage->lock, flags);\n\t\t\tspin_unlock(&page->mapping->private_lock);\n\t\t\tbit_start++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tstart = page_start + bit_start * fs_info->sectorsize;\n\t\tbit_start += sectors_per_node;\n\n\t\t \n\t\teb = find_extent_buffer_nolock(fs_info, start);\n\t\tspin_unlock_irqrestore(&subpage->lock, flags);\n\t\tspin_unlock(&page->mapping->private_lock);\n\n\t\t \n\t\tif (!eb)\n\t\t\tcontinue;\n\n\t\tif (lock_extent_buffer_for_io(eb, wbc)) {\n\t\t\twrite_one_eb(eb, wbc);\n\t\t\tsubmitted++;\n\t\t}\n\t\tfree_extent_buffer(eb);\n\t}\n\treturn submitted;\n}\n\n \nstatic int submit_eb_page(struct page *page, struct btrfs_eb_write_context *ctx)\n{\n\tstruct writeback_control *wbc = ctx->wbc;\n\tstruct address_space *mapping = page->mapping;\n\tstruct extent_buffer *eb;\n\tint ret;\n\n\tif (!PagePrivate(page))\n\t\treturn 0;\n\n\tif (btrfs_sb(page->mapping->host->i_sb)->nodesize < PAGE_SIZE)\n\t\treturn submit_eb_subpage(page, wbc);\n\n\tspin_lock(&mapping->private_lock);\n\tif (!PagePrivate(page)) {\n\t\tspin_unlock(&mapping->private_lock);\n\t\treturn 0;\n\t}\n\n\teb = (struct extent_buffer *)page->private;\n\n\t \n\tif (WARN_ON(!eb)) {\n\t\tspin_unlock(&mapping->private_lock);\n\t\treturn 0;\n\t}\n\n\tif (eb == ctx->eb) {\n\t\tspin_unlock(&mapping->private_lock);\n\t\treturn 0;\n\t}\n\tret = atomic_inc_not_zero(&eb->refs);\n\tspin_unlock(&mapping->private_lock);\n\tif (!ret)\n\t\treturn 0;\n\n\tctx->eb = eb;\n\n\tret = btrfs_check_meta_write_pointer(eb->fs_info, ctx);\n\tif (ret) {\n\t\tif (ret == -EBUSY)\n\t\t\tret = 0;\n\t\tfree_extent_buffer(eb);\n\t\treturn ret;\n\t}\n\n\tif (!lock_extent_buffer_for_io(eb, wbc)) {\n\t\tfree_extent_buffer(eb);\n\t\treturn 0;\n\t}\n\t \n\tif (ctx->zoned_bg) {\n\t\t \n\t\tbtrfs_schedule_zone_finish_bg(ctx->zoned_bg, eb);\n\t\tctx->zoned_bg->meta_write_pointer += eb->len;\n\t}\n\twrite_one_eb(eb, wbc);\n\tfree_extent_buffer(eb);\n\treturn 1;\n}\n\nint btree_write_cache_pages(struct address_space *mapping,\n\t\t\t\t   struct writeback_control *wbc)\n{\n\tstruct btrfs_eb_write_context ctx = { .wbc = wbc };\n\tstruct btrfs_fs_info *fs_info = BTRFS_I(mapping->host)->root->fs_info;\n\tint ret = 0;\n\tint done = 0;\n\tint nr_to_write_done = 0;\n\tstruct folio_batch fbatch;\n\tunsigned int nr_folios;\n\tpgoff_t index;\n\tpgoff_t end;\t\t \n\tint scanned = 0;\n\txa_mark_t tag;\n\n\tfolio_batch_init(&fbatch);\n\tif (wbc->range_cyclic) {\n\t\tindex = mapping->writeback_index;  \n\t\tend = -1;\n\t\t \n\t\tscanned = (index == 0);\n\t} else {\n\t\tindex = wbc->range_start >> PAGE_SHIFT;\n\t\tend = wbc->range_end >> PAGE_SHIFT;\n\t\tscanned = 1;\n\t}\n\tif (wbc->sync_mode == WB_SYNC_ALL)\n\t\ttag = PAGECACHE_TAG_TOWRITE;\n\telse\n\t\ttag = PAGECACHE_TAG_DIRTY;\n\tbtrfs_zoned_meta_io_lock(fs_info);\nretry:\n\tif (wbc->sync_mode == WB_SYNC_ALL)\n\t\ttag_pages_for_writeback(mapping, index, end);\n\twhile (!done && !nr_to_write_done && (index <= end) &&\n\t       (nr_folios = filemap_get_folios_tag(mapping, &index, end,\n\t\t\t\t\t    tag, &fbatch))) {\n\t\tunsigned i;\n\n\t\tfor (i = 0; i < nr_folios; i++) {\n\t\t\tstruct folio *folio = fbatch.folios[i];\n\n\t\t\tret = submit_eb_page(&folio->page, &ctx);\n\t\t\tif (ret == 0)\n\t\t\t\tcontinue;\n\t\t\tif (ret < 0) {\n\t\t\t\tdone = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t \n\t\t\tnr_to_write_done = wbc->nr_to_write <= 0;\n\t\t}\n\t\tfolio_batch_release(&fbatch);\n\t\tcond_resched();\n\t}\n\tif (!scanned && !done) {\n\t\t \n\t\tscanned = 1;\n\t\tindex = 0;\n\t\tgoto retry;\n\t}\n\t \n\tif (ret > 0)\n\t\tret = 0;\n\tif (!ret && BTRFS_FS_ERROR(fs_info))\n\t\tret = -EROFS;\n\n\tif (ctx.zoned_bg)\n\t\tbtrfs_put_block_group(ctx.zoned_bg);\n\tbtrfs_zoned_meta_io_unlock(fs_info);\n\treturn ret;\n}\n\n \nstatic int extent_write_cache_pages(struct address_space *mapping,\n\t\t\t     struct btrfs_bio_ctrl *bio_ctrl)\n{\n\tstruct writeback_control *wbc = bio_ctrl->wbc;\n\tstruct inode *inode = mapping->host;\n\tint ret = 0;\n\tint done = 0;\n\tint nr_to_write_done = 0;\n\tstruct folio_batch fbatch;\n\tunsigned int nr_folios;\n\tpgoff_t index;\n\tpgoff_t end;\t\t \n\tpgoff_t done_index;\n\tint range_whole = 0;\n\tint scanned = 0;\n\txa_mark_t tag;\n\n\t \n\tif (!igrab(inode))\n\t\treturn 0;\n\n\tfolio_batch_init(&fbatch);\n\tif (wbc->range_cyclic) {\n\t\tindex = mapping->writeback_index;  \n\t\tend = -1;\n\t\t \n\t\tscanned = (index == 0);\n\t} else {\n\t\tindex = wbc->range_start >> PAGE_SHIFT;\n\t\tend = wbc->range_end >> PAGE_SHIFT;\n\t\tif (wbc->range_start == 0 && wbc->range_end == LLONG_MAX)\n\t\t\trange_whole = 1;\n\t\tscanned = 1;\n\t}\n\n\t \n\tif (range_whole && wbc->nr_to_write == LONG_MAX &&\n\t    test_and_clear_bit(BTRFS_INODE_SNAPSHOT_FLUSH,\n\t\t\t       &BTRFS_I(inode)->runtime_flags))\n\t\twbc->tagged_writepages = 1;\n\n\tif (wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages)\n\t\ttag = PAGECACHE_TAG_TOWRITE;\n\telse\n\t\ttag = PAGECACHE_TAG_DIRTY;\nretry:\n\tif (wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages)\n\t\ttag_pages_for_writeback(mapping, index, end);\n\tdone_index = index;\n\twhile (!done && !nr_to_write_done && (index <= end) &&\n\t\t\t(nr_folios = filemap_get_folios_tag(mapping, &index,\n\t\t\t\t\t\t\tend, tag, &fbatch))) {\n\t\tunsigned i;\n\n\t\tfor (i = 0; i < nr_folios; i++) {\n\t\t\tstruct folio *folio = fbatch.folios[i];\n\n\t\t\tdone_index = folio_next_index(folio);\n\t\t\t \n\t\t\tif (!folio_trylock(folio)) {\n\t\t\t\tsubmit_write_bio(bio_ctrl, 0);\n\t\t\t\tfolio_lock(folio);\n\t\t\t}\n\n\t\t\tif (unlikely(folio->mapping != mapping)) {\n\t\t\t\tfolio_unlock(folio);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (!folio_test_dirty(folio)) {\n\t\t\t\t \n\t\t\t\tfolio_unlock(folio);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (wbc->sync_mode != WB_SYNC_NONE) {\n\t\t\t\tif (folio_test_writeback(folio))\n\t\t\t\t\tsubmit_write_bio(bio_ctrl, 0);\n\t\t\t\tfolio_wait_writeback(folio);\n\t\t\t}\n\n\t\t\tif (folio_test_writeback(folio) ||\n\t\t\t    !folio_clear_dirty_for_io(folio)) {\n\t\t\t\tfolio_unlock(folio);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tret = __extent_writepage(&folio->page, bio_ctrl);\n\t\t\tif (ret < 0) {\n\t\t\t\tdone = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t \n\t\t\tnr_to_write_done = (wbc->sync_mode == WB_SYNC_NONE &&\n\t\t\t\t\t    wbc->nr_to_write <= 0);\n\t\t}\n\t\tfolio_batch_release(&fbatch);\n\t\tcond_resched();\n\t}\n\tif (!scanned && !done) {\n\t\t \n\t\tscanned = 1;\n\t\tindex = 0;\n\n\t\t \n\t\tsubmit_write_bio(bio_ctrl, 0);\n\t\tgoto retry;\n\t}\n\n\tif (wbc->range_cyclic || (wbc->nr_to_write > 0 && range_whole))\n\t\tmapping->writeback_index = done_index;\n\n\tbtrfs_add_delayed_iput(BTRFS_I(inode));\n\treturn ret;\n}\n\n \nvoid extent_write_locked_range(struct inode *inode, struct page *locked_page,\n\t\t\t       u64 start, u64 end, struct writeback_control *wbc,\n\t\t\t       bool pages_dirty)\n{\n\tbool found_error = false;\n\tint ret = 0;\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tconst u32 sectorsize = fs_info->sectorsize;\n\tloff_t i_size = i_size_read(inode);\n\tu64 cur = start;\n\tstruct btrfs_bio_ctrl bio_ctrl = {\n\t\t.wbc = wbc,\n\t\t.opf = REQ_OP_WRITE | wbc_to_write_flags(wbc),\n\t};\n\n\tif (wbc->no_cgroup_owner)\n\t\tbio_ctrl.opf |= REQ_BTRFS_CGROUP_PUNT;\n\n\tASSERT(IS_ALIGNED(start, sectorsize) && IS_ALIGNED(end + 1, sectorsize));\n\n\twhile (cur <= end) {\n\t\tu64 cur_end = min(round_down(cur, PAGE_SIZE) + PAGE_SIZE - 1, end);\n\t\tu32 cur_len = cur_end + 1 - cur;\n\t\tstruct page *page;\n\t\tint nr = 0;\n\n\t\tpage = find_get_page(mapping, cur >> PAGE_SHIFT);\n\t\tASSERT(PageLocked(page));\n\t\tif (pages_dirty && page != locked_page) {\n\t\t\tASSERT(PageDirty(page));\n\t\t\tclear_page_dirty_for_io(page);\n\t\t}\n\n\t\tret = __extent_writepage_io(BTRFS_I(inode), page, &bio_ctrl,\n\t\t\t\t\t    i_size, &nr);\n\t\tif (ret == 1)\n\t\t\tgoto next_page;\n\n\t\t \n\t\tif (nr == 0) {\n\t\t\tset_page_writeback(page);\n\t\t\tend_page_writeback(page);\n\t\t}\n\t\tif (ret) {\n\t\t\tbtrfs_mark_ordered_io_finished(BTRFS_I(inode), page,\n\t\t\t\t\t\t       cur, cur_len, !ret);\n\t\t\tmapping_set_error(page->mapping, ret);\n\t\t}\n\t\tbtrfs_page_unlock_writer(fs_info, page, cur, cur_len);\n\t\tif (ret < 0)\n\t\t\tfound_error = true;\nnext_page:\n\t\tput_page(page);\n\t\tcur = cur_end + 1;\n\t}\n\n\tsubmit_write_bio(&bio_ctrl, found_error ? ret : 0);\n}\n\nint extent_writepages(struct address_space *mapping,\n\t\t      struct writeback_control *wbc)\n{\n\tstruct inode *inode = mapping->host;\n\tint ret = 0;\n\tstruct btrfs_bio_ctrl bio_ctrl = {\n\t\t.wbc = wbc,\n\t\t.opf = REQ_OP_WRITE | wbc_to_write_flags(wbc),\n\t};\n\n\t \n\tbtrfs_zoned_data_reloc_lock(BTRFS_I(inode));\n\tret = extent_write_cache_pages(mapping, &bio_ctrl);\n\tsubmit_write_bio(&bio_ctrl, ret);\n\tbtrfs_zoned_data_reloc_unlock(BTRFS_I(inode));\n\treturn ret;\n}\n\nvoid extent_readahead(struct readahead_control *rac)\n{\n\tstruct btrfs_bio_ctrl bio_ctrl = { .opf = REQ_OP_READ | REQ_RAHEAD };\n\tstruct page *pagepool[16];\n\tstruct extent_map *em_cached = NULL;\n\tu64 prev_em_start = (u64)-1;\n\tint nr;\n\n\twhile ((nr = readahead_page_batch(rac, pagepool))) {\n\t\tu64 contig_start = readahead_pos(rac);\n\t\tu64 contig_end = contig_start + readahead_batch_length(rac) - 1;\n\n\t\tcontiguous_readpages(pagepool, nr, contig_start, contig_end,\n\t\t\t\t&em_cached, &bio_ctrl, &prev_em_start);\n\t}\n\n\tif (em_cached)\n\t\tfree_extent_map(em_cached);\n\tsubmit_one_bio(&bio_ctrl);\n}\n\n \nint extent_invalidate_folio(struct extent_io_tree *tree,\n\t\t\t  struct folio *folio, size_t offset)\n{\n\tstruct extent_state *cached_state = NULL;\n\tu64 start = folio_pos(folio);\n\tu64 end = start + folio_size(folio) - 1;\n\tsize_t blocksize = folio->mapping->host->i_sb->s_blocksize;\n\n\t \n\tASSERT(tree->owner == IO_TREE_BTREE_INODE_IO);\n\n\tstart += ALIGN(offset, blocksize);\n\tif (start > end)\n\t\treturn 0;\n\n\tlock_extent(tree, start, end, &cached_state);\n\tfolio_wait_writeback(folio);\n\n\t \n\tunlock_extent(tree, start, end, &cached_state);\n\treturn 0;\n}\n\n \nstatic int try_release_extent_state(struct extent_io_tree *tree,\n\t\t\t\t    struct page *page, gfp_t mask)\n{\n\tu64 start = page_offset(page);\n\tu64 end = start + PAGE_SIZE - 1;\n\tint ret = 1;\n\n\tif (test_range_bit(tree, start, end, EXTENT_LOCKED, 0, NULL)) {\n\t\tret = 0;\n\t} else {\n\t\tu32 clear_bits = ~(EXTENT_LOCKED | EXTENT_NODATASUM |\n\t\t\t\t   EXTENT_DELALLOC_NEW | EXTENT_CTLBITS |\n\t\t\t\t   EXTENT_QGROUP_RESERVED);\n\n\t\t \n\t\tret = __clear_extent_bit(tree, start, end, clear_bits, NULL, NULL);\n\n\t\t \n\t\tif (ret < 0)\n\t\t\tret = 0;\n\t\telse\n\t\t\tret = 1;\n\t}\n\treturn ret;\n}\n\n \nint try_release_extent_mapping(struct page *page, gfp_t mask)\n{\n\tstruct extent_map *em;\n\tu64 start = page_offset(page);\n\tu64 end = start + PAGE_SIZE - 1;\n\tstruct btrfs_inode *btrfs_inode = BTRFS_I(page->mapping->host);\n\tstruct extent_io_tree *tree = &btrfs_inode->io_tree;\n\tstruct extent_map_tree *map = &btrfs_inode->extent_tree;\n\n\tif (gfpflags_allow_blocking(mask) &&\n\t    page->mapping->host->i_size > SZ_16M) {\n\t\tu64 len;\n\t\twhile (start <= end) {\n\t\t\tstruct btrfs_fs_info *fs_info;\n\t\t\tu64 cur_gen;\n\n\t\t\tlen = end - start + 1;\n\t\t\twrite_lock(&map->lock);\n\t\t\tem = lookup_extent_mapping(map, start, len);\n\t\t\tif (!em) {\n\t\t\t\twrite_unlock(&map->lock);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (test_bit(EXTENT_FLAG_PINNED, &em->flags) ||\n\t\t\t    em->start != start) {\n\t\t\t\twrite_unlock(&map->lock);\n\t\t\t\tfree_extent_map(em);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (test_range_bit(tree, em->start,\n\t\t\t\t\t   extent_map_end(em) - 1,\n\t\t\t\t\t   EXTENT_LOCKED, 0, NULL))\n\t\t\t\tgoto next;\n\t\t\t \n\t\t\tif (list_empty(&em->list) ||\n\t\t\t    test_bit(EXTENT_FLAG_LOGGING, &em->flags))\n\t\t\t\tgoto remove_em;\n\t\t\t \n\t\t\tfs_info = btrfs_inode->root->fs_info;\n\t\t\tspin_lock(&fs_info->trans_lock);\n\t\t\tcur_gen = fs_info->generation;\n\t\t\tspin_unlock(&fs_info->trans_lock);\n\t\t\tif (em->generation >= cur_gen)\n\t\t\t\tgoto next;\nremove_em:\n\t\t\t \n\t\t\tremove_extent_mapping(map, em);\n\t\t\t \n\t\t\tfree_extent_map(em);\nnext:\n\t\t\tstart = extent_map_end(em);\n\t\t\twrite_unlock(&map->lock);\n\n\t\t\t \n\t\t\tfree_extent_map(em);\n\n\t\t\tcond_resched();  \n\t\t}\n\t}\n\treturn try_release_extent_state(tree, page, mask);\n}\n\n \nstruct fiemap_cache {\n\tu64 offset;\n\tu64 phys;\n\tu64 len;\n\tu32 flags;\n\tbool cached;\n};\n\n \nstatic int emit_fiemap_extent(struct fiemap_extent_info *fieinfo,\n\t\t\t\tstruct fiemap_cache *cache,\n\t\t\t\tu64 offset, u64 phys, u64 len, u32 flags)\n{\n\tint ret = 0;\n\n\t \n\tASSERT((flags & FIEMAP_EXTENT_LAST) == 0);\n\n\tif (!cache->cached)\n\t\tgoto assign;\n\n\t \n\tif (cache->offset + cache->len > offset) {\n\t\tWARN_ON(1);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (cache->offset + cache->len  == offset &&\n\t    cache->phys + cache->len == phys  &&\n\t    cache->flags == flags) {\n\t\tcache->len += len;\n\t\treturn 0;\n\t}\n\n\t \n\tret = fiemap_fill_next_extent(fieinfo, cache->offset, cache->phys,\n\t\t\t\t      cache->len, cache->flags);\n\tcache->cached = false;\n\tif (ret)\n\t\treturn ret;\nassign:\n\tcache->cached = true;\n\tcache->offset = offset;\n\tcache->phys = phys;\n\tcache->len = len;\n\tcache->flags = flags;\n\n\treturn 0;\n}\n\n \nstatic int emit_last_fiemap_cache(struct fiemap_extent_info *fieinfo,\n\t\t\t\t  struct fiemap_cache *cache)\n{\n\tint ret;\n\n\tif (!cache->cached)\n\t\treturn 0;\n\n\tret = fiemap_fill_next_extent(fieinfo, cache->offset, cache->phys,\n\t\t\t\t      cache->len, cache->flags);\n\tcache->cached = false;\n\tif (ret > 0)\n\t\tret = 0;\n\treturn ret;\n}\n\nstatic int fiemap_next_leaf_item(struct btrfs_inode *inode, struct btrfs_path *path)\n{\n\tstruct extent_buffer *clone;\n\tstruct btrfs_key key;\n\tint slot;\n\tint ret;\n\n\tpath->slots[0]++;\n\tif (path->slots[0] < btrfs_header_nritems(path->nodes[0]))\n\t\treturn 0;\n\n\tret = btrfs_next_leaf(inode->root, path);\n\tif (ret != 0)\n\t\treturn ret;\n\n\t \n\tbtrfs_item_key_to_cpu(path->nodes[0], &key, path->slots[0]);\n\tif (key.objectid != btrfs_ino(inode) || key.type != BTRFS_EXTENT_DATA_KEY)\n\t\treturn 1;\n\n\t \n\tclone = btrfs_clone_extent_buffer(path->nodes[0]);\n\tif (!clone)\n\t\treturn -ENOMEM;\n\n\tslot = path->slots[0];\n\tbtrfs_release_path(path);\n\tpath->nodes[0] = clone;\n\tpath->slots[0] = slot;\n\n\treturn 0;\n}\n\n \nstatic int fiemap_search_slot(struct btrfs_inode *inode, struct btrfs_path *path,\n\t\t\t      u64 file_offset)\n{\n\tconst u64 ino = btrfs_ino(inode);\n\tstruct btrfs_root *root = inode->root;\n\tstruct extent_buffer *clone;\n\tstruct btrfs_key key;\n\tint slot;\n\tint ret;\n\n\tkey.objectid = ino;\n\tkey.type = BTRFS_EXTENT_DATA_KEY;\n\tkey.offset = file_offset;\n\n\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (ret > 0 && path->slots[0] > 0) {\n\t\tbtrfs_item_key_to_cpu(path->nodes[0], &key, path->slots[0] - 1);\n\t\tif (key.objectid == ino && key.type == BTRFS_EXTENT_DATA_KEY)\n\t\t\tpath->slots[0]--;\n\t}\n\n\tif (path->slots[0] >= btrfs_header_nritems(path->nodes[0])) {\n\t\tret = btrfs_next_leaf(root, path);\n\t\tif (ret != 0)\n\t\t\treturn ret;\n\n\t\tbtrfs_item_key_to_cpu(path->nodes[0], &key, path->slots[0]);\n\t\tif (key.objectid != ino || key.type != BTRFS_EXTENT_DATA_KEY)\n\t\t\treturn 1;\n\t}\n\n\t \n\tclone = btrfs_clone_extent_buffer(path->nodes[0]);\n\tif (!clone)\n\t\treturn -ENOMEM;\n\n\tslot = path->slots[0];\n\tbtrfs_release_path(path);\n\tpath->nodes[0] = clone;\n\tpath->slots[0] = slot;\n\n\treturn 0;\n}\n\n \nstatic int fiemap_process_hole(struct btrfs_inode *inode,\n\t\t\t       struct fiemap_extent_info *fieinfo,\n\t\t\t       struct fiemap_cache *cache,\n\t\t\t       struct extent_state **delalloc_cached_state,\n\t\t\t       struct btrfs_backref_share_check_ctx *backref_ctx,\n\t\t\t       u64 disk_bytenr, u64 extent_offset,\n\t\t\t       u64 extent_gen,\n\t\t\t       u64 start, u64 end)\n{\n\tconst u64 i_size = i_size_read(&inode->vfs_inode);\n\tu64 cur_offset = start;\n\tu64 last_delalloc_end = 0;\n\tu32 prealloc_flags = FIEMAP_EXTENT_UNWRITTEN;\n\tbool checked_extent_shared = false;\n\tint ret;\n\n\t \n\twhile (cur_offset < end && cur_offset < i_size) {\n\t\tu64 delalloc_start;\n\t\tu64 delalloc_end;\n\t\tu64 prealloc_start;\n\t\tu64 prealloc_len = 0;\n\t\tbool delalloc;\n\n\t\tdelalloc = btrfs_find_delalloc_in_range(inode, cur_offset, end,\n\t\t\t\t\t\t\tdelalloc_cached_state,\n\t\t\t\t\t\t\t&delalloc_start,\n\t\t\t\t\t\t\t&delalloc_end);\n\t\tif (!delalloc)\n\t\t\tbreak;\n\n\t\t \n\t\tif (disk_bytenr != 0) {\n\t\t\tif (last_delalloc_end == 0) {\n\t\t\t\tprealloc_start = start;\n\t\t\t\tprealloc_len = delalloc_start - start;\n\t\t\t} else {\n\t\t\t\tprealloc_start = last_delalloc_end + 1;\n\t\t\t\tprealloc_len = delalloc_start - prealloc_start;\n\t\t\t}\n\t\t}\n\n\t\tif (prealloc_len > 0) {\n\t\t\tif (!checked_extent_shared && fieinfo->fi_extents_max) {\n\t\t\t\tret = btrfs_is_data_extent_shared(inode,\n\t\t\t\t\t\t\t\t  disk_bytenr,\n\t\t\t\t\t\t\t\t  extent_gen,\n\t\t\t\t\t\t\t\t  backref_ctx);\n\t\t\t\tif (ret < 0)\n\t\t\t\t\treturn ret;\n\t\t\t\telse if (ret > 0)\n\t\t\t\t\tprealloc_flags |= FIEMAP_EXTENT_SHARED;\n\n\t\t\t\tchecked_extent_shared = true;\n\t\t\t}\n\t\t\tret = emit_fiemap_extent(fieinfo, cache, prealloc_start,\n\t\t\t\t\t\t disk_bytenr + extent_offset,\n\t\t\t\t\t\t prealloc_len, prealloc_flags);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t\textent_offset += prealloc_len;\n\t\t}\n\n\t\tret = emit_fiemap_extent(fieinfo, cache, delalloc_start, 0,\n\t\t\t\t\t delalloc_end + 1 - delalloc_start,\n\t\t\t\t\t FIEMAP_EXTENT_DELALLOC |\n\t\t\t\t\t FIEMAP_EXTENT_UNKNOWN);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tlast_delalloc_end = delalloc_end;\n\t\tcur_offset = delalloc_end + 1;\n\t\textent_offset += cur_offset - delalloc_start;\n\t\tcond_resched();\n\t}\n\n\t \n\tif (disk_bytenr != 0 && last_delalloc_end < end) {\n\t\tu64 prealloc_start;\n\t\tu64 prealloc_len;\n\n\t\tif (last_delalloc_end == 0) {\n\t\t\tprealloc_start = start;\n\t\t\tprealloc_len = end + 1 - start;\n\t\t} else {\n\t\t\tprealloc_start = last_delalloc_end + 1;\n\t\t\tprealloc_len = end + 1 - prealloc_start;\n\t\t}\n\n\t\tif (!checked_extent_shared && fieinfo->fi_extents_max) {\n\t\t\tret = btrfs_is_data_extent_shared(inode,\n\t\t\t\t\t\t\t  disk_bytenr,\n\t\t\t\t\t\t\t  extent_gen,\n\t\t\t\t\t\t\t  backref_ctx);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\t\t\telse if (ret > 0)\n\t\t\t\tprealloc_flags |= FIEMAP_EXTENT_SHARED;\n\t\t}\n\t\tret = emit_fiemap_extent(fieinfo, cache, prealloc_start,\n\t\t\t\t\t disk_bytenr + extent_offset,\n\t\t\t\t\t prealloc_len, prealloc_flags);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int fiemap_find_last_extent_offset(struct btrfs_inode *inode,\n\t\t\t\t\t  struct btrfs_path *path,\n\t\t\t\t\t  u64 *last_extent_end_ret)\n{\n\tconst u64 ino = btrfs_ino(inode);\n\tstruct btrfs_root *root = inode->root;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_file_extent_item *ei;\n\tstruct btrfs_key key;\n\tu64 disk_bytenr;\n\tint ret;\n\n\t \n\tret = btrfs_lookup_file_extent(NULL, root, path, ino, (u64)-1, 0);\n\t \n\tASSERT(ret != 0);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t \n\tASSERT(path->slots[0] > 0);\n\tpath->slots[0]--;\n\tleaf = path->nodes[0];\n\tbtrfs_item_key_to_cpu(leaf, &key, path->slots[0]);\n\tif (key.objectid != ino || key.type != BTRFS_EXTENT_DATA_KEY) {\n\t\t \n\t\t*last_extent_end_ret = 0;\n\t\treturn 0;\n\t}\n\n\t \n\tei = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_file_extent_item);\n\tif (btrfs_file_extent_type(leaf, ei) == BTRFS_FILE_EXTENT_INLINE) {\n\t\t*last_extent_end_ret = btrfs_file_extent_end(path);\n\t\treturn 0;\n\t}\n\n\t \n\tdisk_bytenr = btrfs_file_extent_disk_bytenr(leaf, ei);\n\twhile (disk_bytenr == 0) {\n\t\tret = btrfs_previous_item(root, path, ino, BTRFS_EXTENT_DATA_KEY);\n\t\tif (ret < 0) {\n\t\t\treturn ret;\n\t\t} else if (ret > 0) {\n\t\t\t \n\t\t\t*last_extent_end_ret = 0;\n\t\t\treturn 0;\n\t\t}\n\t\tleaf = path->nodes[0];\n\t\tei = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t    struct btrfs_file_extent_item);\n\t\tdisk_bytenr = btrfs_file_extent_disk_bytenr(leaf, ei);\n\t}\n\n\t*last_extent_end_ret = btrfs_file_extent_end(path);\n\treturn 0;\n}\n\nint extent_fiemap(struct btrfs_inode *inode, struct fiemap_extent_info *fieinfo,\n\t\t  u64 start, u64 len)\n{\n\tconst u64 ino = btrfs_ino(inode);\n\tstruct extent_state *cached_state = NULL;\n\tstruct extent_state *delalloc_cached_state = NULL;\n\tstruct btrfs_path *path;\n\tstruct fiemap_cache cache = { 0 };\n\tstruct btrfs_backref_share_check_ctx *backref_ctx;\n\tu64 last_extent_end;\n\tu64 prev_extent_end;\n\tu64 lockstart;\n\tu64 lockend;\n\tbool stopped = false;\n\tint ret;\n\n\tbackref_ctx = btrfs_alloc_backref_share_check_ctx();\n\tpath = btrfs_alloc_path();\n\tif (!backref_ctx || !path) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tlockstart = round_down(start, inode->root->fs_info->sectorsize);\n\tlockend = round_up(start + len, inode->root->fs_info->sectorsize);\n\tprev_extent_end = lockstart;\n\n\tbtrfs_inode_lock(inode, BTRFS_ILOCK_SHARED);\n\tlock_extent(&inode->io_tree, lockstart, lockend, &cached_state);\n\n\tret = fiemap_find_last_extent_offset(inode, path, &last_extent_end);\n\tif (ret < 0)\n\t\tgoto out_unlock;\n\tbtrfs_release_path(path);\n\n\tpath->reada = READA_FORWARD;\n\tret = fiemap_search_slot(inode, path, lockstart);\n\tif (ret < 0) {\n\t\tgoto out_unlock;\n\t} else if (ret > 0) {\n\t\t \n\t\tret = 0;\n\t\tgoto check_eof_delalloc;\n\t}\n\n\twhile (prev_extent_end < lockend) {\n\t\tstruct extent_buffer *leaf = path->nodes[0];\n\t\tstruct btrfs_file_extent_item *ei;\n\t\tstruct btrfs_key key;\n\t\tu64 extent_end;\n\t\tu64 extent_len;\n\t\tu64 extent_offset = 0;\n\t\tu64 extent_gen;\n\t\tu64 disk_bytenr = 0;\n\t\tu64 flags = 0;\n\t\tint extent_type;\n\t\tu8 compression;\n\n\t\tbtrfs_item_key_to_cpu(leaf, &key, path->slots[0]);\n\t\tif (key.objectid != ino || key.type != BTRFS_EXTENT_DATA_KEY)\n\t\t\tbreak;\n\n\t\textent_end = btrfs_file_extent_end(path);\n\n\t\t \n\t\tif (extent_end <= lockstart)\n\t\t\tgoto next_item;\n\n\t\tbackref_ctx->curr_leaf_bytenr = leaf->start;\n\n\t\t \n\t\tif (prev_extent_end < key.offset) {\n\t\t\tconst u64 range_end = min(key.offset, lockend) - 1;\n\n\t\t\tret = fiemap_process_hole(inode, fieinfo, &cache,\n\t\t\t\t\t\t  &delalloc_cached_state,\n\t\t\t\t\t\t  backref_ctx, 0, 0, 0,\n\t\t\t\t\t\t  prev_extent_end, range_end);\n\t\t\tif (ret < 0) {\n\t\t\t\tgoto out_unlock;\n\t\t\t} else if (ret > 0) {\n\t\t\t\t \n\t\t\t\tstopped = true;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (key.offset >= lockend) {\n\t\t\t\tstopped = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\textent_len = extent_end - key.offset;\n\t\tei = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t    struct btrfs_file_extent_item);\n\t\tcompression = btrfs_file_extent_compression(leaf, ei);\n\t\textent_type = btrfs_file_extent_type(leaf, ei);\n\t\textent_gen = btrfs_file_extent_generation(leaf, ei);\n\n\t\tif (extent_type != BTRFS_FILE_EXTENT_INLINE) {\n\t\t\tdisk_bytenr = btrfs_file_extent_disk_bytenr(leaf, ei);\n\t\t\tif (compression == BTRFS_COMPRESS_NONE)\n\t\t\t\textent_offset = btrfs_file_extent_offset(leaf, ei);\n\t\t}\n\n\t\tif (compression != BTRFS_COMPRESS_NONE)\n\t\t\tflags |= FIEMAP_EXTENT_ENCODED;\n\n\t\tif (extent_type == BTRFS_FILE_EXTENT_INLINE) {\n\t\t\tflags |= FIEMAP_EXTENT_DATA_INLINE;\n\t\t\tflags |= FIEMAP_EXTENT_NOT_ALIGNED;\n\t\t\tret = emit_fiemap_extent(fieinfo, &cache, key.offset, 0,\n\t\t\t\t\t\t extent_len, flags);\n\t\t} else if (extent_type == BTRFS_FILE_EXTENT_PREALLOC) {\n\t\t\tret = fiemap_process_hole(inode, fieinfo, &cache,\n\t\t\t\t\t\t  &delalloc_cached_state,\n\t\t\t\t\t\t  backref_ctx,\n\t\t\t\t\t\t  disk_bytenr, extent_offset,\n\t\t\t\t\t\t  extent_gen, key.offset,\n\t\t\t\t\t\t  extent_end - 1);\n\t\t} else if (disk_bytenr == 0) {\n\t\t\t \n\t\t\tret = fiemap_process_hole(inode, fieinfo, &cache,\n\t\t\t\t\t\t  &delalloc_cached_state,\n\t\t\t\t\t\t  backref_ctx, 0, 0, 0,\n\t\t\t\t\t\t  key.offset, extent_end - 1);\n\t\t} else {\n\t\t\t \n\t\t\tif (fieinfo->fi_extents_max) {\n\t\t\t\tret = btrfs_is_data_extent_shared(inode,\n\t\t\t\t\t\t\t\t  disk_bytenr,\n\t\t\t\t\t\t\t\t  extent_gen,\n\t\t\t\t\t\t\t\t  backref_ctx);\n\t\t\t\tif (ret < 0)\n\t\t\t\t\tgoto out_unlock;\n\t\t\t\telse if (ret > 0)\n\t\t\t\t\tflags |= FIEMAP_EXTENT_SHARED;\n\t\t\t}\n\n\t\t\tret = emit_fiemap_extent(fieinfo, &cache, key.offset,\n\t\t\t\t\t\t disk_bytenr + extent_offset,\n\t\t\t\t\t\t extent_len, flags);\n\t\t}\n\n\t\tif (ret < 0) {\n\t\t\tgoto out_unlock;\n\t\t} else if (ret > 0) {\n\t\t\t \n\t\t\tstopped = true;\n\t\t\tbreak;\n\t\t}\n\n\t\tprev_extent_end = extent_end;\nnext_item:\n\t\tif (fatal_signal_pending(current)) {\n\t\t\tret = -EINTR;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tret = fiemap_next_leaf_item(inode, path);\n\t\tif (ret < 0) {\n\t\t\tgoto out_unlock;\n\t\t} else if (ret > 0) {\n\t\t\t \n\t\t\tbreak;\n\t\t}\n\t\tcond_resched();\n\t}\n\ncheck_eof_delalloc:\n\t \n\tbtrfs_free_path(path);\n\tpath = NULL;\n\n\tif (!stopped && prev_extent_end < lockend) {\n\t\tret = fiemap_process_hole(inode, fieinfo, &cache,\n\t\t\t\t\t  &delalloc_cached_state, backref_ctx,\n\t\t\t\t\t  0, 0, 0, prev_extent_end, lockend - 1);\n\t\tif (ret < 0)\n\t\t\tgoto out_unlock;\n\t\tprev_extent_end = lockend;\n\t}\n\n\tif (cache.cached && cache.offset + cache.len >= last_extent_end) {\n\t\tconst u64 i_size = i_size_read(&inode->vfs_inode);\n\n\t\tif (prev_extent_end < i_size) {\n\t\t\tu64 delalloc_start;\n\t\t\tu64 delalloc_end;\n\t\t\tbool delalloc;\n\n\t\t\tdelalloc = btrfs_find_delalloc_in_range(inode,\n\t\t\t\t\t\t\t\tprev_extent_end,\n\t\t\t\t\t\t\t\ti_size - 1,\n\t\t\t\t\t\t\t\t&delalloc_cached_state,\n\t\t\t\t\t\t\t\t&delalloc_start,\n\t\t\t\t\t\t\t\t&delalloc_end);\n\t\t\tif (!delalloc)\n\t\t\t\tcache.flags |= FIEMAP_EXTENT_LAST;\n\t\t} else {\n\t\t\tcache.flags |= FIEMAP_EXTENT_LAST;\n\t\t}\n\t}\n\n\tret = emit_last_fiemap_cache(fieinfo, &cache);\n\nout_unlock:\n\tunlock_extent(&inode->io_tree, lockstart, lockend, &cached_state);\n\tbtrfs_inode_unlock(inode, BTRFS_ILOCK_SHARED);\nout:\n\tfree_extent_state(delalloc_cached_state);\n\tbtrfs_free_backref_share_ctx(backref_ctx);\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nstatic void __free_extent_buffer(struct extent_buffer *eb)\n{\n\tkmem_cache_free(extent_buffer_cache, eb);\n}\n\nstatic int extent_buffer_under_io(const struct extent_buffer *eb)\n{\n\treturn (test_bit(EXTENT_BUFFER_WRITEBACK, &eb->bflags) ||\n\t\ttest_bit(EXTENT_BUFFER_DIRTY, &eb->bflags));\n}\n\nstatic bool page_range_has_eb(struct btrfs_fs_info *fs_info, struct page *page)\n{\n\tstruct btrfs_subpage *subpage;\n\n\tlockdep_assert_held(&page->mapping->private_lock);\n\n\tif (PagePrivate(page)) {\n\t\tsubpage = (struct btrfs_subpage *)page->private;\n\t\tif (atomic_read(&subpage->eb_refs))\n\t\t\treturn true;\n\t\t \n\t\tif (atomic_read(&subpage->readers))\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic void detach_extent_buffer_page(struct extent_buffer *eb, struct page *page)\n{\n\tstruct btrfs_fs_info *fs_info = eb->fs_info;\n\tconst bool mapped = !test_bit(EXTENT_BUFFER_UNMAPPED, &eb->bflags);\n\n\t \n\tif (mapped)\n\t\tspin_lock(&page->mapping->private_lock);\n\n\tif (!PagePrivate(page)) {\n\t\tif (mapped)\n\t\t\tspin_unlock(&page->mapping->private_lock);\n\t\treturn;\n\t}\n\n\tif (fs_info->nodesize >= PAGE_SIZE) {\n\t\t \n\t\tif (PagePrivate(page) &&\n\t\t    page->private == (unsigned long)eb) {\n\t\t\tBUG_ON(test_bit(EXTENT_BUFFER_DIRTY, &eb->bflags));\n\t\t\tBUG_ON(PageDirty(page));\n\t\t\tBUG_ON(PageWriteback(page));\n\t\t\t \n\t\t\tdetach_page_private(page);\n\t\t}\n\t\tif (mapped)\n\t\t\tspin_unlock(&page->mapping->private_lock);\n\t\treturn;\n\t}\n\n\t \n\tif (!mapped) {\n\t\tbtrfs_detach_subpage(fs_info, page);\n\t\treturn;\n\t}\n\n\tbtrfs_page_dec_eb_refs(fs_info, page);\n\n\t \n\tif (!page_range_has_eb(fs_info, page))\n\t\tbtrfs_detach_subpage(fs_info, page);\n\n\tspin_unlock(&page->mapping->private_lock);\n}\n\n \nstatic void btrfs_release_extent_buffer_pages(struct extent_buffer *eb)\n{\n\tint i;\n\tint num_pages;\n\n\tASSERT(!extent_buffer_under_io(eb));\n\n\tnum_pages = num_extent_pages(eb);\n\tfor (i = 0; i < num_pages; i++) {\n\t\tstruct page *page = eb->pages[i];\n\n\t\tif (!page)\n\t\t\tcontinue;\n\n\t\tdetach_extent_buffer_page(eb, page);\n\n\t\t \n\t\tput_page(page);\n\t}\n}\n\n \nstatic inline void btrfs_release_extent_buffer(struct extent_buffer *eb)\n{\n\tbtrfs_release_extent_buffer_pages(eb);\n\tbtrfs_leak_debug_del_eb(eb);\n\t__free_extent_buffer(eb);\n}\n\nstatic struct extent_buffer *\n__alloc_extent_buffer(struct btrfs_fs_info *fs_info, u64 start,\n\t\t      unsigned long len)\n{\n\tstruct extent_buffer *eb = NULL;\n\n\teb = kmem_cache_zalloc(extent_buffer_cache, GFP_NOFS|__GFP_NOFAIL);\n\teb->start = start;\n\teb->len = len;\n\teb->fs_info = fs_info;\n\tinit_rwsem(&eb->lock);\n\n\tbtrfs_leak_debug_add_eb(eb);\n\n\tspin_lock_init(&eb->refs_lock);\n\tatomic_set(&eb->refs, 1);\n\n\tASSERT(len <= BTRFS_MAX_METADATA_BLOCKSIZE);\n\n\treturn eb;\n}\n\nstruct extent_buffer *btrfs_clone_extent_buffer(const struct extent_buffer *src)\n{\n\tint i;\n\tstruct extent_buffer *new;\n\tint num_pages = num_extent_pages(src);\n\tint ret;\n\n\tnew = __alloc_extent_buffer(src->fs_info, src->start, src->len);\n\tif (new == NULL)\n\t\treturn NULL;\n\n\t \n\tset_bit(EXTENT_BUFFER_UNMAPPED, &new->bflags);\n\n\tret = btrfs_alloc_page_array(num_pages, new->pages);\n\tif (ret) {\n\t\tbtrfs_release_extent_buffer(new);\n\t\treturn NULL;\n\t}\n\n\tfor (i = 0; i < num_pages; i++) {\n\t\tint ret;\n\t\tstruct page *p = new->pages[i];\n\n\t\tret = attach_extent_buffer_page(new, p, NULL);\n\t\tif (ret < 0) {\n\t\t\tbtrfs_release_extent_buffer(new);\n\t\t\treturn NULL;\n\t\t}\n\t\tWARN_ON(PageDirty(p));\n\t}\n\tcopy_extent_buffer_full(new, src);\n\tset_extent_buffer_uptodate(new);\n\n\treturn new;\n}\n\nstruct extent_buffer *__alloc_dummy_extent_buffer(struct btrfs_fs_info *fs_info,\n\t\t\t\t\t\t  u64 start, unsigned long len)\n{\n\tstruct extent_buffer *eb;\n\tint num_pages;\n\tint i;\n\tint ret;\n\n\teb = __alloc_extent_buffer(fs_info, start, len);\n\tif (!eb)\n\t\treturn NULL;\n\n\tnum_pages = num_extent_pages(eb);\n\tret = btrfs_alloc_page_array(num_pages, eb->pages);\n\tif (ret)\n\t\tgoto err;\n\n\tfor (i = 0; i < num_pages; i++) {\n\t\tstruct page *p = eb->pages[i];\n\n\t\tret = attach_extent_buffer_page(eb, p, NULL);\n\t\tif (ret < 0)\n\t\t\tgoto err;\n\t}\n\n\tset_extent_buffer_uptodate(eb);\n\tbtrfs_set_header_nritems(eb, 0);\n\tset_bit(EXTENT_BUFFER_UNMAPPED, &eb->bflags);\n\n\treturn eb;\nerr:\n\tfor (i = 0; i < num_pages; i++) {\n\t\tif (eb->pages[i]) {\n\t\t\tdetach_extent_buffer_page(eb, eb->pages[i]);\n\t\t\t__free_page(eb->pages[i]);\n\t\t}\n\t}\n\t__free_extent_buffer(eb);\n\treturn NULL;\n}\n\nstruct extent_buffer *alloc_dummy_extent_buffer(struct btrfs_fs_info *fs_info,\n\t\t\t\t\t\tu64 start)\n{\n\treturn __alloc_dummy_extent_buffer(fs_info, start, fs_info->nodesize);\n}\n\nstatic void check_buffer_tree_ref(struct extent_buffer *eb)\n{\n\tint refs;\n\t \n\trefs = atomic_read(&eb->refs);\n\tif (refs >= 2 && test_bit(EXTENT_BUFFER_TREE_REF, &eb->bflags))\n\t\treturn;\n\n\tspin_lock(&eb->refs_lock);\n\tif (!test_and_set_bit(EXTENT_BUFFER_TREE_REF, &eb->bflags))\n\t\tatomic_inc(&eb->refs);\n\tspin_unlock(&eb->refs_lock);\n}\n\nstatic void mark_extent_buffer_accessed(struct extent_buffer *eb,\n\t\tstruct page *accessed)\n{\n\tint num_pages, i;\n\n\tcheck_buffer_tree_ref(eb);\n\n\tnum_pages = num_extent_pages(eb);\n\tfor (i = 0; i < num_pages; i++) {\n\t\tstruct page *p = eb->pages[i];\n\n\t\tif (p != accessed)\n\t\t\tmark_page_accessed(p);\n\t}\n}\n\nstruct extent_buffer *find_extent_buffer(struct btrfs_fs_info *fs_info,\n\t\t\t\t\t u64 start)\n{\n\tstruct extent_buffer *eb;\n\n\teb = find_extent_buffer_nolock(fs_info, start);\n\tif (!eb)\n\t\treturn NULL;\n\t \n\tif (test_bit(EXTENT_BUFFER_STALE, &eb->bflags)) {\n\t\tspin_lock(&eb->refs_lock);\n\t\tspin_unlock(&eb->refs_lock);\n\t}\n\tmark_extent_buffer_accessed(eb, NULL);\n\treturn eb;\n}\n\n#ifdef CONFIG_BTRFS_FS_RUN_SANITY_TESTS\nstruct extent_buffer *alloc_test_extent_buffer(struct btrfs_fs_info *fs_info,\n\t\t\t\t\tu64 start)\n{\n\tstruct extent_buffer *eb, *exists = NULL;\n\tint ret;\n\n\teb = find_extent_buffer(fs_info, start);\n\tif (eb)\n\t\treturn eb;\n\teb = alloc_dummy_extent_buffer(fs_info, start);\n\tif (!eb)\n\t\treturn ERR_PTR(-ENOMEM);\n\teb->fs_info = fs_info;\nagain:\n\tret = radix_tree_preload(GFP_NOFS);\n\tif (ret) {\n\t\texists = ERR_PTR(ret);\n\t\tgoto free_eb;\n\t}\n\tspin_lock(&fs_info->buffer_lock);\n\tret = radix_tree_insert(&fs_info->buffer_radix,\n\t\t\t\tstart >> fs_info->sectorsize_bits, eb);\n\tspin_unlock(&fs_info->buffer_lock);\n\tradix_tree_preload_end();\n\tif (ret == -EEXIST) {\n\t\texists = find_extent_buffer(fs_info, start);\n\t\tif (exists)\n\t\t\tgoto free_eb;\n\t\telse\n\t\t\tgoto again;\n\t}\n\tcheck_buffer_tree_ref(eb);\n\tset_bit(EXTENT_BUFFER_IN_TREE, &eb->bflags);\n\n\treturn eb;\nfree_eb:\n\tbtrfs_release_extent_buffer(eb);\n\treturn exists;\n}\n#endif\n\nstatic struct extent_buffer *grab_extent_buffer(\n\t\tstruct btrfs_fs_info *fs_info, struct page *page)\n{\n\tstruct extent_buffer *exists;\n\n\t \n\tif (fs_info->nodesize < PAGE_SIZE)\n\t\treturn NULL;\n\n\t \n\tif (!PagePrivate(page))\n\t\treturn NULL;\n\n\t \n\texists = (struct extent_buffer *)page->private;\n\tif (atomic_inc_not_zero(&exists->refs))\n\t\treturn exists;\n\n\tWARN_ON(PageDirty(page));\n\tdetach_page_private(page);\n\treturn NULL;\n}\n\nstatic int check_eb_alignment(struct btrfs_fs_info *fs_info, u64 start)\n{\n\tif (!IS_ALIGNED(start, fs_info->sectorsize)) {\n\t\tbtrfs_err(fs_info, \"bad tree block start %llu\", start);\n\t\treturn -EINVAL;\n\t}\n\n\tif (fs_info->nodesize < PAGE_SIZE &&\n\t    offset_in_page(start) + fs_info->nodesize > PAGE_SIZE) {\n\t\tbtrfs_err(fs_info,\n\t\t\"tree block crosses page boundary, start %llu nodesize %u\",\n\t\t\t  start, fs_info->nodesize);\n\t\treturn -EINVAL;\n\t}\n\tif (fs_info->nodesize >= PAGE_SIZE &&\n\t    !PAGE_ALIGNED(start)) {\n\t\tbtrfs_err(fs_info,\n\t\t\"tree block is not page aligned, start %llu nodesize %u\",\n\t\t\t  start, fs_info->nodesize);\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nstruct extent_buffer *alloc_extent_buffer(struct btrfs_fs_info *fs_info,\n\t\t\t\t\t  u64 start, u64 owner_root, int level)\n{\n\tunsigned long len = fs_info->nodesize;\n\tint num_pages;\n\tint i;\n\tunsigned long index = start >> PAGE_SHIFT;\n\tstruct extent_buffer *eb;\n\tstruct extent_buffer *exists = NULL;\n\tstruct page *p;\n\tstruct address_space *mapping = fs_info->btree_inode->i_mapping;\n\tstruct btrfs_subpage *prealloc = NULL;\n\tu64 lockdep_owner = owner_root;\n\tint uptodate = 1;\n\tint ret;\n\n\tif (check_eb_alignment(fs_info, start))\n\t\treturn ERR_PTR(-EINVAL);\n\n#if BITS_PER_LONG == 32\n\tif (start >= MAX_LFS_FILESIZE) {\n\t\tbtrfs_err_rl(fs_info,\n\t\t\"extent buffer %llu is beyond 32bit page cache limit\", start);\n\t\tbtrfs_err_32bit_limit(fs_info);\n\t\treturn ERR_PTR(-EOVERFLOW);\n\t}\n\tif (start >= BTRFS_32BIT_EARLY_WARN_THRESHOLD)\n\t\tbtrfs_warn_32bit_limit(fs_info);\n#endif\n\n\teb = find_extent_buffer(fs_info, start);\n\tif (eb)\n\t\treturn eb;\n\n\teb = __alloc_extent_buffer(fs_info, start, len);\n\tif (!eb)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\t \n\tif (lockdep_owner == BTRFS_TREE_RELOC_OBJECTID)\n\t\tlockdep_owner = BTRFS_FS_TREE_OBJECTID;\n\n\tbtrfs_set_buffer_lockdep_class(lockdep_owner, eb, level);\n\n\tnum_pages = num_extent_pages(eb);\n\n\t \n\tif (fs_info->nodesize < PAGE_SIZE) {\n\t\tprealloc = btrfs_alloc_subpage(fs_info, BTRFS_SUBPAGE_METADATA);\n\t\tif (IS_ERR(prealloc)) {\n\t\t\texists = ERR_CAST(prealloc);\n\t\t\tgoto free_eb;\n\t\t}\n\t}\n\n\tfor (i = 0; i < num_pages; i++, index++) {\n\t\tp = find_or_create_page(mapping, index, GFP_NOFS|__GFP_NOFAIL);\n\t\tif (!p) {\n\t\t\texists = ERR_PTR(-ENOMEM);\n\t\t\tbtrfs_free_subpage(prealloc);\n\t\t\tgoto free_eb;\n\t\t}\n\n\t\tspin_lock(&mapping->private_lock);\n\t\texists = grab_extent_buffer(fs_info, p);\n\t\tif (exists) {\n\t\t\tspin_unlock(&mapping->private_lock);\n\t\t\tunlock_page(p);\n\t\t\tput_page(p);\n\t\t\tmark_extent_buffer_accessed(exists, p);\n\t\t\tbtrfs_free_subpage(prealloc);\n\t\t\tgoto free_eb;\n\t\t}\n\t\t \n\t\tret = attach_extent_buffer_page(eb, p, prealloc);\n\t\tASSERT(!ret);\n\t\t \n\t\tbtrfs_page_inc_eb_refs(fs_info, p);\n\t\tspin_unlock(&mapping->private_lock);\n\n\t\tWARN_ON(btrfs_page_test_dirty(fs_info, p, eb->start, eb->len));\n\t\teb->pages[i] = p;\n\t\tif (!btrfs_page_test_uptodate(fs_info, p, eb->start, eb->len))\n\t\t\tuptodate = 0;\n\n\t\t \n\t}\n\tif (uptodate)\n\t\tset_bit(EXTENT_BUFFER_UPTODATE, &eb->bflags);\nagain:\n\tret = radix_tree_preload(GFP_NOFS);\n\tif (ret) {\n\t\texists = ERR_PTR(ret);\n\t\tgoto free_eb;\n\t}\n\n\tspin_lock(&fs_info->buffer_lock);\n\tret = radix_tree_insert(&fs_info->buffer_radix,\n\t\t\t\tstart >> fs_info->sectorsize_bits, eb);\n\tspin_unlock(&fs_info->buffer_lock);\n\tradix_tree_preload_end();\n\tif (ret == -EEXIST) {\n\t\texists = find_extent_buffer(fs_info, start);\n\t\tif (exists)\n\t\t\tgoto free_eb;\n\t\telse\n\t\t\tgoto again;\n\t}\n\t \n\tcheck_buffer_tree_ref(eb);\n\tset_bit(EXTENT_BUFFER_IN_TREE, &eb->bflags);\n\n\t \n\tfor (i = 0; i < num_pages; i++)\n\t\tunlock_page(eb->pages[i]);\n\treturn eb;\n\nfree_eb:\n\tWARN_ON(!atomic_dec_and_test(&eb->refs));\n\tfor (i = 0; i < num_pages; i++) {\n\t\tif (eb->pages[i])\n\t\t\tunlock_page(eb->pages[i]);\n\t}\n\n\tbtrfs_release_extent_buffer(eb);\n\treturn exists;\n}\n\nstatic inline void btrfs_release_extent_buffer_rcu(struct rcu_head *head)\n{\n\tstruct extent_buffer *eb =\n\t\t\tcontainer_of(head, struct extent_buffer, rcu_head);\n\n\t__free_extent_buffer(eb);\n}\n\nstatic int release_extent_buffer(struct extent_buffer *eb)\n\t__releases(&eb->refs_lock)\n{\n\tlockdep_assert_held(&eb->refs_lock);\n\n\tWARN_ON(atomic_read(&eb->refs) == 0);\n\tif (atomic_dec_and_test(&eb->refs)) {\n\t\tif (test_and_clear_bit(EXTENT_BUFFER_IN_TREE, &eb->bflags)) {\n\t\t\tstruct btrfs_fs_info *fs_info = eb->fs_info;\n\n\t\t\tspin_unlock(&eb->refs_lock);\n\n\t\t\tspin_lock(&fs_info->buffer_lock);\n\t\t\tradix_tree_delete(&fs_info->buffer_radix,\n\t\t\t\t\t  eb->start >> fs_info->sectorsize_bits);\n\t\t\tspin_unlock(&fs_info->buffer_lock);\n\t\t} else {\n\t\t\tspin_unlock(&eb->refs_lock);\n\t\t}\n\n\t\tbtrfs_leak_debug_del_eb(eb);\n\t\t \n\t\tbtrfs_release_extent_buffer_pages(eb);\n#ifdef CONFIG_BTRFS_FS_RUN_SANITY_TESTS\n\t\tif (unlikely(test_bit(EXTENT_BUFFER_UNMAPPED, &eb->bflags))) {\n\t\t\t__free_extent_buffer(eb);\n\t\t\treturn 1;\n\t\t}\n#endif\n\t\tcall_rcu(&eb->rcu_head, btrfs_release_extent_buffer_rcu);\n\t\treturn 1;\n\t}\n\tspin_unlock(&eb->refs_lock);\n\n\treturn 0;\n}\n\nvoid free_extent_buffer(struct extent_buffer *eb)\n{\n\tint refs;\n\tif (!eb)\n\t\treturn;\n\n\trefs = atomic_read(&eb->refs);\n\twhile (1) {\n\t\tif ((!test_bit(EXTENT_BUFFER_UNMAPPED, &eb->bflags) && refs <= 3)\n\t\t    || (test_bit(EXTENT_BUFFER_UNMAPPED, &eb->bflags) &&\n\t\t\trefs == 1))\n\t\t\tbreak;\n\t\tif (atomic_try_cmpxchg(&eb->refs, &refs, refs - 1))\n\t\t\treturn;\n\t}\n\n\tspin_lock(&eb->refs_lock);\n\tif (atomic_read(&eb->refs) == 2 &&\n\t    test_bit(EXTENT_BUFFER_STALE, &eb->bflags) &&\n\t    !extent_buffer_under_io(eb) &&\n\t    test_and_clear_bit(EXTENT_BUFFER_TREE_REF, &eb->bflags))\n\t\tatomic_dec(&eb->refs);\n\n\t \n\trelease_extent_buffer(eb);\n}\n\nvoid free_extent_buffer_stale(struct extent_buffer *eb)\n{\n\tif (!eb)\n\t\treturn;\n\n\tspin_lock(&eb->refs_lock);\n\tset_bit(EXTENT_BUFFER_STALE, &eb->bflags);\n\n\tif (atomic_read(&eb->refs) == 2 && !extent_buffer_under_io(eb) &&\n\t    test_and_clear_bit(EXTENT_BUFFER_TREE_REF, &eb->bflags))\n\t\tatomic_dec(&eb->refs);\n\trelease_extent_buffer(eb);\n}\n\nstatic void btree_clear_page_dirty(struct page *page)\n{\n\tASSERT(PageDirty(page));\n\tASSERT(PageLocked(page));\n\tclear_page_dirty_for_io(page);\n\txa_lock_irq(&page->mapping->i_pages);\n\tif (!PageDirty(page))\n\t\t__xa_clear_mark(&page->mapping->i_pages,\n\t\t\t\tpage_index(page), PAGECACHE_TAG_DIRTY);\n\txa_unlock_irq(&page->mapping->i_pages);\n}\n\nstatic void clear_subpage_extent_buffer_dirty(const struct extent_buffer *eb)\n{\n\tstruct btrfs_fs_info *fs_info = eb->fs_info;\n\tstruct page *page = eb->pages[0];\n\tbool last;\n\n\t \n\tlock_page(page);\n\tlast = btrfs_subpage_clear_and_test_dirty(fs_info, page, eb->start,\n\t\t\t\t\t\t  eb->len);\n\tif (last)\n\t\tbtree_clear_page_dirty(page);\n\tunlock_page(page);\n\tWARN_ON(atomic_read(&eb->refs) == 0);\n}\n\nvoid btrfs_clear_buffer_dirty(struct btrfs_trans_handle *trans,\n\t\t\t      struct extent_buffer *eb)\n{\n\tstruct btrfs_fs_info *fs_info = eb->fs_info;\n\tint i;\n\tint num_pages;\n\tstruct page *page;\n\n\tbtrfs_assert_tree_write_locked(eb);\n\n\tif (trans && btrfs_header_generation(eb) != trans->transid)\n\t\treturn;\n\n\tif (!test_and_clear_bit(EXTENT_BUFFER_DIRTY, &eb->bflags))\n\t\treturn;\n\n\tpercpu_counter_add_batch(&fs_info->dirty_metadata_bytes, -eb->len,\n\t\t\t\t fs_info->dirty_metadata_batch);\n\n\tif (eb->fs_info->nodesize < PAGE_SIZE)\n\t\treturn clear_subpage_extent_buffer_dirty(eb);\n\n\tnum_pages = num_extent_pages(eb);\n\n\tfor (i = 0; i < num_pages; i++) {\n\t\tpage = eb->pages[i];\n\t\tif (!PageDirty(page))\n\t\t\tcontinue;\n\t\tlock_page(page);\n\t\tbtree_clear_page_dirty(page);\n\t\tunlock_page(page);\n\t}\n\tWARN_ON(atomic_read(&eb->refs) == 0);\n}\n\nvoid set_extent_buffer_dirty(struct extent_buffer *eb)\n{\n\tint i;\n\tint num_pages;\n\tbool was_dirty;\n\n\tcheck_buffer_tree_ref(eb);\n\n\twas_dirty = test_and_set_bit(EXTENT_BUFFER_DIRTY, &eb->bflags);\n\n\tnum_pages = num_extent_pages(eb);\n\tWARN_ON(atomic_read(&eb->refs) == 0);\n\tWARN_ON(!test_bit(EXTENT_BUFFER_TREE_REF, &eb->bflags));\n\n\tif (!was_dirty) {\n\t\tbool subpage = eb->fs_info->nodesize < PAGE_SIZE;\n\n\t\t \n\t\tif (subpage)\n\t\t\tlock_page(eb->pages[0]);\n\t\tfor (i = 0; i < num_pages; i++)\n\t\t\tbtrfs_page_set_dirty(eb->fs_info, eb->pages[i],\n\t\t\t\t\t     eb->start, eb->len);\n\t\tif (subpage)\n\t\t\tunlock_page(eb->pages[0]);\n\t\tpercpu_counter_add_batch(&eb->fs_info->dirty_metadata_bytes,\n\t\t\t\t\t eb->len,\n\t\t\t\t\t eb->fs_info->dirty_metadata_batch);\n\t}\n#ifdef CONFIG_BTRFS_DEBUG\n\tfor (i = 0; i < num_pages; i++)\n\t\tASSERT(PageDirty(eb->pages[i]));\n#endif\n}\n\nvoid clear_extent_buffer_uptodate(struct extent_buffer *eb)\n{\n\tstruct btrfs_fs_info *fs_info = eb->fs_info;\n\tstruct page *page;\n\tint num_pages;\n\tint i;\n\n\tclear_bit(EXTENT_BUFFER_UPTODATE, &eb->bflags);\n\tnum_pages = num_extent_pages(eb);\n\tfor (i = 0; i < num_pages; i++) {\n\t\tpage = eb->pages[i];\n\t\tif (!page)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (fs_info->nodesize >= PAGE_SIZE)\n\t\t\tClearPageUptodate(page);\n\t\telse\n\t\t\tbtrfs_subpage_clear_uptodate(fs_info, page, eb->start,\n\t\t\t\t\t\t     eb->len);\n\t}\n}\n\nvoid set_extent_buffer_uptodate(struct extent_buffer *eb)\n{\n\tstruct btrfs_fs_info *fs_info = eb->fs_info;\n\tstruct page *page;\n\tint num_pages;\n\tint i;\n\n\tset_bit(EXTENT_BUFFER_UPTODATE, &eb->bflags);\n\tnum_pages = num_extent_pages(eb);\n\tfor (i = 0; i < num_pages; i++) {\n\t\tpage = eb->pages[i];\n\n\t\t \n\t\tif (fs_info->nodesize >= PAGE_SIZE)\n\t\t\tSetPageUptodate(page);\n\t\telse\n\t\t\tbtrfs_subpage_set_uptodate(fs_info, page, eb->start,\n\t\t\t\t\t\t   eb->len);\n\t}\n}\n\nstatic void extent_buffer_read_end_io(struct btrfs_bio *bbio)\n{\n\tstruct extent_buffer *eb = bbio->private;\n\tstruct btrfs_fs_info *fs_info = eb->fs_info;\n\tbool uptodate = !bbio->bio.bi_status;\n\tstruct bvec_iter_all iter_all;\n\tstruct bio_vec *bvec;\n\tu32 bio_offset = 0;\n\n\teb->read_mirror = bbio->mirror_num;\n\n\tif (uptodate &&\n\t    btrfs_validate_extent_buffer(eb, &bbio->parent_check) < 0)\n\t\tuptodate = false;\n\n\tif (uptodate) {\n\t\tset_extent_buffer_uptodate(eb);\n\t} else {\n\t\tclear_extent_buffer_uptodate(eb);\n\t\tset_bit(EXTENT_BUFFER_READ_ERR, &eb->bflags);\n\t}\n\n\tbio_for_each_segment_all(bvec, &bbio->bio, iter_all) {\n\t\tu64 start = eb->start + bio_offset;\n\t\tstruct page *page = bvec->bv_page;\n\t\tu32 len = bvec->bv_len;\n\n\t\tif (uptodate)\n\t\t\tbtrfs_page_set_uptodate(fs_info, page, start, len);\n\t\telse\n\t\t\tbtrfs_page_clear_uptodate(fs_info, page, start, len);\n\n\t\tbio_offset += len;\n\t}\n\n\tclear_bit(EXTENT_BUFFER_READING, &eb->bflags);\n\tsmp_mb__after_atomic();\n\twake_up_bit(&eb->bflags, EXTENT_BUFFER_READING);\n\tfree_extent_buffer(eb);\n\n\tbio_put(&bbio->bio);\n}\n\nint read_extent_buffer_pages(struct extent_buffer *eb, int wait, int mirror_num,\n\t\t\t     struct btrfs_tree_parent_check *check)\n{\n\tint num_pages = num_extent_pages(eb), i;\n\tstruct btrfs_bio *bbio;\n\n\tif (test_bit(EXTENT_BUFFER_UPTODATE, &eb->bflags))\n\t\treturn 0;\n\n\t \n\tif (unlikely(test_bit(EXTENT_BUFFER_WRITE_ERR, &eb->bflags)))\n\t\treturn -EIO;\n\n\t \n\tif (test_and_set_bit(EXTENT_BUFFER_READING, &eb->bflags))\n\t\tgoto done;\n\n\tclear_bit(EXTENT_BUFFER_READ_ERR, &eb->bflags);\n\teb->read_mirror = 0;\n\tcheck_buffer_tree_ref(eb);\n\tatomic_inc(&eb->refs);\n\n\tbbio = btrfs_bio_alloc(INLINE_EXTENT_BUFFER_PAGES,\n\t\t\t       REQ_OP_READ | REQ_META, eb->fs_info,\n\t\t\t       extent_buffer_read_end_io, eb);\n\tbbio->bio.bi_iter.bi_sector = eb->start >> SECTOR_SHIFT;\n\tbbio->inode = BTRFS_I(eb->fs_info->btree_inode);\n\tbbio->file_offset = eb->start;\n\tmemcpy(&bbio->parent_check, check, sizeof(*check));\n\tif (eb->fs_info->nodesize < PAGE_SIZE) {\n\t\t__bio_add_page(&bbio->bio, eb->pages[0], eb->len,\n\t\t\t       eb->start - page_offset(eb->pages[0]));\n\t} else {\n\t\tfor (i = 0; i < num_pages; i++)\n\t\t\t__bio_add_page(&bbio->bio, eb->pages[i], PAGE_SIZE, 0);\n\t}\n\tbtrfs_submit_bio(bbio, mirror_num);\n\ndone:\n\tif (wait == WAIT_COMPLETE) {\n\t\twait_on_bit_io(&eb->bflags, EXTENT_BUFFER_READING, TASK_UNINTERRUPTIBLE);\n\t\tif (!test_bit(EXTENT_BUFFER_UPTODATE, &eb->bflags))\n\t\t\treturn -EIO;\n\t}\n\n\treturn 0;\n}\n\nstatic bool report_eb_range(const struct extent_buffer *eb, unsigned long start,\n\t\t\t    unsigned long len)\n{\n\tbtrfs_warn(eb->fs_info,\n\t\t\"access to eb bytenr %llu len %lu out of range start %lu len %lu\",\n\t\teb->start, eb->len, start, len);\n\tWARN_ON(IS_ENABLED(CONFIG_BTRFS_DEBUG));\n\n\treturn true;\n}\n\n \nstatic inline int check_eb_range(const struct extent_buffer *eb,\n\t\t\t\t unsigned long start, unsigned long len)\n{\n\tunsigned long offset;\n\n\t \n\tif (unlikely(check_add_overflow(start, len, &offset) || offset > eb->len))\n\t\treturn report_eb_range(eb, start, len);\n\n\treturn false;\n}\n\nvoid read_extent_buffer(const struct extent_buffer *eb, void *dstv,\n\t\t\tunsigned long start, unsigned long len)\n{\n\tsize_t cur;\n\tsize_t offset;\n\tstruct page *page;\n\tchar *kaddr;\n\tchar *dst = (char *)dstv;\n\tunsigned long i = get_eb_page_index(start);\n\n\tif (check_eb_range(eb, start, len)) {\n\t\t \n\t\tmemset(dstv, 0, len);\n\t\treturn;\n\t}\n\n\toffset = get_eb_offset_in_page(eb, start);\n\n\twhile (len > 0) {\n\t\tpage = eb->pages[i];\n\n\t\tcur = min(len, (PAGE_SIZE - offset));\n\t\tkaddr = page_address(page);\n\t\tmemcpy(dst, kaddr + offset, cur);\n\n\t\tdst += cur;\n\t\tlen -= cur;\n\t\toffset = 0;\n\t\ti++;\n\t}\n}\n\nint read_extent_buffer_to_user_nofault(const struct extent_buffer *eb,\n\t\t\t\t       void __user *dstv,\n\t\t\t\t       unsigned long start, unsigned long len)\n{\n\tsize_t cur;\n\tsize_t offset;\n\tstruct page *page;\n\tchar *kaddr;\n\tchar __user *dst = (char __user *)dstv;\n\tunsigned long i = get_eb_page_index(start);\n\tint ret = 0;\n\n\tWARN_ON(start > eb->len);\n\tWARN_ON(start + len > eb->start + eb->len);\n\n\toffset = get_eb_offset_in_page(eb, start);\n\n\twhile (len > 0) {\n\t\tpage = eb->pages[i];\n\n\t\tcur = min(len, (PAGE_SIZE - offset));\n\t\tkaddr = page_address(page);\n\t\tif (copy_to_user_nofault(dst, kaddr + offset, cur)) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tdst += cur;\n\t\tlen -= cur;\n\t\toffset = 0;\n\t\ti++;\n\t}\n\n\treturn ret;\n}\n\nint memcmp_extent_buffer(const struct extent_buffer *eb, const void *ptrv,\n\t\t\t unsigned long start, unsigned long len)\n{\n\tsize_t cur;\n\tsize_t offset;\n\tstruct page *page;\n\tchar *kaddr;\n\tchar *ptr = (char *)ptrv;\n\tunsigned long i = get_eb_page_index(start);\n\tint ret = 0;\n\n\tif (check_eb_range(eb, start, len))\n\t\treturn -EINVAL;\n\n\toffset = get_eb_offset_in_page(eb, start);\n\n\twhile (len > 0) {\n\t\tpage = eb->pages[i];\n\n\t\tcur = min(len, (PAGE_SIZE - offset));\n\n\t\tkaddr = page_address(page);\n\t\tret = memcmp(ptr, kaddr + offset, cur);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tptr += cur;\n\t\tlen -= cur;\n\t\toffset = 0;\n\t\ti++;\n\t}\n\treturn ret;\n}\n\n \nstatic void assert_eb_page_uptodate(const struct extent_buffer *eb,\n\t\t\t\t    struct page *page)\n{\n\tstruct btrfs_fs_info *fs_info = eb->fs_info;\n\n\t \n\tif (test_bit(EXTENT_BUFFER_WRITE_ERR, &eb->bflags))\n\t\treturn;\n\n\tif (fs_info->nodesize < PAGE_SIZE) {\n\t\tif (WARN_ON(!btrfs_subpage_test_uptodate(fs_info, page,\n\t\t\t\t\t\t\t eb->start, eb->len)))\n\t\t\tbtrfs_subpage_dump_bitmap(fs_info, page, eb->start, eb->len);\n\t} else {\n\t\tWARN_ON(!PageUptodate(page));\n\t}\n}\n\nstatic void __write_extent_buffer(const struct extent_buffer *eb,\n\t\t\t\t  const void *srcv, unsigned long start,\n\t\t\t\t  unsigned long len, bool use_memmove)\n{\n\tsize_t cur;\n\tsize_t offset;\n\tstruct page *page;\n\tchar *kaddr;\n\tchar *src = (char *)srcv;\n\tunsigned long i = get_eb_page_index(start);\n\t \n\tconst bool check_uptodate = !test_bit(EXTENT_BUFFER_UNMAPPED, &eb->bflags);\n\n\tWARN_ON(test_bit(EXTENT_BUFFER_NO_CHECK, &eb->bflags));\n\n\tif (check_eb_range(eb, start, len))\n\t\treturn;\n\n\toffset = get_eb_offset_in_page(eb, start);\n\n\twhile (len > 0) {\n\t\tpage = eb->pages[i];\n\t\tif (check_uptodate)\n\t\t\tassert_eb_page_uptodate(eb, page);\n\n\t\tcur = min(len, PAGE_SIZE - offset);\n\t\tkaddr = page_address(page);\n\t\tif (use_memmove)\n\t\t\tmemmove(kaddr + offset, src, cur);\n\t\telse\n\t\t\tmemcpy(kaddr + offset, src, cur);\n\n\t\tsrc += cur;\n\t\tlen -= cur;\n\t\toffset = 0;\n\t\ti++;\n\t}\n}\n\nvoid write_extent_buffer(const struct extent_buffer *eb, const void *srcv,\n\t\t\t unsigned long start, unsigned long len)\n{\n\treturn __write_extent_buffer(eb, srcv, start, len, false);\n}\n\nstatic void memset_extent_buffer(const struct extent_buffer *eb, int c,\n\t\t\t\t unsigned long start, unsigned long len)\n{\n\tunsigned long cur = start;\n\n\twhile (cur < start + len) {\n\t\tunsigned long index = get_eb_page_index(cur);\n\t\tunsigned int offset = get_eb_offset_in_page(eb, cur);\n\t\tunsigned int cur_len = min(start + len - cur, PAGE_SIZE - offset);\n\t\tstruct page *page = eb->pages[index];\n\n\t\tassert_eb_page_uptodate(eb, page);\n\t\tmemset(page_address(page) + offset, c, cur_len);\n\n\t\tcur += cur_len;\n\t}\n}\n\nvoid memzero_extent_buffer(const struct extent_buffer *eb, unsigned long start,\n\t\t\t   unsigned long len)\n{\n\tif (check_eb_range(eb, start, len))\n\t\treturn;\n\treturn memset_extent_buffer(eb, 0, start, len);\n}\n\nvoid copy_extent_buffer_full(const struct extent_buffer *dst,\n\t\t\t     const struct extent_buffer *src)\n{\n\tunsigned long cur = 0;\n\n\tASSERT(dst->len == src->len);\n\n\twhile (cur < src->len) {\n\t\tunsigned long index = get_eb_page_index(cur);\n\t\tunsigned long offset = get_eb_offset_in_page(src, cur);\n\t\tunsigned long cur_len = min(src->len, PAGE_SIZE - offset);\n\t\tvoid *addr = page_address(src->pages[index]) + offset;\n\n\t\twrite_extent_buffer(dst, addr, cur, cur_len);\n\n\t\tcur += cur_len;\n\t}\n}\n\nvoid copy_extent_buffer(const struct extent_buffer *dst,\n\t\t\tconst struct extent_buffer *src,\n\t\t\tunsigned long dst_offset, unsigned long src_offset,\n\t\t\tunsigned long len)\n{\n\tu64 dst_len = dst->len;\n\tsize_t cur;\n\tsize_t offset;\n\tstruct page *page;\n\tchar *kaddr;\n\tunsigned long i = get_eb_page_index(dst_offset);\n\n\tif (check_eb_range(dst, dst_offset, len) ||\n\t    check_eb_range(src, src_offset, len))\n\t\treturn;\n\n\tWARN_ON(src->len != dst_len);\n\n\toffset = get_eb_offset_in_page(dst, dst_offset);\n\n\twhile (len > 0) {\n\t\tpage = dst->pages[i];\n\t\tassert_eb_page_uptodate(dst, page);\n\n\t\tcur = min(len, (unsigned long)(PAGE_SIZE - offset));\n\n\t\tkaddr = page_address(page);\n\t\tread_extent_buffer(src, kaddr + offset, src_offset, cur);\n\n\t\tsrc_offset += cur;\n\t\tlen -= cur;\n\t\toffset = 0;\n\t\ti++;\n\t}\n}\n\n \nstatic inline void eb_bitmap_offset(const struct extent_buffer *eb,\n\t\t\t\t    unsigned long start, unsigned long nr,\n\t\t\t\t    unsigned long *page_index,\n\t\t\t\t    size_t *page_offset)\n{\n\tsize_t byte_offset = BIT_BYTE(nr);\n\tsize_t offset;\n\n\t \n\toffset = start + offset_in_page(eb->start) + byte_offset;\n\n\t*page_index = offset >> PAGE_SHIFT;\n\t*page_offset = offset_in_page(offset);\n}\n\n \nint extent_buffer_test_bit(const struct extent_buffer *eb, unsigned long start,\n\t\t\t   unsigned long nr)\n{\n\tu8 *kaddr;\n\tstruct page *page;\n\tunsigned long i;\n\tsize_t offset;\n\n\teb_bitmap_offset(eb, start, nr, &i, &offset);\n\tpage = eb->pages[i];\n\tassert_eb_page_uptodate(eb, page);\n\tkaddr = page_address(page);\n\treturn 1U & (kaddr[offset] >> (nr & (BITS_PER_BYTE - 1)));\n}\n\nstatic u8 *extent_buffer_get_byte(const struct extent_buffer *eb, unsigned long bytenr)\n{\n\tunsigned long index = get_eb_page_index(bytenr);\n\n\tif (check_eb_range(eb, bytenr, 1))\n\t\treturn NULL;\n\treturn page_address(eb->pages[index]) + get_eb_offset_in_page(eb, bytenr);\n}\n\n \nvoid extent_buffer_bitmap_set(const struct extent_buffer *eb, unsigned long start,\n\t\t\t      unsigned long pos, unsigned long len)\n{\n\tunsigned int first_byte = start + BIT_BYTE(pos);\n\tunsigned int last_byte = start + BIT_BYTE(pos + len - 1);\n\tconst bool same_byte = (first_byte == last_byte);\n\tu8 mask = BITMAP_FIRST_BYTE_MASK(pos);\n\tu8 *kaddr;\n\n\tif (same_byte)\n\t\tmask &= BITMAP_LAST_BYTE_MASK(pos + len);\n\n\t \n\tkaddr = extent_buffer_get_byte(eb, first_byte);\n\t*kaddr |= mask;\n\tif (same_byte)\n\t\treturn;\n\n\t \n\tASSERT(first_byte + 1 <= last_byte);\n\tmemset_extent_buffer(eb, 0xff, first_byte + 1, last_byte - first_byte - 1);\n\n\t \n\tkaddr = extent_buffer_get_byte(eb, last_byte);\n\t*kaddr |= BITMAP_LAST_BYTE_MASK(pos + len);\n}\n\n\n \nvoid extent_buffer_bitmap_clear(const struct extent_buffer *eb,\n\t\t\t\tunsigned long start, unsigned long pos,\n\t\t\t\tunsigned long len)\n{\n\tunsigned int first_byte = start + BIT_BYTE(pos);\n\tunsigned int last_byte = start + BIT_BYTE(pos + len - 1);\n\tconst bool same_byte = (first_byte == last_byte);\n\tu8 mask = BITMAP_FIRST_BYTE_MASK(pos);\n\tu8 *kaddr;\n\n\tif (same_byte)\n\t\tmask &= BITMAP_LAST_BYTE_MASK(pos + len);\n\n\t \n\tkaddr = extent_buffer_get_byte(eb, first_byte);\n\t*kaddr &= ~mask;\n\tif (same_byte)\n\t\treturn;\n\n\t \n\tASSERT(first_byte + 1 <= last_byte);\n\tmemset_extent_buffer(eb, 0, first_byte + 1, last_byte - first_byte - 1);\n\n\t \n\tkaddr = extent_buffer_get_byte(eb, last_byte);\n\t*kaddr &= ~BITMAP_LAST_BYTE_MASK(pos + len);\n}\n\nstatic inline bool areas_overlap(unsigned long src, unsigned long dst, unsigned long len)\n{\n\tunsigned long distance = (src > dst) ? src - dst : dst - src;\n\treturn distance < len;\n}\n\nvoid memcpy_extent_buffer(const struct extent_buffer *dst,\n\t\t\t  unsigned long dst_offset, unsigned long src_offset,\n\t\t\t  unsigned long len)\n{\n\tunsigned long cur_off = 0;\n\n\tif (check_eb_range(dst, dst_offset, len) ||\n\t    check_eb_range(dst, src_offset, len))\n\t\treturn;\n\n\twhile (cur_off < len) {\n\t\tunsigned long cur_src = cur_off + src_offset;\n\t\tunsigned long pg_index = get_eb_page_index(cur_src);\n\t\tunsigned long pg_off = get_eb_offset_in_page(dst, cur_src);\n\t\tunsigned long cur_len = min(src_offset + len - cur_src,\n\t\t\t\t\t    PAGE_SIZE - pg_off);\n\t\tvoid *src_addr = page_address(dst->pages[pg_index]) + pg_off;\n\t\tconst bool use_memmove = areas_overlap(src_offset + cur_off,\n\t\t\t\t\t\t       dst_offset + cur_off, cur_len);\n\n\t\t__write_extent_buffer(dst, src_addr, dst_offset + cur_off, cur_len,\n\t\t\t\t      use_memmove);\n\t\tcur_off += cur_len;\n\t}\n}\n\nvoid memmove_extent_buffer(const struct extent_buffer *dst,\n\t\t\t   unsigned long dst_offset, unsigned long src_offset,\n\t\t\t   unsigned long len)\n{\n\tunsigned long dst_end = dst_offset + len - 1;\n\tunsigned long src_end = src_offset + len - 1;\n\n\tif (check_eb_range(dst, dst_offset, len) ||\n\t    check_eb_range(dst, src_offset, len))\n\t\treturn;\n\n\tif (dst_offset < src_offset) {\n\t\tmemcpy_extent_buffer(dst, dst_offset, src_offset, len);\n\t\treturn;\n\t}\n\n\twhile (len > 0) {\n\t\tunsigned long src_i;\n\t\tsize_t cur;\n\t\tsize_t dst_off_in_page;\n\t\tsize_t src_off_in_page;\n\t\tvoid *src_addr;\n\t\tbool use_memmove;\n\n\t\tsrc_i = get_eb_page_index(src_end);\n\n\t\tdst_off_in_page = get_eb_offset_in_page(dst, dst_end);\n\t\tsrc_off_in_page = get_eb_offset_in_page(dst, src_end);\n\n\t\tcur = min_t(unsigned long, len, src_off_in_page + 1);\n\t\tcur = min(cur, dst_off_in_page + 1);\n\n\t\tsrc_addr = page_address(dst->pages[src_i]) + src_off_in_page -\n\t\t\t\t\tcur + 1;\n\t\tuse_memmove = areas_overlap(src_end - cur + 1, dst_end - cur + 1,\n\t\t\t\t\t    cur);\n\n\t\t__write_extent_buffer(dst, src_addr, dst_end - cur + 1, cur,\n\t\t\t\t      use_memmove);\n\n\t\tdst_end -= cur;\n\t\tsrc_end -= cur;\n\t\tlen -= cur;\n\t}\n}\n\n#define GANG_LOOKUP_SIZE\t16\nstatic struct extent_buffer *get_next_extent_buffer(\n\t\tstruct btrfs_fs_info *fs_info, struct page *page, u64 bytenr)\n{\n\tstruct extent_buffer *gang[GANG_LOOKUP_SIZE];\n\tstruct extent_buffer *found = NULL;\n\tu64 page_start = page_offset(page);\n\tu64 cur = page_start;\n\n\tASSERT(in_range(bytenr, page_start, PAGE_SIZE));\n\tlockdep_assert_held(&fs_info->buffer_lock);\n\n\twhile (cur < page_start + PAGE_SIZE) {\n\t\tint ret;\n\t\tint i;\n\n\t\tret = radix_tree_gang_lookup(&fs_info->buffer_radix,\n\t\t\t\t(void **)gang, cur >> fs_info->sectorsize_bits,\n\t\t\t\tmin_t(unsigned int, GANG_LOOKUP_SIZE,\n\t\t\t\t      PAGE_SIZE / fs_info->nodesize));\n\t\tif (ret == 0)\n\t\t\tgoto out;\n\t\tfor (i = 0; i < ret; i++) {\n\t\t\t \n\t\t\tif (gang[i]->start >= page_start + PAGE_SIZE)\n\t\t\t\tgoto out;\n\t\t\t \n\t\t\tif (gang[i]->start >= bytenr) {\n\t\t\t\tfound = gang[i];\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\tcur = gang[ret - 1]->start + gang[ret - 1]->len;\n\t}\nout:\n\treturn found;\n}\n\nstatic int try_release_subpage_extent_buffer(struct page *page)\n{\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(page->mapping->host->i_sb);\n\tu64 cur = page_offset(page);\n\tconst u64 end = page_offset(page) + PAGE_SIZE;\n\tint ret;\n\n\twhile (cur < end) {\n\t\tstruct extent_buffer *eb = NULL;\n\n\t\t \n\t\tspin_lock(&fs_info->buffer_lock);\n\t\teb = get_next_extent_buffer(fs_info, page, cur);\n\t\tif (!eb) {\n\t\t\t \n\t\t\tspin_unlock(&fs_info->buffer_lock);\n\t\t\tbreak;\n\t\t}\n\t\tcur = eb->start + eb->len;\n\n\t\t \n\t\tspin_lock(&eb->refs_lock);\n\t\tif (atomic_read(&eb->refs) != 1 || extent_buffer_under_io(eb)) {\n\t\t\tspin_unlock(&eb->refs_lock);\n\t\t\tspin_unlock(&fs_info->buffer_lock);\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock(&fs_info->buffer_lock);\n\n\t\t \n\t\tif (!test_and_clear_bit(EXTENT_BUFFER_TREE_REF, &eb->bflags)) {\n\t\t\tspin_unlock(&eb->refs_lock);\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\trelease_extent_buffer(eb);\n\t}\n\t \n\tspin_lock(&page->mapping->private_lock);\n\tif (!PagePrivate(page))\n\t\tret = 1;\n\telse\n\t\tret = 0;\n\tspin_unlock(&page->mapping->private_lock);\n\treturn ret;\n\n}\n\nint try_release_extent_buffer(struct page *page)\n{\n\tstruct extent_buffer *eb;\n\n\tif (btrfs_sb(page->mapping->host->i_sb)->nodesize < PAGE_SIZE)\n\t\treturn try_release_subpage_extent_buffer(page);\n\n\t \n\tspin_lock(&page->mapping->private_lock);\n\tif (!PagePrivate(page)) {\n\t\tspin_unlock(&page->mapping->private_lock);\n\t\treturn 1;\n\t}\n\n\teb = (struct extent_buffer *)page->private;\n\tBUG_ON(!eb);\n\n\t \n\tspin_lock(&eb->refs_lock);\n\tif (atomic_read(&eb->refs) != 1 || extent_buffer_under_io(eb)) {\n\t\tspin_unlock(&eb->refs_lock);\n\t\tspin_unlock(&page->mapping->private_lock);\n\t\treturn 0;\n\t}\n\tspin_unlock(&page->mapping->private_lock);\n\n\t \n\tif (!test_and_clear_bit(EXTENT_BUFFER_TREE_REF, &eb->bflags)) {\n\t\tspin_unlock(&eb->refs_lock);\n\t\treturn 0;\n\t}\n\n\treturn release_extent_buffer(eb);\n}\n\n \nvoid btrfs_readahead_tree_block(struct btrfs_fs_info *fs_info,\n\t\t\t\tu64 bytenr, u64 owner_root, u64 gen, int level)\n{\n\tstruct btrfs_tree_parent_check check = {\n\t\t.has_first_key = 0,\n\t\t.level = level,\n\t\t.transid = gen\n\t};\n\tstruct extent_buffer *eb;\n\tint ret;\n\n\teb = btrfs_find_create_tree_block(fs_info, bytenr, owner_root, level);\n\tif (IS_ERR(eb))\n\t\treturn;\n\n\tif (btrfs_buffer_uptodate(eb, gen, 1)) {\n\t\tfree_extent_buffer(eb);\n\t\treturn;\n\t}\n\n\tret = read_extent_buffer_pages(eb, WAIT_NONE, 0, &check);\n\tif (ret < 0)\n\t\tfree_extent_buffer_stale(eb);\n\telse\n\t\tfree_extent_buffer(eb);\n}\n\n \nvoid btrfs_readahead_node_child(struct extent_buffer *node, int slot)\n{\n\tbtrfs_readahead_tree_block(node->fs_info,\n\t\t\t\t   btrfs_node_blockptr(node, slot),\n\t\t\t\t   btrfs_header_owner(node),\n\t\t\t\t   btrfs_node_ptr_generation(node, slot),\n\t\t\t\t   btrfs_header_level(node) - 1);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}