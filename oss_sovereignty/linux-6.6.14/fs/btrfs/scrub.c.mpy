{
  "module_name": "scrub.c",
  "hash_id": "b71c523acf8aaf2b37c7f9e22ee4a18af2278b71f65ae22bb786767eccb67a3b",
  "original_prompt": "Ingested from linux-6.6.14/fs/btrfs/scrub.c",
  "human_readable_source": "\n \n\n#include <linux/blkdev.h>\n#include <linux/ratelimit.h>\n#include <linux/sched/mm.h>\n#include <crypto/hash.h>\n#include \"ctree.h\"\n#include \"discard.h\"\n#include \"volumes.h\"\n#include \"disk-io.h\"\n#include \"ordered-data.h\"\n#include \"transaction.h\"\n#include \"backref.h\"\n#include \"extent_io.h\"\n#include \"dev-replace.h\"\n#include \"check-integrity.h\"\n#include \"raid56.h\"\n#include \"block-group.h\"\n#include \"zoned.h\"\n#include \"fs.h\"\n#include \"accessors.h\"\n#include \"file-item.h\"\n#include \"scrub.h\"\n\n \n\nstruct scrub_ctx;\n\n \n#define SCRUB_STRIPES_PER_GROUP\t\t8\n\n \n#define SCRUB_GROUPS_PER_SCTX\t\t16\n\n#define SCRUB_TOTAL_STRIPES\t\t(SCRUB_GROUPS_PER_SCTX * SCRUB_STRIPES_PER_GROUP)\n\n \n#define SCRUB_MAX_SECTORS_PER_BLOCK\t(BTRFS_MAX_METADATA_BLOCKSIZE / SZ_4K)\n\n \nstruct scrub_sector_verification {\n\tbool is_metadata;\n\n\tunion {\n\t\t \n\t\tu8 *csum;\n\n\t\t \n\t\tu64 generation;\n\t};\n};\n\nenum scrub_stripe_flags {\n\t \n\tSCRUB_STRIPE_FLAG_INITIALIZED,\n\n\t \n\tSCRUB_STRIPE_FLAG_REPAIR_DONE,\n\n\t \n\tSCRUB_STRIPE_FLAG_NO_REPORT,\n};\n\n#define SCRUB_STRIPE_PAGES\t\t(BTRFS_STRIPE_LEN / PAGE_SIZE)\n\n \nstruct scrub_stripe {\n\tstruct scrub_ctx *sctx;\n\tstruct btrfs_block_group *bg;\n\n\tstruct page *pages[SCRUB_STRIPE_PAGES];\n\tstruct scrub_sector_verification *sectors;\n\n\tstruct btrfs_device *dev;\n\tu64 logical;\n\tu64 physical;\n\n\tu16 mirror_num;\n\n\t \n\tu16 nr_sectors;\n\n\t \n\tu16 nr_data_extents;\n\tu16 nr_meta_extents;\n\n\tatomic_t pending_io;\n\twait_queue_head_t io_wait;\n\twait_queue_head_t repair_wait;\n\n\t \n\tunsigned long state;\n\n\t \n\tunsigned long extent_sector_bitmap;\n\n\t \n\tunsigned long init_error_bitmap;\n\tunsigned int init_nr_io_errors;\n\tunsigned int init_nr_csum_errors;\n\tunsigned int init_nr_meta_errors;\n\n\t \n\tunsigned long error_bitmap;\n\tunsigned long io_error_bitmap;\n\tunsigned long csum_error_bitmap;\n\tunsigned long meta_error_bitmap;\n\n\t \n\tunsigned long write_error_bitmap;\n\n\t \n\tspinlock_t write_error_lock;\n\n\t \n\tu8 *csums;\n\n\tstruct work_struct work;\n};\n\nstruct scrub_ctx {\n\tstruct scrub_stripe\tstripes[SCRUB_TOTAL_STRIPES];\n\tstruct scrub_stripe\t*raid56_data_stripes;\n\tstruct btrfs_fs_info\t*fs_info;\n\tstruct btrfs_path\textent_path;\n\tstruct btrfs_path\tcsum_path;\n\tint\t\t\tfirst_free;\n\tint\t\t\tcur_stripe;\n\tatomic_t\t\tcancel_req;\n\tint\t\t\treadonly;\n\tint\t\t\tsectors_per_bio;\n\n\t \n\tktime_t\t\t\tthrottle_deadline;\n\tu64\t\t\tthrottle_sent;\n\n\tint\t\t\tis_dev_replace;\n\tu64\t\t\twrite_pointer;\n\n\tstruct mutex            wr_lock;\n\tstruct btrfs_device     *wr_tgtdev;\n\n\t \n\tstruct btrfs_scrub_progress stat;\n\tspinlock_t\t\tstat_lock;\n\n\t \n\trefcount_t              refs;\n};\n\nstruct scrub_warning {\n\tstruct btrfs_path\t*path;\n\tu64\t\t\textent_item_size;\n\tconst char\t\t*errstr;\n\tu64\t\t\tphysical;\n\tu64\t\t\tlogical;\n\tstruct btrfs_device\t*dev;\n};\n\nstatic void release_scrub_stripe(struct scrub_stripe *stripe)\n{\n\tif (!stripe)\n\t\treturn;\n\n\tfor (int i = 0; i < SCRUB_STRIPE_PAGES; i++) {\n\t\tif (stripe->pages[i])\n\t\t\t__free_page(stripe->pages[i]);\n\t\tstripe->pages[i] = NULL;\n\t}\n\tkfree(stripe->sectors);\n\tkfree(stripe->csums);\n\tstripe->sectors = NULL;\n\tstripe->csums = NULL;\n\tstripe->sctx = NULL;\n\tstripe->state = 0;\n}\n\nstatic int init_scrub_stripe(struct btrfs_fs_info *fs_info,\n\t\t\t     struct scrub_stripe *stripe)\n{\n\tint ret;\n\n\tmemset(stripe, 0, sizeof(*stripe));\n\n\tstripe->nr_sectors = BTRFS_STRIPE_LEN >> fs_info->sectorsize_bits;\n\tstripe->state = 0;\n\n\tinit_waitqueue_head(&stripe->io_wait);\n\tinit_waitqueue_head(&stripe->repair_wait);\n\tatomic_set(&stripe->pending_io, 0);\n\tspin_lock_init(&stripe->write_error_lock);\n\n\tret = btrfs_alloc_page_array(SCRUB_STRIPE_PAGES, stripe->pages);\n\tif (ret < 0)\n\t\tgoto error;\n\n\tstripe->sectors = kcalloc(stripe->nr_sectors,\n\t\t\t\t  sizeof(struct scrub_sector_verification),\n\t\t\t\t  GFP_KERNEL);\n\tif (!stripe->sectors)\n\t\tgoto error;\n\n\tstripe->csums = kcalloc(BTRFS_STRIPE_LEN >> fs_info->sectorsize_bits,\n\t\t\t\tfs_info->csum_size, GFP_KERNEL);\n\tif (!stripe->csums)\n\t\tgoto error;\n\treturn 0;\nerror:\n\trelease_scrub_stripe(stripe);\n\treturn -ENOMEM;\n}\n\nstatic void wait_scrub_stripe_io(struct scrub_stripe *stripe)\n{\n\twait_event(stripe->io_wait, atomic_read(&stripe->pending_io) == 0);\n}\n\nstatic void scrub_put_ctx(struct scrub_ctx *sctx);\n\nstatic void __scrub_blocked_if_needed(struct btrfs_fs_info *fs_info)\n{\n\twhile (atomic_read(&fs_info->scrub_pause_req)) {\n\t\tmutex_unlock(&fs_info->scrub_lock);\n\t\twait_event(fs_info->scrub_pause_wait,\n\t\t   atomic_read(&fs_info->scrub_pause_req) == 0);\n\t\tmutex_lock(&fs_info->scrub_lock);\n\t}\n}\n\nstatic void scrub_pause_on(struct btrfs_fs_info *fs_info)\n{\n\tatomic_inc(&fs_info->scrubs_paused);\n\twake_up(&fs_info->scrub_pause_wait);\n}\n\nstatic void scrub_pause_off(struct btrfs_fs_info *fs_info)\n{\n\tmutex_lock(&fs_info->scrub_lock);\n\t__scrub_blocked_if_needed(fs_info);\n\tatomic_dec(&fs_info->scrubs_paused);\n\tmutex_unlock(&fs_info->scrub_lock);\n\n\twake_up(&fs_info->scrub_pause_wait);\n}\n\nstatic void scrub_blocked_if_needed(struct btrfs_fs_info *fs_info)\n{\n\tscrub_pause_on(fs_info);\n\tscrub_pause_off(fs_info);\n}\n\nstatic noinline_for_stack void scrub_free_ctx(struct scrub_ctx *sctx)\n{\n\tint i;\n\n\tif (!sctx)\n\t\treturn;\n\n\tfor (i = 0; i < SCRUB_TOTAL_STRIPES; i++)\n\t\trelease_scrub_stripe(&sctx->stripes[i]);\n\n\tkvfree(sctx);\n}\n\nstatic void scrub_put_ctx(struct scrub_ctx *sctx)\n{\n\tif (refcount_dec_and_test(&sctx->refs))\n\t\tscrub_free_ctx(sctx);\n}\n\nstatic noinline_for_stack struct scrub_ctx *scrub_setup_ctx(\n\t\tstruct btrfs_fs_info *fs_info, int is_dev_replace)\n{\n\tstruct scrub_ctx *sctx;\n\tint\t\ti;\n\n\t \n\tsctx = kvzalloc(sizeof(*sctx), GFP_KERNEL);\n\tif (!sctx)\n\t\tgoto nomem;\n\trefcount_set(&sctx->refs, 1);\n\tsctx->is_dev_replace = is_dev_replace;\n\tsctx->fs_info = fs_info;\n\tsctx->extent_path.search_commit_root = 1;\n\tsctx->extent_path.skip_locking = 1;\n\tsctx->csum_path.search_commit_root = 1;\n\tsctx->csum_path.skip_locking = 1;\n\tfor (i = 0; i < SCRUB_TOTAL_STRIPES; i++) {\n\t\tint ret;\n\n\t\tret = init_scrub_stripe(fs_info, &sctx->stripes[i]);\n\t\tif (ret < 0)\n\t\t\tgoto nomem;\n\t\tsctx->stripes[i].sctx = sctx;\n\t}\n\tsctx->first_free = 0;\n\tatomic_set(&sctx->cancel_req, 0);\n\n\tspin_lock_init(&sctx->stat_lock);\n\tsctx->throttle_deadline = 0;\n\n\tmutex_init(&sctx->wr_lock);\n\tif (is_dev_replace) {\n\t\tWARN_ON(!fs_info->dev_replace.tgtdev);\n\t\tsctx->wr_tgtdev = fs_info->dev_replace.tgtdev;\n\t}\n\n\treturn sctx;\n\nnomem:\n\tscrub_free_ctx(sctx);\n\treturn ERR_PTR(-ENOMEM);\n}\n\nstatic int scrub_print_warning_inode(u64 inum, u64 offset, u64 num_bytes,\n\t\t\t\t     u64 root, void *warn_ctx)\n{\n\tu32 nlink;\n\tint ret;\n\tint i;\n\tunsigned nofs_flag;\n\tstruct extent_buffer *eb;\n\tstruct btrfs_inode_item *inode_item;\n\tstruct scrub_warning *swarn = warn_ctx;\n\tstruct btrfs_fs_info *fs_info = swarn->dev->fs_info;\n\tstruct inode_fs_paths *ipath = NULL;\n\tstruct btrfs_root *local_root;\n\tstruct btrfs_key key;\n\n\tlocal_root = btrfs_get_fs_root(fs_info, root, true);\n\tif (IS_ERR(local_root)) {\n\t\tret = PTR_ERR(local_root);\n\t\tgoto err;\n\t}\n\n\t \n\tkey.objectid = inum;\n\tkey.type = BTRFS_INODE_ITEM_KEY;\n\tkey.offset = 0;\n\n\tret = btrfs_search_slot(NULL, local_root, &key, swarn->path, 0, 0);\n\tif (ret) {\n\t\tbtrfs_put_root(local_root);\n\t\tbtrfs_release_path(swarn->path);\n\t\tgoto err;\n\t}\n\n\teb = swarn->path->nodes[0];\n\tinode_item = btrfs_item_ptr(eb, swarn->path->slots[0],\n\t\t\t\t\tstruct btrfs_inode_item);\n\tnlink = btrfs_inode_nlink(eb, inode_item);\n\tbtrfs_release_path(swarn->path);\n\n\t \n\tnofs_flag = memalloc_nofs_save();\n\tipath = init_ipath(4096, local_root, swarn->path);\n\tmemalloc_nofs_restore(nofs_flag);\n\tif (IS_ERR(ipath)) {\n\t\tbtrfs_put_root(local_root);\n\t\tret = PTR_ERR(ipath);\n\t\tipath = NULL;\n\t\tgoto err;\n\t}\n\tret = paths_from_inode(inum, ipath);\n\n\tif (ret < 0)\n\t\tgoto err;\n\n\t \n\tfor (i = 0; i < ipath->fspath->elem_cnt; ++i)\n\t\tbtrfs_warn_in_rcu(fs_info,\n\"%s at logical %llu on dev %s, physical %llu, root %llu, inode %llu, offset %llu, length %u, links %u (path: %s)\",\n\t\t\t\t  swarn->errstr, swarn->logical,\n\t\t\t\t  btrfs_dev_name(swarn->dev),\n\t\t\t\t  swarn->physical,\n\t\t\t\t  root, inum, offset,\n\t\t\t\t  fs_info->sectorsize, nlink,\n\t\t\t\t  (char *)(unsigned long)ipath->fspath->val[i]);\n\n\tbtrfs_put_root(local_root);\n\tfree_ipath(ipath);\n\treturn 0;\n\nerr:\n\tbtrfs_warn_in_rcu(fs_info,\n\t\t\t  \"%s at logical %llu on dev %s, physical %llu, root %llu, inode %llu, offset %llu: path resolving failed with ret=%d\",\n\t\t\t  swarn->errstr, swarn->logical,\n\t\t\t  btrfs_dev_name(swarn->dev),\n\t\t\t  swarn->physical,\n\t\t\t  root, inum, offset, ret);\n\n\tfree_ipath(ipath);\n\treturn 0;\n}\n\nstatic void scrub_print_common_warning(const char *errstr, struct btrfs_device *dev,\n\t\t\t\t       bool is_super, u64 logical, u64 physical)\n{\n\tstruct btrfs_fs_info *fs_info = dev->fs_info;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key found_key;\n\tstruct extent_buffer *eb;\n\tstruct btrfs_extent_item *ei;\n\tstruct scrub_warning swarn;\n\tu64 flags = 0;\n\tu32 item_size;\n\tint ret;\n\n\t \n\tif (is_super) {\n\t\tbtrfs_warn_in_rcu(fs_info, \"%s on device %s, physical %llu\",\n\t\t\t\t  errstr, btrfs_dev_name(dev), physical);\n\t\treturn;\n\t}\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn;\n\n\tswarn.physical = physical;\n\tswarn.logical = logical;\n\tswarn.errstr = errstr;\n\tswarn.dev = NULL;\n\n\tret = extent_from_logical(fs_info, swarn.logical, path, &found_key,\n\t\t\t\t  &flags);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tswarn.extent_item_size = found_key.offset;\n\n\teb = path->nodes[0];\n\tei = btrfs_item_ptr(eb, path->slots[0], struct btrfs_extent_item);\n\titem_size = btrfs_item_size(eb, path->slots[0]);\n\n\tif (flags & BTRFS_EXTENT_FLAG_TREE_BLOCK) {\n\t\tunsigned long ptr = 0;\n\t\tu8 ref_level;\n\t\tu64 ref_root;\n\n\t\twhile (true) {\n\t\t\tret = tree_backref_for_extent(&ptr, eb, &found_key, ei,\n\t\t\t\t\t\t      item_size, &ref_root,\n\t\t\t\t\t\t      &ref_level);\n\t\t\tif (ret < 0) {\n\t\t\t\tbtrfs_warn(fs_info,\n\t\t\t\t\"failed to resolve tree backref for logical %llu: %d\",\n\t\t\t\t\t\t  swarn.logical, ret);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (ret > 0)\n\t\t\t\tbreak;\n\t\t\tbtrfs_warn_in_rcu(fs_info,\n\"%s at logical %llu on dev %s, physical %llu: metadata %s (level %d) in tree %llu\",\n\t\t\t\terrstr, swarn.logical, btrfs_dev_name(dev),\n\t\t\t\tswarn.physical, (ref_level ? \"node\" : \"leaf\"),\n\t\t\t\tref_level, ref_root);\n\t\t}\n\t\tbtrfs_release_path(path);\n\t} else {\n\t\tstruct btrfs_backref_walk_ctx ctx = { 0 };\n\n\t\tbtrfs_release_path(path);\n\n\t\tctx.bytenr = found_key.objectid;\n\t\tctx.extent_item_pos = swarn.logical - found_key.objectid;\n\t\tctx.fs_info = fs_info;\n\n\t\tswarn.path = path;\n\t\tswarn.dev = dev;\n\n\t\titerate_extent_inodes(&ctx, true, scrub_print_warning_inode, &swarn);\n\t}\n\nout:\n\tbtrfs_free_path(path);\n}\n\nstatic int fill_writer_pointer_gap(struct scrub_ctx *sctx, u64 physical)\n{\n\tint ret = 0;\n\tu64 length;\n\n\tif (!btrfs_is_zoned(sctx->fs_info))\n\t\treturn 0;\n\n\tif (!btrfs_dev_is_sequential(sctx->wr_tgtdev, physical))\n\t\treturn 0;\n\n\tif (sctx->write_pointer < physical) {\n\t\tlength = physical - sctx->write_pointer;\n\n\t\tret = btrfs_zoned_issue_zeroout(sctx->wr_tgtdev,\n\t\t\t\t\t\tsctx->write_pointer, length);\n\t\tif (!ret)\n\t\t\tsctx->write_pointer = physical;\n\t}\n\treturn ret;\n}\n\nstatic struct page *scrub_stripe_get_page(struct scrub_stripe *stripe, int sector_nr)\n{\n\tstruct btrfs_fs_info *fs_info = stripe->bg->fs_info;\n\tint page_index = (sector_nr << fs_info->sectorsize_bits) >> PAGE_SHIFT;\n\n\treturn stripe->pages[page_index];\n}\n\nstatic unsigned int scrub_stripe_get_page_offset(struct scrub_stripe *stripe,\n\t\t\t\t\t\t int sector_nr)\n{\n\tstruct btrfs_fs_info *fs_info = stripe->bg->fs_info;\n\n\treturn offset_in_page(sector_nr << fs_info->sectorsize_bits);\n}\n\nstatic void scrub_verify_one_metadata(struct scrub_stripe *stripe, int sector_nr)\n{\n\tstruct btrfs_fs_info *fs_info = stripe->bg->fs_info;\n\tconst u32 sectors_per_tree = fs_info->nodesize >> fs_info->sectorsize_bits;\n\tconst u64 logical = stripe->logical + (sector_nr << fs_info->sectorsize_bits);\n\tconst struct page *first_page = scrub_stripe_get_page(stripe, sector_nr);\n\tconst unsigned int first_off = scrub_stripe_get_page_offset(stripe, sector_nr);\n\tSHASH_DESC_ON_STACK(shash, fs_info->csum_shash);\n\tu8 on_disk_csum[BTRFS_CSUM_SIZE];\n\tu8 calculated_csum[BTRFS_CSUM_SIZE];\n\tstruct btrfs_header *header;\n\n\t \n\theader = (struct btrfs_header *)(page_address(first_page) + first_off);\n\tmemcpy(on_disk_csum, header->csum, fs_info->csum_size);\n\n\tif (logical != btrfs_stack_header_bytenr(header)) {\n\t\tbitmap_set(&stripe->csum_error_bitmap, sector_nr, sectors_per_tree);\n\t\tbitmap_set(&stripe->error_bitmap, sector_nr, sectors_per_tree);\n\t\tbtrfs_warn_rl(fs_info,\n\t\t\"tree block %llu mirror %u has bad bytenr, has %llu want %llu\",\n\t\t\t      logical, stripe->mirror_num,\n\t\t\t      btrfs_stack_header_bytenr(header), logical);\n\t\treturn;\n\t}\n\tif (memcmp(header->fsid, fs_info->fs_devices->metadata_uuid,\n\t\t   BTRFS_FSID_SIZE) != 0) {\n\t\tbitmap_set(&stripe->meta_error_bitmap, sector_nr, sectors_per_tree);\n\t\tbitmap_set(&stripe->error_bitmap, sector_nr, sectors_per_tree);\n\t\tbtrfs_warn_rl(fs_info,\n\t\t\"tree block %llu mirror %u has bad fsid, has %pU want %pU\",\n\t\t\t      logical, stripe->mirror_num,\n\t\t\t      header->fsid, fs_info->fs_devices->fsid);\n\t\treturn;\n\t}\n\tif (memcmp(header->chunk_tree_uuid, fs_info->chunk_tree_uuid,\n\t\t   BTRFS_UUID_SIZE) != 0) {\n\t\tbitmap_set(&stripe->meta_error_bitmap, sector_nr, sectors_per_tree);\n\t\tbitmap_set(&stripe->error_bitmap, sector_nr, sectors_per_tree);\n\t\tbtrfs_warn_rl(fs_info,\n\t\t\"tree block %llu mirror %u has bad chunk tree uuid, has %pU want %pU\",\n\t\t\t      logical, stripe->mirror_num,\n\t\t\t      header->chunk_tree_uuid, fs_info->chunk_tree_uuid);\n\t\treturn;\n\t}\n\n\t \n\tshash->tfm = fs_info->csum_shash;\n\tcrypto_shash_init(shash);\n\tcrypto_shash_update(shash, page_address(first_page) + first_off +\n\t\t\t    BTRFS_CSUM_SIZE, fs_info->sectorsize - BTRFS_CSUM_SIZE);\n\n\tfor (int i = sector_nr + 1; i < sector_nr + sectors_per_tree; i++) {\n\t\tstruct page *page = scrub_stripe_get_page(stripe, i);\n\t\tunsigned int page_off = scrub_stripe_get_page_offset(stripe, i);\n\n\t\tcrypto_shash_update(shash, page_address(page) + page_off,\n\t\t\t\t    fs_info->sectorsize);\n\t}\n\n\tcrypto_shash_final(shash, calculated_csum);\n\tif (memcmp(calculated_csum, on_disk_csum, fs_info->csum_size) != 0) {\n\t\tbitmap_set(&stripe->meta_error_bitmap, sector_nr, sectors_per_tree);\n\t\tbitmap_set(&stripe->error_bitmap, sector_nr, sectors_per_tree);\n\t\tbtrfs_warn_rl(fs_info,\n\t\t\"tree block %llu mirror %u has bad csum, has \" CSUM_FMT \" want \" CSUM_FMT,\n\t\t\t      logical, stripe->mirror_num,\n\t\t\t      CSUM_FMT_VALUE(fs_info->csum_size, on_disk_csum),\n\t\t\t      CSUM_FMT_VALUE(fs_info->csum_size, calculated_csum));\n\t\treturn;\n\t}\n\tif (stripe->sectors[sector_nr].generation !=\n\t    btrfs_stack_header_generation(header)) {\n\t\tbitmap_set(&stripe->meta_error_bitmap, sector_nr, sectors_per_tree);\n\t\tbitmap_set(&stripe->error_bitmap, sector_nr, sectors_per_tree);\n\t\tbtrfs_warn_rl(fs_info,\n\t\t\"tree block %llu mirror %u has bad generation, has %llu want %llu\",\n\t\t\t      logical, stripe->mirror_num,\n\t\t\t      btrfs_stack_header_generation(header),\n\t\t\t      stripe->sectors[sector_nr].generation);\n\t\treturn;\n\t}\n\tbitmap_clear(&stripe->error_bitmap, sector_nr, sectors_per_tree);\n\tbitmap_clear(&stripe->csum_error_bitmap, sector_nr, sectors_per_tree);\n\tbitmap_clear(&stripe->meta_error_bitmap, sector_nr, sectors_per_tree);\n}\n\nstatic void scrub_verify_one_sector(struct scrub_stripe *stripe, int sector_nr)\n{\n\tstruct btrfs_fs_info *fs_info = stripe->bg->fs_info;\n\tstruct scrub_sector_verification *sector = &stripe->sectors[sector_nr];\n\tconst u32 sectors_per_tree = fs_info->nodesize >> fs_info->sectorsize_bits;\n\tstruct page *page = scrub_stripe_get_page(stripe, sector_nr);\n\tunsigned int pgoff = scrub_stripe_get_page_offset(stripe, sector_nr);\n\tu8 csum_buf[BTRFS_CSUM_SIZE];\n\tint ret;\n\n\tASSERT(sector_nr >= 0 && sector_nr < stripe->nr_sectors);\n\n\t \n\tif (!test_bit(sector_nr, &stripe->extent_sector_bitmap))\n\t\treturn;\n\n\t \n\tif (test_bit(sector_nr, &stripe->io_error_bitmap))\n\t\treturn;\n\n\t \n\tif (sector->is_metadata) {\n\t\t \n\t\tif (unlikely(sector_nr + sectors_per_tree > stripe->nr_sectors)) {\n\t\t\tbtrfs_warn_rl(fs_info,\n\t\t\t\"tree block at %llu crosses stripe boundary %llu\",\n\t\t\t\t      stripe->logical +\n\t\t\t\t      (sector_nr << fs_info->sectorsize_bits),\n\t\t\t\t      stripe->logical);\n\t\t\treturn;\n\t\t}\n\t\tscrub_verify_one_metadata(stripe, sector_nr);\n\t\treturn;\n\t}\n\n\t \n\tif (!sector->csum) {\n\t\tclear_bit(sector_nr, &stripe->error_bitmap);\n\t\treturn;\n\t}\n\n\tret = btrfs_check_sector_csum(fs_info, page, pgoff, csum_buf, sector->csum);\n\tif (ret < 0) {\n\t\tset_bit(sector_nr, &stripe->csum_error_bitmap);\n\t\tset_bit(sector_nr, &stripe->error_bitmap);\n\t} else {\n\t\tclear_bit(sector_nr, &stripe->csum_error_bitmap);\n\t\tclear_bit(sector_nr, &stripe->error_bitmap);\n\t}\n}\n\n \nstatic void scrub_verify_one_stripe(struct scrub_stripe *stripe, unsigned long bitmap)\n{\n\tstruct btrfs_fs_info *fs_info = stripe->bg->fs_info;\n\tconst u32 sectors_per_tree = fs_info->nodesize >> fs_info->sectorsize_bits;\n\tint sector_nr;\n\n\tfor_each_set_bit(sector_nr, &bitmap, stripe->nr_sectors) {\n\t\tscrub_verify_one_sector(stripe, sector_nr);\n\t\tif (stripe->sectors[sector_nr].is_metadata)\n\t\t\tsector_nr += sectors_per_tree - 1;\n\t}\n}\n\nstatic int calc_sector_number(struct scrub_stripe *stripe, struct bio_vec *first_bvec)\n{\n\tint i;\n\n\tfor (i = 0; i < stripe->nr_sectors; i++) {\n\t\tif (scrub_stripe_get_page(stripe, i) == first_bvec->bv_page &&\n\t\t    scrub_stripe_get_page_offset(stripe, i) == first_bvec->bv_offset)\n\t\t\tbreak;\n\t}\n\tASSERT(i < stripe->nr_sectors);\n\treturn i;\n}\n\n \nstatic void scrub_repair_read_endio(struct btrfs_bio *bbio)\n{\n\tstruct scrub_stripe *stripe = bbio->private;\n\tstruct btrfs_fs_info *fs_info = stripe->bg->fs_info;\n\tstruct bio_vec *bvec;\n\tint sector_nr = calc_sector_number(stripe, bio_first_bvec_all(&bbio->bio));\n\tu32 bio_size = 0;\n\tint i;\n\n\tASSERT(sector_nr < stripe->nr_sectors);\n\n\tbio_for_each_bvec_all(bvec, &bbio->bio, i)\n\t\tbio_size += bvec->bv_len;\n\n\tif (bbio->bio.bi_status) {\n\t\tbitmap_set(&stripe->io_error_bitmap, sector_nr,\n\t\t\t   bio_size >> fs_info->sectorsize_bits);\n\t\tbitmap_set(&stripe->error_bitmap, sector_nr,\n\t\t\t   bio_size >> fs_info->sectorsize_bits);\n\t} else {\n\t\tbitmap_clear(&stripe->io_error_bitmap, sector_nr,\n\t\t\t     bio_size >> fs_info->sectorsize_bits);\n\t}\n\tbio_put(&bbio->bio);\n\tif (atomic_dec_and_test(&stripe->pending_io))\n\t\twake_up(&stripe->io_wait);\n}\n\nstatic int calc_next_mirror(int mirror, int num_copies)\n{\n\tASSERT(mirror <= num_copies);\n\treturn (mirror + 1 > num_copies) ? 1 : mirror + 1;\n}\n\nstatic void scrub_stripe_submit_repair_read(struct scrub_stripe *stripe,\n\t\t\t\t\t    int mirror, int blocksize, bool wait)\n{\n\tstruct btrfs_fs_info *fs_info = stripe->bg->fs_info;\n\tstruct btrfs_bio *bbio = NULL;\n\tconst unsigned long old_error_bitmap = stripe->error_bitmap;\n\tint i;\n\n\tASSERT(stripe->mirror_num >= 1);\n\tASSERT(atomic_read(&stripe->pending_io) == 0);\n\n\tfor_each_set_bit(i, &old_error_bitmap, stripe->nr_sectors) {\n\t\tstruct page *page;\n\t\tint pgoff;\n\t\tint ret;\n\n\t\tpage = scrub_stripe_get_page(stripe, i);\n\t\tpgoff = scrub_stripe_get_page_offset(stripe, i);\n\n\t\t \n\t\tif (bbio && ((i > 0 && !test_bit(i - 1, &stripe->error_bitmap)) ||\n\t\t\t     bbio->bio.bi_iter.bi_size >= blocksize)) {\n\t\t\tASSERT(bbio->bio.bi_iter.bi_size);\n\t\t\tatomic_inc(&stripe->pending_io);\n\t\t\tbtrfs_submit_bio(bbio, mirror);\n\t\t\tif (wait)\n\t\t\t\twait_scrub_stripe_io(stripe);\n\t\t\tbbio = NULL;\n\t\t}\n\n\t\tif (!bbio) {\n\t\t\tbbio = btrfs_bio_alloc(stripe->nr_sectors, REQ_OP_READ,\n\t\t\t\tfs_info, scrub_repair_read_endio, stripe);\n\t\t\tbbio->bio.bi_iter.bi_sector = (stripe->logical +\n\t\t\t\t(i << fs_info->sectorsize_bits)) >> SECTOR_SHIFT;\n\t\t}\n\n\t\tret = bio_add_page(&bbio->bio, page, fs_info->sectorsize, pgoff);\n\t\tASSERT(ret == fs_info->sectorsize);\n\t}\n\tif (bbio) {\n\t\tASSERT(bbio->bio.bi_iter.bi_size);\n\t\tatomic_inc(&stripe->pending_io);\n\t\tbtrfs_submit_bio(bbio, mirror);\n\t\tif (wait)\n\t\t\twait_scrub_stripe_io(stripe);\n\t}\n}\n\nstatic void scrub_stripe_report_errors(struct scrub_ctx *sctx,\n\t\t\t\t       struct scrub_stripe *stripe)\n{\n\tstatic DEFINE_RATELIMIT_STATE(rs, DEFAULT_RATELIMIT_INTERVAL,\n\t\t\t\t      DEFAULT_RATELIMIT_BURST);\n\tstruct btrfs_fs_info *fs_info = sctx->fs_info;\n\tstruct btrfs_device *dev = NULL;\n\tu64 physical = 0;\n\tint nr_data_sectors = 0;\n\tint nr_meta_sectors = 0;\n\tint nr_nodatacsum_sectors = 0;\n\tint nr_repaired_sectors = 0;\n\tint sector_nr;\n\n\tif (test_bit(SCRUB_STRIPE_FLAG_NO_REPORT, &stripe->state))\n\t\treturn;\n\n\t \n\tif (!bitmap_empty(&stripe->init_error_bitmap, stripe->nr_sectors)) {\n\t\tu64 mapped_len = fs_info->sectorsize;\n\t\tstruct btrfs_io_context *bioc = NULL;\n\t\tint stripe_index = stripe->mirror_num - 1;\n\t\tint ret;\n\n\t\t \n\t\tASSERT(stripe->mirror_num >= 1);\n\t\tret = btrfs_map_block(fs_info, BTRFS_MAP_GET_READ_MIRRORS,\n\t\t\t\t      stripe->logical, &mapped_len, &bioc,\n\t\t\t\t      NULL, NULL, 1);\n\t\t \n\t\tif (ret < 0)\n\t\t\tgoto skip;\n\t\tphysical = bioc->stripes[stripe_index].physical;\n\t\tdev = bioc->stripes[stripe_index].dev;\n\t\tbtrfs_put_bioc(bioc);\n\t}\n\nskip:\n\tfor_each_set_bit(sector_nr, &stripe->extent_sector_bitmap, stripe->nr_sectors) {\n\t\tbool repaired = false;\n\n\t\tif (stripe->sectors[sector_nr].is_metadata) {\n\t\t\tnr_meta_sectors++;\n\t\t} else {\n\t\t\tnr_data_sectors++;\n\t\t\tif (!stripe->sectors[sector_nr].csum)\n\t\t\t\tnr_nodatacsum_sectors++;\n\t\t}\n\n\t\tif (test_bit(sector_nr, &stripe->init_error_bitmap) &&\n\t\t    !test_bit(sector_nr, &stripe->error_bitmap)) {\n\t\t\tnr_repaired_sectors++;\n\t\t\trepaired = true;\n\t\t}\n\n\t\t \n\t\tif (!test_bit(sector_nr, &stripe->init_error_bitmap))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (repaired) {\n\t\t\tif (dev) {\n\t\t\t\tbtrfs_err_rl_in_rcu(fs_info,\n\t\t\t\"fixed up error at logical %llu on dev %s physical %llu\",\n\t\t\t\t\t    stripe->logical, btrfs_dev_name(dev),\n\t\t\t\t\t    physical);\n\t\t\t} else {\n\t\t\t\tbtrfs_err_rl_in_rcu(fs_info,\n\t\t\t\"fixed up error at logical %llu on mirror %u\",\n\t\t\t\t\t    stripe->logical, stripe->mirror_num);\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (dev) {\n\t\t\tbtrfs_err_rl_in_rcu(fs_info,\n\t\"unable to fixup (regular) error at logical %llu on dev %s physical %llu\",\n\t\t\t\t\t    stripe->logical, btrfs_dev_name(dev),\n\t\t\t\t\t    physical);\n\t\t} else {\n\t\t\tbtrfs_err_rl_in_rcu(fs_info,\n\t\"unable to fixup (regular) error at logical %llu on mirror %u\",\n\t\t\t\t\t    stripe->logical, stripe->mirror_num);\n\t\t}\n\n\t\tif (test_bit(sector_nr, &stripe->io_error_bitmap))\n\t\t\tif (__ratelimit(&rs) && dev)\n\t\t\t\tscrub_print_common_warning(\"i/o error\", dev, false,\n\t\t\t\t\t\t     stripe->logical, physical);\n\t\tif (test_bit(sector_nr, &stripe->csum_error_bitmap))\n\t\t\tif (__ratelimit(&rs) && dev)\n\t\t\t\tscrub_print_common_warning(\"checksum error\", dev, false,\n\t\t\t\t\t\t     stripe->logical, physical);\n\t\tif (test_bit(sector_nr, &stripe->meta_error_bitmap))\n\t\t\tif (__ratelimit(&rs) && dev)\n\t\t\t\tscrub_print_common_warning(\"header error\", dev, false,\n\t\t\t\t\t\t     stripe->logical, physical);\n\t}\n\n\tspin_lock(&sctx->stat_lock);\n\tsctx->stat.data_extents_scrubbed += stripe->nr_data_extents;\n\tsctx->stat.tree_extents_scrubbed += stripe->nr_meta_extents;\n\tsctx->stat.data_bytes_scrubbed += nr_data_sectors << fs_info->sectorsize_bits;\n\tsctx->stat.tree_bytes_scrubbed += nr_meta_sectors << fs_info->sectorsize_bits;\n\tsctx->stat.no_csum += nr_nodatacsum_sectors;\n\tsctx->stat.read_errors += stripe->init_nr_io_errors;\n\tsctx->stat.csum_errors += stripe->init_nr_csum_errors;\n\tsctx->stat.verify_errors += stripe->init_nr_meta_errors;\n\tsctx->stat.uncorrectable_errors +=\n\t\tbitmap_weight(&stripe->error_bitmap, stripe->nr_sectors);\n\tsctx->stat.corrected_errors += nr_repaired_sectors;\n\tspin_unlock(&sctx->stat_lock);\n}\n\nstatic void scrub_write_sectors(struct scrub_ctx *sctx, struct scrub_stripe *stripe,\n\t\t\t\tunsigned long write_bitmap, bool dev_replace);\n\n \nstatic void scrub_stripe_read_repair_worker(struct work_struct *work)\n{\n\tstruct scrub_stripe *stripe = container_of(work, struct scrub_stripe, work);\n\tstruct scrub_ctx *sctx = stripe->sctx;\n\tstruct btrfs_fs_info *fs_info = sctx->fs_info;\n\tint num_copies = btrfs_num_copies(fs_info, stripe->bg->start,\n\t\t\t\t\t  stripe->bg->length);\n\tint mirror;\n\tint i;\n\n\tASSERT(stripe->mirror_num > 0);\n\n\twait_scrub_stripe_io(stripe);\n\tscrub_verify_one_stripe(stripe, stripe->extent_sector_bitmap);\n\t \n\tstripe->init_error_bitmap = stripe->error_bitmap;\n\tstripe->init_nr_io_errors = bitmap_weight(&stripe->io_error_bitmap,\n\t\t\t\t\t\t  stripe->nr_sectors);\n\tstripe->init_nr_csum_errors = bitmap_weight(&stripe->csum_error_bitmap,\n\t\t\t\t\t\t    stripe->nr_sectors);\n\tstripe->init_nr_meta_errors = bitmap_weight(&stripe->meta_error_bitmap,\n\t\t\t\t\t\t    stripe->nr_sectors);\n\n\tif (bitmap_empty(&stripe->init_error_bitmap, stripe->nr_sectors))\n\t\tgoto out;\n\n\t \n\tfor (mirror = calc_next_mirror(stripe->mirror_num, num_copies);\n\t     mirror != stripe->mirror_num;\n\t     mirror = calc_next_mirror(mirror, num_copies)) {\n\t\tconst unsigned long old_error_bitmap = stripe->error_bitmap;\n\n\t\tscrub_stripe_submit_repair_read(stripe, mirror,\n\t\t\t\t\t\tBTRFS_STRIPE_LEN, false);\n\t\twait_scrub_stripe_io(stripe);\n\t\tscrub_verify_one_stripe(stripe, old_error_bitmap);\n\t\tif (bitmap_empty(&stripe->error_bitmap, stripe->nr_sectors))\n\t\t\tgoto out;\n\t}\n\n\t \n\n\tfor (i = 0, mirror = stripe->mirror_num;\n\t     i < num_copies;\n\t     i++, mirror = calc_next_mirror(mirror, num_copies)) {\n\t\tconst unsigned long old_error_bitmap = stripe->error_bitmap;\n\n\t\tscrub_stripe_submit_repair_read(stripe, mirror,\n\t\t\t\t\t\tfs_info->sectorsize, true);\n\t\twait_scrub_stripe_io(stripe);\n\t\tscrub_verify_one_stripe(stripe, old_error_bitmap);\n\t\tif (bitmap_empty(&stripe->error_bitmap, stripe->nr_sectors))\n\t\t\tgoto out;\n\t}\nout:\n\t \n\tif (btrfs_is_zoned(fs_info)) {\n\t\tif (!bitmap_empty(&stripe->error_bitmap, stripe->nr_sectors))\n\t\t\tbtrfs_repair_one_zone(fs_info, sctx->stripes[0].bg->start);\n\t} else if (!sctx->readonly) {\n\t\tunsigned long repaired;\n\n\t\tbitmap_andnot(&repaired, &stripe->init_error_bitmap,\n\t\t\t      &stripe->error_bitmap, stripe->nr_sectors);\n\t\tscrub_write_sectors(sctx, stripe, repaired, false);\n\t\twait_scrub_stripe_io(stripe);\n\t}\n\n\tscrub_stripe_report_errors(sctx, stripe);\n\tset_bit(SCRUB_STRIPE_FLAG_REPAIR_DONE, &stripe->state);\n\twake_up(&stripe->repair_wait);\n}\n\nstatic void scrub_read_endio(struct btrfs_bio *bbio)\n{\n\tstruct scrub_stripe *stripe = bbio->private;\n\n\tif (bbio->bio.bi_status) {\n\t\tbitmap_set(&stripe->io_error_bitmap, 0, stripe->nr_sectors);\n\t\tbitmap_set(&stripe->error_bitmap, 0, stripe->nr_sectors);\n\t} else {\n\t\tbitmap_clear(&stripe->io_error_bitmap, 0, stripe->nr_sectors);\n\t}\n\tbio_put(&bbio->bio);\n\tif (atomic_dec_and_test(&stripe->pending_io)) {\n\t\twake_up(&stripe->io_wait);\n\t\tINIT_WORK(&stripe->work, scrub_stripe_read_repair_worker);\n\t\tqueue_work(stripe->bg->fs_info->scrub_workers, &stripe->work);\n\t}\n}\n\nstatic void scrub_write_endio(struct btrfs_bio *bbio)\n{\n\tstruct scrub_stripe *stripe = bbio->private;\n\tstruct btrfs_fs_info *fs_info = stripe->bg->fs_info;\n\tstruct bio_vec *bvec;\n\tint sector_nr = calc_sector_number(stripe, bio_first_bvec_all(&bbio->bio));\n\tu32 bio_size = 0;\n\tint i;\n\n\tbio_for_each_bvec_all(bvec, &bbio->bio, i)\n\t\tbio_size += bvec->bv_len;\n\n\tif (bbio->bio.bi_status) {\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(&stripe->write_error_lock, flags);\n\t\tbitmap_set(&stripe->write_error_bitmap, sector_nr,\n\t\t\t   bio_size >> fs_info->sectorsize_bits);\n\t\tspin_unlock_irqrestore(&stripe->write_error_lock, flags);\n\t}\n\tbio_put(&bbio->bio);\n\n\tif (atomic_dec_and_test(&stripe->pending_io))\n\t\twake_up(&stripe->io_wait);\n}\n\nstatic void scrub_submit_write_bio(struct scrub_ctx *sctx,\n\t\t\t\t   struct scrub_stripe *stripe,\n\t\t\t\t   struct btrfs_bio *bbio, bool dev_replace)\n{\n\tstruct btrfs_fs_info *fs_info = sctx->fs_info;\n\tu32 bio_len = bbio->bio.bi_iter.bi_size;\n\tu32 bio_off = (bbio->bio.bi_iter.bi_sector << SECTOR_SHIFT) -\n\t\t      stripe->logical;\n\n\tfill_writer_pointer_gap(sctx, stripe->physical + bio_off);\n\tatomic_inc(&stripe->pending_io);\n\tbtrfs_submit_repair_write(bbio, stripe->mirror_num, dev_replace);\n\tif (!btrfs_is_zoned(fs_info))\n\t\treturn;\n\t \n\twait_scrub_stripe_io(stripe);\n\n\t \n\tif (!test_bit(bio_off >> fs_info->sectorsize_bits,\n\t\t      &stripe->write_error_bitmap))\n\t\tsctx->write_pointer += bio_len;\n}\n\n \nstatic void scrub_write_sectors(struct scrub_ctx *sctx, struct scrub_stripe *stripe,\n\t\t\t\tunsigned long write_bitmap, bool dev_replace)\n{\n\tstruct btrfs_fs_info *fs_info = stripe->bg->fs_info;\n\tstruct btrfs_bio *bbio = NULL;\n\tint sector_nr;\n\n\tfor_each_set_bit(sector_nr, &write_bitmap, stripe->nr_sectors) {\n\t\tstruct page *page = scrub_stripe_get_page(stripe, sector_nr);\n\t\tunsigned int pgoff = scrub_stripe_get_page_offset(stripe, sector_nr);\n\t\tint ret;\n\n\t\t \n\t\tASSERT(test_bit(sector_nr, &stripe->extent_sector_bitmap));\n\n\t\t \n\t\tif (bbio && sector_nr && !test_bit(sector_nr - 1, &write_bitmap)) {\n\t\t\tscrub_submit_write_bio(sctx, stripe, bbio, dev_replace);\n\t\t\tbbio = NULL;\n\t\t}\n\t\tif (!bbio) {\n\t\t\tbbio = btrfs_bio_alloc(stripe->nr_sectors, REQ_OP_WRITE,\n\t\t\t\t\t       fs_info, scrub_write_endio, stripe);\n\t\t\tbbio->bio.bi_iter.bi_sector = (stripe->logical +\n\t\t\t\t(sector_nr << fs_info->sectorsize_bits)) >>\n\t\t\t\tSECTOR_SHIFT;\n\t\t}\n\t\tret = bio_add_page(&bbio->bio, page, fs_info->sectorsize, pgoff);\n\t\tASSERT(ret == fs_info->sectorsize);\n\t}\n\tif (bbio)\n\t\tscrub_submit_write_bio(sctx, stripe, bbio, dev_replace);\n}\n\n \nstatic void scrub_throttle_dev_io(struct scrub_ctx *sctx, struct btrfs_device *device,\n\t\t\t\t  unsigned int bio_size)\n{\n\tconst int time_slice = 1000;\n\ts64 delta;\n\tktime_t now;\n\tu32 div;\n\tu64 bwlimit;\n\n\tbwlimit = READ_ONCE(device->scrub_speed_max);\n\tif (bwlimit == 0)\n\t\treturn;\n\n\t \n\tdiv = max_t(u32, 1, (u32)(bwlimit / (16 * 1024 * 1024)));\n\tdiv = min_t(u32, 64, div);\n\n\t \n\tnow = ktime_get();\n\tif (sctx->throttle_deadline == 0) {\n\t\tsctx->throttle_deadline = ktime_add_ms(now, time_slice / div);\n\t\tsctx->throttle_sent = 0;\n\t}\n\n\t \n\tif (ktime_before(now, sctx->throttle_deadline)) {\n\t\t \n\t\tsctx->throttle_sent += bio_size;\n\t\tif (sctx->throttle_sent <= div_u64(bwlimit, div))\n\t\t\treturn;\n\n\t\t \n\t\tdelta = ktime_ms_delta(sctx->throttle_deadline, now);\n\t} else {\n\t\t \n\t\tdelta = 0;\n\t}\n\n\tif (delta) {\n\t\tlong timeout;\n\n\t\ttimeout = div_u64(delta * HZ, 1000);\n\t\tschedule_timeout_interruptible(timeout);\n\t}\n\n\t \n\tsctx->throttle_deadline = 0;\n}\n\n \nstatic int get_raid56_logic_offset(u64 physical, int num,\n\t\t\t\t   struct map_lookup *map, u64 *offset,\n\t\t\t\t   u64 *stripe_start)\n{\n\tint i;\n\tint j = 0;\n\tu64 last_offset;\n\tconst int data_stripes = nr_data_stripes(map);\n\n\tlast_offset = (physical - map->stripes[num].physical) * data_stripes;\n\tif (stripe_start)\n\t\t*stripe_start = last_offset;\n\n\t*offset = last_offset;\n\tfor (i = 0; i < data_stripes; i++) {\n\t\tu32 stripe_nr;\n\t\tu32 stripe_index;\n\t\tu32 rot;\n\n\t\t*offset = last_offset + btrfs_stripe_nr_to_offset(i);\n\n\t\tstripe_nr = (u32)(*offset >> BTRFS_STRIPE_LEN_SHIFT) / data_stripes;\n\n\t\t \n\t\trot = stripe_nr % map->num_stripes;\n\t\t \n\t\trot += i;\n\t\tstripe_index = rot % map->num_stripes;\n\t\tif (stripe_index == num)\n\t\t\treturn 0;\n\t\tif (stripe_index < num)\n\t\t\tj++;\n\t}\n\t*offset = last_offset + btrfs_stripe_nr_to_offset(j);\n\treturn 1;\n}\n\n \nstatic int compare_extent_item_range(struct btrfs_path *path,\n\t\t\t\t     u64 search_start, u64 search_len)\n{\n\tstruct btrfs_fs_info *fs_info = path->nodes[0]->fs_info;\n\tu64 len;\n\tstruct btrfs_key key;\n\n\tbtrfs_item_key_to_cpu(path->nodes[0], &key, path->slots[0]);\n\tASSERT(key.type == BTRFS_EXTENT_ITEM_KEY ||\n\t       key.type == BTRFS_METADATA_ITEM_KEY);\n\tif (key.type == BTRFS_METADATA_ITEM_KEY)\n\t\tlen = fs_info->nodesize;\n\telse\n\t\tlen = key.offset;\n\n\tif (key.objectid + len <= search_start)\n\t\treturn -1;\n\tif (key.objectid >= search_start + search_len)\n\t\treturn 1;\n\treturn 0;\n}\n\n \nstatic int find_first_extent_item(struct btrfs_root *extent_root,\n\t\t\t\t  struct btrfs_path *path,\n\t\t\t\t  u64 search_start, u64 search_len)\n{\n\tstruct btrfs_fs_info *fs_info = extent_root->fs_info;\n\tstruct btrfs_key key;\n\tint ret;\n\n\t \n\tif (path->nodes[0])\n\t\tgoto search_forward;\n\n\tif (btrfs_fs_incompat(fs_info, SKINNY_METADATA))\n\t\tkey.type = BTRFS_METADATA_ITEM_KEY;\n\telse\n\t\tkey.type = BTRFS_EXTENT_ITEM_KEY;\n\tkey.objectid = search_start;\n\tkey.offset = (u64)-1;\n\n\tret = btrfs_search_slot(NULL, extent_root, &key, path, 0, 0);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tASSERT(ret > 0);\n\t \n\tret = btrfs_previous_extent_item(extent_root, path, 0);\n\tif (ret < 0)\n\t\treturn ret;\n\t \nsearch_forward:\n\twhile (true) {\n\t\tbtrfs_item_key_to_cpu(path->nodes[0], &key, path->slots[0]);\n\t\tif (key.objectid >= search_start + search_len)\n\t\t\tbreak;\n\t\tif (key.type != BTRFS_METADATA_ITEM_KEY &&\n\t\t    key.type != BTRFS_EXTENT_ITEM_KEY)\n\t\t\tgoto next;\n\n\t\tret = compare_extent_item_range(path, search_start, search_len);\n\t\tif (ret == 0)\n\t\t\treturn ret;\n\t\tif (ret > 0)\n\t\t\tbreak;\nnext:\n\t\tpath->slots[0]++;\n\t\tif (path->slots[0] >= btrfs_header_nritems(path->nodes[0])) {\n\t\t\tret = btrfs_next_leaf(extent_root, path);\n\t\t\tif (ret) {\n\t\t\t\t \n\t\t\t\tbtrfs_release_path(path);\n\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\t}\n\tbtrfs_release_path(path);\n\treturn 1;\n}\n\nstatic void get_extent_info(struct btrfs_path *path, u64 *extent_start_ret,\n\t\t\t    u64 *size_ret, u64 *flags_ret, u64 *generation_ret)\n{\n\tstruct btrfs_key key;\n\tstruct btrfs_extent_item *ei;\n\n\tbtrfs_item_key_to_cpu(path->nodes[0], &key, path->slots[0]);\n\tASSERT(key.type == BTRFS_METADATA_ITEM_KEY ||\n\t       key.type == BTRFS_EXTENT_ITEM_KEY);\n\t*extent_start_ret = key.objectid;\n\tif (key.type == BTRFS_METADATA_ITEM_KEY)\n\t\t*size_ret = path->nodes[0]->fs_info->nodesize;\n\telse\n\t\t*size_ret = key.offset;\n\tei = btrfs_item_ptr(path->nodes[0], path->slots[0], struct btrfs_extent_item);\n\t*flags_ret = btrfs_extent_flags(path->nodes[0], ei);\n\t*generation_ret = btrfs_extent_generation(path->nodes[0], ei);\n}\n\nstatic int sync_write_pointer_for_zoned(struct scrub_ctx *sctx, u64 logical,\n\t\t\t\t\tu64 physical, u64 physical_end)\n{\n\tstruct btrfs_fs_info *fs_info = sctx->fs_info;\n\tint ret = 0;\n\n\tif (!btrfs_is_zoned(fs_info))\n\t\treturn 0;\n\n\tmutex_lock(&sctx->wr_lock);\n\tif (sctx->write_pointer < physical_end) {\n\t\tret = btrfs_sync_zone_write_pointer(sctx->wr_tgtdev, logical,\n\t\t\t\t\t\t    physical,\n\t\t\t\t\t\t    sctx->write_pointer);\n\t\tif (ret)\n\t\t\tbtrfs_err(fs_info,\n\t\t\t\t  \"zoned: failed to recover write pointer\");\n\t}\n\tmutex_unlock(&sctx->wr_lock);\n\tbtrfs_dev_clear_zone_empty(sctx->wr_tgtdev, physical);\n\n\treturn ret;\n}\n\nstatic void fill_one_extent_info(struct btrfs_fs_info *fs_info,\n\t\t\t\t struct scrub_stripe *stripe,\n\t\t\t\t u64 extent_start, u64 extent_len,\n\t\t\t\t u64 extent_flags, u64 extent_gen)\n{\n\tfor (u64 cur_logical = max(stripe->logical, extent_start);\n\t     cur_logical < min(stripe->logical + BTRFS_STRIPE_LEN,\n\t\t\t       extent_start + extent_len);\n\t     cur_logical += fs_info->sectorsize) {\n\t\tconst int nr_sector = (cur_logical - stripe->logical) >>\n\t\t\t\t      fs_info->sectorsize_bits;\n\t\tstruct scrub_sector_verification *sector =\n\t\t\t\t\t\t&stripe->sectors[nr_sector];\n\n\t\tset_bit(nr_sector, &stripe->extent_sector_bitmap);\n\t\tif (extent_flags & BTRFS_EXTENT_FLAG_TREE_BLOCK) {\n\t\t\tsector->is_metadata = true;\n\t\t\tsector->generation = extent_gen;\n\t\t}\n\t}\n}\n\nstatic void scrub_stripe_reset_bitmaps(struct scrub_stripe *stripe)\n{\n\tstripe->extent_sector_bitmap = 0;\n\tstripe->init_error_bitmap = 0;\n\tstripe->init_nr_io_errors = 0;\n\tstripe->init_nr_csum_errors = 0;\n\tstripe->init_nr_meta_errors = 0;\n\tstripe->error_bitmap = 0;\n\tstripe->io_error_bitmap = 0;\n\tstripe->csum_error_bitmap = 0;\n\tstripe->meta_error_bitmap = 0;\n}\n\n \nstatic int scrub_find_fill_first_stripe(struct btrfs_block_group *bg,\n\t\t\t\t\tstruct btrfs_path *extent_path,\n\t\t\t\t\tstruct btrfs_path *csum_path,\n\t\t\t\t\tstruct btrfs_device *dev, u64 physical,\n\t\t\t\t\tint mirror_num, u64 logical_start,\n\t\t\t\t\tu32 logical_len,\n\t\t\t\t\tstruct scrub_stripe *stripe)\n{\n\tstruct btrfs_fs_info *fs_info = bg->fs_info;\n\tstruct btrfs_root *extent_root = btrfs_extent_root(fs_info, bg->start);\n\tstruct btrfs_root *csum_root = btrfs_csum_root(fs_info, bg->start);\n\tconst u64 logical_end = logical_start + logical_len;\n\tu64 cur_logical = logical_start;\n\tu64 stripe_end;\n\tu64 extent_start;\n\tu64 extent_len;\n\tu64 extent_flags;\n\tu64 extent_gen;\n\tint ret;\n\n\tmemset(stripe->sectors, 0, sizeof(struct scrub_sector_verification) *\n\t\t\t\t   stripe->nr_sectors);\n\tscrub_stripe_reset_bitmaps(stripe);\n\n\t \n\tASSERT(logical_start >= bg->start && logical_end <= bg->start + bg->length);\n\n\tret = find_first_extent_item(extent_root, extent_path, logical_start,\n\t\t\t\t     logical_len);\n\t \n\tif (ret)\n\t\tgoto out;\n\tget_extent_info(extent_path, &extent_start, &extent_len, &extent_flags,\n\t\t\t&extent_gen);\n\tif (extent_flags & BTRFS_EXTENT_FLAG_TREE_BLOCK)\n\t\tstripe->nr_meta_extents++;\n\tif (extent_flags & BTRFS_EXTENT_FLAG_DATA)\n\t\tstripe->nr_data_extents++;\n\tcur_logical = max(extent_start, cur_logical);\n\n\t \n\tstripe->logical = round_down(cur_logical - bg->start, BTRFS_STRIPE_LEN) +\n\t\t\t  bg->start;\n\tstripe->physical = physical + stripe->logical - logical_start;\n\tstripe->dev = dev;\n\tstripe->bg = bg;\n\tstripe->mirror_num = mirror_num;\n\tstripe_end = stripe->logical + BTRFS_STRIPE_LEN - 1;\n\n\t \n\tfill_one_extent_info(fs_info, stripe, extent_start, extent_len,\n\t\t\t     extent_flags, extent_gen);\n\tcur_logical = extent_start + extent_len;\n\n\t \n\twhile (cur_logical <= stripe_end) {\n\t\tret = find_first_extent_item(extent_root, extent_path, cur_logical,\n\t\t\t\t\t     stripe_end - cur_logical + 1);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\tif (ret > 0) {\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t\tget_extent_info(extent_path, &extent_start, &extent_len,\n\t\t\t\t&extent_flags, &extent_gen);\n\t\tif (extent_flags & BTRFS_EXTENT_FLAG_TREE_BLOCK)\n\t\t\tstripe->nr_meta_extents++;\n\t\tif (extent_flags & BTRFS_EXTENT_FLAG_DATA)\n\t\t\tstripe->nr_data_extents++;\n\t\tfill_one_extent_info(fs_info, stripe, extent_start, extent_len,\n\t\t\t\t     extent_flags, extent_gen);\n\t\tcur_logical = extent_start + extent_len;\n\t}\n\n\t \n\tif (bg->flags & BTRFS_BLOCK_GROUP_DATA) {\n\t\tint sector_nr;\n\t\tunsigned long csum_bitmap = 0;\n\n\t\t \n\t\tASSERT(stripe->csums);\n\n\t\t \n\t\tASSERT(BITS_PER_LONG >= BTRFS_STRIPE_LEN >> fs_info->sectorsize_bits);\n\n\t\tret = btrfs_lookup_csums_bitmap(csum_root, csum_path,\n\t\t\t\t\t\tstripe->logical, stripe_end,\n\t\t\t\t\t\tstripe->csums, &csum_bitmap);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\tif (ret > 0)\n\t\t\tret = 0;\n\n\t\tfor_each_set_bit(sector_nr, &csum_bitmap, stripe->nr_sectors) {\n\t\t\tstripe->sectors[sector_nr].csum = stripe->csums +\n\t\t\t\tsector_nr * fs_info->csum_size;\n\t\t}\n\t}\n\tset_bit(SCRUB_STRIPE_FLAG_INITIALIZED, &stripe->state);\nout:\n\treturn ret;\n}\n\nstatic void scrub_reset_stripe(struct scrub_stripe *stripe)\n{\n\tscrub_stripe_reset_bitmaps(stripe);\n\n\tstripe->nr_meta_extents = 0;\n\tstripe->nr_data_extents = 0;\n\tstripe->state = 0;\n\n\tfor (int i = 0; i < stripe->nr_sectors; i++) {\n\t\tstripe->sectors[i].is_metadata = false;\n\t\tstripe->sectors[i].csum = NULL;\n\t\tstripe->sectors[i].generation = 0;\n\t}\n}\n\nstatic void scrub_submit_initial_read(struct scrub_ctx *sctx,\n\t\t\t\t      struct scrub_stripe *stripe)\n{\n\tstruct btrfs_fs_info *fs_info = sctx->fs_info;\n\tstruct btrfs_bio *bbio;\n\tint mirror = stripe->mirror_num;\n\n\tASSERT(stripe->bg);\n\tASSERT(stripe->mirror_num > 0);\n\tASSERT(test_bit(SCRUB_STRIPE_FLAG_INITIALIZED, &stripe->state));\n\n\tbbio = btrfs_bio_alloc(SCRUB_STRIPE_PAGES, REQ_OP_READ, fs_info,\n\t\t\t       scrub_read_endio, stripe);\n\n\t \n\tbbio->bio.bi_iter.bi_sector = stripe->logical >> SECTOR_SHIFT;\n\tfor (int i = 0; i < BTRFS_STRIPE_LEN >> PAGE_SHIFT; i++) {\n\t\tint ret;\n\n\t\tret = bio_add_page(&bbio->bio, stripe->pages[i], PAGE_SIZE, 0);\n\t\t \n\t\tASSERT(ret == PAGE_SIZE);\n\t}\n\tatomic_inc(&stripe->pending_io);\n\n\t \n\tif (sctx->is_dev_replace &&\n\t    (fs_info->dev_replace.cont_reading_from_srcdev_mode ==\n\t     BTRFS_DEV_REPLACE_ITEM_CONT_READING_FROM_SRCDEV_MODE_AVOID ||\n\t     !stripe->dev->bdev)) {\n\t\tint num_copies = btrfs_num_copies(fs_info, stripe->bg->start,\n\t\t\t\t\t\t  stripe->bg->length);\n\n\t\tmirror = calc_next_mirror(mirror, num_copies);\n\t}\n\tbtrfs_submit_bio(bbio, mirror);\n}\n\nstatic bool stripe_has_metadata_error(struct scrub_stripe *stripe)\n{\n\tint i;\n\n\tfor_each_set_bit(i, &stripe->error_bitmap, stripe->nr_sectors) {\n\t\tif (stripe->sectors[i].is_metadata) {\n\t\t\tstruct btrfs_fs_info *fs_info = stripe->bg->fs_info;\n\n\t\t\tbtrfs_err(fs_info,\n\t\t\t\"stripe %llu has unrepaired metadata sector at %llu\",\n\t\t\t\t  stripe->logical,\n\t\t\t\t  stripe->logical + (i << fs_info->sectorsize_bits));\n\t\t\treturn true;\n\t\t}\n\t}\n\treturn false;\n}\n\nstatic void submit_initial_group_read(struct scrub_ctx *sctx,\n\t\t\t\t      unsigned int first_slot,\n\t\t\t\t      unsigned int nr_stripes)\n{\n\tstruct blk_plug plug;\n\n\tASSERT(first_slot < SCRUB_TOTAL_STRIPES);\n\tASSERT(first_slot + nr_stripes <= SCRUB_TOTAL_STRIPES);\n\n\tscrub_throttle_dev_io(sctx, sctx->stripes[0].dev,\n\t\t\t      btrfs_stripe_nr_to_offset(nr_stripes));\n\tblk_start_plug(&plug);\n\tfor (int i = 0; i < nr_stripes; i++) {\n\t\tstruct scrub_stripe *stripe = &sctx->stripes[first_slot + i];\n\n\t\t \n\t\tASSERT(test_bit(SCRUB_STRIPE_FLAG_INITIALIZED, &stripe->state));\n\t\tscrub_submit_initial_read(sctx, stripe);\n\t}\n\tblk_finish_plug(&plug);\n}\n\nstatic int flush_scrub_stripes(struct scrub_ctx *sctx)\n{\n\tstruct btrfs_fs_info *fs_info = sctx->fs_info;\n\tstruct scrub_stripe *stripe;\n\tconst int nr_stripes = sctx->cur_stripe;\n\tint ret = 0;\n\n\tif (!nr_stripes)\n\t\treturn 0;\n\n\tASSERT(test_bit(SCRUB_STRIPE_FLAG_INITIALIZED, &sctx->stripes[0].state));\n\n\t \n\tif (nr_stripes % SCRUB_STRIPES_PER_GROUP) {\n\t\tconst int first_slot = round_down(nr_stripes, SCRUB_STRIPES_PER_GROUP);\n\n\t\tsubmit_initial_group_read(sctx, first_slot, nr_stripes - first_slot);\n\t}\n\n\tfor (int i = 0; i < nr_stripes; i++) {\n\t\tstripe = &sctx->stripes[i];\n\n\t\twait_event(stripe->repair_wait,\n\t\t\t   test_bit(SCRUB_STRIPE_FLAG_REPAIR_DONE, &stripe->state));\n\t}\n\n\t \n\tif (sctx->is_dev_replace) {\n\t\t \n\t\tfor (int i = 0; i < nr_stripes; i++) {\n\t\t\tif (stripe_has_metadata_error(&sctx->stripes[i])) {\n\t\t\t\tret = -EIO;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\tfor (int i = 0; i < nr_stripes; i++) {\n\t\t\tunsigned long good;\n\n\t\t\tstripe = &sctx->stripes[i];\n\n\t\t\tASSERT(stripe->dev == fs_info->dev_replace.srcdev);\n\n\t\t\tbitmap_andnot(&good, &stripe->extent_sector_bitmap,\n\t\t\t\t      &stripe->error_bitmap, stripe->nr_sectors);\n\t\t\tscrub_write_sectors(sctx, stripe, good, true);\n\t\t}\n\t}\n\n\t \n\tfor (int i = 0; i < nr_stripes; i++) {\n\t\tstripe = &sctx->stripes[i];\n\n\t\twait_scrub_stripe_io(stripe);\n\t\tscrub_reset_stripe(stripe);\n\t}\nout:\n\tsctx->cur_stripe = 0;\n\treturn ret;\n}\n\nstatic void raid56_scrub_wait_endio(struct bio *bio)\n{\n\tcomplete(bio->bi_private);\n}\n\nstatic int queue_scrub_stripe(struct scrub_ctx *sctx, struct btrfs_block_group *bg,\n\t\t\t      struct btrfs_device *dev, int mirror_num,\n\t\t\t      u64 logical, u32 length, u64 physical,\n\t\t\t      u64 *found_logical_ret)\n{\n\tstruct scrub_stripe *stripe;\n\tint ret;\n\n\t \n\tASSERT(sctx->cur_stripe < SCRUB_TOTAL_STRIPES);\n\n\t \n\tASSERT(found_logical_ret);\n\n\tstripe = &sctx->stripes[sctx->cur_stripe];\n\tscrub_reset_stripe(stripe);\n\tret = scrub_find_fill_first_stripe(bg, &sctx->extent_path,\n\t\t\t\t\t   &sctx->csum_path, dev, physical,\n\t\t\t\t\t   mirror_num, logical, length, stripe);\n\t \n\tif (ret)\n\t\treturn ret;\n\t*found_logical_ret = stripe->logical;\n\tsctx->cur_stripe++;\n\n\t \n\tif (sctx->cur_stripe % SCRUB_STRIPES_PER_GROUP == 0) {\n\t\tconst int first_slot = sctx->cur_stripe - SCRUB_STRIPES_PER_GROUP;\n\n\t\tsubmit_initial_group_read(sctx, first_slot, SCRUB_STRIPES_PER_GROUP);\n\t}\n\n\t \n\tif (sctx->cur_stripe == SCRUB_TOTAL_STRIPES)\n\t\treturn flush_scrub_stripes(sctx);\n\treturn 0;\n}\n\nstatic int scrub_raid56_parity_stripe(struct scrub_ctx *sctx,\n\t\t\t\t      struct btrfs_device *scrub_dev,\n\t\t\t\t      struct btrfs_block_group *bg,\n\t\t\t\t      struct map_lookup *map,\n\t\t\t\t      u64 full_stripe_start)\n{\n\tDECLARE_COMPLETION_ONSTACK(io_done);\n\tstruct btrfs_fs_info *fs_info = sctx->fs_info;\n\tstruct btrfs_raid_bio *rbio;\n\tstruct btrfs_io_context *bioc = NULL;\n\tstruct btrfs_path extent_path = { 0 };\n\tstruct btrfs_path csum_path = { 0 };\n\tstruct bio *bio;\n\tstruct scrub_stripe *stripe;\n\tbool all_empty = true;\n\tconst int data_stripes = nr_data_stripes(map);\n\tunsigned long extent_bitmap = 0;\n\tu64 length = btrfs_stripe_nr_to_offset(data_stripes);\n\tint ret;\n\n\tASSERT(sctx->raid56_data_stripes);\n\n\t \n\textent_path.search_commit_root = 1;\n\textent_path.skip_locking = 1;\n\tcsum_path.search_commit_root = 1;\n\tcsum_path.skip_locking = 1;\n\n\tfor (int i = 0; i < data_stripes; i++) {\n\t\tint stripe_index;\n\t\tint rot;\n\t\tu64 physical;\n\n\t\tstripe = &sctx->raid56_data_stripes[i];\n\t\trot = div_u64(full_stripe_start - bg->start,\n\t\t\t      data_stripes) >> BTRFS_STRIPE_LEN_SHIFT;\n\t\tstripe_index = (i + rot) % map->num_stripes;\n\t\tphysical = map->stripes[stripe_index].physical +\n\t\t\t   btrfs_stripe_nr_to_offset(rot);\n\n\t\tscrub_reset_stripe(stripe);\n\t\tset_bit(SCRUB_STRIPE_FLAG_NO_REPORT, &stripe->state);\n\t\tret = scrub_find_fill_first_stripe(bg, &extent_path, &csum_path,\n\t\t\t\tmap->stripes[stripe_index].dev, physical, 1,\n\t\t\t\tfull_stripe_start + btrfs_stripe_nr_to_offset(i),\n\t\t\t\tBTRFS_STRIPE_LEN, stripe);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\t \n\t\tif (ret > 0) {\n\t\t\tstripe->logical = full_stripe_start +\n\t\t\t\t\t  btrfs_stripe_nr_to_offset(i);\n\t\t\tstripe->dev = map->stripes[stripe_index].dev;\n\t\t\tstripe->mirror_num = 1;\n\t\t\tset_bit(SCRUB_STRIPE_FLAG_INITIALIZED, &stripe->state);\n\t\t}\n\t}\n\n\t \n\tfor (int i = 0; i < data_stripes; i++) {\n\t\tstripe = &sctx->raid56_data_stripes[i];\n\t\tif (!bitmap_empty(&stripe->extent_sector_bitmap, stripe->nr_sectors)) {\n\t\t\tall_empty = false;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (all_empty) {\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\tfor (int i = 0; i < data_stripes; i++) {\n\t\tstripe = &sctx->raid56_data_stripes[i];\n\t\tscrub_submit_initial_read(sctx, stripe);\n\t}\n\tfor (int i = 0; i < data_stripes; i++) {\n\t\tstripe = &sctx->raid56_data_stripes[i];\n\n\t\twait_event(stripe->repair_wait,\n\t\t\t   test_bit(SCRUB_STRIPE_FLAG_REPAIR_DONE, &stripe->state));\n\t}\n\t \n\tASSERT(!btrfs_is_zoned(sctx->fs_info));\n\n\t \n\tfor (int i = 0; i < data_stripes; i++) {\n\t\tunsigned long error;\n\n\t\tstripe = &sctx->raid56_data_stripes[i];\n\n\t\t \n\t\tbitmap_and(&error, &stripe->error_bitmap,\n\t\t\t   &stripe->extent_sector_bitmap, stripe->nr_sectors);\n\t\tif (!bitmap_empty(&error, stripe->nr_sectors)) {\n\t\t\tbtrfs_err(fs_info,\n\"unrepaired sectors detected, full stripe %llu data stripe %u errors %*pbl\",\n\t\t\t\t  full_stripe_start, i, stripe->nr_sectors,\n\t\t\t\t  &error);\n\t\t\tret = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t\tbitmap_or(&extent_bitmap, &extent_bitmap,\n\t\t\t  &stripe->extent_sector_bitmap, stripe->nr_sectors);\n\t}\n\n\t \n\tbio = bio_alloc(NULL, 1, REQ_OP_READ, GFP_NOFS);\n\tbio->bi_iter.bi_sector = full_stripe_start >> SECTOR_SHIFT;\n\tbio->bi_private = &io_done;\n\tbio->bi_end_io = raid56_scrub_wait_endio;\n\n\tbtrfs_bio_counter_inc_blocked(fs_info);\n\tret = btrfs_map_block(fs_info, BTRFS_MAP_WRITE, full_stripe_start,\n\t\t\t      &length, &bioc, NULL, NULL, 1);\n\tif (ret < 0) {\n\t\tbtrfs_put_bioc(bioc);\n\t\tbtrfs_bio_counter_dec(fs_info);\n\t\tgoto out;\n\t}\n\trbio = raid56_parity_alloc_scrub_rbio(bio, bioc, scrub_dev, &extent_bitmap,\n\t\t\t\tBTRFS_STRIPE_LEN >> fs_info->sectorsize_bits);\n\tbtrfs_put_bioc(bioc);\n\tif (!rbio) {\n\t\tret = -ENOMEM;\n\t\tbtrfs_bio_counter_dec(fs_info);\n\t\tgoto out;\n\t}\n\t \n\tfor (int i = 0; i < data_stripes; i++) {\n\t\tstripe = &sctx->raid56_data_stripes[i];\n\n\t\traid56_parity_cache_data_pages(rbio, stripe->pages,\n\t\t\t\tfull_stripe_start + (i << BTRFS_STRIPE_LEN_SHIFT));\n\t}\n\traid56_parity_submit_scrub_rbio(rbio);\n\twait_for_completion_io(&io_done);\n\tret = blk_status_to_errno(bio->bi_status);\n\tbio_put(bio);\n\tbtrfs_bio_counter_dec(fs_info);\n\n\tbtrfs_release_path(&extent_path);\n\tbtrfs_release_path(&csum_path);\nout:\n\treturn ret;\n}\n\n \nstatic int scrub_simple_mirror(struct scrub_ctx *sctx,\n\t\t\t       struct btrfs_block_group *bg,\n\t\t\t       struct map_lookup *map,\n\t\t\t       u64 logical_start, u64 logical_length,\n\t\t\t       struct btrfs_device *device,\n\t\t\t       u64 physical, int mirror_num)\n{\n\tstruct btrfs_fs_info *fs_info = sctx->fs_info;\n\tconst u64 logical_end = logical_start + logical_length;\n\tu64 cur_logical = logical_start;\n\tint ret;\n\n\t \n\tASSERT(logical_start >= bg->start && logical_end <= bg->start + bg->length);\n\n\t \n\twhile (cur_logical < logical_end) {\n\t\tu64 found_logical = U64_MAX;\n\t\tu64 cur_physical = physical + cur_logical - logical_start;\n\n\t\t \n\t\tif (atomic_read(&fs_info->scrub_cancel_req) ||\n\t\t    atomic_read(&sctx->cancel_req)) {\n\t\t\tret = -ECANCELED;\n\t\t\tbreak;\n\t\t}\n\t\t \n\t\tif (atomic_read(&fs_info->scrub_pause_req)) {\n\t\t\t \n\t\t\tscrub_blocked_if_needed(fs_info);\n\t\t}\n\t\t \n\t\tspin_lock(&bg->lock);\n\t\tif (test_bit(BLOCK_GROUP_FLAG_REMOVED, &bg->runtime_flags)) {\n\t\t\tspin_unlock(&bg->lock);\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock(&bg->lock);\n\n\t\tret = queue_scrub_stripe(sctx, bg, device, mirror_num,\n\t\t\t\t\t cur_logical, logical_end - cur_logical,\n\t\t\t\t\t cur_physical, &found_logical);\n\t\tif (ret > 0) {\n\t\t\t \n\t\t\tsctx->stat.last_physical = physical + logical_length;\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t\tif (ret < 0)\n\t\t\tbreak;\n\n\t\t \n\t\tASSERT(found_logical != U64_MAX);\n\t\tcur_logical = found_logical + BTRFS_STRIPE_LEN;\n\n\t\t \n\t\tcond_resched();\n\t}\n\treturn ret;\n}\n\n \nstatic u64 simple_stripe_full_stripe_len(const struct map_lookup *map)\n{\n\tASSERT(map->type & (BTRFS_BLOCK_GROUP_RAID0 |\n\t\t\t    BTRFS_BLOCK_GROUP_RAID10));\n\n\treturn btrfs_stripe_nr_to_offset(map->num_stripes / map->sub_stripes);\n}\n\n \nstatic u64 simple_stripe_get_logical(struct map_lookup *map,\n\t\t\t\t     struct btrfs_block_group *bg,\n\t\t\t\t     int stripe_index)\n{\n\tASSERT(map->type & (BTRFS_BLOCK_GROUP_RAID0 |\n\t\t\t    BTRFS_BLOCK_GROUP_RAID10));\n\tASSERT(stripe_index < map->num_stripes);\n\n\t \n\treturn btrfs_stripe_nr_to_offset(stripe_index / map->sub_stripes) +\n\t       bg->start;\n}\n\n \nstatic int simple_stripe_mirror_num(struct map_lookup *map, int stripe_index)\n{\n\tASSERT(map->type & (BTRFS_BLOCK_GROUP_RAID0 |\n\t\t\t    BTRFS_BLOCK_GROUP_RAID10));\n\tASSERT(stripe_index < map->num_stripes);\n\n\t \n\treturn stripe_index % map->sub_stripes + 1;\n}\n\nstatic int scrub_simple_stripe(struct scrub_ctx *sctx,\n\t\t\t       struct btrfs_block_group *bg,\n\t\t\t       struct map_lookup *map,\n\t\t\t       struct btrfs_device *device,\n\t\t\t       int stripe_index)\n{\n\tconst u64 logical_increment = simple_stripe_full_stripe_len(map);\n\tconst u64 orig_logical = simple_stripe_get_logical(map, bg, stripe_index);\n\tconst u64 orig_physical = map->stripes[stripe_index].physical;\n\tconst int mirror_num = simple_stripe_mirror_num(map, stripe_index);\n\tu64 cur_logical = orig_logical;\n\tu64 cur_physical = orig_physical;\n\tint ret = 0;\n\n\twhile (cur_logical < bg->start + bg->length) {\n\t\t \n\t\tret = scrub_simple_mirror(sctx, bg, map, cur_logical,\n\t\t\t\t\t  BTRFS_STRIPE_LEN, device, cur_physical,\n\t\t\t\t\t  mirror_num);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\t \n\t\tcur_logical += logical_increment;\n\t\t \n\t\tcur_physical += BTRFS_STRIPE_LEN;\n\t}\n\treturn ret;\n}\n\nstatic noinline_for_stack int scrub_stripe(struct scrub_ctx *sctx,\n\t\t\t\t\t   struct btrfs_block_group *bg,\n\t\t\t\t\t   struct extent_map *em,\n\t\t\t\t\t   struct btrfs_device *scrub_dev,\n\t\t\t\t\t   int stripe_index)\n{\n\tstruct btrfs_fs_info *fs_info = sctx->fs_info;\n\tstruct map_lookup *map = em->map_lookup;\n\tconst u64 profile = map->type & BTRFS_BLOCK_GROUP_PROFILE_MASK;\n\tconst u64 chunk_logical = bg->start;\n\tint ret;\n\tint ret2;\n\tu64 physical = map->stripes[stripe_index].physical;\n\tconst u64 dev_stripe_len = btrfs_calc_stripe_length(em);\n\tconst u64 physical_end = physical + dev_stripe_len;\n\tu64 logical;\n\tu64 logic_end;\n\t \n\tu64 increment;\n\t \n\tu64 offset;\n\tu64 stripe_logical;\n\tint stop_loop = 0;\n\n\t \n\tASSERT(sctx->extent_path.nodes[0] == NULL);\n\n\tscrub_blocked_if_needed(fs_info);\n\n\tif (sctx->is_dev_replace &&\n\t    btrfs_dev_is_sequential(sctx->wr_tgtdev, physical)) {\n\t\tmutex_lock(&sctx->wr_lock);\n\t\tsctx->write_pointer = physical;\n\t\tmutex_unlock(&sctx->wr_lock);\n\t}\n\n\t \n\tif (profile & BTRFS_BLOCK_GROUP_RAID56_MASK) {\n\t\tASSERT(sctx->raid56_data_stripes == NULL);\n\n\t\tsctx->raid56_data_stripes = kcalloc(nr_data_stripes(map),\n\t\t\t\t\t\t    sizeof(struct scrub_stripe),\n\t\t\t\t\t\t    GFP_KERNEL);\n\t\tif (!sctx->raid56_data_stripes) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tfor (int i = 0; i < nr_data_stripes(map); i++) {\n\t\t\tret = init_scrub_stripe(fs_info,\n\t\t\t\t\t\t&sctx->raid56_data_stripes[i]);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out;\n\t\t\tsctx->raid56_data_stripes[i].bg = bg;\n\t\t\tsctx->raid56_data_stripes[i].sctx = sctx;\n\t\t}\n\t}\n\t \n\tif (!(profile & (BTRFS_BLOCK_GROUP_RAID0 | BTRFS_BLOCK_GROUP_RAID10 |\n\t\t\t BTRFS_BLOCK_GROUP_RAID56_MASK))) {\n\t\t \n\t\tret = scrub_simple_mirror(sctx, bg, map, bg->start, bg->length,\n\t\t\t\tscrub_dev, map->stripes[stripe_index].physical,\n\t\t\t\tstripe_index + 1);\n\t\toffset = 0;\n\t\tgoto out;\n\t}\n\tif (profile & (BTRFS_BLOCK_GROUP_RAID0 | BTRFS_BLOCK_GROUP_RAID10)) {\n\t\tret = scrub_simple_stripe(sctx, bg, map, scrub_dev, stripe_index);\n\t\toffset = btrfs_stripe_nr_to_offset(stripe_index / map->sub_stripes);\n\t\tgoto out;\n\t}\n\n\t \n\tASSERT(map->type & BTRFS_BLOCK_GROUP_RAID56_MASK);\n\tret = 0;\n\n\t \n\tget_raid56_logic_offset(physical_end, stripe_index,\n\t\t\t\tmap, &logic_end, NULL);\n\tlogic_end += chunk_logical;\n\n\t \n\tget_raid56_logic_offset(physical, stripe_index, map, &offset, NULL);\n\tincrement = btrfs_stripe_nr_to_offset(nr_data_stripes(map));\n\n\t \n\twhile (physical < physical_end) {\n\t\tret = get_raid56_logic_offset(physical, stripe_index, map,\n\t\t\t\t\t      &logical, &stripe_logical);\n\t\tlogical += chunk_logical;\n\t\tif (ret) {\n\t\t\t \n\t\t\tstripe_logical += chunk_logical;\n\t\t\tret = scrub_raid56_parity_stripe(sctx, scrub_dev, bg,\n\t\t\t\t\t\t\t map, stripe_logical);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t\tgoto next;\n\t\t}\n\n\t\t \n\t\tret = scrub_simple_mirror(sctx, bg, map, logical, BTRFS_STRIPE_LEN,\n\t\t\t\t\t  scrub_dev, physical, 1);\n\t\tif (ret < 0)\n\t\t\tgoto out;\nnext:\n\t\tlogical += increment;\n\t\tphysical += BTRFS_STRIPE_LEN;\n\t\tspin_lock(&sctx->stat_lock);\n\t\tif (stop_loop)\n\t\t\tsctx->stat.last_physical =\n\t\t\t\tmap->stripes[stripe_index].physical + dev_stripe_len;\n\t\telse\n\t\t\tsctx->stat.last_physical = physical;\n\t\tspin_unlock(&sctx->stat_lock);\n\t\tif (stop_loop)\n\t\t\tbreak;\n\t}\nout:\n\tret2 = flush_scrub_stripes(sctx);\n\tif (!ret)\n\t\tret = ret2;\n\tbtrfs_release_path(&sctx->extent_path);\n\tbtrfs_release_path(&sctx->csum_path);\n\n\tif (sctx->raid56_data_stripes) {\n\t\tfor (int i = 0; i < nr_data_stripes(map); i++)\n\t\t\trelease_scrub_stripe(&sctx->raid56_data_stripes[i]);\n\t\tkfree(sctx->raid56_data_stripes);\n\t\tsctx->raid56_data_stripes = NULL;\n\t}\n\n\tif (sctx->is_dev_replace && ret >= 0) {\n\t\tint ret2;\n\n\t\tret2 = sync_write_pointer_for_zoned(sctx,\n\t\t\t\tchunk_logical + offset,\n\t\t\t\tmap->stripes[stripe_index].physical,\n\t\t\t\tphysical_end);\n\t\tif (ret2)\n\t\t\tret = ret2;\n\t}\n\n\treturn ret < 0 ? ret : 0;\n}\n\nstatic noinline_for_stack int scrub_chunk(struct scrub_ctx *sctx,\n\t\t\t\t\t  struct btrfs_block_group *bg,\n\t\t\t\t\t  struct btrfs_device *scrub_dev,\n\t\t\t\t\t  u64 dev_offset,\n\t\t\t\t\t  u64 dev_extent_len)\n{\n\tstruct btrfs_fs_info *fs_info = sctx->fs_info;\n\tstruct extent_map_tree *map_tree = &fs_info->mapping_tree;\n\tstruct map_lookup *map;\n\tstruct extent_map *em;\n\tint i;\n\tint ret = 0;\n\n\tread_lock(&map_tree->lock);\n\tem = lookup_extent_mapping(map_tree, bg->start, bg->length);\n\tread_unlock(&map_tree->lock);\n\n\tif (!em) {\n\t\t \n\t\tspin_lock(&bg->lock);\n\t\tif (!test_bit(BLOCK_GROUP_FLAG_REMOVED, &bg->runtime_flags))\n\t\t\tret = -EINVAL;\n\t\tspin_unlock(&bg->lock);\n\n\t\treturn ret;\n\t}\n\tif (em->start != bg->start)\n\t\tgoto out;\n\tif (em->len < dev_extent_len)\n\t\tgoto out;\n\n\tmap = em->map_lookup;\n\tfor (i = 0; i < map->num_stripes; ++i) {\n\t\tif (map->stripes[i].dev->bdev == scrub_dev->bdev &&\n\t\t    map->stripes[i].physical == dev_offset) {\n\t\t\tret = scrub_stripe(sctx, bg, em, scrub_dev, i);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t}\n\t}\nout:\n\tfree_extent_map(em);\n\n\treturn ret;\n}\n\nstatic int finish_extent_writes_for_zoned(struct btrfs_root *root,\n\t\t\t\t\t  struct btrfs_block_group *cache)\n{\n\tstruct btrfs_fs_info *fs_info = cache->fs_info;\n\tstruct btrfs_trans_handle *trans;\n\n\tif (!btrfs_is_zoned(fs_info))\n\t\treturn 0;\n\n\tbtrfs_wait_block_group_reservations(cache);\n\tbtrfs_wait_nocow_writers(cache);\n\tbtrfs_wait_ordered_roots(fs_info, U64_MAX, cache->start, cache->length);\n\n\ttrans = btrfs_join_transaction(root);\n\tif (IS_ERR(trans))\n\t\treturn PTR_ERR(trans);\n\treturn btrfs_commit_transaction(trans);\n}\n\nstatic noinline_for_stack\nint scrub_enumerate_chunks(struct scrub_ctx *sctx,\n\t\t\t   struct btrfs_device *scrub_dev, u64 start, u64 end)\n{\n\tstruct btrfs_dev_extent *dev_extent = NULL;\n\tstruct btrfs_path *path;\n\tstruct btrfs_fs_info *fs_info = sctx->fs_info;\n\tstruct btrfs_root *root = fs_info->dev_root;\n\tu64 chunk_offset;\n\tint ret = 0;\n\tint ro_set;\n\tint slot;\n\tstruct extent_buffer *l;\n\tstruct btrfs_key key;\n\tstruct btrfs_key found_key;\n\tstruct btrfs_block_group *cache;\n\tstruct btrfs_dev_replace *dev_replace = &fs_info->dev_replace;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tpath->reada = READA_FORWARD;\n\tpath->search_commit_root = 1;\n\tpath->skip_locking = 1;\n\n\tkey.objectid = scrub_dev->devid;\n\tkey.offset = 0ull;\n\tkey.type = BTRFS_DEV_EXTENT_KEY;\n\n\twhile (1) {\n\t\tu64 dev_extent_len;\n\n\t\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\t\tif (ret > 0) {\n\t\t\tif (path->slots[0] >=\n\t\t\t    btrfs_header_nritems(path->nodes[0])) {\n\t\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\t\tif (ret < 0)\n\t\t\t\t\tbreak;\n\t\t\t\tif (ret > 0) {\n\t\t\t\t\tret = 0;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tret = 0;\n\t\t\t}\n\t\t}\n\n\t\tl = path->nodes[0];\n\t\tslot = path->slots[0];\n\n\t\tbtrfs_item_key_to_cpu(l, &found_key, slot);\n\n\t\tif (found_key.objectid != scrub_dev->devid)\n\t\t\tbreak;\n\n\t\tif (found_key.type != BTRFS_DEV_EXTENT_KEY)\n\t\t\tbreak;\n\n\t\tif (found_key.offset >= end)\n\t\t\tbreak;\n\n\t\tif (found_key.offset < key.offset)\n\t\t\tbreak;\n\n\t\tdev_extent = btrfs_item_ptr(l, slot, struct btrfs_dev_extent);\n\t\tdev_extent_len = btrfs_dev_extent_length(l, dev_extent);\n\n\t\tif (found_key.offset + dev_extent_len <= start)\n\t\t\tgoto skip;\n\n\t\tchunk_offset = btrfs_dev_extent_chunk_offset(l, dev_extent);\n\n\t\t \n\t\tcache = btrfs_lookup_block_group(fs_info, chunk_offset);\n\n\t\t \n\t\tif (!cache)\n\t\t\tgoto skip;\n\n\t\tASSERT(cache->start <= chunk_offset);\n\t\t \n\t\tif (cache->start < chunk_offset) {\n\t\t\tbtrfs_put_block_group(cache);\n\t\t\tgoto skip;\n\t\t}\n\n\t\tif (sctx->is_dev_replace && btrfs_is_zoned(fs_info)) {\n\t\t\tif (!test_bit(BLOCK_GROUP_FLAG_TO_COPY, &cache->runtime_flags)) {\n\t\t\t\tbtrfs_put_block_group(cache);\n\t\t\t\tgoto skip;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tspin_lock(&cache->lock);\n\t\tif (test_bit(BLOCK_GROUP_FLAG_REMOVED, &cache->runtime_flags)) {\n\t\t\tspin_unlock(&cache->lock);\n\t\t\tbtrfs_put_block_group(cache);\n\t\t\tgoto skip;\n\t\t}\n\t\tbtrfs_freeze_block_group(cache);\n\t\tspin_unlock(&cache->lock);\n\n\t\t \n\t\tscrub_pause_on(fs_info);\n\n\t\t \n\t\tret = btrfs_inc_block_group_ro(cache, sctx->is_dev_replace);\n\t\tif (!ret && sctx->is_dev_replace) {\n\t\t\tret = finish_extent_writes_for_zoned(root, cache);\n\t\t\tif (ret) {\n\t\t\t\tbtrfs_dec_block_group_ro(cache);\n\t\t\t\tscrub_pause_off(fs_info);\n\t\t\t\tbtrfs_put_block_group(cache);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (ret == 0) {\n\t\t\tro_set = 1;\n\t\t} else if (ret == -ENOSPC && !sctx->is_dev_replace &&\n\t\t\t   !(cache->flags & BTRFS_BLOCK_GROUP_RAID56_MASK)) {\n\t\t\t \n\t\t\tro_set = 0;\n\t\t} else if (ret == -ETXTBSY) {\n\t\t\tbtrfs_warn(fs_info,\n\t\t   \"skipping scrub of block group %llu due to active swapfile\",\n\t\t\t\t   cache->start);\n\t\t\tscrub_pause_off(fs_info);\n\t\t\tret = 0;\n\t\t\tgoto skip_unfreeze;\n\t\t} else {\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t\t   \"failed setting block group ro: %d\", ret);\n\t\t\tbtrfs_unfreeze_block_group(cache);\n\t\t\tbtrfs_put_block_group(cache);\n\t\t\tscrub_pause_off(fs_info);\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (sctx->is_dev_replace) {\n\t\t\tbtrfs_wait_nocow_writers(cache);\n\t\t\tbtrfs_wait_ordered_roots(fs_info, U64_MAX, cache->start,\n\t\t\t\t\tcache->length);\n\t\t}\n\n\t\tscrub_pause_off(fs_info);\n\t\tdown_write(&dev_replace->rwsem);\n\t\tdev_replace->cursor_right = found_key.offset + dev_extent_len;\n\t\tdev_replace->cursor_left = found_key.offset;\n\t\tdev_replace->item_needs_writeback = 1;\n\t\tup_write(&dev_replace->rwsem);\n\n\t\tret = scrub_chunk(sctx, cache, scrub_dev, found_key.offset,\n\t\t\t\t  dev_extent_len);\n\t\tif (sctx->is_dev_replace &&\n\t\t    !btrfs_finish_block_group_to_copy(dev_replace->srcdev,\n\t\t\t\t\t\t      cache, found_key.offset))\n\t\t\tro_set = 0;\n\n\t\tdown_write(&dev_replace->rwsem);\n\t\tdev_replace->cursor_left = dev_replace->cursor_right;\n\t\tdev_replace->item_needs_writeback = 1;\n\t\tup_write(&dev_replace->rwsem);\n\n\t\tif (ro_set)\n\t\t\tbtrfs_dec_block_group_ro(cache);\n\n\t\t \n\t\tspin_lock(&cache->lock);\n\t\tif (!test_bit(BLOCK_GROUP_FLAG_REMOVED, &cache->runtime_flags) &&\n\t\t    !cache->ro && cache->reserved == 0 && cache->used == 0) {\n\t\t\tspin_unlock(&cache->lock);\n\t\t\tif (btrfs_test_opt(fs_info, DISCARD_ASYNC))\n\t\t\t\tbtrfs_discard_queue_work(&fs_info->discard_ctl,\n\t\t\t\t\t\t\t cache);\n\t\t\telse\n\t\t\t\tbtrfs_mark_bg_unused(cache);\n\t\t} else {\n\t\t\tspin_unlock(&cache->lock);\n\t\t}\nskip_unfreeze:\n\t\tbtrfs_unfreeze_block_group(cache);\n\t\tbtrfs_put_block_group(cache);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tif (sctx->is_dev_replace &&\n\t\t    atomic64_read(&dev_replace->num_write_errors) > 0) {\n\t\t\tret = -EIO;\n\t\t\tbreak;\n\t\t}\n\t\tif (sctx->stat.malloc_errors > 0) {\n\t\t\tret = -ENOMEM;\n\t\t\tbreak;\n\t\t}\nskip:\n\t\tkey.offset = found_key.offset + dev_extent_len;\n\t\tbtrfs_release_path(path);\n\t}\n\n\tbtrfs_free_path(path);\n\n\treturn ret;\n}\n\nstatic int scrub_one_super(struct scrub_ctx *sctx, struct btrfs_device *dev,\n\t\t\t   struct page *page, u64 physical, u64 generation)\n{\n\tstruct btrfs_fs_info *fs_info = sctx->fs_info;\n\tstruct bio_vec bvec;\n\tstruct bio bio;\n\tstruct btrfs_super_block *sb = page_address(page);\n\tint ret;\n\n\tbio_init(&bio, dev->bdev, &bvec, 1, REQ_OP_READ);\n\tbio.bi_iter.bi_sector = physical >> SECTOR_SHIFT;\n\t__bio_add_page(&bio, page, BTRFS_SUPER_INFO_SIZE, 0);\n\tret = submit_bio_wait(&bio);\n\tbio_uninit(&bio);\n\n\tif (ret < 0)\n\t\treturn ret;\n\tret = btrfs_check_super_csum(fs_info, sb);\n\tif (ret != 0) {\n\t\tbtrfs_err_rl(fs_info,\n\t\t\t\"super block at physical %llu devid %llu has bad csum\",\n\t\t\tphysical, dev->devid);\n\t\treturn -EIO;\n\t}\n\tif (btrfs_super_generation(sb) != generation) {\n\t\tbtrfs_err_rl(fs_info,\n\"super block at physical %llu devid %llu has bad generation %llu expect %llu\",\n\t\t\t     physical, dev->devid,\n\t\t\t     btrfs_super_generation(sb), generation);\n\t\treturn -EUCLEAN;\n\t}\n\n\treturn btrfs_validate_super(fs_info, sb, -1);\n}\n\nstatic noinline_for_stack int scrub_supers(struct scrub_ctx *sctx,\n\t\t\t\t\t   struct btrfs_device *scrub_dev)\n{\n\tint\ti;\n\tu64\tbytenr;\n\tu64\tgen;\n\tint ret = 0;\n\tstruct page *page;\n\tstruct btrfs_fs_info *fs_info = sctx->fs_info;\n\n\tif (BTRFS_FS_ERROR(fs_info))\n\t\treturn -EROFS;\n\n\tpage = alloc_page(GFP_KERNEL);\n\tif (!page) {\n\t\tspin_lock(&sctx->stat_lock);\n\t\tsctx->stat.malloc_errors++;\n\t\tspin_unlock(&sctx->stat_lock);\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tif (scrub_dev->fs_devices != fs_info->fs_devices)\n\t\tgen = scrub_dev->generation;\n\telse\n\t\tgen = fs_info->last_trans_committed;\n\n\tfor (i = 0; i < BTRFS_SUPER_MIRROR_MAX; i++) {\n\t\tbytenr = btrfs_sb_offset(i);\n\t\tif (bytenr + BTRFS_SUPER_INFO_SIZE >\n\t\t    scrub_dev->commit_total_bytes)\n\t\t\tbreak;\n\t\tif (!btrfs_check_super_location(scrub_dev, bytenr))\n\t\t\tcontinue;\n\n\t\tret = scrub_one_super(sctx, scrub_dev, page, bytenr, gen);\n\t\tif (ret) {\n\t\t\tspin_lock(&sctx->stat_lock);\n\t\t\tsctx->stat.super_errors++;\n\t\t\tspin_unlock(&sctx->stat_lock);\n\t\t}\n\t}\n\t__free_page(page);\n\treturn 0;\n}\n\nstatic void scrub_workers_put(struct btrfs_fs_info *fs_info)\n{\n\tif (refcount_dec_and_mutex_lock(&fs_info->scrub_workers_refcnt,\n\t\t\t\t\t&fs_info->scrub_lock)) {\n\t\tstruct workqueue_struct *scrub_workers = fs_info->scrub_workers;\n\n\t\tfs_info->scrub_workers = NULL;\n\t\tmutex_unlock(&fs_info->scrub_lock);\n\n\t\tif (scrub_workers)\n\t\t\tdestroy_workqueue(scrub_workers);\n\t}\n}\n\n \nstatic noinline_for_stack int scrub_workers_get(struct btrfs_fs_info *fs_info)\n{\n\tstruct workqueue_struct *scrub_workers = NULL;\n\tunsigned int flags = WQ_FREEZABLE | WQ_UNBOUND;\n\tint max_active = fs_info->thread_pool_size;\n\tint ret = -ENOMEM;\n\n\tif (refcount_inc_not_zero(&fs_info->scrub_workers_refcnt))\n\t\treturn 0;\n\n\tscrub_workers = alloc_workqueue(\"btrfs-scrub\", flags, max_active);\n\tif (!scrub_workers)\n\t\treturn -ENOMEM;\n\n\tmutex_lock(&fs_info->scrub_lock);\n\tif (refcount_read(&fs_info->scrub_workers_refcnt) == 0) {\n\t\tASSERT(fs_info->scrub_workers == NULL);\n\t\tfs_info->scrub_workers = scrub_workers;\n\t\trefcount_set(&fs_info->scrub_workers_refcnt, 1);\n\t\tmutex_unlock(&fs_info->scrub_lock);\n\t\treturn 0;\n\t}\n\t \n\trefcount_inc(&fs_info->scrub_workers_refcnt);\n\tmutex_unlock(&fs_info->scrub_lock);\n\n\tret = 0;\n\n\tdestroy_workqueue(scrub_workers);\n\treturn ret;\n}\n\nint btrfs_scrub_dev(struct btrfs_fs_info *fs_info, u64 devid, u64 start,\n\t\t    u64 end, struct btrfs_scrub_progress *progress,\n\t\t    int readonly, int is_dev_replace)\n{\n\tstruct btrfs_dev_lookup_args args = { .devid = devid };\n\tstruct scrub_ctx *sctx;\n\tint ret;\n\tstruct btrfs_device *dev;\n\tunsigned int nofs_flag;\n\tbool need_commit = false;\n\n\tif (btrfs_fs_closing(fs_info))\n\t\treturn -EAGAIN;\n\n\t \n\tASSERT(fs_info->nodesize <= BTRFS_STRIPE_LEN);\n\n\t \n\tASSERT(fs_info->nodesize <=\n\t       SCRUB_MAX_SECTORS_PER_BLOCK << fs_info->sectorsize_bits);\n\n\t \n\tsctx = scrub_setup_ctx(fs_info, is_dev_replace);\n\tif (IS_ERR(sctx))\n\t\treturn PTR_ERR(sctx);\n\n\tret = scrub_workers_get(fs_info);\n\tif (ret)\n\t\tgoto out_free_ctx;\n\n\tmutex_lock(&fs_info->fs_devices->device_list_mutex);\n\tdev = btrfs_find_device(fs_info->fs_devices, &args);\n\tif (!dev || (test_bit(BTRFS_DEV_STATE_MISSING, &dev->dev_state) &&\n\t\t     !is_dev_replace)) {\n\t\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\t\tret = -ENODEV;\n\t\tgoto out;\n\t}\n\n\tif (!is_dev_replace && !readonly &&\n\t    !test_bit(BTRFS_DEV_STATE_WRITEABLE, &dev->dev_state)) {\n\t\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\t\tbtrfs_err_in_rcu(fs_info,\n\t\t\t\"scrub on devid %llu: filesystem on %s is not writable\",\n\t\t\t\t devid, btrfs_dev_name(dev));\n\t\tret = -EROFS;\n\t\tgoto out;\n\t}\n\n\tmutex_lock(&fs_info->scrub_lock);\n\tif (!test_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &dev->dev_state) ||\n\t    test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &dev->dev_state)) {\n\t\tmutex_unlock(&fs_info->scrub_lock);\n\t\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\t\tret = -EIO;\n\t\tgoto out;\n\t}\n\n\tdown_read(&fs_info->dev_replace.rwsem);\n\tif (dev->scrub_ctx ||\n\t    (!is_dev_replace &&\n\t     btrfs_dev_replace_is_ongoing(&fs_info->dev_replace))) {\n\t\tup_read(&fs_info->dev_replace.rwsem);\n\t\tmutex_unlock(&fs_info->scrub_lock);\n\t\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\t\tret = -EINPROGRESS;\n\t\tgoto out;\n\t}\n\tup_read(&fs_info->dev_replace.rwsem);\n\n\tsctx->readonly = readonly;\n\tdev->scrub_ctx = sctx;\n\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\n\t \n\t__scrub_blocked_if_needed(fs_info);\n\tatomic_inc(&fs_info->scrubs_running);\n\tmutex_unlock(&fs_info->scrub_lock);\n\n\t \n\tnofs_flag = memalloc_nofs_save();\n\tif (!is_dev_replace) {\n\t\tu64 old_super_errors;\n\n\t\tspin_lock(&sctx->stat_lock);\n\t\told_super_errors = sctx->stat.super_errors;\n\t\tspin_unlock(&sctx->stat_lock);\n\n\t\tbtrfs_info(fs_info, \"scrub: started on devid %llu\", devid);\n\t\t \n\t\tmutex_lock(&fs_info->fs_devices->device_list_mutex);\n\t\tret = scrub_supers(sctx, dev);\n\t\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\n\t\tspin_lock(&sctx->stat_lock);\n\t\t \n\t\tif (sctx->stat.super_errors > old_super_errors && !sctx->readonly)\n\t\t\tneed_commit = true;\n\t\tspin_unlock(&sctx->stat_lock);\n\t}\n\n\tif (!ret)\n\t\tret = scrub_enumerate_chunks(sctx, dev, start, end);\n\tmemalloc_nofs_restore(nofs_flag);\n\n\tatomic_dec(&fs_info->scrubs_running);\n\twake_up(&fs_info->scrub_pause_wait);\n\n\tif (progress)\n\t\tmemcpy(progress, &sctx->stat, sizeof(*progress));\n\n\tif (!is_dev_replace)\n\t\tbtrfs_info(fs_info, \"scrub: %s on devid %llu with status: %d\",\n\t\t\tret ? \"not finished\" : \"finished\", devid, ret);\n\n\tmutex_lock(&fs_info->scrub_lock);\n\tdev->scrub_ctx = NULL;\n\tmutex_unlock(&fs_info->scrub_lock);\n\n\tscrub_workers_put(fs_info);\n\tscrub_put_ctx(sctx);\n\n\t \n\tif (need_commit) {\n\t\tstruct btrfs_trans_handle *trans;\n\n\t\ttrans = btrfs_start_transaction(fs_info->tree_root, 0);\n\t\tif (IS_ERR(trans)) {\n\t\t\tret = PTR_ERR(trans);\n\t\t\tbtrfs_err(fs_info,\n\t\"scrub: failed to start transaction to fix super block errors: %d\", ret);\n\t\t\treturn ret;\n\t\t}\n\t\tret = btrfs_commit_transaction(trans);\n\t\tif (ret < 0)\n\t\t\tbtrfs_err(fs_info,\n\t\"scrub: failed to commit transaction to fix super block errors: %d\", ret);\n\t}\n\treturn ret;\nout:\n\tscrub_workers_put(fs_info);\nout_free_ctx:\n\tscrub_free_ctx(sctx);\n\n\treturn ret;\n}\n\nvoid btrfs_scrub_pause(struct btrfs_fs_info *fs_info)\n{\n\tmutex_lock(&fs_info->scrub_lock);\n\tatomic_inc(&fs_info->scrub_pause_req);\n\twhile (atomic_read(&fs_info->scrubs_paused) !=\n\t       atomic_read(&fs_info->scrubs_running)) {\n\t\tmutex_unlock(&fs_info->scrub_lock);\n\t\twait_event(fs_info->scrub_pause_wait,\n\t\t\t   atomic_read(&fs_info->scrubs_paused) ==\n\t\t\t   atomic_read(&fs_info->scrubs_running));\n\t\tmutex_lock(&fs_info->scrub_lock);\n\t}\n\tmutex_unlock(&fs_info->scrub_lock);\n}\n\nvoid btrfs_scrub_continue(struct btrfs_fs_info *fs_info)\n{\n\tatomic_dec(&fs_info->scrub_pause_req);\n\twake_up(&fs_info->scrub_pause_wait);\n}\n\nint btrfs_scrub_cancel(struct btrfs_fs_info *fs_info)\n{\n\tmutex_lock(&fs_info->scrub_lock);\n\tif (!atomic_read(&fs_info->scrubs_running)) {\n\t\tmutex_unlock(&fs_info->scrub_lock);\n\t\treturn -ENOTCONN;\n\t}\n\n\tatomic_inc(&fs_info->scrub_cancel_req);\n\twhile (atomic_read(&fs_info->scrubs_running)) {\n\t\tmutex_unlock(&fs_info->scrub_lock);\n\t\twait_event(fs_info->scrub_pause_wait,\n\t\t\t   atomic_read(&fs_info->scrubs_running) == 0);\n\t\tmutex_lock(&fs_info->scrub_lock);\n\t}\n\tatomic_dec(&fs_info->scrub_cancel_req);\n\tmutex_unlock(&fs_info->scrub_lock);\n\n\treturn 0;\n}\n\nint btrfs_scrub_cancel_dev(struct btrfs_device *dev)\n{\n\tstruct btrfs_fs_info *fs_info = dev->fs_info;\n\tstruct scrub_ctx *sctx;\n\n\tmutex_lock(&fs_info->scrub_lock);\n\tsctx = dev->scrub_ctx;\n\tif (!sctx) {\n\t\tmutex_unlock(&fs_info->scrub_lock);\n\t\treturn -ENOTCONN;\n\t}\n\tatomic_inc(&sctx->cancel_req);\n\twhile (dev->scrub_ctx) {\n\t\tmutex_unlock(&fs_info->scrub_lock);\n\t\twait_event(fs_info->scrub_pause_wait,\n\t\t\t   dev->scrub_ctx == NULL);\n\t\tmutex_lock(&fs_info->scrub_lock);\n\t}\n\tmutex_unlock(&fs_info->scrub_lock);\n\n\treturn 0;\n}\n\nint btrfs_scrub_progress(struct btrfs_fs_info *fs_info, u64 devid,\n\t\t\t struct btrfs_scrub_progress *progress)\n{\n\tstruct btrfs_dev_lookup_args args = { .devid = devid };\n\tstruct btrfs_device *dev;\n\tstruct scrub_ctx *sctx = NULL;\n\n\tmutex_lock(&fs_info->fs_devices->device_list_mutex);\n\tdev = btrfs_find_device(fs_info->fs_devices, &args);\n\tif (dev)\n\t\tsctx = dev->scrub_ctx;\n\tif (sctx)\n\t\tmemcpy(progress, &sctx->stat, sizeof(*progress));\n\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\n\treturn dev ? (sctx ? 0 : -ENOTCONN) : -ENODEV;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}