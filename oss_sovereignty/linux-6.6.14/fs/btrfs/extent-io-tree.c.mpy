{
  "module_name": "extent-io-tree.c",
  "hash_id": "9d51b39303206793a612d278605098e174a5fecf89aacea1c73ec74d3635b567",
  "original_prompt": "Ingested from linux-6.6.14/fs/btrfs/extent-io-tree.c",
  "human_readable_source": "\n\n#include <linux/slab.h>\n#include <trace/events/btrfs.h>\n#include \"messages.h\"\n#include \"ctree.h\"\n#include \"extent-io-tree.h\"\n#include \"btrfs_inode.h\"\n#include \"misc.h\"\n\nstatic struct kmem_cache *extent_state_cache;\n\nstatic inline bool extent_state_in_tree(const struct extent_state *state)\n{\n\treturn !RB_EMPTY_NODE(&state->rb_node);\n}\n\n#ifdef CONFIG_BTRFS_DEBUG\nstatic LIST_HEAD(states);\nstatic DEFINE_SPINLOCK(leak_lock);\n\nstatic inline void btrfs_leak_debug_add_state(struct extent_state *state)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&leak_lock, flags);\n\tlist_add(&state->leak_list, &states);\n\tspin_unlock_irqrestore(&leak_lock, flags);\n}\n\nstatic inline void btrfs_leak_debug_del_state(struct extent_state *state)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&leak_lock, flags);\n\tlist_del(&state->leak_list);\n\tspin_unlock_irqrestore(&leak_lock, flags);\n}\n\nstatic inline void btrfs_extent_state_leak_debug_check(void)\n{\n\tstruct extent_state *state;\n\n\twhile (!list_empty(&states)) {\n\t\tstate = list_entry(states.next, struct extent_state, leak_list);\n\t\tpr_err(\"BTRFS: state leak: start %llu end %llu state %u in tree %d refs %d\\n\",\n\t\t       state->start, state->end, state->state,\n\t\t       extent_state_in_tree(state),\n\t\t       refcount_read(&state->refs));\n\t\tlist_del(&state->leak_list);\n\t\tkmem_cache_free(extent_state_cache, state);\n\t}\n}\n\n#define btrfs_debug_check_extent_io_range(tree, start, end)\t\t\\\n\t__btrfs_debug_check_extent_io_range(__func__, (tree), (start), (end))\nstatic inline void __btrfs_debug_check_extent_io_range(const char *caller,\n\t\t\t\t\t\t       struct extent_io_tree *tree,\n\t\t\t\t\t\t       u64 start, u64 end)\n{\n\tstruct btrfs_inode *inode = tree->inode;\n\tu64 isize;\n\n\tif (!inode)\n\t\treturn;\n\n\tisize = i_size_read(&inode->vfs_inode);\n\tif (end >= PAGE_SIZE && (end % 2) == 0 && end != isize - 1) {\n\t\tbtrfs_debug_rl(inode->root->fs_info,\n\t\t    \"%s: ino %llu isize %llu odd range [%llu,%llu]\",\n\t\t\tcaller, btrfs_ino(inode), isize, start, end);\n\t}\n}\n#else\n#define btrfs_leak_debug_add_state(state)\t\tdo {} while (0)\n#define btrfs_leak_debug_del_state(state)\t\tdo {} while (0)\n#define btrfs_extent_state_leak_debug_check()\t\tdo {} while (0)\n#define btrfs_debug_check_extent_io_range(c, s, e)\tdo {} while (0)\n#endif\n\n \nstatic struct lock_class_key file_extent_tree_class;\n\nstruct tree_entry {\n\tu64 start;\n\tu64 end;\n\tstruct rb_node rb_node;\n};\n\nvoid extent_io_tree_init(struct btrfs_fs_info *fs_info,\n\t\t\t struct extent_io_tree *tree, unsigned int owner)\n{\n\ttree->fs_info = fs_info;\n\ttree->state = RB_ROOT;\n\tspin_lock_init(&tree->lock);\n\ttree->inode = NULL;\n\ttree->owner = owner;\n\tif (owner == IO_TREE_INODE_FILE_EXTENT)\n\t\tlockdep_set_class(&tree->lock, &file_extent_tree_class);\n}\n\nvoid extent_io_tree_release(struct extent_io_tree *tree)\n{\n\tspin_lock(&tree->lock);\n\t \n\tsmp_mb();\n\twhile (!RB_EMPTY_ROOT(&tree->state)) {\n\t\tstruct rb_node *node;\n\t\tstruct extent_state *state;\n\n\t\tnode = rb_first(&tree->state);\n\t\tstate = rb_entry(node, struct extent_state, rb_node);\n\t\trb_erase(&state->rb_node, &tree->state);\n\t\tRB_CLEAR_NODE(&state->rb_node);\n\t\t \n\t\tASSERT(!waitqueue_active(&state->wq));\n\t\tfree_extent_state(state);\n\n\t\tcond_resched_lock(&tree->lock);\n\t}\n\tspin_unlock(&tree->lock);\n}\n\nstatic struct extent_state *alloc_extent_state(gfp_t mask)\n{\n\tstruct extent_state *state;\n\n\t \n\tmask &= ~(__GFP_DMA32|__GFP_HIGHMEM);\n\tstate = kmem_cache_alloc(extent_state_cache, mask);\n\tif (!state)\n\t\treturn state;\n\tstate->state = 0;\n\tRB_CLEAR_NODE(&state->rb_node);\n\tbtrfs_leak_debug_add_state(state);\n\trefcount_set(&state->refs, 1);\n\tinit_waitqueue_head(&state->wq);\n\ttrace_alloc_extent_state(state, mask, _RET_IP_);\n\treturn state;\n}\n\nstatic struct extent_state *alloc_extent_state_atomic(struct extent_state *prealloc)\n{\n\tif (!prealloc)\n\t\tprealloc = alloc_extent_state(GFP_ATOMIC);\n\n\treturn prealloc;\n}\n\nvoid free_extent_state(struct extent_state *state)\n{\n\tif (!state)\n\t\treturn;\n\tif (refcount_dec_and_test(&state->refs)) {\n\t\tWARN_ON(extent_state_in_tree(state));\n\t\tbtrfs_leak_debug_del_state(state);\n\t\ttrace_free_extent_state(state, _RET_IP_);\n\t\tkmem_cache_free(extent_state_cache, state);\n\t}\n}\n\nstatic int add_extent_changeset(struct extent_state *state, u32 bits,\n\t\t\t\t struct extent_changeset *changeset,\n\t\t\t\t int set)\n{\n\tint ret;\n\n\tif (!changeset)\n\t\treturn 0;\n\tif (set && (state->state & bits) == bits)\n\t\treturn 0;\n\tif (!set && (state->state & bits) == 0)\n\t\treturn 0;\n\tchangeset->bytes_changed += state->end - state->start + 1;\n\tret = ulist_add(&changeset->range_changed, state->start, state->end,\n\t\t\tGFP_ATOMIC);\n\treturn ret;\n}\n\nstatic inline struct extent_state *next_state(struct extent_state *state)\n{\n\tstruct rb_node *next = rb_next(&state->rb_node);\n\n\tif (next)\n\t\treturn rb_entry(next, struct extent_state, rb_node);\n\telse\n\t\treturn NULL;\n}\n\nstatic inline struct extent_state *prev_state(struct extent_state *state)\n{\n\tstruct rb_node *next = rb_prev(&state->rb_node);\n\n\tif (next)\n\t\treturn rb_entry(next, struct extent_state, rb_node);\n\telse\n\t\treturn NULL;\n}\n\n \nstatic inline struct extent_state *tree_search_for_insert(struct extent_io_tree *tree,\n\t\t\t\t\t\t\t  u64 offset,\n\t\t\t\t\t\t\t  struct rb_node ***node_ret,\n\t\t\t\t\t\t\t  struct rb_node **parent_ret)\n{\n\tstruct rb_root *root = &tree->state;\n\tstruct rb_node **node = &root->rb_node;\n\tstruct rb_node *prev = NULL;\n\tstruct extent_state *entry = NULL;\n\n\twhile (*node) {\n\t\tprev = *node;\n\t\tentry = rb_entry(prev, struct extent_state, rb_node);\n\n\t\tif (offset < entry->start)\n\t\t\tnode = &(*node)->rb_left;\n\t\telse if (offset > entry->end)\n\t\t\tnode = &(*node)->rb_right;\n\t\telse\n\t\t\treturn entry;\n\t}\n\n\tif (node_ret)\n\t\t*node_ret = node;\n\tif (parent_ret)\n\t\t*parent_ret = prev;\n\n\t \n\twhile (entry && offset > entry->end)\n\t\tentry = next_state(entry);\n\n\treturn entry;\n}\n\n \nstatic struct extent_state *tree_search_prev_next(struct extent_io_tree *tree,\n\t\t\t\t\t\t  u64 offset,\n\t\t\t\t\t\t  struct extent_state **prev_ret,\n\t\t\t\t\t\t  struct extent_state **next_ret)\n{\n\tstruct rb_root *root = &tree->state;\n\tstruct rb_node **node = &root->rb_node;\n\tstruct extent_state *orig_prev;\n\tstruct extent_state *entry = NULL;\n\n\tASSERT(prev_ret);\n\tASSERT(next_ret);\n\n\twhile (*node) {\n\t\tentry = rb_entry(*node, struct extent_state, rb_node);\n\n\t\tif (offset < entry->start)\n\t\t\tnode = &(*node)->rb_left;\n\t\telse if (offset > entry->end)\n\t\t\tnode = &(*node)->rb_right;\n\t\telse\n\t\t\treturn entry;\n\t}\n\n\torig_prev = entry;\n\twhile (entry && offset > entry->end)\n\t\tentry = next_state(entry);\n\t*next_ret = entry;\n\tentry = orig_prev;\n\n\twhile (entry && offset < entry->start)\n\t\tentry = prev_state(entry);\n\t*prev_ret = entry;\n\n\treturn NULL;\n}\n\n \nstatic inline struct extent_state *tree_search(struct extent_io_tree *tree, u64 offset)\n{\n\treturn tree_search_for_insert(tree, offset, NULL, NULL);\n}\n\nstatic void extent_io_tree_panic(struct extent_io_tree *tree, int err)\n{\n\tbtrfs_panic(tree->fs_info, err,\n\t\"locking error: extent tree was modified by another thread while locked\");\n}\n\n \nstatic void merge_state(struct extent_io_tree *tree, struct extent_state *state)\n{\n\tstruct extent_state *other;\n\n\tif (state->state & (EXTENT_LOCKED | EXTENT_BOUNDARY))\n\t\treturn;\n\n\tother = prev_state(state);\n\tif (other && other->end == state->start - 1 &&\n\t    other->state == state->state) {\n\t\tif (tree->inode)\n\t\t\tbtrfs_merge_delalloc_extent(tree->inode, state, other);\n\t\tstate->start = other->start;\n\t\trb_erase(&other->rb_node, &tree->state);\n\t\tRB_CLEAR_NODE(&other->rb_node);\n\t\tfree_extent_state(other);\n\t}\n\tother = next_state(state);\n\tif (other && other->start == state->end + 1 &&\n\t    other->state == state->state) {\n\t\tif (tree->inode)\n\t\t\tbtrfs_merge_delalloc_extent(tree->inode, state, other);\n\t\tstate->end = other->end;\n\t\trb_erase(&other->rb_node, &tree->state);\n\t\tRB_CLEAR_NODE(&other->rb_node);\n\t\tfree_extent_state(other);\n\t}\n}\n\nstatic void set_state_bits(struct extent_io_tree *tree,\n\t\t\t   struct extent_state *state,\n\t\t\t   u32 bits, struct extent_changeset *changeset)\n{\n\tu32 bits_to_set = bits & ~EXTENT_CTLBITS;\n\tint ret;\n\n\tif (tree->inode)\n\t\tbtrfs_set_delalloc_extent(tree->inode, state, bits);\n\n\tret = add_extent_changeset(state, bits_to_set, changeset, 1);\n\tBUG_ON(ret < 0);\n\tstate->state |= bits_to_set;\n}\n\n \nstatic int insert_state(struct extent_io_tree *tree,\n\t\t\tstruct extent_state *state,\n\t\t\tu32 bits, struct extent_changeset *changeset)\n{\n\tstruct rb_node **node;\n\tstruct rb_node *parent = NULL;\n\tconst u64 end = state->end;\n\n\tset_state_bits(tree, state, bits, changeset);\n\n\tnode = &tree->state.rb_node;\n\twhile (*node) {\n\t\tstruct extent_state *entry;\n\n\t\tparent = *node;\n\t\tentry = rb_entry(parent, struct extent_state, rb_node);\n\n\t\tif (end < entry->start) {\n\t\t\tnode = &(*node)->rb_left;\n\t\t} else if (end > entry->end) {\n\t\t\tnode = &(*node)->rb_right;\n\t\t} else {\n\t\t\tbtrfs_err(tree->fs_info,\n\t\t\t       \"found node %llu %llu on insert of %llu %llu\",\n\t\t\t       entry->start, entry->end, state->start, end);\n\t\t\treturn -EEXIST;\n\t\t}\n\t}\n\n\trb_link_node(&state->rb_node, parent, node);\n\trb_insert_color(&state->rb_node, &tree->state);\n\n\tmerge_state(tree, state);\n\treturn 0;\n}\n\n \nstatic void insert_state_fast(struct extent_io_tree *tree,\n\t\t\t      struct extent_state *state, struct rb_node **node,\n\t\t\t      struct rb_node *parent, unsigned bits,\n\t\t\t      struct extent_changeset *changeset)\n{\n\tset_state_bits(tree, state, bits, changeset);\n\trb_link_node(&state->rb_node, parent, node);\n\trb_insert_color(&state->rb_node, &tree->state);\n\tmerge_state(tree, state);\n}\n\n \nstatic int split_state(struct extent_io_tree *tree, struct extent_state *orig,\n\t\t       struct extent_state *prealloc, u64 split)\n{\n\tstruct rb_node *parent = NULL;\n\tstruct rb_node **node;\n\n\tif (tree->inode)\n\t\tbtrfs_split_delalloc_extent(tree->inode, orig, split);\n\n\tprealloc->start = orig->start;\n\tprealloc->end = split - 1;\n\tprealloc->state = orig->state;\n\torig->start = split;\n\n\tparent = &orig->rb_node;\n\tnode = &parent;\n\twhile (*node) {\n\t\tstruct extent_state *entry;\n\n\t\tparent = *node;\n\t\tentry = rb_entry(parent, struct extent_state, rb_node);\n\n\t\tif (prealloc->end < entry->start) {\n\t\t\tnode = &(*node)->rb_left;\n\t\t} else if (prealloc->end > entry->end) {\n\t\t\tnode = &(*node)->rb_right;\n\t\t} else {\n\t\t\tfree_extent_state(prealloc);\n\t\t\treturn -EEXIST;\n\t\t}\n\t}\n\n\trb_link_node(&prealloc->rb_node, parent, node);\n\trb_insert_color(&prealloc->rb_node, &tree->state);\n\n\treturn 0;\n}\n\n \nstatic struct extent_state *clear_state_bit(struct extent_io_tree *tree,\n\t\t\t\t\t    struct extent_state *state,\n\t\t\t\t\t    u32 bits, int wake,\n\t\t\t\t\t    struct extent_changeset *changeset)\n{\n\tstruct extent_state *next;\n\tu32 bits_to_clear = bits & ~EXTENT_CTLBITS;\n\tint ret;\n\n\tif (tree->inode)\n\t\tbtrfs_clear_delalloc_extent(tree->inode, state, bits);\n\n\tret = add_extent_changeset(state, bits_to_clear, changeset, 0);\n\tBUG_ON(ret < 0);\n\tstate->state &= ~bits_to_clear;\n\tif (wake)\n\t\twake_up(&state->wq);\n\tif (state->state == 0) {\n\t\tnext = next_state(state);\n\t\tif (extent_state_in_tree(state)) {\n\t\t\trb_erase(&state->rb_node, &tree->state);\n\t\t\tRB_CLEAR_NODE(&state->rb_node);\n\t\t\tfree_extent_state(state);\n\t\t} else {\n\t\t\tWARN_ON(1);\n\t\t}\n\t} else {\n\t\tmerge_state(tree, state);\n\t\tnext = next_state(state);\n\t}\n\treturn next;\n}\n\n \nstatic void set_gfp_mask_from_bits(u32 *bits, gfp_t *mask)\n{\n\t*mask = (*bits & EXTENT_NOWAIT ? GFP_NOWAIT : GFP_NOFS);\n\t*bits &= EXTENT_NOWAIT - 1;\n}\n\n \nint __clear_extent_bit(struct extent_io_tree *tree, u64 start, u64 end,\n\t\t       u32 bits, struct extent_state **cached_state,\n\t\t       struct extent_changeset *changeset)\n{\n\tstruct extent_state *state;\n\tstruct extent_state *cached;\n\tstruct extent_state *prealloc = NULL;\n\tu64 last_end;\n\tint err;\n\tint clear = 0;\n\tint wake;\n\tint delete = (bits & EXTENT_CLEAR_ALL_BITS);\n\tgfp_t mask;\n\n\tset_gfp_mask_from_bits(&bits, &mask);\n\tbtrfs_debug_check_extent_io_range(tree, start, end);\n\ttrace_btrfs_clear_extent_bit(tree, start, end - start + 1, bits);\n\n\tif (delete)\n\t\tbits |= ~EXTENT_CTLBITS;\n\n\tif (bits & EXTENT_DELALLOC)\n\t\tbits |= EXTENT_NORESERVE;\n\n\twake = (bits & EXTENT_LOCKED) ? 1 : 0;\n\tif (bits & (EXTENT_LOCKED | EXTENT_BOUNDARY))\n\t\tclear = 1;\nagain:\n\tif (!prealloc) {\n\t\t \n\t\tprealloc = alloc_extent_state(mask);\n\t}\n\n\tspin_lock(&tree->lock);\n\tif (cached_state) {\n\t\tcached = *cached_state;\n\n\t\tif (clear) {\n\t\t\t*cached_state = NULL;\n\t\t\tcached_state = NULL;\n\t\t}\n\n\t\tif (cached && extent_state_in_tree(cached) &&\n\t\t    cached->start <= start && cached->end > start) {\n\t\t\tif (clear)\n\t\t\t\trefcount_dec(&cached->refs);\n\t\t\tstate = cached;\n\t\t\tgoto hit_next;\n\t\t}\n\t\tif (clear)\n\t\t\tfree_extent_state(cached);\n\t}\n\n\t \n\tstate = tree_search(tree, start);\n\tif (!state)\n\t\tgoto out;\nhit_next:\n\tif (state->start > end)\n\t\tgoto out;\n\tWARN_ON(state->end < start);\n\tlast_end = state->end;\n\n\t \n\tif (!(state->state & bits)) {\n\t\tstate = next_state(state);\n\t\tgoto next;\n\t}\n\n\t \n\n\tif (state->start < start) {\n\t\tprealloc = alloc_extent_state_atomic(prealloc);\n\t\tif (!prealloc)\n\t\t\tgoto search_again;\n\t\terr = split_state(tree, state, prealloc, start);\n\t\tif (err)\n\t\t\textent_io_tree_panic(tree, err);\n\n\t\tprealloc = NULL;\n\t\tif (err)\n\t\t\tgoto out;\n\t\tif (state->end <= end) {\n\t\t\tstate = clear_state_bit(tree, state, bits, wake, changeset);\n\t\t\tgoto next;\n\t\t}\n\t\tgoto search_again;\n\t}\n\t \n\tif (state->start <= end && state->end > end) {\n\t\tprealloc = alloc_extent_state_atomic(prealloc);\n\t\tif (!prealloc)\n\t\t\tgoto search_again;\n\t\terr = split_state(tree, state, prealloc, end + 1);\n\t\tif (err)\n\t\t\textent_io_tree_panic(tree, err);\n\n\t\tif (wake)\n\t\t\twake_up(&state->wq);\n\n\t\tclear_state_bit(tree, prealloc, bits, wake, changeset);\n\n\t\tprealloc = NULL;\n\t\tgoto out;\n\t}\n\n\tstate = clear_state_bit(tree, state, bits, wake, changeset);\nnext:\n\tif (last_end == (u64)-1)\n\t\tgoto out;\n\tstart = last_end + 1;\n\tif (start <= end && state && !need_resched())\n\t\tgoto hit_next;\n\nsearch_again:\n\tif (start > end)\n\t\tgoto out;\n\tspin_unlock(&tree->lock);\n\tif (gfpflags_allow_blocking(mask))\n\t\tcond_resched();\n\tgoto again;\n\nout:\n\tspin_unlock(&tree->lock);\n\tif (prealloc)\n\t\tfree_extent_state(prealloc);\n\n\treturn 0;\n\n}\n\nstatic void wait_on_state(struct extent_io_tree *tree,\n\t\t\t  struct extent_state *state)\n\t\t__releases(tree->lock)\n\t\t__acquires(tree->lock)\n{\n\tDEFINE_WAIT(wait);\n\tprepare_to_wait(&state->wq, &wait, TASK_UNINTERRUPTIBLE);\n\tspin_unlock(&tree->lock);\n\tschedule();\n\tspin_lock(&tree->lock);\n\tfinish_wait(&state->wq, &wait);\n}\n\n \nvoid wait_extent_bit(struct extent_io_tree *tree, u64 start, u64 end, u32 bits,\n\t\t     struct extent_state **cached_state)\n{\n\tstruct extent_state *state;\n\n\tbtrfs_debug_check_extent_io_range(tree, start, end);\n\n\tspin_lock(&tree->lock);\nagain:\n\t \n\tif (cached_state && *cached_state) {\n\t\tstate = *cached_state;\n\t\tif (extent_state_in_tree(state) &&\n\t\t    state->start <= start && start < state->end)\n\t\t\tgoto process_node;\n\t}\n\twhile (1) {\n\t\t \n\t\tstate = tree_search(tree, start);\nprocess_node:\n\t\tif (!state)\n\t\t\tbreak;\n\t\tif (state->start > end)\n\t\t\tgoto out;\n\n\t\tif (state->state & bits) {\n\t\t\tstart = state->start;\n\t\t\trefcount_inc(&state->refs);\n\t\t\twait_on_state(tree, state);\n\t\t\tfree_extent_state(state);\n\t\t\tgoto again;\n\t\t}\n\t\tstart = state->end + 1;\n\n\t\tif (start > end)\n\t\t\tbreak;\n\n\t\tif (!cond_resched_lock(&tree->lock)) {\n\t\t\tstate = next_state(state);\n\t\t\tgoto process_node;\n\t\t}\n\t}\nout:\n\t \n\tif (cached_state && *cached_state) {\n\t\tstate = *cached_state;\n\t\t*cached_state = NULL;\n\t\tfree_extent_state(state);\n\t}\n\tspin_unlock(&tree->lock);\n}\n\nstatic void cache_state_if_flags(struct extent_state *state,\n\t\t\t\t struct extent_state **cached_ptr,\n\t\t\t\t unsigned flags)\n{\n\tif (cached_ptr && !(*cached_ptr)) {\n\t\tif (!flags || (state->state & flags)) {\n\t\t\t*cached_ptr = state;\n\t\t\trefcount_inc(&state->refs);\n\t\t}\n\t}\n}\n\nstatic void cache_state(struct extent_state *state,\n\t\t\tstruct extent_state **cached_ptr)\n{\n\treturn cache_state_if_flags(state, cached_ptr,\n\t\t\t\t    EXTENT_LOCKED | EXTENT_BOUNDARY);\n}\n\n \nstatic struct extent_state *find_first_extent_bit_state(struct extent_io_tree *tree,\n\t\t\t\t\t\t\tu64 start, u32 bits)\n{\n\tstruct extent_state *state;\n\n\t \n\tstate = tree_search(tree, start);\n\twhile (state) {\n\t\tif (state->end >= start && (state->state & bits))\n\t\t\treturn state;\n\t\tstate = next_state(state);\n\t}\n\treturn NULL;\n}\n\n \nbool find_first_extent_bit(struct extent_io_tree *tree, u64 start,\n\t\t\t   u64 *start_ret, u64 *end_ret, u32 bits,\n\t\t\t   struct extent_state **cached_state)\n{\n\tstruct extent_state *state;\n\tbool ret = false;\n\n\tspin_lock(&tree->lock);\n\tif (cached_state && *cached_state) {\n\t\tstate = *cached_state;\n\t\tif (state->end == start - 1 && extent_state_in_tree(state)) {\n\t\t\twhile ((state = next_state(state)) != NULL) {\n\t\t\t\tif (state->state & bits)\n\t\t\t\t\tgoto got_it;\n\t\t\t}\n\t\t\tfree_extent_state(*cached_state);\n\t\t\t*cached_state = NULL;\n\t\t\tgoto out;\n\t\t}\n\t\tfree_extent_state(*cached_state);\n\t\t*cached_state = NULL;\n\t}\n\n\tstate = find_first_extent_bit_state(tree, start, bits);\ngot_it:\n\tif (state) {\n\t\tcache_state_if_flags(state, cached_state, 0);\n\t\t*start_ret = state->start;\n\t\t*end_ret = state->end;\n\t\tret = true;\n\t}\nout:\n\tspin_unlock(&tree->lock);\n\treturn ret;\n}\n\n \nint find_contiguous_extent_bit(struct extent_io_tree *tree, u64 start,\n\t\t\t       u64 *start_ret, u64 *end_ret, u32 bits)\n{\n\tstruct extent_state *state;\n\tint ret = 1;\n\n\tspin_lock(&tree->lock);\n\tstate = find_first_extent_bit_state(tree, start, bits);\n\tif (state) {\n\t\t*start_ret = state->start;\n\t\t*end_ret = state->end;\n\t\twhile ((state = next_state(state)) != NULL) {\n\t\t\tif (state->start > (*end_ret + 1))\n\t\t\t\tbreak;\n\t\t\t*end_ret = state->end;\n\t\t}\n\t\tret = 0;\n\t}\n\tspin_unlock(&tree->lock);\n\treturn ret;\n}\n\n \nbool btrfs_find_delalloc_range(struct extent_io_tree *tree, u64 *start,\n\t\t\t       u64 *end, u64 max_bytes,\n\t\t\t       struct extent_state **cached_state)\n{\n\tstruct extent_state *state;\n\tu64 cur_start = *start;\n\tbool found = false;\n\tu64 total_bytes = 0;\n\n\tspin_lock(&tree->lock);\n\n\t \n\tstate = tree_search(tree, cur_start);\n\tif (!state) {\n\t\t*end = (u64)-1;\n\t\tgoto out;\n\t}\n\n\twhile (state) {\n\t\tif (found && (state->start != cur_start ||\n\t\t\t      (state->state & EXTENT_BOUNDARY))) {\n\t\t\tgoto out;\n\t\t}\n\t\tif (!(state->state & EXTENT_DELALLOC)) {\n\t\t\tif (!found)\n\t\t\t\t*end = state->end;\n\t\t\tgoto out;\n\t\t}\n\t\tif (!found) {\n\t\t\t*start = state->start;\n\t\t\t*cached_state = state;\n\t\t\trefcount_inc(&state->refs);\n\t\t}\n\t\tfound = true;\n\t\t*end = state->end;\n\t\tcur_start = state->end + 1;\n\t\ttotal_bytes += state->end - state->start + 1;\n\t\tif (total_bytes >= max_bytes)\n\t\t\tbreak;\n\t\tstate = next_state(state);\n\t}\nout:\n\tspin_unlock(&tree->lock);\n\treturn found;\n}\n\n \nstatic int __set_extent_bit(struct extent_io_tree *tree, u64 start, u64 end,\n\t\t\t    u32 bits, u64 *failed_start,\n\t\t\t    struct extent_state **failed_state,\n\t\t\t    struct extent_state **cached_state,\n\t\t\t    struct extent_changeset *changeset)\n{\n\tstruct extent_state *state;\n\tstruct extent_state *prealloc = NULL;\n\tstruct rb_node **p = NULL;\n\tstruct rb_node *parent = NULL;\n\tint err = 0;\n\tu64 last_start;\n\tu64 last_end;\n\tu32 exclusive_bits = (bits & EXTENT_LOCKED);\n\tgfp_t mask;\n\n\tset_gfp_mask_from_bits(&bits, &mask);\n\tbtrfs_debug_check_extent_io_range(tree, start, end);\n\ttrace_btrfs_set_extent_bit(tree, start, end - start + 1, bits);\n\n\tif (exclusive_bits)\n\t\tASSERT(failed_start);\n\telse\n\t\tASSERT(failed_start == NULL && failed_state == NULL);\nagain:\n\tif (!prealloc) {\n\t\t \n\t\tprealloc = alloc_extent_state(mask);\n\t}\n\n\tspin_lock(&tree->lock);\n\tif (cached_state && *cached_state) {\n\t\tstate = *cached_state;\n\t\tif (state->start <= start && state->end > start &&\n\t\t    extent_state_in_tree(state))\n\t\t\tgoto hit_next;\n\t}\n\t \n\tstate = tree_search_for_insert(tree, start, &p, &parent);\n\tif (!state) {\n\t\tprealloc = alloc_extent_state_atomic(prealloc);\n\t\tif (!prealloc)\n\t\t\tgoto search_again;\n\t\tprealloc->start = start;\n\t\tprealloc->end = end;\n\t\tinsert_state_fast(tree, prealloc, p, parent, bits, changeset);\n\t\tcache_state(prealloc, cached_state);\n\t\tprealloc = NULL;\n\t\tgoto out;\n\t}\nhit_next:\n\tlast_start = state->start;\n\tlast_end = state->end;\n\n\t \n\tif (state->start == start && state->end <= end) {\n\t\tif (state->state & exclusive_bits) {\n\t\t\t*failed_start = state->start;\n\t\t\tcache_state(state, failed_state);\n\t\t\terr = -EEXIST;\n\t\t\tgoto out;\n\t\t}\n\n\t\tset_state_bits(tree, state, bits, changeset);\n\t\tcache_state(state, cached_state);\n\t\tmerge_state(tree, state);\n\t\tif (last_end == (u64)-1)\n\t\t\tgoto out;\n\t\tstart = last_end + 1;\n\t\tstate = next_state(state);\n\t\tif (start < end && state && state->start == start &&\n\t\t    !need_resched())\n\t\t\tgoto hit_next;\n\t\tgoto search_again;\n\t}\n\n\t \n\tif (state->start < start) {\n\t\tif (state->state & exclusive_bits) {\n\t\t\t*failed_start = start;\n\t\t\tcache_state(state, failed_state);\n\t\t\terr = -EEXIST;\n\t\t\tgoto out;\n\t\t}\n\n\t\t \n\t\tif ((state->state & bits) == bits) {\n\t\t\tstart = state->end + 1;\n\t\t\tcache_state(state, cached_state);\n\t\t\tgoto search_again;\n\t\t}\n\n\t\tprealloc = alloc_extent_state_atomic(prealloc);\n\t\tif (!prealloc)\n\t\t\tgoto search_again;\n\t\terr = split_state(tree, state, prealloc, start);\n\t\tif (err)\n\t\t\textent_io_tree_panic(tree, err);\n\n\t\tprealloc = NULL;\n\t\tif (err)\n\t\t\tgoto out;\n\t\tif (state->end <= end) {\n\t\t\tset_state_bits(tree, state, bits, changeset);\n\t\t\tcache_state(state, cached_state);\n\t\t\tmerge_state(tree, state);\n\t\t\tif (last_end == (u64)-1)\n\t\t\t\tgoto out;\n\t\t\tstart = last_end + 1;\n\t\t\tstate = next_state(state);\n\t\t\tif (start < end && state && state->start == start &&\n\t\t\t    !need_resched())\n\t\t\t\tgoto hit_next;\n\t\t}\n\t\tgoto search_again;\n\t}\n\t \n\tif (state->start > start) {\n\t\tu64 this_end;\n\t\tif (end < last_start)\n\t\t\tthis_end = end;\n\t\telse\n\t\t\tthis_end = last_start - 1;\n\n\t\tprealloc = alloc_extent_state_atomic(prealloc);\n\t\tif (!prealloc)\n\t\t\tgoto search_again;\n\n\t\t \n\t\tprealloc->start = start;\n\t\tprealloc->end = this_end;\n\t\terr = insert_state(tree, prealloc, bits, changeset);\n\t\tif (err)\n\t\t\textent_io_tree_panic(tree, err);\n\n\t\tcache_state(prealloc, cached_state);\n\t\tprealloc = NULL;\n\t\tstart = this_end + 1;\n\t\tgoto search_again;\n\t}\n\t \n\tif (state->start <= end && state->end > end) {\n\t\tif (state->state & exclusive_bits) {\n\t\t\t*failed_start = start;\n\t\t\tcache_state(state, failed_state);\n\t\t\terr = -EEXIST;\n\t\t\tgoto out;\n\t\t}\n\n\t\tprealloc = alloc_extent_state_atomic(prealloc);\n\t\tif (!prealloc)\n\t\t\tgoto search_again;\n\t\terr = split_state(tree, state, prealloc, end + 1);\n\t\tif (err)\n\t\t\textent_io_tree_panic(tree, err);\n\n\t\tset_state_bits(tree, prealloc, bits, changeset);\n\t\tcache_state(prealloc, cached_state);\n\t\tmerge_state(tree, prealloc);\n\t\tprealloc = NULL;\n\t\tgoto out;\n\t}\n\nsearch_again:\n\tif (start > end)\n\t\tgoto out;\n\tspin_unlock(&tree->lock);\n\tif (gfpflags_allow_blocking(mask))\n\t\tcond_resched();\n\tgoto again;\n\nout:\n\tspin_unlock(&tree->lock);\n\tif (prealloc)\n\t\tfree_extent_state(prealloc);\n\n\treturn err;\n\n}\n\nint set_extent_bit(struct extent_io_tree *tree, u64 start, u64 end,\n\t\t   u32 bits, struct extent_state **cached_state)\n{\n\treturn __set_extent_bit(tree, start, end, bits, NULL, NULL,\n\t\t\t\tcached_state, NULL);\n}\n\n \nint convert_extent_bit(struct extent_io_tree *tree, u64 start, u64 end,\n\t\t       u32 bits, u32 clear_bits,\n\t\t       struct extent_state **cached_state)\n{\n\tstruct extent_state *state;\n\tstruct extent_state *prealloc = NULL;\n\tstruct rb_node **p = NULL;\n\tstruct rb_node *parent = NULL;\n\tint err = 0;\n\tu64 last_start;\n\tu64 last_end;\n\tbool first_iteration = true;\n\n\tbtrfs_debug_check_extent_io_range(tree, start, end);\n\ttrace_btrfs_convert_extent_bit(tree, start, end - start + 1, bits,\n\t\t\t\t       clear_bits);\n\nagain:\n\tif (!prealloc) {\n\t\t \n\t\tprealloc = alloc_extent_state(GFP_NOFS);\n\t\tif (!prealloc && !first_iteration)\n\t\t\treturn -ENOMEM;\n\t}\n\n\tspin_lock(&tree->lock);\n\tif (cached_state && *cached_state) {\n\t\tstate = *cached_state;\n\t\tif (state->start <= start && state->end > start &&\n\t\t    extent_state_in_tree(state))\n\t\t\tgoto hit_next;\n\t}\n\n\t \n\tstate = tree_search_for_insert(tree, start, &p, &parent);\n\tif (!state) {\n\t\tprealloc = alloc_extent_state_atomic(prealloc);\n\t\tif (!prealloc) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tprealloc->start = start;\n\t\tprealloc->end = end;\n\t\tinsert_state_fast(tree, prealloc, p, parent, bits, NULL);\n\t\tcache_state(prealloc, cached_state);\n\t\tprealloc = NULL;\n\t\tgoto out;\n\t}\nhit_next:\n\tlast_start = state->start;\n\tlast_end = state->end;\n\n\t \n\tif (state->start == start && state->end <= end) {\n\t\tset_state_bits(tree, state, bits, NULL);\n\t\tcache_state(state, cached_state);\n\t\tstate = clear_state_bit(tree, state, clear_bits, 0, NULL);\n\t\tif (last_end == (u64)-1)\n\t\t\tgoto out;\n\t\tstart = last_end + 1;\n\t\tif (start < end && state && state->start == start &&\n\t\t    !need_resched())\n\t\t\tgoto hit_next;\n\t\tgoto search_again;\n\t}\n\n\t \n\tif (state->start < start) {\n\t\tprealloc = alloc_extent_state_atomic(prealloc);\n\t\tif (!prealloc) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\terr = split_state(tree, state, prealloc, start);\n\t\tif (err)\n\t\t\textent_io_tree_panic(tree, err);\n\t\tprealloc = NULL;\n\t\tif (err)\n\t\t\tgoto out;\n\t\tif (state->end <= end) {\n\t\t\tset_state_bits(tree, state, bits, NULL);\n\t\t\tcache_state(state, cached_state);\n\t\t\tstate = clear_state_bit(tree, state, clear_bits, 0, NULL);\n\t\t\tif (last_end == (u64)-1)\n\t\t\t\tgoto out;\n\t\t\tstart = last_end + 1;\n\t\t\tif (start < end && state && state->start == start &&\n\t\t\t    !need_resched())\n\t\t\t\tgoto hit_next;\n\t\t}\n\t\tgoto search_again;\n\t}\n\t \n\tif (state->start > start) {\n\t\tu64 this_end;\n\t\tif (end < last_start)\n\t\t\tthis_end = end;\n\t\telse\n\t\t\tthis_end = last_start - 1;\n\n\t\tprealloc = alloc_extent_state_atomic(prealloc);\n\t\tif (!prealloc) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\n\t\t \n\t\tprealloc->start = start;\n\t\tprealloc->end = this_end;\n\t\terr = insert_state(tree, prealloc, bits, NULL);\n\t\tif (err)\n\t\t\textent_io_tree_panic(tree, err);\n\t\tcache_state(prealloc, cached_state);\n\t\tprealloc = NULL;\n\t\tstart = this_end + 1;\n\t\tgoto search_again;\n\t}\n\t \n\tif (state->start <= end && state->end > end) {\n\t\tprealloc = alloc_extent_state_atomic(prealloc);\n\t\tif (!prealloc) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\n\t\terr = split_state(tree, state, prealloc, end + 1);\n\t\tif (err)\n\t\t\textent_io_tree_panic(tree, err);\n\n\t\tset_state_bits(tree, prealloc, bits, NULL);\n\t\tcache_state(prealloc, cached_state);\n\t\tclear_state_bit(tree, prealloc, clear_bits, 0, NULL);\n\t\tprealloc = NULL;\n\t\tgoto out;\n\t}\n\nsearch_again:\n\tif (start > end)\n\t\tgoto out;\n\tspin_unlock(&tree->lock);\n\tcond_resched();\n\tfirst_iteration = false;\n\tgoto again;\n\nout:\n\tspin_unlock(&tree->lock);\n\tif (prealloc)\n\t\tfree_extent_state(prealloc);\n\n\treturn err;\n}\n\n \nvoid find_first_clear_extent_bit(struct extent_io_tree *tree, u64 start,\n\t\t\t\t u64 *start_ret, u64 *end_ret, u32 bits)\n{\n\tstruct extent_state *state;\n\tstruct extent_state *prev = NULL, *next = NULL;\n\n\tspin_lock(&tree->lock);\n\n\t \n\twhile (1) {\n\t\tstate = tree_search_prev_next(tree, start, &prev, &next);\n\t\tif (!state && !next && !prev) {\n\t\t\t \n\t\t\t*start_ret = 0;\n\t\t\t*end_ret = -1;\n\t\t\tgoto out;\n\t\t} else if (!state && !next) {\n\t\t\t \n\t\t\t*start_ret = prev->end + 1;\n\t\t\t*end_ret = -1;\n\t\t\tgoto out;\n\t\t} else if (!state) {\n\t\t\tstate = next;\n\t\t}\n\n\t\t \n\t\tif (in_range(start, state->start, state->end - state->start + 1)) {\n\t\t\tif (state->state & bits) {\n\t\t\t\t \n\t\t\t\tstart = state->end + 1;\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\t*start_ret = state->start;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} else {\n\t\t\t \n\t\t\tif (prev)\n\t\t\t\t*start_ret = prev->end + 1;\n\t\t\telse\n\t\t\t\t*start_ret = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\twhile (state) {\n\t\tif (state->end >= start && !(state->state & bits)) {\n\t\t\t*end_ret = state->end;\n\t\t} else {\n\t\t\t*end_ret = state->start - 1;\n\t\t\tbreak;\n\t\t}\n\t\tstate = next_state(state);\n\t}\nout:\n\tspin_unlock(&tree->lock);\n}\n\n \nu64 count_range_bits(struct extent_io_tree *tree,\n\t\t     u64 *start, u64 search_end, u64 max_bytes,\n\t\t     u32 bits, int contig,\n\t\t     struct extent_state **cached_state)\n{\n\tstruct extent_state *state = NULL;\n\tstruct extent_state *cached;\n\tu64 cur_start = *start;\n\tu64 total_bytes = 0;\n\tu64 last = 0;\n\tint found = 0;\n\n\tif (WARN_ON(search_end < cur_start))\n\t\treturn 0;\n\n\tspin_lock(&tree->lock);\n\n\tif (!cached_state || !*cached_state)\n\t\tgoto search;\n\n\tcached = *cached_state;\n\n\tif (!extent_state_in_tree(cached))\n\t\tgoto search;\n\n\tif (cached->start <= cur_start && cur_start <= cached->end) {\n\t\tstate = cached;\n\t} else if (cached->start > cur_start) {\n\t\tstruct extent_state *prev;\n\n\t\t \n\t\tprev = prev_state(cached);\n\t\tif (!prev)\n\t\t\tstate = cached;\n\t\telse if (prev->start <= cur_start && cur_start <= prev->end)\n\t\t\tstate = prev;\n\t}\n\n\t \nsearch:\n\tif (!state)\n\t\tstate = tree_search(tree, cur_start);\n\n\twhile (state) {\n\t\tif (state->start > search_end)\n\t\t\tbreak;\n\t\tif (contig && found && state->start > last + 1)\n\t\t\tbreak;\n\t\tif (state->end >= cur_start && (state->state & bits) == bits) {\n\t\t\ttotal_bytes += min(search_end, state->end) + 1 -\n\t\t\t\t       max(cur_start, state->start);\n\t\t\tif (total_bytes >= max_bytes)\n\t\t\t\tbreak;\n\t\t\tif (!found) {\n\t\t\t\t*start = max(cur_start, state->start);\n\t\t\t\tfound = 1;\n\t\t\t}\n\t\t\tlast = state->end;\n\t\t} else if (contig && found) {\n\t\t\tbreak;\n\t\t}\n\t\tstate = next_state(state);\n\t}\n\n\tif (cached_state) {\n\t\tfree_extent_state(*cached_state);\n\t\t*cached_state = state;\n\t\tif (state)\n\t\t\trefcount_inc(&state->refs);\n\t}\n\n\tspin_unlock(&tree->lock);\n\n\treturn total_bytes;\n}\n\n \nint test_range_bit(struct extent_io_tree *tree, u64 start, u64 end,\n\t\t   u32 bits, int filled, struct extent_state *cached)\n{\n\tstruct extent_state *state = NULL;\n\tint bitset = 0;\n\n\tspin_lock(&tree->lock);\n\tif (cached && extent_state_in_tree(cached) && cached->start <= start &&\n\t    cached->end > start)\n\t\tstate = cached;\n\telse\n\t\tstate = tree_search(tree, start);\n\twhile (state && start <= end) {\n\t\tif (filled && state->start > start) {\n\t\t\tbitset = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (state->start > end)\n\t\t\tbreak;\n\n\t\tif (state->state & bits) {\n\t\t\tbitset = 1;\n\t\t\tif (!filled)\n\t\t\t\tbreak;\n\t\t} else if (filled) {\n\t\t\tbitset = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (state->end == (u64)-1)\n\t\t\tbreak;\n\n\t\tstart = state->end + 1;\n\t\tif (start > end)\n\t\t\tbreak;\n\t\tstate = next_state(state);\n\t}\n\n\t \n\tif (filled && !state)\n\t\tbitset = 0;\n\tspin_unlock(&tree->lock);\n\treturn bitset;\n}\n\n \nint set_record_extent_bits(struct extent_io_tree *tree, u64 start, u64 end,\n\t\t\t   u32 bits, struct extent_changeset *changeset)\n{\n\t \n\tASSERT(!(bits & EXTENT_LOCKED));\n\n\treturn __set_extent_bit(tree, start, end, bits, NULL, NULL, NULL, changeset);\n}\n\nint clear_record_extent_bits(struct extent_io_tree *tree, u64 start, u64 end,\n\t\t\t     u32 bits, struct extent_changeset *changeset)\n{\n\t \n\tASSERT(!(bits & EXTENT_LOCKED));\n\n\treturn __clear_extent_bit(tree, start, end, bits, NULL, changeset);\n}\n\nint try_lock_extent(struct extent_io_tree *tree, u64 start, u64 end,\n\t\t    struct extent_state **cached)\n{\n\tint err;\n\tu64 failed_start;\n\n\terr = __set_extent_bit(tree, start, end, EXTENT_LOCKED, &failed_start,\n\t\t\t       NULL, cached, NULL);\n\tif (err == -EEXIST) {\n\t\tif (failed_start > start)\n\t\t\tclear_extent_bit(tree, start, failed_start - 1,\n\t\t\t\t\t EXTENT_LOCKED, cached);\n\t\treturn 0;\n\t}\n\treturn 1;\n}\n\n \nint lock_extent(struct extent_io_tree *tree, u64 start, u64 end,\n\t\tstruct extent_state **cached_state)\n{\n\tstruct extent_state *failed_state = NULL;\n\tint err;\n\tu64 failed_start;\n\n\terr = __set_extent_bit(tree, start, end, EXTENT_LOCKED, &failed_start,\n\t\t\t       &failed_state, cached_state, NULL);\n\twhile (err == -EEXIST) {\n\t\tif (failed_start != start)\n\t\t\tclear_extent_bit(tree, start, failed_start - 1,\n\t\t\t\t\t EXTENT_LOCKED, cached_state);\n\n\t\twait_extent_bit(tree, failed_start, end, EXTENT_LOCKED,\n\t\t\t\t&failed_state);\n\t\terr = __set_extent_bit(tree, start, end, EXTENT_LOCKED,\n\t\t\t\t       &failed_start, &failed_state,\n\t\t\t\t       cached_state, NULL);\n\t}\n\treturn err;\n}\n\nvoid __cold extent_state_free_cachep(void)\n{\n\tbtrfs_extent_state_leak_debug_check();\n\tkmem_cache_destroy(extent_state_cache);\n}\n\nint __init extent_state_init_cachep(void)\n{\n\textent_state_cache = kmem_cache_create(\"btrfs_extent_state\",\n\t\t\tsizeof(struct extent_state), 0,\n\t\t\tSLAB_MEM_SPREAD, NULL);\n\tif (!extent_state_cache)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}