{
  "module_name": "compression.c",
  "hash_id": "9a76d677ec6a90fb7f696021961e3b0c74433c919fe81f4c731a9eb039dbfb8b",
  "original_prompt": "Ingested from linux-6.6.14/fs/btrfs/compression.c",
  "human_readable_source": "\n \n\n#include <linux/kernel.h>\n#include <linux/bio.h>\n#include <linux/file.h>\n#include <linux/fs.h>\n#include <linux/pagemap.h>\n#include <linux/pagevec.h>\n#include <linux/highmem.h>\n#include <linux/kthread.h>\n#include <linux/time.h>\n#include <linux/init.h>\n#include <linux/string.h>\n#include <linux/backing-dev.h>\n#include <linux/writeback.h>\n#include <linux/psi.h>\n#include <linux/slab.h>\n#include <linux/sched/mm.h>\n#include <linux/log2.h>\n#include <crypto/hash.h>\n#include \"misc.h\"\n#include \"ctree.h\"\n#include \"fs.h\"\n#include \"disk-io.h\"\n#include \"transaction.h\"\n#include \"btrfs_inode.h\"\n#include \"bio.h\"\n#include \"ordered-data.h\"\n#include \"compression.h\"\n#include \"extent_io.h\"\n#include \"extent_map.h\"\n#include \"subpage.h\"\n#include \"zoned.h\"\n#include \"file-item.h\"\n#include \"super.h\"\n\nstatic struct bio_set btrfs_compressed_bioset;\n\nstatic const char* const btrfs_compress_types[] = { \"\", \"zlib\", \"lzo\", \"zstd\" };\n\nconst char* btrfs_compress_type2str(enum btrfs_compression_type type)\n{\n\tswitch (type) {\n\tcase BTRFS_COMPRESS_ZLIB:\n\tcase BTRFS_COMPRESS_LZO:\n\tcase BTRFS_COMPRESS_ZSTD:\n\tcase BTRFS_COMPRESS_NONE:\n\t\treturn btrfs_compress_types[type];\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn NULL;\n}\n\nstatic inline struct compressed_bio *to_compressed_bio(struct btrfs_bio *bbio)\n{\n\treturn container_of(bbio, struct compressed_bio, bbio);\n}\n\nstatic struct compressed_bio *alloc_compressed_bio(struct btrfs_inode *inode,\n\t\t\t\t\t\t   u64 start, blk_opf_t op,\n\t\t\t\t\t\t   btrfs_bio_end_io_t end_io)\n{\n\tstruct btrfs_bio *bbio;\n\n\tbbio = btrfs_bio(bio_alloc_bioset(NULL, BTRFS_MAX_COMPRESSED_PAGES, op,\n\t\t\t\t\t  GFP_NOFS, &btrfs_compressed_bioset));\n\tbtrfs_bio_init(bbio, inode->root->fs_info, end_io, NULL);\n\tbbio->inode = inode;\n\tbbio->file_offset = start;\n\treturn to_compressed_bio(bbio);\n}\n\nbool btrfs_compress_is_valid_type(const char *str, size_t len)\n{\n\tint i;\n\n\tfor (i = 1; i < ARRAY_SIZE(btrfs_compress_types); i++) {\n\t\tsize_t comp_len = strlen(btrfs_compress_types[i]);\n\n\t\tif (len < comp_len)\n\t\t\tcontinue;\n\n\t\tif (!strncmp(btrfs_compress_types[i], str, comp_len))\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic int compression_compress_pages(int type, struct list_head *ws,\n               struct address_space *mapping, u64 start, struct page **pages,\n               unsigned long *out_pages, unsigned long *total_in,\n               unsigned long *total_out)\n{\n\tswitch (type) {\n\tcase BTRFS_COMPRESS_ZLIB:\n\t\treturn zlib_compress_pages(ws, mapping, start, pages,\n\t\t\t\tout_pages, total_in, total_out);\n\tcase BTRFS_COMPRESS_LZO:\n\t\treturn lzo_compress_pages(ws, mapping, start, pages,\n\t\t\t\tout_pages, total_in, total_out);\n\tcase BTRFS_COMPRESS_ZSTD:\n\t\treturn zstd_compress_pages(ws, mapping, start, pages,\n\t\t\t\tout_pages, total_in, total_out);\n\tcase BTRFS_COMPRESS_NONE:\n\tdefault:\n\t\t \n\t\t*out_pages = 0;\n\t\treturn -E2BIG;\n\t}\n}\n\nstatic int compression_decompress_bio(struct list_head *ws,\n\t\t\t\t      struct compressed_bio *cb)\n{\n\tswitch (cb->compress_type) {\n\tcase BTRFS_COMPRESS_ZLIB: return zlib_decompress_bio(ws, cb);\n\tcase BTRFS_COMPRESS_LZO:  return lzo_decompress_bio(ws, cb);\n\tcase BTRFS_COMPRESS_ZSTD: return zstd_decompress_bio(ws, cb);\n\tcase BTRFS_COMPRESS_NONE:\n\tdefault:\n\t\t \n\t\tBUG();\n\t}\n}\n\nstatic int compression_decompress(int type, struct list_head *ws,\n               const u8 *data_in, struct page *dest_page,\n               unsigned long start_byte, size_t srclen, size_t destlen)\n{\n\tswitch (type) {\n\tcase BTRFS_COMPRESS_ZLIB: return zlib_decompress(ws, data_in, dest_page,\n\t\t\t\t\t\tstart_byte, srclen, destlen);\n\tcase BTRFS_COMPRESS_LZO:  return lzo_decompress(ws, data_in, dest_page,\n\t\t\t\t\t\tstart_byte, srclen, destlen);\n\tcase BTRFS_COMPRESS_ZSTD: return zstd_decompress(ws, data_in, dest_page,\n\t\t\t\t\t\tstart_byte, srclen, destlen);\n\tcase BTRFS_COMPRESS_NONE:\n\tdefault:\n\t\t \n\t\tBUG();\n\t}\n}\n\nstatic void btrfs_free_compressed_pages(struct compressed_bio *cb)\n{\n\tfor (unsigned int i = 0; i < cb->nr_pages; i++)\n\t\tput_page(cb->compressed_pages[i]);\n\tkfree(cb->compressed_pages);\n}\n\nstatic int btrfs_decompress_bio(struct compressed_bio *cb);\n\nstatic void end_compressed_bio_read(struct btrfs_bio *bbio)\n{\n\tstruct compressed_bio *cb = to_compressed_bio(bbio);\n\tblk_status_t status = bbio->bio.bi_status;\n\n\tif (!status)\n\t\tstatus = errno_to_blk_status(btrfs_decompress_bio(cb));\n\n\tbtrfs_free_compressed_pages(cb);\n\tbtrfs_bio_end_io(cb->orig_bbio, status);\n\tbio_put(&bbio->bio);\n}\n\n \nstatic noinline void end_compressed_writeback(const struct compressed_bio *cb)\n{\n\tstruct inode *inode = &cb->bbio.inode->vfs_inode;\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tunsigned long index = cb->start >> PAGE_SHIFT;\n\tunsigned long end_index = (cb->start + cb->len - 1) >> PAGE_SHIFT;\n\tstruct folio_batch fbatch;\n\tconst int errno = blk_status_to_errno(cb->bbio.bio.bi_status);\n\tint i;\n\tint ret;\n\n\tif (errno)\n\t\tmapping_set_error(inode->i_mapping, errno);\n\n\tfolio_batch_init(&fbatch);\n\twhile (index <= end_index) {\n\t\tret = filemap_get_folios(inode->i_mapping, &index, end_index,\n\t\t\t\t&fbatch);\n\n\t\tif (ret == 0)\n\t\t\treturn;\n\n\t\tfor (i = 0; i < ret; i++) {\n\t\t\tstruct folio *folio = fbatch.folios[i];\n\n\t\t\tbtrfs_page_clamp_clear_writeback(fs_info, &folio->page,\n\t\t\t\t\t\t\t cb->start, cb->len);\n\t\t}\n\t\tfolio_batch_release(&fbatch);\n\t}\n\t \n}\n\nstatic void btrfs_finish_compressed_write_work(struct work_struct *work)\n{\n\tstruct compressed_bio *cb =\n\t\tcontainer_of(work, struct compressed_bio, write_end_work);\n\n\tbtrfs_finish_ordered_extent(cb->bbio.ordered, NULL, cb->start, cb->len,\n\t\t\t\t    cb->bbio.bio.bi_status == BLK_STS_OK);\n\n\tif (cb->writeback)\n\t\tend_compressed_writeback(cb);\n\t \n\n\tbtrfs_free_compressed_pages(cb);\n\tbio_put(&cb->bbio.bio);\n}\n\n \nstatic void end_compressed_bio_write(struct btrfs_bio *bbio)\n{\n\tstruct compressed_bio *cb = to_compressed_bio(bbio);\n\tstruct btrfs_fs_info *fs_info = bbio->inode->root->fs_info;\n\n\tqueue_work(fs_info->compressed_write_workers, &cb->write_end_work);\n}\n\nstatic void btrfs_add_compressed_bio_pages(struct compressed_bio *cb)\n{\n\tstruct bio *bio = &cb->bbio.bio;\n\tu32 offset = 0;\n\n\twhile (offset < cb->compressed_len) {\n\t\tu32 len = min_t(u32, cb->compressed_len - offset, PAGE_SIZE);\n\n\t\t \n\t\t__bio_add_page(bio, cb->compressed_pages[offset >> PAGE_SHIFT],\n\t\t\t       len, 0);\n\t\toffset += len;\n\t}\n}\n\n \nvoid btrfs_submit_compressed_write(struct btrfs_ordered_extent *ordered,\n\t\t\t\t   struct page **compressed_pages,\n\t\t\t\t   unsigned int nr_pages,\n\t\t\t\t   blk_opf_t write_flags,\n\t\t\t\t   bool writeback)\n{\n\tstruct btrfs_inode *inode = BTRFS_I(ordered->inode);\n\tstruct btrfs_fs_info *fs_info = inode->root->fs_info;\n\tstruct compressed_bio *cb;\n\n\tASSERT(IS_ALIGNED(ordered->file_offset, fs_info->sectorsize));\n\tASSERT(IS_ALIGNED(ordered->num_bytes, fs_info->sectorsize));\n\n\tcb = alloc_compressed_bio(inode, ordered->file_offset,\n\t\t\t\t  REQ_OP_WRITE | write_flags,\n\t\t\t\t  end_compressed_bio_write);\n\tcb->start = ordered->file_offset;\n\tcb->len = ordered->num_bytes;\n\tcb->compressed_pages = compressed_pages;\n\tcb->compressed_len = ordered->disk_num_bytes;\n\tcb->writeback = writeback;\n\tINIT_WORK(&cb->write_end_work, btrfs_finish_compressed_write_work);\n\tcb->nr_pages = nr_pages;\n\tcb->bbio.bio.bi_iter.bi_sector = ordered->disk_bytenr >> SECTOR_SHIFT;\n\tcb->bbio.ordered = ordered;\n\tbtrfs_add_compressed_bio_pages(cb);\n\n\tbtrfs_submit_bio(&cb->bbio, 0);\n}\n\n \nstatic noinline int add_ra_bio_pages(struct inode *inode,\n\t\t\t\t     u64 compressed_end,\n\t\t\t\t     struct compressed_bio *cb,\n\t\t\t\t     int *memstall, unsigned long *pflags)\n{\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tunsigned long end_index;\n\tstruct bio *orig_bio = &cb->orig_bbio->bio;\n\tu64 cur = cb->orig_bbio->file_offset + orig_bio->bi_iter.bi_size;\n\tu64 isize = i_size_read(inode);\n\tint ret;\n\tstruct page *page;\n\tstruct extent_map *em;\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct extent_map_tree *em_tree;\n\tstruct extent_io_tree *tree;\n\tint sectors_missed = 0;\n\n\tem_tree = &BTRFS_I(inode)->extent_tree;\n\ttree = &BTRFS_I(inode)->io_tree;\n\n\tif (isize == 0)\n\t\treturn 0;\n\n\t \n\tif (btrfs_sb(inode->i_sb)->sectorsize < PAGE_SIZE)\n\t\treturn 0;\n\n\tend_index = (i_size_read(inode) - 1) >> PAGE_SHIFT;\n\n\twhile (cur < compressed_end) {\n\t\tu64 page_end;\n\t\tu64 pg_index = cur >> PAGE_SHIFT;\n\t\tu32 add_size;\n\n\t\tif (pg_index > end_index)\n\t\t\tbreak;\n\n\t\tpage = xa_load(&mapping->i_pages, pg_index);\n\t\tif (page && !xa_is_value(page)) {\n\t\t\tsectors_missed += (PAGE_SIZE - offset_in_page(cur)) >>\n\t\t\t\t\t  fs_info->sectorsize_bits;\n\n\t\t\t \n\t\t\tif (sectors_missed > 4)\n\t\t\t\tbreak;\n\n\t\t\t \n\t\t\tcur = (pg_index << PAGE_SHIFT) + PAGE_SIZE;\n\t\t\tcontinue;\n\t\t}\n\n\t\tpage = __page_cache_alloc(mapping_gfp_constraint(mapping,\n\t\t\t\t\t\t\t\t ~__GFP_FS));\n\t\tif (!page)\n\t\t\tbreak;\n\n\t\tif (add_to_page_cache_lru(page, mapping, pg_index, GFP_NOFS)) {\n\t\t\tput_page(page);\n\t\t\t \n\t\t\tcur = (pg_index << PAGE_SHIFT) + PAGE_SIZE;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!*memstall && PageWorkingset(page)) {\n\t\t\tpsi_memstall_enter(pflags);\n\t\t\t*memstall = 1;\n\t\t}\n\n\t\tret = set_page_extent_mapped(page);\n\t\tif (ret < 0) {\n\t\t\tunlock_page(page);\n\t\t\tput_page(page);\n\t\t\tbreak;\n\t\t}\n\n\t\tpage_end = (pg_index << PAGE_SHIFT) + PAGE_SIZE - 1;\n\t\tlock_extent(tree, cur, page_end, NULL);\n\t\tread_lock(&em_tree->lock);\n\t\tem = lookup_extent_mapping(em_tree, cur, page_end + 1 - cur);\n\t\tread_unlock(&em_tree->lock);\n\n\t\t \n\t\tif (!em || cur < em->start ||\n\t\t    (cur + fs_info->sectorsize > extent_map_end(em)) ||\n\t\t    (em->block_start >> SECTOR_SHIFT) != orig_bio->bi_iter.bi_sector) {\n\t\t\tfree_extent_map(em);\n\t\t\tunlock_extent(tree, cur, page_end, NULL);\n\t\t\tunlock_page(page);\n\t\t\tput_page(page);\n\t\t\tbreak;\n\t\t}\n\t\tfree_extent_map(em);\n\n\t\tif (page->index == end_index) {\n\t\t\tsize_t zero_offset = offset_in_page(isize);\n\n\t\t\tif (zero_offset) {\n\t\t\t\tint zeros;\n\t\t\t\tzeros = PAGE_SIZE - zero_offset;\n\t\t\t\tmemzero_page(page, zero_offset, zeros);\n\t\t\t}\n\t\t}\n\n\t\tadd_size = min(em->start + em->len, page_end + 1) - cur;\n\t\tret = bio_add_page(orig_bio, page, add_size, offset_in_page(cur));\n\t\tif (ret != add_size) {\n\t\t\tunlock_extent(tree, cur, page_end, NULL);\n\t\t\tunlock_page(page);\n\t\t\tput_page(page);\n\t\t\tbreak;\n\t\t}\n\t\t \n\t\tif (fs_info->sectorsize < PAGE_SIZE)\n\t\t\tbtrfs_subpage_start_reader(fs_info, page, cur, add_size);\n\t\tput_page(page);\n\t\tcur += add_size;\n\t}\n\treturn 0;\n}\n\n \nvoid btrfs_submit_compressed_read(struct btrfs_bio *bbio)\n{\n\tstruct btrfs_inode *inode = bbio->inode;\n\tstruct btrfs_fs_info *fs_info = inode->root->fs_info;\n\tstruct extent_map_tree *em_tree = &inode->extent_tree;\n\tstruct compressed_bio *cb;\n\tunsigned int compressed_len;\n\tu64 file_offset = bbio->file_offset;\n\tu64 em_len;\n\tu64 em_start;\n\tstruct extent_map *em;\n\tunsigned long pflags;\n\tint memstall = 0;\n\tblk_status_t ret;\n\tint ret2;\n\n\t \n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, file_offset, fs_info->sectorsize);\n\tread_unlock(&em_tree->lock);\n\tif (!em) {\n\t\tret = BLK_STS_IOERR;\n\t\tgoto out;\n\t}\n\n\tASSERT(em->compress_type != BTRFS_COMPRESS_NONE);\n\tcompressed_len = em->block_len;\n\n\tcb = alloc_compressed_bio(inode, file_offset, REQ_OP_READ,\n\t\t\t\t  end_compressed_bio_read);\n\n\tcb->start = em->orig_start;\n\tem_len = em->len;\n\tem_start = em->start;\n\n\tcb->len = bbio->bio.bi_iter.bi_size;\n\tcb->compressed_len = compressed_len;\n\tcb->compress_type = em->compress_type;\n\tcb->orig_bbio = bbio;\n\n\tfree_extent_map(em);\n\n\tcb->nr_pages = DIV_ROUND_UP(compressed_len, PAGE_SIZE);\n\tcb->compressed_pages = kcalloc(cb->nr_pages, sizeof(struct page *), GFP_NOFS);\n\tif (!cb->compressed_pages) {\n\t\tret = BLK_STS_RESOURCE;\n\t\tgoto out_free_bio;\n\t}\n\n\tret2 = btrfs_alloc_page_array(cb->nr_pages, cb->compressed_pages);\n\tif (ret2) {\n\t\tret = BLK_STS_RESOURCE;\n\t\tgoto out_free_compressed_pages;\n\t}\n\n\tadd_ra_bio_pages(&inode->vfs_inode, em_start + em_len, cb, &memstall,\n\t\t\t &pflags);\n\n\t \n\tcb->len = bbio->bio.bi_iter.bi_size;\n\tcb->bbio.bio.bi_iter.bi_sector = bbio->bio.bi_iter.bi_sector;\n\tbtrfs_add_compressed_bio_pages(cb);\n\n\tif (memstall)\n\t\tpsi_memstall_leave(&pflags);\n\n\tbtrfs_submit_bio(&cb->bbio, 0);\n\treturn;\n\nout_free_compressed_pages:\n\tkfree(cb->compressed_pages);\nout_free_bio:\n\tbio_put(&cb->bbio.bio);\nout:\n\tbtrfs_bio_end_io(bbio, ret);\n}\n\n \n#define SAMPLING_READ_SIZE\t(16)\n#define SAMPLING_INTERVAL\t(256)\n\n \n#define BUCKET_SIZE\t\t(256)\n\n \n#define MAX_SAMPLE_SIZE\t\t(BTRFS_MAX_UNCOMPRESSED *\t\t\\\n\t\t\t\t SAMPLING_READ_SIZE / SAMPLING_INTERVAL)\n\nstruct bucket_item {\n\tu32 count;\n};\n\nstruct heuristic_ws {\n\t \n\tu8 *sample;\n\tu32 sample_size;\n\t \n\tstruct bucket_item *bucket;\n\t \n\tstruct bucket_item *bucket_b;\n\tstruct list_head list;\n};\n\nstatic struct workspace_manager heuristic_wsm;\n\nstatic void free_heuristic_ws(struct list_head *ws)\n{\n\tstruct heuristic_ws *workspace;\n\n\tworkspace = list_entry(ws, struct heuristic_ws, list);\n\n\tkvfree(workspace->sample);\n\tkfree(workspace->bucket);\n\tkfree(workspace->bucket_b);\n\tkfree(workspace);\n}\n\nstatic struct list_head *alloc_heuristic_ws(unsigned int level)\n{\n\tstruct heuristic_ws *ws;\n\n\tws = kzalloc(sizeof(*ws), GFP_KERNEL);\n\tif (!ws)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tws->sample = kvmalloc(MAX_SAMPLE_SIZE, GFP_KERNEL);\n\tif (!ws->sample)\n\t\tgoto fail;\n\n\tws->bucket = kcalloc(BUCKET_SIZE, sizeof(*ws->bucket), GFP_KERNEL);\n\tif (!ws->bucket)\n\t\tgoto fail;\n\n\tws->bucket_b = kcalloc(BUCKET_SIZE, sizeof(*ws->bucket_b), GFP_KERNEL);\n\tif (!ws->bucket_b)\n\t\tgoto fail;\n\n\tINIT_LIST_HEAD(&ws->list);\n\treturn &ws->list;\nfail:\n\tfree_heuristic_ws(&ws->list);\n\treturn ERR_PTR(-ENOMEM);\n}\n\nconst struct btrfs_compress_op btrfs_heuristic_compress = {\n\t.workspace_manager = &heuristic_wsm,\n};\n\nstatic const struct btrfs_compress_op * const btrfs_compress_op[] = {\n\t \n\t&btrfs_heuristic_compress,\n\t&btrfs_zlib_compress,\n\t&btrfs_lzo_compress,\n\t&btrfs_zstd_compress,\n};\n\nstatic struct list_head *alloc_workspace(int type, unsigned int level)\n{\n\tswitch (type) {\n\tcase BTRFS_COMPRESS_NONE: return alloc_heuristic_ws(level);\n\tcase BTRFS_COMPRESS_ZLIB: return zlib_alloc_workspace(level);\n\tcase BTRFS_COMPRESS_LZO:  return lzo_alloc_workspace(level);\n\tcase BTRFS_COMPRESS_ZSTD: return zstd_alloc_workspace(level);\n\tdefault:\n\t\t \n\t\tBUG();\n\t}\n}\n\nstatic void free_workspace(int type, struct list_head *ws)\n{\n\tswitch (type) {\n\tcase BTRFS_COMPRESS_NONE: return free_heuristic_ws(ws);\n\tcase BTRFS_COMPRESS_ZLIB: return zlib_free_workspace(ws);\n\tcase BTRFS_COMPRESS_LZO:  return lzo_free_workspace(ws);\n\tcase BTRFS_COMPRESS_ZSTD: return zstd_free_workspace(ws);\n\tdefault:\n\t\t \n\t\tBUG();\n\t}\n}\n\nstatic void btrfs_init_workspace_manager(int type)\n{\n\tstruct workspace_manager *wsm;\n\tstruct list_head *workspace;\n\n\twsm = btrfs_compress_op[type]->workspace_manager;\n\tINIT_LIST_HEAD(&wsm->idle_ws);\n\tspin_lock_init(&wsm->ws_lock);\n\tatomic_set(&wsm->total_ws, 0);\n\tinit_waitqueue_head(&wsm->ws_wait);\n\n\t \n\tworkspace = alloc_workspace(type, 0);\n\tif (IS_ERR(workspace)) {\n\t\tpr_warn(\n\t\"BTRFS: cannot preallocate compression workspace, will try later\\n\");\n\t} else {\n\t\tatomic_set(&wsm->total_ws, 1);\n\t\twsm->free_ws = 1;\n\t\tlist_add(workspace, &wsm->idle_ws);\n\t}\n}\n\nstatic void btrfs_cleanup_workspace_manager(int type)\n{\n\tstruct workspace_manager *wsman;\n\tstruct list_head *ws;\n\n\twsman = btrfs_compress_op[type]->workspace_manager;\n\twhile (!list_empty(&wsman->idle_ws)) {\n\t\tws = wsman->idle_ws.next;\n\t\tlist_del(ws);\n\t\tfree_workspace(type, ws);\n\t\tatomic_dec(&wsman->total_ws);\n\t}\n}\n\n \nstruct list_head *btrfs_get_workspace(int type, unsigned int level)\n{\n\tstruct workspace_manager *wsm;\n\tstruct list_head *workspace;\n\tint cpus = num_online_cpus();\n\tunsigned nofs_flag;\n\tstruct list_head *idle_ws;\n\tspinlock_t *ws_lock;\n\tatomic_t *total_ws;\n\twait_queue_head_t *ws_wait;\n\tint *free_ws;\n\n\twsm = btrfs_compress_op[type]->workspace_manager;\n\tidle_ws\t = &wsm->idle_ws;\n\tws_lock\t = &wsm->ws_lock;\n\ttotal_ws = &wsm->total_ws;\n\tws_wait\t = &wsm->ws_wait;\n\tfree_ws\t = &wsm->free_ws;\n\nagain:\n\tspin_lock(ws_lock);\n\tif (!list_empty(idle_ws)) {\n\t\tworkspace = idle_ws->next;\n\t\tlist_del(workspace);\n\t\t(*free_ws)--;\n\t\tspin_unlock(ws_lock);\n\t\treturn workspace;\n\n\t}\n\tif (atomic_read(total_ws) > cpus) {\n\t\tDEFINE_WAIT(wait);\n\n\t\tspin_unlock(ws_lock);\n\t\tprepare_to_wait(ws_wait, &wait, TASK_UNINTERRUPTIBLE);\n\t\tif (atomic_read(total_ws) > cpus && !*free_ws)\n\t\t\tschedule();\n\t\tfinish_wait(ws_wait, &wait);\n\t\tgoto again;\n\t}\n\tatomic_inc(total_ws);\n\tspin_unlock(ws_lock);\n\n\t \n\tnofs_flag = memalloc_nofs_save();\n\tworkspace = alloc_workspace(type, level);\n\tmemalloc_nofs_restore(nofs_flag);\n\n\tif (IS_ERR(workspace)) {\n\t\tatomic_dec(total_ws);\n\t\twake_up(ws_wait);\n\n\t\t \n\t\tif (atomic_read(total_ws) == 0) {\n\t\t\tstatic DEFINE_RATELIMIT_STATE(_rs,\n\t\t\t\t\t  60 * HZ,\n\t\t\t\t\t  1);\n\n\t\t\tif (__ratelimit(&_rs)) {\n\t\t\t\tpr_warn(\"BTRFS: no compression workspaces, low memory, retrying\\n\");\n\t\t\t}\n\t\t}\n\t\tgoto again;\n\t}\n\treturn workspace;\n}\n\nstatic struct list_head *get_workspace(int type, int level)\n{\n\tswitch (type) {\n\tcase BTRFS_COMPRESS_NONE: return btrfs_get_workspace(type, level);\n\tcase BTRFS_COMPRESS_ZLIB: return zlib_get_workspace(level);\n\tcase BTRFS_COMPRESS_LZO:  return btrfs_get_workspace(type, level);\n\tcase BTRFS_COMPRESS_ZSTD: return zstd_get_workspace(level);\n\tdefault:\n\t\t \n\t\tBUG();\n\t}\n}\n\n \nvoid btrfs_put_workspace(int type, struct list_head *ws)\n{\n\tstruct workspace_manager *wsm;\n\tstruct list_head *idle_ws;\n\tspinlock_t *ws_lock;\n\tatomic_t *total_ws;\n\twait_queue_head_t *ws_wait;\n\tint *free_ws;\n\n\twsm = btrfs_compress_op[type]->workspace_manager;\n\tidle_ws\t = &wsm->idle_ws;\n\tws_lock\t = &wsm->ws_lock;\n\ttotal_ws = &wsm->total_ws;\n\tws_wait\t = &wsm->ws_wait;\n\tfree_ws\t = &wsm->free_ws;\n\n\tspin_lock(ws_lock);\n\tif (*free_ws <= num_online_cpus()) {\n\t\tlist_add(ws, idle_ws);\n\t\t(*free_ws)++;\n\t\tspin_unlock(ws_lock);\n\t\tgoto wake;\n\t}\n\tspin_unlock(ws_lock);\n\n\tfree_workspace(type, ws);\n\tatomic_dec(total_ws);\nwake:\n\tcond_wake_up(ws_wait);\n}\n\nstatic void put_workspace(int type, struct list_head *ws)\n{\n\tswitch (type) {\n\tcase BTRFS_COMPRESS_NONE: return btrfs_put_workspace(type, ws);\n\tcase BTRFS_COMPRESS_ZLIB: return btrfs_put_workspace(type, ws);\n\tcase BTRFS_COMPRESS_LZO:  return btrfs_put_workspace(type, ws);\n\tcase BTRFS_COMPRESS_ZSTD: return zstd_put_workspace(ws);\n\tdefault:\n\t\t \n\t\tBUG();\n\t}\n}\n\n \nstatic unsigned int btrfs_compress_set_level(int type, unsigned level)\n{\n\tconst struct btrfs_compress_op *ops = btrfs_compress_op[type];\n\n\tif (level == 0)\n\t\tlevel = ops->default_level;\n\telse\n\t\tlevel = min(level, ops->max_level);\n\n\treturn level;\n}\n\n \nint btrfs_compress_pages(unsigned int type_level, struct address_space *mapping,\n\t\t\t u64 start, struct page **pages,\n\t\t\t unsigned long *out_pages,\n\t\t\t unsigned long *total_in,\n\t\t\t unsigned long *total_out)\n{\n\tint type = btrfs_compress_type(type_level);\n\tint level = btrfs_compress_level(type_level);\n\tstruct list_head *workspace;\n\tint ret;\n\n\tlevel = btrfs_compress_set_level(type, level);\n\tworkspace = get_workspace(type, level);\n\tret = compression_compress_pages(type, workspace, mapping, start, pages,\n\t\t\t\t\t out_pages, total_in, total_out);\n\tput_workspace(type, workspace);\n\treturn ret;\n}\n\nstatic int btrfs_decompress_bio(struct compressed_bio *cb)\n{\n\tstruct list_head *workspace;\n\tint ret;\n\tint type = cb->compress_type;\n\n\tworkspace = get_workspace(type, 0);\n\tret = compression_decompress_bio(workspace, cb);\n\tput_workspace(type, workspace);\n\n\tif (!ret)\n\t\tzero_fill_bio(&cb->orig_bbio->bio);\n\treturn ret;\n}\n\n \nint btrfs_decompress(int type, const u8 *data_in, struct page *dest_page,\n\t\t     unsigned long start_byte, size_t srclen, size_t destlen)\n{\n\tstruct list_head *workspace;\n\tint ret;\n\n\tworkspace = get_workspace(type, 0);\n\tret = compression_decompress(type, workspace, data_in, dest_page,\n\t\t\t\t     start_byte, srclen, destlen);\n\tput_workspace(type, workspace);\n\n\treturn ret;\n}\n\nint __init btrfs_init_compress(void)\n{\n\tif (bioset_init(&btrfs_compressed_bioset, BIO_POOL_SIZE,\n\t\t\toffsetof(struct compressed_bio, bbio.bio),\n\t\t\tBIOSET_NEED_BVECS))\n\t\treturn -ENOMEM;\n\tbtrfs_init_workspace_manager(BTRFS_COMPRESS_NONE);\n\tbtrfs_init_workspace_manager(BTRFS_COMPRESS_ZLIB);\n\tbtrfs_init_workspace_manager(BTRFS_COMPRESS_LZO);\n\tzstd_init_workspace_manager();\n\treturn 0;\n}\n\nvoid __cold btrfs_exit_compress(void)\n{\n\tbtrfs_cleanup_workspace_manager(BTRFS_COMPRESS_NONE);\n\tbtrfs_cleanup_workspace_manager(BTRFS_COMPRESS_ZLIB);\n\tbtrfs_cleanup_workspace_manager(BTRFS_COMPRESS_LZO);\n\tzstd_cleanup_workspace_manager();\n\tbioset_exit(&btrfs_compressed_bioset);\n}\n\n \nint btrfs_decompress_buf2page(const char *buf, u32 buf_len,\n\t\t\t      struct compressed_bio *cb, u32 decompressed)\n{\n\tstruct bio *orig_bio = &cb->orig_bbio->bio;\n\t \n\tu32 cur_offset;\n\n\tcur_offset = decompressed;\n\t \n\twhile (cur_offset < decompressed + buf_len) {\n\t\tstruct bio_vec bvec;\n\t\tsize_t copy_len;\n\t\tu32 copy_start;\n\t\t \n\t\tu32 bvec_offset;\n\n\t\tbvec = bio_iter_iovec(orig_bio, orig_bio->bi_iter);\n\t\t \n\t\tbvec_offset = page_offset(bvec.bv_page) + bvec.bv_offset - cb->start;\n\n\t\t \n\t\tif (decompressed + buf_len <= bvec_offset)\n\t\t\treturn 1;\n\n\t\tcopy_start = max(cur_offset, bvec_offset);\n\t\tcopy_len = min(bvec_offset + bvec.bv_len,\n\t\t\t       decompressed + buf_len) - copy_start;\n\t\tASSERT(copy_len);\n\n\t\t \n\t\tASSERT(copy_start - decompressed < buf_len);\n\t\tmemcpy_to_page(bvec.bv_page, bvec.bv_offset,\n\t\t\t       buf + copy_start - decompressed, copy_len);\n\t\tcur_offset += copy_len;\n\n\t\tbio_advance(orig_bio, copy_len);\n\t\t \n\t\tif (!orig_bio->bi_iter.bi_size)\n\t\t\treturn 0;\n\t}\n\treturn 1;\n}\n\n \n#define ENTROPY_LVL_ACEPTABLE\t\t(65)\n#define ENTROPY_LVL_HIGH\t\t(80)\n\n \nstatic inline u32 ilog2_w(u64 n)\n{\n\treturn ilog2(n * n * n * n);\n}\n\nstatic u32 shannon_entropy(struct heuristic_ws *ws)\n{\n\tconst u32 entropy_max = 8 * ilog2_w(2);\n\tu32 entropy_sum = 0;\n\tu32 p, p_base, sz_base;\n\tu32 i;\n\n\tsz_base = ilog2_w(ws->sample_size);\n\tfor (i = 0; i < BUCKET_SIZE && ws->bucket[i].count > 0; i++) {\n\t\tp = ws->bucket[i].count;\n\t\tp_base = ilog2_w(p);\n\t\tentropy_sum += p * (sz_base - p_base);\n\t}\n\n\tentropy_sum /= ws->sample_size;\n\treturn entropy_sum * 100 / entropy_max;\n}\n\n#define RADIX_BASE\t\t4U\n#define COUNTERS_SIZE\t\t(1U << RADIX_BASE)\n\nstatic u8 get4bits(u64 num, int shift) {\n\tu8 low4bits;\n\n\tnum >>= shift;\n\t \n\tlow4bits = (COUNTERS_SIZE - 1) - (num % COUNTERS_SIZE);\n\treturn low4bits;\n}\n\n \nstatic void radix_sort(struct bucket_item *array, struct bucket_item *array_buf,\n\t\t       int num)\n{\n\tu64 max_num;\n\tu64 buf_num;\n\tu32 counters[COUNTERS_SIZE];\n\tu32 new_addr;\n\tu32 addr;\n\tint bitlen;\n\tint shift;\n\tint i;\n\n\t \n\tmax_num = array[0].count;\n\tfor (i = 1; i < num; i++) {\n\t\tbuf_num = array[i].count;\n\t\tif (buf_num > max_num)\n\t\t\tmax_num = buf_num;\n\t}\n\n\tbuf_num = ilog2(max_num);\n\tbitlen = ALIGN(buf_num, RADIX_BASE * 2);\n\n\tshift = 0;\n\twhile (shift < bitlen) {\n\t\tmemset(counters, 0, sizeof(counters));\n\n\t\tfor (i = 0; i < num; i++) {\n\t\t\tbuf_num = array[i].count;\n\t\t\taddr = get4bits(buf_num, shift);\n\t\t\tcounters[addr]++;\n\t\t}\n\n\t\tfor (i = 1; i < COUNTERS_SIZE; i++)\n\t\t\tcounters[i] += counters[i - 1];\n\n\t\tfor (i = num - 1; i >= 0; i--) {\n\t\t\tbuf_num = array[i].count;\n\t\t\taddr = get4bits(buf_num, shift);\n\t\t\tcounters[addr]--;\n\t\t\tnew_addr = counters[addr];\n\t\t\tarray_buf[new_addr] = array[i];\n\t\t}\n\n\t\tshift += RADIX_BASE;\n\n\t\t \n\t\tmemset(counters, 0, sizeof(counters));\n\n\t\tfor (i = 0; i < num; i ++) {\n\t\t\tbuf_num = array_buf[i].count;\n\t\t\taddr = get4bits(buf_num, shift);\n\t\t\tcounters[addr]++;\n\t\t}\n\n\t\tfor (i = 1; i < COUNTERS_SIZE; i++)\n\t\t\tcounters[i] += counters[i - 1];\n\n\t\tfor (i = num - 1; i >= 0; i--) {\n\t\t\tbuf_num = array_buf[i].count;\n\t\t\taddr = get4bits(buf_num, shift);\n\t\t\tcounters[addr]--;\n\t\t\tnew_addr = counters[addr];\n\t\t\tarray[new_addr] = array_buf[i];\n\t\t}\n\n\t\tshift += RADIX_BASE;\n\t}\n}\n\n \n#define BYTE_CORE_SET_LOW\t\t(64)\n#define BYTE_CORE_SET_HIGH\t\t(200)\n\nstatic int byte_core_set_size(struct heuristic_ws *ws)\n{\n\tu32 i;\n\tu32 coreset_sum = 0;\n\tconst u32 core_set_threshold = ws->sample_size * 90 / 100;\n\tstruct bucket_item *bucket = ws->bucket;\n\n\t \n\tradix_sort(ws->bucket, ws->bucket_b, BUCKET_SIZE);\n\n\tfor (i = 0; i < BYTE_CORE_SET_LOW; i++)\n\t\tcoreset_sum += bucket[i].count;\n\n\tif (coreset_sum > core_set_threshold)\n\t\treturn i;\n\n\tfor (; i < BYTE_CORE_SET_HIGH && bucket[i].count > 0; i++) {\n\t\tcoreset_sum += bucket[i].count;\n\t\tif (coreset_sum > core_set_threshold)\n\t\t\tbreak;\n\t}\n\n\treturn i;\n}\n\n \n#define BYTE_SET_THRESHOLD\t\t(64)\n\nstatic u32 byte_set_size(const struct heuristic_ws *ws)\n{\n\tu32 i;\n\tu32 byte_set_size = 0;\n\n\tfor (i = 0; i < BYTE_SET_THRESHOLD; i++) {\n\t\tif (ws->bucket[i].count > 0)\n\t\t\tbyte_set_size++;\n\t}\n\n\t \n\tfor (; i < BUCKET_SIZE; i++) {\n\t\tif (ws->bucket[i].count > 0) {\n\t\t\tbyte_set_size++;\n\t\t\tif (byte_set_size > BYTE_SET_THRESHOLD)\n\t\t\t\treturn byte_set_size;\n\t\t}\n\t}\n\n\treturn byte_set_size;\n}\n\nstatic bool sample_repeated_patterns(struct heuristic_ws *ws)\n{\n\tconst u32 half_of_sample = ws->sample_size / 2;\n\tconst u8 *data = ws->sample;\n\n\treturn memcmp(&data[0], &data[half_of_sample], half_of_sample) == 0;\n}\n\nstatic void heuristic_collect_sample(struct inode *inode, u64 start, u64 end,\n\t\t\t\t     struct heuristic_ws *ws)\n{\n\tstruct page *page;\n\tu64 index, index_end;\n\tu32 i, curr_sample_pos;\n\tu8 *in_data;\n\n\t \n\tif (end - start > BTRFS_MAX_UNCOMPRESSED)\n\t\tend = start + BTRFS_MAX_UNCOMPRESSED;\n\n\tindex = start >> PAGE_SHIFT;\n\tindex_end = end >> PAGE_SHIFT;\n\n\t \n\tif (!PAGE_ALIGNED(end))\n\t\tindex_end++;\n\n\tcurr_sample_pos = 0;\n\twhile (index < index_end) {\n\t\tpage = find_get_page(inode->i_mapping, index);\n\t\tin_data = kmap_local_page(page);\n\t\t \n\t\ti = start % PAGE_SIZE;\n\t\twhile (i < PAGE_SIZE - SAMPLING_READ_SIZE) {\n\t\t\t \n\t\t\tif (start > end - SAMPLING_READ_SIZE)\n\t\t\t\tbreak;\n\t\t\tmemcpy(&ws->sample[curr_sample_pos], &in_data[i],\n\t\t\t\t\tSAMPLING_READ_SIZE);\n\t\t\ti += SAMPLING_INTERVAL;\n\t\t\tstart += SAMPLING_INTERVAL;\n\t\t\tcurr_sample_pos += SAMPLING_READ_SIZE;\n\t\t}\n\t\tkunmap_local(in_data);\n\t\tput_page(page);\n\n\t\tindex++;\n\t}\n\n\tws->sample_size = curr_sample_pos;\n}\n\n \nint btrfs_compress_heuristic(struct inode *inode, u64 start, u64 end)\n{\n\tstruct list_head *ws_list = get_workspace(0, 0);\n\tstruct heuristic_ws *ws;\n\tu32 i;\n\tu8 byte;\n\tint ret = 0;\n\n\tws = list_entry(ws_list, struct heuristic_ws, list);\n\n\theuristic_collect_sample(inode, start, end, ws);\n\n\tif (sample_repeated_patterns(ws)) {\n\t\tret = 1;\n\t\tgoto out;\n\t}\n\n\tmemset(ws->bucket, 0, sizeof(*ws->bucket)*BUCKET_SIZE);\n\n\tfor (i = 0; i < ws->sample_size; i++) {\n\t\tbyte = ws->sample[i];\n\t\tws->bucket[byte].count++;\n\t}\n\n\ti = byte_set_size(ws);\n\tif (i < BYTE_SET_THRESHOLD) {\n\t\tret = 2;\n\t\tgoto out;\n\t}\n\n\ti = byte_core_set_size(ws);\n\tif (i <= BYTE_CORE_SET_LOW) {\n\t\tret = 3;\n\t\tgoto out;\n\t}\n\n\tif (i >= BYTE_CORE_SET_HIGH) {\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\ti = shannon_entropy(ws);\n\tif (i <= ENTROPY_LVL_ACEPTABLE) {\n\t\tret = 4;\n\t\tgoto out;\n\t}\n\n\t \n\tif (i < ENTROPY_LVL_HIGH) {\n\t\tret = 5;\n\t\tgoto out;\n\t} else {\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\nout:\n\tput_workspace(0, ws_list);\n\treturn ret;\n}\n\n \nunsigned int btrfs_compress_str2level(unsigned int type, const char *str)\n{\n\tunsigned int level = 0;\n\tint ret;\n\n\tif (!type)\n\t\treturn 0;\n\n\tif (str[0] == ':') {\n\t\tret = kstrtouint(str + 1, 10, &level);\n\t\tif (ret)\n\t\t\tlevel = 0;\n\t}\n\n\tlevel = btrfs_compress_set_level(type, level);\n\n\treturn level;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}