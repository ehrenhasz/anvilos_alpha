{
  "module_name": "defrag.c",
  "hash_id": "3aa939129fb728a24ddd3a7b1ff3373f66f8af630ec46d502f0cbf03af5b7aa7",
  "original_prompt": "Ingested from linux-6.6.14/fs/btrfs/defrag.c",
  "human_readable_source": "\n \n\n#include <linux/sched.h>\n#include \"ctree.h\"\n#include \"disk-io.h\"\n#include \"print-tree.h\"\n#include \"transaction.h\"\n#include \"locking.h\"\n#include \"accessors.h\"\n#include \"messages.h\"\n#include \"delalloc-space.h\"\n#include \"subpage.h\"\n#include \"defrag.h\"\n#include \"file-item.h\"\n#include \"super.h\"\n\nstatic struct kmem_cache *btrfs_inode_defrag_cachep;\n\n \nstruct inode_defrag {\n\tstruct rb_node rb_node;\n\t \n\tu64 ino;\n\t \n\tu64 transid;\n\n\t \n\tu64 root;\n\n\t \n\tu32 extent_thresh;\n};\n\nstatic int __compare_inode_defrag(struct inode_defrag *defrag1,\n\t\t\t\t  struct inode_defrag *defrag2)\n{\n\tif (defrag1->root > defrag2->root)\n\t\treturn 1;\n\telse if (defrag1->root < defrag2->root)\n\t\treturn -1;\n\telse if (defrag1->ino > defrag2->ino)\n\t\treturn 1;\n\telse if (defrag1->ino < defrag2->ino)\n\t\treturn -1;\n\telse\n\t\treturn 0;\n}\n\n \nstatic int __btrfs_add_inode_defrag(struct btrfs_inode *inode,\n\t\t\t\t    struct inode_defrag *defrag)\n{\n\tstruct btrfs_fs_info *fs_info = inode->root->fs_info;\n\tstruct inode_defrag *entry;\n\tstruct rb_node **p;\n\tstruct rb_node *parent = NULL;\n\tint ret;\n\n\tp = &fs_info->defrag_inodes.rb_node;\n\twhile (*p) {\n\t\tparent = *p;\n\t\tentry = rb_entry(parent, struct inode_defrag, rb_node);\n\n\t\tret = __compare_inode_defrag(defrag, entry);\n\t\tif (ret < 0)\n\t\t\tp = &parent->rb_left;\n\t\telse if (ret > 0)\n\t\t\tp = &parent->rb_right;\n\t\telse {\n\t\t\t \n\t\t\tif (defrag->transid < entry->transid)\n\t\t\t\tentry->transid = defrag->transid;\n\t\t\tentry->extent_thresh = min(defrag->extent_thresh,\n\t\t\t\t\t\t   entry->extent_thresh);\n\t\t\treturn -EEXIST;\n\t\t}\n\t}\n\tset_bit(BTRFS_INODE_IN_DEFRAG, &inode->runtime_flags);\n\trb_link_node(&defrag->rb_node, parent, p);\n\trb_insert_color(&defrag->rb_node, &fs_info->defrag_inodes);\n\treturn 0;\n}\n\nstatic inline int __need_auto_defrag(struct btrfs_fs_info *fs_info)\n{\n\tif (!btrfs_test_opt(fs_info, AUTO_DEFRAG))\n\t\treturn 0;\n\n\tif (btrfs_fs_closing(fs_info))\n\t\treturn 0;\n\n\treturn 1;\n}\n\n \nint btrfs_add_inode_defrag(struct btrfs_trans_handle *trans,\n\t\t\t   struct btrfs_inode *inode, u32 extent_thresh)\n{\n\tstruct btrfs_root *root = inode->root;\n\tstruct btrfs_fs_info *fs_info = root->fs_info;\n\tstruct inode_defrag *defrag;\n\tu64 transid;\n\tint ret;\n\n\tif (!__need_auto_defrag(fs_info))\n\t\treturn 0;\n\n\tif (test_bit(BTRFS_INODE_IN_DEFRAG, &inode->runtime_flags))\n\t\treturn 0;\n\n\tif (trans)\n\t\ttransid = trans->transid;\n\telse\n\t\ttransid = inode->root->last_trans;\n\n\tdefrag = kmem_cache_zalloc(btrfs_inode_defrag_cachep, GFP_NOFS);\n\tif (!defrag)\n\t\treturn -ENOMEM;\n\n\tdefrag->ino = btrfs_ino(inode);\n\tdefrag->transid = transid;\n\tdefrag->root = root->root_key.objectid;\n\tdefrag->extent_thresh = extent_thresh;\n\n\tspin_lock(&fs_info->defrag_inodes_lock);\n\tif (!test_bit(BTRFS_INODE_IN_DEFRAG, &inode->runtime_flags)) {\n\t\t \n\t\tret = __btrfs_add_inode_defrag(inode, defrag);\n\t\tif (ret)\n\t\t\tkmem_cache_free(btrfs_inode_defrag_cachep, defrag);\n\t} else {\n\t\tkmem_cache_free(btrfs_inode_defrag_cachep, defrag);\n\t}\n\tspin_unlock(&fs_info->defrag_inodes_lock);\n\treturn 0;\n}\n\n \nstatic struct inode_defrag *btrfs_pick_defrag_inode(\n\t\t\tstruct btrfs_fs_info *fs_info, u64 root, u64 ino)\n{\n\tstruct inode_defrag *entry = NULL;\n\tstruct inode_defrag tmp;\n\tstruct rb_node *p;\n\tstruct rb_node *parent = NULL;\n\tint ret;\n\n\ttmp.ino = ino;\n\ttmp.root = root;\n\n\tspin_lock(&fs_info->defrag_inodes_lock);\n\tp = fs_info->defrag_inodes.rb_node;\n\twhile (p) {\n\t\tparent = p;\n\t\tentry = rb_entry(parent, struct inode_defrag, rb_node);\n\n\t\tret = __compare_inode_defrag(&tmp, entry);\n\t\tif (ret < 0)\n\t\t\tp = parent->rb_left;\n\t\telse if (ret > 0)\n\t\t\tp = parent->rb_right;\n\t\telse\n\t\t\tgoto out;\n\t}\n\n\tif (parent && __compare_inode_defrag(&tmp, entry) > 0) {\n\t\tparent = rb_next(parent);\n\t\tif (parent)\n\t\t\tentry = rb_entry(parent, struct inode_defrag, rb_node);\n\t\telse\n\t\t\tentry = NULL;\n\t}\nout:\n\tif (entry)\n\t\trb_erase(parent, &fs_info->defrag_inodes);\n\tspin_unlock(&fs_info->defrag_inodes_lock);\n\treturn entry;\n}\n\nvoid btrfs_cleanup_defrag_inodes(struct btrfs_fs_info *fs_info)\n{\n\tstruct inode_defrag *defrag;\n\tstruct rb_node *node;\n\n\tspin_lock(&fs_info->defrag_inodes_lock);\n\tnode = rb_first(&fs_info->defrag_inodes);\n\twhile (node) {\n\t\trb_erase(node, &fs_info->defrag_inodes);\n\t\tdefrag = rb_entry(node, struct inode_defrag, rb_node);\n\t\tkmem_cache_free(btrfs_inode_defrag_cachep, defrag);\n\n\t\tcond_resched_lock(&fs_info->defrag_inodes_lock);\n\n\t\tnode = rb_first(&fs_info->defrag_inodes);\n\t}\n\tspin_unlock(&fs_info->defrag_inodes_lock);\n}\n\n#define BTRFS_DEFRAG_BATCH\t1024\n\nstatic int __btrfs_run_defrag_inode(struct btrfs_fs_info *fs_info,\n\t\t\t\t    struct inode_defrag *defrag)\n{\n\tstruct btrfs_root *inode_root;\n\tstruct inode *inode;\n\tstruct btrfs_ioctl_defrag_range_args range;\n\tint ret = 0;\n\tu64 cur = 0;\n\nagain:\n\tif (test_bit(BTRFS_FS_STATE_REMOUNTING, &fs_info->fs_state))\n\t\tgoto cleanup;\n\tif (!__need_auto_defrag(fs_info))\n\t\tgoto cleanup;\n\n\t \n\tinode_root = btrfs_get_fs_root(fs_info, defrag->root, true);\n\tif (IS_ERR(inode_root)) {\n\t\tret = PTR_ERR(inode_root);\n\t\tgoto cleanup;\n\t}\n\n\tinode = btrfs_iget(fs_info->sb, defrag->ino, inode_root);\n\tbtrfs_put_root(inode_root);\n\tif (IS_ERR(inode)) {\n\t\tret = PTR_ERR(inode);\n\t\tgoto cleanup;\n\t}\n\n\tif (cur >= i_size_read(inode)) {\n\t\tiput(inode);\n\t\tgoto cleanup;\n\t}\n\n\t \n\tclear_bit(BTRFS_INODE_IN_DEFRAG, &BTRFS_I(inode)->runtime_flags);\n\tmemset(&range, 0, sizeof(range));\n\trange.len = (u64)-1;\n\trange.start = cur;\n\trange.extent_thresh = defrag->extent_thresh;\n\n\tsb_start_write(fs_info->sb);\n\tret = btrfs_defrag_file(inode, NULL, &range, defrag->transid,\n\t\t\t\t       BTRFS_DEFRAG_BATCH);\n\tsb_end_write(fs_info->sb);\n\tiput(inode);\n\n\tif (ret < 0)\n\t\tgoto cleanup;\n\n\tcur = max(cur + fs_info->sectorsize, range.start);\n\tgoto again;\n\ncleanup:\n\tkmem_cache_free(btrfs_inode_defrag_cachep, defrag);\n\treturn ret;\n}\n\n \nint btrfs_run_defrag_inodes(struct btrfs_fs_info *fs_info)\n{\n\tstruct inode_defrag *defrag;\n\tu64 first_ino = 0;\n\tu64 root_objectid = 0;\n\n\tatomic_inc(&fs_info->defrag_running);\n\twhile (1) {\n\t\t \n\t\tif (test_bit(BTRFS_FS_STATE_REMOUNTING, &fs_info->fs_state))\n\t\t\tbreak;\n\n\t\tif (!__need_auto_defrag(fs_info))\n\t\t\tbreak;\n\n\t\t \n\t\tdefrag = btrfs_pick_defrag_inode(fs_info, root_objectid, first_ino);\n\t\tif (!defrag) {\n\t\t\tif (root_objectid || first_ino) {\n\t\t\t\troot_objectid = 0;\n\t\t\t\tfirst_ino = 0;\n\t\t\t\tcontinue;\n\t\t\t} else {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tfirst_ino = defrag->ino + 1;\n\t\troot_objectid = defrag->root;\n\n\t\t__btrfs_run_defrag_inode(fs_info, defrag);\n\t}\n\tatomic_dec(&fs_info->defrag_running);\n\n\t \n\twake_up(&fs_info->transaction_wait);\n\treturn 0;\n}\n\n \n\nint btrfs_defrag_leaves(struct btrfs_trans_handle *trans,\n\t\t\tstruct btrfs_root *root)\n{\n\tstruct btrfs_path *path = NULL;\n\tstruct btrfs_key key;\n\tint ret = 0;\n\tint wret;\n\tint level;\n\tint next_key_ret = 0;\n\tu64 last_ret = 0;\n\n\tif (!test_bit(BTRFS_ROOT_SHAREABLE, &root->state))\n\t\tgoto out;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tlevel = btrfs_header_level(root->node);\n\n\tif (level == 0)\n\t\tgoto out;\n\n\tif (root->defrag_progress.objectid == 0) {\n\t\tstruct extent_buffer *root_node;\n\t\tu32 nritems;\n\n\t\troot_node = btrfs_lock_root_node(root);\n\t\tnritems = btrfs_header_nritems(root_node);\n\t\troot->defrag_max.objectid = 0;\n\t\t \n\t\tbtrfs_node_key_to_cpu(root_node, &root->defrag_max,\n\t\t\t\t      nritems - 1);\n\t\tbtrfs_tree_unlock(root_node);\n\t\tfree_extent_buffer(root_node);\n\t\tmemset(&key, 0, sizeof(key));\n\t} else {\n\t\tmemcpy(&key, &root->defrag_progress, sizeof(key));\n\t}\n\n\tpath->keep_locks = 1;\n\n\tret = btrfs_search_forward(root, &key, path, BTRFS_OLDEST_GENERATION);\n\tif (ret < 0)\n\t\tgoto out;\n\tif (ret > 0) {\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\tbtrfs_release_path(path);\n\t \n\tpath->lowest_level = 1;\n\twret = btrfs_search_slot(trans, root, &key, path, 0, 1);\n\n\tif (wret < 0) {\n\t\tret = wret;\n\t\tgoto out;\n\t}\n\tif (!path->nodes[1]) {\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\t \n\tBUG_ON(path->locks[1] == 0);\n\tret = btrfs_realloc_node(trans, root,\n\t\t\t\t path->nodes[1], 0,\n\t\t\t\t &last_ret,\n\t\t\t\t &root->defrag_progress);\n\tif (ret) {\n\t\tWARN_ON(ret == -EAGAIN);\n\t\tgoto out;\n\t}\n\t \n\tpath->slots[1] = btrfs_header_nritems(path->nodes[1]);\n\tnext_key_ret = btrfs_find_next_key(root, path, &key, 1,\n\t\t\t\t\t   BTRFS_OLDEST_GENERATION);\n\tif (next_key_ret == 0) {\n\t\tmemcpy(&root->defrag_progress, &key, sizeof(key));\n\t\tret = -EAGAIN;\n\t}\nout:\n\tbtrfs_free_path(path);\n\tif (ret == -EAGAIN) {\n\t\tif (root->defrag_max.objectid > root->defrag_progress.objectid)\n\t\t\tgoto done;\n\t\tif (root->defrag_max.type > root->defrag_progress.type)\n\t\t\tgoto done;\n\t\tif (root->defrag_max.offset > root->defrag_progress.offset)\n\t\t\tgoto done;\n\t\tret = 0;\n\t}\ndone:\n\tif (ret != -EAGAIN)\n\t\tmemset(&root->defrag_progress, 0,\n\t\t       sizeof(root->defrag_progress));\n\n\treturn ret;\n}\n\n \nstatic struct extent_map *defrag_get_extent(struct btrfs_inode *inode,\n\t\t\t\t\t    u64 start, u64 newer_than)\n{\n\tstruct btrfs_root *root = inode->root;\n\tstruct btrfs_file_extent_item *fi;\n\tstruct btrfs_path path = { 0 };\n\tstruct extent_map *em;\n\tstruct btrfs_key key;\n\tu64 ino = btrfs_ino(inode);\n\tint ret;\n\n\tem = alloc_extent_map();\n\tif (!em) {\n\t\tret = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\tkey.objectid = ino;\n\tkey.type = BTRFS_EXTENT_DATA_KEY;\n\tkey.offset = start;\n\n\tif (newer_than) {\n\t\tret = btrfs_search_forward(root, &key, &path, newer_than);\n\t\tif (ret < 0)\n\t\t\tgoto err;\n\t\t \n\t\tif (ret > 0)\n\t\t\tgoto not_found;\n\t} else {\n\t\tret = btrfs_search_slot(NULL, root, &key, &path, 0, 0);\n\t\tif (ret < 0)\n\t\t\tgoto err;\n\t}\n\tif (path.slots[0] >= btrfs_header_nritems(path.nodes[0])) {\n\t\t \n\t\tASSERT(btrfs_header_nritems(path.nodes[0]));\n\t\tpath.slots[0] = btrfs_header_nritems(path.nodes[0]) - 1;\n\t}\n\tbtrfs_item_key_to_cpu(path.nodes[0], &key, path.slots[0]);\n\t \n\tif (key.objectid == ino && key.type == BTRFS_EXTENT_DATA_KEY &&\n\t    key.offset == start)\n\t\tgoto iterate;\n\n\t \n\tif (path.slots[0] > 0) {\n\t\tbtrfs_item_key_to_cpu(path.nodes[0], &key, path.slots[0]);\n\t\tif (key.objectid == ino && key.type == BTRFS_EXTENT_DATA_KEY)\n\t\t\tpath.slots[0]--;\n\t}\n\niterate:\n\t \n\twhile (true) {\n\t\tu64 extent_end;\n\n\t\tif (path.slots[0] >= btrfs_header_nritems(path.nodes[0]))\n\t\t\tgoto next;\n\n\t\tbtrfs_item_key_to_cpu(path.nodes[0], &key, path.slots[0]);\n\n\t\t \n\t\tif (WARN_ON(key.objectid < ino) || key.type < BTRFS_EXTENT_DATA_KEY)\n\t\t\tgoto next;\n\n\t\t \n\t\tif (key.objectid > ino || key.type > BTRFS_EXTENT_DATA_KEY)\n\t\t\tgoto not_found;\n\n\t\t \n\t\tif (key.offset > start) {\n\t\t\tem->start = start;\n\t\t\tem->orig_start = start;\n\t\t\tem->block_start = EXTENT_MAP_HOLE;\n\t\t\tem->len = key.offset - start;\n\t\t\tbreak;\n\t\t}\n\n\t\tfi = btrfs_item_ptr(path.nodes[0], path.slots[0],\n\t\t\t\t    struct btrfs_file_extent_item);\n\t\textent_end = btrfs_file_extent_end(&path);\n\n\t\t \n\t\tif (extent_end <= start)\n\t\t\tgoto next;\n\n\t\t \n\t\tbtrfs_extent_item_to_extent_map(inode, &path, fi, em);\n\t\tbreak;\nnext:\n\t\tret = btrfs_next_item(root, &path);\n\t\tif (ret < 0)\n\t\t\tgoto err;\n\t\tif (ret > 0)\n\t\t\tgoto not_found;\n\t}\n\tbtrfs_release_path(&path);\n\treturn em;\n\nnot_found:\n\tbtrfs_release_path(&path);\n\tfree_extent_map(em);\n\treturn NULL;\n\nerr:\n\tbtrfs_release_path(&path);\n\tfree_extent_map(em);\n\treturn ERR_PTR(ret);\n}\n\nstatic struct extent_map *defrag_lookup_extent(struct inode *inode, u64 start,\n\t\t\t\t\t       u64 newer_than, bool locked)\n{\n\tstruct extent_map_tree *em_tree = &BTRFS_I(inode)->extent_tree;\n\tstruct extent_io_tree *io_tree = &BTRFS_I(inode)->io_tree;\n\tstruct extent_map *em;\n\tconst u32 sectorsize = BTRFS_I(inode)->root->fs_info->sectorsize;\n\n\t \n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, start, sectorsize);\n\tread_unlock(&em_tree->lock);\n\n\t \n\tif (em && test_bit(EXTENT_FLAG_MERGED, &em->flags) &&\n\t    newer_than && em->generation >= newer_than) {\n\t\tfree_extent_map(em);\n\t\tem = NULL;\n\t}\n\n\tif (!em) {\n\t\tstruct extent_state *cached = NULL;\n\t\tu64 end = start + sectorsize - 1;\n\n\t\t \n\t\tif (!locked)\n\t\t\tlock_extent(io_tree, start, end, &cached);\n\t\tem = defrag_get_extent(BTRFS_I(inode), start, newer_than);\n\t\tif (!locked)\n\t\t\tunlock_extent(io_tree, start, end, &cached);\n\n\t\tif (IS_ERR(em))\n\t\t\treturn NULL;\n\t}\n\n\treturn em;\n}\n\nstatic u32 get_extent_max_capacity(const struct btrfs_fs_info *fs_info,\n\t\t\t\t   const struct extent_map *em)\n{\n\tif (test_bit(EXTENT_FLAG_COMPRESSED, &em->flags))\n\t\treturn BTRFS_MAX_COMPRESSED;\n\treturn fs_info->max_extent_size;\n}\n\nstatic bool defrag_check_next_extent(struct inode *inode, struct extent_map *em,\n\t\t\t\t     u32 extent_thresh, u64 newer_than, bool locked)\n{\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tstruct extent_map *next;\n\tbool ret = false;\n\n\t \n\tif (em->start + em->len >= i_size_read(inode))\n\t\treturn false;\n\n\t \n\tnext = defrag_lookup_extent(inode, em->start + em->len, newer_than, locked);\n\t \n\tif (!next || next->block_start >= EXTENT_MAP_LAST_BYTE)\n\t\tgoto out;\n\tif (test_bit(EXTENT_FLAG_PREALLOC, &next->flags))\n\t\tgoto out;\n\t \n\tif (next->len >= get_extent_max_capacity(fs_info, em))\n\t\tgoto out;\n\t \n\tif (next->generation < newer_than)\n\t\tgoto out;\n\t \n\tif (next->len >= extent_thresh)\n\t\tgoto out;\n\n\tret = true;\nout:\n\tfree_extent_map(next);\n\treturn ret;\n}\n\n \nstatic struct page *defrag_prepare_one_page(struct btrfs_inode *inode, pgoff_t index)\n{\n\tstruct address_space *mapping = inode->vfs_inode.i_mapping;\n\tgfp_t mask = btrfs_alloc_write_mask(mapping);\n\tu64 page_start = (u64)index << PAGE_SHIFT;\n\tu64 page_end = page_start + PAGE_SIZE - 1;\n\tstruct extent_state *cached_state = NULL;\n\tstruct page *page;\n\tint ret;\n\nagain:\n\tpage = find_or_create_page(mapping, index, mask);\n\tif (!page)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\t \n\tif (PageCompound(page)) {\n\t\tunlock_page(page);\n\t\tput_page(page);\n\t\treturn ERR_PTR(-ETXTBSY);\n\t}\n\n\tret = set_page_extent_mapped(page);\n\tif (ret < 0) {\n\t\tunlock_page(page);\n\t\tput_page(page);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\t \n\twhile (1) {\n\t\tstruct btrfs_ordered_extent *ordered;\n\n\t\tlock_extent(&inode->io_tree, page_start, page_end, &cached_state);\n\t\tordered = btrfs_lookup_ordered_range(inode, page_start, PAGE_SIZE);\n\t\tunlock_extent(&inode->io_tree, page_start, page_end,\n\t\t\t      &cached_state);\n\t\tif (!ordered)\n\t\t\tbreak;\n\n\t\tunlock_page(page);\n\t\tbtrfs_start_ordered_extent(ordered);\n\t\tbtrfs_put_ordered_extent(ordered);\n\t\tlock_page(page);\n\t\t \n\t\tif (page->mapping != mapping || !PagePrivate(page)) {\n\t\t\tunlock_page(page);\n\t\t\tput_page(page);\n\t\t\tgoto again;\n\t\t}\n\t}\n\n\t \n\tif (!PageUptodate(page)) {\n\t\tbtrfs_read_folio(NULL, page_folio(page));\n\t\tlock_page(page);\n\t\tif (page->mapping != mapping || !PagePrivate(page)) {\n\t\t\tunlock_page(page);\n\t\t\tput_page(page);\n\t\t\tgoto again;\n\t\t}\n\t\tif (!PageUptodate(page)) {\n\t\t\tunlock_page(page);\n\t\t\tput_page(page);\n\t\t\treturn ERR_PTR(-EIO);\n\t\t}\n\t}\n\treturn page;\n}\n\nstruct defrag_target_range {\n\tstruct list_head list;\n\tu64 start;\n\tu64 len;\n};\n\n \nstatic int defrag_collect_targets(struct btrfs_inode *inode,\n\t\t\t\t  u64 start, u64 len, u32 extent_thresh,\n\t\t\t\t  u64 newer_than, bool do_compress,\n\t\t\t\t  bool locked, struct list_head *target_list,\n\t\t\t\t  u64 *last_scanned_ret)\n{\n\tstruct btrfs_fs_info *fs_info = inode->root->fs_info;\n\tbool last_is_target = false;\n\tu64 cur = start;\n\tint ret = 0;\n\n\twhile (cur < start + len) {\n\t\tstruct extent_map *em;\n\t\tstruct defrag_target_range *new;\n\t\tbool next_mergeable = true;\n\t\tu64 range_len;\n\n\t\tlast_is_target = false;\n\t\tem = defrag_lookup_extent(&inode->vfs_inode, cur, newer_than, locked);\n\t\tif (!em)\n\t\t\tbreak;\n\n\t\t \n\t\tif (em->block_start == EXTENT_MAP_INLINE &&\n\t\t    em->len <= inode->root->fs_info->max_inline)\n\t\t\tgoto next;\n\n\t\t \n\t\tif (em->block_start == EXTENT_MAP_HOLE ||\n\t\t    em->block_start == EXTENT_MAP_DELALLOC ||\n\t\t    test_bit(EXTENT_FLAG_PREALLOC, &em->flags))\n\t\t\tgoto next;\n\n\t\t \n\t\tif (em->generation < newer_than)\n\t\t\tgoto next;\n\n\t\t \n\t\tif (em->generation == (u64)-1)\n\t\t\tgoto next;\n\n\t\t \n\t\trange_len = em->len - (cur - em->start);\n\t\t \n\t\tif (test_range_bit(&inode->io_tree, cur, cur + range_len - 1,\n\t\t\t\t   EXTENT_DELALLOC, 0, NULL))\n\t\t\tgoto next;\n\n\t\t \n\t\tif (do_compress)\n\t\t\tgoto add;\n\n\t\t \n\t\tif (range_len >= extent_thresh)\n\t\t\tgoto next;\n\n\t\t \n\t\tif (em->len >= get_extent_max_capacity(fs_info, em))\n\t\t\tgoto next;\n\n\t\t \n\t\tif (em->block_start == EXTENT_MAP_INLINE)\n\t\t\tgoto add;\n\n\t\tnext_mergeable = defrag_check_next_extent(&inode->vfs_inode, em,\n\t\t\t\t\t\textent_thresh, newer_than, locked);\n\t\tif (!next_mergeable) {\n\t\t\tstruct defrag_target_range *last;\n\n\t\t\t \n\t\t\tif (list_empty(target_list))\n\t\t\t\tgoto next;\n\t\t\tlast = list_entry(target_list->prev,\n\t\t\t\t\t  struct defrag_target_range, list);\n\t\t\t \n\t\t\tif (last->start + last->len != cur)\n\t\t\t\tgoto next;\n\n\t\t\t \n\t\t}\n\nadd:\n\t\tlast_is_target = true;\n\t\trange_len = min(extent_map_end(em), start + len) - cur;\n\t\t \n\t\tif (!list_empty(target_list)) {\n\t\t\tstruct defrag_target_range *last;\n\n\t\t\tlast = list_entry(target_list->prev,\n\t\t\t\t\t  struct defrag_target_range, list);\n\t\t\tASSERT(last->start + last->len <= cur);\n\t\t\tif (last->start + last->len == cur) {\n\t\t\t\t \n\t\t\t\tlast->len += range_len;\n\t\t\t\tgoto next;\n\t\t\t}\n\t\t\t \n\t\t}\n\n\t\t \n\t\tnew = kmalloc(sizeof(*new), GFP_NOFS);\n\t\tif (!new) {\n\t\t\tfree_extent_map(em);\n\t\t\tret = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\t\tnew->start = cur;\n\t\tnew->len = range_len;\n\t\tlist_add_tail(&new->list, target_list);\n\nnext:\n\t\tcur = extent_map_end(em);\n\t\tfree_extent_map(em);\n\t}\n\tif (ret < 0) {\n\t\tstruct defrag_target_range *entry;\n\t\tstruct defrag_target_range *tmp;\n\n\t\tlist_for_each_entry_safe(entry, tmp, target_list, list) {\n\t\t\tlist_del_init(&entry->list);\n\t\t\tkfree(entry);\n\t\t}\n\t}\n\tif (!ret && last_scanned_ret) {\n\t\t \n\t\tif (!last_is_target)\n\t\t\t*last_scanned_ret = max(cur, *last_scanned_ret);\n\t\telse\n\t\t\t*last_scanned_ret = max(start + len, *last_scanned_ret);\n\t}\n\treturn ret;\n}\n\n#define CLUSTER_SIZE\t(SZ_256K)\nstatic_assert(PAGE_ALIGNED(CLUSTER_SIZE));\n\n \nstatic int defrag_one_locked_target(struct btrfs_inode *inode,\n\t\t\t\t    struct defrag_target_range *target,\n\t\t\t\t    struct page **pages, int nr_pages,\n\t\t\t\t    struct extent_state **cached_state)\n{\n\tstruct btrfs_fs_info *fs_info = inode->root->fs_info;\n\tstruct extent_changeset *data_reserved = NULL;\n\tconst u64 start = target->start;\n\tconst u64 len = target->len;\n\tunsigned long last_index = (start + len - 1) >> PAGE_SHIFT;\n\tunsigned long start_index = start >> PAGE_SHIFT;\n\tunsigned long first_index = page_index(pages[0]);\n\tint ret = 0;\n\tint i;\n\n\tASSERT(last_index - first_index + 1 <= nr_pages);\n\n\tret = btrfs_delalloc_reserve_space(inode, &data_reserved, start, len);\n\tif (ret < 0)\n\t\treturn ret;\n\tclear_extent_bit(&inode->io_tree, start, start + len - 1,\n\t\t\t EXTENT_DELALLOC | EXTENT_DO_ACCOUNTING |\n\t\t\t EXTENT_DEFRAG, cached_state);\n\tset_extent_bit(&inode->io_tree, start, start + len - 1,\n\t\t       EXTENT_DELALLOC | EXTENT_DEFRAG, cached_state);\n\n\t \n\tfor (i = start_index - first_index; i <= last_index - first_index; i++) {\n\t\tClearPageChecked(pages[i]);\n\t\tbtrfs_page_clamp_set_dirty(fs_info, pages[i], start, len);\n\t}\n\tbtrfs_delalloc_release_extents(inode, len);\n\textent_changeset_free(data_reserved);\n\n\treturn ret;\n}\n\nstatic int defrag_one_range(struct btrfs_inode *inode, u64 start, u32 len,\n\t\t\t    u32 extent_thresh, u64 newer_than, bool do_compress,\n\t\t\t    u64 *last_scanned_ret)\n{\n\tstruct extent_state *cached_state = NULL;\n\tstruct defrag_target_range *entry;\n\tstruct defrag_target_range *tmp;\n\tLIST_HEAD(target_list);\n\tstruct page **pages;\n\tconst u32 sectorsize = inode->root->fs_info->sectorsize;\n\tu64 last_index = (start + len - 1) >> PAGE_SHIFT;\n\tu64 start_index = start >> PAGE_SHIFT;\n\tunsigned int nr_pages = last_index - start_index + 1;\n\tint ret = 0;\n\tint i;\n\n\tASSERT(nr_pages <= CLUSTER_SIZE / PAGE_SIZE);\n\tASSERT(IS_ALIGNED(start, sectorsize) && IS_ALIGNED(len, sectorsize));\n\n\tpages = kcalloc(nr_pages, sizeof(struct page *), GFP_NOFS);\n\tif (!pages)\n\t\treturn -ENOMEM;\n\n\t \n\tfor (i = 0; i < nr_pages; i++) {\n\t\tpages[i] = defrag_prepare_one_page(inode, start_index + i);\n\t\tif (IS_ERR(pages[i])) {\n\t\t\tret = PTR_ERR(pages[i]);\n\t\t\tpages[i] = NULL;\n\t\t\tgoto free_pages;\n\t\t}\n\t}\n\tfor (i = 0; i < nr_pages; i++)\n\t\twait_on_page_writeback(pages[i]);\n\n\t \n\tlock_extent(&inode->io_tree, start_index << PAGE_SHIFT,\n\t\t    (last_index << PAGE_SHIFT) + PAGE_SIZE - 1,\n\t\t    &cached_state);\n\t \n\tret = defrag_collect_targets(inode, start, len, extent_thresh,\n\t\t\t\t     newer_than, do_compress, true,\n\t\t\t\t     &target_list, last_scanned_ret);\n\tif (ret < 0)\n\t\tgoto unlock_extent;\n\n\tlist_for_each_entry(entry, &target_list, list) {\n\t\tret = defrag_one_locked_target(inode, entry, pages, nr_pages,\n\t\t\t\t\t       &cached_state);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\t}\n\n\tlist_for_each_entry_safe(entry, tmp, &target_list, list) {\n\t\tlist_del_init(&entry->list);\n\t\tkfree(entry);\n\t}\nunlock_extent:\n\tunlock_extent(&inode->io_tree, start_index << PAGE_SHIFT,\n\t\t      (last_index << PAGE_SHIFT) + PAGE_SIZE - 1,\n\t\t      &cached_state);\nfree_pages:\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tif (pages[i]) {\n\t\t\tunlock_page(pages[i]);\n\t\t\tput_page(pages[i]);\n\t\t}\n\t}\n\tkfree(pages);\n\treturn ret;\n}\n\nstatic int defrag_one_cluster(struct btrfs_inode *inode,\n\t\t\t      struct file_ra_state *ra,\n\t\t\t      u64 start, u32 len, u32 extent_thresh,\n\t\t\t      u64 newer_than, bool do_compress,\n\t\t\t      unsigned long *sectors_defragged,\n\t\t\t      unsigned long max_sectors,\n\t\t\t      u64 *last_scanned_ret)\n{\n\tconst u32 sectorsize = inode->root->fs_info->sectorsize;\n\tstruct defrag_target_range *entry;\n\tstruct defrag_target_range *tmp;\n\tLIST_HEAD(target_list);\n\tint ret;\n\n\tret = defrag_collect_targets(inode, start, len, extent_thresh,\n\t\t\t\t     newer_than, do_compress, false,\n\t\t\t\t     &target_list, NULL);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tlist_for_each_entry(entry, &target_list, list) {\n\t\tu32 range_len = entry->len;\n\n\t\t \n\t\tif (max_sectors && *sectors_defragged >= max_sectors) {\n\t\t\tret = 1;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (max_sectors)\n\t\t\trange_len = min_t(u32, range_len,\n\t\t\t\t(max_sectors - *sectors_defragged) * sectorsize);\n\n\t\t \n\t\tif (entry->start + range_len <= *last_scanned_ret)\n\t\t\tcontinue;\n\n\t\tif (ra)\n\t\t\tpage_cache_sync_readahead(inode->vfs_inode.i_mapping,\n\t\t\t\tra, NULL, entry->start >> PAGE_SHIFT,\n\t\t\t\t((entry->start + range_len - 1) >> PAGE_SHIFT) -\n\t\t\t\t(entry->start >> PAGE_SHIFT) + 1);\n\t\t \n\t\tret = defrag_one_range(inode, entry->start, range_len,\n\t\t\t\t       extent_thresh, newer_than, do_compress,\n\t\t\t\t       last_scanned_ret);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\t\t*sectors_defragged += range_len >>\n\t\t\t\t      inode->root->fs_info->sectorsize_bits;\n\t}\nout:\n\tlist_for_each_entry_safe(entry, tmp, &target_list, list) {\n\t\tlist_del_init(&entry->list);\n\t\tkfree(entry);\n\t}\n\tif (ret >= 0)\n\t\t*last_scanned_ret = max(*last_scanned_ret, start + len);\n\treturn ret;\n}\n\n \nint btrfs_defrag_file(struct inode *inode, struct file_ra_state *ra,\n\t\t      struct btrfs_ioctl_defrag_range_args *range,\n\t\t      u64 newer_than, unsigned long max_to_defrag)\n{\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tunsigned long sectors_defragged = 0;\n\tu64 isize = i_size_read(inode);\n\tu64 cur;\n\tu64 last_byte;\n\tbool do_compress = (range->flags & BTRFS_DEFRAG_RANGE_COMPRESS);\n\tbool ra_allocated = false;\n\tint compress_type = BTRFS_COMPRESS_ZLIB;\n\tint ret = 0;\n\tu32 extent_thresh = range->extent_thresh;\n\tpgoff_t start_index;\n\n\tif (isize == 0)\n\t\treturn 0;\n\n\tif (range->start >= isize)\n\t\treturn -EINVAL;\n\n\tif (do_compress) {\n\t\tif (range->compress_type >= BTRFS_NR_COMPRESS_TYPES)\n\t\t\treturn -EINVAL;\n\t\tif (range->compress_type)\n\t\t\tcompress_type = range->compress_type;\n\t}\n\n\tif (extent_thresh == 0)\n\t\textent_thresh = SZ_256K;\n\n\tif (range->start + range->len > range->start) {\n\t\t \n\t\tlast_byte = min(isize, range->start + range->len);\n\t} else {\n\t\t \n\t\tlast_byte = isize;\n\t}\n\n\t \n\tcur = round_down(range->start, fs_info->sectorsize);\n\tlast_byte = round_up(last_byte, fs_info->sectorsize) - 1;\n\n\t \n\tif (!ra) {\n\t\tra_allocated = true;\n\t\tra = kzalloc(sizeof(*ra), GFP_KERNEL);\n\t\tif (ra)\n\t\t\tfile_ra_state_init(ra, inode->i_mapping);\n\t}\n\n\t \n\tstart_index = cur >> PAGE_SHIFT;\n\tif (start_index < inode->i_mapping->writeback_index)\n\t\tinode->i_mapping->writeback_index = start_index;\n\n\twhile (cur < last_byte) {\n\t\tconst unsigned long prev_sectors_defragged = sectors_defragged;\n\t\tu64 last_scanned = cur;\n\t\tu64 cluster_end;\n\n\t\tif (btrfs_defrag_cancelled(fs_info)) {\n\t\t\tret = -EAGAIN;\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tcluster_end = (((cur >> PAGE_SHIFT) +\n\t\t\t       (SZ_256K >> PAGE_SHIFT)) << PAGE_SHIFT) - 1;\n\t\tcluster_end = min(cluster_end, last_byte);\n\n\t\tbtrfs_inode_lock(BTRFS_I(inode), 0);\n\t\tif (IS_SWAPFILE(inode)) {\n\t\t\tret = -ETXTBSY;\n\t\t\tbtrfs_inode_unlock(BTRFS_I(inode), 0);\n\t\t\tbreak;\n\t\t}\n\t\tif (!(inode->i_sb->s_flags & SB_ACTIVE)) {\n\t\t\tbtrfs_inode_unlock(BTRFS_I(inode), 0);\n\t\t\tbreak;\n\t\t}\n\t\tif (do_compress)\n\t\t\tBTRFS_I(inode)->defrag_compress = compress_type;\n\t\tret = defrag_one_cluster(BTRFS_I(inode), ra, cur,\n\t\t\t\tcluster_end + 1 - cur, extent_thresh,\n\t\t\t\tnewer_than, do_compress, &sectors_defragged,\n\t\t\t\tmax_to_defrag, &last_scanned);\n\n\t\tif (sectors_defragged > prev_sectors_defragged)\n\t\t\tbalance_dirty_pages_ratelimited(inode->i_mapping);\n\n\t\tbtrfs_inode_unlock(BTRFS_I(inode), 0);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\t\tcur = max(cluster_end + 1, last_scanned);\n\t\tif (ret > 0) {\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t\tcond_resched();\n\t}\n\n\tif (ra_allocated)\n\t\tkfree(ra);\n\t \n\trange->start = cur;\n\tif (sectors_defragged) {\n\t\t \n\t\tif (range->flags & BTRFS_DEFRAG_RANGE_START_IO) {\n\t\t\tfilemap_flush(inode->i_mapping);\n\t\t\tif (test_bit(BTRFS_INODE_HAS_ASYNC_EXTENT,\n\t\t\t\t     &BTRFS_I(inode)->runtime_flags))\n\t\t\t\tfilemap_flush(inode->i_mapping);\n\t\t}\n\t\tif (range->compress_type == BTRFS_COMPRESS_LZO)\n\t\t\tbtrfs_set_fs_incompat(fs_info, COMPRESS_LZO);\n\t\telse if (range->compress_type == BTRFS_COMPRESS_ZSTD)\n\t\t\tbtrfs_set_fs_incompat(fs_info, COMPRESS_ZSTD);\n\t\tret = sectors_defragged;\n\t}\n\tif (do_compress) {\n\t\tbtrfs_inode_lock(BTRFS_I(inode), 0);\n\t\tBTRFS_I(inode)->defrag_compress = BTRFS_COMPRESS_NONE;\n\t\tbtrfs_inode_unlock(BTRFS_I(inode), 0);\n\t}\n\treturn ret;\n}\n\nvoid __cold btrfs_auto_defrag_exit(void)\n{\n\tkmem_cache_destroy(btrfs_inode_defrag_cachep);\n}\n\nint __init btrfs_auto_defrag_init(void)\n{\n\tbtrfs_inode_defrag_cachep = kmem_cache_create(\"btrfs_inode_defrag\",\n\t\t\t\t\tsizeof(struct inode_defrag), 0,\n\t\t\t\t\tSLAB_MEM_SPREAD,\n\t\t\t\t\tNULL);\n\tif (!btrfs_inode_defrag_cachep)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}