{
  "module_name": "delayed-ref.c",
  "hash_id": "03829b44f9b7188df20e78512fb2d1b1fc7a46015eea04456bbd458828db6e23",
  "original_prompt": "Ingested from linux-6.6.14/fs/btrfs/delayed-ref.c",
  "human_readable_source": "\n \n\n#include <linux/sched.h>\n#include <linux/slab.h>\n#include <linux/sort.h>\n#include \"messages.h\"\n#include \"ctree.h\"\n#include \"delayed-ref.h\"\n#include \"transaction.h\"\n#include \"qgroup.h\"\n#include \"space-info.h\"\n#include \"tree-mod-log.h\"\n#include \"fs.h\"\n\nstruct kmem_cache *btrfs_delayed_ref_head_cachep;\nstruct kmem_cache *btrfs_delayed_tree_ref_cachep;\nstruct kmem_cache *btrfs_delayed_data_ref_cachep;\nstruct kmem_cache *btrfs_delayed_extent_op_cachep;\n \n\nbool btrfs_check_space_for_delayed_refs(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_block_rsv *delayed_refs_rsv = &fs_info->delayed_refs_rsv;\n\tstruct btrfs_block_rsv *global_rsv = &fs_info->global_block_rsv;\n\tbool ret = false;\n\tu64 reserved;\n\n\tspin_lock(&global_rsv->lock);\n\treserved = global_rsv->reserved;\n\tspin_unlock(&global_rsv->lock);\n\n\t \n\tspin_lock(&delayed_refs_rsv->lock);\n\treserved += delayed_refs_rsv->reserved;\n\tif (delayed_refs_rsv->size >= reserved)\n\t\tret = true;\n\tspin_unlock(&delayed_refs_rsv->lock);\n\treturn ret;\n}\n\n \nvoid btrfs_delayed_refs_rsv_release(struct btrfs_fs_info *fs_info, int nr)\n{\n\tstruct btrfs_block_rsv *block_rsv = &fs_info->delayed_refs_rsv;\n\tconst u64 num_bytes = btrfs_calc_delayed_ref_bytes(fs_info, nr);\n\tu64 released = 0;\n\n\treleased = btrfs_block_rsv_release(fs_info, block_rsv, num_bytes, NULL);\n\tif (released)\n\t\ttrace_btrfs_space_reservation(fs_info, \"delayed_refs_rsv\",\n\t\t\t\t\t      0, released, 0);\n}\n\n \nvoid btrfs_update_delayed_refs_rsv(struct btrfs_trans_handle *trans)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_block_rsv *delayed_rsv = &fs_info->delayed_refs_rsv;\n\tu64 num_bytes;\n\n\tif (!trans->delayed_ref_updates)\n\t\treturn;\n\n\tnum_bytes = btrfs_calc_delayed_ref_bytes(fs_info,\n\t\t\t\t\t\t trans->delayed_ref_updates);\n\n\tspin_lock(&delayed_rsv->lock);\n\tdelayed_rsv->size += num_bytes;\n\tdelayed_rsv->full = false;\n\tspin_unlock(&delayed_rsv->lock);\n\ttrans->delayed_ref_updates = 0;\n}\n\n \nvoid btrfs_migrate_to_delayed_refs_rsv(struct btrfs_fs_info *fs_info,\n\t\t\t\t       u64 num_bytes)\n{\n\tstruct btrfs_block_rsv *delayed_refs_rsv = &fs_info->delayed_refs_rsv;\n\tu64 to_free = 0;\n\n\tspin_lock(&delayed_refs_rsv->lock);\n\tif (delayed_refs_rsv->size > delayed_refs_rsv->reserved) {\n\t\tu64 delta = delayed_refs_rsv->size -\n\t\t\tdelayed_refs_rsv->reserved;\n\t\tif (num_bytes > delta) {\n\t\t\tto_free = num_bytes - delta;\n\t\t\tnum_bytes = delta;\n\t\t}\n\t} else {\n\t\tto_free = num_bytes;\n\t\tnum_bytes = 0;\n\t}\n\n\tif (num_bytes)\n\t\tdelayed_refs_rsv->reserved += num_bytes;\n\tif (delayed_refs_rsv->reserved >= delayed_refs_rsv->size)\n\t\tdelayed_refs_rsv->full = true;\n\tspin_unlock(&delayed_refs_rsv->lock);\n\n\tif (num_bytes)\n\t\ttrace_btrfs_space_reservation(fs_info, \"delayed_refs_rsv\",\n\t\t\t\t\t      0, num_bytes, 1);\n\tif (to_free)\n\t\tbtrfs_space_info_free_bytes_may_use(fs_info,\n\t\t\t\tdelayed_refs_rsv->space_info, to_free);\n}\n\n \nint btrfs_delayed_refs_rsv_refill(struct btrfs_fs_info *fs_info,\n\t\t\t\t  enum btrfs_reserve_flush_enum flush)\n{\n\tstruct btrfs_block_rsv *block_rsv = &fs_info->delayed_refs_rsv;\n\tu64 limit = btrfs_calc_delayed_ref_bytes(fs_info, 1);\n\tu64 num_bytes = 0;\n\tu64 refilled_bytes;\n\tu64 to_free;\n\tint ret = -ENOSPC;\n\n\tspin_lock(&block_rsv->lock);\n\tif (block_rsv->reserved < block_rsv->size) {\n\t\tnum_bytes = block_rsv->size - block_rsv->reserved;\n\t\tnum_bytes = min(num_bytes, limit);\n\t}\n\tspin_unlock(&block_rsv->lock);\n\n\tif (!num_bytes)\n\t\treturn 0;\n\n\tret = btrfs_reserve_metadata_bytes(fs_info, block_rsv, num_bytes, flush);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tspin_lock(&block_rsv->lock);\n\tif (block_rsv->reserved < block_rsv->size) {\n\t\tu64 needed = block_rsv->size - block_rsv->reserved;\n\n\t\tif (num_bytes >= needed) {\n\t\t\tblock_rsv->reserved += needed;\n\t\t\tblock_rsv->full = true;\n\t\t\tto_free = num_bytes - needed;\n\t\t\trefilled_bytes = needed;\n\t\t} else {\n\t\t\tblock_rsv->reserved += num_bytes;\n\t\t\tto_free = 0;\n\t\t\trefilled_bytes = num_bytes;\n\t\t}\n\t} else {\n\t\tto_free = num_bytes;\n\t\trefilled_bytes = 0;\n\t}\n\tspin_unlock(&block_rsv->lock);\n\n\tif (to_free > 0)\n\t\tbtrfs_space_info_free_bytes_may_use(fs_info, block_rsv->space_info,\n\t\t\t\t\t\t    to_free);\n\n\tif (refilled_bytes > 0)\n\t\ttrace_btrfs_space_reservation(fs_info, \"delayed_refs_rsv\", 0,\n\t\t\t\t\t      refilled_bytes, 1);\n\treturn 0;\n}\n\n \nstatic int comp_tree_refs(struct btrfs_delayed_tree_ref *ref1,\n\t\t\t  struct btrfs_delayed_tree_ref *ref2)\n{\n\tif (ref1->node.type == BTRFS_TREE_BLOCK_REF_KEY) {\n\t\tif (ref1->root < ref2->root)\n\t\t\treturn -1;\n\t\tif (ref1->root > ref2->root)\n\t\t\treturn 1;\n\t} else {\n\t\tif (ref1->parent < ref2->parent)\n\t\t\treturn -1;\n\t\tif (ref1->parent > ref2->parent)\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}\n\n \nstatic int comp_data_refs(struct btrfs_delayed_data_ref *ref1,\n\t\t\t  struct btrfs_delayed_data_ref *ref2)\n{\n\tif (ref1->node.type == BTRFS_EXTENT_DATA_REF_KEY) {\n\t\tif (ref1->root < ref2->root)\n\t\t\treturn -1;\n\t\tif (ref1->root > ref2->root)\n\t\t\treturn 1;\n\t\tif (ref1->objectid < ref2->objectid)\n\t\t\treturn -1;\n\t\tif (ref1->objectid > ref2->objectid)\n\t\t\treturn 1;\n\t\tif (ref1->offset < ref2->offset)\n\t\t\treturn -1;\n\t\tif (ref1->offset > ref2->offset)\n\t\t\treturn 1;\n\t} else {\n\t\tif (ref1->parent < ref2->parent)\n\t\t\treturn -1;\n\t\tif (ref1->parent > ref2->parent)\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic int comp_refs(struct btrfs_delayed_ref_node *ref1,\n\t\t     struct btrfs_delayed_ref_node *ref2,\n\t\t     bool check_seq)\n{\n\tint ret = 0;\n\n\tif (ref1->type < ref2->type)\n\t\treturn -1;\n\tif (ref1->type > ref2->type)\n\t\treturn 1;\n\tif (ref1->type == BTRFS_TREE_BLOCK_REF_KEY ||\n\t    ref1->type == BTRFS_SHARED_BLOCK_REF_KEY)\n\t\tret = comp_tree_refs(btrfs_delayed_node_to_tree_ref(ref1),\n\t\t\t\t     btrfs_delayed_node_to_tree_ref(ref2));\n\telse\n\t\tret = comp_data_refs(btrfs_delayed_node_to_data_ref(ref1),\n\t\t\t\t     btrfs_delayed_node_to_data_ref(ref2));\n\tif (ret)\n\t\treturn ret;\n\tif (check_seq) {\n\t\tif (ref1->seq < ref2->seq)\n\t\t\treturn -1;\n\t\tif (ref1->seq > ref2->seq)\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}\n\n \nstatic struct btrfs_delayed_ref_head *htree_insert(struct rb_root_cached *root,\n\t\t\t\t\t\t   struct rb_node *node)\n{\n\tstruct rb_node **p = &root->rb_root.rb_node;\n\tstruct rb_node *parent_node = NULL;\n\tstruct btrfs_delayed_ref_head *entry;\n\tstruct btrfs_delayed_ref_head *ins;\n\tu64 bytenr;\n\tbool leftmost = true;\n\n\tins = rb_entry(node, struct btrfs_delayed_ref_head, href_node);\n\tbytenr = ins->bytenr;\n\twhile (*p) {\n\t\tparent_node = *p;\n\t\tentry = rb_entry(parent_node, struct btrfs_delayed_ref_head,\n\t\t\t\t href_node);\n\n\t\tif (bytenr < entry->bytenr) {\n\t\t\tp = &(*p)->rb_left;\n\t\t} else if (bytenr > entry->bytenr) {\n\t\t\tp = &(*p)->rb_right;\n\t\t\tleftmost = false;\n\t\t} else {\n\t\t\treturn entry;\n\t\t}\n\t}\n\n\trb_link_node(node, parent_node, p);\n\trb_insert_color_cached(node, root, leftmost);\n\treturn NULL;\n}\n\nstatic struct btrfs_delayed_ref_node* tree_insert(struct rb_root_cached *root,\n\t\tstruct btrfs_delayed_ref_node *ins)\n{\n\tstruct rb_node **p = &root->rb_root.rb_node;\n\tstruct rb_node *node = &ins->ref_node;\n\tstruct rb_node *parent_node = NULL;\n\tstruct btrfs_delayed_ref_node *entry;\n\tbool leftmost = true;\n\n\twhile (*p) {\n\t\tint comp;\n\n\t\tparent_node = *p;\n\t\tentry = rb_entry(parent_node, struct btrfs_delayed_ref_node,\n\t\t\t\t ref_node);\n\t\tcomp = comp_refs(ins, entry, true);\n\t\tif (comp < 0) {\n\t\t\tp = &(*p)->rb_left;\n\t\t} else if (comp > 0) {\n\t\t\tp = &(*p)->rb_right;\n\t\t\tleftmost = false;\n\t\t} else {\n\t\t\treturn entry;\n\t\t}\n\t}\n\n\trb_link_node(node, parent_node, p);\n\trb_insert_color_cached(node, root, leftmost);\n\treturn NULL;\n}\n\nstatic struct btrfs_delayed_ref_head *find_first_ref_head(\n\t\tstruct btrfs_delayed_ref_root *dr)\n{\n\tstruct rb_node *n;\n\tstruct btrfs_delayed_ref_head *entry;\n\n\tn = rb_first_cached(&dr->href_root);\n\tif (!n)\n\t\treturn NULL;\n\n\tentry = rb_entry(n, struct btrfs_delayed_ref_head, href_node);\n\n\treturn entry;\n}\n\n \nstatic struct btrfs_delayed_ref_head *find_ref_head(\n\t\tstruct btrfs_delayed_ref_root *dr, u64 bytenr,\n\t\tbool return_bigger)\n{\n\tstruct rb_root *root = &dr->href_root.rb_root;\n\tstruct rb_node *n;\n\tstruct btrfs_delayed_ref_head *entry;\n\n\tn = root->rb_node;\n\tentry = NULL;\n\twhile (n) {\n\t\tentry = rb_entry(n, struct btrfs_delayed_ref_head, href_node);\n\n\t\tif (bytenr < entry->bytenr)\n\t\t\tn = n->rb_left;\n\t\telse if (bytenr > entry->bytenr)\n\t\t\tn = n->rb_right;\n\t\telse\n\t\t\treturn entry;\n\t}\n\tif (entry && return_bigger) {\n\t\tif (bytenr > entry->bytenr) {\n\t\t\tn = rb_next(&entry->href_node);\n\t\t\tif (!n)\n\t\t\t\treturn NULL;\n\t\t\tentry = rb_entry(n, struct btrfs_delayed_ref_head,\n\t\t\t\t\t href_node);\n\t\t}\n\t\treturn entry;\n\t}\n\treturn NULL;\n}\n\nint btrfs_delayed_ref_lock(struct btrfs_delayed_ref_root *delayed_refs,\n\t\t\t   struct btrfs_delayed_ref_head *head)\n{\n\tlockdep_assert_held(&delayed_refs->lock);\n\tif (mutex_trylock(&head->mutex))\n\t\treturn 0;\n\n\trefcount_inc(&head->refs);\n\tspin_unlock(&delayed_refs->lock);\n\n\tmutex_lock(&head->mutex);\n\tspin_lock(&delayed_refs->lock);\n\tif (RB_EMPTY_NODE(&head->href_node)) {\n\t\tmutex_unlock(&head->mutex);\n\t\tbtrfs_put_delayed_ref_head(head);\n\t\treturn -EAGAIN;\n\t}\n\tbtrfs_put_delayed_ref_head(head);\n\treturn 0;\n}\n\nstatic inline void drop_delayed_ref(struct btrfs_delayed_ref_root *delayed_refs,\n\t\t\t\t    struct btrfs_delayed_ref_head *head,\n\t\t\t\t    struct btrfs_delayed_ref_node *ref)\n{\n\tlockdep_assert_held(&head->lock);\n\trb_erase_cached(&ref->ref_node, &head->ref_tree);\n\tRB_CLEAR_NODE(&ref->ref_node);\n\tif (!list_empty(&ref->add_list))\n\t\tlist_del(&ref->add_list);\n\tbtrfs_put_delayed_ref(ref);\n\tatomic_dec(&delayed_refs->num_entries);\n}\n\nstatic bool merge_ref(struct btrfs_delayed_ref_root *delayed_refs,\n\t\t      struct btrfs_delayed_ref_head *head,\n\t\t      struct btrfs_delayed_ref_node *ref,\n\t\t      u64 seq)\n{\n\tstruct btrfs_delayed_ref_node *next;\n\tstruct rb_node *node = rb_next(&ref->ref_node);\n\tbool done = false;\n\n\twhile (!done && node) {\n\t\tint mod;\n\n\t\tnext = rb_entry(node, struct btrfs_delayed_ref_node, ref_node);\n\t\tnode = rb_next(node);\n\t\tif (seq && next->seq >= seq)\n\t\t\tbreak;\n\t\tif (comp_refs(ref, next, false))\n\t\t\tbreak;\n\n\t\tif (ref->action == next->action) {\n\t\t\tmod = next->ref_mod;\n\t\t} else {\n\t\t\tif (ref->ref_mod < next->ref_mod) {\n\t\t\t\tswap(ref, next);\n\t\t\t\tdone = true;\n\t\t\t}\n\t\t\tmod = -next->ref_mod;\n\t\t}\n\n\t\tdrop_delayed_ref(delayed_refs, head, next);\n\t\tref->ref_mod += mod;\n\t\tif (ref->ref_mod == 0) {\n\t\t\tdrop_delayed_ref(delayed_refs, head, ref);\n\t\t\tdone = true;\n\t\t} else {\n\t\t\t \n\t\t\tWARN_ON(ref->type == BTRFS_TREE_BLOCK_REF_KEY ||\n\t\t\t\tref->type == BTRFS_SHARED_BLOCK_REF_KEY);\n\t\t}\n\t}\n\n\treturn done;\n}\n\nvoid btrfs_merge_delayed_refs(struct btrfs_fs_info *fs_info,\n\t\t\t      struct btrfs_delayed_ref_root *delayed_refs,\n\t\t\t      struct btrfs_delayed_ref_head *head)\n{\n\tstruct btrfs_delayed_ref_node *ref;\n\tstruct rb_node *node;\n\tu64 seq = 0;\n\n\tlockdep_assert_held(&head->lock);\n\n\tif (RB_EMPTY_ROOT(&head->ref_tree.rb_root))\n\t\treturn;\n\n\t \n\tif (head->is_data)\n\t\treturn;\n\n\tseq = btrfs_tree_mod_log_lowest_seq(fs_info);\nagain:\n\tfor (node = rb_first_cached(&head->ref_tree); node;\n\t     node = rb_next(node)) {\n\t\tref = rb_entry(node, struct btrfs_delayed_ref_node, ref_node);\n\t\tif (seq && ref->seq >= seq)\n\t\t\tcontinue;\n\t\tif (merge_ref(delayed_refs, head, ref, seq))\n\t\t\tgoto again;\n\t}\n}\n\nint btrfs_check_delayed_seq(struct btrfs_fs_info *fs_info, u64 seq)\n{\n\tint ret = 0;\n\tu64 min_seq = btrfs_tree_mod_log_lowest_seq(fs_info);\n\n\tif (min_seq != 0 && seq >= min_seq) {\n\t\tbtrfs_debug(fs_info,\n\t\t\t    \"holding back delayed_ref %llu, lowest is %llu\",\n\t\t\t    seq, min_seq);\n\t\tret = 1;\n\t}\n\n\treturn ret;\n}\n\nstruct btrfs_delayed_ref_head *btrfs_select_ref_head(\n\t\tstruct btrfs_delayed_ref_root *delayed_refs)\n{\n\tstruct btrfs_delayed_ref_head *head;\n\n\tlockdep_assert_held(&delayed_refs->lock);\nagain:\n\thead = find_ref_head(delayed_refs, delayed_refs->run_delayed_start,\n\t\t\t     true);\n\tif (!head && delayed_refs->run_delayed_start != 0) {\n\t\tdelayed_refs->run_delayed_start = 0;\n\t\thead = find_first_ref_head(delayed_refs);\n\t}\n\tif (!head)\n\t\treturn NULL;\n\n\twhile (head->processing) {\n\t\tstruct rb_node *node;\n\n\t\tnode = rb_next(&head->href_node);\n\t\tif (!node) {\n\t\t\tif (delayed_refs->run_delayed_start == 0)\n\t\t\t\treturn NULL;\n\t\t\tdelayed_refs->run_delayed_start = 0;\n\t\t\tgoto again;\n\t\t}\n\t\thead = rb_entry(node, struct btrfs_delayed_ref_head,\n\t\t\t\thref_node);\n\t}\n\n\thead->processing = true;\n\tWARN_ON(delayed_refs->num_heads_ready == 0);\n\tdelayed_refs->num_heads_ready--;\n\tdelayed_refs->run_delayed_start = head->bytenr +\n\t\thead->num_bytes;\n\treturn head;\n}\n\nvoid btrfs_delete_ref_head(struct btrfs_delayed_ref_root *delayed_refs,\n\t\t\t   struct btrfs_delayed_ref_head *head)\n{\n\tlockdep_assert_held(&delayed_refs->lock);\n\tlockdep_assert_held(&head->lock);\n\n\trb_erase_cached(&head->href_node, &delayed_refs->href_root);\n\tRB_CLEAR_NODE(&head->href_node);\n\tatomic_dec(&delayed_refs->num_entries);\n\tdelayed_refs->num_heads--;\n\tif (!head->processing)\n\t\tdelayed_refs->num_heads_ready--;\n}\n\n \nstatic bool insert_delayed_ref(struct btrfs_delayed_ref_root *root,\n\t\t\t       struct btrfs_delayed_ref_head *href,\n\t\t\t       struct btrfs_delayed_ref_node *ref)\n{\n\tstruct btrfs_delayed_ref_node *exist;\n\tint mod;\n\n\tspin_lock(&href->lock);\n\texist = tree_insert(&href->ref_tree, ref);\n\tif (!exist) {\n\t\tif (ref->action == BTRFS_ADD_DELAYED_REF)\n\t\t\tlist_add_tail(&ref->add_list, &href->ref_add_list);\n\t\tatomic_inc(&root->num_entries);\n\t\tspin_unlock(&href->lock);\n\t\treturn false;\n\t}\n\n\t \n\tif (exist->action == ref->action) {\n\t\tmod = ref->ref_mod;\n\t} else {\n\t\t \n\t\tif (exist->ref_mod < ref->ref_mod) {\n\t\t\texist->action = ref->action;\n\t\t\tmod = -exist->ref_mod;\n\t\t\texist->ref_mod = ref->ref_mod;\n\t\t\tif (ref->action == BTRFS_ADD_DELAYED_REF)\n\t\t\t\tlist_add_tail(&exist->add_list,\n\t\t\t\t\t      &href->ref_add_list);\n\t\t\telse if (ref->action == BTRFS_DROP_DELAYED_REF) {\n\t\t\t\tASSERT(!list_empty(&exist->add_list));\n\t\t\t\tlist_del(&exist->add_list);\n\t\t\t} else {\n\t\t\t\tASSERT(0);\n\t\t\t}\n\t\t} else\n\t\t\tmod = -ref->ref_mod;\n\t}\n\texist->ref_mod += mod;\n\n\t \n\tif (exist->ref_mod == 0)\n\t\tdrop_delayed_ref(root, href, exist);\n\tspin_unlock(&href->lock);\n\treturn true;\n}\n\n \nstatic noinline void update_existing_head_ref(struct btrfs_trans_handle *trans,\n\t\t\t struct btrfs_delayed_ref_head *existing,\n\t\t\t struct btrfs_delayed_ref_head *update)\n{\n\tstruct btrfs_delayed_ref_root *delayed_refs =\n\t\t&trans->transaction->delayed_refs;\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tint old_ref_mod;\n\n\tBUG_ON(existing->is_data != update->is_data);\n\n\tspin_lock(&existing->lock);\n\tif (update->must_insert_reserved) {\n\t\t \n\t\texisting->must_insert_reserved = update->must_insert_reserved;\n\n\t\t \n\t\texisting->num_bytes = update->num_bytes;\n\n\t}\n\n\tif (update->extent_op) {\n\t\tif (!existing->extent_op) {\n\t\t\texisting->extent_op = update->extent_op;\n\t\t} else {\n\t\t\tif (update->extent_op->update_key) {\n\t\t\t\tmemcpy(&existing->extent_op->key,\n\t\t\t\t       &update->extent_op->key,\n\t\t\t\t       sizeof(update->extent_op->key));\n\t\t\t\texisting->extent_op->update_key = true;\n\t\t\t}\n\t\t\tif (update->extent_op->update_flags) {\n\t\t\t\texisting->extent_op->flags_to_set |=\n\t\t\t\t\tupdate->extent_op->flags_to_set;\n\t\t\t\texisting->extent_op->update_flags = true;\n\t\t\t}\n\t\t\tbtrfs_free_delayed_extent_op(update->extent_op);\n\t\t}\n\t}\n\t \n\told_ref_mod = existing->total_ref_mod;\n\texisting->ref_mod += update->ref_mod;\n\texisting->total_ref_mod += update->ref_mod;\n\n\t \n\tif (existing->is_data) {\n\t\tu64 csum_leaves =\n\t\t\tbtrfs_csum_bytes_to_leaves(fs_info,\n\t\t\t\t\t\t   existing->num_bytes);\n\n\t\tif (existing->total_ref_mod >= 0 && old_ref_mod < 0) {\n\t\t\tdelayed_refs->pending_csums -= existing->num_bytes;\n\t\t\tbtrfs_delayed_refs_rsv_release(fs_info, csum_leaves);\n\t\t}\n\t\tif (existing->total_ref_mod < 0 && old_ref_mod >= 0) {\n\t\t\tdelayed_refs->pending_csums += existing->num_bytes;\n\t\t\ttrans->delayed_ref_updates += csum_leaves;\n\t\t}\n\t}\n\n\tspin_unlock(&existing->lock);\n}\n\nstatic void init_delayed_ref_head(struct btrfs_delayed_ref_head *head_ref,\n\t\t\t\t  struct btrfs_qgroup_extent_record *qrecord,\n\t\t\t\t  u64 bytenr, u64 num_bytes, u64 ref_root,\n\t\t\t\t  u64 reserved, int action, bool is_data,\n\t\t\t\t  bool is_system)\n{\n\tint count_mod = 1;\n\tbool must_insert_reserved = false;\n\n\t \n\tBUG_ON(!is_data && reserved);\n\n\tswitch (action) {\n\tcase BTRFS_UPDATE_DELAYED_HEAD:\n\t\tcount_mod = 0;\n\t\tbreak;\n\tcase BTRFS_DROP_DELAYED_REF:\n\t\t \n\t\tcount_mod = -1;\n\t\tbreak;\n\tcase BTRFS_ADD_DELAYED_EXTENT:\n\t\t \n\t\tmust_insert_reserved = true;\n\t\tbreak;\n\t}\n\n\trefcount_set(&head_ref->refs, 1);\n\thead_ref->bytenr = bytenr;\n\thead_ref->num_bytes = num_bytes;\n\thead_ref->ref_mod = count_mod;\n\thead_ref->must_insert_reserved = must_insert_reserved;\n\thead_ref->is_data = is_data;\n\thead_ref->is_system = is_system;\n\thead_ref->ref_tree = RB_ROOT_CACHED;\n\tINIT_LIST_HEAD(&head_ref->ref_add_list);\n\tRB_CLEAR_NODE(&head_ref->href_node);\n\thead_ref->processing = false;\n\thead_ref->total_ref_mod = count_mod;\n\tspin_lock_init(&head_ref->lock);\n\tmutex_init(&head_ref->mutex);\n\n\tif (qrecord) {\n\t\tif (ref_root && reserved) {\n\t\t\tqrecord->data_rsv = reserved;\n\t\t\tqrecord->data_rsv_refroot = ref_root;\n\t\t}\n\t\tqrecord->bytenr = bytenr;\n\t\tqrecord->num_bytes = num_bytes;\n\t\tqrecord->old_roots = NULL;\n\t}\n}\n\n \nstatic noinline struct btrfs_delayed_ref_head *\nadd_delayed_ref_head(struct btrfs_trans_handle *trans,\n\t\t     struct btrfs_delayed_ref_head *head_ref,\n\t\t     struct btrfs_qgroup_extent_record *qrecord,\n\t\t     int action, bool *qrecord_inserted_ret)\n{\n\tstruct btrfs_delayed_ref_head *existing;\n\tstruct btrfs_delayed_ref_root *delayed_refs;\n\tbool qrecord_inserted = false;\n\n\tdelayed_refs = &trans->transaction->delayed_refs;\n\n\t \n\tif (qrecord) {\n\t\tif (btrfs_qgroup_trace_extent_nolock(trans->fs_info,\n\t\t\t\t\tdelayed_refs, qrecord))\n\t\t\tkfree(qrecord);\n\t\telse\n\t\t\tqrecord_inserted = true;\n\t}\n\n\ttrace_add_delayed_ref_head(trans->fs_info, head_ref, action);\n\n\texisting = htree_insert(&delayed_refs->href_root,\n\t\t\t\t&head_ref->href_node);\n\tif (existing) {\n\t\tupdate_existing_head_ref(trans, existing, head_ref);\n\t\t \n\t\tkmem_cache_free(btrfs_delayed_ref_head_cachep, head_ref);\n\t\thead_ref = existing;\n\t} else {\n\t\tif (head_ref->is_data && head_ref->ref_mod < 0) {\n\t\t\tdelayed_refs->pending_csums += head_ref->num_bytes;\n\t\t\ttrans->delayed_ref_updates +=\n\t\t\t\tbtrfs_csum_bytes_to_leaves(trans->fs_info,\n\t\t\t\t\t\t\t   head_ref->num_bytes);\n\t\t}\n\t\tdelayed_refs->num_heads++;\n\t\tdelayed_refs->num_heads_ready++;\n\t\tatomic_inc(&delayed_refs->num_entries);\n\t\ttrans->delayed_ref_updates++;\n\t}\n\tif (qrecord_inserted_ret)\n\t\t*qrecord_inserted_ret = qrecord_inserted;\n\n\treturn head_ref;\n}\n\n \nstatic void init_delayed_ref_common(struct btrfs_fs_info *fs_info,\n\t\t\t\t    struct btrfs_delayed_ref_node *ref,\n\t\t\t\t    u64 bytenr, u64 num_bytes, u64 ref_root,\n\t\t\t\t    int action, u8 ref_type)\n{\n\tu64 seq = 0;\n\n\tif (action == BTRFS_ADD_DELAYED_EXTENT)\n\t\taction = BTRFS_ADD_DELAYED_REF;\n\n\tif (is_fstree(ref_root))\n\t\tseq = atomic64_read(&fs_info->tree_mod_seq);\n\n\trefcount_set(&ref->refs, 1);\n\tref->bytenr = bytenr;\n\tref->num_bytes = num_bytes;\n\tref->ref_mod = 1;\n\tref->action = action;\n\tref->seq = seq;\n\tref->type = ref_type;\n\tRB_CLEAR_NODE(&ref->ref_node);\n\tINIT_LIST_HEAD(&ref->add_list);\n}\n\n \nint btrfs_add_delayed_tree_ref(struct btrfs_trans_handle *trans,\n\t\t\t       struct btrfs_ref *generic_ref,\n\t\t\t       struct btrfs_delayed_extent_op *extent_op)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_delayed_tree_ref *ref;\n\tstruct btrfs_delayed_ref_head *head_ref;\n\tstruct btrfs_delayed_ref_root *delayed_refs;\n\tstruct btrfs_qgroup_extent_record *record = NULL;\n\tbool qrecord_inserted;\n\tbool is_system;\n\tbool merged;\n\tint action = generic_ref->action;\n\tint level = generic_ref->tree_ref.level;\n\tu64 bytenr = generic_ref->bytenr;\n\tu64 num_bytes = generic_ref->len;\n\tu64 parent = generic_ref->parent;\n\tu8 ref_type;\n\n\tis_system = (generic_ref->tree_ref.owning_root == BTRFS_CHUNK_TREE_OBJECTID);\n\n\tASSERT(generic_ref->type == BTRFS_REF_METADATA && generic_ref->action);\n\tref = kmem_cache_alloc(btrfs_delayed_tree_ref_cachep, GFP_NOFS);\n\tif (!ref)\n\t\treturn -ENOMEM;\n\n\thead_ref = kmem_cache_alloc(btrfs_delayed_ref_head_cachep, GFP_NOFS);\n\tif (!head_ref) {\n\t\tkmem_cache_free(btrfs_delayed_tree_ref_cachep, ref);\n\t\treturn -ENOMEM;\n\t}\n\n\tif (test_bit(BTRFS_FS_QUOTA_ENABLED, &fs_info->flags) &&\n\t    !generic_ref->skip_qgroup) {\n\t\trecord = kzalloc(sizeof(*record), GFP_NOFS);\n\t\tif (!record) {\n\t\t\tkmem_cache_free(btrfs_delayed_tree_ref_cachep, ref);\n\t\t\tkmem_cache_free(btrfs_delayed_ref_head_cachep, head_ref);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\tif (parent)\n\t\tref_type = BTRFS_SHARED_BLOCK_REF_KEY;\n\telse\n\t\tref_type = BTRFS_TREE_BLOCK_REF_KEY;\n\n\tinit_delayed_ref_common(fs_info, &ref->node, bytenr, num_bytes,\n\t\t\t\tgeneric_ref->tree_ref.owning_root, action,\n\t\t\t\tref_type);\n\tref->root = generic_ref->tree_ref.owning_root;\n\tref->parent = parent;\n\tref->level = level;\n\n\tinit_delayed_ref_head(head_ref, record, bytenr, num_bytes,\n\t\t\t      generic_ref->tree_ref.owning_root, 0, action,\n\t\t\t      false, is_system);\n\thead_ref->extent_op = extent_op;\n\n\tdelayed_refs = &trans->transaction->delayed_refs;\n\tspin_lock(&delayed_refs->lock);\n\n\t \n\thead_ref = add_delayed_ref_head(trans, head_ref, record,\n\t\t\t\t\taction, &qrecord_inserted);\n\n\tmerged = insert_delayed_ref(delayed_refs, head_ref, &ref->node);\n\tspin_unlock(&delayed_refs->lock);\n\n\t \n\tbtrfs_update_delayed_refs_rsv(trans);\n\n\ttrace_add_delayed_tree_ref(fs_info, &ref->node, ref,\n\t\t\t\t   action == BTRFS_ADD_DELAYED_EXTENT ?\n\t\t\t\t   BTRFS_ADD_DELAYED_REF : action);\n\tif (merged)\n\t\tkmem_cache_free(btrfs_delayed_tree_ref_cachep, ref);\n\n\tif (qrecord_inserted)\n\t\tbtrfs_qgroup_trace_extent_post(trans, record);\n\n\treturn 0;\n}\n\n \nint btrfs_add_delayed_data_ref(struct btrfs_trans_handle *trans,\n\t\t\t       struct btrfs_ref *generic_ref,\n\t\t\t       u64 reserved)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_delayed_data_ref *ref;\n\tstruct btrfs_delayed_ref_head *head_ref;\n\tstruct btrfs_delayed_ref_root *delayed_refs;\n\tstruct btrfs_qgroup_extent_record *record = NULL;\n\tbool qrecord_inserted;\n\tint action = generic_ref->action;\n\tbool merged;\n\tu64 bytenr = generic_ref->bytenr;\n\tu64 num_bytes = generic_ref->len;\n\tu64 parent = generic_ref->parent;\n\tu64 ref_root = generic_ref->data_ref.owning_root;\n\tu64 owner = generic_ref->data_ref.ino;\n\tu64 offset = generic_ref->data_ref.offset;\n\tu8 ref_type;\n\n\tASSERT(generic_ref->type == BTRFS_REF_DATA && action);\n\tref = kmem_cache_alloc(btrfs_delayed_data_ref_cachep, GFP_NOFS);\n\tif (!ref)\n\t\treturn -ENOMEM;\n\n\tif (parent)\n\t        ref_type = BTRFS_SHARED_DATA_REF_KEY;\n\telse\n\t        ref_type = BTRFS_EXTENT_DATA_REF_KEY;\n\tinit_delayed_ref_common(fs_info, &ref->node, bytenr, num_bytes,\n\t\t\t\tref_root, action, ref_type);\n\tref->root = ref_root;\n\tref->parent = parent;\n\tref->objectid = owner;\n\tref->offset = offset;\n\n\n\thead_ref = kmem_cache_alloc(btrfs_delayed_ref_head_cachep, GFP_NOFS);\n\tif (!head_ref) {\n\t\tkmem_cache_free(btrfs_delayed_data_ref_cachep, ref);\n\t\treturn -ENOMEM;\n\t}\n\n\tif (test_bit(BTRFS_FS_QUOTA_ENABLED, &fs_info->flags) &&\n\t    !generic_ref->skip_qgroup) {\n\t\trecord = kzalloc(sizeof(*record), GFP_NOFS);\n\t\tif (!record) {\n\t\t\tkmem_cache_free(btrfs_delayed_data_ref_cachep, ref);\n\t\t\tkmem_cache_free(btrfs_delayed_ref_head_cachep,\n\t\t\t\t\thead_ref);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\tinit_delayed_ref_head(head_ref, record, bytenr, num_bytes, ref_root,\n\t\t\t      reserved, action, true, false);\n\thead_ref->extent_op = NULL;\n\n\tdelayed_refs = &trans->transaction->delayed_refs;\n\tspin_lock(&delayed_refs->lock);\n\n\t \n\thead_ref = add_delayed_ref_head(trans, head_ref, record,\n\t\t\t\t\taction, &qrecord_inserted);\n\n\tmerged = insert_delayed_ref(delayed_refs, head_ref, &ref->node);\n\tspin_unlock(&delayed_refs->lock);\n\n\t \n\tbtrfs_update_delayed_refs_rsv(trans);\n\n\ttrace_add_delayed_data_ref(trans->fs_info, &ref->node, ref,\n\t\t\t\t   action == BTRFS_ADD_DELAYED_EXTENT ?\n\t\t\t\t   BTRFS_ADD_DELAYED_REF : action);\n\tif (merged)\n\t\tkmem_cache_free(btrfs_delayed_data_ref_cachep, ref);\n\n\n\tif (qrecord_inserted)\n\t\treturn btrfs_qgroup_trace_extent_post(trans, record);\n\treturn 0;\n}\n\nint btrfs_add_delayed_extent_op(struct btrfs_trans_handle *trans,\n\t\t\t\tu64 bytenr, u64 num_bytes,\n\t\t\t\tstruct btrfs_delayed_extent_op *extent_op)\n{\n\tstruct btrfs_delayed_ref_head *head_ref;\n\tstruct btrfs_delayed_ref_root *delayed_refs;\n\n\thead_ref = kmem_cache_alloc(btrfs_delayed_ref_head_cachep, GFP_NOFS);\n\tif (!head_ref)\n\t\treturn -ENOMEM;\n\n\tinit_delayed_ref_head(head_ref, NULL, bytenr, num_bytes, 0, 0,\n\t\t\t      BTRFS_UPDATE_DELAYED_HEAD, false, false);\n\thead_ref->extent_op = extent_op;\n\n\tdelayed_refs = &trans->transaction->delayed_refs;\n\tspin_lock(&delayed_refs->lock);\n\n\tadd_delayed_ref_head(trans, head_ref, NULL, BTRFS_UPDATE_DELAYED_HEAD,\n\t\t\t     NULL);\n\n\tspin_unlock(&delayed_refs->lock);\n\n\t \n\tbtrfs_update_delayed_refs_rsv(trans);\n\treturn 0;\n}\n\n \nstruct btrfs_delayed_ref_head *\nbtrfs_find_delayed_ref_head(struct btrfs_delayed_ref_root *delayed_refs, u64 bytenr)\n{\n\tlockdep_assert_held(&delayed_refs->lock);\n\n\treturn find_ref_head(delayed_refs, bytenr, false);\n}\n\nvoid __cold btrfs_delayed_ref_exit(void)\n{\n\tkmem_cache_destroy(btrfs_delayed_ref_head_cachep);\n\tkmem_cache_destroy(btrfs_delayed_tree_ref_cachep);\n\tkmem_cache_destroy(btrfs_delayed_data_ref_cachep);\n\tkmem_cache_destroy(btrfs_delayed_extent_op_cachep);\n}\n\nint __init btrfs_delayed_ref_init(void)\n{\n\tbtrfs_delayed_ref_head_cachep = kmem_cache_create(\n\t\t\t\t\"btrfs_delayed_ref_head\",\n\t\t\t\tsizeof(struct btrfs_delayed_ref_head), 0,\n\t\t\t\tSLAB_MEM_SPREAD, NULL);\n\tif (!btrfs_delayed_ref_head_cachep)\n\t\tgoto fail;\n\n\tbtrfs_delayed_tree_ref_cachep = kmem_cache_create(\n\t\t\t\t\"btrfs_delayed_tree_ref\",\n\t\t\t\tsizeof(struct btrfs_delayed_tree_ref), 0,\n\t\t\t\tSLAB_MEM_SPREAD, NULL);\n\tif (!btrfs_delayed_tree_ref_cachep)\n\t\tgoto fail;\n\n\tbtrfs_delayed_data_ref_cachep = kmem_cache_create(\n\t\t\t\t\"btrfs_delayed_data_ref\",\n\t\t\t\tsizeof(struct btrfs_delayed_data_ref), 0,\n\t\t\t\tSLAB_MEM_SPREAD, NULL);\n\tif (!btrfs_delayed_data_ref_cachep)\n\t\tgoto fail;\n\n\tbtrfs_delayed_extent_op_cachep = kmem_cache_create(\n\t\t\t\t\"btrfs_delayed_extent_op\",\n\t\t\t\tsizeof(struct btrfs_delayed_extent_op), 0,\n\t\t\t\tSLAB_MEM_SPREAD, NULL);\n\tif (!btrfs_delayed_extent_op_cachep)\n\t\tgoto fail;\n\n\treturn 0;\nfail:\n\tbtrfs_delayed_ref_exit();\n\treturn -ENOMEM;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}