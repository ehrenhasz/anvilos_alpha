{
  "module_name": "volumes.c",
  "hash_id": "42d6910259dbe73c2477122e10c7528f5a16914d332da60ce26bf00f3c4a5357",
  "original_prompt": "Ingested from linux-6.6.14/fs/btrfs/volumes.c",
  "human_readable_source": "\n \n\n#include <linux/sched.h>\n#include <linux/sched/mm.h>\n#include <linux/slab.h>\n#include <linux/ratelimit.h>\n#include <linux/kthread.h>\n#include <linux/semaphore.h>\n#include <linux/uuid.h>\n#include <linux/list_sort.h>\n#include <linux/namei.h>\n#include \"misc.h\"\n#include \"ctree.h\"\n#include \"extent_map.h\"\n#include \"disk-io.h\"\n#include \"transaction.h\"\n#include \"print-tree.h\"\n#include \"volumes.h\"\n#include \"raid56.h\"\n#include \"rcu-string.h\"\n#include \"dev-replace.h\"\n#include \"sysfs.h\"\n#include \"tree-checker.h\"\n#include \"space-info.h\"\n#include \"block-group.h\"\n#include \"discard.h\"\n#include \"zoned.h\"\n#include \"fs.h\"\n#include \"accessors.h\"\n#include \"uuid-tree.h\"\n#include \"ioctl.h\"\n#include \"relocation.h\"\n#include \"scrub.h\"\n#include \"super.h\"\n\n#define BTRFS_BLOCK_GROUP_STRIPE_MASK\t(BTRFS_BLOCK_GROUP_RAID0 | \\\n\t\t\t\t\t BTRFS_BLOCK_GROUP_RAID10 | \\\n\t\t\t\t\t BTRFS_BLOCK_GROUP_RAID56_MASK)\n\nconst struct btrfs_raid_attr btrfs_raid_array[BTRFS_NR_RAID_TYPES] = {\n\t[BTRFS_RAID_RAID10] = {\n\t\t.sub_stripes\t= 2,\n\t\t.dev_stripes\t= 1,\n\t\t.devs_max\t= 0,\t \n\t\t.devs_min\t= 2,\n\t\t.tolerated_failures = 1,\n\t\t.devs_increment\t= 2,\n\t\t.ncopies\t= 2,\n\t\t.nparity        = 0,\n\t\t.raid_name\t= \"raid10\",\n\t\t.bg_flag\t= BTRFS_BLOCK_GROUP_RAID10,\n\t\t.mindev_error\t= BTRFS_ERROR_DEV_RAID10_MIN_NOT_MET,\n\t},\n\t[BTRFS_RAID_RAID1] = {\n\t\t.sub_stripes\t= 1,\n\t\t.dev_stripes\t= 1,\n\t\t.devs_max\t= 2,\n\t\t.devs_min\t= 2,\n\t\t.tolerated_failures = 1,\n\t\t.devs_increment\t= 2,\n\t\t.ncopies\t= 2,\n\t\t.nparity        = 0,\n\t\t.raid_name\t= \"raid1\",\n\t\t.bg_flag\t= BTRFS_BLOCK_GROUP_RAID1,\n\t\t.mindev_error\t= BTRFS_ERROR_DEV_RAID1_MIN_NOT_MET,\n\t},\n\t[BTRFS_RAID_RAID1C3] = {\n\t\t.sub_stripes\t= 1,\n\t\t.dev_stripes\t= 1,\n\t\t.devs_max\t= 3,\n\t\t.devs_min\t= 3,\n\t\t.tolerated_failures = 2,\n\t\t.devs_increment\t= 3,\n\t\t.ncopies\t= 3,\n\t\t.nparity        = 0,\n\t\t.raid_name\t= \"raid1c3\",\n\t\t.bg_flag\t= BTRFS_BLOCK_GROUP_RAID1C3,\n\t\t.mindev_error\t= BTRFS_ERROR_DEV_RAID1C3_MIN_NOT_MET,\n\t},\n\t[BTRFS_RAID_RAID1C4] = {\n\t\t.sub_stripes\t= 1,\n\t\t.dev_stripes\t= 1,\n\t\t.devs_max\t= 4,\n\t\t.devs_min\t= 4,\n\t\t.tolerated_failures = 3,\n\t\t.devs_increment\t= 4,\n\t\t.ncopies\t= 4,\n\t\t.nparity        = 0,\n\t\t.raid_name\t= \"raid1c4\",\n\t\t.bg_flag\t= BTRFS_BLOCK_GROUP_RAID1C4,\n\t\t.mindev_error\t= BTRFS_ERROR_DEV_RAID1C4_MIN_NOT_MET,\n\t},\n\t[BTRFS_RAID_DUP] = {\n\t\t.sub_stripes\t= 1,\n\t\t.dev_stripes\t= 2,\n\t\t.devs_max\t= 1,\n\t\t.devs_min\t= 1,\n\t\t.tolerated_failures = 0,\n\t\t.devs_increment\t= 1,\n\t\t.ncopies\t= 2,\n\t\t.nparity        = 0,\n\t\t.raid_name\t= \"dup\",\n\t\t.bg_flag\t= BTRFS_BLOCK_GROUP_DUP,\n\t\t.mindev_error\t= 0,\n\t},\n\t[BTRFS_RAID_RAID0] = {\n\t\t.sub_stripes\t= 1,\n\t\t.dev_stripes\t= 1,\n\t\t.devs_max\t= 0,\n\t\t.devs_min\t= 1,\n\t\t.tolerated_failures = 0,\n\t\t.devs_increment\t= 1,\n\t\t.ncopies\t= 1,\n\t\t.nparity        = 0,\n\t\t.raid_name\t= \"raid0\",\n\t\t.bg_flag\t= BTRFS_BLOCK_GROUP_RAID0,\n\t\t.mindev_error\t= 0,\n\t},\n\t[BTRFS_RAID_SINGLE] = {\n\t\t.sub_stripes\t= 1,\n\t\t.dev_stripes\t= 1,\n\t\t.devs_max\t= 1,\n\t\t.devs_min\t= 1,\n\t\t.tolerated_failures = 0,\n\t\t.devs_increment\t= 1,\n\t\t.ncopies\t= 1,\n\t\t.nparity        = 0,\n\t\t.raid_name\t= \"single\",\n\t\t.bg_flag\t= 0,\n\t\t.mindev_error\t= 0,\n\t},\n\t[BTRFS_RAID_RAID5] = {\n\t\t.sub_stripes\t= 1,\n\t\t.dev_stripes\t= 1,\n\t\t.devs_max\t= 0,\n\t\t.devs_min\t= 2,\n\t\t.tolerated_failures = 1,\n\t\t.devs_increment\t= 1,\n\t\t.ncopies\t= 1,\n\t\t.nparity        = 1,\n\t\t.raid_name\t= \"raid5\",\n\t\t.bg_flag\t= BTRFS_BLOCK_GROUP_RAID5,\n\t\t.mindev_error\t= BTRFS_ERROR_DEV_RAID5_MIN_NOT_MET,\n\t},\n\t[BTRFS_RAID_RAID6] = {\n\t\t.sub_stripes\t= 1,\n\t\t.dev_stripes\t= 1,\n\t\t.devs_max\t= 0,\n\t\t.devs_min\t= 3,\n\t\t.tolerated_failures = 2,\n\t\t.devs_increment\t= 1,\n\t\t.ncopies\t= 1,\n\t\t.nparity        = 2,\n\t\t.raid_name\t= \"raid6\",\n\t\t.bg_flag\t= BTRFS_BLOCK_GROUP_RAID6,\n\t\t.mindev_error\t= BTRFS_ERROR_DEV_RAID6_MIN_NOT_MET,\n\t},\n};\n\n \nenum btrfs_raid_types __attribute_const__ btrfs_bg_flags_to_raid_index(u64 flags)\n{\n\tconst u64 profile = (flags & BTRFS_BLOCK_GROUP_PROFILE_MASK);\n\n\tif (!profile)\n\t\treturn BTRFS_RAID_SINGLE;\n\n\treturn BTRFS_BG_FLAG_TO_INDEX(profile);\n}\n\nconst char *btrfs_bg_type_to_raid_name(u64 flags)\n{\n\tconst int index = btrfs_bg_flags_to_raid_index(flags);\n\n\tif (index >= BTRFS_NR_RAID_TYPES)\n\t\treturn NULL;\n\n\treturn btrfs_raid_array[index].raid_name;\n}\n\nint btrfs_nr_parity_stripes(u64 type)\n{\n\tenum btrfs_raid_types index = btrfs_bg_flags_to_raid_index(type);\n\n\treturn btrfs_raid_array[index].nparity;\n}\n\n \nvoid btrfs_describe_block_groups(u64 bg_flags, char *buf, u32 size_buf)\n{\n\tint i;\n\tint ret;\n\tchar *bp = buf;\n\tu64 flags = bg_flags;\n\tu32 size_bp = size_buf;\n\n\tif (!flags) {\n\t\tstrcpy(bp, \"NONE\");\n\t\treturn;\n\t}\n\n#define DESCRIBE_FLAG(flag, desc)\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (flags & (flag)) {\t\t\t\t\t\\\n\t\t\tret = snprintf(bp, size_bp, \"%s|\", (desc));\t\\\n\t\t\tif (ret < 0 || ret >= size_bp)\t\t\t\\\n\t\t\t\tgoto out_overflow;\t\t\t\\\n\t\t\tsize_bp -= ret;\t\t\t\t\t\\\n\t\t\tbp += ret;\t\t\t\t\t\\\n\t\t\tflags &= ~(flag);\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t} while (0)\n\n\tDESCRIBE_FLAG(BTRFS_BLOCK_GROUP_DATA, \"data\");\n\tDESCRIBE_FLAG(BTRFS_BLOCK_GROUP_SYSTEM, \"system\");\n\tDESCRIBE_FLAG(BTRFS_BLOCK_GROUP_METADATA, \"metadata\");\n\n\tDESCRIBE_FLAG(BTRFS_AVAIL_ALLOC_BIT_SINGLE, \"single\");\n\tfor (i = 0; i < BTRFS_NR_RAID_TYPES; i++)\n\t\tDESCRIBE_FLAG(btrfs_raid_array[i].bg_flag,\n\t\t\t      btrfs_raid_array[i].raid_name);\n#undef DESCRIBE_FLAG\n\n\tif (flags) {\n\t\tret = snprintf(bp, size_bp, \"0x%llx|\", flags);\n\t\tsize_bp -= ret;\n\t}\n\n\tif (size_bp < size_buf)\n\t\tbuf[size_buf - size_bp - 1] = '\\0';  \n\n\t \nout_overflow:;\n}\n\nstatic int init_first_rw_device(struct btrfs_trans_handle *trans);\nstatic int btrfs_relocate_sys_chunks(struct btrfs_fs_info *fs_info);\nstatic void btrfs_dev_stat_print_on_load(struct btrfs_device *device);\n\n \n\nDEFINE_MUTEX(uuid_mutex);\nstatic LIST_HEAD(fs_uuids);\nstruct list_head * __attribute_const__ btrfs_get_fs_uuids(void)\n{\n\treturn &fs_uuids;\n}\n\n \nstatic struct btrfs_fs_devices *alloc_fs_devices(const u8 *fsid,\n\t\t\t\t\t\t const u8 *metadata_fsid)\n{\n\tstruct btrfs_fs_devices *fs_devs;\n\n\tASSERT(fsid || !metadata_fsid);\n\n\tfs_devs = kzalloc(sizeof(*fs_devs), GFP_KERNEL);\n\tif (!fs_devs)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmutex_init(&fs_devs->device_list_mutex);\n\n\tINIT_LIST_HEAD(&fs_devs->devices);\n\tINIT_LIST_HEAD(&fs_devs->alloc_list);\n\tINIT_LIST_HEAD(&fs_devs->fs_list);\n\tINIT_LIST_HEAD(&fs_devs->seed_list);\n\n\tif (fsid) {\n\t\tmemcpy(fs_devs->fsid, fsid, BTRFS_FSID_SIZE);\n\t\tmemcpy(fs_devs->metadata_uuid,\n\t\t       metadata_fsid ?: fsid, BTRFS_FSID_SIZE);\n\t}\n\n\treturn fs_devs;\n}\n\nstatic void btrfs_free_device(struct btrfs_device *device)\n{\n\tWARN_ON(!list_empty(&device->post_commit_list));\n\trcu_string_free(device->name);\n\textent_io_tree_release(&device->alloc_state);\n\tbtrfs_destroy_dev_zone_info(device);\n\tkfree(device);\n}\n\nstatic void free_fs_devices(struct btrfs_fs_devices *fs_devices)\n{\n\tstruct btrfs_device *device;\n\n\tWARN_ON(fs_devices->opened);\n\twhile (!list_empty(&fs_devices->devices)) {\n\t\tdevice = list_entry(fs_devices->devices.next,\n\t\t\t\t    struct btrfs_device, dev_list);\n\t\tlist_del(&device->dev_list);\n\t\tbtrfs_free_device(device);\n\t}\n\tkfree(fs_devices);\n}\n\nvoid __exit btrfs_cleanup_fs_uuids(void)\n{\n\tstruct btrfs_fs_devices *fs_devices;\n\n\twhile (!list_empty(&fs_uuids)) {\n\t\tfs_devices = list_entry(fs_uuids.next,\n\t\t\t\t\tstruct btrfs_fs_devices, fs_list);\n\t\tlist_del(&fs_devices->fs_list);\n\t\tfree_fs_devices(fs_devices);\n\t}\n}\n\nstatic bool match_fsid_fs_devices(const struct btrfs_fs_devices *fs_devices,\n\t\t\t\t  const u8 *fsid, const u8 *metadata_fsid)\n{\n\tif (memcmp(fsid, fs_devices->fsid, BTRFS_FSID_SIZE) != 0)\n\t\treturn false;\n\n\tif (!metadata_fsid)\n\t\treturn true;\n\n\tif (memcmp(metadata_fsid, fs_devices->metadata_uuid, BTRFS_FSID_SIZE) != 0)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic noinline struct btrfs_fs_devices *find_fsid(\n\t\tconst u8 *fsid, const u8 *metadata_fsid)\n{\n\tstruct btrfs_fs_devices *fs_devices;\n\n\tASSERT(fsid);\n\n\t \n\tlist_for_each_entry(fs_devices, &fs_uuids, fs_list) {\n\t\tif (match_fsid_fs_devices(fs_devices, fsid, metadata_fsid))\n\t\t\treturn fs_devices;\n\t}\n\treturn NULL;\n}\n\n \nstatic inline bool check_fsid_changed(const struct btrfs_fs_devices *fs_devices,\n\t\t\t\t      const u8 *fsid)\n{\n\treturn memcmp(fs_devices->fsid, fs_devices->metadata_uuid,\n\t\t      BTRFS_FSID_SIZE) != 0 &&\n\t       memcmp(fs_devices->metadata_uuid, fsid, BTRFS_FSID_SIZE) == 0;\n}\n\nstatic struct btrfs_fs_devices *find_fsid_with_metadata_uuid(\n\t\t\t\tstruct btrfs_super_block *disk_super)\n{\n\n\tstruct btrfs_fs_devices *fs_devices;\n\n\t \n\tlist_for_each_entry(fs_devices, &fs_uuids, fs_list) {\n\t\tif (!fs_devices->fsid_change)\n\t\t\tcontinue;\n\n\t\tif (match_fsid_fs_devices(fs_devices, disk_super->metadata_uuid,\n\t\t\t\t\t  fs_devices->fsid))\n\t\t\treturn fs_devices;\n\t}\n\n\t \n\tlist_for_each_entry(fs_devices, &fs_uuids, fs_list) {\n\t\tif (!fs_devices->fsid_change)\n\t\t\tcontinue;\n\n\t\tif (check_fsid_changed(fs_devices, disk_super->metadata_uuid))\n\t\t\treturn fs_devices;\n\t}\n\n\treturn find_fsid(disk_super->fsid, disk_super->metadata_uuid);\n}\n\n\nstatic int\nbtrfs_get_bdev_and_sb(const char *device_path, blk_mode_t flags, void *holder,\n\t\t      int flush, struct block_device **bdev,\n\t\t      struct btrfs_super_block **disk_super)\n{\n\tint ret;\n\n\t*bdev = blkdev_get_by_path(device_path, flags, holder, NULL);\n\n\tif (IS_ERR(*bdev)) {\n\t\tret = PTR_ERR(*bdev);\n\t\tgoto error;\n\t}\n\n\tif (flush)\n\t\tsync_blockdev(*bdev);\n\tret = set_blocksize(*bdev, BTRFS_BDEV_BLOCKSIZE);\n\tif (ret) {\n\t\tblkdev_put(*bdev, holder);\n\t\tgoto error;\n\t}\n\tinvalidate_bdev(*bdev);\n\t*disk_super = btrfs_read_dev_super(*bdev);\n\tif (IS_ERR(*disk_super)) {\n\t\tret = PTR_ERR(*disk_super);\n\t\tblkdev_put(*bdev, holder);\n\t\tgoto error;\n\t}\n\n\treturn 0;\n\nerror:\n\t*bdev = NULL;\n\treturn ret;\n}\n\n \nstatic int btrfs_free_stale_devices(dev_t devt, struct btrfs_device *skip_device)\n{\n\tstruct btrfs_fs_devices *fs_devices, *tmp_fs_devices;\n\tstruct btrfs_device *device, *tmp_device;\n\tint ret = 0;\n\n\tlockdep_assert_held(&uuid_mutex);\n\n\tif (devt)\n\t\tret = -ENOENT;\n\n\tlist_for_each_entry_safe(fs_devices, tmp_fs_devices, &fs_uuids, fs_list) {\n\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tlist_for_each_entry_safe(device, tmp_device,\n\t\t\t\t\t &fs_devices->devices, dev_list) {\n\t\t\tif (skip_device && skip_device == device)\n\t\t\t\tcontinue;\n\t\t\tif (devt && devt != device->devt)\n\t\t\t\tcontinue;\n\t\t\tif (fs_devices->opened) {\n\t\t\t\t \n\t\t\t\tif (devt && ret != 0)\n\t\t\t\t\tret = -EBUSY;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t \n\t\t\tfs_devices->num_devices--;\n\t\t\tlist_del(&device->dev_list);\n\t\t\tbtrfs_free_device(device);\n\n\t\t\tret = 0;\n\t\t}\n\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\t\tif (fs_devices->num_devices == 0) {\n\t\t\tbtrfs_sysfs_remove_fsid(fs_devices);\n\t\t\tlist_del(&fs_devices->fs_list);\n\t\t\tfree_fs_devices(fs_devices);\n\t\t}\n\t}\n\n\treturn ret;\n}\n\n \nstatic int btrfs_open_one_device(struct btrfs_fs_devices *fs_devices,\n\t\t\tstruct btrfs_device *device, blk_mode_t flags,\n\t\t\tvoid *holder)\n{\n\tstruct block_device *bdev;\n\tstruct btrfs_super_block *disk_super;\n\tu64 devid;\n\tint ret;\n\n\tif (device->bdev)\n\t\treturn -EINVAL;\n\tif (!device->name)\n\t\treturn -EINVAL;\n\n\tret = btrfs_get_bdev_and_sb(device->name->str, flags, holder, 1,\n\t\t\t\t    &bdev, &disk_super);\n\tif (ret)\n\t\treturn ret;\n\n\tdevid = btrfs_stack_device_id(&disk_super->dev_item);\n\tif (devid != device->devid)\n\t\tgoto error_free_page;\n\n\tif (memcmp(device->uuid, disk_super->dev_item.uuid, BTRFS_UUID_SIZE))\n\t\tgoto error_free_page;\n\n\tdevice->generation = btrfs_super_generation(disk_super);\n\n\tif (btrfs_super_flags(disk_super) & BTRFS_SUPER_FLAG_SEEDING) {\n\t\tif (btrfs_super_incompat_flags(disk_super) &\n\t\t    BTRFS_FEATURE_INCOMPAT_METADATA_UUID) {\n\t\t\tpr_err(\n\t\t\"BTRFS: Invalid seeding and uuid-changed device detected\\n\");\n\t\t\tgoto error_free_page;\n\t\t}\n\n\t\tclear_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state);\n\t\tfs_devices->seeding = true;\n\t} else {\n\t\tif (bdev_read_only(bdev))\n\t\t\tclear_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state);\n\t\telse\n\t\t\tset_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state);\n\t}\n\n\tif (!bdev_nonrot(bdev))\n\t\tfs_devices->rotating = true;\n\n\tif (bdev_max_discard_sectors(bdev))\n\t\tfs_devices->discardable = true;\n\n\tdevice->bdev = bdev;\n\tclear_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tdevice->holder = holder;\n\n\tfs_devices->open_devices++;\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state) &&\n\t    device->devid != BTRFS_DEV_REPLACE_DEVID) {\n\t\tfs_devices->rw_devices++;\n\t\tlist_add_tail(&device->dev_alloc_list, &fs_devices->alloc_list);\n\t}\n\tbtrfs_release_disk_super(disk_super);\n\n\treturn 0;\n\nerror_free_page:\n\tbtrfs_release_disk_super(disk_super);\n\tblkdev_put(bdev, holder);\n\n\treturn -EINVAL;\n}\n\nu8 *btrfs_sb_fsid_ptr(struct btrfs_super_block *sb)\n{\n\tbool has_metadata_uuid = (btrfs_super_incompat_flags(sb) &\n\t\t\t\t  BTRFS_FEATURE_INCOMPAT_METADATA_UUID);\n\n\treturn has_metadata_uuid ? sb->metadata_uuid : sb->fsid;\n}\n\n \nstatic struct btrfs_fs_devices *find_fsid_inprogress(\n\t\t\t\t\tstruct btrfs_super_block *disk_super)\n{\n\tstruct btrfs_fs_devices *fs_devices;\n\n\tlist_for_each_entry(fs_devices, &fs_uuids, fs_list) {\n\t\tif (fs_devices->fsid_change)\n\t\t\tcontinue;\n\n\t\tif (check_fsid_changed(fs_devices,  disk_super->fsid))\n\t\t\treturn fs_devices;\n\t}\n\n\treturn find_fsid(disk_super->fsid, NULL);\n}\n\nstatic struct btrfs_fs_devices *find_fsid_changed(\n\t\t\t\t\tstruct btrfs_super_block *disk_super)\n{\n\tstruct btrfs_fs_devices *fs_devices;\n\n\t \n\tlist_for_each_entry(fs_devices, &fs_uuids, fs_list) {\n\t\t \n\t\tif (check_fsid_changed(fs_devices, disk_super->metadata_uuid) &&\n\t\t    memcmp(fs_devices->fsid, disk_super->fsid,\n\t\t\t   BTRFS_FSID_SIZE) != 0)\n\t\t\treturn fs_devices;\n\n\t\t \n\t\tif (memcmp(fs_devices->metadata_uuid, fs_devices->fsid,\n\t\t\t   BTRFS_FSID_SIZE) == 0 &&\n\t\t    memcmp(fs_devices->fsid, disk_super->metadata_uuid,\n\t\t\t   BTRFS_FSID_SIZE) == 0)\n\t\t\treturn fs_devices;\n\t}\n\n\treturn NULL;\n}\n\nstatic struct btrfs_fs_devices *find_fsid_reverted_metadata(\n\t\t\t\tstruct btrfs_super_block *disk_super)\n{\n\tstruct btrfs_fs_devices *fs_devices;\n\n\t \n\tlist_for_each_entry(fs_devices, &fs_uuids, fs_list) {\n\t\tif (!fs_devices->fsid_change)\n\t\t\tcontinue;\n\n\t\tif (check_fsid_changed(fs_devices, disk_super->fsid))\n\t\t\treturn fs_devices;\n\t}\n\n\treturn NULL;\n}\n \nstatic noinline struct btrfs_device *device_list_add(const char *path,\n\t\t\t   struct btrfs_super_block *disk_super,\n\t\t\t   bool *new_device_added)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *fs_devices = NULL;\n\tstruct rcu_string *name;\n\tu64 found_transid = btrfs_super_generation(disk_super);\n\tu64 devid = btrfs_stack_device_id(&disk_super->dev_item);\n\tdev_t path_devt;\n\tint error;\n\tbool has_metadata_uuid = (btrfs_super_incompat_flags(disk_super) &\n\t\tBTRFS_FEATURE_INCOMPAT_METADATA_UUID);\n\tbool fsid_change_in_progress = (btrfs_super_flags(disk_super) &\n\t\t\t\t\tBTRFS_SUPER_FLAG_CHANGING_FSID_V2);\n\n\terror = lookup_bdev(path, &path_devt);\n\tif (error) {\n\t\tbtrfs_err(NULL, \"failed to lookup block device for path %s: %d\",\n\t\t\t  path, error);\n\t\treturn ERR_PTR(error);\n\t}\n\n\tif (fsid_change_in_progress) {\n\t\tif (!has_metadata_uuid)\n\t\t\tfs_devices = find_fsid_inprogress(disk_super);\n\t\telse\n\t\t\tfs_devices = find_fsid_changed(disk_super);\n\t} else if (has_metadata_uuid) {\n\t\tfs_devices = find_fsid_with_metadata_uuid(disk_super);\n\t} else {\n\t\tfs_devices = find_fsid_reverted_metadata(disk_super);\n\t\tif (!fs_devices)\n\t\t\tfs_devices = find_fsid(disk_super->fsid, NULL);\n\t}\n\n\n\tif (!fs_devices) {\n\t\tfs_devices = alloc_fs_devices(disk_super->fsid,\n\t\t\t\thas_metadata_uuid ? disk_super->metadata_uuid : NULL);\n\t\tif (IS_ERR(fs_devices))\n\t\t\treturn ERR_CAST(fs_devices);\n\n\t\tfs_devices->fsid_change = fsid_change_in_progress;\n\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tlist_add(&fs_devices->fs_list, &fs_uuids);\n\n\t\tdevice = NULL;\n\t} else {\n\t\tstruct btrfs_dev_lookup_args args = {\n\t\t\t.devid = devid,\n\t\t\t.uuid = disk_super->dev_item.uuid,\n\t\t};\n\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tdevice = btrfs_find_device(fs_devices, &args);\n\n\t\t \n\t\tif (fs_devices->fsid_change &&\n\t\t    found_transid > fs_devices->latest_generation) {\n\t\t\tmemcpy(fs_devices->fsid, disk_super->fsid,\n\t\t\t\t\tBTRFS_FSID_SIZE);\n\t\t\tmemcpy(fs_devices->metadata_uuid,\n\t\t\t       btrfs_sb_fsid_ptr(disk_super), BTRFS_FSID_SIZE);\n\t\t\tfs_devices->fsid_change = false;\n\t\t}\n\t}\n\n\tif (!device) {\n\t\tunsigned int nofs_flag;\n\n\t\tif (fs_devices->opened) {\n\t\t\tbtrfs_err(NULL,\n\"device %s belongs to fsid %pU, and the fs is already mounted, scanned by %s (%d)\",\n\t\t\t\t  path, fs_devices->fsid, current->comm,\n\t\t\t\t  task_pid_nr(current));\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-EBUSY);\n\t\t}\n\n\t\tnofs_flag = memalloc_nofs_save();\n\t\tdevice = btrfs_alloc_device(NULL, &devid,\n\t\t\t\t\t    disk_super->dev_item.uuid, path);\n\t\tmemalloc_nofs_restore(nofs_flag);\n\t\tif (IS_ERR(device)) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t \n\t\t\treturn device;\n\t\t}\n\n\t\tdevice->devt = path_devt;\n\n\t\tlist_add_rcu(&device->dev_list, &fs_devices->devices);\n\t\tfs_devices->num_devices++;\n\n\t\tdevice->fs_devices = fs_devices;\n\t\t*new_device_added = true;\n\n\t\tif (disk_super->label[0])\n\t\t\tpr_info(\n\t\"BTRFS: device label %s devid %llu transid %llu %s scanned by %s (%d)\\n\",\n\t\t\t\tdisk_super->label, devid, found_transid, path,\n\t\t\t\tcurrent->comm, task_pid_nr(current));\n\t\telse\n\t\t\tpr_info(\n\t\"BTRFS: device fsid %pU devid %llu transid %llu %s scanned by %s (%d)\\n\",\n\t\t\t\tdisk_super->fsid, devid, found_transid, path,\n\t\t\t\tcurrent->comm, task_pid_nr(current));\n\n\t} else if (!device->name || strcmp(device->name->str, path)) {\n\t\t \n\n\t\t \n\t\tif (!fs_devices->opened && found_transid < device->generation) {\n\t\t\t \n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\tbtrfs_err(NULL,\n\"device %s already registered with a higher generation, found %llu expect %llu\",\n\t\t\t\t  path, found_transid, device->generation);\n\t\t\treturn ERR_PTR(-EEXIST);\n\t\t}\n\n\t\t \n\t\tif (device->bdev) {\n\t\t\tif (device->devt != path_devt) {\n\t\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t\tbtrfs_warn_in_rcu(NULL,\n\t\"duplicate device %s devid %llu generation %llu scanned by %s (%d)\",\n\t\t\t\t\t\t  path, devid, found_transid,\n\t\t\t\t\t\t  current->comm,\n\t\t\t\t\t\t  task_pid_nr(current));\n\t\t\t\treturn ERR_PTR(-EEXIST);\n\t\t\t}\n\t\t\tbtrfs_info_in_rcu(NULL,\n\t\"devid %llu device path %s changed to %s scanned by %s (%d)\",\n\t\t\t\t\t  devid, btrfs_dev_name(device),\n\t\t\t\t\t  path, current->comm,\n\t\t\t\t\t  task_pid_nr(current));\n\t\t}\n\n\t\tname = rcu_string_strdup(path, GFP_NOFS);\n\t\tif (!name) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\trcu_string_free(device->name);\n\t\trcu_assign_pointer(device->name, name);\n\t\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state)) {\n\t\t\tfs_devices->missing_devices--;\n\t\t\tclear_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state);\n\t\t}\n\t\tdevice->devt = path_devt;\n\t}\n\n\t \n\tif (!fs_devices->opened) {\n\t\tdevice->generation = found_transid;\n\t\tfs_devices->latest_generation = max_t(u64, found_transid,\n\t\t\t\t\t\tfs_devices->latest_generation);\n\t}\n\n\tfs_devices->total_devices = btrfs_super_num_devices(disk_super);\n\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\treturn device;\n}\n\nstatic struct btrfs_fs_devices *clone_fs_devices(struct btrfs_fs_devices *orig)\n{\n\tstruct btrfs_fs_devices *fs_devices;\n\tstruct btrfs_device *device;\n\tstruct btrfs_device *orig_dev;\n\tint ret = 0;\n\n\tlockdep_assert_held(&uuid_mutex);\n\n\tfs_devices = alloc_fs_devices(orig->fsid, NULL);\n\tif (IS_ERR(fs_devices))\n\t\treturn fs_devices;\n\n\tfs_devices->total_devices = orig->total_devices;\n\n\tlist_for_each_entry(orig_dev, &orig->devices, dev_list) {\n\t\tconst char *dev_path = NULL;\n\n\t\t \n\t\tif (orig_dev->name)\n\t\t\tdev_path = orig_dev->name->str;\n\n\t\tdevice = btrfs_alloc_device(NULL, &orig_dev->devid,\n\t\t\t\t\t    orig_dev->uuid, dev_path);\n\t\tif (IS_ERR(device)) {\n\t\t\tret = PTR_ERR(device);\n\t\t\tgoto error;\n\t\t}\n\n\t\tif (orig_dev->zone_info) {\n\t\t\tstruct btrfs_zoned_device_info *zone_info;\n\n\t\t\tzone_info = btrfs_clone_dev_zone_info(orig_dev);\n\t\t\tif (!zone_info) {\n\t\t\t\tbtrfs_free_device(device);\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tdevice->zone_info = zone_info;\n\t\t}\n\n\t\tlist_add(&device->dev_list, &fs_devices->devices);\n\t\tdevice->fs_devices = fs_devices;\n\t\tfs_devices->num_devices++;\n\t}\n\treturn fs_devices;\nerror:\n\tfree_fs_devices(fs_devices);\n\treturn ERR_PTR(ret);\n}\n\nstatic void __btrfs_free_extra_devids(struct btrfs_fs_devices *fs_devices,\n\t\t\t\t      struct btrfs_device **latest_dev)\n{\n\tstruct btrfs_device *device, *next;\n\n\t \n\tlist_for_each_entry_safe(device, next, &fs_devices->devices, dev_list) {\n\t\tif (test_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state)) {\n\t\t\tif (!test_bit(BTRFS_DEV_STATE_REPLACE_TGT,\n\t\t\t\t      &device->dev_state) &&\n\t\t\t    !test_bit(BTRFS_DEV_STATE_MISSING,\n\t\t\t\t      &device->dev_state) &&\n\t\t\t    (!*latest_dev ||\n\t\t\t     device->generation > (*latest_dev)->generation)) {\n\t\t\t\t*latest_dev = device;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (device->devid == BTRFS_DEV_REPLACE_DEVID)\n\t\t\tcontinue;\n\n\t\tif (device->bdev) {\n\t\t\tblkdev_put(device->bdev, device->holder);\n\t\t\tdevice->bdev = NULL;\n\t\t\tfs_devices->open_devices--;\n\t\t}\n\t\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\t\tlist_del_init(&device->dev_alloc_list);\n\t\t\tclear_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state);\n\t\t\tfs_devices->rw_devices--;\n\t\t}\n\t\tlist_del_init(&device->dev_list);\n\t\tfs_devices->num_devices--;\n\t\tbtrfs_free_device(device);\n\t}\n\n}\n\n \nvoid btrfs_free_extra_devids(struct btrfs_fs_devices *fs_devices)\n{\n\tstruct btrfs_device *latest_dev = NULL;\n\tstruct btrfs_fs_devices *seed_dev;\n\n\tmutex_lock(&uuid_mutex);\n\t__btrfs_free_extra_devids(fs_devices, &latest_dev);\n\n\tlist_for_each_entry(seed_dev, &fs_devices->seed_list, seed_list)\n\t\t__btrfs_free_extra_devids(seed_dev, &latest_dev);\n\n\tfs_devices->latest_dev = latest_dev;\n\n\tmutex_unlock(&uuid_mutex);\n}\n\nstatic void btrfs_close_bdev(struct btrfs_device *device)\n{\n\tif (!device->bdev)\n\t\treturn;\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tsync_blockdev(device->bdev);\n\t\tinvalidate_bdev(device->bdev);\n\t}\n\n\tblkdev_put(device->bdev, device->holder);\n}\n\nstatic void btrfs_close_one_device(struct btrfs_device *device)\n{\n\tstruct btrfs_fs_devices *fs_devices = device->fs_devices;\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state) &&\n\t    device->devid != BTRFS_DEV_REPLACE_DEVID) {\n\t\tlist_del_init(&device->dev_alloc_list);\n\t\tfs_devices->rw_devices--;\n\t}\n\n\tif (device->devid == BTRFS_DEV_REPLACE_DEVID)\n\t\tclear_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state);\n\n\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state)) {\n\t\tclear_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state);\n\t\tfs_devices->missing_devices--;\n\t}\n\n\tbtrfs_close_bdev(device);\n\tif (device->bdev) {\n\t\tfs_devices->open_devices--;\n\t\tdevice->bdev = NULL;\n\t}\n\tclear_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state);\n\tbtrfs_destroy_dev_zone_info(device);\n\n\tdevice->fs_info = NULL;\n\tatomic_set(&device->dev_stats_ccnt, 0);\n\textent_io_tree_release(&device->alloc_state);\n\n\t \n\tdevice->last_flush_error = 0;\n\n\t \n\tWARN_ON(test_bit(BTRFS_DEV_STATE_FLUSH_SENT, &device->dev_state));\n\tWARN_ON(test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state));\n\tWARN_ON(!list_empty(&device->dev_alloc_list));\n\tWARN_ON(!list_empty(&device->post_commit_list));\n}\n\nstatic void close_fs_devices(struct btrfs_fs_devices *fs_devices)\n{\n\tstruct btrfs_device *device, *tmp;\n\n\tlockdep_assert_held(&uuid_mutex);\n\n\tif (--fs_devices->opened > 0)\n\t\treturn;\n\n\tlist_for_each_entry_safe(device, tmp, &fs_devices->devices, dev_list)\n\t\tbtrfs_close_one_device(device);\n\n\tWARN_ON(fs_devices->open_devices);\n\tWARN_ON(fs_devices->rw_devices);\n\tfs_devices->opened = 0;\n\tfs_devices->seeding = false;\n\tfs_devices->fs_info = NULL;\n}\n\nvoid btrfs_close_devices(struct btrfs_fs_devices *fs_devices)\n{\n\tLIST_HEAD(list);\n\tstruct btrfs_fs_devices *tmp;\n\n\tmutex_lock(&uuid_mutex);\n\tclose_fs_devices(fs_devices);\n\tif (!fs_devices->opened) {\n\t\tlist_splice_init(&fs_devices->seed_list, &list);\n\n\t\t \n\t\tif (fs_devices->num_devices == 1) {\n\t\t\tlist_del(&fs_devices->fs_list);\n\t\t\tfree_fs_devices(fs_devices);\n\t\t}\n\t}\n\n\n\tlist_for_each_entry_safe(fs_devices, tmp, &list, seed_list) {\n\t\tclose_fs_devices(fs_devices);\n\t\tlist_del(&fs_devices->seed_list);\n\t\tfree_fs_devices(fs_devices);\n\t}\n\tmutex_unlock(&uuid_mutex);\n}\n\nstatic int open_fs_devices(struct btrfs_fs_devices *fs_devices,\n\t\t\t\tblk_mode_t flags, void *holder)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_device *latest_dev = NULL;\n\tstruct btrfs_device *tmp_device;\n\n\tlist_for_each_entry_safe(device, tmp_device, &fs_devices->devices,\n\t\t\t\t dev_list) {\n\t\tint ret;\n\n\t\tret = btrfs_open_one_device(fs_devices, device, flags, holder);\n\t\tif (ret == 0 &&\n\t\t    (!latest_dev || device->generation > latest_dev->generation)) {\n\t\t\tlatest_dev = device;\n\t\t} else if (ret == -ENODATA) {\n\t\t\tfs_devices->num_devices--;\n\t\t\tlist_del(&device->dev_list);\n\t\t\tbtrfs_free_device(device);\n\t\t}\n\t}\n\tif (fs_devices->open_devices == 0)\n\t\treturn -EINVAL;\n\n\tfs_devices->opened = 1;\n\tfs_devices->latest_dev = latest_dev;\n\tfs_devices->total_rw_bytes = 0;\n\tfs_devices->chunk_alloc_policy = BTRFS_CHUNK_ALLOC_REGULAR;\n\tfs_devices->read_policy = BTRFS_READ_POLICY_PID;\n\n\treturn 0;\n}\n\nstatic int devid_cmp(void *priv, const struct list_head *a,\n\t\t     const struct list_head *b)\n{\n\tconst struct btrfs_device *dev1, *dev2;\n\n\tdev1 = list_entry(a, struct btrfs_device, dev_list);\n\tdev2 = list_entry(b, struct btrfs_device, dev_list);\n\n\tif (dev1->devid < dev2->devid)\n\t\treturn -1;\n\telse if (dev1->devid > dev2->devid)\n\t\treturn 1;\n\treturn 0;\n}\n\nint btrfs_open_devices(struct btrfs_fs_devices *fs_devices,\n\t\t       blk_mode_t flags, void *holder)\n{\n\tint ret;\n\n\tlockdep_assert_held(&uuid_mutex);\n\t \n\n\tif (fs_devices->opened) {\n\t\tfs_devices->opened++;\n\t\tret = 0;\n\t} else {\n\t\tlist_sort(NULL, &fs_devices->devices, devid_cmp);\n\t\tret = open_fs_devices(fs_devices, flags, holder);\n\t}\n\n\treturn ret;\n}\n\nvoid btrfs_release_disk_super(struct btrfs_super_block *super)\n{\n\tstruct page *page = virt_to_page(super);\n\n\tput_page(page);\n}\n\nstatic struct btrfs_super_block *btrfs_read_disk_super(struct block_device *bdev,\n\t\t\t\t\t\t       u64 bytenr, u64 bytenr_orig)\n{\n\tstruct btrfs_super_block *disk_super;\n\tstruct page *page;\n\tvoid *p;\n\tpgoff_t index;\n\n\t \n\tif (bytenr + PAGE_SIZE >= bdev_nr_bytes(bdev))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t \n\tif (sizeof(*disk_super) > PAGE_SIZE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t \n\tindex = bytenr >> PAGE_SHIFT;\n\tif ((bytenr + sizeof(*disk_super) - 1) >> PAGE_SHIFT != index)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t \n\tpage = read_cache_page_gfp(bdev->bd_inode->i_mapping, index, GFP_KERNEL);\n\n\tif (IS_ERR(page))\n\t\treturn ERR_CAST(page);\n\n\tp = page_address(page);\n\n\t \n\tdisk_super = p + offset_in_page(bytenr);\n\n\tif (btrfs_super_bytenr(disk_super) != bytenr_orig ||\n\t    btrfs_super_magic(disk_super) != BTRFS_MAGIC) {\n\t\tbtrfs_release_disk_super(p);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif (disk_super->label[0] && disk_super->label[BTRFS_LABEL_SIZE - 1])\n\t\tdisk_super->label[BTRFS_LABEL_SIZE - 1] = 0;\n\n\treturn disk_super;\n}\n\nint btrfs_forget_devices(dev_t devt)\n{\n\tint ret;\n\n\tmutex_lock(&uuid_mutex);\n\tret = btrfs_free_stale_devices(devt, NULL);\n\tmutex_unlock(&uuid_mutex);\n\n\treturn ret;\n}\n\n \nstruct btrfs_device *btrfs_scan_one_device(const char *path, blk_mode_t flags)\n{\n\tstruct btrfs_super_block *disk_super;\n\tbool new_device_added = false;\n\tstruct btrfs_device *device = NULL;\n\tstruct block_device *bdev;\n\tu64 bytenr, bytenr_orig;\n\tint ret;\n\n\tlockdep_assert_held(&uuid_mutex);\n\n\t \n\n\t \n\tbdev = blkdev_get_by_path(path, flags, NULL, NULL);\n\tif (IS_ERR(bdev))\n\t\treturn ERR_CAST(bdev);\n\n\tbytenr_orig = btrfs_sb_offset(0);\n\tret = btrfs_sb_log_location_bdev(bdev, 0, READ, &bytenr);\n\tif (ret) {\n\t\tdevice = ERR_PTR(ret);\n\t\tgoto error_bdev_put;\n\t}\n\n\tdisk_super = btrfs_read_disk_super(bdev, bytenr, bytenr_orig);\n\tif (IS_ERR(disk_super)) {\n\t\tdevice = ERR_CAST(disk_super);\n\t\tgoto error_bdev_put;\n\t}\n\n\tdevice = device_list_add(path, disk_super, &new_device_added);\n\tif (!IS_ERR(device) && new_device_added)\n\t\tbtrfs_free_stale_devices(device->devt, device);\n\n\tbtrfs_release_disk_super(disk_super);\n\nerror_bdev_put:\n\tblkdev_put(bdev, NULL);\n\n\treturn device;\n}\n\n \nstatic bool contains_pending_extent(struct btrfs_device *device, u64 *start,\n\t\t\t\t    u64 len)\n{\n\tu64 physical_start, physical_end;\n\n\tlockdep_assert_held(&device->fs_info->chunk_mutex);\n\n\tif (find_first_extent_bit(&device->alloc_state, *start,\n\t\t\t\t  &physical_start, &physical_end,\n\t\t\t\t  CHUNK_ALLOCATED, NULL)) {\n\n\t\tif (in_range(physical_start, *start, len) ||\n\t\t    in_range(*start, physical_start,\n\t\t\t     physical_end - physical_start)) {\n\t\t\t*start = physical_end + 1;\n\t\t\treturn true;\n\t\t}\n\t}\n\treturn false;\n}\n\nstatic u64 dev_extent_search_start(struct btrfs_device *device)\n{\n\tswitch (device->fs_devices->chunk_alloc_policy) {\n\tcase BTRFS_CHUNK_ALLOC_REGULAR:\n\t\treturn BTRFS_DEVICE_RANGE_RESERVED;\n\tcase BTRFS_CHUNK_ALLOC_ZONED:\n\t\t \n\t\treturn 0;\n\tdefault:\n\t\tBUG();\n\t}\n}\n\nstatic bool dev_extent_hole_check_zoned(struct btrfs_device *device,\n\t\t\t\t\tu64 *hole_start, u64 *hole_size,\n\t\t\t\t\tu64 num_bytes)\n{\n\tu64 zone_size = device->zone_info->zone_size;\n\tu64 pos;\n\tint ret;\n\tbool changed = false;\n\n\tASSERT(IS_ALIGNED(*hole_start, zone_size));\n\n\twhile (*hole_size > 0) {\n\t\tpos = btrfs_find_allocatable_zones(device, *hole_start,\n\t\t\t\t\t\t   *hole_start + *hole_size,\n\t\t\t\t\t\t   num_bytes);\n\t\tif (pos != *hole_start) {\n\t\t\t*hole_size = *hole_start + *hole_size - pos;\n\t\t\t*hole_start = pos;\n\t\t\tchanged = true;\n\t\t\tif (*hole_size < num_bytes)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tret = btrfs_ensure_empty_zones(device, pos, num_bytes);\n\n\t\t \n\t\tif (!ret)\n\t\t\treturn changed;\n\n\t\t \n\t\tif (ret == -ERANGE) {\n\t\t\t*hole_start += *hole_size;\n\t\t\t*hole_size = 0;\n\t\t\treturn true;\n\t\t}\n\n\t\t*hole_start += zone_size;\n\t\t*hole_size -= zone_size;\n\t\tchanged = true;\n\t}\n\n\treturn changed;\n}\n\n \nstatic bool dev_extent_hole_check(struct btrfs_device *device, u64 *hole_start,\n\t\t\t\t  u64 *hole_size, u64 num_bytes)\n{\n\tbool changed = false;\n\tu64 hole_end = *hole_start + *hole_size;\n\n\tfor (;;) {\n\t\t \n\t\tif (contains_pending_extent(device, hole_start, *hole_size)) {\n\t\t\tif (hole_end >= *hole_start)\n\t\t\t\t*hole_size = hole_end - *hole_start;\n\t\t\telse\n\t\t\t\t*hole_size = 0;\n\t\t\tchanged = true;\n\t\t}\n\n\t\tswitch (device->fs_devices->chunk_alloc_policy) {\n\t\tcase BTRFS_CHUNK_ALLOC_REGULAR:\n\t\t\t \n\t\t\tbreak;\n\t\tcase BTRFS_CHUNK_ALLOC_ZONED:\n\t\t\tif (dev_extent_hole_check_zoned(device, hole_start,\n\t\t\t\t\t\t\thole_size, num_bytes)) {\n\t\t\t\tchanged = true;\n\t\t\t\t \n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tBUG();\n\t\t}\n\n\t\tbreak;\n\t}\n\n\treturn changed;\n}\n\n \nstatic int find_free_dev_extent(struct btrfs_device *device, u64 num_bytes,\n\t\t\t\tu64 *start, u64 *len)\n{\n\tstruct btrfs_fs_info *fs_info = device->fs_info;\n\tstruct btrfs_root *root = fs_info->dev_root;\n\tstruct btrfs_key key;\n\tstruct btrfs_dev_extent *dev_extent;\n\tstruct btrfs_path *path;\n\tu64 search_start;\n\tu64 hole_size;\n\tu64 max_hole_start;\n\tu64 max_hole_size = 0;\n\tu64 extent_end;\n\tu64 search_end = device->total_bytes;\n\tint ret;\n\tint slot;\n\tstruct extent_buffer *l;\n\n\tsearch_start = dev_extent_search_start(device);\n\tmax_hole_start = search_start;\n\n\tWARN_ON(device->zone_info &&\n\t\t!IS_ALIGNED(num_bytes, device->zone_info->zone_size));\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\nagain:\n\tif (search_start >= search_end ||\n\t\ttest_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tret = -ENOSPC;\n\t\tgoto out;\n\t}\n\n\tpath->reada = READA_FORWARD;\n\tpath->search_commit_root = 1;\n\tpath->skip_locking = 1;\n\n\tkey.objectid = device->devid;\n\tkey.offset = search_start;\n\tkey.type = BTRFS_DEV_EXTENT_KEY;\n\n\tret = btrfs_search_backwards(root, &key, path);\n\tif (ret < 0)\n\t\tgoto out;\n\n\twhile (search_start < search_end) {\n\t\tl = path->nodes[0];\n\t\tslot = path->slots[0];\n\t\tif (slot >= btrfs_header_nritems(l)) {\n\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\tif (ret == 0)\n\t\t\t\tcontinue;\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out;\n\n\t\t\tbreak;\n\t\t}\n\t\tbtrfs_item_key_to_cpu(l, &key, slot);\n\n\t\tif (key.objectid < device->devid)\n\t\t\tgoto next;\n\n\t\tif (key.objectid > device->devid)\n\t\t\tbreak;\n\n\t\tif (key.type != BTRFS_DEV_EXTENT_KEY)\n\t\t\tgoto next;\n\n\t\tif (key.offset > search_end)\n\t\t\tbreak;\n\n\t\tif (key.offset > search_start) {\n\t\t\thole_size = key.offset - search_start;\n\t\t\tdev_extent_hole_check(device, &search_start, &hole_size,\n\t\t\t\t\t      num_bytes);\n\n\t\t\tif (hole_size > max_hole_size) {\n\t\t\t\tmax_hole_start = search_start;\n\t\t\t\tmax_hole_size = hole_size;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (hole_size >= num_bytes) {\n\t\t\t\tret = 0;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\tdev_extent = btrfs_item_ptr(l, slot, struct btrfs_dev_extent);\n\t\textent_end = key.offset + btrfs_dev_extent_length(l,\n\t\t\t\t\t\t\t\t  dev_extent);\n\t\tif (extent_end > search_start)\n\t\t\tsearch_start = extent_end;\nnext:\n\t\tpath->slots[0]++;\n\t\tcond_resched();\n\t}\n\n\t \n\tif (search_end > search_start) {\n\t\thole_size = search_end - search_start;\n\t\tif (dev_extent_hole_check(device, &search_start, &hole_size,\n\t\t\t\t\t  num_bytes)) {\n\t\t\tbtrfs_release_path(path);\n\t\t\tgoto again;\n\t\t}\n\n\t\tif (hole_size > max_hole_size) {\n\t\t\tmax_hole_start = search_start;\n\t\t\tmax_hole_size = hole_size;\n\t\t}\n\t}\n\n\t \n\tif (max_hole_size < num_bytes)\n\t\tret = -ENOSPC;\n\telse\n\t\tret = 0;\n\n\tASSERT(max_hole_start + max_hole_size <= search_end);\nout:\n\tbtrfs_free_path(path);\n\t*start = max_hole_start;\n\tif (len)\n\t\t*len = max_hole_size;\n\treturn ret;\n}\n\nstatic int btrfs_free_dev_extent(struct btrfs_trans_handle *trans,\n\t\t\t  struct btrfs_device *device,\n\t\t\t  u64 start, u64 *dev_extent_len)\n{\n\tstruct btrfs_fs_info *fs_info = device->fs_info;\n\tstruct btrfs_root *root = fs_info->dev_root;\n\tint ret;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key key;\n\tstruct btrfs_key found_key;\n\tstruct extent_buffer *leaf = NULL;\n\tstruct btrfs_dev_extent *extent = NULL;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = device->devid;\n\tkey.offset = start;\n\tkey.type = BTRFS_DEV_EXTENT_KEY;\nagain:\n\tret = btrfs_search_slot(trans, root, &key, path, -1, 1);\n\tif (ret > 0) {\n\t\tret = btrfs_previous_item(root, path, key.objectid,\n\t\t\t\t\t  BTRFS_DEV_EXTENT_KEY);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\tleaf = path->nodes[0];\n\t\tbtrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);\n\t\textent = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t\tstruct btrfs_dev_extent);\n\t\tBUG_ON(found_key.offset > start || found_key.offset +\n\t\t       btrfs_dev_extent_length(leaf, extent) < start);\n\t\tkey = found_key;\n\t\tbtrfs_release_path(path);\n\t\tgoto again;\n\t} else if (ret == 0) {\n\t\tleaf = path->nodes[0];\n\t\textent = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t\tstruct btrfs_dev_extent);\n\t} else {\n\t\tgoto out;\n\t}\n\n\t*dev_extent_len = btrfs_dev_extent_length(leaf, extent);\n\n\tret = btrfs_del_item(trans, root, path);\n\tif (ret == 0)\n\t\tset_bit(BTRFS_TRANS_HAVE_FREE_BGS, &trans->transaction->flags);\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nstatic u64 find_next_chunk(struct btrfs_fs_info *fs_info)\n{\n\tstruct extent_map_tree *em_tree;\n\tstruct extent_map *em;\n\tstruct rb_node *n;\n\tu64 ret = 0;\n\n\tem_tree = &fs_info->mapping_tree;\n\tread_lock(&em_tree->lock);\n\tn = rb_last(&em_tree->map.rb_root);\n\tif (n) {\n\t\tem = rb_entry(n, struct extent_map, rb_node);\n\t\tret = em->start + em->len;\n\t}\n\tread_unlock(&em_tree->lock);\n\n\treturn ret;\n}\n\nstatic noinline int find_next_devid(struct btrfs_fs_info *fs_info,\n\t\t\t\t    u64 *devid_ret)\n{\n\tint ret;\n\tstruct btrfs_key key;\n\tstruct btrfs_key found_key;\n\tstruct btrfs_path *path;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = BTRFS_DEV_ITEMS_OBJECTID;\n\tkey.type = BTRFS_DEV_ITEM_KEY;\n\tkey.offset = (u64)-1;\n\n\tret = btrfs_search_slot(NULL, fs_info->chunk_root, &key, path, 0, 0);\n\tif (ret < 0)\n\t\tgoto error;\n\n\tif (ret == 0) {\n\t\t \n\t\tbtrfs_err(fs_info, \"corrupted chunk tree devid -1 matched\");\n\t\tret = -EUCLEAN;\n\t\tgoto error;\n\t}\n\n\tret = btrfs_previous_item(fs_info->chunk_root, path,\n\t\t\t\t  BTRFS_DEV_ITEMS_OBJECTID,\n\t\t\t\t  BTRFS_DEV_ITEM_KEY);\n\tif (ret) {\n\t\t*devid_ret = 1;\n\t} else {\n\t\tbtrfs_item_key_to_cpu(path->nodes[0], &found_key,\n\t\t\t\t      path->slots[0]);\n\t\t*devid_ret = found_key.offset + 1;\n\t}\n\tret = 0;\nerror:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n \nstatic int btrfs_add_dev_item(struct btrfs_trans_handle *trans,\n\t\t\t    struct btrfs_device *device)\n{\n\tint ret;\n\tstruct btrfs_path *path;\n\tstruct btrfs_dev_item *dev_item;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_key key;\n\tunsigned long ptr;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = BTRFS_DEV_ITEMS_OBJECTID;\n\tkey.type = BTRFS_DEV_ITEM_KEY;\n\tkey.offset = device->devid;\n\n\tbtrfs_reserve_chunk_metadata(trans, true);\n\tret = btrfs_insert_empty_item(trans, trans->fs_info->chunk_root, path,\n\t\t\t\t      &key, sizeof(*dev_item));\n\tbtrfs_trans_release_chunk_metadata(trans);\n\tif (ret)\n\t\tgoto out;\n\n\tleaf = path->nodes[0];\n\tdev_item = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_dev_item);\n\n\tbtrfs_set_device_id(leaf, dev_item, device->devid);\n\tbtrfs_set_device_generation(leaf, dev_item, 0);\n\tbtrfs_set_device_type(leaf, dev_item, device->type);\n\tbtrfs_set_device_io_align(leaf, dev_item, device->io_align);\n\tbtrfs_set_device_io_width(leaf, dev_item, device->io_width);\n\tbtrfs_set_device_sector_size(leaf, dev_item, device->sector_size);\n\tbtrfs_set_device_total_bytes(leaf, dev_item,\n\t\t\t\t     btrfs_device_get_disk_total_bytes(device));\n\tbtrfs_set_device_bytes_used(leaf, dev_item,\n\t\t\t\t    btrfs_device_get_bytes_used(device));\n\tbtrfs_set_device_group(leaf, dev_item, 0);\n\tbtrfs_set_device_seek_speed(leaf, dev_item, 0);\n\tbtrfs_set_device_bandwidth(leaf, dev_item, 0);\n\tbtrfs_set_device_start_offset(leaf, dev_item, 0);\n\n\tptr = btrfs_device_uuid(dev_item);\n\twrite_extent_buffer(leaf, device->uuid, ptr, BTRFS_UUID_SIZE);\n\tptr = btrfs_device_fsid(dev_item);\n\twrite_extent_buffer(leaf, trans->fs_info->fs_devices->metadata_uuid,\n\t\t\t    ptr, BTRFS_FSID_SIZE);\n\tbtrfs_mark_buffer_dirty(trans, leaf);\n\n\tret = 0;\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n \nstatic void update_dev_time(const char *device_path)\n{\n\tstruct path path;\n\tint ret;\n\n\tret = kern_path(device_path, LOOKUP_FOLLOW, &path);\n\tif (ret)\n\t\treturn;\n\n\tinode_update_time(d_inode(path.dentry), S_MTIME | S_CTIME | S_VERSION);\n\tpath_put(&path);\n}\n\nstatic int btrfs_rm_dev_item(struct btrfs_trans_handle *trans,\n\t\t\t     struct btrfs_device *device)\n{\n\tstruct btrfs_root *root = device->fs_info->chunk_root;\n\tint ret;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key key;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = BTRFS_DEV_ITEMS_OBJECTID;\n\tkey.type = BTRFS_DEV_ITEM_KEY;\n\tkey.offset = device->devid;\n\n\tbtrfs_reserve_chunk_metadata(trans, false);\n\tret = btrfs_search_slot(trans, root, &key, path, -1, 1);\n\tbtrfs_trans_release_chunk_metadata(trans);\n\tif (ret) {\n\t\tif (ret > 0)\n\t\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\tret = btrfs_del_item(trans, root, path);\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n \nstatic int btrfs_check_raid_min_devices(struct btrfs_fs_info *fs_info,\n\t\tu64 num_devices)\n{\n\tu64 all_avail;\n\tunsigned seq;\n\tint i;\n\n\tdo {\n\t\tseq = read_seqbegin(&fs_info->profiles_lock);\n\n\t\tall_avail = fs_info->avail_data_alloc_bits |\n\t\t\t    fs_info->avail_system_alloc_bits |\n\t\t\t    fs_info->avail_metadata_alloc_bits;\n\t} while (read_seqretry(&fs_info->profiles_lock, seq));\n\n\tfor (i = 0; i < BTRFS_NR_RAID_TYPES; i++) {\n\t\tif (!(all_avail & btrfs_raid_array[i].bg_flag))\n\t\t\tcontinue;\n\n\t\tif (num_devices < btrfs_raid_array[i].devs_min)\n\t\t\treturn btrfs_raid_array[i].mindev_error;\n\t}\n\n\treturn 0;\n}\n\nstatic struct btrfs_device * btrfs_find_next_active_device(\n\t\tstruct btrfs_fs_devices *fs_devs, struct btrfs_device *device)\n{\n\tstruct btrfs_device *next_device;\n\n\tlist_for_each_entry(next_device, &fs_devs->devices, dev_list) {\n\t\tif (next_device != device &&\n\t\t    !test_bit(BTRFS_DEV_STATE_MISSING, &next_device->dev_state)\n\t\t    && next_device->bdev)\n\t\t\treturn next_device;\n\t}\n\n\treturn NULL;\n}\n\n \nvoid __cold btrfs_assign_next_active_device(struct btrfs_device *device,\n\t\t\t\t\t    struct btrfs_device *next_device)\n{\n\tstruct btrfs_fs_info *fs_info = device->fs_info;\n\n\tif (!next_device)\n\t\tnext_device = btrfs_find_next_active_device(fs_info->fs_devices,\n\t\t\t\t\t\t\t    device);\n\tASSERT(next_device);\n\n\tif (fs_info->sb->s_bdev &&\n\t\t\t(fs_info->sb->s_bdev == device->bdev))\n\t\tfs_info->sb->s_bdev = next_device->bdev;\n\n\tif (fs_info->fs_devices->latest_dev->bdev == device->bdev)\n\t\tfs_info->fs_devices->latest_dev = next_device;\n}\n\n \nstatic u64 btrfs_num_devices(struct btrfs_fs_info *fs_info)\n{\n\tu64 num_devices = fs_info->fs_devices->num_devices;\n\n\tdown_read(&fs_info->dev_replace.rwsem);\n\tif (btrfs_dev_replace_is_ongoing(&fs_info->dev_replace)) {\n\t\tASSERT(num_devices > 1);\n\t\tnum_devices--;\n\t}\n\tup_read(&fs_info->dev_replace.rwsem);\n\n\treturn num_devices;\n}\n\nstatic void btrfs_scratch_superblock(struct btrfs_fs_info *fs_info,\n\t\t\t\t     struct block_device *bdev, int copy_num)\n{\n\tstruct btrfs_super_block *disk_super;\n\tconst size_t len = sizeof(disk_super->magic);\n\tconst u64 bytenr = btrfs_sb_offset(copy_num);\n\tint ret;\n\n\tdisk_super = btrfs_read_disk_super(bdev, bytenr, bytenr);\n\tif (IS_ERR(disk_super))\n\t\treturn;\n\n\tmemset(&disk_super->magic, 0, len);\n\tfolio_mark_dirty(virt_to_folio(disk_super));\n\tbtrfs_release_disk_super(disk_super);\n\n\tret = sync_blockdev_range(bdev, bytenr, bytenr + len - 1);\n\tif (ret)\n\t\tbtrfs_warn(fs_info, \"error clearing superblock number %d (%d)\",\n\t\t\tcopy_num, ret);\n}\n\nvoid btrfs_scratch_superblocks(struct btrfs_fs_info *fs_info,\n\t\t\t       struct block_device *bdev,\n\t\t\t       const char *device_path)\n{\n\tint copy_num;\n\n\tif (!bdev)\n\t\treturn;\n\n\tfor (copy_num = 0; copy_num < BTRFS_SUPER_MIRROR_MAX; copy_num++) {\n\t\tif (bdev_is_zoned(bdev))\n\t\t\tbtrfs_reset_sb_log_zones(bdev, copy_num);\n\t\telse\n\t\t\tbtrfs_scratch_superblock(fs_info, bdev, copy_num);\n\t}\n\n\t \n\tbtrfs_kobject_uevent(bdev, KOBJ_CHANGE);\n\n\t \n\tupdate_dev_time(device_path);\n}\n\nint btrfs_rm_device(struct btrfs_fs_info *fs_info,\n\t\t    struct btrfs_dev_lookup_args *args,\n\t\t    struct block_device **bdev, void **holder)\n{\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *cur_devices;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tu64 num_devices;\n\tint ret = 0;\n\n\tif (btrfs_fs_incompat(fs_info, EXTENT_TREE_V2)) {\n\t\tbtrfs_err(fs_info, \"device remove not supported on extent tree v2 yet\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tnum_devices = btrfs_num_devices(fs_info);\n\n\tret = btrfs_check_raid_min_devices(fs_info, num_devices - 1);\n\tif (ret)\n\t\treturn ret;\n\n\tdevice = btrfs_find_device(fs_info->fs_devices, args);\n\tif (!device) {\n\t\tif (args->missing)\n\t\t\tret = BTRFS_ERROR_DEV_MISSING_NOT_FOUND;\n\t\telse\n\t\t\tret = -ENOENT;\n\t\treturn ret;\n\t}\n\n\tif (btrfs_pinned_by_swapfile(fs_info, device)) {\n\t\tbtrfs_warn_in_rcu(fs_info,\n\t\t  \"cannot remove device %s (devid %llu) due to active swapfile\",\n\t\t\t\t  btrfs_dev_name(device), device->devid);\n\t\treturn -ETXTBSY;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state))\n\t\treturn BTRFS_ERROR_DEV_TGT_REPLACE;\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state) &&\n\t    fs_info->fs_devices->rw_devices == 1)\n\t\treturn BTRFS_ERROR_DEV_ONLY_WRITABLE;\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_del_init(&device->dev_alloc_list);\n\t\tdevice->fs_devices->rw_devices--;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\n\tret = btrfs_shrink_device(device, 0);\n\tif (ret)\n\t\tgoto error_undo;\n\n\ttrans = btrfs_start_transaction(fs_info->chunk_root, 0);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto error_undo;\n\t}\n\n\tret = btrfs_rm_dev_item(trans, device);\n\tif (ret) {\n\t\t \n\t\tbtrfs_crit(fs_info,\n\t\t\t   \"failed to remove device item for devid %llu: %d\",\n\t\t\t   device->devid, ret);\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tbtrfs_end_transaction(trans);\n\t\treturn ret;\n\t}\n\n\tclear_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tbtrfs_scrub_cancel_dev(device);\n\n\t \n\n\t \n\tcur_devices = device->fs_devices;\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_del_rcu(&device->dev_list);\n\n\tcur_devices->num_devices--;\n\tcur_devices->total_devices--;\n\t \n\tif (cur_devices != fs_devices)\n\t\tfs_devices->total_devices--;\n\n\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state))\n\t\tcur_devices->missing_devices--;\n\n\tbtrfs_assign_next_active_device(device, NULL);\n\n\tif (device->bdev) {\n\t\tcur_devices->open_devices--;\n\t\t \n\t\tbtrfs_sysfs_remove_device(device);\n\t}\n\n\tnum_devices = btrfs_super_num_devices(fs_info->super_copy) - 1;\n\tbtrfs_set_super_num_devices(fs_info->super_copy, num_devices);\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\t \n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tbtrfs_scratch_superblocks(fs_info, device->bdev,\n\t\t\t\t\t  device->name->str);\n\t\tif (device->bdev) {\n\t\t\tsync_blockdev(device->bdev);\n\t\t\tinvalidate_bdev(device->bdev);\n\t\t}\n\t}\n\n\t*bdev = device->bdev;\n\t*holder = device->holder;\n\tsynchronize_rcu();\n\tbtrfs_free_device(device);\n\n\t \n\tif (cur_devices->num_devices == 0) {\n\t\tlist_del_init(&cur_devices->seed_list);\n\t\tASSERT(cur_devices->opened == 1);\n\t\tcur_devices->opened--;\n\t\tfree_fs_devices(cur_devices);\n\t}\n\n\tret = btrfs_commit_transaction(trans);\n\n\treturn ret;\n\nerror_undo:\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_add(&device->dev_alloc_list,\n\t\t\t &fs_devices->alloc_list);\n\t\tdevice->fs_devices->rw_devices++;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\treturn ret;\n}\n\nvoid btrfs_rm_dev_replace_remove_srcdev(struct btrfs_device *srcdev)\n{\n\tstruct btrfs_fs_devices *fs_devices;\n\n\tlockdep_assert_held(&srcdev->fs_info->fs_devices->device_list_mutex);\n\n\t \n\tfs_devices = srcdev->fs_devices;\n\n\tlist_del_rcu(&srcdev->dev_list);\n\tlist_del(&srcdev->dev_alloc_list);\n\tfs_devices->num_devices--;\n\tif (test_bit(BTRFS_DEV_STATE_MISSING, &srcdev->dev_state))\n\t\tfs_devices->missing_devices--;\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &srcdev->dev_state))\n\t\tfs_devices->rw_devices--;\n\n\tif (srcdev->bdev)\n\t\tfs_devices->open_devices--;\n}\n\nvoid btrfs_rm_dev_replace_free_srcdev(struct btrfs_device *srcdev)\n{\n\tstruct btrfs_fs_devices *fs_devices = srcdev->fs_devices;\n\n\tmutex_lock(&uuid_mutex);\n\n\tbtrfs_close_bdev(srcdev);\n\tsynchronize_rcu();\n\tbtrfs_free_device(srcdev);\n\n\t \n\tif (!fs_devices->num_devices) {\n\t\t \n\t\tASSERT(fs_devices->seeding);\n\n\t\tlist_del_init(&fs_devices->seed_list);\n\t\tclose_fs_devices(fs_devices);\n\t\tfree_fs_devices(fs_devices);\n\t}\n\tmutex_unlock(&uuid_mutex);\n}\n\nvoid btrfs_destroy_dev_replace_tgtdev(struct btrfs_device *tgtdev)\n{\n\tstruct btrfs_fs_devices *fs_devices = tgtdev->fs_info->fs_devices;\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\n\tbtrfs_sysfs_remove_device(tgtdev);\n\n\tif (tgtdev->bdev)\n\t\tfs_devices->open_devices--;\n\n\tfs_devices->num_devices--;\n\n\tbtrfs_assign_next_active_device(tgtdev, NULL);\n\n\tlist_del_rcu(&tgtdev->dev_list);\n\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\tbtrfs_scratch_superblocks(tgtdev->fs_info, tgtdev->bdev,\n\t\t\t\t  tgtdev->name->str);\n\n\tbtrfs_close_bdev(tgtdev);\n\tsynchronize_rcu();\n\tbtrfs_free_device(tgtdev);\n}\n\n \nint btrfs_get_dev_args_from_path(struct btrfs_fs_info *fs_info,\n\t\t\t\t struct btrfs_dev_lookup_args *args,\n\t\t\t\t const char *path)\n{\n\tstruct btrfs_super_block *disk_super;\n\tstruct block_device *bdev;\n\tint ret;\n\n\tif (!path || !path[0])\n\t\treturn -EINVAL;\n\tif (!strcmp(path, \"missing\")) {\n\t\targs->missing = true;\n\t\treturn 0;\n\t}\n\n\targs->uuid = kzalloc(BTRFS_UUID_SIZE, GFP_KERNEL);\n\targs->fsid = kzalloc(BTRFS_FSID_SIZE, GFP_KERNEL);\n\tif (!args->uuid || !args->fsid) {\n\t\tbtrfs_put_dev_args_from_path(args);\n\t\treturn -ENOMEM;\n\t}\n\n\tret = btrfs_get_bdev_and_sb(path, BLK_OPEN_READ, NULL, 0,\n\t\t\t\t    &bdev, &disk_super);\n\tif (ret) {\n\t\tbtrfs_put_dev_args_from_path(args);\n\t\treturn ret;\n\t}\n\n\targs->devid = btrfs_stack_device_id(&disk_super->dev_item);\n\tmemcpy(args->uuid, disk_super->dev_item.uuid, BTRFS_UUID_SIZE);\n\tif (btrfs_fs_incompat(fs_info, METADATA_UUID))\n\t\tmemcpy(args->fsid, disk_super->metadata_uuid, BTRFS_FSID_SIZE);\n\telse\n\t\tmemcpy(args->fsid, disk_super->fsid, BTRFS_FSID_SIZE);\n\tbtrfs_release_disk_super(disk_super);\n\tblkdev_put(bdev, NULL);\n\treturn 0;\n}\n\n \nvoid btrfs_put_dev_args_from_path(struct btrfs_dev_lookup_args *args)\n{\n\tkfree(args->uuid);\n\tkfree(args->fsid);\n\targs->uuid = NULL;\n\targs->fsid = NULL;\n}\n\nstruct btrfs_device *btrfs_find_device_by_devspec(\n\t\tstruct btrfs_fs_info *fs_info, u64 devid,\n\t\tconst char *device_path)\n{\n\tBTRFS_DEV_LOOKUP_ARGS(args);\n\tstruct btrfs_device *device;\n\tint ret;\n\n\tif (devid) {\n\t\targs.devid = devid;\n\t\tdevice = btrfs_find_device(fs_info->fs_devices, &args);\n\t\tif (!device)\n\t\t\treturn ERR_PTR(-ENOENT);\n\t\treturn device;\n\t}\n\n\tret = btrfs_get_dev_args_from_path(fs_info, &args, device_path);\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\tdevice = btrfs_find_device(fs_info->fs_devices, &args);\n\tbtrfs_put_dev_args_from_path(&args);\n\tif (!device)\n\t\treturn ERR_PTR(-ENOENT);\n\treturn device;\n}\n\nstatic struct btrfs_fs_devices *btrfs_init_sprout(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tstruct btrfs_fs_devices *old_devices;\n\tstruct btrfs_fs_devices *seed_devices;\n\n\tlockdep_assert_held(&uuid_mutex);\n\tif (!fs_devices->seeding)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t \n\tseed_devices = alloc_fs_devices(NULL, NULL);\n\tif (IS_ERR(seed_devices))\n\t\treturn seed_devices;\n\n\t \n\told_devices = clone_fs_devices(fs_devices);\n\tif (IS_ERR(old_devices)) {\n\t\tkfree(seed_devices);\n\t\treturn old_devices;\n\t}\n\n\tlist_add(&old_devices->fs_list, &fs_uuids);\n\n\tmemcpy(seed_devices, fs_devices, sizeof(*seed_devices));\n\tseed_devices->opened = 1;\n\tINIT_LIST_HEAD(&seed_devices->devices);\n\tINIT_LIST_HEAD(&seed_devices->alloc_list);\n\tmutex_init(&seed_devices->device_list_mutex);\n\n\treturn seed_devices;\n}\n\n \nstatic void btrfs_setup_sprout(struct btrfs_fs_info *fs_info,\n\t\t\t       struct btrfs_fs_devices *seed_devices)\n{\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tstruct btrfs_super_block *disk_super = fs_info->super_copy;\n\tstruct btrfs_device *device;\n\tu64 super_flags;\n\n\t \n\tlockdep_assert_held(&uuid_mutex);\n\n\t \n\tlockdep_assert_held(&fs_devices->device_list_mutex);\n\n\tlist_splice_init_rcu(&fs_devices->devices, &seed_devices->devices,\n\t\t\t      synchronize_rcu);\n\tlist_for_each_entry(device, &seed_devices->devices, dev_list)\n\t\tdevice->fs_devices = seed_devices;\n\n\tfs_devices->seeding = false;\n\tfs_devices->num_devices = 0;\n\tfs_devices->open_devices = 0;\n\tfs_devices->missing_devices = 0;\n\tfs_devices->rotating = false;\n\tlist_add(&seed_devices->seed_list, &fs_devices->seed_list);\n\n\tgenerate_random_uuid(fs_devices->fsid);\n\tmemcpy(fs_devices->metadata_uuid, fs_devices->fsid, BTRFS_FSID_SIZE);\n\tmemcpy(disk_super->fsid, fs_devices->fsid, BTRFS_FSID_SIZE);\n\n\tsuper_flags = btrfs_super_flags(disk_super) &\n\t\t      ~BTRFS_SUPER_FLAG_SEEDING;\n\tbtrfs_set_super_flags(disk_super, super_flags);\n}\n\n \nstatic int btrfs_finish_sprout(struct btrfs_trans_handle *trans)\n{\n\tBTRFS_DEV_LOOKUP_ARGS(args);\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_root *root = fs_info->chunk_root;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_dev_item *dev_item;\n\tstruct btrfs_device *device;\n\tstruct btrfs_key key;\n\tu8 fs_uuid[BTRFS_FSID_SIZE];\n\tu8 dev_uuid[BTRFS_UUID_SIZE];\n\tint ret;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = BTRFS_DEV_ITEMS_OBJECTID;\n\tkey.offset = 0;\n\tkey.type = BTRFS_DEV_ITEM_KEY;\n\n\twhile (1) {\n\t\tbtrfs_reserve_chunk_metadata(trans, false);\n\t\tret = btrfs_search_slot(trans, root, &key, path, 0, 1);\n\t\tbtrfs_trans_release_chunk_metadata(trans);\n\t\tif (ret < 0)\n\t\t\tgoto error;\n\n\t\tleaf = path->nodes[0];\nnext_slot:\n\t\tif (path->slots[0] >= btrfs_header_nritems(leaf)) {\n\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\tif (ret > 0)\n\t\t\t\tbreak;\n\t\t\tif (ret < 0)\n\t\t\t\tgoto error;\n\t\t\tleaf = path->nodes[0];\n\t\t\tbtrfs_item_key_to_cpu(leaf, &key, path->slots[0]);\n\t\t\tbtrfs_release_path(path);\n\t\t\tcontinue;\n\t\t}\n\n\t\tbtrfs_item_key_to_cpu(leaf, &key, path->slots[0]);\n\t\tif (key.objectid != BTRFS_DEV_ITEMS_OBJECTID ||\n\t\t    key.type != BTRFS_DEV_ITEM_KEY)\n\t\t\tbreak;\n\n\t\tdev_item = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t\t  struct btrfs_dev_item);\n\t\targs.devid = btrfs_device_id(leaf, dev_item);\n\t\tread_extent_buffer(leaf, dev_uuid, btrfs_device_uuid(dev_item),\n\t\t\t\t   BTRFS_UUID_SIZE);\n\t\tread_extent_buffer(leaf, fs_uuid, btrfs_device_fsid(dev_item),\n\t\t\t\t   BTRFS_FSID_SIZE);\n\t\targs.uuid = dev_uuid;\n\t\targs.fsid = fs_uuid;\n\t\tdevice = btrfs_find_device(fs_info->fs_devices, &args);\n\t\tBUG_ON(!device);  \n\n\t\tif (device->fs_devices->seeding) {\n\t\t\tbtrfs_set_device_generation(leaf, dev_item,\n\t\t\t\t\t\t    device->generation);\n\t\t\tbtrfs_mark_buffer_dirty(trans, leaf);\n\t\t}\n\n\t\tpath->slots[0]++;\n\t\tgoto next_slot;\n\t}\n\tret = 0;\nerror:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nint btrfs_init_new_device(struct btrfs_fs_info *fs_info, const char *device_path)\n{\n\tstruct btrfs_root *root = fs_info->dev_root;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_device *device;\n\tstruct block_device *bdev;\n\tstruct super_block *sb = fs_info->sb;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tstruct btrfs_fs_devices *seed_devices = NULL;\n\tu64 orig_super_total_bytes;\n\tu64 orig_super_num_devices;\n\tint ret = 0;\n\tbool seeding_dev = false;\n\tbool locked = false;\n\n\tif (sb_rdonly(sb) && !fs_devices->seeding)\n\t\treturn -EROFS;\n\n\tbdev = blkdev_get_by_path(device_path, BLK_OPEN_WRITE,\n\t\t\t\t  fs_info->bdev_holder, NULL);\n\tif (IS_ERR(bdev))\n\t\treturn PTR_ERR(bdev);\n\n\tif (!btrfs_check_device_zone_type(fs_info, bdev)) {\n\t\tret = -EINVAL;\n\t\tgoto error;\n\t}\n\n\tif (fs_devices->seeding) {\n\t\tseeding_dev = true;\n\t\tdown_write(&sb->s_umount);\n\t\tmutex_lock(&uuid_mutex);\n\t\tlocked = true;\n\t}\n\n\tsync_blockdev(bdev);\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(device, &fs_devices->devices, dev_list) {\n\t\tif (device->bdev == bdev) {\n\t\t\tret = -EEXIST;\n\t\t\trcu_read_unlock();\n\t\t\tgoto error;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\tdevice = btrfs_alloc_device(fs_info, NULL, NULL, device_path);\n\tif (IS_ERR(device)) {\n\t\t \n\t\tret = PTR_ERR(device);\n\t\tgoto error;\n\t}\n\n\tdevice->fs_info = fs_info;\n\tdevice->bdev = bdev;\n\tret = lookup_bdev(device_path, &device->devt);\n\tif (ret)\n\t\tgoto error_free_device;\n\n\tret = btrfs_get_dev_zone_info(device, false);\n\tif (ret)\n\t\tgoto error_free_device;\n\n\ttrans = btrfs_start_transaction(root, 0);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto error_free_zone;\n\t}\n\n\tset_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state);\n\tdevice->generation = trans->transid;\n\tdevice->io_width = fs_info->sectorsize;\n\tdevice->io_align = fs_info->sectorsize;\n\tdevice->sector_size = fs_info->sectorsize;\n\tdevice->total_bytes =\n\t\tround_down(bdev_nr_bytes(bdev), fs_info->sectorsize);\n\tdevice->disk_total_bytes = device->total_bytes;\n\tdevice->commit_total_bytes = device->total_bytes;\n\tset_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tclear_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state);\n\tdevice->holder = fs_info->bdev_holder;\n\tdevice->dev_stats_valid = 1;\n\tset_blocksize(device->bdev, BTRFS_BDEV_BLOCKSIZE);\n\n\tif (seeding_dev) {\n\t\tbtrfs_clear_sb_rdonly(sb);\n\n\t\t \n\t\tseed_devices = btrfs_init_sprout(fs_info);\n\t\tif (IS_ERR(seed_devices)) {\n\t\t\tret = PTR_ERR(seed_devices);\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tgoto error_trans;\n\t\t}\n\t}\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tif (seeding_dev) {\n\t\tbtrfs_setup_sprout(fs_info, seed_devices);\n\t\tbtrfs_assign_next_active_device(fs_info->fs_devices->latest_dev,\n\t\t\t\t\t\tdevice);\n\t}\n\n\tdevice->fs_devices = fs_devices;\n\n\tmutex_lock(&fs_info->chunk_mutex);\n\tlist_add_rcu(&device->dev_list, &fs_devices->devices);\n\tlist_add(&device->dev_alloc_list, &fs_devices->alloc_list);\n\tfs_devices->num_devices++;\n\tfs_devices->open_devices++;\n\tfs_devices->rw_devices++;\n\tfs_devices->total_devices++;\n\tfs_devices->total_rw_bytes += device->total_bytes;\n\n\tatomic64_add(device->total_bytes, &fs_info->free_chunk_space);\n\n\tif (!bdev_nonrot(bdev))\n\t\tfs_devices->rotating = true;\n\n\torig_super_total_bytes = btrfs_super_total_bytes(fs_info->super_copy);\n\tbtrfs_set_super_total_bytes(fs_info->super_copy,\n\t\tround_down(orig_super_total_bytes + device->total_bytes,\n\t\t\t   fs_info->sectorsize));\n\n\torig_super_num_devices = btrfs_super_num_devices(fs_info->super_copy);\n\tbtrfs_set_super_num_devices(fs_info->super_copy,\n\t\t\t\t    orig_super_num_devices + 1);\n\n\t \n\tbtrfs_clear_space_info_full(fs_info);\n\n\tmutex_unlock(&fs_info->chunk_mutex);\n\n\t \n\tbtrfs_sysfs_add_device(device);\n\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\tif (seeding_dev) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tret = init_first_rw_device(trans);\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t\tif (ret) {\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tgoto error_sysfs;\n\t\t}\n\t}\n\n\tret = btrfs_add_dev_item(trans, device);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tgoto error_sysfs;\n\t}\n\n\tif (seeding_dev) {\n\t\tret = btrfs_finish_sprout(trans);\n\t\tif (ret) {\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tgoto error_sysfs;\n\t\t}\n\n\t\t \n\t\tbtrfs_sysfs_update_sprout_fsid(fs_devices);\n\t}\n\n\tret = btrfs_commit_transaction(trans);\n\n\tif (seeding_dev) {\n\t\tmutex_unlock(&uuid_mutex);\n\t\tup_write(&sb->s_umount);\n\t\tlocked = false;\n\n\t\tif (ret)  \n\t\t\treturn ret;\n\n\t\tret = btrfs_relocate_sys_chunks(fs_info);\n\t\tif (ret < 0)\n\t\t\tbtrfs_handle_fs_error(fs_info, ret,\n\t\t\t\t    \"Failed to relocate sys chunks after device initialization. This can be fixed using the \\\"btrfs balance\\\" command.\");\n\t\ttrans = btrfs_attach_transaction(root);\n\t\tif (IS_ERR(trans)) {\n\t\t\tif (PTR_ERR(trans) == -ENOENT)\n\t\t\t\treturn 0;\n\t\t\tret = PTR_ERR(trans);\n\t\t\ttrans = NULL;\n\t\t\tgoto error_sysfs;\n\t\t}\n\t\tret = btrfs_commit_transaction(trans);\n\t}\n\n\t \n\tbtrfs_forget_devices(device->devt);\n\n\t \n\tupdate_dev_time(device_path);\n\n\treturn ret;\n\nerror_sysfs:\n\tbtrfs_sysfs_remove_device(device);\n\tmutex_lock(&fs_info->fs_devices->device_list_mutex);\n\tmutex_lock(&fs_info->chunk_mutex);\n\tlist_del_rcu(&device->dev_list);\n\tlist_del(&device->dev_alloc_list);\n\tfs_info->fs_devices->num_devices--;\n\tfs_info->fs_devices->open_devices--;\n\tfs_info->fs_devices->rw_devices--;\n\tfs_info->fs_devices->total_devices--;\n\tfs_info->fs_devices->total_rw_bytes -= device->total_bytes;\n\tatomic64_sub(device->total_bytes, &fs_info->free_chunk_space);\n\tbtrfs_set_super_total_bytes(fs_info->super_copy,\n\t\t\t\t    orig_super_total_bytes);\n\tbtrfs_set_super_num_devices(fs_info->super_copy,\n\t\t\t\t    orig_super_num_devices);\n\tmutex_unlock(&fs_info->chunk_mutex);\n\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\nerror_trans:\n\tif (seeding_dev)\n\t\tbtrfs_set_sb_rdonly(sb);\n\tif (trans)\n\t\tbtrfs_end_transaction(trans);\nerror_free_zone:\n\tbtrfs_destroy_dev_zone_info(device);\nerror_free_device:\n\tbtrfs_free_device(device);\nerror:\n\tblkdev_put(bdev, fs_info->bdev_holder);\n\tif (locked) {\n\t\tmutex_unlock(&uuid_mutex);\n\t\tup_write(&sb->s_umount);\n\t}\n\treturn ret;\n}\n\nstatic noinline int btrfs_update_device(struct btrfs_trans_handle *trans,\n\t\t\t\t\tstruct btrfs_device *device)\n{\n\tint ret;\n\tstruct btrfs_path *path;\n\tstruct btrfs_root *root = device->fs_info->chunk_root;\n\tstruct btrfs_dev_item *dev_item;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_key key;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = BTRFS_DEV_ITEMS_OBJECTID;\n\tkey.type = BTRFS_DEV_ITEM_KEY;\n\tkey.offset = device->devid;\n\n\tret = btrfs_search_slot(trans, root, &key, path, 0, 1);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tif (ret > 0) {\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\tleaf = path->nodes[0];\n\tdev_item = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_dev_item);\n\n\tbtrfs_set_device_id(leaf, dev_item, device->devid);\n\tbtrfs_set_device_type(leaf, dev_item, device->type);\n\tbtrfs_set_device_io_align(leaf, dev_item, device->io_align);\n\tbtrfs_set_device_io_width(leaf, dev_item, device->io_width);\n\tbtrfs_set_device_sector_size(leaf, dev_item, device->sector_size);\n\tbtrfs_set_device_total_bytes(leaf, dev_item,\n\t\t\t\t     btrfs_device_get_disk_total_bytes(device));\n\tbtrfs_set_device_bytes_used(leaf, dev_item,\n\t\t\t\t    btrfs_device_get_bytes_used(device));\n\tbtrfs_mark_buffer_dirty(trans, leaf);\n\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nint btrfs_grow_device(struct btrfs_trans_handle *trans,\n\t\t      struct btrfs_device *device, u64 new_size)\n{\n\tstruct btrfs_fs_info *fs_info = device->fs_info;\n\tstruct btrfs_super_block *super_copy = fs_info->super_copy;\n\tu64 old_total;\n\tu64 diff;\n\tint ret;\n\n\tif (!test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state))\n\t\treturn -EACCES;\n\n\tnew_size = round_down(new_size, fs_info->sectorsize);\n\n\tmutex_lock(&fs_info->chunk_mutex);\n\told_total = btrfs_super_total_bytes(super_copy);\n\tdiff = round_down(new_size - device->total_bytes, fs_info->sectorsize);\n\n\tif (new_size <= device->total_bytes ||\n\t    test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t\treturn -EINVAL;\n\t}\n\n\tbtrfs_set_super_total_bytes(super_copy,\n\t\t\tround_down(old_total + diff, fs_info->sectorsize));\n\tdevice->fs_devices->total_rw_bytes += diff;\n\n\tbtrfs_device_set_total_bytes(device, new_size);\n\tbtrfs_device_set_disk_total_bytes(device, new_size);\n\tbtrfs_clear_space_info_full(device->fs_info);\n\tif (list_empty(&device->post_commit_list))\n\t\tlist_add_tail(&device->post_commit_list,\n\t\t\t      &trans->transaction->dev_update_list);\n\tmutex_unlock(&fs_info->chunk_mutex);\n\n\tbtrfs_reserve_chunk_metadata(trans, false);\n\tret = btrfs_update_device(trans, device);\n\tbtrfs_trans_release_chunk_metadata(trans);\n\n\treturn ret;\n}\n\nstatic int btrfs_free_chunk(struct btrfs_trans_handle *trans, u64 chunk_offset)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_root *root = fs_info->chunk_root;\n\tint ret;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key key;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = BTRFS_FIRST_CHUNK_TREE_OBJECTID;\n\tkey.offset = chunk_offset;\n\tkey.type = BTRFS_CHUNK_ITEM_KEY;\n\n\tret = btrfs_search_slot(trans, root, &key, path, -1, 1);\n\tif (ret < 0)\n\t\tgoto out;\n\telse if (ret > 0) {  \n\t\tbtrfs_handle_fs_error(fs_info, -ENOENT,\n\t\t\t\t      \"Failed lookup while freeing chunk.\");\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\tret = btrfs_del_item(trans, root, path);\n\tif (ret < 0)\n\t\tbtrfs_handle_fs_error(fs_info, ret,\n\t\t\t\t      \"Failed to delete chunk item.\");\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nstatic int btrfs_del_sys_chunk(struct btrfs_fs_info *fs_info, u64 chunk_offset)\n{\n\tstruct btrfs_super_block *super_copy = fs_info->super_copy;\n\tstruct btrfs_disk_key *disk_key;\n\tstruct btrfs_chunk *chunk;\n\tu8 *ptr;\n\tint ret = 0;\n\tu32 num_stripes;\n\tu32 array_size;\n\tu32 len = 0;\n\tu32 cur;\n\tstruct btrfs_key key;\n\n\tlockdep_assert_held(&fs_info->chunk_mutex);\n\tarray_size = btrfs_super_sys_array_size(super_copy);\n\n\tptr = super_copy->sys_chunk_array;\n\tcur = 0;\n\n\twhile (cur < array_size) {\n\t\tdisk_key = (struct btrfs_disk_key *)ptr;\n\t\tbtrfs_disk_key_to_cpu(&key, disk_key);\n\n\t\tlen = sizeof(*disk_key);\n\n\t\tif (key.type == BTRFS_CHUNK_ITEM_KEY) {\n\t\t\tchunk = (struct btrfs_chunk *)(ptr + len);\n\t\t\tnum_stripes = btrfs_stack_chunk_num_stripes(chunk);\n\t\t\tlen += btrfs_chunk_item_size(num_stripes);\n\t\t} else {\n\t\t\tret = -EIO;\n\t\t\tbreak;\n\t\t}\n\t\tif (key.objectid == BTRFS_FIRST_CHUNK_TREE_OBJECTID &&\n\t\t    key.offset == chunk_offset) {\n\t\t\tmemmove(ptr, ptr + len, array_size - (cur + len));\n\t\t\tarray_size -= len;\n\t\t\tbtrfs_set_super_sys_array_size(super_copy, array_size);\n\t\t} else {\n\t\t\tptr += len;\n\t\t\tcur += len;\n\t\t}\n\t}\n\treturn ret;\n}\n\n \nstruct extent_map *btrfs_get_chunk_map(struct btrfs_fs_info *fs_info,\n\t\t\t\t       u64 logical, u64 length)\n{\n\tstruct extent_map_tree *em_tree;\n\tstruct extent_map *em;\n\n\tem_tree = &fs_info->mapping_tree;\n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, logical, length);\n\tread_unlock(&em_tree->lock);\n\n\tif (!em) {\n\t\tbtrfs_crit(fs_info,\n\t\t\t   \"unable to find chunk map for logical %llu length %llu\",\n\t\t\t   logical, length);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif (em->start > logical || em->start + em->len <= logical) {\n\t\tbtrfs_crit(fs_info,\n\t\t\t   \"found a bad chunk map, wanted %llu-%llu, found %llu-%llu\",\n\t\t\t   logical, logical + length, em->start, em->start + em->len);\n\t\tfree_extent_map(em);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\t \n\treturn em;\n}\n\nstatic int remove_chunk_item(struct btrfs_trans_handle *trans,\n\t\t\t     struct map_lookup *map, u64 chunk_offset)\n{\n\tint i;\n\n\t \n\tlockdep_assert_held(&trans->fs_info->chunk_mutex);\n\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tint ret;\n\n\t\tret = btrfs_update_device(trans, map->stripes[i].dev);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn btrfs_free_chunk(trans, chunk_offset);\n}\n\nint btrfs_remove_chunk(struct btrfs_trans_handle *trans, u64 chunk_offset)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tu64 dev_extent_len = 0;\n\tint i, ret = 0;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\n\tem = btrfs_get_chunk_map(fs_info, chunk_offset, 1);\n\tif (IS_ERR(em)) {\n\t\t \n\t\tASSERT(0);\n\t\treturn PTR_ERR(em);\n\t}\n\tmap = em->map_lookup;\n\n\t \n\tmutex_lock(&fs_devices->device_list_mutex);\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tstruct btrfs_device *device = map->stripes[i].dev;\n\t\tret = btrfs_free_dev_extent(trans, device,\n\t\t\t\t\t    map->stripes[i].physical,\n\t\t\t\t\t    &dev_extent_len);\n\t\tif (ret) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (device->bytes_used > 0) {\n\t\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\t\tbtrfs_device_set_bytes_used(device,\n\t\t\t\t\tdevice->bytes_used - dev_extent_len);\n\t\t\tatomic64_add(dev_extent_len, &fs_info->free_chunk_space);\n\t\t\tbtrfs_clear_space_info_full(fs_info);\n\t\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t\t}\n\t}\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\t \n\ttrans->removing_chunk = true;\n\tmutex_lock(&fs_info->chunk_mutex);\n\n\tcheck_system_chunk(trans, map->type);\n\n\tret = remove_chunk_item(trans, map, chunk_offset);\n\t \n\tif (ret == -ENOSPC) {\n\t\tconst u64 sys_flags = btrfs_system_alloc_profile(fs_info);\n\t\tstruct btrfs_block_group *sys_bg;\n\n\t\tsys_bg = btrfs_create_chunk(trans, sys_flags);\n\t\tif (IS_ERR(sys_bg)) {\n\t\t\tret = PTR_ERR(sys_bg);\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tgoto out;\n\t\t}\n\n\t\tret = btrfs_chunk_alloc_add_chunk_item(trans, sys_bg);\n\t\tif (ret) {\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tgoto out;\n\t\t}\n\n\t\tret = remove_chunk_item(trans, map, chunk_offset);\n\t\tif (ret) {\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tgoto out;\n\t\t}\n\t} else if (ret) {\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tgoto out;\n\t}\n\n\ttrace_btrfs_chunk_free(fs_info, map, chunk_offset, em->len);\n\n\tif (map->type & BTRFS_BLOCK_GROUP_SYSTEM) {\n\t\tret = btrfs_del_sys_chunk(fs_info, chunk_offset);\n\t\tif (ret) {\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tmutex_unlock(&fs_info->chunk_mutex);\n\ttrans->removing_chunk = false;\n\n\t \n\tbtrfs_trans_release_chunk_metadata(trans);\n\n\tret = btrfs_remove_block_group(trans, chunk_offset, em);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tgoto out;\n\t}\n\nout:\n\tif (trans->removing_chunk) {\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t\ttrans->removing_chunk = false;\n\t}\n\t \n\tfree_extent_map(em);\n\treturn ret;\n}\n\nint btrfs_relocate_chunk(struct btrfs_fs_info *fs_info, u64 chunk_offset)\n{\n\tstruct btrfs_root *root = fs_info->chunk_root;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_block_group *block_group;\n\tu64 length;\n\tint ret;\n\n\tif (btrfs_fs_incompat(fs_info, EXTENT_TREE_V2)) {\n\t\tbtrfs_err(fs_info,\n\t\t\t  \"relocate: not supported on extent tree v2 yet\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tlockdep_assert_held(&fs_info->reclaim_bgs_lock);\n\n\t \n\tbtrfs_scrub_pause(fs_info);\n\tret = btrfs_relocate_block_group(fs_info, chunk_offset);\n\tbtrfs_scrub_continue(fs_info);\n\tif (ret) {\n\t\t \n\t\tif (BTRFS_FS_ERROR(fs_info))\n\t\t\tbtrfs_scrub_cancel(fs_info);\n\t\treturn ret;\n\t}\n\n\tblock_group = btrfs_lookup_block_group(fs_info, chunk_offset);\n\tif (!block_group)\n\t\treturn -ENOENT;\n\tbtrfs_discard_cancel_work(&fs_info->discard_ctl, block_group);\n\tlength = block_group->length;\n\tbtrfs_put_block_group(block_group);\n\n\t \n\tif (btrfs_is_zoned(fs_info)) {\n\t\tret = btrfs_discard_extent(fs_info, chunk_offset, length, NULL);\n\t\tif (ret)\n\t\t\tbtrfs_info(fs_info,\n\t\t\t\t\"failed to reset zone %llu after relocation\",\n\t\t\t\tchunk_offset);\n\t}\n\n\ttrans = btrfs_start_trans_remove_block_group(root->fs_info,\n\t\t\t\t\t\t     chunk_offset);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tbtrfs_handle_fs_error(root->fs_info, ret, NULL);\n\t\treturn ret;\n\t}\n\n\t \n\tret = btrfs_remove_chunk(trans, chunk_offset);\n\tbtrfs_end_transaction(trans);\n\treturn ret;\n}\n\nstatic int btrfs_relocate_sys_chunks(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_root *chunk_root = fs_info->chunk_root;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_chunk *chunk;\n\tstruct btrfs_key key;\n\tstruct btrfs_key found_key;\n\tu64 chunk_type;\n\tbool retried = false;\n\tint failed = 0;\n\tint ret;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\nagain:\n\tkey.objectid = BTRFS_FIRST_CHUNK_TREE_OBJECTID;\n\tkey.offset = (u64)-1;\n\tkey.type = BTRFS_CHUNK_ITEM_KEY;\n\n\twhile (1) {\n\t\tmutex_lock(&fs_info->reclaim_bgs_lock);\n\t\tret = btrfs_search_slot(NULL, chunk_root, &key, path, 0, 0);\n\t\tif (ret < 0) {\n\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\t\tgoto error;\n\t\t}\n\t\tBUG_ON(ret == 0);  \n\n\t\tret = btrfs_previous_item(chunk_root, path, key.objectid,\n\t\t\t\t\t  key.type);\n\t\tif (ret)\n\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\tif (ret < 0)\n\t\t\tgoto error;\n\t\tif (ret > 0)\n\t\t\tbreak;\n\n\t\tleaf = path->nodes[0];\n\t\tbtrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);\n\n\t\tchunk = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t       struct btrfs_chunk);\n\t\tchunk_type = btrfs_chunk_type(leaf, chunk);\n\t\tbtrfs_release_path(path);\n\n\t\tif (chunk_type & BTRFS_BLOCK_GROUP_SYSTEM) {\n\t\t\tret = btrfs_relocate_chunk(fs_info, found_key.offset);\n\t\t\tif (ret == -ENOSPC)\n\t\t\t\tfailed++;\n\t\t\telse\n\t\t\t\tBUG_ON(ret);\n\t\t}\n\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\n\t\tif (found_key.offset == 0)\n\t\t\tbreak;\n\t\tkey.offset = found_key.offset - 1;\n\t}\n\tret = 0;\n\tif (failed && !retried) {\n\t\tfailed = 0;\n\t\tretried = true;\n\t\tgoto again;\n\t} else if (WARN_ON(failed && retried)) {\n\t\tret = -ENOSPC;\n\t}\nerror:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n \nstatic int btrfs_may_alloc_data_chunk(struct btrfs_fs_info *fs_info,\n\t\t\t\t      u64 chunk_offset)\n{\n\tstruct btrfs_block_group *cache;\n\tu64 bytes_used;\n\tu64 chunk_type;\n\n\tcache = btrfs_lookup_block_group(fs_info, chunk_offset);\n\tASSERT(cache);\n\tchunk_type = cache->flags;\n\tbtrfs_put_block_group(cache);\n\n\tif (!(chunk_type & BTRFS_BLOCK_GROUP_DATA))\n\t\treturn 0;\n\n\tspin_lock(&fs_info->data_sinfo->lock);\n\tbytes_used = fs_info->data_sinfo->bytes_used;\n\tspin_unlock(&fs_info->data_sinfo->lock);\n\n\tif (!bytes_used) {\n\t\tstruct btrfs_trans_handle *trans;\n\t\tint ret;\n\n\t\ttrans =\tbtrfs_join_transaction(fs_info->tree_root);\n\t\tif (IS_ERR(trans))\n\t\t\treturn PTR_ERR(trans);\n\n\t\tret = btrfs_force_chunk_alloc(trans, BTRFS_BLOCK_GROUP_DATA);\n\t\tbtrfs_end_transaction(trans);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic int insert_balance_item(struct btrfs_fs_info *fs_info,\n\t\t\t       struct btrfs_balance_control *bctl)\n{\n\tstruct btrfs_root *root = fs_info->tree_root;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_balance_item *item;\n\tstruct btrfs_disk_balance_args disk_bargs;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_key key;\n\tint ret, err;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\ttrans = btrfs_start_transaction(root, 0);\n\tif (IS_ERR(trans)) {\n\t\tbtrfs_free_path(path);\n\t\treturn PTR_ERR(trans);\n\t}\n\n\tkey.objectid = BTRFS_BALANCE_OBJECTID;\n\tkey.type = BTRFS_TEMPORARY_ITEM_KEY;\n\tkey.offset = 0;\n\n\tret = btrfs_insert_empty_item(trans, root, path, &key,\n\t\t\t\t      sizeof(*item));\n\tif (ret)\n\t\tgoto out;\n\n\tleaf = path->nodes[0];\n\titem = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_balance_item);\n\n\tmemzero_extent_buffer(leaf, (unsigned long)item, sizeof(*item));\n\n\tbtrfs_cpu_balance_args_to_disk(&disk_bargs, &bctl->data);\n\tbtrfs_set_balance_data(leaf, item, &disk_bargs);\n\tbtrfs_cpu_balance_args_to_disk(&disk_bargs, &bctl->meta);\n\tbtrfs_set_balance_meta(leaf, item, &disk_bargs);\n\tbtrfs_cpu_balance_args_to_disk(&disk_bargs, &bctl->sys);\n\tbtrfs_set_balance_sys(leaf, item, &disk_bargs);\n\n\tbtrfs_set_balance_flags(leaf, item, bctl->flags);\n\n\tbtrfs_mark_buffer_dirty(trans, leaf);\nout:\n\tbtrfs_free_path(path);\n\terr = btrfs_commit_transaction(trans);\n\tif (err && !ret)\n\t\tret = err;\n\treturn ret;\n}\n\nstatic int del_balance_item(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_root *root = fs_info->tree_root;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key key;\n\tint ret, err;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\ttrans = btrfs_start_transaction_fallback_global_rsv(root, 0);\n\tif (IS_ERR(trans)) {\n\t\tbtrfs_free_path(path);\n\t\treturn PTR_ERR(trans);\n\t}\n\n\tkey.objectid = BTRFS_BALANCE_OBJECTID;\n\tkey.type = BTRFS_TEMPORARY_ITEM_KEY;\n\tkey.offset = 0;\n\n\tret = btrfs_search_slot(trans, root, &key, path, -1, 1);\n\tif (ret < 0)\n\t\tgoto out;\n\tif (ret > 0) {\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\tret = btrfs_del_item(trans, root, path);\nout:\n\tbtrfs_free_path(path);\n\terr = btrfs_commit_transaction(trans);\n\tif (err && !ret)\n\t\tret = err;\n\treturn ret;\n}\n\n \nstatic void update_balance_args(struct btrfs_balance_control *bctl)\n{\n\t \n\tif (bctl->data.flags & BTRFS_BALANCE_ARGS_CONVERT)\n\t\tbctl->data.flags |= BTRFS_BALANCE_ARGS_SOFT;\n\tif (bctl->sys.flags & BTRFS_BALANCE_ARGS_CONVERT)\n\t\tbctl->sys.flags |= BTRFS_BALANCE_ARGS_SOFT;\n\tif (bctl->meta.flags & BTRFS_BALANCE_ARGS_CONVERT)\n\t\tbctl->meta.flags |= BTRFS_BALANCE_ARGS_SOFT;\n\n\t \n\tif (!(bctl->data.flags & BTRFS_BALANCE_ARGS_USAGE) &&\n\t    !(bctl->data.flags & BTRFS_BALANCE_ARGS_USAGE_RANGE) &&\n\t    !(bctl->data.flags & BTRFS_BALANCE_ARGS_CONVERT)) {\n\t\tbctl->data.flags |= BTRFS_BALANCE_ARGS_USAGE;\n\t\tbctl->data.usage = 90;\n\t}\n\tif (!(bctl->sys.flags & BTRFS_BALANCE_ARGS_USAGE) &&\n\t    !(bctl->sys.flags & BTRFS_BALANCE_ARGS_USAGE_RANGE) &&\n\t    !(bctl->sys.flags & BTRFS_BALANCE_ARGS_CONVERT)) {\n\t\tbctl->sys.flags |= BTRFS_BALANCE_ARGS_USAGE;\n\t\tbctl->sys.usage = 90;\n\t}\n\tif (!(bctl->meta.flags & BTRFS_BALANCE_ARGS_USAGE) &&\n\t    !(bctl->meta.flags & BTRFS_BALANCE_ARGS_USAGE_RANGE) &&\n\t    !(bctl->meta.flags & BTRFS_BALANCE_ARGS_CONVERT)) {\n\t\tbctl->meta.flags |= BTRFS_BALANCE_ARGS_USAGE;\n\t\tbctl->meta.usage = 90;\n\t}\n}\n\n \nstatic void reset_balance_state(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_balance_control *bctl = fs_info->balance_ctl;\n\tint ret;\n\n\tBUG_ON(!fs_info->balance_ctl);\n\n\tspin_lock(&fs_info->balance_lock);\n\tfs_info->balance_ctl = NULL;\n\tspin_unlock(&fs_info->balance_lock);\n\n\tkfree(bctl);\n\tret = del_balance_item(fs_info);\n\tif (ret)\n\t\tbtrfs_handle_fs_error(fs_info, ret, NULL);\n}\n\n \nstatic int chunk_profiles_filter(u64 chunk_type,\n\t\t\t\t struct btrfs_balance_args *bargs)\n{\n\tchunk_type = chunk_to_extended(chunk_type) &\n\t\t\t\tBTRFS_EXTENDED_PROFILE_MASK;\n\n\tif (bargs->profiles & chunk_type)\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic int chunk_usage_range_filter(struct btrfs_fs_info *fs_info, u64 chunk_offset,\n\t\t\t      struct btrfs_balance_args *bargs)\n{\n\tstruct btrfs_block_group *cache;\n\tu64 chunk_used;\n\tu64 user_thresh_min;\n\tu64 user_thresh_max;\n\tint ret = 1;\n\n\tcache = btrfs_lookup_block_group(fs_info, chunk_offset);\n\tchunk_used = cache->used;\n\n\tif (bargs->usage_min == 0)\n\t\tuser_thresh_min = 0;\n\telse\n\t\tuser_thresh_min = mult_perc(cache->length, bargs->usage_min);\n\n\tif (bargs->usage_max == 0)\n\t\tuser_thresh_max = 1;\n\telse if (bargs->usage_max > 100)\n\t\tuser_thresh_max = cache->length;\n\telse\n\t\tuser_thresh_max = mult_perc(cache->length, bargs->usage_max);\n\n\tif (user_thresh_min <= chunk_used && chunk_used < user_thresh_max)\n\t\tret = 0;\n\n\tbtrfs_put_block_group(cache);\n\treturn ret;\n}\n\nstatic int chunk_usage_filter(struct btrfs_fs_info *fs_info,\n\t\tu64 chunk_offset, struct btrfs_balance_args *bargs)\n{\n\tstruct btrfs_block_group *cache;\n\tu64 chunk_used, user_thresh;\n\tint ret = 1;\n\n\tcache = btrfs_lookup_block_group(fs_info, chunk_offset);\n\tchunk_used = cache->used;\n\n\tif (bargs->usage_min == 0)\n\t\tuser_thresh = 1;\n\telse if (bargs->usage > 100)\n\t\tuser_thresh = cache->length;\n\telse\n\t\tuser_thresh = mult_perc(cache->length, bargs->usage);\n\n\tif (chunk_used < user_thresh)\n\t\tret = 0;\n\n\tbtrfs_put_block_group(cache);\n\treturn ret;\n}\n\nstatic int chunk_devid_filter(struct extent_buffer *leaf,\n\t\t\t      struct btrfs_chunk *chunk,\n\t\t\t      struct btrfs_balance_args *bargs)\n{\n\tstruct btrfs_stripe *stripe;\n\tint num_stripes = btrfs_chunk_num_stripes(leaf, chunk);\n\tint i;\n\n\tfor (i = 0; i < num_stripes; i++) {\n\t\tstripe = btrfs_stripe_nr(chunk, i);\n\t\tif (btrfs_stripe_devid(leaf, stripe) == bargs->devid)\n\t\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\nstatic u64 calc_data_stripes(u64 type, int num_stripes)\n{\n\tconst int index = btrfs_bg_flags_to_raid_index(type);\n\tconst int ncopies = btrfs_raid_array[index].ncopies;\n\tconst int nparity = btrfs_raid_array[index].nparity;\n\n\treturn (num_stripes - nparity) / ncopies;\n}\n\n \nstatic int chunk_drange_filter(struct extent_buffer *leaf,\n\t\t\t       struct btrfs_chunk *chunk,\n\t\t\t       struct btrfs_balance_args *bargs)\n{\n\tstruct btrfs_stripe *stripe;\n\tint num_stripes = btrfs_chunk_num_stripes(leaf, chunk);\n\tu64 stripe_offset;\n\tu64 stripe_length;\n\tu64 type;\n\tint factor;\n\tint i;\n\n\tif (!(bargs->flags & BTRFS_BALANCE_ARGS_DEVID))\n\t\treturn 0;\n\n\ttype = btrfs_chunk_type(leaf, chunk);\n\tfactor = calc_data_stripes(type, num_stripes);\n\n\tfor (i = 0; i < num_stripes; i++) {\n\t\tstripe = btrfs_stripe_nr(chunk, i);\n\t\tif (btrfs_stripe_devid(leaf, stripe) != bargs->devid)\n\t\t\tcontinue;\n\n\t\tstripe_offset = btrfs_stripe_offset(leaf, stripe);\n\t\tstripe_length = btrfs_chunk_length(leaf, chunk);\n\t\tstripe_length = div_u64(stripe_length, factor);\n\n\t\tif (stripe_offset < bargs->pend &&\n\t\t    stripe_offset + stripe_length > bargs->pstart)\n\t\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\n \nstatic int chunk_vrange_filter(struct extent_buffer *leaf,\n\t\t\t       struct btrfs_chunk *chunk,\n\t\t\t       u64 chunk_offset,\n\t\t\t       struct btrfs_balance_args *bargs)\n{\n\tif (chunk_offset < bargs->vend &&\n\t    chunk_offset + btrfs_chunk_length(leaf, chunk) > bargs->vstart)\n\t\t \n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic int chunk_stripes_range_filter(struct extent_buffer *leaf,\n\t\t\t       struct btrfs_chunk *chunk,\n\t\t\t       struct btrfs_balance_args *bargs)\n{\n\tint num_stripes = btrfs_chunk_num_stripes(leaf, chunk);\n\n\tif (bargs->stripes_min <= num_stripes\n\t\t\t&& num_stripes <= bargs->stripes_max)\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic int chunk_soft_convert_filter(u64 chunk_type,\n\t\t\t\t     struct btrfs_balance_args *bargs)\n{\n\tif (!(bargs->flags & BTRFS_BALANCE_ARGS_CONVERT))\n\t\treturn 0;\n\n\tchunk_type = chunk_to_extended(chunk_type) &\n\t\t\t\tBTRFS_EXTENDED_PROFILE_MASK;\n\n\tif (bargs->target == chunk_type)\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic int should_balance_chunk(struct extent_buffer *leaf,\n\t\t\t\tstruct btrfs_chunk *chunk, u64 chunk_offset)\n{\n\tstruct btrfs_fs_info *fs_info = leaf->fs_info;\n\tstruct btrfs_balance_control *bctl = fs_info->balance_ctl;\n\tstruct btrfs_balance_args *bargs = NULL;\n\tu64 chunk_type = btrfs_chunk_type(leaf, chunk);\n\n\t \n\tif (!((chunk_type & BTRFS_BLOCK_GROUP_TYPE_MASK) &\n\t      (bctl->flags & BTRFS_BALANCE_TYPE_MASK))) {\n\t\treturn 0;\n\t}\n\n\tif (chunk_type & BTRFS_BLOCK_GROUP_DATA)\n\t\tbargs = &bctl->data;\n\telse if (chunk_type & BTRFS_BLOCK_GROUP_SYSTEM)\n\t\tbargs = &bctl->sys;\n\telse if (chunk_type & BTRFS_BLOCK_GROUP_METADATA)\n\t\tbargs = &bctl->meta;\n\n\t \n\tif ((bargs->flags & BTRFS_BALANCE_ARGS_PROFILES) &&\n\t    chunk_profiles_filter(chunk_type, bargs)) {\n\t\treturn 0;\n\t}\n\n\t \n\tif ((bargs->flags & BTRFS_BALANCE_ARGS_USAGE) &&\n\t    chunk_usage_filter(fs_info, chunk_offset, bargs)) {\n\t\treturn 0;\n\t} else if ((bargs->flags & BTRFS_BALANCE_ARGS_USAGE_RANGE) &&\n\t    chunk_usage_range_filter(fs_info, chunk_offset, bargs)) {\n\t\treturn 0;\n\t}\n\n\t \n\tif ((bargs->flags & BTRFS_BALANCE_ARGS_DEVID) &&\n\t    chunk_devid_filter(leaf, chunk, bargs)) {\n\t\treturn 0;\n\t}\n\n\t \n\tif ((bargs->flags & BTRFS_BALANCE_ARGS_DRANGE) &&\n\t    chunk_drange_filter(leaf, chunk, bargs)) {\n\t\treturn 0;\n\t}\n\n\t \n\tif ((bargs->flags & BTRFS_BALANCE_ARGS_VRANGE) &&\n\t    chunk_vrange_filter(leaf, chunk, chunk_offset, bargs)) {\n\t\treturn 0;\n\t}\n\n\t \n\tif ((bargs->flags & BTRFS_BALANCE_ARGS_STRIPES_RANGE) &&\n\t    chunk_stripes_range_filter(leaf, chunk, bargs)) {\n\t\treturn 0;\n\t}\n\n\t \n\tif ((bargs->flags & BTRFS_BALANCE_ARGS_SOFT) &&\n\t    chunk_soft_convert_filter(chunk_type, bargs)) {\n\t\treturn 0;\n\t}\n\n\t \n\tif ((bargs->flags & BTRFS_BALANCE_ARGS_LIMIT)) {\n\t\tif (bargs->limit == 0)\n\t\t\treturn 0;\n\t\telse\n\t\t\tbargs->limit--;\n\t} else if ((bargs->flags & BTRFS_BALANCE_ARGS_LIMIT_RANGE)) {\n\t\t \n\t\tif (bargs->limit_max == 0)\n\t\t\treturn 0;\n\t\telse\n\t\t\tbargs->limit_max--;\n\t}\n\n\treturn 1;\n}\n\nstatic int __btrfs_balance(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_balance_control *bctl = fs_info->balance_ctl;\n\tstruct btrfs_root *chunk_root = fs_info->chunk_root;\n\tu64 chunk_type;\n\tstruct btrfs_chunk *chunk;\n\tstruct btrfs_path *path = NULL;\n\tstruct btrfs_key key;\n\tstruct btrfs_key found_key;\n\tstruct extent_buffer *leaf;\n\tint slot;\n\tint ret;\n\tint enospc_errors = 0;\n\tbool counting = true;\n\t \n\tu64 limit_data = bctl->data.limit;\n\tu64 limit_meta = bctl->meta.limit;\n\tu64 limit_sys = bctl->sys.limit;\n\tu32 count_data = 0;\n\tu32 count_meta = 0;\n\tu32 count_sys = 0;\n\tint chunk_reserved = 0;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = -ENOMEM;\n\t\tgoto error;\n\t}\n\n\t \n\tspin_lock(&fs_info->balance_lock);\n\tmemset(&bctl->stat, 0, sizeof(bctl->stat));\n\tspin_unlock(&fs_info->balance_lock);\nagain:\n\tif (!counting) {\n\t\t \n\t\tbctl->data.limit = limit_data;\n\t\tbctl->meta.limit = limit_meta;\n\t\tbctl->sys.limit = limit_sys;\n\t}\n\tkey.objectid = BTRFS_FIRST_CHUNK_TREE_OBJECTID;\n\tkey.offset = (u64)-1;\n\tkey.type = BTRFS_CHUNK_ITEM_KEY;\n\n\twhile (1) {\n\t\tif ((!counting && atomic_read(&fs_info->balance_pause_req)) ||\n\t\t    atomic_read(&fs_info->balance_cancel_req)) {\n\t\t\tret = -ECANCELED;\n\t\t\tgoto error;\n\t\t}\n\n\t\tmutex_lock(&fs_info->reclaim_bgs_lock);\n\t\tret = btrfs_search_slot(NULL, chunk_root, &key, path, 0, 0);\n\t\tif (ret < 0) {\n\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\t\tgoto error;\n\t\t}\n\n\t\t \n\t\tif (ret == 0)\n\t\t\tBUG();  \n\n\t\tret = btrfs_previous_item(chunk_root, path, 0,\n\t\t\t\t\t  BTRFS_CHUNK_ITEM_KEY);\n\t\tif (ret) {\n\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tleaf = path->nodes[0];\n\t\tslot = path->slots[0];\n\t\tbtrfs_item_key_to_cpu(leaf, &found_key, slot);\n\n\t\tif (found_key.objectid != key.objectid) {\n\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\t\tbreak;\n\t\t}\n\n\t\tchunk = btrfs_item_ptr(leaf, slot, struct btrfs_chunk);\n\t\tchunk_type = btrfs_chunk_type(leaf, chunk);\n\n\t\tif (!counting) {\n\t\t\tspin_lock(&fs_info->balance_lock);\n\t\t\tbctl->stat.considered++;\n\t\t\tspin_unlock(&fs_info->balance_lock);\n\t\t}\n\n\t\tret = should_balance_chunk(leaf, chunk, found_key.offset);\n\n\t\tbtrfs_release_path(path);\n\t\tif (!ret) {\n\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\t\tgoto loop;\n\t\t}\n\n\t\tif (counting) {\n\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\t\tspin_lock(&fs_info->balance_lock);\n\t\t\tbctl->stat.expected++;\n\t\t\tspin_unlock(&fs_info->balance_lock);\n\n\t\t\tif (chunk_type & BTRFS_BLOCK_GROUP_DATA)\n\t\t\t\tcount_data++;\n\t\t\telse if (chunk_type & BTRFS_BLOCK_GROUP_SYSTEM)\n\t\t\t\tcount_sys++;\n\t\t\telse if (chunk_type & BTRFS_BLOCK_GROUP_METADATA)\n\t\t\t\tcount_meta++;\n\n\t\t\tgoto loop;\n\t\t}\n\n\t\t \n\t\tif (((chunk_type & BTRFS_BLOCK_GROUP_DATA) &&\n\t\t\t\t\tcount_data < bctl->data.limit_min)\n\t\t\t\t|| ((chunk_type & BTRFS_BLOCK_GROUP_METADATA) &&\n\t\t\t\t\tcount_meta < bctl->meta.limit_min)\n\t\t\t\t|| ((chunk_type & BTRFS_BLOCK_GROUP_SYSTEM) &&\n\t\t\t\t\tcount_sys < bctl->sys.limit_min)) {\n\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\t\tgoto loop;\n\t\t}\n\n\t\tif (!chunk_reserved) {\n\t\t\t \n\t\t\tret = btrfs_may_alloc_data_chunk(fs_info,\n\t\t\t\t\t\t\t found_key.offset);\n\t\t\tif (ret < 0) {\n\t\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\t\t\tgoto error;\n\t\t\t} else if (ret == 1) {\n\t\t\t\tchunk_reserved = 1;\n\t\t\t}\n\t\t}\n\n\t\tret = btrfs_relocate_chunk(fs_info, found_key.offset);\n\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\tif (ret == -ENOSPC) {\n\t\t\tenospc_errors++;\n\t\t} else if (ret == -ETXTBSY) {\n\t\t\tbtrfs_info(fs_info,\n\t   \"skipping relocation of block group %llu due to active swapfile\",\n\t\t\t\t   found_key.offset);\n\t\t\tret = 0;\n\t\t} else if (ret) {\n\t\t\tgoto error;\n\t\t} else {\n\t\t\tspin_lock(&fs_info->balance_lock);\n\t\t\tbctl->stat.completed++;\n\t\t\tspin_unlock(&fs_info->balance_lock);\n\t\t}\nloop:\n\t\tif (found_key.offset == 0)\n\t\t\tbreak;\n\t\tkey.offset = found_key.offset - 1;\n\t}\n\n\tif (counting) {\n\t\tbtrfs_release_path(path);\n\t\tcounting = false;\n\t\tgoto again;\n\t}\nerror:\n\tbtrfs_free_path(path);\n\tif (enospc_errors) {\n\t\tbtrfs_info(fs_info, \"%d enospc errors during balance\",\n\t\t\t   enospc_errors);\n\t\tif (!ret)\n\t\t\tret = -ENOSPC;\n\t}\n\n\treturn ret;\n}\n\n \nstatic int alloc_profile_is_valid(u64 flags, int extended)\n{\n\tu64 mask = (extended ? BTRFS_EXTENDED_PROFILE_MASK :\n\t\t\t       BTRFS_BLOCK_GROUP_PROFILE_MASK);\n\n\tflags &= ~BTRFS_BLOCK_GROUP_TYPE_MASK;\n\n\t \n\tif (flags & ~mask)\n\t\treturn 0;\n\n\t \n\tif (flags == 0)\n\t\treturn !extended;  \n\n\treturn has_single_bit_set(flags);\n}\n\n \nstatic inline int validate_convert_profile(struct btrfs_fs_info *fs_info,\n\t\tconst struct btrfs_balance_args *bargs,\n\t\tu64 allowed, const char *type)\n{\n\tif (!(bargs->flags & BTRFS_BALANCE_ARGS_CONVERT))\n\t\treturn true;\n\n\t \n\tif (alloc_profile_is_valid(bargs->target, 1) &&\n\t    (bargs->target & ~allowed) == 0)\n\t\treturn true;\n\n\tbtrfs_err(fs_info, \"balance: invalid convert %s profile %s\",\n\t\t\ttype, btrfs_bg_type_to_raid_name(bargs->target));\n\treturn false;\n}\n\n \nstatic void describe_balance_args(struct btrfs_balance_args *bargs, char *buf,\n\t\t\t\t u32 size_buf)\n{\n\tint ret;\n\tu32 size_bp = size_buf;\n\tchar *bp = buf;\n\tu64 flags = bargs->flags;\n\tchar tmp_buf[128] = {'\\0'};\n\n\tif (!flags)\n\t\treturn;\n\n#define CHECK_APPEND_NOARG(a)\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tret = snprintf(bp, size_bp, (a));\t\t\t\\\n\t\tif (ret < 0 || ret >= size_bp)\t\t\t\t\\\n\t\t\tgoto out_overflow;\t\t\t\t\\\n\t\tsize_bp -= ret;\t\t\t\t\t\t\\\n\t\tbp += ret;\t\t\t\t\t\t\\\n\t} while (0)\n\n#define CHECK_APPEND_1ARG(a, v1)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tret = snprintf(bp, size_bp, (a), (v1));\t\t\t\\\n\t\tif (ret < 0 || ret >= size_bp)\t\t\t\t\\\n\t\t\tgoto out_overflow;\t\t\t\t\\\n\t\tsize_bp -= ret;\t\t\t\t\t\t\\\n\t\tbp += ret;\t\t\t\t\t\t\\\n\t} while (0)\n\n#define CHECK_APPEND_2ARG(a, v1, v2)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tret = snprintf(bp, size_bp, (a), (v1), (v2));\t\t\\\n\t\tif (ret < 0 || ret >= size_bp)\t\t\t\t\\\n\t\t\tgoto out_overflow;\t\t\t\t\\\n\t\tsize_bp -= ret;\t\t\t\t\t\t\\\n\t\tbp += ret;\t\t\t\t\t\t\\\n\t} while (0)\n\n\tif (flags & BTRFS_BALANCE_ARGS_CONVERT)\n\t\tCHECK_APPEND_1ARG(\"convert=%s,\",\n\t\t\t\t  btrfs_bg_type_to_raid_name(bargs->target));\n\n\tif (flags & BTRFS_BALANCE_ARGS_SOFT)\n\t\tCHECK_APPEND_NOARG(\"soft,\");\n\n\tif (flags & BTRFS_BALANCE_ARGS_PROFILES) {\n\t\tbtrfs_describe_block_groups(bargs->profiles, tmp_buf,\n\t\t\t\t\t    sizeof(tmp_buf));\n\t\tCHECK_APPEND_1ARG(\"profiles=%s,\", tmp_buf);\n\t}\n\n\tif (flags & BTRFS_BALANCE_ARGS_USAGE)\n\t\tCHECK_APPEND_1ARG(\"usage=%llu,\", bargs->usage);\n\n\tif (flags & BTRFS_BALANCE_ARGS_USAGE_RANGE)\n\t\tCHECK_APPEND_2ARG(\"usage=%u..%u,\",\n\t\t\t\t  bargs->usage_min, bargs->usage_max);\n\n\tif (flags & BTRFS_BALANCE_ARGS_DEVID)\n\t\tCHECK_APPEND_1ARG(\"devid=%llu,\", bargs->devid);\n\n\tif (flags & BTRFS_BALANCE_ARGS_DRANGE)\n\t\tCHECK_APPEND_2ARG(\"drange=%llu..%llu,\",\n\t\t\t\t  bargs->pstart, bargs->pend);\n\n\tif (flags & BTRFS_BALANCE_ARGS_VRANGE)\n\t\tCHECK_APPEND_2ARG(\"vrange=%llu..%llu,\",\n\t\t\t\t  bargs->vstart, bargs->vend);\n\n\tif (flags & BTRFS_BALANCE_ARGS_LIMIT)\n\t\tCHECK_APPEND_1ARG(\"limit=%llu,\", bargs->limit);\n\n\tif (flags & BTRFS_BALANCE_ARGS_LIMIT_RANGE)\n\t\tCHECK_APPEND_2ARG(\"limit=%u..%u,\",\n\t\t\t\tbargs->limit_min, bargs->limit_max);\n\n\tif (flags & BTRFS_BALANCE_ARGS_STRIPES_RANGE)\n\t\tCHECK_APPEND_2ARG(\"stripes=%u..%u,\",\n\t\t\t\t  bargs->stripes_min, bargs->stripes_max);\n\n#undef CHECK_APPEND_2ARG\n#undef CHECK_APPEND_1ARG\n#undef CHECK_APPEND_NOARG\n\nout_overflow:\n\n\tif (size_bp < size_buf)\n\t\tbuf[size_buf - size_bp - 1] = '\\0';  \n\telse\n\t\tbuf[0] = '\\0';\n}\n\nstatic void describe_balance_start_or_resume(struct btrfs_fs_info *fs_info)\n{\n\tu32 size_buf = 1024;\n\tchar tmp_buf[192] = {'\\0'};\n\tchar *buf;\n\tchar *bp;\n\tu32 size_bp = size_buf;\n\tint ret;\n\tstruct btrfs_balance_control *bctl = fs_info->balance_ctl;\n\n\tbuf = kzalloc(size_buf, GFP_KERNEL);\n\tif (!buf)\n\t\treturn;\n\n\tbp = buf;\n\n#define CHECK_APPEND_1ARG(a, v1)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tret = snprintf(bp, size_bp, (a), (v1));\t\t\t\\\n\t\tif (ret < 0 || ret >= size_bp)\t\t\t\t\\\n\t\t\tgoto out_overflow;\t\t\t\t\\\n\t\tsize_bp -= ret;\t\t\t\t\t\t\\\n\t\tbp += ret;\t\t\t\t\t\t\\\n\t} while (0)\n\n\tif (bctl->flags & BTRFS_BALANCE_FORCE)\n\t\tCHECK_APPEND_1ARG(\"%s\", \"-f \");\n\n\tif (bctl->flags & BTRFS_BALANCE_DATA) {\n\t\tdescribe_balance_args(&bctl->data, tmp_buf, sizeof(tmp_buf));\n\t\tCHECK_APPEND_1ARG(\"-d%s \", tmp_buf);\n\t}\n\n\tif (bctl->flags & BTRFS_BALANCE_METADATA) {\n\t\tdescribe_balance_args(&bctl->meta, tmp_buf, sizeof(tmp_buf));\n\t\tCHECK_APPEND_1ARG(\"-m%s \", tmp_buf);\n\t}\n\n\tif (bctl->flags & BTRFS_BALANCE_SYSTEM) {\n\t\tdescribe_balance_args(&bctl->sys, tmp_buf, sizeof(tmp_buf));\n\t\tCHECK_APPEND_1ARG(\"-s%s \", tmp_buf);\n\t}\n\n#undef CHECK_APPEND_1ARG\n\nout_overflow:\n\n\tif (size_bp < size_buf)\n\t\tbuf[size_buf - size_bp - 1] = '\\0';  \n\tbtrfs_info(fs_info, \"balance: %s %s\",\n\t\t   (bctl->flags & BTRFS_BALANCE_RESUME) ?\n\t\t   \"resume\" : \"start\", buf);\n\n\tkfree(buf);\n}\n\n \nint btrfs_balance(struct btrfs_fs_info *fs_info,\n\t\t  struct btrfs_balance_control *bctl,\n\t\t  struct btrfs_ioctl_balance_args *bargs)\n{\n\tu64 meta_target, data_target;\n\tu64 allowed;\n\tint mixed = 0;\n\tint ret;\n\tu64 num_devices;\n\tunsigned seq;\n\tbool reducing_redundancy;\n\tbool paused = false;\n\tint i;\n\n\tif (btrfs_fs_closing(fs_info) ||\n\t    atomic_read(&fs_info->balance_pause_req) ||\n\t    btrfs_should_cancel_balance(fs_info)) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tallowed = btrfs_super_incompat_flags(fs_info->super_copy);\n\tif (allowed & BTRFS_FEATURE_INCOMPAT_MIXED_GROUPS)\n\t\tmixed = 1;\n\n\t \n\tallowed = BTRFS_BALANCE_DATA | BTRFS_BALANCE_METADATA;\n\tif (mixed && (bctl->flags & allowed)) {\n\t\tif (!(bctl->flags & BTRFS_BALANCE_DATA) ||\n\t\t    !(bctl->flags & BTRFS_BALANCE_METADATA) ||\n\t\t    memcmp(&bctl->data, &bctl->meta, sizeof(bctl->data))) {\n\t\t\tbtrfs_err(fs_info,\n\t  \"balance: mixed groups data and metadata options must be the same\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t \n\tnum_devices = fs_info->fs_devices->rw_devices;\n\n\t \n\tallowed = BTRFS_AVAIL_ALLOC_BIT_SINGLE;\n\tfor (i = 0; i < ARRAY_SIZE(btrfs_raid_array); i++)\n\t\tif (num_devices >= btrfs_raid_array[i].devs_min)\n\t\t\tallowed |= btrfs_raid_array[i].bg_flag;\n\n\tif (!validate_convert_profile(fs_info, &bctl->data, allowed, \"data\") ||\n\t    !validate_convert_profile(fs_info, &bctl->meta, allowed, \"metadata\") ||\n\t    !validate_convert_profile(fs_info, &bctl->sys,  allowed, \"system\")) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t \n\tallowed = 0;\n\tfor (i = 0; i < ARRAY_SIZE(btrfs_raid_array); i++) {\n\t\tif (btrfs_raid_array[i].ncopies >= 2 ||\n\t\t    btrfs_raid_array[i].tolerated_failures >= 1)\n\t\t\tallowed |= btrfs_raid_array[i].bg_flag;\n\t}\n\tdo {\n\t\tseq = read_seqbegin(&fs_info->profiles_lock);\n\n\t\tif (((bctl->sys.flags & BTRFS_BALANCE_ARGS_CONVERT) &&\n\t\t     (fs_info->avail_system_alloc_bits & allowed) &&\n\t\t     !(bctl->sys.target & allowed)) ||\n\t\t    ((bctl->meta.flags & BTRFS_BALANCE_ARGS_CONVERT) &&\n\t\t     (fs_info->avail_metadata_alloc_bits & allowed) &&\n\t\t     !(bctl->meta.target & allowed)))\n\t\t\treducing_redundancy = true;\n\t\telse\n\t\t\treducing_redundancy = false;\n\n\t\t \n\t\tmeta_target = (bctl->meta.flags & BTRFS_BALANCE_ARGS_CONVERT) ?\n\t\t\tbctl->meta.target : fs_info->avail_metadata_alloc_bits;\n\t\tdata_target = (bctl->data.flags & BTRFS_BALANCE_ARGS_CONVERT) ?\n\t\t\tbctl->data.target : fs_info->avail_data_alloc_bits;\n\t} while (read_seqretry(&fs_info->profiles_lock, seq));\n\n\tif (reducing_redundancy) {\n\t\tif (bctl->flags & BTRFS_BALANCE_FORCE) {\n\t\t\tbtrfs_info(fs_info,\n\t\t\t   \"balance: force reducing metadata redundancy\");\n\t\t} else {\n\t\t\tbtrfs_err(fs_info,\n\t\"balance: reduces metadata redundancy, use --force if you want this\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (btrfs_get_num_tolerated_disk_barrier_failures(meta_target) <\n\t\tbtrfs_get_num_tolerated_disk_barrier_failures(data_target)) {\n\t\tbtrfs_warn(fs_info,\n\t\"balance: metadata profile %s has lower redundancy than data profile %s\",\n\t\t\t\tbtrfs_bg_type_to_raid_name(meta_target),\n\t\t\t\tbtrfs_bg_type_to_raid_name(data_target));\n\t}\n\n\tret = insert_balance_item(fs_info, bctl);\n\tif (ret && ret != -EEXIST)\n\t\tgoto out;\n\n\tif (!(bctl->flags & BTRFS_BALANCE_RESUME)) {\n\t\tBUG_ON(ret == -EEXIST);\n\t\tBUG_ON(fs_info->balance_ctl);\n\t\tspin_lock(&fs_info->balance_lock);\n\t\tfs_info->balance_ctl = bctl;\n\t\tspin_unlock(&fs_info->balance_lock);\n\t} else {\n\t\tBUG_ON(ret != -EEXIST);\n\t\tspin_lock(&fs_info->balance_lock);\n\t\tupdate_balance_args(bctl);\n\t\tspin_unlock(&fs_info->balance_lock);\n\t}\n\n\tASSERT(!test_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags));\n\tset_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags);\n\tdescribe_balance_start_or_resume(fs_info);\n\tmutex_unlock(&fs_info->balance_mutex);\n\n\tret = __btrfs_balance(fs_info);\n\n\tmutex_lock(&fs_info->balance_mutex);\n\tif (ret == -ECANCELED && atomic_read(&fs_info->balance_pause_req)) {\n\t\tbtrfs_info(fs_info, \"balance: paused\");\n\t\tbtrfs_exclop_balance(fs_info, BTRFS_EXCLOP_BALANCE_PAUSED);\n\t\tpaused = true;\n\t}\n\t \n\telse if (ret == -ECANCELED || ret == -EINTR)\n\t\tbtrfs_info(fs_info, \"balance: canceled\");\n\telse\n\t\tbtrfs_info(fs_info, \"balance: ended with status: %d\", ret);\n\n\tclear_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags);\n\n\tif (bargs) {\n\t\tmemset(bargs, 0, sizeof(*bargs));\n\t\tbtrfs_update_ioctl_balance_args(fs_info, bargs);\n\t}\n\n\t \n\tif (!paused) {\n\t\treset_balance_state(fs_info);\n\t\tbtrfs_exclop_finish(fs_info);\n\t}\n\n\twake_up(&fs_info->balance_wait_q);\n\n\treturn ret;\nout:\n\tif (bctl->flags & BTRFS_BALANCE_RESUME)\n\t\treset_balance_state(fs_info);\n\telse\n\t\tkfree(bctl);\n\tbtrfs_exclop_finish(fs_info);\n\n\treturn ret;\n}\n\nstatic int balance_kthread(void *data)\n{\n\tstruct btrfs_fs_info *fs_info = data;\n\tint ret = 0;\n\n\tsb_start_write(fs_info->sb);\n\tmutex_lock(&fs_info->balance_mutex);\n\tif (fs_info->balance_ctl)\n\t\tret = btrfs_balance(fs_info, fs_info->balance_ctl, NULL);\n\tmutex_unlock(&fs_info->balance_mutex);\n\tsb_end_write(fs_info->sb);\n\n\treturn ret;\n}\n\nint btrfs_resume_balance_async(struct btrfs_fs_info *fs_info)\n{\n\tstruct task_struct *tsk;\n\n\tmutex_lock(&fs_info->balance_mutex);\n\tif (!fs_info->balance_ctl) {\n\t\tmutex_unlock(&fs_info->balance_mutex);\n\t\treturn 0;\n\t}\n\tmutex_unlock(&fs_info->balance_mutex);\n\n\tif (btrfs_test_opt(fs_info, SKIP_BALANCE)) {\n\t\tbtrfs_info(fs_info, \"balance: resume skipped\");\n\t\treturn 0;\n\t}\n\n\tspin_lock(&fs_info->super_lock);\n\tASSERT(fs_info->exclusive_operation == BTRFS_EXCLOP_BALANCE_PAUSED);\n\tfs_info->exclusive_operation = BTRFS_EXCLOP_BALANCE;\n\tspin_unlock(&fs_info->super_lock);\n\t \n\tspin_lock(&fs_info->balance_lock);\n\tfs_info->balance_ctl->flags |= BTRFS_BALANCE_RESUME;\n\tspin_unlock(&fs_info->balance_lock);\n\n\ttsk = kthread_run(balance_kthread, fs_info, \"btrfs-balance\");\n\treturn PTR_ERR_OR_ZERO(tsk);\n}\n\nint btrfs_recover_balance(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_balance_control *bctl;\n\tstruct btrfs_balance_item *item;\n\tstruct btrfs_disk_balance_args disk_bargs;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_key key;\n\tint ret;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = BTRFS_BALANCE_OBJECTID;\n\tkey.type = BTRFS_TEMPORARY_ITEM_KEY;\n\tkey.offset = 0;\n\n\tret = btrfs_search_slot(NULL, fs_info->tree_root, &key, path, 0, 0);\n\tif (ret < 0)\n\t\tgoto out;\n\tif (ret > 0) {  \n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\tbctl = kzalloc(sizeof(*bctl), GFP_NOFS);\n\tif (!bctl) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tleaf = path->nodes[0];\n\titem = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_balance_item);\n\n\tbctl->flags = btrfs_balance_flags(leaf, item);\n\tbctl->flags |= BTRFS_BALANCE_RESUME;\n\n\tbtrfs_balance_data(leaf, item, &disk_bargs);\n\tbtrfs_disk_balance_args_to_cpu(&bctl->data, &disk_bargs);\n\tbtrfs_balance_meta(leaf, item, &disk_bargs);\n\tbtrfs_disk_balance_args_to_cpu(&bctl->meta, &disk_bargs);\n\tbtrfs_balance_sys(leaf, item, &disk_bargs);\n\tbtrfs_disk_balance_args_to_cpu(&bctl->sys, &disk_bargs);\n\n\t \n\tif (!btrfs_exclop_start(fs_info, BTRFS_EXCLOP_BALANCE_PAUSED))\n\t\tbtrfs_warn(fs_info,\n\t\"balance: cannot set exclusive op status, resume manually\");\n\n\tbtrfs_release_path(path);\n\n\tmutex_lock(&fs_info->balance_mutex);\n\tBUG_ON(fs_info->balance_ctl);\n\tspin_lock(&fs_info->balance_lock);\n\tfs_info->balance_ctl = bctl;\n\tspin_unlock(&fs_info->balance_lock);\n\tmutex_unlock(&fs_info->balance_mutex);\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nint btrfs_pause_balance(struct btrfs_fs_info *fs_info)\n{\n\tint ret = 0;\n\n\tmutex_lock(&fs_info->balance_mutex);\n\tif (!fs_info->balance_ctl) {\n\t\tmutex_unlock(&fs_info->balance_mutex);\n\t\treturn -ENOTCONN;\n\t}\n\n\tif (test_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags)) {\n\t\tatomic_inc(&fs_info->balance_pause_req);\n\t\tmutex_unlock(&fs_info->balance_mutex);\n\n\t\twait_event(fs_info->balance_wait_q,\n\t\t\t   !test_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags));\n\n\t\tmutex_lock(&fs_info->balance_mutex);\n\t\t \n\t\tBUG_ON(test_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags));\n\t\tatomic_dec(&fs_info->balance_pause_req);\n\t} else {\n\t\tret = -ENOTCONN;\n\t}\n\n\tmutex_unlock(&fs_info->balance_mutex);\n\treturn ret;\n}\n\nint btrfs_cancel_balance(struct btrfs_fs_info *fs_info)\n{\n\tmutex_lock(&fs_info->balance_mutex);\n\tif (!fs_info->balance_ctl) {\n\t\tmutex_unlock(&fs_info->balance_mutex);\n\t\treturn -ENOTCONN;\n\t}\n\n\t \n\tif (sb_rdonly(fs_info->sb)) {\n\t\tmutex_unlock(&fs_info->balance_mutex);\n\t\treturn -EROFS;\n\t}\n\n\tatomic_inc(&fs_info->balance_cancel_req);\n\t \n\tif (test_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags)) {\n\t\tmutex_unlock(&fs_info->balance_mutex);\n\t\twait_event(fs_info->balance_wait_q,\n\t\t\t   !test_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags));\n\t\tmutex_lock(&fs_info->balance_mutex);\n\t} else {\n\t\tmutex_unlock(&fs_info->balance_mutex);\n\t\t \n\t\tmutex_lock(&fs_info->balance_mutex);\n\n\t\tif (fs_info->balance_ctl) {\n\t\t\treset_balance_state(fs_info);\n\t\t\tbtrfs_exclop_finish(fs_info);\n\t\t\tbtrfs_info(fs_info, \"balance: canceled\");\n\t\t}\n\t}\n\n\tASSERT(!test_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags));\n\tatomic_dec(&fs_info->balance_cancel_req);\n\tmutex_unlock(&fs_info->balance_mutex);\n\treturn 0;\n}\n\nint btrfs_uuid_scan_kthread(void *data)\n{\n\tstruct btrfs_fs_info *fs_info = data;\n\tstruct btrfs_root *root = fs_info->tree_root;\n\tstruct btrfs_key key;\n\tstruct btrfs_path *path = NULL;\n\tint ret = 0;\n\tstruct extent_buffer *eb;\n\tint slot;\n\tstruct btrfs_root_item root_item;\n\tu32 item_size;\n\tstruct btrfs_trans_handle *trans = NULL;\n\tbool closing = false;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tkey.objectid = 0;\n\tkey.type = BTRFS_ROOT_ITEM_KEY;\n\tkey.offset = 0;\n\n\twhile (1) {\n\t\tif (btrfs_fs_closing(fs_info)) {\n\t\t\tclosing = true;\n\t\t\tbreak;\n\t\t}\n\t\tret = btrfs_search_forward(root, &key, path,\n\t\t\t\tBTRFS_OLDEST_GENERATION);\n\t\tif (ret) {\n\t\t\tif (ret > 0)\n\t\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (key.type != BTRFS_ROOT_ITEM_KEY ||\n\t\t    (key.objectid < BTRFS_FIRST_FREE_OBJECTID &&\n\t\t     key.objectid != BTRFS_FS_TREE_OBJECTID) ||\n\t\t    key.objectid > BTRFS_LAST_FREE_OBJECTID)\n\t\t\tgoto skip;\n\n\t\teb = path->nodes[0];\n\t\tslot = path->slots[0];\n\t\titem_size = btrfs_item_size(eb, slot);\n\t\tif (item_size < sizeof(root_item))\n\t\t\tgoto skip;\n\n\t\tread_extent_buffer(eb, &root_item,\n\t\t\t\t   btrfs_item_ptr_offset(eb, slot),\n\t\t\t\t   (int)sizeof(root_item));\n\t\tif (btrfs_root_refs(&root_item) == 0)\n\t\t\tgoto skip;\n\n\t\tif (!btrfs_is_empty_uuid(root_item.uuid) ||\n\t\t    !btrfs_is_empty_uuid(root_item.received_uuid)) {\n\t\t\tif (trans)\n\t\t\t\tgoto update_tree;\n\n\t\t\tbtrfs_release_path(path);\n\t\t\t \n\t\t\ttrans = btrfs_start_transaction(fs_info->uuid_root, 2);\n\t\t\tif (IS_ERR(trans)) {\n\t\t\t\tret = PTR_ERR(trans);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcontinue;\n\t\t} else {\n\t\t\tgoto skip;\n\t\t}\nupdate_tree:\n\t\tbtrfs_release_path(path);\n\t\tif (!btrfs_is_empty_uuid(root_item.uuid)) {\n\t\t\tret = btrfs_uuid_tree_add(trans, root_item.uuid,\n\t\t\t\t\t\t  BTRFS_UUID_KEY_SUBVOL,\n\t\t\t\t\t\t  key.objectid);\n\t\t\tif (ret < 0) {\n\t\t\t\tbtrfs_warn(fs_info, \"uuid_tree_add failed %d\",\n\t\t\t\t\tret);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (!btrfs_is_empty_uuid(root_item.received_uuid)) {\n\t\t\tret = btrfs_uuid_tree_add(trans,\n\t\t\t\t\t\t  root_item.received_uuid,\n\t\t\t\t\t\t BTRFS_UUID_KEY_RECEIVED_SUBVOL,\n\t\t\t\t\t\t  key.objectid);\n\t\t\tif (ret < 0) {\n\t\t\t\tbtrfs_warn(fs_info, \"uuid_tree_add failed %d\",\n\t\t\t\t\tret);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\nskip:\n\t\tbtrfs_release_path(path);\n\t\tif (trans) {\n\t\t\tret = btrfs_end_transaction(trans);\n\t\t\ttrans = NULL;\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (key.offset < (u64)-1) {\n\t\t\tkey.offset++;\n\t\t} else if (key.type < BTRFS_ROOT_ITEM_KEY) {\n\t\t\tkey.offset = 0;\n\t\t\tkey.type = BTRFS_ROOT_ITEM_KEY;\n\t\t} else if (key.objectid < (u64)-1) {\n\t\t\tkey.offset = 0;\n\t\t\tkey.type = BTRFS_ROOT_ITEM_KEY;\n\t\t\tkey.objectid++;\n\t\t} else {\n\t\t\tbreak;\n\t\t}\n\t\tcond_resched();\n\t}\n\nout:\n\tbtrfs_free_path(path);\n\tif (trans && !IS_ERR(trans))\n\t\tbtrfs_end_transaction(trans);\n\tif (ret)\n\t\tbtrfs_warn(fs_info, \"btrfs_uuid_scan_kthread failed %d\", ret);\n\telse if (!closing)\n\t\tset_bit(BTRFS_FS_UPDATE_UUID_TREE_GEN, &fs_info->flags);\n\tup(&fs_info->uuid_tree_rescan_sem);\n\treturn 0;\n}\n\nint btrfs_create_uuid_tree(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_root *tree_root = fs_info->tree_root;\n\tstruct btrfs_root *uuid_root;\n\tstruct task_struct *task;\n\tint ret;\n\n\t \n\ttrans = btrfs_start_transaction(tree_root, 2);\n\tif (IS_ERR(trans))\n\t\treturn PTR_ERR(trans);\n\n\tuuid_root = btrfs_create_tree(trans, BTRFS_UUID_TREE_OBJECTID);\n\tif (IS_ERR(uuid_root)) {\n\t\tret = PTR_ERR(uuid_root);\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tbtrfs_end_transaction(trans);\n\t\treturn ret;\n\t}\n\n\tfs_info->uuid_root = uuid_root;\n\n\tret = btrfs_commit_transaction(trans);\n\tif (ret)\n\t\treturn ret;\n\n\tdown(&fs_info->uuid_tree_rescan_sem);\n\ttask = kthread_run(btrfs_uuid_scan_kthread, fs_info, \"btrfs-uuid\");\n\tif (IS_ERR(task)) {\n\t\t \n\t\tbtrfs_warn(fs_info, \"failed to start uuid_scan task\");\n\t\tup(&fs_info->uuid_tree_rescan_sem);\n\t\treturn PTR_ERR(task);\n\t}\n\n\treturn 0;\n}\n\n \nint btrfs_shrink_device(struct btrfs_device *device, u64 new_size)\n{\n\tstruct btrfs_fs_info *fs_info = device->fs_info;\n\tstruct btrfs_root *root = fs_info->dev_root;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_dev_extent *dev_extent = NULL;\n\tstruct btrfs_path *path;\n\tu64 length;\n\tu64 chunk_offset;\n\tint ret;\n\tint slot;\n\tint failed = 0;\n\tbool retried = false;\n\tstruct extent_buffer *l;\n\tstruct btrfs_key key;\n\tstruct btrfs_super_block *super_copy = fs_info->super_copy;\n\tu64 old_total = btrfs_super_total_bytes(super_copy);\n\tu64 old_size = btrfs_device_get_total_bytes(device);\n\tu64 diff;\n\tu64 start;\n\n\tnew_size = round_down(new_size, fs_info->sectorsize);\n\tstart = new_size;\n\tdiff = round_down(old_size - new_size, fs_info->sectorsize);\n\n\tif (test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state))\n\t\treturn -EINVAL;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tpath->reada = READA_BACK;\n\n\ttrans = btrfs_start_transaction(root, 0);\n\tif (IS_ERR(trans)) {\n\t\tbtrfs_free_path(path);\n\t\treturn PTR_ERR(trans);\n\t}\n\n\tmutex_lock(&fs_info->chunk_mutex);\n\n\tbtrfs_device_set_total_bytes(device, new_size);\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tdevice->fs_devices->total_rw_bytes -= diff;\n\t\tatomic64_sub(diff, &fs_info->free_chunk_space);\n\t}\n\n\t \n\tif (contains_pending_extent(device, &start, diff)) {\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t\tret = btrfs_commit_transaction(trans);\n\t\tif (ret)\n\t\t\tgoto done;\n\t} else {\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t\tbtrfs_end_transaction(trans);\n\t}\n\nagain:\n\tkey.objectid = device->devid;\n\tkey.offset = (u64)-1;\n\tkey.type = BTRFS_DEV_EXTENT_KEY;\n\n\tdo {\n\t\tmutex_lock(&fs_info->reclaim_bgs_lock);\n\t\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\t\tif (ret < 0) {\n\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\t\tgoto done;\n\t\t}\n\n\t\tret = btrfs_previous_item(root, path, 0, key.type);\n\t\tif (ret) {\n\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto done;\n\t\t\tret = 0;\n\t\t\tbtrfs_release_path(path);\n\t\t\tbreak;\n\t\t}\n\n\t\tl = path->nodes[0];\n\t\tslot = path->slots[0];\n\t\tbtrfs_item_key_to_cpu(l, &key, path->slots[0]);\n\n\t\tif (key.objectid != device->devid) {\n\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\t\tbtrfs_release_path(path);\n\t\t\tbreak;\n\t\t}\n\n\t\tdev_extent = btrfs_item_ptr(l, slot, struct btrfs_dev_extent);\n\t\tlength = btrfs_dev_extent_length(l, dev_extent);\n\n\t\tif (key.offset + length <= new_size) {\n\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\t\tbtrfs_release_path(path);\n\t\t\tbreak;\n\t\t}\n\n\t\tchunk_offset = btrfs_dev_extent_chunk_offset(l, dev_extent);\n\t\tbtrfs_release_path(path);\n\n\t\t \n\t\tret = btrfs_may_alloc_data_chunk(fs_info, chunk_offset);\n\t\tif (ret < 0) {\n\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\t\tgoto done;\n\t\t}\n\n\t\tret = btrfs_relocate_chunk(fs_info, chunk_offset);\n\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\tif (ret == -ENOSPC) {\n\t\t\tfailed++;\n\t\t} else if (ret) {\n\t\t\tif (ret == -ETXTBSY) {\n\t\t\t\tbtrfs_warn(fs_info,\n\t\t   \"could not shrink block group %llu due to active swapfile\",\n\t\t\t\t\t   chunk_offset);\n\t\t\t}\n\t\t\tgoto done;\n\t\t}\n\t} while (key.offset-- > 0);\n\n\tif (failed && !retried) {\n\t\tfailed = 0;\n\t\tretried = true;\n\t\tgoto again;\n\t} else if (failed && retried) {\n\t\tret = -ENOSPC;\n\t\tgoto done;\n\t}\n\n\t \n\ttrans = btrfs_start_transaction(root, 0);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto done;\n\t}\n\n\tmutex_lock(&fs_info->chunk_mutex);\n\t \n\tclear_extent_bits(&device->alloc_state, new_size, (u64)-1,\n\t\t\t  CHUNK_STATE_MASK);\n\n\tbtrfs_device_set_disk_total_bytes(device, new_size);\n\tif (list_empty(&device->post_commit_list))\n\t\tlist_add_tail(&device->post_commit_list,\n\t\t\t      &trans->transaction->dev_update_list);\n\n\tWARN_ON(diff > old_total);\n\tbtrfs_set_super_total_bytes(super_copy,\n\t\t\tround_down(old_total - diff, fs_info->sectorsize));\n\tmutex_unlock(&fs_info->chunk_mutex);\n\n\tbtrfs_reserve_chunk_metadata(trans, false);\n\t \n\tret = btrfs_update_device(trans, device);\n\tbtrfs_trans_release_chunk_metadata(trans);\n\tif (ret < 0) {\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tbtrfs_end_transaction(trans);\n\t} else {\n\t\tret = btrfs_commit_transaction(trans);\n\t}\ndone:\n\tbtrfs_free_path(path);\n\tif (ret) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tbtrfs_device_set_total_bytes(device, old_size);\n\t\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state))\n\t\t\tdevice->fs_devices->total_rw_bytes += diff;\n\t\tatomic64_add(diff, &fs_info->free_chunk_space);\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\treturn ret;\n}\n\nstatic int btrfs_add_system_chunk(struct btrfs_fs_info *fs_info,\n\t\t\t   struct btrfs_key *key,\n\t\t\t   struct btrfs_chunk *chunk, int item_size)\n{\n\tstruct btrfs_super_block *super_copy = fs_info->super_copy;\n\tstruct btrfs_disk_key disk_key;\n\tu32 array_size;\n\tu8 *ptr;\n\n\tlockdep_assert_held(&fs_info->chunk_mutex);\n\n\tarray_size = btrfs_super_sys_array_size(super_copy);\n\tif (array_size + item_size + sizeof(disk_key)\n\t\t\t> BTRFS_SYSTEM_CHUNK_ARRAY_SIZE)\n\t\treturn -EFBIG;\n\n\tptr = super_copy->sys_chunk_array + array_size;\n\tbtrfs_cpu_key_to_disk(&disk_key, key);\n\tmemcpy(ptr, &disk_key, sizeof(disk_key));\n\tptr += sizeof(disk_key);\n\tmemcpy(ptr, chunk, item_size);\n\titem_size += sizeof(disk_key);\n\tbtrfs_set_super_sys_array_size(super_copy, array_size + item_size);\n\n\treturn 0;\n}\n\n \nstatic int btrfs_cmp_device_info(const void *a, const void *b)\n{\n\tconst struct btrfs_device_info *di_a = a;\n\tconst struct btrfs_device_info *di_b = b;\n\n\tif (di_a->max_avail > di_b->max_avail)\n\t\treturn -1;\n\tif (di_a->max_avail < di_b->max_avail)\n\t\treturn 1;\n\tif (di_a->total_avail > di_b->total_avail)\n\t\treturn -1;\n\tif (di_a->total_avail < di_b->total_avail)\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic void check_raid56_incompat_flag(struct btrfs_fs_info *info, u64 type)\n{\n\tif (!(type & BTRFS_BLOCK_GROUP_RAID56_MASK))\n\t\treturn;\n\n\tbtrfs_set_fs_incompat(info, RAID56);\n}\n\nstatic void check_raid1c34_incompat_flag(struct btrfs_fs_info *info, u64 type)\n{\n\tif (!(type & (BTRFS_BLOCK_GROUP_RAID1C3 | BTRFS_BLOCK_GROUP_RAID1C4)))\n\t\treturn;\n\n\tbtrfs_set_fs_incompat(info, RAID1C34);\n}\n\n \nstruct alloc_chunk_ctl {\n\tu64 start;\n\tu64 type;\n\t \n\tint num_stripes;\n\t \n\tint sub_stripes;\n\t \n\tint dev_stripes;\n\t \n\tint devs_max;\n\t \n\tint devs_min;\n\t \n\tint devs_increment;\n\t \n\tint ncopies;\n\t \n\tint nparity;\n\tu64 max_stripe_size;\n\tu64 max_chunk_size;\n\tu64 dev_extent_min;\n\tu64 stripe_size;\n\tu64 chunk_size;\n\tint ndevs;\n};\n\nstatic void init_alloc_chunk_ctl_policy_regular(\n\t\t\t\tstruct btrfs_fs_devices *fs_devices,\n\t\t\t\tstruct alloc_chunk_ctl *ctl)\n{\n\tstruct btrfs_space_info *space_info;\n\n\tspace_info = btrfs_find_space_info(fs_devices->fs_info, ctl->type);\n\tASSERT(space_info);\n\n\tctl->max_chunk_size = READ_ONCE(space_info->chunk_size);\n\tctl->max_stripe_size = min_t(u64, ctl->max_chunk_size, SZ_1G);\n\n\tif (ctl->type & BTRFS_BLOCK_GROUP_SYSTEM)\n\t\tctl->devs_max = min_t(int, ctl->devs_max, BTRFS_MAX_DEVS_SYS_CHUNK);\n\n\t \n\tctl->max_chunk_size = min(mult_perc(fs_devices->total_rw_bytes, 10),\n\t\t\t\t  ctl->max_chunk_size);\n\tctl->dev_extent_min = btrfs_stripe_nr_to_offset(ctl->dev_stripes);\n}\n\nstatic void init_alloc_chunk_ctl_policy_zoned(\n\t\t\t\t      struct btrfs_fs_devices *fs_devices,\n\t\t\t\t      struct alloc_chunk_ctl *ctl)\n{\n\tu64 zone_size = fs_devices->fs_info->zone_size;\n\tu64 limit;\n\tint min_num_stripes = ctl->devs_min * ctl->dev_stripes;\n\tint min_data_stripes = (min_num_stripes - ctl->nparity) / ctl->ncopies;\n\tu64 min_chunk_size = min_data_stripes * zone_size;\n\tu64 type = ctl->type;\n\n\tctl->max_stripe_size = zone_size;\n\tif (type & BTRFS_BLOCK_GROUP_DATA) {\n\t\tctl->max_chunk_size = round_down(BTRFS_MAX_DATA_CHUNK_SIZE,\n\t\t\t\t\t\t zone_size);\n\t} else if (type & BTRFS_BLOCK_GROUP_METADATA) {\n\t\tctl->max_chunk_size = ctl->max_stripe_size;\n\t} else if (type & BTRFS_BLOCK_GROUP_SYSTEM) {\n\t\tctl->max_chunk_size = 2 * ctl->max_stripe_size;\n\t\tctl->devs_max = min_t(int, ctl->devs_max,\n\t\t\t\t      BTRFS_MAX_DEVS_SYS_CHUNK);\n\t} else {\n\t\tBUG();\n\t}\n\n\t \n\tlimit = max(round_down(mult_perc(fs_devices->total_rw_bytes, 10),\n\t\t\t       zone_size),\n\t\t    min_chunk_size);\n\tctl->max_chunk_size = min(limit, ctl->max_chunk_size);\n\tctl->dev_extent_min = zone_size * ctl->dev_stripes;\n}\n\nstatic void init_alloc_chunk_ctl(struct btrfs_fs_devices *fs_devices,\n\t\t\t\t struct alloc_chunk_ctl *ctl)\n{\n\tint index = btrfs_bg_flags_to_raid_index(ctl->type);\n\n\tctl->sub_stripes = btrfs_raid_array[index].sub_stripes;\n\tctl->dev_stripes = btrfs_raid_array[index].dev_stripes;\n\tctl->devs_max = btrfs_raid_array[index].devs_max;\n\tif (!ctl->devs_max)\n\t\tctl->devs_max = BTRFS_MAX_DEVS(fs_devices->fs_info);\n\tctl->devs_min = btrfs_raid_array[index].devs_min;\n\tctl->devs_increment = btrfs_raid_array[index].devs_increment;\n\tctl->ncopies = btrfs_raid_array[index].ncopies;\n\tctl->nparity = btrfs_raid_array[index].nparity;\n\tctl->ndevs = 0;\n\n\tswitch (fs_devices->chunk_alloc_policy) {\n\tcase BTRFS_CHUNK_ALLOC_REGULAR:\n\t\tinit_alloc_chunk_ctl_policy_regular(fs_devices, ctl);\n\t\tbreak;\n\tcase BTRFS_CHUNK_ALLOC_ZONED:\n\t\tinit_alloc_chunk_ctl_policy_zoned(fs_devices, ctl);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n}\n\nstatic int gather_device_info(struct btrfs_fs_devices *fs_devices,\n\t\t\t      struct alloc_chunk_ctl *ctl,\n\t\t\t      struct btrfs_device_info *devices_info)\n{\n\tstruct btrfs_fs_info *info = fs_devices->fs_info;\n\tstruct btrfs_device *device;\n\tu64 total_avail;\n\tu64 dev_extent_want = ctl->max_stripe_size * ctl->dev_stripes;\n\tint ret;\n\tint ndevs = 0;\n\tu64 max_avail;\n\tu64 dev_offset;\n\n\t \n\tlist_for_each_entry(device, &fs_devices->alloc_list, dev_alloc_list) {\n\t\tif (!test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\t\tWARN(1, KERN_ERR\n\t\t\t       \"BTRFS: read-only device in alloc_list\\n\");\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!test_bit(BTRFS_DEV_STATE_IN_FS_METADATA,\n\t\t\t\t\t&device->dev_state) ||\n\t\t    test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state))\n\t\t\tcontinue;\n\n\t\tif (device->total_bytes > device->bytes_used)\n\t\t\ttotal_avail = device->total_bytes - device->bytes_used;\n\t\telse\n\t\t\ttotal_avail = 0;\n\n\t\t \n\t\tif (total_avail < ctl->dev_extent_min)\n\t\t\tcontinue;\n\n\t\tret = find_free_dev_extent(device, dev_extent_want, &dev_offset,\n\t\t\t\t\t   &max_avail);\n\t\tif (ret && ret != -ENOSPC)\n\t\t\treturn ret;\n\n\t\tif (ret == 0)\n\t\t\tmax_avail = dev_extent_want;\n\n\t\tif (max_avail < ctl->dev_extent_min) {\n\t\t\tif (btrfs_test_opt(info, ENOSPC_DEBUG))\n\t\t\t\tbtrfs_debug(info,\n\t\t\t\"%s: devid %llu has no free space, have=%llu want=%llu\",\n\t\t\t\t\t    __func__, device->devid, max_avail,\n\t\t\t\t\t    ctl->dev_extent_min);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (ndevs == fs_devices->rw_devices) {\n\t\t\tWARN(1, \"%s: found more than %llu devices\\n\",\n\t\t\t     __func__, fs_devices->rw_devices);\n\t\t\tbreak;\n\t\t}\n\t\tdevices_info[ndevs].dev_offset = dev_offset;\n\t\tdevices_info[ndevs].max_avail = max_avail;\n\t\tdevices_info[ndevs].total_avail = total_avail;\n\t\tdevices_info[ndevs].dev = device;\n\t\t++ndevs;\n\t}\n\tctl->ndevs = ndevs;\n\n\t \n\tsort(devices_info, ndevs, sizeof(struct btrfs_device_info),\n\t     btrfs_cmp_device_info, NULL);\n\n\treturn 0;\n}\n\nstatic int decide_stripe_size_regular(struct alloc_chunk_ctl *ctl,\n\t\t\t\t      struct btrfs_device_info *devices_info)\n{\n\t \n\tint data_stripes;\n\n\t \n\tctl->stripe_size = div_u64(devices_info[ctl->ndevs - 1].max_avail,\n\t\t\t\t   ctl->dev_stripes);\n\tctl->num_stripes = ctl->ndevs * ctl->dev_stripes;\n\n\t \n\tdata_stripes = (ctl->num_stripes - ctl->nparity) / ctl->ncopies;\n\n\t \n\tif (ctl->stripe_size * data_stripes > ctl->max_chunk_size) {\n\t\t \n\t\tctl->stripe_size = min(round_up(div_u64(ctl->max_chunk_size,\n\t\t\t\t\t\t\tdata_stripes), SZ_16M),\n\t\t\t\t       ctl->stripe_size);\n\t}\n\n\t \n\tctl->stripe_size = min_t(u64, ctl->stripe_size, SZ_1G);\n\n\t \n\tctl->stripe_size = round_down(ctl->stripe_size, BTRFS_STRIPE_LEN);\n\tctl->chunk_size = ctl->stripe_size * data_stripes;\n\n\treturn 0;\n}\n\nstatic int decide_stripe_size_zoned(struct alloc_chunk_ctl *ctl,\n\t\t\t\t    struct btrfs_device_info *devices_info)\n{\n\tu64 zone_size = devices_info[0].dev->zone_info->zone_size;\n\t \n\tint data_stripes;\n\n\t \n\tASSERT(devices_info[ctl->ndevs - 1].max_avail == ctl->dev_extent_min);\n\n\tctl->stripe_size = zone_size;\n\tctl->num_stripes = ctl->ndevs * ctl->dev_stripes;\n\tdata_stripes = (ctl->num_stripes - ctl->nparity) / ctl->ncopies;\n\n\t \n\tif (ctl->stripe_size * data_stripes > ctl->max_chunk_size) {\n\t\tctl->ndevs = div_u64(div_u64(ctl->max_chunk_size * ctl->ncopies,\n\t\t\t\t\t     ctl->stripe_size) + ctl->nparity,\n\t\t\t\t     ctl->dev_stripes);\n\t\tctl->num_stripes = ctl->ndevs * ctl->dev_stripes;\n\t\tdata_stripes = (ctl->num_stripes - ctl->nparity) / ctl->ncopies;\n\t\tASSERT(ctl->stripe_size * data_stripes <= ctl->max_chunk_size);\n\t}\n\n\tctl->chunk_size = ctl->stripe_size * data_stripes;\n\n\treturn 0;\n}\n\nstatic int decide_stripe_size(struct btrfs_fs_devices *fs_devices,\n\t\t\t      struct alloc_chunk_ctl *ctl,\n\t\t\t      struct btrfs_device_info *devices_info)\n{\n\tstruct btrfs_fs_info *info = fs_devices->fs_info;\n\n\t \n\tctl->ndevs = rounddown(ctl->ndevs, ctl->devs_increment);\n\n\tif (ctl->ndevs < ctl->devs_min) {\n\t\tif (btrfs_test_opt(info, ENOSPC_DEBUG)) {\n\t\t\tbtrfs_debug(info,\n\t\"%s: not enough devices with free space: have=%d minimum required=%d\",\n\t\t\t\t    __func__, ctl->ndevs, ctl->devs_min);\n\t\t}\n\t\treturn -ENOSPC;\n\t}\n\n\tctl->ndevs = min(ctl->ndevs, ctl->devs_max);\n\n\tswitch (fs_devices->chunk_alloc_policy) {\n\tcase BTRFS_CHUNK_ALLOC_REGULAR:\n\t\treturn decide_stripe_size_regular(ctl, devices_info);\n\tcase BTRFS_CHUNK_ALLOC_ZONED:\n\t\treturn decide_stripe_size_zoned(ctl, devices_info);\n\tdefault:\n\t\tBUG();\n\t}\n}\n\nstatic struct btrfs_block_group *create_chunk(struct btrfs_trans_handle *trans,\n\t\t\tstruct alloc_chunk_ctl *ctl,\n\t\t\tstruct btrfs_device_info *devices_info)\n{\n\tstruct btrfs_fs_info *info = trans->fs_info;\n\tstruct map_lookup *map = NULL;\n\tstruct extent_map_tree *em_tree;\n\tstruct btrfs_block_group *block_group;\n\tstruct extent_map *em;\n\tu64 start = ctl->start;\n\tu64 type = ctl->type;\n\tint ret;\n\tint i;\n\tint j;\n\n\tmap = kmalloc(map_lookup_size(ctl->num_stripes), GFP_NOFS);\n\tif (!map)\n\t\treturn ERR_PTR(-ENOMEM);\n\tmap->num_stripes = ctl->num_stripes;\n\n\tfor (i = 0; i < ctl->ndevs; ++i) {\n\t\tfor (j = 0; j < ctl->dev_stripes; ++j) {\n\t\t\tint s = i * ctl->dev_stripes + j;\n\t\t\tmap->stripes[s].dev = devices_info[i].dev;\n\t\t\tmap->stripes[s].physical = devices_info[i].dev_offset +\n\t\t\t\t\t\t   j * ctl->stripe_size;\n\t\t}\n\t}\n\tmap->io_align = BTRFS_STRIPE_LEN;\n\tmap->io_width = BTRFS_STRIPE_LEN;\n\tmap->type = type;\n\tmap->sub_stripes = ctl->sub_stripes;\n\n\ttrace_btrfs_chunk_alloc(info, map, start, ctl->chunk_size);\n\n\tem = alloc_extent_map();\n\tif (!em) {\n\t\tkfree(map);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tset_bit(EXTENT_FLAG_FS_MAPPING, &em->flags);\n\tem->map_lookup = map;\n\tem->start = start;\n\tem->len = ctl->chunk_size;\n\tem->block_start = 0;\n\tem->block_len = em->len;\n\tem->orig_block_len = ctl->stripe_size;\n\n\tem_tree = &info->mapping_tree;\n\twrite_lock(&em_tree->lock);\n\tret = add_extent_mapping(em_tree, em, 0);\n\tif (ret) {\n\t\twrite_unlock(&em_tree->lock);\n\t\tfree_extent_map(em);\n\t\treturn ERR_PTR(ret);\n\t}\n\twrite_unlock(&em_tree->lock);\n\n\tblock_group = btrfs_make_block_group(trans, type, start, ctl->chunk_size);\n\tif (IS_ERR(block_group))\n\t\tgoto error_del_extent;\n\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tstruct btrfs_device *dev = map->stripes[i].dev;\n\n\t\tbtrfs_device_set_bytes_used(dev,\n\t\t\t\t\t    dev->bytes_used + ctl->stripe_size);\n\t\tif (list_empty(&dev->post_commit_list))\n\t\t\tlist_add_tail(&dev->post_commit_list,\n\t\t\t\t      &trans->transaction->dev_update_list);\n\t}\n\n\tatomic64_sub(ctl->stripe_size * map->num_stripes,\n\t\t     &info->free_chunk_space);\n\n\tfree_extent_map(em);\n\tcheck_raid56_incompat_flag(info, type);\n\tcheck_raid1c34_incompat_flag(info, type);\n\n\treturn block_group;\n\nerror_del_extent:\n\twrite_lock(&em_tree->lock);\n\tremove_extent_mapping(em_tree, em);\n\twrite_unlock(&em_tree->lock);\n\n\t \n\tfree_extent_map(em);\n\t \n\tfree_extent_map(em);\n\n\treturn block_group;\n}\n\nstruct btrfs_block_group *btrfs_create_chunk(struct btrfs_trans_handle *trans,\n\t\t\t\t\t    u64 type)\n{\n\tstruct btrfs_fs_info *info = trans->fs_info;\n\tstruct btrfs_fs_devices *fs_devices = info->fs_devices;\n\tstruct btrfs_device_info *devices_info = NULL;\n\tstruct alloc_chunk_ctl ctl;\n\tstruct btrfs_block_group *block_group;\n\tint ret;\n\n\tlockdep_assert_held(&info->chunk_mutex);\n\n\tif (!alloc_profile_is_valid(type, 0)) {\n\t\tASSERT(0);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif (list_empty(&fs_devices->alloc_list)) {\n\t\tif (btrfs_test_opt(info, ENOSPC_DEBUG))\n\t\t\tbtrfs_debug(info, \"%s: no writable device\", __func__);\n\t\treturn ERR_PTR(-ENOSPC);\n\t}\n\n\tif (!(type & BTRFS_BLOCK_GROUP_TYPE_MASK)) {\n\t\tbtrfs_err(info, \"invalid chunk type 0x%llx requested\", type);\n\t\tASSERT(0);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tctl.start = find_next_chunk(info);\n\tctl.type = type;\n\tinit_alloc_chunk_ctl(fs_devices, &ctl);\n\n\tdevices_info = kcalloc(fs_devices->rw_devices, sizeof(*devices_info),\n\t\t\t       GFP_NOFS);\n\tif (!devices_info)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tret = gather_device_info(fs_devices, &ctl, devices_info);\n\tif (ret < 0) {\n\t\tblock_group = ERR_PTR(ret);\n\t\tgoto out;\n\t}\n\n\tret = decide_stripe_size(fs_devices, &ctl, devices_info);\n\tif (ret < 0) {\n\t\tblock_group = ERR_PTR(ret);\n\t\tgoto out;\n\t}\n\n\tblock_group = create_chunk(trans, &ctl, devices_info);\n\nout:\n\tkfree(devices_info);\n\treturn block_group;\n}\n\n \nint btrfs_chunk_alloc_add_chunk_item(struct btrfs_trans_handle *trans,\n\t\t\t\t     struct btrfs_block_group *bg)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_root *chunk_root = fs_info->chunk_root;\n\tstruct btrfs_key key;\n\tstruct btrfs_chunk *chunk;\n\tstruct btrfs_stripe *stripe;\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tsize_t item_size;\n\tint i;\n\tint ret;\n\n\t \n\tlockdep_assert_held(&fs_info->chunk_mutex);\n\n\tem = btrfs_get_chunk_map(fs_info, bg->start, bg->length);\n\tif (IS_ERR(em)) {\n\t\tret = PTR_ERR(em);\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\treturn ret;\n\t}\n\n\tmap = em->map_lookup;\n\titem_size = btrfs_chunk_item_size(map->num_stripes);\n\n\tchunk = kzalloc(item_size, GFP_NOFS);\n\tif (!chunk) {\n\t\tret = -ENOMEM;\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tstruct btrfs_device *device = map->stripes[i].dev;\n\n\t\tret = btrfs_update_device(trans, device);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tstripe = &chunk->stripe;\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tstruct btrfs_device *device = map->stripes[i].dev;\n\t\tconst u64 dev_offset = map->stripes[i].physical;\n\n\t\tbtrfs_set_stack_stripe_devid(stripe, device->devid);\n\t\tbtrfs_set_stack_stripe_offset(stripe, dev_offset);\n\t\tmemcpy(stripe->dev_uuid, device->uuid, BTRFS_UUID_SIZE);\n\t\tstripe++;\n\t}\n\n\tbtrfs_set_stack_chunk_length(chunk, bg->length);\n\tbtrfs_set_stack_chunk_owner(chunk, BTRFS_EXTENT_TREE_OBJECTID);\n\tbtrfs_set_stack_chunk_stripe_len(chunk, BTRFS_STRIPE_LEN);\n\tbtrfs_set_stack_chunk_type(chunk, map->type);\n\tbtrfs_set_stack_chunk_num_stripes(chunk, map->num_stripes);\n\tbtrfs_set_stack_chunk_io_align(chunk, BTRFS_STRIPE_LEN);\n\tbtrfs_set_stack_chunk_io_width(chunk, BTRFS_STRIPE_LEN);\n\tbtrfs_set_stack_chunk_sector_size(chunk, fs_info->sectorsize);\n\tbtrfs_set_stack_chunk_sub_stripes(chunk, map->sub_stripes);\n\n\tkey.objectid = BTRFS_FIRST_CHUNK_TREE_OBJECTID;\n\tkey.type = BTRFS_CHUNK_ITEM_KEY;\n\tkey.offset = bg->start;\n\n\tret = btrfs_insert_item(trans, chunk_root, &key, chunk, item_size);\n\tif (ret)\n\t\tgoto out;\n\n\tset_bit(BLOCK_GROUP_FLAG_CHUNK_ITEM_INSERTED, &bg->runtime_flags);\n\n\tif (map->type & BTRFS_BLOCK_GROUP_SYSTEM) {\n\t\tret = btrfs_add_system_chunk(fs_info, &key, chunk, item_size);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\nout:\n\tkfree(chunk);\n\tfree_extent_map(em);\n\treturn ret;\n}\n\nstatic noinline int init_first_rw_device(struct btrfs_trans_handle *trans)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tu64 alloc_profile;\n\tstruct btrfs_block_group *meta_bg;\n\tstruct btrfs_block_group *sys_bg;\n\n\t \n\n\talloc_profile = btrfs_metadata_alloc_profile(fs_info);\n\tmeta_bg = btrfs_create_chunk(trans, alloc_profile);\n\tif (IS_ERR(meta_bg))\n\t\treturn PTR_ERR(meta_bg);\n\n\talloc_profile = btrfs_system_alloc_profile(fs_info);\n\tsys_bg = btrfs_create_chunk(trans, alloc_profile);\n\tif (IS_ERR(sys_bg))\n\t\treturn PTR_ERR(sys_bg);\n\n\treturn 0;\n}\n\nstatic inline int btrfs_chunk_max_errors(struct map_lookup *map)\n{\n\tconst int index = btrfs_bg_flags_to_raid_index(map->type);\n\n\treturn btrfs_raid_array[index].tolerated_failures;\n}\n\nbool btrfs_chunk_writeable(struct btrfs_fs_info *fs_info, u64 chunk_offset)\n{\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tint miss_ndevs = 0;\n\tint i;\n\tbool ret = true;\n\n\tem = btrfs_get_chunk_map(fs_info, chunk_offset, 1);\n\tif (IS_ERR(em))\n\t\treturn false;\n\n\tmap = em->map_lookup;\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tif (test_bit(BTRFS_DEV_STATE_MISSING,\n\t\t\t\t\t&map->stripes[i].dev->dev_state)) {\n\t\t\tmiss_ndevs++;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!test_bit(BTRFS_DEV_STATE_WRITEABLE,\n\t\t\t\t\t&map->stripes[i].dev->dev_state)) {\n\t\t\tret = false;\n\t\t\tgoto end;\n\t\t}\n\t}\n\n\t \n\tif (miss_ndevs > btrfs_chunk_max_errors(map))\n\t\tret = false;\nend:\n\tfree_extent_map(em);\n\treturn ret;\n}\n\nvoid btrfs_mapping_tree_free(struct extent_map_tree *tree)\n{\n\tstruct extent_map *em;\n\n\twhile (1) {\n\t\twrite_lock(&tree->lock);\n\t\tem = lookup_extent_mapping(tree, 0, (u64)-1);\n\t\tif (em)\n\t\t\tremove_extent_mapping(tree, em);\n\t\twrite_unlock(&tree->lock);\n\t\tif (!em)\n\t\t\tbreak;\n\t\t \n\t\tfree_extent_map(em);\n\t\t \n\t\tfree_extent_map(em);\n\t}\n}\n\nint btrfs_num_copies(struct btrfs_fs_info *fs_info, u64 logical, u64 len)\n{\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tenum btrfs_raid_types index;\n\tint ret = 1;\n\n\tem = btrfs_get_chunk_map(fs_info, logical, len);\n\tif (IS_ERR(em))\n\t\t \n\t\treturn 1;\n\n\tmap = em->map_lookup;\n\tindex = btrfs_bg_flags_to_raid_index(map->type);\n\n\t \n\tif (!(map->type & BTRFS_BLOCK_GROUP_RAID56_MASK))\n\t\tret = btrfs_raid_array[index].ncopies;\n\telse if (map->type & BTRFS_BLOCK_GROUP_RAID5)\n\t\tret = 2;\n\telse if (map->type & BTRFS_BLOCK_GROUP_RAID6)\n\t\t \n\t\tret = map->num_stripes;\n\tfree_extent_map(em);\n\treturn ret;\n}\n\nunsigned long btrfs_full_stripe_len(struct btrfs_fs_info *fs_info,\n\t\t\t\t    u64 logical)\n{\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tunsigned long len = fs_info->sectorsize;\n\n\tif (!btrfs_fs_incompat(fs_info, RAID56))\n\t\treturn len;\n\n\tem = btrfs_get_chunk_map(fs_info, logical, len);\n\n\tif (!WARN_ON(IS_ERR(em))) {\n\t\tmap = em->map_lookup;\n\t\tif (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK)\n\t\t\tlen = btrfs_stripe_nr_to_offset(nr_data_stripes(map));\n\t\tfree_extent_map(em);\n\t}\n\treturn len;\n}\n\nint btrfs_is_parity_mirror(struct btrfs_fs_info *fs_info, u64 logical, u64 len)\n{\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tint ret = 0;\n\n\tif (!btrfs_fs_incompat(fs_info, RAID56))\n\t\treturn 0;\n\n\tem = btrfs_get_chunk_map(fs_info, logical, len);\n\n\tif(!WARN_ON(IS_ERR(em))) {\n\t\tmap = em->map_lookup;\n\t\tif (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK)\n\t\t\tret = 1;\n\t\tfree_extent_map(em);\n\t}\n\treturn ret;\n}\n\nstatic int find_live_mirror(struct btrfs_fs_info *fs_info,\n\t\t\t    struct map_lookup *map, int first,\n\t\t\t    int dev_replace_is_ongoing)\n{\n\tint i;\n\tint num_stripes;\n\tint preferred_mirror;\n\tint tolerance;\n\tstruct btrfs_device *srcdev;\n\n\tASSERT((map->type &\n\t\t (BTRFS_BLOCK_GROUP_RAID1_MASK | BTRFS_BLOCK_GROUP_RAID10)));\n\n\tif (map->type & BTRFS_BLOCK_GROUP_RAID10)\n\t\tnum_stripes = map->sub_stripes;\n\telse\n\t\tnum_stripes = map->num_stripes;\n\n\tswitch (fs_info->fs_devices->read_policy) {\n\tdefault:\n\t\t \n\t\tbtrfs_warn_rl(fs_info,\n\t\t\t      \"unknown read_policy type %u, reset to pid\",\n\t\t\t      fs_info->fs_devices->read_policy);\n\t\tfs_info->fs_devices->read_policy = BTRFS_READ_POLICY_PID;\n\t\tfallthrough;\n\tcase BTRFS_READ_POLICY_PID:\n\t\tpreferred_mirror = first + (current->pid % num_stripes);\n\t\tbreak;\n\t}\n\n\tif (dev_replace_is_ongoing &&\n\t    fs_info->dev_replace.cont_reading_from_srcdev_mode ==\n\t     BTRFS_DEV_REPLACE_ITEM_CONT_READING_FROM_SRCDEV_MODE_AVOID)\n\t\tsrcdev = fs_info->dev_replace.srcdev;\n\telse\n\t\tsrcdev = NULL;\n\n\t \n\tfor (tolerance = 0; tolerance < 2; tolerance++) {\n\t\tif (map->stripes[preferred_mirror].dev->bdev &&\n\t\t    (tolerance || map->stripes[preferred_mirror].dev != srcdev))\n\t\t\treturn preferred_mirror;\n\t\tfor (i = first; i < first + num_stripes; i++) {\n\t\t\tif (map->stripes[i].dev->bdev &&\n\t\t\t    (tolerance || map->stripes[i].dev != srcdev))\n\t\t\t\treturn i;\n\t\t}\n\t}\n\n\t \n\treturn preferred_mirror;\n}\n\nstatic struct btrfs_io_context *alloc_btrfs_io_context(struct btrfs_fs_info *fs_info,\n\t\t\t\t\t\t       u16 total_stripes)\n{\n\tstruct btrfs_io_context *bioc;\n\n\tbioc = kzalloc(\n\t\t  \n\t\tsizeof(struct btrfs_io_context) +\n\t\t \n\t\tsizeof(struct btrfs_io_stripe) * (total_stripes),\n\t\tGFP_NOFS);\n\n\tif (!bioc)\n\t\treturn NULL;\n\n\trefcount_set(&bioc->refs, 1);\n\n\tbioc->fs_info = fs_info;\n\tbioc->replace_stripe_src = -1;\n\tbioc->full_stripe_logical = (u64)-1;\n\n\treturn bioc;\n}\n\nvoid btrfs_get_bioc(struct btrfs_io_context *bioc)\n{\n\tWARN_ON(!refcount_read(&bioc->refs));\n\trefcount_inc(&bioc->refs);\n}\n\nvoid btrfs_put_bioc(struct btrfs_io_context *bioc)\n{\n\tif (!bioc)\n\t\treturn;\n\tif (refcount_dec_and_test(&bioc->refs))\n\t\tkfree(bioc);\n}\n\n \nstruct btrfs_discard_stripe *btrfs_map_discard(struct btrfs_fs_info *fs_info,\n\t\t\t\t\t       u64 logical, u64 *length_ret,\n\t\t\t\t\t       u32 *num_stripes)\n{\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tstruct btrfs_discard_stripe *stripes;\n\tu64 length = *length_ret;\n\tu64 offset;\n\tu32 stripe_nr;\n\tu32 stripe_nr_end;\n\tu32 stripe_cnt;\n\tu64 stripe_end_offset;\n\tu64 stripe_offset;\n\tu32 stripe_index;\n\tu32 factor = 0;\n\tu32 sub_stripes = 0;\n\tu32 stripes_per_dev = 0;\n\tu32 remaining_stripes = 0;\n\tu32 last_stripe = 0;\n\tint ret;\n\tint i;\n\n\tem = btrfs_get_chunk_map(fs_info, logical, length);\n\tif (IS_ERR(em))\n\t\treturn ERR_CAST(em);\n\n\tmap = em->map_lookup;\n\n\t \n\tif (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out_free_map;\n\t}\n\n\toffset = logical - em->start;\n\tlength = min_t(u64, em->start + em->len - logical, length);\n\t*length_ret = length;\n\n\t \n\tstripe_nr = offset >> BTRFS_STRIPE_LEN_SHIFT;\n\n\t \n\tstripe_offset = offset - btrfs_stripe_nr_to_offset(stripe_nr);\n\n\tstripe_nr_end = round_up(offset + length, BTRFS_STRIPE_LEN) >>\n\t\t\tBTRFS_STRIPE_LEN_SHIFT;\n\tstripe_cnt = stripe_nr_end - stripe_nr;\n\tstripe_end_offset = btrfs_stripe_nr_to_offset(stripe_nr_end) -\n\t\t\t    (offset + length);\n\t \n\t*num_stripes = 1;\n\tstripe_index = 0;\n\tif (map->type & (BTRFS_BLOCK_GROUP_RAID0 |\n\t\t\t BTRFS_BLOCK_GROUP_RAID10)) {\n\t\tif (map->type & BTRFS_BLOCK_GROUP_RAID0)\n\t\t\tsub_stripes = 1;\n\t\telse\n\t\t\tsub_stripes = map->sub_stripes;\n\n\t\tfactor = map->num_stripes / sub_stripes;\n\t\t*num_stripes = min_t(u64, map->num_stripes,\n\t\t\t\t    sub_stripes * stripe_cnt);\n\t\tstripe_index = stripe_nr % factor;\n\t\tstripe_nr /= factor;\n\t\tstripe_index *= sub_stripes;\n\n\t\tremaining_stripes = stripe_cnt % factor;\n\t\tstripes_per_dev = stripe_cnt / factor;\n\t\tlast_stripe = ((stripe_nr_end - 1) % factor) * sub_stripes;\n\t} else if (map->type & (BTRFS_BLOCK_GROUP_RAID1_MASK |\n\t\t\t\tBTRFS_BLOCK_GROUP_DUP)) {\n\t\t*num_stripes = map->num_stripes;\n\t} else {\n\t\tstripe_index = stripe_nr % map->num_stripes;\n\t\tstripe_nr /= map->num_stripes;\n\t}\n\n\tstripes = kcalloc(*num_stripes, sizeof(*stripes), GFP_NOFS);\n\tif (!stripes) {\n\t\tret = -ENOMEM;\n\t\tgoto out_free_map;\n\t}\n\n\tfor (i = 0; i < *num_stripes; i++) {\n\t\tstripes[i].physical =\n\t\t\tmap->stripes[stripe_index].physical +\n\t\t\tstripe_offset + btrfs_stripe_nr_to_offset(stripe_nr);\n\t\tstripes[i].dev = map->stripes[stripe_index].dev;\n\n\t\tif (map->type & (BTRFS_BLOCK_GROUP_RAID0 |\n\t\t\t\t BTRFS_BLOCK_GROUP_RAID10)) {\n\t\t\tstripes[i].length = btrfs_stripe_nr_to_offset(stripes_per_dev);\n\n\t\t\tif (i / sub_stripes < remaining_stripes)\n\t\t\t\tstripes[i].length += BTRFS_STRIPE_LEN;\n\n\t\t\t \n\t\t\tif (i < sub_stripes)\n\t\t\t\tstripes[i].length -= stripe_offset;\n\n\t\t\tif (stripe_index >= last_stripe &&\n\t\t\t    stripe_index <= (last_stripe +\n\t\t\t\t\t     sub_stripes - 1))\n\t\t\t\tstripes[i].length -= stripe_end_offset;\n\n\t\t\tif (i == sub_stripes - 1)\n\t\t\t\tstripe_offset = 0;\n\t\t} else {\n\t\t\tstripes[i].length = length;\n\t\t}\n\n\t\tstripe_index++;\n\t\tif (stripe_index == map->num_stripes) {\n\t\t\tstripe_index = 0;\n\t\t\tstripe_nr++;\n\t\t}\n\t}\n\n\tfree_extent_map(em);\n\treturn stripes;\nout_free_map:\n\tfree_extent_map(em);\n\treturn ERR_PTR(ret);\n}\n\nstatic bool is_block_group_to_copy(struct btrfs_fs_info *fs_info, u64 logical)\n{\n\tstruct btrfs_block_group *cache;\n\tbool ret;\n\n\t \n\tif (!btrfs_is_zoned(fs_info))\n\t\treturn false;\n\n\tcache = btrfs_lookup_block_group(fs_info, logical);\n\n\tret = test_bit(BLOCK_GROUP_FLAG_TO_COPY, &cache->runtime_flags);\n\n\tbtrfs_put_block_group(cache);\n\treturn ret;\n}\n\nstatic void handle_ops_on_dev_replace(enum btrfs_map_op op,\n\t\t\t\t      struct btrfs_io_context *bioc,\n\t\t\t\t      struct btrfs_dev_replace *dev_replace,\n\t\t\t\t      u64 logical,\n\t\t\t\t      int *num_stripes_ret, int *max_errors_ret)\n{\n\tu64 srcdev_devid = dev_replace->srcdev->devid;\n\t \n\tint num_stripes = *num_stripes_ret;\n\tint nr_extra_stripes = 0;\n\tint max_errors = *max_errors_ret;\n\tint i;\n\n\t \n\tif (is_block_group_to_copy(dev_replace->srcdev->fs_info, logical))\n\t\treturn;\n\n\t \n\tfor (i = 0; i < num_stripes; i++) {\n\t\tstruct btrfs_io_stripe *old = &bioc->stripes[i];\n\t\tstruct btrfs_io_stripe *new = &bioc->stripes[num_stripes + nr_extra_stripes];\n\n\t\tif (old->dev->devid != srcdev_devid)\n\t\t\tcontinue;\n\n\t\tnew->physical = old->physical;\n\t\tnew->dev = dev_replace->tgtdev;\n\t\tif (bioc->map_type & BTRFS_BLOCK_GROUP_RAID56_MASK)\n\t\t\tbioc->replace_stripe_src = i;\n\t\tnr_extra_stripes++;\n\t}\n\n\t \n\tASSERT(nr_extra_stripes <= 2);\n\t \n\tif (op == BTRFS_MAP_GET_READ_MIRRORS && nr_extra_stripes == 2) {\n\t\tstruct btrfs_io_stripe *first = &bioc->stripes[num_stripes];\n\t\tstruct btrfs_io_stripe *second = &bioc->stripes[num_stripes + 1];\n\n\t\t \n\t\tASSERT(bioc->map_type & BTRFS_BLOCK_GROUP_DUP);\n\n\t\t \n\t\tif (first->physical > second->physical) {\n\t\t\tswap(second->physical, first->physical);\n\t\t\tswap(second->dev, first->dev);\n\t\t\tnr_extra_stripes--;\n\t\t}\n\t}\n\n\t*num_stripes_ret = num_stripes + nr_extra_stripes;\n\t*max_errors_ret = max_errors + nr_extra_stripes;\n\tbioc->replace_nr_stripes = nr_extra_stripes;\n}\n\nstatic u64 btrfs_max_io_len(struct map_lookup *map, enum btrfs_map_op op,\n\t\t\t    u64 offset, u32 *stripe_nr, u64 *stripe_offset,\n\t\t\t    u64 *full_stripe_start)\n{\n\t \n\t*stripe_offset = offset & BTRFS_STRIPE_LEN_MASK;\n\t*stripe_nr = offset >> BTRFS_STRIPE_LEN_SHIFT;\n\tASSERT(*stripe_offset < U32_MAX);\n\n\tif (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK) {\n\t\tunsigned long full_stripe_len =\n\t\t\tbtrfs_stripe_nr_to_offset(nr_data_stripes(map));\n\n\t\t \n\t\t*full_stripe_start =\n\t\t\tbtrfs_stripe_nr_to_offset(\n\t\t\t\trounddown(*stripe_nr, nr_data_stripes(map)));\n\n\t\tASSERT(*full_stripe_start + full_stripe_len > offset);\n\t\tASSERT(*full_stripe_start <= offset);\n\t\t \n\t\tif (op == BTRFS_MAP_WRITE)\n\t\t\treturn full_stripe_len - (offset - *full_stripe_start);\n\t}\n\n\t \n\tif (map->type & BTRFS_BLOCK_GROUP_STRIPE_MASK)\n\t\treturn BTRFS_STRIPE_LEN - *stripe_offset;\n\treturn U64_MAX;\n}\n\nstatic void set_io_stripe(struct btrfs_io_stripe *dst, const struct map_lookup *map,\n\t\t\t  u32 stripe_index, u64 stripe_offset, u32 stripe_nr)\n{\n\tdst->dev = map->stripes[stripe_index].dev;\n\tdst->physical = map->stripes[stripe_index].physical +\n\t\t\tstripe_offset + btrfs_stripe_nr_to_offset(stripe_nr);\n}\n\n \nint btrfs_map_block(struct btrfs_fs_info *fs_info, enum btrfs_map_op op,\n\t\t    u64 logical, u64 *length,\n\t\t    struct btrfs_io_context **bioc_ret,\n\t\t    struct btrfs_io_stripe *smap, int *mirror_num_ret,\n\t\t    int need_raid_map)\n{\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tu64 map_offset;\n\tu64 stripe_offset;\n\tu32 stripe_nr;\n\tu32 stripe_index;\n\tint data_stripes;\n\tint i;\n\tint ret = 0;\n\tint mirror_num = (mirror_num_ret ? *mirror_num_ret : 0);\n\tint num_stripes;\n\tint num_copies;\n\tint max_errors = 0;\n\tstruct btrfs_io_context *bioc = NULL;\n\tstruct btrfs_dev_replace *dev_replace = &fs_info->dev_replace;\n\tint dev_replace_is_ongoing = 0;\n\tu16 num_alloc_stripes;\n\tu64 raid56_full_stripe_start = (u64)-1;\n\tu64 max_len;\n\n\tASSERT(bioc_ret);\n\n\tnum_copies = btrfs_num_copies(fs_info, logical, fs_info->sectorsize);\n\tif (mirror_num > num_copies)\n\t\treturn -EINVAL;\n\n\tem = btrfs_get_chunk_map(fs_info, logical, *length);\n\tif (IS_ERR(em))\n\t\treturn PTR_ERR(em);\n\n\tmap = em->map_lookup;\n\tdata_stripes = nr_data_stripes(map);\n\n\tmap_offset = logical - em->start;\n\tmax_len = btrfs_max_io_len(map, op, map_offset, &stripe_nr,\n\t\t\t\t   &stripe_offset, &raid56_full_stripe_start);\n\t*length = min_t(u64, em->len - map_offset, max_len);\n\n\tdown_read(&dev_replace->rwsem);\n\tdev_replace_is_ongoing = btrfs_dev_replace_is_ongoing(dev_replace);\n\t \n\tif (!dev_replace_is_ongoing)\n\t\tup_read(&dev_replace->rwsem);\n\n\tnum_stripes = 1;\n\tstripe_index = 0;\n\tif (map->type & BTRFS_BLOCK_GROUP_RAID0) {\n\t\tstripe_index = stripe_nr % map->num_stripes;\n\t\tstripe_nr /= map->num_stripes;\n\t\tif (op == BTRFS_MAP_READ)\n\t\t\tmirror_num = 1;\n\t} else if (map->type & BTRFS_BLOCK_GROUP_RAID1_MASK) {\n\t\tif (op != BTRFS_MAP_READ) {\n\t\t\tnum_stripes = map->num_stripes;\n\t\t} else if (mirror_num) {\n\t\t\tstripe_index = mirror_num - 1;\n\t\t} else {\n\t\t\tstripe_index = find_live_mirror(fs_info, map, 0,\n\t\t\t\t\t    dev_replace_is_ongoing);\n\t\t\tmirror_num = stripe_index + 1;\n\t\t}\n\n\t} else if (map->type & BTRFS_BLOCK_GROUP_DUP) {\n\t\tif (op != BTRFS_MAP_READ) {\n\t\t\tnum_stripes = map->num_stripes;\n\t\t} else if (mirror_num) {\n\t\t\tstripe_index = mirror_num - 1;\n\t\t} else {\n\t\t\tmirror_num = 1;\n\t\t}\n\n\t} else if (map->type & BTRFS_BLOCK_GROUP_RAID10) {\n\t\tu32 factor = map->num_stripes / map->sub_stripes;\n\n\t\tstripe_index = (stripe_nr % factor) * map->sub_stripes;\n\t\tstripe_nr /= factor;\n\n\t\tif (op != BTRFS_MAP_READ)\n\t\t\tnum_stripes = map->sub_stripes;\n\t\telse if (mirror_num)\n\t\t\tstripe_index += mirror_num - 1;\n\t\telse {\n\t\t\tint old_stripe_index = stripe_index;\n\t\t\tstripe_index = find_live_mirror(fs_info, map,\n\t\t\t\t\t      stripe_index,\n\t\t\t\t\t      dev_replace_is_ongoing);\n\t\t\tmirror_num = stripe_index - old_stripe_index + 1;\n\t\t}\n\n\t} else if (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK) {\n\t\tif (need_raid_map && (op != BTRFS_MAP_READ || mirror_num > 1)) {\n\t\t\t \n\t\t\tstripe_nr /= data_stripes;\n\n\t\t\t \n\t\t\tnum_stripes = map->num_stripes;\n\t\t\tmax_errors = btrfs_chunk_max_errors(map);\n\n\t\t\t \n\t\t\t*length = min(logical + *length,\n\t\t\t\t      raid56_full_stripe_start + em->start +\n\t\t\t\t      btrfs_stripe_nr_to_offset(data_stripes)) -\n\t\t\t\t  logical;\n\t\t\tstripe_index = 0;\n\t\t\tstripe_offset = 0;\n\t\t} else {\n\t\t\t \n\t\t\tstripe_index = stripe_nr % data_stripes;\n\t\t\tstripe_nr /= data_stripes;\n\t\t\tif (mirror_num > 1)\n\t\t\t\tstripe_index = data_stripes + mirror_num - 2;\n\n\t\t\t \n\t\t\tstripe_index = (stripe_nr + stripe_index) % map->num_stripes;\n\t\t\tif (op == BTRFS_MAP_READ && mirror_num <= 1)\n\t\t\t\tmirror_num = 1;\n\t\t}\n\t} else {\n\t\t \n\t\tstripe_index = stripe_nr % map->num_stripes;\n\t\tstripe_nr /= map->num_stripes;\n\t\tmirror_num = stripe_index + 1;\n\t}\n\tif (stripe_index >= map->num_stripes) {\n\t\tbtrfs_crit(fs_info,\n\t\t\t   \"stripe index math went horribly wrong, got stripe_index=%u, num_stripes=%u\",\n\t\t\t   stripe_index, map->num_stripes);\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tnum_alloc_stripes = num_stripes;\n\tif (dev_replace_is_ongoing && dev_replace->tgtdev != NULL &&\n\t    op != BTRFS_MAP_READ)\n\t\t \n\t\tnum_alloc_stripes += 2;\n\n\t \n\tif (smap && num_alloc_stripes == 1 &&\n\t    !((map->type & BTRFS_BLOCK_GROUP_RAID56_MASK) && mirror_num > 1)) {\n\t\tset_io_stripe(smap, map, stripe_index, stripe_offset, stripe_nr);\n\t\tif (mirror_num_ret)\n\t\t\t*mirror_num_ret = mirror_num;\n\t\t*bioc_ret = NULL;\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\tbioc = alloc_btrfs_io_context(fs_info, num_alloc_stripes);\n\tif (!bioc) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tbioc->map_type = map->type;\n\n\t \n\tif (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK && need_raid_map &&\n\t    (op != BTRFS_MAP_READ || mirror_num > 1)) {\n\t\t \n\t\tbioc->full_stripe_logical = em->start +\n\t\t\tbtrfs_stripe_nr_to_offset(stripe_nr * data_stripes);\n\t\tfor (i = 0; i < num_stripes; i++)\n\t\t\tset_io_stripe(&bioc->stripes[i], map,\n\t\t\t\t      (i + stripe_nr) % num_stripes,\n\t\t\t\t      stripe_offset, stripe_nr);\n\t} else {\n\t\t \n\t\tfor (i = 0; i < num_stripes; i++) {\n\t\t\tset_io_stripe(&bioc->stripes[i], map, stripe_index,\n\t\t\t\t      stripe_offset, stripe_nr);\n\t\t\tstripe_index++;\n\t\t}\n\t}\n\n\tif (op != BTRFS_MAP_READ)\n\t\tmax_errors = btrfs_chunk_max_errors(map);\n\n\tif (dev_replace_is_ongoing && dev_replace->tgtdev != NULL &&\n\t    op != BTRFS_MAP_READ) {\n\t\thandle_ops_on_dev_replace(op, bioc, dev_replace, logical,\n\t\t\t\t\t  &num_stripes, &max_errors);\n\t}\n\n\t*bioc_ret = bioc;\n\tbioc->num_stripes = num_stripes;\n\tbioc->max_errors = max_errors;\n\tbioc->mirror_num = mirror_num;\n\nout:\n\tif (dev_replace_is_ongoing) {\n\t\tlockdep_assert_held(&dev_replace->rwsem);\n\t\t \n\t\tup_read(&dev_replace->rwsem);\n\t}\n\tfree_extent_map(em);\n\treturn ret;\n}\n\nstatic bool dev_args_match_fs_devices(const struct btrfs_dev_lookup_args *args,\n\t\t\t\t      const struct btrfs_fs_devices *fs_devices)\n{\n\tif (args->fsid == NULL)\n\t\treturn true;\n\tif (memcmp(fs_devices->metadata_uuid, args->fsid, BTRFS_FSID_SIZE) == 0)\n\t\treturn true;\n\treturn false;\n}\n\nstatic bool dev_args_match_device(const struct btrfs_dev_lookup_args *args,\n\t\t\t\t  const struct btrfs_device *device)\n{\n\tif (args->missing) {\n\t\tif (test_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state) &&\n\t\t    !device->bdev)\n\t\t\treturn true;\n\t\treturn false;\n\t}\n\n\tif (device->devid != args->devid)\n\t\treturn false;\n\tif (args->uuid && memcmp(device->uuid, args->uuid, BTRFS_UUID_SIZE) != 0)\n\t\treturn false;\n\treturn true;\n}\n\n \nstruct btrfs_device *btrfs_find_device(const struct btrfs_fs_devices *fs_devices,\n\t\t\t\t       const struct btrfs_dev_lookup_args *args)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *seed_devs;\n\n\tif (dev_args_match_fs_devices(args, fs_devices)) {\n\t\tlist_for_each_entry(device, &fs_devices->devices, dev_list) {\n\t\t\tif (dev_args_match_device(args, device))\n\t\t\t\treturn device;\n\t\t}\n\t}\n\n\tlist_for_each_entry(seed_devs, &fs_devices->seed_list, seed_list) {\n\t\tif (!dev_args_match_fs_devices(args, seed_devs))\n\t\t\tcontinue;\n\t\tlist_for_each_entry(device, &seed_devs->devices, dev_list) {\n\t\t\tif (dev_args_match_device(args, device))\n\t\t\t\treturn device;\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\nstatic struct btrfs_device *add_missing_dev(struct btrfs_fs_devices *fs_devices,\n\t\t\t\t\t    u64 devid, u8 *dev_uuid)\n{\n\tstruct btrfs_device *device;\n\tunsigned int nofs_flag;\n\n\t \n\n\tnofs_flag = memalloc_nofs_save();\n\tdevice = btrfs_alloc_device(NULL, &devid, dev_uuid, NULL);\n\tmemalloc_nofs_restore(nofs_flag);\n\tif (IS_ERR(device))\n\t\treturn device;\n\n\tlist_add(&device->dev_list, &fs_devices->devices);\n\tdevice->fs_devices = fs_devices;\n\tfs_devices->num_devices++;\n\n\tset_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state);\n\tfs_devices->missing_devices++;\n\n\treturn device;\n}\n\n \nstruct btrfs_device *btrfs_alloc_device(struct btrfs_fs_info *fs_info,\n\t\t\t\t\tconst u64 *devid, const u8 *uuid,\n\t\t\t\t\tconst char *path)\n{\n\tstruct btrfs_device *dev;\n\tu64 tmp;\n\n\tif (WARN_ON(!devid && !fs_info))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tdev = kzalloc(sizeof(*dev), GFP_KERNEL);\n\tif (!dev)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tINIT_LIST_HEAD(&dev->dev_list);\n\tINIT_LIST_HEAD(&dev->dev_alloc_list);\n\tINIT_LIST_HEAD(&dev->post_commit_list);\n\n\tatomic_set(&dev->dev_stats_ccnt, 0);\n\tbtrfs_device_data_ordered_init(dev);\n\textent_io_tree_init(fs_info, &dev->alloc_state, IO_TREE_DEVICE_ALLOC_STATE);\n\n\tif (devid)\n\t\ttmp = *devid;\n\telse {\n\t\tint ret;\n\n\t\tret = find_next_devid(fs_info, &tmp);\n\t\tif (ret) {\n\t\t\tbtrfs_free_device(dev);\n\t\t\treturn ERR_PTR(ret);\n\t\t}\n\t}\n\tdev->devid = tmp;\n\n\tif (uuid)\n\t\tmemcpy(dev->uuid, uuid, BTRFS_UUID_SIZE);\n\telse\n\t\tgenerate_random_uuid(dev->uuid);\n\n\tif (path) {\n\t\tstruct rcu_string *name;\n\n\t\tname = rcu_string_strdup(path, GFP_KERNEL);\n\t\tif (!name) {\n\t\t\tbtrfs_free_device(dev);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\trcu_assign_pointer(dev->name, name);\n\t}\n\n\treturn dev;\n}\n\nstatic void btrfs_report_missing_device(struct btrfs_fs_info *fs_info,\n\t\t\t\t\tu64 devid, u8 *uuid, bool error)\n{\n\tif (error)\n\t\tbtrfs_err_rl(fs_info, \"devid %llu uuid %pU is missing\",\n\t\t\t      devid, uuid);\n\telse\n\t\tbtrfs_warn_rl(fs_info, \"devid %llu uuid %pU is missing\",\n\t\t\t      devid, uuid);\n}\n\nu64 btrfs_calc_stripe_length(const struct extent_map *em)\n{\n\tconst struct map_lookup *map = em->map_lookup;\n\tconst int data_stripes = calc_data_stripes(map->type, map->num_stripes);\n\n\treturn div_u64(em->len, data_stripes);\n}\n\n#if BITS_PER_LONG == 32\n \nstatic int check_32bit_meta_chunk(struct btrfs_fs_info *fs_info,\n\t\t\t\t  u64 logical, u64 length, u64 type)\n{\n\tif (!(type & BTRFS_BLOCK_GROUP_METADATA))\n\t\treturn 0;\n\n\tif (logical + length < MAX_LFS_FILESIZE)\n\t\treturn 0;\n\n\tbtrfs_err_32bit_limit(fs_info);\n\treturn -EOVERFLOW;\n}\n\n \nstatic void warn_32bit_meta_chunk(struct btrfs_fs_info *fs_info,\n\t\t\t\t  u64 logical, u64 length, u64 type)\n{\n\tif (!(type & BTRFS_BLOCK_GROUP_METADATA))\n\t\treturn;\n\n\tif (logical + length < BTRFS_32BIT_EARLY_WARN_THRESHOLD)\n\t\treturn;\n\n\tbtrfs_warn_32bit_limit(fs_info);\n}\n#endif\n\nstatic struct btrfs_device *handle_missing_device(struct btrfs_fs_info *fs_info,\n\t\t\t\t\t\t  u64 devid, u8 *uuid)\n{\n\tstruct btrfs_device *dev;\n\n\tif (!btrfs_test_opt(fs_info, DEGRADED)) {\n\t\tbtrfs_report_missing_device(fs_info, devid, uuid, true);\n\t\treturn ERR_PTR(-ENOENT);\n\t}\n\n\tdev = add_missing_dev(fs_info->fs_devices, devid, uuid);\n\tif (IS_ERR(dev)) {\n\t\tbtrfs_err(fs_info, \"failed to init missing device %llu: %ld\",\n\t\t\t  devid, PTR_ERR(dev));\n\t\treturn dev;\n\t}\n\tbtrfs_report_missing_device(fs_info, devid, uuid, false);\n\n\treturn dev;\n}\n\nstatic int read_one_chunk(struct btrfs_key *key, struct extent_buffer *leaf,\n\t\t\t  struct btrfs_chunk *chunk)\n{\n\tBTRFS_DEV_LOOKUP_ARGS(args);\n\tstruct btrfs_fs_info *fs_info = leaf->fs_info;\n\tstruct extent_map_tree *map_tree = &fs_info->mapping_tree;\n\tstruct map_lookup *map;\n\tstruct extent_map *em;\n\tu64 logical;\n\tu64 length;\n\tu64 devid;\n\tu64 type;\n\tu8 uuid[BTRFS_UUID_SIZE];\n\tint index;\n\tint num_stripes;\n\tint ret;\n\tint i;\n\n\tlogical = key->offset;\n\tlength = btrfs_chunk_length(leaf, chunk);\n\ttype = btrfs_chunk_type(leaf, chunk);\n\tindex = btrfs_bg_flags_to_raid_index(type);\n\tnum_stripes = btrfs_chunk_num_stripes(leaf, chunk);\n\n#if BITS_PER_LONG == 32\n\tret = check_32bit_meta_chunk(fs_info, logical, length, type);\n\tif (ret < 0)\n\t\treturn ret;\n\twarn_32bit_meta_chunk(fs_info, logical, length, type);\n#endif\n\n\t \n\tif (leaf->start == BTRFS_SUPER_INFO_OFFSET) {\n\t\tret = btrfs_check_chunk_valid(leaf, chunk, logical);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tread_lock(&map_tree->lock);\n\tem = lookup_extent_mapping(map_tree, logical, 1);\n\tread_unlock(&map_tree->lock);\n\n\t \n\tif (em && em->start <= logical && em->start + em->len > logical) {\n\t\tfree_extent_map(em);\n\t\treturn 0;\n\t} else if (em) {\n\t\tfree_extent_map(em);\n\t}\n\n\tem = alloc_extent_map();\n\tif (!em)\n\t\treturn -ENOMEM;\n\tmap = kmalloc(map_lookup_size(num_stripes), GFP_NOFS);\n\tif (!map) {\n\t\tfree_extent_map(em);\n\t\treturn -ENOMEM;\n\t}\n\n\tset_bit(EXTENT_FLAG_FS_MAPPING, &em->flags);\n\tem->map_lookup = map;\n\tem->start = logical;\n\tem->len = length;\n\tem->orig_start = 0;\n\tem->block_start = 0;\n\tem->block_len = em->len;\n\n\tmap->num_stripes = num_stripes;\n\tmap->io_width = btrfs_chunk_io_width(leaf, chunk);\n\tmap->io_align = btrfs_chunk_io_align(leaf, chunk);\n\tmap->type = type;\n\t \n\tmap->sub_stripes = btrfs_raid_array[index].sub_stripes;\n\tmap->verified_stripes = 0;\n\tem->orig_block_len = btrfs_calc_stripe_length(em);\n\tfor (i = 0; i < num_stripes; i++) {\n\t\tmap->stripes[i].physical =\n\t\t\tbtrfs_stripe_offset_nr(leaf, chunk, i);\n\t\tdevid = btrfs_stripe_devid_nr(leaf, chunk, i);\n\t\targs.devid = devid;\n\t\tread_extent_buffer(leaf, uuid, (unsigned long)\n\t\t\t\t   btrfs_stripe_dev_uuid_nr(chunk, i),\n\t\t\t\t   BTRFS_UUID_SIZE);\n\t\targs.uuid = uuid;\n\t\tmap->stripes[i].dev = btrfs_find_device(fs_info->fs_devices, &args);\n\t\tif (!map->stripes[i].dev) {\n\t\t\tmap->stripes[i].dev = handle_missing_device(fs_info,\n\t\t\t\t\t\t\t\t    devid, uuid);\n\t\t\tif (IS_ERR(map->stripes[i].dev)) {\n\t\t\t\tret = PTR_ERR(map->stripes[i].dev);\n\t\t\t\tfree_extent_map(em);\n\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\n\t\tset_bit(BTRFS_DEV_STATE_IN_FS_METADATA,\n\t\t\t\t&(map->stripes[i].dev->dev_state));\n\t}\n\n\twrite_lock(&map_tree->lock);\n\tret = add_extent_mapping(map_tree, em, 0);\n\twrite_unlock(&map_tree->lock);\n\tif (ret < 0) {\n\t\tbtrfs_err(fs_info,\n\t\t\t  \"failed to add chunk map, start=%llu len=%llu: %d\",\n\t\t\t  em->start, em->len, ret);\n\t}\n\tfree_extent_map(em);\n\n\treturn ret;\n}\n\nstatic void fill_device_from_item(struct extent_buffer *leaf,\n\t\t\t\t struct btrfs_dev_item *dev_item,\n\t\t\t\t struct btrfs_device *device)\n{\n\tunsigned long ptr;\n\n\tdevice->devid = btrfs_device_id(leaf, dev_item);\n\tdevice->disk_total_bytes = btrfs_device_total_bytes(leaf, dev_item);\n\tdevice->total_bytes = device->disk_total_bytes;\n\tdevice->commit_total_bytes = device->disk_total_bytes;\n\tdevice->bytes_used = btrfs_device_bytes_used(leaf, dev_item);\n\tdevice->commit_bytes_used = device->bytes_used;\n\tdevice->type = btrfs_device_type(leaf, dev_item);\n\tdevice->io_align = btrfs_device_io_align(leaf, dev_item);\n\tdevice->io_width = btrfs_device_io_width(leaf, dev_item);\n\tdevice->sector_size = btrfs_device_sector_size(leaf, dev_item);\n\tWARN_ON(device->devid == BTRFS_DEV_REPLACE_DEVID);\n\tclear_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state);\n\n\tptr = btrfs_device_uuid(dev_item);\n\tread_extent_buffer(leaf, device->uuid, ptr, BTRFS_UUID_SIZE);\n}\n\nstatic struct btrfs_fs_devices *open_seed_devices(struct btrfs_fs_info *fs_info,\n\t\t\t\t\t\t  u8 *fsid)\n{\n\tstruct btrfs_fs_devices *fs_devices;\n\tint ret;\n\n\tlockdep_assert_held(&uuid_mutex);\n\tASSERT(fsid);\n\n\t \n\tlist_for_each_entry(fs_devices, &fs_info->fs_devices->seed_list, seed_list)\n\t\tif (!memcmp(fs_devices->fsid, fsid, BTRFS_FSID_SIZE))\n\t\t\treturn fs_devices;\n\n\n\tfs_devices = find_fsid(fsid, NULL);\n\tif (!fs_devices) {\n\t\tif (!btrfs_test_opt(fs_info, DEGRADED))\n\t\t\treturn ERR_PTR(-ENOENT);\n\n\t\tfs_devices = alloc_fs_devices(fsid, NULL);\n\t\tif (IS_ERR(fs_devices))\n\t\t\treturn fs_devices;\n\n\t\tfs_devices->seeding = true;\n\t\tfs_devices->opened = 1;\n\t\treturn fs_devices;\n\t}\n\n\t \n\tfs_devices = clone_fs_devices(fs_devices);\n\tif (IS_ERR(fs_devices))\n\t\treturn fs_devices;\n\n\tret = open_fs_devices(fs_devices, BLK_OPEN_READ, fs_info->bdev_holder);\n\tif (ret) {\n\t\tfree_fs_devices(fs_devices);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\tif (!fs_devices->seeding) {\n\t\tclose_fs_devices(fs_devices);\n\t\tfree_fs_devices(fs_devices);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tlist_add(&fs_devices->seed_list, &fs_info->fs_devices->seed_list);\n\n\treturn fs_devices;\n}\n\nstatic int read_one_dev(struct extent_buffer *leaf,\n\t\t\tstruct btrfs_dev_item *dev_item)\n{\n\tBTRFS_DEV_LOOKUP_ARGS(args);\n\tstruct btrfs_fs_info *fs_info = leaf->fs_info;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tstruct btrfs_device *device;\n\tu64 devid;\n\tint ret;\n\tu8 fs_uuid[BTRFS_FSID_SIZE];\n\tu8 dev_uuid[BTRFS_UUID_SIZE];\n\n\tdevid = btrfs_device_id(leaf, dev_item);\n\targs.devid = devid;\n\tread_extent_buffer(leaf, dev_uuid, btrfs_device_uuid(dev_item),\n\t\t\t   BTRFS_UUID_SIZE);\n\tread_extent_buffer(leaf, fs_uuid, btrfs_device_fsid(dev_item),\n\t\t\t   BTRFS_FSID_SIZE);\n\targs.uuid = dev_uuid;\n\targs.fsid = fs_uuid;\n\n\tif (memcmp(fs_uuid, fs_devices->metadata_uuid, BTRFS_FSID_SIZE)) {\n\t\tfs_devices = open_seed_devices(fs_info, fs_uuid);\n\t\tif (IS_ERR(fs_devices))\n\t\t\treturn PTR_ERR(fs_devices);\n\t}\n\n\tdevice = btrfs_find_device(fs_info->fs_devices, &args);\n\tif (!device) {\n\t\tif (!btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\tbtrfs_report_missing_device(fs_info, devid,\n\t\t\t\t\t\t\tdev_uuid, true);\n\t\t\treturn -ENOENT;\n\t\t}\n\n\t\tdevice = add_missing_dev(fs_devices, devid, dev_uuid);\n\t\tif (IS_ERR(device)) {\n\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\"failed to add missing dev %llu: %ld\",\n\t\t\t\tdevid, PTR_ERR(device));\n\t\t\treturn PTR_ERR(device);\n\t\t}\n\t\tbtrfs_report_missing_device(fs_info, devid, dev_uuid, false);\n\t} else {\n\t\tif (!device->bdev) {\n\t\t\tif (!btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\t\tbtrfs_report_missing_device(fs_info,\n\t\t\t\t\t\tdevid, dev_uuid, true);\n\t\t\t\treturn -ENOENT;\n\t\t\t}\n\t\t\tbtrfs_report_missing_device(fs_info, devid,\n\t\t\t\t\t\t\tdev_uuid, false);\n\t\t}\n\n\t\tif (!device->bdev &&\n\t\t    !test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state)) {\n\t\t\t \n\t\t\tdevice->fs_devices->missing_devices++;\n\t\t\tset_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state);\n\t\t}\n\n\t\t \n\t\tif (device->fs_devices != fs_devices) {\n\t\t\tASSERT(test_bit(BTRFS_DEV_STATE_MISSING,\n\t\t\t\t\t\t\t&device->dev_state));\n\n\t\t\tlist_move(&device->dev_list, &fs_devices->devices);\n\t\t\tdevice->fs_devices->num_devices--;\n\t\t\tfs_devices->num_devices++;\n\n\t\t\tdevice->fs_devices->missing_devices--;\n\t\t\tfs_devices->missing_devices++;\n\n\t\t\tdevice->fs_devices = fs_devices;\n\t\t}\n\t}\n\n\tif (device->fs_devices != fs_info->fs_devices) {\n\t\tBUG_ON(test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state));\n\t\tif (device->generation !=\n\t\t    btrfs_device_generation(leaf, dev_item))\n\t\t\treturn -EINVAL;\n\t}\n\n\tfill_device_from_item(leaf, dev_item, device);\n\tif (device->bdev) {\n\t\tu64 max_total_bytes = bdev_nr_bytes(device->bdev);\n\n\t\tif (device->total_bytes > max_total_bytes) {\n\t\t\tbtrfs_err(fs_info,\n\t\t\t\"device total_bytes should be at most %llu but found %llu\",\n\t\t\t\t  max_total_bytes, device->total_bytes);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\tset_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state) &&\n\t   !test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tdevice->fs_devices->total_rw_bytes += device->total_bytes;\n\t\tatomic64_add(device->total_bytes - device->bytes_used,\n\t\t\t\t&fs_info->free_chunk_space);\n\t}\n\tret = 0;\n\treturn ret;\n}\n\nint btrfs_read_sys_array(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_super_block *super_copy = fs_info->super_copy;\n\tstruct extent_buffer *sb;\n\tstruct btrfs_disk_key *disk_key;\n\tstruct btrfs_chunk *chunk;\n\tu8 *array_ptr;\n\tunsigned long sb_array_offset;\n\tint ret = 0;\n\tu32 num_stripes;\n\tu32 array_size;\n\tu32 len = 0;\n\tu32 cur_offset;\n\tu64 type;\n\tstruct btrfs_key key;\n\n\tASSERT(BTRFS_SUPER_INFO_SIZE <= fs_info->nodesize);\n\n\t \n\tsb = alloc_dummy_extent_buffer(fs_info, BTRFS_SUPER_INFO_OFFSET);\n\tif (!sb)\n\t\treturn -ENOMEM;\n\tset_extent_buffer_uptodate(sb);\n\n\twrite_extent_buffer(sb, super_copy, 0, BTRFS_SUPER_INFO_SIZE);\n\tarray_size = btrfs_super_sys_array_size(super_copy);\n\n\tarray_ptr = super_copy->sys_chunk_array;\n\tsb_array_offset = offsetof(struct btrfs_super_block, sys_chunk_array);\n\tcur_offset = 0;\n\n\twhile (cur_offset < array_size) {\n\t\tdisk_key = (struct btrfs_disk_key *)array_ptr;\n\t\tlen = sizeof(*disk_key);\n\t\tif (cur_offset + len > array_size)\n\t\t\tgoto out_short_read;\n\n\t\tbtrfs_disk_key_to_cpu(&key, disk_key);\n\n\t\tarray_ptr += len;\n\t\tsb_array_offset += len;\n\t\tcur_offset += len;\n\n\t\tif (key.type != BTRFS_CHUNK_ITEM_KEY) {\n\t\t\tbtrfs_err(fs_info,\n\t\t\t    \"unexpected item type %u in sys_array at offset %u\",\n\t\t\t\t  (u32)key.type, cur_offset);\n\t\t\tret = -EIO;\n\t\t\tbreak;\n\t\t}\n\n\t\tchunk = (struct btrfs_chunk *)sb_array_offset;\n\t\t \n\t\tlen = btrfs_chunk_item_size(1);\n\t\tif (cur_offset + len > array_size)\n\t\t\tgoto out_short_read;\n\n\t\tnum_stripes = btrfs_chunk_num_stripes(sb, chunk);\n\t\tif (!num_stripes) {\n\t\t\tbtrfs_err(fs_info,\n\t\t\t\"invalid number of stripes %u in sys_array at offset %u\",\n\t\t\t\t  num_stripes, cur_offset);\n\t\t\tret = -EIO;\n\t\t\tbreak;\n\t\t}\n\n\t\ttype = btrfs_chunk_type(sb, chunk);\n\t\tif ((type & BTRFS_BLOCK_GROUP_SYSTEM) == 0) {\n\t\t\tbtrfs_err(fs_info,\n\t\t\t\"invalid chunk type %llu in sys_array at offset %u\",\n\t\t\t\t  type, cur_offset);\n\t\t\tret = -EIO;\n\t\t\tbreak;\n\t\t}\n\n\t\tlen = btrfs_chunk_item_size(num_stripes);\n\t\tif (cur_offset + len > array_size)\n\t\t\tgoto out_short_read;\n\n\t\tret = read_one_chunk(&key, sb, chunk);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tarray_ptr += len;\n\t\tsb_array_offset += len;\n\t\tcur_offset += len;\n\t}\n\tclear_extent_buffer_uptodate(sb);\n\tfree_extent_buffer_stale(sb);\n\treturn ret;\n\nout_short_read:\n\tbtrfs_err(fs_info, \"sys_array too short to read %u bytes at offset %u\",\n\t\t\tlen, cur_offset);\n\tclear_extent_buffer_uptodate(sb);\n\tfree_extent_buffer_stale(sb);\n\treturn -EIO;\n}\n\n \nbool btrfs_check_rw_degradable(struct btrfs_fs_info *fs_info,\n\t\t\t\t\tstruct btrfs_device *failing_dev)\n{\n\tstruct extent_map_tree *map_tree = &fs_info->mapping_tree;\n\tstruct extent_map *em;\n\tu64 next_start = 0;\n\tbool ret = true;\n\n\tread_lock(&map_tree->lock);\n\tem = lookup_extent_mapping(map_tree, 0, (u64)-1);\n\tread_unlock(&map_tree->lock);\n\t \n\tif (!em) {\n\t\tret = false;\n\t\tgoto out;\n\t}\n\twhile (em) {\n\t\tstruct map_lookup *map;\n\t\tint missing = 0;\n\t\tint max_tolerated;\n\t\tint i;\n\n\t\tmap = em->map_lookup;\n\t\tmax_tolerated =\n\t\t\tbtrfs_get_num_tolerated_disk_barrier_failures(\n\t\t\t\t\tmap->type);\n\t\tfor (i = 0; i < map->num_stripes; i++) {\n\t\t\tstruct btrfs_device *dev = map->stripes[i].dev;\n\n\t\t\tif (!dev || !dev->bdev ||\n\t\t\t    test_bit(BTRFS_DEV_STATE_MISSING, &dev->dev_state) ||\n\t\t\t    dev->last_flush_error)\n\t\t\t\tmissing++;\n\t\t\telse if (failing_dev && failing_dev == dev)\n\t\t\t\tmissing++;\n\t\t}\n\t\tif (missing > max_tolerated) {\n\t\t\tif (!failing_dev)\n\t\t\t\tbtrfs_warn(fs_info,\n\t\"chunk %llu missing %d devices, max tolerance is %d for writable mount\",\n\t\t\t\t   em->start, missing, max_tolerated);\n\t\t\tfree_extent_map(em);\n\t\t\tret = false;\n\t\t\tgoto out;\n\t\t}\n\t\tnext_start = extent_map_end(em);\n\t\tfree_extent_map(em);\n\n\t\tread_lock(&map_tree->lock);\n\t\tem = lookup_extent_mapping(map_tree, next_start,\n\t\t\t\t\t   (u64)(-1) - next_start);\n\t\tread_unlock(&map_tree->lock);\n\t}\nout:\n\treturn ret;\n}\n\nstatic void readahead_tree_node_children(struct extent_buffer *node)\n{\n\tint i;\n\tconst int nr_items = btrfs_header_nritems(node);\n\n\tfor (i = 0; i < nr_items; i++)\n\t\tbtrfs_readahead_node_child(node, i);\n}\n\nint btrfs_read_chunk_tree(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_root *root = fs_info->chunk_root;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_key key;\n\tstruct btrfs_key found_key;\n\tint ret;\n\tint slot;\n\tint iter_ret = 0;\n\tu64 total_dev = 0;\n\tu64 last_ra_node = 0;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\t \n\tmutex_lock(&uuid_mutex);\n\n\t \n\tfs_info->fs_devices->total_rw_bytes = 0;\n\n\t \n\tASSERT(!test_bit(BTRFS_FS_OPEN, &fs_info->flags));\n\tpath->skip_locking = 1;\n\n\t \n\tkey.objectid = BTRFS_DEV_ITEMS_OBJECTID;\n\tkey.offset = 0;\n\tkey.type = 0;\n\tbtrfs_for_each_slot(root, &key, &found_key, path, iter_ret) {\n\t\tstruct extent_buffer *node = path->nodes[1];\n\n\t\tleaf = path->nodes[0];\n\t\tslot = path->slots[0];\n\n\t\tif (node) {\n\t\t\tif (last_ra_node != node->start) {\n\t\t\t\treadahead_tree_node_children(node);\n\t\t\t\tlast_ra_node = node->start;\n\t\t\t}\n\t\t}\n\t\tif (found_key.type == BTRFS_DEV_ITEM_KEY) {\n\t\t\tstruct btrfs_dev_item *dev_item;\n\t\t\tdev_item = btrfs_item_ptr(leaf, slot,\n\t\t\t\t\t\t  struct btrfs_dev_item);\n\t\t\tret = read_one_dev(leaf, dev_item);\n\t\t\tif (ret)\n\t\t\t\tgoto error;\n\t\t\ttotal_dev++;\n\t\t} else if (found_key.type == BTRFS_CHUNK_ITEM_KEY) {\n\t\t\tstruct btrfs_chunk *chunk;\n\n\t\t\t \n\t\t\tchunk = btrfs_item_ptr(leaf, slot, struct btrfs_chunk);\n\t\t\tret = read_one_chunk(&found_key, leaf, chunk);\n\t\t\tif (ret)\n\t\t\t\tgoto error;\n\t\t}\n\t}\n\t \n\tif (iter_ret < 0) {\n\t\tret = iter_ret;\n\t\tgoto error;\n\t}\n\n\t \n\tif (total_dev != fs_info->fs_devices->total_devices) {\n\t\tbtrfs_warn(fs_info,\n\"super block num_devices %llu mismatch with DEV_ITEM count %llu, will be repaired on next transaction commit\",\n\t\t\t  btrfs_super_num_devices(fs_info->super_copy),\n\t\t\t  total_dev);\n\t\tfs_info->fs_devices->total_devices = total_dev;\n\t\tbtrfs_set_super_num_devices(fs_info->super_copy, total_dev);\n\t}\n\tif (btrfs_super_total_bytes(fs_info->super_copy) <\n\t    fs_info->fs_devices->total_rw_bytes) {\n\t\tbtrfs_err(fs_info,\n\t\"super_total_bytes %llu mismatch with fs_devices total_rw_bytes %llu\",\n\t\t\t  btrfs_super_total_bytes(fs_info->super_copy),\n\t\t\t  fs_info->fs_devices->total_rw_bytes);\n\t\tret = -EINVAL;\n\t\tgoto error;\n\t}\n\tret = 0;\nerror:\n\tmutex_unlock(&uuid_mutex);\n\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nint btrfs_init_devices_late(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices, *seed_devs;\n\tstruct btrfs_device *device;\n\tint ret = 0;\n\n\tfs_devices->fs_info = fs_info;\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_for_each_entry(device, &fs_devices->devices, dev_list)\n\t\tdevice->fs_info = fs_info;\n\n\tlist_for_each_entry(seed_devs, &fs_devices->seed_list, seed_list) {\n\t\tlist_for_each_entry(device, &seed_devs->devices, dev_list) {\n\t\t\tdevice->fs_info = fs_info;\n\t\t\tret = btrfs_get_dev_zone_info(device, false);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tseed_devs->fs_info = fs_info;\n\t}\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\treturn ret;\n}\n\nstatic u64 btrfs_dev_stats_value(const struct extent_buffer *eb,\n\t\t\t\t const struct btrfs_dev_stats_item *ptr,\n\t\t\t\t int index)\n{\n\tu64 val;\n\n\tread_extent_buffer(eb, &val,\n\t\t\t   offsetof(struct btrfs_dev_stats_item, values) +\n\t\t\t    ((unsigned long)ptr) + (index * sizeof(u64)),\n\t\t\t   sizeof(val));\n\treturn val;\n}\n\nstatic void btrfs_set_dev_stats_value(struct extent_buffer *eb,\n\t\t\t\t      struct btrfs_dev_stats_item *ptr,\n\t\t\t\t      int index, u64 val)\n{\n\twrite_extent_buffer(eb, &val,\n\t\t\t    offsetof(struct btrfs_dev_stats_item, values) +\n\t\t\t     ((unsigned long)ptr) + (index * sizeof(u64)),\n\t\t\t    sizeof(val));\n}\n\nstatic int btrfs_device_init_dev_stats(struct btrfs_device *device,\n\t\t\t\t       struct btrfs_path *path)\n{\n\tstruct btrfs_dev_stats_item *ptr;\n\tstruct extent_buffer *eb;\n\tstruct btrfs_key key;\n\tint item_size;\n\tint i, ret, slot;\n\n\tif (!device->fs_info->dev_root)\n\t\treturn 0;\n\n\tkey.objectid = BTRFS_DEV_STATS_OBJECTID;\n\tkey.type = BTRFS_PERSISTENT_ITEM_KEY;\n\tkey.offset = device->devid;\n\tret = btrfs_search_slot(NULL, device->fs_info->dev_root, &key, path, 0, 0);\n\tif (ret) {\n\t\tfor (i = 0; i < BTRFS_DEV_STAT_VALUES_MAX; i++)\n\t\t\tbtrfs_dev_stat_set(device, i, 0);\n\t\tdevice->dev_stats_valid = 1;\n\t\tbtrfs_release_path(path);\n\t\treturn ret < 0 ? ret : 0;\n\t}\n\tslot = path->slots[0];\n\teb = path->nodes[0];\n\titem_size = btrfs_item_size(eb, slot);\n\n\tptr = btrfs_item_ptr(eb, slot, struct btrfs_dev_stats_item);\n\n\tfor (i = 0; i < BTRFS_DEV_STAT_VALUES_MAX; i++) {\n\t\tif (item_size >= (1 + i) * sizeof(__le64))\n\t\t\tbtrfs_dev_stat_set(device, i,\n\t\t\t\t\t   btrfs_dev_stats_value(eb, ptr, i));\n\t\telse\n\t\t\tbtrfs_dev_stat_set(device, i, 0);\n\t}\n\n\tdevice->dev_stats_valid = 1;\n\tbtrfs_dev_stat_print_on_load(device);\n\tbtrfs_release_path(path);\n\n\treturn 0;\n}\n\nint btrfs_init_dev_stats(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices, *seed_devs;\n\tstruct btrfs_device *device;\n\tstruct btrfs_path *path = NULL;\n\tint ret = 0;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_for_each_entry(device, &fs_devices->devices, dev_list) {\n\t\tret = btrfs_device_init_dev_stats(device, path);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\tlist_for_each_entry(seed_devs, &fs_devices->seed_list, seed_list) {\n\t\tlist_for_each_entry(device, &seed_devs->devices, dev_list) {\n\t\t\tret = btrfs_device_init_dev_stats(device, path);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t}\n\t}\nout:\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nstatic int update_dev_stat_item(struct btrfs_trans_handle *trans,\n\t\t\t\tstruct btrfs_device *device)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_root *dev_root = fs_info->dev_root;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key key;\n\tstruct extent_buffer *eb;\n\tstruct btrfs_dev_stats_item *ptr;\n\tint ret;\n\tint i;\n\n\tkey.objectid = BTRFS_DEV_STATS_OBJECTID;\n\tkey.type = BTRFS_PERSISTENT_ITEM_KEY;\n\tkey.offset = device->devid;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\tret = btrfs_search_slot(trans, dev_root, &key, path, -1, 1);\n\tif (ret < 0) {\n\t\tbtrfs_warn_in_rcu(fs_info,\n\t\t\t\"error %d while searching for dev_stats item for device %s\",\n\t\t\t\t  ret, btrfs_dev_name(device));\n\t\tgoto out;\n\t}\n\n\tif (ret == 0 &&\n\t    btrfs_item_size(path->nodes[0], path->slots[0]) < sizeof(*ptr)) {\n\t\t \n\t\tret = btrfs_del_item(trans, dev_root, path);\n\t\tif (ret != 0) {\n\t\t\tbtrfs_warn_in_rcu(fs_info,\n\t\t\t\t\"delete too small dev_stats item for device %s failed %d\",\n\t\t\t\t\t  btrfs_dev_name(device), ret);\n\t\t\tgoto out;\n\t\t}\n\t\tret = 1;\n\t}\n\n\tif (ret == 1) {\n\t\t \n\t\tbtrfs_release_path(path);\n\t\tret = btrfs_insert_empty_item(trans, dev_root, path,\n\t\t\t\t\t      &key, sizeof(*ptr));\n\t\tif (ret < 0) {\n\t\t\tbtrfs_warn_in_rcu(fs_info,\n\t\t\t\t\"insert dev_stats item for device %s failed %d\",\n\t\t\t\tbtrfs_dev_name(device), ret);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\teb = path->nodes[0];\n\tptr = btrfs_item_ptr(eb, path->slots[0], struct btrfs_dev_stats_item);\n\tfor (i = 0; i < BTRFS_DEV_STAT_VALUES_MAX; i++)\n\t\tbtrfs_set_dev_stats_value(eb, ptr, i,\n\t\t\t\t\t  btrfs_dev_stat_read(device, i));\n\tbtrfs_mark_buffer_dirty(trans, eb);\n\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n \nint btrfs_run_dev_stats(struct btrfs_trans_handle *trans)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tstruct btrfs_device *device;\n\tint stats_cnt;\n\tint ret = 0;\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_for_each_entry(device, &fs_devices->devices, dev_list) {\n\t\tstats_cnt = atomic_read(&device->dev_stats_ccnt);\n\t\tif (!device->dev_stats_valid || stats_cnt == 0)\n\t\t\tcontinue;\n\n\n\t\t \n\t\tsmp_rmb();\n\n\t\tret = update_dev_stat_item(trans, device);\n\t\tif (!ret)\n\t\t\tatomic_sub(stats_cnt, &device->dev_stats_ccnt);\n\t}\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\treturn ret;\n}\n\nvoid btrfs_dev_stat_inc_and_print(struct btrfs_device *dev, int index)\n{\n\tbtrfs_dev_stat_inc(dev, index);\n\n\tif (!dev->dev_stats_valid)\n\t\treturn;\n\tbtrfs_err_rl_in_rcu(dev->fs_info,\n\t\t\"bdev %s errs: wr %u, rd %u, flush %u, corrupt %u, gen %u\",\n\t\t\t   btrfs_dev_name(dev),\n\t\t\t   btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_WRITE_ERRS),\n\t\t\t   btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_READ_ERRS),\n\t\t\t   btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_FLUSH_ERRS),\n\t\t\t   btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_CORRUPTION_ERRS),\n\t\t\t   btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_GENERATION_ERRS));\n}\n\nstatic void btrfs_dev_stat_print_on_load(struct btrfs_device *dev)\n{\n\tint i;\n\n\tfor (i = 0; i < BTRFS_DEV_STAT_VALUES_MAX; i++)\n\t\tif (btrfs_dev_stat_read(dev, i) != 0)\n\t\t\tbreak;\n\tif (i == BTRFS_DEV_STAT_VALUES_MAX)\n\t\treturn;  \n\n\tbtrfs_info_in_rcu(dev->fs_info,\n\t\t\"bdev %s errs: wr %u, rd %u, flush %u, corrupt %u, gen %u\",\n\t       btrfs_dev_name(dev),\n\t       btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_WRITE_ERRS),\n\t       btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_READ_ERRS),\n\t       btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_FLUSH_ERRS),\n\t       btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_CORRUPTION_ERRS),\n\t       btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_GENERATION_ERRS));\n}\n\nint btrfs_get_dev_stats(struct btrfs_fs_info *fs_info,\n\t\t\tstruct btrfs_ioctl_get_dev_stats *stats)\n{\n\tBTRFS_DEV_LOOKUP_ARGS(args);\n\tstruct btrfs_device *dev;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tint i;\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\targs.devid = stats->devid;\n\tdev = btrfs_find_device(fs_info->fs_devices, &args);\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\tif (!dev) {\n\t\tbtrfs_warn(fs_info, \"get dev_stats failed, device not found\");\n\t\treturn -ENODEV;\n\t} else if (!dev->dev_stats_valid) {\n\t\tbtrfs_warn(fs_info, \"get dev_stats failed, not yet valid\");\n\t\treturn -ENODEV;\n\t} else if (stats->flags & BTRFS_DEV_STATS_RESET) {\n\t\tfor (i = 0; i < BTRFS_DEV_STAT_VALUES_MAX; i++) {\n\t\t\tif (stats->nr_items > i)\n\t\t\t\tstats->values[i] =\n\t\t\t\t\tbtrfs_dev_stat_read_and_reset(dev, i);\n\t\t\telse\n\t\t\t\tbtrfs_dev_stat_set(dev, i, 0);\n\t\t}\n\t\tbtrfs_info(fs_info, \"device stats zeroed by %s (%d)\",\n\t\t\t   current->comm, task_pid_nr(current));\n\t} else {\n\t\tfor (i = 0; i < BTRFS_DEV_STAT_VALUES_MAX; i++)\n\t\t\tif (stats->nr_items > i)\n\t\t\t\tstats->values[i] = btrfs_dev_stat_read(dev, i);\n\t}\n\tif (stats->nr_items > BTRFS_DEV_STAT_VALUES_MAX)\n\t\tstats->nr_items = BTRFS_DEV_STAT_VALUES_MAX;\n\treturn 0;\n}\n\n \nvoid btrfs_commit_device_sizes(struct btrfs_transaction *trans)\n{\n\tstruct btrfs_device *curr, *next;\n\n\tASSERT(trans->state == TRANS_STATE_COMMIT_DOING);\n\n\tif (list_empty(&trans->dev_update_list))\n\t\treturn;\n\n\t \n\tmutex_lock(&trans->fs_info->chunk_mutex);\n\tlist_for_each_entry_safe(curr, next, &trans->dev_update_list,\n\t\t\t\t post_commit_list) {\n\t\tlist_del_init(&curr->post_commit_list);\n\t\tcurr->commit_total_bytes = curr->disk_total_bytes;\n\t\tcurr->commit_bytes_used = curr->bytes_used;\n\t}\n\tmutex_unlock(&trans->fs_info->chunk_mutex);\n}\n\n \nint btrfs_bg_type_to_factor(u64 flags)\n{\n\tconst int index = btrfs_bg_flags_to_raid_index(flags);\n\n\treturn btrfs_raid_array[index].ncopies;\n}\n\n\n\nstatic int verify_one_dev_extent(struct btrfs_fs_info *fs_info,\n\t\t\t\t u64 chunk_offset, u64 devid,\n\t\t\t\t u64 physical_offset, u64 physical_len)\n{\n\tstruct btrfs_dev_lookup_args args = { .devid = devid };\n\tstruct extent_map_tree *em_tree = &fs_info->mapping_tree;\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tstruct btrfs_device *dev;\n\tu64 stripe_len;\n\tbool found = false;\n\tint ret = 0;\n\tint i;\n\n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, chunk_offset, 1);\n\tread_unlock(&em_tree->lock);\n\n\tif (!em) {\n\t\tbtrfs_err(fs_info,\n\"dev extent physical offset %llu on devid %llu doesn't have corresponding chunk\",\n\t\t\t  physical_offset, devid);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tmap = em->map_lookup;\n\tstripe_len = btrfs_calc_stripe_length(em);\n\tif (physical_len != stripe_len) {\n\t\tbtrfs_err(fs_info,\n\"dev extent physical offset %llu on devid %llu length doesn't match chunk %llu, have %llu expect %llu\",\n\t\t\t  physical_offset, devid, em->start, physical_len,\n\t\t\t  stripe_len);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\t \n\tif (physical_offset < BTRFS_DEVICE_RANGE_RESERVED)\n\t\tbtrfs_warn(fs_info,\n\t\t\"devid %llu physical %llu len %llu inside the reserved space\",\n\t\t\t   devid, physical_offset, physical_len);\n\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tif (map->stripes[i].dev->devid == devid &&\n\t\t    map->stripes[i].physical == physical_offset) {\n\t\t\tfound = true;\n\t\t\tif (map->verified_stripes >= map->num_stripes) {\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\"too many dev extents for chunk %llu found\",\n\t\t\t\t\t  em->start);\n\t\t\t\tret = -EUCLEAN;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tmap->verified_stripes++;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!found) {\n\t\tbtrfs_err(fs_info,\n\t\"dev extent physical offset %llu devid %llu has no corresponding chunk\",\n\t\t\tphysical_offset, devid);\n\t\tret = -EUCLEAN;\n\t}\n\n\t \n\tdev = btrfs_find_device(fs_info->fs_devices, &args);\n\tif (!dev) {\n\t\tbtrfs_err(fs_info, \"failed to find devid %llu\", devid);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tif (physical_offset + physical_len > dev->disk_total_bytes) {\n\t\tbtrfs_err(fs_info,\n\"dev extent devid %llu physical offset %llu len %llu is beyond device boundary %llu\",\n\t\t\t  devid, physical_offset, physical_len,\n\t\t\t  dev->disk_total_bytes);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tif (dev->zone_info) {\n\t\tu64 zone_size = dev->zone_info->zone_size;\n\n\t\tif (!IS_ALIGNED(physical_offset, zone_size) ||\n\t\t    !IS_ALIGNED(physical_len, zone_size)) {\n\t\t\tbtrfs_err(fs_info,\n\"zoned: dev extent devid %llu physical offset %llu len %llu is not aligned to device zone\",\n\t\t\t\t  devid, physical_offset, physical_len);\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\t}\n\nout:\n\tfree_extent_map(em);\n\treturn ret;\n}\n\nstatic int verify_chunk_dev_extent_mapping(struct btrfs_fs_info *fs_info)\n{\n\tstruct extent_map_tree *em_tree = &fs_info->mapping_tree;\n\tstruct extent_map *em;\n\tstruct rb_node *node;\n\tint ret = 0;\n\n\tread_lock(&em_tree->lock);\n\tfor (node = rb_first_cached(&em_tree->map); node; node = rb_next(node)) {\n\t\tem = rb_entry(node, struct extent_map, rb_node);\n\t\tif (em->map_lookup->num_stripes !=\n\t\t    em->map_lookup->verified_stripes) {\n\t\t\tbtrfs_err(fs_info,\n\t\t\t\"chunk %llu has missing dev extent, have %d expect %d\",\n\t\t\t\t  em->start, em->map_lookup->verified_stripes,\n\t\t\t\t  em->map_lookup->num_stripes);\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\t}\nout:\n\tread_unlock(&em_tree->lock);\n\treturn ret;\n}\n\n \nint btrfs_verify_dev_extents(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_path *path;\n\tstruct btrfs_root *root = fs_info->dev_root;\n\tstruct btrfs_key key;\n\tu64 prev_devid = 0;\n\tu64 prev_dev_ext_end = 0;\n\tint ret = 0;\n\n\t \n\tif (btrfs_test_opt(fs_info, IGNOREBADROOTS))\n\t\treturn 0;\n\n\tkey.objectid = 1;\n\tkey.type = BTRFS_DEV_EXTENT_KEY;\n\tkey.offset = 0;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tpath->reada = READA_FORWARD;\n\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tif (path->slots[0] >= btrfs_header_nritems(path->nodes[0])) {\n\t\tret = btrfs_next_leaf(root, path);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\t \n\t\tif (ret > 0) {\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\t}\n\twhile (1) {\n\t\tstruct extent_buffer *leaf = path->nodes[0];\n\t\tstruct btrfs_dev_extent *dext;\n\t\tint slot = path->slots[0];\n\t\tu64 chunk_offset;\n\t\tu64 physical_offset;\n\t\tu64 physical_len;\n\t\tu64 devid;\n\n\t\tbtrfs_item_key_to_cpu(leaf, &key, slot);\n\t\tif (key.type != BTRFS_DEV_EXTENT_KEY)\n\t\t\tbreak;\n\t\tdevid = key.objectid;\n\t\tphysical_offset = key.offset;\n\n\t\tdext = btrfs_item_ptr(leaf, slot, struct btrfs_dev_extent);\n\t\tchunk_offset = btrfs_dev_extent_chunk_offset(leaf, dext);\n\t\tphysical_len = btrfs_dev_extent_length(leaf, dext);\n\n\t\t \n\t\tif (devid == prev_devid && physical_offset < prev_dev_ext_end) {\n\t\t\tbtrfs_err(fs_info,\n\"dev extent devid %llu physical offset %llu overlap with previous dev extent end %llu\",\n\t\t\t\t  devid, physical_offset, prev_dev_ext_end);\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\n\t\tret = verify_one_dev_extent(fs_info, chunk_offset, devid,\n\t\t\t\t\t    physical_offset, physical_len);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\tprev_devid = devid;\n\t\tprev_dev_ext_end = physical_offset + physical_len;\n\n\t\tret = btrfs_next_item(root, path);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\tif (ret > 0) {\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\tret = verify_chunk_dev_extent_mapping(fs_info);\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n \nbool btrfs_pinned_by_swapfile(struct btrfs_fs_info *fs_info, void *ptr)\n{\n\tstruct btrfs_swapfile_pin *sp;\n\tstruct rb_node *node;\n\n\tspin_lock(&fs_info->swapfile_pins_lock);\n\tnode = fs_info->swapfile_pins.rb_node;\n\twhile (node) {\n\t\tsp = rb_entry(node, struct btrfs_swapfile_pin, node);\n\t\tif (ptr < sp->ptr)\n\t\t\tnode = node->rb_left;\n\t\telse if (ptr > sp->ptr)\n\t\t\tnode = node->rb_right;\n\t\telse\n\t\t\tbreak;\n\t}\n\tspin_unlock(&fs_info->swapfile_pins_lock);\n\treturn node != NULL;\n}\n\nstatic int relocating_repair_kthread(void *data)\n{\n\tstruct btrfs_block_group *cache = data;\n\tstruct btrfs_fs_info *fs_info = cache->fs_info;\n\tu64 target;\n\tint ret = 0;\n\n\ttarget = cache->start;\n\tbtrfs_put_block_group(cache);\n\n\tsb_start_write(fs_info->sb);\n\tif (!btrfs_exclop_start(fs_info, BTRFS_EXCLOP_BALANCE)) {\n\t\tbtrfs_info(fs_info,\n\t\t\t   \"zoned: skip relocating block group %llu to repair: EBUSY\",\n\t\t\t   target);\n\t\tsb_end_write(fs_info->sb);\n\t\treturn -EBUSY;\n\t}\n\n\tmutex_lock(&fs_info->reclaim_bgs_lock);\n\n\t \n\tcache = btrfs_lookup_block_group(fs_info, target);\n\tif (!cache)\n\t\tgoto out;\n\n\tif (!test_bit(BLOCK_GROUP_FLAG_RELOCATING_REPAIR, &cache->runtime_flags))\n\t\tgoto out;\n\n\tret = btrfs_may_alloc_data_chunk(fs_info, target);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tbtrfs_info(fs_info,\n\t\t   \"zoned: relocating block group %llu to repair IO failure\",\n\t\t   target);\n\tret = btrfs_relocate_chunk(fs_info, target);\n\nout:\n\tif (cache)\n\t\tbtrfs_put_block_group(cache);\n\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\tbtrfs_exclop_finish(fs_info);\n\tsb_end_write(fs_info->sb);\n\n\treturn ret;\n}\n\nbool btrfs_repair_one_zone(struct btrfs_fs_info *fs_info, u64 logical)\n{\n\tstruct btrfs_block_group *cache;\n\n\tif (!btrfs_is_zoned(fs_info))\n\t\treturn false;\n\n\t \n\tif (btrfs_test_opt(fs_info, DEGRADED))\n\t\treturn true;\n\n\tcache = btrfs_lookup_block_group(fs_info, logical);\n\tif (!cache)\n\t\treturn true;\n\n\tif (test_and_set_bit(BLOCK_GROUP_FLAG_RELOCATING_REPAIR, &cache->runtime_flags)) {\n\t\tbtrfs_put_block_group(cache);\n\t\treturn true;\n\t}\n\n\tkthread_run(relocating_repair_kthread, cache,\n\t\t    \"btrfs-relocating-repair\");\n\n\treturn true;\n}\n\nstatic void map_raid56_repair_block(struct btrfs_io_context *bioc,\n\t\t\t\t    struct btrfs_io_stripe *smap,\n\t\t\t\t    u64 logical)\n{\n\tint data_stripes = nr_bioc_data_stripes(bioc);\n\tint i;\n\n\tfor (i = 0; i < data_stripes; i++) {\n\t\tu64 stripe_start = bioc->full_stripe_logical +\n\t\t\t\t   btrfs_stripe_nr_to_offset(i);\n\n\t\tif (logical >= stripe_start &&\n\t\t    logical < stripe_start + BTRFS_STRIPE_LEN)\n\t\t\tbreak;\n\t}\n\tASSERT(i < data_stripes);\n\tsmap->dev = bioc->stripes[i].dev;\n\tsmap->physical = bioc->stripes[i].physical +\n\t\t\t((logical - bioc->full_stripe_logical) &\n\t\t\t BTRFS_STRIPE_LEN_MASK);\n}\n\n \nint btrfs_map_repair_block(struct btrfs_fs_info *fs_info,\n\t\t\t   struct btrfs_io_stripe *smap, u64 logical,\n\t\t\t   u32 length, int mirror_num)\n{\n\tstruct btrfs_io_context *bioc = NULL;\n\tu64 map_length = length;\n\tint mirror_ret = mirror_num;\n\tint ret;\n\n\tASSERT(mirror_num > 0);\n\n\tret = btrfs_map_block(fs_info, BTRFS_MAP_WRITE, logical, &map_length,\n\t\t\t      &bioc, smap, &mirror_ret, true);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t \n\tASSERT(map_length >= length);\n\n\t \n\tif (!bioc)\n\t\tgoto out;\n\n\t \n\tif (bioc->map_type & BTRFS_BLOCK_GROUP_RAID56_MASK) {\n\t\tmap_raid56_repair_block(bioc, smap, logical);\n\t\tgoto out;\n\t}\n\n\tASSERT(mirror_num <= bioc->num_stripes);\n\tsmap->dev = bioc->stripes[mirror_num - 1].dev;\n\tsmap->physical = bioc->stripes[mirror_num - 1].physical;\nout:\n\tbtrfs_put_bioc(bioc);\n\tASSERT(smap->dev);\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}