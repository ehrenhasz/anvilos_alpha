{
  "module_name": "zoned.c",
  "hash_id": "86e4fb361fc43577ffe1fdd0bfa453a3381ce5e45ac311f3b5806bf29a5be174",
  "original_prompt": "Ingested from linux-6.6.14/fs/btrfs/zoned.c",
  "human_readable_source": "\n\n#include <linux/bitops.h>\n#include <linux/slab.h>\n#include <linux/blkdev.h>\n#include <linux/sched/mm.h>\n#include <linux/atomic.h>\n#include <linux/vmalloc.h>\n#include \"ctree.h\"\n#include \"volumes.h\"\n#include \"zoned.h\"\n#include \"rcu-string.h\"\n#include \"disk-io.h\"\n#include \"block-group.h\"\n#include \"transaction.h\"\n#include \"dev-replace.h\"\n#include \"space-info.h\"\n#include \"super.h\"\n#include \"fs.h\"\n#include \"accessors.h\"\n#include \"bio.h\"\n\n \n#define BTRFS_REPORT_NR_ZONES   4096\n \n#define WP_MISSING_DEV ((u64)-1)\n \n#define WP_CONVENTIONAL ((u64)-2)\n\n \n#define BTRFS_SB_LOG_PRIMARY_OFFSET\t(0ULL)\n#define BTRFS_SB_LOG_FIRST_OFFSET\t(512ULL * SZ_1G)\n#define BTRFS_SB_LOG_SECOND_OFFSET\t(4096ULL * SZ_1G)\n\n#define BTRFS_SB_LOG_FIRST_SHIFT\tconst_ilog2(BTRFS_SB_LOG_FIRST_OFFSET)\n#define BTRFS_SB_LOG_SECOND_SHIFT\tconst_ilog2(BTRFS_SB_LOG_SECOND_OFFSET)\n\n \n#define BTRFS_NR_SB_LOG_ZONES 2\n\n \n#define BTRFS_MIN_ACTIVE_ZONES\t\t(BTRFS_SUPER_MIRROR_MAX + 5)\n\n \n#define BTRFS_MAX_ZONE_SIZE\t\tSZ_8G\n#define BTRFS_MIN_ZONE_SIZE\t\tSZ_4M\n\n#define SUPER_INFO_SECTORS\t((u64)BTRFS_SUPER_INFO_SIZE >> SECTOR_SHIFT)\n\nstatic void wait_eb_writebacks(struct btrfs_block_group *block_group);\nstatic int do_zone_finish(struct btrfs_block_group *block_group, bool fully_written);\n\nstatic inline bool sb_zone_is_full(const struct blk_zone *zone)\n{\n\treturn (zone->cond == BLK_ZONE_COND_FULL) ||\n\t\t(zone->wp + SUPER_INFO_SECTORS > zone->start + zone->capacity);\n}\n\nstatic int copy_zone_info_cb(struct blk_zone *zone, unsigned int idx, void *data)\n{\n\tstruct blk_zone *zones = data;\n\n\tmemcpy(&zones[idx], zone, sizeof(*zone));\n\n\treturn 0;\n}\n\nstatic int sb_write_pointer(struct block_device *bdev, struct blk_zone *zones,\n\t\t\t    u64 *wp_ret)\n{\n\tbool empty[BTRFS_NR_SB_LOG_ZONES];\n\tbool full[BTRFS_NR_SB_LOG_ZONES];\n\tsector_t sector;\n\tint i;\n\n\tfor (i = 0; i < BTRFS_NR_SB_LOG_ZONES; i++) {\n\t\tASSERT(zones[i].type != BLK_ZONE_TYPE_CONVENTIONAL);\n\t\tempty[i] = (zones[i].cond == BLK_ZONE_COND_EMPTY);\n\t\tfull[i] = sb_zone_is_full(&zones[i]);\n\t}\n\n\t \n\n\tif (empty[0] && empty[1]) {\n\t\t \n\t\t*wp_ret = zones[0].start << SECTOR_SHIFT;\n\t\treturn -ENOENT;\n\t} else if (full[0] && full[1]) {\n\t\t \n\t\tstruct address_space *mapping = bdev->bd_inode->i_mapping;\n\t\tstruct page *page[BTRFS_NR_SB_LOG_ZONES];\n\t\tstruct btrfs_super_block *super[BTRFS_NR_SB_LOG_ZONES];\n\t\tint i;\n\n\t\tfor (i = 0; i < BTRFS_NR_SB_LOG_ZONES; i++) {\n\t\t\tu64 zone_end = (zones[i].start + zones[i].capacity) << SECTOR_SHIFT;\n\t\t\tu64 bytenr = ALIGN_DOWN(zone_end, BTRFS_SUPER_INFO_SIZE) -\n\t\t\t\t\t\tBTRFS_SUPER_INFO_SIZE;\n\n\t\t\tpage[i] = read_cache_page_gfp(mapping,\n\t\t\t\t\tbytenr >> PAGE_SHIFT, GFP_NOFS);\n\t\t\tif (IS_ERR(page[i])) {\n\t\t\t\tif (i == 1)\n\t\t\t\t\tbtrfs_release_disk_super(super[0]);\n\t\t\t\treturn PTR_ERR(page[i]);\n\t\t\t}\n\t\t\tsuper[i] = page_address(page[i]);\n\t\t}\n\n\t\tif (btrfs_super_generation(super[0]) >\n\t\t    btrfs_super_generation(super[1]))\n\t\t\tsector = zones[1].start;\n\t\telse\n\t\t\tsector = zones[0].start;\n\n\t\tfor (i = 0; i < BTRFS_NR_SB_LOG_ZONES; i++)\n\t\t\tbtrfs_release_disk_super(super[i]);\n\t} else if (!full[0] && (empty[1] || full[1])) {\n\t\tsector = zones[0].wp;\n\t} else if (full[0]) {\n\t\tsector = zones[1].wp;\n\t} else {\n\t\treturn -EUCLEAN;\n\t}\n\t*wp_ret = sector << SECTOR_SHIFT;\n\treturn 0;\n}\n\n \nstatic inline u32 sb_zone_number(int shift, int mirror)\n{\n\tu64 zone = U64_MAX;\n\n\tASSERT(mirror < BTRFS_SUPER_MIRROR_MAX);\n\tswitch (mirror) {\n\tcase 0: zone = 0; break;\n\tcase 1: zone = 1ULL << (BTRFS_SB_LOG_FIRST_SHIFT - shift); break;\n\tcase 2: zone = 1ULL << (BTRFS_SB_LOG_SECOND_SHIFT - shift); break;\n\t}\n\n\tASSERT(zone <= U32_MAX);\n\n\treturn (u32)zone;\n}\n\nstatic inline sector_t zone_start_sector(u32 zone_number,\n\t\t\t\t\t struct block_device *bdev)\n{\n\treturn (sector_t)zone_number << ilog2(bdev_zone_sectors(bdev));\n}\n\nstatic inline u64 zone_start_physical(u32 zone_number,\n\t\t\t\t      struct btrfs_zoned_device_info *zone_info)\n{\n\treturn (u64)zone_number << zone_info->zone_size_shift;\n}\n\n \nstatic int emulate_report_zones(struct btrfs_device *device, u64 pos,\n\t\t\t\tstruct blk_zone *zones, unsigned int nr_zones)\n{\n\tconst sector_t zone_sectors = device->fs_info->zone_size >> SECTOR_SHIFT;\n\tsector_t bdev_size = bdev_nr_sectors(device->bdev);\n\tunsigned int i;\n\n\tpos >>= SECTOR_SHIFT;\n\tfor (i = 0; i < nr_zones; i++) {\n\t\tzones[i].start = i * zone_sectors + pos;\n\t\tzones[i].len = zone_sectors;\n\t\tzones[i].capacity = zone_sectors;\n\t\tzones[i].wp = zones[i].start + zone_sectors;\n\t\tzones[i].type = BLK_ZONE_TYPE_CONVENTIONAL;\n\t\tzones[i].cond = BLK_ZONE_COND_NOT_WP;\n\n\t\tif (zones[i].wp >= bdev_size) {\n\t\t\ti++;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn i;\n}\n\nstatic int btrfs_get_dev_zones(struct btrfs_device *device, u64 pos,\n\t\t\t       struct blk_zone *zones, unsigned int *nr_zones)\n{\n\tstruct btrfs_zoned_device_info *zinfo = device->zone_info;\n\tint ret;\n\n\tif (!*nr_zones)\n\t\treturn 0;\n\n\tif (!bdev_is_zoned(device->bdev)) {\n\t\tret = emulate_report_zones(device, pos, zones, *nr_zones);\n\t\t*nr_zones = ret;\n\t\treturn 0;\n\t}\n\n\t \n\tif (zinfo->zone_cache) {\n\t\tunsigned int i;\n\t\tu32 zno;\n\n\t\tASSERT(IS_ALIGNED(pos, zinfo->zone_size));\n\t\tzno = pos >> zinfo->zone_size_shift;\n\t\t \n\t\t*nr_zones = min_t(u32, *nr_zones, zinfo->nr_zones - zno);\n\n\t\tfor (i = 0; i < *nr_zones; i++) {\n\t\t\tstruct blk_zone *zone_info;\n\n\t\t\tzone_info = &zinfo->zone_cache[zno + i];\n\t\t\tif (!zone_info->len)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (i == *nr_zones) {\n\t\t\t \n\t\t\tmemcpy(zones, zinfo->zone_cache + zno,\n\t\t\t       sizeof(*zinfo->zone_cache) * *nr_zones);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tret = blkdev_report_zones(device->bdev, pos >> SECTOR_SHIFT, *nr_zones,\n\t\t\t\t  copy_zone_info_cb, zones);\n\tif (ret < 0) {\n\t\tbtrfs_err_in_rcu(device->fs_info,\n\t\t\t\t \"zoned: failed to read zone %llu on %s (devid %llu)\",\n\t\t\t\t pos, rcu_str_deref(device->name),\n\t\t\t\t device->devid);\n\t\treturn ret;\n\t}\n\t*nr_zones = ret;\n\tif (!ret)\n\t\treturn -EIO;\n\n\t \n\tif (zinfo->zone_cache) {\n\t\tu32 zno = pos >> zinfo->zone_size_shift;\n\n\t\tmemcpy(zinfo->zone_cache + zno, zones,\n\t\t       sizeof(*zinfo->zone_cache) * *nr_zones);\n\t}\n\n\treturn 0;\n}\n\n \nstatic int calculate_emulated_zone_size(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_path *path;\n\tstruct btrfs_root *root = fs_info->dev_root;\n\tstruct btrfs_key key;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_dev_extent *dext;\n\tint ret = 0;\n\n\tkey.objectid = 1;\n\tkey.type = BTRFS_DEV_EXTENT_KEY;\n\tkey.offset = 0;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tif (path->slots[0] >= btrfs_header_nritems(path->nodes[0])) {\n\t\tret = btrfs_next_leaf(root, path);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\t \n\t\tif (ret > 0) {\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tleaf = path->nodes[0];\n\tdext = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_dev_extent);\n\tfs_info->zone_size = btrfs_dev_extent_length(leaf, dext);\n\tret = 0;\n\nout:\n\tbtrfs_free_path(path);\n\n\treturn ret;\n}\n\nint btrfs_get_dev_zone_info_all_devices(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tstruct btrfs_device *device;\n\tint ret = 0;\n\n\t \n\tif (!btrfs_fs_incompat(fs_info, ZONED))\n\t\treturn 0;\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_for_each_entry(device, &fs_devices->devices, dev_list) {\n\t\t \n\t\tif (!device->bdev)\n\t\t\tcontinue;\n\n\t\tret = btrfs_get_dev_zone_info(device, true);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\treturn ret;\n}\n\nint btrfs_get_dev_zone_info(struct btrfs_device *device, bool populate_cache)\n{\n\tstruct btrfs_fs_info *fs_info = device->fs_info;\n\tstruct btrfs_zoned_device_info *zone_info = NULL;\n\tstruct block_device *bdev = device->bdev;\n\tunsigned int max_active_zones;\n\tunsigned int nactive;\n\tsector_t nr_sectors;\n\tsector_t sector = 0;\n\tstruct blk_zone *zones = NULL;\n\tunsigned int i, nreported = 0, nr_zones;\n\tsector_t zone_sectors;\n\tchar *model, *emulated;\n\tint ret;\n\n\t \n\tif (!btrfs_fs_incompat(fs_info, ZONED))\n\t\treturn 0;\n\n\tif (device->zone_info)\n\t\treturn 0;\n\n\tzone_info = kzalloc(sizeof(*zone_info), GFP_KERNEL);\n\tif (!zone_info)\n\t\treturn -ENOMEM;\n\n\tdevice->zone_info = zone_info;\n\n\tif (!bdev_is_zoned(bdev)) {\n\t\tif (!fs_info->zone_size) {\n\t\t\tret = calculate_emulated_zone_size(fs_info);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t}\n\n\t\tASSERT(fs_info->zone_size);\n\t\tzone_sectors = fs_info->zone_size >> SECTOR_SHIFT;\n\t} else {\n\t\tzone_sectors = bdev_zone_sectors(bdev);\n\t}\n\n\tASSERT(is_power_of_two_u64(zone_sectors));\n\tzone_info->zone_size = zone_sectors << SECTOR_SHIFT;\n\n\t \n\tif (zone_info->zone_size > BTRFS_MAX_ZONE_SIZE) {\n\t\tbtrfs_err_in_rcu(fs_info,\n\t\t\"zoned: %s: zone size %llu larger than supported maximum %llu\",\n\t\t\t\t rcu_str_deref(device->name),\n\t\t\t\t zone_info->zone_size, BTRFS_MAX_ZONE_SIZE);\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t} else if (zone_info->zone_size < BTRFS_MIN_ZONE_SIZE) {\n\t\tbtrfs_err_in_rcu(fs_info,\n\t\t\"zoned: %s: zone size %llu smaller than supported minimum %u\",\n\t\t\t\t rcu_str_deref(device->name),\n\t\t\t\t zone_info->zone_size, BTRFS_MIN_ZONE_SIZE);\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tnr_sectors = bdev_nr_sectors(bdev);\n\tzone_info->zone_size_shift = ilog2(zone_info->zone_size);\n\tzone_info->nr_zones = nr_sectors >> ilog2(zone_sectors);\n\tif (!IS_ALIGNED(nr_sectors, zone_sectors))\n\t\tzone_info->nr_zones++;\n\n\tmax_active_zones = bdev_max_active_zones(bdev);\n\tif (max_active_zones && max_active_zones < BTRFS_MIN_ACTIVE_ZONES) {\n\t\tbtrfs_err_in_rcu(fs_info,\n\"zoned: %s: max active zones %u is too small, need at least %u active zones\",\n\t\t\t\t rcu_str_deref(device->name), max_active_zones,\n\t\t\t\t BTRFS_MIN_ACTIVE_ZONES);\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\tzone_info->max_active_zones = max_active_zones;\n\n\tzone_info->seq_zones = bitmap_zalloc(zone_info->nr_zones, GFP_KERNEL);\n\tif (!zone_info->seq_zones) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tzone_info->empty_zones = bitmap_zalloc(zone_info->nr_zones, GFP_KERNEL);\n\tif (!zone_info->empty_zones) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tzone_info->active_zones = bitmap_zalloc(zone_info->nr_zones, GFP_KERNEL);\n\tif (!zone_info->active_zones) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tzones = kvcalloc(BTRFS_REPORT_NR_ZONES, sizeof(struct blk_zone), GFP_KERNEL);\n\tif (!zones) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\t \n\tif (populate_cache && bdev_is_zoned(device->bdev)) {\n\t\tzone_info->zone_cache = vcalloc(zone_info->nr_zones,\n\t\t\t\t\t\tsizeof(struct blk_zone));\n\t\tif (!zone_info->zone_cache) {\n\t\t\tbtrfs_err_in_rcu(device->fs_info,\n\t\t\t\t\"zoned: failed to allocate zone cache for %s\",\n\t\t\t\trcu_str_deref(device->name));\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t \n\tnactive = 0;\n\twhile (sector < nr_sectors) {\n\t\tnr_zones = BTRFS_REPORT_NR_ZONES;\n\t\tret = btrfs_get_dev_zones(device, sector << SECTOR_SHIFT, zones,\n\t\t\t\t\t  &nr_zones);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tfor (i = 0; i < nr_zones; i++) {\n\t\t\tif (zones[i].type == BLK_ZONE_TYPE_SEQWRITE_REQ)\n\t\t\t\t__set_bit(nreported, zone_info->seq_zones);\n\t\t\tswitch (zones[i].cond) {\n\t\t\tcase BLK_ZONE_COND_EMPTY:\n\t\t\t\t__set_bit(nreported, zone_info->empty_zones);\n\t\t\t\tbreak;\n\t\t\tcase BLK_ZONE_COND_IMP_OPEN:\n\t\t\tcase BLK_ZONE_COND_EXP_OPEN:\n\t\t\tcase BLK_ZONE_COND_CLOSED:\n\t\t\t\t__set_bit(nreported, zone_info->active_zones);\n\t\t\t\tnactive++;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tnreported++;\n\t\t}\n\t\tsector = zones[nr_zones - 1].start + zones[nr_zones - 1].len;\n\t}\n\n\tif (nreported != zone_info->nr_zones) {\n\t\tbtrfs_err_in_rcu(device->fs_info,\n\t\t\t\t \"inconsistent number of zones on %s (%u/%u)\",\n\t\t\t\t rcu_str_deref(device->name), nreported,\n\t\t\t\t zone_info->nr_zones);\n\t\tret = -EIO;\n\t\tgoto out;\n\t}\n\n\tif (max_active_zones) {\n\t\tif (nactive > max_active_zones) {\n\t\t\tbtrfs_err_in_rcu(device->fs_info,\n\t\t\t\"zoned: %u active zones on %s exceeds max_active_zones %u\",\n\t\t\t\t\t nactive, rcu_str_deref(device->name),\n\t\t\t\t\t max_active_zones);\n\t\t\tret = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t\tatomic_set(&zone_info->active_zones_left,\n\t\t\t   max_active_zones - nactive);\n\t\tset_bit(BTRFS_FS_ACTIVE_ZONE_TRACKING, &fs_info->flags);\n\t}\n\n\t \n\tnr_zones = BTRFS_NR_SB_LOG_ZONES;\n\tfor (i = 0; i < BTRFS_SUPER_MIRROR_MAX; i++) {\n\t\tu32 sb_zone;\n\t\tu64 sb_wp;\n\t\tint sb_pos = BTRFS_NR_SB_LOG_ZONES * i;\n\n\t\tsb_zone = sb_zone_number(zone_info->zone_size_shift, i);\n\t\tif (sb_zone + 1 >= zone_info->nr_zones)\n\t\t\tcontinue;\n\n\t\tret = btrfs_get_dev_zones(device,\n\t\t\t\t\t  zone_start_physical(sb_zone, zone_info),\n\t\t\t\t\t  &zone_info->sb_zones[sb_pos],\n\t\t\t\t\t  &nr_zones);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tif (nr_zones != BTRFS_NR_SB_LOG_ZONES) {\n\t\t\tbtrfs_err_in_rcu(device->fs_info,\n\t\"zoned: failed to read super block log zone info at devid %llu zone %u\",\n\t\t\t\t\t device->devid, sb_zone);\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\n\t\t \n\t\tif (zone_info->sb_zones[BTRFS_NR_SB_LOG_ZONES * i].type ==\n\t\t    BLK_ZONE_TYPE_CONVENTIONAL)\n\t\t\tcontinue;\n\n\t\tret = sb_write_pointer(device->bdev,\n\t\t\t\t       &zone_info->sb_zones[sb_pos], &sb_wp);\n\t\tif (ret != -ENOENT && ret) {\n\t\t\tbtrfs_err_in_rcu(device->fs_info,\n\t\t\t\"zoned: super block log zone corrupted devid %llu zone %u\",\n\t\t\t\t\t device->devid, sb_zone);\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\n\tkvfree(zones);\n\n\tswitch (bdev_zoned_model(bdev)) {\n\tcase BLK_ZONED_HM:\n\t\tmodel = \"host-managed zoned\";\n\t\temulated = \"\";\n\t\tbreak;\n\tcase BLK_ZONED_HA:\n\t\tmodel = \"host-aware zoned\";\n\t\temulated = \"\";\n\t\tbreak;\n\tcase BLK_ZONED_NONE:\n\t\tmodel = \"regular\";\n\t\temulated = \"emulated \";\n\t\tbreak;\n\tdefault:\n\t\t \n\t\tbtrfs_err_in_rcu(fs_info, \"zoned: unsupported model %d on %s\",\n\t\t\t\t bdev_zoned_model(bdev),\n\t\t\t\t rcu_str_deref(device->name));\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out_free_zone_info;\n\t}\n\n\tbtrfs_info_in_rcu(fs_info,\n\t\t\"%s block device %s, %u %szones of %llu bytes\",\n\t\tmodel, rcu_str_deref(device->name), zone_info->nr_zones,\n\t\temulated, zone_info->zone_size);\n\n\treturn 0;\n\nout:\n\tkvfree(zones);\nout_free_zone_info:\n\tbtrfs_destroy_dev_zone_info(device);\n\n\treturn ret;\n}\n\nvoid btrfs_destroy_dev_zone_info(struct btrfs_device *device)\n{\n\tstruct btrfs_zoned_device_info *zone_info = device->zone_info;\n\n\tif (!zone_info)\n\t\treturn;\n\n\tbitmap_free(zone_info->active_zones);\n\tbitmap_free(zone_info->seq_zones);\n\tbitmap_free(zone_info->empty_zones);\n\tvfree(zone_info->zone_cache);\n\tkfree(zone_info);\n\tdevice->zone_info = NULL;\n}\n\nstruct btrfs_zoned_device_info *btrfs_clone_dev_zone_info(struct btrfs_device *orig_dev)\n{\n\tstruct btrfs_zoned_device_info *zone_info;\n\n\tzone_info = kmemdup(orig_dev->zone_info, sizeof(*zone_info), GFP_KERNEL);\n\tif (!zone_info)\n\t\treturn NULL;\n\n\tzone_info->seq_zones = bitmap_zalloc(zone_info->nr_zones, GFP_KERNEL);\n\tif (!zone_info->seq_zones)\n\t\tgoto out;\n\n\tbitmap_copy(zone_info->seq_zones, orig_dev->zone_info->seq_zones,\n\t\t    zone_info->nr_zones);\n\n\tzone_info->empty_zones = bitmap_zalloc(zone_info->nr_zones, GFP_KERNEL);\n\tif (!zone_info->empty_zones)\n\t\tgoto out;\n\n\tbitmap_copy(zone_info->empty_zones, orig_dev->zone_info->empty_zones,\n\t\t    zone_info->nr_zones);\n\n\tzone_info->active_zones = bitmap_zalloc(zone_info->nr_zones, GFP_KERNEL);\n\tif (!zone_info->active_zones)\n\t\tgoto out;\n\n\tbitmap_copy(zone_info->active_zones, orig_dev->zone_info->active_zones,\n\t\t    zone_info->nr_zones);\n\tzone_info->zone_cache = NULL;\n\n\treturn zone_info;\n\nout:\n\tbitmap_free(zone_info->seq_zones);\n\tbitmap_free(zone_info->empty_zones);\n\tbitmap_free(zone_info->active_zones);\n\tkfree(zone_info);\n\treturn NULL;\n}\n\nint btrfs_get_dev_zone(struct btrfs_device *device, u64 pos,\n\t\t       struct blk_zone *zone)\n{\n\tunsigned int nr_zones = 1;\n\tint ret;\n\n\tret = btrfs_get_dev_zones(device, pos, zone, &nr_zones);\n\tif (ret != 0 || !nr_zones)\n\t\treturn ret ? ret : -EIO;\n\n\treturn 0;\n}\n\nstatic int btrfs_check_for_zoned_device(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_device *device;\n\n\tlist_for_each_entry(device, &fs_info->fs_devices->devices, dev_list) {\n\t\tif (device->bdev &&\n\t\t    bdev_zoned_model(device->bdev) == BLK_ZONED_HM) {\n\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\"zoned: mode not enabled but zoned device found: %pg\",\n\t\t\t\tdevice->bdev);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nint btrfs_check_zoned_mode(struct btrfs_fs_info *fs_info)\n{\n\tstruct queue_limits *lim = &fs_info->limits;\n\tstruct btrfs_device *device;\n\tu64 zone_size = 0;\n\tint ret;\n\n\t \n\tif (!btrfs_fs_incompat(fs_info, ZONED))\n\t\treturn btrfs_check_for_zoned_device(fs_info);\n\n\tblk_set_stacking_limits(lim);\n\n\tlist_for_each_entry(device, &fs_info->fs_devices->devices, dev_list) {\n\t\tstruct btrfs_zoned_device_info *zone_info = device->zone_info;\n\n\t\tif (!device->bdev)\n\t\t\tcontinue;\n\n\t\tif (!zone_size) {\n\t\t\tzone_size = zone_info->zone_size;\n\t\t} else if (zone_info->zone_size != zone_size) {\n\t\t\tbtrfs_err(fs_info,\n\t\t\"zoned: unequal block device zone sizes: have %llu found %llu\",\n\t\t\t\t  zone_info->zone_size, zone_size);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t \n\t\tif (bdev_is_zoned(device->bdev)) {\n\t\t\tblk_stack_limits(lim,\n\t\t\t\t\t &bdev_get_queue(device->bdev)->limits,\n\t\t\t\t\t 0);\n\t\t}\n\t}\n\n\t \n\tif (!IS_ALIGNED(zone_size, BTRFS_STRIPE_LEN)) {\n\t\tbtrfs_err(fs_info,\n\t\t\t  \"zoned: zone size %llu not aligned to stripe %u\",\n\t\t\t  zone_size, BTRFS_STRIPE_LEN);\n\t\treturn -EINVAL;\n\t}\n\n\tif (btrfs_fs_incompat(fs_info, MIXED_GROUPS)) {\n\t\tbtrfs_err(fs_info, \"zoned: mixed block groups not supported\");\n\t\treturn -EINVAL;\n\t}\n\n\tfs_info->zone_size = zone_size;\n\t \n\tfs_info->max_zone_append_size = ALIGN_DOWN(\n\t\tmin3((u64)lim->max_zone_append_sectors << SECTOR_SHIFT,\n\t\t     (u64)lim->max_sectors << SECTOR_SHIFT,\n\t\t     (u64)lim->max_segments << PAGE_SHIFT),\n\t\tfs_info->sectorsize);\n\tfs_info->fs_devices->chunk_alloc_policy = BTRFS_CHUNK_ALLOC_ZONED;\n\tif (fs_info->max_zone_append_size < fs_info->max_extent_size)\n\t\tfs_info->max_extent_size = fs_info->max_zone_append_size;\n\n\t \n\tret = btrfs_check_mountopts_zoned(fs_info);\n\tif (ret)\n\t\treturn ret;\n\n\tbtrfs_info(fs_info, \"zoned mode enabled with zone size %llu\", zone_size);\n\treturn 0;\n}\n\nint btrfs_check_mountopts_zoned(struct btrfs_fs_info *info)\n{\n\tif (!btrfs_is_zoned(info))\n\t\treturn 0;\n\n\t \n\tif (btrfs_test_opt(info, SPACE_CACHE)) {\n\t\tbtrfs_err(info, \"zoned: space cache v1 is not supported\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (btrfs_test_opt(info, NODATACOW)) {\n\t\tbtrfs_err(info, \"zoned: NODATACOW not supported\");\n\t\treturn -EINVAL;\n\t}\n\n\tbtrfs_clear_and_info(info, DISCARD_ASYNC,\n\t\t\t\"zoned: async discard ignored and disabled for zoned mode\");\n\n\treturn 0;\n}\n\nstatic int sb_log_location(struct block_device *bdev, struct blk_zone *zones,\n\t\t\t   int rw, u64 *bytenr_ret)\n{\n\tu64 wp;\n\tint ret;\n\n\tif (zones[0].type == BLK_ZONE_TYPE_CONVENTIONAL) {\n\t\t*bytenr_ret = zones[0].start << SECTOR_SHIFT;\n\t\treturn 0;\n\t}\n\n\tret = sb_write_pointer(bdev, zones, &wp);\n\tif (ret != -ENOENT && ret < 0)\n\t\treturn ret;\n\n\tif (rw == WRITE) {\n\t\tstruct blk_zone *reset = NULL;\n\n\t\tif (wp == zones[0].start << SECTOR_SHIFT)\n\t\t\treset = &zones[0];\n\t\telse if (wp == zones[1].start << SECTOR_SHIFT)\n\t\t\treset = &zones[1];\n\n\t\tif (reset && reset->cond != BLK_ZONE_COND_EMPTY) {\n\t\t\tASSERT(sb_zone_is_full(reset));\n\n\t\t\tret = blkdev_zone_mgmt(bdev, REQ_OP_ZONE_RESET,\n\t\t\t\t\t       reset->start, reset->len,\n\t\t\t\t\t       GFP_NOFS);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\n\t\t\treset->cond = BLK_ZONE_COND_EMPTY;\n\t\t\treset->wp = reset->start;\n\t\t}\n\t} else if (ret != -ENOENT) {\n\t\t \n\t\tu64 zone_end = 0;\n\n\t\tif (wp == zones[0].start << SECTOR_SHIFT)\n\t\t\tzone_end = zones[1].start + zones[1].capacity;\n\t\telse if (wp == zones[1].start << SECTOR_SHIFT)\n\t\t\tzone_end = zones[0].start + zones[0].capacity;\n\t\tif (zone_end)\n\t\t\twp = ALIGN_DOWN(zone_end << SECTOR_SHIFT,\n\t\t\t\t\tBTRFS_SUPER_INFO_SIZE);\n\n\t\twp -= BTRFS_SUPER_INFO_SIZE;\n\t}\n\n\t*bytenr_ret = wp;\n\treturn 0;\n\n}\n\nint btrfs_sb_log_location_bdev(struct block_device *bdev, int mirror, int rw,\n\t\t\t       u64 *bytenr_ret)\n{\n\tstruct blk_zone zones[BTRFS_NR_SB_LOG_ZONES];\n\tsector_t zone_sectors;\n\tu32 sb_zone;\n\tint ret;\n\tu8 zone_sectors_shift;\n\tsector_t nr_sectors;\n\tu32 nr_zones;\n\n\tif (!bdev_is_zoned(bdev)) {\n\t\t*bytenr_ret = btrfs_sb_offset(mirror);\n\t\treturn 0;\n\t}\n\n\tASSERT(rw == READ || rw == WRITE);\n\n\tzone_sectors = bdev_zone_sectors(bdev);\n\tif (!is_power_of_2(zone_sectors))\n\t\treturn -EINVAL;\n\tzone_sectors_shift = ilog2(zone_sectors);\n\tnr_sectors = bdev_nr_sectors(bdev);\n\tnr_zones = nr_sectors >> zone_sectors_shift;\n\n\tsb_zone = sb_zone_number(zone_sectors_shift + SECTOR_SHIFT, mirror);\n\tif (sb_zone + 1 >= nr_zones)\n\t\treturn -ENOENT;\n\n\tret = blkdev_report_zones(bdev, zone_start_sector(sb_zone, bdev),\n\t\t\t\t  BTRFS_NR_SB_LOG_ZONES, copy_zone_info_cb,\n\t\t\t\t  zones);\n\tif (ret < 0)\n\t\treturn ret;\n\tif (ret != BTRFS_NR_SB_LOG_ZONES)\n\t\treturn -EIO;\n\n\treturn sb_log_location(bdev, zones, rw, bytenr_ret);\n}\n\nint btrfs_sb_log_location(struct btrfs_device *device, int mirror, int rw,\n\t\t\t  u64 *bytenr_ret)\n{\n\tstruct btrfs_zoned_device_info *zinfo = device->zone_info;\n\tu32 zone_num;\n\n\t \n\tif (!bdev_is_zoned(device->bdev)) {\n\t\t*bytenr_ret = btrfs_sb_offset(mirror);\n\t\treturn 0;\n\t}\n\n\tzone_num = sb_zone_number(zinfo->zone_size_shift, mirror);\n\tif (zone_num + 1 >= zinfo->nr_zones)\n\t\treturn -ENOENT;\n\n\treturn sb_log_location(device->bdev,\n\t\t\t       &zinfo->sb_zones[BTRFS_NR_SB_LOG_ZONES * mirror],\n\t\t\t       rw, bytenr_ret);\n}\n\nstatic inline bool is_sb_log_zone(struct btrfs_zoned_device_info *zinfo,\n\t\t\t\t  int mirror)\n{\n\tu32 zone_num;\n\n\tif (!zinfo)\n\t\treturn false;\n\n\tzone_num = sb_zone_number(zinfo->zone_size_shift, mirror);\n\tif (zone_num + 1 >= zinfo->nr_zones)\n\t\treturn false;\n\n\tif (!test_bit(zone_num, zinfo->seq_zones))\n\t\treturn false;\n\n\treturn true;\n}\n\nint btrfs_advance_sb_log(struct btrfs_device *device, int mirror)\n{\n\tstruct btrfs_zoned_device_info *zinfo = device->zone_info;\n\tstruct blk_zone *zone;\n\tint i;\n\n\tif (!is_sb_log_zone(zinfo, mirror))\n\t\treturn 0;\n\n\tzone = &zinfo->sb_zones[BTRFS_NR_SB_LOG_ZONES * mirror];\n\tfor (i = 0; i < BTRFS_NR_SB_LOG_ZONES; i++) {\n\t\t \n\t\tif (zone->cond == BLK_ZONE_COND_FULL) {\n\t\t\tzone++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (zone->cond == BLK_ZONE_COND_EMPTY)\n\t\t\tzone->cond = BLK_ZONE_COND_IMP_OPEN;\n\n\t\tzone->wp += SUPER_INFO_SECTORS;\n\n\t\tif (sb_zone_is_full(zone)) {\n\t\t\t \n\t\t\tif (zone->wp != zone->start + zone->capacity) {\n\t\t\t\tint ret;\n\n\t\t\t\tret = blkdev_zone_mgmt(device->bdev,\n\t\t\t\t\t\tREQ_OP_ZONE_FINISH, zone->start,\n\t\t\t\t\t\tzone->len, GFP_NOFS);\n\t\t\t\tif (ret)\n\t\t\t\t\treturn ret;\n\t\t\t}\n\n\t\t\tzone->wp = zone->start + zone->len;\n\t\t\tzone->cond = BLK_ZONE_COND_FULL;\n\t\t}\n\t\treturn 0;\n\t}\n\n\t \n\tASSERT(0);\n\treturn -EIO;\n}\n\nint btrfs_reset_sb_log_zones(struct block_device *bdev, int mirror)\n{\n\tsector_t zone_sectors;\n\tsector_t nr_sectors;\n\tu8 zone_sectors_shift;\n\tu32 sb_zone;\n\tu32 nr_zones;\n\n\tzone_sectors = bdev_zone_sectors(bdev);\n\tzone_sectors_shift = ilog2(zone_sectors);\n\tnr_sectors = bdev_nr_sectors(bdev);\n\tnr_zones = nr_sectors >> zone_sectors_shift;\n\n\tsb_zone = sb_zone_number(zone_sectors_shift + SECTOR_SHIFT, mirror);\n\tif (sb_zone + 1 >= nr_zones)\n\t\treturn -ENOENT;\n\n\treturn blkdev_zone_mgmt(bdev, REQ_OP_ZONE_RESET,\n\t\t\t\tzone_start_sector(sb_zone, bdev),\n\t\t\t\tzone_sectors * BTRFS_NR_SB_LOG_ZONES, GFP_NOFS);\n}\n\n \nu64 btrfs_find_allocatable_zones(struct btrfs_device *device, u64 hole_start,\n\t\t\t\t u64 hole_end, u64 num_bytes)\n{\n\tstruct btrfs_zoned_device_info *zinfo = device->zone_info;\n\tconst u8 shift = zinfo->zone_size_shift;\n\tu64 nzones = num_bytes >> shift;\n\tu64 pos = hole_start;\n\tu64 begin, end;\n\tbool have_sb;\n\tint i;\n\n\tASSERT(IS_ALIGNED(hole_start, zinfo->zone_size));\n\tASSERT(IS_ALIGNED(num_bytes, zinfo->zone_size));\n\n\twhile (pos < hole_end) {\n\t\tbegin = pos >> shift;\n\t\tend = begin + nzones;\n\n\t\tif (end > zinfo->nr_zones)\n\t\t\treturn hole_end;\n\n\t\t \n\t\tif (btrfs_dev_is_sequential(device, pos) &&\n\t\t    !bitmap_test_range_all_set(zinfo->empty_zones, begin, nzones)) {\n\t\t\tpos += zinfo->zone_size;\n\t\t\tcontinue;\n\t\t}\n\n\t\thave_sb = false;\n\t\tfor (i = 0; i < BTRFS_SUPER_MIRROR_MAX; i++) {\n\t\t\tu32 sb_zone;\n\t\t\tu64 sb_pos;\n\n\t\t\tsb_zone = sb_zone_number(shift, i);\n\t\t\tif (!(end <= sb_zone ||\n\t\t\t      sb_zone + BTRFS_NR_SB_LOG_ZONES <= begin)) {\n\t\t\t\thave_sb = true;\n\t\t\t\tpos = zone_start_physical(\n\t\t\t\t\tsb_zone + BTRFS_NR_SB_LOG_ZONES, zinfo);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t \n\t\t\tsb_pos = btrfs_sb_offset(i);\n\t\t\tif (!(pos + num_bytes <= sb_pos ||\n\t\t\t      sb_pos + BTRFS_SUPER_INFO_SIZE <= pos)) {\n\t\t\t\thave_sb = true;\n\t\t\t\tpos = ALIGN(sb_pos + BTRFS_SUPER_INFO_SIZE,\n\t\t\t\t\t    zinfo->zone_size);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (!have_sb)\n\t\t\tbreak;\n\t}\n\n\treturn pos;\n}\n\nstatic bool btrfs_dev_set_active_zone(struct btrfs_device *device, u64 pos)\n{\n\tstruct btrfs_zoned_device_info *zone_info = device->zone_info;\n\tunsigned int zno = (pos >> zone_info->zone_size_shift);\n\n\t \n\tif (zone_info->max_active_zones == 0)\n\t\treturn true;\n\n\tif (!test_bit(zno, zone_info->active_zones)) {\n\t\t \n\t\tif (atomic_dec_if_positive(&zone_info->active_zones_left) < 0)\n\t\t\treturn false;\n\t\tif (test_and_set_bit(zno, zone_info->active_zones)) {\n\t\t\t \n\t\t\tatomic_inc(&zone_info->active_zones_left);\n\t\t}\n\t}\n\n\treturn true;\n}\n\nstatic void btrfs_dev_clear_active_zone(struct btrfs_device *device, u64 pos)\n{\n\tstruct btrfs_zoned_device_info *zone_info = device->zone_info;\n\tunsigned int zno = (pos >> zone_info->zone_size_shift);\n\n\t \n\tif (zone_info->max_active_zones == 0)\n\t\treturn;\n\n\tif (test_and_clear_bit(zno, zone_info->active_zones))\n\t\tatomic_inc(&zone_info->active_zones_left);\n}\n\nint btrfs_reset_device_zone(struct btrfs_device *device, u64 physical,\n\t\t\t    u64 length, u64 *bytes)\n{\n\tint ret;\n\n\t*bytes = 0;\n\tret = blkdev_zone_mgmt(device->bdev, REQ_OP_ZONE_RESET,\n\t\t\t       physical >> SECTOR_SHIFT, length >> SECTOR_SHIFT,\n\t\t\t       GFP_NOFS);\n\tif (ret)\n\t\treturn ret;\n\n\t*bytes = length;\n\twhile (length) {\n\t\tbtrfs_dev_set_zone_empty(device, physical);\n\t\tbtrfs_dev_clear_active_zone(device, physical);\n\t\tphysical += device->zone_info->zone_size;\n\t\tlength -= device->zone_info->zone_size;\n\t}\n\n\treturn 0;\n}\n\nint btrfs_ensure_empty_zones(struct btrfs_device *device, u64 start, u64 size)\n{\n\tstruct btrfs_zoned_device_info *zinfo = device->zone_info;\n\tconst u8 shift = zinfo->zone_size_shift;\n\tunsigned long begin = start >> shift;\n\tunsigned long nbits = size >> shift;\n\tu64 pos;\n\tint ret;\n\n\tASSERT(IS_ALIGNED(start, zinfo->zone_size));\n\tASSERT(IS_ALIGNED(size, zinfo->zone_size));\n\n\tif (begin + nbits > zinfo->nr_zones)\n\t\treturn -ERANGE;\n\n\t \n\tif (bitmap_test_range_all_zero(zinfo->seq_zones, begin, nbits))\n\t\treturn 0;\n\n\t \n\tif (bitmap_test_range_all_set(zinfo->seq_zones, begin, nbits) &&\n\t    bitmap_test_range_all_set(zinfo->empty_zones, begin, nbits))\n\t\treturn 0;\n\n\tfor (pos = start; pos < start + size; pos += zinfo->zone_size) {\n\t\tu64 reset_bytes;\n\n\t\tif (!btrfs_dev_is_sequential(device, pos) ||\n\t\t    btrfs_dev_is_empty_zone(device, pos))\n\t\t\tcontinue;\n\n\t\t \n\t\tbtrfs_warn_in_rcu(\n\t\t\tdevice->fs_info,\n\t\t\"zoned: resetting device %s (devid %llu) zone %llu for allocation\",\n\t\t\trcu_str_deref(device->name), device->devid, pos >> shift);\n\t\tWARN_ON_ONCE(1);\n\n\t\tret = btrfs_reset_device_zone(device, pos, zinfo->zone_size,\n\t\t\t\t\t      &reset_bytes);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int calculate_alloc_pointer(struct btrfs_block_group *cache,\n\t\t\t\t   u64 *offset_ret, bool new)\n{\n\tstruct btrfs_fs_info *fs_info = cache->fs_info;\n\tstruct btrfs_root *root;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key key;\n\tstruct btrfs_key found_key;\n\tint ret;\n\tu64 length;\n\n\t \n\tif (new) {\n\t\t*offset_ret = 0;\n\t\treturn 0;\n\t}\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = cache->start + cache->length;\n\tkey.type = 0;\n\tkey.offset = 0;\n\n\troot = btrfs_extent_root(fs_info, key.objectid);\n\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\t \n\tif (!ret)\n\t\tret = -EUCLEAN;\n\tif (ret < 0)\n\t\tgoto out;\n\n\tret = btrfs_previous_extent_item(root, path, cache->start);\n\tif (ret) {\n\t\tif (ret == 1) {\n\t\t\tret = 0;\n\t\t\t*offset_ret = 0;\n\t\t}\n\t\tgoto out;\n\t}\n\n\tbtrfs_item_key_to_cpu(path->nodes[0], &found_key, path->slots[0]);\n\n\tif (found_key.type == BTRFS_EXTENT_ITEM_KEY)\n\t\tlength = found_key.offset;\n\telse\n\t\tlength = fs_info->nodesize;\n\n\tif (!(found_key.objectid >= cache->start &&\n\t       found_key.objectid + length <= cache->start + cache->length)) {\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\t*offset_ret = found_key.objectid + length - cache->start;\n\tret = 0;\n\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nint btrfs_load_block_group_zone_info(struct btrfs_block_group *cache, bool new)\n{\n\tstruct btrfs_fs_info *fs_info = cache->fs_info;\n\tstruct extent_map_tree *em_tree = &fs_info->mapping_tree;\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tstruct btrfs_device *device;\n\tu64 logical = cache->start;\n\tu64 length = cache->length;\n\tint ret;\n\tint i;\n\tunsigned int nofs_flag;\n\tu64 *alloc_offsets = NULL;\n\tu64 *caps = NULL;\n\tu64 *physical = NULL;\n\tunsigned long *active = NULL;\n\tu64 last_alloc = 0;\n\tu32 num_sequential = 0, num_conventional = 0;\n\n\tif (!btrfs_is_zoned(fs_info))\n\t\treturn 0;\n\n\t \n\tif (!IS_ALIGNED(length, fs_info->zone_size)) {\n\t\tbtrfs_err(fs_info,\n\t\t\"zoned: block group %llu len %llu unaligned to zone size %llu\",\n\t\t\t  logical, length, fs_info->zone_size);\n\t\treturn -EIO;\n\t}\n\n\t \n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, logical, length);\n\tread_unlock(&em_tree->lock);\n\n\tif (!em)\n\t\treturn -EINVAL;\n\n\tmap = em->map_lookup;\n\n\tcache->physical_map = kmemdup(map, map_lookup_size(map->num_stripes), GFP_NOFS);\n\tif (!cache->physical_map) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\talloc_offsets = kcalloc(map->num_stripes, sizeof(*alloc_offsets), GFP_NOFS);\n\tif (!alloc_offsets) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tcaps = kcalloc(map->num_stripes, sizeof(*caps), GFP_NOFS);\n\tif (!caps) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tphysical = kcalloc(map->num_stripes, sizeof(*physical), GFP_NOFS);\n\tif (!physical) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tactive = bitmap_zalloc(map->num_stripes, GFP_NOFS);\n\tif (!active) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tbool is_sequential;\n\t\tstruct blk_zone zone;\n\t\tstruct btrfs_dev_replace *dev_replace = &fs_info->dev_replace;\n\t\tint dev_replace_is_ongoing = 0;\n\n\t\tdevice = map->stripes[i].dev;\n\t\tphysical[i] = map->stripes[i].physical;\n\n\t\tif (device->bdev == NULL) {\n\t\t\talloc_offsets[i] = WP_MISSING_DEV;\n\t\t\tcontinue;\n\t\t}\n\n\t\tis_sequential = btrfs_dev_is_sequential(device, physical[i]);\n\t\tif (is_sequential)\n\t\t\tnum_sequential++;\n\t\telse\n\t\t\tnum_conventional++;\n\n\t\t \n\t\tif (!device->zone_info->max_active_zones)\n\t\t\t__set_bit(i, active);\n\n\t\tif (!is_sequential) {\n\t\t\talloc_offsets[i] = WP_CONVENTIONAL;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tbtrfs_dev_clear_zone_empty(device, physical[i]);\n\n\t\tdown_read(&dev_replace->rwsem);\n\t\tdev_replace_is_ongoing = btrfs_dev_replace_is_ongoing(dev_replace);\n\t\tif (dev_replace_is_ongoing && dev_replace->tgtdev != NULL)\n\t\t\tbtrfs_dev_clear_zone_empty(dev_replace->tgtdev, physical[i]);\n\t\tup_read(&dev_replace->rwsem);\n\n\t\t \n\t\tWARN_ON(!IS_ALIGNED(physical[i], fs_info->zone_size));\n\t\tnofs_flag = memalloc_nofs_save();\n\t\tret = btrfs_get_dev_zone(device, physical[i], &zone);\n\t\tmemalloc_nofs_restore(nofs_flag);\n\t\tif (ret == -EIO || ret == -EOPNOTSUPP) {\n\t\t\tret = 0;\n\t\t\talloc_offsets[i] = WP_MISSING_DEV;\n\t\t\tcontinue;\n\t\t} else if (ret) {\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (zone.type == BLK_ZONE_TYPE_CONVENTIONAL) {\n\t\t\tbtrfs_err_in_rcu(fs_info,\n\t\"zoned: unexpected conventional zone %llu on device %s (devid %llu)\",\n\t\t\t\tzone.start << SECTOR_SHIFT,\n\t\t\t\trcu_str_deref(device->name), device->devid);\n\t\t\tret = -EIO;\n\t\t\tgoto out;\n\t\t}\n\n\t\tcaps[i] = (zone.capacity << SECTOR_SHIFT);\n\n\t\tswitch (zone.cond) {\n\t\tcase BLK_ZONE_COND_OFFLINE:\n\t\tcase BLK_ZONE_COND_READONLY:\n\t\t\tbtrfs_err(fs_info,\n\t\t\"zoned: offline/readonly zone %llu on device %s (devid %llu)\",\n\t\t\t\t  physical[i] >> device->zone_info->zone_size_shift,\n\t\t\t\t  rcu_str_deref(device->name), device->devid);\n\t\t\talloc_offsets[i] = WP_MISSING_DEV;\n\t\t\tbreak;\n\t\tcase BLK_ZONE_COND_EMPTY:\n\t\t\talloc_offsets[i] = 0;\n\t\t\tbreak;\n\t\tcase BLK_ZONE_COND_FULL:\n\t\t\talloc_offsets[i] = caps[i];\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t \n\t\t\talloc_offsets[i] =\n\t\t\t\t\t((zone.wp - zone.start) << SECTOR_SHIFT);\n\t\t\t__set_bit(i, active);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (num_sequential > 0)\n\t\tset_bit(BLOCK_GROUP_FLAG_SEQUENTIAL_ZONE, &cache->runtime_flags);\n\n\tif (num_conventional > 0) {\n\t\t \n\t\tcache->zone_capacity = cache->length;\n\t\tret = calculate_alloc_pointer(cache, &last_alloc, new);\n\t\tif (ret) {\n\t\t\tbtrfs_err(fs_info,\n\t\t\t\"zoned: failed to determine allocation offset of bg %llu\",\n\t\t\t\t  cache->start);\n\t\t\tgoto out;\n\t\t} else if (map->num_stripes == num_conventional) {\n\t\t\tcache->alloc_offset = last_alloc;\n\t\t\tset_bit(BLOCK_GROUP_FLAG_ZONE_IS_ACTIVE, &cache->runtime_flags);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tswitch (map->type & BTRFS_BLOCK_GROUP_PROFILE_MASK) {\n\tcase 0:  \n\t\tif (alloc_offsets[0] == WP_MISSING_DEV) {\n\t\t\tbtrfs_err(fs_info,\n\t\t\t\"zoned: cannot recover write pointer for zone %llu\",\n\t\t\t\tphysical[0]);\n\t\t\tret = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t\tcache->alloc_offset = alloc_offsets[0];\n\t\tcache->zone_capacity = caps[0];\n\t\tif (test_bit(0, active))\n\t\t\tset_bit(BLOCK_GROUP_FLAG_ZONE_IS_ACTIVE, &cache->runtime_flags);\n\t\tbreak;\n\tcase BTRFS_BLOCK_GROUP_DUP:\n\t\tif (map->type & BTRFS_BLOCK_GROUP_DATA) {\n\t\t\tbtrfs_err(fs_info, \"zoned: profile DUP not yet supported on data bg\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tif (alloc_offsets[0] == WP_MISSING_DEV) {\n\t\t\tbtrfs_err(fs_info,\n\t\t\t\"zoned: cannot recover write pointer for zone %llu\",\n\t\t\t\tphysical[0]);\n\t\t\tret = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t\tif (alloc_offsets[1] == WP_MISSING_DEV) {\n\t\t\tbtrfs_err(fs_info,\n\t\t\t\"zoned: cannot recover write pointer for zone %llu\",\n\t\t\t\tphysical[1]);\n\t\t\tret = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t\tif (alloc_offsets[0] != alloc_offsets[1]) {\n\t\t\tbtrfs_err(fs_info,\n\t\t\t\"zoned: write pointer offset mismatch of zones in DUP profile\");\n\t\t\tret = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t\tif (test_bit(0, active) != test_bit(1, active)) {\n\t\t\tif (!btrfs_zone_activate(cache)) {\n\t\t\t\tret = -EIO;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t} else {\n\t\t\tif (test_bit(0, active))\n\t\t\t\tset_bit(BLOCK_GROUP_FLAG_ZONE_IS_ACTIVE,\n\t\t\t\t\t&cache->runtime_flags);\n\t\t}\n\t\tcache->alloc_offset = alloc_offsets[0];\n\t\tcache->zone_capacity = min(caps[0], caps[1]);\n\t\tbreak;\n\tcase BTRFS_BLOCK_GROUP_RAID1:\n\tcase BTRFS_BLOCK_GROUP_RAID0:\n\tcase BTRFS_BLOCK_GROUP_RAID10:\n\tcase BTRFS_BLOCK_GROUP_RAID5:\n\tcase BTRFS_BLOCK_GROUP_RAID6:\n\t\t \n\tdefault:\n\t\tbtrfs_err(fs_info, \"zoned: profile %s not yet supported\",\n\t\t\t  btrfs_bg_type_to_raid_name(map->type));\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\nout:\n\tif (cache->alloc_offset > fs_info->zone_size) {\n\t\tbtrfs_err(fs_info,\n\t\t\t\"zoned: invalid write pointer %llu in block group %llu\",\n\t\t\tcache->alloc_offset, cache->start);\n\t\tret = -EIO;\n\t}\n\n\tif (cache->alloc_offset > cache->zone_capacity) {\n\t\tbtrfs_err(fs_info,\n\"zoned: invalid write pointer %llu (larger than zone capacity %llu) in block group %llu\",\n\t\t\t  cache->alloc_offset, cache->zone_capacity,\n\t\t\t  cache->start);\n\t\tret = -EIO;\n\t}\n\n\t \n\tif (!ret && num_conventional && last_alloc > cache->alloc_offset) {\n\t\tbtrfs_err(fs_info,\n\t\t\t  \"zoned: got wrong write pointer in BG %llu: %llu > %llu\",\n\t\t\t  logical, last_alloc, cache->alloc_offset);\n\t\tret = -EIO;\n\t}\n\n\tif (!ret) {\n\t\tcache->meta_write_pointer = cache->alloc_offset + cache->start;\n\t\tif (test_bit(BLOCK_GROUP_FLAG_ZONE_IS_ACTIVE, &cache->runtime_flags)) {\n\t\t\tbtrfs_get_block_group(cache);\n\t\t\tspin_lock(&fs_info->zone_active_bgs_lock);\n\t\t\tlist_add_tail(&cache->active_bg_list,\n\t\t\t\t      &fs_info->zone_active_bgs);\n\t\t\tspin_unlock(&fs_info->zone_active_bgs_lock);\n\t\t}\n\t} else {\n\t\tkfree(cache->physical_map);\n\t\tcache->physical_map = NULL;\n\t}\n\tbitmap_free(active);\n\tkfree(physical);\n\tkfree(caps);\n\tkfree(alloc_offsets);\n\tfree_extent_map(em);\n\n\treturn ret;\n}\n\nvoid btrfs_calc_zone_unusable(struct btrfs_block_group *cache)\n{\n\tu64 unusable, free;\n\n\tif (!btrfs_is_zoned(cache->fs_info))\n\t\treturn;\n\n\tWARN_ON(cache->bytes_super != 0);\n\tunusable = (cache->alloc_offset - cache->used) +\n\t\t   (cache->length - cache->zone_capacity);\n\tfree = cache->zone_capacity - cache->alloc_offset;\n\n\t \n\tcache->cached = BTRFS_CACHE_FINISHED;\n\tcache->free_space_ctl->free_space = free;\n\tcache->zone_unusable = unusable;\n}\n\nvoid btrfs_redirty_list_add(struct btrfs_transaction *trans,\n\t\t\t    struct extent_buffer *eb)\n{\n\tif (!btrfs_is_zoned(eb->fs_info) ||\n\t    btrfs_header_flag(eb, BTRFS_HEADER_FLAG_WRITTEN))\n\t\treturn;\n\n\tASSERT(!test_bit(EXTENT_BUFFER_DIRTY, &eb->bflags));\n\n\tmemzero_extent_buffer(eb, 0, eb->len);\n\tset_bit(EXTENT_BUFFER_NO_CHECK, &eb->bflags);\n\tset_extent_buffer_dirty(eb);\n\tset_extent_bit(&trans->dirty_pages, eb->start, eb->start + eb->len - 1,\n\t\t\tEXTENT_DIRTY | EXTENT_NOWAIT, NULL);\n}\n\nbool btrfs_use_zone_append(struct btrfs_bio *bbio)\n{\n\tu64 start = (bbio->bio.bi_iter.bi_sector << SECTOR_SHIFT);\n\tstruct btrfs_inode *inode = bbio->inode;\n\tstruct btrfs_fs_info *fs_info = bbio->fs_info;\n\tstruct btrfs_block_group *cache;\n\tbool ret = false;\n\n\tif (!btrfs_is_zoned(fs_info))\n\t\treturn false;\n\n\tif (!inode || !is_data_inode(&inode->vfs_inode))\n\t\treturn false;\n\n\tif (btrfs_op(&bbio->bio) != BTRFS_MAP_WRITE)\n\t\treturn false;\n\n\t \n\tif (btrfs_is_data_reloc_root(inode->root))\n\t\treturn false;\n\n\tcache = btrfs_lookup_block_group(fs_info, start);\n\tASSERT(cache);\n\tif (!cache)\n\t\treturn false;\n\n\tret = !!test_bit(BLOCK_GROUP_FLAG_SEQUENTIAL_ZONE, &cache->runtime_flags);\n\tbtrfs_put_block_group(cache);\n\n\treturn ret;\n}\n\nvoid btrfs_record_physical_zoned(struct btrfs_bio *bbio)\n{\n\tconst u64 physical = bbio->bio.bi_iter.bi_sector << SECTOR_SHIFT;\n\tstruct btrfs_ordered_sum *sum = bbio->sums;\n\n\tif (physical < bbio->orig_physical)\n\t\tsum->logical -= bbio->orig_physical - physical;\n\telse\n\t\tsum->logical += physical - bbio->orig_physical;\n}\n\nstatic void btrfs_rewrite_logical_zoned(struct btrfs_ordered_extent *ordered,\n\t\t\t\t\tu64 logical)\n{\n\tstruct extent_map_tree *em_tree = &BTRFS_I(ordered->inode)->extent_tree;\n\tstruct extent_map *em;\n\n\tordered->disk_bytenr = logical;\n\n\twrite_lock(&em_tree->lock);\n\tem = search_extent_mapping(em_tree, ordered->file_offset,\n\t\t\t\t   ordered->num_bytes);\n\tem->block_start = logical;\n\tfree_extent_map(em);\n\twrite_unlock(&em_tree->lock);\n}\n\nstatic bool btrfs_zoned_split_ordered(struct btrfs_ordered_extent *ordered,\n\t\t\t\t      u64 logical, u64 len)\n{\n\tstruct btrfs_ordered_extent *new;\n\n\tif (!test_bit(BTRFS_ORDERED_NOCOW, &ordered->flags) &&\n\t    split_extent_map(BTRFS_I(ordered->inode), ordered->file_offset,\n\t\t\t     ordered->num_bytes, len, logical))\n\t\treturn false;\n\n\tnew = btrfs_split_ordered_extent(ordered, len);\n\tif (IS_ERR(new))\n\t\treturn false;\n\tnew->disk_bytenr = logical;\n\tbtrfs_finish_one_ordered(new);\n\treturn true;\n}\n\nvoid btrfs_finish_ordered_zoned(struct btrfs_ordered_extent *ordered)\n{\n\tstruct btrfs_inode *inode = BTRFS_I(ordered->inode);\n\tstruct btrfs_fs_info *fs_info = inode->root->fs_info;\n\tstruct btrfs_ordered_sum *sum;\n\tu64 logical, len;\n\n\t \n\tif (test_bit(BTRFS_ORDERED_PREALLOC, &ordered->flags))\n\t\treturn;\n\n\tASSERT(!list_empty(&ordered->list));\n\t \n\tsum = list_first_entry(&ordered->list, struct btrfs_ordered_sum, list);\n\tlogical = sum->logical;\n\tlen = sum->len;\n\n\twhile (len < ordered->disk_num_bytes) {\n\t\tsum = list_next_entry(sum, list);\n\t\tif (sum->logical == logical + len) {\n\t\t\tlen += sum->len;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!btrfs_zoned_split_ordered(ordered, logical, len)) {\n\t\t\tset_bit(BTRFS_ORDERED_IOERR, &ordered->flags);\n\t\t\tbtrfs_err(fs_info, \"failed to split ordered extent\");\n\t\t\tgoto out;\n\t\t}\n\t\tlogical = sum->logical;\n\t\tlen = sum->len;\n\t}\n\n\tif (ordered->disk_bytenr != logical)\n\t\tbtrfs_rewrite_logical_zoned(ordered, logical);\n\nout:\n\t \n\tif ((inode->flags & BTRFS_INODE_NODATASUM) ||\n\t    test_bit(BTRFS_FS_STATE_NO_CSUMS, &fs_info->fs_state)) {\n\t\twhile ((sum = list_first_entry_or_null(&ordered->list,\n\t\t\t\t\t\t       typeof(*sum), list))) {\n\t\t\tlist_del(&sum->list);\n\t\t\tkfree(sum);\n\t\t}\n\t}\n}\n\nstatic bool check_bg_is_active(struct btrfs_eb_write_context *ctx,\n\t\t\t       struct btrfs_block_group **active_bg)\n{\n\tconst struct writeback_control *wbc = ctx->wbc;\n\tstruct btrfs_block_group *block_group = ctx->zoned_bg;\n\tstruct btrfs_fs_info *fs_info = block_group->fs_info;\n\n\tif (test_bit(BLOCK_GROUP_FLAG_ZONE_IS_ACTIVE, &block_group->runtime_flags))\n\t\treturn true;\n\n\tif (fs_info->treelog_bg == block_group->start) {\n\t\tif (!btrfs_zone_activate(block_group)) {\n\t\t\tint ret_fin = btrfs_zone_finish_one_bg(fs_info);\n\n\t\t\tif (ret_fin != 1 || !btrfs_zone_activate(block_group))\n\t\t\t\treturn false;\n\t\t}\n\t} else if (*active_bg != block_group) {\n\t\tstruct btrfs_block_group *tgt = *active_bg;\n\n\t\t \n\t\tlockdep_assert_held(&fs_info->zoned_meta_io_lock);\n\n\t\tif (tgt) {\n\t\t\t \n\t\t\tif (tgt->meta_write_pointer < tgt->start + tgt->alloc_offset) {\n\t\t\t\tif (wbc->sync_mode == WB_SYNC_NONE ||\n\t\t\t\t    (wbc->sync_mode == WB_SYNC_ALL && !wbc->for_sync))\n\t\t\t\t\treturn false;\n\t\t\t}\n\n\t\t\t \n\t\t\tbtrfs_zoned_meta_io_unlock(fs_info);\n\t\t\twait_eb_writebacks(tgt);\n\t\t\tdo_zone_finish(tgt, true);\n\t\t\tbtrfs_zoned_meta_io_lock(fs_info);\n\t\t\tif (*active_bg == tgt) {\n\t\t\t\tbtrfs_put_block_group(tgt);\n\t\t\t\t*active_bg = NULL;\n\t\t\t}\n\t\t}\n\t\tif (!btrfs_zone_activate(block_group))\n\t\t\treturn false;\n\t\tif (*active_bg != block_group) {\n\t\t\tASSERT(*active_bg == NULL);\n\t\t\t*active_bg = block_group;\n\t\t\tbtrfs_get_block_group(block_group);\n\t\t}\n\t}\n\n\treturn true;\n}\n\n \nint btrfs_check_meta_write_pointer(struct btrfs_fs_info *fs_info,\n\t\t\t\t   struct btrfs_eb_write_context *ctx)\n{\n\tconst struct writeback_control *wbc = ctx->wbc;\n\tconst struct extent_buffer *eb = ctx->eb;\n\tstruct btrfs_block_group *block_group = ctx->zoned_bg;\n\n\tif (!btrfs_is_zoned(fs_info))\n\t\treturn 0;\n\n\tif (block_group) {\n\t\tif (block_group->start > eb->start ||\n\t\t    block_group->start + block_group->length <= eb->start) {\n\t\t\tbtrfs_put_block_group(block_group);\n\t\t\tblock_group = NULL;\n\t\t\tctx->zoned_bg = NULL;\n\t\t}\n\t}\n\n\tif (!block_group) {\n\t\tblock_group = btrfs_lookup_block_group(fs_info, eb->start);\n\t\tif (!block_group)\n\t\t\treturn 0;\n\t\tctx->zoned_bg = block_group;\n\t}\n\n\tif (block_group->meta_write_pointer == eb->start) {\n\t\tstruct btrfs_block_group **tgt;\n\n\t\tif (!test_bit(BTRFS_FS_ACTIVE_ZONE_TRACKING, &fs_info->flags))\n\t\t\treturn 0;\n\n\t\tif (block_group->flags & BTRFS_BLOCK_GROUP_SYSTEM)\n\t\t\ttgt = &fs_info->active_system_bg;\n\t\telse\n\t\t\ttgt = &fs_info->active_meta_bg;\n\t\tif (check_bg_is_active(ctx, tgt))\n\t\t\treturn 0;\n\t}\n\n\t \n\tif (block_group->meta_write_pointer > eb->start)\n\t\treturn -EBUSY;\n\n\t \n\tif (wbc->sync_mode == WB_SYNC_ALL && !wbc->for_sync)\n\t\treturn -EAGAIN;\n\treturn -EBUSY;\n}\n\nint btrfs_zoned_issue_zeroout(struct btrfs_device *device, u64 physical, u64 length)\n{\n\tif (!btrfs_dev_is_sequential(device, physical))\n\t\treturn -EOPNOTSUPP;\n\n\treturn blkdev_issue_zeroout(device->bdev, physical >> SECTOR_SHIFT,\n\t\t\t\t    length >> SECTOR_SHIFT, GFP_NOFS, 0);\n}\n\nstatic int read_zone_info(struct btrfs_fs_info *fs_info, u64 logical,\n\t\t\t  struct blk_zone *zone)\n{\n\tstruct btrfs_io_context *bioc = NULL;\n\tu64 mapped_length = PAGE_SIZE;\n\tunsigned int nofs_flag;\n\tint nmirrors;\n\tint i, ret;\n\n\tret = btrfs_map_block(fs_info, BTRFS_MAP_GET_READ_MIRRORS, logical,\n\t\t\t      &mapped_length, &bioc, NULL, NULL, 1);\n\tif (ret || !bioc || mapped_length < PAGE_SIZE) {\n\t\tret = -EIO;\n\t\tgoto out_put_bioc;\n\t}\n\n\tif (bioc->map_type & BTRFS_BLOCK_GROUP_RAID56_MASK) {\n\t\tret = -EINVAL;\n\t\tgoto out_put_bioc;\n\t}\n\n\tnofs_flag = memalloc_nofs_save();\n\tnmirrors = (int)bioc->num_stripes;\n\tfor (i = 0; i < nmirrors; i++) {\n\t\tu64 physical = bioc->stripes[i].physical;\n\t\tstruct btrfs_device *dev = bioc->stripes[i].dev;\n\n\t\t \n\t\tif (!dev->bdev)\n\t\t\tcontinue;\n\n\t\tret = btrfs_get_dev_zone(dev, physical, zone);\n\t\t \n\t\tif (ret == -EIO || ret == -EOPNOTSUPP)\n\t\t\tcontinue;\n\t\tbreak;\n\t}\n\tmemalloc_nofs_restore(nofs_flag);\nout_put_bioc:\n\tbtrfs_put_bioc(bioc);\n\treturn ret;\n}\n\n \nint btrfs_sync_zone_write_pointer(struct btrfs_device *tgt_dev, u64 logical,\n\t\t\t\t    u64 physical_start, u64 physical_pos)\n{\n\tstruct btrfs_fs_info *fs_info = tgt_dev->fs_info;\n\tstruct blk_zone zone;\n\tu64 length;\n\tu64 wp;\n\tint ret;\n\n\tif (!btrfs_dev_is_sequential(tgt_dev, physical_pos))\n\t\treturn 0;\n\n\tret = read_zone_info(fs_info, logical, &zone);\n\tif (ret)\n\t\treturn ret;\n\n\twp = physical_start + ((zone.wp - zone.start) << SECTOR_SHIFT);\n\n\tif (physical_pos == wp)\n\t\treturn 0;\n\n\tif (physical_pos > wp)\n\t\treturn -EUCLEAN;\n\n\tlength = wp - physical_pos;\n\treturn btrfs_zoned_issue_zeroout(tgt_dev, physical_pos, length);\n}\n\n \nbool btrfs_zone_activate(struct btrfs_block_group *block_group)\n{\n\tstruct btrfs_fs_info *fs_info = block_group->fs_info;\n\tstruct map_lookup *map;\n\tstruct btrfs_device *device;\n\tu64 physical;\n\tconst bool is_data = (block_group->flags & BTRFS_BLOCK_GROUP_DATA);\n\tbool ret;\n\tint i;\n\n\tif (!btrfs_is_zoned(block_group->fs_info))\n\t\treturn true;\n\n\tmap = block_group->physical_map;\n\n\tspin_lock(&block_group->lock);\n\tif (test_bit(BLOCK_GROUP_FLAG_ZONE_IS_ACTIVE, &block_group->runtime_flags)) {\n\t\tret = true;\n\t\tgoto out_unlock;\n\t}\n\n\t \n\tif (btrfs_zoned_bg_is_full(block_group)) {\n\t\tret = false;\n\t\tgoto out_unlock;\n\t}\n\n\tspin_lock(&fs_info->zone_active_bgs_lock);\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tstruct btrfs_zoned_device_info *zinfo;\n\t\tint reserved = 0;\n\n\t\tdevice = map->stripes[i].dev;\n\t\tphysical = map->stripes[i].physical;\n\t\tzinfo = device->zone_info;\n\n\t\tif (zinfo->max_active_zones == 0)\n\t\t\tcontinue;\n\n\t\tif (is_data)\n\t\t\treserved = zinfo->reserved_active_zones;\n\t\t \n\t\tif (atomic_read(&zinfo->active_zones_left) <= reserved) {\n\t\t\tret = false;\n\t\t\tspin_unlock(&fs_info->zone_active_bgs_lock);\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tif (!btrfs_dev_set_active_zone(device, physical)) {\n\t\t\t \n\t\t\tret = false;\n\t\t\tspin_unlock(&fs_info->zone_active_bgs_lock);\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tif (!is_data)\n\t\t\tzinfo->reserved_active_zones--;\n\t}\n\tspin_unlock(&fs_info->zone_active_bgs_lock);\n\n\t \n\tset_bit(BLOCK_GROUP_FLAG_ZONE_IS_ACTIVE, &block_group->runtime_flags);\n\tspin_unlock(&block_group->lock);\n\n\t \n\tbtrfs_get_block_group(block_group);\n\n\tspin_lock(&fs_info->zone_active_bgs_lock);\n\tlist_add_tail(&block_group->active_bg_list, &fs_info->zone_active_bgs);\n\tspin_unlock(&fs_info->zone_active_bgs_lock);\n\n\treturn true;\n\nout_unlock:\n\tspin_unlock(&block_group->lock);\n\treturn ret;\n}\n\nstatic void wait_eb_writebacks(struct btrfs_block_group *block_group)\n{\n\tstruct btrfs_fs_info *fs_info = block_group->fs_info;\n\tconst u64 end = block_group->start + block_group->length;\n\tstruct radix_tree_iter iter;\n\tstruct extent_buffer *eb;\n\tvoid __rcu **slot;\n\n\trcu_read_lock();\n\tradix_tree_for_each_slot(slot, &fs_info->buffer_radix, &iter,\n\t\t\t\t block_group->start >> fs_info->sectorsize_bits) {\n\t\teb = radix_tree_deref_slot(slot);\n\t\tif (!eb)\n\t\t\tcontinue;\n\t\tif (radix_tree_deref_retry(eb)) {\n\t\t\tslot = radix_tree_iter_retry(&iter);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (eb->start < block_group->start)\n\t\t\tcontinue;\n\t\tif (eb->start >= end)\n\t\t\tbreak;\n\n\t\tslot = radix_tree_iter_resume(slot, &iter);\n\t\trcu_read_unlock();\n\t\twait_on_extent_buffer_writeback(eb);\n\t\trcu_read_lock();\n\t}\n\trcu_read_unlock();\n}\n\nstatic int do_zone_finish(struct btrfs_block_group *block_group, bool fully_written)\n{\n\tstruct btrfs_fs_info *fs_info = block_group->fs_info;\n\tstruct map_lookup *map;\n\tconst bool is_metadata = (block_group->flags &\n\t\t\t(BTRFS_BLOCK_GROUP_METADATA | BTRFS_BLOCK_GROUP_SYSTEM));\n\tint ret = 0;\n\tint i;\n\n\tspin_lock(&block_group->lock);\n\tif (!test_bit(BLOCK_GROUP_FLAG_ZONE_IS_ACTIVE, &block_group->runtime_flags)) {\n\t\tspin_unlock(&block_group->lock);\n\t\treturn 0;\n\t}\n\n\t \n\tif (is_metadata &&\n\t    block_group->start + block_group->alloc_offset > block_group->meta_write_pointer) {\n\t\tspin_unlock(&block_group->lock);\n\t\treturn -EAGAIN;\n\t}\n\n\t \n\tif (!fully_written) {\n\t\tif (test_bit(BLOCK_GROUP_FLAG_ZONED_DATA_RELOC, &block_group->runtime_flags)) {\n\t\t\tspin_unlock(&block_group->lock);\n\t\t\treturn -EAGAIN;\n\t\t}\n\t\tspin_unlock(&block_group->lock);\n\n\t\tret = btrfs_inc_block_group_ro(block_group, false);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\t \n\t\tbtrfs_wait_block_group_reservations(block_group);\n\t\t \n\t\tbtrfs_wait_ordered_roots(fs_info, U64_MAX, block_group->start,\n\t\t\t\t\t block_group->length);\n\t\t \n\t\tif (is_metadata)\n\t\t\twait_eb_writebacks(block_group);\n\n\t\tspin_lock(&block_group->lock);\n\n\t\t \n\t\tif (!test_bit(BLOCK_GROUP_FLAG_ZONE_IS_ACTIVE,\n\t\t\t      &block_group->runtime_flags)) {\n\t\t\tspin_unlock(&block_group->lock);\n\t\t\tbtrfs_dec_block_group_ro(block_group);\n\t\t\treturn 0;\n\t\t}\n\n\t\tif (block_group->reserved ||\n\t\t    test_bit(BLOCK_GROUP_FLAG_ZONED_DATA_RELOC,\n\t\t\t     &block_group->runtime_flags)) {\n\t\t\tspin_unlock(&block_group->lock);\n\t\t\tbtrfs_dec_block_group_ro(block_group);\n\t\t\treturn -EAGAIN;\n\t\t}\n\t}\n\n\tclear_bit(BLOCK_GROUP_FLAG_ZONE_IS_ACTIVE, &block_group->runtime_flags);\n\tblock_group->alloc_offset = block_group->zone_capacity;\n\tif (block_group->flags & (BTRFS_BLOCK_GROUP_METADATA | BTRFS_BLOCK_GROUP_SYSTEM))\n\t\tblock_group->meta_write_pointer = block_group->start +\n\t\t\t\t\t\t  block_group->zone_capacity;\n\tblock_group->free_space_ctl->free_space = 0;\n\tbtrfs_clear_treelog_bg(block_group);\n\tbtrfs_clear_data_reloc_bg(block_group);\n\tspin_unlock(&block_group->lock);\n\n\tmap = block_group->physical_map;\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tstruct btrfs_device *device = map->stripes[i].dev;\n\t\tconst u64 physical = map->stripes[i].physical;\n\t\tstruct btrfs_zoned_device_info *zinfo = device->zone_info;\n\n\t\tif (zinfo->max_active_zones == 0)\n\t\t\tcontinue;\n\n\t\tret = blkdev_zone_mgmt(device->bdev, REQ_OP_ZONE_FINISH,\n\t\t\t\t       physical >> SECTOR_SHIFT,\n\t\t\t\t       zinfo->zone_size >> SECTOR_SHIFT,\n\t\t\t\t       GFP_NOFS);\n\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tif (!(block_group->flags & BTRFS_BLOCK_GROUP_DATA))\n\t\t\tzinfo->reserved_active_zones++;\n\t\tbtrfs_dev_clear_active_zone(device, physical);\n\t}\n\n\tif (!fully_written)\n\t\tbtrfs_dec_block_group_ro(block_group);\n\n\tspin_lock(&fs_info->zone_active_bgs_lock);\n\tASSERT(!list_empty(&block_group->active_bg_list));\n\tlist_del_init(&block_group->active_bg_list);\n\tspin_unlock(&fs_info->zone_active_bgs_lock);\n\n\t \n\tbtrfs_put_block_group(block_group);\n\n\tclear_and_wake_up_bit(BTRFS_FS_NEED_ZONE_FINISH, &fs_info->flags);\n\n\treturn 0;\n}\n\nint btrfs_zone_finish(struct btrfs_block_group *block_group)\n{\n\tif (!btrfs_is_zoned(block_group->fs_info))\n\t\treturn 0;\n\n\treturn do_zone_finish(block_group, false);\n}\n\nbool btrfs_can_activate_zone(struct btrfs_fs_devices *fs_devices, u64 flags)\n{\n\tstruct btrfs_fs_info *fs_info = fs_devices->fs_info;\n\tstruct btrfs_device *device;\n\tbool ret = false;\n\n\tif (!btrfs_is_zoned(fs_info))\n\t\treturn true;\n\n\t \n\tmutex_lock(&fs_info->chunk_mutex);\n\tspin_lock(&fs_info->zone_active_bgs_lock);\n\tlist_for_each_entry(device, &fs_devices->alloc_list, dev_alloc_list) {\n\t\tstruct btrfs_zoned_device_info *zinfo = device->zone_info;\n\t\tint reserved = 0;\n\n\t\tif (!device->bdev)\n\t\t\tcontinue;\n\n\t\tif (!zinfo->max_active_zones) {\n\t\t\tret = true;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (flags & BTRFS_BLOCK_GROUP_DATA)\n\t\t\treserved = zinfo->reserved_active_zones;\n\n\t\tswitch (flags & BTRFS_BLOCK_GROUP_PROFILE_MASK) {\n\t\tcase 0:  \n\t\t\tret = (atomic_read(&zinfo->active_zones_left) >= (1 + reserved));\n\t\t\tbreak;\n\t\tcase BTRFS_BLOCK_GROUP_DUP:\n\t\t\tret = (atomic_read(&zinfo->active_zones_left) >= (2 + reserved));\n\t\t\tbreak;\n\t\t}\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\tspin_unlock(&fs_info->zone_active_bgs_lock);\n\tmutex_unlock(&fs_info->chunk_mutex);\n\n\tif (!ret)\n\t\tset_bit(BTRFS_FS_NEED_ZONE_FINISH, &fs_info->flags);\n\n\treturn ret;\n}\n\nvoid btrfs_zone_finish_endio(struct btrfs_fs_info *fs_info, u64 logical, u64 length)\n{\n\tstruct btrfs_block_group *block_group;\n\tu64 min_alloc_bytes;\n\n\tif (!btrfs_is_zoned(fs_info))\n\t\treturn;\n\n\tblock_group = btrfs_lookup_block_group(fs_info, logical);\n\tASSERT(block_group);\n\n\t \n\tif (block_group->flags & BTRFS_BLOCK_GROUP_DATA)\n\t\tmin_alloc_bytes = fs_info->sectorsize;\n\telse\n\t\tmin_alloc_bytes = fs_info->nodesize;\n\n\t \n\tif (logical + length + min_alloc_bytes <=\n\t    block_group->start + block_group->zone_capacity)\n\t\tgoto out;\n\n\tdo_zone_finish(block_group, true);\n\nout:\n\tbtrfs_put_block_group(block_group);\n}\n\nstatic void btrfs_zone_finish_endio_workfn(struct work_struct *work)\n{\n\tstruct btrfs_block_group *bg =\n\t\tcontainer_of(work, struct btrfs_block_group, zone_finish_work);\n\n\twait_on_extent_buffer_writeback(bg->last_eb);\n\tfree_extent_buffer(bg->last_eb);\n\tbtrfs_zone_finish_endio(bg->fs_info, bg->start, bg->length);\n\tbtrfs_put_block_group(bg);\n}\n\nvoid btrfs_schedule_zone_finish_bg(struct btrfs_block_group *bg,\n\t\t\t\t   struct extent_buffer *eb)\n{\n\tif (!test_bit(BLOCK_GROUP_FLAG_SEQUENTIAL_ZONE, &bg->runtime_flags) ||\n\t    eb->start + eb->len * 2 <= bg->start + bg->zone_capacity)\n\t\treturn;\n\n\tif (WARN_ON(bg->zone_finish_work.func == btrfs_zone_finish_endio_workfn)) {\n\t\tbtrfs_err(bg->fs_info, \"double scheduling of bg %llu zone finishing\",\n\t\t\t  bg->start);\n\t\treturn;\n\t}\n\n\t \n\tbtrfs_get_block_group(bg);\n\tatomic_inc(&eb->refs);\n\tbg->last_eb = eb;\n\tINIT_WORK(&bg->zone_finish_work, btrfs_zone_finish_endio_workfn);\n\tqueue_work(system_unbound_wq, &bg->zone_finish_work);\n}\n\nvoid btrfs_clear_data_reloc_bg(struct btrfs_block_group *bg)\n{\n\tstruct btrfs_fs_info *fs_info = bg->fs_info;\n\n\tspin_lock(&fs_info->relocation_bg_lock);\n\tif (fs_info->data_reloc_bg == bg->start)\n\t\tfs_info->data_reloc_bg = 0;\n\tspin_unlock(&fs_info->relocation_bg_lock);\n}\n\nvoid btrfs_free_zone_cache(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tstruct btrfs_device *device;\n\n\tif (!btrfs_is_zoned(fs_info))\n\t\treturn;\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_for_each_entry(device, &fs_devices->devices, dev_list) {\n\t\tif (device->zone_info) {\n\t\t\tvfree(device->zone_info->zone_cache);\n\t\t\tdevice->zone_info->zone_cache = NULL;\n\t\t}\n\t}\n\tmutex_unlock(&fs_devices->device_list_mutex);\n}\n\nbool btrfs_zoned_should_reclaim(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tstruct btrfs_device *device;\n\tu64 used = 0;\n\tu64 total = 0;\n\tu64 factor;\n\n\tASSERT(btrfs_is_zoned(fs_info));\n\n\tif (fs_info->bg_reclaim_threshold == 0)\n\t\treturn false;\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_for_each_entry(device, &fs_devices->devices, dev_list) {\n\t\tif (!device->bdev)\n\t\t\tcontinue;\n\n\t\ttotal += device->disk_total_bytes;\n\t\tused += device->bytes_used;\n\t}\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\tfactor = div64_u64(used * 100, total);\n\treturn factor >= fs_info->bg_reclaim_threshold;\n}\n\nvoid btrfs_zoned_release_data_reloc_bg(struct btrfs_fs_info *fs_info, u64 logical,\n\t\t\t\t       u64 length)\n{\n\tstruct btrfs_block_group *block_group;\n\n\tif (!btrfs_is_zoned(fs_info))\n\t\treturn;\n\n\tblock_group = btrfs_lookup_block_group(fs_info, logical);\n\t \n\tASSERT(block_group && (block_group->flags & BTRFS_BLOCK_GROUP_DATA));\n\n\tspin_lock(&block_group->lock);\n\tif (!test_bit(BLOCK_GROUP_FLAG_ZONED_DATA_RELOC, &block_group->runtime_flags))\n\t\tgoto out;\n\n\t \n\tif (block_group->start + block_group->alloc_offset == logical + length) {\n\t\t \n\t\tclear_bit(BLOCK_GROUP_FLAG_ZONED_DATA_RELOC,\n\t\t\t  &block_group->runtime_flags);\n\t}\n\nout:\n\tspin_unlock(&block_group->lock);\n\tbtrfs_put_block_group(block_group);\n}\n\nint btrfs_zone_finish_one_bg(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_block_group *block_group;\n\tstruct btrfs_block_group *min_bg = NULL;\n\tu64 min_avail = U64_MAX;\n\tint ret;\n\n\tspin_lock(&fs_info->zone_active_bgs_lock);\n\tlist_for_each_entry(block_group, &fs_info->zone_active_bgs,\n\t\t\t    active_bg_list) {\n\t\tu64 avail;\n\n\t\tspin_lock(&block_group->lock);\n\t\tif (block_group->reserved || block_group->alloc_offset == 0 ||\n\t\t    (block_group->flags & BTRFS_BLOCK_GROUP_SYSTEM) ||\n\t\t    test_bit(BLOCK_GROUP_FLAG_ZONED_DATA_RELOC, &block_group->runtime_flags)) {\n\t\t\tspin_unlock(&block_group->lock);\n\t\t\tcontinue;\n\t\t}\n\n\t\tavail = block_group->zone_capacity - block_group->alloc_offset;\n\t\tif (min_avail > avail) {\n\t\t\tif (min_bg)\n\t\t\t\tbtrfs_put_block_group(min_bg);\n\t\t\tmin_bg = block_group;\n\t\t\tmin_avail = avail;\n\t\t\tbtrfs_get_block_group(min_bg);\n\t\t}\n\t\tspin_unlock(&block_group->lock);\n\t}\n\tspin_unlock(&fs_info->zone_active_bgs_lock);\n\n\tif (!min_bg)\n\t\treturn 0;\n\n\tret = btrfs_zone_finish(min_bg);\n\tbtrfs_put_block_group(min_bg);\n\n\treturn ret < 0 ? ret : 1;\n}\n\nint btrfs_zoned_activate_one_bg(struct btrfs_fs_info *fs_info,\n\t\t\t\tstruct btrfs_space_info *space_info,\n\t\t\t\tbool do_finish)\n{\n\tstruct btrfs_block_group *bg;\n\tint index;\n\n\tif (!btrfs_is_zoned(fs_info) || (space_info->flags & BTRFS_BLOCK_GROUP_DATA))\n\t\treturn 0;\n\n\tfor (;;) {\n\t\tint ret;\n\t\tbool need_finish = false;\n\n\t\tdown_read(&space_info->groups_sem);\n\t\tfor (index = 0; index < BTRFS_NR_RAID_TYPES; index++) {\n\t\t\tlist_for_each_entry(bg, &space_info->block_groups[index],\n\t\t\t\t\t    list) {\n\t\t\t\tif (!spin_trylock(&bg->lock))\n\t\t\t\t\tcontinue;\n\t\t\t\tif (btrfs_zoned_bg_is_full(bg) ||\n\t\t\t\t    test_bit(BLOCK_GROUP_FLAG_ZONE_IS_ACTIVE,\n\t\t\t\t\t     &bg->runtime_flags)) {\n\t\t\t\t\tspin_unlock(&bg->lock);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tspin_unlock(&bg->lock);\n\n\t\t\t\tif (btrfs_zone_activate(bg)) {\n\t\t\t\t\tup_read(&space_info->groups_sem);\n\t\t\t\t\treturn 1;\n\t\t\t\t}\n\n\t\t\t\tneed_finish = true;\n\t\t\t}\n\t\t}\n\t\tup_read(&space_info->groups_sem);\n\n\t\tif (!do_finish || !need_finish)\n\t\t\tbreak;\n\n\t\tret = btrfs_zone_finish_one_bg(fs_info);\n\t\tif (ret == 0)\n\t\t\tbreak;\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\n \nvoid btrfs_check_active_zone_reservation(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tstruct btrfs_block_group *block_group;\n\tstruct btrfs_device *device;\n\t \n\tunsigned int metadata_reserve = 2;\n\t \n\tunsigned int system_reserve = 1;\n\n\tif (!test_bit(BTRFS_FS_ACTIVE_ZONE_TRACKING, &fs_info->flags))\n\t\treturn;\n\n\t \n\tif (fs_info->avail_metadata_alloc_bits & BTRFS_BLOCK_GROUP_DUP)\n\t\tmetadata_reserve = 4;\n\tif (fs_info->avail_system_alloc_bits & BTRFS_BLOCK_GROUP_DUP)\n\t\tsystem_reserve = 2;\n\n\t \n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_for_each_entry(device, &fs_devices->devices, dev_list) {\n\t\tif (!device->bdev)\n\t\t\tcontinue;\n\n\t\tdevice->zone_info->reserved_active_zones =\n\t\t\tmetadata_reserve + system_reserve;\n\t}\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\t \n\tspin_lock(&fs_info->zone_active_bgs_lock);\n\tlist_for_each_entry(block_group, &fs_info->zone_active_bgs, active_bg_list) {\n\t\tstruct map_lookup *map = block_group->physical_map;\n\n\t\tif (!(block_group->flags &\n\t\t      (BTRFS_BLOCK_GROUP_METADATA | BTRFS_BLOCK_GROUP_SYSTEM)))\n\t\t\tcontinue;\n\n\t\tfor (int i = 0; i < map->num_stripes; i++)\n\t\t\tmap->stripes[i].dev->zone_info->reserved_active_zones--;\n\t}\n\tspin_unlock(&fs_info->zone_active_bgs_lock);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}