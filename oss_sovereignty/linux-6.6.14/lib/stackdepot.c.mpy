{
  "module_name": "stackdepot.c",
  "hash_id": "836d4fb120c2af7443d209a0396ae092e67530f3fab0bb2520806f275eec8df4",
  "original_prompt": "Ingested from linux-6.6.14/lib/stackdepot.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) \"stackdepot: \" fmt\n\n#include <linux/gfp.h>\n#include <linux/jhash.h>\n#include <linux/kernel.h>\n#include <linux/kmsan.h>\n#include <linux/mm.h>\n#include <linux/mutex.h>\n#include <linux/percpu.h>\n#include <linux/printk.h>\n#include <linux/slab.h>\n#include <linux/stacktrace.h>\n#include <linux/stackdepot.h>\n#include <linux/string.h>\n#include <linux/types.h>\n#include <linux/memblock.h>\n#include <linux/kasan-enabled.h>\n\n#define DEPOT_HANDLE_BITS (sizeof(depot_stack_handle_t) * 8)\n\n#define DEPOT_VALID_BITS 1\n#define DEPOT_POOL_ORDER 2  \n#define DEPOT_POOL_SIZE (1LL << (PAGE_SHIFT + DEPOT_POOL_ORDER))\n#define DEPOT_STACK_ALIGN 4\n#define DEPOT_OFFSET_BITS (DEPOT_POOL_ORDER + PAGE_SHIFT - DEPOT_STACK_ALIGN)\n#define DEPOT_POOL_INDEX_BITS (DEPOT_HANDLE_BITS - DEPOT_VALID_BITS - \\\n\t\t\t       DEPOT_OFFSET_BITS - STACK_DEPOT_EXTRA_BITS)\n#define DEPOT_POOLS_CAP 8192\n#define DEPOT_MAX_POOLS \\\n\t(((1LL << (DEPOT_POOL_INDEX_BITS)) < DEPOT_POOLS_CAP) ? \\\n\t (1LL << (DEPOT_POOL_INDEX_BITS)) : DEPOT_POOLS_CAP)\n\n \nunion handle_parts {\n\tdepot_stack_handle_t handle;\n\tstruct {\n\t\tu32 pool_index\t: DEPOT_POOL_INDEX_BITS;\n\t\tu32 offset\t: DEPOT_OFFSET_BITS;\n\t\tu32 valid\t: DEPOT_VALID_BITS;\n\t\tu32 extra\t: STACK_DEPOT_EXTRA_BITS;\n\t};\n};\n\nstruct stack_record {\n\tstruct stack_record *next;\t \n\tu32 hash;\t\t\t \n\tu32 size;\t\t\t \n\tunion handle_parts handle;\n\tunsigned long entries[];\t \n};\n\nstatic bool stack_depot_disabled;\nstatic bool __stack_depot_early_init_requested __initdata = IS_ENABLED(CONFIG_STACKDEPOT_ALWAYS_INIT);\nstatic bool __stack_depot_early_init_passed __initdata;\n\n \n#define STACK_HASH_TABLE_SCALE 14\n \n#define STACK_BUCKET_NUMBER_ORDER_MIN 12\n#define STACK_BUCKET_NUMBER_ORDER_MAX 20\n \n#define STACK_HASH_SEED 0x9747b28c\n\n \nstatic struct stack_record **stack_table;\n \nstatic unsigned int stack_bucket_number_order;\n \nstatic unsigned int stack_hash_mask;\n\n \nstatic void *stack_pools[DEPOT_MAX_POOLS];\n \nstatic int pool_index;\n \nstatic size_t pool_offset;\n \nstatic DEFINE_RAW_SPINLOCK(pool_lock);\n \nstatic int next_pool_required = 1;\n\nstatic int __init disable_stack_depot(char *str)\n{\n\tint ret;\n\n\tret = kstrtobool(str, &stack_depot_disabled);\n\tif (!ret && stack_depot_disabled) {\n\t\tpr_info(\"disabled\\n\");\n\t\tstack_table = NULL;\n\t}\n\treturn 0;\n}\nearly_param(\"stack_depot_disable\", disable_stack_depot);\n\nvoid __init stack_depot_request_early_init(void)\n{\n\t \n\tWARN_ON(__stack_depot_early_init_passed);\n\n\t__stack_depot_early_init_requested = true;\n}\n\n \nint __init stack_depot_early_init(void)\n{\n\tunsigned long entries = 0;\n\n\t \n\tif (WARN_ON(__stack_depot_early_init_passed))\n\t\treturn 0;\n\t__stack_depot_early_init_passed = true;\n\n\t \n\tif (kasan_enabled() && !stack_bucket_number_order)\n\t\tstack_bucket_number_order = STACK_BUCKET_NUMBER_ORDER_MAX;\n\n\tif (!__stack_depot_early_init_requested || stack_depot_disabled)\n\t\treturn 0;\n\n\t \n\tif (stack_bucket_number_order)\n\t\tentries = 1UL << stack_bucket_number_order;\n\tpr_info(\"allocating hash table via alloc_large_system_hash\\n\");\n\tstack_table = alloc_large_system_hash(\"stackdepot\",\n\t\t\t\t\t\tsizeof(struct stack_record *),\n\t\t\t\t\t\tentries,\n\t\t\t\t\t\tSTACK_HASH_TABLE_SCALE,\n\t\t\t\t\t\tHASH_EARLY | HASH_ZERO,\n\t\t\t\t\t\tNULL,\n\t\t\t\t\t\t&stack_hash_mask,\n\t\t\t\t\t\t1UL << STACK_BUCKET_NUMBER_ORDER_MIN,\n\t\t\t\t\t\t1UL << STACK_BUCKET_NUMBER_ORDER_MAX);\n\tif (!stack_table) {\n\t\tpr_err(\"hash table allocation failed, disabling\\n\");\n\t\tstack_depot_disabled = true;\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\n \nint stack_depot_init(void)\n{\n\tstatic DEFINE_MUTEX(stack_depot_init_mutex);\n\tunsigned long entries;\n\tint ret = 0;\n\n\tmutex_lock(&stack_depot_init_mutex);\n\n\tif (stack_depot_disabled || stack_table)\n\t\tgoto out_unlock;\n\n\t \n\tif (stack_bucket_number_order) {\n\t\tentries = 1UL << stack_bucket_number_order;\n\t} else {\n\t\tint scale = STACK_HASH_TABLE_SCALE;\n\n\t\tentries = nr_free_buffer_pages();\n\t\tentries = roundup_pow_of_two(entries);\n\n\t\tif (scale > PAGE_SHIFT)\n\t\t\tentries >>= (scale - PAGE_SHIFT);\n\t\telse\n\t\t\tentries <<= (PAGE_SHIFT - scale);\n\t}\n\n\tif (entries < 1UL << STACK_BUCKET_NUMBER_ORDER_MIN)\n\t\tentries = 1UL << STACK_BUCKET_NUMBER_ORDER_MIN;\n\tif (entries > 1UL << STACK_BUCKET_NUMBER_ORDER_MAX)\n\t\tentries = 1UL << STACK_BUCKET_NUMBER_ORDER_MAX;\n\n\tpr_info(\"allocating hash table of %lu entries via kvcalloc\\n\", entries);\n\tstack_table = kvcalloc(entries, sizeof(struct stack_record *), GFP_KERNEL);\n\tif (!stack_table) {\n\t\tpr_err(\"hash table allocation failed, disabling\\n\");\n\t\tstack_depot_disabled = true;\n\t\tret = -ENOMEM;\n\t\tgoto out_unlock;\n\t}\n\tstack_hash_mask = entries - 1;\n\nout_unlock:\n\tmutex_unlock(&stack_depot_init_mutex);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(stack_depot_init);\n\n \nstatic void depot_init_pool(void **prealloc)\n{\n\t \n\tif (!smp_load_acquire(&next_pool_required))\n\t\treturn;\n\n\t \n\tif (stack_pools[pool_index] == NULL) {\n\t\t \n\t\tstack_pools[pool_index] = *prealloc;\n\t\t*prealloc = NULL;\n\t} else {\n\t\t \n\t\tif (pool_index + 1 < DEPOT_MAX_POOLS) {\n\t\t\tstack_pools[pool_index + 1] = *prealloc;\n\t\t\t*prealloc = NULL;\n\t\t}\n\t\t \n\t\tsmp_store_release(&next_pool_required, 0);\n\t}\n}\n\n \nstatic struct stack_record *\ndepot_alloc_stack(unsigned long *entries, int size, u32 hash, void **prealloc)\n{\n\tstruct stack_record *stack;\n\tsize_t required_size = struct_size(stack, entries, size);\n\n\trequired_size = ALIGN(required_size, 1 << DEPOT_STACK_ALIGN);\n\n\t \n\tif (unlikely(pool_offset + required_size > DEPOT_POOL_SIZE)) {\n\t\t \n\t\tif (unlikely(pool_index + 1 >= DEPOT_MAX_POOLS)) {\n\t\t\tWARN_ONCE(1, \"Stack depot reached limit capacity\");\n\t\t\treturn NULL;\n\t\t}\n\n\t\t \n\t\tWRITE_ONCE(pool_index, pool_index + 1);\n\t\tpool_offset = 0;\n\t\t \n\t\tif (pool_index + 1 < DEPOT_MAX_POOLS)\n\t\t\tsmp_store_release(&next_pool_required, 1);\n\t}\n\n\t \n\tif (*prealloc)\n\t\tdepot_init_pool(prealloc);\n\n\t \n\tif (stack_pools[pool_index] == NULL)\n\t\treturn NULL;\n\n\t \n\tstack = stack_pools[pool_index] + pool_offset;\n\tstack->hash = hash;\n\tstack->size = size;\n\tstack->handle.pool_index = pool_index;\n\tstack->handle.offset = pool_offset >> DEPOT_STACK_ALIGN;\n\tstack->handle.valid = 1;\n\tstack->handle.extra = 0;\n\tmemcpy(stack->entries, entries, flex_array_size(stack, entries, size));\n\tpool_offset += required_size;\n\t \n\tkmsan_unpoison_memory(stack, required_size);\n\n\treturn stack;\n}\n\n \nstatic inline u32 hash_stack(unsigned long *entries, unsigned int size)\n{\n\treturn jhash2((u32 *)entries,\n\t\t      array_size(size,  sizeof(*entries)) / sizeof(u32),\n\t\t      STACK_HASH_SEED);\n}\n\n \nstatic inline\nint stackdepot_memcmp(const unsigned long *u1, const unsigned long *u2,\n\t\t\tunsigned int n)\n{\n\tfor ( ; n-- ; u1++, u2++) {\n\t\tif (*u1 != *u2)\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}\n\n \nstatic inline struct stack_record *find_stack(struct stack_record *bucket,\n\t\t\t\t\t     unsigned long *entries, int size,\n\t\t\t\t\t     u32 hash)\n{\n\tstruct stack_record *found;\n\n\tfor (found = bucket; found; found = found->next) {\n\t\tif (found->hash == hash &&\n\t\t    found->size == size &&\n\t\t    !stackdepot_memcmp(entries, found->entries, size))\n\t\t\treturn found;\n\t}\n\treturn NULL;\n}\n\ndepot_stack_handle_t __stack_depot_save(unsigned long *entries,\n\t\t\t\t\tunsigned int nr_entries,\n\t\t\t\t\tgfp_t alloc_flags, bool can_alloc)\n{\n\tstruct stack_record *found = NULL, **bucket;\n\tunion handle_parts retval = { .handle = 0 };\n\tstruct page *page = NULL;\n\tvoid *prealloc = NULL;\n\tunsigned long flags;\n\tu32 hash;\n\n\t \n\tnr_entries = filter_irq_stacks(entries, nr_entries);\n\n\tif (unlikely(nr_entries == 0) || stack_depot_disabled)\n\t\tgoto fast_exit;\n\n\thash = hash_stack(entries, nr_entries);\n\tbucket = &stack_table[hash & stack_hash_mask];\n\n\t \n\tfound = find_stack(smp_load_acquire(bucket), entries, nr_entries, hash);\n\tif (found)\n\t\tgoto exit;\n\n\t \n\tif (unlikely(can_alloc && smp_load_acquire(&next_pool_required))) {\n\t\t \n\t\talloc_flags &= ~GFP_ZONEMASK;\n\t\talloc_flags &= (GFP_ATOMIC | GFP_KERNEL);\n\t\talloc_flags |= __GFP_NOWARN;\n\t\tpage = alloc_pages(alloc_flags, DEPOT_POOL_ORDER);\n\t\tif (page)\n\t\t\tprealloc = page_address(page);\n\t}\n\n\traw_spin_lock_irqsave(&pool_lock, flags);\n\n\tfound = find_stack(*bucket, entries, nr_entries, hash);\n\tif (!found) {\n\t\tstruct stack_record *new =\n\t\t\tdepot_alloc_stack(entries, nr_entries, hash, &prealloc);\n\n\t\tif (new) {\n\t\t\tnew->next = *bucket;\n\t\t\t \n\t\t\tsmp_store_release(bucket, new);\n\t\t\tfound = new;\n\t\t}\n\t} else if (prealloc) {\n\t\t \n\t\tdepot_init_pool(&prealloc);\n\t}\n\n\traw_spin_unlock_irqrestore(&pool_lock, flags);\nexit:\n\tif (prealloc) {\n\t\t \n\t\tfree_pages((unsigned long)prealloc, DEPOT_POOL_ORDER);\n\t}\n\tif (found)\n\t\tretval.handle = found->handle.handle;\nfast_exit:\n\treturn retval.handle;\n}\nEXPORT_SYMBOL_GPL(__stack_depot_save);\n\ndepot_stack_handle_t stack_depot_save(unsigned long *entries,\n\t\t\t\t      unsigned int nr_entries,\n\t\t\t\t      gfp_t alloc_flags)\n{\n\treturn __stack_depot_save(entries, nr_entries, alloc_flags, true);\n}\nEXPORT_SYMBOL_GPL(stack_depot_save);\n\nunsigned int stack_depot_fetch(depot_stack_handle_t handle,\n\t\t\t       unsigned long **entries)\n{\n\tunion handle_parts parts = { .handle = handle };\n\t \n\tint pool_index_cached = READ_ONCE(pool_index);\n\tvoid *pool;\n\tsize_t offset = parts.offset << DEPOT_STACK_ALIGN;\n\tstruct stack_record *stack;\n\n\t*entries = NULL;\n\t \n\tkmsan_unpoison_memory(entries, sizeof(*entries));\n\n\tif (!handle)\n\t\treturn 0;\n\n\tif (parts.pool_index > pool_index_cached) {\n\t\tWARN(1, \"pool index %d out of bounds (%d) for stack id %08x\\n\",\n\t\t\tparts.pool_index, pool_index_cached, handle);\n\t\treturn 0;\n\t}\n\tpool = stack_pools[parts.pool_index];\n\tif (!pool)\n\t\treturn 0;\n\tstack = pool + offset;\n\n\t*entries = stack->entries;\n\treturn stack->size;\n}\nEXPORT_SYMBOL_GPL(stack_depot_fetch);\n\nvoid stack_depot_print(depot_stack_handle_t stack)\n{\n\tunsigned long *entries;\n\tunsigned int nr_entries;\n\n\tnr_entries = stack_depot_fetch(stack, &entries);\n\tif (nr_entries > 0)\n\t\tstack_trace_print(entries, nr_entries, 0);\n}\nEXPORT_SYMBOL_GPL(stack_depot_print);\n\nint stack_depot_snprint(depot_stack_handle_t handle, char *buf, size_t size,\n\t\t       int spaces)\n{\n\tunsigned long *entries;\n\tunsigned int nr_entries;\n\n\tnr_entries = stack_depot_fetch(handle, &entries);\n\treturn nr_entries ? stack_trace_snprint(buf, size, entries, nr_entries,\n\t\t\t\t\t\tspaces) : 0;\n}\nEXPORT_SYMBOL_GPL(stack_depot_snprint);\n\ndepot_stack_handle_t __must_check stack_depot_set_extra_bits(\n\t\t\tdepot_stack_handle_t handle, unsigned int extra_bits)\n{\n\tunion handle_parts parts = { .handle = handle };\n\n\t \n\tif (!handle)\n\t\treturn 0;\n\n\tparts.extra = extra_bits;\n\treturn parts.handle;\n}\nEXPORT_SYMBOL(stack_depot_set_extra_bits);\n\nunsigned int stack_depot_get_extra_bits(depot_stack_handle_t handle)\n{\n\tunion handle_parts parts = { .handle = handle };\n\n\treturn parts.extra;\n}\nEXPORT_SYMBOL(stack_depot_get_extra_bits);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}