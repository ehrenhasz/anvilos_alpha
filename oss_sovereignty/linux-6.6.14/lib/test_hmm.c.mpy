{
  "module_name": "test_hmm.c",
  "hash_id": "4c673cc7e8a6b208d581b80df6fa83187cfbe64ef39e20fa6afdbb3a538c9273",
  "original_prompt": "Ingested from linux-6.6.14/lib/test_hmm.c",
  "human_readable_source": "\n \n#include <linux/init.h>\n#include <linux/fs.h>\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <linux/kernel.h>\n#include <linux/cdev.h>\n#include <linux/device.h>\n#include <linux/memremap.h>\n#include <linux/mutex.h>\n#include <linux/rwsem.h>\n#include <linux/sched.h>\n#include <linux/slab.h>\n#include <linux/highmem.h>\n#include <linux/delay.h>\n#include <linux/pagemap.h>\n#include <linux/hmm.h>\n#include <linux/vmalloc.h>\n#include <linux/swap.h>\n#include <linux/swapops.h>\n#include <linux/sched/mm.h>\n#include <linux/platform_device.h>\n#include <linux/rmap.h>\n#include <linux/mmu_notifier.h>\n#include <linux/migrate.h>\n\n#include \"test_hmm_uapi.h\"\n\n#define DMIRROR_NDEVICES\t\t4\n#define DMIRROR_RANGE_FAULT_TIMEOUT\t1000\n#define DEVMEM_CHUNK_SIZE\t\t(256 * 1024 * 1024U)\n#define DEVMEM_CHUNKS_RESERVE\t\t16\n\n \n#define BACKING_PAGE(page) (is_device_private_page((page)) ? \\\n\t\t\t   (page)->zone_device_data : (page))\n\nstatic unsigned long spm_addr_dev0;\nmodule_param(spm_addr_dev0, long, 0644);\nMODULE_PARM_DESC(spm_addr_dev0,\n\t\t\"Specify start address for SPM (special purpose memory) used for device 0. By setting this Coherent device type will be used. Make sure spm_addr_dev1 is set too. Minimum SPM size should be DEVMEM_CHUNK_SIZE.\");\n\nstatic unsigned long spm_addr_dev1;\nmodule_param(spm_addr_dev1, long, 0644);\nMODULE_PARM_DESC(spm_addr_dev1,\n\t\t\"Specify start address for SPM (special purpose memory) used for device 1. By setting this Coherent device type will be used. Make sure spm_addr_dev0 is set too. Minimum SPM size should be DEVMEM_CHUNK_SIZE.\");\n\nstatic const struct dev_pagemap_ops dmirror_devmem_ops;\nstatic const struct mmu_interval_notifier_ops dmirror_min_ops;\nstatic dev_t dmirror_dev;\n\nstruct dmirror_device;\n\nstruct dmirror_bounce {\n\tvoid\t\t\t*ptr;\n\tunsigned long\t\tsize;\n\tunsigned long\t\taddr;\n\tunsigned long\t\tcpages;\n};\n\n#define DPT_XA_TAG_ATOMIC 1UL\n#define DPT_XA_TAG_WRITE 3UL\n\n \nstruct dmirror_interval {\n\tstruct mmu_interval_notifier\tnotifier;\n\tstruct dmirror\t\t\t*dmirror;\n};\n\n \nstruct dmirror {\n\tstruct dmirror_device\t\t*mdevice;\n\tstruct xarray\t\t\tpt;\n\tstruct mmu_interval_notifier\tnotifier;\n\tstruct mutex\t\t\tmutex;\n};\n\n \nstruct dmirror_chunk {\n\tstruct dev_pagemap\tpagemap;\n\tstruct dmirror_device\t*mdevice;\n\tbool remove;\n};\n\n \nstruct dmirror_device {\n\tstruct cdev\t\tcdevice;\n\tunsigned int            zone_device_type;\n\tstruct device\t\tdevice;\n\n\tunsigned int\t\tdevmem_capacity;\n\tunsigned int\t\tdevmem_count;\n\tstruct dmirror_chunk\t**devmem_chunks;\n\tstruct mutex\t\tdevmem_lock;\t \n\n\tunsigned long\t\tcalloc;\n\tunsigned long\t\tcfree;\n\tstruct page\t\t*free_pages;\n\tspinlock_t\t\tlock;\t\t \n};\n\nstatic struct dmirror_device dmirror_devices[DMIRROR_NDEVICES];\n\nstatic int dmirror_bounce_init(struct dmirror_bounce *bounce,\n\t\t\t       unsigned long addr,\n\t\t\t       unsigned long size)\n{\n\tbounce->addr = addr;\n\tbounce->size = size;\n\tbounce->cpages = 0;\n\tbounce->ptr = vmalloc(size);\n\tif (!bounce->ptr)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nstatic bool dmirror_is_private_zone(struct dmirror_device *mdevice)\n{\n\treturn (mdevice->zone_device_type ==\n\t\tHMM_DMIRROR_MEMORY_DEVICE_PRIVATE) ? true : false;\n}\n\nstatic enum migrate_vma_direction\ndmirror_select_device(struct dmirror *dmirror)\n{\n\treturn (dmirror->mdevice->zone_device_type ==\n\t\tHMM_DMIRROR_MEMORY_DEVICE_PRIVATE) ?\n\t\tMIGRATE_VMA_SELECT_DEVICE_PRIVATE :\n\t\tMIGRATE_VMA_SELECT_DEVICE_COHERENT;\n}\n\nstatic void dmirror_bounce_fini(struct dmirror_bounce *bounce)\n{\n\tvfree(bounce->ptr);\n}\n\nstatic int dmirror_fops_open(struct inode *inode, struct file *filp)\n{\n\tstruct cdev *cdev = inode->i_cdev;\n\tstruct dmirror *dmirror;\n\tint ret;\n\n\t \n\tdmirror = kzalloc(sizeof(*dmirror), GFP_KERNEL);\n\tif (dmirror == NULL)\n\t\treturn -ENOMEM;\n\n\tdmirror->mdevice = container_of(cdev, struct dmirror_device, cdevice);\n\tmutex_init(&dmirror->mutex);\n\txa_init(&dmirror->pt);\n\n\tret = mmu_interval_notifier_insert(&dmirror->notifier, current->mm,\n\t\t\t\t0, ULONG_MAX & PAGE_MASK, &dmirror_min_ops);\n\tif (ret) {\n\t\tkfree(dmirror);\n\t\treturn ret;\n\t}\n\n\tfilp->private_data = dmirror;\n\treturn 0;\n}\n\nstatic int dmirror_fops_release(struct inode *inode, struct file *filp)\n{\n\tstruct dmirror *dmirror = filp->private_data;\n\n\tmmu_interval_notifier_remove(&dmirror->notifier);\n\txa_destroy(&dmirror->pt);\n\tkfree(dmirror);\n\treturn 0;\n}\n\nstatic struct dmirror_chunk *dmirror_page_to_chunk(struct page *page)\n{\n\treturn container_of(page->pgmap, struct dmirror_chunk, pagemap);\n}\n\nstatic struct dmirror_device *dmirror_page_to_device(struct page *page)\n\n{\n\treturn dmirror_page_to_chunk(page)->mdevice;\n}\n\nstatic int dmirror_do_fault(struct dmirror *dmirror, struct hmm_range *range)\n{\n\tunsigned long *pfns = range->hmm_pfns;\n\tunsigned long pfn;\n\n\tfor (pfn = (range->start >> PAGE_SHIFT);\n\t     pfn < (range->end >> PAGE_SHIFT);\n\t     pfn++, pfns++) {\n\t\tstruct page *page;\n\t\tvoid *entry;\n\n\t\t \n\t\tWARN_ON(*pfns & HMM_PFN_ERROR);\n\t\tWARN_ON(!(*pfns & HMM_PFN_VALID));\n\n\t\tpage = hmm_pfn_to_page(*pfns);\n\t\tWARN_ON(!page);\n\n\t\tentry = page;\n\t\tif (*pfns & HMM_PFN_WRITE)\n\t\t\tentry = xa_tag_pointer(entry, DPT_XA_TAG_WRITE);\n\t\telse if (WARN_ON(range->default_flags & HMM_PFN_WRITE))\n\t\t\treturn -EFAULT;\n\t\tentry = xa_store(&dmirror->pt, pfn, entry, GFP_ATOMIC);\n\t\tif (xa_is_err(entry))\n\t\t\treturn xa_err(entry);\n\t}\n\n\treturn 0;\n}\n\nstatic void dmirror_do_update(struct dmirror *dmirror, unsigned long start,\n\t\t\t      unsigned long end)\n{\n\tunsigned long pfn;\n\tvoid *entry;\n\n\t \n\txa_for_each_range(&dmirror->pt, pfn, entry, start >> PAGE_SHIFT,\n\t\t\t  end >> PAGE_SHIFT)\n\t\txa_erase(&dmirror->pt, pfn);\n}\n\nstatic bool dmirror_interval_invalidate(struct mmu_interval_notifier *mni,\n\t\t\t\tconst struct mmu_notifier_range *range,\n\t\t\t\tunsigned long cur_seq)\n{\n\tstruct dmirror *dmirror = container_of(mni, struct dmirror, notifier);\n\n\t \n\tif (range->event == MMU_NOTIFY_MIGRATE &&\n\t    range->owner == dmirror->mdevice)\n\t\treturn true;\n\n\tif (mmu_notifier_range_blockable(range))\n\t\tmutex_lock(&dmirror->mutex);\n\telse if (!mutex_trylock(&dmirror->mutex))\n\t\treturn false;\n\n\tmmu_interval_set_seq(mni, cur_seq);\n\tdmirror_do_update(dmirror, range->start, range->end);\n\n\tmutex_unlock(&dmirror->mutex);\n\treturn true;\n}\n\nstatic const struct mmu_interval_notifier_ops dmirror_min_ops = {\n\t.invalidate = dmirror_interval_invalidate,\n};\n\nstatic int dmirror_range_fault(struct dmirror *dmirror,\n\t\t\t\tstruct hmm_range *range)\n{\n\tstruct mm_struct *mm = dmirror->notifier.mm;\n\tunsigned long timeout =\n\t\tjiffies + msecs_to_jiffies(HMM_RANGE_DEFAULT_TIMEOUT);\n\tint ret;\n\n\twhile (true) {\n\t\tif (time_after(jiffies, timeout)) {\n\t\t\tret = -EBUSY;\n\t\t\tgoto out;\n\t\t}\n\n\t\trange->notifier_seq = mmu_interval_read_begin(range->notifier);\n\t\tmmap_read_lock(mm);\n\t\tret = hmm_range_fault(range);\n\t\tmmap_read_unlock(mm);\n\t\tif (ret) {\n\t\t\tif (ret == -EBUSY)\n\t\t\t\tcontinue;\n\t\t\tgoto out;\n\t\t}\n\n\t\tmutex_lock(&dmirror->mutex);\n\t\tif (mmu_interval_read_retry(range->notifier,\n\t\t\t\t\t    range->notifier_seq)) {\n\t\t\tmutex_unlock(&dmirror->mutex);\n\t\t\tcontinue;\n\t\t}\n\t\tbreak;\n\t}\n\n\tret = dmirror_do_fault(dmirror, range);\n\n\tmutex_unlock(&dmirror->mutex);\nout:\n\treturn ret;\n}\n\nstatic int dmirror_fault(struct dmirror *dmirror, unsigned long start,\n\t\t\t unsigned long end, bool write)\n{\n\tstruct mm_struct *mm = dmirror->notifier.mm;\n\tunsigned long addr;\n\tunsigned long pfns[64];\n\tstruct hmm_range range = {\n\t\t.notifier = &dmirror->notifier,\n\t\t.hmm_pfns = pfns,\n\t\t.pfn_flags_mask = 0,\n\t\t.default_flags =\n\t\t\tHMM_PFN_REQ_FAULT | (write ? HMM_PFN_REQ_WRITE : 0),\n\t\t.dev_private_owner = dmirror->mdevice,\n\t};\n\tint ret = 0;\n\n\t \n\tif (!mmget_not_zero(mm))\n\t\treturn 0;\n\n\tfor (addr = start; addr < end; addr = range.end) {\n\t\trange.start = addr;\n\t\trange.end = min(addr + (ARRAY_SIZE(pfns) << PAGE_SHIFT), end);\n\n\t\tret = dmirror_range_fault(dmirror, &range);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\n\tmmput(mm);\n\treturn ret;\n}\n\nstatic int dmirror_do_read(struct dmirror *dmirror, unsigned long start,\n\t\t\t   unsigned long end, struct dmirror_bounce *bounce)\n{\n\tunsigned long pfn;\n\tvoid *ptr;\n\n\tptr = bounce->ptr + ((start - bounce->addr) & PAGE_MASK);\n\n\tfor (pfn = start >> PAGE_SHIFT; pfn < (end >> PAGE_SHIFT); pfn++) {\n\t\tvoid *entry;\n\t\tstruct page *page;\n\n\t\tentry = xa_load(&dmirror->pt, pfn);\n\t\tpage = xa_untag_pointer(entry);\n\t\tif (!page)\n\t\t\treturn -ENOENT;\n\n\t\tmemcpy_from_page(ptr, page, 0, PAGE_SIZE);\n\n\t\tptr += PAGE_SIZE;\n\t\tbounce->cpages++;\n\t}\n\n\treturn 0;\n}\n\nstatic int dmirror_read(struct dmirror *dmirror, struct hmm_dmirror_cmd *cmd)\n{\n\tstruct dmirror_bounce bounce;\n\tunsigned long start, end;\n\tunsigned long size = cmd->npages << PAGE_SHIFT;\n\tint ret;\n\n\tstart = cmd->addr;\n\tend = start + size;\n\tif (end < start)\n\t\treturn -EINVAL;\n\n\tret = dmirror_bounce_init(&bounce, start, size);\n\tif (ret)\n\t\treturn ret;\n\n\twhile (1) {\n\t\tmutex_lock(&dmirror->mutex);\n\t\tret = dmirror_do_read(dmirror, start, end, &bounce);\n\t\tmutex_unlock(&dmirror->mutex);\n\t\tif (ret != -ENOENT)\n\t\t\tbreak;\n\n\t\tstart = cmd->addr + (bounce.cpages << PAGE_SHIFT);\n\t\tret = dmirror_fault(dmirror, start, end, false);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tcmd->faults++;\n\t}\n\n\tif (ret == 0) {\n\t\tif (copy_to_user(u64_to_user_ptr(cmd->ptr), bounce.ptr,\n\t\t\t\t bounce.size))\n\t\t\tret = -EFAULT;\n\t}\n\tcmd->cpages = bounce.cpages;\n\tdmirror_bounce_fini(&bounce);\n\treturn ret;\n}\n\nstatic int dmirror_do_write(struct dmirror *dmirror, unsigned long start,\n\t\t\t    unsigned long end, struct dmirror_bounce *bounce)\n{\n\tunsigned long pfn;\n\tvoid *ptr;\n\n\tptr = bounce->ptr + ((start - bounce->addr) & PAGE_MASK);\n\n\tfor (pfn = start >> PAGE_SHIFT; pfn < (end >> PAGE_SHIFT); pfn++) {\n\t\tvoid *entry;\n\t\tstruct page *page;\n\n\t\tentry = xa_load(&dmirror->pt, pfn);\n\t\tpage = xa_untag_pointer(entry);\n\t\tif (!page || xa_pointer_tag(entry) != DPT_XA_TAG_WRITE)\n\t\t\treturn -ENOENT;\n\n\t\tmemcpy_to_page(page, 0, ptr, PAGE_SIZE);\n\n\t\tptr += PAGE_SIZE;\n\t\tbounce->cpages++;\n\t}\n\n\treturn 0;\n}\n\nstatic int dmirror_write(struct dmirror *dmirror, struct hmm_dmirror_cmd *cmd)\n{\n\tstruct dmirror_bounce bounce;\n\tunsigned long start, end;\n\tunsigned long size = cmd->npages << PAGE_SHIFT;\n\tint ret;\n\n\tstart = cmd->addr;\n\tend = start + size;\n\tif (end < start)\n\t\treturn -EINVAL;\n\n\tret = dmirror_bounce_init(&bounce, start, size);\n\tif (ret)\n\t\treturn ret;\n\tif (copy_from_user(bounce.ptr, u64_to_user_ptr(cmd->ptr),\n\t\t\t   bounce.size)) {\n\t\tret = -EFAULT;\n\t\tgoto fini;\n\t}\n\n\twhile (1) {\n\t\tmutex_lock(&dmirror->mutex);\n\t\tret = dmirror_do_write(dmirror, start, end, &bounce);\n\t\tmutex_unlock(&dmirror->mutex);\n\t\tif (ret != -ENOENT)\n\t\t\tbreak;\n\n\t\tstart = cmd->addr + (bounce.cpages << PAGE_SHIFT);\n\t\tret = dmirror_fault(dmirror, start, end, true);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tcmd->faults++;\n\t}\n\nfini:\n\tcmd->cpages = bounce.cpages;\n\tdmirror_bounce_fini(&bounce);\n\treturn ret;\n}\n\nstatic int dmirror_allocate_chunk(struct dmirror_device *mdevice,\n\t\t\t\t   struct page **ppage)\n{\n\tstruct dmirror_chunk *devmem;\n\tstruct resource *res = NULL;\n\tunsigned long pfn;\n\tunsigned long pfn_first;\n\tunsigned long pfn_last;\n\tvoid *ptr;\n\tint ret = -ENOMEM;\n\n\tdevmem = kzalloc(sizeof(*devmem), GFP_KERNEL);\n\tif (!devmem)\n\t\treturn ret;\n\n\tswitch (mdevice->zone_device_type) {\n\tcase HMM_DMIRROR_MEMORY_DEVICE_PRIVATE:\n\t\tres = request_free_mem_region(&iomem_resource, DEVMEM_CHUNK_SIZE,\n\t\t\t\t\t      \"hmm_dmirror\");\n\t\tif (IS_ERR_OR_NULL(res))\n\t\t\tgoto err_devmem;\n\t\tdevmem->pagemap.range.start = res->start;\n\t\tdevmem->pagemap.range.end = res->end;\n\t\tdevmem->pagemap.type = MEMORY_DEVICE_PRIVATE;\n\t\tbreak;\n\tcase HMM_DMIRROR_MEMORY_DEVICE_COHERENT:\n\t\tdevmem->pagemap.range.start = (MINOR(mdevice->cdevice.dev) - 2) ?\n\t\t\t\t\t\t\tspm_addr_dev0 :\n\t\t\t\t\t\t\tspm_addr_dev1;\n\t\tdevmem->pagemap.range.end = devmem->pagemap.range.start +\n\t\t\t\t\t    DEVMEM_CHUNK_SIZE - 1;\n\t\tdevmem->pagemap.type = MEMORY_DEVICE_COHERENT;\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tgoto err_devmem;\n\t}\n\n\tdevmem->pagemap.nr_range = 1;\n\tdevmem->pagemap.ops = &dmirror_devmem_ops;\n\tdevmem->pagemap.owner = mdevice;\n\n\tmutex_lock(&mdevice->devmem_lock);\n\n\tif (mdevice->devmem_count == mdevice->devmem_capacity) {\n\t\tstruct dmirror_chunk **new_chunks;\n\t\tunsigned int new_capacity;\n\n\t\tnew_capacity = mdevice->devmem_capacity +\n\t\t\t\tDEVMEM_CHUNKS_RESERVE;\n\t\tnew_chunks = krealloc(mdevice->devmem_chunks,\n\t\t\t\tsizeof(new_chunks[0]) * new_capacity,\n\t\t\t\tGFP_KERNEL);\n\t\tif (!new_chunks)\n\t\t\tgoto err_release;\n\t\tmdevice->devmem_capacity = new_capacity;\n\t\tmdevice->devmem_chunks = new_chunks;\n\t}\n\tptr = memremap_pages(&devmem->pagemap, numa_node_id());\n\tif (IS_ERR_OR_NULL(ptr)) {\n\t\tif (ptr)\n\t\t\tret = PTR_ERR(ptr);\n\t\telse\n\t\t\tret = -EFAULT;\n\t\tgoto err_release;\n\t}\n\n\tdevmem->mdevice = mdevice;\n\tpfn_first = devmem->pagemap.range.start >> PAGE_SHIFT;\n\tpfn_last = pfn_first + (range_len(&devmem->pagemap.range) >> PAGE_SHIFT);\n\tmdevice->devmem_chunks[mdevice->devmem_count++] = devmem;\n\n\tmutex_unlock(&mdevice->devmem_lock);\n\n\tpr_info(\"added new %u MB chunk (total %u chunks, %u MB) PFNs [0x%lx 0x%lx)\\n\",\n\t\tDEVMEM_CHUNK_SIZE / (1024 * 1024),\n\t\tmdevice->devmem_count,\n\t\tmdevice->devmem_count * (DEVMEM_CHUNK_SIZE / (1024 * 1024)),\n\t\tpfn_first, pfn_last);\n\n\tspin_lock(&mdevice->lock);\n\tfor (pfn = pfn_first; pfn < pfn_last; pfn++) {\n\t\tstruct page *page = pfn_to_page(pfn);\n\n\t\tpage->zone_device_data = mdevice->free_pages;\n\t\tmdevice->free_pages = page;\n\t}\n\tif (ppage) {\n\t\t*ppage = mdevice->free_pages;\n\t\tmdevice->free_pages = (*ppage)->zone_device_data;\n\t\tmdevice->calloc++;\n\t}\n\tspin_unlock(&mdevice->lock);\n\n\treturn 0;\n\nerr_release:\n\tmutex_unlock(&mdevice->devmem_lock);\n\tif (res && devmem->pagemap.type == MEMORY_DEVICE_PRIVATE)\n\t\trelease_mem_region(devmem->pagemap.range.start,\n\t\t\t\t   range_len(&devmem->pagemap.range));\nerr_devmem:\n\tkfree(devmem);\n\n\treturn ret;\n}\n\nstatic struct page *dmirror_devmem_alloc_page(struct dmirror_device *mdevice)\n{\n\tstruct page *dpage = NULL;\n\tstruct page *rpage = NULL;\n\n\t \n\tif (dmirror_is_private_zone(mdevice)) {\n\t\trpage = alloc_page(GFP_HIGHUSER);\n\t\tif (!rpage)\n\t\t\treturn NULL;\n\t}\n\tspin_lock(&mdevice->lock);\n\n\tif (mdevice->free_pages) {\n\t\tdpage = mdevice->free_pages;\n\t\tmdevice->free_pages = dpage->zone_device_data;\n\t\tmdevice->calloc++;\n\t\tspin_unlock(&mdevice->lock);\n\t} else {\n\t\tspin_unlock(&mdevice->lock);\n\t\tif (dmirror_allocate_chunk(mdevice, &dpage))\n\t\t\tgoto error;\n\t}\n\n\tzone_device_page_init(dpage);\n\tdpage->zone_device_data = rpage;\n\treturn dpage;\n\nerror:\n\tif (rpage)\n\t\t__free_page(rpage);\n\treturn NULL;\n}\n\nstatic void dmirror_migrate_alloc_and_copy(struct migrate_vma *args,\n\t\t\t\t\t   struct dmirror *dmirror)\n{\n\tstruct dmirror_device *mdevice = dmirror->mdevice;\n\tconst unsigned long *src = args->src;\n\tunsigned long *dst = args->dst;\n\tunsigned long addr;\n\n\tfor (addr = args->start; addr < args->end; addr += PAGE_SIZE,\n\t\t\t\t\t\t   src++, dst++) {\n\t\tstruct page *spage;\n\t\tstruct page *dpage;\n\t\tstruct page *rpage;\n\n\t\tif (!(*src & MIGRATE_PFN_MIGRATE))\n\t\t\tcontinue;\n\n\t\t \n\t\tspage = migrate_pfn_to_page(*src);\n\t\tif (WARN(spage && is_zone_device_page(spage),\n\t\t     \"page already in device spage pfn: 0x%lx\\n\",\n\t\t     page_to_pfn(spage)))\n\t\t\tcontinue;\n\n\t\tdpage = dmirror_devmem_alloc_page(mdevice);\n\t\tif (!dpage)\n\t\t\tcontinue;\n\n\t\trpage = BACKING_PAGE(dpage);\n\t\tif (spage)\n\t\t\tcopy_highpage(rpage, spage);\n\t\telse\n\t\t\tclear_highpage(rpage);\n\n\t\t \n\t\trpage->zone_device_data = dmirror;\n\n\t\tpr_debug(\"migrating from sys to dev pfn src: 0x%lx pfn dst: 0x%lx\\n\",\n\t\t\t page_to_pfn(spage), page_to_pfn(dpage));\n\t\t*dst = migrate_pfn(page_to_pfn(dpage));\n\t\tif ((*src & MIGRATE_PFN_WRITE) ||\n\t\t    (!spage && args->vma->vm_flags & VM_WRITE))\n\t\t\t*dst |= MIGRATE_PFN_WRITE;\n\t}\n}\n\nstatic int dmirror_check_atomic(struct dmirror *dmirror, unsigned long start,\n\t\t\t     unsigned long end)\n{\n\tunsigned long pfn;\n\n\tfor (pfn = start >> PAGE_SHIFT; pfn < (end >> PAGE_SHIFT); pfn++) {\n\t\tvoid *entry;\n\n\t\tentry = xa_load(&dmirror->pt, pfn);\n\t\tif (xa_pointer_tag(entry) == DPT_XA_TAG_ATOMIC)\n\t\t\treturn -EPERM;\n\t}\n\n\treturn 0;\n}\n\nstatic int dmirror_atomic_map(unsigned long start, unsigned long end,\n\t\t\t      struct page **pages, struct dmirror *dmirror)\n{\n\tunsigned long pfn, mapped = 0;\n\tint i;\n\n\t \n\tmutex_lock(&dmirror->mutex);\n\n\tfor (i = 0, pfn = start >> PAGE_SHIFT; pfn < (end >> PAGE_SHIFT); pfn++, i++) {\n\t\tvoid *entry;\n\n\t\tif (!pages[i])\n\t\t\tcontinue;\n\n\t\tentry = pages[i];\n\t\tentry = xa_tag_pointer(entry, DPT_XA_TAG_ATOMIC);\n\t\tentry = xa_store(&dmirror->pt, pfn, entry, GFP_ATOMIC);\n\t\tif (xa_is_err(entry)) {\n\t\t\tmutex_unlock(&dmirror->mutex);\n\t\t\treturn xa_err(entry);\n\t\t}\n\n\t\tmapped++;\n\t}\n\n\tmutex_unlock(&dmirror->mutex);\n\treturn mapped;\n}\n\nstatic int dmirror_migrate_finalize_and_map(struct migrate_vma *args,\n\t\t\t\t\t    struct dmirror *dmirror)\n{\n\tunsigned long start = args->start;\n\tunsigned long end = args->end;\n\tconst unsigned long *src = args->src;\n\tconst unsigned long *dst = args->dst;\n\tunsigned long pfn;\n\n\t \n\tmutex_lock(&dmirror->mutex);\n\n\tfor (pfn = start >> PAGE_SHIFT; pfn < (end >> PAGE_SHIFT); pfn++,\n\t\t\t\t\t\t\t\tsrc++, dst++) {\n\t\tstruct page *dpage;\n\t\tvoid *entry;\n\n\t\tif (!(*src & MIGRATE_PFN_MIGRATE))\n\t\t\tcontinue;\n\n\t\tdpage = migrate_pfn_to_page(*dst);\n\t\tif (!dpage)\n\t\t\tcontinue;\n\n\t\tentry = BACKING_PAGE(dpage);\n\t\tif (*dst & MIGRATE_PFN_WRITE)\n\t\t\tentry = xa_tag_pointer(entry, DPT_XA_TAG_WRITE);\n\t\tentry = xa_store(&dmirror->pt, pfn, entry, GFP_ATOMIC);\n\t\tif (xa_is_err(entry)) {\n\t\t\tmutex_unlock(&dmirror->mutex);\n\t\t\treturn xa_err(entry);\n\t\t}\n\t}\n\n\tmutex_unlock(&dmirror->mutex);\n\treturn 0;\n}\n\nstatic int dmirror_exclusive(struct dmirror *dmirror,\n\t\t\t     struct hmm_dmirror_cmd *cmd)\n{\n\tunsigned long start, end, addr;\n\tunsigned long size = cmd->npages << PAGE_SHIFT;\n\tstruct mm_struct *mm = dmirror->notifier.mm;\n\tstruct page *pages[64];\n\tstruct dmirror_bounce bounce;\n\tunsigned long next;\n\tint ret;\n\n\tstart = cmd->addr;\n\tend = start + size;\n\tif (end < start)\n\t\treturn -EINVAL;\n\n\t \n\tif (!mmget_not_zero(mm))\n\t\treturn -EINVAL;\n\n\tmmap_read_lock(mm);\n\tfor (addr = start; addr < end; addr = next) {\n\t\tunsigned long mapped = 0;\n\t\tint i;\n\n\t\tif (end < addr + (ARRAY_SIZE(pages) << PAGE_SHIFT))\n\t\t\tnext = end;\n\t\telse\n\t\t\tnext = addr + (ARRAY_SIZE(pages) << PAGE_SHIFT);\n\n\t\tret = make_device_exclusive_range(mm, addr, next, pages, NULL);\n\t\t \n\t\tif (ret == (next - addr) >> PAGE_SHIFT)\n\t\t\tmapped = dmirror_atomic_map(addr, next, pages, dmirror);\n\t\tfor (i = 0; i < ret; i++) {\n\t\t\tif (pages[i]) {\n\t\t\t\tunlock_page(pages[i]);\n\t\t\t\tput_page(pages[i]);\n\t\t\t}\n\t\t}\n\n\t\tif (addr + (mapped << PAGE_SHIFT) < next) {\n\t\t\tmmap_read_unlock(mm);\n\t\t\tmmput(mm);\n\t\t\treturn -EBUSY;\n\t\t}\n\t}\n\tmmap_read_unlock(mm);\n\tmmput(mm);\n\n\t \n\tret = dmirror_bounce_init(&bounce, start, size);\n\tif (ret)\n\t\treturn ret;\n\tmutex_lock(&dmirror->mutex);\n\tret = dmirror_do_read(dmirror, start, end, &bounce);\n\tmutex_unlock(&dmirror->mutex);\n\tif (ret == 0) {\n\t\tif (copy_to_user(u64_to_user_ptr(cmd->ptr), bounce.ptr,\n\t\t\t\t bounce.size))\n\t\t\tret = -EFAULT;\n\t}\n\n\tcmd->cpages = bounce.cpages;\n\tdmirror_bounce_fini(&bounce);\n\treturn ret;\n}\n\nstatic vm_fault_t dmirror_devmem_fault_alloc_and_copy(struct migrate_vma *args,\n\t\t\t\t\t\t      struct dmirror *dmirror)\n{\n\tconst unsigned long *src = args->src;\n\tunsigned long *dst = args->dst;\n\tunsigned long start = args->start;\n\tunsigned long end = args->end;\n\tunsigned long addr;\n\n\tfor (addr = start; addr < end; addr += PAGE_SIZE,\n\t\t\t\t       src++, dst++) {\n\t\tstruct page *dpage, *spage;\n\n\t\tspage = migrate_pfn_to_page(*src);\n\t\tif (!spage || !(*src & MIGRATE_PFN_MIGRATE))\n\t\t\tcontinue;\n\n\t\tif (WARN_ON(!is_device_private_page(spage) &&\n\t\t\t    !is_device_coherent_page(spage)))\n\t\t\tcontinue;\n\t\tspage = BACKING_PAGE(spage);\n\t\tdpage = alloc_page_vma(GFP_HIGHUSER_MOVABLE, args->vma, addr);\n\t\tif (!dpage)\n\t\t\tcontinue;\n\t\tpr_debug(\"migrating from dev to sys pfn src: 0x%lx pfn dst: 0x%lx\\n\",\n\t\t\t page_to_pfn(spage), page_to_pfn(dpage));\n\n\t\tlock_page(dpage);\n\t\txa_erase(&dmirror->pt, addr >> PAGE_SHIFT);\n\t\tcopy_highpage(dpage, spage);\n\t\t*dst = migrate_pfn(page_to_pfn(dpage));\n\t\tif (*src & MIGRATE_PFN_WRITE)\n\t\t\t*dst |= MIGRATE_PFN_WRITE;\n\t}\n\treturn 0;\n}\n\nstatic unsigned long\ndmirror_successful_migrated_pages(struct migrate_vma *migrate)\n{\n\tunsigned long cpages = 0;\n\tunsigned long i;\n\n\tfor (i = 0; i < migrate->npages; i++) {\n\t\tif (migrate->src[i] & MIGRATE_PFN_VALID &&\n\t\t    migrate->src[i] & MIGRATE_PFN_MIGRATE)\n\t\t\tcpages++;\n\t}\n\treturn cpages;\n}\n\nstatic int dmirror_migrate_to_system(struct dmirror *dmirror,\n\t\t\t\t     struct hmm_dmirror_cmd *cmd)\n{\n\tunsigned long start, end, addr;\n\tunsigned long size = cmd->npages << PAGE_SHIFT;\n\tstruct mm_struct *mm = dmirror->notifier.mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long src_pfns[64] = { 0 };\n\tunsigned long dst_pfns[64] = { 0 };\n\tstruct migrate_vma args = { 0 };\n\tunsigned long next;\n\tint ret;\n\n\tstart = cmd->addr;\n\tend = start + size;\n\tif (end < start)\n\t\treturn -EINVAL;\n\n\t \n\tif (!mmget_not_zero(mm))\n\t\treturn -EINVAL;\n\n\tcmd->cpages = 0;\n\tmmap_read_lock(mm);\n\tfor (addr = start; addr < end; addr = next) {\n\t\tvma = vma_lookup(mm, addr);\n\t\tif (!vma || !(vma->vm_flags & VM_READ)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tnext = min(end, addr + (ARRAY_SIZE(src_pfns) << PAGE_SHIFT));\n\t\tif (next > vma->vm_end)\n\t\t\tnext = vma->vm_end;\n\n\t\targs.vma = vma;\n\t\targs.src = src_pfns;\n\t\targs.dst = dst_pfns;\n\t\targs.start = addr;\n\t\targs.end = next;\n\t\targs.pgmap_owner = dmirror->mdevice;\n\t\targs.flags = dmirror_select_device(dmirror);\n\n\t\tret = migrate_vma_setup(&args);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tpr_debug(\"Migrating from device mem to sys mem\\n\");\n\t\tdmirror_devmem_fault_alloc_and_copy(&args, dmirror);\n\n\t\tmigrate_vma_pages(&args);\n\t\tcmd->cpages += dmirror_successful_migrated_pages(&args);\n\t\tmigrate_vma_finalize(&args);\n\t}\nout:\n\tmmap_read_unlock(mm);\n\tmmput(mm);\n\n\treturn ret;\n}\n\nstatic int dmirror_migrate_to_device(struct dmirror *dmirror,\n\t\t\t\tstruct hmm_dmirror_cmd *cmd)\n{\n\tunsigned long start, end, addr;\n\tunsigned long size = cmd->npages << PAGE_SHIFT;\n\tstruct mm_struct *mm = dmirror->notifier.mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long src_pfns[64] = { 0 };\n\tunsigned long dst_pfns[64] = { 0 };\n\tstruct dmirror_bounce bounce;\n\tstruct migrate_vma args = { 0 };\n\tunsigned long next;\n\tint ret;\n\n\tstart = cmd->addr;\n\tend = start + size;\n\tif (end < start)\n\t\treturn -EINVAL;\n\n\t \n\tif (!mmget_not_zero(mm))\n\t\treturn -EINVAL;\n\n\tmmap_read_lock(mm);\n\tfor (addr = start; addr < end; addr = next) {\n\t\tvma = vma_lookup(mm, addr);\n\t\tif (!vma || !(vma->vm_flags & VM_READ)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tnext = min(end, addr + (ARRAY_SIZE(src_pfns) << PAGE_SHIFT));\n\t\tif (next > vma->vm_end)\n\t\t\tnext = vma->vm_end;\n\n\t\targs.vma = vma;\n\t\targs.src = src_pfns;\n\t\targs.dst = dst_pfns;\n\t\targs.start = addr;\n\t\targs.end = next;\n\t\targs.pgmap_owner = dmirror->mdevice;\n\t\targs.flags = MIGRATE_VMA_SELECT_SYSTEM;\n\t\tret = migrate_vma_setup(&args);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tpr_debug(\"Migrating from sys mem to device mem\\n\");\n\t\tdmirror_migrate_alloc_and_copy(&args, dmirror);\n\t\tmigrate_vma_pages(&args);\n\t\tdmirror_migrate_finalize_and_map(&args, dmirror);\n\t\tmigrate_vma_finalize(&args);\n\t}\n\tmmap_read_unlock(mm);\n\tmmput(mm);\n\n\t \n\tret = dmirror_bounce_init(&bounce, start, size);\n\tif (ret)\n\t\treturn ret;\n\tmutex_lock(&dmirror->mutex);\n\tret = dmirror_do_read(dmirror, start, end, &bounce);\n\tmutex_unlock(&dmirror->mutex);\n\tif (ret == 0) {\n\t\tif (copy_to_user(u64_to_user_ptr(cmd->ptr), bounce.ptr,\n\t\t\t\t bounce.size))\n\t\t\tret = -EFAULT;\n\t}\n\tcmd->cpages = bounce.cpages;\n\tdmirror_bounce_fini(&bounce);\n\treturn ret;\n\nout:\n\tmmap_read_unlock(mm);\n\tmmput(mm);\n\treturn ret;\n}\n\nstatic void dmirror_mkentry(struct dmirror *dmirror, struct hmm_range *range,\n\t\t\t    unsigned char *perm, unsigned long entry)\n{\n\tstruct page *page;\n\n\tif (entry & HMM_PFN_ERROR) {\n\t\t*perm = HMM_DMIRROR_PROT_ERROR;\n\t\treturn;\n\t}\n\tif (!(entry & HMM_PFN_VALID)) {\n\t\t*perm = HMM_DMIRROR_PROT_NONE;\n\t\treturn;\n\t}\n\n\tpage = hmm_pfn_to_page(entry);\n\tif (is_device_private_page(page)) {\n\t\t \n\t\tif (dmirror->mdevice == dmirror_page_to_device(page))\n\t\t\t*perm = HMM_DMIRROR_PROT_DEV_PRIVATE_LOCAL;\n\t\telse\n\t\t\t*perm = HMM_DMIRROR_PROT_DEV_PRIVATE_REMOTE;\n\t} else if (is_device_coherent_page(page)) {\n\t\t \n\t\tif (dmirror->mdevice == dmirror_page_to_device(page))\n\t\t\t*perm = HMM_DMIRROR_PROT_DEV_COHERENT_LOCAL;\n\t\telse\n\t\t\t*perm = HMM_DMIRROR_PROT_DEV_COHERENT_REMOTE;\n\t} else if (is_zero_pfn(page_to_pfn(page)))\n\t\t*perm = HMM_DMIRROR_PROT_ZERO;\n\telse\n\t\t*perm = HMM_DMIRROR_PROT_NONE;\n\tif (entry & HMM_PFN_WRITE)\n\t\t*perm |= HMM_DMIRROR_PROT_WRITE;\n\telse\n\t\t*perm |= HMM_DMIRROR_PROT_READ;\n\tif (hmm_pfn_to_map_order(entry) + PAGE_SHIFT == PMD_SHIFT)\n\t\t*perm |= HMM_DMIRROR_PROT_PMD;\n\telse if (hmm_pfn_to_map_order(entry) + PAGE_SHIFT == PUD_SHIFT)\n\t\t*perm |= HMM_DMIRROR_PROT_PUD;\n}\n\nstatic bool dmirror_snapshot_invalidate(struct mmu_interval_notifier *mni,\n\t\t\t\tconst struct mmu_notifier_range *range,\n\t\t\t\tunsigned long cur_seq)\n{\n\tstruct dmirror_interval *dmi =\n\t\tcontainer_of(mni, struct dmirror_interval, notifier);\n\tstruct dmirror *dmirror = dmi->dmirror;\n\n\tif (mmu_notifier_range_blockable(range))\n\t\tmutex_lock(&dmirror->mutex);\n\telse if (!mutex_trylock(&dmirror->mutex))\n\t\treturn false;\n\n\t \n\tmmu_interval_set_seq(mni, cur_seq);\n\n\tmutex_unlock(&dmirror->mutex);\n\treturn true;\n}\n\nstatic const struct mmu_interval_notifier_ops dmirror_mrn_ops = {\n\t.invalidate = dmirror_snapshot_invalidate,\n};\n\nstatic int dmirror_range_snapshot(struct dmirror *dmirror,\n\t\t\t\t  struct hmm_range *range,\n\t\t\t\t  unsigned char *perm)\n{\n\tstruct mm_struct *mm = dmirror->notifier.mm;\n\tstruct dmirror_interval notifier;\n\tunsigned long timeout =\n\t\tjiffies + msecs_to_jiffies(HMM_RANGE_DEFAULT_TIMEOUT);\n\tunsigned long i;\n\tunsigned long n;\n\tint ret = 0;\n\n\tnotifier.dmirror = dmirror;\n\trange->notifier = &notifier.notifier;\n\n\tret = mmu_interval_notifier_insert(range->notifier, mm,\n\t\t\trange->start, range->end - range->start,\n\t\t\t&dmirror_mrn_ops);\n\tif (ret)\n\t\treturn ret;\n\n\twhile (true) {\n\t\tif (time_after(jiffies, timeout)) {\n\t\t\tret = -EBUSY;\n\t\t\tgoto out;\n\t\t}\n\n\t\trange->notifier_seq = mmu_interval_read_begin(range->notifier);\n\n\t\tmmap_read_lock(mm);\n\t\tret = hmm_range_fault(range);\n\t\tmmap_read_unlock(mm);\n\t\tif (ret) {\n\t\t\tif (ret == -EBUSY)\n\t\t\t\tcontinue;\n\t\t\tgoto out;\n\t\t}\n\n\t\tmutex_lock(&dmirror->mutex);\n\t\tif (mmu_interval_read_retry(range->notifier,\n\t\t\t\t\t    range->notifier_seq)) {\n\t\t\tmutex_unlock(&dmirror->mutex);\n\t\t\tcontinue;\n\t\t}\n\t\tbreak;\n\t}\n\n\tn = (range->end - range->start) >> PAGE_SHIFT;\n\tfor (i = 0; i < n; i++)\n\t\tdmirror_mkentry(dmirror, range, perm + i, range->hmm_pfns[i]);\n\n\tmutex_unlock(&dmirror->mutex);\nout:\n\tmmu_interval_notifier_remove(range->notifier);\n\treturn ret;\n}\n\nstatic int dmirror_snapshot(struct dmirror *dmirror,\n\t\t\t    struct hmm_dmirror_cmd *cmd)\n{\n\tstruct mm_struct *mm = dmirror->notifier.mm;\n\tunsigned long start, end;\n\tunsigned long size = cmd->npages << PAGE_SHIFT;\n\tunsigned long addr;\n\tunsigned long next;\n\tunsigned long pfns[64];\n\tunsigned char perm[64];\n\tchar __user *uptr;\n\tstruct hmm_range range = {\n\t\t.hmm_pfns = pfns,\n\t\t.dev_private_owner = dmirror->mdevice,\n\t};\n\tint ret = 0;\n\n\tstart = cmd->addr;\n\tend = start + size;\n\tif (end < start)\n\t\treturn -EINVAL;\n\n\t \n\tif (!mmget_not_zero(mm))\n\t\treturn -EINVAL;\n\n\t \n\tuptr = u64_to_user_ptr(cmd->ptr);\n\tfor (addr = start; addr < end; addr = next) {\n\t\tunsigned long n;\n\n\t\tnext = min(addr + (ARRAY_SIZE(pfns) << PAGE_SHIFT), end);\n\t\trange.start = addr;\n\t\trange.end = next;\n\n\t\tret = dmirror_range_snapshot(dmirror, &range, perm);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tn = (range.end - range.start) >> PAGE_SHIFT;\n\t\tif (copy_to_user(uptr, perm, n)) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tcmd->cpages += n;\n\t\tuptr += n;\n\t}\n\tmmput(mm);\n\n\treturn ret;\n}\n\nstatic void dmirror_device_evict_chunk(struct dmirror_chunk *chunk)\n{\n\tunsigned long start_pfn = chunk->pagemap.range.start >> PAGE_SHIFT;\n\tunsigned long end_pfn = chunk->pagemap.range.end >> PAGE_SHIFT;\n\tunsigned long npages = end_pfn - start_pfn + 1;\n\tunsigned long i;\n\tunsigned long *src_pfns;\n\tunsigned long *dst_pfns;\n\n\tsrc_pfns = kcalloc(npages, sizeof(*src_pfns), GFP_KERNEL);\n\tdst_pfns = kcalloc(npages, sizeof(*dst_pfns), GFP_KERNEL);\n\n\tmigrate_device_range(src_pfns, start_pfn, npages);\n\tfor (i = 0; i < npages; i++) {\n\t\tstruct page *dpage, *spage;\n\n\t\tspage = migrate_pfn_to_page(src_pfns[i]);\n\t\tif (!spage || !(src_pfns[i] & MIGRATE_PFN_MIGRATE))\n\t\t\tcontinue;\n\n\t\tif (WARN_ON(!is_device_private_page(spage) &&\n\t\t\t    !is_device_coherent_page(spage)))\n\t\t\tcontinue;\n\t\tspage = BACKING_PAGE(spage);\n\t\tdpage = alloc_page(GFP_HIGHUSER_MOVABLE | __GFP_NOFAIL);\n\t\tlock_page(dpage);\n\t\tcopy_highpage(dpage, spage);\n\t\tdst_pfns[i] = migrate_pfn(page_to_pfn(dpage));\n\t\tif (src_pfns[i] & MIGRATE_PFN_WRITE)\n\t\t\tdst_pfns[i] |= MIGRATE_PFN_WRITE;\n\t}\n\tmigrate_device_pages(src_pfns, dst_pfns, npages);\n\tmigrate_device_finalize(src_pfns, dst_pfns, npages);\n\tkfree(src_pfns);\n\tkfree(dst_pfns);\n}\n\n \nstatic void dmirror_remove_free_pages(struct dmirror_chunk *devmem)\n{\n\tstruct dmirror_device *mdevice = devmem->mdevice;\n\tstruct page *page;\n\n\tfor (page = mdevice->free_pages; page; page = page->zone_device_data)\n\t\tif (dmirror_page_to_chunk(page) == devmem)\n\t\t\tmdevice->free_pages = page->zone_device_data;\n}\n\nstatic void dmirror_device_remove_chunks(struct dmirror_device *mdevice)\n{\n\tunsigned int i;\n\n\tmutex_lock(&mdevice->devmem_lock);\n\tif (mdevice->devmem_chunks) {\n\t\tfor (i = 0; i < mdevice->devmem_count; i++) {\n\t\t\tstruct dmirror_chunk *devmem =\n\t\t\t\tmdevice->devmem_chunks[i];\n\n\t\t\tspin_lock(&mdevice->lock);\n\t\t\tdevmem->remove = true;\n\t\t\tdmirror_remove_free_pages(devmem);\n\t\t\tspin_unlock(&mdevice->lock);\n\n\t\t\tdmirror_device_evict_chunk(devmem);\n\t\t\tmemunmap_pages(&devmem->pagemap);\n\t\t\tif (devmem->pagemap.type == MEMORY_DEVICE_PRIVATE)\n\t\t\t\trelease_mem_region(devmem->pagemap.range.start,\n\t\t\t\t\t\t   range_len(&devmem->pagemap.range));\n\t\t\tkfree(devmem);\n\t\t}\n\t\tmdevice->devmem_count = 0;\n\t\tmdevice->devmem_capacity = 0;\n\t\tmdevice->free_pages = NULL;\n\t\tkfree(mdevice->devmem_chunks);\n\t\tmdevice->devmem_chunks = NULL;\n\t}\n\tmutex_unlock(&mdevice->devmem_lock);\n}\n\nstatic long dmirror_fops_unlocked_ioctl(struct file *filp,\n\t\t\t\t\tunsigned int command,\n\t\t\t\t\tunsigned long arg)\n{\n\tvoid __user *uarg = (void __user *)arg;\n\tstruct hmm_dmirror_cmd cmd;\n\tstruct dmirror *dmirror;\n\tint ret;\n\n\tdmirror = filp->private_data;\n\tif (!dmirror)\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(&cmd, uarg, sizeof(cmd)))\n\t\treturn -EFAULT;\n\n\tif (cmd.addr & ~PAGE_MASK)\n\t\treturn -EINVAL;\n\tif (cmd.addr >= (cmd.addr + (cmd.npages << PAGE_SHIFT)))\n\t\treturn -EINVAL;\n\n\tcmd.cpages = 0;\n\tcmd.faults = 0;\n\n\tswitch (command) {\n\tcase HMM_DMIRROR_READ:\n\t\tret = dmirror_read(dmirror, &cmd);\n\t\tbreak;\n\n\tcase HMM_DMIRROR_WRITE:\n\t\tret = dmirror_write(dmirror, &cmd);\n\t\tbreak;\n\n\tcase HMM_DMIRROR_MIGRATE_TO_DEV:\n\t\tret = dmirror_migrate_to_device(dmirror, &cmd);\n\t\tbreak;\n\n\tcase HMM_DMIRROR_MIGRATE_TO_SYS:\n\t\tret = dmirror_migrate_to_system(dmirror, &cmd);\n\t\tbreak;\n\n\tcase HMM_DMIRROR_EXCLUSIVE:\n\t\tret = dmirror_exclusive(dmirror, &cmd);\n\t\tbreak;\n\n\tcase HMM_DMIRROR_CHECK_EXCLUSIVE:\n\t\tret = dmirror_check_atomic(dmirror, cmd.addr,\n\t\t\t\t\tcmd.addr + (cmd.npages << PAGE_SHIFT));\n\t\tbreak;\n\n\tcase HMM_DMIRROR_SNAPSHOT:\n\t\tret = dmirror_snapshot(dmirror, &cmd);\n\t\tbreak;\n\n\tcase HMM_DMIRROR_RELEASE:\n\t\tdmirror_device_remove_chunks(dmirror->mdevice);\n\t\tret = 0;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\tif (ret)\n\t\treturn ret;\n\n\tif (copy_to_user(uarg, &cmd, sizeof(cmd)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic int dmirror_fops_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\tunsigned long addr;\n\n\tfor (addr = vma->vm_start; addr < vma->vm_end; addr += PAGE_SIZE) {\n\t\tstruct page *page;\n\t\tint ret;\n\n\t\tpage = alloc_page(GFP_KERNEL | __GFP_ZERO);\n\t\tif (!page)\n\t\t\treturn -ENOMEM;\n\n\t\tret = vm_insert_page(vma, addr, page);\n\t\tif (ret) {\n\t\t\t__free_page(page);\n\t\t\treturn ret;\n\t\t}\n\t\tput_page(page);\n\t}\n\n\treturn 0;\n}\n\nstatic const struct file_operations dmirror_fops = {\n\t.open\t\t= dmirror_fops_open,\n\t.release\t= dmirror_fops_release,\n\t.mmap\t\t= dmirror_fops_mmap,\n\t.unlocked_ioctl = dmirror_fops_unlocked_ioctl,\n\t.llseek\t\t= default_llseek,\n\t.owner\t\t= THIS_MODULE,\n};\n\nstatic void dmirror_devmem_free(struct page *page)\n{\n\tstruct page *rpage = BACKING_PAGE(page);\n\tstruct dmirror_device *mdevice;\n\n\tif (rpage != page)\n\t\t__free_page(rpage);\n\n\tmdevice = dmirror_page_to_device(page);\n\tspin_lock(&mdevice->lock);\n\n\t \n\tif (!dmirror_page_to_chunk(page)->remove) {\n\t\tmdevice->cfree++;\n\t\tpage->zone_device_data = mdevice->free_pages;\n\t\tmdevice->free_pages = page;\n\t}\n\tspin_unlock(&mdevice->lock);\n}\n\nstatic vm_fault_t dmirror_devmem_fault(struct vm_fault *vmf)\n{\n\tstruct migrate_vma args = { 0 };\n\tunsigned long src_pfns = 0;\n\tunsigned long dst_pfns = 0;\n\tstruct page *rpage;\n\tstruct dmirror *dmirror;\n\tvm_fault_t ret;\n\n\t \n\trpage = vmf->page->zone_device_data;\n\tdmirror = rpage->zone_device_data;\n\n\t \n\targs.vma = vmf->vma;\n\targs.start = vmf->address;\n\targs.end = args.start + PAGE_SIZE;\n\targs.src = &src_pfns;\n\targs.dst = &dst_pfns;\n\targs.pgmap_owner = dmirror->mdevice;\n\targs.flags = dmirror_select_device(dmirror);\n\targs.fault_page = vmf->page;\n\n\tif (migrate_vma_setup(&args))\n\t\treturn VM_FAULT_SIGBUS;\n\n\tret = dmirror_devmem_fault_alloc_and_copy(&args, dmirror);\n\tif (ret)\n\t\treturn ret;\n\tmigrate_vma_pages(&args);\n\t \n\tmigrate_vma_finalize(&args);\n\treturn 0;\n}\n\nstatic const struct dev_pagemap_ops dmirror_devmem_ops = {\n\t.page_free\t= dmirror_devmem_free,\n\t.migrate_to_ram\t= dmirror_devmem_fault,\n};\n\nstatic int dmirror_device_init(struct dmirror_device *mdevice, int id)\n{\n\tdev_t dev;\n\tint ret;\n\n\tdev = MKDEV(MAJOR(dmirror_dev), id);\n\tmutex_init(&mdevice->devmem_lock);\n\tspin_lock_init(&mdevice->lock);\n\n\tcdev_init(&mdevice->cdevice, &dmirror_fops);\n\tmdevice->cdevice.owner = THIS_MODULE;\n\tdevice_initialize(&mdevice->device);\n\tmdevice->device.devt = dev;\n\n\tret = dev_set_name(&mdevice->device, \"hmm_dmirror%u\", id);\n\tif (ret)\n\t\treturn ret;\n\n\tret = cdev_device_add(&mdevice->cdevice, &mdevice->device);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\treturn dmirror_allocate_chunk(mdevice, NULL);\n}\n\nstatic void dmirror_device_remove(struct dmirror_device *mdevice)\n{\n\tdmirror_device_remove_chunks(mdevice);\n\tcdev_device_del(&mdevice->cdevice, &mdevice->device);\n}\n\nstatic int __init hmm_dmirror_init(void)\n{\n\tint ret;\n\tint id = 0;\n\tint ndevices = 0;\n\n\tret = alloc_chrdev_region(&dmirror_dev, 0, DMIRROR_NDEVICES,\n\t\t\t\t  \"HMM_DMIRROR\");\n\tif (ret)\n\t\tgoto err_unreg;\n\n\tmemset(dmirror_devices, 0, DMIRROR_NDEVICES * sizeof(dmirror_devices[0]));\n\tdmirror_devices[ndevices++].zone_device_type =\n\t\t\t\tHMM_DMIRROR_MEMORY_DEVICE_PRIVATE;\n\tdmirror_devices[ndevices++].zone_device_type =\n\t\t\t\tHMM_DMIRROR_MEMORY_DEVICE_PRIVATE;\n\tif (spm_addr_dev0 && spm_addr_dev1) {\n\t\tdmirror_devices[ndevices++].zone_device_type =\n\t\t\t\t\tHMM_DMIRROR_MEMORY_DEVICE_COHERENT;\n\t\tdmirror_devices[ndevices++].zone_device_type =\n\t\t\t\t\tHMM_DMIRROR_MEMORY_DEVICE_COHERENT;\n\t}\n\tfor (id = 0; id < ndevices; id++) {\n\t\tret = dmirror_device_init(dmirror_devices + id, id);\n\t\tif (ret)\n\t\t\tgoto err_chrdev;\n\t}\n\n\tpr_info(\"HMM test module loaded. This is only for testing HMM.\\n\");\n\treturn 0;\n\nerr_chrdev:\n\twhile (--id >= 0)\n\t\tdmirror_device_remove(dmirror_devices + id);\n\tunregister_chrdev_region(dmirror_dev, DMIRROR_NDEVICES);\nerr_unreg:\n\treturn ret;\n}\n\nstatic void __exit hmm_dmirror_exit(void)\n{\n\tint id;\n\n\tfor (id = 0; id < DMIRROR_NDEVICES; id++)\n\t\tif (dmirror_devices[id].zone_device_type)\n\t\t\tdmirror_device_remove(dmirror_devices + id);\n\tunregister_chrdev_region(dmirror_dev, DMIRROR_NDEVICES);\n}\n\nmodule_init(hmm_dmirror_init);\nmodule_exit(hmm_dmirror_exit);\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}