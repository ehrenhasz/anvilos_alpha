{
  "module_name": "sbitmap.c",
  "hash_id": "9731a4559e44cc820dd012ad1cd01a9e171b05566bd7fc62113b41ec0712f722",
  "original_prompt": "Ingested from linux-6.6.14/lib/sbitmap.c",
  "human_readable_source": "\n \n\n#include <linux/sched.h>\n#include <linux/random.h>\n#include <linux/sbitmap.h>\n#include <linux/seq_file.h>\n\nstatic int init_alloc_hint(struct sbitmap *sb, gfp_t flags)\n{\n\tunsigned depth = sb->depth;\n\n\tsb->alloc_hint = alloc_percpu_gfp(unsigned int, flags);\n\tif (!sb->alloc_hint)\n\t\treturn -ENOMEM;\n\n\tif (depth && !sb->round_robin) {\n\t\tint i;\n\n\t\tfor_each_possible_cpu(i)\n\t\t\t*per_cpu_ptr(sb->alloc_hint, i) = get_random_u32_below(depth);\n\t}\n\treturn 0;\n}\n\nstatic inline unsigned update_alloc_hint_before_get(struct sbitmap *sb,\n\t\t\t\t\t\t    unsigned int depth)\n{\n\tunsigned hint;\n\n\thint = this_cpu_read(*sb->alloc_hint);\n\tif (unlikely(hint >= depth)) {\n\t\thint = depth ? get_random_u32_below(depth) : 0;\n\t\tthis_cpu_write(*sb->alloc_hint, hint);\n\t}\n\n\treturn hint;\n}\n\nstatic inline void update_alloc_hint_after_get(struct sbitmap *sb,\n\t\t\t\t\t       unsigned int depth,\n\t\t\t\t\t       unsigned int hint,\n\t\t\t\t\t       unsigned int nr)\n{\n\tif (nr == -1) {\n\t\t \n\t\tthis_cpu_write(*sb->alloc_hint, 0);\n\t} else if (nr == hint || unlikely(sb->round_robin)) {\n\t\t \n\t\thint = nr + 1;\n\t\tif (hint >= depth - 1)\n\t\t\thint = 0;\n\t\tthis_cpu_write(*sb->alloc_hint, hint);\n\t}\n}\n\n \nstatic inline bool sbitmap_deferred_clear(struct sbitmap_word *map)\n{\n\tunsigned long mask;\n\n\tif (!READ_ONCE(map->cleared))\n\t\treturn false;\n\n\t \n\tmask = xchg(&map->cleared, 0);\n\n\t \n\tatomic_long_andnot(mask, (atomic_long_t *)&map->word);\n\tBUILD_BUG_ON(sizeof(atomic_long_t) != sizeof(map->word));\n\treturn true;\n}\n\nint sbitmap_init_node(struct sbitmap *sb, unsigned int depth, int shift,\n\t\t      gfp_t flags, int node, bool round_robin,\n\t\t      bool alloc_hint)\n{\n\tunsigned int bits_per_word;\n\n\tif (shift < 0)\n\t\tshift = sbitmap_calculate_shift(depth);\n\n\tbits_per_word = 1U << shift;\n\tif (bits_per_word > BITS_PER_LONG)\n\t\treturn -EINVAL;\n\n\tsb->shift = shift;\n\tsb->depth = depth;\n\tsb->map_nr = DIV_ROUND_UP(sb->depth, bits_per_word);\n\tsb->round_robin = round_robin;\n\n\tif (depth == 0) {\n\t\tsb->map = NULL;\n\t\treturn 0;\n\t}\n\n\tif (alloc_hint) {\n\t\tif (init_alloc_hint(sb, flags))\n\t\t\treturn -ENOMEM;\n\t} else {\n\t\tsb->alloc_hint = NULL;\n\t}\n\n\tsb->map = kvzalloc_node(sb->map_nr * sizeof(*sb->map), flags, node);\n\tif (!sb->map) {\n\t\tfree_percpu(sb->alloc_hint);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(sbitmap_init_node);\n\nvoid sbitmap_resize(struct sbitmap *sb, unsigned int depth)\n{\n\tunsigned int bits_per_word = 1U << sb->shift;\n\tunsigned int i;\n\n\tfor (i = 0; i < sb->map_nr; i++)\n\t\tsbitmap_deferred_clear(&sb->map[i]);\n\n\tsb->depth = depth;\n\tsb->map_nr = DIV_ROUND_UP(sb->depth, bits_per_word);\n}\nEXPORT_SYMBOL_GPL(sbitmap_resize);\n\nstatic int __sbitmap_get_word(unsigned long *word, unsigned long depth,\n\t\t\t      unsigned int hint, bool wrap)\n{\n\tint nr;\n\n\t \n\twrap = wrap && hint;\n\n\twhile (1) {\n\t\tnr = find_next_zero_bit(word, depth, hint);\n\t\tif (unlikely(nr >= depth)) {\n\t\t\t \n\t\t\tif (hint && wrap) {\n\t\t\t\thint = 0;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\treturn -1;\n\t\t}\n\n\t\tif (!test_and_set_bit_lock(nr, word))\n\t\t\tbreak;\n\n\t\thint = nr + 1;\n\t\tif (hint >= depth - 1)\n\t\t\thint = 0;\n\t}\n\n\treturn nr;\n}\n\nstatic int sbitmap_find_bit_in_word(struct sbitmap_word *map,\n\t\t\t\t    unsigned int depth,\n\t\t\t\t    unsigned int alloc_hint,\n\t\t\t\t    bool wrap)\n{\n\tint nr;\n\n\tdo {\n\t\tnr = __sbitmap_get_word(&map->word, depth,\n\t\t\t\t\talloc_hint, wrap);\n\t\tif (nr != -1)\n\t\t\tbreak;\n\t\tif (!sbitmap_deferred_clear(map))\n\t\t\tbreak;\n\t} while (1);\n\n\treturn nr;\n}\n\nstatic int sbitmap_find_bit(struct sbitmap *sb,\n\t\t\t    unsigned int depth,\n\t\t\t    unsigned int index,\n\t\t\t    unsigned int alloc_hint,\n\t\t\t    bool wrap)\n{\n\tunsigned int i;\n\tint nr = -1;\n\n\tfor (i = 0; i < sb->map_nr; i++) {\n\t\tnr = sbitmap_find_bit_in_word(&sb->map[index],\n\t\t\t\t\t      min_t(unsigned int,\n\t\t\t\t\t\t    __map_depth(sb, index),\n\t\t\t\t\t\t    depth),\n\t\t\t\t\t      alloc_hint, wrap);\n\n\t\tif (nr != -1) {\n\t\t\tnr += index << sb->shift;\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\talloc_hint = 0;\n\t\tif (++index >= sb->map_nr)\n\t\t\tindex = 0;\n\t}\n\n\treturn nr;\n}\n\nstatic int __sbitmap_get(struct sbitmap *sb, unsigned int alloc_hint)\n{\n\tunsigned int index;\n\n\tindex = SB_NR_TO_INDEX(sb, alloc_hint);\n\n\t \n\tif (sb->round_robin)\n\t\talloc_hint = SB_NR_TO_BIT(sb, alloc_hint);\n\telse\n\t\talloc_hint = 0;\n\n\treturn sbitmap_find_bit(sb, UINT_MAX, index, alloc_hint,\n\t\t\t\t!sb->round_robin);\n}\n\nint sbitmap_get(struct sbitmap *sb)\n{\n\tint nr;\n\tunsigned int hint, depth;\n\n\tif (WARN_ON_ONCE(unlikely(!sb->alloc_hint)))\n\t\treturn -1;\n\n\tdepth = READ_ONCE(sb->depth);\n\thint = update_alloc_hint_before_get(sb, depth);\n\tnr = __sbitmap_get(sb, hint);\n\tupdate_alloc_hint_after_get(sb, depth, hint, nr);\n\n\treturn nr;\n}\nEXPORT_SYMBOL_GPL(sbitmap_get);\n\nstatic int __sbitmap_get_shallow(struct sbitmap *sb,\n\t\t\t\t unsigned int alloc_hint,\n\t\t\t\t unsigned long shallow_depth)\n{\n\tunsigned int index;\n\n\tindex = SB_NR_TO_INDEX(sb, alloc_hint);\n\talloc_hint = SB_NR_TO_BIT(sb, alloc_hint);\n\n\treturn sbitmap_find_bit(sb, shallow_depth, index, alloc_hint, true);\n}\n\nint sbitmap_get_shallow(struct sbitmap *sb, unsigned long shallow_depth)\n{\n\tint nr;\n\tunsigned int hint, depth;\n\n\tif (WARN_ON_ONCE(unlikely(!sb->alloc_hint)))\n\t\treturn -1;\n\n\tdepth = READ_ONCE(sb->depth);\n\thint = update_alloc_hint_before_get(sb, depth);\n\tnr = __sbitmap_get_shallow(sb, hint, shallow_depth);\n\tupdate_alloc_hint_after_get(sb, depth, hint, nr);\n\n\treturn nr;\n}\nEXPORT_SYMBOL_GPL(sbitmap_get_shallow);\n\nbool sbitmap_any_bit_set(const struct sbitmap *sb)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < sb->map_nr; i++) {\n\t\tif (sb->map[i].word & ~sb->map[i].cleared)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\nEXPORT_SYMBOL_GPL(sbitmap_any_bit_set);\n\nstatic unsigned int __sbitmap_weight(const struct sbitmap *sb, bool set)\n{\n\tunsigned int i, weight = 0;\n\n\tfor (i = 0; i < sb->map_nr; i++) {\n\t\tconst struct sbitmap_word *word = &sb->map[i];\n\t\tunsigned int word_depth = __map_depth(sb, i);\n\n\t\tif (set)\n\t\t\tweight += bitmap_weight(&word->word, word_depth);\n\t\telse\n\t\t\tweight += bitmap_weight(&word->cleared, word_depth);\n\t}\n\treturn weight;\n}\n\nstatic unsigned int sbitmap_cleared(const struct sbitmap *sb)\n{\n\treturn __sbitmap_weight(sb, false);\n}\n\nunsigned int sbitmap_weight(const struct sbitmap *sb)\n{\n\treturn __sbitmap_weight(sb, true) - sbitmap_cleared(sb);\n}\nEXPORT_SYMBOL_GPL(sbitmap_weight);\n\nvoid sbitmap_show(struct sbitmap *sb, struct seq_file *m)\n{\n\tseq_printf(m, \"depth=%u\\n\", sb->depth);\n\tseq_printf(m, \"busy=%u\\n\", sbitmap_weight(sb));\n\tseq_printf(m, \"cleared=%u\\n\", sbitmap_cleared(sb));\n\tseq_printf(m, \"bits_per_word=%u\\n\", 1U << sb->shift);\n\tseq_printf(m, \"map_nr=%u\\n\", sb->map_nr);\n}\nEXPORT_SYMBOL_GPL(sbitmap_show);\n\nstatic inline void emit_byte(struct seq_file *m, unsigned int offset, u8 byte)\n{\n\tif ((offset & 0xf) == 0) {\n\t\tif (offset != 0)\n\t\t\tseq_putc(m, '\\n');\n\t\tseq_printf(m, \"%08x:\", offset);\n\t}\n\tif ((offset & 0x1) == 0)\n\t\tseq_putc(m, ' ');\n\tseq_printf(m, \"%02x\", byte);\n}\n\nvoid sbitmap_bitmap_show(struct sbitmap *sb, struct seq_file *m)\n{\n\tu8 byte = 0;\n\tunsigned int byte_bits = 0;\n\tunsigned int offset = 0;\n\tint i;\n\n\tfor (i = 0; i < sb->map_nr; i++) {\n\t\tunsigned long word = READ_ONCE(sb->map[i].word);\n\t\tunsigned long cleared = READ_ONCE(sb->map[i].cleared);\n\t\tunsigned int word_bits = __map_depth(sb, i);\n\n\t\tword &= ~cleared;\n\n\t\twhile (word_bits > 0) {\n\t\t\tunsigned int bits = min(8 - byte_bits, word_bits);\n\n\t\t\tbyte |= (word & (BIT(bits) - 1)) << byte_bits;\n\t\t\tbyte_bits += bits;\n\t\t\tif (byte_bits == 8) {\n\t\t\t\temit_byte(m, offset, byte);\n\t\t\t\tbyte = 0;\n\t\t\t\tbyte_bits = 0;\n\t\t\t\toffset++;\n\t\t\t}\n\t\t\tword >>= bits;\n\t\t\tword_bits -= bits;\n\t\t}\n\t}\n\tif (byte_bits) {\n\t\temit_byte(m, offset, byte);\n\t\toffset++;\n\t}\n\tif (offset)\n\t\tseq_putc(m, '\\n');\n}\nEXPORT_SYMBOL_GPL(sbitmap_bitmap_show);\n\nstatic unsigned int sbq_calc_wake_batch(struct sbitmap_queue *sbq,\n\t\t\t\t\tunsigned int depth)\n{\n\tunsigned int wake_batch;\n\tunsigned int shallow_depth;\n\n\t \n\tshallow_depth = min(1U << sbq->sb.shift, sbq->min_shallow_depth);\n\tdepth = ((depth >> sbq->sb.shift) * shallow_depth +\n\t\t min(depth & ((1U << sbq->sb.shift) - 1), shallow_depth));\n\twake_batch = clamp_t(unsigned int, depth / SBQ_WAIT_QUEUES, 1,\n\t\t\t     SBQ_WAKE_BATCH);\n\n\treturn wake_batch;\n}\n\nint sbitmap_queue_init_node(struct sbitmap_queue *sbq, unsigned int depth,\n\t\t\t    int shift, bool round_robin, gfp_t flags, int node)\n{\n\tint ret;\n\tint i;\n\n\tret = sbitmap_init_node(&sbq->sb, depth, shift, flags, node,\n\t\t\t\tround_robin, true);\n\tif (ret)\n\t\treturn ret;\n\n\tsbq->min_shallow_depth = UINT_MAX;\n\tsbq->wake_batch = sbq_calc_wake_batch(sbq, depth);\n\tatomic_set(&sbq->wake_index, 0);\n\tatomic_set(&sbq->ws_active, 0);\n\tatomic_set(&sbq->completion_cnt, 0);\n\tatomic_set(&sbq->wakeup_cnt, 0);\n\n\tsbq->ws = kzalloc_node(SBQ_WAIT_QUEUES * sizeof(*sbq->ws), flags, node);\n\tif (!sbq->ws) {\n\t\tsbitmap_free(&sbq->sb);\n\t\treturn -ENOMEM;\n\t}\n\n\tfor (i = 0; i < SBQ_WAIT_QUEUES; i++)\n\t\tinit_waitqueue_head(&sbq->ws[i].wait);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(sbitmap_queue_init_node);\n\nstatic void sbitmap_queue_update_wake_batch(struct sbitmap_queue *sbq,\n\t\t\t\t\t    unsigned int depth)\n{\n\tunsigned int wake_batch;\n\n\twake_batch = sbq_calc_wake_batch(sbq, depth);\n\tif (sbq->wake_batch != wake_batch)\n\t\tWRITE_ONCE(sbq->wake_batch, wake_batch);\n}\n\nvoid sbitmap_queue_recalculate_wake_batch(struct sbitmap_queue *sbq,\n\t\t\t\t\t    unsigned int users)\n{\n\tunsigned int wake_batch;\n\tunsigned int depth = (sbq->sb.depth + users - 1) / users;\n\n\twake_batch = clamp_val(depth / SBQ_WAIT_QUEUES,\n\t\t\t1, SBQ_WAKE_BATCH);\n\n\tWRITE_ONCE(sbq->wake_batch, wake_batch);\n}\nEXPORT_SYMBOL_GPL(sbitmap_queue_recalculate_wake_batch);\n\nvoid sbitmap_queue_resize(struct sbitmap_queue *sbq, unsigned int depth)\n{\n\tsbitmap_queue_update_wake_batch(sbq, depth);\n\tsbitmap_resize(&sbq->sb, depth);\n}\nEXPORT_SYMBOL_GPL(sbitmap_queue_resize);\n\nint __sbitmap_queue_get(struct sbitmap_queue *sbq)\n{\n\treturn sbitmap_get(&sbq->sb);\n}\nEXPORT_SYMBOL_GPL(__sbitmap_queue_get);\n\nunsigned long __sbitmap_queue_get_batch(struct sbitmap_queue *sbq, int nr_tags,\n\t\t\t\t\tunsigned int *offset)\n{\n\tstruct sbitmap *sb = &sbq->sb;\n\tunsigned int hint, depth;\n\tunsigned long index, nr;\n\tint i;\n\n\tif (unlikely(sb->round_robin))\n\t\treturn 0;\n\n\tdepth = READ_ONCE(sb->depth);\n\thint = update_alloc_hint_before_get(sb, depth);\n\n\tindex = SB_NR_TO_INDEX(sb, hint);\n\n\tfor (i = 0; i < sb->map_nr; i++) {\n\t\tstruct sbitmap_word *map = &sb->map[index];\n\t\tunsigned long get_mask;\n\t\tunsigned int map_depth = __map_depth(sb, index);\n\n\t\tsbitmap_deferred_clear(map);\n\t\tif (map->word == (1UL << (map_depth - 1)) - 1)\n\t\t\tgoto next;\n\n\t\tnr = find_first_zero_bit(&map->word, map_depth);\n\t\tif (nr + nr_tags <= map_depth) {\n\t\t\tatomic_long_t *ptr = (atomic_long_t *) &map->word;\n\t\t\tunsigned long val;\n\n\t\t\tget_mask = ((1UL << nr_tags) - 1) << nr;\n\t\t\tval = READ_ONCE(map->word);\n\t\t\twhile (!atomic_long_try_cmpxchg(ptr, &val,\n\t\t\t\t\t\t\t  get_mask | val))\n\t\t\t\t;\n\t\t\tget_mask = (get_mask & ~val) >> nr;\n\t\t\tif (get_mask) {\n\t\t\t\t*offset = nr + (index << sb->shift);\n\t\t\t\tupdate_alloc_hint_after_get(sb, depth, hint,\n\t\t\t\t\t\t\t*offset + nr_tags - 1);\n\t\t\t\treturn get_mask;\n\t\t\t}\n\t\t}\nnext:\n\t\t \n\t\tif (++index >= sb->map_nr)\n\t\t\tindex = 0;\n\t}\n\n\treturn 0;\n}\n\nint sbitmap_queue_get_shallow(struct sbitmap_queue *sbq,\n\t\t\t      unsigned int shallow_depth)\n{\n\tWARN_ON_ONCE(shallow_depth < sbq->min_shallow_depth);\n\n\treturn sbitmap_get_shallow(&sbq->sb, shallow_depth);\n}\nEXPORT_SYMBOL_GPL(sbitmap_queue_get_shallow);\n\nvoid sbitmap_queue_min_shallow_depth(struct sbitmap_queue *sbq,\n\t\t\t\t     unsigned int min_shallow_depth)\n{\n\tsbq->min_shallow_depth = min_shallow_depth;\n\tsbitmap_queue_update_wake_batch(sbq, sbq->sb.depth);\n}\nEXPORT_SYMBOL_GPL(sbitmap_queue_min_shallow_depth);\n\nstatic void __sbitmap_queue_wake_up(struct sbitmap_queue *sbq, int nr)\n{\n\tint i, wake_index, woken;\n\n\tif (!atomic_read(&sbq->ws_active))\n\t\treturn;\n\n\twake_index = atomic_read(&sbq->wake_index);\n\tfor (i = 0; i < SBQ_WAIT_QUEUES; i++) {\n\t\tstruct sbq_wait_state *ws = &sbq->ws[wake_index];\n\n\t\t \n\t\twake_index = sbq_index_inc(wake_index);\n\n\t\tif (waitqueue_active(&ws->wait)) {\n\t\t\twoken = wake_up_nr(&ws->wait, nr);\n\t\t\tif (woken == nr)\n\t\t\t\tbreak;\n\t\t\tnr -= woken;\n\t\t}\n\t}\n\n\tif (wake_index != atomic_read(&sbq->wake_index))\n\t\tatomic_set(&sbq->wake_index, wake_index);\n}\n\nvoid sbitmap_queue_wake_up(struct sbitmap_queue *sbq, int nr)\n{\n\tunsigned int wake_batch = READ_ONCE(sbq->wake_batch);\n\tunsigned int wakeups;\n\n\tif (!atomic_read(&sbq->ws_active))\n\t\treturn;\n\n\tatomic_add(nr, &sbq->completion_cnt);\n\twakeups = atomic_read(&sbq->wakeup_cnt);\n\n\tdo {\n\t\tif (atomic_read(&sbq->completion_cnt) - wakeups < wake_batch)\n\t\t\treturn;\n\t} while (!atomic_try_cmpxchg(&sbq->wakeup_cnt,\n\t\t\t\t     &wakeups, wakeups + wake_batch));\n\n\t__sbitmap_queue_wake_up(sbq, wake_batch);\n}\nEXPORT_SYMBOL_GPL(sbitmap_queue_wake_up);\n\nstatic inline void sbitmap_update_cpu_hint(struct sbitmap *sb, int cpu, int tag)\n{\n\tif (likely(!sb->round_robin && tag < sb->depth))\n\t\tdata_race(*per_cpu_ptr(sb->alloc_hint, cpu) = tag);\n}\n\nvoid sbitmap_queue_clear_batch(struct sbitmap_queue *sbq, int offset,\n\t\t\t\tint *tags, int nr_tags)\n{\n\tstruct sbitmap *sb = &sbq->sb;\n\tunsigned long *addr = NULL;\n\tunsigned long mask = 0;\n\tint i;\n\n\tsmp_mb__before_atomic();\n\tfor (i = 0; i < nr_tags; i++) {\n\t\tconst int tag = tags[i] - offset;\n\t\tunsigned long *this_addr;\n\n\t\t \n\t\tthis_addr = &sb->map[SB_NR_TO_INDEX(sb, tag)].word;\n\t\tif (!addr) {\n\t\t\taddr = this_addr;\n\t\t} else if (addr != this_addr) {\n\t\t\tatomic_long_andnot(mask, (atomic_long_t *) addr);\n\t\t\tmask = 0;\n\t\t\taddr = this_addr;\n\t\t}\n\t\tmask |= (1UL << SB_NR_TO_BIT(sb, tag));\n\t}\n\n\tif (mask)\n\t\tatomic_long_andnot(mask, (atomic_long_t *) addr);\n\n\tsmp_mb__after_atomic();\n\tsbitmap_queue_wake_up(sbq, nr_tags);\n\tsbitmap_update_cpu_hint(&sbq->sb, raw_smp_processor_id(),\n\t\t\t\t\ttags[nr_tags - 1] - offset);\n}\n\nvoid sbitmap_queue_clear(struct sbitmap_queue *sbq, unsigned int nr,\n\t\t\t unsigned int cpu)\n{\n\t \n\tsmp_mb__before_atomic();\n\tsbitmap_deferred_clear_bit(&sbq->sb, nr);\n\n\t \n\tsmp_mb__after_atomic();\n\tsbitmap_queue_wake_up(sbq, 1);\n\tsbitmap_update_cpu_hint(&sbq->sb, cpu, nr);\n}\nEXPORT_SYMBOL_GPL(sbitmap_queue_clear);\n\nvoid sbitmap_queue_wake_all(struct sbitmap_queue *sbq)\n{\n\tint i, wake_index;\n\n\t \n\tsmp_mb();\n\twake_index = atomic_read(&sbq->wake_index);\n\tfor (i = 0; i < SBQ_WAIT_QUEUES; i++) {\n\t\tstruct sbq_wait_state *ws = &sbq->ws[wake_index];\n\n\t\tif (waitqueue_active(&ws->wait))\n\t\t\twake_up(&ws->wait);\n\n\t\twake_index = sbq_index_inc(wake_index);\n\t}\n}\nEXPORT_SYMBOL_GPL(sbitmap_queue_wake_all);\n\nvoid sbitmap_queue_show(struct sbitmap_queue *sbq, struct seq_file *m)\n{\n\tbool first;\n\tint i;\n\n\tsbitmap_show(&sbq->sb, m);\n\n\tseq_puts(m, \"alloc_hint={\");\n\tfirst = true;\n\tfor_each_possible_cpu(i) {\n\t\tif (!first)\n\t\t\tseq_puts(m, \", \");\n\t\tfirst = false;\n\t\tseq_printf(m, \"%u\", *per_cpu_ptr(sbq->sb.alloc_hint, i));\n\t}\n\tseq_puts(m, \"}\\n\");\n\n\tseq_printf(m, \"wake_batch=%u\\n\", sbq->wake_batch);\n\tseq_printf(m, \"wake_index=%d\\n\", atomic_read(&sbq->wake_index));\n\tseq_printf(m, \"ws_active=%d\\n\", atomic_read(&sbq->ws_active));\n\n\tseq_puts(m, \"ws={\\n\");\n\tfor (i = 0; i < SBQ_WAIT_QUEUES; i++) {\n\t\tstruct sbq_wait_state *ws = &sbq->ws[i];\n\t\tseq_printf(m, \"\\t{.wait=%s},\\n\",\n\t\t\t   waitqueue_active(&ws->wait) ? \"active\" : \"inactive\");\n\t}\n\tseq_puts(m, \"}\\n\");\n\n\tseq_printf(m, \"round_robin=%d\\n\", sbq->sb.round_robin);\n\tseq_printf(m, \"min_shallow_depth=%u\\n\", sbq->min_shallow_depth);\n}\nEXPORT_SYMBOL_GPL(sbitmap_queue_show);\n\nvoid sbitmap_add_wait_queue(struct sbitmap_queue *sbq,\n\t\t\t    struct sbq_wait_state *ws,\n\t\t\t    struct sbq_wait *sbq_wait)\n{\n\tif (!sbq_wait->sbq) {\n\t\tsbq_wait->sbq = sbq;\n\t\tatomic_inc(&sbq->ws_active);\n\t\tadd_wait_queue(&ws->wait, &sbq_wait->wait);\n\t}\n}\nEXPORT_SYMBOL_GPL(sbitmap_add_wait_queue);\n\nvoid sbitmap_del_wait_queue(struct sbq_wait *sbq_wait)\n{\n\tlist_del_init(&sbq_wait->wait.entry);\n\tif (sbq_wait->sbq) {\n\t\tatomic_dec(&sbq_wait->sbq->ws_active);\n\t\tsbq_wait->sbq = NULL;\n\t}\n}\nEXPORT_SYMBOL_GPL(sbitmap_del_wait_queue);\n\nvoid sbitmap_prepare_to_wait(struct sbitmap_queue *sbq,\n\t\t\t     struct sbq_wait_state *ws,\n\t\t\t     struct sbq_wait *sbq_wait, int state)\n{\n\tif (!sbq_wait->sbq) {\n\t\tatomic_inc(&sbq->ws_active);\n\t\tsbq_wait->sbq = sbq;\n\t}\n\tprepare_to_wait_exclusive(&ws->wait, &sbq_wait->wait, state);\n}\nEXPORT_SYMBOL_GPL(sbitmap_prepare_to_wait);\n\nvoid sbitmap_finish_wait(struct sbitmap_queue *sbq, struct sbq_wait_state *ws,\n\t\t\t struct sbq_wait *sbq_wait)\n{\n\tfinish_wait(&ws->wait, &sbq_wait->wait);\n\tif (sbq_wait->sbq) {\n\t\tatomic_dec(&sbq->ws_active);\n\t\tsbq_wait->sbq = NULL;\n\t}\n}\nEXPORT_SYMBOL_GPL(sbitmap_finish_wait);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}