{
  "module_name": "curve25519-hacl64.c",
  "hash_id": "bc411f7bfa306bb9dceb0ff290fe3222f076c7c313b52a5760ce78f292f87754",
  "original_prompt": "Ingested from linux-6.6.14/lib/crypto/curve25519-hacl64.c",
  "human_readable_source": "\n \n\n#include <asm/unaligned.h>\n#include <crypto/curve25519.h>\n#include <linux/string.h>\n\nstatic __always_inline u64 u64_eq_mask(u64 a, u64 b)\n{\n\tu64 x = a ^ b;\n\tu64 minus_x = ~x + (u64)1U;\n\tu64 x_or_minus_x = x | minus_x;\n\tu64 xnx = x_or_minus_x >> (u32)63U;\n\tu64 c = xnx - (u64)1U;\n\treturn c;\n}\n\nstatic __always_inline u64 u64_gte_mask(u64 a, u64 b)\n{\n\tu64 x = a;\n\tu64 y = b;\n\tu64 x_xor_y = x ^ y;\n\tu64 x_sub_y = x - y;\n\tu64 x_sub_y_xor_y = x_sub_y ^ y;\n\tu64 q = x_xor_y | x_sub_y_xor_y;\n\tu64 x_xor_q = x ^ q;\n\tu64 x_xor_q_ = x_xor_q >> (u32)63U;\n\tu64 c = x_xor_q_ - (u64)1U;\n\treturn c;\n}\n\nstatic __always_inline void modulo_carry_top(u64 *b)\n{\n\tu64 b4 = b[4];\n\tu64 b0 = b[0];\n\tu64 b4_ = b4 & 0x7ffffffffffffLLU;\n\tu64 b0_ = b0 + 19 * (b4 >> 51);\n\tb[4] = b4_;\n\tb[0] = b0_;\n}\n\nstatic __always_inline void fproduct_copy_from_wide_(u64 *output, u128 *input)\n{\n\t{\n\t\tu128 xi = input[0];\n\t\toutput[0] = ((u64)(xi));\n\t}\n\t{\n\t\tu128 xi = input[1];\n\t\toutput[1] = ((u64)(xi));\n\t}\n\t{\n\t\tu128 xi = input[2];\n\t\toutput[2] = ((u64)(xi));\n\t}\n\t{\n\t\tu128 xi = input[3];\n\t\toutput[3] = ((u64)(xi));\n\t}\n\t{\n\t\tu128 xi = input[4];\n\t\toutput[4] = ((u64)(xi));\n\t}\n}\n\nstatic __always_inline void\nfproduct_sum_scalar_multiplication_(u128 *output, u64 *input, u64 s)\n{\n\toutput[0] += (u128)input[0] * s;\n\toutput[1] += (u128)input[1] * s;\n\toutput[2] += (u128)input[2] * s;\n\toutput[3] += (u128)input[3] * s;\n\toutput[4] += (u128)input[4] * s;\n}\n\nstatic __always_inline void fproduct_carry_wide_(u128 *tmp)\n{\n\t{\n\t\tu32 ctr = 0;\n\t\tu128 tctr = tmp[ctr];\n\t\tu128 tctrp1 = tmp[ctr + 1];\n\t\tu64 r0 = ((u64)(tctr)) & 0x7ffffffffffffLLU;\n\t\tu128 c = ((tctr) >> (51));\n\t\ttmp[ctr] = ((u128)(r0));\n\t\ttmp[ctr + 1] = ((tctrp1) + (c));\n\t}\n\t{\n\t\tu32 ctr = 1;\n\t\tu128 tctr = tmp[ctr];\n\t\tu128 tctrp1 = tmp[ctr + 1];\n\t\tu64 r0 = ((u64)(tctr)) & 0x7ffffffffffffLLU;\n\t\tu128 c = ((tctr) >> (51));\n\t\ttmp[ctr] = ((u128)(r0));\n\t\ttmp[ctr + 1] = ((tctrp1) + (c));\n\t}\n\n\t{\n\t\tu32 ctr = 2;\n\t\tu128 tctr = tmp[ctr];\n\t\tu128 tctrp1 = tmp[ctr + 1];\n\t\tu64 r0 = ((u64)(tctr)) & 0x7ffffffffffffLLU;\n\t\tu128 c = ((tctr) >> (51));\n\t\ttmp[ctr] = ((u128)(r0));\n\t\ttmp[ctr + 1] = ((tctrp1) + (c));\n\t}\n\t{\n\t\tu32 ctr = 3;\n\t\tu128 tctr = tmp[ctr];\n\t\tu128 tctrp1 = tmp[ctr + 1];\n\t\tu64 r0 = ((u64)(tctr)) & 0x7ffffffffffffLLU;\n\t\tu128 c = ((tctr) >> (51));\n\t\ttmp[ctr] = ((u128)(r0));\n\t\ttmp[ctr + 1] = ((tctrp1) + (c));\n\t}\n}\n\nstatic __always_inline void fmul_shift_reduce(u64 *output)\n{\n\tu64 tmp = output[4];\n\tu64 b0;\n\t{\n\t\tu32 ctr = 5 - 0 - 1;\n\t\tu64 z = output[ctr - 1];\n\t\toutput[ctr] = z;\n\t}\n\t{\n\t\tu32 ctr = 5 - 1 - 1;\n\t\tu64 z = output[ctr - 1];\n\t\toutput[ctr] = z;\n\t}\n\t{\n\t\tu32 ctr = 5 - 2 - 1;\n\t\tu64 z = output[ctr - 1];\n\t\toutput[ctr] = z;\n\t}\n\t{\n\t\tu32 ctr = 5 - 3 - 1;\n\t\tu64 z = output[ctr - 1];\n\t\toutput[ctr] = z;\n\t}\n\toutput[0] = tmp;\n\tb0 = output[0];\n\toutput[0] = 19 * b0;\n}\n\nstatic __always_inline void fmul_mul_shift_reduce_(u128 *output, u64 *input,\n\t\t\t\t\t\t   u64 *input21)\n{\n\tu32 i;\n\tu64 input2i;\n\t{\n\t\tu64 input2i = input21[0];\n\t\tfproduct_sum_scalar_multiplication_(output, input, input2i);\n\t\tfmul_shift_reduce(input);\n\t}\n\t{\n\t\tu64 input2i = input21[1];\n\t\tfproduct_sum_scalar_multiplication_(output, input, input2i);\n\t\tfmul_shift_reduce(input);\n\t}\n\t{\n\t\tu64 input2i = input21[2];\n\t\tfproduct_sum_scalar_multiplication_(output, input, input2i);\n\t\tfmul_shift_reduce(input);\n\t}\n\t{\n\t\tu64 input2i = input21[3];\n\t\tfproduct_sum_scalar_multiplication_(output, input, input2i);\n\t\tfmul_shift_reduce(input);\n\t}\n\ti = 4;\n\tinput2i = input21[i];\n\tfproduct_sum_scalar_multiplication_(output, input, input2i);\n}\n\nstatic __always_inline void fmul_fmul(u64 *output, u64 *input, u64 *input21)\n{\n\tu64 tmp[5] = { input[0], input[1], input[2], input[3], input[4] };\n\t{\n\t\tu128 b4;\n\t\tu128 b0;\n\t\tu128 b4_;\n\t\tu128 b0_;\n\t\tu64 i0;\n\t\tu64 i1;\n\t\tu64 i0_;\n\t\tu64 i1_;\n\t\tu128 t[5] = { 0 };\n\t\tfmul_mul_shift_reduce_(t, tmp, input21);\n\t\tfproduct_carry_wide_(t);\n\t\tb4 = t[4];\n\t\tb0 = t[0];\n\t\tb4_ = ((b4) & (((u128)(0x7ffffffffffffLLU))));\n\t\tb0_ = ((b0) + (((u128)(19) * (((u64)(((b4) >> (51))))))));\n\t\tt[4] = b4_;\n\t\tt[0] = b0_;\n\t\tfproduct_copy_from_wide_(output, t);\n\t\ti0 = output[0];\n\t\ti1 = output[1];\n\t\ti0_ = i0 & 0x7ffffffffffffLLU;\n\t\ti1_ = i1 + (i0 >> 51);\n\t\toutput[0] = i0_;\n\t\toutput[1] = i1_;\n\t}\n}\n\nstatic __always_inline void fsquare_fsquare__(u128 *tmp, u64 *output)\n{\n\tu64 r0 = output[0];\n\tu64 r1 = output[1];\n\tu64 r2 = output[2];\n\tu64 r3 = output[3];\n\tu64 r4 = output[4];\n\tu64 d0 = r0 * 2;\n\tu64 d1 = r1 * 2;\n\tu64 d2 = r2 * 2 * 19;\n\tu64 d419 = r4 * 19;\n\tu64 d4 = d419 * 2;\n\tu128 s0 = ((((((u128)(r0) * (r0))) + (((u128)(d4) * (r1))))) +\n\t\t   (((u128)(d2) * (r3))));\n\tu128 s1 = ((((((u128)(d0) * (r1))) + (((u128)(d4) * (r2))))) +\n\t\t   (((u128)(r3 * 19) * (r3))));\n\tu128 s2 = ((((((u128)(d0) * (r2))) + (((u128)(r1) * (r1))))) +\n\t\t   (((u128)(d4) * (r3))));\n\tu128 s3 = ((((((u128)(d0) * (r3))) + (((u128)(d1) * (r2))))) +\n\t\t   (((u128)(r4) * (d419))));\n\tu128 s4 = ((((((u128)(d0) * (r4))) + (((u128)(d1) * (r3))))) +\n\t\t   (((u128)(r2) * (r2))));\n\ttmp[0] = s0;\n\ttmp[1] = s1;\n\ttmp[2] = s2;\n\ttmp[3] = s3;\n\ttmp[4] = s4;\n}\n\nstatic __always_inline void fsquare_fsquare_(u128 *tmp, u64 *output)\n{\n\tu128 b4;\n\tu128 b0;\n\tu128 b4_;\n\tu128 b0_;\n\tu64 i0;\n\tu64 i1;\n\tu64 i0_;\n\tu64 i1_;\n\tfsquare_fsquare__(tmp, output);\n\tfproduct_carry_wide_(tmp);\n\tb4 = tmp[4];\n\tb0 = tmp[0];\n\tb4_ = ((b4) & (((u128)(0x7ffffffffffffLLU))));\n\tb0_ = ((b0) + (((u128)(19) * (((u64)(((b4) >> (51))))))));\n\ttmp[4] = b4_;\n\ttmp[0] = b0_;\n\tfproduct_copy_from_wide_(output, tmp);\n\ti0 = output[0];\n\ti1 = output[1];\n\ti0_ = i0 & 0x7ffffffffffffLLU;\n\ti1_ = i1 + (i0 >> 51);\n\toutput[0] = i0_;\n\toutput[1] = i1_;\n}\n\nstatic __always_inline void fsquare_fsquare_times_(u64 *output, u128 *tmp,\n\t\t\t\t\t\t   u32 count1)\n{\n\tu32 i;\n\tfsquare_fsquare_(tmp, output);\n\tfor (i = 1; i < count1; ++i)\n\t\tfsquare_fsquare_(tmp, output);\n}\n\nstatic __always_inline void fsquare_fsquare_times(u64 *output, u64 *input,\n\t\t\t\t\t\t  u32 count1)\n{\n\tu128 t[5];\n\tmemcpy(output, input, 5 * sizeof(*input));\n\tfsquare_fsquare_times_(output, t, count1);\n}\n\nstatic __always_inline void fsquare_fsquare_times_inplace(u64 *output,\n\t\t\t\t\t\t\t  u32 count1)\n{\n\tu128 t[5];\n\tfsquare_fsquare_times_(output, t, count1);\n}\n\nstatic __always_inline void crecip_crecip(u64 *out, u64 *z)\n{\n\tu64 buf[20] = { 0 };\n\tu64 *a0 = buf;\n\tu64 *t00 = buf + 5;\n\tu64 *b0 = buf + 10;\n\tu64 *t01;\n\tu64 *b1;\n\tu64 *c0;\n\tu64 *a;\n\tu64 *t0;\n\tu64 *b;\n\tu64 *c;\n\tfsquare_fsquare_times(a0, z, 1);\n\tfsquare_fsquare_times(t00, a0, 2);\n\tfmul_fmul(b0, t00, z);\n\tfmul_fmul(a0, b0, a0);\n\tfsquare_fsquare_times(t00, a0, 1);\n\tfmul_fmul(b0, t00, b0);\n\tfsquare_fsquare_times(t00, b0, 5);\n\tt01 = buf + 5;\n\tb1 = buf + 10;\n\tc0 = buf + 15;\n\tfmul_fmul(b1, t01, b1);\n\tfsquare_fsquare_times(t01, b1, 10);\n\tfmul_fmul(c0, t01, b1);\n\tfsquare_fsquare_times(t01, c0, 20);\n\tfmul_fmul(t01, t01, c0);\n\tfsquare_fsquare_times_inplace(t01, 10);\n\tfmul_fmul(b1, t01, b1);\n\tfsquare_fsquare_times(t01, b1, 50);\n\ta = buf;\n\tt0 = buf + 5;\n\tb = buf + 10;\n\tc = buf + 15;\n\tfmul_fmul(c, t0, b);\n\tfsquare_fsquare_times(t0, c, 100);\n\tfmul_fmul(t0, t0, c);\n\tfsquare_fsquare_times_inplace(t0, 50);\n\tfmul_fmul(t0, t0, b);\n\tfsquare_fsquare_times_inplace(t0, 5);\n\tfmul_fmul(out, t0, a);\n}\n\nstatic __always_inline void fsum(u64 *a, u64 *b)\n{\n\ta[0] += b[0];\n\ta[1] += b[1];\n\ta[2] += b[2];\n\ta[3] += b[3];\n\ta[4] += b[4];\n}\n\nstatic __always_inline void fdifference(u64 *a, u64 *b)\n{\n\tu64 tmp[5] = { 0 };\n\tu64 b0;\n\tu64 b1;\n\tu64 b2;\n\tu64 b3;\n\tu64 b4;\n\tmemcpy(tmp, b, 5 * sizeof(*b));\n\tb0 = tmp[0];\n\tb1 = tmp[1];\n\tb2 = tmp[2];\n\tb3 = tmp[3];\n\tb4 = tmp[4];\n\ttmp[0] = b0 + 0x3fffffffffff68LLU;\n\ttmp[1] = b1 + 0x3ffffffffffff8LLU;\n\ttmp[2] = b2 + 0x3ffffffffffff8LLU;\n\ttmp[3] = b3 + 0x3ffffffffffff8LLU;\n\ttmp[4] = b4 + 0x3ffffffffffff8LLU;\n\t{\n\t\tu64 xi = a[0];\n\t\tu64 yi = tmp[0];\n\t\ta[0] = yi - xi;\n\t}\n\t{\n\t\tu64 xi = a[1];\n\t\tu64 yi = tmp[1];\n\t\ta[1] = yi - xi;\n\t}\n\t{\n\t\tu64 xi = a[2];\n\t\tu64 yi = tmp[2];\n\t\ta[2] = yi - xi;\n\t}\n\t{\n\t\tu64 xi = a[3];\n\t\tu64 yi = tmp[3];\n\t\ta[3] = yi - xi;\n\t}\n\t{\n\t\tu64 xi = a[4];\n\t\tu64 yi = tmp[4];\n\t\ta[4] = yi - xi;\n\t}\n}\n\nstatic __always_inline void fscalar(u64 *output, u64 *b, u64 s)\n{\n\tu128 tmp[5];\n\tu128 b4;\n\tu128 b0;\n\tu128 b4_;\n\tu128 b0_;\n\t{\n\t\tu64 xi = b[0];\n\t\ttmp[0] = ((u128)(xi) * (s));\n\t}\n\t{\n\t\tu64 xi = b[1];\n\t\ttmp[1] = ((u128)(xi) * (s));\n\t}\n\t{\n\t\tu64 xi = b[2];\n\t\ttmp[2] = ((u128)(xi) * (s));\n\t}\n\t{\n\t\tu64 xi = b[3];\n\t\ttmp[3] = ((u128)(xi) * (s));\n\t}\n\t{\n\t\tu64 xi = b[4];\n\t\ttmp[4] = ((u128)(xi) * (s));\n\t}\n\tfproduct_carry_wide_(tmp);\n\tb4 = tmp[4];\n\tb0 = tmp[0];\n\tb4_ = ((b4) & (((u128)(0x7ffffffffffffLLU))));\n\tb0_ = ((b0) + (((u128)(19) * (((u64)(((b4) >> (51))))))));\n\ttmp[4] = b4_;\n\ttmp[0] = b0_;\n\tfproduct_copy_from_wide_(output, tmp);\n}\n\nstatic __always_inline void fmul(u64 *output, u64 *a, u64 *b)\n{\n\tfmul_fmul(output, a, b);\n}\n\nstatic __always_inline void crecip(u64 *output, u64 *input)\n{\n\tcrecip_crecip(output, input);\n}\n\nstatic __always_inline void point_swap_conditional_step(u64 *a, u64 *b,\n\t\t\t\t\t\t\tu64 swap1, u32 ctr)\n{\n\tu32 i = ctr - 1;\n\tu64 ai = a[i];\n\tu64 bi = b[i];\n\tu64 x = swap1 & (ai ^ bi);\n\tu64 ai1 = ai ^ x;\n\tu64 bi1 = bi ^ x;\n\ta[i] = ai1;\n\tb[i] = bi1;\n}\n\nstatic __always_inline void point_swap_conditional5(u64 *a, u64 *b, u64 swap1)\n{\n\tpoint_swap_conditional_step(a, b, swap1, 5);\n\tpoint_swap_conditional_step(a, b, swap1, 4);\n\tpoint_swap_conditional_step(a, b, swap1, 3);\n\tpoint_swap_conditional_step(a, b, swap1, 2);\n\tpoint_swap_conditional_step(a, b, swap1, 1);\n}\n\nstatic __always_inline void point_swap_conditional(u64 *a, u64 *b, u64 iswap)\n{\n\tu64 swap1 = 0 - iswap;\n\tpoint_swap_conditional5(a, b, swap1);\n\tpoint_swap_conditional5(a + 5, b + 5, swap1);\n}\n\nstatic __always_inline void point_copy(u64 *output, u64 *input)\n{\n\tmemcpy(output, input, 5 * sizeof(*input));\n\tmemcpy(output + 5, input + 5, 5 * sizeof(*input));\n}\n\nstatic __always_inline void addanddouble_fmonty(u64 *pp, u64 *ppq, u64 *p,\n\t\t\t\t\t\tu64 *pq, u64 *qmqp)\n{\n\tu64 *qx = qmqp;\n\tu64 *x2 = pp;\n\tu64 *z2 = pp + 5;\n\tu64 *x3 = ppq;\n\tu64 *z3 = ppq + 5;\n\tu64 *x = p;\n\tu64 *z = p + 5;\n\tu64 *xprime = pq;\n\tu64 *zprime = pq + 5;\n\tu64 buf[40] = { 0 };\n\tu64 *origx = buf;\n\tu64 *origxprime0 = buf + 5;\n\tu64 *xxprime0;\n\tu64 *zzprime0;\n\tu64 *origxprime;\n\txxprime0 = buf + 25;\n\tzzprime0 = buf + 30;\n\tmemcpy(origx, x, 5 * sizeof(*x));\n\tfsum(x, z);\n\tfdifference(z, origx);\n\tmemcpy(origxprime0, xprime, 5 * sizeof(*xprime));\n\tfsum(xprime, zprime);\n\tfdifference(zprime, origxprime0);\n\tfmul(xxprime0, xprime, z);\n\tfmul(zzprime0, x, zprime);\n\torigxprime = buf + 5;\n\t{\n\t\tu64 *xx0;\n\t\tu64 *zz0;\n\t\tu64 *xxprime;\n\t\tu64 *zzprime;\n\t\tu64 *zzzprime;\n\t\txx0 = buf + 15;\n\t\tzz0 = buf + 20;\n\t\txxprime = buf + 25;\n\t\tzzprime = buf + 30;\n\t\tzzzprime = buf + 35;\n\t\tmemcpy(origxprime, xxprime, 5 * sizeof(*xxprime));\n\t\tfsum(xxprime, zzprime);\n\t\tfdifference(zzprime, origxprime);\n\t\tfsquare_fsquare_times(x3, xxprime, 1);\n\t\tfsquare_fsquare_times(zzzprime, zzprime, 1);\n\t\tfmul(z3, zzzprime, qx);\n\t\tfsquare_fsquare_times(xx0, x, 1);\n\t\tfsquare_fsquare_times(zz0, z, 1);\n\t\t{\n\t\t\tu64 *zzz;\n\t\t\tu64 *xx;\n\t\t\tu64 *zz;\n\t\t\tu64 scalar;\n\t\t\tzzz = buf + 10;\n\t\t\txx = buf + 15;\n\t\t\tzz = buf + 20;\n\t\t\tfmul(x2, xx, zz);\n\t\t\tfdifference(zz, xx);\n\t\t\tscalar = 121665;\n\t\t\tfscalar(zzz, zz, scalar);\n\t\t\tfsum(zzz, xx);\n\t\t\tfmul(z2, zzz, zz);\n\t\t}\n\t}\n}\n\nstatic __always_inline void\nladder_smallloop_cmult_small_loop_step(u64 *nq, u64 *nqpq, u64 *nq2, u64 *nqpq2,\n\t\t\t\t       u64 *q, u8 byt)\n{\n\tu64 bit0 = (u64)(byt >> 7);\n\tu64 bit;\n\tpoint_swap_conditional(nq, nqpq, bit0);\n\taddanddouble_fmonty(nq2, nqpq2, nq, nqpq, q);\n\tbit = (u64)(byt >> 7);\n\tpoint_swap_conditional(nq2, nqpq2, bit);\n}\n\nstatic __always_inline void\nladder_smallloop_cmult_small_loop_double_step(u64 *nq, u64 *nqpq, u64 *nq2,\n\t\t\t\t\t      u64 *nqpq2, u64 *q, u8 byt)\n{\n\tu8 byt1;\n\tladder_smallloop_cmult_small_loop_step(nq, nqpq, nq2, nqpq2, q, byt);\n\tbyt1 = byt << 1;\n\tladder_smallloop_cmult_small_loop_step(nq2, nqpq2, nq, nqpq, q, byt1);\n}\n\nstatic __always_inline void\nladder_smallloop_cmult_small_loop(u64 *nq, u64 *nqpq, u64 *nq2, u64 *nqpq2,\n\t\t\t\t  u64 *q, u8 byt, u32 i)\n{\n\twhile (i--) {\n\t\tladder_smallloop_cmult_small_loop_double_step(nq, nqpq, nq2,\n\t\t\t\t\t\t\t      nqpq2, q, byt);\n\t\tbyt <<= 2;\n\t}\n}\n\nstatic __always_inline void ladder_bigloop_cmult_big_loop(u8 *n1, u64 *nq,\n\t\t\t\t\t\t\t  u64 *nqpq, u64 *nq2,\n\t\t\t\t\t\t\t  u64 *nqpq2, u64 *q,\n\t\t\t\t\t\t\t  u32 i)\n{\n\twhile (i--) {\n\t\tu8 byte = n1[i];\n\t\tladder_smallloop_cmult_small_loop(nq, nqpq, nq2, nqpq2, q,\n\t\t\t\t\t\t  byte, 4);\n\t}\n}\n\nstatic void ladder_cmult(u64 *result, u8 *n1, u64 *q)\n{\n\tu64 point_buf[40] = { 0 };\n\tu64 *nq = point_buf;\n\tu64 *nqpq = point_buf + 10;\n\tu64 *nq2 = point_buf + 20;\n\tu64 *nqpq2 = point_buf + 30;\n\tpoint_copy(nqpq, q);\n\tnq[0] = 1;\n\tladder_bigloop_cmult_big_loop(n1, nq, nqpq, nq2, nqpq2, q, 32);\n\tpoint_copy(result, nq);\n}\n\nstatic __always_inline void format_fexpand(u64 *output, const u8 *input)\n{\n\tconst u8 *x00 = input + 6;\n\tconst u8 *x01 = input + 12;\n\tconst u8 *x02 = input + 19;\n\tconst u8 *x0 = input + 24;\n\tu64 i0, i1, i2, i3, i4, output0, output1, output2, output3, output4;\n\ti0 = get_unaligned_le64(input);\n\ti1 = get_unaligned_le64(x00);\n\ti2 = get_unaligned_le64(x01);\n\ti3 = get_unaligned_le64(x02);\n\ti4 = get_unaligned_le64(x0);\n\toutput0 = i0 & 0x7ffffffffffffLLU;\n\toutput1 = i1 >> 3 & 0x7ffffffffffffLLU;\n\toutput2 = i2 >> 6 & 0x7ffffffffffffLLU;\n\toutput3 = i3 >> 1 & 0x7ffffffffffffLLU;\n\toutput4 = i4 >> 12 & 0x7ffffffffffffLLU;\n\toutput[0] = output0;\n\toutput[1] = output1;\n\toutput[2] = output2;\n\toutput[3] = output3;\n\toutput[4] = output4;\n}\n\nstatic __always_inline void format_fcontract_first_carry_pass(u64 *input)\n{\n\tu64 t0 = input[0];\n\tu64 t1 = input[1];\n\tu64 t2 = input[2];\n\tu64 t3 = input[3];\n\tu64 t4 = input[4];\n\tu64 t1_ = t1 + (t0 >> 51);\n\tu64 t0_ = t0 & 0x7ffffffffffffLLU;\n\tu64 t2_ = t2 + (t1_ >> 51);\n\tu64 t1__ = t1_ & 0x7ffffffffffffLLU;\n\tu64 t3_ = t3 + (t2_ >> 51);\n\tu64 t2__ = t2_ & 0x7ffffffffffffLLU;\n\tu64 t4_ = t4 + (t3_ >> 51);\n\tu64 t3__ = t3_ & 0x7ffffffffffffLLU;\n\tinput[0] = t0_;\n\tinput[1] = t1__;\n\tinput[2] = t2__;\n\tinput[3] = t3__;\n\tinput[4] = t4_;\n}\n\nstatic __always_inline void format_fcontract_first_carry_full(u64 *input)\n{\n\tformat_fcontract_first_carry_pass(input);\n\tmodulo_carry_top(input);\n}\n\nstatic __always_inline void format_fcontract_second_carry_pass(u64 *input)\n{\n\tu64 t0 = input[0];\n\tu64 t1 = input[1];\n\tu64 t2 = input[2];\n\tu64 t3 = input[3];\n\tu64 t4 = input[4];\n\tu64 t1_ = t1 + (t0 >> 51);\n\tu64 t0_ = t0 & 0x7ffffffffffffLLU;\n\tu64 t2_ = t2 + (t1_ >> 51);\n\tu64 t1__ = t1_ & 0x7ffffffffffffLLU;\n\tu64 t3_ = t3 + (t2_ >> 51);\n\tu64 t2__ = t2_ & 0x7ffffffffffffLLU;\n\tu64 t4_ = t4 + (t3_ >> 51);\n\tu64 t3__ = t3_ & 0x7ffffffffffffLLU;\n\tinput[0] = t0_;\n\tinput[1] = t1__;\n\tinput[2] = t2__;\n\tinput[3] = t3__;\n\tinput[4] = t4_;\n}\n\nstatic __always_inline void format_fcontract_second_carry_full(u64 *input)\n{\n\tu64 i0;\n\tu64 i1;\n\tu64 i0_;\n\tu64 i1_;\n\tformat_fcontract_second_carry_pass(input);\n\tmodulo_carry_top(input);\n\ti0 = input[0];\n\ti1 = input[1];\n\ti0_ = i0 & 0x7ffffffffffffLLU;\n\ti1_ = i1 + (i0 >> 51);\n\tinput[0] = i0_;\n\tinput[1] = i1_;\n}\n\nstatic __always_inline void format_fcontract_trim(u64 *input)\n{\n\tu64 a0 = input[0];\n\tu64 a1 = input[1];\n\tu64 a2 = input[2];\n\tu64 a3 = input[3];\n\tu64 a4 = input[4];\n\tu64 mask0 = u64_gte_mask(a0, 0x7ffffffffffedLLU);\n\tu64 mask1 = u64_eq_mask(a1, 0x7ffffffffffffLLU);\n\tu64 mask2 = u64_eq_mask(a2, 0x7ffffffffffffLLU);\n\tu64 mask3 = u64_eq_mask(a3, 0x7ffffffffffffLLU);\n\tu64 mask4 = u64_eq_mask(a4, 0x7ffffffffffffLLU);\n\tu64 mask = (((mask0 & mask1) & mask2) & mask3) & mask4;\n\tu64 a0_ = a0 - (0x7ffffffffffedLLU & mask);\n\tu64 a1_ = a1 - (0x7ffffffffffffLLU & mask);\n\tu64 a2_ = a2 - (0x7ffffffffffffLLU & mask);\n\tu64 a3_ = a3 - (0x7ffffffffffffLLU & mask);\n\tu64 a4_ = a4 - (0x7ffffffffffffLLU & mask);\n\tinput[0] = a0_;\n\tinput[1] = a1_;\n\tinput[2] = a2_;\n\tinput[3] = a3_;\n\tinput[4] = a4_;\n}\n\nstatic __always_inline void format_fcontract_store(u8 *output, u64 *input)\n{\n\tu64 t0 = input[0];\n\tu64 t1 = input[1];\n\tu64 t2 = input[2];\n\tu64 t3 = input[3];\n\tu64 t4 = input[4];\n\tu64 o0 = t1 << 51 | t0;\n\tu64 o1 = t2 << 38 | t1 >> 13;\n\tu64 o2 = t3 << 25 | t2 >> 26;\n\tu64 o3 = t4 << 12 | t3 >> 39;\n\tu8 *b0 = output;\n\tu8 *b1 = output + 8;\n\tu8 *b2 = output + 16;\n\tu8 *b3 = output + 24;\n\tput_unaligned_le64(o0, b0);\n\tput_unaligned_le64(o1, b1);\n\tput_unaligned_le64(o2, b2);\n\tput_unaligned_le64(o3, b3);\n}\n\nstatic __always_inline void format_fcontract(u8 *output, u64 *input)\n{\n\tformat_fcontract_first_carry_full(input);\n\tformat_fcontract_second_carry_full(input);\n\tformat_fcontract_trim(input);\n\tformat_fcontract_store(output, input);\n}\n\nstatic __always_inline void format_scalar_of_point(u8 *scalar, u64 *point)\n{\n\tu64 *x = point;\n\tu64 *z = point + 5;\n\tu64 buf[10] __aligned(32) = { 0 };\n\tu64 *zmone = buf;\n\tu64 *sc = buf + 5;\n\tcrecip(zmone, z);\n\tfmul(sc, x, zmone);\n\tformat_fcontract(scalar, sc);\n}\n\nvoid curve25519_generic(u8 mypublic[CURVE25519_KEY_SIZE],\n\t\t\tconst u8 secret[CURVE25519_KEY_SIZE],\n\t\t\tconst u8 basepoint[CURVE25519_KEY_SIZE])\n{\n\tu64 buf0[10] __aligned(32) = { 0 };\n\tu64 *x0 = buf0;\n\tu64 *z = buf0 + 5;\n\tu64 *q;\n\tformat_fexpand(x0, basepoint);\n\tz[0] = 1;\n\tq = buf0;\n\t{\n\t\tu8 e[32] __aligned(32) = { 0 };\n\t\tu8 *scalar;\n\t\tmemcpy(e, secret, 32);\n\t\tcurve25519_clamp_secret(e);\n\t\tscalar = e;\n\t\t{\n\t\t\tu64 buf[15] = { 0 };\n\t\t\tu64 *nq = buf;\n\t\t\tu64 *x = nq;\n\t\t\tx[0] = 1;\n\t\t\tladder_cmult(nq, scalar, q);\n\t\t\tformat_scalar_of_point(mypublic, nq);\n\t\t\tmemzero_explicit(buf, sizeof(buf));\n\t\t}\n\t\tmemzero_explicit(e, sizeof(e));\n\t}\n\tmemzero_explicit(buf0, sizeof(buf0));\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}