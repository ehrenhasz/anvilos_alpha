{
  "module_name": "xarray.c",
  "hash_id": "d2a1e4ae1d3d5ca3dfb4da96383b15dd5f432d9c421eac3c0f4facdf184d7c8e",
  "original_prompt": "Ingested from linux-6.6.14/lib/xarray.c",
  "human_readable_source": "\n \n\n#include <linux/bitmap.h>\n#include <linux/export.h>\n#include <linux/list.h>\n#include <linux/slab.h>\n#include <linux/xarray.h>\n\n#include \"radix-tree.h\"\n\n \n\nstatic inline unsigned int xa_lock_type(const struct xarray *xa)\n{\n\treturn (__force unsigned int)xa->xa_flags & 3;\n}\n\nstatic inline void xas_lock_type(struct xa_state *xas, unsigned int lock_type)\n{\n\tif (lock_type == XA_LOCK_IRQ)\n\t\txas_lock_irq(xas);\n\telse if (lock_type == XA_LOCK_BH)\n\t\txas_lock_bh(xas);\n\telse\n\t\txas_lock(xas);\n}\n\nstatic inline void xas_unlock_type(struct xa_state *xas, unsigned int lock_type)\n{\n\tif (lock_type == XA_LOCK_IRQ)\n\t\txas_unlock_irq(xas);\n\telse if (lock_type == XA_LOCK_BH)\n\t\txas_unlock_bh(xas);\n\telse\n\t\txas_unlock(xas);\n}\n\nstatic inline bool xa_track_free(const struct xarray *xa)\n{\n\treturn xa->xa_flags & XA_FLAGS_TRACK_FREE;\n}\n\nstatic inline bool xa_zero_busy(const struct xarray *xa)\n{\n\treturn xa->xa_flags & XA_FLAGS_ZERO_BUSY;\n}\n\nstatic inline void xa_mark_set(struct xarray *xa, xa_mark_t mark)\n{\n\tif (!(xa->xa_flags & XA_FLAGS_MARK(mark)))\n\t\txa->xa_flags |= XA_FLAGS_MARK(mark);\n}\n\nstatic inline void xa_mark_clear(struct xarray *xa, xa_mark_t mark)\n{\n\tif (xa->xa_flags & XA_FLAGS_MARK(mark))\n\t\txa->xa_flags &= ~(XA_FLAGS_MARK(mark));\n}\n\nstatic inline unsigned long *node_marks(struct xa_node *node, xa_mark_t mark)\n{\n\treturn node->marks[(__force unsigned)mark];\n}\n\nstatic inline bool node_get_mark(struct xa_node *node,\n\t\tunsigned int offset, xa_mark_t mark)\n{\n\treturn test_bit(offset, node_marks(node, mark));\n}\n\n \nstatic inline bool node_set_mark(struct xa_node *node, unsigned int offset,\n\t\t\t\txa_mark_t mark)\n{\n\treturn __test_and_set_bit(offset, node_marks(node, mark));\n}\n\n \nstatic inline bool node_clear_mark(struct xa_node *node, unsigned int offset,\n\t\t\t\txa_mark_t mark)\n{\n\treturn __test_and_clear_bit(offset, node_marks(node, mark));\n}\n\nstatic inline bool node_any_mark(struct xa_node *node, xa_mark_t mark)\n{\n\treturn !bitmap_empty(node_marks(node, mark), XA_CHUNK_SIZE);\n}\n\nstatic inline void node_mark_all(struct xa_node *node, xa_mark_t mark)\n{\n\tbitmap_fill(node_marks(node, mark), XA_CHUNK_SIZE);\n}\n\n#define mark_inc(mark) do { \\\n\tmark = (__force xa_mark_t)((__force unsigned)(mark) + 1); \\\n} while (0)\n\n \nstatic void xas_squash_marks(const struct xa_state *xas)\n{\n\tunsigned int mark = 0;\n\tunsigned int limit = xas->xa_offset + xas->xa_sibs + 1;\n\n\tif (!xas->xa_sibs)\n\t\treturn;\n\n\tdo {\n\t\tunsigned long *marks = xas->xa_node->marks[mark];\n\t\tif (find_next_bit(marks, limit, xas->xa_offset + 1) == limit)\n\t\t\tcontinue;\n\t\t__set_bit(xas->xa_offset, marks);\n\t\tbitmap_clear(marks, xas->xa_offset + 1, xas->xa_sibs);\n\t} while (mark++ != (__force unsigned)XA_MARK_MAX);\n}\n\n \nstatic unsigned int get_offset(unsigned long index, struct xa_node *node)\n{\n\treturn (index >> node->shift) & XA_CHUNK_MASK;\n}\n\nstatic void xas_set_offset(struct xa_state *xas)\n{\n\txas->xa_offset = get_offset(xas->xa_index, xas->xa_node);\n}\n\n \nstatic void xas_move_index(struct xa_state *xas, unsigned long offset)\n{\n\tunsigned int shift = xas->xa_node->shift;\n\txas->xa_index &= ~XA_CHUNK_MASK << shift;\n\txas->xa_index += offset << shift;\n}\n\nstatic void xas_next_offset(struct xa_state *xas)\n{\n\txas->xa_offset++;\n\txas_move_index(xas, xas->xa_offset);\n}\n\nstatic void *set_bounds(struct xa_state *xas)\n{\n\txas->xa_node = XAS_BOUNDS;\n\treturn NULL;\n}\n\n \nstatic void *xas_start(struct xa_state *xas)\n{\n\tvoid *entry;\n\n\tif (xas_valid(xas))\n\t\treturn xas_reload(xas);\n\tif (xas_error(xas))\n\t\treturn NULL;\n\n\tentry = xa_head(xas->xa);\n\tif (!xa_is_node(entry)) {\n\t\tif (xas->xa_index)\n\t\t\treturn set_bounds(xas);\n\t} else {\n\t\tif ((xas->xa_index >> xa_to_node(entry)->shift) > XA_CHUNK_MASK)\n\t\t\treturn set_bounds(xas);\n\t}\n\n\txas->xa_node = NULL;\n\treturn entry;\n}\n\nstatic void *xas_descend(struct xa_state *xas, struct xa_node *node)\n{\n\tunsigned int offset = get_offset(xas->xa_index, node);\n\tvoid *entry = xa_entry(xas->xa, node, offset);\n\n\txas->xa_node = node;\n\twhile (xa_is_sibling(entry)) {\n\t\toffset = xa_to_sibling(entry);\n\t\tentry = xa_entry(xas->xa, node, offset);\n\t\tif (node->shift && xa_is_node(entry))\n\t\t\tentry = XA_RETRY_ENTRY;\n\t}\n\n\txas->xa_offset = offset;\n\treturn entry;\n}\n\n \nvoid *xas_load(struct xa_state *xas)\n{\n\tvoid *entry = xas_start(xas);\n\n\twhile (xa_is_node(entry)) {\n\t\tstruct xa_node *node = xa_to_node(entry);\n\n\t\tif (xas->xa_shift > node->shift)\n\t\t\tbreak;\n\t\tentry = xas_descend(xas, node);\n\t\tif (node->shift == 0)\n\t\t\tbreak;\n\t}\n\treturn entry;\n}\nEXPORT_SYMBOL_GPL(xas_load);\n\n#define XA_RCU_FREE\t((struct xarray *)1)\n\nstatic void xa_node_free(struct xa_node *node)\n{\n\tXA_NODE_BUG_ON(node, !list_empty(&node->private_list));\n\tnode->array = XA_RCU_FREE;\n\tcall_rcu(&node->rcu_head, radix_tree_node_rcu_free);\n}\n\n \nvoid xas_destroy(struct xa_state *xas)\n{\n\tstruct xa_node *next, *node = xas->xa_alloc;\n\n\twhile (node) {\n\t\tXA_NODE_BUG_ON(node, !list_empty(&node->private_list));\n\t\tnext = rcu_dereference_raw(node->parent);\n\t\tradix_tree_node_rcu_free(&node->rcu_head);\n\t\txas->xa_alloc = node = next;\n\t}\n}\n\n \nbool xas_nomem(struct xa_state *xas, gfp_t gfp)\n{\n\tif (xas->xa_node != XA_ERROR(-ENOMEM)) {\n\t\txas_destroy(xas);\n\t\treturn false;\n\t}\n\tif (xas->xa->xa_flags & XA_FLAGS_ACCOUNT)\n\t\tgfp |= __GFP_ACCOUNT;\n\txas->xa_alloc = kmem_cache_alloc_lru(radix_tree_node_cachep, xas->xa_lru, gfp);\n\tif (!xas->xa_alloc)\n\t\treturn false;\n\txas->xa_alloc->parent = NULL;\n\tXA_NODE_BUG_ON(xas->xa_alloc, !list_empty(&xas->xa_alloc->private_list));\n\txas->xa_node = XAS_RESTART;\n\treturn true;\n}\nEXPORT_SYMBOL_GPL(xas_nomem);\n\n \nstatic bool __xas_nomem(struct xa_state *xas, gfp_t gfp)\n\t__must_hold(xas->xa->xa_lock)\n{\n\tunsigned int lock_type = xa_lock_type(xas->xa);\n\n\tif (xas->xa_node != XA_ERROR(-ENOMEM)) {\n\t\txas_destroy(xas);\n\t\treturn false;\n\t}\n\tif (xas->xa->xa_flags & XA_FLAGS_ACCOUNT)\n\t\tgfp |= __GFP_ACCOUNT;\n\tif (gfpflags_allow_blocking(gfp)) {\n\t\txas_unlock_type(xas, lock_type);\n\t\txas->xa_alloc = kmem_cache_alloc_lru(radix_tree_node_cachep, xas->xa_lru, gfp);\n\t\txas_lock_type(xas, lock_type);\n\t} else {\n\t\txas->xa_alloc = kmem_cache_alloc_lru(radix_tree_node_cachep, xas->xa_lru, gfp);\n\t}\n\tif (!xas->xa_alloc)\n\t\treturn false;\n\txas->xa_alloc->parent = NULL;\n\tXA_NODE_BUG_ON(xas->xa_alloc, !list_empty(&xas->xa_alloc->private_list));\n\txas->xa_node = XAS_RESTART;\n\treturn true;\n}\n\nstatic void xas_update(struct xa_state *xas, struct xa_node *node)\n{\n\tif (xas->xa_update)\n\t\txas->xa_update(node);\n\telse\n\t\tXA_NODE_BUG_ON(node, !list_empty(&node->private_list));\n}\n\nstatic void *xas_alloc(struct xa_state *xas, unsigned int shift)\n{\n\tstruct xa_node *parent = xas->xa_node;\n\tstruct xa_node *node = xas->xa_alloc;\n\n\tif (xas_invalid(xas))\n\t\treturn NULL;\n\n\tif (node) {\n\t\txas->xa_alloc = NULL;\n\t} else {\n\t\tgfp_t gfp = GFP_NOWAIT | __GFP_NOWARN;\n\n\t\tif (xas->xa->xa_flags & XA_FLAGS_ACCOUNT)\n\t\t\tgfp |= __GFP_ACCOUNT;\n\n\t\tnode = kmem_cache_alloc_lru(radix_tree_node_cachep, xas->xa_lru, gfp);\n\t\tif (!node) {\n\t\t\txas_set_err(xas, -ENOMEM);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\n\tif (parent) {\n\t\tnode->offset = xas->xa_offset;\n\t\tparent->count++;\n\t\tXA_NODE_BUG_ON(node, parent->count > XA_CHUNK_SIZE);\n\t\txas_update(xas, parent);\n\t}\n\tXA_NODE_BUG_ON(node, shift > BITS_PER_LONG);\n\tXA_NODE_BUG_ON(node, !list_empty(&node->private_list));\n\tnode->shift = shift;\n\tnode->count = 0;\n\tnode->nr_values = 0;\n\tRCU_INIT_POINTER(node->parent, xas->xa_node);\n\tnode->array = xas->xa;\n\n\treturn node;\n}\n\n#ifdef CONFIG_XARRAY_MULTI\n \nstatic unsigned long xas_size(const struct xa_state *xas)\n{\n\treturn (xas->xa_sibs + 1UL) << xas->xa_shift;\n}\n#endif\n\n \nstatic unsigned long xas_max(struct xa_state *xas)\n{\n\tunsigned long max = xas->xa_index;\n\n#ifdef CONFIG_XARRAY_MULTI\n\tif (xas->xa_shift || xas->xa_sibs) {\n\t\tunsigned long mask = xas_size(xas) - 1;\n\t\tmax |= mask;\n\t\tif (mask == max)\n\t\t\tmax++;\n\t}\n#endif\n\n\treturn max;\n}\n\n \nstatic unsigned long max_index(void *entry)\n{\n\tif (!xa_is_node(entry))\n\t\treturn 0;\n\treturn (XA_CHUNK_SIZE << xa_to_node(entry)->shift) - 1;\n}\n\nstatic void xas_shrink(struct xa_state *xas)\n{\n\tstruct xarray *xa = xas->xa;\n\tstruct xa_node *node = xas->xa_node;\n\n\tfor (;;) {\n\t\tvoid *entry;\n\n\t\tXA_NODE_BUG_ON(node, node->count > XA_CHUNK_SIZE);\n\t\tif (node->count != 1)\n\t\t\tbreak;\n\t\tentry = xa_entry_locked(xa, node, 0);\n\t\tif (!entry)\n\t\t\tbreak;\n\t\tif (!xa_is_node(entry) && node->shift)\n\t\t\tbreak;\n\t\tif (xa_is_zero(entry) && xa_zero_busy(xa))\n\t\t\tentry = NULL;\n\t\txas->xa_node = XAS_BOUNDS;\n\n\t\tRCU_INIT_POINTER(xa->xa_head, entry);\n\t\tif (xa_track_free(xa) && !node_get_mark(node, 0, XA_FREE_MARK))\n\t\t\txa_mark_clear(xa, XA_FREE_MARK);\n\n\t\tnode->count = 0;\n\t\tnode->nr_values = 0;\n\t\tif (!xa_is_node(entry))\n\t\t\tRCU_INIT_POINTER(node->slots[0], XA_RETRY_ENTRY);\n\t\txas_update(xas, node);\n\t\txa_node_free(node);\n\t\tif (!xa_is_node(entry))\n\t\t\tbreak;\n\t\tnode = xa_to_node(entry);\n\t\tnode->parent = NULL;\n\t}\n}\n\n \nstatic void xas_delete_node(struct xa_state *xas)\n{\n\tstruct xa_node *node = xas->xa_node;\n\n\tfor (;;) {\n\t\tstruct xa_node *parent;\n\n\t\tXA_NODE_BUG_ON(node, node->count > XA_CHUNK_SIZE);\n\t\tif (node->count)\n\t\t\tbreak;\n\n\t\tparent = xa_parent_locked(xas->xa, node);\n\t\txas->xa_node = parent;\n\t\txas->xa_offset = node->offset;\n\t\txa_node_free(node);\n\n\t\tif (!parent) {\n\t\t\txas->xa->xa_head = NULL;\n\t\t\txas->xa_node = XAS_BOUNDS;\n\t\t\treturn;\n\t\t}\n\n\t\tparent->slots[xas->xa_offset] = NULL;\n\t\tparent->count--;\n\t\tXA_NODE_BUG_ON(parent, parent->count > XA_CHUNK_SIZE);\n\t\tnode = parent;\n\t\txas_update(xas, node);\n\t}\n\n\tif (!node->parent)\n\t\txas_shrink(xas);\n}\n\n \nstatic void xas_free_nodes(struct xa_state *xas, struct xa_node *top)\n{\n\tunsigned int offset = 0;\n\tstruct xa_node *node = top;\n\n\tfor (;;) {\n\t\tvoid *entry = xa_entry_locked(xas->xa, node, offset);\n\n\t\tif (node->shift && xa_is_node(entry)) {\n\t\t\tnode = xa_to_node(entry);\n\t\t\toffset = 0;\n\t\t\tcontinue;\n\t\t}\n\t\tif (entry)\n\t\t\tRCU_INIT_POINTER(node->slots[offset], XA_RETRY_ENTRY);\n\t\toffset++;\n\t\twhile (offset == XA_CHUNK_SIZE) {\n\t\t\tstruct xa_node *parent;\n\n\t\t\tparent = xa_parent_locked(xas->xa, node);\n\t\t\toffset = node->offset + 1;\n\t\t\tnode->count = 0;\n\t\t\tnode->nr_values = 0;\n\t\t\txas_update(xas, node);\n\t\t\txa_node_free(node);\n\t\t\tif (node == top)\n\t\t\t\treturn;\n\t\t\tnode = parent;\n\t\t}\n\t}\n}\n\n \nstatic int xas_expand(struct xa_state *xas, void *head)\n{\n\tstruct xarray *xa = xas->xa;\n\tstruct xa_node *node = NULL;\n\tunsigned int shift = 0;\n\tunsigned long max = xas_max(xas);\n\n\tif (!head) {\n\t\tif (max == 0)\n\t\t\treturn 0;\n\t\twhile ((max >> shift) >= XA_CHUNK_SIZE)\n\t\t\tshift += XA_CHUNK_SHIFT;\n\t\treturn shift + XA_CHUNK_SHIFT;\n\t} else if (xa_is_node(head)) {\n\t\tnode = xa_to_node(head);\n\t\tshift = node->shift + XA_CHUNK_SHIFT;\n\t}\n\txas->xa_node = NULL;\n\n\twhile (max > max_index(head)) {\n\t\txa_mark_t mark = 0;\n\n\t\tXA_NODE_BUG_ON(node, shift > BITS_PER_LONG);\n\t\tnode = xas_alloc(xas, shift);\n\t\tif (!node)\n\t\t\treturn -ENOMEM;\n\n\t\tnode->count = 1;\n\t\tif (xa_is_value(head))\n\t\t\tnode->nr_values = 1;\n\t\tRCU_INIT_POINTER(node->slots[0], head);\n\n\t\t \n\t\tfor (;;) {\n\t\t\tif (xa_track_free(xa) && mark == XA_FREE_MARK) {\n\t\t\t\tnode_mark_all(node, XA_FREE_MARK);\n\t\t\t\tif (!xa_marked(xa, XA_FREE_MARK)) {\n\t\t\t\t\tnode_clear_mark(node, 0, XA_FREE_MARK);\n\t\t\t\t\txa_mark_set(xa, XA_FREE_MARK);\n\t\t\t\t}\n\t\t\t} else if (xa_marked(xa, mark)) {\n\t\t\t\tnode_set_mark(node, 0, mark);\n\t\t\t}\n\t\t\tif (mark == XA_MARK_MAX)\n\t\t\t\tbreak;\n\t\t\tmark_inc(mark);\n\t\t}\n\n\t\t \n\t\tif (xa_is_node(head)) {\n\t\t\txa_to_node(head)->offset = 0;\n\t\t\trcu_assign_pointer(xa_to_node(head)->parent, node);\n\t\t}\n\t\thead = xa_mk_node(node);\n\t\trcu_assign_pointer(xa->xa_head, head);\n\t\txas_update(xas, node);\n\n\t\tshift += XA_CHUNK_SHIFT;\n\t}\n\n\txas->xa_node = node;\n\treturn shift;\n}\n\n \nstatic void *xas_create(struct xa_state *xas, bool allow_root)\n{\n\tstruct xarray *xa = xas->xa;\n\tvoid *entry;\n\tvoid __rcu **slot;\n\tstruct xa_node *node = xas->xa_node;\n\tint shift;\n\tunsigned int order = xas->xa_shift;\n\n\tif (xas_top(node)) {\n\t\tentry = xa_head_locked(xa);\n\t\txas->xa_node = NULL;\n\t\tif (!entry && xa_zero_busy(xa))\n\t\t\tentry = XA_ZERO_ENTRY;\n\t\tshift = xas_expand(xas, entry);\n\t\tif (shift < 0)\n\t\t\treturn NULL;\n\t\tif (!shift && !allow_root)\n\t\t\tshift = XA_CHUNK_SHIFT;\n\t\tentry = xa_head_locked(xa);\n\t\tslot = &xa->xa_head;\n\t} else if (xas_error(xas)) {\n\t\treturn NULL;\n\t} else if (node) {\n\t\tunsigned int offset = xas->xa_offset;\n\n\t\tshift = node->shift;\n\t\tentry = xa_entry_locked(xa, node, offset);\n\t\tslot = &node->slots[offset];\n\t} else {\n\t\tshift = 0;\n\t\tentry = xa_head_locked(xa);\n\t\tslot = &xa->xa_head;\n\t}\n\n\twhile (shift > order) {\n\t\tshift -= XA_CHUNK_SHIFT;\n\t\tif (!entry) {\n\t\t\tnode = xas_alloc(xas, shift);\n\t\t\tif (!node)\n\t\t\t\tbreak;\n\t\t\tif (xa_track_free(xa))\n\t\t\t\tnode_mark_all(node, XA_FREE_MARK);\n\t\t\trcu_assign_pointer(*slot, xa_mk_node(node));\n\t\t} else if (xa_is_node(entry)) {\n\t\t\tnode = xa_to_node(entry);\n\t\t} else {\n\t\t\tbreak;\n\t\t}\n\t\tentry = xas_descend(xas, node);\n\t\tslot = &node->slots[xas->xa_offset];\n\t}\n\n\treturn entry;\n}\n\n \nvoid xas_create_range(struct xa_state *xas)\n{\n\tunsigned long index = xas->xa_index;\n\tunsigned char shift = xas->xa_shift;\n\tunsigned char sibs = xas->xa_sibs;\n\n\txas->xa_index |= ((sibs + 1UL) << shift) - 1;\n\tif (xas_is_node(xas) && xas->xa_node->shift == xas->xa_shift)\n\t\txas->xa_offset |= sibs;\n\txas->xa_shift = 0;\n\txas->xa_sibs = 0;\n\n\tfor (;;) {\n\t\txas_create(xas, true);\n\t\tif (xas_error(xas))\n\t\t\tgoto restore;\n\t\tif (xas->xa_index <= (index | XA_CHUNK_MASK))\n\t\t\tgoto success;\n\t\txas->xa_index -= XA_CHUNK_SIZE;\n\n\t\tfor (;;) {\n\t\t\tstruct xa_node *node = xas->xa_node;\n\t\t\tif (node->shift >= shift)\n\t\t\t\tbreak;\n\t\t\txas->xa_node = xa_parent_locked(xas->xa, node);\n\t\t\txas->xa_offset = node->offset - 1;\n\t\t\tif (node->offset != 0)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\nrestore:\n\txas->xa_shift = shift;\n\txas->xa_sibs = sibs;\n\txas->xa_index = index;\n\treturn;\nsuccess:\n\txas->xa_index = index;\n\tif (xas->xa_node)\n\t\txas_set_offset(xas);\n}\nEXPORT_SYMBOL_GPL(xas_create_range);\n\nstatic void update_node(struct xa_state *xas, struct xa_node *node,\n\t\tint count, int values)\n{\n\tif (!node || (!count && !values))\n\t\treturn;\n\n\tnode->count += count;\n\tnode->nr_values += values;\n\tXA_NODE_BUG_ON(node, node->count > XA_CHUNK_SIZE);\n\tXA_NODE_BUG_ON(node, node->nr_values > XA_CHUNK_SIZE);\n\txas_update(xas, node);\n\tif (count < 0)\n\t\txas_delete_node(xas);\n}\n\n \nvoid *xas_store(struct xa_state *xas, void *entry)\n{\n\tstruct xa_node *node;\n\tvoid __rcu **slot = &xas->xa->xa_head;\n\tunsigned int offset, max;\n\tint count = 0;\n\tint values = 0;\n\tvoid *first, *next;\n\tbool value = xa_is_value(entry);\n\n\tif (entry) {\n\t\tbool allow_root = !xa_is_node(entry) && !xa_is_zero(entry);\n\t\tfirst = xas_create(xas, allow_root);\n\t} else {\n\t\tfirst = xas_load(xas);\n\t}\n\n\tif (xas_invalid(xas))\n\t\treturn first;\n\tnode = xas->xa_node;\n\tif (node && (xas->xa_shift < node->shift))\n\t\txas->xa_sibs = 0;\n\tif ((first == entry) && !xas->xa_sibs)\n\t\treturn first;\n\n\tnext = first;\n\toffset = xas->xa_offset;\n\tmax = xas->xa_offset + xas->xa_sibs;\n\tif (node) {\n\t\tslot = &node->slots[offset];\n\t\tif (xas->xa_sibs)\n\t\t\txas_squash_marks(xas);\n\t}\n\tif (!entry)\n\t\txas_init_marks(xas);\n\n\tfor (;;) {\n\t\t \n\t\trcu_assign_pointer(*slot, entry);\n\t\tif (xa_is_node(next) && (!node || node->shift))\n\t\t\txas_free_nodes(xas, xa_to_node(next));\n\t\tif (!node)\n\t\t\tbreak;\n\t\tcount += !next - !entry;\n\t\tvalues += !xa_is_value(first) - !value;\n\t\tif (entry) {\n\t\t\tif (offset == max)\n\t\t\t\tbreak;\n\t\t\tif (!xa_is_sibling(entry))\n\t\t\t\tentry = xa_mk_sibling(xas->xa_offset);\n\t\t} else {\n\t\t\tif (offset == XA_CHUNK_MASK)\n\t\t\t\tbreak;\n\t\t}\n\t\tnext = xa_entry_locked(xas->xa, node, ++offset);\n\t\tif (!xa_is_sibling(next)) {\n\t\t\tif (!entry && (offset > max))\n\t\t\t\tbreak;\n\t\t\tfirst = next;\n\t\t}\n\t\tslot++;\n\t}\n\n\tupdate_node(xas, node, count, values);\n\treturn first;\n}\nEXPORT_SYMBOL_GPL(xas_store);\n\n \nbool xas_get_mark(const struct xa_state *xas, xa_mark_t mark)\n{\n\tif (xas_invalid(xas))\n\t\treturn false;\n\tif (!xas->xa_node)\n\t\treturn xa_marked(xas->xa, mark);\n\treturn node_get_mark(xas->xa_node, xas->xa_offset, mark);\n}\nEXPORT_SYMBOL_GPL(xas_get_mark);\n\n \nvoid xas_set_mark(const struct xa_state *xas, xa_mark_t mark)\n{\n\tstruct xa_node *node = xas->xa_node;\n\tunsigned int offset = xas->xa_offset;\n\n\tif (xas_invalid(xas))\n\t\treturn;\n\n\twhile (node) {\n\t\tif (node_set_mark(node, offset, mark))\n\t\t\treturn;\n\t\toffset = node->offset;\n\t\tnode = xa_parent_locked(xas->xa, node);\n\t}\n\n\tif (!xa_marked(xas->xa, mark))\n\t\txa_mark_set(xas->xa, mark);\n}\nEXPORT_SYMBOL_GPL(xas_set_mark);\n\n \nvoid xas_clear_mark(const struct xa_state *xas, xa_mark_t mark)\n{\n\tstruct xa_node *node = xas->xa_node;\n\tunsigned int offset = xas->xa_offset;\n\n\tif (xas_invalid(xas))\n\t\treturn;\n\n\twhile (node) {\n\t\tif (!node_clear_mark(node, offset, mark))\n\t\t\treturn;\n\t\tif (node_any_mark(node, mark))\n\t\t\treturn;\n\n\t\toffset = node->offset;\n\t\tnode = xa_parent_locked(xas->xa, node);\n\t}\n\n\tif (xa_marked(xas->xa, mark))\n\t\txa_mark_clear(xas->xa, mark);\n}\nEXPORT_SYMBOL_GPL(xas_clear_mark);\n\n \nvoid xas_init_marks(const struct xa_state *xas)\n{\n\txa_mark_t mark = 0;\n\n\tfor (;;) {\n\t\tif (xa_track_free(xas->xa) && mark == XA_FREE_MARK)\n\t\t\txas_set_mark(xas, mark);\n\t\telse\n\t\t\txas_clear_mark(xas, mark);\n\t\tif (mark == XA_MARK_MAX)\n\t\t\tbreak;\n\t\tmark_inc(mark);\n\t}\n}\nEXPORT_SYMBOL_GPL(xas_init_marks);\n\n#ifdef CONFIG_XARRAY_MULTI\nstatic unsigned int node_get_marks(struct xa_node *node, unsigned int offset)\n{\n\tunsigned int marks = 0;\n\txa_mark_t mark = XA_MARK_0;\n\n\tfor (;;) {\n\t\tif (node_get_mark(node, offset, mark))\n\t\t\tmarks |= 1 << (__force unsigned int)mark;\n\t\tif (mark == XA_MARK_MAX)\n\t\t\tbreak;\n\t\tmark_inc(mark);\n\t}\n\n\treturn marks;\n}\n\nstatic void node_set_marks(struct xa_node *node, unsigned int offset,\n\t\t\tstruct xa_node *child, unsigned int marks)\n{\n\txa_mark_t mark = XA_MARK_0;\n\n\tfor (;;) {\n\t\tif (marks & (1 << (__force unsigned int)mark)) {\n\t\t\tnode_set_mark(node, offset, mark);\n\t\t\tif (child)\n\t\t\t\tnode_mark_all(child, mark);\n\t\t}\n\t\tif (mark == XA_MARK_MAX)\n\t\t\tbreak;\n\t\tmark_inc(mark);\n\t}\n}\n\n \nvoid xas_split_alloc(struct xa_state *xas, void *entry, unsigned int order,\n\t\tgfp_t gfp)\n{\n\tunsigned int sibs = (1 << (order % XA_CHUNK_SHIFT)) - 1;\n\tunsigned int mask = xas->xa_sibs;\n\n\t \n\tif (WARN_ON(xas->xa_shift + 2 * XA_CHUNK_SHIFT < order))\n\t\tgoto nomem;\n\tif (xas->xa_shift + XA_CHUNK_SHIFT > order)\n\t\treturn;\n\n\tdo {\n\t\tunsigned int i;\n\t\tvoid *sibling = NULL;\n\t\tstruct xa_node *node;\n\n\t\tnode = kmem_cache_alloc_lru(radix_tree_node_cachep, xas->xa_lru, gfp);\n\t\tif (!node)\n\t\t\tgoto nomem;\n\t\tnode->array = xas->xa;\n\t\tfor (i = 0; i < XA_CHUNK_SIZE; i++) {\n\t\t\tif ((i & mask) == 0) {\n\t\t\t\tRCU_INIT_POINTER(node->slots[i], entry);\n\t\t\t\tsibling = xa_mk_sibling(i);\n\t\t\t} else {\n\t\t\t\tRCU_INIT_POINTER(node->slots[i], sibling);\n\t\t\t}\n\t\t}\n\t\tRCU_INIT_POINTER(node->parent, xas->xa_alloc);\n\t\txas->xa_alloc = node;\n\t} while (sibs-- > 0);\n\n\treturn;\nnomem:\n\txas_destroy(xas);\n\txas_set_err(xas, -ENOMEM);\n}\nEXPORT_SYMBOL_GPL(xas_split_alloc);\n\n \nvoid xas_split(struct xa_state *xas, void *entry, unsigned int order)\n{\n\tunsigned int sibs = (1 << (order % XA_CHUNK_SHIFT)) - 1;\n\tunsigned int offset, marks;\n\tstruct xa_node *node;\n\tvoid *curr = xas_load(xas);\n\tint values = 0;\n\n\tnode = xas->xa_node;\n\tif (xas_top(node))\n\t\treturn;\n\n\tmarks = node_get_marks(node, xas->xa_offset);\n\n\toffset = xas->xa_offset + sibs;\n\tdo {\n\t\tif (xas->xa_shift < node->shift) {\n\t\t\tstruct xa_node *child = xas->xa_alloc;\n\n\t\t\txas->xa_alloc = rcu_dereference_raw(child->parent);\n\t\t\tchild->shift = node->shift - XA_CHUNK_SHIFT;\n\t\t\tchild->offset = offset;\n\t\t\tchild->count = XA_CHUNK_SIZE;\n\t\t\tchild->nr_values = xa_is_value(entry) ?\n\t\t\t\t\tXA_CHUNK_SIZE : 0;\n\t\t\tRCU_INIT_POINTER(child->parent, node);\n\t\t\tnode_set_marks(node, offset, child, marks);\n\t\t\trcu_assign_pointer(node->slots[offset],\n\t\t\t\t\txa_mk_node(child));\n\t\t\tif (xa_is_value(curr))\n\t\t\t\tvalues--;\n\t\t\txas_update(xas, child);\n\t\t} else {\n\t\t\tunsigned int canon = offset - xas->xa_sibs;\n\n\t\t\tnode_set_marks(node, canon, NULL, marks);\n\t\t\trcu_assign_pointer(node->slots[canon], entry);\n\t\t\twhile (offset > canon)\n\t\t\t\trcu_assign_pointer(node->slots[offset--],\n\t\t\t\t\t\txa_mk_sibling(canon));\n\t\t\tvalues += (xa_is_value(entry) - xa_is_value(curr)) *\n\t\t\t\t\t(xas->xa_sibs + 1);\n\t\t}\n\t} while (offset-- > xas->xa_offset);\n\n\tnode->nr_values += values;\n\txas_update(xas, node);\n}\nEXPORT_SYMBOL_GPL(xas_split);\n#endif\n\n \nvoid xas_pause(struct xa_state *xas)\n{\n\tstruct xa_node *node = xas->xa_node;\n\n\tif (xas_invalid(xas))\n\t\treturn;\n\n\txas->xa_node = XAS_RESTART;\n\tif (node) {\n\t\tunsigned long offset = xas->xa_offset;\n\t\twhile (++offset < XA_CHUNK_SIZE) {\n\t\t\tif (!xa_is_sibling(xa_entry(xas->xa, node, offset)))\n\t\t\t\tbreak;\n\t\t}\n\t\txas->xa_index += (offset - xas->xa_offset) << node->shift;\n\t\tif (xas->xa_index == 0)\n\t\t\txas->xa_node = XAS_BOUNDS;\n\t} else {\n\t\txas->xa_index++;\n\t}\n}\nEXPORT_SYMBOL_GPL(xas_pause);\n\n \nvoid *__xas_prev(struct xa_state *xas)\n{\n\tvoid *entry;\n\n\tif (!xas_frozen(xas->xa_node))\n\t\txas->xa_index--;\n\tif (!xas->xa_node)\n\t\treturn set_bounds(xas);\n\tif (xas_not_node(xas->xa_node))\n\t\treturn xas_load(xas);\n\n\tif (xas->xa_offset != get_offset(xas->xa_index, xas->xa_node))\n\t\txas->xa_offset--;\n\n\twhile (xas->xa_offset == 255) {\n\t\txas->xa_offset = xas->xa_node->offset - 1;\n\t\txas->xa_node = xa_parent(xas->xa, xas->xa_node);\n\t\tif (!xas->xa_node)\n\t\t\treturn set_bounds(xas);\n\t}\n\n\tfor (;;) {\n\t\tentry = xa_entry(xas->xa, xas->xa_node, xas->xa_offset);\n\t\tif (!xa_is_node(entry))\n\t\t\treturn entry;\n\n\t\txas->xa_node = xa_to_node(entry);\n\t\txas_set_offset(xas);\n\t}\n}\nEXPORT_SYMBOL_GPL(__xas_prev);\n\n \nvoid *__xas_next(struct xa_state *xas)\n{\n\tvoid *entry;\n\n\tif (!xas_frozen(xas->xa_node))\n\t\txas->xa_index++;\n\tif (!xas->xa_node)\n\t\treturn set_bounds(xas);\n\tif (xas_not_node(xas->xa_node))\n\t\treturn xas_load(xas);\n\n\tif (xas->xa_offset != get_offset(xas->xa_index, xas->xa_node))\n\t\txas->xa_offset++;\n\n\twhile (xas->xa_offset == XA_CHUNK_SIZE) {\n\t\txas->xa_offset = xas->xa_node->offset + 1;\n\t\txas->xa_node = xa_parent(xas->xa, xas->xa_node);\n\t\tif (!xas->xa_node)\n\t\t\treturn set_bounds(xas);\n\t}\n\n\tfor (;;) {\n\t\tentry = xa_entry(xas->xa, xas->xa_node, xas->xa_offset);\n\t\tif (!xa_is_node(entry))\n\t\t\treturn entry;\n\n\t\txas->xa_node = xa_to_node(entry);\n\t\txas_set_offset(xas);\n\t}\n}\nEXPORT_SYMBOL_GPL(__xas_next);\n\n \nvoid *xas_find(struct xa_state *xas, unsigned long max)\n{\n\tvoid *entry;\n\n\tif (xas_error(xas) || xas->xa_node == XAS_BOUNDS)\n\t\treturn NULL;\n\tif (xas->xa_index > max)\n\t\treturn set_bounds(xas);\n\n\tif (!xas->xa_node) {\n\t\txas->xa_index = 1;\n\t\treturn set_bounds(xas);\n\t} else if (xas->xa_node == XAS_RESTART) {\n\t\tentry = xas_load(xas);\n\t\tif (entry || xas_not_node(xas->xa_node))\n\t\t\treturn entry;\n\t} else if (!xas->xa_node->shift &&\n\t\t    xas->xa_offset != (xas->xa_index & XA_CHUNK_MASK)) {\n\t\txas->xa_offset = ((xas->xa_index - 1) & XA_CHUNK_MASK) + 1;\n\t}\n\n\txas_next_offset(xas);\n\n\twhile (xas->xa_node && (xas->xa_index <= max)) {\n\t\tif (unlikely(xas->xa_offset == XA_CHUNK_SIZE)) {\n\t\t\txas->xa_offset = xas->xa_node->offset + 1;\n\t\t\txas->xa_node = xa_parent(xas->xa, xas->xa_node);\n\t\t\tcontinue;\n\t\t}\n\n\t\tentry = xa_entry(xas->xa, xas->xa_node, xas->xa_offset);\n\t\tif (xa_is_node(entry)) {\n\t\t\txas->xa_node = xa_to_node(entry);\n\t\t\txas->xa_offset = 0;\n\t\t\tcontinue;\n\t\t}\n\t\tif (entry && !xa_is_sibling(entry))\n\t\t\treturn entry;\n\n\t\txas_next_offset(xas);\n\t}\n\n\tif (!xas->xa_node)\n\t\txas->xa_node = XAS_BOUNDS;\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(xas_find);\n\n \nvoid *xas_find_marked(struct xa_state *xas, unsigned long max, xa_mark_t mark)\n{\n\tbool advance = true;\n\tunsigned int offset;\n\tvoid *entry;\n\n\tif (xas_error(xas))\n\t\treturn NULL;\n\tif (xas->xa_index > max)\n\t\tgoto max;\n\n\tif (!xas->xa_node) {\n\t\txas->xa_index = 1;\n\t\tgoto out;\n\t} else if (xas_top(xas->xa_node)) {\n\t\tadvance = false;\n\t\tentry = xa_head(xas->xa);\n\t\txas->xa_node = NULL;\n\t\tif (xas->xa_index > max_index(entry))\n\t\t\tgoto out;\n\t\tif (!xa_is_node(entry)) {\n\t\t\tif (xa_marked(xas->xa, mark))\n\t\t\t\treturn entry;\n\t\t\txas->xa_index = 1;\n\t\t\tgoto out;\n\t\t}\n\t\txas->xa_node = xa_to_node(entry);\n\t\txas->xa_offset = xas->xa_index >> xas->xa_node->shift;\n\t}\n\n\twhile (xas->xa_index <= max) {\n\t\tif (unlikely(xas->xa_offset == XA_CHUNK_SIZE)) {\n\t\t\txas->xa_offset = xas->xa_node->offset + 1;\n\t\t\txas->xa_node = xa_parent(xas->xa, xas->xa_node);\n\t\t\tif (!xas->xa_node)\n\t\t\t\tbreak;\n\t\t\tadvance = false;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!advance) {\n\t\t\tentry = xa_entry(xas->xa, xas->xa_node, xas->xa_offset);\n\t\t\tif (xa_is_sibling(entry)) {\n\t\t\t\txas->xa_offset = xa_to_sibling(entry);\n\t\t\t\txas_move_index(xas, xas->xa_offset);\n\t\t\t}\n\t\t}\n\n\t\toffset = xas_find_chunk(xas, advance, mark);\n\t\tif (offset > xas->xa_offset) {\n\t\t\tadvance = false;\n\t\t\txas_move_index(xas, offset);\n\t\t\t \n\t\t\tif ((xas->xa_index - 1) >= max)\n\t\t\t\tgoto max;\n\t\t\txas->xa_offset = offset;\n\t\t\tif (offset == XA_CHUNK_SIZE)\n\t\t\t\tcontinue;\n\t\t}\n\n\t\tentry = xa_entry(xas->xa, xas->xa_node, xas->xa_offset);\n\t\tif (!entry && !(xa_track_free(xas->xa) && mark == XA_FREE_MARK))\n\t\t\tcontinue;\n\t\tif (!xa_is_node(entry))\n\t\t\treturn entry;\n\t\txas->xa_node = xa_to_node(entry);\n\t\txas_set_offset(xas);\n\t}\n\nout:\n\tif (xas->xa_index > max)\n\t\tgoto max;\n\treturn set_bounds(xas);\nmax:\n\txas->xa_node = XAS_RESTART;\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(xas_find_marked);\n\n \nvoid *xas_find_conflict(struct xa_state *xas)\n{\n\tvoid *curr;\n\n\tif (xas_error(xas))\n\t\treturn NULL;\n\n\tif (!xas->xa_node)\n\t\treturn NULL;\n\n\tif (xas_top(xas->xa_node)) {\n\t\tcurr = xas_start(xas);\n\t\tif (!curr)\n\t\t\treturn NULL;\n\t\twhile (xa_is_node(curr)) {\n\t\t\tstruct xa_node *node = xa_to_node(curr);\n\t\t\tcurr = xas_descend(xas, node);\n\t\t}\n\t\tif (curr)\n\t\t\treturn curr;\n\t}\n\n\tif (xas->xa_node->shift > xas->xa_shift)\n\t\treturn NULL;\n\n\tfor (;;) {\n\t\tif (xas->xa_node->shift == xas->xa_shift) {\n\t\t\tif ((xas->xa_offset & xas->xa_sibs) == xas->xa_sibs)\n\t\t\t\tbreak;\n\t\t} else if (xas->xa_offset == XA_CHUNK_MASK) {\n\t\t\txas->xa_offset = xas->xa_node->offset;\n\t\t\txas->xa_node = xa_parent_locked(xas->xa, xas->xa_node);\n\t\t\tif (!xas->xa_node)\n\t\t\t\tbreak;\n\t\t\tcontinue;\n\t\t}\n\t\tcurr = xa_entry_locked(xas->xa, xas->xa_node, ++xas->xa_offset);\n\t\tif (xa_is_sibling(curr))\n\t\t\tcontinue;\n\t\twhile (xa_is_node(curr)) {\n\t\t\txas->xa_node = xa_to_node(curr);\n\t\t\txas->xa_offset = 0;\n\t\t\tcurr = xa_entry_locked(xas->xa, xas->xa_node, 0);\n\t\t}\n\t\tif (curr)\n\t\t\treturn curr;\n\t}\n\txas->xa_offset -= xas->xa_sibs;\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(xas_find_conflict);\n\n \nvoid *xa_load(struct xarray *xa, unsigned long index)\n{\n\tXA_STATE(xas, xa, index);\n\tvoid *entry;\n\n\trcu_read_lock();\n\tdo {\n\t\tentry = xas_load(&xas);\n\t\tif (xa_is_zero(entry))\n\t\t\tentry = NULL;\n\t} while (xas_retry(&xas, entry));\n\trcu_read_unlock();\n\n\treturn entry;\n}\nEXPORT_SYMBOL(xa_load);\n\nstatic void *xas_result(struct xa_state *xas, void *curr)\n{\n\tif (xa_is_zero(curr))\n\t\treturn NULL;\n\tif (xas_error(xas))\n\t\tcurr = xas->xa_node;\n\treturn curr;\n}\n\n \nvoid *__xa_erase(struct xarray *xa, unsigned long index)\n{\n\tXA_STATE(xas, xa, index);\n\treturn xas_result(&xas, xas_store(&xas, NULL));\n}\nEXPORT_SYMBOL(__xa_erase);\n\n \nvoid *xa_erase(struct xarray *xa, unsigned long index)\n{\n\tvoid *entry;\n\n\txa_lock(xa);\n\tentry = __xa_erase(xa, index);\n\txa_unlock(xa);\n\n\treturn entry;\n}\nEXPORT_SYMBOL(xa_erase);\n\n \nvoid *__xa_store(struct xarray *xa, unsigned long index, void *entry, gfp_t gfp)\n{\n\tXA_STATE(xas, xa, index);\n\tvoid *curr;\n\n\tif (WARN_ON_ONCE(xa_is_advanced(entry)))\n\t\treturn XA_ERROR(-EINVAL);\n\tif (xa_track_free(xa) && !entry)\n\t\tentry = XA_ZERO_ENTRY;\n\n\tdo {\n\t\tcurr = xas_store(&xas, entry);\n\t\tif (xa_track_free(xa))\n\t\t\txas_clear_mark(&xas, XA_FREE_MARK);\n\t} while (__xas_nomem(&xas, gfp));\n\n\treturn xas_result(&xas, curr);\n}\nEXPORT_SYMBOL(__xa_store);\n\n \nvoid *xa_store(struct xarray *xa, unsigned long index, void *entry, gfp_t gfp)\n{\n\tvoid *curr;\n\n\txa_lock(xa);\n\tcurr = __xa_store(xa, index, entry, gfp);\n\txa_unlock(xa);\n\n\treturn curr;\n}\nEXPORT_SYMBOL(xa_store);\n\n \nvoid *__xa_cmpxchg(struct xarray *xa, unsigned long index,\n\t\t\tvoid *old, void *entry, gfp_t gfp)\n{\n\tXA_STATE(xas, xa, index);\n\tvoid *curr;\n\n\tif (WARN_ON_ONCE(xa_is_advanced(entry)))\n\t\treturn XA_ERROR(-EINVAL);\n\n\tdo {\n\t\tcurr = xas_load(&xas);\n\t\tif (curr == old) {\n\t\t\txas_store(&xas, entry);\n\t\t\tif (xa_track_free(xa) && entry && !curr)\n\t\t\t\txas_clear_mark(&xas, XA_FREE_MARK);\n\t\t}\n\t} while (__xas_nomem(&xas, gfp));\n\n\treturn xas_result(&xas, curr);\n}\nEXPORT_SYMBOL(__xa_cmpxchg);\n\n \nint __xa_insert(struct xarray *xa, unsigned long index, void *entry, gfp_t gfp)\n{\n\tXA_STATE(xas, xa, index);\n\tvoid *curr;\n\n\tif (WARN_ON_ONCE(xa_is_advanced(entry)))\n\t\treturn -EINVAL;\n\tif (!entry)\n\t\tentry = XA_ZERO_ENTRY;\n\n\tdo {\n\t\tcurr = xas_load(&xas);\n\t\tif (!curr) {\n\t\t\txas_store(&xas, entry);\n\t\t\tif (xa_track_free(xa))\n\t\t\t\txas_clear_mark(&xas, XA_FREE_MARK);\n\t\t} else {\n\t\t\txas_set_err(&xas, -EBUSY);\n\t\t}\n\t} while (__xas_nomem(&xas, gfp));\n\n\treturn xas_error(&xas);\n}\nEXPORT_SYMBOL(__xa_insert);\n\n#ifdef CONFIG_XARRAY_MULTI\nstatic void xas_set_range(struct xa_state *xas, unsigned long first,\n\t\tunsigned long last)\n{\n\tunsigned int shift = 0;\n\tunsigned long sibs = last - first;\n\tunsigned int offset = XA_CHUNK_MASK;\n\n\txas_set(xas, first);\n\n\twhile ((first & XA_CHUNK_MASK) == 0) {\n\t\tif (sibs < XA_CHUNK_MASK)\n\t\t\tbreak;\n\t\tif ((sibs == XA_CHUNK_MASK) && (offset < XA_CHUNK_MASK))\n\t\t\tbreak;\n\t\tshift += XA_CHUNK_SHIFT;\n\t\tif (offset == XA_CHUNK_MASK)\n\t\t\toffset = sibs & XA_CHUNK_MASK;\n\t\tsibs >>= XA_CHUNK_SHIFT;\n\t\tfirst >>= XA_CHUNK_SHIFT;\n\t}\n\n\toffset = first & XA_CHUNK_MASK;\n\tif (offset + sibs > XA_CHUNK_MASK)\n\t\tsibs = XA_CHUNK_MASK - offset;\n\tif ((((first + sibs + 1) << shift) - 1) > last)\n\t\tsibs -= 1;\n\n\txas->xa_shift = shift;\n\txas->xa_sibs = sibs;\n}\n\n \nvoid *xa_store_range(struct xarray *xa, unsigned long first,\n\t\tunsigned long last, void *entry, gfp_t gfp)\n{\n\tXA_STATE(xas, xa, 0);\n\n\tif (WARN_ON_ONCE(xa_is_internal(entry)))\n\t\treturn XA_ERROR(-EINVAL);\n\tif (last < first)\n\t\treturn XA_ERROR(-EINVAL);\n\n\tdo {\n\t\txas_lock(&xas);\n\t\tif (entry) {\n\t\t\tunsigned int order = BITS_PER_LONG;\n\t\t\tif (last + 1)\n\t\t\t\torder = __ffs(last + 1);\n\t\t\txas_set_order(&xas, last, order);\n\t\t\txas_create(&xas, true);\n\t\t\tif (xas_error(&xas))\n\t\t\t\tgoto unlock;\n\t\t}\n\t\tdo {\n\t\t\txas_set_range(&xas, first, last);\n\t\t\txas_store(&xas, entry);\n\t\t\tif (xas_error(&xas))\n\t\t\t\tgoto unlock;\n\t\t\tfirst += xas_size(&xas);\n\t\t} while (first <= last);\nunlock:\n\t\txas_unlock(&xas);\n\t} while (xas_nomem(&xas, gfp));\n\n\treturn xas_result(&xas, NULL);\n}\nEXPORT_SYMBOL(xa_store_range);\n\n \nint xa_get_order(struct xarray *xa, unsigned long index)\n{\n\tXA_STATE(xas, xa, index);\n\tvoid *entry;\n\tint order = 0;\n\n\trcu_read_lock();\n\tentry = xas_load(&xas);\n\n\tif (!entry)\n\t\tgoto unlock;\n\n\tif (!xas.xa_node)\n\t\tgoto unlock;\n\n\tfor (;;) {\n\t\tunsigned int slot = xas.xa_offset + (1 << order);\n\n\t\tif (slot >= XA_CHUNK_SIZE)\n\t\t\tbreak;\n\t\tif (!xa_is_sibling(xas.xa_node->slots[slot]))\n\t\t\tbreak;\n\t\torder++;\n\t}\n\n\torder += xas.xa_node->shift;\nunlock:\n\trcu_read_unlock();\n\n\treturn order;\n}\nEXPORT_SYMBOL(xa_get_order);\n#endif  \n\n \nint __xa_alloc(struct xarray *xa, u32 *id, void *entry,\n\t\tstruct xa_limit limit, gfp_t gfp)\n{\n\tXA_STATE(xas, xa, 0);\n\n\tif (WARN_ON_ONCE(xa_is_advanced(entry)))\n\t\treturn -EINVAL;\n\tif (WARN_ON_ONCE(!xa_track_free(xa)))\n\t\treturn -EINVAL;\n\n\tif (!entry)\n\t\tentry = XA_ZERO_ENTRY;\n\n\tdo {\n\t\txas.xa_index = limit.min;\n\t\txas_find_marked(&xas, limit.max, XA_FREE_MARK);\n\t\tif (xas.xa_node == XAS_RESTART)\n\t\t\txas_set_err(&xas, -EBUSY);\n\t\telse\n\t\t\t*id = xas.xa_index;\n\t\txas_store(&xas, entry);\n\t\txas_clear_mark(&xas, XA_FREE_MARK);\n\t} while (__xas_nomem(&xas, gfp));\n\n\treturn xas_error(&xas);\n}\nEXPORT_SYMBOL(__xa_alloc);\n\n \nint __xa_alloc_cyclic(struct xarray *xa, u32 *id, void *entry,\n\t\tstruct xa_limit limit, u32 *next, gfp_t gfp)\n{\n\tu32 min = limit.min;\n\tint ret;\n\n\tlimit.min = max(min, *next);\n\tret = __xa_alloc(xa, id, entry, limit, gfp);\n\tif ((xa->xa_flags & XA_FLAGS_ALLOC_WRAPPED) && ret == 0) {\n\t\txa->xa_flags &= ~XA_FLAGS_ALLOC_WRAPPED;\n\t\tret = 1;\n\t}\n\n\tif (ret < 0 && limit.min > min) {\n\t\tlimit.min = min;\n\t\tret = __xa_alloc(xa, id, entry, limit, gfp);\n\t\tif (ret == 0)\n\t\t\tret = 1;\n\t}\n\n\tif (ret >= 0) {\n\t\t*next = *id + 1;\n\t\tif (*next == 0)\n\t\t\txa->xa_flags |= XA_FLAGS_ALLOC_WRAPPED;\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL(__xa_alloc_cyclic);\n\n \nvoid __xa_set_mark(struct xarray *xa, unsigned long index, xa_mark_t mark)\n{\n\tXA_STATE(xas, xa, index);\n\tvoid *entry = xas_load(&xas);\n\n\tif (entry)\n\t\txas_set_mark(&xas, mark);\n}\nEXPORT_SYMBOL(__xa_set_mark);\n\n \nvoid __xa_clear_mark(struct xarray *xa, unsigned long index, xa_mark_t mark)\n{\n\tXA_STATE(xas, xa, index);\n\tvoid *entry = xas_load(&xas);\n\n\tif (entry)\n\t\txas_clear_mark(&xas, mark);\n}\nEXPORT_SYMBOL(__xa_clear_mark);\n\n \nbool xa_get_mark(struct xarray *xa, unsigned long index, xa_mark_t mark)\n{\n\tXA_STATE(xas, xa, index);\n\tvoid *entry;\n\n\trcu_read_lock();\n\tentry = xas_start(&xas);\n\twhile (xas_get_mark(&xas, mark)) {\n\t\tif (!xa_is_node(entry))\n\t\t\tgoto found;\n\t\tentry = xas_descend(&xas, xa_to_node(entry));\n\t}\n\trcu_read_unlock();\n\treturn false;\n found:\n\trcu_read_unlock();\n\treturn true;\n}\nEXPORT_SYMBOL(xa_get_mark);\n\n \nvoid xa_set_mark(struct xarray *xa, unsigned long index, xa_mark_t mark)\n{\n\txa_lock(xa);\n\t__xa_set_mark(xa, index, mark);\n\txa_unlock(xa);\n}\nEXPORT_SYMBOL(xa_set_mark);\n\n \nvoid xa_clear_mark(struct xarray *xa, unsigned long index, xa_mark_t mark)\n{\n\txa_lock(xa);\n\t__xa_clear_mark(xa, index, mark);\n\txa_unlock(xa);\n}\nEXPORT_SYMBOL(xa_clear_mark);\n\n \nvoid *xa_find(struct xarray *xa, unsigned long *indexp,\n\t\t\tunsigned long max, xa_mark_t filter)\n{\n\tXA_STATE(xas, xa, *indexp);\n\tvoid *entry;\n\n\trcu_read_lock();\n\tdo {\n\t\tif ((__force unsigned int)filter < XA_MAX_MARKS)\n\t\t\tentry = xas_find_marked(&xas, max, filter);\n\t\telse\n\t\t\tentry = xas_find(&xas, max);\n\t} while (xas_retry(&xas, entry));\n\trcu_read_unlock();\n\n\tif (entry)\n\t\t*indexp = xas.xa_index;\n\treturn entry;\n}\nEXPORT_SYMBOL(xa_find);\n\nstatic bool xas_sibling(struct xa_state *xas)\n{\n\tstruct xa_node *node = xas->xa_node;\n\tunsigned long mask;\n\n\tif (!IS_ENABLED(CONFIG_XARRAY_MULTI) || !node)\n\t\treturn false;\n\tmask = (XA_CHUNK_SIZE << node->shift) - 1;\n\treturn (xas->xa_index & mask) >\n\t\t((unsigned long)xas->xa_offset << node->shift);\n}\n\n \nvoid *xa_find_after(struct xarray *xa, unsigned long *indexp,\n\t\t\tunsigned long max, xa_mark_t filter)\n{\n\tXA_STATE(xas, xa, *indexp + 1);\n\tvoid *entry;\n\n\tif (xas.xa_index == 0)\n\t\treturn NULL;\n\n\trcu_read_lock();\n\tfor (;;) {\n\t\tif ((__force unsigned int)filter < XA_MAX_MARKS)\n\t\t\tentry = xas_find_marked(&xas, max, filter);\n\t\telse\n\t\t\tentry = xas_find(&xas, max);\n\n\t\tif (xas_invalid(&xas))\n\t\t\tbreak;\n\t\tif (xas_sibling(&xas))\n\t\t\tcontinue;\n\t\tif (!xas_retry(&xas, entry))\n\t\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\tif (entry)\n\t\t*indexp = xas.xa_index;\n\treturn entry;\n}\nEXPORT_SYMBOL(xa_find_after);\n\nstatic unsigned int xas_extract_present(struct xa_state *xas, void **dst,\n\t\t\tunsigned long max, unsigned int n)\n{\n\tvoid *entry;\n\tunsigned int i = 0;\n\n\trcu_read_lock();\n\txas_for_each(xas, entry, max) {\n\t\tif (xas_retry(xas, entry))\n\t\t\tcontinue;\n\t\tdst[i++] = entry;\n\t\tif (i == n)\n\t\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\treturn i;\n}\n\nstatic unsigned int xas_extract_marked(struct xa_state *xas, void **dst,\n\t\t\tunsigned long max, unsigned int n, xa_mark_t mark)\n{\n\tvoid *entry;\n\tunsigned int i = 0;\n\n\trcu_read_lock();\n\txas_for_each_marked(xas, entry, max, mark) {\n\t\tif (xas_retry(xas, entry))\n\t\t\tcontinue;\n\t\tdst[i++] = entry;\n\t\tif (i == n)\n\t\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\treturn i;\n}\n\n \nunsigned int xa_extract(struct xarray *xa, void **dst, unsigned long start,\n\t\t\tunsigned long max, unsigned int n, xa_mark_t filter)\n{\n\tXA_STATE(xas, xa, start);\n\n\tif (!n)\n\t\treturn 0;\n\n\tif ((__force unsigned int)filter < XA_MAX_MARKS)\n\t\treturn xas_extract_marked(&xas, dst, max, n, filter);\n\treturn xas_extract_present(&xas, dst, max, n);\n}\nEXPORT_SYMBOL(xa_extract);\n\n \nvoid xa_delete_node(struct xa_node *node, xa_update_node_t update)\n{\n\tstruct xa_state xas = {\n\t\t.xa = node->array,\n\t\t.xa_index = (unsigned long)node->offset <<\n\t\t\t\t(node->shift + XA_CHUNK_SHIFT),\n\t\t.xa_shift = node->shift + XA_CHUNK_SHIFT,\n\t\t.xa_offset = node->offset,\n\t\t.xa_node = xa_parent_locked(node->array, node),\n\t\t.xa_update = update,\n\t};\n\n\txas_store(&xas, NULL);\n}\nEXPORT_SYMBOL_GPL(xa_delete_node);\t \n\n \nvoid xa_destroy(struct xarray *xa)\n{\n\tXA_STATE(xas, xa, 0);\n\tunsigned long flags;\n\tvoid *entry;\n\n\txas.xa_node = NULL;\n\txas_lock_irqsave(&xas, flags);\n\tentry = xa_head_locked(xa);\n\tRCU_INIT_POINTER(xa->xa_head, NULL);\n\txas_init_marks(&xas);\n\tif (xa_zero_busy(xa))\n\t\txa_mark_clear(xa, XA_FREE_MARK);\n\t \n\tif (xa_is_node(entry))\n\t\txas_free_nodes(&xas, xa_to_node(entry));\n\txas_unlock_irqrestore(&xas, flags);\n}\nEXPORT_SYMBOL(xa_destroy);\n\n#ifdef XA_DEBUG\nvoid xa_dump_node(const struct xa_node *node)\n{\n\tunsigned i, j;\n\n\tif (!node)\n\t\treturn;\n\tif ((unsigned long)node & 3) {\n\t\tpr_cont(\"node %px\\n\", node);\n\t\treturn;\n\t}\n\n\tpr_cont(\"node %px %s %d parent %px shift %d count %d values %d \"\n\t\t\"array %px list %px %px marks\",\n\t\tnode, node->parent ? \"offset\" : \"max\", node->offset,\n\t\tnode->parent, node->shift, node->count, node->nr_values,\n\t\tnode->array, node->private_list.prev, node->private_list.next);\n\tfor (i = 0; i < XA_MAX_MARKS; i++)\n\t\tfor (j = 0; j < XA_MARK_LONGS; j++)\n\t\t\tpr_cont(\" %lx\", node->marks[i][j]);\n\tpr_cont(\"\\n\");\n}\n\nvoid xa_dump_index(unsigned long index, unsigned int shift)\n{\n\tif (!shift)\n\t\tpr_info(\"%lu: \", index);\n\telse if (shift >= BITS_PER_LONG)\n\t\tpr_info(\"0-%lu: \", ~0UL);\n\telse\n\t\tpr_info(\"%lu-%lu: \", index, index | ((1UL << shift) - 1));\n}\n\nvoid xa_dump_entry(const void *entry, unsigned long index, unsigned long shift)\n{\n\tif (!entry)\n\t\treturn;\n\n\txa_dump_index(index, shift);\n\n\tif (xa_is_node(entry)) {\n\t\tif (shift == 0) {\n\t\t\tpr_cont(\"%px\\n\", entry);\n\t\t} else {\n\t\t\tunsigned long i;\n\t\t\tstruct xa_node *node = xa_to_node(entry);\n\t\t\txa_dump_node(node);\n\t\t\tfor (i = 0; i < XA_CHUNK_SIZE; i++)\n\t\t\t\txa_dump_entry(node->slots[i],\n\t\t\t\t      index + (i << node->shift), node->shift);\n\t\t}\n\t} else if (xa_is_value(entry))\n\t\tpr_cont(\"value %ld (0x%lx) [%px]\\n\", xa_to_value(entry),\n\t\t\t\t\t\txa_to_value(entry), entry);\n\telse if (!xa_is_internal(entry))\n\t\tpr_cont(\"%px\\n\", entry);\n\telse if (xa_is_retry(entry))\n\t\tpr_cont(\"retry (%ld)\\n\", xa_to_internal(entry));\n\telse if (xa_is_sibling(entry))\n\t\tpr_cont(\"sibling (slot %ld)\\n\", xa_to_sibling(entry));\n\telse if (xa_is_zero(entry))\n\t\tpr_cont(\"zero (%ld)\\n\", xa_to_internal(entry));\n\telse\n\t\tpr_cont(\"UNKNOWN ENTRY (%px)\\n\", entry);\n}\n\nvoid xa_dump(const struct xarray *xa)\n{\n\tvoid *entry = xa->xa_head;\n\tunsigned int shift = 0;\n\n\tpr_info(\"xarray: %px head %px flags %x marks %d %d %d\\n\", xa, entry,\n\t\t\txa->xa_flags, xa_marked(xa, XA_MARK_0),\n\t\t\txa_marked(xa, XA_MARK_1), xa_marked(xa, XA_MARK_2));\n\tif (xa_is_node(entry))\n\t\tshift = xa_to_node(entry)->shift + XA_CHUNK_SHIFT;\n\txa_dump_entry(entry, 0, shift);\n}\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}