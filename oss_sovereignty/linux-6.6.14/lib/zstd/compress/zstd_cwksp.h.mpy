{
  "module_name": "zstd_cwksp.h",
  "hash_id": "f7f9e790ee5dd8c1eb9d0df48c9e1bd0e04da39ef9e9dbff91d3b1b664098aa0",
  "original_prompt": "Ingested from linux-6.6.14/lib/zstd/compress/zstd_cwksp.h",
  "human_readable_source": " \n\n#ifndef ZSTD_CWKSP_H\n#define ZSTD_CWKSP_H\n\n \n#include \"../common/zstd_internal.h\"\n\n\n \n\n \n#ifndef ZSTD_CWKSP_ASAN_REDZONE_SIZE\n#define ZSTD_CWKSP_ASAN_REDZONE_SIZE 128\n#endif\n\n\n \n#define ZSTD_CWKSP_ALIGNMENT_BYTES 64\n\n \ntypedef enum {\n    ZSTD_cwksp_alloc_objects,\n    ZSTD_cwksp_alloc_buffers,\n    ZSTD_cwksp_alloc_aligned\n} ZSTD_cwksp_alloc_phase_e;\n\n \ntypedef enum {\n    ZSTD_cwksp_dynamic_alloc,\n    ZSTD_cwksp_static_alloc\n} ZSTD_cwksp_static_alloc_e;\n\n \ntypedef struct {\n    void* workspace;\n    void* workspaceEnd;\n\n    void* objectEnd;\n    void* tableEnd;\n    void* tableValidEnd;\n    void* allocStart;\n\n    BYTE allocFailed;\n    int workspaceOversizedDuration;\n    ZSTD_cwksp_alloc_phase_e phase;\n    ZSTD_cwksp_static_alloc_e isStatic;\n} ZSTD_cwksp;\n\n \n\nMEM_STATIC size_t ZSTD_cwksp_available_space(ZSTD_cwksp* ws);\n\nMEM_STATIC void ZSTD_cwksp_assert_internal_consistency(ZSTD_cwksp* ws) {\n    (void)ws;\n    assert(ws->workspace <= ws->objectEnd);\n    assert(ws->objectEnd <= ws->tableEnd);\n    assert(ws->objectEnd <= ws->tableValidEnd);\n    assert(ws->tableEnd <= ws->allocStart);\n    assert(ws->tableValidEnd <= ws->allocStart);\n    assert(ws->allocStart <= ws->workspaceEnd);\n}\n\n \nMEM_STATIC size_t ZSTD_cwksp_align(size_t size, size_t const align) {\n    size_t const mask = align - 1;\n    assert((align & mask) == 0);\n    return (size + mask) & ~mask;\n}\n\n \nMEM_STATIC size_t ZSTD_cwksp_alloc_size(size_t size) {\n    if (size == 0)\n        return 0;\n    return size;\n}\n\n \nMEM_STATIC size_t ZSTD_cwksp_aligned_alloc_size(size_t size) {\n    return ZSTD_cwksp_alloc_size(ZSTD_cwksp_align(size, ZSTD_CWKSP_ALIGNMENT_BYTES));\n}\n\n \nMEM_STATIC size_t ZSTD_cwksp_slack_space_required(void) {\n     \n    size_t const slackSpace = ZSTD_CWKSP_ALIGNMENT_BYTES;\n    return slackSpace;\n}\n\n\n \nMEM_STATIC size_t ZSTD_cwksp_bytes_to_align_ptr(void* ptr, const size_t alignBytes) {\n    size_t const alignBytesMask = alignBytes - 1;\n    size_t const bytes = (alignBytes - ((size_t)ptr & (alignBytesMask))) & alignBytesMask;\n    assert((alignBytes & alignBytesMask) == 0);\n    assert(bytes != ZSTD_CWKSP_ALIGNMENT_BYTES);\n    return bytes;\n}\n\n \nMEM_STATIC void*\nZSTD_cwksp_reserve_internal_buffer_space(ZSTD_cwksp* ws, size_t const bytes)\n{\n    void* const alloc = (BYTE*)ws->allocStart - bytes;\n    void* const bottom = ws->tableEnd;\n    DEBUGLOG(5, \"cwksp: reserving %p %zd bytes, %zd bytes remaining\",\n        alloc, bytes, ZSTD_cwksp_available_space(ws) - bytes);\n    ZSTD_cwksp_assert_internal_consistency(ws);\n    assert(alloc >= bottom);\n    if (alloc < bottom) {\n        DEBUGLOG(4, \"cwksp: alloc failed!\");\n        ws->allocFailed = 1;\n        return NULL;\n    }\n     \n    if (alloc < ws->tableValidEnd) {\n        ws->tableValidEnd = alloc;\n    }\n    ws->allocStart = alloc;\n    return alloc;\n}\n\n \nMEM_STATIC size_t\nZSTD_cwksp_internal_advance_phase(ZSTD_cwksp* ws, ZSTD_cwksp_alloc_phase_e phase)\n{\n    assert(phase >= ws->phase);\n    if (phase > ws->phase) {\n         \n        if (ws->phase < ZSTD_cwksp_alloc_buffers &&\n                phase >= ZSTD_cwksp_alloc_buffers) {\n            ws->tableValidEnd = ws->objectEnd;\n        }\n\n         \n        if (ws->phase < ZSTD_cwksp_alloc_aligned &&\n                phase >= ZSTD_cwksp_alloc_aligned) {\n            {    \n                size_t const bytesToAlign =\n                    ZSTD_CWKSP_ALIGNMENT_BYTES - ZSTD_cwksp_bytes_to_align_ptr(ws->allocStart, ZSTD_CWKSP_ALIGNMENT_BYTES);\n                DEBUGLOG(5, \"reserving aligned alignment addtl space: %zu\", bytesToAlign);\n                ZSTD_STATIC_ASSERT((ZSTD_CWKSP_ALIGNMENT_BYTES & (ZSTD_CWKSP_ALIGNMENT_BYTES - 1)) == 0);  \n                RETURN_ERROR_IF(!ZSTD_cwksp_reserve_internal_buffer_space(ws, bytesToAlign),\n                                memory_allocation, \"aligned phase - alignment initial allocation failed!\");\n            }\n            {    \n                void* const alloc = ws->objectEnd;\n                size_t const bytesToAlign = ZSTD_cwksp_bytes_to_align_ptr(alloc, ZSTD_CWKSP_ALIGNMENT_BYTES);\n                void* const objectEnd = (BYTE*)alloc + bytesToAlign;\n                DEBUGLOG(5, \"reserving table alignment addtl space: %zu\", bytesToAlign);\n                RETURN_ERROR_IF(objectEnd > ws->workspaceEnd, memory_allocation,\n                                \"table phase - alignment initial allocation failed!\");\n                ws->objectEnd = objectEnd;\n                ws->tableEnd = objectEnd;   \n                if (ws->tableValidEnd < ws->tableEnd) {\n                    ws->tableValidEnd = ws->tableEnd;\n        }   }   }\n        ws->phase = phase;\n        ZSTD_cwksp_assert_internal_consistency(ws);\n    }\n    return 0;\n}\n\n \nMEM_STATIC int ZSTD_cwksp_owns_buffer(const ZSTD_cwksp* ws, const void* ptr)\n{\n    return (ptr != NULL) && (ws->workspace <= ptr) && (ptr <= ws->workspaceEnd);\n}\n\n \nMEM_STATIC void*\nZSTD_cwksp_reserve_internal(ZSTD_cwksp* ws, size_t bytes, ZSTD_cwksp_alloc_phase_e phase)\n{\n    void* alloc;\n    if (ZSTD_isError(ZSTD_cwksp_internal_advance_phase(ws, phase)) || bytes == 0) {\n        return NULL;\n    }\n\n\n    alloc = ZSTD_cwksp_reserve_internal_buffer_space(ws, bytes);\n\n\n    return alloc;\n}\n\n \nMEM_STATIC BYTE* ZSTD_cwksp_reserve_buffer(ZSTD_cwksp* ws, size_t bytes)\n{\n    return (BYTE*)ZSTD_cwksp_reserve_internal(ws, bytes, ZSTD_cwksp_alloc_buffers);\n}\n\n \nMEM_STATIC void* ZSTD_cwksp_reserve_aligned(ZSTD_cwksp* ws, size_t bytes)\n{\n    void* ptr = ZSTD_cwksp_reserve_internal(ws, ZSTD_cwksp_align(bytes, ZSTD_CWKSP_ALIGNMENT_BYTES),\n                                            ZSTD_cwksp_alloc_aligned);\n    assert(((size_t)ptr & (ZSTD_CWKSP_ALIGNMENT_BYTES-1))== 0);\n    return ptr;\n}\n\n \nMEM_STATIC void* ZSTD_cwksp_reserve_table(ZSTD_cwksp* ws, size_t bytes)\n{\n    const ZSTD_cwksp_alloc_phase_e phase = ZSTD_cwksp_alloc_aligned;\n    void* alloc;\n    void* end;\n    void* top;\n\n    if (ZSTD_isError(ZSTD_cwksp_internal_advance_phase(ws, phase))) {\n        return NULL;\n    }\n    alloc = ws->tableEnd;\n    end = (BYTE *)alloc + bytes;\n    top = ws->allocStart;\n\n    DEBUGLOG(5, \"cwksp: reserving %p table %zd bytes, %zd bytes remaining\",\n        alloc, bytes, ZSTD_cwksp_available_space(ws) - bytes);\n    assert((bytes & (sizeof(U32)-1)) == 0);\n    ZSTD_cwksp_assert_internal_consistency(ws);\n    assert(end <= top);\n    if (end > top) {\n        DEBUGLOG(4, \"cwksp: table alloc failed!\");\n        ws->allocFailed = 1;\n        return NULL;\n    }\n    ws->tableEnd = end;\n\n\n    assert((bytes & (ZSTD_CWKSP_ALIGNMENT_BYTES-1)) == 0);\n    assert(((size_t)alloc & (ZSTD_CWKSP_ALIGNMENT_BYTES-1))== 0);\n    return alloc;\n}\n\n \nMEM_STATIC void* ZSTD_cwksp_reserve_object(ZSTD_cwksp* ws, size_t bytes)\n{\n    size_t const roundedBytes = ZSTD_cwksp_align(bytes, sizeof(void*));\n    void* alloc = ws->objectEnd;\n    void* end = (BYTE*)alloc + roundedBytes;\n\n\n    DEBUGLOG(4,\n        \"cwksp: reserving %p object %zd bytes (rounded to %zd), %zd bytes remaining\",\n        alloc, bytes, roundedBytes, ZSTD_cwksp_available_space(ws) - roundedBytes);\n    assert((size_t)alloc % ZSTD_ALIGNOF(void*) == 0);\n    assert(bytes % ZSTD_ALIGNOF(void*) == 0);\n    ZSTD_cwksp_assert_internal_consistency(ws);\n     \n    if (ws->phase != ZSTD_cwksp_alloc_objects || end > ws->workspaceEnd) {\n        DEBUGLOG(3, \"cwksp: object alloc failed!\");\n        ws->allocFailed = 1;\n        return NULL;\n    }\n    ws->objectEnd = end;\n    ws->tableEnd = end;\n    ws->tableValidEnd = end;\n\n\n    return alloc;\n}\n\nMEM_STATIC void ZSTD_cwksp_mark_tables_dirty(ZSTD_cwksp* ws)\n{\n    DEBUGLOG(4, \"cwksp: ZSTD_cwksp_mark_tables_dirty\");\n\n\n    assert(ws->tableValidEnd >= ws->objectEnd);\n    assert(ws->tableValidEnd <= ws->allocStart);\n    ws->tableValidEnd = ws->objectEnd;\n    ZSTD_cwksp_assert_internal_consistency(ws);\n}\n\nMEM_STATIC void ZSTD_cwksp_mark_tables_clean(ZSTD_cwksp* ws) {\n    DEBUGLOG(4, \"cwksp: ZSTD_cwksp_mark_tables_clean\");\n    assert(ws->tableValidEnd >= ws->objectEnd);\n    assert(ws->tableValidEnd <= ws->allocStart);\n    if (ws->tableValidEnd < ws->tableEnd) {\n        ws->tableValidEnd = ws->tableEnd;\n    }\n    ZSTD_cwksp_assert_internal_consistency(ws);\n}\n\n \nMEM_STATIC void ZSTD_cwksp_clean_tables(ZSTD_cwksp* ws) {\n    DEBUGLOG(4, \"cwksp: ZSTD_cwksp_clean_tables\");\n    assert(ws->tableValidEnd >= ws->objectEnd);\n    assert(ws->tableValidEnd <= ws->allocStart);\n    if (ws->tableValidEnd < ws->tableEnd) {\n        ZSTD_memset(ws->tableValidEnd, 0, (BYTE*)ws->tableEnd - (BYTE*)ws->tableValidEnd);\n    }\n    ZSTD_cwksp_mark_tables_clean(ws);\n}\n\n \nMEM_STATIC void ZSTD_cwksp_clear_tables(ZSTD_cwksp* ws) {\n    DEBUGLOG(4, \"cwksp: clearing tables!\");\n\n\n    ws->tableEnd = ws->objectEnd;\n    ZSTD_cwksp_assert_internal_consistency(ws);\n}\n\n \nMEM_STATIC void ZSTD_cwksp_clear(ZSTD_cwksp* ws) {\n    DEBUGLOG(4, \"cwksp: clearing!\");\n\n\n\n    ws->tableEnd = ws->objectEnd;\n    ws->allocStart = ws->workspaceEnd;\n    ws->allocFailed = 0;\n    if (ws->phase > ZSTD_cwksp_alloc_buffers) {\n        ws->phase = ZSTD_cwksp_alloc_buffers;\n    }\n    ZSTD_cwksp_assert_internal_consistency(ws);\n}\n\n \nMEM_STATIC void ZSTD_cwksp_init(ZSTD_cwksp* ws, void* start, size_t size, ZSTD_cwksp_static_alloc_e isStatic) {\n    DEBUGLOG(4, \"cwksp: init'ing workspace with %zd bytes\", size);\n    assert(((size_t)start & (sizeof(void*)-1)) == 0);  \n    ws->workspace = start;\n    ws->workspaceEnd = (BYTE*)start + size;\n    ws->objectEnd = ws->workspace;\n    ws->tableValidEnd = ws->objectEnd;\n    ws->phase = ZSTD_cwksp_alloc_objects;\n    ws->isStatic = isStatic;\n    ZSTD_cwksp_clear(ws);\n    ws->workspaceOversizedDuration = 0;\n    ZSTD_cwksp_assert_internal_consistency(ws);\n}\n\nMEM_STATIC size_t ZSTD_cwksp_create(ZSTD_cwksp* ws, size_t size, ZSTD_customMem customMem) {\n    void* workspace = ZSTD_customMalloc(size, customMem);\n    DEBUGLOG(4, \"cwksp: creating new workspace with %zd bytes\", size);\n    RETURN_ERROR_IF(workspace == NULL, memory_allocation, \"NULL pointer!\");\n    ZSTD_cwksp_init(ws, workspace, size, ZSTD_cwksp_dynamic_alloc);\n    return 0;\n}\n\nMEM_STATIC void ZSTD_cwksp_free(ZSTD_cwksp* ws, ZSTD_customMem customMem) {\n    void *ptr = ws->workspace;\n    DEBUGLOG(4, \"cwksp: freeing workspace\");\n    ZSTD_memset(ws, 0, sizeof(ZSTD_cwksp));\n    ZSTD_customFree(ptr, customMem);\n}\n\n \nMEM_STATIC void ZSTD_cwksp_move(ZSTD_cwksp* dst, ZSTD_cwksp* src) {\n    *dst = *src;\n    ZSTD_memset(src, 0, sizeof(ZSTD_cwksp));\n}\n\nMEM_STATIC size_t ZSTD_cwksp_sizeof(const ZSTD_cwksp* ws) {\n    return (size_t)((BYTE*)ws->workspaceEnd - (BYTE*)ws->workspace);\n}\n\nMEM_STATIC size_t ZSTD_cwksp_used(const ZSTD_cwksp* ws) {\n    return (size_t)((BYTE*)ws->tableEnd - (BYTE*)ws->workspace)\n         + (size_t)((BYTE*)ws->workspaceEnd - (BYTE*)ws->allocStart);\n}\n\nMEM_STATIC int ZSTD_cwksp_reserve_failed(const ZSTD_cwksp* ws) {\n    return ws->allocFailed;\n}\n\n \n\n \nMEM_STATIC int ZSTD_cwksp_estimated_space_within_bounds(const ZSTD_cwksp* const ws,\n                                                        size_t const estimatedSpace, int resizedWorkspace) {\n    if (resizedWorkspace) {\n         \n        return ZSTD_cwksp_used(ws) == estimatedSpace;\n    } else {\n         \n        return (ZSTD_cwksp_used(ws) >= estimatedSpace - 63) && (ZSTD_cwksp_used(ws) <= estimatedSpace + 63);\n    }\n}\n\n\nMEM_STATIC size_t ZSTD_cwksp_available_space(ZSTD_cwksp* ws) {\n    return (size_t)((BYTE*)ws->allocStart - (BYTE*)ws->tableEnd);\n}\n\nMEM_STATIC int ZSTD_cwksp_check_available(ZSTD_cwksp* ws, size_t additionalNeededSpace) {\n    return ZSTD_cwksp_available_space(ws) >= additionalNeededSpace;\n}\n\nMEM_STATIC int ZSTD_cwksp_check_too_large(ZSTD_cwksp* ws, size_t additionalNeededSpace) {\n    return ZSTD_cwksp_check_available(\n        ws, additionalNeededSpace * ZSTD_WORKSPACETOOLARGE_FACTOR);\n}\n\nMEM_STATIC int ZSTD_cwksp_check_wasteful(ZSTD_cwksp* ws, size_t additionalNeededSpace) {\n    return ZSTD_cwksp_check_too_large(ws, additionalNeededSpace)\n        && ws->workspaceOversizedDuration > ZSTD_WORKSPACETOOLARGE_MAXDURATION;\n}\n\nMEM_STATIC void ZSTD_cwksp_bump_oversized_duration(\n        ZSTD_cwksp* ws, size_t additionalNeededSpace) {\n    if (ZSTD_cwksp_check_too_large(ws, additionalNeededSpace)) {\n        ws->workspaceOversizedDuration++;\n    } else {\n        ws->workspaceOversizedDuration = 0;\n    }\n}\n\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}