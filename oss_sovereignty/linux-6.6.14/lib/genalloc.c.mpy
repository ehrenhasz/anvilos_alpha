{
  "module_name": "genalloc.c",
  "hash_id": "9d3392ff4059b80ae98fc6a7de7aa575db6479a57c867326d571c14af08b4e00",
  "original_prompt": "Ingested from linux-6.6.14/lib/genalloc.c",
  "human_readable_source": "\n \n\n#include <linux/slab.h>\n#include <linux/export.h>\n#include <linux/bitmap.h>\n#include <linux/rculist.h>\n#include <linux/interrupt.h>\n#include <linux/genalloc.h>\n#include <linux/of.h>\n#include <linux/of_platform.h>\n#include <linux/platform_device.h>\n#include <linux/vmalloc.h>\n\nstatic inline size_t chunk_size(const struct gen_pool_chunk *chunk)\n{\n\treturn chunk->end_addr - chunk->start_addr + 1;\n}\n\nstatic inline int\nset_bits_ll(unsigned long *addr, unsigned long mask_to_set)\n{\n\tunsigned long val = READ_ONCE(*addr);\n\n\tdo {\n\t\tif (val & mask_to_set)\n\t\t\treturn -EBUSY;\n\t\tcpu_relax();\n\t} while (!try_cmpxchg(addr, &val, val | mask_to_set));\n\n\treturn 0;\n}\n\nstatic inline int\nclear_bits_ll(unsigned long *addr, unsigned long mask_to_clear)\n{\n\tunsigned long val = READ_ONCE(*addr);\n\n\tdo {\n\t\tif ((val & mask_to_clear) != mask_to_clear)\n\t\t\treturn -EBUSY;\n\t\tcpu_relax();\n\t} while (!try_cmpxchg(addr, &val, val & ~mask_to_clear));\n\n\treturn 0;\n}\n\n \nstatic unsigned long\nbitmap_set_ll(unsigned long *map, unsigned long start, unsigned long nr)\n{\n\tunsigned long *p = map + BIT_WORD(start);\n\tconst unsigned long size = start + nr;\n\tint bits_to_set = BITS_PER_LONG - (start % BITS_PER_LONG);\n\tunsigned long mask_to_set = BITMAP_FIRST_WORD_MASK(start);\n\n\twhile (nr >= bits_to_set) {\n\t\tif (set_bits_ll(p, mask_to_set))\n\t\t\treturn nr;\n\t\tnr -= bits_to_set;\n\t\tbits_to_set = BITS_PER_LONG;\n\t\tmask_to_set = ~0UL;\n\t\tp++;\n\t}\n\tif (nr) {\n\t\tmask_to_set &= BITMAP_LAST_WORD_MASK(size);\n\t\tif (set_bits_ll(p, mask_to_set))\n\t\t\treturn nr;\n\t}\n\n\treturn 0;\n}\n\n \nstatic unsigned long\nbitmap_clear_ll(unsigned long *map, unsigned long start, unsigned long nr)\n{\n\tunsigned long *p = map + BIT_WORD(start);\n\tconst unsigned long size = start + nr;\n\tint bits_to_clear = BITS_PER_LONG - (start % BITS_PER_LONG);\n\tunsigned long mask_to_clear = BITMAP_FIRST_WORD_MASK(start);\n\n\twhile (nr >= bits_to_clear) {\n\t\tif (clear_bits_ll(p, mask_to_clear))\n\t\t\treturn nr;\n\t\tnr -= bits_to_clear;\n\t\tbits_to_clear = BITS_PER_LONG;\n\t\tmask_to_clear = ~0UL;\n\t\tp++;\n\t}\n\tif (nr) {\n\t\tmask_to_clear &= BITMAP_LAST_WORD_MASK(size);\n\t\tif (clear_bits_ll(p, mask_to_clear))\n\t\t\treturn nr;\n\t}\n\n\treturn 0;\n}\n\n \nstruct gen_pool *gen_pool_create(int min_alloc_order, int nid)\n{\n\tstruct gen_pool *pool;\n\n\tpool = kmalloc_node(sizeof(struct gen_pool), GFP_KERNEL, nid);\n\tif (pool != NULL) {\n\t\tspin_lock_init(&pool->lock);\n\t\tINIT_LIST_HEAD(&pool->chunks);\n\t\tpool->min_alloc_order = min_alloc_order;\n\t\tpool->algo = gen_pool_first_fit;\n\t\tpool->data = NULL;\n\t\tpool->name = NULL;\n\t}\n\treturn pool;\n}\nEXPORT_SYMBOL(gen_pool_create);\n\n \nint gen_pool_add_owner(struct gen_pool *pool, unsigned long virt, phys_addr_t phys,\n\t\t size_t size, int nid, void *owner)\n{\n\tstruct gen_pool_chunk *chunk;\n\tunsigned long nbits = size >> pool->min_alloc_order;\n\tunsigned long nbytes = sizeof(struct gen_pool_chunk) +\n\t\t\t\tBITS_TO_LONGS(nbits) * sizeof(long);\n\n\tchunk = vzalloc_node(nbytes, nid);\n\tif (unlikely(chunk == NULL))\n\t\treturn -ENOMEM;\n\n\tchunk->phys_addr = phys;\n\tchunk->start_addr = virt;\n\tchunk->end_addr = virt + size - 1;\n\tchunk->owner = owner;\n\tatomic_long_set(&chunk->avail, size);\n\n\tspin_lock(&pool->lock);\n\tlist_add_rcu(&chunk->next_chunk, &pool->chunks);\n\tspin_unlock(&pool->lock);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(gen_pool_add_owner);\n\n \nphys_addr_t gen_pool_virt_to_phys(struct gen_pool *pool, unsigned long addr)\n{\n\tstruct gen_pool_chunk *chunk;\n\tphys_addr_t paddr = -1;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(chunk, &pool->chunks, next_chunk) {\n\t\tif (addr >= chunk->start_addr && addr <= chunk->end_addr) {\n\t\t\tpaddr = chunk->phys_addr + (addr - chunk->start_addr);\n\t\t\tbreak;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\treturn paddr;\n}\nEXPORT_SYMBOL(gen_pool_virt_to_phys);\n\n \nvoid gen_pool_destroy(struct gen_pool *pool)\n{\n\tstruct list_head *_chunk, *_next_chunk;\n\tstruct gen_pool_chunk *chunk;\n\tint order = pool->min_alloc_order;\n\tunsigned long bit, end_bit;\n\n\tlist_for_each_safe(_chunk, _next_chunk, &pool->chunks) {\n\t\tchunk = list_entry(_chunk, struct gen_pool_chunk, next_chunk);\n\t\tlist_del(&chunk->next_chunk);\n\n\t\tend_bit = chunk_size(chunk) >> order;\n\t\tbit = find_first_bit(chunk->bits, end_bit);\n\t\tBUG_ON(bit < end_bit);\n\n\t\tvfree(chunk);\n\t}\n\tkfree_const(pool->name);\n\tkfree(pool);\n}\nEXPORT_SYMBOL(gen_pool_destroy);\n\n \nunsigned long gen_pool_alloc_algo_owner(struct gen_pool *pool, size_t size,\n\t\tgenpool_algo_t algo, void *data, void **owner)\n{\n\tstruct gen_pool_chunk *chunk;\n\tunsigned long addr = 0;\n\tint order = pool->min_alloc_order;\n\tunsigned long nbits, start_bit, end_bit, remain;\n\n#ifndef CONFIG_ARCH_HAVE_NMI_SAFE_CMPXCHG\n\tBUG_ON(in_nmi());\n#endif\n\n\tif (owner)\n\t\t*owner = NULL;\n\n\tif (size == 0)\n\t\treturn 0;\n\n\tnbits = (size + (1UL << order) - 1) >> order;\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(chunk, &pool->chunks, next_chunk) {\n\t\tif (size > atomic_long_read(&chunk->avail))\n\t\t\tcontinue;\n\n\t\tstart_bit = 0;\n\t\tend_bit = chunk_size(chunk) >> order;\nretry:\n\t\tstart_bit = algo(chunk->bits, end_bit, start_bit,\n\t\t\t\t nbits, data, pool, chunk->start_addr);\n\t\tif (start_bit >= end_bit)\n\t\t\tcontinue;\n\t\tremain = bitmap_set_ll(chunk->bits, start_bit, nbits);\n\t\tif (remain) {\n\t\t\tremain = bitmap_clear_ll(chunk->bits, start_bit,\n\t\t\t\t\t\t nbits - remain);\n\t\t\tBUG_ON(remain);\n\t\t\tgoto retry;\n\t\t}\n\n\t\taddr = chunk->start_addr + ((unsigned long)start_bit << order);\n\t\tsize = nbits << order;\n\t\tatomic_long_sub(size, &chunk->avail);\n\t\tif (owner)\n\t\t\t*owner = chunk->owner;\n\t\tbreak;\n\t}\n\trcu_read_unlock();\n\treturn addr;\n}\nEXPORT_SYMBOL(gen_pool_alloc_algo_owner);\n\n \nvoid *gen_pool_dma_alloc(struct gen_pool *pool, size_t size, dma_addr_t *dma)\n{\n\treturn gen_pool_dma_alloc_algo(pool, size, dma, pool->algo, pool->data);\n}\nEXPORT_SYMBOL(gen_pool_dma_alloc);\n\n \nvoid *gen_pool_dma_alloc_algo(struct gen_pool *pool, size_t size,\n\t\tdma_addr_t *dma, genpool_algo_t algo, void *data)\n{\n\tunsigned long vaddr;\n\n\tif (!pool)\n\t\treturn NULL;\n\n\tvaddr = gen_pool_alloc_algo(pool, size, algo, data);\n\tif (!vaddr)\n\t\treturn NULL;\n\n\tif (dma)\n\t\t*dma = gen_pool_virt_to_phys(pool, vaddr);\n\n\treturn (void *)vaddr;\n}\nEXPORT_SYMBOL(gen_pool_dma_alloc_algo);\n\n \nvoid *gen_pool_dma_alloc_align(struct gen_pool *pool, size_t size,\n\t\tdma_addr_t *dma, int align)\n{\n\tstruct genpool_data_align data = { .align = align };\n\n\treturn gen_pool_dma_alloc_algo(pool, size, dma,\n\t\t\tgen_pool_first_fit_align, &data);\n}\nEXPORT_SYMBOL(gen_pool_dma_alloc_align);\n\n \nvoid *gen_pool_dma_zalloc(struct gen_pool *pool, size_t size, dma_addr_t *dma)\n{\n\treturn gen_pool_dma_zalloc_algo(pool, size, dma, pool->algo, pool->data);\n}\nEXPORT_SYMBOL(gen_pool_dma_zalloc);\n\n \nvoid *gen_pool_dma_zalloc_algo(struct gen_pool *pool, size_t size,\n\t\tdma_addr_t *dma, genpool_algo_t algo, void *data)\n{\n\tvoid *vaddr = gen_pool_dma_alloc_algo(pool, size, dma, algo, data);\n\n\tif (vaddr)\n\t\tmemset(vaddr, 0, size);\n\n\treturn vaddr;\n}\nEXPORT_SYMBOL(gen_pool_dma_zalloc_algo);\n\n \nvoid *gen_pool_dma_zalloc_align(struct gen_pool *pool, size_t size,\n\t\tdma_addr_t *dma, int align)\n{\n\tstruct genpool_data_align data = { .align = align };\n\n\treturn gen_pool_dma_zalloc_algo(pool, size, dma,\n\t\t\tgen_pool_first_fit_align, &data);\n}\nEXPORT_SYMBOL(gen_pool_dma_zalloc_align);\n\n \nvoid gen_pool_free_owner(struct gen_pool *pool, unsigned long addr, size_t size,\n\t\tvoid **owner)\n{\n\tstruct gen_pool_chunk *chunk;\n\tint order = pool->min_alloc_order;\n\tunsigned long start_bit, nbits, remain;\n\n#ifndef CONFIG_ARCH_HAVE_NMI_SAFE_CMPXCHG\n\tBUG_ON(in_nmi());\n#endif\n\n\tif (owner)\n\t\t*owner = NULL;\n\n\tnbits = (size + (1UL << order) - 1) >> order;\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(chunk, &pool->chunks, next_chunk) {\n\t\tif (addr >= chunk->start_addr && addr <= chunk->end_addr) {\n\t\t\tBUG_ON(addr + size - 1 > chunk->end_addr);\n\t\t\tstart_bit = (addr - chunk->start_addr) >> order;\n\t\t\tremain = bitmap_clear_ll(chunk->bits, start_bit, nbits);\n\t\t\tBUG_ON(remain);\n\t\t\tsize = nbits << order;\n\t\t\tatomic_long_add(size, &chunk->avail);\n\t\t\tif (owner)\n\t\t\t\t*owner = chunk->owner;\n\t\t\trcu_read_unlock();\n\t\t\treturn;\n\t\t}\n\t}\n\trcu_read_unlock();\n\tBUG();\n}\nEXPORT_SYMBOL(gen_pool_free_owner);\n\n \nvoid gen_pool_for_each_chunk(struct gen_pool *pool,\n\tvoid (*func)(struct gen_pool *pool, struct gen_pool_chunk *chunk, void *data),\n\tvoid *data)\n{\n\tstruct gen_pool_chunk *chunk;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(chunk, &(pool)->chunks, next_chunk)\n\t\tfunc(pool, chunk, data);\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL(gen_pool_for_each_chunk);\n\n \nbool gen_pool_has_addr(struct gen_pool *pool, unsigned long start,\n\t\t\tsize_t size)\n{\n\tbool found = false;\n\tunsigned long end = start + size - 1;\n\tstruct gen_pool_chunk *chunk;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(chunk, &(pool)->chunks, next_chunk) {\n\t\tif (start >= chunk->start_addr && start <= chunk->end_addr) {\n\t\t\tif (end <= chunk->end_addr) {\n\t\t\t\tfound = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn found;\n}\nEXPORT_SYMBOL(gen_pool_has_addr);\n\n \nsize_t gen_pool_avail(struct gen_pool *pool)\n{\n\tstruct gen_pool_chunk *chunk;\n\tsize_t avail = 0;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(chunk, &pool->chunks, next_chunk)\n\t\tavail += atomic_long_read(&chunk->avail);\n\trcu_read_unlock();\n\treturn avail;\n}\nEXPORT_SYMBOL_GPL(gen_pool_avail);\n\n \nsize_t gen_pool_size(struct gen_pool *pool)\n{\n\tstruct gen_pool_chunk *chunk;\n\tsize_t size = 0;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(chunk, &pool->chunks, next_chunk)\n\t\tsize += chunk_size(chunk);\n\trcu_read_unlock();\n\treturn size;\n}\nEXPORT_SYMBOL_GPL(gen_pool_size);\n\n \nvoid gen_pool_set_algo(struct gen_pool *pool, genpool_algo_t algo, void *data)\n{\n\trcu_read_lock();\n\n\tpool->algo = algo;\n\tif (!pool->algo)\n\t\tpool->algo = gen_pool_first_fit;\n\n\tpool->data = data;\n\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL(gen_pool_set_algo);\n\n \nunsigned long gen_pool_first_fit(unsigned long *map, unsigned long size,\n\t\tunsigned long start, unsigned int nr, void *data,\n\t\tstruct gen_pool *pool, unsigned long start_addr)\n{\n\treturn bitmap_find_next_zero_area(map, size, start, nr, 0);\n}\nEXPORT_SYMBOL(gen_pool_first_fit);\n\n \nunsigned long gen_pool_first_fit_align(unsigned long *map, unsigned long size,\n\t\tunsigned long start, unsigned int nr, void *data,\n\t\tstruct gen_pool *pool, unsigned long start_addr)\n{\n\tstruct genpool_data_align *alignment;\n\tunsigned long align_mask, align_off;\n\tint order;\n\n\talignment = data;\n\torder = pool->min_alloc_order;\n\talign_mask = ((alignment->align + (1UL << order) - 1) >> order) - 1;\n\talign_off = (start_addr & (alignment->align - 1)) >> order;\n\n\treturn bitmap_find_next_zero_area_off(map, size, start, nr,\n\t\t\t\t\t      align_mask, align_off);\n}\nEXPORT_SYMBOL(gen_pool_first_fit_align);\n\n \nunsigned long gen_pool_fixed_alloc(unsigned long *map, unsigned long size,\n\t\tunsigned long start, unsigned int nr, void *data,\n\t\tstruct gen_pool *pool, unsigned long start_addr)\n{\n\tstruct genpool_data_fixed *fixed_data;\n\tint order;\n\tunsigned long offset_bit;\n\tunsigned long start_bit;\n\n\tfixed_data = data;\n\torder = pool->min_alloc_order;\n\toffset_bit = fixed_data->offset >> order;\n\tif (WARN_ON(fixed_data->offset & ((1UL << order) - 1)))\n\t\treturn size;\n\n\tstart_bit = bitmap_find_next_zero_area(map, size,\n\t\t\tstart + offset_bit, nr, 0);\n\tif (start_bit != offset_bit)\n\t\tstart_bit = size;\n\treturn start_bit;\n}\nEXPORT_SYMBOL(gen_pool_fixed_alloc);\n\n \nunsigned long gen_pool_first_fit_order_align(unsigned long *map,\n\t\tunsigned long size, unsigned long start,\n\t\tunsigned int nr, void *data, struct gen_pool *pool,\n\t\tunsigned long start_addr)\n{\n\tunsigned long align_mask = roundup_pow_of_two(nr) - 1;\n\n\treturn bitmap_find_next_zero_area(map, size, start, nr, align_mask);\n}\nEXPORT_SYMBOL(gen_pool_first_fit_order_align);\n\n \nunsigned long gen_pool_best_fit(unsigned long *map, unsigned long size,\n\t\tunsigned long start, unsigned int nr, void *data,\n\t\tstruct gen_pool *pool, unsigned long start_addr)\n{\n\tunsigned long start_bit = size;\n\tunsigned long len = size + 1;\n\tunsigned long index;\n\n\tindex = bitmap_find_next_zero_area(map, size, start, nr, 0);\n\n\twhile (index < size) {\n\t\tunsigned long next_bit = find_next_bit(map, size, index + nr);\n\t\tif ((next_bit - index) < len) {\n\t\t\tlen = next_bit - index;\n\t\t\tstart_bit = index;\n\t\t\tif (len == nr)\n\t\t\t\treturn start_bit;\n\t\t}\n\t\tindex = bitmap_find_next_zero_area(map, size,\n\t\t\t\t\t\t   next_bit + 1, nr, 0);\n\t}\n\n\treturn start_bit;\n}\nEXPORT_SYMBOL(gen_pool_best_fit);\n\nstatic void devm_gen_pool_release(struct device *dev, void *res)\n{\n\tgen_pool_destroy(*(struct gen_pool **)res);\n}\n\nstatic int devm_gen_pool_match(struct device *dev, void *res, void *data)\n{\n\tstruct gen_pool **p = res;\n\n\t \n\tif (!data && !(*p)->name)\n\t\treturn 1;\n\n\tif (!data || !(*p)->name)\n\t\treturn 0;\n\n\treturn !strcmp((*p)->name, data);\n}\n\n \nstruct gen_pool *gen_pool_get(struct device *dev, const char *name)\n{\n\tstruct gen_pool **p;\n\n\tp = devres_find(dev, devm_gen_pool_release, devm_gen_pool_match,\n\t\t\t(void *)name);\n\tif (!p)\n\t\treturn NULL;\n\treturn *p;\n}\nEXPORT_SYMBOL_GPL(gen_pool_get);\n\n \nstruct gen_pool *devm_gen_pool_create(struct device *dev, int min_alloc_order,\n\t\t\t\t      int nid, const char *name)\n{\n\tstruct gen_pool **ptr, *pool;\n\tconst char *pool_name = NULL;\n\n\t \n\tif (gen_pool_get(dev, name))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (name) {\n\t\tpool_name = kstrdup_const(name, GFP_KERNEL);\n\t\tif (!pool_name)\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tptr = devres_alloc(devm_gen_pool_release, sizeof(*ptr), GFP_KERNEL);\n\tif (!ptr)\n\t\tgoto free_pool_name;\n\n\tpool = gen_pool_create(min_alloc_order, nid);\n\tif (!pool)\n\t\tgoto free_devres;\n\n\t*ptr = pool;\n\tpool->name = pool_name;\n\tdevres_add(dev, ptr);\n\n\treturn pool;\n\nfree_devres:\n\tdevres_free(ptr);\nfree_pool_name:\n\tkfree_const(pool_name);\n\n\treturn ERR_PTR(-ENOMEM);\n}\nEXPORT_SYMBOL(devm_gen_pool_create);\n\n#ifdef CONFIG_OF\n \nstruct gen_pool *of_gen_pool_get(struct device_node *np,\n\tconst char *propname, int index)\n{\n\tstruct platform_device *pdev;\n\tstruct device_node *np_pool, *parent;\n\tconst char *name = NULL;\n\tstruct gen_pool *pool = NULL;\n\n\tnp_pool = of_parse_phandle(np, propname, index);\n\tif (!np_pool)\n\t\treturn NULL;\n\n\tpdev = of_find_device_by_node(np_pool);\n\tif (!pdev) {\n\t\t \n\t\tparent = of_get_parent(np_pool);\n\t\tpdev = of_find_device_by_node(parent);\n\t\tof_node_put(parent);\n\n\t\tof_property_read_string(np_pool, \"label\", &name);\n\t\tif (!name)\n\t\t\tname = of_node_full_name(np_pool);\n\t}\n\tif (pdev)\n\t\tpool = gen_pool_get(&pdev->dev, name);\n\tof_node_put(np_pool);\n\n\treturn pool;\n}\nEXPORT_SYMBOL_GPL(of_gen_pool_get);\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}