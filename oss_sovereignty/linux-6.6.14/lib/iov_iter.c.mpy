{
  "module_name": "iov_iter.c",
  "hash_id": "bcc149ee6b2f2ff79db0a4ce6b5febb7188420a441331af45bbe1cf182da4e55",
  "original_prompt": "Ingested from linux-6.6.14/lib/iov_iter.c",
  "human_readable_source": "\n#include <crypto/hash.h>\n#include <linux/export.h>\n#include <linux/bvec.h>\n#include <linux/fault-inject-usercopy.h>\n#include <linux/uio.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/slab.h>\n#include <linux/vmalloc.h>\n#include <linux/splice.h>\n#include <linux/compat.h>\n#include <net/checksum.h>\n#include <linux/scatterlist.h>\n#include <linux/instrumented.h>\n\n \n#define iterate_buf(i, n, base, len, off, __p, STEP) {\t\t\\\n\tsize_t __maybe_unused off = 0;\t\t\t\t\\\n\tlen = n;\t\t\t\t\t\t\\\n\tbase = __p + i->iov_offset;\t\t\t\t\\\n\tlen -= (STEP);\t\t\t\t\t\t\\\n\ti->iov_offset += len;\t\t\t\t\t\\\n\tn = len;\t\t\t\t\t\t\\\n}\n\n \n#define iterate_iovec(i, n, base, len, off, __p, STEP) {\t\\\n\tsize_t off = 0;\t\t\t\t\t\t\\\n\tsize_t skip = i->iov_offset;\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\tlen = min(n, __p->iov_len - skip);\t\t\\\n\t\tif (likely(len)) {\t\t\t\t\\\n\t\t\tbase = __p->iov_base + skip;\t\t\\\n\t\t\tlen -= (STEP);\t\t\t\t\\\n\t\t\toff += len;\t\t\t\t\\\n\t\t\tskip += len;\t\t\t\t\\\n\t\t\tn -= len;\t\t\t\t\\\n\t\t\tif (skip < __p->iov_len)\t\t\\\n\t\t\t\tbreak;\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\\\n\t\t__p++;\t\t\t\t\t\t\\\n\t\tskip = 0;\t\t\t\t\t\\\n\t} while (n);\t\t\t\t\t\t\\\n\ti->iov_offset = skip;\t\t\t\t\t\\\n\tn = off;\t\t\t\t\t\t\\\n}\n\n#define iterate_bvec(i, n, base, len, off, p, STEP) {\t\t\\\n\tsize_t off = 0;\t\t\t\t\t\t\\\n\tunsigned skip = i->iov_offset;\t\t\t\t\\\n\twhile (n) {\t\t\t\t\t\t\\\n\t\tunsigned offset = p->bv_offset + skip;\t\t\\\n\t\tunsigned left;\t\t\t\t\t\\\n\t\tvoid *kaddr = kmap_local_page(p->bv_page +\t\\\n\t\t\t\t\toffset / PAGE_SIZE);\t\\\n\t\tbase = kaddr + offset % PAGE_SIZE;\t\t\\\n\t\tlen = min(min(n, (size_t)(p->bv_len - skip)),\t\\\n\t\t     (size_t)(PAGE_SIZE - offset % PAGE_SIZE));\t\\\n\t\tleft = (STEP);\t\t\t\t\t\\\n\t\tkunmap_local(kaddr);\t\t\t\t\\\n\t\tlen -= left;\t\t\t\t\t\\\n\t\toff += len;\t\t\t\t\t\\\n\t\tskip += len;\t\t\t\t\t\\\n\t\tif (skip == p->bv_len) {\t\t\t\\\n\t\t\tskip = 0;\t\t\t\t\\\n\t\t\tp++;\t\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\\\n\t\tn -= len;\t\t\t\t\t\\\n\t\tif (left)\t\t\t\t\t\\\n\t\t\tbreak;\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\ti->iov_offset = skip;\t\t\t\t\t\\\n\tn = off;\t\t\t\t\t\t\\\n}\n\n#define iterate_xarray(i, n, base, len, __off, STEP) {\t\t\\\n\t__label__ __out;\t\t\t\t\t\\\n\tsize_t __off = 0;\t\t\t\t\t\\\n\tstruct folio *folio;\t\t\t\t\t\\\n\tloff_t start = i->xarray_start + i->iov_offset;\t\t\\\n\tpgoff_t index = start / PAGE_SIZE;\t\t\t\\\n\tXA_STATE(xas, i->xarray, index);\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tlen = PAGE_SIZE - offset_in_page(start);\t\t\\\n\trcu_read_lock();\t\t\t\t\t\\\n\txas_for_each(&xas, folio, ULONG_MAX) {\t\t\t\\\n\t\tunsigned left;\t\t\t\t\t\\\n\t\tsize_t offset;\t\t\t\t\t\\\n\t\tif (xas_retry(&xas, folio))\t\t\t\\\n\t\t\tcontinue;\t\t\t\t\\\n\t\tif (WARN_ON(xa_is_value(folio)))\t\t\\\n\t\t\tbreak;\t\t\t\t\t\\\n\t\tif (WARN_ON(folio_test_hugetlb(folio)))\t\t\\\n\t\t\tbreak;\t\t\t\t\t\\\n\t\toffset = offset_in_folio(folio, start + __off);\t\\\n\t\twhile (offset < folio_size(folio)) {\t\t\\\n\t\t\tbase = kmap_local_folio(folio, offset);\t\\\n\t\t\tlen = min(n, len);\t\t\t\\\n\t\t\tleft = (STEP);\t\t\t\t\\\n\t\t\tkunmap_local(base);\t\t\t\\\n\t\t\tlen -= left;\t\t\t\t\\\n\t\t\t__off += len;\t\t\t\t\\\n\t\t\tn -= len;\t\t\t\t\\\n\t\t\tif (left || n == 0)\t\t\t\\\n\t\t\t\tgoto __out;\t\t\t\\\n\t\t\toffset += len;\t\t\t\t\\\n\t\t\tlen = PAGE_SIZE;\t\t\t\\\n\t\t}\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n__out:\t\t\t\t\t\t\t\t\\\n\trcu_read_unlock();\t\t\t\t\t\\\n\ti->iov_offset += __off;\t\t\t\t\t\\\n\tn = __off;\t\t\t\t\t\t\\\n}\n\n#define __iterate_and_advance(i, n, base, len, off, I, K) {\t\\\n\tif (unlikely(i->count < n))\t\t\t\t\\\n\t\tn = i->count;\t\t\t\t\t\\\n\tif (likely(n)) {\t\t\t\t\t\\\n\t\tif (likely(iter_is_ubuf(i))) {\t\t\t\\\n\t\t\tvoid __user *base;\t\t\t\\\n\t\t\tsize_t len;\t\t\t\t\\\n\t\t\titerate_buf(i, n, base, len, off,\t\\\n\t\t\t\t\t\ti->ubuf, (I)) \t\\\n\t\t} else if (likely(iter_is_iovec(i))) {\t\t\\\n\t\t\tconst struct iovec *iov = iter_iov(i);\t\\\n\t\t\tvoid __user *base;\t\t\t\\\n\t\t\tsize_t len;\t\t\t\t\\\n\t\t\titerate_iovec(i, n, base, len, off,\t\\\n\t\t\t\t\t\tiov, (I))\t\\\n\t\t\ti->nr_segs -= iov - iter_iov(i);\t\\\n\t\t\ti->__iov = iov;\t\t\t\t\\\n\t\t} else if (iov_iter_is_bvec(i)) {\t\t\\\n\t\t\tconst struct bio_vec *bvec = i->bvec;\t\\\n\t\t\tvoid *base;\t\t\t\t\\\n\t\t\tsize_t len;\t\t\t\t\\\n\t\t\titerate_bvec(i, n, base, len, off,\t\\\n\t\t\t\t\t\tbvec, (K))\t\\\n\t\t\ti->nr_segs -= bvec - i->bvec;\t\t\\\n\t\t\ti->bvec = bvec;\t\t\t\t\\\n\t\t} else if (iov_iter_is_kvec(i)) {\t\t\\\n\t\t\tconst struct kvec *kvec = i->kvec;\t\\\n\t\t\tvoid *base;\t\t\t\t\\\n\t\t\tsize_t len;\t\t\t\t\\\n\t\t\titerate_iovec(i, n, base, len, off,\t\\\n\t\t\t\t\t\tkvec, (K))\t\\\n\t\t\ti->nr_segs -= kvec - i->kvec;\t\t\\\n\t\t\ti->kvec = kvec;\t\t\t\t\\\n\t\t} else if (iov_iter_is_xarray(i)) {\t\t\\\n\t\t\tvoid *base;\t\t\t\t\\\n\t\t\tsize_t len;\t\t\t\t\\\n\t\t\titerate_xarray(i, n, base, len, off,\t\\\n\t\t\t\t\t\t\t(K))\t\\\n\t\t}\t\t\t\t\t\t\\\n\t\ti->count -= n;\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n}\n#define iterate_and_advance(i, n, base, len, off, I, K) \\\n\t__iterate_and_advance(i, n, base, len, off, I, ((void)(K),0))\n\nstatic int copyout(void __user *to, const void *from, size_t n)\n{\n\tif (should_fail_usercopy())\n\t\treturn n;\n\tif (access_ok(to, n)) {\n\t\tinstrument_copy_to_user(to, from, n);\n\t\tn = raw_copy_to_user(to, from, n);\n\t}\n\treturn n;\n}\n\nstatic int copyout_nofault(void __user *to, const void *from, size_t n)\n{\n\tlong res;\n\n\tif (should_fail_usercopy())\n\t\treturn n;\n\n\tres = copy_to_user_nofault(to, from, n);\n\n\treturn res < 0 ? n : res;\n}\n\nstatic int copyin(void *to, const void __user *from, size_t n)\n{\n\tsize_t res = n;\n\n\tif (should_fail_usercopy())\n\t\treturn n;\n\tif (access_ok(from, n)) {\n\t\tinstrument_copy_from_user_before(to, from, n);\n\t\tres = raw_copy_from_user(to, from, n);\n\t\tinstrument_copy_from_user_after(to, from, n, res);\n\t}\n\treturn res;\n}\n\n \nsize_t fault_in_iov_iter_readable(const struct iov_iter *i, size_t size)\n{\n\tif (iter_is_ubuf(i)) {\n\t\tsize_t n = min(size, iov_iter_count(i));\n\t\tn -= fault_in_readable(i->ubuf + i->iov_offset, n);\n\t\treturn size - n;\n\t} else if (iter_is_iovec(i)) {\n\t\tsize_t count = min(size, iov_iter_count(i));\n\t\tconst struct iovec *p;\n\t\tsize_t skip;\n\n\t\tsize -= count;\n\t\tfor (p = iter_iov(i), skip = i->iov_offset; count; p++, skip = 0) {\n\t\t\tsize_t len = min(count, p->iov_len - skip);\n\t\t\tsize_t ret;\n\n\t\t\tif (unlikely(!len))\n\t\t\t\tcontinue;\n\t\t\tret = fault_in_readable(p->iov_base + skip, len);\n\t\t\tcount -= len - ret;\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\treturn count + size;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(fault_in_iov_iter_readable);\n\n \nsize_t fault_in_iov_iter_writeable(const struct iov_iter *i, size_t size)\n{\n\tif (iter_is_ubuf(i)) {\n\t\tsize_t n = min(size, iov_iter_count(i));\n\t\tn -= fault_in_safe_writeable(i->ubuf + i->iov_offset, n);\n\t\treturn size - n;\n\t} else if (iter_is_iovec(i)) {\n\t\tsize_t count = min(size, iov_iter_count(i));\n\t\tconst struct iovec *p;\n\t\tsize_t skip;\n\n\t\tsize -= count;\n\t\tfor (p = iter_iov(i), skip = i->iov_offset; count; p++, skip = 0) {\n\t\t\tsize_t len = min(count, p->iov_len - skip);\n\t\t\tsize_t ret;\n\n\t\t\tif (unlikely(!len))\n\t\t\t\tcontinue;\n\t\t\tret = fault_in_safe_writeable(p->iov_base + skip, len);\n\t\t\tcount -= len - ret;\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\treturn count + size;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(fault_in_iov_iter_writeable);\n\nvoid iov_iter_init(struct iov_iter *i, unsigned int direction,\n\t\t\tconst struct iovec *iov, unsigned long nr_segs,\n\t\t\tsize_t count)\n{\n\tWARN_ON(direction & ~(READ | WRITE));\n\t*i = (struct iov_iter) {\n\t\t.iter_type = ITER_IOVEC,\n\t\t.copy_mc = false,\n\t\t.nofault = false,\n\t\t.user_backed = true,\n\t\t.data_source = direction,\n\t\t.__iov = iov,\n\t\t.nr_segs = nr_segs,\n\t\t.iov_offset = 0,\n\t\t.count = count\n\t};\n}\nEXPORT_SYMBOL(iov_iter_init);\n\nstatic __wsum csum_and_memcpy(void *to, const void *from, size_t len,\n\t\t\t      __wsum sum, size_t off)\n{\n\t__wsum next = csum_partial_copy_nocheck(from, to, len);\n\treturn csum_block_add(sum, next, off);\n}\n\nsize_t _copy_to_iter(const void *addr, size_t bytes, struct iov_iter *i)\n{\n\tif (WARN_ON_ONCE(i->data_source))\n\t\treturn 0;\n\tif (user_backed_iter(i))\n\t\tmight_fault();\n\titerate_and_advance(i, bytes, base, len, off,\n\t\tcopyout(base, addr + off, len),\n\t\tmemcpy(base, addr + off, len)\n\t)\n\n\treturn bytes;\n}\nEXPORT_SYMBOL(_copy_to_iter);\n\n#ifdef CONFIG_ARCH_HAS_COPY_MC\nstatic int copyout_mc(void __user *to, const void *from, size_t n)\n{\n\tif (access_ok(to, n)) {\n\t\tinstrument_copy_to_user(to, from, n);\n\t\tn = copy_mc_to_user((__force void *) to, from, n);\n\t}\n\treturn n;\n}\n\n \nsize_t _copy_mc_to_iter(const void *addr, size_t bytes, struct iov_iter *i)\n{\n\tif (WARN_ON_ONCE(i->data_source))\n\t\treturn 0;\n\tif (user_backed_iter(i))\n\t\tmight_fault();\n\t__iterate_and_advance(i, bytes, base, len, off,\n\t\tcopyout_mc(base, addr + off, len),\n\t\tcopy_mc_to_kernel(base, addr + off, len)\n\t)\n\n\treturn bytes;\n}\nEXPORT_SYMBOL_GPL(_copy_mc_to_iter);\n#endif  \n\nstatic void *memcpy_from_iter(struct iov_iter *i, void *to, const void *from,\n\t\t\t\t size_t size)\n{\n\tif (iov_iter_is_copy_mc(i))\n\t\treturn (void *)copy_mc_to_kernel(to, from, size);\n\treturn memcpy(to, from, size);\n}\n\nsize_t _copy_from_iter(void *addr, size_t bytes, struct iov_iter *i)\n{\n\tif (WARN_ON_ONCE(!i->data_source))\n\t\treturn 0;\n\n\tif (user_backed_iter(i))\n\t\tmight_fault();\n\titerate_and_advance(i, bytes, base, len, off,\n\t\tcopyin(addr + off, base, len),\n\t\tmemcpy_from_iter(i, addr + off, base, len)\n\t)\n\n\treturn bytes;\n}\nEXPORT_SYMBOL(_copy_from_iter);\n\nsize_t _copy_from_iter_nocache(void *addr, size_t bytes, struct iov_iter *i)\n{\n\tif (WARN_ON_ONCE(!i->data_source))\n\t\treturn 0;\n\n\titerate_and_advance(i, bytes, base, len, off,\n\t\t__copy_from_user_inatomic_nocache(addr + off, base, len),\n\t\tmemcpy(addr + off, base, len)\n\t)\n\n\treturn bytes;\n}\nEXPORT_SYMBOL(_copy_from_iter_nocache);\n\n#ifdef CONFIG_ARCH_HAS_UACCESS_FLUSHCACHE\n \nsize_t _copy_from_iter_flushcache(void *addr, size_t bytes, struct iov_iter *i)\n{\n\tif (WARN_ON_ONCE(!i->data_source))\n\t\treturn 0;\n\n\titerate_and_advance(i, bytes, base, len, off,\n\t\t__copy_from_user_flushcache(addr + off, base, len),\n\t\tmemcpy_flushcache(addr + off, base, len)\n\t)\n\n\treturn bytes;\n}\nEXPORT_SYMBOL_GPL(_copy_from_iter_flushcache);\n#endif\n\nstatic inline bool page_copy_sane(struct page *page, size_t offset, size_t n)\n{\n\tstruct page *head;\n\tsize_t v = n + offset;\n\n\t \n\tif (n <= v && v <= PAGE_SIZE)\n\t\treturn true;\n\n\thead = compound_head(page);\n\tv += (page - head) << PAGE_SHIFT;\n\n\tif (WARN_ON(n > v || v > page_size(head)))\n\t\treturn false;\n\treturn true;\n}\n\nsize_t copy_page_to_iter(struct page *page, size_t offset, size_t bytes,\n\t\t\t struct iov_iter *i)\n{\n\tsize_t res = 0;\n\tif (!page_copy_sane(page, offset, bytes))\n\t\treturn 0;\n\tif (WARN_ON_ONCE(i->data_source))\n\t\treturn 0;\n\tpage += offset / PAGE_SIZE;  \n\toffset %= PAGE_SIZE;\n\twhile (1) {\n\t\tvoid *kaddr = kmap_local_page(page);\n\t\tsize_t n = min(bytes, (size_t)PAGE_SIZE - offset);\n\t\tn = _copy_to_iter(kaddr + offset, n, i);\n\t\tkunmap_local(kaddr);\n\t\tres += n;\n\t\tbytes -= n;\n\t\tif (!bytes || !n)\n\t\t\tbreak;\n\t\toffset += n;\n\t\tif (offset == PAGE_SIZE) {\n\t\t\tpage++;\n\t\t\toffset = 0;\n\t\t}\n\t}\n\treturn res;\n}\nEXPORT_SYMBOL(copy_page_to_iter);\n\nsize_t copy_page_to_iter_nofault(struct page *page, unsigned offset, size_t bytes,\n\t\t\t\t struct iov_iter *i)\n{\n\tsize_t res = 0;\n\n\tif (!page_copy_sane(page, offset, bytes))\n\t\treturn 0;\n\tif (WARN_ON_ONCE(i->data_source))\n\t\treturn 0;\n\tpage += offset / PAGE_SIZE;  \n\toffset %= PAGE_SIZE;\n\twhile (1) {\n\t\tvoid *kaddr = kmap_local_page(page);\n\t\tsize_t n = min(bytes, (size_t)PAGE_SIZE - offset);\n\n\t\titerate_and_advance(i, n, base, len, off,\n\t\t\tcopyout_nofault(base, kaddr + offset + off, len),\n\t\t\tmemcpy(base, kaddr + offset + off, len)\n\t\t)\n\t\tkunmap_local(kaddr);\n\t\tres += n;\n\t\tbytes -= n;\n\t\tif (!bytes || !n)\n\t\t\tbreak;\n\t\toffset += n;\n\t\tif (offset == PAGE_SIZE) {\n\t\t\tpage++;\n\t\t\toffset = 0;\n\t\t}\n\t}\n\treturn res;\n}\nEXPORT_SYMBOL(copy_page_to_iter_nofault);\n\nsize_t copy_page_from_iter(struct page *page, size_t offset, size_t bytes,\n\t\t\t struct iov_iter *i)\n{\n\tsize_t res = 0;\n\tif (!page_copy_sane(page, offset, bytes))\n\t\treturn 0;\n\tpage += offset / PAGE_SIZE;  \n\toffset %= PAGE_SIZE;\n\twhile (1) {\n\t\tvoid *kaddr = kmap_local_page(page);\n\t\tsize_t n = min(bytes, (size_t)PAGE_SIZE - offset);\n\t\tn = _copy_from_iter(kaddr + offset, n, i);\n\t\tkunmap_local(kaddr);\n\t\tres += n;\n\t\tbytes -= n;\n\t\tif (!bytes || !n)\n\t\t\tbreak;\n\t\toffset += n;\n\t\tif (offset == PAGE_SIZE) {\n\t\t\tpage++;\n\t\t\toffset = 0;\n\t\t}\n\t}\n\treturn res;\n}\nEXPORT_SYMBOL(copy_page_from_iter);\n\nsize_t iov_iter_zero(size_t bytes, struct iov_iter *i)\n{\n\titerate_and_advance(i, bytes, base, len, count,\n\t\tclear_user(base, len),\n\t\tmemset(base, 0, len)\n\t)\n\n\treturn bytes;\n}\nEXPORT_SYMBOL(iov_iter_zero);\n\nsize_t copy_page_from_iter_atomic(struct page *page, size_t offset,\n\t\tsize_t bytes, struct iov_iter *i)\n{\n\tsize_t n, copied = 0;\n\n\tif (!page_copy_sane(page, offset, bytes))\n\t\treturn 0;\n\tif (WARN_ON_ONCE(!i->data_source))\n\t\treturn 0;\n\n\tdo {\n\t\tchar *p;\n\n\t\tn = bytes - copied;\n\t\tif (PageHighMem(page)) {\n\t\t\tpage += offset / PAGE_SIZE;\n\t\t\toffset %= PAGE_SIZE;\n\t\t\tn = min_t(size_t, n, PAGE_SIZE - offset);\n\t\t}\n\n\t\tp = kmap_atomic(page) + offset;\n\t\titerate_and_advance(i, n, base, len, off,\n\t\t\tcopyin(p + off, base, len),\n\t\t\tmemcpy_from_iter(i, p + off, base, len)\n\t\t)\n\t\tkunmap_atomic(p);\n\t\tcopied += n;\n\t\toffset += n;\n\t} while (PageHighMem(page) && copied != bytes && n > 0);\n\n\treturn copied;\n}\nEXPORT_SYMBOL(copy_page_from_iter_atomic);\n\nstatic void iov_iter_bvec_advance(struct iov_iter *i, size_t size)\n{\n\tconst struct bio_vec *bvec, *end;\n\n\tif (!i->count)\n\t\treturn;\n\ti->count -= size;\n\n\tsize += i->iov_offset;\n\n\tfor (bvec = i->bvec, end = bvec + i->nr_segs; bvec < end; bvec++) {\n\t\tif (likely(size < bvec->bv_len))\n\t\t\tbreak;\n\t\tsize -= bvec->bv_len;\n\t}\n\ti->iov_offset = size;\n\ti->nr_segs -= bvec - i->bvec;\n\ti->bvec = bvec;\n}\n\nstatic void iov_iter_iovec_advance(struct iov_iter *i, size_t size)\n{\n\tconst struct iovec *iov, *end;\n\n\tif (!i->count)\n\t\treturn;\n\ti->count -= size;\n\n\tsize += i->iov_offset;  \n\tfor (iov = iter_iov(i), end = iov + i->nr_segs; iov < end; iov++) {\n\t\tif (likely(size < iov->iov_len))\n\t\t\tbreak;\n\t\tsize -= iov->iov_len;\n\t}\n\ti->iov_offset = size;\n\ti->nr_segs -= iov - iter_iov(i);\n\ti->__iov = iov;\n}\n\nvoid iov_iter_advance(struct iov_iter *i, size_t size)\n{\n\tif (unlikely(i->count < size))\n\t\tsize = i->count;\n\tif (likely(iter_is_ubuf(i)) || unlikely(iov_iter_is_xarray(i))) {\n\t\ti->iov_offset += size;\n\t\ti->count -= size;\n\t} else if (likely(iter_is_iovec(i) || iov_iter_is_kvec(i))) {\n\t\t \n\t\tiov_iter_iovec_advance(i, size);\n\t} else if (iov_iter_is_bvec(i)) {\n\t\tiov_iter_bvec_advance(i, size);\n\t} else if (iov_iter_is_discard(i)) {\n\t\ti->count -= size;\n\t}\n}\nEXPORT_SYMBOL(iov_iter_advance);\n\nvoid iov_iter_revert(struct iov_iter *i, size_t unroll)\n{\n\tif (!unroll)\n\t\treturn;\n\tif (WARN_ON(unroll > MAX_RW_COUNT))\n\t\treturn;\n\ti->count += unroll;\n\tif (unlikely(iov_iter_is_discard(i)))\n\t\treturn;\n\tif (unroll <= i->iov_offset) {\n\t\ti->iov_offset -= unroll;\n\t\treturn;\n\t}\n\tunroll -= i->iov_offset;\n\tif (iov_iter_is_xarray(i) || iter_is_ubuf(i)) {\n\t\tBUG();  \n\t} else if (iov_iter_is_bvec(i)) {\n\t\tconst struct bio_vec *bvec = i->bvec;\n\t\twhile (1) {\n\t\t\tsize_t n = (--bvec)->bv_len;\n\t\t\ti->nr_segs++;\n\t\t\tif (unroll <= n) {\n\t\t\t\ti->bvec = bvec;\n\t\t\t\ti->iov_offset = n - unroll;\n\t\t\t\treturn;\n\t\t\t}\n\t\t\tunroll -= n;\n\t\t}\n\t} else {  \n\t\tconst struct iovec *iov = iter_iov(i);\n\t\twhile (1) {\n\t\t\tsize_t n = (--iov)->iov_len;\n\t\t\ti->nr_segs++;\n\t\t\tif (unroll <= n) {\n\t\t\t\ti->__iov = iov;\n\t\t\t\ti->iov_offset = n - unroll;\n\t\t\t\treturn;\n\t\t\t}\n\t\t\tunroll -= n;\n\t\t}\n\t}\n}\nEXPORT_SYMBOL(iov_iter_revert);\n\n \nsize_t iov_iter_single_seg_count(const struct iov_iter *i)\n{\n\tif (i->nr_segs > 1) {\n\t\tif (likely(iter_is_iovec(i) || iov_iter_is_kvec(i)))\n\t\t\treturn min(i->count, iter_iov(i)->iov_len - i->iov_offset);\n\t\tif (iov_iter_is_bvec(i))\n\t\t\treturn min(i->count, i->bvec->bv_len - i->iov_offset);\n\t}\n\treturn i->count;\n}\nEXPORT_SYMBOL(iov_iter_single_seg_count);\n\nvoid iov_iter_kvec(struct iov_iter *i, unsigned int direction,\n\t\t\tconst struct kvec *kvec, unsigned long nr_segs,\n\t\t\tsize_t count)\n{\n\tWARN_ON(direction & ~(READ | WRITE));\n\t*i = (struct iov_iter){\n\t\t.iter_type = ITER_KVEC,\n\t\t.copy_mc = false,\n\t\t.data_source = direction,\n\t\t.kvec = kvec,\n\t\t.nr_segs = nr_segs,\n\t\t.iov_offset = 0,\n\t\t.count = count\n\t};\n}\nEXPORT_SYMBOL(iov_iter_kvec);\n\nvoid iov_iter_bvec(struct iov_iter *i, unsigned int direction,\n\t\t\tconst struct bio_vec *bvec, unsigned long nr_segs,\n\t\t\tsize_t count)\n{\n\tWARN_ON(direction & ~(READ | WRITE));\n\t*i = (struct iov_iter){\n\t\t.iter_type = ITER_BVEC,\n\t\t.copy_mc = false,\n\t\t.data_source = direction,\n\t\t.bvec = bvec,\n\t\t.nr_segs = nr_segs,\n\t\t.iov_offset = 0,\n\t\t.count = count\n\t};\n}\nEXPORT_SYMBOL(iov_iter_bvec);\n\n \nvoid iov_iter_xarray(struct iov_iter *i, unsigned int direction,\n\t\t     struct xarray *xarray, loff_t start, size_t count)\n{\n\tBUG_ON(direction & ~1);\n\t*i = (struct iov_iter) {\n\t\t.iter_type = ITER_XARRAY,\n\t\t.copy_mc = false,\n\t\t.data_source = direction,\n\t\t.xarray = xarray,\n\t\t.xarray_start = start,\n\t\t.count = count,\n\t\t.iov_offset = 0\n\t};\n}\nEXPORT_SYMBOL(iov_iter_xarray);\n\n \nvoid iov_iter_discard(struct iov_iter *i, unsigned int direction, size_t count)\n{\n\tBUG_ON(direction != READ);\n\t*i = (struct iov_iter){\n\t\t.iter_type = ITER_DISCARD,\n\t\t.copy_mc = false,\n\t\t.data_source = false,\n\t\t.count = count,\n\t\t.iov_offset = 0\n\t};\n}\nEXPORT_SYMBOL(iov_iter_discard);\n\nstatic bool iov_iter_aligned_iovec(const struct iov_iter *i, unsigned addr_mask,\n\t\t\t\t   unsigned len_mask)\n{\n\tsize_t size = i->count;\n\tsize_t skip = i->iov_offset;\n\tunsigned k;\n\n\tfor (k = 0; k < i->nr_segs; k++, skip = 0) {\n\t\tconst struct iovec *iov = iter_iov(i) + k;\n\t\tsize_t len = iov->iov_len - skip;\n\n\t\tif (len > size)\n\t\t\tlen = size;\n\t\tif (len & len_mask)\n\t\t\treturn false;\n\t\tif ((unsigned long)(iov->iov_base + skip) & addr_mask)\n\t\t\treturn false;\n\n\t\tsize -= len;\n\t\tif (!size)\n\t\t\tbreak;\n\t}\n\treturn true;\n}\n\nstatic bool iov_iter_aligned_bvec(const struct iov_iter *i, unsigned addr_mask,\n\t\t\t\t  unsigned len_mask)\n{\n\tsize_t size = i->count;\n\tunsigned skip = i->iov_offset;\n\tunsigned k;\n\n\tfor (k = 0; k < i->nr_segs; k++, skip = 0) {\n\t\tsize_t len = i->bvec[k].bv_len - skip;\n\n\t\tif (len > size)\n\t\t\tlen = size;\n\t\tif (len & len_mask)\n\t\t\treturn false;\n\t\tif ((unsigned long)(i->bvec[k].bv_offset + skip) & addr_mask)\n\t\t\treturn false;\n\n\t\tsize -= len;\n\t\tif (!size)\n\t\t\tbreak;\n\t}\n\treturn true;\n}\n\n \nbool iov_iter_is_aligned(const struct iov_iter *i, unsigned addr_mask,\n\t\t\t unsigned len_mask)\n{\n\tif (likely(iter_is_ubuf(i))) {\n\t\tif (i->count & len_mask)\n\t\t\treturn false;\n\t\tif ((unsigned long)(i->ubuf + i->iov_offset) & addr_mask)\n\t\t\treturn false;\n\t\treturn true;\n\t}\n\n\tif (likely(iter_is_iovec(i) || iov_iter_is_kvec(i)))\n\t\treturn iov_iter_aligned_iovec(i, addr_mask, len_mask);\n\n\tif (iov_iter_is_bvec(i))\n\t\treturn iov_iter_aligned_bvec(i, addr_mask, len_mask);\n\n\tif (iov_iter_is_xarray(i)) {\n\t\tif (i->count & len_mask)\n\t\t\treturn false;\n\t\tif ((i->xarray_start + i->iov_offset) & addr_mask)\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\nEXPORT_SYMBOL_GPL(iov_iter_is_aligned);\n\nstatic unsigned long iov_iter_alignment_iovec(const struct iov_iter *i)\n{\n\tunsigned long res = 0;\n\tsize_t size = i->count;\n\tsize_t skip = i->iov_offset;\n\tunsigned k;\n\n\tfor (k = 0; k < i->nr_segs; k++, skip = 0) {\n\t\tconst struct iovec *iov = iter_iov(i) + k;\n\t\tsize_t len = iov->iov_len - skip;\n\t\tif (len) {\n\t\t\tres |= (unsigned long)iov->iov_base + skip;\n\t\t\tif (len > size)\n\t\t\t\tlen = size;\n\t\t\tres |= len;\n\t\t\tsize -= len;\n\t\t\tif (!size)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\treturn res;\n}\n\nstatic unsigned long iov_iter_alignment_bvec(const struct iov_iter *i)\n{\n\tunsigned res = 0;\n\tsize_t size = i->count;\n\tunsigned skip = i->iov_offset;\n\tunsigned k;\n\n\tfor (k = 0; k < i->nr_segs; k++, skip = 0) {\n\t\tsize_t len = i->bvec[k].bv_len - skip;\n\t\tres |= (unsigned long)i->bvec[k].bv_offset + skip;\n\t\tif (len > size)\n\t\t\tlen = size;\n\t\tres |= len;\n\t\tsize -= len;\n\t\tif (!size)\n\t\t\tbreak;\n\t}\n\treturn res;\n}\n\nunsigned long iov_iter_alignment(const struct iov_iter *i)\n{\n\tif (likely(iter_is_ubuf(i))) {\n\t\tsize_t size = i->count;\n\t\tif (size)\n\t\t\treturn ((unsigned long)i->ubuf + i->iov_offset) | size;\n\t\treturn 0;\n\t}\n\n\t \n\tif (likely(iter_is_iovec(i) || iov_iter_is_kvec(i)))\n\t\treturn iov_iter_alignment_iovec(i);\n\n\tif (iov_iter_is_bvec(i))\n\t\treturn iov_iter_alignment_bvec(i);\n\n\tif (iov_iter_is_xarray(i))\n\t\treturn (i->xarray_start + i->iov_offset) | i->count;\n\n\treturn 0;\n}\nEXPORT_SYMBOL(iov_iter_alignment);\n\nunsigned long iov_iter_gap_alignment(const struct iov_iter *i)\n{\n\tunsigned long res = 0;\n\tunsigned long v = 0;\n\tsize_t size = i->count;\n\tunsigned k;\n\n\tif (iter_is_ubuf(i))\n\t\treturn 0;\n\n\tif (WARN_ON(!iter_is_iovec(i)))\n\t\treturn ~0U;\n\n\tfor (k = 0; k < i->nr_segs; k++) {\n\t\tconst struct iovec *iov = iter_iov(i) + k;\n\t\tif (iov->iov_len) {\n\t\t\tunsigned long base = (unsigned long)iov->iov_base;\n\t\t\tif (v) \n\t\t\t\tres |= base | v; \n\t\t\tv = base + iov->iov_len;\n\t\t\tif (size <= iov->iov_len)\n\t\t\t\tbreak;\n\t\t\tsize -= iov->iov_len;\n\t\t}\n\t}\n\treturn res;\n}\nEXPORT_SYMBOL(iov_iter_gap_alignment);\n\nstatic int want_pages_array(struct page ***res, size_t size,\n\t\t\t    size_t start, unsigned int maxpages)\n{\n\tunsigned int count = DIV_ROUND_UP(size + start, PAGE_SIZE);\n\n\tif (count > maxpages)\n\t\tcount = maxpages;\n\tWARN_ON(!count);\t\n\tif (!*res) {\n\t\t*res = kvmalloc_array(count, sizeof(struct page *), GFP_KERNEL);\n\t\tif (!*res)\n\t\t\treturn 0;\n\t}\n\treturn count;\n}\n\nstatic ssize_t iter_xarray_populate_pages(struct page **pages, struct xarray *xa,\n\t\t\t\t\t  pgoff_t index, unsigned int nr_pages)\n{\n\tXA_STATE(xas, xa, index);\n\tstruct page *page;\n\tunsigned int ret = 0;\n\n\trcu_read_lock();\n\tfor (page = xas_load(&xas); page; page = xas_next(&xas)) {\n\t\tif (xas_retry(&xas, page))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (unlikely(page != xas_reload(&xas))) {\n\t\t\txas_reset(&xas);\n\t\t\tcontinue;\n\t\t}\n\n\t\tpages[ret] = find_subpage(page, xas.xa_index);\n\t\tget_page(pages[ret]);\n\t\tif (++ret == nr_pages)\n\t\t\tbreak;\n\t}\n\trcu_read_unlock();\n\treturn ret;\n}\n\nstatic ssize_t iter_xarray_get_pages(struct iov_iter *i,\n\t\t\t\t     struct page ***pages, size_t maxsize,\n\t\t\t\t     unsigned maxpages, size_t *_start_offset)\n{\n\tunsigned nr, offset, count;\n\tpgoff_t index;\n\tloff_t pos;\n\n\tpos = i->xarray_start + i->iov_offset;\n\tindex = pos >> PAGE_SHIFT;\n\toffset = pos & ~PAGE_MASK;\n\t*_start_offset = offset;\n\n\tcount = want_pages_array(pages, maxsize, offset, maxpages);\n\tif (!count)\n\t\treturn -ENOMEM;\n\tnr = iter_xarray_populate_pages(*pages, i->xarray, index, count);\n\tif (nr == 0)\n\t\treturn 0;\n\n\tmaxsize = min_t(size_t, nr * PAGE_SIZE - offset, maxsize);\n\ti->iov_offset += maxsize;\n\ti->count -= maxsize;\n\treturn maxsize;\n}\n\n \nstatic unsigned long first_iovec_segment(const struct iov_iter *i, size_t *size)\n{\n\tsize_t skip;\n\tlong k;\n\n\tif (iter_is_ubuf(i))\n\t\treturn (unsigned long)i->ubuf + i->iov_offset;\n\n\tfor (k = 0, skip = i->iov_offset; k < i->nr_segs; k++, skip = 0) {\n\t\tconst struct iovec *iov = iter_iov(i) + k;\n\t\tsize_t len = iov->iov_len - skip;\n\n\t\tif (unlikely(!len))\n\t\t\tcontinue;\n\t\tif (*size > len)\n\t\t\t*size = len;\n\t\treturn (unsigned long)iov->iov_base + skip;\n\t}\n\tBUG(); \n}\n\n \nstatic struct page *first_bvec_segment(const struct iov_iter *i,\n\t\t\t\t       size_t *size, size_t *start)\n{\n\tstruct page *page;\n\tsize_t skip = i->iov_offset, len;\n\n\tlen = i->bvec->bv_len - skip;\n\tif (*size > len)\n\t\t*size = len;\n\tskip += i->bvec->bv_offset;\n\tpage = i->bvec->bv_page + skip / PAGE_SIZE;\n\t*start = skip % PAGE_SIZE;\n\treturn page;\n}\n\nstatic ssize_t __iov_iter_get_pages_alloc(struct iov_iter *i,\n\t\t   struct page ***pages, size_t maxsize,\n\t\t   unsigned int maxpages, size_t *start)\n{\n\tunsigned int n, gup_flags = 0;\n\n\tif (maxsize > i->count)\n\t\tmaxsize = i->count;\n\tif (!maxsize)\n\t\treturn 0;\n\tif (maxsize > MAX_RW_COUNT)\n\t\tmaxsize = MAX_RW_COUNT;\n\n\tif (likely(user_backed_iter(i))) {\n\t\tunsigned long addr;\n\t\tint res;\n\n\t\tif (iov_iter_rw(i) != WRITE)\n\t\t\tgup_flags |= FOLL_WRITE;\n\t\tif (i->nofault)\n\t\t\tgup_flags |= FOLL_NOFAULT;\n\n\t\taddr = first_iovec_segment(i, &maxsize);\n\t\t*start = addr % PAGE_SIZE;\n\t\taddr &= PAGE_MASK;\n\t\tn = want_pages_array(pages, maxsize, *start, maxpages);\n\t\tif (!n)\n\t\t\treturn -ENOMEM;\n\t\tres = get_user_pages_fast(addr, n, gup_flags, *pages);\n\t\tif (unlikely(res <= 0))\n\t\t\treturn res;\n\t\tmaxsize = min_t(size_t, maxsize, res * PAGE_SIZE - *start);\n\t\tiov_iter_advance(i, maxsize);\n\t\treturn maxsize;\n\t}\n\tif (iov_iter_is_bvec(i)) {\n\t\tstruct page **p;\n\t\tstruct page *page;\n\n\t\tpage = first_bvec_segment(i, &maxsize, start);\n\t\tn = want_pages_array(pages, maxsize, *start, maxpages);\n\t\tif (!n)\n\t\t\treturn -ENOMEM;\n\t\tp = *pages;\n\t\tfor (int k = 0; k < n; k++)\n\t\t\tget_page(p[k] = page + k);\n\t\tmaxsize = min_t(size_t, maxsize, n * PAGE_SIZE - *start);\n\t\ti->count -= maxsize;\n\t\ti->iov_offset += maxsize;\n\t\tif (i->iov_offset == i->bvec->bv_len) {\n\t\t\ti->iov_offset = 0;\n\t\t\ti->bvec++;\n\t\t\ti->nr_segs--;\n\t\t}\n\t\treturn maxsize;\n\t}\n\tif (iov_iter_is_xarray(i))\n\t\treturn iter_xarray_get_pages(i, pages, maxsize, maxpages, start);\n\treturn -EFAULT;\n}\n\nssize_t iov_iter_get_pages2(struct iov_iter *i, struct page **pages,\n\t\tsize_t maxsize, unsigned maxpages, size_t *start)\n{\n\tif (!maxpages)\n\t\treturn 0;\n\tBUG_ON(!pages);\n\n\treturn __iov_iter_get_pages_alloc(i, &pages, maxsize, maxpages, start);\n}\nEXPORT_SYMBOL(iov_iter_get_pages2);\n\nssize_t iov_iter_get_pages_alloc2(struct iov_iter *i,\n\t\tstruct page ***pages, size_t maxsize, size_t *start)\n{\n\tssize_t len;\n\n\t*pages = NULL;\n\n\tlen = __iov_iter_get_pages_alloc(i, pages, maxsize, ~0U, start);\n\tif (len <= 0) {\n\t\tkvfree(*pages);\n\t\t*pages = NULL;\n\t}\n\treturn len;\n}\nEXPORT_SYMBOL(iov_iter_get_pages_alloc2);\n\nsize_t csum_and_copy_from_iter(void *addr, size_t bytes, __wsum *csum,\n\t\t\t       struct iov_iter *i)\n{\n\t__wsum sum, next;\n\tsum = *csum;\n\tif (WARN_ON_ONCE(!i->data_source))\n\t\treturn 0;\n\n\titerate_and_advance(i, bytes, base, len, off, ({\n\t\tnext = csum_and_copy_from_user(base, addr + off, len);\n\t\tsum = csum_block_add(sum, next, off);\n\t\tnext ? 0 : len;\n\t}), ({\n\t\tsum = csum_and_memcpy(addr + off, base, len, sum, off);\n\t})\n\t)\n\t*csum = sum;\n\treturn bytes;\n}\nEXPORT_SYMBOL(csum_and_copy_from_iter);\n\nsize_t csum_and_copy_to_iter(const void *addr, size_t bytes, void *_csstate,\n\t\t\t     struct iov_iter *i)\n{\n\tstruct csum_state *csstate = _csstate;\n\t__wsum sum, next;\n\n\tif (WARN_ON_ONCE(i->data_source))\n\t\treturn 0;\n\tif (unlikely(iov_iter_is_discard(i))) {\n\t\t\n\t\tcsstate->csum = csum_block_add(csstate->csum,\n\t\t\t\t\t       csum_partial(addr, bytes, 0),\n\t\t\t\t\t       csstate->off);\n\t\tcsstate->off += bytes;\n\t\treturn bytes;\n\t}\n\n\tsum = csum_shift(csstate->csum, csstate->off);\n\titerate_and_advance(i, bytes, base, len, off, ({\n\t\tnext = csum_and_copy_to_user(addr + off, base, len);\n\t\tsum = csum_block_add(sum, next, off);\n\t\tnext ? 0 : len;\n\t}), ({\n\t\tsum = csum_and_memcpy(base, addr + off, len, sum, off);\n\t})\n\t)\n\tcsstate->csum = csum_shift(sum, csstate->off);\n\tcsstate->off += bytes;\n\treturn bytes;\n}\nEXPORT_SYMBOL(csum_and_copy_to_iter);\n\nsize_t hash_and_copy_to_iter(const void *addr, size_t bytes, void *hashp,\n\t\tstruct iov_iter *i)\n{\n#ifdef CONFIG_CRYPTO_HASH\n\tstruct ahash_request *hash = hashp;\n\tstruct scatterlist sg;\n\tsize_t copied;\n\n\tcopied = copy_to_iter(addr, bytes, i);\n\tsg_init_one(&sg, addr, copied);\n\tahash_request_set_crypt(hash, &sg, NULL, copied);\n\tcrypto_ahash_update(hash);\n\treturn copied;\n#else\n\treturn 0;\n#endif\n}\nEXPORT_SYMBOL(hash_and_copy_to_iter);\n\nstatic int iov_npages(const struct iov_iter *i, int maxpages)\n{\n\tsize_t skip = i->iov_offset, size = i->count;\n\tconst struct iovec *p;\n\tint npages = 0;\n\n\tfor (p = iter_iov(i); size; skip = 0, p++) {\n\t\tunsigned offs = offset_in_page(p->iov_base + skip);\n\t\tsize_t len = min(p->iov_len - skip, size);\n\n\t\tif (len) {\n\t\t\tsize -= len;\n\t\t\tnpages += DIV_ROUND_UP(offs + len, PAGE_SIZE);\n\t\t\tif (unlikely(npages > maxpages))\n\t\t\t\treturn maxpages;\n\t\t}\n\t}\n\treturn npages;\n}\n\nstatic int bvec_npages(const struct iov_iter *i, int maxpages)\n{\n\tsize_t skip = i->iov_offset, size = i->count;\n\tconst struct bio_vec *p;\n\tint npages = 0;\n\n\tfor (p = i->bvec; size; skip = 0, p++) {\n\t\tunsigned offs = (p->bv_offset + skip) % PAGE_SIZE;\n\t\tsize_t len = min(p->bv_len - skip, size);\n\n\t\tsize -= len;\n\t\tnpages += DIV_ROUND_UP(offs + len, PAGE_SIZE);\n\t\tif (unlikely(npages > maxpages))\n\t\t\treturn maxpages;\n\t}\n\treturn npages;\n}\n\nint iov_iter_npages(const struct iov_iter *i, int maxpages)\n{\n\tif (unlikely(!i->count))\n\t\treturn 0;\n\tif (likely(iter_is_ubuf(i))) {\n\t\tunsigned offs = offset_in_page(i->ubuf + i->iov_offset);\n\t\tint npages = DIV_ROUND_UP(offs + i->count, PAGE_SIZE);\n\t\treturn min(npages, maxpages);\n\t}\n\t \n\tif (likely(iter_is_iovec(i) || iov_iter_is_kvec(i)))\n\t\treturn iov_npages(i, maxpages);\n\tif (iov_iter_is_bvec(i))\n\t\treturn bvec_npages(i, maxpages);\n\tif (iov_iter_is_xarray(i)) {\n\t\tunsigned offset = (i->xarray_start + i->iov_offset) % PAGE_SIZE;\n\t\tint npages = DIV_ROUND_UP(offset + i->count, PAGE_SIZE);\n\t\treturn min(npages, maxpages);\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(iov_iter_npages);\n\nconst void *dup_iter(struct iov_iter *new, struct iov_iter *old, gfp_t flags)\n{\n\t*new = *old;\n\tif (iov_iter_is_bvec(new))\n\t\treturn new->bvec = kmemdup(new->bvec,\n\t\t\t\t    new->nr_segs * sizeof(struct bio_vec),\n\t\t\t\t    flags);\n\telse if (iov_iter_is_kvec(new) || iter_is_iovec(new))\n\t\t \n\t\treturn new->__iov = kmemdup(new->__iov,\n\t\t\t\t   new->nr_segs * sizeof(struct iovec),\n\t\t\t\t   flags);\n\treturn NULL;\n}\nEXPORT_SYMBOL(dup_iter);\n\nstatic __noclone int copy_compat_iovec_from_user(struct iovec *iov,\n\t\tconst struct iovec __user *uvec, unsigned long nr_segs)\n{\n\tconst struct compat_iovec __user *uiov =\n\t\t(const struct compat_iovec __user *)uvec;\n\tint ret = -EFAULT, i;\n\n\tif (!user_access_begin(uiov, nr_segs * sizeof(*uiov)))\n\t\treturn -EFAULT;\n\n\tfor (i = 0; i < nr_segs; i++) {\n\t\tcompat_uptr_t buf;\n\t\tcompat_ssize_t len;\n\n\t\tunsafe_get_user(len, &uiov[i].iov_len, uaccess_end);\n\t\tunsafe_get_user(buf, &uiov[i].iov_base, uaccess_end);\n\n\t\t \n\t\tif (len < 0) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto uaccess_end;\n\t\t}\n\t\tiov[i].iov_base = compat_ptr(buf);\n\t\tiov[i].iov_len = len;\n\t}\n\n\tret = 0;\nuaccess_end:\n\tuser_access_end();\n\treturn ret;\n}\n\nstatic __noclone int copy_iovec_from_user(struct iovec *iov,\n\t\tconst struct iovec __user *uiov, unsigned long nr_segs)\n{\n\tint ret = -EFAULT;\n\n\tif (!user_access_begin(uiov, nr_segs * sizeof(*uiov)))\n\t\treturn -EFAULT;\n\n\tdo {\n\t\tvoid __user *buf;\n\t\tssize_t len;\n\n\t\tunsafe_get_user(len, &uiov->iov_len, uaccess_end);\n\t\tunsafe_get_user(buf, &uiov->iov_base, uaccess_end);\n\n\t\t \n\t\tif (unlikely(len < 0)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto uaccess_end;\n\t\t}\n\t\tiov->iov_base = buf;\n\t\tiov->iov_len = len;\n\n\t\tuiov++; iov++;\n\t} while (--nr_segs);\n\n\tret = 0;\nuaccess_end:\n\tuser_access_end();\n\treturn ret;\n}\n\nstruct iovec *iovec_from_user(const struct iovec __user *uvec,\n\t\tunsigned long nr_segs, unsigned long fast_segs,\n\t\tstruct iovec *fast_iov, bool compat)\n{\n\tstruct iovec *iov = fast_iov;\n\tint ret;\n\n\t \n\tif (nr_segs == 0)\n\t\treturn iov;\n\tif (nr_segs > UIO_MAXIOV)\n\t\treturn ERR_PTR(-EINVAL);\n\tif (nr_segs > fast_segs) {\n\t\tiov = kmalloc_array(nr_segs, sizeof(struct iovec), GFP_KERNEL);\n\t\tif (!iov)\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tif (unlikely(compat))\n\t\tret = copy_compat_iovec_from_user(iov, uvec, nr_segs);\n\telse\n\t\tret = copy_iovec_from_user(iov, uvec, nr_segs);\n\tif (ret) {\n\t\tif (iov != fast_iov)\n\t\t\tkfree(iov);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\treturn iov;\n}\n\n \nstatic ssize_t __import_iovec_ubuf(int type, const struct iovec __user *uvec,\n\t\t\t\t   struct iovec **iovp, struct iov_iter *i,\n\t\t\t\t   bool compat)\n{\n\tstruct iovec *iov = *iovp;\n\tssize_t ret;\n\n\tif (compat)\n\t\tret = copy_compat_iovec_from_user(iov, uvec, 1);\n\telse\n\t\tret = copy_iovec_from_user(iov, uvec, 1);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tret = import_ubuf(type, iov->iov_base, iov->iov_len, i);\n\tif (unlikely(ret))\n\t\treturn ret;\n\t*iovp = NULL;\n\treturn i->count;\n}\n\nssize_t __import_iovec(int type, const struct iovec __user *uvec,\n\t\t unsigned nr_segs, unsigned fast_segs, struct iovec **iovp,\n\t\t struct iov_iter *i, bool compat)\n{\n\tssize_t total_len = 0;\n\tunsigned long seg;\n\tstruct iovec *iov;\n\n\tif (nr_segs == 1)\n\t\treturn __import_iovec_ubuf(type, uvec, iovp, i, compat);\n\n\tiov = iovec_from_user(uvec, nr_segs, fast_segs, *iovp, compat);\n\tif (IS_ERR(iov)) {\n\t\t*iovp = NULL;\n\t\treturn PTR_ERR(iov);\n\t}\n\n\t \n\tfor (seg = 0; seg < nr_segs; seg++) {\n\t\tssize_t len = (ssize_t)iov[seg].iov_len;\n\n\t\tif (!access_ok(iov[seg].iov_base, len)) {\n\t\t\tif (iov != *iovp)\n\t\t\t\tkfree(iov);\n\t\t\t*iovp = NULL;\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tif (len > MAX_RW_COUNT - total_len) {\n\t\t\tlen = MAX_RW_COUNT - total_len;\n\t\t\tiov[seg].iov_len = len;\n\t\t}\n\t\ttotal_len += len;\n\t}\n\n\tiov_iter_init(i, type, iov, nr_segs, total_len);\n\tif (iov == *iovp)\n\t\t*iovp = NULL;\n\telse\n\t\t*iovp = iov;\n\treturn total_len;\n}\n\n \nssize_t import_iovec(int type, const struct iovec __user *uvec,\n\t\t unsigned nr_segs, unsigned fast_segs,\n\t\t struct iovec **iovp, struct iov_iter *i)\n{\n\treturn __import_iovec(type, uvec, nr_segs, fast_segs, iovp, i,\n\t\t\t      in_compat_syscall());\n}\nEXPORT_SYMBOL(import_iovec);\n\nint import_single_range(int rw, void __user *buf, size_t len,\n\t\t struct iovec *iov, struct iov_iter *i)\n{\n\tif (len > MAX_RW_COUNT)\n\t\tlen = MAX_RW_COUNT;\n\tif (unlikely(!access_ok(buf, len)))\n\t\treturn -EFAULT;\n\n\tiov_iter_ubuf(i, rw, buf, len);\n\treturn 0;\n}\nEXPORT_SYMBOL(import_single_range);\n\nint import_ubuf(int rw, void __user *buf, size_t len, struct iov_iter *i)\n{\n\tif (len > MAX_RW_COUNT)\n\t\tlen = MAX_RW_COUNT;\n\tif (unlikely(!access_ok(buf, len)))\n\t\treturn -EFAULT;\n\n\tiov_iter_ubuf(i, rw, buf, len);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(import_ubuf);\n\n \nvoid iov_iter_restore(struct iov_iter *i, struct iov_iter_state *state)\n{\n\tif (WARN_ON_ONCE(!iov_iter_is_bvec(i) && !iter_is_iovec(i) &&\n\t\t\t !iter_is_ubuf(i)) && !iov_iter_is_kvec(i))\n\t\treturn;\n\ti->iov_offset = state->iov_offset;\n\ti->count = state->count;\n\tif (iter_is_ubuf(i))\n\t\treturn;\n\t \n\tBUILD_BUG_ON(sizeof(struct iovec) != sizeof(struct kvec));\n\tif (iov_iter_is_bvec(i))\n\t\ti->bvec -= state->nr_segs - i->nr_segs;\n\telse\n\t\ti->__iov -= state->nr_segs - i->nr_segs;\n\ti->nr_segs = state->nr_segs;\n}\n\n \nstatic ssize_t iov_iter_extract_xarray_pages(struct iov_iter *i,\n\t\t\t\t\t     struct page ***pages, size_t maxsize,\n\t\t\t\t\t     unsigned int maxpages,\n\t\t\t\t\t     iov_iter_extraction_t extraction_flags,\n\t\t\t\t\t     size_t *offset0)\n{\n\tstruct page *page, **p;\n\tunsigned int nr = 0, offset;\n\tloff_t pos = i->xarray_start + i->iov_offset;\n\tpgoff_t index = pos >> PAGE_SHIFT;\n\tXA_STATE(xas, i->xarray, index);\n\n\toffset = pos & ~PAGE_MASK;\n\t*offset0 = offset;\n\n\tmaxpages = want_pages_array(pages, maxsize, offset, maxpages);\n\tif (!maxpages)\n\t\treturn -ENOMEM;\n\tp = *pages;\n\n\trcu_read_lock();\n\tfor (page = xas_load(&xas); page; page = xas_next(&xas)) {\n\t\tif (xas_retry(&xas, page))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (unlikely(page != xas_reload(&xas))) {\n\t\t\txas_reset(&xas);\n\t\t\tcontinue;\n\t\t}\n\n\t\tp[nr++] = find_subpage(page, xas.xa_index);\n\t\tif (nr == maxpages)\n\t\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\tmaxsize = min_t(size_t, nr * PAGE_SIZE - offset, maxsize);\n\tiov_iter_advance(i, maxsize);\n\treturn maxsize;\n}\n\n \nstatic ssize_t iov_iter_extract_bvec_pages(struct iov_iter *i,\n\t\t\t\t\t   struct page ***pages, size_t maxsize,\n\t\t\t\t\t   unsigned int maxpages,\n\t\t\t\t\t   iov_iter_extraction_t extraction_flags,\n\t\t\t\t\t   size_t *offset0)\n{\n\tstruct page **p, *page;\n\tsize_t skip = i->iov_offset, offset, size;\n\tint k;\n\n\tfor (;;) {\n\t\tif (i->nr_segs == 0)\n\t\t\treturn 0;\n\t\tsize = min(maxsize, i->bvec->bv_len - skip);\n\t\tif (size)\n\t\t\tbreak;\n\t\ti->iov_offset = 0;\n\t\ti->nr_segs--;\n\t\ti->bvec++;\n\t\tskip = 0;\n\t}\n\n\tskip += i->bvec->bv_offset;\n\tpage = i->bvec->bv_page + skip / PAGE_SIZE;\n\toffset = skip % PAGE_SIZE;\n\t*offset0 = offset;\n\n\tmaxpages = want_pages_array(pages, size, offset, maxpages);\n\tif (!maxpages)\n\t\treturn -ENOMEM;\n\tp = *pages;\n\tfor (k = 0; k < maxpages; k++)\n\t\tp[k] = page + k;\n\n\tsize = min_t(size_t, size, maxpages * PAGE_SIZE - offset);\n\tiov_iter_advance(i, size);\n\treturn size;\n}\n\n \nstatic ssize_t iov_iter_extract_kvec_pages(struct iov_iter *i,\n\t\t\t\t\t   struct page ***pages, size_t maxsize,\n\t\t\t\t\t   unsigned int maxpages,\n\t\t\t\t\t   iov_iter_extraction_t extraction_flags,\n\t\t\t\t\t   size_t *offset0)\n{\n\tstruct page **p, *page;\n\tconst void *kaddr;\n\tsize_t skip = i->iov_offset, offset, len, size;\n\tint k;\n\n\tfor (;;) {\n\t\tif (i->nr_segs == 0)\n\t\t\treturn 0;\n\t\tsize = min(maxsize, i->kvec->iov_len - skip);\n\t\tif (size)\n\t\t\tbreak;\n\t\ti->iov_offset = 0;\n\t\ti->nr_segs--;\n\t\ti->kvec++;\n\t\tskip = 0;\n\t}\n\n\tkaddr = i->kvec->iov_base + skip;\n\toffset = (unsigned long)kaddr & ~PAGE_MASK;\n\t*offset0 = offset;\n\n\tmaxpages = want_pages_array(pages, size, offset, maxpages);\n\tif (!maxpages)\n\t\treturn -ENOMEM;\n\tp = *pages;\n\n\tkaddr -= offset;\n\tlen = offset + size;\n\tfor (k = 0; k < maxpages; k++) {\n\t\tsize_t seg = min_t(size_t, len, PAGE_SIZE);\n\n\t\tif (is_vmalloc_or_module_addr(kaddr))\n\t\t\tpage = vmalloc_to_page(kaddr);\n\t\telse\n\t\t\tpage = virt_to_page(kaddr);\n\n\t\tp[k] = page;\n\t\tlen -= seg;\n\t\tkaddr += PAGE_SIZE;\n\t}\n\n\tsize = min_t(size_t, size, maxpages * PAGE_SIZE - offset);\n\tiov_iter_advance(i, size);\n\treturn size;\n}\n\n \nstatic ssize_t iov_iter_extract_user_pages(struct iov_iter *i,\n\t\t\t\t\t   struct page ***pages,\n\t\t\t\t\t   size_t maxsize,\n\t\t\t\t\t   unsigned int maxpages,\n\t\t\t\t\t   iov_iter_extraction_t extraction_flags,\n\t\t\t\t\t   size_t *offset0)\n{\n\tunsigned long addr;\n\tunsigned int gup_flags = 0;\n\tsize_t offset;\n\tint res;\n\n\tif (i->data_source == ITER_DEST)\n\t\tgup_flags |= FOLL_WRITE;\n\tif (extraction_flags & ITER_ALLOW_P2PDMA)\n\t\tgup_flags |= FOLL_PCI_P2PDMA;\n\tif (i->nofault)\n\t\tgup_flags |= FOLL_NOFAULT;\n\n\taddr = first_iovec_segment(i, &maxsize);\n\t*offset0 = offset = addr % PAGE_SIZE;\n\taddr &= PAGE_MASK;\n\tmaxpages = want_pages_array(pages, maxsize, offset, maxpages);\n\tif (!maxpages)\n\t\treturn -ENOMEM;\n\tres = pin_user_pages_fast(addr, maxpages, gup_flags, *pages);\n\tif (unlikely(res <= 0))\n\t\treturn res;\n\tmaxsize = min_t(size_t, maxsize, res * PAGE_SIZE - offset);\n\tiov_iter_advance(i, maxsize);\n\treturn maxsize;\n}\n\n \nssize_t iov_iter_extract_pages(struct iov_iter *i,\n\t\t\t       struct page ***pages,\n\t\t\t       size_t maxsize,\n\t\t\t       unsigned int maxpages,\n\t\t\t       iov_iter_extraction_t extraction_flags,\n\t\t\t       size_t *offset0)\n{\n\tmaxsize = min_t(size_t, min_t(size_t, maxsize, i->count), MAX_RW_COUNT);\n\tif (!maxsize)\n\t\treturn 0;\n\n\tif (likely(user_backed_iter(i)))\n\t\treturn iov_iter_extract_user_pages(i, pages, maxsize,\n\t\t\t\t\t\t   maxpages, extraction_flags,\n\t\t\t\t\t\t   offset0);\n\tif (iov_iter_is_kvec(i))\n\t\treturn iov_iter_extract_kvec_pages(i, pages, maxsize,\n\t\t\t\t\t\t   maxpages, extraction_flags,\n\t\t\t\t\t\t   offset0);\n\tif (iov_iter_is_bvec(i))\n\t\treturn iov_iter_extract_bvec_pages(i, pages, maxsize,\n\t\t\t\t\t\t   maxpages, extraction_flags,\n\t\t\t\t\t\t   offset0);\n\tif (iov_iter_is_xarray(i))\n\t\treturn iov_iter_extract_xarray_pages(i, pages, maxsize,\n\t\t\t\t\t\t     maxpages, extraction_flags,\n\t\t\t\t\t\t     offset0);\n\treturn -EFAULT;\n}\nEXPORT_SYMBOL_GPL(iov_iter_extract_pages);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}