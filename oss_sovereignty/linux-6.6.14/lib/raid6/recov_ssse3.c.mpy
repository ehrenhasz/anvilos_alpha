{
  "module_name": "recov_ssse3.c",
  "hash_id": "e4d3be053d9fb00d154d5b850a343f5a5e6ef500e6ef1009ca6f6f761d554134",
  "original_prompt": "Ingested from linux-6.6.14/lib/raid6/recov_ssse3.c",
  "human_readable_source": "\n \n\n#include <linux/raid/pq.h>\n#include \"x86.h\"\n\nstatic int raid6_has_ssse3(void)\n{\n\treturn boot_cpu_has(X86_FEATURE_XMM) &&\n\t\tboot_cpu_has(X86_FEATURE_XMM2) &&\n\t\tboot_cpu_has(X86_FEATURE_SSSE3);\n}\n\nstatic void raid6_2data_recov_ssse3(int disks, size_t bytes, int faila,\n\t\tint failb, void **ptrs)\n{\n\tu8 *p, *q, *dp, *dq;\n\tconst u8 *pbmul;\t \n\tconst u8 *qmul;\t\t \n\tstatic const u8 __aligned(16) x0f[16] = {\n\t\t 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f,\n\t\t 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f};\n\n\tp = (u8 *)ptrs[disks-2];\n\tq = (u8 *)ptrs[disks-1];\n\n\t \n\tdp = (u8 *)ptrs[faila];\n\tptrs[faila] = (void *)raid6_empty_zero_page;\n\tptrs[disks-2] = dp;\n\tdq = (u8 *)ptrs[failb];\n\tptrs[failb] = (void *)raid6_empty_zero_page;\n\tptrs[disks-1] = dq;\n\n\traid6_call.gen_syndrome(disks, bytes, ptrs);\n\n\t \n\tptrs[faila]   = dp;\n\tptrs[failb]   = dq;\n\tptrs[disks-2] = p;\n\tptrs[disks-1] = q;\n\n\t \n\tpbmul = raid6_vgfmul[raid6_gfexi[failb-faila]];\n\tqmul  = raid6_vgfmul[raid6_gfinv[raid6_gfexp[faila] ^\n\t\traid6_gfexp[failb]]];\n\n\tkernel_fpu_begin();\n\n\tasm volatile(\"movdqa %0,%%xmm7\" : : \"m\" (x0f[0]));\n\n#ifdef CONFIG_X86_64\n\tasm volatile(\"movdqa %0,%%xmm6\" : : \"m\" (qmul[0]));\n\tasm volatile(\"movdqa %0,%%xmm14\" : : \"m\" (pbmul[0]));\n\tasm volatile(\"movdqa %0,%%xmm15\" : : \"m\" (pbmul[16]));\n#endif\n\n\t \n\twhile (bytes) {\n#ifdef CONFIG_X86_64\n\t\t \n\n\t\tasm volatile(\"movdqa %0,%%xmm1\" : : \"m\" (q[0]));\n\t\tasm volatile(\"movdqa %0,%%xmm9\" : : \"m\" (q[16]));\n\t\tasm volatile(\"movdqa %0,%%xmm0\" : : \"m\" (p[0]));\n\t\tasm volatile(\"movdqa %0,%%xmm8\" : : \"m\" (p[16]));\n\t\tasm volatile(\"pxor   %0,%%xmm1\" : : \"m\" (dq[0]));\n\t\tasm volatile(\"pxor   %0,%%xmm9\" : : \"m\" (dq[16]));\n\t\tasm volatile(\"pxor   %0,%%xmm0\" : : \"m\" (dp[0]));\n\t\tasm volatile(\"pxor   %0,%%xmm8\" : : \"m\" (dp[16]));\n\n\t\t \n\n\t\tasm volatile(\"movdqa %xmm6,%xmm4\");\n\t\tasm volatile(\"movdqa %0,%%xmm5\" : : \"m\" (qmul[16]));\n\t\tasm volatile(\"movdqa %xmm6,%xmm12\");\n\t\tasm volatile(\"movdqa %xmm5,%xmm13\");\n\t\tasm volatile(\"movdqa %xmm1,%xmm3\");\n\t\tasm volatile(\"movdqa %xmm9,%xmm11\");\n\t\tasm volatile(\"movdqa %xmm0,%xmm2\");  \n\t\tasm volatile(\"movdqa %xmm8,%xmm10\");\n\t\tasm volatile(\"psraw  $4,%xmm1\");\n\t\tasm volatile(\"psraw  $4,%xmm9\");\n\t\tasm volatile(\"pand   %xmm7,%xmm3\");\n\t\tasm volatile(\"pand   %xmm7,%xmm11\");\n\t\tasm volatile(\"pand   %xmm7,%xmm1\");\n\t\tasm volatile(\"pand   %xmm7,%xmm9\");\n\t\tasm volatile(\"pshufb %xmm3,%xmm4\");\n\t\tasm volatile(\"pshufb %xmm11,%xmm12\");\n\t\tasm volatile(\"pshufb %xmm1,%xmm5\");\n\t\tasm volatile(\"pshufb %xmm9,%xmm13\");\n\t\tasm volatile(\"pxor   %xmm4,%xmm5\");\n\t\tasm volatile(\"pxor   %xmm12,%xmm13\");\n\n\t\t \n\n\t\tasm volatile(\"movdqa %xmm14,%xmm4\");\n\t\tasm volatile(\"movdqa %xmm15,%xmm1\");\n\t\tasm volatile(\"movdqa %xmm14,%xmm12\");\n\t\tasm volatile(\"movdqa %xmm15,%xmm9\");\n\t\tasm volatile(\"movdqa %xmm2,%xmm3\");\n\t\tasm volatile(\"movdqa %xmm10,%xmm11\");\n\t\tasm volatile(\"psraw  $4,%xmm2\");\n\t\tasm volatile(\"psraw  $4,%xmm10\");\n\t\tasm volatile(\"pand   %xmm7,%xmm3\");\n\t\tasm volatile(\"pand   %xmm7,%xmm11\");\n\t\tasm volatile(\"pand   %xmm7,%xmm2\");\n\t\tasm volatile(\"pand   %xmm7,%xmm10\");\n\t\tasm volatile(\"pshufb %xmm3,%xmm4\");\n\t\tasm volatile(\"pshufb %xmm11,%xmm12\");\n\t\tasm volatile(\"pshufb %xmm2,%xmm1\");\n\t\tasm volatile(\"pshufb %xmm10,%xmm9\");\n\t\tasm volatile(\"pxor   %xmm4,%xmm1\");\n\t\tasm volatile(\"pxor   %xmm12,%xmm9\");\n\n\t\t \n\t\tasm volatile(\"pxor   %xmm5,%xmm1\");\n\t\tasm volatile(\"pxor   %xmm13,%xmm9\");\n\t\t \n\t\tasm volatile(\"movdqa %%xmm1,%0\" : \"=m\" (dq[0]));\n\t\tasm volatile(\"movdqa %%xmm9,%0\" : \"=m\" (dq[16]));\n\n\t\tasm volatile(\"pxor   %xmm1,%xmm0\");\n\t\tasm volatile(\"pxor   %xmm9,%xmm8\");\n\t\tasm volatile(\"movdqa %%xmm0,%0\" : \"=m\" (dp[0]));\n\t\tasm volatile(\"movdqa %%xmm8,%0\" : \"=m\" (dp[16]));\n\n\t\tbytes -= 32;\n\t\tp += 32;\n\t\tq += 32;\n\t\tdp += 32;\n\t\tdq += 32;\n#else\n\t\tasm volatile(\"movdqa %0,%%xmm1\" : : \"m\" (*q));\n\t\tasm volatile(\"movdqa %0,%%xmm0\" : : \"m\" (*p));\n\t\tasm volatile(\"pxor   %0,%%xmm1\" : : \"m\" (*dq));\n\t\tasm volatile(\"pxor   %0,%%xmm0\" : : \"m\" (*dp));\n\n\t\t \n\t\tasm volatile(\"movdqa %0,%%xmm4\" : : \"m\" (qmul[0]));\n\t\tasm volatile(\"movdqa %0,%%xmm5\" : : \"m\" (qmul[16]));\n\n\t\tasm volatile(\"movdqa %xmm1,%xmm3\");\n\t\tasm volatile(\"psraw  $4,%xmm1\");\n\t\tasm volatile(\"pand   %xmm7,%xmm3\");\n\t\tasm volatile(\"pand   %xmm7,%xmm1\");\n\t\tasm volatile(\"pshufb %xmm3,%xmm4\");\n\t\tasm volatile(\"pshufb %xmm1,%xmm5\");\n\t\tasm volatile(\"pxor   %xmm4,%xmm5\");\n\n\t\tasm volatile(\"movdqa %xmm0,%xmm2\");  \n\n\t\t \n\n\t\tasm volatile(\"movdqa %0,%%xmm4\" : : \"m\" (pbmul[0]));\n\t\tasm volatile(\"movdqa %0,%%xmm1\" : : \"m\" (pbmul[16]));\n\t\tasm volatile(\"movdqa %xmm2,%xmm3\");\n\t\tasm volatile(\"psraw  $4,%xmm2\");\n\t\tasm volatile(\"pand   %xmm7,%xmm3\");\n\t\tasm volatile(\"pand   %xmm7,%xmm2\");\n\t\tasm volatile(\"pshufb %xmm3,%xmm4\");\n\t\tasm volatile(\"pshufb %xmm2,%xmm1\");\n\t\tasm volatile(\"pxor   %xmm4,%xmm1\");\n\n\t\t \n\t\tasm volatile(\"pxor   %xmm5,%xmm1\");\n\t\t \n\t\tasm volatile(\"movdqa %%xmm1,%0\" : \"=m\" (*dq));\n\n\t\tasm volatile(\"pxor   %xmm1,%xmm0\");\n\t\tasm volatile(\"movdqa %%xmm0,%0\" : \"=m\" (*dp));\n\n\t\tbytes -= 16;\n\t\tp += 16;\n\t\tq += 16;\n\t\tdp += 16;\n\t\tdq += 16;\n#endif\n\t}\n\n\tkernel_fpu_end();\n}\n\n\nstatic void raid6_datap_recov_ssse3(int disks, size_t bytes, int faila,\n\t\tvoid **ptrs)\n{\n\tu8 *p, *q, *dq;\n\tconst u8 *qmul;\t\t \n\tstatic const u8 __aligned(16) x0f[16] = {\n\t\t 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f,\n\t\t 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f};\n\n\tp = (u8 *)ptrs[disks-2];\n\tq = (u8 *)ptrs[disks-1];\n\n\t \n\tdq = (u8 *)ptrs[faila];\n\tptrs[faila] = (void *)raid6_empty_zero_page;\n\tptrs[disks-1] = dq;\n\n\traid6_call.gen_syndrome(disks, bytes, ptrs);\n\n\t \n\tptrs[faila]   = dq;\n\tptrs[disks-1] = q;\n\n\t \n\tqmul  = raid6_vgfmul[raid6_gfinv[raid6_gfexp[faila]]];\n\n\tkernel_fpu_begin();\n\n\tasm volatile(\"movdqa %0, %%xmm7\" : : \"m\" (x0f[0]));\n\n\twhile (bytes) {\n#ifdef CONFIG_X86_64\n\t\tasm volatile(\"movdqa %0, %%xmm3\" : : \"m\" (dq[0]));\n\t\tasm volatile(\"movdqa %0, %%xmm4\" : : \"m\" (dq[16]));\n\t\tasm volatile(\"pxor %0, %%xmm3\" : : \"m\" (q[0]));\n\t\tasm volatile(\"movdqa %0, %%xmm0\" : : \"m\" (qmul[0]));\n\n\t\t \n\n\t\tasm volatile(\"pxor %0, %%xmm4\" : : \"m\" (q[16]));\n\t\tasm volatile(\"movdqa %0, %%xmm1\" : : \"m\" (qmul[16]));\n\n\t\t \n\n\t\tasm volatile(\"movdqa %xmm3, %xmm6\");\n\t\tasm volatile(\"movdqa %xmm4, %xmm8\");\n\n\t\t \n\n\t\tasm volatile(\"psraw $4, %xmm3\");\n\t\tasm volatile(\"pand %xmm7, %xmm6\");\n\t\tasm volatile(\"pand %xmm7, %xmm3\");\n\t\tasm volatile(\"pshufb %xmm6, %xmm0\");\n\t\tasm volatile(\"pshufb %xmm3, %xmm1\");\n\t\tasm volatile(\"movdqa %0, %%xmm10\" : : \"m\" (qmul[0]));\n\t\tasm volatile(\"pxor %xmm0, %xmm1\");\n\t\tasm volatile(\"movdqa %0, %%xmm11\" : : \"m\" (qmul[16]));\n\n\t\t \n\n\t\tasm volatile(\"psraw $4, %xmm4\");\n\t\tasm volatile(\"pand %xmm7, %xmm8\");\n\t\tasm volatile(\"pand %xmm7, %xmm4\");\n\t\tasm volatile(\"pshufb %xmm8, %xmm10\");\n\t\tasm volatile(\"pshufb %xmm4, %xmm11\");\n\t\tasm volatile(\"movdqa %0, %%xmm2\" : : \"m\" (p[0]));\n\t\tasm volatile(\"pxor %xmm10, %xmm11\");\n\t\tasm volatile(\"movdqa %0, %%xmm12\" : : \"m\" (p[16]));\n\n\t\t \n\n\t\tasm volatile(\"pxor %xmm1, %xmm2\");\n\n\t\t \n\n\t\tasm volatile(\"pxor %xmm11, %xmm12\");\n\n\t\t \n\n\t\tasm volatile(\"movdqa %%xmm1, %0\" : \"=m\" (dq[0]));\n\t\tasm volatile(\"movdqa %%xmm11, %0\" : \"=m\" (dq[16]));\n\n\t\tasm volatile(\"movdqa %%xmm2, %0\" : \"=m\" (p[0]));\n\t\tasm volatile(\"movdqa %%xmm12, %0\" : \"=m\" (p[16]));\n\n\t\tbytes -= 32;\n\t\tp += 32;\n\t\tq += 32;\n\t\tdq += 32;\n\n#else\n\t\tasm volatile(\"movdqa %0, %%xmm3\" : : \"m\" (dq[0]));\n\t\tasm volatile(\"movdqa %0, %%xmm0\" : : \"m\" (qmul[0]));\n\t\tasm volatile(\"pxor %0, %%xmm3\" : : \"m\" (q[0]));\n\t\tasm volatile(\"movdqa %0, %%xmm1\" : : \"m\" (qmul[16]));\n\n\t\t \n\n\t\tasm volatile(\"movdqa %xmm3, %xmm6\");\n\t\tasm volatile(\"movdqa %0, %%xmm2\" : : \"m\" (p[0]));\n\t\tasm volatile(\"psraw $4, %xmm3\");\n\t\tasm volatile(\"pand %xmm7, %xmm6\");\n\t\tasm volatile(\"pand %xmm7, %xmm3\");\n\t\tasm volatile(\"pshufb %xmm6, %xmm0\");\n\t\tasm volatile(\"pshufb %xmm3, %xmm1\");\n\t\tasm volatile(\"pxor %xmm0, %xmm1\");\n\n\t\t \n\n\t\tasm volatile(\"pxor %xmm1, %xmm2\");\n\n\t\t \n\n\t\tasm volatile(\"movdqa %%xmm1, %0\" : \"=m\" (dq[0]));\n\t\tasm volatile(\"movdqa %%xmm2, %0\" : \"=m\" (p[0]));\n\n\t\tbytes -= 16;\n\t\tp += 16;\n\t\tq += 16;\n\t\tdq += 16;\n#endif\n\t}\n\n\tkernel_fpu_end();\n}\n\nconst struct raid6_recov_calls raid6_recov_ssse3 = {\n\t.data2 = raid6_2data_recov_ssse3,\n\t.datap = raid6_datap_recov_ssse3,\n\t.valid = raid6_has_ssse3,\n#ifdef CONFIG_X86_64\n\t.name = \"ssse3x2\",\n#else\n\t.name = \"ssse3x1\",\n#endif\n\t.priority = 1,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}