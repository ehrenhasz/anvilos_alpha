{
  "module_name": "avx2.c",
  "hash_id": "a4f8d41d84deb54f3e669cb32e387741d69fb54a7ba137a524a208f04b4402e0",
  "original_prompt": "Ingested from linux-6.6.14/lib/raid6/avx2.c",
  "human_readable_source": "\n \n\n \n\n#include <linux/raid/pq.h>\n#include \"x86.h\"\n\nstatic const struct raid6_avx2_constants {\n\tu64 x1d[4];\n} raid6_avx2_constants __aligned(32) = {\n\t{ 0x1d1d1d1d1d1d1d1dULL, 0x1d1d1d1d1d1d1d1dULL,\n\t  0x1d1d1d1d1d1d1d1dULL, 0x1d1d1d1d1d1d1d1dULL,},\n};\n\nstatic int raid6_have_avx2(void)\n{\n\treturn boot_cpu_has(X86_FEATURE_AVX2) && boot_cpu_has(X86_FEATURE_AVX);\n}\n\n \nstatic void raid6_avx21_gen_syndrome(int disks, size_t bytes, void **ptrs)\n{\n\tu8 **dptr = (u8 **)ptrs;\n\tu8 *p, *q;\n\tint d, z, z0;\n\n\tz0 = disks - 3;\t\t \n\tp = dptr[z0+1];\t\t \n\tq = dptr[z0+2];\t\t \n\n\tkernel_fpu_begin();\n\n\tasm volatile(\"vmovdqa %0,%%ymm0\" : : \"m\" (raid6_avx2_constants.x1d[0]));\n\tasm volatile(\"vpxor %ymm3,%ymm3,%ymm3\");\t \n\n\tfor (d = 0; d < bytes; d += 32) {\n\t\tasm volatile(\"prefetchnta %0\" : : \"m\" (dptr[z0][d]));\n\t\tasm volatile(\"vmovdqa %0,%%ymm2\" : : \"m\" (dptr[z0][d])); \n\t\tasm volatile(\"prefetchnta %0\" : : \"m\" (dptr[z0-1][d]));\n\t\tasm volatile(\"vmovdqa %ymm2,%ymm4\"); \n\t\tasm volatile(\"vmovdqa %0,%%ymm6\" : : \"m\" (dptr[z0-1][d]));\n\t\tfor (z = z0-2; z >= 0; z--) {\n\t\t\tasm volatile(\"prefetchnta %0\" : : \"m\" (dptr[z][d]));\n\t\t\tasm volatile(\"vpcmpgtb %ymm4,%ymm3,%ymm5\");\n\t\t\tasm volatile(\"vpaddb %ymm4,%ymm4,%ymm4\");\n\t\t\tasm volatile(\"vpand %ymm0,%ymm5,%ymm5\");\n\t\t\tasm volatile(\"vpxor %ymm5,%ymm4,%ymm4\");\n\t\t\tasm volatile(\"vpxor %ymm6,%ymm2,%ymm2\");\n\t\t\tasm volatile(\"vpxor %ymm6,%ymm4,%ymm4\");\n\t\t\tasm volatile(\"vmovdqa %0,%%ymm6\" : : \"m\" (dptr[z][d]));\n\t\t}\n\t\tasm volatile(\"vpcmpgtb %ymm4,%ymm3,%ymm5\");\n\t\tasm volatile(\"vpaddb %ymm4,%ymm4,%ymm4\");\n\t\tasm volatile(\"vpand %ymm0,%ymm5,%ymm5\");\n\t\tasm volatile(\"vpxor %ymm5,%ymm4,%ymm4\");\n\t\tasm volatile(\"vpxor %ymm6,%ymm2,%ymm2\");\n\t\tasm volatile(\"vpxor %ymm6,%ymm4,%ymm4\");\n\n\t\tasm volatile(\"vmovntdq %%ymm2,%0\" : \"=m\" (p[d]));\n\t\tasm volatile(\"vpxor %ymm2,%ymm2,%ymm2\");\n\t\tasm volatile(\"vmovntdq %%ymm4,%0\" : \"=m\" (q[d]));\n\t\tasm volatile(\"vpxor %ymm4,%ymm4,%ymm4\");\n\t}\n\n\tasm volatile(\"sfence\" : : : \"memory\");\n\tkernel_fpu_end();\n}\n\nstatic void raid6_avx21_xor_syndrome(int disks, int start, int stop,\n\t\t\t\t     size_t bytes, void **ptrs)\n{\n\tu8 **dptr = (u8 **)ptrs;\n\tu8 *p, *q;\n\tint d, z, z0;\n\n\tz0 = stop;\t\t \n\tp = dptr[disks-2];\t \n\tq = dptr[disks-1];\t \n\n\tkernel_fpu_begin();\n\n\tasm volatile(\"vmovdqa %0,%%ymm0\" : : \"m\" (raid6_avx2_constants.x1d[0]));\n\n\tfor (d = 0 ; d < bytes ; d += 32) {\n\t\tasm volatile(\"vmovdqa %0,%%ymm4\" :: \"m\" (dptr[z0][d]));\n\t\tasm volatile(\"vmovdqa %0,%%ymm2\" : : \"m\" (p[d]));\n\t\tasm volatile(\"vpxor %ymm4,%ymm2,%ymm2\");\n\t\t \n\t\tfor (z = z0-1 ; z >= start ; z--) {\n\t\t\tasm volatile(\"vpxor %ymm5,%ymm5,%ymm5\");\n\t\t\tasm volatile(\"vpcmpgtb %ymm4,%ymm5,%ymm5\");\n\t\t\tasm volatile(\"vpaddb %ymm4,%ymm4,%ymm4\");\n\t\t\tasm volatile(\"vpand %ymm0,%ymm5,%ymm5\");\n\t\t\tasm volatile(\"vpxor %ymm5,%ymm4,%ymm4\");\n\t\t\tasm volatile(\"vmovdqa %0,%%ymm5\" :: \"m\" (dptr[z][d]));\n\t\t\tasm volatile(\"vpxor %ymm5,%ymm2,%ymm2\");\n\t\t\tasm volatile(\"vpxor %ymm5,%ymm4,%ymm4\");\n\t\t}\n\t\t \n\t\tfor (z = start-1 ; z >= 0 ; z--) {\n\t\t\tasm volatile(\"vpxor %ymm5,%ymm5,%ymm5\");\n\t\t\tasm volatile(\"vpcmpgtb %ymm4,%ymm5,%ymm5\");\n\t\t\tasm volatile(\"vpaddb %ymm4,%ymm4,%ymm4\");\n\t\t\tasm volatile(\"vpand %ymm0,%ymm5,%ymm5\");\n\t\t\tasm volatile(\"vpxor %ymm5,%ymm4,%ymm4\");\n\t\t}\n\t\tasm volatile(\"vpxor %0,%%ymm4,%%ymm4\" : : \"m\" (q[d]));\n\t\t \n\t\tasm volatile(\"vmovdqa %%ymm4,%0\" : \"=m\" (q[d]));\n\t\tasm volatile(\"vmovdqa %%ymm2,%0\" : \"=m\" (p[d]));\n\t}\n\n\tasm volatile(\"sfence\" : : : \"memory\");\n\tkernel_fpu_end();\n}\n\nconst struct raid6_calls raid6_avx2x1 = {\n\traid6_avx21_gen_syndrome,\n\traid6_avx21_xor_syndrome,\n\traid6_have_avx2,\n\t\"avx2x1\",\n\t.priority = 2\t\t \n};\n\n \nstatic void raid6_avx22_gen_syndrome(int disks, size_t bytes, void **ptrs)\n{\n\tu8 **dptr = (u8 **)ptrs;\n\tu8 *p, *q;\n\tint d, z, z0;\n\n\tz0 = disks - 3;\t\t \n\tp = dptr[z0+1];\t\t \n\tq = dptr[z0+2];\t\t \n\n\tkernel_fpu_begin();\n\n\tasm volatile(\"vmovdqa %0,%%ymm0\" : : \"m\" (raid6_avx2_constants.x1d[0]));\n\tasm volatile(\"vpxor %ymm1,%ymm1,%ymm1\");  \n\n\t \n\tfor (d = 0; d < bytes; d += 64) {\n\t\tasm volatile(\"prefetchnta %0\" : : \"m\" (dptr[z0][d]));\n\t\tasm volatile(\"prefetchnta %0\" : : \"m\" (dptr[z0][d+32]));\n\t\tasm volatile(\"vmovdqa %0,%%ymm2\" : : \"m\" (dptr[z0][d])); \n\t\tasm volatile(\"vmovdqa %0,%%ymm3\" : : \"m\" (dptr[z0][d+32])); \n\t\tasm volatile(\"vmovdqa %ymm2,%ymm4\");  \n\t\tasm volatile(\"vmovdqa %ymm3,%ymm6\");  \n\t\tfor (z = z0-1; z >= 0; z--) {\n\t\t\tasm volatile(\"prefetchnta %0\" : : \"m\" (dptr[z][d]));\n\t\t\tasm volatile(\"prefetchnta %0\" : : \"m\" (dptr[z][d+32]));\n\t\t\tasm volatile(\"vpcmpgtb %ymm4,%ymm1,%ymm5\");\n\t\t\tasm volatile(\"vpcmpgtb %ymm6,%ymm1,%ymm7\");\n\t\t\tasm volatile(\"vpaddb %ymm4,%ymm4,%ymm4\");\n\t\t\tasm volatile(\"vpaddb %ymm6,%ymm6,%ymm6\");\n\t\t\tasm volatile(\"vpand %ymm0,%ymm5,%ymm5\");\n\t\t\tasm volatile(\"vpand %ymm0,%ymm7,%ymm7\");\n\t\t\tasm volatile(\"vpxor %ymm5,%ymm4,%ymm4\");\n\t\t\tasm volatile(\"vpxor %ymm7,%ymm6,%ymm6\");\n\t\t\tasm volatile(\"vmovdqa %0,%%ymm5\" : : \"m\" (dptr[z][d]));\n\t\t\tasm volatile(\"vmovdqa %0,%%ymm7\" : : \"m\" (dptr[z][d+32]));\n\t\t\tasm volatile(\"vpxor %ymm5,%ymm2,%ymm2\");\n\t\t\tasm volatile(\"vpxor %ymm7,%ymm3,%ymm3\");\n\t\t\tasm volatile(\"vpxor %ymm5,%ymm4,%ymm4\");\n\t\t\tasm volatile(\"vpxor %ymm7,%ymm6,%ymm6\");\n\t\t}\n\t\tasm volatile(\"vmovntdq %%ymm2,%0\" : \"=m\" (p[d]));\n\t\tasm volatile(\"vmovntdq %%ymm3,%0\" : \"=m\" (p[d+32]));\n\t\tasm volatile(\"vmovntdq %%ymm4,%0\" : \"=m\" (q[d]));\n\t\tasm volatile(\"vmovntdq %%ymm6,%0\" : \"=m\" (q[d+32]));\n\t}\n\n\tasm volatile(\"sfence\" : : : \"memory\");\n\tkernel_fpu_end();\n}\n\nstatic void raid6_avx22_xor_syndrome(int disks, int start, int stop,\n\t\t\t\t     size_t bytes, void **ptrs)\n{\n\tu8 **dptr = (u8 **)ptrs;\n\tu8 *p, *q;\n\tint d, z, z0;\n\n\tz0 = stop;\t\t \n\tp = dptr[disks-2];\t \n\tq = dptr[disks-1];\t \n\n\tkernel_fpu_begin();\n\n\tasm volatile(\"vmovdqa %0,%%ymm0\" : : \"m\" (raid6_avx2_constants.x1d[0]));\n\n\tfor (d = 0 ; d < bytes ; d += 64) {\n\t\tasm volatile(\"vmovdqa %0,%%ymm4\" :: \"m\" (dptr[z0][d]));\n\t\tasm volatile(\"vmovdqa %0,%%ymm6\" :: \"m\" (dptr[z0][d+32]));\n\t\tasm volatile(\"vmovdqa %0,%%ymm2\" : : \"m\" (p[d]));\n\t\tasm volatile(\"vmovdqa %0,%%ymm3\" : : \"m\" (p[d+32]));\n\t\tasm volatile(\"vpxor %ymm4,%ymm2,%ymm2\");\n\t\tasm volatile(\"vpxor %ymm6,%ymm3,%ymm3\");\n\t\t \n\t\tfor (z = z0-1 ; z >= start ; z--) {\n\t\t\tasm volatile(\"vpxor %ymm5,%ymm5,%ymm5\");\n\t\t\tasm volatile(\"vpxor %ymm7,%ymm7,%ymm7\");\n\t\t\tasm volatile(\"vpcmpgtb %ymm4,%ymm5,%ymm5\");\n\t\t\tasm volatile(\"vpcmpgtb %ymm6,%ymm7,%ymm7\");\n\t\t\tasm volatile(\"vpaddb %ymm4,%ymm4,%ymm4\");\n\t\t\tasm volatile(\"vpaddb %ymm6,%ymm6,%ymm6\");\n\t\t\tasm volatile(\"vpand %ymm0,%ymm5,%ymm5\");\n\t\t\tasm volatile(\"vpand %ymm0,%ymm7,%ymm7\");\n\t\t\tasm volatile(\"vpxor %ymm5,%ymm4,%ymm4\");\n\t\t\tasm volatile(\"vpxor %ymm7,%ymm6,%ymm6\");\n\t\t\tasm volatile(\"vmovdqa %0,%%ymm5\" :: \"m\" (dptr[z][d]));\n\t\t\tasm volatile(\"vmovdqa %0,%%ymm7\"\n\t\t\t\t     :: \"m\" (dptr[z][d+32]));\n\t\t\tasm volatile(\"vpxor %ymm5,%ymm2,%ymm2\");\n\t\t\tasm volatile(\"vpxor %ymm7,%ymm3,%ymm3\");\n\t\t\tasm volatile(\"vpxor %ymm5,%ymm4,%ymm4\");\n\t\t\tasm volatile(\"vpxor %ymm7,%ymm6,%ymm6\");\n\t\t}\n\t\t \n\t\tfor (z = start-1 ; z >= 0 ; z--) {\n\t\t\tasm volatile(\"vpxor %ymm5,%ymm5,%ymm5\");\n\t\t\tasm volatile(\"vpxor %ymm7,%ymm7,%ymm7\");\n\t\t\tasm volatile(\"vpcmpgtb %ymm4,%ymm5,%ymm5\");\n\t\t\tasm volatile(\"vpcmpgtb %ymm6,%ymm7,%ymm7\");\n\t\t\tasm volatile(\"vpaddb %ymm4,%ymm4,%ymm4\");\n\t\t\tasm volatile(\"vpaddb %ymm6,%ymm6,%ymm6\");\n\t\t\tasm volatile(\"vpand %ymm0,%ymm5,%ymm5\");\n\t\t\tasm volatile(\"vpand %ymm0,%ymm7,%ymm7\");\n\t\t\tasm volatile(\"vpxor %ymm5,%ymm4,%ymm4\");\n\t\t\tasm volatile(\"vpxor %ymm7,%ymm6,%ymm6\");\n\t\t}\n\t\tasm volatile(\"vpxor %0,%%ymm4,%%ymm4\" : : \"m\" (q[d]));\n\t\tasm volatile(\"vpxor %0,%%ymm6,%%ymm6\" : : \"m\" (q[d+32]));\n\t\t \n\t\tasm volatile(\"vmovdqa %%ymm4,%0\" : \"=m\" (q[d]));\n\t\tasm volatile(\"vmovdqa %%ymm6,%0\" : \"=m\" (q[d+32]));\n\t\tasm volatile(\"vmovdqa %%ymm2,%0\" : \"=m\" (p[d]));\n\t\tasm volatile(\"vmovdqa %%ymm3,%0\" : \"=m\" (p[d+32]));\n\t}\n\n\tasm volatile(\"sfence\" : : : \"memory\");\n\tkernel_fpu_end();\n}\n\nconst struct raid6_calls raid6_avx2x2 = {\n\traid6_avx22_gen_syndrome,\n\traid6_avx22_xor_syndrome,\n\traid6_have_avx2,\n\t\"avx2x2\",\n\t.priority = 2\t\t \n};\n\n#ifdef CONFIG_X86_64\n\n \nstatic void raid6_avx24_gen_syndrome(int disks, size_t bytes, void **ptrs)\n{\n\tu8 **dptr = (u8 **)ptrs;\n\tu8 *p, *q;\n\tint d, z, z0;\n\n\tz0 = disks - 3;\t\t \n\tp = dptr[z0+1];\t\t \n\tq = dptr[z0+2];\t\t \n\n\tkernel_fpu_begin();\n\n\tasm volatile(\"vmovdqa %0,%%ymm0\" : : \"m\" (raid6_avx2_constants.x1d[0]));\n\tasm volatile(\"vpxor %ymm1,%ymm1,%ymm1\");\t \n\tasm volatile(\"vpxor %ymm2,%ymm2,%ymm2\");\t \n\tasm volatile(\"vpxor %ymm3,%ymm3,%ymm3\");\t \n\tasm volatile(\"vpxor %ymm4,%ymm4,%ymm4\");\t \n\tasm volatile(\"vpxor %ymm6,%ymm6,%ymm6\");\t \n\tasm volatile(\"vpxor %ymm10,%ymm10,%ymm10\");\t \n\tasm volatile(\"vpxor %ymm11,%ymm11,%ymm11\");\t \n\tasm volatile(\"vpxor %ymm12,%ymm12,%ymm12\");\t \n\tasm volatile(\"vpxor %ymm14,%ymm14,%ymm14\");\t \n\n\tfor (d = 0; d < bytes; d += 128) {\n\t\tfor (z = z0; z >= 0; z--) {\n\t\t\tasm volatile(\"prefetchnta %0\" : : \"m\" (dptr[z][d]));\n\t\t\tasm volatile(\"prefetchnta %0\" : : \"m\" (dptr[z][d+32]));\n\t\t\tasm volatile(\"prefetchnta %0\" : : \"m\" (dptr[z][d+64]));\n\t\t\tasm volatile(\"prefetchnta %0\" : : \"m\" (dptr[z][d+96]));\n\t\t\tasm volatile(\"vpcmpgtb %ymm4,%ymm1,%ymm5\");\n\t\t\tasm volatile(\"vpcmpgtb %ymm6,%ymm1,%ymm7\");\n\t\t\tasm volatile(\"vpcmpgtb %ymm12,%ymm1,%ymm13\");\n\t\t\tasm volatile(\"vpcmpgtb %ymm14,%ymm1,%ymm15\");\n\t\t\tasm volatile(\"vpaddb %ymm4,%ymm4,%ymm4\");\n\t\t\tasm volatile(\"vpaddb %ymm6,%ymm6,%ymm6\");\n\t\t\tasm volatile(\"vpaddb %ymm12,%ymm12,%ymm12\");\n\t\t\tasm volatile(\"vpaddb %ymm14,%ymm14,%ymm14\");\n\t\t\tasm volatile(\"vpand %ymm0,%ymm5,%ymm5\");\n\t\t\tasm volatile(\"vpand %ymm0,%ymm7,%ymm7\");\n\t\t\tasm volatile(\"vpand %ymm0,%ymm13,%ymm13\");\n\t\t\tasm volatile(\"vpand %ymm0,%ymm15,%ymm15\");\n\t\t\tasm volatile(\"vpxor %ymm5,%ymm4,%ymm4\");\n\t\t\tasm volatile(\"vpxor %ymm7,%ymm6,%ymm6\");\n\t\t\tasm volatile(\"vpxor %ymm13,%ymm12,%ymm12\");\n\t\t\tasm volatile(\"vpxor %ymm15,%ymm14,%ymm14\");\n\t\t\tasm volatile(\"vmovdqa %0,%%ymm5\" : : \"m\" (dptr[z][d]));\n\t\t\tasm volatile(\"vmovdqa %0,%%ymm7\" : : \"m\" (dptr[z][d+32]));\n\t\t\tasm volatile(\"vmovdqa %0,%%ymm13\" : : \"m\" (dptr[z][d+64]));\n\t\t\tasm volatile(\"vmovdqa %0,%%ymm15\" : : \"m\" (dptr[z][d+96]));\n\t\t\tasm volatile(\"vpxor %ymm5,%ymm2,%ymm2\");\n\t\t\tasm volatile(\"vpxor %ymm7,%ymm3,%ymm3\");\n\t\t\tasm volatile(\"vpxor %ymm13,%ymm10,%ymm10\");\n\t\t\tasm volatile(\"vpxor %ymm15,%ymm11,%ymm11\");\n\t\t\tasm volatile(\"vpxor %ymm5,%ymm4,%ymm4\");\n\t\t\tasm volatile(\"vpxor %ymm7,%ymm6,%ymm6\");\n\t\t\tasm volatile(\"vpxor %ymm13,%ymm12,%ymm12\");\n\t\t\tasm volatile(\"vpxor %ymm15,%ymm14,%ymm14\");\n\t\t}\n\t\tasm volatile(\"vmovntdq %%ymm2,%0\" : \"=m\" (p[d]));\n\t\tasm volatile(\"vpxor %ymm2,%ymm2,%ymm2\");\n\t\tasm volatile(\"vmovntdq %%ymm3,%0\" : \"=m\" (p[d+32]));\n\t\tasm volatile(\"vpxor %ymm3,%ymm3,%ymm3\");\n\t\tasm volatile(\"vmovntdq %%ymm10,%0\" : \"=m\" (p[d+64]));\n\t\tasm volatile(\"vpxor %ymm10,%ymm10,%ymm10\");\n\t\tasm volatile(\"vmovntdq %%ymm11,%0\" : \"=m\" (p[d+96]));\n\t\tasm volatile(\"vpxor %ymm11,%ymm11,%ymm11\");\n\t\tasm volatile(\"vmovntdq %%ymm4,%0\" : \"=m\" (q[d]));\n\t\tasm volatile(\"vpxor %ymm4,%ymm4,%ymm4\");\n\t\tasm volatile(\"vmovntdq %%ymm6,%0\" : \"=m\" (q[d+32]));\n\t\tasm volatile(\"vpxor %ymm6,%ymm6,%ymm6\");\n\t\tasm volatile(\"vmovntdq %%ymm12,%0\" : \"=m\" (q[d+64]));\n\t\tasm volatile(\"vpxor %ymm12,%ymm12,%ymm12\");\n\t\tasm volatile(\"vmovntdq %%ymm14,%0\" : \"=m\" (q[d+96]));\n\t\tasm volatile(\"vpxor %ymm14,%ymm14,%ymm14\");\n\t}\n\n\tasm volatile(\"sfence\" : : : \"memory\");\n\tkernel_fpu_end();\n}\n\nstatic void raid6_avx24_xor_syndrome(int disks, int start, int stop,\n\t\t\t\t     size_t bytes, void **ptrs)\n{\n\tu8 **dptr = (u8 **)ptrs;\n\tu8 *p, *q;\n\tint d, z, z0;\n\n\tz0 = stop;\t\t \n\tp = dptr[disks-2];\t \n\tq = dptr[disks-1];\t \n\n\tkernel_fpu_begin();\n\n\tasm volatile(\"vmovdqa %0,%%ymm0\" :: \"m\" (raid6_avx2_constants.x1d[0]));\n\n\tfor (d = 0 ; d < bytes ; d += 128) {\n\t\tasm volatile(\"vmovdqa %0,%%ymm4\" :: \"m\" (dptr[z0][d]));\n\t\tasm volatile(\"vmovdqa %0,%%ymm6\" :: \"m\" (dptr[z0][d+32]));\n\t\tasm volatile(\"vmovdqa %0,%%ymm12\" :: \"m\" (dptr[z0][d+64]));\n\t\tasm volatile(\"vmovdqa %0,%%ymm14\" :: \"m\" (dptr[z0][d+96]));\n\t\tasm volatile(\"vmovdqa %0,%%ymm2\" : : \"m\" (p[d]));\n\t\tasm volatile(\"vmovdqa %0,%%ymm3\" : : \"m\" (p[d+32]));\n\t\tasm volatile(\"vmovdqa %0,%%ymm10\" : : \"m\" (p[d+64]));\n\t\tasm volatile(\"vmovdqa %0,%%ymm11\" : : \"m\" (p[d+96]));\n\t\tasm volatile(\"vpxor %ymm4,%ymm2,%ymm2\");\n\t\tasm volatile(\"vpxor %ymm6,%ymm3,%ymm3\");\n\t\tasm volatile(\"vpxor %ymm12,%ymm10,%ymm10\");\n\t\tasm volatile(\"vpxor %ymm14,%ymm11,%ymm11\");\n\t\t \n\t\tfor (z = z0-1 ; z >= start ; z--) {\n\t\t\tasm volatile(\"prefetchnta %0\" :: \"m\" (dptr[z][d]));\n\t\t\tasm volatile(\"prefetchnta %0\" :: \"m\" (dptr[z][d+64]));\n\t\t\tasm volatile(\"vpxor %ymm5,%ymm5,%ymm5\");\n\t\t\tasm volatile(\"vpxor %ymm7,%ymm7,%ymm7\");\n\t\t\tasm volatile(\"vpxor %ymm13,%ymm13,%ymm13\");\n\t\t\tasm volatile(\"vpxor %ymm15,%ymm15,%ymm15\");\n\t\t\tasm volatile(\"vpcmpgtb %ymm4,%ymm5,%ymm5\");\n\t\t\tasm volatile(\"vpcmpgtb %ymm6,%ymm7,%ymm7\");\n\t\t\tasm volatile(\"vpcmpgtb %ymm12,%ymm13,%ymm13\");\n\t\t\tasm volatile(\"vpcmpgtb %ymm14,%ymm15,%ymm15\");\n\t\t\tasm volatile(\"vpaddb %ymm4,%ymm4,%ymm4\");\n\t\t\tasm volatile(\"vpaddb %ymm6,%ymm6,%ymm6\");\n\t\t\tasm volatile(\"vpaddb %ymm12,%ymm12,%ymm12\");\n\t\t\tasm volatile(\"vpaddb %ymm14,%ymm14,%ymm14\");\n\t\t\tasm volatile(\"vpand %ymm0,%ymm5,%ymm5\");\n\t\t\tasm volatile(\"vpand %ymm0,%ymm7,%ymm7\");\n\t\t\tasm volatile(\"vpand %ymm0,%ymm13,%ymm13\");\n\t\t\tasm volatile(\"vpand %ymm0,%ymm15,%ymm15\");\n\t\t\tasm volatile(\"vpxor %ymm5,%ymm4,%ymm4\");\n\t\t\tasm volatile(\"vpxor %ymm7,%ymm6,%ymm6\");\n\t\t\tasm volatile(\"vpxor %ymm13,%ymm12,%ymm12\");\n\t\t\tasm volatile(\"vpxor %ymm15,%ymm14,%ymm14\");\n\t\t\tasm volatile(\"vmovdqa %0,%%ymm5\" :: \"m\" (dptr[z][d]));\n\t\t\tasm volatile(\"vmovdqa %0,%%ymm7\"\n\t\t\t\t     :: \"m\" (dptr[z][d+32]));\n\t\t\tasm volatile(\"vmovdqa %0,%%ymm13\"\n\t\t\t\t     :: \"m\" (dptr[z][d+64]));\n\t\t\tasm volatile(\"vmovdqa %0,%%ymm15\"\n\t\t\t\t     :: \"m\" (dptr[z][d+96]));\n\t\t\tasm volatile(\"vpxor %ymm5,%ymm2,%ymm2\");\n\t\t\tasm volatile(\"vpxor %ymm7,%ymm3,%ymm3\");\n\t\t\tasm volatile(\"vpxor %ymm13,%ymm10,%ymm10\");\n\t\t\tasm volatile(\"vpxor %ymm15,%ymm11,%ymm11\");\n\t\t\tasm volatile(\"vpxor %ymm5,%ymm4,%ymm4\");\n\t\t\tasm volatile(\"vpxor %ymm7,%ymm6,%ymm6\");\n\t\t\tasm volatile(\"vpxor %ymm13,%ymm12,%ymm12\");\n\t\t\tasm volatile(\"vpxor %ymm15,%ymm14,%ymm14\");\n\t\t}\n\t\tasm volatile(\"prefetchnta %0\" :: \"m\" (q[d]));\n\t\tasm volatile(\"prefetchnta %0\" :: \"m\" (q[d+64]));\n\t\t \n\t\tfor (z = start-1 ; z >= 0 ; z--) {\n\t\t\tasm volatile(\"vpxor %ymm5,%ymm5,%ymm5\");\n\t\t\tasm volatile(\"vpxor %ymm7,%ymm7,%ymm7\");\n\t\t\tasm volatile(\"vpxor %ymm13,%ymm13,%ymm13\");\n\t\t\tasm volatile(\"vpxor %ymm15,%ymm15,%ymm15\");\n\t\t\tasm volatile(\"vpcmpgtb %ymm4,%ymm5,%ymm5\");\n\t\t\tasm volatile(\"vpcmpgtb %ymm6,%ymm7,%ymm7\");\n\t\t\tasm volatile(\"vpcmpgtb %ymm12,%ymm13,%ymm13\");\n\t\t\tasm volatile(\"vpcmpgtb %ymm14,%ymm15,%ymm15\");\n\t\t\tasm volatile(\"vpaddb %ymm4,%ymm4,%ymm4\");\n\t\t\tasm volatile(\"vpaddb %ymm6,%ymm6,%ymm6\");\n\t\t\tasm volatile(\"vpaddb %ymm12,%ymm12,%ymm12\");\n\t\t\tasm volatile(\"vpaddb %ymm14,%ymm14,%ymm14\");\n\t\t\tasm volatile(\"vpand %ymm0,%ymm5,%ymm5\");\n\t\t\tasm volatile(\"vpand %ymm0,%ymm7,%ymm7\");\n\t\t\tasm volatile(\"vpand %ymm0,%ymm13,%ymm13\");\n\t\t\tasm volatile(\"vpand %ymm0,%ymm15,%ymm15\");\n\t\t\tasm volatile(\"vpxor %ymm5,%ymm4,%ymm4\");\n\t\t\tasm volatile(\"vpxor %ymm7,%ymm6,%ymm6\");\n\t\t\tasm volatile(\"vpxor %ymm13,%ymm12,%ymm12\");\n\t\t\tasm volatile(\"vpxor %ymm15,%ymm14,%ymm14\");\n\t\t}\n\t\tasm volatile(\"vmovntdq %%ymm2,%0\" : \"=m\" (p[d]));\n\t\tasm volatile(\"vmovntdq %%ymm3,%0\" : \"=m\" (p[d+32]));\n\t\tasm volatile(\"vmovntdq %%ymm10,%0\" : \"=m\" (p[d+64]));\n\t\tasm volatile(\"vmovntdq %%ymm11,%0\" : \"=m\" (p[d+96]));\n\t\tasm volatile(\"vpxor %0,%%ymm4,%%ymm4\" : : \"m\" (q[d]));\n\t\tasm volatile(\"vpxor %0,%%ymm6,%%ymm6\" : : \"m\" (q[d+32]));\n\t\tasm volatile(\"vpxor %0,%%ymm12,%%ymm12\" : : \"m\" (q[d+64]));\n\t\tasm volatile(\"vpxor %0,%%ymm14,%%ymm14\" : : \"m\" (q[d+96]));\n\t\tasm volatile(\"vmovntdq %%ymm4,%0\" : \"=m\" (q[d]));\n\t\tasm volatile(\"vmovntdq %%ymm6,%0\" : \"=m\" (q[d+32]));\n\t\tasm volatile(\"vmovntdq %%ymm12,%0\" : \"=m\" (q[d+64]));\n\t\tasm volatile(\"vmovntdq %%ymm14,%0\" : \"=m\" (q[d+96]));\n\t}\n\tasm volatile(\"sfence\" : : : \"memory\");\n\tkernel_fpu_end();\n}\n\nconst struct raid6_calls raid6_avx2x4 = {\n\traid6_avx24_gen_syndrome,\n\traid6_avx24_xor_syndrome,\n\traid6_have_avx2,\n\t\"avx2x4\",\n\t.priority = 2\t\t \n};\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}