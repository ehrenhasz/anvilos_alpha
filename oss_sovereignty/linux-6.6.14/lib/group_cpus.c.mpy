{
  "module_name": "group_cpus.c",
  "hash_id": "1b083522b430a961b02787c01bb4da055074e3a6295df1f49247a7bd62ec266d",
  "original_prompt": "Ingested from linux-6.6.14/lib/group_cpus.c",
  "human_readable_source": "\n \n#include <linux/kernel.h>\n#include <linux/slab.h>\n#include <linux/cpu.h>\n#include <linux/sort.h>\n#include <linux/group_cpus.h>\n\n#ifdef CONFIG_SMP\n\nstatic void grp_spread_init_one(struct cpumask *irqmsk, struct cpumask *nmsk,\n\t\t\t\tunsigned int cpus_per_grp)\n{\n\tconst struct cpumask *siblmsk;\n\tint cpu, sibl;\n\n\tfor ( ; cpus_per_grp > 0; ) {\n\t\tcpu = cpumask_first(nmsk);\n\n\t\t \n\t\tif (cpu >= nr_cpu_ids)\n\t\t\treturn;\n\n\t\tcpumask_clear_cpu(cpu, nmsk);\n\t\tcpumask_set_cpu(cpu, irqmsk);\n\t\tcpus_per_grp--;\n\n\t\t \n\t\tsiblmsk = topology_sibling_cpumask(cpu);\n\t\tfor (sibl = -1; cpus_per_grp > 0; ) {\n\t\t\tsibl = cpumask_next(sibl, siblmsk);\n\t\t\tif (sibl >= nr_cpu_ids)\n\t\t\t\tbreak;\n\t\t\tif (!cpumask_test_and_clear_cpu(sibl, nmsk))\n\t\t\t\tcontinue;\n\t\t\tcpumask_set_cpu(sibl, irqmsk);\n\t\t\tcpus_per_grp--;\n\t\t}\n\t}\n}\n\nstatic cpumask_var_t *alloc_node_to_cpumask(void)\n{\n\tcpumask_var_t *masks;\n\tint node;\n\n\tmasks = kcalloc(nr_node_ids, sizeof(cpumask_var_t), GFP_KERNEL);\n\tif (!masks)\n\t\treturn NULL;\n\n\tfor (node = 0; node < nr_node_ids; node++) {\n\t\tif (!zalloc_cpumask_var(&masks[node], GFP_KERNEL))\n\t\t\tgoto out_unwind;\n\t}\n\n\treturn masks;\n\nout_unwind:\n\twhile (--node >= 0)\n\t\tfree_cpumask_var(masks[node]);\n\tkfree(masks);\n\treturn NULL;\n}\n\nstatic void free_node_to_cpumask(cpumask_var_t *masks)\n{\n\tint node;\n\n\tfor (node = 0; node < nr_node_ids; node++)\n\t\tfree_cpumask_var(masks[node]);\n\tkfree(masks);\n}\n\nstatic void build_node_to_cpumask(cpumask_var_t *masks)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu)\n\t\tcpumask_set_cpu(cpu, masks[cpu_to_node(cpu)]);\n}\n\nstatic int get_nodes_in_cpumask(cpumask_var_t *node_to_cpumask,\n\t\t\t\tconst struct cpumask *mask, nodemask_t *nodemsk)\n{\n\tint n, nodes = 0;\n\n\t \n\tfor_each_node(n) {\n\t\tif (cpumask_intersects(mask, node_to_cpumask[n])) {\n\t\t\tnode_set(n, *nodemsk);\n\t\t\tnodes++;\n\t\t}\n\t}\n\treturn nodes;\n}\n\nstruct node_groups {\n\tunsigned id;\n\n\tunion {\n\t\tunsigned ngroups;\n\t\tunsigned ncpus;\n\t};\n};\n\nstatic int ncpus_cmp_func(const void *l, const void *r)\n{\n\tconst struct node_groups *ln = l;\n\tconst struct node_groups *rn = r;\n\n\treturn ln->ncpus - rn->ncpus;\n}\n\n \nstatic void alloc_nodes_groups(unsigned int numgrps,\n\t\t\t       cpumask_var_t *node_to_cpumask,\n\t\t\t       const struct cpumask *cpu_mask,\n\t\t\t       const nodemask_t nodemsk,\n\t\t\t       struct cpumask *nmsk,\n\t\t\t       struct node_groups *node_groups)\n{\n\tunsigned n, remaining_ncpus = 0;\n\n\tfor (n = 0; n < nr_node_ids; n++) {\n\t\tnode_groups[n].id = n;\n\t\tnode_groups[n].ncpus = UINT_MAX;\n\t}\n\n\tfor_each_node_mask(n, nodemsk) {\n\t\tunsigned ncpus;\n\n\t\tcpumask_and(nmsk, cpu_mask, node_to_cpumask[n]);\n\t\tncpus = cpumask_weight(nmsk);\n\n\t\tif (!ncpus)\n\t\t\tcontinue;\n\t\tremaining_ncpus += ncpus;\n\t\tnode_groups[n].ncpus = ncpus;\n\t}\n\n\tnumgrps = min_t(unsigned, remaining_ncpus, numgrps);\n\n\tsort(node_groups, nr_node_ids, sizeof(node_groups[0]),\n\t     ncpus_cmp_func, NULL);\n\n\t \n\tfor (n = 0; n < nr_node_ids; n++) {\n\t\tunsigned ngroups, ncpus;\n\n\t\tif (node_groups[n].ncpus == UINT_MAX)\n\t\t\tcontinue;\n\n\t\tWARN_ON_ONCE(numgrps == 0);\n\n\t\tncpus = node_groups[n].ncpus;\n\t\tngroups = max_t(unsigned, 1,\n\t\t\t\t numgrps * ncpus / remaining_ncpus);\n\t\tWARN_ON_ONCE(ngroups > ncpus);\n\n\t\tnode_groups[n].ngroups = ngroups;\n\n\t\tremaining_ncpus -= ncpus;\n\t\tnumgrps -= ngroups;\n\t}\n}\n\nstatic int __group_cpus_evenly(unsigned int startgrp, unsigned int numgrps,\n\t\t\t       cpumask_var_t *node_to_cpumask,\n\t\t\t       const struct cpumask *cpu_mask,\n\t\t\t       struct cpumask *nmsk, struct cpumask *masks)\n{\n\tunsigned int i, n, nodes, cpus_per_grp, extra_grps, done = 0;\n\tunsigned int last_grp = numgrps;\n\tunsigned int curgrp = startgrp;\n\tnodemask_t nodemsk = NODE_MASK_NONE;\n\tstruct node_groups *node_groups;\n\n\tif (cpumask_empty(cpu_mask))\n\t\treturn 0;\n\n\tnodes = get_nodes_in_cpumask(node_to_cpumask, cpu_mask, &nodemsk);\n\n\t \n\tif (numgrps <= nodes) {\n\t\tfor_each_node_mask(n, nodemsk) {\n\t\t\t \n\t\t\tcpumask_and(nmsk, cpu_mask, node_to_cpumask[n]);\n\t\t\tcpumask_or(&masks[curgrp], &masks[curgrp], nmsk);\n\t\t\tif (++curgrp == last_grp)\n\t\t\t\tcurgrp = 0;\n\t\t}\n\t\treturn numgrps;\n\t}\n\n\tnode_groups = kcalloc(nr_node_ids,\n\t\t\t       sizeof(struct node_groups),\n\t\t\t       GFP_KERNEL);\n\tif (!node_groups)\n\t\treturn -ENOMEM;\n\n\t \n\talloc_nodes_groups(numgrps, node_to_cpumask, cpu_mask,\n\t\t\t   nodemsk, nmsk, node_groups);\n\tfor (i = 0; i < nr_node_ids; i++) {\n\t\tunsigned int ncpus, v;\n\t\tstruct node_groups *nv = &node_groups[i];\n\n\t\tif (nv->ngroups == UINT_MAX)\n\t\t\tcontinue;\n\n\t\t \n\t\tcpumask_and(nmsk, cpu_mask, node_to_cpumask[nv->id]);\n\t\tncpus = cpumask_weight(nmsk);\n\t\tif (!ncpus)\n\t\t\tcontinue;\n\n\t\tWARN_ON_ONCE(nv->ngroups > ncpus);\n\n\t\t \n\t\textra_grps = ncpus - nv->ngroups * (ncpus / nv->ngroups);\n\n\t\t \n\t\tfor (v = 0; v < nv->ngroups; v++, curgrp++) {\n\t\t\tcpus_per_grp = ncpus / nv->ngroups;\n\n\t\t\t \n\t\t\tif (extra_grps) {\n\t\t\t\tcpus_per_grp++;\n\t\t\t\t--extra_grps;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (curgrp >= last_grp)\n\t\t\t\tcurgrp = 0;\n\t\t\tgrp_spread_init_one(&masks[curgrp], nmsk,\n\t\t\t\t\t\tcpus_per_grp);\n\t\t}\n\t\tdone += nv->ngroups;\n\t}\n\tkfree(node_groups);\n\treturn done;\n}\n\n \nstruct cpumask *group_cpus_evenly(unsigned int numgrps)\n{\n\tunsigned int curgrp = 0, nr_present = 0, nr_others = 0;\n\tcpumask_var_t *node_to_cpumask;\n\tcpumask_var_t nmsk, npresmsk;\n\tint ret = -ENOMEM;\n\tstruct cpumask *masks = NULL;\n\n\tif (!zalloc_cpumask_var(&nmsk, GFP_KERNEL))\n\t\treturn NULL;\n\n\tif (!zalloc_cpumask_var(&npresmsk, GFP_KERNEL))\n\t\tgoto fail_nmsk;\n\n\tnode_to_cpumask = alloc_node_to_cpumask();\n\tif (!node_to_cpumask)\n\t\tgoto fail_npresmsk;\n\n\tmasks = kcalloc(numgrps, sizeof(*masks), GFP_KERNEL);\n\tif (!masks)\n\t\tgoto fail_node_to_cpumask;\n\n\tbuild_node_to_cpumask(node_to_cpumask);\n\n\t \n\tcpumask_copy(npresmsk, data_race(cpu_present_mask));\n\n\t \n\tret = __group_cpus_evenly(curgrp, numgrps, node_to_cpumask,\n\t\t\t\t  npresmsk, nmsk, masks);\n\tif (ret < 0)\n\t\tgoto fail_build_affinity;\n\tnr_present = ret;\n\n\t \n\tif (nr_present >= numgrps)\n\t\tcurgrp = 0;\n\telse\n\t\tcurgrp = nr_present;\n\tcpumask_andnot(npresmsk, cpu_possible_mask, npresmsk);\n\tret = __group_cpus_evenly(curgrp, numgrps, node_to_cpumask,\n\t\t\t\t  npresmsk, nmsk, masks);\n\tif (ret >= 0)\n\t\tnr_others = ret;\n\n fail_build_affinity:\n\tif (ret >= 0)\n\t\tWARN_ON(nr_present + nr_others < numgrps);\n\n fail_node_to_cpumask:\n\tfree_node_to_cpumask(node_to_cpumask);\n\n fail_npresmsk:\n\tfree_cpumask_var(npresmsk);\n\n fail_nmsk:\n\tfree_cpumask_var(nmsk);\n\tif (ret < 0) {\n\t\tkfree(masks);\n\t\treturn NULL;\n\t}\n\treturn masks;\n}\n#else  \nstruct cpumask *group_cpus_evenly(unsigned int numgrps)\n{\n\tstruct cpumask *masks = kcalloc(numgrps, sizeof(*masks), GFP_KERNEL);\n\n\tif (!masks)\n\t\treturn NULL;\n\n\t \n\tcpumask_copy(&masks[0], cpu_possible_mask);\n\treturn masks;\n}\n#endif  \nEXPORT_SYMBOL_GPL(group_cpus_evenly);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}