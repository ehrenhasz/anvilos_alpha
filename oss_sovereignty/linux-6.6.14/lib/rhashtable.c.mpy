{
  "module_name": "rhashtable.c",
  "hash_id": "5269ff28584f0d79d007a2f7d904c8ba445d6f1cf2b32c6757790404a02d0b23",
  "original_prompt": "Ingested from linux-6.6.14/lib/rhashtable.c",
  "human_readable_source": "\n \n\n#include <linux/atomic.h>\n#include <linux/kernel.h>\n#include <linux/init.h>\n#include <linux/log2.h>\n#include <linux/sched.h>\n#include <linux/rculist.h>\n#include <linux/slab.h>\n#include <linux/vmalloc.h>\n#include <linux/mm.h>\n#include <linux/jhash.h>\n#include <linux/random.h>\n#include <linux/rhashtable.h>\n#include <linux/err.h>\n#include <linux/export.h>\n\n#define HASH_DEFAULT_SIZE\t64UL\n#define HASH_MIN_SIZE\t\t4U\n\nunion nested_table {\n\tunion nested_table __rcu *table;\n\tstruct rhash_lock_head __rcu *bucket;\n};\n\nstatic u32 head_hashfn(struct rhashtable *ht,\n\t\t       const struct bucket_table *tbl,\n\t\t       const struct rhash_head *he)\n{\n\treturn rht_head_hashfn(ht, tbl, he, ht->p);\n}\n\n#ifdef CONFIG_PROVE_LOCKING\n#define ASSERT_RHT_MUTEX(HT) BUG_ON(!lockdep_rht_mutex_is_held(HT))\n\nint lockdep_rht_mutex_is_held(struct rhashtable *ht)\n{\n\treturn (debug_locks) ? lockdep_is_held(&ht->mutex) : 1;\n}\nEXPORT_SYMBOL_GPL(lockdep_rht_mutex_is_held);\n\nint lockdep_rht_bucket_is_held(const struct bucket_table *tbl, u32 hash)\n{\n\tif (!debug_locks)\n\t\treturn 1;\n\tif (unlikely(tbl->nest))\n\t\treturn 1;\n\treturn bit_spin_is_locked(0, (unsigned long *)&tbl->buckets[hash]);\n}\nEXPORT_SYMBOL_GPL(lockdep_rht_bucket_is_held);\n#else\n#define ASSERT_RHT_MUTEX(HT)\n#endif\n\nstatic inline union nested_table *nested_table_top(\n\tconst struct bucket_table *tbl)\n{\n\t \n\treturn (void *)rcu_dereference_protected(tbl->buckets[0], 1);\n}\n\nstatic void nested_table_free(union nested_table *ntbl, unsigned int size)\n{\n\tconst unsigned int shift = PAGE_SHIFT - ilog2(sizeof(void *));\n\tconst unsigned int len = 1 << shift;\n\tunsigned int i;\n\n\tntbl = rcu_dereference_protected(ntbl->table, 1);\n\tif (!ntbl)\n\t\treturn;\n\n\tif (size > len) {\n\t\tsize >>= shift;\n\t\tfor (i = 0; i < len; i++)\n\t\t\tnested_table_free(ntbl + i, size);\n\t}\n\n\tkfree(ntbl);\n}\n\nstatic void nested_bucket_table_free(const struct bucket_table *tbl)\n{\n\tunsigned int size = tbl->size >> tbl->nest;\n\tunsigned int len = 1 << tbl->nest;\n\tunion nested_table *ntbl;\n\tunsigned int i;\n\n\tntbl = nested_table_top(tbl);\n\n\tfor (i = 0; i < len; i++)\n\t\tnested_table_free(ntbl + i, size);\n\n\tkfree(ntbl);\n}\n\nstatic void bucket_table_free(const struct bucket_table *tbl)\n{\n\tif (tbl->nest)\n\t\tnested_bucket_table_free(tbl);\n\n\tkvfree(tbl);\n}\n\nstatic void bucket_table_free_rcu(struct rcu_head *head)\n{\n\tbucket_table_free(container_of(head, struct bucket_table, rcu));\n}\n\nstatic union nested_table *nested_table_alloc(struct rhashtable *ht,\n\t\t\t\t\t      union nested_table __rcu **prev,\n\t\t\t\t\t      bool leaf)\n{\n\tunion nested_table *ntbl;\n\tint i;\n\n\tntbl = rcu_dereference(*prev);\n\tif (ntbl)\n\t\treturn ntbl;\n\n\tntbl = kzalloc(PAGE_SIZE, GFP_ATOMIC);\n\n\tif (ntbl && leaf) {\n\t\tfor (i = 0; i < PAGE_SIZE / sizeof(ntbl[0]); i++)\n\t\t\tINIT_RHT_NULLS_HEAD(ntbl[i].bucket);\n\t}\n\n\tif (cmpxchg((union nested_table **)prev, NULL, ntbl) == NULL)\n\t\treturn ntbl;\n\t \n\tkfree(ntbl);\n\treturn rcu_dereference(*prev);\n}\n\nstatic struct bucket_table *nested_bucket_table_alloc(struct rhashtable *ht,\n\t\t\t\t\t\t      size_t nbuckets,\n\t\t\t\t\t\t      gfp_t gfp)\n{\n\tconst unsigned int shift = PAGE_SHIFT - ilog2(sizeof(void *));\n\tstruct bucket_table *tbl;\n\tsize_t size;\n\n\tif (nbuckets < (1 << (shift + 1)))\n\t\treturn NULL;\n\n\tsize = sizeof(*tbl) + sizeof(tbl->buckets[0]);\n\n\ttbl = kzalloc(size, gfp);\n\tif (!tbl)\n\t\treturn NULL;\n\n\tif (!nested_table_alloc(ht, (union nested_table __rcu **)tbl->buckets,\n\t\t\t\tfalse)) {\n\t\tkfree(tbl);\n\t\treturn NULL;\n\t}\n\n\ttbl->nest = (ilog2(nbuckets) - 1) % shift + 1;\n\n\treturn tbl;\n}\n\nstatic struct bucket_table *bucket_table_alloc(struct rhashtable *ht,\n\t\t\t\t\t       size_t nbuckets,\n\t\t\t\t\t       gfp_t gfp)\n{\n\tstruct bucket_table *tbl = NULL;\n\tsize_t size;\n\tint i;\n\tstatic struct lock_class_key __key;\n\n\ttbl = kvzalloc(struct_size(tbl, buckets, nbuckets), gfp);\n\n\tsize = nbuckets;\n\n\tif (tbl == NULL && (gfp & ~__GFP_NOFAIL) != GFP_KERNEL) {\n\t\ttbl = nested_bucket_table_alloc(ht, nbuckets, gfp);\n\t\tnbuckets = 0;\n\t}\n\n\tif (tbl == NULL)\n\t\treturn NULL;\n\n\tlockdep_init_map(&tbl->dep_map, \"rhashtable_bucket\", &__key, 0);\n\n\ttbl->size = size;\n\n\trcu_head_init(&tbl->rcu);\n\tINIT_LIST_HEAD(&tbl->walkers);\n\n\ttbl->hash_rnd = get_random_u32();\n\n\tfor (i = 0; i < nbuckets; i++)\n\t\tINIT_RHT_NULLS_HEAD(tbl->buckets[i]);\n\n\treturn tbl;\n}\n\nstatic struct bucket_table *rhashtable_last_table(struct rhashtable *ht,\n\t\t\t\t\t\t  struct bucket_table *tbl)\n{\n\tstruct bucket_table *new_tbl;\n\n\tdo {\n\t\tnew_tbl = tbl;\n\t\ttbl = rht_dereference_rcu(tbl->future_tbl, ht);\n\t} while (tbl);\n\n\treturn new_tbl;\n}\n\nstatic int rhashtable_rehash_one(struct rhashtable *ht,\n\t\t\t\t struct rhash_lock_head __rcu **bkt,\n\t\t\t\t unsigned int old_hash)\n{\n\tstruct bucket_table *old_tbl = rht_dereference(ht->tbl, ht);\n\tstruct bucket_table *new_tbl = rhashtable_last_table(ht, old_tbl);\n\tint err = -EAGAIN;\n\tstruct rhash_head *head, *next, *entry;\n\tstruct rhash_head __rcu **pprev = NULL;\n\tunsigned int new_hash;\n\tunsigned long flags;\n\n\tif (new_tbl->nest)\n\t\tgoto out;\n\n\terr = -ENOENT;\n\n\trht_for_each_from(entry, rht_ptr(bkt, old_tbl, old_hash),\n\t\t\t  old_tbl, old_hash) {\n\t\terr = 0;\n\t\tnext = rht_dereference_bucket(entry->next, old_tbl, old_hash);\n\n\t\tif (rht_is_a_nulls(next))\n\t\t\tbreak;\n\n\t\tpprev = &entry->next;\n\t}\n\n\tif (err)\n\t\tgoto out;\n\n\tnew_hash = head_hashfn(ht, new_tbl, entry);\n\n\tflags = rht_lock_nested(new_tbl, &new_tbl->buckets[new_hash],\n\t\t\t\tSINGLE_DEPTH_NESTING);\n\n\thead = rht_ptr(new_tbl->buckets + new_hash, new_tbl, new_hash);\n\n\tRCU_INIT_POINTER(entry->next, head);\n\n\trht_assign_unlock(new_tbl, &new_tbl->buckets[new_hash], entry, flags);\n\n\tif (pprev)\n\t\trcu_assign_pointer(*pprev, next);\n\telse\n\t\t \n\t\trht_assign_locked(bkt, next);\n\nout:\n\treturn err;\n}\n\nstatic int rhashtable_rehash_chain(struct rhashtable *ht,\n\t\t\t\t    unsigned int old_hash)\n{\n\tstruct bucket_table *old_tbl = rht_dereference(ht->tbl, ht);\n\tstruct rhash_lock_head __rcu **bkt = rht_bucket_var(old_tbl, old_hash);\n\tunsigned long flags;\n\tint err;\n\n\tif (!bkt)\n\t\treturn 0;\n\tflags = rht_lock(old_tbl, bkt);\n\n\twhile (!(err = rhashtable_rehash_one(ht, bkt, old_hash)))\n\t\t;\n\n\tif (err == -ENOENT)\n\t\terr = 0;\n\trht_unlock(old_tbl, bkt, flags);\n\n\treturn err;\n}\n\nstatic int rhashtable_rehash_attach(struct rhashtable *ht,\n\t\t\t\t    struct bucket_table *old_tbl,\n\t\t\t\t    struct bucket_table *new_tbl)\n{\n\t \n\n\tif (cmpxchg((struct bucket_table **)&old_tbl->future_tbl, NULL,\n\t\t    new_tbl) != NULL)\n\t\treturn -EEXIST;\n\n\treturn 0;\n}\n\nstatic int rhashtable_rehash_table(struct rhashtable *ht)\n{\n\tstruct bucket_table *old_tbl = rht_dereference(ht->tbl, ht);\n\tstruct bucket_table *new_tbl;\n\tstruct rhashtable_walker *walker;\n\tunsigned int old_hash;\n\tint err;\n\n\tnew_tbl = rht_dereference(old_tbl->future_tbl, ht);\n\tif (!new_tbl)\n\t\treturn 0;\n\n\tfor (old_hash = 0; old_hash < old_tbl->size; old_hash++) {\n\t\terr = rhashtable_rehash_chain(ht, old_hash);\n\t\tif (err)\n\t\t\treturn err;\n\t\tcond_resched();\n\t}\n\n\t \n\trcu_assign_pointer(ht->tbl, new_tbl);\n\n\tspin_lock(&ht->lock);\n\tlist_for_each_entry(walker, &old_tbl->walkers, list)\n\t\twalker->tbl = NULL;\n\n\t \n\tcall_rcu(&old_tbl->rcu, bucket_table_free_rcu);\n\tspin_unlock(&ht->lock);\n\n\treturn rht_dereference(new_tbl->future_tbl, ht) ? -EAGAIN : 0;\n}\n\nstatic int rhashtable_rehash_alloc(struct rhashtable *ht,\n\t\t\t\t   struct bucket_table *old_tbl,\n\t\t\t\t   unsigned int size)\n{\n\tstruct bucket_table *new_tbl;\n\tint err;\n\n\tASSERT_RHT_MUTEX(ht);\n\n\tnew_tbl = bucket_table_alloc(ht, size, GFP_KERNEL);\n\tif (new_tbl == NULL)\n\t\treturn -ENOMEM;\n\n\terr = rhashtable_rehash_attach(ht, old_tbl, new_tbl);\n\tif (err)\n\t\tbucket_table_free(new_tbl);\n\n\treturn err;\n}\n\n \nstatic int rhashtable_shrink(struct rhashtable *ht)\n{\n\tstruct bucket_table *old_tbl = rht_dereference(ht->tbl, ht);\n\tunsigned int nelems = atomic_read(&ht->nelems);\n\tunsigned int size = 0;\n\n\tif (nelems)\n\t\tsize = roundup_pow_of_two(nelems * 3 / 2);\n\tif (size < ht->p.min_size)\n\t\tsize = ht->p.min_size;\n\n\tif (old_tbl->size <= size)\n\t\treturn 0;\n\n\tif (rht_dereference(old_tbl->future_tbl, ht))\n\t\treturn -EEXIST;\n\n\treturn rhashtable_rehash_alloc(ht, old_tbl, size);\n}\n\nstatic void rht_deferred_worker(struct work_struct *work)\n{\n\tstruct rhashtable *ht;\n\tstruct bucket_table *tbl;\n\tint err = 0;\n\n\tht = container_of(work, struct rhashtable, run_work);\n\tmutex_lock(&ht->mutex);\n\n\ttbl = rht_dereference(ht->tbl, ht);\n\ttbl = rhashtable_last_table(ht, tbl);\n\n\tif (rht_grow_above_75(ht, tbl))\n\t\terr = rhashtable_rehash_alloc(ht, tbl, tbl->size * 2);\n\telse if (ht->p.automatic_shrinking && rht_shrink_below_30(ht, tbl))\n\t\terr = rhashtable_shrink(ht);\n\telse if (tbl->nest)\n\t\terr = rhashtable_rehash_alloc(ht, tbl, tbl->size);\n\n\tif (!err || err == -EEXIST) {\n\t\tint nerr;\n\n\t\tnerr = rhashtable_rehash_table(ht);\n\t\terr = err ?: nerr;\n\t}\n\n\tmutex_unlock(&ht->mutex);\n\n\tif (err)\n\t\tschedule_work(&ht->run_work);\n}\n\nstatic int rhashtable_insert_rehash(struct rhashtable *ht,\n\t\t\t\t    struct bucket_table *tbl)\n{\n\tstruct bucket_table *old_tbl;\n\tstruct bucket_table *new_tbl;\n\tunsigned int size;\n\tint err;\n\n\told_tbl = rht_dereference_rcu(ht->tbl, ht);\n\n\tsize = tbl->size;\n\n\terr = -EBUSY;\n\n\tif (rht_grow_above_75(ht, tbl))\n\t\tsize *= 2;\n\t \n\telse if (old_tbl != tbl)\n\t\tgoto fail;\n\n\terr = -ENOMEM;\n\n\tnew_tbl = bucket_table_alloc(ht, size, GFP_ATOMIC | __GFP_NOWARN);\n\tif (new_tbl == NULL)\n\t\tgoto fail;\n\n\terr = rhashtable_rehash_attach(ht, tbl, new_tbl);\n\tif (err) {\n\t\tbucket_table_free(new_tbl);\n\t\tif (err == -EEXIST)\n\t\t\terr = 0;\n\t} else\n\t\tschedule_work(&ht->run_work);\n\n\treturn err;\n\nfail:\n\t \n\tif (likely(rcu_access_pointer(tbl->future_tbl)))\n\t\treturn 0;\n\n\t \n\tif (err == -ENOMEM)\n\t\tschedule_work(&ht->run_work);\n\n\treturn err;\n}\n\nstatic void *rhashtable_lookup_one(struct rhashtable *ht,\n\t\t\t\t   struct rhash_lock_head __rcu **bkt,\n\t\t\t\t   struct bucket_table *tbl, unsigned int hash,\n\t\t\t\t   const void *key, struct rhash_head *obj)\n{\n\tstruct rhashtable_compare_arg arg = {\n\t\t.ht = ht,\n\t\t.key = key,\n\t};\n\tstruct rhash_head __rcu **pprev = NULL;\n\tstruct rhash_head *head;\n\tint elasticity;\n\n\telasticity = RHT_ELASTICITY;\n\trht_for_each_from(head, rht_ptr(bkt, tbl, hash), tbl, hash) {\n\t\tstruct rhlist_head *list;\n\t\tstruct rhlist_head *plist;\n\n\t\telasticity--;\n\t\tif (!key ||\n\t\t    (ht->p.obj_cmpfn ?\n\t\t     ht->p.obj_cmpfn(&arg, rht_obj(ht, head)) :\n\t\t     rhashtable_compare(&arg, rht_obj(ht, head)))) {\n\t\t\tpprev = &head->next;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!ht->rhlist)\n\t\t\treturn rht_obj(ht, head);\n\n\t\tlist = container_of(obj, struct rhlist_head, rhead);\n\t\tplist = container_of(head, struct rhlist_head, rhead);\n\n\t\tRCU_INIT_POINTER(list->next, plist);\n\t\thead = rht_dereference_bucket(head->next, tbl, hash);\n\t\tRCU_INIT_POINTER(list->rhead.next, head);\n\t\tif (pprev)\n\t\t\trcu_assign_pointer(*pprev, obj);\n\t\telse\n\t\t\t \n\t\t\trht_assign_locked(bkt, obj);\n\n\t\treturn NULL;\n\t}\n\n\tif (elasticity <= 0)\n\t\treturn ERR_PTR(-EAGAIN);\n\n\treturn ERR_PTR(-ENOENT);\n}\n\nstatic struct bucket_table *rhashtable_insert_one(\n\tstruct rhashtable *ht, struct rhash_lock_head __rcu **bkt,\n\tstruct bucket_table *tbl, unsigned int hash, struct rhash_head *obj,\n\tvoid *data)\n{\n\tstruct bucket_table *new_tbl;\n\tstruct rhash_head *head;\n\n\tif (!IS_ERR_OR_NULL(data))\n\t\treturn ERR_PTR(-EEXIST);\n\n\tif (PTR_ERR(data) != -EAGAIN && PTR_ERR(data) != -ENOENT)\n\t\treturn ERR_CAST(data);\n\n\tnew_tbl = rht_dereference_rcu(tbl->future_tbl, ht);\n\tif (new_tbl)\n\t\treturn new_tbl;\n\n\tif (PTR_ERR(data) != -ENOENT)\n\t\treturn ERR_CAST(data);\n\n\tif (unlikely(rht_grow_above_max(ht, tbl)))\n\t\treturn ERR_PTR(-E2BIG);\n\n\tif (unlikely(rht_grow_above_100(ht, tbl)))\n\t\treturn ERR_PTR(-EAGAIN);\n\n\thead = rht_ptr(bkt, tbl, hash);\n\n\tRCU_INIT_POINTER(obj->next, head);\n\tif (ht->rhlist) {\n\t\tstruct rhlist_head *list;\n\n\t\tlist = container_of(obj, struct rhlist_head, rhead);\n\t\tRCU_INIT_POINTER(list->next, NULL);\n\t}\n\n\t \n\trht_assign_locked(bkt, obj);\n\n\tatomic_inc(&ht->nelems);\n\tif (rht_grow_above_75(ht, tbl))\n\t\tschedule_work(&ht->run_work);\n\n\treturn NULL;\n}\n\nstatic void *rhashtable_try_insert(struct rhashtable *ht, const void *key,\n\t\t\t\t   struct rhash_head *obj)\n{\n\tstruct bucket_table *new_tbl;\n\tstruct bucket_table *tbl;\n\tstruct rhash_lock_head __rcu **bkt;\n\tunsigned long flags;\n\tunsigned int hash;\n\tvoid *data;\n\n\tnew_tbl = rcu_dereference(ht->tbl);\n\n\tdo {\n\t\ttbl = new_tbl;\n\t\thash = rht_head_hashfn(ht, tbl, obj, ht->p);\n\t\tif (rcu_access_pointer(tbl->future_tbl))\n\t\t\t \n\t\t\tbkt = rht_bucket_var(tbl, hash);\n\t\telse\n\t\t\tbkt = rht_bucket_insert(ht, tbl, hash);\n\t\tif (bkt == NULL) {\n\t\t\tnew_tbl = rht_dereference_rcu(tbl->future_tbl, ht);\n\t\t\tdata = ERR_PTR(-EAGAIN);\n\t\t} else {\n\t\t\tflags = rht_lock(tbl, bkt);\n\t\t\tdata = rhashtable_lookup_one(ht, bkt, tbl,\n\t\t\t\t\t\t     hash, key, obj);\n\t\t\tnew_tbl = rhashtable_insert_one(ht, bkt, tbl,\n\t\t\t\t\t\t\thash, obj, data);\n\t\t\tif (PTR_ERR(new_tbl) != -EEXIST)\n\t\t\t\tdata = ERR_CAST(new_tbl);\n\n\t\t\trht_unlock(tbl, bkt, flags);\n\t\t}\n\t} while (!IS_ERR_OR_NULL(new_tbl));\n\n\tif (PTR_ERR(data) == -EAGAIN)\n\t\tdata = ERR_PTR(rhashtable_insert_rehash(ht, tbl) ?:\n\t\t\t       -EAGAIN);\n\n\treturn data;\n}\n\nvoid *rhashtable_insert_slow(struct rhashtable *ht, const void *key,\n\t\t\t     struct rhash_head *obj)\n{\n\tvoid *data;\n\n\tdo {\n\t\trcu_read_lock();\n\t\tdata = rhashtable_try_insert(ht, key, obj);\n\t\trcu_read_unlock();\n\t} while (PTR_ERR(data) == -EAGAIN);\n\n\treturn data;\n}\nEXPORT_SYMBOL_GPL(rhashtable_insert_slow);\n\n \nvoid rhashtable_walk_enter(struct rhashtable *ht, struct rhashtable_iter *iter)\n{\n\titer->ht = ht;\n\titer->p = NULL;\n\titer->slot = 0;\n\titer->skip = 0;\n\titer->end_of_table = 0;\n\n\tspin_lock(&ht->lock);\n\titer->walker.tbl =\n\t\trcu_dereference_protected(ht->tbl, lockdep_is_held(&ht->lock));\n\tlist_add(&iter->walker.list, &iter->walker.tbl->walkers);\n\tspin_unlock(&ht->lock);\n}\nEXPORT_SYMBOL_GPL(rhashtable_walk_enter);\n\n \nvoid rhashtable_walk_exit(struct rhashtable_iter *iter)\n{\n\tspin_lock(&iter->ht->lock);\n\tif (iter->walker.tbl)\n\t\tlist_del(&iter->walker.list);\n\tspin_unlock(&iter->ht->lock);\n}\nEXPORT_SYMBOL_GPL(rhashtable_walk_exit);\n\n \nint rhashtable_walk_start_check(struct rhashtable_iter *iter)\n\t__acquires(RCU)\n{\n\tstruct rhashtable *ht = iter->ht;\n\tbool rhlist = ht->rhlist;\n\n\trcu_read_lock();\n\n\tspin_lock(&ht->lock);\n\tif (iter->walker.tbl)\n\t\tlist_del(&iter->walker.list);\n\tspin_unlock(&ht->lock);\n\n\tif (iter->end_of_table)\n\t\treturn 0;\n\tif (!iter->walker.tbl) {\n\t\titer->walker.tbl = rht_dereference_rcu(ht->tbl, ht);\n\t\titer->slot = 0;\n\t\titer->skip = 0;\n\t\treturn -EAGAIN;\n\t}\n\n\tif (iter->p && !rhlist) {\n\t\t \n\t\tstruct rhash_head *p;\n\t\tint skip = 0;\n\t\trht_for_each_rcu(p, iter->walker.tbl, iter->slot) {\n\t\t\tskip++;\n\t\t\tif (p == iter->p) {\n\t\t\t\titer->skip = skip;\n\t\t\t\tgoto found;\n\t\t\t}\n\t\t}\n\t\titer->p = NULL;\n\t} else if (iter->p && rhlist) {\n\t\t \n\t\tstruct rhash_head *p;\n\t\tstruct rhlist_head *list;\n\t\tint skip = 0;\n\t\trht_for_each_rcu(p, iter->walker.tbl, iter->slot) {\n\t\t\tfor (list = container_of(p, struct rhlist_head, rhead);\n\t\t\t     list;\n\t\t\t     list = rcu_dereference(list->next)) {\n\t\t\t\tskip++;\n\t\t\t\tif (list == iter->list) {\n\t\t\t\t\titer->p = p;\n\t\t\t\t\titer->skip = skip;\n\t\t\t\t\tgoto found;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\titer->p = NULL;\n\t}\nfound:\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(rhashtable_walk_start_check);\n\n \nstatic void *__rhashtable_walk_find_next(struct rhashtable_iter *iter)\n{\n\tstruct bucket_table *tbl = iter->walker.tbl;\n\tstruct rhlist_head *list = iter->list;\n\tstruct rhashtable *ht = iter->ht;\n\tstruct rhash_head *p = iter->p;\n\tbool rhlist = ht->rhlist;\n\n\tif (!tbl)\n\t\treturn NULL;\n\n\tfor (; iter->slot < tbl->size; iter->slot++) {\n\t\tint skip = iter->skip;\n\n\t\trht_for_each_rcu(p, tbl, iter->slot) {\n\t\t\tif (rhlist) {\n\t\t\t\tlist = container_of(p, struct rhlist_head,\n\t\t\t\t\t\t    rhead);\n\t\t\t\tdo {\n\t\t\t\t\tif (!skip)\n\t\t\t\t\t\tgoto next;\n\t\t\t\t\tskip--;\n\t\t\t\t\tlist = rcu_dereference(list->next);\n\t\t\t\t} while (list);\n\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (!skip)\n\t\t\t\tbreak;\n\t\t\tskip--;\n\t\t}\n\nnext:\n\t\tif (!rht_is_a_nulls(p)) {\n\t\t\titer->skip++;\n\t\t\titer->p = p;\n\t\t\titer->list = list;\n\t\t\treturn rht_obj(ht, rhlist ? &list->rhead : p);\n\t\t}\n\n\t\titer->skip = 0;\n\t}\n\n\titer->p = NULL;\n\n\t \n\tsmp_rmb();\n\n\titer->walker.tbl = rht_dereference_rcu(tbl->future_tbl, ht);\n\tif (iter->walker.tbl) {\n\t\titer->slot = 0;\n\t\titer->skip = 0;\n\t\treturn ERR_PTR(-EAGAIN);\n\t} else {\n\t\titer->end_of_table = true;\n\t}\n\n\treturn NULL;\n}\n\n \nvoid *rhashtable_walk_next(struct rhashtable_iter *iter)\n{\n\tstruct rhlist_head *list = iter->list;\n\tstruct rhashtable *ht = iter->ht;\n\tstruct rhash_head *p = iter->p;\n\tbool rhlist = ht->rhlist;\n\n\tif (p) {\n\t\tif (!rhlist || !(list = rcu_dereference(list->next))) {\n\t\t\tp = rcu_dereference(p->next);\n\t\t\tlist = container_of(p, struct rhlist_head, rhead);\n\t\t}\n\t\tif (!rht_is_a_nulls(p)) {\n\t\t\titer->skip++;\n\t\t\titer->p = p;\n\t\t\titer->list = list;\n\t\t\treturn rht_obj(ht, rhlist ? &list->rhead : p);\n\t\t}\n\n\t\t \n\t\titer->skip = 0;\n\t\titer->slot++;\n\t}\n\n\treturn __rhashtable_walk_find_next(iter);\n}\nEXPORT_SYMBOL_GPL(rhashtable_walk_next);\n\n \nvoid *rhashtable_walk_peek(struct rhashtable_iter *iter)\n{\n\tstruct rhlist_head *list = iter->list;\n\tstruct rhashtable *ht = iter->ht;\n\tstruct rhash_head *p = iter->p;\n\n\tif (p)\n\t\treturn rht_obj(ht, ht->rhlist ? &list->rhead : p);\n\n\t \n\n\tif (iter->skip) {\n\t\t \n\t\titer->skip--;\n\t}\n\n\treturn __rhashtable_walk_find_next(iter);\n}\nEXPORT_SYMBOL_GPL(rhashtable_walk_peek);\n\n \nvoid rhashtable_walk_stop(struct rhashtable_iter *iter)\n\t__releases(RCU)\n{\n\tstruct rhashtable *ht;\n\tstruct bucket_table *tbl = iter->walker.tbl;\n\n\tif (!tbl)\n\t\tgoto out;\n\n\tht = iter->ht;\n\n\tspin_lock(&ht->lock);\n\tif (rcu_head_after_call_rcu(&tbl->rcu, bucket_table_free_rcu))\n\t\t \n\t\titer->walker.tbl = NULL;\n\telse\n\t\tlist_add(&iter->walker.list, &tbl->walkers);\n\tspin_unlock(&ht->lock);\n\nout:\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL_GPL(rhashtable_walk_stop);\n\nstatic size_t rounded_hashtable_size(const struct rhashtable_params *params)\n{\n\tsize_t retsize;\n\n\tif (params->nelem_hint)\n\t\tretsize = max(roundup_pow_of_two(params->nelem_hint * 4 / 3),\n\t\t\t      (unsigned long)params->min_size);\n\telse\n\t\tretsize = max(HASH_DEFAULT_SIZE,\n\t\t\t      (unsigned long)params->min_size);\n\n\treturn retsize;\n}\n\nstatic u32 rhashtable_jhash2(const void *key, u32 length, u32 seed)\n{\n\treturn jhash2(key, length, seed);\n}\n\n \nint rhashtable_init(struct rhashtable *ht,\n\t\t    const struct rhashtable_params *params)\n{\n\tstruct bucket_table *tbl;\n\tsize_t size;\n\n\tif ((!params->key_len && !params->obj_hashfn) ||\n\t    (params->obj_hashfn && !params->obj_cmpfn))\n\t\treturn -EINVAL;\n\n\tmemset(ht, 0, sizeof(*ht));\n\tmutex_init(&ht->mutex);\n\tspin_lock_init(&ht->lock);\n\tmemcpy(&ht->p, params, sizeof(*params));\n\n\tif (params->min_size)\n\t\tht->p.min_size = roundup_pow_of_two(params->min_size);\n\n\t \n\tht->max_elems = 1u << 31;\n\n\tif (params->max_size) {\n\t\tht->p.max_size = rounddown_pow_of_two(params->max_size);\n\t\tif (ht->p.max_size < ht->max_elems / 2)\n\t\t\tht->max_elems = ht->p.max_size * 2;\n\t}\n\n\tht->p.min_size = max_t(u16, ht->p.min_size, HASH_MIN_SIZE);\n\n\tsize = rounded_hashtable_size(&ht->p);\n\n\tht->key_len = ht->p.key_len;\n\tif (!params->hashfn) {\n\t\tht->p.hashfn = jhash;\n\n\t\tif (!(ht->key_len & (sizeof(u32) - 1))) {\n\t\t\tht->key_len /= sizeof(u32);\n\t\t\tht->p.hashfn = rhashtable_jhash2;\n\t\t}\n\t}\n\n\t \n\ttbl = bucket_table_alloc(ht, size, GFP_KERNEL);\n\tif (unlikely(tbl == NULL)) {\n\t\tsize = max_t(u16, ht->p.min_size, HASH_MIN_SIZE);\n\t\ttbl = bucket_table_alloc(ht, size, GFP_KERNEL | __GFP_NOFAIL);\n\t}\n\n\tatomic_set(&ht->nelems, 0);\n\n\tRCU_INIT_POINTER(ht->tbl, tbl);\n\n\tINIT_WORK(&ht->run_work, rht_deferred_worker);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(rhashtable_init);\n\n \nint rhltable_init(struct rhltable *hlt, const struct rhashtable_params *params)\n{\n\tint err;\n\n\terr = rhashtable_init(&hlt->ht, params);\n\thlt->ht.rhlist = true;\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(rhltable_init);\n\nstatic void rhashtable_free_one(struct rhashtable *ht, struct rhash_head *obj,\n\t\t\t\tvoid (*free_fn)(void *ptr, void *arg),\n\t\t\t\tvoid *arg)\n{\n\tstruct rhlist_head *list;\n\n\tif (!ht->rhlist) {\n\t\tfree_fn(rht_obj(ht, obj), arg);\n\t\treturn;\n\t}\n\n\tlist = container_of(obj, struct rhlist_head, rhead);\n\tdo {\n\t\tobj = &list->rhead;\n\t\tlist = rht_dereference(list->next, ht);\n\t\tfree_fn(rht_obj(ht, obj), arg);\n\t} while (list);\n}\n\n \nvoid rhashtable_free_and_destroy(struct rhashtable *ht,\n\t\t\t\t void (*free_fn)(void *ptr, void *arg),\n\t\t\t\t void *arg)\n{\n\tstruct bucket_table *tbl, *next_tbl;\n\tunsigned int i;\n\n\tcancel_work_sync(&ht->run_work);\n\n\tmutex_lock(&ht->mutex);\n\ttbl = rht_dereference(ht->tbl, ht);\nrestart:\n\tif (free_fn) {\n\t\tfor (i = 0; i < tbl->size; i++) {\n\t\t\tstruct rhash_head *pos, *next;\n\n\t\t\tcond_resched();\n\t\t\tfor (pos = rht_ptr_exclusive(rht_bucket(tbl, i)),\n\t\t\t     next = !rht_is_a_nulls(pos) ?\n\t\t\t\t\trht_dereference(pos->next, ht) : NULL;\n\t\t\t     !rht_is_a_nulls(pos);\n\t\t\t     pos = next,\n\t\t\t     next = !rht_is_a_nulls(pos) ?\n\t\t\t\t\trht_dereference(pos->next, ht) : NULL)\n\t\t\t\trhashtable_free_one(ht, pos, free_fn, arg);\n\t\t}\n\t}\n\n\tnext_tbl = rht_dereference(tbl->future_tbl, ht);\n\tbucket_table_free(tbl);\n\tif (next_tbl) {\n\t\ttbl = next_tbl;\n\t\tgoto restart;\n\t}\n\tmutex_unlock(&ht->mutex);\n}\nEXPORT_SYMBOL_GPL(rhashtable_free_and_destroy);\n\nvoid rhashtable_destroy(struct rhashtable *ht)\n{\n\treturn rhashtable_free_and_destroy(ht, NULL, NULL);\n}\nEXPORT_SYMBOL_GPL(rhashtable_destroy);\n\nstruct rhash_lock_head __rcu **__rht_bucket_nested(\n\tconst struct bucket_table *tbl, unsigned int hash)\n{\n\tconst unsigned int shift = PAGE_SHIFT - ilog2(sizeof(void *));\n\tunsigned int index = hash & ((1 << tbl->nest) - 1);\n\tunsigned int size = tbl->size >> tbl->nest;\n\tunsigned int subhash = hash;\n\tunion nested_table *ntbl;\n\n\tntbl = nested_table_top(tbl);\n\tntbl = rht_dereference_bucket_rcu(ntbl[index].table, tbl, hash);\n\tsubhash >>= tbl->nest;\n\n\twhile (ntbl && size > (1 << shift)) {\n\t\tindex = subhash & ((1 << shift) - 1);\n\t\tntbl = rht_dereference_bucket_rcu(ntbl[index].table,\n\t\t\t\t\t\t  tbl, hash);\n\t\tsize >>= shift;\n\t\tsubhash >>= shift;\n\t}\n\n\tif (!ntbl)\n\t\treturn NULL;\n\n\treturn &ntbl[subhash].bucket;\n\n}\nEXPORT_SYMBOL_GPL(__rht_bucket_nested);\n\nstruct rhash_lock_head __rcu **rht_bucket_nested(\n\tconst struct bucket_table *tbl, unsigned int hash)\n{\n\tstatic struct rhash_lock_head __rcu *rhnull;\n\n\tif (!rhnull)\n\t\tINIT_RHT_NULLS_HEAD(rhnull);\n\treturn __rht_bucket_nested(tbl, hash) ?: &rhnull;\n}\nEXPORT_SYMBOL_GPL(rht_bucket_nested);\n\nstruct rhash_lock_head __rcu **rht_bucket_nested_insert(\n\tstruct rhashtable *ht, struct bucket_table *tbl, unsigned int hash)\n{\n\tconst unsigned int shift = PAGE_SHIFT - ilog2(sizeof(void *));\n\tunsigned int index = hash & ((1 << tbl->nest) - 1);\n\tunsigned int size = tbl->size >> tbl->nest;\n\tunion nested_table *ntbl;\n\n\tntbl = nested_table_top(tbl);\n\thash >>= tbl->nest;\n\tntbl = nested_table_alloc(ht, &ntbl[index].table,\n\t\t\t\t  size <= (1 << shift));\n\n\twhile (ntbl && size > (1 << shift)) {\n\t\tindex = hash & ((1 << shift) - 1);\n\t\tsize >>= shift;\n\t\thash >>= shift;\n\t\tntbl = nested_table_alloc(ht, &ntbl[index].table,\n\t\t\t\t\t  size <= (1 << shift));\n\t}\n\n\tif (!ntbl)\n\t\treturn NULL;\n\n\treturn &ntbl[hash].bucket;\n\n}\nEXPORT_SYMBOL_GPL(rht_bucket_nested_insert);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}