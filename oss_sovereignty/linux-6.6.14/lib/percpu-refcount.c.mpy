{
  "module_name": "percpu-refcount.c",
  "hash_id": "ca0b270cbdc3fd3a77ecf00694883f1130923c226df4e79a4981d98430e377fc",
  "original_prompt": "Ingested from linux-6.6.14/lib/percpu-refcount.c",
  "human_readable_source": "\n#define pr_fmt(fmt) \"%s: \" fmt, __func__\n\n#include <linux/kernel.h>\n#include <linux/sched.h>\n#include <linux/wait.h>\n#include <linux/slab.h>\n#include <linux/mm.h>\n#include <linux/percpu-refcount.h>\n\n \n\n#define PERCPU_COUNT_BIAS\t(1LU << (BITS_PER_LONG - 1))\n\nstatic DEFINE_SPINLOCK(percpu_ref_switch_lock);\nstatic DECLARE_WAIT_QUEUE_HEAD(percpu_ref_switch_waitq);\n\nstatic unsigned long __percpu *percpu_count_ptr(struct percpu_ref *ref)\n{\n\treturn (unsigned long __percpu *)\n\t\t(ref->percpu_count_ptr & ~__PERCPU_REF_ATOMIC_DEAD);\n}\n\n \nint percpu_ref_init(struct percpu_ref *ref, percpu_ref_func_t *release,\n\t\t    unsigned int flags, gfp_t gfp)\n{\n\tsize_t align = max_t(size_t, 1 << __PERCPU_REF_FLAG_BITS,\n\t\t\t     __alignof__(unsigned long));\n\tunsigned long start_count = 0;\n\tstruct percpu_ref_data *data;\n\n\tref->percpu_count_ptr = (unsigned long)\n\t\t__alloc_percpu_gfp(sizeof(unsigned long), align, gfp);\n\tif (!ref->percpu_count_ptr)\n\t\treturn -ENOMEM;\n\n\tdata = kzalloc(sizeof(*ref->data), gfp);\n\tif (!data) {\n\t\tfree_percpu((void __percpu *)ref->percpu_count_ptr);\n\t\tref->percpu_count_ptr = 0;\n\t\treturn -ENOMEM;\n\t}\n\n\tdata->force_atomic = flags & PERCPU_REF_INIT_ATOMIC;\n\tdata->allow_reinit = flags & PERCPU_REF_ALLOW_REINIT;\n\n\tif (flags & (PERCPU_REF_INIT_ATOMIC | PERCPU_REF_INIT_DEAD)) {\n\t\tref->percpu_count_ptr |= __PERCPU_REF_ATOMIC;\n\t\tdata->allow_reinit = true;\n\t} else {\n\t\tstart_count += PERCPU_COUNT_BIAS;\n\t}\n\n\tif (flags & PERCPU_REF_INIT_DEAD)\n\t\tref->percpu_count_ptr |= __PERCPU_REF_DEAD;\n\telse\n\t\tstart_count++;\n\n\tatomic_long_set(&data->count, start_count);\n\n\tdata->release = release;\n\tdata->confirm_switch = NULL;\n\tdata->ref = ref;\n\tref->data = data;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(percpu_ref_init);\n\nstatic void __percpu_ref_exit(struct percpu_ref *ref)\n{\n\tunsigned long __percpu *percpu_count = percpu_count_ptr(ref);\n\n\tif (percpu_count) {\n\t\t \n\t\tWARN_ON_ONCE(ref->data && ref->data->confirm_switch);\n\t\tfree_percpu(percpu_count);\n\t\tref->percpu_count_ptr = __PERCPU_REF_ATOMIC_DEAD;\n\t}\n}\n\n \nvoid percpu_ref_exit(struct percpu_ref *ref)\n{\n\tstruct percpu_ref_data *data = ref->data;\n\tunsigned long flags;\n\n\t__percpu_ref_exit(ref);\n\n\tif (!data)\n\t\treturn;\n\n\tspin_lock_irqsave(&percpu_ref_switch_lock, flags);\n\tref->percpu_count_ptr |= atomic_long_read(&ref->data->count) <<\n\t\t__PERCPU_REF_FLAG_BITS;\n\tref->data = NULL;\n\tspin_unlock_irqrestore(&percpu_ref_switch_lock, flags);\n\n\tkfree(data);\n}\nEXPORT_SYMBOL_GPL(percpu_ref_exit);\n\nstatic void percpu_ref_call_confirm_rcu(struct rcu_head *rcu)\n{\n\tstruct percpu_ref_data *data = container_of(rcu,\n\t\t\tstruct percpu_ref_data, rcu);\n\tstruct percpu_ref *ref = data->ref;\n\n\tdata->confirm_switch(ref);\n\tdata->confirm_switch = NULL;\n\twake_up_all(&percpu_ref_switch_waitq);\n\n\tif (!data->allow_reinit)\n\t\t__percpu_ref_exit(ref);\n\n\t \n\tpercpu_ref_put(ref);\n}\n\nstatic void percpu_ref_switch_to_atomic_rcu(struct rcu_head *rcu)\n{\n\tstruct percpu_ref_data *data = container_of(rcu,\n\t\t\tstruct percpu_ref_data, rcu);\n\tstruct percpu_ref *ref = data->ref;\n\tunsigned long __percpu *percpu_count = percpu_count_ptr(ref);\n\tstatic atomic_t underflows;\n\tunsigned long count = 0;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu)\n\t\tcount += *per_cpu_ptr(percpu_count, cpu);\n\n\tpr_debug(\"global %lu percpu %lu\\n\",\n\t\t atomic_long_read(&data->count), count);\n\n\t \n\tatomic_long_add((long)count - PERCPU_COUNT_BIAS, &data->count);\n\n\tif (WARN_ONCE(atomic_long_read(&data->count) <= 0,\n\t\t      \"percpu ref (%ps) <= 0 (%ld) after switching to atomic\",\n\t\t      data->release, atomic_long_read(&data->count)) &&\n\t    atomic_inc_return(&underflows) < 4) {\n\t\tpr_err(\"%s(): percpu_ref underflow\", __func__);\n\t\tmem_dump_obj(data);\n\t}\n\n\t \n\tpercpu_ref_call_confirm_rcu(rcu);\n}\n\nstatic void percpu_ref_noop_confirm_switch(struct percpu_ref *ref)\n{\n}\n\nstatic void __percpu_ref_switch_to_atomic(struct percpu_ref *ref,\n\t\t\t\t\t  percpu_ref_func_t *confirm_switch)\n{\n\tif (ref->percpu_count_ptr & __PERCPU_REF_ATOMIC) {\n\t\tif (confirm_switch)\n\t\t\tconfirm_switch(ref);\n\t\treturn;\n\t}\n\n\t \n\tref->percpu_count_ptr |= __PERCPU_REF_ATOMIC;\n\n\t \n\tref->data->confirm_switch = confirm_switch ?:\n\t\tpercpu_ref_noop_confirm_switch;\n\n\tpercpu_ref_get(ref);\t \n\tcall_rcu_hurry(&ref->data->rcu,\n\t\t       percpu_ref_switch_to_atomic_rcu);\n}\n\nstatic void __percpu_ref_switch_to_percpu(struct percpu_ref *ref)\n{\n\tunsigned long __percpu *percpu_count = percpu_count_ptr(ref);\n\tint cpu;\n\n\tBUG_ON(!percpu_count);\n\n\tif (!(ref->percpu_count_ptr & __PERCPU_REF_ATOMIC))\n\t\treturn;\n\n\tif (WARN_ON_ONCE(!ref->data->allow_reinit))\n\t\treturn;\n\n\tatomic_long_add(PERCPU_COUNT_BIAS, &ref->data->count);\n\n\t \n\tfor_each_possible_cpu(cpu)\n\t\t*per_cpu_ptr(percpu_count, cpu) = 0;\n\n\tsmp_store_release(&ref->percpu_count_ptr,\n\t\t\t  ref->percpu_count_ptr & ~__PERCPU_REF_ATOMIC);\n}\n\nstatic void __percpu_ref_switch_mode(struct percpu_ref *ref,\n\t\t\t\t     percpu_ref_func_t *confirm_switch)\n{\n\tstruct percpu_ref_data *data = ref->data;\n\n\tlockdep_assert_held(&percpu_ref_switch_lock);\n\n\t \n\twait_event_lock_irq(percpu_ref_switch_waitq, !data->confirm_switch,\n\t\t\t    percpu_ref_switch_lock);\n\n\tif (data->force_atomic || percpu_ref_is_dying(ref))\n\t\t__percpu_ref_switch_to_atomic(ref, confirm_switch);\n\telse\n\t\t__percpu_ref_switch_to_percpu(ref);\n}\n\n \nvoid percpu_ref_switch_to_atomic(struct percpu_ref *ref,\n\t\t\t\t percpu_ref_func_t *confirm_switch)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&percpu_ref_switch_lock, flags);\n\n\tref->data->force_atomic = true;\n\t__percpu_ref_switch_mode(ref, confirm_switch);\n\n\tspin_unlock_irqrestore(&percpu_ref_switch_lock, flags);\n}\nEXPORT_SYMBOL_GPL(percpu_ref_switch_to_atomic);\n\n \nvoid percpu_ref_switch_to_atomic_sync(struct percpu_ref *ref)\n{\n\tpercpu_ref_switch_to_atomic(ref, NULL);\n\twait_event(percpu_ref_switch_waitq, !ref->data->confirm_switch);\n}\nEXPORT_SYMBOL_GPL(percpu_ref_switch_to_atomic_sync);\n\n \nvoid percpu_ref_switch_to_percpu(struct percpu_ref *ref)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&percpu_ref_switch_lock, flags);\n\n\tref->data->force_atomic = false;\n\t__percpu_ref_switch_mode(ref, NULL);\n\n\tspin_unlock_irqrestore(&percpu_ref_switch_lock, flags);\n}\nEXPORT_SYMBOL_GPL(percpu_ref_switch_to_percpu);\n\n \nvoid percpu_ref_kill_and_confirm(struct percpu_ref *ref,\n\t\t\t\t percpu_ref_func_t *confirm_kill)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&percpu_ref_switch_lock, flags);\n\n\tWARN_ONCE(percpu_ref_is_dying(ref),\n\t\t  \"%s called more than once on %ps!\", __func__,\n\t\t  ref->data->release);\n\n\tref->percpu_count_ptr |= __PERCPU_REF_DEAD;\n\t__percpu_ref_switch_mode(ref, confirm_kill);\n\tpercpu_ref_put(ref);\n\n\tspin_unlock_irqrestore(&percpu_ref_switch_lock, flags);\n}\nEXPORT_SYMBOL_GPL(percpu_ref_kill_and_confirm);\n\n \nbool percpu_ref_is_zero(struct percpu_ref *ref)\n{\n\tunsigned long __percpu *percpu_count;\n\tunsigned long count, flags;\n\n\tif (__ref_is_percpu(ref, &percpu_count))\n\t\treturn false;\n\n\t \n\tspin_lock_irqsave(&percpu_ref_switch_lock, flags);\n\tif (ref->data)\n\t\tcount = atomic_long_read(&ref->data->count);\n\telse\n\t\tcount = ref->percpu_count_ptr >> __PERCPU_REF_FLAG_BITS;\n\tspin_unlock_irqrestore(&percpu_ref_switch_lock, flags);\n\n\treturn count == 0;\n}\nEXPORT_SYMBOL_GPL(percpu_ref_is_zero);\n\n \nvoid percpu_ref_reinit(struct percpu_ref *ref)\n{\n\tWARN_ON_ONCE(!percpu_ref_is_zero(ref));\n\n\tpercpu_ref_resurrect(ref);\n}\nEXPORT_SYMBOL_GPL(percpu_ref_reinit);\n\n \nvoid percpu_ref_resurrect(struct percpu_ref *ref)\n{\n\tunsigned long __percpu *percpu_count;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&percpu_ref_switch_lock, flags);\n\n\tWARN_ON_ONCE(!percpu_ref_is_dying(ref));\n\tWARN_ON_ONCE(__ref_is_percpu(ref, &percpu_count));\n\n\tref->percpu_count_ptr &= ~__PERCPU_REF_DEAD;\n\tpercpu_ref_get(ref);\n\t__percpu_ref_switch_mode(ref, NULL);\n\n\tspin_unlock_irqrestore(&percpu_ref_switch_lock, flags);\n}\nEXPORT_SYMBOL_GPL(percpu_ref_resurrect);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}