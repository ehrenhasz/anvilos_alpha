{
  "module_name": "scatterlist.c",
  "hash_id": "8f42c5555329d6317e1e4520f56ddcee7fea1edea1f5b319e13fc2ae9b29c36e",
  "original_prompt": "Ingested from linux-6.6.14/lib/scatterlist.c",
  "human_readable_source": "\n \n#include <linux/export.h>\n#include <linux/slab.h>\n#include <linux/scatterlist.h>\n#include <linux/highmem.h>\n#include <linux/kmemleak.h>\n#include <linux/bvec.h>\n#include <linux/uio.h>\n\n \nstruct scatterlist *sg_next(struct scatterlist *sg)\n{\n\tif (sg_is_last(sg))\n\t\treturn NULL;\n\n\tsg++;\n\tif (unlikely(sg_is_chain(sg)))\n\t\tsg = sg_chain_ptr(sg);\n\n\treturn sg;\n}\nEXPORT_SYMBOL(sg_next);\n\n \nint sg_nents(struct scatterlist *sg)\n{\n\tint nents;\n\tfor (nents = 0; sg; sg = sg_next(sg))\n\t\tnents++;\n\treturn nents;\n}\nEXPORT_SYMBOL(sg_nents);\n\n \nint sg_nents_for_len(struct scatterlist *sg, u64 len)\n{\n\tint nents;\n\tu64 total;\n\n\tif (!len)\n\t\treturn 0;\n\n\tfor (nents = 0, total = 0; sg; sg = sg_next(sg)) {\n\t\tnents++;\n\t\ttotal += sg->length;\n\t\tif (total >= len)\n\t\t\treturn nents;\n\t}\n\n\treturn -EINVAL;\n}\nEXPORT_SYMBOL(sg_nents_for_len);\n\n \nstruct scatterlist *sg_last(struct scatterlist *sgl, unsigned int nents)\n{\n\tstruct scatterlist *sg, *ret = NULL;\n\tunsigned int i;\n\n\tfor_each_sg(sgl, sg, nents, i)\n\t\tret = sg;\n\n\tBUG_ON(!sg_is_last(ret));\n\treturn ret;\n}\nEXPORT_SYMBOL(sg_last);\n\n \nvoid sg_init_table(struct scatterlist *sgl, unsigned int nents)\n{\n\tmemset(sgl, 0, sizeof(*sgl) * nents);\n\tsg_init_marker(sgl, nents);\n}\nEXPORT_SYMBOL(sg_init_table);\n\n \nvoid sg_init_one(struct scatterlist *sg, const void *buf, unsigned int buflen)\n{\n\tsg_init_table(sg, 1);\n\tsg_set_buf(sg, buf, buflen);\n}\nEXPORT_SYMBOL(sg_init_one);\n\n \nstatic struct scatterlist *sg_kmalloc(unsigned int nents, gfp_t gfp_mask)\n{\n\tif (nents == SG_MAX_SINGLE_ALLOC) {\n\t\t \n\t\tvoid *ptr = (void *) __get_free_page(gfp_mask);\n\t\tkmemleak_alloc(ptr, PAGE_SIZE, 1, gfp_mask);\n\t\treturn ptr;\n\t} else\n\t\treturn kmalloc_array(nents, sizeof(struct scatterlist),\n\t\t\t\t     gfp_mask);\n}\n\nstatic void sg_kfree(struct scatterlist *sg, unsigned int nents)\n{\n\tif (nents == SG_MAX_SINGLE_ALLOC) {\n\t\tkmemleak_free(sg);\n\t\tfree_page((unsigned long) sg);\n\t} else\n\t\tkfree(sg);\n}\n\n \nvoid __sg_free_table(struct sg_table *table, unsigned int max_ents,\n\t\t     unsigned int nents_first_chunk, sg_free_fn *free_fn,\n\t\t     unsigned int num_ents)\n{\n\tstruct scatterlist *sgl, *next;\n\tunsigned curr_max_ents = nents_first_chunk ?: max_ents;\n\n\tif (unlikely(!table->sgl))\n\t\treturn;\n\n\tsgl = table->sgl;\n\twhile (num_ents) {\n\t\tunsigned int alloc_size = num_ents;\n\t\tunsigned int sg_size;\n\n\t\t \n\t\tif (alloc_size > curr_max_ents) {\n\t\t\tnext = sg_chain_ptr(&sgl[curr_max_ents - 1]);\n\t\t\talloc_size = curr_max_ents;\n\t\t\tsg_size = alloc_size - 1;\n\t\t} else {\n\t\t\tsg_size = alloc_size;\n\t\t\tnext = NULL;\n\t\t}\n\n\t\tnum_ents -= sg_size;\n\t\tif (nents_first_chunk)\n\t\t\tnents_first_chunk = 0;\n\t\telse\n\t\t\tfree_fn(sgl, alloc_size);\n\t\tsgl = next;\n\t\tcurr_max_ents = max_ents;\n\t}\n\n\ttable->sgl = NULL;\n}\nEXPORT_SYMBOL(__sg_free_table);\n\n \nvoid sg_free_append_table(struct sg_append_table *table)\n{\n\t__sg_free_table(&table->sgt, SG_MAX_SINGLE_ALLOC, 0, sg_kfree,\n\t\t\ttable->total_nents);\n}\nEXPORT_SYMBOL(sg_free_append_table);\n\n\n \nvoid sg_free_table(struct sg_table *table)\n{\n\t__sg_free_table(table, SG_MAX_SINGLE_ALLOC, 0, sg_kfree,\n\t\t\ttable->orig_nents);\n}\nEXPORT_SYMBOL(sg_free_table);\n\n \nint __sg_alloc_table(struct sg_table *table, unsigned int nents,\n\t\t     unsigned int max_ents, struct scatterlist *first_chunk,\n\t\t     unsigned int nents_first_chunk, gfp_t gfp_mask,\n\t\t     sg_alloc_fn *alloc_fn)\n{\n\tstruct scatterlist *sg, *prv;\n\tunsigned int left;\n\tunsigned curr_max_ents = nents_first_chunk ?: max_ents;\n\tunsigned prv_max_ents;\n\n\tmemset(table, 0, sizeof(*table));\n\n\tif (nents == 0)\n\t\treturn -EINVAL;\n#ifdef CONFIG_ARCH_NO_SG_CHAIN\n\tif (WARN_ON_ONCE(nents > max_ents))\n\t\treturn -EINVAL;\n#endif\n\n\tleft = nents;\n\tprv = NULL;\n\tdo {\n\t\tunsigned int sg_size, alloc_size = left;\n\n\t\tif (alloc_size > curr_max_ents) {\n\t\t\talloc_size = curr_max_ents;\n\t\t\tsg_size = alloc_size - 1;\n\t\t} else\n\t\t\tsg_size = alloc_size;\n\n\t\tleft -= sg_size;\n\n\t\tif (first_chunk) {\n\t\t\tsg = first_chunk;\n\t\t\tfirst_chunk = NULL;\n\t\t} else {\n\t\t\tsg = alloc_fn(alloc_size, gfp_mask);\n\t\t}\n\t\tif (unlikely(!sg)) {\n\t\t\t \n\t\t\tif (prv)\n\t\t\t\ttable->nents = ++table->orig_nents;\n\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tsg_init_table(sg, alloc_size);\n\t\ttable->nents = table->orig_nents += sg_size;\n\n\t\t \n\t\tif (prv)\n\t\t\tsg_chain(prv, prv_max_ents, sg);\n\t\telse\n\t\t\ttable->sgl = sg;\n\n\t\t \n\t\tif (!left)\n\t\t\tsg_mark_end(&sg[sg_size - 1]);\n\n\t\tprv = sg;\n\t\tprv_max_ents = curr_max_ents;\n\t\tcurr_max_ents = max_ents;\n\t} while (left);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(__sg_alloc_table);\n\n \nint sg_alloc_table(struct sg_table *table, unsigned int nents, gfp_t gfp_mask)\n{\n\tint ret;\n\n\tret = __sg_alloc_table(table, nents, SG_MAX_SINGLE_ALLOC,\n\t\t\t       NULL, 0, gfp_mask, sg_kmalloc);\n\tif (unlikely(ret))\n\t\tsg_free_table(table);\n\treturn ret;\n}\nEXPORT_SYMBOL(sg_alloc_table);\n\nstatic struct scatterlist *get_next_sg(struct sg_append_table *table,\n\t\t\t\t       struct scatterlist *cur,\n\t\t\t\t       unsigned long needed_sges,\n\t\t\t\t       gfp_t gfp_mask)\n{\n\tstruct scatterlist *new_sg, *next_sg;\n\tunsigned int alloc_size;\n\n\tif (cur) {\n\t\tnext_sg = sg_next(cur);\n\t\t \n\t\tif (!sg_is_last(next_sg) || needed_sges == 1)\n\t\t\treturn next_sg;\n\t}\n\n\talloc_size = min_t(unsigned long, needed_sges, SG_MAX_SINGLE_ALLOC);\n\tnew_sg = sg_kmalloc(alloc_size, gfp_mask);\n\tif (!new_sg)\n\t\treturn ERR_PTR(-ENOMEM);\n\tsg_init_table(new_sg, alloc_size);\n\tif (cur) {\n\t\ttable->total_nents += alloc_size - 1;\n\t\t__sg_chain(next_sg, new_sg);\n\t} else {\n\t\ttable->sgt.sgl = new_sg;\n\t\ttable->total_nents = alloc_size;\n\t}\n\treturn new_sg;\n}\n\nstatic bool pages_are_mergeable(struct page *a, struct page *b)\n{\n\tif (page_to_pfn(a) != page_to_pfn(b) + 1)\n\t\treturn false;\n\tif (!zone_device_pages_have_same_pgmap(a, b))\n\t\treturn false;\n\treturn true;\n}\n\n \nint sg_alloc_append_table_from_pages(struct sg_append_table *sgt_append,\n\t\tstruct page **pages, unsigned int n_pages, unsigned int offset,\n\t\tunsigned long size, unsigned int max_segment,\n\t\tunsigned int left_pages, gfp_t gfp_mask)\n{\n\tunsigned int chunks, cur_page, seg_len, i, prv_len = 0;\n\tunsigned int added_nents = 0;\n\tstruct scatterlist *s = sgt_append->prv;\n\tstruct page *last_pg;\n\n\t \n\tmax_segment = ALIGN_DOWN(max_segment, PAGE_SIZE);\n\tif (WARN_ON(max_segment < PAGE_SIZE))\n\t\treturn -EINVAL;\n\n\tif (IS_ENABLED(CONFIG_ARCH_NO_SG_CHAIN) && sgt_append->prv)\n\t\treturn -EOPNOTSUPP;\n\n\tif (sgt_append->prv) {\n\t\tunsigned long next_pfn = (page_to_phys(sg_page(sgt_append->prv)) +\n\t\t\tsgt_append->prv->offset + sgt_append->prv->length) / PAGE_SIZE;\n\n\t\tif (WARN_ON(offset))\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tprv_len = sgt_append->prv->length;\n\t\tif (page_to_pfn(pages[0]) == next_pfn) {\n\t\t\tlast_pg = pfn_to_page(next_pfn - 1);\n\t\t\twhile (n_pages && pages_are_mergeable(pages[0], last_pg)) {\n\t\t\t\tif (sgt_append->prv->length + PAGE_SIZE > max_segment)\n\t\t\t\t\tbreak;\n\t\t\t\tsgt_append->prv->length += PAGE_SIZE;\n\t\t\t\tlast_pg = pages[0];\n\t\t\t\tpages++;\n\t\t\t\tn_pages--;\n\t\t\t}\n\t\t\tif (!n_pages)\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\n\t \n\tchunks = 1;\n\tseg_len = 0;\n\tfor (i = 1; i < n_pages; i++) {\n\t\tseg_len += PAGE_SIZE;\n\t\tif (seg_len >= max_segment ||\n\t\t    !pages_are_mergeable(pages[i], pages[i - 1])) {\n\t\t\tchunks++;\n\t\t\tseg_len = 0;\n\t\t}\n\t}\n\n\t \n\tcur_page = 0;\n\tfor (i = 0; i < chunks; i++) {\n\t\tunsigned int j, chunk_size;\n\n\t\t \n\t\tseg_len = 0;\n\t\tfor (j = cur_page + 1; j < n_pages; j++) {\n\t\t\tseg_len += PAGE_SIZE;\n\t\t\tif (seg_len >= max_segment ||\n\t\t\t    !pages_are_mergeable(pages[j], pages[j - 1]))\n\t\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\ts = get_next_sg(sgt_append, s, chunks - i + left_pages,\n\t\t\t\tgfp_mask);\n\t\tif (IS_ERR(s)) {\n\t\t\t \n\t\t\tif (sgt_append->prv)\n\t\t\t\tsgt_append->prv->length = prv_len;\n\t\t\treturn PTR_ERR(s);\n\t\t}\n\t\tchunk_size = ((j - cur_page) << PAGE_SHIFT) - offset;\n\t\tsg_set_page(s, pages[cur_page],\n\t\t\t    min_t(unsigned long, size, chunk_size), offset);\n\t\tadded_nents++;\n\t\tsize -= chunk_size;\n\t\toffset = 0;\n\t\tcur_page = j;\n\t}\n\tsgt_append->sgt.nents += added_nents;\n\tsgt_append->sgt.orig_nents = sgt_append->sgt.nents;\n\tsgt_append->prv = s;\nout:\n\tif (!left_pages)\n\t\tsg_mark_end(s);\n\treturn 0;\n}\nEXPORT_SYMBOL(sg_alloc_append_table_from_pages);\n\n \nint sg_alloc_table_from_pages_segment(struct sg_table *sgt, struct page **pages,\n\t\t\t\tunsigned int n_pages, unsigned int offset,\n\t\t\t\tunsigned long size, unsigned int max_segment,\n\t\t\t\tgfp_t gfp_mask)\n{\n\tstruct sg_append_table append = {};\n\tint err;\n\n\terr = sg_alloc_append_table_from_pages(&append, pages, n_pages, offset,\n\t\t\t\t\t       size, max_segment, 0, gfp_mask);\n\tif (err) {\n\t\tsg_free_append_table(&append);\n\t\treturn err;\n\t}\n\tmemcpy(sgt, &append.sgt, sizeof(*sgt));\n\tWARN_ON(append.total_nents != sgt->orig_nents);\n\treturn 0;\n}\nEXPORT_SYMBOL(sg_alloc_table_from_pages_segment);\n\n#ifdef CONFIG_SGL_ALLOC\n\n \nstruct scatterlist *sgl_alloc_order(unsigned long long length,\n\t\t\t\t    unsigned int order, bool chainable,\n\t\t\t\t    gfp_t gfp, unsigned int *nent_p)\n{\n\tstruct scatterlist *sgl, *sg;\n\tstruct page *page;\n\tunsigned int nent, nalloc;\n\tu32 elem_len;\n\n\tnent = round_up(length, PAGE_SIZE << order) >> (PAGE_SHIFT + order);\n\t \n\tif (length > (nent << (PAGE_SHIFT + order)))\n\t\treturn NULL;\n\tnalloc = nent;\n\tif (chainable) {\n\t\t \n\t\tif (nalloc + 1 < nalloc)\n\t\t\treturn NULL;\n\t\tnalloc++;\n\t}\n\tsgl = kmalloc_array(nalloc, sizeof(struct scatterlist),\n\t\t\t    gfp & ~GFP_DMA);\n\tif (!sgl)\n\t\treturn NULL;\n\n\tsg_init_table(sgl, nalloc);\n\tsg = sgl;\n\twhile (length) {\n\t\telem_len = min_t(u64, length, PAGE_SIZE << order);\n\t\tpage = alloc_pages(gfp, order);\n\t\tif (!page) {\n\t\t\tsgl_free_order(sgl, order);\n\t\t\treturn NULL;\n\t\t}\n\n\t\tsg_set_page(sg, page, elem_len, 0);\n\t\tlength -= elem_len;\n\t\tsg = sg_next(sg);\n\t}\n\tWARN_ONCE(length, \"length = %lld\\n\", length);\n\tif (nent_p)\n\t\t*nent_p = nent;\n\treturn sgl;\n}\nEXPORT_SYMBOL(sgl_alloc_order);\n\n \nstruct scatterlist *sgl_alloc(unsigned long long length, gfp_t gfp,\n\t\t\t      unsigned int *nent_p)\n{\n\treturn sgl_alloc_order(length, 0, false, gfp, nent_p);\n}\nEXPORT_SYMBOL(sgl_alloc);\n\n \nvoid sgl_free_n_order(struct scatterlist *sgl, int nents, int order)\n{\n\tstruct scatterlist *sg;\n\tstruct page *page;\n\tint i;\n\n\tfor_each_sg(sgl, sg, nents, i) {\n\t\tif (!sg)\n\t\t\tbreak;\n\t\tpage = sg_page(sg);\n\t\tif (page)\n\t\t\t__free_pages(page, order);\n\t}\n\tkfree(sgl);\n}\nEXPORT_SYMBOL(sgl_free_n_order);\n\n \nvoid sgl_free_order(struct scatterlist *sgl, int order)\n{\n\tsgl_free_n_order(sgl, INT_MAX, order);\n}\nEXPORT_SYMBOL(sgl_free_order);\n\n \nvoid sgl_free(struct scatterlist *sgl)\n{\n\tsgl_free_order(sgl, 0);\n}\nEXPORT_SYMBOL(sgl_free);\n\n#endif  \n\nvoid __sg_page_iter_start(struct sg_page_iter *piter,\n\t\t\t  struct scatterlist *sglist, unsigned int nents,\n\t\t\t  unsigned long pgoffset)\n{\n\tpiter->__pg_advance = 0;\n\tpiter->__nents = nents;\n\n\tpiter->sg = sglist;\n\tpiter->sg_pgoffset = pgoffset;\n}\nEXPORT_SYMBOL(__sg_page_iter_start);\n\nstatic int sg_page_count(struct scatterlist *sg)\n{\n\treturn PAGE_ALIGN(sg->offset + sg->length) >> PAGE_SHIFT;\n}\n\nbool __sg_page_iter_next(struct sg_page_iter *piter)\n{\n\tif (!piter->__nents || !piter->sg)\n\t\treturn false;\n\n\tpiter->sg_pgoffset += piter->__pg_advance;\n\tpiter->__pg_advance = 1;\n\n\twhile (piter->sg_pgoffset >= sg_page_count(piter->sg)) {\n\t\tpiter->sg_pgoffset -= sg_page_count(piter->sg);\n\t\tpiter->sg = sg_next(piter->sg);\n\t\tif (!--piter->__nents || !piter->sg)\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\nEXPORT_SYMBOL(__sg_page_iter_next);\n\nstatic int sg_dma_page_count(struct scatterlist *sg)\n{\n\treturn PAGE_ALIGN(sg->offset + sg_dma_len(sg)) >> PAGE_SHIFT;\n}\n\nbool __sg_page_iter_dma_next(struct sg_dma_page_iter *dma_iter)\n{\n\tstruct sg_page_iter *piter = &dma_iter->base;\n\n\tif (!piter->__nents || !piter->sg)\n\t\treturn false;\n\n\tpiter->sg_pgoffset += piter->__pg_advance;\n\tpiter->__pg_advance = 1;\n\n\twhile (piter->sg_pgoffset >= sg_dma_page_count(piter->sg)) {\n\t\tpiter->sg_pgoffset -= sg_dma_page_count(piter->sg);\n\t\tpiter->sg = sg_next(piter->sg);\n\t\tif (!--piter->__nents || !piter->sg)\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\nEXPORT_SYMBOL(__sg_page_iter_dma_next);\n\n \nvoid sg_miter_start(struct sg_mapping_iter *miter, struct scatterlist *sgl,\n\t\t    unsigned int nents, unsigned int flags)\n{\n\tmemset(miter, 0, sizeof(struct sg_mapping_iter));\n\n\t__sg_page_iter_start(&miter->piter, sgl, nents, 0);\n\tWARN_ON(!(flags & (SG_MITER_TO_SG | SG_MITER_FROM_SG)));\n\tmiter->__flags = flags;\n}\nEXPORT_SYMBOL(sg_miter_start);\n\nstatic bool sg_miter_get_next_page(struct sg_mapping_iter *miter)\n{\n\tif (!miter->__remaining) {\n\t\tstruct scatterlist *sg;\n\n\t\tif (!__sg_page_iter_next(&miter->piter))\n\t\t\treturn false;\n\n\t\tsg = miter->piter.sg;\n\n\t\tmiter->__offset = miter->piter.sg_pgoffset ? 0 : sg->offset;\n\t\tmiter->piter.sg_pgoffset += miter->__offset >> PAGE_SHIFT;\n\t\tmiter->__offset &= PAGE_SIZE - 1;\n\t\tmiter->__remaining = sg->offset + sg->length -\n\t\t\t\t     (miter->piter.sg_pgoffset << PAGE_SHIFT) -\n\t\t\t\t     miter->__offset;\n\t\tmiter->__remaining = min_t(unsigned long, miter->__remaining,\n\t\t\t\t\t   PAGE_SIZE - miter->__offset);\n\t}\n\n\treturn true;\n}\n\n \nbool sg_miter_skip(struct sg_mapping_iter *miter, off_t offset)\n{\n\tsg_miter_stop(miter);\n\n\twhile (offset) {\n\t\toff_t consumed;\n\n\t\tif (!sg_miter_get_next_page(miter))\n\t\t\treturn false;\n\n\t\tconsumed = min_t(off_t, offset, miter->__remaining);\n\t\tmiter->__offset += consumed;\n\t\tmiter->__remaining -= consumed;\n\t\toffset -= consumed;\n\t}\n\n\treturn true;\n}\nEXPORT_SYMBOL(sg_miter_skip);\n\n \nbool sg_miter_next(struct sg_mapping_iter *miter)\n{\n\tsg_miter_stop(miter);\n\n\t \n\tif (!sg_miter_get_next_page(miter))\n\t\treturn false;\n\n\tmiter->page = sg_page_iter_page(&miter->piter);\n\tmiter->consumed = miter->length = miter->__remaining;\n\n\tif (miter->__flags & SG_MITER_ATOMIC)\n\t\tmiter->addr = kmap_atomic(miter->page) + miter->__offset;\n\telse\n\t\tmiter->addr = kmap(miter->page) + miter->__offset;\n\n\treturn true;\n}\nEXPORT_SYMBOL(sg_miter_next);\n\n \nvoid sg_miter_stop(struct sg_mapping_iter *miter)\n{\n\tWARN_ON(miter->consumed > miter->length);\n\n\t \n\tif (miter->addr) {\n\t\tmiter->__offset += miter->consumed;\n\t\tmiter->__remaining -= miter->consumed;\n\n\t\tif (miter->__flags & SG_MITER_TO_SG)\n\t\t\tflush_dcache_page(miter->page);\n\n\t\tif (miter->__flags & SG_MITER_ATOMIC) {\n\t\t\tWARN_ON_ONCE(!pagefault_disabled());\n\t\t\tkunmap_atomic(miter->addr);\n\t\t} else\n\t\t\tkunmap(miter->page);\n\n\t\tmiter->page = NULL;\n\t\tmiter->addr = NULL;\n\t\tmiter->length = 0;\n\t\tmiter->consumed = 0;\n\t}\n}\nEXPORT_SYMBOL(sg_miter_stop);\n\n \nsize_t sg_copy_buffer(struct scatterlist *sgl, unsigned int nents, void *buf,\n\t\t      size_t buflen, off_t skip, bool to_buffer)\n{\n\tunsigned int offset = 0;\n\tstruct sg_mapping_iter miter;\n\tunsigned int sg_flags = SG_MITER_ATOMIC;\n\n\tif (to_buffer)\n\t\tsg_flags |= SG_MITER_FROM_SG;\n\telse\n\t\tsg_flags |= SG_MITER_TO_SG;\n\n\tsg_miter_start(&miter, sgl, nents, sg_flags);\n\n\tif (!sg_miter_skip(&miter, skip))\n\t\treturn 0;\n\n\twhile ((offset < buflen) && sg_miter_next(&miter)) {\n\t\tunsigned int len;\n\n\t\tlen = min(miter.length, buflen - offset);\n\n\t\tif (to_buffer)\n\t\t\tmemcpy(buf + offset, miter.addr, len);\n\t\telse\n\t\t\tmemcpy(miter.addr, buf + offset, len);\n\n\t\toffset += len;\n\t}\n\n\tsg_miter_stop(&miter);\n\n\treturn offset;\n}\nEXPORT_SYMBOL(sg_copy_buffer);\n\n \nsize_t sg_copy_from_buffer(struct scatterlist *sgl, unsigned int nents,\n\t\t\t   const void *buf, size_t buflen)\n{\n\treturn sg_copy_buffer(sgl, nents, (void *)buf, buflen, 0, false);\n}\nEXPORT_SYMBOL(sg_copy_from_buffer);\n\n \nsize_t sg_copy_to_buffer(struct scatterlist *sgl, unsigned int nents,\n\t\t\t void *buf, size_t buflen)\n{\n\treturn sg_copy_buffer(sgl, nents, buf, buflen, 0, true);\n}\nEXPORT_SYMBOL(sg_copy_to_buffer);\n\n \nsize_t sg_pcopy_from_buffer(struct scatterlist *sgl, unsigned int nents,\n\t\t\t    const void *buf, size_t buflen, off_t skip)\n{\n\treturn sg_copy_buffer(sgl, nents, (void *)buf, buflen, skip, false);\n}\nEXPORT_SYMBOL(sg_pcopy_from_buffer);\n\n \nsize_t sg_pcopy_to_buffer(struct scatterlist *sgl, unsigned int nents,\n\t\t\t  void *buf, size_t buflen, off_t skip)\n{\n\treturn sg_copy_buffer(sgl, nents, buf, buflen, skip, true);\n}\nEXPORT_SYMBOL(sg_pcopy_to_buffer);\n\n \nsize_t sg_zero_buffer(struct scatterlist *sgl, unsigned int nents,\n\t\t       size_t buflen, off_t skip)\n{\n\tunsigned int offset = 0;\n\tstruct sg_mapping_iter miter;\n\tunsigned int sg_flags = SG_MITER_ATOMIC | SG_MITER_TO_SG;\n\n\tsg_miter_start(&miter, sgl, nents, sg_flags);\n\n\tif (!sg_miter_skip(&miter, skip))\n\t\treturn false;\n\n\twhile (offset < buflen && sg_miter_next(&miter)) {\n\t\tunsigned int len;\n\n\t\tlen = min(miter.length, buflen - offset);\n\t\tmemset(miter.addr, 0, len);\n\n\t\toffset += len;\n\t}\n\n\tsg_miter_stop(&miter);\n\treturn offset;\n}\nEXPORT_SYMBOL(sg_zero_buffer);\n\n \nstatic ssize_t extract_user_to_sg(struct iov_iter *iter,\n\t\t\t\t  ssize_t maxsize,\n\t\t\t\t  struct sg_table *sgtable,\n\t\t\t\t  unsigned int sg_max,\n\t\t\t\t  iov_iter_extraction_t extraction_flags)\n{\n\tstruct scatterlist *sg = sgtable->sgl + sgtable->nents;\n\tstruct page **pages;\n\tunsigned int npages;\n\tssize_t ret = 0, res;\n\tsize_t len, off;\n\n\t \n\tpages = (void *)sgtable->sgl +\n\t\tarray_size(sg_max, sizeof(struct scatterlist));\n\tpages -= sg_max;\n\n\tdo {\n\t\tres = iov_iter_extract_pages(iter, &pages, maxsize, sg_max,\n\t\t\t\t\t     extraction_flags, &off);\n\t\tif (res < 0)\n\t\t\tgoto failed;\n\n\t\tlen = res;\n\t\tmaxsize -= len;\n\t\tret += len;\n\t\tnpages = DIV_ROUND_UP(off + len, PAGE_SIZE);\n\t\tsg_max -= npages;\n\n\t\tfor (; npages > 0; npages--) {\n\t\t\tstruct page *page = *pages;\n\t\t\tsize_t seg = min_t(size_t, PAGE_SIZE - off, len);\n\n\t\t\t*pages++ = NULL;\n\t\t\tsg_set_page(sg, page, seg, off);\n\t\t\tsgtable->nents++;\n\t\t\tsg++;\n\t\t\tlen -= seg;\n\t\t\toff = 0;\n\t\t}\n\t} while (maxsize > 0 && sg_max > 0);\n\n\treturn ret;\n\nfailed:\n\twhile (sgtable->nents > sgtable->orig_nents)\n\t\tunpin_user_page(sg_page(&sgtable->sgl[--sgtable->nents]));\n\treturn res;\n}\n\n \nstatic ssize_t extract_bvec_to_sg(struct iov_iter *iter,\n\t\t\t\t  ssize_t maxsize,\n\t\t\t\t  struct sg_table *sgtable,\n\t\t\t\t  unsigned int sg_max,\n\t\t\t\t  iov_iter_extraction_t extraction_flags)\n{\n\tconst struct bio_vec *bv = iter->bvec;\n\tstruct scatterlist *sg = sgtable->sgl + sgtable->nents;\n\tunsigned long start = iter->iov_offset;\n\tunsigned int i;\n\tssize_t ret = 0;\n\n\tfor (i = 0; i < iter->nr_segs; i++) {\n\t\tsize_t off, len;\n\n\t\tlen = bv[i].bv_len;\n\t\tif (start >= len) {\n\t\t\tstart -= len;\n\t\t\tcontinue;\n\t\t}\n\n\t\tlen = min_t(size_t, maxsize, len - start);\n\t\toff = bv[i].bv_offset + start;\n\n\t\tsg_set_page(sg, bv[i].bv_page, len, off);\n\t\tsgtable->nents++;\n\t\tsg++;\n\t\tsg_max--;\n\n\t\tret += len;\n\t\tmaxsize -= len;\n\t\tif (maxsize <= 0 || sg_max == 0)\n\t\t\tbreak;\n\t\tstart = 0;\n\t}\n\n\tif (ret > 0)\n\t\tiov_iter_advance(iter, ret);\n\treturn ret;\n}\n\n \nstatic ssize_t extract_kvec_to_sg(struct iov_iter *iter,\n\t\t\t\t  ssize_t maxsize,\n\t\t\t\t  struct sg_table *sgtable,\n\t\t\t\t  unsigned int sg_max,\n\t\t\t\t  iov_iter_extraction_t extraction_flags)\n{\n\tconst struct kvec *kv = iter->kvec;\n\tstruct scatterlist *sg = sgtable->sgl + sgtable->nents;\n\tunsigned long start = iter->iov_offset;\n\tunsigned int i;\n\tssize_t ret = 0;\n\n\tfor (i = 0; i < iter->nr_segs; i++) {\n\t\tstruct page *page;\n\t\tunsigned long kaddr;\n\t\tsize_t off, len, seg;\n\n\t\tlen = kv[i].iov_len;\n\t\tif (start >= len) {\n\t\t\tstart -= len;\n\t\t\tcontinue;\n\t\t}\n\n\t\tkaddr = (unsigned long)kv[i].iov_base + start;\n\t\toff = kaddr & ~PAGE_MASK;\n\t\tlen = min_t(size_t, maxsize, len - start);\n\t\tkaddr &= PAGE_MASK;\n\n\t\tmaxsize -= len;\n\t\tret += len;\n\t\tdo {\n\t\t\tseg = min_t(size_t, len, PAGE_SIZE - off);\n\t\t\tif (is_vmalloc_or_module_addr((void *)kaddr))\n\t\t\t\tpage = vmalloc_to_page((void *)kaddr);\n\t\t\telse\n\t\t\t\tpage = virt_to_page((void *)kaddr);\n\n\t\t\tsg_set_page(sg, page, len, off);\n\t\t\tsgtable->nents++;\n\t\t\tsg++;\n\t\t\tsg_max--;\n\n\t\t\tlen -= seg;\n\t\t\tkaddr += PAGE_SIZE;\n\t\t\toff = 0;\n\t\t} while (len > 0 && sg_max > 0);\n\n\t\tif (maxsize <= 0 || sg_max == 0)\n\t\t\tbreak;\n\t\tstart = 0;\n\t}\n\n\tif (ret > 0)\n\t\tiov_iter_advance(iter, ret);\n\treturn ret;\n}\n\n \nstatic ssize_t extract_xarray_to_sg(struct iov_iter *iter,\n\t\t\t\t    ssize_t maxsize,\n\t\t\t\t    struct sg_table *sgtable,\n\t\t\t\t    unsigned int sg_max,\n\t\t\t\t    iov_iter_extraction_t extraction_flags)\n{\n\tstruct scatterlist *sg = sgtable->sgl + sgtable->nents;\n\tstruct xarray *xa = iter->xarray;\n\tstruct folio *folio;\n\tloff_t start = iter->xarray_start + iter->iov_offset;\n\tpgoff_t index = start / PAGE_SIZE;\n\tssize_t ret = 0;\n\tsize_t offset, len;\n\tXA_STATE(xas, xa, index);\n\n\trcu_read_lock();\n\n\txas_for_each(&xas, folio, ULONG_MAX) {\n\t\tif (xas_retry(&xas, folio))\n\t\t\tcontinue;\n\t\tif (WARN_ON(xa_is_value(folio)))\n\t\t\tbreak;\n\t\tif (WARN_ON(folio_test_hugetlb(folio)))\n\t\t\tbreak;\n\n\t\toffset = offset_in_folio(folio, start);\n\t\tlen = min_t(size_t, maxsize, folio_size(folio) - offset);\n\n\t\tsg_set_page(sg, folio_page(folio, 0), len, offset);\n\t\tsgtable->nents++;\n\t\tsg++;\n\t\tsg_max--;\n\n\t\tmaxsize -= len;\n\t\tret += len;\n\t\tif (maxsize <= 0 || sg_max == 0)\n\t\t\tbreak;\n\t}\n\n\trcu_read_unlock();\n\tif (ret > 0)\n\t\tiov_iter_advance(iter, ret);\n\treturn ret;\n}\n\n \nssize_t extract_iter_to_sg(struct iov_iter *iter, size_t maxsize,\n\t\t\t   struct sg_table *sgtable, unsigned int sg_max,\n\t\t\t   iov_iter_extraction_t extraction_flags)\n{\n\tif (maxsize == 0)\n\t\treturn 0;\n\n\tswitch (iov_iter_type(iter)) {\n\tcase ITER_UBUF:\n\tcase ITER_IOVEC:\n\t\treturn extract_user_to_sg(iter, maxsize, sgtable, sg_max,\n\t\t\t\t\t  extraction_flags);\n\tcase ITER_BVEC:\n\t\treturn extract_bvec_to_sg(iter, maxsize, sgtable, sg_max,\n\t\t\t\t\t  extraction_flags);\n\tcase ITER_KVEC:\n\t\treturn extract_kvec_to_sg(iter, maxsize, sgtable, sg_max,\n\t\t\t\t\t  extraction_flags);\n\tcase ITER_XARRAY:\n\t\treturn extract_xarray_to_sg(iter, maxsize, sgtable, sg_max,\n\t\t\t\t\t    extraction_flags);\n\tdefault:\n\t\tpr_err(\"%s(%u) unsupported\\n\", __func__, iov_iter_type(iter));\n\t\tWARN_ON_ONCE(1);\n\t\treturn -EIO;\n\t}\n}\nEXPORT_SYMBOL_GPL(extract_iter_to_sg);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}