{
  "module_name": "sock_reuseport.c",
  "hash_id": "281bc4d55f46aa5ce43f3a16d4122524ed600b38acad8c261d6905ac5174fb5a",
  "original_prompt": "Ingested from linux-6.6.14/net/core/sock_reuseport.c",
  "human_readable_source": "\n \n\n#include <net/ip.h>\n#include <net/sock_reuseport.h>\n#include <linux/bpf.h>\n#include <linux/idr.h>\n#include <linux/filter.h>\n#include <linux/rcupdate.h>\n\n#define INIT_SOCKS 128\n\nDEFINE_SPINLOCK(reuseport_lock);\n\nstatic DEFINE_IDA(reuseport_ida);\nstatic int reuseport_resurrect(struct sock *sk, struct sock_reuseport *old_reuse,\n\t\t\t       struct sock_reuseport *reuse, bool bind_inany);\n\nvoid reuseport_has_conns_set(struct sock *sk)\n{\n\tstruct sock_reuseport *reuse;\n\n\tif (!rcu_access_pointer(sk->sk_reuseport_cb))\n\t\treturn;\n\n\tspin_lock_bh(&reuseport_lock);\n\treuse = rcu_dereference_protected(sk->sk_reuseport_cb,\n\t\t\t\t\t  lockdep_is_held(&reuseport_lock));\n\tif (likely(reuse))\n\t\treuse->has_conns = 1;\n\tspin_unlock_bh(&reuseport_lock);\n}\nEXPORT_SYMBOL(reuseport_has_conns_set);\n\nstatic void __reuseport_get_incoming_cpu(struct sock_reuseport *reuse)\n{\n\t \n\tWRITE_ONCE(reuse->incoming_cpu, reuse->incoming_cpu + 1);\n}\n\nstatic void __reuseport_put_incoming_cpu(struct sock_reuseport *reuse)\n{\n\t \n\tWRITE_ONCE(reuse->incoming_cpu, reuse->incoming_cpu - 1);\n}\n\nstatic void reuseport_get_incoming_cpu(struct sock *sk, struct sock_reuseport *reuse)\n{\n\tif (sk->sk_incoming_cpu >= 0)\n\t\t__reuseport_get_incoming_cpu(reuse);\n}\n\nstatic void reuseport_put_incoming_cpu(struct sock *sk, struct sock_reuseport *reuse)\n{\n\tif (sk->sk_incoming_cpu >= 0)\n\t\t__reuseport_put_incoming_cpu(reuse);\n}\n\nvoid reuseport_update_incoming_cpu(struct sock *sk, int val)\n{\n\tstruct sock_reuseport *reuse;\n\tint old_sk_incoming_cpu;\n\n\tif (unlikely(!rcu_access_pointer(sk->sk_reuseport_cb))) {\n\t\t \n\t\tWRITE_ONCE(sk->sk_incoming_cpu, val);\n\t\treturn;\n\t}\n\n\tspin_lock_bh(&reuseport_lock);\n\n\t \n\told_sk_incoming_cpu = sk->sk_incoming_cpu;\n\tWRITE_ONCE(sk->sk_incoming_cpu, val);\n\n\treuse = rcu_dereference_protected(sk->sk_reuseport_cb,\n\t\t\t\t\t  lockdep_is_held(&reuseport_lock));\n\n\t \n\tif (!reuse)\n\t\tgoto out;\n\n\tif (old_sk_incoming_cpu < 0 && val >= 0)\n\t\t__reuseport_get_incoming_cpu(reuse);\n\telse if (old_sk_incoming_cpu >= 0 && val < 0)\n\t\t__reuseport_put_incoming_cpu(reuse);\n\nout:\n\tspin_unlock_bh(&reuseport_lock);\n}\n\nstatic int reuseport_sock_index(struct sock *sk,\n\t\t\t\tconst struct sock_reuseport *reuse,\n\t\t\t\tbool closed)\n{\n\tint left, right;\n\n\tif (!closed) {\n\t\tleft = 0;\n\t\tright = reuse->num_socks;\n\t} else {\n\t\tleft = reuse->max_socks - reuse->num_closed_socks;\n\t\tright = reuse->max_socks;\n\t}\n\n\tfor (; left < right; left++)\n\t\tif (reuse->socks[left] == sk)\n\t\t\treturn left;\n\treturn -1;\n}\n\nstatic void __reuseport_add_sock(struct sock *sk,\n\t\t\t\t struct sock_reuseport *reuse)\n{\n\treuse->socks[reuse->num_socks] = sk;\n\t \n\tsmp_wmb();\n\treuse->num_socks++;\n\treuseport_get_incoming_cpu(sk, reuse);\n}\n\nstatic bool __reuseport_detach_sock(struct sock *sk,\n\t\t\t\t    struct sock_reuseport *reuse)\n{\n\tint i = reuseport_sock_index(sk, reuse, false);\n\n\tif (i == -1)\n\t\treturn false;\n\n\treuse->socks[i] = reuse->socks[reuse->num_socks - 1];\n\treuse->num_socks--;\n\treuseport_put_incoming_cpu(sk, reuse);\n\n\treturn true;\n}\n\nstatic void __reuseport_add_closed_sock(struct sock *sk,\n\t\t\t\t\tstruct sock_reuseport *reuse)\n{\n\treuse->socks[reuse->max_socks - reuse->num_closed_socks - 1] = sk;\n\t \n\tWRITE_ONCE(reuse->num_closed_socks, reuse->num_closed_socks + 1);\n\treuseport_get_incoming_cpu(sk, reuse);\n}\n\nstatic bool __reuseport_detach_closed_sock(struct sock *sk,\n\t\t\t\t\t   struct sock_reuseport *reuse)\n{\n\tint i = reuseport_sock_index(sk, reuse, true);\n\n\tif (i == -1)\n\t\treturn false;\n\n\treuse->socks[i] = reuse->socks[reuse->max_socks - reuse->num_closed_socks];\n\t \n\tWRITE_ONCE(reuse->num_closed_socks, reuse->num_closed_socks - 1);\n\treuseport_put_incoming_cpu(sk, reuse);\n\n\treturn true;\n}\n\nstatic struct sock_reuseport *__reuseport_alloc(unsigned int max_socks)\n{\n\tunsigned int size = sizeof(struct sock_reuseport) +\n\t\t      sizeof(struct sock *) * max_socks;\n\tstruct sock_reuseport *reuse = kzalloc(size, GFP_ATOMIC);\n\n\tif (!reuse)\n\t\treturn NULL;\n\n\treuse->max_socks = max_socks;\n\n\tRCU_INIT_POINTER(reuse->prog, NULL);\n\treturn reuse;\n}\n\nint reuseport_alloc(struct sock *sk, bool bind_inany)\n{\n\tstruct sock_reuseport *reuse;\n\tint id, ret = 0;\n\n\t \n\tspin_lock_bh(&reuseport_lock);\n\n\t \n\treuse = rcu_dereference_protected(sk->sk_reuseport_cb,\n\t\t\t\t\t  lockdep_is_held(&reuseport_lock));\n\tif (reuse) {\n\t\tif (reuse->num_closed_socks) {\n\t\t\t \n\t\t\tret = reuseport_resurrect(sk, reuse, NULL, bind_inany);\n\t\t\tgoto out;\n\t\t}\n\n\t\t \n\t\tif (bind_inany)\n\t\t\treuse->bind_inany = bind_inany;\n\t\tgoto out;\n\t}\n\n\treuse = __reuseport_alloc(INIT_SOCKS);\n\tif (!reuse) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tid = ida_alloc(&reuseport_ida, GFP_ATOMIC);\n\tif (id < 0) {\n\t\tkfree(reuse);\n\t\tret = id;\n\t\tgoto out;\n\t}\n\n\treuse->reuseport_id = id;\n\treuse->bind_inany = bind_inany;\n\treuse->socks[0] = sk;\n\treuse->num_socks = 1;\n\treuseport_get_incoming_cpu(sk, reuse);\n\trcu_assign_pointer(sk->sk_reuseport_cb, reuse);\n\nout:\n\tspin_unlock_bh(&reuseport_lock);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(reuseport_alloc);\n\nstatic struct sock_reuseport *reuseport_grow(struct sock_reuseport *reuse)\n{\n\tstruct sock_reuseport *more_reuse;\n\tu32 more_socks_size, i;\n\n\tmore_socks_size = reuse->max_socks * 2U;\n\tif (more_socks_size > U16_MAX) {\n\t\tif (reuse->num_closed_socks) {\n\t\t\t \n\t\t\tstruct sock *sk;\n\n\t\t\tsk = reuse->socks[reuse->max_socks - reuse->num_closed_socks];\n\t\t\tRCU_INIT_POINTER(sk->sk_reuseport_cb, NULL);\n\t\t\t__reuseport_detach_closed_sock(sk, reuse);\n\n\t\t\treturn reuse;\n\t\t}\n\n\t\treturn NULL;\n\t}\n\n\tmore_reuse = __reuseport_alloc(more_socks_size);\n\tif (!more_reuse)\n\t\treturn NULL;\n\n\tmore_reuse->num_socks = reuse->num_socks;\n\tmore_reuse->num_closed_socks = reuse->num_closed_socks;\n\tmore_reuse->prog = reuse->prog;\n\tmore_reuse->reuseport_id = reuse->reuseport_id;\n\tmore_reuse->bind_inany = reuse->bind_inany;\n\tmore_reuse->has_conns = reuse->has_conns;\n\tmore_reuse->incoming_cpu = reuse->incoming_cpu;\n\n\tmemcpy(more_reuse->socks, reuse->socks,\n\t       reuse->num_socks * sizeof(struct sock *));\n\tmemcpy(more_reuse->socks +\n\t       (more_reuse->max_socks - more_reuse->num_closed_socks),\n\t       reuse->socks + (reuse->max_socks - reuse->num_closed_socks),\n\t       reuse->num_closed_socks * sizeof(struct sock *));\n\tmore_reuse->synq_overflow_ts = READ_ONCE(reuse->synq_overflow_ts);\n\n\tfor (i = 0; i < reuse->max_socks; ++i)\n\t\trcu_assign_pointer(reuse->socks[i]->sk_reuseport_cb,\n\t\t\t\t   more_reuse);\n\n\t \n\tkfree_rcu(reuse, rcu);\n\treturn more_reuse;\n}\n\nstatic void reuseport_free_rcu(struct rcu_head *head)\n{\n\tstruct sock_reuseport *reuse;\n\n\treuse = container_of(head, struct sock_reuseport, rcu);\n\tsk_reuseport_prog_free(rcu_dereference_protected(reuse->prog, 1));\n\tida_free(&reuseport_ida, reuse->reuseport_id);\n\tkfree(reuse);\n}\n\n \nint reuseport_add_sock(struct sock *sk, struct sock *sk2, bool bind_inany)\n{\n\tstruct sock_reuseport *old_reuse, *reuse;\n\n\tif (!rcu_access_pointer(sk2->sk_reuseport_cb)) {\n\t\tint err = reuseport_alloc(sk2, bind_inany);\n\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tspin_lock_bh(&reuseport_lock);\n\treuse = rcu_dereference_protected(sk2->sk_reuseport_cb,\n\t\t\t\t\t  lockdep_is_held(&reuseport_lock));\n\told_reuse = rcu_dereference_protected(sk->sk_reuseport_cb,\n\t\t\t\t\t      lockdep_is_held(&reuseport_lock));\n\tif (old_reuse && old_reuse->num_closed_socks) {\n\t\t \n\t\tint err = reuseport_resurrect(sk, old_reuse, reuse, reuse->bind_inany);\n\n\t\tspin_unlock_bh(&reuseport_lock);\n\t\treturn err;\n\t}\n\n\tif (old_reuse && old_reuse->num_socks != 1) {\n\t\tspin_unlock_bh(&reuseport_lock);\n\t\treturn -EBUSY;\n\t}\n\n\tif (reuse->num_socks + reuse->num_closed_socks == reuse->max_socks) {\n\t\treuse = reuseport_grow(reuse);\n\t\tif (!reuse) {\n\t\t\tspin_unlock_bh(&reuseport_lock);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\t__reuseport_add_sock(sk, reuse);\n\trcu_assign_pointer(sk->sk_reuseport_cb, reuse);\n\n\tspin_unlock_bh(&reuseport_lock);\n\n\tif (old_reuse)\n\t\tcall_rcu(&old_reuse->rcu, reuseport_free_rcu);\n\treturn 0;\n}\nEXPORT_SYMBOL(reuseport_add_sock);\n\nstatic int reuseport_resurrect(struct sock *sk, struct sock_reuseport *old_reuse,\n\t\t\t       struct sock_reuseport *reuse, bool bind_inany)\n{\n\tif (old_reuse == reuse) {\n\t\t \n\t\t__reuseport_detach_closed_sock(sk, old_reuse);\n\t\t__reuseport_add_sock(sk, old_reuse);\n\t\treturn 0;\n\t}\n\n\tif (!reuse) {\n\t\t \n\t\tint id;\n\n\t\treuse = __reuseport_alloc(INIT_SOCKS);\n\t\tif (!reuse)\n\t\t\treturn -ENOMEM;\n\n\t\tid = ida_alloc(&reuseport_ida, GFP_ATOMIC);\n\t\tif (id < 0) {\n\t\t\tkfree(reuse);\n\t\t\treturn id;\n\t\t}\n\n\t\treuse->reuseport_id = id;\n\t\treuse->bind_inany = bind_inany;\n\t} else {\n\t\t \n\t\tif (reuse->num_socks + reuse->num_closed_socks == reuse->max_socks) {\n\t\t\treuse = reuseport_grow(reuse);\n\t\t\tif (!reuse)\n\t\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\t__reuseport_detach_closed_sock(sk, old_reuse);\n\t__reuseport_add_sock(sk, reuse);\n\trcu_assign_pointer(sk->sk_reuseport_cb, reuse);\n\n\tif (old_reuse->num_socks + old_reuse->num_closed_socks == 0)\n\t\tcall_rcu(&old_reuse->rcu, reuseport_free_rcu);\n\n\treturn 0;\n}\n\nvoid reuseport_detach_sock(struct sock *sk)\n{\n\tstruct sock_reuseport *reuse;\n\n\tspin_lock_bh(&reuseport_lock);\n\treuse = rcu_dereference_protected(sk->sk_reuseport_cb,\n\t\t\t\t\t  lockdep_is_held(&reuseport_lock));\n\n\t \n\tif (!reuse)\n\t\tgoto out;\n\n\t \n\tbpf_sk_reuseport_detach(sk);\n\n\trcu_assign_pointer(sk->sk_reuseport_cb, NULL);\n\n\tif (!__reuseport_detach_closed_sock(sk, reuse))\n\t\t__reuseport_detach_sock(sk, reuse);\n\n\tif (reuse->num_socks + reuse->num_closed_socks == 0)\n\t\tcall_rcu(&reuse->rcu, reuseport_free_rcu);\n\nout:\n\tspin_unlock_bh(&reuseport_lock);\n}\nEXPORT_SYMBOL(reuseport_detach_sock);\n\nvoid reuseport_stop_listen_sock(struct sock *sk)\n{\n\tif (sk->sk_protocol == IPPROTO_TCP) {\n\t\tstruct sock_reuseport *reuse;\n\t\tstruct bpf_prog *prog;\n\n\t\tspin_lock_bh(&reuseport_lock);\n\n\t\treuse = rcu_dereference_protected(sk->sk_reuseport_cb,\n\t\t\t\t\t\t  lockdep_is_held(&reuseport_lock));\n\t\tprog = rcu_dereference_protected(reuse->prog,\n\t\t\t\t\t\t lockdep_is_held(&reuseport_lock));\n\n\t\tif (READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_migrate_req) ||\n\t\t    (prog && prog->expected_attach_type == BPF_SK_REUSEPORT_SELECT_OR_MIGRATE)) {\n\t\t\t \n\t\t\tbpf_sk_reuseport_detach(sk);\n\n\t\t\t__reuseport_detach_sock(sk, reuse);\n\t\t\t__reuseport_add_closed_sock(sk, reuse);\n\n\t\t\tspin_unlock_bh(&reuseport_lock);\n\t\t\treturn;\n\t\t}\n\n\t\tspin_unlock_bh(&reuseport_lock);\n\t}\n\n\t \n\treuseport_detach_sock(sk);\n}\nEXPORT_SYMBOL(reuseport_stop_listen_sock);\n\nstatic struct sock *run_bpf_filter(struct sock_reuseport *reuse, u16 socks,\n\t\t\t\t   struct bpf_prog *prog, struct sk_buff *skb,\n\t\t\t\t   int hdr_len)\n{\n\tstruct sk_buff *nskb = NULL;\n\tu32 index;\n\n\tif (skb_shared(skb)) {\n\t\tnskb = skb_clone(skb, GFP_ATOMIC);\n\t\tif (!nskb)\n\t\t\treturn NULL;\n\t\tskb = nskb;\n\t}\n\n\t \n\tif (!pskb_pull(skb, hdr_len)) {\n\t\tkfree_skb(nskb);\n\t\treturn NULL;\n\t}\n\tindex = bpf_prog_run_save_cb(prog, skb);\n\t__skb_push(skb, hdr_len);\n\n\tconsume_skb(nskb);\n\n\tif (index >= socks)\n\t\treturn NULL;\n\n\treturn reuse->socks[index];\n}\n\nstatic struct sock *reuseport_select_sock_by_hash(struct sock_reuseport *reuse,\n\t\t\t\t\t\t  u32 hash, u16 num_socks)\n{\n\tstruct sock *first_valid_sk = NULL;\n\tint i, j;\n\n\ti = j = reciprocal_scale(hash, num_socks);\n\tdo {\n\t\tstruct sock *sk = reuse->socks[i];\n\n\t\tif (sk->sk_state != TCP_ESTABLISHED) {\n\t\t\t \n\t\t\tif (!READ_ONCE(reuse->incoming_cpu))\n\t\t\t\treturn sk;\n\n\t\t\t \n\t\t\tif (READ_ONCE(sk->sk_incoming_cpu) == raw_smp_processor_id())\n\t\t\t\treturn sk;\n\n\t\t\tif (!first_valid_sk)\n\t\t\t\tfirst_valid_sk = sk;\n\t\t}\n\n\t\ti++;\n\t\tif (i >= num_socks)\n\t\t\ti = 0;\n\t} while (i != j);\n\n\treturn first_valid_sk;\n}\n\n \nstruct sock *reuseport_select_sock(struct sock *sk,\n\t\t\t\t   u32 hash,\n\t\t\t\t   struct sk_buff *skb,\n\t\t\t\t   int hdr_len)\n{\n\tstruct sock_reuseport *reuse;\n\tstruct bpf_prog *prog;\n\tstruct sock *sk2 = NULL;\n\tu16 socks;\n\n\trcu_read_lock();\n\treuse = rcu_dereference(sk->sk_reuseport_cb);\n\n\t \n\tif (!reuse)\n\t\tgoto out;\n\n\tprog = rcu_dereference(reuse->prog);\n\tsocks = READ_ONCE(reuse->num_socks);\n\tif (likely(socks)) {\n\t\t \n\t\tsmp_rmb();\n\n\t\tif (!prog || !skb)\n\t\t\tgoto select_by_hash;\n\n\t\tif (prog->type == BPF_PROG_TYPE_SK_REUSEPORT)\n\t\t\tsk2 = bpf_run_sk_reuseport(reuse, sk, prog, skb, NULL, hash);\n\t\telse\n\t\t\tsk2 = run_bpf_filter(reuse, socks, prog, skb, hdr_len);\n\nselect_by_hash:\n\t\t \n\t\tif (!sk2)\n\t\t\tsk2 = reuseport_select_sock_by_hash(reuse, hash, socks);\n\t}\n\nout:\n\trcu_read_unlock();\n\treturn sk2;\n}\nEXPORT_SYMBOL(reuseport_select_sock);\n\n \nstruct sock *reuseport_migrate_sock(struct sock *sk,\n\t\t\t\t    struct sock *migrating_sk,\n\t\t\t\t    struct sk_buff *skb)\n{\n\tstruct sock_reuseport *reuse;\n\tstruct sock *nsk = NULL;\n\tbool allocated = false;\n\tstruct bpf_prog *prog;\n\tu16 socks;\n\tu32 hash;\n\n\trcu_read_lock();\n\n\treuse = rcu_dereference(sk->sk_reuseport_cb);\n\tif (!reuse)\n\t\tgoto out;\n\n\tsocks = READ_ONCE(reuse->num_socks);\n\tif (unlikely(!socks))\n\t\tgoto failure;\n\n\t \n\tsmp_rmb();\n\n\thash = migrating_sk->sk_hash;\n\tprog = rcu_dereference(reuse->prog);\n\tif (!prog || prog->expected_attach_type != BPF_SK_REUSEPORT_SELECT_OR_MIGRATE) {\n\t\tif (READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_migrate_req))\n\t\t\tgoto select_by_hash;\n\t\tgoto failure;\n\t}\n\n\tif (!skb) {\n\t\tskb = alloc_skb(0, GFP_ATOMIC);\n\t\tif (!skb)\n\t\t\tgoto failure;\n\t\tallocated = true;\n\t}\n\n\tnsk = bpf_run_sk_reuseport(reuse, sk, prog, skb, migrating_sk, hash);\n\n\tif (allocated)\n\t\tkfree_skb(skb);\n\nselect_by_hash:\n\tif (!nsk)\n\t\tnsk = reuseport_select_sock_by_hash(reuse, hash, socks);\n\n\tif (IS_ERR_OR_NULL(nsk) || unlikely(!refcount_inc_not_zero(&nsk->sk_refcnt))) {\n\t\tnsk = NULL;\n\t\tgoto failure;\n\t}\n\nout:\n\trcu_read_unlock();\n\treturn nsk;\n\nfailure:\n\t__NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPMIGRATEREQFAILURE);\n\tgoto out;\n}\nEXPORT_SYMBOL(reuseport_migrate_sock);\n\nint reuseport_attach_prog(struct sock *sk, struct bpf_prog *prog)\n{\n\tstruct sock_reuseport *reuse;\n\tstruct bpf_prog *old_prog;\n\n\tif (sk_unhashed(sk)) {\n\t\tint err;\n\n\t\tif (!sk->sk_reuseport)\n\t\t\treturn -EINVAL;\n\n\t\terr = reuseport_alloc(sk, false);\n\t\tif (err)\n\t\t\treturn err;\n\t} else if (!rcu_access_pointer(sk->sk_reuseport_cb)) {\n\t\t \n\t\treturn -EINVAL;\n\t}\n\n\tspin_lock_bh(&reuseport_lock);\n\treuse = rcu_dereference_protected(sk->sk_reuseport_cb,\n\t\t\t\t\t  lockdep_is_held(&reuseport_lock));\n\told_prog = rcu_dereference_protected(reuse->prog,\n\t\t\t\t\t     lockdep_is_held(&reuseport_lock));\n\trcu_assign_pointer(reuse->prog, prog);\n\tspin_unlock_bh(&reuseport_lock);\n\n\tsk_reuseport_prog_free(old_prog);\n\treturn 0;\n}\nEXPORT_SYMBOL(reuseport_attach_prog);\n\nint reuseport_detach_prog(struct sock *sk)\n{\n\tstruct sock_reuseport *reuse;\n\tstruct bpf_prog *old_prog;\n\n\told_prog = NULL;\n\tspin_lock_bh(&reuseport_lock);\n\treuse = rcu_dereference_protected(sk->sk_reuseport_cb,\n\t\t\t\t\t  lockdep_is_held(&reuseport_lock));\n\n\t \n\tif (!reuse) {\n\t\tspin_unlock_bh(&reuseport_lock);\n\t\treturn sk->sk_reuseport ? -ENOENT : -EINVAL;\n\t}\n\n\tif (sk_unhashed(sk) && reuse->num_closed_socks) {\n\t\tspin_unlock_bh(&reuseport_lock);\n\t\treturn -ENOENT;\n\t}\n\n\told_prog = rcu_replace_pointer(reuse->prog, old_prog,\n\t\t\t\t       lockdep_is_held(&reuseport_lock));\n\tspin_unlock_bh(&reuseport_lock);\n\n\tif (!old_prog)\n\t\treturn -ENOENT;\n\n\tsk_reuseport_prog_free(old_prog);\n\treturn 0;\n}\nEXPORT_SYMBOL(reuseport_detach_prog);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}