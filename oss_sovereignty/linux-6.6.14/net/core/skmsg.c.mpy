{
  "module_name": "skmsg.c",
  "hash_id": "4761713694a0f8887d9dee37086b9ad5df758e8959f53ac6df3ece1098f65839",
  "original_prompt": "Ingested from linux-6.6.14/net/core/skmsg.c",
  "human_readable_source": "\n \n\tif (!msg->skb) {\n\t\tif (charge)\n\t\t\tsk_mem_uncharge(sk, len);\n\t\tput_page(sg_page(sge));\n\t}\n\tmemset(sge, 0, sizeof(*sge));\n\treturn len;\n}\n\nstatic int __sk_msg_free(struct sock *sk, struct sk_msg *msg, u32 i,\n\t\t\t bool charge)\n{\n\tstruct scatterlist *sge = sk_msg_elem(msg, i);\n\tint freed = 0;\n\n\twhile (msg->sg.size) {\n\t\tmsg->sg.size -= sge->length;\n\t\tfreed += sk_msg_free_elem(sk, msg, i, charge);\n\t\tsk_msg_iter_var_next(i);\n\t\tsk_msg_check_to_free(msg, i, msg->sg.size);\n\t\tsge = sk_msg_elem(msg, i);\n\t}\n\tconsume_skb(msg->skb);\n\tsk_msg_init(msg);\n\treturn freed;\n}\n\nint sk_msg_free_nocharge(struct sock *sk, struct sk_msg *msg)\n{\n\treturn __sk_msg_free(sk, msg, msg->sg.start, false);\n}\nEXPORT_SYMBOL_GPL(sk_msg_free_nocharge);\n\nint sk_msg_free(struct sock *sk, struct sk_msg *msg)\n{\n\treturn __sk_msg_free(sk, msg, msg->sg.start, true);\n}\nEXPORT_SYMBOL_GPL(sk_msg_free);\n\nstatic void __sk_msg_free_partial(struct sock *sk, struct sk_msg *msg,\n\t\t\t\t  u32 bytes, bool charge)\n{\n\tstruct scatterlist *sge;\n\tu32 i = msg->sg.start;\n\n\twhile (bytes) {\n\t\tsge = sk_msg_elem(msg, i);\n\t\tif (!sge->length)\n\t\t\tbreak;\n\t\tif (bytes < sge->length) {\n\t\t\tif (charge)\n\t\t\t\tsk_mem_uncharge(sk, bytes);\n\t\t\tsge->length -= bytes;\n\t\t\tsge->offset += bytes;\n\t\t\tmsg->sg.size -= bytes;\n\t\t\tbreak;\n\t\t}\n\n\t\tmsg->sg.size -= sge->length;\n\t\tbytes -= sge->length;\n\t\tsk_msg_free_elem(sk, msg, i, charge);\n\t\tsk_msg_iter_var_next(i);\n\t\tsk_msg_check_to_free(msg, i, bytes);\n\t}\n\tmsg->sg.start = i;\n}\n\nvoid sk_msg_free_partial(struct sock *sk, struct sk_msg *msg, u32 bytes)\n{\n\t__sk_msg_free_partial(sk, msg, bytes, true);\n}\nEXPORT_SYMBOL_GPL(sk_msg_free_partial);\n\nvoid sk_msg_free_partial_nocharge(struct sock *sk, struct sk_msg *msg,\n\t\t\t\t  u32 bytes)\n{\n\t__sk_msg_free_partial(sk, msg, bytes, false);\n}\n\nvoid sk_msg_trim(struct sock *sk, struct sk_msg *msg, int len)\n{\n\tint trim = msg->sg.size - len;\n\tu32 i = msg->sg.end;\n\n\tif (trim <= 0) {\n\t\tWARN_ON(trim < 0);\n\t\treturn;\n\t}\n\n\tsk_msg_iter_var_prev(i);\n\tmsg->sg.size = len;\n\twhile (msg->sg.data[i].length &&\n\t       trim >= msg->sg.data[i].length) {\n\t\ttrim -= msg->sg.data[i].length;\n\t\tsk_msg_free_elem(sk, msg, i, true);\n\t\tsk_msg_iter_var_prev(i);\n\t\tif (!trim)\n\t\t\tgoto out;\n\t}\n\n\tmsg->sg.data[i].length -= trim;\n\tsk_mem_uncharge(sk, trim);\n\t \n\tif (msg->sg.curr == i && msg->sg.copybreak > msg->sg.data[i].length)\n\t\tmsg->sg.copybreak = msg->sg.data[i].length;\nout:\n\tsk_msg_iter_var_next(i);\n\tmsg->sg.end = i;\n\n\t \n\tif (!msg->sg.size) {\n\t\tmsg->sg.curr = msg->sg.start;\n\t\tmsg->sg.copybreak = 0;\n\t} else if (sk_msg_iter_dist(msg->sg.start, msg->sg.curr) >=\n\t\t   sk_msg_iter_dist(msg->sg.start, msg->sg.end)) {\n\t\tsk_msg_iter_var_prev(i);\n\t\tmsg->sg.curr = i;\n\t\tmsg->sg.copybreak = msg->sg.data[i].length;\n\t}\n}\nEXPORT_SYMBOL_GPL(sk_msg_trim);\n\nint sk_msg_zerocopy_from_iter(struct sock *sk, struct iov_iter *from,\n\t\t\t      struct sk_msg *msg, u32 bytes)\n{\n\tint i, maxpages, ret = 0, num_elems = sk_msg_elem_used(msg);\n\tconst int to_max_pages = MAX_MSG_FRAGS;\n\tstruct page *pages[MAX_MSG_FRAGS];\n\tssize_t orig, copied, use, offset;\n\n\torig = msg->sg.size;\n\twhile (bytes > 0) {\n\t\ti = 0;\n\t\tmaxpages = to_max_pages - num_elems;\n\t\tif (maxpages == 0) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\tcopied = iov_iter_get_pages2(from, pages, bytes, maxpages,\n\t\t\t\t\t    &offset);\n\t\tif (copied <= 0) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\tbytes -= copied;\n\t\tmsg->sg.size += copied;\n\n\t\twhile (copied) {\n\t\t\tuse = min_t(int, copied, PAGE_SIZE - offset);\n\t\t\tsg_set_page(&msg->sg.data[msg->sg.end],\n\t\t\t\t    pages[i], use, offset);\n\t\t\tsg_unmark_end(&msg->sg.data[msg->sg.end]);\n\t\t\tsk_mem_charge(sk, use);\n\n\t\t\toffset = 0;\n\t\t\tcopied -= use;\n\t\t\tsk_msg_iter_next(msg, end);\n\t\t\tnum_elems++;\n\t\t\ti++;\n\t\t}\n\t\t \n\t\tmsg->sg.copybreak = 0;\n\t\tmsg->sg.curr = msg->sg.end;\n\t}\nout:\n\t \n\tif (ret)\n\t\tiov_iter_revert(from, msg->sg.size - orig);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(sk_msg_zerocopy_from_iter);\n\nint sk_msg_memcopy_from_iter(struct sock *sk, struct iov_iter *from,\n\t\t\t     struct sk_msg *msg, u32 bytes)\n{\n\tint ret = -ENOSPC, i = msg->sg.curr;\n\tstruct scatterlist *sge;\n\tu32 copy, buf_size;\n\tvoid *to;\n\n\tdo {\n\t\tsge = sk_msg_elem(msg, i);\n\t\t \n\t\tif (msg->sg.copybreak >= sge->length) {\n\t\t\tmsg->sg.copybreak = 0;\n\t\t\tsk_msg_iter_var_next(i);\n\t\t\tif (i == msg->sg.end)\n\t\t\t\tbreak;\n\t\t\tsge = sk_msg_elem(msg, i);\n\t\t}\n\n\t\tbuf_size = sge->length - msg->sg.copybreak;\n\t\tcopy = (buf_size > bytes) ? bytes : buf_size;\n\t\tto = sg_virt(sge) + msg->sg.copybreak;\n\t\tmsg->sg.copybreak += copy;\n\t\tif (sk->sk_route_caps & NETIF_F_NOCACHE_COPY)\n\t\t\tret = copy_from_iter_nocache(to, copy, from);\n\t\telse\n\t\t\tret = copy_from_iter(to, copy, from);\n\t\tif (ret != copy) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t\tbytes -= copy;\n\t\tif (!bytes)\n\t\t\tbreak;\n\t\tmsg->sg.copybreak = 0;\n\t\tsk_msg_iter_var_next(i);\n\t} while (i != msg->sg.end);\nout:\n\tmsg->sg.curr = i;\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(sk_msg_memcopy_from_iter);\n\n \nint sk_msg_recvmsg(struct sock *sk, struct sk_psock *psock, struct msghdr *msg,\n\t\t   int len, int flags)\n{\n\tstruct iov_iter *iter = &msg->msg_iter;\n\tint peek = flags & MSG_PEEK;\n\tstruct sk_msg *msg_rx;\n\tint i, copied = 0;\n\n\tmsg_rx = sk_psock_peek_msg(psock);\n\twhile (copied != len) {\n\t\tstruct scatterlist *sge;\n\n\t\tif (unlikely(!msg_rx))\n\t\t\tbreak;\n\n\t\ti = msg_rx->sg.start;\n\t\tdo {\n\t\t\tstruct page *page;\n\t\t\tint copy;\n\n\t\t\tsge = sk_msg_elem(msg_rx, i);\n\t\t\tcopy = sge->length;\n\t\t\tpage = sg_page(sge);\n\t\t\tif (copied + copy > len)\n\t\t\t\tcopy = len - copied;\n\t\t\tcopy = copy_page_to_iter(page, sge->offset, copy, iter);\n\t\t\tif (!copy) {\n\t\t\t\tcopied = copied ? copied : -EFAULT;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tcopied += copy;\n\t\t\tif (likely(!peek)) {\n\t\t\t\tsge->offset += copy;\n\t\t\t\tsge->length -= copy;\n\t\t\t\tif (!msg_rx->skb)\n\t\t\t\t\tsk_mem_uncharge(sk, copy);\n\t\t\t\tmsg_rx->sg.size -= copy;\n\n\t\t\t\tif (!sge->length) {\n\t\t\t\t\tsk_msg_iter_var_next(i);\n\t\t\t\t\tif (!msg_rx->skb)\n\t\t\t\t\t\tput_page(page);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tif (copy != sge->length)\n\t\t\t\t\tgoto out;\n\t\t\t\tsk_msg_iter_var_next(i);\n\t\t\t}\n\n\t\t\tif (copied == len)\n\t\t\t\tbreak;\n\t\t} while ((i != msg_rx->sg.end) && !sg_is_last(sge));\n\n\t\tif (unlikely(peek)) {\n\t\t\tmsg_rx = sk_psock_next_msg(psock, msg_rx);\n\t\t\tif (!msg_rx)\n\t\t\t\tbreak;\n\t\t\tcontinue;\n\t\t}\n\n\t\tmsg_rx->sg.start = i;\n\t\tif (!sge->length && (i == msg_rx->sg.end || sg_is_last(sge))) {\n\t\t\tmsg_rx = sk_psock_dequeue_msg(psock);\n\t\t\tkfree_sk_msg(msg_rx);\n\t\t}\n\t\tmsg_rx = sk_psock_peek_msg(psock);\n\t}\nout:\n\treturn copied;\n}\nEXPORT_SYMBOL_GPL(sk_msg_recvmsg);\n\nbool sk_msg_is_readable(struct sock *sk)\n{\n\tstruct sk_psock *psock;\n\tbool empty = true;\n\n\trcu_read_lock();\n\tpsock = sk_psock(sk);\n\tif (likely(psock))\n\t\tempty = list_empty(&psock->ingress_msg);\n\trcu_read_unlock();\n\treturn !empty;\n}\nEXPORT_SYMBOL_GPL(sk_msg_is_readable);\n\nstatic struct sk_msg *alloc_sk_msg(gfp_t gfp)\n{\n\tstruct sk_msg *msg;\n\n\tmsg = kzalloc(sizeof(*msg), gfp | __GFP_NOWARN);\n\tif (unlikely(!msg))\n\t\treturn NULL;\n\tsg_init_marker(msg->sg.data, NR_MSG_FRAG_IDS);\n\treturn msg;\n}\n\nstatic struct sk_msg *sk_psock_create_ingress_msg(struct sock *sk,\n\t\t\t\t\t\t  struct sk_buff *skb)\n{\n\tif (atomic_read(&sk->sk_rmem_alloc) > sk->sk_rcvbuf)\n\t\treturn NULL;\n\n\tif (!sk_rmem_schedule(sk, skb, skb->truesize))\n\t\treturn NULL;\n\n\treturn alloc_sk_msg(GFP_KERNEL);\n}\n\nstatic int sk_psock_skb_ingress_enqueue(struct sk_buff *skb,\n\t\t\t\t\tu32 off, u32 len,\n\t\t\t\t\tstruct sk_psock *psock,\n\t\t\t\t\tstruct sock *sk,\n\t\t\t\t\tstruct sk_msg *msg)\n{\n\tint num_sge, copied;\n\n\tnum_sge = skb_to_sgvec(skb, msg->sg.data, off, len);\n\tif (num_sge < 0) {\n\t\t \n\t\tif (skb_linearize(skb))\n\t\t\treturn -EAGAIN;\n\n\t\tnum_sge = skb_to_sgvec(skb, msg->sg.data, off, len);\n\t\tif (unlikely(num_sge < 0))\n\t\t\treturn num_sge;\n\t}\n\n\tcopied = len;\n\tmsg->sg.start = 0;\n\tmsg->sg.size = copied;\n\tmsg->sg.end = num_sge;\n\tmsg->skb = skb;\n\n\tsk_psock_queue_msg(psock, msg);\n\tsk_psock_data_ready(sk, psock);\n\treturn copied;\n}\n\nstatic int sk_psock_skb_ingress_self(struct sk_psock *psock, struct sk_buff *skb,\n\t\t\t\t     u32 off, u32 len);\n\nstatic int sk_psock_skb_ingress(struct sk_psock *psock, struct sk_buff *skb,\n\t\t\t\tu32 off, u32 len)\n{\n\tstruct sock *sk = psock->sk;\n\tstruct sk_msg *msg;\n\tint err;\n\n\t \n\tif (unlikely(skb->sk == sk))\n\t\treturn sk_psock_skb_ingress_self(psock, skb, off, len);\n\tmsg = sk_psock_create_ingress_msg(sk, skb);\n\tif (!msg)\n\t\treturn -EAGAIN;\n\n\t \n\tskb_set_owner_r(skb, sk);\n\terr = sk_psock_skb_ingress_enqueue(skb, off, len, psock, sk, msg);\n\tif (err < 0)\n\t\tkfree(msg);\n\treturn err;\n}\n\n \nstatic int sk_psock_skb_ingress_self(struct sk_psock *psock, struct sk_buff *skb,\n\t\t\t\t     u32 off, u32 len)\n{\n\tstruct sk_msg *msg = alloc_sk_msg(GFP_ATOMIC);\n\tstruct sock *sk = psock->sk;\n\tint err;\n\n\tif (unlikely(!msg))\n\t\treturn -EAGAIN;\n\tskb_set_owner_r(skb, sk);\n\terr = sk_psock_skb_ingress_enqueue(skb, off, len, psock, sk, msg);\n\tif (err < 0)\n\t\tkfree(msg);\n\treturn err;\n}\n\nstatic int sk_psock_handle_skb(struct sk_psock *psock, struct sk_buff *skb,\n\t\t\t       u32 off, u32 len, bool ingress)\n{\n\tint err = 0;\n\n\tif (!ingress) {\n\t\tif (!sock_writeable(psock->sk))\n\t\t\treturn -EAGAIN;\n\t\treturn skb_send_sock(psock->sk, skb, off, len);\n\t}\n\tskb_get(skb);\n\terr = sk_psock_skb_ingress(psock, skb, off, len);\n\tif (err < 0)\n\t\tkfree_skb(skb);\n\treturn err;\n}\n\nstatic void sk_psock_skb_state(struct sk_psock *psock,\n\t\t\t       struct sk_psock_work_state *state,\n\t\t\t       int len, int off)\n{\n\tspin_lock_bh(&psock->ingress_lock);\n\tif (sk_psock_test_state(psock, SK_PSOCK_TX_ENABLED)) {\n\t\tstate->len = len;\n\t\tstate->off = off;\n\t}\n\tspin_unlock_bh(&psock->ingress_lock);\n}\n\nstatic void sk_psock_backlog(struct work_struct *work)\n{\n\tstruct delayed_work *dwork = to_delayed_work(work);\n\tstruct sk_psock *psock = container_of(dwork, struct sk_psock, work);\n\tstruct sk_psock_work_state *state = &psock->work_state;\n\tstruct sk_buff *skb = NULL;\n\tu32 len = 0, off = 0;\n\tbool ingress;\n\tint ret;\n\n\tmutex_lock(&psock->work_mutex);\n\tif (unlikely(state->len)) {\n\t\tlen = state->len;\n\t\toff = state->off;\n\t}\n\n\twhile ((skb = skb_peek(&psock->ingress_skb))) {\n\t\tlen = skb->len;\n\t\toff = 0;\n\t\tif (skb_bpf_strparser(skb)) {\n\t\t\tstruct strp_msg *stm = strp_msg(skb);\n\n\t\t\toff = stm->offset;\n\t\t\tlen = stm->full_len;\n\t\t}\n\t\tingress = skb_bpf_ingress(skb);\n\t\tskb_bpf_redirect_clear(skb);\n\t\tdo {\n\t\t\tret = -EIO;\n\t\t\tif (!sock_flag(psock->sk, SOCK_DEAD))\n\t\t\t\tret = sk_psock_handle_skb(psock, skb, off,\n\t\t\t\t\t\t\t  len, ingress);\n\t\t\tif (ret <= 0) {\n\t\t\t\tif (ret == -EAGAIN) {\n\t\t\t\t\tsk_psock_skb_state(psock, state, len, off);\n\n\t\t\t\t\t \n\t\t\t\t\tif (sk_psock_test_state(psock, SK_PSOCK_TX_ENABLED))\n\t\t\t\t\t\tschedule_delayed_work(&psock->work, 1);\n\t\t\t\t\tgoto end;\n\t\t\t\t}\n\t\t\t\t \n\t\t\t\tsk_psock_report_error(psock, ret ? -ret : EPIPE);\n\t\t\t\tsk_psock_clear_state(psock, SK_PSOCK_TX_ENABLED);\n\t\t\t\tgoto end;\n\t\t\t}\n\t\t\toff += ret;\n\t\t\tlen -= ret;\n\t\t} while (len);\n\n\t\tskb = skb_dequeue(&psock->ingress_skb);\n\t\tkfree_skb(skb);\n\t}\nend:\n\tmutex_unlock(&psock->work_mutex);\n}\n\nstruct sk_psock *sk_psock_init(struct sock *sk, int node)\n{\n\tstruct sk_psock *psock;\n\tstruct proto *prot;\n\n\twrite_lock_bh(&sk->sk_callback_lock);\n\n\tif (sk_is_inet(sk) && inet_csk_has_ulp(sk)) {\n\t\tpsock = ERR_PTR(-EINVAL);\n\t\tgoto out;\n\t}\n\n\tif (sk->sk_user_data) {\n\t\tpsock = ERR_PTR(-EBUSY);\n\t\tgoto out;\n\t}\n\n\tpsock = kzalloc_node(sizeof(*psock), GFP_ATOMIC | __GFP_NOWARN, node);\n\tif (!psock) {\n\t\tpsock = ERR_PTR(-ENOMEM);\n\t\tgoto out;\n\t}\n\n\tprot = READ_ONCE(sk->sk_prot);\n\tpsock->sk = sk;\n\tpsock->eval = __SK_NONE;\n\tpsock->sk_proto = prot;\n\tpsock->saved_unhash = prot->unhash;\n\tpsock->saved_destroy = prot->destroy;\n\tpsock->saved_close = prot->close;\n\tpsock->saved_write_space = sk->sk_write_space;\n\n\tINIT_LIST_HEAD(&psock->link);\n\tspin_lock_init(&psock->link_lock);\n\n\tINIT_DELAYED_WORK(&psock->work, sk_psock_backlog);\n\tmutex_init(&psock->work_mutex);\n\tINIT_LIST_HEAD(&psock->ingress_msg);\n\tspin_lock_init(&psock->ingress_lock);\n\tskb_queue_head_init(&psock->ingress_skb);\n\n\tsk_psock_set_state(psock, SK_PSOCK_TX_ENABLED);\n\trefcount_set(&psock->refcnt, 1);\n\n\t__rcu_assign_sk_user_data_with_flags(sk, psock,\n\t\t\t\t\t     SK_USER_DATA_NOCOPY |\n\t\t\t\t\t     SK_USER_DATA_PSOCK);\n\tsock_hold(sk);\n\nout:\n\twrite_unlock_bh(&sk->sk_callback_lock);\n\treturn psock;\n}\nEXPORT_SYMBOL_GPL(sk_psock_init);\n\nstruct sk_psock_link *sk_psock_link_pop(struct sk_psock *psock)\n{\n\tstruct sk_psock_link *link;\n\n\tspin_lock_bh(&psock->link_lock);\n\tlink = list_first_entry_or_null(&psock->link, struct sk_psock_link,\n\t\t\t\t\tlist);\n\tif (link)\n\t\tlist_del(&link->list);\n\tspin_unlock_bh(&psock->link_lock);\n\treturn link;\n}\n\nstatic void __sk_psock_purge_ingress_msg(struct sk_psock *psock)\n{\n\tstruct sk_msg *msg, *tmp;\n\n\tlist_for_each_entry_safe(msg, tmp, &psock->ingress_msg, list) {\n\t\tlist_del(&msg->list);\n\t\tsk_msg_free(psock->sk, msg);\n\t\tkfree(msg);\n\t}\n}\n\nstatic void __sk_psock_zap_ingress(struct sk_psock *psock)\n{\n\tstruct sk_buff *skb;\n\n\twhile ((skb = skb_dequeue(&psock->ingress_skb)) != NULL) {\n\t\tskb_bpf_redirect_clear(skb);\n\t\tsock_drop(psock->sk, skb);\n\t}\n\t__sk_psock_purge_ingress_msg(psock);\n}\n\nstatic void sk_psock_link_destroy(struct sk_psock *psock)\n{\n\tstruct sk_psock_link *link, *tmp;\n\n\tlist_for_each_entry_safe(link, tmp, &psock->link, list) {\n\t\tlist_del(&link->list);\n\t\tsk_psock_free_link(link);\n\t}\n}\n\nvoid sk_psock_stop(struct sk_psock *psock)\n{\n\tspin_lock_bh(&psock->ingress_lock);\n\tsk_psock_clear_state(psock, SK_PSOCK_TX_ENABLED);\n\tsk_psock_cork_free(psock);\n\tspin_unlock_bh(&psock->ingress_lock);\n}\n\nstatic void sk_psock_done_strp(struct sk_psock *psock);\n\nstatic void sk_psock_destroy(struct work_struct *work)\n{\n\tstruct sk_psock *psock = container_of(to_rcu_work(work),\n\t\t\t\t\t      struct sk_psock, rwork);\n\t \n\n\tsk_psock_done_strp(psock);\n\n\tcancel_delayed_work_sync(&psock->work);\n\t__sk_psock_zap_ingress(psock);\n\tmutex_destroy(&psock->work_mutex);\n\n\tpsock_progs_drop(&psock->progs);\n\n\tsk_psock_link_destroy(psock);\n\tsk_psock_cork_free(psock);\n\n\tif (psock->sk_redir)\n\t\tsock_put(psock->sk_redir);\n\tif (psock->sk_pair)\n\t\tsock_put(psock->sk_pair);\n\tsock_put(psock->sk);\n\tkfree(psock);\n}\n\nvoid sk_psock_drop(struct sock *sk, struct sk_psock *psock)\n{\n\twrite_lock_bh(&sk->sk_callback_lock);\n\tsk_psock_restore_proto(sk, psock);\n\trcu_assign_sk_user_data(sk, NULL);\n\tif (psock->progs.stream_parser)\n\t\tsk_psock_stop_strp(sk, psock);\n\telse if (psock->progs.stream_verdict || psock->progs.skb_verdict)\n\t\tsk_psock_stop_verdict(sk, psock);\n\twrite_unlock_bh(&sk->sk_callback_lock);\n\n\tsk_psock_stop(psock);\n\n\tINIT_RCU_WORK(&psock->rwork, sk_psock_destroy);\n\tqueue_rcu_work(system_wq, &psock->rwork);\n}\nEXPORT_SYMBOL_GPL(sk_psock_drop);\n\nstatic int sk_psock_map_verd(int verdict, bool redir)\n{\n\tswitch (verdict) {\n\tcase SK_PASS:\n\t\treturn redir ? __SK_REDIRECT : __SK_PASS;\n\tcase SK_DROP:\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn __SK_DROP;\n}\n\nint sk_psock_msg_verdict(struct sock *sk, struct sk_psock *psock,\n\t\t\t struct sk_msg *msg)\n{\n\tstruct bpf_prog *prog;\n\tint ret;\n\n\trcu_read_lock();\n\tprog = READ_ONCE(psock->progs.msg_parser);\n\tif (unlikely(!prog)) {\n\t\tret = __SK_PASS;\n\t\tgoto out;\n\t}\n\n\tsk_msg_compute_data_pointers(msg);\n\tmsg->sk = sk;\n\tret = bpf_prog_run_pin_on_cpu(prog, msg);\n\tret = sk_psock_map_verd(ret, msg->sk_redir);\n\tpsock->apply_bytes = msg->apply_bytes;\n\tif (ret == __SK_REDIRECT) {\n\t\tif (psock->sk_redir) {\n\t\t\tsock_put(psock->sk_redir);\n\t\t\tpsock->sk_redir = NULL;\n\t\t}\n\t\tif (!msg->sk_redir) {\n\t\t\tret = __SK_DROP;\n\t\t\tgoto out;\n\t\t}\n\t\tpsock->redir_ingress = sk_msg_to_ingress(msg);\n\t\tpsock->sk_redir = msg->sk_redir;\n\t\tsock_hold(psock->sk_redir);\n\t}\nout:\n\trcu_read_unlock();\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(sk_psock_msg_verdict);\n\nstatic int sk_psock_skb_redirect(struct sk_psock *from, struct sk_buff *skb)\n{\n\tstruct sk_psock *psock_other;\n\tstruct sock *sk_other;\n\n\tsk_other = skb_bpf_redirect_fetch(skb);\n\t \n\tif (unlikely(!sk_other)) {\n\t\tskb_bpf_redirect_clear(skb);\n\t\tsock_drop(from->sk, skb);\n\t\treturn -EIO;\n\t}\n\tpsock_other = sk_psock(sk_other);\n\t \n\tif (!psock_other || sock_flag(sk_other, SOCK_DEAD)) {\n\t\tskb_bpf_redirect_clear(skb);\n\t\tsock_drop(from->sk, skb);\n\t\treturn -EIO;\n\t}\n\tspin_lock_bh(&psock_other->ingress_lock);\n\tif (!sk_psock_test_state(psock_other, SK_PSOCK_TX_ENABLED)) {\n\t\tspin_unlock_bh(&psock_other->ingress_lock);\n\t\tskb_bpf_redirect_clear(skb);\n\t\tsock_drop(from->sk, skb);\n\t\treturn -EIO;\n\t}\n\n\tskb_queue_tail(&psock_other->ingress_skb, skb);\n\tschedule_delayed_work(&psock_other->work, 0);\n\tspin_unlock_bh(&psock_other->ingress_lock);\n\treturn 0;\n}\n\nstatic void sk_psock_tls_verdict_apply(struct sk_buff *skb,\n\t\t\t\t       struct sk_psock *from, int verdict)\n{\n\tswitch (verdict) {\n\tcase __SK_REDIRECT:\n\t\tsk_psock_skb_redirect(from, skb);\n\t\tbreak;\n\tcase __SK_PASS:\n\tcase __SK_DROP:\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nint sk_psock_tls_strp_read(struct sk_psock *psock, struct sk_buff *skb)\n{\n\tstruct bpf_prog *prog;\n\tint ret = __SK_PASS;\n\n\trcu_read_lock();\n\tprog = READ_ONCE(psock->progs.stream_verdict);\n\tif (likely(prog)) {\n\t\tskb->sk = psock->sk;\n\t\tskb_dst_drop(skb);\n\t\tskb_bpf_redirect_clear(skb);\n\t\tret = bpf_prog_run_pin_on_cpu(prog, skb);\n\t\tret = sk_psock_map_verd(ret, skb_bpf_redirect_fetch(skb));\n\t\tskb->sk = NULL;\n\t}\n\tsk_psock_tls_verdict_apply(skb, psock, ret);\n\trcu_read_unlock();\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(sk_psock_tls_strp_read);\n\nstatic int sk_psock_verdict_apply(struct sk_psock *psock, struct sk_buff *skb,\n\t\t\t\t  int verdict)\n{\n\tstruct sock *sk_other;\n\tint err = 0;\n\tu32 len, off;\n\n\tswitch (verdict) {\n\tcase __SK_PASS:\n\t\terr = -EIO;\n\t\tsk_other = psock->sk;\n\t\tif (sock_flag(sk_other, SOCK_DEAD) ||\n\t\t    !sk_psock_test_state(psock, SK_PSOCK_TX_ENABLED))\n\t\t\tgoto out_free;\n\n\t\tskb_bpf_set_ingress(skb);\n\n\t\t \n\t\tif (skb_queue_empty(&psock->ingress_skb)) {\n\t\t\tlen = skb->len;\n\t\t\toff = 0;\n\t\t\tif (skb_bpf_strparser(skb)) {\n\t\t\t\tstruct strp_msg *stm = strp_msg(skb);\n\n\t\t\t\toff = stm->offset;\n\t\t\t\tlen = stm->full_len;\n\t\t\t}\n\t\t\terr = sk_psock_skb_ingress_self(psock, skb, off, len);\n\t\t}\n\t\tif (err < 0) {\n\t\t\tspin_lock_bh(&psock->ingress_lock);\n\t\t\tif (sk_psock_test_state(psock, SK_PSOCK_TX_ENABLED)) {\n\t\t\t\tskb_queue_tail(&psock->ingress_skb, skb);\n\t\t\t\tschedule_delayed_work(&psock->work, 0);\n\t\t\t\terr = 0;\n\t\t\t}\n\t\t\tspin_unlock_bh(&psock->ingress_lock);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out_free;\n\t\t}\n\t\tbreak;\n\tcase __SK_REDIRECT:\n\t\ttcp_eat_skb(psock->sk, skb);\n\t\terr = sk_psock_skb_redirect(psock, skb);\n\t\tbreak;\n\tcase __SK_DROP:\n\tdefault:\nout_free:\n\t\tskb_bpf_redirect_clear(skb);\n\t\ttcp_eat_skb(psock->sk, skb);\n\t\tsock_drop(psock->sk, skb);\n\t}\n\n\treturn err;\n}\n\nstatic void sk_psock_write_space(struct sock *sk)\n{\n\tstruct sk_psock *psock;\n\tvoid (*write_space)(struct sock *sk) = NULL;\n\n\trcu_read_lock();\n\tpsock = sk_psock(sk);\n\tif (likely(psock)) {\n\t\tif (sk_psock_test_state(psock, SK_PSOCK_TX_ENABLED))\n\t\t\tschedule_delayed_work(&psock->work, 0);\n\t\twrite_space = psock->saved_write_space;\n\t}\n\trcu_read_unlock();\n\tif (write_space)\n\t\twrite_space(sk);\n}\n\n#if IS_ENABLED(CONFIG_BPF_STREAM_PARSER)\nstatic void sk_psock_strp_read(struct strparser *strp, struct sk_buff *skb)\n{\n\tstruct sk_psock *psock;\n\tstruct bpf_prog *prog;\n\tint ret = __SK_DROP;\n\tstruct sock *sk;\n\n\trcu_read_lock();\n\tsk = strp->sk;\n\tpsock = sk_psock(sk);\n\tif (unlikely(!psock)) {\n\t\tsock_drop(sk, skb);\n\t\tgoto out;\n\t}\n\tprog = READ_ONCE(psock->progs.stream_verdict);\n\tif (likely(prog)) {\n\t\tskb->sk = sk;\n\t\tskb_dst_drop(skb);\n\t\tskb_bpf_redirect_clear(skb);\n\t\tret = bpf_prog_run_pin_on_cpu(prog, skb);\n\t\tskb_bpf_set_strparser(skb);\n\t\tret = sk_psock_map_verd(ret, skb_bpf_redirect_fetch(skb));\n\t\tskb->sk = NULL;\n\t}\n\tsk_psock_verdict_apply(psock, skb, ret);\nout:\n\trcu_read_unlock();\n}\n\nstatic int sk_psock_strp_read_done(struct strparser *strp, int err)\n{\n\treturn err;\n}\n\nstatic int sk_psock_strp_parse(struct strparser *strp, struct sk_buff *skb)\n{\n\tstruct sk_psock *psock = container_of(strp, struct sk_psock, strp);\n\tstruct bpf_prog *prog;\n\tint ret = skb->len;\n\n\trcu_read_lock();\n\tprog = READ_ONCE(psock->progs.stream_parser);\n\tif (likely(prog)) {\n\t\tskb->sk = psock->sk;\n\t\tret = bpf_prog_run_pin_on_cpu(prog, skb);\n\t\tskb->sk = NULL;\n\t}\n\trcu_read_unlock();\n\treturn ret;\n}\n\n \nstatic void sk_psock_strp_data_ready(struct sock *sk)\n{\n\tstruct sk_psock *psock;\n\n\ttrace_sk_data_ready(sk);\n\n\trcu_read_lock();\n\tpsock = sk_psock(sk);\n\tif (likely(psock)) {\n\t\tif (tls_sw_has_ctx_rx(sk)) {\n\t\t\tpsock->saved_data_ready(sk);\n\t\t} else {\n\t\t\twrite_lock_bh(&sk->sk_callback_lock);\n\t\t\tstrp_data_ready(&psock->strp);\n\t\t\twrite_unlock_bh(&sk->sk_callback_lock);\n\t\t}\n\t}\n\trcu_read_unlock();\n}\n\nint sk_psock_init_strp(struct sock *sk, struct sk_psock *psock)\n{\n\tint ret;\n\n\tstatic const struct strp_callbacks cb = {\n\t\t.rcv_msg\t= sk_psock_strp_read,\n\t\t.read_sock_done\t= sk_psock_strp_read_done,\n\t\t.parse_msg\t= sk_psock_strp_parse,\n\t};\n\n\tret = strp_init(&psock->strp, sk, &cb);\n\tif (!ret)\n\t\tsk_psock_set_state(psock, SK_PSOCK_RX_STRP_ENABLED);\n\n\treturn ret;\n}\n\nvoid sk_psock_start_strp(struct sock *sk, struct sk_psock *psock)\n{\n\tif (psock->saved_data_ready)\n\t\treturn;\n\n\tpsock->saved_data_ready = sk->sk_data_ready;\n\tsk->sk_data_ready = sk_psock_strp_data_ready;\n\tsk->sk_write_space = sk_psock_write_space;\n}\n\nvoid sk_psock_stop_strp(struct sock *sk, struct sk_psock *psock)\n{\n\tpsock_set_prog(&psock->progs.stream_parser, NULL);\n\n\tif (!psock->saved_data_ready)\n\t\treturn;\n\n\tsk->sk_data_ready = psock->saved_data_ready;\n\tpsock->saved_data_ready = NULL;\n\tstrp_stop(&psock->strp);\n}\n\nstatic void sk_psock_done_strp(struct sk_psock *psock)\n{\n\t \n\tif (sk_psock_test_state(psock, SK_PSOCK_RX_STRP_ENABLED))\n\t\tstrp_done(&psock->strp);\n}\n#else\nstatic void sk_psock_done_strp(struct sk_psock *psock)\n{\n}\n#endif  \n\nstatic int sk_psock_verdict_recv(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct sk_psock *psock;\n\tstruct bpf_prog *prog;\n\tint ret = __SK_DROP;\n\tint len = skb->len;\n\n\trcu_read_lock();\n\tpsock = sk_psock(sk);\n\tif (unlikely(!psock)) {\n\t\tlen = 0;\n\t\ttcp_eat_skb(sk, skb);\n\t\tsock_drop(sk, skb);\n\t\tgoto out;\n\t}\n\tprog = READ_ONCE(psock->progs.stream_verdict);\n\tif (!prog)\n\t\tprog = READ_ONCE(psock->progs.skb_verdict);\n\tif (likely(prog)) {\n\t\tskb_dst_drop(skb);\n\t\tskb_bpf_redirect_clear(skb);\n\t\tret = bpf_prog_run_pin_on_cpu(prog, skb);\n\t\tret = sk_psock_map_verd(ret, skb_bpf_redirect_fetch(skb));\n\t}\n\tret = sk_psock_verdict_apply(psock, skb, ret);\n\tif (ret < 0)\n\t\tlen = ret;\nout:\n\trcu_read_unlock();\n\treturn len;\n}\n\nstatic void sk_psock_verdict_data_ready(struct sock *sk)\n{\n\tstruct socket *sock = sk->sk_socket;\n\tconst struct proto_ops *ops;\n\tint copied;\n\n\ttrace_sk_data_ready(sk);\n\n\tif (unlikely(!sock))\n\t\treturn;\n\tops = READ_ONCE(sock->ops);\n\tif (!ops || !ops->read_skb)\n\t\treturn;\n\tcopied = ops->read_skb(sk, sk_psock_verdict_recv);\n\tif (copied >= 0) {\n\t\tstruct sk_psock *psock;\n\n\t\trcu_read_lock();\n\t\tpsock = sk_psock(sk);\n\t\tif (psock)\n\t\t\tpsock->saved_data_ready(sk);\n\t\trcu_read_unlock();\n\t}\n}\n\nvoid sk_psock_start_verdict(struct sock *sk, struct sk_psock *psock)\n{\n\tif (psock->saved_data_ready)\n\t\treturn;\n\n\tpsock->saved_data_ready = sk->sk_data_ready;\n\tsk->sk_data_ready = sk_psock_verdict_data_ready;\n\tsk->sk_write_space = sk_psock_write_space;\n}\n\nvoid sk_psock_stop_verdict(struct sock *sk, struct sk_psock *psock)\n{\n\tpsock_set_prog(&psock->progs.stream_verdict, NULL);\n\tpsock_set_prog(&psock->progs.skb_verdict, NULL);\n\n\tif (!psock->saved_data_ready)\n\t\treturn;\n\n\tsk->sk_data_ready = psock->saved_data_ready;\n\tpsock->saved_data_ready = NULL;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}