{
  "module_name": "flow_offload.c",
  "hash_id": "5230af42e6c34bb600beab7f2c1ae59d039dbe5a421981ea413330c4fadb55a4",
  "original_prompt": "Ingested from linux-6.6.14/net/core/flow_offload.c",
  "human_readable_source": " \n#include <linux/kernel.h>\n#include <linux/slab.h>\n#include <net/act_api.h>\n#include <net/flow_offload.h>\n#include <linux/rtnetlink.h>\n#include <linux/mutex.h>\n#include <linux/rhashtable.h>\n\nstruct flow_rule *flow_rule_alloc(unsigned int num_actions)\n{\n\tstruct flow_rule *rule;\n\tint i;\n\n\trule = kzalloc(struct_size(rule, action.entries, num_actions),\n\t\t       GFP_KERNEL);\n\tif (!rule)\n\t\treturn NULL;\n\n\trule->action.num_entries = num_actions;\n\t \n\tfor (i = 0; i < num_actions; i++)\n\t\trule->action.entries[i].hw_stats = FLOW_ACTION_HW_STATS_DONT_CARE;\n\n\treturn rule;\n}\nEXPORT_SYMBOL(flow_rule_alloc);\n\nstruct flow_offload_action *offload_action_alloc(unsigned int num_actions)\n{\n\tstruct flow_offload_action *fl_action;\n\tint i;\n\n\tfl_action = kzalloc(struct_size(fl_action, action.entries, num_actions),\n\t\t\t    GFP_KERNEL);\n\tif (!fl_action)\n\t\treturn NULL;\n\n\tfl_action->action.num_entries = num_actions;\n\t \n\tfor (i = 0; i < num_actions; i++)\n\t\tfl_action->action.entries[i].hw_stats = FLOW_ACTION_HW_STATS_DONT_CARE;\n\n\treturn fl_action;\n}\n\n#define FLOW_DISSECTOR_MATCH(__rule, __type, __out)\t\t\t\t\\\n\tconst struct flow_match *__m = &(__rule)->match;\t\t\t\\\n\tstruct flow_dissector *__d = (__m)->dissector;\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\t\\\n\t(__out)->key = skb_flow_dissector_target(__d, __type, (__m)->key);\t\\\n\t(__out)->mask = skb_flow_dissector_target(__d, __type, (__m)->mask);\t\\\n\nvoid flow_rule_match_meta(const struct flow_rule *rule,\n\t\t\t  struct flow_match_meta *out)\n{\n\tFLOW_DISSECTOR_MATCH(rule, FLOW_DISSECTOR_KEY_META, out);\n}\nEXPORT_SYMBOL(flow_rule_match_meta);\n\nvoid flow_rule_match_basic(const struct flow_rule *rule,\n\t\t\t   struct flow_match_basic *out)\n{\n\tFLOW_DISSECTOR_MATCH(rule, FLOW_DISSECTOR_KEY_BASIC, out);\n}\nEXPORT_SYMBOL(flow_rule_match_basic);\n\nvoid flow_rule_match_control(const struct flow_rule *rule,\n\t\t\t     struct flow_match_control *out)\n{\n\tFLOW_DISSECTOR_MATCH(rule, FLOW_DISSECTOR_KEY_CONTROL, out);\n}\nEXPORT_SYMBOL(flow_rule_match_control);\n\nvoid flow_rule_match_eth_addrs(const struct flow_rule *rule,\n\t\t\t       struct flow_match_eth_addrs *out)\n{\n\tFLOW_DISSECTOR_MATCH(rule, FLOW_DISSECTOR_KEY_ETH_ADDRS, out);\n}\nEXPORT_SYMBOL(flow_rule_match_eth_addrs);\n\nvoid flow_rule_match_vlan(const struct flow_rule *rule,\n\t\t\t  struct flow_match_vlan *out)\n{\n\tFLOW_DISSECTOR_MATCH(rule, FLOW_DISSECTOR_KEY_VLAN, out);\n}\nEXPORT_SYMBOL(flow_rule_match_vlan);\n\nvoid flow_rule_match_cvlan(const struct flow_rule *rule,\n\t\t\t   struct flow_match_vlan *out)\n{\n\tFLOW_DISSECTOR_MATCH(rule, FLOW_DISSECTOR_KEY_CVLAN, out);\n}\nEXPORT_SYMBOL(flow_rule_match_cvlan);\n\nvoid flow_rule_match_arp(const struct flow_rule *rule,\n\t\t\t struct flow_match_arp *out)\n{\n\tFLOW_DISSECTOR_MATCH(rule, FLOW_DISSECTOR_KEY_ARP, out);\n}\nEXPORT_SYMBOL(flow_rule_match_arp);\n\nvoid flow_rule_match_ipv4_addrs(const struct flow_rule *rule,\n\t\t\t\tstruct flow_match_ipv4_addrs *out)\n{\n\tFLOW_DISSECTOR_MATCH(rule, FLOW_DISSECTOR_KEY_IPV4_ADDRS, out);\n}\nEXPORT_SYMBOL(flow_rule_match_ipv4_addrs);\n\nvoid flow_rule_match_ipv6_addrs(const struct flow_rule *rule,\n\t\t\t\tstruct flow_match_ipv6_addrs *out)\n{\n\tFLOW_DISSECTOR_MATCH(rule, FLOW_DISSECTOR_KEY_IPV6_ADDRS, out);\n}\nEXPORT_SYMBOL(flow_rule_match_ipv6_addrs);\n\nvoid flow_rule_match_ip(const struct flow_rule *rule,\n\t\t\tstruct flow_match_ip *out)\n{\n\tFLOW_DISSECTOR_MATCH(rule, FLOW_DISSECTOR_KEY_IP, out);\n}\nEXPORT_SYMBOL(flow_rule_match_ip);\n\nvoid flow_rule_match_ports(const struct flow_rule *rule,\n\t\t\t   struct flow_match_ports *out)\n{\n\tFLOW_DISSECTOR_MATCH(rule, FLOW_DISSECTOR_KEY_PORTS, out);\n}\nEXPORT_SYMBOL(flow_rule_match_ports);\n\nvoid flow_rule_match_ports_range(const struct flow_rule *rule,\n\t\t\t\t struct flow_match_ports_range *out)\n{\n\tFLOW_DISSECTOR_MATCH(rule, FLOW_DISSECTOR_KEY_PORTS_RANGE, out);\n}\nEXPORT_SYMBOL(flow_rule_match_ports_range);\n\nvoid flow_rule_match_tcp(const struct flow_rule *rule,\n\t\t\t struct flow_match_tcp *out)\n{\n\tFLOW_DISSECTOR_MATCH(rule, FLOW_DISSECTOR_KEY_TCP, out);\n}\nEXPORT_SYMBOL(flow_rule_match_tcp);\n\nvoid flow_rule_match_ipsec(const struct flow_rule *rule,\n\t\t\t   struct flow_match_ipsec *out)\n{\n\tFLOW_DISSECTOR_MATCH(rule, FLOW_DISSECTOR_KEY_IPSEC, out);\n}\nEXPORT_SYMBOL(flow_rule_match_ipsec);\n\nvoid flow_rule_match_icmp(const struct flow_rule *rule,\n\t\t\t  struct flow_match_icmp *out)\n{\n\tFLOW_DISSECTOR_MATCH(rule, FLOW_DISSECTOR_KEY_ICMP, out);\n}\nEXPORT_SYMBOL(flow_rule_match_icmp);\n\nvoid flow_rule_match_mpls(const struct flow_rule *rule,\n\t\t\t  struct flow_match_mpls *out)\n{\n\tFLOW_DISSECTOR_MATCH(rule, FLOW_DISSECTOR_KEY_MPLS, out);\n}\nEXPORT_SYMBOL(flow_rule_match_mpls);\n\nvoid flow_rule_match_enc_control(const struct flow_rule *rule,\n\t\t\t\t struct flow_match_control *out)\n{\n\tFLOW_DISSECTOR_MATCH(rule, FLOW_DISSECTOR_KEY_ENC_CONTROL, out);\n}\nEXPORT_SYMBOL(flow_rule_match_enc_control);\n\nvoid flow_rule_match_enc_ipv4_addrs(const struct flow_rule *rule,\n\t\t\t\t    struct flow_match_ipv4_addrs *out)\n{\n\tFLOW_DISSECTOR_MATCH(rule, FLOW_DISSECTOR_KEY_ENC_IPV4_ADDRS, out);\n}\nEXPORT_SYMBOL(flow_rule_match_enc_ipv4_addrs);\n\nvoid flow_rule_match_enc_ipv6_addrs(const struct flow_rule *rule,\n\t\t\t\t    struct flow_match_ipv6_addrs *out)\n{\n\tFLOW_DISSECTOR_MATCH(rule, FLOW_DISSECTOR_KEY_ENC_IPV6_ADDRS, out);\n}\nEXPORT_SYMBOL(flow_rule_match_enc_ipv6_addrs);\n\nvoid flow_rule_match_enc_ip(const struct flow_rule *rule,\n\t\t\t    struct flow_match_ip *out)\n{\n\tFLOW_DISSECTOR_MATCH(rule, FLOW_DISSECTOR_KEY_ENC_IP, out);\n}\nEXPORT_SYMBOL(flow_rule_match_enc_ip);\n\nvoid flow_rule_match_enc_ports(const struct flow_rule *rule,\n\t\t\t       struct flow_match_ports *out)\n{\n\tFLOW_DISSECTOR_MATCH(rule, FLOW_DISSECTOR_KEY_ENC_PORTS, out);\n}\nEXPORT_SYMBOL(flow_rule_match_enc_ports);\n\nvoid flow_rule_match_enc_keyid(const struct flow_rule *rule,\n\t\t\t       struct flow_match_enc_keyid *out)\n{\n\tFLOW_DISSECTOR_MATCH(rule, FLOW_DISSECTOR_KEY_ENC_KEYID, out);\n}\nEXPORT_SYMBOL(flow_rule_match_enc_keyid);\n\nvoid flow_rule_match_enc_opts(const struct flow_rule *rule,\n\t\t\t      struct flow_match_enc_opts *out)\n{\n\tFLOW_DISSECTOR_MATCH(rule, FLOW_DISSECTOR_KEY_ENC_OPTS, out);\n}\nEXPORT_SYMBOL(flow_rule_match_enc_opts);\n\nstruct flow_action_cookie *flow_action_cookie_create(void *data,\n\t\t\t\t\t\t     unsigned int len,\n\t\t\t\t\t\t     gfp_t gfp)\n{\n\tstruct flow_action_cookie *cookie;\n\n\tcookie = kmalloc(sizeof(*cookie) + len, gfp);\n\tif (!cookie)\n\t\treturn NULL;\n\tcookie->cookie_len = len;\n\tmemcpy(cookie->cookie, data, len);\n\treturn cookie;\n}\nEXPORT_SYMBOL(flow_action_cookie_create);\n\nvoid flow_action_cookie_destroy(struct flow_action_cookie *cookie)\n{\n\tkfree(cookie);\n}\nEXPORT_SYMBOL(flow_action_cookie_destroy);\n\nvoid flow_rule_match_ct(const struct flow_rule *rule,\n\t\t\tstruct flow_match_ct *out)\n{\n\tFLOW_DISSECTOR_MATCH(rule, FLOW_DISSECTOR_KEY_CT, out);\n}\nEXPORT_SYMBOL(flow_rule_match_ct);\n\nvoid flow_rule_match_pppoe(const struct flow_rule *rule,\n\t\t\t   struct flow_match_pppoe *out)\n{\n\tFLOW_DISSECTOR_MATCH(rule, FLOW_DISSECTOR_KEY_PPPOE, out);\n}\nEXPORT_SYMBOL(flow_rule_match_pppoe);\n\nvoid flow_rule_match_l2tpv3(const struct flow_rule *rule,\n\t\t\t    struct flow_match_l2tpv3 *out)\n{\n\tFLOW_DISSECTOR_MATCH(rule, FLOW_DISSECTOR_KEY_L2TPV3, out);\n}\nEXPORT_SYMBOL(flow_rule_match_l2tpv3);\n\nstruct flow_block_cb *flow_block_cb_alloc(flow_setup_cb_t *cb,\n\t\t\t\t\t  void *cb_ident, void *cb_priv,\n\t\t\t\t\t  void (*release)(void *cb_priv))\n{\n\tstruct flow_block_cb *block_cb;\n\n\tblock_cb = kzalloc(sizeof(*block_cb), GFP_KERNEL);\n\tif (!block_cb)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tblock_cb->cb = cb;\n\tblock_cb->cb_ident = cb_ident;\n\tblock_cb->cb_priv = cb_priv;\n\tblock_cb->release = release;\n\n\treturn block_cb;\n}\nEXPORT_SYMBOL(flow_block_cb_alloc);\n\nvoid flow_block_cb_free(struct flow_block_cb *block_cb)\n{\n\tif (block_cb->release)\n\t\tblock_cb->release(block_cb->cb_priv);\n\n\tkfree(block_cb);\n}\nEXPORT_SYMBOL(flow_block_cb_free);\n\nstruct flow_block_cb *flow_block_cb_lookup(struct flow_block *block,\n\t\t\t\t\t   flow_setup_cb_t *cb, void *cb_ident)\n{\n\tstruct flow_block_cb *block_cb;\n\n\tlist_for_each_entry(block_cb, &block->cb_list, list) {\n\t\tif (block_cb->cb == cb &&\n\t\t    block_cb->cb_ident == cb_ident)\n\t\t\treturn block_cb;\n\t}\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(flow_block_cb_lookup);\n\nvoid *flow_block_cb_priv(struct flow_block_cb *block_cb)\n{\n\treturn block_cb->cb_priv;\n}\nEXPORT_SYMBOL(flow_block_cb_priv);\n\nvoid flow_block_cb_incref(struct flow_block_cb *block_cb)\n{\n\tblock_cb->refcnt++;\n}\nEXPORT_SYMBOL(flow_block_cb_incref);\n\nunsigned int flow_block_cb_decref(struct flow_block_cb *block_cb)\n{\n\treturn --block_cb->refcnt;\n}\nEXPORT_SYMBOL(flow_block_cb_decref);\n\nbool flow_block_cb_is_busy(flow_setup_cb_t *cb, void *cb_ident,\n\t\t\t   struct list_head *driver_block_list)\n{\n\tstruct flow_block_cb *block_cb;\n\n\tlist_for_each_entry(block_cb, driver_block_list, driver_list) {\n\t\tif (block_cb->cb == cb &&\n\t\t    block_cb->cb_ident == cb_ident)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\nEXPORT_SYMBOL(flow_block_cb_is_busy);\n\nint flow_block_cb_setup_simple(struct flow_block_offload *f,\n\t\t\t       struct list_head *driver_block_list,\n\t\t\t       flow_setup_cb_t *cb,\n\t\t\t       void *cb_ident, void *cb_priv,\n\t\t\t       bool ingress_only)\n{\n\tstruct flow_block_cb *block_cb;\n\n\tif (ingress_only &&\n\t    f->binder_type != FLOW_BLOCK_BINDER_TYPE_CLSACT_INGRESS)\n\t\treturn -EOPNOTSUPP;\n\n\tf->driver_block_list = driver_block_list;\n\n\tswitch (f->command) {\n\tcase FLOW_BLOCK_BIND:\n\t\tif (flow_block_cb_is_busy(cb, cb_ident, driver_block_list))\n\t\t\treturn -EBUSY;\n\n\t\tblock_cb = flow_block_cb_alloc(cb, cb_ident, cb_priv, NULL);\n\t\tif (IS_ERR(block_cb))\n\t\t\treturn PTR_ERR(block_cb);\n\n\t\tflow_block_cb_add(block_cb, f);\n\t\tlist_add_tail(&block_cb->driver_list, driver_block_list);\n\t\treturn 0;\n\tcase FLOW_BLOCK_UNBIND:\n\t\tblock_cb = flow_block_cb_lookup(f->block, cb, cb_ident);\n\t\tif (!block_cb)\n\t\t\treturn -ENOENT;\n\n\t\tflow_block_cb_remove(block_cb, f);\n\t\tlist_del(&block_cb->driver_list);\n\t\treturn 0;\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\nEXPORT_SYMBOL(flow_block_cb_setup_simple);\n\nstatic DEFINE_MUTEX(flow_indr_block_lock);\nstatic LIST_HEAD(flow_block_indr_list);\nstatic LIST_HEAD(flow_block_indr_dev_list);\nstatic LIST_HEAD(flow_indir_dev_list);\n\nstruct flow_indr_dev {\n\tstruct list_head\t\tlist;\n\tflow_indr_block_bind_cb_t\t*cb;\n\tvoid\t\t\t\t*cb_priv;\n\trefcount_t\t\t\trefcnt;\n};\n\nstatic struct flow_indr_dev *flow_indr_dev_alloc(flow_indr_block_bind_cb_t *cb,\n\t\t\t\t\t\t void *cb_priv)\n{\n\tstruct flow_indr_dev *indr_dev;\n\n\tindr_dev = kmalloc(sizeof(*indr_dev), GFP_KERNEL);\n\tif (!indr_dev)\n\t\treturn NULL;\n\n\tindr_dev->cb\t\t= cb;\n\tindr_dev->cb_priv\t= cb_priv;\n\trefcount_set(&indr_dev->refcnt, 1);\n\n\treturn indr_dev;\n}\n\nstruct flow_indir_dev_info {\n\tvoid *data;\n\tstruct net_device *dev;\n\tstruct Qdisc *sch;\n\tenum tc_setup_type type;\n\tvoid (*cleanup)(struct flow_block_cb *block_cb);\n\tstruct list_head list;\n\tenum flow_block_command command;\n\tenum flow_block_binder_type binder_type;\n\tstruct list_head *cb_list;\n};\n\nstatic void existing_qdiscs_register(flow_indr_block_bind_cb_t *cb, void *cb_priv)\n{\n\tstruct flow_block_offload bo;\n\tstruct flow_indir_dev_info *cur;\n\n\tlist_for_each_entry(cur, &flow_indir_dev_list, list) {\n\t\tmemset(&bo, 0, sizeof(bo));\n\t\tbo.command = cur->command;\n\t\tbo.binder_type = cur->binder_type;\n\t\tINIT_LIST_HEAD(&bo.cb_list);\n\t\tcb(cur->dev, cur->sch, cb_priv, cur->type, &bo, cur->data, cur->cleanup);\n\t\tlist_splice(&bo.cb_list, cur->cb_list);\n\t}\n}\n\nint flow_indr_dev_register(flow_indr_block_bind_cb_t *cb, void *cb_priv)\n{\n\tstruct flow_indr_dev *indr_dev;\n\n\tmutex_lock(&flow_indr_block_lock);\n\tlist_for_each_entry(indr_dev, &flow_block_indr_dev_list, list) {\n\t\tif (indr_dev->cb == cb &&\n\t\t    indr_dev->cb_priv == cb_priv) {\n\t\t\trefcount_inc(&indr_dev->refcnt);\n\t\t\tmutex_unlock(&flow_indr_block_lock);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tindr_dev = flow_indr_dev_alloc(cb, cb_priv);\n\tif (!indr_dev) {\n\t\tmutex_unlock(&flow_indr_block_lock);\n\t\treturn -ENOMEM;\n\t}\n\n\tlist_add(&indr_dev->list, &flow_block_indr_dev_list);\n\texisting_qdiscs_register(cb, cb_priv);\n\tmutex_unlock(&flow_indr_block_lock);\n\n\ttcf_action_reoffload_cb(cb, cb_priv, true);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(flow_indr_dev_register);\n\nstatic void __flow_block_indr_cleanup(void (*release)(void *cb_priv),\n\t\t\t\t      void *cb_priv,\n\t\t\t\t      struct list_head *cleanup_list)\n{\n\tstruct flow_block_cb *this, *next;\n\n\tlist_for_each_entry_safe(this, next, &flow_block_indr_list, indr.list) {\n\t\tif (this->release == release &&\n\t\t    this->indr.cb_priv == cb_priv)\n\t\t\tlist_move(&this->indr.list, cleanup_list);\n\t}\n}\n\nstatic void flow_block_indr_notify(struct list_head *cleanup_list)\n{\n\tstruct flow_block_cb *this, *next;\n\n\tlist_for_each_entry_safe(this, next, cleanup_list, indr.list) {\n\t\tlist_del(&this->indr.list);\n\t\tthis->indr.cleanup(this);\n\t}\n}\n\nvoid flow_indr_dev_unregister(flow_indr_block_bind_cb_t *cb, void *cb_priv,\n\t\t\t      void (*release)(void *cb_priv))\n{\n\tstruct flow_indr_dev *this, *next, *indr_dev = NULL;\n\tLIST_HEAD(cleanup_list);\n\n\tmutex_lock(&flow_indr_block_lock);\n\tlist_for_each_entry_safe(this, next, &flow_block_indr_dev_list, list) {\n\t\tif (this->cb == cb &&\n\t\t    this->cb_priv == cb_priv &&\n\t\t    refcount_dec_and_test(&this->refcnt)) {\n\t\t\tindr_dev = this;\n\t\t\tlist_del(&indr_dev->list);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!indr_dev) {\n\t\tmutex_unlock(&flow_indr_block_lock);\n\t\treturn;\n\t}\n\n\t__flow_block_indr_cleanup(release, cb_priv, &cleanup_list);\n\tmutex_unlock(&flow_indr_block_lock);\n\n\ttcf_action_reoffload_cb(cb, cb_priv, false);\n\tflow_block_indr_notify(&cleanup_list);\n\tkfree(indr_dev);\n}\nEXPORT_SYMBOL(flow_indr_dev_unregister);\n\nstatic void flow_block_indr_init(struct flow_block_cb *flow_block,\n\t\t\t\t struct flow_block_offload *bo,\n\t\t\t\t struct net_device *dev, struct Qdisc *sch, void *data,\n\t\t\t\t void *cb_priv,\n\t\t\t\t void (*cleanup)(struct flow_block_cb *block_cb))\n{\n\tflow_block->indr.binder_type = bo->binder_type;\n\tflow_block->indr.data = data;\n\tflow_block->indr.cb_priv = cb_priv;\n\tflow_block->indr.dev = dev;\n\tflow_block->indr.sch = sch;\n\tflow_block->indr.cleanup = cleanup;\n}\n\nstruct flow_block_cb *flow_indr_block_cb_alloc(flow_setup_cb_t *cb,\n\t\t\t\t\t       void *cb_ident, void *cb_priv,\n\t\t\t\t\t       void (*release)(void *cb_priv),\n\t\t\t\t\t       struct flow_block_offload *bo,\n\t\t\t\t\t       struct net_device *dev,\n\t\t\t\t\t       struct Qdisc *sch, void *data,\n\t\t\t\t\t       void *indr_cb_priv,\n\t\t\t\t\t       void (*cleanup)(struct flow_block_cb *block_cb))\n{\n\tstruct flow_block_cb *block_cb;\n\n\tblock_cb = flow_block_cb_alloc(cb, cb_ident, cb_priv, release);\n\tif (IS_ERR(block_cb))\n\t\tgoto out;\n\n\tflow_block_indr_init(block_cb, bo, dev, sch, data, indr_cb_priv, cleanup);\n\tlist_add(&block_cb->indr.list, &flow_block_indr_list);\n\nout:\n\treturn block_cb;\n}\nEXPORT_SYMBOL(flow_indr_block_cb_alloc);\n\nstatic struct flow_indir_dev_info *find_indir_dev(void *data)\n{\n\tstruct flow_indir_dev_info *cur;\n\n\tlist_for_each_entry(cur, &flow_indir_dev_list, list) {\n\t\tif (cur->data == data)\n\t\t\treturn cur;\n\t}\n\treturn NULL;\n}\n\nstatic int indir_dev_add(void *data, struct net_device *dev, struct Qdisc *sch,\n\t\t\t enum tc_setup_type type, void (*cleanup)(struct flow_block_cb *block_cb),\n\t\t\t struct flow_block_offload *bo)\n{\n\tstruct flow_indir_dev_info *info;\n\n\tinfo = find_indir_dev(data);\n\tif (info)\n\t\treturn -EEXIST;\n\n\tinfo = kzalloc(sizeof(*info), GFP_KERNEL);\n\tif (!info)\n\t\treturn -ENOMEM;\n\n\tinfo->data = data;\n\tinfo->dev = dev;\n\tinfo->sch = sch;\n\tinfo->type = type;\n\tinfo->cleanup = cleanup;\n\tinfo->command = bo->command;\n\tinfo->binder_type = bo->binder_type;\n\tinfo->cb_list = bo->cb_list_head;\n\n\tlist_add(&info->list, &flow_indir_dev_list);\n\treturn 0;\n}\n\nstatic int indir_dev_remove(void *data)\n{\n\tstruct flow_indir_dev_info *info;\n\n\tinfo = find_indir_dev(data);\n\tif (!info)\n\t\treturn -ENOENT;\n\n\tlist_del(&info->list);\n\n\tkfree(info);\n\treturn 0;\n}\n\nint flow_indr_dev_setup_offload(struct net_device *dev,\tstruct Qdisc *sch,\n\t\t\t\tenum tc_setup_type type, void *data,\n\t\t\t\tstruct flow_block_offload *bo,\n\t\t\t\tvoid (*cleanup)(struct flow_block_cb *block_cb))\n{\n\tstruct flow_indr_dev *this;\n\tu32 count = 0;\n\tint err;\n\n\tmutex_lock(&flow_indr_block_lock);\n\tif (bo) {\n\t\tif (bo->command == FLOW_BLOCK_BIND)\n\t\t\tindir_dev_add(data, dev, sch, type, cleanup, bo);\n\t\telse if (bo->command == FLOW_BLOCK_UNBIND)\n\t\t\tindir_dev_remove(data);\n\t}\n\n\tlist_for_each_entry(this, &flow_block_indr_dev_list, list) {\n\t\terr = this->cb(dev, sch, this->cb_priv, type, bo, data, cleanup);\n\t\tif (!err)\n\t\t\tcount++;\n\t}\n\n\tmutex_unlock(&flow_indr_block_lock);\n\n\treturn (bo && list_empty(&bo->cb_list)) ? -EOPNOTSUPP : count;\n}\nEXPORT_SYMBOL(flow_indr_dev_setup_offload);\n\nbool flow_indr_dev_exists(void)\n{\n\treturn !list_empty(&flow_block_indr_dev_list);\n}\nEXPORT_SYMBOL(flow_indr_dev_exists);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}