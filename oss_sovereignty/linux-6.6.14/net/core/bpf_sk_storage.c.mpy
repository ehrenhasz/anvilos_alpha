{
  "module_name": "bpf_sk_storage.c",
  "hash_id": "87b5384a00d348f0de98186497c30d39113aff9f0144a7be2c5fb696b27edeb6",
  "original_prompt": "Ingested from linux-6.6.14/net/core/bpf_sk_storage.c",
  "human_readable_source": "\n \n#include <linux/rculist.h>\n#include <linux/list.h>\n#include <linux/hash.h>\n#include <linux/types.h>\n#include <linux/spinlock.h>\n#include <linux/bpf.h>\n#include <linux/btf.h>\n#include <linux/btf_ids.h>\n#include <linux/bpf_local_storage.h>\n#include <net/bpf_sk_storage.h>\n#include <net/sock.h>\n#include <uapi/linux/sock_diag.h>\n#include <uapi/linux/btf.h>\n#include <linux/rcupdate_trace.h>\n\nDEFINE_BPF_STORAGE_CACHE(sk_cache);\n\nstatic struct bpf_local_storage_data *\nbpf_sk_storage_lookup(struct sock *sk, struct bpf_map *map, bool cacheit_lockit)\n{\n\tstruct bpf_local_storage *sk_storage;\n\tstruct bpf_local_storage_map *smap;\n\n\tsk_storage =\n\t\trcu_dereference_check(sk->sk_bpf_storage, bpf_rcu_lock_held());\n\tif (!sk_storage)\n\t\treturn NULL;\n\n\tsmap = (struct bpf_local_storage_map *)map;\n\treturn bpf_local_storage_lookup(sk_storage, smap, cacheit_lockit);\n}\n\nstatic int bpf_sk_storage_del(struct sock *sk, struct bpf_map *map)\n{\n\tstruct bpf_local_storage_data *sdata;\n\n\tsdata = bpf_sk_storage_lookup(sk, map, false);\n\tif (!sdata)\n\t\treturn -ENOENT;\n\n\tbpf_selem_unlink(SELEM(sdata), false);\n\n\treturn 0;\n}\n\n \nvoid bpf_sk_storage_free(struct sock *sk)\n{\n\tstruct bpf_local_storage *sk_storage;\n\n\trcu_read_lock();\n\tsk_storage = rcu_dereference(sk->sk_bpf_storage);\n\tif (!sk_storage) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\n\tbpf_local_storage_destroy(sk_storage);\n\trcu_read_unlock();\n}\n\nstatic void bpf_sk_storage_map_free(struct bpf_map *map)\n{\n\tbpf_local_storage_map_free(map, &sk_cache, NULL);\n}\n\nstatic struct bpf_map *bpf_sk_storage_map_alloc(union bpf_attr *attr)\n{\n\treturn bpf_local_storage_map_alloc(attr, &sk_cache, false);\n}\n\nstatic int notsupp_get_next_key(struct bpf_map *map, void *key,\n\t\t\t\tvoid *next_key)\n{\n\treturn -ENOTSUPP;\n}\n\nstatic void *bpf_fd_sk_storage_lookup_elem(struct bpf_map *map, void *key)\n{\n\tstruct bpf_local_storage_data *sdata;\n\tstruct socket *sock;\n\tint fd, err;\n\n\tfd = *(int *)key;\n\tsock = sockfd_lookup(fd, &err);\n\tif (sock) {\n\t\tsdata = bpf_sk_storage_lookup(sock->sk, map, true);\n\t\tsockfd_put(sock);\n\t\treturn sdata ? sdata->data : NULL;\n\t}\n\n\treturn ERR_PTR(err);\n}\n\nstatic long bpf_fd_sk_storage_update_elem(struct bpf_map *map, void *key,\n\t\t\t\t\t  void *value, u64 map_flags)\n{\n\tstruct bpf_local_storage_data *sdata;\n\tstruct socket *sock;\n\tint fd, err;\n\n\tfd = *(int *)key;\n\tsock = sockfd_lookup(fd, &err);\n\tif (sock) {\n\t\tsdata = bpf_local_storage_update(\n\t\t\tsock->sk, (struct bpf_local_storage_map *)map, value,\n\t\t\tmap_flags, GFP_ATOMIC);\n\t\tsockfd_put(sock);\n\t\treturn PTR_ERR_OR_ZERO(sdata);\n\t}\n\n\treturn err;\n}\n\nstatic long bpf_fd_sk_storage_delete_elem(struct bpf_map *map, void *key)\n{\n\tstruct socket *sock;\n\tint fd, err;\n\n\tfd = *(int *)key;\n\tsock = sockfd_lookup(fd, &err);\n\tif (sock) {\n\t\terr = bpf_sk_storage_del(sock->sk, map);\n\t\tsockfd_put(sock);\n\t\treturn err;\n\t}\n\n\treturn err;\n}\n\nstatic struct bpf_local_storage_elem *\nbpf_sk_storage_clone_elem(struct sock *newsk,\n\t\t\t  struct bpf_local_storage_map *smap,\n\t\t\t  struct bpf_local_storage_elem *selem)\n{\n\tstruct bpf_local_storage_elem *copy_selem;\n\n\tcopy_selem = bpf_selem_alloc(smap, newsk, NULL, true, GFP_ATOMIC);\n\tif (!copy_selem)\n\t\treturn NULL;\n\n\tif (btf_record_has_field(smap->map.record, BPF_SPIN_LOCK))\n\t\tcopy_map_value_locked(&smap->map, SDATA(copy_selem)->data,\n\t\t\t\t      SDATA(selem)->data, true);\n\telse\n\t\tcopy_map_value(&smap->map, SDATA(copy_selem)->data,\n\t\t\t       SDATA(selem)->data);\n\n\treturn copy_selem;\n}\n\nint bpf_sk_storage_clone(const struct sock *sk, struct sock *newsk)\n{\n\tstruct bpf_local_storage *new_sk_storage = NULL;\n\tstruct bpf_local_storage *sk_storage;\n\tstruct bpf_local_storage_elem *selem;\n\tint ret = 0;\n\n\tRCU_INIT_POINTER(newsk->sk_bpf_storage, NULL);\n\n\trcu_read_lock();\n\tsk_storage = rcu_dereference(sk->sk_bpf_storage);\n\n\tif (!sk_storage || hlist_empty(&sk_storage->list))\n\t\tgoto out;\n\n\thlist_for_each_entry_rcu(selem, &sk_storage->list, snode) {\n\t\tstruct bpf_local_storage_elem *copy_selem;\n\t\tstruct bpf_local_storage_map *smap;\n\t\tstruct bpf_map *map;\n\n\t\tsmap = rcu_dereference(SDATA(selem)->smap);\n\t\tif (!(smap->map.map_flags & BPF_F_CLONE))\n\t\t\tcontinue;\n\n\t\t \n\t\tmap = bpf_map_inc_not_zero(&smap->map);\n\t\tif (IS_ERR(map))\n\t\t\tcontinue;\n\n\t\tcopy_selem = bpf_sk_storage_clone_elem(newsk, smap, selem);\n\t\tif (!copy_selem) {\n\t\t\tret = -ENOMEM;\n\t\t\tbpf_map_put(map);\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (new_sk_storage) {\n\t\t\tbpf_selem_link_map(smap, copy_selem);\n\t\t\tbpf_selem_link_storage_nolock(new_sk_storage, copy_selem);\n\t\t} else {\n\t\t\tret = bpf_local_storage_alloc(newsk, smap, copy_selem, GFP_ATOMIC);\n\t\t\tif (ret) {\n\t\t\t\tbpf_selem_free(copy_selem, smap, true);\n\t\t\t\tatomic_sub(smap->elem_size,\n\t\t\t\t\t   &newsk->sk_omem_alloc);\n\t\t\t\tbpf_map_put(map);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tnew_sk_storage =\n\t\t\t\trcu_dereference(copy_selem->local_storage);\n\t\t}\n\t\tbpf_map_put(map);\n\t}\n\nout:\n\trcu_read_unlock();\n\n\t \n\n\treturn ret;\n}\n\n \nBPF_CALL_5(bpf_sk_storage_get, struct bpf_map *, map, struct sock *, sk,\n\t   void *, value, u64, flags, gfp_t, gfp_flags)\n{\n\tstruct bpf_local_storage_data *sdata;\n\n\tWARN_ON_ONCE(!bpf_rcu_lock_held());\n\tif (!sk || !sk_fullsock(sk) || flags > BPF_SK_STORAGE_GET_F_CREATE)\n\t\treturn (unsigned long)NULL;\n\n\tsdata = bpf_sk_storage_lookup(sk, map, true);\n\tif (sdata)\n\t\treturn (unsigned long)sdata->data;\n\n\tif (flags == BPF_SK_STORAGE_GET_F_CREATE &&\n\t     \n\t    refcount_inc_not_zero(&sk->sk_refcnt)) {\n\t\tsdata = bpf_local_storage_update(\n\t\t\tsk, (struct bpf_local_storage_map *)map, value,\n\t\t\tBPF_NOEXIST, gfp_flags);\n\t\t \n\t\tsock_put(sk);\n\t\treturn IS_ERR(sdata) ?\n\t\t\t(unsigned long)NULL : (unsigned long)sdata->data;\n\t}\n\n\treturn (unsigned long)NULL;\n}\n\nBPF_CALL_2(bpf_sk_storage_delete, struct bpf_map *, map, struct sock *, sk)\n{\n\tWARN_ON_ONCE(!bpf_rcu_lock_held());\n\tif (!sk || !sk_fullsock(sk))\n\t\treturn -EINVAL;\n\n\tif (refcount_inc_not_zero(&sk->sk_refcnt)) {\n\t\tint err;\n\n\t\terr = bpf_sk_storage_del(sk, map);\n\t\tsock_put(sk);\n\t\treturn err;\n\t}\n\n\treturn -ENOENT;\n}\n\nstatic int bpf_sk_storage_charge(struct bpf_local_storage_map *smap,\n\t\t\t\t void *owner, u32 size)\n{\n\tint optmem_max = READ_ONCE(sysctl_optmem_max);\n\tstruct sock *sk = (struct sock *)owner;\n\n\t \n\tif (size <= optmem_max &&\n\t    atomic_read(&sk->sk_omem_alloc) + size < optmem_max) {\n\t\tatomic_add(size, &sk->sk_omem_alloc);\n\t\treturn 0;\n\t}\n\n\treturn -ENOMEM;\n}\n\nstatic void bpf_sk_storage_uncharge(struct bpf_local_storage_map *smap,\n\t\t\t\t    void *owner, u32 size)\n{\n\tstruct sock *sk = owner;\n\n\tatomic_sub(size, &sk->sk_omem_alloc);\n}\n\nstatic struct bpf_local_storage __rcu **\nbpf_sk_storage_ptr(void *owner)\n{\n\tstruct sock *sk = owner;\n\n\treturn &sk->sk_bpf_storage;\n}\n\nconst struct bpf_map_ops sk_storage_map_ops = {\n\t.map_meta_equal = bpf_map_meta_equal,\n\t.map_alloc_check = bpf_local_storage_map_alloc_check,\n\t.map_alloc = bpf_sk_storage_map_alloc,\n\t.map_free = bpf_sk_storage_map_free,\n\t.map_get_next_key = notsupp_get_next_key,\n\t.map_lookup_elem = bpf_fd_sk_storage_lookup_elem,\n\t.map_update_elem = bpf_fd_sk_storage_update_elem,\n\t.map_delete_elem = bpf_fd_sk_storage_delete_elem,\n\t.map_check_btf = bpf_local_storage_map_check_btf,\n\t.map_btf_id = &bpf_local_storage_map_btf_id[0],\n\t.map_local_storage_charge = bpf_sk_storage_charge,\n\t.map_local_storage_uncharge = bpf_sk_storage_uncharge,\n\t.map_owner_storage_ptr = bpf_sk_storage_ptr,\n\t.map_mem_usage = bpf_local_storage_map_mem_usage,\n};\n\nconst struct bpf_func_proto bpf_sk_storage_get_proto = {\n\t.func\t\t= bpf_sk_storage_get,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_PTR_TO_MAP_VALUE_OR_NULL,\n\t.arg1_type\t= ARG_CONST_MAP_PTR,\n\t.arg2_type\t= ARG_PTR_TO_BTF_ID_SOCK_COMMON,\n\t.arg3_type\t= ARG_PTR_TO_MAP_VALUE_OR_NULL,\n\t.arg4_type\t= ARG_ANYTHING,\n};\n\nconst struct bpf_func_proto bpf_sk_storage_get_cg_sock_proto = {\n\t.func\t\t= bpf_sk_storage_get,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_PTR_TO_MAP_VALUE_OR_NULL,\n\t.arg1_type\t= ARG_CONST_MAP_PTR,\n\t.arg2_type\t= ARG_PTR_TO_CTX,  \n\t.arg3_type\t= ARG_PTR_TO_MAP_VALUE_OR_NULL,\n\t.arg4_type\t= ARG_ANYTHING,\n};\n\nconst struct bpf_func_proto bpf_sk_storage_delete_proto = {\n\t.func\t\t= bpf_sk_storage_delete,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_CONST_MAP_PTR,\n\t.arg2_type\t= ARG_PTR_TO_BTF_ID_SOCK_COMMON,\n};\n\nstatic bool bpf_sk_storage_tracing_allowed(const struct bpf_prog *prog)\n{\n\tconst struct btf *btf_vmlinux;\n\tconst struct btf_type *t;\n\tconst char *tname;\n\tu32 btf_id;\n\n\tif (prog->aux->dst_prog)\n\t\treturn false;\n\n\t \n\tswitch (prog->expected_attach_type) {\n\tcase BPF_TRACE_ITER:\n\tcase BPF_TRACE_RAW_TP:\n\t\t \n\t\treturn true;\n\tcase BPF_TRACE_FENTRY:\n\tcase BPF_TRACE_FEXIT:\n\t\tbtf_vmlinux = bpf_get_btf_vmlinux();\n\t\tif (IS_ERR_OR_NULL(btf_vmlinux))\n\t\t\treturn false;\n\t\tbtf_id = prog->aux->attach_btf_id;\n\t\tt = btf_type_by_id(btf_vmlinux, btf_id);\n\t\ttname = btf_name_by_offset(btf_vmlinux, t->name_off);\n\t\treturn !!strncmp(tname, \"bpf_sk_storage\",\n\t\t\t\t strlen(\"bpf_sk_storage\"));\n\tdefault:\n\t\treturn false;\n\t}\n\n\treturn false;\n}\n\n \nBPF_CALL_5(bpf_sk_storage_get_tracing, struct bpf_map *, map, struct sock *, sk,\n\t   void *, value, u64, flags, gfp_t, gfp_flags)\n{\n\tWARN_ON_ONCE(!bpf_rcu_lock_held());\n\tif (in_hardirq() || in_nmi())\n\t\treturn (unsigned long)NULL;\n\n\treturn (unsigned long)____bpf_sk_storage_get(map, sk, value, flags,\n\t\t\t\t\t\t     gfp_flags);\n}\n\nBPF_CALL_2(bpf_sk_storage_delete_tracing, struct bpf_map *, map,\n\t   struct sock *, sk)\n{\n\tWARN_ON_ONCE(!bpf_rcu_lock_held());\n\tif (in_hardirq() || in_nmi())\n\t\treturn -EPERM;\n\n\treturn ____bpf_sk_storage_delete(map, sk);\n}\n\nconst struct bpf_func_proto bpf_sk_storage_get_tracing_proto = {\n\t.func\t\t= bpf_sk_storage_get_tracing,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_PTR_TO_MAP_VALUE_OR_NULL,\n\t.arg1_type\t= ARG_CONST_MAP_PTR,\n\t.arg2_type\t= ARG_PTR_TO_BTF_ID_OR_NULL,\n\t.arg2_btf_id\t= &btf_sock_ids[BTF_SOCK_TYPE_SOCK_COMMON],\n\t.arg3_type\t= ARG_PTR_TO_MAP_VALUE_OR_NULL,\n\t.arg4_type\t= ARG_ANYTHING,\n\t.allowed\t= bpf_sk_storage_tracing_allowed,\n};\n\nconst struct bpf_func_proto bpf_sk_storage_delete_tracing_proto = {\n\t.func\t\t= bpf_sk_storage_delete_tracing,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_CONST_MAP_PTR,\n\t.arg2_type\t= ARG_PTR_TO_BTF_ID_OR_NULL,\n\t.arg2_btf_id\t= &btf_sock_ids[BTF_SOCK_TYPE_SOCK_COMMON],\n\t.allowed\t= bpf_sk_storage_tracing_allowed,\n};\n\nstruct bpf_sk_storage_diag {\n\tu32 nr_maps;\n\tstruct bpf_map *maps[];\n};\n\n \nstatic int nla_value_size(u32 value_size)\n{\n\t \n\treturn nla_total_size(0) + nla_total_size(sizeof(u32)) +\n\t\tnla_total_size_64bit(value_size);\n}\n\nvoid bpf_sk_storage_diag_free(struct bpf_sk_storage_diag *diag)\n{\n\tu32 i;\n\n\tif (!diag)\n\t\treturn;\n\n\tfor (i = 0; i < diag->nr_maps; i++)\n\t\tbpf_map_put(diag->maps[i]);\n\n\tkfree(diag);\n}\nEXPORT_SYMBOL_GPL(bpf_sk_storage_diag_free);\n\nstatic bool diag_check_dup(const struct bpf_sk_storage_diag *diag,\n\t\t\t   const struct bpf_map *map)\n{\n\tu32 i;\n\n\tfor (i = 0; i < diag->nr_maps; i++) {\n\t\tif (diag->maps[i] == map)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstruct bpf_sk_storage_diag *\nbpf_sk_storage_diag_alloc(const struct nlattr *nla_stgs)\n{\n\tstruct bpf_sk_storage_diag *diag;\n\tstruct nlattr *nla;\n\tu32 nr_maps = 0;\n\tint rem, err;\n\n\t \n\tif (!bpf_capable())\n\t\treturn ERR_PTR(-EPERM);\n\n\tnla_for_each_nested(nla, nla_stgs, rem) {\n\t\tif (nla_type(nla) == SK_DIAG_BPF_STORAGE_REQ_MAP_FD) {\n\t\t\tif (nla_len(nla) != sizeof(u32))\n\t\t\t\treturn ERR_PTR(-EINVAL);\n\t\t\tnr_maps++;\n\t\t}\n\t}\n\n\tdiag = kzalloc(struct_size(diag, maps, nr_maps), GFP_KERNEL);\n\tif (!diag)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tnla_for_each_nested(nla, nla_stgs, rem) {\n\t\tstruct bpf_map *map;\n\t\tint map_fd;\n\n\t\tif (nla_type(nla) != SK_DIAG_BPF_STORAGE_REQ_MAP_FD)\n\t\t\tcontinue;\n\n\t\tmap_fd = nla_get_u32(nla);\n\t\tmap = bpf_map_get(map_fd);\n\t\tif (IS_ERR(map)) {\n\t\t\terr = PTR_ERR(map);\n\t\t\tgoto err_free;\n\t\t}\n\t\tif (map->map_type != BPF_MAP_TYPE_SK_STORAGE) {\n\t\t\tbpf_map_put(map);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\t\tif (diag_check_dup(diag, map)) {\n\t\t\tbpf_map_put(map);\n\t\t\terr = -EEXIST;\n\t\t\tgoto err_free;\n\t\t}\n\t\tdiag->maps[diag->nr_maps++] = map;\n\t}\n\n\treturn diag;\n\nerr_free:\n\tbpf_sk_storage_diag_free(diag);\n\treturn ERR_PTR(err);\n}\nEXPORT_SYMBOL_GPL(bpf_sk_storage_diag_alloc);\n\nstatic int diag_get(struct bpf_local_storage_data *sdata, struct sk_buff *skb)\n{\n\tstruct nlattr *nla_stg, *nla_value;\n\tstruct bpf_local_storage_map *smap;\n\n\t \n\tBUILD_BUG_ON(U16_MAX - NLA_HDRLEN < BPF_LOCAL_STORAGE_MAX_VALUE_SIZE);\n\n\tnla_stg = nla_nest_start(skb, SK_DIAG_BPF_STORAGE);\n\tif (!nla_stg)\n\t\treturn -EMSGSIZE;\n\n\tsmap = rcu_dereference(sdata->smap);\n\tif (nla_put_u32(skb, SK_DIAG_BPF_STORAGE_MAP_ID, smap->map.id))\n\t\tgoto errout;\n\n\tnla_value = nla_reserve_64bit(skb, SK_DIAG_BPF_STORAGE_MAP_VALUE,\n\t\t\t\t      smap->map.value_size,\n\t\t\t\t      SK_DIAG_BPF_STORAGE_PAD);\n\tif (!nla_value)\n\t\tgoto errout;\n\n\tif (btf_record_has_field(smap->map.record, BPF_SPIN_LOCK))\n\t\tcopy_map_value_locked(&smap->map, nla_data(nla_value),\n\t\t\t\t      sdata->data, true);\n\telse\n\t\tcopy_map_value(&smap->map, nla_data(nla_value), sdata->data);\n\n\tnla_nest_end(skb, nla_stg);\n\treturn 0;\n\nerrout:\n\tnla_nest_cancel(skb, nla_stg);\n\treturn -EMSGSIZE;\n}\n\nstatic int bpf_sk_storage_diag_put_all(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t       int stg_array_type,\n\t\t\t\t       unsigned int *res_diag_size)\n{\n\t \n\tunsigned int diag_size = nla_total_size(0);\n\tstruct bpf_local_storage *sk_storage;\n\tstruct bpf_local_storage_elem *selem;\n\tstruct bpf_local_storage_map *smap;\n\tstruct nlattr *nla_stgs;\n\tunsigned int saved_len;\n\tint err = 0;\n\n\trcu_read_lock();\n\n\tsk_storage = rcu_dereference(sk->sk_bpf_storage);\n\tif (!sk_storage || hlist_empty(&sk_storage->list)) {\n\t\trcu_read_unlock();\n\t\treturn 0;\n\t}\n\n\tnla_stgs = nla_nest_start(skb, stg_array_type);\n\tif (!nla_stgs)\n\t\t \n\t\terr = -EMSGSIZE;\n\n\tsaved_len = skb->len;\n\thlist_for_each_entry_rcu(selem, &sk_storage->list, snode) {\n\t\tsmap = rcu_dereference(SDATA(selem)->smap);\n\t\tdiag_size += nla_value_size(smap->map.value_size);\n\n\t\tif (nla_stgs && diag_get(SDATA(selem), skb))\n\t\t\t \n\t\t\terr = -EMSGSIZE;\n\t}\n\n\trcu_read_unlock();\n\n\tif (nla_stgs) {\n\t\tif (saved_len == skb->len)\n\t\t\tnla_nest_cancel(skb, nla_stgs);\n\t\telse\n\t\t\tnla_nest_end(skb, nla_stgs);\n\t}\n\n\tif (diag_size == nla_total_size(0)) {\n\t\t*res_diag_size = 0;\n\t\treturn 0;\n\t}\n\n\t*res_diag_size = diag_size;\n\treturn err;\n}\n\nint bpf_sk_storage_diag_put(struct bpf_sk_storage_diag *diag,\n\t\t\t    struct sock *sk, struct sk_buff *skb,\n\t\t\t    int stg_array_type,\n\t\t\t    unsigned int *res_diag_size)\n{\n\t \n\tunsigned int diag_size = nla_total_size(0);\n\tstruct bpf_local_storage *sk_storage;\n\tstruct bpf_local_storage_data *sdata;\n\tstruct nlattr *nla_stgs;\n\tunsigned int saved_len;\n\tint err = 0;\n\tu32 i;\n\n\t*res_diag_size = 0;\n\n\t \n\tif (!diag->nr_maps)\n\t\treturn bpf_sk_storage_diag_put_all(sk, skb, stg_array_type,\n\t\t\t\t\t\t   res_diag_size);\n\n\trcu_read_lock();\n\tsk_storage = rcu_dereference(sk->sk_bpf_storage);\n\tif (!sk_storage || hlist_empty(&sk_storage->list)) {\n\t\trcu_read_unlock();\n\t\treturn 0;\n\t}\n\n\tnla_stgs = nla_nest_start(skb, stg_array_type);\n\tif (!nla_stgs)\n\t\t \n\t\terr = -EMSGSIZE;\n\n\tsaved_len = skb->len;\n\tfor (i = 0; i < diag->nr_maps; i++) {\n\t\tsdata = bpf_local_storage_lookup(sk_storage,\n\t\t\t\t(struct bpf_local_storage_map *)diag->maps[i],\n\t\t\t\tfalse);\n\n\t\tif (!sdata)\n\t\t\tcontinue;\n\n\t\tdiag_size += nla_value_size(diag->maps[i]->value_size);\n\n\t\tif (nla_stgs && diag_get(sdata, skb))\n\t\t\t \n\t\t\terr = -EMSGSIZE;\n\t}\n\trcu_read_unlock();\n\n\tif (nla_stgs) {\n\t\tif (saved_len == skb->len)\n\t\t\tnla_nest_cancel(skb, nla_stgs);\n\t\telse\n\t\t\tnla_nest_end(skb, nla_stgs);\n\t}\n\n\tif (diag_size == nla_total_size(0)) {\n\t\t*res_diag_size = 0;\n\t\treturn 0;\n\t}\n\n\t*res_diag_size = diag_size;\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(bpf_sk_storage_diag_put);\n\nstruct bpf_iter_seq_sk_storage_map_info {\n\tstruct bpf_map *map;\n\tunsigned int bucket_id;\n\tunsigned skip_elems;\n};\n\nstatic struct bpf_local_storage_elem *\nbpf_sk_storage_map_seq_find_next(struct bpf_iter_seq_sk_storage_map_info *info,\n\t\t\t\t struct bpf_local_storage_elem *prev_selem)\n\t__acquires(RCU) __releases(RCU)\n{\n\tstruct bpf_local_storage *sk_storage;\n\tstruct bpf_local_storage_elem *selem;\n\tu32 skip_elems = info->skip_elems;\n\tstruct bpf_local_storage_map *smap;\n\tu32 bucket_id = info->bucket_id;\n\tu32 i, count, n_buckets;\n\tstruct bpf_local_storage_map_bucket *b;\n\n\tsmap = (struct bpf_local_storage_map *)info->map;\n\tn_buckets = 1U << smap->bucket_log;\n\tif (bucket_id >= n_buckets)\n\t\treturn NULL;\n\n\t \n\tselem = prev_selem;\n\tcount = 0;\n\twhile (selem) {\n\t\tselem = hlist_entry_safe(rcu_dereference(hlist_next_rcu(&selem->map_node)),\n\t\t\t\t\t struct bpf_local_storage_elem, map_node);\n\t\tif (!selem) {\n\t\t\t \n\t\t\tb = &smap->buckets[bucket_id++];\n\t\t\trcu_read_unlock();\n\t\t\tskip_elems = 0;\n\t\t\tbreak;\n\t\t}\n\t\tsk_storage = rcu_dereference(selem->local_storage);\n\t\tif (sk_storage) {\n\t\t\tinfo->skip_elems = skip_elems + count;\n\t\t\treturn selem;\n\t\t}\n\t\tcount++;\n\t}\n\n\tfor (i = bucket_id; i < (1U << smap->bucket_log); i++) {\n\t\tb = &smap->buckets[i];\n\t\trcu_read_lock();\n\t\tcount = 0;\n\t\thlist_for_each_entry_rcu(selem, &b->list, map_node) {\n\t\t\tsk_storage = rcu_dereference(selem->local_storage);\n\t\t\tif (sk_storage && count >= skip_elems) {\n\t\t\t\tinfo->bucket_id = i;\n\t\t\t\tinfo->skip_elems = count;\n\t\t\t\treturn selem;\n\t\t\t}\n\t\t\tcount++;\n\t\t}\n\t\trcu_read_unlock();\n\t\tskip_elems = 0;\n\t}\n\n\tinfo->bucket_id = i;\n\tinfo->skip_elems = 0;\n\treturn NULL;\n}\n\nstatic void *bpf_sk_storage_map_seq_start(struct seq_file *seq, loff_t *pos)\n{\n\tstruct bpf_local_storage_elem *selem;\n\n\tselem = bpf_sk_storage_map_seq_find_next(seq->private, NULL);\n\tif (!selem)\n\t\treturn NULL;\n\n\tif (*pos == 0)\n\t\t++*pos;\n\treturn selem;\n}\n\nstatic void *bpf_sk_storage_map_seq_next(struct seq_file *seq, void *v,\n\t\t\t\t\t loff_t *pos)\n{\n\tstruct bpf_iter_seq_sk_storage_map_info *info = seq->private;\n\n\t++*pos;\n\t++info->skip_elems;\n\treturn bpf_sk_storage_map_seq_find_next(seq->private, v);\n}\n\nstruct bpf_iter__bpf_sk_storage_map {\n\t__bpf_md_ptr(struct bpf_iter_meta *, meta);\n\t__bpf_md_ptr(struct bpf_map *, map);\n\t__bpf_md_ptr(struct sock *, sk);\n\t__bpf_md_ptr(void *, value);\n};\n\nDEFINE_BPF_ITER_FUNC(bpf_sk_storage_map, struct bpf_iter_meta *meta,\n\t\t     struct bpf_map *map, struct sock *sk,\n\t\t     void *value)\n\nstatic int __bpf_sk_storage_map_seq_show(struct seq_file *seq,\n\t\t\t\t\t struct bpf_local_storage_elem *selem)\n{\n\tstruct bpf_iter_seq_sk_storage_map_info *info = seq->private;\n\tstruct bpf_iter__bpf_sk_storage_map ctx = {};\n\tstruct bpf_local_storage *sk_storage;\n\tstruct bpf_iter_meta meta;\n\tstruct bpf_prog *prog;\n\tint ret = 0;\n\n\tmeta.seq = seq;\n\tprog = bpf_iter_get_info(&meta, selem == NULL);\n\tif (prog) {\n\t\tctx.meta = &meta;\n\t\tctx.map = info->map;\n\t\tif (selem) {\n\t\t\tsk_storage = rcu_dereference(selem->local_storage);\n\t\t\tctx.sk = sk_storage->owner;\n\t\t\tctx.value = SDATA(selem)->data;\n\t\t}\n\t\tret = bpf_iter_run_prog(prog, &ctx);\n\t}\n\n\treturn ret;\n}\n\nstatic int bpf_sk_storage_map_seq_show(struct seq_file *seq, void *v)\n{\n\treturn __bpf_sk_storage_map_seq_show(seq, v);\n}\n\nstatic void bpf_sk_storage_map_seq_stop(struct seq_file *seq, void *v)\n\t__releases(RCU)\n{\n\tif (!v)\n\t\t(void)__bpf_sk_storage_map_seq_show(seq, v);\n\telse\n\t\trcu_read_unlock();\n}\n\nstatic int bpf_iter_init_sk_storage_map(void *priv_data,\n\t\t\t\t\tstruct bpf_iter_aux_info *aux)\n{\n\tstruct bpf_iter_seq_sk_storage_map_info *seq_info = priv_data;\n\n\tbpf_map_inc_with_uref(aux->map);\n\tseq_info->map = aux->map;\n\treturn 0;\n}\n\nstatic void bpf_iter_fini_sk_storage_map(void *priv_data)\n{\n\tstruct bpf_iter_seq_sk_storage_map_info *seq_info = priv_data;\n\n\tbpf_map_put_with_uref(seq_info->map);\n}\n\nstatic int bpf_iter_attach_map(struct bpf_prog *prog,\n\t\t\t       union bpf_iter_link_info *linfo,\n\t\t\t       struct bpf_iter_aux_info *aux)\n{\n\tstruct bpf_map *map;\n\tint err = -EINVAL;\n\n\tif (!linfo->map.map_fd)\n\t\treturn -EBADF;\n\n\tmap = bpf_map_get_with_uref(linfo->map.map_fd);\n\tif (IS_ERR(map))\n\t\treturn PTR_ERR(map);\n\n\tif (map->map_type != BPF_MAP_TYPE_SK_STORAGE)\n\t\tgoto put_map;\n\n\tif (prog->aux->max_rdwr_access > map->value_size) {\n\t\terr = -EACCES;\n\t\tgoto put_map;\n\t}\n\n\taux->map = map;\n\treturn 0;\n\nput_map:\n\tbpf_map_put_with_uref(map);\n\treturn err;\n}\n\nstatic void bpf_iter_detach_map(struct bpf_iter_aux_info *aux)\n{\n\tbpf_map_put_with_uref(aux->map);\n}\n\nstatic const struct seq_operations bpf_sk_storage_map_seq_ops = {\n\t.start  = bpf_sk_storage_map_seq_start,\n\t.next   = bpf_sk_storage_map_seq_next,\n\t.stop   = bpf_sk_storage_map_seq_stop,\n\t.show   = bpf_sk_storage_map_seq_show,\n};\n\nstatic const struct bpf_iter_seq_info iter_seq_info = {\n\t.seq_ops\t\t= &bpf_sk_storage_map_seq_ops,\n\t.init_seq_private\t= bpf_iter_init_sk_storage_map,\n\t.fini_seq_private\t= bpf_iter_fini_sk_storage_map,\n\t.seq_priv_size\t\t= sizeof(struct bpf_iter_seq_sk_storage_map_info),\n};\n\nstatic struct bpf_iter_reg bpf_sk_storage_map_reg_info = {\n\t.target\t\t\t= \"bpf_sk_storage_map\",\n\t.attach_target\t\t= bpf_iter_attach_map,\n\t.detach_target\t\t= bpf_iter_detach_map,\n\t.show_fdinfo\t\t= bpf_iter_map_show_fdinfo,\n\t.fill_link_info\t\t= bpf_iter_map_fill_link_info,\n\t.ctx_arg_info_size\t= 2,\n\t.ctx_arg_info\t\t= {\n\t\t{ offsetof(struct bpf_iter__bpf_sk_storage_map, sk),\n\t\t  PTR_TO_BTF_ID_OR_NULL },\n\t\t{ offsetof(struct bpf_iter__bpf_sk_storage_map, value),\n\t\t  PTR_TO_BUF | PTR_MAYBE_NULL },\n\t},\n\t.seq_info\t\t= &iter_seq_info,\n};\n\nstatic int __init bpf_sk_storage_map_iter_init(void)\n{\n\tbpf_sk_storage_map_reg_info.ctx_arg_info[0].btf_id =\n\t\tbtf_sock_ids[BTF_SOCK_TYPE_SOCK];\n\treturn bpf_iter_reg_target(&bpf_sk_storage_map_reg_info);\n}\nlate_initcall(bpf_sk_storage_map_iter_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}