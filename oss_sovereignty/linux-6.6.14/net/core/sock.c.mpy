{
  "module_name": "sock.c",
  "hash_id": "62c4c20dcfba4d00a0b378150838f36543502ac4a8e3f3b7d6377a9a409401a8",
  "original_prompt": "Ingested from linux-6.6.14/net/core/sock.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <asm/unaligned.h>\n#include <linux/capability.h>\n#include <linux/errno.h>\n#include <linux/errqueue.h>\n#include <linux/types.h>\n#include <linux/socket.h>\n#include <linux/in.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <linux/sched.h>\n#include <linux/sched/mm.h>\n#include <linux/timer.h>\n#include <linux/string.h>\n#include <linux/sockios.h>\n#include <linux/net.h>\n#include <linux/mm.h>\n#include <linux/slab.h>\n#include <linux/interrupt.h>\n#include <linux/poll.h>\n#include <linux/tcp.h>\n#include <linux/init.h>\n#include <linux/highmem.h>\n#include <linux/user_namespace.h>\n#include <linux/static_key.h>\n#include <linux/memcontrol.h>\n#include <linux/prefetch.h>\n#include <linux/compat.h>\n#include <linux/mroute.h>\n#include <linux/mroute6.h>\n#include <linux/icmpv6.h>\n\n#include <linux/uaccess.h>\n\n#include <linux/netdevice.h>\n#include <net/protocol.h>\n#include <linux/skbuff.h>\n#include <net/net_namespace.h>\n#include <net/request_sock.h>\n#include <net/sock.h>\n#include <linux/net_tstamp.h>\n#include <net/xfrm.h>\n#include <linux/ipsec.h>\n#include <net/cls_cgroup.h>\n#include <net/netprio_cgroup.h>\n#include <linux/sock_diag.h>\n\n#include <linux/filter.h>\n#include <net/sock_reuseport.h>\n#include <net/bpf_sk_storage.h>\n\n#include <trace/events/sock.h>\n\n#include <net/tcp.h>\n#include <net/busy_poll.h>\n#include <net/phonet/phonet.h>\n\n#include <linux/ethtool.h>\n\n#include \"dev.h\"\n\nstatic DEFINE_MUTEX(proto_list_mutex);\nstatic LIST_HEAD(proto_list);\n\nstatic void sock_def_write_space_wfree(struct sock *sk);\nstatic void sock_def_write_space(struct sock *sk);\n\n \nbool sk_ns_capable(const struct sock *sk,\n\t\t   struct user_namespace *user_ns, int cap)\n{\n\treturn file_ns_capable(sk->sk_socket->file, user_ns, cap) &&\n\t\tns_capable(user_ns, cap);\n}\nEXPORT_SYMBOL(sk_ns_capable);\n\n \nbool sk_capable(const struct sock *sk, int cap)\n{\n\treturn sk_ns_capable(sk, &init_user_ns, cap);\n}\nEXPORT_SYMBOL(sk_capable);\n\n \nbool sk_net_capable(const struct sock *sk, int cap)\n{\n\treturn sk_ns_capable(sk, sock_net(sk)->user_ns, cap);\n}\nEXPORT_SYMBOL(sk_net_capable);\n\n \nstatic struct lock_class_key af_family_keys[AF_MAX];\nstatic struct lock_class_key af_family_kern_keys[AF_MAX];\nstatic struct lock_class_key af_family_slock_keys[AF_MAX];\nstatic struct lock_class_key af_family_kern_slock_keys[AF_MAX];\n\n \n\n#define _sock_locks(x)\t\t\t\t\t\t  \\\n  x \"AF_UNSPEC\",\tx \"AF_UNIX\"     ,\tx \"AF_INET\"     , \\\n  x \"AF_AX25\"  ,\tx \"AF_IPX\"      ,\tx \"AF_APPLETALK\", \\\n  x \"AF_NETROM\",\tx \"AF_BRIDGE\"   ,\tx \"AF_ATMPVC\"   , \\\n  x \"AF_X25\"   ,\tx \"AF_INET6\"    ,\tx \"AF_ROSE\"     , \\\n  x \"AF_DECnet\",\tx \"AF_NETBEUI\"  ,\tx \"AF_SECURITY\" , \\\n  x \"AF_KEY\"   ,\tx \"AF_NETLINK\"  ,\tx \"AF_PACKET\"   , \\\n  x \"AF_ASH\"   ,\tx \"AF_ECONET\"   ,\tx \"AF_ATMSVC\"   , \\\n  x \"AF_RDS\"   ,\tx \"AF_SNA\"      ,\tx \"AF_IRDA\"     , \\\n  x \"AF_PPPOX\" ,\tx \"AF_WANPIPE\"  ,\tx \"AF_LLC\"      , \\\n  x \"27\"       ,\tx \"28\"          ,\tx \"AF_CAN\"      , \\\n  x \"AF_TIPC\"  ,\tx \"AF_BLUETOOTH\",\tx \"IUCV\"        , \\\n  x \"AF_RXRPC\" ,\tx \"AF_ISDN\"     ,\tx \"AF_PHONET\"   , \\\n  x \"AF_IEEE802154\",\tx \"AF_CAIF\"\t,\tx \"AF_ALG\"      , \\\n  x \"AF_NFC\"   ,\tx \"AF_VSOCK\"    ,\tx \"AF_KCM\"      , \\\n  x \"AF_QIPCRTR\",\tx \"AF_SMC\"\t,\tx \"AF_XDP\"\t, \\\n  x \"AF_MCTP\"  , \\\n  x \"AF_MAX\"\n\nstatic const char *const af_family_key_strings[AF_MAX+1] = {\n\t_sock_locks(\"sk_lock-\")\n};\nstatic const char *const af_family_slock_key_strings[AF_MAX+1] = {\n\t_sock_locks(\"slock-\")\n};\nstatic const char *const af_family_clock_key_strings[AF_MAX+1] = {\n\t_sock_locks(\"clock-\")\n};\n\nstatic const char *const af_family_kern_key_strings[AF_MAX+1] = {\n\t_sock_locks(\"k-sk_lock-\")\n};\nstatic const char *const af_family_kern_slock_key_strings[AF_MAX+1] = {\n\t_sock_locks(\"k-slock-\")\n};\nstatic const char *const af_family_kern_clock_key_strings[AF_MAX+1] = {\n\t_sock_locks(\"k-clock-\")\n};\nstatic const char *const af_family_rlock_key_strings[AF_MAX+1] = {\n\t_sock_locks(\"rlock-\")\n};\nstatic const char *const af_family_wlock_key_strings[AF_MAX+1] = {\n\t_sock_locks(\"wlock-\")\n};\nstatic const char *const af_family_elock_key_strings[AF_MAX+1] = {\n\t_sock_locks(\"elock-\")\n};\n\n \nstatic struct lock_class_key af_callback_keys[AF_MAX];\nstatic struct lock_class_key af_rlock_keys[AF_MAX];\nstatic struct lock_class_key af_wlock_keys[AF_MAX];\nstatic struct lock_class_key af_elock_keys[AF_MAX];\nstatic struct lock_class_key af_kern_callback_keys[AF_MAX];\n\n \n__u32 sysctl_wmem_max __read_mostly = SK_WMEM_MAX;\nEXPORT_SYMBOL(sysctl_wmem_max);\n__u32 sysctl_rmem_max __read_mostly = SK_RMEM_MAX;\nEXPORT_SYMBOL(sysctl_rmem_max);\n__u32 sysctl_wmem_default __read_mostly = SK_WMEM_MAX;\n__u32 sysctl_rmem_default __read_mostly = SK_RMEM_MAX;\n\n \nint sysctl_optmem_max __read_mostly = sizeof(unsigned long)*(2*UIO_MAXIOV+512);\nEXPORT_SYMBOL(sysctl_optmem_max);\n\nint sysctl_tstamp_allow_data __read_mostly = 1;\n\nDEFINE_STATIC_KEY_FALSE(memalloc_socks_key);\nEXPORT_SYMBOL_GPL(memalloc_socks_key);\n\n \nvoid sk_set_memalloc(struct sock *sk)\n{\n\tsock_set_flag(sk, SOCK_MEMALLOC);\n\tsk->sk_allocation |= __GFP_MEMALLOC;\n\tstatic_branch_inc(&memalloc_socks_key);\n}\nEXPORT_SYMBOL_GPL(sk_set_memalloc);\n\nvoid sk_clear_memalloc(struct sock *sk)\n{\n\tsock_reset_flag(sk, SOCK_MEMALLOC);\n\tsk->sk_allocation &= ~__GFP_MEMALLOC;\n\tstatic_branch_dec(&memalloc_socks_key);\n\n\t \n\tsk_mem_reclaim(sk);\n}\nEXPORT_SYMBOL_GPL(sk_clear_memalloc);\n\nint __sk_backlog_rcv(struct sock *sk, struct sk_buff *skb)\n{\n\tint ret;\n\tunsigned int noreclaim_flag;\n\n\t \n\tBUG_ON(!sock_flag(sk, SOCK_MEMALLOC));\n\n\tnoreclaim_flag = memalloc_noreclaim_save();\n\tret = INDIRECT_CALL_INET(sk->sk_backlog_rcv,\n\t\t\t\t tcp_v6_do_rcv,\n\t\t\t\t tcp_v4_do_rcv,\n\t\t\t\t sk, skb);\n\tmemalloc_noreclaim_restore(noreclaim_flag);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(__sk_backlog_rcv);\n\nvoid sk_error_report(struct sock *sk)\n{\n\tsk->sk_error_report(sk);\n\n\tswitch (sk->sk_family) {\n\tcase AF_INET:\n\t\tfallthrough;\n\tcase AF_INET6:\n\t\ttrace_inet_sk_error_report(sk);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\nEXPORT_SYMBOL(sk_error_report);\n\nint sock_get_timeout(long timeo, void *optval, bool old_timeval)\n{\n\tstruct __kernel_sock_timeval tv;\n\n\tif (timeo == MAX_SCHEDULE_TIMEOUT) {\n\t\ttv.tv_sec = 0;\n\t\ttv.tv_usec = 0;\n\t} else {\n\t\ttv.tv_sec = timeo / HZ;\n\t\ttv.tv_usec = ((timeo % HZ) * USEC_PER_SEC) / HZ;\n\t}\n\n\tif (old_timeval && in_compat_syscall() && !COMPAT_USE_64BIT_TIME) {\n\t\tstruct old_timeval32 tv32 = { tv.tv_sec, tv.tv_usec };\n\t\t*(struct old_timeval32 *)optval = tv32;\n\t\treturn sizeof(tv32);\n\t}\n\n\tif (old_timeval) {\n\t\tstruct __kernel_old_timeval old_tv;\n\t\told_tv.tv_sec = tv.tv_sec;\n\t\told_tv.tv_usec = tv.tv_usec;\n\t\t*(struct __kernel_old_timeval *)optval = old_tv;\n\t\treturn sizeof(old_tv);\n\t}\n\n\t*(struct __kernel_sock_timeval *)optval = tv;\n\treturn sizeof(tv);\n}\nEXPORT_SYMBOL(sock_get_timeout);\n\nint sock_copy_user_timeval(struct __kernel_sock_timeval *tv,\n\t\t\t   sockptr_t optval, int optlen, bool old_timeval)\n{\n\tif (old_timeval && in_compat_syscall() && !COMPAT_USE_64BIT_TIME) {\n\t\tstruct old_timeval32 tv32;\n\n\t\tif (optlen < sizeof(tv32))\n\t\t\treturn -EINVAL;\n\n\t\tif (copy_from_sockptr(&tv32, optval, sizeof(tv32)))\n\t\t\treturn -EFAULT;\n\t\ttv->tv_sec = tv32.tv_sec;\n\t\ttv->tv_usec = tv32.tv_usec;\n\t} else if (old_timeval) {\n\t\tstruct __kernel_old_timeval old_tv;\n\n\t\tif (optlen < sizeof(old_tv))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_sockptr(&old_tv, optval, sizeof(old_tv)))\n\t\t\treturn -EFAULT;\n\t\ttv->tv_sec = old_tv.tv_sec;\n\t\ttv->tv_usec = old_tv.tv_usec;\n\t} else {\n\t\tif (optlen < sizeof(*tv))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_sockptr(tv, optval, sizeof(*tv)))\n\t\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(sock_copy_user_timeval);\n\nstatic int sock_set_timeout(long *timeo_p, sockptr_t optval, int optlen,\n\t\t\t    bool old_timeval)\n{\n\tstruct __kernel_sock_timeval tv;\n\tint err = sock_copy_user_timeval(&tv, optval, optlen, old_timeval);\n\tlong val;\n\n\tif (err)\n\t\treturn err;\n\n\tif (tv.tv_usec < 0 || tv.tv_usec >= USEC_PER_SEC)\n\t\treturn -EDOM;\n\n\tif (tv.tv_sec < 0) {\n\t\tstatic int warned __read_mostly;\n\n\t\tWRITE_ONCE(*timeo_p, 0);\n\t\tif (warned < 10 && net_ratelimit()) {\n\t\t\twarned++;\n\t\t\tpr_info(\"%s: `%s' (pid %d) tries to set negative timeout\\n\",\n\t\t\t\t__func__, current->comm, task_pid_nr(current));\n\t\t}\n\t\treturn 0;\n\t}\n\tval = MAX_SCHEDULE_TIMEOUT;\n\tif ((tv.tv_sec || tv.tv_usec) &&\n\t    (tv.tv_sec < (MAX_SCHEDULE_TIMEOUT / HZ - 1)))\n\t\tval = tv.tv_sec * HZ + DIV_ROUND_UP((unsigned long)tv.tv_usec,\n\t\t\t\t\t\t    USEC_PER_SEC / HZ);\n\tWRITE_ONCE(*timeo_p, val);\n\treturn 0;\n}\n\nstatic bool sock_needs_netstamp(const struct sock *sk)\n{\n\tswitch (sk->sk_family) {\n\tcase AF_UNSPEC:\n\tcase AF_UNIX:\n\t\treturn false;\n\tdefault:\n\t\treturn true;\n\t}\n}\n\nstatic void sock_disable_timestamp(struct sock *sk, unsigned long flags)\n{\n\tif (sk->sk_flags & flags) {\n\t\tsk->sk_flags &= ~flags;\n\t\tif (sock_needs_netstamp(sk) &&\n\t\t    !(sk->sk_flags & SK_FLAGS_TIMESTAMP))\n\t\t\tnet_disable_timestamp();\n\t}\n}\n\n\nint __sock_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tunsigned long flags;\n\tstruct sk_buff_head *list = &sk->sk_receive_queue;\n\n\tif (atomic_read(&sk->sk_rmem_alloc) >= sk->sk_rcvbuf) {\n\t\tatomic_inc(&sk->sk_drops);\n\t\ttrace_sock_rcvqueue_full(sk, skb);\n\t\treturn -ENOMEM;\n\t}\n\n\tif (!sk_rmem_schedule(sk, skb, skb->truesize)) {\n\t\tatomic_inc(&sk->sk_drops);\n\t\treturn -ENOBUFS;\n\t}\n\n\tskb->dev = NULL;\n\tskb_set_owner_r(skb, sk);\n\n\t \n\tskb_dst_force(skb);\n\n\tspin_lock_irqsave(&list->lock, flags);\n\tsock_skb_set_dropcount(sk, skb);\n\t__skb_queue_tail(list, skb);\n\tspin_unlock_irqrestore(&list->lock, flags);\n\n\tif (!sock_flag(sk, SOCK_DEAD))\n\t\tsk->sk_data_ready(sk);\n\treturn 0;\n}\nEXPORT_SYMBOL(__sock_queue_rcv_skb);\n\nint sock_queue_rcv_skb_reason(struct sock *sk, struct sk_buff *skb,\n\t\t\t      enum skb_drop_reason *reason)\n{\n\tenum skb_drop_reason drop_reason;\n\tint err;\n\n\terr = sk_filter(sk, skb);\n\tif (err) {\n\t\tdrop_reason = SKB_DROP_REASON_SOCKET_FILTER;\n\t\tgoto out;\n\t}\n\terr = __sock_queue_rcv_skb(sk, skb);\n\tswitch (err) {\n\tcase -ENOMEM:\n\t\tdrop_reason = SKB_DROP_REASON_SOCKET_RCVBUFF;\n\t\tbreak;\n\tcase -ENOBUFS:\n\t\tdrop_reason = SKB_DROP_REASON_PROTO_MEM;\n\t\tbreak;\n\tdefault:\n\t\tdrop_reason = SKB_NOT_DROPPED_YET;\n\t\tbreak;\n\t}\nout:\n\tif (reason)\n\t\t*reason = drop_reason;\n\treturn err;\n}\nEXPORT_SYMBOL(sock_queue_rcv_skb_reason);\n\nint __sk_receive_skb(struct sock *sk, struct sk_buff *skb,\n\t\t     const int nested, unsigned int trim_cap, bool refcounted)\n{\n\tint rc = NET_RX_SUCCESS;\n\n\tif (sk_filter_trim_cap(sk, skb, trim_cap))\n\t\tgoto discard_and_relse;\n\n\tskb->dev = NULL;\n\n\tif (sk_rcvqueues_full(sk, sk->sk_rcvbuf)) {\n\t\tatomic_inc(&sk->sk_drops);\n\t\tgoto discard_and_relse;\n\t}\n\tif (nested)\n\t\tbh_lock_sock_nested(sk);\n\telse\n\t\tbh_lock_sock(sk);\n\tif (!sock_owned_by_user(sk)) {\n\t\t \n\t\tmutex_acquire(&sk->sk_lock.dep_map, 0, 1, _RET_IP_);\n\n\t\trc = sk_backlog_rcv(sk, skb);\n\n\t\tmutex_release(&sk->sk_lock.dep_map, _RET_IP_);\n\t} else if (sk_add_backlog(sk, skb, READ_ONCE(sk->sk_rcvbuf))) {\n\t\tbh_unlock_sock(sk);\n\t\tatomic_inc(&sk->sk_drops);\n\t\tgoto discard_and_relse;\n\t}\n\n\tbh_unlock_sock(sk);\nout:\n\tif (refcounted)\n\t\tsock_put(sk);\n\treturn rc;\ndiscard_and_relse:\n\tkfree_skb(skb);\n\tgoto out;\n}\nEXPORT_SYMBOL(__sk_receive_skb);\n\nINDIRECT_CALLABLE_DECLARE(struct dst_entry *ip6_dst_check(struct dst_entry *,\n\t\t\t\t\t\t\t  u32));\nINDIRECT_CALLABLE_DECLARE(struct dst_entry *ipv4_dst_check(struct dst_entry *,\n\t\t\t\t\t\t\t   u32));\nstruct dst_entry *__sk_dst_check(struct sock *sk, u32 cookie)\n{\n\tstruct dst_entry *dst = __sk_dst_get(sk);\n\n\tif (dst && dst->obsolete &&\n\t    INDIRECT_CALL_INET(dst->ops->check, ip6_dst_check, ipv4_dst_check,\n\t\t\t       dst, cookie) == NULL) {\n\t\tsk_tx_queue_clear(sk);\n\t\tWRITE_ONCE(sk->sk_dst_pending_confirm, 0);\n\t\tRCU_INIT_POINTER(sk->sk_dst_cache, NULL);\n\t\tdst_release(dst);\n\t\treturn NULL;\n\t}\n\n\treturn dst;\n}\nEXPORT_SYMBOL(__sk_dst_check);\n\nstruct dst_entry *sk_dst_check(struct sock *sk, u32 cookie)\n{\n\tstruct dst_entry *dst = sk_dst_get(sk);\n\n\tif (dst && dst->obsolete &&\n\t    INDIRECT_CALL_INET(dst->ops->check, ip6_dst_check, ipv4_dst_check,\n\t\t\t       dst, cookie) == NULL) {\n\t\tsk_dst_reset(sk);\n\t\tdst_release(dst);\n\t\treturn NULL;\n\t}\n\n\treturn dst;\n}\nEXPORT_SYMBOL(sk_dst_check);\n\nstatic int sock_bindtoindex_locked(struct sock *sk, int ifindex)\n{\n\tint ret = -ENOPROTOOPT;\n#ifdef CONFIG_NETDEVICES\n\tstruct net *net = sock_net(sk);\n\n\t \n\tret = -EPERM;\n\tif (sk->sk_bound_dev_if && !ns_capable(net->user_ns, CAP_NET_RAW))\n\t\tgoto out;\n\n\tret = -EINVAL;\n\tif (ifindex < 0)\n\t\tgoto out;\n\n\t \n\tWRITE_ONCE(sk->sk_bound_dev_if, ifindex);\n\n\tif (sk->sk_prot->rehash)\n\t\tsk->sk_prot->rehash(sk);\n\tsk_dst_reset(sk);\n\n\tret = 0;\n\nout:\n#endif\n\n\treturn ret;\n}\n\nint sock_bindtoindex(struct sock *sk, int ifindex, bool lock_sk)\n{\n\tint ret;\n\n\tif (lock_sk)\n\t\tlock_sock(sk);\n\tret = sock_bindtoindex_locked(sk, ifindex);\n\tif (lock_sk)\n\t\trelease_sock(sk);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(sock_bindtoindex);\n\nstatic int sock_setbindtodevice(struct sock *sk, sockptr_t optval, int optlen)\n{\n\tint ret = -ENOPROTOOPT;\n#ifdef CONFIG_NETDEVICES\n\tstruct net *net = sock_net(sk);\n\tchar devname[IFNAMSIZ];\n\tint index;\n\n\tret = -EINVAL;\n\tif (optlen < 0)\n\t\tgoto out;\n\n\t \n\tif (optlen > IFNAMSIZ - 1)\n\t\toptlen = IFNAMSIZ - 1;\n\tmemset(devname, 0, sizeof(devname));\n\n\tret = -EFAULT;\n\tif (copy_from_sockptr(devname, optval, optlen))\n\t\tgoto out;\n\n\tindex = 0;\n\tif (devname[0] != '\\0') {\n\t\tstruct net_device *dev;\n\n\t\trcu_read_lock();\n\t\tdev = dev_get_by_name_rcu(net, devname);\n\t\tif (dev)\n\t\t\tindex = dev->ifindex;\n\t\trcu_read_unlock();\n\t\tret = -ENODEV;\n\t\tif (!dev)\n\t\t\tgoto out;\n\t}\n\n\tsockopt_lock_sock(sk);\n\tret = sock_bindtoindex_locked(sk, index);\n\tsockopt_release_sock(sk);\nout:\n#endif\n\n\treturn ret;\n}\n\nstatic int sock_getbindtodevice(struct sock *sk, sockptr_t optval,\n\t\t\t\tsockptr_t optlen, int len)\n{\n\tint ret = -ENOPROTOOPT;\n#ifdef CONFIG_NETDEVICES\n\tint bound_dev_if = READ_ONCE(sk->sk_bound_dev_if);\n\tstruct net *net = sock_net(sk);\n\tchar devname[IFNAMSIZ];\n\n\tif (bound_dev_if == 0) {\n\t\tlen = 0;\n\t\tgoto zero;\n\t}\n\n\tret = -EINVAL;\n\tif (len < IFNAMSIZ)\n\t\tgoto out;\n\n\tret = netdev_get_name(net, devname, bound_dev_if);\n\tif (ret)\n\t\tgoto out;\n\n\tlen = strlen(devname) + 1;\n\n\tret = -EFAULT;\n\tif (copy_to_sockptr(optval, devname, len))\n\t\tgoto out;\n\nzero:\n\tret = -EFAULT;\n\tif (copy_to_sockptr(optlen, &len, sizeof(int)))\n\t\tgoto out;\n\n\tret = 0;\n\nout:\n#endif\n\n\treturn ret;\n}\n\nbool sk_mc_loop(struct sock *sk)\n{\n\tif (dev_recursion_level())\n\t\treturn false;\n\tif (!sk)\n\t\treturn true;\n\t \n\tswitch (READ_ONCE(sk->sk_family)) {\n\tcase AF_INET:\n\t\treturn inet_test_bit(MC_LOOP, sk);\n#if IS_ENABLED(CONFIG_IPV6)\n\tcase AF_INET6:\n\t\treturn inet6_sk(sk)->mc_loop;\n#endif\n\t}\n\tWARN_ON_ONCE(1);\n\treturn true;\n}\nEXPORT_SYMBOL(sk_mc_loop);\n\nvoid sock_set_reuseaddr(struct sock *sk)\n{\n\tlock_sock(sk);\n\tsk->sk_reuse = SK_CAN_REUSE;\n\trelease_sock(sk);\n}\nEXPORT_SYMBOL(sock_set_reuseaddr);\n\nvoid sock_set_reuseport(struct sock *sk)\n{\n\tlock_sock(sk);\n\tsk->sk_reuseport = true;\n\trelease_sock(sk);\n}\nEXPORT_SYMBOL(sock_set_reuseport);\n\nvoid sock_no_linger(struct sock *sk)\n{\n\tlock_sock(sk);\n\tWRITE_ONCE(sk->sk_lingertime, 0);\n\tsock_set_flag(sk, SOCK_LINGER);\n\trelease_sock(sk);\n}\nEXPORT_SYMBOL(sock_no_linger);\n\nvoid sock_set_priority(struct sock *sk, u32 priority)\n{\n\tlock_sock(sk);\n\tWRITE_ONCE(sk->sk_priority, priority);\n\trelease_sock(sk);\n}\nEXPORT_SYMBOL(sock_set_priority);\n\nvoid sock_set_sndtimeo(struct sock *sk, s64 secs)\n{\n\tlock_sock(sk);\n\tif (secs && secs < MAX_SCHEDULE_TIMEOUT / HZ - 1)\n\t\tWRITE_ONCE(sk->sk_sndtimeo, secs * HZ);\n\telse\n\t\tWRITE_ONCE(sk->sk_sndtimeo, MAX_SCHEDULE_TIMEOUT);\n\trelease_sock(sk);\n}\nEXPORT_SYMBOL(sock_set_sndtimeo);\n\nstatic void __sock_set_timestamps(struct sock *sk, bool val, bool new, bool ns)\n{\n\tif (val)  {\n\t\tsock_valbool_flag(sk, SOCK_TSTAMP_NEW, new);\n\t\tsock_valbool_flag(sk, SOCK_RCVTSTAMPNS, ns);\n\t\tsock_set_flag(sk, SOCK_RCVTSTAMP);\n\t\tsock_enable_timestamp(sk, SOCK_TIMESTAMP);\n\t} else {\n\t\tsock_reset_flag(sk, SOCK_RCVTSTAMP);\n\t\tsock_reset_flag(sk, SOCK_RCVTSTAMPNS);\n\t}\n}\n\nvoid sock_enable_timestamps(struct sock *sk)\n{\n\tlock_sock(sk);\n\t__sock_set_timestamps(sk, true, false, true);\n\trelease_sock(sk);\n}\nEXPORT_SYMBOL(sock_enable_timestamps);\n\nvoid sock_set_timestamp(struct sock *sk, int optname, bool valbool)\n{\n\tswitch (optname) {\n\tcase SO_TIMESTAMP_OLD:\n\t\t__sock_set_timestamps(sk, valbool, false, false);\n\t\tbreak;\n\tcase SO_TIMESTAMP_NEW:\n\t\t__sock_set_timestamps(sk, valbool, true, false);\n\t\tbreak;\n\tcase SO_TIMESTAMPNS_OLD:\n\t\t__sock_set_timestamps(sk, valbool, false, true);\n\t\tbreak;\n\tcase SO_TIMESTAMPNS_NEW:\n\t\t__sock_set_timestamps(sk, valbool, true, true);\n\t\tbreak;\n\t}\n}\n\nstatic int sock_timestamping_bind_phc(struct sock *sk, int phc_index)\n{\n\tstruct net *net = sock_net(sk);\n\tstruct net_device *dev = NULL;\n\tbool match = false;\n\tint *vclock_index;\n\tint i, num;\n\n\tif (sk->sk_bound_dev_if)\n\t\tdev = dev_get_by_index(net, sk->sk_bound_dev_if);\n\n\tif (!dev) {\n\t\tpr_err(\"%s: sock not bind to device\\n\", __func__);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tnum = ethtool_get_phc_vclocks(dev, &vclock_index);\n\tdev_put(dev);\n\n\tfor (i = 0; i < num; i++) {\n\t\tif (*(vclock_index + i) == phc_index) {\n\t\t\tmatch = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (num > 0)\n\t\tkfree(vclock_index);\n\n\tif (!match)\n\t\treturn -EINVAL;\n\n\tWRITE_ONCE(sk->sk_bind_phc, phc_index);\n\n\treturn 0;\n}\n\nint sock_set_timestamping(struct sock *sk, int optname,\n\t\t\t  struct so_timestamping timestamping)\n{\n\tint val = timestamping.flags;\n\tint ret;\n\n\tif (val & ~SOF_TIMESTAMPING_MASK)\n\t\treturn -EINVAL;\n\n\tif (val & SOF_TIMESTAMPING_OPT_ID_TCP &&\n\t    !(val & SOF_TIMESTAMPING_OPT_ID))\n\t\treturn -EINVAL;\n\n\tif (val & SOF_TIMESTAMPING_OPT_ID &&\n\t    !(sk->sk_tsflags & SOF_TIMESTAMPING_OPT_ID)) {\n\t\tif (sk_is_tcp(sk)) {\n\t\t\tif ((1 << sk->sk_state) &\n\t\t\t    (TCPF_CLOSE | TCPF_LISTEN))\n\t\t\t\treturn -EINVAL;\n\t\t\tif (val & SOF_TIMESTAMPING_OPT_ID_TCP)\n\t\t\t\tatomic_set(&sk->sk_tskey, tcp_sk(sk)->write_seq);\n\t\t\telse\n\t\t\t\tatomic_set(&sk->sk_tskey, tcp_sk(sk)->snd_una);\n\t\t} else {\n\t\t\tatomic_set(&sk->sk_tskey, 0);\n\t\t}\n\t}\n\n\tif (val & SOF_TIMESTAMPING_OPT_STATS &&\n\t    !(val & SOF_TIMESTAMPING_OPT_TSONLY))\n\t\treturn -EINVAL;\n\n\tif (val & SOF_TIMESTAMPING_BIND_PHC) {\n\t\tret = sock_timestamping_bind_phc(sk, timestamping.bind_phc);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tWRITE_ONCE(sk->sk_tsflags, val);\n\tsock_valbool_flag(sk, SOCK_TSTAMP_NEW, optname == SO_TIMESTAMPING_NEW);\n\n\tif (val & SOF_TIMESTAMPING_RX_SOFTWARE)\n\t\tsock_enable_timestamp(sk,\n\t\t\t\t      SOCK_TIMESTAMPING_RX_SOFTWARE);\n\telse\n\t\tsock_disable_timestamp(sk,\n\t\t\t\t       (1UL << SOCK_TIMESTAMPING_RX_SOFTWARE));\n\treturn 0;\n}\n\nvoid sock_set_keepalive(struct sock *sk)\n{\n\tlock_sock(sk);\n\tif (sk->sk_prot->keepalive)\n\t\tsk->sk_prot->keepalive(sk, true);\n\tsock_valbool_flag(sk, SOCK_KEEPOPEN, true);\n\trelease_sock(sk);\n}\nEXPORT_SYMBOL(sock_set_keepalive);\n\nstatic void __sock_set_rcvbuf(struct sock *sk, int val)\n{\n\t \n\tval = min_t(int, val, INT_MAX / 2);\n\tsk->sk_userlocks |= SOCK_RCVBUF_LOCK;\n\n\t \n\tWRITE_ONCE(sk->sk_rcvbuf, max_t(int, val * 2, SOCK_MIN_RCVBUF));\n}\n\nvoid sock_set_rcvbuf(struct sock *sk, int val)\n{\n\tlock_sock(sk);\n\t__sock_set_rcvbuf(sk, val);\n\trelease_sock(sk);\n}\nEXPORT_SYMBOL(sock_set_rcvbuf);\n\nstatic void __sock_set_mark(struct sock *sk, u32 val)\n{\n\tif (val != sk->sk_mark) {\n\t\tWRITE_ONCE(sk->sk_mark, val);\n\t\tsk_dst_reset(sk);\n\t}\n}\n\nvoid sock_set_mark(struct sock *sk, u32 val)\n{\n\tlock_sock(sk);\n\t__sock_set_mark(sk, val);\n\trelease_sock(sk);\n}\nEXPORT_SYMBOL(sock_set_mark);\n\nstatic void sock_release_reserved_memory(struct sock *sk, int bytes)\n{\n\t \n\tbytes = round_down(bytes, PAGE_SIZE);\n\n\tWARN_ON(bytes > sk->sk_reserved_mem);\n\tWRITE_ONCE(sk->sk_reserved_mem, sk->sk_reserved_mem - bytes);\n\tsk_mem_reclaim(sk);\n}\n\nstatic int sock_reserve_memory(struct sock *sk, int bytes)\n{\n\tlong allocated;\n\tbool charged;\n\tint pages;\n\n\tif (!mem_cgroup_sockets_enabled || !sk->sk_memcg || !sk_has_account(sk))\n\t\treturn -EOPNOTSUPP;\n\n\tif (!bytes)\n\t\treturn 0;\n\n\tpages = sk_mem_pages(bytes);\n\n\t \n\tcharged = mem_cgroup_charge_skmem(sk->sk_memcg, pages,\n\t\t\t\t\t  GFP_KERNEL | __GFP_RETRY_MAYFAIL);\n\tif (!charged)\n\t\treturn -ENOMEM;\n\n\t \n\tsk_memory_allocated_add(sk, pages);\n\tallocated = sk_memory_allocated(sk);\n\t \n\tif (allocated > sk_prot_mem_limits(sk, 1)) {\n\t\tsk_memory_allocated_sub(sk, pages);\n\t\tmem_cgroup_uncharge_skmem(sk->sk_memcg, pages);\n\t\treturn -ENOMEM;\n\t}\n\tsk_forward_alloc_add(sk, pages << PAGE_SHIFT);\n\n\tWRITE_ONCE(sk->sk_reserved_mem,\n\t\t   sk->sk_reserved_mem + (pages << PAGE_SHIFT));\n\n\treturn 0;\n}\n\nvoid sockopt_lock_sock(struct sock *sk)\n{\n\t \n\tif (has_current_bpf_ctx())\n\t\treturn;\n\n\tlock_sock(sk);\n}\nEXPORT_SYMBOL(sockopt_lock_sock);\n\nvoid sockopt_release_sock(struct sock *sk)\n{\n\tif (has_current_bpf_ctx())\n\t\treturn;\n\n\trelease_sock(sk);\n}\nEXPORT_SYMBOL(sockopt_release_sock);\n\nbool sockopt_ns_capable(struct user_namespace *ns, int cap)\n{\n\treturn has_current_bpf_ctx() || ns_capable(ns, cap);\n}\nEXPORT_SYMBOL(sockopt_ns_capable);\n\nbool sockopt_capable(int cap)\n{\n\treturn has_current_bpf_ctx() || capable(cap);\n}\nEXPORT_SYMBOL(sockopt_capable);\n\n \n\nint sk_setsockopt(struct sock *sk, int level, int optname,\n\t\t  sockptr_t optval, unsigned int optlen)\n{\n\tstruct so_timestamping timestamping;\n\tstruct socket *sock = sk->sk_socket;\n\tstruct sock_txtime sk_txtime;\n\tint val;\n\tint valbool;\n\tstruct linger ling;\n\tint ret = 0;\n\n\t \n\n\tif (optname == SO_BINDTODEVICE)\n\t\treturn sock_setbindtodevice(sk, optval, optlen);\n\n\tif (optlen < sizeof(int))\n\t\treturn -EINVAL;\n\n\tif (copy_from_sockptr(&val, optval, sizeof(val)))\n\t\treturn -EFAULT;\n\n\tvalbool = val ? 1 : 0;\n\n\tsockopt_lock_sock(sk);\n\n\tswitch (optname) {\n\tcase SO_DEBUG:\n\t\tif (val && !sockopt_capable(CAP_NET_ADMIN))\n\t\t\tret = -EACCES;\n\t\telse\n\t\t\tsock_valbool_flag(sk, SOCK_DBG, valbool);\n\t\tbreak;\n\tcase SO_REUSEADDR:\n\t\tsk->sk_reuse = (valbool ? SK_CAN_REUSE : SK_NO_REUSE);\n\t\tbreak;\n\tcase SO_REUSEPORT:\n\t\tsk->sk_reuseport = valbool;\n\t\tbreak;\n\tcase SO_TYPE:\n\tcase SO_PROTOCOL:\n\tcase SO_DOMAIN:\n\tcase SO_ERROR:\n\t\tret = -ENOPROTOOPT;\n\t\tbreak;\n\tcase SO_DONTROUTE:\n\t\tsock_valbool_flag(sk, SOCK_LOCALROUTE, valbool);\n\t\tsk_dst_reset(sk);\n\t\tbreak;\n\tcase SO_BROADCAST:\n\t\tsock_valbool_flag(sk, SOCK_BROADCAST, valbool);\n\t\tbreak;\n\tcase SO_SNDBUF:\n\t\t \n\t\tval = min_t(u32, val, READ_ONCE(sysctl_wmem_max));\nset_sndbuf:\n\t\t \n\t\tval = min_t(int, val, INT_MAX / 2);\n\t\tsk->sk_userlocks |= SOCK_SNDBUF_LOCK;\n\t\tWRITE_ONCE(sk->sk_sndbuf,\n\t\t\t   max_t(int, val * 2, SOCK_MIN_SNDBUF));\n\t\t \n\t\tsk->sk_write_space(sk);\n\t\tbreak;\n\n\tcase SO_SNDBUFFORCE:\n\t\tif (!sockopt_capable(CAP_NET_ADMIN)) {\n\t\t\tret = -EPERM;\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (val < 0)\n\t\t\tval = 0;\n\t\tgoto set_sndbuf;\n\n\tcase SO_RCVBUF:\n\t\t \n\t\t__sock_set_rcvbuf(sk, min_t(u32, val, READ_ONCE(sysctl_rmem_max)));\n\t\tbreak;\n\n\tcase SO_RCVBUFFORCE:\n\t\tif (!sockopt_capable(CAP_NET_ADMIN)) {\n\t\t\tret = -EPERM;\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\t__sock_set_rcvbuf(sk, max(val, 0));\n\t\tbreak;\n\n\tcase SO_KEEPALIVE:\n\t\tif (sk->sk_prot->keepalive)\n\t\t\tsk->sk_prot->keepalive(sk, valbool);\n\t\tsock_valbool_flag(sk, SOCK_KEEPOPEN, valbool);\n\t\tbreak;\n\n\tcase SO_OOBINLINE:\n\t\tsock_valbool_flag(sk, SOCK_URGINLINE, valbool);\n\t\tbreak;\n\n\tcase SO_NO_CHECK:\n\t\tsk->sk_no_check_tx = valbool;\n\t\tbreak;\n\n\tcase SO_PRIORITY:\n\t\tif ((val >= 0 && val <= 6) ||\n\t\t    sockopt_ns_capable(sock_net(sk)->user_ns, CAP_NET_RAW) ||\n\t\t    sockopt_ns_capable(sock_net(sk)->user_ns, CAP_NET_ADMIN))\n\t\t\tWRITE_ONCE(sk->sk_priority, val);\n\t\telse\n\t\t\tret = -EPERM;\n\t\tbreak;\n\n\tcase SO_LINGER:\n\t\tif (optlen < sizeof(ling)) {\n\t\t\tret = -EINVAL;\t \n\t\t\tbreak;\n\t\t}\n\t\tif (copy_from_sockptr(&ling, optval, sizeof(ling))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (!ling.l_onoff) {\n\t\t\tsock_reset_flag(sk, SOCK_LINGER);\n\t\t} else {\n\t\t\tunsigned long t_sec = ling.l_linger;\n\n\t\t\tif (t_sec >= MAX_SCHEDULE_TIMEOUT / HZ)\n\t\t\t\tWRITE_ONCE(sk->sk_lingertime, MAX_SCHEDULE_TIMEOUT);\n\t\t\telse\n\t\t\t\tWRITE_ONCE(sk->sk_lingertime, t_sec * HZ);\n\t\t\tsock_set_flag(sk, SOCK_LINGER);\n\t\t}\n\t\tbreak;\n\n\tcase SO_BSDCOMPAT:\n\t\tbreak;\n\n\tcase SO_PASSCRED:\n\t\tassign_bit(SOCK_PASSCRED, &sock->flags, valbool);\n\t\tbreak;\n\n\tcase SO_PASSPIDFD:\n\t\tassign_bit(SOCK_PASSPIDFD, &sock->flags, valbool);\n\t\tbreak;\n\n\tcase SO_TIMESTAMP_OLD:\n\tcase SO_TIMESTAMP_NEW:\n\tcase SO_TIMESTAMPNS_OLD:\n\tcase SO_TIMESTAMPNS_NEW:\n\t\tsock_set_timestamp(sk, optname, valbool);\n\t\tbreak;\n\n\tcase SO_TIMESTAMPING_NEW:\n\tcase SO_TIMESTAMPING_OLD:\n\t\tif (optlen == sizeof(timestamping)) {\n\t\t\tif (copy_from_sockptr(&timestamping, optval,\n\t\t\t\t\t      sizeof(timestamping))) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} else {\n\t\t\tmemset(&timestamping, 0, sizeof(timestamping));\n\t\t\ttimestamping.flags = val;\n\t\t}\n\t\tret = sock_set_timestamping(sk, optname, timestamping);\n\t\tbreak;\n\n\tcase SO_RCVLOWAT:\n\t\t{\n\t\tint (*set_rcvlowat)(struct sock *sk, int val) = NULL;\n\n\t\tif (val < 0)\n\t\t\tval = INT_MAX;\n\t\tif (sock)\n\t\t\tset_rcvlowat = READ_ONCE(sock->ops)->set_rcvlowat;\n\t\tif (set_rcvlowat)\n\t\t\tret = set_rcvlowat(sk, val);\n\t\telse\n\t\t\tWRITE_ONCE(sk->sk_rcvlowat, val ? : 1);\n\t\tbreak;\n\t\t}\n\tcase SO_RCVTIMEO_OLD:\n\tcase SO_RCVTIMEO_NEW:\n\t\tret = sock_set_timeout(&sk->sk_rcvtimeo, optval,\n\t\t\t\t       optlen, optname == SO_RCVTIMEO_OLD);\n\t\tbreak;\n\n\tcase SO_SNDTIMEO_OLD:\n\tcase SO_SNDTIMEO_NEW:\n\t\tret = sock_set_timeout(&sk->sk_sndtimeo, optval,\n\t\t\t\t       optlen, optname == SO_SNDTIMEO_OLD);\n\t\tbreak;\n\n\tcase SO_ATTACH_FILTER: {\n\t\tstruct sock_fprog fprog;\n\n\t\tret = copy_bpf_fprog_from_user(&fprog, optval, optlen);\n\t\tif (!ret)\n\t\t\tret = sk_attach_filter(&fprog, sk);\n\t\tbreak;\n\t}\n\tcase SO_ATTACH_BPF:\n\t\tret = -EINVAL;\n\t\tif (optlen == sizeof(u32)) {\n\t\t\tu32 ufd;\n\n\t\t\tret = -EFAULT;\n\t\t\tif (copy_from_sockptr(&ufd, optval, sizeof(ufd)))\n\t\t\t\tbreak;\n\n\t\t\tret = sk_attach_bpf(ufd, sk);\n\t\t}\n\t\tbreak;\n\n\tcase SO_ATTACH_REUSEPORT_CBPF: {\n\t\tstruct sock_fprog fprog;\n\n\t\tret = copy_bpf_fprog_from_user(&fprog, optval, optlen);\n\t\tif (!ret)\n\t\t\tret = sk_reuseport_attach_filter(&fprog, sk);\n\t\tbreak;\n\t}\n\tcase SO_ATTACH_REUSEPORT_EBPF:\n\t\tret = -EINVAL;\n\t\tif (optlen == sizeof(u32)) {\n\t\t\tu32 ufd;\n\n\t\t\tret = -EFAULT;\n\t\t\tif (copy_from_sockptr(&ufd, optval, sizeof(ufd)))\n\t\t\t\tbreak;\n\n\t\t\tret = sk_reuseport_attach_bpf(ufd, sk);\n\t\t}\n\t\tbreak;\n\n\tcase SO_DETACH_REUSEPORT_BPF:\n\t\tret = reuseport_detach_prog(sk);\n\t\tbreak;\n\n\tcase SO_DETACH_FILTER:\n\t\tret = sk_detach_filter(sk);\n\t\tbreak;\n\n\tcase SO_LOCK_FILTER:\n\t\tif (sock_flag(sk, SOCK_FILTER_LOCKED) && !valbool)\n\t\t\tret = -EPERM;\n\t\telse\n\t\t\tsock_valbool_flag(sk, SOCK_FILTER_LOCKED, valbool);\n\t\tbreak;\n\n\tcase SO_PASSSEC:\n\t\tassign_bit(SOCK_PASSSEC, &sock->flags, valbool);\n\t\tbreak;\n\tcase SO_MARK:\n\t\tif (!sockopt_ns_capable(sock_net(sk)->user_ns, CAP_NET_RAW) &&\n\t\t    !sockopt_ns_capable(sock_net(sk)->user_ns, CAP_NET_ADMIN)) {\n\t\t\tret = -EPERM;\n\t\t\tbreak;\n\t\t}\n\n\t\t__sock_set_mark(sk, val);\n\t\tbreak;\n\tcase SO_RCVMARK:\n\t\tsock_valbool_flag(sk, SOCK_RCVMARK, valbool);\n\t\tbreak;\n\n\tcase SO_RXQ_OVFL:\n\t\tsock_valbool_flag(sk, SOCK_RXQ_OVFL, valbool);\n\t\tbreak;\n\n\tcase SO_WIFI_STATUS:\n\t\tsock_valbool_flag(sk, SOCK_WIFI_STATUS, valbool);\n\t\tbreak;\n\n\tcase SO_PEEK_OFF:\n\t\t{\n\t\tint (*set_peek_off)(struct sock *sk, int val);\n\n\t\tset_peek_off = READ_ONCE(sock->ops)->set_peek_off;\n\t\tif (set_peek_off)\n\t\t\tret = set_peek_off(sk, val);\n\t\telse\n\t\t\tret = -EOPNOTSUPP;\n\t\tbreak;\n\t\t}\n\n\tcase SO_NOFCS:\n\t\tsock_valbool_flag(sk, SOCK_NOFCS, valbool);\n\t\tbreak;\n\n\tcase SO_SELECT_ERR_QUEUE:\n\t\tsock_valbool_flag(sk, SOCK_SELECT_ERR_QUEUE, valbool);\n\t\tbreak;\n\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tcase SO_BUSY_POLL:\n\t\tif (val < 0)\n\t\t\tret = -EINVAL;\n\t\telse\n\t\t\tWRITE_ONCE(sk->sk_ll_usec, val);\n\t\tbreak;\n\tcase SO_PREFER_BUSY_POLL:\n\t\tif (valbool && !sockopt_capable(CAP_NET_ADMIN))\n\t\t\tret = -EPERM;\n\t\telse\n\t\t\tWRITE_ONCE(sk->sk_prefer_busy_poll, valbool);\n\t\tbreak;\n\tcase SO_BUSY_POLL_BUDGET:\n\t\tif (val > READ_ONCE(sk->sk_busy_poll_budget) && !sockopt_capable(CAP_NET_ADMIN)) {\n\t\t\tret = -EPERM;\n\t\t} else {\n\t\t\tif (val < 0 || val > U16_MAX)\n\t\t\t\tret = -EINVAL;\n\t\t\telse\n\t\t\t\tWRITE_ONCE(sk->sk_busy_poll_budget, val);\n\t\t}\n\t\tbreak;\n#endif\n\n\tcase SO_MAX_PACING_RATE:\n\t\t{\n\t\tunsigned long ulval = (val == ~0U) ? ~0UL : (unsigned int)val;\n\n\t\tif (sizeof(ulval) != sizeof(val) &&\n\t\t    optlen >= sizeof(ulval) &&\n\t\t    copy_from_sockptr(&ulval, optval, sizeof(ulval))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (ulval != ~0UL)\n\t\t\tcmpxchg(&sk->sk_pacing_status,\n\t\t\t\tSK_PACING_NONE,\n\t\t\t\tSK_PACING_NEEDED);\n\t\t \n\t\tWRITE_ONCE(sk->sk_max_pacing_rate, ulval);\n\t\tsk->sk_pacing_rate = min(sk->sk_pacing_rate, ulval);\n\t\tbreak;\n\t\t}\n\tcase SO_INCOMING_CPU:\n\t\treuseport_update_incoming_cpu(sk, val);\n\t\tbreak;\n\n\tcase SO_CNX_ADVICE:\n\t\tif (val == 1)\n\t\t\tdst_negative_advice(sk);\n\t\tbreak;\n\n\tcase SO_ZEROCOPY:\n\t\tif (sk->sk_family == PF_INET || sk->sk_family == PF_INET6) {\n\t\t\tif (!(sk_is_tcp(sk) ||\n\t\t\t      (sk->sk_type == SOCK_DGRAM &&\n\t\t\t       sk->sk_protocol == IPPROTO_UDP)))\n\t\t\t\tret = -EOPNOTSUPP;\n\t\t} else if (sk->sk_family != PF_RDS) {\n\t\t\tret = -EOPNOTSUPP;\n\t\t}\n\t\tif (!ret) {\n\t\t\tif (val < 0 || val > 1)\n\t\t\t\tret = -EINVAL;\n\t\t\telse\n\t\t\t\tsock_valbool_flag(sk, SOCK_ZEROCOPY, valbool);\n\t\t}\n\t\tbreak;\n\n\tcase SO_TXTIME:\n\t\tif (optlen != sizeof(struct sock_txtime)) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t} else if (copy_from_sockptr(&sk_txtime, optval,\n\t\t\t   sizeof(struct sock_txtime))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t} else if (sk_txtime.flags & ~SOF_TXTIME_FLAGS_MASK) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\t \n\t\tif (sk_txtime.clockid != CLOCK_MONOTONIC &&\n\t\t    !sockopt_ns_capable(sock_net(sk)->user_ns, CAP_NET_ADMIN)) {\n\t\t\tret = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tsock_valbool_flag(sk, SOCK_TXTIME, true);\n\t\tsk->sk_clockid = sk_txtime.clockid;\n\t\tsk->sk_txtime_deadline_mode =\n\t\t\t!!(sk_txtime.flags & SOF_TXTIME_DEADLINE_MODE);\n\t\tsk->sk_txtime_report_errors =\n\t\t\t!!(sk_txtime.flags & SOF_TXTIME_REPORT_ERRORS);\n\t\tbreak;\n\n\tcase SO_BINDTOIFINDEX:\n\t\tret = sock_bindtoindex_locked(sk, val);\n\t\tbreak;\n\n\tcase SO_BUF_LOCK:\n\t\tif (val & ~SOCK_BUF_LOCK_MASK) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tsk->sk_userlocks = val | (sk->sk_userlocks &\n\t\t\t\t\t  ~SOCK_BUF_LOCK_MASK);\n\t\tbreak;\n\n\tcase SO_RESERVE_MEM:\n\t{\n\t\tint delta;\n\n\t\tif (val < 0) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tdelta = val - sk->sk_reserved_mem;\n\t\tif (delta < 0)\n\t\t\tsock_release_reserved_memory(sk, -delta);\n\t\telse\n\t\t\tret = sock_reserve_memory(sk, delta);\n\t\tbreak;\n\t}\n\n\tcase SO_TXREHASH:\n\t\tif (val < -1 || val > 1) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tif ((u8)val == SOCK_TXREHASH_DEFAULT)\n\t\t\tval = READ_ONCE(sock_net(sk)->core.sysctl_txrehash);\n\t\t \n\t\tWRITE_ONCE(sk->sk_txrehash, (u8)val);\n\t\tbreak;\n\n\tdefault:\n\t\tret = -ENOPROTOOPT;\n\t\tbreak;\n\t}\n\tsockopt_release_sock(sk);\n\treturn ret;\n}\n\nint sock_setsockopt(struct socket *sock, int level, int optname,\n\t\t    sockptr_t optval, unsigned int optlen)\n{\n\treturn sk_setsockopt(sock->sk, level, optname,\n\t\t\t     optval, optlen);\n}\nEXPORT_SYMBOL(sock_setsockopt);\n\nstatic const struct cred *sk_get_peer_cred(struct sock *sk)\n{\n\tconst struct cred *cred;\n\n\tspin_lock(&sk->sk_peer_lock);\n\tcred = get_cred(sk->sk_peer_cred);\n\tspin_unlock(&sk->sk_peer_lock);\n\n\treturn cred;\n}\n\nstatic void cred_to_ucred(struct pid *pid, const struct cred *cred,\n\t\t\t  struct ucred *ucred)\n{\n\tucred->pid = pid_vnr(pid);\n\tucred->uid = ucred->gid = -1;\n\tif (cred) {\n\t\tstruct user_namespace *current_ns = current_user_ns();\n\n\t\tucred->uid = from_kuid_munged(current_ns, cred->euid);\n\t\tucred->gid = from_kgid_munged(current_ns, cred->egid);\n\t}\n}\n\nstatic int groups_to_user(sockptr_t dst, const struct group_info *src)\n{\n\tstruct user_namespace *user_ns = current_user_ns();\n\tint i;\n\n\tfor (i = 0; i < src->ngroups; i++) {\n\t\tgid_t gid = from_kgid_munged(user_ns, src->gid[i]);\n\n\t\tif (copy_to_sockptr_offset(dst, i * sizeof(gid), &gid, sizeof(gid)))\n\t\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n}\n\nint sk_getsockopt(struct sock *sk, int level, int optname,\n\t\t  sockptr_t optval, sockptr_t optlen)\n{\n\tstruct socket *sock = sk->sk_socket;\n\n\tunion {\n\t\tint val;\n\t\tu64 val64;\n\t\tunsigned long ulval;\n\t\tstruct linger ling;\n\t\tstruct old_timeval32 tm32;\n\t\tstruct __kernel_old_timeval tm;\n\t\tstruct  __kernel_sock_timeval stm;\n\t\tstruct sock_txtime txtime;\n\t\tstruct so_timestamping timestamping;\n\t} v;\n\n\tint lv = sizeof(int);\n\tint len;\n\n\tif (copy_from_sockptr(&len, optlen, sizeof(int)))\n\t\treturn -EFAULT;\n\tif (len < 0)\n\t\treturn -EINVAL;\n\n\tmemset(&v, 0, sizeof(v));\n\n\tswitch (optname) {\n\tcase SO_DEBUG:\n\t\tv.val = sock_flag(sk, SOCK_DBG);\n\t\tbreak;\n\n\tcase SO_DONTROUTE:\n\t\tv.val = sock_flag(sk, SOCK_LOCALROUTE);\n\t\tbreak;\n\n\tcase SO_BROADCAST:\n\t\tv.val = sock_flag(sk, SOCK_BROADCAST);\n\t\tbreak;\n\n\tcase SO_SNDBUF:\n\t\tv.val = READ_ONCE(sk->sk_sndbuf);\n\t\tbreak;\n\n\tcase SO_RCVBUF:\n\t\tv.val = READ_ONCE(sk->sk_rcvbuf);\n\t\tbreak;\n\n\tcase SO_REUSEADDR:\n\t\tv.val = sk->sk_reuse;\n\t\tbreak;\n\n\tcase SO_REUSEPORT:\n\t\tv.val = sk->sk_reuseport;\n\t\tbreak;\n\n\tcase SO_KEEPALIVE:\n\t\tv.val = sock_flag(sk, SOCK_KEEPOPEN);\n\t\tbreak;\n\n\tcase SO_TYPE:\n\t\tv.val = sk->sk_type;\n\t\tbreak;\n\n\tcase SO_PROTOCOL:\n\t\tv.val = sk->sk_protocol;\n\t\tbreak;\n\n\tcase SO_DOMAIN:\n\t\tv.val = sk->sk_family;\n\t\tbreak;\n\n\tcase SO_ERROR:\n\t\tv.val = -sock_error(sk);\n\t\tif (v.val == 0)\n\t\t\tv.val = xchg(&sk->sk_err_soft, 0);\n\t\tbreak;\n\n\tcase SO_OOBINLINE:\n\t\tv.val = sock_flag(sk, SOCK_URGINLINE);\n\t\tbreak;\n\n\tcase SO_NO_CHECK:\n\t\tv.val = sk->sk_no_check_tx;\n\t\tbreak;\n\n\tcase SO_PRIORITY:\n\t\tv.val = READ_ONCE(sk->sk_priority);\n\t\tbreak;\n\n\tcase SO_LINGER:\n\t\tlv\t\t= sizeof(v.ling);\n\t\tv.ling.l_onoff\t= sock_flag(sk, SOCK_LINGER);\n\t\tv.ling.l_linger\t= READ_ONCE(sk->sk_lingertime) / HZ;\n\t\tbreak;\n\n\tcase SO_BSDCOMPAT:\n\t\tbreak;\n\n\tcase SO_TIMESTAMP_OLD:\n\t\tv.val = sock_flag(sk, SOCK_RCVTSTAMP) &&\n\t\t\t\t!sock_flag(sk, SOCK_TSTAMP_NEW) &&\n\t\t\t\t!sock_flag(sk, SOCK_RCVTSTAMPNS);\n\t\tbreak;\n\n\tcase SO_TIMESTAMPNS_OLD:\n\t\tv.val = sock_flag(sk, SOCK_RCVTSTAMPNS) && !sock_flag(sk, SOCK_TSTAMP_NEW);\n\t\tbreak;\n\n\tcase SO_TIMESTAMP_NEW:\n\t\tv.val = sock_flag(sk, SOCK_RCVTSTAMP) && sock_flag(sk, SOCK_TSTAMP_NEW);\n\t\tbreak;\n\n\tcase SO_TIMESTAMPNS_NEW:\n\t\tv.val = sock_flag(sk, SOCK_RCVTSTAMPNS) && sock_flag(sk, SOCK_TSTAMP_NEW);\n\t\tbreak;\n\n\tcase SO_TIMESTAMPING_OLD:\n\tcase SO_TIMESTAMPING_NEW:\n\t\tlv = sizeof(v.timestamping);\n\t\t \n\t\tif (optname == SO_TIMESTAMPING_OLD || sock_flag(sk, SOCK_TSTAMP_NEW)) {\n\t\t\tv.timestamping.flags = READ_ONCE(sk->sk_tsflags);\n\t\t\tv.timestamping.bind_phc = READ_ONCE(sk->sk_bind_phc);\n\t\t}\n\t\tbreak;\n\n\tcase SO_RCVTIMEO_OLD:\n\tcase SO_RCVTIMEO_NEW:\n\t\tlv = sock_get_timeout(READ_ONCE(sk->sk_rcvtimeo), &v,\n\t\t\t\t      SO_RCVTIMEO_OLD == optname);\n\t\tbreak;\n\n\tcase SO_SNDTIMEO_OLD:\n\tcase SO_SNDTIMEO_NEW:\n\t\tlv = sock_get_timeout(READ_ONCE(sk->sk_sndtimeo), &v,\n\t\t\t\t      SO_SNDTIMEO_OLD == optname);\n\t\tbreak;\n\n\tcase SO_RCVLOWAT:\n\t\tv.val = READ_ONCE(sk->sk_rcvlowat);\n\t\tbreak;\n\n\tcase SO_SNDLOWAT:\n\t\tv.val = 1;\n\t\tbreak;\n\n\tcase SO_PASSCRED:\n\t\tv.val = !!test_bit(SOCK_PASSCRED, &sock->flags);\n\t\tbreak;\n\n\tcase SO_PASSPIDFD:\n\t\tv.val = !!test_bit(SOCK_PASSPIDFD, &sock->flags);\n\t\tbreak;\n\n\tcase SO_PEERCRED:\n\t{\n\t\tstruct ucred peercred;\n\t\tif (len > sizeof(peercred))\n\t\t\tlen = sizeof(peercred);\n\n\t\tspin_lock(&sk->sk_peer_lock);\n\t\tcred_to_ucred(sk->sk_peer_pid, sk->sk_peer_cred, &peercred);\n\t\tspin_unlock(&sk->sk_peer_lock);\n\n\t\tif (copy_to_sockptr(optval, &peercred, len))\n\t\t\treturn -EFAULT;\n\t\tgoto lenout;\n\t}\n\n\tcase SO_PEERPIDFD:\n\t{\n\t\tstruct pid *peer_pid;\n\t\tstruct file *pidfd_file = NULL;\n\t\tint pidfd;\n\n\t\tif (len > sizeof(pidfd))\n\t\t\tlen = sizeof(pidfd);\n\n\t\tspin_lock(&sk->sk_peer_lock);\n\t\tpeer_pid = get_pid(sk->sk_peer_pid);\n\t\tspin_unlock(&sk->sk_peer_lock);\n\n\t\tif (!peer_pid)\n\t\t\treturn -ENODATA;\n\n\t\tpidfd = pidfd_prepare(peer_pid, 0, &pidfd_file);\n\t\tput_pid(peer_pid);\n\t\tif (pidfd < 0)\n\t\t\treturn pidfd;\n\n\t\tif (copy_to_sockptr(optval, &pidfd, len) ||\n\t\t    copy_to_sockptr(optlen, &len, sizeof(int))) {\n\t\t\tput_unused_fd(pidfd);\n\t\t\tfput(pidfd_file);\n\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tfd_install(pidfd, pidfd_file);\n\t\treturn 0;\n\t}\n\n\tcase SO_PEERGROUPS:\n\t{\n\t\tconst struct cred *cred;\n\t\tint ret, n;\n\n\t\tcred = sk_get_peer_cred(sk);\n\t\tif (!cred)\n\t\t\treturn -ENODATA;\n\n\t\tn = cred->group_info->ngroups;\n\t\tif (len < n * sizeof(gid_t)) {\n\t\t\tlen = n * sizeof(gid_t);\n\t\t\tput_cred(cred);\n\t\t\treturn copy_to_sockptr(optlen, &len, sizeof(int)) ? -EFAULT : -ERANGE;\n\t\t}\n\t\tlen = n * sizeof(gid_t);\n\n\t\tret = groups_to_user(optval, cred->group_info);\n\t\tput_cred(cred);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tgoto lenout;\n\t}\n\n\tcase SO_PEERNAME:\n\t{\n\t\tstruct sockaddr_storage address;\n\n\t\tlv = READ_ONCE(sock->ops)->getname(sock, (struct sockaddr *)&address, 2);\n\t\tif (lv < 0)\n\t\t\treturn -ENOTCONN;\n\t\tif (lv < len)\n\t\t\treturn -EINVAL;\n\t\tif (copy_to_sockptr(optval, &address, len))\n\t\t\treturn -EFAULT;\n\t\tgoto lenout;\n\t}\n\n\t \n\tcase SO_ACCEPTCONN:\n\t\tv.val = sk->sk_state == TCP_LISTEN;\n\t\tbreak;\n\n\tcase SO_PASSSEC:\n\t\tv.val = !!test_bit(SOCK_PASSSEC, &sock->flags);\n\t\tbreak;\n\n\tcase SO_PEERSEC:\n\t\treturn security_socket_getpeersec_stream(sock,\n\t\t\t\t\t\t\t optval, optlen, len);\n\n\tcase SO_MARK:\n\t\tv.val = READ_ONCE(sk->sk_mark);\n\t\tbreak;\n\n\tcase SO_RCVMARK:\n\t\tv.val = sock_flag(sk, SOCK_RCVMARK);\n\t\tbreak;\n\n\tcase SO_RXQ_OVFL:\n\t\tv.val = sock_flag(sk, SOCK_RXQ_OVFL);\n\t\tbreak;\n\n\tcase SO_WIFI_STATUS:\n\t\tv.val = sock_flag(sk, SOCK_WIFI_STATUS);\n\t\tbreak;\n\n\tcase SO_PEEK_OFF:\n\t\tif (!READ_ONCE(sock->ops)->set_peek_off)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tv.val = READ_ONCE(sk->sk_peek_off);\n\t\tbreak;\n\tcase SO_NOFCS:\n\t\tv.val = sock_flag(sk, SOCK_NOFCS);\n\t\tbreak;\n\n\tcase SO_BINDTODEVICE:\n\t\treturn sock_getbindtodevice(sk, optval, optlen, len);\n\n\tcase SO_GET_FILTER:\n\t\tlen = sk_get_filter(sk, optval, len);\n\t\tif (len < 0)\n\t\t\treturn len;\n\n\t\tgoto lenout;\n\n\tcase SO_LOCK_FILTER:\n\t\tv.val = sock_flag(sk, SOCK_FILTER_LOCKED);\n\t\tbreak;\n\n\tcase SO_BPF_EXTENSIONS:\n\t\tv.val = bpf_tell_extensions();\n\t\tbreak;\n\n\tcase SO_SELECT_ERR_QUEUE:\n\t\tv.val = sock_flag(sk, SOCK_SELECT_ERR_QUEUE);\n\t\tbreak;\n\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tcase SO_BUSY_POLL:\n\t\tv.val = READ_ONCE(sk->sk_ll_usec);\n\t\tbreak;\n\tcase SO_PREFER_BUSY_POLL:\n\t\tv.val = READ_ONCE(sk->sk_prefer_busy_poll);\n\t\tbreak;\n#endif\n\n\tcase SO_MAX_PACING_RATE:\n\t\t \n\t\tif (sizeof(v.ulval) != sizeof(v.val) && len >= sizeof(v.ulval)) {\n\t\t\tlv = sizeof(v.ulval);\n\t\t\tv.ulval = READ_ONCE(sk->sk_max_pacing_rate);\n\t\t} else {\n\t\t\t \n\t\t\tv.val = min_t(unsigned long, ~0U,\n\t\t\t\t      READ_ONCE(sk->sk_max_pacing_rate));\n\t\t}\n\t\tbreak;\n\n\tcase SO_INCOMING_CPU:\n\t\tv.val = READ_ONCE(sk->sk_incoming_cpu);\n\t\tbreak;\n\n\tcase SO_MEMINFO:\n\t{\n\t\tu32 meminfo[SK_MEMINFO_VARS];\n\n\t\tsk_get_meminfo(sk, meminfo);\n\n\t\tlen = min_t(unsigned int, len, sizeof(meminfo));\n\t\tif (copy_to_sockptr(optval, &meminfo, len))\n\t\t\treturn -EFAULT;\n\n\t\tgoto lenout;\n\t}\n\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tcase SO_INCOMING_NAPI_ID:\n\t\tv.val = READ_ONCE(sk->sk_napi_id);\n\n\t\t \n\t\tif (v.val < MIN_NAPI_ID)\n\t\t\tv.val = 0;\n\n\t\tbreak;\n#endif\n\n\tcase SO_COOKIE:\n\t\tlv = sizeof(u64);\n\t\tif (len < lv)\n\t\t\treturn -EINVAL;\n\t\tv.val64 = sock_gen_cookie(sk);\n\t\tbreak;\n\n\tcase SO_ZEROCOPY:\n\t\tv.val = sock_flag(sk, SOCK_ZEROCOPY);\n\t\tbreak;\n\n\tcase SO_TXTIME:\n\t\tlv = sizeof(v.txtime);\n\t\tv.txtime.clockid = sk->sk_clockid;\n\t\tv.txtime.flags |= sk->sk_txtime_deadline_mode ?\n\t\t\t\t  SOF_TXTIME_DEADLINE_MODE : 0;\n\t\tv.txtime.flags |= sk->sk_txtime_report_errors ?\n\t\t\t\t  SOF_TXTIME_REPORT_ERRORS : 0;\n\t\tbreak;\n\n\tcase SO_BINDTOIFINDEX:\n\t\tv.val = READ_ONCE(sk->sk_bound_dev_if);\n\t\tbreak;\n\n\tcase SO_NETNS_COOKIE:\n\t\tlv = sizeof(u64);\n\t\tif (len != lv)\n\t\t\treturn -EINVAL;\n\t\tv.val64 = sock_net(sk)->net_cookie;\n\t\tbreak;\n\n\tcase SO_BUF_LOCK:\n\t\tv.val = sk->sk_userlocks & SOCK_BUF_LOCK_MASK;\n\t\tbreak;\n\n\tcase SO_RESERVE_MEM:\n\t\tv.val = READ_ONCE(sk->sk_reserved_mem);\n\t\tbreak;\n\n\tcase SO_TXREHASH:\n\t\t \n\t\tv.val = READ_ONCE(sk->sk_txrehash);\n\t\tbreak;\n\n\tdefault:\n\t\t \n\t\treturn -ENOPROTOOPT;\n\t}\n\n\tif (len > lv)\n\t\tlen = lv;\n\tif (copy_to_sockptr(optval, &v, len))\n\t\treturn -EFAULT;\nlenout:\n\tif (copy_to_sockptr(optlen, &len, sizeof(int)))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nint sock_getsockopt(struct socket *sock, int level, int optname,\n\t\t    char __user *optval, int __user *optlen)\n{\n\treturn sk_getsockopt(sock->sk, level, optname,\n\t\t\t     USER_SOCKPTR(optval),\n\t\t\t     USER_SOCKPTR(optlen));\n}\n\n \nstatic inline void sock_lock_init(struct sock *sk)\n{\n\tif (sk->sk_kern_sock)\n\t\tsock_lock_init_class_and_name(\n\t\t\tsk,\n\t\t\taf_family_kern_slock_key_strings[sk->sk_family],\n\t\t\taf_family_kern_slock_keys + sk->sk_family,\n\t\t\taf_family_kern_key_strings[sk->sk_family],\n\t\t\taf_family_kern_keys + sk->sk_family);\n\telse\n\t\tsock_lock_init_class_and_name(\n\t\t\tsk,\n\t\t\taf_family_slock_key_strings[sk->sk_family],\n\t\t\taf_family_slock_keys + sk->sk_family,\n\t\t\taf_family_key_strings[sk->sk_family],\n\t\t\taf_family_keys + sk->sk_family);\n}\n\n \nstatic void sock_copy(struct sock *nsk, const struct sock *osk)\n{\n\tconst struct proto *prot = READ_ONCE(osk->sk_prot);\n#ifdef CONFIG_SECURITY_NETWORK\n\tvoid *sptr = nsk->sk_security;\n#endif\n\n\t \n\tBUILD_BUG_ON(offsetof(struct sock, sk_tx_queue_mapping) <\n\t\t     offsetof(struct sock, sk_dontcopy_begin) ||\n\t\t     offsetof(struct sock, sk_tx_queue_mapping) >=\n\t\t     offsetof(struct sock, sk_dontcopy_end));\n\n\tmemcpy(nsk, osk, offsetof(struct sock, sk_dontcopy_begin));\n\n\tmemcpy(&nsk->sk_dontcopy_end, &osk->sk_dontcopy_end,\n\t       prot->obj_size - offsetof(struct sock, sk_dontcopy_end));\n\n#ifdef CONFIG_SECURITY_NETWORK\n\tnsk->sk_security = sptr;\n\tsecurity_sk_clone(osk, nsk);\n#endif\n}\n\nstatic struct sock *sk_prot_alloc(struct proto *prot, gfp_t priority,\n\t\tint family)\n{\n\tstruct sock *sk;\n\tstruct kmem_cache *slab;\n\n\tslab = prot->slab;\n\tif (slab != NULL) {\n\t\tsk = kmem_cache_alloc(slab, priority & ~__GFP_ZERO);\n\t\tif (!sk)\n\t\t\treturn sk;\n\t\tif (want_init_on_alloc(priority))\n\t\t\tsk_prot_clear_nulls(sk, prot->obj_size);\n\t} else\n\t\tsk = kmalloc(prot->obj_size, priority);\n\n\tif (sk != NULL) {\n\t\tif (security_sk_alloc(sk, family, priority))\n\t\t\tgoto out_free;\n\n\t\tif (!try_module_get(prot->owner))\n\t\t\tgoto out_free_sec;\n\t}\n\n\treturn sk;\n\nout_free_sec:\n\tsecurity_sk_free(sk);\nout_free:\n\tif (slab != NULL)\n\t\tkmem_cache_free(slab, sk);\n\telse\n\t\tkfree(sk);\n\treturn NULL;\n}\n\nstatic void sk_prot_free(struct proto *prot, struct sock *sk)\n{\n\tstruct kmem_cache *slab;\n\tstruct module *owner;\n\n\towner = prot->owner;\n\tslab = prot->slab;\n\n\tcgroup_sk_free(&sk->sk_cgrp_data);\n\tmem_cgroup_sk_free(sk);\n\tsecurity_sk_free(sk);\n\tif (slab != NULL)\n\t\tkmem_cache_free(slab, sk);\n\telse\n\t\tkfree(sk);\n\tmodule_put(owner);\n}\n\n \nstruct sock *sk_alloc(struct net *net, int family, gfp_t priority,\n\t\t      struct proto *prot, int kern)\n{\n\tstruct sock *sk;\n\n\tsk = sk_prot_alloc(prot, priority | __GFP_ZERO, family);\n\tif (sk) {\n\t\tsk->sk_family = family;\n\t\t \n\t\tsk->sk_prot = sk->sk_prot_creator = prot;\n\t\tsk->sk_kern_sock = kern;\n\t\tsock_lock_init(sk);\n\t\tsk->sk_net_refcnt = kern ? 0 : 1;\n\t\tif (likely(sk->sk_net_refcnt)) {\n\t\t\tget_net_track(net, &sk->ns_tracker, priority);\n\t\t\tsock_inuse_add(net, 1);\n\t\t} else {\n\t\t\t__netns_tracker_alloc(net, &sk->ns_tracker,\n\t\t\t\t\t      false, priority);\n\t\t}\n\n\t\tsock_net_set(sk, net);\n\t\trefcount_set(&sk->sk_wmem_alloc, 1);\n\n\t\tmem_cgroup_sk_alloc(sk);\n\t\tcgroup_sk_alloc(&sk->sk_cgrp_data);\n\t\tsock_update_classid(&sk->sk_cgrp_data);\n\t\tsock_update_netprioidx(&sk->sk_cgrp_data);\n\t\tsk_tx_queue_clear(sk);\n\t}\n\n\treturn sk;\n}\nEXPORT_SYMBOL(sk_alloc);\n\n \nstatic void __sk_destruct(struct rcu_head *head)\n{\n\tstruct sock *sk = container_of(head, struct sock, sk_rcu);\n\tstruct sk_filter *filter;\n\n\tif (sk->sk_destruct)\n\t\tsk->sk_destruct(sk);\n\n\tfilter = rcu_dereference_check(sk->sk_filter,\n\t\t\t\t       refcount_read(&sk->sk_wmem_alloc) == 0);\n\tif (filter) {\n\t\tsk_filter_uncharge(sk, filter);\n\t\tRCU_INIT_POINTER(sk->sk_filter, NULL);\n\t}\n\n\tsock_disable_timestamp(sk, SK_FLAGS_TIMESTAMP);\n\n#ifdef CONFIG_BPF_SYSCALL\n\tbpf_sk_storage_free(sk);\n#endif\n\n\tif (atomic_read(&sk->sk_omem_alloc))\n\t\tpr_debug(\"%s: optmem leakage (%d bytes) detected\\n\",\n\t\t\t __func__, atomic_read(&sk->sk_omem_alloc));\n\n\tif (sk->sk_frag.page) {\n\t\tput_page(sk->sk_frag.page);\n\t\tsk->sk_frag.page = NULL;\n\t}\n\n\t \n\tput_cred(sk->sk_peer_cred);\n\tput_pid(sk->sk_peer_pid);\n\n\tif (likely(sk->sk_net_refcnt))\n\t\tput_net_track(sock_net(sk), &sk->ns_tracker);\n\telse\n\t\t__netns_tracker_free(sock_net(sk), &sk->ns_tracker, false);\n\n\tsk_prot_free(sk->sk_prot_creator, sk);\n}\n\nvoid sk_destruct(struct sock *sk)\n{\n\tbool use_call_rcu = sock_flag(sk, SOCK_RCU_FREE);\n\n\tif (rcu_access_pointer(sk->sk_reuseport_cb)) {\n\t\treuseport_detach_sock(sk);\n\t\tuse_call_rcu = true;\n\t}\n\n\tif (use_call_rcu)\n\t\tcall_rcu(&sk->sk_rcu, __sk_destruct);\n\telse\n\t\t__sk_destruct(&sk->sk_rcu);\n}\n\nstatic void __sk_free(struct sock *sk)\n{\n\tif (likely(sk->sk_net_refcnt))\n\t\tsock_inuse_add(sock_net(sk), -1);\n\n\tif (unlikely(sk->sk_net_refcnt && sock_diag_has_destroy_listeners(sk)))\n\t\tsock_diag_broadcast_destroy(sk);\n\telse\n\t\tsk_destruct(sk);\n}\n\nvoid sk_free(struct sock *sk)\n{\n\t \n\tif (refcount_dec_and_test(&sk->sk_wmem_alloc))\n\t\t__sk_free(sk);\n}\nEXPORT_SYMBOL(sk_free);\n\nstatic void sk_init_common(struct sock *sk)\n{\n\tskb_queue_head_init(&sk->sk_receive_queue);\n\tskb_queue_head_init(&sk->sk_write_queue);\n\tskb_queue_head_init(&sk->sk_error_queue);\n\n\trwlock_init(&sk->sk_callback_lock);\n\tlockdep_set_class_and_name(&sk->sk_receive_queue.lock,\n\t\t\taf_rlock_keys + sk->sk_family,\n\t\t\taf_family_rlock_key_strings[sk->sk_family]);\n\tlockdep_set_class_and_name(&sk->sk_write_queue.lock,\n\t\t\taf_wlock_keys + sk->sk_family,\n\t\t\taf_family_wlock_key_strings[sk->sk_family]);\n\tlockdep_set_class_and_name(&sk->sk_error_queue.lock,\n\t\t\taf_elock_keys + sk->sk_family,\n\t\t\taf_family_elock_key_strings[sk->sk_family]);\n\tlockdep_set_class_and_name(&sk->sk_callback_lock,\n\t\t\taf_callback_keys + sk->sk_family,\n\t\t\taf_family_clock_key_strings[sk->sk_family]);\n}\n\n \nstruct sock *sk_clone_lock(const struct sock *sk, const gfp_t priority)\n{\n\tstruct proto *prot = READ_ONCE(sk->sk_prot);\n\tstruct sk_filter *filter;\n\tbool is_charged = true;\n\tstruct sock *newsk;\n\n\tnewsk = sk_prot_alloc(prot, priority, sk->sk_family);\n\tif (!newsk)\n\t\tgoto out;\n\n\tsock_copy(newsk, sk);\n\n\tnewsk->sk_prot_creator = prot;\n\n\t \n\tif (likely(newsk->sk_net_refcnt)) {\n\t\tget_net_track(sock_net(newsk), &newsk->ns_tracker, priority);\n\t\tsock_inuse_add(sock_net(newsk), 1);\n\t} else {\n\t\t \n\t\t__netns_tracker_alloc(sock_net(newsk), &newsk->ns_tracker,\n\t\t\t\t      false, priority);\n\t}\n\tsk_node_init(&newsk->sk_node);\n\tsock_lock_init(newsk);\n\tbh_lock_sock(newsk);\n\tnewsk->sk_backlog.head\t= newsk->sk_backlog.tail = NULL;\n\tnewsk->sk_backlog.len = 0;\n\n\tatomic_set(&newsk->sk_rmem_alloc, 0);\n\n\t \n\trefcount_set(&newsk->sk_wmem_alloc, 1);\n\n\tatomic_set(&newsk->sk_omem_alloc, 0);\n\tsk_init_common(newsk);\n\n\tnewsk->sk_dst_cache\t= NULL;\n\tnewsk->sk_dst_pending_confirm = 0;\n\tnewsk->sk_wmem_queued\t= 0;\n\tnewsk->sk_forward_alloc = 0;\n\tnewsk->sk_reserved_mem  = 0;\n\tatomic_set(&newsk->sk_drops, 0);\n\tnewsk->sk_send_head\t= NULL;\n\tnewsk->sk_userlocks\t= sk->sk_userlocks & ~SOCK_BINDPORT_LOCK;\n\tatomic_set(&newsk->sk_zckey, 0);\n\n\tsock_reset_flag(newsk, SOCK_DONE);\n\n\t \n\tnewsk->sk_memcg = NULL;\n\n\tcgroup_sk_clone(&newsk->sk_cgrp_data);\n\n\trcu_read_lock();\n\tfilter = rcu_dereference(sk->sk_filter);\n\tif (filter != NULL)\n\t\t \n\t\tis_charged = sk_filter_charge(newsk, filter);\n\tRCU_INIT_POINTER(newsk->sk_filter, filter);\n\trcu_read_unlock();\n\n\tif (unlikely(!is_charged || xfrm_sk_clone_policy(newsk, sk))) {\n\t\t \n\t\tif (!is_charged)\n\t\t\tRCU_INIT_POINTER(newsk->sk_filter, NULL);\n\t\tsk_free_unlock_clone(newsk);\n\t\tnewsk = NULL;\n\t\tgoto out;\n\t}\n\tRCU_INIT_POINTER(newsk->sk_reuseport_cb, NULL);\n\n\tif (bpf_sk_storage_clone(sk, newsk)) {\n\t\tsk_free_unlock_clone(newsk);\n\t\tnewsk = NULL;\n\t\tgoto out;\n\t}\n\n\t \n\tif (sk_user_data_is_nocopy(newsk))\n\t\tnewsk->sk_user_data = NULL;\n\n\tnewsk->sk_err\t   = 0;\n\tnewsk->sk_err_soft = 0;\n\tnewsk->sk_priority = 0;\n\tnewsk->sk_incoming_cpu = raw_smp_processor_id();\n\n\t \n\tsmp_wmb();\n\trefcount_set(&newsk->sk_refcnt, 2);\n\n\tsk_set_socket(newsk, NULL);\n\tsk_tx_queue_clear(newsk);\n\tRCU_INIT_POINTER(newsk->sk_wq, NULL);\n\n\tif (newsk->sk_prot->sockets_allocated)\n\t\tsk_sockets_allocated_inc(newsk);\n\n\tif (sock_needs_netstamp(sk) && newsk->sk_flags & SK_FLAGS_TIMESTAMP)\n\t\tnet_enable_timestamp();\nout:\n\treturn newsk;\n}\nEXPORT_SYMBOL_GPL(sk_clone_lock);\n\nvoid sk_free_unlock_clone(struct sock *sk)\n{\n\t \n\tsk->sk_destruct = NULL;\n\tbh_unlock_sock(sk);\n\tsk_free(sk);\n}\nEXPORT_SYMBOL_GPL(sk_free_unlock_clone);\n\nstatic u32 sk_dst_gso_max_size(struct sock *sk, struct dst_entry *dst)\n{\n\tbool is_ipv6 = false;\n\tu32 max_size;\n\n#if IS_ENABLED(CONFIG_IPV6)\n\tis_ipv6 = (sk->sk_family == AF_INET6 &&\n\t\t   !ipv6_addr_v4mapped(&sk->sk_v6_rcv_saddr));\n#endif\n\t \n\tmax_size = is_ipv6 ? READ_ONCE(dst->dev->gso_max_size) :\n\t\t\tREAD_ONCE(dst->dev->gso_ipv4_max_size);\n\tif (max_size > GSO_LEGACY_MAX_SIZE && !sk_is_tcp(sk))\n\t\tmax_size = GSO_LEGACY_MAX_SIZE;\n\n\treturn max_size - (MAX_TCP_HEADER + 1);\n}\n\nvoid sk_setup_caps(struct sock *sk, struct dst_entry *dst)\n{\n\tu32 max_segs = 1;\n\n\tsk->sk_route_caps = dst->dev->features;\n\tif (sk_is_tcp(sk))\n\t\tsk->sk_route_caps |= NETIF_F_GSO;\n\tif (sk->sk_route_caps & NETIF_F_GSO)\n\t\tsk->sk_route_caps |= NETIF_F_GSO_SOFTWARE;\n\tif (unlikely(sk->sk_gso_disabled))\n\t\tsk->sk_route_caps &= ~NETIF_F_GSO_MASK;\n\tif (sk_can_gso(sk)) {\n\t\tif (dst->header_len && !xfrm_dst_offload_ok(dst)) {\n\t\t\tsk->sk_route_caps &= ~NETIF_F_GSO_MASK;\n\t\t} else {\n\t\t\tsk->sk_route_caps |= NETIF_F_SG | NETIF_F_HW_CSUM;\n\t\t\tsk->sk_gso_max_size = sk_dst_gso_max_size(sk, dst);\n\t\t\t \n\t\t\tmax_segs = max_t(u32, READ_ONCE(dst->dev->gso_max_segs), 1);\n\t\t}\n\t}\n\tsk->sk_gso_max_segs = max_segs;\n\tsk_dst_set(sk, dst);\n}\nEXPORT_SYMBOL_GPL(sk_setup_caps);\n\n \n\n\n \nvoid sock_wfree(struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\tunsigned int len = skb->truesize;\n\tbool free;\n\n\tif (!sock_flag(sk, SOCK_USE_WRITE_QUEUE)) {\n\t\tif (sock_flag(sk, SOCK_RCU_FREE) &&\n\t\t    sk->sk_write_space == sock_def_write_space) {\n\t\t\trcu_read_lock();\n\t\t\tfree = refcount_sub_and_test(len, &sk->sk_wmem_alloc);\n\t\t\tsock_def_write_space_wfree(sk);\n\t\t\trcu_read_unlock();\n\t\t\tif (unlikely(free))\n\t\t\t\t__sk_free(sk);\n\t\t\treturn;\n\t\t}\n\n\t\t \n\t\tWARN_ON(refcount_sub_and_test(len - 1, &sk->sk_wmem_alloc));\n\t\tsk->sk_write_space(sk);\n\t\tlen = 1;\n\t}\n\t \n\tif (refcount_sub_and_test(len, &sk->sk_wmem_alloc))\n\t\t__sk_free(sk);\n}\nEXPORT_SYMBOL(sock_wfree);\n\n \nvoid __sock_wfree(struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\n\tif (refcount_sub_and_test(skb->truesize, &sk->sk_wmem_alloc))\n\t\t__sk_free(sk);\n}\n\nvoid skb_set_owner_w(struct sk_buff *skb, struct sock *sk)\n{\n\tskb_orphan(skb);\n\tskb->sk = sk;\n#ifdef CONFIG_INET\n\tif (unlikely(!sk_fullsock(sk))) {\n\t\tskb->destructor = sock_edemux;\n\t\tsock_hold(sk);\n\t\treturn;\n\t}\n#endif\n\tskb->destructor = sock_wfree;\n\tskb_set_hash_from_sk(skb, sk);\n\t \n\trefcount_add(skb->truesize, &sk->sk_wmem_alloc);\n}\nEXPORT_SYMBOL(skb_set_owner_w);\n\nstatic bool can_skb_orphan_partial(const struct sk_buff *skb)\n{\n#ifdef CONFIG_TLS_DEVICE\n\t \n\tif (skb->decrypted)\n\t\treturn false;\n#endif\n\treturn (skb->destructor == sock_wfree ||\n\t\t(IS_ENABLED(CONFIG_INET) && skb->destructor == tcp_wfree));\n}\n\n \nvoid skb_orphan_partial(struct sk_buff *skb)\n{\n\tif (skb_is_tcp_pure_ack(skb))\n\t\treturn;\n\n\tif (can_skb_orphan_partial(skb) && skb_set_owner_sk_safe(skb, skb->sk))\n\t\treturn;\n\n\tskb_orphan(skb);\n}\nEXPORT_SYMBOL(skb_orphan_partial);\n\n \nvoid sock_rfree(struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\tunsigned int len = skb->truesize;\n\n\tatomic_sub(len, &sk->sk_rmem_alloc);\n\tsk_mem_uncharge(sk, len);\n}\nEXPORT_SYMBOL(sock_rfree);\n\n \nvoid sock_efree(struct sk_buff *skb)\n{\n\tsock_put(skb->sk);\n}\nEXPORT_SYMBOL(sock_efree);\n\n \n#ifdef CONFIG_INET\nvoid sock_pfree(struct sk_buff *skb)\n{\n\tif (sk_is_refcounted(skb->sk))\n\t\tsock_gen_put(skb->sk);\n}\nEXPORT_SYMBOL(sock_pfree);\n#endif  \n\nkuid_t sock_i_uid(struct sock *sk)\n{\n\tkuid_t uid;\n\n\tread_lock_bh(&sk->sk_callback_lock);\n\tuid = sk->sk_socket ? SOCK_INODE(sk->sk_socket)->i_uid : GLOBAL_ROOT_UID;\n\tread_unlock_bh(&sk->sk_callback_lock);\n\treturn uid;\n}\nEXPORT_SYMBOL(sock_i_uid);\n\nunsigned long __sock_i_ino(struct sock *sk)\n{\n\tunsigned long ino;\n\n\tread_lock(&sk->sk_callback_lock);\n\tino = sk->sk_socket ? SOCK_INODE(sk->sk_socket)->i_ino : 0;\n\tread_unlock(&sk->sk_callback_lock);\n\treturn ino;\n}\nEXPORT_SYMBOL(__sock_i_ino);\n\nunsigned long sock_i_ino(struct sock *sk)\n{\n\tunsigned long ino;\n\n\tlocal_bh_disable();\n\tino = __sock_i_ino(sk);\n\tlocal_bh_enable();\n\treturn ino;\n}\nEXPORT_SYMBOL(sock_i_ino);\n\n \nstruct sk_buff *sock_wmalloc(struct sock *sk, unsigned long size, int force,\n\t\t\t     gfp_t priority)\n{\n\tif (force ||\n\t    refcount_read(&sk->sk_wmem_alloc) < READ_ONCE(sk->sk_sndbuf)) {\n\t\tstruct sk_buff *skb = alloc_skb(size, priority);\n\n\t\tif (skb) {\n\t\t\tskb_set_owner_w(skb, sk);\n\t\t\treturn skb;\n\t\t}\n\t}\n\treturn NULL;\n}\nEXPORT_SYMBOL(sock_wmalloc);\n\nstatic void sock_ofree(struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\n\tatomic_sub(skb->truesize, &sk->sk_omem_alloc);\n}\n\nstruct sk_buff *sock_omalloc(struct sock *sk, unsigned long size,\n\t\t\t     gfp_t priority)\n{\n\tstruct sk_buff *skb;\n\n\t \n\tif (atomic_read(&sk->sk_omem_alloc) + SKB_TRUESIZE(size) >\n\t    READ_ONCE(sysctl_optmem_max))\n\t\treturn NULL;\n\n\tskb = alloc_skb(size, priority);\n\tif (!skb)\n\t\treturn NULL;\n\n\tatomic_add(skb->truesize, &sk->sk_omem_alloc);\n\tskb->sk = sk;\n\tskb->destructor = sock_ofree;\n\treturn skb;\n}\n\n \nvoid *sock_kmalloc(struct sock *sk, int size, gfp_t priority)\n{\n\tint optmem_max = READ_ONCE(sysctl_optmem_max);\n\n\tif ((unsigned int)size <= optmem_max &&\n\t    atomic_read(&sk->sk_omem_alloc) + size < optmem_max) {\n\t\tvoid *mem;\n\t\t \n\t\tatomic_add(size, &sk->sk_omem_alloc);\n\t\tmem = kmalloc(size, priority);\n\t\tif (mem)\n\t\t\treturn mem;\n\t\tatomic_sub(size, &sk->sk_omem_alloc);\n\t}\n\treturn NULL;\n}\nEXPORT_SYMBOL(sock_kmalloc);\n\n \nstatic inline void __sock_kfree_s(struct sock *sk, void *mem, int size,\n\t\t\t\t  const bool nullify)\n{\n\tif (WARN_ON_ONCE(!mem))\n\t\treturn;\n\tif (nullify)\n\t\tkfree_sensitive(mem);\n\telse\n\t\tkfree(mem);\n\tatomic_sub(size, &sk->sk_omem_alloc);\n}\n\nvoid sock_kfree_s(struct sock *sk, void *mem, int size)\n{\n\t__sock_kfree_s(sk, mem, size, false);\n}\nEXPORT_SYMBOL(sock_kfree_s);\n\nvoid sock_kzfree_s(struct sock *sk, void *mem, int size)\n{\n\t__sock_kfree_s(sk, mem, size, true);\n}\nEXPORT_SYMBOL(sock_kzfree_s);\n\n \nstatic long sock_wait_for_wmem(struct sock *sk, long timeo)\n{\n\tDEFINE_WAIT(wait);\n\n\tsk_clear_bit(SOCKWQ_ASYNC_NOSPACE, sk);\n\tfor (;;) {\n\t\tif (!timeo)\n\t\t\tbreak;\n\t\tif (signal_pending(current))\n\t\t\tbreak;\n\t\tset_bit(SOCK_NOSPACE, &sk->sk_socket->flags);\n\t\tprepare_to_wait(sk_sleep(sk), &wait, TASK_INTERRUPTIBLE);\n\t\tif (refcount_read(&sk->sk_wmem_alloc) < READ_ONCE(sk->sk_sndbuf))\n\t\t\tbreak;\n\t\tif (READ_ONCE(sk->sk_shutdown) & SEND_SHUTDOWN)\n\t\t\tbreak;\n\t\tif (READ_ONCE(sk->sk_err))\n\t\t\tbreak;\n\t\ttimeo = schedule_timeout(timeo);\n\t}\n\tfinish_wait(sk_sleep(sk), &wait);\n\treturn timeo;\n}\n\n\n \n\nstruct sk_buff *sock_alloc_send_pskb(struct sock *sk, unsigned long header_len,\n\t\t\t\t     unsigned long data_len, int noblock,\n\t\t\t\t     int *errcode, int max_page_order)\n{\n\tstruct sk_buff *skb;\n\tlong timeo;\n\tint err;\n\n\ttimeo = sock_sndtimeo(sk, noblock);\n\tfor (;;) {\n\t\terr = sock_error(sk);\n\t\tif (err != 0)\n\t\t\tgoto failure;\n\n\t\terr = -EPIPE;\n\t\tif (READ_ONCE(sk->sk_shutdown) & SEND_SHUTDOWN)\n\t\t\tgoto failure;\n\n\t\tif (sk_wmem_alloc_get(sk) < READ_ONCE(sk->sk_sndbuf))\n\t\t\tbreak;\n\n\t\tsk_set_bit(SOCKWQ_ASYNC_NOSPACE, sk);\n\t\tset_bit(SOCK_NOSPACE, &sk->sk_socket->flags);\n\t\terr = -EAGAIN;\n\t\tif (!timeo)\n\t\t\tgoto failure;\n\t\tif (signal_pending(current))\n\t\t\tgoto interrupted;\n\t\ttimeo = sock_wait_for_wmem(sk, timeo);\n\t}\n\tskb = alloc_skb_with_frags(header_len, data_len, max_page_order,\n\t\t\t\t   errcode, sk->sk_allocation);\n\tif (skb)\n\t\tskb_set_owner_w(skb, sk);\n\treturn skb;\n\ninterrupted:\n\terr = sock_intr_errno(timeo);\nfailure:\n\t*errcode = err;\n\treturn NULL;\n}\nEXPORT_SYMBOL(sock_alloc_send_pskb);\n\nint __sock_cmsg_send(struct sock *sk, struct cmsghdr *cmsg,\n\t\t     struct sockcm_cookie *sockc)\n{\n\tu32 tsflags;\n\n\tswitch (cmsg->cmsg_type) {\n\tcase SO_MARK:\n\t\tif (!ns_capable(sock_net(sk)->user_ns, CAP_NET_RAW) &&\n\t\t    !ns_capable(sock_net(sk)->user_ns, CAP_NET_ADMIN))\n\t\t\treturn -EPERM;\n\t\tif (cmsg->cmsg_len != CMSG_LEN(sizeof(u32)))\n\t\t\treturn -EINVAL;\n\t\tsockc->mark = *(u32 *)CMSG_DATA(cmsg);\n\t\tbreak;\n\tcase SO_TIMESTAMPING_OLD:\n\tcase SO_TIMESTAMPING_NEW:\n\t\tif (cmsg->cmsg_len != CMSG_LEN(sizeof(u32)))\n\t\t\treturn -EINVAL;\n\n\t\ttsflags = *(u32 *)CMSG_DATA(cmsg);\n\t\tif (tsflags & ~SOF_TIMESTAMPING_TX_RECORD_MASK)\n\t\t\treturn -EINVAL;\n\n\t\tsockc->tsflags &= ~SOF_TIMESTAMPING_TX_RECORD_MASK;\n\t\tsockc->tsflags |= tsflags;\n\t\tbreak;\n\tcase SCM_TXTIME:\n\t\tif (!sock_flag(sk, SOCK_TXTIME))\n\t\t\treturn -EINVAL;\n\t\tif (cmsg->cmsg_len != CMSG_LEN(sizeof(u64)))\n\t\t\treturn -EINVAL;\n\t\tsockc->transmit_time = get_unaligned((u64 *)CMSG_DATA(cmsg));\n\t\tbreak;\n\t \n\tcase SCM_RIGHTS:\n\tcase SCM_CREDENTIALS:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(__sock_cmsg_send);\n\nint sock_cmsg_send(struct sock *sk, struct msghdr *msg,\n\t\t   struct sockcm_cookie *sockc)\n{\n\tstruct cmsghdr *cmsg;\n\tint ret;\n\n\tfor_each_cmsghdr(cmsg, msg) {\n\t\tif (!CMSG_OK(msg, cmsg))\n\t\t\treturn -EINVAL;\n\t\tif (cmsg->cmsg_level != SOL_SOCKET)\n\t\t\tcontinue;\n\t\tret = __sock_cmsg_send(sk, cmsg, sockc);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(sock_cmsg_send);\n\nstatic void sk_enter_memory_pressure(struct sock *sk)\n{\n\tif (!sk->sk_prot->enter_memory_pressure)\n\t\treturn;\n\n\tsk->sk_prot->enter_memory_pressure(sk);\n}\n\nstatic void sk_leave_memory_pressure(struct sock *sk)\n{\n\tif (sk->sk_prot->leave_memory_pressure) {\n\t\tINDIRECT_CALL_INET_1(sk->sk_prot->leave_memory_pressure,\n\t\t\t\t     tcp_leave_memory_pressure, sk);\n\t} else {\n\t\tunsigned long *memory_pressure = sk->sk_prot->memory_pressure;\n\n\t\tif (memory_pressure && READ_ONCE(*memory_pressure))\n\t\t\tWRITE_ONCE(*memory_pressure, 0);\n\t}\n}\n\nDEFINE_STATIC_KEY_FALSE(net_high_order_alloc_disable_key);\n\n \nbool skb_page_frag_refill(unsigned int sz, struct page_frag *pfrag, gfp_t gfp)\n{\n\tif (pfrag->page) {\n\t\tif (page_ref_count(pfrag->page) == 1) {\n\t\t\tpfrag->offset = 0;\n\t\t\treturn true;\n\t\t}\n\t\tif (pfrag->offset + sz <= pfrag->size)\n\t\t\treturn true;\n\t\tput_page(pfrag->page);\n\t}\n\n\tpfrag->offset = 0;\n\tif (SKB_FRAG_PAGE_ORDER &&\n\t    !static_branch_unlikely(&net_high_order_alloc_disable_key)) {\n\t\t \n\t\tpfrag->page = alloc_pages((gfp & ~__GFP_DIRECT_RECLAIM) |\n\t\t\t\t\t  __GFP_COMP | __GFP_NOWARN |\n\t\t\t\t\t  __GFP_NORETRY,\n\t\t\t\t\t  SKB_FRAG_PAGE_ORDER);\n\t\tif (likely(pfrag->page)) {\n\t\t\tpfrag->size = PAGE_SIZE << SKB_FRAG_PAGE_ORDER;\n\t\t\treturn true;\n\t\t}\n\t}\n\tpfrag->page = alloc_page(gfp);\n\tif (likely(pfrag->page)) {\n\t\tpfrag->size = PAGE_SIZE;\n\t\treturn true;\n\t}\n\treturn false;\n}\nEXPORT_SYMBOL(skb_page_frag_refill);\n\nbool sk_page_frag_refill(struct sock *sk, struct page_frag *pfrag)\n{\n\tif (likely(skb_page_frag_refill(32U, pfrag, sk->sk_allocation)))\n\t\treturn true;\n\n\tsk_enter_memory_pressure(sk);\n\tsk_stream_moderate_sndbuf(sk);\n\treturn false;\n}\nEXPORT_SYMBOL(sk_page_frag_refill);\n\nvoid __lock_sock(struct sock *sk)\n\t__releases(&sk->sk_lock.slock)\n\t__acquires(&sk->sk_lock.slock)\n{\n\tDEFINE_WAIT(wait);\n\n\tfor (;;) {\n\t\tprepare_to_wait_exclusive(&sk->sk_lock.wq, &wait,\n\t\t\t\t\tTASK_UNINTERRUPTIBLE);\n\t\tspin_unlock_bh(&sk->sk_lock.slock);\n\t\tschedule();\n\t\tspin_lock_bh(&sk->sk_lock.slock);\n\t\tif (!sock_owned_by_user(sk))\n\t\t\tbreak;\n\t}\n\tfinish_wait(&sk->sk_lock.wq, &wait);\n}\n\nvoid __release_sock(struct sock *sk)\n\t__releases(&sk->sk_lock.slock)\n\t__acquires(&sk->sk_lock.slock)\n{\n\tstruct sk_buff *skb, *next;\n\n\twhile ((skb = sk->sk_backlog.head) != NULL) {\n\t\tsk->sk_backlog.head = sk->sk_backlog.tail = NULL;\n\n\t\tspin_unlock_bh(&sk->sk_lock.slock);\n\n\t\tdo {\n\t\t\tnext = skb->next;\n\t\t\tprefetch(next);\n\t\t\tDEBUG_NET_WARN_ON_ONCE(skb_dst_is_noref(skb));\n\t\t\tskb_mark_not_on_list(skb);\n\t\t\tsk_backlog_rcv(sk, skb);\n\n\t\t\tcond_resched();\n\n\t\t\tskb = next;\n\t\t} while (skb != NULL);\n\n\t\tspin_lock_bh(&sk->sk_lock.slock);\n\t}\n\n\t \n\tsk->sk_backlog.len = 0;\n}\n\nvoid __sk_flush_backlog(struct sock *sk)\n{\n\tspin_lock_bh(&sk->sk_lock.slock);\n\t__release_sock(sk);\n\tspin_unlock_bh(&sk->sk_lock.slock);\n}\nEXPORT_SYMBOL_GPL(__sk_flush_backlog);\n\n \nint sk_wait_data(struct sock *sk, long *timeo, const struct sk_buff *skb)\n{\n\tDEFINE_WAIT_FUNC(wait, woken_wake_function);\n\tint rc;\n\n\tadd_wait_queue(sk_sleep(sk), &wait);\n\tsk_set_bit(SOCKWQ_ASYNC_WAITDATA, sk);\n\trc = sk_wait_event(sk, timeo, skb_peek_tail(&sk->sk_receive_queue) != skb, &wait);\n\tsk_clear_bit(SOCKWQ_ASYNC_WAITDATA, sk);\n\tremove_wait_queue(sk_sleep(sk), &wait);\n\treturn rc;\n}\nEXPORT_SYMBOL(sk_wait_data);\n\n \nint __sk_mem_raise_allocated(struct sock *sk, int size, int amt, int kind)\n{\n\tbool memcg_charge = mem_cgroup_sockets_enabled && sk->sk_memcg;\n\tstruct proto *prot = sk->sk_prot;\n\tbool charged = true;\n\tlong allocated;\n\n\tsk_memory_allocated_add(sk, amt);\n\tallocated = sk_memory_allocated(sk);\n\tif (memcg_charge &&\n\t    !(charged = mem_cgroup_charge_skmem(sk->sk_memcg, amt,\n\t\t\t\t\t\tgfp_memcg_charge())))\n\t\tgoto suppress_allocation;\n\n\t \n\tif (allocated <= sk_prot_mem_limits(sk, 0)) {\n\t\tsk_leave_memory_pressure(sk);\n\t\treturn 1;\n\t}\n\n\t \n\tif (allocated > sk_prot_mem_limits(sk, 1))\n\t\tsk_enter_memory_pressure(sk);\n\n\t \n\tif (allocated > sk_prot_mem_limits(sk, 2))\n\t\tgoto suppress_allocation;\n\n\t \n\tif (kind == SK_MEM_RECV) {\n\t\tif (atomic_read(&sk->sk_rmem_alloc) < sk_get_rmem0(sk, prot))\n\t\t\treturn 1;\n\n\t} else {  \n\t\tint wmem0 = sk_get_wmem0(sk, prot);\n\n\t\tif (sk->sk_type == SOCK_STREAM) {\n\t\t\tif (sk->sk_wmem_queued < wmem0)\n\t\t\t\treturn 1;\n\t\t} else if (refcount_read(&sk->sk_wmem_alloc) < wmem0) {\n\t\t\t\treturn 1;\n\t\t}\n\t}\n\n\tif (sk_has_memory_pressure(sk)) {\n\t\tu64 alloc;\n\n\t\tif (!sk_under_memory_pressure(sk))\n\t\t\treturn 1;\n\t\talloc = sk_sockets_allocated_read_positive(sk);\n\t\tif (sk_prot_mem_limits(sk, 2) > alloc *\n\t\t    sk_mem_pages(sk->sk_wmem_queued +\n\t\t\t\t atomic_read(&sk->sk_rmem_alloc) +\n\t\t\t\t sk->sk_forward_alloc))\n\t\t\treturn 1;\n\t}\n\nsuppress_allocation:\n\n\tif (kind == SK_MEM_SEND && sk->sk_type == SOCK_STREAM) {\n\t\tsk_stream_moderate_sndbuf(sk);\n\n\t\t \n\t\tif (sk->sk_wmem_queued + size >= sk->sk_sndbuf) {\n\t\t\t \n\t\t\tif (memcg_charge && !charged) {\n\t\t\t\tmem_cgroup_charge_skmem(sk->sk_memcg, amt,\n\t\t\t\t\tgfp_memcg_charge() | __GFP_NOFAIL);\n\t\t\t}\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\tif (kind == SK_MEM_SEND || (kind == SK_MEM_RECV && charged))\n\t\ttrace_sock_exceed_buf_limit(sk, prot, allocated, kind);\n\n\tsk_memory_allocated_sub(sk, amt);\n\n\tif (memcg_charge && charged)\n\t\tmem_cgroup_uncharge_skmem(sk->sk_memcg, amt);\n\n\treturn 0;\n}\n\n \nint __sk_mem_schedule(struct sock *sk, int size, int kind)\n{\n\tint ret, amt = sk_mem_pages(size);\n\n\tsk_forward_alloc_add(sk, amt << PAGE_SHIFT);\n\tret = __sk_mem_raise_allocated(sk, size, amt, kind);\n\tif (!ret)\n\t\tsk_forward_alloc_add(sk, -(amt << PAGE_SHIFT));\n\treturn ret;\n}\nEXPORT_SYMBOL(__sk_mem_schedule);\n\n \nvoid __sk_mem_reduce_allocated(struct sock *sk, int amount)\n{\n\tsk_memory_allocated_sub(sk, amount);\n\n\tif (mem_cgroup_sockets_enabled && sk->sk_memcg)\n\t\tmem_cgroup_uncharge_skmem(sk->sk_memcg, amount);\n\n\tif (sk_under_global_memory_pressure(sk) &&\n\t    (sk_memory_allocated(sk) < sk_prot_mem_limits(sk, 0)))\n\t\tsk_leave_memory_pressure(sk);\n}\n\n \nvoid __sk_mem_reclaim(struct sock *sk, int amount)\n{\n\tamount >>= PAGE_SHIFT;\n\tsk_forward_alloc_add(sk, -(amount << PAGE_SHIFT));\n\t__sk_mem_reduce_allocated(sk, amount);\n}\nEXPORT_SYMBOL(__sk_mem_reclaim);\n\nint sk_set_peek_off(struct sock *sk, int val)\n{\n\tWRITE_ONCE(sk->sk_peek_off, val);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(sk_set_peek_off);\n\n \n\nint sock_no_bind(struct socket *sock, struct sockaddr *saddr, int len)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_bind);\n\nint sock_no_connect(struct socket *sock, struct sockaddr *saddr,\n\t\t    int len, int flags)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_connect);\n\nint sock_no_socketpair(struct socket *sock1, struct socket *sock2)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_socketpair);\n\nint sock_no_accept(struct socket *sock, struct socket *newsock, int flags,\n\t\t   bool kern)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_accept);\n\nint sock_no_getname(struct socket *sock, struct sockaddr *saddr,\n\t\t    int peer)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_getname);\n\nint sock_no_ioctl(struct socket *sock, unsigned int cmd, unsigned long arg)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_ioctl);\n\nint sock_no_listen(struct socket *sock, int backlog)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_listen);\n\nint sock_no_shutdown(struct socket *sock, int how)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_shutdown);\n\nint sock_no_sendmsg(struct socket *sock, struct msghdr *m, size_t len)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_sendmsg);\n\nint sock_no_sendmsg_locked(struct sock *sk, struct msghdr *m, size_t len)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_sendmsg_locked);\n\nint sock_no_recvmsg(struct socket *sock, struct msghdr *m, size_t len,\n\t\t    int flags)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_recvmsg);\n\nint sock_no_mmap(struct file *file, struct socket *sock, struct vm_area_struct *vma)\n{\n\t \n\treturn -ENODEV;\n}\nEXPORT_SYMBOL(sock_no_mmap);\n\n \nvoid __receive_sock(struct file *file)\n{\n\tstruct socket *sock;\n\n\tsock = sock_from_file(file);\n\tif (sock) {\n\t\tsock_update_netprioidx(&sock->sk->sk_cgrp_data);\n\t\tsock_update_classid(&sock->sk->sk_cgrp_data);\n\t}\n}\n\n \n\nstatic void sock_def_wakeup(struct sock *sk)\n{\n\tstruct socket_wq *wq;\n\n\trcu_read_lock();\n\twq = rcu_dereference(sk->sk_wq);\n\tif (skwq_has_sleeper(wq))\n\t\twake_up_interruptible_all(&wq->wait);\n\trcu_read_unlock();\n}\n\nstatic void sock_def_error_report(struct sock *sk)\n{\n\tstruct socket_wq *wq;\n\n\trcu_read_lock();\n\twq = rcu_dereference(sk->sk_wq);\n\tif (skwq_has_sleeper(wq))\n\t\twake_up_interruptible_poll(&wq->wait, EPOLLERR);\n\tsk_wake_async(sk, SOCK_WAKE_IO, POLL_ERR);\n\trcu_read_unlock();\n}\n\nvoid sock_def_readable(struct sock *sk)\n{\n\tstruct socket_wq *wq;\n\n\ttrace_sk_data_ready(sk);\n\n\trcu_read_lock();\n\twq = rcu_dereference(sk->sk_wq);\n\tif (skwq_has_sleeper(wq))\n\t\twake_up_interruptible_sync_poll(&wq->wait, EPOLLIN | EPOLLPRI |\n\t\t\t\t\t\tEPOLLRDNORM | EPOLLRDBAND);\n\tsk_wake_async(sk, SOCK_WAKE_WAITD, POLL_IN);\n\trcu_read_unlock();\n}\n\nstatic void sock_def_write_space(struct sock *sk)\n{\n\tstruct socket_wq *wq;\n\n\trcu_read_lock();\n\n\t \n\tif (sock_writeable(sk)) {\n\t\twq = rcu_dereference(sk->sk_wq);\n\t\tif (skwq_has_sleeper(wq))\n\t\t\twake_up_interruptible_sync_poll(&wq->wait, EPOLLOUT |\n\t\t\t\t\t\tEPOLLWRNORM | EPOLLWRBAND);\n\n\t\t \n\t\tsk_wake_async(sk, SOCK_WAKE_SPACE, POLL_OUT);\n\t}\n\n\trcu_read_unlock();\n}\n\n \nstatic void sock_def_write_space_wfree(struct sock *sk)\n{\n\t \n\tif (sock_writeable(sk)) {\n\t\tstruct socket_wq *wq = rcu_dereference(sk->sk_wq);\n\n\t\t \n\t\tsmp_mb__after_atomic();\n\t\tif (wq && waitqueue_active(&wq->wait))\n\t\t\twake_up_interruptible_sync_poll(&wq->wait, EPOLLOUT |\n\t\t\t\t\t\tEPOLLWRNORM | EPOLLWRBAND);\n\n\t\t \n\t\tsk_wake_async(sk, SOCK_WAKE_SPACE, POLL_OUT);\n\t}\n}\n\nstatic void sock_def_destruct(struct sock *sk)\n{\n}\n\nvoid sk_send_sigurg(struct sock *sk)\n{\n\tif (sk->sk_socket && sk->sk_socket->file)\n\t\tif (send_sigurg(&sk->sk_socket->file->f_owner))\n\t\t\tsk_wake_async(sk, SOCK_WAKE_URG, POLL_PRI);\n}\nEXPORT_SYMBOL(sk_send_sigurg);\n\nvoid sk_reset_timer(struct sock *sk, struct timer_list* timer,\n\t\t    unsigned long expires)\n{\n\tif (!mod_timer(timer, expires))\n\t\tsock_hold(sk);\n}\nEXPORT_SYMBOL(sk_reset_timer);\n\nvoid sk_stop_timer(struct sock *sk, struct timer_list* timer)\n{\n\tif (del_timer(timer))\n\t\t__sock_put(sk);\n}\nEXPORT_SYMBOL(sk_stop_timer);\n\nvoid sk_stop_timer_sync(struct sock *sk, struct timer_list *timer)\n{\n\tif (del_timer_sync(timer))\n\t\t__sock_put(sk);\n}\nEXPORT_SYMBOL(sk_stop_timer_sync);\n\nvoid sock_init_data_uid(struct socket *sock, struct sock *sk, kuid_t uid)\n{\n\tsk_init_common(sk);\n\tsk->sk_send_head\t=\tNULL;\n\n\ttimer_setup(&sk->sk_timer, NULL, 0);\n\n\tsk->sk_allocation\t=\tGFP_KERNEL;\n\tsk->sk_rcvbuf\t\t=\tREAD_ONCE(sysctl_rmem_default);\n\tsk->sk_sndbuf\t\t=\tREAD_ONCE(sysctl_wmem_default);\n\tsk->sk_state\t\t=\tTCP_CLOSE;\n\tsk->sk_use_task_frag\t=\ttrue;\n\tsk_set_socket(sk, sock);\n\n\tsock_set_flag(sk, SOCK_ZAPPED);\n\n\tif (sock) {\n\t\tsk->sk_type\t=\tsock->type;\n\t\tRCU_INIT_POINTER(sk->sk_wq, &sock->wq);\n\t\tsock->sk\t=\tsk;\n\t} else {\n\t\tRCU_INIT_POINTER(sk->sk_wq, NULL);\n\t}\n\tsk->sk_uid\t=\tuid;\n\n\trwlock_init(&sk->sk_callback_lock);\n\tif (sk->sk_kern_sock)\n\t\tlockdep_set_class_and_name(\n\t\t\t&sk->sk_callback_lock,\n\t\t\taf_kern_callback_keys + sk->sk_family,\n\t\t\taf_family_kern_clock_key_strings[sk->sk_family]);\n\telse\n\t\tlockdep_set_class_and_name(\n\t\t\t&sk->sk_callback_lock,\n\t\t\taf_callback_keys + sk->sk_family,\n\t\t\taf_family_clock_key_strings[sk->sk_family]);\n\n\tsk->sk_state_change\t=\tsock_def_wakeup;\n\tsk->sk_data_ready\t=\tsock_def_readable;\n\tsk->sk_write_space\t=\tsock_def_write_space;\n\tsk->sk_error_report\t=\tsock_def_error_report;\n\tsk->sk_destruct\t\t=\tsock_def_destruct;\n\n\tsk->sk_frag.page\t=\tNULL;\n\tsk->sk_frag.offset\t=\t0;\n\tsk->sk_peek_off\t\t=\t-1;\n\n\tsk->sk_peer_pid \t=\tNULL;\n\tsk->sk_peer_cred\t=\tNULL;\n\tspin_lock_init(&sk->sk_peer_lock);\n\n\tsk->sk_write_pending\t=\t0;\n\tsk->sk_rcvlowat\t\t=\t1;\n\tsk->sk_rcvtimeo\t\t=\tMAX_SCHEDULE_TIMEOUT;\n\tsk->sk_sndtimeo\t\t=\tMAX_SCHEDULE_TIMEOUT;\n\n\tsk->sk_stamp = SK_DEFAULT_STAMP;\n#if BITS_PER_LONG==32\n\tseqlock_init(&sk->sk_stamp_seq);\n#endif\n\tatomic_set(&sk->sk_zckey, 0);\n\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tsk->sk_napi_id\t\t=\t0;\n\tsk->sk_ll_usec\t\t=\tREAD_ONCE(sysctl_net_busy_read);\n#endif\n\n\tsk->sk_max_pacing_rate = ~0UL;\n\tsk->sk_pacing_rate = ~0UL;\n\tWRITE_ONCE(sk->sk_pacing_shift, 10);\n\tsk->sk_incoming_cpu = -1;\n\n\tsk_rx_queue_clear(sk);\n\t \n\tsmp_wmb();\n\trefcount_set(&sk->sk_refcnt, 1);\n\tatomic_set(&sk->sk_drops, 0);\n}\nEXPORT_SYMBOL(sock_init_data_uid);\n\nvoid sock_init_data(struct socket *sock, struct sock *sk)\n{\n\tkuid_t uid = sock ?\n\t\tSOCK_INODE(sock)->i_uid :\n\t\tmake_kuid(sock_net(sk)->user_ns, 0);\n\n\tsock_init_data_uid(sock, sk, uid);\n}\nEXPORT_SYMBOL(sock_init_data);\n\nvoid lock_sock_nested(struct sock *sk, int subclass)\n{\n\t \n\tmutex_acquire(&sk->sk_lock.dep_map, subclass, 0, _RET_IP_);\n\n\tmight_sleep();\n\tspin_lock_bh(&sk->sk_lock.slock);\n\tif (sock_owned_by_user_nocheck(sk))\n\t\t__lock_sock(sk);\n\tsk->sk_lock.owned = 1;\n\tspin_unlock_bh(&sk->sk_lock.slock);\n}\nEXPORT_SYMBOL(lock_sock_nested);\n\nvoid release_sock(struct sock *sk)\n{\n\tspin_lock_bh(&sk->sk_lock.slock);\n\tif (sk->sk_backlog.tail)\n\t\t__release_sock(sk);\n\n\t \n\tif (sk->sk_prot->release_cb)\n\t\tsk->sk_prot->release_cb(sk);\n\n\tsock_release_ownership(sk);\n\tif (waitqueue_active(&sk->sk_lock.wq))\n\t\twake_up(&sk->sk_lock.wq);\n\tspin_unlock_bh(&sk->sk_lock.slock);\n}\nEXPORT_SYMBOL(release_sock);\n\nbool __lock_sock_fast(struct sock *sk) __acquires(&sk->sk_lock.slock)\n{\n\tmight_sleep();\n\tspin_lock_bh(&sk->sk_lock.slock);\n\n\tif (!sock_owned_by_user_nocheck(sk)) {\n\t\t \n\t\treturn false;\n\t}\n\n\t__lock_sock(sk);\n\tsk->sk_lock.owned = 1;\n\t__acquire(&sk->sk_lock.slock);\n\tspin_unlock_bh(&sk->sk_lock.slock);\n\treturn true;\n}\nEXPORT_SYMBOL(__lock_sock_fast);\n\nint sock_gettstamp(struct socket *sock, void __user *userstamp,\n\t\t   bool timeval, bool time32)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct timespec64 ts;\n\n\tsock_enable_timestamp(sk, SOCK_TIMESTAMP);\n\tts = ktime_to_timespec64(sock_read_timestamp(sk));\n\tif (ts.tv_sec == -1)\n\t\treturn -ENOENT;\n\tif (ts.tv_sec == 0) {\n\t\tktime_t kt = ktime_get_real();\n\t\tsock_write_timestamp(sk, kt);\n\t\tts = ktime_to_timespec64(kt);\n\t}\n\n\tif (timeval)\n\t\tts.tv_nsec /= 1000;\n\n#ifdef CONFIG_COMPAT_32BIT_TIME\n\tif (time32)\n\t\treturn put_old_timespec32(&ts, userstamp);\n#endif\n#ifdef CONFIG_SPARC64\n\t \n\tif (timeval && !in_compat_syscall()) {\n\t\tstruct __kernel_old_timeval __user tv = {\n\t\t\t.tv_sec = ts.tv_sec,\n\t\t\t.tv_usec = ts.tv_nsec,\n\t\t};\n\t\tif (copy_to_user(userstamp, &tv, sizeof(tv)))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\t}\n#endif\n\treturn put_timespec64(&ts, userstamp);\n}\nEXPORT_SYMBOL(sock_gettstamp);\n\nvoid sock_enable_timestamp(struct sock *sk, enum sock_flags flag)\n{\n\tif (!sock_flag(sk, flag)) {\n\t\tunsigned long previous_flags = sk->sk_flags;\n\n\t\tsock_set_flag(sk, flag);\n\t\t \n\t\tif (sock_needs_netstamp(sk) &&\n\t\t    !(previous_flags & SK_FLAGS_TIMESTAMP))\n\t\t\tnet_enable_timestamp();\n\t}\n}\n\nint sock_recv_errqueue(struct sock *sk, struct msghdr *msg, int len,\n\t\t       int level, int type)\n{\n\tstruct sock_exterr_skb *serr;\n\tstruct sk_buff *skb;\n\tint copied, err;\n\n\terr = -EAGAIN;\n\tskb = sock_dequeue_err_skb(sk);\n\tif (skb == NULL)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (copied > len) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\terr = skb_copy_datagram_msg(skb, 0, msg, copied);\n\tif (err)\n\t\tgoto out_free_skb;\n\n\tsock_recv_timestamp(msg, sk, skb);\n\n\tserr = SKB_EXT_ERR(skb);\n\tput_cmsg(msg, level, type, sizeof(serr->ee), &serr->ee);\n\n\tmsg->msg_flags |= MSG_ERRQUEUE;\n\terr = copied;\n\nout_free_skb:\n\tkfree_skb(skb);\nout:\n\treturn err;\n}\nEXPORT_SYMBOL(sock_recv_errqueue);\n\n \nint sock_common_getsockopt(struct socket *sock, int level, int optname,\n\t\t\t   char __user *optval, int __user *optlen)\n{\n\tstruct sock *sk = sock->sk;\n\n\t \n\treturn READ_ONCE(sk->sk_prot)->getsockopt(sk, level, optname, optval, optlen);\n}\nEXPORT_SYMBOL(sock_common_getsockopt);\n\nint sock_common_recvmsg(struct socket *sock, struct msghdr *msg, size_t size,\n\t\t\tint flags)\n{\n\tstruct sock *sk = sock->sk;\n\tint addr_len = 0;\n\tint err;\n\n\terr = sk->sk_prot->recvmsg(sk, msg, size, flags, &addr_len);\n\tif (err >= 0)\n\t\tmsg->msg_namelen = addr_len;\n\treturn err;\n}\nEXPORT_SYMBOL(sock_common_recvmsg);\n\n \nint sock_common_setsockopt(struct socket *sock, int level, int optname,\n\t\t\t   sockptr_t optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\n\t \n\treturn READ_ONCE(sk->sk_prot)->setsockopt(sk, level, optname, optval, optlen);\n}\nEXPORT_SYMBOL(sock_common_setsockopt);\n\nvoid sk_common_release(struct sock *sk)\n{\n\tif (sk->sk_prot->destroy)\n\t\tsk->sk_prot->destroy(sk);\n\n\t \n\n\tsk->sk_prot->unhash(sk);\n\n\t \n\n\tsock_orphan(sk);\n\n\txfrm_sk_free_policy(sk);\n\n\tsock_put(sk);\n}\nEXPORT_SYMBOL(sk_common_release);\n\nvoid sk_get_meminfo(const struct sock *sk, u32 *mem)\n{\n\tmemset(mem, 0, sizeof(*mem) * SK_MEMINFO_VARS);\n\n\tmem[SK_MEMINFO_RMEM_ALLOC] = sk_rmem_alloc_get(sk);\n\tmem[SK_MEMINFO_RCVBUF] = READ_ONCE(sk->sk_rcvbuf);\n\tmem[SK_MEMINFO_WMEM_ALLOC] = sk_wmem_alloc_get(sk);\n\tmem[SK_MEMINFO_SNDBUF] = READ_ONCE(sk->sk_sndbuf);\n\tmem[SK_MEMINFO_FWD_ALLOC] = sk_forward_alloc_get(sk);\n\tmem[SK_MEMINFO_WMEM_QUEUED] = READ_ONCE(sk->sk_wmem_queued);\n\tmem[SK_MEMINFO_OPTMEM] = atomic_read(&sk->sk_omem_alloc);\n\tmem[SK_MEMINFO_BACKLOG] = READ_ONCE(sk->sk_backlog.len);\n\tmem[SK_MEMINFO_DROPS] = atomic_read(&sk->sk_drops);\n}\n\n#ifdef CONFIG_PROC_FS\nstatic DECLARE_BITMAP(proto_inuse_idx, PROTO_INUSE_NR);\n\nint sock_prot_inuse_get(struct net *net, struct proto *prot)\n{\n\tint cpu, idx = prot->inuse_idx;\n\tint res = 0;\n\n\tfor_each_possible_cpu(cpu)\n\t\tres += per_cpu_ptr(net->core.prot_inuse, cpu)->val[idx];\n\n\treturn res >= 0 ? res : 0;\n}\nEXPORT_SYMBOL_GPL(sock_prot_inuse_get);\n\nint sock_inuse_get(struct net *net)\n{\n\tint cpu, res = 0;\n\n\tfor_each_possible_cpu(cpu)\n\t\tres += per_cpu_ptr(net->core.prot_inuse, cpu)->all;\n\n\treturn res;\n}\n\nEXPORT_SYMBOL_GPL(sock_inuse_get);\n\nstatic int __net_init sock_inuse_init_net(struct net *net)\n{\n\tnet->core.prot_inuse = alloc_percpu(struct prot_inuse);\n\tif (net->core.prot_inuse == NULL)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nstatic void __net_exit sock_inuse_exit_net(struct net *net)\n{\n\tfree_percpu(net->core.prot_inuse);\n}\n\nstatic struct pernet_operations net_inuse_ops = {\n\t.init = sock_inuse_init_net,\n\t.exit = sock_inuse_exit_net,\n};\n\nstatic __init int net_inuse_init(void)\n{\n\tif (register_pernet_subsys(&net_inuse_ops))\n\t\tpanic(\"Cannot initialize net inuse counters\");\n\n\treturn 0;\n}\n\ncore_initcall(net_inuse_init);\n\nstatic int assign_proto_idx(struct proto *prot)\n{\n\tprot->inuse_idx = find_first_zero_bit(proto_inuse_idx, PROTO_INUSE_NR);\n\n\tif (unlikely(prot->inuse_idx == PROTO_INUSE_NR - 1)) {\n\t\tpr_err(\"PROTO_INUSE_NR exhausted\\n\");\n\t\treturn -ENOSPC;\n\t}\n\n\tset_bit(prot->inuse_idx, proto_inuse_idx);\n\treturn 0;\n}\n\nstatic void release_proto_idx(struct proto *prot)\n{\n\tif (prot->inuse_idx != PROTO_INUSE_NR - 1)\n\t\tclear_bit(prot->inuse_idx, proto_inuse_idx);\n}\n#else\nstatic inline int assign_proto_idx(struct proto *prot)\n{\n\treturn 0;\n}\n\nstatic inline void release_proto_idx(struct proto *prot)\n{\n}\n\n#endif\n\nstatic void tw_prot_cleanup(struct timewait_sock_ops *twsk_prot)\n{\n\tif (!twsk_prot)\n\t\treturn;\n\tkfree(twsk_prot->twsk_slab_name);\n\ttwsk_prot->twsk_slab_name = NULL;\n\tkmem_cache_destroy(twsk_prot->twsk_slab);\n\ttwsk_prot->twsk_slab = NULL;\n}\n\nstatic int tw_prot_init(const struct proto *prot)\n{\n\tstruct timewait_sock_ops *twsk_prot = prot->twsk_prot;\n\n\tif (!twsk_prot)\n\t\treturn 0;\n\n\ttwsk_prot->twsk_slab_name = kasprintf(GFP_KERNEL, \"tw_sock_%s\",\n\t\t\t\t\t      prot->name);\n\tif (!twsk_prot->twsk_slab_name)\n\t\treturn -ENOMEM;\n\n\ttwsk_prot->twsk_slab =\n\t\tkmem_cache_create(twsk_prot->twsk_slab_name,\n\t\t\t\t  twsk_prot->twsk_obj_size, 0,\n\t\t\t\t  SLAB_ACCOUNT | prot->slab_flags,\n\t\t\t\t  NULL);\n\tif (!twsk_prot->twsk_slab) {\n\t\tpr_crit(\"%s: Can't create timewait sock SLAB cache!\\n\",\n\t\t\tprot->name);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic void req_prot_cleanup(struct request_sock_ops *rsk_prot)\n{\n\tif (!rsk_prot)\n\t\treturn;\n\tkfree(rsk_prot->slab_name);\n\trsk_prot->slab_name = NULL;\n\tkmem_cache_destroy(rsk_prot->slab);\n\trsk_prot->slab = NULL;\n}\n\nstatic int req_prot_init(const struct proto *prot)\n{\n\tstruct request_sock_ops *rsk_prot = prot->rsk_prot;\n\n\tif (!rsk_prot)\n\t\treturn 0;\n\n\trsk_prot->slab_name = kasprintf(GFP_KERNEL, \"request_sock_%s\",\n\t\t\t\t\tprot->name);\n\tif (!rsk_prot->slab_name)\n\t\treturn -ENOMEM;\n\n\trsk_prot->slab = kmem_cache_create(rsk_prot->slab_name,\n\t\t\t\t\t   rsk_prot->obj_size, 0,\n\t\t\t\t\t   SLAB_ACCOUNT | prot->slab_flags,\n\t\t\t\t\t   NULL);\n\n\tif (!rsk_prot->slab) {\n\t\tpr_crit(\"%s: Can't create request sock SLAB cache!\\n\",\n\t\t\tprot->name);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nint proto_register(struct proto *prot, int alloc_slab)\n{\n\tint ret = -ENOBUFS;\n\n\tif (prot->memory_allocated && !prot->sysctl_mem) {\n\t\tpr_err(\"%s: missing sysctl_mem\\n\", prot->name);\n\t\treturn -EINVAL;\n\t}\n\tif (prot->memory_allocated && !prot->per_cpu_fw_alloc) {\n\t\tpr_err(\"%s: missing per_cpu_fw_alloc\\n\", prot->name);\n\t\treturn -EINVAL;\n\t}\n\tif (alloc_slab) {\n\t\tprot->slab = kmem_cache_create_usercopy(prot->name,\n\t\t\t\t\tprot->obj_size, 0,\n\t\t\t\t\tSLAB_HWCACHE_ALIGN | SLAB_ACCOUNT |\n\t\t\t\t\tprot->slab_flags,\n\t\t\t\t\tprot->useroffset, prot->usersize,\n\t\t\t\t\tNULL);\n\n\t\tif (prot->slab == NULL) {\n\t\t\tpr_crit(\"%s: Can't create sock SLAB cache!\\n\",\n\t\t\t\tprot->name);\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (req_prot_init(prot))\n\t\t\tgoto out_free_request_sock_slab;\n\n\t\tif (tw_prot_init(prot))\n\t\t\tgoto out_free_timewait_sock_slab;\n\t}\n\n\tmutex_lock(&proto_list_mutex);\n\tret = assign_proto_idx(prot);\n\tif (ret) {\n\t\tmutex_unlock(&proto_list_mutex);\n\t\tgoto out_free_timewait_sock_slab;\n\t}\n\tlist_add(&prot->node, &proto_list);\n\tmutex_unlock(&proto_list_mutex);\n\treturn ret;\n\nout_free_timewait_sock_slab:\n\tif (alloc_slab)\n\t\ttw_prot_cleanup(prot->twsk_prot);\nout_free_request_sock_slab:\n\tif (alloc_slab) {\n\t\treq_prot_cleanup(prot->rsk_prot);\n\n\t\tkmem_cache_destroy(prot->slab);\n\t\tprot->slab = NULL;\n\t}\nout:\n\treturn ret;\n}\nEXPORT_SYMBOL(proto_register);\n\nvoid proto_unregister(struct proto *prot)\n{\n\tmutex_lock(&proto_list_mutex);\n\trelease_proto_idx(prot);\n\tlist_del(&prot->node);\n\tmutex_unlock(&proto_list_mutex);\n\n\tkmem_cache_destroy(prot->slab);\n\tprot->slab = NULL;\n\n\treq_prot_cleanup(prot->rsk_prot);\n\ttw_prot_cleanup(prot->twsk_prot);\n}\nEXPORT_SYMBOL(proto_unregister);\n\nint sock_load_diag_module(int family, int protocol)\n{\n\tif (!protocol) {\n\t\tif (!sock_is_registered(family))\n\t\t\treturn -ENOENT;\n\n\t\treturn request_module(\"net-pf-%d-proto-%d-type-%d\", PF_NETLINK,\n\t\t\t\t      NETLINK_SOCK_DIAG, family);\n\t}\n\n#ifdef CONFIG_INET\n\tif (family == AF_INET &&\n\t    protocol != IPPROTO_RAW &&\n\t    protocol < MAX_INET_PROTOS &&\n\t    !rcu_access_pointer(inet_protos[protocol]))\n\t\treturn -ENOENT;\n#endif\n\n\treturn request_module(\"net-pf-%d-proto-%d-type-%d-%d\", PF_NETLINK,\n\t\t\t      NETLINK_SOCK_DIAG, family, protocol);\n}\nEXPORT_SYMBOL(sock_load_diag_module);\n\n#ifdef CONFIG_PROC_FS\nstatic void *proto_seq_start(struct seq_file *seq, loff_t *pos)\n\t__acquires(proto_list_mutex)\n{\n\tmutex_lock(&proto_list_mutex);\n\treturn seq_list_start_head(&proto_list, *pos);\n}\n\nstatic void *proto_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\treturn seq_list_next(v, &proto_list, pos);\n}\n\nstatic void proto_seq_stop(struct seq_file *seq, void *v)\n\t__releases(proto_list_mutex)\n{\n\tmutex_unlock(&proto_list_mutex);\n}\n\nstatic char proto_method_implemented(const void *method)\n{\n\treturn method == NULL ? 'n' : 'y';\n}\nstatic long sock_prot_memory_allocated(struct proto *proto)\n{\n\treturn proto->memory_allocated != NULL ? proto_memory_allocated(proto) : -1L;\n}\n\nstatic const char *sock_prot_memory_pressure(struct proto *proto)\n{\n\treturn proto->memory_pressure != NULL ?\n\tproto_memory_pressure(proto) ? \"yes\" : \"no\" : \"NI\";\n}\n\nstatic void proto_seq_printf(struct seq_file *seq, struct proto *proto)\n{\n\n\tseq_printf(seq, \"%-9s %4u %6d  %6ld   %-3s %6u   %-3s  %-10s \"\n\t\t\t\"%2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c\\n\",\n\t\t   proto->name,\n\t\t   proto->obj_size,\n\t\t   sock_prot_inuse_get(seq_file_net(seq), proto),\n\t\t   sock_prot_memory_allocated(proto),\n\t\t   sock_prot_memory_pressure(proto),\n\t\t   proto->max_header,\n\t\t   proto->slab == NULL ? \"no\" : \"yes\",\n\t\t   module_name(proto->owner),\n\t\t   proto_method_implemented(proto->close),\n\t\t   proto_method_implemented(proto->connect),\n\t\t   proto_method_implemented(proto->disconnect),\n\t\t   proto_method_implemented(proto->accept),\n\t\t   proto_method_implemented(proto->ioctl),\n\t\t   proto_method_implemented(proto->init),\n\t\t   proto_method_implemented(proto->destroy),\n\t\t   proto_method_implemented(proto->shutdown),\n\t\t   proto_method_implemented(proto->setsockopt),\n\t\t   proto_method_implemented(proto->getsockopt),\n\t\t   proto_method_implemented(proto->sendmsg),\n\t\t   proto_method_implemented(proto->recvmsg),\n\t\t   proto_method_implemented(proto->bind),\n\t\t   proto_method_implemented(proto->backlog_rcv),\n\t\t   proto_method_implemented(proto->hash),\n\t\t   proto_method_implemented(proto->unhash),\n\t\t   proto_method_implemented(proto->get_port),\n\t\t   proto_method_implemented(proto->enter_memory_pressure));\n}\n\nstatic int proto_seq_show(struct seq_file *seq, void *v)\n{\n\tif (v == &proto_list)\n\t\tseq_printf(seq, \"%-9s %-4s %-8s %-6s %-5s %-7s %-4s %-10s %s\",\n\t\t\t   \"protocol\",\n\t\t\t   \"size\",\n\t\t\t   \"sockets\",\n\t\t\t   \"memory\",\n\t\t\t   \"press\",\n\t\t\t   \"maxhdr\",\n\t\t\t   \"slab\",\n\t\t\t   \"module\",\n\t\t\t   \"cl co di ac io in de sh ss gs se re bi br ha uh gp em\\n\");\n\telse\n\t\tproto_seq_printf(seq, list_entry(v, struct proto, node));\n\treturn 0;\n}\n\nstatic const struct seq_operations proto_seq_ops = {\n\t.start  = proto_seq_start,\n\t.next   = proto_seq_next,\n\t.stop   = proto_seq_stop,\n\t.show   = proto_seq_show,\n};\n\nstatic __net_init int proto_init_net(struct net *net)\n{\n\tif (!proc_create_net(\"protocols\", 0444, net->proc_net, &proto_seq_ops,\n\t\t\tsizeof(struct seq_net_private)))\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic __net_exit void proto_exit_net(struct net *net)\n{\n\tremove_proc_entry(\"protocols\", net->proc_net);\n}\n\n\nstatic __net_initdata struct pernet_operations proto_net_ops = {\n\t.init = proto_init_net,\n\t.exit = proto_exit_net,\n};\n\nstatic int __init proto_init(void)\n{\n\treturn register_pernet_subsys(&proto_net_ops);\n}\n\nsubsys_initcall(proto_init);\n\n#endif  \n\n#ifdef CONFIG_NET_RX_BUSY_POLL\nbool sk_busy_loop_end(void *p, unsigned long start_time)\n{\n\tstruct sock *sk = p;\n\n\treturn !skb_queue_empty_lockless(&sk->sk_receive_queue) ||\n\t       sk_busy_loop_timeout(sk, start_time);\n}\nEXPORT_SYMBOL(sk_busy_loop_end);\n#endif  \n\nint sock_bind_add(struct sock *sk, struct sockaddr *addr, int addr_len)\n{\n\tif (!sk->sk_prot->bind_add)\n\t\treturn -EOPNOTSUPP;\n\treturn sk->sk_prot->bind_add(sk, addr, addr_len);\n}\nEXPORT_SYMBOL(sock_bind_add);\n\n \nint sock_ioctl_inout(struct sock *sk, unsigned int cmd,\n\t\t     void __user *arg, void *karg, size_t size)\n{\n\tint ret;\n\n\tif (copy_from_user(karg, arg, size))\n\t\treturn -EFAULT;\n\n\tret = READ_ONCE(sk->sk_prot)->ioctl(sk, cmd, karg);\n\tif (ret)\n\t\treturn ret;\n\n\tif (copy_to_user(arg, karg, size))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\nEXPORT_SYMBOL(sock_ioctl_inout);\n\n \nstatic int sock_ioctl_out(struct sock *sk, unsigned int cmd, void __user *arg)\n{\n\tint ret, karg = 0;\n\n\tret = READ_ONCE(sk->sk_prot)->ioctl(sk, cmd, &karg);\n\tif (ret)\n\t\treturn ret;\n\n\treturn put_user(karg, (int __user *)arg);\n}\n\n \nint sk_ioctl(struct sock *sk, unsigned int cmd, void __user *arg)\n{\n\tint rc = 1;\n\n\tif (sk->sk_type == SOCK_RAW && sk->sk_family == AF_INET)\n\t\trc = ipmr_sk_ioctl(sk, cmd, arg);\n\telse if (sk->sk_type == SOCK_RAW && sk->sk_family == AF_INET6)\n\t\trc = ip6mr_sk_ioctl(sk, cmd, arg);\n\telse if (sk_is_phonet(sk))\n\t\trc = phonet_sk_ioctl(sk, cmd, arg);\n\n\t \n\tif (rc <= 0)\n\t\treturn rc;\n\n\t \n\treturn sock_ioctl_out(sk, cmd, arg);\n}\nEXPORT_SYMBOL(sk_ioctl);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}