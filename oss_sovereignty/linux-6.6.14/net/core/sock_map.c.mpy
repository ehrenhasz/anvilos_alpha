{
  "module_name": "sock_map.c",
  "hash_id": "88ff8b29389e1a6f8aff4518a09375fe812e35fe7b4632404a424578852ac741",
  "original_prompt": "Ingested from linux-6.6.14/net/core/sock_map.c",
  "human_readable_source": "\n \n\tret = sock_map_init_proto(sk, psock);\n\tif (ret < 0) {\n\t\tsk_psock_put(sk, psock);\n\t\tgoto out;\n\t}\n\n\twrite_lock_bh(&sk->sk_callback_lock);\n\tif (stream_parser && stream_verdict && !psock->saved_data_ready) {\n\t\tret = sk_psock_init_strp(sk, psock);\n\t\tif (ret) {\n\t\t\twrite_unlock_bh(&sk->sk_callback_lock);\n\t\t\tsk_psock_put(sk, psock);\n\t\t\tgoto out;\n\t\t}\n\t\tsk_psock_start_strp(sk, psock);\n\t} else if (!stream_parser && stream_verdict && !psock->saved_data_ready) {\n\t\tsk_psock_start_verdict(sk,psock);\n\t} else if (!stream_verdict && skb_verdict && !psock->saved_data_ready) {\n\t\tsk_psock_start_verdict(sk, psock);\n\t}\n\twrite_unlock_bh(&sk->sk_callback_lock);\n\treturn 0;\nout_progs:\n\tif (skb_verdict)\n\t\tbpf_prog_put(skb_verdict);\nout_put_msg_parser:\n\tif (msg_parser)\n\t\tbpf_prog_put(msg_parser);\nout_put_stream_parser:\n\tif (stream_parser)\n\t\tbpf_prog_put(stream_parser);\nout_put_stream_verdict:\n\tif (stream_verdict)\n\t\tbpf_prog_put(stream_verdict);\nout:\n\treturn ret;\n}\n\nstatic void sock_map_free(struct bpf_map *map)\n{\n\tstruct bpf_stab *stab = container_of(map, struct bpf_stab, map);\n\tint i;\n\n\t \n\tsynchronize_rcu();\n\tfor (i = 0; i < stab->map.max_entries; i++) {\n\t\tstruct sock **psk = &stab->sks[i];\n\t\tstruct sock *sk;\n\n\t\tsk = xchg(psk, NULL);\n\t\tif (sk) {\n\t\t\tsock_hold(sk);\n\t\t\tlock_sock(sk);\n\t\t\trcu_read_lock();\n\t\t\tsock_map_unref(sk, psk);\n\t\t\trcu_read_unlock();\n\t\t\trelease_sock(sk);\n\t\t\tsock_put(sk);\n\t\t}\n\t}\n\n\t \n\tsynchronize_rcu();\n\n\tbpf_map_area_free(stab->sks);\n\tbpf_map_area_free(stab);\n}\n\nstatic void sock_map_release_progs(struct bpf_map *map)\n{\n\tpsock_progs_drop(&container_of(map, struct bpf_stab, map)->progs);\n}\n\nstatic struct sock *__sock_map_lookup_elem(struct bpf_map *map, u32 key)\n{\n\tstruct bpf_stab *stab = container_of(map, struct bpf_stab, map);\n\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\n\tif (unlikely(key >= map->max_entries))\n\t\treturn NULL;\n\treturn READ_ONCE(stab->sks[key]);\n}\n\nstatic void *sock_map_lookup(struct bpf_map *map, void *key)\n{\n\tstruct sock *sk;\n\n\tsk = __sock_map_lookup_elem(map, *(u32 *)key);\n\tif (!sk)\n\t\treturn NULL;\n\tif (sk_is_refcounted(sk) && !refcount_inc_not_zero(&sk->sk_refcnt))\n\t\treturn NULL;\n\treturn sk;\n}\n\nstatic void *sock_map_lookup_sys(struct bpf_map *map, void *key)\n{\n\tstruct sock *sk;\n\n\tif (map->value_size != sizeof(u64))\n\t\treturn ERR_PTR(-ENOSPC);\n\n\tsk = __sock_map_lookup_elem(map, *(u32 *)key);\n\tif (!sk)\n\t\treturn ERR_PTR(-ENOENT);\n\n\t__sock_gen_cookie(sk);\n\treturn &sk->sk_cookie;\n}\n\nstatic int __sock_map_delete(struct bpf_stab *stab, struct sock *sk_test,\n\t\t\t     struct sock **psk)\n{\n\tstruct sock *sk;\n\tint err = 0;\n\n\tspin_lock_bh(&stab->lock);\n\tsk = *psk;\n\tif (!sk_test || sk_test == sk)\n\t\tsk = xchg(psk, NULL);\n\n\tif (likely(sk))\n\t\tsock_map_unref(sk, psk);\n\telse\n\t\terr = -EINVAL;\n\n\tspin_unlock_bh(&stab->lock);\n\treturn err;\n}\n\nstatic void sock_map_delete_from_link(struct bpf_map *map, struct sock *sk,\n\t\t\t\t      void *link_raw)\n{\n\tstruct bpf_stab *stab = container_of(map, struct bpf_stab, map);\n\n\t__sock_map_delete(stab, sk, link_raw);\n}\n\nstatic long sock_map_delete_elem(struct bpf_map *map, void *key)\n{\n\tstruct bpf_stab *stab = container_of(map, struct bpf_stab, map);\n\tu32 i = *(u32 *)key;\n\tstruct sock **psk;\n\n\tif (unlikely(i >= map->max_entries))\n\t\treturn -EINVAL;\n\n\tpsk = &stab->sks[i];\n\treturn __sock_map_delete(stab, NULL, psk);\n}\n\nstatic int sock_map_get_next_key(struct bpf_map *map, void *key, void *next)\n{\n\tstruct bpf_stab *stab = container_of(map, struct bpf_stab, map);\n\tu32 i = key ? *(u32 *)key : U32_MAX;\n\tu32 *key_next = next;\n\n\tif (i == stab->map.max_entries - 1)\n\t\treturn -ENOENT;\n\tif (i >= stab->map.max_entries)\n\t\t*key_next = 0;\n\telse\n\t\t*key_next = i + 1;\n\treturn 0;\n}\n\nstatic int sock_map_update_common(struct bpf_map *map, u32 idx,\n\t\t\t\t  struct sock *sk, u64 flags)\n{\n\tstruct bpf_stab *stab = container_of(map, struct bpf_stab, map);\n\tstruct sk_psock_link *link;\n\tstruct sk_psock *psock;\n\tstruct sock *osk;\n\tint ret;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\tif (unlikely(flags > BPF_EXIST))\n\t\treturn -EINVAL;\n\tif (unlikely(idx >= map->max_entries))\n\t\treturn -E2BIG;\n\n\tlink = sk_psock_init_link();\n\tif (!link)\n\t\treturn -ENOMEM;\n\n\tret = sock_map_link(map, sk);\n\tif (ret < 0)\n\t\tgoto out_free;\n\n\tpsock = sk_psock(sk);\n\tWARN_ON_ONCE(!psock);\n\n\tspin_lock_bh(&stab->lock);\n\tosk = stab->sks[idx];\n\tif (osk && flags == BPF_NOEXIST) {\n\t\tret = -EEXIST;\n\t\tgoto out_unlock;\n\t} else if (!osk && flags == BPF_EXIST) {\n\t\tret = -ENOENT;\n\t\tgoto out_unlock;\n\t}\n\n\tsock_map_add_link(psock, link, map, &stab->sks[idx]);\n\tstab->sks[idx] = sk;\n\tif (osk)\n\t\tsock_map_unref(osk, &stab->sks[idx]);\n\tspin_unlock_bh(&stab->lock);\n\treturn 0;\nout_unlock:\n\tspin_unlock_bh(&stab->lock);\n\tif (psock)\n\t\tsk_psock_put(sk, psock);\nout_free:\n\tsk_psock_free_link(link);\n\treturn ret;\n}\n\nstatic bool sock_map_op_okay(const struct bpf_sock_ops_kern *ops)\n{\n\treturn ops->op == BPF_SOCK_OPS_PASSIVE_ESTABLISHED_CB ||\n\t       ops->op == BPF_SOCK_OPS_ACTIVE_ESTABLISHED_CB ||\n\t       ops->op == BPF_SOCK_OPS_TCP_LISTEN_CB;\n}\n\nstatic bool sock_map_redirect_allowed(const struct sock *sk)\n{\n\tif (sk_is_tcp(sk))\n\t\treturn sk->sk_state != TCP_LISTEN;\n\telse\n\t\treturn sk->sk_state == TCP_ESTABLISHED;\n}\n\nstatic bool sock_map_sk_is_suitable(const struct sock *sk)\n{\n\treturn !!sk->sk_prot->psock_update_sk_prot;\n}\n\nstatic bool sock_map_sk_state_allowed(const struct sock *sk)\n{\n\tif (sk_is_tcp(sk))\n\t\treturn (1 << sk->sk_state) & (TCPF_ESTABLISHED | TCPF_LISTEN);\n\tif (sk_is_stream_unix(sk))\n\t\treturn (1 << sk->sk_state) & TCPF_ESTABLISHED;\n\treturn true;\n}\n\nstatic int sock_hash_update_common(struct bpf_map *map, void *key,\n\t\t\t\t   struct sock *sk, u64 flags);\n\nint sock_map_update_elem_sys(struct bpf_map *map, void *key, void *value,\n\t\t\t     u64 flags)\n{\n\tstruct socket *sock;\n\tstruct sock *sk;\n\tint ret;\n\tu64 ufd;\n\n\tif (map->value_size == sizeof(u64))\n\t\tufd = *(u64 *)value;\n\telse\n\t\tufd = *(u32 *)value;\n\tif (ufd > S32_MAX)\n\t\treturn -EINVAL;\n\n\tsock = sockfd_lookup(ufd, &ret);\n\tif (!sock)\n\t\treturn ret;\n\tsk = sock->sk;\n\tif (!sk) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\tif (!sock_map_sk_is_suitable(sk)) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\tsock_map_sk_acquire(sk);\n\tif (!sock_map_sk_state_allowed(sk))\n\t\tret = -EOPNOTSUPP;\n\telse if (map->map_type == BPF_MAP_TYPE_SOCKMAP)\n\t\tret = sock_map_update_common(map, *(u32 *)key, sk, flags);\n\telse\n\t\tret = sock_hash_update_common(map, key, sk, flags);\n\tsock_map_sk_release(sk);\nout:\n\tsockfd_put(sock);\n\treturn ret;\n}\n\nstatic long sock_map_update_elem(struct bpf_map *map, void *key,\n\t\t\t\t void *value, u64 flags)\n{\n\tstruct sock *sk = (struct sock *)value;\n\tint ret;\n\n\tif (unlikely(!sk || !sk_fullsock(sk)))\n\t\treturn -EINVAL;\n\n\tif (!sock_map_sk_is_suitable(sk))\n\t\treturn -EOPNOTSUPP;\n\n\tlocal_bh_disable();\n\tbh_lock_sock(sk);\n\tif (!sock_map_sk_state_allowed(sk))\n\t\tret = -EOPNOTSUPP;\n\telse if (map->map_type == BPF_MAP_TYPE_SOCKMAP)\n\t\tret = sock_map_update_common(map, *(u32 *)key, sk, flags);\n\telse\n\t\tret = sock_hash_update_common(map, key, sk, flags);\n\tbh_unlock_sock(sk);\n\tlocal_bh_enable();\n\treturn ret;\n}\n\nBPF_CALL_4(bpf_sock_map_update, struct bpf_sock_ops_kern *, sops,\n\t   struct bpf_map *, map, void *, key, u64, flags)\n{\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\n\tif (likely(sock_map_sk_is_suitable(sops->sk) &&\n\t\t   sock_map_op_okay(sops)))\n\t\treturn sock_map_update_common(map, *(u32 *)key, sops->sk,\n\t\t\t\t\t      flags);\n\treturn -EOPNOTSUPP;\n}\n\nconst struct bpf_func_proto bpf_sock_map_update_proto = {\n\t.func\t\t= bpf_sock_map_update,\n\t.gpl_only\t= false,\n\t.pkt_access\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_CONST_MAP_PTR,\n\t.arg3_type\t= ARG_PTR_TO_MAP_KEY,\n\t.arg4_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_4(bpf_sk_redirect_map, struct sk_buff *, skb,\n\t   struct bpf_map *, map, u32, key, u64, flags)\n{\n\tstruct sock *sk;\n\n\tif (unlikely(flags & ~(BPF_F_INGRESS)))\n\t\treturn SK_DROP;\n\n\tsk = __sock_map_lookup_elem(map, key);\n\tif (unlikely(!sk || !sock_map_redirect_allowed(sk)))\n\t\treturn SK_DROP;\n\n\tskb_bpf_set_redir(skb, sk, flags & BPF_F_INGRESS);\n\treturn SK_PASS;\n}\n\nconst struct bpf_func_proto bpf_sk_redirect_map_proto = {\n\t.func           = bpf_sk_redirect_map,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type      = ARG_CONST_MAP_PTR,\n\t.arg3_type      = ARG_ANYTHING,\n\t.arg4_type      = ARG_ANYTHING,\n};\n\nBPF_CALL_4(bpf_msg_redirect_map, struct sk_msg *, msg,\n\t   struct bpf_map *, map, u32, key, u64, flags)\n{\n\tstruct sock *sk;\n\n\tif (unlikely(flags & ~(BPF_F_INGRESS)))\n\t\treturn SK_DROP;\n\n\tsk = __sock_map_lookup_elem(map, key);\n\tif (unlikely(!sk || !sock_map_redirect_allowed(sk)))\n\t\treturn SK_DROP;\n\tif (!(flags & BPF_F_INGRESS) && !sk_is_tcp(sk))\n\t\treturn SK_DROP;\n\n\tmsg->flags = flags;\n\tmsg->sk_redir = sk;\n\treturn SK_PASS;\n}\n\nconst struct bpf_func_proto bpf_msg_redirect_map_proto = {\n\t.func           = bpf_msg_redirect_map,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type      = ARG_CONST_MAP_PTR,\n\t.arg3_type      = ARG_ANYTHING,\n\t.arg4_type      = ARG_ANYTHING,\n};\n\nstruct sock_map_seq_info {\n\tstruct bpf_map *map;\n\tstruct sock *sk;\n\tu32 index;\n};\n\nstruct bpf_iter__sockmap {\n\t__bpf_md_ptr(struct bpf_iter_meta *, meta);\n\t__bpf_md_ptr(struct bpf_map *, map);\n\t__bpf_md_ptr(void *, key);\n\t__bpf_md_ptr(struct sock *, sk);\n};\n\nDEFINE_BPF_ITER_FUNC(sockmap, struct bpf_iter_meta *meta,\n\t\t     struct bpf_map *map, void *key,\n\t\t     struct sock *sk)\n\nstatic void *sock_map_seq_lookup_elem(struct sock_map_seq_info *info)\n{\n\tif (unlikely(info->index >= info->map->max_entries))\n\t\treturn NULL;\n\n\tinfo->sk = __sock_map_lookup_elem(info->map, info->index);\n\n\t \n\treturn info;\n}\n\nstatic void *sock_map_seq_start(struct seq_file *seq, loff_t *pos)\n\t__acquires(rcu)\n{\n\tstruct sock_map_seq_info *info = seq->private;\n\n\tif (*pos == 0)\n\t\t++*pos;\n\n\t \n\trcu_read_lock();\n\treturn sock_map_seq_lookup_elem(info);\n}\n\nstatic void *sock_map_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n\t__must_hold(rcu)\n{\n\tstruct sock_map_seq_info *info = seq->private;\n\n\t++*pos;\n\t++info->index;\n\n\treturn sock_map_seq_lookup_elem(info);\n}\n\nstatic int sock_map_seq_show(struct seq_file *seq, void *v)\n\t__must_hold(rcu)\n{\n\tstruct sock_map_seq_info *info = seq->private;\n\tstruct bpf_iter__sockmap ctx = {};\n\tstruct bpf_iter_meta meta;\n\tstruct bpf_prog *prog;\n\n\tmeta.seq = seq;\n\tprog = bpf_iter_get_info(&meta, !v);\n\tif (!prog)\n\t\treturn 0;\n\n\tctx.meta = &meta;\n\tctx.map = info->map;\n\tif (v) {\n\t\tctx.key = &info->index;\n\t\tctx.sk = info->sk;\n\t}\n\n\treturn bpf_iter_run_prog(prog, &ctx);\n}\n\nstatic void sock_map_seq_stop(struct seq_file *seq, void *v)\n\t__releases(rcu)\n{\n\tif (!v)\n\t\t(void)sock_map_seq_show(seq, NULL);\n\n\t \n\trcu_read_unlock();\n}\n\nstatic const struct seq_operations sock_map_seq_ops = {\n\t.start\t= sock_map_seq_start,\n\t.next\t= sock_map_seq_next,\n\t.stop\t= sock_map_seq_stop,\n\t.show\t= sock_map_seq_show,\n};\n\nstatic int sock_map_init_seq_private(void *priv_data,\n\t\t\t\t     struct bpf_iter_aux_info *aux)\n{\n\tstruct sock_map_seq_info *info = priv_data;\n\n\tbpf_map_inc_with_uref(aux->map);\n\tinfo->map = aux->map;\n\treturn 0;\n}\n\nstatic void sock_map_fini_seq_private(void *priv_data)\n{\n\tstruct sock_map_seq_info *info = priv_data;\n\n\tbpf_map_put_with_uref(info->map);\n}\n\nstatic u64 sock_map_mem_usage(const struct bpf_map *map)\n{\n\tu64 usage = sizeof(struct bpf_stab);\n\n\tusage += (u64)map->max_entries * sizeof(struct sock *);\n\treturn usage;\n}\n\nstatic const struct bpf_iter_seq_info sock_map_iter_seq_info = {\n\t.seq_ops\t\t= &sock_map_seq_ops,\n\t.init_seq_private\t= sock_map_init_seq_private,\n\t.fini_seq_private\t= sock_map_fini_seq_private,\n\t.seq_priv_size\t\t= sizeof(struct sock_map_seq_info),\n};\n\nBTF_ID_LIST_SINGLE(sock_map_btf_ids, struct, bpf_stab)\nconst struct bpf_map_ops sock_map_ops = {\n\t.map_meta_equal\t\t= bpf_map_meta_equal,\n\t.map_alloc\t\t= sock_map_alloc,\n\t.map_free\t\t= sock_map_free,\n\t.map_get_next_key\t= sock_map_get_next_key,\n\t.map_lookup_elem_sys_only = sock_map_lookup_sys,\n\t.map_update_elem\t= sock_map_update_elem,\n\t.map_delete_elem\t= sock_map_delete_elem,\n\t.map_lookup_elem\t= sock_map_lookup,\n\t.map_release_uref\t= sock_map_release_progs,\n\t.map_check_btf\t\t= map_check_no_btf,\n\t.map_mem_usage\t\t= sock_map_mem_usage,\n\t.map_btf_id\t\t= &sock_map_btf_ids[0],\n\t.iter_seq_info\t\t= &sock_map_iter_seq_info,\n};\n\nstruct bpf_shtab_elem {\n\tstruct rcu_head rcu;\n\tu32 hash;\n\tstruct sock *sk;\n\tstruct hlist_node node;\n\tu8 key[];\n};\n\nstruct bpf_shtab_bucket {\n\tstruct hlist_head head;\n\tspinlock_t lock;\n};\n\nstruct bpf_shtab {\n\tstruct bpf_map map;\n\tstruct bpf_shtab_bucket *buckets;\n\tu32 buckets_num;\n\tu32 elem_size;\n\tstruct sk_psock_progs progs;\n\tatomic_t count;\n};\n\nstatic inline u32 sock_hash_bucket_hash(const void *key, u32 len)\n{\n\treturn jhash(key, len, 0);\n}\n\nstatic struct bpf_shtab_bucket *sock_hash_select_bucket(struct bpf_shtab *htab,\n\t\t\t\t\t\t\tu32 hash)\n{\n\treturn &htab->buckets[hash & (htab->buckets_num - 1)];\n}\n\nstatic struct bpf_shtab_elem *\nsock_hash_lookup_elem_raw(struct hlist_head *head, u32 hash, void *key,\n\t\t\t  u32 key_size)\n{\n\tstruct bpf_shtab_elem *elem;\n\n\thlist_for_each_entry_rcu(elem, head, node) {\n\t\tif (elem->hash == hash &&\n\t\t    !memcmp(&elem->key, key, key_size))\n\t\t\treturn elem;\n\t}\n\n\treturn NULL;\n}\n\nstatic struct sock *__sock_hash_lookup_elem(struct bpf_map *map, void *key)\n{\n\tstruct bpf_shtab *htab = container_of(map, struct bpf_shtab, map);\n\tu32 key_size = map->key_size, hash;\n\tstruct bpf_shtab_bucket *bucket;\n\tstruct bpf_shtab_elem *elem;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\n\thash = sock_hash_bucket_hash(key, key_size);\n\tbucket = sock_hash_select_bucket(htab, hash);\n\telem = sock_hash_lookup_elem_raw(&bucket->head, hash, key, key_size);\n\n\treturn elem ? elem->sk : NULL;\n}\n\nstatic void sock_hash_free_elem(struct bpf_shtab *htab,\n\t\t\t\tstruct bpf_shtab_elem *elem)\n{\n\tatomic_dec(&htab->count);\n\tkfree_rcu(elem, rcu);\n}\n\nstatic void sock_hash_delete_from_link(struct bpf_map *map, struct sock *sk,\n\t\t\t\t       void *link_raw)\n{\n\tstruct bpf_shtab *htab = container_of(map, struct bpf_shtab, map);\n\tstruct bpf_shtab_elem *elem_probe, *elem = link_raw;\n\tstruct bpf_shtab_bucket *bucket;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\tbucket = sock_hash_select_bucket(htab, elem->hash);\n\n\t \n\tspin_lock_bh(&bucket->lock);\n\telem_probe = sock_hash_lookup_elem_raw(&bucket->head, elem->hash,\n\t\t\t\t\t       elem->key, map->key_size);\n\tif (elem_probe && elem_probe == elem) {\n\t\thlist_del_rcu(&elem->node);\n\t\tsock_map_unref(elem->sk, elem);\n\t\tsock_hash_free_elem(htab, elem);\n\t}\n\tspin_unlock_bh(&bucket->lock);\n}\n\nstatic long sock_hash_delete_elem(struct bpf_map *map, void *key)\n{\n\tstruct bpf_shtab *htab = container_of(map, struct bpf_shtab, map);\n\tu32 hash, key_size = map->key_size;\n\tstruct bpf_shtab_bucket *bucket;\n\tstruct bpf_shtab_elem *elem;\n\tint ret = -ENOENT;\n\n\thash = sock_hash_bucket_hash(key, key_size);\n\tbucket = sock_hash_select_bucket(htab, hash);\n\n\tspin_lock_bh(&bucket->lock);\n\telem = sock_hash_lookup_elem_raw(&bucket->head, hash, key, key_size);\n\tif (elem) {\n\t\thlist_del_rcu(&elem->node);\n\t\tsock_map_unref(elem->sk, elem);\n\t\tsock_hash_free_elem(htab, elem);\n\t\tret = 0;\n\t}\n\tspin_unlock_bh(&bucket->lock);\n\treturn ret;\n}\n\nstatic struct bpf_shtab_elem *sock_hash_alloc_elem(struct bpf_shtab *htab,\n\t\t\t\t\t\t   void *key, u32 key_size,\n\t\t\t\t\t\t   u32 hash, struct sock *sk,\n\t\t\t\t\t\t   struct bpf_shtab_elem *old)\n{\n\tstruct bpf_shtab_elem *new;\n\n\tif (atomic_inc_return(&htab->count) > htab->map.max_entries) {\n\t\tif (!old) {\n\t\t\tatomic_dec(&htab->count);\n\t\t\treturn ERR_PTR(-E2BIG);\n\t\t}\n\t}\n\n\tnew = bpf_map_kmalloc_node(&htab->map, htab->elem_size,\n\t\t\t\t   GFP_ATOMIC | __GFP_NOWARN,\n\t\t\t\t   htab->map.numa_node);\n\tif (!new) {\n\t\tatomic_dec(&htab->count);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tmemcpy(new->key, key, key_size);\n\tnew->sk = sk;\n\tnew->hash = hash;\n\treturn new;\n}\n\nstatic int sock_hash_update_common(struct bpf_map *map, void *key,\n\t\t\t\t   struct sock *sk, u64 flags)\n{\n\tstruct bpf_shtab *htab = container_of(map, struct bpf_shtab, map);\n\tu32 key_size = map->key_size, hash;\n\tstruct bpf_shtab_elem *elem, *elem_new;\n\tstruct bpf_shtab_bucket *bucket;\n\tstruct sk_psock_link *link;\n\tstruct sk_psock *psock;\n\tint ret;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\tif (unlikely(flags > BPF_EXIST))\n\t\treturn -EINVAL;\n\n\tlink = sk_psock_init_link();\n\tif (!link)\n\t\treturn -ENOMEM;\n\n\tret = sock_map_link(map, sk);\n\tif (ret < 0)\n\t\tgoto out_free;\n\n\tpsock = sk_psock(sk);\n\tWARN_ON_ONCE(!psock);\n\n\thash = sock_hash_bucket_hash(key, key_size);\n\tbucket = sock_hash_select_bucket(htab, hash);\n\n\tspin_lock_bh(&bucket->lock);\n\telem = sock_hash_lookup_elem_raw(&bucket->head, hash, key, key_size);\n\tif (elem && flags == BPF_NOEXIST) {\n\t\tret = -EEXIST;\n\t\tgoto out_unlock;\n\t} else if (!elem && flags == BPF_EXIST) {\n\t\tret = -ENOENT;\n\t\tgoto out_unlock;\n\t}\n\n\telem_new = sock_hash_alloc_elem(htab, key, key_size, hash, sk, elem);\n\tif (IS_ERR(elem_new)) {\n\t\tret = PTR_ERR(elem_new);\n\t\tgoto out_unlock;\n\t}\n\n\tsock_map_add_link(psock, link, map, elem_new);\n\t \n\thlist_add_head_rcu(&elem_new->node, &bucket->head);\n\tif (elem) {\n\t\thlist_del_rcu(&elem->node);\n\t\tsock_map_unref(elem->sk, elem);\n\t\tsock_hash_free_elem(htab, elem);\n\t}\n\tspin_unlock_bh(&bucket->lock);\n\treturn 0;\nout_unlock:\n\tspin_unlock_bh(&bucket->lock);\n\tsk_psock_put(sk, psock);\nout_free:\n\tsk_psock_free_link(link);\n\treturn ret;\n}\n\nstatic int sock_hash_get_next_key(struct bpf_map *map, void *key,\n\t\t\t\t  void *key_next)\n{\n\tstruct bpf_shtab *htab = container_of(map, struct bpf_shtab, map);\n\tstruct bpf_shtab_elem *elem, *elem_next;\n\tu32 hash, key_size = map->key_size;\n\tstruct hlist_head *head;\n\tint i = 0;\n\n\tif (!key)\n\t\tgoto find_first_elem;\n\thash = sock_hash_bucket_hash(key, key_size);\n\thead = &sock_hash_select_bucket(htab, hash)->head;\n\telem = sock_hash_lookup_elem_raw(head, hash, key, key_size);\n\tif (!elem)\n\t\tgoto find_first_elem;\n\n\telem_next = hlist_entry_safe(rcu_dereference(hlist_next_rcu(&elem->node)),\n\t\t\t\t     struct bpf_shtab_elem, node);\n\tif (elem_next) {\n\t\tmemcpy(key_next, elem_next->key, key_size);\n\t\treturn 0;\n\t}\n\n\ti = hash & (htab->buckets_num - 1);\n\ti++;\nfind_first_elem:\n\tfor (; i < htab->buckets_num; i++) {\n\t\thead = &sock_hash_select_bucket(htab, i)->head;\n\t\telem_next = hlist_entry_safe(rcu_dereference(hlist_first_rcu(head)),\n\t\t\t\t\t     struct bpf_shtab_elem, node);\n\t\tif (elem_next) {\n\t\t\tmemcpy(key_next, elem_next->key, key_size);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn -ENOENT;\n}\n\nstatic struct bpf_map *sock_hash_alloc(union bpf_attr *attr)\n{\n\tstruct bpf_shtab *htab;\n\tint i, err;\n\n\tif (attr->max_entries == 0 ||\n\t    attr->key_size    == 0 ||\n\t    (attr->value_size != sizeof(u32) &&\n\t     attr->value_size != sizeof(u64)) ||\n\t    attr->map_flags & ~SOCK_CREATE_FLAG_MASK)\n\t\treturn ERR_PTR(-EINVAL);\n\tif (attr->key_size > MAX_BPF_STACK)\n\t\treturn ERR_PTR(-E2BIG);\n\n\thtab = bpf_map_area_alloc(sizeof(*htab), NUMA_NO_NODE);\n\tif (!htab)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tbpf_map_init_from_attr(&htab->map, attr);\n\n\thtab->buckets_num = roundup_pow_of_two(htab->map.max_entries);\n\thtab->elem_size = sizeof(struct bpf_shtab_elem) +\n\t\t\t  round_up(htab->map.key_size, 8);\n\tif (htab->buckets_num == 0 ||\n\t    htab->buckets_num > U32_MAX / sizeof(struct bpf_shtab_bucket)) {\n\t\terr = -EINVAL;\n\t\tgoto free_htab;\n\t}\n\n\thtab->buckets = bpf_map_area_alloc(htab->buckets_num *\n\t\t\t\t\t   sizeof(struct bpf_shtab_bucket),\n\t\t\t\t\t   htab->map.numa_node);\n\tif (!htab->buckets) {\n\t\terr = -ENOMEM;\n\t\tgoto free_htab;\n\t}\n\n\tfor (i = 0; i < htab->buckets_num; i++) {\n\t\tINIT_HLIST_HEAD(&htab->buckets[i].head);\n\t\tspin_lock_init(&htab->buckets[i].lock);\n\t}\n\n\treturn &htab->map;\nfree_htab:\n\tbpf_map_area_free(htab);\n\treturn ERR_PTR(err);\n}\n\nstatic void sock_hash_free(struct bpf_map *map)\n{\n\tstruct bpf_shtab *htab = container_of(map, struct bpf_shtab, map);\n\tstruct bpf_shtab_bucket *bucket;\n\tstruct hlist_head unlink_list;\n\tstruct bpf_shtab_elem *elem;\n\tstruct hlist_node *node;\n\tint i;\n\n\t \n\tsynchronize_rcu();\n\tfor (i = 0; i < htab->buckets_num; i++) {\n\t\tbucket = sock_hash_select_bucket(htab, i);\n\n\t\t \n\t\tspin_lock_bh(&bucket->lock);\n\t\thlist_for_each_entry(elem, &bucket->head, node)\n\t\t\tsock_hold(elem->sk);\n\t\thlist_move_list(&bucket->head, &unlink_list);\n\t\tspin_unlock_bh(&bucket->lock);\n\n\t\t \n\t\thlist_for_each_entry_safe(elem, node, &unlink_list, node) {\n\t\t\thlist_del(&elem->node);\n\t\t\tlock_sock(elem->sk);\n\t\t\trcu_read_lock();\n\t\t\tsock_map_unref(elem->sk, elem);\n\t\t\trcu_read_unlock();\n\t\t\trelease_sock(elem->sk);\n\t\t\tsock_put(elem->sk);\n\t\t\tsock_hash_free_elem(htab, elem);\n\t\t}\n\t}\n\n\t \n\tsynchronize_rcu();\n\n\tbpf_map_area_free(htab->buckets);\n\tbpf_map_area_free(htab);\n}\n\nstatic void *sock_hash_lookup_sys(struct bpf_map *map, void *key)\n{\n\tstruct sock *sk;\n\n\tif (map->value_size != sizeof(u64))\n\t\treturn ERR_PTR(-ENOSPC);\n\n\tsk = __sock_hash_lookup_elem(map, key);\n\tif (!sk)\n\t\treturn ERR_PTR(-ENOENT);\n\n\t__sock_gen_cookie(sk);\n\treturn &sk->sk_cookie;\n}\n\nstatic void *sock_hash_lookup(struct bpf_map *map, void *key)\n{\n\tstruct sock *sk;\n\n\tsk = __sock_hash_lookup_elem(map, key);\n\tif (!sk)\n\t\treturn NULL;\n\tif (sk_is_refcounted(sk) && !refcount_inc_not_zero(&sk->sk_refcnt))\n\t\treturn NULL;\n\treturn sk;\n}\n\nstatic void sock_hash_release_progs(struct bpf_map *map)\n{\n\tpsock_progs_drop(&container_of(map, struct bpf_shtab, map)->progs);\n}\n\nBPF_CALL_4(bpf_sock_hash_update, struct bpf_sock_ops_kern *, sops,\n\t   struct bpf_map *, map, void *, key, u64, flags)\n{\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\n\tif (likely(sock_map_sk_is_suitable(sops->sk) &&\n\t\t   sock_map_op_okay(sops)))\n\t\treturn sock_hash_update_common(map, key, sops->sk, flags);\n\treturn -EOPNOTSUPP;\n}\n\nconst struct bpf_func_proto bpf_sock_hash_update_proto = {\n\t.func\t\t= bpf_sock_hash_update,\n\t.gpl_only\t= false,\n\t.pkt_access\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_CONST_MAP_PTR,\n\t.arg3_type\t= ARG_PTR_TO_MAP_KEY,\n\t.arg4_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_4(bpf_sk_redirect_hash, struct sk_buff *, skb,\n\t   struct bpf_map *, map, void *, key, u64, flags)\n{\n\tstruct sock *sk;\n\n\tif (unlikely(flags & ~(BPF_F_INGRESS)))\n\t\treturn SK_DROP;\n\n\tsk = __sock_hash_lookup_elem(map, key);\n\tif (unlikely(!sk || !sock_map_redirect_allowed(sk)))\n\t\treturn SK_DROP;\n\n\tskb_bpf_set_redir(skb, sk, flags & BPF_F_INGRESS);\n\treturn SK_PASS;\n}\n\nconst struct bpf_func_proto bpf_sk_redirect_hash_proto = {\n\t.func           = bpf_sk_redirect_hash,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type      = ARG_CONST_MAP_PTR,\n\t.arg3_type      = ARG_PTR_TO_MAP_KEY,\n\t.arg4_type      = ARG_ANYTHING,\n};\n\nBPF_CALL_4(bpf_msg_redirect_hash, struct sk_msg *, msg,\n\t   struct bpf_map *, map, void *, key, u64, flags)\n{\n\tstruct sock *sk;\n\n\tif (unlikely(flags & ~(BPF_F_INGRESS)))\n\t\treturn SK_DROP;\n\n\tsk = __sock_hash_lookup_elem(map, key);\n\tif (unlikely(!sk || !sock_map_redirect_allowed(sk)))\n\t\treturn SK_DROP;\n\tif (!(flags & BPF_F_INGRESS) && !sk_is_tcp(sk))\n\t\treturn SK_DROP;\n\n\tmsg->flags = flags;\n\tmsg->sk_redir = sk;\n\treturn SK_PASS;\n}\n\nconst struct bpf_func_proto bpf_msg_redirect_hash_proto = {\n\t.func           = bpf_msg_redirect_hash,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type      = ARG_CONST_MAP_PTR,\n\t.arg3_type      = ARG_PTR_TO_MAP_KEY,\n\t.arg4_type      = ARG_ANYTHING,\n};\n\nstruct sock_hash_seq_info {\n\tstruct bpf_map *map;\n\tstruct bpf_shtab *htab;\n\tu32 bucket_id;\n};\n\nstatic void *sock_hash_seq_find_next(struct sock_hash_seq_info *info,\n\t\t\t\t     struct bpf_shtab_elem *prev_elem)\n{\n\tconst struct bpf_shtab *htab = info->htab;\n\tstruct bpf_shtab_bucket *bucket;\n\tstruct bpf_shtab_elem *elem;\n\tstruct hlist_node *node;\n\n\t \n\tif (prev_elem) {\n\t\tnode = rcu_dereference(hlist_next_rcu(&prev_elem->node));\n\t\telem = hlist_entry_safe(node, struct bpf_shtab_elem, node);\n\t\tif (elem)\n\t\t\treturn elem;\n\n\t\t \n\t\tinfo->bucket_id++;\n\t}\n\n\tfor (; info->bucket_id < htab->buckets_num; info->bucket_id++) {\n\t\tbucket = &htab->buckets[info->bucket_id];\n\t\tnode = rcu_dereference(hlist_first_rcu(&bucket->head));\n\t\telem = hlist_entry_safe(node, struct bpf_shtab_elem, node);\n\t\tif (elem)\n\t\t\treturn elem;\n\t}\n\n\treturn NULL;\n}\n\nstatic void *sock_hash_seq_start(struct seq_file *seq, loff_t *pos)\n\t__acquires(rcu)\n{\n\tstruct sock_hash_seq_info *info = seq->private;\n\n\tif (*pos == 0)\n\t\t++*pos;\n\n\t \n\trcu_read_lock();\n\treturn sock_hash_seq_find_next(info, NULL);\n}\n\nstatic void *sock_hash_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n\t__must_hold(rcu)\n{\n\tstruct sock_hash_seq_info *info = seq->private;\n\n\t++*pos;\n\treturn sock_hash_seq_find_next(info, v);\n}\n\nstatic int sock_hash_seq_show(struct seq_file *seq, void *v)\n\t__must_hold(rcu)\n{\n\tstruct sock_hash_seq_info *info = seq->private;\n\tstruct bpf_iter__sockmap ctx = {};\n\tstruct bpf_shtab_elem *elem = v;\n\tstruct bpf_iter_meta meta;\n\tstruct bpf_prog *prog;\n\n\tmeta.seq = seq;\n\tprog = bpf_iter_get_info(&meta, !elem);\n\tif (!prog)\n\t\treturn 0;\n\n\tctx.meta = &meta;\n\tctx.map = info->map;\n\tif (elem) {\n\t\tctx.key = elem->key;\n\t\tctx.sk = elem->sk;\n\t}\n\n\treturn bpf_iter_run_prog(prog, &ctx);\n}\n\nstatic void sock_hash_seq_stop(struct seq_file *seq, void *v)\n\t__releases(rcu)\n{\n\tif (!v)\n\t\t(void)sock_hash_seq_show(seq, NULL);\n\n\t \n\trcu_read_unlock();\n}\n\nstatic const struct seq_operations sock_hash_seq_ops = {\n\t.start\t= sock_hash_seq_start,\n\t.next\t= sock_hash_seq_next,\n\t.stop\t= sock_hash_seq_stop,\n\t.show\t= sock_hash_seq_show,\n};\n\nstatic int sock_hash_init_seq_private(void *priv_data,\n\t\t\t\t      struct bpf_iter_aux_info *aux)\n{\n\tstruct sock_hash_seq_info *info = priv_data;\n\n\tbpf_map_inc_with_uref(aux->map);\n\tinfo->map = aux->map;\n\tinfo->htab = container_of(aux->map, struct bpf_shtab, map);\n\treturn 0;\n}\n\nstatic void sock_hash_fini_seq_private(void *priv_data)\n{\n\tstruct sock_hash_seq_info *info = priv_data;\n\n\tbpf_map_put_with_uref(info->map);\n}\n\nstatic u64 sock_hash_mem_usage(const struct bpf_map *map)\n{\n\tstruct bpf_shtab *htab = container_of(map, struct bpf_shtab, map);\n\tu64 usage = sizeof(*htab);\n\n\tusage += htab->buckets_num * sizeof(struct bpf_shtab_bucket);\n\tusage += atomic_read(&htab->count) * (u64)htab->elem_size;\n\treturn usage;\n}\n\nstatic const struct bpf_iter_seq_info sock_hash_iter_seq_info = {\n\t.seq_ops\t\t= &sock_hash_seq_ops,\n\t.init_seq_private\t= sock_hash_init_seq_private,\n\t.fini_seq_private\t= sock_hash_fini_seq_private,\n\t.seq_priv_size\t\t= sizeof(struct sock_hash_seq_info),\n};\n\nBTF_ID_LIST_SINGLE(sock_hash_map_btf_ids, struct, bpf_shtab)\nconst struct bpf_map_ops sock_hash_ops = {\n\t.map_meta_equal\t\t= bpf_map_meta_equal,\n\t.map_alloc\t\t= sock_hash_alloc,\n\t.map_free\t\t= sock_hash_free,\n\t.map_get_next_key\t= sock_hash_get_next_key,\n\t.map_update_elem\t= sock_map_update_elem,\n\t.map_delete_elem\t= sock_hash_delete_elem,\n\t.map_lookup_elem\t= sock_hash_lookup,\n\t.map_lookup_elem_sys_only = sock_hash_lookup_sys,\n\t.map_release_uref\t= sock_hash_release_progs,\n\t.map_check_btf\t\t= map_check_no_btf,\n\t.map_mem_usage\t\t= sock_hash_mem_usage,\n\t.map_btf_id\t\t= &sock_hash_map_btf_ids[0],\n\t.iter_seq_info\t\t= &sock_hash_iter_seq_info,\n};\n\nstatic struct sk_psock_progs *sock_map_progs(struct bpf_map *map)\n{\n\tswitch (map->map_type) {\n\tcase BPF_MAP_TYPE_SOCKMAP:\n\t\treturn &container_of(map, struct bpf_stab, map)->progs;\n\tcase BPF_MAP_TYPE_SOCKHASH:\n\t\treturn &container_of(map, struct bpf_shtab, map)->progs;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn NULL;\n}\n\nstatic int sock_map_prog_lookup(struct bpf_map *map, struct bpf_prog ***pprog,\n\t\t\t\tu32 which)\n{\n\tstruct sk_psock_progs *progs = sock_map_progs(map);\n\n\tif (!progs)\n\t\treturn -EOPNOTSUPP;\n\n\tswitch (which) {\n\tcase BPF_SK_MSG_VERDICT:\n\t\t*pprog = &progs->msg_parser;\n\t\tbreak;\n#if IS_ENABLED(CONFIG_BPF_STREAM_PARSER)\n\tcase BPF_SK_SKB_STREAM_PARSER:\n\t\t*pprog = &progs->stream_parser;\n\t\tbreak;\n#endif\n\tcase BPF_SK_SKB_STREAM_VERDICT:\n\t\tif (progs->skb_verdict)\n\t\t\treturn -EBUSY;\n\t\t*pprog = &progs->stream_verdict;\n\t\tbreak;\n\tcase BPF_SK_SKB_VERDICT:\n\t\tif (progs->stream_verdict)\n\t\t\treturn -EBUSY;\n\t\t*pprog = &progs->skb_verdict;\n\t\tbreak;\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\treturn 0;\n}\n\nstatic int sock_map_prog_update(struct bpf_map *map, struct bpf_prog *prog,\n\t\t\t\tstruct bpf_prog *old, u32 which)\n{\n\tstruct bpf_prog **pprog;\n\tint ret;\n\n\tret = sock_map_prog_lookup(map, &pprog, which);\n\tif (ret)\n\t\treturn ret;\n\n\tif (old)\n\t\treturn psock_replace_prog(pprog, prog, old);\n\n\tpsock_set_prog(pprog, prog);\n\treturn 0;\n}\n\nint sock_map_bpf_prog_query(const union bpf_attr *attr,\n\t\t\t    union bpf_attr __user *uattr)\n{\n\t__u32 __user *prog_ids = u64_to_user_ptr(attr->query.prog_ids);\n\tu32 prog_cnt = 0, flags = 0, ufd = attr->target_fd;\n\tstruct bpf_prog **pprog;\n\tstruct bpf_prog *prog;\n\tstruct bpf_map *map;\n\tstruct fd f;\n\tu32 id = 0;\n\tint ret;\n\n\tif (attr->query.query_flags)\n\t\treturn -EINVAL;\n\n\tf = fdget(ufd);\n\tmap = __bpf_map_get(f);\n\tif (IS_ERR(map))\n\t\treturn PTR_ERR(map);\n\n\trcu_read_lock();\n\n\tret = sock_map_prog_lookup(map, &pprog, attr->query.attach_type);\n\tif (ret)\n\t\tgoto end;\n\n\tprog = *pprog;\n\tprog_cnt = !prog ? 0 : 1;\n\n\tif (!attr->query.prog_cnt || !prog_ids || !prog_cnt)\n\t\tgoto end;\n\n\t \n\tid = data_race(prog->aux->id);\n\tif (id == 0)\n\t\tprog_cnt = 0;\n\nend:\n\trcu_read_unlock();\n\n\tif (copy_to_user(&uattr->query.attach_flags, &flags, sizeof(flags)) ||\n\t    (id != 0 && copy_to_user(prog_ids, &id, sizeof(u32))) ||\n\t    copy_to_user(&uattr->query.prog_cnt, &prog_cnt, sizeof(prog_cnt)))\n\t\tret = -EFAULT;\n\n\tfdput(f);\n\treturn ret;\n}\n\nstatic void sock_map_unlink(struct sock *sk, struct sk_psock_link *link)\n{\n\tswitch (link->map->map_type) {\n\tcase BPF_MAP_TYPE_SOCKMAP:\n\t\treturn sock_map_delete_from_link(link->map, sk,\n\t\t\t\t\t\t link->link_raw);\n\tcase BPF_MAP_TYPE_SOCKHASH:\n\t\treturn sock_hash_delete_from_link(link->map, sk,\n\t\t\t\t\t\t  link->link_raw);\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic void sock_map_remove_links(struct sock *sk, struct sk_psock *psock)\n{\n\tstruct sk_psock_link *link;\n\n\twhile ((link = sk_psock_link_pop(psock))) {\n\t\tsock_map_unlink(sk, link);\n\t\tsk_psock_free_link(link);\n\t}\n}\n\nvoid sock_map_unhash(struct sock *sk)\n{\n\tvoid (*saved_unhash)(struct sock *sk);\n\tstruct sk_psock *psock;\n\n\trcu_read_lock();\n\tpsock = sk_psock(sk);\n\tif (unlikely(!psock)) {\n\t\trcu_read_unlock();\n\t\tsaved_unhash = READ_ONCE(sk->sk_prot)->unhash;\n\t} else {\n\t\tsaved_unhash = psock->saved_unhash;\n\t\tsock_map_remove_links(sk, psock);\n\t\trcu_read_unlock();\n\t}\n\tif (WARN_ON_ONCE(saved_unhash == sock_map_unhash))\n\t\treturn;\n\tif (saved_unhash)\n\t\tsaved_unhash(sk);\n}\nEXPORT_SYMBOL_GPL(sock_map_unhash);\n\nvoid sock_map_destroy(struct sock *sk)\n{\n\tvoid (*saved_destroy)(struct sock *sk);\n\tstruct sk_psock *psock;\n\n\trcu_read_lock();\n\tpsock = sk_psock_get(sk);\n\tif (unlikely(!psock)) {\n\t\trcu_read_unlock();\n\t\tsaved_destroy = READ_ONCE(sk->sk_prot)->destroy;\n\t} else {\n\t\tsaved_destroy = psock->saved_destroy;\n\t\tsock_map_remove_links(sk, psock);\n\t\trcu_read_unlock();\n\t\tsk_psock_stop(psock);\n\t\tsk_psock_put(sk, psock);\n\t}\n\tif (WARN_ON_ONCE(saved_destroy == sock_map_destroy))\n\t\treturn;\n\tif (saved_destroy)\n\t\tsaved_destroy(sk);\n}\nEXPORT_SYMBOL_GPL(sock_map_destroy);\n\nvoid sock_map_close(struct sock *sk, long timeout)\n{\n\tvoid (*saved_close)(struct sock *sk, long timeout);\n\tstruct sk_psock *psock;\n\n\tlock_sock(sk);\n\trcu_read_lock();\n\tpsock = sk_psock_get(sk);\n\tif (unlikely(!psock)) {\n\t\trcu_read_unlock();\n\t\trelease_sock(sk);\n\t\tsaved_close = READ_ONCE(sk->sk_prot)->close;\n\t} else {\n\t\tsaved_close = psock->saved_close;\n\t\tsock_map_remove_links(sk, psock);\n\t\trcu_read_unlock();\n\t\tsk_psock_stop(psock);\n\t\trelease_sock(sk);\n\t\tcancel_delayed_work_sync(&psock->work);\n\t\tsk_psock_put(sk, psock);\n\t}\n\n\t \n\tif (WARN_ON_ONCE(saved_close == sock_map_close))\n\t\treturn;\n\tsaved_close(sk, timeout);\n}\nEXPORT_SYMBOL_GPL(sock_map_close);\n\nstatic int sock_map_iter_attach_target(struct bpf_prog *prog,\n\t\t\t\t       union bpf_iter_link_info *linfo,\n\t\t\t\t       struct bpf_iter_aux_info *aux)\n{\n\tstruct bpf_map *map;\n\tint err = -EINVAL;\n\n\tif (!linfo->map.map_fd)\n\t\treturn -EBADF;\n\n\tmap = bpf_map_get_with_uref(linfo->map.map_fd);\n\tif (IS_ERR(map))\n\t\treturn PTR_ERR(map);\n\n\tif (map->map_type != BPF_MAP_TYPE_SOCKMAP &&\n\t    map->map_type != BPF_MAP_TYPE_SOCKHASH)\n\t\tgoto put_map;\n\n\tif (prog->aux->max_rdonly_access > map->key_size) {\n\t\terr = -EACCES;\n\t\tgoto put_map;\n\t}\n\n\taux->map = map;\n\treturn 0;\n\nput_map:\n\tbpf_map_put_with_uref(map);\n\treturn err;\n}\n\nstatic void sock_map_iter_detach_target(struct bpf_iter_aux_info *aux)\n{\n\tbpf_map_put_with_uref(aux->map);\n}\n\nstatic struct bpf_iter_reg sock_map_iter_reg = {\n\t.target\t\t\t= \"sockmap\",\n\t.attach_target\t\t= sock_map_iter_attach_target,\n\t.detach_target\t\t= sock_map_iter_detach_target,\n\t.show_fdinfo\t\t= bpf_iter_map_show_fdinfo,\n\t.fill_link_info\t\t= bpf_iter_map_fill_link_info,\n\t.ctx_arg_info_size\t= 2,\n\t.ctx_arg_info\t\t= {\n\t\t{ offsetof(struct bpf_iter__sockmap, key),\n\t\t  PTR_TO_BUF | PTR_MAYBE_NULL | MEM_RDONLY },\n\t\t{ offsetof(struct bpf_iter__sockmap, sk),\n\t\t  PTR_TO_BTF_ID_OR_NULL },\n\t},\n};\n\nstatic int __init bpf_sockmap_iter_init(void)\n{\n\tsock_map_iter_reg.ctx_arg_info[1].btf_id =\n\t\tbtf_sock_ids[BTF_SOCK_TYPE_SOCK];\n\treturn bpf_iter_reg_target(&sock_map_iter_reg);\n}\nlate_initcall(bpf_sockmap_iter_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}