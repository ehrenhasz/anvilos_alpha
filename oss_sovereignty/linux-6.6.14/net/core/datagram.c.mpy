{
  "module_name": "datagram.c",
  "hash_id": "69aaf95f5fbc994bebe2ab4dbf587da5917af564d3a297ab8f1b52834953a38b",
  "original_prompt": "Ingested from linux-6.6.14/net/core/datagram.c",
  "human_readable_source": "\n \n\n#include <linux/module.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/uaccess.h>\n#include <linux/mm.h>\n#include <linux/interrupt.h>\n#include <linux/errno.h>\n#include <linux/sched.h>\n#include <linux/inet.h>\n#include <linux/netdevice.h>\n#include <linux/rtnetlink.h>\n#include <linux/poll.h>\n#include <linux/highmem.h>\n#include <linux/spinlock.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/uio.h>\n#include <linux/indirect_call_wrapper.h>\n\n#include <net/protocol.h>\n#include <linux/skbuff.h>\n\n#include <net/checksum.h>\n#include <net/sock.h>\n#include <net/tcp_states.h>\n#include <trace/events/skb.h>\n#include <net/busy_poll.h>\n\n \nstatic inline int connection_based(struct sock *sk)\n{\n\treturn sk->sk_type == SOCK_SEQPACKET || sk->sk_type == SOCK_STREAM;\n}\n\nstatic int receiver_wake_function(wait_queue_entry_t *wait, unsigned int mode, int sync,\n\t\t\t\t  void *key)\n{\n\t \n\tif (key && !(key_to_poll(key) & (EPOLLIN | EPOLLERR)))\n\t\treturn 0;\n\treturn autoremove_wake_function(wait, mode, sync, key);\n}\n \nint __skb_wait_for_more_packets(struct sock *sk, struct sk_buff_head *queue,\n\t\t\t\tint *err, long *timeo_p,\n\t\t\t\tconst struct sk_buff *skb)\n{\n\tint error;\n\tDEFINE_WAIT_FUNC(wait, receiver_wake_function);\n\n\tprepare_to_wait_exclusive(sk_sleep(sk), &wait, TASK_INTERRUPTIBLE);\n\n\t \n\terror = sock_error(sk);\n\tif (error)\n\t\tgoto out_err;\n\n\tif (READ_ONCE(queue->prev) != skb)\n\t\tgoto out;\n\n\t \n\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\tgoto out_noerr;\n\n\t \n\terror = -ENOTCONN;\n\tif (connection_based(sk) &&\n\t    !(sk->sk_state == TCP_ESTABLISHED || sk->sk_state == TCP_LISTEN))\n\t\tgoto out_err;\n\n\t \n\tif (signal_pending(current))\n\t\tgoto interrupted;\n\n\terror = 0;\n\t*timeo_p = schedule_timeout(*timeo_p);\nout:\n\tfinish_wait(sk_sleep(sk), &wait);\n\treturn error;\ninterrupted:\n\terror = sock_intr_errno(*timeo_p);\nout_err:\n\t*err = error;\n\tgoto out;\nout_noerr:\n\t*err = 0;\n\terror = 1;\n\tgoto out;\n}\nEXPORT_SYMBOL(__skb_wait_for_more_packets);\n\nstatic struct sk_buff *skb_set_peeked(struct sk_buff *skb)\n{\n\tstruct sk_buff *nskb;\n\n\tif (skb->peeked)\n\t\treturn skb;\n\n\t \n\tif (!skb_shared(skb))\n\t\tgoto done;\n\n\tnskb = skb_clone(skb, GFP_ATOMIC);\n\tif (!nskb)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tskb->prev->next = nskb;\n\tskb->next->prev = nskb;\n\tnskb->prev = skb->prev;\n\tnskb->next = skb->next;\n\n\tconsume_skb(skb);\n\tskb = nskb;\n\ndone:\n\tskb->peeked = 1;\n\n\treturn skb;\n}\n\nstruct sk_buff *__skb_try_recv_from_queue(struct sock *sk,\n\t\t\t\t\t  struct sk_buff_head *queue,\n\t\t\t\t\t  unsigned int flags,\n\t\t\t\t\t  int *off, int *err,\n\t\t\t\t\t  struct sk_buff **last)\n{\n\tbool peek_at_off = false;\n\tstruct sk_buff *skb;\n\tint _off = 0;\n\n\tif (unlikely(flags & MSG_PEEK && *off >= 0)) {\n\t\tpeek_at_off = true;\n\t\t_off = *off;\n\t}\n\n\t*last = queue->prev;\n\tskb_queue_walk(queue, skb) {\n\t\tif (flags & MSG_PEEK) {\n\t\t\tif (peek_at_off && _off >= skb->len &&\n\t\t\t    (_off || skb->peeked)) {\n\t\t\t\t_off -= skb->len;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (!skb->len) {\n\t\t\t\tskb = skb_set_peeked(skb);\n\t\t\t\tif (IS_ERR(skb)) {\n\t\t\t\t\t*err = PTR_ERR(skb);\n\t\t\t\t\treturn NULL;\n\t\t\t\t}\n\t\t\t}\n\t\t\trefcount_inc(&skb->users);\n\t\t} else {\n\t\t\t__skb_unlink(skb, queue);\n\t\t}\n\t\t*off = _off;\n\t\treturn skb;\n\t}\n\treturn NULL;\n}\n\n \nstruct sk_buff *__skb_try_recv_datagram(struct sock *sk,\n\t\t\t\t\tstruct sk_buff_head *queue,\n\t\t\t\t\tunsigned int flags, int *off, int *err,\n\t\t\t\t\tstruct sk_buff **last)\n{\n\tstruct sk_buff *skb;\n\tunsigned long cpu_flags;\n\t \n\tint error = sock_error(sk);\n\n\tif (error)\n\t\tgoto no_packet;\n\n\tdo {\n\t\t \n\t\tspin_lock_irqsave(&queue->lock, cpu_flags);\n\t\tskb = __skb_try_recv_from_queue(sk, queue, flags, off, &error,\n\t\t\t\t\t\tlast);\n\t\tspin_unlock_irqrestore(&queue->lock, cpu_flags);\n\t\tif (error)\n\t\t\tgoto no_packet;\n\t\tif (skb)\n\t\t\treturn skb;\n\n\t\tif (!sk_can_busy_loop(sk))\n\t\t\tbreak;\n\n\t\tsk_busy_loop(sk, flags & MSG_DONTWAIT);\n\t} while (READ_ONCE(queue->prev) != *last);\n\n\terror = -EAGAIN;\n\nno_packet:\n\t*err = error;\n\treturn NULL;\n}\nEXPORT_SYMBOL(__skb_try_recv_datagram);\n\nstruct sk_buff *__skb_recv_datagram(struct sock *sk,\n\t\t\t\t    struct sk_buff_head *sk_queue,\n\t\t\t\t    unsigned int flags, int *off, int *err)\n{\n\tstruct sk_buff *skb, *last;\n\tlong timeo;\n\n\ttimeo = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);\n\n\tdo {\n\t\tskb = __skb_try_recv_datagram(sk, sk_queue, flags, off, err,\n\t\t\t\t\t      &last);\n\t\tif (skb)\n\t\t\treturn skb;\n\n\t\tif (*err != -EAGAIN)\n\t\t\tbreak;\n\t} while (timeo &&\n\t\t !__skb_wait_for_more_packets(sk, sk_queue, err,\n\t\t\t\t\t      &timeo, last));\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(__skb_recv_datagram);\n\nstruct sk_buff *skb_recv_datagram(struct sock *sk, unsigned int flags,\n\t\t\t\t  int *err)\n{\n\tint off = 0;\n\n\treturn __skb_recv_datagram(sk, &sk->sk_receive_queue, flags,\n\t\t\t\t   &off, err);\n}\nEXPORT_SYMBOL(skb_recv_datagram);\n\nvoid skb_free_datagram(struct sock *sk, struct sk_buff *skb)\n{\n\tconsume_skb(skb);\n}\nEXPORT_SYMBOL(skb_free_datagram);\n\nvoid __skb_free_datagram_locked(struct sock *sk, struct sk_buff *skb, int len)\n{\n\tbool slow;\n\n\tif (!skb_unref(skb)) {\n\t\tsk_peek_offset_bwd(sk, len);\n\t\treturn;\n\t}\n\n\tslow = lock_sock_fast(sk);\n\tsk_peek_offset_bwd(sk, len);\n\tskb_orphan(skb);\n\tunlock_sock_fast(sk, slow);\n\n\t \n\t__kfree_skb(skb);\n}\nEXPORT_SYMBOL(__skb_free_datagram_locked);\n\nint __sk_queue_drop_skb(struct sock *sk, struct sk_buff_head *sk_queue,\n\t\t\tstruct sk_buff *skb, unsigned int flags,\n\t\t\tvoid (*destructor)(struct sock *sk,\n\t\t\t\t\t   struct sk_buff *skb))\n{\n\tint err = 0;\n\n\tif (flags & MSG_PEEK) {\n\t\terr = -ENOENT;\n\t\tspin_lock_bh(&sk_queue->lock);\n\t\tif (skb->next) {\n\t\t\t__skb_unlink(skb, sk_queue);\n\t\t\trefcount_dec(&skb->users);\n\t\t\tif (destructor)\n\t\t\t\tdestructor(sk, skb);\n\t\t\terr = 0;\n\t\t}\n\t\tspin_unlock_bh(&sk_queue->lock);\n\t}\n\n\tatomic_inc(&sk->sk_drops);\n\treturn err;\n}\nEXPORT_SYMBOL(__sk_queue_drop_skb);\n\n \n\nint skb_kill_datagram(struct sock *sk, struct sk_buff *skb, unsigned int flags)\n{\n\tint err = __sk_queue_drop_skb(sk, &sk->sk_receive_queue, skb, flags,\n\t\t\t\t      NULL);\n\n\tkfree_skb(skb);\n\treturn err;\n}\nEXPORT_SYMBOL(skb_kill_datagram);\n\nINDIRECT_CALLABLE_DECLARE(static size_t simple_copy_to_iter(const void *addr,\n\t\t\t\t\t\tsize_t bytes,\n\t\t\t\t\t\tvoid *data __always_unused,\n\t\t\t\t\t\tstruct iov_iter *i));\n\nstatic int __skb_datagram_iter(const struct sk_buff *skb, int offset,\n\t\t\t       struct iov_iter *to, int len, bool fault_short,\n\t\t\t       size_t (*cb)(const void *, size_t, void *,\n\t\t\t\t\t    struct iov_iter *), void *data)\n{\n\tint start = skb_headlen(skb);\n\tint i, copy = start - offset, start_off = offset, n;\n\tstruct sk_buff *frag_iter;\n\n\t \n\tif (copy > 0) {\n\t\tif (copy > len)\n\t\t\tcopy = len;\n\t\tn = INDIRECT_CALL_1(cb, simple_copy_to_iter,\n\t\t\t\t    skb->data + offset, copy, data, to);\n\t\toffset += n;\n\t\tif (n != copy)\n\t\t\tgoto short_copy;\n\t\tif ((len -= copy) == 0)\n\t\t\treturn 0;\n\t}\n\n\t \n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tint end;\n\t\tconst skb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + skb_frag_size(frag);\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tstruct page *page = skb_frag_page(frag);\n\t\t\tu8 *vaddr = kmap(page);\n\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tn = INDIRECT_CALL_1(cb, simple_copy_to_iter,\n\t\t\t\t\tvaddr + skb_frag_off(frag) + offset - start,\n\t\t\t\t\tcopy, data, to);\n\t\t\tkunmap(page);\n\t\t\toffset += n;\n\t\t\tif (n != copy)\n\t\t\t\tgoto short_copy;\n\t\t\tif (!(len -= copy))\n\t\t\t\treturn 0;\n\t\t}\n\t\tstart = end;\n\t}\n\n\tskb_walk_frags(skb, frag_iter) {\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + frag_iter->len;\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tif (__skb_datagram_iter(frag_iter, offset - start,\n\t\t\t\t\t\tto, copy, fault_short, cb, data))\n\t\t\t\tgoto fault;\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn 0;\n\t\t\toffset += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\tif (!len)\n\t\treturn 0;\n\n\t \n\nfault:\n\tiov_iter_revert(to, offset - start_off);\n\treturn -EFAULT;\n\nshort_copy:\n\tif (fault_short || iov_iter_count(to))\n\t\tgoto fault;\n\n\treturn 0;\n}\n\n \nint skb_copy_and_hash_datagram_iter(const struct sk_buff *skb, int offset,\n\t\t\t   struct iov_iter *to, int len,\n\t\t\t   struct ahash_request *hash)\n{\n\treturn __skb_datagram_iter(skb, offset, to, len, true,\n\t\t\thash_and_copy_to_iter, hash);\n}\nEXPORT_SYMBOL(skb_copy_and_hash_datagram_iter);\n\nstatic size_t simple_copy_to_iter(const void *addr, size_t bytes,\n\t\tvoid *data __always_unused, struct iov_iter *i)\n{\n\treturn copy_to_iter(addr, bytes, i);\n}\n\n \nint skb_copy_datagram_iter(const struct sk_buff *skb, int offset,\n\t\t\t   struct iov_iter *to, int len)\n{\n\ttrace_skb_copy_datagram_iovec(skb, len);\n\treturn __skb_datagram_iter(skb, offset, to, len, false,\n\t\t\tsimple_copy_to_iter, NULL);\n}\nEXPORT_SYMBOL(skb_copy_datagram_iter);\n\n \nint skb_copy_datagram_from_iter(struct sk_buff *skb, int offset,\n\t\t\t\t struct iov_iter *from,\n\t\t\t\t int len)\n{\n\tint start = skb_headlen(skb);\n\tint i, copy = start - offset;\n\tstruct sk_buff *frag_iter;\n\n\t \n\tif (copy > 0) {\n\t\tif (copy > len)\n\t\t\tcopy = len;\n\t\tif (copy_from_iter(skb->data + offset, copy, from) != copy)\n\t\t\tgoto fault;\n\t\tif ((len -= copy) == 0)\n\t\t\treturn 0;\n\t\toffset += copy;\n\t}\n\n\t \n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tint end;\n\t\tconst skb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + skb_frag_size(frag);\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tsize_t copied;\n\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tcopied = copy_page_from_iter(skb_frag_page(frag),\n\t\t\t\t\t  skb_frag_off(frag) + offset - start,\n\t\t\t\t\t  copy, from);\n\t\t\tif (copied != copy)\n\t\t\t\tgoto fault;\n\n\t\t\tif (!(len -= copy))\n\t\t\t\treturn 0;\n\t\t\toffset += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\n\tskb_walk_frags(skb, frag_iter) {\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + frag_iter->len;\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tif (skb_copy_datagram_from_iter(frag_iter,\n\t\t\t\t\t\t\toffset - start,\n\t\t\t\t\t\t\tfrom, copy))\n\t\t\t\tgoto fault;\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn 0;\n\t\t\toffset += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\tif (!len)\n\t\treturn 0;\n\nfault:\n\treturn -EFAULT;\n}\nEXPORT_SYMBOL(skb_copy_datagram_from_iter);\n\nint __zerocopy_sg_from_iter(struct msghdr *msg, struct sock *sk,\n\t\t\t    struct sk_buff *skb, struct iov_iter *from,\n\t\t\t    size_t length)\n{\n\tint frag;\n\n\tif (msg && msg->msg_ubuf && msg->sg_from_iter)\n\t\treturn msg->sg_from_iter(sk, skb, from, length);\n\n\tfrag = skb_shinfo(skb)->nr_frags;\n\n\twhile (length && iov_iter_count(from)) {\n\t\tstruct page *head, *last_head = NULL;\n\t\tstruct page *pages[MAX_SKB_FRAGS];\n\t\tint refs, order, n = 0;\n\t\tsize_t start;\n\t\tssize_t copied;\n\t\tunsigned long truesize;\n\n\t\tif (frag == MAX_SKB_FRAGS)\n\t\t\treturn -EMSGSIZE;\n\n\t\tcopied = iov_iter_get_pages2(from, pages, length,\n\t\t\t\t\t    MAX_SKB_FRAGS - frag, &start);\n\t\tif (copied < 0)\n\t\t\treturn -EFAULT;\n\n\t\tlength -= copied;\n\n\t\ttruesize = PAGE_ALIGN(copied + start);\n\t\tskb->data_len += copied;\n\t\tskb->len += copied;\n\t\tskb->truesize += truesize;\n\t\tif (sk && sk->sk_type == SOCK_STREAM) {\n\t\t\tsk_wmem_queued_add(sk, truesize);\n\t\t\tif (!skb_zcopy_pure(skb))\n\t\t\t\tsk_mem_charge(sk, truesize);\n\t\t} else {\n\t\t\trefcount_add(truesize, &skb->sk->sk_wmem_alloc);\n\t\t}\n\n\t\thead = compound_head(pages[n]);\n\t\torder = compound_order(head);\n\n\t\tfor (refs = 0; copied != 0; start = 0) {\n\t\t\tint size = min_t(int, copied, PAGE_SIZE - start);\n\n\t\t\tif (pages[n] - head > (1UL << order) - 1) {\n\t\t\t\thead = compound_head(pages[n]);\n\t\t\t\torder = compound_order(head);\n\t\t\t}\n\n\t\t\tstart += (pages[n] - head) << PAGE_SHIFT;\n\t\t\tcopied -= size;\n\t\t\tn++;\n\t\t\tif (frag) {\n\t\t\t\tskb_frag_t *last = &skb_shinfo(skb)->frags[frag - 1];\n\n\t\t\t\tif (head == skb_frag_page(last) &&\n\t\t\t\t    start == skb_frag_off(last) + skb_frag_size(last)) {\n\t\t\t\t\tskb_frag_size_add(last, size);\n\t\t\t\t\t \n\t\t\t\t\tlast_head = head;\n\t\t\t\t\trefs++;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (refs) {\n\t\t\t\tpage_ref_sub(last_head, refs);\n\t\t\t\trefs = 0;\n\t\t\t}\n\t\t\tskb_fill_page_desc_noacc(skb, frag++, head, start, size);\n\t\t}\n\t\tif (refs)\n\t\t\tpage_ref_sub(last_head, refs);\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(__zerocopy_sg_from_iter);\n\n \nint zerocopy_sg_from_iter(struct sk_buff *skb, struct iov_iter *from)\n{\n\tint copy = min_t(int, skb_headlen(skb), iov_iter_count(from));\n\n\t \n\tif (skb_copy_datagram_from_iter(skb, 0, from, copy))\n\t\treturn -EFAULT;\n\n\treturn __zerocopy_sg_from_iter(NULL, NULL, skb, from, ~0U);\n}\nEXPORT_SYMBOL(zerocopy_sg_from_iter);\n\n \nstatic int skb_copy_and_csum_datagram(const struct sk_buff *skb, int offset,\n\t\t\t\t      struct iov_iter *to, int len,\n\t\t\t\t      __wsum *csump)\n{\n\tstruct csum_state csdata = { .csum = *csump };\n\tint ret;\n\n\tret = __skb_datagram_iter(skb, offset, to, len, true,\n\t\t\t\t  csum_and_copy_to_iter, &csdata);\n\tif (ret)\n\t\treturn ret;\n\n\t*csump = csdata.csum;\n\treturn 0;\n}\n\n \nint skb_copy_and_csum_datagram_msg(struct sk_buff *skb,\n\t\t\t\t   int hlen, struct msghdr *msg)\n{\n\t__wsum csum;\n\tint chunk = skb->len - hlen;\n\n\tif (!chunk)\n\t\treturn 0;\n\n\tif (msg_data_left(msg) < chunk) {\n\t\tif (__skb_checksum_complete(skb))\n\t\t\treturn -EINVAL;\n\t\tif (skb_copy_datagram_msg(skb, hlen, msg, chunk))\n\t\t\tgoto fault;\n\t} else {\n\t\tcsum = csum_partial(skb->data, hlen, skb->csum);\n\t\tif (skb_copy_and_csum_datagram(skb, hlen, &msg->msg_iter,\n\t\t\t\t\t       chunk, &csum))\n\t\t\tgoto fault;\n\n\t\tif (csum_fold(csum)) {\n\t\t\tiov_iter_revert(&msg->msg_iter, chunk);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (unlikely(skb->ip_summed == CHECKSUM_COMPLETE) &&\n\t\t    !skb->csum_complete_sw)\n\t\t\tnetdev_rx_csum_fault(NULL, skb);\n\t}\n\treturn 0;\nfault:\n\treturn -EFAULT;\n}\nEXPORT_SYMBOL(skb_copy_and_csum_datagram_msg);\n\n \n__poll_t datagram_poll(struct file *file, struct socket *sock,\n\t\t\t   poll_table *wait)\n{\n\tstruct sock *sk = sock->sk;\n\t__poll_t mask;\n\tu8 shutdown;\n\n\tsock_poll_wait(file, sock, wait);\n\tmask = 0;\n\n\t \n\tif (READ_ONCE(sk->sk_err) ||\n\t    !skb_queue_empty_lockless(&sk->sk_error_queue))\n\t\tmask |= EPOLLERR |\n\t\t\t(sock_flag(sk, SOCK_SELECT_ERR_QUEUE) ? EPOLLPRI : 0);\n\n\tshutdown = READ_ONCE(sk->sk_shutdown);\n\tif (shutdown & RCV_SHUTDOWN)\n\t\tmask |= EPOLLRDHUP | EPOLLIN | EPOLLRDNORM;\n\tif (shutdown == SHUTDOWN_MASK)\n\t\tmask |= EPOLLHUP;\n\n\t \n\tif (!skb_queue_empty_lockless(&sk->sk_receive_queue))\n\t\tmask |= EPOLLIN | EPOLLRDNORM;\n\n\t \n\tif (connection_based(sk)) {\n\t\tint state = READ_ONCE(sk->sk_state);\n\n\t\tif (state == TCP_CLOSE)\n\t\t\tmask |= EPOLLHUP;\n\t\t \n\t\tif (state == TCP_SYN_SENT)\n\t\t\treturn mask;\n\t}\n\n\t \n\tif (sock_writeable(sk))\n\t\tmask |= EPOLLOUT | EPOLLWRNORM | EPOLLWRBAND;\n\telse\n\t\tsk_set_bit(SOCKWQ_ASYNC_NOSPACE, sk);\n\n\treturn mask;\n}\nEXPORT_SYMBOL(datagram_poll);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}