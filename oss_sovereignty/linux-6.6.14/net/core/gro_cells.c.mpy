{
  "module_name": "gro_cells.c",
  "hash_id": "c6022ac41456ce605a37ddf446485b539a52854e819aef1c6a8a3a5189dcc3dd",
  "original_prompt": "Ingested from linux-6.6.14/net/core/gro_cells.c",
  "human_readable_source": "\n#include <linux/skbuff.h>\n#include <linux/slab.h>\n#include <linux/netdevice.h>\n#include <net/gro_cells.h>\n\nstruct gro_cell {\n\tstruct sk_buff_head\tnapi_skbs;\n\tstruct napi_struct\tnapi;\n};\n\nint gro_cells_receive(struct gro_cells *gcells, struct sk_buff *skb)\n{\n\tstruct net_device *dev = skb->dev;\n\tstruct gro_cell *cell;\n\tint res;\n\n\trcu_read_lock();\n\tif (unlikely(!(dev->flags & IFF_UP)))\n\t\tgoto drop;\n\n\tif (!gcells->cells || skb_cloned(skb) || netif_elide_gro(dev)) {\n\t\tres = netif_rx(skb);\n\t\tgoto unlock;\n\t}\n\n\tcell = this_cpu_ptr(gcells->cells);\n\n\tif (skb_queue_len(&cell->napi_skbs) > READ_ONCE(netdev_max_backlog)) {\ndrop:\n\t\tdev_core_stats_rx_dropped_inc(dev);\n\t\tkfree_skb(skb);\n\t\tres = NET_RX_DROP;\n\t\tgoto unlock;\n\t}\n\n\t__skb_queue_tail(&cell->napi_skbs, skb);\n\tif (skb_queue_len(&cell->napi_skbs) == 1)\n\t\tnapi_schedule(&cell->napi);\n\n\tres = NET_RX_SUCCESS;\n\nunlock:\n\trcu_read_unlock();\n\treturn res;\n}\nEXPORT_SYMBOL(gro_cells_receive);\n\n \nstatic int gro_cell_poll(struct napi_struct *napi, int budget)\n{\n\tstruct gro_cell *cell = container_of(napi, struct gro_cell, napi);\n\tstruct sk_buff *skb;\n\tint work_done = 0;\n\n\twhile (work_done < budget) {\n\t\tskb = __skb_dequeue(&cell->napi_skbs);\n\t\tif (!skb)\n\t\t\tbreak;\n\t\tnapi_gro_receive(napi, skb);\n\t\twork_done++;\n\t}\n\n\tif (work_done < budget)\n\t\tnapi_complete_done(napi, work_done);\n\treturn work_done;\n}\n\nint gro_cells_init(struct gro_cells *gcells, struct net_device *dev)\n{\n\tint i;\n\n\tgcells->cells = alloc_percpu(struct gro_cell);\n\tif (!gcells->cells)\n\t\treturn -ENOMEM;\n\n\tfor_each_possible_cpu(i) {\n\t\tstruct gro_cell *cell = per_cpu_ptr(gcells->cells, i);\n\n\t\t__skb_queue_head_init(&cell->napi_skbs);\n\n\t\tset_bit(NAPI_STATE_NO_BUSY_POLL, &cell->napi.state);\n\n\t\tnetif_napi_add(dev, &cell->napi, gro_cell_poll);\n\t\tnapi_enable(&cell->napi);\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(gro_cells_init);\n\nstruct percpu_free_defer {\n\tstruct rcu_head rcu;\n\tvoid __percpu\t*ptr;\n};\n\nstatic void percpu_free_defer_callback(struct rcu_head *head)\n{\n\tstruct percpu_free_defer *defer;\n\n\tdefer = container_of(head, struct percpu_free_defer, rcu);\n\tfree_percpu(defer->ptr);\n\tkfree(defer);\n}\n\nvoid gro_cells_destroy(struct gro_cells *gcells)\n{\n\tstruct percpu_free_defer *defer;\n\tint i;\n\n\tif (!gcells->cells)\n\t\treturn;\n\tfor_each_possible_cpu(i) {\n\t\tstruct gro_cell *cell = per_cpu_ptr(gcells->cells, i);\n\n\t\tnapi_disable(&cell->napi);\n\t\t__netif_napi_del(&cell->napi);\n\t\t__skb_queue_purge(&cell->napi_skbs);\n\t}\n\t \n\tdefer = kmalloc(sizeof(*defer), GFP_KERNEL | __GFP_NOWARN);\n\tif (likely(defer)) {\n\t\tdefer->ptr = gcells->cells;\n\t\tcall_rcu(&defer->rcu, percpu_free_defer_callback);\n\t} else {\n\t\t \n\t\tsynchronize_rcu_expedited();\n\t\tfree_percpu(gcells->cells);\n\t}\n\tgcells->cells = NULL;\n}\nEXPORT_SYMBOL(gro_cells_destroy);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}