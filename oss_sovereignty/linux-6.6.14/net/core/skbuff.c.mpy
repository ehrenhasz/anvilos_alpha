{
  "module_name": "skbuff.c",
  "hash_id": "c40947d9ddf00548bcf55a3998ab725e457e4679c453b94c3bcee13dc716e7cb",
  "original_prompt": "Ingested from linux-6.6.14/net/core/skbuff.c",
  "human_readable_source": "\n \n\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/module.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/mm.h>\n#include <linux/interrupt.h>\n#include <linux/in.h>\n#include <linux/inet.h>\n#include <linux/slab.h>\n#include <linux/tcp.h>\n#include <linux/udp.h>\n#include <linux/sctp.h>\n#include <linux/netdevice.h>\n#ifdef CONFIG_NET_CLS_ACT\n#include <net/pkt_sched.h>\n#endif\n#include <linux/string.h>\n#include <linux/skbuff.h>\n#include <linux/splice.h>\n#include <linux/cache.h>\n#include <linux/rtnetlink.h>\n#include <linux/init.h>\n#include <linux/scatterlist.h>\n#include <linux/errqueue.h>\n#include <linux/prefetch.h>\n#include <linux/bitfield.h>\n#include <linux/if_vlan.h>\n#include <linux/mpls.h>\n#include <linux/kcov.h>\n\n#include <net/protocol.h>\n#include <net/dst.h>\n#include <net/sock.h>\n#include <net/checksum.h>\n#include <net/gso.h>\n#include <net/ip6_checksum.h>\n#include <net/xfrm.h>\n#include <net/mpls.h>\n#include <net/mptcp.h>\n#include <net/mctp.h>\n#include <net/page_pool/helpers.h>\n#include <net/dropreason.h>\n\n#include <linux/uaccess.h>\n#include <trace/events/skb.h>\n#include <linux/highmem.h>\n#include <linux/capability.h>\n#include <linux/user_namespace.h>\n#include <linux/indirect_call_wrapper.h>\n#include <linux/textsearch.h>\n\n#include \"dev.h\"\n#include \"sock_destructor.h\"\n\nstruct kmem_cache *skbuff_cache __ro_after_init;\nstatic struct kmem_cache *skbuff_fclone_cache __ro_after_init;\n#ifdef CONFIG_SKB_EXTENSIONS\nstatic struct kmem_cache *skbuff_ext_cache __ro_after_init;\n#endif\n\n\nstatic struct kmem_cache *skb_small_head_cache __ro_after_init;\n\n#define SKB_SMALL_HEAD_SIZE SKB_HEAD_ALIGN(MAX_TCP_HEADER)\n\n \n#define SKB_SMALL_HEAD_CACHE_SIZE\t\t\t\t\t\\\n\t(is_power_of_2(SKB_SMALL_HEAD_SIZE) ?\t\t\t\\\n\t\t(SKB_SMALL_HEAD_SIZE + L1_CACHE_BYTES) :\t\\\n\t\tSKB_SMALL_HEAD_SIZE)\n\n#define SKB_SMALL_HEAD_HEADROOM\t\t\t\t\t\t\\\n\tSKB_WITH_OVERHEAD(SKB_SMALL_HEAD_CACHE_SIZE)\n\nint sysctl_max_skb_frags __read_mostly = MAX_SKB_FRAGS;\nEXPORT_SYMBOL(sysctl_max_skb_frags);\n\n#undef FN\n#define FN(reason) [SKB_DROP_REASON_##reason] = #reason,\nstatic const char * const drop_reasons[] = {\n\t[SKB_CONSUMED] = \"CONSUMED\",\n\tDEFINE_DROP_REASON(FN, FN)\n};\n\nstatic const struct drop_reason_list drop_reasons_core = {\n\t.reasons = drop_reasons,\n\t.n_reasons = ARRAY_SIZE(drop_reasons),\n};\n\nconst struct drop_reason_list __rcu *\ndrop_reasons_by_subsys[SKB_DROP_REASON_SUBSYS_NUM] = {\n\t[SKB_DROP_REASON_SUBSYS_CORE] = RCU_INITIALIZER(&drop_reasons_core),\n};\nEXPORT_SYMBOL(drop_reasons_by_subsys);\n\n \nvoid drop_reasons_register_subsys(enum skb_drop_reason_subsys subsys,\n\t\t\t\t  const struct drop_reason_list *list)\n{\n\tif (WARN(subsys <= SKB_DROP_REASON_SUBSYS_CORE ||\n\t\t subsys >= ARRAY_SIZE(drop_reasons_by_subsys),\n\t\t \"invalid subsystem %d\\n\", subsys))\n\t\treturn;\n\n\t \n\tRCU_INIT_POINTER(drop_reasons_by_subsys[subsys], list);\n}\nEXPORT_SYMBOL_GPL(drop_reasons_register_subsys);\n\n \nvoid drop_reasons_unregister_subsys(enum skb_drop_reason_subsys subsys)\n{\n\tif (WARN(subsys <= SKB_DROP_REASON_SUBSYS_CORE ||\n\t\t subsys >= ARRAY_SIZE(drop_reasons_by_subsys),\n\t\t \"invalid subsystem %d\\n\", subsys))\n\t\treturn;\n\n\tRCU_INIT_POINTER(drop_reasons_by_subsys[subsys], NULL);\n\n\tsynchronize_rcu();\n}\nEXPORT_SYMBOL_GPL(drop_reasons_unregister_subsys);\n\n \nstatic void skb_panic(struct sk_buff *skb, unsigned int sz, void *addr,\n\t\t      const char msg[])\n{\n\tpr_emerg(\"%s: text:%px len:%d put:%d head:%px data:%px tail:%#lx end:%#lx dev:%s\\n\",\n\t\t msg, addr, skb->len, sz, skb->head, skb->data,\n\t\t (unsigned long)skb->tail, (unsigned long)skb->end,\n\t\t skb->dev ? skb->dev->name : \"<NULL>\");\n\tBUG();\n}\n\nstatic void skb_over_panic(struct sk_buff *skb, unsigned int sz, void *addr)\n{\n\tskb_panic(skb, sz, addr, __func__);\n}\n\nstatic void skb_under_panic(struct sk_buff *skb, unsigned int sz, void *addr)\n{\n\tskb_panic(skb, sz, addr, __func__);\n}\n\n#define NAPI_SKB_CACHE_SIZE\t64\n#define NAPI_SKB_CACHE_BULK\t16\n#define NAPI_SKB_CACHE_HALF\t(NAPI_SKB_CACHE_SIZE / 2)\n\n#if PAGE_SIZE == SZ_4K\n\n#define NAPI_HAS_SMALL_PAGE_FRAG\t1\n#define NAPI_SMALL_PAGE_PFMEMALLOC(nc)\t((nc).pfmemalloc)\n\n \n\nstruct page_frag_1k {\n\tvoid *va;\n\tu16 offset;\n\tbool pfmemalloc;\n};\n\nstatic void *page_frag_alloc_1k(struct page_frag_1k *nc, gfp_t gfp)\n{\n\tstruct page *page;\n\tint offset;\n\n\toffset = nc->offset - SZ_1K;\n\tif (likely(offset >= 0))\n\t\tgoto use_frag;\n\n\tpage = alloc_pages_node(NUMA_NO_NODE, gfp, 0);\n\tif (!page)\n\t\treturn NULL;\n\n\tnc->va = page_address(page);\n\tnc->pfmemalloc = page_is_pfmemalloc(page);\n\toffset = PAGE_SIZE - SZ_1K;\n\tpage_ref_add(page, offset / SZ_1K);\n\nuse_frag:\n\tnc->offset = offset;\n\treturn nc->va + offset;\n}\n#else\n\n \n#define NAPI_HAS_SMALL_PAGE_FRAG\t0\n#define NAPI_SMALL_PAGE_PFMEMALLOC(nc)\tfalse\n\nstruct page_frag_1k {\n};\n\nstatic void *page_frag_alloc_1k(struct page_frag_1k *nc, gfp_t gfp_mask)\n{\n\treturn NULL;\n}\n\n#endif\n\nstruct napi_alloc_cache {\n\tstruct page_frag_cache page;\n\tstruct page_frag_1k page_small;\n\tunsigned int skb_count;\n\tvoid *skb_cache[NAPI_SKB_CACHE_SIZE];\n};\n\nstatic DEFINE_PER_CPU(struct page_frag_cache, netdev_alloc_cache);\nstatic DEFINE_PER_CPU(struct napi_alloc_cache, napi_alloc_cache);\n\n \nvoid napi_get_frags_check(struct napi_struct *napi)\n{\n\tstruct sk_buff *skb;\n\n\tlocal_bh_disable();\n\tskb = napi_get_frags(napi);\n\tWARN_ON_ONCE(!NAPI_HAS_SMALL_PAGE_FRAG && skb && skb->head_frag);\n\tnapi_free_frags(napi);\n\tlocal_bh_enable();\n}\n\nvoid *__napi_alloc_frag_align(unsigned int fragsz, unsigned int align_mask)\n{\n\tstruct napi_alloc_cache *nc = this_cpu_ptr(&napi_alloc_cache);\n\n\tfragsz = SKB_DATA_ALIGN(fragsz);\n\n\treturn page_frag_alloc_align(&nc->page, fragsz, GFP_ATOMIC, align_mask);\n}\nEXPORT_SYMBOL(__napi_alloc_frag_align);\n\nvoid *__netdev_alloc_frag_align(unsigned int fragsz, unsigned int align_mask)\n{\n\tvoid *data;\n\n\tfragsz = SKB_DATA_ALIGN(fragsz);\n\tif (in_hardirq() || irqs_disabled()) {\n\t\tstruct page_frag_cache *nc = this_cpu_ptr(&netdev_alloc_cache);\n\n\t\tdata = page_frag_alloc_align(nc, fragsz, GFP_ATOMIC, align_mask);\n\t} else {\n\t\tstruct napi_alloc_cache *nc;\n\n\t\tlocal_bh_disable();\n\t\tnc = this_cpu_ptr(&napi_alloc_cache);\n\t\tdata = page_frag_alloc_align(&nc->page, fragsz, GFP_ATOMIC, align_mask);\n\t\tlocal_bh_enable();\n\t}\n\treturn data;\n}\nEXPORT_SYMBOL(__netdev_alloc_frag_align);\n\nstatic struct sk_buff *napi_skb_cache_get(void)\n{\n\tstruct napi_alloc_cache *nc = this_cpu_ptr(&napi_alloc_cache);\n\tstruct sk_buff *skb;\n\n\tif (unlikely(!nc->skb_count)) {\n\t\tnc->skb_count = kmem_cache_alloc_bulk(skbuff_cache,\n\t\t\t\t\t\t      GFP_ATOMIC,\n\t\t\t\t\t\t      NAPI_SKB_CACHE_BULK,\n\t\t\t\t\t\t      nc->skb_cache);\n\t\tif (unlikely(!nc->skb_count))\n\t\t\treturn NULL;\n\t}\n\n\tskb = nc->skb_cache[--nc->skb_count];\n\tkasan_unpoison_object_data(skbuff_cache, skb);\n\n\treturn skb;\n}\n\nstatic inline void __finalize_skb_around(struct sk_buff *skb, void *data,\n\t\t\t\t\t unsigned int size)\n{\n\tstruct skb_shared_info *shinfo;\n\n\tsize -= SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\n\t \n\tskb->truesize = SKB_TRUESIZE(size);\n\trefcount_set(&skb->users, 1);\n\tskb->head = data;\n\tskb->data = data;\n\tskb_reset_tail_pointer(skb);\n\tskb_set_end_offset(skb, size);\n\tskb->mac_header = (typeof(skb->mac_header))~0U;\n\tskb->transport_header = (typeof(skb->transport_header))~0U;\n\tskb->alloc_cpu = raw_smp_processor_id();\n\t \n\tshinfo = skb_shinfo(skb);\n\tmemset(shinfo, 0, offsetof(struct skb_shared_info, dataref));\n\tatomic_set(&shinfo->dataref, 1);\n\n\tskb_set_kcov_handle(skb, kcov_common_handle());\n}\n\nstatic inline void *__slab_build_skb(struct sk_buff *skb, void *data,\n\t\t\t\t     unsigned int *size)\n{\n\tvoid *resized;\n\n\t \n\t*size = ksize(data);\n\t \n\tresized = krealloc(data, *size, GFP_ATOMIC);\n\tWARN_ON_ONCE(resized != data);\n\treturn resized;\n}\n\n \nstruct sk_buff *slab_build_skb(void *data)\n{\n\tstruct sk_buff *skb;\n\tunsigned int size;\n\n\tskb = kmem_cache_alloc(skbuff_cache, GFP_ATOMIC);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\tmemset(skb, 0, offsetof(struct sk_buff, tail));\n\tdata = __slab_build_skb(skb, data, &size);\n\t__finalize_skb_around(skb, data, size);\n\n\treturn skb;\n}\nEXPORT_SYMBOL(slab_build_skb);\n\n \nstatic void __build_skb_around(struct sk_buff *skb, void *data,\n\t\t\t       unsigned int frag_size)\n{\n\tunsigned int size = frag_size;\n\n\t \n\tif (WARN_ONCE(size == 0, \"Use slab_build_skb() instead\"))\n\t\tdata = __slab_build_skb(skb, data, &size);\n\n\t__finalize_skb_around(skb, data, size);\n}\n\n \nstruct sk_buff *__build_skb(void *data, unsigned int frag_size)\n{\n\tstruct sk_buff *skb;\n\n\tskb = kmem_cache_alloc(skbuff_cache, GFP_ATOMIC);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\tmemset(skb, 0, offsetof(struct sk_buff, tail));\n\t__build_skb_around(skb, data, frag_size);\n\n\treturn skb;\n}\n\n \nstruct sk_buff *build_skb(void *data, unsigned int frag_size)\n{\n\tstruct sk_buff *skb = __build_skb(data, frag_size);\n\n\tif (likely(skb && frag_size)) {\n\t\tskb->head_frag = 1;\n\t\tskb_propagate_pfmemalloc(virt_to_head_page(data), skb);\n\t}\n\treturn skb;\n}\nEXPORT_SYMBOL(build_skb);\n\n \nstruct sk_buff *build_skb_around(struct sk_buff *skb,\n\t\t\t\t void *data, unsigned int frag_size)\n{\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\t__build_skb_around(skb, data, frag_size);\n\n\tif (frag_size) {\n\t\tskb->head_frag = 1;\n\t\tskb_propagate_pfmemalloc(virt_to_head_page(data), skb);\n\t}\n\treturn skb;\n}\nEXPORT_SYMBOL(build_skb_around);\n\n \nstatic struct sk_buff *__napi_build_skb(void *data, unsigned int frag_size)\n{\n\tstruct sk_buff *skb;\n\n\tskb = napi_skb_cache_get();\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\tmemset(skb, 0, offsetof(struct sk_buff, tail));\n\t__build_skb_around(skb, data, frag_size);\n\n\treturn skb;\n}\n\n \nstruct sk_buff *napi_build_skb(void *data, unsigned int frag_size)\n{\n\tstruct sk_buff *skb = __napi_build_skb(data, frag_size);\n\n\tif (likely(skb) && frag_size) {\n\t\tskb->head_frag = 1;\n\t\tskb_propagate_pfmemalloc(virt_to_head_page(data), skb);\n\t}\n\n\treturn skb;\n}\nEXPORT_SYMBOL(napi_build_skb);\n\n \nstatic void *kmalloc_reserve(unsigned int *size, gfp_t flags, int node,\n\t\t\t     bool *pfmemalloc)\n{\n\tbool ret_pfmemalloc = false;\n\tsize_t obj_size;\n\tvoid *obj;\n\n\tobj_size = SKB_HEAD_ALIGN(*size);\n\tif (obj_size <= SKB_SMALL_HEAD_CACHE_SIZE &&\n\t    !(flags & KMALLOC_NOT_NORMAL_BITS)) {\n\t\tobj = kmem_cache_alloc_node(skb_small_head_cache,\n\t\t\t\tflags | __GFP_NOMEMALLOC | __GFP_NOWARN,\n\t\t\t\tnode);\n\t\t*size = SKB_SMALL_HEAD_CACHE_SIZE;\n\t\tif (obj || !(gfp_pfmemalloc_allowed(flags)))\n\t\t\tgoto out;\n\t\t \n\t\tret_pfmemalloc = true;\n\t\tobj = kmem_cache_alloc_node(skb_small_head_cache, flags, node);\n\t\tgoto out;\n\t}\n\n\tobj_size = kmalloc_size_roundup(obj_size);\n\t \n\t*size = (unsigned int)obj_size;\n\n\t \n\tobj = kmalloc_node_track_caller(obj_size,\n\t\t\t\t\tflags | __GFP_NOMEMALLOC | __GFP_NOWARN,\n\t\t\t\t\tnode);\n\tif (obj || !(gfp_pfmemalloc_allowed(flags)))\n\t\tgoto out;\n\n\t \n\tret_pfmemalloc = true;\n\tobj = kmalloc_node_track_caller(obj_size, flags, node);\n\nout:\n\tif (pfmemalloc)\n\t\t*pfmemalloc = ret_pfmemalloc;\n\n\treturn obj;\n}\n\n \n\n \nstruct sk_buff *__alloc_skb(unsigned int size, gfp_t gfp_mask,\n\t\t\t    int flags, int node)\n{\n\tstruct kmem_cache *cache;\n\tstruct sk_buff *skb;\n\tbool pfmemalloc;\n\tu8 *data;\n\n\tcache = (flags & SKB_ALLOC_FCLONE)\n\t\t? skbuff_fclone_cache : skbuff_cache;\n\n\tif (sk_memalloc_socks() && (flags & SKB_ALLOC_RX))\n\t\tgfp_mask |= __GFP_MEMALLOC;\n\n\t \n\tif ((flags & (SKB_ALLOC_FCLONE | SKB_ALLOC_NAPI)) == SKB_ALLOC_NAPI &&\n\t    likely(node == NUMA_NO_NODE || node == numa_mem_id()))\n\t\tskb = napi_skb_cache_get();\n\telse\n\t\tskb = kmem_cache_alloc_node(cache, gfp_mask & ~GFP_DMA, node);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\tprefetchw(skb);\n\n\t \n\tdata = kmalloc_reserve(&size, gfp_mask, node, &pfmemalloc);\n\tif (unlikely(!data))\n\t\tgoto nodata;\n\t \n\tprefetchw(data + SKB_WITH_OVERHEAD(size));\n\n\t \n\tmemset(skb, 0, offsetof(struct sk_buff, tail));\n\t__build_skb_around(skb, data, size);\n\tskb->pfmemalloc = pfmemalloc;\n\n\tif (flags & SKB_ALLOC_FCLONE) {\n\t\tstruct sk_buff_fclones *fclones;\n\n\t\tfclones = container_of(skb, struct sk_buff_fclones, skb1);\n\n\t\tskb->fclone = SKB_FCLONE_ORIG;\n\t\trefcount_set(&fclones->fclone_ref, 1);\n\t}\n\n\treturn skb;\n\nnodata:\n\tkmem_cache_free(cache, skb);\n\treturn NULL;\n}\nEXPORT_SYMBOL(__alloc_skb);\n\n \nstruct sk_buff *__netdev_alloc_skb(struct net_device *dev, unsigned int len,\n\t\t\t\t   gfp_t gfp_mask)\n{\n\tstruct page_frag_cache *nc;\n\tstruct sk_buff *skb;\n\tbool pfmemalloc;\n\tvoid *data;\n\n\tlen += NET_SKB_PAD;\n\n\t \n\tif (len <= SKB_WITH_OVERHEAD(1024) ||\n\t    len > SKB_WITH_OVERHEAD(PAGE_SIZE) ||\n\t    (gfp_mask & (__GFP_DIRECT_RECLAIM | GFP_DMA))) {\n\t\tskb = __alloc_skb(len, gfp_mask, SKB_ALLOC_RX, NUMA_NO_NODE);\n\t\tif (!skb)\n\t\t\tgoto skb_fail;\n\t\tgoto skb_success;\n\t}\n\n\tlen = SKB_HEAD_ALIGN(len);\n\n\tif (sk_memalloc_socks())\n\t\tgfp_mask |= __GFP_MEMALLOC;\n\n\tif (in_hardirq() || irqs_disabled()) {\n\t\tnc = this_cpu_ptr(&netdev_alloc_cache);\n\t\tdata = page_frag_alloc(nc, len, gfp_mask);\n\t\tpfmemalloc = nc->pfmemalloc;\n\t} else {\n\t\tlocal_bh_disable();\n\t\tnc = this_cpu_ptr(&napi_alloc_cache.page);\n\t\tdata = page_frag_alloc(nc, len, gfp_mask);\n\t\tpfmemalloc = nc->pfmemalloc;\n\t\tlocal_bh_enable();\n\t}\n\n\tif (unlikely(!data))\n\t\treturn NULL;\n\n\tskb = __build_skb(data, len);\n\tif (unlikely(!skb)) {\n\t\tskb_free_frag(data);\n\t\treturn NULL;\n\t}\n\n\tif (pfmemalloc)\n\t\tskb->pfmemalloc = 1;\n\tskb->head_frag = 1;\n\nskb_success:\n\tskb_reserve(skb, NET_SKB_PAD);\n\tskb->dev = dev;\n\nskb_fail:\n\treturn skb;\n}\nEXPORT_SYMBOL(__netdev_alloc_skb);\n\n \nstruct sk_buff *__napi_alloc_skb(struct napi_struct *napi, unsigned int len,\n\t\t\t\t gfp_t gfp_mask)\n{\n\tstruct napi_alloc_cache *nc;\n\tstruct sk_buff *skb;\n\tbool pfmemalloc;\n\tvoid *data;\n\n\tDEBUG_NET_WARN_ON_ONCE(!in_softirq());\n\tlen += NET_SKB_PAD + NET_IP_ALIGN;\n\n\t \n\tif ((!NAPI_HAS_SMALL_PAGE_FRAG && len <= SKB_WITH_OVERHEAD(1024)) ||\n\t    len > SKB_WITH_OVERHEAD(PAGE_SIZE) ||\n\t    (gfp_mask & (__GFP_DIRECT_RECLAIM | GFP_DMA))) {\n\t\tskb = __alloc_skb(len, gfp_mask, SKB_ALLOC_RX | SKB_ALLOC_NAPI,\n\t\t\t\t  NUMA_NO_NODE);\n\t\tif (!skb)\n\t\t\tgoto skb_fail;\n\t\tgoto skb_success;\n\t}\n\n\tnc = this_cpu_ptr(&napi_alloc_cache);\n\n\tif (sk_memalloc_socks())\n\t\tgfp_mask |= __GFP_MEMALLOC;\n\n\tif (NAPI_HAS_SMALL_PAGE_FRAG && len <= SKB_WITH_OVERHEAD(1024)) {\n\t\t \n\t\tlen = SZ_1K;\n\n\t\tdata = page_frag_alloc_1k(&nc->page_small, gfp_mask);\n\t\tpfmemalloc = NAPI_SMALL_PAGE_PFMEMALLOC(nc->page_small);\n\t} else {\n\t\tlen = SKB_HEAD_ALIGN(len);\n\n\t\tdata = page_frag_alloc(&nc->page, len, gfp_mask);\n\t\tpfmemalloc = nc->page.pfmemalloc;\n\t}\n\n\tif (unlikely(!data))\n\t\treturn NULL;\n\n\tskb = __napi_build_skb(data, len);\n\tif (unlikely(!skb)) {\n\t\tskb_free_frag(data);\n\t\treturn NULL;\n\t}\n\n\tif (pfmemalloc)\n\t\tskb->pfmemalloc = 1;\n\tskb->head_frag = 1;\n\nskb_success:\n\tskb_reserve(skb, NET_SKB_PAD + NET_IP_ALIGN);\n\tskb->dev = napi->dev;\n\nskb_fail:\n\treturn skb;\n}\nEXPORT_SYMBOL(__napi_alloc_skb);\n\nvoid skb_add_rx_frag(struct sk_buff *skb, int i, struct page *page, int off,\n\t\t     int size, unsigned int truesize)\n{\n\tskb_fill_page_desc(skb, i, page, off, size);\n\tskb->len += size;\n\tskb->data_len += size;\n\tskb->truesize += truesize;\n}\nEXPORT_SYMBOL(skb_add_rx_frag);\n\nvoid skb_coalesce_rx_frag(struct sk_buff *skb, int i, int size,\n\t\t\t  unsigned int truesize)\n{\n\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\tskb_frag_size_add(frag, size);\n\tskb->len += size;\n\tskb->data_len += size;\n\tskb->truesize += truesize;\n}\nEXPORT_SYMBOL(skb_coalesce_rx_frag);\n\nstatic void skb_drop_list(struct sk_buff **listp)\n{\n\tkfree_skb_list(*listp);\n\t*listp = NULL;\n}\n\nstatic inline void skb_drop_fraglist(struct sk_buff *skb)\n{\n\tskb_drop_list(&skb_shinfo(skb)->frag_list);\n}\n\nstatic void skb_clone_fraglist(struct sk_buff *skb)\n{\n\tstruct sk_buff *list;\n\n\tskb_walk_frags(skb, list)\n\t\tskb_get(list);\n}\n\n#if IS_ENABLED(CONFIG_PAGE_POOL)\nbool napi_pp_put_page(struct page *page, bool napi_safe)\n{\n\tbool allow_direct = false;\n\tstruct page_pool *pp;\n\n\tpage = compound_head(page);\n\n\t \n\tif (unlikely((page->pp_magic & ~0x3UL) != PP_SIGNATURE))\n\t\treturn false;\n\n\tpp = page->pp;\n\n\t \n\tif (napi_safe || in_softirq()) {\n\t\tconst struct napi_struct *napi = READ_ONCE(pp->p.napi);\n\n\t\tallow_direct = napi &&\n\t\t\tREAD_ONCE(napi->list_owner) == smp_processor_id();\n\t}\n\n\t \n\tpage_pool_put_full_page(pp, page, allow_direct);\n\n\treturn true;\n}\nEXPORT_SYMBOL(napi_pp_put_page);\n#endif\n\nstatic bool skb_pp_recycle(struct sk_buff *skb, void *data, bool napi_safe)\n{\n\tif (!IS_ENABLED(CONFIG_PAGE_POOL) || !skb->pp_recycle)\n\t\treturn false;\n\treturn napi_pp_put_page(virt_to_page(data), napi_safe);\n}\n\nstatic void skb_kfree_head(void *head, unsigned int end_offset)\n{\n\tif (end_offset == SKB_SMALL_HEAD_HEADROOM)\n\t\tkmem_cache_free(skb_small_head_cache, head);\n\telse\n\t\tkfree(head);\n}\n\nstatic void skb_free_head(struct sk_buff *skb, bool napi_safe)\n{\n\tunsigned char *head = skb->head;\n\n\tif (skb->head_frag) {\n\t\tif (skb_pp_recycle(skb, head, napi_safe))\n\t\t\treturn;\n\t\tskb_free_frag(head);\n\t} else {\n\t\tskb_kfree_head(head, skb_end_offset(skb));\n\t}\n}\n\nstatic void skb_release_data(struct sk_buff *skb, enum skb_drop_reason reason,\n\t\t\t     bool napi_safe)\n{\n\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\tint i;\n\n\tif (skb->cloned &&\n\t    atomic_sub_return(skb->nohdr ? (1 << SKB_DATAREF_SHIFT) + 1 : 1,\n\t\t\t      &shinfo->dataref))\n\t\tgoto exit;\n\n\tif (skb_zcopy(skb)) {\n\t\tbool skip_unref = shinfo->flags & SKBFL_MANAGED_FRAG_REFS;\n\n\t\tskb_zcopy_clear(skb, true);\n\t\tif (skip_unref)\n\t\t\tgoto free_head;\n\t}\n\n\tfor (i = 0; i < shinfo->nr_frags; i++)\n\t\tnapi_frag_unref(&shinfo->frags[i], skb->pp_recycle, napi_safe);\n\nfree_head:\n\tif (shinfo->frag_list)\n\t\tkfree_skb_list_reason(shinfo->frag_list, reason);\n\n\tskb_free_head(skb, napi_safe);\nexit:\n\t \n\tskb->pp_recycle = 0;\n}\n\n \nstatic void kfree_skbmem(struct sk_buff *skb)\n{\n\tstruct sk_buff_fclones *fclones;\n\n\tswitch (skb->fclone) {\n\tcase SKB_FCLONE_UNAVAILABLE:\n\t\tkmem_cache_free(skbuff_cache, skb);\n\t\treturn;\n\n\tcase SKB_FCLONE_ORIG:\n\t\tfclones = container_of(skb, struct sk_buff_fclones, skb1);\n\n\t\t \n\t\tif (refcount_read(&fclones->fclone_ref) == 1)\n\t\t\tgoto fastpath;\n\t\tbreak;\n\n\tdefault:  \n\t\tfclones = container_of(skb, struct sk_buff_fclones, skb2);\n\t\tbreak;\n\t}\n\tif (!refcount_dec_and_test(&fclones->fclone_ref))\n\t\treturn;\nfastpath:\n\tkmem_cache_free(skbuff_fclone_cache, fclones);\n}\n\nvoid skb_release_head_state(struct sk_buff *skb)\n{\n\tskb_dst_drop(skb);\n\tif (skb->destructor) {\n\t\tDEBUG_NET_WARN_ON_ONCE(in_hardirq());\n\t\tskb->destructor(skb);\n\t}\n#if IS_ENABLED(CONFIG_NF_CONNTRACK)\n\tnf_conntrack_put(skb_nfct(skb));\n#endif\n\tskb_ext_put(skb);\n}\n\n \nstatic void skb_release_all(struct sk_buff *skb, enum skb_drop_reason reason,\n\t\t\t    bool napi_safe)\n{\n\tskb_release_head_state(skb);\n\tif (likely(skb->head))\n\t\tskb_release_data(skb, reason, napi_safe);\n}\n\n \n\nvoid __kfree_skb(struct sk_buff *skb)\n{\n\tskb_release_all(skb, SKB_DROP_REASON_NOT_SPECIFIED, false);\n\tkfree_skbmem(skb);\n}\nEXPORT_SYMBOL(__kfree_skb);\n\nstatic __always_inline\nbool __kfree_skb_reason(struct sk_buff *skb, enum skb_drop_reason reason)\n{\n\tif (unlikely(!skb_unref(skb)))\n\t\treturn false;\n\n\tDEBUG_NET_WARN_ON_ONCE(reason == SKB_NOT_DROPPED_YET ||\n\t\t\t       u32_get_bits(reason,\n\t\t\t\t\t    SKB_DROP_REASON_SUBSYS_MASK) >=\n\t\t\t\tSKB_DROP_REASON_SUBSYS_NUM);\n\n\tif (reason == SKB_CONSUMED)\n\t\ttrace_consume_skb(skb, __builtin_return_address(0));\n\telse\n\t\ttrace_kfree_skb(skb, __builtin_return_address(0), reason);\n\treturn true;\n}\n\n \nvoid __fix_address\nkfree_skb_reason(struct sk_buff *skb, enum skb_drop_reason reason)\n{\n\tif (__kfree_skb_reason(skb, reason))\n\t\t__kfree_skb(skb);\n}\nEXPORT_SYMBOL(kfree_skb_reason);\n\n#define KFREE_SKB_BULK_SIZE\t16\n\nstruct skb_free_array {\n\tunsigned int skb_count;\n\tvoid *skb_array[KFREE_SKB_BULK_SIZE];\n};\n\nstatic void kfree_skb_add_bulk(struct sk_buff *skb,\n\t\t\t       struct skb_free_array *sa,\n\t\t\t       enum skb_drop_reason reason)\n{\n\t \n\tif (unlikely(skb->fclone != SKB_FCLONE_UNAVAILABLE)) {\n\t\t__kfree_skb(skb);\n\t\treturn;\n\t}\n\n\tskb_release_all(skb, reason, false);\n\tsa->skb_array[sa->skb_count++] = skb;\n\n\tif (unlikely(sa->skb_count == KFREE_SKB_BULK_SIZE)) {\n\t\tkmem_cache_free_bulk(skbuff_cache, KFREE_SKB_BULK_SIZE,\n\t\t\t\t     sa->skb_array);\n\t\tsa->skb_count = 0;\n\t}\n}\n\nvoid __fix_address\nkfree_skb_list_reason(struct sk_buff *segs, enum skb_drop_reason reason)\n{\n\tstruct skb_free_array sa;\n\n\tsa.skb_count = 0;\n\n\twhile (segs) {\n\t\tstruct sk_buff *next = segs->next;\n\n\t\tif (__kfree_skb_reason(segs, reason)) {\n\t\t\tskb_poison_list(segs);\n\t\t\tkfree_skb_add_bulk(segs, &sa, reason);\n\t\t}\n\n\t\tsegs = next;\n\t}\n\n\tif (sa.skb_count)\n\t\tkmem_cache_free_bulk(skbuff_cache, sa.skb_count, sa.skb_array);\n}\nEXPORT_SYMBOL(kfree_skb_list_reason);\n\n \nvoid skb_dump(const char *level, const struct sk_buff *skb, bool full_pkt)\n{\n\tstruct skb_shared_info *sh = skb_shinfo(skb);\n\tstruct net_device *dev = skb->dev;\n\tstruct sock *sk = skb->sk;\n\tstruct sk_buff *list_skb;\n\tbool has_mac, has_trans;\n\tint headroom, tailroom;\n\tint i, len, seg_len;\n\n\tif (full_pkt)\n\t\tlen = skb->len;\n\telse\n\t\tlen = min_t(int, skb->len, MAX_HEADER + 128);\n\n\theadroom = skb_headroom(skb);\n\ttailroom = skb_tailroom(skb);\n\n\thas_mac = skb_mac_header_was_set(skb);\n\thas_trans = skb_transport_header_was_set(skb);\n\n\tprintk(\"%sskb len=%u headroom=%u headlen=%u tailroom=%u\\n\"\n\t       \"mac=(%d,%d) net=(%d,%d) trans=%d\\n\"\n\t       \"shinfo(txflags=%u nr_frags=%u gso(size=%hu type=%u segs=%hu))\\n\"\n\t       \"csum(0x%x ip_summed=%u complete_sw=%u valid=%u level=%u)\\n\"\n\t       \"hash(0x%x sw=%u l4=%u) proto=0x%04x pkttype=%u iif=%d\\n\",\n\t       level, skb->len, headroom, skb_headlen(skb), tailroom,\n\t       has_mac ? skb->mac_header : -1,\n\t       has_mac ? skb_mac_header_len(skb) : -1,\n\t       skb->network_header,\n\t       has_trans ? skb_network_header_len(skb) : -1,\n\t       has_trans ? skb->transport_header : -1,\n\t       sh->tx_flags, sh->nr_frags,\n\t       sh->gso_size, sh->gso_type, sh->gso_segs,\n\t       skb->csum, skb->ip_summed, skb->csum_complete_sw,\n\t       skb->csum_valid, skb->csum_level,\n\t       skb->hash, skb->sw_hash, skb->l4_hash,\n\t       ntohs(skb->protocol), skb->pkt_type, skb->skb_iif);\n\n\tif (dev)\n\t\tprintk(\"%sdev name=%s feat=%pNF\\n\",\n\t\t       level, dev->name, &dev->features);\n\tif (sk)\n\t\tprintk(\"%ssk family=%hu type=%u proto=%u\\n\",\n\t\t       level, sk->sk_family, sk->sk_type, sk->sk_protocol);\n\n\tif (full_pkt && headroom)\n\t\tprint_hex_dump(level, \"skb headroom: \", DUMP_PREFIX_OFFSET,\n\t\t\t       16, 1, skb->head, headroom, false);\n\n\tseg_len = min_t(int, skb_headlen(skb), len);\n\tif (seg_len)\n\t\tprint_hex_dump(level, \"skb linear:   \", DUMP_PREFIX_OFFSET,\n\t\t\t       16, 1, skb->data, seg_len, false);\n\tlen -= seg_len;\n\n\tif (full_pkt && tailroom)\n\t\tprint_hex_dump(level, \"skb tailroom: \", DUMP_PREFIX_OFFSET,\n\t\t\t       16, 1, skb_tail_pointer(skb), tailroom, false);\n\n\tfor (i = 0; len && i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\t\tu32 p_off, p_len, copied;\n\t\tstruct page *p;\n\t\tu8 *vaddr;\n\n\t\tskb_frag_foreach_page(frag, skb_frag_off(frag),\n\t\t\t\t      skb_frag_size(frag), p, p_off, p_len,\n\t\t\t\t      copied) {\n\t\t\tseg_len = min_t(int, p_len, len);\n\t\t\tvaddr = kmap_atomic(p);\n\t\t\tprint_hex_dump(level, \"skb frag:     \",\n\t\t\t\t       DUMP_PREFIX_OFFSET,\n\t\t\t\t       16, 1, vaddr + p_off, seg_len, false);\n\t\t\tkunmap_atomic(vaddr);\n\t\t\tlen -= seg_len;\n\t\t\tif (!len)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (full_pkt && skb_has_frag_list(skb)) {\n\t\tprintk(\"skb fraglist:\\n\");\n\t\tskb_walk_frags(skb, list_skb)\n\t\t\tskb_dump(level, list_skb, true);\n\t}\n}\nEXPORT_SYMBOL(skb_dump);\n\n \nvoid skb_tx_error(struct sk_buff *skb)\n{\n\tif (skb) {\n\t\tskb_zcopy_downgrade_managed(skb);\n\t\tskb_zcopy_clear(skb, true);\n\t}\n}\nEXPORT_SYMBOL(skb_tx_error);\n\n#ifdef CONFIG_TRACEPOINTS\n \nvoid consume_skb(struct sk_buff *skb)\n{\n\tif (!skb_unref(skb))\n\t\treturn;\n\n\ttrace_consume_skb(skb, __builtin_return_address(0));\n\t__kfree_skb(skb);\n}\nEXPORT_SYMBOL(consume_skb);\n#endif\n\n \nvoid __consume_stateless_skb(struct sk_buff *skb)\n{\n\ttrace_consume_skb(skb, __builtin_return_address(0));\n\tskb_release_data(skb, SKB_CONSUMED, false);\n\tkfree_skbmem(skb);\n}\n\nstatic void napi_skb_cache_put(struct sk_buff *skb)\n{\n\tstruct napi_alloc_cache *nc = this_cpu_ptr(&napi_alloc_cache);\n\tu32 i;\n\n\tkasan_poison_object_data(skbuff_cache, skb);\n\tnc->skb_cache[nc->skb_count++] = skb;\n\n\tif (unlikely(nc->skb_count == NAPI_SKB_CACHE_SIZE)) {\n\t\tfor (i = NAPI_SKB_CACHE_HALF; i < NAPI_SKB_CACHE_SIZE; i++)\n\t\t\tkasan_unpoison_object_data(skbuff_cache,\n\t\t\t\t\t\t   nc->skb_cache[i]);\n\n\t\tkmem_cache_free_bulk(skbuff_cache, NAPI_SKB_CACHE_HALF,\n\t\t\t\t     nc->skb_cache + NAPI_SKB_CACHE_HALF);\n\t\tnc->skb_count = NAPI_SKB_CACHE_HALF;\n\t}\n}\n\nvoid __napi_kfree_skb(struct sk_buff *skb, enum skb_drop_reason reason)\n{\n\tskb_release_all(skb, reason, true);\n\tnapi_skb_cache_put(skb);\n}\n\nvoid napi_skb_free_stolen_head(struct sk_buff *skb)\n{\n\tif (unlikely(skb->slow_gro)) {\n\t\tnf_reset_ct(skb);\n\t\tskb_dst_drop(skb);\n\t\tskb_ext_put(skb);\n\t\tskb_orphan(skb);\n\t\tskb->slow_gro = 0;\n\t}\n\tnapi_skb_cache_put(skb);\n}\n\nvoid napi_consume_skb(struct sk_buff *skb, int budget)\n{\n\t \n\tif (unlikely(!budget)) {\n\t\tdev_consume_skb_any(skb);\n\t\treturn;\n\t}\n\n\tDEBUG_NET_WARN_ON_ONCE(!in_softirq());\n\n\tif (!skb_unref(skb))\n\t\treturn;\n\n\t \n\ttrace_consume_skb(skb, __builtin_return_address(0));\n\n\t \n\tif (skb->fclone != SKB_FCLONE_UNAVAILABLE) {\n\t\t__kfree_skb(skb);\n\t\treturn;\n\t}\n\n\tskb_release_all(skb, SKB_CONSUMED, !!budget);\n\tnapi_skb_cache_put(skb);\n}\nEXPORT_SYMBOL(napi_consume_skb);\n\n \n#define CHECK_SKB_FIELD(field) \\\n\tBUILD_BUG_ON(offsetof(struct sk_buff, field) !=\t\t\\\n\t\t     offsetof(struct sk_buff, headers.field));\t\\\n\nstatic void __copy_skb_header(struct sk_buff *new, const struct sk_buff *old)\n{\n\tnew->tstamp\t\t= old->tstamp;\n\t \n\tnew->dev\t\t= old->dev;\n\tmemcpy(new->cb, old->cb, sizeof(old->cb));\n\tskb_dst_copy(new, old);\n\t__skb_ext_copy(new, old);\n\t__nf_copy(new, old, false);\n\n\t \n\tnew->queue_mapping = old->queue_mapping;\n\n\tmemcpy(&new->headers, &old->headers, sizeof(new->headers));\n\tCHECK_SKB_FIELD(protocol);\n\tCHECK_SKB_FIELD(csum);\n\tCHECK_SKB_FIELD(hash);\n\tCHECK_SKB_FIELD(priority);\n\tCHECK_SKB_FIELD(skb_iif);\n\tCHECK_SKB_FIELD(vlan_proto);\n\tCHECK_SKB_FIELD(vlan_tci);\n\tCHECK_SKB_FIELD(transport_header);\n\tCHECK_SKB_FIELD(network_header);\n\tCHECK_SKB_FIELD(mac_header);\n\tCHECK_SKB_FIELD(inner_protocol);\n\tCHECK_SKB_FIELD(inner_transport_header);\n\tCHECK_SKB_FIELD(inner_network_header);\n\tCHECK_SKB_FIELD(inner_mac_header);\n\tCHECK_SKB_FIELD(mark);\n#ifdef CONFIG_NETWORK_SECMARK\n\tCHECK_SKB_FIELD(secmark);\n#endif\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tCHECK_SKB_FIELD(napi_id);\n#endif\n\tCHECK_SKB_FIELD(alloc_cpu);\n#ifdef CONFIG_XPS\n\tCHECK_SKB_FIELD(sender_cpu);\n#endif\n#ifdef CONFIG_NET_SCHED\n\tCHECK_SKB_FIELD(tc_index);\n#endif\n\n}\n\n \nstatic struct sk_buff *__skb_clone(struct sk_buff *n, struct sk_buff *skb)\n{\n#define C(x) n->x = skb->x\n\n\tn->next = n->prev = NULL;\n\tn->sk = NULL;\n\t__copy_skb_header(n, skb);\n\n\tC(len);\n\tC(data_len);\n\tC(mac_len);\n\tn->hdr_len = skb->nohdr ? skb_headroom(skb) : skb->hdr_len;\n\tn->cloned = 1;\n\tn->nohdr = 0;\n\tn->peeked = 0;\n\tC(pfmemalloc);\n\tC(pp_recycle);\n\tn->destructor = NULL;\n\tC(tail);\n\tC(end);\n\tC(head);\n\tC(head_frag);\n\tC(data);\n\tC(truesize);\n\trefcount_set(&n->users, 1);\n\n\tatomic_inc(&(skb_shinfo(skb)->dataref));\n\tskb->cloned = 1;\n\n\treturn n;\n#undef C\n}\n\n \nstruct sk_buff *alloc_skb_for_msg(struct sk_buff *first)\n{\n\tstruct sk_buff *n;\n\n\tn = alloc_skb(0, GFP_ATOMIC);\n\tif (!n)\n\t\treturn NULL;\n\n\tn->len = first->len;\n\tn->data_len = first->len;\n\tn->truesize = first->truesize;\n\n\tskb_shinfo(n)->frag_list = first;\n\n\t__copy_skb_header(n, first);\n\tn->destructor = NULL;\n\n\treturn n;\n}\nEXPORT_SYMBOL_GPL(alloc_skb_for_msg);\n\n \nstruct sk_buff *skb_morph(struct sk_buff *dst, struct sk_buff *src)\n{\n\tskb_release_all(dst, SKB_CONSUMED, false);\n\treturn __skb_clone(dst, src);\n}\nEXPORT_SYMBOL_GPL(skb_morph);\n\nint mm_account_pinned_pages(struct mmpin *mmp, size_t size)\n{\n\tunsigned long max_pg, num_pg, new_pg, old_pg, rlim;\n\tstruct user_struct *user;\n\n\tif (capable(CAP_IPC_LOCK) || !size)\n\t\treturn 0;\n\n\trlim = rlimit(RLIMIT_MEMLOCK);\n\tif (rlim == RLIM_INFINITY)\n\t\treturn 0;\n\n\tnum_pg = (size >> PAGE_SHIFT) + 2;\t \n\tmax_pg = rlim >> PAGE_SHIFT;\n\tuser = mmp->user ? : current_user();\n\n\told_pg = atomic_long_read(&user->locked_vm);\n\tdo {\n\t\tnew_pg = old_pg + num_pg;\n\t\tif (new_pg > max_pg)\n\t\t\treturn -ENOBUFS;\n\t} while (!atomic_long_try_cmpxchg(&user->locked_vm, &old_pg, new_pg));\n\n\tif (!mmp->user) {\n\t\tmmp->user = get_uid(user);\n\t\tmmp->num_pg = num_pg;\n\t} else {\n\t\tmmp->num_pg += num_pg;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(mm_account_pinned_pages);\n\nvoid mm_unaccount_pinned_pages(struct mmpin *mmp)\n{\n\tif (mmp->user) {\n\t\tatomic_long_sub(mmp->num_pg, &mmp->user->locked_vm);\n\t\tfree_uid(mmp->user);\n\t}\n}\nEXPORT_SYMBOL_GPL(mm_unaccount_pinned_pages);\n\nstatic struct ubuf_info *msg_zerocopy_alloc(struct sock *sk, size_t size)\n{\n\tstruct ubuf_info_msgzc *uarg;\n\tstruct sk_buff *skb;\n\n\tWARN_ON_ONCE(!in_task());\n\n\tskb = sock_omalloc(sk, 0, GFP_KERNEL);\n\tif (!skb)\n\t\treturn NULL;\n\n\tBUILD_BUG_ON(sizeof(*uarg) > sizeof(skb->cb));\n\tuarg = (void *)skb->cb;\n\tuarg->mmp.user = NULL;\n\n\tif (mm_account_pinned_pages(&uarg->mmp, size)) {\n\t\tkfree_skb(skb);\n\t\treturn NULL;\n\t}\n\n\tuarg->ubuf.callback = msg_zerocopy_callback;\n\tuarg->id = ((u32)atomic_inc_return(&sk->sk_zckey)) - 1;\n\tuarg->len = 1;\n\tuarg->bytelen = size;\n\tuarg->zerocopy = 1;\n\tuarg->ubuf.flags = SKBFL_ZEROCOPY_FRAG | SKBFL_DONT_ORPHAN;\n\trefcount_set(&uarg->ubuf.refcnt, 1);\n\tsock_hold(sk);\n\n\treturn &uarg->ubuf;\n}\n\nstatic inline struct sk_buff *skb_from_uarg(struct ubuf_info_msgzc *uarg)\n{\n\treturn container_of((void *)uarg, struct sk_buff, cb);\n}\n\nstruct ubuf_info *msg_zerocopy_realloc(struct sock *sk, size_t size,\n\t\t\t\t       struct ubuf_info *uarg)\n{\n\tif (uarg) {\n\t\tstruct ubuf_info_msgzc *uarg_zc;\n\t\tconst u32 byte_limit = 1 << 19;\t\t \n\t\tu32 bytelen, next;\n\n\t\t \n\t\tif (uarg->callback != msg_zerocopy_callback)\n\t\t\treturn NULL;\n\n\t\t \n\t\tif (!sock_owned_by_user(sk)) {\n\t\t\tWARN_ON_ONCE(1);\n\t\t\treturn NULL;\n\t\t}\n\n\t\tuarg_zc = uarg_to_msgzc(uarg);\n\t\tbytelen = uarg_zc->bytelen + size;\n\t\tif (uarg_zc->len == USHRT_MAX - 1 || bytelen > byte_limit) {\n\t\t\t \n\t\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\t\tgoto new_alloc;\n\t\t\treturn NULL;\n\t\t}\n\n\t\tnext = (u32)atomic_read(&sk->sk_zckey);\n\t\tif ((u32)(uarg_zc->id + uarg_zc->len) == next) {\n\t\t\tif (mm_account_pinned_pages(&uarg_zc->mmp, size))\n\t\t\t\treturn NULL;\n\t\t\tuarg_zc->len++;\n\t\t\tuarg_zc->bytelen = bytelen;\n\t\t\tatomic_set(&sk->sk_zckey, ++next);\n\n\t\t\t \n\t\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\t\tnet_zcopy_get(uarg);\n\n\t\t\treturn uarg;\n\t\t}\n\t}\n\nnew_alloc:\n\treturn msg_zerocopy_alloc(sk, size);\n}\nEXPORT_SYMBOL_GPL(msg_zerocopy_realloc);\n\nstatic bool skb_zerocopy_notify_extend(struct sk_buff *skb, u32 lo, u16 len)\n{\n\tstruct sock_exterr_skb *serr = SKB_EXT_ERR(skb);\n\tu32 old_lo, old_hi;\n\tu64 sum_len;\n\n\told_lo = serr->ee.ee_info;\n\told_hi = serr->ee.ee_data;\n\tsum_len = old_hi - old_lo + 1ULL + len;\n\n\tif (sum_len >= (1ULL << 32))\n\t\treturn false;\n\n\tif (lo != old_hi + 1)\n\t\treturn false;\n\n\tserr->ee.ee_data += len;\n\treturn true;\n}\n\nstatic void __msg_zerocopy_callback(struct ubuf_info_msgzc *uarg)\n{\n\tstruct sk_buff *tail, *skb = skb_from_uarg(uarg);\n\tstruct sock_exterr_skb *serr;\n\tstruct sock *sk = skb->sk;\n\tstruct sk_buff_head *q;\n\tunsigned long flags;\n\tbool is_zerocopy;\n\tu32 lo, hi;\n\tu16 len;\n\n\tmm_unaccount_pinned_pages(&uarg->mmp);\n\n\t \n\tif (!uarg->len || sock_flag(sk, SOCK_DEAD))\n\t\tgoto release;\n\n\tlen = uarg->len;\n\tlo = uarg->id;\n\thi = uarg->id + len - 1;\n\tis_zerocopy = uarg->zerocopy;\n\n\tserr = SKB_EXT_ERR(skb);\n\tmemset(serr, 0, sizeof(*serr));\n\tserr->ee.ee_errno = 0;\n\tserr->ee.ee_origin = SO_EE_ORIGIN_ZEROCOPY;\n\tserr->ee.ee_data = hi;\n\tserr->ee.ee_info = lo;\n\tif (!is_zerocopy)\n\t\tserr->ee.ee_code |= SO_EE_CODE_ZEROCOPY_COPIED;\n\n\tq = &sk->sk_error_queue;\n\tspin_lock_irqsave(&q->lock, flags);\n\ttail = skb_peek_tail(q);\n\tif (!tail || SKB_EXT_ERR(tail)->ee.ee_origin != SO_EE_ORIGIN_ZEROCOPY ||\n\t    !skb_zerocopy_notify_extend(tail, lo, len)) {\n\t\t__skb_queue_tail(q, skb);\n\t\tskb = NULL;\n\t}\n\tspin_unlock_irqrestore(&q->lock, flags);\n\n\tsk_error_report(sk);\n\nrelease:\n\tconsume_skb(skb);\n\tsock_put(sk);\n}\n\nvoid msg_zerocopy_callback(struct sk_buff *skb, struct ubuf_info *uarg,\n\t\t\t   bool success)\n{\n\tstruct ubuf_info_msgzc *uarg_zc = uarg_to_msgzc(uarg);\n\n\tuarg_zc->zerocopy = uarg_zc->zerocopy & success;\n\n\tif (refcount_dec_and_test(&uarg->refcnt))\n\t\t__msg_zerocopy_callback(uarg_zc);\n}\nEXPORT_SYMBOL_GPL(msg_zerocopy_callback);\n\nvoid msg_zerocopy_put_abort(struct ubuf_info *uarg, bool have_uref)\n{\n\tstruct sock *sk = skb_from_uarg(uarg_to_msgzc(uarg))->sk;\n\n\tatomic_dec(&sk->sk_zckey);\n\tuarg_to_msgzc(uarg)->len--;\n\n\tif (have_uref)\n\t\tmsg_zerocopy_callback(NULL, uarg, true);\n}\nEXPORT_SYMBOL_GPL(msg_zerocopy_put_abort);\n\nint skb_zerocopy_iter_stream(struct sock *sk, struct sk_buff *skb,\n\t\t\t     struct msghdr *msg, int len,\n\t\t\t     struct ubuf_info *uarg)\n{\n\tstruct ubuf_info *orig_uarg = skb_zcopy(skb);\n\tint err, orig_len = skb->len;\n\n\t \n\tif (orig_uarg && uarg != orig_uarg)\n\t\treturn -EEXIST;\n\n\terr = __zerocopy_sg_from_iter(msg, sk, skb, &msg->msg_iter, len);\n\tif (err == -EFAULT || (err == -EMSGSIZE && skb->len == orig_len)) {\n\t\tstruct sock *save_sk = skb->sk;\n\n\t\t \n\t\tiov_iter_revert(&msg->msg_iter, skb->len - orig_len);\n\t\tskb->sk = sk;\n\t\t___pskb_trim(skb, orig_len);\n\t\tskb->sk = save_sk;\n\t\treturn err;\n\t}\n\n\tskb_zcopy_set(skb, uarg, NULL);\n\treturn skb->len - orig_len;\n}\nEXPORT_SYMBOL_GPL(skb_zerocopy_iter_stream);\n\nvoid __skb_zcopy_downgrade_managed(struct sk_buff *skb)\n{\n\tint i;\n\n\tskb_shinfo(skb)->flags &= ~SKBFL_MANAGED_FRAG_REFS;\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++)\n\t\tskb_frag_ref(skb, i);\n}\nEXPORT_SYMBOL_GPL(__skb_zcopy_downgrade_managed);\n\nstatic int skb_zerocopy_clone(struct sk_buff *nskb, struct sk_buff *orig,\n\t\t\t      gfp_t gfp_mask)\n{\n\tif (skb_zcopy(orig)) {\n\t\tif (skb_zcopy(nskb)) {\n\t\t\t \n\t\t\tif (!gfp_mask) {\n\t\t\t\tWARN_ON_ONCE(1);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t\tif (skb_uarg(nskb) == skb_uarg(orig))\n\t\t\t\treturn 0;\n\t\t\tif (skb_copy_ubufs(nskb, GFP_ATOMIC))\n\t\t\t\treturn -EIO;\n\t\t}\n\t\tskb_zcopy_set(nskb, skb_uarg(orig), NULL);\n\t}\n\treturn 0;\n}\n\n \nint skb_copy_ubufs(struct sk_buff *skb, gfp_t gfp_mask)\n{\n\tint num_frags = skb_shinfo(skb)->nr_frags;\n\tstruct page *page, *head = NULL;\n\tint i, order, psize, new_frags;\n\tu32 d_off;\n\n\tif (skb_shared(skb) || skb_unclone(skb, gfp_mask))\n\t\treturn -EINVAL;\n\n\tif (!num_frags)\n\t\tgoto release;\n\n\t \n\torder = 0;\n\twhile ((PAGE_SIZE << order) * MAX_SKB_FRAGS < __skb_pagelen(skb))\n\t\torder++;\n\tpsize = (PAGE_SIZE << order);\n\n\tnew_frags = (__skb_pagelen(skb) + psize - 1) >> (PAGE_SHIFT + order);\n\tfor (i = 0; i < new_frags; i++) {\n\t\tpage = alloc_pages(gfp_mask | __GFP_COMP, order);\n\t\tif (!page) {\n\t\t\twhile (head) {\n\t\t\t\tstruct page *next = (struct page *)page_private(head);\n\t\t\t\tput_page(head);\n\t\t\t\thead = next;\n\t\t\t}\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tset_page_private(page, (unsigned long)head);\n\t\thead = page;\n\t}\n\n\tpage = head;\n\td_off = 0;\n\tfor (i = 0; i < num_frags; i++) {\n\t\tskb_frag_t *f = &skb_shinfo(skb)->frags[i];\n\t\tu32 p_off, p_len, copied;\n\t\tstruct page *p;\n\t\tu8 *vaddr;\n\n\t\tskb_frag_foreach_page(f, skb_frag_off(f), skb_frag_size(f),\n\t\t\t\t      p, p_off, p_len, copied) {\n\t\t\tu32 copy, done = 0;\n\t\t\tvaddr = kmap_atomic(p);\n\n\t\t\twhile (done < p_len) {\n\t\t\t\tif (d_off == psize) {\n\t\t\t\t\td_off = 0;\n\t\t\t\t\tpage = (struct page *)page_private(page);\n\t\t\t\t}\n\t\t\t\tcopy = min_t(u32, psize - d_off, p_len - done);\n\t\t\t\tmemcpy(page_address(page) + d_off,\n\t\t\t\t       vaddr + p_off + done, copy);\n\t\t\t\tdone += copy;\n\t\t\t\td_off += copy;\n\t\t\t}\n\t\t\tkunmap_atomic(vaddr);\n\t\t}\n\t}\n\n\t \n\tfor (i = 0; i < num_frags; i++)\n\t\tskb_frag_unref(skb, i);\n\n\t \n\tfor (i = 0; i < new_frags - 1; i++) {\n\t\t__skb_fill_page_desc(skb, i, head, 0, psize);\n\t\thead = (struct page *)page_private(head);\n\t}\n\t__skb_fill_page_desc(skb, new_frags - 1, head, 0, d_off);\n\tskb_shinfo(skb)->nr_frags = new_frags;\n\nrelease:\n\tskb_zcopy_clear(skb, false);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(skb_copy_ubufs);\n\n \n\nstruct sk_buff *skb_clone(struct sk_buff *skb, gfp_t gfp_mask)\n{\n\tstruct sk_buff_fclones *fclones = container_of(skb,\n\t\t\t\t\t\t       struct sk_buff_fclones,\n\t\t\t\t\t\t       skb1);\n\tstruct sk_buff *n;\n\n\tif (skb_orphan_frags(skb, gfp_mask))\n\t\treturn NULL;\n\n\tif (skb->fclone == SKB_FCLONE_ORIG &&\n\t    refcount_read(&fclones->fclone_ref) == 1) {\n\t\tn = &fclones->skb2;\n\t\trefcount_set(&fclones->fclone_ref, 2);\n\t\tn->fclone = SKB_FCLONE_CLONE;\n\t} else {\n\t\tif (skb_pfmemalloc(skb))\n\t\t\tgfp_mask |= __GFP_MEMALLOC;\n\n\t\tn = kmem_cache_alloc(skbuff_cache, gfp_mask);\n\t\tif (!n)\n\t\t\treturn NULL;\n\n\t\tn->fclone = SKB_FCLONE_UNAVAILABLE;\n\t}\n\n\treturn __skb_clone(n, skb);\n}\nEXPORT_SYMBOL(skb_clone);\n\nvoid skb_headers_offset_update(struct sk_buff *skb, int off)\n{\n\t \n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\tskb->csum_start += off;\n\t \n\tskb->transport_header += off;\n\tskb->network_header   += off;\n\tif (skb_mac_header_was_set(skb))\n\t\tskb->mac_header += off;\n\tskb->inner_transport_header += off;\n\tskb->inner_network_header += off;\n\tskb->inner_mac_header += off;\n}\nEXPORT_SYMBOL(skb_headers_offset_update);\n\nvoid skb_copy_header(struct sk_buff *new, const struct sk_buff *old)\n{\n\t__copy_skb_header(new, old);\n\n\tskb_shinfo(new)->gso_size = skb_shinfo(old)->gso_size;\n\tskb_shinfo(new)->gso_segs = skb_shinfo(old)->gso_segs;\n\tskb_shinfo(new)->gso_type = skb_shinfo(old)->gso_type;\n}\nEXPORT_SYMBOL(skb_copy_header);\n\nstatic inline int skb_alloc_rx_flag(const struct sk_buff *skb)\n{\n\tif (skb_pfmemalloc(skb))\n\t\treturn SKB_ALLOC_RX;\n\treturn 0;\n}\n\n \n\nstruct sk_buff *skb_copy(const struct sk_buff *skb, gfp_t gfp_mask)\n{\n\tint headerlen = skb_headroom(skb);\n\tunsigned int size = skb_end_offset(skb) + skb->data_len;\n\tstruct sk_buff *n = __alloc_skb(size, gfp_mask,\n\t\t\t\t\tskb_alloc_rx_flag(skb), NUMA_NO_NODE);\n\n\tif (!n)\n\t\treturn NULL;\n\n\t \n\tskb_reserve(n, headerlen);\n\t \n\tskb_put(n, skb->len);\n\n\tBUG_ON(skb_copy_bits(skb, -headerlen, n->head, headerlen + skb->len));\n\n\tskb_copy_header(n, skb);\n\treturn n;\n}\nEXPORT_SYMBOL(skb_copy);\n\n \n\nstruct sk_buff *__pskb_copy_fclone(struct sk_buff *skb, int headroom,\n\t\t\t\t   gfp_t gfp_mask, bool fclone)\n{\n\tunsigned int size = skb_headlen(skb) + headroom;\n\tint flags = skb_alloc_rx_flag(skb) | (fclone ? SKB_ALLOC_FCLONE : 0);\n\tstruct sk_buff *n = __alloc_skb(size, gfp_mask, flags, NUMA_NO_NODE);\n\n\tif (!n)\n\t\tgoto out;\n\n\t \n\tskb_reserve(n, headroom);\n\t \n\tskb_put(n, skb_headlen(skb));\n\t \n\tskb_copy_from_linear_data(skb, n->data, n->len);\n\n\tn->truesize += skb->data_len;\n\tn->data_len  = skb->data_len;\n\tn->len\t     = skb->len;\n\n\tif (skb_shinfo(skb)->nr_frags) {\n\t\tint i;\n\n\t\tif (skb_orphan_frags(skb, gfp_mask) ||\n\t\t    skb_zerocopy_clone(n, skb, gfp_mask)) {\n\t\t\tkfree_skb(n);\n\t\t\tn = NULL;\n\t\t\tgoto out;\n\t\t}\n\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\t\tskb_shinfo(n)->frags[i] = skb_shinfo(skb)->frags[i];\n\t\t\tskb_frag_ref(skb, i);\n\t\t}\n\t\tskb_shinfo(n)->nr_frags = i;\n\t}\n\n\tif (skb_has_frag_list(skb)) {\n\t\tskb_shinfo(n)->frag_list = skb_shinfo(skb)->frag_list;\n\t\tskb_clone_fraglist(n);\n\t}\n\n\tskb_copy_header(n, skb);\nout:\n\treturn n;\n}\nEXPORT_SYMBOL(__pskb_copy_fclone);\n\n \n\nint pskb_expand_head(struct sk_buff *skb, int nhead, int ntail,\n\t\t     gfp_t gfp_mask)\n{\n\tunsigned int osize = skb_end_offset(skb);\n\tunsigned int size = osize + nhead + ntail;\n\tlong off;\n\tu8 *data;\n\tint i;\n\n\tBUG_ON(nhead < 0);\n\n\tBUG_ON(skb_shared(skb));\n\n\tskb_zcopy_downgrade_managed(skb);\n\n\tif (skb_pfmemalloc(skb))\n\t\tgfp_mask |= __GFP_MEMALLOC;\n\n\tdata = kmalloc_reserve(&size, gfp_mask, NUMA_NO_NODE, NULL);\n\tif (!data)\n\t\tgoto nodata;\n\tsize = SKB_WITH_OVERHEAD(size);\n\n\t \n\tmemcpy(data + nhead, skb->head, skb_tail_pointer(skb) - skb->head);\n\n\tmemcpy((struct skb_shared_info *)(data + size),\n\t       skb_shinfo(skb),\n\t       offsetof(struct skb_shared_info, frags[skb_shinfo(skb)->nr_frags]));\n\n\t \n\tif (skb_cloned(skb)) {\n\t\tif (skb_orphan_frags(skb, gfp_mask))\n\t\t\tgoto nofrags;\n\t\tif (skb_zcopy(skb))\n\t\t\trefcount_inc(&skb_uarg(skb)->refcnt);\n\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++)\n\t\t\tskb_frag_ref(skb, i);\n\n\t\tif (skb_has_frag_list(skb))\n\t\t\tskb_clone_fraglist(skb);\n\n\t\tskb_release_data(skb, SKB_CONSUMED, false);\n\t} else {\n\t\tskb_free_head(skb, false);\n\t}\n\toff = (data + nhead) - skb->head;\n\n\tskb->head     = data;\n\tskb->head_frag = 0;\n\tskb->data    += off;\n\n\tskb_set_end_offset(skb, size);\n#ifdef NET_SKBUFF_DATA_USES_OFFSET\n\toff           = nhead;\n#endif\n\tskb->tail\t      += off;\n\tskb_headers_offset_update(skb, nhead);\n\tskb->cloned   = 0;\n\tskb->hdr_len  = 0;\n\tskb->nohdr    = 0;\n\tatomic_set(&skb_shinfo(skb)->dataref, 1);\n\n\tskb_metadata_clear(skb);\n\n\t \n\tif (!skb->sk || skb->destructor == sock_edemux)\n\t\tskb->truesize += size - osize;\n\n\treturn 0;\n\nnofrags:\n\tskb_kfree_head(data, size);\nnodata:\n\treturn -ENOMEM;\n}\nEXPORT_SYMBOL(pskb_expand_head);\n\n \n\nstruct sk_buff *skb_realloc_headroom(struct sk_buff *skb, unsigned int headroom)\n{\n\tstruct sk_buff *skb2;\n\tint delta = headroom - skb_headroom(skb);\n\n\tif (delta <= 0)\n\t\tskb2 = pskb_copy(skb, GFP_ATOMIC);\n\telse {\n\t\tskb2 = skb_clone(skb, GFP_ATOMIC);\n\t\tif (skb2 && pskb_expand_head(skb2, SKB_DATA_ALIGN(delta), 0,\n\t\t\t\t\t     GFP_ATOMIC)) {\n\t\t\tkfree_skb(skb2);\n\t\t\tskb2 = NULL;\n\t\t}\n\t}\n\treturn skb2;\n}\nEXPORT_SYMBOL(skb_realloc_headroom);\n\n \nint __skb_unclone_keeptruesize(struct sk_buff *skb, gfp_t pri)\n{\n\tunsigned int saved_end_offset, saved_truesize;\n\tstruct skb_shared_info *shinfo;\n\tint res;\n\n\tsaved_end_offset = skb_end_offset(skb);\n\tsaved_truesize = skb->truesize;\n\n\tres = pskb_expand_head(skb, 0, 0, pri);\n\tif (res)\n\t\treturn res;\n\n\tskb->truesize = saved_truesize;\n\n\tif (likely(skb_end_offset(skb) == saved_end_offset))\n\t\treturn 0;\n\n\t \n\tif (saved_end_offset == SKB_SMALL_HEAD_HEADROOM ||\n\t    skb_end_offset(skb) == SKB_SMALL_HEAD_HEADROOM) {\n\t\t \n\t\tpr_err_once(\"__skb_unclone_keeptruesize() skb_end_offset() %u -> %u\\n\",\n\t\t\t    saved_end_offset, skb_end_offset(skb));\n\t\tWARN_ON_ONCE(1);\n\t\treturn 0;\n\t}\n\n\tshinfo = skb_shinfo(skb);\n\n\t \n\tmemmove(skb->head + saved_end_offset,\n\t\tshinfo,\n\t\toffsetof(struct skb_shared_info, frags[shinfo->nr_frags]));\n\n\tskb_set_end_offset(skb, saved_end_offset);\n\n\treturn 0;\n}\n\n \n\nstruct sk_buff *skb_expand_head(struct sk_buff *skb, unsigned int headroom)\n{\n\tint delta = headroom - skb_headroom(skb);\n\tint osize = skb_end_offset(skb);\n\tstruct sock *sk = skb->sk;\n\n\tif (WARN_ONCE(delta <= 0,\n\t\t      \"%s is expecting an increase in the headroom\", __func__))\n\t\treturn skb;\n\n\tdelta = SKB_DATA_ALIGN(delta);\n\t \n\tif (skb_shared(skb) || !is_skb_wmem(skb)) {\n\t\tstruct sk_buff *nskb = skb_clone(skb, GFP_ATOMIC);\n\n\t\tif (unlikely(!nskb))\n\t\t\tgoto fail;\n\n\t\tif (sk)\n\t\t\tskb_set_owner_w(nskb, sk);\n\t\tconsume_skb(skb);\n\t\tskb = nskb;\n\t}\n\tif (pskb_expand_head(skb, delta, 0, GFP_ATOMIC))\n\t\tgoto fail;\n\n\tif (sk && is_skb_wmem(skb)) {\n\t\tdelta = skb_end_offset(skb) - osize;\n\t\trefcount_add(delta, &sk->sk_wmem_alloc);\n\t\tskb->truesize += delta;\n\t}\n\treturn skb;\n\nfail:\n\tkfree_skb(skb);\n\treturn NULL;\n}\nEXPORT_SYMBOL(skb_expand_head);\n\n \nstruct sk_buff *skb_copy_expand(const struct sk_buff *skb,\n\t\t\t\tint newheadroom, int newtailroom,\n\t\t\t\tgfp_t gfp_mask)\n{\n\t \n\tstruct sk_buff *n = __alloc_skb(newheadroom + skb->len + newtailroom,\n\t\t\t\t\tgfp_mask, skb_alloc_rx_flag(skb),\n\t\t\t\t\tNUMA_NO_NODE);\n\tint oldheadroom = skb_headroom(skb);\n\tint head_copy_len, head_copy_off;\n\n\tif (!n)\n\t\treturn NULL;\n\n\tskb_reserve(n, newheadroom);\n\n\t \n\tskb_put(n, skb->len);\n\n\thead_copy_len = oldheadroom;\n\thead_copy_off = 0;\n\tif (newheadroom <= head_copy_len)\n\t\thead_copy_len = newheadroom;\n\telse\n\t\thead_copy_off = newheadroom - head_copy_len;\n\n\t \n\tBUG_ON(skb_copy_bits(skb, -head_copy_len, n->head + head_copy_off,\n\t\t\t     skb->len + head_copy_len));\n\n\tskb_copy_header(n, skb);\n\n\tskb_headers_offset_update(n, newheadroom - oldheadroom);\n\n\treturn n;\n}\nEXPORT_SYMBOL(skb_copy_expand);\n\n \n\nint __skb_pad(struct sk_buff *skb, int pad, bool free_on_error)\n{\n\tint err;\n\tint ntail;\n\n\t \n\tif (!skb_cloned(skb) && skb_tailroom(skb) >= pad) {\n\t\tmemset(skb->data+skb->len, 0, pad);\n\t\treturn 0;\n\t}\n\n\tntail = skb->data_len + pad - (skb->end - skb->tail);\n\tif (likely(skb_cloned(skb) || ntail > 0)) {\n\t\terr = pskb_expand_head(skb, 0, ntail, GFP_ATOMIC);\n\t\tif (unlikely(err))\n\t\t\tgoto free_skb;\n\t}\n\n\t \n\terr = skb_linearize(skb);\n\tif (unlikely(err))\n\t\tgoto free_skb;\n\n\tmemset(skb->data + skb->len, 0, pad);\n\treturn 0;\n\nfree_skb:\n\tif (free_on_error)\n\t\tkfree_skb(skb);\n\treturn err;\n}\nEXPORT_SYMBOL(__skb_pad);\n\n \n\nvoid *pskb_put(struct sk_buff *skb, struct sk_buff *tail, int len)\n{\n\tif (tail != skb) {\n\t\tskb->data_len += len;\n\t\tskb->len += len;\n\t}\n\treturn skb_put(tail, len);\n}\nEXPORT_SYMBOL_GPL(pskb_put);\n\n \nvoid *skb_put(struct sk_buff *skb, unsigned int len)\n{\n\tvoid *tmp = skb_tail_pointer(skb);\n\tSKB_LINEAR_ASSERT(skb);\n\tskb->tail += len;\n\tskb->len  += len;\n\tif (unlikely(skb->tail > skb->end))\n\t\tskb_over_panic(skb, len, __builtin_return_address(0));\n\treturn tmp;\n}\nEXPORT_SYMBOL(skb_put);\n\n \nvoid *skb_push(struct sk_buff *skb, unsigned int len)\n{\n\tskb->data -= len;\n\tskb->len  += len;\n\tif (unlikely(skb->data < skb->head))\n\t\tskb_under_panic(skb, len, __builtin_return_address(0));\n\treturn skb->data;\n}\nEXPORT_SYMBOL(skb_push);\n\n \nvoid *skb_pull(struct sk_buff *skb, unsigned int len)\n{\n\treturn skb_pull_inline(skb, len);\n}\nEXPORT_SYMBOL(skb_pull);\n\n \nvoid *skb_pull_data(struct sk_buff *skb, size_t len)\n{\n\tvoid *data = skb->data;\n\n\tif (skb->len < len)\n\t\treturn NULL;\n\n\tskb_pull(skb, len);\n\n\treturn data;\n}\nEXPORT_SYMBOL(skb_pull_data);\n\n \nvoid skb_trim(struct sk_buff *skb, unsigned int len)\n{\n\tif (skb->len > len)\n\t\t__skb_trim(skb, len);\n}\nEXPORT_SYMBOL(skb_trim);\n\n \n\nint ___pskb_trim(struct sk_buff *skb, unsigned int len)\n{\n\tstruct sk_buff **fragp;\n\tstruct sk_buff *frag;\n\tint offset = skb_headlen(skb);\n\tint nfrags = skb_shinfo(skb)->nr_frags;\n\tint i;\n\tint err;\n\n\tif (skb_cloned(skb) &&\n\t    unlikely((err = pskb_expand_head(skb, 0, 0, GFP_ATOMIC))))\n\t\treturn err;\n\n\ti = 0;\n\tif (offset >= len)\n\t\tgoto drop_pages;\n\n\tfor (; i < nfrags; i++) {\n\t\tint end = offset + skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\n\t\tif (end < len) {\n\t\t\toffset = end;\n\t\t\tcontinue;\n\t\t}\n\n\t\tskb_frag_size_set(&skb_shinfo(skb)->frags[i++], len - offset);\n\ndrop_pages:\n\t\tskb_shinfo(skb)->nr_frags = i;\n\n\t\tfor (; i < nfrags; i++)\n\t\t\tskb_frag_unref(skb, i);\n\n\t\tif (skb_has_frag_list(skb))\n\t\t\tskb_drop_fraglist(skb);\n\t\tgoto done;\n\t}\n\n\tfor (fragp = &skb_shinfo(skb)->frag_list; (frag = *fragp);\n\t     fragp = &frag->next) {\n\t\tint end = offset + frag->len;\n\n\t\tif (skb_shared(frag)) {\n\t\t\tstruct sk_buff *nfrag;\n\n\t\t\tnfrag = skb_clone(frag, GFP_ATOMIC);\n\t\t\tif (unlikely(!nfrag))\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tnfrag->next = frag->next;\n\t\t\tconsume_skb(frag);\n\t\t\tfrag = nfrag;\n\t\t\t*fragp = frag;\n\t\t}\n\n\t\tif (end < len) {\n\t\t\toffset = end;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (end > len &&\n\t\t    unlikely((err = pskb_trim(frag, len - offset))))\n\t\t\treturn err;\n\n\t\tif (frag->next)\n\t\t\tskb_drop_list(&frag->next);\n\t\tbreak;\n\t}\n\ndone:\n\tif (len > skb_headlen(skb)) {\n\t\tskb->data_len -= skb->len - len;\n\t\tskb->len       = len;\n\t} else {\n\t\tskb->len       = len;\n\t\tskb->data_len  = 0;\n\t\tskb_set_tail_pointer(skb, len);\n\t}\n\n\tif (!skb->sk || skb->destructor == sock_edemux)\n\t\tskb_condense(skb);\n\treturn 0;\n}\nEXPORT_SYMBOL(___pskb_trim);\n\n \nint pskb_trim_rcsum_slow(struct sk_buff *skb, unsigned int len)\n{\n\tif (skb->ip_summed == CHECKSUM_COMPLETE) {\n\t\tint delta = skb->len - len;\n\n\t\tskb->csum = csum_block_sub(skb->csum,\n\t\t\t\t\t   skb_checksum(skb, len, delta, 0),\n\t\t\t\t\t   len);\n\t} else if (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\tint hdlen = (len > skb_headlen(skb)) ? skb_headlen(skb) : len;\n\t\tint offset = skb_checksum_start_offset(skb) + skb->csum_offset;\n\n\t\tif (offset + sizeof(__sum16) > hdlen)\n\t\t\treturn -EINVAL;\n\t}\n\treturn __pskb_trim(skb, len);\n}\nEXPORT_SYMBOL(pskb_trim_rcsum_slow);\n\n \n\n \nvoid *__pskb_pull_tail(struct sk_buff *skb, int delta)\n{\n\t \n\tint i, k, eat = (skb->tail + delta) - skb->end;\n\n\tif (eat > 0 || skb_cloned(skb)) {\n\t\tif (pskb_expand_head(skb, 0, eat > 0 ? eat + 128 : 0,\n\t\t\t\t     GFP_ATOMIC))\n\t\t\treturn NULL;\n\t}\n\n\tBUG_ON(skb_copy_bits(skb, skb_headlen(skb),\n\t\t\t     skb_tail_pointer(skb), delta));\n\n\t \n\tif (!skb_has_frag_list(skb))\n\t\tgoto pull_pages;\n\n\t \n\teat = delta;\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tint size = skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\n\t\tif (size >= eat)\n\t\t\tgoto pull_pages;\n\t\teat -= size;\n\t}\n\n\t \n\tif (eat) {\n\t\tstruct sk_buff *list = skb_shinfo(skb)->frag_list;\n\t\tstruct sk_buff *clone = NULL;\n\t\tstruct sk_buff *insp = NULL;\n\n\t\tdo {\n\t\t\tif (list->len <= eat) {\n\t\t\t\t \n\t\t\t\teat -= list->len;\n\t\t\t\tlist = list->next;\n\t\t\t\tinsp = list;\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tif (skb_is_gso(skb) && !list->head_frag &&\n\t\t\t\t    skb_headlen(list))\n\t\t\t\t\tskb_shinfo(skb)->gso_type |= SKB_GSO_DODGY;\n\n\t\t\t\tif (skb_shared(list)) {\n\t\t\t\t\t \n\t\t\t\t\tclone = skb_clone(list, GFP_ATOMIC);\n\t\t\t\t\tif (!clone)\n\t\t\t\t\t\treturn NULL;\n\t\t\t\t\tinsp = list->next;\n\t\t\t\t\tlist = clone;\n\t\t\t\t} else {\n\t\t\t\t\t \n\t\t\t\t\tinsp = list;\n\t\t\t\t}\n\t\t\t\tif (!pskb_pull(list, eat)) {\n\t\t\t\t\tkfree_skb(clone);\n\t\t\t\t\treturn NULL;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} while (eat);\n\n\t\t \n\t\twhile ((list = skb_shinfo(skb)->frag_list) != insp) {\n\t\t\tskb_shinfo(skb)->frag_list = list->next;\n\t\t\tconsume_skb(list);\n\t\t}\n\t\t \n\t\tif (clone) {\n\t\t\tclone->next = list;\n\t\t\tskb_shinfo(skb)->frag_list = clone;\n\t\t}\n\t}\n\t \n\npull_pages:\n\teat = delta;\n\tk = 0;\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tint size = skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\n\t\tif (size <= eat) {\n\t\t\tskb_frag_unref(skb, i);\n\t\t\teat -= size;\n\t\t} else {\n\t\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[k];\n\n\t\t\t*frag = skb_shinfo(skb)->frags[i];\n\t\t\tif (eat) {\n\t\t\t\tskb_frag_off_add(frag, eat);\n\t\t\t\tskb_frag_size_sub(frag, eat);\n\t\t\t\tif (!i)\n\t\t\t\t\tgoto end;\n\t\t\t\teat = 0;\n\t\t\t}\n\t\t\tk++;\n\t\t}\n\t}\n\tskb_shinfo(skb)->nr_frags = k;\n\nend:\n\tskb->tail     += delta;\n\tskb->data_len -= delta;\n\n\tif (!skb->data_len)\n\t\tskb_zcopy_clear(skb, false);\n\n\treturn skb_tail_pointer(skb);\n}\nEXPORT_SYMBOL(__pskb_pull_tail);\n\n \nint skb_copy_bits(const struct sk_buff *skb, int offset, void *to, int len)\n{\n\tint start = skb_headlen(skb);\n\tstruct sk_buff *frag_iter;\n\tint i, copy;\n\n\tif (offset > (int)skb->len - len)\n\t\tgoto fault;\n\n\t \n\tif ((copy = start - offset) > 0) {\n\t\tif (copy > len)\n\t\t\tcopy = len;\n\t\tskb_copy_from_linear_data_offset(skb, offset, to, copy);\n\t\tif ((len -= copy) == 0)\n\t\t\treturn 0;\n\t\toffset += copy;\n\t\tto     += copy;\n\t}\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tint end;\n\t\tskb_frag_t *f = &skb_shinfo(skb)->frags[i];\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + skb_frag_size(f);\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tu32 p_off, p_len, copied;\n\t\t\tstruct page *p;\n\t\t\tu8 *vaddr;\n\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\n\t\t\tskb_frag_foreach_page(f,\n\t\t\t\t\t      skb_frag_off(f) + offset - start,\n\t\t\t\t\t      copy, p, p_off, p_len, copied) {\n\t\t\t\tvaddr = kmap_atomic(p);\n\t\t\t\tmemcpy(to + copied, vaddr + p_off, p_len);\n\t\t\t\tkunmap_atomic(vaddr);\n\t\t\t}\n\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn 0;\n\t\t\toffset += copy;\n\t\t\tto     += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\n\tskb_walk_frags(skb, frag_iter) {\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + frag_iter->len;\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tif (skb_copy_bits(frag_iter, offset - start, to, copy))\n\t\t\t\tgoto fault;\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn 0;\n\t\t\toffset += copy;\n\t\t\tto     += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\n\tif (!len)\n\t\treturn 0;\n\nfault:\n\treturn -EFAULT;\n}\nEXPORT_SYMBOL(skb_copy_bits);\n\n \nstatic void sock_spd_release(struct splice_pipe_desc *spd, unsigned int i)\n{\n\tput_page(spd->pages[i]);\n}\n\nstatic struct page *linear_to_page(struct page *page, unsigned int *len,\n\t\t\t\t   unsigned int *offset,\n\t\t\t\t   struct sock *sk)\n{\n\tstruct page_frag *pfrag = sk_page_frag(sk);\n\n\tif (!sk_page_frag_refill(sk, pfrag))\n\t\treturn NULL;\n\n\t*len = min_t(unsigned int, *len, pfrag->size - pfrag->offset);\n\n\tmemcpy(page_address(pfrag->page) + pfrag->offset,\n\t       page_address(page) + *offset, *len);\n\t*offset = pfrag->offset;\n\tpfrag->offset += *len;\n\n\treturn pfrag->page;\n}\n\nstatic bool spd_can_coalesce(const struct splice_pipe_desc *spd,\n\t\t\t     struct page *page,\n\t\t\t     unsigned int offset)\n{\n\treturn\tspd->nr_pages &&\n\t\tspd->pages[spd->nr_pages - 1] == page &&\n\t\t(spd->partial[spd->nr_pages - 1].offset +\n\t\t spd->partial[spd->nr_pages - 1].len == offset);\n}\n\n \nstatic bool spd_fill_page(struct splice_pipe_desc *spd,\n\t\t\t  struct pipe_inode_info *pipe, struct page *page,\n\t\t\t  unsigned int *len, unsigned int offset,\n\t\t\t  bool linear,\n\t\t\t  struct sock *sk)\n{\n\tif (unlikely(spd->nr_pages == MAX_SKB_FRAGS))\n\t\treturn true;\n\n\tif (linear) {\n\t\tpage = linear_to_page(page, len, &offset, sk);\n\t\tif (!page)\n\t\t\treturn true;\n\t}\n\tif (spd_can_coalesce(spd, page, offset)) {\n\t\tspd->partial[spd->nr_pages - 1].len += *len;\n\t\treturn false;\n\t}\n\tget_page(page);\n\tspd->pages[spd->nr_pages] = page;\n\tspd->partial[spd->nr_pages].len = *len;\n\tspd->partial[spd->nr_pages].offset = offset;\n\tspd->nr_pages++;\n\n\treturn false;\n}\n\nstatic bool __splice_segment(struct page *page, unsigned int poff,\n\t\t\t     unsigned int plen, unsigned int *off,\n\t\t\t     unsigned int *len,\n\t\t\t     struct splice_pipe_desc *spd, bool linear,\n\t\t\t     struct sock *sk,\n\t\t\t     struct pipe_inode_info *pipe)\n{\n\tif (!*len)\n\t\treturn true;\n\n\t \n\tif (*off >= plen) {\n\t\t*off -= plen;\n\t\treturn false;\n\t}\n\n\t \n\tpoff += *off;\n\tplen -= *off;\n\t*off = 0;\n\n\tdo {\n\t\tunsigned int flen = min(*len, plen);\n\n\t\tif (spd_fill_page(spd, pipe, page, &flen, poff,\n\t\t\t\t  linear, sk))\n\t\t\treturn true;\n\t\tpoff += flen;\n\t\tplen -= flen;\n\t\t*len -= flen;\n\t} while (*len && plen);\n\n\treturn false;\n}\n\n \nstatic bool __skb_splice_bits(struct sk_buff *skb, struct pipe_inode_info *pipe,\n\t\t\t      unsigned int *offset, unsigned int *len,\n\t\t\t      struct splice_pipe_desc *spd, struct sock *sk)\n{\n\tint seg;\n\tstruct sk_buff *iter;\n\n\t \n\tif (__splice_segment(virt_to_page(skb->data),\n\t\t\t     (unsigned long) skb->data & (PAGE_SIZE - 1),\n\t\t\t     skb_headlen(skb),\n\t\t\t     offset, len, spd,\n\t\t\t     skb_head_is_locked(skb),\n\t\t\t     sk, pipe))\n\t\treturn true;\n\n\t \n\tfor (seg = 0; seg < skb_shinfo(skb)->nr_frags; seg++) {\n\t\tconst skb_frag_t *f = &skb_shinfo(skb)->frags[seg];\n\n\t\tif (__splice_segment(skb_frag_page(f),\n\t\t\t\t     skb_frag_off(f), skb_frag_size(f),\n\t\t\t\t     offset, len, spd, false, sk, pipe))\n\t\t\treturn true;\n\t}\n\n\tskb_walk_frags(skb, iter) {\n\t\tif (*offset >= iter->len) {\n\t\t\t*offset -= iter->len;\n\t\t\tcontinue;\n\t\t}\n\t\t \n\t\tif (__skb_splice_bits(iter, pipe, offset, len, spd, sk))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nint skb_splice_bits(struct sk_buff *skb, struct sock *sk, unsigned int offset,\n\t\t    struct pipe_inode_info *pipe, unsigned int tlen,\n\t\t    unsigned int flags)\n{\n\tstruct partial_page partial[MAX_SKB_FRAGS];\n\tstruct page *pages[MAX_SKB_FRAGS];\n\tstruct splice_pipe_desc spd = {\n\t\t.pages = pages,\n\t\t.partial = partial,\n\t\t.nr_pages_max = MAX_SKB_FRAGS,\n\t\t.ops = &nosteal_pipe_buf_ops,\n\t\t.spd_release = sock_spd_release,\n\t};\n\tint ret = 0;\n\n\t__skb_splice_bits(skb, pipe, &offset, &tlen, &spd, sk);\n\n\tif (spd.nr_pages)\n\t\tret = splice_to_pipe(pipe, &spd);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(skb_splice_bits);\n\nstatic int sendmsg_locked(struct sock *sk, struct msghdr *msg)\n{\n\tstruct socket *sock = sk->sk_socket;\n\tsize_t size = msg_data_left(msg);\n\n\tif (!sock)\n\t\treturn -EINVAL;\n\n\tif (!sock->ops->sendmsg_locked)\n\t\treturn sock_no_sendmsg_locked(sk, msg, size);\n\n\treturn sock->ops->sendmsg_locked(sk, msg, size);\n}\n\nstatic int sendmsg_unlocked(struct sock *sk, struct msghdr *msg)\n{\n\tstruct socket *sock = sk->sk_socket;\n\n\tif (!sock)\n\t\treturn -EINVAL;\n\treturn sock_sendmsg(sock, msg);\n}\n\ntypedef int (*sendmsg_func)(struct sock *sk, struct msghdr *msg);\nstatic int __skb_send_sock(struct sock *sk, struct sk_buff *skb, int offset,\n\t\t\t   int len, sendmsg_func sendmsg)\n{\n\tunsigned int orig_len = len;\n\tstruct sk_buff *head = skb;\n\tunsigned short fragidx;\n\tint slen, ret;\n\ndo_frag_list:\n\n\t \n\twhile (offset < skb_headlen(skb) && len) {\n\t\tstruct kvec kv;\n\t\tstruct msghdr msg;\n\n\t\tslen = min_t(int, len, skb_headlen(skb) - offset);\n\t\tkv.iov_base = skb->data + offset;\n\t\tkv.iov_len = slen;\n\t\tmemset(&msg, 0, sizeof(msg));\n\t\tmsg.msg_flags = MSG_DONTWAIT;\n\n\t\tiov_iter_kvec(&msg.msg_iter, ITER_SOURCE, &kv, 1, slen);\n\t\tret = INDIRECT_CALL_2(sendmsg, sendmsg_locked,\n\t\t\t\t      sendmsg_unlocked, sk, &msg);\n\t\tif (ret <= 0)\n\t\t\tgoto error;\n\n\t\toffset += ret;\n\t\tlen -= ret;\n\t}\n\n\t \n\tif (!len)\n\t\tgoto out;\n\n\t \n\toffset -= skb_headlen(skb);\n\n\t \n\tfor (fragidx = 0; fragidx < skb_shinfo(skb)->nr_frags; fragidx++) {\n\t\tskb_frag_t *frag  = &skb_shinfo(skb)->frags[fragidx];\n\n\t\tif (offset < skb_frag_size(frag))\n\t\t\tbreak;\n\n\t\toffset -= skb_frag_size(frag);\n\t}\n\n\tfor (; len && fragidx < skb_shinfo(skb)->nr_frags; fragidx++) {\n\t\tskb_frag_t *frag  = &skb_shinfo(skb)->frags[fragidx];\n\n\t\tslen = min_t(size_t, len, skb_frag_size(frag) - offset);\n\n\t\twhile (slen) {\n\t\t\tstruct bio_vec bvec;\n\t\t\tstruct msghdr msg = {\n\t\t\t\t.msg_flags = MSG_SPLICE_PAGES | MSG_DONTWAIT,\n\t\t\t};\n\n\t\t\tbvec_set_page(&bvec, skb_frag_page(frag), slen,\n\t\t\t\t      skb_frag_off(frag) + offset);\n\t\t\tiov_iter_bvec(&msg.msg_iter, ITER_SOURCE, &bvec, 1,\n\t\t\t\t      slen);\n\n\t\t\tret = INDIRECT_CALL_2(sendmsg, sendmsg_locked,\n\t\t\t\t\t      sendmsg_unlocked, sk, &msg);\n\t\t\tif (ret <= 0)\n\t\t\t\tgoto error;\n\n\t\t\tlen -= ret;\n\t\t\toffset += ret;\n\t\t\tslen -= ret;\n\t\t}\n\n\t\toffset = 0;\n\t}\n\n\tif (len) {\n\t\t \n\n\t\tif (skb == head) {\n\t\t\tif (skb_has_frag_list(skb)) {\n\t\t\t\tskb = skb_shinfo(skb)->frag_list;\n\t\t\t\tgoto do_frag_list;\n\t\t\t}\n\t\t} else if (skb->next) {\n\t\t\tskb = skb->next;\n\t\t\tgoto do_frag_list;\n\t\t}\n\t}\n\nout:\n\treturn orig_len - len;\n\nerror:\n\treturn orig_len == len ? ret : orig_len - len;\n}\n\n \nint skb_send_sock_locked(struct sock *sk, struct sk_buff *skb, int offset,\n\t\t\t int len)\n{\n\treturn __skb_send_sock(sk, skb, offset, len, sendmsg_locked);\n}\nEXPORT_SYMBOL_GPL(skb_send_sock_locked);\n\n \nint skb_send_sock(struct sock *sk, struct sk_buff *skb, int offset, int len)\n{\n\treturn __skb_send_sock(sk, skb, offset, len, sendmsg_unlocked);\n}\n\n \n\nint skb_store_bits(struct sk_buff *skb, int offset, const void *from, int len)\n{\n\tint start = skb_headlen(skb);\n\tstruct sk_buff *frag_iter;\n\tint i, copy;\n\n\tif (offset > (int)skb->len - len)\n\t\tgoto fault;\n\n\tif ((copy = start - offset) > 0) {\n\t\tif (copy > len)\n\t\t\tcopy = len;\n\t\tskb_copy_to_linear_data_offset(skb, offset, from, copy);\n\t\tif ((len -= copy) == 0)\n\t\t\treturn 0;\n\t\toffset += copy;\n\t\tfrom += copy;\n\t}\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + skb_frag_size(frag);\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tu32 p_off, p_len, copied;\n\t\t\tstruct page *p;\n\t\t\tu8 *vaddr;\n\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\n\t\t\tskb_frag_foreach_page(frag,\n\t\t\t\t\t      skb_frag_off(frag) + offset - start,\n\t\t\t\t\t      copy, p, p_off, p_len, copied) {\n\t\t\t\tvaddr = kmap_atomic(p);\n\t\t\t\tmemcpy(vaddr + p_off, from + copied, p_len);\n\t\t\t\tkunmap_atomic(vaddr);\n\t\t\t}\n\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn 0;\n\t\t\toffset += copy;\n\t\t\tfrom += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\n\tskb_walk_frags(skb, frag_iter) {\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + frag_iter->len;\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tif (skb_store_bits(frag_iter, offset - start,\n\t\t\t\t\t   from, copy))\n\t\t\t\tgoto fault;\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn 0;\n\t\t\toffset += copy;\n\t\t\tfrom += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\tif (!len)\n\t\treturn 0;\n\nfault:\n\treturn -EFAULT;\n}\nEXPORT_SYMBOL(skb_store_bits);\n\n \n__wsum __skb_checksum(const struct sk_buff *skb, int offset, int len,\n\t\t      __wsum csum, const struct skb_checksum_ops *ops)\n{\n\tint start = skb_headlen(skb);\n\tint i, copy = start - offset;\n\tstruct sk_buff *frag_iter;\n\tint pos = 0;\n\n\t \n\tif (copy > 0) {\n\t\tif (copy > len)\n\t\t\tcopy = len;\n\t\tcsum = INDIRECT_CALL_1(ops->update, csum_partial_ext,\n\t\t\t\t       skb->data + offset, copy, csum);\n\t\tif ((len -= copy) == 0)\n\t\t\treturn csum;\n\t\toffset += copy;\n\t\tpos\t= copy;\n\t}\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tint end;\n\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + skb_frag_size(frag);\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tu32 p_off, p_len, copied;\n\t\t\tstruct page *p;\n\t\t\t__wsum csum2;\n\t\t\tu8 *vaddr;\n\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\n\t\t\tskb_frag_foreach_page(frag,\n\t\t\t\t\t      skb_frag_off(frag) + offset - start,\n\t\t\t\t\t      copy, p, p_off, p_len, copied) {\n\t\t\t\tvaddr = kmap_atomic(p);\n\t\t\t\tcsum2 = INDIRECT_CALL_1(ops->update,\n\t\t\t\t\t\t\tcsum_partial_ext,\n\t\t\t\t\t\t\tvaddr + p_off, p_len, 0);\n\t\t\t\tkunmap_atomic(vaddr);\n\t\t\t\tcsum = INDIRECT_CALL_1(ops->combine,\n\t\t\t\t\t\t       csum_block_add_ext, csum,\n\t\t\t\t\t\t       csum2, pos, p_len);\n\t\t\t\tpos += p_len;\n\t\t\t}\n\n\t\t\tif (!(len -= copy))\n\t\t\t\treturn csum;\n\t\t\toffset += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\n\tskb_walk_frags(skb, frag_iter) {\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + frag_iter->len;\n\t\tif ((copy = end - offset) > 0) {\n\t\t\t__wsum csum2;\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tcsum2 = __skb_checksum(frag_iter, offset - start,\n\t\t\t\t\t       copy, 0, ops);\n\t\t\tcsum = INDIRECT_CALL_1(ops->combine, csum_block_add_ext,\n\t\t\t\t\t       csum, csum2, pos, copy);\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn csum;\n\t\t\toffset += copy;\n\t\t\tpos    += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\tBUG_ON(len);\n\n\treturn csum;\n}\nEXPORT_SYMBOL(__skb_checksum);\n\n__wsum skb_checksum(const struct sk_buff *skb, int offset,\n\t\t    int len, __wsum csum)\n{\n\tconst struct skb_checksum_ops ops = {\n\t\t.update  = csum_partial_ext,\n\t\t.combine = csum_block_add_ext,\n\t};\n\n\treturn __skb_checksum(skb, offset, len, csum, &ops);\n}\nEXPORT_SYMBOL(skb_checksum);\n\n \n\n__wsum skb_copy_and_csum_bits(const struct sk_buff *skb, int offset,\n\t\t\t\t    u8 *to, int len)\n{\n\tint start = skb_headlen(skb);\n\tint i, copy = start - offset;\n\tstruct sk_buff *frag_iter;\n\tint pos = 0;\n\t__wsum csum = 0;\n\n\t \n\tif (copy > 0) {\n\t\tif (copy > len)\n\t\t\tcopy = len;\n\t\tcsum = csum_partial_copy_nocheck(skb->data + offset, to,\n\t\t\t\t\t\t copy);\n\t\tif ((len -= copy) == 0)\n\t\t\treturn csum;\n\t\toffset += copy;\n\t\tto     += copy;\n\t\tpos\t= copy;\n\t}\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\t\t\tu32 p_off, p_len, copied;\n\t\t\tstruct page *p;\n\t\t\t__wsum csum2;\n\t\t\tu8 *vaddr;\n\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\n\t\t\tskb_frag_foreach_page(frag,\n\t\t\t\t\t      skb_frag_off(frag) + offset - start,\n\t\t\t\t\t      copy, p, p_off, p_len, copied) {\n\t\t\t\tvaddr = kmap_atomic(p);\n\t\t\t\tcsum2 = csum_partial_copy_nocheck(vaddr + p_off,\n\t\t\t\t\t\t\t\t  to + copied,\n\t\t\t\t\t\t\t\t  p_len);\n\t\t\t\tkunmap_atomic(vaddr);\n\t\t\t\tcsum = csum_block_add(csum, csum2, pos);\n\t\t\t\tpos += p_len;\n\t\t\t}\n\n\t\t\tif (!(len -= copy))\n\t\t\t\treturn csum;\n\t\t\toffset += copy;\n\t\t\tto     += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\n\tskb_walk_frags(skb, frag_iter) {\n\t\t__wsum csum2;\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + frag_iter->len;\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tcsum2 = skb_copy_and_csum_bits(frag_iter,\n\t\t\t\t\t\t       offset - start,\n\t\t\t\t\t\t       to, copy);\n\t\t\tcsum = csum_block_add(csum, csum2, pos);\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn csum;\n\t\t\toffset += copy;\n\t\t\tto     += copy;\n\t\t\tpos    += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\tBUG_ON(len);\n\treturn csum;\n}\nEXPORT_SYMBOL(skb_copy_and_csum_bits);\n\n__sum16 __skb_checksum_complete_head(struct sk_buff *skb, int len)\n{\n\t__sum16 sum;\n\n\tsum = csum_fold(skb_checksum(skb, 0, len, skb->csum));\n\t \n\tif (likely(!sum)) {\n\t\tif (unlikely(skb->ip_summed == CHECKSUM_COMPLETE) &&\n\t\t    !skb->csum_complete_sw)\n\t\t\tnetdev_rx_csum_fault(skb->dev, skb);\n\t}\n\tif (!skb_shared(skb))\n\t\tskb->csum_valid = !sum;\n\treturn sum;\n}\nEXPORT_SYMBOL(__skb_checksum_complete_head);\n\n \n__sum16 __skb_checksum_complete(struct sk_buff *skb)\n{\n\t__wsum csum;\n\t__sum16 sum;\n\n\tcsum = skb_checksum(skb, 0, skb->len, 0);\n\n\tsum = csum_fold(csum_add(skb->csum, csum));\n\t \n\tif (likely(!sum)) {\n\t\tif (unlikely(skb->ip_summed == CHECKSUM_COMPLETE) &&\n\t\t    !skb->csum_complete_sw)\n\t\t\tnetdev_rx_csum_fault(skb->dev, skb);\n\t}\n\n\tif (!skb_shared(skb)) {\n\t\t \n\t\tskb->csum = csum;\n\t\tskb->ip_summed = CHECKSUM_COMPLETE;\n\t\tskb->csum_complete_sw = 1;\n\t\tskb->csum_valid = !sum;\n\t}\n\n\treturn sum;\n}\nEXPORT_SYMBOL(__skb_checksum_complete);\n\nstatic __wsum warn_crc32c_csum_update(const void *buff, int len, __wsum sum)\n{\n\tnet_warn_ratelimited(\n\t\t\"%s: attempt to compute crc32c without libcrc32c.ko\\n\",\n\t\t__func__);\n\treturn 0;\n}\n\nstatic __wsum warn_crc32c_csum_combine(__wsum csum, __wsum csum2,\n\t\t\t\t       int offset, int len)\n{\n\tnet_warn_ratelimited(\n\t\t\"%s: attempt to compute crc32c without libcrc32c.ko\\n\",\n\t\t__func__);\n\treturn 0;\n}\n\nstatic const struct skb_checksum_ops default_crc32c_ops = {\n\t.update  = warn_crc32c_csum_update,\n\t.combine = warn_crc32c_csum_combine,\n};\n\nconst struct skb_checksum_ops *crc32c_csum_stub __read_mostly =\n\t&default_crc32c_ops;\nEXPORT_SYMBOL(crc32c_csum_stub);\n\n  \nunsigned int\nskb_zerocopy_headlen(const struct sk_buff *from)\n{\n\tunsigned int hlen = 0;\n\n\tif (!from->head_frag ||\n\t    skb_headlen(from) < L1_CACHE_BYTES ||\n\t    skb_shinfo(from)->nr_frags >= MAX_SKB_FRAGS) {\n\t\thlen = skb_headlen(from);\n\t\tif (!hlen)\n\t\t\thlen = from->len;\n\t}\n\n\tif (skb_has_frag_list(from))\n\t\thlen = from->len;\n\n\treturn hlen;\n}\nEXPORT_SYMBOL_GPL(skb_zerocopy_headlen);\n\n \nint\nskb_zerocopy(struct sk_buff *to, struct sk_buff *from, int len, int hlen)\n{\n\tint i, j = 0;\n\tint plen = 0;  \n\tint ret;\n\tstruct page *page;\n\tunsigned int offset;\n\n\tBUG_ON(!from->head_frag && !hlen);\n\n\t \n\tif (len <= skb_tailroom(to))\n\t\treturn skb_copy_bits(from, 0, skb_put(to, len), len);\n\n\tif (hlen) {\n\t\tret = skb_copy_bits(from, 0, skb_put(to, hlen), hlen);\n\t\tif (unlikely(ret))\n\t\t\treturn ret;\n\t\tlen -= hlen;\n\t} else {\n\t\tplen = min_t(int, skb_headlen(from), len);\n\t\tif (plen) {\n\t\t\tpage = virt_to_head_page(from->head);\n\t\t\toffset = from->data - (unsigned char *)page_address(page);\n\t\t\t__skb_fill_page_desc(to, 0, page, offset, plen);\n\t\t\tget_page(page);\n\t\t\tj = 1;\n\t\t\tlen -= plen;\n\t\t}\n\t}\n\n\tskb_len_add(to, len + plen);\n\n\tif (unlikely(skb_orphan_frags(from, GFP_ATOMIC))) {\n\t\tskb_tx_error(from);\n\t\treturn -ENOMEM;\n\t}\n\tskb_zerocopy_clone(to, from, GFP_ATOMIC);\n\n\tfor (i = 0; i < skb_shinfo(from)->nr_frags; i++) {\n\t\tint size;\n\n\t\tif (!len)\n\t\t\tbreak;\n\t\tskb_shinfo(to)->frags[j] = skb_shinfo(from)->frags[i];\n\t\tsize = min_t(int, skb_frag_size(&skb_shinfo(to)->frags[j]),\n\t\t\t\t\tlen);\n\t\tskb_frag_size_set(&skb_shinfo(to)->frags[j], size);\n\t\tlen -= size;\n\t\tskb_frag_ref(to, j);\n\t\tj++;\n\t}\n\tskb_shinfo(to)->nr_frags = j;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(skb_zerocopy);\n\nvoid skb_copy_and_csum_dev(const struct sk_buff *skb, u8 *to)\n{\n\t__wsum csum;\n\tlong csstart;\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\tcsstart = skb_checksum_start_offset(skb);\n\telse\n\t\tcsstart = skb_headlen(skb);\n\n\tBUG_ON(csstart > skb_headlen(skb));\n\n\tskb_copy_from_linear_data(skb, to, csstart);\n\n\tcsum = 0;\n\tif (csstart != skb->len)\n\t\tcsum = skb_copy_and_csum_bits(skb, csstart, to + csstart,\n\t\t\t\t\t      skb->len - csstart);\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\tlong csstuff = csstart + skb->csum_offset;\n\n\t\t*((__sum16 *)(to + csstuff)) = csum_fold(csum);\n\t}\n}\nEXPORT_SYMBOL(skb_copy_and_csum_dev);\n\n \n\nstruct sk_buff *skb_dequeue(struct sk_buff_head *list)\n{\n\tunsigned long flags;\n\tstruct sk_buff *result;\n\n\tspin_lock_irqsave(&list->lock, flags);\n\tresult = __skb_dequeue(list);\n\tspin_unlock_irqrestore(&list->lock, flags);\n\treturn result;\n}\nEXPORT_SYMBOL(skb_dequeue);\n\n \nstruct sk_buff *skb_dequeue_tail(struct sk_buff_head *list)\n{\n\tunsigned long flags;\n\tstruct sk_buff *result;\n\n\tspin_lock_irqsave(&list->lock, flags);\n\tresult = __skb_dequeue_tail(list);\n\tspin_unlock_irqrestore(&list->lock, flags);\n\treturn result;\n}\nEXPORT_SYMBOL(skb_dequeue_tail);\n\n \nvoid skb_queue_purge_reason(struct sk_buff_head *list,\n\t\t\t    enum skb_drop_reason reason)\n{\n\tstruct sk_buff *skb;\n\n\twhile ((skb = skb_dequeue(list)) != NULL)\n\t\tkfree_skb_reason(skb, reason);\n}\nEXPORT_SYMBOL(skb_queue_purge_reason);\n\n \nunsigned int skb_rbtree_purge(struct rb_root *root)\n{\n\tstruct rb_node *p = rb_first(root);\n\tunsigned int sum = 0;\n\n\twhile (p) {\n\t\tstruct sk_buff *skb = rb_entry(p, struct sk_buff, rbnode);\n\n\t\tp = rb_next(p);\n\t\trb_erase(&skb->rbnode, root);\n\t\tsum += skb->truesize;\n\t\tkfree_skb(skb);\n\t}\n\treturn sum;\n}\n\nvoid skb_errqueue_purge(struct sk_buff_head *list)\n{\n\tstruct sk_buff *skb, *next;\n\tstruct sk_buff_head kill;\n\tunsigned long flags;\n\n\t__skb_queue_head_init(&kill);\n\n\tspin_lock_irqsave(&list->lock, flags);\n\tskb_queue_walk_safe(list, skb, next) {\n\t\tif (SKB_EXT_ERR(skb)->ee.ee_origin == SO_EE_ORIGIN_ZEROCOPY ||\n\t\t    SKB_EXT_ERR(skb)->ee.ee_origin == SO_EE_ORIGIN_TIMESTAMPING)\n\t\t\tcontinue;\n\t\t__skb_unlink(skb, list);\n\t\t__skb_queue_tail(&kill, skb);\n\t}\n\tspin_unlock_irqrestore(&list->lock, flags);\n\t__skb_queue_purge(&kill);\n}\nEXPORT_SYMBOL(skb_errqueue_purge);\n\n \nvoid skb_queue_head(struct sk_buff_head *list, struct sk_buff *newsk)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&list->lock, flags);\n\t__skb_queue_head(list, newsk);\n\tspin_unlock_irqrestore(&list->lock, flags);\n}\nEXPORT_SYMBOL(skb_queue_head);\n\n \nvoid skb_queue_tail(struct sk_buff_head *list, struct sk_buff *newsk)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&list->lock, flags);\n\t__skb_queue_tail(list, newsk);\n\tspin_unlock_irqrestore(&list->lock, flags);\n}\nEXPORT_SYMBOL(skb_queue_tail);\n\n \nvoid skb_unlink(struct sk_buff *skb, struct sk_buff_head *list)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&list->lock, flags);\n\t__skb_unlink(skb, list);\n\tspin_unlock_irqrestore(&list->lock, flags);\n}\nEXPORT_SYMBOL(skb_unlink);\n\n \nvoid skb_append(struct sk_buff *old, struct sk_buff *newsk, struct sk_buff_head *list)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&list->lock, flags);\n\t__skb_queue_after(list, old, newsk);\n\tspin_unlock_irqrestore(&list->lock, flags);\n}\nEXPORT_SYMBOL(skb_append);\n\nstatic inline void skb_split_inside_header(struct sk_buff *skb,\n\t\t\t\t\t   struct sk_buff* skb1,\n\t\t\t\t\t   const u32 len, const int pos)\n{\n\tint i;\n\n\tskb_copy_from_linear_data_offset(skb, len, skb_put(skb1, pos - len),\n\t\t\t\t\t pos - len);\n\t \n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++)\n\t\tskb_shinfo(skb1)->frags[i] = skb_shinfo(skb)->frags[i];\n\n\tskb_shinfo(skb1)->nr_frags = skb_shinfo(skb)->nr_frags;\n\tskb_shinfo(skb)->nr_frags  = 0;\n\tskb1->data_len\t\t   = skb->data_len;\n\tskb1->len\t\t   += skb1->data_len;\n\tskb->data_len\t\t   = 0;\n\tskb->len\t\t   = len;\n\tskb_set_tail_pointer(skb, len);\n}\n\nstatic inline void skb_split_no_header(struct sk_buff *skb,\n\t\t\t\t       struct sk_buff* skb1,\n\t\t\t\t       const u32 len, int pos)\n{\n\tint i, k = 0;\n\tconst int nfrags = skb_shinfo(skb)->nr_frags;\n\n\tskb_shinfo(skb)->nr_frags = 0;\n\tskb1->len\t\t  = skb1->data_len = skb->len - len;\n\tskb->len\t\t  = len;\n\tskb->data_len\t\t  = len - pos;\n\n\tfor (i = 0; i < nfrags; i++) {\n\t\tint size = skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\n\t\tif (pos + size > len) {\n\t\t\tskb_shinfo(skb1)->frags[k] = skb_shinfo(skb)->frags[i];\n\n\t\t\tif (pos < len) {\n\t\t\t\t \n\t\t\t\tskb_frag_ref(skb, i);\n\t\t\t\tskb_frag_off_add(&skb_shinfo(skb1)->frags[0], len - pos);\n\t\t\t\tskb_frag_size_sub(&skb_shinfo(skb1)->frags[0], len - pos);\n\t\t\t\tskb_frag_size_set(&skb_shinfo(skb)->frags[i], len - pos);\n\t\t\t\tskb_shinfo(skb)->nr_frags++;\n\t\t\t}\n\t\t\tk++;\n\t\t} else\n\t\t\tskb_shinfo(skb)->nr_frags++;\n\t\tpos += size;\n\t}\n\tskb_shinfo(skb1)->nr_frags = k;\n}\n\n \nvoid skb_split(struct sk_buff *skb, struct sk_buff *skb1, const u32 len)\n{\n\tint pos = skb_headlen(skb);\n\tconst int zc_flags = SKBFL_SHARED_FRAG | SKBFL_PURE_ZEROCOPY;\n\n\tskb_zcopy_downgrade_managed(skb);\n\n\tskb_shinfo(skb1)->flags |= skb_shinfo(skb)->flags & zc_flags;\n\tskb_zerocopy_clone(skb1, skb, 0);\n\tif (len < pos)\t \n\t\tskb_split_inside_header(skb, skb1, len, pos);\n\telse\t\t \n\t\tskb_split_no_header(skb, skb1, len, pos);\n}\nEXPORT_SYMBOL(skb_split);\n\n \nstatic int skb_prepare_for_shift(struct sk_buff *skb)\n{\n\treturn skb_unclone_keeptruesize(skb, GFP_ATOMIC);\n}\n\n \nint skb_shift(struct sk_buff *tgt, struct sk_buff *skb, int shiftlen)\n{\n\tint from, to, merge, todo;\n\tskb_frag_t *fragfrom, *fragto;\n\n\tBUG_ON(shiftlen > skb->len);\n\n\tif (skb_headlen(skb))\n\t\treturn 0;\n\tif (skb_zcopy(tgt) || skb_zcopy(skb))\n\t\treturn 0;\n\n\ttodo = shiftlen;\n\tfrom = 0;\n\tto = skb_shinfo(tgt)->nr_frags;\n\tfragfrom = &skb_shinfo(skb)->frags[from];\n\n\t \n\tif (!to ||\n\t    !skb_can_coalesce(tgt, to, skb_frag_page(fragfrom),\n\t\t\t      skb_frag_off(fragfrom))) {\n\t\tmerge = -1;\n\t} else {\n\t\tmerge = to - 1;\n\n\t\ttodo -= skb_frag_size(fragfrom);\n\t\tif (todo < 0) {\n\t\t\tif (skb_prepare_for_shift(skb) ||\n\t\t\t    skb_prepare_for_shift(tgt))\n\t\t\t\treturn 0;\n\n\t\t\t \n\t\t\tfragfrom = &skb_shinfo(skb)->frags[from];\n\t\t\tfragto = &skb_shinfo(tgt)->frags[merge];\n\n\t\t\tskb_frag_size_add(fragto, shiftlen);\n\t\t\tskb_frag_size_sub(fragfrom, shiftlen);\n\t\t\tskb_frag_off_add(fragfrom, shiftlen);\n\n\t\t\tgoto onlymerged;\n\t\t}\n\n\t\tfrom++;\n\t}\n\n\t \n\tif ((shiftlen == skb->len) &&\n\t    (skb_shinfo(skb)->nr_frags - from) > (MAX_SKB_FRAGS - to))\n\t\treturn 0;\n\n\tif (skb_prepare_for_shift(skb) || skb_prepare_for_shift(tgt))\n\t\treturn 0;\n\n\twhile ((todo > 0) && (from < skb_shinfo(skb)->nr_frags)) {\n\t\tif (to == MAX_SKB_FRAGS)\n\t\t\treturn 0;\n\n\t\tfragfrom = &skb_shinfo(skb)->frags[from];\n\t\tfragto = &skb_shinfo(tgt)->frags[to];\n\n\t\tif (todo >= skb_frag_size(fragfrom)) {\n\t\t\t*fragto = *fragfrom;\n\t\t\ttodo -= skb_frag_size(fragfrom);\n\t\t\tfrom++;\n\t\t\tto++;\n\n\t\t} else {\n\t\t\t__skb_frag_ref(fragfrom);\n\t\t\tskb_frag_page_copy(fragto, fragfrom);\n\t\t\tskb_frag_off_copy(fragto, fragfrom);\n\t\t\tskb_frag_size_set(fragto, todo);\n\n\t\t\tskb_frag_off_add(fragfrom, todo);\n\t\t\tskb_frag_size_sub(fragfrom, todo);\n\t\t\ttodo = 0;\n\n\t\t\tto++;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\tskb_shinfo(tgt)->nr_frags = to;\n\n\tif (merge >= 0) {\n\t\tfragfrom = &skb_shinfo(skb)->frags[0];\n\t\tfragto = &skb_shinfo(tgt)->frags[merge];\n\n\t\tskb_frag_size_add(fragto, skb_frag_size(fragfrom));\n\t\t__skb_frag_unref(fragfrom, skb->pp_recycle);\n\t}\n\n\t \n\tto = 0;\n\twhile (from < skb_shinfo(skb)->nr_frags)\n\t\tskb_shinfo(skb)->frags[to++] = skb_shinfo(skb)->frags[from++];\n\tskb_shinfo(skb)->nr_frags = to;\n\n\tBUG_ON(todo > 0 && !skb_shinfo(skb)->nr_frags);\n\nonlymerged:\n\t \n\ttgt->ip_summed = CHECKSUM_PARTIAL;\n\tskb->ip_summed = CHECKSUM_PARTIAL;\n\n\tskb_len_add(skb, -shiftlen);\n\tskb_len_add(tgt, shiftlen);\n\n\treturn shiftlen;\n}\n\n \nvoid skb_prepare_seq_read(struct sk_buff *skb, unsigned int from,\n\t\t\t  unsigned int to, struct skb_seq_state *st)\n{\n\tst->lower_offset = from;\n\tst->upper_offset = to;\n\tst->root_skb = st->cur_skb = skb;\n\tst->frag_idx = st->stepped_offset = 0;\n\tst->frag_data = NULL;\n\tst->frag_off = 0;\n}\nEXPORT_SYMBOL(skb_prepare_seq_read);\n\n \nunsigned int skb_seq_read(unsigned int consumed, const u8 **data,\n\t\t\t  struct skb_seq_state *st)\n{\n\tunsigned int block_limit, abs_offset = consumed + st->lower_offset;\n\tskb_frag_t *frag;\n\n\tif (unlikely(abs_offset >= st->upper_offset)) {\n\t\tif (st->frag_data) {\n\t\t\tkunmap_atomic(st->frag_data);\n\t\t\tst->frag_data = NULL;\n\t\t}\n\t\treturn 0;\n\t}\n\nnext_skb:\n\tblock_limit = skb_headlen(st->cur_skb) + st->stepped_offset;\n\n\tif (abs_offset < block_limit && !st->frag_data) {\n\t\t*data = st->cur_skb->data + (abs_offset - st->stepped_offset);\n\t\treturn block_limit - abs_offset;\n\t}\n\n\tif (st->frag_idx == 0 && !st->frag_data)\n\t\tst->stepped_offset += skb_headlen(st->cur_skb);\n\n\twhile (st->frag_idx < skb_shinfo(st->cur_skb)->nr_frags) {\n\t\tunsigned int pg_idx, pg_off, pg_sz;\n\n\t\tfrag = &skb_shinfo(st->cur_skb)->frags[st->frag_idx];\n\n\t\tpg_idx = 0;\n\t\tpg_off = skb_frag_off(frag);\n\t\tpg_sz = skb_frag_size(frag);\n\n\t\tif (skb_frag_must_loop(skb_frag_page(frag))) {\n\t\t\tpg_idx = (pg_off + st->frag_off) >> PAGE_SHIFT;\n\t\t\tpg_off = offset_in_page(pg_off + st->frag_off);\n\t\t\tpg_sz = min_t(unsigned int, pg_sz - st->frag_off,\n\t\t\t\t\t\t    PAGE_SIZE - pg_off);\n\t\t}\n\n\t\tblock_limit = pg_sz + st->stepped_offset;\n\t\tif (abs_offset < block_limit) {\n\t\t\tif (!st->frag_data)\n\t\t\t\tst->frag_data = kmap_atomic(skb_frag_page(frag) + pg_idx);\n\n\t\t\t*data = (u8 *)st->frag_data + pg_off +\n\t\t\t\t(abs_offset - st->stepped_offset);\n\n\t\t\treturn block_limit - abs_offset;\n\t\t}\n\n\t\tif (st->frag_data) {\n\t\t\tkunmap_atomic(st->frag_data);\n\t\t\tst->frag_data = NULL;\n\t\t}\n\n\t\tst->stepped_offset += pg_sz;\n\t\tst->frag_off += pg_sz;\n\t\tif (st->frag_off == skb_frag_size(frag)) {\n\t\t\tst->frag_off = 0;\n\t\t\tst->frag_idx++;\n\t\t}\n\t}\n\n\tif (st->frag_data) {\n\t\tkunmap_atomic(st->frag_data);\n\t\tst->frag_data = NULL;\n\t}\n\n\tif (st->root_skb == st->cur_skb && skb_has_frag_list(st->root_skb)) {\n\t\tst->cur_skb = skb_shinfo(st->root_skb)->frag_list;\n\t\tst->frag_idx = 0;\n\t\tgoto next_skb;\n\t} else if (st->cur_skb->next) {\n\t\tst->cur_skb = st->cur_skb->next;\n\t\tst->frag_idx = 0;\n\t\tgoto next_skb;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(skb_seq_read);\n\n \nvoid skb_abort_seq_read(struct skb_seq_state *st)\n{\n\tif (st->frag_data)\n\t\tkunmap_atomic(st->frag_data);\n}\nEXPORT_SYMBOL(skb_abort_seq_read);\n\n#define TS_SKB_CB(state)\t((struct skb_seq_state *) &((state)->cb))\n\nstatic unsigned int skb_ts_get_next_block(unsigned int offset, const u8 **text,\n\t\t\t\t\t  struct ts_config *conf,\n\t\t\t\t\t  struct ts_state *state)\n{\n\treturn skb_seq_read(offset, text, TS_SKB_CB(state));\n}\n\nstatic void skb_ts_finish(struct ts_config *conf, struct ts_state *state)\n{\n\tskb_abort_seq_read(TS_SKB_CB(state));\n}\n\n \nunsigned int skb_find_text(struct sk_buff *skb, unsigned int from,\n\t\t\t   unsigned int to, struct ts_config *config)\n{\n\tunsigned int patlen = config->ops->get_pattern_len(config);\n\tstruct ts_state state;\n\tunsigned int ret;\n\n\tBUILD_BUG_ON(sizeof(struct skb_seq_state) > sizeof(state.cb));\n\n\tconfig->get_next_block = skb_ts_get_next_block;\n\tconfig->finish = skb_ts_finish;\n\n\tskb_prepare_seq_read(skb, from, to, TS_SKB_CB(&state));\n\n\tret = textsearch_find(config, &state);\n\treturn (ret + patlen <= to - from ? ret : UINT_MAX);\n}\nEXPORT_SYMBOL(skb_find_text);\n\nint skb_append_pagefrags(struct sk_buff *skb, struct page *page,\n\t\t\t int offset, size_t size, size_t max_frags)\n{\n\tint i = skb_shinfo(skb)->nr_frags;\n\n\tif (skb_can_coalesce(skb, i, page, offset)) {\n\t\tskb_frag_size_add(&skb_shinfo(skb)->frags[i - 1], size);\n\t} else if (i < max_frags) {\n\t\tskb_zcopy_downgrade_managed(skb);\n\t\tget_page(page);\n\t\tskb_fill_page_desc_noacc(skb, i, page, offset, size);\n\t} else {\n\t\treturn -EMSGSIZE;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(skb_append_pagefrags);\n\n \nvoid *skb_pull_rcsum(struct sk_buff *skb, unsigned int len)\n{\n\tunsigned char *data = skb->data;\n\n\tBUG_ON(len > skb->len);\n\t__skb_pull(skb, len);\n\tskb_postpull_rcsum(skb, data, len);\n\treturn skb->data;\n}\nEXPORT_SYMBOL_GPL(skb_pull_rcsum);\n\nstatic inline skb_frag_t skb_head_frag_to_page_desc(struct sk_buff *frag_skb)\n{\n\tskb_frag_t head_frag;\n\tstruct page *page;\n\n\tpage = virt_to_head_page(frag_skb->head);\n\tskb_frag_fill_page_desc(&head_frag, page, frag_skb->data -\n\t\t\t\t(unsigned char *)page_address(page),\n\t\t\t\tskb_headlen(frag_skb));\n\treturn head_frag;\n}\n\nstruct sk_buff *skb_segment_list(struct sk_buff *skb,\n\t\t\t\t netdev_features_t features,\n\t\t\t\t unsigned int offset)\n{\n\tstruct sk_buff *list_skb = skb_shinfo(skb)->frag_list;\n\tunsigned int tnl_hlen = skb_tnl_header_len(skb);\n\tunsigned int delta_truesize = 0;\n\tunsigned int delta_len = 0;\n\tstruct sk_buff *tail = NULL;\n\tstruct sk_buff *nskb, *tmp;\n\tint len_diff, err;\n\n\tskb_push(skb, -skb_network_offset(skb) + offset);\n\n\t \n\terr = skb_unclone(skb, GFP_ATOMIC);\n\tif (err)\n\t\tgoto err_linearize;\n\n\tskb_shinfo(skb)->frag_list = NULL;\n\n\twhile (list_skb) {\n\t\tnskb = list_skb;\n\t\tlist_skb = list_skb->next;\n\n\t\terr = 0;\n\t\tdelta_truesize += nskb->truesize;\n\t\tif (skb_shared(nskb)) {\n\t\t\ttmp = skb_clone(nskb, GFP_ATOMIC);\n\t\t\tif (tmp) {\n\t\t\t\tconsume_skb(nskb);\n\t\t\t\tnskb = tmp;\n\t\t\t\terr = skb_unclone(nskb, GFP_ATOMIC);\n\t\t\t} else {\n\t\t\t\terr = -ENOMEM;\n\t\t\t}\n\t\t}\n\n\t\tif (!tail)\n\t\t\tskb->next = nskb;\n\t\telse\n\t\t\ttail->next = nskb;\n\n\t\tif (unlikely(err)) {\n\t\t\tnskb->next = list_skb;\n\t\t\tgoto err_linearize;\n\t\t}\n\n\t\ttail = nskb;\n\n\t\tdelta_len += nskb->len;\n\n\t\tskb_push(nskb, -skb_network_offset(nskb) + offset);\n\n\t\tskb_release_head_state(nskb);\n\t\tlen_diff = skb_network_header_len(nskb) - skb_network_header_len(skb);\n\t\t__copy_skb_header(nskb, skb);\n\n\t\tskb_headers_offset_update(nskb, skb_headroom(nskb) - skb_headroom(skb));\n\t\tnskb->transport_header += len_diff;\n\t\tskb_copy_from_linear_data_offset(skb, -tnl_hlen,\n\t\t\t\t\t\t nskb->data - tnl_hlen,\n\t\t\t\t\t\t offset + tnl_hlen);\n\n\t\tif (skb_needs_linearize(nskb, features) &&\n\t\t    __skb_linearize(nskb))\n\t\t\tgoto err_linearize;\n\t}\n\n\tskb->truesize = skb->truesize - delta_truesize;\n\tskb->data_len = skb->data_len - delta_len;\n\tskb->len = skb->len - delta_len;\n\n\tskb_gso_reset(skb);\n\n\tskb->prev = tail;\n\n\tif (skb_needs_linearize(skb, features) &&\n\t    __skb_linearize(skb))\n\t\tgoto err_linearize;\n\n\tskb_get(skb);\n\n\treturn skb;\n\nerr_linearize:\n\tkfree_skb_list(skb->next);\n\tskb->next = NULL;\n\treturn ERR_PTR(-ENOMEM);\n}\nEXPORT_SYMBOL_GPL(skb_segment_list);\n\n \nstruct sk_buff *skb_segment(struct sk_buff *head_skb,\n\t\t\t    netdev_features_t features)\n{\n\tstruct sk_buff *segs = NULL;\n\tstruct sk_buff *tail = NULL;\n\tstruct sk_buff *list_skb = skb_shinfo(head_skb)->frag_list;\n\tunsigned int mss = skb_shinfo(head_skb)->gso_size;\n\tunsigned int doffset = head_skb->data - skb_mac_header(head_skb);\n\tunsigned int offset = doffset;\n\tunsigned int tnl_hlen = skb_tnl_header_len(head_skb);\n\tunsigned int partial_segs = 0;\n\tunsigned int headroom;\n\tunsigned int len = head_skb->len;\n\tstruct sk_buff *frag_skb;\n\tskb_frag_t *frag;\n\t__be16 proto;\n\tbool csum, sg;\n\tint err = -ENOMEM;\n\tint i = 0;\n\tint nfrags, pos;\n\n\tif ((skb_shinfo(head_skb)->gso_type & SKB_GSO_DODGY) &&\n\t    mss != GSO_BY_FRAGS && mss != skb_headlen(head_skb)) {\n\t\tstruct sk_buff *check_skb;\n\n\t\tfor (check_skb = list_skb; check_skb; check_skb = check_skb->next) {\n\t\t\tif (skb_headlen(check_skb) && !check_skb->head_frag) {\n\t\t\t\t \n\t\t\t\tfeatures &= ~NETIF_F_SG;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\t__skb_push(head_skb, doffset);\n\tproto = skb_network_protocol(head_skb, NULL);\n\tif (unlikely(!proto))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tsg = !!(features & NETIF_F_SG);\n\tcsum = !!can_checksum_protocol(features, proto);\n\n\tif (sg && csum && (mss != GSO_BY_FRAGS))  {\n\t\tif (!(features & NETIF_F_GSO_PARTIAL)) {\n\t\t\tstruct sk_buff *iter;\n\t\t\tunsigned int frag_len;\n\n\t\t\tif (!list_skb ||\n\t\t\t    !net_gso_ok(features, skb_shinfo(head_skb)->gso_type))\n\t\t\t\tgoto normal;\n\n\t\t\t \n\t\t\tfrag_len = list_skb->len;\n\t\t\tskb_walk_frags(head_skb, iter) {\n\t\t\t\tif (frag_len != iter->len && iter->next)\n\t\t\t\t\tgoto normal;\n\t\t\t\tif (skb_headlen(iter) && !iter->head_frag)\n\t\t\t\t\tgoto normal;\n\n\t\t\t\tlen -= iter->len;\n\t\t\t}\n\n\t\t\tif (len != frag_len)\n\t\t\t\tgoto normal;\n\t\t}\n\n\t\t \n\t\tpartial_segs = min(len, GSO_BY_FRAGS - 1U) / mss;\n\t\tif (partial_segs > 1)\n\t\t\tmss *= partial_segs;\n\t\telse\n\t\t\tpartial_segs = 0;\n\t}\n\nnormal:\n\theadroom = skb_headroom(head_skb);\n\tpos = skb_headlen(head_skb);\n\n\tif (skb_orphan_frags(head_skb, GFP_ATOMIC))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tnfrags = skb_shinfo(head_skb)->nr_frags;\n\tfrag = skb_shinfo(head_skb)->frags;\n\tfrag_skb = head_skb;\n\n\tdo {\n\t\tstruct sk_buff *nskb;\n\t\tskb_frag_t *nskb_frag;\n\t\tint hsize;\n\t\tint size;\n\n\t\tif (unlikely(mss == GSO_BY_FRAGS)) {\n\t\t\tlen = list_skb->len;\n\t\t} else {\n\t\t\tlen = head_skb->len - offset;\n\t\t\tif (len > mss)\n\t\t\t\tlen = mss;\n\t\t}\n\n\t\thsize = skb_headlen(head_skb) - offset;\n\n\t\tif (hsize <= 0 && i >= nfrags && skb_headlen(list_skb) &&\n\t\t    (skb_headlen(list_skb) == len || sg)) {\n\t\t\tBUG_ON(skb_headlen(list_skb) > len);\n\n\t\t\tnskb = skb_clone(list_skb, GFP_ATOMIC);\n\t\t\tif (unlikely(!nskb))\n\t\t\t\tgoto err;\n\n\t\t\ti = 0;\n\t\t\tnfrags = skb_shinfo(list_skb)->nr_frags;\n\t\t\tfrag = skb_shinfo(list_skb)->frags;\n\t\t\tfrag_skb = list_skb;\n\t\t\tpos += skb_headlen(list_skb);\n\n\t\t\twhile (pos < offset + len) {\n\t\t\t\tBUG_ON(i >= nfrags);\n\n\t\t\t\tsize = skb_frag_size(frag);\n\t\t\t\tif (pos + size > offset + len)\n\t\t\t\t\tbreak;\n\n\t\t\t\ti++;\n\t\t\t\tpos += size;\n\t\t\t\tfrag++;\n\t\t\t}\n\n\t\t\tlist_skb = list_skb->next;\n\n\t\t\tif (unlikely(pskb_trim(nskb, len))) {\n\t\t\t\tkfree_skb(nskb);\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\thsize = skb_end_offset(nskb);\n\t\t\tif (skb_cow_head(nskb, doffset + headroom)) {\n\t\t\t\tkfree_skb(nskb);\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\tnskb->truesize += skb_end_offset(nskb) - hsize;\n\t\t\tskb_release_head_state(nskb);\n\t\t\t__skb_push(nskb, doffset);\n\t\t} else {\n\t\t\tif (hsize < 0)\n\t\t\t\thsize = 0;\n\t\t\tif (hsize > len || !sg)\n\t\t\t\thsize = len;\n\n\t\t\tnskb = __alloc_skb(hsize + doffset + headroom,\n\t\t\t\t\t   GFP_ATOMIC, skb_alloc_rx_flag(head_skb),\n\t\t\t\t\t   NUMA_NO_NODE);\n\n\t\t\tif (unlikely(!nskb))\n\t\t\t\tgoto err;\n\n\t\t\tskb_reserve(nskb, headroom);\n\t\t\t__skb_put(nskb, doffset);\n\t\t}\n\n\t\tif (segs)\n\t\t\ttail->next = nskb;\n\t\telse\n\t\t\tsegs = nskb;\n\t\ttail = nskb;\n\n\t\t__copy_skb_header(nskb, head_skb);\n\n\t\tskb_headers_offset_update(nskb, skb_headroom(nskb) - headroom);\n\t\tskb_reset_mac_len(nskb);\n\n\t\tskb_copy_from_linear_data_offset(head_skb, -tnl_hlen,\n\t\t\t\t\t\t nskb->data - tnl_hlen,\n\t\t\t\t\t\t doffset + tnl_hlen);\n\n\t\tif (nskb->len == len + doffset)\n\t\t\tgoto perform_csum_check;\n\n\t\tif (!sg) {\n\t\t\tif (!csum) {\n\t\t\t\tif (!nskb->remcsum_offload)\n\t\t\t\t\tnskb->ip_summed = CHECKSUM_NONE;\n\t\t\t\tSKB_GSO_CB(nskb)->csum =\n\t\t\t\t\tskb_copy_and_csum_bits(head_skb, offset,\n\t\t\t\t\t\t\t       skb_put(nskb,\n\t\t\t\t\t\t\t\t       len),\n\t\t\t\t\t\t\t       len);\n\t\t\t\tSKB_GSO_CB(nskb)->csum_start =\n\t\t\t\t\tskb_headroom(nskb) + doffset;\n\t\t\t} else {\n\t\t\t\tif (skb_copy_bits(head_skb, offset, skb_put(nskb, len), len))\n\t\t\t\t\tgoto err;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\tnskb_frag = skb_shinfo(nskb)->frags;\n\n\t\tskb_copy_from_linear_data_offset(head_skb, offset,\n\t\t\t\t\t\t skb_put(nskb, hsize), hsize);\n\n\t\tskb_shinfo(nskb)->flags |= skb_shinfo(head_skb)->flags &\n\t\t\t\t\t   SKBFL_SHARED_FRAG;\n\n\t\tif (skb_zerocopy_clone(nskb, frag_skb, GFP_ATOMIC))\n\t\t\tgoto err;\n\n\t\twhile (pos < offset + len) {\n\t\t\tif (i >= nfrags) {\n\t\t\t\tif (skb_orphan_frags(list_skb, GFP_ATOMIC) ||\n\t\t\t\t    skb_zerocopy_clone(nskb, list_skb,\n\t\t\t\t\t\t       GFP_ATOMIC))\n\t\t\t\t\tgoto err;\n\n\t\t\t\ti = 0;\n\t\t\t\tnfrags = skb_shinfo(list_skb)->nr_frags;\n\t\t\t\tfrag = skb_shinfo(list_skb)->frags;\n\t\t\t\tfrag_skb = list_skb;\n\t\t\t\tif (!skb_headlen(list_skb)) {\n\t\t\t\t\tBUG_ON(!nfrags);\n\t\t\t\t} else {\n\t\t\t\t\tBUG_ON(!list_skb->head_frag);\n\n\t\t\t\t\t \n\t\t\t\t\ti--;\n\t\t\t\t\tfrag--;\n\t\t\t\t}\n\n\t\t\t\tlist_skb = list_skb->next;\n\t\t\t}\n\n\t\t\tif (unlikely(skb_shinfo(nskb)->nr_frags >=\n\t\t\t\t     MAX_SKB_FRAGS)) {\n\t\t\t\tnet_warn_ratelimited(\n\t\t\t\t\t\"skb_segment: too many frags: %u %u\\n\",\n\t\t\t\t\tpos, mss);\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\t*nskb_frag = (i < 0) ? skb_head_frag_to_page_desc(frag_skb) : *frag;\n\t\t\t__skb_frag_ref(nskb_frag);\n\t\t\tsize = skb_frag_size(nskb_frag);\n\n\t\t\tif (pos < offset) {\n\t\t\t\tskb_frag_off_add(nskb_frag, offset - pos);\n\t\t\t\tskb_frag_size_sub(nskb_frag, offset - pos);\n\t\t\t}\n\n\t\t\tskb_shinfo(nskb)->nr_frags++;\n\n\t\t\tif (pos + size <= offset + len) {\n\t\t\t\ti++;\n\t\t\t\tfrag++;\n\t\t\t\tpos += size;\n\t\t\t} else {\n\t\t\t\tskb_frag_size_sub(nskb_frag, pos + size - (offset + len));\n\t\t\t\tgoto skip_fraglist;\n\t\t\t}\n\n\t\t\tnskb_frag++;\n\t\t}\n\nskip_fraglist:\n\t\tnskb->data_len = len - hsize;\n\t\tnskb->len += nskb->data_len;\n\t\tnskb->truesize += nskb->data_len;\n\nperform_csum_check:\n\t\tif (!csum) {\n\t\t\tif (skb_has_shared_frag(nskb) &&\n\t\t\t    __skb_linearize(nskb))\n\t\t\t\tgoto err;\n\n\t\t\tif (!nskb->remcsum_offload)\n\t\t\t\tnskb->ip_summed = CHECKSUM_NONE;\n\t\t\tSKB_GSO_CB(nskb)->csum =\n\t\t\t\tskb_checksum(nskb, doffset,\n\t\t\t\t\t     nskb->len - doffset, 0);\n\t\t\tSKB_GSO_CB(nskb)->csum_start =\n\t\t\t\tskb_headroom(nskb) + doffset;\n\t\t}\n\t} while ((offset += len) < head_skb->len);\n\n\t \n\tsegs->prev = tail;\n\n\tif (partial_segs) {\n\t\tstruct sk_buff *iter;\n\t\tint type = skb_shinfo(head_skb)->gso_type;\n\t\tunsigned short gso_size = skb_shinfo(head_skb)->gso_size;\n\n\t\t \n\t\ttype |= (features & NETIF_F_GSO_PARTIAL) / NETIF_F_GSO_PARTIAL * SKB_GSO_PARTIAL;\n\t\ttype &= ~SKB_GSO_DODGY;\n\n\t\t \n\t\tfor (iter = segs; iter; iter = iter->next) {\n\t\t\tskb_shinfo(iter)->gso_size = gso_size;\n\t\t\tskb_shinfo(iter)->gso_segs = partial_segs;\n\t\t\tskb_shinfo(iter)->gso_type = type;\n\t\t\tSKB_GSO_CB(iter)->data_offset = skb_headroom(iter) + doffset;\n\t\t}\n\n\t\tif (tail->len - doffset <= gso_size)\n\t\t\tskb_shinfo(tail)->gso_size = 0;\n\t\telse if (tail != segs)\n\t\t\tskb_shinfo(tail)->gso_segs = DIV_ROUND_UP(tail->len - doffset, gso_size);\n\t}\n\n\t \n\tif (head_skb->destructor == sock_wfree) {\n\t\tswap(tail->truesize, head_skb->truesize);\n\t\tswap(tail->destructor, head_skb->destructor);\n\t\tswap(tail->sk, head_skb->sk);\n\t}\n\treturn segs;\n\nerr:\n\tkfree_skb_list(segs);\n\treturn ERR_PTR(err);\n}\nEXPORT_SYMBOL_GPL(skb_segment);\n\n#ifdef CONFIG_SKB_EXTENSIONS\n#define SKB_EXT_ALIGN_VALUE\t8\n#define SKB_EXT_CHUNKSIZEOF(x)\t(ALIGN((sizeof(x)), SKB_EXT_ALIGN_VALUE) / SKB_EXT_ALIGN_VALUE)\n\nstatic const u8 skb_ext_type_len[] = {\n#if IS_ENABLED(CONFIG_BRIDGE_NETFILTER)\n\t[SKB_EXT_BRIDGE_NF] = SKB_EXT_CHUNKSIZEOF(struct nf_bridge_info),\n#endif\n#ifdef CONFIG_XFRM\n\t[SKB_EXT_SEC_PATH] = SKB_EXT_CHUNKSIZEOF(struct sec_path),\n#endif\n#if IS_ENABLED(CONFIG_NET_TC_SKB_EXT)\n\t[TC_SKB_EXT] = SKB_EXT_CHUNKSIZEOF(struct tc_skb_ext),\n#endif\n#if IS_ENABLED(CONFIG_MPTCP)\n\t[SKB_EXT_MPTCP] = SKB_EXT_CHUNKSIZEOF(struct mptcp_ext),\n#endif\n#if IS_ENABLED(CONFIG_MCTP_FLOWS)\n\t[SKB_EXT_MCTP] = SKB_EXT_CHUNKSIZEOF(struct mctp_flow),\n#endif\n};\n\nstatic __always_inline unsigned int skb_ext_total_length(void)\n{\n\tunsigned int l = SKB_EXT_CHUNKSIZEOF(struct skb_ext);\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(skb_ext_type_len); i++)\n\t\tl += skb_ext_type_len[i];\n\n\treturn l;\n}\n\nstatic void skb_extensions_init(void)\n{\n\tBUILD_BUG_ON(SKB_EXT_NUM >= 8);\n#if !IS_ENABLED(CONFIG_KCOV_INSTRUMENT_ALL)\n\tBUILD_BUG_ON(skb_ext_total_length() > 255);\n#endif\n\n\tskbuff_ext_cache = kmem_cache_create(\"skbuff_ext_cache\",\n\t\t\t\t\t     SKB_EXT_ALIGN_VALUE * skb_ext_total_length(),\n\t\t\t\t\t     0,\n\t\t\t\t\t     SLAB_HWCACHE_ALIGN|SLAB_PANIC,\n\t\t\t\t\t     NULL);\n}\n#else\nstatic void skb_extensions_init(void) {}\n#endif\n\n \n#ifndef CONFIG_SLUB_TINY\n#define FLAG_SKB_NO_MERGE\tSLAB_NO_MERGE\n#else  \n#define FLAG_SKB_NO_MERGE\t0\n#endif\n\nvoid __init skb_init(void)\n{\n\tskbuff_cache = kmem_cache_create_usercopy(\"skbuff_head_cache\",\n\t\t\t\t\t      sizeof(struct sk_buff),\n\t\t\t\t\t      0,\n\t\t\t\t\t      SLAB_HWCACHE_ALIGN|SLAB_PANIC|\n\t\t\t\t\t\tFLAG_SKB_NO_MERGE,\n\t\t\t\t\t      offsetof(struct sk_buff, cb),\n\t\t\t\t\t      sizeof_field(struct sk_buff, cb),\n\t\t\t\t\t      NULL);\n\tskbuff_fclone_cache = kmem_cache_create(\"skbuff_fclone_cache\",\n\t\t\t\t\t\tsizeof(struct sk_buff_fclones),\n\t\t\t\t\t\t0,\n\t\t\t\t\t\tSLAB_HWCACHE_ALIGN|SLAB_PANIC,\n\t\t\t\t\t\tNULL);\n\t \n\tskb_small_head_cache = kmem_cache_create_usercopy(\"skbuff_small_head\",\n\t\t\t\t\t\tSKB_SMALL_HEAD_CACHE_SIZE,\n\t\t\t\t\t\t0,\n\t\t\t\t\t\tSLAB_HWCACHE_ALIGN | SLAB_PANIC,\n\t\t\t\t\t\t0,\n\t\t\t\t\t\tSKB_SMALL_HEAD_HEADROOM,\n\t\t\t\t\t\tNULL);\n\tskb_extensions_init();\n}\n\nstatic int\n__skb_to_sgvec(struct sk_buff *skb, struct scatterlist *sg, int offset, int len,\n\t       unsigned int recursion_level)\n{\n\tint start = skb_headlen(skb);\n\tint i, copy = start - offset;\n\tstruct sk_buff *frag_iter;\n\tint elt = 0;\n\n\tif (unlikely(recursion_level >= 24))\n\t\treturn -EMSGSIZE;\n\n\tif (copy > 0) {\n\t\tif (copy > len)\n\t\t\tcopy = len;\n\t\tsg_set_buf(sg, skb->data + offset, copy);\n\t\telt++;\n\t\tif ((len -= copy) == 0)\n\t\t\treturn elt;\n\t\toffset += copy;\n\t}\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\t\t\tif (unlikely(elt && sg_is_last(&sg[elt - 1])))\n\t\t\t\treturn -EMSGSIZE;\n\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tsg_set_page(&sg[elt], skb_frag_page(frag), copy,\n\t\t\t\t    skb_frag_off(frag) + offset - start);\n\t\t\telt++;\n\t\t\tif (!(len -= copy))\n\t\t\t\treturn elt;\n\t\t\toffset += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\n\tskb_walk_frags(skb, frag_iter) {\n\t\tint end, ret;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + frag_iter->len;\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tif (unlikely(elt && sg_is_last(&sg[elt - 1])))\n\t\t\t\treturn -EMSGSIZE;\n\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tret = __skb_to_sgvec(frag_iter, sg+elt, offset - start,\n\t\t\t\t\t      copy, recursion_level + 1);\n\t\t\tif (unlikely(ret < 0))\n\t\t\t\treturn ret;\n\t\t\telt += ret;\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn elt;\n\t\t\toffset += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\tBUG_ON(len);\n\treturn elt;\n}\n\n \nint skb_to_sgvec(struct sk_buff *skb, struct scatterlist *sg, int offset, int len)\n{\n\tint nsg = __skb_to_sgvec(skb, sg, offset, len, 0);\n\n\tif (nsg <= 0)\n\t\treturn nsg;\n\n\tsg_mark_end(&sg[nsg - 1]);\n\n\treturn nsg;\n}\nEXPORT_SYMBOL_GPL(skb_to_sgvec);\n\n \nint skb_to_sgvec_nomark(struct sk_buff *skb, struct scatterlist *sg,\n\t\t\tint offset, int len)\n{\n\treturn __skb_to_sgvec(skb, sg, offset, len, 0);\n}\nEXPORT_SYMBOL_GPL(skb_to_sgvec_nomark);\n\n\n\n \nint skb_cow_data(struct sk_buff *skb, int tailbits, struct sk_buff **trailer)\n{\n\tint copyflag;\n\tint elt;\n\tstruct sk_buff *skb1, **skb_p;\n\n\t \n\tif ((skb_cloned(skb) || skb_shinfo(skb)->nr_frags) &&\n\t    !__pskb_pull_tail(skb, __skb_pagelen(skb)))\n\t\treturn -ENOMEM;\n\n\t \n\tif (!skb_has_frag_list(skb)) {\n\t\t \n\n\t\tif (skb_tailroom(skb) < tailbits &&\n\t\t    pskb_expand_head(skb, 0, tailbits-skb_tailroom(skb)+128, GFP_ATOMIC))\n\t\t\treturn -ENOMEM;\n\n\t\t \n\t\t*trailer = skb;\n\t\treturn 1;\n\t}\n\n\t \n\n\telt = 1;\n\tskb_p = &skb_shinfo(skb)->frag_list;\n\tcopyflag = 0;\n\n\twhile ((skb1 = *skb_p) != NULL) {\n\t\tint ntail = 0;\n\n\t\t \n\n\t\tif (skb_shared(skb1))\n\t\t\tcopyflag = 1;\n\n\t\t \n\n\t\tif (skb1->next == NULL && tailbits) {\n\t\t\tif (skb_shinfo(skb1)->nr_frags ||\n\t\t\t    skb_has_frag_list(skb1) ||\n\t\t\t    skb_tailroom(skb1) < tailbits)\n\t\t\t\tntail = tailbits + 128;\n\t\t}\n\n\t\tif (copyflag ||\n\t\t    skb_cloned(skb1) ||\n\t\t    ntail ||\n\t\t    skb_shinfo(skb1)->nr_frags ||\n\t\t    skb_has_frag_list(skb1)) {\n\t\t\tstruct sk_buff *skb2;\n\n\t\t\t \n\t\t\tif (ntail == 0)\n\t\t\t\tskb2 = skb_copy(skb1, GFP_ATOMIC);\n\t\t\telse\n\t\t\t\tskb2 = skb_copy_expand(skb1,\n\t\t\t\t\t\t       skb_headroom(skb1),\n\t\t\t\t\t\t       ntail,\n\t\t\t\t\t\t       GFP_ATOMIC);\n\t\t\tif (unlikely(skb2 == NULL))\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tif (skb1->sk)\n\t\t\t\tskb_set_owner_w(skb2, skb1->sk);\n\n\t\t\t \n\n\t\t\tskb2->next = skb1->next;\n\t\t\t*skb_p = skb2;\n\t\t\tkfree_skb(skb1);\n\t\t\tskb1 = skb2;\n\t\t}\n\t\telt++;\n\t\t*trailer = skb1;\n\t\tskb_p = &skb1->next;\n\t}\n\n\treturn elt;\n}\nEXPORT_SYMBOL_GPL(skb_cow_data);\n\nstatic void sock_rmem_free(struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\n\tatomic_sub(skb->truesize, &sk->sk_rmem_alloc);\n}\n\nstatic void skb_set_err_queue(struct sk_buff *skb)\n{\n\t \n\tskb->pkt_type = PACKET_OUTGOING;\n\tBUILD_BUG_ON(PACKET_OUTGOING == 0);\n}\n\n \nint sock_queue_err_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tif (atomic_read(&sk->sk_rmem_alloc) + skb->truesize >=\n\t    (unsigned int)READ_ONCE(sk->sk_rcvbuf))\n\t\treturn -ENOMEM;\n\n\tskb_orphan(skb);\n\tskb->sk = sk;\n\tskb->destructor = sock_rmem_free;\n\tatomic_add(skb->truesize, &sk->sk_rmem_alloc);\n\tskb_set_err_queue(skb);\n\n\t \n\tskb_dst_force(skb);\n\n\tskb_queue_tail(&sk->sk_error_queue, skb);\n\tif (!sock_flag(sk, SOCK_DEAD))\n\t\tsk_error_report(sk);\n\treturn 0;\n}\nEXPORT_SYMBOL(sock_queue_err_skb);\n\nstatic bool is_icmp_err_skb(const struct sk_buff *skb)\n{\n\treturn skb && (SKB_EXT_ERR(skb)->ee.ee_origin == SO_EE_ORIGIN_ICMP ||\n\t\t       SKB_EXT_ERR(skb)->ee.ee_origin == SO_EE_ORIGIN_ICMP6);\n}\n\nstruct sk_buff *sock_dequeue_err_skb(struct sock *sk)\n{\n\tstruct sk_buff_head *q = &sk->sk_error_queue;\n\tstruct sk_buff *skb, *skb_next = NULL;\n\tbool icmp_next = false;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&q->lock, flags);\n\tskb = __skb_dequeue(q);\n\tif (skb && (skb_next = skb_peek(q))) {\n\t\ticmp_next = is_icmp_err_skb(skb_next);\n\t\tif (icmp_next)\n\t\t\tsk->sk_err = SKB_EXT_ERR(skb_next)->ee.ee_errno;\n\t}\n\tspin_unlock_irqrestore(&q->lock, flags);\n\n\tif (is_icmp_err_skb(skb) && !icmp_next)\n\t\tsk->sk_err = 0;\n\n\tif (skb_next)\n\t\tsk_error_report(sk);\n\n\treturn skb;\n}\nEXPORT_SYMBOL(sock_dequeue_err_skb);\n\n \nstruct sk_buff *skb_clone_sk(struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\tstruct sk_buff *clone;\n\n\tif (!sk || !refcount_inc_not_zero(&sk->sk_refcnt))\n\t\treturn NULL;\n\n\tclone = skb_clone(skb, GFP_ATOMIC);\n\tif (!clone) {\n\t\tsock_put(sk);\n\t\treturn NULL;\n\t}\n\n\tclone->sk = sk;\n\tclone->destructor = sock_efree;\n\n\treturn clone;\n}\nEXPORT_SYMBOL(skb_clone_sk);\n\nstatic void __skb_complete_tx_timestamp(struct sk_buff *skb,\n\t\t\t\t\tstruct sock *sk,\n\t\t\t\t\tint tstype,\n\t\t\t\t\tbool opt_stats)\n{\n\tstruct sock_exterr_skb *serr;\n\tint err;\n\n\tBUILD_BUG_ON(sizeof(struct sock_exterr_skb) > sizeof(skb->cb));\n\n\tserr = SKB_EXT_ERR(skb);\n\tmemset(serr, 0, sizeof(*serr));\n\tserr->ee.ee_errno = ENOMSG;\n\tserr->ee.ee_origin = SO_EE_ORIGIN_TIMESTAMPING;\n\tserr->ee.ee_info = tstype;\n\tserr->opt_stats = opt_stats;\n\tserr->header.h4.iif = skb->dev ? skb->dev->ifindex : 0;\n\tif (READ_ONCE(sk->sk_tsflags) & SOF_TIMESTAMPING_OPT_ID) {\n\t\tserr->ee.ee_data = skb_shinfo(skb)->tskey;\n\t\tif (sk_is_tcp(sk))\n\t\t\tserr->ee.ee_data -= atomic_read(&sk->sk_tskey);\n\t}\n\n\terr = sock_queue_err_skb(sk, skb);\n\n\tif (err)\n\t\tkfree_skb(skb);\n}\n\nstatic bool skb_may_tx_timestamp(struct sock *sk, bool tsonly)\n{\n\tbool ret;\n\n\tif (likely(READ_ONCE(sysctl_tstamp_allow_data) || tsonly))\n\t\treturn true;\n\n\tread_lock_bh(&sk->sk_callback_lock);\n\tret = sk->sk_socket && sk->sk_socket->file &&\n\t      file_ns_capable(sk->sk_socket->file, &init_user_ns, CAP_NET_RAW);\n\tread_unlock_bh(&sk->sk_callback_lock);\n\treturn ret;\n}\n\nvoid skb_complete_tx_timestamp(struct sk_buff *skb,\n\t\t\t       struct skb_shared_hwtstamps *hwtstamps)\n{\n\tstruct sock *sk = skb->sk;\n\n\tif (!skb_may_tx_timestamp(sk, false))\n\t\tgoto err;\n\n\t \n\tif (likely(refcount_inc_not_zero(&sk->sk_refcnt))) {\n\t\t*skb_hwtstamps(skb) = *hwtstamps;\n\t\t__skb_complete_tx_timestamp(skb, sk, SCM_TSTAMP_SND, false);\n\t\tsock_put(sk);\n\t\treturn;\n\t}\n\nerr:\n\tkfree_skb(skb);\n}\nEXPORT_SYMBOL_GPL(skb_complete_tx_timestamp);\n\nvoid __skb_tstamp_tx(struct sk_buff *orig_skb,\n\t\t     const struct sk_buff *ack_skb,\n\t\t     struct skb_shared_hwtstamps *hwtstamps,\n\t\t     struct sock *sk, int tstype)\n{\n\tstruct sk_buff *skb;\n\tbool tsonly, opt_stats = false;\n\tu32 tsflags;\n\n\tif (!sk)\n\t\treturn;\n\n\ttsflags = READ_ONCE(sk->sk_tsflags);\n\tif (!hwtstamps && !(tsflags & SOF_TIMESTAMPING_OPT_TX_SWHW) &&\n\t    skb_shinfo(orig_skb)->tx_flags & SKBTX_IN_PROGRESS)\n\t\treturn;\n\n\ttsonly = tsflags & SOF_TIMESTAMPING_OPT_TSONLY;\n\tif (!skb_may_tx_timestamp(sk, tsonly))\n\t\treturn;\n\n\tif (tsonly) {\n#ifdef CONFIG_INET\n\t\tif ((tsflags & SOF_TIMESTAMPING_OPT_STATS) &&\n\t\t    sk_is_tcp(sk)) {\n\t\t\tskb = tcp_get_timestamping_opt_stats(sk, orig_skb,\n\t\t\t\t\t\t\t     ack_skb);\n\t\t\topt_stats = true;\n\t\t} else\n#endif\n\t\t\tskb = alloc_skb(0, GFP_ATOMIC);\n\t} else {\n\t\tskb = skb_clone(orig_skb, GFP_ATOMIC);\n\n\t\tif (skb_orphan_frags_rx(skb, GFP_ATOMIC)) {\n\t\t\tkfree_skb(skb);\n\t\t\treturn;\n\t\t}\n\t}\n\tif (!skb)\n\t\treturn;\n\n\tif (tsonly) {\n\t\tskb_shinfo(skb)->tx_flags |= skb_shinfo(orig_skb)->tx_flags &\n\t\t\t\t\t     SKBTX_ANY_TSTAMP;\n\t\tskb_shinfo(skb)->tskey = skb_shinfo(orig_skb)->tskey;\n\t}\n\n\tif (hwtstamps)\n\t\t*skb_hwtstamps(skb) = *hwtstamps;\n\telse\n\t\t__net_timestamp(skb);\n\n\t__skb_complete_tx_timestamp(skb, sk, tstype, opt_stats);\n}\nEXPORT_SYMBOL_GPL(__skb_tstamp_tx);\n\nvoid skb_tstamp_tx(struct sk_buff *orig_skb,\n\t\t   struct skb_shared_hwtstamps *hwtstamps)\n{\n\treturn __skb_tstamp_tx(orig_skb, NULL, hwtstamps, orig_skb->sk,\n\t\t\t       SCM_TSTAMP_SND);\n}\nEXPORT_SYMBOL_GPL(skb_tstamp_tx);\n\n#ifdef CONFIG_WIRELESS\nvoid skb_complete_wifi_ack(struct sk_buff *skb, bool acked)\n{\n\tstruct sock *sk = skb->sk;\n\tstruct sock_exterr_skb *serr;\n\tint err = 1;\n\n\tskb->wifi_acked_valid = 1;\n\tskb->wifi_acked = acked;\n\n\tserr = SKB_EXT_ERR(skb);\n\tmemset(serr, 0, sizeof(*serr));\n\tserr->ee.ee_errno = ENOMSG;\n\tserr->ee.ee_origin = SO_EE_ORIGIN_TXSTATUS;\n\n\t \n\tif (likely(refcount_inc_not_zero(&sk->sk_refcnt))) {\n\t\terr = sock_queue_err_skb(sk, skb);\n\t\tsock_put(sk);\n\t}\n\tif (err)\n\t\tkfree_skb(skb);\n}\nEXPORT_SYMBOL_GPL(skb_complete_wifi_ack);\n#endif  \n\n \nbool skb_partial_csum_set(struct sk_buff *skb, u16 start, u16 off)\n{\n\tu32 csum_end = (u32)start + (u32)off + sizeof(__sum16);\n\tu32 csum_start = skb_headroom(skb) + (u32)start;\n\n\tif (unlikely(csum_start >= U16_MAX || csum_end > skb_headlen(skb))) {\n\t\tnet_warn_ratelimited(\"bad partial csum: csum=%u/%u headroom=%u headlen=%u\\n\",\n\t\t\t\t     start, off, skb_headroom(skb), skb_headlen(skb));\n\t\treturn false;\n\t}\n\tskb->ip_summed = CHECKSUM_PARTIAL;\n\tskb->csum_start = csum_start;\n\tskb->csum_offset = off;\n\tskb->transport_header = csum_start;\n\treturn true;\n}\nEXPORT_SYMBOL_GPL(skb_partial_csum_set);\n\nstatic int skb_maybe_pull_tail(struct sk_buff *skb, unsigned int len,\n\t\t\t       unsigned int max)\n{\n\tif (skb_headlen(skb) >= len)\n\t\treturn 0;\n\n\t \n\tif (max > skb->len)\n\t\tmax = skb->len;\n\n\tif (__pskb_pull_tail(skb, max - skb_headlen(skb)) == NULL)\n\t\treturn -ENOMEM;\n\n\tif (skb_headlen(skb) < len)\n\t\treturn -EPROTO;\n\n\treturn 0;\n}\n\n#define MAX_TCP_HDR_LEN (15 * 4)\n\nstatic __sum16 *skb_checksum_setup_ip(struct sk_buff *skb,\n\t\t\t\t      typeof(IPPROTO_IP) proto,\n\t\t\t\t      unsigned int off)\n{\n\tint err;\n\n\tswitch (proto) {\n\tcase IPPROTO_TCP:\n\t\terr = skb_maybe_pull_tail(skb, off + sizeof(struct tcphdr),\n\t\t\t\t\t  off + MAX_TCP_HDR_LEN);\n\t\tif (!err && !skb_partial_csum_set(skb, off,\n\t\t\t\t\t\t  offsetof(struct tcphdr,\n\t\t\t\t\t\t\t   check)))\n\t\t\terr = -EPROTO;\n\t\treturn err ? ERR_PTR(err) : &tcp_hdr(skb)->check;\n\n\tcase IPPROTO_UDP:\n\t\terr = skb_maybe_pull_tail(skb, off + sizeof(struct udphdr),\n\t\t\t\t\t  off + sizeof(struct udphdr));\n\t\tif (!err && !skb_partial_csum_set(skb, off,\n\t\t\t\t\t\t  offsetof(struct udphdr,\n\t\t\t\t\t\t\t   check)))\n\t\t\terr = -EPROTO;\n\t\treturn err ? ERR_PTR(err) : &udp_hdr(skb)->check;\n\t}\n\n\treturn ERR_PTR(-EPROTO);\n}\n\n \n#define MAX_IP_HDR_LEN 128\n\nstatic int skb_checksum_setup_ipv4(struct sk_buff *skb, bool recalculate)\n{\n\tunsigned int off;\n\tbool fragment;\n\t__sum16 *csum;\n\tint err;\n\n\tfragment = false;\n\n\terr = skb_maybe_pull_tail(skb,\n\t\t\t\t  sizeof(struct iphdr),\n\t\t\t\t  MAX_IP_HDR_LEN);\n\tif (err < 0)\n\t\tgoto out;\n\n\tif (ip_is_fragment(ip_hdr(skb)))\n\t\tfragment = true;\n\n\toff = ip_hdrlen(skb);\n\n\terr = -EPROTO;\n\n\tif (fragment)\n\t\tgoto out;\n\n\tcsum = skb_checksum_setup_ip(skb, ip_hdr(skb)->protocol, off);\n\tif (IS_ERR(csum))\n\t\treturn PTR_ERR(csum);\n\n\tif (recalculate)\n\t\t*csum = ~csum_tcpudp_magic(ip_hdr(skb)->saddr,\n\t\t\t\t\t   ip_hdr(skb)->daddr,\n\t\t\t\t\t   skb->len - off,\n\t\t\t\t\t   ip_hdr(skb)->protocol, 0);\n\terr = 0;\n\nout:\n\treturn err;\n}\n\n \n#define MAX_IPV6_HDR_LEN 256\n\n#define OPT_HDR(type, skb, off) \\\n\t(type *)(skb_network_header(skb) + (off))\n\nstatic int skb_checksum_setup_ipv6(struct sk_buff *skb, bool recalculate)\n{\n\tint err;\n\tu8 nexthdr;\n\tunsigned int off;\n\tunsigned int len;\n\tbool fragment;\n\tbool done;\n\t__sum16 *csum;\n\n\tfragment = false;\n\tdone = false;\n\n\toff = sizeof(struct ipv6hdr);\n\n\terr = skb_maybe_pull_tail(skb, off, MAX_IPV6_HDR_LEN);\n\tif (err < 0)\n\t\tgoto out;\n\n\tnexthdr = ipv6_hdr(skb)->nexthdr;\n\n\tlen = sizeof(struct ipv6hdr) + ntohs(ipv6_hdr(skb)->payload_len);\n\twhile (off <= len && !done) {\n\t\tswitch (nexthdr) {\n\t\tcase IPPROTO_DSTOPTS:\n\t\tcase IPPROTO_HOPOPTS:\n\t\tcase IPPROTO_ROUTING: {\n\t\t\tstruct ipv6_opt_hdr *hp;\n\n\t\t\terr = skb_maybe_pull_tail(skb,\n\t\t\t\t\t\t  off +\n\t\t\t\t\t\t  sizeof(struct ipv6_opt_hdr),\n\t\t\t\t\t\t  MAX_IPV6_HDR_LEN);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\n\t\t\thp = OPT_HDR(struct ipv6_opt_hdr, skb, off);\n\t\t\tnexthdr = hp->nexthdr;\n\t\t\toff += ipv6_optlen(hp);\n\t\t\tbreak;\n\t\t}\n\t\tcase IPPROTO_AH: {\n\t\t\tstruct ip_auth_hdr *hp;\n\n\t\t\terr = skb_maybe_pull_tail(skb,\n\t\t\t\t\t\t  off +\n\t\t\t\t\t\t  sizeof(struct ip_auth_hdr),\n\t\t\t\t\t\t  MAX_IPV6_HDR_LEN);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\n\t\t\thp = OPT_HDR(struct ip_auth_hdr, skb, off);\n\t\t\tnexthdr = hp->nexthdr;\n\t\t\toff += ipv6_authlen(hp);\n\t\t\tbreak;\n\t\t}\n\t\tcase IPPROTO_FRAGMENT: {\n\t\t\tstruct frag_hdr *hp;\n\n\t\t\terr = skb_maybe_pull_tail(skb,\n\t\t\t\t\t\t  off +\n\t\t\t\t\t\t  sizeof(struct frag_hdr),\n\t\t\t\t\t\t  MAX_IPV6_HDR_LEN);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\n\t\t\thp = OPT_HDR(struct frag_hdr, skb, off);\n\n\t\t\tif (hp->frag_off & htons(IP6_OFFSET | IP6_MF))\n\t\t\t\tfragment = true;\n\n\t\t\tnexthdr = hp->nexthdr;\n\t\t\toff += sizeof(struct frag_hdr);\n\t\t\tbreak;\n\t\t}\n\t\tdefault:\n\t\t\tdone = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\terr = -EPROTO;\n\n\tif (!done || fragment)\n\t\tgoto out;\n\n\tcsum = skb_checksum_setup_ip(skb, nexthdr, off);\n\tif (IS_ERR(csum))\n\t\treturn PTR_ERR(csum);\n\n\tif (recalculate)\n\t\t*csum = ~csum_ipv6_magic(&ipv6_hdr(skb)->saddr,\n\t\t\t\t\t &ipv6_hdr(skb)->daddr,\n\t\t\t\t\t skb->len - off, nexthdr, 0);\n\terr = 0;\n\nout:\n\treturn err;\n}\n\n \nint skb_checksum_setup(struct sk_buff *skb, bool recalculate)\n{\n\tint err;\n\n\tswitch (skb->protocol) {\n\tcase htons(ETH_P_IP):\n\t\terr = skb_checksum_setup_ipv4(skb, recalculate);\n\t\tbreak;\n\n\tcase htons(ETH_P_IPV6):\n\t\terr = skb_checksum_setup_ipv6(skb, recalculate);\n\t\tbreak;\n\n\tdefault:\n\t\terr = -EPROTO;\n\t\tbreak;\n\t}\n\n\treturn err;\n}\nEXPORT_SYMBOL(skb_checksum_setup);\n\n \nstatic struct sk_buff *skb_checksum_maybe_trim(struct sk_buff *skb,\n\t\t\t\t\t       unsigned int transport_len)\n{\n\tstruct sk_buff *skb_chk;\n\tunsigned int len = skb_transport_offset(skb) + transport_len;\n\tint ret;\n\n\tif (skb->len < len)\n\t\treturn NULL;\n\telse if (skb->len == len)\n\t\treturn skb;\n\n\tskb_chk = skb_clone(skb, GFP_ATOMIC);\n\tif (!skb_chk)\n\t\treturn NULL;\n\n\tret = pskb_trim_rcsum(skb_chk, len);\n\tif (ret) {\n\t\tkfree_skb(skb_chk);\n\t\treturn NULL;\n\t}\n\n\treturn skb_chk;\n}\n\n \nstruct sk_buff *skb_checksum_trimmed(struct sk_buff *skb,\n\t\t\t\t     unsigned int transport_len,\n\t\t\t\t     __sum16(*skb_chkf)(struct sk_buff *skb))\n{\n\tstruct sk_buff *skb_chk;\n\tunsigned int offset = skb_transport_offset(skb);\n\t__sum16 ret;\n\n\tskb_chk = skb_checksum_maybe_trim(skb, transport_len);\n\tif (!skb_chk)\n\t\tgoto err;\n\n\tif (!pskb_may_pull(skb_chk, offset))\n\t\tgoto err;\n\n\tskb_pull_rcsum(skb_chk, offset);\n\tret = skb_chkf(skb_chk);\n\tskb_push_rcsum(skb_chk, offset);\n\n\tif (ret)\n\t\tgoto err;\n\n\treturn skb_chk;\n\nerr:\n\tif (skb_chk && skb_chk != skb)\n\t\tkfree_skb(skb_chk);\n\n\treturn NULL;\n\n}\nEXPORT_SYMBOL(skb_checksum_trimmed);\n\nvoid __skb_warn_lro_forwarding(const struct sk_buff *skb)\n{\n\tnet_warn_ratelimited(\"%s: received packets cannot be forwarded while LRO is enabled\\n\",\n\t\t\t     skb->dev->name);\n}\nEXPORT_SYMBOL(__skb_warn_lro_forwarding);\n\nvoid kfree_skb_partial(struct sk_buff *skb, bool head_stolen)\n{\n\tif (head_stolen) {\n\t\tskb_release_head_state(skb);\n\t\tkmem_cache_free(skbuff_cache, skb);\n\t} else {\n\t\t__kfree_skb(skb);\n\t}\n}\nEXPORT_SYMBOL(kfree_skb_partial);\n\n \nbool skb_try_coalesce(struct sk_buff *to, struct sk_buff *from,\n\t\t      bool *fragstolen, int *delta_truesize)\n{\n\tstruct skb_shared_info *to_shinfo, *from_shinfo;\n\tint i, delta, len = from->len;\n\n\t*fragstolen = false;\n\n\tif (skb_cloned(to))\n\t\treturn false;\n\n\t \n\tif (to->pp_recycle != from->pp_recycle ||\n\t    (from->pp_recycle && skb_cloned(from)))\n\t\treturn false;\n\n\tif (len <= skb_tailroom(to)) {\n\t\tif (len)\n\t\t\tBUG_ON(skb_copy_bits(from, 0, skb_put(to, len), len));\n\t\t*delta_truesize = 0;\n\t\treturn true;\n\t}\n\n\tto_shinfo = skb_shinfo(to);\n\tfrom_shinfo = skb_shinfo(from);\n\tif (to_shinfo->frag_list || from_shinfo->frag_list)\n\t\treturn false;\n\tif (skb_zcopy(to) || skb_zcopy(from))\n\t\treturn false;\n\n\tif (skb_headlen(from) != 0) {\n\t\tstruct page *page;\n\t\tunsigned int offset;\n\n\t\tif (to_shinfo->nr_frags +\n\t\t    from_shinfo->nr_frags >= MAX_SKB_FRAGS)\n\t\t\treturn false;\n\n\t\tif (skb_head_is_locked(from))\n\t\t\treturn false;\n\n\t\tdelta = from->truesize - SKB_DATA_ALIGN(sizeof(struct sk_buff));\n\n\t\tpage = virt_to_head_page(from->head);\n\t\toffset = from->data - (unsigned char *)page_address(page);\n\n\t\tskb_fill_page_desc(to, to_shinfo->nr_frags,\n\t\t\t\t   page, offset, skb_headlen(from));\n\t\t*fragstolen = true;\n\t} else {\n\t\tif (to_shinfo->nr_frags +\n\t\t    from_shinfo->nr_frags > MAX_SKB_FRAGS)\n\t\t\treturn false;\n\n\t\tdelta = from->truesize - SKB_TRUESIZE(skb_end_offset(from));\n\t}\n\n\tWARN_ON_ONCE(delta < len);\n\n\tmemcpy(to_shinfo->frags + to_shinfo->nr_frags,\n\t       from_shinfo->frags,\n\t       from_shinfo->nr_frags * sizeof(skb_frag_t));\n\tto_shinfo->nr_frags += from_shinfo->nr_frags;\n\n\tif (!skb_cloned(from))\n\t\tfrom_shinfo->nr_frags = 0;\n\n\t \n\tfor (i = 0; i < from_shinfo->nr_frags; i++)\n\t\t__skb_frag_ref(&from_shinfo->frags[i]);\n\n\tto->truesize += delta;\n\tto->len += len;\n\tto->data_len += len;\n\n\t*delta_truesize = delta;\n\treturn true;\n}\nEXPORT_SYMBOL(skb_try_coalesce);\n\n \nvoid skb_scrub_packet(struct sk_buff *skb, bool xnet)\n{\n\tskb->pkt_type = PACKET_HOST;\n\tskb->skb_iif = 0;\n\tskb->ignore_df = 0;\n\tskb_dst_drop(skb);\n\tskb_ext_reset(skb);\n\tnf_reset_ct(skb);\n\tnf_reset_trace(skb);\n\n#ifdef CONFIG_NET_SWITCHDEV\n\tskb->offload_fwd_mark = 0;\n\tskb->offload_l3_fwd_mark = 0;\n#endif\n\n\tif (!xnet)\n\t\treturn;\n\n\tipvs_reset(skb);\n\tskb->mark = 0;\n\tskb_clear_tstamp(skb);\n}\nEXPORT_SYMBOL_GPL(skb_scrub_packet);\n\nstatic struct sk_buff *skb_reorder_vlan_header(struct sk_buff *skb)\n{\n\tint mac_len, meta_len;\n\tvoid *meta;\n\n\tif (skb_cow(skb, skb_headroom(skb)) < 0) {\n\t\tkfree_skb(skb);\n\t\treturn NULL;\n\t}\n\n\tmac_len = skb->data - skb_mac_header(skb);\n\tif (likely(mac_len > VLAN_HLEN + ETH_TLEN)) {\n\t\tmemmove(skb_mac_header(skb) + VLAN_HLEN, skb_mac_header(skb),\n\t\t\tmac_len - VLAN_HLEN - ETH_TLEN);\n\t}\n\n\tmeta_len = skb_metadata_len(skb);\n\tif (meta_len) {\n\t\tmeta = skb_metadata_end(skb) - meta_len;\n\t\tmemmove(meta + VLAN_HLEN, meta, meta_len);\n\t}\n\n\tskb->mac_header += VLAN_HLEN;\n\treturn skb;\n}\n\nstruct sk_buff *skb_vlan_untag(struct sk_buff *skb)\n{\n\tstruct vlan_hdr *vhdr;\n\tu16 vlan_tci;\n\n\tif (unlikely(skb_vlan_tag_present(skb))) {\n\t\t \n\t\treturn skb;\n\t}\n\n\tskb = skb_share_check(skb, GFP_ATOMIC);\n\tif (unlikely(!skb))\n\t\tgoto err_free;\n\t \n\tif (unlikely(!pskb_may_pull(skb, VLAN_HLEN + sizeof(unsigned short))))\n\t\tgoto err_free;\n\n\tvhdr = (struct vlan_hdr *)skb->data;\n\tvlan_tci = ntohs(vhdr->h_vlan_TCI);\n\t__vlan_hwaccel_put_tag(skb, skb->protocol, vlan_tci);\n\n\tskb_pull_rcsum(skb, VLAN_HLEN);\n\tvlan_set_encap_proto(skb, vhdr);\n\n\tskb = skb_reorder_vlan_header(skb);\n\tif (unlikely(!skb))\n\t\tgoto err_free;\n\n\tskb_reset_network_header(skb);\n\tif (!skb_transport_header_was_set(skb))\n\t\tskb_reset_transport_header(skb);\n\tskb_reset_mac_len(skb);\n\n\treturn skb;\n\nerr_free:\n\tkfree_skb(skb);\n\treturn NULL;\n}\nEXPORT_SYMBOL(skb_vlan_untag);\n\nint skb_ensure_writable(struct sk_buff *skb, unsigned int write_len)\n{\n\tif (!pskb_may_pull(skb, write_len))\n\t\treturn -ENOMEM;\n\n\tif (!skb_cloned(skb) || skb_clone_writable(skb, write_len))\n\t\treturn 0;\n\n\treturn pskb_expand_head(skb, 0, 0, GFP_ATOMIC);\n}\nEXPORT_SYMBOL(skb_ensure_writable);\n\n \nint __skb_vlan_pop(struct sk_buff *skb, u16 *vlan_tci)\n{\n\tint offset = skb->data - skb_mac_header(skb);\n\tint err;\n\n\tif (WARN_ONCE(offset,\n\t\t      \"__skb_vlan_pop got skb with skb->data not at mac header (offset %d)\\n\",\n\t\t      offset)) {\n\t\treturn -EINVAL;\n\t}\n\n\terr = skb_ensure_writable(skb, VLAN_ETH_HLEN);\n\tif (unlikely(err))\n\t\treturn err;\n\n\tskb_postpull_rcsum(skb, skb->data + (2 * ETH_ALEN), VLAN_HLEN);\n\n\tvlan_remove_tag(skb, vlan_tci);\n\n\tskb->mac_header += VLAN_HLEN;\n\n\tif (skb_network_offset(skb) < ETH_HLEN)\n\t\tskb_set_network_header(skb, ETH_HLEN);\n\n\tskb_reset_mac_len(skb);\n\n\treturn err;\n}\nEXPORT_SYMBOL(__skb_vlan_pop);\n\n \nint skb_vlan_pop(struct sk_buff *skb)\n{\n\tu16 vlan_tci;\n\t__be16 vlan_proto;\n\tint err;\n\n\tif (likely(skb_vlan_tag_present(skb))) {\n\t\t__vlan_hwaccel_clear_tag(skb);\n\t} else {\n\t\tif (unlikely(!eth_type_vlan(skb->protocol)))\n\t\t\treturn 0;\n\n\t\terr = __skb_vlan_pop(skb, &vlan_tci);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\t \n\tif (likely(!eth_type_vlan(skb->protocol)))\n\t\treturn 0;\n\n\tvlan_proto = skb->protocol;\n\terr = __skb_vlan_pop(skb, &vlan_tci);\n\tif (unlikely(err))\n\t\treturn err;\n\n\t__vlan_hwaccel_put_tag(skb, vlan_proto, vlan_tci);\n\treturn 0;\n}\nEXPORT_SYMBOL(skb_vlan_pop);\n\n \nint skb_vlan_push(struct sk_buff *skb, __be16 vlan_proto, u16 vlan_tci)\n{\n\tif (skb_vlan_tag_present(skb)) {\n\t\tint offset = skb->data - skb_mac_header(skb);\n\t\tint err;\n\n\t\tif (WARN_ONCE(offset,\n\t\t\t      \"skb_vlan_push got skb with skb->data not at mac header (offset %d)\\n\",\n\t\t\t      offset)) {\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\terr = __vlan_insert_tag(skb, skb->vlan_proto,\n\t\t\t\t\tskb_vlan_tag_get(skb));\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tskb->protocol = skb->vlan_proto;\n\t\tskb->mac_len += VLAN_HLEN;\n\n\t\tskb_postpush_rcsum(skb, skb->data + (2 * ETH_ALEN), VLAN_HLEN);\n\t}\n\t__vlan_hwaccel_put_tag(skb, vlan_proto, vlan_tci);\n\treturn 0;\n}\nEXPORT_SYMBOL(skb_vlan_push);\n\n \nint skb_eth_pop(struct sk_buff *skb)\n{\n\tif (!pskb_may_pull(skb, ETH_HLEN) || skb_vlan_tagged(skb) ||\n\t    skb_network_offset(skb) < ETH_HLEN)\n\t\treturn -EPROTO;\n\n\tskb_pull_rcsum(skb, ETH_HLEN);\n\tskb_reset_mac_header(skb);\n\tskb_reset_mac_len(skb);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(skb_eth_pop);\n\n \nint skb_eth_push(struct sk_buff *skb, const unsigned char *dst,\n\t\t const unsigned char *src)\n{\n\tstruct ethhdr *eth;\n\tint err;\n\n\tif (skb_network_offset(skb) || skb_vlan_tag_present(skb))\n\t\treturn -EPROTO;\n\n\terr = skb_cow_head(skb, sizeof(*eth));\n\tif (err < 0)\n\t\treturn err;\n\n\tskb_push(skb, sizeof(*eth));\n\tskb_reset_mac_header(skb);\n\tskb_reset_mac_len(skb);\n\n\teth = eth_hdr(skb);\n\tether_addr_copy(eth->h_dest, dst);\n\tether_addr_copy(eth->h_source, src);\n\teth->h_proto = skb->protocol;\n\n\tskb_postpush_rcsum(skb, eth, sizeof(*eth));\n\n\treturn 0;\n}\nEXPORT_SYMBOL(skb_eth_push);\n\n \nstatic void skb_mod_eth_type(struct sk_buff *skb, struct ethhdr *hdr,\n\t\t\t     __be16 ethertype)\n{\n\tif (skb->ip_summed == CHECKSUM_COMPLETE) {\n\t\t__be16 diff[] = { ~hdr->h_proto, ethertype };\n\n\t\tskb->csum = csum_partial((char *)diff, sizeof(diff), skb->csum);\n\t}\n\n\thdr->h_proto = ethertype;\n}\n\n \nint skb_mpls_push(struct sk_buff *skb, __be32 mpls_lse, __be16 mpls_proto,\n\t\t  int mac_len, bool ethernet)\n{\n\tstruct mpls_shim_hdr *lse;\n\tint err;\n\n\tif (unlikely(!eth_p_mpls(mpls_proto)))\n\t\treturn -EINVAL;\n\n\t \n\tif (skb->encapsulation)\n\t\treturn -EINVAL;\n\n\terr = skb_cow_head(skb, MPLS_HLEN);\n\tif (unlikely(err))\n\t\treturn err;\n\n\tif (!skb->inner_protocol) {\n\t\tskb_set_inner_network_header(skb, skb_network_offset(skb));\n\t\tskb_set_inner_protocol(skb, skb->protocol);\n\t}\n\n\tskb_push(skb, MPLS_HLEN);\n\tmemmove(skb_mac_header(skb) - MPLS_HLEN, skb_mac_header(skb),\n\t\tmac_len);\n\tskb_reset_mac_header(skb);\n\tskb_set_network_header(skb, mac_len);\n\tskb_reset_mac_len(skb);\n\n\tlse = mpls_hdr(skb);\n\tlse->label_stack_entry = mpls_lse;\n\tskb_postpush_rcsum(skb, lse, MPLS_HLEN);\n\n\tif (ethernet && mac_len >= ETH_HLEN)\n\t\tskb_mod_eth_type(skb, eth_hdr(skb), mpls_proto);\n\tskb->protocol = mpls_proto;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(skb_mpls_push);\n\n \nint skb_mpls_pop(struct sk_buff *skb, __be16 next_proto, int mac_len,\n\t\t bool ethernet)\n{\n\tint err;\n\n\tif (unlikely(!eth_p_mpls(skb->protocol)))\n\t\treturn 0;\n\n\terr = skb_ensure_writable(skb, mac_len + MPLS_HLEN);\n\tif (unlikely(err))\n\t\treturn err;\n\n\tskb_postpull_rcsum(skb, mpls_hdr(skb), MPLS_HLEN);\n\tmemmove(skb_mac_header(skb) + MPLS_HLEN, skb_mac_header(skb),\n\t\tmac_len);\n\n\t__skb_pull(skb, MPLS_HLEN);\n\tskb_reset_mac_header(skb);\n\tskb_set_network_header(skb, mac_len);\n\n\tif (ethernet && mac_len >= ETH_HLEN) {\n\t\tstruct ethhdr *hdr;\n\n\t\t \n\t\thdr = (struct ethhdr *)((void *)mpls_hdr(skb) - ETH_HLEN);\n\t\tskb_mod_eth_type(skb, hdr, next_proto);\n\t}\n\tskb->protocol = next_proto;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(skb_mpls_pop);\n\n \nint skb_mpls_update_lse(struct sk_buff *skb, __be32 mpls_lse)\n{\n\tint err;\n\n\tif (unlikely(!eth_p_mpls(skb->protocol)))\n\t\treturn -EINVAL;\n\n\terr = skb_ensure_writable(skb, skb->mac_len + MPLS_HLEN);\n\tif (unlikely(err))\n\t\treturn err;\n\n\tif (skb->ip_summed == CHECKSUM_COMPLETE) {\n\t\t__be32 diff[] = { ~mpls_hdr(skb)->label_stack_entry, mpls_lse };\n\n\t\tskb->csum = csum_partial((char *)diff, sizeof(diff), skb->csum);\n\t}\n\n\tmpls_hdr(skb)->label_stack_entry = mpls_lse;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(skb_mpls_update_lse);\n\n \nint skb_mpls_dec_ttl(struct sk_buff *skb)\n{\n\tu32 lse;\n\tu8 ttl;\n\n\tif (unlikely(!eth_p_mpls(skb->protocol)))\n\t\treturn -EINVAL;\n\n\tif (!pskb_may_pull(skb, skb_network_offset(skb) + MPLS_HLEN))\n\t\treturn -ENOMEM;\n\n\tlse = be32_to_cpu(mpls_hdr(skb)->label_stack_entry);\n\tttl = (lse & MPLS_LS_TTL_MASK) >> MPLS_LS_TTL_SHIFT;\n\tif (!--ttl)\n\t\treturn -EINVAL;\n\n\tlse &= ~MPLS_LS_TTL_MASK;\n\tlse |= ttl << MPLS_LS_TTL_SHIFT;\n\n\treturn skb_mpls_update_lse(skb, cpu_to_be32(lse));\n}\nEXPORT_SYMBOL_GPL(skb_mpls_dec_ttl);\n\n \nstruct sk_buff *alloc_skb_with_frags(unsigned long header_len,\n\t\t\t\t     unsigned long data_len,\n\t\t\t\t     int order,\n\t\t\t\t     int *errcode,\n\t\t\t\t     gfp_t gfp_mask)\n{\n\tunsigned long chunk;\n\tstruct sk_buff *skb;\n\tstruct page *page;\n\tint nr_frags = 0;\n\n\t*errcode = -EMSGSIZE;\n\tif (unlikely(data_len > MAX_SKB_FRAGS * (PAGE_SIZE << order)))\n\t\treturn NULL;\n\n\t*errcode = -ENOBUFS;\n\tskb = alloc_skb(header_len, gfp_mask);\n\tif (!skb)\n\t\treturn NULL;\n\n\twhile (data_len) {\n\t\tif (nr_frags == MAX_SKB_FRAGS - 1)\n\t\t\tgoto failure;\n\t\twhile (order && PAGE_ALIGN(data_len) < (PAGE_SIZE << order))\n\t\t\torder--;\n\n\t\tif (order) {\n\t\t\tpage = alloc_pages((gfp_mask & ~__GFP_DIRECT_RECLAIM) |\n\t\t\t\t\t   __GFP_COMP |\n\t\t\t\t\t   __GFP_NOWARN,\n\t\t\t\t\t   order);\n\t\t\tif (!page) {\n\t\t\t\torder--;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t} else {\n\t\t\tpage = alloc_page(gfp_mask);\n\t\t\tif (!page)\n\t\t\t\tgoto failure;\n\t\t}\n\t\tchunk = min_t(unsigned long, data_len,\n\t\t\t      PAGE_SIZE << order);\n\t\tskb_fill_page_desc(skb, nr_frags, page, 0, chunk);\n\t\tnr_frags++;\n\t\tskb->truesize += (PAGE_SIZE << order);\n\t\tdata_len -= chunk;\n\t}\n\treturn skb;\n\nfailure:\n\tkfree_skb(skb);\n\treturn NULL;\n}\nEXPORT_SYMBOL(alloc_skb_with_frags);\n\n \nstatic int pskb_carve_inside_header(struct sk_buff *skb, const u32 off,\n\t\t\t\t    const int headlen, gfp_t gfp_mask)\n{\n\tint i;\n\tunsigned int size = skb_end_offset(skb);\n\tint new_hlen = headlen - off;\n\tu8 *data;\n\n\tif (skb_pfmemalloc(skb))\n\t\tgfp_mask |= __GFP_MEMALLOC;\n\n\tdata = kmalloc_reserve(&size, gfp_mask, NUMA_NO_NODE, NULL);\n\tif (!data)\n\t\treturn -ENOMEM;\n\tsize = SKB_WITH_OVERHEAD(size);\n\n\t \n\tskb_copy_from_linear_data_offset(skb, off, data, new_hlen);\n\tskb->len -= off;\n\n\tmemcpy((struct skb_shared_info *)(data + size),\n\t       skb_shinfo(skb),\n\t       offsetof(struct skb_shared_info,\n\t\t\tfrags[skb_shinfo(skb)->nr_frags]));\n\tif (skb_cloned(skb)) {\n\t\t \n\t\tif (skb_orphan_frags(skb, gfp_mask)) {\n\t\t\tskb_kfree_head(data, size);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++)\n\t\t\tskb_frag_ref(skb, i);\n\t\tif (skb_has_frag_list(skb))\n\t\t\tskb_clone_fraglist(skb);\n\t\tskb_release_data(skb, SKB_CONSUMED, false);\n\t} else {\n\t\t \n\t\tskb_free_head(skb, false);\n\t}\n\n\tskb->head = data;\n\tskb->data = data;\n\tskb->head_frag = 0;\n\tskb_set_end_offset(skb, size);\n\tskb_set_tail_pointer(skb, skb_headlen(skb));\n\tskb_headers_offset_update(skb, 0);\n\tskb->cloned = 0;\n\tskb->hdr_len = 0;\n\tskb->nohdr = 0;\n\tatomic_set(&skb_shinfo(skb)->dataref, 1);\n\n\treturn 0;\n}\n\nstatic int pskb_carve(struct sk_buff *skb, const u32 off, gfp_t gfp);\n\n \nstatic int pskb_carve_frag_list(struct sk_buff *skb,\n\t\t\t\tstruct skb_shared_info *shinfo, int eat,\n\t\t\t\tgfp_t gfp_mask)\n{\n\tstruct sk_buff *list = shinfo->frag_list;\n\tstruct sk_buff *clone = NULL;\n\tstruct sk_buff *insp = NULL;\n\n\tdo {\n\t\tif (!list) {\n\t\t\tpr_err(\"Not enough bytes to eat. Want %d\\n\", eat);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tif (list->len <= eat) {\n\t\t\t \n\t\t\teat -= list->len;\n\t\t\tlist = list->next;\n\t\t\tinsp = list;\n\t\t} else {\n\t\t\t \n\t\t\tif (skb_shared(list)) {\n\t\t\t\tclone = skb_clone(list, gfp_mask);\n\t\t\t\tif (!clone)\n\t\t\t\t\treturn -ENOMEM;\n\t\t\t\tinsp = list->next;\n\t\t\t\tlist = clone;\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tinsp = list;\n\t\t\t}\n\t\t\tif (pskb_carve(list, eat, gfp_mask) < 0) {\n\t\t\t\tkfree_skb(clone);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t} while (eat);\n\n\t \n\twhile ((list = shinfo->frag_list) != insp) {\n\t\tshinfo->frag_list = list->next;\n\t\tconsume_skb(list);\n\t}\n\t \n\tif (clone) {\n\t\tclone->next = list;\n\t\tshinfo->frag_list = clone;\n\t}\n\treturn 0;\n}\n\n \nstatic int pskb_carve_inside_nonlinear(struct sk_buff *skb, const u32 off,\n\t\t\t\t       int pos, gfp_t gfp_mask)\n{\n\tint i, k = 0;\n\tunsigned int size = skb_end_offset(skb);\n\tu8 *data;\n\tconst int nfrags = skb_shinfo(skb)->nr_frags;\n\tstruct skb_shared_info *shinfo;\n\n\tif (skb_pfmemalloc(skb))\n\t\tgfp_mask |= __GFP_MEMALLOC;\n\n\tdata = kmalloc_reserve(&size, gfp_mask, NUMA_NO_NODE, NULL);\n\tif (!data)\n\t\treturn -ENOMEM;\n\tsize = SKB_WITH_OVERHEAD(size);\n\n\tmemcpy((struct skb_shared_info *)(data + size),\n\t       skb_shinfo(skb), offsetof(struct skb_shared_info, frags[0]));\n\tif (skb_orphan_frags(skb, gfp_mask)) {\n\t\tskb_kfree_head(data, size);\n\t\treturn -ENOMEM;\n\t}\n\tshinfo = (struct skb_shared_info *)(data + size);\n\tfor (i = 0; i < nfrags; i++) {\n\t\tint fsize = skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\n\t\tif (pos + fsize > off) {\n\t\t\tshinfo->frags[k] = skb_shinfo(skb)->frags[i];\n\n\t\t\tif (pos < off) {\n\t\t\t\t \n\t\t\t\tskb_frag_off_add(&shinfo->frags[0], off - pos);\n\t\t\t\tskb_frag_size_sub(&shinfo->frags[0], off - pos);\n\t\t\t}\n\t\t\tskb_frag_ref(skb, i);\n\t\t\tk++;\n\t\t}\n\t\tpos += fsize;\n\t}\n\tshinfo->nr_frags = k;\n\tif (skb_has_frag_list(skb))\n\t\tskb_clone_fraglist(skb);\n\n\t \n\tif (k == 0 && pskb_carve_frag_list(skb, shinfo, off - pos, gfp_mask)) {\n\t\t \n\t\tif (skb_has_frag_list(skb))\n\t\t\tkfree_skb_list(skb_shinfo(skb)->frag_list);\n\t\tskb_kfree_head(data, size);\n\t\treturn -ENOMEM;\n\t}\n\tskb_release_data(skb, SKB_CONSUMED, false);\n\n\tskb->head = data;\n\tskb->head_frag = 0;\n\tskb->data = data;\n\tskb_set_end_offset(skb, size);\n\tskb_reset_tail_pointer(skb);\n\tskb_headers_offset_update(skb, 0);\n\tskb->cloned   = 0;\n\tskb->hdr_len  = 0;\n\tskb->nohdr    = 0;\n\tskb->len -= off;\n\tskb->data_len = skb->len;\n\tatomic_set(&skb_shinfo(skb)->dataref, 1);\n\treturn 0;\n}\n\n \nstatic int pskb_carve(struct sk_buff *skb, const u32 len, gfp_t gfp)\n{\n\tint headlen = skb_headlen(skb);\n\n\tif (len < headlen)\n\t\treturn pskb_carve_inside_header(skb, len, headlen, gfp);\n\telse\n\t\treturn pskb_carve_inside_nonlinear(skb, len, headlen, gfp);\n}\n\n \nstruct sk_buff *pskb_extract(struct sk_buff *skb, int off,\n\t\t\t     int to_copy, gfp_t gfp)\n{\n\tstruct sk_buff  *clone = skb_clone(skb, gfp);\n\n\tif (!clone)\n\t\treturn NULL;\n\n\tif (pskb_carve(clone, off, gfp) < 0 ||\n\t    pskb_trim(clone, to_copy)) {\n\t\tkfree_skb(clone);\n\t\treturn NULL;\n\t}\n\treturn clone;\n}\nEXPORT_SYMBOL(pskb_extract);\n\n \nvoid skb_condense(struct sk_buff *skb)\n{\n\tif (skb->data_len) {\n\t\tif (skb->data_len > skb->end - skb->tail ||\n\t\t    skb_cloned(skb))\n\t\t\treturn;\n\n\t\t \n\t\t__pskb_pull_tail(skb, skb->data_len);\n\t}\n\t \n\tskb->truesize = SKB_TRUESIZE(skb_end_offset(skb));\n}\nEXPORT_SYMBOL(skb_condense);\n\n#ifdef CONFIG_SKB_EXTENSIONS\nstatic void *skb_ext_get_ptr(struct skb_ext *ext, enum skb_ext_id id)\n{\n\treturn (void *)ext + (ext->offset[id] * SKB_EXT_ALIGN_VALUE);\n}\n\n \nstruct skb_ext *__skb_ext_alloc(gfp_t flags)\n{\n\tstruct skb_ext *new = kmem_cache_alloc(skbuff_ext_cache, flags);\n\n\tif (new) {\n\t\tmemset(new->offset, 0, sizeof(new->offset));\n\t\trefcount_set(&new->refcnt, 1);\n\t}\n\n\treturn new;\n}\n\nstatic struct skb_ext *skb_ext_maybe_cow(struct skb_ext *old,\n\t\t\t\t\t unsigned int old_active)\n{\n\tstruct skb_ext *new;\n\n\tif (refcount_read(&old->refcnt) == 1)\n\t\treturn old;\n\n\tnew = kmem_cache_alloc(skbuff_ext_cache, GFP_ATOMIC);\n\tif (!new)\n\t\treturn NULL;\n\n\tmemcpy(new, old, old->chunks * SKB_EXT_ALIGN_VALUE);\n\trefcount_set(&new->refcnt, 1);\n\n#ifdef CONFIG_XFRM\n\tif (old_active & (1 << SKB_EXT_SEC_PATH)) {\n\t\tstruct sec_path *sp = skb_ext_get_ptr(old, SKB_EXT_SEC_PATH);\n\t\tunsigned int i;\n\n\t\tfor (i = 0; i < sp->len; i++)\n\t\t\txfrm_state_hold(sp->xvec[i]);\n\t}\n#endif\n\t__skb_ext_put(old);\n\treturn new;\n}\n\n \nvoid *__skb_ext_set(struct sk_buff *skb, enum skb_ext_id id,\n\t\t    struct skb_ext *ext)\n{\n\tunsigned int newlen, newoff = SKB_EXT_CHUNKSIZEOF(*ext);\n\n\tskb_ext_put(skb);\n\tnewlen = newoff + skb_ext_type_len[id];\n\text->chunks = newlen;\n\text->offset[id] = newoff;\n\tskb->extensions = ext;\n\tskb->active_extensions = 1 << id;\n\treturn skb_ext_get_ptr(ext, id);\n}\n\n \nvoid *skb_ext_add(struct sk_buff *skb, enum skb_ext_id id)\n{\n\tstruct skb_ext *new, *old = NULL;\n\tunsigned int newlen, newoff;\n\n\tif (skb->active_extensions) {\n\t\told = skb->extensions;\n\n\t\tnew = skb_ext_maybe_cow(old, skb->active_extensions);\n\t\tif (!new)\n\t\t\treturn NULL;\n\n\t\tif (__skb_ext_exist(new, id))\n\t\t\tgoto set_active;\n\n\t\tnewoff = new->chunks;\n\t} else {\n\t\tnewoff = SKB_EXT_CHUNKSIZEOF(*new);\n\n\t\tnew = __skb_ext_alloc(GFP_ATOMIC);\n\t\tif (!new)\n\t\t\treturn NULL;\n\t}\n\n\tnewlen = newoff + skb_ext_type_len[id];\n\tnew->chunks = newlen;\n\tnew->offset[id] = newoff;\nset_active:\n\tskb->slow_gro = 1;\n\tskb->extensions = new;\n\tskb->active_extensions |= 1 << id;\n\treturn skb_ext_get_ptr(new, id);\n}\nEXPORT_SYMBOL(skb_ext_add);\n\n#ifdef CONFIG_XFRM\nstatic void skb_ext_put_sp(struct sec_path *sp)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < sp->len; i++)\n\t\txfrm_state_put(sp->xvec[i]);\n}\n#endif\n\n#ifdef CONFIG_MCTP_FLOWS\nstatic void skb_ext_put_mctp(struct mctp_flow *flow)\n{\n\tif (flow->key)\n\t\tmctp_key_unref(flow->key);\n}\n#endif\n\nvoid __skb_ext_del(struct sk_buff *skb, enum skb_ext_id id)\n{\n\tstruct skb_ext *ext = skb->extensions;\n\n\tskb->active_extensions &= ~(1 << id);\n\tif (skb->active_extensions == 0) {\n\t\tskb->extensions = NULL;\n\t\t__skb_ext_put(ext);\n#ifdef CONFIG_XFRM\n\t} else if (id == SKB_EXT_SEC_PATH &&\n\t\t   refcount_read(&ext->refcnt) == 1) {\n\t\tstruct sec_path *sp = skb_ext_get_ptr(ext, SKB_EXT_SEC_PATH);\n\n\t\tskb_ext_put_sp(sp);\n\t\tsp->len = 0;\n#endif\n\t}\n}\nEXPORT_SYMBOL(__skb_ext_del);\n\nvoid __skb_ext_put(struct skb_ext *ext)\n{\n\t \n\tif (refcount_read(&ext->refcnt) == 1)\n\t\tgoto free_now;\n\n\tif (!refcount_dec_and_test(&ext->refcnt))\n\t\treturn;\nfree_now:\n#ifdef CONFIG_XFRM\n\tif (__skb_ext_exist(ext, SKB_EXT_SEC_PATH))\n\t\tskb_ext_put_sp(skb_ext_get_ptr(ext, SKB_EXT_SEC_PATH));\n#endif\n#ifdef CONFIG_MCTP_FLOWS\n\tif (__skb_ext_exist(ext, SKB_EXT_MCTP))\n\t\tskb_ext_put_mctp(skb_ext_get_ptr(ext, SKB_EXT_MCTP));\n#endif\n\n\tkmem_cache_free(skbuff_ext_cache, ext);\n}\nEXPORT_SYMBOL(__skb_ext_put);\n#endif  \n\n \nvoid skb_attempt_defer_free(struct sk_buff *skb)\n{\n\tint cpu = skb->alloc_cpu;\n\tstruct softnet_data *sd;\n\tunsigned int defer_max;\n\tbool kick;\n\n\tif (WARN_ON_ONCE(cpu >= nr_cpu_ids) ||\n\t    !cpu_online(cpu) ||\n\t    cpu == raw_smp_processor_id()) {\nnodefer:\t__kfree_skb(skb);\n\t\treturn;\n\t}\n\n\tDEBUG_NET_WARN_ON_ONCE(skb_dst(skb));\n\tDEBUG_NET_WARN_ON_ONCE(skb->destructor);\n\n\tsd = &per_cpu(softnet_data, cpu);\n\tdefer_max = READ_ONCE(sysctl_skb_defer_max);\n\tif (READ_ONCE(sd->defer_count) >= defer_max)\n\t\tgoto nodefer;\n\n\tspin_lock_bh(&sd->defer_lock);\n\t \n\tkick = sd->defer_count == (defer_max >> 1);\n\t \n\tWRITE_ONCE(sd->defer_count, sd->defer_count + 1);\n\n\tskb->next = sd->defer_list;\n\t \n\tWRITE_ONCE(sd->defer_list, skb);\n\tspin_unlock_bh(&sd->defer_lock);\n\n\t \n\tif (unlikely(kick) && !cmpxchg(&sd->defer_ipi_scheduled, 0, 1))\n\t\tsmp_call_function_single_async(cpu, &sd->defer_csd);\n}\n\nstatic void skb_splice_csum_page(struct sk_buff *skb, struct page *page,\n\t\t\t\t size_t offset, size_t len)\n{\n\tconst char *kaddr;\n\t__wsum csum;\n\n\tkaddr = kmap_local_page(page);\n\tcsum = csum_partial(kaddr + offset, len, 0);\n\tkunmap_local(kaddr);\n\tskb->csum = csum_block_add(skb->csum, csum, skb->len);\n}\n\n \nssize_t skb_splice_from_iter(struct sk_buff *skb, struct iov_iter *iter,\n\t\t\t     ssize_t maxsize, gfp_t gfp)\n{\n\tsize_t frag_limit = READ_ONCE(sysctl_max_skb_frags);\n\tstruct page *pages[8], **ppages = pages;\n\tssize_t spliced = 0, ret = 0;\n\tunsigned int i;\n\n\twhile (iter->count > 0) {\n\t\tssize_t space, nr, len;\n\t\tsize_t off;\n\n\t\tret = -EMSGSIZE;\n\t\tspace = frag_limit - skb_shinfo(skb)->nr_frags;\n\t\tif (space < 0)\n\t\t\tbreak;\n\n\t\t \n\t\tnr = clamp_t(size_t, space, 1, ARRAY_SIZE(pages));\n\n\t\tlen = iov_iter_extract_pages(iter, &ppages, maxsize, nr, 0, &off);\n\t\tif (len <= 0) {\n\t\t\tret = len ?: -EIO;\n\t\t\tbreak;\n\t\t}\n\n\t\ti = 0;\n\t\tdo {\n\t\t\tstruct page *page = pages[i++];\n\t\t\tsize_t part = min_t(size_t, PAGE_SIZE - off, len);\n\n\t\t\tret = -EIO;\n\t\t\tif (WARN_ON_ONCE(!sendpage_ok(page)))\n\t\t\t\tgoto out;\n\n\t\t\tret = skb_append_pagefrags(skb, page, off, part,\n\t\t\t\t\t\t   frag_limit);\n\t\t\tif (ret < 0) {\n\t\t\t\tiov_iter_revert(iter, len);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tif (skb->ip_summed == CHECKSUM_NONE)\n\t\t\t\tskb_splice_csum_page(skb, page, off, part);\n\n\t\t\toff = 0;\n\t\t\tspliced += part;\n\t\t\tmaxsize -= part;\n\t\t\tlen -= part;\n\t\t} while (len > 0);\n\n\t\tif (maxsize <= 0)\n\t\t\tbreak;\n\t}\n\nout:\n\tskb_len_add(skb, spliced);\n\treturn spliced ?: ret;\n}\nEXPORT_SYMBOL(skb_splice_from_iter);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}