{
  "module_name": "xdp.c",
  "hash_id": "ec56e4e49aac0676c317c13bc42f10acc03e4a102fb6ae05827a985199502322",
  "original_prompt": "Ingested from linux-6.6.14/net/core/xdp.c",
  "human_readable_source": "\n \n#include <linux/bpf.h>\n#include <linux/btf.h>\n#include <linux/btf_ids.h>\n#include <linux/filter.h>\n#include <linux/types.h>\n#include <linux/mm.h>\n#include <linux/netdevice.h>\n#include <linux/slab.h>\n#include <linux/idr.h>\n#include <linux/rhashtable.h>\n#include <linux/bug.h>\n#include <net/page_pool/helpers.h>\n\n#include <net/xdp.h>\n#include <net/xdp_priv.h>  \n#include <trace/events/xdp.h>\n#include <net/xdp_sock_drv.h>\n\n#define REG_STATE_NEW\t\t0x0\n#define REG_STATE_REGISTERED\t0x1\n#define REG_STATE_UNREGISTERED\t0x2\n#define REG_STATE_UNUSED\t0x3\n\nstatic DEFINE_IDA(mem_id_pool);\nstatic DEFINE_MUTEX(mem_id_lock);\n#define MEM_ID_MAX 0xFFFE\n#define MEM_ID_MIN 1\nstatic int mem_id_next = MEM_ID_MIN;\n\nstatic bool mem_id_init;  \nstatic struct rhashtable *mem_id_ht;\n\nstatic u32 xdp_mem_id_hashfn(const void *data, u32 len, u32 seed)\n{\n\tconst u32 *k = data;\n\tconst u32 key = *k;\n\n\tBUILD_BUG_ON(sizeof_field(struct xdp_mem_allocator, mem.id)\n\t\t     != sizeof(u32));\n\n\t \n\treturn key;\n}\n\nstatic int xdp_mem_id_cmp(struct rhashtable_compare_arg *arg,\n\t\t\t  const void *ptr)\n{\n\tconst struct xdp_mem_allocator *xa = ptr;\n\tu32 mem_id = *(u32 *)arg->key;\n\n\treturn xa->mem.id != mem_id;\n}\n\nstatic const struct rhashtable_params mem_id_rht_params = {\n\t.nelem_hint = 64,\n\t.head_offset = offsetof(struct xdp_mem_allocator, node),\n\t.key_offset  = offsetof(struct xdp_mem_allocator, mem.id),\n\t.key_len = sizeof_field(struct xdp_mem_allocator, mem.id),\n\t.max_size = MEM_ID_MAX,\n\t.min_size = 8,\n\t.automatic_shrinking = true,\n\t.hashfn    = xdp_mem_id_hashfn,\n\t.obj_cmpfn = xdp_mem_id_cmp,\n};\n\nstatic void __xdp_mem_allocator_rcu_free(struct rcu_head *rcu)\n{\n\tstruct xdp_mem_allocator *xa;\n\n\txa = container_of(rcu, struct xdp_mem_allocator, rcu);\n\n\t \n\tida_simple_remove(&mem_id_pool, xa->mem.id);\n\n\tkfree(xa);\n}\n\nstatic void mem_xa_remove(struct xdp_mem_allocator *xa)\n{\n\ttrace_mem_disconnect(xa);\n\n\tif (!rhashtable_remove_fast(mem_id_ht, &xa->node, mem_id_rht_params))\n\t\tcall_rcu(&xa->rcu, __xdp_mem_allocator_rcu_free);\n}\n\nstatic void mem_allocator_disconnect(void *allocator)\n{\n\tstruct xdp_mem_allocator *xa;\n\tstruct rhashtable_iter iter;\n\n\tmutex_lock(&mem_id_lock);\n\n\trhashtable_walk_enter(mem_id_ht, &iter);\n\tdo {\n\t\trhashtable_walk_start(&iter);\n\n\t\twhile ((xa = rhashtable_walk_next(&iter)) && !IS_ERR(xa)) {\n\t\t\tif (xa->allocator == allocator)\n\t\t\t\tmem_xa_remove(xa);\n\t\t}\n\n\t\trhashtable_walk_stop(&iter);\n\n\t} while (xa == ERR_PTR(-EAGAIN));\n\trhashtable_walk_exit(&iter);\n\n\tmutex_unlock(&mem_id_lock);\n}\n\nvoid xdp_unreg_mem_model(struct xdp_mem_info *mem)\n{\n\tstruct xdp_mem_allocator *xa;\n\tint type = mem->type;\n\tint id = mem->id;\n\n\t \n\tmem->id = 0;\n\tmem->type = 0;\n\n\tif (id == 0)\n\t\treturn;\n\n\tif (type == MEM_TYPE_PAGE_POOL) {\n\t\trcu_read_lock();\n\t\txa = rhashtable_lookup(mem_id_ht, &id, mem_id_rht_params);\n\t\tpage_pool_destroy(xa->page_pool);\n\t\trcu_read_unlock();\n\t}\n}\nEXPORT_SYMBOL_GPL(xdp_unreg_mem_model);\n\nvoid xdp_rxq_info_unreg_mem_model(struct xdp_rxq_info *xdp_rxq)\n{\n\tif (xdp_rxq->reg_state != REG_STATE_REGISTERED) {\n\t\tWARN(1, \"Missing register, driver bug\");\n\t\treturn;\n\t}\n\n\txdp_unreg_mem_model(&xdp_rxq->mem);\n}\nEXPORT_SYMBOL_GPL(xdp_rxq_info_unreg_mem_model);\n\nvoid xdp_rxq_info_unreg(struct xdp_rxq_info *xdp_rxq)\n{\n\t \n\tif (xdp_rxq->reg_state == REG_STATE_UNUSED)\n\t\treturn;\n\n\txdp_rxq_info_unreg_mem_model(xdp_rxq);\n\n\txdp_rxq->reg_state = REG_STATE_UNREGISTERED;\n\txdp_rxq->dev = NULL;\n}\nEXPORT_SYMBOL_GPL(xdp_rxq_info_unreg);\n\nstatic void xdp_rxq_info_init(struct xdp_rxq_info *xdp_rxq)\n{\n\tmemset(xdp_rxq, 0, sizeof(*xdp_rxq));\n}\n\n \nint __xdp_rxq_info_reg(struct xdp_rxq_info *xdp_rxq,\n\t\t       struct net_device *dev, u32 queue_index,\n\t\t       unsigned int napi_id, u32 frag_size)\n{\n\tif (!dev) {\n\t\tWARN(1, \"Missing net_device from driver\");\n\t\treturn -ENODEV;\n\t}\n\n\tif (xdp_rxq->reg_state == REG_STATE_UNUSED) {\n\t\tWARN(1, \"Driver promised not to register this\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (xdp_rxq->reg_state == REG_STATE_REGISTERED) {\n\t\tWARN(1, \"Missing unregister, handled but fix driver\");\n\t\txdp_rxq_info_unreg(xdp_rxq);\n\t}\n\n\t \n\txdp_rxq_info_init(xdp_rxq);\n\txdp_rxq->dev = dev;\n\txdp_rxq->queue_index = queue_index;\n\txdp_rxq->napi_id = napi_id;\n\txdp_rxq->frag_size = frag_size;\n\n\txdp_rxq->reg_state = REG_STATE_REGISTERED;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(__xdp_rxq_info_reg);\n\nvoid xdp_rxq_info_unused(struct xdp_rxq_info *xdp_rxq)\n{\n\txdp_rxq->reg_state = REG_STATE_UNUSED;\n}\nEXPORT_SYMBOL_GPL(xdp_rxq_info_unused);\n\nbool xdp_rxq_info_is_reg(struct xdp_rxq_info *xdp_rxq)\n{\n\treturn (xdp_rxq->reg_state == REG_STATE_REGISTERED);\n}\nEXPORT_SYMBOL_GPL(xdp_rxq_info_is_reg);\n\nstatic int __mem_id_init_hash_table(void)\n{\n\tstruct rhashtable *rht;\n\tint ret;\n\n\tif (unlikely(mem_id_init))\n\t\treturn 0;\n\n\trht = kzalloc(sizeof(*rht), GFP_KERNEL);\n\tif (!rht)\n\t\treturn -ENOMEM;\n\n\tret = rhashtable_init(rht, &mem_id_rht_params);\n\tif (ret < 0) {\n\t\tkfree(rht);\n\t\treturn ret;\n\t}\n\tmem_id_ht = rht;\n\tsmp_mb();  \n\tmem_id_init = true;\n\n\treturn 0;\n}\n\n \nstatic int __mem_id_cyclic_get(gfp_t gfp)\n{\n\tint retries = 1;\n\tint id;\n\nagain:\n\tid = ida_simple_get(&mem_id_pool, mem_id_next, MEM_ID_MAX, gfp);\n\tif (id < 0) {\n\t\tif (id == -ENOSPC) {\n\t\t\t \n\t\t\tif (retries--) {\n\t\t\t\tmem_id_next = MEM_ID_MIN;\n\t\t\t\tgoto again;\n\t\t\t}\n\t\t}\n\t\treturn id;  \n\t}\n\tmem_id_next = id + 1;\n\n\treturn id;\n}\n\nstatic bool __is_supported_mem_type(enum xdp_mem_type type)\n{\n\tif (type == MEM_TYPE_PAGE_POOL)\n\t\treturn is_page_pool_compiled_in();\n\n\tif (type >= MEM_TYPE_MAX)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic struct xdp_mem_allocator *__xdp_reg_mem_model(struct xdp_mem_info *mem,\n\t\t\t\t\t\t     enum xdp_mem_type type,\n\t\t\t\t\t\t     void *allocator)\n{\n\tstruct xdp_mem_allocator *xdp_alloc;\n\tgfp_t gfp = GFP_KERNEL;\n\tint id, errno, ret;\n\tvoid *ptr;\n\n\tif (!__is_supported_mem_type(type))\n\t\treturn ERR_PTR(-EOPNOTSUPP);\n\n\tmem->type = type;\n\n\tif (!allocator) {\n\t\tif (type == MEM_TYPE_PAGE_POOL)\n\t\t\treturn ERR_PTR(-EINVAL);  \n\t\treturn NULL;\n\t}\n\n\t \n\tif (!mem_id_init) {\n\t\tmutex_lock(&mem_id_lock);\n\t\tret = __mem_id_init_hash_table();\n\t\tmutex_unlock(&mem_id_lock);\n\t\tif (ret < 0) {\n\t\t\tWARN_ON(1);\n\t\t\treturn ERR_PTR(ret);\n\t\t}\n\t}\n\n\txdp_alloc = kzalloc(sizeof(*xdp_alloc), gfp);\n\tif (!xdp_alloc)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmutex_lock(&mem_id_lock);\n\tid = __mem_id_cyclic_get(gfp);\n\tif (id < 0) {\n\t\terrno = id;\n\t\tgoto err;\n\t}\n\tmem->id = id;\n\txdp_alloc->mem = *mem;\n\txdp_alloc->allocator = allocator;\n\n\t \n\tptr = rhashtable_insert_slow(mem_id_ht, &id, &xdp_alloc->node);\n\tif (IS_ERR(ptr)) {\n\t\tida_simple_remove(&mem_id_pool, mem->id);\n\t\tmem->id = 0;\n\t\terrno = PTR_ERR(ptr);\n\t\tgoto err;\n\t}\n\n\tif (type == MEM_TYPE_PAGE_POOL)\n\t\tpage_pool_use_xdp_mem(allocator, mem_allocator_disconnect, mem);\n\n\tmutex_unlock(&mem_id_lock);\n\n\treturn xdp_alloc;\nerr:\n\tmutex_unlock(&mem_id_lock);\n\tkfree(xdp_alloc);\n\treturn ERR_PTR(errno);\n}\n\nint xdp_reg_mem_model(struct xdp_mem_info *mem,\n\t\t      enum xdp_mem_type type, void *allocator)\n{\n\tstruct xdp_mem_allocator *xdp_alloc;\n\n\txdp_alloc = __xdp_reg_mem_model(mem, type, allocator);\n\tif (IS_ERR(xdp_alloc))\n\t\treturn PTR_ERR(xdp_alloc);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(xdp_reg_mem_model);\n\nint xdp_rxq_info_reg_mem_model(struct xdp_rxq_info *xdp_rxq,\n\t\t\t       enum xdp_mem_type type, void *allocator)\n{\n\tstruct xdp_mem_allocator *xdp_alloc;\n\n\tif (xdp_rxq->reg_state != REG_STATE_REGISTERED) {\n\t\tWARN(1, \"Missing register, driver bug\");\n\t\treturn -EFAULT;\n\t}\n\n\txdp_alloc = __xdp_reg_mem_model(&xdp_rxq->mem, type, allocator);\n\tif (IS_ERR(xdp_alloc))\n\t\treturn PTR_ERR(xdp_alloc);\n\n\tif (trace_mem_connect_enabled() && xdp_alloc)\n\t\ttrace_mem_connect(xdp_alloc, xdp_rxq);\n\treturn 0;\n}\n\nEXPORT_SYMBOL_GPL(xdp_rxq_info_reg_mem_model);\n\n \nvoid __xdp_return(void *data, struct xdp_mem_info *mem, bool napi_direct,\n\t\t  struct xdp_buff *xdp)\n{\n\tstruct page *page;\n\n\tswitch (mem->type) {\n\tcase MEM_TYPE_PAGE_POOL:\n\t\tpage = virt_to_head_page(data);\n\t\tif (napi_direct && xdp_return_frame_no_direct())\n\t\t\tnapi_direct = false;\n\t\t \n\t\tpage_pool_put_full_page(page->pp, page, napi_direct);\n\t\tbreak;\n\tcase MEM_TYPE_PAGE_SHARED:\n\t\tpage_frag_free(data);\n\t\tbreak;\n\tcase MEM_TYPE_PAGE_ORDER0:\n\t\tpage = virt_to_page(data);  \n\t\tput_page(page);\n\t\tbreak;\n\tcase MEM_TYPE_XSK_BUFF_POOL:\n\t\t \n\t\txsk_buff_free(xdp);\n\t\tbreak;\n\tdefault:\n\t\t \n\t\tWARN(1, \"Incorrect XDP memory type (%d) usage\", mem->type);\n\t\tbreak;\n\t}\n}\n\nvoid xdp_return_frame(struct xdp_frame *xdpf)\n{\n\tstruct skb_shared_info *sinfo;\n\tint i;\n\n\tif (likely(!xdp_frame_has_frags(xdpf)))\n\t\tgoto out;\n\n\tsinfo = xdp_get_shared_info_from_frame(xdpf);\n\tfor (i = 0; i < sinfo->nr_frags; i++) {\n\t\tstruct page *page = skb_frag_page(&sinfo->frags[i]);\n\n\t\t__xdp_return(page_address(page), &xdpf->mem, false, NULL);\n\t}\nout:\n\t__xdp_return(xdpf->data, &xdpf->mem, false, NULL);\n}\nEXPORT_SYMBOL_GPL(xdp_return_frame);\n\nvoid xdp_return_frame_rx_napi(struct xdp_frame *xdpf)\n{\n\tstruct skb_shared_info *sinfo;\n\tint i;\n\n\tif (likely(!xdp_frame_has_frags(xdpf)))\n\t\tgoto out;\n\n\tsinfo = xdp_get_shared_info_from_frame(xdpf);\n\tfor (i = 0; i < sinfo->nr_frags; i++) {\n\t\tstruct page *page = skb_frag_page(&sinfo->frags[i]);\n\n\t\t__xdp_return(page_address(page), &xdpf->mem, true, NULL);\n\t}\nout:\n\t__xdp_return(xdpf->data, &xdpf->mem, true, NULL);\n}\nEXPORT_SYMBOL_GPL(xdp_return_frame_rx_napi);\n\n \nvoid xdp_flush_frame_bulk(struct xdp_frame_bulk *bq)\n{\n\tstruct xdp_mem_allocator *xa = bq->xa;\n\n\tif (unlikely(!xa || !bq->count))\n\t\treturn;\n\n\tpage_pool_put_page_bulk(xa->page_pool, bq->q, bq->count);\n\t \n\tbq->count = 0;\n}\nEXPORT_SYMBOL_GPL(xdp_flush_frame_bulk);\n\n \nvoid xdp_return_frame_bulk(struct xdp_frame *xdpf,\n\t\t\t   struct xdp_frame_bulk *bq)\n{\n\tstruct xdp_mem_info *mem = &xdpf->mem;\n\tstruct xdp_mem_allocator *xa;\n\n\tif (mem->type != MEM_TYPE_PAGE_POOL) {\n\t\txdp_return_frame(xdpf);\n\t\treturn;\n\t}\n\n\txa = bq->xa;\n\tif (unlikely(!xa)) {\n\t\txa = rhashtable_lookup(mem_id_ht, &mem->id, mem_id_rht_params);\n\t\tbq->count = 0;\n\t\tbq->xa = xa;\n\t}\n\n\tif (bq->count == XDP_BULK_QUEUE_SIZE)\n\t\txdp_flush_frame_bulk(bq);\n\n\tif (unlikely(mem->id != xa->mem.id)) {\n\t\txdp_flush_frame_bulk(bq);\n\t\tbq->xa = rhashtable_lookup(mem_id_ht, &mem->id, mem_id_rht_params);\n\t}\n\n\tif (unlikely(xdp_frame_has_frags(xdpf))) {\n\t\tstruct skb_shared_info *sinfo;\n\t\tint i;\n\n\t\tsinfo = xdp_get_shared_info_from_frame(xdpf);\n\t\tfor (i = 0; i < sinfo->nr_frags; i++) {\n\t\t\tskb_frag_t *frag = &sinfo->frags[i];\n\n\t\t\tbq->q[bq->count++] = skb_frag_address(frag);\n\t\t\tif (bq->count == XDP_BULK_QUEUE_SIZE)\n\t\t\t\txdp_flush_frame_bulk(bq);\n\t\t}\n\t}\n\tbq->q[bq->count++] = xdpf->data;\n}\nEXPORT_SYMBOL_GPL(xdp_return_frame_bulk);\n\nvoid xdp_return_buff(struct xdp_buff *xdp)\n{\n\tstruct skb_shared_info *sinfo;\n\tint i;\n\n\tif (likely(!xdp_buff_has_frags(xdp)))\n\t\tgoto out;\n\n\tsinfo = xdp_get_shared_info_from_buff(xdp);\n\tfor (i = 0; i < sinfo->nr_frags; i++) {\n\t\tstruct page *page = skb_frag_page(&sinfo->frags[i]);\n\n\t\t__xdp_return(page_address(page), &xdp->rxq->mem, true, xdp);\n\t}\nout:\n\t__xdp_return(xdp->data, &xdp->rxq->mem, true, xdp);\n}\nEXPORT_SYMBOL_GPL(xdp_return_buff);\n\nvoid xdp_attachment_setup(struct xdp_attachment_info *info,\n\t\t\t  struct netdev_bpf *bpf)\n{\n\tif (info->prog)\n\t\tbpf_prog_put(info->prog);\n\tinfo->prog = bpf->prog;\n\tinfo->flags = bpf->flags;\n}\nEXPORT_SYMBOL_GPL(xdp_attachment_setup);\n\nstruct xdp_frame *xdp_convert_zc_to_xdp_frame(struct xdp_buff *xdp)\n{\n\tunsigned int metasize, totsize;\n\tvoid *addr, *data_to_copy;\n\tstruct xdp_frame *xdpf;\n\tstruct page *page;\n\n\t \n\tmetasize = xdp_data_meta_unsupported(xdp) ? 0 :\n\t\t   xdp->data - xdp->data_meta;\n\ttotsize = xdp->data_end - xdp->data + metasize;\n\n\tif (sizeof(*xdpf) + totsize > PAGE_SIZE)\n\t\treturn NULL;\n\n\tpage = dev_alloc_page();\n\tif (!page)\n\t\treturn NULL;\n\n\taddr = page_to_virt(page);\n\txdpf = addr;\n\tmemset(xdpf, 0, sizeof(*xdpf));\n\n\taddr += sizeof(*xdpf);\n\tdata_to_copy = metasize ? xdp->data_meta : xdp->data;\n\tmemcpy(addr, data_to_copy, totsize);\n\n\txdpf->data = addr + metasize;\n\txdpf->len = totsize - metasize;\n\txdpf->headroom = 0;\n\txdpf->metasize = metasize;\n\txdpf->frame_sz = PAGE_SIZE;\n\txdpf->mem.type = MEM_TYPE_PAGE_ORDER0;\n\n\txsk_buff_free(xdp);\n\treturn xdpf;\n}\nEXPORT_SYMBOL_GPL(xdp_convert_zc_to_xdp_frame);\n\n \nvoid xdp_warn(const char *msg, const char *func, const int line)\n{\n\tWARN(1, \"XDP_WARN: %s(line:%d): %s\\n\", func, line, msg);\n};\nEXPORT_SYMBOL_GPL(xdp_warn);\n\nint xdp_alloc_skb_bulk(void **skbs, int n_skb, gfp_t gfp)\n{\n\tn_skb = kmem_cache_alloc_bulk(skbuff_cache, gfp, n_skb, skbs);\n\tif (unlikely(!n_skb))\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(xdp_alloc_skb_bulk);\n\nstruct sk_buff *__xdp_build_skb_from_frame(struct xdp_frame *xdpf,\n\t\t\t\t\t   struct sk_buff *skb,\n\t\t\t\t\t   struct net_device *dev)\n{\n\tstruct skb_shared_info *sinfo = xdp_get_shared_info_from_frame(xdpf);\n\tunsigned int headroom, frame_size;\n\tvoid *hard_start;\n\tu8 nr_frags;\n\n\t \n\tif (unlikely(xdp_frame_has_frags(xdpf)))\n\t\tnr_frags = sinfo->nr_frags;\n\n\t \n\theadroom = sizeof(*xdpf) + xdpf->headroom;\n\n\t \n\tframe_size = xdpf->frame_sz;\n\n\thard_start = xdpf->data - headroom;\n\tskb = build_skb_around(skb, hard_start, frame_size);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\tskb_reserve(skb, headroom);\n\t__skb_put(skb, xdpf->len);\n\tif (xdpf->metasize)\n\t\tskb_metadata_set(skb, xdpf->metasize);\n\n\tif (unlikely(xdp_frame_has_frags(xdpf)))\n\t\txdp_update_skb_shared_info(skb, nr_frags,\n\t\t\t\t\t   sinfo->xdp_frags_size,\n\t\t\t\t\t   nr_frags * xdpf->frame_sz,\n\t\t\t\t\t   xdp_frame_is_frag_pfmemalloc(xdpf));\n\n\t \n\tskb->protocol = eth_type_trans(skb, dev);\n\n\t \n\n\tif (xdpf->mem.type == MEM_TYPE_PAGE_POOL)\n\t\tskb_mark_for_recycle(skb);\n\n\t \n\txdp_scrub_frame(xdpf);\n\n\treturn skb;\n}\nEXPORT_SYMBOL_GPL(__xdp_build_skb_from_frame);\n\nstruct sk_buff *xdp_build_skb_from_frame(struct xdp_frame *xdpf,\n\t\t\t\t\t struct net_device *dev)\n{\n\tstruct sk_buff *skb;\n\n\tskb = kmem_cache_alloc(skbuff_cache, GFP_ATOMIC);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\tmemset(skb, 0, offsetof(struct sk_buff, tail));\n\n\treturn __xdp_build_skb_from_frame(xdpf, skb, dev);\n}\nEXPORT_SYMBOL_GPL(xdp_build_skb_from_frame);\n\nstruct xdp_frame *xdpf_clone(struct xdp_frame *xdpf)\n{\n\tunsigned int headroom, totalsize;\n\tstruct xdp_frame *nxdpf;\n\tstruct page *page;\n\tvoid *addr;\n\n\theadroom = xdpf->headroom + sizeof(*xdpf);\n\ttotalsize = headroom + xdpf->len;\n\n\tif (unlikely(totalsize > PAGE_SIZE))\n\t\treturn NULL;\n\tpage = dev_alloc_page();\n\tif (!page)\n\t\treturn NULL;\n\taddr = page_to_virt(page);\n\n\tmemcpy(addr, xdpf, totalsize);\n\n\tnxdpf = addr;\n\tnxdpf->data = addr + headroom;\n\tnxdpf->frame_sz = PAGE_SIZE;\n\tnxdpf->mem.type = MEM_TYPE_PAGE_ORDER0;\n\tnxdpf->mem.id = 0;\n\n\treturn nxdpf;\n}\n\n__diag_push();\n__diag_ignore_all(\"-Wmissing-prototypes\",\n\t\t  \"Global functions as their definitions will be in vmlinux BTF\");\n\n \n__bpf_kfunc int bpf_xdp_metadata_rx_timestamp(const struct xdp_md *ctx, u64 *timestamp)\n{\n\treturn -EOPNOTSUPP;\n}\n\n \n__bpf_kfunc int bpf_xdp_metadata_rx_hash(const struct xdp_md *ctx, u32 *hash,\n\t\t\t\t\t enum xdp_rss_hash_type *rss_type)\n{\n\treturn -EOPNOTSUPP;\n}\n\n__diag_pop();\n\nBTF_SET8_START(xdp_metadata_kfunc_ids)\n#define XDP_METADATA_KFUNC(_, name) BTF_ID_FLAGS(func, name, KF_TRUSTED_ARGS)\nXDP_METADATA_KFUNC_xxx\n#undef XDP_METADATA_KFUNC\nBTF_SET8_END(xdp_metadata_kfunc_ids)\n\nstatic const struct btf_kfunc_id_set xdp_metadata_kfunc_set = {\n\t.owner = THIS_MODULE,\n\t.set   = &xdp_metadata_kfunc_ids,\n};\n\nBTF_ID_LIST(xdp_metadata_kfunc_ids_unsorted)\n#define XDP_METADATA_KFUNC(name, str) BTF_ID(func, str)\nXDP_METADATA_KFUNC_xxx\n#undef XDP_METADATA_KFUNC\n\nu32 bpf_xdp_metadata_kfunc_id(int id)\n{\n\t \n\treturn xdp_metadata_kfunc_ids_unsorted[id];\n}\n\nbool bpf_dev_bound_kfunc_id(u32 btf_id)\n{\n\treturn btf_id_set8_contains(&xdp_metadata_kfunc_ids, btf_id);\n}\n\nstatic int __init xdp_metadata_init(void)\n{\n\treturn register_btf_kfunc_id_set(BPF_PROG_TYPE_XDP, &xdp_metadata_kfunc_set);\n}\nlate_initcall(xdp_metadata_init);\n\nvoid xdp_set_features_flag(struct net_device *dev, xdp_features_t val)\n{\n\tval &= NETDEV_XDP_ACT_MASK;\n\tif (dev->xdp_features == val)\n\t\treturn;\n\n\tdev->xdp_features = val;\n\n\tif (dev->reg_state == NETREG_REGISTERED)\n\t\tcall_netdevice_notifiers(NETDEV_XDP_FEAT_CHANGE, dev);\n}\nEXPORT_SYMBOL_GPL(xdp_set_features_flag);\n\nvoid xdp_features_set_redirect_target(struct net_device *dev, bool support_sg)\n{\n\txdp_features_t val = (dev->xdp_features | NETDEV_XDP_ACT_NDO_XMIT);\n\n\tif (support_sg)\n\t\tval |= NETDEV_XDP_ACT_NDO_XMIT_SG;\n\txdp_set_features_flag(dev, val);\n}\nEXPORT_SYMBOL_GPL(xdp_features_set_redirect_target);\n\nvoid xdp_features_clear_redirect_target(struct net_device *dev)\n{\n\txdp_features_t val = dev->xdp_features;\n\n\tval &= ~(NETDEV_XDP_ACT_NDO_XMIT | NETDEV_XDP_ACT_NDO_XMIT_SG);\n\txdp_set_features_flag(dev, val);\n}\nEXPORT_SYMBOL_GPL(xdp_features_clear_redirect_target);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}