{
  "module_name": "dev.c",
  "hash_id": "003e8ef1698fcd27bf770b03e65207d0061e0a9ea26615b02f76a4e840ab7c6f",
  "original_prompt": "Ingested from linux-6.6.14/net/core/dev.c",
  "human_readable_source": "\n \n\n#include <linux/uaccess.h>\n#include <linux/bitmap.h>\n#include <linux/capability.h>\n#include <linux/cpu.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/hash.h>\n#include <linux/slab.h>\n#include <linux/sched.h>\n#include <linux/sched/mm.h>\n#include <linux/mutex.h>\n#include <linux/rwsem.h>\n#include <linux/string.h>\n#include <linux/mm.h>\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/errno.h>\n#include <linux/interrupt.h>\n#include <linux/if_ether.h>\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/ethtool.h>\n#include <linux/skbuff.h>\n#include <linux/kthread.h>\n#include <linux/bpf.h>\n#include <linux/bpf_trace.h>\n#include <net/net_namespace.h>\n#include <net/sock.h>\n#include <net/busy_poll.h>\n#include <linux/rtnetlink.h>\n#include <linux/stat.h>\n#include <net/dsa.h>\n#include <net/dst.h>\n#include <net/dst_metadata.h>\n#include <net/gro.h>\n#include <net/pkt_sched.h>\n#include <net/pkt_cls.h>\n#include <net/checksum.h>\n#include <net/xfrm.h>\n#include <net/tcx.h>\n#include <linux/highmem.h>\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/netpoll.h>\n#include <linux/rcupdate.h>\n#include <linux/delay.h>\n#include <net/iw_handler.h>\n#include <asm/current.h>\n#include <linux/audit.h>\n#include <linux/dmaengine.h>\n#include <linux/err.h>\n#include <linux/ctype.h>\n#include <linux/if_arp.h>\n#include <linux/if_vlan.h>\n#include <linux/ip.h>\n#include <net/ip.h>\n#include <net/mpls.h>\n#include <linux/ipv6.h>\n#include <linux/in.h>\n#include <linux/jhash.h>\n#include <linux/random.h>\n#include <trace/events/napi.h>\n#include <trace/events/net.h>\n#include <trace/events/skb.h>\n#include <trace/events/qdisc.h>\n#include <trace/events/xdp.h>\n#include <linux/inetdevice.h>\n#include <linux/cpu_rmap.h>\n#include <linux/static_key.h>\n#include <linux/hashtable.h>\n#include <linux/vmalloc.h>\n#include <linux/if_macvlan.h>\n#include <linux/errqueue.h>\n#include <linux/hrtimer.h>\n#include <linux/netfilter_netdev.h>\n#include <linux/crash_dump.h>\n#include <linux/sctp.h>\n#include <net/udp_tunnel.h>\n#include <linux/net_namespace.h>\n#include <linux/indirect_call_wrapper.h>\n#include <net/devlink.h>\n#include <linux/pm_runtime.h>\n#include <linux/prandom.h>\n#include <linux/once_lite.h>\n#include <net/netdev_rx_queue.h>\n\n#include \"dev.h\"\n#include \"net-sysfs.h\"\n\nstatic DEFINE_SPINLOCK(ptype_lock);\nstruct list_head ptype_base[PTYPE_HASH_SIZE] __read_mostly;\nstruct list_head ptype_all __read_mostly;\t \n\nstatic int netif_rx_internal(struct sk_buff *skb);\nstatic int call_netdevice_notifiers_extack(unsigned long val,\n\t\t\t\t\t   struct net_device *dev,\n\t\t\t\t\t   struct netlink_ext_ack *extack);\nstatic struct napi_struct *napi_by_id(unsigned int napi_id);\n\n \nDEFINE_RWLOCK(dev_base_lock);\nEXPORT_SYMBOL(dev_base_lock);\n\nstatic DEFINE_MUTEX(ifalias_mutex);\n\n \nstatic DEFINE_SPINLOCK(napi_hash_lock);\n\nstatic unsigned int napi_gen_id = NR_CPUS;\nstatic DEFINE_READ_MOSTLY_HASHTABLE(napi_hash, 8);\n\nstatic DECLARE_RWSEM(devnet_rename_sem);\n\nstatic inline void dev_base_seq_inc(struct net *net)\n{\n\twhile (++net->dev_base_seq == 0)\n\t\t;\n}\n\nstatic inline struct hlist_head *dev_name_hash(struct net *net, const char *name)\n{\n\tunsigned int hash = full_name_hash(net, name, strnlen(name, IFNAMSIZ));\n\n\treturn &net->dev_name_head[hash_32(hash, NETDEV_HASHBITS)];\n}\n\nstatic inline struct hlist_head *dev_index_hash(struct net *net, int ifindex)\n{\n\treturn &net->dev_index_head[ifindex & (NETDEV_HASHENTRIES - 1)];\n}\n\nstatic inline void rps_lock_irqsave(struct softnet_data *sd,\n\t\t\t\t    unsigned long *flags)\n{\n\tif (IS_ENABLED(CONFIG_RPS))\n\t\tspin_lock_irqsave(&sd->input_pkt_queue.lock, *flags);\n\telse if (!IS_ENABLED(CONFIG_PREEMPT_RT))\n\t\tlocal_irq_save(*flags);\n}\n\nstatic inline void rps_lock_irq_disable(struct softnet_data *sd)\n{\n\tif (IS_ENABLED(CONFIG_RPS))\n\t\tspin_lock_irq(&sd->input_pkt_queue.lock);\n\telse if (!IS_ENABLED(CONFIG_PREEMPT_RT))\n\t\tlocal_irq_disable();\n}\n\nstatic inline void rps_unlock_irq_restore(struct softnet_data *sd,\n\t\t\t\t\t  unsigned long *flags)\n{\n\tif (IS_ENABLED(CONFIG_RPS))\n\t\tspin_unlock_irqrestore(&sd->input_pkt_queue.lock, *flags);\n\telse if (!IS_ENABLED(CONFIG_PREEMPT_RT))\n\t\tlocal_irq_restore(*flags);\n}\n\nstatic inline void rps_unlock_irq_enable(struct softnet_data *sd)\n{\n\tif (IS_ENABLED(CONFIG_RPS))\n\t\tspin_unlock_irq(&sd->input_pkt_queue.lock);\n\telse if (!IS_ENABLED(CONFIG_PREEMPT_RT))\n\t\tlocal_irq_enable();\n}\n\nstatic struct netdev_name_node *netdev_name_node_alloc(struct net_device *dev,\n\t\t\t\t\t\t       const char *name)\n{\n\tstruct netdev_name_node *name_node;\n\n\tname_node = kmalloc(sizeof(*name_node), GFP_KERNEL);\n\tif (!name_node)\n\t\treturn NULL;\n\tINIT_HLIST_NODE(&name_node->hlist);\n\tname_node->dev = dev;\n\tname_node->name = name;\n\treturn name_node;\n}\n\nstatic struct netdev_name_node *\nnetdev_name_node_head_alloc(struct net_device *dev)\n{\n\tstruct netdev_name_node *name_node;\n\n\tname_node = netdev_name_node_alloc(dev, dev->name);\n\tif (!name_node)\n\t\treturn NULL;\n\tINIT_LIST_HEAD(&name_node->list);\n\treturn name_node;\n}\n\nstatic void netdev_name_node_free(struct netdev_name_node *name_node)\n{\n\tkfree(name_node);\n}\n\nstatic void netdev_name_node_add(struct net *net,\n\t\t\t\t struct netdev_name_node *name_node)\n{\n\thlist_add_head_rcu(&name_node->hlist,\n\t\t\t   dev_name_hash(net, name_node->name));\n}\n\nstatic void netdev_name_node_del(struct netdev_name_node *name_node)\n{\n\thlist_del_rcu(&name_node->hlist);\n}\n\nstatic struct netdev_name_node *netdev_name_node_lookup(struct net *net,\n\t\t\t\t\t\t\tconst char *name)\n{\n\tstruct hlist_head *head = dev_name_hash(net, name);\n\tstruct netdev_name_node *name_node;\n\n\thlist_for_each_entry(name_node, head, hlist)\n\t\tif (!strcmp(name_node->name, name))\n\t\t\treturn name_node;\n\treturn NULL;\n}\n\nstatic struct netdev_name_node *netdev_name_node_lookup_rcu(struct net *net,\n\t\t\t\t\t\t\t    const char *name)\n{\n\tstruct hlist_head *head = dev_name_hash(net, name);\n\tstruct netdev_name_node *name_node;\n\n\thlist_for_each_entry_rcu(name_node, head, hlist)\n\t\tif (!strcmp(name_node->name, name))\n\t\t\treturn name_node;\n\treturn NULL;\n}\n\nbool netdev_name_in_use(struct net *net, const char *name)\n{\n\treturn netdev_name_node_lookup(net, name);\n}\nEXPORT_SYMBOL(netdev_name_in_use);\n\nint netdev_name_node_alt_create(struct net_device *dev, const char *name)\n{\n\tstruct netdev_name_node *name_node;\n\tstruct net *net = dev_net(dev);\n\n\tname_node = netdev_name_node_lookup(net, name);\n\tif (name_node)\n\t\treturn -EEXIST;\n\tname_node = netdev_name_node_alloc(dev, name);\n\tif (!name_node)\n\t\treturn -ENOMEM;\n\tnetdev_name_node_add(net, name_node);\n\t \n\tlist_add_tail(&name_node->list, &dev->name_node->list);\n\n\treturn 0;\n}\n\nstatic void __netdev_name_node_alt_destroy(struct netdev_name_node *name_node)\n{\n\tlist_del(&name_node->list);\n\tkfree(name_node->name);\n\tnetdev_name_node_free(name_node);\n}\n\nint netdev_name_node_alt_destroy(struct net_device *dev, const char *name)\n{\n\tstruct netdev_name_node *name_node;\n\tstruct net *net = dev_net(dev);\n\n\tname_node = netdev_name_node_lookup(net, name);\n\tif (!name_node)\n\t\treturn -ENOENT;\n\t \n\tif (name_node == dev->name_node || name_node->dev != dev)\n\t\treturn -EINVAL;\n\n\tnetdev_name_node_del(name_node);\n\tsynchronize_rcu();\n\t__netdev_name_node_alt_destroy(name_node);\n\n\treturn 0;\n}\n\nstatic void netdev_name_node_alt_flush(struct net_device *dev)\n{\n\tstruct netdev_name_node *name_node, *tmp;\n\n\tlist_for_each_entry_safe(name_node, tmp, &dev->name_node->list, list)\n\t\t__netdev_name_node_alt_destroy(name_node);\n}\n\n \nstatic void list_netdevice(struct net_device *dev)\n{\n\tstruct netdev_name_node *name_node;\n\tstruct net *net = dev_net(dev);\n\n\tASSERT_RTNL();\n\n\twrite_lock(&dev_base_lock);\n\tlist_add_tail_rcu(&dev->dev_list, &net->dev_base_head);\n\tnetdev_name_node_add(net, dev->name_node);\n\thlist_add_head_rcu(&dev->index_hlist,\n\t\t\t   dev_index_hash(net, dev->ifindex));\n\twrite_unlock(&dev_base_lock);\n\n\tnetdev_for_each_altname(dev, name_node)\n\t\tnetdev_name_node_add(net, name_node);\n\n\t \n\tWARN_ON(xa_store(&net->dev_by_index, dev->ifindex, dev, GFP_KERNEL));\n\n\tdev_base_seq_inc(net);\n}\n\n \nstatic void unlist_netdevice(struct net_device *dev, bool lock)\n{\n\tstruct netdev_name_node *name_node;\n\tstruct net *net = dev_net(dev);\n\n\tASSERT_RTNL();\n\n\txa_erase(&net->dev_by_index, dev->ifindex);\n\n\tnetdev_for_each_altname(dev, name_node)\n\t\tnetdev_name_node_del(name_node);\n\n\t \n\tif (lock)\n\t\twrite_lock(&dev_base_lock);\n\tlist_del_rcu(&dev->dev_list);\n\tnetdev_name_node_del(dev->name_node);\n\thlist_del_rcu(&dev->index_hlist);\n\tif (lock)\n\t\twrite_unlock(&dev_base_lock);\n\n\tdev_base_seq_inc(dev_net(dev));\n}\n\n \n\nstatic RAW_NOTIFIER_HEAD(netdev_chain);\n\n \n\nDEFINE_PER_CPU_ALIGNED(struct softnet_data, softnet_data);\nEXPORT_PER_CPU_SYMBOL(softnet_data);\n\n#ifdef CONFIG_LOCKDEP\n \nstatic const unsigned short netdev_lock_type[] = {\n\t ARPHRD_NETROM, ARPHRD_ETHER, ARPHRD_EETHER, ARPHRD_AX25,\n\t ARPHRD_PRONET, ARPHRD_CHAOS, ARPHRD_IEEE802, ARPHRD_ARCNET,\n\t ARPHRD_APPLETLK, ARPHRD_DLCI, ARPHRD_ATM, ARPHRD_METRICOM,\n\t ARPHRD_IEEE1394, ARPHRD_EUI64, ARPHRD_INFINIBAND, ARPHRD_SLIP,\n\t ARPHRD_CSLIP, ARPHRD_SLIP6, ARPHRD_CSLIP6, ARPHRD_RSRVD,\n\t ARPHRD_ADAPT, ARPHRD_ROSE, ARPHRD_X25, ARPHRD_HWX25,\n\t ARPHRD_PPP, ARPHRD_CISCO, ARPHRD_LAPB, ARPHRD_DDCMP,\n\t ARPHRD_RAWHDLC, ARPHRD_TUNNEL, ARPHRD_TUNNEL6, ARPHRD_FRAD,\n\t ARPHRD_SKIP, ARPHRD_LOOPBACK, ARPHRD_LOCALTLK, ARPHRD_FDDI,\n\t ARPHRD_BIF, ARPHRD_SIT, ARPHRD_IPDDP, ARPHRD_IPGRE,\n\t ARPHRD_PIMREG, ARPHRD_HIPPI, ARPHRD_ASH, ARPHRD_ECONET,\n\t ARPHRD_IRDA, ARPHRD_FCPP, ARPHRD_FCAL, ARPHRD_FCPL,\n\t ARPHRD_FCFABRIC, ARPHRD_IEEE80211, ARPHRD_IEEE80211_PRISM,\n\t ARPHRD_IEEE80211_RADIOTAP, ARPHRD_PHONET, ARPHRD_PHONET_PIPE,\n\t ARPHRD_IEEE802154, ARPHRD_VOID, ARPHRD_NONE};\n\nstatic const char *const netdev_lock_name[] = {\n\t\"_xmit_NETROM\", \"_xmit_ETHER\", \"_xmit_EETHER\", \"_xmit_AX25\",\n\t\"_xmit_PRONET\", \"_xmit_CHAOS\", \"_xmit_IEEE802\", \"_xmit_ARCNET\",\n\t\"_xmit_APPLETLK\", \"_xmit_DLCI\", \"_xmit_ATM\", \"_xmit_METRICOM\",\n\t\"_xmit_IEEE1394\", \"_xmit_EUI64\", \"_xmit_INFINIBAND\", \"_xmit_SLIP\",\n\t\"_xmit_CSLIP\", \"_xmit_SLIP6\", \"_xmit_CSLIP6\", \"_xmit_RSRVD\",\n\t\"_xmit_ADAPT\", \"_xmit_ROSE\", \"_xmit_X25\", \"_xmit_HWX25\",\n\t\"_xmit_PPP\", \"_xmit_CISCO\", \"_xmit_LAPB\", \"_xmit_DDCMP\",\n\t\"_xmit_RAWHDLC\", \"_xmit_TUNNEL\", \"_xmit_TUNNEL6\", \"_xmit_FRAD\",\n\t\"_xmit_SKIP\", \"_xmit_LOOPBACK\", \"_xmit_LOCALTLK\", \"_xmit_FDDI\",\n\t\"_xmit_BIF\", \"_xmit_SIT\", \"_xmit_IPDDP\", \"_xmit_IPGRE\",\n\t\"_xmit_PIMREG\", \"_xmit_HIPPI\", \"_xmit_ASH\", \"_xmit_ECONET\",\n\t\"_xmit_IRDA\", \"_xmit_FCPP\", \"_xmit_FCAL\", \"_xmit_FCPL\",\n\t\"_xmit_FCFABRIC\", \"_xmit_IEEE80211\", \"_xmit_IEEE80211_PRISM\",\n\t\"_xmit_IEEE80211_RADIOTAP\", \"_xmit_PHONET\", \"_xmit_PHONET_PIPE\",\n\t\"_xmit_IEEE802154\", \"_xmit_VOID\", \"_xmit_NONE\"};\n\nstatic struct lock_class_key netdev_xmit_lock_key[ARRAY_SIZE(netdev_lock_type)];\nstatic struct lock_class_key netdev_addr_lock_key[ARRAY_SIZE(netdev_lock_type)];\n\nstatic inline unsigned short netdev_lock_pos(unsigned short dev_type)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(netdev_lock_type); i++)\n\t\tif (netdev_lock_type[i] == dev_type)\n\t\t\treturn i;\n\t \n\treturn ARRAY_SIZE(netdev_lock_type) - 1;\n}\n\nstatic inline void netdev_set_xmit_lockdep_class(spinlock_t *lock,\n\t\t\t\t\t\t unsigned short dev_type)\n{\n\tint i;\n\n\ti = netdev_lock_pos(dev_type);\n\tlockdep_set_class_and_name(lock, &netdev_xmit_lock_key[i],\n\t\t\t\t   netdev_lock_name[i]);\n}\n\nstatic inline void netdev_set_addr_lockdep_class(struct net_device *dev)\n{\n\tint i;\n\n\ti = netdev_lock_pos(dev->type);\n\tlockdep_set_class_and_name(&dev->addr_list_lock,\n\t\t\t\t   &netdev_addr_lock_key[i],\n\t\t\t\t   netdev_lock_name[i]);\n}\n#else\nstatic inline void netdev_set_xmit_lockdep_class(spinlock_t *lock,\n\t\t\t\t\t\t unsigned short dev_type)\n{\n}\n\nstatic inline void netdev_set_addr_lockdep_class(struct net_device *dev)\n{\n}\n#endif\n\n \n\n\n \n\nstatic inline struct list_head *ptype_head(const struct packet_type *pt)\n{\n\tif (pt->type == htons(ETH_P_ALL))\n\t\treturn pt->dev ? &pt->dev->ptype_all : &ptype_all;\n\telse\n\t\treturn pt->dev ? &pt->dev->ptype_specific :\n\t\t\t\t &ptype_base[ntohs(pt->type) & PTYPE_HASH_MASK];\n}\n\n \n\nvoid dev_add_pack(struct packet_type *pt)\n{\n\tstruct list_head *head = ptype_head(pt);\n\n\tspin_lock(&ptype_lock);\n\tlist_add_rcu(&pt->list, head);\n\tspin_unlock(&ptype_lock);\n}\nEXPORT_SYMBOL(dev_add_pack);\n\n \nvoid __dev_remove_pack(struct packet_type *pt)\n{\n\tstruct list_head *head = ptype_head(pt);\n\tstruct packet_type *pt1;\n\n\tspin_lock(&ptype_lock);\n\n\tlist_for_each_entry(pt1, head, list) {\n\t\tif (pt == pt1) {\n\t\t\tlist_del_rcu(&pt->list);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tpr_warn(\"dev_remove_pack: %p not found\\n\", pt);\nout:\n\tspin_unlock(&ptype_lock);\n}\nEXPORT_SYMBOL(__dev_remove_pack);\n\n \nvoid dev_remove_pack(struct packet_type *pt)\n{\n\t__dev_remove_pack(pt);\n\n\tsynchronize_net();\n}\nEXPORT_SYMBOL(dev_remove_pack);\n\n\n \n\n \n\nint dev_get_iflink(const struct net_device *dev)\n{\n\tif (dev->netdev_ops && dev->netdev_ops->ndo_get_iflink)\n\t\treturn dev->netdev_ops->ndo_get_iflink(dev);\n\n\treturn dev->ifindex;\n}\nEXPORT_SYMBOL(dev_get_iflink);\n\n \nint dev_fill_metadata_dst(struct net_device *dev, struct sk_buff *skb)\n{\n\tstruct ip_tunnel_info *info;\n\n\tif (!dev->netdev_ops  || !dev->netdev_ops->ndo_fill_metadata_dst)\n\t\treturn -EINVAL;\n\n\tinfo = skb_tunnel_info_unclone(skb);\n\tif (!info)\n\t\treturn -ENOMEM;\n\tif (unlikely(!(info->mode & IP_TUNNEL_INFO_TX)))\n\t\treturn -EINVAL;\n\n\treturn dev->netdev_ops->ndo_fill_metadata_dst(dev, skb);\n}\nEXPORT_SYMBOL_GPL(dev_fill_metadata_dst);\n\nstatic struct net_device_path *dev_fwd_path(struct net_device_path_stack *stack)\n{\n\tint k = stack->num_paths++;\n\n\tif (WARN_ON_ONCE(k >= NET_DEVICE_PATH_STACK_MAX))\n\t\treturn NULL;\n\n\treturn &stack->path[k];\n}\n\nint dev_fill_forward_path(const struct net_device *dev, const u8 *daddr,\n\t\t\t  struct net_device_path_stack *stack)\n{\n\tconst struct net_device *last_dev;\n\tstruct net_device_path_ctx ctx = {\n\t\t.dev\t= dev,\n\t};\n\tstruct net_device_path *path;\n\tint ret = 0;\n\n\tmemcpy(ctx.daddr, daddr, sizeof(ctx.daddr));\n\tstack->num_paths = 0;\n\twhile (ctx.dev && ctx.dev->netdev_ops->ndo_fill_forward_path) {\n\t\tlast_dev = ctx.dev;\n\t\tpath = dev_fwd_path(stack);\n\t\tif (!path)\n\t\t\treturn -1;\n\n\t\tmemset(path, 0, sizeof(struct net_device_path));\n\t\tret = ctx.dev->netdev_ops->ndo_fill_forward_path(&ctx, path);\n\t\tif (ret < 0)\n\t\t\treturn -1;\n\n\t\tif (WARN_ON_ONCE(last_dev == ctx.dev))\n\t\t\treturn -1;\n\t}\n\n\tif (!ctx.dev)\n\t\treturn ret;\n\n\tpath = dev_fwd_path(stack);\n\tif (!path)\n\t\treturn -1;\n\tpath->type = DEV_PATH_ETHERNET;\n\tpath->dev = ctx.dev;\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(dev_fill_forward_path);\n\n \n\nstruct net_device *__dev_get_by_name(struct net *net, const char *name)\n{\n\tstruct netdev_name_node *node_name;\n\n\tnode_name = netdev_name_node_lookup(net, name);\n\treturn node_name ? node_name->dev : NULL;\n}\nEXPORT_SYMBOL(__dev_get_by_name);\n\n \n\nstruct net_device *dev_get_by_name_rcu(struct net *net, const char *name)\n{\n\tstruct netdev_name_node *node_name;\n\n\tnode_name = netdev_name_node_lookup_rcu(net, name);\n\treturn node_name ? node_name->dev : NULL;\n}\nEXPORT_SYMBOL(dev_get_by_name_rcu);\n\n \nstruct net_device *dev_get_by_name(struct net *net, const char *name)\n{\n\tstruct net_device *dev;\n\n\trcu_read_lock();\n\tdev = dev_get_by_name_rcu(net, name);\n\tdev_hold(dev);\n\trcu_read_unlock();\n\treturn dev;\n}\nEXPORT_SYMBOL(dev_get_by_name);\n\n \nstruct net_device *netdev_get_by_name(struct net *net, const char *name,\n\t\t\t\t      netdevice_tracker *tracker, gfp_t gfp)\n{\n\tstruct net_device *dev;\n\n\tdev = dev_get_by_name(net, name);\n\tif (dev)\n\t\tnetdev_tracker_alloc(dev, tracker, gfp);\n\treturn dev;\n}\nEXPORT_SYMBOL(netdev_get_by_name);\n\n \n\nstruct net_device *__dev_get_by_index(struct net *net, int ifindex)\n{\n\tstruct net_device *dev;\n\tstruct hlist_head *head = dev_index_hash(net, ifindex);\n\n\thlist_for_each_entry(dev, head, index_hlist)\n\t\tif (dev->ifindex == ifindex)\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(__dev_get_by_index);\n\n \n\nstruct net_device *dev_get_by_index_rcu(struct net *net, int ifindex)\n{\n\tstruct net_device *dev;\n\tstruct hlist_head *head = dev_index_hash(net, ifindex);\n\n\thlist_for_each_entry_rcu(dev, head, index_hlist)\n\t\tif (dev->ifindex == ifindex)\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(dev_get_by_index_rcu);\n\n \nstruct net_device *dev_get_by_index(struct net *net, int ifindex)\n{\n\tstruct net_device *dev;\n\n\trcu_read_lock();\n\tdev = dev_get_by_index_rcu(net, ifindex);\n\tdev_hold(dev);\n\trcu_read_unlock();\n\treturn dev;\n}\nEXPORT_SYMBOL(dev_get_by_index);\n\n \nstruct net_device *netdev_get_by_index(struct net *net, int ifindex,\n\t\t\t\t       netdevice_tracker *tracker, gfp_t gfp)\n{\n\tstruct net_device *dev;\n\n\tdev = dev_get_by_index(net, ifindex);\n\tif (dev)\n\t\tnetdev_tracker_alloc(dev, tracker, gfp);\n\treturn dev;\n}\nEXPORT_SYMBOL(netdev_get_by_index);\n\n \n\nstruct net_device *dev_get_by_napi_id(unsigned int napi_id)\n{\n\tstruct napi_struct *napi;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\n\tif (napi_id < MIN_NAPI_ID)\n\t\treturn NULL;\n\n\tnapi = napi_by_id(napi_id);\n\n\treturn napi ? napi->dev : NULL;\n}\nEXPORT_SYMBOL(dev_get_by_napi_id);\n\n \nint netdev_get_name(struct net *net, char *name, int ifindex)\n{\n\tstruct net_device *dev;\n\tint ret;\n\n\tdown_read(&devnet_rename_sem);\n\trcu_read_lock();\n\n\tdev = dev_get_by_index_rcu(net, ifindex);\n\tif (!dev) {\n\t\tret = -ENODEV;\n\t\tgoto out;\n\t}\n\n\tstrcpy(name, dev->name);\n\n\tret = 0;\nout:\n\trcu_read_unlock();\n\tup_read(&devnet_rename_sem);\n\treturn ret;\n}\n\n \n\nstruct net_device *dev_getbyhwaddr_rcu(struct net *net, unsigned short type,\n\t\t\t\t       const char *ha)\n{\n\tstruct net_device *dev;\n\n\tfor_each_netdev_rcu(net, dev)\n\t\tif (dev->type == type &&\n\t\t    !memcmp(dev->dev_addr, ha, dev->addr_len))\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(dev_getbyhwaddr_rcu);\n\nstruct net_device *dev_getfirstbyhwtype(struct net *net, unsigned short type)\n{\n\tstruct net_device *dev, *ret = NULL;\n\n\trcu_read_lock();\n\tfor_each_netdev_rcu(net, dev)\n\t\tif (dev->type == type) {\n\t\t\tdev_hold(dev);\n\t\t\tret = dev;\n\t\t\tbreak;\n\t\t}\n\trcu_read_unlock();\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_getfirstbyhwtype);\n\n \n\nstruct net_device *__dev_get_by_flags(struct net *net, unsigned short if_flags,\n\t\t\t\t      unsigned short mask)\n{\n\tstruct net_device *dev, *ret;\n\n\tASSERT_RTNL();\n\n\tret = NULL;\n\tfor_each_netdev(net, dev) {\n\t\tif (((dev->flags ^ if_flags) & mask) == 0) {\n\t\t\tret = dev;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL(__dev_get_by_flags);\n\n \nbool dev_valid_name(const char *name)\n{\n\tif (*name == '\\0')\n\t\treturn false;\n\tif (strnlen(name, IFNAMSIZ) == IFNAMSIZ)\n\t\treturn false;\n\tif (!strcmp(name, \".\") || !strcmp(name, \"..\"))\n\t\treturn false;\n\n\twhile (*name) {\n\t\tif (*name == '/' || *name == ':' || isspace(*name))\n\t\t\treturn false;\n\t\tname++;\n\t}\n\treturn true;\n}\nEXPORT_SYMBOL(dev_valid_name);\n\n \n\nstatic int __dev_alloc_name(struct net *net, const char *name, char *buf)\n{\n\tint i = 0;\n\tconst char *p;\n\tconst int max_netdevices = 8*PAGE_SIZE;\n\tunsigned long *inuse;\n\tstruct net_device *d;\n\n\tif (!dev_valid_name(name))\n\t\treturn -EINVAL;\n\n\tp = strchr(name, '%');\n\tif (p) {\n\t\t \n\t\tif (p[1] != 'd' || strchr(p + 2, '%'))\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tinuse = bitmap_zalloc(max_netdevices, GFP_ATOMIC);\n\t\tif (!inuse)\n\t\t\treturn -ENOMEM;\n\n\t\tfor_each_netdev(net, d) {\n\t\t\tstruct netdev_name_node *name_node;\n\n\t\t\tnetdev_for_each_altname(d, name_node) {\n\t\t\t\tif (!sscanf(name_node->name, name, &i))\n\t\t\t\t\tcontinue;\n\t\t\t\tif (i < 0 || i >= max_netdevices)\n\t\t\t\t\tcontinue;\n\n\t\t\t\t \n\t\t\t\tsnprintf(buf, IFNAMSIZ, name, i);\n\t\t\t\tif (!strncmp(buf, name_node->name, IFNAMSIZ))\n\t\t\t\t\t__set_bit(i, inuse);\n\t\t\t}\n\t\t\tif (!sscanf(d->name, name, &i))\n\t\t\t\tcontinue;\n\t\t\tif (i < 0 || i >= max_netdevices)\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tsnprintf(buf, IFNAMSIZ, name, i);\n\t\t\tif (!strncmp(buf, d->name, IFNAMSIZ))\n\t\t\t\t__set_bit(i, inuse);\n\t\t}\n\n\t\ti = find_first_zero_bit(inuse, max_netdevices);\n\t\tbitmap_free(inuse);\n\t}\n\n\tsnprintf(buf, IFNAMSIZ, name, i);\n\tif (!netdev_name_in_use(net, buf))\n\t\treturn i;\n\n\t \n\treturn -ENFILE;\n}\n\nstatic int dev_prep_valid_name(struct net *net, struct net_device *dev,\n\t\t\t       const char *want_name, char *out_name)\n{\n\tint ret;\n\n\tif (!dev_valid_name(want_name))\n\t\treturn -EINVAL;\n\n\tif (strchr(want_name, '%')) {\n\t\tret = __dev_alloc_name(net, want_name, out_name);\n\t\treturn ret < 0 ? ret : 0;\n\t} else if (netdev_name_in_use(net, want_name)) {\n\t\treturn -EEXIST;\n\t} else if (out_name != want_name) {\n\t\tstrscpy(out_name, want_name, IFNAMSIZ);\n\t}\n\n\treturn 0;\n}\n\nstatic int dev_alloc_name_ns(struct net *net,\n\t\t\t     struct net_device *dev,\n\t\t\t     const char *name)\n{\n\tchar buf[IFNAMSIZ];\n\tint ret;\n\n\tBUG_ON(!net);\n\tret = __dev_alloc_name(net, name, buf);\n\tif (ret >= 0)\n\t\tstrscpy(dev->name, buf, IFNAMSIZ);\n\treturn ret;\n}\n\n \n\nint dev_alloc_name(struct net_device *dev, const char *name)\n{\n\treturn dev_alloc_name_ns(dev_net(dev), dev, name);\n}\nEXPORT_SYMBOL(dev_alloc_name);\n\nstatic int dev_get_valid_name(struct net *net, struct net_device *dev,\n\t\t\t      const char *name)\n{\n\tchar buf[IFNAMSIZ];\n\tint ret;\n\n\tret = dev_prep_valid_name(net, dev, name, buf);\n\tif (ret >= 0)\n\t\tstrscpy(dev->name, buf, IFNAMSIZ);\n\treturn ret;\n}\n\n \nint dev_change_name(struct net_device *dev, const char *newname)\n{\n\tunsigned char old_assign_type;\n\tchar oldname[IFNAMSIZ];\n\tint err = 0;\n\tint ret;\n\tstruct net *net;\n\n\tASSERT_RTNL();\n\tBUG_ON(!dev_net(dev));\n\n\tnet = dev_net(dev);\n\n\tdown_write(&devnet_rename_sem);\n\n\tif (strncmp(newname, dev->name, IFNAMSIZ) == 0) {\n\t\tup_write(&devnet_rename_sem);\n\t\treturn 0;\n\t}\n\n\tmemcpy(oldname, dev->name, IFNAMSIZ);\n\n\terr = dev_get_valid_name(net, dev, newname);\n\tif (err < 0) {\n\t\tup_write(&devnet_rename_sem);\n\t\treturn err;\n\t}\n\n\tif (oldname[0] && !strchr(oldname, '%'))\n\t\tnetdev_info(dev, \"renamed from %s%s\\n\", oldname,\n\t\t\t    dev->flags & IFF_UP ? \" (while UP)\" : \"\");\n\n\told_assign_type = dev->name_assign_type;\n\tdev->name_assign_type = NET_NAME_RENAMED;\n\nrollback:\n\tret = device_rename(&dev->dev, dev->name);\n\tif (ret) {\n\t\tmemcpy(dev->name, oldname, IFNAMSIZ);\n\t\tdev->name_assign_type = old_assign_type;\n\t\tup_write(&devnet_rename_sem);\n\t\treturn ret;\n\t}\n\n\tup_write(&devnet_rename_sem);\n\n\tnetdev_adjacent_rename_links(dev, oldname);\n\n\twrite_lock(&dev_base_lock);\n\tnetdev_name_node_del(dev->name_node);\n\twrite_unlock(&dev_base_lock);\n\n\tsynchronize_rcu();\n\n\twrite_lock(&dev_base_lock);\n\tnetdev_name_node_add(net, dev->name_node);\n\twrite_unlock(&dev_base_lock);\n\n\tret = call_netdevice_notifiers(NETDEV_CHANGENAME, dev);\n\tret = notifier_to_errno(ret);\n\n\tif (ret) {\n\t\t \n\t\tif (err >= 0) {\n\t\t\terr = ret;\n\t\t\tdown_write(&devnet_rename_sem);\n\t\t\tmemcpy(dev->name, oldname, IFNAMSIZ);\n\t\t\tmemcpy(oldname, newname, IFNAMSIZ);\n\t\t\tdev->name_assign_type = old_assign_type;\n\t\t\told_assign_type = NET_NAME_RENAMED;\n\t\t\tgoto rollback;\n\t\t} else {\n\t\t\tnetdev_err(dev, \"name change rollback failed: %d\\n\",\n\t\t\t\t   ret);\n\t\t}\n\t}\n\n\treturn err;\n}\n\n \nint dev_set_alias(struct net_device *dev, const char *alias, size_t len)\n{\n\tstruct dev_ifalias *new_alias = NULL;\n\n\tif (len >= IFALIASZ)\n\t\treturn -EINVAL;\n\n\tif (len) {\n\t\tnew_alias = kmalloc(sizeof(*new_alias) + len + 1, GFP_KERNEL);\n\t\tif (!new_alias)\n\t\t\treturn -ENOMEM;\n\n\t\tmemcpy(new_alias->ifalias, alias, len);\n\t\tnew_alias->ifalias[len] = 0;\n\t}\n\n\tmutex_lock(&ifalias_mutex);\n\tnew_alias = rcu_replace_pointer(dev->ifalias, new_alias,\n\t\t\t\t\tmutex_is_locked(&ifalias_mutex));\n\tmutex_unlock(&ifalias_mutex);\n\n\tif (new_alias)\n\t\tkfree_rcu(new_alias, rcuhead);\n\n\treturn len;\n}\nEXPORT_SYMBOL(dev_set_alias);\n\n \nint dev_get_alias(const struct net_device *dev, char *name, size_t len)\n{\n\tconst struct dev_ifalias *alias;\n\tint ret = 0;\n\n\trcu_read_lock();\n\talias = rcu_dereference(dev->ifalias);\n\tif (alias)\n\t\tret = snprintf(name, len, \"%s\", alias->ifalias);\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\n \nvoid netdev_features_change(struct net_device *dev)\n{\n\tcall_netdevice_notifiers(NETDEV_FEAT_CHANGE, dev);\n}\nEXPORT_SYMBOL(netdev_features_change);\n\n \nvoid netdev_state_change(struct net_device *dev)\n{\n\tif (dev->flags & IFF_UP) {\n\t\tstruct netdev_notifier_change_info change_info = {\n\t\t\t.info.dev = dev,\n\t\t};\n\n\t\tcall_netdevice_notifiers_info(NETDEV_CHANGE,\n\t\t\t\t\t      &change_info.info);\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, 0, GFP_KERNEL, 0, NULL);\n\t}\n}\nEXPORT_SYMBOL(netdev_state_change);\n\n \nvoid __netdev_notify_peers(struct net_device *dev)\n{\n\tASSERT_RTNL();\n\tcall_netdevice_notifiers(NETDEV_NOTIFY_PEERS, dev);\n\tcall_netdevice_notifiers(NETDEV_RESEND_IGMP, dev);\n}\nEXPORT_SYMBOL(__netdev_notify_peers);\n\n \nvoid netdev_notify_peers(struct net_device *dev)\n{\n\trtnl_lock();\n\t__netdev_notify_peers(dev);\n\trtnl_unlock();\n}\nEXPORT_SYMBOL(netdev_notify_peers);\n\nstatic int napi_threaded_poll(void *data);\n\nstatic int napi_kthread_create(struct napi_struct *n)\n{\n\tint err = 0;\n\n\t \n\tn->thread = kthread_run(napi_threaded_poll, n, \"napi/%s-%d\",\n\t\t\t\tn->dev->name, n->napi_id);\n\tif (IS_ERR(n->thread)) {\n\t\terr = PTR_ERR(n->thread);\n\t\tpr_err(\"kthread_run failed with err %d\\n\", err);\n\t\tn->thread = NULL;\n\t}\n\n\treturn err;\n}\n\nstatic int __dev_open(struct net_device *dev, struct netlink_ext_ack *extack)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tint ret;\n\n\tASSERT_RTNL();\n\tdev_addr_check(dev);\n\n\tif (!netif_device_present(dev)) {\n\t\t \n\t\tif (dev->dev.parent)\n\t\t\tpm_runtime_resume(dev->dev.parent);\n\t\tif (!netif_device_present(dev))\n\t\t\treturn -ENODEV;\n\t}\n\n\t \n\tnetpoll_poll_disable(dev);\n\n\tret = call_netdevice_notifiers_extack(NETDEV_PRE_UP, dev, extack);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\treturn ret;\n\n\tset_bit(__LINK_STATE_START, &dev->state);\n\n\tif (ops->ndo_validate_addr)\n\t\tret = ops->ndo_validate_addr(dev);\n\n\tif (!ret && ops->ndo_open)\n\t\tret = ops->ndo_open(dev);\n\n\tnetpoll_poll_enable(dev);\n\n\tif (ret)\n\t\tclear_bit(__LINK_STATE_START, &dev->state);\n\telse {\n\t\tdev->flags |= IFF_UP;\n\t\tdev_set_rx_mode(dev);\n\t\tdev_activate(dev);\n\t\tadd_device_randomness(dev->dev_addr, dev->addr_len);\n\t}\n\n\treturn ret;\n}\n\n \nint dev_open(struct net_device *dev, struct netlink_ext_ack *extack)\n{\n\tint ret;\n\n\tif (dev->flags & IFF_UP)\n\t\treturn 0;\n\n\tret = __dev_open(dev, extack);\n\tif (ret < 0)\n\t\treturn ret;\n\n\trtmsg_ifinfo(RTM_NEWLINK, dev, IFF_UP | IFF_RUNNING, GFP_KERNEL, 0, NULL);\n\tcall_netdevice_notifiers(NETDEV_UP, dev);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_open);\n\nstatic void __dev_close_many(struct list_head *head)\n{\n\tstruct net_device *dev;\n\n\tASSERT_RTNL();\n\tmight_sleep();\n\n\tlist_for_each_entry(dev, head, close_list) {\n\t\t \n\t\tnetpoll_poll_disable(dev);\n\n\t\tcall_netdevice_notifiers(NETDEV_GOING_DOWN, dev);\n\n\t\tclear_bit(__LINK_STATE_START, &dev->state);\n\n\t\t \n\t\tsmp_mb__after_atomic();  \n\t}\n\n\tdev_deactivate_many(head);\n\n\tlist_for_each_entry(dev, head, close_list) {\n\t\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\t\t \n\t\tif (ops->ndo_stop)\n\t\t\tops->ndo_stop(dev);\n\n\t\tdev->flags &= ~IFF_UP;\n\t\tnetpoll_poll_enable(dev);\n\t}\n}\n\nstatic void __dev_close(struct net_device *dev)\n{\n\tLIST_HEAD(single);\n\n\tlist_add(&dev->close_list, &single);\n\t__dev_close_many(&single);\n\tlist_del(&single);\n}\n\nvoid dev_close_many(struct list_head *head, bool unlink)\n{\n\tstruct net_device *dev, *tmp;\n\n\t \n\tlist_for_each_entry_safe(dev, tmp, head, close_list)\n\t\tif (!(dev->flags & IFF_UP))\n\t\t\tlist_del_init(&dev->close_list);\n\n\t__dev_close_many(head);\n\n\tlist_for_each_entry_safe(dev, tmp, head, close_list) {\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, IFF_UP | IFF_RUNNING, GFP_KERNEL, 0, NULL);\n\t\tcall_netdevice_notifiers(NETDEV_DOWN, dev);\n\t\tif (unlink)\n\t\t\tlist_del_init(&dev->close_list);\n\t}\n}\nEXPORT_SYMBOL(dev_close_many);\n\n \nvoid dev_close(struct net_device *dev)\n{\n\tif (dev->flags & IFF_UP) {\n\t\tLIST_HEAD(single);\n\n\t\tlist_add(&dev->close_list, &single);\n\t\tdev_close_many(&single, true);\n\t\tlist_del(&single);\n\t}\n}\nEXPORT_SYMBOL(dev_close);\n\n\n \nvoid dev_disable_lro(struct net_device *dev)\n{\n\tstruct net_device *lower_dev;\n\tstruct list_head *iter;\n\n\tdev->wanted_features &= ~NETIF_F_LRO;\n\tnetdev_update_features(dev);\n\n\tif (unlikely(dev->features & NETIF_F_LRO))\n\t\tnetdev_WARN(dev, \"failed to disable LRO!\\n\");\n\n\tnetdev_for_each_lower_dev(dev, lower_dev, iter)\n\t\tdev_disable_lro(lower_dev);\n}\nEXPORT_SYMBOL(dev_disable_lro);\n\n \nstatic void dev_disable_gro_hw(struct net_device *dev)\n{\n\tdev->wanted_features &= ~NETIF_F_GRO_HW;\n\tnetdev_update_features(dev);\n\n\tif (unlikely(dev->features & NETIF_F_GRO_HW))\n\t\tnetdev_WARN(dev, \"failed to disable GRO_HW!\\n\");\n}\n\nconst char *netdev_cmd_to_name(enum netdev_cmd cmd)\n{\n#define N(val) \t\t\t\t\t\t\\\n\tcase NETDEV_##val:\t\t\t\t\\\n\t\treturn \"NETDEV_\" __stringify(val);\n\tswitch (cmd) {\n\tN(UP) N(DOWN) N(REBOOT) N(CHANGE) N(REGISTER) N(UNREGISTER)\n\tN(CHANGEMTU) N(CHANGEADDR) N(GOING_DOWN) N(CHANGENAME) N(FEAT_CHANGE)\n\tN(BONDING_FAILOVER) N(PRE_UP) N(PRE_TYPE_CHANGE) N(POST_TYPE_CHANGE)\n\tN(POST_INIT) N(PRE_UNINIT) N(RELEASE) N(NOTIFY_PEERS) N(JOIN)\n\tN(CHANGEUPPER) N(RESEND_IGMP) N(PRECHANGEMTU) N(CHANGEINFODATA)\n\tN(BONDING_INFO) N(PRECHANGEUPPER) N(CHANGELOWERSTATE)\n\tN(UDP_TUNNEL_PUSH_INFO) N(UDP_TUNNEL_DROP_INFO) N(CHANGE_TX_QUEUE_LEN)\n\tN(CVLAN_FILTER_PUSH_INFO) N(CVLAN_FILTER_DROP_INFO)\n\tN(SVLAN_FILTER_PUSH_INFO) N(SVLAN_FILTER_DROP_INFO)\n\tN(PRE_CHANGEADDR) N(OFFLOAD_XSTATS_ENABLE) N(OFFLOAD_XSTATS_DISABLE)\n\tN(OFFLOAD_XSTATS_REPORT_USED) N(OFFLOAD_XSTATS_REPORT_DELTA)\n\tN(XDP_FEAT_CHANGE)\n\t}\n#undef N\n\treturn \"UNKNOWN_NETDEV_EVENT\";\n}\nEXPORT_SYMBOL_GPL(netdev_cmd_to_name);\n\nstatic int call_netdevice_notifier(struct notifier_block *nb, unsigned long val,\n\t\t\t\t   struct net_device *dev)\n{\n\tstruct netdev_notifier_info info = {\n\t\t.dev = dev,\n\t};\n\n\treturn nb->notifier_call(nb, val, &info);\n}\n\nstatic int call_netdevice_register_notifiers(struct notifier_block *nb,\n\t\t\t\t\t     struct net_device *dev)\n{\n\tint err;\n\n\terr = call_netdevice_notifier(nb, NETDEV_REGISTER, dev);\n\terr = notifier_to_errno(err);\n\tif (err)\n\t\treturn err;\n\n\tif (!(dev->flags & IFF_UP))\n\t\treturn 0;\n\n\tcall_netdevice_notifier(nb, NETDEV_UP, dev);\n\treturn 0;\n}\n\nstatic void call_netdevice_unregister_notifiers(struct notifier_block *nb,\n\t\t\t\t\t\tstruct net_device *dev)\n{\n\tif (dev->flags & IFF_UP) {\n\t\tcall_netdevice_notifier(nb, NETDEV_GOING_DOWN,\n\t\t\t\t\tdev);\n\t\tcall_netdevice_notifier(nb, NETDEV_DOWN, dev);\n\t}\n\tcall_netdevice_notifier(nb, NETDEV_UNREGISTER, dev);\n}\n\nstatic int call_netdevice_register_net_notifiers(struct notifier_block *nb,\n\t\t\t\t\t\t struct net *net)\n{\n\tstruct net_device *dev;\n\tint err;\n\n\tfor_each_netdev(net, dev) {\n\t\terr = call_netdevice_register_notifiers(nb, dev);\n\t\tif (err)\n\t\t\tgoto rollback;\n\t}\n\treturn 0;\n\nrollback:\n\tfor_each_netdev_continue_reverse(net, dev)\n\t\tcall_netdevice_unregister_notifiers(nb, dev);\n\treturn err;\n}\n\nstatic void call_netdevice_unregister_net_notifiers(struct notifier_block *nb,\n\t\t\t\t\t\t    struct net *net)\n{\n\tstruct net_device *dev;\n\n\tfor_each_netdev(net, dev)\n\t\tcall_netdevice_unregister_notifiers(nb, dev);\n}\n\nstatic int dev_boot_phase = 1;\n\n \n\nint register_netdevice_notifier(struct notifier_block *nb)\n{\n\tstruct net *net;\n\tint err;\n\n\t \n\tdown_write(&pernet_ops_rwsem);\n\trtnl_lock();\n\terr = raw_notifier_chain_register(&netdev_chain, nb);\n\tif (err)\n\t\tgoto unlock;\n\tif (dev_boot_phase)\n\t\tgoto unlock;\n\tfor_each_net(net) {\n\t\terr = call_netdevice_register_net_notifiers(nb, net);\n\t\tif (err)\n\t\t\tgoto rollback;\n\t}\n\nunlock:\n\trtnl_unlock();\n\tup_write(&pernet_ops_rwsem);\n\treturn err;\n\nrollback:\n\tfor_each_net_continue_reverse(net)\n\t\tcall_netdevice_unregister_net_notifiers(nb, net);\n\n\traw_notifier_chain_unregister(&netdev_chain, nb);\n\tgoto unlock;\n}\nEXPORT_SYMBOL(register_netdevice_notifier);\n\n \n\nint unregister_netdevice_notifier(struct notifier_block *nb)\n{\n\tstruct net *net;\n\tint err;\n\n\t \n\tdown_write(&pernet_ops_rwsem);\n\trtnl_lock();\n\terr = raw_notifier_chain_unregister(&netdev_chain, nb);\n\tif (err)\n\t\tgoto unlock;\n\n\tfor_each_net(net)\n\t\tcall_netdevice_unregister_net_notifiers(nb, net);\n\nunlock:\n\trtnl_unlock();\n\tup_write(&pernet_ops_rwsem);\n\treturn err;\n}\nEXPORT_SYMBOL(unregister_netdevice_notifier);\n\nstatic int __register_netdevice_notifier_net(struct net *net,\n\t\t\t\t\t     struct notifier_block *nb,\n\t\t\t\t\t     bool ignore_call_fail)\n{\n\tint err;\n\n\terr = raw_notifier_chain_register(&net->netdev_chain, nb);\n\tif (err)\n\t\treturn err;\n\tif (dev_boot_phase)\n\t\treturn 0;\n\n\terr = call_netdevice_register_net_notifiers(nb, net);\n\tif (err && !ignore_call_fail)\n\t\tgoto chain_unregister;\n\n\treturn 0;\n\nchain_unregister:\n\traw_notifier_chain_unregister(&net->netdev_chain, nb);\n\treturn err;\n}\n\nstatic int __unregister_netdevice_notifier_net(struct net *net,\n\t\t\t\t\t       struct notifier_block *nb)\n{\n\tint err;\n\n\terr = raw_notifier_chain_unregister(&net->netdev_chain, nb);\n\tif (err)\n\t\treturn err;\n\n\tcall_netdevice_unregister_net_notifiers(nb, net);\n\treturn 0;\n}\n\n \n\nint register_netdevice_notifier_net(struct net *net, struct notifier_block *nb)\n{\n\tint err;\n\n\trtnl_lock();\n\terr = __register_netdevice_notifier_net(net, nb, false);\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(register_netdevice_notifier_net);\n\n \n\nint unregister_netdevice_notifier_net(struct net *net,\n\t\t\t\t      struct notifier_block *nb)\n{\n\tint err;\n\n\trtnl_lock();\n\terr = __unregister_netdevice_notifier_net(net, nb);\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(unregister_netdevice_notifier_net);\n\nstatic void __move_netdevice_notifier_net(struct net *src_net,\n\t\t\t\t\t  struct net *dst_net,\n\t\t\t\t\t  struct notifier_block *nb)\n{\n\t__unregister_netdevice_notifier_net(src_net, nb);\n\t__register_netdevice_notifier_net(dst_net, nb, true);\n}\n\nint register_netdevice_notifier_dev_net(struct net_device *dev,\n\t\t\t\t\tstruct notifier_block *nb,\n\t\t\t\t\tstruct netdev_net_notifier *nn)\n{\n\tint err;\n\n\trtnl_lock();\n\terr = __register_netdevice_notifier_net(dev_net(dev), nb, false);\n\tif (!err) {\n\t\tnn->nb = nb;\n\t\tlist_add(&nn->list, &dev->net_notifier_list);\n\t}\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(register_netdevice_notifier_dev_net);\n\nint unregister_netdevice_notifier_dev_net(struct net_device *dev,\n\t\t\t\t\t  struct notifier_block *nb,\n\t\t\t\t\t  struct netdev_net_notifier *nn)\n{\n\tint err;\n\n\trtnl_lock();\n\tlist_del(&nn->list);\n\terr = __unregister_netdevice_notifier_net(dev_net(dev), nb);\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(unregister_netdevice_notifier_dev_net);\n\nstatic void move_netdevice_notifiers_dev_net(struct net_device *dev,\n\t\t\t\t\t     struct net *net)\n{\n\tstruct netdev_net_notifier *nn;\n\n\tlist_for_each_entry(nn, &dev->net_notifier_list, list)\n\t\t__move_netdevice_notifier_net(dev_net(dev), net, nn->nb);\n}\n\n \n\nint call_netdevice_notifiers_info(unsigned long val,\n\t\t\t\t  struct netdev_notifier_info *info)\n{\n\tstruct net *net = dev_net(info->dev);\n\tint ret;\n\n\tASSERT_RTNL();\n\n\t \n\tret = raw_notifier_call_chain(&net->netdev_chain, val, info);\n\tif (ret & NOTIFY_STOP_MASK)\n\t\treturn ret;\n\treturn raw_notifier_call_chain(&netdev_chain, val, info);\n}\n\n \n\nstatic int\ncall_netdevice_notifiers_info_robust(unsigned long val_up,\n\t\t\t\t     unsigned long val_down,\n\t\t\t\t     struct netdev_notifier_info *info)\n{\n\tstruct net *net = dev_net(info->dev);\n\n\tASSERT_RTNL();\n\n\treturn raw_notifier_call_chain_robust(&net->netdev_chain,\n\t\t\t\t\t      val_up, val_down, info);\n}\n\nstatic int call_netdevice_notifiers_extack(unsigned long val,\n\t\t\t\t\t   struct net_device *dev,\n\t\t\t\t\t   struct netlink_ext_ack *extack)\n{\n\tstruct netdev_notifier_info info = {\n\t\t.dev = dev,\n\t\t.extack = extack,\n\t};\n\n\treturn call_netdevice_notifiers_info(val, &info);\n}\n\n \n\nint call_netdevice_notifiers(unsigned long val, struct net_device *dev)\n{\n\treturn call_netdevice_notifiers_extack(val, dev, NULL);\n}\nEXPORT_SYMBOL(call_netdevice_notifiers);\n\n \nstatic int call_netdevice_notifiers_mtu(unsigned long val,\n\t\t\t\t\tstruct net_device *dev, u32 arg)\n{\n\tstruct netdev_notifier_info_ext info = {\n\t\t.info.dev = dev,\n\t\t.ext.mtu = arg,\n\t};\n\n\tBUILD_BUG_ON(offsetof(struct netdev_notifier_info_ext, info) != 0);\n\n\treturn call_netdevice_notifiers_info(val, &info.info);\n}\n\n#ifdef CONFIG_NET_INGRESS\nstatic DEFINE_STATIC_KEY_FALSE(ingress_needed_key);\n\nvoid net_inc_ingress_queue(void)\n{\n\tstatic_branch_inc(&ingress_needed_key);\n}\nEXPORT_SYMBOL_GPL(net_inc_ingress_queue);\n\nvoid net_dec_ingress_queue(void)\n{\n\tstatic_branch_dec(&ingress_needed_key);\n}\nEXPORT_SYMBOL_GPL(net_dec_ingress_queue);\n#endif\n\n#ifdef CONFIG_NET_EGRESS\nstatic DEFINE_STATIC_KEY_FALSE(egress_needed_key);\n\nvoid net_inc_egress_queue(void)\n{\n\tstatic_branch_inc(&egress_needed_key);\n}\nEXPORT_SYMBOL_GPL(net_inc_egress_queue);\n\nvoid net_dec_egress_queue(void)\n{\n\tstatic_branch_dec(&egress_needed_key);\n}\nEXPORT_SYMBOL_GPL(net_dec_egress_queue);\n#endif\n\nDEFINE_STATIC_KEY_FALSE(netstamp_needed_key);\nEXPORT_SYMBOL(netstamp_needed_key);\n#ifdef CONFIG_JUMP_LABEL\nstatic atomic_t netstamp_needed_deferred;\nstatic atomic_t netstamp_wanted;\nstatic void netstamp_clear(struct work_struct *work)\n{\n\tint deferred = atomic_xchg(&netstamp_needed_deferred, 0);\n\tint wanted;\n\n\twanted = atomic_add_return(deferred, &netstamp_wanted);\n\tif (wanted > 0)\n\t\tstatic_branch_enable(&netstamp_needed_key);\n\telse\n\t\tstatic_branch_disable(&netstamp_needed_key);\n}\nstatic DECLARE_WORK(netstamp_work, netstamp_clear);\n#endif\n\nvoid net_enable_timestamp(void)\n{\n#ifdef CONFIG_JUMP_LABEL\n\tint wanted = atomic_read(&netstamp_wanted);\n\n\twhile (wanted > 0) {\n\t\tif (atomic_try_cmpxchg(&netstamp_wanted, &wanted, wanted + 1))\n\t\t\treturn;\n\t}\n\tatomic_inc(&netstamp_needed_deferred);\n\tschedule_work(&netstamp_work);\n#else\n\tstatic_branch_inc(&netstamp_needed_key);\n#endif\n}\nEXPORT_SYMBOL(net_enable_timestamp);\n\nvoid net_disable_timestamp(void)\n{\n#ifdef CONFIG_JUMP_LABEL\n\tint wanted = atomic_read(&netstamp_wanted);\n\n\twhile (wanted > 1) {\n\t\tif (atomic_try_cmpxchg(&netstamp_wanted, &wanted, wanted - 1))\n\t\t\treturn;\n\t}\n\tatomic_dec(&netstamp_needed_deferred);\n\tschedule_work(&netstamp_work);\n#else\n\tstatic_branch_dec(&netstamp_needed_key);\n#endif\n}\nEXPORT_SYMBOL(net_disable_timestamp);\n\nstatic inline void net_timestamp_set(struct sk_buff *skb)\n{\n\tskb->tstamp = 0;\n\tskb->mono_delivery_time = 0;\n\tif (static_branch_unlikely(&netstamp_needed_key))\n\t\tskb->tstamp = ktime_get_real();\n}\n\n#define net_timestamp_check(COND, SKB)\t\t\t\t\\\n\tif (static_branch_unlikely(&netstamp_needed_key)) {\t\\\n\t\tif ((COND) && !(SKB)->tstamp)\t\t\t\\\n\t\t\t(SKB)->tstamp = ktime_get_real();\t\\\n\t}\t\t\t\t\t\t\t\\\n\nbool is_skb_forwardable(const struct net_device *dev, const struct sk_buff *skb)\n{\n\treturn __is_skb_forwardable(dev, skb, true);\n}\nEXPORT_SYMBOL_GPL(is_skb_forwardable);\n\nstatic int __dev_forward_skb2(struct net_device *dev, struct sk_buff *skb,\n\t\t\t      bool check_mtu)\n{\n\tint ret = ____dev_forward_skb(dev, skb, check_mtu);\n\n\tif (likely(!ret)) {\n\t\tskb->protocol = eth_type_trans(skb, dev);\n\t\tskb_postpull_rcsum(skb, eth_hdr(skb), ETH_HLEN);\n\t}\n\n\treturn ret;\n}\n\nint __dev_forward_skb(struct net_device *dev, struct sk_buff *skb)\n{\n\treturn __dev_forward_skb2(dev, skb, true);\n}\nEXPORT_SYMBOL_GPL(__dev_forward_skb);\n\n \nint dev_forward_skb(struct net_device *dev, struct sk_buff *skb)\n{\n\treturn __dev_forward_skb(dev, skb) ?: netif_rx_internal(skb);\n}\nEXPORT_SYMBOL_GPL(dev_forward_skb);\n\nint dev_forward_skb_nomtu(struct net_device *dev, struct sk_buff *skb)\n{\n\treturn __dev_forward_skb2(dev, skb, false) ?: netif_rx_internal(skb);\n}\n\nstatic inline int deliver_skb(struct sk_buff *skb,\n\t\t\t      struct packet_type *pt_prev,\n\t\t\t      struct net_device *orig_dev)\n{\n\tif (unlikely(skb_orphan_frags_rx(skb, GFP_ATOMIC)))\n\t\treturn -ENOMEM;\n\trefcount_inc(&skb->users);\n\treturn pt_prev->func(skb, skb->dev, pt_prev, orig_dev);\n}\n\nstatic inline void deliver_ptype_list_skb(struct sk_buff *skb,\n\t\t\t\t\t  struct packet_type **pt,\n\t\t\t\t\t  struct net_device *orig_dev,\n\t\t\t\t\t  __be16 type,\n\t\t\t\t\t  struct list_head *ptype_list)\n{\n\tstruct packet_type *ptype, *pt_prev = *pt;\n\n\tlist_for_each_entry_rcu(ptype, ptype_list, list) {\n\t\tif (ptype->type != type)\n\t\t\tcontinue;\n\t\tif (pt_prev)\n\t\t\tdeliver_skb(skb, pt_prev, orig_dev);\n\t\tpt_prev = ptype;\n\t}\n\t*pt = pt_prev;\n}\n\nstatic inline bool skb_loop_sk(struct packet_type *ptype, struct sk_buff *skb)\n{\n\tif (!ptype->af_packet_priv || !skb->sk)\n\t\treturn false;\n\n\tif (ptype->id_match)\n\t\treturn ptype->id_match(ptype, skb->sk);\n\telse if ((struct sock *)ptype->af_packet_priv == skb->sk)\n\t\treturn true;\n\n\treturn false;\n}\n\n \nbool dev_nit_active(struct net_device *dev)\n{\n\treturn !list_empty(&ptype_all) || !list_empty(&dev->ptype_all);\n}\nEXPORT_SYMBOL_GPL(dev_nit_active);\n\n \n\nvoid dev_queue_xmit_nit(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct packet_type *ptype;\n\tstruct sk_buff *skb2 = NULL;\n\tstruct packet_type *pt_prev = NULL;\n\tstruct list_head *ptype_list = &ptype_all;\n\n\trcu_read_lock();\nagain:\n\tlist_for_each_entry_rcu(ptype, ptype_list, list) {\n\t\tif (ptype->ignore_outgoing)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (skb_loop_sk(ptype, skb))\n\t\t\tcontinue;\n\n\t\tif (pt_prev) {\n\t\t\tdeliver_skb(skb2, pt_prev, skb->dev);\n\t\t\tpt_prev = ptype;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tskb2 = skb_clone(skb, GFP_ATOMIC);\n\t\tif (!skb2)\n\t\t\tgoto out_unlock;\n\n\t\tnet_timestamp_set(skb2);\n\n\t\t \n\t\tskb_reset_mac_header(skb2);\n\n\t\tif (skb_network_header(skb2) < skb2->data ||\n\t\t    skb_network_header(skb2) > skb_tail_pointer(skb2)) {\n\t\t\tnet_crit_ratelimited(\"protocol %04x is buggy, dev %s\\n\",\n\t\t\t\t\t     ntohs(skb2->protocol),\n\t\t\t\t\t     dev->name);\n\t\t\tskb_reset_network_header(skb2);\n\t\t}\n\n\t\tskb2->transport_header = skb2->network_header;\n\t\tskb2->pkt_type = PACKET_OUTGOING;\n\t\tpt_prev = ptype;\n\t}\n\n\tif (ptype_list == &ptype_all) {\n\t\tptype_list = &dev->ptype_all;\n\t\tgoto again;\n\t}\nout_unlock:\n\tif (pt_prev) {\n\t\tif (!skb_orphan_frags_rx(skb2, GFP_ATOMIC))\n\t\t\tpt_prev->func(skb2, skb->dev, pt_prev, skb->dev);\n\t\telse\n\t\t\tkfree_skb(skb2);\n\t}\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL_GPL(dev_queue_xmit_nit);\n\n \nstatic void netif_setup_tc(struct net_device *dev, unsigned int txq)\n{\n\tint i;\n\tstruct netdev_tc_txq *tc = &dev->tc_to_txq[0];\n\n\t \n\tif (tc->offset + tc->count > txq) {\n\t\tnetdev_warn(dev, \"Number of in use tx queues changed invalidating tc mappings. Priority traffic classification disabled!\\n\");\n\t\tdev->num_tc = 0;\n\t\treturn;\n\t}\n\n\t \n\tfor (i = 1; i < TC_BITMASK + 1; i++) {\n\t\tint q = netdev_get_prio_tc_map(dev, i);\n\n\t\ttc = &dev->tc_to_txq[q];\n\t\tif (tc->offset + tc->count > txq) {\n\t\t\tnetdev_warn(dev, \"Number of in use tx queues changed. Priority %i to tc mapping %i is no longer valid. Setting map to 0\\n\",\n\t\t\t\t    i, q);\n\t\t\tnetdev_set_prio_tc_map(dev, i, 0);\n\t\t}\n\t}\n}\n\nint netdev_txq_to_tc(struct net_device *dev, unsigned int txq)\n{\n\tif (dev->num_tc) {\n\t\tstruct netdev_tc_txq *tc = &dev->tc_to_txq[0];\n\t\tint i;\n\n\t\t \n\t\tfor (i = 0; i < TC_MAX_QUEUE; i++, tc++) {\n\t\t\tif ((txq - tc->offset) < tc->count)\n\t\t\t\treturn i;\n\t\t}\n\n\t\t \n\t\treturn -1;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_txq_to_tc);\n\n#ifdef CONFIG_XPS\nstatic struct static_key xps_needed __read_mostly;\nstatic struct static_key xps_rxqs_needed __read_mostly;\nstatic DEFINE_MUTEX(xps_map_mutex);\n#define xmap_dereference(P)\t\t\\\n\trcu_dereference_protected((P), lockdep_is_held(&xps_map_mutex))\n\nstatic bool remove_xps_queue(struct xps_dev_maps *dev_maps,\n\t\t\t     struct xps_dev_maps *old_maps, int tci, u16 index)\n{\n\tstruct xps_map *map = NULL;\n\tint pos;\n\n\tmap = xmap_dereference(dev_maps->attr_map[tci]);\n\tif (!map)\n\t\treturn false;\n\n\tfor (pos = map->len; pos--;) {\n\t\tif (map->queues[pos] != index)\n\t\t\tcontinue;\n\n\t\tif (map->len > 1) {\n\t\t\tmap->queues[pos] = map->queues[--map->len];\n\t\t\tbreak;\n\t\t}\n\n\t\tif (old_maps)\n\t\t\tRCU_INIT_POINTER(old_maps->attr_map[tci], NULL);\n\t\tRCU_INIT_POINTER(dev_maps->attr_map[tci], NULL);\n\t\tkfree_rcu(map, rcu);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic bool remove_xps_queue_cpu(struct net_device *dev,\n\t\t\t\t struct xps_dev_maps *dev_maps,\n\t\t\t\t int cpu, u16 offset, u16 count)\n{\n\tint num_tc = dev_maps->num_tc;\n\tbool active = false;\n\tint tci;\n\n\tfor (tci = cpu * num_tc; num_tc--; tci++) {\n\t\tint i, j;\n\n\t\tfor (i = count, j = offset; i--; j++) {\n\t\t\tif (!remove_xps_queue(dev_maps, NULL, tci, j))\n\t\t\t\tbreak;\n\t\t}\n\n\t\tactive |= i < 0;\n\t}\n\n\treturn active;\n}\n\nstatic void reset_xps_maps(struct net_device *dev,\n\t\t\t   struct xps_dev_maps *dev_maps,\n\t\t\t   enum xps_map_type type)\n{\n\tstatic_key_slow_dec_cpuslocked(&xps_needed);\n\tif (type == XPS_RXQS)\n\t\tstatic_key_slow_dec_cpuslocked(&xps_rxqs_needed);\n\n\tRCU_INIT_POINTER(dev->xps_maps[type], NULL);\n\n\tkfree_rcu(dev_maps, rcu);\n}\n\nstatic void clean_xps_maps(struct net_device *dev, enum xps_map_type type,\n\t\t\t   u16 offset, u16 count)\n{\n\tstruct xps_dev_maps *dev_maps;\n\tbool active = false;\n\tint i, j;\n\n\tdev_maps = xmap_dereference(dev->xps_maps[type]);\n\tif (!dev_maps)\n\t\treturn;\n\n\tfor (j = 0; j < dev_maps->nr_ids; j++)\n\t\tactive |= remove_xps_queue_cpu(dev, dev_maps, j, offset, count);\n\tif (!active)\n\t\treset_xps_maps(dev, dev_maps, type);\n\n\tif (type == XPS_CPUS) {\n\t\tfor (i = offset + (count - 1); count--; i--)\n\t\t\tnetdev_queue_numa_node_write(\n\t\t\t\tnetdev_get_tx_queue(dev, i), NUMA_NO_NODE);\n\t}\n}\n\nstatic void netif_reset_xps_queues(struct net_device *dev, u16 offset,\n\t\t\t\t   u16 count)\n{\n\tif (!static_key_false(&xps_needed))\n\t\treturn;\n\n\tcpus_read_lock();\n\tmutex_lock(&xps_map_mutex);\n\n\tif (static_key_false(&xps_rxqs_needed))\n\t\tclean_xps_maps(dev, XPS_RXQS, offset, count);\n\n\tclean_xps_maps(dev, XPS_CPUS, offset, count);\n\n\tmutex_unlock(&xps_map_mutex);\n\tcpus_read_unlock();\n}\n\nstatic void netif_reset_xps_queues_gt(struct net_device *dev, u16 index)\n{\n\tnetif_reset_xps_queues(dev, index, dev->num_tx_queues - index);\n}\n\nstatic struct xps_map *expand_xps_map(struct xps_map *map, int attr_index,\n\t\t\t\t      u16 index, bool is_rxqs_map)\n{\n\tstruct xps_map *new_map;\n\tint alloc_len = XPS_MIN_MAP_ALLOC;\n\tint i, pos;\n\n\tfor (pos = 0; map && pos < map->len; pos++) {\n\t\tif (map->queues[pos] != index)\n\t\t\tcontinue;\n\t\treturn map;\n\t}\n\n\t \n\tif (map) {\n\t\tif (pos < map->alloc_len)\n\t\t\treturn map;\n\n\t\talloc_len = map->alloc_len * 2;\n\t}\n\n\t \n\tif (is_rxqs_map)\n\t\tnew_map = kzalloc(XPS_MAP_SIZE(alloc_len), GFP_KERNEL);\n\telse\n\t\tnew_map = kzalloc_node(XPS_MAP_SIZE(alloc_len), GFP_KERNEL,\n\t\t\t\t       cpu_to_node(attr_index));\n\tif (!new_map)\n\t\treturn NULL;\n\n\tfor (i = 0; i < pos; i++)\n\t\tnew_map->queues[i] = map->queues[i];\n\tnew_map->alloc_len = alloc_len;\n\tnew_map->len = pos;\n\n\treturn new_map;\n}\n\n \nstatic void xps_copy_dev_maps(struct xps_dev_maps *dev_maps,\n\t\t\t      struct xps_dev_maps *new_dev_maps, int index,\n\t\t\t      int tc, bool skip_tc)\n{\n\tint i, tci = index * dev_maps->num_tc;\n\tstruct xps_map *map;\n\n\t \n\tfor (i = 0; i < dev_maps->num_tc; i++, tci++) {\n\t\tif (i == tc && skip_tc)\n\t\t\tcontinue;\n\n\t\t \n\t\tmap = xmap_dereference(dev_maps->attr_map[tci]);\n\t\tRCU_INIT_POINTER(new_dev_maps->attr_map[tci], map);\n\t}\n}\n\n \nint __netif_set_xps_queue(struct net_device *dev, const unsigned long *mask,\n\t\t\t  u16 index, enum xps_map_type type)\n{\n\tstruct xps_dev_maps *dev_maps, *new_dev_maps = NULL, *old_dev_maps = NULL;\n\tconst unsigned long *online_mask = NULL;\n\tbool active = false, copy = false;\n\tint i, j, tci, numa_node_id = -2;\n\tint maps_sz, num_tc = 1, tc = 0;\n\tstruct xps_map *map, *new_map;\n\tunsigned int nr_ids;\n\n\tWARN_ON_ONCE(index >= dev->num_tx_queues);\n\n\tif (dev->num_tc) {\n\t\t \n\t\tnum_tc = dev->num_tc;\n\t\tif (num_tc < 0)\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tdev = netdev_get_tx_queue(dev, index)->sb_dev ? : dev;\n\n\t\ttc = netdev_txq_to_tc(dev, index);\n\t\tif (tc < 0)\n\t\t\treturn -EINVAL;\n\t}\n\n\tmutex_lock(&xps_map_mutex);\n\n\tdev_maps = xmap_dereference(dev->xps_maps[type]);\n\tif (type == XPS_RXQS) {\n\t\tmaps_sz = XPS_RXQ_DEV_MAPS_SIZE(num_tc, dev->num_rx_queues);\n\t\tnr_ids = dev->num_rx_queues;\n\t} else {\n\t\tmaps_sz = XPS_CPU_DEV_MAPS_SIZE(num_tc);\n\t\tif (num_possible_cpus() > 1)\n\t\t\tonline_mask = cpumask_bits(cpu_online_mask);\n\t\tnr_ids = nr_cpu_ids;\n\t}\n\n\tif (maps_sz < L1_CACHE_BYTES)\n\t\tmaps_sz = L1_CACHE_BYTES;\n\n\t \n\tif (dev_maps &&\n\t    dev_maps->num_tc == num_tc && dev_maps->nr_ids == nr_ids)\n\t\tcopy = true;\n\n\t \n\tfor (j = -1; j = netif_attrmask_next_and(j, online_mask, mask, nr_ids),\n\t     j < nr_ids;) {\n\t\tif (!new_dev_maps) {\n\t\t\tnew_dev_maps = kzalloc(maps_sz, GFP_KERNEL);\n\t\t\tif (!new_dev_maps) {\n\t\t\t\tmutex_unlock(&xps_map_mutex);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\n\t\t\tnew_dev_maps->nr_ids = nr_ids;\n\t\t\tnew_dev_maps->num_tc = num_tc;\n\t\t}\n\n\t\ttci = j * num_tc + tc;\n\t\tmap = copy ? xmap_dereference(dev_maps->attr_map[tci]) : NULL;\n\n\t\tmap = expand_xps_map(map, j, index, type == XPS_RXQS);\n\t\tif (!map)\n\t\t\tgoto error;\n\n\t\tRCU_INIT_POINTER(new_dev_maps->attr_map[tci], map);\n\t}\n\n\tif (!new_dev_maps)\n\t\tgoto out_no_new_maps;\n\n\tif (!dev_maps) {\n\t\t \n\t\tstatic_key_slow_inc_cpuslocked(&xps_needed);\n\t\tif (type == XPS_RXQS)\n\t\t\tstatic_key_slow_inc_cpuslocked(&xps_rxqs_needed);\n\t}\n\n\tfor (j = 0; j < nr_ids; j++) {\n\t\tbool skip_tc = false;\n\n\t\ttci = j * num_tc + tc;\n\t\tif (netif_attr_test_mask(j, mask, nr_ids) &&\n\t\t    netif_attr_test_online(j, online_mask, nr_ids)) {\n\t\t\t \n\t\t\tint pos = 0;\n\n\t\t\tskip_tc = true;\n\n\t\t\tmap = xmap_dereference(new_dev_maps->attr_map[tci]);\n\t\t\twhile ((pos < map->len) && (map->queues[pos] != index))\n\t\t\t\tpos++;\n\n\t\t\tif (pos == map->len)\n\t\t\t\tmap->queues[map->len++] = index;\n#ifdef CONFIG_NUMA\n\t\t\tif (type == XPS_CPUS) {\n\t\t\t\tif (numa_node_id == -2)\n\t\t\t\t\tnuma_node_id = cpu_to_node(j);\n\t\t\t\telse if (numa_node_id != cpu_to_node(j))\n\t\t\t\t\tnuma_node_id = -1;\n\t\t\t}\n#endif\n\t\t}\n\n\t\tif (copy)\n\t\t\txps_copy_dev_maps(dev_maps, new_dev_maps, j, tc,\n\t\t\t\t\t  skip_tc);\n\t}\n\n\trcu_assign_pointer(dev->xps_maps[type], new_dev_maps);\n\n\t \n\tif (!dev_maps)\n\t\tgoto out_no_old_maps;\n\n\tfor (j = 0; j < dev_maps->nr_ids; j++) {\n\t\tfor (i = num_tc, tci = j * dev_maps->num_tc; i--; tci++) {\n\t\t\tmap = xmap_dereference(dev_maps->attr_map[tci]);\n\t\t\tif (!map)\n\t\t\t\tcontinue;\n\n\t\t\tif (copy) {\n\t\t\t\tnew_map = xmap_dereference(new_dev_maps->attr_map[tci]);\n\t\t\t\tif (map == new_map)\n\t\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tRCU_INIT_POINTER(dev_maps->attr_map[tci], NULL);\n\t\t\tkfree_rcu(map, rcu);\n\t\t}\n\t}\n\n\told_dev_maps = dev_maps;\n\nout_no_old_maps:\n\tdev_maps = new_dev_maps;\n\tactive = true;\n\nout_no_new_maps:\n\tif (type == XPS_CPUS)\n\t\t \n\t\tnetdev_queue_numa_node_write(netdev_get_tx_queue(dev, index),\n\t\t\t\t\t     (numa_node_id >= 0) ?\n\t\t\t\t\t     numa_node_id : NUMA_NO_NODE);\n\n\tif (!dev_maps)\n\t\tgoto out_no_maps;\n\n\t \n\tfor (j = 0; j < dev_maps->nr_ids; j++) {\n\t\ttci = j * dev_maps->num_tc;\n\n\t\tfor (i = 0; i < dev_maps->num_tc; i++, tci++) {\n\t\t\tif (i == tc &&\n\t\t\t    netif_attr_test_mask(j, mask, dev_maps->nr_ids) &&\n\t\t\t    netif_attr_test_online(j, online_mask, dev_maps->nr_ids))\n\t\t\t\tcontinue;\n\n\t\t\tactive |= remove_xps_queue(dev_maps,\n\t\t\t\t\t\t   copy ? old_dev_maps : NULL,\n\t\t\t\t\t\t   tci, index);\n\t\t}\n\t}\n\n\tif (old_dev_maps)\n\t\tkfree_rcu(old_dev_maps, rcu);\n\n\t \n\tif (!active)\n\t\treset_xps_maps(dev, dev_maps, type);\n\nout_no_maps:\n\tmutex_unlock(&xps_map_mutex);\n\n\treturn 0;\nerror:\n\t \n\tfor (j = 0; j < nr_ids; j++) {\n\t\tfor (i = num_tc, tci = j * num_tc; i--; tci++) {\n\t\t\tnew_map = xmap_dereference(new_dev_maps->attr_map[tci]);\n\t\t\tmap = copy ?\n\t\t\t      xmap_dereference(dev_maps->attr_map[tci]) :\n\t\t\t      NULL;\n\t\t\tif (new_map && new_map != map)\n\t\t\t\tkfree(new_map);\n\t\t}\n\t}\n\n\tmutex_unlock(&xps_map_mutex);\n\n\tkfree(new_dev_maps);\n\treturn -ENOMEM;\n}\nEXPORT_SYMBOL_GPL(__netif_set_xps_queue);\n\nint netif_set_xps_queue(struct net_device *dev, const struct cpumask *mask,\n\t\t\tu16 index)\n{\n\tint ret;\n\n\tcpus_read_lock();\n\tret =  __netif_set_xps_queue(dev, cpumask_bits(mask), index, XPS_CPUS);\n\tcpus_read_unlock();\n\n\treturn ret;\n}\nEXPORT_SYMBOL(netif_set_xps_queue);\n\n#endif\nstatic void netdev_unbind_all_sb_channels(struct net_device *dev)\n{\n\tstruct netdev_queue *txq = &dev->_tx[dev->num_tx_queues];\n\n\t \n\twhile (txq-- != &dev->_tx[0]) {\n\t\tif (txq->sb_dev)\n\t\t\tnetdev_unbind_sb_channel(dev, txq->sb_dev);\n\t}\n}\n\nvoid netdev_reset_tc(struct net_device *dev)\n{\n#ifdef CONFIG_XPS\n\tnetif_reset_xps_queues_gt(dev, 0);\n#endif\n\tnetdev_unbind_all_sb_channels(dev);\n\n\t \n\tdev->num_tc = 0;\n\tmemset(dev->tc_to_txq, 0, sizeof(dev->tc_to_txq));\n\tmemset(dev->prio_tc_map, 0, sizeof(dev->prio_tc_map));\n}\nEXPORT_SYMBOL(netdev_reset_tc);\n\nint netdev_set_tc_queue(struct net_device *dev, u8 tc, u16 count, u16 offset)\n{\n\tif (tc >= dev->num_tc)\n\t\treturn -EINVAL;\n\n#ifdef CONFIG_XPS\n\tnetif_reset_xps_queues(dev, offset, count);\n#endif\n\tdev->tc_to_txq[tc].count = count;\n\tdev->tc_to_txq[tc].offset = offset;\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_set_tc_queue);\n\nint netdev_set_num_tc(struct net_device *dev, u8 num_tc)\n{\n\tif (num_tc > TC_MAX_QUEUE)\n\t\treturn -EINVAL;\n\n#ifdef CONFIG_XPS\n\tnetif_reset_xps_queues_gt(dev, 0);\n#endif\n\tnetdev_unbind_all_sb_channels(dev);\n\n\tdev->num_tc = num_tc;\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_set_num_tc);\n\nvoid netdev_unbind_sb_channel(struct net_device *dev,\n\t\t\t      struct net_device *sb_dev)\n{\n\tstruct netdev_queue *txq = &dev->_tx[dev->num_tx_queues];\n\n#ifdef CONFIG_XPS\n\tnetif_reset_xps_queues_gt(sb_dev, 0);\n#endif\n\tmemset(sb_dev->tc_to_txq, 0, sizeof(sb_dev->tc_to_txq));\n\tmemset(sb_dev->prio_tc_map, 0, sizeof(sb_dev->prio_tc_map));\n\n\twhile (txq-- != &dev->_tx[0]) {\n\t\tif (txq->sb_dev == sb_dev)\n\t\t\ttxq->sb_dev = NULL;\n\t}\n}\nEXPORT_SYMBOL(netdev_unbind_sb_channel);\n\nint netdev_bind_sb_channel_queue(struct net_device *dev,\n\t\t\t\t struct net_device *sb_dev,\n\t\t\t\t u8 tc, u16 count, u16 offset)\n{\n\t \n\tif (sb_dev->num_tc >= 0 || tc >= dev->num_tc)\n\t\treturn -EINVAL;\n\n\t \n\tif ((offset + count) > dev->real_num_tx_queues)\n\t\treturn -EINVAL;\n\n\t \n\tsb_dev->tc_to_txq[tc].count = count;\n\tsb_dev->tc_to_txq[tc].offset = offset;\n\n\t \n\twhile (count--)\n\t\tnetdev_get_tx_queue(dev, count + offset)->sb_dev = sb_dev;\n\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_bind_sb_channel_queue);\n\nint netdev_set_sb_channel(struct net_device *dev, u16 channel)\n{\n\t \n\tif (netif_is_multiqueue(dev))\n\t\treturn -ENODEV;\n\n\t \n\tif (channel > S16_MAX)\n\t\treturn -EINVAL;\n\n\tdev->num_tc = -channel;\n\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_set_sb_channel);\n\n \nint netif_set_real_num_tx_queues(struct net_device *dev, unsigned int txq)\n{\n\tbool disabling;\n\tint rc;\n\n\tdisabling = txq < dev->real_num_tx_queues;\n\n\tif (txq < 1 || txq > dev->num_tx_queues)\n\t\treturn -EINVAL;\n\n\tif (dev->reg_state == NETREG_REGISTERED ||\n\t    dev->reg_state == NETREG_UNREGISTERING) {\n\t\tASSERT_RTNL();\n\n\t\trc = netdev_queue_update_kobjects(dev, dev->real_num_tx_queues,\n\t\t\t\t\t\t  txq);\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t\tif (dev->num_tc)\n\t\t\tnetif_setup_tc(dev, txq);\n\n\t\tdev_qdisc_change_real_num_tx(dev, txq);\n\n\t\tdev->real_num_tx_queues = txq;\n\n\t\tif (disabling) {\n\t\t\tsynchronize_net();\n\t\t\tqdisc_reset_all_tx_gt(dev, txq);\n#ifdef CONFIG_XPS\n\t\t\tnetif_reset_xps_queues_gt(dev, txq);\n#endif\n\t\t}\n\t} else {\n\t\tdev->real_num_tx_queues = txq;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(netif_set_real_num_tx_queues);\n\n#ifdef CONFIG_SYSFS\n \nint netif_set_real_num_rx_queues(struct net_device *dev, unsigned int rxq)\n{\n\tint rc;\n\n\tif (rxq < 1 || rxq > dev->num_rx_queues)\n\t\treturn -EINVAL;\n\n\tif (dev->reg_state == NETREG_REGISTERED) {\n\t\tASSERT_RTNL();\n\n\t\trc = net_rx_queue_update_kobjects(dev, dev->real_num_rx_queues,\n\t\t\t\t\t\t  rxq);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\n\tdev->real_num_rx_queues = rxq;\n\treturn 0;\n}\nEXPORT_SYMBOL(netif_set_real_num_rx_queues);\n#endif\n\n \nint netif_set_real_num_queues(struct net_device *dev,\n\t\t\t      unsigned int txq, unsigned int rxq)\n{\n\tunsigned int old_rxq = dev->real_num_rx_queues;\n\tint err;\n\n\tif (txq < 1 || txq > dev->num_tx_queues ||\n\t    rxq < 1 || rxq > dev->num_rx_queues)\n\t\treturn -EINVAL;\n\n\t \n\tif (rxq > dev->real_num_rx_queues) {\n\t\terr = netif_set_real_num_rx_queues(dev, rxq);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\tif (txq > dev->real_num_tx_queues) {\n\t\terr = netif_set_real_num_tx_queues(dev, txq);\n\t\tif (err)\n\t\t\tgoto undo_rx;\n\t}\n\tif (rxq < dev->real_num_rx_queues)\n\t\tWARN_ON(netif_set_real_num_rx_queues(dev, rxq));\n\tif (txq < dev->real_num_tx_queues)\n\t\tWARN_ON(netif_set_real_num_tx_queues(dev, txq));\n\n\treturn 0;\nundo_rx:\n\tWARN_ON(netif_set_real_num_rx_queues(dev, old_rxq));\n\treturn err;\n}\nEXPORT_SYMBOL(netif_set_real_num_queues);\n\n \nvoid netif_set_tso_max_size(struct net_device *dev, unsigned int size)\n{\n\tdev->tso_max_size = min(GSO_MAX_SIZE, size);\n\tif (size < READ_ONCE(dev->gso_max_size))\n\t\tnetif_set_gso_max_size(dev, size);\n\tif (size < READ_ONCE(dev->gso_ipv4_max_size))\n\t\tnetif_set_gso_ipv4_max_size(dev, size);\n}\nEXPORT_SYMBOL(netif_set_tso_max_size);\n\n \nvoid netif_set_tso_max_segs(struct net_device *dev, unsigned int segs)\n{\n\tdev->tso_max_segs = segs;\n\tif (segs < READ_ONCE(dev->gso_max_segs))\n\t\tnetif_set_gso_max_segs(dev, segs);\n}\nEXPORT_SYMBOL(netif_set_tso_max_segs);\n\n \nvoid netif_inherit_tso_max(struct net_device *to, const struct net_device *from)\n{\n\tnetif_set_tso_max_size(to, from->tso_max_size);\n\tnetif_set_tso_max_segs(to, from->tso_max_segs);\n}\nEXPORT_SYMBOL(netif_inherit_tso_max);\n\n \nint netif_get_num_default_rss_queues(void)\n{\n\tcpumask_var_t cpus;\n\tint cpu, count = 0;\n\n\tif (unlikely(is_kdump_kernel() || !zalloc_cpumask_var(&cpus, GFP_KERNEL)))\n\t\treturn 1;\n\n\tcpumask_copy(cpus, cpu_online_mask);\n\tfor_each_cpu(cpu, cpus) {\n\t\t++count;\n\t\tcpumask_andnot(cpus, cpus, topology_sibling_cpumask(cpu));\n\t}\n\tfree_cpumask_var(cpus);\n\n\treturn count > 2 ? DIV_ROUND_UP(count, 2) : count;\n}\nEXPORT_SYMBOL(netif_get_num_default_rss_queues);\n\nstatic void __netif_reschedule(struct Qdisc *q)\n{\n\tstruct softnet_data *sd;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tsd = this_cpu_ptr(&softnet_data);\n\tq->next_sched = NULL;\n\t*sd->output_queue_tailp = q;\n\tsd->output_queue_tailp = &q->next_sched;\n\traise_softirq_irqoff(NET_TX_SOFTIRQ);\n\tlocal_irq_restore(flags);\n}\n\nvoid __netif_schedule(struct Qdisc *q)\n{\n\tif (!test_and_set_bit(__QDISC_STATE_SCHED, &q->state))\n\t\t__netif_reschedule(q);\n}\nEXPORT_SYMBOL(__netif_schedule);\n\nstruct dev_kfree_skb_cb {\n\tenum skb_drop_reason reason;\n};\n\nstatic struct dev_kfree_skb_cb *get_kfree_skb_cb(const struct sk_buff *skb)\n{\n\treturn (struct dev_kfree_skb_cb *)skb->cb;\n}\n\nvoid netif_schedule_queue(struct netdev_queue *txq)\n{\n\trcu_read_lock();\n\tif (!netif_xmit_stopped(txq)) {\n\t\tstruct Qdisc *q = rcu_dereference(txq->qdisc);\n\n\t\t__netif_schedule(q);\n\t}\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL(netif_schedule_queue);\n\nvoid netif_tx_wake_queue(struct netdev_queue *dev_queue)\n{\n\tif (test_and_clear_bit(__QUEUE_STATE_DRV_XOFF, &dev_queue->state)) {\n\t\tstruct Qdisc *q;\n\n\t\trcu_read_lock();\n\t\tq = rcu_dereference(dev_queue->qdisc);\n\t\t__netif_schedule(q);\n\t\trcu_read_unlock();\n\t}\n}\nEXPORT_SYMBOL(netif_tx_wake_queue);\n\nvoid dev_kfree_skb_irq_reason(struct sk_buff *skb, enum skb_drop_reason reason)\n{\n\tunsigned long flags;\n\n\tif (unlikely(!skb))\n\t\treturn;\n\n\tif (likely(refcount_read(&skb->users) == 1)) {\n\t\tsmp_rmb();\n\t\trefcount_set(&skb->users, 0);\n\t} else if (likely(!refcount_dec_and_test(&skb->users))) {\n\t\treturn;\n\t}\n\tget_kfree_skb_cb(skb)->reason = reason;\n\tlocal_irq_save(flags);\n\tskb->next = __this_cpu_read(softnet_data.completion_queue);\n\t__this_cpu_write(softnet_data.completion_queue, skb);\n\traise_softirq_irqoff(NET_TX_SOFTIRQ);\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL(dev_kfree_skb_irq_reason);\n\nvoid dev_kfree_skb_any_reason(struct sk_buff *skb, enum skb_drop_reason reason)\n{\n\tif (in_hardirq() || irqs_disabled())\n\t\tdev_kfree_skb_irq_reason(skb, reason);\n\telse\n\t\tkfree_skb_reason(skb, reason);\n}\nEXPORT_SYMBOL(dev_kfree_skb_any_reason);\n\n\n \nvoid netif_device_detach(struct net_device *dev)\n{\n\tif (test_and_clear_bit(__LINK_STATE_PRESENT, &dev->state) &&\n\t    netif_running(dev)) {\n\t\tnetif_tx_stop_all_queues(dev);\n\t}\n}\nEXPORT_SYMBOL(netif_device_detach);\n\n \nvoid netif_device_attach(struct net_device *dev)\n{\n\tif (!test_and_set_bit(__LINK_STATE_PRESENT, &dev->state) &&\n\t    netif_running(dev)) {\n\t\tnetif_tx_wake_all_queues(dev);\n\t\t__netdev_watchdog_up(dev);\n\t}\n}\nEXPORT_SYMBOL(netif_device_attach);\n\n \nstatic u16 skb_tx_hash(const struct net_device *dev,\n\t\t       const struct net_device *sb_dev,\n\t\t       struct sk_buff *skb)\n{\n\tu32 hash;\n\tu16 qoffset = 0;\n\tu16 qcount = dev->real_num_tx_queues;\n\n\tif (dev->num_tc) {\n\t\tu8 tc = netdev_get_prio_tc_map(dev, skb->priority);\n\n\t\tqoffset = sb_dev->tc_to_txq[tc].offset;\n\t\tqcount = sb_dev->tc_to_txq[tc].count;\n\t\tif (unlikely(!qcount)) {\n\t\t\tnet_warn_ratelimited(\"%s: invalid qcount, qoffset %u for tc %u\\n\",\n\t\t\t\t\t     sb_dev->name, qoffset, tc);\n\t\t\tqoffset = 0;\n\t\t\tqcount = dev->real_num_tx_queues;\n\t\t}\n\t}\n\n\tif (skb_rx_queue_recorded(skb)) {\n\t\tDEBUG_NET_WARN_ON_ONCE(qcount == 0);\n\t\thash = skb_get_rx_queue(skb);\n\t\tif (hash >= qoffset)\n\t\t\thash -= qoffset;\n\t\twhile (unlikely(hash >= qcount))\n\t\t\thash -= qcount;\n\t\treturn hash + qoffset;\n\t}\n\n\treturn (u16) reciprocal_scale(skb_get_hash(skb), qcount) + qoffset;\n}\n\nvoid skb_warn_bad_offload(const struct sk_buff *skb)\n{\n\tstatic const netdev_features_t null_features;\n\tstruct net_device *dev = skb->dev;\n\tconst char *name = \"\";\n\n\tif (!net_ratelimit())\n\t\treturn;\n\n\tif (dev) {\n\t\tif (dev->dev.parent)\n\t\t\tname = dev_driver_string(dev->dev.parent);\n\t\telse\n\t\t\tname = netdev_name(dev);\n\t}\n\tskb_dump(KERN_WARNING, skb, false);\n\tWARN(1, \"%s: caps=(%pNF, %pNF)\\n\",\n\t     name, dev ? &dev->features : &null_features,\n\t     skb->sk ? &skb->sk->sk_route_caps : &null_features);\n}\n\n \nint skb_checksum_help(struct sk_buff *skb)\n{\n\t__wsum csum;\n\tint ret = 0, offset;\n\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tgoto out_set_summed;\n\n\tif (unlikely(skb_is_gso(skb))) {\n\t\tskb_warn_bad_offload(skb);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (skb_has_shared_frag(skb)) {\n\t\tret = __skb_linearize(skb);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\toffset = skb_checksum_start_offset(skb);\n\tret = -EINVAL;\n\tif (unlikely(offset >= skb_headlen(skb))) {\n\t\tDO_ONCE_LITE(skb_dump, KERN_ERR, skb, false);\n\t\tWARN_ONCE(true, \"offset (%d) >= skb_headlen() (%u)\\n\",\n\t\t\t  offset, skb_headlen(skb));\n\t\tgoto out;\n\t}\n\tcsum = skb_checksum(skb, offset, skb->len - offset, 0);\n\n\toffset += skb->csum_offset;\n\tif (unlikely(offset + sizeof(__sum16) > skb_headlen(skb))) {\n\t\tDO_ONCE_LITE(skb_dump, KERN_ERR, skb, false);\n\t\tWARN_ONCE(true, \"offset+2 (%zu) > skb_headlen() (%u)\\n\",\n\t\t\t  offset + sizeof(__sum16), skb_headlen(skb));\n\t\tgoto out;\n\t}\n\tret = skb_ensure_writable(skb, offset + sizeof(__sum16));\n\tif (ret)\n\t\tgoto out;\n\n\t*(__sum16 *)(skb->data + offset) = csum_fold(csum) ?: CSUM_MANGLED_0;\nout_set_summed:\n\tskb->ip_summed = CHECKSUM_NONE;\nout:\n\treturn ret;\n}\nEXPORT_SYMBOL(skb_checksum_help);\n\nint skb_crc32c_csum_help(struct sk_buff *skb)\n{\n\t__le32 crc32c_csum;\n\tint ret = 0, offset, start;\n\n\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\tgoto out;\n\n\tif (unlikely(skb_is_gso(skb)))\n\t\tgoto out;\n\n\t \n\tif (unlikely(skb_has_shared_frag(skb))) {\n\t\tret = __skb_linearize(skb);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\tstart = skb_checksum_start_offset(skb);\n\toffset = start + offsetof(struct sctphdr, checksum);\n\tif (WARN_ON_ONCE(offset >= skb_headlen(skb))) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tret = skb_ensure_writable(skb, offset + sizeof(__le32));\n\tif (ret)\n\t\tgoto out;\n\n\tcrc32c_csum = cpu_to_le32(~__skb_checksum(skb, start,\n\t\t\t\t\t\t  skb->len - start, ~(__u32)0,\n\t\t\t\t\t\t  crc32c_csum_stub));\n\t*(__le32 *)(skb->data + offset) = crc32c_csum;\n\tskb_reset_csum_not_inet(skb);\nout:\n\treturn ret;\n}\n\n__be16 skb_network_protocol(struct sk_buff *skb, int *depth)\n{\n\t__be16 type = skb->protocol;\n\n\t \n\tif (type == htons(ETH_P_TEB)) {\n\t\tstruct ethhdr *eth;\n\n\t\tif (unlikely(!pskb_may_pull(skb, sizeof(struct ethhdr))))\n\t\t\treturn 0;\n\n\t\teth = (struct ethhdr *)skb->data;\n\t\ttype = eth->h_proto;\n\t}\n\n\treturn vlan_get_protocol_and_depth(skb, type, depth);\n}\n\n\n \n#ifdef CONFIG_BUG\nstatic void do_netdev_rx_csum_fault(struct net_device *dev, struct sk_buff *skb)\n{\n\tnetdev_err(dev, \"hw csum failure\\n\");\n\tskb_dump(KERN_ERR, skb, true);\n\tdump_stack();\n}\n\nvoid netdev_rx_csum_fault(struct net_device *dev, struct sk_buff *skb)\n{\n\tDO_ONCE_LITE(do_netdev_rx_csum_fault, dev, skb);\n}\nEXPORT_SYMBOL(netdev_rx_csum_fault);\n#endif\n\n \nstatic int illegal_highdma(struct net_device *dev, struct sk_buff *skb)\n{\n#ifdef CONFIG_HIGHMEM\n\tint i;\n\n\tif (!(dev->features & NETIF_F_HIGHDMA)) {\n\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t\t\tif (PageHighMem(skb_frag_page(frag)))\n\t\t\t\treturn 1;\n\t\t}\n\t}\n#endif\n\treturn 0;\n}\n\n \n#if IS_ENABLED(CONFIG_NET_MPLS_GSO)\nstatic netdev_features_t net_mpls_features(struct sk_buff *skb,\n\t\t\t\t\t   netdev_features_t features,\n\t\t\t\t\t   __be16 type)\n{\n\tif (eth_p_mpls(type))\n\t\tfeatures &= skb->dev->mpls_features;\n\n\treturn features;\n}\n#else\nstatic netdev_features_t net_mpls_features(struct sk_buff *skb,\n\t\t\t\t\t   netdev_features_t features,\n\t\t\t\t\t   __be16 type)\n{\n\treturn features;\n}\n#endif\n\nstatic netdev_features_t harmonize_features(struct sk_buff *skb,\n\tnetdev_features_t features)\n{\n\t__be16 type;\n\n\ttype = skb_network_protocol(skb, NULL);\n\tfeatures = net_mpls_features(skb, features, type);\n\n\tif (skb->ip_summed != CHECKSUM_NONE &&\n\t    !can_checksum_protocol(features, type)) {\n\t\tfeatures &= ~(NETIF_F_CSUM_MASK | NETIF_F_GSO_MASK);\n\t}\n\tif (illegal_highdma(skb->dev, skb))\n\t\tfeatures &= ~NETIF_F_SG;\n\n\treturn features;\n}\n\nnetdev_features_t passthru_features_check(struct sk_buff *skb,\n\t\t\t\t\t  struct net_device *dev,\n\t\t\t\t\t  netdev_features_t features)\n{\n\treturn features;\n}\nEXPORT_SYMBOL(passthru_features_check);\n\nstatic netdev_features_t dflt_features_check(struct sk_buff *skb,\n\t\t\t\t\t     struct net_device *dev,\n\t\t\t\t\t     netdev_features_t features)\n{\n\treturn vlan_features_check(skb, features);\n}\n\nstatic netdev_features_t gso_features_check(const struct sk_buff *skb,\n\t\t\t\t\t    struct net_device *dev,\n\t\t\t\t\t    netdev_features_t features)\n{\n\tu16 gso_segs = skb_shinfo(skb)->gso_segs;\n\n\tif (gso_segs > READ_ONCE(dev->gso_max_segs))\n\t\treturn features & ~NETIF_F_GSO_MASK;\n\n\tif (unlikely(skb->len >= READ_ONCE(dev->gso_max_size)))\n\t\treturn features & ~NETIF_F_GSO_MASK;\n\n\tif (!skb_shinfo(skb)->gso_type) {\n\t\tskb_warn_bad_offload(skb);\n\t\treturn features & ~NETIF_F_GSO_MASK;\n\t}\n\n\t \n\tif (!(skb_shinfo(skb)->gso_type & SKB_GSO_PARTIAL))\n\t\tfeatures &= ~dev->gso_partial_features;\n\n\t \n\tif (skb_shinfo(skb)->gso_type & SKB_GSO_TCPV4) {\n\t\tstruct iphdr *iph = skb->encapsulation ?\n\t\t\t\t    inner_ip_hdr(skb) : ip_hdr(skb);\n\n\t\tif (!(iph->frag_off & htons(IP_DF)))\n\t\t\tfeatures &= ~NETIF_F_TSO_MANGLEID;\n\t}\n\n\treturn features;\n}\n\nnetdev_features_t netif_skb_features(struct sk_buff *skb)\n{\n\tstruct net_device *dev = skb->dev;\n\tnetdev_features_t features = dev->features;\n\n\tif (skb_is_gso(skb))\n\t\tfeatures = gso_features_check(skb, dev, features);\n\n\t \n\tif (skb->encapsulation)\n\t\tfeatures &= dev->hw_enc_features;\n\n\tif (skb_vlan_tagged(skb))\n\t\tfeatures = netdev_intersect_features(features,\n\t\t\t\t\t\t     dev->vlan_features |\n\t\t\t\t\t\t     NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\t\t\t\t     NETIF_F_HW_VLAN_STAG_TX);\n\n\tif (dev->netdev_ops->ndo_features_check)\n\t\tfeatures &= dev->netdev_ops->ndo_features_check(skb, dev,\n\t\t\t\t\t\t\t\tfeatures);\n\telse\n\t\tfeatures &= dflt_features_check(skb, dev, features);\n\n\treturn harmonize_features(skb, features);\n}\nEXPORT_SYMBOL(netif_skb_features);\n\nstatic int xmit_one(struct sk_buff *skb, struct net_device *dev,\n\t\t    struct netdev_queue *txq, bool more)\n{\n\tunsigned int len;\n\tint rc;\n\n\tif (dev_nit_active(dev))\n\t\tdev_queue_xmit_nit(skb, dev);\n\n\tlen = skb->len;\n\ttrace_net_dev_start_xmit(skb, dev);\n\trc = netdev_start_xmit(skb, dev, txq, more);\n\ttrace_net_dev_xmit(skb, rc, dev, len);\n\n\treturn rc;\n}\n\nstruct sk_buff *dev_hard_start_xmit(struct sk_buff *first, struct net_device *dev,\n\t\t\t\t    struct netdev_queue *txq, int *ret)\n{\n\tstruct sk_buff *skb = first;\n\tint rc = NETDEV_TX_OK;\n\n\twhile (skb) {\n\t\tstruct sk_buff *next = skb->next;\n\n\t\tskb_mark_not_on_list(skb);\n\t\trc = xmit_one(skb, dev, txq, next != NULL);\n\t\tif (unlikely(!dev_xmit_complete(rc))) {\n\t\t\tskb->next = next;\n\t\t\tgoto out;\n\t\t}\n\n\t\tskb = next;\n\t\tif (netif_tx_queue_stopped(txq) && skb) {\n\t\t\trc = NETDEV_TX_BUSY;\n\t\t\tbreak;\n\t\t}\n\t}\n\nout:\n\t*ret = rc;\n\treturn skb;\n}\n\nstatic struct sk_buff *validate_xmit_vlan(struct sk_buff *skb,\n\t\t\t\t\t  netdev_features_t features)\n{\n\tif (skb_vlan_tag_present(skb) &&\n\t    !vlan_hw_offload_capable(features, skb->vlan_proto))\n\t\tskb = __vlan_hwaccel_push_inside(skb);\n\treturn skb;\n}\n\nint skb_csum_hwoffload_help(struct sk_buff *skb,\n\t\t\t    const netdev_features_t features)\n{\n\tif (unlikely(skb_csum_is_sctp(skb)))\n\t\treturn !!(features & NETIF_F_SCTP_CRC) ? 0 :\n\t\t\tskb_crc32c_csum_help(skb);\n\n\tif (features & NETIF_F_HW_CSUM)\n\t\treturn 0;\n\n\tif (features & (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM)) {\n\t\tswitch (skb->csum_offset) {\n\t\tcase offsetof(struct tcphdr, check):\n\t\tcase offsetof(struct udphdr, check):\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn skb_checksum_help(skb);\n}\nEXPORT_SYMBOL(skb_csum_hwoffload_help);\n\nstatic struct sk_buff *validate_xmit_skb(struct sk_buff *skb, struct net_device *dev, bool *again)\n{\n\tnetdev_features_t features;\n\n\tfeatures = netif_skb_features(skb);\n\tskb = validate_xmit_vlan(skb, features);\n\tif (unlikely(!skb))\n\t\tgoto out_null;\n\n\tskb = sk_validate_xmit_skb(skb, dev);\n\tif (unlikely(!skb))\n\t\tgoto out_null;\n\n\tif (netif_needs_gso(skb, features)) {\n\t\tstruct sk_buff *segs;\n\n\t\tsegs = skb_gso_segment(skb, features);\n\t\tif (IS_ERR(segs)) {\n\t\t\tgoto out_kfree_skb;\n\t\t} else if (segs) {\n\t\t\tconsume_skb(skb);\n\t\t\tskb = segs;\n\t\t}\n\t} else {\n\t\tif (skb_needs_linearize(skb, features) &&\n\t\t    __skb_linearize(skb))\n\t\t\tgoto out_kfree_skb;\n\n\t\t \n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\t\tif (skb->encapsulation)\n\t\t\t\tskb_set_inner_transport_header(skb,\n\t\t\t\t\t\t\t       skb_checksum_start_offset(skb));\n\t\t\telse\n\t\t\t\tskb_set_transport_header(skb,\n\t\t\t\t\t\t\t skb_checksum_start_offset(skb));\n\t\t\tif (skb_csum_hwoffload_help(skb, features))\n\t\t\t\tgoto out_kfree_skb;\n\t\t}\n\t}\n\n\tskb = validate_xmit_xfrm(skb, features, again);\n\n\treturn skb;\n\nout_kfree_skb:\n\tkfree_skb(skb);\nout_null:\n\tdev_core_stats_tx_dropped_inc(dev);\n\treturn NULL;\n}\n\nstruct sk_buff *validate_xmit_skb_list(struct sk_buff *skb, struct net_device *dev, bool *again)\n{\n\tstruct sk_buff *next, *head = NULL, *tail;\n\n\tfor (; skb != NULL; skb = next) {\n\t\tnext = skb->next;\n\t\tskb_mark_not_on_list(skb);\n\n\t\t \n\t\tskb->prev = skb;\n\n\t\tskb = validate_xmit_skb(skb, dev, again);\n\t\tif (!skb)\n\t\t\tcontinue;\n\n\t\tif (!head)\n\t\t\thead = skb;\n\t\telse\n\t\t\ttail->next = skb;\n\t\t \n\t\ttail = skb->prev;\n\t}\n\treturn head;\n}\nEXPORT_SYMBOL_GPL(validate_xmit_skb_list);\n\nstatic void qdisc_pkt_len_init(struct sk_buff *skb)\n{\n\tconst struct skb_shared_info *shinfo = skb_shinfo(skb);\n\n\tqdisc_skb_cb(skb)->pkt_len = skb->len;\n\n\t \n\tif (shinfo->gso_size && skb_transport_header_was_set(skb)) {\n\t\tu16 gso_segs = shinfo->gso_segs;\n\t\tunsigned int hdr_len;\n\n\t\t \n\t\thdr_len = skb_transport_offset(skb);\n\n\t\t \n\t\tif (likely(shinfo->gso_type & (SKB_GSO_TCPV4 | SKB_GSO_TCPV6))) {\n\t\t\tconst struct tcphdr *th;\n\t\t\tstruct tcphdr _tcphdr;\n\n\t\t\tth = skb_header_pointer(skb, hdr_len,\n\t\t\t\t\t\tsizeof(_tcphdr), &_tcphdr);\n\t\t\tif (likely(th))\n\t\t\t\thdr_len += __tcp_hdrlen(th);\n\t\t} else {\n\t\t\tstruct udphdr _udphdr;\n\n\t\t\tif (skb_header_pointer(skb, hdr_len,\n\t\t\t\t\t       sizeof(_udphdr), &_udphdr))\n\t\t\t\thdr_len += sizeof(struct udphdr);\n\t\t}\n\n\t\tif (shinfo->gso_type & SKB_GSO_DODGY)\n\t\t\tgso_segs = DIV_ROUND_UP(skb->len - hdr_len,\n\t\t\t\t\t\tshinfo->gso_size);\n\n\t\tqdisc_skb_cb(skb)->pkt_len += (gso_segs - 1) * hdr_len;\n\t}\n}\n\nstatic int dev_qdisc_enqueue(struct sk_buff *skb, struct Qdisc *q,\n\t\t\t     struct sk_buff **to_free,\n\t\t\t     struct netdev_queue *txq)\n{\n\tint rc;\n\n\trc = q->enqueue(skb, q, to_free) & NET_XMIT_MASK;\n\tif (rc == NET_XMIT_SUCCESS)\n\t\ttrace_qdisc_enqueue(q, txq, skb);\n\treturn rc;\n}\n\nstatic inline int __dev_xmit_skb(struct sk_buff *skb, struct Qdisc *q,\n\t\t\t\t struct net_device *dev,\n\t\t\t\t struct netdev_queue *txq)\n{\n\tspinlock_t *root_lock = qdisc_lock(q);\n\tstruct sk_buff *to_free = NULL;\n\tbool contended;\n\tint rc;\n\n\tqdisc_calculate_pkt_len(skb, q);\n\n\tif (q->flags & TCQ_F_NOLOCK) {\n\t\tif (q->flags & TCQ_F_CAN_BYPASS && nolock_qdisc_is_empty(q) &&\n\t\t    qdisc_run_begin(q)) {\n\t\t\t \n\t\t\tif (unlikely(!nolock_qdisc_is_empty(q))) {\n\t\t\t\trc = dev_qdisc_enqueue(skb, q, &to_free, txq);\n\t\t\t\t__qdisc_run(q);\n\t\t\t\tqdisc_run_end(q);\n\n\t\t\t\tgoto no_lock_out;\n\t\t\t}\n\n\t\t\tqdisc_bstats_cpu_update(q, skb);\n\t\t\tif (sch_direct_xmit(skb, q, dev, txq, NULL, true) &&\n\t\t\t    !nolock_qdisc_is_empty(q))\n\t\t\t\t__qdisc_run(q);\n\n\t\t\tqdisc_run_end(q);\n\t\t\treturn NET_XMIT_SUCCESS;\n\t\t}\n\n\t\trc = dev_qdisc_enqueue(skb, q, &to_free, txq);\n\t\tqdisc_run(q);\n\nno_lock_out:\n\t\tif (unlikely(to_free))\n\t\t\tkfree_skb_list_reason(to_free,\n\t\t\t\t\t      SKB_DROP_REASON_QDISC_DROP);\n\t\treturn rc;\n\t}\n\n\t \n\tcontended = qdisc_is_running(q) || IS_ENABLED(CONFIG_PREEMPT_RT);\n\tif (unlikely(contended))\n\t\tspin_lock(&q->busylock);\n\n\tspin_lock(root_lock);\n\tif (unlikely(test_bit(__QDISC_STATE_DEACTIVATED, &q->state))) {\n\t\t__qdisc_drop(skb, &to_free);\n\t\trc = NET_XMIT_DROP;\n\t} else if ((q->flags & TCQ_F_CAN_BYPASS) && !qdisc_qlen(q) &&\n\t\t   qdisc_run_begin(q)) {\n\t\t \n\n\t\tqdisc_bstats_update(q, skb);\n\n\t\tif (sch_direct_xmit(skb, q, dev, txq, root_lock, true)) {\n\t\t\tif (unlikely(contended)) {\n\t\t\t\tspin_unlock(&q->busylock);\n\t\t\t\tcontended = false;\n\t\t\t}\n\t\t\t__qdisc_run(q);\n\t\t}\n\n\t\tqdisc_run_end(q);\n\t\trc = NET_XMIT_SUCCESS;\n\t} else {\n\t\trc = dev_qdisc_enqueue(skb, q, &to_free, txq);\n\t\tif (qdisc_run_begin(q)) {\n\t\t\tif (unlikely(contended)) {\n\t\t\t\tspin_unlock(&q->busylock);\n\t\t\t\tcontended = false;\n\t\t\t}\n\t\t\t__qdisc_run(q);\n\t\t\tqdisc_run_end(q);\n\t\t}\n\t}\n\tspin_unlock(root_lock);\n\tif (unlikely(to_free))\n\t\tkfree_skb_list_reason(to_free, SKB_DROP_REASON_QDISC_DROP);\n\tif (unlikely(contended))\n\t\tspin_unlock(&q->busylock);\n\treturn rc;\n}\n\n#if IS_ENABLED(CONFIG_CGROUP_NET_PRIO)\nstatic void skb_update_prio(struct sk_buff *skb)\n{\n\tconst struct netprio_map *map;\n\tconst struct sock *sk;\n\tunsigned int prioidx;\n\n\tif (skb->priority)\n\t\treturn;\n\tmap = rcu_dereference_bh(skb->dev->priomap);\n\tif (!map)\n\t\treturn;\n\tsk = skb_to_full_sk(skb);\n\tif (!sk)\n\t\treturn;\n\n\tprioidx = sock_cgroup_prioidx(&sk->sk_cgrp_data);\n\n\tif (prioidx < map->priomap_len)\n\t\tskb->priority = map->priomap[prioidx];\n}\n#else\n#define skb_update_prio(skb)\n#endif\n\n \nint dev_loopback_xmit(struct net *net, struct sock *sk, struct sk_buff *skb)\n{\n\tskb_reset_mac_header(skb);\n\t__skb_pull(skb, skb_network_offset(skb));\n\tskb->pkt_type = PACKET_LOOPBACK;\n\tif (skb->ip_summed == CHECKSUM_NONE)\n\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\tDEBUG_NET_WARN_ON_ONCE(!skb_dst(skb));\n\tskb_dst_force(skb);\n\tnetif_rx(skb);\n\treturn 0;\n}\nEXPORT_SYMBOL(dev_loopback_xmit);\n\n#ifdef CONFIG_NET_EGRESS\nstatic struct netdev_queue *\nnetdev_tx_queue_mapping(struct net_device *dev, struct sk_buff *skb)\n{\n\tint qm = skb_get_queue_mapping(skb);\n\n\treturn netdev_get_tx_queue(dev, netdev_cap_txqueue(dev, qm));\n}\n\nstatic bool netdev_xmit_txqueue_skipped(void)\n{\n\treturn __this_cpu_read(softnet_data.xmit.skip_txqueue);\n}\n\nvoid netdev_xmit_skip_txqueue(bool skip)\n{\n\t__this_cpu_write(softnet_data.xmit.skip_txqueue, skip);\n}\nEXPORT_SYMBOL_GPL(netdev_xmit_skip_txqueue);\n#endif  \n\n#ifdef CONFIG_NET_XGRESS\nstatic int tc_run(struct tcx_entry *entry, struct sk_buff *skb)\n{\n\tint ret = TC_ACT_UNSPEC;\n#ifdef CONFIG_NET_CLS_ACT\n\tstruct mini_Qdisc *miniq = rcu_dereference_bh(entry->miniq);\n\tstruct tcf_result res;\n\n\tif (!miniq)\n\t\treturn ret;\n\n\ttc_skb_cb(skb)->mru = 0;\n\ttc_skb_cb(skb)->post_ct = false;\n\n\tmini_qdisc_bstats_cpu_update(miniq, skb);\n\tret = tcf_classify(skb, miniq->block, miniq->filter_list, &res, false);\n\t \n\tswitch (ret) {\n\tcase TC_ACT_SHOT:\n\t\tmini_qdisc_qstats_cpu_drop(miniq);\n\t\tbreak;\n\tcase TC_ACT_OK:\n\tcase TC_ACT_RECLASSIFY:\n\t\tskb->tc_index = TC_H_MIN(res.classid);\n\t\tbreak;\n\t}\n#endif  \n\treturn ret;\n}\n\nstatic DEFINE_STATIC_KEY_FALSE(tcx_needed_key);\n\nvoid tcx_inc(void)\n{\n\tstatic_branch_inc(&tcx_needed_key);\n}\n\nvoid tcx_dec(void)\n{\n\tstatic_branch_dec(&tcx_needed_key);\n}\n\nstatic __always_inline enum tcx_action_base\ntcx_run(const struct bpf_mprog_entry *entry, struct sk_buff *skb,\n\tconst bool needs_mac)\n{\n\tconst struct bpf_mprog_fp *fp;\n\tconst struct bpf_prog *prog;\n\tint ret = TCX_NEXT;\n\n\tif (needs_mac)\n\t\t__skb_push(skb, skb->mac_len);\n\tbpf_mprog_foreach_prog(entry, fp, prog) {\n\t\tbpf_compute_data_pointers(skb);\n\t\tret = bpf_prog_run(prog, skb);\n\t\tif (ret != TCX_NEXT)\n\t\t\tbreak;\n\t}\n\tif (needs_mac)\n\t\t__skb_pull(skb, skb->mac_len);\n\treturn tcx_action_code(skb, ret);\n}\n\nstatic __always_inline struct sk_buff *\nsch_handle_ingress(struct sk_buff *skb, struct packet_type **pt_prev, int *ret,\n\t\t   struct net_device *orig_dev, bool *another)\n{\n\tstruct bpf_mprog_entry *entry = rcu_dereference_bh(skb->dev->tcx_ingress);\n\tint sch_ret;\n\n\tif (!entry)\n\t\treturn skb;\n\tif (*pt_prev) {\n\t\t*ret = deliver_skb(skb, *pt_prev, orig_dev);\n\t\t*pt_prev = NULL;\n\t}\n\n\tqdisc_skb_cb(skb)->pkt_len = skb->len;\n\ttcx_set_ingress(skb, true);\n\n\tif (static_branch_unlikely(&tcx_needed_key)) {\n\t\tsch_ret = tcx_run(entry, skb, true);\n\t\tif (sch_ret != TC_ACT_UNSPEC)\n\t\t\tgoto ingress_verdict;\n\t}\n\tsch_ret = tc_run(tcx_entry(entry), skb);\ningress_verdict:\n\tswitch (sch_ret) {\n\tcase TC_ACT_REDIRECT:\n\t\t \n\t\t__skb_push(skb, skb->mac_len);\n\t\tif (skb_do_redirect(skb) == -EAGAIN) {\n\t\t\t__skb_pull(skb, skb->mac_len);\n\t\t\t*another = true;\n\t\t\tbreak;\n\t\t}\n\t\t*ret = NET_RX_SUCCESS;\n\t\treturn NULL;\n\tcase TC_ACT_SHOT:\n\t\tkfree_skb_reason(skb, SKB_DROP_REASON_TC_INGRESS);\n\t\t*ret = NET_RX_DROP;\n\t\treturn NULL;\n\t \n\tcase TC_ACT_STOLEN:\n\tcase TC_ACT_QUEUED:\n\tcase TC_ACT_TRAP:\n\t\tconsume_skb(skb);\n\t\tfallthrough;\n\tcase TC_ACT_CONSUMED:\n\t\t*ret = NET_RX_SUCCESS;\n\t\treturn NULL;\n\t}\n\n\treturn skb;\n}\n\nstatic __always_inline struct sk_buff *\nsch_handle_egress(struct sk_buff *skb, int *ret, struct net_device *dev)\n{\n\tstruct bpf_mprog_entry *entry = rcu_dereference_bh(dev->tcx_egress);\n\tint sch_ret;\n\n\tif (!entry)\n\t\treturn skb;\n\n\t \n\tif (static_branch_unlikely(&tcx_needed_key)) {\n\t\tsch_ret = tcx_run(entry, skb, false);\n\t\tif (sch_ret != TC_ACT_UNSPEC)\n\t\t\tgoto egress_verdict;\n\t}\n\tsch_ret = tc_run(tcx_entry(entry), skb);\negress_verdict:\n\tswitch (sch_ret) {\n\tcase TC_ACT_REDIRECT:\n\t\t \n\t\tskb_do_redirect(skb);\n\t\t*ret = NET_XMIT_SUCCESS;\n\t\treturn NULL;\n\tcase TC_ACT_SHOT:\n\t\tkfree_skb_reason(skb, SKB_DROP_REASON_TC_EGRESS);\n\t\t*ret = NET_XMIT_DROP;\n\t\treturn NULL;\n\t \n\tcase TC_ACT_STOLEN:\n\tcase TC_ACT_QUEUED:\n\tcase TC_ACT_TRAP:\n\t\tconsume_skb(skb);\n\t\tfallthrough;\n\tcase TC_ACT_CONSUMED:\n\t\t*ret = NET_XMIT_SUCCESS;\n\t\treturn NULL;\n\t}\n\n\treturn skb;\n}\n#else\nstatic __always_inline struct sk_buff *\nsch_handle_ingress(struct sk_buff *skb, struct packet_type **pt_prev, int *ret,\n\t\t   struct net_device *orig_dev, bool *another)\n{\n\treturn skb;\n}\n\nstatic __always_inline struct sk_buff *\nsch_handle_egress(struct sk_buff *skb, int *ret, struct net_device *dev)\n{\n\treturn skb;\n}\n#endif  \n\n#ifdef CONFIG_XPS\nstatic int __get_xps_queue_idx(struct net_device *dev, struct sk_buff *skb,\n\t\t\t       struct xps_dev_maps *dev_maps, unsigned int tci)\n{\n\tint tc = netdev_get_prio_tc_map(dev, skb->priority);\n\tstruct xps_map *map;\n\tint queue_index = -1;\n\n\tif (tc >= dev_maps->num_tc || tci >= dev_maps->nr_ids)\n\t\treturn queue_index;\n\n\ttci *= dev_maps->num_tc;\n\ttci += tc;\n\n\tmap = rcu_dereference(dev_maps->attr_map[tci]);\n\tif (map) {\n\t\tif (map->len == 1)\n\t\t\tqueue_index = map->queues[0];\n\t\telse\n\t\t\tqueue_index = map->queues[reciprocal_scale(\n\t\t\t\t\t\tskb_get_hash(skb), map->len)];\n\t\tif (unlikely(queue_index >= dev->real_num_tx_queues))\n\t\t\tqueue_index = -1;\n\t}\n\treturn queue_index;\n}\n#endif\n\nstatic int get_xps_queue(struct net_device *dev, struct net_device *sb_dev,\n\t\t\t struct sk_buff *skb)\n{\n#ifdef CONFIG_XPS\n\tstruct xps_dev_maps *dev_maps;\n\tstruct sock *sk = skb->sk;\n\tint queue_index = -1;\n\n\tif (!static_key_false(&xps_needed))\n\t\treturn -1;\n\n\trcu_read_lock();\n\tif (!static_key_false(&xps_rxqs_needed))\n\t\tgoto get_cpus_map;\n\n\tdev_maps = rcu_dereference(sb_dev->xps_maps[XPS_RXQS]);\n\tif (dev_maps) {\n\t\tint tci = sk_rx_queue_get(sk);\n\n\t\tif (tci >= 0)\n\t\t\tqueue_index = __get_xps_queue_idx(dev, skb, dev_maps,\n\t\t\t\t\t\t\t  tci);\n\t}\n\nget_cpus_map:\n\tif (queue_index < 0) {\n\t\tdev_maps = rcu_dereference(sb_dev->xps_maps[XPS_CPUS]);\n\t\tif (dev_maps) {\n\t\t\tunsigned int tci = skb->sender_cpu - 1;\n\n\t\t\tqueue_index = __get_xps_queue_idx(dev, skb, dev_maps,\n\t\t\t\t\t\t\t  tci);\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\treturn queue_index;\n#else\n\treturn -1;\n#endif\n}\n\nu16 dev_pick_tx_zero(struct net_device *dev, struct sk_buff *skb,\n\t\t     struct net_device *sb_dev)\n{\n\treturn 0;\n}\nEXPORT_SYMBOL(dev_pick_tx_zero);\n\nu16 dev_pick_tx_cpu_id(struct net_device *dev, struct sk_buff *skb,\n\t\t       struct net_device *sb_dev)\n{\n\treturn (u16)raw_smp_processor_id() % dev->real_num_tx_queues;\n}\nEXPORT_SYMBOL(dev_pick_tx_cpu_id);\n\nu16 netdev_pick_tx(struct net_device *dev, struct sk_buff *skb,\n\t\t     struct net_device *sb_dev)\n{\n\tstruct sock *sk = skb->sk;\n\tint queue_index = sk_tx_queue_get(sk);\n\n\tsb_dev = sb_dev ? : dev;\n\n\tif (queue_index < 0 || skb->ooo_okay ||\n\t    queue_index >= dev->real_num_tx_queues) {\n\t\tint new_index = get_xps_queue(dev, sb_dev, skb);\n\n\t\tif (new_index < 0)\n\t\t\tnew_index = skb_tx_hash(dev, sb_dev, skb);\n\n\t\tif (queue_index != new_index && sk &&\n\t\t    sk_fullsock(sk) &&\n\t\t    rcu_access_pointer(sk->sk_dst_cache))\n\t\t\tsk_tx_queue_set(sk, new_index);\n\n\t\tqueue_index = new_index;\n\t}\n\n\treturn queue_index;\n}\nEXPORT_SYMBOL(netdev_pick_tx);\n\nstruct netdev_queue *netdev_core_pick_tx(struct net_device *dev,\n\t\t\t\t\t struct sk_buff *skb,\n\t\t\t\t\t struct net_device *sb_dev)\n{\n\tint queue_index = 0;\n\n#ifdef CONFIG_XPS\n\tu32 sender_cpu = skb->sender_cpu - 1;\n\n\tif (sender_cpu >= (u32)NR_CPUS)\n\t\tskb->sender_cpu = raw_smp_processor_id() + 1;\n#endif\n\n\tif (dev->real_num_tx_queues != 1) {\n\t\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\t\tif (ops->ndo_select_queue)\n\t\t\tqueue_index = ops->ndo_select_queue(dev, skb, sb_dev);\n\t\telse\n\t\t\tqueue_index = netdev_pick_tx(dev, skb, sb_dev);\n\n\t\tqueue_index = netdev_cap_txqueue(dev, queue_index);\n\t}\n\n\tskb_set_queue_mapping(skb, queue_index);\n\treturn netdev_get_tx_queue(dev, queue_index);\n}\n\n \nint __dev_queue_xmit(struct sk_buff *skb, struct net_device *sb_dev)\n{\n\tstruct net_device *dev = skb->dev;\n\tstruct netdev_queue *txq = NULL;\n\tstruct Qdisc *q;\n\tint rc = -ENOMEM;\n\tbool again = false;\n\n\tskb_reset_mac_header(skb);\n\tskb_assert_len(skb);\n\n\tif (unlikely(skb_shinfo(skb)->tx_flags & SKBTX_SCHED_TSTAMP))\n\t\t__skb_tstamp_tx(skb, NULL, NULL, skb->sk, SCM_TSTAMP_SCHED);\n\n\t \n\trcu_read_lock_bh();\n\n\tskb_update_prio(skb);\n\n\tqdisc_pkt_len_init(skb);\n\ttcx_set_ingress(skb, false);\n#ifdef CONFIG_NET_EGRESS\n\tif (static_branch_unlikely(&egress_needed_key)) {\n\t\tif (nf_hook_egress_active()) {\n\t\t\tskb = nf_hook_egress(skb, &rc, dev);\n\t\t\tif (!skb)\n\t\t\t\tgoto out;\n\t\t}\n\n\t\tnetdev_xmit_skip_txqueue(false);\n\n\t\tnf_skip_egress(skb, true);\n\t\tskb = sch_handle_egress(skb, &rc, dev);\n\t\tif (!skb)\n\t\t\tgoto out;\n\t\tnf_skip_egress(skb, false);\n\n\t\tif (netdev_xmit_txqueue_skipped())\n\t\t\ttxq = netdev_tx_queue_mapping(dev, skb);\n\t}\n#endif\n\t \n\tif (dev->priv_flags & IFF_XMIT_DST_RELEASE)\n\t\tskb_dst_drop(skb);\n\telse\n\t\tskb_dst_force(skb);\n\n\tif (!txq)\n\t\ttxq = netdev_core_pick_tx(dev, skb, sb_dev);\n\n\tq = rcu_dereference_bh(txq->qdisc);\n\n\ttrace_net_dev_queue(skb);\n\tif (q->enqueue) {\n\t\trc = __dev_xmit_skb(skb, q, dev, txq);\n\t\tgoto out;\n\t}\n\n\t \n\tif (dev->flags & IFF_UP) {\n\t\tint cpu = smp_processor_id();  \n\n\t\t \n\t\tif (READ_ONCE(txq->xmit_lock_owner) != cpu) {\n\t\t\tif (dev_xmit_recursion())\n\t\t\t\tgoto recursion_alert;\n\n\t\t\tskb = validate_xmit_skb(skb, dev, &again);\n\t\t\tif (!skb)\n\t\t\t\tgoto out;\n\n\t\t\tHARD_TX_LOCK(dev, txq, cpu);\n\n\t\t\tif (!netif_xmit_stopped(txq)) {\n\t\t\t\tdev_xmit_recursion_inc();\n\t\t\t\tskb = dev_hard_start_xmit(skb, dev, txq, &rc);\n\t\t\t\tdev_xmit_recursion_dec();\n\t\t\t\tif (dev_xmit_complete(rc)) {\n\t\t\t\t\tHARD_TX_UNLOCK(dev, txq);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t}\n\t\t\tHARD_TX_UNLOCK(dev, txq);\n\t\t\tnet_crit_ratelimited(\"Virtual device %s asks to queue packet!\\n\",\n\t\t\t\t\t     dev->name);\n\t\t} else {\n\t\t\t \nrecursion_alert:\n\t\t\tnet_crit_ratelimited(\"Dead loop on virtual device %s, fix it urgently!\\n\",\n\t\t\t\t\t     dev->name);\n\t\t}\n\t}\n\n\trc = -ENETDOWN;\n\trcu_read_unlock_bh();\n\n\tdev_core_stats_tx_dropped_inc(dev);\n\tkfree_skb_list(skb);\n\treturn rc;\nout:\n\trcu_read_unlock_bh();\n\treturn rc;\n}\nEXPORT_SYMBOL(__dev_queue_xmit);\n\nint __dev_direct_xmit(struct sk_buff *skb, u16 queue_id)\n{\n\tstruct net_device *dev = skb->dev;\n\tstruct sk_buff *orig_skb = skb;\n\tstruct netdev_queue *txq;\n\tint ret = NETDEV_TX_BUSY;\n\tbool again = false;\n\n\tif (unlikely(!netif_running(dev) ||\n\t\t     !netif_carrier_ok(dev)))\n\t\tgoto drop;\n\n\tskb = validate_xmit_skb_list(skb, dev, &again);\n\tif (skb != orig_skb)\n\t\tgoto drop;\n\n\tskb_set_queue_mapping(skb, queue_id);\n\ttxq = skb_get_tx_queue(dev, skb);\n\n\tlocal_bh_disable();\n\n\tdev_xmit_recursion_inc();\n\tHARD_TX_LOCK(dev, txq, smp_processor_id());\n\tif (!netif_xmit_frozen_or_drv_stopped(txq))\n\t\tret = netdev_start_xmit(skb, dev, txq, false);\n\tHARD_TX_UNLOCK(dev, txq);\n\tdev_xmit_recursion_dec();\n\n\tlocal_bh_enable();\n\treturn ret;\ndrop:\n\tdev_core_stats_tx_dropped_inc(dev);\n\tkfree_skb_list(skb);\n\treturn NET_XMIT_DROP;\n}\nEXPORT_SYMBOL(__dev_direct_xmit);\n\n \n\nint netdev_max_backlog __read_mostly = 1000;\nEXPORT_SYMBOL(netdev_max_backlog);\n\nint netdev_tstamp_prequeue __read_mostly = 1;\nunsigned int sysctl_skb_defer_max __read_mostly = 64;\nint netdev_budget __read_mostly = 300;\n \nunsigned int __read_mostly netdev_budget_usecs = 2 * USEC_PER_SEC / HZ;\nint weight_p __read_mostly = 64;            \nint dev_weight_rx_bias __read_mostly = 1;   \nint dev_weight_tx_bias __read_mostly = 1;   \nint dev_rx_weight __read_mostly = 64;\nint dev_tx_weight __read_mostly = 64;\n\n \nstatic inline void ____napi_schedule(struct softnet_data *sd,\n\t\t\t\t     struct napi_struct *napi)\n{\n\tstruct task_struct *thread;\n\n\tlockdep_assert_irqs_disabled();\n\n\tif (test_bit(NAPI_STATE_THREADED, &napi->state)) {\n\t\t \n\t\tthread = READ_ONCE(napi->thread);\n\t\tif (thread) {\n\t\t\t \n\t\t\tif (READ_ONCE(thread->__state) != TASK_INTERRUPTIBLE)\n\t\t\t\tset_bit(NAPI_STATE_SCHED_THREADED, &napi->state);\n\t\t\twake_up_process(thread);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tlist_add_tail(&napi->poll_list, &sd->poll_list);\n\tWRITE_ONCE(napi->list_owner, smp_processor_id());\n\t \n\tif (!sd->in_net_rx_action)\n\t\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n}\n\n#ifdef CONFIG_RPS\n\n \nstruct rps_sock_flow_table __rcu *rps_sock_flow_table __read_mostly;\nEXPORT_SYMBOL(rps_sock_flow_table);\nu32 rps_cpu_mask __read_mostly;\nEXPORT_SYMBOL(rps_cpu_mask);\n\nstruct static_key_false rps_needed __read_mostly;\nEXPORT_SYMBOL(rps_needed);\nstruct static_key_false rfs_needed __read_mostly;\nEXPORT_SYMBOL(rfs_needed);\n\nstatic struct rps_dev_flow *\nset_rps_cpu(struct net_device *dev, struct sk_buff *skb,\n\t    struct rps_dev_flow *rflow, u16 next_cpu)\n{\n\tif (next_cpu < nr_cpu_ids) {\n#ifdef CONFIG_RFS_ACCEL\n\t\tstruct netdev_rx_queue *rxqueue;\n\t\tstruct rps_dev_flow_table *flow_table;\n\t\tstruct rps_dev_flow *old_rflow;\n\t\tu32 flow_id;\n\t\tu16 rxq_index;\n\t\tint rc;\n\n\t\t \n\t\tif (!skb_rx_queue_recorded(skb) || !dev->rx_cpu_rmap ||\n\t\t    !(dev->features & NETIF_F_NTUPLE))\n\t\t\tgoto out;\n\t\trxq_index = cpu_rmap_lookup_index(dev->rx_cpu_rmap, next_cpu);\n\t\tif (rxq_index == skb_get_rx_queue(skb))\n\t\t\tgoto out;\n\n\t\trxqueue = dev->_rx + rxq_index;\n\t\tflow_table = rcu_dereference(rxqueue->rps_flow_table);\n\t\tif (!flow_table)\n\t\t\tgoto out;\n\t\tflow_id = skb_get_hash(skb) & flow_table->mask;\n\t\trc = dev->netdev_ops->ndo_rx_flow_steer(dev, skb,\n\t\t\t\t\t\t\trxq_index, flow_id);\n\t\tif (rc < 0)\n\t\t\tgoto out;\n\t\told_rflow = rflow;\n\t\trflow = &flow_table->flows[flow_id];\n\t\trflow->filter = rc;\n\t\tif (old_rflow->filter == rflow->filter)\n\t\t\told_rflow->filter = RPS_NO_FILTER;\n\tout:\n#endif\n\t\trflow->last_qtail =\n\t\t\tper_cpu(softnet_data, next_cpu).input_queue_head;\n\t}\n\n\trflow->cpu = next_cpu;\n\treturn rflow;\n}\n\n \nstatic int get_rps_cpu(struct net_device *dev, struct sk_buff *skb,\n\t\t       struct rps_dev_flow **rflowp)\n{\n\tconst struct rps_sock_flow_table *sock_flow_table;\n\tstruct netdev_rx_queue *rxqueue = dev->_rx;\n\tstruct rps_dev_flow_table *flow_table;\n\tstruct rps_map *map;\n\tint cpu = -1;\n\tu32 tcpu;\n\tu32 hash;\n\n\tif (skb_rx_queue_recorded(skb)) {\n\t\tu16 index = skb_get_rx_queue(skb);\n\n\t\tif (unlikely(index >= dev->real_num_rx_queues)) {\n\t\t\tWARN_ONCE(dev->real_num_rx_queues > 1,\n\t\t\t\t  \"%s received packet on queue %u, but number \"\n\t\t\t\t  \"of RX queues is %u\\n\",\n\t\t\t\t  dev->name, index, dev->real_num_rx_queues);\n\t\t\tgoto done;\n\t\t}\n\t\trxqueue += index;\n\t}\n\n\t \n\n\tflow_table = rcu_dereference(rxqueue->rps_flow_table);\n\tmap = rcu_dereference(rxqueue->rps_map);\n\tif (!flow_table && !map)\n\t\tgoto done;\n\n\tskb_reset_network_header(skb);\n\thash = skb_get_hash(skb);\n\tif (!hash)\n\t\tgoto done;\n\n\tsock_flow_table = rcu_dereference(rps_sock_flow_table);\n\tif (flow_table && sock_flow_table) {\n\t\tstruct rps_dev_flow *rflow;\n\t\tu32 next_cpu;\n\t\tu32 ident;\n\n\t\t \n\t\tident = READ_ONCE(sock_flow_table->ents[hash & sock_flow_table->mask]);\n\t\tif ((ident ^ hash) & ~rps_cpu_mask)\n\t\t\tgoto try_rps;\n\n\t\tnext_cpu = ident & rps_cpu_mask;\n\n\t\t \n\t\trflow = &flow_table->flows[hash & flow_table->mask];\n\t\ttcpu = rflow->cpu;\n\n\t\t \n\t\tif (unlikely(tcpu != next_cpu) &&\n\t\t    (tcpu >= nr_cpu_ids || !cpu_online(tcpu) ||\n\t\t     ((int)(per_cpu(softnet_data, tcpu).input_queue_head -\n\t\t      rflow->last_qtail)) >= 0)) {\n\t\t\ttcpu = next_cpu;\n\t\t\trflow = set_rps_cpu(dev, skb, rflow, next_cpu);\n\t\t}\n\n\t\tif (tcpu < nr_cpu_ids && cpu_online(tcpu)) {\n\t\t\t*rflowp = rflow;\n\t\t\tcpu = tcpu;\n\t\t\tgoto done;\n\t\t}\n\t}\n\ntry_rps:\n\n\tif (map) {\n\t\ttcpu = map->cpus[reciprocal_scale(hash, map->len)];\n\t\tif (cpu_online(tcpu)) {\n\t\t\tcpu = tcpu;\n\t\t\tgoto done;\n\t\t}\n\t}\n\ndone:\n\treturn cpu;\n}\n\n#ifdef CONFIG_RFS_ACCEL\n\n \nbool rps_may_expire_flow(struct net_device *dev, u16 rxq_index,\n\t\t\t u32 flow_id, u16 filter_id)\n{\n\tstruct netdev_rx_queue *rxqueue = dev->_rx + rxq_index;\n\tstruct rps_dev_flow_table *flow_table;\n\tstruct rps_dev_flow *rflow;\n\tbool expire = true;\n\tunsigned int cpu;\n\n\trcu_read_lock();\n\tflow_table = rcu_dereference(rxqueue->rps_flow_table);\n\tif (flow_table && flow_id <= flow_table->mask) {\n\t\trflow = &flow_table->flows[flow_id];\n\t\tcpu = READ_ONCE(rflow->cpu);\n\t\tif (rflow->filter == filter_id && cpu < nr_cpu_ids &&\n\t\t    ((int)(per_cpu(softnet_data, cpu).input_queue_head -\n\t\t\t   rflow->last_qtail) <\n\t\t     (int)(10 * flow_table->mask)))\n\t\t\texpire = false;\n\t}\n\trcu_read_unlock();\n\treturn expire;\n}\nEXPORT_SYMBOL(rps_may_expire_flow);\n\n#endif  \n\n \nstatic void rps_trigger_softirq(void *data)\n{\n\tstruct softnet_data *sd = data;\n\n\t____napi_schedule(sd, &sd->backlog);\n\tsd->received_rps++;\n}\n\n#endif  \n\n \nstatic void trigger_rx_softirq(void *data)\n{\n\tstruct softnet_data *sd = data;\n\n\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n\tsmp_store_release(&sd->defer_ipi_scheduled, 0);\n}\n\n \nstatic void napi_schedule_rps(struct softnet_data *sd)\n{\n\tstruct softnet_data *mysd = this_cpu_ptr(&softnet_data);\n\n#ifdef CONFIG_RPS\n\tif (sd != mysd) {\n\t\tsd->rps_ipi_next = mysd->rps_ipi_list;\n\t\tmysd->rps_ipi_list = sd;\n\n\t\t \n\t\tif (!mysd->in_net_rx_action && !mysd->in_napi_threaded_poll)\n\t\t\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n\t\treturn;\n\t}\n#endif  \n\t__napi_schedule_irqoff(&mysd->backlog);\n}\n\n#ifdef CONFIG_NET_FLOW_LIMIT\nint netdev_flow_limit_table_len __read_mostly = (1 << 12);\n#endif\n\nstatic bool skb_flow_limit(struct sk_buff *skb, unsigned int qlen)\n{\n#ifdef CONFIG_NET_FLOW_LIMIT\n\tstruct sd_flow_limit *fl;\n\tstruct softnet_data *sd;\n\tunsigned int old_flow, new_flow;\n\n\tif (qlen < (READ_ONCE(netdev_max_backlog) >> 1))\n\t\treturn false;\n\n\tsd = this_cpu_ptr(&softnet_data);\n\n\trcu_read_lock();\n\tfl = rcu_dereference(sd->flow_limit);\n\tif (fl) {\n\t\tnew_flow = skb_get_hash(skb) & (fl->num_buckets - 1);\n\t\told_flow = fl->history[fl->history_head];\n\t\tfl->history[fl->history_head] = new_flow;\n\n\t\tfl->history_head++;\n\t\tfl->history_head &= FLOW_LIMIT_HISTORY - 1;\n\n\t\tif (likely(fl->buckets[old_flow]))\n\t\t\tfl->buckets[old_flow]--;\n\n\t\tif (++fl->buckets[new_flow] > (FLOW_LIMIT_HISTORY >> 1)) {\n\t\t\tfl->count++;\n\t\t\trcu_read_unlock();\n\t\t\treturn true;\n\t\t}\n\t}\n\trcu_read_unlock();\n#endif\n\treturn false;\n}\n\n \nstatic int enqueue_to_backlog(struct sk_buff *skb, int cpu,\n\t\t\t      unsigned int *qtail)\n{\n\tenum skb_drop_reason reason;\n\tstruct softnet_data *sd;\n\tunsigned long flags;\n\tunsigned int qlen;\n\n\treason = SKB_DROP_REASON_NOT_SPECIFIED;\n\tsd = &per_cpu(softnet_data, cpu);\n\n\trps_lock_irqsave(sd, &flags);\n\tif (!netif_running(skb->dev))\n\t\tgoto drop;\n\tqlen = skb_queue_len(&sd->input_pkt_queue);\n\tif (qlen <= READ_ONCE(netdev_max_backlog) && !skb_flow_limit(skb, qlen)) {\n\t\tif (qlen) {\nenqueue:\n\t\t\t__skb_queue_tail(&sd->input_pkt_queue, skb);\n\t\t\tinput_queue_tail_incr_save(sd, qtail);\n\t\t\trps_unlock_irq_restore(sd, &flags);\n\t\t\treturn NET_RX_SUCCESS;\n\t\t}\n\n\t\t \n\t\tif (!__test_and_set_bit(NAPI_STATE_SCHED, &sd->backlog.state))\n\t\t\tnapi_schedule_rps(sd);\n\t\tgoto enqueue;\n\t}\n\treason = SKB_DROP_REASON_CPU_BACKLOG;\n\ndrop:\n\tsd->dropped++;\n\trps_unlock_irq_restore(sd, &flags);\n\n\tdev_core_stats_rx_dropped_inc(skb->dev);\n\tkfree_skb_reason(skb, reason);\n\treturn NET_RX_DROP;\n}\n\nstatic struct netdev_rx_queue *netif_get_rxqueue(struct sk_buff *skb)\n{\n\tstruct net_device *dev = skb->dev;\n\tstruct netdev_rx_queue *rxqueue;\n\n\trxqueue = dev->_rx;\n\n\tif (skb_rx_queue_recorded(skb)) {\n\t\tu16 index = skb_get_rx_queue(skb);\n\n\t\tif (unlikely(index >= dev->real_num_rx_queues)) {\n\t\t\tWARN_ONCE(dev->real_num_rx_queues > 1,\n\t\t\t\t  \"%s received packet on queue %u, but number \"\n\t\t\t\t  \"of RX queues is %u\\n\",\n\t\t\t\t  dev->name, index, dev->real_num_rx_queues);\n\n\t\t\treturn rxqueue;  \n\t\t}\n\t\trxqueue += index;\n\t}\n\treturn rxqueue;\n}\n\nu32 bpf_prog_run_generic_xdp(struct sk_buff *skb, struct xdp_buff *xdp,\n\t\t\t     struct bpf_prog *xdp_prog)\n{\n\tvoid *orig_data, *orig_data_end, *hard_start;\n\tstruct netdev_rx_queue *rxqueue;\n\tbool orig_bcast, orig_host;\n\tu32 mac_len, frame_sz;\n\t__be16 orig_eth_type;\n\tstruct ethhdr *eth;\n\tu32 metalen, act;\n\tint off;\n\n\t \n\tmac_len = skb->data - skb_mac_header(skb);\n\thard_start = skb->data - skb_headroom(skb);\n\n\t \n\tframe_sz = (void *)skb_end_pointer(skb) - hard_start;\n\tframe_sz += SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\n\trxqueue = netif_get_rxqueue(skb);\n\txdp_init_buff(xdp, frame_sz, &rxqueue->xdp_rxq);\n\txdp_prepare_buff(xdp, hard_start, skb_headroom(skb) - mac_len,\n\t\t\t skb_headlen(skb) + mac_len, true);\n\n\torig_data_end = xdp->data_end;\n\torig_data = xdp->data;\n\teth = (struct ethhdr *)xdp->data;\n\torig_host = ether_addr_equal_64bits(eth->h_dest, skb->dev->dev_addr);\n\torig_bcast = is_multicast_ether_addr_64bits(eth->h_dest);\n\torig_eth_type = eth->h_proto;\n\n\tact = bpf_prog_run_xdp(xdp_prog, xdp);\n\n\t \n\toff = xdp->data - orig_data;\n\tif (off) {\n\t\tif (off > 0)\n\t\t\t__skb_pull(skb, off);\n\t\telse if (off < 0)\n\t\t\t__skb_push(skb, -off);\n\n\t\tskb->mac_header += off;\n\t\tskb_reset_network_header(skb);\n\t}\n\n\t \n\toff = xdp->data_end - orig_data_end;\n\tif (off != 0) {\n\t\tskb_set_tail_pointer(skb, xdp->data_end - xdp->data);\n\t\tskb->len += off;  \n\t}\n\n\t \n\teth = (struct ethhdr *)xdp->data;\n\tif ((orig_eth_type != eth->h_proto) ||\n\t    (orig_host != ether_addr_equal_64bits(eth->h_dest,\n\t\t\t\t\t\t  skb->dev->dev_addr)) ||\n\t    (orig_bcast != is_multicast_ether_addr_64bits(eth->h_dest))) {\n\t\t__skb_push(skb, ETH_HLEN);\n\t\tskb->pkt_type = PACKET_HOST;\n\t\tskb->protocol = eth_type_trans(skb, skb->dev);\n\t}\n\n\t \n\tswitch (act) {\n\tcase XDP_REDIRECT:\n\tcase XDP_TX:\n\t\t__skb_push(skb, mac_len);\n\t\tbreak;\n\tcase XDP_PASS:\n\t\tmetalen = xdp->data - xdp->data_meta;\n\t\tif (metalen)\n\t\t\tskb_metadata_set(skb, metalen);\n\t\tbreak;\n\t}\n\n\treturn act;\n}\n\nstatic u32 netif_receive_generic_xdp(struct sk_buff *skb,\n\t\t\t\t     struct xdp_buff *xdp,\n\t\t\t\t     struct bpf_prog *xdp_prog)\n{\n\tu32 act = XDP_DROP;\n\n\t \n\tif (skb_is_redirected(skb))\n\t\treturn XDP_PASS;\n\n\t \n\tif (skb_cloned(skb) || skb_is_nonlinear(skb) ||\n\t    skb_headroom(skb) < XDP_PACKET_HEADROOM) {\n\t\tint hroom = XDP_PACKET_HEADROOM - skb_headroom(skb);\n\t\tint troom = skb->tail + skb->data_len - skb->end;\n\n\t\t \n\t\tif (pskb_expand_head(skb,\n\t\t\t\t     hroom > 0 ? ALIGN(hroom, NET_SKB_PAD) : 0,\n\t\t\t\t     troom > 0 ? troom + 128 : 0, GFP_ATOMIC))\n\t\t\tgoto do_drop;\n\t\tif (skb_linearize(skb))\n\t\t\tgoto do_drop;\n\t}\n\n\tact = bpf_prog_run_generic_xdp(skb, xdp, xdp_prog);\n\tswitch (act) {\n\tcase XDP_REDIRECT:\n\tcase XDP_TX:\n\tcase XDP_PASS:\n\t\tbreak;\n\tdefault:\n\t\tbpf_warn_invalid_xdp_action(skb->dev, xdp_prog, act);\n\t\tfallthrough;\n\tcase XDP_ABORTED:\n\t\ttrace_xdp_exception(skb->dev, xdp_prog, act);\n\t\tfallthrough;\n\tcase XDP_DROP:\n\tdo_drop:\n\t\tkfree_skb(skb);\n\t\tbreak;\n\t}\n\n\treturn act;\n}\n\n \nvoid generic_xdp_tx(struct sk_buff *skb, struct bpf_prog *xdp_prog)\n{\n\tstruct net_device *dev = skb->dev;\n\tstruct netdev_queue *txq;\n\tbool free_skb = true;\n\tint cpu, rc;\n\n\ttxq = netdev_core_pick_tx(dev, skb, NULL);\n\tcpu = smp_processor_id();\n\tHARD_TX_LOCK(dev, txq, cpu);\n\tif (!netif_xmit_frozen_or_drv_stopped(txq)) {\n\t\trc = netdev_start_xmit(skb, dev, txq, 0);\n\t\tif (dev_xmit_complete(rc))\n\t\t\tfree_skb = false;\n\t}\n\tHARD_TX_UNLOCK(dev, txq);\n\tif (free_skb) {\n\t\ttrace_xdp_exception(dev, xdp_prog, XDP_TX);\n\t\tdev_core_stats_tx_dropped_inc(dev);\n\t\tkfree_skb(skb);\n\t}\n}\n\nstatic DEFINE_STATIC_KEY_FALSE(generic_xdp_needed_key);\n\nint do_xdp_generic(struct bpf_prog *xdp_prog, struct sk_buff *skb)\n{\n\tif (xdp_prog) {\n\t\tstruct xdp_buff xdp;\n\t\tu32 act;\n\t\tint err;\n\n\t\tact = netif_receive_generic_xdp(skb, &xdp, xdp_prog);\n\t\tif (act != XDP_PASS) {\n\t\t\tswitch (act) {\n\t\t\tcase XDP_REDIRECT:\n\t\t\t\terr = xdp_do_generic_redirect(skb->dev, skb,\n\t\t\t\t\t\t\t      &xdp, xdp_prog);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto out_redir;\n\t\t\t\tbreak;\n\t\t\tcase XDP_TX:\n\t\t\t\tgeneric_xdp_tx(skb, xdp_prog);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\treturn XDP_DROP;\n\t\t}\n\t}\n\treturn XDP_PASS;\nout_redir:\n\tkfree_skb_reason(skb, SKB_DROP_REASON_XDP);\n\treturn XDP_DROP;\n}\nEXPORT_SYMBOL_GPL(do_xdp_generic);\n\nstatic int netif_rx_internal(struct sk_buff *skb)\n{\n\tint ret;\n\n\tnet_timestamp_check(READ_ONCE(netdev_tstamp_prequeue), skb);\n\n\ttrace_netif_rx(skb);\n\n#ifdef CONFIG_RPS\n\tif (static_branch_unlikely(&rps_needed)) {\n\t\tstruct rps_dev_flow voidflow, *rflow = &voidflow;\n\t\tint cpu;\n\n\t\trcu_read_lock();\n\n\t\tcpu = get_rps_cpu(skb->dev, skb, &rflow);\n\t\tif (cpu < 0)\n\t\t\tcpu = smp_processor_id();\n\n\t\tret = enqueue_to_backlog(skb, cpu, &rflow->last_qtail);\n\n\t\trcu_read_unlock();\n\t} else\n#endif\n\t{\n\t\tunsigned int qtail;\n\n\t\tret = enqueue_to_backlog(skb, smp_processor_id(), &qtail);\n\t}\n\treturn ret;\n}\n\n \nint __netif_rx(struct sk_buff *skb)\n{\n\tint ret;\n\n\tlockdep_assert_once(hardirq_count() | softirq_count());\n\n\ttrace_netif_rx_entry(skb);\n\tret = netif_rx_internal(skb);\n\ttrace_netif_rx_exit(ret);\n\treturn ret;\n}\nEXPORT_SYMBOL(__netif_rx);\n\n \nint netif_rx(struct sk_buff *skb)\n{\n\tbool need_bh_off = !(hardirq_count() | softirq_count());\n\tint ret;\n\n\tif (need_bh_off)\n\t\tlocal_bh_disable();\n\ttrace_netif_rx_entry(skb);\n\tret = netif_rx_internal(skb);\n\ttrace_netif_rx_exit(ret);\n\tif (need_bh_off)\n\t\tlocal_bh_enable();\n\treturn ret;\n}\nEXPORT_SYMBOL(netif_rx);\n\nstatic __latent_entropy void net_tx_action(struct softirq_action *h)\n{\n\tstruct softnet_data *sd = this_cpu_ptr(&softnet_data);\n\n\tif (sd->completion_queue) {\n\t\tstruct sk_buff *clist;\n\n\t\tlocal_irq_disable();\n\t\tclist = sd->completion_queue;\n\t\tsd->completion_queue = NULL;\n\t\tlocal_irq_enable();\n\n\t\twhile (clist) {\n\t\t\tstruct sk_buff *skb = clist;\n\n\t\t\tclist = clist->next;\n\n\t\t\tWARN_ON(refcount_read(&skb->users));\n\t\t\tif (likely(get_kfree_skb_cb(skb)->reason == SKB_CONSUMED))\n\t\t\t\ttrace_consume_skb(skb, net_tx_action);\n\t\t\telse\n\t\t\t\ttrace_kfree_skb(skb, net_tx_action,\n\t\t\t\t\t\tget_kfree_skb_cb(skb)->reason);\n\n\t\t\tif (skb->fclone != SKB_FCLONE_UNAVAILABLE)\n\t\t\t\t__kfree_skb(skb);\n\t\t\telse\n\t\t\t\t__napi_kfree_skb(skb,\n\t\t\t\t\t\t get_kfree_skb_cb(skb)->reason);\n\t\t}\n\t}\n\n\tif (sd->output_queue) {\n\t\tstruct Qdisc *head;\n\n\t\tlocal_irq_disable();\n\t\thead = sd->output_queue;\n\t\tsd->output_queue = NULL;\n\t\tsd->output_queue_tailp = &sd->output_queue;\n\t\tlocal_irq_enable();\n\n\t\trcu_read_lock();\n\n\t\twhile (head) {\n\t\t\tstruct Qdisc *q = head;\n\t\t\tspinlock_t *root_lock = NULL;\n\n\t\t\thead = head->next_sched;\n\n\t\t\t \n\t\t\tsmp_mb__before_atomic();\n\n\t\t\tif (!(q->flags & TCQ_F_NOLOCK)) {\n\t\t\t\troot_lock = qdisc_lock(q);\n\t\t\t\tspin_lock(root_lock);\n\t\t\t} else if (unlikely(test_bit(__QDISC_STATE_DEACTIVATED,\n\t\t\t\t\t\t     &q->state))) {\n\t\t\t\t \n\t\t\t\tclear_bit(__QDISC_STATE_SCHED, &q->state);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tclear_bit(__QDISC_STATE_SCHED, &q->state);\n\t\t\tqdisc_run(q);\n\t\t\tif (root_lock)\n\t\t\t\tspin_unlock(root_lock);\n\t\t}\n\n\t\trcu_read_unlock();\n\t}\n\n\txfrm_dev_backlog(sd);\n}\n\n#if IS_ENABLED(CONFIG_BRIDGE) && IS_ENABLED(CONFIG_ATM_LANE)\n \nint (*br_fdb_test_addr_hook)(struct net_device *dev,\n\t\t\t     unsigned char *addr) __read_mostly;\nEXPORT_SYMBOL_GPL(br_fdb_test_addr_hook);\n#endif\n\n \nbool netdev_is_rx_handler_busy(struct net_device *dev)\n{\n\tASSERT_RTNL();\n\treturn dev && rtnl_dereference(dev->rx_handler);\n}\nEXPORT_SYMBOL_GPL(netdev_is_rx_handler_busy);\n\n \nint netdev_rx_handler_register(struct net_device *dev,\n\t\t\t       rx_handler_func_t *rx_handler,\n\t\t\t       void *rx_handler_data)\n{\n\tif (netdev_is_rx_handler_busy(dev))\n\t\treturn -EBUSY;\n\n\tif (dev->priv_flags & IFF_NO_RX_HANDLER)\n\t\treturn -EINVAL;\n\n\t \n\trcu_assign_pointer(dev->rx_handler_data, rx_handler_data);\n\trcu_assign_pointer(dev->rx_handler, rx_handler);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(netdev_rx_handler_register);\n\n \nvoid netdev_rx_handler_unregister(struct net_device *dev)\n{\n\n\tASSERT_RTNL();\n\tRCU_INIT_POINTER(dev->rx_handler, NULL);\n\t \n\tsynchronize_net();\n\tRCU_INIT_POINTER(dev->rx_handler_data, NULL);\n}\nEXPORT_SYMBOL_GPL(netdev_rx_handler_unregister);\n\n \nstatic bool skb_pfmemalloc_protocol(struct sk_buff *skb)\n{\n\tswitch (skb->protocol) {\n\tcase htons(ETH_P_ARP):\n\tcase htons(ETH_P_IP):\n\tcase htons(ETH_P_IPV6):\n\tcase htons(ETH_P_8021Q):\n\tcase htons(ETH_P_8021AD):\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic inline int nf_ingress(struct sk_buff *skb, struct packet_type **pt_prev,\n\t\t\t     int *ret, struct net_device *orig_dev)\n{\n\tif (nf_hook_ingress_active(skb)) {\n\t\tint ingress_retval;\n\n\t\tif (*pt_prev) {\n\t\t\t*ret = deliver_skb(skb, *pt_prev, orig_dev);\n\t\t\t*pt_prev = NULL;\n\t\t}\n\n\t\trcu_read_lock();\n\t\tingress_retval = nf_hook_ingress(skb);\n\t\trcu_read_unlock();\n\t\treturn ingress_retval;\n\t}\n\treturn 0;\n}\n\nstatic int __netif_receive_skb_core(struct sk_buff **pskb, bool pfmemalloc,\n\t\t\t\t    struct packet_type **ppt_prev)\n{\n\tstruct packet_type *ptype, *pt_prev;\n\trx_handler_func_t *rx_handler;\n\tstruct sk_buff *skb = *pskb;\n\tstruct net_device *orig_dev;\n\tbool deliver_exact = false;\n\tint ret = NET_RX_DROP;\n\t__be16 type;\n\n\tnet_timestamp_check(!READ_ONCE(netdev_tstamp_prequeue), skb);\n\n\ttrace_netif_receive_skb(skb);\n\n\torig_dev = skb->dev;\n\n\tskb_reset_network_header(skb);\n\tif (!skb_transport_header_was_set(skb))\n\t\tskb_reset_transport_header(skb);\n\tskb_reset_mac_len(skb);\n\n\tpt_prev = NULL;\n\nanother_round:\n\tskb->skb_iif = skb->dev->ifindex;\n\n\t__this_cpu_inc(softnet_data.processed);\n\n\tif (static_branch_unlikely(&generic_xdp_needed_key)) {\n\t\tint ret2;\n\n\t\tmigrate_disable();\n\t\tret2 = do_xdp_generic(rcu_dereference(skb->dev->xdp_prog), skb);\n\t\tmigrate_enable();\n\n\t\tif (ret2 != XDP_PASS) {\n\t\t\tret = NET_RX_DROP;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (eth_type_vlan(skb->protocol)) {\n\t\tskb = skb_vlan_untag(skb);\n\t\tif (unlikely(!skb))\n\t\t\tgoto out;\n\t}\n\n\tif (skb_skip_tc_classify(skb))\n\t\tgoto skip_classify;\n\n\tif (pfmemalloc)\n\t\tgoto skip_taps;\n\n\tlist_for_each_entry_rcu(ptype, &ptype_all, list) {\n\t\tif (pt_prev)\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\tpt_prev = ptype;\n\t}\n\n\tlist_for_each_entry_rcu(ptype, &skb->dev->ptype_all, list) {\n\t\tif (pt_prev)\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\tpt_prev = ptype;\n\t}\n\nskip_taps:\n#ifdef CONFIG_NET_INGRESS\n\tif (static_branch_unlikely(&ingress_needed_key)) {\n\t\tbool another = false;\n\n\t\tnf_skip_egress(skb, true);\n\t\tskb = sch_handle_ingress(skb, &pt_prev, &ret, orig_dev,\n\t\t\t\t\t &another);\n\t\tif (another)\n\t\t\tgoto another_round;\n\t\tif (!skb)\n\t\t\tgoto out;\n\n\t\tnf_skip_egress(skb, false);\n\t\tif (nf_ingress(skb, &pt_prev, &ret, orig_dev) < 0)\n\t\t\tgoto out;\n\t}\n#endif\n\tskb_reset_redirect(skb);\nskip_classify:\n\tif (pfmemalloc && !skb_pfmemalloc_protocol(skb))\n\t\tgoto drop;\n\n\tif (skb_vlan_tag_present(skb)) {\n\t\tif (pt_prev) {\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\t\tpt_prev = NULL;\n\t\t}\n\t\tif (vlan_do_receive(&skb))\n\t\t\tgoto another_round;\n\t\telse if (unlikely(!skb))\n\t\t\tgoto out;\n\t}\n\n\trx_handler = rcu_dereference(skb->dev->rx_handler);\n\tif (rx_handler) {\n\t\tif (pt_prev) {\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\t\tpt_prev = NULL;\n\t\t}\n\t\tswitch (rx_handler(&skb)) {\n\t\tcase RX_HANDLER_CONSUMED:\n\t\t\tret = NET_RX_SUCCESS;\n\t\t\tgoto out;\n\t\tcase RX_HANDLER_ANOTHER:\n\t\t\tgoto another_round;\n\t\tcase RX_HANDLER_EXACT:\n\t\t\tdeliver_exact = true;\n\t\t\tbreak;\n\t\tcase RX_HANDLER_PASS:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tBUG();\n\t\t}\n\t}\n\n\tif (unlikely(skb_vlan_tag_present(skb)) && !netdev_uses_dsa(skb->dev)) {\ncheck_vlan_id:\n\t\tif (skb_vlan_tag_get_id(skb)) {\n\t\t\t \n\t\t\tskb->pkt_type = PACKET_OTHERHOST;\n\t\t} else if (eth_type_vlan(skb->protocol)) {\n\t\t\t \n\t\t\t__vlan_hwaccel_clear_tag(skb);\n\t\t\tskb = skb_vlan_untag(skb);\n\t\t\tif (unlikely(!skb))\n\t\t\t\tgoto out;\n\t\t\tif (vlan_do_receive(&skb))\n\t\t\t\t \n\t\t\t\tgoto another_round;\n\t\t\telse if (unlikely(!skb))\n\t\t\t\tgoto out;\n\t\t\telse\n\t\t\t\t \n\t\t\t\tgoto check_vlan_id;\n\t\t}\n\t\t \n\t\t__vlan_hwaccel_clear_tag(skb);\n\t}\n\n\ttype = skb->protocol;\n\n\t \n\tif (likely(!deliver_exact)) {\n\t\tdeliver_ptype_list_skb(skb, &pt_prev, orig_dev, type,\n\t\t\t\t       &ptype_base[ntohs(type) &\n\t\t\t\t\t\t   PTYPE_HASH_MASK]);\n\t}\n\n\tdeliver_ptype_list_skb(skb, &pt_prev, orig_dev, type,\n\t\t\t       &orig_dev->ptype_specific);\n\n\tif (unlikely(skb->dev != orig_dev)) {\n\t\tdeliver_ptype_list_skb(skb, &pt_prev, orig_dev, type,\n\t\t\t\t       &skb->dev->ptype_specific);\n\t}\n\n\tif (pt_prev) {\n\t\tif (unlikely(skb_orphan_frags_rx(skb, GFP_ATOMIC)))\n\t\t\tgoto drop;\n\t\t*ppt_prev = pt_prev;\n\t} else {\ndrop:\n\t\tif (!deliver_exact)\n\t\t\tdev_core_stats_rx_dropped_inc(skb->dev);\n\t\telse\n\t\t\tdev_core_stats_rx_nohandler_inc(skb->dev);\n\t\tkfree_skb_reason(skb, SKB_DROP_REASON_UNHANDLED_PROTO);\n\t\t \n\t\tret = NET_RX_DROP;\n\t}\n\nout:\n\t \n\t*pskb = skb;\n\treturn ret;\n}\n\nstatic int __netif_receive_skb_one_core(struct sk_buff *skb, bool pfmemalloc)\n{\n\tstruct net_device *orig_dev = skb->dev;\n\tstruct packet_type *pt_prev = NULL;\n\tint ret;\n\n\tret = __netif_receive_skb_core(&skb, pfmemalloc, &pt_prev);\n\tif (pt_prev)\n\t\tret = INDIRECT_CALL_INET(pt_prev->func, ipv6_rcv, ip_rcv, skb,\n\t\t\t\t\t skb->dev, pt_prev, orig_dev);\n\treturn ret;\n}\n\n \nint netif_receive_skb_core(struct sk_buff *skb)\n{\n\tint ret;\n\n\trcu_read_lock();\n\tret = __netif_receive_skb_one_core(skb, false);\n\trcu_read_unlock();\n\n\treturn ret;\n}\nEXPORT_SYMBOL(netif_receive_skb_core);\n\nstatic inline void __netif_receive_skb_list_ptype(struct list_head *head,\n\t\t\t\t\t\t  struct packet_type *pt_prev,\n\t\t\t\t\t\t  struct net_device *orig_dev)\n{\n\tstruct sk_buff *skb, *next;\n\n\tif (!pt_prev)\n\t\treturn;\n\tif (list_empty(head))\n\t\treturn;\n\tif (pt_prev->list_func != NULL)\n\t\tINDIRECT_CALL_INET(pt_prev->list_func, ipv6_list_rcv,\n\t\t\t\t   ip_list_rcv, head, pt_prev, orig_dev);\n\telse\n\t\tlist_for_each_entry_safe(skb, next, head, list) {\n\t\t\tskb_list_del_init(skb);\n\t\t\tpt_prev->func(skb, skb->dev, pt_prev, orig_dev);\n\t\t}\n}\n\nstatic void __netif_receive_skb_list_core(struct list_head *head, bool pfmemalloc)\n{\n\t \n\t \n\tstruct packet_type *pt_curr = NULL;\n\t \n\tstruct net_device *od_curr = NULL;\n\tstruct list_head sublist;\n\tstruct sk_buff *skb, *next;\n\n\tINIT_LIST_HEAD(&sublist);\n\tlist_for_each_entry_safe(skb, next, head, list) {\n\t\tstruct net_device *orig_dev = skb->dev;\n\t\tstruct packet_type *pt_prev = NULL;\n\n\t\tskb_list_del_init(skb);\n\t\t__netif_receive_skb_core(&skb, pfmemalloc, &pt_prev);\n\t\tif (!pt_prev)\n\t\t\tcontinue;\n\t\tif (pt_curr != pt_prev || od_curr != orig_dev) {\n\t\t\t \n\t\t\t__netif_receive_skb_list_ptype(&sublist, pt_curr, od_curr);\n\t\t\t \n\t\t\tINIT_LIST_HEAD(&sublist);\n\t\t\tpt_curr = pt_prev;\n\t\t\tod_curr = orig_dev;\n\t\t}\n\t\tlist_add_tail(&skb->list, &sublist);\n\t}\n\n\t \n\t__netif_receive_skb_list_ptype(&sublist, pt_curr, od_curr);\n}\n\nstatic int __netif_receive_skb(struct sk_buff *skb)\n{\n\tint ret;\n\n\tif (sk_memalloc_socks() && skb_pfmemalloc(skb)) {\n\t\tunsigned int noreclaim_flag;\n\n\t\t \n\t\tnoreclaim_flag = memalloc_noreclaim_save();\n\t\tret = __netif_receive_skb_one_core(skb, true);\n\t\tmemalloc_noreclaim_restore(noreclaim_flag);\n\t} else\n\t\tret = __netif_receive_skb_one_core(skb, false);\n\n\treturn ret;\n}\n\nstatic void __netif_receive_skb_list(struct list_head *head)\n{\n\tunsigned long noreclaim_flag = 0;\n\tstruct sk_buff *skb, *next;\n\tbool pfmemalloc = false;  \n\n\tlist_for_each_entry_safe(skb, next, head, list) {\n\t\tif ((sk_memalloc_socks() && skb_pfmemalloc(skb)) != pfmemalloc) {\n\t\t\tstruct list_head sublist;\n\n\t\t\t \n\t\t\tlist_cut_before(&sublist, head, &skb->list);\n\t\t\tif (!list_empty(&sublist))\n\t\t\t\t__netif_receive_skb_list_core(&sublist, pfmemalloc);\n\t\t\tpfmemalloc = !pfmemalloc;\n\t\t\t \n\t\t\tif (pfmemalloc)\n\t\t\t\tnoreclaim_flag = memalloc_noreclaim_save();\n\t\t\telse\n\t\t\t\tmemalloc_noreclaim_restore(noreclaim_flag);\n\t\t}\n\t}\n\t \n\tif (!list_empty(head))\n\t\t__netif_receive_skb_list_core(head, pfmemalloc);\n\t \n\tif (pfmemalloc)\n\t\tmemalloc_noreclaim_restore(noreclaim_flag);\n}\n\nstatic int generic_xdp_install(struct net_device *dev, struct netdev_bpf *xdp)\n{\n\tstruct bpf_prog *old = rtnl_dereference(dev->xdp_prog);\n\tstruct bpf_prog *new = xdp->prog;\n\tint ret = 0;\n\n\tswitch (xdp->command) {\n\tcase XDP_SETUP_PROG:\n\t\trcu_assign_pointer(dev->xdp_prog, new);\n\t\tif (old)\n\t\t\tbpf_prog_put(old);\n\n\t\tif (old && !new) {\n\t\t\tstatic_branch_dec(&generic_xdp_needed_key);\n\t\t} else if (new && !old) {\n\t\t\tstatic_branch_inc(&generic_xdp_needed_key);\n\t\t\tdev_disable_lro(dev);\n\t\t\tdev_disable_gro_hw(dev);\n\t\t}\n\t\tbreak;\n\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic int netif_receive_skb_internal(struct sk_buff *skb)\n{\n\tint ret;\n\n\tnet_timestamp_check(READ_ONCE(netdev_tstamp_prequeue), skb);\n\n\tif (skb_defer_rx_timestamp(skb))\n\t\treturn NET_RX_SUCCESS;\n\n\trcu_read_lock();\n#ifdef CONFIG_RPS\n\tif (static_branch_unlikely(&rps_needed)) {\n\t\tstruct rps_dev_flow voidflow, *rflow = &voidflow;\n\t\tint cpu = get_rps_cpu(skb->dev, skb, &rflow);\n\n\t\tif (cpu >= 0) {\n\t\t\tret = enqueue_to_backlog(skb, cpu, &rflow->last_qtail);\n\t\t\trcu_read_unlock();\n\t\t\treturn ret;\n\t\t}\n\t}\n#endif\n\tret = __netif_receive_skb(skb);\n\trcu_read_unlock();\n\treturn ret;\n}\n\nvoid netif_receive_skb_list_internal(struct list_head *head)\n{\n\tstruct sk_buff *skb, *next;\n\tstruct list_head sublist;\n\n\tINIT_LIST_HEAD(&sublist);\n\tlist_for_each_entry_safe(skb, next, head, list) {\n\t\tnet_timestamp_check(READ_ONCE(netdev_tstamp_prequeue), skb);\n\t\tskb_list_del_init(skb);\n\t\tif (!skb_defer_rx_timestamp(skb))\n\t\t\tlist_add_tail(&skb->list, &sublist);\n\t}\n\tlist_splice_init(&sublist, head);\n\n\trcu_read_lock();\n#ifdef CONFIG_RPS\n\tif (static_branch_unlikely(&rps_needed)) {\n\t\tlist_for_each_entry_safe(skb, next, head, list) {\n\t\t\tstruct rps_dev_flow voidflow, *rflow = &voidflow;\n\t\t\tint cpu = get_rps_cpu(skb->dev, skb, &rflow);\n\n\t\t\tif (cpu >= 0) {\n\t\t\t\t \n\t\t\t\tskb_list_del_init(skb);\n\t\t\t\tenqueue_to_backlog(skb, cpu, &rflow->last_qtail);\n\t\t\t}\n\t\t}\n\t}\n#endif\n\t__netif_receive_skb_list(head);\n\trcu_read_unlock();\n}\n\n \nint netif_receive_skb(struct sk_buff *skb)\n{\n\tint ret;\n\n\ttrace_netif_receive_skb_entry(skb);\n\n\tret = netif_receive_skb_internal(skb);\n\ttrace_netif_receive_skb_exit(ret);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(netif_receive_skb);\n\n \nvoid netif_receive_skb_list(struct list_head *head)\n{\n\tstruct sk_buff *skb;\n\n\tif (list_empty(head))\n\t\treturn;\n\tif (trace_netif_receive_skb_list_entry_enabled()) {\n\t\tlist_for_each_entry(skb, head, list)\n\t\t\ttrace_netif_receive_skb_list_entry(skb);\n\t}\n\tnetif_receive_skb_list_internal(head);\n\ttrace_netif_receive_skb_list_exit(0);\n}\nEXPORT_SYMBOL(netif_receive_skb_list);\n\nstatic DEFINE_PER_CPU(struct work_struct, flush_works);\n\n \nstatic void flush_backlog(struct work_struct *work)\n{\n\tstruct sk_buff *skb, *tmp;\n\tstruct softnet_data *sd;\n\n\tlocal_bh_disable();\n\tsd = this_cpu_ptr(&softnet_data);\n\n\trps_lock_irq_disable(sd);\n\tskb_queue_walk_safe(&sd->input_pkt_queue, skb, tmp) {\n\t\tif (skb->dev->reg_state == NETREG_UNREGISTERING) {\n\t\t\t__skb_unlink(skb, &sd->input_pkt_queue);\n\t\t\tdev_kfree_skb_irq(skb);\n\t\t\tinput_queue_head_incr(sd);\n\t\t}\n\t}\n\trps_unlock_irq_enable(sd);\n\n\tskb_queue_walk_safe(&sd->process_queue, skb, tmp) {\n\t\tif (skb->dev->reg_state == NETREG_UNREGISTERING) {\n\t\t\t__skb_unlink(skb, &sd->process_queue);\n\t\t\tkfree_skb(skb);\n\t\t\tinput_queue_head_incr(sd);\n\t\t}\n\t}\n\tlocal_bh_enable();\n}\n\nstatic bool flush_required(int cpu)\n{\n#if IS_ENABLED(CONFIG_RPS)\n\tstruct softnet_data *sd = &per_cpu(softnet_data, cpu);\n\tbool do_flush;\n\n\trps_lock_irq_disable(sd);\n\n\t \n\tdo_flush = !skb_queue_empty(&sd->input_pkt_queue) ||\n\t\t   !skb_queue_empty_lockless(&sd->process_queue);\n\trps_unlock_irq_enable(sd);\n\n\treturn do_flush;\n#endif\n\t \n\treturn true;\n}\n\nstatic void flush_all_backlogs(void)\n{\n\tstatic cpumask_t flush_cpus;\n\tunsigned int cpu;\n\n\t \n\tASSERT_RTNL();\n\n\tcpus_read_lock();\n\n\tcpumask_clear(&flush_cpus);\n\tfor_each_online_cpu(cpu) {\n\t\tif (flush_required(cpu)) {\n\t\t\tqueue_work_on(cpu, system_highpri_wq,\n\t\t\t\t      per_cpu_ptr(&flush_works, cpu));\n\t\t\tcpumask_set_cpu(cpu, &flush_cpus);\n\t\t}\n\t}\n\n\t \n\tfor_each_cpu(cpu, &flush_cpus)\n\t\tflush_work(per_cpu_ptr(&flush_works, cpu));\n\n\tcpus_read_unlock();\n}\n\nstatic void net_rps_send_ipi(struct softnet_data *remsd)\n{\n#ifdef CONFIG_RPS\n\twhile (remsd) {\n\t\tstruct softnet_data *next = remsd->rps_ipi_next;\n\n\t\tif (cpu_online(remsd->cpu))\n\t\t\tsmp_call_function_single_async(remsd->cpu, &remsd->csd);\n\t\tremsd = next;\n\t}\n#endif\n}\n\n \nstatic void net_rps_action_and_irq_enable(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tstruct softnet_data *remsd = sd->rps_ipi_list;\n\n\tif (remsd) {\n\t\tsd->rps_ipi_list = NULL;\n\n\t\tlocal_irq_enable();\n\n\t\t \n\t\tnet_rps_send_ipi(remsd);\n\t} else\n#endif\n\t\tlocal_irq_enable();\n}\n\nstatic bool sd_has_rps_ipi_waiting(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\treturn sd->rps_ipi_list != NULL;\n#else\n\treturn false;\n#endif\n}\n\nstatic int process_backlog(struct napi_struct *napi, int quota)\n{\n\tstruct softnet_data *sd = container_of(napi, struct softnet_data, backlog);\n\tbool again = true;\n\tint work = 0;\n\n\t \n\tif (sd_has_rps_ipi_waiting(sd)) {\n\t\tlocal_irq_disable();\n\t\tnet_rps_action_and_irq_enable(sd);\n\t}\n\n\tnapi->weight = READ_ONCE(dev_rx_weight);\n\twhile (again) {\n\t\tstruct sk_buff *skb;\n\n\t\twhile ((skb = __skb_dequeue(&sd->process_queue))) {\n\t\t\trcu_read_lock();\n\t\t\t__netif_receive_skb(skb);\n\t\t\trcu_read_unlock();\n\t\t\tinput_queue_head_incr(sd);\n\t\t\tif (++work >= quota)\n\t\t\t\treturn work;\n\n\t\t}\n\n\t\trps_lock_irq_disable(sd);\n\t\tif (skb_queue_empty(&sd->input_pkt_queue)) {\n\t\t\t \n\t\t\tnapi->state = 0;\n\t\t\tagain = false;\n\t\t} else {\n\t\t\tskb_queue_splice_tail_init(&sd->input_pkt_queue,\n\t\t\t\t\t\t   &sd->process_queue);\n\t\t}\n\t\trps_unlock_irq_enable(sd);\n\t}\n\n\treturn work;\n}\n\n \nvoid __napi_schedule(struct napi_struct *n)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\t____napi_schedule(this_cpu_ptr(&softnet_data), n);\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL(__napi_schedule);\n\n \nbool napi_schedule_prep(struct napi_struct *n)\n{\n\tunsigned long new, val = READ_ONCE(n->state);\n\n\tdo {\n\t\tif (unlikely(val & NAPIF_STATE_DISABLE))\n\t\t\treturn false;\n\t\tnew = val | NAPIF_STATE_SCHED;\n\n\t\t \n\t\tnew |= (val & NAPIF_STATE_SCHED) / NAPIF_STATE_SCHED *\n\t\t\t\t\t\t   NAPIF_STATE_MISSED;\n\t} while (!try_cmpxchg(&n->state, &val, new));\n\n\treturn !(val & NAPIF_STATE_SCHED);\n}\nEXPORT_SYMBOL(napi_schedule_prep);\n\n \nvoid __napi_schedule_irqoff(struct napi_struct *n)\n{\n\tif (!IS_ENABLED(CONFIG_PREEMPT_RT))\n\t\t____napi_schedule(this_cpu_ptr(&softnet_data), n);\n\telse\n\t\t__napi_schedule(n);\n}\nEXPORT_SYMBOL(__napi_schedule_irqoff);\n\nbool napi_complete_done(struct napi_struct *n, int work_done)\n{\n\tunsigned long flags, val, new, timeout = 0;\n\tbool ret = true;\n\n\t \n\tif (unlikely(n->state & (NAPIF_STATE_NPSVC |\n\t\t\t\t NAPIF_STATE_IN_BUSY_POLL)))\n\t\treturn false;\n\n\tif (work_done) {\n\t\tif (n->gro_bitmask)\n\t\t\ttimeout = READ_ONCE(n->dev->gro_flush_timeout);\n\t\tn->defer_hard_irqs_count = READ_ONCE(n->dev->napi_defer_hard_irqs);\n\t}\n\tif (n->defer_hard_irqs_count > 0) {\n\t\tn->defer_hard_irqs_count--;\n\t\ttimeout = READ_ONCE(n->dev->gro_flush_timeout);\n\t\tif (timeout)\n\t\t\tret = false;\n\t}\n\tif (n->gro_bitmask) {\n\t\t \n\t\tnapi_gro_flush(n, !!timeout);\n\t}\n\n\tgro_normal_list(n);\n\n\tif (unlikely(!list_empty(&n->poll_list))) {\n\t\t \n\t\tlocal_irq_save(flags);\n\t\tlist_del_init(&n->poll_list);\n\t\tlocal_irq_restore(flags);\n\t}\n\tWRITE_ONCE(n->list_owner, -1);\n\n\tval = READ_ONCE(n->state);\n\tdo {\n\t\tWARN_ON_ONCE(!(val & NAPIF_STATE_SCHED));\n\n\t\tnew = val & ~(NAPIF_STATE_MISSED | NAPIF_STATE_SCHED |\n\t\t\t      NAPIF_STATE_SCHED_THREADED |\n\t\t\t      NAPIF_STATE_PREFER_BUSY_POLL);\n\n\t\t \n\t\tnew |= (val & NAPIF_STATE_MISSED) / NAPIF_STATE_MISSED *\n\t\t\t\t\t\t    NAPIF_STATE_SCHED;\n\t} while (!try_cmpxchg(&n->state, &val, new));\n\n\tif (unlikely(val & NAPIF_STATE_MISSED)) {\n\t\t__napi_schedule(n);\n\t\treturn false;\n\t}\n\n\tif (timeout)\n\t\thrtimer_start(&n->timer, ns_to_ktime(timeout),\n\t\t\t      HRTIMER_MODE_REL_PINNED);\n\treturn ret;\n}\nEXPORT_SYMBOL(napi_complete_done);\n\n \nstatic struct napi_struct *napi_by_id(unsigned int napi_id)\n{\n\tunsigned int hash = napi_id % HASH_SIZE(napi_hash);\n\tstruct napi_struct *napi;\n\n\thlist_for_each_entry_rcu(napi, &napi_hash[hash], napi_hash_node)\n\t\tif (napi->napi_id == napi_id)\n\t\t\treturn napi;\n\n\treturn NULL;\n}\n\n#if defined(CONFIG_NET_RX_BUSY_POLL)\n\nstatic void __busy_poll_stop(struct napi_struct *napi, bool skip_schedule)\n{\n\tif (!skip_schedule) {\n\t\tgro_normal_list(napi);\n\t\t__napi_schedule(napi);\n\t\treturn;\n\t}\n\n\tif (napi->gro_bitmask) {\n\t\t \n\t\tnapi_gro_flush(napi, HZ >= 1000);\n\t}\n\n\tgro_normal_list(napi);\n\tclear_bit(NAPI_STATE_SCHED, &napi->state);\n}\n\nstatic void busy_poll_stop(struct napi_struct *napi, void *have_poll_lock, bool prefer_busy_poll,\n\t\t\t   u16 budget)\n{\n\tbool skip_schedule = false;\n\tunsigned long timeout;\n\tint rc;\n\n\t \n\tclear_bit(NAPI_STATE_MISSED, &napi->state);\n\tclear_bit(NAPI_STATE_IN_BUSY_POLL, &napi->state);\n\n\tlocal_bh_disable();\n\n\tif (prefer_busy_poll) {\n\t\tnapi->defer_hard_irqs_count = READ_ONCE(napi->dev->napi_defer_hard_irqs);\n\t\ttimeout = READ_ONCE(napi->dev->gro_flush_timeout);\n\t\tif (napi->defer_hard_irqs_count && timeout) {\n\t\t\thrtimer_start(&napi->timer, ns_to_ktime(timeout), HRTIMER_MODE_REL_PINNED);\n\t\t\tskip_schedule = true;\n\t\t}\n\t}\n\n\t \n\trc = napi->poll(napi, budget);\n\t \n\ttrace_napi_poll(napi, rc, budget);\n\tnetpoll_poll_unlock(have_poll_lock);\n\tif (rc == budget)\n\t\t__busy_poll_stop(napi, skip_schedule);\n\tlocal_bh_enable();\n}\n\nvoid napi_busy_loop(unsigned int napi_id,\n\t\t    bool (*loop_end)(void *, unsigned long),\n\t\t    void *loop_end_arg, bool prefer_busy_poll, u16 budget)\n{\n\tunsigned long start_time = loop_end ? busy_loop_current_time() : 0;\n\tint (*napi_poll)(struct napi_struct *napi, int budget);\n\tvoid *have_poll_lock = NULL;\n\tstruct napi_struct *napi;\n\nrestart:\n\tnapi_poll = NULL;\n\n\trcu_read_lock();\n\n\tnapi = napi_by_id(napi_id);\n\tif (!napi)\n\t\tgoto out;\n\n\tif (!IS_ENABLED(CONFIG_PREEMPT_RT))\n\t\tpreempt_disable();\n\tfor (;;) {\n\t\tint work = 0;\n\n\t\tlocal_bh_disable();\n\t\tif (!napi_poll) {\n\t\t\tunsigned long val = READ_ONCE(napi->state);\n\n\t\t\t \n\t\t\tif (val & (NAPIF_STATE_DISABLE | NAPIF_STATE_SCHED |\n\t\t\t\t   NAPIF_STATE_IN_BUSY_POLL)) {\n\t\t\t\tif (prefer_busy_poll)\n\t\t\t\t\tset_bit(NAPI_STATE_PREFER_BUSY_POLL, &napi->state);\n\t\t\t\tgoto count;\n\t\t\t}\n\t\t\tif (cmpxchg(&napi->state, val,\n\t\t\t\t    val | NAPIF_STATE_IN_BUSY_POLL |\n\t\t\t\t\t  NAPIF_STATE_SCHED) != val) {\n\t\t\t\tif (prefer_busy_poll)\n\t\t\t\t\tset_bit(NAPI_STATE_PREFER_BUSY_POLL, &napi->state);\n\t\t\t\tgoto count;\n\t\t\t}\n\t\t\thave_poll_lock = netpoll_poll_lock(napi);\n\t\t\tnapi_poll = napi->poll;\n\t\t}\n\t\twork = napi_poll(napi, budget);\n\t\ttrace_napi_poll(napi, work, budget);\n\t\tgro_normal_list(napi);\ncount:\n\t\tif (work > 0)\n\t\t\t__NET_ADD_STATS(dev_net(napi->dev),\n\t\t\t\t\tLINUX_MIB_BUSYPOLLRXPACKETS, work);\n\t\tlocal_bh_enable();\n\n\t\tif (!loop_end || loop_end(loop_end_arg, start_time))\n\t\t\tbreak;\n\n\t\tif (unlikely(need_resched())) {\n\t\t\tif (napi_poll)\n\t\t\t\tbusy_poll_stop(napi, have_poll_lock, prefer_busy_poll, budget);\n\t\t\tif (!IS_ENABLED(CONFIG_PREEMPT_RT))\n\t\t\t\tpreempt_enable();\n\t\t\trcu_read_unlock();\n\t\t\tcond_resched();\n\t\t\tif (loop_end(loop_end_arg, start_time))\n\t\t\t\treturn;\n\t\t\tgoto restart;\n\t\t}\n\t\tcpu_relax();\n\t}\n\tif (napi_poll)\n\t\tbusy_poll_stop(napi, have_poll_lock, prefer_busy_poll, budget);\n\tif (!IS_ENABLED(CONFIG_PREEMPT_RT))\n\t\tpreempt_enable();\nout:\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL(napi_busy_loop);\n\n#endif  \n\nstatic void napi_hash_add(struct napi_struct *napi)\n{\n\tif (test_bit(NAPI_STATE_NO_BUSY_POLL, &napi->state))\n\t\treturn;\n\n\tspin_lock(&napi_hash_lock);\n\n\t \n\tdo {\n\t\tif (unlikely(++napi_gen_id < MIN_NAPI_ID))\n\t\t\tnapi_gen_id = MIN_NAPI_ID;\n\t} while (napi_by_id(napi_gen_id));\n\tnapi->napi_id = napi_gen_id;\n\n\thlist_add_head_rcu(&napi->napi_hash_node,\n\t\t\t   &napi_hash[napi->napi_id % HASH_SIZE(napi_hash)]);\n\n\tspin_unlock(&napi_hash_lock);\n}\n\n \nstatic void napi_hash_del(struct napi_struct *napi)\n{\n\tspin_lock(&napi_hash_lock);\n\n\thlist_del_init_rcu(&napi->napi_hash_node);\n\n\tspin_unlock(&napi_hash_lock);\n}\n\nstatic enum hrtimer_restart napi_watchdog(struct hrtimer *timer)\n{\n\tstruct napi_struct *napi;\n\n\tnapi = container_of(timer, struct napi_struct, timer);\n\n\t \n\tif (!napi_disable_pending(napi) &&\n\t    !test_and_set_bit(NAPI_STATE_SCHED, &napi->state)) {\n\t\tclear_bit(NAPI_STATE_PREFER_BUSY_POLL, &napi->state);\n\t\t__napi_schedule_irqoff(napi);\n\t}\n\n\treturn HRTIMER_NORESTART;\n}\n\nstatic void init_gro_hash(struct napi_struct *napi)\n{\n\tint i;\n\n\tfor (i = 0; i < GRO_HASH_BUCKETS; i++) {\n\t\tINIT_LIST_HEAD(&napi->gro_hash[i].list);\n\t\tnapi->gro_hash[i].count = 0;\n\t}\n\tnapi->gro_bitmask = 0;\n}\n\nint dev_set_threaded(struct net_device *dev, bool threaded)\n{\n\tstruct napi_struct *napi;\n\tint err = 0;\n\n\tif (dev->threaded == threaded)\n\t\treturn 0;\n\n\tif (threaded) {\n\t\tlist_for_each_entry(napi, &dev->napi_list, dev_list) {\n\t\t\tif (!napi->thread) {\n\t\t\t\terr = napi_kthread_create(napi);\n\t\t\t\tif (err) {\n\t\t\t\t\tthreaded = false;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tdev->threaded = threaded;\n\n\t \n\tsmp_mb__before_atomic();\n\n\t \n\tlist_for_each_entry(napi, &dev->napi_list, dev_list)\n\t\tassign_bit(NAPI_STATE_THREADED, &napi->state, threaded);\n\n\treturn err;\n}\nEXPORT_SYMBOL(dev_set_threaded);\n\nvoid netif_napi_add_weight(struct net_device *dev, struct napi_struct *napi,\n\t\t\t   int (*poll)(struct napi_struct *, int), int weight)\n{\n\tif (WARN_ON(test_and_set_bit(NAPI_STATE_LISTED, &napi->state)))\n\t\treturn;\n\n\tINIT_LIST_HEAD(&napi->poll_list);\n\tINIT_HLIST_NODE(&napi->napi_hash_node);\n\thrtimer_init(&napi->timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL_PINNED);\n\tnapi->timer.function = napi_watchdog;\n\tinit_gro_hash(napi);\n\tnapi->skb = NULL;\n\tINIT_LIST_HEAD(&napi->rx_list);\n\tnapi->rx_count = 0;\n\tnapi->poll = poll;\n\tif (weight > NAPI_POLL_WEIGHT)\n\t\tnetdev_err_once(dev, \"%s() called with weight %d\\n\", __func__,\n\t\t\t\tweight);\n\tnapi->weight = weight;\n\tnapi->dev = dev;\n#ifdef CONFIG_NETPOLL\n\tnapi->poll_owner = -1;\n#endif\n\tnapi->list_owner = -1;\n\tset_bit(NAPI_STATE_SCHED, &napi->state);\n\tset_bit(NAPI_STATE_NPSVC, &napi->state);\n\tlist_add_rcu(&napi->dev_list, &dev->napi_list);\n\tnapi_hash_add(napi);\n\tnapi_get_frags_check(napi);\n\t \n\tif (dev->threaded && napi_kthread_create(napi))\n\t\tdev->threaded = 0;\n}\nEXPORT_SYMBOL(netif_napi_add_weight);\n\nvoid napi_disable(struct napi_struct *n)\n{\n\tunsigned long val, new;\n\n\tmight_sleep();\n\tset_bit(NAPI_STATE_DISABLE, &n->state);\n\n\tval = READ_ONCE(n->state);\n\tdo {\n\t\twhile (val & (NAPIF_STATE_SCHED | NAPIF_STATE_NPSVC)) {\n\t\t\tusleep_range(20, 200);\n\t\t\tval = READ_ONCE(n->state);\n\t\t}\n\n\t\tnew = val | NAPIF_STATE_SCHED | NAPIF_STATE_NPSVC;\n\t\tnew &= ~(NAPIF_STATE_THREADED | NAPIF_STATE_PREFER_BUSY_POLL);\n\t} while (!try_cmpxchg(&n->state, &val, new));\n\n\thrtimer_cancel(&n->timer);\n\n\tclear_bit(NAPI_STATE_DISABLE, &n->state);\n}\nEXPORT_SYMBOL(napi_disable);\n\n \nvoid napi_enable(struct napi_struct *n)\n{\n\tunsigned long new, val = READ_ONCE(n->state);\n\n\tdo {\n\t\tBUG_ON(!test_bit(NAPI_STATE_SCHED, &val));\n\n\t\tnew = val & ~(NAPIF_STATE_SCHED | NAPIF_STATE_NPSVC);\n\t\tif (n->dev->threaded && n->thread)\n\t\t\tnew |= NAPIF_STATE_THREADED;\n\t} while (!try_cmpxchg(&n->state, &val, new));\n}\nEXPORT_SYMBOL(napi_enable);\n\nstatic void flush_gro_hash(struct napi_struct *napi)\n{\n\tint i;\n\n\tfor (i = 0; i < GRO_HASH_BUCKETS; i++) {\n\t\tstruct sk_buff *skb, *n;\n\n\t\tlist_for_each_entry_safe(skb, n, &napi->gro_hash[i].list, list)\n\t\t\tkfree_skb(skb);\n\t\tnapi->gro_hash[i].count = 0;\n\t}\n}\n\n \nvoid __netif_napi_del(struct napi_struct *napi)\n{\n\tif (!test_and_clear_bit(NAPI_STATE_LISTED, &napi->state))\n\t\treturn;\n\n\tnapi_hash_del(napi);\n\tlist_del_rcu(&napi->dev_list);\n\tnapi_free_frags(napi);\n\n\tflush_gro_hash(napi);\n\tnapi->gro_bitmask = 0;\n\n\tif (napi->thread) {\n\t\tkthread_stop(napi->thread);\n\t\tnapi->thread = NULL;\n\t}\n}\nEXPORT_SYMBOL(__netif_napi_del);\n\nstatic int __napi_poll(struct napi_struct *n, bool *repoll)\n{\n\tint work, weight;\n\n\tweight = n->weight;\n\n\t \n\twork = 0;\n\tif (test_bit(NAPI_STATE_SCHED, &n->state)) {\n\t\twork = n->poll(n, weight);\n\t\ttrace_napi_poll(n, work, weight);\n\t}\n\n\tif (unlikely(work > weight))\n\t\tnetdev_err_once(n->dev, \"NAPI poll function %pS returned %d, exceeding its budget of %d.\\n\",\n\t\t\t\tn->poll, work, weight);\n\n\tif (likely(work < weight))\n\t\treturn work;\n\n\t \n\tif (unlikely(napi_disable_pending(n))) {\n\t\tnapi_complete(n);\n\t\treturn work;\n\t}\n\n\t \n\tif (napi_prefer_busy_poll(n)) {\n\t\tif (napi_complete_done(n, work)) {\n\t\t\t \n\t\t\tnapi_schedule(n);\n\t\t}\n\t\treturn work;\n\t}\n\n\tif (n->gro_bitmask) {\n\t\t \n\t\tnapi_gro_flush(n, HZ >= 1000);\n\t}\n\n\tgro_normal_list(n);\n\n\t \n\tif (unlikely(!list_empty(&n->poll_list))) {\n\t\tpr_warn_once(\"%s: Budget exhausted after napi rescheduled\\n\",\n\t\t\t     n->dev ? n->dev->name : \"backlog\");\n\t\treturn work;\n\t}\n\n\t*repoll = true;\n\n\treturn work;\n}\n\nstatic int napi_poll(struct napi_struct *n, struct list_head *repoll)\n{\n\tbool do_repoll = false;\n\tvoid *have;\n\tint work;\n\n\tlist_del_init(&n->poll_list);\n\n\thave = netpoll_poll_lock(n);\n\n\twork = __napi_poll(n, &do_repoll);\n\n\tif (do_repoll)\n\t\tlist_add_tail(&n->poll_list, repoll);\n\n\tnetpoll_poll_unlock(have);\n\n\treturn work;\n}\n\nstatic int napi_thread_wait(struct napi_struct *napi)\n{\n\tbool woken = false;\n\n\tset_current_state(TASK_INTERRUPTIBLE);\n\n\twhile (!kthread_should_stop()) {\n\t\t \n\t\tif (test_bit(NAPI_STATE_SCHED_THREADED, &napi->state) || woken) {\n\t\t\tWARN_ON(!list_empty(&napi->poll_list));\n\t\t\t__set_current_state(TASK_RUNNING);\n\t\t\treturn 0;\n\t\t}\n\n\t\tschedule();\n\t\t \n\t\twoken = true;\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t}\n\t__set_current_state(TASK_RUNNING);\n\n\treturn -1;\n}\n\nstatic void skb_defer_free_flush(struct softnet_data *sd)\n{\n\tstruct sk_buff *skb, *next;\n\n\t \n\tif (!READ_ONCE(sd->defer_list))\n\t\treturn;\n\n\tspin_lock(&sd->defer_lock);\n\tskb = sd->defer_list;\n\tsd->defer_list = NULL;\n\tsd->defer_count = 0;\n\tspin_unlock(&sd->defer_lock);\n\n\twhile (skb != NULL) {\n\t\tnext = skb->next;\n\t\tnapi_consume_skb(skb, 1);\n\t\tskb = next;\n\t}\n}\n\nstatic int napi_threaded_poll(void *data)\n{\n\tstruct napi_struct *napi = data;\n\tstruct softnet_data *sd;\n\tvoid *have;\n\n\twhile (!napi_thread_wait(napi)) {\n\t\tfor (;;) {\n\t\t\tbool repoll = false;\n\n\t\t\tlocal_bh_disable();\n\t\t\tsd = this_cpu_ptr(&softnet_data);\n\t\t\tsd->in_napi_threaded_poll = true;\n\n\t\t\thave = netpoll_poll_lock(napi);\n\t\t\t__napi_poll(napi, &repoll);\n\t\t\tnetpoll_poll_unlock(have);\n\n\t\t\tsd->in_napi_threaded_poll = false;\n\t\t\tbarrier();\n\n\t\t\tif (sd_has_rps_ipi_waiting(sd)) {\n\t\t\t\tlocal_irq_disable();\n\t\t\t\tnet_rps_action_and_irq_enable(sd);\n\t\t\t}\n\t\t\tskb_defer_free_flush(sd);\n\t\t\tlocal_bh_enable();\n\n\t\t\tif (!repoll)\n\t\t\t\tbreak;\n\n\t\t\tcond_resched();\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic __latent_entropy void net_rx_action(struct softirq_action *h)\n{\n\tstruct softnet_data *sd = this_cpu_ptr(&softnet_data);\n\tunsigned long time_limit = jiffies +\n\t\tusecs_to_jiffies(READ_ONCE(netdev_budget_usecs));\n\tint budget = READ_ONCE(netdev_budget);\n\tLIST_HEAD(list);\n\tLIST_HEAD(repoll);\n\nstart:\n\tsd->in_net_rx_action = true;\n\tlocal_irq_disable();\n\tlist_splice_init(&sd->poll_list, &list);\n\tlocal_irq_enable();\n\n\tfor (;;) {\n\t\tstruct napi_struct *n;\n\n\t\tskb_defer_free_flush(sd);\n\n\t\tif (list_empty(&list)) {\n\t\t\tif (list_empty(&repoll)) {\n\t\t\t\tsd->in_net_rx_action = false;\n\t\t\t\tbarrier();\n\t\t\t\t \n\t\t\t\tif (!list_empty(&sd->poll_list))\n\t\t\t\t\tgoto start;\n\t\t\t\tif (!sd_has_rps_ipi_waiting(sd))\n\t\t\t\t\tgoto end;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\n\t\tn = list_first_entry(&list, struct napi_struct, poll_list);\n\t\tbudget -= napi_poll(n, &repoll);\n\n\t\t \n\t\tif (unlikely(budget <= 0 ||\n\t\t\t     time_after_eq(jiffies, time_limit))) {\n\t\t\tsd->time_squeeze++;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tlocal_irq_disable();\n\n\tlist_splice_tail_init(&sd->poll_list, &list);\n\tlist_splice_tail(&repoll, &list);\n\tlist_splice(&list, &sd->poll_list);\n\tif (!list_empty(&sd->poll_list))\n\t\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n\telse\n\t\tsd->in_net_rx_action = false;\n\n\tnet_rps_action_and_irq_enable(sd);\nend:;\n}\n\nstruct netdev_adjacent {\n\tstruct net_device *dev;\n\tnetdevice_tracker dev_tracker;\n\n\t \n\tbool master;\n\n\t \n\tbool ignore;\n\n\t \n\tu16 ref_nr;\n\n\t \n\tvoid *private;\n\n\tstruct list_head list;\n\tstruct rcu_head rcu;\n};\n\nstatic struct netdev_adjacent *__netdev_find_adj(struct net_device *adj_dev,\n\t\t\t\t\t\t struct list_head *adj_list)\n{\n\tstruct netdev_adjacent *adj;\n\n\tlist_for_each_entry(adj, adj_list, list) {\n\t\tif (adj->dev == adj_dev)\n\t\t\treturn adj;\n\t}\n\treturn NULL;\n}\n\nstatic int ____netdev_has_upper_dev(struct net_device *upper_dev,\n\t\t\t\t    struct netdev_nested_priv *priv)\n{\n\tstruct net_device *dev = (struct net_device *)priv->data;\n\n\treturn upper_dev == dev;\n}\n\n \nbool netdev_has_upper_dev(struct net_device *dev,\n\t\t\t  struct net_device *upper_dev)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.data = (void *)upper_dev,\n\t};\n\n\tASSERT_RTNL();\n\n\treturn netdev_walk_all_upper_dev_rcu(dev, ____netdev_has_upper_dev,\n\t\t\t\t\t     &priv);\n}\nEXPORT_SYMBOL(netdev_has_upper_dev);\n\n \n\nbool netdev_has_upper_dev_all_rcu(struct net_device *dev,\n\t\t\t\t  struct net_device *upper_dev)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.data = (void *)upper_dev,\n\t};\n\n\treturn !!netdev_walk_all_upper_dev_rcu(dev, ____netdev_has_upper_dev,\n\t\t\t\t\t       &priv);\n}\nEXPORT_SYMBOL(netdev_has_upper_dev_all_rcu);\n\n \nbool netdev_has_any_upper_dev(struct net_device *dev)\n{\n\tASSERT_RTNL();\n\n\treturn !list_empty(&dev->adj_list.upper);\n}\nEXPORT_SYMBOL(netdev_has_any_upper_dev);\n\n \nstruct net_device *netdev_master_upper_dev_get(struct net_device *dev)\n{\n\tstruct netdev_adjacent *upper;\n\n\tASSERT_RTNL();\n\n\tif (list_empty(&dev->adj_list.upper))\n\t\treturn NULL;\n\n\tupper = list_first_entry(&dev->adj_list.upper,\n\t\t\t\t struct netdev_adjacent, list);\n\tif (likely(upper->master))\n\t\treturn upper->dev;\n\treturn NULL;\n}\nEXPORT_SYMBOL(netdev_master_upper_dev_get);\n\nstatic struct net_device *__netdev_master_upper_dev_get(struct net_device *dev)\n{\n\tstruct netdev_adjacent *upper;\n\n\tASSERT_RTNL();\n\n\tif (list_empty(&dev->adj_list.upper))\n\t\treturn NULL;\n\n\tupper = list_first_entry(&dev->adj_list.upper,\n\t\t\t\t struct netdev_adjacent, list);\n\tif (likely(upper->master) && !upper->ignore)\n\t\treturn upper->dev;\n\treturn NULL;\n}\n\n \nstatic bool netdev_has_any_lower_dev(struct net_device *dev)\n{\n\tASSERT_RTNL();\n\n\treturn !list_empty(&dev->adj_list.lower);\n}\n\nvoid *netdev_adjacent_get_private(struct list_head *adj_list)\n{\n\tstruct netdev_adjacent *adj;\n\n\tadj = list_entry(adj_list, struct netdev_adjacent, list);\n\n\treturn adj->private;\n}\nEXPORT_SYMBOL(netdev_adjacent_get_private);\n\n \nstruct net_device *netdev_upper_get_next_dev_rcu(struct net_device *dev,\n\t\t\t\t\t\t struct list_head **iter)\n{\n\tstruct netdev_adjacent *upper;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held() && !lockdep_rtnl_is_held());\n\n\tupper = list_entry_rcu((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&upper->list == &dev->adj_list.upper)\n\t\treturn NULL;\n\n\t*iter = &upper->list;\n\n\treturn upper->dev;\n}\nEXPORT_SYMBOL(netdev_upper_get_next_dev_rcu);\n\nstatic struct net_device *__netdev_next_upper_dev(struct net_device *dev,\n\t\t\t\t\t\t  struct list_head **iter,\n\t\t\t\t\t\t  bool *ignore)\n{\n\tstruct netdev_adjacent *upper;\n\n\tupper = list_entry((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&upper->list == &dev->adj_list.upper)\n\t\treturn NULL;\n\n\t*iter = &upper->list;\n\t*ignore = upper->ignore;\n\n\treturn upper->dev;\n}\n\nstatic struct net_device *netdev_next_upper_dev_rcu(struct net_device *dev,\n\t\t\t\t\t\t    struct list_head **iter)\n{\n\tstruct netdev_adjacent *upper;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held() && !lockdep_rtnl_is_held());\n\n\tupper = list_entry_rcu((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&upper->list == &dev->adj_list.upper)\n\t\treturn NULL;\n\n\t*iter = &upper->list;\n\n\treturn upper->dev;\n}\n\nstatic int __netdev_walk_all_upper_dev(struct net_device *dev,\n\t\t\t\t       int (*fn)(struct net_device *dev,\n\t\t\t\t\t struct netdev_nested_priv *priv),\n\t\t\t\t       struct netdev_nested_priv *priv)\n{\n\tstruct net_device *udev, *next, *now, *dev_stack[MAX_NEST_DEV + 1];\n\tstruct list_head *niter, *iter, *iter_stack[MAX_NEST_DEV + 1];\n\tint ret, cur = 0;\n\tbool ignore;\n\n\tnow = dev;\n\titer = &dev->adj_list.upper;\n\n\twhile (1) {\n\t\tif (now != dev) {\n\t\t\tret = fn(now, priv);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tnext = NULL;\n\t\twhile (1) {\n\t\t\tudev = __netdev_next_upper_dev(now, &iter, &ignore);\n\t\t\tif (!udev)\n\t\t\t\tbreak;\n\t\t\tif (ignore)\n\t\t\t\tcontinue;\n\n\t\t\tnext = udev;\n\t\t\tniter = &udev->adj_list.upper;\n\t\t\tdev_stack[cur] = now;\n\t\t\titer_stack[cur++] = iter;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!next) {\n\t\t\tif (!cur)\n\t\t\t\treturn 0;\n\t\t\tnext = dev_stack[--cur];\n\t\t\tniter = iter_stack[cur];\n\t\t}\n\n\t\tnow = next;\n\t\titer = niter;\n\t}\n\n\treturn 0;\n}\n\nint netdev_walk_all_upper_dev_rcu(struct net_device *dev,\n\t\t\t\t  int (*fn)(struct net_device *dev,\n\t\t\t\t\t    struct netdev_nested_priv *priv),\n\t\t\t\t  struct netdev_nested_priv *priv)\n{\n\tstruct net_device *udev, *next, *now, *dev_stack[MAX_NEST_DEV + 1];\n\tstruct list_head *niter, *iter, *iter_stack[MAX_NEST_DEV + 1];\n\tint ret, cur = 0;\n\n\tnow = dev;\n\titer = &dev->adj_list.upper;\n\n\twhile (1) {\n\t\tif (now != dev) {\n\t\t\tret = fn(now, priv);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tnext = NULL;\n\t\twhile (1) {\n\t\t\tudev = netdev_next_upper_dev_rcu(now, &iter);\n\t\t\tif (!udev)\n\t\t\t\tbreak;\n\n\t\t\tnext = udev;\n\t\t\tniter = &udev->adj_list.upper;\n\t\t\tdev_stack[cur] = now;\n\t\t\titer_stack[cur++] = iter;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!next) {\n\t\t\tif (!cur)\n\t\t\t\treturn 0;\n\t\t\tnext = dev_stack[--cur];\n\t\t\tniter = iter_stack[cur];\n\t\t}\n\n\t\tnow = next;\n\t\titer = niter;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(netdev_walk_all_upper_dev_rcu);\n\nstatic bool __netdev_has_upper_dev(struct net_device *dev,\n\t\t\t\t   struct net_device *upper_dev)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = 0,\n\t\t.data = (void *)upper_dev,\n\t};\n\n\tASSERT_RTNL();\n\n\treturn __netdev_walk_all_upper_dev(dev, ____netdev_has_upper_dev,\n\t\t\t\t\t   &priv);\n}\n\n \nvoid *netdev_lower_get_next_private(struct net_device *dev,\n\t\t\t\t    struct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry(*iter, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = lower->list.next;\n\n\treturn lower->private;\n}\nEXPORT_SYMBOL(netdev_lower_get_next_private);\n\n \nvoid *netdev_lower_get_next_private_rcu(struct net_device *dev,\n\t\t\t\t\tstruct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held() && !rcu_read_lock_bh_held());\n\n\tlower = list_entry_rcu((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = &lower->list;\n\n\treturn lower->private;\n}\nEXPORT_SYMBOL(netdev_lower_get_next_private_rcu);\n\n \nvoid *netdev_lower_get_next(struct net_device *dev, struct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry(*iter, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = lower->list.next;\n\n\treturn lower->dev;\n}\nEXPORT_SYMBOL(netdev_lower_get_next);\n\nstatic struct net_device *netdev_next_lower_dev(struct net_device *dev,\n\t\t\t\t\t\tstruct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = &lower->list;\n\n\treturn lower->dev;\n}\n\nstatic struct net_device *__netdev_next_lower_dev(struct net_device *dev,\n\t\t\t\t\t\t  struct list_head **iter,\n\t\t\t\t\t\t  bool *ignore)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = &lower->list;\n\t*ignore = lower->ignore;\n\n\treturn lower->dev;\n}\n\nint netdev_walk_all_lower_dev(struct net_device *dev,\n\t\t\t      int (*fn)(struct net_device *dev,\n\t\t\t\t\tstruct netdev_nested_priv *priv),\n\t\t\t      struct netdev_nested_priv *priv)\n{\n\tstruct net_device *ldev, *next, *now, *dev_stack[MAX_NEST_DEV + 1];\n\tstruct list_head *niter, *iter, *iter_stack[MAX_NEST_DEV + 1];\n\tint ret, cur = 0;\n\n\tnow = dev;\n\titer = &dev->adj_list.lower;\n\n\twhile (1) {\n\t\tif (now != dev) {\n\t\t\tret = fn(now, priv);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tnext = NULL;\n\t\twhile (1) {\n\t\t\tldev = netdev_next_lower_dev(now, &iter);\n\t\t\tif (!ldev)\n\t\t\t\tbreak;\n\n\t\t\tnext = ldev;\n\t\t\tniter = &ldev->adj_list.lower;\n\t\t\tdev_stack[cur] = now;\n\t\t\titer_stack[cur++] = iter;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!next) {\n\t\t\tif (!cur)\n\t\t\t\treturn 0;\n\t\t\tnext = dev_stack[--cur];\n\t\t\tniter = iter_stack[cur];\n\t\t}\n\n\t\tnow = next;\n\t\titer = niter;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(netdev_walk_all_lower_dev);\n\nstatic int __netdev_walk_all_lower_dev(struct net_device *dev,\n\t\t\t\t       int (*fn)(struct net_device *dev,\n\t\t\t\t\t struct netdev_nested_priv *priv),\n\t\t\t\t       struct netdev_nested_priv *priv)\n{\n\tstruct net_device *ldev, *next, *now, *dev_stack[MAX_NEST_DEV + 1];\n\tstruct list_head *niter, *iter, *iter_stack[MAX_NEST_DEV + 1];\n\tint ret, cur = 0;\n\tbool ignore;\n\n\tnow = dev;\n\titer = &dev->adj_list.lower;\n\n\twhile (1) {\n\t\tif (now != dev) {\n\t\t\tret = fn(now, priv);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tnext = NULL;\n\t\twhile (1) {\n\t\t\tldev = __netdev_next_lower_dev(now, &iter, &ignore);\n\t\t\tif (!ldev)\n\t\t\t\tbreak;\n\t\t\tif (ignore)\n\t\t\t\tcontinue;\n\n\t\t\tnext = ldev;\n\t\t\tniter = &ldev->adj_list.lower;\n\t\t\tdev_stack[cur] = now;\n\t\t\titer_stack[cur++] = iter;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!next) {\n\t\t\tif (!cur)\n\t\t\t\treturn 0;\n\t\t\tnext = dev_stack[--cur];\n\t\t\tniter = iter_stack[cur];\n\t\t}\n\n\t\tnow = next;\n\t\titer = niter;\n\t}\n\n\treturn 0;\n}\n\nstruct net_device *netdev_next_lower_dev_rcu(struct net_device *dev,\n\t\t\t\t\t     struct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry_rcu((*iter)->next, struct netdev_adjacent, list);\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = &lower->list;\n\n\treturn lower->dev;\n}\nEXPORT_SYMBOL(netdev_next_lower_dev_rcu);\n\nstatic u8 __netdev_upper_depth(struct net_device *dev)\n{\n\tstruct net_device *udev;\n\tstruct list_head *iter;\n\tu8 max_depth = 0;\n\tbool ignore;\n\n\tfor (iter = &dev->adj_list.upper,\n\t     udev = __netdev_next_upper_dev(dev, &iter, &ignore);\n\t     udev;\n\t     udev = __netdev_next_upper_dev(dev, &iter, &ignore)) {\n\t\tif (ignore)\n\t\t\tcontinue;\n\t\tif (max_depth < udev->upper_level)\n\t\t\tmax_depth = udev->upper_level;\n\t}\n\n\treturn max_depth;\n}\n\nstatic u8 __netdev_lower_depth(struct net_device *dev)\n{\n\tstruct net_device *ldev;\n\tstruct list_head *iter;\n\tu8 max_depth = 0;\n\tbool ignore;\n\n\tfor (iter = &dev->adj_list.lower,\n\t     ldev = __netdev_next_lower_dev(dev, &iter, &ignore);\n\t     ldev;\n\t     ldev = __netdev_next_lower_dev(dev, &iter, &ignore)) {\n\t\tif (ignore)\n\t\t\tcontinue;\n\t\tif (max_depth < ldev->lower_level)\n\t\t\tmax_depth = ldev->lower_level;\n\t}\n\n\treturn max_depth;\n}\n\nstatic int __netdev_update_upper_level(struct net_device *dev,\n\t\t\t\t       struct netdev_nested_priv *__unused)\n{\n\tdev->upper_level = __netdev_upper_depth(dev) + 1;\n\treturn 0;\n}\n\n#ifdef CONFIG_LOCKDEP\nstatic LIST_HEAD(net_unlink_list);\n\nstatic void net_unlink_todo(struct net_device *dev)\n{\n\tif (list_empty(&dev->unlink_list))\n\t\tlist_add_tail(&dev->unlink_list, &net_unlink_list);\n}\n#endif\n\nstatic int __netdev_update_lower_level(struct net_device *dev,\n\t\t\t\t       struct netdev_nested_priv *priv)\n{\n\tdev->lower_level = __netdev_lower_depth(dev) + 1;\n\n#ifdef CONFIG_LOCKDEP\n\tif (!priv)\n\t\treturn 0;\n\n\tif (priv->flags & NESTED_SYNC_IMM)\n\t\tdev->nested_level = dev->lower_level - 1;\n\tif (priv->flags & NESTED_SYNC_TODO)\n\t\tnet_unlink_todo(dev);\n#endif\n\treturn 0;\n}\n\nint netdev_walk_all_lower_dev_rcu(struct net_device *dev,\n\t\t\t\t  int (*fn)(struct net_device *dev,\n\t\t\t\t\t    struct netdev_nested_priv *priv),\n\t\t\t\t  struct netdev_nested_priv *priv)\n{\n\tstruct net_device *ldev, *next, *now, *dev_stack[MAX_NEST_DEV + 1];\n\tstruct list_head *niter, *iter, *iter_stack[MAX_NEST_DEV + 1];\n\tint ret, cur = 0;\n\n\tnow = dev;\n\titer = &dev->adj_list.lower;\n\n\twhile (1) {\n\t\tif (now != dev) {\n\t\t\tret = fn(now, priv);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tnext = NULL;\n\t\twhile (1) {\n\t\t\tldev = netdev_next_lower_dev_rcu(now, &iter);\n\t\t\tif (!ldev)\n\t\t\t\tbreak;\n\n\t\t\tnext = ldev;\n\t\t\tniter = &ldev->adj_list.lower;\n\t\t\tdev_stack[cur] = now;\n\t\t\titer_stack[cur++] = iter;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!next) {\n\t\t\tif (!cur)\n\t\t\t\treturn 0;\n\t\t\tnext = dev_stack[--cur];\n\t\t\tniter = iter_stack[cur];\n\t\t}\n\n\t\tnow = next;\n\t\titer = niter;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(netdev_walk_all_lower_dev_rcu);\n\n \nvoid *netdev_lower_get_first_private_rcu(struct net_device *dev)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_first_or_null_rcu(&dev->adj_list.lower,\n\t\t\tstruct netdev_adjacent, list);\n\tif (lower)\n\t\treturn lower->private;\n\treturn NULL;\n}\nEXPORT_SYMBOL(netdev_lower_get_first_private_rcu);\n\n \nstruct net_device *netdev_master_upper_dev_get_rcu(struct net_device *dev)\n{\n\tstruct netdev_adjacent *upper;\n\n\tupper = list_first_or_null_rcu(&dev->adj_list.upper,\n\t\t\t\t       struct netdev_adjacent, list);\n\tif (upper && likely(upper->master))\n\t\treturn upper->dev;\n\treturn NULL;\n}\nEXPORT_SYMBOL(netdev_master_upper_dev_get_rcu);\n\nstatic int netdev_adjacent_sysfs_add(struct net_device *dev,\n\t\t\t      struct net_device *adj_dev,\n\t\t\t      struct list_head *dev_list)\n{\n\tchar linkname[IFNAMSIZ+7];\n\n\tsprintf(linkname, dev_list == &dev->adj_list.upper ?\n\t\t\"upper_%s\" : \"lower_%s\", adj_dev->name);\n\treturn sysfs_create_link(&(dev->dev.kobj), &(adj_dev->dev.kobj),\n\t\t\t\t linkname);\n}\nstatic void netdev_adjacent_sysfs_del(struct net_device *dev,\n\t\t\t       char *name,\n\t\t\t       struct list_head *dev_list)\n{\n\tchar linkname[IFNAMSIZ+7];\n\n\tsprintf(linkname, dev_list == &dev->adj_list.upper ?\n\t\t\"upper_%s\" : \"lower_%s\", name);\n\tsysfs_remove_link(&(dev->dev.kobj), linkname);\n}\n\nstatic inline bool netdev_adjacent_is_neigh_list(struct net_device *dev,\n\t\t\t\t\t\t struct net_device *adj_dev,\n\t\t\t\t\t\t struct list_head *dev_list)\n{\n\treturn (dev_list == &dev->adj_list.upper ||\n\t\tdev_list == &dev->adj_list.lower) &&\n\t\tnet_eq(dev_net(dev), dev_net(adj_dev));\n}\n\nstatic int __netdev_adjacent_dev_insert(struct net_device *dev,\n\t\t\t\t\tstruct net_device *adj_dev,\n\t\t\t\t\tstruct list_head *dev_list,\n\t\t\t\t\tvoid *private, bool master)\n{\n\tstruct netdev_adjacent *adj;\n\tint ret;\n\n\tadj = __netdev_find_adj(adj_dev, dev_list);\n\n\tif (adj) {\n\t\tadj->ref_nr += 1;\n\t\tpr_debug(\"Insert adjacency: dev %s adj_dev %s adj->ref_nr %d\\n\",\n\t\t\t dev->name, adj_dev->name, adj->ref_nr);\n\n\t\treturn 0;\n\t}\n\n\tadj = kmalloc(sizeof(*adj), GFP_KERNEL);\n\tif (!adj)\n\t\treturn -ENOMEM;\n\n\tadj->dev = adj_dev;\n\tadj->master = master;\n\tadj->ref_nr = 1;\n\tadj->private = private;\n\tadj->ignore = false;\n\tnetdev_hold(adj_dev, &adj->dev_tracker, GFP_KERNEL);\n\n\tpr_debug(\"Insert adjacency: dev %s adj_dev %s adj->ref_nr %d; dev_hold on %s\\n\",\n\t\t dev->name, adj_dev->name, adj->ref_nr, adj_dev->name);\n\n\tif (netdev_adjacent_is_neigh_list(dev, adj_dev, dev_list)) {\n\t\tret = netdev_adjacent_sysfs_add(dev, adj_dev, dev_list);\n\t\tif (ret)\n\t\t\tgoto free_adj;\n\t}\n\n\t \n\tif (master) {\n\t\tret = sysfs_create_link(&(dev->dev.kobj),\n\t\t\t\t\t&(adj_dev->dev.kobj), \"master\");\n\t\tif (ret)\n\t\t\tgoto remove_symlinks;\n\n\t\tlist_add_rcu(&adj->list, dev_list);\n\t} else {\n\t\tlist_add_tail_rcu(&adj->list, dev_list);\n\t}\n\n\treturn 0;\n\nremove_symlinks:\n\tif (netdev_adjacent_is_neigh_list(dev, adj_dev, dev_list))\n\t\tnetdev_adjacent_sysfs_del(dev, adj_dev->name, dev_list);\nfree_adj:\n\tnetdev_put(adj_dev, &adj->dev_tracker);\n\tkfree(adj);\n\n\treturn ret;\n}\n\nstatic void __netdev_adjacent_dev_remove(struct net_device *dev,\n\t\t\t\t\t struct net_device *adj_dev,\n\t\t\t\t\t u16 ref_nr,\n\t\t\t\t\t struct list_head *dev_list)\n{\n\tstruct netdev_adjacent *adj;\n\n\tpr_debug(\"Remove adjacency: dev %s adj_dev %s ref_nr %d\\n\",\n\t\t dev->name, adj_dev->name, ref_nr);\n\n\tadj = __netdev_find_adj(adj_dev, dev_list);\n\n\tif (!adj) {\n\t\tpr_err(\"Adjacency does not exist for device %s from %s\\n\",\n\t\t       dev->name, adj_dev->name);\n\t\tWARN_ON(1);\n\t\treturn;\n\t}\n\n\tif (adj->ref_nr > ref_nr) {\n\t\tpr_debug(\"adjacency: %s to %s ref_nr - %d = %d\\n\",\n\t\t\t dev->name, adj_dev->name, ref_nr,\n\t\t\t adj->ref_nr - ref_nr);\n\t\tadj->ref_nr -= ref_nr;\n\t\treturn;\n\t}\n\n\tif (adj->master)\n\t\tsysfs_remove_link(&(dev->dev.kobj), \"master\");\n\n\tif (netdev_adjacent_is_neigh_list(dev, adj_dev, dev_list))\n\t\tnetdev_adjacent_sysfs_del(dev, adj_dev->name, dev_list);\n\n\tlist_del_rcu(&adj->list);\n\tpr_debug(\"adjacency: dev_put for %s, because link removed from %s to %s\\n\",\n\t\t adj_dev->name, dev->name, adj_dev->name);\n\tnetdev_put(adj_dev, &adj->dev_tracker);\n\tkfree_rcu(adj, rcu);\n}\n\nstatic int __netdev_adjacent_dev_link_lists(struct net_device *dev,\n\t\t\t\t\t    struct net_device *upper_dev,\n\t\t\t\t\t    struct list_head *up_list,\n\t\t\t\t\t    struct list_head *down_list,\n\t\t\t\t\t    void *private, bool master)\n{\n\tint ret;\n\n\tret = __netdev_adjacent_dev_insert(dev, upper_dev, up_list,\n\t\t\t\t\t   private, master);\n\tif (ret)\n\t\treturn ret;\n\n\tret = __netdev_adjacent_dev_insert(upper_dev, dev, down_list,\n\t\t\t\t\t   private, false);\n\tif (ret) {\n\t\t__netdev_adjacent_dev_remove(dev, upper_dev, 1, up_list);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic void __netdev_adjacent_dev_unlink_lists(struct net_device *dev,\n\t\t\t\t\t       struct net_device *upper_dev,\n\t\t\t\t\t       u16 ref_nr,\n\t\t\t\t\t       struct list_head *up_list,\n\t\t\t\t\t       struct list_head *down_list)\n{\n\t__netdev_adjacent_dev_remove(dev, upper_dev, ref_nr, up_list);\n\t__netdev_adjacent_dev_remove(upper_dev, dev, ref_nr, down_list);\n}\n\nstatic int __netdev_adjacent_dev_link_neighbour(struct net_device *dev,\n\t\t\t\t\t\tstruct net_device *upper_dev,\n\t\t\t\t\t\tvoid *private, bool master)\n{\n\treturn __netdev_adjacent_dev_link_lists(dev, upper_dev,\n\t\t\t\t\t\t&dev->adj_list.upper,\n\t\t\t\t\t\t&upper_dev->adj_list.lower,\n\t\t\t\t\t\tprivate, master);\n}\n\nstatic void __netdev_adjacent_dev_unlink_neighbour(struct net_device *dev,\n\t\t\t\t\t\t   struct net_device *upper_dev)\n{\n\t__netdev_adjacent_dev_unlink_lists(dev, upper_dev, 1,\n\t\t\t\t\t   &dev->adj_list.upper,\n\t\t\t\t\t   &upper_dev->adj_list.lower);\n}\n\nstatic int __netdev_upper_dev_link(struct net_device *dev,\n\t\t\t\t   struct net_device *upper_dev, bool master,\n\t\t\t\t   void *upper_priv, void *upper_info,\n\t\t\t\t   struct netdev_nested_priv *priv,\n\t\t\t\t   struct netlink_ext_ack *extack)\n{\n\tstruct netdev_notifier_changeupper_info changeupper_info = {\n\t\t.info = {\n\t\t\t.dev = dev,\n\t\t\t.extack = extack,\n\t\t},\n\t\t.upper_dev = upper_dev,\n\t\t.master = master,\n\t\t.linking = true,\n\t\t.upper_info = upper_info,\n\t};\n\tstruct net_device *master_dev;\n\tint ret = 0;\n\n\tASSERT_RTNL();\n\n\tif (dev == upper_dev)\n\t\treturn -EBUSY;\n\n\t \n\tif (__netdev_has_upper_dev(upper_dev, dev))\n\t\treturn -EBUSY;\n\n\tif ((dev->lower_level + upper_dev->upper_level) > MAX_NEST_DEV)\n\t\treturn -EMLINK;\n\n\tif (!master) {\n\t\tif (__netdev_has_upper_dev(dev, upper_dev))\n\t\t\treturn -EEXIST;\n\t} else {\n\t\tmaster_dev = __netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\treturn master_dev == upper_dev ? -EEXIST : -EBUSY;\n\t}\n\n\tret = call_netdevice_notifiers_info(NETDEV_PRECHANGEUPPER,\n\t\t\t\t\t    &changeupper_info.info);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\treturn ret;\n\n\tret = __netdev_adjacent_dev_link_neighbour(dev, upper_dev, upper_priv,\n\t\t\t\t\t\t   master);\n\tif (ret)\n\t\treturn ret;\n\n\tret = call_netdevice_notifiers_info(NETDEV_CHANGEUPPER,\n\t\t\t\t\t    &changeupper_info.info);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\tgoto rollback;\n\n\t__netdev_update_upper_level(dev, NULL);\n\t__netdev_walk_all_lower_dev(dev, __netdev_update_upper_level, NULL);\n\n\t__netdev_update_lower_level(upper_dev, priv);\n\t__netdev_walk_all_upper_dev(upper_dev, __netdev_update_lower_level,\n\t\t\t\t    priv);\n\n\treturn 0;\n\nrollback:\n\t__netdev_adjacent_dev_unlink_neighbour(dev, upper_dev);\n\n\treturn ret;\n}\n\n \nint netdev_upper_dev_link(struct net_device *dev,\n\t\t\t  struct net_device *upper_dev,\n\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = NESTED_SYNC_IMM | NESTED_SYNC_TODO,\n\t\t.data = NULL,\n\t};\n\n\treturn __netdev_upper_dev_link(dev, upper_dev, false,\n\t\t\t\t       NULL, NULL, &priv, extack);\n}\nEXPORT_SYMBOL(netdev_upper_dev_link);\n\n \nint netdev_master_upper_dev_link(struct net_device *dev,\n\t\t\t\t struct net_device *upper_dev,\n\t\t\t\t void *upper_priv, void *upper_info,\n\t\t\t\t struct netlink_ext_ack *extack)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = NESTED_SYNC_IMM | NESTED_SYNC_TODO,\n\t\t.data = NULL,\n\t};\n\n\treturn __netdev_upper_dev_link(dev, upper_dev, true,\n\t\t\t\t       upper_priv, upper_info, &priv, extack);\n}\nEXPORT_SYMBOL(netdev_master_upper_dev_link);\n\nstatic void __netdev_upper_dev_unlink(struct net_device *dev,\n\t\t\t\t      struct net_device *upper_dev,\n\t\t\t\t      struct netdev_nested_priv *priv)\n{\n\tstruct netdev_notifier_changeupper_info changeupper_info = {\n\t\t.info = {\n\t\t\t.dev = dev,\n\t\t},\n\t\t.upper_dev = upper_dev,\n\t\t.linking = false,\n\t};\n\n\tASSERT_RTNL();\n\n\tchangeupper_info.master = netdev_master_upper_dev_get(dev) == upper_dev;\n\n\tcall_netdevice_notifiers_info(NETDEV_PRECHANGEUPPER,\n\t\t\t\t      &changeupper_info.info);\n\n\t__netdev_adjacent_dev_unlink_neighbour(dev, upper_dev);\n\n\tcall_netdevice_notifiers_info(NETDEV_CHANGEUPPER,\n\t\t\t\t      &changeupper_info.info);\n\n\t__netdev_update_upper_level(dev, NULL);\n\t__netdev_walk_all_lower_dev(dev, __netdev_update_upper_level, NULL);\n\n\t__netdev_update_lower_level(upper_dev, priv);\n\t__netdev_walk_all_upper_dev(upper_dev, __netdev_update_lower_level,\n\t\t\t\t    priv);\n}\n\n \nvoid netdev_upper_dev_unlink(struct net_device *dev,\n\t\t\t     struct net_device *upper_dev)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = NESTED_SYNC_TODO,\n\t\t.data = NULL,\n\t};\n\n\t__netdev_upper_dev_unlink(dev, upper_dev, &priv);\n}\nEXPORT_SYMBOL(netdev_upper_dev_unlink);\n\nstatic void __netdev_adjacent_dev_set(struct net_device *upper_dev,\n\t\t\t\t      struct net_device *lower_dev,\n\t\t\t\t      bool val)\n{\n\tstruct netdev_adjacent *adj;\n\n\tadj = __netdev_find_adj(lower_dev, &upper_dev->adj_list.lower);\n\tif (adj)\n\t\tadj->ignore = val;\n\n\tadj = __netdev_find_adj(upper_dev, &lower_dev->adj_list.upper);\n\tif (adj)\n\t\tadj->ignore = val;\n}\n\nstatic void netdev_adjacent_dev_disable(struct net_device *upper_dev,\n\t\t\t\t\tstruct net_device *lower_dev)\n{\n\t__netdev_adjacent_dev_set(upper_dev, lower_dev, true);\n}\n\nstatic void netdev_adjacent_dev_enable(struct net_device *upper_dev,\n\t\t\t\t       struct net_device *lower_dev)\n{\n\t__netdev_adjacent_dev_set(upper_dev, lower_dev, false);\n}\n\nint netdev_adjacent_change_prepare(struct net_device *old_dev,\n\t\t\t\t   struct net_device *new_dev,\n\t\t\t\t   struct net_device *dev,\n\t\t\t\t   struct netlink_ext_ack *extack)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = 0,\n\t\t.data = NULL,\n\t};\n\tint err;\n\n\tif (!new_dev)\n\t\treturn 0;\n\n\tif (old_dev && new_dev != old_dev)\n\t\tnetdev_adjacent_dev_disable(dev, old_dev);\n\terr = __netdev_upper_dev_link(new_dev, dev, false, NULL, NULL, &priv,\n\t\t\t\t      extack);\n\tif (err) {\n\t\tif (old_dev && new_dev != old_dev)\n\t\t\tnetdev_adjacent_dev_enable(dev, old_dev);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_adjacent_change_prepare);\n\nvoid netdev_adjacent_change_commit(struct net_device *old_dev,\n\t\t\t\t   struct net_device *new_dev,\n\t\t\t\t   struct net_device *dev)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = NESTED_SYNC_IMM | NESTED_SYNC_TODO,\n\t\t.data = NULL,\n\t};\n\n\tif (!new_dev || !old_dev)\n\t\treturn;\n\n\tif (new_dev == old_dev)\n\t\treturn;\n\n\tnetdev_adjacent_dev_enable(dev, old_dev);\n\t__netdev_upper_dev_unlink(old_dev, dev, &priv);\n}\nEXPORT_SYMBOL(netdev_adjacent_change_commit);\n\nvoid netdev_adjacent_change_abort(struct net_device *old_dev,\n\t\t\t\t  struct net_device *new_dev,\n\t\t\t\t  struct net_device *dev)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = 0,\n\t\t.data = NULL,\n\t};\n\n\tif (!new_dev)\n\t\treturn;\n\n\tif (old_dev && new_dev != old_dev)\n\t\tnetdev_adjacent_dev_enable(dev, old_dev);\n\n\t__netdev_upper_dev_unlink(new_dev, dev, &priv);\n}\nEXPORT_SYMBOL(netdev_adjacent_change_abort);\n\n \nvoid netdev_bonding_info_change(struct net_device *dev,\n\t\t\t\tstruct netdev_bonding_info *bonding_info)\n{\n\tstruct netdev_notifier_bonding_info info = {\n\t\t.info.dev = dev,\n\t};\n\n\tmemcpy(&info.bonding_info, bonding_info,\n\t       sizeof(struct netdev_bonding_info));\n\tcall_netdevice_notifiers_info(NETDEV_BONDING_INFO,\n\t\t\t\t      &info.info);\n}\nEXPORT_SYMBOL(netdev_bonding_info_change);\n\nstatic int netdev_offload_xstats_enable_l3(struct net_device *dev,\n\t\t\t\t\t   struct netlink_ext_ack *extack)\n{\n\tstruct netdev_notifier_offload_xstats_info info = {\n\t\t.info.dev = dev,\n\t\t.info.extack = extack,\n\t\t.type = NETDEV_OFFLOAD_XSTATS_TYPE_L3,\n\t};\n\tint err;\n\tint rc;\n\n\tdev->offload_xstats_l3 = kzalloc(sizeof(*dev->offload_xstats_l3),\n\t\t\t\t\t GFP_KERNEL);\n\tif (!dev->offload_xstats_l3)\n\t\treturn -ENOMEM;\n\n\trc = call_netdevice_notifiers_info_robust(NETDEV_OFFLOAD_XSTATS_ENABLE,\n\t\t\t\t\t\t  NETDEV_OFFLOAD_XSTATS_DISABLE,\n\t\t\t\t\t\t  &info.info);\n\terr = notifier_to_errno(rc);\n\tif (err)\n\t\tgoto free_stats;\n\n\treturn 0;\n\nfree_stats:\n\tkfree(dev->offload_xstats_l3);\n\tdev->offload_xstats_l3 = NULL;\n\treturn err;\n}\n\nint netdev_offload_xstats_enable(struct net_device *dev,\n\t\t\t\t enum netdev_offload_xstats_type type,\n\t\t\t\t struct netlink_ext_ack *extack)\n{\n\tASSERT_RTNL();\n\n\tif (netdev_offload_xstats_enabled(dev, type))\n\t\treturn -EALREADY;\n\n\tswitch (type) {\n\tcase NETDEV_OFFLOAD_XSTATS_TYPE_L3:\n\t\treturn netdev_offload_xstats_enable_l3(dev, extack);\n\t}\n\n\tWARN_ON(1);\n\treturn -EINVAL;\n}\nEXPORT_SYMBOL(netdev_offload_xstats_enable);\n\nstatic void netdev_offload_xstats_disable_l3(struct net_device *dev)\n{\n\tstruct netdev_notifier_offload_xstats_info info = {\n\t\t.info.dev = dev,\n\t\t.type = NETDEV_OFFLOAD_XSTATS_TYPE_L3,\n\t};\n\n\tcall_netdevice_notifiers_info(NETDEV_OFFLOAD_XSTATS_DISABLE,\n\t\t\t\t      &info.info);\n\tkfree(dev->offload_xstats_l3);\n\tdev->offload_xstats_l3 = NULL;\n}\n\nint netdev_offload_xstats_disable(struct net_device *dev,\n\t\t\t\t  enum netdev_offload_xstats_type type)\n{\n\tASSERT_RTNL();\n\n\tif (!netdev_offload_xstats_enabled(dev, type))\n\t\treturn -EALREADY;\n\n\tswitch (type) {\n\tcase NETDEV_OFFLOAD_XSTATS_TYPE_L3:\n\t\tnetdev_offload_xstats_disable_l3(dev);\n\t\treturn 0;\n\t}\n\n\tWARN_ON(1);\n\treturn -EINVAL;\n}\nEXPORT_SYMBOL(netdev_offload_xstats_disable);\n\nstatic void netdev_offload_xstats_disable_all(struct net_device *dev)\n{\n\tnetdev_offload_xstats_disable(dev, NETDEV_OFFLOAD_XSTATS_TYPE_L3);\n}\n\nstatic struct rtnl_hw_stats64 *\nnetdev_offload_xstats_get_ptr(const struct net_device *dev,\n\t\t\t      enum netdev_offload_xstats_type type)\n{\n\tswitch (type) {\n\tcase NETDEV_OFFLOAD_XSTATS_TYPE_L3:\n\t\treturn dev->offload_xstats_l3;\n\t}\n\n\tWARN_ON(1);\n\treturn NULL;\n}\n\nbool netdev_offload_xstats_enabled(const struct net_device *dev,\n\t\t\t\t   enum netdev_offload_xstats_type type)\n{\n\tASSERT_RTNL();\n\n\treturn netdev_offload_xstats_get_ptr(dev, type);\n}\nEXPORT_SYMBOL(netdev_offload_xstats_enabled);\n\nstruct netdev_notifier_offload_xstats_ru {\n\tbool used;\n};\n\nstruct netdev_notifier_offload_xstats_rd {\n\tstruct rtnl_hw_stats64 stats;\n\tbool used;\n};\n\nstatic void netdev_hw_stats64_add(struct rtnl_hw_stats64 *dest,\n\t\t\t\t  const struct rtnl_hw_stats64 *src)\n{\n\tdest->rx_packets\t  += src->rx_packets;\n\tdest->tx_packets\t  += src->tx_packets;\n\tdest->rx_bytes\t\t  += src->rx_bytes;\n\tdest->tx_bytes\t\t  += src->tx_bytes;\n\tdest->rx_errors\t\t  += src->rx_errors;\n\tdest->tx_errors\t\t  += src->tx_errors;\n\tdest->rx_dropped\t  += src->rx_dropped;\n\tdest->tx_dropped\t  += src->tx_dropped;\n\tdest->multicast\t\t  += src->multicast;\n}\n\nstatic int netdev_offload_xstats_get_used(struct net_device *dev,\n\t\t\t\t\t  enum netdev_offload_xstats_type type,\n\t\t\t\t\t  bool *p_used,\n\t\t\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct netdev_notifier_offload_xstats_ru report_used = {};\n\tstruct netdev_notifier_offload_xstats_info info = {\n\t\t.info.dev = dev,\n\t\t.info.extack = extack,\n\t\t.type = type,\n\t\t.report_used = &report_used,\n\t};\n\tint rc;\n\n\tWARN_ON(!netdev_offload_xstats_enabled(dev, type));\n\trc = call_netdevice_notifiers_info(NETDEV_OFFLOAD_XSTATS_REPORT_USED,\n\t\t\t\t\t   &info.info);\n\t*p_used = report_used.used;\n\treturn notifier_to_errno(rc);\n}\n\nstatic int netdev_offload_xstats_get_stats(struct net_device *dev,\n\t\t\t\t\t   enum netdev_offload_xstats_type type,\n\t\t\t\t\t   struct rtnl_hw_stats64 *p_stats,\n\t\t\t\t\t   bool *p_used,\n\t\t\t\t\t   struct netlink_ext_ack *extack)\n{\n\tstruct netdev_notifier_offload_xstats_rd report_delta = {};\n\tstruct netdev_notifier_offload_xstats_info info = {\n\t\t.info.dev = dev,\n\t\t.info.extack = extack,\n\t\t.type = type,\n\t\t.report_delta = &report_delta,\n\t};\n\tstruct rtnl_hw_stats64 *stats;\n\tint rc;\n\n\tstats = netdev_offload_xstats_get_ptr(dev, type);\n\tif (WARN_ON(!stats))\n\t\treturn -EINVAL;\n\n\trc = call_netdevice_notifiers_info(NETDEV_OFFLOAD_XSTATS_REPORT_DELTA,\n\t\t\t\t\t   &info.info);\n\n\t \n\tnetdev_hw_stats64_add(stats, &report_delta.stats);\n\n\tif (p_stats)\n\t\t*p_stats = *stats;\n\t*p_used = report_delta.used;\n\n\treturn notifier_to_errno(rc);\n}\n\nint netdev_offload_xstats_get(struct net_device *dev,\n\t\t\t      enum netdev_offload_xstats_type type,\n\t\t\t      struct rtnl_hw_stats64 *p_stats, bool *p_used,\n\t\t\t      struct netlink_ext_ack *extack)\n{\n\tASSERT_RTNL();\n\n\tif (p_stats)\n\t\treturn netdev_offload_xstats_get_stats(dev, type, p_stats,\n\t\t\t\t\t\t       p_used, extack);\n\telse\n\t\treturn netdev_offload_xstats_get_used(dev, type, p_used,\n\t\t\t\t\t\t      extack);\n}\nEXPORT_SYMBOL(netdev_offload_xstats_get);\n\nvoid\nnetdev_offload_xstats_report_delta(struct netdev_notifier_offload_xstats_rd *report_delta,\n\t\t\t\t   const struct rtnl_hw_stats64 *stats)\n{\n\treport_delta->used = true;\n\tnetdev_hw_stats64_add(&report_delta->stats, stats);\n}\nEXPORT_SYMBOL(netdev_offload_xstats_report_delta);\n\nvoid\nnetdev_offload_xstats_report_used(struct netdev_notifier_offload_xstats_ru *report_used)\n{\n\treport_used->used = true;\n}\nEXPORT_SYMBOL(netdev_offload_xstats_report_used);\n\nvoid netdev_offload_xstats_push_delta(struct net_device *dev,\n\t\t\t\t      enum netdev_offload_xstats_type type,\n\t\t\t\t      const struct rtnl_hw_stats64 *p_stats)\n{\n\tstruct rtnl_hw_stats64 *stats;\n\n\tASSERT_RTNL();\n\n\tstats = netdev_offload_xstats_get_ptr(dev, type);\n\tif (WARN_ON(!stats))\n\t\treturn;\n\n\tnetdev_hw_stats64_add(stats, p_stats);\n}\nEXPORT_SYMBOL(netdev_offload_xstats_push_delta);\n\n \n\nstruct net_device *netdev_get_xmit_slave(struct net_device *dev,\n\t\t\t\t\t struct sk_buff *skb,\n\t\t\t\t\t bool all_slaves)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_get_xmit_slave)\n\t\treturn NULL;\n\treturn ops->ndo_get_xmit_slave(dev, skb, all_slaves);\n}\nEXPORT_SYMBOL(netdev_get_xmit_slave);\n\nstatic struct net_device *netdev_sk_get_lower_dev(struct net_device *dev,\n\t\t\t\t\t\t  struct sock *sk)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_sk_get_lower_dev)\n\t\treturn NULL;\n\treturn ops->ndo_sk_get_lower_dev(dev, sk);\n}\n\n \n\nstruct net_device *netdev_sk_get_lowest_dev(struct net_device *dev,\n\t\t\t\t\t    struct sock *sk)\n{\n\tstruct net_device *lower;\n\n\tlower = netdev_sk_get_lower_dev(dev, sk);\n\twhile (lower) {\n\t\tdev = lower;\n\t\tlower = netdev_sk_get_lower_dev(dev, sk);\n\t}\n\n\treturn dev;\n}\nEXPORT_SYMBOL(netdev_sk_get_lowest_dev);\n\nstatic void netdev_adjacent_add_links(struct net_device *dev)\n{\n\tstruct netdev_adjacent *iter;\n\n\tstruct net *net = dev_net(dev);\n\n\tlist_for_each_entry(iter, &dev->adj_list.upper, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_add(iter->dev, dev,\n\t\t\t\t\t  &iter->dev->adj_list.lower);\n\t\tnetdev_adjacent_sysfs_add(dev, iter->dev,\n\t\t\t\t\t  &dev->adj_list.upper);\n\t}\n\n\tlist_for_each_entry(iter, &dev->adj_list.lower, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_add(iter->dev, dev,\n\t\t\t\t\t  &iter->dev->adj_list.upper);\n\t\tnetdev_adjacent_sysfs_add(dev, iter->dev,\n\t\t\t\t\t  &dev->adj_list.lower);\n\t}\n}\n\nstatic void netdev_adjacent_del_links(struct net_device *dev)\n{\n\tstruct netdev_adjacent *iter;\n\n\tstruct net *net = dev_net(dev);\n\n\tlist_for_each_entry(iter, &dev->adj_list.upper, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_del(iter->dev, dev->name,\n\t\t\t\t\t  &iter->dev->adj_list.lower);\n\t\tnetdev_adjacent_sysfs_del(dev, iter->dev->name,\n\t\t\t\t\t  &dev->adj_list.upper);\n\t}\n\n\tlist_for_each_entry(iter, &dev->adj_list.lower, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_del(iter->dev, dev->name,\n\t\t\t\t\t  &iter->dev->adj_list.upper);\n\t\tnetdev_adjacent_sysfs_del(dev, iter->dev->name,\n\t\t\t\t\t  &dev->adj_list.lower);\n\t}\n}\n\nvoid netdev_adjacent_rename_links(struct net_device *dev, char *oldname)\n{\n\tstruct netdev_adjacent *iter;\n\n\tstruct net *net = dev_net(dev);\n\n\tlist_for_each_entry(iter, &dev->adj_list.upper, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_del(iter->dev, oldname,\n\t\t\t\t\t  &iter->dev->adj_list.lower);\n\t\tnetdev_adjacent_sysfs_add(iter->dev, dev,\n\t\t\t\t\t  &iter->dev->adj_list.lower);\n\t}\n\n\tlist_for_each_entry(iter, &dev->adj_list.lower, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_del(iter->dev, oldname,\n\t\t\t\t\t  &iter->dev->adj_list.upper);\n\t\tnetdev_adjacent_sysfs_add(iter->dev, dev,\n\t\t\t\t\t  &iter->dev->adj_list.upper);\n\t}\n}\n\nvoid *netdev_lower_dev_get_private(struct net_device *dev,\n\t\t\t\t   struct net_device *lower_dev)\n{\n\tstruct netdev_adjacent *lower;\n\n\tif (!lower_dev)\n\t\treturn NULL;\n\tlower = __netdev_find_adj(lower_dev, &dev->adj_list.lower);\n\tif (!lower)\n\t\treturn NULL;\n\n\treturn lower->private;\n}\nEXPORT_SYMBOL(netdev_lower_dev_get_private);\n\n\n \nvoid netdev_lower_state_changed(struct net_device *lower_dev,\n\t\t\t\tvoid *lower_state_info)\n{\n\tstruct netdev_notifier_changelowerstate_info changelowerstate_info = {\n\t\t.info.dev = lower_dev,\n\t};\n\n\tASSERT_RTNL();\n\tchangelowerstate_info.lower_state_info = lower_state_info;\n\tcall_netdevice_notifiers_info(NETDEV_CHANGELOWERSTATE,\n\t\t\t\t      &changelowerstate_info.info);\n}\nEXPORT_SYMBOL(netdev_lower_state_changed);\n\nstatic void dev_change_rx_flags(struct net_device *dev, int flags)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (ops->ndo_change_rx_flags)\n\t\tops->ndo_change_rx_flags(dev, flags);\n}\n\nstatic int __dev_set_promiscuity(struct net_device *dev, int inc, bool notify)\n{\n\tunsigned int old_flags = dev->flags;\n\tkuid_t uid;\n\tkgid_t gid;\n\n\tASSERT_RTNL();\n\n\tdev->flags |= IFF_PROMISC;\n\tdev->promiscuity += inc;\n\tif (dev->promiscuity == 0) {\n\t\t \n\t\tif (inc < 0)\n\t\t\tdev->flags &= ~IFF_PROMISC;\n\t\telse {\n\t\t\tdev->promiscuity -= inc;\n\t\t\tnetdev_warn(dev, \"promiscuity touches roof, set promiscuity failed. promiscuity feature of device might be broken.\\n\");\n\t\t\treturn -EOVERFLOW;\n\t\t}\n\t}\n\tif (dev->flags != old_flags) {\n\t\tnetdev_info(dev, \"%s promiscuous mode\\n\",\n\t\t\t    dev->flags & IFF_PROMISC ? \"entered\" : \"left\");\n\t\tif (audit_enabled) {\n\t\t\tcurrent_uid_gid(&uid, &gid);\n\t\t\taudit_log(audit_context(), GFP_ATOMIC,\n\t\t\t\t  AUDIT_ANOM_PROMISCUOUS,\n\t\t\t\t  \"dev=%s prom=%d old_prom=%d auid=%u uid=%u gid=%u ses=%u\",\n\t\t\t\t  dev->name, (dev->flags & IFF_PROMISC),\n\t\t\t\t  (old_flags & IFF_PROMISC),\n\t\t\t\t  from_kuid(&init_user_ns, audit_get_loginuid(current)),\n\t\t\t\t  from_kuid(&init_user_ns, uid),\n\t\t\t\t  from_kgid(&init_user_ns, gid),\n\t\t\t\t  audit_get_sessionid(current));\n\t\t}\n\n\t\tdev_change_rx_flags(dev, IFF_PROMISC);\n\t}\n\tif (notify)\n\t\t__dev_notify_flags(dev, old_flags, IFF_PROMISC, 0, NULL);\n\treturn 0;\n}\n\n \nint dev_set_promiscuity(struct net_device *dev, int inc)\n{\n\tunsigned int old_flags = dev->flags;\n\tint err;\n\n\terr = __dev_set_promiscuity(dev, inc, true);\n\tif (err < 0)\n\t\treturn err;\n\tif (dev->flags != old_flags)\n\t\tdev_set_rx_mode(dev);\n\treturn err;\n}\nEXPORT_SYMBOL(dev_set_promiscuity);\n\nstatic int __dev_set_allmulti(struct net_device *dev, int inc, bool notify)\n{\n\tunsigned int old_flags = dev->flags, old_gflags = dev->gflags;\n\n\tASSERT_RTNL();\n\n\tdev->flags |= IFF_ALLMULTI;\n\tdev->allmulti += inc;\n\tif (dev->allmulti == 0) {\n\t\t \n\t\tif (inc < 0)\n\t\t\tdev->flags &= ~IFF_ALLMULTI;\n\t\telse {\n\t\t\tdev->allmulti -= inc;\n\t\t\tnetdev_warn(dev, \"allmulti touches roof, set allmulti failed. allmulti feature of device might be broken.\\n\");\n\t\t\treturn -EOVERFLOW;\n\t\t}\n\t}\n\tif (dev->flags ^ old_flags) {\n\t\tnetdev_info(dev, \"%s allmulticast mode\\n\",\n\t\t\t    dev->flags & IFF_ALLMULTI ? \"entered\" : \"left\");\n\t\tdev_change_rx_flags(dev, IFF_ALLMULTI);\n\t\tdev_set_rx_mode(dev);\n\t\tif (notify)\n\t\t\t__dev_notify_flags(dev, old_flags,\n\t\t\t\t\t   dev->gflags ^ old_gflags, 0, NULL);\n\t}\n\treturn 0;\n}\n\n \n\nint dev_set_allmulti(struct net_device *dev, int inc)\n{\n\treturn __dev_set_allmulti(dev, inc, true);\n}\nEXPORT_SYMBOL(dev_set_allmulti);\n\n \nvoid __dev_set_rx_mode(struct net_device *dev)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\t \n\tif (!(dev->flags&IFF_UP))\n\t\treturn;\n\n\tif (!netif_device_present(dev))\n\t\treturn;\n\n\tif (!(dev->priv_flags & IFF_UNICAST_FLT)) {\n\t\t \n\t\tif (!netdev_uc_empty(dev) && !dev->uc_promisc) {\n\t\t\t__dev_set_promiscuity(dev, 1, false);\n\t\t\tdev->uc_promisc = true;\n\t\t} else if (netdev_uc_empty(dev) && dev->uc_promisc) {\n\t\t\t__dev_set_promiscuity(dev, -1, false);\n\t\t\tdev->uc_promisc = false;\n\t\t}\n\t}\n\n\tif (ops->ndo_set_rx_mode)\n\t\tops->ndo_set_rx_mode(dev);\n}\n\nvoid dev_set_rx_mode(struct net_device *dev)\n{\n\tnetif_addr_lock_bh(dev);\n\t__dev_set_rx_mode(dev);\n\tnetif_addr_unlock_bh(dev);\n}\n\n \nunsigned int dev_get_flags(const struct net_device *dev)\n{\n\tunsigned int flags;\n\n\tflags = (dev->flags & ~(IFF_PROMISC |\n\t\t\t\tIFF_ALLMULTI |\n\t\t\t\tIFF_RUNNING |\n\t\t\t\tIFF_LOWER_UP |\n\t\t\t\tIFF_DORMANT)) |\n\t\t(dev->gflags & (IFF_PROMISC |\n\t\t\t\tIFF_ALLMULTI));\n\n\tif (netif_running(dev)) {\n\t\tif (netif_oper_up(dev))\n\t\t\tflags |= IFF_RUNNING;\n\t\tif (netif_carrier_ok(dev))\n\t\t\tflags |= IFF_LOWER_UP;\n\t\tif (netif_dormant(dev))\n\t\t\tflags |= IFF_DORMANT;\n\t}\n\n\treturn flags;\n}\nEXPORT_SYMBOL(dev_get_flags);\n\nint __dev_change_flags(struct net_device *dev, unsigned int flags,\n\t\t       struct netlink_ext_ack *extack)\n{\n\tunsigned int old_flags = dev->flags;\n\tint ret;\n\n\tASSERT_RTNL();\n\n\t \n\n\tdev->flags = (flags & (IFF_DEBUG | IFF_NOTRAILERS | IFF_NOARP |\n\t\t\t       IFF_DYNAMIC | IFF_MULTICAST | IFF_PORTSEL |\n\t\t\t       IFF_AUTOMEDIA)) |\n\t\t     (dev->flags & (IFF_UP | IFF_VOLATILE | IFF_PROMISC |\n\t\t\t\t    IFF_ALLMULTI));\n\n\t \n\n\tif ((old_flags ^ flags) & IFF_MULTICAST)\n\t\tdev_change_rx_flags(dev, IFF_MULTICAST);\n\n\tdev_set_rx_mode(dev);\n\n\t \n\n\tret = 0;\n\tif ((old_flags ^ flags) & IFF_UP) {\n\t\tif (old_flags & IFF_UP)\n\t\t\t__dev_close(dev);\n\t\telse\n\t\t\tret = __dev_open(dev, extack);\n\t}\n\n\tif ((flags ^ dev->gflags) & IFF_PROMISC) {\n\t\tint inc = (flags & IFF_PROMISC) ? 1 : -1;\n\t\tunsigned int old_flags = dev->flags;\n\n\t\tdev->gflags ^= IFF_PROMISC;\n\n\t\tif (__dev_set_promiscuity(dev, inc, false) >= 0)\n\t\t\tif (dev->flags != old_flags)\n\t\t\t\tdev_set_rx_mode(dev);\n\t}\n\n\t \n\tif ((flags ^ dev->gflags) & IFF_ALLMULTI) {\n\t\tint inc = (flags & IFF_ALLMULTI) ? 1 : -1;\n\n\t\tdev->gflags ^= IFF_ALLMULTI;\n\t\t__dev_set_allmulti(dev, inc, false);\n\t}\n\n\treturn ret;\n}\n\nvoid __dev_notify_flags(struct net_device *dev, unsigned int old_flags,\n\t\t\tunsigned int gchanges, u32 portid,\n\t\t\tconst struct nlmsghdr *nlh)\n{\n\tunsigned int changes = dev->flags ^ old_flags;\n\n\tif (gchanges)\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, gchanges, GFP_ATOMIC, portid, nlh);\n\n\tif (changes & IFF_UP) {\n\t\tif (dev->flags & IFF_UP)\n\t\t\tcall_netdevice_notifiers(NETDEV_UP, dev);\n\t\telse\n\t\t\tcall_netdevice_notifiers(NETDEV_DOWN, dev);\n\t}\n\n\tif (dev->flags & IFF_UP &&\n\t    (changes & ~(IFF_UP | IFF_PROMISC | IFF_ALLMULTI | IFF_VOLATILE))) {\n\t\tstruct netdev_notifier_change_info change_info = {\n\t\t\t.info = {\n\t\t\t\t.dev = dev,\n\t\t\t},\n\t\t\t.flags_changed = changes,\n\t\t};\n\n\t\tcall_netdevice_notifiers_info(NETDEV_CHANGE, &change_info.info);\n\t}\n}\n\n \nint dev_change_flags(struct net_device *dev, unsigned int flags,\n\t\t     struct netlink_ext_ack *extack)\n{\n\tint ret;\n\tunsigned int changes, old_flags = dev->flags, old_gflags = dev->gflags;\n\n\tret = __dev_change_flags(dev, flags, extack);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tchanges = (old_flags ^ dev->flags) | (old_gflags ^ dev->gflags);\n\t__dev_notify_flags(dev, old_flags, changes, 0, NULL);\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_change_flags);\n\nint __dev_set_mtu(struct net_device *dev, int new_mtu)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (ops->ndo_change_mtu)\n\t\treturn ops->ndo_change_mtu(dev, new_mtu);\n\n\t \n\tWRITE_ONCE(dev->mtu, new_mtu);\n\treturn 0;\n}\nEXPORT_SYMBOL(__dev_set_mtu);\n\nint dev_validate_mtu(struct net_device *dev, int new_mtu,\n\t\t     struct netlink_ext_ack *extack)\n{\n\t \n\tif (new_mtu < 0 || new_mtu < dev->min_mtu) {\n\t\tNL_SET_ERR_MSG(extack, \"mtu less than device minimum\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (dev->max_mtu > 0 && new_mtu > dev->max_mtu) {\n\t\tNL_SET_ERR_MSG(extack, \"mtu greater than device maximum\");\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n \nint dev_set_mtu_ext(struct net_device *dev, int new_mtu,\n\t\t    struct netlink_ext_ack *extack)\n{\n\tint err, orig_mtu;\n\n\tif (new_mtu == dev->mtu)\n\t\treturn 0;\n\n\terr = dev_validate_mtu(dev, new_mtu, extack);\n\tif (err)\n\t\treturn err;\n\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\n\terr = call_netdevice_notifiers(NETDEV_PRECHANGEMTU, dev);\n\terr = notifier_to_errno(err);\n\tif (err)\n\t\treturn err;\n\n\torig_mtu = dev->mtu;\n\terr = __dev_set_mtu(dev, new_mtu);\n\n\tif (!err) {\n\t\terr = call_netdevice_notifiers_mtu(NETDEV_CHANGEMTU, dev,\n\t\t\t\t\t\t   orig_mtu);\n\t\terr = notifier_to_errno(err);\n\t\tif (err) {\n\t\t\t \n\t\t\t__dev_set_mtu(dev, orig_mtu);\n\t\t\tcall_netdevice_notifiers_mtu(NETDEV_CHANGEMTU, dev,\n\t\t\t\t\t\t     new_mtu);\n\t\t}\n\t}\n\treturn err;\n}\n\nint dev_set_mtu(struct net_device *dev, int new_mtu)\n{\n\tstruct netlink_ext_ack extack;\n\tint err;\n\n\tmemset(&extack, 0, sizeof(extack));\n\terr = dev_set_mtu_ext(dev, new_mtu, &extack);\n\tif (err && extack._msg)\n\t\tnet_err_ratelimited(\"%s: %s\\n\", dev->name, extack._msg);\n\treturn err;\n}\nEXPORT_SYMBOL(dev_set_mtu);\n\n \nint dev_change_tx_queue_len(struct net_device *dev, unsigned long new_len)\n{\n\tunsigned int orig_len = dev->tx_queue_len;\n\tint res;\n\n\tif (new_len != (unsigned int)new_len)\n\t\treturn -ERANGE;\n\n\tif (new_len != orig_len) {\n\t\tdev->tx_queue_len = new_len;\n\t\tres = call_netdevice_notifiers(NETDEV_CHANGE_TX_QUEUE_LEN, dev);\n\t\tres = notifier_to_errno(res);\n\t\tif (res)\n\t\t\tgoto err_rollback;\n\t\tres = dev_qdisc_change_tx_queue_len(dev);\n\t\tif (res)\n\t\t\tgoto err_rollback;\n\t}\n\n\treturn 0;\n\nerr_rollback:\n\tnetdev_err(dev, \"refused to change device tx_queue_len\\n\");\n\tdev->tx_queue_len = orig_len;\n\treturn res;\n}\n\n \nvoid dev_set_group(struct net_device *dev, int new_group)\n{\n\tdev->group = new_group;\n}\n\n \nint dev_pre_changeaddr_notify(struct net_device *dev, const char *addr,\n\t\t\t      struct netlink_ext_ack *extack)\n{\n\tstruct netdev_notifier_pre_changeaddr_info info = {\n\t\t.info.dev = dev,\n\t\t.info.extack = extack,\n\t\t.dev_addr = addr,\n\t};\n\tint rc;\n\n\trc = call_netdevice_notifiers_info(NETDEV_PRE_CHANGEADDR, &info.info);\n\treturn notifier_to_errno(rc);\n}\nEXPORT_SYMBOL(dev_pre_changeaddr_notify);\n\n \nint dev_set_mac_address(struct net_device *dev, struct sockaddr *sa,\n\t\t\tstruct netlink_ext_ack *extack)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tint err;\n\n\tif (!ops->ndo_set_mac_address)\n\t\treturn -EOPNOTSUPP;\n\tif (sa->sa_family != dev->type)\n\t\treturn -EINVAL;\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\terr = dev_pre_changeaddr_notify(dev, sa->sa_data, extack);\n\tif (err)\n\t\treturn err;\n\tif (memcmp(dev->dev_addr, sa->sa_data, dev->addr_len)) {\n\t\terr = ops->ndo_set_mac_address(dev, sa);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\tdev->addr_assign_type = NET_ADDR_SET;\n\tcall_netdevice_notifiers(NETDEV_CHANGEADDR, dev);\n\tadd_device_randomness(dev->dev_addr, dev->addr_len);\n\treturn 0;\n}\nEXPORT_SYMBOL(dev_set_mac_address);\n\nstatic DECLARE_RWSEM(dev_addr_sem);\n\nint dev_set_mac_address_user(struct net_device *dev, struct sockaddr *sa,\n\t\t\t     struct netlink_ext_ack *extack)\n{\n\tint ret;\n\n\tdown_write(&dev_addr_sem);\n\tret = dev_set_mac_address(dev, sa, extack);\n\tup_write(&dev_addr_sem);\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_set_mac_address_user);\n\nint dev_get_mac_address(struct sockaddr *sa, struct net *net, char *dev_name)\n{\n\tsize_t size = sizeof(sa->sa_data_min);\n\tstruct net_device *dev;\n\tint ret = 0;\n\n\tdown_read(&dev_addr_sem);\n\trcu_read_lock();\n\n\tdev = dev_get_by_name_rcu(net, dev_name);\n\tif (!dev) {\n\t\tret = -ENODEV;\n\t\tgoto unlock;\n\t}\n\tif (!dev->addr_len)\n\t\tmemset(sa->sa_data, 0, size);\n\telse\n\t\tmemcpy(sa->sa_data, dev->dev_addr,\n\t\t       min_t(size_t, size, dev->addr_len));\n\tsa->sa_family = dev->type;\n\nunlock:\n\trcu_read_unlock();\n\tup_read(&dev_addr_sem);\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_get_mac_address);\n\n \nint dev_change_carrier(struct net_device *dev, bool new_carrier)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_change_carrier)\n\t\treturn -EOPNOTSUPP;\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\treturn ops->ndo_change_carrier(dev, new_carrier);\n}\n\n \nint dev_get_phys_port_id(struct net_device *dev,\n\t\t\t struct netdev_phys_item_id *ppid)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_get_phys_port_id)\n\t\treturn -EOPNOTSUPP;\n\treturn ops->ndo_get_phys_port_id(dev, ppid);\n}\n\n \nint dev_get_phys_port_name(struct net_device *dev,\n\t\t\t   char *name, size_t len)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tint err;\n\n\tif (ops->ndo_get_phys_port_name) {\n\t\terr = ops->ndo_get_phys_port_name(dev, name, len);\n\t\tif (err != -EOPNOTSUPP)\n\t\t\treturn err;\n\t}\n\treturn devlink_compat_phys_port_name_get(dev, name, len);\n}\n\n \nint dev_get_port_parent_id(struct net_device *dev,\n\t\t\t   struct netdev_phys_item_id *ppid,\n\t\t\t   bool recurse)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tstruct netdev_phys_item_id first = { };\n\tstruct net_device *lower_dev;\n\tstruct list_head *iter;\n\tint err;\n\n\tif (ops->ndo_get_port_parent_id) {\n\t\terr = ops->ndo_get_port_parent_id(dev, ppid);\n\t\tif (err != -EOPNOTSUPP)\n\t\t\treturn err;\n\t}\n\n\terr = devlink_compat_switch_id_get(dev, ppid);\n\tif (!recurse || err != -EOPNOTSUPP)\n\t\treturn err;\n\n\tnetdev_for_each_lower_dev(dev, lower_dev, iter) {\n\t\terr = dev_get_port_parent_id(lower_dev, ppid, true);\n\t\tif (err)\n\t\t\tbreak;\n\t\tif (!first.id_len)\n\t\t\tfirst = *ppid;\n\t\telse if (memcmp(&first, ppid, sizeof(*ppid)))\n\t\t\treturn -EOPNOTSUPP;\n\t}\n\n\treturn err;\n}\nEXPORT_SYMBOL(dev_get_port_parent_id);\n\n \nbool netdev_port_same_parent_id(struct net_device *a, struct net_device *b)\n{\n\tstruct netdev_phys_item_id a_id = { };\n\tstruct netdev_phys_item_id b_id = { };\n\n\tif (dev_get_port_parent_id(a, &a_id, true) ||\n\t    dev_get_port_parent_id(b, &b_id, true))\n\t\treturn false;\n\n\treturn netdev_phys_item_id_same(&a_id, &b_id);\n}\nEXPORT_SYMBOL(netdev_port_same_parent_id);\n\n \nint dev_change_proto_down(struct net_device *dev, bool proto_down)\n{\n\tif (!(dev->priv_flags & IFF_CHANGE_PROTO_DOWN))\n\t\treturn -EOPNOTSUPP;\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\tif (proto_down)\n\t\tnetif_carrier_off(dev);\n\telse\n\t\tnetif_carrier_on(dev);\n\tdev->proto_down = proto_down;\n\treturn 0;\n}\n\n \nvoid dev_change_proto_down_reason(struct net_device *dev, unsigned long mask,\n\t\t\t\t  u32 value)\n{\n\tint b;\n\n\tif (!mask) {\n\t\tdev->proto_down_reason = value;\n\t} else {\n\t\tfor_each_set_bit(b, &mask, 32) {\n\t\t\tif (value & (1 << b))\n\t\t\t\tdev->proto_down_reason |= BIT(b);\n\t\t\telse\n\t\t\t\tdev->proto_down_reason &= ~BIT(b);\n\t\t}\n\t}\n}\n\nstruct bpf_xdp_link {\n\tstruct bpf_link link;\n\tstruct net_device *dev;  \n\tint flags;\n};\n\nstatic enum bpf_xdp_mode dev_xdp_mode(struct net_device *dev, u32 flags)\n{\n\tif (flags & XDP_FLAGS_HW_MODE)\n\t\treturn XDP_MODE_HW;\n\tif (flags & XDP_FLAGS_DRV_MODE)\n\t\treturn XDP_MODE_DRV;\n\tif (flags & XDP_FLAGS_SKB_MODE)\n\t\treturn XDP_MODE_SKB;\n\treturn dev->netdev_ops->ndo_bpf ? XDP_MODE_DRV : XDP_MODE_SKB;\n}\n\nstatic bpf_op_t dev_xdp_bpf_op(struct net_device *dev, enum bpf_xdp_mode mode)\n{\n\tswitch (mode) {\n\tcase XDP_MODE_SKB:\n\t\treturn generic_xdp_install;\n\tcase XDP_MODE_DRV:\n\tcase XDP_MODE_HW:\n\t\treturn dev->netdev_ops->ndo_bpf;\n\tdefault:\n\t\treturn NULL;\n\t}\n}\n\nstatic struct bpf_xdp_link *dev_xdp_link(struct net_device *dev,\n\t\t\t\t\t enum bpf_xdp_mode mode)\n{\n\treturn dev->xdp_state[mode].link;\n}\n\nstatic struct bpf_prog *dev_xdp_prog(struct net_device *dev,\n\t\t\t\t     enum bpf_xdp_mode mode)\n{\n\tstruct bpf_xdp_link *link = dev_xdp_link(dev, mode);\n\n\tif (link)\n\t\treturn link->link.prog;\n\treturn dev->xdp_state[mode].prog;\n}\n\nu8 dev_xdp_prog_count(struct net_device *dev)\n{\n\tu8 count = 0;\n\tint i;\n\n\tfor (i = 0; i < __MAX_XDP_MODE; i++)\n\t\tif (dev->xdp_state[i].prog || dev->xdp_state[i].link)\n\t\t\tcount++;\n\treturn count;\n}\nEXPORT_SYMBOL_GPL(dev_xdp_prog_count);\n\nu32 dev_xdp_prog_id(struct net_device *dev, enum bpf_xdp_mode mode)\n{\n\tstruct bpf_prog *prog = dev_xdp_prog(dev, mode);\n\n\treturn prog ? prog->aux->id : 0;\n}\n\nstatic void dev_xdp_set_link(struct net_device *dev, enum bpf_xdp_mode mode,\n\t\t\t     struct bpf_xdp_link *link)\n{\n\tdev->xdp_state[mode].link = link;\n\tdev->xdp_state[mode].prog = NULL;\n}\n\nstatic void dev_xdp_set_prog(struct net_device *dev, enum bpf_xdp_mode mode,\n\t\t\t     struct bpf_prog *prog)\n{\n\tdev->xdp_state[mode].link = NULL;\n\tdev->xdp_state[mode].prog = prog;\n}\n\nstatic int dev_xdp_install(struct net_device *dev, enum bpf_xdp_mode mode,\n\t\t\t   bpf_op_t bpf_op, struct netlink_ext_ack *extack,\n\t\t\t   u32 flags, struct bpf_prog *prog)\n{\n\tstruct netdev_bpf xdp;\n\tint err;\n\n\tmemset(&xdp, 0, sizeof(xdp));\n\txdp.command = mode == XDP_MODE_HW ? XDP_SETUP_PROG_HW : XDP_SETUP_PROG;\n\txdp.extack = extack;\n\txdp.flags = flags;\n\txdp.prog = prog;\n\n\t \n\tif (prog)\n\t\tbpf_prog_inc(prog);\n\terr = bpf_op(dev, &xdp);\n\tif (err) {\n\t\tif (prog)\n\t\t\tbpf_prog_put(prog);\n\t\treturn err;\n\t}\n\n\tif (mode != XDP_MODE_HW)\n\t\tbpf_prog_change_xdp(dev_xdp_prog(dev, mode), prog);\n\n\treturn 0;\n}\n\nstatic void dev_xdp_uninstall(struct net_device *dev)\n{\n\tstruct bpf_xdp_link *link;\n\tstruct bpf_prog *prog;\n\tenum bpf_xdp_mode mode;\n\tbpf_op_t bpf_op;\n\n\tASSERT_RTNL();\n\n\tfor (mode = XDP_MODE_SKB; mode < __MAX_XDP_MODE; mode++) {\n\t\tprog = dev_xdp_prog(dev, mode);\n\t\tif (!prog)\n\t\t\tcontinue;\n\n\t\tbpf_op = dev_xdp_bpf_op(dev, mode);\n\t\tif (!bpf_op)\n\t\t\tcontinue;\n\n\t\tWARN_ON(dev_xdp_install(dev, mode, bpf_op, NULL, 0, NULL));\n\n\t\t \n\t\tlink = dev_xdp_link(dev, mode);\n\t\tif (link)\n\t\t\tlink->dev = NULL;\n\t\telse\n\t\t\tbpf_prog_put(prog);\n\n\t\tdev_xdp_set_link(dev, mode, NULL);\n\t}\n}\n\nstatic int dev_xdp_attach(struct net_device *dev, struct netlink_ext_ack *extack,\n\t\t\t  struct bpf_xdp_link *link, struct bpf_prog *new_prog,\n\t\t\t  struct bpf_prog *old_prog, u32 flags)\n{\n\tunsigned int num_modes = hweight32(flags & XDP_FLAGS_MODES);\n\tstruct bpf_prog *cur_prog;\n\tstruct net_device *upper;\n\tstruct list_head *iter;\n\tenum bpf_xdp_mode mode;\n\tbpf_op_t bpf_op;\n\tint err;\n\n\tASSERT_RTNL();\n\n\t \n\tif (link && (new_prog || old_prog))\n\t\treturn -EINVAL;\n\t \n\tif (link && (flags & ~XDP_FLAGS_MODES)) {\n\t\tNL_SET_ERR_MSG(extack, \"Invalid XDP flags for BPF link attachment\");\n\t\treturn -EINVAL;\n\t}\n\t \n\tif (num_modes > 1) {\n\t\tNL_SET_ERR_MSG(extack, \"Only one XDP mode flag can be set\");\n\t\treturn -EINVAL;\n\t}\n\t \n\tif (!num_modes && dev_xdp_prog_count(dev) > 1) {\n\t\tNL_SET_ERR_MSG(extack,\n\t\t\t       \"More than one program loaded, unset mode is ambiguous\");\n\t\treturn -EINVAL;\n\t}\n\t \n\tif (old_prog && !(flags & XDP_FLAGS_REPLACE)) {\n\t\tNL_SET_ERR_MSG(extack, \"XDP_FLAGS_REPLACE is not specified\");\n\t\treturn -EINVAL;\n\t}\n\n\tmode = dev_xdp_mode(dev, flags);\n\t \n\tif (dev_xdp_link(dev, mode)) {\n\t\tNL_SET_ERR_MSG(extack, \"Can't replace active BPF XDP link\");\n\t\treturn -EBUSY;\n\t}\n\n\t \n\tnetdev_for_each_upper_dev_rcu(dev, upper, iter) {\n\t\tif (dev_xdp_prog_count(upper) > 0) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Cannot attach when an upper device already has a program\");\n\t\t\treturn -EEXIST;\n\t\t}\n\t}\n\n\tcur_prog = dev_xdp_prog(dev, mode);\n\t \n\tif (link && cur_prog) {\n\t\tNL_SET_ERR_MSG(extack, \"Can't replace active XDP program with BPF link\");\n\t\treturn -EBUSY;\n\t}\n\tif ((flags & XDP_FLAGS_REPLACE) && cur_prog != old_prog) {\n\t\tNL_SET_ERR_MSG(extack, \"Active program does not match expected\");\n\t\treturn -EEXIST;\n\t}\n\n\t \n\tif (link)\n\t\tnew_prog = link->link.prog;\n\n\tif (new_prog) {\n\t\tbool offload = mode == XDP_MODE_HW;\n\t\tenum bpf_xdp_mode other_mode = mode == XDP_MODE_SKB\n\t\t\t\t\t       ? XDP_MODE_DRV : XDP_MODE_SKB;\n\n\t\tif ((flags & XDP_FLAGS_UPDATE_IF_NOEXIST) && cur_prog) {\n\t\t\tNL_SET_ERR_MSG(extack, \"XDP program already attached\");\n\t\t\treturn -EBUSY;\n\t\t}\n\t\tif (!offload && dev_xdp_prog(dev, other_mode)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Native and generic XDP can't be active at the same time\");\n\t\t\treturn -EEXIST;\n\t\t}\n\t\tif (!offload && bpf_prog_is_offloaded(new_prog->aux)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Using offloaded program without HW_MODE flag is not supported\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (bpf_prog_is_dev_bound(new_prog->aux) && !bpf_offload_dev_match(new_prog, dev)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Program bound to different device\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (new_prog->expected_attach_type == BPF_XDP_DEVMAP) {\n\t\t\tNL_SET_ERR_MSG(extack, \"BPF_XDP_DEVMAP programs can not be attached to a device\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (new_prog->expected_attach_type == BPF_XDP_CPUMAP) {\n\t\t\tNL_SET_ERR_MSG(extack, \"BPF_XDP_CPUMAP programs can not be attached to a device\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t \n\tif (new_prog != cur_prog) {\n\t\tbpf_op = dev_xdp_bpf_op(dev, mode);\n\t\tif (!bpf_op) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Underlying driver does not support XDP in native mode\");\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\terr = dev_xdp_install(dev, mode, bpf_op, extack, flags, new_prog);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (link)\n\t\tdev_xdp_set_link(dev, mode, link);\n\telse\n\t\tdev_xdp_set_prog(dev, mode, new_prog);\n\tif (cur_prog)\n\t\tbpf_prog_put(cur_prog);\n\n\treturn 0;\n}\n\nstatic int dev_xdp_attach_link(struct net_device *dev,\n\t\t\t       struct netlink_ext_ack *extack,\n\t\t\t       struct bpf_xdp_link *link)\n{\n\treturn dev_xdp_attach(dev, extack, link, NULL, NULL, link->flags);\n}\n\nstatic int dev_xdp_detach_link(struct net_device *dev,\n\t\t\t       struct netlink_ext_ack *extack,\n\t\t\t       struct bpf_xdp_link *link)\n{\n\tenum bpf_xdp_mode mode;\n\tbpf_op_t bpf_op;\n\n\tASSERT_RTNL();\n\n\tmode = dev_xdp_mode(dev, link->flags);\n\tif (dev_xdp_link(dev, mode) != link)\n\t\treturn -EINVAL;\n\n\tbpf_op = dev_xdp_bpf_op(dev, mode);\n\tWARN_ON(dev_xdp_install(dev, mode, bpf_op, NULL, 0, NULL));\n\tdev_xdp_set_link(dev, mode, NULL);\n\treturn 0;\n}\n\nstatic void bpf_xdp_link_release(struct bpf_link *link)\n{\n\tstruct bpf_xdp_link *xdp_link = container_of(link, struct bpf_xdp_link, link);\n\n\trtnl_lock();\n\n\t \n\tif (xdp_link->dev) {\n\t\tWARN_ON(dev_xdp_detach_link(xdp_link->dev, NULL, xdp_link));\n\t\txdp_link->dev = NULL;\n\t}\n\n\trtnl_unlock();\n}\n\nstatic int bpf_xdp_link_detach(struct bpf_link *link)\n{\n\tbpf_xdp_link_release(link);\n\treturn 0;\n}\n\nstatic void bpf_xdp_link_dealloc(struct bpf_link *link)\n{\n\tstruct bpf_xdp_link *xdp_link = container_of(link, struct bpf_xdp_link, link);\n\n\tkfree(xdp_link);\n}\n\nstatic void bpf_xdp_link_show_fdinfo(const struct bpf_link *link,\n\t\t\t\t     struct seq_file *seq)\n{\n\tstruct bpf_xdp_link *xdp_link = container_of(link, struct bpf_xdp_link, link);\n\tu32 ifindex = 0;\n\n\trtnl_lock();\n\tif (xdp_link->dev)\n\t\tifindex = xdp_link->dev->ifindex;\n\trtnl_unlock();\n\n\tseq_printf(seq, \"ifindex:\\t%u\\n\", ifindex);\n}\n\nstatic int bpf_xdp_link_fill_link_info(const struct bpf_link *link,\n\t\t\t\t       struct bpf_link_info *info)\n{\n\tstruct bpf_xdp_link *xdp_link = container_of(link, struct bpf_xdp_link, link);\n\tu32 ifindex = 0;\n\n\trtnl_lock();\n\tif (xdp_link->dev)\n\t\tifindex = xdp_link->dev->ifindex;\n\trtnl_unlock();\n\n\tinfo->xdp.ifindex = ifindex;\n\treturn 0;\n}\n\nstatic int bpf_xdp_link_update(struct bpf_link *link, struct bpf_prog *new_prog,\n\t\t\t       struct bpf_prog *old_prog)\n{\n\tstruct bpf_xdp_link *xdp_link = container_of(link, struct bpf_xdp_link, link);\n\tenum bpf_xdp_mode mode;\n\tbpf_op_t bpf_op;\n\tint err = 0;\n\n\trtnl_lock();\n\n\t \n\tif (!xdp_link->dev) {\n\t\terr = -ENOLINK;\n\t\tgoto out_unlock;\n\t}\n\n\tif (old_prog && link->prog != old_prog) {\n\t\terr = -EPERM;\n\t\tgoto out_unlock;\n\t}\n\told_prog = link->prog;\n\tif (old_prog->type != new_prog->type ||\n\t    old_prog->expected_attach_type != new_prog->expected_attach_type) {\n\t\terr = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\tif (old_prog == new_prog) {\n\t\t \n\t\tbpf_prog_put(new_prog);\n\t\tgoto out_unlock;\n\t}\n\n\tmode = dev_xdp_mode(xdp_link->dev, xdp_link->flags);\n\tbpf_op = dev_xdp_bpf_op(xdp_link->dev, mode);\n\terr = dev_xdp_install(xdp_link->dev, mode, bpf_op, NULL,\n\t\t\t      xdp_link->flags, new_prog);\n\tif (err)\n\t\tgoto out_unlock;\n\n\told_prog = xchg(&link->prog, new_prog);\n\tbpf_prog_put(old_prog);\n\nout_unlock:\n\trtnl_unlock();\n\treturn err;\n}\n\nstatic const struct bpf_link_ops bpf_xdp_link_lops = {\n\t.release = bpf_xdp_link_release,\n\t.dealloc = bpf_xdp_link_dealloc,\n\t.detach = bpf_xdp_link_detach,\n\t.show_fdinfo = bpf_xdp_link_show_fdinfo,\n\t.fill_link_info = bpf_xdp_link_fill_link_info,\n\t.update_prog = bpf_xdp_link_update,\n};\n\nint bpf_xdp_link_attach(const union bpf_attr *attr, struct bpf_prog *prog)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct bpf_link_primer link_primer;\n\tstruct netlink_ext_ack extack = {};\n\tstruct bpf_xdp_link *link;\n\tstruct net_device *dev;\n\tint err, fd;\n\n\trtnl_lock();\n\tdev = dev_get_by_index(net, attr->link_create.target_ifindex);\n\tif (!dev) {\n\t\trtnl_unlock();\n\t\treturn -EINVAL;\n\t}\n\n\tlink = kzalloc(sizeof(*link), GFP_USER);\n\tif (!link) {\n\t\terr = -ENOMEM;\n\t\tgoto unlock;\n\t}\n\n\tbpf_link_init(&link->link, BPF_LINK_TYPE_XDP, &bpf_xdp_link_lops, prog);\n\tlink->dev = dev;\n\tlink->flags = attr->link_create.flags;\n\n\terr = bpf_link_prime(&link->link, &link_primer);\n\tif (err) {\n\t\tkfree(link);\n\t\tgoto unlock;\n\t}\n\n\terr = dev_xdp_attach_link(dev, &extack, link);\n\trtnl_unlock();\n\n\tif (err) {\n\t\tlink->dev = NULL;\n\t\tbpf_link_cleanup(&link_primer);\n\t\ttrace_bpf_xdp_link_attach_failed(extack._msg);\n\t\tgoto out_put_dev;\n\t}\n\n\tfd = bpf_link_settle(&link_primer);\n\t \n\tdev_put(dev);\n\treturn fd;\n\nunlock:\n\trtnl_unlock();\n\nout_put_dev:\n\tdev_put(dev);\n\treturn err;\n}\n\n \nint dev_change_xdp_fd(struct net_device *dev, struct netlink_ext_ack *extack,\n\t\t      int fd, int expected_fd, u32 flags)\n{\n\tenum bpf_xdp_mode mode = dev_xdp_mode(dev, flags);\n\tstruct bpf_prog *new_prog = NULL, *old_prog = NULL;\n\tint err;\n\n\tASSERT_RTNL();\n\n\tif (fd >= 0) {\n\t\tnew_prog = bpf_prog_get_type_dev(fd, BPF_PROG_TYPE_XDP,\n\t\t\t\t\t\t mode != XDP_MODE_SKB);\n\t\tif (IS_ERR(new_prog))\n\t\t\treturn PTR_ERR(new_prog);\n\t}\n\n\tif (expected_fd >= 0) {\n\t\told_prog = bpf_prog_get_type_dev(expected_fd, BPF_PROG_TYPE_XDP,\n\t\t\t\t\t\t mode != XDP_MODE_SKB);\n\t\tif (IS_ERR(old_prog)) {\n\t\t\terr = PTR_ERR(old_prog);\n\t\t\told_prog = NULL;\n\t\t\tgoto err_out;\n\t\t}\n\t}\n\n\terr = dev_xdp_attach(dev, extack, NULL, new_prog, old_prog, flags);\n\nerr_out:\n\tif (err && new_prog)\n\t\tbpf_prog_put(new_prog);\n\tif (old_prog)\n\t\tbpf_prog_put(old_prog);\n\treturn err;\n}\n\n \nstatic int dev_index_reserve(struct net *net, u32 ifindex)\n{\n\tint err;\n\n\tif (ifindex > INT_MAX) {\n\t\tDEBUG_NET_WARN_ON_ONCE(1);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ifindex)\n\t\terr = xa_alloc_cyclic(&net->dev_by_index, &ifindex, NULL,\n\t\t\t\t      xa_limit_31b, &net->ifindex, GFP_KERNEL);\n\telse\n\t\terr = xa_insert(&net->dev_by_index, ifindex, NULL, GFP_KERNEL);\n\tif (err < 0)\n\t\treturn err;\n\n\treturn ifindex;\n}\n\nstatic void dev_index_release(struct net *net, int ifindex)\n{\n\t \n\tWARN_ON(xa_erase(&net->dev_by_index, ifindex));\n}\n\n \nLIST_HEAD(net_todo_list);\nDECLARE_WAIT_QUEUE_HEAD(netdev_unregistering_wq);\n\nstatic void net_set_todo(struct net_device *dev)\n{\n\tlist_add_tail(&dev->todo_list, &net_todo_list);\n\tatomic_inc(&dev_net(dev)->dev_unreg_count);\n}\n\nstatic netdev_features_t netdev_sync_upper_features(struct net_device *lower,\n\tstruct net_device *upper, netdev_features_t features)\n{\n\tnetdev_features_t upper_disables = NETIF_F_UPPER_DISABLES;\n\tnetdev_features_t feature;\n\tint feature_bit;\n\n\tfor_each_netdev_feature(upper_disables, feature_bit) {\n\t\tfeature = __NETIF_F_BIT(feature_bit);\n\t\tif (!(upper->wanted_features & feature)\n\t\t    && (features & feature)) {\n\t\t\tnetdev_dbg(lower, \"Dropping feature %pNF, upper dev %s has it off.\\n\",\n\t\t\t\t   &feature, upper->name);\n\t\t\tfeatures &= ~feature;\n\t\t}\n\t}\n\n\treturn features;\n}\n\nstatic void netdev_sync_lower_features(struct net_device *upper,\n\tstruct net_device *lower, netdev_features_t features)\n{\n\tnetdev_features_t upper_disables = NETIF_F_UPPER_DISABLES;\n\tnetdev_features_t feature;\n\tint feature_bit;\n\n\tfor_each_netdev_feature(upper_disables, feature_bit) {\n\t\tfeature = __NETIF_F_BIT(feature_bit);\n\t\tif (!(features & feature) && (lower->features & feature)) {\n\t\t\tnetdev_dbg(upper, \"Disabling feature %pNF on lower dev %s.\\n\",\n\t\t\t\t   &feature, lower->name);\n\t\t\tlower->wanted_features &= ~feature;\n\t\t\t__netdev_update_features(lower);\n\n\t\t\tif (unlikely(lower->features & feature))\n\t\t\t\tnetdev_WARN(upper, \"failed to disable %pNF on %s!\\n\",\n\t\t\t\t\t    &feature, lower->name);\n\t\t\telse\n\t\t\t\tnetdev_features_change(lower);\n\t\t}\n\t}\n}\n\nstatic netdev_features_t netdev_fix_features(struct net_device *dev,\n\tnetdev_features_t features)\n{\n\t \n\tif ((features & NETIF_F_HW_CSUM) &&\n\t    (features & (NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM))) {\n\t\tnetdev_warn(dev, \"mixed HW and IP checksum settings.\\n\");\n\t\tfeatures &= ~(NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM);\n\t}\n\n\t \n\tif ((features & NETIF_F_ALL_TSO) && !(features & NETIF_F_SG)) {\n\t\tnetdev_dbg(dev, \"Dropping TSO features since no SG feature.\\n\");\n\t\tfeatures &= ~NETIF_F_ALL_TSO;\n\t}\n\n\tif ((features & NETIF_F_TSO) && !(features & NETIF_F_HW_CSUM) &&\n\t\t\t\t\t!(features & NETIF_F_IP_CSUM)) {\n\t\tnetdev_dbg(dev, \"Dropping TSO features since no CSUM feature.\\n\");\n\t\tfeatures &= ~NETIF_F_TSO;\n\t\tfeatures &= ~NETIF_F_TSO_ECN;\n\t}\n\n\tif ((features & NETIF_F_TSO6) && !(features & NETIF_F_HW_CSUM) &&\n\t\t\t\t\t !(features & NETIF_F_IPV6_CSUM)) {\n\t\tnetdev_dbg(dev, \"Dropping TSO6 features since no CSUM feature.\\n\");\n\t\tfeatures &= ~NETIF_F_TSO6;\n\t}\n\n\t \n\tif ((features & NETIF_F_TSO_MANGLEID) && !(features & NETIF_F_TSO))\n\t\tfeatures &= ~NETIF_F_TSO_MANGLEID;\n\n\t \n\tif ((features & NETIF_F_ALL_TSO) == NETIF_F_TSO_ECN)\n\t\tfeatures &= ~NETIF_F_TSO_ECN;\n\n\t \n\tif ((features & NETIF_F_GSO) && !(features & NETIF_F_SG)) {\n\t\tnetdev_dbg(dev, \"Dropping NETIF_F_GSO since no SG feature.\\n\");\n\t\tfeatures &= ~NETIF_F_GSO;\n\t}\n\n\t \n\tif ((features & dev->gso_partial_features) &&\n\t    !(features & NETIF_F_GSO_PARTIAL)) {\n\t\tnetdev_dbg(dev,\n\t\t\t   \"Dropping partially supported GSO features since no GSO partial.\\n\");\n\t\tfeatures &= ~dev->gso_partial_features;\n\t}\n\n\tif (!(features & NETIF_F_RXCSUM)) {\n\t\t \n\t\tif (features & NETIF_F_GRO_HW) {\n\t\t\tnetdev_dbg(dev, \"Dropping NETIF_F_GRO_HW since no RXCSUM feature.\\n\");\n\t\t\tfeatures &= ~NETIF_F_GRO_HW;\n\t\t}\n\t}\n\n\t \n\tif (features & NETIF_F_RXFCS) {\n\t\tif (features & NETIF_F_LRO) {\n\t\t\tnetdev_dbg(dev, \"Dropping LRO feature since RX-FCS is requested.\\n\");\n\t\t\tfeatures &= ~NETIF_F_LRO;\n\t\t}\n\n\t\tif (features & NETIF_F_GRO_HW) {\n\t\t\tnetdev_dbg(dev, \"Dropping HW-GRO feature since RX-FCS is requested.\\n\");\n\t\t\tfeatures &= ~NETIF_F_GRO_HW;\n\t\t}\n\t}\n\n\tif ((features & NETIF_F_GRO_HW) && (features & NETIF_F_LRO)) {\n\t\tnetdev_dbg(dev, \"Dropping LRO feature since HW-GRO is requested.\\n\");\n\t\tfeatures &= ~NETIF_F_LRO;\n\t}\n\n\tif (features & NETIF_F_HW_TLS_TX) {\n\t\tbool ip_csum = (features & (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM)) ==\n\t\t\t(NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM);\n\t\tbool hw_csum = features & NETIF_F_HW_CSUM;\n\n\t\tif (!ip_csum && !hw_csum) {\n\t\t\tnetdev_dbg(dev, \"Dropping TLS TX HW offload feature since no CSUM feature.\\n\");\n\t\t\tfeatures &= ~NETIF_F_HW_TLS_TX;\n\t\t}\n\t}\n\n\tif ((features & NETIF_F_HW_TLS_RX) && !(features & NETIF_F_RXCSUM)) {\n\t\tnetdev_dbg(dev, \"Dropping TLS RX HW offload feature since no RXCSUM feature.\\n\");\n\t\tfeatures &= ~NETIF_F_HW_TLS_RX;\n\t}\n\n\treturn features;\n}\n\nint __netdev_update_features(struct net_device *dev)\n{\n\tstruct net_device *upper, *lower;\n\tnetdev_features_t features;\n\tstruct list_head *iter;\n\tint err = -1;\n\n\tASSERT_RTNL();\n\n\tfeatures = netdev_get_wanted_features(dev);\n\n\tif (dev->netdev_ops->ndo_fix_features)\n\t\tfeatures = dev->netdev_ops->ndo_fix_features(dev, features);\n\n\t \n\tfeatures = netdev_fix_features(dev, features);\n\n\t \n\tnetdev_for_each_upper_dev_rcu(dev, upper, iter)\n\t\tfeatures = netdev_sync_upper_features(dev, upper, features);\n\n\tif (dev->features == features)\n\t\tgoto sync_lower;\n\n\tnetdev_dbg(dev, \"Features changed: %pNF -> %pNF\\n\",\n\t\t&dev->features, &features);\n\n\tif (dev->netdev_ops->ndo_set_features)\n\t\terr = dev->netdev_ops->ndo_set_features(dev, features);\n\telse\n\t\terr = 0;\n\n\tif (unlikely(err < 0)) {\n\t\tnetdev_err(dev,\n\t\t\t\"set_features() failed (%d); wanted %pNF, left %pNF\\n\",\n\t\t\terr, &features, &dev->features);\n\t\t \n\t\treturn -1;\n\t}\n\nsync_lower:\n\t \n\tnetdev_for_each_lower_dev(dev, lower, iter)\n\t\tnetdev_sync_lower_features(dev, lower, features);\n\n\tif (!err) {\n\t\tnetdev_features_t diff = features ^ dev->features;\n\n\t\tif (diff & NETIF_F_RX_UDP_TUNNEL_PORT) {\n\t\t\t \n\t\t\tif (features & NETIF_F_RX_UDP_TUNNEL_PORT) {\n\t\t\t\tdev->features = features;\n\t\t\t\tudp_tunnel_get_rx_info(dev);\n\t\t\t} else {\n\t\t\t\tudp_tunnel_drop_rx_info(dev);\n\t\t\t}\n\t\t}\n\n\t\tif (diff & NETIF_F_HW_VLAN_CTAG_FILTER) {\n\t\t\tif (features & NETIF_F_HW_VLAN_CTAG_FILTER) {\n\t\t\t\tdev->features = features;\n\t\t\t\terr |= vlan_get_rx_ctag_filter_info(dev);\n\t\t\t} else {\n\t\t\t\tvlan_drop_rx_ctag_filter_info(dev);\n\t\t\t}\n\t\t}\n\n\t\tif (diff & NETIF_F_HW_VLAN_STAG_FILTER) {\n\t\t\tif (features & NETIF_F_HW_VLAN_STAG_FILTER) {\n\t\t\t\tdev->features = features;\n\t\t\t\terr |= vlan_get_rx_stag_filter_info(dev);\n\t\t\t} else {\n\t\t\t\tvlan_drop_rx_stag_filter_info(dev);\n\t\t\t}\n\t\t}\n\n\t\tdev->features = features;\n\t}\n\n\treturn err < 0 ? 0 : 1;\n}\n\n \nvoid netdev_update_features(struct net_device *dev)\n{\n\tif (__netdev_update_features(dev))\n\t\tnetdev_features_change(dev);\n}\nEXPORT_SYMBOL(netdev_update_features);\n\n \nvoid netdev_change_features(struct net_device *dev)\n{\n\t__netdev_update_features(dev);\n\tnetdev_features_change(dev);\n}\nEXPORT_SYMBOL(netdev_change_features);\n\n \nvoid netif_stacked_transfer_operstate(const struct net_device *rootdev,\n\t\t\t\t\tstruct net_device *dev)\n{\n\tif (rootdev->operstate == IF_OPER_DORMANT)\n\t\tnetif_dormant_on(dev);\n\telse\n\t\tnetif_dormant_off(dev);\n\n\tif (rootdev->operstate == IF_OPER_TESTING)\n\t\tnetif_testing_on(dev);\n\telse\n\t\tnetif_testing_off(dev);\n\n\tif (netif_carrier_ok(rootdev))\n\t\tnetif_carrier_on(dev);\n\telse\n\t\tnetif_carrier_off(dev);\n}\nEXPORT_SYMBOL(netif_stacked_transfer_operstate);\n\nstatic int netif_alloc_rx_queues(struct net_device *dev)\n{\n\tunsigned int i, count = dev->num_rx_queues;\n\tstruct netdev_rx_queue *rx;\n\tsize_t sz = count * sizeof(*rx);\n\tint err = 0;\n\n\tBUG_ON(count < 1);\n\n\trx = kvzalloc(sz, GFP_KERNEL_ACCOUNT | __GFP_RETRY_MAYFAIL);\n\tif (!rx)\n\t\treturn -ENOMEM;\n\n\tdev->_rx = rx;\n\n\tfor (i = 0; i < count; i++) {\n\t\trx[i].dev = dev;\n\n\t\t \n\t\terr = xdp_rxq_info_reg(&rx[i].xdp_rxq, dev, i, 0);\n\t\tif (err < 0)\n\t\t\tgoto err_rxq_info;\n\t}\n\treturn 0;\n\nerr_rxq_info:\n\t \n\twhile (i--)\n\t\txdp_rxq_info_unreg(&rx[i].xdp_rxq);\n\tkvfree(dev->_rx);\n\tdev->_rx = NULL;\n\treturn err;\n}\n\nstatic void netif_free_rx_queues(struct net_device *dev)\n{\n\tunsigned int i, count = dev->num_rx_queues;\n\n\t \n\tif (!dev->_rx)\n\t\treturn;\n\n\tfor (i = 0; i < count; i++)\n\t\txdp_rxq_info_unreg(&dev->_rx[i].xdp_rxq);\n\n\tkvfree(dev->_rx);\n}\n\nstatic void netdev_init_one_queue(struct net_device *dev,\n\t\t\t\t  struct netdev_queue *queue, void *_unused)\n{\n\t \n\tspin_lock_init(&queue->_xmit_lock);\n\tnetdev_set_xmit_lockdep_class(&queue->_xmit_lock, dev->type);\n\tqueue->xmit_lock_owner = -1;\n\tnetdev_queue_numa_node_write(queue, NUMA_NO_NODE);\n\tqueue->dev = dev;\n#ifdef CONFIG_BQL\n\tdql_init(&queue->dql, HZ);\n#endif\n}\n\nstatic void netif_free_tx_queues(struct net_device *dev)\n{\n\tkvfree(dev->_tx);\n}\n\nstatic int netif_alloc_netdev_queues(struct net_device *dev)\n{\n\tunsigned int count = dev->num_tx_queues;\n\tstruct netdev_queue *tx;\n\tsize_t sz = count * sizeof(*tx);\n\n\tif (count < 1 || count > 0xffff)\n\t\treturn -EINVAL;\n\n\ttx = kvzalloc(sz, GFP_KERNEL_ACCOUNT | __GFP_RETRY_MAYFAIL);\n\tif (!tx)\n\t\treturn -ENOMEM;\n\n\tdev->_tx = tx;\n\n\tnetdev_for_each_tx_queue(dev, netdev_init_one_queue, NULL);\n\tspin_lock_init(&dev->tx_global_lock);\n\n\treturn 0;\n}\n\nvoid netif_tx_stop_all_queues(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\n\t\tnetif_tx_stop_queue(txq);\n\t}\n}\nEXPORT_SYMBOL(netif_tx_stop_all_queues);\n\nstatic int netdev_do_alloc_pcpu_stats(struct net_device *dev)\n{\n\tvoid __percpu *v;\n\n\t \n\tif (dev->netdev_ops->ndo_get_peer_dev &&\n\t    dev->pcpu_stat_type != NETDEV_PCPU_STAT_TSTATS)\n\t\treturn -EOPNOTSUPP;\n\n\tswitch (dev->pcpu_stat_type) {\n\tcase NETDEV_PCPU_STAT_NONE:\n\t\treturn 0;\n\tcase NETDEV_PCPU_STAT_LSTATS:\n\t\tv = dev->lstats = netdev_alloc_pcpu_stats(struct pcpu_lstats);\n\t\tbreak;\n\tcase NETDEV_PCPU_STAT_TSTATS:\n\t\tv = dev->tstats = netdev_alloc_pcpu_stats(struct pcpu_sw_netstats);\n\t\tbreak;\n\tcase NETDEV_PCPU_STAT_DSTATS:\n\t\tv = dev->dstats = netdev_alloc_pcpu_stats(struct pcpu_dstats);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn v ? 0 : -ENOMEM;\n}\n\nstatic void netdev_do_free_pcpu_stats(struct net_device *dev)\n{\n\tswitch (dev->pcpu_stat_type) {\n\tcase NETDEV_PCPU_STAT_NONE:\n\t\treturn;\n\tcase NETDEV_PCPU_STAT_LSTATS:\n\t\tfree_percpu(dev->lstats);\n\t\tbreak;\n\tcase NETDEV_PCPU_STAT_TSTATS:\n\t\tfree_percpu(dev->tstats);\n\t\tbreak;\n\tcase NETDEV_PCPU_STAT_DSTATS:\n\t\tfree_percpu(dev->dstats);\n\t\tbreak;\n\t}\n}\n\n \nint register_netdevice(struct net_device *dev)\n{\n\tint ret;\n\tstruct net *net = dev_net(dev);\n\n\tBUILD_BUG_ON(sizeof(netdev_features_t) * BITS_PER_BYTE <\n\t\t     NETDEV_FEATURE_COUNT);\n\tBUG_ON(dev_boot_phase);\n\tASSERT_RTNL();\n\n\tmight_sleep();\n\n\t \n\tBUG_ON(dev->reg_state != NETREG_UNINITIALIZED);\n\tBUG_ON(!net);\n\n\tret = ethtool_check_ops(dev->ethtool_ops);\n\tif (ret)\n\t\treturn ret;\n\n\tspin_lock_init(&dev->addr_list_lock);\n\tnetdev_set_addr_lockdep_class(dev);\n\n\tret = dev_get_valid_name(net, dev, dev->name);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tret = -ENOMEM;\n\tdev->name_node = netdev_name_node_head_alloc(dev);\n\tif (!dev->name_node)\n\t\tgoto out;\n\n\t \n\tif (dev->netdev_ops->ndo_init) {\n\t\tret = dev->netdev_ops->ndo_init(dev);\n\t\tif (ret) {\n\t\t\tif (ret > 0)\n\t\t\t\tret = -EIO;\n\t\t\tgoto err_free_name;\n\t\t}\n\t}\n\n\tif (((dev->hw_features | dev->features) &\n\t     NETIF_F_HW_VLAN_CTAG_FILTER) &&\n\t    (!dev->netdev_ops->ndo_vlan_rx_add_vid ||\n\t     !dev->netdev_ops->ndo_vlan_rx_kill_vid)) {\n\t\tnetdev_WARN(dev, \"Buggy VLAN acceleration in driver!\\n\");\n\t\tret = -EINVAL;\n\t\tgoto err_uninit;\n\t}\n\n\tret = netdev_do_alloc_pcpu_stats(dev);\n\tif (ret)\n\t\tgoto err_uninit;\n\n\tret = dev_index_reserve(net, dev->ifindex);\n\tif (ret < 0)\n\t\tgoto err_free_pcpu;\n\tdev->ifindex = ret;\n\n\t \n\tdev->hw_features |= (NETIF_F_SOFT_FEATURES | NETIF_F_SOFT_FEATURES_OFF);\n\tdev->features |= NETIF_F_SOFT_FEATURES;\n\n\tif (dev->udp_tunnel_nic_info) {\n\t\tdev->features |= NETIF_F_RX_UDP_TUNNEL_PORT;\n\t\tdev->hw_features |= NETIF_F_RX_UDP_TUNNEL_PORT;\n\t}\n\n\tdev->wanted_features = dev->features & dev->hw_features;\n\n\tif (!(dev->flags & IFF_LOOPBACK))\n\t\tdev->hw_features |= NETIF_F_NOCACHE_COPY;\n\n\t \n\tif (dev->hw_features & NETIF_F_TSO)\n\t\tdev->hw_features |= NETIF_F_TSO_MANGLEID;\n\tif (dev->vlan_features & NETIF_F_TSO)\n\t\tdev->vlan_features |= NETIF_F_TSO_MANGLEID;\n\tif (dev->mpls_features & NETIF_F_TSO)\n\t\tdev->mpls_features |= NETIF_F_TSO_MANGLEID;\n\tif (dev->hw_enc_features & NETIF_F_TSO)\n\t\tdev->hw_enc_features |= NETIF_F_TSO_MANGLEID;\n\n\t \n\tdev->vlan_features |= NETIF_F_HIGHDMA;\n\n\t \n\tdev->hw_enc_features |= NETIF_F_SG | NETIF_F_GSO_PARTIAL;\n\n\t \n\tdev->mpls_features |= NETIF_F_SG;\n\n\tret = call_netdevice_notifiers(NETDEV_POST_INIT, dev);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\tgoto err_ifindex_release;\n\n\tret = netdev_register_kobject(dev);\n\twrite_lock(&dev_base_lock);\n\tdev->reg_state = ret ? NETREG_UNREGISTERED : NETREG_REGISTERED;\n\twrite_unlock(&dev_base_lock);\n\tif (ret)\n\t\tgoto err_uninit_notify;\n\n\t__netdev_update_features(dev);\n\n\t \n\n\tset_bit(__LINK_STATE_PRESENT, &dev->state);\n\n\tlinkwatch_init_dev(dev);\n\n\tdev_init_scheduler(dev);\n\n\tnetdev_hold(dev, &dev->dev_registered_tracker, GFP_KERNEL);\n\tlist_netdevice(dev);\n\n\tadd_device_randomness(dev->dev_addr, dev->addr_len);\n\n\t \n\tif (dev->addr_assign_type == NET_ADDR_PERM)\n\t\tmemcpy(dev->perm_addr, dev->dev_addr, dev->addr_len);\n\n\t \n\tret = call_netdevice_notifiers(NETDEV_REGISTER, dev);\n\tret = notifier_to_errno(ret);\n\tif (ret) {\n\t\t \n\t\tdev->needs_free_netdev = false;\n\t\tunregister_netdevice_queue(dev, NULL);\n\t\tgoto out;\n\t}\n\t \n\tif (!dev->rtnl_link_ops ||\n\t    dev->rtnl_link_state == RTNL_LINK_INITIALIZED)\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, ~0U, GFP_KERNEL, 0, NULL);\n\nout:\n\treturn ret;\n\nerr_uninit_notify:\n\tcall_netdevice_notifiers(NETDEV_PRE_UNINIT, dev);\nerr_ifindex_release:\n\tdev_index_release(net, dev->ifindex);\nerr_free_pcpu:\n\tnetdev_do_free_pcpu_stats(dev);\nerr_uninit:\n\tif (dev->netdev_ops->ndo_uninit)\n\t\tdev->netdev_ops->ndo_uninit(dev);\n\tif (dev->priv_destructor)\n\t\tdev->priv_destructor(dev);\nerr_free_name:\n\tnetdev_name_node_free(dev->name_node);\n\tgoto out;\n}\nEXPORT_SYMBOL(register_netdevice);\n\n \nint init_dummy_netdev(struct net_device *dev)\n{\n\t \n\tmemset(dev, 0, sizeof(struct net_device));\n\n\t \n\tdev->reg_state = NETREG_DUMMY;\n\n\t \n\tINIT_LIST_HEAD(&dev->napi_list);\n\n\t \n\tset_bit(__LINK_STATE_PRESENT, &dev->state);\n\tset_bit(__LINK_STATE_START, &dev->state);\n\n\t \n\tdev_net_set(dev, &init_net);\n\n\t \n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(init_dummy_netdev);\n\n\n \nint register_netdev(struct net_device *dev)\n{\n\tint err;\n\n\tif (rtnl_lock_killable())\n\t\treturn -EINTR;\n\terr = register_netdevice(dev);\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(register_netdev);\n\nint netdev_refcnt_read(const struct net_device *dev)\n{\n#ifdef CONFIG_PCPU_DEV_REFCNT\n\tint i, refcnt = 0;\n\n\tfor_each_possible_cpu(i)\n\t\trefcnt += *per_cpu_ptr(dev->pcpu_refcnt, i);\n\treturn refcnt;\n#else\n\treturn refcount_read(&dev->dev_refcnt);\n#endif\n}\nEXPORT_SYMBOL(netdev_refcnt_read);\n\nint netdev_unregister_timeout_secs __read_mostly = 10;\n\n#define WAIT_REFS_MIN_MSECS 1\n#define WAIT_REFS_MAX_MSECS 250\n \nstatic struct net_device *netdev_wait_allrefs_any(struct list_head *list)\n{\n\tunsigned long rebroadcast_time, warning_time;\n\tstruct net_device *dev;\n\tint wait = 0;\n\n\trebroadcast_time = warning_time = jiffies;\n\n\tlist_for_each_entry(dev, list, todo_list)\n\t\tif (netdev_refcnt_read(dev) == 1)\n\t\t\treturn dev;\n\n\twhile (true) {\n\t\tif (time_after(jiffies, rebroadcast_time + 1 * HZ)) {\n\t\t\trtnl_lock();\n\n\t\t\t \n\t\t\tlist_for_each_entry(dev, list, todo_list)\n\t\t\t\tcall_netdevice_notifiers(NETDEV_UNREGISTER, dev);\n\n\t\t\t__rtnl_unlock();\n\t\t\trcu_barrier();\n\t\t\trtnl_lock();\n\n\t\t\tlist_for_each_entry(dev, list, todo_list)\n\t\t\t\tif (test_bit(__LINK_STATE_LINKWATCH_PENDING,\n\t\t\t\t\t     &dev->state)) {\n\t\t\t\t\t \n\t\t\t\t\tlinkwatch_run_queue();\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t__rtnl_unlock();\n\n\t\t\trebroadcast_time = jiffies;\n\t\t}\n\n\t\tif (!wait) {\n\t\t\trcu_barrier();\n\t\t\twait = WAIT_REFS_MIN_MSECS;\n\t\t} else {\n\t\t\tmsleep(wait);\n\t\t\twait = min(wait << 1, WAIT_REFS_MAX_MSECS);\n\t\t}\n\n\t\tlist_for_each_entry(dev, list, todo_list)\n\t\t\tif (netdev_refcnt_read(dev) == 1)\n\t\t\t\treturn dev;\n\n\t\tif (time_after(jiffies, warning_time +\n\t\t\t       READ_ONCE(netdev_unregister_timeout_secs) * HZ)) {\n\t\t\tlist_for_each_entry(dev, list, todo_list) {\n\t\t\t\tpr_emerg(\"unregister_netdevice: waiting for %s to become free. Usage count = %d\\n\",\n\t\t\t\t\t dev->name, netdev_refcnt_read(dev));\n\t\t\t\tref_tracker_dir_print(&dev->refcnt_tracker, 10);\n\t\t\t}\n\n\t\t\twarning_time = jiffies;\n\t\t}\n\t}\n}\n\n \nvoid netdev_run_todo(void)\n{\n\tstruct net_device *dev, *tmp;\n\tstruct list_head list;\n#ifdef CONFIG_LOCKDEP\n\tstruct list_head unlink_list;\n\n\tlist_replace_init(&net_unlink_list, &unlink_list);\n\n\twhile (!list_empty(&unlink_list)) {\n\t\tstruct net_device *dev = list_first_entry(&unlink_list,\n\t\t\t\t\t\t\t  struct net_device,\n\t\t\t\t\t\t\t  unlink_list);\n\t\tlist_del_init(&dev->unlink_list);\n\t\tdev->nested_level = dev->lower_level - 1;\n\t}\n#endif\n\n\t \n\tlist_replace_init(&net_todo_list, &list);\n\n\t__rtnl_unlock();\n\n\t \n\tif (!list_empty(&list))\n\t\trcu_barrier();\n\n\tlist_for_each_entry_safe(dev, tmp, &list, todo_list) {\n\t\tif (unlikely(dev->reg_state != NETREG_UNREGISTERING)) {\n\t\t\tnetdev_WARN(dev, \"run_todo but not unregistering\\n\");\n\t\t\tlist_del(&dev->todo_list);\n\t\t\tcontinue;\n\t\t}\n\n\t\twrite_lock(&dev_base_lock);\n\t\tdev->reg_state = NETREG_UNREGISTERED;\n\t\twrite_unlock(&dev_base_lock);\n\t\tlinkwatch_forget_dev(dev);\n\t}\n\n\twhile (!list_empty(&list)) {\n\t\tdev = netdev_wait_allrefs_any(&list);\n\t\tlist_del(&dev->todo_list);\n\n\t\t \n\t\tBUG_ON(netdev_refcnt_read(dev) != 1);\n\t\tBUG_ON(!list_empty(&dev->ptype_all));\n\t\tBUG_ON(!list_empty(&dev->ptype_specific));\n\t\tWARN_ON(rcu_access_pointer(dev->ip_ptr));\n\t\tWARN_ON(rcu_access_pointer(dev->ip6_ptr));\n\n\t\tnetdev_do_free_pcpu_stats(dev);\n\t\tif (dev->priv_destructor)\n\t\t\tdev->priv_destructor(dev);\n\t\tif (dev->needs_free_netdev)\n\t\t\tfree_netdev(dev);\n\n\t\tif (atomic_dec_and_test(&dev_net(dev)->dev_unreg_count))\n\t\t\twake_up(&netdev_unregistering_wq);\n\n\t\t \n\t\tkobject_put(&dev->dev.kobj);\n\t}\n}\n\n \nvoid netdev_stats_to_stats64(struct rtnl_link_stats64 *stats64,\n\t\t\t     const struct net_device_stats *netdev_stats)\n{\n\tsize_t i, n = sizeof(*netdev_stats) / sizeof(atomic_long_t);\n\tconst atomic_long_t *src = (atomic_long_t *)netdev_stats;\n\tu64 *dst = (u64 *)stats64;\n\n\tBUILD_BUG_ON(n > sizeof(*stats64) / sizeof(u64));\n\tfor (i = 0; i < n; i++)\n\t\tdst[i] = (unsigned long)atomic_long_read(&src[i]);\n\t \n\tmemset((char *)stats64 + n * sizeof(u64), 0,\n\t       sizeof(*stats64) - n * sizeof(u64));\n}\nEXPORT_SYMBOL(netdev_stats_to_stats64);\n\nstruct net_device_core_stats __percpu *netdev_core_stats_alloc(struct net_device *dev)\n{\n\tstruct net_device_core_stats __percpu *p;\n\n\tp = alloc_percpu_gfp(struct net_device_core_stats,\n\t\t\t     GFP_ATOMIC | __GFP_NOWARN);\n\n\tif (p && cmpxchg(&dev->core_stats, NULL, p))\n\t\tfree_percpu(p);\n\n\t \n\treturn READ_ONCE(dev->core_stats);\n}\nEXPORT_SYMBOL(netdev_core_stats_alloc);\n\n \nstruct rtnl_link_stats64 *dev_get_stats(struct net_device *dev,\n\t\t\t\t\tstruct rtnl_link_stats64 *storage)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tconst struct net_device_core_stats __percpu *p;\n\n\tif (ops->ndo_get_stats64) {\n\t\tmemset(storage, 0, sizeof(*storage));\n\t\tops->ndo_get_stats64(dev, storage);\n\t} else if (ops->ndo_get_stats) {\n\t\tnetdev_stats_to_stats64(storage, ops->ndo_get_stats(dev));\n\t} else {\n\t\tnetdev_stats_to_stats64(storage, &dev->stats);\n\t}\n\n\t \n\tp = READ_ONCE(dev->core_stats);\n\tif (p) {\n\t\tconst struct net_device_core_stats *core_stats;\n\t\tint i;\n\n\t\tfor_each_possible_cpu(i) {\n\t\t\tcore_stats = per_cpu_ptr(p, i);\n\t\t\tstorage->rx_dropped += READ_ONCE(core_stats->rx_dropped);\n\t\t\tstorage->tx_dropped += READ_ONCE(core_stats->tx_dropped);\n\t\t\tstorage->rx_nohandler += READ_ONCE(core_stats->rx_nohandler);\n\t\t\tstorage->rx_otherhost_dropped += READ_ONCE(core_stats->rx_otherhost_dropped);\n\t\t}\n\t}\n\treturn storage;\n}\nEXPORT_SYMBOL(dev_get_stats);\n\n \nvoid dev_fetch_sw_netstats(struct rtnl_link_stats64 *s,\n\t\t\t   const struct pcpu_sw_netstats __percpu *netstats)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tu64 rx_packets, rx_bytes, tx_packets, tx_bytes;\n\t\tconst struct pcpu_sw_netstats *stats;\n\t\tunsigned int start;\n\n\t\tstats = per_cpu_ptr(netstats, cpu);\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin(&stats->syncp);\n\t\t\trx_packets = u64_stats_read(&stats->rx_packets);\n\t\t\trx_bytes   = u64_stats_read(&stats->rx_bytes);\n\t\t\ttx_packets = u64_stats_read(&stats->tx_packets);\n\t\t\ttx_bytes   = u64_stats_read(&stats->tx_bytes);\n\t\t} while (u64_stats_fetch_retry(&stats->syncp, start));\n\n\t\ts->rx_packets += rx_packets;\n\t\ts->rx_bytes   += rx_bytes;\n\t\ts->tx_packets += tx_packets;\n\t\ts->tx_bytes   += tx_bytes;\n\t}\n}\nEXPORT_SYMBOL_GPL(dev_fetch_sw_netstats);\n\n \nvoid dev_get_tstats64(struct net_device *dev, struct rtnl_link_stats64 *s)\n{\n\tnetdev_stats_to_stats64(s, &dev->stats);\n\tdev_fetch_sw_netstats(s, dev->tstats);\n}\nEXPORT_SYMBOL_GPL(dev_get_tstats64);\n\nstruct netdev_queue *dev_ingress_queue_create(struct net_device *dev)\n{\n\tstruct netdev_queue *queue = dev_ingress_queue(dev);\n\n#ifdef CONFIG_NET_CLS_ACT\n\tif (queue)\n\t\treturn queue;\n\tqueue = kzalloc(sizeof(*queue), GFP_KERNEL);\n\tif (!queue)\n\t\treturn NULL;\n\tnetdev_init_one_queue(dev, queue, NULL);\n\tRCU_INIT_POINTER(queue->qdisc, &noop_qdisc);\n\tRCU_INIT_POINTER(queue->qdisc_sleeping, &noop_qdisc);\n\trcu_assign_pointer(dev->ingress_queue, queue);\n#endif\n\treturn queue;\n}\n\nstatic const struct ethtool_ops default_ethtool_ops;\n\nvoid netdev_set_default_ethtool_ops(struct net_device *dev,\n\t\t\t\t    const struct ethtool_ops *ops)\n{\n\tif (dev->ethtool_ops == &default_ethtool_ops)\n\t\tdev->ethtool_ops = ops;\n}\nEXPORT_SYMBOL_GPL(netdev_set_default_ethtool_ops);\n\n \nvoid netdev_sw_irq_coalesce_default_on(struct net_device *dev)\n{\n\tWARN_ON(dev->reg_state == NETREG_REGISTERED);\n\n\tif (!IS_ENABLED(CONFIG_PREEMPT_RT)) {\n\t\tdev->gro_flush_timeout = 20000;\n\t\tdev->napi_defer_hard_irqs = 1;\n\t}\n}\nEXPORT_SYMBOL_GPL(netdev_sw_irq_coalesce_default_on);\n\nvoid netdev_freemem(struct net_device *dev)\n{\n\tchar *addr = (char *)dev - dev->padded;\n\n\tkvfree(addr);\n}\n\n \nstruct net_device *alloc_netdev_mqs(int sizeof_priv, const char *name,\n\t\tunsigned char name_assign_type,\n\t\tvoid (*setup)(struct net_device *),\n\t\tunsigned int txqs, unsigned int rxqs)\n{\n\tstruct net_device *dev;\n\tunsigned int alloc_size;\n\tstruct net_device *p;\n\n\tBUG_ON(strlen(name) >= sizeof(dev->name));\n\n\tif (txqs < 1) {\n\t\tpr_err(\"alloc_netdev: Unable to allocate device with zero queues\\n\");\n\t\treturn NULL;\n\t}\n\n\tif (rxqs < 1) {\n\t\tpr_err(\"alloc_netdev: Unable to allocate device with zero RX queues\\n\");\n\t\treturn NULL;\n\t}\n\n\talloc_size = sizeof(struct net_device);\n\tif (sizeof_priv) {\n\t\t \n\t\talloc_size = ALIGN(alloc_size, NETDEV_ALIGN);\n\t\talloc_size += sizeof_priv;\n\t}\n\t \n\talloc_size += NETDEV_ALIGN - 1;\n\n\tp = kvzalloc(alloc_size, GFP_KERNEL_ACCOUNT | __GFP_RETRY_MAYFAIL);\n\tif (!p)\n\t\treturn NULL;\n\n\tdev = PTR_ALIGN(p, NETDEV_ALIGN);\n\tdev->padded = (char *)dev - (char *)p;\n\n\tref_tracker_dir_init(&dev->refcnt_tracker, 128, name);\n#ifdef CONFIG_PCPU_DEV_REFCNT\n\tdev->pcpu_refcnt = alloc_percpu(int);\n\tif (!dev->pcpu_refcnt)\n\t\tgoto free_dev;\n\t__dev_hold(dev);\n#else\n\trefcount_set(&dev->dev_refcnt, 1);\n#endif\n\n\tif (dev_addr_init(dev))\n\t\tgoto free_pcpu;\n\n\tdev_mc_init(dev);\n\tdev_uc_init(dev);\n\n\tdev_net_set(dev, &init_net);\n\n\tdev->gso_max_size = GSO_LEGACY_MAX_SIZE;\n\tdev->xdp_zc_max_segs = 1;\n\tdev->gso_max_segs = GSO_MAX_SEGS;\n\tdev->gro_max_size = GRO_LEGACY_MAX_SIZE;\n\tdev->gso_ipv4_max_size = GSO_LEGACY_MAX_SIZE;\n\tdev->gro_ipv4_max_size = GRO_LEGACY_MAX_SIZE;\n\tdev->tso_max_size = TSO_LEGACY_MAX_SIZE;\n\tdev->tso_max_segs = TSO_MAX_SEGS;\n\tdev->upper_level = 1;\n\tdev->lower_level = 1;\n#ifdef CONFIG_LOCKDEP\n\tdev->nested_level = 0;\n\tINIT_LIST_HEAD(&dev->unlink_list);\n#endif\n\n\tINIT_LIST_HEAD(&dev->napi_list);\n\tINIT_LIST_HEAD(&dev->unreg_list);\n\tINIT_LIST_HEAD(&dev->close_list);\n\tINIT_LIST_HEAD(&dev->link_watch_list);\n\tINIT_LIST_HEAD(&dev->adj_list.upper);\n\tINIT_LIST_HEAD(&dev->adj_list.lower);\n\tINIT_LIST_HEAD(&dev->ptype_all);\n\tINIT_LIST_HEAD(&dev->ptype_specific);\n\tINIT_LIST_HEAD(&dev->net_notifier_list);\n#ifdef CONFIG_NET_SCHED\n\thash_init(dev->qdisc_hash);\n#endif\n\tdev->priv_flags = IFF_XMIT_DST_RELEASE | IFF_XMIT_DST_RELEASE_PERM;\n\tsetup(dev);\n\n\tif (!dev->tx_queue_len) {\n\t\tdev->priv_flags |= IFF_NO_QUEUE;\n\t\tdev->tx_queue_len = DEFAULT_TX_QUEUE_LEN;\n\t}\n\n\tdev->num_tx_queues = txqs;\n\tdev->real_num_tx_queues = txqs;\n\tif (netif_alloc_netdev_queues(dev))\n\t\tgoto free_all;\n\n\tdev->num_rx_queues = rxqs;\n\tdev->real_num_rx_queues = rxqs;\n\tif (netif_alloc_rx_queues(dev))\n\t\tgoto free_all;\n\n\tstrcpy(dev->name, name);\n\tdev->name_assign_type = name_assign_type;\n\tdev->group = INIT_NETDEV_GROUP;\n\tif (!dev->ethtool_ops)\n\t\tdev->ethtool_ops = &default_ethtool_ops;\n\n\tnf_hook_netdev_init(dev);\n\n\treturn dev;\n\nfree_all:\n\tfree_netdev(dev);\n\treturn NULL;\n\nfree_pcpu:\n#ifdef CONFIG_PCPU_DEV_REFCNT\n\tfree_percpu(dev->pcpu_refcnt);\nfree_dev:\n#endif\n\tnetdev_freemem(dev);\n\treturn NULL;\n}\nEXPORT_SYMBOL(alloc_netdev_mqs);\n\n \nvoid free_netdev(struct net_device *dev)\n{\n\tstruct napi_struct *p, *n;\n\n\tmight_sleep();\n\n\t \n\tif (dev->reg_state == NETREG_UNREGISTERING) {\n\t\tASSERT_RTNL();\n\t\tdev->needs_free_netdev = true;\n\t\treturn;\n\t}\n\n\tnetif_free_tx_queues(dev);\n\tnetif_free_rx_queues(dev);\n\n\tkfree(rcu_dereference_protected(dev->ingress_queue, 1));\n\n\t \n\tdev_addr_flush(dev);\n\n\tlist_for_each_entry_safe(p, n, &dev->napi_list, dev_list)\n\t\tnetif_napi_del(p);\n\n\tref_tracker_dir_exit(&dev->refcnt_tracker);\n#ifdef CONFIG_PCPU_DEV_REFCNT\n\tfree_percpu(dev->pcpu_refcnt);\n\tdev->pcpu_refcnt = NULL;\n#endif\n\tfree_percpu(dev->core_stats);\n\tdev->core_stats = NULL;\n\tfree_percpu(dev->xdp_bulkq);\n\tdev->xdp_bulkq = NULL;\n\n\t \n\tif (dev->reg_state == NETREG_UNINITIALIZED) {\n\t\tnetdev_freemem(dev);\n\t\treturn;\n\t}\n\n\tBUG_ON(dev->reg_state != NETREG_UNREGISTERED);\n\tdev->reg_state = NETREG_RELEASED;\n\n\t \n\tput_device(&dev->dev);\n}\nEXPORT_SYMBOL(free_netdev);\n\n \nvoid synchronize_net(void)\n{\n\tmight_sleep();\n\tif (rtnl_is_locked())\n\t\tsynchronize_rcu_expedited();\n\telse\n\t\tsynchronize_rcu();\n}\nEXPORT_SYMBOL(synchronize_net);\n\n \n\nvoid unregister_netdevice_queue(struct net_device *dev, struct list_head *head)\n{\n\tASSERT_RTNL();\n\n\tif (head) {\n\t\tlist_move_tail(&dev->unreg_list, head);\n\t} else {\n\t\tLIST_HEAD(single);\n\n\t\tlist_add(&dev->unreg_list, &single);\n\t\tunregister_netdevice_many(&single);\n\t}\n}\nEXPORT_SYMBOL(unregister_netdevice_queue);\n\nvoid unregister_netdevice_many_notify(struct list_head *head,\n\t\t\t\t      u32 portid, const struct nlmsghdr *nlh)\n{\n\tstruct net_device *dev, *tmp;\n\tLIST_HEAD(close_head);\n\n\tBUG_ON(dev_boot_phase);\n\tASSERT_RTNL();\n\n\tif (list_empty(head))\n\t\treturn;\n\n\tlist_for_each_entry_safe(dev, tmp, head, unreg_list) {\n\t\t \n\t\tif (dev->reg_state == NETREG_UNINITIALIZED) {\n\t\t\tpr_debug(\"unregister_netdevice: device %s/%p never was registered\\n\",\n\t\t\t\t dev->name, dev);\n\n\t\t\tWARN_ON(1);\n\t\t\tlist_del(&dev->unreg_list);\n\t\t\tcontinue;\n\t\t}\n\t\tdev->dismantle = true;\n\t\tBUG_ON(dev->reg_state != NETREG_REGISTERED);\n\t}\n\n\t \n\tlist_for_each_entry(dev, head, unreg_list)\n\t\tlist_add_tail(&dev->close_list, &close_head);\n\tdev_close_many(&close_head, true);\n\n\tlist_for_each_entry(dev, head, unreg_list) {\n\t\t \n\t\twrite_lock(&dev_base_lock);\n\t\tunlist_netdevice(dev, false);\n\t\tdev->reg_state = NETREG_UNREGISTERING;\n\t\twrite_unlock(&dev_base_lock);\n\t}\n\tflush_all_backlogs();\n\n\tsynchronize_net();\n\n\tlist_for_each_entry(dev, head, unreg_list) {\n\t\tstruct sk_buff *skb = NULL;\n\n\t\t \n\t\tdev_shutdown(dev);\n\t\tdev_tcx_uninstall(dev);\n\t\tdev_xdp_uninstall(dev);\n\t\tbpf_dev_bound_netdev_unregister(dev);\n\n\t\tnetdev_offload_xstats_disable_all(dev);\n\n\t\t \n\t\tcall_netdevice_notifiers(NETDEV_UNREGISTER, dev);\n\n\t\tif (!dev->rtnl_link_ops ||\n\t\t    dev->rtnl_link_state == RTNL_LINK_INITIALIZED)\n\t\t\tskb = rtmsg_ifinfo_build_skb(RTM_DELLINK, dev, ~0U, 0,\n\t\t\t\t\t\t     GFP_KERNEL, NULL, 0,\n\t\t\t\t\t\t     portid, nlh);\n\n\t\t \n\t\tdev_uc_flush(dev);\n\t\tdev_mc_flush(dev);\n\n\t\tnetdev_name_node_alt_flush(dev);\n\t\tnetdev_name_node_free(dev->name_node);\n\n\t\tcall_netdevice_notifiers(NETDEV_PRE_UNINIT, dev);\n\n\t\tif (dev->netdev_ops->ndo_uninit)\n\t\t\tdev->netdev_ops->ndo_uninit(dev);\n\n\t\tif (skb)\n\t\t\trtmsg_ifinfo_send(skb, dev, GFP_KERNEL, portid, nlh);\n\n\t\t \n\t\tWARN_ON(netdev_has_any_upper_dev(dev));\n\t\tWARN_ON(netdev_has_any_lower_dev(dev));\n\n\t\t \n\t\tnetdev_unregister_kobject(dev);\n#ifdef CONFIG_XPS\n\t\t \n\t\tnetif_reset_xps_queues_gt(dev, 0);\n#endif\n\t}\n\n\tsynchronize_net();\n\n\tlist_for_each_entry(dev, head, unreg_list) {\n\t\tnetdev_put(dev, &dev->dev_registered_tracker);\n\t\tnet_set_todo(dev);\n\t}\n\n\tlist_del(head);\n}\n\n \nvoid unregister_netdevice_many(struct list_head *head)\n{\n\tunregister_netdevice_many_notify(head, 0, NULL);\n}\nEXPORT_SYMBOL(unregister_netdevice_many);\n\n \nvoid unregister_netdev(struct net_device *dev)\n{\n\trtnl_lock();\n\tunregister_netdevice(dev);\n\trtnl_unlock();\n}\nEXPORT_SYMBOL(unregister_netdev);\n\n \n\nint __dev_change_net_namespace(struct net_device *dev, struct net *net,\n\t\t\t       const char *pat, int new_ifindex)\n{\n\tstruct netdev_name_node *name_node;\n\tstruct net *net_old = dev_net(dev);\n\tchar new_name[IFNAMSIZ] = {};\n\tint err, new_nsid;\n\n\tASSERT_RTNL();\n\n\t \n\terr = -EINVAL;\n\tif (dev->features & NETIF_F_NETNS_LOCAL)\n\t\tgoto out;\n\n\t \n\tif (dev->reg_state != NETREG_REGISTERED)\n\t\tgoto out;\n\n\t \n\terr = 0;\n\tif (net_eq(net_old, net))\n\t\tgoto out;\n\n\t \n\terr = -EEXIST;\n\tif (netdev_name_in_use(net, dev->name)) {\n\t\t \n\t\tif (!pat)\n\t\t\tgoto out;\n\t\terr = dev_prep_valid_name(net, dev, pat, new_name);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\t \n\terr = -EEXIST;\n\tnetdev_for_each_altname(dev, name_node)\n\t\tif (netdev_name_in_use(net, name_node->name))\n\t\t\tgoto out;\n\n\t \n\tif (new_ifindex) {\n\t\terr = dev_index_reserve(net, new_ifindex);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t} else {\n\t\t \n\t\terr = dev_index_reserve(net, dev->ifindex);\n\t\tif (err == -EBUSY)\n\t\t\terr = dev_index_reserve(net, 0);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t\tnew_ifindex = err;\n\t}\n\n\t \n\n\t \n\tdev_close(dev);\n\n\t \n\tunlist_netdevice(dev, true);\n\n\tsynchronize_net();\n\n\t \n\tdev_shutdown(dev);\n\n\t \n\tcall_netdevice_notifiers(NETDEV_UNREGISTER, dev);\n\trcu_barrier();\n\n\tnew_nsid = peernet2id_alloc(dev_net(dev), net, GFP_KERNEL);\n\n\trtmsg_ifinfo_newnet(RTM_DELLINK, dev, ~0U, GFP_KERNEL, &new_nsid,\n\t\t\t    new_ifindex);\n\n\t \n\tdev_uc_flush(dev);\n\tdev_mc_flush(dev);\n\n\t \n\tkobject_uevent(&dev->dev.kobj, KOBJ_REMOVE);\n\tnetdev_adjacent_del_links(dev);\n\n\t \n\tmove_netdevice_notifiers_dev_net(dev, net);\n\n\t \n\tdev_net_set(dev, net);\n\tdev->ifindex = new_ifindex;\n\n\t \n\tkobject_uevent(&dev->dev.kobj, KOBJ_ADD);\n\tnetdev_adjacent_add_links(dev);\n\n\tif (new_name[0])  \n\t\tstrscpy(dev->name, new_name, IFNAMSIZ);\n\n\t \n\terr = device_rename(&dev->dev, dev->name);\n\tWARN_ON(err);\n\n\t \n\terr = netdev_change_owner(dev, net_old, net);\n\tWARN_ON(err);\n\n\t \n\tlist_netdevice(dev);\n\n\t \n\tcall_netdevice_notifiers(NETDEV_REGISTER, dev);\n\n\t \n\trtmsg_ifinfo(RTM_NEWLINK, dev, ~0U, GFP_KERNEL, 0, NULL);\n\n\tsynchronize_net();\n\terr = 0;\nout:\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(__dev_change_net_namespace);\n\nstatic int dev_cpu_dead(unsigned int oldcpu)\n{\n\tstruct sk_buff **list_skb;\n\tstruct sk_buff *skb;\n\tunsigned int cpu;\n\tstruct softnet_data *sd, *oldsd, *remsd = NULL;\n\n\tlocal_irq_disable();\n\tcpu = smp_processor_id();\n\tsd = &per_cpu(softnet_data, cpu);\n\toldsd = &per_cpu(softnet_data, oldcpu);\n\n\t \n\tlist_skb = &sd->completion_queue;\n\twhile (*list_skb)\n\t\tlist_skb = &(*list_skb)->next;\n\t \n\t*list_skb = oldsd->completion_queue;\n\toldsd->completion_queue = NULL;\n\n\t \n\tif (oldsd->output_queue) {\n\t\t*sd->output_queue_tailp = oldsd->output_queue;\n\t\tsd->output_queue_tailp = oldsd->output_queue_tailp;\n\t\toldsd->output_queue = NULL;\n\t\toldsd->output_queue_tailp = &oldsd->output_queue;\n\t}\n\t \n\twhile (!list_empty(&oldsd->poll_list)) {\n\t\tstruct napi_struct *napi = list_first_entry(&oldsd->poll_list,\n\t\t\t\t\t\t\t    struct napi_struct,\n\t\t\t\t\t\t\t    poll_list);\n\n\t\tlist_del_init(&napi->poll_list);\n\t\tif (napi->poll == process_backlog)\n\t\t\tnapi->state = 0;\n\t\telse\n\t\t\t____napi_schedule(sd, napi);\n\t}\n\n\traise_softirq_irqoff(NET_TX_SOFTIRQ);\n\tlocal_irq_enable();\n\n#ifdef CONFIG_RPS\n\tremsd = oldsd->rps_ipi_list;\n\toldsd->rps_ipi_list = NULL;\n#endif\n\t \n\tnet_rps_send_ipi(remsd);\n\n\t \n\twhile ((skb = __skb_dequeue(&oldsd->process_queue))) {\n\t\tnetif_rx(skb);\n\t\tinput_queue_head_incr(oldsd);\n\t}\n\twhile ((skb = skb_dequeue(&oldsd->input_pkt_queue))) {\n\t\tnetif_rx(skb);\n\t\tinput_queue_head_incr(oldsd);\n\t}\n\n\treturn 0;\n}\n\n \nnetdev_features_t netdev_increment_features(netdev_features_t all,\n\tnetdev_features_t one, netdev_features_t mask)\n{\n\tif (mask & NETIF_F_HW_CSUM)\n\t\tmask |= NETIF_F_CSUM_MASK;\n\tmask |= NETIF_F_VLAN_CHALLENGED;\n\n\tall |= one & (NETIF_F_ONE_FOR_ALL | NETIF_F_CSUM_MASK) & mask;\n\tall &= one | ~NETIF_F_ALL_FOR_ALL;\n\n\t \n\tif (all & NETIF_F_HW_CSUM)\n\t\tall &= ~(NETIF_F_CSUM_MASK & ~NETIF_F_HW_CSUM);\n\n\treturn all;\n}\nEXPORT_SYMBOL(netdev_increment_features);\n\nstatic struct hlist_head * __net_init netdev_create_hash(void)\n{\n\tint i;\n\tstruct hlist_head *hash;\n\n\thash = kmalloc_array(NETDEV_HASHENTRIES, sizeof(*hash), GFP_KERNEL);\n\tif (hash != NULL)\n\t\tfor (i = 0; i < NETDEV_HASHENTRIES; i++)\n\t\t\tINIT_HLIST_HEAD(&hash[i]);\n\n\treturn hash;\n}\n\n \nstatic int __net_init netdev_init(struct net *net)\n{\n\tBUILD_BUG_ON(GRO_HASH_BUCKETS >\n\t\t     8 * sizeof_field(struct napi_struct, gro_bitmask));\n\n\tINIT_LIST_HEAD(&net->dev_base_head);\n\n\tnet->dev_name_head = netdev_create_hash();\n\tif (net->dev_name_head == NULL)\n\t\tgoto err_name;\n\n\tnet->dev_index_head = netdev_create_hash();\n\tif (net->dev_index_head == NULL)\n\t\tgoto err_idx;\n\n\txa_init_flags(&net->dev_by_index, XA_FLAGS_ALLOC1);\n\n\tRAW_INIT_NOTIFIER_HEAD(&net->netdev_chain);\n\n\treturn 0;\n\nerr_idx:\n\tkfree(net->dev_name_head);\nerr_name:\n\treturn -ENOMEM;\n}\n\n \nconst char *netdev_drivername(const struct net_device *dev)\n{\n\tconst struct device_driver *driver;\n\tconst struct device *parent;\n\tconst char *empty = \"\";\n\n\tparent = dev->dev.parent;\n\tif (!parent)\n\t\treturn empty;\n\n\tdriver = parent->driver;\n\tif (driver && driver->name)\n\t\treturn driver->name;\n\treturn empty;\n}\n\nstatic void __netdev_printk(const char *level, const struct net_device *dev,\n\t\t\t    struct va_format *vaf)\n{\n\tif (dev && dev->dev.parent) {\n\t\tdev_printk_emit(level[1] - '0',\n\t\t\t\tdev->dev.parent,\n\t\t\t\t\"%s %s %s%s: %pV\",\n\t\t\t\tdev_driver_string(dev->dev.parent),\n\t\t\t\tdev_name(dev->dev.parent),\n\t\t\t\tnetdev_name(dev), netdev_reg_state(dev),\n\t\t\t\tvaf);\n\t} else if (dev) {\n\t\tprintk(\"%s%s%s: %pV\",\n\t\t       level, netdev_name(dev), netdev_reg_state(dev), vaf);\n\t} else {\n\t\tprintk(\"%s(NULL net_device): %pV\", level, vaf);\n\t}\n}\n\nvoid netdev_printk(const char *level, const struct net_device *dev,\n\t\t   const char *format, ...)\n{\n\tstruct va_format vaf;\n\tva_list args;\n\n\tva_start(args, format);\n\n\tvaf.fmt = format;\n\tvaf.va = &args;\n\n\t__netdev_printk(level, dev, &vaf);\n\n\tva_end(args);\n}\nEXPORT_SYMBOL(netdev_printk);\n\n#define define_netdev_printk_level(func, level)\t\t\t\\\nvoid func(const struct net_device *dev, const char *fmt, ...)\t\\\n{\t\t\t\t\t\t\t\t\\\n\tstruct va_format vaf;\t\t\t\t\t\\\n\tva_list args;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tva_start(args, fmt);\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tvaf.fmt = fmt;\t\t\t\t\t\t\\\n\tvaf.va = &args;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\t__netdev_printk(level, dev, &vaf);\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tva_end(args);\t\t\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\\\nEXPORT_SYMBOL(func);\n\ndefine_netdev_printk_level(netdev_emerg, KERN_EMERG);\ndefine_netdev_printk_level(netdev_alert, KERN_ALERT);\ndefine_netdev_printk_level(netdev_crit, KERN_CRIT);\ndefine_netdev_printk_level(netdev_err, KERN_ERR);\ndefine_netdev_printk_level(netdev_warn, KERN_WARNING);\ndefine_netdev_printk_level(netdev_notice, KERN_NOTICE);\ndefine_netdev_printk_level(netdev_info, KERN_INFO);\n\nstatic void __net_exit netdev_exit(struct net *net)\n{\n\tkfree(net->dev_name_head);\n\tkfree(net->dev_index_head);\n\txa_destroy(&net->dev_by_index);\n\tif (net != &init_net)\n\t\tWARN_ON_ONCE(!list_empty(&net->dev_base_head));\n}\n\nstatic struct pernet_operations __net_initdata netdev_net_ops = {\n\t.init = netdev_init,\n\t.exit = netdev_exit,\n};\n\nstatic void __net_exit default_device_exit_net(struct net *net)\n{\n\tstruct net_device *dev, *aux;\n\t \n\tASSERT_RTNL();\n\tfor_each_netdev_safe(net, dev, aux) {\n\t\tint err;\n\t\tchar fb_name[IFNAMSIZ];\n\n\t\t \n\t\tif (dev->features & NETIF_F_NETNS_LOCAL)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (dev->rtnl_link_ops && !dev->rtnl_link_ops->netns_refund)\n\t\t\tcontinue;\n\n\t\t \n\t\tsnprintf(fb_name, IFNAMSIZ, \"dev%d\", dev->ifindex);\n\t\tif (netdev_name_in_use(&init_net, fb_name))\n\t\t\tsnprintf(fb_name, IFNAMSIZ, \"dev%%d\");\n\t\terr = dev_change_net_namespace(dev, &init_net, fb_name);\n\t\tif (err) {\n\t\t\tpr_emerg(\"%s: failed to move %s to init_net: %d\\n\",\n\t\t\t\t __func__, dev->name, err);\n\t\t\tBUG();\n\t\t}\n\t}\n}\n\nstatic void __net_exit default_device_exit_batch(struct list_head *net_list)\n{\n\t \n\tstruct net_device *dev;\n\tstruct net *net;\n\tLIST_HEAD(dev_kill_list);\n\n\trtnl_lock();\n\tlist_for_each_entry(net, net_list, exit_list) {\n\t\tdefault_device_exit_net(net);\n\t\tcond_resched();\n\t}\n\n\tlist_for_each_entry(net, net_list, exit_list) {\n\t\tfor_each_netdev_reverse(net, dev) {\n\t\t\tif (dev->rtnl_link_ops && dev->rtnl_link_ops->dellink)\n\t\t\t\tdev->rtnl_link_ops->dellink(dev, &dev_kill_list);\n\t\t\telse\n\t\t\t\tunregister_netdevice_queue(dev, &dev_kill_list);\n\t\t}\n\t}\n\tunregister_netdevice_many(&dev_kill_list);\n\trtnl_unlock();\n}\n\nstatic struct pernet_operations __net_initdata default_device_ops = {\n\t.exit_batch = default_device_exit_batch,\n};\n\n \n\n \nstatic int __init net_dev_init(void)\n{\n\tint i, rc = -ENOMEM;\n\n\tBUG_ON(!dev_boot_phase);\n\n\tif (dev_proc_init())\n\t\tgoto out;\n\n\tif (netdev_kobject_init())\n\t\tgoto out;\n\n\tINIT_LIST_HEAD(&ptype_all);\n\tfor (i = 0; i < PTYPE_HASH_SIZE; i++)\n\t\tINIT_LIST_HEAD(&ptype_base[i]);\n\n\tif (register_pernet_subsys(&netdev_net_ops))\n\t\tgoto out;\n\n\t \n\n\tfor_each_possible_cpu(i) {\n\t\tstruct work_struct *flush = per_cpu_ptr(&flush_works, i);\n\t\tstruct softnet_data *sd = &per_cpu(softnet_data, i);\n\n\t\tINIT_WORK(flush, flush_backlog);\n\n\t\tskb_queue_head_init(&sd->input_pkt_queue);\n\t\tskb_queue_head_init(&sd->process_queue);\n#ifdef CONFIG_XFRM_OFFLOAD\n\t\tskb_queue_head_init(&sd->xfrm_backlog);\n#endif\n\t\tINIT_LIST_HEAD(&sd->poll_list);\n\t\tsd->output_queue_tailp = &sd->output_queue;\n#ifdef CONFIG_RPS\n\t\tINIT_CSD(&sd->csd, rps_trigger_softirq, sd);\n\t\tsd->cpu = i;\n#endif\n\t\tINIT_CSD(&sd->defer_csd, trigger_rx_softirq, sd);\n\t\tspin_lock_init(&sd->defer_lock);\n\n\t\tinit_gro_hash(&sd->backlog);\n\t\tsd->backlog.poll = process_backlog;\n\t\tsd->backlog.weight = weight_p;\n\t}\n\n\tdev_boot_phase = 0;\n\n\t \n\tif (register_pernet_device(&loopback_net_ops))\n\t\tgoto out;\n\n\tif (register_pernet_device(&default_device_ops))\n\t\tgoto out;\n\n\topen_softirq(NET_TX_SOFTIRQ, net_tx_action);\n\topen_softirq(NET_RX_SOFTIRQ, net_rx_action);\n\n\trc = cpuhp_setup_state_nocalls(CPUHP_NET_DEV_DEAD, \"net/dev:dead\",\n\t\t\t\t       NULL, dev_cpu_dead);\n\tWARN_ON(rc < 0);\n\trc = 0;\nout:\n\treturn rc;\n}\n\nsubsys_initcall(net_dev_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}