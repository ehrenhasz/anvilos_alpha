{
  "module_name": "filter.c",
  "hash_id": "67ff14ad0dc111b7fd0a3c436b29845d56aae4271a918b5f2c6d8799823bc8ea",
  "original_prompt": "Ingested from linux-6.6.14/net/core/filter.c",
  "human_readable_source": "\n \n\n#include <linux/atomic.h>\n#include <linux/bpf_verifier.h>\n#include <linux/module.h>\n#include <linux/types.h>\n#include <linux/mm.h>\n#include <linux/fcntl.h>\n#include <linux/socket.h>\n#include <linux/sock_diag.h>\n#include <linux/in.h>\n#include <linux/inet.h>\n#include <linux/netdevice.h>\n#include <linux/if_packet.h>\n#include <linux/if_arp.h>\n#include <linux/gfp.h>\n#include <net/inet_common.h>\n#include <net/ip.h>\n#include <net/protocol.h>\n#include <net/netlink.h>\n#include <linux/skbuff.h>\n#include <linux/skmsg.h>\n#include <net/sock.h>\n#include <net/flow_dissector.h>\n#include <linux/errno.h>\n#include <linux/timer.h>\n#include <linux/uaccess.h>\n#include <asm/unaligned.h>\n#include <linux/filter.h>\n#include <linux/ratelimit.h>\n#include <linux/seccomp.h>\n#include <linux/if_vlan.h>\n#include <linux/bpf.h>\n#include <linux/btf.h>\n#include <net/sch_generic.h>\n#include <net/cls_cgroup.h>\n#include <net/dst_metadata.h>\n#include <net/dst.h>\n#include <net/sock_reuseport.h>\n#include <net/busy_poll.h>\n#include <net/tcp.h>\n#include <net/xfrm.h>\n#include <net/udp.h>\n#include <linux/bpf_trace.h>\n#include <net/xdp_sock.h>\n#include <linux/inetdevice.h>\n#include <net/inet_hashtables.h>\n#include <net/inet6_hashtables.h>\n#include <net/ip_fib.h>\n#include <net/nexthop.h>\n#include <net/flow.h>\n#include <net/arp.h>\n#include <net/ipv6.h>\n#include <net/net_namespace.h>\n#include <linux/seg6_local.h>\n#include <net/seg6.h>\n#include <net/seg6_local.h>\n#include <net/lwtunnel.h>\n#include <net/ipv6_stubs.h>\n#include <net/bpf_sk_storage.h>\n#include <net/transp_v6.h>\n#include <linux/btf_ids.h>\n#include <net/tls.h>\n#include <net/xdp.h>\n#include <net/mptcp.h>\n#include <net/netfilter/nf_conntrack_bpf.h>\n\nstatic const struct bpf_func_proto *\nbpf_sk_base_func_proto(enum bpf_func_id func_id);\n\nint copy_bpf_fprog_from_user(struct sock_fprog *dst, sockptr_t src, int len)\n{\n\tif (in_compat_syscall()) {\n\t\tstruct compat_sock_fprog f32;\n\n\t\tif (len != sizeof(f32))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_sockptr(&f32, src, sizeof(f32)))\n\t\t\treturn -EFAULT;\n\t\tmemset(dst, 0, sizeof(*dst));\n\t\tdst->len = f32.len;\n\t\tdst->filter = compat_ptr(f32.filter);\n\t} else {\n\t\tif (len != sizeof(*dst))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_sockptr(dst, src, sizeof(*dst)))\n\t\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(copy_bpf_fprog_from_user);\n\n \nint sk_filter_trim_cap(struct sock *sk, struct sk_buff *skb, unsigned int cap)\n{\n\tint err;\n\tstruct sk_filter *filter;\n\n\t \n\tif (skb_pfmemalloc(skb) && !sock_flag(sk, SOCK_MEMALLOC)) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_PFMEMALLOCDROP);\n\t\treturn -ENOMEM;\n\t}\n\terr = BPF_CGROUP_RUN_PROG_INET_INGRESS(sk, skb);\n\tif (err)\n\t\treturn err;\n\n\terr = security_sock_rcv_skb(sk, skb);\n\tif (err)\n\t\treturn err;\n\n\trcu_read_lock();\n\tfilter = rcu_dereference(sk->sk_filter);\n\tif (filter) {\n\t\tstruct sock *save_sk = skb->sk;\n\t\tunsigned int pkt_len;\n\n\t\tskb->sk = sk;\n\t\tpkt_len = bpf_prog_run_save_cb(filter->prog, skb);\n\t\tskb->sk = save_sk;\n\t\terr = pkt_len ? pskb_trim(skb, max(cap, pkt_len)) : -EPERM;\n\t}\n\trcu_read_unlock();\n\n\treturn err;\n}\nEXPORT_SYMBOL(sk_filter_trim_cap);\n\nBPF_CALL_1(bpf_skb_get_pay_offset, struct sk_buff *, skb)\n{\n\treturn skb_get_poff(skb);\n}\n\nBPF_CALL_3(bpf_skb_get_nlattr, struct sk_buff *, skb, u32, a, u32, x)\n{\n\tstruct nlattr *nla;\n\n\tif (skb_is_nonlinear(skb))\n\t\treturn 0;\n\n\tif (skb->len < sizeof(struct nlattr))\n\t\treturn 0;\n\n\tif (a > skb->len - sizeof(struct nlattr))\n\t\treturn 0;\n\n\tnla = nla_find((struct nlattr *) &skb->data[a], skb->len - a, x);\n\tif (nla)\n\t\treturn (void *) nla - (void *) skb->data;\n\n\treturn 0;\n}\n\nBPF_CALL_3(bpf_skb_get_nlattr_nest, struct sk_buff *, skb, u32, a, u32, x)\n{\n\tstruct nlattr *nla;\n\n\tif (skb_is_nonlinear(skb))\n\t\treturn 0;\n\n\tif (skb->len < sizeof(struct nlattr))\n\t\treturn 0;\n\n\tif (a > skb->len - sizeof(struct nlattr))\n\t\treturn 0;\n\n\tnla = (struct nlattr *) &skb->data[a];\n\tif (nla->nla_len > skb->len - a)\n\t\treturn 0;\n\n\tnla = nla_find_nested(nla, x);\n\tif (nla)\n\t\treturn (void *) nla - (void *) skb->data;\n\n\treturn 0;\n}\n\nBPF_CALL_4(bpf_skb_load_helper_8, const struct sk_buff *, skb, const void *,\n\t   data, int, headlen, int, offset)\n{\n\tu8 tmp, *ptr;\n\tconst int len = sizeof(tmp);\n\n\tif (offset >= 0) {\n\t\tif (headlen - offset >= len)\n\t\t\treturn *(u8 *)(data + offset);\n\t\tif (!skb_copy_bits(skb, offset, &tmp, sizeof(tmp)))\n\t\t\treturn tmp;\n\t} else {\n\t\tptr = bpf_internal_load_pointer_neg_helper(skb, offset, len);\n\t\tif (likely(ptr))\n\t\t\treturn *(u8 *)ptr;\n\t}\n\n\treturn -EFAULT;\n}\n\nBPF_CALL_2(bpf_skb_load_helper_8_no_cache, const struct sk_buff *, skb,\n\t   int, offset)\n{\n\treturn ____bpf_skb_load_helper_8(skb, skb->data, skb->len - skb->data_len,\n\t\t\t\t\t offset);\n}\n\nBPF_CALL_4(bpf_skb_load_helper_16, const struct sk_buff *, skb, const void *,\n\t   data, int, headlen, int, offset)\n{\n\t__be16 tmp, *ptr;\n\tconst int len = sizeof(tmp);\n\n\tif (offset >= 0) {\n\t\tif (headlen - offset >= len)\n\t\t\treturn get_unaligned_be16(data + offset);\n\t\tif (!skb_copy_bits(skb, offset, &tmp, sizeof(tmp)))\n\t\t\treturn be16_to_cpu(tmp);\n\t} else {\n\t\tptr = bpf_internal_load_pointer_neg_helper(skb, offset, len);\n\t\tif (likely(ptr))\n\t\t\treturn get_unaligned_be16(ptr);\n\t}\n\n\treturn -EFAULT;\n}\n\nBPF_CALL_2(bpf_skb_load_helper_16_no_cache, const struct sk_buff *, skb,\n\t   int, offset)\n{\n\treturn ____bpf_skb_load_helper_16(skb, skb->data, skb->len - skb->data_len,\n\t\t\t\t\t  offset);\n}\n\nBPF_CALL_4(bpf_skb_load_helper_32, const struct sk_buff *, skb, const void *,\n\t   data, int, headlen, int, offset)\n{\n\t__be32 tmp, *ptr;\n\tconst int len = sizeof(tmp);\n\n\tif (likely(offset >= 0)) {\n\t\tif (headlen - offset >= len)\n\t\t\treturn get_unaligned_be32(data + offset);\n\t\tif (!skb_copy_bits(skb, offset, &tmp, sizeof(tmp)))\n\t\t\treturn be32_to_cpu(tmp);\n\t} else {\n\t\tptr = bpf_internal_load_pointer_neg_helper(skb, offset, len);\n\t\tif (likely(ptr))\n\t\t\treturn get_unaligned_be32(ptr);\n\t}\n\n\treturn -EFAULT;\n}\n\nBPF_CALL_2(bpf_skb_load_helper_32_no_cache, const struct sk_buff *, skb,\n\t   int, offset)\n{\n\treturn ____bpf_skb_load_helper_32(skb, skb->data, skb->len - skb->data_len,\n\t\t\t\t\t  offset);\n}\n\nstatic u32 convert_skb_access(int skb_field, int dst_reg, int src_reg,\n\t\t\t      struct bpf_insn *insn_buf)\n{\n\tstruct bpf_insn *insn = insn_buf;\n\n\tswitch (skb_field) {\n\tcase SKF_AD_MARK:\n\t\tBUILD_BUG_ON(sizeof_field(struct sk_buff, mark) != 4);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, dst_reg, src_reg,\n\t\t\t\t      offsetof(struct sk_buff, mark));\n\t\tbreak;\n\n\tcase SKF_AD_PKTTYPE:\n\t\t*insn++ = BPF_LDX_MEM(BPF_B, dst_reg, src_reg, PKT_TYPE_OFFSET);\n\t\t*insn++ = BPF_ALU32_IMM(BPF_AND, dst_reg, PKT_TYPE_MAX);\n#ifdef __BIG_ENDIAN_BITFIELD\n\t\t*insn++ = BPF_ALU32_IMM(BPF_RSH, dst_reg, 5);\n#endif\n\t\tbreak;\n\n\tcase SKF_AD_QUEUE:\n\t\tBUILD_BUG_ON(sizeof_field(struct sk_buff, queue_mapping) != 2);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, dst_reg, src_reg,\n\t\t\t\t      offsetof(struct sk_buff, queue_mapping));\n\t\tbreak;\n\n\tcase SKF_AD_VLAN_TAG:\n\t\tBUILD_BUG_ON(sizeof_field(struct sk_buff, vlan_tci) != 2);\n\n\t\t \n\t\t*insn++ = BPF_LDX_MEM(BPF_H, dst_reg, src_reg,\n\t\t\t\t      offsetof(struct sk_buff, vlan_tci));\n\t\tbreak;\n\tcase SKF_AD_VLAN_TAG_PRESENT:\n\t\tBUILD_BUG_ON(sizeof_field(struct sk_buff, vlan_all) != 4);\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, dst_reg, src_reg,\n\t\t\t\t      offsetof(struct sk_buff, vlan_all));\n\t\t*insn++ = BPF_JMP_IMM(BPF_JEQ, dst_reg, 0, 1);\n\t\t*insn++ = BPF_ALU32_IMM(BPF_MOV, dst_reg, 1);\n\t\tbreak;\n\t}\n\n\treturn insn - insn_buf;\n}\n\nstatic bool convert_bpf_extensions(struct sock_filter *fp,\n\t\t\t\t   struct bpf_insn **insnp)\n{\n\tstruct bpf_insn *insn = *insnp;\n\tu32 cnt;\n\n\tswitch (fp->k) {\n\tcase SKF_AD_OFF + SKF_AD_PROTOCOL:\n\t\tBUILD_BUG_ON(sizeof_field(struct sk_buff, protocol) != 2);\n\n\t\t \n\t\t*insn++ = BPF_LDX_MEM(BPF_H, BPF_REG_A, BPF_REG_CTX,\n\t\t\t\t      offsetof(struct sk_buff, protocol));\n\t\t \n\t\t*insn = BPF_ENDIAN(BPF_FROM_BE, BPF_REG_A, 16);\n\t\tbreak;\n\n\tcase SKF_AD_OFF + SKF_AD_PKTTYPE:\n\t\tcnt = convert_skb_access(SKF_AD_PKTTYPE, BPF_REG_A, BPF_REG_CTX, insn);\n\t\tinsn += cnt - 1;\n\t\tbreak;\n\n\tcase SKF_AD_OFF + SKF_AD_IFINDEX:\n\tcase SKF_AD_OFF + SKF_AD_HATYPE:\n\t\tBUILD_BUG_ON(sizeof_field(struct net_device, ifindex) != 4);\n\t\tBUILD_BUG_ON(sizeof_field(struct net_device, type) != 2);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, dev),\n\t\t\t\t      BPF_REG_TMP, BPF_REG_CTX,\n\t\t\t\t      offsetof(struct sk_buff, dev));\n\t\t \n\t\t*insn++ = BPF_JMP_IMM(BPF_JNE, BPF_REG_TMP, 0, 1);\n\t\t*insn++ = BPF_EXIT_INSN();\n\t\tif (fp->k == SKF_AD_OFF + SKF_AD_IFINDEX)\n\t\t\t*insn = BPF_LDX_MEM(BPF_W, BPF_REG_A, BPF_REG_TMP,\n\t\t\t\t\t    offsetof(struct net_device, ifindex));\n\t\telse\n\t\t\t*insn = BPF_LDX_MEM(BPF_H, BPF_REG_A, BPF_REG_TMP,\n\t\t\t\t\t    offsetof(struct net_device, type));\n\t\tbreak;\n\n\tcase SKF_AD_OFF + SKF_AD_MARK:\n\t\tcnt = convert_skb_access(SKF_AD_MARK, BPF_REG_A, BPF_REG_CTX, insn);\n\t\tinsn += cnt - 1;\n\t\tbreak;\n\n\tcase SKF_AD_OFF + SKF_AD_RXHASH:\n\t\tBUILD_BUG_ON(sizeof_field(struct sk_buff, hash) != 4);\n\n\t\t*insn = BPF_LDX_MEM(BPF_W, BPF_REG_A, BPF_REG_CTX,\n\t\t\t\t    offsetof(struct sk_buff, hash));\n\t\tbreak;\n\n\tcase SKF_AD_OFF + SKF_AD_QUEUE:\n\t\tcnt = convert_skb_access(SKF_AD_QUEUE, BPF_REG_A, BPF_REG_CTX, insn);\n\t\tinsn += cnt - 1;\n\t\tbreak;\n\n\tcase SKF_AD_OFF + SKF_AD_VLAN_TAG:\n\t\tcnt = convert_skb_access(SKF_AD_VLAN_TAG,\n\t\t\t\t\t BPF_REG_A, BPF_REG_CTX, insn);\n\t\tinsn += cnt - 1;\n\t\tbreak;\n\n\tcase SKF_AD_OFF + SKF_AD_VLAN_TAG_PRESENT:\n\t\tcnt = convert_skb_access(SKF_AD_VLAN_TAG_PRESENT,\n\t\t\t\t\t BPF_REG_A, BPF_REG_CTX, insn);\n\t\tinsn += cnt - 1;\n\t\tbreak;\n\n\tcase SKF_AD_OFF + SKF_AD_VLAN_TPID:\n\t\tBUILD_BUG_ON(sizeof_field(struct sk_buff, vlan_proto) != 2);\n\n\t\t \n\t\t*insn++ = BPF_LDX_MEM(BPF_H, BPF_REG_A, BPF_REG_CTX,\n\t\t\t\t      offsetof(struct sk_buff, vlan_proto));\n\t\t \n\t\t*insn = BPF_ENDIAN(BPF_FROM_BE, BPF_REG_A, 16);\n\t\tbreak;\n\n\tcase SKF_AD_OFF + SKF_AD_PAY_OFFSET:\n\tcase SKF_AD_OFF + SKF_AD_NLATTR:\n\tcase SKF_AD_OFF + SKF_AD_NLATTR_NEST:\n\tcase SKF_AD_OFF + SKF_AD_CPU:\n\tcase SKF_AD_OFF + SKF_AD_RANDOM:\n\t\t \n\t\t*insn++ = BPF_MOV64_REG(BPF_REG_ARG1, BPF_REG_CTX);\n\t\t \n\t\t*insn++ = BPF_MOV64_REG(BPF_REG_ARG2, BPF_REG_A);\n\t\t \n\t\t*insn++ = BPF_MOV64_REG(BPF_REG_ARG3, BPF_REG_X);\n\t\t \n\t\tswitch (fp->k) {\n\t\tcase SKF_AD_OFF + SKF_AD_PAY_OFFSET:\n\t\t\t*insn = BPF_EMIT_CALL(bpf_skb_get_pay_offset);\n\t\t\tbreak;\n\t\tcase SKF_AD_OFF + SKF_AD_NLATTR:\n\t\t\t*insn = BPF_EMIT_CALL(bpf_skb_get_nlattr);\n\t\t\tbreak;\n\t\tcase SKF_AD_OFF + SKF_AD_NLATTR_NEST:\n\t\t\t*insn = BPF_EMIT_CALL(bpf_skb_get_nlattr_nest);\n\t\t\tbreak;\n\t\tcase SKF_AD_OFF + SKF_AD_CPU:\n\t\t\t*insn = BPF_EMIT_CALL(bpf_get_raw_cpu_id);\n\t\t\tbreak;\n\t\tcase SKF_AD_OFF + SKF_AD_RANDOM:\n\t\t\t*insn = BPF_EMIT_CALL(bpf_user_rnd_u32);\n\t\t\tbpf_user_rnd_init_once();\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase SKF_AD_OFF + SKF_AD_ALU_XOR_X:\n\t\t \n\t\t*insn = BPF_ALU32_REG(BPF_XOR, BPF_REG_A, BPF_REG_X);\n\t\tbreak;\n\n\tdefault:\n\t\t \n\t\tBUG_ON(__bpf_call_base(0, 0, 0, 0, 0) != 0);\n\t\treturn false;\n\t}\n\n\t*insnp = insn;\n\treturn true;\n}\n\nstatic bool convert_bpf_ld_abs(struct sock_filter *fp, struct bpf_insn **insnp)\n{\n\tconst bool unaligned_ok = IS_BUILTIN(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS);\n\tint size = bpf_size_to_bytes(BPF_SIZE(fp->code));\n\tbool endian = BPF_SIZE(fp->code) == BPF_H ||\n\t\t      BPF_SIZE(fp->code) == BPF_W;\n\tbool indirect = BPF_MODE(fp->code) == BPF_IND;\n\tconst int ip_align = NET_IP_ALIGN;\n\tstruct bpf_insn *insn = *insnp;\n\tint offset = fp->k;\n\n\tif (!indirect &&\n\t    ((unaligned_ok && offset >= 0) ||\n\t     (!unaligned_ok && offset >= 0 &&\n\t      offset + ip_align >= 0 &&\n\t      offset + ip_align % size == 0))) {\n\t\tbool ldx_off_ok = offset <= S16_MAX;\n\n\t\t*insn++ = BPF_MOV64_REG(BPF_REG_TMP, BPF_REG_H);\n\t\tif (offset)\n\t\t\t*insn++ = BPF_ALU64_IMM(BPF_SUB, BPF_REG_TMP, offset);\n\t\t*insn++ = BPF_JMP_IMM(BPF_JSLT, BPF_REG_TMP,\n\t\t\t\t      size, 2 + endian + (!ldx_off_ok * 2));\n\t\tif (ldx_off_ok) {\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_SIZE(fp->code), BPF_REG_A,\n\t\t\t\t\t      BPF_REG_D, offset);\n\t\t} else {\n\t\t\t*insn++ = BPF_MOV64_REG(BPF_REG_TMP, BPF_REG_D);\n\t\t\t*insn++ = BPF_ALU64_IMM(BPF_ADD, BPF_REG_TMP, offset);\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_SIZE(fp->code), BPF_REG_A,\n\t\t\t\t\t      BPF_REG_TMP, 0);\n\t\t}\n\t\tif (endian)\n\t\t\t*insn++ = BPF_ENDIAN(BPF_FROM_BE, BPF_REG_A, size * 8);\n\t\t*insn++ = BPF_JMP_A(8);\n\t}\n\n\t*insn++ = BPF_MOV64_REG(BPF_REG_ARG1, BPF_REG_CTX);\n\t*insn++ = BPF_MOV64_REG(BPF_REG_ARG2, BPF_REG_D);\n\t*insn++ = BPF_MOV64_REG(BPF_REG_ARG3, BPF_REG_H);\n\tif (!indirect) {\n\t\t*insn++ = BPF_MOV64_IMM(BPF_REG_ARG4, offset);\n\t} else {\n\t\t*insn++ = BPF_MOV64_REG(BPF_REG_ARG4, BPF_REG_X);\n\t\tif (fp->k)\n\t\t\t*insn++ = BPF_ALU64_IMM(BPF_ADD, BPF_REG_ARG4, offset);\n\t}\n\n\tswitch (BPF_SIZE(fp->code)) {\n\tcase BPF_B:\n\t\t*insn++ = BPF_EMIT_CALL(bpf_skb_load_helper_8);\n\t\tbreak;\n\tcase BPF_H:\n\t\t*insn++ = BPF_EMIT_CALL(bpf_skb_load_helper_16);\n\t\tbreak;\n\tcase BPF_W:\n\t\t*insn++ = BPF_EMIT_CALL(bpf_skb_load_helper_32);\n\t\tbreak;\n\tdefault:\n\t\treturn false;\n\t}\n\n\t*insn++ = BPF_JMP_IMM(BPF_JSGE, BPF_REG_A, 0, 2);\n\t*insn++ = BPF_ALU32_REG(BPF_XOR, BPF_REG_A, BPF_REG_A);\n\t*insn   = BPF_EXIT_INSN();\n\n\t*insnp = insn;\n\treturn true;\n}\n\n \nstatic int bpf_convert_filter(struct sock_filter *prog, int len,\n\t\t\t      struct bpf_prog *new_prog, int *new_len,\n\t\t\t      bool *seen_ld_abs)\n{\n\tint new_flen = 0, pass = 0, target, i, stack_off;\n\tstruct bpf_insn *new_insn, *first_insn = NULL;\n\tstruct sock_filter *fp;\n\tint *addrs = NULL;\n\tu8 bpf_src;\n\n\tBUILD_BUG_ON(BPF_MEMWORDS * sizeof(u32) > MAX_BPF_STACK);\n\tBUILD_BUG_ON(BPF_REG_FP + 1 != MAX_BPF_REG);\n\n\tif (len <= 0 || len > BPF_MAXINSNS)\n\t\treturn -EINVAL;\n\n\tif (new_prog) {\n\t\tfirst_insn = new_prog->insnsi;\n\t\taddrs = kcalloc(len, sizeof(*addrs),\n\t\t\t\tGFP_KERNEL | __GFP_NOWARN);\n\t\tif (!addrs)\n\t\t\treturn -ENOMEM;\n\t}\n\ndo_pass:\n\tnew_insn = first_insn;\n\tfp = prog;\n\n\t \n\tif (new_prog) {\n\t\t \n\t\t*new_insn++ = BPF_ALU32_REG(BPF_XOR, BPF_REG_A, BPF_REG_A);\n\t\t*new_insn++ = BPF_ALU32_REG(BPF_XOR, BPF_REG_X, BPF_REG_X);\n\n\t\t \n\t\t*new_insn++ = BPF_MOV64_REG(BPF_REG_CTX, BPF_REG_ARG1);\n\t\tif (*seen_ld_abs) {\n\t\t\t \n\t\t\t*new_insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, data),\n\t\t\t\t\t\t  BPF_REG_D, BPF_REG_CTX,\n\t\t\t\t\t\t  offsetof(struct sk_buff, data));\n\t\t\t*new_insn++ = BPF_LDX_MEM(BPF_W, BPF_REG_H, BPF_REG_CTX,\n\t\t\t\t\t\t  offsetof(struct sk_buff, len));\n\t\t\t*new_insn++ = BPF_LDX_MEM(BPF_W, BPF_REG_TMP, BPF_REG_CTX,\n\t\t\t\t\t\t  offsetof(struct sk_buff, data_len));\n\t\t\t*new_insn++ = BPF_ALU32_REG(BPF_SUB, BPF_REG_H, BPF_REG_TMP);\n\t\t}\n\t} else {\n\t\tnew_insn += 3;\n\t}\n\n\tfor (i = 0; i < len; fp++, i++) {\n\t\tstruct bpf_insn tmp_insns[32] = { };\n\t\tstruct bpf_insn *insn = tmp_insns;\n\n\t\tif (addrs)\n\t\t\taddrs[i] = new_insn - first_insn;\n\n\t\tswitch (fp->code) {\n\t\t \n\t\tcase BPF_ALU | BPF_ADD | BPF_X:\n\t\tcase BPF_ALU | BPF_ADD | BPF_K:\n\t\tcase BPF_ALU | BPF_SUB | BPF_X:\n\t\tcase BPF_ALU | BPF_SUB | BPF_K:\n\t\tcase BPF_ALU | BPF_AND | BPF_X:\n\t\tcase BPF_ALU | BPF_AND | BPF_K:\n\t\tcase BPF_ALU | BPF_OR | BPF_X:\n\t\tcase BPF_ALU | BPF_OR | BPF_K:\n\t\tcase BPF_ALU | BPF_LSH | BPF_X:\n\t\tcase BPF_ALU | BPF_LSH | BPF_K:\n\t\tcase BPF_ALU | BPF_RSH | BPF_X:\n\t\tcase BPF_ALU | BPF_RSH | BPF_K:\n\t\tcase BPF_ALU | BPF_XOR | BPF_X:\n\t\tcase BPF_ALU | BPF_XOR | BPF_K:\n\t\tcase BPF_ALU | BPF_MUL | BPF_X:\n\t\tcase BPF_ALU | BPF_MUL | BPF_K:\n\t\tcase BPF_ALU | BPF_DIV | BPF_X:\n\t\tcase BPF_ALU | BPF_DIV | BPF_K:\n\t\tcase BPF_ALU | BPF_MOD | BPF_X:\n\t\tcase BPF_ALU | BPF_MOD | BPF_K:\n\t\tcase BPF_ALU | BPF_NEG:\n\t\tcase BPF_LD | BPF_ABS | BPF_W:\n\t\tcase BPF_LD | BPF_ABS | BPF_H:\n\t\tcase BPF_LD | BPF_ABS | BPF_B:\n\t\tcase BPF_LD | BPF_IND | BPF_W:\n\t\tcase BPF_LD | BPF_IND | BPF_H:\n\t\tcase BPF_LD | BPF_IND | BPF_B:\n\t\t\t \n\t\t\tif (BPF_CLASS(fp->code) == BPF_LD &&\n\t\t\t    BPF_MODE(fp->code) == BPF_ABS &&\n\t\t\t    convert_bpf_extensions(fp, &insn))\n\t\t\t\tbreak;\n\t\t\tif (BPF_CLASS(fp->code) == BPF_LD &&\n\t\t\t    convert_bpf_ld_abs(fp, &insn)) {\n\t\t\t\t*seen_ld_abs = true;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (fp->code == (BPF_ALU | BPF_DIV | BPF_X) ||\n\t\t\t    fp->code == (BPF_ALU | BPF_MOD | BPF_X)) {\n\t\t\t\t*insn++ = BPF_MOV32_REG(BPF_REG_X, BPF_REG_X);\n\t\t\t\t \n\t\t\t\t*insn++ = BPF_JMP_IMM(BPF_JNE, BPF_REG_X, 0, 2);\n\t\t\t\t*insn++ = BPF_ALU32_REG(BPF_XOR, BPF_REG_A, BPF_REG_A);\n\t\t\t\t*insn++ = BPF_EXIT_INSN();\n\t\t\t}\n\n\t\t\t*insn = BPF_RAW_INSN(fp->code, BPF_REG_A, BPF_REG_X, 0, fp->k);\n\t\t\tbreak;\n\n\t\t \n\n#define BPF_EMIT_JMP\t\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tconst s32 off_min = S16_MIN, off_max = S16_MAX;\t\t\\\n\t\ts32 off;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\tif (target >= len || target < 0)\t\t\t\\\n\t\t\tgoto err;\t\t\t\t\t\\\n\t\toff = addrs ? addrs[target] - addrs[i] - 1 : 0;\t\t\\\n\t\t \t\\\n\t\toff -= insn - tmp_insns;\t\t\t\t\\\n\t\t \t\\\n\t\tif (off < off_min || off > off_max)\t\t\t\\\n\t\t\tgoto err;\t\t\t\t\t\\\n\t\tinsn->off = off;\t\t\t\t\t\\\n\t} while (0)\n\n\t\tcase BPF_JMP | BPF_JA:\n\t\t\ttarget = i + fp->k + 1;\n\t\t\tinsn->code = fp->code;\n\t\t\tBPF_EMIT_JMP;\n\t\t\tbreak;\n\n\t\tcase BPF_JMP | BPF_JEQ | BPF_K:\n\t\tcase BPF_JMP | BPF_JEQ | BPF_X:\n\t\tcase BPF_JMP | BPF_JSET | BPF_K:\n\t\tcase BPF_JMP | BPF_JSET | BPF_X:\n\t\tcase BPF_JMP | BPF_JGT | BPF_K:\n\t\tcase BPF_JMP | BPF_JGT | BPF_X:\n\t\tcase BPF_JMP | BPF_JGE | BPF_K:\n\t\tcase BPF_JMP | BPF_JGE | BPF_X:\n\t\t\tif (BPF_SRC(fp->code) == BPF_K && (int) fp->k < 0) {\n\t\t\t\t \n\t\t\t\t*insn++ = BPF_MOV32_IMM(BPF_REG_TMP, fp->k);\n\n\t\t\t\tinsn->dst_reg = BPF_REG_A;\n\t\t\t\tinsn->src_reg = BPF_REG_TMP;\n\t\t\t\tbpf_src = BPF_X;\n\t\t\t} else {\n\t\t\t\tinsn->dst_reg = BPF_REG_A;\n\t\t\t\tinsn->imm = fp->k;\n\t\t\t\tbpf_src = BPF_SRC(fp->code);\n\t\t\t\tinsn->src_reg = bpf_src == BPF_X ? BPF_REG_X : 0;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (fp->jf == 0) {\n\t\t\t\tinsn->code = BPF_JMP | BPF_OP(fp->code) | bpf_src;\n\t\t\t\ttarget = i + fp->jt + 1;\n\t\t\t\tBPF_EMIT_JMP;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (fp->jt == 0) {\n\t\t\t\tswitch (BPF_OP(fp->code)) {\n\t\t\t\tcase BPF_JEQ:\n\t\t\t\t\tinsn->code = BPF_JMP | BPF_JNE | bpf_src;\n\t\t\t\t\tbreak;\n\t\t\t\tcase BPF_JGT:\n\t\t\t\t\tinsn->code = BPF_JMP | BPF_JLE | bpf_src;\n\t\t\t\t\tbreak;\n\t\t\t\tcase BPF_JGE:\n\t\t\t\t\tinsn->code = BPF_JMP | BPF_JLT | bpf_src;\n\t\t\t\t\tbreak;\n\t\t\t\tdefault:\n\t\t\t\t\tgoto jmp_rest;\n\t\t\t\t}\n\n\t\t\t\ttarget = i + fp->jf + 1;\n\t\t\t\tBPF_EMIT_JMP;\n\t\t\t\tbreak;\n\t\t\t}\njmp_rest:\n\t\t\t \n\t\t\ttarget = i + fp->jt + 1;\n\t\t\tinsn->code = BPF_JMP | BPF_OP(fp->code) | bpf_src;\n\t\t\tBPF_EMIT_JMP;\n\t\t\tinsn++;\n\n\t\t\tinsn->code = BPF_JMP | BPF_JA;\n\t\t\ttarget = i + fp->jf + 1;\n\t\t\tBPF_EMIT_JMP;\n\t\t\tbreak;\n\n\t\t \n\t\tcase BPF_LDX | BPF_MSH | BPF_B: {\n\t\t\tstruct sock_filter tmp = {\n\t\t\t\t.code\t= BPF_LD | BPF_ABS | BPF_B,\n\t\t\t\t.k\t= fp->k,\n\t\t\t};\n\n\t\t\t*seen_ld_abs = true;\n\n\t\t\t \n\t\t\t*insn++ = BPF_MOV64_REG(BPF_REG_X, BPF_REG_A);\n\t\t\t \n\t\t\tconvert_bpf_ld_abs(&tmp, &insn);\n\t\t\tinsn++;\n\t\t\t \n\t\t\t*insn++ = BPF_ALU32_IMM(BPF_AND, BPF_REG_A, 0xf);\n\t\t\t \n\t\t\t*insn++ = BPF_ALU32_IMM(BPF_LSH, BPF_REG_A, 2);\n\t\t\t \n\t\t\t*insn++ = BPF_MOV64_REG(BPF_REG_TMP, BPF_REG_X);\n\t\t\t \n\t\t\t*insn++ = BPF_MOV64_REG(BPF_REG_X, BPF_REG_A);\n\t\t\t \n\t\t\t*insn = BPF_MOV64_REG(BPF_REG_A, BPF_REG_TMP);\n\t\t\tbreak;\n\t\t}\n\t\t \n\t\tcase BPF_RET | BPF_A:\n\t\tcase BPF_RET | BPF_K:\n\t\t\tif (BPF_RVAL(fp->code) == BPF_K)\n\t\t\t\t*insn++ = BPF_MOV32_RAW(BPF_K, BPF_REG_0,\n\t\t\t\t\t\t\t0, fp->k);\n\t\t\t*insn = BPF_EXIT_INSN();\n\t\t\tbreak;\n\n\t\t \n\t\tcase BPF_ST:\n\t\tcase BPF_STX:\n\t\t\tstack_off = fp->k * 4  + 4;\n\t\t\t*insn = BPF_STX_MEM(BPF_W, BPF_REG_FP, BPF_CLASS(fp->code) ==\n\t\t\t\t\t    BPF_ST ? BPF_REG_A : BPF_REG_X,\n\t\t\t\t\t    -stack_off);\n\t\t\t \n\t\t\tif (new_prog && new_prog->aux->stack_depth < stack_off)\n\t\t\t\tnew_prog->aux->stack_depth = stack_off;\n\t\t\tbreak;\n\n\t\t \n\t\tcase BPF_LD | BPF_MEM:\n\t\tcase BPF_LDX | BPF_MEM:\n\t\t\tstack_off = fp->k * 4  + 4;\n\t\t\t*insn = BPF_LDX_MEM(BPF_W, BPF_CLASS(fp->code) == BPF_LD  ?\n\t\t\t\t\t    BPF_REG_A : BPF_REG_X, BPF_REG_FP,\n\t\t\t\t\t    -stack_off);\n\t\t\tbreak;\n\n\t\t \n\t\tcase BPF_LD | BPF_IMM:\n\t\tcase BPF_LDX | BPF_IMM:\n\t\t\t*insn = BPF_MOV32_IMM(BPF_CLASS(fp->code) == BPF_LD ?\n\t\t\t\t\t      BPF_REG_A : BPF_REG_X, fp->k);\n\t\t\tbreak;\n\n\t\t \n\t\tcase BPF_MISC | BPF_TAX:\n\t\t\t*insn = BPF_MOV64_REG(BPF_REG_X, BPF_REG_A);\n\t\t\tbreak;\n\n\t\t \n\t\tcase BPF_MISC | BPF_TXA:\n\t\t\t*insn = BPF_MOV64_REG(BPF_REG_A, BPF_REG_X);\n\t\t\tbreak;\n\n\t\t \n\t\tcase BPF_LD | BPF_W | BPF_LEN:\n\t\tcase BPF_LDX | BPF_W | BPF_LEN:\n\t\t\t*insn = BPF_LDX_MEM(BPF_W, BPF_CLASS(fp->code) == BPF_LD ?\n\t\t\t\t\t    BPF_REG_A : BPF_REG_X, BPF_REG_CTX,\n\t\t\t\t\t    offsetof(struct sk_buff, len));\n\t\t\tbreak;\n\n\t\t \n\t\tcase BPF_LDX | BPF_ABS | BPF_W:\n\t\t\t \n\t\t\t*insn = BPF_LDX_MEM(BPF_W, BPF_REG_A, BPF_REG_CTX, fp->k);\n\t\t\tbreak;\n\n\t\t \n\t\tdefault:\n\t\t\tgoto err;\n\t\t}\n\n\t\tinsn++;\n\t\tif (new_prog)\n\t\t\tmemcpy(new_insn, tmp_insns,\n\t\t\t       sizeof(*insn) * (insn - tmp_insns));\n\t\tnew_insn += insn - tmp_insns;\n\t}\n\n\tif (!new_prog) {\n\t\t \n\t\t*new_len = new_insn - first_insn;\n\t\tif (*seen_ld_abs)\n\t\t\t*new_len += 4;  \n\t\treturn 0;\n\t}\n\n\tpass++;\n\tif (new_flen != new_insn - first_insn) {\n\t\tnew_flen = new_insn - first_insn;\n\t\tif (pass > 2)\n\t\t\tgoto err;\n\t\tgoto do_pass;\n\t}\n\n\tkfree(addrs);\n\tBUG_ON(*new_len != new_flen);\n\treturn 0;\nerr:\n\tkfree(addrs);\n\treturn -EINVAL;\n}\n\n \nstatic int check_load_and_stores(const struct sock_filter *filter, int flen)\n{\n\tu16 *masks, memvalid = 0;  \n\tint pc, ret = 0;\n\n\tBUILD_BUG_ON(BPF_MEMWORDS > 16);\n\n\tmasks = kmalloc_array(flen, sizeof(*masks), GFP_KERNEL);\n\tif (!masks)\n\t\treturn -ENOMEM;\n\n\tmemset(masks, 0xff, flen * sizeof(*masks));\n\n\tfor (pc = 0; pc < flen; pc++) {\n\t\tmemvalid &= masks[pc];\n\n\t\tswitch (filter[pc].code) {\n\t\tcase BPF_ST:\n\t\tcase BPF_STX:\n\t\t\tmemvalid |= (1 << filter[pc].k);\n\t\t\tbreak;\n\t\tcase BPF_LD | BPF_MEM:\n\t\tcase BPF_LDX | BPF_MEM:\n\t\t\tif (!(memvalid & (1 << filter[pc].k))) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase BPF_JMP | BPF_JA:\n\t\t\t \n\t\t\tmasks[pc + 1 + filter[pc].k] &= memvalid;\n\t\t\tmemvalid = ~0;\n\t\t\tbreak;\n\t\tcase BPF_JMP | BPF_JEQ | BPF_K:\n\t\tcase BPF_JMP | BPF_JEQ | BPF_X:\n\t\tcase BPF_JMP | BPF_JGE | BPF_K:\n\t\tcase BPF_JMP | BPF_JGE | BPF_X:\n\t\tcase BPF_JMP | BPF_JGT | BPF_K:\n\t\tcase BPF_JMP | BPF_JGT | BPF_X:\n\t\tcase BPF_JMP | BPF_JSET | BPF_K:\n\t\tcase BPF_JMP | BPF_JSET | BPF_X:\n\t\t\t \n\t\t\tmasks[pc + 1 + filter[pc].jt] &= memvalid;\n\t\t\tmasks[pc + 1 + filter[pc].jf] &= memvalid;\n\t\t\tmemvalid = ~0;\n\t\t\tbreak;\n\t\t}\n\t}\nerror:\n\tkfree(masks);\n\treturn ret;\n}\n\nstatic bool chk_code_allowed(u16 code_to_probe)\n{\n\tstatic const bool codes[] = {\n\t\t \n\t\t[BPF_ALU | BPF_ADD | BPF_K] = true,\n\t\t[BPF_ALU | BPF_ADD | BPF_X] = true,\n\t\t[BPF_ALU | BPF_SUB | BPF_K] = true,\n\t\t[BPF_ALU | BPF_SUB | BPF_X] = true,\n\t\t[BPF_ALU | BPF_MUL | BPF_K] = true,\n\t\t[BPF_ALU | BPF_MUL | BPF_X] = true,\n\t\t[BPF_ALU | BPF_DIV | BPF_K] = true,\n\t\t[BPF_ALU | BPF_DIV | BPF_X] = true,\n\t\t[BPF_ALU | BPF_MOD | BPF_K] = true,\n\t\t[BPF_ALU | BPF_MOD | BPF_X] = true,\n\t\t[BPF_ALU | BPF_AND | BPF_K] = true,\n\t\t[BPF_ALU | BPF_AND | BPF_X] = true,\n\t\t[BPF_ALU | BPF_OR | BPF_K] = true,\n\t\t[BPF_ALU | BPF_OR | BPF_X] = true,\n\t\t[BPF_ALU | BPF_XOR | BPF_K] = true,\n\t\t[BPF_ALU | BPF_XOR | BPF_X] = true,\n\t\t[BPF_ALU | BPF_LSH | BPF_K] = true,\n\t\t[BPF_ALU | BPF_LSH | BPF_X] = true,\n\t\t[BPF_ALU | BPF_RSH | BPF_K] = true,\n\t\t[BPF_ALU | BPF_RSH | BPF_X] = true,\n\t\t[BPF_ALU | BPF_NEG] = true,\n\t\t \n\t\t[BPF_LD | BPF_W | BPF_ABS] = true,\n\t\t[BPF_LD | BPF_H | BPF_ABS] = true,\n\t\t[BPF_LD | BPF_B | BPF_ABS] = true,\n\t\t[BPF_LD | BPF_W | BPF_LEN] = true,\n\t\t[BPF_LD | BPF_W | BPF_IND] = true,\n\t\t[BPF_LD | BPF_H | BPF_IND] = true,\n\t\t[BPF_LD | BPF_B | BPF_IND] = true,\n\t\t[BPF_LD | BPF_IMM] = true,\n\t\t[BPF_LD | BPF_MEM] = true,\n\t\t[BPF_LDX | BPF_W | BPF_LEN] = true,\n\t\t[BPF_LDX | BPF_B | BPF_MSH] = true,\n\t\t[BPF_LDX | BPF_IMM] = true,\n\t\t[BPF_LDX | BPF_MEM] = true,\n\t\t \n\t\t[BPF_ST] = true,\n\t\t[BPF_STX] = true,\n\t\t \n\t\t[BPF_MISC | BPF_TAX] = true,\n\t\t[BPF_MISC | BPF_TXA] = true,\n\t\t \n\t\t[BPF_RET | BPF_K] = true,\n\t\t[BPF_RET | BPF_A] = true,\n\t\t \n\t\t[BPF_JMP | BPF_JA] = true,\n\t\t[BPF_JMP | BPF_JEQ | BPF_K] = true,\n\t\t[BPF_JMP | BPF_JEQ | BPF_X] = true,\n\t\t[BPF_JMP | BPF_JGE | BPF_K] = true,\n\t\t[BPF_JMP | BPF_JGE | BPF_X] = true,\n\t\t[BPF_JMP | BPF_JGT | BPF_K] = true,\n\t\t[BPF_JMP | BPF_JGT | BPF_X] = true,\n\t\t[BPF_JMP | BPF_JSET | BPF_K] = true,\n\t\t[BPF_JMP | BPF_JSET | BPF_X] = true,\n\t};\n\n\tif (code_to_probe >= ARRAY_SIZE(codes))\n\t\treturn false;\n\n\treturn codes[code_to_probe];\n}\n\nstatic bool bpf_check_basics_ok(const struct sock_filter *filter,\n\t\t\t\tunsigned int flen)\n{\n\tif (filter == NULL)\n\t\treturn false;\n\tif (flen == 0 || flen > BPF_MAXINSNS)\n\t\treturn false;\n\n\treturn true;\n}\n\n \nstatic int bpf_check_classic(const struct sock_filter *filter,\n\t\t\t     unsigned int flen)\n{\n\tbool anc_found;\n\tint pc;\n\n\t \n\tfor (pc = 0; pc < flen; pc++) {\n\t\tconst struct sock_filter *ftest = &filter[pc];\n\n\t\t \n\t\tif (!chk_code_allowed(ftest->code))\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tswitch (ftest->code) {\n\t\tcase BPF_ALU | BPF_DIV | BPF_K:\n\t\tcase BPF_ALU | BPF_MOD | BPF_K:\n\t\t\t \n\t\t\tif (ftest->k == 0)\n\t\t\t\treturn -EINVAL;\n\t\t\tbreak;\n\t\tcase BPF_ALU | BPF_LSH | BPF_K:\n\t\tcase BPF_ALU | BPF_RSH | BPF_K:\n\t\t\tif (ftest->k >= 32)\n\t\t\t\treturn -EINVAL;\n\t\t\tbreak;\n\t\tcase BPF_LD | BPF_MEM:\n\t\tcase BPF_LDX | BPF_MEM:\n\t\tcase BPF_ST:\n\t\tcase BPF_STX:\n\t\t\t \n\t\t\tif (ftest->k >= BPF_MEMWORDS)\n\t\t\t\treturn -EINVAL;\n\t\t\tbreak;\n\t\tcase BPF_JMP | BPF_JA:\n\t\t\t \n\t\t\tif (ftest->k >= (unsigned int)(flen - pc - 1))\n\t\t\t\treturn -EINVAL;\n\t\t\tbreak;\n\t\tcase BPF_JMP | BPF_JEQ | BPF_K:\n\t\tcase BPF_JMP | BPF_JEQ | BPF_X:\n\t\tcase BPF_JMP | BPF_JGE | BPF_K:\n\t\tcase BPF_JMP | BPF_JGE | BPF_X:\n\t\tcase BPF_JMP | BPF_JGT | BPF_K:\n\t\tcase BPF_JMP | BPF_JGT | BPF_X:\n\t\tcase BPF_JMP | BPF_JSET | BPF_K:\n\t\tcase BPF_JMP | BPF_JSET | BPF_X:\n\t\t\t \n\t\t\tif (pc + ftest->jt + 1 >= flen ||\n\t\t\t    pc + ftest->jf + 1 >= flen)\n\t\t\t\treturn -EINVAL;\n\t\t\tbreak;\n\t\tcase BPF_LD | BPF_W | BPF_ABS:\n\t\tcase BPF_LD | BPF_H | BPF_ABS:\n\t\tcase BPF_LD | BPF_B | BPF_ABS:\n\t\t\tanc_found = false;\n\t\t\tif (bpf_anc_helper(ftest) & BPF_ANC)\n\t\t\t\tanc_found = true;\n\t\t\t \n\t\t\tif (anc_found == false && ftest->k >= SKF_AD_OFF)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t \n\tswitch (filter[flen - 1].code) {\n\tcase BPF_RET | BPF_K:\n\tcase BPF_RET | BPF_A:\n\t\treturn check_load_and_stores(filter, flen);\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic int bpf_prog_store_orig_filter(struct bpf_prog *fp,\n\t\t\t\t      const struct sock_fprog *fprog)\n{\n\tunsigned int fsize = bpf_classic_proglen(fprog);\n\tstruct sock_fprog_kern *fkprog;\n\n\tfp->orig_prog = kmalloc(sizeof(*fkprog), GFP_KERNEL);\n\tif (!fp->orig_prog)\n\t\treturn -ENOMEM;\n\n\tfkprog = fp->orig_prog;\n\tfkprog->len = fprog->len;\n\n\tfkprog->filter = kmemdup(fp->insns, fsize,\n\t\t\t\t GFP_KERNEL | __GFP_NOWARN);\n\tif (!fkprog->filter) {\n\t\tkfree(fp->orig_prog);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic void bpf_release_orig_filter(struct bpf_prog *fp)\n{\n\tstruct sock_fprog_kern *fprog = fp->orig_prog;\n\n\tif (fprog) {\n\t\tkfree(fprog->filter);\n\t\tkfree(fprog);\n\t}\n}\n\nstatic void __bpf_prog_release(struct bpf_prog *prog)\n{\n\tif (prog->type == BPF_PROG_TYPE_SOCKET_FILTER) {\n\t\tbpf_prog_put(prog);\n\t} else {\n\t\tbpf_release_orig_filter(prog);\n\t\tbpf_prog_free(prog);\n\t}\n}\n\nstatic void __sk_filter_release(struct sk_filter *fp)\n{\n\t__bpf_prog_release(fp->prog);\n\tkfree(fp);\n}\n\n \nstatic void sk_filter_release_rcu(struct rcu_head *rcu)\n{\n\tstruct sk_filter *fp = container_of(rcu, struct sk_filter, rcu);\n\n\t__sk_filter_release(fp);\n}\n\n \nstatic void sk_filter_release(struct sk_filter *fp)\n{\n\tif (refcount_dec_and_test(&fp->refcnt))\n\t\tcall_rcu(&fp->rcu, sk_filter_release_rcu);\n}\n\nvoid sk_filter_uncharge(struct sock *sk, struct sk_filter *fp)\n{\n\tu32 filter_size = bpf_prog_size(fp->prog->len);\n\n\tatomic_sub(filter_size, &sk->sk_omem_alloc);\n\tsk_filter_release(fp);\n}\n\n \nstatic bool __sk_filter_charge(struct sock *sk, struct sk_filter *fp)\n{\n\tu32 filter_size = bpf_prog_size(fp->prog->len);\n\tint optmem_max = READ_ONCE(sysctl_optmem_max);\n\n\t \n\tif (filter_size <= optmem_max &&\n\t    atomic_read(&sk->sk_omem_alloc) + filter_size < optmem_max) {\n\t\tatomic_add(filter_size, &sk->sk_omem_alloc);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nbool sk_filter_charge(struct sock *sk, struct sk_filter *fp)\n{\n\tif (!refcount_inc_not_zero(&fp->refcnt))\n\t\treturn false;\n\n\tif (!__sk_filter_charge(sk, fp)) {\n\t\tsk_filter_release(fp);\n\t\treturn false;\n\t}\n\treturn true;\n}\n\nstatic struct bpf_prog *bpf_migrate_filter(struct bpf_prog *fp)\n{\n\tstruct sock_filter *old_prog;\n\tstruct bpf_prog *old_fp;\n\tint err, new_len, old_len = fp->len;\n\tbool seen_ld_abs = false;\n\n\t \n\tBUILD_BUG_ON(sizeof(struct sock_filter) !=\n\t\t     sizeof(struct bpf_insn));\n\n\t \n\told_prog = kmemdup(fp->insns, old_len * sizeof(struct sock_filter),\n\t\t\t   GFP_KERNEL | __GFP_NOWARN);\n\tif (!old_prog) {\n\t\terr = -ENOMEM;\n\t\tgoto out_err;\n\t}\n\n\t \n\terr = bpf_convert_filter(old_prog, old_len, NULL, &new_len,\n\t\t\t\t &seen_ld_abs);\n\tif (err)\n\t\tgoto out_err_free;\n\n\t \n\told_fp = fp;\n\tfp = bpf_prog_realloc(old_fp, bpf_prog_size(new_len), 0);\n\tif (!fp) {\n\t\t \n\t\tfp = old_fp;\n\t\terr = -ENOMEM;\n\t\tgoto out_err_free;\n\t}\n\n\tfp->len = new_len;\n\n\t \n\terr = bpf_convert_filter(old_prog, old_len, fp, &new_len,\n\t\t\t\t &seen_ld_abs);\n\tif (err)\n\t\t \n\t\tgoto out_err_free;\n\n\tfp = bpf_prog_select_runtime(fp, &err);\n\tif (err)\n\t\tgoto out_err_free;\n\n\tkfree(old_prog);\n\treturn fp;\n\nout_err_free:\n\tkfree(old_prog);\nout_err:\n\t__bpf_prog_release(fp);\n\treturn ERR_PTR(err);\n}\n\nstatic struct bpf_prog *bpf_prepare_filter(struct bpf_prog *fp,\n\t\t\t\t\t   bpf_aux_classic_check_t trans)\n{\n\tint err;\n\n\tfp->bpf_func = NULL;\n\tfp->jited = 0;\n\n\terr = bpf_check_classic(fp->insns, fp->len);\n\tif (err) {\n\t\t__bpf_prog_release(fp);\n\t\treturn ERR_PTR(err);\n\t}\n\n\t \n\tif (trans) {\n\t\terr = trans(fp->insns, fp->len);\n\t\tif (err) {\n\t\t\t__bpf_prog_release(fp);\n\t\t\treturn ERR_PTR(err);\n\t\t}\n\t}\n\n\t \n\tbpf_jit_compile(fp);\n\n\t \n\tif (!fp->jited)\n\t\tfp = bpf_migrate_filter(fp);\n\n\treturn fp;\n}\n\n \nint bpf_prog_create(struct bpf_prog **pfp, struct sock_fprog_kern *fprog)\n{\n\tunsigned int fsize = bpf_classic_proglen(fprog);\n\tstruct bpf_prog *fp;\n\n\t \n\tif (!bpf_check_basics_ok(fprog->filter, fprog->len))\n\t\treturn -EINVAL;\n\n\tfp = bpf_prog_alloc(bpf_prog_size(fprog->len), 0);\n\tif (!fp)\n\t\treturn -ENOMEM;\n\n\tmemcpy(fp->insns, fprog->filter, fsize);\n\n\tfp->len = fprog->len;\n\t \n\tfp->orig_prog = NULL;\n\n\t \n\tfp = bpf_prepare_filter(fp, NULL);\n\tif (IS_ERR(fp))\n\t\treturn PTR_ERR(fp);\n\n\t*pfp = fp;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(bpf_prog_create);\n\n \nint bpf_prog_create_from_user(struct bpf_prog **pfp, struct sock_fprog *fprog,\n\t\t\t      bpf_aux_classic_check_t trans, bool save_orig)\n{\n\tunsigned int fsize = bpf_classic_proglen(fprog);\n\tstruct bpf_prog *fp;\n\tint err;\n\n\t \n\tif (!bpf_check_basics_ok(fprog->filter, fprog->len))\n\t\treturn -EINVAL;\n\n\tfp = bpf_prog_alloc(bpf_prog_size(fprog->len), 0);\n\tif (!fp)\n\t\treturn -ENOMEM;\n\n\tif (copy_from_user(fp->insns, fprog->filter, fsize)) {\n\t\t__bpf_prog_free(fp);\n\t\treturn -EFAULT;\n\t}\n\n\tfp->len = fprog->len;\n\tfp->orig_prog = NULL;\n\n\tif (save_orig) {\n\t\terr = bpf_prog_store_orig_filter(fp, fprog);\n\t\tif (err) {\n\t\t\t__bpf_prog_free(fp);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\t \n\tfp = bpf_prepare_filter(fp, trans);\n\tif (IS_ERR(fp))\n\t\treturn PTR_ERR(fp);\n\n\t*pfp = fp;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(bpf_prog_create_from_user);\n\nvoid bpf_prog_destroy(struct bpf_prog *fp)\n{\n\t__bpf_prog_release(fp);\n}\nEXPORT_SYMBOL_GPL(bpf_prog_destroy);\n\nstatic int __sk_attach_prog(struct bpf_prog *prog, struct sock *sk)\n{\n\tstruct sk_filter *fp, *old_fp;\n\n\tfp = kmalloc(sizeof(*fp), GFP_KERNEL);\n\tif (!fp)\n\t\treturn -ENOMEM;\n\n\tfp->prog = prog;\n\n\tif (!__sk_filter_charge(sk, fp)) {\n\t\tkfree(fp);\n\t\treturn -ENOMEM;\n\t}\n\trefcount_set(&fp->refcnt, 1);\n\n\told_fp = rcu_dereference_protected(sk->sk_filter,\n\t\t\t\t\t   lockdep_sock_is_held(sk));\n\trcu_assign_pointer(sk->sk_filter, fp);\n\n\tif (old_fp)\n\t\tsk_filter_uncharge(sk, old_fp);\n\n\treturn 0;\n}\n\nstatic\nstruct bpf_prog *__get_filter(struct sock_fprog *fprog, struct sock *sk)\n{\n\tunsigned int fsize = bpf_classic_proglen(fprog);\n\tstruct bpf_prog *prog;\n\tint err;\n\n\tif (sock_flag(sk, SOCK_FILTER_LOCKED))\n\t\treturn ERR_PTR(-EPERM);\n\n\t \n\tif (!bpf_check_basics_ok(fprog->filter, fprog->len))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tprog = bpf_prog_alloc(bpf_prog_size(fprog->len), 0);\n\tif (!prog)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (copy_from_user(prog->insns, fprog->filter, fsize)) {\n\t\t__bpf_prog_free(prog);\n\t\treturn ERR_PTR(-EFAULT);\n\t}\n\n\tprog->len = fprog->len;\n\n\terr = bpf_prog_store_orig_filter(prog, fprog);\n\tif (err) {\n\t\t__bpf_prog_free(prog);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\t \n\treturn bpf_prepare_filter(prog, NULL);\n}\n\n \nint sk_attach_filter(struct sock_fprog *fprog, struct sock *sk)\n{\n\tstruct bpf_prog *prog = __get_filter(fprog, sk);\n\tint err;\n\n\tif (IS_ERR(prog))\n\t\treturn PTR_ERR(prog);\n\n\terr = __sk_attach_prog(prog, sk);\n\tif (err < 0) {\n\t\t__bpf_prog_release(prog);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(sk_attach_filter);\n\nint sk_reuseport_attach_filter(struct sock_fprog *fprog, struct sock *sk)\n{\n\tstruct bpf_prog *prog = __get_filter(fprog, sk);\n\tint err;\n\n\tif (IS_ERR(prog))\n\t\treturn PTR_ERR(prog);\n\n\tif (bpf_prog_size(prog->len) > READ_ONCE(sysctl_optmem_max))\n\t\terr = -ENOMEM;\n\telse\n\t\terr = reuseport_attach_prog(sk, prog);\n\n\tif (err)\n\t\t__bpf_prog_release(prog);\n\n\treturn err;\n}\n\nstatic struct bpf_prog *__get_bpf(u32 ufd, struct sock *sk)\n{\n\tif (sock_flag(sk, SOCK_FILTER_LOCKED))\n\t\treturn ERR_PTR(-EPERM);\n\n\treturn bpf_prog_get_type(ufd, BPF_PROG_TYPE_SOCKET_FILTER);\n}\n\nint sk_attach_bpf(u32 ufd, struct sock *sk)\n{\n\tstruct bpf_prog *prog = __get_bpf(ufd, sk);\n\tint err;\n\n\tif (IS_ERR(prog))\n\t\treturn PTR_ERR(prog);\n\n\terr = __sk_attach_prog(prog, sk);\n\tif (err < 0) {\n\t\tbpf_prog_put(prog);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nint sk_reuseport_attach_bpf(u32 ufd, struct sock *sk)\n{\n\tstruct bpf_prog *prog;\n\tint err;\n\n\tif (sock_flag(sk, SOCK_FILTER_LOCKED))\n\t\treturn -EPERM;\n\n\tprog = bpf_prog_get_type(ufd, BPF_PROG_TYPE_SOCKET_FILTER);\n\tif (PTR_ERR(prog) == -EINVAL)\n\t\tprog = bpf_prog_get_type(ufd, BPF_PROG_TYPE_SK_REUSEPORT);\n\tif (IS_ERR(prog))\n\t\treturn PTR_ERR(prog);\n\n\tif (prog->type == BPF_PROG_TYPE_SK_REUSEPORT) {\n\t\t \n\t\tif ((sk->sk_type != SOCK_STREAM &&\n\t\t     sk->sk_type != SOCK_DGRAM) ||\n\t\t    (sk->sk_protocol != IPPROTO_UDP &&\n\t\t     sk->sk_protocol != IPPROTO_TCP) ||\n\t\t    (sk->sk_family != AF_INET &&\n\t\t     sk->sk_family != AF_INET6)) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto err_prog_put;\n\t\t}\n\t} else {\n\t\t \n\t\tif (bpf_prog_size(prog->len) > READ_ONCE(sysctl_optmem_max)) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_prog_put;\n\t\t}\n\t}\n\n\terr = reuseport_attach_prog(sk, prog);\nerr_prog_put:\n\tif (err)\n\t\tbpf_prog_put(prog);\n\n\treturn err;\n}\n\nvoid sk_reuseport_prog_free(struct bpf_prog *prog)\n{\n\tif (!prog)\n\t\treturn;\n\n\tif (prog->type == BPF_PROG_TYPE_SK_REUSEPORT)\n\t\tbpf_prog_put(prog);\n\telse\n\t\tbpf_prog_destroy(prog);\n}\n\nstruct bpf_scratchpad {\n\tunion {\n\t\t__be32 diff[MAX_BPF_STACK / sizeof(__be32)];\n\t\tu8     buff[MAX_BPF_STACK];\n\t};\n};\n\nstatic DEFINE_PER_CPU(struct bpf_scratchpad, bpf_sp);\n\nstatic inline int __bpf_try_make_writable(struct sk_buff *skb,\n\t\t\t\t\t  unsigned int write_len)\n{\n\treturn skb_ensure_writable(skb, write_len);\n}\n\nstatic inline int bpf_try_make_writable(struct sk_buff *skb,\n\t\t\t\t\tunsigned int write_len)\n{\n\tint err = __bpf_try_make_writable(skb, write_len);\n\n\tbpf_compute_data_pointers(skb);\n\treturn err;\n}\n\nstatic int bpf_try_make_head_writable(struct sk_buff *skb)\n{\n\treturn bpf_try_make_writable(skb, skb_headlen(skb));\n}\n\nstatic inline void bpf_push_mac_rcsum(struct sk_buff *skb)\n{\n\tif (skb_at_tc_ingress(skb))\n\t\tskb_postpush_rcsum(skb, skb_mac_header(skb), skb->mac_len);\n}\n\nstatic inline void bpf_pull_mac_rcsum(struct sk_buff *skb)\n{\n\tif (skb_at_tc_ingress(skb))\n\t\tskb_postpull_rcsum(skb, skb_mac_header(skb), skb->mac_len);\n}\n\nBPF_CALL_5(bpf_skb_store_bytes, struct sk_buff *, skb, u32, offset,\n\t   const void *, from, u32, len, u64, flags)\n{\n\tvoid *ptr;\n\n\tif (unlikely(flags & ~(BPF_F_RECOMPUTE_CSUM | BPF_F_INVALIDATE_HASH)))\n\t\treturn -EINVAL;\n\tif (unlikely(offset > INT_MAX))\n\t\treturn -EFAULT;\n\tif (unlikely(bpf_try_make_writable(skb, offset + len)))\n\t\treturn -EFAULT;\n\n\tptr = skb->data + offset;\n\tif (flags & BPF_F_RECOMPUTE_CSUM)\n\t\t__skb_postpull_rcsum(skb, ptr, len, offset);\n\n\tmemcpy(ptr, from, len);\n\n\tif (flags & BPF_F_RECOMPUTE_CSUM)\n\t\t__skb_postpush_rcsum(skb, ptr, len, offset);\n\tif (flags & BPF_F_INVALIDATE_HASH)\n\t\tskb_clear_hash(skb);\n\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_skb_store_bytes_proto = {\n\t.func\t\t= bpf_skb_store_bytes,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg4_type\t= ARG_CONST_SIZE,\n\t.arg5_type\t= ARG_ANYTHING,\n};\n\nint __bpf_skb_store_bytes(struct sk_buff *skb, u32 offset, const void *from,\n\t\t\t  u32 len, u64 flags)\n{\n\treturn ____bpf_skb_store_bytes(skb, offset, from, len, flags);\n}\n\nBPF_CALL_4(bpf_skb_load_bytes, const struct sk_buff *, skb, u32, offset,\n\t   void *, to, u32, len)\n{\n\tvoid *ptr;\n\n\tif (unlikely(offset > INT_MAX))\n\t\tgoto err_clear;\n\n\tptr = skb_header_pointer(skb, offset, len, to);\n\tif (unlikely(!ptr))\n\t\tgoto err_clear;\n\tif (ptr != to)\n\t\tmemcpy(to, ptr, len);\n\n\treturn 0;\nerr_clear:\n\tmemset(to, 0, len);\n\treturn -EFAULT;\n}\n\nstatic const struct bpf_func_proto bpf_skb_load_bytes_proto = {\n\t.func\t\t= bpf_skb_load_bytes,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg4_type\t= ARG_CONST_SIZE,\n};\n\nint __bpf_skb_load_bytes(const struct sk_buff *skb, u32 offset, void *to, u32 len)\n{\n\treturn ____bpf_skb_load_bytes(skb, offset, to, len);\n}\n\nBPF_CALL_4(bpf_flow_dissector_load_bytes,\n\t   const struct bpf_flow_dissector *, ctx, u32, offset,\n\t   void *, to, u32, len)\n{\n\tvoid *ptr;\n\n\tif (unlikely(offset > 0xffff))\n\t\tgoto err_clear;\n\n\tif (unlikely(!ctx->skb))\n\t\tgoto err_clear;\n\n\tptr = skb_header_pointer(ctx->skb, offset, len, to);\n\tif (unlikely(!ptr))\n\t\tgoto err_clear;\n\tif (ptr != to)\n\t\tmemcpy(to, ptr, len);\n\n\treturn 0;\nerr_clear:\n\tmemset(to, 0, len);\n\treturn -EFAULT;\n}\n\nstatic const struct bpf_func_proto bpf_flow_dissector_load_bytes_proto = {\n\t.func\t\t= bpf_flow_dissector_load_bytes,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg4_type\t= ARG_CONST_SIZE,\n};\n\nBPF_CALL_5(bpf_skb_load_bytes_relative, const struct sk_buff *, skb,\n\t   u32, offset, void *, to, u32, len, u32, start_header)\n{\n\tu8 *end = skb_tail_pointer(skb);\n\tu8 *start, *ptr;\n\n\tif (unlikely(offset > 0xffff))\n\t\tgoto err_clear;\n\n\tswitch (start_header) {\n\tcase BPF_HDR_START_MAC:\n\t\tif (unlikely(!skb_mac_header_was_set(skb)))\n\t\t\tgoto err_clear;\n\t\tstart = skb_mac_header(skb);\n\t\tbreak;\n\tcase BPF_HDR_START_NET:\n\t\tstart = skb_network_header(skb);\n\t\tbreak;\n\tdefault:\n\t\tgoto err_clear;\n\t}\n\n\tptr = start + offset;\n\n\tif (likely(ptr + len <= end)) {\n\t\tmemcpy(to, ptr, len);\n\t\treturn 0;\n\t}\n\nerr_clear:\n\tmemset(to, 0, len);\n\treturn -EFAULT;\n}\n\nstatic const struct bpf_func_proto bpf_skb_load_bytes_relative_proto = {\n\t.func\t\t= bpf_skb_load_bytes_relative,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg4_type\t= ARG_CONST_SIZE,\n\t.arg5_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_2(bpf_skb_pull_data, struct sk_buff *, skb, u32, len)\n{\n\t \n\treturn bpf_try_make_writable(skb, len ? : skb_headlen(skb));\n}\n\nstatic const struct bpf_func_proto bpf_skb_pull_data_proto = {\n\t.func\t\t= bpf_skb_pull_data,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_1(bpf_sk_fullsock, struct sock *, sk)\n{\n\treturn sk_fullsock(sk) ? (unsigned long)sk : (unsigned long)NULL;\n}\n\nstatic const struct bpf_func_proto bpf_sk_fullsock_proto = {\n\t.func\t\t= bpf_sk_fullsock,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_PTR_TO_SOCKET_OR_NULL,\n\t.arg1_type\t= ARG_PTR_TO_SOCK_COMMON,\n};\n\nstatic inline int sk_skb_try_make_writable(struct sk_buff *skb,\n\t\t\t\t\t   unsigned int write_len)\n{\n\treturn __bpf_try_make_writable(skb, write_len);\n}\n\nBPF_CALL_2(sk_skb_pull_data, struct sk_buff *, skb, u32, len)\n{\n\t \n\treturn sk_skb_try_make_writable(skb, len ? : skb_headlen(skb));\n}\n\nstatic const struct bpf_func_proto sk_skb_pull_data_proto = {\n\t.func\t\t= sk_skb_pull_data,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_5(bpf_l3_csum_replace, struct sk_buff *, skb, u32, offset,\n\t   u64, from, u64, to, u64, flags)\n{\n\t__sum16 *ptr;\n\n\tif (unlikely(flags & ~(BPF_F_HDR_FIELD_MASK)))\n\t\treturn -EINVAL;\n\tif (unlikely(offset > 0xffff || offset & 1))\n\t\treturn -EFAULT;\n\tif (unlikely(bpf_try_make_writable(skb, offset + sizeof(*ptr))))\n\t\treturn -EFAULT;\n\n\tptr = (__sum16 *)(skb->data + offset);\n\tswitch (flags & BPF_F_HDR_FIELD_MASK) {\n\tcase 0:\n\t\tif (unlikely(from != 0))\n\t\t\treturn -EINVAL;\n\n\t\tcsum_replace_by_diff(ptr, to);\n\t\tbreak;\n\tcase 2:\n\t\tcsum_replace2(ptr, from, to);\n\t\tbreak;\n\tcase 4:\n\t\tcsum_replace4(ptr, from, to);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_l3_csum_replace_proto = {\n\t.func\t\t= bpf_l3_csum_replace,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_ANYTHING,\n\t.arg5_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_5(bpf_l4_csum_replace, struct sk_buff *, skb, u32, offset,\n\t   u64, from, u64, to, u64, flags)\n{\n\tbool is_pseudo = flags & BPF_F_PSEUDO_HDR;\n\tbool is_mmzero = flags & BPF_F_MARK_MANGLED_0;\n\tbool do_mforce = flags & BPF_F_MARK_ENFORCE;\n\t__sum16 *ptr;\n\n\tif (unlikely(flags & ~(BPF_F_MARK_MANGLED_0 | BPF_F_MARK_ENFORCE |\n\t\t\t       BPF_F_PSEUDO_HDR | BPF_F_HDR_FIELD_MASK)))\n\t\treturn -EINVAL;\n\tif (unlikely(offset > 0xffff || offset & 1))\n\t\treturn -EFAULT;\n\tif (unlikely(bpf_try_make_writable(skb, offset + sizeof(*ptr))))\n\t\treturn -EFAULT;\n\n\tptr = (__sum16 *)(skb->data + offset);\n\tif (is_mmzero && !do_mforce && !*ptr)\n\t\treturn 0;\n\n\tswitch (flags & BPF_F_HDR_FIELD_MASK) {\n\tcase 0:\n\t\tif (unlikely(from != 0))\n\t\t\treturn -EINVAL;\n\n\t\tinet_proto_csum_replace_by_diff(ptr, skb, to, is_pseudo);\n\t\tbreak;\n\tcase 2:\n\t\tinet_proto_csum_replace2(ptr, skb, from, to, is_pseudo);\n\t\tbreak;\n\tcase 4:\n\t\tinet_proto_csum_replace4(ptr, skb, from, to, is_pseudo);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif (is_mmzero && !*ptr)\n\t\t*ptr = CSUM_MANGLED_0;\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_l4_csum_replace_proto = {\n\t.func\t\t= bpf_l4_csum_replace,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_ANYTHING,\n\t.arg5_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_5(bpf_csum_diff, __be32 *, from, u32, from_size,\n\t   __be32 *, to, u32, to_size, __wsum, seed)\n{\n\tstruct bpf_scratchpad *sp = this_cpu_ptr(&bpf_sp);\n\tu32 diff_size = from_size + to_size;\n\tint i, j = 0;\n\n\t \n\tif (unlikely(((from_size | to_size) & (sizeof(__be32) - 1)) ||\n\t\t     diff_size > sizeof(sp->diff)))\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < from_size / sizeof(__be32); i++, j++)\n\t\tsp->diff[j] = ~from[i];\n\tfor (i = 0; i <   to_size / sizeof(__be32); i++, j++)\n\t\tsp->diff[j] = to[i];\n\n\treturn csum_partial(sp->diff, diff_size, seed);\n}\n\nstatic const struct bpf_func_proto bpf_csum_diff_proto = {\n\t.func\t\t= bpf_csum_diff,\n\t.gpl_only\t= false,\n\t.pkt_access\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_MEM | PTR_MAYBE_NULL | MEM_RDONLY,\n\t.arg2_type\t= ARG_CONST_SIZE_OR_ZERO,\n\t.arg3_type\t= ARG_PTR_TO_MEM | PTR_MAYBE_NULL | MEM_RDONLY,\n\t.arg4_type\t= ARG_CONST_SIZE_OR_ZERO,\n\t.arg5_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_2(bpf_csum_update, struct sk_buff *, skb, __wsum, csum)\n{\n\t \n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\treturn (skb->csum = csum_add(skb->csum, csum));\n\n\treturn -ENOTSUPP;\n}\n\nstatic const struct bpf_func_proto bpf_csum_update_proto = {\n\t.func\t\t= bpf_csum_update,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_2(bpf_csum_level, struct sk_buff *, skb, u64, level)\n{\n\t \n\tswitch (level) {\n\tcase BPF_CSUM_LEVEL_INC:\n\t\t__skb_incr_checksum_unnecessary(skb);\n\t\tbreak;\n\tcase BPF_CSUM_LEVEL_DEC:\n\t\t__skb_decr_checksum_unnecessary(skb);\n\t\tbreak;\n\tcase BPF_CSUM_LEVEL_RESET:\n\t\t__skb_reset_checksum_unnecessary(skb);\n\t\tbreak;\n\tcase BPF_CSUM_LEVEL_QUERY:\n\t\treturn skb->ip_summed == CHECKSUM_UNNECESSARY ?\n\t\t       skb->csum_level : -EACCES;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_csum_level_proto = {\n\t.func\t\t= bpf_csum_level,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n};\n\nstatic inline int __bpf_rx_skb(struct net_device *dev, struct sk_buff *skb)\n{\n\treturn dev_forward_skb_nomtu(dev, skb);\n}\n\nstatic inline int __bpf_rx_skb_no_mac(struct net_device *dev,\n\t\t\t\t      struct sk_buff *skb)\n{\n\tint ret = ____dev_forward_skb(dev, skb, false);\n\n\tif (likely(!ret)) {\n\t\tskb->dev = dev;\n\t\tret = netif_rx(skb);\n\t}\n\n\treturn ret;\n}\n\nstatic inline int __bpf_tx_skb(struct net_device *dev, struct sk_buff *skb)\n{\n\tint ret;\n\n\tif (dev_xmit_recursion()) {\n\t\tnet_crit_ratelimited(\"bpf: recursion limit reached on datapath, buggy bpf program?\\n\");\n\t\tkfree_skb(skb);\n\t\treturn -ENETDOWN;\n\t}\n\n\tskb->dev = dev;\n\tskb_set_redirected_noclear(skb, skb_at_tc_ingress(skb));\n\tskb_clear_tstamp(skb);\n\n\tdev_xmit_recursion_inc();\n\tret = dev_queue_xmit(skb);\n\tdev_xmit_recursion_dec();\n\n\treturn ret;\n}\n\nstatic int __bpf_redirect_no_mac(struct sk_buff *skb, struct net_device *dev,\n\t\t\t\t u32 flags)\n{\n\tunsigned int mlen = skb_network_offset(skb);\n\n\tif (unlikely(skb->len <= mlen)) {\n\t\tkfree_skb(skb);\n\t\treturn -ERANGE;\n\t}\n\n\tif (mlen) {\n\t\t__skb_pull(skb, mlen);\n\n\t\t \n\t\tif (!skb_at_tc_ingress(skb))\n\t\t\tskb_postpull_rcsum(skb, skb_mac_header(skb), mlen);\n\t}\n\tskb_pop_mac_header(skb);\n\tskb_reset_mac_len(skb);\n\treturn flags & BPF_F_INGRESS ?\n\t       __bpf_rx_skb_no_mac(dev, skb) : __bpf_tx_skb(dev, skb);\n}\n\nstatic int __bpf_redirect_common(struct sk_buff *skb, struct net_device *dev,\n\t\t\t\t u32 flags)\n{\n\t \n\tif (unlikely(skb->mac_header >= skb->network_header || skb->len == 0)) {\n\t\tkfree_skb(skb);\n\t\treturn -ERANGE;\n\t}\n\n\tbpf_push_mac_rcsum(skb);\n\treturn flags & BPF_F_INGRESS ?\n\t       __bpf_rx_skb(dev, skb) : __bpf_tx_skb(dev, skb);\n}\n\nstatic int __bpf_redirect(struct sk_buff *skb, struct net_device *dev,\n\t\t\t  u32 flags)\n{\n\tif (dev_is_mac_header_xmit(dev))\n\t\treturn __bpf_redirect_common(skb, dev, flags);\n\telse\n\t\treturn __bpf_redirect_no_mac(skb, dev, flags);\n}\n\n#if IS_ENABLED(CONFIG_IPV6)\nstatic int bpf_out_neigh_v6(struct net *net, struct sk_buff *skb,\n\t\t\t    struct net_device *dev, struct bpf_nh_params *nh)\n{\n\tu32 hh_len = LL_RESERVED_SPACE(dev);\n\tconst struct in6_addr *nexthop;\n\tstruct dst_entry *dst = NULL;\n\tstruct neighbour *neigh;\n\n\tif (dev_xmit_recursion()) {\n\t\tnet_crit_ratelimited(\"bpf: recursion limit reached on datapath, buggy bpf program?\\n\");\n\t\tgoto out_drop;\n\t}\n\n\tskb->dev = dev;\n\tskb_clear_tstamp(skb);\n\n\tif (unlikely(skb_headroom(skb) < hh_len && dev->header_ops)) {\n\t\tskb = skb_expand_head(skb, hh_len);\n\t\tif (!skb)\n\t\t\treturn -ENOMEM;\n\t}\n\n\trcu_read_lock();\n\tif (!nh) {\n\t\tdst = skb_dst(skb);\n\t\tnexthop = rt6_nexthop(container_of(dst, struct rt6_info, dst),\n\t\t\t\t      &ipv6_hdr(skb)->daddr);\n\t} else {\n\t\tnexthop = &nh->ipv6_nh;\n\t}\n\tneigh = ip_neigh_gw6(dev, nexthop);\n\tif (likely(!IS_ERR(neigh))) {\n\t\tint ret;\n\n\t\tsock_confirm_neigh(skb, neigh);\n\t\tlocal_bh_disable();\n\t\tdev_xmit_recursion_inc();\n\t\tret = neigh_output(neigh, skb, false);\n\t\tdev_xmit_recursion_dec();\n\t\tlocal_bh_enable();\n\t\trcu_read_unlock();\n\t\treturn ret;\n\t}\n\trcu_read_unlock_bh();\n\tif (dst)\n\t\tIP6_INC_STATS(net, ip6_dst_idev(dst), IPSTATS_MIB_OUTNOROUTES);\nout_drop:\n\tkfree_skb(skb);\n\treturn -ENETDOWN;\n}\n\nstatic int __bpf_redirect_neigh_v6(struct sk_buff *skb, struct net_device *dev,\n\t\t\t\t   struct bpf_nh_params *nh)\n{\n\tconst struct ipv6hdr *ip6h = ipv6_hdr(skb);\n\tstruct net *net = dev_net(dev);\n\tint err, ret = NET_XMIT_DROP;\n\n\tif (!nh) {\n\t\tstruct dst_entry *dst;\n\t\tstruct flowi6 fl6 = {\n\t\t\t.flowi6_flags = FLOWI_FLAG_ANYSRC,\n\t\t\t.flowi6_mark  = skb->mark,\n\t\t\t.flowlabel    = ip6_flowinfo(ip6h),\n\t\t\t.flowi6_oif   = dev->ifindex,\n\t\t\t.flowi6_proto = ip6h->nexthdr,\n\t\t\t.daddr\t      = ip6h->daddr,\n\t\t\t.saddr\t      = ip6h->saddr,\n\t\t};\n\n\t\tdst = ipv6_stub->ipv6_dst_lookup_flow(net, NULL, &fl6, NULL);\n\t\tif (IS_ERR(dst))\n\t\t\tgoto out_drop;\n\n\t\tskb_dst_set(skb, dst);\n\t} else if (nh->nh_family != AF_INET6) {\n\t\tgoto out_drop;\n\t}\n\n\terr = bpf_out_neigh_v6(net, skb, dev, nh);\n\tif (unlikely(net_xmit_eval(err)))\n\t\tdev->stats.tx_errors++;\n\telse\n\t\tret = NET_XMIT_SUCCESS;\n\tgoto out_xmit;\nout_drop:\n\tdev->stats.tx_errors++;\n\tkfree_skb(skb);\nout_xmit:\n\treturn ret;\n}\n#else\nstatic int __bpf_redirect_neigh_v6(struct sk_buff *skb, struct net_device *dev,\n\t\t\t\t   struct bpf_nh_params *nh)\n{\n\tkfree_skb(skb);\n\treturn NET_XMIT_DROP;\n}\n#endif  \n\n#if IS_ENABLED(CONFIG_INET)\nstatic int bpf_out_neigh_v4(struct net *net, struct sk_buff *skb,\n\t\t\t    struct net_device *dev, struct bpf_nh_params *nh)\n{\n\tu32 hh_len = LL_RESERVED_SPACE(dev);\n\tstruct neighbour *neigh;\n\tbool is_v6gw = false;\n\n\tif (dev_xmit_recursion()) {\n\t\tnet_crit_ratelimited(\"bpf: recursion limit reached on datapath, buggy bpf program?\\n\");\n\t\tgoto out_drop;\n\t}\n\n\tskb->dev = dev;\n\tskb_clear_tstamp(skb);\n\n\tif (unlikely(skb_headroom(skb) < hh_len && dev->header_ops)) {\n\t\tskb = skb_expand_head(skb, hh_len);\n\t\tif (!skb)\n\t\t\treturn -ENOMEM;\n\t}\n\n\trcu_read_lock();\n\tif (!nh) {\n\t\tstruct dst_entry *dst = skb_dst(skb);\n\t\tstruct rtable *rt = container_of(dst, struct rtable, dst);\n\n\t\tneigh = ip_neigh_for_gw(rt, skb, &is_v6gw);\n\t} else if (nh->nh_family == AF_INET6) {\n\t\tneigh = ip_neigh_gw6(dev, &nh->ipv6_nh);\n\t\tis_v6gw = true;\n\t} else if (nh->nh_family == AF_INET) {\n\t\tneigh = ip_neigh_gw4(dev, nh->ipv4_nh);\n\t} else {\n\t\trcu_read_unlock();\n\t\tgoto out_drop;\n\t}\n\n\tif (likely(!IS_ERR(neigh))) {\n\t\tint ret;\n\n\t\tsock_confirm_neigh(skb, neigh);\n\t\tlocal_bh_disable();\n\t\tdev_xmit_recursion_inc();\n\t\tret = neigh_output(neigh, skb, is_v6gw);\n\t\tdev_xmit_recursion_dec();\n\t\tlocal_bh_enable();\n\t\trcu_read_unlock();\n\t\treturn ret;\n\t}\n\trcu_read_unlock();\nout_drop:\n\tkfree_skb(skb);\n\treturn -ENETDOWN;\n}\n\nstatic int __bpf_redirect_neigh_v4(struct sk_buff *skb, struct net_device *dev,\n\t\t\t\t   struct bpf_nh_params *nh)\n{\n\tconst struct iphdr *ip4h = ip_hdr(skb);\n\tstruct net *net = dev_net(dev);\n\tint err, ret = NET_XMIT_DROP;\n\n\tif (!nh) {\n\t\tstruct flowi4 fl4 = {\n\t\t\t.flowi4_flags = FLOWI_FLAG_ANYSRC,\n\t\t\t.flowi4_mark  = skb->mark,\n\t\t\t.flowi4_tos   = RT_TOS(ip4h->tos),\n\t\t\t.flowi4_oif   = dev->ifindex,\n\t\t\t.flowi4_proto = ip4h->protocol,\n\t\t\t.daddr\t      = ip4h->daddr,\n\t\t\t.saddr\t      = ip4h->saddr,\n\t\t};\n\t\tstruct rtable *rt;\n\n\t\trt = ip_route_output_flow(net, &fl4, NULL);\n\t\tif (IS_ERR(rt))\n\t\t\tgoto out_drop;\n\t\tif (rt->rt_type != RTN_UNICAST && rt->rt_type != RTN_LOCAL) {\n\t\t\tip_rt_put(rt);\n\t\t\tgoto out_drop;\n\t\t}\n\n\t\tskb_dst_set(skb, &rt->dst);\n\t}\n\n\terr = bpf_out_neigh_v4(net, skb, dev, nh);\n\tif (unlikely(net_xmit_eval(err)))\n\t\tdev->stats.tx_errors++;\n\telse\n\t\tret = NET_XMIT_SUCCESS;\n\tgoto out_xmit;\nout_drop:\n\tdev->stats.tx_errors++;\n\tkfree_skb(skb);\nout_xmit:\n\treturn ret;\n}\n#else\nstatic int __bpf_redirect_neigh_v4(struct sk_buff *skb, struct net_device *dev,\n\t\t\t\t   struct bpf_nh_params *nh)\n{\n\tkfree_skb(skb);\n\treturn NET_XMIT_DROP;\n}\n#endif  \n\nstatic int __bpf_redirect_neigh(struct sk_buff *skb, struct net_device *dev,\n\t\t\t\tstruct bpf_nh_params *nh)\n{\n\tstruct ethhdr *ethh = eth_hdr(skb);\n\n\tif (unlikely(skb->mac_header >= skb->network_header))\n\t\tgoto out;\n\tbpf_push_mac_rcsum(skb);\n\tif (is_multicast_ether_addr(ethh->h_dest))\n\t\tgoto out;\n\n\tskb_pull(skb, sizeof(*ethh));\n\tskb_unset_mac_header(skb);\n\tskb_reset_network_header(skb);\n\n\tif (skb->protocol == htons(ETH_P_IP))\n\t\treturn __bpf_redirect_neigh_v4(skb, dev, nh);\n\telse if (skb->protocol == htons(ETH_P_IPV6))\n\t\treturn __bpf_redirect_neigh_v6(skb, dev, nh);\nout:\n\tkfree_skb(skb);\n\treturn -ENOTSUPP;\n}\n\n \nenum {\n\tBPF_F_NEIGH\t= (1ULL << 1),\n\tBPF_F_PEER\t= (1ULL << 2),\n\tBPF_F_NEXTHOP\t= (1ULL << 3),\n#define BPF_F_REDIRECT_INTERNAL\t(BPF_F_NEIGH | BPF_F_PEER | BPF_F_NEXTHOP)\n};\n\nBPF_CALL_3(bpf_clone_redirect, struct sk_buff *, skb, u32, ifindex, u64, flags)\n{\n\tstruct net_device *dev;\n\tstruct sk_buff *clone;\n\tint ret;\n\n\tif (unlikely(flags & (~(BPF_F_INGRESS) | BPF_F_REDIRECT_INTERNAL)))\n\t\treturn -EINVAL;\n\n\tdev = dev_get_by_index_rcu(dev_net(skb->dev), ifindex);\n\tif (unlikely(!dev))\n\t\treturn -EINVAL;\n\n\tclone = skb_clone(skb, GFP_ATOMIC);\n\tif (unlikely(!clone))\n\t\treturn -ENOMEM;\n\n\t \n\tret = bpf_try_make_head_writable(skb);\n\tif (unlikely(ret)) {\n\t\tkfree_skb(clone);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn __bpf_redirect(clone, dev, flags);\n}\n\nstatic const struct bpf_func_proto bpf_clone_redirect_proto = {\n\t.func           = bpf_clone_redirect,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type      = ARG_PTR_TO_CTX,\n\t.arg2_type      = ARG_ANYTHING,\n\t.arg3_type      = ARG_ANYTHING,\n};\n\nDEFINE_PER_CPU(struct bpf_redirect_info, bpf_redirect_info);\nEXPORT_PER_CPU_SYMBOL_GPL(bpf_redirect_info);\n\nint skb_do_redirect(struct sk_buff *skb)\n{\n\tstruct bpf_redirect_info *ri = this_cpu_ptr(&bpf_redirect_info);\n\tstruct net *net = dev_net(skb->dev);\n\tstruct net_device *dev;\n\tu32 flags = ri->flags;\n\n\tdev = dev_get_by_index_rcu(net, ri->tgt_index);\n\tri->tgt_index = 0;\n\tri->flags = 0;\n\tif (unlikely(!dev))\n\t\tgoto out_drop;\n\tif (flags & BPF_F_PEER) {\n\t\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\t\tif (unlikely(!ops->ndo_get_peer_dev ||\n\t\t\t     !skb_at_tc_ingress(skb)))\n\t\t\tgoto out_drop;\n\t\tdev = ops->ndo_get_peer_dev(dev);\n\t\tif (unlikely(!dev ||\n\t\t\t     !(dev->flags & IFF_UP) ||\n\t\t\t     net_eq(net, dev_net(dev))))\n\t\t\tgoto out_drop;\n\t\tskb->dev = dev;\n\t\tdev_sw_netstats_rx_add(dev, skb->len);\n\t\treturn -EAGAIN;\n\t}\n\treturn flags & BPF_F_NEIGH ?\n\t       __bpf_redirect_neigh(skb, dev, flags & BPF_F_NEXTHOP ?\n\t\t\t\t    &ri->nh : NULL) :\n\t       __bpf_redirect(skb, dev, flags);\nout_drop:\n\tkfree_skb(skb);\n\treturn -EINVAL;\n}\n\nBPF_CALL_2(bpf_redirect, u32, ifindex, u64, flags)\n{\n\tstruct bpf_redirect_info *ri = this_cpu_ptr(&bpf_redirect_info);\n\n\tif (unlikely(flags & (~(BPF_F_INGRESS) | BPF_F_REDIRECT_INTERNAL)))\n\t\treturn TC_ACT_SHOT;\n\n\tri->flags = flags;\n\tri->tgt_index = ifindex;\n\n\treturn TC_ACT_REDIRECT;\n}\n\nstatic const struct bpf_func_proto bpf_redirect_proto = {\n\t.func           = bpf_redirect,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type      = ARG_ANYTHING,\n\t.arg2_type      = ARG_ANYTHING,\n};\n\nBPF_CALL_2(bpf_redirect_peer, u32, ifindex, u64, flags)\n{\n\tstruct bpf_redirect_info *ri = this_cpu_ptr(&bpf_redirect_info);\n\n\tif (unlikely(flags))\n\t\treturn TC_ACT_SHOT;\n\n\tri->flags = BPF_F_PEER;\n\tri->tgt_index = ifindex;\n\n\treturn TC_ACT_REDIRECT;\n}\n\nstatic const struct bpf_func_proto bpf_redirect_peer_proto = {\n\t.func           = bpf_redirect_peer,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type      = ARG_ANYTHING,\n\t.arg2_type      = ARG_ANYTHING,\n};\n\nBPF_CALL_4(bpf_redirect_neigh, u32, ifindex, struct bpf_redir_neigh *, params,\n\t   int, plen, u64, flags)\n{\n\tstruct bpf_redirect_info *ri = this_cpu_ptr(&bpf_redirect_info);\n\n\tif (unlikely((plen && plen < sizeof(*params)) || flags))\n\t\treturn TC_ACT_SHOT;\n\n\tri->flags = BPF_F_NEIGH | (plen ? BPF_F_NEXTHOP : 0);\n\tri->tgt_index = ifindex;\n\n\tBUILD_BUG_ON(sizeof(struct bpf_redir_neigh) != sizeof(struct bpf_nh_params));\n\tif (plen)\n\t\tmemcpy(&ri->nh, params, sizeof(ri->nh));\n\n\treturn TC_ACT_REDIRECT;\n}\n\nstatic const struct bpf_func_proto bpf_redirect_neigh_proto = {\n\t.func\t\t= bpf_redirect_neigh,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_ANYTHING,\n\t.arg2_type      = ARG_PTR_TO_MEM | PTR_MAYBE_NULL | MEM_RDONLY,\n\t.arg3_type      = ARG_CONST_SIZE_OR_ZERO,\n\t.arg4_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_2(bpf_msg_apply_bytes, struct sk_msg *, msg, u32, bytes)\n{\n\tmsg->apply_bytes = bytes;\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_msg_apply_bytes_proto = {\n\t.func           = bpf_msg_apply_bytes,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type      = ARG_ANYTHING,\n};\n\nBPF_CALL_2(bpf_msg_cork_bytes, struct sk_msg *, msg, u32, bytes)\n{\n\tmsg->cork_bytes = bytes;\n\treturn 0;\n}\n\nstatic void sk_msg_reset_curr(struct sk_msg *msg)\n{\n\tu32 i = msg->sg.start;\n\tu32 len = 0;\n\n\tdo {\n\t\tlen += sk_msg_elem(msg, i)->length;\n\t\tsk_msg_iter_var_next(i);\n\t\tif (len >= msg->sg.size)\n\t\t\tbreak;\n\t} while (i != msg->sg.end);\n\n\tmsg->sg.curr = i;\n\tmsg->sg.copybreak = 0;\n}\n\nstatic const struct bpf_func_proto bpf_msg_cork_bytes_proto = {\n\t.func           = bpf_msg_cork_bytes,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type      = ARG_ANYTHING,\n};\n\nBPF_CALL_4(bpf_msg_pull_data, struct sk_msg *, msg, u32, start,\n\t   u32, end, u64, flags)\n{\n\tu32 len = 0, offset = 0, copy = 0, poffset = 0, bytes = end - start;\n\tu32 first_sge, last_sge, i, shift, bytes_sg_total;\n\tstruct scatterlist *sge;\n\tu8 *raw, *to, *from;\n\tstruct page *page;\n\n\tif (unlikely(flags || end <= start))\n\t\treturn -EINVAL;\n\n\t \n\ti = msg->sg.start;\n\tdo {\n\t\toffset += len;\n\t\tlen = sk_msg_elem(msg, i)->length;\n\t\tif (start < offset + len)\n\t\t\tbreak;\n\t\tsk_msg_iter_var_next(i);\n\t} while (i != msg->sg.end);\n\n\tif (unlikely(start >= offset + len))\n\t\treturn -EINVAL;\n\n\tfirst_sge = i;\n\t \n\tbytes_sg_total = start - offset + bytes;\n\tif (!test_bit(i, msg->sg.copy) && bytes_sg_total <= len)\n\t\tgoto out;\n\n\t \n\tdo {\n\t\tcopy += sk_msg_elem(msg, i)->length;\n\t\tsk_msg_iter_var_next(i);\n\t\tif (bytes_sg_total <= copy)\n\t\t\tbreak;\n\t} while (i != msg->sg.end);\n\tlast_sge = i;\n\n\tif (unlikely(bytes_sg_total > copy))\n\t\treturn -EINVAL;\n\n\tpage = alloc_pages(__GFP_NOWARN | GFP_ATOMIC | __GFP_COMP,\n\t\t\t   get_order(copy));\n\tif (unlikely(!page))\n\t\treturn -ENOMEM;\n\n\traw = page_address(page);\n\ti = first_sge;\n\tdo {\n\t\tsge = sk_msg_elem(msg, i);\n\t\tfrom = sg_virt(sge);\n\t\tlen = sge->length;\n\t\tto = raw + poffset;\n\n\t\tmemcpy(to, from, len);\n\t\tpoffset += len;\n\t\tsge->length = 0;\n\t\tput_page(sg_page(sge));\n\n\t\tsk_msg_iter_var_next(i);\n\t} while (i != last_sge);\n\n\tsg_set_page(&msg->sg.data[first_sge], page, copy, 0);\n\n\t \n\tWARN_ON_ONCE(last_sge == first_sge);\n\tshift = last_sge > first_sge ?\n\t\tlast_sge - first_sge - 1 :\n\t\tNR_MSG_FRAG_IDS - first_sge + last_sge - 1;\n\tif (!shift)\n\t\tgoto out;\n\n\ti = first_sge;\n\tsk_msg_iter_var_next(i);\n\tdo {\n\t\tu32 move_from;\n\n\t\tif (i + shift >= NR_MSG_FRAG_IDS)\n\t\t\tmove_from = i + shift - NR_MSG_FRAG_IDS;\n\t\telse\n\t\t\tmove_from = i + shift;\n\t\tif (move_from == msg->sg.end)\n\t\t\tbreak;\n\n\t\tmsg->sg.data[i] = msg->sg.data[move_from];\n\t\tmsg->sg.data[move_from].length = 0;\n\t\tmsg->sg.data[move_from].page_link = 0;\n\t\tmsg->sg.data[move_from].offset = 0;\n\t\tsk_msg_iter_var_next(i);\n\t} while (1);\n\n\tmsg->sg.end = msg->sg.end - shift > msg->sg.end ?\n\t\t      msg->sg.end - shift + NR_MSG_FRAG_IDS :\n\t\t      msg->sg.end - shift;\nout:\n\tsk_msg_reset_curr(msg);\n\tmsg->data = sg_virt(&msg->sg.data[first_sge]) + start - offset;\n\tmsg->data_end = msg->data + bytes;\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_msg_pull_data_proto = {\n\t.func\t\t= bpf_msg_pull_data,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_4(bpf_msg_push_data, struct sk_msg *, msg, u32, start,\n\t   u32, len, u64, flags)\n{\n\tstruct scatterlist sge, nsge, nnsge, rsge = {0}, *psge;\n\tu32 new, i = 0, l = 0, space, copy = 0, offset = 0;\n\tu8 *raw, *to, *from;\n\tstruct page *page;\n\n\tif (unlikely(flags))\n\t\treturn -EINVAL;\n\n\tif (unlikely(len == 0))\n\t\treturn 0;\n\n\t \n\ti = msg->sg.start;\n\tdo {\n\t\toffset += l;\n\t\tl = sk_msg_elem(msg, i)->length;\n\n\t\tif (start < offset + l)\n\t\t\tbreak;\n\t\tsk_msg_iter_var_next(i);\n\t} while (i != msg->sg.end);\n\n\tif (start >= offset + l)\n\t\treturn -EINVAL;\n\n\tspace = MAX_MSG_FRAGS - sk_msg_elem_used(msg);\n\n\t \n\tif (!space || (space == 1 && start != offset))\n\t\tcopy = msg->sg.data[i].length;\n\n\tpage = alloc_pages(__GFP_NOWARN | GFP_ATOMIC | __GFP_COMP,\n\t\t\t   get_order(copy + len));\n\tif (unlikely(!page))\n\t\treturn -ENOMEM;\n\n\tif (copy) {\n\t\tint front, back;\n\n\t\traw = page_address(page);\n\n\t\tpsge = sk_msg_elem(msg, i);\n\t\tfront = start - offset;\n\t\tback = psge->length - front;\n\t\tfrom = sg_virt(psge);\n\n\t\tif (front)\n\t\t\tmemcpy(raw, from, front);\n\n\t\tif (back) {\n\t\t\tfrom += front;\n\t\t\tto = raw + front + len;\n\n\t\t\tmemcpy(to, from, back);\n\t\t}\n\n\t\tput_page(sg_page(psge));\n\t} else if (start - offset) {\n\t\tpsge = sk_msg_elem(msg, i);\n\t\trsge = sk_msg_elem_cpy(msg, i);\n\n\t\tpsge->length = start - offset;\n\t\trsge.length -= psge->length;\n\t\trsge.offset += start;\n\n\t\tsk_msg_iter_var_next(i);\n\t\tsg_unmark_end(psge);\n\t\tsg_unmark_end(&rsge);\n\t\tsk_msg_iter_next(msg, end);\n\t}\n\n\t \n\tnew = i;\n\n\t \n\tif (!copy) {\n\t\tsge = sk_msg_elem_cpy(msg, i);\n\n\t\tsk_msg_iter_var_next(i);\n\t\tsg_unmark_end(&sge);\n\t\tsk_msg_iter_next(msg, end);\n\n\t\tnsge = sk_msg_elem_cpy(msg, i);\n\t\tif (rsge.length) {\n\t\t\tsk_msg_iter_var_next(i);\n\t\t\tnnsge = sk_msg_elem_cpy(msg, i);\n\t\t}\n\n\t\twhile (i != msg->sg.end) {\n\t\t\tmsg->sg.data[i] = sge;\n\t\t\tsge = nsge;\n\t\t\tsk_msg_iter_var_next(i);\n\t\t\tif (rsge.length) {\n\t\t\t\tnsge = nnsge;\n\t\t\t\tnnsge = sk_msg_elem_cpy(msg, i);\n\t\t\t} else {\n\t\t\t\tnsge = sk_msg_elem_cpy(msg, i);\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tsk_mem_charge(msg->sk, len);\n\tmsg->sg.size += len;\n\t__clear_bit(new, msg->sg.copy);\n\tsg_set_page(&msg->sg.data[new], page, len + copy, 0);\n\tif (rsge.length) {\n\t\tget_page(sg_page(&rsge));\n\t\tsk_msg_iter_var_next(new);\n\t\tmsg->sg.data[new] = rsge;\n\t}\n\n\tsk_msg_reset_curr(msg);\n\tsk_msg_compute_data_pointers(msg);\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_msg_push_data_proto = {\n\t.func\t\t= bpf_msg_push_data,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_ANYTHING,\n};\n\nstatic void sk_msg_shift_left(struct sk_msg *msg, int i)\n{\n\tint prev;\n\n\tdo {\n\t\tprev = i;\n\t\tsk_msg_iter_var_next(i);\n\t\tmsg->sg.data[prev] = msg->sg.data[i];\n\t} while (i != msg->sg.end);\n\n\tsk_msg_iter_prev(msg, end);\n}\n\nstatic void sk_msg_shift_right(struct sk_msg *msg, int i)\n{\n\tstruct scatterlist tmp, sge;\n\n\tsk_msg_iter_next(msg, end);\n\tsge = sk_msg_elem_cpy(msg, i);\n\tsk_msg_iter_var_next(i);\n\ttmp = sk_msg_elem_cpy(msg, i);\n\n\twhile (i != msg->sg.end) {\n\t\tmsg->sg.data[i] = sge;\n\t\tsk_msg_iter_var_next(i);\n\t\tsge = tmp;\n\t\ttmp = sk_msg_elem_cpy(msg, i);\n\t}\n}\n\nBPF_CALL_4(bpf_msg_pop_data, struct sk_msg *, msg, u32, start,\n\t   u32, len, u64, flags)\n{\n\tu32 i = 0, l = 0, space, offset = 0;\n\tu64 last = start + len;\n\tint pop;\n\n\tif (unlikely(flags))\n\t\treturn -EINVAL;\n\n\t \n\ti = msg->sg.start;\n\tdo {\n\t\toffset += l;\n\t\tl = sk_msg_elem(msg, i)->length;\n\n\t\tif (start < offset + l)\n\t\t\tbreak;\n\t\tsk_msg_iter_var_next(i);\n\t} while (i != msg->sg.end);\n\n\t \n\tif (start >= offset + l || last >= msg->sg.size)\n\t\treturn -EINVAL;\n\n\tspace = MAX_MSG_FRAGS - sk_msg_elem_used(msg);\n\n\tpop = len;\n\t \n\tif (start != offset) {\n\t\tstruct scatterlist *nsge, *sge = sk_msg_elem(msg, i);\n\t\tint a = start;\n\t\tint b = sge->length - pop - a;\n\n\t\tsk_msg_iter_var_next(i);\n\n\t\tif (pop < sge->length - a) {\n\t\t\tif (space) {\n\t\t\t\tsge->length = a;\n\t\t\t\tsk_msg_shift_right(msg, i);\n\t\t\t\tnsge = sk_msg_elem(msg, i);\n\t\t\t\tget_page(sg_page(sge));\n\t\t\t\tsg_set_page(nsge,\n\t\t\t\t\t    sg_page(sge),\n\t\t\t\t\t    b, sge->offset + pop + a);\n\t\t\t} else {\n\t\t\t\tstruct page *page, *orig;\n\t\t\t\tu8 *to, *from;\n\n\t\t\t\tpage = alloc_pages(__GFP_NOWARN |\n\t\t\t\t\t\t   __GFP_COMP   | GFP_ATOMIC,\n\t\t\t\t\t\t   get_order(a + b));\n\t\t\t\tif (unlikely(!page))\n\t\t\t\t\treturn -ENOMEM;\n\n\t\t\t\tsge->length = a;\n\t\t\t\torig = sg_page(sge);\n\t\t\t\tfrom = sg_virt(sge);\n\t\t\t\tto = page_address(page);\n\t\t\t\tmemcpy(to, from, a);\n\t\t\t\tmemcpy(to + a, from + a + pop, b);\n\t\t\t\tsg_set_page(sge, page, a + b, 0);\n\t\t\t\tput_page(orig);\n\t\t\t}\n\t\t\tpop = 0;\n\t\t} else if (pop >= sge->length - a) {\n\t\t\tpop -= (sge->length - a);\n\t\t\tsge->length = a;\n\t\t}\n\t}\n\n\t \n\twhile (pop) {\n\t\tstruct scatterlist *sge = sk_msg_elem(msg, i);\n\n\t\tif (pop < sge->length) {\n\t\t\tsge->length -= pop;\n\t\t\tsge->offset += pop;\n\t\t\tpop = 0;\n\t\t} else {\n\t\t\tpop -= sge->length;\n\t\t\tsk_msg_shift_left(msg, i);\n\t\t}\n\t\tsk_msg_iter_var_next(i);\n\t}\n\n\tsk_mem_uncharge(msg->sk, len - pop);\n\tmsg->sg.size -= (len - pop);\n\tsk_msg_reset_curr(msg);\n\tsk_msg_compute_data_pointers(msg);\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_msg_pop_data_proto = {\n\t.func\t\t= bpf_msg_pop_data,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_ANYTHING,\n};\n\n#ifdef CONFIG_CGROUP_NET_CLASSID\nBPF_CALL_0(bpf_get_cgroup_classid_curr)\n{\n\treturn __task_get_classid(current);\n}\n\nconst struct bpf_func_proto bpf_get_cgroup_classid_curr_proto = {\n\t.func\t\t= bpf_get_cgroup_classid_curr,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n};\n\nBPF_CALL_1(bpf_skb_cgroup_classid, const struct sk_buff *, skb)\n{\n\tstruct sock *sk = skb_to_full_sk(skb);\n\n\tif (!sk || !sk_fullsock(sk))\n\t\treturn 0;\n\n\treturn sock_cgroup_classid(&sk->sk_cgrp_data);\n}\n\nstatic const struct bpf_func_proto bpf_skb_cgroup_classid_proto = {\n\t.func\t\t= bpf_skb_cgroup_classid,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n};\n#endif\n\nBPF_CALL_1(bpf_get_cgroup_classid, const struct sk_buff *, skb)\n{\n\treturn task_get_classid(skb);\n}\n\nstatic const struct bpf_func_proto bpf_get_cgroup_classid_proto = {\n\t.func           = bpf_get_cgroup_classid,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type      = ARG_PTR_TO_CTX,\n};\n\nBPF_CALL_1(bpf_get_route_realm, const struct sk_buff *, skb)\n{\n\treturn dst_tclassid(skb);\n}\n\nstatic const struct bpf_func_proto bpf_get_route_realm_proto = {\n\t.func           = bpf_get_route_realm,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type      = ARG_PTR_TO_CTX,\n};\n\nBPF_CALL_1(bpf_get_hash_recalc, struct sk_buff *, skb)\n{\n\t \n\treturn skb_get_hash(skb);\n}\n\nstatic const struct bpf_func_proto bpf_get_hash_recalc_proto = {\n\t.func\t\t= bpf_get_hash_recalc,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n};\n\nBPF_CALL_1(bpf_set_hash_invalid, struct sk_buff *, skb)\n{\n\t \n\tskb_clear_hash(skb);\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_set_hash_invalid_proto = {\n\t.func\t\t= bpf_set_hash_invalid,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n};\n\nBPF_CALL_2(bpf_set_hash, struct sk_buff *, skb, u32, hash)\n{\n\t \n\t__skb_set_sw_hash(skb, hash, true);\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_set_hash_proto = {\n\t.func\t\t= bpf_set_hash,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_3(bpf_skb_vlan_push, struct sk_buff *, skb, __be16, vlan_proto,\n\t   u16, vlan_tci)\n{\n\tint ret;\n\n\tif (unlikely(vlan_proto != htons(ETH_P_8021Q) &&\n\t\t     vlan_proto != htons(ETH_P_8021AD)))\n\t\tvlan_proto = htons(ETH_P_8021Q);\n\n\tbpf_push_mac_rcsum(skb);\n\tret = skb_vlan_push(skb, vlan_proto, vlan_tci);\n\tbpf_pull_mac_rcsum(skb);\n\n\tbpf_compute_data_pointers(skb);\n\treturn ret;\n}\n\nstatic const struct bpf_func_proto bpf_skb_vlan_push_proto = {\n\t.func           = bpf_skb_vlan_push,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type      = ARG_PTR_TO_CTX,\n\t.arg2_type      = ARG_ANYTHING,\n\t.arg3_type      = ARG_ANYTHING,\n};\n\nBPF_CALL_1(bpf_skb_vlan_pop, struct sk_buff *, skb)\n{\n\tint ret;\n\n\tbpf_push_mac_rcsum(skb);\n\tret = skb_vlan_pop(skb);\n\tbpf_pull_mac_rcsum(skb);\n\n\tbpf_compute_data_pointers(skb);\n\treturn ret;\n}\n\nstatic const struct bpf_func_proto bpf_skb_vlan_pop_proto = {\n\t.func           = bpf_skb_vlan_pop,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type      = ARG_PTR_TO_CTX,\n};\n\nstatic int bpf_skb_generic_push(struct sk_buff *skb, u32 off, u32 len)\n{\n\t \n\tskb_push(skb, len);\n\tmemmove(skb->data, skb->data + len, off);\n\tmemset(skb->data + off, 0, len);\n\n\t \n\treturn 0;\n}\n\nstatic int bpf_skb_generic_pop(struct sk_buff *skb, u32 off, u32 len)\n{\n\tvoid *old_data;\n\n\t \n\tif (unlikely(!pskb_may_pull(skb, off + len)))\n\t\treturn -ENOMEM;\n\n\told_data = skb->data;\n\t__skb_pull(skb, len);\n\tskb_postpull_rcsum(skb, old_data + off, len);\n\tmemmove(skb->data, old_data, off);\n\n\treturn 0;\n}\n\nstatic int bpf_skb_net_hdr_push(struct sk_buff *skb, u32 off, u32 len)\n{\n\tbool trans_same = skb->transport_header == skb->network_header;\n\tint ret;\n\n\t \n\tret = bpf_skb_generic_push(skb, off, len);\n\tif (likely(!ret)) {\n\t\tskb->mac_header -= len;\n\t\tskb->network_header -= len;\n\t\tif (trans_same)\n\t\t\tskb->transport_header = skb->network_header;\n\t}\n\n\treturn ret;\n}\n\nstatic int bpf_skb_net_hdr_pop(struct sk_buff *skb, u32 off, u32 len)\n{\n\tbool trans_same = skb->transport_header == skb->network_header;\n\tint ret;\n\n\t \n\tret = bpf_skb_generic_pop(skb, off, len);\n\tif (likely(!ret)) {\n\t\tskb->mac_header += len;\n\t\tskb->network_header += len;\n\t\tif (trans_same)\n\t\t\tskb->transport_header = skb->network_header;\n\t}\n\n\treturn ret;\n}\n\nstatic int bpf_skb_proto_4_to_6(struct sk_buff *skb)\n{\n\tconst u32 len_diff = sizeof(struct ipv6hdr) - sizeof(struct iphdr);\n\tu32 off = skb_mac_header_len(skb);\n\tint ret;\n\n\tret = skb_cow(skb, len_diff);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\tret = bpf_skb_net_hdr_push(skb, off, len_diff);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\tif (skb_is_gso(skb)) {\n\t\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\n\t\t \n\t\tif (shinfo->gso_type & SKB_GSO_TCPV4) {\n\t\t\tshinfo->gso_type &= ~SKB_GSO_TCPV4;\n\t\t\tshinfo->gso_type |=  SKB_GSO_TCPV6;\n\t\t}\n\t}\n\n\tskb->protocol = htons(ETH_P_IPV6);\n\tskb_clear_hash(skb);\n\n\treturn 0;\n}\n\nstatic int bpf_skb_proto_6_to_4(struct sk_buff *skb)\n{\n\tconst u32 len_diff = sizeof(struct ipv6hdr) - sizeof(struct iphdr);\n\tu32 off = skb_mac_header_len(skb);\n\tint ret;\n\n\tret = skb_unclone(skb, GFP_ATOMIC);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\tret = bpf_skb_net_hdr_pop(skb, off, len_diff);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\tif (skb_is_gso(skb)) {\n\t\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\n\t\t \n\t\tif (shinfo->gso_type & SKB_GSO_TCPV6) {\n\t\t\tshinfo->gso_type &= ~SKB_GSO_TCPV6;\n\t\t\tshinfo->gso_type |=  SKB_GSO_TCPV4;\n\t\t}\n\t}\n\n\tskb->protocol = htons(ETH_P_IP);\n\tskb_clear_hash(skb);\n\n\treturn 0;\n}\n\nstatic int bpf_skb_proto_xlat(struct sk_buff *skb, __be16 to_proto)\n{\n\t__be16 from_proto = skb->protocol;\n\n\tif (from_proto == htons(ETH_P_IP) &&\n\t      to_proto == htons(ETH_P_IPV6))\n\t\treturn bpf_skb_proto_4_to_6(skb);\n\n\tif (from_proto == htons(ETH_P_IPV6) &&\n\t      to_proto == htons(ETH_P_IP))\n\t\treturn bpf_skb_proto_6_to_4(skb);\n\n\treturn -ENOTSUPP;\n}\n\nBPF_CALL_3(bpf_skb_change_proto, struct sk_buff *, skb, __be16, proto,\n\t   u64, flags)\n{\n\tint ret;\n\n\tif (unlikely(flags))\n\t\treturn -EINVAL;\n\n\t \n\tret = bpf_skb_proto_xlat(skb, proto);\n\tbpf_compute_data_pointers(skb);\n\treturn ret;\n}\n\nstatic const struct bpf_func_proto bpf_skb_change_proto_proto = {\n\t.func\t\t= bpf_skb_change_proto,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_2(bpf_skb_change_type, struct sk_buff *, skb, u32, pkt_type)\n{\n\t \n\tif (unlikely(!skb_pkt_type_ok(skb->pkt_type) ||\n\t\t     !skb_pkt_type_ok(pkt_type)))\n\t\treturn -EINVAL;\n\n\tskb->pkt_type = pkt_type;\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_skb_change_type_proto = {\n\t.func\t\t= bpf_skb_change_type,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n};\n\nstatic u32 bpf_skb_net_base_len(const struct sk_buff *skb)\n{\n\tswitch (skb->protocol) {\n\tcase htons(ETH_P_IP):\n\t\treturn sizeof(struct iphdr);\n\tcase htons(ETH_P_IPV6):\n\t\treturn sizeof(struct ipv6hdr);\n\tdefault:\n\t\treturn ~0U;\n\t}\n}\n\n#define BPF_F_ADJ_ROOM_ENCAP_L3_MASK\t(BPF_F_ADJ_ROOM_ENCAP_L3_IPV4 | \\\n\t\t\t\t\t BPF_F_ADJ_ROOM_ENCAP_L3_IPV6)\n\n#define BPF_F_ADJ_ROOM_DECAP_L3_MASK\t(BPF_F_ADJ_ROOM_DECAP_L3_IPV4 | \\\n\t\t\t\t\t BPF_F_ADJ_ROOM_DECAP_L3_IPV6)\n\n#define BPF_F_ADJ_ROOM_MASK\t\t(BPF_F_ADJ_ROOM_FIXED_GSO | \\\n\t\t\t\t\t BPF_F_ADJ_ROOM_ENCAP_L3_MASK | \\\n\t\t\t\t\t BPF_F_ADJ_ROOM_ENCAP_L4_GRE | \\\n\t\t\t\t\t BPF_F_ADJ_ROOM_ENCAP_L4_UDP | \\\n\t\t\t\t\t BPF_F_ADJ_ROOM_ENCAP_L2_ETH | \\\n\t\t\t\t\t BPF_F_ADJ_ROOM_ENCAP_L2( \\\n\t\t\t\t\t  BPF_ADJ_ROOM_ENCAP_L2_MASK) | \\\n\t\t\t\t\t BPF_F_ADJ_ROOM_DECAP_L3_MASK)\n\nstatic int bpf_skb_net_grow(struct sk_buff *skb, u32 off, u32 len_diff,\n\t\t\t    u64 flags)\n{\n\tu8 inner_mac_len = flags >> BPF_ADJ_ROOM_ENCAP_L2_SHIFT;\n\tbool encap = flags & BPF_F_ADJ_ROOM_ENCAP_L3_MASK;\n\tu16 mac_len = 0, inner_net = 0, inner_trans = 0;\n\tunsigned int gso_type = SKB_GSO_DODGY;\n\tint ret;\n\n\tif (skb_is_gso(skb) && !skb_is_gso_tcp(skb)) {\n\t\t \n\t\tif (!(skb_shinfo(skb)->gso_type & SKB_GSO_UDP_L4) ||\n\t\t    !(flags & BPF_F_ADJ_ROOM_FIXED_GSO))\n\t\t\treturn -ENOTSUPP;\n\t}\n\n\tret = skb_cow_head(skb, len_diff);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\tif (encap) {\n\t\tif (skb->protocol != htons(ETH_P_IP) &&\n\t\t    skb->protocol != htons(ETH_P_IPV6))\n\t\t\treturn -ENOTSUPP;\n\n\t\tif (flags & BPF_F_ADJ_ROOM_ENCAP_L3_IPV4 &&\n\t\t    flags & BPF_F_ADJ_ROOM_ENCAP_L3_IPV6)\n\t\t\treturn -EINVAL;\n\n\t\tif (flags & BPF_F_ADJ_ROOM_ENCAP_L4_GRE &&\n\t\t    flags & BPF_F_ADJ_ROOM_ENCAP_L4_UDP)\n\t\t\treturn -EINVAL;\n\n\t\tif (flags & BPF_F_ADJ_ROOM_ENCAP_L2_ETH &&\n\t\t    inner_mac_len < ETH_HLEN)\n\t\t\treturn -EINVAL;\n\n\t\tif (skb->encapsulation)\n\t\t\treturn -EALREADY;\n\n\t\tmac_len = skb->network_header - skb->mac_header;\n\t\tinner_net = skb->network_header;\n\t\tif (inner_mac_len > len_diff)\n\t\t\treturn -EINVAL;\n\t\tinner_trans = skb->transport_header;\n\t}\n\n\tret = bpf_skb_net_hdr_push(skb, off, len_diff);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\tif (encap) {\n\t\tskb->inner_mac_header = inner_net - inner_mac_len;\n\t\tskb->inner_network_header = inner_net;\n\t\tskb->inner_transport_header = inner_trans;\n\n\t\tif (flags & BPF_F_ADJ_ROOM_ENCAP_L2_ETH)\n\t\t\tskb_set_inner_protocol(skb, htons(ETH_P_TEB));\n\t\telse\n\t\t\tskb_set_inner_protocol(skb, skb->protocol);\n\n\t\tskb->encapsulation = 1;\n\t\tskb_set_network_header(skb, mac_len);\n\n\t\tif (flags & BPF_F_ADJ_ROOM_ENCAP_L4_UDP)\n\t\t\tgso_type |= SKB_GSO_UDP_TUNNEL;\n\t\telse if (flags & BPF_F_ADJ_ROOM_ENCAP_L4_GRE)\n\t\t\tgso_type |= SKB_GSO_GRE;\n\t\telse if (flags & BPF_F_ADJ_ROOM_ENCAP_L3_IPV6)\n\t\t\tgso_type |= SKB_GSO_IPXIP6;\n\t\telse if (flags & BPF_F_ADJ_ROOM_ENCAP_L3_IPV4)\n\t\t\tgso_type |= SKB_GSO_IPXIP4;\n\n\t\tif (flags & BPF_F_ADJ_ROOM_ENCAP_L4_GRE ||\n\t\t    flags & BPF_F_ADJ_ROOM_ENCAP_L4_UDP) {\n\t\t\tint nh_len = flags & BPF_F_ADJ_ROOM_ENCAP_L3_IPV6 ?\n\t\t\t\t\tsizeof(struct ipv6hdr) :\n\t\t\t\t\tsizeof(struct iphdr);\n\n\t\t\tskb_set_transport_header(skb, mac_len + nh_len);\n\t\t}\n\n\t\t \n\t\tif (skb->protocol == htons(ETH_P_IP) &&\n\t\t    flags & BPF_F_ADJ_ROOM_ENCAP_L3_IPV6)\n\t\t\tskb->protocol = htons(ETH_P_IPV6);\n\t\telse if (skb->protocol == htons(ETH_P_IPV6) &&\n\t\t\t flags & BPF_F_ADJ_ROOM_ENCAP_L3_IPV4)\n\t\t\tskb->protocol = htons(ETH_P_IP);\n\t}\n\n\tif (skb_is_gso(skb)) {\n\t\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\n\t\t \n\t\tif (!(flags & BPF_F_ADJ_ROOM_FIXED_GSO))\n\t\t\tskb_decrease_gso_size(shinfo, len_diff);\n\n\t\t \n\t\tshinfo->gso_type |= gso_type;\n\t\tshinfo->gso_segs = 0;\n\t}\n\n\treturn 0;\n}\n\nstatic int bpf_skb_net_shrink(struct sk_buff *skb, u32 off, u32 len_diff,\n\t\t\t      u64 flags)\n{\n\tint ret;\n\n\tif (unlikely(flags & ~(BPF_F_ADJ_ROOM_FIXED_GSO |\n\t\t\t       BPF_F_ADJ_ROOM_DECAP_L3_MASK |\n\t\t\t       BPF_F_ADJ_ROOM_NO_CSUM_RESET)))\n\t\treturn -EINVAL;\n\n\tif (skb_is_gso(skb) && !skb_is_gso_tcp(skb)) {\n\t\t \n\t\tif (!(skb_shinfo(skb)->gso_type & SKB_GSO_UDP_L4) ||\n\t\t    !(flags & BPF_F_ADJ_ROOM_FIXED_GSO))\n\t\t\treturn -ENOTSUPP;\n\t}\n\n\tret = skb_unclone(skb, GFP_ATOMIC);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\tret = bpf_skb_net_hdr_pop(skb, off, len_diff);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\t \n\tif (skb->protocol == htons(ETH_P_IP) &&\n\t    flags & BPF_F_ADJ_ROOM_DECAP_L3_IPV6)\n\t\tskb->protocol = htons(ETH_P_IPV6);\n\telse if (skb->protocol == htons(ETH_P_IPV6) &&\n\t\t flags & BPF_F_ADJ_ROOM_DECAP_L3_IPV4)\n\t\tskb->protocol = htons(ETH_P_IP);\n\n\tif (skb_is_gso(skb)) {\n\t\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\n\t\t \n\t\tif (!(flags & BPF_F_ADJ_ROOM_FIXED_GSO))\n\t\t\tskb_increase_gso_size(shinfo, len_diff);\n\n\t\t \n\t\tshinfo->gso_type |= SKB_GSO_DODGY;\n\t\tshinfo->gso_segs = 0;\n\t}\n\n\treturn 0;\n}\n\n#define BPF_SKB_MAX_LEN SKB_MAX_ALLOC\n\nBPF_CALL_4(sk_skb_adjust_room, struct sk_buff *, skb, s32, len_diff,\n\t   u32, mode, u64, flags)\n{\n\tu32 len_diff_abs = abs(len_diff);\n\tbool shrink = len_diff < 0;\n\tint ret = 0;\n\n\tif (unlikely(flags || mode))\n\t\treturn -EINVAL;\n\tif (unlikely(len_diff_abs > 0xfffU))\n\t\treturn -EFAULT;\n\n\tif (!shrink) {\n\t\tret = skb_cow(skb, len_diff);\n\t\tif (unlikely(ret < 0))\n\t\t\treturn ret;\n\t\t__skb_push(skb, len_diff_abs);\n\t\tmemset(skb->data, 0, len_diff_abs);\n\t} else {\n\t\tif (unlikely(!pskb_may_pull(skb, len_diff_abs)))\n\t\t\treturn -ENOMEM;\n\t\t__skb_pull(skb, len_diff_abs);\n\t}\n\tif (tls_sw_has_ctx_rx(skb->sk)) {\n\t\tstruct strp_msg *rxm = strp_msg(skb);\n\n\t\trxm->full_len += len_diff;\n\t}\n\treturn ret;\n}\n\nstatic const struct bpf_func_proto sk_skb_adjust_room_proto = {\n\t.func\t\t= sk_skb_adjust_room,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_4(bpf_skb_adjust_room, struct sk_buff *, skb, s32, len_diff,\n\t   u32, mode, u64, flags)\n{\n\tu32 len_cur, len_diff_abs = abs(len_diff);\n\tu32 len_min = bpf_skb_net_base_len(skb);\n\tu32 len_max = BPF_SKB_MAX_LEN;\n\t__be16 proto = skb->protocol;\n\tbool shrink = len_diff < 0;\n\tu32 off;\n\tint ret;\n\n\tif (unlikely(flags & ~(BPF_F_ADJ_ROOM_MASK |\n\t\t\t       BPF_F_ADJ_ROOM_NO_CSUM_RESET)))\n\t\treturn -EINVAL;\n\tif (unlikely(len_diff_abs > 0xfffU))\n\t\treturn -EFAULT;\n\tif (unlikely(proto != htons(ETH_P_IP) &&\n\t\t     proto != htons(ETH_P_IPV6)))\n\t\treturn -ENOTSUPP;\n\n\toff = skb_mac_header_len(skb);\n\tswitch (mode) {\n\tcase BPF_ADJ_ROOM_NET:\n\t\toff += bpf_skb_net_base_len(skb);\n\t\tbreak;\n\tcase BPF_ADJ_ROOM_MAC:\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOTSUPP;\n\t}\n\n\tif (flags & BPF_F_ADJ_ROOM_DECAP_L3_MASK) {\n\t\tif (!shrink)\n\t\t\treturn -EINVAL;\n\n\t\tswitch (flags & BPF_F_ADJ_ROOM_DECAP_L3_MASK) {\n\t\tcase BPF_F_ADJ_ROOM_DECAP_L3_IPV4:\n\t\t\tlen_min = sizeof(struct iphdr);\n\t\t\tbreak;\n\t\tcase BPF_F_ADJ_ROOM_DECAP_L3_IPV6:\n\t\t\tlen_min = sizeof(struct ipv6hdr);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tlen_cur = skb->len - skb_network_offset(skb);\n\tif ((shrink && (len_diff_abs >= len_cur ||\n\t\t\tlen_cur - len_diff_abs < len_min)) ||\n\t    (!shrink && (skb->len + len_diff_abs > len_max &&\n\t\t\t !skb_is_gso(skb))))\n\t\treturn -ENOTSUPP;\n\n\tret = shrink ? bpf_skb_net_shrink(skb, off, len_diff_abs, flags) :\n\t\t       bpf_skb_net_grow(skb, off, len_diff_abs, flags);\n\tif (!ret && !(flags & BPF_F_ADJ_ROOM_NO_CSUM_RESET))\n\t\t__skb_reset_checksum_unnecessary(skb);\n\n\tbpf_compute_data_pointers(skb);\n\treturn ret;\n}\n\nstatic const struct bpf_func_proto bpf_skb_adjust_room_proto = {\n\t.func\t\t= bpf_skb_adjust_room,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_ANYTHING,\n};\n\nstatic u32 __bpf_skb_min_len(const struct sk_buff *skb)\n{\n\tu32 min_len = skb_network_offset(skb);\n\n\tif (skb_transport_header_was_set(skb))\n\t\tmin_len = skb_transport_offset(skb);\n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\tmin_len = skb_checksum_start_offset(skb) +\n\t\t\t  skb->csum_offset + sizeof(__sum16);\n\treturn min_len;\n}\n\nstatic int bpf_skb_grow_rcsum(struct sk_buff *skb, unsigned int new_len)\n{\n\tunsigned int old_len = skb->len;\n\tint ret;\n\n\tret = __skb_grow_rcsum(skb, new_len);\n\tif (!ret)\n\t\tmemset(skb->data + old_len, 0, new_len - old_len);\n\treturn ret;\n}\n\nstatic int bpf_skb_trim_rcsum(struct sk_buff *skb, unsigned int new_len)\n{\n\treturn __skb_trim_rcsum(skb, new_len);\n}\n\nstatic inline int __bpf_skb_change_tail(struct sk_buff *skb, u32 new_len,\n\t\t\t\t\tu64 flags)\n{\n\tu32 max_len = BPF_SKB_MAX_LEN;\n\tu32 min_len = __bpf_skb_min_len(skb);\n\tint ret;\n\n\tif (unlikely(flags || new_len > max_len || new_len < min_len))\n\t\treturn -EINVAL;\n\tif (skb->encapsulation)\n\t\treturn -ENOTSUPP;\n\n\t \n\tret = __bpf_try_make_writable(skb, skb->len);\n\tif (!ret) {\n\t\tif (new_len > skb->len)\n\t\t\tret = bpf_skb_grow_rcsum(skb, new_len);\n\t\telse if (new_len < skb->len)\n\t\t\tret = bpf_skb_trim_rcsum(skb, new_len);\n\t\tif (!ret && skb_is_gso(skb))\n\t\t\tskb_gso_reset(skb);\n\t}\n\treturn ret;\n}\n\nBPF_CALL_3(bpf_skb_change_tail, struct sk_buff *, skb, u32, new_len,\n\t   u64, flags)\n{\n\tint ret = __bpf_skb_change_tail(skb, new_len, flags);\n\n\tbpf_compute_data_pointers(skb);\n\treturn ret;\n}\n\nstatic const struct bpf_func_proto bpf_skb_change_tail_proto = {\n\t.func\t\t= bpf_skb_change_tail,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_3(sk_skb_change_tail, struct sk_buff *, skb, u32, new_len,\n\t   u64, flags)\n{\n\treturn __bpf_skb_change_tail(skb, new_len, flags);\n}\n\nstatic const struct bpf_func_proto sk_skb_change_tail_proto = {\n\t.func\t\t= sk_skb_change_tail,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n};\n\nstatic inline int __bpf_skb_change_head(struct sk_buff *skb, u32 head_room,\n\t\t\t\t\tu64 flags)\n{\n\tu32 max_len = BPF_SKB_MAX_LEN;\n\tu32 new_len = skb->len + head_room;\n\tint ret;\n\n\tif (unlikely(flags || (!skb_is_gso(skb) && new_len > max_len) ||\n\t\t     new_len < skb->len))\n\t\treturn -EINVAL;\n\n\tret = skb_cow(skb, head_room);\n\tif (likely(!ret)) {\n\t\t \n\t\t__skb_push(skb, head_room);\n\t\tmemset(skb->data, 0, head_room);\n\t\tskb_reset_mac_header(skb);\n\t\tskb_reset_mac_len(skb);\n\t}\n\n\treturn ret;\n}\n\nBPF_CALL_3(bpf_skb_change_head, struct sk_buff *, skb, u32, head_room,\n\t   u64, flags)\n{\n\tint ret = __bpf_skb_change_head(skb, head_room, flags);\n\n\tbpf_compute_data_pointers(skb);\n\treturn ret;\n}\n\nstatic const struct bpf_func_proto bpf_skb_change_head_proto = {\n\t.func\t\t= bpf_skb_change_head,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_3(sk_skb_change_head, struct sk_buff *, skb, u32, head_room,\n\t   u64, flags)\n{\n\treturn __bpf_skb_change_head(skb, head_room, flags);\n}\n\nstatic const struct bpf_func_proto sk_skb_change_head_proto = {\n\t.func\t\t= sk_skb_change_head,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_1(bpf_xdp_get_buff_len, struct xdp_buff*, xdp)\n{\n\treturn xdp_get_buff_len(xdp);\n}\n\nstatic const struct bpf_func_proto bpf_xdp_get_buff_len_proto = {\n\t.func\t\t= bpf_xdp_get_buff_len,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n};\n\nBTF_ID_LIST_SINGLE(bpf_xdp_get_buff_len_bpf_ids, struct, xdp_buff)\n\nconst struct bpf_func_proto bpf_xdp_get_buff_len_trace_proto = {\n\t.func\t\t= bpf_xdp_get_buff_len,\n\t.gpl_only\t= false,\n\t.arg1_type\t= ARG_PTR_TO_BTF_ID,\n\t.arg1_btf_id\t= &bpf_xdp_get_buff_len_bpf_ids[0],\n};\n\nstatic unsigned long xdp_get_metalen(const struct xdp_buff *xdp)\n{\n\treturn xdp_data_meta_unsupported(xdp) ? 0 :\n\t       xdp->data - xdp->data_meta;\n}\n\nBPF_CALL_2(bpf_xdp_adjust_head, struct xdp_buff *, xdp, int, offset)\n{\n\tvoid *xdp_frame_end = xdp->data_hard_start + sizeof(struct xdp_frame);\n\tunsigned long metalen = xdp_get_metalen(xdp);\n\tvoid *data_start = xdp_frame_end + metalen;\n\tvoid *data = xdp->data + offset;\n\n\tif (unlikely(data < data_start ||\n\t\t     data > xdp->data_end - ETH_HLEN))\n\t\treturn -EINVAL;\n\n\tif (metalen)\n\t\tmemmove(xdp->data_meta + offset,\n\t\t\txdp->data_meta, metalen);\n\txdp->data_meta += offset;\n\txdp->data = data;\n\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_xdp_adjust_head_proto = {\n\t.func\t\t= bpf_xdp_adjust_head,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n};\n\nvoid bpf_xdp_copy_buf(struct xdp_buff *xdp, unsigned long off,\n\t\t      void *buf, unsigned long len, bool flush)\n{\n\tunsigned long ptr_len, ptr_off = 0;\n\tskb_frag_t *next_frag, *end_frag;\n\tstruct skb_shared_info *sinfo;\n\tvoid *src, *dst;\n\tu8 *ptr_buf;\n\n\tif (likely(xdp->data_end - xdp->data >= off + len)) {\n\t\tsrc = flush ? buf : xdp->data + off;\n\t\tdst = flush ? xdp->data + off : buf;\n\t\tmemcpy(dst, src, len);\n\t\treturn;\n\t}\n\n\tsinfo = xdp_get_shared_info_from_buff(xdp);\n\tend_frag = &sinfo->frags[sinfo->nr_frags];\n\tnext_frag = &sinfo->frags[0];\n\n\tptr_len = xdp->data_end - xdp->data;\n\tptr_buf = xdp->data;\n\n\twhile (true) {\n\t\tif (off < ptr_off + ptr_len) {\n\t\t\tunsigned long copy_off = off - ptr_off;\n\t\t\tunsigned long copy_len = min(len, ptr_len - copy_off);\n\n\t\t\tsrc = flush ? buf : ptr_buf + copy_off;\n\t\t\tdst = flush ? ptr_buf + copy_off : buf;\n\t\t\tmemcpy(dst, src, copy_len);\n\n\t\t\toff += copy_len;\n\t\t\tlen -= copy_len;\n\t\t\tbuf += copy_len;\n\t\t}\n\n\t\tif (!len || next_frag == end_frag)\n\t\t\tbreak;\n\n\t\tptr_off += ptr_len;\n\t\tptr_buf = skb_frag_address(next_frag);\n\t\tptr_len = skb_frag_size(next_frag);\n\t\tnext_frag++;\n\t}\n}\n\nvoid *bpf_xdp_pointer(struct xdp_buff *xdp, u32 offset, u32 len)\n{\n\tu32 size = xdp->data_end - xdp->data;\n\tstruct skb_shared_info *sinfo;\n\tvoid *addr = xdp->data;\n\tint i;\n\n\tif (unlikely(offset > 0xffff || len > 0xffff))\n\t\treturn ERR_PTR(-EFAULT);\n\n\tif (unlikely(offset + len > xdp_get_buff_len(xdp)))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (likely(offset < size))  \n\t\tgoto out;\n\n\tsinfo = xdp_get_shared_info_from_buff(xdp);\n\toffset -= size;\n\tfor (i = 0; i < sinfo->nr_frags; i++) {  \n\t\tu32 frag_size = skb_frag_size(&sinfo->frags[i]);\n\n\t\tif  (offset < frag_size) {\n\t\t\taddr = skb_frag_address(&sinfo->frags[i]);\n\t\t\tsize = frag_size;\n\t\t\tbreak;\n\t\t}\n\t\toffset -= frag_size;\n\t}\nout:\n\treturn offset + len <= size ? addr + offset : NULL;\n}\n\nBPF_CALL_4(bpf_xdp_load_bytes, struct xdp_buff *, xdp, u32, offset,\n\t   void *, buf, u32, len)\n{\n\tvoid *ptr;\n\n\tptr = bpf_xdp_pointer(xdp, offset, len);\n\tif (IS_ERR(ptr))\n\t\treturn PTR_ERR(ptr);\n\n\tif (!ptr)\n\t\tbpf_xdp_copy_buf(xdp, offset, buf, len, false);\n\telse\n\t\tmemcpy(buf, ptr, len);\n\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_xdp_load_bytes_proto = {\n\t.func\t\t= bpf_xdp_load_bytes,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg4_type\t= ARG_CONST_SIZE,\n};\n\nint __bpf_xdp_load_bytes(struct xdp_buff *xdp, u32 offset, void *buf, u32 len)\n{\n\treturn ____bpf_xdp_load_bytes(xdp, offset, buf, len);\n}\n\nBPF_CALL_4(bpf_xdp_store_bytes, struct xdp_buff *, xdp, u32, offset,\n\t   void *, buf, u32, len)\n{\n\tvoid *ptr;\n\n\tptr = bpf_xdp_pointer(xdp, offset, len);\n\tif (IS_ERR(ptr))\n\t\treturn PTR_ERR(ptr);\n\n\tif (!ptr)\n\t\tbpf_xdp_copy_buf(xdp, offset, buf, len, true);\n\telse\n\t\tmemcpy(ptr, buf, len);\n\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_xdp_store_bytes_proto = {\n\t.func\t\t= bpf_xdp_store_bytes,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg4_type\t= ARG_CONST_SIZE,\n};\n\nint __bpf_xdp_store_bytes(struct xdp_buff *xdp, u32 offset, void *buf, u32 len)\n{\n\treturn ____bpf_xdp_store_bytes(xdp, offset, buf, len);\n}\n\nstatic int bpf_xdp_frags_increase_tail(struct xdp_buff *xdp, int offset)\n{\n\tstruct skb_shared_info *sinfo = xdp_get_shared_info_from_buff(xdp);\n\tskb_frag_t *frag = &sinfo->frags[sinfo->nr_frags - 1];\n\tstruct xdp_rxq_info *rxq = xdp->rxq;\n\tunsigned int tailroom;\n\n\tif (!rxq->frag_size || rxq->frag_size > xdp->frame_sz)\n\t\treturn -EOPNOTSUPP;\n\n\ttailroom = rxq->frag_size - skb_frag_size(frag) - skb_frag_off(frag);\n\tif (unlikely(offset > tailroom))\n\t\treturn -EINVAL;\n\n\tmemset(skb_frag_address(frag) + skb_frag_size(frag), 0, offset);\n\tskb_frag_size_add(frag, offset);\n\tsinfo->xdp_frags_size += offset;\n\n\treturn 0;\n}\n\nstatic int bpf_xdp_frags_shrink_tail(struct xdp_buff *xdp, int offset)\n{\n\tstruct skb_shared_info *sinfo = xdp_get_shared_info_from_buff(xdp);\n\tint i, n_frags_free = 0, len_free = 0;\n\n\tif (unlikely(offset > (int)xdp_get_buff_len(xdp) - ETH_HLEN))\n\t\treturn -EINVAL;\n\n\tfor (i = sinfo->nr_frags - 1; i >= 0 && offset > 0; i--) {\n\t\tskb_frag_t *frag = &sinfo->frags[i];\n\t\tint shrink = min_t(int, offset, skb_frag_size(frag));\n\n\t\tlen_free += shrink;\n\t\toffset -= shrink;\n\n\t\tif (skb_frag_size(frag) == shrink) {\n\t\t\tstruct page *page = skb_frag_page(frag);\n\n\t\t\t__xdp_return(page_address(page), &xdp->rxq->mem,\n\t\t\t\t     false, NULL);\n\t\t\tn_frags_free++;\n\t\t} else {\n\t\t\tskb_frag_size_sub(frag, shrink);\n\t\t\tbreak;\n\t\t}\n\t}\n\tsinfo->nr_frags -= n_frags_free;\n\tsinfo->xdp_frags_size -= len_free;\n\n\tif (unlikely(!sinfo->nr_frags)) {\n\t\txdp_buff_clear_frags_flag(xdp);\n\t\txdp->data_end -= offset;\n\t}\n\n\treturn 0;\n}\n\nBPF_CALL_2(bpf_xdp_adjust_tail, struct xdp_buff *, xdp, int, offset)\n{\n\tvoid *data_hard_end = xdp_data_hard_end(xdp);  \n\tvoid *data_end = xdp->data_end + offset;\n\n\tif (unlikely(xdp_buff_has_frags(xdp))) {  \n\t\tif (offset < 0)\n\t\t\treturn bpf_xdp_frags_shrink_tail(xdp, -offset);\n\n\t\treturn bpf_xdp_frags_increase_tail(xdp, offset);\n\t}\n\n\t \n\tif (unlikely(data_end > data_hard_end))\n\t\treturn -EINVAL;\n\n\tif (unlikely(data_end < xdp->data + ETH_HLEN))\n\t\treturn -EINVAL;\n\n\t \n\tif (offset > 0)\n\t\tmemset(xdp->data_end, 0, offset);\n\n\txdp->data_end = data_end;\n\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_xdp_adjust_tail_proto = {\n\t.func\t\t= bpf_xdp_adjust_tail,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_2(bpf_xdp_adjust_meta, struct xdp_buff *, xdp, int, offset)\n{\n\tvoid *xdp_frame_end = xdp->data_hard_start + sizeof(struct xdp_frame);\n\tvoid *meta = xdp->data_meta + offset;\n\tunsigned long metalen = xdp->data - meta;\n\n\tif (xdp_data_meta_unsupported(xdp))\n\t\treturn -ENOTSUPP;\n\tif (unlikely(meta < xdp_frame_end ||\n\t\t     meta > xdp->data))\n\t\treturn -EINVAL;\n\tif (unlikely(xdp_metalen_invalid(metalen)))\n\t\treturn -EACCES;\n\n\txdp->data_meta = meta;\n\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_xdp_adjust_meta_proto = {\n\t.func\t\t= bpf_xdp_adjust_meta,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n};\n\n \n \nvoid xdp_do_flush(void)\n{\n\t__dev_flush();\n\t__cpu_map_flush();\n\t__xsk_map_flush();\n}\nEXPORT_SYMBOL_GPL(xdp_do_flush);\n\nvoid bpf_clear_redirect_map(struct bpf_map *map)\n{\n\tstruct bpf_redirect_info *ri;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tri = per_cpu_ptr(&bpf_redirect_info, cpu);\n\t\t \n\t\tif (unlikely(READ_ONCE(ri->map) == map))\n\t\t\tcmpxchg(&ri->map, map, NULL);\n\t}\n}\n\nDEFINE_STATIC_KEY_FALSE(bpf_master_redirect_enabled_key);\nEXPORT_SYMBOL_GPL(bpf_master_redirect_enabled_key);\n\nu32 xdp_master_redirect(struct xdp_buff *xdp)\n{\n\tstruct net_device *master, *slave;\n\tstruct bpf_redirect_info *ri = this_cpu_ptr(&bpf_redirect_info);\n\n\tmaster = netdev_master_upper_dev_get_rcu(xdp->rxq->dev);\n\tslave = master->netdev_ops->ndo_xdp_get_xmit_slave(master, xdp);\n\tif (slave && slave != xdp->rxq->dev) {\n\t\t \n\t\tri->tgt_index = slave->ifindex;\n\t\tri->map_id = INT_MAX;\n\t\tri->map_type = BPF_MAP_TYPE_UNSPEC;\n\t\treturn XDP_REDIRECT;\n\t}\n\treturn XDP_TX;\n}\nEXPORT_SYMBOL_GPL(xdp_master_redirect);\n\nstatic inline int __xdp_do_redirect_xsk(struct bpf_redirect_info *ri,\n\t\t\t\t\tstruct net_device *dev,\n\t\t\t\t\tstruct xdp_buff *xdp,\n\t\t\t\t\tstruct bpf_prog *xdp_prog)\n{\n\tenum bpf_map_type map_type = ri->map_type;\n\tvoid *fwd = ri->tgt_value;\n\tu32 map_id = ri->map_id;\n\tint err;\n\n\tri->map_id = 0;  \n\tri->map_type = BPF_MAP_TYPE_UNSPEC;\n\n\terr = __xsk_map_redirect(fwd, xdp);\n\tif (unlikely(err))\n\t\tgoto err;\n\n\t_trace_xdp_redirect_map(dev, xdp_prog, fwd, map_type, map_id, ri->tgt_index);\n\treturn 0;\nerr:\n\t_trace_xdp_redirect_map_err(dev, xdp_prog, fwd, map_type, map_id, ri->tgt_index, err);\n\treturn err;\n}\n\nstatic __always_inline int __xdp_do_redirect_frame(struct bpf_redirect_info *ri,\n\t\t\t\t\t\t   struct net_device *dev,\n\t\t\t\t\t\t   struct xdp_frame *xdpf,\n\t\t\t\t\t\t   struct bpf_prog *xdp_prog)\n{\n\tenum bpf_map_type map_type = ri->map_type;\n\tvoid *fwd = ri->tgt_value;\n\tu32 map_id = ri->map_id;\n\tstruct bpf_map *map;\n\tint err;\n\n\tri->map_id = 0;  \n\tri->map_type = BPF_MAP_TYPE_UNSPEC;\n\n\tif (unlikely(!xdpf)) {\n\t\terr = -EOVERFLOW;\n\t\tgoto err;\n\t}\n\n\tswitch (map_type) {\n\tcase BPF_MAP_TYPE_DEVMAP:\n\t\tfallthrough;\n\tcase BPF_MAP_TYPE_DEVMAP_HASH:\n\t\tmap = READ_ONCE(ri->map);\n\t\tif (unlikely(map)) {\n\t\t\tWRITE_ONCE(ri->map, NULL);\n\t\t\terr = dev_map_enqueue_multi(xdpf, dev, map,\n\t\t\t\t\t\t    ri->flags & BPF_F_EXCLUDE_INGRESS);\n\t\t} else {\n\t\t\terr = dev_map_enqueue(fwd, xdpf, dev);\n\t\t}\n\t\tbreak;\n\tcase BPF_MAP_TYPE_CPUMAP:\n\t\terr = cpu_map_enqueue(fwd, xdpf, dev);\n\t\tbreak;\n\tcase BPF_MAP_TYPE_UNSPEC:\n\t\tif (map_id == INT_MAX) {\n\t\t\tfwd = dev_get_by_index_rcu(dev_net(dev), ri->tgt_index);\n\t\t\tif (unlikely(!fwd)) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\terr = dev_xdp_enqueue(fwd, xdpf, dev);\n\t\t\tbreak;\n\t\t}\n\t\tfallthrough;\n\tdefault:\n\t\terr = -EBADRQC;\n\t}\n\n\tif (unlikely(err))\n\t\tgoto err;\n\n\t_trace_xdp_redirect_map(dev, xdp_prog, fwd, map_type, map_id, ri->tgt_index);\n\treturn 0;\nerr:\n\t_trace_xdp_redirect_map_err(dev, xdp_prog, fwd, map_type, map_id, ri->tgt_index, err);\n\treturn err;\n}\n\nint xdp_do_redirect(struct net_device *dev, struct xdp_buff *xdp,\n\t\t    struct bpf_prog *xdp_prog)\n{\n\tstruct bpf_redirect_info *ri = this_cpu_ptr(&bpf_redirect_info);\n\tenum bpf_map_type map_type = ri->map_type;\n\n\tif (map_type == BPF_MAP_TYPE_XSKMAP)\n\t\treturn __xdp_do_redirect_xsk(ri, dev, xdp, xdp_prog);\n\n\treturn __xdp_do_redirect_frame(ri, dev, xdp_convert_buff_to_frame(xdp),\n\t\t\t\t       xdp_prog);\n}\nEXPORT_SYMBOL_GPL(xdp_do_redirect);\n\nint xdp_do_redirect_frame(struct net_device *dev, struct xdp_buff *xdp,\n\t\t\t  struct xdp_frame *xdpf, struct bpf_prog *xdp_prog)\n{\n\tstruct bpf_redirect_info *ri = this_cpu_ptr(&bpf_redirect_info);\n\tenum bpf_map_type map_type = ri->map_type;\n\n\tif (map_type == BPF_MAP_TYPE_XSKMAP)\n\t\treturn __xdp_do_redirect_xsk(ri, dev, xdp, xdp_prog);\n\n\treturn __xdp_do_redirect_frame(ri, dev, xdpf, xdp_prog);\n}\nEXPORT_SYMBOL_GPL(xdp_do_redirect_frame);\n\nstatic int xdp_do_generic_redirect_map(struct net_device *dev,\n\t\t\t\t       struct sk_buff *skb,\n\t\t\t\t       struct xdp_buff *xdp,\n\t\t\t\t       struct bpf_prog *xdp_prog,\n\t\t\t\t       void *fwd,\n\t\t\t\t       enum bpf_map_type map_type, u32 map_id)\n{\n\tstruct bpf_redirect_info *ri = this_cpu_ptr(&bpf_redirect_info);\n\tstruct bpf_map *map;\n\tint err;\n\n\tswitch (map_type) {\n\tcase BPF_MAP_TYPE_DEVMAP:\n\t\tfallthrough;\n\tcase BPF_MAP_TYPE_DEVMAP_HASH:\n\t\tmap = READ_ONCE(ri->map);\n\t\tif (unlikely(map)) {\n\t\t\tWRITE_ONCE(ri->map, NULL);\n\t\t\terr = dev_map_redirect_multi(dev, skb, xdp_prog, map,\n\t\t\t\t\t\t     ri->flags & BPF_F_EXCLUDE_INGRESS);\n\t\t} else {\n\t\t\terr = dev_map_generic_redirect(fwd, skb, xdp_prog);\n\t\t}\n\t\tif (unlikely(err))\n\t\t\tgoto err;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_XSKMAP:\n\t\terr = xsk_generic_rcv(fwd, xdp);\n\t\tif (err)\n\t\t\tgoto err;\n\t\tconsume_skb(skb);\n\t\tbreak;\n\tcase BPF_MAP_TYPE_CPUMAP:\n\t\terr = cpu_map_generic_redirect(fwd, skb);\n\t\tif (unlikely(err))\n\t\t\tgoto err;\n\t\tbreak;\n\tdefault:\n\t\terr = -EBADRQC;\n\t\tgoto err;\n\t}\n\n\t_trace_xdp_redirect_map(dev, xdp_prog, fwd, map_type, map_id, ri->tgt_index);\n\treturn 0;\nerr:\n\t_trace_xdp_redirect_map_err(dev, xdp_prog, fwd, map_type, map_id, ri->tgt_index, err);\n\treturn err;\n}\n\nint xdp_do_generic_redirect(struct net_device *dev, struct sk_buff *skb,\n\t\t\t    struct xdp_buff *xdp, struct bpf_prog *xdp_prog)\n{\n\tstruct bpf_redirect_info *ri = this_cpu_ptr(&bpf_redirect_info);\n\tenum bpf_map_type map_type = ri->map_type;\n\tvoid *fwd = ri->tgt_value;\n\tu32 map_id = ri->map_id;\n\tint err;\n\n\tri->map_id = 0;  \n\tri->map_type = BPF_MAP_TYPE_UNSPEC;\n\n\tif (map_type == BPF_MAP_TYPE_UNSPEC && map_id == INT_MAX) {\n\t\tfwd = dev_get_by_index_rcu(dev_net(dev), ri->tgt_index);\n\t\tif (unlikely(!fwd)) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto err;\n\t\t}\n\n\t\terr = xdp_ok_fwd_dev(fwd, skb->len);\n\t\tif (unlikely(err))\n\t\t\tgoto err;\n\n\t\tskb->dev = fwd;\n\t\t_trace_xdp_redirect(dev, xdp_prog, ri->tgt_index);\n\t\tgeneric_xdp_tx(skb, xdp_prog);\n\t\treturn 0;\n\t}\n\n\treturn xdp_do_generic_redirect_map(dev, skb, xdp, xdp_prog, fwd, map_type, map_id);\nerr:\n\t_trace_xdp_redirect_err(dev, xdp_prog, ri->tgt_index, err);\n\treturn err;\n}\n\nBPF_CALL_2(bpf_xdp_redirect, u32, ifindex, u64, flags)\n{\n\tstruct bpf_redirect_info *ri = this_cpu_ptr(&bpf_redirect_info);\n\n\tif (unlikely(flags))\n\t\treturn XDP_ABORTED;\n\n\t \n\tri->tgt_index = ifindex;\n\tri->map_id = INT_MAX;\n\tri->map_type = BPF_MAP_TYPE_UNSPEC;\n\n\treturn XDP_REDIRECT;\n}\n\nstatic const struct bpf_func_proto bpf_xdp_redirect_proto = {\n\t.func           = bpf_xdp_redirect,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type      = ARG_ANYTHING,\n\t.arg2_type      = ARG_ANYTHING,\n};\n\nBPF_CALL_3(bpf_xdp_redirect_map, struct bpf_map *, map, u64, key,\n\t   u64, flags)\n{\n\treturn map->ops->map_redirect(map, key, flags);\n}\n\nstatic const struct bpf_func_proto bpf_xdp_redirect_map_proto = {\n\t.func           = bpf_xdp_redirect_map,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type      = ARG_CONST_MAP_PTR,\n\t.arg2_type      = ARG_ANYTHING,\n\t.arg3_type      = ARG_ANYTHING,\n};\n\nstatic unsigned long bpf_skb_copy(void *dst_buff, const void *skb,\n\t\t\t\t  unsigned long off, unsigned long len)\n{\n\tvoid *ptr = skb_header_pointer(skb, off, len, dst_buff);\n\n\tif (unlikely(!ptr))\n\t\treturn len;\n\tif (ptr != dst_buff)\n\t\tmemcpy(dst_buff, ptr, len);\n\n\treturn 0;\n}\n\nBPF_CALL_5(bpf_skb_event_output, struct sk_buff *, skb, struct bpf_map *, map,\n\t   u64, flags, void *, meta, u64, meta_size)\n{\n\tu64 skb_size = (flags & BPF_F_CTXLEN_MASK) >> 32;\n\n\tif (unlikely(flags & ~(BPF_F_CTXLEN_MASK | BPF_F_INDEX_MASK)))\n\t\treturn -EINVAL;\n\tif (unlikely(!skb || skb_size > skb->len))\n\t\treturn -EFAULT;\n\n\treturn bpf_event_output(map, flags, meta, meta_size, skb, skb_size,\n\t\t\t\tbpf_skb_copy);\n}\n\nstatic const struct bpf_func_proto bpf_skb_event_output_proto = {\n\t.func\t\t= bpf_skb_event_output,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_CONST_MAP_PTR,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg5_type\t= ARG_CONST_SIZE_OR_ZERO,\n};\n\nBTF_ID_LIST_SINGLE(bpf_skb_output_btf_ids, struct, sk_buff)\n\nconst struct bpf_func_proto bpf_skb_output_proto = {\n\t.func\t\t= bpf_skb_event_output,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_BTF_ID,\n\t.arg1_btf_id\t= &bpf_skb_output_btf_ids[0],\n\t.arg2_type\t= ARG_CONST_MAP_PTR,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg5_type\t= ARG_CONST_SIZE_OR_ZERO,\n};\n\nstatic unsigned short bpf_tunnel_key_af(u64 flags)\n{\n\treturn flags & BPF_F_TUNINFO_IPV6 ? AF_INET6 : AF_INET;\n}\n\nBPF_CALL_4(bpf_skb_get_tunnel_key, struct sk_buff *, skb, struct bpf_tunnel_key *, to,\n\t   u32, size, u64, flags)\n{\n\tconst struct ip_tunnel_info *info = skb_tunnel_info(skb);\n\tu8 compat[sizeof(struct bpf_tunnel_key)];\n\tvoid *to_orig = to;\n\tint err;\n\n\tif (unlikely(!info || (flags & ~(BPF_F_TUNINFO_IPV6 |\n\t\t\t\t\t BPF_F_TUNINFO_FLAGS)))) {\n\t\terr = -EINVAL;\n\t\tgoto err_clear;\n\t}\n\tif (ip_tunnel_info_af(info) != bpf_tunnel_key_af(flags)) {\n\t\terr = -EPROTO;\n\t\tgoto err_clear;\n\t}\n\tif (unlikely(size != sizeof(struct bpf_tunnel_key))) {\n\t\terr = -EINVAL;\n\t\tswitch (size) {\n\t\tcase offsetof(struct bpf_tunnel_key, local_ipv6[0]):\n\t\tcase offsetof(struct bpf_tunnel_key, tunnel_label):\n\t\tcase offsetof(struct bpf_tunnel_key, tunnel_ext):\n\t\t\tgoto set_compat;\n\t\tcase offsetof(struct bpf_tunnel_key, remote_ipv6[1]):\n\t\t\t \n\t\t\tif (ip_tunnel_info_af(info) != AF_INET)\n\t\t\t\tgoto err_clear;\nset_compat:\n\t\t\tto = (struct bpf_tunnel_key *)compat;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto err_clear;\n\t\t}\n\t}\n\n\tto->tunnel_id = be64_to_cpu(info->key.tun_id);\n\tto->tunnel_tos = info->key.tos;\n\tto->tunnel_ttl = info->key.ttl;\n\tif (flags & BPF_F_TUNINFO_FLAGS)\n\t\tto->tunnel_flags = info->key.tun_flags;\n\telse\n\t\tto->tunnel_ext = 0;\n\n\tif (flags & BPF_F_TUNINFO_IPV6) {\n\t\tmemcpy(to->remote_ipv6, &info->key.u.ipv6.src,\n\t\t       sizeof(to->remote_ipv6));\n\t\tmemcpy(to->local_ipv6, &info->key.u.ipv6.dst,\n\t\t       sizeof(to->local_ipv6));\n\t\tto->tunnel_label = be32_to_cpu(info->key.label);\n\t} else {\n\t\tto->remote_ipv4 = be32_to_cpu(info->key.u.ipv4.src);\n\t\tmemset(&to->remote_ipv6[1], 0, sizeof(__u32) * 3);\n\t\tto->local_ipv4 = be32_to_cpu(info->key.u.ipv4.dst);\n\t\tmemset(&to->local_ipv6[1], 0, sizeof(__u32) * 3);\n\t\tto->tunnel_label = 0;\n\t}\n\n\tif (unlikely(size != sizeof(struct bpf_tunnel_key)))\n\t\tmemcpy(to_orig, to, size);\n\n\treturn 0;\nerr_clear:\n\tmemset(to_orig, 0, size);\n\treturn err;\n}\n\nstatic const struct bpf_func_proto bpf_skb_get_tunnel_key_proto = {\n\t.func\t\t= bpf_skb_get_tunnel_key,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg3_type\t= ARG_CONST_SIZE,\n\t.arg4_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_3(bpf_skb_get_tunnel_opt, struct sk_buff *, skb, u8 *, to, u32, size)\n{\n\tconst struct ip_tunnel_info *info = skb_tunnel_info(skb);\n\tint err;\n\n\tif (unlikely(!info ||\n\t\t     !(info->key.tun_flags & TUNNEL_OPTIONS_PRESENT))) {\n\t\terr = -ENOENT;\n\t\tgoto err_clear;\n\t}\n\tif (unlikely(size < info->options_len)) {\n\t\terr = -ENOMEM;\n\t\tgoto err_clear;\n\t}\n\n\tip_tunnel_info_opts_get(to, info);\n\tif (size > info->options_len)\n\t\tmemset(to + info->options_len, 0, size - info->options_len);\n\n\treturn info->options_len;\nerr_clear:\n\tmemset(to, 0, size);\n\treturn err;\n}\n\nstatic const struct bpf_func_proto bpf_skb_get_tunnel_opt_proto = {\n\t.func\t\t= bpf_skb_get_tunnel_opt,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg3_type\t= ARG_CONST_SIZE,\n};\n\nstatic struct metadata_dst __percpu *md_dst;\n\nBPF_CALL_4(bpf_skb_set_tunnel_key, struct sk_buff *, skb,\n\t   const struct bpf_tunnel_key *, from, u32, size, u64, flags)\n{\n\tstruct metadata_dst *md = this_cpu_ptr(md_dst);\n\tu8 compat[sizeof(struct bpf_tunnel_key)];\n\tstruct ip_tunnel_info *info;\n\n\tif (unlikely(flags & ~(BPF_F_TUNINFO_IPV6 | BPF_F_ZERO_CSUM_TX |\n\t\t\t       BPF_F_DONT_FRAGMENT | BPF_F_SEQ_NUMBER |\n\t\t\t       BPF_F_NO_TUNNEL_KEY)))\n\t\treturn -EINVAL;\n\tif (unlikely(size != sizeof(struct bpf_tunnel_key))) {\n\t\tswitch (size) {\n\t\tcase offsetof(struct bpf_tunnel_key, local_ipv6[0]):\n\t\tcase offsetof(struct bpf_tunnel_key, tunnel_label):\n\t\tcase offsetof(struct bpf_tunnel_key, tunnel_ext):\n\t\tcase offsetof(struct bpf_tunnel_key, remote_ipv6[1]):\n\t\t\t \n\t\t\tmemcpy(compat, from, size);\n\t\t\tmemset(compat + size, 0, sizeof(compat) - size);\n\t\t\tfrom = (const struct bpf_tunnel_key *) compat;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\tif (unlikely((!(flags & BPF_F_TUNINFO_IPV6) && from->tunnel_label) ||\n\t\t     from->tunnel_ext))\n\t\treturn -EINVAL;\n\n\tskb_dst_drop(skb);\n\tdst_hold((struct dst_entry *) md);\n\tskb_dst_set(skb, (struct dst_entry *) md);\n\n\tinfo = &md->u.tun_info;\n\tmemset(info, 0, sizeof(*info));\n\tinfo->mode = IP_TUNNEL_INFO_TX;\n\n\tinfo->key.tun_flags = TUNNEL_KEY | TUNNEL_CSUM | TUNNEL_NOCACHE;\n\tif (flags & BPF_F_DONT_FRAGMENT)\n\t\tinfo->key.tun_flags |= TUNNEL_DONT_FRAGMENT;\n\tif (flags & BPF_F_ZERO_CSUM_TX)\n\t\tinfo->key.tun_flags &= ~TUNNEL_CSUM;\n\tif (flags & BPF_F_SEQ_NUMBER)\n\t\tinfo->key.tun_flags |= TUNNEL_SEQ;\n\tif (flags & BPF_F_NO_TUNNEL_KEY)\n\t\tinfo->key.tun_flags &= ~TUNNEL_KEY;\n\n\tinfo->key.tun_id = cpu_to_be64(from->tunnel_id);\n\tinfo->key.tos = from->tunnel_tos;\n\tinfo->key.ttl = from->tunnel_ttl;\n\n\tif (flags & BPF_F_TUNINFO_IPV6) {\n\t\tinfo->mode |= IP_TUNNEL_INFO_IPV6;\n\t\tmemcpy(&info->key.u.ipv6.dst, from->remote_ipv6,\n\t\t       sizeof(from->remote_ipv6));\n\t\tmemcpy(&info->key.u.ipv6.src, from->local_ipv6,\n\t\t       sizeof(from->local_ipv6));\n\t\tinfo->key.label = cpu_to_be32(from->tunnel_label) &\n\t\t\t\t  IPV6_FLOWLABEL_MASK;\n\t} else {\n\t\tinfo->key.u.ipv4.dst = cpu_to_be32(from->remote_ipv4);\n\t\tinfo->key.u.ipv4.src = cpu_to_be32(from->local_ipv4);\n\t\tinfo->key.flow_flags = FLOWI_FLAG_ANYSRC;\n\t}\n\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_skb_set_tunnel_key_proto = {\n\t.func\t\t= bpf_skb_set_tunnel_key,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg3_type\t= ARG_CONST_SIZE,\n\t.arg4_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_3(bpf_skb_set_tunnel_opt, struct sk_buff *, skb,\n\t   const u8 *, from, u32, size)\n{\n\tstruct ip_tunnel_info *info = skb_tunnel_info(skb);\n\tconst struct metadata_dst *md = this_cpu_ptr(md_dst);\n\n\tif (unlikely(info != &md->u.tun_info || (size & (sizeof(u32) - 1))))\n\t\treturn -EINVAL;\n\tif (unlikely(size > IP_TUNNEL_OPTS_MAX))\n\t\treturn -ENOMEM;\n\n\tip_tunnel_info_opts_set(info, from, size, TUNNEL_OPTIONS_PRESENT);\n\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_skb_set_tunnel_opt_proto = {\n\t.func\t\t= bpf_skb_set_tunnel_opt,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg3_type\t= ARG_CONST_SIZE,\n};\n\nstatic const struct bpf_func_proto *\nbpf_get_skb_set_tunnel_proto(enum bpf_func_id which)\n{\n\tif (!md_dst) {\n\t\tstruct metadata_dst __percpu *tmp;\n\n\t\ttmp = metadata_dst_alloc_percpu(IP_TUNNEL_OPTS_MAX,\n\t\t\t\t\t\tMETADATA_IP_TUNNEL,\n\t\t\t\t\t\tGFP_KERNEL);\n\t\tif (!tmp)\n\t\t\treturn NULL;\n\t\tif (cmpxchg(&md_dst, NULL, tmp))\n\t\t\tmetadata_dst_free_percpu(tmp);\n\t}\n\n\tswitch (which) {\n\tcase BPF_FUNC_skb_set_tunnel_key:\n\t\treturn &bpf_skb_set_tunnel_key_proto;\n\tcase BPF_FUNC_skb_set_tunnel_opt:\n\t\treturn &bpf_skb_set_tunnel_opt_proto;\n\tdefault:\n\t\treturn NULL;\n\t}\n}\n\nBPF_CALL_3(bpf_skb_under_cgroup, struct sk_buff *, skb, struct bpf_map *, map,\n\t   u32, idx)\n{\n\tstruct bpf_array *array = container_of(map, struct bpf_array, map);\n\tstruct cgroup *cgrp;\n\tstruct sock *sk;\n\n\tsk = skb_to_full_sk(skb);\n\tif (!sk || !sk_fullsock(sk))\n\t\treturn -ENOENT;\n\tif (unlikely(idx >= array->map.max_entries))\n\t\treturn -E2BIG;\n\n\tcgrp = READ_ONCE(array->ptrs[idx]);\n\tif (unlikely(!cgrp))\n\t\treturn -EAGAIN;\n\n\treturn sk_under_cgroup_hierarchy(sk, cgrp);\n}\n\nstatic const struct bpf_func_proto bpf_skb_under_cgroup_proto = {\n\t.func\t\t= bpf_skb_under_cgroup,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_CONST_MAP_PTR,\n\t.arg3_type\t= ARG_ANYTHING,\n};\n\n#ifdef CONFIG_SOCK_CGROUP_DATA\nstatic inline u64 __bpf_sk_cgroup_id(struct sock *sk)\n{\n\tstruct cgroup *cgrp;\n\n\tsk = sk_to_full_sk(sk);\n\tif (!sk || !sk_fullsock(sk))\n\t\treturn 0;\n\n\tcgrp = sock_cgroup_ptr(&sk->sk_cgrp_data);\n\treturn cgroup_id(cgrp);\n}\n\nBPF_CALL_1(bpf_skb_cgroup_id, const struct sk_buff *, skb)\n{\n\treturn __bpf_sk_cgroup_id(skb->sk);\n}\n\nstatic const struct bpf_func_proto bpf_skb_cgroup_id_proto = {\n\t.func           = bpf_skb_cgroup_id,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type      = ARG_PTR_TO_CTX,\n};\n\nstatic inline u64 __bpf_sk_ancestor_cgroup_id(struct sock *sk,\n\t\t\t\t\t      int ancestor_level)\n{\n\tstruct cgroup *ancestor;\n\tstruct cgroup *cgrp;\n\n\tsk = sk_to_full_sk(sk);\n\tif (!sk || !sk_fullsock(sk))\n\t\treturn 0;\n\n\tcgrp = sock_cgroup_ptr(&sk->sk_cgrp_data);\n\tancestor = cgroup_ancestor(cgrp, ancestor_level);\n\tif (!ancestor)\n\t\treturn 0;\n\n\treturn cgroup_id(ancestor);\n}\n\nBPF_CALL_2(bpf_skb_ancestor_cgroup_id, const struct sk_buff *, skb, int,\n\t   ancestor_level)\n{\n\treturn __bpf_sk_ancestor_cgroup_id(skb->sk, ancestor_level);\n}\n\nstatic const struct bpf_func_proto bpf_skb_ancestor_cgroup_id_proto = {\n\t.func           = bpf_skb_ancestor_cgroup_id,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type      = ARG_PTR_TO_CTX,\n\t.arg2_type      = ARG_ANYTHING,\n};\n\nBPF_CALL_1(bpf_sk_cgroup_id, struct sock *, sk)\n{\n\treturn __bpf_sk_cgroup_id(sk);\n}\n\nstatic const struct bpf_func_proto bpf_sk_cgroup_id_proto = {\n\t.func           = bpf_sk_cgroup_id,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type      = ARG_PTR_TO_BTF_ID_SOCK_COMMON,\n};\n\nBPF_CALL_2(bpf_sk_ancestor_cgroup_id, struct sock *, sk, int, ancestor_level)\n{\n\treturn __bpf_sk_ancestor_cgroup_id(sk, ancestor_level);\n}\n\nstatic const struct bpf_func_proto bpf_sk_ancestor_cgroup_id_proto = {\n\t.func           = bpf_sk_ancestor_cgroup_id,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type      = ARG_PTR_TO_BTF_ID_SOCK_COMMON,\n\t.arg2_type      = ARG_ANYTHING,\n};\n#endif\n\nstatic unsigned long bpf_xdp_copy(void *dst, const void *ctx,\n\t\t\t\t  unsigned long off, unsigned long len)\n{\n\tstruct xdp_buff *xdp = (struct xdp_buff *)ctx;\n\n\tbpf_xdp_copy_buf(xdp, off, dst, len, false);\n\treturn 0;\n}\n\nBPF_CALL_5(bpf_xdp_event_output, struct xdp_buff *, xdp, struct bpf_map *, map,\n\t   u64, flags, void *, meta, u64, meta_size)\n{\n\tu64 xdp_size = (flags & BPF_F_CTXLEN_MASK) >> 32;\n\n\tif (unlikely(flags & ~(BPF_F_CTXLEN_MASK | BPF_F_INDEX_MASK)))\n\t\treturn -EINVAL;\n\n\tif (unlikely(!xdp || xdp_size > xdp_get_buff_len(xdp)))\n\t\treturn -EFAULT;\n\n\treturn bpf_event_output(map, flags, meta, meta_size, xdp,\n\t\t\t\txdp_size, bpf_xdp_copy);\n}\n\nstatic const struct bpf_func_proto bpf_xdp_event_output_proto = {\n\t.func\t\t= bpf_xdp_event_output,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_CONST_MAP_PTR,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg5_type\t= ARG_CONST_SIZE_OR_ZERO,\n};\n\nBTF_ID_LIST_SINGLE(bpf_xdp_output_btf_ids, struct, xdp_buff)\n\nconst struct bpf_func_proto bpf_xdp_output_proto = {\n\t.func\t\t= bpf_xdp_event_output,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_BTF_ID,\n\t.arg1_btf_id\t= &bpf_xdp_output_btf_ids[0],\n\t.arg2_type\t= ARG_CONST_MAP_PTR,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg5_type\t= ARG_CONST_SIZE_OR_ZERO,\n};\n\nBPF_CALL_1(bpf_get_socket_cookie, struct sk_buff *, skb)\n{\n\treturn skb->sk ? __sock_gen_cookie(skb->sk) : 0;\n}\n\nstatic const struct bpf_func_proto bpf_get_socket_cookie_proto = {\n\t.func           = bpf_get_socket_cookie,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type      = ARG_PTR_TO_CTX,\n};\n\nBPF_CALL_1(bpf_get_socket_cookie_sock_addr, struct bpf_sock_addr_kern *, ctx)\n{\n\treturn __sock_gen_cookie(ctx->sk);\n}\n\nstatic const struct bpf_func_proto bpf_get_socket_cookie_sock_addr_proto = {\n\t.func\t\t= bpf_get_socket_cookie_sock_addr,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n};\n\nBPF_CALL_1(bpf_get_socket_cookie_sock, struct sock *, ctx)\n{\n\treturn __sock_gen_cookie(ctx);\n}\n\nstatic const struct bpf_func_proto bpf_get_socket_cookie_sock_proto = {\n\t.func\t\t= bpf_get_socket_cookie_sock,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n};\n\nBPF_CALL_1(bpf_get_socket_ptr_cookie, struct sock *, sk)\n{\n\treturn sk ? sock_gen_cookie(sk) : 0;\n}\n\nconst struct bpf_func_proto bpf_get_socket_ptr_cookie_proto = {\n\t.func\t\t= bpf_get_socket_ptr_cookie,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_BTF_ID_SOCK_COMMON | PTR_MAYBE_NULL,\n};\n\nBPF_CALL_1(bpf_get_socket_cookie_sock_ops, struct bpf_sock_ops_kern *, ctx)\n{\n\treturn __sock_gen_cookie(ctx->sk);\n}\n\nstatic const struct bpf_func_proto bpf_get_socket_cookie_sock_ops_proto = {\n\t.func\t\t= bpf_get_socket_cookie_sock_ops,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n};\n\nstatic u64 __bpf_get_netns_cookie(struct sock *sk)\n{\n\tconst struct net *net = sk ? sock_net(sk) : &init_net;\n\n\treturn net->net_cookie;\n}\n\nBPF_CALL_1(bpf_get_netns_cookie_sock, struct sock *, ctx)\n{\n\treturn __bpf_get_netns_cookie(ctx);\n}\n\nstatic const struct bpf_func_proto bpf_get_netns_cookie_sock_proto = {\n\t.func\t\t= bpf_get_netns_cookie_sock,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX_OR_NULL,\n};\n\nBPF_CALL_1(bpf_get_netns_cookie_sock_addr, struct bpf_sock_addr_kern *, ctx)\n{\n\treturn __bpf_get_netns_cookie(ctx ? ctx->sk : NULL);\n}\n\nstatic const struct bpf_func_proto bpf_get_netns_cookie_sock_addr_proto = {\n\t.func\t\t= bpf_get_netns_cookie_sock_addr,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX_OR_NULL,\n};\n\nBPF_CALL_1(bpf_get_netns_cookie_sock_ops, struct bpf_sock_ops_kern *, ctx)\n{\n\treturn __bpf_get_netns_cookie(ctx ? ctx->sk : NULL);\n}\n\nstatic const struct bpf_func_proto bpf_get_netns_cookie_sock_ops_proto = {\n\t.func\t\t= bpf_get_netns_cookie_sock_ops,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX_OR_NULL,\n};\n\nBPF_CALL_1(bpf_get_netns_cookie_sk_msg, struct sk_msg *, ctx)\n{\n\treturn __bpf_get_netns_cookie(ctx ? ctx->sk : NULL);\n}\n\nstatic const struct bpf_func_proto bpf_get_netns_cookie_sk_msg_proto = {\n\t.func\t\t= bpf_get_netns_cookie_sk_msg,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX_OR_NULL,\n};\n\nBPF_CALL_1(bpf_get_socket_uid, struct sk_buff *, skb)\n{\n\tstruct sock *sk = sk_to_full_sk(skb->sk);\n\tkuid_t kuid;\n\n\tif (!sk || !sk_fullsock(sk))\n\t\treturn overflowuid;\n\tkuid = sock_net_uid(sock_net(sk), sk);\n\treturn from_kuid_munged(sock_net(sk)->user_ns, kuid);\n}\n\nstatic const struct bpf_func_proto bpf_get_socket_uid_proto = {\n\t.func           = bpf_get_socket_uid,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type      = ARG_PTR_TO_CTX,\n};\n\nstatic int sol_socket_sockopt(struct sock *sk, int optname,\n\t\t\t      char *optval, int *optlen,\n\t\t\t      bool getopt)\n{\n\tswitch (optname) {\n\tcase SO_REUSEADDR:\n\tcase SO_SNDBUF:\n\tcase SO_RCVBUF:\n\tcase SO_KEEPALIVE:\n\tcase SO_PRIORITY:\n\tcase SO_REUSEPORT:\n\tcase SO_RCVLOWAT:\n\tcase SO_MARK:\n\tcase SO_MAX_PACING_RATE:\n\tcase SO_BINDTOIFINDEX:\n\tcase SO_TXREHASH:\n\t\tif (*optlen != sizeof(int))\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase SO_BINDTODEVICE:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif (getopt) {\n\t\tif (optname == SO_BINDTODEVICE)\n\t\t\treturn -EINVAL;\n\t\treturn sk_getsockopt(sk, SOL_SOCKET, optname,\n\t\t\t\t     KERNEL_SOCKPTR(optval),\n\t\t\t\t     KERNEL_SOCKPTR(optlen));\n\t}\n\n\treturn sk_setsockopt(sk, SOL_SOCKET, optname,\n\t\t\t     KERNEL_SOCKPTR(optval), *optlen);\n}\n\nstatic int bpf_sol_tcp_setsockopt(struct sock *sk, int optname,\n\t\t\t\t  char *optval, int optlen)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tunsigned long timeout;\n\tint val;\n\n\tif (optlen != sizeof(int))\n\t\treturn -EINVAL;\n\n\tval = *(int *)optval;\n\n\t \n\tswitch (optname) {\n\tcase TCP_BPF_IW:\n\t\tif (val <= 0 || tp->data_segs_out > tp->syn_data)\n\t\t\treturn -EINVAL;\n\t\ttcp_snd_cwnd_set(tp, val);\n\t\tbreak;\n\tcase TCP_BPF_SNDCWND_CLAMP:\n\t\tif (val <= 0)\n\t\t\treturn -EINVAL;\n\t\ttp->snd_cwnd_clamp = val;\n\t\ttp->snd_ssthresh = val;\n\t\tbreak;\n\tcase TCP_BPF_DELACK_MAX:\n\t\ttimeout = usecs_to_jiffies(val);\n\t\tif (timeout > TCP_DELACK_MAX ||\n\t\t    timeout < TCP_TIMEOUT_MIN)\n\t\t\treturn -EINVAL;\n\t\tinet_csk(sk)->icsk_delack_max = timeout;\n\t\tbreak;\n\tcase TCP_BPF_RTO_MIN:\n\t\ttimeout = usecs_to_jiffies(val);\n\t\tif (timeout > TCP_RTO_MIN ||\n\t\t    timeout < TCP_TIMEOUT_MIN)\n\t\t\treturn -EINVAL;\n\t\tinet_csk(sk)->icsk_rto_min = timeout;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int sol_tcp_sockopt_congestion(struct sock *sk, char *optval,\n\t\t\t\t      int *optlen, bool getopt)\n{\n\tstruct tcp_sock *tp;\n\tint ret;\n\n\tif (*optlen < 2)\n\t\treturn -EINVAL;\n\n\tif (getopt) {\n\t\tif (!inet_csk(sk)->icsk_ca_ops)\n\t\t\treturn -EINVAL;\n\t\t \n\t\toptval[--(*optlen)] = '\\0';\n\t\treturn do_tcp_getsockopt(sk, SOL_TCP, TCP_CONGESTION,\n\t\t\t\t\t KERNEL_SOCKPTR(optval),\n\t\t\t\t\t KERNEL_SOCKPTR(optlen));\n\t}\n\n\t \n\tif (*optlen >= sizeof(\"cdg\") - 1 && !strncmp(\"cdg\", optval, *optlen))\n\t\treturn -ENOTSUPP;\n\n\t \n\ttp = tcp_sk(sk);\n\tif (tp->bpf_chg_cc_inprogress)\n\t\treturn -EBUSY;\n\n\ttp->bpf_chg_cc_inprogress = 1;\n\tret = do_tcp_setsockopt(sk, SOL_TCP, TCP_CONGESTION,\n\t\t\t\tKERNEL_SOCKPTR(optval), *optlen);\n\ttp->bpf_chg_cc_inprogress = 0;\n\treturn ret;\n}\n\nstatic int sol_tcp_sockopt(struct sock *sk, int optname,\n\t\t\t   char *optval, int *optlen,\n\t\t\t   bool getopt)\n{\n\tif (sk->sk_protocol != IPPROTO_TCP)\n\t\treturn -EINVAL;\n\n\tswitch (optname) {\n\tcase TCP_NODELAY:\n\tcase TCP_MAXSEG:\n\tcase TCP_KEEPIDLE:\n\tcase TCP_KEEPINTVL:\n\tcase TCP_KEEPCNT:\n\tcase TCP_SYNCNT:\n\tcase TCP_WINDOW_CLAMP:\n\tcase TCP_THIN_LINEAR_TIMEOUTS:\n\tcase TCP_USER_TIMEOUT:\n\tcase TCP_NOTSENT_LOWAT:\n\tcase TCP_SAVE_SYN:\n\t\tif (*optlen != sizeof(int))\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase TCP_CONGESTION:\n\t\treturn sol_tcp_sockopt_congestion(sk, optval, optlen, getopt);\n\tcase TCP_SAVED_SYN:\n\t\tif (*optlen < 1)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tdefault:\n\t\tif (getopt)\n\t\t\treturn -EINVAL;\n\t\treturn bpf_sol_tcp_setsockopt(sk, optname, optval, *optlen);\n\t}\n\n\tif (getopt) {\n\t\tif (optname == TCP_SAVED_SYN) {\n\t\t\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t\t\tif (!tp->saved_syn ||\n\t\t\t    *optlen > tcp_saved_syn_len(tp->saved_syn))\n\t\t\t\treturn -EINVAL;\n\t\t\tmemcpy(optval, tp->saved_syn->data, *optlen);\n\t\t\t \n\t\t\treturn 0;\n\t\t}\n\n\t\treturn do_tcp_getsockopt(sk, SOL_TCP, optname,\n\t\t\t\t\t KERNEL_SOCKPTR(optval),\n\t\t\t\t\t KERNEL_SOCKPTR(optlen));\n\t}\n\n\treturn do_tcp_setsockopt(sk, SOL_TCP, optname,\n\t\t\t\t KERNEL_SOCKPTR(optval), *optlen);\n}\n\nstatic int sol_ip_sockopt(struct sock *sk, int optname,\n\t\t\t  char *optval, int *optlen,\n\t\t\t  bool getopt)\n{\n\tif (sk->sk_family != AF_INET)\n\t\treturn -EINVAL;\n\n\tswitch (optname) {\n\tcase IP_TOS:\n\t\tif (*optlen != sizeof(int))\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif (getopt)\n\t\treturn do_ip_getsockopt(sk, SOL_IP, optname,\n\t\t\t\t\tKERNEL_SOCKPTR(optval),\n\t\t\t\t\tKERNEL_SOCKPTR(optlen));\n\n\treturn do_ip_setsockopt(sk, SOL_IP, optname,\n\t\t\t\tKERNEL_SOCKPTR(optval), *optlen);\n}\n\nstatic int sol_ipv6_sockopt(struct sock *sk, int optname,\n\t\t\t    char *optval, int *optlen,\n\t\t\t    bool getopt)\n{\n\tif (sk->sk_family != AF_INET6)\n\t\treturn -EINVAL;\n\n\tswitch (optname) {\n\tcase IPV6_TCLASS:\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tif (*optlen != sizeof(int))\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif (getopt)\n\t\treturn ipv6_bpf_stub->ipv6_getsockopt(sk, SOL_IPV6, optname,\n\t\t\t\t\t\t      KERNEL_SOCKPTR(optval),\n\t\t\t\t\t\t      KERNEL_SOCKPTR(optlen));\n\n\treturn ipv6_bpf_stub->ipv6_setsockopt(sk, SOL_IPV6, optname,\n\t\t\t\t\t      KERNEL_SOCKPTR(optval), *optlen);\n}\n\nstatic int __bpf_setsockopt(struct sock *sk, int level, int optname,\n\t\t\t    char *optval, int optlen)\n{\n\tif (!sk_fullsock(sk))\n\t\treturn -EINVAL;\n\n\tif (level == SOL_SOCKET)\n\t\treturn sol_socket_sockopt(sk, optname, optval, &optlen, false);\n\telse if (IS_ENABLED(CONFIG_INET) && level == SOL_IP)\n\t\treturn sol_ip_sockopt(sk, optname, optval, &optlen, false);\n\telse if (IS_ENABLED(CONFIG_IPV6) && level == SOL_IPV6)\n\t\treturn sol_ipv6_sockopt(sk, optname, optval, &optlen, false);\n\telse if (IS_ENABLED(CONFIG_INET) && level == SOL_TCP)\n\t\treturn sol_tcp_sockopt(sk, optname, optval, &optlen, false);\n\n\treturn -EINVAL;\n}\n\nstatic int _bpf_setsockopt(struct sock *sk, int level, int optname,\n\t\t\t   char *optval, int optlen)\n{\n\tif (sk_fullsock(sk))\n\t\tsock_owned_by_me(sk);\n\treturn __bpf_setsockopt(sk, level, optname, optval, optlen);\n}\n\nstatic int __bpf_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t    char *optval, int optlen)\n{\n\tint err, saved_optlen = optlen;\n\n\tif (!sk_fullsock(sk)) {\n\t\terr = -EINVAL;\n\t\tgoto done;\n\t}\n\n\tif (level == SOL_SOCKET)\n\t\terr = sol_socket_sockopt(sk, optname, optval, &optlen, true);\n\telse if (IS_ENABLED(CONFIG_INET) && level == SOL_TCP)\n\t\terr = sol_tcp_sockopt(sk, optname, optval, &optlen, true);\n\telse if (IS_ENABLED(CONFIG_INET) && level == SOL_IP)\n\t\terr = sol_ip_sockopt(sk, optname, optval, &optlen, true);\n\telse if (IS_ENABLED(CONFIG_IPV6) && level == SOL_IPV6)\n\t\terr = sol_ipv6_sockopt(sk, optname, optval, &optlen, true);\n\telse\n\t\terr = -EINVAL;\n\ndone:\n\tif (err)\n\t\toptlen = 0;\n\tif (optlen < saved_optlen)\n\t\tmemset(optval + optlen, 0, saved_optlen - optlen);\n\treturn err;\n}\n\nstatic int _bpf_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t   char *optval, int optlen)\n{\n\tif (sk_fullsock(sk))\n\t\tsock_owned_by_me(sk);\n\treturn __bpf_getsockopt(sk, level, optname, optval, optlen);\n}\n\nBPF_CALL_5(bpf_sk_setsockopt, struct sock *, sk, int, level,\n\t   int, optname, char *, optval, int, optlen)\n{\n\treturn _bpf_setsockopt(sk, level, optname, optval, optlen);\n}\n\nconst struct bpf_func_proto bpf_sk_setsockopt_proto = {\n\t.func\t\t= bpf_sk_setsockopt,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_BTF_ID_SOCK_COMMON,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg5_type\t= ARG_CONST_SIZE,\n};\n\nBPF_CALL_5(bpf_sk_getsockopt, struct sock *, sk, int, level,\n\t   int, optname, char *, optval, int, optlen)\n{\n\treturn _bpf_getsockopt(sk, level, optname, optval, optlen);\n}\n\nconst struct bpf_func_proto bpf_sk_getsockopt_proto = {\n\t.func\t\t= bpf_sk_getsockopt,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_BTF_ID_SOCK_COMMON,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg5_type\t= ARG_CONST_SIZE,\n};\n\nBPF_CALL_5(bpf_unlocked_sk_setsockopt, struct sock *, sk, int, level,\n\t   int, optname, char *, optval, int, optlen)\n{\n\treturn __bpf_setsockopt(sk, level, optname, optval, optlen);\n}\n\nconst struct bpf_func_proto bpf_unlocked_sk_setsockopt_proto = {\n\t.func\t\t= bpf_unlocked_sk_setsockopt,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_BTF_ID_SOCK_COMMON,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg5_type\t= ARG_CONST_SIZE,\n};\n\nBPF_CALL_5(bpf_unlocked_sk_getsockopt, struct sock *, sk, int, level,\n\t   int, optname, char *, optval, int, optlen)\n{\n\treturn __bpf_getsockopt(sk, level, optname, optval, optlen);\n}\n\nconst struct bpf_func_proto bpf_unlocked_sk_getsockopt_proto = {\n\t.func\t\t= bpf_unlocked_sk_getsockopt,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_BTF_ID_SOCK_COMMON,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg5_type\t= ARG_CONST_SIZE,\n};\n\nBPF_CALL_5(bpf_sock_addr_setsockopt, struct bpf_sock_addr_kern *, ctx,\n\t   int, level, int, optname, char *, optval, int, optlen)\n{\n\treturn _bpf_setsockopt(ctx->sk, level, optname, optval, optlen);\n}\n\nstatic const struct bpf_func_proto bpf_sock_addr_setsockopt_proto = {\n\t.func\t\t= bpf_sock_addr_setsockopt,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg5_type\t= ARG_CONST_SIZE,\n};\n\nBPF_CALL_5(bpf_sock_addr_getsockopt, struct bpf_sock_addr_kern *, ctx,\n\t   int, level, int, optname, char *, optval, int, optlen)\n{\n\treturn _bpf_getsockopt(ctx->sk, level, optname, optval, optlen);\n}\n\nstatic const struct bpf_func_proto bpf_sock_addr_getsockopt_proto = {\n\t.func\t\t= bpf_sock_addr_getsockopt,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg5_type\t= ARG_CONST_SIZE,\n};\n\nBPF_CALL_5(bpf_sock_ops_setsockopt, struct bpf_sock_ops_kern *, bpf_sock,\n\t   int, level, int, optname, char *, optval, int, optlen)\n{\n\treturn _bpf_setsockopt(bpf_sock->sk, level, optname, optval, optlen);\n}\n\nstatic const struct bpf_func_proto bpf_sock_ops_setsockopt_proto = {\n\t.func\t\t= bpf_sock_ops_setsockopt,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg5_type\t= ARG_CONST_SIZE,\n};\n\nstatic int bpf_sock_ops_get_syn(struct bpf_sock_ops_kern *bpf_sock,\n\t\t\t\tint optname, const u8 **start)\n{\n\tstruct sk_buff *syn_skb = bpf_sock->syn_skb;\n\tconst u8 *hdr_start;\n\tint ret;\n\n\tif (syn_skb) {\n\t\t \n\n\t\tif (optname == TCP_BPF_SYN) {\n\t\t\thdr_start = syn_skb->data;\n\t\t\tret = tcp_hdrlen(syn_skb);\n\t\t} else if (optname == TCP_BPF_SYN_IP) {\n\t\t\thdr_start = skb_network_header(syn_skb);\n\t\t\tret = skb_network_header_len(syn_skb) +\n\t\t\t\ttcp_hdrlen(syn_skb);\n\t\t} else {\n\t\t\t \n\t\t\thdr_start = skb_mac_header(syn_skb);\n\t\t\tret = skb_mac_header_len(syn_skb) +\n\t\t\t\tskb_network_header_len(syn_skb) +\n\t\t\t\ttcp_hdrlen(syn_skb);\n\t\t}\n\t} else {\n\t\tstruct sock *sk = bpf_sock->sk;\n\t\tstruct saved_syn *saved_syn;\n\n\t\tif (sk->sk_state == TCP_NEW_SYN_RECV)\n\t\t\t \n\t\t\tsaved_syn = inet_reqsk(sk)->saved_syn;\n\t\telse\n\t\t\tsaved_syn = tcp_sk(sk)->saved_syn;\n\n\t\tif (!saved_syn)\n\t\t\treturn -ENOENT;\n\n\t\tif (optname == TCP_BPF_SYN) {\n\t\t\thdr_start = saved_syn->data +\n\t\t\t\tsaved_syn->mac_hdrlen +\n\t\t\t\tsaved_syn->network_hdrlen;\n\t\t\tret = saved_syn->tcp_hdrlen;\n\t\t} else if (optname == TCP_BPF_SYN_IP) {\n\t\t\thdr_start = saved_syn->data +\n\t\t\t\tsaved_syn->mac_hdrlen;\n\t\t\tret = saved_syn->network_hdrlen +\n\t\t\t\tsaved_syn->tcp_hdrlen;\n\t\t} else {\n\t\t\t \n\n\t\t\t \n\t\t\tif (!saved_syn->mac_hdrlen)\n\t\t\t\treturn -ENOENT;\n\n\t\t\thdr_start = saved_syn->data;\n\t\t\tret = saved_syn->mac_hdrlen +\n\t\t\t\tsaved_syn->network_hdrlen +\n\t\t\t\tsaved_syn->tcp_hdrlen;\n\t\t}\n\t}\n\n\t*start = hdr_start;\n\treturn ret;\n}\n\nBPF_CALL_5(bpf_sock_ops_getsockopt, struct bpf_sock_ops_kern *, bpf_sock,\n\t   int, level, int, optname, char *, optval, int, optlen)\n{\n\tif (IS_ENABLED(CONFIG_INET) && level == SOL_TCP &&\n\t    optname >= TCP_BPF_SYN && optname <= TCP_BPF_SYN_MAC) {\n\t\tint ret, copy_len = 0;\n\t\tconst u8 *start;\n\n\t\tret = bpf_sock_ops_get_syn(bpf_sock, optname, &start);\n\t\tif (ret > 0) {\n\t\t\tcopy_len = ret;\n\t\t\tif (optlen < copy_len) {\n\t\t\t\tcopy_len = optlen;\n\t\t\t\tret = -ENOSPC;\n\t\t\t}\n\n\t\t\tmemcpy(optval, start, copy_len);\n\t\t}\n\n\t\t \n\t\tmemset(optval + copy_len, 0, optlen - copy_len);\n\n\t\treturn ret;\n\t}\n\n\treturn _bpf_getsockopt(bpf_sock->sk, level, optname, optval, optlen);\n}\n\nstatic const struct bpf_func_proto bpf_sock_ops_getsockopt_proto = {\n\t.func\t\t= bpf_sock_ops_getsockopt,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg5_type\t= ARG_CONST_SIZE,\n};\n\nBPF_CALL_2(bpf_sock_ops_cb_flags_set, struct bpf_sock_ops_kern *, bpf_sock,\n\t   int, argval)\n{\n\tstruct sock *sk = bpf_sock->sk;\n\tint val = argval & BPF_SOCK_OPS_ALL_CB_FLAGS;\n\n\tif (!IS_ENABLED(CONFIG_INET) || !sk_fullsock(sk))\n\t\treturn -EINVAL;\n\n\ttcp_sk(sk)->bpf_sock_ops_cb_flags = val;\n\n\treturn argval & (~BPF_SOCK_OPS_ALL_CB_FLAGS);\n}\n\nstatic const struct bpf_func_proto bpf_sock_ops_cb_flags_set_proto = {\n\t.func\t\t= bpf_sock_ops_cb_flags_set,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n};\n\nconst struct ipv6_bpf_stub *ipv6_bpf_stub __read_mostly;\nEXPORT_SYMBOL_GPL(ipv6_bpf_stub);\n\nBPF_CALL_3(bpf_bind, struct bpf_sock_addr_kern *, ctx, struct sockaddr *, addr,\n\t   int, addr_len)\n{\n#ifdef CONFIG_INET\n\tstruct sock *sk = ctx->sk;\n\tu32 flags = BIND_FROM_BPF;\n\tint err;\n\n\terr = -EINVAL;\n\tif (addr_len < offsetofend(struct sockaddr, sa_family))\n\t\treturn err;\n\tif (addr->sa_family == AF_INET) {\n\t\tif (addr_len < sizeof(struct sockaddr_in))\n\t\t\treturn err;\n\t\tif (((struct sockaddr_in *)addr)->sin_port == htons(0))\n\t\t\tflags |= BIND_FORCE_ADDRESS_NO_PORT;\n\t\treturn __inet_bind(sk, addr, addr_len, flags);\n#if IS_ENABLED(CONFIG_IPV6)\n\t} else if (addr->sa_family == AF_INET6) {\n\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\treturn err;\n\t\tif (((struct sockaddr_in6 *)addr)->sin6_port == htons(0))\n\t\t\tflags |= BIND_FORCE_ADDRESS_NO_PORT;\n\t\t \n\t\treturn ipv6_bpf_stub->inet6_bind(sk, addr, addr_len, flags);\n#endif  \n\t}\n#endif  \n\n\treturn -EAFNOSUPPORT;\n}\n\nstatic const struct bpf_func_proto bpf_bind_proto = {\n\t.func\t\t= bpf_bind,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg3_type\t= ARG_CONST_SIZE,\n};\n\n#ifdef CONFIG_XFRM\n\n#if (IS_BUILTIN(CONFIG_XFRM_INTERFACE) && IS_ENABLED(CONFIG_DEBUG_INFO_BTF)) || \\\n    (IS_MODULE(CONFIG_XFRM_INTERFACE) && IS_ENABLED(CONFIG_DEBUG_INFO_BTF_MODULES))\n\nstruct metadata_dst __percpu *xfrm_bpf_md_dst;\nEXPORT_SYMBOL_GPL(xfrm_bpf_md_dst);\n\n#endif\n\nBPF_CALL_5(bpf_skb_get_xfrm_state, struct sk_buff *, skb, u32, index,\n\t   struct bpf_xfrm_state *, to, u32, size, u64, flags)\n{\n\tconst struct sec_path *sp = skb_sec_path(skb);\n\tconst struct xfrm_state *x;\n\n\tif (!sp || unlikely(index >= sp->len || flags))\n\t\tgoto err_clear;\n\n\tx = sp->xvec[index];\n\n\tif (unlikely(size != sizeof(struct bpf_xfrm_state)))\n\t\tgoto err_clear;\n\n\tto->reqid = x->props.reqid;\n\tto->spi = x->id.spi;\n\tto->family = x->props.family;\n\tto->ext = 0;\n\n\tif (to->family == AF_INET6) {\n\t\tmemcpy(to->remote_ipv6, x->props.saddr.a6,\n\t\t       sizeof(to->remote_ipv6));\n\t} else {\n\t\tto->remote_ipv4 = x->props.saddr.a4;\n\t\tmemset(&to->remote_ipv6[1], 0, sizeof(__u32) * 3);\n\t}\n\n\treturn 0;\nerr_clear:\n\tmemset(to, 0, size);\n\treturn -EINVAL;\n}\n\nstatic const struct bpf_func_proto bpf_skb_get_xfrm_state_proto = {\n\t.func\t\t= bpf_skb_get_xfrm_state,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg4_type\t= ARG_CONST_SIZE,\n\t.arg5_type\t= ARG_ANYTHING,\n};\n#endif\n\n#if IS_ENABLED(CONFIG_INET) || IS_ENABLED(CONFIG_IPV6)\nstatic int bpf_fib_set_fwd_params(struct bpf_fib_lookup *params, u32 mtu)\n{\n\tparams->h_vlan_TCI = 0;\n\tparams->h_vlan_proto = 0;\n\tif (mtu)\n\t\tparams->mtu_result = mtu;  \n\n\treturn 0;\n}\n#endif\n\n#if IS_ENABLED(CONFIG_INET)\nstatic int bpf_ipv4_fib_lookup(struct net *net, struct bpf_fib_lookup *params,\n\t\t\t       u32 flags, bool check_mtu)\n{\n\tstruct fib_nh_common *nhc;\n\tstruct in_device *in_dev;\n\tstruct neighbour *neigh;\n\tstruct net_device *dev;\n\tstruct fib_result res;\n\tstruct flowi4 fl4;\n\tu32 mtu = 0;\n\tint err;\n\n\tdev = dev_get_by_index_rcu(net, params->ifindex);\n\tif (unlikely(!dev))\n\t\treturn -ENODEV;\n\n\t \n\tin_dev = __in_dev_get_rcu(dev);\n\tif (unlikely(!in_dev || !IN_DEV_FORWARD(in_dev)))\n\t\treturn BPF_FIB_LKUP_RET_FWD_DISABLED;\n\n\tif (flags & BPF_FIB_LOOKUP_OUTPUT) {\n\t\tfl4.flowi4_iif = 1;\n\t\tfl4.flowi4_oif = params->ifindex;\n\t} else {\n\t\tfl4.flowi4_iif = params->ifindex;\n\t\tfl4.flowi4_oif = 0;\n\t}\n\tfl4.flowi4_tos = params->tos & IPTOS_RT_MASK;\n\tfl4.flowi4_scope = RT_SCOPE_UNIVERSE;\n\tfl4.flowi4_flags = 0;\n\n\tfl4.flowi4_proto = params->l4_protocol;\n\tfl4.daddr = params->ipv4_dst;\n\tfl4.saddr = params->ipv4_src;\n\tfl4.fl4_sport = params->sport;\n\tfl4.fl4_dport = params->dport;\n\tfl4.flowi4_multipath_hash = 0;\n\n\tif (flags & BPF_FIB_LOOKUP_DIRECT) {\n\t\tu32 tbid = l3mdev_fib_table_rcu(dev) ? : RT_TABLE_MAIN;\n\t\tstruct fib_table *tb;\n\n\t\tif (flags & BPF_FIB_LOOKUP_TBID) {\n\t\t\ttbid = params->tbid;\n\t\t\t \n\t\t\tparams->tbid = 0;\n\t\t}\n\n\t\ttb = fib_get_table(net, tbid);\n\t\tif (unlikely(!tb))\n\t\t\treturn BPF_FIB_LKUP_RET_NOT_FWDED;\n\n\t\terr = fib_table_lookup(tb, &fl4, &res, FIB_LOOKUP_NOREF);\n\t} else {\n\t\tfl4.flowi4_mark = 0;\n\t\tfl4.flowi4_secid = 0;\n\t\tfl4.flowi4_tun_key.tun_id = 0;\n\t\tfl4.flowi4_uid = sock_net_uid(net, NULL);\n\n\t\terr = fib_lookup(net, &fl4, &res, FIB_LOOKUP_NOREF);\n\t}\n\n\tif (err) {\n\t\t \n\t\tif (err == -EINVAL)\n\t\t\treturn BPF_FIB_LKUP_RET_BLACKHOLE;\n\t\tif (err == -EHOSTUNREACH)\n\t\t\treturn BPF_FIB_LKUP_RET_UNREACHABLE;\n\t\tif (err == -EACCES)\n\t\t\treturn BPF_FIB_LKUP_RET_PROHIBIT;\n\n\t\treturn BPF_FIB_LKUP_RET_NOT_FWDED;\n\t}\n\n\tif (res.type != RTN_UNICAST)\n\t\treturn BPF_FIB_LKUP_RET_NOT_FWDED;\n\n\tif (fib_info_num_path(res.fi) > 1)\n\t\tfib_select_path(net, &res, &fl4, NULL);\n\n\tif (check_mtu) {\n\t\tmtu = ip_mtu_from_fib_result(&res, params->ipv4_dst);\n\t\tif (params->tot_len > mtu) {\n\t\t\tparams->mtu_result = mtu;  \n\t\t\treturn BPF_FIB_LKUP_RET_FRAG_NEEDED;\n\t\t}\n\t}\n\n\tnhc = res.nhc;\n\n\t \n\tif (nhc->nhc_lwtstate)\n\t\treturn BPF_FIB_LKUP_RET_UNSUPP_LWT;\n\n\tdev = nhc->nhc_dev;\n\n\tparams->rt_metric = res.fi->fib_priority;\n\tparams->ifindex = dev->ifindex;\n\n\t \n\tif (likely(nhc->nhc_gw_family != AF_INET6)) {\n\t\tif (nhc->nhc_gw_family)\n\t\t\tparams->ipv4_dst = nhc->nhc_gw.ipv4;\n\t} else {\n\t\tstruct in6_addr *dst = (struct in6_addr *)params->ipv6_dst;\n\n\t\tparams->family = AF_INET6;\n\t\t*dst = nhc->nhc_gw.ipv6;\n\t}\n\n\tif (flags & BPF_FIB_LOOKUP_SKIP_NEIGH)\n\t\tgoto set_fwd_params;\n\n\tif (likely(nhc->nhc_gw_family != AF_INET6))\n\t\tneigh = __ipv4_neigh_lookup_noref(dev,\n\t\t\t\t\t\t  (__force u32)params->ipv4_dst);\n\telse\n\t\tneigh = __ipv6_neigh_lookup_noref_stub(dev, params->ipv6_dst);\n\n\tif (!neigh || !(READ_ONCE(neigh->nud_state) & NUD_VALID))\n\t\treturn BPF_FIB_LKUP_RET_NO_NEIGH;\n\tmemcpy(params->dmac, neigh->ha, ETH_ALEN);\n\tmemcpy(params->smac, dev->dev_addr, ETH_ALEN);\n\nset_fwd_params:\n\treturn bpf_fib_set_fwd_params(params, mtu);\n}\n#endif\n\n#if IS_ENABLED(CONFIG_IPV6)\nstatic int bpf_ipv6_fib_lookup(struct net *net, struct bpf_fib_lookup *params,\n\t\t\t       u32 flags, bool check_mtu)\n{\n\tstruct in6_addr *src = (struct in6_addr *) params->ipv6_src;\n\tstruct in6_addr *dst = (struct in6_addr *) params->ipv6_dst;\n\tstruct fib6_result res = {};\n\tstruct neighbour *neigh;\n\tstruct net_device *dev;\n\tstruct inet6_dev *idev;\n\tstruct flowi6 fl6;\n\tint strict = 0;\n\tint oif, err;\n\tu32 mtu = 0;\n\n\t \n\tif (rt6_need_strict(dst) || rt6_need_strict(src))\n\t\treturn BPF_FIB_LKUP_RET_NOT_FWDED;\n\n\tdev = dev_get_by_index_rcu(net, params->ifindex);\n\tif (unlikely(!dev))\n\t\treturn -ENODEV;\n\n\tidev = __in6_dev_get_safely(dev);\n\tif (unlikely(!idev || !idev->cnf.forwarding))\n\t\treturn BPF_FIB_LKUP_RET_FWD_DISABLED;\n\n\tif (flags & BPF_FIB_LOOKUP_OUTPUT) {\n\t\tfl6.flowi6_iif = 1;\n\t\toif = fl6.flowi6_oif = params->ifindex;\n\t} else {\n\t\toif = fl6.flowi6_iif = params->ifindex;\n\t\tfl6.flowi6_oif = 0;\n\t\tstrict = RT6_LOOKUP_F_HAS_SADDR;\n\t}\n\tfl6.flowlabel = params->flowinfo;\n\tfl6.flowi6_scope = 0;\n\tfl6.flowi6_flags = 0;\n\tfl6.mp_hash = 0;\n\n\tfl6.flowi6_proto = params->l4_protocol;\n\tfl6.daddr = *dst;\n\tfl6.saddr = *src;\n\tfl6.fl6_sport = params->sport;\n\tfl6.fl6_dport = params->dport;\n\n\tif (flags & BPF_FIB_LOOKUP_DIRECT) {\n\t\tu32 tbid = l3mdev_fib_table_rcu(dev) ? : RT_TABLE_MAIN;\n\t\tstruct fib6_table *tb;\n\n\t\tif (flags & BPF_FIB_LOOKUP_TBID) {\n\t\t\ttbid = params->tbid;\n\t\t\t \n\t\t\tparams->tbid = 0;\n\t\t}\n\n\t\ttb = ipv6_stub->fib6_get_table(net, tbid);\n\t\tif (unlikely(!tb))\n\t\t\treturn BPF_FIB_LKUP_RET_NOT_FWDED;\n\n\t\terr = ipv6_stub->fib6_table_lookup(net, tb, oif, &fl6, &res,\n\t\t\t\t\t\t   strict);\n\t} else {\n\t\tfl6.flowi6_mark = 0;\n\t\tfl6.flowi6_secid = 0;\n\t\tfl6.flowi6_tun_key.tun_id = 0;\n\t\tfl6.flowi6_uid = sock_net_uid(net, NULL);\n\n\t\terr = ipv6_stub->fib6_lookup(net, oif, &fl6, &res, strict);\n\t}\n\n\tif (unlikely(err || IS_ERR_OR_NULL(res.f6i) ||\n\t\t     res.f6i == net->ipv6.fib6_null_entry))\n\t\treturn BPF_FIB_LKUP_RET_NOT_FWDED;\n\n\tswitch (res.fib6_type) {\n\t \n\tcase RTN_UNICAST:\n\t\tbreak;\n\tcase RTN_BLACKHOLE:\n\t\treturn BPF_FIB_LKUP_RET_BLACKHOLE;\n\tcase RTN_UNREACHABLE:\n\t\treturn BPF_FIB_LKUP_RET_UNREACHABLE;\n\tcase RTN_PROHIBIT:\n\t\treturn BPF_FIB_LKUP_RET_PROHIBIT;\n\tdefault:\n\t\treturn BPF_FIB_LKUP_RET_NOT_FWDED;\n\t}\n\n\tipv6_stub->fib6_select_path(net, &res, &fl6, fl6.flowi6_oif,\n\t\t\t\t    fl6.flowi6_oif != 0, NULL, strict);\n\n\tif (check_mtu) {\n\t\tmtu = ipv6_stub->ip6_mtu_from_fib6(&res, dst, src);\n\t\tif (params->tot_len > mtu) {\n\t\t\tparams->mtu_result = mtu;  \n\t\t\treturn BPF_FIB_LKUP_RET_FRAG_NEEDED;\n\t\t}\n\t}\n\n\tif (res.nh->fib_nh_lws)\n\t\treturn BPF_FIB_LKUP_RET_UNSUPP_LWT;\n\n\tif (res.nh->fib_nh_gw_family)\n\t\t*dst = res.nh->fib_nh_gw6;\n\n\tdev = res.nh->fib_nh_dev;\n\tparams->rt_metric = res.f6i->fib6_metric;\n\tparams->ifindex = dev->ifindex;\n\n\tif (flags & BPF_FIB_LOOKUP_SKIP_NEIGH)\n\t\tgoto set_fwd_params;\n\n\t \n\tneigh = __ipv6_neigh_lookup_noref_stub(dev, dst);\n\tif (!neigh || !(READ_ONCE(neigh->nud_state) & NUD_VALID))\n\t\treturn BPF_FIB_LKUP_RET_NO_NEIGH;\n\tmemcpy(params->dmac, neigh->ha, ETH_ALEN);\n\tmemcpy(params->smac, dev->dev_addr, ETH_ALEN);\n\nset_fwd_params:\n\treturn bpf_fib_set_fwd_params(params, mtu);\n}\n#endif\n\n#define BPF_FIB_LOOKUP_MASK (BPF_FIB_LOOKUP_DIRECT | BPF_FIB_LOOKUP_OUTPUT | \\\n\t\t\t     BPF_FIB_LOOKUP_SKIP_NEIGH | BPF_FIB_LOOKUP_TBID)\n\nBPF_CALL_4(bpf_xdp_fib_lookup, struct xdp_buff *, ctx,\n\t   struct bpf_fib_lookup *, params, int, plen, u32, flags)\n{\n\tif (plen < sizeof(*params))\n\t\treturn -EINVAL;\n\n\tif (flags & ~BPF_FIB_LOOKUP_MASK)\n\t\treturn -EINVAL;\n\n\tswitch (params->family) {\n#if IS_ENABLED(CONFIG_INET)\n\tcase AF_INET:\n\t\treturn bpf_ipv4_fib_lookup(dev_net(ctx->rxq->dev), params,\n\t\t\t\t\t   flags, true);\n#endif\n#if IS_ENABLED(CONFIG_IPV6)\n\tcase AF_INET6:\n\t\treturn bpf_ipv6_fib_lookup(dev_net(ctx->rxq->dev), params,\n\t\t\t\t\t   flags, true);\n#endif\n\t}\n\treturn -EAFNOSUPPORT;\n}\n\nstatic const struct bpf_func_proto bpf_xdp_fib_lookup_proto = {\n\t.func\t\t= bpf_xdp_fib_lookup,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type      = ARG_PTR_TO_CTX,\n\t.arg2_type      = ARG_PTR_TO_MEM,\n\t.arg3_type      = ARG_CONST_SIZE,\n\t.arg4_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_4(bpf_skb_fib_lookup, struct sk_buff *, skb,\n\t   struct bpf_fib_lookup *, params, int, plen, u32, flags)\n{\n\tstruct net *net = dev_net(skb->dev);\n\tint rc = -EAFNOSUPPORT;\n\tbool check_mtu = false;\n\n\tif (plen < sizeof(*params))\n\t\treturn -EINVAL;\n\n\tif (flags & ~BPF_FIB_LOOKUP_MASK)\n\t\treturn -EINVAL;\n\n\tif (params->tot_len)\n\t\tcheck_mtu = true;\n\n\tswitch (params->family) {\n#if IS_ENABLED(CONFIG_INET)\n\tcase AF_INET:\n\t\trc = bpf_ipv4_fib_lookup(net, params, flags, check_mtu);\n\t\tbreak;\n#endif\n#if IS_ENABLED(CONFIG_IPV6)\n\tcase AF_INET6:\n\t\trc = bpf_ipv6_fib_lookup(net, params, flags, check_mtu);\n\t\tbreak;\n#endif\n\t}\n\n\tif (rc == BPF_FIB_LKUP_RET_SUCCESS && !check_mtu) {\n\t\tstruct net_device *dev;\n\n\t\t \n\t\tdev = dev_get_by_index_rcu(net, params->ifindex);\n\t\tif (!is_skb_forwardable(dev, skb))\n\t\t\trc = BPF_FIB_LKUP_RET_FRAG_NEEDED;\n\n\t\tparams->mtu_result = dev->mtu;  \n\t}\n\n\treturn rc;\n}\n\nstatic const struct bpf_func_proto bpf_skb_fib_lookup_proto = {\n\t.func\t\t= bpf_skb_fib_lookup,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type      = ARG_PTR_TO_CTX,\n\t.arg2_type      = ARG_PTR_TO_MEM,\n\t.arg3_type      = ARG_CONST_SIZE,\n\t.arg4_type\t= ARG_ANYTHING,\n};\n\nstatic struct net_device *__dev_via_ifindex(struct net_device *dev_curr,\n\t\t\t\t\t    u32 ifindex)\n{\n\tstruct net *netns = dev_net(dev_curr);\n\n\t \n\tif (ifindex == 0)\n\t\treturn dev_curr;\n\n\treturn dev_get_by_index_rcu(netns, ifindex);\n}\n\nBPF_CALL_5(bpf_skb_check_mtu, struct sk_buff *, skb,\n\t   u32, ifindex, u32 *, mtu_len, s32, len_diff, u64, flags)\n{\n\tint ret = BPF_MTU_CHK_RET_FRAG_NEEDED;\n\tstruct net_device *dev = skb->dev;\n\tint skb_len, dev_len;\n\tint mtu;\n\n\tif (unlikely(flags & ~(BPF_MTU_CHK_SEGS)))\n\t\treturn -EINVAL;\n\n\tif (unlikely(flags & BPF_MTU_CHK_SEGS && (len_diff || *mtu_len)))\n\t\treturn -EINVAL;\n\n\tdev = __dev_via_ifindex(dev, ifindex);\n\tif (unlikely(!dev))\n\t\treturn -ENODEV;\n\n\tmtu = READ_ONCE(dev->mtu);\n\n\tdev_len = mtu + dev->hard_header_len;\n\n\t \n\tskb_len = *mtu_len ? *mtu_len + dev->hard_header_len : skb->len;\n\n\tskb_len += len_diff;  \n\tif (skb_len <= dev_len) {\n\t\tret = BPF_MTU_CHK_RET_SUCCESS;\n\t\tgoto out;\n\t}\n\t \n\tif (skb_is_gso(skb)) {\n\t\tret = BPF_MTU_CHK_RET_SUCCESS;\n\n\t\tif (flags & BPF_MTU_CHK_SEGS &&\n\t\t    !skb_gso_validate_network_len(skb, mtu))\n\t\t\tret = BPF_MTU_CHK_RET_SEGS_TOOBIG;\n\t}\nout:\n\t \n\t*mtu_len = mtu;\n\n\treturn ret;\n}\n\nBPF_CALL_5(bpf_xdp_check_mtu, struct xdp_buff *, xdp,\n\t   u32, ifindex, u32 *, mtu_len, s32, len_diff, u64, flags)\n{\n\tstruct net_device *dev = xdp->rxq->dev;\n\tint xdp_len = xdp->data_end - xdp->data;\n\tint ret = BPF_MTU_CHK_RET_SUCCESS;\n\tint mtu, dev_len;\n\n\t \n\tif (unlikely(flags))\n\t\treturn -EINVAL;\n\n\tdev = __dev_via_ifindex(dev, ifindex);\n\tif (unlikely(!dev))\n\t\treturn -ENODEV;\n\n\tmtu = READ_ONCE(dev->mtu);\n\n\t \n\tdev_len = mtu + dev->hard_header_len;\n\n\t \n\tif (*mtu_len)\n\t\txdp_len = *mtu_len + dev->hard_header_len;\n\n\txdp_len += len_diff;  \n\tif (xdp_len > dev_len)\n\t\tret = BPF_MTU_CHK_RET_FRAG_NEEDED;\n\n\t \n\t*mtu_len = mtu;\n\n\treturn ret;\n}\n\nstatic const struct bpf_func_proto bpf_skb_check_mtu_proto = {\n\t.func\t\t= bpf_skb_check_mtu,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type      = ARG_PTR_TO_CTX,\n\t.arg2_type      = ARG_ANYTHING,\n\t.arg3_type      = ARG_PTR_TO_INT,\n\t.arg4_type      = ARG_ANYTHING,\n\t.arg5_type      = ARG_ANYTHING,\n};\n\nstatic const struct bpf_func_proto bpf_xdp_check_mtu_proto = {\n\t.func\t\t= bpf_xdp_check_mtu,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type      = ARG_PTR_TO_CTX,\n\t.arg2_type      = ARG_ANYTHING,\n\t.arg3_type      = ARG_PTR_TO_INT,\n\t.arg4_type      = ARG_ANYTHING,\n\t.arg5_type      = ARG_ANYTHING,\n};\n\n#if IS_ENABLED(CONFIG_IPV6_SEG6_BPF)\nstatic int bpf_push_seg6_encap(struct sk_buff *skb, u32 type, void *hdr, u32 len)\n{\n\tint err;\n\tstruct ipv6_sr_hdr *srh = (struct ipv6_sr_hdr *)hdr;\n\n\tif (!seg6_validate_srh(srh, len, false))\n\t\treturn -EINVAL;\n\n\tswitch (type) {\n\tcase BPF_LWT_ENCAP_SEG6_INLINE:\n\t\tif (skb->protocol != htons(ETH_P_IPV6))\n\t\t\treturn -EBADMSG;\n\n\t\terr = seg6_do_srh_inline(skb, srh);\n\t\tbreak;\n\tcase BPF_LWT_ENCAP_SEG6:\n\t\tskb_reset_inner_headers(skb);\n\t\tskb->encapsulation = 1;\n\t\terr = seg6_do_srh_encap(skb, srh, IPPROTO_IPV6);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tbpf_compute_data_pointers(skb);\n\tif (err)\n\t\treturn err;\n\n\tskb_set_transport_header(skb, sizeof(struct ipv6hdr));\n\n\treturn seg6_lookup_nexthop(skb, NULL, 0);\n}\n#endif  \n\n#if IS_ENABLED(CONFIG_LWTUNNEL_BPF)\nstatic int bpf_push_ip_encap(struct sk_buff *skb, void *hdr, u32 len,\n\t\t\t     bool ingress)\n{\n\treturn bpf_lwt_push_ip_encap(skb, hdr, len, ingress);\n}\n#endif\n\nBPF_CALL_4(bpf_lwt_in_push_encap, struct sk_buff *, skb, u32, type, void *, hdr,\n\t   u32, len)\n{\n\tswitch (type) {\n#if IS_ENABLED(CONFIG_IPV6_SEG6_BPF)\n\tcase BPF_LWT_ENCAP_SEG6:\n\tcase BPF_LWT_ENCAP_SEG6_INLINE:\n\t\treturn bpf_push_seg6_encap(skb, type, hdr, len);\n#endif\n#if IS_ENABLED(CONFIG_LWTUNNEL_BPF)\n\tcase BPF_LWT_ENCAP_IP:\n\t\treturn bpf_push_ip_encap(skb, hdr, len, true  );\n#endif\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nBPF_CALL_4(bpf_lwt_xmit_push_encap, struct sk_buff *, skb, u32, type,\n\t   void *, hdr, u32, len)\n{\n\tswitch (type) {\n#if IS_ENABLED(CONFIG_LWTUNNEL_BPF)\n\tcase BPF_LWT_ENCAP_IP:\n\t\treturn bpf_push_ip_encap(skb, hdr, len, false  );\n#endif\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic const struct bpf_func_proto bpf_lwt_in_push_encap_proto = {\n\t.func\t\t= bpf_lwt_in_push_encap,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg4_type\t= ARG_CONST_SIZE\n};\n\nstatic const struct bpf_func_proto bpf_lwt_xmit_push_encap_proto = {\n\t.func\t\t= bpf_lwt_xmit_push_encap,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg4_type\t= ARG_CONST_SIZE\n};\n\n#if IS_ENABLED(CONFIG_IPV6_SEG6_BPF)\nBPF_CALL_4(bpf_lwt_seg6_store_bytes, struct sk_buff *, skb, u32, offset,\n\t   const void *, from, u32, len)\n{\n\tstruct seg6_bpf_srh_state *srh_state =\n\t\tthis_cpu_ptr(&seg6_bpf_srh_states);\n\tstruct ipv6_sr_hdr *srh = srh_state->srh;\n\tvoid *srh_tlvs, *srh_end, *ptr;\n\tint srhoff = 0;\n\n\tif (srh == NULL)\n\t\treturn -EINVAL;\n\n\tsrh_tlvs = (void *)((char *)srh + ((srh->first_segment + 1) << 4));\n\tsrh_end = (void *)((char *)srh + sizeof(*srh) + srh_state->hdrlen);\n\n\tptr = skb->data + offset;\n\tif (ptr >= srh_tlvs && ptr + len <= srh_end)\n\t\tsrh_state->valid = false;\n\telse if (ptr < (void *)&srh->flags ||\n\t\t ptr + len > (void *)&srh->segments)\n\t\treturn -EFAULT;\n\n\tif (unlikely(bpf_try_make_writable(skb, offset + len)))\n\t\treturn -EFAULT;\n\tif (ipv6_find_hdr(skb, &srhoff, IPPROTO_ROUTING, NULL, NULL) < 0)\n\t\treturn -EINVAL;\n\tsrh_state->srh = (struct ipv6_sr_hdr *)(skb->data + srhoff);\n\n\tmemcpy(skb->data + offset, from, len);\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_lwt_seg6_store_bytes_proto = {\n\t.func\t\t= bpf_lwt_seg6_store_bytes,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg4_type\t= ARG_CONST_SIZE\n};\n\nstatic void bpf_update_srh_state(struct sk_buff *skb)\n{\n\tstruct seg6_bpf_srh_state *srh_state =\n\t\tthis_cpu_ptr(&seg6_bpf_srh_states);\n\tint srhoff = 0;\n\n\tif (ipv6_find_hdr(skb, &srhoff, IPPROTO_ROUTING, NULL, NULL) < 0) {\n\t\tsrh_state->srh = NULL;\n\t} else {\n\t\tsrh_state->srh = (struct ipv6_sr_hdr *)(skb->data + srhoff);\n\t\tsrh_state->hdrlen = srh_state->srh->hdrlen << 3;\n\t\tsrh_state->valid = true;\n\t}\n}\n\nBPF_CALL_4(bpf_lwt_seg6_action, struct sk_buff *, skb,\n\t   u32, action, void *, param, u32, param_len)\n{\n\tstruct seg6_bpf_srh_state *srh_state =\n\t\tthis_cpu_ptr(&seg6_bpf_srh_states);\n\tint hdroff = 0;\n\tint err;\n\n\tswitch (action) {\n\tcase SEG6_LOCAL_ACTION_END_X:\n\t\tif (!seg6_bpf_has_valid_srh(skb))\n\t\t\treturn -EBADMSG;\n\t\tif (param_len != sizeof(struct in6_addr))\n\t\t\treturn -EINVAL;\n\t\treturn seg6_lookup_nexthop(skb, (struct in6_addr *)param, 0);\n\tcase SEG6_LOCAL_ACTION_END_T:\n\t\tif (!seg6_bpf_has_valid_srh(skb))\n\t\t\treturn -EBADMSG;\n\t\tif (param_len != sizeof(int))\n\t\t\treturn -EINVAL;\n\t\treturn seg6_lookup_nexthop(skb, NULL, *(int *)param);\n\tcase SEG6_LOCAL_ACTION_END_DT6:\n\t\tif (!seg6_bpf_has_valid_srh(skb))\n\t\t\treturn -EBADMSG;\n\t\tif (param_len != sizeof(int))\n\t\t\treturn -EINVAL;\n\n\t\tif (ipv6_find_hdr(skb, &hdroff, IPPROTO_IPV6, NULL, NULL) < 0)\n\t\t\treturn -EBADMSG;\n\t\tif (!pskb_pull(skb, hdroff))\n\t\t\treturn -EBADMSG;\n\n\t\tskb_postpull_rcsum(skb, skb_network_header(skb), hdroff);\n\t\tskb_reset_network_header(skb);\n\t\tskb_reset_transport_header(skb);\n\t\tskb->encapsulation = 0;\n\n\t\tbpf_compute_data_pointers(skb);\n\t\tbpf_update_srh_state(skb);\n\t\treturn seg6_lookup_nexthop(skb, NULL, *(int *)param);\n\tcase SEG6_LOCAL_ACTION_END_B6:\n\t\tif (srh_state->srh && !seg6_bpf_has_valid_srh(skb))\n\t\t\treturn -EBADMSG;\n\t\terr = bpf_push_seg6_encap(skb, BPF_LWT_ENCAP_SEG6_INLINE,\n\t\t\t\t\t  param, param_len);\n\t\tif (!err)\n\t\t\tbpf_update_srh_state(skb);\n\n\t\treturn err;\n\tcase SEG6_LOCAL_ACTION_END_B6_ENCAP:\n\t\tif (srh_state->srh && !seg6_bpf_has_valid_srh(skb))\n\t\t\treturn -EBADMSG;\n\t\terr = bpf_push_seg6_encap(skb, BPF_LWT_ENCAP_SEG6,\n\t\t\t\t\t  param, param_len);\n\t\tif (!err)\n\t\t\tbpf_update_srh_state(skb);\n\n\t\treturn err;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic const struct bpf_func_proto bpf_lwt_seg6_action_proto = {\n\t.func\t\t= bpf_lwt_seg6_action,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg4_type\t= ARG_CONST_SIZE\n};\n\nBPF_CALL_3(bpf_lwt_seg6_adjust_srh, struct sk_buff *, skb, u32, offset,\n\t   s32, len)\n{\n\tstruct seg6_bpf_srh_state *srh_state =\n\t\tthis_cpu_ptr(&seg6_bpf_srh_states);\n\tstruct ipv6_sr_hdr *srh = srh_state->srh;\n\tvoid *srh_end, *srh_tlvs, *ptr;\n\tstruct ipv6hdr *hdr;\n\tint srhoff = 0;\n\tint ret;\n\n\tif (unlikely(srh == NULL))\n\t\treturn -EINVAL;\n\n\tsrh_tlvs = (void *)((unsigned char *)srh + sizeof(*srh) +\n\t\t\t((srh->first_segment + 1) << 4));\n\tsrh_end = (void *)((unsigned char *)srh + sizeof(*srh) +\n\t\t\tsrh_state->hdrlen);\n\tptr = skb->data + offset;\n\n\tif (unlikely(ptr < srh_tlvs || ptr > srh_end))\n\t\treturn -EFAULT;\n\tif (unlikely(len < 0 && (void *)((char *)ptr - len) > srh_end))\n\t\treturn -EFAULT;\n\n\tif (len > 0) {\n\t\tret = skb_cow_head(skb, len);\n\t\tif (unlikely(ret < 0))\n\t\t\treturn ret;\n\n\t\tret = bpf_skb_net_hdr_push(skb, offset, len);\n\t} else {\n\t\tret = bpf_skb_net_hdr_pop(skb, offset, -1 * len);\n\t}\n\n\tbpf_compute_data_pointers(skb);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\thdr = (struct ipv6hdr *)skb->data;\n\thdr->payload_len = htons(skb->len - sizeof(struct ipv6hdr));\n\n\tif (ipv6_find_hdr(skb, &srhoff, IPPROTO_ROUTING, NULL, NULL) < 0)\n\t\treturn -EINVAL;\n\tsrh_state->srh = (struct ipv6_sr_hdr *)(skb->data + srhoff);\n\tsrh_state->hdrlen += len;\n\tsrh_state->valid = false;\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_lwt_seg6_adjust_srh_proto = {\n\t.func\t\t= bpf_lwt_seg6_adjust_srh,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n};\n#endif  \n\n#ifdef CONFIG_INET\nstatic struct sock *sk_lookup(struct net *net, struct bpf_sock_tuple *tuple,\n\t\t\t      int dif, int sdif, u8 family, u8 proto)\n{\n\tstruct inet_hashinfo *hinfo = net->ipv4.tcp_death_row.hashinfo;\n\tbool refcounted = false;\n\tstruct sock *sk = NULL;\n\n\tif (family == AF_INET) {\n\t\t__be32 src4 = tuple->ipv4.saddr;\n\t\t__be32 dst4 = tuple->ipv4.daddr;\n\n\t\tif (proto == IPPROTO_TCP)\n\t\t\tsk = __inet_lookup(net, hinfo, NULL, 0,\n\t\t\t\t\t   src4, tuple->ipv4.sport,\n\t\t\t\t\t   dst4, tuple->ipv4.dport,\n\t\t\t\t\t   dif, sdif, &refcounted);\n\t\telse\n\t\t\tsk = __udp4_lib_lookup(net, src4, tuple->ipv4.sport,\n\t\t\t\t\t       dst4, tuple->ipv4.dport,\n\t\t\t\t\t       dif, sdif, net->ipv4.udp_table, NULL);\n#if IS_ENABLED(CONFIG_IPV6)\n\t} else {\n\t\tstruct in6_addr *src6 = (struct in6_addr *)&tuple->ipv6.saddr;\n\t\tstruct in6_addr *dst6 = (struct in6_addr *)&tuple->ipv6.daddr;\n\n\t\tif (proto == IPPROTO_TCP)\n\t\t\tsk = __inet6_lookup(net, hinfo, NULL, 0,\n\t\t\t\t\t    src6, tuple->ipv6.sport,\n\t\t\t\t\t    dst6, ntohs(tuple->ipv6.dport),\n\t\t\t\t\t    dif, sdif, &refcounted);\n\t\telse if (likely(ipv6_bpf_stub))\n\t\t\tsk = ipv6_bpf_stub->udp6_lib_lookup(net,\n\t\t\t\t\t\t\t    src6, tuple->ipv6.sport,\n\t\t\t\t\t\t\t    dst6, tuple->ipv6.dport,\n\t\t\t\t\t\t\t    dif, sdif,\n\t\t\t\t\t\t\t    net->ipv4.udp_table, NULL);\n#endif\n\t}\n\n\tif (unlikely(sk && !refcounted && !sock_flag(sk, SOCK_RCU_FREE))) {\n\t\tWARN_ONCE(1, \"Found non-RCU, unreferenced socket!\");\n\t\tsk = NULL;\n\t}\n\treturn sk;\n}\n\n \nstatic struct sock *\n__bpf_skc_lookup(struct sk_buff *skb, struct bpf_sock_tuple *tuple, u32 len,\n\t\t struct net *caller_net, u32 ifindex, u8 proto, u64 netns_id,\n\t\t u64 flags, int sdif)\n{\n\tstruct sock *sk = NULL;\n\tstruct net *net;\n\tu8 family;\n\n\tif (len == sizeof(tuple->ipv4))\n\t\tfamily = AF_INET;\n\telse if (len == sizeof(tuple->ipv6))\n\t\tfamily = AF_INET6;\n\telse\n\t\treturn NULL;\n\n\tif (unlikely(flags || !((s32)netns_id < 0 || netns_id <= S32_MAX)))\n\t\tgoto out;\n\n\tif (sdif < 0) {\n\t\tif (family == AF_INET)\n\t\t\tsdif = inet_sdif(skb);\n\t\telse\n\t\t\tsdif = inet6_sdif(skb);\n\t}\n\n\tif ((s32)netns_id < 0) {\n\t\tnet = caller_net;\n\t\tsk = sk_lookup(net, tuple, ifindex, sdif, family, proto);\n\t} else {\n\t\tnet = get_net_ns_by_id(caller_net, netns_id);\n\t\tif (unlikely(!net))\n\t\t\tgoto out;\n\t\tsk = sk_lookup(net, tuple, ifindex, sdif, family, proto);\n\t\tput_net(net);\n\t}\n\nout:\n\treturn sk;\n}\n\nstatic struct sock *\n__bpf_sk_lookup(struct sk_buff *skb, struct bpf_sock_tuple *tuple, u32 len,\n\t\tstruct net *caller_net, u32 ifindex, u8 proto, u64 netns_id,\n\t\tu64 flags, int sdif)\n{\n\tstruct sock *sk = __bpf_skc_lookup(skb, tuple, len, caller_net,\n\t\t\t\t\t   ifindex, proto, netns_id, flags,\n\t\t\t\t\t   sdif);\n\n\tif (sk) {\n\t\tstruct sock *sk2 = sk_to_full_sk(sk);\n\n\t\t \n\t\tif (!sk_fullsock(sk2))\n\t\t\tsk2 = NULL;\n\t\tif (sk2 != sk) {\n\t\t\tsock_gen_put(sk);\n\t\t\t \n\t\t\tif (unlikely(sk2 && !sock_flag(sk2, SOCK_RCU_FREE))) {\n\t\t\t\tWARN_ONCE(1, \"Found non-RCU, unreferenced socket!\");\n\t\t\t\treturn NULL;\n\t\t\t}\n\t\t\tsk = sk2;\n\t\t}\n\t}\n\n\treturn sk;\n}\n\nstatic struct sock *\nbpf_skc_lookup(struct sk_buff *skb, struct bpf_sock_tuple *tuple, u32 len,\n\t       u8 proto, u64 netns_id, u64 flags)\n{\n\tstruct net *caller_net;\n\tint ifindex;\n\n\tif (skb->dev) {\n\t\tcaller_net = dev_net(skb->dev);\n\t\tifindex = skb->dev->ifindex;\n\t} else {\n\t\tcaller_net = sock_net(skb->sk);\n\t\tifindex = 0;\n\t}\n\n\treturn __bpf_skc_lookup(skb, tuple, len, caller_net, ifindex, proto,\n\t\t\t\tnetns_id, flags, -1);\n}\n\nstatic struct sock *\nbpf_sk_lookup(struct sk_buff *skb, struct bpf_sock_tuple *tuple, u32 len,\n\t      u8 proto, u64 netns_id, u64 flags)\n{\n\tstruct sock *sk = bpf_skc_lookup(skb, tuple, len, proto, netns_id,\n\t\t\t\t\t flags);\n\n\tif (sk) {\n\t\tstruct sock *sk2 = sk_to_full_sk(sk);\n\n\t\t \n\t\tif (!sk_fullsock(sk2))\n\t\t\tsk2 = NULL;\n\t\tif (sk2 != sk) {\n\t\t\tsock_gen_put(sk);\n\t\t\t \n\t\t\tif (unlikely(sk2 && !sock_flag(sk2, SOCK_RCU_FREE))) {\n\t\t\t\tWARN_ONCE(1, \"Found non-RCU, unreferenced socket!\");\n\t\t\t\treturn NULL;\n\t\t\t}\n\t\t\tsk = sk2;\n\t\t}\n\t}\n\n\treturn sk;\n}\n\nBPF_CALL_5(bpf_skc_lookup_tcp, struct sk_buff *, skb,\n\t   struct bpf_sock_tuple *, tuple, u32, len, u64, netns_id, u64, flags)\n{\n\treturn (unsigned long)bpf_skc_lookup(skb, tuple, len, IPPROTO_TCP,\n\t\t\t\t\t     netns_id, flags);\n}\n\nstatic const struct bpf_func_proto bpf_skc_lookup_tcp_proto = {\n\t.func\t\t= bpf_skc_lookup_tcp,\n\t.gpl_only\t= false,\n\t.pkt_access\t= true,\n\t.ret_type\t= RET_PTR_TO_SOCK_COMMON_OR_NULL,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg3_type\t= ARG_CONST_SIZE,\n\t.arg4_type\t= ARG_ANYTHING,\n\t.arg5_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_5(bpf_sk_lookup_tcp, struct sk_buff *, skb,\n\t   struct bpf_sock_tuple *, tuple, u32, len, u64, netns_id, u64, flags)\n{\n\treturn (unsigned long)bpf_sk_lookup(skb, tuple, len, IPPROTO_TCP,\n\t\t\t\t\t    netns_id, flags);\n}\n\nstatic const struct bpf_func_proto bpf_sk_lookup_tcp_proto = {\n\t.func\t\t= bpf_sk_lookup_tcp,\n\t.gpl_only\t= false,\n\t.pkt_access\t= true,\n\t.ret_type\t= RET_PTR_TO_SOCKET_OR_NULL,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg3_type\t= ARG_CONST_SIZE,\n\t.arg4_type\t= ARG_ANYTHING,\n\t.arg5_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_5(bpf_sk_lookup_udp, struct sk_buff *, skb,\n\t   struct bpf_sock_tuple *, tuple, u32, len, u64, netns_id, u64, flags)\n{\n\treturn (unsigned long)bpf_sk_lookup(skb, tuple, len, IPPROTO_UDP,\n\t\t\t\t\t    netns_id, flags);\n}\n\nstatic const struct bpf_func_proto bpf_sk_lookup_udp_proto = {\n\t.func\t\t= bpf_sk_lookup_udp,\n\t.gpl_only\t= false,\n\t.pkt_access\t= true,\n\t.ret_type\t= RET_PTR_TO_SOCKET_OR_NULL,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg3_type\t= ARG_CONST_SIZE,\n\t.arg4_type\t= ARG_ANYTHING,\n\t.arg5_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_5(bpf_tc_skc_lookup_tcp, struct sk_buff *, skb,\n\t   struct bpf_sock_tuple *, tuple, u32, len, u64, netns_id, u64, flags)\n{\n\tstruct net_device *dev = skb->dev;\n\tint ifindex = dev->ifindex, sdif = dev_sdif(dev);\n\tstruct net *caller_net = dev_net(dev);\n\n\treturn (unsigned long)__bpf_skc_lookup(skb, tuple, len, caller_net,\n\t\t\t\t\t       ifindex, IPPROTO_TCP, netns_id,\n\t\t\t\t\t       flags, sdif);\n}\n\nstatic const struct bpf_func_proto bpf_tc_skc_lookup_tcp_proto = {\n\t.func\t\t= bpf_tc_skc_lookup_tcp,\n\t.gpl_only\t= false,\n\t.pkt_access\t= true,\n\t.ret_type\t= RET_PTR_TO_SOCK_COMMON_OR_NULL,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg3_type\t= ARG_CONST_SIZE,\n\t.arg4_type\t= ARG_ANYTHING,\n\t.arg5_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_5(bpf_tc_sk_lookup_tcp, struct sk_buff *, skb,\n\t   struct bpf_sock_tuple *, tuple, u32, len, u64, netns_id, u64, flags)\n{\n\tstruct net_device *dev = skb->dev;\n\tint ifindex = dev->ifindex, sdif = dev_sdif(dev);\n\tstruct net *caller_net = dev_net(dev);\n\n\treturn (unsigned long)__bpf_sk_lookup(skb, tuple, len, caller_net,\n\t\t\t\t\t      ifindex, IPPROTO_TCP, netns_id,\n\t\t\t\t\t      flags, sdif);\n}\n\nstatic const struct bpf_func_proto bpf_tc_sk_lookup_tcp_proto = {\n\t.func\t\t= bpf_tc_sk_lookup_tcp,\n\t.gpl_only\t= false,\n\t.pkt_access\t= true,\n\t.ret_type\t= RET_PTR_TO_SOCKET_OR_NULL,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg3_type\t= ARG_CONST_SIZE,\n\t.arg4_type\t= ARG_ANYTHING,\n\t.arg5_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_5(bpf_tc_sk_lookup_udp, struct sk_buff *, skb,\n\t   struct bpf_sock_tuple *, tuple, u32, len, u64, netns_id, u64, flags)\n{\n\tstruct net_device *dev = skb->dev;\n\tint ifindex = dev->ifindex, sdif = dev_sdif(dev);\n\tstruct net *caller_net = dev_net(dev);\n\n\treturn (unsigned long)__bpf_sk_lookup(skb, tuple, len, caller_net,\n\t\t\t\t\t      ifindex, IPPROTO_UDP, netns_id,\n\t\t\t\t\t      flags, sdif);\n}\n\nstatic const struct bpf_func_proto bpf_tc_sk_lookup_udp_proto = {\n\t.func\t\t= bpf_tc_sk_lookup_udp,\n\t.gpl_only\t= false,\n\t.pkt_access\t= true,\n\t.ret_type\t= RET_PTR_TO_SOCKET_OR_NULL,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg3_type\t= ARG_CONST_SIZE,\n\t.arg4_type\t= ARG_ANYTHING,\n\t.arg5_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_1(bpf_sk_release, struct sock *, sk)\n{\n\tif (sk && sk_is_refcounted(sk))\n\t\tsock_gen_put(sk);\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_sk_release_proto = {\n\t.func\t\t= bpf_sk_release,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_BTF_ID_SOCK_COMMON | OBJ_RELEASE,\n};\n\nBPF_CALL_5(bpf_xdp_sk_lookup_udp, struct xdp_buff *, ctx,\n\t   struct bpf_sock_tuple *, tuple, u32, len, u32, netns_id, u64, flags)\n{\n\tstruct net_device *dev = ctx->rxq->dev;\n\tint ifindex = dev->ifindex, sdif = dev_sdif(dev);\n\tstruct net *caller_net = dev_net(dev);\n\n\treturn (unsigned long)__bpf_sk_lookup(NULL, tuple, len, caller_net,\n\t\t\t\t\t      ifindex, IPPROTO_UDP, netns_id,\n\t\t\t\t\t      flags, sdif);\n}\n\nstatic const struct bpf_func_proto bpf_xdp_sk_lookup_udp_proto = {\n\t.func           = bpf_xdp_sk_lookup_udp,\n\t.gpl_only       = false,\n\t.pkt_access     = true,\n\t.ret_type       = RET_PTR_TO_SOCKET_OR_NULL,\n\t.arg1_type      = ARG_PTR_TO_CTX,\n\t.arg2_type      = ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg3_type      = ARG_CONST_SIZE,\n\t.arg4_type      = ARG_ANYTHING,\n\t.arg5_type      = ARG_ANYTHING,\n};\n\nBPF_CALL_5(bpf_xdp_skc_lookup_tcp, struct xdp_buff *, ctx,\n\t   struct bpf_sock_tuple *, tuple, u32, len, u32, netns_id, u64, flags)\n{\n\tstruct net_device *dev = ctx->rxq->dev;\n\tint ifindex = dev->ifindex, sdif = dev_sdif(dev);\n\tstruct net *caller_net = dev_net(dev);\n\n\treturn (unsigned long)__bpf_skc_lookup(NULL, tuple, len, caller_net,\n\t\t\t\t\t       ifindex, IPPROTO_TCP, netns_id,\n\t\t\t\t\t       flags, sdif);\n}\n\nstatic const struct bpf_func_proto bpf_xdp_skc_lookup_tcp_proto = {\n\t.func           = bpf_xdp_skc_lookup_tcp,\n\t.gpl_only       = false,\n\t.pkt_access     = true,\n\t.ret_type       = RET_PTR_TO_SOCK_COMMON_OR_NULL,\n\t.arg1_type      = ARG_PTR_TO_CTX,\n\t.arg2_type      = ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg3_type      = ARG_CONST_SIZE,\n\t.arg4_type      = ARG_ANYTHING,\n\t.arg5_type      = ARG_ANYTHING,\n};\n\nBPF_CALL_5(bpf_xdp_sk_lookup_tcp, struct xdp_buff *, ctx,\n\t   struct bpf_sock_tuple *, tuple, u32, len, u32, netns_id, u64, flags)\n{\n\tstruct net_device *dev = ctx->rxq->dev;\n\tint ifindex = dev->ifindex, sdif = dev_sdif(dev);\n\tstruct net *caller_net = dev_net(dev);\n\n\treturn (unsigned long)__bpf_sk_lookup(NULL, tuple, len, caller_net,\n\t\t\t\t\t      ifindex, IPPROTO_TCP, netns_id,\n\t\t\t\t\t      flags, sdif);\n}\n\nstatic const struct bpf_func_proto bpf_xdp_sk_lookup_tcp_proto = {\n\t.func           = bpf_xdp_sk_lookup_tcp,\n\t.gpl_only       = false,\n\t.pkt_access     = true,\n\t.ret_type       = RET_PTR_TO_SOCKET_OR_NULL,\n\t.arg1_type      = ARG_PTR_TO_CTX,\n\t.arg2_type      = ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg3_type      = ARG_CONST_SIZE,\n\t.arg4_type      = ARG_ANYTHING,\n\t.arg5_type      = ARG_ANYTHING,\n};\n\nBPF_CALL_5(bpf_sock_addr_skc_lookup_tcp, struct bpf_sock_addr_kern *, ctx,\n\t   struct bpf_sock_tuple *, tuple, u32, len, u64, netns_id, u64, flags)\n{\n\treturn (unsigned long)__bpf_skc_lookup(NULL, tuple, len,\n\t\t\t\t\t       sock_net(ctx->sk), 0,\n\t\t\t\t\t       IPPROTO_TCP, netns_id, flags,\n\t\t\t\t\t       -1);\n}\n\nstatic const struct bpf_func_proto bpf_sock_addr_skc_lookup_tcp_proto = {\n\t.func\t\t= bpf_sock_addr_skc_lookup_tcp,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_PTR_TO_SOCK_COMMON_OR_NULL,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg3_type\t= ARG_CONST_SIZE,\n\t.arg4_type\t= ARG_ANYTHING,\n\t.arg5_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_5(bpf_sock_addr_sk_lookup_tcp, struct bpf_sock_addr_kern *, ctx,\n\t   struct bpf_sock_tuple *, tuple, u32, len, u64, netns_id, u64, flags)\n{\n\treturn (unsigned long)__bpf_sk_lookup(NULL, tuple, len,\n\t\t\t\t\t      sock_net(ctx->sk), 0, IPPROTO_TCP,\n\t\t\t\t\t      netns_id, flags, -1);\n}\n\nstatic const struct bpf_func_proto bpf_sock_addr_sk_lookup_tcp_proto = {\n\t.func\t\t= bpf_sock_addr_sk_lookup_tcp,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_PTR_TO_SOCKET_OR_NULL,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg3_type\t= ARG_CONST_SIZE,\n\t.arg4_type\t= ARG_ANYTHING,\n\t.arg5_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_5(bpf_sock_addr_sk_lookup_udp, struct bpf_sock_addr_kern *, ctx,\n\t   struct bpf_sock_tuple *, tuple, u32, len, u64, netns_id, u64, flags)\n{\n\treturn (unsigned long)__bpf_sk_lookup(NULL, tuple, len,\n\t\t\t\t\t      sock_net(ctx->sk), 0, IPPROTO_UDP,\n\t\t\t\t\t      netns_id, flags, -1);\n}\n\nstatic const struct bpf_func_proto bpf_sock_addr_sk_lookup_udp_proto = {\n\t.func\t\t= bpf_sock_addr_sk_lookup_udp,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_PTR_TO_SOCKET_OR_NULL,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg3_type\t= ARG_CONST_SIZE,\n\t.arg4_type\t= ARG_ANYTHING,\n\t.arg5_type\t= ARG_ANYTHING,\n};\n\nbool bpf_tcp_sock_is_valid_access(int off, int size, enum bpf_access_type type,\n\t\t\t\t  struct bpf_insn_access_aux *info)\n{\n\tif (off < 0 || off >= offsetofend(struct bpf_tcp_sock,\n\t\t\t\t\t  icsk_retransmits))\n\t\treturn false;\n\n\tif (off % size != 0)\n\t\treturn false;\n\n\tswitch (off) {\n\tcase offsetof(struct bpf_tcp_sock, bytes_received):\n\tcase offsetof(struct bpf_tcp_sock, bytes_acked):\n\t\treturn size == sizeof(__u64);\n\tdefault:\n\t\treturn size == sizeof(__u32);\n\t}\n}\n\nu32 bpf_tcp_sock_convert_ctx_access(enum bpf_access_type type,\n\t\t\t\t    const struct bpf_insn *si,\n\t\t\t\t    struct bpf_insn *insn_buf,\n\t\t\t\t    struct bpf_prog *prog, u32 *target_size)\n{\n\tstruct bpf_insn *insn = insn_buf;\n\n#define BPF_TCP_SOCK_GET_COMMON(FIELD)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tBUILD_BUG_ON(sizeof_field(struct tcp_sock, FIELD) >\t\\\n\t\t\t     sizeof_field(struct bpf_tcp_sock, FIELD));\t\\\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct tcp_sock, FIELD),\\\n\t\t\t\t      si->dst_reg, si->src_reg,\t\t\\\n\t\t\t\t      offsetof(struct tcp_sock, FIELD)); \\\n\t} while (0)\n\n#define BPF_INET_SOCK_GET_COMMON(FIELD)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tBUILD_BUG_ON(sizeof_field(struct inet_connection_sock,\t\\\n\t\t\t\t\t  FIELD) >\t\t\t\\\n\t\t\t     sizeof_field(struct bpf_tcp_sock, FIELD));\t\\\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\t\t\t\\\n\t\t\t\t\tstruct inet_connection_sock,\t\\\n\t\t\t\t\tFIELD),\t\t\t\t\\\n\t\t\t\t      si->dst_reg, si->src_reg,\t\t\\\n\t\t\t\t      offsetof(\t\t\t\t\\\n\t\t\t\t\tstruct inet_connection_sock,\t\\\n\t\t\t\t\tFIELD));\t\t\t\\\n\t} while (0)\n\n\tBTF_TYPE_EMIT(struct bpf_tcp_sock);\n\n\tswitch (si->off) {\n\tcase offsetof(struct bpf_tcp_sock, rtt_min):\n\t\tBUILD_BUG_ON(sizeof_field(struct tcp_sock, rtt_min) !=\n\t\t\t     sizeof(struct minmax));\n\t\tBUILD_BUG_ON(sizeof(struct minmax) <\n\t\t\t     sizeof(struct minmax_sample));\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct tcp_sock, rtt_min) +\n\t\t\t\t      offsetof(struct minmax_sample, v));\n\t\tbreak;\n\tcase offsetof(struct bpf_tcp_sock, snd_cwnd):\n\t\tBPF_TCP_SOCK_GET_COMMON(snd_cwnd);\n\t\tbreak;\n\tcase offsetof(struct bpf_tcp_sock, srtt_us):\n\t\tBPF_TCP_SOCK_GET_COMMON(srtt_us);\n\t\tbreak;\n\tcase offsetof(struct bpf_tcp_sock, snd_ssthresh):\n\t\tBPF_TCP_SOCK_GET_COMMON(snd_ssthresh);\n\t\tbreak;\n\tcase offsetof(struct bpf_tcp_sock, rcv_nxt):\n\t\tBPF_TCP_SOCK_GET_COMMON(rcv_nxt);\n\t\tbreak;\n\tcase offsetof(struct bpf_tcp_sock, snd_nxt):\n\t\tBPF_TCP_SOCK_GET_COMMON(snd_nxt);\n\t\tbreak;\n\tcase offsetof(struct bpf_tcp_sock, snd_una):\n\t\tBPF_TCP_SOCK_GET_COMMON(snd_una);\n\t\tbreak;\n\tcase offsetof(struct bpf_tcp_sock, mss_cache):\n\t\tBPF_TCP_SOCK_GET_COMMON(mss_cache);\n\t\tbreak;\n\tcase offsetof(struct bpf_tcp_sock, ecn_flags):\n\t\tBPF_TCP_SOCK_GET_COMMON(ecn_flags);\n\t\tbreak;\n\tcase offsetof(struct bpf_tcp_sock, rate_delivered):\n\t\tBPF_TCP_SOCK_GET_COMMON(rate_delivered);\n\t\tbreak;\n\tcase offsetof(struct bpf_tcp_sock, rate_interval_us):\n\t\tBPF_TCP_SOCK_GET_COMMON(rate_interval_us);\n\t\tbreak;\n\tcase offsetof(struct bpf_tcp_sock, packets_out):\n\t\tBPF_TCP_SOCK_GET_COMMON(packets_out);\n\t\tbreak;\n\tcase offsetof(struct bpf_tcp_sock, retrans_out):\n\t\tBPF_TCP_SOCK_GET_COMMON(retrans_out);\n\t\tbreak;\n\tcase offsetof(struct bpf_tcp_sock, total_retrans):\n\t\tBPF_TCP_SOCK_GET_COMMON(total_retrans);\n\t\tbreak;\n\tcase offsetof(struct bpf_tcp_sock, segs_in):\n\t\tBPF_TCP_SOCK_GET_COMMON(segs_in);\n\t\tbreak;\n\tcase offsetof(struct bpf_tcp_sock, data_segs_in):\n\t\tBPF_TCP_SOCK_GET_COMMON(data_segs_in);\n\t\tbreak;\n\tcase offsetof(struct bpf_tcp_sock, segs_out):\n\t\tBPF_TCP_SOCK_GET_COMMON(segs_out);\n\t\tbreak;\n\tcase offsetof(struct bpf_tcp_sock, data_segs_out):\n\t\tBPF_TCP_SOCK_GET_COMMON(data_segs_out);\n\t\tbreak;\n\tcase offsetof(struct bpf_tcp_sock, lost_out):\n\t\tBPF_TCP_SOCK_GET_COMMON(lost_out);\n\t\tbreak;\n\tcase offsetof(struct bpf_tcp_sock, sacked_out):\n\t\tBPF_TCP_SOCK_GET_COMMON(sacked_out);\n\t\tbreak;\n\tcase offsetof(struct bpf_tcp_sock, bytes_received):\n\t\tBPF_TCP_SOCK_GET_COMMON(bytes_received);\n\t\tbreak;\n\tcase offsetof(struct bpf_tcp_sock, bytes_acked):\n\t\tBPF_TCP_SOCK_GET_COMMON(bytes_acked);\n\t\tbreak;\n\tcase offsetof(struct bpf_tcp_sock, dsack_dups):\n\t\tBPF_TCP_SOCK_GET_COMMON(dsack_dups);\n\t\tbreak;\n\tcase offsetof(struct bpf_tcp_sock, delivered):\n\t\tBPF_TCP_SOCK_GET_COMMON(delivered);\n\t\tbreak;\n\tcase offsetof(struct bpf_tcp_sock, delivered_ce):\n\t\tBPF_TCP_SOCK_GET_COMMON(delivered_ce);\n\t\tbreak;\n\tcase offsetof(struct bpf_tcp_sock, icsk_retransmits):\n\t\tBPF_INET_SOCK_GET_COMMON(icsk_retransmits);\n\t\tbreak;\n\t}\n\n\treturn insn - insn_buf;\n}\n\nBPF_CALL_1(bpf_tcp_sock, struct sock *, sk)\n{\n\tif (sk_fullsock(sk) && sk->sk_protocol == IPPROTO_TCP)\n\t\treturn (unsigned long)sk;\n\n\treturn (unsigned long)NULL;\n}\n\nconst struct bpf_func_proto bpf_tcp_sock_proto = {\n\t.func\t\t= bpf_tcp_sock,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_PTR_TO_TCP_SOCK_OR_NULL,\n\t.arg1_type\t= ARG_PTR_TO_SOCK_COMMON,\n};\n\nBPF_CALL_1(bpf_get_listener_sock, struct sock *, sk)\n{\n\tsk = sk_to_full_sk(sk);\n\n\tif (sk->sk_state == TCP_LISTEN && sock_flag(sk, SOCK_RCU_FREE))\n\t\treturn (unsigned long)sk;\n\n\treturn (unsigned long)NULL;\n}\n\nstatic const struct bpf_func_proto bpf_get_listener_sock_proto = {\n\t.func\t\t= bpf_get_listener_sock,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_PTR_TO_SOCKET_OR_NULL,\n\t.arg1_type\t= ARG_PTR_TO_SOCK_COMMON,\n};\n\nBPF_CALL_1(bpf_skb_ecn_set_ce, struct sk_buff *, skb)\n{\n\tunsigned int iphdr_len;\n\n\tswitch (skb_protocol(skb, true)) {\n\tcase cpu_to_be16(ETH_P_IP):\n\t\tiphdr_len = sizeof(struct iphdr);\n\t\tbreak;\n\tcase cpu_to_be16(ETH_P_IPV6):\n\t\tiphdr_len = sizeof(struct ipv6hdr);\n\t\tbreak;\n\tdefault:\n\t\treturn 0;\n\t}\n\n\tif (skb_headlen(skb) < iphdr_len)\n\t\treturn 0;\n\n\tif (skb_cloned(skb) && !skb_clone_writable(skb, iphdr_len))\n\t\treturn 0;\n\n\treturn INET_ECN_set_ce(skb);\n}\n\nbool bpf_xdp_sock_is_valid_access(int off, int size, enum bpf_access_type type,\n\t\t\t\t  struct bpf_insn_access_aux *info)\n{\n\tif (off < 0 || off >= offsetofend(struct bpf_xdp_sock, queue_id))\n\t\treturn false;\n\n\tif (off % size != 0)\n\t\treturn false;\n\n\tswitch (off) {\n\tdefault:\n\t\treturn size == sizeof(__u32);\n\t}\n}\n\nu32 bpf_xdp_sock_convert_ctx_access(enum bpf_access_type type,\n\t\t\t\t    const struct bpf_insn *si,\n\t\t\t\t    struct bpf_insn *insn_buf,\n\t\t\t\t    struct bpf_prog *prog, u32 *target_size)\n{\n\tstruct bpf_insn *insn = insn_buf;\n\n#define BPF_XDP_SOCK_GET(FIELD)\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tBUILD_BUG_ON(sizeof_field(struct xdp_sock, FIELD) >\t\\\n\t\t\t     sizeof_field(struct bpf_xdp_sock, FIELD));\t\\\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct xdp_sock, FIELD),\\\n\t\t\t\t      si->dst_reg, si->src_reg,\t\t\\\n\t\t\t\t      offsetof(struct xdp_sock, FIELD)); \\\n\t} while (0)\n\n\tswitch (si->off) {\n\tcase offsetof(struct bpf_xdp_sock, queue_id):\n\t\tBPF_XDP_SOCK_GET(queue_id);\n\t\tbreak;\n\t}\n\n\treturn insn - insn_buf;\n}\n\nstatic const struct bpf_func_proto bpf_skb_ecn_set_ce_proto = {\n\t.func           = bpf_skb_ecn_set_ce,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type      = ARG_PTR_TO_CTX,\n};\n\nBPF_CALL_5(bpf_tcp_check_syncookie, struct sock *, sk, void *, iph, u32, iph_len,\n\t   struct tcphdr *, th, u32, th_len)\n{\n#ifdef CONFIG_SYN_COOKIES\n\tu32 cookie;\n\tint ret;\n\n\tif (unlikely(!sk || th_len < sizeof(*th)))\n\t\treturn -EINVAL;\n\n\t \n\tif (sk->sk_protocol != IPPROTO_TCP || sk->sk_state != TCP_LISTEN)\n\t\treturn -EINVAL;\n\n\tif (!READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_syncookies))\n\t\treturn -EINVAL;\n\n\tif (!th->ack || th->rst || th->syn)\n\t\treturn -ENOENT;\n\n\tif (unlikely(iph_len < sizeof(struct iphdr)))\n\t\treturn -EINVAL;\n\n\tif (tcp_synq_no_recent_overflow(sk))\n\t\treturn -ENOENT;\n\n\tcookie = ntohl(th->ack_seq) - 1;\n\n\t \n\tswitch (((struct iphdr *)iph)->version) {\n\tcase 4:\n\t\tif (sk->sk_family == AF_INET6 && ipv6_only_sock(sk))\n\t\t\treturn -EINVAL;\n\n\t\tret = __cookie_v4_check((struct iphdr *)iph, th, cookie);\n\t\tbreak;\n\n#if IS_BUILTIN(CONFIG_IPV6)\n\tcase 6:\n\t\tif (unlikely(iph_len < sizeof(struct ipv6hdr)))\n\t\t\treturn -EINVAL;\n\n\t\tif (sk->sk_family != AF_INET6)\n\t\t\treturn -EINVAL;\n\n\t\tret = __cookie_v6_check((struct ipv6hdr *)iph, th, cookie);\n\t\tbreak;\n#endif  \n\n\tdefault:\n\t\treturn -EPROTONOSUPPORT;\n\t}\n\n\tif (ret > 0)\n\t\treturn 0;\n\n\treturn -ENOENT;\n#else\n\treturn -ENOTSUPP;\n#endif\n}\n\nstatic const struct bpf_func_proto bpf_tcp_check_syncookie_proto = {\n\t.func\t\t= bpf_tcp_check_syncookie,\n\t.gpl_only\t= true,\n\t.pkt_access\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_BTF_ID_SOCK_COMMON,\n\t.arg2_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg3_type\t= ARG_CONST_SIZE,\n\t.arg4_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg5_type\t= ARG_CONST_SIZE,\n};\n\nBPF_CALL_5(bpf_tcp_gen_syncookie, struct sock *, sk, void *, iph, u32, iph_len,\n\t   struct tcphdr *, th, u32, th_len)\n{\n#ifdef CONFIG_SYN_COOKIES\n\tu32 cookie;\n\tu16 mss;\n\n\tif (unlikely(!sk || th_len < sizeof(*th) || th_len != th->doff * 4))\n\t\treturn -EINVAL;\n\n\tif (sk->sk_protocol != IPPROTO_TCP || sk->sk_state != TCP_LISTEN)\n\t\treturn -EINVAL;\n\n\tif (!READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_syncookies))\n\t\treturn -ENOENT;\n\n\tif (!th->syn || th->ack || th->fin || th->rst)\n\t\treturn -EINVAL;\n\n\tif (unlikely(iph_len < sizeof(struct iphdr)))\n\t\treturn -EINVAL;\n\n\t \n\tswitch (((struct iphdr *)iph)->version) {\n\tcase 4:\n\t\tif (sk->sk_family == AF_INET6 && ipv6_only_sock(sk))\n\t\t\treturn -EINVAL;\n\n\t\tmss = tcp_v4_get_syncookie(sk, iph, th, &cookie);\n\t\tbreak;\n\n#if IS_BUILTIN(CONFIG_IPV6)\n\tcase 6:\n\t\tif (unlikely(iph_len < sizeof(struct ipv6hdr)))\n\t\t\treturn -EINVAL;\n\n\t\tif (sk->sk_family != AF_INET6)\n\t\t\treturn -EINVAL;\n\n\t\tmss = tcp_v6_get_syncookie(sk, iph, th, &cookie);\n\t\tbreak;\n#endif  \n\n\tdefault:\n\t\treturn -EPROTONOSUPPORT;\n\t}\n\tif (mss == 0)\n\t\treturn -ENOENT;\n\n\treturn cookie | ((u64)mss << 32);\n#else\n\treturn -EOPNOTSUPP;\n#endif  \n}\n\nstatic const struct bpf_func_proto bpf_tcp_gen_syncookie_proto = {\n\t.func\t\t= bpf_tcp_gen_syncookie,\n\t.gpl_only\t= true,  \n\t.pkt_access\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_BTF_ID_SOCK_COMMON,\n\t.arg2_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg3_type\t= ARG_CONST_SIZE,\n\t.arg4_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg5_type\t= ARG_CONST_SIZE,\n};\n\nBPF_CALL_3(bpf_sk_assign, struct sk_buff *, skb, struct sock *, sk, u64, flags)\n{\n\tif (!sk || flags != 0)\n\t\treturn -EINVAL;\n\tif (!skb_at_tc_ingress(skb))\n\t\treturn -EOPNOTSUPP;\n\tif (unlikely(dev_net(skb->dev) != sock_net(sk)))\n\t\treturn -ENETUNREACH;\n\tif (sk_unhashed(sk))\n\t\treturn -EOPNOTSUPP;\n\tif (sk_is_refcounted(sk) &&\n\t    unlikely(!refcount_inc_not_zero(&sk->sk_refcnt)))\n\t\treturn -ENOENT;\n\n\tskb_orphan(skb);\n\tskb->sk = sk;\n\tskb->destructor = sock_pfree;\n\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_sk_assign_proto = {\n\t.func\t\t= bpf_sk_assign,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type      = ARG_PTR_TO_CTX,\n\t.arg2_type      = ARG_PTR_TO_BTF_ID_SOCK_COMMON,\n\t.arg3_type\t= ARG_ANYTHING,\n};\n\nstatic const u8 *bpf_search_tcp_opt(const u8 *op, const u8 *opend,\n\t\t\t\t    u8 search_kind, const u8 *magic,\n\t\t\t\t    u8 magic_len, bool *eol)\n{\n\tu8 kind, kind_len;\n\n\t*eol = false;\n\n\twhile (op < opend) {\n\t\tkind = op[0];\n\n\t\tif (kind == TCPOPT_EOL) {\n\t\t\t*eol = true;\n\t\t\treturn ERR_PTR(-ENOMSG);\n\t\t} else if (kind == TCPOPT_NOP) {\n\t\t\top++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (opend - op < 2 || opend - op < op[1] || op[1] < 2)\n\t\t\t \n\t\t\treturn ERR_PTR(-EFAULT);\n\n\t\tkind_len = op[1];\n\t\tif (search_kind == kind) {\n\t\t\tif (!magic_len)\n\t\t\t\treturn op;\n\n\t\t\tif (magic_len > kind_len - 2)\n\t\t\t\treturn ERR_PTR(-ENOMSG);\n\n\t\t\tif (!memcmp(&op[2], magic, magic_len))\n\t\t\t\treturn op;\n\t\t}\n\n\t\top += kind_len;\n\t}\n\n\treturn ERR_PTR(-ENOMSG);\n}\n\nBPF_CALL_4(bpf_sock_ops_load_hdr_opt, struct bpf_sock_ops_kern *, bpf_sock,\n\t   void *, search_res, u32, len, u64, flags)\n{\n\tbool eol, load_syn = flags & BPF_LOAD_HDR_OPT_TCP_SYN;\n\tconst u8 *op, *opend, *magic, *search = search_res;\n\tu8 search_kind, search_len, copy_len, magic_len;\n\tint ret;\n\n\t \n\tif (len < 2 || flags & ~BPF_LOAD_HDR_OPT_TCP_SYN)\n\t\treturn -EINVAL;\n\n\tsearch_kind = search[0];\n\tsearch_len = search[1];\n\n\tif (search_len > len || search_kind == TCPOPT_NOP ||\n\t    search_kind == TCPOPT_EOL)\n\t\treturn -EINVAL;\n\n\tif (search_kind == TCPOPT_EXP || search_kind == 253) {\n\t\t \n\t\tif (search_len != 4 && search_len != 6)\n\t\t\treturn -EINVAL;\n\t\tmagic = &search[2];\n\t\tmagic_len = search_len - 2;\n\t} else {\n\t\tif (search_len)\n\t\t\treturn -EINVAL;\n\t\tmagic = NULL;\n\t\tmagic_len = 0;\n\t}\n\n\tif (load_syn) {\n\t\tret = bpf_sock_ops_get_syn(bpf_sock, TCP_BPF_SYN, &op);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\topend = op + ret;\n\t\top += sizeof(struct tcphdr);\n\t} else {\n\t\tif (!bpf_sock->skb ||\n\t\t    bpf_sock->op == BPF_SOCK_OPS_HDR_OPT_LEN_CB)\n\t\t\t \n\t\t\treturn -EPERM;\n\n\t\topend = bpf_sock->skb_data_end;\n\t\top = bpf_sock->skb->data + sizeof(struct tcphdr);\n\t}\n\n\top = bpf_search_tcp_opt(op, opend, search_kind, magic, magic_len,\n\t\t\t\t&eol);\n\tif (IS_ERR(op))\n\t\treturn PTR_ERR(op);\n\n\tcopy_len = op[1];\n\tret = copy_len;\n\tif (copy_len > len) {\n\t\tret = -ENOSPC;\n\t\tcopy_len = len;\n\t}\n\n\tmemcpy(search_res, op, copy_len);\n\treturn ret;\n}\n\nstatic const struct bpf_func_proto bpf_sock_ops_load_hdr_opt_proto = {\n\t.func\t\t= bpf_sock_ops_load_hdr_opt,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_PTR_TO_MEM,\n\t.arg3_type\t= ARG_CONST_SIZE,\n\t.arg4_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_4(bpf_sock_ops_store_hdr_opt, struct bpf_sock_ops_kern *, bpf_sock,\n\t   const void *, from, u32, len, u64, flags)\n{\n\tu8 new_kind, new_kind_len, magic_len = 0, *opend;\n\tconst u8 *op, *new_op, *magic = NULL;\n\tstruct sk_buff *skb;\n\tbool eol;\n\n\tif (bpf_sock->op != BPF_SOCK_OPS_WRITE_HDR_OPT_CB)\n\t\treturn -EPERM;\n\n\tif (len < 2 || flags)\n\t\treturn -EINVAL;\n\n\tnew_op = from;\n\tnew_kind = new_op[0];\n\tnew_kind_len = new_op[1];\n\n\tif (new_kind_len > len || new_kind == TCPOPT_NOP ||\n\t    new_kind == TCPOPT_EOL)\n\t\treturn -EINVAL;\n\n\tif (new_kind_len > bpf_sock->remaining_opt_len)\n\t\treturn -ENOSPC;\n\n\t \n\tif (new_kind == TCPOPT_EXP || new_kind == 253)  {\n\t\tif (new_kind_len < 4)\n\t\t\treturn -EINVAL;\n\t\t \n\t\tmagic = &new_op[2];\n\t\tmagic_len = 2;\n\t}\n\n\t \n\tskb = bpf_sock->skb;\n\top = skb->data + sizeof(struct tcphdr);\n\topend = bpf_sock->skb_data_end;\n\n\top = bpf_search_tcp_opt(op, opend, new_kind, magic, magic_len,\n\t\t\t\t&eol);\n\tif (!IS_ERR(op))\n\t\treturn -EEXIST;\n\n\tif (PTR_ERR(op) != -ENOMSG)\n\t\treturn PTR_ERR(op);\n\n\tif (eol)\n\t\t \n\t\treturn -ENOSPC;\n\n\t \n\tmemcpy(opend, from, new_kind_len);\n\n\tbpf_sock->remaining_opt_len -= new_kind_len;\n\tbpf_sock->skb_data_end += new_kind_len;\n\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_sock_ops_store_hdr_opt_proto = {\n\t.func\t\t= bpf_sock_ops_store_hdr_opt,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg3_type\t= ARG_CONST_SIZE,\n\t.arg4_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_3(bpf_sock_ops_reserve_hdr_opt, struct bpf_sock_ops_kern *, bpf_sock,\n\t   u32, len, u64, flags)\n{\n\tif (bpf_sock->op != BPF_SOCK_OPS_HDR_OPT_LEN_CB)\n\t\treturn -EPERM;\n\n\tif (flags || len < 2)\n\t\treturn -EINVAL;\n\n\tif (len > bpf_sock->remaining_opt_len)\n\t\treturn -ENOSPC;\n\n\tbpf_sock->remaining_opt_len -= len;\n\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_sock_ops_reserve_hdr_opt_proto = {\n\t.func\t\t= bpf_sock_ops_reserve_hdr_opt,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_3(bpf_skb_set_tstamp, struct sk_buff *, skb,\n\t   u64, tstamp, u32, tstamp_type)\n{\n\t \n\tif (skb->protocol != htons(ETH_P_IP) &&\n\t    skb->protocol != htons(ETH_P_IPV6))\n\t\treturn -EOPNOTSUPP;\n\n\tswitch (tstamp_type) {\n\tcase BPF_SKB_TSTAMP_DELIVERY_MONO:\n\t\tif (!tstamp)\n\t\t\treturn -EINVAL;\n\t\tskb->tstamp = tstamp;\n\t\tskb->mono_delivery_time = 1;\n\t\tbreak;\n\tcase BPF_SKB_TSTAMP_UNSPEC:\n\t\tif (tstamp)\n\t\t\treturn -EINVAL;\n\t\tskb->tstamp = 0;\n\t\tskb->mono_delivery_time = 0;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_skb_set_tstamp_proto = {\n\t.func           = bpf_skb_set_tstamp,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type      = ARG_PTR_TO_CTX,\n\t.arg2_type      = ARG_ANYTHING,\n\t.arg3_type      = ARG_ANYTHING,\n};\n\n#ifdef CONFIG_SYN_COOKIES\nBPF_CALL_3(bpf_tcp_raw_gen_syncookie_ipv4, struct iphdr *, iph,\n\t   struct tcphdr *, th, u32, th_len)\n{\n\tu32 cookie;\n\tu16 mss;\n\n\tif (unlikely(th_len < sizeof(*th) || th_len != th->doff * 4))\n\t\treturn -EINVAL;\n\n\tmss = tcp_parse_mss_option(th, 0) ?: TCP_MSS_DEFAULT;\n\tcookie = __cookie_v4_init_sequence(iph, th, &mss);\n\n\treturn cookie | ((u64)mss << 32);\n}\n\nstatic const struct bpf_func_proto bpf_tcp_raw_gen_syncookie_ipv4_proto = {\n\t.func\t\t= bpf_tcp_raw_gen_syncookie_ipv4,\n\t.gpl_only\t= true,  \n\t.pkt_access\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_FIXED_SIZE_MEM,\n\t.arg1_size\t= sizeof(struct iphdr),\n\t.arg2_type\t= ARG_PTR_TO_MEM,\n\t.arg3_type\t= ARG_CONST_SIZE_OR_ZERO,\n};\n\nBPF_CALL_3(bpf_tcp_raw_gen_syncookie_ipv6, struct ipv6hdr *, iph,\n\t   struct tcphdr *, th, u32, th_len)\n{\n#if IS_BUILTIN(CONFIG_IPV6)\n\tconst u16 mss_clamp = IPV6_MIN_MTU - sizeof(struct tcphdr) -\n\t\tsizeof(struct ipv6hdr);\n\tu32 cookie;\n\tu16 mss;\n\n\tif (unlikely(th_len < sizeof(*th) || th_len != th->doff * 4))\n\t\treturn -EINVAL;\n\n\tmss = tcp_parse_mss_option(th, 0) ?: mss_clamp;\n\tcookie = __cookie_v6_init_sequence(iph, th, &mss);\n\n\treturn cookie | ((u64)mss << 32);\n#else\n\treturn -EPROTONOSUPPORT;\n#endif\n}\n\nstatic const struct bpf_func_proto bpf_tcp_raw_gen_syncookie_ipv6_proto = {\n\t.func\t\t= bpf_tcp_raw_gen_syncookie_ipv6,\n\t.gpl_only\t= true,  \n\t.pkt_access\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_FIXED_SIZE_MEM,\n\t.arg1_size\t= sizeof(struct ipv6hdr),\n\t.arg2_type\t= ARG_PTR_TO_MEM,\n\t.arg3_type\t= ARG_CONST_SIZE_OR_ZERO,\n};\n\nBPF_CALL_2(bpf_tcp_raw_check_syncookie_ipv4, struct iphdr *, iph,\n\t   struct tcphdr *, th)\n{\n\tu32 cookie = ntohl(th->ack_seq) - 1;\n\n\tif (__cookie_v4_check(iph, th, cookie) > 0)\n\t\treturn 0;\n\n\treturn -EACCES;\n}\n\nstatic const struct bpf_func_proto bpf_tcp_raw_check_syncookie_ipv4_proto = {\n\t.func\t\t= bpf_tcp_raw_check_syncookie_ipv4,\n\t.gpl_only\t= true,  \n\t.pkt_access\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_FIXED_SIZE_MEM,\n\t.arg1_size\t= sizeof(struct iphdr),\n\t.arg2_type\t= ARG_PTR_TO_FIXED_SIZE_MEM,\n\t.arg2_size\t= sizeof(struct tcphdr),\n};\n\nBPF_CALL_2(bpf_tcp_raw_check_syncookie_ipv6, struct ipv6hdr *, iph,\n\t   struct tcphdr *, th)\n{\n#if IS_BUILTIN(CONFIG_IPV6)\n\tu32 cookie = ntohl(th->ack_seq) - 1;\n\n\tif (__cookie_v6_check(iph, th, cookie) > 0)\n\t\treturn 0;\n\n\treturn -EACCES;\n#else\n\treturn -EPROTONOSUPPORT;\n#endif\n}\n\nstatic const struct bpf_func_proto bpf_tcp_raw_check_syncookie_ipv6_proto = {\n\t.func\t\t= bpf_tcp_raw_check_syncookie_ipv6,\n\t.gpl_only\t= true,  \n\t.pkt_access\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_FIXED_SIZE_MEM,\n\t.arg1_size\t= sizeof(struct ipv6hdr),\n\t.arg2_type\t= ARG_PTR_TO_FIXED_SIZE_MEM,\n\t.arg2_size\t= sizeof(struct tcphdr),\n};\n#endif  \n\n#endif  \n\nbool bpf_helper_changes_pkt_data(void *func)\n{\n\tif (func == bpf_skb_vlan_push ||\n\t    func == bpf_skb_vlan_pop ||\n\t    func == bpf_skb_store_bytes ||\n\t    func == bpf_skb_change_proto ||\n\t    func == bpf_skb_change_head ||\n\t    func == sk_skb_change_head ||\n\t    func == bpf_skb_change_tail ||\n\t    func == sk_skb_change_tail ||\n\t    func == bpf_skb_adjust_room ||\n\t    func == sk_skb_adjust_room ||\n\t    func == bpf_skb_pull_data ||\n\t    func == sk_skb_pull_data ||\n\t    func == bpf_clone_redirect ||\n\t    func == bpf_l3_csum_replace ||\n\t    func == bpf_l4_csum_replace ||\n\t    func == bpf_xdp_adjust_head ||\n\t    func == bpf_xdp_adjust_meta ||\n\t    func == bpf_msg_pull_data ||\n\t    func == bpf_msg_push_data ||\n\t    func == bpf_msg_pop_data ||\n\t    func == bpf_xdp_adjust_tail ||\n#if IS_ENABLED(CONFIG_IPV6_SEG6_BPF)\n\t    func == bpf_lwt_seg6_store_bytes ||\n\t    func == bpf_lwt_seg6_adjust_srh ||\n\t    func == bpf_lwt_seg6_action ||\n#endif\n#ifdef CONFIG_INET\n\t    func == bpf_sock_ops_store_hdr_opt ||\n#endif\n\t    func == bpf_lwt_in_push_encap ||\n\t    func == bpf_lwt_xmit_push_encap)\n\t\treturn true;\n\n\treturn false;\n}\n\nconst struct bpf_func_proto bpf_event_output_data_proto __weak;\nconst struct bpf_func_proto bpf_sk_storage_get_cg_sock_proto __weak;\n\nstatic const struct bpf_func_proto *\nsock_filter_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tconst struct bpf_func_proto *func_proto;\n\n\tfunc_proto = cgroup_common_func_proto(func_id, prog);\n\tif (func_proto)\n\t\treturn func_proto;\n\n\tfunc_proto = cgroup_current_func_proto(func_id, prog);\n\tif (func_proto)\n\t\treturn func_proto;\n\n\tswitch (func_id) {\n\tcase BPF_FUNC_get_socket_cookie:\n\t\treturn &bpf_get_socket_cookie_sock_proto;\n\tcase BPF_FUNC_get_netns_cookie:\n\t\treturn &bpf_get_netns_cookie_sock_proto;\n\tcase BPF_FUNC_perf_event_output:\n\t\treturn &bpf_event_output_data_proto;\n\tcase BPF_FUNC_sk_storage_get:\n\t\treturn &bpf_sk_storage_get_cg_sock_proto;\n\tcase BPF_FUNC_ktime_get_coarse_ns:\n\t\treturn &bpf_ktime_get_coarse_ns_proto;\n\tdefault:\n\t\treturn bpf_base_func_proto(func_id);\n\t}\n}\n\nstatic const struct bpf_func_proto *\nsock_addr_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tconst struct bpf_func_proto *func_proto;\n\n\tfunc_proto = cgroup_common_func_proto(func_id, prog);\n\tif (func_proto)\n\t\treturn func_proto;\n\n\tfunc_proto = cgroup_current_func_proto(func_id, prog);\n\tif (func_proto)\n\t\treturn func_proto;\n\n\tswitch (func_id) {\n\tcase BPF_FUNC_bind:\n\t\tswitch (prog->expected_attach_type) {\n\t\tcase BPF_CGROUP_INET4_CONNECT:\n\t\tcase BPF_CGROUP_INET6_CONNECT:\n\t\t\treturn &bpf_bind_proto;\n\t\tdefault:\n\t\t\treturn NULL;\n\t\t}\n\tcase BPF_FUNC_get_socket_cookie:\n\t\treturn &bpf_get_socket_cookie_sock_addr_proto;\n\tcase BPF_FUNC_get_netns_cookie:\n\t\treturn &bpf_get_netns_cookie_sock_addr_proto;\n\tcase BPF_FUNC_perf_event_output:\n\t\treturn &bpf_event_output_data_proto;\n#ifdef CONFIG_INET\n\tcase BPF_FUNC_sk_lookup_tcp:\n\t\treturn &bpf_sock_addr_sk_lookup_tcp_proto;\n\tcase BPF_FUNC_sk_lookup_udp:\n\t\treturn &bpf_sock_addr_sk_lookup_udp_proto;\n\tcase BPF_FUNC_sk_release:\n\t\treturn &bpf_sk_release_proto;\n\tcase BPF_FUNC_skc_lookup_tcp:\n\t\treturn &bpf_sock_addr_skc_lookup_tcp_proto;\n#endif  \n\tcase BPF_FUNC_sk_storage_get:\n\t\treturn &bpf_sk_storage_get_proto;\n\tcase BPF_FUNC_sk_storage_delete:\n\t\treturn &bpf_sk_storage_delete_proto;\n\tcase BPF_FUNC_setsockopt:\n\t\tswitch (prog->expected_attach_type) {\n\t\tcase BPF_CGROUP_INET4_BIND:\n\t\tcase BPF_CGROUP_INET6_BIND:\n\t\tcase BPF_CGROUP_INET4_CONNECT:\n\t\tcase BPF_CGROUP_INET6_CONNECT:\n\t\tcase BPF_CGROUP_UDP4_RECVMSG:\n\t\tcase BPF_CGROUP_UDP6_RECVMSG:\n\t\tcase BPF_CGROUP_UDP4_SENDMSG:\n\t\tcase BPF_CGROUP_UDP6_SENDMSG:\n\t\tcase BPF_CGROUP_INET4_GETPEERNAME:\n\t\tcase BPF_CGROUP_INET6_GETPEERNAME:\n\t\tcase BPF_CGROUP_INET4_GETSOCKNAME:\n\t\tcase BPF_CGROUP_INET6_GETSOCKNAME:\n\t\t\treturn &bpf_sock_addr_setsockopt_proto;\n\t\tdefault:\n\t\t\treturn NULL;\n\t\t}\n\tcase BPF_FUNC_getsockopt:\n\t\tswitch (prog->expected_attach_type) {\n\t\tcase BPF_CGROUP_INET4_BIND:\n\t\tcase BPF_CGROUP_INET6_BIND:\n\t\tcase BPF_CGROUP_INET4_CONNECT:\n\t\tcase BPF_CGROUP_INET6_CONNECT:\n\t\tcase BPF_CGROUP_UDP4_RECVMSG:\n\t\tcase BPF_CGROUP_UDP6_RECVMSG:\n\t\tcase BPF_CGROUP_UDP4_SENDMSG:\n\t\tcase BPF_CGROUP_UDP6_SENDMSG:\n\t\tcase BPF_CGROUP_INET4_GETPEERNAME:\n\t\tcase BPF_CGROUP_INET6_GETPEERNAME:\n\t\tcase BPF_CGROUP_INET4_GETSOCKNAME:\n\t\tcase BPF_CGROUP_INET6_GETSOCKNAME:\n\t\t\treturn &bpf_sock_addr_getsockopt_proto;\n\t\tdefault:\n\t\t\treturn NULL;\n\t\t}\n\tdefault:\n\t\treturn bpf_sk_base_func_proto(func_id);\n\t}\n}\n\nstatic const struct bpf_func_proto *\nsk_filter_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n\tcase BPF_FUNC_skb_load_bytes:\n\t\treturn &bpf_skb_load_bytes_proto;\n\tcase BPF_FUNC_skb_load_bytes_relative:\n\t\treturn &bpf_skb_load_bytes_relative_proto;\n\tcase BPF_FUNC_get_socket_cookie:\n\t\treturn &bpf_get_socket_cookie_proto;\n\tcase BPF_FUNC_get_socket_uid:\n\t\treturn &bpf_get_socket_uid_proto;\n\tcase BPF_FUNC_perf_event_output:\n\t\treturn &bpf_skb_event_output_proto;\n\tdefault:\n\t\treturn bpf_sk_base_func_proto(func_id);\n\t}\n}\n\nconst struct bpf_func_proto bpf_sk_storage_get_proto __weak;\nconst struct bpf_func_proto bpf_sk_storage_delete_proto __weak;\n\nstatic const struct bpf_func_proto *\ncg_skb_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tconst struct bpf_func_proto *func_proto;\n\n\tfunc_proto = cgroup_common_func_proto(func_id, prog);\n\tif (func_proto)\n\t\treturn func_proto;\n\n\tswitch (func_id) {\n\tcase BPF_FUNC_sk_fullsock:\n\t\treturn &bpf_sk_fullsock_proto;\n\tcase BPF_FUNC_sk_storage_get:\n\t\treturn &bpf_sk_storage_get_proto;\n\tcase BPF_FUNC_sk_storage_delete:\n\t\treturn &bpf_sk_storage_delete_proto;\n\tcase BPF_FUNC_perf_event_output:\n\t\treturn &bpf_skb_event_output_proto;\n#ifdef CONFIG_SOCK_CGROUP_DATA\n\tcase BPF_FUNC_skb_cgroup_id:\n\t\treturn &bpf_skb_cgroup_id_proto;\n\tcase BPF_FUNC_skb_ancestor_cgroup_id:\n\t\treturn &bpf_skb_ancestor_cgroup_id_proto;\n\tcase BPF_FUNC_sk_cgroup_id:\n\t\treturn &bpf_sk_cgroup_id_proto;\n\tcase BPF_FUNC_sk_ancestor_cgroup_id:\n\t\treturn &bpf_sk_ancestor_cgroup_id_proto;\n#endif\n#ifdef CONFIG_INET\n\tcase BPF_FUNC_sk_lookup_tcp:\n\t\treturn &bpf_sk_lookup_tcp_proto;\n\tcase BPF_FUNC_sk_lookup_udp:\n\t\treturn &bpf_sk_lookup_udp_proto;\n\tcase BPF_FUNC_sk_release:\n\t\treturn &bpf_sk_release_proto;\n\tcase BPF_FUNC_skc_lookup_tcp:\n\t\treturn &bpf_skc_lookup_tcp_proto;\n\tcase BPF_FUNC_tcp_sock:\n\t\treturn &bpf_tcp_sock_proto;\n\tcase BPF_FUNC_get_listener_sock:\n\t\treturn &bpf_get_listener_sock_proto;\n\tcase BPF_FUNC_skb_ecn_set_ce:\n\t\treturn &bpf_skb_ecn_set_ce_proto;\n#endif\n\tdefault:\n\t\treturn sk_filter_func_proto(func_id, prog);\n\t}\n}\n\nstatic const struct bpf_func_proto *\ntc_cls_act_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n\tcase BPF_FUNC_skb_store_bytes:\n\t\treturn &bpf_skb_store_bytes_proto;\n\tcase BPF_FUNC_skb_load_bytes:\n\t\treturn &bpf_skb_load_bytes_proto;\n\tcase BPF_FUNC_skb_load_bytes_relative:\n\t\treturn &bpf_skb_load_bytes_relative_proto;\n\tcase BPF_FUNC_skb_pull_data:\n\t\treturn &bpf_skb_pull_data_proto;\n\tcase BPF_FUNC_csum_diff:\n\t\treturn &bpf_csum_diff_proto;\n\tcase BPF_FUNC_csum_update:\n\t\treturn &bpf_csum_update_proto;\n\tcase BPF_FUNC_csum_level:\n\t\treturn &bpf_csum_level_proto;\n\tcase BPF_FUNC_l3_csum_replace:\n\t\treturn &bpf_l3_csum_replace_proto;\n\tcase BPF_FUNC_l4_csum_replace:\n\t\treturn &bpf_l4_csum_replace_proto;\n\tcase BPF_FUNC_clone_redirect:\n\t\treturn &bpf_clone_redirect_proto;\n\tcase BPF_FUNC_get_cgroup_classid:\n\t\treturn &bpf_get_cgroup_classid_proto;\n\tcase BPF_FUNC_skb_vlan_push:\n\t\treturn &bpf_skb_vlan_push_proto;\n\tcase BPF_FUNC_skb_vlan_pop:\n\t\treturn &bpf_skb_vlan_pop_proto;\n\tcase BPF_FUNC_skb_change_proto:\n\t\treturn &bpf_skb_change_proto_proto;\n\tcase BPF_FUNC_skb_change_type:\n\t\treturn &bpf_skb_change_type_proto;\n\tcase BPF_FUNC_skb_adjust_room:\n\t\treturn &bpf_skb_adjust_room_proto;\n\tcase BPF_FUNC_skb_change_tail:\n\t\treturn &bpf_skb_change_tail_proto;\n\tcase BPF_FUNC_skb_change_head:\n\t\treturn &bpf_skb_change_head_proto;\n\tcase BPF_FUNC_skb_get_tunnel_key:\n\t\treturn &bpf_skb_get_tunnel_key_proto;\n\tcase BPF_FUNC_skb_set_tunnel_key:\n\t\treturn bpf_get_skb_set_tunnel_proto(func_id);\n\tcase BPF_FUNC_skb_get_tunnel_opt:\n\t\treturn &bpf_skb_get_tunnel_opt_proto;\n\tcase BPF_FUNC_skb_set_tunnel_opt:\n\t\treturn bpf_get_skb_set_tunnel_proto(func_id);\n\tcase BPF_FUNC_redirect:\n\t\treturn &bpf_redirect_proto;\n\tcase BPF_FUNC_redirect_neigh:\n\t\treturn &bpf_redirect_neigh_proto;\n\tcase BPF_FUNC_redirect_peer:\n\t\treturn &bpf_redirect_peer_proto;\n\tcase BPF_FUNC_get_route_realm:\n\t\treturn &bpf_get_route_realm_proto;\n\tcase BPF_FUNC_get_hash_recalc:\n\t\treturn &bpf_get_hash_recalc_proto;\n\tcase BPF_FUNC_set_hash_invalid:\n\t\treturn &bpf_set_hash_invalid_proto;\n\tcase BPF_FUNC_set_hash:\n\t\treturn &bpf_set_hash_proto;\n\tcase BPF_FUNC_perf_event_output:\n\t\treturn &bpf_skb_event_output_proto;\n\tcase BPF_FUNC_get_smp_processor_id:\n\t\treturn &bpf_get_smp_processor_id_proto;\n\tcase BPF_FUNC_skb_under_cgroup:\n\t\treturn &bpf_skb_under_cgroup_proto;\n\tcase BPF_FUNC_get_socket_cookie:\n\t\treturn &bpf_get_socket_cookie_proto;\n\tcase BPF_FUNC_get_socket_uid:\n\t\treturn &bpf_get_socket_uid_proto;\n\tcase BPF_FUNC_fib_lookup:\n\t\treturn &bpf_skb_fib_lookup_proto;\n\tcase BPF_FUNC_check_mtu:\n\t\treturn &bpf_skb_check_mtu_proto;\n\tcase BPF_FUNC_sk_fullsock:\n\t\treturn &bpf_sk_fullsock_proto;\n\tcase BPF_FUNC_sk_storage_get:\n\t\treturn &bpf_sk_storage_get_proto;\n\tcase BPF_FUNC_sk_storage_delete:\n\t\treturn &bpf_sk_storage_delete_proto;\n#ifdef CONFIG_XFRM\n\tcase BPF_FUNC_skb_get_xfrm_state:\n\t\treturn &bpf_skb_get_xfrm_state_proto;\n#endif\n#ifdef CONFIG_CGROUP_NET_CLASSID\n\tcase BPF_FUNC_skb_cgroup_classid:\n\t\treturn &bpf_skb_cgroup_classid_proto;\n#endif\n#ifdef CONFIG_SOCK_CGROUP_DATA\n\tcase BPF_FUNC_skb_cgroup_id:\n\t\treturn &bpf_skb_cgroup_id_proto;\n\tcase BPF_FUNC_skb_ancestor_cgroup_id:\n\t\treturn &bpf_skb_ancestor_cgroup_id_proto;\n#endif\n#ifdef CONFIG_INET\n\tcase BPF_FUNC_sk_lookup_tcp:\n\t\treturn &bpf_tc_sk_lookup_tcp_proto;\n\tcase BPF_FUNC_sk_lookup_udp:\n\t\treturn &bpf_tc_sk_lookup_udp_proto;\n\tcase BPF_FUNC_sk_release:\n\t\treturn &bpf_sk_release_proto;\n\tcase BPF_FUNC_tcp_sock:\n\t\treturn &bpf_tcp_sock_proto;\n\tcase BPF_FUNC_get_listener_sock:\n\t\treturn &bpf_get_listener_sock_proto;\n\tcase BPF_FUNC_skc_lookup_tcp:\n\t\treturn &bpf_tc_skc_lookup_tcp_proto;\n\tcase BPF_FUNC_tcp_check_syncookie:\n\t\treturn &bpf_tcp_check_syncookie_proto;\n\tcase BPF_FUNC_skb_ecn_set_ce:\n\t\treturn &bpf_skb_ecn_set_ce_proto;\n\tcase BPF_FUNC_tcp_gen_syncookie:\n\t\treturn &bpf_tcp_gen_syncookie_proto;\n\tcase BPF_FUNC_sk_assign:\n\t\treturn &bpf_sk_assign_proto;\n\tcase BPF_FUNC_skb_set_tstamp:\n\t\treturn &bpf_skb_set_tstamp_proto;\n#ifdef CONFIG_SYN_COOKIES\n\tcase BPF_FUNC_tcp_raw_gen_syncookie_ipv4:\n\t\treturn &bpf_tcp_raw_gen_syncookie_ipv4_proto;\n\tcase BPF_FUNC_tcp_raw_gen_syncookie_ipv6:\n\t\treturn &bpf_tcp_raw_gen_syncookie_ipv6_proto;\n\tcase BPF_FUNC_tcp_raw_check_syncookie_ipv4:\n\t\treturn &bpf_tcp_raw_check_syncookie_ipv4_proto;\n\tcase BPF_FUNC_tcp_raw_check_syncookie_ipv6:\n\t\treturn &bpf_tcp_raw_check_syncookie_ipv6_proto;\n#endif\n#endif\n\tdefault:\n\t\treturn bpf_sk_base_func_proto(func_id);\n\t}\n}\n\nstatic const struct bpf_func_proto *\nxdp_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n\tcase BPF_FUNC_perf_event_output:\n\t\treturn &bpf_xdp_event_output_proto;\n\tcase BPF_FUNC_get_smp_processor_id:\n\t\treturn &bpf_get_smp_processor_id_proto;\n\tcase BPF_FUNC_csum_diff:\n\t\treturn &bpf_csum_diff_proto;\n\tcase BPF_FUNC_xdp_adjust_head:\n\t\treturn &bpf_xdp_adjust_head_proto;\n\tcase BPF_FUNC_xdp_adjust_meta:\n\t\treturn &bpf_xdp_adjust_meta_proto;\n\tcase BPF_FUNC_redirect:\n\t\treturn &bpf_xdp_redirect_proto;\n\tcase BPF_FUNC_redirect_map:\n\t\treturn &bpf_xdp_redirect_map_proto;\n\tcase BPF_FUNC_xdp_adjust_tail:\n\t\treturn &bpf_xdp_adjust_tail_proto;\n\tcase BPF_FUNC_xdp_get_buff_len:\n\t\treturn &bpf_xdp_get_buff_len_proto;\n\tcase BPF_FUNC_xdp_load_bytes:\n\t\treturn &bpf_xdp_load_bytes_proto;\n\tcase BPF_FUNC_xdp_store_bytes:\n\t\treturn &bpf_xdp_store_bytes_proto;\n\tcase BPF_FUNC_fib_lookup:\n\t\treturn &bpf_xdp_fib_lookup_proto;\n\tcase BPF_FUNC_check_mtu:\n\t\treturn &bpf_xdp_check_mtu_proto;\n#ifdef CONFIG_INET\n\tcase BPF_FUNC_sk_lookup_udp:\n\t\treturn &bpf_xdp_sk_lookup_udp_proto;\n\tcase BPF_FUNC_sk_lookup_tcp:\n\t\treturn &bpf_xdp_sk_lookup_tcp_proto;\n\tcase BPF_FUNC_sk_release:\n\t\treturn &bpf_sk_release_proto;\n\tcase BPF_FUNC_skc_lookup_tcp:\n\t\treturn &bpf_xdp_skc_lookup_tcp_proto;\n\tcase BPF_FUNC_tcp_check_syncookie:\n\t\treturn &bpf_tcp_check_syncookie_proto;\n\tcase BPF_FUNC_tcp_gen_syncookie:\n\t\treturn &bpf_tcp_gen_syncookie_proto;\n#ifdef CONFIG_SYN_COOKIES\n\tcase BPF_FUNC_tcp_raw_gen_syncookie_ipv4:\n\t\treturn &bpf_tcp_raw_gen_syncookie_ipv4_proto;\n\tcase BPF_FUNC_tcp_raw_gen_syncookie_ipv6:\n\t\treturn &bpf_tcp_raw_gen_syncookie_ipv6_proto;\n\tcase BPF_FUNC_tcp_raw_check_syncookie_ipv4:\n\t\treturn &bpf_tcp_raw_check_syncookie_ipv4_proto;\n\tcase BPF_FUNC_tcp_raw_check_syncookie_ipv6:\n\t\treturn &bpf_tcp_raw_check_syncookie_ipv6_proto;\n#endif\n#endif\n\tdefault:\n\t\treturn bpf_sk_base_func_proto(func_id);\n\t}\n\n#if IS_MODULE(CONFIG_NF_CONNTRACK) && IS_ENABLED(CONFIG_DEBUG_INFO_BTF_MODULES)\n\t \n\tBTF_TYPE_EMIT(struct nf_conn___init);\n#endif\n}\n\nconst struct bpf_func_proto bpf_sock_map_update_proto __weak;\nconst struct bpf_func_proto bpf_sock_hash_update_proto __weak;\n\nstatic const struct bpf_func_proto *\nsock_ops_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tconst struct bpf_func_proto *func_proto;\n\n\tfunc_proto = cgroup_common_func_proto(func_id, prog);\n\tif (func_proto)\n\t\treturn func_proto;\n\n\tswitch (func_id) {\n\tcase BPF_FUNC_setsockopt:\n\t\treturn &bpf_sock_ops_setsockopt_proto;\n\tcase BPF_FUNC_getsockopt:\n\t\treturn &bpf_sock_ops_getsockopt_proto;\n\tcase BPF_FUNC_sock_ops_cb_flags_set:\n\t\treturn &bpf_sock_ops_cb_flags_set_proto;\n\tcase BPF_FUNC_sock_map_update:\n\t\treturn &bpf_sock_map_update_proto;\n\tcase BPF_FUNC_sock_hash_update:\n\t\treturn &bpf_sock_hash_update_proto;\n\tcase BPF_FUNC_get_socket_cookie:\n\t\treturn &bpf_get_socket_cookie_sock_ops_proto;\n\tcase BPF_FUNC_perf_event_output:\n\t\treturn &bpf_event_output_data_proto;\n\tcase BPF_FUNC_sk_storage_get:\n\t\treturn &bpf_sk_storage_get_proto;\n\tcase BPF_FUNC_sk_storage_delete:\n\t\treturn &bpf_sk_storage_delete_proto;\n\tcase BPF_FUNC_get_netns_cookie:\n\t\treturn &bpf_get_netns_cookie_sock_ops_proto;\n#ifdef CONFIG_INET\n\tcase BPF_FUNC_load_hdr_opt:\n\t\treturn &bpf_sock_ops_load_hdr_opt_proto;\n\tcase BPF_FUNC_store_hdr_opt:\n\t\treturn &bpf_sock_ops_store_hdr_opt_proto;\n\tcase BPF_FUNC_reserve_hdr_opt:\n\t\treturn &bpf_sock_ops_reserve_hdr_opt_proto;\n\tcase BPF_FUNC_tcp_sock:\n\t\treturn &bpf_tcp_sock_proto;\n#endif  \n\tdefault:\n\t\treturn bpf_sk_base_func_proto(func_id);\n\t}\n}\n\nconst struct bpf_func_proto bpf_msg_redirect_map_proto __weak;\nconst struct bpf_func_proto bpf_msg_redirect_hash_proto __weak;\n\nstatic const struct bpf_func_proto *\nsk_msg_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n\tcase BPF_FUNC_msg_redirect_map:\n\t\treturn &bpf_msg_redirect_map_proto;\n\tcase BPF_FUNC_msg_redirect_hash:\n\t\treturn &bpf_msg_redirect_hash_proto;\n\tcase BPF_FUNC_msg_apply_bytes:\n\t\treturn &bpf_msg_apply_bytes_proto;\n\tcase BPF_FUNC_msg_cork_bytes:\n\t\treturn &bpf_msg_cork_bytes_proto;\n\tcase BPF_FUNC_msg_pull_data:\n\t\treturn &bpf_msg_pull_data_proto;\n\tcase BPF_FUNC_msg_push_data:\n\t\treturn &bpf_msg_push_data_proto;\n\tcase BPF_FUNC_msg_pop_data:\n\t\treturn &bpf_msg_pop_data_proto;\n\tcase BPF_FUNC_perf_event_output:\n\t\treturn &bpf_event_output_data_proto;\n\tcase BPF_FUNC_get_current_uid_gid:\n\t\treturn &bpf_get_current_uid_gid_proto;\n\tcase BPF_FUNC_get_current_pid_tgid:\n\t\treturn &bpf_get_current_pid_tgid_proto;\n\tcase BPF_FUNC_sk_storage_get:\n\t\treturn &bpf_sk_storage_get_proto;\n\tcase BPF_FUNC_sk_storage_delete:\n\t\treturn &bpf_sk_storage_delete_proto;\n\tcase BPF_FUNC_get_netns_cookie:\n\t\treturn &bpf_get_netns_cookie_sk_msg_proto;\n#ifdef CONFIG_CGROUP_NET_CLASSID\n\tcase BPF_FUNC_get_cgroup_classid:\n\t\treturn &bpf_get_cgroup_classid_curr_proto;\n#endif\n\tdefault:\n\t\treturn bpf_sk_base_func_proto(func_id);\n\t}\n}\n\nconst struct bpf_func_proto bpf_sk_redirect_map_proto __weak;\nconst struct bpf_func_proto bpf_sk_redirect_hash_proto __weak;\n\nstatic const struct bpf_func_proto *\nsk_skb_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n\tcase BPF_FUNC_skb_store_bytes:\n\t\treturn &bpf_skb_store_bytes_proto;\n\tcase BPF_FUNC_skb_load_bytes:\n\t\treturn &bpf_skb_load_bytes_proto;\n\tcase BPF_FUNC_skb_pull_data:\n\t\treturn &sk_skb_pull_data_proto;\n\tcase BPF_FUNC_skb_change_tail:\n\t\treturn &sk_skb_change_tail_proto;\n\tcase BPF_FUNC_skb_change_head:\n\t\treturn &sk_skb_change_head_proto;\n\tcase BPF_FUNC_skb_adjust_room:\n\t\treturn &sk_skb_adjust_room_proto;\n\tcase BPF_FUNC_get_socket_cookie:\n\t\treturn &bpf_get_socket_cookie_proto;\n\tcase BPF_FUNC_get_socket_uid:\n\t\treturn &bpf_get_socket_uid_proto;\n\tcase BPF_FUNC_sk_redirect_map:\n\t\treturn &bpf_sk_redirect_map_proto;\n\tcase BPF_FUNC_sk_redirect_hash:\n\t\treturn &bpf_sk_redirect_hash_proto;\n\tcase BPF_FUNC_perf_event_output:\n\t\treturn &bpf_skb_event_output_proto;\n#ifdef CONFIG_INET\n\tcase BPF_FUNC_sk_lookup_tcp:\n\t\treturn &bpf_sk_lookup_tcp_proto;\n\tcase BPF_FUNC_sk_lookup_udp:\n\t\treturn &bpf_sk_lookup_udp_proto;\n\tcase BPF_FUNC_sk_release:\n\t\treturn &bpf_sk_release_proto;\n\tcase BPF_FUNC_skc_lookup_tcp:\n\t\treturn &bpf_skc_lookup_tcp_proto;\n#endif\n\tdefault:\n\t\treturn bpf_sk_base_func_proto(func_id);\n\t}\n}\n\nstatic const struct bpf_func_proto *\nflow_dissector_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n\tcase BPF_FUNC_skb_load_bytes:\n\t\treturn &bpf_flow_dissector_load_bytes_proto;\n\tdefault:\n\t\treturn bpf_sk_base_func_proto(func_id);\n\t}\n}\n\nstatic const struct bpf_func_proto *\nlwt_out_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n\tcase BPF_FUNC_skb_load_bytes:\n\t\treturn &bpf_skb_load_bytes_proto;\n\tcase BPF_FUNC_skb_pull_data:\n\t\treturn &bpf_skb_pull_data_proto;\n\tcase BPF_FUNC_csum_diff:\n\t\treturn &bpf_csum_diff_proto;\n\tcase BPF_FUNC_get_cgroup_classid:\n\t\treturn &bpf_get_cgroup_classid_proto;\n\tcase BPF_FUNC_get_route_realm:\n\t\treturn &bpf_get_route_realm_proto;\n\tcase BPF_FUNC_get_hash_recalc:\n\t\treturn &bpf_get_hash_recalc_proto;\n\tcase BPF_FUNC_perf_event_output:\n\t\treturn &bpf_skb_event_output_proto;\n\tcase BPF_FUNC_get_smp_processor_id:\n\t\treturn &bpf_get_smp_processor_id_proto;\n\tcase BPF_FUNC_skb_under_cgroup:\n\t\treturn &bpf_skb_under_cgroup_proto;\n\tdefault:\n\t\treturn bpf_sk_base_func_proto(func_id);\n\t}\n}\n\nstatic const struct bpf_func_proto *\nlwt_in_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n\tcase BPF_FUNC_lwt_push_encap:\n\t\treturn &bpf_lwt_in_push_encap_proto;\n\tdefault:\n\t\treturn lwt_out_func_proto(func_id, prog);\n\t}\n}\n\nstatic const struct bpf_func_proto *\nlwt_xmit_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n\tcase BPF_FUNC_skb_get_tunnel_key:\n\t\treturn &bpf_skb_get_tunnel_key_proto;\n\tcase BPF_FUNC_skb_set_tunnel_key:\n\t\treturn bpf_get_skb_set_tunnel_proto(func_id);\n\tcase BPF_FUNC_skb_get_tunnel_opt:\n\t\treturn &bpf_skb_get_tunnel_opt_proto;\n\tcase BPF_FUNC_skb_set_tunnel_opt:\n\t\treturn bpf_get_skb_set_tunnel_proto(func_id);\n\tcase BPF_FUNC_redirect:\n\t\treturn &bpf_redirect_proto;\n\tcase BPF_FUNC_clone_redirect:\n\t\treturn &bpf_clone_redirect_proto;\n\tcase BPF_FUNC_skb_change_tail:\n\t\treturn &bpf_skb_change_tail_proto;\n\tcase BPF_FUNC_skb_change_head:\n\t\treturn &bpf_skb_change_head_proto;\n\tcase BPF_FUNC_skb_store_bytes:\n\t\treturn &bpf_skb_store_bytes_proto;\n\tcase BPF_FUNC_csum_update:\n\t\treturn &bpf_csum_update_proto;\n\tcase BPF_FUNC_csum_level:\n\t\treturn &bpf_csum_level_proto;\n\tcase BPF_FUNC_l3_csum_replace:\n\t\treturn &bpf_l3_csum_replace_proto;\n\tcase BPF_FUNC_l4_csum_replace:\n\t\treturn &bpf_l4_csum_replace_proto;\n\tcase BPF_FUNC_set_hash_invalid:\n\t\treturn &bpf_set_hash_invalid_proto;\n\tcase BPF_FUNC_lwt_push_encap:\n\t\treturn &bpf_lwt_xmit_push_encap_proto;\n\tdefault:\n\t\treturn lwt_out_func_proto(func_id, prog);\n\t}\n}\n\nstatic const struct bpf_func_proto *\nlwt_seg6local_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n#if IS_ENABLED(CONFIG_IPV6_SEG6_BPF)\n\tcase BPF_FUNC_lwt_seg6_store_bytes:\n\t\treturn &bpf_lwt_seg6_store_bytes_proto;\n\tcase BPF_FUNC_lwt_seg6_action:\n\t\treturn &bpf_lwt_seg6_action_proto;\n\tcase BPF_FUNC_lwt_seg6_adjust_srh:\n\t\treturn &bpf_lwt_seg6_adjust_srh_proto;\n#endif\n\tdefault:\n\t\treturn lwt_out_func_proto(func_id, prog);\n\t}\n}\n\nstatic bool bpf_skb_is_valid_access(int off, int size, enum bpf_access_type type,\n\t\t\t\t    const struct bpf_prog *prog,\n\t\t\t\t    struct bpf_insn_access_aux *info)\n{\n\tconst int size_default = sizeof(__u32);\n\n\tif (off < 0 || off >= sizeof(struct __sk_buff))\n\t\treturn false;\n\n\t \n\tif (off % size != 0)\n\t\treturn false;\n\n\tswitch (off) {\n\tcase bpf_ctx_range_till(struct __sk_buff, cb[0], cb[4]):\n\t\tif (off + size > offsetofend(struct __sk_buff, cb[4]))\n\t\t\treturn false;\n\t\tbreak;\n\tcase bpf_ctx_range_till(struct __sk_buff, remote_ip6[0], remote_ip6[3]):\n\tcase bpf_ctx_range_till(struct __sk_buff, local_ip6[0], local_ip6[3]):\n\tcase bpf_ctx_range_till(struct __sk_buff, remote_ip4, remote_ip4):\n\tcase bpf_ctx_range_till(struct __sk_buff, local_ip4, local_ip4):\n\tcase bpf_ctx_range(struct __sk_buff, data):\n\tcase bpf_ctx_range(struct __sk_buff, data_meta):\n\tcase bpf_ctx_range(struct __sk_buff, data_end):\n\t\tif (size != size_default)\n\t\t\treturn false;\n\t\tbreak;\n\tcase bpf_ctx_range_ptr(struct __sk_buff, flow_keys):\n\t\treturn false;\n\tcase bpf_ctx_range(struct __sk_buff, hwtstamp):\n\t\tif (type == BPF_WRITE || size != sizeof(__u64))\n\t\t\treturn false;\n\t\tbreak;\n\tcase bpf_ctx_range(struct __sk_buff, tstamp):\n\t\tif (size != sizeof(__u64))\n\t\t\treturn false;\n\t\tbreak;\n\tcase offsetof(struct __sk_buff, sk):\n\t\tif (type == BPF_WRITE || size != sizeof(__u64))\n\t\t\treturn false;\n\t\tinfo->reg_type = PTR_TO_SOCK_COMMON_OR_NULL;\n\t\tbreak;\n\tcase offsetof(struct __sk_buff, tstamp_type):\n\t\treturn false;\n\tcase offsetofend(struct __sk_buff, tstamp_type) ... offsetof(struct __sk_buff, hwtstamp) - 1:\n\t\t \n\t\treturn false;\n\tdefault:\n\t\t \n\t\tif (type == BPF_WRITE) {\n\t\t\tif (size != size_default)\n\t\t\t\treturn false;\n\t\t} else {\n\t\t\tbpf_ctx_record_field_size(info, size_default);\n\t\t\tif (!bpf_ctx_narrow_access_ok(off, size, size_default))\n\t\t\t\treturn false;\n\t\t}\n\t}\n\n\treturn true;\n}\n\nstatic bool sk_filter_is_valid_access(int off, int size,\n\t\t\t\t      enum bpf_access_type type,\n\t\t\t\t      const struct bpf_prog *prog,\n\t\t\t\t      struct bpf_insn_access_aux *info)\n{\n\tswitch (off) {\n\tcase bpf_ctx_range(struct __sk_buff, tc_classid):\n\tcase bpf_ctx_range(struct __sk_buff, data):\n\tcase bpf_ctx_range(struct __sk_buff, data_meta):\n\tcase bpf_ctx_range(struct __sk_buff, data_end):\n\tcase bpf_ctx_range_till(struct __sk_buff, family, local_port):\n\tcase bpf_ctx_range(struct __sk_buff, tstamp):\n\tcase bpf_ctx_range(struct __sk_buff, wire_len):\n\tcase bpf_ctx_range(struct __sk_buff, hwtstamp):\n\t\treturn false;\n\t}\n\n\tif (type == BPF_WRITE) {\n\t\tswitch (off) {\n\t\tcase bpf_ctx_range_till(struct __sk_buff, cb[0], cb[4]):\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\t}\n\n\treturn bpf_skb_is_valid_access(off, size, type, prog, info);\n}\n\nstatic bool cg_skb_is_valid_access(int off, int size,\n\t\t\t\t   enum bpf_access_type type,\n\t\t\t\t   const struct bpf_prog *prog,\n\t\t\t\t   struct bpf_insn_access_aux *info)\n{\n\tswitch (off) {\n\tcase bpf_ctx_range(struct __sk_buff, tc_classid):\n\tcase bpf_ctx_range(struct __sk_buff, data_meta):\n\tcase bpf_ctx_range(struct __sk_buff, wire_len):\n\t\treturn false;\n\tcase bpf_ctx_range(struct __sk_buff, data):\n\tcase bpf_ctx_range(struct __sk_buff, data_end):\n\t\tif (!bpf_capable())\n\t\t\treturn false;\n\t\tbreak;\n\t}\n\n\tif (type == BPF_WRITE) {\n\t\tswitch (off) {\n\t\tcase bpf_ctx_range(struct __sk_buff, mark):\n\t\tcase bpf_ctx_range(struct __sk_buff, priority):\n\t\tcase bpf_ctx_range_till(struct __sk_buff, cb[0], cb[4]):\n\t\t\tbreak;\n\t\tcase bpf_ctx_range(struct __sk_buff, tstamp):\n\t\t\tif (!bpf_capable())\n\t\t\t\treturn false;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tswitch (off) {\n\tcase bpf_ctx_range(struct __sk_buff, data):\n\t\tinfo->reg_type = PTR_TO_PACKET;\n\t\tbreak;\n\tcase bpf_ctx_range(struct __sk_buff, data_end):\n\t\tinfo->reg_type = PTR_TO_PACKET_END;\n\t\tbreak;\n\t}\n\n\treturn bpf_skb_is_valid_access(off, size, type, prog, info);\n}\n\nstatic bool lwt_is_valid_access(int off, int size,\n\t\t\t\tenum bpf_access_type type,\n\t\t\t\tconst struct bpf_prog *prog,\n\t\t\t\tstruct bpf_insn_access_aux *info)\n{\n\tswitch (off) {\n\tcase bpf_ctx_range(struct __sk_buff, tc_classid):\n\tcase bpf_ctx_range_till(struct __sk_buff, family, local_port):\n\tcase bpf_ctx_range(struct __sk_buff, data_meta):\n\tcase bpf_ctx_range(struct __sk_buff, tstamp):\n\tcase bpf_ctx_range(struct __sk_buff, wire_len):\n\tcase bpf_ctx_range(struct __sk_buff, hwtstamp):\n\t\treturn false;\n\t}\n\n\tif (type == BPF_WRITE) {\n\t\tswitch (off) {\n\t\tcase bpf_ctx_range(struct __sk_buff, mark):\n\t\tcase bpf_ctx_range(struct __sk_buff, priority):\n\t\tcase bpf_ctx_range_till(struct __sk_buff, cb[0], cb[4]):\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tswitch (off) {\n\tcase bpf_ctx_range(struct __sk_buff, data):\n\t\tinfo->reg_type = PTR_TO_PACKET;\n\t\tbreak;\n\tcase bpf_ctx_range(struct __sk_buff, data_end):\n\t\tinfo->reg_type = PTR_TO_PACKET_END;\n\t\tbreak;\n\t}\n\n\treturn bpf_skb_is_valid_access(off, size, type, prog, info);\n}\n\n \nstatic bool __sock_filter_check_attach_type(int off,\n\t\t\t\t\t    enum bpf_access_type access_type,\n\t\t\t\t\t    enum bpf_attach_type attach_type)\n{\n\tswitch (off) {\n\tcase offsetof(struct bpf_sock, bound_dev_if):\n\tcase offsetof(struct bpf_sock, mark):\n\tcase offsetof(struct bpf_sock, priority):\n\t\tswitch (attach_type) {\n\t\tcase BPF_CGROUP_INET_SOCK_CREATE:\n\t\tcase BPF_CGROUP_INET_SOCK_RELEASE:\n\t\t\tgoto full_access;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\tcase bpf_ctx_range(struct bpf_sock, src_ip4):\n\t\tswitch (attach_type) {\n\t\tcase BPF_CGROUP_INET4_POST_BIND:\n\t\t\tgoto read_only;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\tcase bpf_ctx_range_till(struct bpf_sock, src_ip6[0], src_ip6[3]):\n\t\tswitch (attach_type) {\n\t\tcase BPF_CGROUP_INET6_POST_BIND:\n\t\t\tgoto read_only;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\tcase bpf_ctx_range(struct bpf_sock, src_port):\n\t\tswitch (attach_type) {\n\t\tcase BPF_CGROUP_INET4_POST_BIND:\n\t\tcase BPF_CGROUP_INET6_POST_BIND:\n\t\t\tgoto read_only;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\t}\nread_only:\n\treturn access_type == BPF_READ;\nfull_access:\n\treturn true;\n}\n\nbool bpf_sock_common_is_valid_access(int off, int size,\n\t\t\t\t     enum bpf_access_type type,\n\t\t\t\t     struct bpf_insn_access_aux *info)\n{\n\tswitch (off) {\n\tcase bpf_ctx_range_till(struct bpf_sock, type, priority):\n\t\treturn false;\n\tdefault:\n\t\treturn bpf_sock_is_valid_access(off, size, type, info);\n\t}\n}\n\nbool bpf_sock_is_valid_access(int off, int size, enum bpf_access_type type,\n\t\t\t      struct bpf_insn_access_aux *info)\n{\n\tconst int size_default = sizeof(__u32);\n\tint field_size;\n\n\tif (off < 0 || off >= sizeof(struct bpf_sock))\n\t\treturn false;\n\tif (off % size != 0)\n\t\treturn false;\n\n\tswitch (off) {\n\tcase offsetof(struct bpf_sock, state):\n\tcase offsetof(struct bpf_sock, family):\n\tcase offsetof(struct bpf_sock, type):\n\tcase offsetof(struct bpf_sock, protocol):\n\tcase offsetof(struct bpf_sock, src_port):\n\tcase offsetof(struct bpf_sock, rx_queue_mapping):\n\tcase bpf_ctx_range(struct bpf_sock, src_ip4):\n\tcase bpf_ctx_range_till(struct bpf_sock, src_ip6[0], src_ip6[3]):\n\tcase bpf_ctx_range(struct bpf_sock, dst_ip4):\n\tcase bpf_ctx_range_till(struct bpf_sock, dst_ip6[0], dst_ip6[3]):\n\t\tbpf_ctx_record_field_size(info, size_default);\n\t\treturn bpf_ctx_narrow_access_ok(off, size, size_default);\n\tcase bpf_ctx_range(struct bpf_sock, dst_port):\n\t\tfield_size = size == size_default ?\n\t\t\tsize_default : sizeof_field(struct bpf_sock, dst_port);\n\t\tbpf_ctx_record_field_size(info, field_size);\n\t\treturn bpf_ctx_narrow_access_ok(off, size, field_size);\n\tcase offsetofend(struct bpf_sock, dst_port) ...\n\t     offsetof(struct bpf_sock, dst_ip4) - 1:\n\t\treturn false;\n\t}\n\n\treturn size == size_default;\n}\n\nstatic bool sock_filter_is_valid_access(int off, int size,\n\t\t\t\t\tenum bpf_access_type type,\n\t\t\t\t\tconst struct bpf_prog *prog,\n\t\t\t\t\tstruct bpf_insn_access_aux *info)\n{\n\tif (!bpf_sock_is_valid_access(off, size, type, info))\n\t\treturn false;\n\treturn __sock_filter_check_attach_type(off, type,\n\t\t\t\t\t       prog->expected_attach_type);\n}\n\nstatic int bpf_noop_prologue(struct bpf_insn *insn_buf, bool direct_write,\n\t\t\t     const struct bpf_prog *prog)\n{\n\t \n\treturn 0;\n}\n\nstatic int bpf_unclone_prologue(struct bpf_insn *insn_buf, bool direct_write,\n\t\t\t\tconst struct bpf_prog *prog, int drop_verdict)\n{\n\tstruct bpf_insn *insn = insn_buf;\n\n\tif (!direct_write)\n\t\treturn 0;\n\n\t \n\t*insn++ = BPF_LDX_MEM(BPF_B, BPF_REG_6, BPF_REG_1, CLONED_OFFSET);\n\t*insn++ = BPF_ALU32_IMM(BPF_AND, BPF_REG_6, CLONED_MASK);\n\t*insn++ = BPF_JMP_IMM(BPF_JEQ, BPF_REG_6, 0, 7);\n\n\t \n\t*insn++ = BPF_MOV64_REG(BPF_REG_6, BPF_REG_1);\n\t*insn++ = BPF_ALU64_REG(BPF_XOR, BPF_REG_2, BPF_REG_2);\n\t*insn++ = BPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0,\n\t\t\t       BPF_FUNC_skb_pull_data);\n\t \n\t*insn++ = BPF_JMP_IMM(BPF_JEQ, BPF_REG_0, 0, 2);\n\t*insn++ = BPF_ALU32_IMM(BPF_MOV, BPF_REG_0, drop_verdict);\n\t*insn++ = BPF_EXIT_INSN();\n\n\t \n\t*insn++ = BPF_MOV64_REG(BPF_REG_1, BPF_REG_6);\n\t \n\t*insn++ = prog->insnsi[0];\n\n\treturn insn - insn_buf;\n}\n\nstatic int bpf_gen_ld_abs(const struct bpf_insn *orig,\n\t\t\t  struct bpf_insn *insn_buf)\n{\n\tbool indirect = BPF_MODE(orig->code) == BPF_IND;\n\tstruct bpf_insn *insn = insn_buf;\n\n\tif (!indirect) {\n\t\t*insn++ = BPF_MOV64_IMM(BPF_REG_2, orig->imm);\n\t} else {\n\t\t*insn++ = BPF_MOV64_REG(BPF_REG_2, orig->src_reg);\n\t\tif (orig->imm)\n\t\t\t*insn++ = BPF_ALU64_IMM(BPF_ADD, BPF_REG_2, orig->imm);\n\t}\n\t \n\t*insn++ = BPF_MOV64_REG(BPF_REG_1, BPF_REG_CTX);\n\n\tswitch (BPF_SIZE(orig->code)) {\n\tcase BPF_B:\n\t\t*insn++ = BPF_EMIT_CALL(bpf_skb_load_helper_8_no_cache);\n\t\tbreak;\n\tcase BPF_H:\n\t\t*insn++ = BPF_EMIT_CALL(bpf_skb_load_helper_16_no_cache);\n\t\tbreak;\n\tcase BPF_W:\n\t\t*insn++ = BPF_EMIT_CALL(bpf_skb_load_helper_32_no_cache);\n\t\tbreak;\n\t}\n\n\t*insn++ = BPF_JMP_IMM(BPF_JSGE, BPF_REG_0, 0, 2);\n\t*insn++ = BPF_ALU32_REG(BPF_XOR, BPF_REG_0, BPF_REG_0);\n\t*insn++ = BPF_EXIT_INSN();\n\n\treturn insn - insn_buf;\n}\n\nstatic int tc_cls_act_prologue(struct bpf_insn *insn_buf, bool direct_write,\n\t\t\t       const struct bpf_prog *prog)\n{\n\treturn bpf_unclone_prologue(insn_buf, direct_write, prog, TC_ACT_SHOT);\n}\n\nstatic bool tc_cls_act_is_valid_access(int off, int size,\n\t\t\t\t       enum bpf_access_type type,\n\t\t\t\t       const struct bpf_prog *prog,\n\t\t\t\t       struct bpf_insn_access_aux *info)\n{\n\tif (type == BPF_WRITE) {\n\t\tswitch (off) {\n\t\tcase bpf_ctx_range(struct __sk_buff, mark):\n\t\tcase bpf_ctx_range(struct __sk_buff, tc_index):\n\t\tcase bpf_ctx_range(struct __sk_buff, priority):\n\t\tcase bpf_ctx_range(struct __sk_buff, tc_classid):\n\t\tcase bpf_ctx_range_till(struct __sk_buff, cb[0], cb[4]):\n\t\tcase bpf_ctx_range(struct __sk_buff, tstamp):\n\t\tcase bpf_ctx_range(struct __sk_buff, queue_mapping):\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tswitch (off) {\n\tcase bpf_ctx_range(struct __sk_buff, data):\n\t\tinfo->reg_type = PTR_TO_PACKET;\n\t\tbreak;\n\tcase bpf_ctx_range(struct __sk_buff, data_meta):\n\t\tinfo->reg_type = PTR_TO_PACKET_META;\n\t\tbreak;\n\tcase bpf_ctx_range(struct __sk_buff, data_end):\n\t\tinfo->reg_type = PTR_TO_PACKET_END;\n\t\tbreak;\n\tcase bpf_ctx_range_till(struct __sk_buff, family, local_port):\n\t\treturn false;\n\tcase offsetof(struct __sk_buff, tstamp_type):\n\t\t \n\t\t((struct bpf_prog *)prog)->tstamp_type_access = 1;\n\t\treturn size == sizeof(__u8);\n\t}\n\n\treturn bpf_skb_is_valid_access(off, size, type, prog, info);\n}\n\nDEFINE_MUTEX(nf_conn_btf_access_lock);\nEXPORT_SYMBOL_GPL(nf_conn_btf_access_lock);\n\nint (*nfct_btf_struct_access)(struct bpf_verifier_log *log,\n\t\t\t      const struct bpf_reg_state *reg,\n\t\t\t      int off, int size);\nEXPORT_SYMBOL_GPL(nfct_btf_struct_access);\n\nstatic int tc_cls_act_btf_struct_access(struct bpf_verifier_log *log,\n\t\t\t\t\tconst struct bpf_reg_state *reg,\n\t\t\t\t\tint off, int size)\n{\n\tint ret = -EACCES;\n\n\tmutex_lock(&nf_conn_btf_access_lock);\n\tif (nfct_btf_struct_access)\n\t\tret = nfct_btf_struct_access(log, reg, off, size);\n\tmutex_unlock(&nf_conn_btf_access_lock);\n\n\treturn ret;\n}\n\nstatic bool __is_valid_xdp_access(int off, int size)\n{\n\tif (off < 0 || off >= sizeof(struct xdp_md))\n\t\treturn false;\n\tif (off % size != 0)\n\t\treturn false;\n\tif (size != sizeof(__u32))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic bool xdp_is_valid_access(int off, int size,\n\t\t\t\tenum bpf_access_type type,\n\t\t\t\tconst struct bpf_prog *prog,\n\t\t\t\tstruct bpf_insn_access_aux *info)\n{\n\tif (prog->expected_attach_type != BPF_XDP_DEVMAP) {\n\t\tswitch (off) {\n\t\tcase offsetof(struct xdp_md, egress_ifindex):\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tif (type == BPF_WRITE) {\n\t\tif (bpf_prog_is_offloaded(prog->aux)) {\n\t\t\tswitch (off) {\n\t\t\tcase offsetof(struct xdp_md, rx_queue_index):\n\t\t\t\treturn __is_valid_xdp_access(off, size);\n\t\t\t}\n\t\t}\n\t\treturn false;\n\t}\n\n\tswitch (off) {\n\tcase offsetof(struct xdp_md, data):\n\t\tinfo->reg_type = PTR_TO_PACKET;\n\t\tbreak;\n\tcase offsetof(struct xdp_md, data_meta):\n\t\tinfo->reg_type = PTR_TO_PACKET_META;\n\t\tbreak;\n\tcase offsetof(struct xdp_md, data_end):\n\t\tinfo->reg_type = PTR_TO_PACKET_END;\n\t\tbreak;\n\t}\n\n\treturn __is_valid_xdp_access(off, size);\n}\n\nvoid bpf_warn_invalid_xdp_action(struct net_device *dev, struct bpf_prog *prog, u32 act)\n{\n\tconst u32 act_max = XDP_REDIRECT;\n\n\tpr_warn_once(\"%s XDP return value %u on prog %s (id %d) dev %s, expect packet loss!\\n\",\n\t\t     act > act_max ? \"Illegal\" : \"Driver unsupported\",\n\t\t     act, prog->aux->name, prog->aux->id, dev ? dev->name : \"N/A\");\n}\nEXPORT_SYMBOL_GPL(bpf_warn_invalid_xdp_action);\n\nstatic int xdp_btf_struct_access(struct bpf_verifier_log *log,\n\t\t\t\t const struct bpf_reg_state *reg,\n\t\t\t\t int off, int size)\n{\n\tint ret = -EACCES;\n\n\tmutex_lock(&nf_conn_btf_access_lock);\n\tif (nfct_btf_struct_access)\n\t\tret = nfct_btf_struct_access(log, reg, off, size);\n\tmutex_unlock(&nf_conn_btf_access_lock);\n\n\treturn ret;\n}\n\nstatic bool sock_addr_is_valid_access(int off, int size,\n\t\t\t\t      enum bpf_access_type type,\n\t\t\t\t      const struct bpf_prog *prog,\n\t\t\t\t      struct bpf_insn_access_aux *info)\n{\n\tconst int size_default = sizeof(__u32);\n\n\tif (off < 0 || off >= sizeof(struct bpf_sock_addr))\n\t\treturn false;\n\tif (off % size != 0)\n\t\treturn false;\n\n\t \n\tswitch (off) {\n\tcase bpf_ctx_range(struct bpf_sock_addr, user_ip4):\n\t\tswitch (prog->expected_attach_type) {\n\t\tcase BPF_CGROUP_INET4_BIND:\n\t\tcase BPF_CGROUP_INET4_CONNECT:\n\t\tcase BPF_CGROUP_INET4_GETPEERNAME:\n\t\tcase BPF_CGROUP_INET4_GETSOCKNAME:\n\t\tcase BPF_CGROUP_UDP4_SENDMSG:\n\t\tcase BPF_CGROUP_UDP4_RECVMSG:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase bpf_ctx_range_till(struct bpf_sock_addr, user_ip6[0], user_ip6[3]):\n\t\tswitch (prog->expected_attach_type) {\n\t\tcase BPF_CGROUP_INET6_BIND:\n\t\tcase BPF_CGROUP_INET6_CONNECT:\n\t\tcase BPF_CGROUP_INET6_GETPEERNAME:\n\t\tcase BPF_CGROUP_INET6_GETSOCKNAME:\n\t\tcase BPF_CGROUP_UDP6_SENDMSG:\n\t\tcase BPF_CGROUP_UDP6_RECVMSG:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase bpf_ctx_range(struct bpf_sock_addr, msg_src_ip4):\n\t\tswitch (prog->expected_attach_type) {\n\t\tcase BPF_CGROUP_UDP4_SENDMSG:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase bpf_ctx_range_till(struct bpf_sock_addr, msg_src_ip6[0],\n\t\t\t\tmsg_src_ip6[3]):\n\t\tswitch (prog->expected_attach_type) {\n\t\tcase BPF_CGROUP_UDP6_SENDMSG:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\t}\n\n\tswitch (off) {\n\tcase bpf_ctx_range(struct bpf_sock_addr, user_ip4):\n\tcase bpf_ctx_range_till(struct bpf_sock_addr, user_ip6[0], user_ip6[3]):\n\tcase bpf_ctx_range(struct bpf_sock_addr, msg_src_ip4):\n\tcase bpf_ctx_range_till(struct bpf_sock_addr, msg_src_ip6[0],\n\t\t\t\tmsg_src_ip6[3]):\n\tcase bpf_ctx_range(struct bpf_sock_addr, user_port):\n\t\tif (type == BPF_READ) {\n\t\t\tbpf_ctx_record_field_size(info, size_default);\n\n\t\t\tif (bpf_ctx_wide_access_ok(off, size,\n\t\t\t\t\t\t   struct bpf_sock_addr,\n\t\t\t\t\t\t   user_ip6))\n\t\t\t\treturn true;\n\n\t\t\tif (bpf_ctx_wide_access_ok(off, size,\n\t\t\t\t\t\t   struct bpf_sock_addr,\n\t\t\t\t\t\t   msg_src_ip6))\n\t\t\t\treturn true;\n\n\t\t\tif (!bpf_ctx_narrow_access_ok(off, size, size_default))\n\t\t\t\treturn false;\n\t\t} else {\n\t\t\tif (bpf_ctx_wide_access_ok(off, size,\n\t\t\t\t\t\t   struct bpf_sock_addr,\n\t\t\t\t\t\t   user_ip6))\n\t\t\t\treturn true;\n\n\t\t\tif (bpf_ctx_wide_access_ok(off, size,\n\t\t\t\t\t\t   struct bpf_sock_addr,\n\t\t\t\t\t\t   msg_src_ip6))\n\t\t\t\treturn true;\n\n\t\t\tif (size != size_default)\n\t\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase offsetof(struct bpf_sock_addr, sk):\n\t\tif (type != BPF_READ)\n\t\t\treturn false;\n\t\tif (size != sizeof(__u64))\n\t\t\treturn false;\n\t\tinfo->reg_type = PTR_TO_SOCKET;\n\t\tbreak;\n\tdefault:\n\t\tif (type == BPF_READ) {\n\t\t\tif (size != size_default)\n\t\t\t\treturn false;\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t}\n\n\treturn true;\n}\n\nstatic bool sock_ops_is_valid_access(int off, int size,\n\t\t\t\t     enum bpf_access_type type,\n\t\t\t\t     const struct bpf_prog *prog,\n\t\t\t\t     struct bpf_insn_access_aux *info)\n{\n\tconst int size_default = sizeof(__u32);\n\n\tif (off < 0 || off >= sizeof(struct bpf_sock_ops))\n\t\treturn false;\n\n\t \n\tif (off % size != 0)\n\t\treturn false;\n\n\tif (type == BPF_WRITE) {\n\t\tswitch (off) {\n\t\tcase offsetof(struct bpf_sock_ops, reply):\n\t\tcase offsetof(struct bpf_sock_ops, sk_txhash):\n\t\t\tif (size != size_default)\n\t\t\t\treturn false;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\t} else {\n\t\tswitch (off) {\n\t\tcase bpf_ctx_range_till(struct bpf_sock_ops, bytes_received,\n\t\t\t\t\tbytes_acked):\n\t\t\tif (size != sizeof(__u64))\n\t\t\t\treturn false;\n\t\t\tbreak;\n\t\tcase offsetof(struct bpf_sock_ops, sk):\n\t\t\tif (size != sizeof(__u64))\n\t\t\t\treturn false;\n\t\t\tinfo->reg_type = PTR_TO_SOCKET_OR_NULL;\n\t\t\tbreak;\n\t\tcase offsetof(struct bpf_sock_ops, skb_data):\n\t\t\tif (size != sizeof(__u64))\n\t\t\t\treturn false;\n\t\t\tinfo->reg_type = PTR_TO_PACKET;\n\t\t\tbreak;\n\t\tcase offsetof(struct bpf_sock_ops, skb_data_end):\n\t\t\tif (size != sizeof(__u64))\n\t\t\t\treturn false;\n\t\t\tinfo->reg_type = PTR_TO_PACKET_END;\n\t\t\tbreak;\n\t\tcase offsetof(struct bpf_sock_ops, skb_tcp_flags):\n\t\t\tbpf_ctx_record_field_size(info, size_default);\n\t\t\treturn bpf_ctx_narrow_access_ok(off, size,\n\t\t\t\t\t\t\tsize_default);\n\t\tcase offsetof(struct bpf_sock_ops, skb_hwtstamp):\n\t\t\tif (size != sizeof(__u64))\n\t\t\t\treturn false;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tif (size != size_default)\n\t\t\t\treturn false;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn true;\n}\n\nstatic int sk_skb_prologue(struct bpf_insn *insn_buf, bool direct_write,\n\t\t\t   const struct bpf_prog *prog)\n{\n\treturn bpf_unclone_prologue(insn_buf, direct_write, prog, SK_DROP);\n}\n\nstatic bool sk_skb_is_valid_access(int off, int size,\n\t\t\t\t   enum bpf_access_type type,\n\t\t\t\t   const struct bpf_prog *prog,\n\t\t\t\t   struct bpf_insn_access_aux *info)\n{\n\tswitch (off) {\n\tcase bpf_ctx_range(struct __sk_buff, tc_classid):\n\tcase bpf_ctx_range(struct __sk_buff, data_meta):\n\tcase bpf_ctx_range(struct __sk_buff, tstamp):\n\tcase bpf_ctx_range(struct __sk_buff, wire_len):\n\tcase bpf_ctx_range(struct __sk_buff, hwtstamp):\n\t\treturn false;\n\t}\n\n\tif (type == BPF_WRITE) {\n\t\tswitch (off) {\n\t\tcase bpf_ctx_range(struct __sk_buff, tc_index):\n\t\tcase bpf_ctx_range(struct __sk_buff, priority):\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tswitch (off) {\n\tcase bpf_ctx_range(struct __sk_buff, mark):\n\t\treturn false;\n\tcase bpf_ctx_range(struct __sk_buff, data):\n\t\tinfo->reg_type = PTR_TO_PACKET;\n\t\tbreak;\n\tcase bpf_ctx_range(struct __sk_buff, data_end):\n\t\tinfo->reg_type = PTR_TO_PACKET_END;\n\t\tbreak;\n\t}\n\n\treturn bpf_skb_is_valid_access(off, size, type, prog, info);\n}\n\nstatic bool sk_msg_is_valid_access(int off, int size,\n\t\t\t\t   enum bpf_access_type type,\n\t\t\t\t   const struct bpf_prog *prog,\n\t\t\t\t   struct bpf_insn_access_aux *info)\n{\n\tif (type == BPF_WRITE)\n\t\treturn false;\n\n\tif (off % size != 0)\n\t\treturn false;\n\n\tswitch (off) {\n\tcase offsetof(struct sk_msg_md, data):\n\t\tinfo->reg_type = PTR_TO_PACKET;\n\t\tif (size != sizeof(__u64))\n\t\t\treturn false;\n\t\tbreak;\n\tcase offsetof(struct sk_msg_md, data_end):\n\t\tinfo->reg_type = PTR_TO_PACKET_END;\n\t\tif (size != sizeof(__u64))\n\t\t\treturn false;\n\t\tbreak;\n\tcase offsetof(struct sk_msg_md, sk):\n\t\tif (size != sizeof(__u64))\n\t\t\treturn false;\n\t\tinfo->reg_type = PTR_TO_SOCKET;\n\t\tbreak;\n\tcase bpf_ctx_range(struct sk_msg_md, family):\n\tcase bpf_ctx_range(struct sk_msg_md, remote_ip4):\n\tcase bpf_ctx_range(struct sk_msg_md, local_ip4):\n\tcase bpf_ctx_range_till(struct sk_msg_md, remote_ip6[0], remote_ip6[3]):\n\tcase bpf_ctx_range_till(struct sk_msg_md, local_ip6[0], local_ip6[3]):\n\tcase bpf_ctx_range(struct sk_msg_md, remote_port):\n\tcase bpf_ctx_range(struct sk_msg_md, local_port):\n\tcase bpf_ctx_range(struct sk_msg_md, size):\n\t\tif (size != sizeof(__u32))\n\t\t\treturn false;\n\t\tbreak;\n\tdefault:\n\t\treturn false;\n\t}\n\treturn true;\n}\n\nstatic bool flow_dissector_is_valid_access(int off, int size,\n\t\t\t\t\t   enum bpf_access_type type,\n\t\t\t\t\t   const struct bpf_prog *prog,\n\t\t\t\t\t   struct bpf_insn_access_aux *info)\n{\n\tconst int size_default = sizeof(__u32);\n\n\tif (off < 0 || off >= sizeof(struct __sk_buff))\n\t\treturn false;\n\n\tif (type == BPF_WRITE)\n\t\treturn false;\n\n\tswitch (off) {\n\tcase bpf_ctx_range(struct __sk_buff, data):\n\t\tif (size != size_default)\n\t\t\treturn false;\n\t\tinfo->reg_type = PTR_TO_PACKET;\n\t\treturn true;\n\tcase bpf_ctx_range(struct __sk_buff, data_end):\n\t\tif (size != size_default)\n\t\t\treturn false;\n\t\tinfo->reg_type = PTR_TO_PACKET_END;\n\t\treturn true;\n\tcase bpf_ctx_range_ptr(struct __sk_buff, flow_keys):\n\t\tif (size != sizeof(__u64))\n\t\t\treturn false;\n\t\tinfo->reg_type = PTR_TO_FLOW_KEYS;\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic u32 flow_dissector_convert_ctx_access(enum bpf_access_type type,\n\t\t\t\t\t     const struct bpf_insn *si,\n\t\t\t\t\t     struct bpf_insn *insn_buf,\n\t\t\t\t\t     struct bpf_prog *prog,\n\t\t\t\t\t     u32 *target_size)\n\n{\n\tstruct bpf_insn *insn = insn_buf;\n\n\tswitch (si->off) {\n\tcase offsetof(struct __sk_buff, data):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct bpf_flow_dissector, data),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_flow_dissector, data));\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, data_end):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct bpf_flow_dissector, data_end),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_flow_dissector, data_end));\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, flow_keys):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct bpf_flow_dissector, flow_keys),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_flow_dissector, flow_keys));\n\t\tbreak;\n\t}\n\n\treturn insn - insn_buf;\n}\n\nstatic struct bpf_insn *bpf_convert_tstamp_type_read(const struct bpf_insn *si,\n\t\t\t\t\t\t     struct bpf_insn *insn)\n{\n\t__u8 value_reg = si->dst_reg;\n\t__u8 skb_reg = si->src_reg;\n\t \n\t__u8 tmp_reg = BPF_REG_AX;\n\n\t*insn++ = BPF_LDX_MEM(BPF_B, tmp_reg, skb_reg,\n\t\t\t      SKB_BF_MONO_TC_OFFSET);\n\t*insn++ = BPF_JMP32_IMM(BPF_JSET, tmp_reg,\n\t\t\t\tSKB_MONO_DELIVERY_TIME_MASK, 2);\n\t*insn++ = BPF_MOV32_IMM(value_reg, BPF_SKB_TSTAMP_UNSPEC);\n\t*insn++ = BPF_JMP_A(1);\n\t*insn++ = BPF_MOV32_IMM(value_reg, BPF_SKB_TSTAMP_DELIVERY_MONO);\n\n\treturn insn;\n}\n\nstatic struct bpf_insn *bpf_convert_shinfo_access(__u8 dst_reg, __u8 skb_reg,\n\t\t\t\t\t\t  struct bpf_insn *insn)\n{\n\t \n#ifdef NET_SKBUFF_DATA_USES_OFFSET\n\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, end),\n\t\t\t      BPF_REG_AX, skb_reg,\n\t\t\t      offsetof(struct sk_buff, end));\n\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, head),\n\t\t\t      dst_reg, skb_reg,\n\t\t\t      offsetof(struct sk_buff, head));\n\t*insn++ = BPF_ALU64_REG(BPF_ADD, dst_reg, BPF_REG_AX);\n#else\n\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, end),\n\t\t\t      dst_reg, skb_reg,\n\t\t\t      offsetof(struct sk_buff, end));\n#endif\n\n\treturn insn;\n}\n\nstatic struct bpf_insn *bpf_convert_tstamp_read(const struct bpf_prog *prog,\n\t\t\t\t\t\tconst struct bpf_insn *si,\n\t\t\t\t\t\tstruct bpf_insn *insn)\n{\n\t__u8 value_reg = si->dst_reg;\n\t__u8 skb_reg = si->src_reg;\n\n#ifdef CONFIG_NET_XGRESS\n\t \n\tif (!prog->tstamp_type_access) {\n\t\t \n\t\t__u8 tmp_reg = BPF_REG_AX;\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_B, tmp_reg, skb_reg, SKB_BF_MONO_TC_OFFSET);\n\t\t*insn++ = BPF_ALU32_IMM(BPF_AND, tmp_reg,\n\t\t\t\t\tTC_AT_INGRESS_MASK | SKB_MONO_DELIVERY_TIME_MASK);\n\t\t*insn++ = BPF_JMP32_IMM(BPF_JNE, tmp_reg,\n\t\t\t\t\tTC_AT_INGRESS_MASK | SKB_MONO_DELIVERY_TIME_MASK, 2);\n\t\t \n\t\t*insn++ = BPF_MOV64_IMM(value_reg, 0);\n\t\t*insn++ = BPF_JMP_A(1);\n\t}\n#endif\n\n\t*insn++ = BPF_LDX_MEM(BPF_DW, value_reg, skb_reg,\n\t\t\t      offsetof(struct sk_buff, tstamp));\n\treturn insn;\n}\n\nstatic struct bpf_insn *bpf_convert_tstamp_write(const struct bpf_prog *prog,\n\t\t\t\t\t\t const struct bpf_insn *si,\n\t\t\t\t\t\t struct bpf_insn *insn)\n{\n\t__u8 value_reg = si->src_reg;\n\t__u8 skb_reg = si->dst_reg;\n\n#ifdef CONFIG_NET_XGRESS\n\t \n\tif (!prog->tstamp_type_access) {\n\t\t__u8 tmp_reg = BPF_REG_AX;\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_B, tmp_reg, skb_reg, SKB_BF_MONO_TC_OFFSET);\n\t\t \n\t\t*insn++ = BPF_JMP32_IMM(BPF_JSET, tmp_reg, TC_AT_INGRESS_MASK, 1);\n\t\t \n\t\t*insn++ = BPF_JMP_A(2);\n\t\t \n\t\t*insn++ = BPF_ALU32_IMM(BPF_AND, tmp_reg, ~SKB_MONO_DELIVERY_TIME_MASK);\n\t\t*insn++ = BPF_STX_MEM(BPF_B, skb_reg, tmp_reg, SKB_BF_MONO_TC_OFFSET);\n\t}\n#endif\n\n\t \n\t*insn++ = BPF_RAW_INSN(BPF_CLASS(si->code) | BPF_DW | BPF_MEM,\n\t\t\t       skb_reg, value_reg, offsetof(struct sk_buff, tstamp), si->imm);\n\treturn insn;\n}\n\n#define BPF_EMIT_STORE(size, si, off)\t\t\t\t\t\\\n\tBPF_RAW_INSN(BPF_CLASS((si)->code) | (size) | BPF_MEM,\t\t\\\n\t\t     (si)->dst_reg, (si)->src_reg, (off), (si)->imm)\n\nstatic u32 bpf_convert_ctx_access(enum bpf_access_type type,\n\t\t\t\t  const struct bpf_insn *si,\n\t\t\t\t  struct bpf_insn *insn_buf,\n\t\t\t\t  struct bpf_prog *prog, u32 *target_size)\n{\n\tstruct bpf_insn *insn = insn_buf;\n\tint off;\n\n\tswitch (si->off) {\n\tcase offsetof(struct __sk_buff, len):\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t      bpf_target_off(struct sk_buff, len, 4,\n\t\t\t\t\t\t     target_size));\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, protocol):\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->src_reg,\n\t\t\t\t      bpf_target_off(struct sk_buff, protocol, 2,\n\t\t\t\t\t\t     target_size));\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, vlan_proto):\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->src_reg,\n\t\t\t\t      bpf_target_off(struct sk_buff, vlan_proto, 2,\n\t\t\t\t\t\t     target_size));\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, priority):\n\t\tif (type == BPF_WRITE)\n\t\t\t*insn++ = BPF_EMIT_STORE(BPF_W, si,\n\t\t\t\t\t\t bpf_target_off(struct sk_buff, priority, 4,\n\t\t\t\t\t\t\t\ttarget_size));\n\t\telse\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t\t      bpf_target_off(struct sk_buff, priority, 4,\n\t\t\t\t\t\t\t     target_size));\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, ingress_ifindex):\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t      bpf_target_off(struct sk_buff, skb_iif, 4,\n\t\t\t\t\t\t     target_size));\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, ifindex):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, dev),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_buff, dev));\n\t\t*insn++ = BPF_JMP_IMM(BPF_JEQ, si->dst_reg, 0, 1);\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      bpf_target_off(struct net_device, ifindex, 4,\n\t\t\t\t\t\t     target_size));\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, hash):\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t      bpf_target_off(struct sk_buff, hash, 4,\n\t\t\t\t\t\t     target_size));\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, mark):\n\t\tif (type == BPF_WRITE)\n\t\t\t*insn++ = BPF_EMIT_STORE(BPF_W, si,\n\t\t\t\t\t\t bpf_target_off(struct sk_buff, mark, 4,\n\t\t\t\t\t\t\t\ttarget_size));\n\t\telse\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t\t      bpf_target_off(struct sk_buff, mark, 4,\n\t\t\t\t\t\t\t     target_size));\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, pkt_type):\n\t\t*target_size = 1;\n\t\t*insn++ = BPF_LDX_MEM(BPF_B, si->dst_reg, si->src_reg,\n\t\t\t\t      PKT_TYPE_OFFSET);\n\t\t*insn++ = BPF_ALU32_IMM(BPF_AND, si->dst_reg, PKT_TYPE_MAX);\n#ifdef __BIG_ENDIAN_BITFIELD\n\t\t*insn++ = BPF_ALU32_IMM(BPF_RSH, si->dst_reg, 5);\n#endif\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, queue_mapping):\n\t\tif (type == BPF_WRITE) {\n\t\t\tu32 off = bpf_target_off(struct sk_buff, queue_mapping, 2, target_size);\n\n\t\t\tif (BPF_CLASS(si->code) == BPF_ST && si->imm >= NO_QUEUE_MAPPING) {\n\t\t\t\t*insn++ = BPF_JMP_A(0);  \n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (BPF_CLASS(si->code) == BPF_STX)\n\t\t\t\t*insn++ = BPF_JMP_IMM(BPF_JGE, si->src_reg, NO_QUEUE_MAPPING, 1);\n\t\t\t*insn++ = BPF_EMIT_STORE(BPF_H, si, off);\n\t\t} else {\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->src_reg,\n\t\t\t\t\t      bpf_target_off(struct sk_buff,\n\t\t\t\t\t\t\t     queue_mapping,\n\t\t\t\t\t\t\t     2, target_size));\n\t\t}\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, vlan_present):\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t      bpf_target_off(struct sk_buff,\n\t\t\t\t\t\t     vlan_all, 4, target_size));\n\t\t*insn++ = BPF_JMP_IMM(BPF_JEQ, si->dst_reg, 0, 1);\n\t\t*insn++ = BPF_ALU32_IMM(BPF_MOV, si->dst_reg, 1);\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, vlan_tci):\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->src_reg,\n\t\t\t\t      bpf_target_off(struct sk_buff, vlan_tci, 2,\n\t\t\t\t\t\t     target_size));\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, cb[0]) ...\n\t     offsetofend(struct __sk_buff, cb[4]) - 1:\n\t\tBUILD_BUG_ON(sizeof_field(struct qdisc_skb_cb, data) < 20);\n\t\tBUILD_BUG_ON((offsetof(struct sk_buff, cb) +\n\t\t\t      offsetof(struct qdisc_skb_cb, data)) %\n\t\t\t     sizeof(__u64));\n\n\t\tprog->cb_access = 1;\n\t\toff  = si->off;\n\t\toff -= offsetof(struct __sk_buff, cb[0]);\n\t\toff += offsetof(struct sk_buff, cb);\n\t\toff += offsetof(struct qdisc_skb_cb, data);\n\t\tif (type == BPF_WRITE)\n\t\t\t*insn++ = BPF_EMIT_STORE(BPF_SIZE(si->code), si, off);\n\t\telse\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_SIZE(si->code), si->dst_reg,\n\t\t\t\t\t      si->src_reg, off);\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, tc_classid):\n\t\tBUILD_BUG_ON(sizeof_field(struct qdisc_skb_cb, tc_classid) != 2);\n\n\t\toff  = si->off;\n\t\toff -= offsetof(struct __sk_buff, tc_classid);\n\t\toff += offsetof(struct sk_buff, cb);\n\t\toff += offsetof(struct qdisc_skb_cb, tc_classid);\n\t\t*target_size = 2;\n\t\tif (type == BPF_WRITE)\n\t\t\t*insn++ = BPF_EMIT_STORE(BPF_H, si, off);\n\t\telse\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg,\n\t\t\t\t\t      si->src_reg, off);\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, data):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, data),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_buff, data));\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, data_meta):\n\t\toff  = si->off;\n\t\toff -= offsetof(struct __sk_buff, data_meta);\n\t\toff += offsetof(struct sk_buff, cb);\n\t\toff += offsetof(struct bpf_skb_data_end, data_meta);\n\t\t*insn++ = BPF_LDX_MEM(BPF_SIZEOF(void *), si->dst_reg,\n\t\t\t\t      si->src_reg, off);\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, data_end):\n\t\toff  = si->off;\n\t\toff -= offsetof(struct __sk_buff, data_end);\n\t\toff += offsetof(struct sk_buff, cb);\n\t\toff += offsetof(struct bpf_skb_data_end, data_end);\n\t\t*insn++ = BPF_LDX_MEM(BPF_SIZEOF(void *), si->dst_reg,\n\t\t\t\t      si->src_reg, off);\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, tc_index):\n#ifdef CONFIG_NET_SCHED\n\t\tif (type == BPF_WRITE)\n\t\t\t*insn++ = BPF_EMIT_STORE(BPF_H, si,\n\t\t\t\t\t\t bpf_target_off(struct sk_buff, tc_index, 2,\n\t\t\t\t\t\t\t\ttarget_size));\n\t\telse\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->src_reg,\n\t\t\t\t\t      bpf_target_off(struct sk_buff, tc_index, 2,\n\t\t\t\t\t\t\t     target_size));\n#else\n\t\t*target_size = 2;\n\t\tif (type == BPF_WRITE)\n\t\t\t*insn++ = BPF_MOV64_REG(si->dst_reg, si->dst_reg);\n\t\telse\n\t\t\t*insn++ = BPF_MOV64_IMM(si->dst_reg, 0);\n#endif\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, napi_id):\n#if defined(CONFIG_NET_RX_BUSY_POLL)\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t      bpf_target_off(struct sk_buff, napi_id, 4,\n\t\t\t\t\t\t     target_size));\n\t\t*insn++ = BPF_JMP_IMM(BPF_JGE, si->dst_reg, MIN_NAPI_ID, 1);\n\t\t*insn++ = BPF_MOV64_IMM(si->dst_reg, 0);\n#else\n\t\t*target_size = 4;\n\t\t*insn++ = BPF_MOV64_IMM(si->dst_reg, 0);\n#endif\n\t\tbreak;\n\tcase offsetof(struct __sk_buff, family):\n\t\tBUILD_BUG_ON(sizeof_field(struct sock_common, skc_family) != 2);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_buff, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->dst_reg,\n\t\t\t\t      bpf_target_off(struct sock_common,\n\t\t\t\t\t\t     skc_family,\n\t\t\t\t\t\t     2, target_size));\n\t\tbreak;\n\tcase offsetof(struct __sk_buff, remote_ip4):\n\t\tBUILD_BUG_ON(sizeof_field(struct sock_common, skc_daddr) != 4);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_buff, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      bpf_target_off(struct sock_common,\n\t\t\t\t\t\t     skc_daddr,\n\t\t\t\t\t\t     4, target_size));\n\t\tbreak;\n\tcase offsetof(struct __sk_buff, local_ip4):\n\t\tBUILD_BUG_ON(sizeof_field(struct sock_common,\n\t\t\t\t\t  skc_rcv_saddr) != 4);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_buff, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      bpf_target_off(struct sock_common,\n\t\t\t\t\t\t     skc_rcv_saddr,\n\t\t\t\t\t\t     4, target_size));\n\t\tbreak;\n\tcase offsetof(struct __sk_buff, remote_ip6[0]) ...\n\t     offsetof(struct __sk_buff, remote_ip6[3]):\n#if IS_ENABLED(CONFIG_IPV6)\n\t\tBUILD_BUG_ON(sizeof_field(struct sock_common,\n\t\t\t\t\t  skc_v6_daddr.s6_addr32[0]) != 4);\n\n\t\toff = si->off;\n\t\toff -= offsetof(struct __sk_buff, remote_ip6[0]);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_buff, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct sock_common,\n\t\t\t\t\t       skc_v6_daddr.s6_addr32[0]) +\n\t\t\t\t      off);\n#else\n\t\t*insn++ = BPF_MOV32_IMM(si->dst_reg, 0);\n#endif\n\t\tbreak;\n\tcase offsetof(struct __sk_buff, local_ip6[0]) ...\n\t     offsetof(struct __sk_buff, local_ip6[3]):\n#if IS_ENABLED(CONFIG_IPV6)\n\t\tBUILD_BUG_ON(sizeof_field(struct sock_common,\n\t\t\t\t\t  skc_v6_rcv_saddr.s6_addr32[0]) != 4);\n\n\t\toff = si->off;\n\t\toff -= offsetof(struct __sk_buff, local_ip6[0]);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_buff, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct sock_common,\n\t\t\t\t\t       skc_v6_rcv_saddr.s6_addr32[0]) +\n\t\t\t\t      off);\n#else\n\t\t*insn++ = BPF_MOV32_IMM(si->dst_reg, 0);\n#endif\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, remote_port):\n\t\tBUILD_BUG_ON(sizeof_field(struct sock_common, skc_dport) != 2);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_buff, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->dst_reg,\n\t\t\t\t      bpf_target_off(struct sock_common,\n\t\t\t\t\t\t     skc_dport,\n\t\t\t\t\t\t     2, target_size));\n#ifndef __BIG_ENDIAN_BITFIELD\n\t\t*insn++ = BPF_ALU32_IMM(BPF_LSH, si->dst_reg, 16);\n#endif\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, local_port):\n\t\tBUILD_BUG_ON(sizeof_field(struct sock_common, skc_num) != 2);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_buff, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->dst_reg,\n\t\t\t\t      bpf_target_off(struct sock_common,\n\t\t\t\t\t\t     skc_num, 2, target_size));\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, tstamp):\n\t\tBUILD_BUG_ON(sizeof_field(struct sk_buff, tstamp) != 8);\n\n\t\tif (type == BPF_WRITE)\n\t\t\tinsn = bpf_convert_tstamp_write(prog, si, insn);\n\t\telse\n\t\t\tinsn = bpf_convert_tstamp_read(prog, si, insn);\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, tstamp_type):\n\t\tinsn = bpf_convert_tstamp_type_read(si, insn);\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, gso_segs):\n\t\tinsn = bpf_convert_shinfo_access(si->dst_reg, si->src_reg, insn);\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct skb_shared_info, gso_segs),\n\t\t\t\t      si->dst_reg, si->dst_reg,\n\t\t\t\t      bpf_target_off(struct skb_shared_info,\n\t\t\t\t\t\t     gso_segs, 2,\n\t\t\t\t\t\t     target_size));\n\t\tbreak;\n\tcase offsetof(struct __sk_buff, gso_size):\n\t\tinsn = bpf_convert_shinfo_access(si->dst_reg, si->src_reg, insn);\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct skb_shared_info, gso_size),\n\t\t\t\t      si->dst_reg, si->dst_reg,\n\t\t\t\t      bpf_target_off(struct skb_shared_info,\n\t\t\t\t\t\t     gso_size, 2,\n\t\t\t\t\t\t     target_size));\n\t\tbreak;\n\tcase offsetof(struct __sk_buff, wire_len):\n\t\tBUILD_BUG_ON(sizeof_field(struct qdisc_skb_cb, pkt_len) != 4);\n\n\t\toff = si->off;\n\t\toff -= offsetof(struct __sk_buff, wire_len);\n\t\toff += offsetof(struct sk_buff, cb);\n\t\toff += offsetof(struct qdisc_skb_cb, pkt_len);\n\t\t*target_size = 4;\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg, off);\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, sk):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_buff, sk));\n\t\tbreak;\n\tcase offsetof(struct __sk_buff, hwtstamp):\n\t\tBUILD_BUG_ON(sizeof_field(struct skb_shared_hwtstamps, hwtstamp) != 8);\n\t\tBUILD_BUG_ON(offsetof(struct skb_shared_hwtstamps, hwtstamp) != 0);\n\n\t\tinsn = bpf_convert_shinfo_access(si->dst_reg, si->src_reg, insn);\n\t\t*insn++ = BPF_LDX_MEM(BPF_DW,\n\t\t\t\t      si->dst_reg, si->dst_reg,\n\t\t\t\t      bpf_target_off(struct skb_shared_info,\n\t\t\t\t\t\t     hwtstamps, 8,\n\t\t\t\t\t\t     target_size));\n\t\tbreak;\n\t}\n\n\treturn insn - insn_buf;\n}\n\nu32 bpf_sock_convert_ctx_access(enum bpf_access_type type,\n\t\t\t\tconst struct bpf_insn *si,\n\t\t\t\tstruct bpf_insn *insn_buf,\n\t\t\t\tstruct bpf_prog *prog, u32 *target_size)\n{\n\tstruct bpf_insn *insn = insn_buf;\n\tint off;\n\n\tswitch (si->off) {\n\tcase offsetof(struct bpf_sock, bound_dev_if):\n\t\tBUILD_BUG_ON(sizeof_field(struct sock, sk_bound_dev_if) != 4);\n\n\t\tif (type == BPF_WRITE)\n\t\t\t*insn++ = BPF_EMIT_STORE(BPF_W, si,\n\t\t\t\t\t\t offsetof(struct sock, sk_bound_dev_if));\n\t\telse\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sock, sk_bound_dev_if));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock, mark):\n\t\tBUILD_BUG_ON(sizeof_field(struct sock, sk_mark) != 4);\n\n\t\tif (type == BPF_WRITE)\n\t\t\t*insn++ = BPF_EMIT_STORE(BPF_W, si,\n\t\t\t\t\t\t offsetof(struct sock, sk_mark));\n\t\telse\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sock, sk_mark));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock, priority):\n\t\tBUILD_BUG_ON(sizeof_field(struct sock, sk_priority) != 4);\n\n\t\tif (type == BPF_WRITE)\n\t\t\t*insn++ = BPF_EMIT_STORE(BPF_W, si,\n\t\t\t\t\t\t offsetof(struct sock, sk_priority));\n\t\telse\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sock, sk_priority));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock, family):\n\t\t*insn++ = BPF_LDX_MEM(\n\t\t\tBPF_FIELD_SIZEOF(struct sock_common, skc_family),\n\t\t\tsi->dst_reg, si->src_reg,\n\t\t\tbpf_target_off(struct sock_common,\n\t\t\t\t       skc_family,\n\t\t\t\t       sizeof_field(struct sock_common,\n\t\t\t\t\t\t    skc_family),\n\t\t\t\t       target_size));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock, type):\n\t\t*insn++ = BPF_LDX_MEM(\n\t\t\tBPF_FIELD_SIZEOF(struct sock, sk_type),\n\t\t\tsi->dst_reg, si->src_reg,\n\t\t\tbpf_target_off(struct sock, sk_type,\n\t\t\t\t       sizeof_field(struct sock, sk_type),\n\t\t\t\t       target_size));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock, protocol):\n\t\t*insn++ = BPF_LDX_MEM(\n\t\t\tBPF_FIELD_SIZEOF(struct sock, sk_protocol),\n\t\t\tsi->dst_reg, si->src_reg,\n\t\t\tbpf_target_off(struct sock, sk_protocol,\n\t\t\t\t       sizeof_field(struct sock, sk_protocol),\n\t\t\t\t       target_size));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock, src_ip4):\n\t\t*insn++ = BPF_LDX_MEM(\n\t\t\tBPF_SIZE(si->code), si->dst_reg, si->src_reg,\n\t\t\tbpf_target_off(struct sock_common, skc_rcv_saddr,\n\t\t\t\t       sizeof_field(struct sock_common,\n\t\t\t\t\t\t    skc_rcv_saddr),\n\t\t\t\t       target_size));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock, dst_ip4):\n\t\t*insn++ = BPF_LDX_MEM(\n\t\t\tBPF_SIZE(si->code), si->dst_reg, si->src_reg,\n\t\t\tbpf_target_off(struct sock_common, skc_daddr,\n\t\t\t\t       sizeof_field(struct sock_common,\n\t\t\t\t\t\t    skc_daddr),\n\t\t\t\t       target_size));\n\t\tbreak;\n\n\tcase bpf_ctx_range_till(struct bpf_sock, src_ip6[0], src_ip6[3]):\n#if IS_ENABLED(CONFIG_IPV6)\n\t\toff = si->off;\n\t\toff -= offsetof(struct bpf_sock, src_ip6[0]);\n\t\t*insn++ = BPF_LDX_MEM(\n\t\t\tBPF_SIZE(si->code), si->dst_reg, si->src_reg,\n\t\t\tbpf_target_off(\n\t\t\t\tstruct sock_common,\n\t\t\t\tskc_v6_rcv_saddr.s6_addr32[0],\n\t\t\t\tsizeof_field(struct sock_common,\n\t\t\t\t\t     skc_v6_rcv_saddr.s6_addr32[0]),\n\t\t\t\ttarget_size) + off);\n#else\n\t\t(void)off;\n\t\t*insn++ = BPF_MOV32_IMM(si->dst_reg, 0);\n#endif\n\t\tbreak;\n\n\tcase bpf_ctx_range_till(struct bpf_sock, dst_ip6[0], dst_ip6[3]):\n#if IS_ENABLED(CONFIG_IPV6)\n\t\toff = si->off;\n\t\toff -= offsetof(struct bpf_sock, dst_ip6[0]);\n\t\t*insn++ = BPF_LDX_MEM(\n\t\t\tBPF_SIZE(si->code), si->dst_reg, si->src_reg,\n\t\t\tbpf_target_off(struct sock_common,\n\t\t\t\t       skc_v6_daddr.s6_addr32[0],\n\t\t\t\t       sizeof_field(struct sock_common,\n\t\t\t\t\t\t    skc_v6_daddr.s6_addr32[0]),\n\t\t\t\t       target_size) + off);\n#else\n\t\t*insn++ = BPF_MOV32_IMM(si->dst_reg, 0);\n\t\t*target_size = 4;\n#endif\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock, src_port):\n\t\t*insn++ = BPF_LDX_MEM(\n\t\t\tBPF_FIELD_SIZEOF(struct sock_common, skc_num),\n\t\t\tsi->dst_reg, si->src_reg,\n\t\t\tbpf_target_off(struct sock_common, skc_num,\n\t\t\t\t       sizeof_field(struct sock_common,\n\t\t\t\t\t\t    skc_num),\n\t\t\t\t       target_size));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock, dst_port):\n\t\t*insn++ = BPF_LDX_MEM(\n\t\t\tBPF_FIELD_SIZEOF(struct sock_common, skc_dport),\n\t\t\tsi->dst_reg, si->src_reg,\n\t\t\tbpf_target_off(struct sock_common, skc_dport,\n\t\t\t\t       sizeof_field(struct sock_common,\n\t\t\t\t\t\t    skc_dport),\n\t\t\t\t       target_size));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock, state):\n\t\t*insn++ = BPF_LDX_MEM(\n\t\t\tBPF_FIELD_SIZEOF(struct sock_common, skc_state),\n\t\t\tsi->dst_reg, si->src_reg,\n\t\t\tbpf_target_off(struct sock_common, skc_state,\n\t\t\t\t       sizeof_field(struct sock_common,\n\t\t\t\t\t\t    skc_state),\n\t\t\t\t       target_size));\n\t\tbreak;\n\tcase offsetof(struct bpf_sock, rx_queue_mapping):\n#ifdef CONFIG_SOCK_RX_QUEUE_MAPPING\n\t\t*insn++ = BPF_LDX_MEM(\n\t\t\tBPF_FIELD_SIZEOF(struct sock, sk_rx_queue_mapping),\n\t\t\tsi->dst_reg, si->src_reg,\n\t\t\tbpf_target_off(struct sock, sk_rx_queue_mapping,\n\t\t\t\t       sizeof_field(struct sock,\n\t\t\t\t\t\t    sk_rx_queue_mapping),\n\t\t\t\t       target_size));\n\t\t*insn++ = BPF_JMP_IMM(BPF_JNE, si->dst_reg, NO_QUEUE_MAPPING,\n\t\t\t\t      1);\n\t\t*insn++ = BPF_MOV64_IMM(si->dst_reg, -1);\n#else\n\t\t*insn++ = BPF_MOV64_IMM(si->dst_reg, -1);\n\t\t*target_size = 2;\n#endif\n\t\tbreak;\n\t}\n\n\treturn insn - insn_buf;\n}\n\nstatic u32 tc_cls_act_convert_ctx_access(enum bpf_access_type type,\n\t\t\t\t\t const struct bpf_insn *si,\n\t\t\t\t\t struct bpf_insn *insn_buf,\n\t\t\t\t\t struct bpf_prog *prog, u32 *target_size)\n{\n\tstruct bpf_insn *insn = insn_buf;\n\n\tswitch (si->off) {\n\tcase offsetof(struct __sk_buff, ifindex):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, dev),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_buff, dev));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      bpf_target_off(struct net_device, ifindex, 4,\n\t\t\t\t\t\t     target_size));\n\t\tbreak;\n\tdefault:\n\t\treturn bpf_convert_ctx_access(type, si, insn_buf, prog,\n\t\t\t\t\t      target_size);\n\t}\n\n\treturn insn - insn_buf;\n}\n\nstatic u32 xdp_convert_ctx_access(enum bpf_access_type type,\n\t\t\t\t  const struct bpf_insn *si,\n\t\t\t\t  struct bpf_insn *insn_buf,\n\t\t\t\t  struct bpf_prog *prog, u32 *target_size)\n{\n\tstruct bpf_insn *insn = insn_buf;\n\n\tswitch (si->off) {\n\tcase offsetof(struct xdp_md, data):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct xdp_buff, data),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct xdp_buff, data));\n\t\tbreak;\n\tcase offsetof(struct xdp_md, data_meta):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct xdp_buff, data_meta),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct xdp_buff, data_meta));\n\t\tbreak;\n\tcase offsetof(struct xdp_md, data_end):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct xdp_buff, data_end),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct xdp_buff, data_end));\n\t\tbreak;\n\tcase offsetof(struct xdp_md, ingress_ifindex):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct xdp_buff, rxq),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct xdp_buff, rxq));\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct xdp_rxq_info, dev),\n\t\t\t\t      si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct xdp_rxq_info, dev));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct net_device, ifindex));\n\t\tbreak;\n\tcase offsetof(struct xdp_md, rx_queue_index):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct xdp_buff, rxq),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct xdp_buff, rxq));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct xdp_rxq_info,\n\t\t\t\t\t       queue_index));\n\t\tbreak;\n\tcase offsetof(struct xdp_md, egress_ifindex):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct xdp_buff, txq),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct xdp_buff, txq));\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct xdp_txq_info, dev),\n\t\t\t\t      si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct xdp_txq_info, dev));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct net_device, ifindex));\n\t\tbreak;\n\t}\n\n\treturn insn - insn_buf;\n}\n\n \n#define SOCK_ADDR_LOAD_NESTED_FIELD_SIZE_OFF(S, NS, F, NF, SIZE, OFF)\t       \\\n\tdo {\t\t\t\t\t\t\t\t       \\\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(S, F), si->dst_reg,     \\\n\t\t\t\t      si->src_reg, offsetof(S, F));\t       \\\n\t\t*insn++ = BPF_LDX_MEM(\t\t\t\t\t       \\\n\t\t\tSIZE, si->dst_reg, si->dst_reg,\t\t\t       \\\n\t\t\tbpf_target_off(NS, NF, sizeof_field(NS, NF),\t       \\\n\t\t\t\t       target_size)\t\t\t       \\\n\t\t\t\t+ OFF);\t\t\t\t\t       \\\n\t} while (0)\n\n#define SOCK_ADDR_LOAD_NESTED_FIELD(S, NS, F, NF)\t\t\t       \\\n\tSOCK_ADDR_LOAD_NESTED_FIELD_SIZE_OFF(S, NS, F, NF,\t\t       \\\n\t\t\t\t\t     BPF_FIELD_SIZEOF(NS, NF), 0)\n\n \n#define SOCK_ADDR_STORE_NESTED_FIELD_OFF(S, NS, F, NF, SIZE, OFF, TF)\t       \\\n\tdo {\t\t\t\t\t\t\t\t       \\\n\t\tint tmp_reg = BPF_REG_9;\t\t\t\t       \\\n\t\tif (si->src_reg == tmp_reg || si->dst_reg == tmp_reg)\t       \\\n\t\t\t--tmp_reg;\t\t\t\t\t       \\\n\t\tif (si->src_reg == tmp_reg || si->dst_reg == tmp_reg)\t       \\\n\t\t\t--tmp_reg;\t\t\t\t\t       \\\n\t\t*insn++ = BPF_STX_MEM(BPF_DW, si->dst_reg, tmp_reg,\t       \\\n\t\t\t\t      offsetof(S, TF));\t\t\t       \\\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(S, F), tmp_reg,\t       \\\n\t\t\t\t      si->dst_reg, offsetof(S, F));\t       \\\n\t\t*insn++ = BPF_RAW_INSN(SIZE | BPF_MEM | BPF_CLASS(si->code),   \\\n\t\t\t\t       tmp_reg, si->src_reg,\t\t       \\\n\t\t\tbpf_target_off(NS, NF, sizeof_field(NS, NF),\t       \\\n\t\t\t\t       target_size)\t\t\t       \\\n\t\t\t\t       + OFF,\t\t\t\t       \\\n\t\t\t\t       si->imm);\t\t\t       \\\n\t\t*insn++ = BPF_LDX_MEM(BPF_DW, tmp_reg, si->dst_reg,\t       \\\n\t\t\t\t      offsetof(S, TF));\t\t\t       \\\n\t} while (0)\n\n#define SOCK_ADDR_LOAD_OR_STORE_NESTED_FIELD_SIZE_OFF(S, NS, F, NF, SIZE, OFF, \\\n\t\t\t\t\t\t      TF)\t\t       \\\n\tdo {\t\t\t\t\t\t\t\t       \\\n\t\tif (type == BPF_WRITE) {\t\t\t\t       \\\n\t\t\tSOCK_ADDR_STORE_NESTED_FIELD_OFF(S, NS, F, NF, SIZE,   \\\n\t\t\t\t\t\t\t OFF, TF);\t       \\\n\t\t} else {\t\t\t\t\t\t       \\\n\t\t\tSOCK_ADDR_LOAD_NESTED_FIELD_SIZE_OFF(\t\t       \\\n\t\t\t\tS, NS, F, NF, SIZE, OFF);  \\\n\t\t}\t\t\t\t\t\t\t       \\\n\t} while (0)\n\n#define SOCK_ADDR_LOAD_OR_STORE_NESTED_FIELD(S, NS, F, NF, TF)\t\t       \\\n\tSOCK_ADDR_LOAD_OR_STORE_NESTED_FIELD_SIZE_OFF(\t\t\t       \\\n\t\tS, NS, F, NF, BPF_FIELD_SIZEOF(NS, NF), 0, TF)\n\nstatic u32 sock_addr_convert_ctx_access(enum bpf_access_type type,\n\t\t\t\t\tconst struct bpf_insn *si,\n\t\t\t\t\tstruct bpf_insn *insn_buf,\n\t\t\t\t\tstruct bpf_prog *prog, u32 *target_size)\n{\n\tint off, port_size = sizeof_field(struct sockaddr_in6, sin6_port);\n\tstruct bpf_insn *insn = insn_buf;\n\n\tswitch (si->off) {\n\tcase offsetof(struct bpf_sock_addr, user_family):\n\t\tSOCK_ADDR_LOAD_NESTED_FIELD(struct bpf_sock_addr_kern,\n\t\t\t\t\t    struct sockaddr, uaddr, sa_family);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_addr, user_ip4):\n\t\tSOCK_ADDR_LOAD_OR_STORE_NESTED_FIELD_SIZE_OFF(\n\t\t\tstruct bpf_sock_addr_kern, struct sockaddr_in, uaddr,\n\t\t\tsin_addr, BPF_SIZE(si->code), 0, tmp_reg);\n\t\tbreak;\n\n\tcase bpf_ctx_range_till(struct bpf_sock_addr, user_ip6[0], user_ip6[3]):\n\t\toff = si->off;\n\t\toff -= offsetof(struct bpf_sock_addr, user_ip6[0]);\n\t\tSOCK_ADDR_LOAD_OR_STORE_NESTED_FIELD_SIZE_OFF(\n\t\t\tstruct bpf_sock_addr_kern, struct sockaddr_in6, uaddr,\n\t\t\tsin6_addr.s6_addr32[0], BPF_SIZE(si->code), off,\n\t\t\ttmp_reg);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_addr, user_port):\n\t\t \n\t\tBUILD_BUG_ON(offsetof(struct sockaddr_in, sin_port) !=\n\t\t\t     offsetof(struct sockaddr_in6, sin6_port));\n\t\tBUILD_BUG_ON(sizeof_field(struct sockaddr_in, sin_port) !=\n\t\t\t     sizeof_field(struct sockaddr_in6, sin6_port));\n\t\t \n\t\tport_size = min(port_size, BPF_LDST_BYTES(si));\n\t\tSOCK_ADDR_LOAD_OR_STORE_NESTED_FIELD_SIZE_OFF(\n\t\t\tstruct bpf_sock_addr_kern, struct sockaddr_in6, uaddr,\n\t\t\tsin6_port, bytes_to_bpf_size(port_size), 0, tmp_reg);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_addr, family):\n\t\tSOCK_ADDR_LOAD_NESTED_FIELD(struct bpf_sock_addr_kern,\n\t\t\t\t\t    struct sock, sk, sk_family);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_addr, type):\n\t\tSOCK_ADDR_LOAD_NESTED_FIELD(struct bpf_sock_addr_kern,\n\t\t\t\t\t    struct sock, sk, sk_type);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_addr, protocol):\n\t\tSOCK_ADDR_LOAD_NESTED_FIELD(struct bpf_sock_addr_kern,\n\t\t\t\t\t    struct sock, sk, sk_protocol);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_addr, msg_src_ip4):\n\t\t \n\t\tSOCK_ADDR_LOAD_OR_STORE_NESTED_FIELD_SIZE_OFF(\n\t\t\tstruct bpf_sock_addr_kern, struct in_addr, t_ctx,\n\t\t\ts_addr, BPF_SIZE(si->code), 0, tmp_reg);\n\t\tbreak;\n\n\tcase bpf_ctx_range_till(struct bpf_sock_addr, msg_src_ip6[0],\n\t\t\t\tmsg_src_ip6[3]):\n\t\toff = si->off;\n\t\toff -= offsetof(struct bpf_sock_addr, msg_src_ip6[0]);\n\t\t \n\t\tSOCK_ADDR_LOAD_OR_STORE_NESTED_FIELD_SIZE_OFF(\n\t\t\tstruct bpf_sock_addr_kern, struct in6_addr, t_ctx,\n\t\t\ts6_addr32[0], BPF_SIZE(si->code), off, tmp_reg);\n\t\tbreak;\n\tcase offsetof(struct bpf_sock_addr, sk):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct bpf_sock_addr_kern, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_sock_addr_kern, sk));\n\t\tbreak;\n\t}\n\n\treturn insn - insn_buf;\n}\n\nstatic u32 sock_ops_convert_ctx_access(enum bpf_access_type type,\n\t\t\t\t       const struct bpf_insn *si,\n\t\t\t\t       struct bpf_insn *insn_buf,\n\t\t\t\t       struct bpf_prog *prog,\n\t\t\t\t       u32 *target_size)\n{\n\tstruct bpf_insn *insn = insn_buf;\n\tint off;\n\n \n#define SOCK_OPS_GET_FIELD(BPF_FIELD, OBJ_FIELD, OBJ)\t\t\t      \\\n\tdo {\t\t\t\t\t\t\t\t      \\\n\t\tint fullsock_reg = si->dst_reg, reg = BPF_REG_9, jmp = 2;     \\\n\t\tBUILD_BUG_ON(sizeof_field(OBJ, OBJ_FIELD) >\t\t      \\\n\t\t\t     sizeof_field(struct bpf_sock_ops, BPF_FIELD));   \\\n\t\tif (si->dst_reg == reg || si->src_reg == reg)\t\t      \\\n\t\t\treg--;\t\t\t\t\t\t      \\\n\t\tif (si->dst_reg == reg || si->src_reg == reg)\t\t      \\\n\t\t\treg--;\t\t\t\t\t\t      \\\n\t\tif (si->dst_reg == si->src_reg) {\t\t\t      \\\n\t\t\t*insn++ = BPF_STX_MEM(BPF_DW, si->src_reg, reg,\t      \\\n\t\t\t\t\t  offsetof(struct bpf_sock_ops_kern,  \\\n\t\t\t\t\t  temp));\t\t\t      \\\n\t\t\tfullsock_reg = reg;\t\t\t\t      \\\n\t\t\tjmp += 2;\t\t\t\t\t      \\\n\t\t}\t\t\t\t\t\t\t      \\\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\t\t\t      \\\n\t\t\t\t\t\tstruct bpf_sock_ops_kern,     \\\n\t\t\t\t\t\tis_fullsock),\t\t      \\\n\t\t\t\t      fullsock_reg, si->src_reg,\t      \\\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern,      \\\n\t\t\t\t\t       is_fullsock));\t\t      \\\n\t\t*insn++ = BPF_JMP_IMM(BPF_JEQ, fullsock_reg, 0, jmp);\t      \\\n\t\tif (si->dst_reg == si->src_reg)\t\t\t\t      \\\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_DW, reg, si->src_reg,\t      \\\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern,      \\\n\t\t\t\t      temp));\t\t\t\t      \\\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\t\t\t      \\\n\t\t\t\t\t\tstruct bpf_sock_ops_kern, sk),\\\n\t\t\t\t      si->dst_reg, si->src_reg,\t\t      \\\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern, sk));\\\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(OBJ,\t\t      \\\n\t\t\t\t\t\t       OBJ_FIELD),\t      \\\n\t\t\t\t      si->dst_reg, si->dst_reg,\t\t      \\\n\t\t\t\t      offsetof(OBJ, OBJ_FIELD));\t      \\\n\t\tif (si->dst_reg == si->src_reg)\t{\t\t\t      \\\n\t\t\t*insn++ = BPF_JMP_A(1);\t\t\t\t      \\\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_DW, reg, si->src_reg,\t      \\\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern,      \\\n\t\t\t\t      temp));\t\t\t\t      \\\n\t\t}\t\t\t\t\t\t\t      \\\n\t} while (0)\n\n#define SOCK_OPS_GET_SK()\t\t\t\t\t\t\t      \\\n\tdo {\t\t\t\t\t\t\t\t      \\\n\t\tint fullsock_reg = si->dst_reg, reg = BPF_REG_9, jmp = 1;     \\\n\t\tif (si->dst_reg == reg || si->src_reg == reg)\t\t      \\\n\t\t\treg--;\t\t\t\t\t\t      \\\n\t\tif (si->dst_reg == reg || si->src_reg == reg)\t\t      \\\n\t\t\treg--;\t\t\t\t\t\t      \\\n\t\tif (si->dst_reg == si->src_reg) {\t\t\t      \\\n\t\t\t*insn++ = BPF_STX_MEM(BPF_DW, si->src_reg, reg,\t      \\\n\t\t\t\t\t  offsetof(struct bpf_sock_ops_kern,  \\\n\t\t\t\t\t  temp));\t\t\t      \\\n\t\t\tfullsock_reg = reg;\t\t\t\t      \\\n\t\t\tjmp += 2;\t\t\t\t\t      \\\n\t\t}\t\t\t\t\t\t\t      \\\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\t\t\t      \\\n\t\t\t\t\t\tstruct bpf_sock_ops_kern,     \\\n\t\t\t\t\t\tis_fullsock),\t\t      \\\n\t\t\t\t      fullsock_reg, si->src_reg,\t      \\\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern,      \\\n\t\t\t\t\t       is_fullsock));\t\t      \\\n\t\t*insn++ = BPF_JMP_IMM(BPF_JEQ, fullsock_reg, 0, jmp);\t      \\\n\t\tif (si->dst_reg == si->src_reg)\t\t\t\t      \\\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_DW, reg, si->src_reg,\t      \\\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern,      \\\n\t\t\t\t      temp));\t\t\t\t      \\\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\t\t\t      \\\n\t\t\t\t\t\tstruct bpf_sock_ops_kern, sk),\\\n\t\t\t\t      si->dst_reg, si->src_reg,\t\t      \\\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern, sk));\\\n\t\tif (si->dst_reg == si->src_reg)\t{\t\t\t      \\\n\t\t\t*insn++ = BPF_JMP_A(1);\t\t\t\t      \\\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_DW, reg, si->src_reg,\t      \\\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern,      \\\n\t\t\t\t      temp));\t\t\t\t      \\\n\t\t}\t\t\t\t\t\t\t      \\\n\t} while (0)\n\n#define SOCK_OPS_GET_TCP_SOCK_FIELD(FIELD) \\\n\t\tSOCK_OPS_GET_FIELD(FIELD, FIELD, struct tcp_sock)\n\n \n#define SOCK_OPS_SET_FIELD(BPF_FIELD, OBJ_FIELD, OBJ)\t\t\t      \\\n\tdo {\t\t\t\t\t\t\t\t      \\\n\t\tint reg = BPF_REG_9;\t\t\t\t\t      \\\n\t\tBUILD_BUG_ON(sizeof_field(OBJ, OBJ_FIELD) >\t\t      \\\n\t\t\t     sizeof_field(struct bpf_sock_ops, BPF_FIELD));   \\\n\t\tif (si->dst_reg == reg || si->src_reg == reg)\t\t      \\\n\t\t\treg--;\t\t\t\t\t\t      \\\n\t\tif (si->dst_reg == reg || si->src_reg == reg)\t\t      \\\n\t\t\treg--;\t\t\t\t\t\t      \\\n\t\t*insn++ = BPF_STX_MEM(BPF_DW, si->dst_reg, reg,\t\t      \\\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern,      \\\n\t\t\t\t\t       temp));\t\t\t      \\\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\t\t\t      \\\n\t\t\t\t\t\tstruct bpf_sock_ops_kern,     \\\n\t\t\t\t\t\tis_fullsock),\t\t      \\\n\t\t\t\t      reg, si->dst_reg,\t\t\t      \\\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern,      \\\n\t\t\t\t\t       is_fullsock));\t\t      \\\n\t\t*insn++ = BPF_JMP_IMM(BPF_JEQ, reg, 0, 2);\t\t      \\\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\t\t\t      \\\n\t\t\t\t\t\tstruct bpf_sock_ops_kern, sk),\\\n\t\t\t\t      reg, si->dst_reg,\t\t\t      \\\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern, sk));\\\n\t\t*insn++ = BPF_RAW_INSN(BPF_FIELD_SIZEOF(OBJ, OBJ_FIELD) |     \\\n\t\t\t\t       BPF_MEM | BPF_CLASS(si->code),\t      \\\n\t\t\t\t       reg, si->src_reg,\t\t      \\\n\t\t\t\t       offsetof(OBJ, OBJ_FIELD),\t      \\\n\t\t\t\t       si->imm);\t\t\t      \\\n\t\t*insn++ = BPF_LDX_MEM(BPF_DW, reg, si->dst_reg,\t\t      \\\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern,      \\\n\t\t\t\t\t       temp));\t\t\t      \\\n\t} while (0)\n\n#define SOCK_OPS_GET_OR_SET_FIELD(BPF_FIELD, OBJ_FIELD, OBJ, TYPE)\t      \\\n\tdo {\t\t\t\t\t\t\t\t      \\\n\t\tif (TYPE == BPF_WRITE)\t\t\t\t\t      \\\n\t\t\tSOCK_OPS_SET_FIELD(BPF_FIELD, OBJ_FIELD, OBJ);\t      \\\n\t\telse\t\t\t\t\t\t\t      \\\n\t\t\tSOCK_OPS_GET_FIELD(BPF_FIELD, OBJ_FIELD, OBJ);\t      \\\n\t} while (0)\n\n\tswitch (si->off) {\n\tcase offsetof(struct bpf_sock_ops, op):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct bpf_sock_ops_kern,\n\t\t\t\t\t\t       op),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern, op));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, replylong[0]) ...\n\t     offsetof(struct bpf_sock_ops, replylong[3]):\n\t\tBUILD_BUG_ON(sizeof_field(struct bpf_sock_ops, reply) !=\n\t\t\t     sizeof_field(struct bpf_sock_ops_kern, reply));\n\t\tBUILD_BUG_ON(sizeof_field(struct bpf_sock_ops, replylong) !=\n\t\t\t     sizeof_field(struct bpf_sock_ops_kern, replylong));\n\t\toff = si->off;\n\t\toff -= offsetof(struct bpf_sock_ops, replylong[0]);\n\t\toff += offsetof(struct bpf_sock_ops_kern, replylong[0]);\n\t\tif (type == BPF_WRITE)\n\t\t\t*insn++ = BPF_EMIT_STORE(BPF_W, si, off);\n\t\telse\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t\t      off);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, family):\n\t\tBUILD_BUG_ON(sizeof_field(struct sock_common, skc_family) != 2);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\n\t\t\t\t\t      struct bpf_sock_ops_kern, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct sock_common, skc_family));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, remote_ip4):\n\t\tBUILD_BUG_ON(sizeof_field(struct sock_common, skc_daddr) != 4);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\n\t\t\t\t\t\tstruct bpf_sock_ops_kern, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct sock_common, skc_daddr));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, local_ip4):\n\t\tBUILD_BUG_ON(sizeof_field(struct sock_common,\n\t\t\t\t\t  skc_rcv_saddr) != 4);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\n\t\t\t\t\t      struct bpf_sock_ops_kern, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct sock_common,\n\t\t\t\t\t       skc_rcv_saddr));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, remote_ip6[0]) ...\n\t     offsetof(struct bpf_sock_ops, remote_ip6[3]):\n#if IS_ENABLED(CONFIG_IPV6)\n\t\tBUILD_BUG_ON(sizeof_field(struct sock_common,\n\t\t\t\t\t  skc_v6_daddr.s6_addr32[0]) != 4);\n\n\t\toff = si->off;\n\t\toff -= offsetof(struct bpf_sock_ops, remote_ip6[0]);\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\n\t\t\t\t\t\tstruct bpf_sock_ops_kern, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct sock_common,\n\t\t\t\t\t       skc_v6_daddr.s6_addr32[0]) +\n\t\t\t\t      off);\n#else\n\t\t*insn++ = BPF_MOV32_IMM(si->dst_reg, 0);\n#endif\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, local_ip6[0]) ...\n\t     offsetof(struct bpf_sock_ops, local_ip6[3]):\n#if IS_ENABLED(CONFIG_IPV6)\n\t\tBUILD_BUG_ON(sizeof_field(struct sock_common,\n\t\t\t\t\t  skc_v6_rcv_saddr.s6_addr32[0]) != 4);\n\n\t\toff = si->off;\n\t\toff -= offsetof(struct bpf_sock_ops, local_ip6[0]);\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\n\t\t\t\t\t\tstruct bpf_sock_ops_kern, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct sock_common,\n\t\t\t\t\t       skc_v6_rcv_saddr.s6_addr32[0]) +\n\t\t\t\t      off);\n#else\n\t\t*insn++ = BPF_MOV32_IMM(si->dst_reg, 0);\n#endif\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, remote_port):\n\t\tBUILD_BUG_ON(sizeof_field(struct sock_common, skc_dport) != 2);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\n\t\t\t\t\t\tstruct bpf_sock_ops_kern, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct sock_common, skc_dport));\n#ifndef __BIG_ENDIAN_BITFIELD\n\t\t*insn++ = BPF_ALU32_IMM(BPF_LSH, si->dst_reg, 16);\n#endif\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, local_port):\n\t\tBUILD_BUG_ON(sizeof_field(struct sock_common, skc_num) != 2);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\n\t\t\t\t\t\tstruct bpf_sock_ops_kern, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct sock_common, skc_num));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, is_fullsock):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\n\t\t\t\t\t\tstruct bpf_sock_ops_kern,\n\t\t\t\t\t\tis_fullsock),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern,\n\t\t\t\t\t       is_fullsock));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, state):\n\t\tBUILD_BUG_ON(sizeof_field(struct sock_common, skc_state) != 1);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\n\t\t\t\t\t\tstruct bpf_sock_ops_kern, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_B, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct sock_common, skc_state));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, rtt_min):\n\t\tBUILD_BUG_ON(sizeof_field(struct tcp_sock, rtt_min) !=\n\t\t\t     sizeof(struct minmax));\n\t\tBUILD_BUG_ON(sizeof(struct minmax) <\n\t\t\t     sizeof(struct minmax_sample));\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\n\t\t\t\t\t\tstruct bpf_sock_ops_kern, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct tcp_sock, rtt_min) +\n\t\t\t\t      sizeof_field(struct minmax_sample, t));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, bpf_sock_ops_cb_flags):\n\t\tSOCK_OPS_GET_FIELD(bpf_sock_ops_cb_flags, bpf_sock_ops_cb_flags,\n\t\t\t\t   struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, sk_txhash):\n\t\tSOCK_OPS_GET_OR_SET_FIELD(sk_txhash, sk_txhash,\n\t\t\t\t\t  struct sock, type);\n\t\tbreak;\n\tcase offsetof(struct bpf_sock_ops, snd_cwnd):\n\t\tSOCK_OPS_GET_TCP_SOCK_FIELD(snd_cwnd);\n\t\tbreak;\n\tcase offsetof(struct bpf_sock_ops, srtt_us):\n\t\tSOCK_OPS_GET_TCP_SOCK_FIELD(srtt_us);\n\t\tbreak;\n\tcase offsetof(struct bpf_sock_ops, snd_ssthresh):\n\t\tSOCK_OPS_GET_TCP_SOCK_FIELD(snd_ssthresh);\n\t\tbreak;\n\tcase offsetof(struct bpf_sock_ops, rcv_nxt):\n\t\tSOCK_OPS_GET_TCP_SOCK_FIELD(rcv_nxt);\n\t\tbreak;\n\tcase offsetof(struct bpf_sock_ops, snd_nxt):\n\t\tSOCK_OPS_GET_TCP_SOCK_FIELD(snd_nxt);\n\t\tbreak;\n\tcase offsetof(struct bpf_sock_ops, snd_una):\n\t\tSOCK_OPS_GET_TCP_SOCK_FIELD(snd_una);\n\t\tbreak;\n\tcase offsetof(struct bpf_sock_ops, mss_cache):\n\t\tSOCK_OPS_GET_TCP_SOCK_FIELD(mss_cache);\n\t\tbreak;\n\tcase offsetof(struct bpf_sock_ops, ecn_flags):\n\t\tSOCK_OPS_GET_TCP_SOCK_FIELD(ecn_flags);\n\t\tbreak;\n\tcase offsetof(struct bpf_sock_ops, rate_delivered):\n\t\tSOCK_OPS_GET_TCP_SOCK_FIELD(rate_delivered);\n\t\tbreak;\n\tcase offsetof(struct bpf_sock_ops, rate_interval_us):\n\t\tSOCK_OPS_GET_TCP_SOCK_FIELD(rate_interval_us);\n\t\tbreak;\n\tcase offsetof(struct bpf_sock_ops, packets_out):\n\t\tSOCK_OPS_GET_TCP_SOCK_FIELD(packets_out);\n\t\tbreak;\n\tcase offsetof(struct bpf_sock_ops, retrans_out):\n\t\tSOCK_OPS_GET_TCP_SOCK_FIELD(retrans_out);\n\t\tbreak;\n\tcase offsetof(struct bpf_sock_ops, total_retrans):\n\t\tSOCK_OPS_GET_TCP_SOCK_FIELD(total_retrans);\n\t\tbreak;\n\tcase offsetof(struct bpf_sock_ops, segs_in):\n\t\tSOCK_OPS_GET_TCP_SOCK_FIELD(segs_in);\n\t\tbreak;\n\tcase offsetof(struct bpf_sock_ops, data_segs_in):\n\t\tSOCK_OPS_GET_TCP_SOCK_FIELD(data_segs_in);\n\t\tbreak;\n\tcase offsetof(struct bpf_sock_ops, segs_out):\n\t\tSOCK_OPS_GET_TCP_SOCK_FIELD(segs_out);\n\t\tbreak;\n\tcase offsetof(struct bpf_sock_ops, data_segs_out):\n\t\tSOCK_OPS_GET_TCP_SOCK_FIELD(data_segs_out);\n\t\tbreak;\n\tcase offsetof(struct bpf_sock_ops, lost_out):\n\t\tSOCK_OPS_GET_TCP_SOCK_FIELD(lost_out);\n\t\tbreak;\n\tcase offsetof(struct bpf_sock_ops, sacked_out):\n\t\tSOCK_OPS_GET_TCP_SOCK_FIELD(sacked_out);\n\t\tbreak;\n\tcase offsetof(struct bpf_sock_ops, bytes_received):\n\t\tSOCK_OPS_GET_TCP_SOCK_FIELD(bytes_received);\n\t\tbreak;\n\tcase offsetof(struct bpf_sock_ops, bytes_acked):\n\t\tSOCK_OPS_GET_TCP_SOCK_FIELD(bytes_acked);\n\t\tbreak;\n\tcase offsetof(struct bpf_sock_ops, sk):\n\t\tSOCK_OPS_GET_SK();\n\t\tbreak;\n\tcase offsetof(struct bpf_sock_ops, skb_data_end):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct bpf_sock_ops_kern,\n\t\t\t\t\t\t       skb_data_end),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern,\n\t\t\t\t\t       skb_data_end));\n\t\tbreak;\n\tcase offsetof(struct bpf_sock_ops, skb_data):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct bpf_sock_ops_kern,\n\t\t\t\t\t\t       skb),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern,\n\t\t\t\t\t       skb));\n\t\t*insn++ = BPF_JMP_IMM(BPF_JEQ, si->dst_reg, 0, 1);\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, data),\n\t\t\t\t      si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct sk_buff, data));\n\t\tbreak;\n\tcase offsetof(struct bpf_sock_ops, skb_len):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct bpf_sock_ops_kern,\n\t\t\t\t\t\t       skb),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern,\n\t\t\t\t\t       skb));\n\t\t*insn++ = BPF_JMP_IMM(BPF_JEQ, si->dst_reg, 0, 1);\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, len),\n\t\t\t\t      si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct sk_buff, len));\n\t\tbreak;\n\tcase offsetof(struct bpf_sock_ops, skb_tcp_flags):\n\t\toff = offsetof(struct sk_buff, cb);\n\t\toff += offsetof(struct tcp_skb_cb, tcp_flags);\n\t\t*target_size = sizeof_field(struct tcp_skb_cb, tcp_flags);\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct bpf_sock_ops_kern,\n\t\t\t\t\t\t       skb),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern,\n\t\t\t\t\t       skb));\n\t\t*insn++ = BPF_JMP_IMM(BPF_JEQ, si->dst_reg, 0, 1);\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct tcp_skb_cb,\n\t\t\t\t\t\t       tcp_flags),\n\t\t\t\t      si->dst_reg, si->dst_reg, off);\n\t\tbreak;\n\tcase offsetof(struct bpf_sock_ops, skb_hwtstamp): {\n\t\tstruct bpf_insn *jmp_on_null_skb;\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct bpf_sock_ops_kern,\n\t\t\t\t\t\t       skb),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern,\n\t\t\t\t\t       skb));\n\t\t \n\t\tjmp_on_null_skb = insn++;\n\t\tinsn = bpf_convert_shinfo_access(si->dst_reg, si->dst_reg, insn);\n\t\t*insn++ = BPF_LDX_MEM(BPF_DW, si->dst_reg, si->dst_reg,\n\t\t\t\t      bpf_target_off(struct skb_shared_info,\n\t\t\t\t\t\t     hwtstamps, 8,\n\t\t\t\t\t\t     target_size));\n\t\t*jmp_on_null_skb = BPF_JMP_IMM(BPF_JEQ, si->dst_reg, 0,\n\t\t\t\t\t       insn - jmp_on_null_skb - 1);\n\t\tbreak;\n\t}\n\t}\n\treturn insn - insn_buf;\n}\n\n \nstatic struct bpf_insn *bpf_convert_data_end_access(const struct bpf_insn *si,\n\t\t\t\t\t\t    struct bpf_insn *insn)\n{\n\tint reg;\n\tint temp_reg_off = offsetof(struct sk_buff, cb) +\n\t\t\t   offsetof(struct sk_skb_cb, temp_reg);\n\n\tif (si->src_reg == si->dst_reg) {\n\t\t \n\t\treg = BPF_REG_9;\n\t\tif (si->src_reg == reg || si->dst_reg == reg)\n\t\t\treg--;\n\t\tif (si->src_reg == reg || si->dst_reg == reg)\n\t\t\treg--;\n\t\t*insn++ = BPF_STX_MEM(BPF_DW, si->src_reg, reg, temp_reg_off);\n\t} else {\n\t\treg = si->dst_reg;\n\t}\n\n\t \n\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, data),\n\t\t\t      reg, si->src_reg,\n\t\t\t      offsetof(struct sk_buff, data));\n\t \n\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, len),\n\t\t\t      BPF_REG_AX, si->src_reg,\n\t\t\t      offsetof(struct sk_buff, len));\n\t \n\t*insn++ = BPF_ALU64_REG(BPF_ADD, reg, BPF_REG_AX);\n\t \n\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, data_len),\n\t\t\t      BPF_REG_AX, si->src_reg,\n\t\t\t      offsetof(struct sk_buff, data_len));\n\n\t \n\t*insn++ = BPF_ALU64_REG(BPF_SUB, reg, BPF_REG_AX);\n\n\tif (si->src_reg == si->dst_reg) {\n\t\t \n\t\t*insn++ = BPF_MOV64_REG(BPF_REG_AX, si->src_reg);\n\t\t*insn++ = BPF_MOV64_REG(si->dst_reg, reg);\n\t\t*insn++ = BPF_LDX_MEM(BPF_DW, reg, BPF_REG_AX, temp_reg_off);\n\t}\n\n\treturn insn;\n}\n\nstatic u32 sk_skb_convert_ctx_access(enum bpf_access_type type,\n\t\t\t\t     const struct bpf_insn *si,\n\t\t\t\t     struct bpf_insn *insn_buf,\n\t\t\t\t     struct bpf_prog *prog, u32 *target_size)\n{\n\tstruct bpf_insn *insn = insn_buf;\n\tint off;\n\n\tswitch (si->off) {\n\tcase offsetof(struct __sk_buff, data_end):\n\t\tinsn = bpf_convert_data_end_access(si, insn);\n\t\tbreak;\n\tcase offsetof(struct __sk_buff, cb[0]) ...\n\t     offsetofend(struct __sk_buff, cb[4]) - 1:\n\t\tBUILD_BUG_ON(sizeof_field(struct sk_skb_cb, data) < 20);\n\t\tBUILD_BUG_ON((offsetof(struct sk_buff, cb) +\n\t\t\t      offsetof(struct sk_skb_cb, data)) %\n\t\t\t     sizeof(__u64));\n\n\t\tprog->cb_access = 1;\n\t\toff  = si->off;\n\t\toff -= offsetof(struct __sk_buff, cb[0]);\n\t\toff += offsetof(struct sk_buff, cb);\n\t\toff += offsetof(struct sk_skb_cb, data);\n\t\tif (type == BPF_WRITE)\n\t\t\t*insn++ = BPF_EMIT_STORE(BPF_SIZE(si->code), si, off);\n\t\telse\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_SIZE(si->code), si->dst_reg,\n\t\t\t\t\t      si->src_reg, off);\n\t\tbreak;\n\n\n\tdefault:\n\t\treturn bpf_convert_ctx_access(type, si, insn_buf, prog,\n\t\t\t\t\t      target_size);\n\t}\n\n\treturn insn - insn_buf;\n}\n\nstatic u32 sk_msg_convert_ctx_access(enum bpf_access_type type,\n\t\t\t\t     const struct bpf_insn *si,\n\t\t\t\t     struct bpf_insn *insn_buf,\n\t\t\t\t     struct bpf_prog *prog, u32 *target_size)\n{\n\tstruct bpf_insn *insn = insn_buf;\n#if IS_ENABLED(CONFIG_IPV6)\n\tint off;\n#endif\n\n\t \n\tBUILD_BUG_ON(offsetof(struct sk_msg, sg) != 0);\n\n\tswitch (si->off) {\n\tcase offsetof(struct sk_msg_md, data):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_msg, data),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_msg, data));\n\t\tbreak;\n\tcase offsetof(struct sk_msg_md, data_end):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_msg, data_end),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_msg, data_end));\n\t\tbreak;\n\tcase offsetof(struct sk_msg_md, family):\n\t\tBUILD_BUG_ON(sizeof_field(struct sock_common, skc_family) != 2);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\n\t\t\t\t\t      struct sk_msg, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_msg, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct sock_common, skc_family));\n\t\tbreak;\n\n\tcase offsetof(struct sk_msg_md, remote_ip4):\n\t\tBUILD_BUG_ON(sizeof_field(struct sock_common, skc_daddr) != 4);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\n\t\t\t\t\t\tstruct sk_msg, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_msg, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct sock_common, skc_daddr));\n\t\tbreak;\n\n\tcase offsetof(struct sk_msg_md, local_ip4):\n\t\tBUILD_BUG_ON(sizeof_field(struct sock_common,\n\t\t\t\t\t  skc_rcv_saddr) != 4);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\n\t\t\t\t\t      struct sk_msg, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_msg, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct sock_common,\n\t\t\t\t\t       skc_rcv_saddr));\n\t\tbreak;\n\n\tcase offsetof(struct sk_msg_md, remote_ip6[0]) ...\n\t     offsetof(struct sk_msg_md, remote_ip6[3]):\n#if IS_ENABLED(CONFIG_IPV6)\n\t\tBUILD_BUG_ON(sizeof_field(struct sock_common,\n\t\t\t\t\t  skc_v6_daddr.s6_addr32[0]) != 4);\n\n\t\toff = si->off;\n\t\toff -= offsetof(struct sk_msg_md, remote_ip6[0]);\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\n\t\t\t\t\t\tstruct sk_msg, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_msg, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct sock_common,\n\t\t\t\t\t       skc_v6_daddr.s6_addr32[0]) +\n\t\t\t\t      off);\n#else\n\t\t*insn++ = BPF_MOV32_IMM(si->dst_reg, 0);\n#endif\n\t\tbreak;\n\n\tcase offsetof(struct sk_msg_md, local_ip6[0]) ...\n\t     offsetof(struct sk_msg_md, local_ip6[3]):\n#if IS_ENABLED(CONFIG_IPV6)\n\t\tBUILD_BUG_ON(sizeof_field(struct sock_common,\n\t\t\t\t\t  skc_v6_rcv_saddr.s6_addr32[0]) != 4);\n\n\t\toff = si->off;\n\t\toff -= offsetof(struct sk_msg_md, local_ip6[0]);\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\n\t\t\t\t\t\tstruct sk_msg, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_msg, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct sock_common,\n\t\t\t\t\t       skc_v6_rcv_saddr.s6_addr32[0]) +\n\t\t\t\t      off);\n#else\n\t\t*insn++ = BPF_MOV32_IMM(si->dst_reg, 0);\n#endif\n\t\tbreak;\n\n\tcase offsetof(struct sk_msg_md, remote_port):\n\t\tBUILD_BUG_ON(sizeof_field(struct sock_common, skc_dport) != 2);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\n\t\t\t\t\t\tstruct sk_msg, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_msg, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct sock_common, skc_dport));\n#ifndef __BIG_ENDIAN_BITFIELD\n\t\t*insn++ = BPF_ALU32_IMM(BPF_LSH, si->dst_reg, 16);\n#endif\n\t\tbreak;\n\n\tcase offsetof(struct sk_msg_md, local_port):\n\t\tBUILD_BUG_ON(sizeof_field(struct sock_common, skc_num) != 2);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\n\t\t\t\t\t\tstruct sk_msg, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_msg, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct sock_common, skc_num));\n\t\tbreak;\n\n\tcase offsetof(struct sk_msg_md, size):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_msg_sg, size),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_msg_sg, size));\n\t\tbreak;\n\n\tcase offsetof(struct sk_msg_md, sk):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_msg, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_msg, sk));\n\t\tbreak;\n\t}\n\n\treturn insn - insn_buf;\n}\n\nconst struct bpf_verifier_ops sk_filter_verifier_ops = {\n\t.get_func_proto\t\t= sk_filter_func_proto,\n\t.is_valid_access\t= sk_filter_is_valid_access,\n\t.convert_ctx_access\t= bpf_convert_ctx_access,\n\t.gen_ld_abs\t\t= bpf_gen_ld_abs,\n};\n\nconst struct bpf_prog_ops sk_filter_prog_ops = {\n\t.test_run\t\t= bpf_prog_test_run_skb,\n};\n\nconst struct bpf_verifier_ops tc_cls_act_verifier_ops = {\n\t.get_func_proto\t\t= tc_cls_act_func_proto,\n\t.is_valid_access\t= tc_cls_act_is_valid_access,\n\t.convert_ctx_access\t= tc_cls_act_convert_ctx_access,\n\t.gen_prologue\t\t= tc_cls_act_prologue,\n\t.gen_ld_abs\t\t= bpf_gen_ld_abs,\n\t.btf_struct_access\t= tc_cls_act_btf_struct_access,\n};\n\nconst struct bpf_prog_ops tc_cls_act_prog_ops = {\n\t.test_run\t\t= bpf_prog_test_run_skb,\n};\n\nconst struct bpf_verifier_ops xdp_verifier_ops = {\n\t.get_func_proto\t\t= xdp_func_proto,\n\t.is_valid_access\t= xdp_is_valid_access,\n\t.convert_ctx_access\t= xdp_convert_ctx_access,\n\t.gen_prologue\t\t= bpf_noop_prologue,\n\t.btf_struct_access\t= xdp_btf_struct_access,\n};\n\nconst struct bpf_prog_ops xdp_prog_ops = {\n\t.test_run\t\t= bpf_prog_test_run_xdp,\n};\n\nconst struct bpf_verifier_ops cg_skb_verifier_ops = {\n\t.get_func_proto\t\t= cg_skb_func_proto,\n\t.is_valid_access\t= cg_skb_is_valid_access,\n\t.convert_ctx_access\t= bpf_convert_ctx_access,\n};\n\nconst struct bpf_prog_ops cg_skb_prog_ops = {\n\t.test_run\t\t= bpf_prog_test_run_skb,\n};\n\nconst struct bpf_verifier_ops lwt_in_verifier_ops = {\n\t.get_func_proto\t\t= lwt_in_func_proto,\n\t.is_valid_access\t= lwt_is_valid_access,\n\t.convert_ctx_access\t= bpf_convert_ctx_access,\n};\n\nconst struct bpf_prog_ops lwt_in_prog_ops = {\n\t.test_run\t\t= bpf_prog_test_run_skb,\n};\n\nconst struct bpf_verifier_ops lwt_out_verifier_ops = {\n\t.get_func_proto\t\t= lwt_out_func_proto,\n\t.is_valid_access\t= lwt_is_valid_access,\n\t.convert_ctx_access\t= bpf_convert_ctx_access,\n};\n\nconst struct bpf_prog_ops lwt_out_prog_ops = {\n\t.test_run\t\t= bpf_prog_test_run_skb,\n};\n\nconst struct bpf_verifier_ops lwt_xmit_verifier_ops = {\n\t.get_func_proto\t\t= lwt_xmit_func_proto,\n\t.is_valid_access\t= lwt_is_valid_access,\n\t.convert_ctx_access\t= bpf_convert_ctx_access,\n\t.gen_prologue\t\t= tc_cls_act_prologue,\n};\n\nconst struct bpf_prog_ops lwt_xmit_prog_ops = {\n\t.test_run\t\t= bpf_prog_test_run_skb,\n};\n\nconst struct bpf_verifier_ops lwt_seg6local_verifier_ops = {\n\t.get_func_proto\t\t= lwt_seg6local_func_proto,\n\t.is_valid_access\t= lwt_is_valid_access,\n\t.convert_ctx_access\t= bpf_convert_ctx_access,\n};\n\nconst struct bpf_prog_ops lwt_seg6local_prog_ops = {\n\t.test_run\t\t= bpf_prog_test_run_skb,\n};\n\nconst struct bpf_verifier_ops cg_sock_verifier_ops = {\n\t.get_func_proto\t\t= sock_filter_func_proto,\n\t.is_valid_access\t= sock_filter_is_valid_access,\n\t.convert_ctx_access\t= bpf_sock_convert_ctx_access,\n};\n\nconst struct bpf_prog_ops cg_sock_prog_ops = {\n};\n\nconst struct bpf_verifier_ops cg_sock_addr_verifier_ops = {\n\t.get_func_proto\t\t= sock_addr_func_proto,\n\t.is_valid_access\t= sock_addr_is_valid_access,\n\t.convert_ctx_access\t= sock_addr_convert_ctx_access,\n};\n\nconst struct bpf_prog_ops cg_sock_addr_prog_ops = {\n};\n\nconst struct bpf_verifier_ops sock_ops_verifier_ops = {\n\t.get_func_proto\t\t= sock_ops_func_proto,\n\t.is_valid_access\t= sock_ops_is_valid_access,\n\t.convert_ctx_access\t= sock_ops_convert_ctx_access,\n};\n\nconst struct bpf_prog_ops sock_ops_prog_ops = {\n};\n\nconst struct bpf_verifier_ops sk_skb_verifier_ops = {\n\t.get_func_proto\t\t= sk_skb_func_proto,\n\t.is_valid_access\t= sk_skb_is_valid_access,\n\t.convert_ctx_access\t= sk_skb_convert_ctx_access,\n\t.gen_prologue\t\t= sk_skb_prologue,\n};\n\nconst struct bpf_prog_ops sk_skb_prog_ops = {\n};\n\nconst struct bpf_verifier_ops sk_msg_verifier_ops = {\n\t.get_func_proto\t\t= sk_msg_func_proto,\n\t.is_valid_access\t= sk_msg_is_valid_access,\n\t.convert_ctx_access\t= sk_msg_convert_ctx_access,\n\t.gen_prologue\t\t= bpf_noop_prologue,\n};\n\nconst struct bpf_prog_ops sk_msg_prog_ops = {\n};\n\nconst struct bpf_verifier_ops flow_dissector_verifier_ops = {\n\t.get_func_proto\t\t= flow_dissector_func_proto,\n\t.is_valid_access\t= flow_dissector_is_valid_access,\n\t.convert_ctx_access\t= flow_dissector_convert_ctx_access,\n};\n\nconst struct bpf_prog_ops flow_dissector_prog_ops = {\n\t.test_run\t\t= bpf_prog_test_run_flow_dissector,\n};\n\nint sk_detach_filter(struct sock *sk)\n{\n\tint ret = -ENOENT;\n\tstruct sk_filter *filter;\n\n\tif (sock_flag(sk, SOCK_FILTER_LOCKED))\n\t\treturn -EPERM;\n\n\tfilter = rcu_dereference_protected(sk->sk_filter,\n\t\t\t\t\t   lockdep_sock_is_held(sk));\n\tif (filter) {\n\t\tRCU_INIT_POINTER(sk->sk_filter, NULL);\n\t\tsk_filter_uncharge(sk, filter);\n\t\tret = 0;\n\t}\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(sk_detach_filter);\n\nint sk_get_filter(struct sock *sk, sockptr_t optval, unsigned int len)\n{\n\tstruct sock_fprog_kern *fprog;\n\tstruct sk_filter *filter;\n\tint ret = 0;\n\n\tsockopt_lock_sock(sk);\n\tfilter = rcu_dereference_protected(sk->sk_filter,\n\t\t\t\t\t   lockdep_sock_is_held(sk));\n\tif (!filter)\n\t\tgoto out;\n\n\t \n\tret = -EACCES;\n\tfprog = filter->prog->orig_prog;\n\tif (!fprog)\n\t\tgoto out;\n\n\tret = fprog->len;\n\tif (!len)\n\t\t \n\t\tgoto out;\n\n\tret = -EINVAL;\n\tif (len < fprog->len)\n\t\tgoto out;\n\n\tret = -EFAULT;\n\tif (copy_to_sockptr(optval, fprog->filter, bpf_classic_proglen(fprog)))\n\t\tgoto out;\n\n\t \n\tret = fprog->len;\nout:\n\tsockopt_release_sock(sk);\n\treturn ret;\n}\n\n#ifdef CONFIG_INET\nstatic void bpf_init_reuseport_kern(struct sk_reuseport_kern *reuse_kern,\n\t\t\t\t    struct sock_reuseport *reuse,\n\t\t\t\t    struct sock *sk, struct sk_buff *skb,\n\t\t\t\t    struct sock *migrating_sk,\n\t\t\t\t    u32 hash)\n{\n\treuse_kern->skb = skb;\n\treuse_kern->sk = sk;\n\treuse_kern->selected_sk = NULL;\n\treuse_kern->migrating_sk = migrating_sk;\n\treuse_kern->data_end = skb->data + skb_headlen(skb);\n\treuse_kern->hash = hash;\n\treuse_kern->reuseport_id = reuse->reuseport_id;\n\treuse_kern->bind_inany = reuse->bind_inany;\n}\n\nstruct sock *bpf_run_sk_reuseport(struct sock_reuseport *reuse, struct sock *sk,\n\t\t\t\t  struct bpf_prog *prog, struct sk_buff *skb,\n\t\t\t\t  struct sock *migrating_sk,\n\t\t\t\t  u32 hash)\n{\n\tstruct sk_reuseport_kern reuse_kern;\n\tenum sk_action action;\n\n\tbpf_init_reuseport_kern(&reuse_kern, reuse, sk, skb, migrating_sk, hash);\n\taction = bpf_prog_run(prog, &reuse_kern);\n\n\tif (action == SK_PASS)\n\t\treturn reuse_kern.selected_sk;\n\telse\n\t\treturn ERR_PTR(-ECONNREFUSED);\n}\n\nBPF_CALL_4(sk_select_reuseport, struct sk_reuseport_kern *, reuse_kern,\n\t   struct bpf_map *, map, void *, key, u32, flags)\n{\n\tbool is_sockarray = map->map_type == BPF_MAP_TYPE_REUSEPORT_SOCKARRAY;\n\tstruct sock_reuseport *reuse;\n\tstruct sock *selected_sk;\n\n\tselected_sk = map->ops->map_lookup_elem(map, key);\n\tif (!selected_sk)\n\t\treturn -ENOENT;\n\n\treuse = rcu_dereference(selected_sk->sk_reuseport_cb);\n\tif (!reuse) {\n\t\t \n\t\tif (sk_is_refcounted(selected_sk))\n\t\t\tsock_put(selected_sk);\n\n\t\t \n\t\treturn is_sockarray ? -ENOENT : -EINVAL;\n\t}\n\n\tif (unlikely(reuse->reuseport_id != reuse_kern->reuseport_id)) {\n\t\tstruct sock *sk = reuse_kern->sk;\n\n\t\tif (sk->sk_protocol != selected_sk->sk_protocol)\n\t\t\treturn -EPROTOTYPE;\n\t\telse if (sk->sk_family != selected_sk->sk_family)\n\t\t\treturn -EAFNOSUPPORT;\n\n\t\t \n\t\treturn -EBADFD;\n\t}\n\n\treuse_kern->selected_sk = selected_sk;\n\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto sk_select_reuseport_proto = {\n\t.func           = sk_select_reuseport,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type      = ARG_CONST_MAP_PTR,\n\t.arg3_type      = ARG_PTR_TO_MAP_KEY,\n\t.arg4_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_4(sk_reuseport_load_bytes,\n\t   const struct sk_reuseport_kern *, reuse_kern, u32, offset,\n\t   void *, to, u32, len)\n{\n\treturn ____bpf_skb_load_bytes(reuse_kern->skb, offset, to, len);\n}\n\nstatic const struct bpf_func_proto sk_reuseport_load_bytes_proto = {\n\t.func\t\t= sk_reuseport_load_bytes,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg4_type\t= ARG_CONST_SIZE,\n};\n\nBPF_CALL_5(sk_reuseport_load_bytes_relative,\n\t   const struct sk_reuseport_kern *, reuse_kern, u32, offset,\n\t   void *, to, u32, len, u32, start_header)\n{\n\treturn ____bpf_skb_load_bytes_relative(reuse_kern->skb, offset, to,\n\t\t\t\t\t       len, start_header);\n}\n\nstatic const struct bpf_func_proto sk_reuseport_load_bytes_relative_proto = {\n\t.func\t\t= sk_reuseport_load_bytes_relative,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg4_type\t= ARG_CONST_SIZE,\n\t.arg5_type\t= ARG_ANYTHING,\n};\n\nstatic const struct bpf_func_proto *\nsk_reuseport_func_proto(enum bpf_func_id func_id,\n\t\t\tconst struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n\tcase BPF_FUNC_sk_select_reuseport:\n\t\treturn &sk_select_reuseport_proto;\n\tcase BPF_FUNC_skb_load_bytes:\n\t\treturn &sk_reuseport_load_bytes_proto;\n\tcase BPF_FUNC_skb_load_bytes_relative:\n\t\treturn &sk_reuseport_load_bytes_relative_proto;\n\tcase BPF_FUNC_get_socket_cookie:\n\t\treturn &bpf_get_socket_ptr_cookie_proto;\n\tcase BPF_FUNC_ktime_get_coarse_ns:\n\t\treturn &bpf_ktime_get_coarse_ns_proto;\n\tdefault:\n\t\treturn bpf_base_func_proto(func_id);\n\t}\n}\n\nstatic bool\nsk_reuseport_is_valid_access(int off, int size,\n\t\t\t     enum bpf_access_type type,\n\t\t\t     const struct bpf_prog *prog,\n\t\t\t     struct bpf_insn_access_aux *info)\n{\n\tconst u32 size_default = sizeof(__u32);\n\n\tif (off < 0 || off >= sizeof(struct sk_reuseport_md) ||\n\t    off % size || type != BPF_READ)\n\t\treturn false;\n\n\tswitch (off) {\n\tcase offsetof(struct sk_reuseport_md, data):\n\t\tinfo->reg_type = PTR_TO_PACKET;\n\t\treturn size == sizeof(__u64);\n\n\tcase offsetof(struct sk_reuseport_md, data_end):\n\t\tinfo->reg_type = PTR_TO_PACKET_END;\n\t\treturn size == sizeof(__u64);\n\n\tcase offsetof(struct sk_reuseport_md, hash):\n\t\treturn size == size_default;\n\n\tcase offsetof(struct sk_reuseport_md, sk):\n\t\tinfo->reg_type = PTR_TO_SOCKET;\n\t\treturn size == sizeof(__u64);\n\n\tcase offsetof(struct sk_reuseport_md, migrating_sk):\n\t\tinfo->reg_type = PTR_TO_SOCK_COMMON_OR_NULL;\n\t\treturn size == sizeof(__u64);\n\n\t \n\tcase bpf_ctx_range(struct sk_reuseport_md, eth_protocol):\n\t\tif (size < sizeof_field(struct sk_buff, protocol))\n\t\t\treturn false;\n\t\tfallthrough;\n\tcase bpf_ctx_range(struct sk_reuseport_md, ip_protocol):\n\tcase bpf_ctx_range(struct sk_reuseport_md, bind_inany):\n\tcase bpf_ctx_range(struct sk_reuseport_md, len):\n\t\tbpf_ctx_record_field_size(info, size_default);\n\t\treturn bpf_ctx_narrow_access_ok(off, size, size_default);\n\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n#define SK_REUSEPORT_LOAD_FIELD(F) ({\t\t\t\t\t\\\n\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_reuseport_kern, F), \\\n\t\t\t      si->dst_reg, si->src_reg,\t\t\t\\\n\t\t\t      bpf_target_off(struct sk_reuseport_kern, F, \\\n\t\t\t\t\t     sizeof_field(struct sk_reuseport_kern, F), \\\n\t\t\t\t\t     target_size));\t\t\\\n\t})\n\n#define SK_REUSEPORT_LOAD_SKB_FIELD(SKB_FIELD)\t\t\t\t\\\n\tSOCK_ADDR_LOAD_NESTED_FIELD(struct sk_reuseport_kern,\t\t\\\n\t\t\t\t    struct sk_buff,\t\t\t\\\n\t\t\t\t    skb,\t\t\t\t\\\n\t\t\t\t    SKB_FIELD)\n\n#define SK_REUSEPORT_LOAD_SK_FIELD(SK_FIELD)\t\t\t\t\\\n\tSOCK_ADDR_LOAD_NESTED_FIELD(struct sk_reuseport_kern,\t\t\\\n\t\t\t\t    struct sock,\t\t\t\\\n\t\t\t\t    sk,\t\t\t\t\t\\\n\t\t\t\t    SK_FIELD)\n\nstatic u32 sk_reuseport_convert_ctx_access(enum bpf_access_type type,\n\t\t\t\t\t   const struct bpf_insn *si,\n\t\t\t\t\t   struct bpf_insn *insn_buf,\n\t\t\t\t\t   struct bpf_prog *prog,\n\t\t\t\t\t   u32 *target_size)\n{\n\tstruct bpf_insn *insn = insn_buf;\n\n\tswitch (si->off) {\n\tcase offsetof(struct sk_reuseport_md, data):\n\t\tSK_REUSEPORT_LOAD_SKB_FIELD(data);\n\t\tbreak;\n\n\tcase offsetof(struct sk_reuseport_md, len):\n\t\tSK_REUSEPORT_LOAD_SKB_FIELD(len);\n\t\tbreak;\n\n\tcase offsetof(struct sk_reuseport_md, eth_protocol):\n\t\tSK_REUSEPORT_LOAD_SKB_FIELD(protocol);\n\t\tbreak;\n\n\tcase offsetof(struct sk_reuseport_md, ip_protocol):\n\t\tSK_REUSEPORT_LOAD_SK_FIELD(sk_protocol);\n\t\tbreak;\n\n\tcase offsetof(struct sk_reuseport_md, data_end):\n\t\tSK_REUSEPORT_LOAD_FIELD(data_end);\n\t\tbreak;\n\n\tcase offsetof(struct sk_reuseport_md, hash):\n\t\tSK_REUSEPORT_LOAD_FIELD(hash);\n\t\tbreak;\n\n\tcase offsetof(struct sk_reuseport_md, bind_inany):\n\t\tSK_REUSEPORT_LOAD_FIELD(bind_inany);\n\t\tbreak;\n\n\tcase offsetof(struct sk_reuseport_md, sk):\n\t\tSK_REUSEPORT_LOAD_FIELD(sk);\n\t\tbreak;\n\n\tcase offsetof(struct sk_reuseport_md, migrating_sk):\n\t\tSK_REUSEPORT_LOAD_FIELD(migrating_sk);\n\t\tbreak;\n\t}\n\n\treturn insn - insn_buf;\n}\n\nconst struct bpf_verifier_ops sk_reuseport_verifier_ops = {\n\t.get_func_proto\t\t= sk_reuseport_func_proto,\n\t.is_valid_access\t= sk_reuseport_is_valid_access,\n\t.convert_ctx_access\t= sk_reuseport_convert_ctx_access,\n};\n\nconst struct bpf_prog_ops sk_reuseport_prog_ops = {\n};\n\nDEFINE_STATIC_KEY_FALSE(bpf_sk_lookup_enabled);\nEXPORT_SYMBOL(bpf_sk_lookup_enabled);\n\nBPF_CALL_3(bpf_sk_lookup_assign, struct bpf_sk_lookup_kern *, ctx,\n\t   struct sock *, sk, u64, flags)\n{\n\tif (unlikely(flags & ~(BPF_SK_LOOKUP_F_REPLACE |\n\t\t\t       BPF_SK_LOOKUP_F_NO_REUSEPORT)))\n\t\treturn -EINVAL;\n\tif (unlikely(sk && sk_is_refcounted(sk)))\n\t\treturn -ESOCKTNOSUPPORT;  \n\tif (unlikely(sk && sk_is_tcp(sk) && sk->sk_state != TCP_LISTEN))\n\t\treturn -ESOCKTNOSUPPORT;  \n\tif (unlikely(sk && sk_is_udp(sk) && sk->sk_state != TCP_CLOSE))\n\t\treturn -ESOCKTNOSUPPORT;  \n\n\t \n\tif (sk && sk->sk_protocol != ctx->protocol)\n\t\treturn -EPROTOTYPE;\n\tif (sk && sk->sk_family != ctx->family &&\n\t    (sk->sk_family == AF_INET || ipv6_only_sock(sk)))\n\t\treturn -EAFNOSUPPORT;\n\n\tif (ctx->selected_sk && !(flags & BPF_SK_LOOKUP_F_REPLACE))\n\t\treturn -EEXIST;\n\n\t \n\tctx->selected_sk = sk;\n\tctx->no_reuseport = flags & BPF_SK_LOOKUP_F_NO_REUSEPORT;\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_sk_lookup_assign_proto = {\n\t.func\t\t= bpf_sk_lookup_assign,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_PTR_TO_SOCKET_OR_NULL,\n\t.arg3_type\t= ARG_ANYTHING,\n};\n\nstatic const struct bpf_func_proto *\nsk_lookup_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n\tcase BPF_FUNC_perf_event_output:\n\t\treturn &bpf_event_output_data_proto;\n\tcase BPF_FUNC_sk_assign:\n\t\treturn &bpf_sk_lookup_assign_proto;\n\tcase BPF_FUNC_sk_release:\n\t\treturn &bpf_sk_release_proto;\n\tdefault:\n\t\treturn bpf_sk_base_func_proto(func_id);\n\t}\n}\n\nstatic bool sk_lookup_is_valid_access(int off, int size,\n\t\t\t\t      enum bpf_access_type type,\n\t\t\t\t      const struct bpf_prog *prog,\n\t\t\t\t      struct bpf_insn_access_aux *info)\n{\n\tif (off < 0 || off >= sizeof(struct bpf_sk_lookup))\n\t\treturn false;\n\tif (off % size != 0)\n\t\treturn false;\n\tif (type != BPF_READ)\n\t\treturn false;\n\n\tswitch (off) {\n\tcase offsetof(struct bpf_sk_lookup, sk):\n\t\tinfo->reg_type = PTR_TO_SOCKET_OR_NULL;\n\t\treturn size == sizeof(__u64);\n\n\tcase bpf_ctx_range(struct bpf_sk_lookup, family):\n\tcase bpf_ctx_range(struct bpf_sk_lookup, protocol):\n\tcase bpf_ctx_range(struct bpf_sk_lookup, remote_ip4):\n\tcase bpf_ctx_range(struct bpf_sk_lookup, local_ip4):\n\tcase bpf_ctx_range_till(struct bpf_sk_lookup, remote_ip6[0], remote_ip6[3]):\n\tcase bpf_ctx_range_till(struct bpf_sk_lookup, local_ip6[0], local_ip6[3]):\n\tcase bpf_ctx_range(struct bpf_sk_lookup, local_port):\n\tcase bpf_ctx_range(struct bpf_sk_lookup, ingress_ifindex):\n\t\tbpf_ctx_record_field_size(info, sizeof(__u32));\n\t\treturn bpf_ctx_narrow_access_ok(off, size, sizeof(__u32));\n\n\tcase bpf_ctx_range(struct bpf_sk_lookup, remote_port):\n\t\t \n\t\tif (size == sizeof(__u32))\n\t\t\treturn true;\n\t\tbpf_ctx_record_field_size(info, sizeof(__be16));\n\t\treturn bpf_ctx_narrow_access_ok(off, size, sizeof(__be16));\n\n\tcase offsetofend(struct bpf_sk_lookup, remote_port) ...\n\t     offsetof(struct bpf_sk_lookup, local_ip4) - 1:\n\t\t \n\t\tbpf_ctx_record_field_size(info, sizeof(__u16));\n\t\treturn bpf_ctx_narrow_access_ok(off, size, sizeof(__u16));\n\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic u32 sk_lookup_convert_ctx_access(enum bpf_access_type type,\n\t\t\t\t\tconst struct bpf_insn *si,\n\t\t\t\t\tstruct bpf_insn *insn_buf,\n\t\t\t\t\tstruct bpf_prog *prog,\n\t\t\t\t\tu32 *target_size)\n{\n\tstruct bpf_insn *insn = insn_buf;\n\n\tswitch (si->off) {\n\tcase offsetof(struct bpf_sk_lookup, sk):\n\t\t*insn++ = BPF_LDX_MEM(BPF_SIZEOF(void *), si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_sk_lookup_kern, selected_sk));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sk_lookup, family):\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->src_reg,\n\t\t\t\t      bpf_target_off(struct bpf_sk_lookup_kern,\n\t\t\t\t\t\t     family, 2, target_size));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sk_lookup, protocol):\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->src_reg,\n\t\t\t\t      bpf_target_off(struct bpf_sk_lookup_kern,\n\t\t\t\t\t\t     protocol, 2, target_size));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sk_lookup, remote_ip4):\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t      bpf_target_off(struct bpf_sk_lookup_kern,\n\t\t\t\t\t\t     v4.saddr, 4, target_size));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sk_lookup, local_ip4):\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t      bpf_target_off(struct bpf_sk_lookup_kern,\n\t\t\t\t\t\t     v4.daddr, 4, target_size));\n\t\tbreak;\n\n\tcase bpf_ctx_range_till(struct bpf_sk_lookup,\n\t\t\t\tremote_ip6[0], remote_ip6[3]): {\n#if IS_ENABLED(CONFIG_IPV6)\n\t\tint off = si->off;\n\n\t\toff -= offsetof(struct bpf_sk_lookup, remote_ip6[0]);\n\t\toff += bpf_target_off(struct in6_addr, s6_addr32[0], 4, target_size);\n\t\t*insn++ = BPF_LDX_MEM(BPF_SIZEOF(void *), si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_sk_lookup_kern, v6.saddr));\n\t\t*insn++ = BPF_JMP_IMM(BPF_JEQ, si->dst_reg, 0, 1);\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg, off);\n#else\n\t\t*insn++ = BPF_MOV32_IMM(si->dst_reg, 0);\n#endif\n\t\tbreak;\n\t}\n\tcase bpf_ctx_range_till(struct bpf_sk_lookup,\n\t\t\t\tlocal_ip6[0], local_ip6[3]): {\n#if IS_ENABLED(CONFIG_IPV6)\n\t\tint off = si->off;\n\n\t\toff -= offsetof(struct bpf_sk_lookup, local_ip6[0]);\n\t\toff += bpf_target_off(struct in6_addr, s6_addr32[0], 4, target_size);\n\t\t*insn++ = BPF_LDX_MEM(BPF_SIZEOF(void *), si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_sk_lookup_kern, v6.daddr));\n\t\t*insn++ = BPF_JMP_IMM(BPF_JEQ, si->dst_reg, 0, 1);\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg, off);\n#else\n\t\t*insn++ = BPF_MOV32_IMM(si->dst_reg, 0);\n#endif\n\t\tbreak;\n\t}\n\tcase offsetof(struct bpf_sk_lookup, remote_port):\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->src_reg,\n\t\t\t\t      bpf_target_off(struct bpf_sk_lookup_kern,\n\t\t\t\t\t\t     sport, 2, target_size));\n\t\tbreak;\n\n\tcase offsetofend(struct bpf_sk_lookup, remote_port):\n\t\t*target_size = 2;\n\t\t*insn++ = BPF_MOV32_IMM(si->dst_reg, 0);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sk_lookup, local_port):\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->src_reg,\n\t\t\t\t      bpf_target_off(struct bpf_sk_lookup_kern,\n\t\t\t\t\t\t     dport, 2, target_size));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sk_lookup, ingress_ifindex):\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t      bpf_target_off(struct bpf_sk_lookup_kern,\n\t\t\t\t\t\t     ingress_ifindex, 4, target_size));\n\t\tbreak;\n\t}\n\n\treturn insn - insn_buf;\n}\n\nconst struct bpf_prog_ops sk_lookup_prog_ops = {\n\t.test_run = bpf_prog_test_run_sk_lookup,\n};\n\nconst struct bpf_verifier_ops sk_lookup_verifier_ops = {\n\t.get_func_proto\t\t= sk_lookup_func_proto,\n\t.is_valid_access\t= sk_lookup_is_valid_access,\n\t.convert_ctx_access\t= sk_lookup_convert_ctx_access,\n};\n\n#endif  \n\nDEFINE_BPF_DISPATCHER(xdp)\n\nvoid bpf_prog_change_xdp(struct bpf_prog *prev_prog, struct bpf_prog *prog)\n{\n\tbpf_dispatcher_change_prog(BPF_DISPATCHER_PTR(xdp), prev_prog, prog);\n}\n\nBTF_ID_LIST_GLOBAL(btf_sock_ids, MAX_BTF_SOCK_TYPE)\n#define BTF_SOCK_TYPE(name, type) BTF_ID(struct, type)\nBTF_SOCK_TYPE_xxx\n#undef BTF_SOCK_TYPE\n\nBPF_CALL_1(bpf_skc_to_tcp6_sock, struct sock *, sk)\n{\n\t \n\tBTF_TYPE_EMIT(struct tcp6_sock);\n\tif (sk && sk_fullsock(sk) && sk->sk_protocol == IPPROTO_TCP &&\n\t    sk->sk_family == AF_INET6)\n\t\treturn (unsigned long)sk;\n\n\treturn (unsigned long)NULL;\n}\n\nconst struct bpf_func_proto bpf_skc_to_tcp6_sock_proto = {\n\t.func\t\t\t= bpf_skc_to_tcp6_sock,\n\t.gpl_only\t\t= false,\n\t.ret_type\t\t= RET_PTR_TO_BTF_ID_OR_NULL,\n\t.arg1_type\t\t= ARG_PTR_TO_BTF_ID_SOCK_COMMON,\n\t.ret_btf_id\t\t= &btf_sock_ids[BTF_SOCK_TYPE_TCP6],\n};\n\nBPF_CALL_1(bpf_skc_to_tcp_sock, struct sock *, sk)\n{\n\tif (sk && sk_fullsock(sk) && sk->sk_protocol == IPPROTO_TCP)\n\t\treturn (unsigned long)sk;\n\n\treturn (unsigned long)NULL;\n}\n\nconst struct bpf_func_proto bpf_skc_to_tcp_sock_proto = {\n\t.func\t\t\t= bpf_skc_to_tcp_sock,\n\t.gpl_only\t\t= false,\n\t.ret_type\t\t= RET_PTR_TO_BTF_ID_OR_NULL,\n\t.arg1_type\t\t= ARG_PTR_TO_BTF_ID_SOCK_COMMON,\n\t.ret_btf_id\t\t= &btf_sock_ids[BTF_SOCK_TYPE_TCP],\n};\n\nBPF_CALL_1(bpf_skc_to_tcp_timewait_sock, struct sock *, sk)\n{\n\t \n\tBTF_TYPE_EMIT(struct inet_timewait_sock);\n\tBTF_TYPE_EMIT(struct tcp_timewait_sock);\n\n#ifdef CONFIG_INET\n\tif (sk && sk->sk_prot == &tcp_prot && sk->sk_state == TCP_TIME_WAIT)\n\t\treturn (unsigned long)sk;\n#endif\n\n#if IS_BUILTIN(CONFIG_IPV6)\n\tif (sk && sk->sk_prot == &tcpv6_prot && sk->sk_state == TCP_TIME_WAIT)\n\t\treturn (unsigned long)sk;\n#endif\n\n\treturn (unsigned long)NULL;\n}\n\nconst struct bpf_func_proto bpf_skc_to_tcp_timewait_sock_proto = {\n\t.func\t\t\t= bpf_skc_to_tcp_timewait_sock,\n\t.gpl_only\t\t= false,\n\t.ret_type\t\t= RET_PTR_TO_BTF_ID_OR_NULL,\n\t.arg1_type\t\t= ARG_PTR_TO_BTF_ID_SOCK_COMMON,\n\t.ret_btf_id\t\t= &btf_sock_ids[BTF_SOCK_TYPE_TCP_TW],\n};\n\nBPF_CALL_1(bpf_skc_to_tcp_request_sock, struct sock *, sk)\n{\n#ifdef CONFIG_INET\n\tif (sk && sk->sk_prot == &tcp_prot && sk->sk_state == TCP_NEW_SYN_RECV)\n\t\treturn (unsigned long)sk;\n#endif\n\n#if IS_BUILTIN(CONFIG_IPV6)\n\tif (sk && sk->sk_prot == &tcpv6_prot && sk->sk_state == TCP_NEW_SYN_RECV)\n\t\treturn (unsigned long)sk;\n#endif\n\n\treturn (unsigned long)NULL;\n}\n\nconst struct bpf_func_proto bpf_skc_to_tcp_request_sock_proto = {\n\t.func\t\t\t= bpf_skc_to_tcp_request_sock,\n\t.gpl_only\t\t= false,\n\t.ret_type\t\t= RET_PTR_TO_BTF_ID_OR_NULL,\n\t.arg1_type\t\t= ARG_PTR_TO_BTF_ID_SOCK_COMMON,\n\t.ret_btf_id\t\t= &btf_sock_ids[BTF_SOCK_TYPE_TCP_REQ],\n};\n\nBPF_CALL_1(bpf_skc_to_udp6_sock, struct sock *, sk)\n{\n\t \n\tBTF_TYPE_EMIT(struct udp6_sock);\n\tif (sk && sk_fullsock(sk) && sk->sk_protocol == IPPROTO_UDP &&\n\t    sk->sk_type == SOCK_DGRAM && sk->sk_family == AF_INET6)\n\t\treturn (unsigned long)sk;\n\n\treturn (unsigned long)NULL;\n}\n\nconst struct bpf_func_proto bpf_skc_to_udp6_sock_proto = {\n\t.func\t\t\t= bpf_skc_to_udp6_sock,\n\t.gpl_only\t\t= false,\n\t.ret_type\t\t= RET_PTR_TO_BTF_ID_OR_NULL,\n\t.arg1_type\t\t= ARG_PTR_TO_BTF_ID_SOCK_COMMON,\n\t.ret_btf_id\t\t= &btf_sock_ids[BTF_SOCK_TYPE_UDP6],\n};\n\nBPF_CALL_1(bpf_skc_to_unix_sock, struct sock *, sk)\n{\n\t \n\tBTF_TYPE_EMIT(struct unix_sock);\n\tif (sk && sk_fullsock(sk) && sk->sk_family == AF_UNIX)\n\t\treturn (unsigned long)sk;\n\n\treturn (unsigned long)NULL;\n}\n\nconst struct bpf_func_proto bpf_skc_to_unix_sock_proto = {\n\t.func\t\t\t= bpf_skc_to_unix_sock,\n\t.gpl_only\t\t= false,\n\t.ret_type\t\t= RET_PTR_TO_BTF_ID_OR_NULL,\n\t.arg1_type\t\t= ARG_PTR_TO_BTF_ID_SOCK_COMMON,\n\t.ret_btf_id\t\t= &btf_sock_ids[BTF_SOCK_TYPE_UNIX],\n};\n\nBPF_CALL_1(bpf_skc_to_mptcp_sock, struct sock *, sk)\n{\n\tBTF_TYPE_EMIT(struct mptcp_sock);\n\treturn (unsigned long)bpf_mptcp_sock_from_subflow(sk);\n}\n\nconst struct bpf_func_proto bpf_skc_to_mptcp_sock_proto = {\n\t.func\t\t= bpf_skc_to_mptcp_sock,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_PTR_TO_BTF_ID_OR_NULL,\n\t.arg1_type\t= ARG_PTR_TO_SOCK_COMMON,\n\t.ret_btf_id\t= &btf_sock_ids[BTF_SOCK_TYPE_MPTCP],\n};\n\nBPF_CALL_1(bpf_sock_from_file, struct file *, file)\n{\n\treturn (unsigned long)sock_from_file(file);\n}\n\nBTF_ID_LIST(bpf_sock_from_file_btf_ids)\nBTF_ID(struct, socket)\nBTF_ID(struct, file)\n\nconst struct bpf_func_proto bpf_sock_from_file_proto = {\n\t.func\t\t= bpf_sock_from_file,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_PTR_TO_BTF_ID_OR_NULL,\n\t.ret_btf_id\t= &bpf_sock_from_file_btf_ids[0],\n\t.arg1_type\t= ARG_PTR_TO_BTF_ID,\n\t.arg1_btf_id\t= &bpf_sock_from_file_btf_ids[1],\n};\n\nstatic const struct bpf_func_proto *\nbpf_sk_base_func_proto(enum bpf_func_id func_id)\n{\n\tconst struct bpf_func_proto *func;\n\n\tswitch (func_id) {\n\tcase BPF_FUNC_skc_to_tcp6_sock:\n\t\tfunc = &bpf_skc_to_tcp6_sock_proto;\n\t\tbreak;\n\tcase BPF_FUNC_skc_to_tcp_sock:\n\t\tfunc = &bpf_skc_to_tcp_sock_proto;\n\t\tbreak;\n\tcase BPF_FUNC_skc_to_tcp_timewait_sock:\n\t\tfunc = &bpf_skc_to_tcp_timewait_sock_proto;\n\t\tbreak;\n\tcase BPF_FUNC_skc_to_tcp_request_sock:\n\t\tfunc = &bpf_skc_to_tcp_request_sock_proto;\n\t\tbreak;\n\tcase BPF_FUNC_skc_to_udp6_sock:\n\t\tfunc = &bpf_skc_to_udp6_sock_proto;\n\t\tbreak;\n\tcase BPF_FUNC_skc_to_unix_sock:\n\t\tfunc = &bpf_skc_to_unix_sock_proto;\n\t\tbreak;\n\tcase BPF_FUNC_skc_to_mptcp_sock:\n\t\tfunc = &bpf_skc_to_mptcp_sock_proto;\n\t\tbreak;\n\tcase BPF_FUNC_ktime_get_coarse_ns:\n\t\treturn &bpf_ktime_get_coarse_ns_proto;\n\tdefault:\n\t\treturn bpf_base_func_proto(func_id);\n\t}\n\n\tif (!perfmon_capable())\n\t\treturn NULL;\n\n\treturn func;\n}\n\n__diag_push();\n__diag_ignore_all(\"-Wmissing-prototypes\",\n\t\t  \"Global functions as their definitions will be in vmlinux BTF\");\n__bpf_kfunc int bpf_dynptr_from_skb(struct sk_buff *skb, u64 flags,\n\t\t\t\t    struct bpf_dynptr_kern *ptr__uninit)\n{\n\tif (flags) {\n\t\tbpf_dynptr_set_null(ptr__uninit);\n\t\treturn -EINVAL;\n\t}\n\n\tbpf_dynptr_init(ptr__uninit, skb, BPF_DYNPTR_TYPE_SKB, 0, skb->len);\n\n\treturn 0;\n}\n\n__bpf_kfunc int bpf_dynptr_from_xdp(struct xdp_buff *xdp, u64 flags,\n\t\t\t\t    struct bpf_dynptr_kern *ptr__uninit)\n{\n\tif (flags) {\n\t\tbpf_dynptr_set_null(ptr__uninit);\n\t\treturn -EINVAL;\n\t}\n\n\tbpf_dynptr_init(ptr__uninit, xdp, BPF_DYNPTR_TYPE_XDP, 0, xdp_get_buff_len(xdp));\n\n\treturn 0;\n}\n__diag_pop();\n\nint bpf_dynptr_from_skb_rdonly(struct sk_buff *skb, u64 flags,\n\t\t\t       struct bpf_dynptr_kern *ptr__uninit)\n{\n\tint err;\n\n\terr = bpf_dynptr_from_skb(skb, flags, ptr__uninit);\n\tif (err)\n\t\treturn err;\n\n\tbpf_dynptr_set_rdonly(ptr__uninit);\n\n\treturn 0;\n}\n\nBTF_SET8_START(bpf_kfunc_check_set_skb)\nBTF_ID_FLAGS(func, bpf_dynptr_from_skb)\nBTF_SET8_END(bpf_kfunc_check_set_skb)\n\nBTF_SET8_START(bpf_kfunc_check_set_xdp)\nBTF_ID_FLAGS(func, bpf_dynptr_from_xdp)\nBTF_SET8_END(bpf_kfunc_check_set_xdp)\n\nstatic const struct btf_kfunc_id_set bpf_kfunc_set_skb = {\n\t.owner = THIS_MODULE,\n\t.set = &bpf_kfunc_check_set_skb,\n};\n\nstatic const struct btf_kfunc_id_set bpf_kfunc_set_xdp = {\n\t.owner = THIS_MODULE,\n\t.set = &bpf_kfunc_check_set_xdp,\n};\n\nstatic int __init bpf_kfunc_init(void)\n{\n\tint ret;\n\n\tret = register_btf_kfunc_id_set(BPF_PROG_TYPE_SCHED_CLS, &bpf_kfunc_set_skb);\n\tret = ret ?: register_btf_kfunc_id_set(BPF_PROG_TYPE_SCHED_ACT, &bpf_kfunc_set_skb);\n\tret = ret ?: register_btf_kfunc_id_set(BPF_PROG_TYPE_SK_SKB, &bpf_kfunc_set_skb);\n\tret = ret ?: register_btf_kfunc_id_set(BPF_PROG_TYPE_SOCKET_FILTER, &bpf_kfunc_set_skb);\n\tret = ret ?: register_btf_kfunc_id_set(BPF_PROG_TYPE_CGROUP_SKB, &bpf_kfunc_set_skb);\n\tret = ret ?: register_btf_kfunc_id_set(BPF_PROG_TYPE_LWT_OUT, &bpf_kfunc_set_skb);\n\tret = ret ?: register_btf_kfunc_id_set(BPF_PROG_TYPE_LWT_IN, &bpf_kfunc_set_skb);\n\tret = ret ?: register_btf_kfunc_id_set(BPF_PROG_TYPE_LWT_XMIT, &bpf_kfunc_set_skb);\n\tret = ret ?: register_btf_kfunc_id_set(BPF_PROG_TYPE_LWT_SEG6LOCAL, &bpf_kfunc_set_skb);\n\tret = ret ?: register_btf_kfunc_id_set(BPF_PROG_TYPE_NETFILTER, &bpf_kfunc_set_skb);\n\treturn ret ?: register_btf_kfunc_id_set(BPF_PROG_TYPE_XDP, &bpf_kfunc_set_xdp);\n}\nlate_initcall(bpf_kfunc_init);\n\n \n__diag_push();\n__diag_ignore_all(\"-Wmissing-prototypes\",\n\t\t  \"Global functions as their definitions will be in vmlinux BTF\");\n\n \n__bpf_kfunc int bpf_sock_destroy(struct sock_common *sock)\n{\n\tstruct sock *sk = (struct sock *)sock;\n\n\t \n\tif (!sk->sk_prot->diag_destroy || (sk->sk_protocol != IPPROTO_TCP &&\n\t\t\t\t\t   sk->sk_protocol != IPPROTO_UDP))\n\t\treturn -EOPNOTSUPP;\n\n\treturn sk->sk_prot->diag_destroy(sk, ECONNABORTED);\n}\n\n__diag_pop()\n\nBTF_SET8_START(bpf_sk_iter_kfunc_ids)\nBTF_ID_FLAGS(func, bpf_sock_destroy, KF_TRUSTED_ARGS)\nBTF_SET8_END(bpf_sk_iter_kfunc_ids)\n\nstatic int tracing_iter_filter(const struct bpf_prog *prog, u32 kfunc_id)\n{\n\tif (btf_id_set8_contains(&bpf_sk_iter_kfunc_ids, kfunc_id) &&\n\t    prog->expected_attach_type != BPF_TRACE_ITER)\n\t\treturn -EACCES;\n\treturn 0;\n}\n\nstatic const struct btf_kfunc_id_set bpf_sk_iter_kfunc_set = {\n\t.owner = THIS_MODULE,\n\t.set   = &bpf_sk_iter_kfunc_ids,\n\t.filter = tracing_iter_filter,\n};\n\nstatic int init_subsystem(void)\n{\n\treturn register_btf_kfunc_id_set(BPF_PROG_TYPE_TRACING, &bpf_sk_iter_kfunc_set);\n}\nlate_initcall(init_subsystem);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}