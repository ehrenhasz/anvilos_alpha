{
  "module_name": "ib_recv.c",
  "hash_id": "59bcc19c7a4388c007598030f8766d6fb0e171e30b7cae76273af0063ea14940",
  "original_prompt": "Ingested from linux-6.6.14/net/rds/ib_recv.c",
  "human_readable_source": " \n#include <linux/kernel.h>\n#include <linux/sched/clock.h>\n#include <linux/slab.h>\n#include <linux/pci.h>\n#include <linux/dma-mapping.h>\n#include <rdma/rdma_cm.h>\n\n#include \"rds_single_path.h\"\n#include \"rds.h\"\n#include \"ib.h\"\n\nstatic struct kmem_cache *rds_ib_incoming_slab;\nstatic struct kmem_cache *rds_ib_frag_slab;\nstatic atomic_t\trds_ib_allocation = ATOMIC_INIT(0);\n\nvoid rds_ib_recv_init_ring(struct rds_ib_connection *ic)\n{\n\tstruct rds_ib_recv_work *recv;\n\tu32 i;\n\n\tfor (i = 0, recv = ic->i_recvs; i < ic->i_recv_ring.w_nr; i++, recv++) {\n\t\tstruct ib_sge *sge;\n\n\t\trecv->r_ibinc = NULL;\n\t\trecv->r_frag = NULL;\n\n\t\trecv->r_wr.next = NULL;\n\t\trecv->r_wr.wr_id = i;\n\t\trecv->r_wr.sg_list = recv->r_sge;\n\t\trecv->r_wr.num_sge = RDS_IB_RECV_SGE;\n\n\t\tsge = &recv->r_sge[0];\n\t\tsge->addr = ic->i_recv_hdrs_dma[i];\n\t\tsge->length = sizeof(struct rds_header);\n\t\tsge->lkey = ic->i_pd->local_dma_lkey;\n\n\t\tsge = &recv->r_sge[1];\n\t\tsge->addr = 0;\n\t\tsge->length = RDS_FRAG_SIZE;\n\t\tsge->lkey = ic->i_pd->local_dma_lkey;\n\t}\n}\n\n \nstatic void list_splice_entire_tail(struct list_head *from,\n\t\t\t\t    struct list_head *to)\n{\n\tstruct list_head *from_last = from->prev;\n\n\tlist_splice_tail(from_last, to);\n\tlist_add_tail(from_last, to);\n}\n\nstatic void rds_ib_cache_xfer_to_ready(struct rds_ib_refill_cache *cache)\n{\n\tstruct list_head *tmp;\n\n\ttmp = xchg(&cache->xfer, NULL);\n\tif (tmp) {\n\t\tif (cache->ready)\n\t\t\tlist_splice_entire_tail(tmp, cache->ready);\n\t\telse\n\t\t\tcache->ready = tmp;\n\t}\n}\n\nstatic int rds_ib_recv_alloc_cache(struct rds_ib_refill_cache *cache, gfp_t gfp)\n{\n\tstruct rds_ib_cache_head *head;\n\tint cpu;\n\n\tcache->percpu = alloc_percpu_gfp(struct rds_ib_cache_head, gfp);\n\tif (!cache->percpu)\n\t       return -ENOMEM;\n\n\tfor_each_possible_cpu(cpu) {\n\t\thead = per_cpu_ptr(cache->percpu, cpu);\n\t\thead->first = NULL;\n\t\thead->count = 0;\n\t}\n\tcache->xfer = NULL;\n\tcache->ready = NULL;\n\n\treturn 0;\n}\n\nint rds_ib_recv_alloc_caches(struct rds_ib_connection *ic, gfp_t gfp)\n{\n\tint ret;\n\n\tret = rds_ib_recv_alloc_cache(&ic->i_cache_incs, gfp);\n\tif (!ret) {\n\t\tret = rds_ib_recv_alloc_cache(&ic->i_cache_frags, gfp);\n\t\tif (ret)\n\t\t\tfree_percpu(ic->i_cache_incs.percpu);\n\t}\n\n\treturn ret;\n}\n\nstatic void rds_ib_cache_splice_all_lists(struct rds_ib_refill_cache *cache,\n\t\t\t\t\t  struct list_head *caller_list)\n{\n\tstruct rds_ib_cache_head *head;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\thead = per_cpu_ptr(cache->percpu, cpu);\n\t\tif (head->first) {\n\t\t\tlist_splice_entire_tail(head->first, caller_list);\n\t\t\thead->first = NULL;\n\t\t}\n\t}\n\n\tif (cache->ready) {\n\t\tlist_splice_entire_tail(cache->ready, caller_list);\n\t\tcache->ready = NULL;\n\t}\n}\n\nvoid rds_ib_recv_free_caches(struct rds_ib_connection *ic)\n{\n\tstruct rds_ib_incoming *inc;\n\tstruct rds_ib_incoming *inc_tmp;\n\tstruct rds_page_frag *frag;\n\tstruct rds_page_frag *frag_tmp;\n\tLIST_HEAD(list);\n\n\trds_ib_cache_xfer_to_ready(&ic->i_cache_incs);\n\trds_ib_cache_splice_all_lists(&ic->i_cache_incs, &list);\n\tfree_percpu(ic->i_cache_incs.percpu);\n\n\tlist_for_each_entry_safe(inc, inc_tmp, &list, ii_cache_entry) {\n\t\tlist_del(&inc->ii_cache_entry);\n\t\tWARN_ON(!list_empty(&inc->ii_frags));\n\t\tkmem_cache_free(rds_ib_incoming_slab, inc);\n\t\tatomic_dec(&rds_ib_allocation);\n\t}\n\n\trds_ib_cache_xfer_to_ready(&ic->i_cache_frags);\n\trds_ib_cache_splice_all_lists(&ic->i_cache_frags, &list);\n\tfree_percpu(ic->i_cache_frags.percpu);\n\n\tlist_for_each_entry_safe(frag, frag_tmp, &list, f_cache_entry) {\n\t\tlist_del(&frag->f_cache_entry);\n\t\tWARN_ON(!list_empty(&frag->f_item));\n\t\tkmem_cache_free(rds_ib_frag_slab, frag);\n\t}\n}\n\n \nstatic void rds_ib_recv_cache_put(struct list_head *new_item,\n\t\t\t\t  struct rds_ib_refill_cache *cache);\nstatic struct list_head *rds_ib_recv_cache_get(struct rds_ib_refill_cache *cache);\n\n\n \nstatic void rds_ib_frag_free(struct rds_ib_connection *ic,\n\t\t\t     struct rds_page_frag *frag)\n{\n\trdsdebug(\"frag %p page %p\\n\", frag, sg_page(&frag->f_sg));\n\n\trds_ib_recv_cache_put(&frag->f_cache_entry, &ic->i_cache_frags);\n\tatomic_add(RDS_FRAG_SIZE / SZ_1K, &ic->i_cache_allocs);\n\trds_ib_stats_add(s_ib_recv_added_to_cache, RDS_FRAG_SIZE);\n}\n\n \nvoid rds_ib_inc_free(struct rds_incoming *inc)\n{\n\tstruct rds_ib_incoming *ibinc;\n\tstruct rds_page_frag *frag;\n\tstruct rds_page_frag *pos;\n\tstruct rds_ib_connection *ic = inc->i_conn->c_transport_data;\n\n\tibinc = container_of(inc, struct rds_ib_incoming, ii_inc);\n\n\t \n\tlist_for_each_entry_safe(frag, pos, &ibinc->ii_frags, f_item) {\n\t\tlist_del_init(&frag->f_item);\n\t\trds_ib_frag_free(ic, frag);\n\t}\n\tBUG_ON(!list_empty(&ibinc->ii_frags));\n\n\trdsdebug(\"freeing ibinc %p inc %p\\n\", ibinc, inc);\n\trds_ib_recv_cache_put(&ibinc->ii_cache_entry, &ic->i_cache_incs);\n}\n\nstatic void rds_ib_recv_clear_one(struct rds_ib_connection *ic,\n\t\t\t\t  struct rds_ib_recv_work *recv)\n{\n\tif (recv->r_ibinc) {\n\t\trds_inc_put(&recv->r_ibinc->ii_inc);\n\t\trecv->r_ibinc = NULL;\n\t}\n\tif (recv->r_frag) {\n\t\tib_dma_unmap_sg(ic->i_cm_id->device, &recv->r_frag->f_sg, 1, DMA_FROM_DEVICE);\n\t\trds_ib_frag_free(ic, recv->r_frag);\n\t\trecv->r_frag = NULL;\n\t}\n}\n\nvoid rds_ib_recv_clear_ring(struct rds_ib_connection *ic)\n{\n\tu32 i;\n\n\tfor (i = 0; i < ic->i_recv_ring.w_nr; i++)\n\t\trds_ib_recv_clear_one(ic, &ic->i_recvs[i]);\n}\n\nstatic struct rds_ib_incoming *rds_ib_refill_one_inc(struct rds_ib_connection *ic,\n\t\t\t\t\t\t     gfp_t slab_mask)\n{\n\tstruct rds_ib_incoming *ibinc;\n\tstruct list_head *cache_item;\n\tint avail_allocs;\n\n\tcache_item = rds_ib_recv_cache_get(&ic->i_cache_incs);\n\tif (cache_item) {\n\t\tibinc = container_of(cache_item, struct rds_ib_incoming, ii_cache_entry);\n\t} else {\n\t\tavail_allocs = atomic_add_unless(&rds_ib_allocation,\n\t\t\t\t\t\t 1, rds_ib_sysctl_max_recv_allocation);\n\t\tif (!avail_allocs) {\n\t\t\trds_ib_stats_inc(s_ib_rx_alloc_limit);\n\t\t\treturn NULL;\n\t\t}\n\t\tibinc = kmem_cache_alloc(rds_ib_incoming_slab, slab_mask);\n\t\tif (!ibinc) {\n\t\t\tatomic_dec(&rds_ib_allocation);\n\t\t\treturn NULL;\n\t\t}\n\t\trds_ib_stats_inc(s_ib_rx_total_incs);\n\t}\n\tINIT_LIST_HEAD(&ibinc->ii_frags);\n\trds_inc_init(&ibinc->ii_inc, ic->conn, &ic->conn->c_faddr);\n\n\treturn ibinc;\n}\n\nstatic struct rds_page_frag *rds_ib_refill_one_frag(struct rds_ib_connection *ic,\n\t\t\t\t\t\t    gfp_t slab_mask, gfp_t page_mask)\n{\n\tstruct rds_page_frag *frag;\n\tstruct list_head *cache_item;\n\tint ret;\n\n\tcache_item = rds_ib_recv_cache_get(&ic->i_cache_frags);\n\tif (cache_item) {\n\t\tfrag = container_of(cache_item, struct rds_page_frag, f_cache_entry);\n\t\tatomic_sub(RDS_FRAG_SIZE / SZ_1K, &ic->i_cache_allocs);\n\t\trds_ib_stats_add(s_ib_recv_added_to_cache, RDS_FRAG_SIZE);\n\t} else {\n\t\tfrag = kmem_cache_alloc(rds_ib_frag_slab, slab_mask);\n\t\tif (!frag)\n\t\t\treturn NULL;\n\n\t\tsg_init_table(&frag->f_sg, 1);\n\t\tret = rds_page_remainder_alloc(&frag->f_sg,\n\t\t\t\t\t       RDS_FRAG_SIZE, page_mask);\n\t\tif (ret) {\n\t\t\tkmem_cache_free(rds_ib_frag_slab, frag);\n\t\t\treturn NULL;\n\t\t}\n\t\trds_ib_stats_inc(s_ib_rx_total_frags);\n\t}\n\n\tINIT_LIST_HEAD(&frag->f_item);\n\n\treturn frag;\n}\n\nstatic int rds_ib_recv_refill_one(struct rds_connection *conn,\n\t\t\t\t  struct rds_ib_recv_work *recv, gfp_t gfp)\n{\n\tstruct rds_ib_connection *ic = conn->c_transport_data;\n\tstruct ib_sge *sge;\n\tint ret = -ENOMEM;\n\tgfp_t slab_mask = gfp;\n\tgfp_t page_mask = gfp;\n\n\tif (gfp & __GFP_DIRECT_RECLAIM) {\n\t\tslab_mask = GFP_KERNEL;\n\t\tpage_mask = GFP_HIGHUSER;\n\t}\n\n\tif (!ic->i_cache_incs.ready)\n\t\trds_ib_cache_xfer_to_ready(&ic->i_cache_incs);\n\tif (!ic->i_cache_frags.ready)\n\t\trds_ib_cache_xfer_to_ready(&ic->i_cache_frags);\n\n\t \n\tif (!recv->r_ibinc) {\n\t\trecv->r_ibinc = rds_ib_refill_one_inc(ic, slab_mask);\n\t\tif (!recv->r_ibinc)\n\t\t\tgoto out;\n\t}\n\n\tWARN_ON(recv->r_frag);  \n\trecv->r_frag = rds_ib_refill_one_frag(ic, slab_mask, page_mask);\n\tif (!recv->r_frag)\n\t\tgoto out;\n\n\tret = ib_dma_map_sg(ic->i_cm_id->device, &recv->r_frag->f_sg,\n\t\t\t    1, DMA_FROM_DEVICE);\n\tWARN_ON(ret != 1);\n\n\tsge = &recv->r_sge[0];\n\tsge->addr = ic->i_recv_hdrs_dma[recv - ic->i_recvs];\n\tsge->length = sizeof(struct rds_header);\n\n\tsge = &recv->r_sge[1];\n\tsge->addr = sg_dma_address(&recv->r_frag->f_sg);\n\tsge->length = sg_dma_len(&recv->r_frag->f_sg);\n\n\tret = 0;\nout:\n\treturn ret;\n}\n\nstatic int acquire_refill(struct rds_connection *conn)\n{\n\treturn test_and_set_bit(RDS_RECV_REFILL, &conn->c_flags) == 0;\n}\n\nstatic void release_refill(struct rds_connection *conn)\n{\n\tclear_bit(RDS_RECV_REFILL, &conn->c_flags);\n\tsmp_mb__after_atomic();\n\n\t \n\tif (waitqueue_active(&conn->c_waitq))\n\t\twake_up_all(&conn->c_waitq);\n}\n\n \nvoid rds_ib_recv_refill(struct rds_connection *conn, int prefill, gfp_t gfp)\n{\n\tstruct rds_ib_connection *ic = conn->c_transport_data;\n\tstruct rds_ib_recv_work *recv;\n\tunsigned int posted = 0;\n\tint ret = 0;\n\tbool can_wait = !!(gfp & __GFP_DIRECT_RECLAIM);\n\tbool must_wake = false;\n\tu32 pos;\n\n\t \n\tif (!acquire_refill(conn))\n\t\treturn;\n\n\twhile ((prefill || rds_conn_up(conn)) &&\n\t       rds_ib_ring_alloc(&ic->i_recv_ring, 1, &pos)) {\n\t\tif (pos >= ic->i_recv_ring.w_nr) {\n\t\t\tprintk(KERN_NOTICE \"Argh - ring alloc returned pos=%u\\n\",\n\t\t\t\t\tpos);\n\t\t\tbreak;\n\t\t}\n\n\t\trecv = &ic->i_recvs[pos];\n\t\tret = rds_ib_recv_refill_one(conn, recv, gfp);\n\t\tif (ret) {\n\t\t\tmust_wake = true;\n\t\t\tbreak;\n\t\t}\n\n\t\trdsdebug(\"recv %p ibinc %p page %p addr %lu\\n\", recv,\n\t\t\t recv->r_ibinc, sg_page(&recv->r_frag->f_sg),\n\t\t\t (long)sg_dma_address(&recv->r_frag->f_sg));\n\n\t\t \n\t\tret = ib_post_recv(ic->i_cm_id->qp, &recv->r_wr, NULL);\n\t\tif (ret) {\n\t\t\trds_ib_conn_error(conn, \"recv post on \"\n\t\t\t       \"%pI6c returned %d, disconnecting and \"\n\t\t\t       \"reconnecting\\n\", &conn->c_faddr,\n\t\t\t       ret);\n\t\t\tbreak;\n\t\t}\n\n\t\tposted++;\n\n\t\tif ((posted > 128 && need_resched()) || posted > 8192) {\n\t\t\tmust_wake = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\tif (ic->i_flowctl && posted)\n\t\trds_ib_advertise_credits(conn, posted);\n\n\tif (ret)\n\t\trds_ib_ring_unalloc(&ic->i_recv_ring, 1);\n\n\trelease_refill(conn);\n\n\t \n\tif (rds_conn_up(conn) &&\n\t    (must_wake ||\n\t    (can_wait && rds_ib_ring_low(&ic->i_recv_ring)) ||\n\t    rds_ib_ring_empty(&ic->i_recv_ring))) {\n\t\tqueue_delayed_work(rds_wq, &conn->c_recv_w, 1);\n\t}\n\tif (can_wait)\n\t\tcond_resched();\n}\n\n \nstatic void rds_ib_recv_cache_put(struct list_head *new_item,\n\t\t\t\t struct rds_ib_refill_cache *cache)\n{\n\tunsigned long flags;\n\tstruct list_head *old, *chpfirst;\n\n\tlocal_irq_save(flags);\n\n\tchpfirst = __this_cpu_read(cache->percpu->first);\n\tif (!chpfirst)\n\t\tINIT_LIST_HEAD(new_item);\n\telse  \n\t\tlist_add_tail(new_item, chpfirst);\n\n\t__this_cpu_write(cache->percpu->first, new_item);\n\t__this_cpu_inc(cache->percpu->count);\n\n\tif (__this_cpu_read(cache->percpu->count) < RDS_IB_RECYCLE_BATCH_COUNT)\n\t\tgoto end;\n\n\t \n\tdo {\n\t\told = xchg(&cache->xfer, NULL);\n\t\tif (old)\n\t\t\tlist_splice_entire_tail(old, chpfirst);\n\t\told = cmpxchg(&cache->xfer, NULL, chpfirst);\n\t} while (old);\n\n\n\t__this_cpu_write(cache->percpu->first, NULL);\n\t__this_cpu_write(cache->percpu->count, 0);\nend:\n\tlocal_irq_restore(flags);\n}\n\nstatic struct list_head *rds_ib_recv_cache_get(struct rds_ib_refill_cache *cache)\n{\n\tstruct list_head *head = cache->ready;\n\n\tif (head) {\n\t\tif (!list_empty(head)) {\n\t\t\tcache->ready = head->next;\n\t\t\tlist_del_init(head);\n\t\t} else\n\t\t\tcache->ready = NULL;\n\t}\n\n\treturn head;\n}\n\nint rds_ib_inc_copy_to_user(struct rds_incoming *inc, struct iov_iter *to)\n{\n\tstruct rds_ib_incoming *ibinc;\n\tstruct rds_page_frag *frag;\n\tunsigned long to_copy;\n\tunsigned long frag_off = 0;\n\tint copied = 0;\n\tint ret;\n\tu32 len;\n\n\tibinc = container_of(inc, struct rds_ib_incoming, ii_inc);\n\tfrag = list_entry(ibinc->ii_frags.next, struct rds_page_frag, f_item);\n\tlen = be32_to_cpu(inc->i_hdr.h_len);\n\n\twhile (iov_iter_count(to) && copied < len) {\n\t\tif (frag_off == RDS_FRAG_SIZE) {\n\t\t\tfrag = list_entry(frag->f_item.next,\n\t\t\t\t\t  struct rds_page_frag, f_item);\n\t\t\tfrag_off = 0;\n\t\t}\n\t\tto_copy = min_t(unsigned long, iov_iter_count(to),\n\t\t\t\tRDS_FRAG_SIZE - frag_off);\n\t\tto_copy = min_t(unsigned long, to_copy, len - copied);\n\n\t\t \n\t\trds_stats_add(s_copy_to_user, to_copy);\n\t\tret = copy_page_to_iter(sg_page(&frag->f_sg),\n\t\t\t\t\tfrag->f_sg.offset + frag_off,\n\t\t\t\t\tto_copy,\n\t\t\t\t\tto);\n\t\tif (ret != to_copy)\n\t\t\treturn -EFAULT;\n\n\t\tfrag_off += to_copy;\n\t\tcopied += to_copy;\n\t}\n\n\treturn copied;\n}\n\n \nvoid rds_ib_recv_init_ack(struct rds_ib_connection *ic)\n{\n\tstruct ib_send_wr *wr = &ic->i_ack_wr;\n\tstruct ib_sge *sge = &ic->i_ack_sge;\n\n\tsge->addr = ic->i_ack_dma;\n\tsge->length = sizeof(struct rds_header);\n\tsge->lkey = ic->i_pd->local_dma_lkey;\n\n\twr->sg_list = sge;\n\twr->num_sge = 1;\n\twr->opcode = IB_WR_SEND;\n\twr->wr_id = RDS_IB_ACK_WR_ID;\n\twr->send_flags = IB_SEND_SIGNALED | IB_SEND_SOLICITED;\n}\n\n \n#ifndef KERNEL_HAS_ATOMIC64\nvoid rds_ib_set_ack(struct rds_ib_connection *ic, u64 seq, int ack_required)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ic->i_ack_lock, flags);\n\tic->i_ack_next = seq;\n\tif (ack_required)\n\t\tset_bit(IB_ACK_REQUESTED, &ic->i_ack_flags);\n\tspin_unlock_irqrestore(&ic->i_ack_lock, flags);\n}\n\nstatic u64 rds_ib_get_ack(struct rds_ib_connection *ic)\n{\n\tunsigned long flags;\n\tu64 seq;\n\n\tclear_bit(IB_ACK_REQUESTED, &ic->i_ack_flags);\n\n\tspin_lock_irqsave(&ic->i_ack_lock, flags);\n\tseq = ic->i_ack_next;\n\tspin_unlock_irqrestore(&ic->i_ack_lock, flags);\n\n\treturn seq;\n}\n#else\nvoid rds_ib_set_ack(struct rds_ib_connection *ic, u64 seq, int ack_required)\n{\n\tatomic64_set(&ic->i_ack_next, seq);\n\tif (ack_required) {\n\t\tsmp_mb__before_atomic();\n\t\tset_bit(IB_ACK_REQUESTED, &ic->i_ack_flags);\n\t}\n}\n\nstatic u64 rds_ib_get_ack(struct rds_ib_connection *ic)\n{\n\tclear_bit(IB_ACK_REQUESTED, &ic->i_ack_flags);\n\tsmp_mb__after_atomic();\n\n\treturn atomic64_read(&ic->i_ack_next);\n}\n#endif\n\n\nstatic void rds_ib_send_ack(struct rds_ib_connection *ic, unsigned int adv_credits)\n{\n\tstruct rds_header *hdr = ic->i_ack;\n\tu64 seq;\n\tint ret;\n\n\tseq = rds_ib_get_ack(ic);\n\n\trdsdebug(\"send_ack: ic %p ack %llu\\n\", ic, (unsigned long long) seq);\n\n\tib_dma_sync_single_for_cpu(ic->rds_ibdev->dev, ic->i_ack_dma,\n\t\t\t\t   sizeof(*hdr), DMA_TO_DEVICE);\n\trds_message_populate_header(hdr, 0, 0, 0);\n\thdr->h_ack = cpu_to_be64(seq);\n\thdr->h_credit = adv_credits;\n\trds_message_make_checksum(hdr);\n\tib_dma_sync_single_for_device(ic->rds_ibdev->dev, ic->i_ack_dma,\n\t\t\t\t      sizeof(*hdr), DMA_TO_DEVICE);\n\n\tic->i_ack_queued = jiffies;\n\n\tret = ib_post_send(ic->i_cm_id->qp, &ic->i_ack_wr, NULL);\n\tif (unlikely(ret)) {\n\t\t \n\t\tclear_bit(IB_ACK_IN_FLIGHT, &ic->i_ack_flags);\n\t\tset_bit(IB_ACK_REQUESTED, &ic->i_ack_flags);\n\n\t\trds_ib_stats_inc(s_ib_ack_send_failure);\n\n\t\trds_ib_conn_error(ic->conn, \"sending ack failed\\n\");\n\t} else\n\t\trds_ib_stats_inc(s_ib_ack_sent);\n}\n\n \n\n \nvoid rds_ib_attempt_ack(struct rds_ib_connection *ic)\n{\n\tunsigned int adv_credits;\n\n\tif (!test_bit(IB_ACK_REQUESTED, &ic->i_ack_flags))\n\t\treturn;\n\n\tif (test_and_set_bit(IB_ACK_IN_FLIGHT, &ic->i_ack_flags)) {\n\t\trds_ib_stats_inc(s_ib_ack_send_delayed);\n\t\treturn;\n\t}\n\n\t \n\tif (!rds_ib_send_grab_credits(ic, 1, &adv_credits, 0, RDS_MAX_ADV_CREDIT)) {\n\t\trds_ib_stats_inc(s_ib_tx_throttle);\n\t\tclear_bit(IB_ACK_IN_FLIGHT, &ic->i_ack_flags);\n\t\treturn;\n\t}\n\n\tclear_bit(IB_ACK_REQUESTED, &ic->i_ack_flags);\n\trds_ib_send_ack(ic, adv_credits);\n}\n\n \nvoid rds_ib_ack_send_complete(struct rds_ib_connection *ic)\n{\n\tclear_bit(IB_ACK_IN_FLIGHT, &ic->i_ack_flags);\n\trds_ib_attempt_ack(ic);\n}\n\n \nu64 rds_ib_piggyb_ack(struct rds_ib_connection *ic)\n{\n\tif (test_and_clear_bit(IB_ACK_REQUESTED, &ic->i_ack_flags))\n\t\trds_ib_stats_inc(s_ib_ack_send_piggybacked);\n\treturn rds_ib_get_ack(ic);\n}\n\n \nstatic void rds_ib_cong_recv(struct rds_connection *conn,\n\t\t\t      struct rds_ib_incoming *ibinc)\n{\n\tstruct rds_cong_map *map;\n\tunsigned int map_off;\n\tunsigned int map_page;\n\tstruct rds_page_frag *frag;\n\tunsigned long frag_off;\n\tunsigned long to_copy;\n\tunsigned long copied;\n\t__le64 uncongested = 0;\n\tvoid *addr;\n\n\t \n\tif (be32_to_cpu(ibinc->ii_inc.i_hdr.h_len) != RDS_CONG_MAP_BYTES)\n\t\treturn;\n\n\tmap = conn->c_fcong;\n\tmap_page = 0;\n\tmap_off = 0;\n\n\tfrag = list_entry(ibinc->ii_frags.next, struct rds_page_frag, f_item);\n\tfrag_off = 0;\n\n\tcopied = 0;\n\n\twhile (copied < RDS_CONG_MAP_BYTES) {\n\t\t__le64 *src, *dst;\n\t\tunsigned int k;\n\n\t\tto_copy = min(RDS_FRAG_SIZE - frag_off, PAGE_SIZE - map_off);\n\t\tBUG_ON(to_copy & 7);  \n\n\t\taddr = kmap_atomic(sg_page(&frag->f_sg));\n\n\t\tsrc = addr + frag->f_sg.offset + frag_off;\n\t\tdst = (void *)map->m_page_addrs[map_page] + map_off;\n\t\tfor (k = 0; k < to_copy; k += 8) {\n\t\t\t \n\t\t\tuncongested |= ~(*src) & *dst;\n\t\t\t*dst++ = *src++;\n\t\t}\n\t\tkunmap_atomic(addr);\n\n\t\tcopied += to_copy;\n\n\t\tmap_off += to_copy;\n\t\tif (map_off == PAGE_SIZE) {\n\t\t\tmap_off = 0;\n\t\t\tmap_page++;\n\t\t}\n\n\t\tfrag_off += to_copy;\n\t\tif (frag_off == RDS_FRAG_SIZE) {\n\t\t\tfrag = list_entry(frag->f_item.next,\n\t\t\t\t\t  struct rds_page_frag, f_item);\n\t\t\tfrag_off = 0;\n\t\t}\n\t}\n\n\t \n\trds_cong_map_updated(map, le64_to_cpu(uncongested));\n}\n\nstatic void rds_ib_process_recv(struct rds_connection *conn,\n\t\t\t\tstruct rds_ib_recv_work *recv, u32 data_len,\n\t\t\t\tstruct rds_ib_ack_state *state)\n{\n\tstruct rds_ib_connection *ic = conn->c_transport_data;\n\tstruct rds_ib_incoming *ibinc = ic->i_ibinc;\n\tstruct rds_header *ihdr, *hdr;\n\tdma_addr_t dma_addr = ic->i_recv_hdrs_dma[recv - ic->i_recvs];\n\n\t \n\n\trdsdebug(\"ic %p ibinc %p recv %p byte len %u\\n\", ic, ibinc, recv,\n\t\t data_len);\n\n\tif (data_len < sizeof(struct rds_header)) {\n\t\trds_ib_conn_error(conn, \"incoming message \"\n\t\t       \"from %pI6c didn't include a \"\n\t\t       \"header, disconnecting and \"\n\t\t       \"reconnecting\\n\",\n\t\t       &conn->c_faddr);\n\t\treturn;\n\t}\n\tdata_len -= sizeof(struct rds_header);\n\n\tihdr = ic->i_recv_hdrs[recv - ic->i_recvs];\n\n\tib_dma_sync_single_for_cpu(ic->rds_ibdev->dev, dma_addr,\n\t\t\t\t   sizeof(*ihdr), DMA_FROM_DEVICE);\n\t \n\tif (!rds_message_verify_checksum(ihdr)) {\n\t\trds_ib_conn_error(conn, \"incoming message \"\n\t\t       \"from %pI6c has corrupted header - \"\n\t\t       \"forcing a reconnect\\n\",\n\t\t       &conn->c_faddr);\n\t\trds_stats_inc(s_recv_drop_bad_checksum);\n\t\tgoto done;\n\t}\n\n\t \n\tstate->ack_recv = be64_to_cpu(ihdr->h_ack);\n\tstate->ack_recv_valid = 1;\n\n\t \n\tif (ihdr->h_credit)\n\t\trds_ib_send_add_credits(conn, ihdr->h_credit);\n\n\tif (ihdr->h_sport == 0 && ihdr->h_dport == 0 && data_len == 0) {\n\t\t \n\t\trds_ib_stats_inc(s_ib_ack_received);\n\n\t\t \n\t\trds_ib_frag_free(ic, recv->r_frag);\n\t\trecv->r_frag = NULL;\n\t\tgoto done;\n\t}\n\n\t \n\tif (!ibinc) {\n\t\tibinc = recv->r_ibinc;\n\t\trecv->r_ibinc = NULL;\n\t\tic->i_ibinc = ibinc;\n\n\t\thdr = &ibinc->ii_inc.i_hdr;\n\t\tibinc->ii_inc.i_rx_lat_trace[RDS_MSG_RX_HDR] =\n\t\t\t\tlocal_clock();\n\t\tmemcpy(hdr, ihdr, sizeof(*hdr));\n\t\tic->i_recv_data_rem = be32_to_cpu(hdr->h_len);\n\t\tibinc->ii_inc.i_rx_lat_trace[RDS_MSG_RX_START] =\n\t\t\t\tlocal_clock();\n\n\t\trdsdebug(\"ic %p ibinc %p rem %u flag 0x%x\\n\", ic, ibinc,\n\t\t\t ic->i_recv_data_rem, hdr->h_flags);\n\t} else {\n\t\thdr = &ibinc->ii_inc.i_hdr;\n\t\t \n\t\tif (hdr->h_sequence != ihdr->h_sequence ||\n\t\t    hdr->h_len != ihdr->h_len ||\n\t\t    hdr->h_sport != ihdr->h_sport ||\n\t\t    hdr->h_dport != ihdr->h_dport) {\n\t\t\trds_ib_conn_error(conn,\n\t\t\t\t\"fragment header mismatch; forcing reconnect\\n\");\n\t\t\tgoto done;\n\t\t}\n\t}\n\n\tlist_add_tail(&recv->r_frag->f_item, &ibinc->ii_frags);\n\trecv->r_frag = NULL;\n\n\tif (ic->i_recv_data_rem > RDS_FRAG_SIZE)\n\t\tic->i_recv_data_rem -= RDS_FRAG_SIZE;\n\telse {\n\t\tic->i_recv_data_rem = 0;\n\t\tic->i_ibinc = NULL;\n\n\t\tif (ibinc->ii_inc.i_hdr.h_flags == RDS_FLAG_CONG_BITMAP) {\n\t\t\trds_ib_cong_recv(conn, ibinc);\n\t\t} else {\n\t\t\trds_recv_incoming(conn, &conn->c_faddr, &conn->c_laddr,\n\t\t\t\t\t  &ibinc->ii_inc, GFP_ATOMIC);\n\t\t\tstate->ack_next = be64_to_cpu(hdr->h_sequence);\n\t\t\tstate->ack_next_valid = 1;\n\t\t}\n\n\t\t \n\t\tif (hdr->h_flags & RDS_FLAG_ACK_REQUIRED) {\n\t\t\trds_stats_inc(s_recv_ack_required);\n\t\t\tstate->ack_required = 1;\n\t\t}\n\n\t\trds_inc_put(&ibinc->ii_inc);\n\t}\ndone:\n\tib_dma_sync_single_for_device(ic->rds_ibdev->dev, dma_addr,\n\t\t\t\t      sizeof(*ihdr), DMA_FROM_DEVICE);\n}\n\nvoid rds_ib_recv_cqe_handler(struct rds_ib_connection *ic,\n\t\t\t     struct ib_wc *wc,\n\t\t\t     struct rds_ib_ack_state *state)\n{\n\tstruct rds_connection *conn = ic->conn;\n\tstruct rds_ib_recv_work *recv;\n\n\trdsdebug(\"wc wr_id 0x%llx status %u (%s) byte_len %u imm_data %u\\n\",\n\t\t (unsigned long long)wc->wr_id, wc->status,\n\t\t ib_wc_status_msg(wc->status), wc->byte_len,\n\t\t be32_to_cpu(wc->ex.imm_data));\n\n\trds_ib_stats_inc(s_ib_rx_cq_event);\n\trecv = &ic->i_recvs[rds_ib_ring_oldest(&ic->i_recv_ring)];\n\tib_dma_unmap_sg(ic->i_cm_id->device, &recv->r_frag->f_sg, 1,\n\t\t\tDMA_FROM_DEVICE);\n\n\t \n\tif (wc->status == IB_WC_SUCCESS) {\n\t\trds_ib_process_recv(conn, recv, wc->byte_len, state);\n\t} else {\n\t\t \n\t\tif (rds_conn_up(conn) || rds_conn_connecting(conn))\n\t\t\trds_ib_conn_error(conn, \"recv completion on <%pI6c,%pI6c, %d> had status %u (%s), vendor err 0x%x, disconnecting and reconnecting\\n\",\n\t\t\t\t\t  &conn->c_laddr, &conn->c_faddr,\n\t\t\t\t\t  conn->c_tos, wc->status,\n\t\t\t\t\t  ib_wc_status_msg(wc->status),\n\t\t\t\t\t  wc->vendor_err);\n\t}\n\n\t \n\tif (recv->r_frag) {\n\t\trds_ib_frag_free(ic, recv->r_frag);\n\t\trecv->r_frag = NULL;\n\t}\n\trds_ib_ring_free(&ic->i_recv_ring, 1);\n\n\t \n\tif (rds_ib_ring_empty(&ic->i_recv_ring))\n\t\trds_ib_stats_inc(s_ib_rx_ring_empty);\n\n\tif (rds_ib_ring_low(&ic->i_recv_ring)) {\n\t\trds_ib_recv_refill(conn, 0, GFP_NOWAIT | __GFP_NOWARN);\n\t\trds_ib_stats_inc(s_ib_rx_refill_from_cq);\n\t}\n}\n\nint rds_ib_recv_path(struct rds_conn_path *cp)\n{\n\tstruct rds_connection *conn = cp->cp_conn;\n\tstruct rds_ib_connection *ic = conn->c_transport_data;\n\n\trdsdebug(\"conn %p\\n\", conn);\n\tif (rds_conn_up(conn)) {\n\t\trds_ib_attempt_ack(ic);\n\t\trds_ib_recv_refill(conn, 0, GFP_KERNEL);\n\t\trds_ib_stats_inc(s_ib_rx_refill_from_thread);\n\t}\n\n\treturn 0;\n}\n\nint rds_ib_recv_init(void)\n{\n\tstruct sysinfo si;\n\tint ret = -ENOMEM;\n\n\t \n\tsi_meminfo(&si);\n\trds_ib_sysctl_max_recv_allocation = si.totalram / 3 * PAGE_SIZE / RDS_FRAG_SIZE;\n\n\trds_ib_incoming_slab =\n\t\tkmem_cache_create_usercopy(\"rds_ib_incoming\",\n\t\t\t\t\t   sizeof(struct rds_ib_incoming),\n\t\t\t\t\t   0, SLAB_HWCACHE_ALIGN,\n\t\t\t\t\t   offsetof(struct rds_ib_incoming,\n\t\t\t\t\t\t    ii_inc.i_usercopy),\n\t\t\t\t\t   sizeof(struct rds_inc_usercopy),\n\t\t\t\t\t   NULL);\n\tif (!rds_ib_incoming_slab)\n\t\tgoto out;\n\n\trds_ib_frag_slab = kmem_cache_create(\"rds_ib_frag\",\n\t\t\t\t\tsizeof(struct rds_page_frag),\n\t\t\t\t\t0, SLAB_HWCACHE_ALIGN, NULL);\n\tif (!rds_ib_frag_slab) {\n\t\tkmem_cache_destroy(rds_ib_incoming_slab);\n\t\trds_ib_incoming_slab = NULL;\n\t} else\n\t\tret = 0;\nout:\n\treturn ret;\n}\n\nvoid rds_ib_recv_exit(void)\n{\n\tWARN_ON(atomic_read(&rds_ib_allocation));\n\n\tkmem_cache_destroy(rds_ib_incoming_slab);\n\tkmem_cache_destroy(rds_ib_frag_slab);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}