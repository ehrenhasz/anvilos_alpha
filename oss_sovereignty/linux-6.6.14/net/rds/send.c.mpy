{
  "module_name": "send.c",
  "hash_id": "933aac9dffb13e546d70c031cbad43aff34c2691addec8d125f4d07b7fed9ccb",
  "original_prompt": "Ingested from linux-6.6.14/net/rds/send.c",
  "human_readable_source": " \n#include <linux/kernel.h>\n#include <linux/moduleparam.h>\n#include <linux/gfp.h>\n#include <net/sock.h>\n#include <linux/in.h>\n#include <linux/list.h>\n#include <linux/ratelimit.h>\n#include <linux/export.h>\n#include <linux/sizes.h>\n\n#include \"rds.h\"\n\n \nstatic int send_batch_count = SZ_1K;\nmodule_param(send_batch_count, int, 0444);\nMODULE_PARM_DESC(send_batch_count, \" batch factor when working the send queue\");\n\nstatic void rds_send_remove_from_sock(struct list_head *messages, int status);\n\n \nvoid rds_send_path_reset(struct rds_conn_path *cp)\n{\n\tstruct rds_message *rm, *tmp;\n\tunsigned long flags;\n\n\tif (cp->cp_xmit_rm) {\n\t\trm = cp->cp_xmit_rm;\n\t\tcp->cp_xmit_rm = NULL;\n\t\t \n\t\trds_message_unmapped(rm);\n\t\trds_message_put(rm);\n\t}\n\n\tcp->cp_xmit_sg = 0;\n\tcp->cp_xmit_hdr_off = 0;\n\tcp->cp_xmit_data_off = 0;\n\tcp->cp_xmit_atomic_sent = 0;\n\tcp->cp_xmit_rdma_sent = 0;\n\tcp->cp_xmit_data_sent = 0;\n\n\tcp->cp_conn->c_map_queued = 0;\n\n\tcp->cp_unacked_packets = rds_sysctl_max_unacked_packets;\n\tcp->cp_unacked_bytes = rds_sysctl_max_unacked_bytes;\n\n\t \n\tspin_lock_irqsave(&cp->cp_lock, flags);\n\tlist_for_each_entry_safe(rm, tmp, &cp->cp_retrans, m_conn_item) {\n\t\tset_bit(RDS_MSG_ACK_REQUIRED, &rm->m_flags);\n\t\tset_bit(RDS_MSG_RETRANSMITTED, &rm->m_flags);\n\t}\n\tlist_splice_init(&cp->cp_retrans, &cp->cp_send_queue);\n\tspin_unlock_irqrestore(&cp->cp_lock, flags);\n}\nEXPORT_SYMBOL_GPL(rds_send_path_reset);\n\nstatic int acquire_in_xmit(struct rds_conn_path *cp)\n{\n\treturn test_and_set_bit(RDS_IN_XMIT, &cp->cp_flags) == 0;\n}\n\nstatic void release_in_xmit(struct rds_conn_path *cp)\n{\n\tclear_bit(RDS_IN_XMIT, &cp->cp_flags);\n\tsmp_mb__after_atomic();\n\t \n\tif (waitqueue_active(&cp->cp_waitq))\n\t\twake_up_all(&cp->cp_waitq);\n}\n\n \nint rds_send_xmit(struct rds_conn_path *cp)\n{\n\tstruct rds_connection *conn = cp->cp_conn;\n\tstruct rds_message *rm;\n\tunsigned long flags;\n\tunsigned int tmp;\n\tstruct scatterlist *sg;\n\tint ret = 0;\n\tLIST_HEAD(to_be_dropped);\n\tint batch_count;\n\tunsigned long send_gen = 0;\n\tint same_rm = 0;\n\nrestart:\n\tbatch_count = 0;\n\n\t \n\tif (!acquire_in_xmit(cp)) {\n\t\trds_stats_inc(s_send_lock_contention);\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tif (rds_destroy_pending(cp->cp_conn)) {\n\t\trelease_in_xmit(cp);\n\t\tret = -ENETUNREACH;  \n\t\tgoto out;\n\t}\n\n\t \n\tsend_gen = READ_ONCE(cp->cp_send_gen) + 1;\n\tWRITE_ONCE(cp->cp_send_gen, send_gen);\n\n\t \n\tif (!rds_conn_path_up(cp)) {\n\t\trelease_in_xmit(cp);\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\tif (conn->c_trans->xmit_path_prepare)\n\t\tconn->c_trans->xmit_path_prepare(cp);\n\n\t \n\twhile (1) {\n\n\t\trm = cp->cp_xmit_rm;\n\n\t\tif (!rm) {\n\t\t\tsame_rm = 0;\n\t\t} else {\n\t\t\tsame_rm++;\n\t\t\tif (same_rm >= 4096) {\n\t\t\t\trds_stats_inc(s_send_stuck_rm);\n\t\t\t\tret = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (!rm && test_and_clear_bit(0, &conn->c_map_queued)) {\n\t\t\trm = rds_cong_update_alloc(conn);\n\t\t\tif (IS_ERR(rm)) {\n\t\t\t\tret = PTR_ERR(rm);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\trm->data.op_active = 1;\n\t\t\trm->m_inc.i_conn_path = cp;\n\t\t\trm->m_inc.i_conn = cp->cp_conn;\n\n\t\t\tcp->cp_xmit_rm = rm;\n\t\t}\n\n\t\t \n\t\tif (!rm) {\n\t\t\tunsigned int len;\n\n\t\t\tbatch_count++;\n\n\t\t\t \n\t\t\tif (batch_count >= send_batch_count)\n\t\t\t\tgoto over_batch;\n\n\t\t\tspin_lock_irqsave(&cp->cp_lock, flags);\n\n\t\t\tif (!list_empty(&cp->cp_send_queue)) {\n\t\t\t\trm = list_entry(cp->cp_send_queue.next,\n\t\t\t\t\t\tstruct rds_message,\n\t\t\t\t\t\tm_conn_item);\n\t\t\t\trds_message_addref(rm);\n\n\t\t\t\t \n\t\t\t\tlist_move_tail(&rm->m_conn_item,\n\t\t\t\t\t       &cp->cp_retrans);\n\t\t\t}\n\n\t\t\tspin_unlock_irqrestore(&cp->cp_lock, flags);\n\n\t\t\tif (!rm)\n\t\t\t\tbreak;\n\n\t\t\t \n\t\t\tif (test_bit(RDS_MSG_FLUSH, &rm->m_flags) ||\n\t\t\t    (rm->rdma.op_active &&\n\t\t\t    test_bit(RDS_MSG_RETRANSMITTED, &rm->m_flags))) {\n\t\t\t\tspin_lock_irqsave(&cp->cp_lock, flags);\n\t\t\t\tif (test_and_clear_bit(RDS_MSG_ON_CONN, &rm->m_flags))\n\t\t\t\t\tlist_move(&rm->m_conn_item, &to_be_dropped);\n\t\t\t\tspin_unlock_irqrestore(&cp->cp_lock, flags);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t \n\t\t\tlen = ntohl(rm->m_inc.i_hdr.h_len);\n\t\t\tif (cp->cp_unacked_packets == 0 ||\n\t\t\t    cp->cp_unacked_bytes < len) {\n\t\t\t\tset_bit(RDS_MSG_ACK_REQUIRED, &rm->m_flags);\n\n\t\t\t\tcp->cp_unacked_packets =\n\t\t\t\t\trds_sysctl_max_unacked_packets;\n\t\t\t\tcp->cp_unacked_bytes =\n\t\t\t\t\trds_sysctl_max_unacked_bytes;\n\t\t\t\trds_stats_inc(s_send_ack_required);\n\t\t\t} else {\n\t\t\t\tcp->cp_unacked_bytes -= len;\n\t\t\t\tcp->cp_unacked_packets--;\n\t\t\t}\n\n\t\t\tcp->cp_xmit_rm = rm;\n\t\t}\n\n\t\t \n\t\tif (rm->rdma.op_active && !cp->cp_xmit_rdma_sent) {\n\t\t\trm->m_final_op = &rm->rdma;\n\t\t\t \n\t\t\tset_bit(RDS_MSG_MAPPED, &rm->m_flags);\n\t\t\tret = conn->c_trans->xmit_rdma(conn, &rm->rdma);\n\t\t\tif (ret) {\n\t\t\t\tclear_bit(RDS_MSG_MAPPED, &rm->m_flags);\n\t\t\t\twake_up_interruptible(&rm->m_flush_wait);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcp->cp_xmit_rdma_sent = 1;\n\n\t\t}\n\n\t\tif (rm->atomic.op_active && !cp->cp_xmit_atomic_sent) {\n\t\t\trm->m_final_op = &rm->atomic;\n\t\t\t \n\t\t\tset_bit(RDS_MSG_MAPPED, &rm->m_flags);\n\t\t\tret = conn->c_trans->xmit_atomic(conn, &rm->atomic);\n\t\t\tif (ret) {\n\t\t\t\tclear_bit(RDS_MSG_MAPPED, &rm->m_flags);\n\t\t\t\twake_up_interruptible(&rm->m_flush_wait);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcp->cp_xmit_atomic_sent = 1;\n\n\t\t}\n\n\t\t \n\t\tif (rm->data.op_nents == 0) {\n\t\t\tint ops_present;\n\t\t\tint all_ops_are_silent = 1;\n\n\t\t\tops_present = (rm->atomic.op_active || rm->rdma.op_active);\n\t\t\tif (rm->atomic.op_active && !rm->atomic.op_silent)\n\t\t\t\tall_ops_are_silent = 0;\n\t\t\tif (rm->rdma.op_active && !rm->rdma.op_silent)\n\t\t\t\tall_ops_are_silent = 0;\n\n\t\t\tif (ops_present && all_ops_are_silent\n\t\t\t    && !rm->m_rdma_cookie)\n\t\t\t\trm->data.op_active = 0;\n\t\t}\n\n\t\tif (rm->data.op_active && !cp->cp_xmit_data_sent) {\n\t\t\trm->m_final_op = &rm->data;\n\n\t\t\tret = conn->c_trans->xmit(conn, rm,\n\t\t\t\t\t\t  cp->cp_xmit_hdr_off,\n\t\t\t\t\t\t  cp->cp_xmit_sg,\n\t\t\t\t\t\t  cp->cp_xmit_data_off);\n\t\t\tif (ret <= 0)\n\t\t\t\tbreak;\n\n\t\t\tif (cp->cp_xmit_hdr_off < sizeof(struct rds_header)) {\n\t\t\t\ttmp = min_t(int, ret,\n\t\t\t\t\t    sizeof(struct rds_header) -\n\t\t\t\t\t    cp->cp_xmit_hdr_off);\n\t\t\t\tcp->cp_xmit_hdr_off += tmp;\n\t\t\t\tret -= tmp;\n\t\t\t}\n\n\t\t\tsg = &rm->data.op_sg[cp->cp_xmit_sg];\n\t\t\twhile (ret) {\n\t\t\t\ttmp = min_t(int, ret, sg->length -\n\t\t\t\t\t\t      cp->cp_xmit_data_off);\n\t\t\t\tcp->cp_xmit_data_off += tmp;\n\t\t\t\tret -= tmp;\n\t\t\t\tif (cp->cp_xmit_data_off == sg->length) {\n\t\t\t\t\tcp->cp_xmit_data_off = 0;\n\t\t\t\t\tsg++;\n\t\t\t\t\tcp->cp_xmit_sg++;\n\t\t\t\t\tBUG_ON(ret != 0 && cp->cp_xmit_sg ==\n\t\t\t\t\t       rm->data.op_nents);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (cp->cp_xmit_hdr_off == sizeof(struct rds_header) &&\n\t\t\t    (cp->cp_xmit_sg == rm->data.op_nents))\n\t\t\t\tcp->cp_xmit_data_sent = 1;\n\t\t}\n\n\t\t \n\t\tif (!rm->data.op_active || cp->cp_xmit_data_sent) {\n\t\t\tcp->cp_xmit_rm = NULL;\n\t\t\tcp->cp_xmit_sg = 0;\n\t\t\tcp->cp_xmit_hdr_off = 0;\n\t\t\tcp->cp_xmit_data_off = 0;\n\t\t\tcp->cp_xmit_rdma_sent = 0;\n\t\t\tcp->cp_xmit_atomic_sent = 0;\n\t\t\tcp->cp_xmit_data_sent = 0;\n\n\t\t\trds_message_put(rm);\n\t\t}\n\t}\n\nover_batch:\n\tif (conn->c_trans->xmit_path_complete)\n\t\tconn->c_trans->xmit_path_complete(cp);\n\trelease_in_xmit(cp);\n\n\t \n\tif (!list_empty(&to_be_dropped)) {\n\t\t \n\t\tlist_for_each_entry(rm, &to_be_dropped, m_conn_item)\n\t\t\trds_message_put(rm);\n\t\trds_send_remove_from_sock(&to_be_dropped, RDS_RDMA_DROPPED);\n\t}\n\n\t \n\tif (ret == 0) {\n\t\tbool raced;\n\n\t\tsmp_mb();\n\t\traced = send_gen != READ_ONCE(cp->cp_send_gen);\n\n\t\tif ((test_bit(0, &conn->c_map_queued) ||\n\t\t    !list_empty(&cp->cp_send_queue)) && !raced) {\n\t\t\tif (batch_count < send_batch_count)\n\t\t\t\tgoto restart;\n\t\t\trcu_read_lock();\n\t\t\tif (rds_destroy_pending(cp->cp_conn))\n\t\t\t\tret = -ENETUNREACH;\n\t\t\telse\n\t\t\t\tqueue_delayed_work(rds_wq, &cp->cp_send_w, 1);\n\t\t\trcu_read_unlock();\n\t\t} else if (raced) {\n\t\t\trds_stats_inc(s_send_lock_queue_raced);\n\t\t}\n\t}\nout:\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(rds_send_xmit);\n\nstatic void rds_send_sndbuf_remove(struct rds_sock *rs, struct rds_message *rm)\n{\n\tu32 len = be32_to_cpu(rm->m_inc.i_hdr.h_len);\n\n\tassert_spin_locked(&rs->rs_lock);\n\n\tBUG_ON(rs->rs_snd_bytes < len);\n\trs->rs_snd_bytes -= len;\n\n\tif (rs->rs_snd_bytes == 0)\n\t\trds_stats_inc(s_send_queue_empty);\n}\n\nstatic inline int rds_send_is_acked(struct rds_message *rm, u64 ack,\n\t\t\t\t    is_acked_func is_acked)\n{\n\tif (is_acked)\n\t\treturn is_acked(rm, ack);\n\treturn be64_to_cpu(rm->m_inc.i_hdr.h_sequence) <= ack;\n}\n\n \nvoid rds_rdma_send_complete(struct rds_message *rm, int status)\n{\n\tstruct rds_sock *rs = NULL;\n\tstruct rm_rdma_op *ro;\n\tstruct rds_notifier *notifier;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&rm->m_rs_lock, flags);\n\n\tro = &rm->rdma;\n\tif (test_bit(RDS_MSG_ON_SOCK, &rm->m_flags) &&\n\t    ro->op_active && ro->op_notify && ro->op_notifier) {\n\t\tnotifier = ro->op_notifier;\n\t\trs = rm->m_rs;\n\t\tsock_hold(rds_rs_to_sk(rs));\n\n\t\tnotifier->n_status = status;\n\t\tspin_lock(&rs->rs_lock);\n\t\tlist_add_tail(&notifier->n_list, &rs->rs_notify_queue);\n\t\tspin_unlock(&rs->rs_lock);\n\n\t\tro->op_notifier = NULL;\n\t}\n\n\tspin_unlock_irqrestore(&rm->m_rs_lock, flags);\n\n\tif (rs) {\n\t\trds_wake_sk_sleep(rs);\n\t\tsock_put(rds_rs_to_sk(rs));\n\t}\n}\nEXPORT_SYMBOL_GPL(rds_rdma_send_complete);\n\n \nvoid rds_atomic_send_complete(struct rds_message *rm, int status)\n{\n\tstruct rds_sock *rs = NULL;\n\tstruct rm_atomic_op *ao;\n\tstruct rds_notifier *notifier;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&rm->m_rs_lock, flags);\n\n\tao = &rm->atomic;\n\tif (test_bit(RDS_MSG_ON_SOCK, &rm->m_flags)\n\t    && ao->op_active && ao->op_notify && ao->op_notifier) {\n\t\tnotifier = ao->op_notifier;\n\t\trs = rm->m_rs;\n\t\tsock_hold(rds_rs_to_sk(rs));\n\n\t\tnotifier->n_status = status;\n\t\tspin_lock(&rs->rs_lock);\n\t\tlist_add_tail(&notifier->n_list, &rs->rs_notify_queue);\n\t\tspin_unlock(&rs->rs_lock);\n\n\t\tao->op_notifier = NULL;\n\t}\n\n\tspin_unlock_irqrestore(&rm->m_rs_lock, flags);\n\n\tif (rs) {\n\t\trds_wake_sk_sleep(rs);\n\t\tsock_put(rds_rs_to_sk(rs));\n\t}\n}\nEXPORT_SYMBOL_GPL(rds_atomic_send_complete);\n\n \nstatic inline void\n__rds_send_complete(struct rds_sock *rs, struct rds_message *rm, int status)\n{\n\tstruct rm_rdma_op *ro;\n\tstruct rm_atomic_op *ao;\n\n\tro = &rm->rdma;\n\tif (ro->op_active && ro->op_notify && ro->op_notifier) {\n\t\tro->op_notifier->n_status = status;\n\t\tlist_add_tail(&ro->op_notifier->n_list, &rs->rs_notify_queue);\n\t\tro->op_notifier = NULL;\n\t}\n\n\tao = &rm->atomic;\n\tif (ao->op_active && ao->op_notify && ao->op_notifier) {\n\t\tao->op_notifier->n_status = status;\n\t\tlist_add_tail(&ao->op_notifier->n_list, &rs->rs_notify_queue);\n\t\tao->op_notifier = NULL;\n\t}\n\n\t \n}\n\n \nstatic void rds_send_remove_from_sock(struct list_head *messages, int status)\n{\n\tunsigned long flags;\n\tstruct rds_sock *rs = NULL;\n\tstruct rds_message *rm;\n\n\twhile (!list_empty(messages)) {\n\t\tint was_on_sock = 0;\n\n\t\trm = list_entry(messages->next, struct rds_message,\n\t\t\t\tm_conn_item);\n\t\tlist_del_init(&rm->m_conn_item);\n\n\t\t \n\t\tspin_lock_irqsave(&rm->m_rs_lock, flags);\n\t\tif (!test_bit(RDS_MSG_ON_SOCK, &rm->m_flags))\n\t\t\tgoto unlock_and_drop;\n\n\t\tif (rs != rm->m_rs) {\n\t\t\tif (rs) {\n\t\t\t\trds_wake_sk_sleep(rs);\n\t\t\t\tsock_put(rds_rs_to_sk(rs));\n\t\t\t}\n\t\t\trs = rm->m_rs;\n\t\t\tif (rs)\n\t\t\t\tsock_hold(rds_rs_to_sk(rs));\n\t\t}\n\t\tif (!rs)\n\t\t\tgoto unlock_and_drop;\n\t\tspin_lock(&rs->rs_lock);\n\n\t\tif (test_and_clear_bit(RDS_MSG_ON_SOCK, &rm->m_flags)) {\n\t\t\tstruct rm_rdma_op *ro = &rm->rdma;\n\t\t\tstruct rds_notifier *notifier;\n\n\t\t\tlist_del_init(&rm->m_sock_item);\n\t\t\trds_send_sndbuf_remove(rs, rm);\n\n\t\t\tif (ro->op_active && ro->op_notifier &&\n\t\t\t       (ro->op_notify || (ro->op_recverr && status))) {\n\t\t\t\tnotifier = ro->op_notifier;\n\t\t\t\tlist_add_tail(&notifier->n_list,\n\t\t\t\t\t\t&rs->rs_notify_queue);\n\t\t\t\tif (!notifier->n_status)\n\t\t\t\t\tnotifier->n_status = status;\n\t\t\t\trm->rdma.op_notifier = NULL;\n\t\t\t}\n\t\t\twas_on_sock = 1;\n\t\t}\n\t\tspin_unlock(&rs->rs_lock);\n\nunlock_and_drop:\n\t\tspin_unlock_irqrestore(&rm->m_rs_lock, flags);\n\t\trds_message_put(rm);\n\t\tif (was_on_sock)\n\t\t\trds_message_put(rm);\n\t}\n\n\tif (rs) {\n\t\trds_wake_sk_sleep(rs);\n\t\tsock_put(rds_rs_to_sk(rs));\n\t}\n}\n\n \nvoid rds_send_path_drop_acked(struct rds_conn_path *cp, u64 ack,\n\t\t\t      is_acked_func is_acked)\n{\n\tstruct rds_message *rm, *tmp;\n\tunsigned long flags;\n\tLIST_HEAD(list);\n\n\tspin_lock_irqsave(&cp->cp_lock, flags);\n\n\tlist_for_each_entry_safe(rm, tmp, &cp->cp_retrans, m_conn_item) {\n\t\tif (!rds_send_is_acked(rm, ack, is_acked))\n\t\t\tbreak;\n\n\t\tlist_move(&rm->m_conn_item, &list);\n\t\tclear_bit(RDS_MSG_ON_CONN, &rm->m_flags);\n\t}\n\n\t \n\tif (!list_empty(&list))\n\t\tsmp_mb__after_atomic();\n\n\tspin_unlock_irqrestore(&cp->cp_lock, flags);\n\n\t \n\trds_send_remove_from_sock(&list, RDS_RDMA_SUCCESS);\n}\nEXPORT_SYMBOL_GPL(rds_send_path_drop_acked);\n\nvoid rds_send_drop_acked(struct rds_connection *conn, u64 ack,\n\t\t\t is_acked_func is_acked)\n{\n\tWARN_ON(conn->c_trans->t_mp_capable);\n\trds_send_path_drop_acked(&conn->c_path[0], ack, is_acked);\n}\nEXPORT_SYMBOL_GPL(rds_send_drop_acked);\n\nvoid rds_send_drop_to(struct rds_sock *rs, struct sockaddr_in6 *dest)\n{\n\tstruct rds_message *rm, *tmp;\n\tstruct rds_connection *conn;\n\tstruct rds_conn_path *cp;\n\tunsigned long flags;\n\tLIST_HEAD(list);\n\n\t \n\tspin_lock_irqsave(&rs->rs_lock, flags);\n\n\tlist_for_each_entry_safe(rm, tmp, &rs->rs_send_queue, m_sock_item) {\n\t\tif (dest &&\n\t\t    (!ipv6_addr_equal(&dest->sin6_addr, &rm->m_daddr) ||\n\t\t     dest->sin6_port != rm->m_inc.i_hdr.h_dport))\n\t\t\tcontinue;\n\n\t\tlist_move(&rm->m_sock_item, &list);\n\t\trds_send_sndbuf_remove(rs, rm);\n\t\tclear_bit(RDS_MSG_ON_SOCK, &rm->m_flags);\n\t}\n\n\t \n\tsmp_mb__after_atomic();\n\n\tspin_unlock_irqrestore(&rs->rs_lock, flags);\n\n\tif (list_empty(&list))\n\t\treturn;\n\n\t \n\tlist_for_each_entry(rm, &list, m_sock_item) {\n\n\t\tconn = rm->m_inc.i_conn;\n\t\tif (conn->c_trans->t_mp_capable)\n\t\t\tcp = rm->m_inc.i_conn_path;\n\t\telse\n\t\t\tcp = &conn->c_path[0];\n\n\t\tspin_lock_irqsave(&cp->cp_lock, flags);\n\t\t \n\t\tif (!test_and_clear_bit(RDS_MSG_ON_CONN, &rm->m_flags)) {\n\t\t\tspin_unlock_irqrestore(&cp->cp_lock, flags);\n\t\t\tcontinue;\n\t\t}\n\t\tlist_del_init(&rm->m_conn_item);\n\t\tspin_unlock_irqrestore(&cp->cp_lock, flags);\n\n\t\t \n\t\tspin_lock_irqsave(&rm->m_rs_lock, flags);\n\n\t\tspin_lock(&rs->rs_lock);\n\t\t__rds_send_complete(rs, rm, RDS_RDMA_CANCELED);\n\t\tspin_unlock(&rs->rs_lock);\n\n\t\tspin_unlock_irqrestore(&rm->m_rs_lock, flags);\n\n\t\trds_message_put(rm);\n\t}\n\n\trds_wake_sk_sleep(rs);\n\n\twhile (!list_empty(&list)) {\n\t\trm = list_entry(list.next, struct rds_message, m_sock_item);\n\t\tlist_del_init(&rm->m_sock_item);\n\t\trds_message_wait(rm);\n\n\t\t \n\t\tspin_lock_irqsave(&rm->m_rs_lock, flags);\n\n\t\tspin_lock(&rs->rs_lock);\n\t\t__rds_send_complete(rs, rm, RDS_RDMA_CANCELED);\n\t\tspin_unlock(&rs->rs_lock);\n\n\t\tspin_unlock_irqrestore(&rm->m_rs_lock, flags);\n\n\t\trds_message_put(rm);\n\t}\n}\n\n \nstatic int rds_send_queue_rm(struct rds_sock *rs, struct rds_connection *conn,\n\t\t\t     struct rds_conn_path *cp,\n\t\t\t     struct rds_message *rm, __be16 sport,\n\t\t\t     __be16 dport, int *queued)\n{\n\tunsigned long flags;\n\tu32 len;\n\n\tif (*queued)\n\t\tgoto out;\n\n\tlen = be32_to_cpu(rm->m_inc.i_hdr.h_len);\n\n\t \n\tspin_lock_irqsave(&rs->rs_lock, flags);\n\n\t \n\tif (rs->rs_snd_bytes < rds_sk_sndbuf(rs)) {\n\t\trs->rs_snd_bytes += len;\n\n\t\t \n\t\tif (rs->rs_snd_bytes >= rds_sk_sndbuf(rs) / 2)\n\t\t\tset_bit(RDS_MSG_ACK_REQUIRED, &rm->m_flags);\n\n\t\tlist_add_tail(&rm->m_sock_item, &rs->rs_send_queue);\n\t\tset_bit(RDS_MSG_ON_SOCK, &rm->m_flags);\n\t\trds_message_addref(rm);\n\t\tsock_hold(rds_rs_to_sk(rs));\n\t\trm->m_rs = rs;\n\n\t\t \n\t\trds_message_populate_header(&rm->m_inc.i_hdr, sport, dport, 0);\n\t\trm->m_inc.i_conn = conn;\n\t\trm->m_inc.i_conn_path = cp;\n\t\trds_message_addref(rm);\n\n\t\tspin_lock(&cp->cp_lock);\n\t\trm->m_inc.i_hdr.h_sequence = cpu_to_be64(cp->cp_next_tx_seq++);\n\t\tlist_add_tail(&rm->m_conn_item, &cp->cp_send_queue);\n\t\tset_bit(RDS_MSG_ON_CONN, &rm->m_flags);\n\t\tspin_unlock(&cp->cp_lock);\n\n\t\trdsdebug(\"queued msg %p len %d, rs %p bytes %d seq %llu\\n\",\n\t\t\t rm, len, rs, rs->rs_snd_bytes,\n\t\t\t (unsigned long long)be64_to_cpu(rm->m_inc.i_hdr.h_sequence));\n\n\t\t*queued = 1;\n\t}\n\n\tspin_unlock_irqrestore(&rs->rs_lock, flags);\nout:\n\treturn *queued;\n}\n\n \nstatic int rds_rm_size(struct msghdr *msg, int num_sgs,\n\t\t       struct rds_iov_vector_arr *vct)\n{\n\tstruct cmsghdr *cmsg;\n\tint size = 0;\n\tint cmsg_groups = 0;\n\tint retval;\n\tbool zcopy_cookie = false;\n\tstruct rds_iov_vector *iov, *tmp_iov;\n\n\tif (num_sgs < 0)\n\t\treturn -EINVAL;\n\n\tfor_each_cmsghdr(cmsg, msg) {\n\t\tif (!CMSG_OK(msg, cmsg))\n\t\t\treturn -EINVAL;\n\n\t\tif (cmsg->cmsg_level != SOL_RDS)\n\t\t\tcontinue;\n\n\t\tswitch (cmsg->cmsg_type) {\n\t\tcase RDS_CMSG_RDMA_ARGS:\n\t\t\tif (vct->indx >= vct->len) {\n\t\t\t\tvct->len += vct->incr;\n\t\t\t\ttmp_iov =\n\t\t\t\t\tkrealloc(vct->vec,\n\t\t\t\t\t\t vct->len *\n\t\t\t\t\t\t sizeof(struct rds_iov_vector),\n\t\t\t\t\t\t GFP_KERNEL);\n\t\t\t\tif (!tmp_iov) {\n\t\t\t\t\tvct->len -= vct->incr;\n\t\t\t\t\treturn -ENOMEM;\n\t\t\t\t}\n\t\t\t\tvct->vec = tmp_iov;\n\t\t\t}\n\t\t\tiov = &vct->vec[vct->indx];\n\t\t\tmemset(iov, 0, sizeof(struct rds_iov_vector));\n\t\t\tvct->indx++;\n\t\t\tcmsg_groups |= 1;\n\t\t\tretval = rds_rdma_extra_size(CMSG_DATA(cmsg), iov);\n\t\t\tif (retval < 0)\n\t\t\t\treturn retval;\n\t\t\tsize += retval;\n\n\t\t\tbreak;\n\n\t\tcase RDS_CMSG_ZCOPY_COOKIE:\n\t\t\tzcopy_cookie = true;\n\t\t\tfallthrough;\n\n\t\tcase RDS_CMSG_RDMA_DEST:\n\t\tcase RDS_CMSG_RDMA_MAP:\n\t\t\tcmsg_groups |= 2;\n\t\t\t \n\t\t\tbreak;\n\n\t\tcase RDS_CMSG_ATOMIC_CSWP:\n\t\tcase RDS_CMSG_ATOMIC_FADD:\n\t\tcase RDS_CMSG_MASKED_ATOMIC_CSWP:\n\t\tcase RDS_CMSG_MASKED_ATOMIC_FADD:\n\t\t\tcmsg_groups |= 1;\n\t\t\tsize += sizeof(struct scatterlist);\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t}\n\n\tif ((msg->msg_flags & MSG_ZEROCOPY) && !zcopy_cookie)\n\t\treturn -EINVAL;\n\n\tsize += num_sgs * sizeof(struct scatterlist);\n\n\t \n\tif (cmsg_groups == 3)\n\t\treturn -EINVAL;\n\n\treturn size;\n}\n\nstatic int rds_cmsg_zcopy(struct rds_sock *rs, struct rds_message *rm,\n\t\t\t  struct cmsghdr *cmsg)\n{\n\tu32 *cookie;\n\n\tif (cmsg->cmsg_len < CMSG_LEN(sizeof(*cookie)) ||\n\t    !rm->data.op_mmp_znotifier)\n\t\treturn -EINVAL;\n\tcookie = CMSG_DATA(cmsg);\n\trm->data.op_mmp_znotifier->z_cookie = *cookie;\n\treturn 0;\n}\n\nstatic int rds_cmsg_send(struct rds_sock *rs, struct rds_message *rm,\n\t\t\t struct msghdr *msg, int *allocated_mr,\n\t\t\t struct rds_iov_vector_arr *vct)\n{\n\tstruct cmsghdr *cmsg;\n\tint ret = 0, ind = 0;\n\n\tfor_each_cmsghdr(cmsg, msg) {\n\t\tif (!CMSG_OK(msg, cmsg))\n\t\t\treturn -EINVAL;\n\n\t\tif (cmsg->cmsg_level != SOL_RDS)\n\t\t\tcontinue;\n\n\t\t \n\t\tswitch (cmsg->cmsg_type) {\n\t\tcase RDS_CMSG_RDMA_ARGS:\n\t\t\tif (ind >= vct->indx)\n\t\t\t\treturn -ENOMEM;\n\t\t\tret = rds_cmsg_rdma_args(rs, rm, cmsg, &vct->vec[ind]);\n\t\t\tind++;\n\t\t\tbreak;\n\n\t\tcase RDS_CMSG_RDMA_DEST:\n\t\t\tret = rds_cmsg_rdma_dest(rs, rm, cmsg);\n\t\t\tbreak;\n\n\t\tcase RDS_CMSG_RDMA_MAP:\n\t\t\tret = rds_cmsg_rdma_map(rs, rm, cmsg);\n\t\t\tif (!ret)\n\t\t\t\t*allocated_mr = 1;\n\t\t\telse if (ret == -ENODEV)\n\t\t\t\t \n\t\t\t\tret = -EAGAIN;\n\t\t\tbreak;\n\t\tcase RDS_CMSG_ATOMIC_CSWP:\n\t\tcase RDS_CMSG_ATOMIC_FADD:\n\t\tcase RDS_CMSG_MASKED_ATOMIC_CSWP:\n\t\tcase RDS_CMSG_MASKED_ATOMIC_FADD:\n\t\t\tret = rds_cmsg_atomic(rs, rm, cmsg);\n\t\t\tbreak;\n\n\t\tcase RDS_CMSG_ZCOPY_COOKIE:\n\t\t\tret = rds_cmsg_zcopy(rs, rm, cmsg);\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic int rds_send_mprds_hash(struct rds_sock *rs,\n\t\t\t       struct rds_connection *conn, int nonblock)\n{\n\tint hash;\n\n\tif (conn->c_npaths == 0)\n\t\thash = RDS_MPATH_HASH(rs, RDS_MPATH_WORKERS);\n\telse\n\t\thash = RDS_MPATH_HASH(rs, conn->c_npaths);\n\tif (conn->c_npaths == 0 && hash != 0) {\n\t\trds_send_ping(conn, 0);\n\n\t\t \n\t\tif (conn->c_npaths == 0) {\n\t\t\t \n\t\t\tif (nonblock)\n\t\t\t\treturn 0;\n\t\t\tif (wait_event_interruptible(conn->c_hs_waitq,\n\t\t\t\t\t\t     conn->c_npaths != 0))\n\t\t\t\thash = 0;\n\t\t}\n\t\tif (conn->c_npaths == 1)\n\t\t\thash = 0;\n\t}\n\treturn hash;\n}\n\nstatic int rds_rdma_bytes(struct msghdr *msg, size_t *rdma_bytes)\n{\n\tstruct rds_rdma_args *args;\n\tstruct cmsghdr *cmsg;\n\n\tfor_each_cmsghdr(cmsg, msg) {\n\t\tif (!CMSG_OK(msg, cmsg))\n\t\t\treturn -EINVAL;\n\n\t\tif (cmsg->cmsg_level != SOL_RDS)\n\t\t\tcontinue;\n\n\t\tif (cmsg->cmsg_type == RDS_CMSG_RDMA_ARGS) {\n\t\t\tif (cmsg->cmsg_len <\n\t\t\t    CMSG_LEN(sizeof(struct rds_rdma_args)))\n\t\t\t\treturn -EINVAL;\n\t\t\targs = CMSG_DATA(cmsg);\n\t\t\t*rdma_bytes += args->remote_vec.bytes;\n\t\t}\n\t}\n\treturn 0;\n}\n\nint rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct rds_sock *rs = rds_sk_to_rs(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\tDECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);\n\t__be16 dport;\n\tstruct rds_message *rm = NULL;\n\tstruct rds_connection *conn;\n\tint ret = 0;\n\tint queued = 0, allocated_mr = 0;\n\tint nonblock = msg->msg_flags & MSG_DONTWAIT;\n\tlong timeo = sock_sndtimeo(sk, nonblock);\n\tstruct rds_conn_path *cpath;\n\tstruct in6_addr daddr;\n\t__u32 scope_id = 0;\n\tsize_t rdma_payload_len = 0;\n\tbool zcopy = ((msg->msg_flags & MSG_ZEROCOPY) &&\n\t\t      sock_flag(rds_rs_to_sk(rs), SOCK_ZEROCOPY));\n\tint num_sgs = DIV_ROUND_UP(payload_len, PAGE_SIZE);\n\tint namelen;\n\tstruct rds_iov_vector_arr vct;\n\tint ind;\n\n\tmemset(&vct, 0, sizeof(vct));\n\n\t \n\tvct.incr = 1;\n\n\t \n\t \n\tif (msg->msg_flags & ~(MSG_DONTWAIT | MSG_CMSG_COMPAT | MSG_ZEROCOPY)) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\tnamelen = msg->msg_namelen;\n\tif (namelen != 0) {\n\t\tif (namelen < sizeof(*usin)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tswitch (usin->sin_family) {\n\t\tcase AF_INET:\n\t\t\tif (usin->sin_addr.s_addr == htonl(INADDR_ANY) ||\n\t\t\t    usin->sin_addr.s_addr == htonl(INADDR_BROADCAST) ||\n\t\t\t    ipv4_is_multicast(usin->sin_addr.s_addr)) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tipv6_addr_set_v4mapped(usin->sin_addr.s_addr, &daddr);\n\t\t\tdport = usin->sin_port;\n\t\t\tbreak;\n\n#if IS_ENABLED(CONFIG_IPV6)\n\t\tcase AF_INET6: {\n\t\t\tint addr_type;\n\n\t\t\tif (namelen < sizeof(*sin6)) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\taddr_type = ipv6_addr_type(&sin6->sin6_addr);\n\t\t\tif (!(addr_type & IPV6_ADDR_UNICAST)) {\n\t\t\t\t__be32 addr4;\n\n\t\t\t\tif (!(addr_type & IPV6_ADDR_MAPPED)) {\n\t\t\t\t\tret = -EINVAL;\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\n\t\t\t\t \n\t\t\t\taddr4 = sin6->sin6_addr.s6_addr32[3];\n\t\t\t\tif (addr4 == htonl(INADDR_ANY) ||\n\t\t\t\t    addr4 == htonl(INADDR_BROADCAST) ||\n\t\t\t\t    ipv4_is_multicast(addr4)) {\n\t\t\t\t\tret = -EINVAL;\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\t\t\tif (sin6->sin6_scope_id == 0) {\n\t\t\t\t\tret = -EINVAL;\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t\tscope_id = sin6->sin6_scope_id;\n\t\t\t}\n\n\t\t\tdaddr = sin6->sin6_addr;\n\t\t\tdport = sin6->sin6_port;\n\t\t\tbreak;\n\t\t}\n#endif\n\n\t\tdefault:\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\t \n\t\tlock_sock(sk);\n\t\tdaddr = rs->rs_conn_addr;\n\t\tdport = rs->rs_conn_port;\n\t\tscope_id = rs->rs_bound_scope_id;\n\t\trelease_sock(sk);\n\t}\n\n\tlock_sock(sk);\n\tif (ipv6_addr_any(&rs->rs_bound_addr) || ipv6_addr_any(&daddr)) {\n\t\trelease_sock(sk);\n\t\tret = -ENOTCONN;\n\t\tgoto out;\n\t} else if (namelen != 0) {\n\t\t \n\t\tif (ipv6_addr_v4mapped(&daddr) ^\n\t\t    ipv6_addr_v4mapped(&rs->rs_bound_addr)) {\n\t\t\trelease_sock(sk);\n\t\t\tret = -EOPNOTSUPP;\n\t\t\tgoto out;\n\t\t}\n\t\t \n\t\tif (scope_id != rs->rs_bound_scope_id) {\n\t\t\tif (!scope_id) {\n\t\t\t\tscope_id = rs->rs_bound_scope_id;\n\t\t\t} else if (rs->rs_bound_scope_id) {\n\t\t\t\trelease_sock(sk);\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\trelease_sock(sk);\n\n\tret = rds_rdma_bytes(msg, &rdma_payload_len);\n\tif (ret)\n\t\tgoto out;\n\n\tif (max_t(size_t, payload_len, rdma_payload_len) > RDS_MAX_MSG_SIZE) {\n\t\tret = -EMSGSIZE;\n\t\tgoto out;\n\t}\n\n\tif (payload_len > rds_sk_sndbuf(rs)) {\n\t\tret = -EMSGSIZE;\n\t\tgoto out;\n\t}\n\n\tif (zcopy) {\n\t\tif (rs->rs_transport->t_type != RDS_TRANS_TCP) {\n\t\t\tret = -EOPNOTSUPP;\n\t\t\tgoto out;\n\t\t}\n\t\tnum_sgs = iov_iter_npages(&msg->msg_iter, INT_MAX);\n\t}\n\t \n\tret = rds_rm_size(msg, num_sgs, &vct);\n\tif (ret < 0)\n\t\tgoto out;\n\n\trm = rds_message_alloc(ret, GFP_KERNEL);\n\tif (!rm) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\t \n\tif (payload_len) {\n\t\trm->data.op_sg = rds_message_alloc_sgs(rm, num_sgs);\n\t\tif (IS_ERR(rm->data.op_sg)) {\n\t\t\tret = PTR_ERR(rm->data.op_sg);\n\t\t\tgoto out;\n\t\t}\n\t\tret = rds_message_copy_from_user(rm, &msg->msg_iter, zcopy);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\trm->data.op_active = 1;\n\n\trm->m_daddr = daddr;\n\n\t \n\tif (rs->rs_conn && ipv6_addr_equal(&rs->rs_conn->c_faddr, &daddr) &&\n\t    rs->rs_tos == rs->rs_conn->c_tos) {\n\t\tconn = rs->rs_conn;\n\t} else {\n\t\tconn = rds_conn_create_outgoing(sock_net(sock->sk),\n\t\t\t\t\t\t&rs->rs_bound_addr, &daddr,\n\t\t\t\t\t\trs->rs_transport, rs->rs_tos,\n\t\t\t\t\t\tsock->sk->sk_allocation,\n\t\t\t\t\t\tscope_id);\n\t\tif (IS_ERR(conn)) {\n\t\t\tret = PTR_ERR(conn);\n\t\t\tgoto out;\n\t\t}\n\t\trs->rs_conn = conn;\n\t}\n\n\tif (conn->c_trans->t_mp_capable)\n\t\tcpath = &conn->c_path[rds_send_mprds_hash(rs, conn, nonblock)];\n\telse\n\t\tcpath = &conn->c_path[0];\n\n\trm->m_conn_path = cpath;\n\n\t \n\tret = rds_cmsg_send(rs, rm, msg, &allocated_mr, &vct);\n\tif (ret) {\n\t\t \n\t\tif (ret ==  -EAGAIN)\n\t\t\trds_conn_connect_if_down(conn);\n\t\tgoto out;\n\t}\n\n\tif (rm->rdma.op_active && !conn->c_trans->xmit_rdma) {\n\t\tprintk_ratelimited(KERN_NOTICE \"rdma_op %p conn xmit_rdma %p\\n\",\n\t\t\t       &rm->rdma, conn->c_trans->xmit_rdma);\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\tif (rm->atomic.op_active && !conn->c_trans->xmit_atomic) {\n\t\tprintk_ratelimited(KERN_NOTICE \"atomic_op %p conn xmit_atomic %p\\n\",\n\t\t\t       &rm->atomic, conn->c_trans->xmit_atomic);\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\tif (rds_destroy_pending(conn)) {\n\t\tret = -EAGAIN;\n\t\tgoto out;\n\t}\n\n\tif (rds_conn_path_down(cpath))\n\t\trds_check_all_paths(conn);\n\n\tret = rds_cong_wait(conn->c_fcong, dport, nonblock, rs);\n\tif (ret) {\n\t\trs->rs_seen_congestion = 1;\n\t\tgoto out;\n\t}\n\twhile (!rds_send_queue_rm(rs, conn, cpath, rm, rs->rs_bound_port,\n\t\t\t\t  dport, &queued)) {\n\t\trds_stats_inc(s_send_queue_full);\n\n\t\tif (nonblock) {\n\t\t\tret = -EAGAIN;\n\t\t\tgoto out;\n\t\t}\n\n\t\ttimeo = wait_event_interruptible_timeout(*sk_sleep(sk),\n\t\t\t\t\trds_send_queue_rm(rs, conn, cpath, rm,\n\t\t\t\t\t\t\t  rs->rs_bound_port,\n\t\t\t\t\t\t\t  dport,\n\t\t\t\t\t\t\t  &queued),\n\t\t\t\t\ttimeo);\n\t\trdsdebug(\"sendmsg woke queued %d timeo %ld\\n\", queued, timeo);\n\t\tif (timeo > 0 || timeo == MAX_SCHEDULE_TIMEOUT)\n\t\t\tcontinue;\n\n\t\tret = timeo;\n\t\tif (ret == 0)\n\t\t\tret = -ETIMEDOUT;\n\t\tgoto out;\n\t}\n\n\t \n\trds_stats_inc(s_send_queued);\n\n\tret = rds_send_xmit(cpath);\n\tif (ret == -ENOMEM || ret == -EAGAIN) {\n\t\tret = 0;\n\t\trcu_read_lock();\n\t\tif (rds_destroy_pending(cpath->cp_conn))\n\t\t\tret = -ENETUNREACH;\n\t\telse\n\t\t\tqueue_delayed_work(rds_wq, &cpath->cp_send_w, 1);\n\t\trcu_read_unlock();\n\t}\n\tif (ret)\n\t\tgoto out;\n\trds_message_put(rm);\n\n\tfor (ind = 0; ind < vct.indx; ind++)\n\t\tkfree(vct.vec[ind].iov);\n\tkfree(vct.vec);\n\n\treturn payload_len;\n\nout:\n\tfor (ind = 0; ind < vct.indx; ind++)\n\t\tkfree(vct.vec[ind].iov);\n\tkfree(vct.vec);\n\n\t \n\tif (allocated_mr)\n\t\trds_rdma_unuse(rs, rds_rdma_cookie_key(rm->m_rdma_cookie), 1);\n\n\tif (rm)\n\t\trds_message_put(rm);\n\treturn ret;\n}\n\n \nstatic int\nrds_send_probe(struct rds_conn_path *cp, __be16 sport,\n\t       __be16 dport, u8 h_flags)\n{\n\tstruct rds_message *rm;\n\tunsigned long flags;\n\tint ret = 0;\n\n\trm = rds_message_alloc(0, GFP_ATOMIC);\n\tif (!rm) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\trm->m_daddr = cp->cp_conn->c_faddr;\n\trm->data.op_active = 1;\n\n\trds_conn_path_connect_if_down(cp);\n\n\tret = rds_cong_wait(cp->cp_conn->c_fcong, dport, 1, NULL);\n\tif (ret)\n\t\tgoto out;\n\n\tspin_lock_irqsave(&cp->cp_lock, flags);\n\tlist_add_tail(&rm->m_conn_item, &cp->cp_send_queue);\n\tset_bit(RDS_MSG_ON_CONN, &rm->m_flags);\n\trds_message_addref(rm);\n\trm->m_inc.i_conn = cp->cp_conn;\n\trm->m_inc.i_conn_path = cp;\n\n\trds_message_populate_header(&rm->m_inc.i_hdr, sport, dport,\n\t\t\t\t    cp->cp_next_tx_seq);\n\trm->m_inc.i_hdr.h_flags |= h_flags;\n\tcp->cp_next_tx_seq++;\n\n\tif (RDS_HS_PROBE(be16_to_cpu(sport), be16_to_cpu(dport)) &&\n\t    cp->cp_conn->c_trans->t_mp_capable) {\n\t\tu16 npaths = cpu_to_be16(RDS_MPATH_WORKERS);\n\t\tu32 my_gen_num = cpu_to_be32(cp->cp_conn->c_my_gen_num);\n\n\t\trds_message_add_extension(&rm->m_inc.i_hdr,\n\t\t\t\t\t  RDS_EXTHDR_NPATHS, &npaths,\n\t\t\t\t\t  sizeof(npaths));\n\t\trds_message_add_extension(&rm->m_inc.i_hdr,\n\t\t\t\t\t  RDS_EXTHDR_GEN_NUM,\n\t\t\t\t\t  &my_gen_num,\n\t\t\t\t\t  sizeof(u32));\n\t}\n\tspin_unlock_irqrestore(&cp->cp_lock, flags);\n\n\trds_stats_inc(s_send_queued);\n\trds_stats_inc(s_send_pong);\n\n\t \n\trcu_read_lock();\n\tif (!rds_destroy_pending(cp->cp_conn))\n\t\tqueue_delayed_work(rds_wq, &cp->cp_send_w, 1);\n\trcu_read_unlock();\n\n\trds_message_put(rm);\n\treturn 0;\n\nout:\n\tif (rm)\n\t\trds_message_put(rm);\n\treturn ret;\n}\n\nint\nrds_send_pong(struct rds_conn_path *cp, __be16 dport)\n{\n\treturn rds_send_probe(cp, 0, dport, 0);\n}\n\nvoid\nrds_send_ping(struct rds_connection *conn, int cp_index)\n{\n\tunsigned long flags;\n\tstruct rds_conn_path *cp = &conn->c_path[cp_index];\n\n\tspin_lock_irqsave(&cp->cp_lock, flags);\n\tif (conn->c_ping_triggered) {\n\t\tspin_unlock_irqrestore(&cp->cp_lock, flags);\n\t\treturn;\n\t}\n\tconn->c_ping_triggered = 1;\n\tspin_unlock_irqrestore(&cp->cp_lock, flags);\n\trds_send_probe(cp, cpu_to_be16(RDS_FLAG_PROBE_PORT), 0, 0);\n}\nEXPORT_SYMBOL_GPL(rds_send_ping);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}