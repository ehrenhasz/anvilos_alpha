{
  "module_name": "af_iucv.c",
  "hash_id": "6dde40491699e401208a4e857c7d46e4b5297c15cef9c5d135285d8d69ce6eca",
  "original_prompt": "Ingested from linux-6.6.14/net/iucv/af_iucv.c",
  "human_readable_source": "\n \n\n#define KMSG_COMPONENT \"af_iucv\"\n#define pr_fmt(fmt) KMSG_COMPONENT \": \" fmt\n\n#include <linux/filter.h>\n#include <linux/module.h>\n#include <linux/netdevice.h>\n#include <linux/types.h>\n#include <linux/limits.h>\n#include <linux/list.h>\n#include <linux/errno.h>\n#include <linux/kernel.h>\n#include <linux/sched/signal.h>\n#include <linux/slab.h>\n#include <linux/skbuff.h>\n#include <linux/init.h>\n#include <linux/poll.h>\n#include <linux/security.h>\n#include <net/sock.h>\n#include <asm/ebcdic.h>\n#include <asm/cpcmd.h>\n#include <linux/kmod.h>\n\n#include <net/iucv/af_iucv.h>\n\n#define VERSION \"1.2\"\n\nstatic char iucv_userid[80];\n\nstatic struct proto iucv_proto = {\n\t.name\t\t= \"AF_IUCV\",\n\t.owner\t\t= THIS_MODULE,\n\t.obj_size\t= sizeof(struct iucv_sock),\n};\n\nstatic struct iucv_interface *pr_iucv;\nstatic struct iucv_handler af_iucv_handler;\n\n \nstatic const u8 iprm_shutdown[8] =\n\t{0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x01};\n\n#define TRGCLS_SIZE\tsizeof_field(struct iucv_message, class)\n\n#define __iucv_sock_wait(sk, condition, timeo, ret)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tDEFINE_WAIT(__wait);\t\t\t\t\t\t\\\n\tlong __timeo = timeo;\t\t\t\t\t\t\\\n\tret = 0;\t\t\t\t\t\t\t\\\n\tprepare_to_wait(sk_sleep(sk), &__wait, TASK_INTERRUPTIBLE);\t\\\n\twhile (!(condition)) {\t\t\t\t\t\t\\\n\t\tif (!__timeo) {\t\t\t\t\t\t\\\n\t\t\tret = -EAGAIN;\t\t\t\t\t\\\n\t\t\tbreak;\t\t\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t\tif (signal_pending(current)) {\t\t\t\t\\\n\t\t\tret = sock_intr_errno(__timeo);\t\t\t\\\n\t\t\tbreak;\t\t\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t\trelease_sock(sk);\t\t\t\t\t\\\n\t\t__timeo = schedule_timeout(__timeo);\t\t\t\\\n\t\tlock_sock(sk);\t\t\t\t\t\t\\\n\t\tret = sock_error(sk);\t\t\t\t\t\\\n\t\tif (ret)\t\t\t\t\t\t\\\n\t\t\tbreak;\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tfinish_wait(sk_sleep(sk), &__wait);\t\t\t\t\\\n} while (0)\n\n#define iucv_sock_wait(sk, condition, timeo)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tint __ret = 0;\t\t\t\t\t\t\t\\\n\tif (!(condition))\t\t\t\t\t\t\\\n\t\t__iucv_sock_wait(sk, condition, timeo, __ret);\t\t\\\n\t__ret;\t\t\t\t\t\t\t\t\\\n})\n\nstatic struct sock *iucv_accept_dequeue(struct sock *parent,\n\t\t\t\t\tstruct socket *newsock);\nstatic void iucv_sock_kill(struct sock *sk);\nstatic void iucv_sock_close(struct sock *sk);\n\nstatic void afiucv_hs_callback_txnotify(struct sock *sk, enum iucv_tx_notify);\n\nstatic struct iucv_sock_list iucv_sk_list = {\n\t.lock = __RW_LOCK_UNLOCKED(iucv_sk_list.lock),\n\t.autobind_name = ATOMIC_INIT(0)\n};\n\nstatic inline void high_nmcpy(unsigned char *dst, char *src)\n{\n       memcpy(dst, src, 8);\n}\n\nstatic inline void low_nmcpy(unsigned char *dst, char *src)\n{\n       memcpy(&dst[8], src, 8);\n}\n\n \nstatic inline size_t iucv_msg_length(struct iucv_message *msg)\n{\n\tsize_t datalen;\n\n\tif (msg->flags & IUCV_IPRMDATA) {\n\t\tdatalen = 0xff - msg->rmmsg[7];\n\t\treturn (datalen < 8) ? datalen : 8;\n\t}\n\treturn msg->length;\n}\n\n \nstatic int iucv_sock_in_state(struct sock *sk, int state, int state2)\n{\n\treturn (sk->sk_state == state || sk->sk_state == state2);\n}\n\n \nstatic inline int iucv_below_msglim(struct sock *sk)\n{\n\tstruct iucv_sock *iucv = iucv_sk(sk);\n\n\tif (sk->sk_state != IUCV_CONNECTED)\n\t\treturn 1;\n\tif (iucv->transport == AF_IUCV_TRANS_IUCV)\n\t\treturn (atomic_read(&iucv->skbs_in_xmit) < iucv->path->msglim);\n\telse\n\t\treturn ((atomic_read(&iucv->msg_sent) < iucv->msglimit_peer) &&\n\t\t\t(atomic_read(&iucv->pendings) <= 0));\n}\n\n \nstatic void iucv_sock_wake_msglim(struct sock *sk)\n{\n\tstruct socket_wq *wq;\n\n\trcu_read_lock();\n\twq = rcu_dereference(sk->sk_wq);\n\tif (skwq_has_sleeper(wq))\n\t\twake_up_interruptible_all(&wq->wait);\n\tsk_wake_async(sk, SOCK_WAKE_SPACE, POLL_OUT);\n\trcu_read_unlock();\n}\n\n \nstatic int afiucv_hs_send(struct iucv_message *imsg, struct sock *sock,\n\t\t   struct sk_buff *skb, u8 flags)\n{\n\tstruct iucv_sock *iucv = iucv_sk(sock);\n\tstruct af_iucv_trans_hdr *phs_hdr;\n\tint err, confirm_recv = 0;\n\n\tphs_hdr = skb_push(skb, sizeof(*phs_hdr));\n\tmemset(phs_hdr, 0, sizeof(*phs_hdr));\n\tskb_reset_network_header(skb);\n\n\tphs_hdr->magic = ETH_P_AF_IUCV;\n\tphs_hdr->version = 1;\n\tphs_hdr->flags = flags;\n\tif (flags == AF_IUCV_FLAG_SYN)\n\t\tphs_hdr->window = iucv->msglimit;\n\telse if ((flags == AF_IUCV_FLAG_WIN) || !flags) {\n\t\tconfirm_recv = atomic_read(&iucv->msg_recv);\n\t\tphs_hdr->window = confirm_recv;\n\t\tif (confirm_recv)\n\t\t\tphs_hdr->flags = phs_hdr->flags | AF_IUCV_FLAG_WIN;\n\t}\n\tmemcpy(phs_hdr->destUserID, iucv->dst_user_id, 8);\n\tmemcpy(phs_hdr->destAppName, iucv->dst_name, 8);\n\tmemcpy(phs_hdr->srcUserID, iucv->src_user_id, 8);\n\tmemcpy(phs_hdr->srcAppName, iucv->src_name, 8);\n\tASCEBC(phs_hdr->destUserID, sizeof(phs_hdr->destUserID));\n\tASCEBC(phs_hdr->destAppName, sizeof(phs_hdr->destAppName));\n\tASCEBC(phs_hdr->srcUserID, sizeof(phs_hdr->srcUserID));\n\tASCEBC(phs_hdr->srcAppName, sizeof(phs_hdr->srcAppName));\n\tif (imsg)\n\t\tmemcpy(&phs_hdr->iucv_hdr, imsg, sizeof(struct iucv_message));\n\n\tskb->dev = iucv->hs_dev;\n\tif (!skb->dev) {\n\t\terr = -ENODEV;\n\t\tgoto err_free;\n\t}\n\n\tdev_hard_header(skb, skb->dev, ETH_P_AF_IUCV, NULL, NULL, skb->len);\n\n\tif (!(skb->dev->flags & IFF_UP) || !netif_carrier_ok(skb->dev)) {\n\t\terr = -ENETDOWN;\n\t\tgoto err_free;\n\t}\n\tif (skb->len > skb->dev->mtu) {\n\t\tif (sock->sk_type == SOCK_SEQPACKET) {\n\t\t\terr = -EMSGSIZE;\n\t\t\tgoto err_free;\n\t\t}\n\t\terr = pskb_trim(skb, skb->dev->mtu);\n\t\tif (err)\n\t\t\tgoto err_free;\n\t}\n\tskb->protocol = cpu_to_be16(ETH_P_AF_IUCV);\n\n\tatomic_inc(&iucv->skbs_in_xmit);\n\terr = dev_queue_xmit(skb);\n\tif (net_xmit_eval(err)) {\n\t\tatomic_dec(&iucv->skbs_in_xmit);\n\t} else {\n\t\tatomic_sub(confirm_recv, &iucv->msg_recv);\n\t\tWARN_ON(atomic_read(&iucv->msg_recv) < 0);\n\t}\n\treturn net_xmit_eval(err);\n\nerr_free:\n\tkfree_skb(skb);\n\treturn err;\n}\n\nstatic struct sock *__iucv_get_sock_by_name(char *nm)\n{\n\tstruct sock *sk;\n\n\tsk_for_each(sk, &iucv_sk_list.head)\n\t\tif (!memcmp(&iucv_sk(sk)->src_name, nm, 8))\n\t\t\treturn sk;\n\n\treturn NULL;\n}\n\nstatic void iucv_sock_destruct(struct sock *sk)\n{\n\tskb_queue_purge(&sk->sk_receive_queue);\n\tskb_queue_purge(&sk->sk_error_queue);\n\n\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\tpr_err(\"Attempt to release alive iucv socket %p\\n\", sk);\n\t\treturn;\n\t}\n\n\tWARN_ON(atomic_read(&sk->sk_rmem_alloc));\n\tWARN_ON(refcount_read(&sk->sk_wmem_alloc));\n\tWARN_ON(sk->sk_wmem_queued);\n\tWARN_ON(sk->sk_forward_alloc);\n}\n\n \nstatic void iucv_sock_cleanup_listen(struct sock *parent)\n{\n\tstruct sock *sk;\n\n\t \n\twhile ((sk = iucv_accept_dequeue(parent, NULL))) {\n\t\tiucv_sock_close(sk);\n\t\tiucv_sock_kill(sk);\n\t}\n\n\tparent->sk_state = IUCV_CLOSED;\n}\n\nstatic void iucv_sock_link(struct iucv_sock_list *l, struct sock *sk)\n{\n\twrite_lock_bh(&l->lock);\n\tsk_add_node(sk, &l->head);\n\twrite_unlock_bh(&l->lock);\n}\n\nstatic void iucv_sock_unlink(struct iucv_sock_list *l, struct sock *sk)\n{\n\twrite_lock_bh(&l->lock);\n\tsk_del_node_init(sk);\n\twrite_unlock_bh(&l->lock);\n}\n\n \nstatic void iucv_sock_kill(struct sock *sk)\n{\n\tif (!sock_flag(sk, SOCK_ZAPPED) || sk->sk_socket)\n\t\treturn;\n\n\tiucv_sock_unlink(&iucv_sk_list, sk);\n\tsock_set_flag(sk, SOCK_DEAD);\n\tsock_put(sk);\n}\n\n \nstatic void iucv_sever_path(struct sock *sk, int with_user_data)\n{\n\tunsigned char user_data[16];\n\tstruct iucv_sock *iucv = iucv_sk(sk);\n\tstruct iucv_path *path = iucv->path;\n\n\tif (iucv->path) {\n\t\tiucv->path = NULL;\n\t\tif (with_user_data) {\n\t\t\tlow_nmcpy(user_data, iucv->src_name);\n\t\t\thigh_nmcpy(user_data, iucv->dst_name);\n\t\t\tASCEBC(user_data, sizeof(user_data));\n\t\t\tpr_iucv->path_sever(path, user_data);\n\t\t} else\n\t\t\tpr_iucv->path_sever(path, NULL);\n\t\tiucv_path_free(path);\n\t}\n}\n\n \nstatic int iucv_send_ctrl(struct sock *sk, u8 flags)\n{\n\tstruct iucv_sock *iucv = iucv_sk(sk);\n\tint err = 0;\n\tint blen;\n\tstruct sk_buff *skb;\n\tu8 shutdown = 0;\n\n\tblen = sizeof(struct af_iucv_trans_hdr) +\n\t       LL_RESERVED_SPACE(iucv->hs_dev);\n\tif (sk->sk_shutdown & SEND_SHUTDOWN) {\n\t\t \n\t\tshutdown = sk->sk_shutdown;\n\t\tsk->sk_shutdown &= RCV_SHUTDOWN;\n\t}\n\tskb = sock_alloc_send_skb(sk, blen, 1, &err);\n\tif (skb) {\n\t\tskb_reserve(skb, blen);\n\t\terr = afiucv_hs_send(NULL, sk, skb, flags);\n\t}\n\tif (shutdown)\n\t\tsk->sk_shutdown = shutdown;\n\treturn err;\n}\n\n \nstatic void iucv_sock_close(struct sock *sk)\n{\n\tstruct iucv_sock *iucv = iucv_sk(sk);\n\tunsigned long timeo;\n\tint err = 0;\n\n\tlock_sock(sk);\n\n\tswitch (sk->sk_state) {\n\tcase IUCV_LISTEN:\n\t\tiucv_sock_cleanup_listen(sk);\n\t\tbreak;\n\n\tcase IUCV_CONNECTED:\n\t\tif (iucv->transport == AF_IUCV_TRANS_HIPER) {\n\t\t\terr = iucv_send_ctrl(sk, AF_IUCV_FLAG_FIN);\n\t\t\tsk->sk_state = IUCV_DISCONN;\n\t\t\tsk->sk_state_change(sk);\n\t\t}\n\t\tfallthrough;\n\n\tcase IUCV_DISCONN:\n\t\tsk->sk_state = IUCV_CLOSING;\n\t\tsk->sk_state_change(sk);\n\n\t\tif (!err && atomic_read(&iucv->skbs_in_xmit) > 0) {\n\t\t\tif (sock_flag(sk, SOCK_LINGER) && sk->sk_lingertime)\n\t\t\t\ttimeo = sk->sk_lingertime;\n\t\t\telse\n\t\t\t\ttimeo = IUCV_DISCONN_TIMEOUT;\n\t\t\tiucv_sock_wait(sk,\n\t\t\t\t\tiucv_sock_in_state(sk, IUCV_CLOSED, 0),\n\t\t\t\t\ttimeo);\n\t\t}\n\t\tfallthrough;\n\n\tcase IUCV_CLOSING:\n\t\tsk->sk_state = IUCV_CLOSED;\n\t\tsk->sk_state_change(sk);\n\n\t\tsk->sk_err = ECONNRESET;\n\t\tsk->sk_state_change(sk);\n\n\t\tskb_queue_purge(&iucv->send_skb_q);\n\t\tskb_queue_purge(&iucv->backlog_skb_q);\n\t\tfallthrough;\n\n\tdefault:\n\t\tiucv_sever_path(sk, 1);\n\t}\n\n\tif (iucv->hs_dev) {\n\t\tdev_put(iucv->hs_dev);\n\t\tiucv->hs_dev = NULL;\n\t\tsk->sk_bound_dev_if = 0;\n\t}\n\n\t \n\tsock_set_flag(sk, SOCK_ZAPPED);\n\n\trelease_sock(sk);\n}\n\nstatic void iucv_sock_init(struct sock *sk, struct sock *parent)\n{\n\tif (parent) {\n\t\tsk->sk_type = parent->sk_type;\n\t\tsecurity_sk_clone(parent, sk);\n\t}\n}\n\nstatic struct sock *iucv_sock_alloc(struct socket *sock, int proto, gfp_t prio, int kern)\n{\n\tstruct sock *sk;\n\tstruct iucv_sock *iucv;\n\n\tsk = sk_alloc(&init_net, PF_IUCV, prio, &iucv_proto, kern);\n\tif (!sk)\n\t\treturn NULL;\n\tiucv = iucv_sk(sk);\n\n\tsock_init_data(sock, sk);\n\tINIT_LIST_HEAD(&iucv->accept_q);\n\tspin_lock_init(&iucv->accept_q_lock);\n\tskb_queue_head_init(&iucv->send_skb_q);\n\tINIT_LIST_HEAD(&iucv->message_q.list);\n\tspin_lock_init(&iucv->message_q.lock);\n\tskb_queue_head_init(&iucv->backlog_skb_q);\n\tiucv->send_tag = 0;\n\tatomic_set(&iucv->pendings, 0);\n\tiucv->flags = 0;\n\tiucv->msglimit = 0;\n\tatomic_set(&iucv->skbs_in_xmit, 0);\n\tatomic_set(&iucv->msg_sent, 0);\n\tatomic_set(&iucv->msg_recv, 0);\n\tiucv->path = NULL;\n\tiucv->sk_txnotify = afiucv_hs_callback_txnotify;\n\tmemset(&iucv->init, 0, sizeof(iucv->init));\n\tif (pr_iucv)\n\t\tiucv->transport = AF_IUCV_TRANS_IUCV;\n\telse\n\t\tiucv->transport = AF_IUCV_TRANS_HIPER;\n\n\tsk->sk_destruct = iucv_sock_destruct;\n\tsk->sk_sndtimeo = IUCV_CONN_TIMEOUT;\n\n\tsock_reset_flag(sk, SOCK_ZAPPED);\n\n\tsk->sk_protocol = proto;\n\tsk->sk_state\t= IUCV_OPEN;\n\n\tiucv_sock_link(&iucv_sk_list, sk);\n\treturn sk;\n}\n\nstatic void iucv_accept_enqueue(struct sock *parent, struct sock *sk)\n{\n\tunsigned long flags;\n\tstruct iucv_sock *par = iucv_sk(parent);\n\n\tsock_hold(sk);\n\tspin_lock_irqsave(&par->accept_q_lock, flags);\n\tlist_add_tail(&iucv_sk(sk)->accept_q, &par->accept_q);\n\tspin_unlock_irqrestore(&par->accept_q_lock, flags);\n\tiucv_sk(sk)->parent = parent;\n\tsk_acceptq_added(parent);\n}\n\nstatic void iucv_accept_unlink(struct sock *sk)\n{\n\tunsigned long flags;\n\tstruct iucv_sock *par = iucv_sk(iucv_sk(sk)->parent);\n\n\tspin_lock_irqsave(&par->accept_q_lock, flags);\n\tlist_del_init(&iucv_sk(sk)->accept_q);\n\tspin_unlock_irqrestore(&par->accept_q_lock, flags);\n\tsk_acceptq_removed(iucv_sk(sk)->parent);\n\tiucv_sk(sk)->parent = NULL;\n\tsock_put(sk);\n}\n\nstatic struct sock *iucv_accept_dequeue(struct sock *parent,\n\t\t\t\t\tstruct socket *newsock)\n{\n\tstruct iucv_sock *isk, *n;\n\tstruct sock *sk;\n\n\tlist_for_each_entry_safe(isk, n, &iucv_sk(parent)->accept_q, accept_q) {\n\t\tsk = (struct sock *) isk;\n\t\tlock_sock(sk);\n\n\t\tif (sk->sk_state == IUCV_CLOSED) {\n\t\t\tiucv_accept_unlink(sk);\n\t\t\trelease_sock(sk);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (sk->sk_state == IUCV_CONNECTED ||\n\t\t    sk->sk_state == IUCV_DISCONN ||\n\t\t    !newsock) {\n\t\t\tiucv_accept_unlink(sk);\n\t\t\tif (newsock)\n\t\t\t\tsock_graft(sk, newsock);\n\n\t\t\trelease_sock(sk);\n\t\t\treturn sk;\n\t\t}\n\n\t\trelease_sock(sk);\n\t}\n\treturn NULL;\n}\n\nstatic void __iucv_auto_name(struct iucv_sock *iucv)\n{\n\tchar name[12];\n\n\tsprintf(name, \"%08x\", atomic_inc_return(&iucv_sk_list.autobind_name));\n\twhile (__iucv_get_sock_by_name(name)) {\n\t\tsprintf(name, \"%08x\",\n\t\t\tatomic_inc_return(&iucv_sk_list.autobind_name));\n\t}\n\tmemcpy(iucv->src_name, name, 8);\n}\n\n \nstatic int iucv_sock_bind(struct socket *sock, struct sockaddr *addr,\n\t\t\t  int addr_len)\n{\n\tDECLARE_SOCKADDR(struct sockaddr_iucv *, sa, addr);\n\tchar uid[sizeof(sa->siucv_user_id)];\n\tstruct sock *sk = sock->sk;\n\tstruct iucv_sock *iucv;\n\tint err = 0;\n\tstruct net_device *dev;\n\n\t \n\tif (addr_len < sizeof(struct sockaddr_iucv) ||\n\t    addr->sa_family != AF_IUCV)\n\t\treturn -EINVAL;\n\n\tlock_sock(sk);\n\tif (sk->sk_state != IUCV_OPEN) {\n\t\terr = -EBADFD;\n\t\tgoto done;\n\t}\n\n\twrite_lock_bh(&iucv_sk_list.lock);\n\n\tiucv = iucv_sk(sk);\n\tif (__iucv_get_sock_by_name(sa->siucv_name)) {\n\t\terr = -EADDRINUSE;\n\t\tgoto done_unlock;\n\t}\n\tif (iucv->path)\n\t\tgoto done_unlock;\n\n\t \n\tif (pr_iucv)\n\t\tif (!memcmp(sa->siucv_user_id, iucv_userid, 8))\n\t\t\tgoto vm_bind;  \n\n\t \n\tmemcpy(uid, sa->siucv_user_id, sizeof(uid));\n\tASCEBC(uid, 8);\n\trcu_read_lock();\n\tfor_each_netdev_rcu(&init_net, dev) {\n\t\tif (!memcmp(dev->perm_addr, uid, 8)) {\n\t\t\tmemcpy(iucv->src_user_id, sa->siucv_user_id, 8);\n\t\t\t \n\t\t\tif (strncmp(sa->siucv_name, \"        \", 8) == 0)\n\t\t\t\t__iucv_auto_name(iucv);\n\t\t\telse\n\t\t\t\tmemcpy(iucv->src_name, sa->siucv_name, 8);\n\t\t\tsk->sk_bound_dev_if = dev->ifindex;\n\t\t\tiucv->hs_dev = dev;\n\t\t\tdev_hold(dev);\n\t\t\tsk->sk_state = IUCV_BOUND;\n\t\t\tiucv->transport = AF_IUCV_TRANS_HIPER;\n\t\t\tif (!iucv->msglimit)\n\t\t\t\tiucv->msglimit = IUCV_HIPER_MSGLIM_DEFAULT;\n\t\t\trcu_read_unlock();\n\t\t\tgoto done_unlock;\n\t\t}\n\t}\n\trcu_read_unlock();\nvm_bind:\n\tif (pr_iucv) {\n\t\t \n\t\tmemcpy(iucv->src_name, sa->siucv_name, 8);\n\t\tmemcpy(iucv->src_user_id, iucv_userid, 8);\n\t\tsk->sk_state = IUCV_BOUND;\n\t\tiucv->transport = AF_IUCV_TRANS_IUCV;\n\t\tsk->sk_allocation |= GFP_DMA;\n\t\tif (!iucv->msglimit)\n\t\t\tiucv->msglimit = IUCV_QUEUELEN_DEFAULT;\n\t\tgoto done_unlock;\n\t}\n\t \n\terr = -ENODEV;\ndone_unlock:\n\t \n\twrite_unlock_bh(&iucv_sk_list.lock);\ndone:\n\trelease_sock(sk);\n\treturn err;\n}\n\n \nstatic int iucv_sock_autobind(struct sock *sk)\n{\n\tstruct iucv_sock *iucv = iucv_sk(sk);\n\tint err = 0;\n\n\tif (unlikely(!pr_iucv))\n\t\treturn -EPROTO;\n\n\tmemcpy(iucv->src_user_id, iucv_userid, 8);\n\tiucv->transport = AF_IUCV_TRANS_IUCV;\n\tsk->sk_allocation |= GFP_DMA;\n\n\twrite_lock_bh(&iucv_sk_list.lock);\n\t__iucv_auto_name(iucv);\n\twrite_unlock_bh(&iucv_sk_list.lock);\n\n\tif (!iucv->msglimit)\n\t\tiucv->msglimit = IUCV_QUEUELEN_DEFAULT;\n\n\treturn err;\n}\n\nstatic int afiucv_path_connect(struct socket *sock, struct sockaddr *addr)\n{\n\tDECLARE_SOCKADDR(struct sockaddr_iucv *, sa, addr);\n\tstruct sock *sk = sock->sk;\n\tstruct iucv_sock *iucv = iucv_sk(sk);\n\tunsigned char user_data[16];\n\tint err;\n\n\thigh_nmcpy(user_data, sa->siucv_name);\n\tlow_nmcpy(user_data, iucv->src_name);\n\tASCEBC(user_data, sizeof(user_data));\n\n\t \n\tiucv->path = iucv_path_alloc(iucv->msglimit,\n\t\t\t\t     IUCV_IPRMDATA, GFP_KERNEL);\n\tif (!iucv->path) {\n\t\terr = -ENOMEM;\n\t\tgoto done;\n\t}\n\terr = pr_iucv->path_connect(iucv->path, &af_iucv_handler,\n\t\t\t\t    sa->siucv_user_id, NULL, user_data,\n\t\t\t\t    sk);\n\tif (err) {\n\t\tiucv_path_free(iucv->path);\n\t\tiucv->path = NULL;\n\t\tswitch (err) {\n\t\tcase 0x0b:\t \n\t\t\terr = -ENETUNREACH;\n\t\t\tbreak;\n\t\tcase 0x0d:\t \n\t\tcase 0x0e:\t \n\t\t\terr = -EAGAIN;\n\t\t\tbreak;\n\t\tcase 0x0f:\t \n\t\t\terr = -EACCES;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\terr = -ECONNREFUSED;\n\t\t\tbreak;\n\t\t}\n\t}\ndone:\n\treturn err;\n}\n\n \nstatic int iucv_sock_connect(struct socket *sock, struct sockaddr *addr,\n\t\t\t     int alen, int flags)\n{\n\tDECLARE_SOCKADDR(struct sockaddr_iucv *, sa, addr);\n\tstruct sock *sk = sock->sk;\n\tstruct iucv_sock *iucv = iucv_sk(sk);\n\tint err;\n\n\tif (alen < sizeof(struct sockaddr_iucv) || addr->sa_family != AF_IUCV)\n\t\treturn -EINVAL;\n\n\tif (sk->sk_state != IUCV_OPEN && sk->sk_state != IUCV_BOUND)\n\t\treturn -EBADFD;\n\n\tif (sk->sk_state == IUCV_OPEN &&\n\t    iucv->transport == AF_IUCV_TRANS_HIPER)\n\t\treturn -EBADFD;  \n\n\tif (sk->sk_type != SOCK_STREAM && sk->sk_type != SOCK_SEQPACKET)\n\t\treturn -EINVAL;\n\n\tif (sk->sk_state == IUCV_OPEN) {\n\t\terr = iucv_sock_autobind(sk);\n\t\tif (unlikely(err))\n\t\t\treturn err;\n\t}\n\n\tlock_sock(sk);\n\n\t \n\tmemcpy(iucv->dst_user_id, sa->siucv_user_id, 8);\n\tmemcpy(iucv->dst_name, sa->siucv_name, 8);\n\n\tif (iucv->transport == AF_IUCV_TRANS_HIPER)\n\t\terr = iucv_send_ctrl(sock->sk, AF_IUCV_FLAG_SYN);\n\telse\n\t\terr = afiucv_path_connect(sock, addr);\n\tif (err)\n\t\tgoto done;\n\n\tif (sk->sk_state != IUCV_CONNECTED)\n\t\terr = iucv_sock_wait(sk, iucv_sock_in_state(sk, IUCV_CONNECTED,\n\t\t\t\t\t\t\t    IUCV_DISCONN),\n\t\t\t\t     sock_sndtimeo(sk, flags & O_NONBLOCK));\n\n\tif (sk->sk_state == IUCV_DISCONN || sk->sk_state == IUCV_CLOSED)\n\t\terr = -ECONNREFUSED;\n\n\tif (err && iucv->transport == AF_IUCV_TRANS_IUCV)\n\t\tiucv_sever_path(sk, 0);\n\ndone:\n\trelease_sock(sk);\n\treturn err;\n}\n\n \nstatic int iucv_sock_listen(struct socket *sock, int backlog)\n{\n\tstruct sock *sk = sock->sk;\n\tint err;\n\n\tlock_sock(sk);\n\n\terr = -EINVAL;\n\tif (sk->sk_state != IUCV_BOUND)\n\t\tgoto done;\n\n\tif (sock->type != SOCK_STREAM && sock->type != SOCK_SEQPACKET)\n\t\tgoto done;\n\n\tsk->sk_max_ack_backlog = backlog;\n\tsk->sk_ack_backlog = 0;\n\tsk->sk_state = IUCV_LISTEN;\n\terr = 0;\n\ndone:\n\trelease_sock(sk);\n\treturn err;\n}\n\n \nstatic int iucv_sock_accept(struct socket *sock, struct socket *newsock,\n\t\t\t    int flags, bool kern)\n{\n\tDECLARE_WAITQUEUE(wait, current);\n\tstruct sock *sk = sock->sk, *nsk;\n\tlong timeo;\n\tint err = 0;\n\n\tlock_sock_nested(sk, SINGLE_DEPTH_NESTING);\n\n\tif (sk->sk_state != IUCV_LISTEN) {\n\t\terr = -EBADFD;\n\t\tgoto done;\n\t}\n\n\ttimeo = sock_rcvtimeo(sk, flags & O_NONBLOCK);\n\n\t \n\tadd_wait_queue_exclusive(sk_sleep(sk), &wait);\n\twhile (!(nsk = iucv_accept_dequeue(sk, newsock))) {\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\tif (!timeo) {\n\t\t\terr = -EAGAIN;\n\t\t\tbreak;\n\t\t}\n\n\t\trelease_sock(sk);\n\t\ttimeo = schedule_timeout(timeo);\n\t\tlock_sock_nested(sk, SINGLE_DEPTH_NESTING);\n\n\t\tif (sk->sk_state != IUCV_LISTEN) {\n\t\t\terr = -EBADFD;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (signal_pending(current)) {\n\t\t\terr = sock_intr_errno(timeo);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tset_current_state(TASK_RUNNING);\n\tremove_wait_queue(sk_sleep(sk), &wait);\n\n\tif (err)\n\t\tgoto done;\n\n\tnewsock->state = SS_CONNECTED;\n\ndone:\n\trelease_sock(sk);\n\treturn err;\n}\n\nstatic int iucv_sock_getname(struct socket *sock, struct sockaddr *addr,\n\t\t\t     int peer)\n{\n\tDECLARE_SOCKADDR(struct sockaddr_iucv *, siucv, addr);\n\tstruct sock *sk = sock->sk;\n\tstruct iucv_sock *iucv = iucv_sk(sk);\n\n\taddr->sa_family = AF_IUCV;\n\n\tif (peer) {\n\t\tmemcpy(siucv->siucv_user_id, iucv->dst_user_id, 8);\n\t\tmemcpy(siucv->siucv_name, iucv->dst_name, 8);\n\t} else {\n\t\tmemcpy(siucv->siucv_user_id, iucv->src_user_id, 8);\n\t\tmemcpy(siucv->siucv_name, iucv->src_name, 8);\n\t}\n\tmemset(&siucv->siucv_port, 0, sizeof(siucv->siucv_port));\n\tmemset(&siucv->siucv_addr, 0, sizeof(siucv->siucv_addr));\n\tmemset(&siucv->siucv_nodeid, 0, sizeof(siucv->siucv_nodeid));\n\n\treturn sizeof(struct sockaddr_iucv);\n}\n\n \nstatic int iucv_send_iprm(struct iucv_path *path, struct iucv_message *msg,\n\t\t\t  struct sk_buff *skb)\n{\n\tu8 prmdata[8];\n\n\tmemcpy(prmdata, (void *) skb->data, skb->len);\n\tprmdata[7] = 0xff - (u8) skb->len;\n\treturn pr_iucv->message_send(path, msg, IUCV_IPRMDATA, 0,\n\t\t\t\t (void *) prmdata, 8);\n}\n\nstatic int iucv_sock_sendmsg(struct socket *sock, struct msghdr *msg,\n\t\t\t     size_t len)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct iucv_sock *iucv = iucv_sk(sk);\n\tsize_t headroom = 0;\n\tsize_t linear;\n\tstruct sk_buff *skb;\n\tstruct iucv_message txmsg = {0};\n\tstruct cmsghdr *cmsg;\n\tint cmsg_done;\n\tlong timeo;\n\tchar user_id[9];\n\tchar appl_id[9];\n\tint err;\n\tint noblock = msg->msg_flags & MSG_DONTWAIT;\n\n\terr = sock_error(sk);\n\tif (err)\n\t\treturn err;\n\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\t \n\tif (sk->sk_type == SOCK_SEQPACKET && !(msg->msg_flags & MSG_EOR))\n\t\treturn -EOPNOTSUPP;\n\n\tlock_sock(sk);\n\n\tif (sk->sk_shutdown & SEND_SHUTDOWN) {\n\t\terr = -EPIPE;\n\t\tgoto out;\n\t}\n\n\t \n\tif (sk->sk_state != IUCV_CONNECTED) {\n\t\terr = -ENOTCONN;\n\t\tgoto out;\n\t}\n\n\t \n\tcmsg_done   = 0;\t \n\n\t \n\tfor_each_cmsghdr(cmsg, msg) {\n\t\tif (!CMSG_OK(msg, cmsg)) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (cmsg->cmsg_level != SOL_IUCV)\n\t\t\tcontinue;\n\n\t\tif (cmsg->cmsg_type & cmsg_done) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tcmsg_done |= cmsg->cmsg_type;\n\n\t\tswitch (cmsg->cmsg_type) {\n\t\tcase SCM_IUCV_TRGCLS:\n\t\t\tif (cmsg->cmsg_len != CMSG_LEN(TRGCLS_SIZE)) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\t \n\t\t\tmemcpy(&txmsg.class,\n\t\t\t\t(void *) CMSG_DATA(cmsg), TRGCLS_SIZE);\n\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t \n\tif (iucv->transport == AF_IUCV_TRANS_HIPER) {\n\t\theadroom = sizeof(struct af_iucv_trans_hdr) +\n\t\t\t   LL_RESERVED_SPACE(iucv->hs_dev);\n\t\tlinear = min(len, PAGE_SIZE - headroom);\n\t} else {\n\t\tif (len < PAGE_SIZE) {\n\t\t\tlinear = len;\n\t\t} else {\n\t\t\t \n\t\t\theadroom = sizeof(struct iucv_array) *\n\t\t\t\t   (MAX_SKB_FRAGS + 1);\n\t\t\tlinear = PAGE_SIZE - headroom;\n\t\t}\n\t}\n\tskb = sock_alloc_send_pskb(sk, headroom + linear, len - linear,\n\t\t\t\t   noblock, &err, 0);\n\tif (!skb)\n\t\tgoto out;\n\tif (headroom)\n\t\tskb_reserve(skb, headroom);\n\tskb_put(skb, linear);\n\tskb->len = len;\n\tskb->data_len = len - linear;\n\terr = skb_copy_datagram_from_iter(skb, 0, &msg->msg_iter, len);\n\tif (err)\n\t\tgoto fail;\n\n\t \n\ttimeo = sock_sndtimeo(sk, noblock);\n\terr = iucv_sock_wait(sk, iucv_below_msglim(sk), timeo);\n\tif (err)\n\t\tgoto fail;\n\n\t \n\tif (sk->sk_state != IUCV_CONNECTED) {\n\t\terr = -ECONNRESET;\n\t\tgoto fail;\n\t}\n\n\t \n\ttxmsg.tag = iucv->send_tag++;\n\tIUCV_SKB_CB(skb)->tag = txmsg.tag;\n\n\tif (iucv->transport == AF_IUCV_TRANS_HIPER) {\n\t\tatomic_inc(&iucv->msg_sent);\n\t\terr = afiucv_hs_send(&txmsg, sk, skb, 0);\n\t\tif (err) {\n\t\t\tatomic_dec(&iucv->msg_sent);\n\t\t\tgoto out;\n\t\t}\n\t} else {  \n\t\tskb_queue_tail(&iucv->send_skb_q, skb);\n\t\tatomic_inc(&iucv->skbs_in_xmit);\n\n\t\tif (((iucv->path->flags & IUCV_IPRMDATA) & iucv->flags) &&\n\t\t    skb->len <= 7) {\n\t\t\terr = iucv_send_iprm(iucv->path, &txmsg, skb);\n\n\t\t\t \n\t\t\t \n\t\t\tif (err == 0) {\n\t\t\t\tatomic_dec(&iucv->skbs_in_xmit);\n\t\t\t\tskb_unlink(skb, &iucv->send_skb_q);\n\t\t\t\tconsume_skb(skb);\n\t\t\t}\n\n\t\t\t \n\t\t\t \n\t\t\tif (err == 0x15) {\n\t\t\t\tpr_iucv->path_sever(iucv->path, NULL);\n\t\t\t\tatomic_dec(&iucv->skbs_in_xmit);\n\t\t\t\tskb_unlink(skb, &iucv->send_skb_q);\n\t\t\t\terr = -EPIPE;\n\t\t\t\tgoto fail;\n\t\t\t}\n\t\t} else if (skb_is_nonlinear(skb)) {\n\t\t\tstruct iucv_array *iba = (struct iucv_array *)skb->head;\n\t\t\tint i;\n\n\t\t\t \n\t\t\tiba[0].address = (u32)(addr_t)skb->data;\n\t\t\tiba[0].length = (u32)skb_headlen(skb);\n\t\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\t\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t\t\t\tiba[i + 1].address =\n\t\t\t\t\t(u32)(addr_t)skb_frag_address(frag);\n\t\t\t\tiba[i + 1].length = (u32)skb_frag_size(frag);\n\t\t\t}\n\t\t\terr = pr_iucv->message_send(iucv->path, &txmsg,\n\t\t\t\t\t\t    IUCV_IPBUFLST, 0,\n\t\t\t\t\t\t    (void *)iba, skb->len);\n\t\t} else {  \n\t\t\terr = pr_iucv->message_send(iucv->path, &txmsg,\n\t\t\t\t\t0, 0, (void *)skb->data, skb->len);\n\t\t}\n\t\tif (err) {\n\t\t\tif (err == 3) {\n\t\t\t\tuser_id[8] = 0;\n\t\t\t\tmemcpy(user_id, iucv->dst_user_id, 8);\n\t\t\t\tappl_id[8] = 0;\n\t\t\t\tmemcpy(appl_id, iucv->dst_name, 8);\n\t\t\t\tpr_err(\n\t\t\"Application %s on z/VM guest %s exceeds message limit\\n\",\n\t\t\t\t\tappl_id, user_id);\n\t\t\t\terr = -EAGAIN;\n\t\t\t} else {\n\t\t\t\terr = -EPIPE;\n\t\t\t}\n\n\t\t\tatomic_dec(&iucv->skbs_in_xmit);\n\t\t\tskb_unlink(skb, &iucv->send_skb_q);\n\t\t\tgoto fail;\n\t\t}\n\t}\n\n\trelease_sock(sk);\n\treturn len;\n\nfail:\n\tkfree_skb(skb);\nout:\n\trelease_sock(sk);\n\treturn err;\n}\n\nstatic struct sk_buff *alloc_iucv_recv_skb(unsigned long len)\n{\n\tsize_t headroom, linear;\n\tstruct sk_buff *skb;\n\tint err;\n\n\tif (len < PAGE_SIZE) {\n\t\theadroom = 0;\n\t\tlinear = len;\n\t} else {\n\t\theadroom = sizeof(struct iucv_array) * (MAX_SKB_FRAGS + 1);\n\t\tlinear = PAGE_SIZE - headroom;\n\t}\n\tskb = alloc_skb_with_frags(headroom + linear, len - linear,\n\t\t\t\t   0, &err, GFP_ATOMIC | GFP_DMA);\n\tWARN_ONCE(!skb,\n\t\t  \"alloc of recv iucv skb len=%lu failed with errcode=%d\\n\",\n\t\t  len, err);\n\tif (skb) {\n\t\tif (headroom)\n\t\t\tskb_reserve(skb, headroom);\n\t\tskb_put(skb, linear);\n\t\tskb->len = len;\n\t\tskb->data_len = len - linear;\n\t}\n\treturn skb;\n}\n\n \nstatic void iucv_process_message(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t struct iucv_path *path,\n\t\t\t\t struct iucv_message *msg)\n{\n\tint rc;\n\tunsigned int len;\n\n\tlen = iucv_msg_length(msg);\n\n\t \n\t \n\tIUCV_SKB_CB(skb)->class = msg->class;\n\n\t \n\tif ((msg->flags & IUCV_IPRMDATA) && len > 7) {\n\t\tif (memcmp(msg->rmmsg, iprm_shutdown, 8) == 0) {\n\t\t\tskb->data = NULL;\n\t\t\tskb->len = 0;\n\t\t}\n\t} else {\n\t\tif (skb_is_nonlinear(skb)) {\n\t\t\tstruct iucv_array *iba = (struct iucv_array *)skb->head;\n\t\t\tint i;\n\n\t\t\tiba[0].address = (u32)(addr_t)skb->data;\n\t\t\tiba[0].length = (u32)skb_headlen(skb);\n\t\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\t\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t\t\t\tiba[i + 1].address =\n\t\t\t\t\t(u32)(addr_t)skb_frag_address(frag);\n\t\t\t\tiba[i + 1].length = (u32)skb_frag_size(frag);\n\t\t\t}\n\t\t\trc = pr_iucv->message_receive(path, msg,\n\t\t\t\t\t      IUCV_IPBUFLST,\n\t\t\t\t\t      (void *)iba, len, NULL);\n\t\t} else {\n\t\t\trc = pr_iucv->message_receive(path, msg,\n\t\t\t\t\t      msg->flags & IUCV_IPRMDATA,\n\t\t\t\t\t      skb->data, len, NULL);\n\t\t}\n\t\tif (rc) {\n\t\t\tkfree_skb(skb);\n\t\t\treturn;\n\t\t}\n\t\tWARN_ON_ONCE(skb->len != len);\n\t}\n\n\tIUCV_SKB_CB(skb)->offset = 0;\n\tif (sk_filter(sk, skb)) {\n\t\tatomic_inc(&sk->sk_drops);\t \n\t\tkfree_skb(skb);\n\t\treturn;\n\t}\n\tif (__sock_queue_rcv_skb(sk, skb))\t \n\t\tskb_queue_tail(&iucv_sk(sk)->backlog_skb_q, skb);\n}\n\n \nstatic void iucv_process_message_q(struct sock *sk)\n{\n\tstruct iucv_sock *iucv = iucv_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct sock_msg_q *p, *n;\n\n\tlist_for_each_entry_safe(p, n, &iucv->message_q.list, list) {\n\t\tskb = alloc_iucv_recv_skb(iucv_msg_length(&p->msg));\n\t\tif (!skb)\n\t\t\tbreak;\n\t\tiucv_process_message(sk, skb, p->path, &p->msg);\n\t\tlist_del(&p->list);\n\t\tkfree(p);\n\t\tif (!skb_queue_empty(&iucv->backlog_skb_q))\n\t\t\tbreak;\n\t}\n}\n\nstatic int iucv_sock_recvmsg(struct socket *sock, struct msghdr *msg,\n\t\t\t     size_t len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct iucv_sock *iucv = iucv_sk(sk);\n\tunsigned int copied, rlen;\n\tstruct sk_buff *skb, *rskb, *cskb;\n\tint err = 0;\n\tu32 offset;\n\n\tif ((sk->sk_state == IUCV_DISCONN) &&\n\t    skb_queue_empty(&iucv->backlog_skb_q) &&\n\t    skb_queue_empty(&sk->sk_receive_queue) &&\n\t    list_empty(&iucv->message_q.list))\n\t\treturn 0;\n\n\tif (flags & (MSG_OOB))\n\t\treturn -EOPNOTSUPP;\n\n\t \n\tskb = skb_recv_datagram(sk, flags, &err);\n\tif (!skb) {\n\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\treturn 0;\n\t\treturn err;\n\t}\n\n\toffset = IUCV_SKB_CB(skb)->offset;\n\trlen   = skb->len - offset;\t\t \n\tcopied = min_t(unsigned int, rlen, len);\n\tif (!rlen)\n\t\tsk->sk_shutdown = sk->sk_shutdown | RCV_SHUTDOWN;\n\n\tcskb = skb;\n\tif (skb_copy_datagram_msg(cskb, offset, msg, copied)) {\n\t\tif (!(flags & MSG_PEEK))\n\t\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\t\treturn -EFAULT;\n\t}\n\n\t \n\tif (sk->sk_type == SOCK_SEQPACKET) {\n\t\tif (copied < rlen)\n\t\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\t \n\t\tmsg->msg_flags |= MSG_EOR;\n\t}\n\n\t \n\terr = put_cmsg(msg, SOL_IUCV, SCM_IUCV_TRGCLS,\n\t\t       sizeof(IUCV_SKB_CB(skb)->class),\n\t\t       (void *)&IUCV_SKB_CB(skb)->class);\n\tif (err) {\n\t\tif (!(flags & MSG_PEEK))\n\t\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\t\treturn err;\n\t}\n\n\t \n\tif (!(flags & MSG_PEEK)) {\n\n\t\t \n\t\tif (sk->sk_type == SOCK_STREAM) {\n\t\t\tif (copied < rlen) {\n\t\t\t\tIUCV_SKB_CB(skb)->offset = offset + copied;\n\t\t\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\t\t\t\tgoto done;\n\t\t\t}\n\t\t}\n\n\t\tconsume_skb(skb);\n\t\tif (iucv->transport == AF_IUCV_TRANS_HIPER) {\n\t\t\tatomic_inc(&iucv->msg_recv);\n\t\t\tif (atomic_read(&iucv->msg_recv) > iucv->msglimit) {\n\t\t\t\tWARN_ON(1);\n\t\t\t\tiucv_sock_close(sk);\n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tspin_lock_bh(&iucv->message_q.lock);\n\t\trskb = skb_dequeue(&iucv->backlog_skb_q);\n\t\twhile (rskb) {\n\t\t\tIUCV_SKB_CB(rskb)->offset = 0;\n\t\t\tif (__sock_queue_rcv_skb(sk, rskb)) {\n\t\t\t\t \n\t\t\t\tskb_queue_head(&iucv->backlog_skb_q,\n\t\t\t\t\t\trskb);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\trskb = skb_dequeue(&iucv->backlog_skb_q);\n\t\t}\n\t\tif (skb_queue_empty(&iucv->backlog_skb_q)) {\n\t\t\tif (!list_empty(&iucv->message_q.list))\n\t\t\t\tiucv_process_message_q(sk);\n\t\t\tif (atomic_read(&iucv->msg_recv) >=\n\t\t\t\t\t\t\tiucv->msglimit / 2) {\n\t\t\t\terr = iucv_send_ctrl(sk, AF_IUCV_FLAG_WIN);\n\t\t\t\tif (err) {\n\t\t\t\t\tsk->sk_state = IUCV_DISCONN;\n\t\t\t\t\tsk->sk_state_change(sk);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tspin_unlock_bh(&iucv->message_q.lock);\n\t}\n\ndone:\n\t \n\tif (sk->sk_type == SOCK_SEQPACKET && (flags & MSG_TRUNC))\n\t\tcopied = rlen;\n\n\treturn copied;\n}\n\nstatic inline __poll_t iucv_accept_poll(struct sock *parent)\n{\n\tstruct iucv_sock *isk, *n;\n\tstruct sock *sk;\n\n\tlist_for_each_entry_safe(isk, n, &iucv_sk(parent)->accept_q, accept_q) {\n\t\tsk = (struct sock *) isk;\n\n\t\tif (sk->sk_state == IUCV_CONNECTED)\n\t\t\treturn EPOLLIN | EPOLLRDNORM;\n\t}\n\n\treturn 0;\n}\n\nstatic __poll_t iucv_sock_poll(struct file *file, struct socket *sock,\n\t\t\t       poll_table *wait)\n{\n\tstruct sock *sk = sock->sk;\n\t__poll_t mask = 0;\n\n\tsock_poll_wait(file, sock, wait);\n\n\tif (sk->sk_state == IUCV_LISTEN)\n\t\treturn iucv_accept_poll(sk);\n\n\tif (sk->sk_err || !skb_queue_empty(&sk->sk_error_queue))\n\t\tmask |= EPOLLERR |\n\t\t\t(sock_flag(sk, SOCK_SELECT_ERR_QUEUE) ? EPOLLPRI : 0);\n\n\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\tmask |= EPOLLRDHUP;\n\n\tif (sk->sk_shutdown == SHUTDOWN_MASK)\n\t\tmask |= EPOLLHUP;\n\n\tif (!skb_queue_empty(&sk->sk_receive_queue) ||\n\t    (sk->sk_shutdown & RCV_SHUTDOWN))\n\t\tmask |= EPOLLIN | EPOLLRDNORM;\n\n\tif (sk->sk_state == IUCV_CLOSED)\n\t\tmask |= EPOLLHUP;\n\n\tif (sk->sk_state == IUCV_DISCONN)\n\t\tmask |= EPOLLIN;\n\n\tif (sock_writeable(sk) && iucv_below_msglim(sk))\n\t\tmask |= EPOLLOUT | EPOLLWRNORM | EPOLLWRBAND;\n\telse\n\t\tsk_set_bit(SOCKWQ_ASYNC_NOSPACE, sk);\n\n\treturn mask;\n}\n\nstatic int iucv_sock_shutdown(struct socket *sock, int how)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct iucv_sock *iucv = iucv_sk(sk);\n\tstruct iucv_message txmsg;\n\tint err = 0;\n\n\thow++;\n\n\tif ((how & ~SHUTDOWN_MASK) || !how)\n\t\treturn -EINVAL;\n\n\tlock_sock(sk);\n\tswitch (sk->sk_state) {\n\tcase IUCV_LISTEN:\n\tcase IUCV_DISCONN:\n\tcase IUCV_CLOSING:\n\tcase IUCV_CLOSED:\n\t\terr = -ENOTCONN;\n\t\tgoto fail;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif ((how == SEND_SHUTDOWN || how == SHUTDOWN_MASK) &&\n\t    sk->sk_state == IUCV_CONNECTED) {\n\t\tif (iucv->transport == AF_IUCV_TRANS_IUCV) {\n\t\t\ttxmsg.class = 0;\n\t\t\ttxmsg.tag = 0;\n\t\t\terr = pr_iucv->message_send(iucv->path, &txmsg,\n\t\t\t\tIUCV_IPRMDATA, 0, (void *) iprm_shutdown, 8);\n\t\t\tif (err) {\n\t\t\t\tswitch (err) {\n\t\t\t\tcase 1:\n\t\t\t\t\terr = -ENOTCONN;\n\t\t\t\t\tbreak;\n\t\t\t\tcase 2:\n\t\t\t\t\terr = -ECONNRESET;\n\t\t\t\t\tbreak;\n\t\t\t\tdefault:\n\t\t\t\t\terr = -ENOTCONN;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t} else\n\t\t\tiucv_send_ctrl(sk, AF_IUCV_FLAG_SHT);\n\t}\n\n\tsk->sk_shutdown |= how;\n\tif (how == RCV_SHUTDOWN || how == SHUTDOWN_MASK) {\n\t\tif ((iucv->transport == AF_IUCV_TRANS_IUCV) &&\n\t\t    iucv->path) {\n\t\t\terr = pr_iucv->path_quiesce(iucv->path, NULL);\n\t\t\tif (err)\n\t\t\t\terr = -ENOTCONN;\n \n\t\t}\n\t\tskb_queue_purge(&sk->sk_receive_queue);\n\t}\n\n\t \n\tsk->sk_state_change(sk);\n\nfail:\n\trelease_sock(sk);\n\treturn err;\n}\n\nstatic int iucv_sock_release(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\tint err = 0;\n\n\tif (!sk)\n\t\treturn 0;\n\n\tiucv_sock_close(sk);\n\n\tsock_orphan(sk);\n\tiucv_sock_kill(sk);\n\treturn err;\n}\n\n \nstatic int iucv_sock_setsockopt(struct socket *sock, int level, int optname,\n\t\t\t\tsockptr_t optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct iucv_sock *iucv = iucv_sk(sk);\n\tint val;\n\tint rc;\n\n\tif (level != SOL_IUCV)\n\t\treturn -ENOPROTOOPT;\n\n\tif (optlen < sizeof(int))\n\t\treturn -EINVAL;\n\n\tif (copy_from_sockptr(&val, optval, sizeof(int)))\n\t\treturn -EFAULT;\n\n\trc = 0;\n\n\tlock_sock(sk);\n\tswitch (optname) {\n\tcase SO_IPRMDATA_MSG:\n\t\tif (val)\n\t\t\tiucv->flags |= IUCV_IPRMDATA;\n\t\telse\n\t\t\tiucv->flags &= ~IUCV_IPRMDATA;\n\t\tbreak;\n\tcase SO_MSGLIMIT:\n\t\tswitch (sk->sk_state) {\n\t\tcase IUCV_OPEN:\n\t\tcase IUCV_BOUND:\n\t\t\tif (val < 1 || val > U16_MAX)\n\t\t\t\trc = -EINVAL;\n\t\t\telse\n\t\t\t\tiucv->msglimit = val;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\trc = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\trc = -ENOPROTOOPT;\n\t\tbreak;\n\t}\n\trelease_sock(sk);\n\n\treturn rc;\n}\n\nstatic int iucv_sock_getsockopt(struct socket *sock, int level, int optname,\n\t\t\t\tchar __user *optval, int __user *optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct iucv_sock *iucv = iucv_sk(sk);\n\tunsigned int val;\n\tint len;\n\n\tif (level != SOL_IUCV)\n\t\treturn -ENOPROTOOPT;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\n\tif (len < 0)\n\t\treturn -EINVAL;\n\n\tlen = min_t(unsigned int, len, sizeof(int));\n\n\tswitch (optname) {\n\tcase SO_IPRMDATA_MSG:\n\t\tval = (iucv->flags & IUCV_IPRMDATA) ? 1 : 0;\n\t\tbreak;\n\tcase SO_MSGLIMIT:\n\t\tlock_sock(sk);\n\t\tval = (iucv->path != NULL) ? iucv->path->msglim\t \n\t\t\t\t\t   : iucv->msglimit;\t \n\t\trelease_sock(sk);\n\t\tbreak;\n\tcase SO_MSGSIZE:\n\t\tif (sk->sk_state == IUCV_OPEN)\n\t\t\treturn -EBADFD;\n\t\tval = (iucv->hs_dev) ? iucv->hs_dev->mtu -\n\t\t\t\tsizeof(struct af_iucv_trans_hdr) - ETH_HLEN :\n\t\t\t\t0x7fffffff;\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (copy_to_user(optval, &val, len))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\n\n \nstatic int iucv_callback_connreq(struct iucv_path *path,\n\t\t\t\t u8 ipvmid[8], u8 ipuser[16])\n{\n\tunsigned char user_data[16];\n\tunsigned char nuser_data[16];\n\tunsigned char src_name[8];\n\tstruct sock *sk, *nsk;\n\tstruct iucv_sock *iucv, *niucv;\n\tint err;\n\n\tmemcpy(src_name, ipuser, 8);\n\tEBCASC(src_name, 8);\n\t \n\tread_lock(&iucv_sk_list.lock);\n\tiucv = NULL;\n\tsk = NULL;\n\tsk_for_each(sk, &iucv_sk_list.head)\n\t\tif (sk->sk_state == IUCV_LISTEN &&\n\t\t    !memcmp(&iucv_sk(sk)->src_name, src_name, 8)) {\n\t\t\t \n\t\t\tiucv = iucv_sk(sk);\n\t\t\tbreak;\n\t\t}\n\tread_unlock(&iucv_sk_list.lock);\n\tif (!iucv)\n\t\t \n\t\treturn -EINVAL;\n\n\tbh_lock_sock(sk);\n\n\t \n\tlow_nmcpy(user_data, iucv->src_name);\n\thigh_nmcpy(user_data, iucv->dst_name);\n\tASCEBC(user_data, sizeof(user_data));\n\tif (sk->sk_state != IUCV_LISTEN) {\n\t\terr = pr_iucv->path_sever(path, user_data);\n\t\tiucv_path_free(path);\n\t\tgoto fail;\n\t}\n\n\t \n\tif (sk_acceptq_is_full(sk)) {\n\t\terr = pr_iucv->path_sever(path, user_data);\n\t\tiucv_path_free(path);\n\t\tgoto fail;\n\t}\n\n\t \n\tnsk = iucv_sock_alloc(NULL, sk->sk_protocol, GFP_ATOMIC, 0);\n\tif (!nsk) {\n\t\terr = pr_iucv->path_sever(path, user_data);\n\t\tiucv_path_free(path);\n\t\tgoto fail;\n\t}\n\n\tniucv = iucv_sk(nsk);\n\tiucv_sock_init(nsk, sk);\n\tniucv->transport = AF_IUCV_TRANS_IUCV;\n\tnsk->sk_allocation |= GFP_DMA;\n\n\t \n\tmemcpy(niucv->dst_name, ipuser + 8, 8);\n\tEBCASC(niucv->dst_name, 8);\n\tmemcpy(niucv->dst_user_id, ipvmid, 8);\n\tmemcpy(niucv->src_name, iucv->src_name, 8);\n\tmemcpy(niucv->src_user_id, iucv->src_user_id, 8);\n\tniucv->path = path;\n\n\t \n\thigh_nmcpy(nuser_data, ipuser + 8);\n\tmemcpy(nuser_data + 8, niucv->src_name, 8);\n\tASCEBC(nuser_data + 8, 8);\n\n\t \n\tniucv->msglimit = iucv->msglimit;\n\tpath->msglim = iucv->msglimit;\n\terr = pr_iucv->path_accept(path, &af_iucv_handler, nuser_data, nsk);\n\tif (err) {\n\t\tiucv_sever_path(nsk, 1);\n\t\tiucv_sock_kill(nsk);\n\t\tgoto fail;\n\t}\n\n\tiucv_accept_enqueue(sk, nsk);\n\n\t \n\tnsk->sk_state = IUCV_CONNECTED;\n\tsk->sk_data_ready(sk);\n\terr = 0;\nfail:\n\tbh_unlock_sock(sk);\n\treturn 0;\n}\n\nstatic void iucv_callback_connack(struct iucv_path *path, u8 ipuser[16])\n{\n\tstruct sock *sk = path->private;\n\n\tsk->sk_state = IUCV_CONNECTED;\n\tsk->sk_state_change(sk);\n}\n\nstatic void iucv_callback_rx(struct iucv_path *path, struct iucv_message *msg)\n{\n\tstruct sock *sk = path->private;\n\tstruct iucv_sock *iucv = iucv_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct sock_msg_q *save_msg;\n\tint len;\n\n\tif (sk->sk_shutdown & RCV_SHUTDOWN) {\n\t\tpr_iucv->message_reject(path, msg);\n\t\treturn;\n\t}\n\n\tspin_lock(&iucv->message_q.lock);\n\n\tif (!list_empty(&iucv->message_q.list) ||\n\t    !skb_queue_empty(&iucv->backlog_skb_q))\n\t\tgoto save_message;\n\n\tlen = atomic_read(&sk->sk_rmem_alloc);\n\tlen += SKB_TRUESIZE(iucv_msg_length(msg));\n\tif (len > sk->sk_rcvbuf)\n\t\tgoto save_message;\n\n\tskb = alloc_iucv_recv_skb(iucv_msg_length(msg));\n\tif (!skb)\n\t\tgoto save_message;\n\n\tiucv_process_message(sk, skb, path, msg);\n\tgoto out_unlock;\n\nsave_message:\n\tsave_msg = kzalloc(sizeof(struct sock_msg_q), GFP_ATOMIC | GFP_DMA);\n\tif (!save_msg)\n\t\tgoto out_unlock;\n\tsave_msg->path = path;\n\tsave_msg->msg = *msg;\n\n\tlist_add_tail(&save_msg->list, &iucv->message_q.list);\n\nout_unlock:\n\tspin_unlock(&iucv->message_q.lock);\n}\n\nstatic void iucv_callback_txdone(struct iucv_path *path,\n\t\t\t\t struct iucv_message *msg)\n{\n\tstruct sock *sk = path->private;\n\tstruct sk_buff *this = NULL;\n\tstruct sk_buff_head *list;\n\tstruct sk_buff *list_skb;\n\tstruct iucv_sock *iucv;\n\tunsigned long flags;\n\n\tiucv = iucv_sk(sk);\n\tlist = &iucv->send_skb_q;\n\n\tbh_lock_sock(sk);\n\n\tspin_lock_irqsave(&list->lock, flags);\n\tskb_queue_walk(list, list_skb) {\n\t\tif (msg->tag == IUCV_SKB_CB(list_skb)->tag) {\n\t\t\tthis = list_skb;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (this) {\n\t\tatomic_dec(&iucv->skbs_in_xmit);\n\t\t__skb_unlink(this, list);\n\t}\n\n\tspin_unlock_irqrestore(&list->lock, flags);\n\n\tif (this) {\n\t\tconsume_skb(this);\n\t\t \n\t\tiucv_sock_wake_msglim(sk);\n\t}\n\n\tif (sk->sk_state == IUCV_CLOSING) {\n\t\tif (atomic_read(&iucv->skbs_in_xmit) == 0) {\n\t\t\tsk->sk_state = IUCV_CLOSED;\n\t\t\tsk->sk_state_change(sk);\n\t\t}\n\t}\n\tbh_unlock_sock(sk);\n\n}\n\nstatic void iucv_callback_connrej(struct iucv_path *path, u8 ipuser[16])\n{\n\tstruct sock *sk = path->private;\n\n\tif (sk->sk_state == IUCV_CLOSED)\n\t\treturn;\n\n\tbh_lock_sock(sk);\n\tiucv_sever_path(sk, 1);\n\tsk->sk_state = IUCV_DISCONN;\n\n\tsk->sk_state_change(sk);\n\tbh_unlock_sock(sk);\n}\n\n \nstatic void iucv_callback_shutdown(struct iucv_path *path, u8 ipuser[16])\n{\n\tstruct sock *sk = path->private;\n\n\tbh_lock_sock(sk);\n\tif (sk->sk_state != IUCV_CLOSED) {\n\t\tsk->sk_shutdown |= SEND_SHUTDOWN;\n\t\tsk->sk_state_change(sk);\n\t}\n\tbh_unlock_sock(sk);\n}\n\nstatic struct iucv_handler af_iucv_handler = {\n\t.path_pending\t\t= iucv_callback_connreq,\n\t.path_complete\t\t= iucv_callback_connack,\n\t.path_severed\t\t= iucv_callback_connrej,\n\t.message_pending\t= iucv_callback_rx,\n\t.message_complete\t= iucv_callback_txdone,\n\t.path_quiesced\t\t= iucv_callback_shutdown,\n};\n\n \nstatic void afiucv_swap_src_dest(struct sk_buff *skb)\n{\n\tstruct af_iucv_trans_hdr *trans_hdr = iucv_trans_hdr(skb);\n\tchar tmpID[8];\n\tchar tmpName[8];\n\n\tASCEBC(trans_hdr->destUserID, sizeof(trans_hdr->destUserID));\n\tASCEBC(trans_hdr->destAppName, sizeof(trans_hdr->destAppName));\n\tASCEBC(trans_hdr->srcUserID, sizeof(trans_hdr->srcUserID));\n\tASCEBC(trans_hdr->srcAppName, sizeof(trans_hdr->srcAppName));\n\tmemcpy(tmpID, trans_hdr->srcUserID, 8);\n\tmemcpy(tmpName, trans_hdr->srcAppName, 8);\n\tmemcpy(trans_hdr->srcUserID, trans_hdr->destUserID, 8);\n\tmemcpy(trans_hdr->srcAppName, trans_hdr->destAppName, 8);\n\tmemcpy(trans_hdr->destUserID, tmpID, 8);\n\tmemcpy(trans_hdr->destAppName, tmpName, 8);\n\tskb_push(skb, ETH_HLEN);\n\tmemset(skb->data, 0, ETH_HLEN);\n}\n\n \nstatic int afiucv_hs_callback_syn(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct af_iucv_trans_hdr *trans_hdr = iucv_trans_hdr(skb);\n\tstruct sock *nsk;\n\tstruct iucv_sock *iucv, *niucv;\n\tint err;\n\n\tiucv = iucv_sk(sk);\n\tif (!iucv) {\n\t\t \n\t\tafiucv_swap_src_dest(skb);\n\t\ttrans_hdr->flags = AF_IUCV_FLAG_SYN | AF_IUCV_FLAG_FIN;\n\t\terr = dev_queue_xmit(skb);\n\t\tgoto out;\n\t}\n\n\tnsk = iucv_sock_alloc(NULL, sk->sk_protocol, GFP_ATOMIC, 0);\n\tbh_lock_sock(sk);\n\tif ((sk->sk_state != IUCV_LISTEN) ||\n\t    sk_acceptq_is_full(sk) ||\n\t    !nsk) {\n\t\t \n\t\tafiucv_swap_src_dest(skb);\n\t\ttrans_hdr->flags = AF_IUCV_FLAG_SYN | AF_IUCV_FLAG_FIN;\n\t\terr = dev_queue_xmit(skb);\n\t\tiucv_sock_kill(nsk);\n\t\tbh_unlock_sock(sk);\n\t\tgoto out;\n\t}\n\n\tniucv = iucv_sk(nsk);\n\tiucv_sock_init(nsk, sk);\n\tniucv->transport = AF_IUCV_TRANS_HIPER;\n\tniucv->msglimit = iucv->msglimit;\n\tif (!trans_hdr->window)\n\t\tniucv->msglimit_peer = IUCV_HIPER_MSGLIM_DEFAULT;\n\telse\n\t\tniucv->msglimit_peer = trans_hdr->window;\n\tmemcpy(niucv->dst_name, trans_hdr->srcAppName, 8);\n\tmemcpy(niucv->dst_user_id, trans_hdr->srcUserID, 8);\n\tmemcpy(niucv->src_name, iucv->src_name, 8);\n\tmemcpy(niucv->src_user_id, iucv->src_user_id, 8);\n\tnsk->sk_bound_dev_if = sk->sk_bound_dev_if;\n\tniucv->hs_dev = iucv->hs_dev;\n\tdev_hold(niucv->hs_dev);\n\tafiucv_swap_src_dest(skb);\n\ttrans_hdr->flags = AF_IUCV_FLAG_SYN | AF_IUCV_FLAG_ACK;\n\ttrans_hdr->window = niucv->msglimit;\n\t \n\terr = dev_queue_xmit(skb);\n\tif (!err) {\n\t\tiucv_accept_enqueue(sk, nsk);\n\t\tnsk->sk_state = IUCV_CONNECTED;\n\t\tsk->sk_data_ready(sk);\n\t} else\n\t\tiucv_sock_kill(nsk);\n\tbh_unlock_sock(sk);\n\nout:\n\treturn NET_RX_SUCCESS;\n}\n\n \nstatic int afiucv_hs_callback_synack(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct iucv_sock *iucv = iucv_sk(sk);\n\n\tif (!iucv || sk->sk_state != IUCV_BOUND) {\n\t\tkfree_skb(skb);\n\t\treturn NET_RX_SUCCESS;\n\t}\n\n\tbh_lock_sock(sk);\n\tiucv->msglimit_peer = iucv_trans_hdr(skb)->window;\n\tsk->sk_state = IUCV_CONNECTED;\n\tsk->sk_state_change(sk);\n\tbh_unlock_sock(sk);\n\tconsume_skb(skb);\n\treturn NET_RX_SUCCESS;\n}\n\n \nstatic int afiucv_hs_callback_synfin(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct iucv_sock *iucv = iucv_sk(sk);\n\n\tif (!iucv || sk->sk_state != IUCV_BOUND) {\n\t\tkfree_skb(skb);\n\t\treturn NET_RX_SUCCESS;\n\t}\n\n\tbh_lock_sock(sk);\n\tsk->sk_state = IUCV_DISCONN;\n\tsk->sk_state_change(sk);\n\tbh_unlock_sock(sk);\n\tconsume_skb(skb);\n\treturn NET_RX_SUCCESS;\n}\n\n \nstatic int afiucv_hs_callback_fin(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct iucv_sock *iucv = iucv_sk(sk);\n\n\t \n\tif (!iucv) {\n\t\tkfree_skb(skb);\n\t\treturn NET_RX_SUCCESS;\n\t}\n\n\tbh_lock_sock(sk);\n\tif (sk->sk_state == IUCV_CONNECTED) {\n\t\tsk->sk_state = IUCV_DISCONN;\n\t\tsk->sk_state_change(sk);\n\t}\n\tbh_unlock_sock(sk);\n\tconsume_skb(skb);\n\treturn NET_RX_SUCCESS;\n}\n\n \nstatic int afiucv_hs_callback_win(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct iucv_sock *iucv = iucv_sk(sk);\n\n\tif (!iucv)\n\t\treturn NET_RX_SUCCESS;\n\n\tif (sk->sk_state != IUCV_CONNECTED)\n\t\treturn NET_RX_SUCCESS;\n\n\tatomic_sub(iucv_trans_hdr(skb)->window, &iucv->msg_sent);\n\tiucv_sock_wake_msglim(sk);\n\treturn NET_RX_SUCCESS;\n}\n\n \nstatic int afiucv_hs_callback_rx(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct iucv_sock *iucv = iucv_sk(sk);\n\n\tif (!iucv) {\n\t\tkfree_skb(skb);\n\t\treturn NET_RX_SUCCESS;\n\t}\n\n\tif (sk->sk_state != IUCV_CONNECTED) {\n\t\tkfree_skb(skb);\n\t\treturn NET_RX_SUCCESS;\n\t}\n\n\tif (sk->sk_shutdown & RCV_SHUTDOWN) {\n\t\tkfree_skb(skb);\n\t\treturn NET_RX_SUCCESS;\n\t}\n\n\t \n\tskb_pull(skb, sizeof(struct af_iucv_trans_hdr));\n\tskb_reset_transport_header(skb);\n\tskb_reset_network_header(skb);\n\tIUCV_SKB_CB(skb)->offset = 0;\n\tif (sk_filter(sk, skb)) {\n\t\tatomic_inc(&sk->sk_drops);\t \n\t\tkfree_skb(skb);\n\t\treturn NET_RX_SUCCESS;\n\t}\n\n\tspin_lock(&iucv->message_q.lock);\n\tif (skb_queue_empty(&iucv->backlog_skb_q)) {\n\t\tif (__sock_queue_rcv_skb(sk, skb))\n\t\t\t \n\t\t\tskb_queue_tail(&iucv->backlog_skb_q, skb);\n\t} else\n\t\tskb_queue_tail(&iucv_sk(sk)->backlog_skb_q, skb);\n\tspin_unlock(&iucv->message_q.lock);\n\treturn NET_RX_SUCCESS;\n}\n\n \nstatic int afiucv_hs_rcv(struct sk_buff *skb, struct net_device *dev,\n\tstruct packet_type *pt, struct net_device *orig_dev)\n{\n\tstruct sock *sk;\n\tstruct iucv_sock *iucv;\n\tstruct af_iucv_trans_hdr *trans_hdr;\n\tint err = NET_RX_SUCCESS;\n\tchar nullstring[8];\n\n\tif (!pskb_may_pull(skb, sizeof(*trans_hdr))) {\n\t\tkfree_skb(skb);\n\t\treturn NET_RX_SUCCESS;\n\t}\n\n\ttrans_hdr = iucv_trans_hdr(skb);\n\tEBCASC(trans_hdr->destAppName, sizeof(trans_hdr->destAppName));\n\tEBCASC(trans_hdr->destUserID, sizeof(trans_hdr->destUserID));\n\tEBCASC(trans_hdr->srcAppName, sizeof(trans_hdr->srcAppName));\n\tEBCASC(trans_hdr->srcUserID, sizeof(trans_hdr->srcUserID));\n\tmemset(nullstring, 0, sizeof(nullstring));\n\tiucv = NULL;\n\tsk = NULL;\n\tread_lock(&iucv_sk_list.lock);\n\tsk_for_each(sk, &iucv_sk_list.head) {\n\t\tif (trans_hdr->flags == AF_IUCV_FLAG_SYN) {\n\t\t\tif ((!memcmp(&iucv_sk(sk)->src_name,\n\t\t\t\t     trans_hdr->destAppName, 8)) &&\n\t\t\t    (!memcmp(&iucv_sk(sk)->src_user_id,\n\t\t\t\t     trans_hdr->destUserID, 8)) &&\n\t\t\t    (!memcmp(&iucv_sk(sk)->dst_name, nullstring, 8)) &&\n\t\t\t    (!memcmp(&iucv_sk(sk)->dst_user_id,\n\t\t\t\t     nullstring, 8))) {\n\t\t\t\tiucv = iucv_sk(sk);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} else {\n\t\t\tif ((!memcmp(&iucv_sk(sk)->src_name,\n\t\t\t\t     trans_hdr->destAppName, 8)) &&\n\t\t\t    (!memcmp(&iucv_sk(sk)->src_user_id,\n\t\t\t\t     trans_hdr->destUserID, 8)) &&\n\t\t\t    (!memcmp(&iucv_sk(sk)->dst_name,\n\t\t\t\t     trans_hdr->srcAppName, 8)) &&\n\t\t\t    (!memcmp(&iucv_sk(sk)->dst_user_id,\n\t\t\t\t     trans_hdr->srcUserID, 8))) {\n\t\t\t\tiucv = iucv_sk(sk);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\tread_unlock(&iucv_sk_list.lock);\n\tif (!iucv)\n\t\tsk = NULL;\n\n\t \n\n\tswitch (trans_hdr->flags) {\n\tcase AF_IUCV_FLAG_SYN:\n\t\t \n\t\terr = afiucv_hs_callback_syn(sk, skb);\n\t\tbreak;\n\tcase (AF_IUCV_FLAG_SYN | AF_IUCV_FLAG_ACK):\n\t\t \n\t\terr = afiucv_hs_callback_synack(sk, skb);\n\t\tbreak;\n\tcase (AF_IUCV_FLAG_SYN | AF_IUCV_FLAG_FIN):\n\t\t \n\t\terr = afiucv_hs_callback_synfin(sk, skb);\n\t\tbreak;\n\tcase (AF_IUCV_FLAG_FIN):\n\t\t \n\t\terr = afiucv_hs_callback_fin(sk, skb);\n\t\tbreak;\n\tcase (AF_IUCV_FLAG_WIN):\n\t\terr = afiucv_hs_callback_win(sk, skb);\n\t\tif (skb->len == sizeof(struct af_iucv_trans_hdr)) {\n\t\t\tconsume_skb(skb);\n\t\t\tbreak;\n\t\t}\n\t\tfallthrough;\t \n\tcase (AF_IUCV_FLAG_SHT):\n\t\t \n\t\tfallthrough;\t \n\tcase 0:\n\t\t \n\t\tIUCV_SKB_CB(skb)->class = trans_hdr->iucv_hdr.class;\n\t\terr = afiucv_hs_callback_rx(sk, skb);\n\t\tbreak;\n\tdefault:\n\t\tkfree_skb(skb);\n\t}\n\n\treturn err;\n}\n\n \nstatic void afiucv_hs_callback_txnotify(struct sock *sk, enum iucv_tx_notify n)\n{\n\tstruct iucv_sock *iucv = iucv_sk(sk);\n\n\tif (sock_flag(sk, SOCK_ZAPPED))\n\t\treturn;\n\n\tswitch (n) {\n\tcase TX_NOTIFY_OK:\n\t\tatomic_dec(&iucv->skbs_in_xmit);\n\t\tiucv_sock_wake_msglim(sk);\n\t\tbreak;\n\tcase TX_NOTIFY_PENDING:\n\t\tatomic_inc(&iucv->pendings);\n\t\tbreak;\n\tcase TX_NOTIFY_DELAYED_OK:\n\t\tatomic_dec(&iucv->skbs_in_xmit);\n\t\tif (atomic_dec_return(&iucv->pendings) <= 0)\n\t\t\tiucv_sock_wake_msglim(sk);\n\t\tbreak;\n\tdefault:\n\t\tatomic_dec(&iucv->skbs_in_xmit);\n\t\tif (sk->sk_state == IUCV_CONNECTED) {\n\t\t\tsk->sk_state = IUCV_DISCONN;\n\t\t\tsk->sk_state_change(sk);\n\t\t}\n\t}\n\n\tif (sk->sk_state == IUCV_CLOSING) {\n\t\tif (atomic_read(&iucv->skbs_in_xmit) == 0) {\n\t\t\tsk->sk_state = IUCV_CLOSED;\n\t\t\tsk->sk_state_change(sk);\n\t\t}\n\t}\n}\n\n \nstatic int afiucv_netdev_event(struct notifier_block *this,\n\t\t\t       unsigned long event, void *ptr)\n{\n\tstruct net_device *event_dev = netdev_notifier_info_to_dev(ptr);\n\tstruct sock *sk;\n\tstruct iucv_sock *iucv;\n\n\tswitch (event) {\n\tcase NETDEV_REBOOT:\n\tcase NETDEV_GOING_DOWN:\n\t\tsk_for_each(sk, &iucv_sk_list.head) {\n\t\t\tiucv = iucv_sk(sk);\n\t\t\tif ((iucv->hs_dev == event_dev) &&\n\t\t\t    (sk->sk_state == IUCV_CONNECTED)) {\n\t\t\t\tif (event == NETDEV_GOING_DOWN)\n\t\t\t\t\tiucv_send_ctrl(sk, AF_IUCV_FLAG_FIN);\n\t\t\t\tsk->sk_state = IUCV_DISCONN;\n\t\t\t\tsk->sk_state_change(sk);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase NETDEV_DOWN:\n\tcase NETDEV_UNREGISTER:\n\tdefault:\n\t\tbreak;\n\t}\n\treturn NOTIFY_DONE;\n}\n\nstatic struct notifier_block afiucv_netdev_notifier = {\n\t.notifier_call = afiucv_netdev_event,\n};\n\nstatic const struct proto_ops iucv_sock_ops = {\n\t.family\t\t= PF_IUCV,\n\t.owner\t\t= THIS_MODULE,\n\t.release\t= iucv_sock_release,\n\t.bind\t\t= iucv_sock_bind,\n\t.connect\t= iucv_sock_connect,\n\t.listen\t\t= iucv_sock_listen,\n\t.accept\t\t= iucv_sock_accept,\n\t.getname\t= iucv_sock_getname,\n\t.sendmsg\t= iucv_sock_sendmsg,\n\t.recvmsg\t= iucv_sock_recvmsg,\n\t.poll\t\t= iucv_sock_poll,\n\t.ioctl\t\t= sock_no_ioctl,\n\t.mmap\t\t= sock_no_mmap,\n\t.socketpair\t= sock_no_socketpair,\n\t.shutdown\t= iucv_sock_shutdown,\n\t.setsockopt\t= iucv_sock_setsockopt,\n\t.getsockopt\t= iucv_sock_getsockopt,\n};\n\nstatic int iucv_sock_create(struct net *net, struct socket *sock, int protocol,\n\t\t\t    int kern)\n{\n\tstruct sock *sk;\n\n\tif (protocol && protocol != PF_IUCV)\n\t\treturn -EPROTONOSUPPORT;\n\n\tsock->state = SS_UNCONNECTED;\n\n\tswitch (sock->type) {\n\tcase SOCK_STREAM:\n\tcase SOCK_SEQPACKET:\n\t\t \n\t\tsock->ops = &iucv_sock_ops;\n\t\tbreak;\n\tdefault:\n\t\treturn -ESOCKTNOSUPPORT;\n\t}\n\n\tsk = iucv_sock_alloc(sock, protocol, GFP_KERNEL, kern);\n\tif (!sk)\n\t\treturn -ENOMEM;\n\n\tiucv_sock_init(sk, NULL);\n\n\treturn 0;\n}\n\nstatic const struct net_proto_family iucv_sock_family_ops = {\n\t.family\t= AF_IUCV,\n\t.owner\t= THIS_MODULE,\n\t.create\t= iucv_sock_create,\n};\n\nstatic struct packet_type iucv_packet_type = {\n\t.type = cpu_to_be16(ETH_P_AF_IUCV),\n\t.func = afiucv_hs_rcv,\n};\n\nstatic int __init afiucv_init(void)\n{\n\tint err;\n\n\tif (MACHINE_IS_VM && IS_ENABLED(CONFIG_IUCV)) {\n\t\tcpcmd(\"QUERY USERID\", iucv_userid, sizeof(iucv_userid), &err);\n\t\tif (unlikely(err)) {\n\t\t\tWARN_ON(err);\n\t\t\terr = -EPROTONOSUPPORT;\n\t\t\tgoto out;\n\t\t}\n\n\t\tpr_iucv = &iucv_if;\n\t} else {\n\t\tmemset(&iucv_userid, 0, sizeof(iucv_userid));\n\t\tpr_iucv = NULL;\n\t}\n\n\terr = proto_register(&iucv_proto, 0);\n\tif (err)\n\t\tgoto out;\n\terr = sock_register(&iucv_sock_family_ops);\n\tif (err)\n\t\tgoto out_proto;\n\n\tif (pr_iucv) {\n\t\terr = pr_iucv->iucv_register(&af_iucv_handler, 0);\n\t\tif (err)\n\t\t\tgoto out_sock;\n\t}\n\n\terr = register_netdevice_notifier(&afiucv_netdev_notifier);\n\tif (err)\n\t\tgoto out_notifier;\n\n\tdev_add_pack(&iucv_packet_type);\n\treturn 0;\n\nout_notifier:\n\tif (pr_iucv)\n\t\tpr_iucv->iucv_unregister(&af_iucv_handler, 0);\nout_sock:\n\tsock_unregister(PF_IUCV);\nout_proto:\n\tproto_unregister(&iucv_proto);\nout:\n\treturn err;\n}\n\nstatic void __exit afiucv_exit(void)\n{\n\tif (pr_iucv)\n\t\tpr_iucv->iucv_unregister(&af_iucv_handler, 0);\n\n\tunregister_netdevice_notifier(&afiucv_netdev_notifier);\n\tdev_remove_pack(&iucv_packet_type);\n\tsock_unregister(PF_IUCV);\n\tproto_unregister(&iucv_proto);\n}\n\nmodule_init(afiucv_init);\nmodule_exit(afiucv_exit);\n\nMODULE_AUTHOR(\"Jennifer Hunt <jenhunt@us.ibm.com>\");\nMODULE_DESCRIPTION(\"IUCV Sockets ver \" VERSION);\nMODULE_VERSION(VERSION);\nMODULE_LICENSE(\"GPL\");\nMODULE_ALIAS_NETPROTO(PF_IUCV);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}