{
  "module_name": "af_netlink.c",
  "hash_id": "730a58ecfed438f122421ba2002a8a8106a21338e0eab7ece7f56b4d3f39cbd5",
  "original_prompt": "Ingested from linux-6.6.14/net/netlink/af_netlink.c",
  "human_readable_source": "\n \n\n#include <linux/module.h>\n\n#include <linux/bpf.h>\n#include <linux/capability.h>\n#include <linux/kernel.h>\n#include <linux/filter.h>\n#include <linux/init.h>\n#include <linux/signal.h>\n#include <linux/sched.h>\n#include <linux/errno.h>\n#include <linux/string.h>\n#include <linux/stat.h>\n#include <linux/socket.h>\n#include <linux/un.h>\n#include <linux/fcntl.h>\n#include <linux/termios.h>\n#include <linux/sockios.h>\n#include <linux/net.h>\n#include <linux/fs.h>\n#include <linux/slab.h>\n#include <linux/uaccess.h>\n#include <linux/skbuff.h>\n#include <linux/netdevice.h>\n#include <linux/rtnetlink.h>\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <linux/notifier.h>\n#include <linux/security.h>\n#include <linux/jhash.h>\n#include <linux/jiffies.h>\n#include <linux/random.h>\n#include <linux/bitops.h>\n#include <linux/mm.h>\n#include <linux/types.h>\n#include <linux/audit.h>\n#include <linux/mutex.h>\n#include <linux/vmalloc.h>\n#include <linux/if_arp.h>\n#include <linux/rhashtable.h>\n#include <asm/cacheflush.h>\n#include <linux/hash.h>\n#include <linux/genetlink.h>\n#include <linux/net_namespace.h>\n#include <linux/nospec.h>\n#include <linux/btf_ids.h>\n\n#include <net/net_namespace.h>\n#include <net/netns/generic.h>\n#include <net/sock.h>\n#include <net/scm.h>\n#include <net/netlink.h>\n#define CREATE_TRACE_POINTS\n#include <trace/events/netlink.h>\n\n#include \"af_netlink.h\"\n\nstruct listeners {\n\tstruct rcu_head\t\trcu;\n\tunsigned long\t\tmasks[];\n};\n\n \n#define NETLINK_S_CONGESTED\t\t0x0\n\nstatic inline int netlink_is_kernel(struct sock *sk)\n{\n\treturn nlk_test_bit(KERNEL_SOCKET, sk);\n}\n\nstruct netlink_table *nl_table __read_mostly;\nEXPORT_SYMBOL_GPL(nl_table);\n\nstatic DECLARE_WAIT_QUEUE_HEAD(nl_table_wait);\n\nstatic struct lock_class_key nlk_cb_mutex_keys[MAX_LINKS];\n\nstatic const char *const nlk_cb_mutex_key_strings[MAX_LINKS + 1] = {\n\t\"nlk_cb_mutex-ROUTE\",\n\t\"nlk_cb_mutex-1\",\n\t\"nlk_cb_mutex-USERSOCK\",\n\t\"nlk_cb_mutex-FIREWALL\",\n\t\"nlk_cb_mutex-SOCK_DIAG\",\n\t\"nlk_cb_mutex-NFLOG\",\n\t\"nlk_cb_mutex-XFRM\",\n\t\"nlk_cb_mutex-SELINUX\",\n\t\"nlk_cb_mutex-ISCSI\",\n\t\"nlk_cb_mutex-AUDIT\",\n\t\"nlk_cb_mutex-FIB_LOOKUP\",\n\t\"nlk_cb_mutex-CONNECTOR\",\n\t\"nlk_cb_mutex-NETFILTER\",\n\t\"nlk_cb_mutex-IP6_FW\",\n\t\"nlk_cb_mutex-DNRTMSG\",\n\t\"nlk_cb_mutex-KOBJECT_UEVENT\",\n\t\"nlk_cb_mutex-GENERIC\",\n\t\"nlk_cb_mutex-17\",\n\t\"nlk_cb_mutex-SCSITRANSPORT\",\n\t\"nlk_cb_mutex-ECRYPTFS\",\n\t\"nlk_cb_mutex-RDMA\",\n\t\"nlk_cb_mutex-CRYPTO\",\n\t\"nlk_cb_mutex-SMC\",\n\t\"nlk_cb_mutex-23\",\n\t\"nlk_cb_mutex-24\",\n\t\"nlk_cb_mutex-25\",\n\t\"nlk_cb_mutex-26\",\n\t\"nlk_cb_mutex-27\",\n\t\"nlk_cb_mutex-28\",\n\t\"nlk_cb_mutex-29\",\n\t\"nlk_cb_mutex-30\",\n\t\"nlk_cb_mutex-31\",\n\t\"nlk_cb_mutex-MAX_LINKS\"\n};\n\nstatic int netlink_dump(struct sock *sk);\n\n \nDEFINE_RWLOCK(nl_table_lock);\nEXPORT_SYMBOL_GPL(nl_table_lock);\nstatic atomic_t nl_table_users = ATOMIC_INIT(0);\n\n#define nl_deref_protected(X) rcu_dereference_protected(X, lockdep_is_held(&nl_table_lock));\n\nstatic BLOCKING_NOTIFIER_HEAD(netlink_chain);\n\n\nstatic const struct rhashtable_params netlink_rhashtable_params;\n\nvoid do_trace_netlink_extack(const char *msg)\n{\n\ttrace_netlink_extack(msg);\n}\nEXPORT_SYMBOL(do_trace_netlink_extack);\n\nstatic inline u32 netlink_group_mask(u32 group)\n{\n\tif (group > 32)\n\t\treturn 0;\n\treturn group ? 1 << (group - 1) : 0;\n}\n\nstatic struct sk_buff *netlink_to_full_skb(const struct sk_buff *skb,\n\t\t\t\t\t   gfp_t gfp_mask)\n{\n\tunsigned int len = skb_end_offset(skb);\n\tstruct sk_buff *new;\n\n\tnew = alloc_skb(len, gfp_mask);\n\tif (new == NULL)\n\t\treturn NULL;\n\n\tNETLINK_CB(new).portid = NETLINK_CB(skb).portid;\n\tNETLINK_CB(new).dst_group = NETLINK_CB(skb).dst_group;\n\tNETLINK_CB(new).creds = NETLINK_CB(skb).creds;\n\n\tskb_put_data(new, skb->data, len);\n\treturn new;\n}\n\nstatic unsigned int netlink_tap_net_id;\n\nstruct netlink_tap_net {\n\tstruct list_head netlink_tap_all;\n\tstruct mutex netlink_tap_lock;\n};\n\nint netlink_add_tap(struct netlink_tap *nt)\n{\n\tstruct net *net = dev_net(nt->dev);\n\tstruct netlink_tap_net *nn = net_generic(net, netlink_tap_net_id);\n\n\tif (unlikely(nt->dev->type != ARPHRD_NETLINK))\n\t\treturn -EINVAL;\n\n\tmutex_lock(&nn->netlink_tap_lock);\n\tlist_add_rcu(&nt->list, &nn->netlink_tap_all);\n\tmutex_unlock(&nn->netlink_tap_lock);\n\n\t__module_get(nt->module);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(netlink_add_tap);\n\nstatic int __netlink_remove_tap(struct netlink_tap *nt)\n{\n\tstruct net *net = dev_net(nt->dev);\n\tstruct netlink_tap_net *nn = net_generic(net, netlink_tap_net_id);\n\tbool found = false;\n\tstruct netlink_tap *tmp;\n\n\tmutex_lock(&nn->netlink_tap_lock);\n\n\tlist_for_each_entry(tmp, &nn->netlink_tap_all, list) {\n\t\tif (nt == tmp) {\n\t\t\tlist_del_rcu(&nt->list);\n\t\t\tfound = true;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tpr_warn(\"__netlink_remove_tap: %p not found\\n\", nt);\nout:\n\tmutex_unlock(&nn->netlink_tap_lock);\n\n\tif (found)\n\t\tmodule_put(nt->module);\n\n\treturn found ? 0 : -ENODEV;\n}\n\nint netlink_remove_tap(struct netlink_tap *nt)\n{\n\tint ret;\n\n\tret = __netlink_remove_tap(nt);\n\tsynchronize_net();\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(netlink_remove_tap);\n\nstatic __net_init int netlink_tap_init_net(struct net *net)\n{\n\tstruct netlink_tap_net *nn = net_generic(net, netlink_tap_net_id);\n\n\tINIT_LIST_HEAD(&nn->netlink_tap_all);\n\tmutex_init(&nn->netlink_tap_lock);\n\treturn 0;\n}\n\nstatic struct pernet_operations netlink_tap_net_ops = {\n\t.init = netlink_tap_init_net,\n\t.id   = &netlink_tap_net_id,\n\t.size = sizeof(struct netlink_tap_net),\n};\n\nstatic bool netlink_filter_tap(const struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\n\t \n\tswitch (sk->sk_protocol) {\n\tcase NETLINK_ROUTE:\n\tcase NETLINK_USERSOCK:\n\tcase NETLINK_SOCK_DIAG:\n\tcase NETLINK_NFLOG:\n\tcase NETLINK_XFRM:\n\tcase NETLINK_FIB_LOOKUP:\n\tcase NETLINK_NETFILTER:\n\tcase NETLINK_GENERIC:\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic int __netlink_deliver_tap_skb(struct sk_buff *skb,\n\t\t\t\t     struct net_device *dev)\n{\n\tstruct sk_buff *nskb;\n\tstruct sock *sk = skb->sk;\n\tint ret = -ENOMEM;\n\n\tif (!net_eq(dev_net(dev), sock_net(sk)))\n\t\treturn 0;\n\n\tdev_hold(dev);\n\n\tif (is_vmalloc_addr(skb->head))\n\t\tnskb = netlink_to_full_skb(skb, GFP_ATOMIC);\n\telse\n\t\tnskb = skb_clone(skb, GFP_ATOMIC);\n\tif (nskb) {\n\t\tnskb->dev = dev;\n\t\tnskb->protocol = htons((u16) sk->sk_protocol);\n\t\tnskb->pkt_type = netlink_is_kernel(sk) ?\n\t\t\t\t PACKET_KERNEL : PACKET_USER;\n\t\tskb_reset_network_header(nskb);\n\t\tret = dev_queue_xmit(nskb);\n\t\tif (unlikely(ret > 0))\n\t\t\tret = net_xmit_errno(ret);\n\t}\n\n\tdev_put(dev);\n\treturn ret;\n}\n\nstatic void __netlink_deliver_tap(struct sk_buff *skb, struct netlink_tap_net *nn)\n{\n\tint ret;\n\tstruct netlink_tap *tmp;\n\n\tif (!netlink_filter_tap(skb))\n\t\treturn;\n\n\tlist_for_each_entry_rcu(tmp, &nn->netlink_tap_all, list) {\n\t\tret = __netlink_deliver_tap_skb(skb, tmp->dev);\n\t\tif (unlikely(ret))\n\t\t\tbreak;\n\t}\n}\n\nstatic void netlink_deliver_tap(struct net *net, struct sk_buff *skb)\n{\n\tstruct netlink_tap_net *nn = net_generic(net, netlink_tap_net_id);\n\n\trcu_read_lock();\n\n\tif (unlikely(!list_empty(&nn->netlink_tap_all)))\n\t\t__netlink_deliver_tap(skb, nn);\n\n\trcu_read_unlock();\n}\n\nstatic void netlink_deliver_tap_kernel(struct sock *dst, struct sock *src,\n\t\t\t\t       struct sk_buff *skb)\n{\n\tif (!(netlink_is_kernel(dst) && netlink_is_kernel(src)))\n\t\tnetlink_deliver_tap(sock_net(dst), skb);\n}\n\nstatic void netlink_overrun(struct sock *sk)\n{\n\tif (!nlk_test_bit(RECV_NO_ENOBUFS, sk)) {\n\t\tif (!test_and_set_bit(NETLINK_S_CONGESTED,\n\t\t\t\t      &nlk_sk(sk)->state)) {\n\t\t\tWRITE_ONCE(sk->sk_err, ENOBUFS);\n\t\t\tsk_error_report(sk);\n\t\t}\n\t}\n\tatomic_inc(&sk->sk_drops);\n}\n\nstatic void netlink_rcv_wake(struct sock *sk)\n{\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\n\tif (skb_queue_empty_lockless(&sk->sk_receive_queue))\n\t\tclear_bit(NETLINK_S_CONGESTED, &nlk->state);\n\tif (!test_bit(NETLINK_S_CONGESTED, &nlk->state))\n\t\twake_up_interruptible(&nlk->wait);\n}\n\nstatic void netlink_skb_destructor(struct sk_buff *skb)\n{\n\tif (is_vmalloc_addr(skb->head)) {\n\t\tif (!skb->cloned ||\n\t\t    !atomic_dec_return(&(skb_shinfo(skb)->dataref)))\n\t\t\tvfree(skb->head);\n\n\t\tskb->head = NULL;\n\t}\n\tif (skb->sk != NULL)\n\t\tsock_rfree(skb);\n}\n\nstatic void netlink_skb_set_owner_r(struct sk_buff *skb, struct sock *sk)\n{\n\tWARN_ON(skb->sk != NULL);\n\tskb->sk = sk;\n\tskb->destructor = netlink_skb_destructor;\n\tatomic_add(skb->truesize, &sk->sk_rmem_alloc);\n\tsk_mem_charge(sk, skb->truesize);\n}\n\nstatic void netlink_sock_destruct(struct sock *sk)\n{\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\n\tif (nlk->cb_running) {\n\t\tif (nlk->cb.done)\n\t\t\tnlk->cb.done(&nlk->cb);\n\t\tmodule_put(nlk->cb.module);\n\t\tkfree_skb(nlk->cb.skb);\n\t}\n\n\tskb_queue_purge(&sk->sk_receive_queue);\n\n\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\tprintk(KERN_ERR \"Freeing alive netlink socket %p\\n\", sk);\n\t\treturn;\n\t}\n\n\tWARN_ON(atomic_read(&sk->sk_rmem_alloc));\n\tWARN_ON(refcount_read(&sk->sk_wmem_alloc));\n\tWARN_ON(nlk_sk(sk)->groups);\n}\n\nstatic void netlink_sock_destruct_work(struct work_struct *work)\n{\n\tstruct netlink_sock *nlk = container_of(work, struct netlink_sock,\n\t\t\t\t\t\twork);\n\n\tsk_free(&nlk->sk);\n}\n\n \n\nvoid netlink_table_grab(void)\n\t__acquires(nl_table_lock)\n{\n\tmight_sleep();\n\n\twrite_lock_irq(&nl_table_lock);\n\n\tif (atomic_read(&nl_table_users)) {\n\t\tDECLARE_WAITQUEUE(wait, current);\n\n\t\tadd_wait_queue_exclusive(&nl_table_wait, &wait);\n\t\tfor (;;) {\n\t\t\tset_current_state(TASK_UNINTERRUPTIBLE);\n\t\t\tif (atomic_read(&nl_table_users) == 0)\n\t\t\t\tbreak;\n\t\t\twrite_unlock_irq(&nl_table_lock);\n\t\t\tschedule();\n\t\t\twrite_lock_irq(&nl_table_lock);\n\t\t}\n\n\t\t__set_current_state(TASK_RUNNING);\n\t\tremove_wait_queue(&nl_table_wait, &wait);\n\t}\n}\n\nvoid netlink_table_ungrab(void)\n\t__releases(nl_table_lock)\n{\n\twrite_unlock_irq(&nl_table_lock);\n\twake_up(&nl_table_wait);\n}\n\nstatic inline void\nnetlink_lock_table(void)\n{\n\tunsigned long flags;\n\n\t \n\n\tread_lock_irqsave(&nl_table_lock, flags);\n\tatomic_inc(&nl_table_users);\n\tread_unlock_irqrestore(&nl_table_lock, flags);\n}\n\nstatic inline void\nnetlink_unlock_table(void)\n{\n\tif (atomic_dec_and_test(&nl_table_users))\n\t\twake_up(&nl_table_wait);\n}\n\nstruct netlink_compare_arg\n{\n\tpossible_net_t pnet;\n\tu32 portid;\n};\n\n \n#define netlink_compare_arg_len \\\n\t(offsetof(struct netlink_compare_arg, portid) + sizeof(u32))\n\nstatic inline int netlink_compare(struct rhashtable_compare_arg *arg,\n\t\t\t\t  const void *ptr)\n{\n\tconst struct netlink_compare_arg *x = arg->key;\n\tconst struct netlink_sock *nlk = ptr;\n\n\treturn nlk->portid != x->portid ||\n\t       !net_eq(sock_net(&nlk->sk), read_pnet(&x->pnet));\n}\n\nstatic void netlink_compare_arg_init(struct netlink_compare_arg *arg,\n\t\t\t\t     struct net *net, u32 portid)\n{\n\tmemset(arg, 0, sizeof(*arg));\n\twrite_pnet(&arg->pnet, net);\n\targ->portid = portid;\n}\n\nstatic struct sock *__netlink_lookup(struct netlink_table *table, u32 portid,\n\t\t\t\t     struct net *net)\n{\n\tstruct netlink_compare_arg arg;\n\n\tnetlink_compare_arg_init(&arg, net, portid);\n\treturn rhashtable_lookup_fast(&table->hash, &arg,\n\t\t\t\t      netlink_rhashtable_params);\n}\n\nstatic int __netlink_insert(struct netlink_table *table, struct sock *sk)\n{\n\tstruct netlink_compare_arg arg;\n\n\tnetlink_compare_arg_init(&arg, sock_net(sk), nlk_sk(sk)->portid);\n\treturn rhashtable_lookup_insert_key(&table->hash, &arg,\n\t\t\t\t\t    &nlk_sk(sk)->node,\n\t\t\t\t\t    netlink_rhashtable_params);\n}\n\nstatic struct sock *netlink_lookup(struct net *net, int protocol, u32 portid)\n{\n\tstruct netlink_table *table = &nl_table[protocol];\n\tstruct sock *sk;\n\n\trcu_read_lock();\n\tsk = __netlink_lookup(table, portid, net);\n\tif (sk)\n\t\tsock_hold(sk);\n\trcu_read_unlock();\n\n\treturn sk;\n}\n\nstatic const struct proto_ops netlink_ops;\n\nstatic void\nnetlink_update_listeners(struct sock *sk)\n{\n\tstruct netlink_table *tbl = &nl_table[sk->sk_protocol];\n\tunsigned long mask;\n\tunsigned int i;\n\tstruct listeners *listeners;\n\n\tlisteners = nl_deref_protected(tbl->listeners);\n\tif (!listeners)\n\t\treturn;\n\n\tfor (i = 0; i < NLGRPLONGS(tbl->groups); i++) {\n\t\tmask = 0;\n\t\tsk_for_each_bound(sk, &tbl->mc_list) {\n\t\t\tif (i < NLGRPLONGS(nlk_sk(sk)->ngroups))\n\t\t\t\tmask |= nlk_sk(sk)->groups[i];\n\t\t}\n\t\tlisteners->masks[i] = mask;\n\t}\n\t \n}\n\nstatic int netlink_insert(struct sock *sk, u32 portid)\n{\n\tstruct netlink_table *table = &nl_table[sk->sk_protocol];\n\tint err;\n\n\tlock_sock(sk);\n\n\terr = nlk_sk(sk)->portid == portid ? 0 : -EBUSY;\n\tif (nlk_sk(sk)->bound)\n\t\tgoto err;\n\n\t \n\tWRITE_ONCE(nlk_sk(sk)->portid, portid);\n\n\tsock_hold(sk);\n\n\terr = __netlink_insert(table, sk);\n\tif (err) {\n\t\t \n\t\tif (unlikely(err == -EBUSY))\n\t\t\terr = -EOVERFLOW;\n\t\tif (err == -EEXIST)\n\t\t\terr = -EADDRINUSE;\n\t\tsock_put(sk);\n\t\tgoto err;\n\t}\n\n\t \n\tsmp_wmb();\n\t \n\tWRITE_ONCE(nlk_sk(sk)->bound, portid);\n\nerr:\n\trelease_sock(sk);\n\treturn err;\n}\n\nstatic void netlink_remove(struct sock *sk)\n{\n\tstruct netlink_table *table;\n\n\ttable = &nl_table[sk->sk_protocol];\n\tif (!rhashtable_remove_fast(&table->hash, &nlk_sk(sk)->node,\n\t\t\t\t    netlink_rhashtable_params)) {\n\t\tWARN_ON(refcount_read(&sk->sk_refcnt) == 1);\n\t\t__sock_put(sk);\n\t}\n\n\tnetlink_table_grab();\n\tif (nlk_sk(sk)->subscriptions) {\n\t\t__sk_del_bind_node(sk);\n\t\tnetlink_update_listeners(sk);\n\t}\n\tif (sk->sk_protocol == NETLINK_GENERIC)\n\t\tatomic_inc(&genl_sk_destructing_cnt);\n\tnetlink_table_ungrab();\n}\n\nstatic struct proto netlink_proto = {\n\t.name\t  = \"NETLINK\",\n\t.owner\t  = THIS_MODULE,\n\t.obj_size = sizeof(struct netlink_sock),\n};\n\nstatic int __netlink_create(struct net *net, struct socket *sock,\n\t\t\t    struct mutex *cb_mutex, int protocol,\n\t\t\t    int kern)\n{\n\tstruct sock *sk;\n\tstruct netlink_sock *nlk;\n\n\tsock->ops = &netlink_ops;\n\n\tsk = sk_alloc(net, PF_NETLINK, GFP_KERNEL, &netlink_proto, kern);\n\tif (!sk)\n\t\treturn -ENOMEM;\n\n\tsock_init_data(sock, sk);\n\n\tnlk = nlk_sk(sk);\n\tif (cb_mutex) {\n\t\tnlk->cb_mutex = cb_mutex;\n\t} else {\n\t\tnlk->cb_mutex = &nlk->cb_def_mutex;\n\t\tmutex_init(nlk->cb_mutex);\n\t\tlockdep_set_class_and_name(nlk->cb_mutex,\n\t\t\t\t\t   nlk_cb_mutex_keys + protocol,\n\t\t\t\t\t   nlk_cb_mutex_key_strings[protocol]);\n\t}\n\tinit_waitqueue_head(&nlk->wait);\n\n\tsk->sk_destruct = netlink_sock_destruct;\n\tsk->sk_protocol = protocol;\n\treturn 0;\n}\n\nstatic int netlink_create(struct net *net, struct socket *sock, int protocol,\n\t\t\t  int kern)\n{\n\tstruct module *module = NULL;\n\tstruct mutex *cb_mutex;\n\tstruct netlink_sock *nlk;\n\tint (*bind)(struct net *net, int group);\n\tvoid (*unbind)(struct net *net, int group);\n\tvoid (*release)(struct sock *sock, unsigned long *groups);\n\tint err = 0;\n\n\tsock->state = SS_UNCONNECTED;\n\n\tif (sock->type != SOCK_RAW && sock->type != SOCK_DGRAM)\n\t\treturn -ESOCKTNOSUPPORT;\n\n\tif (protocol < 0 || protocol >= MAX_LINKS)\n\t\treturn -EPROTONOSUPPORT;\n\tprotocol = array_index_nospec(protocol, MAX_LINKS);\n\n\tnetlink_lock_table();\n#ifdef CONFIG_MODULES\n\tif (!nl_table[protocol].registered) {\n\t\tnetlink_unlock_table();\n\t\trequest_module(\"net-pf-%d-proto-%d\", PF_NETLINK, protocol);\n\t\tnetlink_lock_table();\n\t}\n#endif\n\tif (nl_table[protocol].registered &&\n\t    try_module_get(nl_table[protocol].module))\n\t\tmodule = nl_table[protocol].module;\n\telse\n\t\terr = -EPROTONOSUPPORT;\n\tcb_mutex = nl_table[protocol].cb_mutex;\n\tbind = nl_table[protocol].bind;\n\tunbind = nl_table[protocol].unbind;\n\trelease = nl_table[protocol].release;\n\tnetlink_unlock_table();\n\n\tif (err < 0)\n\t\tgoto out;\n\n\terr = __netlink_create(net, sock, cb_mutex, protocol, kern);\n\tif (err < 0)\n\t\tgoto out_module;\n\n\tsock_prot_inuse_add(net, &netlink_proto, 1);\n\n\tnlk = nlk_sk(sock->sk);\n\tnlk->module = module;\n\tnlk->netlink_bind = bind;\n\tnlk->netlink_unbind = unbind;\n\tnlk->netlink_release = release;\nout:\n\treturn err;\n\nout_module:\n\tmodule_put(module);\n\tgoto out;\n}\n\nstatic void deferred_put_nlk_sk(struct rcu_head *head)\n{\n\tstruct netlink_sock *nlk = container_of(head, struct netlink_sock, rcu);\n\tstruct sock *sk = &nlk->sk;\n\n\tkfree(nlk->groups);\n\tnlk->groups = NULL;\n\n\tif (!refcount_dec_and_test(&sk->sk_refcnt))\n\t\treturn;\n\n\tif (nlk->cb_running && nlk->cb.done) {\n\t\tINIT_WORK(&nlk->work, netlink_sock_destruct_work);\n\t\tschedule_work(&nlk->work);\n\t\treturn;\n\t}\n\n\tsk_free(sk);\n}\n\nstatic int netlink_release(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct netlink_sock *nlk;\n\n\tif (!sk)\n\t\treturn 0;\n\n\tnetlink_remove(sk);\n\tsock_orphan(sk);\n\tnlk = nlk_sk(sk);\n\n\t \n\tif (nlk->netlink_release)\n\t\tnlk->netlink_release(sk, nlk->groups);\n\n\t \n\tif (nlk->netlink_unbind) {\n\t\tint i;\n\n\t\tfor (i = 0; i < nlk->ngroups; i++)\n\t\t\tif (test_bit(i, nlk->groups))\n\t\t\t\tnlk->netlink_unbind(sock_net(sk), i + 1);\n\t}\n\tif (sk->sk_protocol == NETLINK_GENERIC &&\n\t    atomic_dec_return(&genl_sk_destructing_cnt) == 0)\n\t\twake_up(&genl_sk_destructing_waitq);\n\n\tsock->sk = NULL;\n\twake_up_interruptible_all(&nlk->wait);\n\n\tskb_queue_purge(&sk->sk_write_queue);\n\n\tif (nlk->portid && nlk->bound) {\n\t\tstruct netlink_notify n = {\n\t\t\t\t\t\t.net = sock_net(sk),\n\t\t\t\t\t\t.protocol = sk->sk_protocol,\n\t\t\t\t\t\t.portid = nlk->portid,\n\t\t\t\t\t  };\n\t\tblocking_notifier_call_chain(&netlink_chain,\n\t\t\t\tNETLINK_URELEASE, &n);\n\t}\n\n\tmodule_put(nlk->module);\n\n\tif (netlink_is_kernel(sk)) {\n\t\tnetlink_table_grab();\n\t\tBUG_ON(nl_table[sk->sk_protocol].registered == 0);\n\t\tif (--nl_table[sk->sk_protocol].registered == 0) {\n\t\t\tstruct listeners *old;\n\n\t\t\told = nl_deref_protected(nl_table[sk->sk_protocol].listeners);\n\t\t\tRCU_INIT_POINTER(nl_table[sk->sk_protocol].listeners, NULL);\n\t\t\tkfree_rcu(old, rcu);\n\t\t\tnl_table[sk->sk_protocol].module = NULL;\n\t\t\tnl_table[sk->sk_protocol].bind = NULL;\n\t\t\tnl_table[sk->sk_protocol].unbind = NULL;\n\t\t\tnl_table[sk->sk_protocol].flags = 0;\n\t\t\tnl_table[sk->sk_protocol].registered = 0;\n\t\t}\n\t\tnetlink_table_ungrab();\n\t}\n\n\tsock_prot_inuse_add(sock_net(sk), &netlink_proto, -1);\n\n\t \n\tif (!sk->sk_net_refcnt && sock_net(sk) != &init_net) {\n\t\t__netns_tracker_free(sock_net(sk), &sk->ns_tracker, false);\n\t\t \n\t\tsock_net_set(sk, &init_net);\n\t\t__netns_tracker_alloc(&init_net, &sk->ns_tracker,\n\t\t\t\t      false, GFP_KERNEL);\n\t}\n\tcall_rcu(&nlk->rcu, deferred_put_nlk_sk);\n\treturn 0;\n}\n\nstatic int netlink_autobind(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct net *net = sock_net(sk);\n\tstruct netlink_table *table = &nl_table[sk->sk_protocol];\n\ts32 portid = task_tgid_vnr(current);\n\tint err;\n\ts32 rover = -4096;\n\tbool ok;\n\nretry:\n\tcond_resched();\n\trcu_read_lock();\n\tok = !__netlink_lookup(table, portid, net);\n\trcu_read_unlock();\n\tif (!ok) {\n\t\t \n\t\tif (rover == -4096)\n\t\t\t \n\t\t\trover = S32_MIN + get_random_u32_below(-4096 - S32_MIN);\n\t\telse if (rover >= -4096)\n\t\t\trover = -4097;\n\t\tportid = rover--;\n\t\tgoto retry;\n\t}\n\n\terr = netlink_insert(sk, portid);\n\tif (err == -EADDRINUSE)\n\t\tgoto retry;\n\n\t \n\tif (err == -EBUSY)\n\t\terr = 0;\n\n\treturn err;\n}\n\n \nbool __netlink_ns_capable(const struct netlink_skb_parms *nsp,\n\t\t\tstruct user_namespace *user_ns, int cap)\n{\n\treturn ((nsp->flags & NETLINK_SKB_DST) ||\n\t\tfile_ns_capable(nsp->sk->sk_socket->file, user_ns, cap)) &&\n\t\tns_capable(user_ns, cap);\n}\nEXPORT_SYMBOL(__netlink_ns_capable);\n\n \nbool netlink_ns_capable(const struct sk_buff *skb,\n\t\t\tstruct user_namespace *user_ns, int cap)\n{\n\treturn __netlink_ns_capable(&NETLINK_CB(skb), user_ns, cap);\n}\nEXPORT_SYMBOL(netlink_ns_capable);\n\n \nbool netlink_capable(const struct sk_buff *skb, int cap)\n{\n\treturn netlink_ns_capable(skb, &init_user_ns, cap);\n}\nEXPORT_SYMBOL(netlink_capable);\n\n \nbool netlink_net_capable(const struct sk_buff *skb, int cap)\n{\n\treturn netlink_ns_capable(skb, sock_net(skb->sk)->user_ns, cap);\n}\nEXPORT_SYMBOL(netlink_net_capable);\n\nstatic inline int netlink_allowed(const struct socket *sock, unsigned int flag)\n{\n\treturn (nl_table[sock->sk->sk_protocol].flags & flag) ||\n\t\tns_capable(sock_net(sock->sk)->user_ns, CAP_NET_ADMIN);\n}\n\nstatic void\nnetlink_update_subscriptions(struct sock *sk, unsigned int subscriptions)\n{\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\n\tif (nlk->subscriptions && !subscriptions)\n\t\t__sk_del_bind_node(sk);\n\telse if (!nlk->subscriptions && subscriptions)\n\t\tsk_add_bind_node(sk, &nl_table[sk->sk_protocol].mc_list);\n\tnlk->subscriptions = subscriptions;\n}\n\nstatic int netlink_realloc_groups(struct sock *sk)\n{\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tunsigned int groups;\n\tunsigned long *new_groups;\n\tint err = 0;\n\n\tnetlink_table_grab();\n\n\tgroups = nl_table[sk->sk_protocol].groups;\n\tif (!nl_table[sk->sk_protocol].registered) {\n\t\terr = -ENOENT;\n\t\tgoto out_unlock;\n\t}\n\n\tif (nlk->ngroups >= groups)\n\t\tgoto out_unlock;\n\n\tnew_groups = krealloc(nlk->groups, NLGRPSZ(groups), GFP_ATOMIC);\n\tif (new_groups == NULL) {\n\t\terr = -ENOMEM;\n\t\tgoto out_unlock;\n\t}\n\tmemset((char *)new_groups + NLGRPSZ(nlk->ngroups), 0,\n\t       NLGRPSZ(groups) - NLGRPSZ(nlk->ngroups));\n\n\tnlk->groups = new_groups;\n\tnlk->ngroups = groups;\n out_unlock:\n\tnetlink_table_ungrab();\n\treturn err;\n}\n\nstatic void netlink_undo_bind(int group, long unsigned int groups,\n\t\t\t      struct sock *sk)\n{\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tint undo;\n\n\tif (!nlk->netlink_unbind)\n\t\treturn;\n\n\tfor (undo = 0; undo < group; undo++)\n\t\tif (test_bit(undo, &groups))\n\t\t\tnlk->netlink_unbind(sock_net(sk), undo + 1);\n}\n\nstatic int netlink_bind(struct socket *sock, struct sockaddr *addr,\n\t\t\tint addr_len)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct net *net = sock_net(sk);\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tstruct sockaddr_nl *nladdr = (struct sockaddr_nl *)addr;\n\tint err = 0;\n\tunsigned long groups;\n\tbool bound;\n\n\tif (addr_len < sizeof(struct sockaddr_nl))\n\t\treturn -EINVAL;\n\n\tif (nladdr->nl_family != AF_NETLINK)\n\t\treturn -EINVAL;\n\tgroups = nladdr->nl_groups;\n\n\t \n\tif (groups) {\n\t\tif (!netlink_allowed(sock, NL_CFG_F_NONROOT_RECV))\n\t\t\treturn -EPERM;\n\t\terr = netlink_realloc_groups(sk);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (nlk->ngroups < BITS_PER_LONG)\n\t\tgroups &= (1UL << nlk->ngroups) - 1;\n\n\t \n\tbound = READ_ONCE(nlk->bound);\n\tif (bound) {\n\t\t \n\t\tsmp_rmb();\n\n\t\tif (nladdr->nl_pid != nlk->portid)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (nlk->netlink_bind && groups) {\n\t\tint group;\n\n\t\t \n\t\tfor (group = 0; group < BITS_PER_TYPE(u32); group++) {\n\t\t\tif (!test_bit(group, &groups))\n\t\t\t\tcontinue;\n\t\t\terr = nlk->netlink_bind(net, group + 1);\n\t\t\tif (!err)\n\t\t\t\tcontinue;\n\t\t\tnetlink_undo_bind(group, groups, sk);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\t \n\tnetlink_lock_table();\n\tif (!bound) {\n\t\terr = nladdr->nl_pid ?\n\t\t\tnetlink_insert(sk, nladdr->nl_pid) :\n\t\t\tnetlink_autobind(sock);\n\t\tif (err) {\n\t\t\tnetlink_undo_bind(BITS_PER_TYPE(u32), groups, sk);\n\t\t\tgoto unlock;\n\t\t}\n\t}\n\n\tif (!groups && (nlk->groups == NULL || !(u32)nlk->groups[0]))\n\t\tgoto unlock;\n\tnetlink_unlock_table();\n\n\tnetlink_table_grab();\n\tnetlink_update_subscriptions(sk, nlk->subscriptions +\n\t\t\t\t\t hweight32(groups) -\n\t\t\t\t\t hweight32(nlk->groups[0]));\n\tnlk->groups[0] = (nlk->groups[0] & ~0xffffffffUL) | groups;\n\tnetlink_update_listeners(sk);\n\tnetlink_table_ungrab();\n\n\treturn 0;\n\nunlock:\n\tnetlink_unlock_table();\n\treturn err;\n}\n\nstatic int netlink_connect(struct socket *sock, struct sockaddr *addr,\n\t\t\t   int alen, int flags)\n{\n\tint err = 0;\n\tstruct sock *sk = sock->sk;\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tstruct sockaddr_nl *nladdr = (struct sockaddr_nl *)addr;\n\n\tif (alen < sizeof(addr->sa_family))\n\t\treturn -EINVAL;\n\n\tif (addr->sa_family == AF_UNSPEC) {\n\t\t \n\t\tWRITE_ONCE(sk->sk_state, NETLINK_UNCONNECTED);\n\t\t \n\t\tWRITE_ONCE(nlk->dst_portid, 0);\n\t\tWRITE_ONCE(nlk->dst_group, 0);\n\t\treturn 0;\n\t}\n\tif (addr->sa_family != AF_NETLINK)\n\t\treturn -EINVAL;\n\n\tif (alen < sizeof(struct sockaddr_nl))\n\t\treturn -EINVAL;\n\n\tif ((nladdr->nl_groups || nladdr->nl_pid) &&\n\t    !netlink_allowed(sock, NL_CFG_F_NONROOT_SEND))\n\t\treturn -EPERM;\n\n\t \n\tif (!READ_ONCE(nlk->bound))\n\t\terr = netlink_autobind(sock);\n\n\tif (err == 0) {\n\t\t \n\t\tWRITE_ONCE(sk->sk_state, NETLINK_CONNECTED);\n\t\t \n\t\tWRITE_ONCE(nlk->dst_portid, nladdr->nl_pid);\n\t\tWRITE_ONCE(nlk->dst_group, ffs(nladdr->nl_groups));\n\t}\n\n\treturn err;\n}\n\nstatic int netlink_getname(struct socket *sock, struct sockaddr *addr,\n\t\t\t   int peer)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_nl *, nladdr, addr);\n\n\tnladdr->nl_family = AF_NETLINK;\n\tnladdr->nl_pad = 0;\n\n\tif (peer) {\n\t\t \n\t\tnladdr->nl_pid = READ_ONCE(nlk->dst_portid);\n\t\tnladdr->nl_groups = netlink_group_mask(READ_ONCE(nlk->dst_group));\n\t} else {\n\t\t \n\t\tnladdr->nl_pid = READ_ONCE(nlk->portid);\n\t\tnetlink_lock_table();\n\t\tnladdr->nl_groups = nlk->groups ? nlk->groups[0] : 0;\n\t\tnetlink_unlock_table();\n\t}\n\treturn sizeof(*nladdr);\n}\n\nstatic int netlink_ioctl(struct socket *sock, unsigned int cmd,\n\t\t\t unsigned long arg)\n{\n\t \n\treturn -ENOIOCTLCMD;\n}\n\nstatic struct sock *netlink_getsockbyportid(struct sock *ssk, u32 portid)\n{\n\tstruct sock *sock;\n\tstruct netlink_sock *nlk;\n\n\tsock = netlink_lookup(sock_net(ssk), ssk->sk_protocol, portid);\n\tif (!sock)\n\t\treturn ERR_PTR(-ECONNREFUSED);\n\n\t \n\tnlk = nlk_sk(sock);\n\t \n\tif (READ_ONCE(sock->sk_state) == NETLINK_CONNECTED &&\n\t    READ_ONCE(nlk->dst_portid) != nlk_sk(ssk)->portid) {\n\t\tsock_put(sock);\n\t\treturn ERR_PTR(-ECONNREFUSED);\n\t}\n\treturn sock;\n}\n\nstruct sock *netlink_getsockbyfilp(struct file *filp)\n{\n\tstruct inode *inode = file_inode(filp);\n\tstruct sock *sock;\n\n\tif (!S_ISSOCK(inode->i_mode))\n\t\treturn ERR_PTR(-ENOTSOCK);\n\n\tsock = SOCKET_I(inode)->sk;\n\tif (sock->sk_family != AF_NETLINK)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tsock_hold(sock);\n\treturn sock;\n}\n\nstatic struct sk_buff *netlink_alloc_large_skb(unsigned int size,\n\t\t\t\t\t       int broadcast)\n{\n\tstruct sk_buff *skb;\n\tvoid *data;\n\n\tif (size <= NLMSG_GOODSIZE || broadcast)\n\t\treturn alloc_skb(size, GFP_KERNEL);\n\n\tsize = SKB_DATA_ALIGN(size) +\n\t       SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\n\tdata = vmalloc(size);\n\tif (data == NULL)\n\t\treturn NULL;\n\n\tskb = __build_skb(data, size);\n\tif (skb == NULL)\n\t\tvfree(data);\n\telse\n\t\tskb->destructor = netlink_skb_destructor;\n\n\treturn skb;\n}\n\n \nint netlink_attachskb(struct sock *sk, struct sk_buff *skb,\n\t\t      long *timeo, struct sock *ssk)\n{\n\tstruct netlink_sock *nlk;\n\n\tnlk = nlk_sk(sk);\n\n\tif ((atomic_read(&sk->sk_rmem_alloc) > sk->sk_rcvbuf ||\n\t     test_bit(NETLINK_S_CONGESTED, &nlk->state))) {\n\t\tDECLARE_WAITQUEUE(wait, current);\n\t\tif (!*timeo) {\n\t\t\tif (!ssk || netlink_is_kernel(ssk))\n\t\t\t\tnetlink_overrun(sk);\n\t\t\tsock_put(sk);\n\t\t\tkfree_skb(skb);\n\t\t\treturn -EAGAIN;\n\t\t}\n\n\t\t__set_current_state(TASK_INTERRUPTIBLE);\n\t\tadd_wait_queue(&nlk->wait, &wait);\n\n\t\tif ((atomic_read(&sk->sk_rmem_alloc) > sk->sk_rcvbuf ||\n\t\t     test_bit(NETLINK_S_CONGESTED, &nlk->state)) &&\n\t\t    !sock_flag(sk, SOCK_DEAD))\n\t\t\t*timeo = schedule_timeout(*timeo);\n\n\t\t__set_current_state(TASK_RUNNING);\n\t\tremove_wait_queue(&nlk->wait, &wait);\n\t\tsock_put(sk);\n\n\t\tif (signal_pending(current)) {\n\t\t\tkfree_skb(skb);\n\t\t\treturn sock_intr_errno(*timeo);\n\t\t}\n\t\treturn 1;\n\t}\n\tnetlink_skb_set_owner_r(skb, sk);\n\treturn 0;\n}\n\nstatic int __netlink_sendskb(struct sock *sk, struct sk_buff *skb)\n{\n\tint len = skb->len;\n\n\tnetlink_deliver_tap(sock_net(sk), skb);\n\n\tskb_queue_tail(&sk->sk_receive_queue, skb);\n\tsk->sk_data_ready(sk);\n\treturn len;\n}\n\nint netlink_sendskb(struct sock *sk, struct sk_buff *skb)\n{\n\tint len = __netlink_sendskb(sk, skb);\n\n\tsock_put(sk);\n\treturn len;\n}\n\nvoid netlink_detachskb(struct sock *sk, struct sk_buff *skb)\n{\n\tkfree_skb(skb);\n\tsock_put(sk);\n}\n\nstatic struct sk_buff *netlink_trim(struct sk_buff *skb, gfp_t allocation)\n{\n\tint delta;\n\n\tWARN_ON(skb->sk != NULL);\n\tdelta = skb->end - skb->tail;\n\tif (is_vmalloc_addr(skb->head) || delta * 2 < skb->truesize)\n\t\treturn skb;\n\n\tif (skb_shared(skb)) {\n\t\tstruct sk_buff *nskb = skb_clone(skb, allocation);\n\t\tif (!nskb)\n\t\t\treturn skb;\n\t\tconsume_skb(skb);\n\t\tskb = nskb;\n\t}\n\n\tpskb_expand_head(skb, 0, -delta,\n\t\t\t (allocation & ~__GFP_DIRECT_RECLAIM) |\n\t\t\t __GFP_NOWARN | __GFP_NORETRY);\n\treturn skb;\n}\n\nstatic int netlink_unicast_kernel(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t  struct sock *ssk)\n{\n\tint ret;\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\n\tret = -ECONNREFUSED;\n\tif (nlk->netlink_rcv != NULL) {\n\t\tret = skb->len;\n\t\tnetlink_skb_set_owner_r(skb, sk);\n\t\tNETLINK_CB(skb).sk = ssk;\n\t\tnetlink_deliver_tap_kernel(sk, ssk, skb);\n\t\tnlk->netlink_rcv(skb);\n\t\tconsume_skb(skb);\n\t} else {\n\t\tkfree_skb(skb);\n\t}\n\tsock_put(sk);\n\treturn ret;\n}\n\nint netlink_unicast(struct sock *ssk, struct sk_buff *skb,\n\t\t    u32 portid, int nonblock)\n{\n\tstruct sock *sk;\n\tint err;\n\tlong timeo;\n\n\tskb = netlink_trim(skb, gfp_any());\n\n\ttimeo = sock_sndtimeo(ssk, nonblock);\nretry:\n\tsk = netlink_getsockbyportid(ssk, portid);\n\tif (IS_ERR(sk)) {\n\t\tkfree_skb(skb);\n\t\treturn PTR_ERR(sk);\n\t}\n\tif (netlink_is_kernel(sk))\n\t\treturn netlink_unicast_kernel(sk, skb, ssk);\n\n\tif (sk_filter(sk, skb)) {\n\t\terr = skb->len;\n\t\tkfree_skb(skb);\n\t\tsock_put(sk);\n\t\treturn err;\n\t}\n\n\terr = netlink_attachskb(sk, skb, &timeo, ssk);\n\tif (err == 1)\n\t\tgoto retry;\n\tif (err)\n\t\treturn err;\n\n\treturn netlink_sendskb(sk, skb);\n}\nEXPORT_SYMBOL(netlink_unicast);\n\nint netlink_has_listeners(struct sock *sk, unsigned int group)\n{\n\tint res = 0;\n\tstruct listeners *listeners;\n\n\tBUG_ON(!netlink_is_kernel(sk));\n\n\trcu_read_lock();\n\tlisteners = rcu_dereference(nl_table[sk->sk_protocol].listeners);\n\n\tif (listeners && group - 1 < nl_table[sk->sk_protocol].groups)\n\t\tres = test_bit(group - 1, listeners->masks);\n\n\trcu_read_unlock();\n\n\treturn res;\n}\nEXPORT_SYMBOL_GPL(netlink_has_listeners);\n\nbool netlink_strict_get_check(struct sk_buff *skb)\n{\n\treturn nlk_test_bit(STRICT_CHK, NETLINK_CB(skb).sk);\n}\nEXPORT_SYMBOL_GPL(netlink_strict_get_check);\n\nstatic int netlink_broadcast_deliver(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\n\tif (atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf &&\n\t    !test_bit(NETLINK_S_CONGESTED, &nlk->state)) {\n\t\tnetlink_skb_set_owner_r(skb, sk);\n\t\t__netlink_sendskb(sk, skb);\n\t\treturn atomic_read(&sk->sk_rmem_alloc) > (sk->sk_rcvbuf >> 1);\n\t}\n\treturn -1;\n}\n\nstruct netlink_broadcast_data {\n\tstruct sock *exclude_sk;\n\tstruct net *net;\n\tu32 portid;\n\tu32 group;\n\tint failure;\n\tint delivery_failure;\n\tint congested;\n\tint delivered;\n\tgfp_t allocation;\n\tstruct sk_buff *skb, *skb2;\n\tint (*tx_filter)(struct sock *dsk, struct sk_buff *skb, void *data);\n\tvoid *tx_data;\n};\n\nstatic void do_one_broadcast(struct sock *sk,\n\t\t\t\t    struct netlink_broadcast_data *p)\n{\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tint val;\n\n\tif (p->exclude_sk == sk)\n\t\treturn;\n\n\tif (nlk->portid == p->portid || p->group - 1 >= nlk->ngroups ||\n\t    !test_bit(p->group - 1, nlk->groups))\n\t\treturn;\n\n\tif (!net_eq(sock_net(sk), p->net)) {\n\t\tif (!nlk_test_bit(LISTEN_ALL_NSID, sk))\n\t\t\treturn;\n\n\t\tif (!peernet_has_id(sock_net(sk), p->net))\n\t\t\treturn;\n\n\t\tif (!file_ns_capable(sk->sk_socket->file, p->net->user_ns,\n\t\t\t\t     CAP_NET_BROADCAST))\n\t\t\treturn;\n\t}\n\n\tif (p->failure) {\n\t\tnetlink_overrun(sk);\n\t\treturn;\n\t}\n\n\tsock_hold(sk);\n\tif (p->skb2 == NULL) {\n\t\tif (skb_shared(p->skb)) {\n\t\t\tp->skb2 = skb_clone(p->skb, p->allocation);\n\t\t} else {\n\t\t\tp->skb2 = skb_get(p->skb);\n\t\t\t \n\t\t\tskb_orphan(p->skb2);\n\t\t}\n\t}\n\tif (p->skb2 == NULL) {\n\t\tnetlink_overrun(sk);\n\t\t \n\t\tp->failure = 1;\n\t\tif (nlk_test_bit(BROADCAST_SEND_ERROR, sk))\n\t\t\tp->delivery_failure = 1;\n\t\tgoto out;\n\t}\n\n\tif (p->tx_filter && p->tx_filter(sk, p->skb2, p->tx_data)) {\n\t\tkfree_skb(p->skb2);\n\t\tp->skb2 = NULL;\n\t\tgoto out;\n\t}\n\n\tif (sk_filter(sk, p->skb2)) {\n\t\tkfree_skb(p->skb2);\n\t\tp->skb2 = NULL;\n\t\tgoto out;\n\t}\n\tNETLINK_CB(p->skb2).nsid = peernet2id(sock_net(sk), p->net);\n\tif (NETLINK_CB(p->skb2).nsid != NETNSA_NSID_NOT_ASSIGNED)\n\t\tNETLINK_CB(p->skb2).nsid_is_set = true;\n\tval = netlink_broadcast_deliver(sk, p->skb2);\n\tif (val < 0) {\n\t\tnetlink_overrun(sk);\n\t\tif (nlk_test_bit(BROADCAST_SEND_ERROR, sk))\n\t\t\tp->delivery_failure = 1;\n\t} else {\n\t\tp->congested |= val;\n\t\tp->delivered = 1;\n\t\tp->skb2 = NULL;\n\t}\nout:\n\tsock_put(sk);\n}\n\nint netlink_broadcast_filtered(struct sock *ssk, struct sk_buff *skb,\n\t\t\t       u32 portid,\n\t\t\t       u32 group, gfp_t allocation,\n\t\t\t       int (*filter)(struct sock *dsk,\n\t\t\t\t\t     struct sk_buff *skb, void *data),\n\t\t\t       void *filter_data)\n{\n\tstruct net *net = sock_net(ssk);\n\tstruct netlink_broadcast_data info;\n\tstruct sock *sk;\n\n\tskb = netlink_trim(skb, allocation);\n\n\tinfo.exclude_sk = ssk;\n\tinfo.net = net;\n\tinfo.portid = portid;\n\tinfo.group = group;\n\tinfo.failure = 0;\n\tinfo.delivery_failure = 0;\n\tinfo.congested = 0;\n\tinfo.delivered = 0;\n\tinfo.allocation = allocation;\n\tinfo.skb = skb;\n\tinfo.skb2 = NULL;\n\tinfo.tx_filter = filter;\n\tinfo.tx_data = filter_data;\n\n\t \n\n\tnetlink_lock_table();\n\n\tsk_for_each_bound(sk, &nl_table[ssk->sk_protocol].mc_list)\n\t\tdo_one_broadcast(sk, &info);\n\n\tconsume_skb(skb);\n\n\tnetlink_unlock_table();\n\n\tif (info.delivery_failure) {\n\t\tkfree_skb(info.skb2);\n\t\treturn -ENOBUFS;\n\t}\n\tconsume_skb(info.skb2);\n\n\tif (info.delivered) {\n\t\tif (info.congested && gfpflags_allow_blocking(allocation))\n\t\t\tyield();\n\t\treturn 0;\n\t}\n\treturn -ESRCH;\n}\nEXPORT_SYMBOL(netlink_broadcast_filtered);\n\nint netlink_broadcast(struct sock *ssk, struct sk_buff *skb, u32 portid,\n\t\t      u32 group, gfp_t allocation)\n{\n\treturn netlink_broadcast_filtered(ssk, skb, portid, group, allocation,\n\t\t\t\t\t  NULL, NULL);\n}\nEXPORT_SYMBOL(netlink_broadcast);\n\nstruct netlink_set_err_data {\n\tstruct sock *exclude_sk;\n\tu32 portid;\n\tu32 group;\n\tint code;\n};\n\nstatic int do_one_set_err(struct sock *sk, struct netlink_set_err_data *p)\n{\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tint ret = 0;\n\n\tif (sk == p->exclude_sk)\n\t\tgoto out;\n\n\tif (!net_eq(sock_net(sk), sock_net(p->exclude_sk)))\n\t\tgoto out;\n\n\tif (nlk->portid == p->portid || p->group - 1 >= nlk->ngroups ||\n\t    !test_bit(p->group - 1, nlk->groups))\n\t\tgoto out;\n\n\tif (p->code == ENOBUFS && nlk_test_bit(RECV_NO_ENOBUFS, sk)) {\n\t\tret = 1;\n\t\tgoto out;\n\t}\n\n\tWRITE_ONCE(sk->sk_err, p->code);\n\tsk_error_report(sk);\nout:\n\treturn ret;\n}\n\n \nint netlink_set_err(struct sock *ssk, u32 portid, u32 group, int code)\n{\n\tstruct netlink_set_err_data info;\n\tunsigned long flags;\n\tstruct sock *sk;\n\tint ret = 0;\n\n\tinfo.exclude_sk = ssk;\n\tinfo.portid = portid;\n\tinfo.group = group;\n\t \n\tinfo.code = -code;\n\n\tread_lock_irqsave(&nl_table_lock, flags);\n\n\tsk_for_each_bound(sk, &nl_table[ssk->sk_protocol].mc_list)\n\t\tret += do_one_set_err(sk, &info);\n\n\tread_unlock_irqrestore(&nl_table_lock, flags);\n\treturn ret;\n}\nEXPORT_SYMBOL(netlink_set_err);\n\n \nstatic void netlink_update_socket_mc(struct netlink_sock *nlk,\n\t\t\t\t     unsigned int group,\n\t\t\t\t     int is_new)\n{\n\tint old, new = !!is_new, subscriptions;\n\n\told = test_bit(group - 1, nlk->groups);\n\tsubscriptions = nlk->subscriptions - old + new;\n\t__assign_bit(group - 1, nlk->groups, new);\n\tnetlink_update_subscriptions(&nlk->sk, subscriptions);\n\tnetlink_update_listeners(&nlk->sk);\n}\n\nstatic int netlink_setsockopt(struct socket *sock, int level, int optname,\n\t\t\t      sockptr_t optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tunsigned int val = 0;\n\tint nr = -1;\n\n\tif (level != SOL_NETLINK)\n\t\treturn -ENOPROTOOPT;\n\n\tif (optlen >= sizeof(int) &&\n\t    copy_from_sockptr(&val, optval, sizeof(val)))\n\t\treturn -EFAULT;\n\n\tswitch (optname) {\n\tcase NETLINK_PKTINFO:\n\t\tnr = NETLINK_F_RECV_PKTINFO;\n\t\tbreak;\n\tcase NETLINK_ADD_MEMBERSHIP:\n\tcase NETLINK_DROP_MEMBERSHIP: {\n\t\tint err;\n\n\t\tif (!netlink_allowed(sock, NL_CFG_F_NONROOT_RECV))\n\t\t\treturn -EPERM;\n\t\terr = netlink_realloc_groups(sk);\n\t\tif (err)\n\t\t\treturn err;\n\t\tif (!val || val - 1 >= nlk->ngroups)\n\t\t\treturn -EINVAL;\n\t\tif (optname == NETLINK_ADD_MEMBERSHIP && nlk->netlink_bind) {\n\t\t\terr = nlk->netlink_bind(sock_net(sk), val);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\t\tnetlink_table_grab();\n\t\tnetlink_update_socket_mc(nlk, val,\n\t\t\t\t\t optname == NETLINK_ADD_MEMBERSHIP);\n\t\tnetlink_table_ungrab();\n\t\tif (optname == NETLINK_DROP_MEMBERSHIP && nlk->netlink_unbind)\n\t\t\tnlk->netlink_unbind(sock_net(sk), val);\n\n\t\tbreak;\n\t}\n\tcase NETLINK_BROADCAST_ERROR:\n\t\tnr = NETLINK_F_BROADCAST_SEND_ERROR;\n\t\tbreak;\n\tcase NETLINK_NO_ENOBUFS:\n\t\tassign_bit(NETLINK_F_RECV_NO_ENOBUFS, &nlk->flags, val);\n\t\tif (val) {\n\t\t\tclear_bit(NETLINK_S_CONGESTED, &nlk->state);\n\t\t\twake_up_interruptible(&nlk->wait);\n\t\t}\n\t\tbreak;\n\tcase NETLINK_LISTEN_ALL_NSID:\n\t\tif (!ns_capable(sock_net(sk)->user_ns, CAP_NET_BROADCAST))\n\t\t\treturn -EPERM;\n\t\tnr = NETLINK_F_LISTEN_ALL_NSID;\n\t\tbreak;\n\tcase NETLINK_CAP_ACK:\n\t\tnr = NETLINK_F_CAP_ACK;\n\t\tbreak;\n\tcase NETLINK_EXT_ACK:\n\t\tnr = NETLINK_F_EXT_ACK;\n\t\tbreak;\n\tcase NETLINK_GET_STRICT_CHK:\n\t\tnr = NETLINK_F_STRICT_CHK;\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\tif (nr >= 0)\n\t\tassign_bit(nr, &nlk->flags, val);\n\treturn 0;\n}\n\nstatic int netlink_getsockopt(struct socket *sock, int level, int optname,\n\t\t\t      char __user *optval, int __user *optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tunsigned int flag;\n\tint len, val;\n\n\tif (level != SOL_NETLINK)\n\t\treturn -ENOPROTOOPT;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (len < 0)\n\t\treturn -EINVAL;\n\n\tswitch (optname) {\n\tcase NETLINK_PKTINFO:\n\t\tflag = NETLINK_F_RECV_PKTINFO;\n\t\tbreak;\n\tcase NETLINK_BROADCAST_ERROR:\n\t\tflag = NETLINK_F_BROADCAST_SEND_ERROR;\n\t\tbreak;\n\tcase NETLINK_NO_ENOBUFS:\n\t\tflag = NETLINK_F_RECV_NO_ENOBUFS;\n\t\tbreak;\n\tcase NETLINK_LIST_MEMBERSHIPS: {\n\t\tint pos, idx, shift, err = 0;\n\n\t\tnetlink_lock_table();\n\t\tfor (pos = 0; pos * 8 < nlk->ngroups; pos += sizeof(u32)) {\n\t\t\tif (len - pos < sizeof(u32))\n\t\t\t\tbreak;\n\n\t\t\tidx = pos / sizeof(unsigned long);\n\t\t\tshift = (pos % sizeof(unsigned long)) * 8;\n\t\t\tif (put_user((u32)(nlk->groups[idx] >> shift),\n\t\t\t\t     (u32 __user *)(optval + pos))) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (put_user(ALIGN(BITS_TO_BYTES(nlk->ngroups), sizeof(u32)), optlen))\n\t\t\terr = -EFAULT;\n\t\tnetlink_unlock_table();\n\t\treturn err;\n\t}\n\tcase NETLINK_CAP_ACK:\n\t\tflag = NETLINK_F_CAP_ACK;\n\t\tbreak;\n\tcase NETLINK_EXT_ACK:\n\t\tflag = NETLINK_F_EXT_ACK;\n\t\tbreak;\n\tcase NETLINK_GET_STRICT_CHK:\n\t\tflag = NETLINK_F_STRICT_CHK;\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\tif (len < sizeof(int))\n\t\treturn -EINVAL;\n\n\tlen = sizeof(int);\n\tval = test_bit(flag, &nlk->flags);\n\n\tif (put_user(len, optlen) ||\n\t    copy_to_user(optval, &val, len))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic void netlink_cmsg_recv_pktinfo(struct msghdr *msg, struct sk_buff *skb)\n{\n\tstruct nl_pktinfo info;\n\n\tinfo.group = NETLINK_CB(skb).dst_group;\n\tput_cmsg(msg, SOL_NETLINK, NETLINK_PKTINFO, sizeof(info), &info);\n}\n\nstatic void netlink_cmsg_listen_all_nsid(struct sock *sk, struct msghdr *msg,\n\t\t\t\t\t struct sk_buff *skb)\n{\n\tif (!NETLINK_CB(skb).nsid_is_set)\n\t\treturn;\n\n\tput_cmsg(msg, SOL_NETLINK, NETLINK_LISTEN_ALL_NSID, sizeof(int),\n\t\t &NETLINK_CB(skb).nsid);\n}\n\nstatic int netlink_sendmsg(struct socket *sock, struct msghdr *msg, size_t len)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_nl *, addr, msg->msg_name);\n\tu32 dst_portid;\n\tu32 dst_group;\n\tstruct sk_buff *skb;\n\tint err;\n\tstruct scm_cookie scm;\n\tu32 netlink_skb_flags = 0;\n\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\tif (len == 0) {\n\t\tpr_warn_once(\"Zero length message leads to an empty skb\\n\");\n\t\treturn -ENODATA;\n\t}\n\n\terr = scm_send(sock, msg, &scm, true);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (msg->msg_namelen) {\n\t\terr = -EINVAL;\n\t\tif (msg->msg_namelen < sizeof(struct sockaddr_nl))\n\t\t\tgoto out;\n\t\tif (addr->nl_family != AF_NETLINK)\n\t\t\tgoto out;\n\t\tdst_portid = addr->nl_pid;\n\t\tdst_group = ffs(addr->nl_groups);\n\t\terr =  -EPERM;\n\t\tif ((dst_group || dst_portid) &&\n\t\t    !netlink_allowed(sock, NL_CFG_F_NONROOT_SEND))\n\t\t\tgoto out;\n\t\tnetlink_skb_flags |= NETLINK_SKB_DST;\n\t} else {\n\t\t \n\t\tdst_portid = READ_ONCE(nlk->dst_portid);\n\t\tdst_group = READ_ONCE(nlk->dst_group);\n\t}\n\n\t \n\tif (!READ_ONCE(nlk->bound)) {\n\t\terr = netlink_autobind(sock);\n\t\tif (err)\n\t\t\tgoto out;\n\t} else {\n\t\t \n\t\tsmp_rmb();\n\t}\n\n\terr = -EMSGSIZE;\n\tif (len > sk->sk_sndbuf - 32)\n\t\tgoto out;\n\terr = -ENOBUFS;\n\tskb = netlink_alloc_large_skb(len, dst_group);\n\tif (skb == NULL)\n\t\tgoto out;\n\n\tNETLINK_CB(skb).portid\t= nlk->portid;\n\tNETLINK_CB(skb).dst_group = dst_group;\n\tNETLINK_CB(skb).creds\t= scm.creds;\n\tNETLINK_CB(skb).flags\t= netlink_skb_flags;\n\n\terr = -EFAULT;\n\tif (memcpy_from_msg(skb_put(skb, len), msg, len)) {\n\t\tkfree_skb(skb);\n\t\tgoto out;\n\t}\n\n\terr = security_netlink_send(sk, skb);\n\tif (err) {\n\t\tkfree_skb(skb);\n\t\tgoto out;\n\t}\n\n\tif (dst_group) {\n\t\trefcount_inc(&skb->users);\n\t\tnetlink_broadcast(sk, skb, dst_portid, dst_group, GFP_KERNEL);\n\t}\n\terr = netlink_unicast(sk, skb, dst_portid, msg->msg_flags & MSG_DONTWAIT);\n\nout:\n\tscm_destroy(&scm);\n\treturn err;\n}\n\nstatic int netlink_recvmsg(struct socket *sock, struct msghdr *msg, size_t len,\n\t\t\t   int flags)\n{\n\tstruct scm_cookie scm;\n\tstruct sock *sk = sock->sk;\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tsize_t copied, max_recvmsg_len;\n\tstruct sk_buff *skb, *data_skb;\n\tint err, ret;\n\n\tif (flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\tcopied = 0;\n\n\tskb = skb_recv_datagram(sk, flags, &err);\n\tif (skb == NULL)\n\t\tgoto out;\n\n\tdata_skb = skb;\n\n#ifdef CONFIG_COMPAT_NETLINK_MESSAGES\n\tif (unlikely(skb_shinfo(skb)->frag_list)) {\n\t\t \n\t\tif (flags & MSG_CMSG_COMPAT)\n\t\t\tdata_skb = skb_shinfo(skb)->frag_list;\n\t}\n#endif\n\n\t \n\tmax_recvmsg_len = max(READ_ONCE(nlk->max_recvmsg_len), len);\n\tmax_recvmsg_len = min_t(size_t, max_recvmsg_len,\n\t\t\t\tSKB_WITH_OVERHEAD(32768));\n\tWRITE_ONCE(nlk->max_recvmsg_len, max_recvmsg_len);\n\n\tcopied = data_skb->len;\n\tif (len < copied) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\n\terr = skb_copy_datagram_msg(data_skb, 0, msg, copied);\n\n\tif (msg->msg_name) {\n\t\tDECLARE_SOCKADDR(struct sockaddr_nl *, addr, msg->msg_name);\n\t\taddr->nl_family = AF_NETLINK;\n\t\taddr->nl_pad    = 0;\n\t\taddr->nl_pid\t= NETLINK_CB(skb).portid;\n\t\taddr->nl_groups\t= netlink_group_mask(NETLINK_CB(skb).dst_group);\n\t\tmsg->msg_namelen = sizeof(*addr);\n\t}\n\n\tif (nlk_test_bit(RECV_PKTINFO, sk))\n\t\tnetlink_cmsg_recv_pktinfo(msg, skb);\n\tif (nlk_test_bit(LISTEN_ALL_NSID, sk))\n\t\tnetlink_cmsg_listen_all_nsid(sk, msg, skb);\n\n\tmemset(&scm, 0, sizeof(scm));\n\tscm.creds = *NETLINK_CREDS(skb);\n\tif (flags & MSG_TRUNC)\n\t\tcopied = data_skb->len;\n\n\tskb_free_datagram(sk, skb);\n\n\tif (READ_ONCE(nlk->cb_running) &&\n\t    atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf / 2) {\n\t\tret = netlink_dump(sk);\n\t\tif (ret) {\n\t\t\tWRITE_ONCE(sk->sk_err, -ret);\n\t\t\tsk_error_report(sk);\n\t\t}\n\t}\n\n\tscm_recv(sock, msg, &scm, flags);\nout:\n\tnetlink_rcv_wake(sk);\n\treturn err ? : copied;\n}\n\nstatic void netlink_data_ready(struct sock *sk)\n{\n\tBUG();\n}\n\n \n\nstruct sock *\n__netlink_kernel_create(struct net *net, int unit, struct module *module,\n\t\t\tstruct netlink_kernel_cfg *cfg)\n{\n\tstruct socket *sock;\n\tstruct sock *sk;\n\tstruct netlink_sock *nlk;\n\tstruct listeners *listeners = NULL;\n\tstruct mutex *cb_mutex = cfg ? cfg->cb_mutex : NULL;\n\tunsigned int groups;\n\n\tBUG_ON(!nl_table);\n\n\tif (unit < 0 || unit >= MAX_LINKS)\n\t\treturn NULL;\n\n\tif (sock_create_lite(PF_NETLINK, SOCK_DGRAM, unit, &sock))\n\t\treturn NULL;\n\n\tif (__netlink_create(net, sock, cb_mutex, unit, 1) < 0)\n\t\tgoto out_sock_release_nosk;\n\n\tsk = sock->sk;\n\n\tif (!cfg || cfg->groups < 32)\n\t\tgroups = 32;\n\telse\n\t\tgroups = cfg->groups;\n\n\tlisteners = kzalloc(sizeof(*listeners) + NLGRPSZ(groups), GFP_KERNEL);\n\tif (!listeners)\n\t\tgoto out_sock_release;\n\n\tsk->sk_data_ready = netlink_data_ready;\n\tif (cfg && cfg->input)\n\t\tnlk_sk(sk)->netlink_rcv = cfg->input;\n\n\tif (netlink_insert(sk, 0))\n\t\tgoto out_sock_release;\n\n\tnlk = nlk_sk(sk);\n\tset_bit(NETLINK_F_KERNEL_SOCKET, &nlk->flags);\n\n\tnetlink_table_grab();\n\tif (!nl_table[unit].registered) {\n\t\tnl_table[unit].groups = groups;\n\t\trcu_assign_pointer(nl_table[unit].listeners, listeners);\n\t\tnl_table[unit].cb_mutex = cb_mutex;\n\t\tnl_table[unit].module = module;\n\t\tif (cfg) {\n\t\t\tnl_table[unit].bind = cfg->bind;\n\t\t\tnl_table[unit].unbind = cfg->unbind;\n\t\t\tnl_table[unit].release = cfg->release;\n\t\t\tnl_table[unit].flags = cfg->flags;\n\t\t}\n\t\tnl_table[unit].registered = 1;\n\t} else {\n\t\tkfree(listeners);\n\t\tnl_table[unit].registered++;\n\t}\n\tnetlink_table_ungrab();\n\treturn sk;\n\nout_sock_release:\n\tkfree(listeners);\n\tnetlink_kernel_release(sk);\n\treturn NULL;\n\nout_sock_release_nosk:\n\tsock_release(sock);\n\treturn NULL;\n}\nEXPORT_SYMBOL(__netlink_kernel_create);\n\nvoid\nnetlink_kernel_release(struct sock *sk)\n{\n\tif (sk == NULL || sk->sk_socket == NULL)\n\t\treturn;\n\n\tsock_release(sk->sk_socket);\n}\nEXPORT_SYMBOL(netlink_kernel_release);\n\nint __netlink_change_ngroups(struct sock *sk, unsigned int groups)\n{\n\tstruct listeners *new, *old;\n\tstruct netlink_table *tbl = &nl_table[sk->sk_protocol];\n\n\tif (groups < 32)\n\t\tgroups = 32;\n\n\tif (NLGRPSZ(tbl->groups) < NLGRPSZ(groups)) {\n\t\tnew = kzalloc(sizeof(*new) + NLGRPSZ(groups), GFP_ATOMIC);\n\t\tif (!new)\n\t\t\treturn -ENOMEM;\n\t\told = nl_deref_protected(tbl->listeners);\n\t\tmemcpy(new->masks, old->masks, NLGRPSZ(tbl->groups));\n\t\trcu_assign_pointer(tbl->listeners, new);\n\n\t\tkfree_rcu(old, rcu);\n\t}\n\ttbl->groups = groups;\n\n\treturn 0;\n}\n\n \nint netlink_change_ngroups(struct sock *sk, unsigned int groups)\n{\n\tint err;\n\n\tnetlink_table_grab();\n\terr = __netlink_change_ngroups(sk, groups);\n\tnetlink_table_ungrab();\n\n\treturn err;\n}\n\nvoid __netlink_clear_multicast_users(struct sock *ksk, unsigned int group)\n{\n\tstruct sock *sk;\n\tstruct netlink_table *tbl = &nl_table[ksk->sk_protocol];\n\n\tsk_for_each_bound(sk, &tbl->mc_list)\n\t\tnetlink_update_socket_mc(nlk_sk(sk), group, 0);\n}\n\nstruct nlmsghdr *\n__nlmsg_put(struct sk_buff *skb, u32 portid, u32 seq, int type, int len, int flags)\n{\n\tstruct nlmsghdr *nlh;\n\tint size = nlmsg_msg_size(len);\n\n\tnlh = skb_put(skb, NLMSG_ALIGN(size));\n\tnlh->nlmsg_type = type;\n\tnlh->nlmsg_len = size;\n\tnlh->nlmsg_flags = flags;\n\tnlh->nlmsg_pid = portid;\n\tnlh->nlmsg_seq = seq;\n\tif (!__builtin_constant_p(size) || NLMSG_ALIGN(size) - size != 0)\n\t\tmemset(nlmsg_data(nlh) + len, 0, NLMSG_ALIGN(size) - size);\n\treturn nlh;\n}\nEXPORT_SYMBOL(__nlmsg_put);\n\n \n\nstatic int netlink_dump_done(struct netlink_sock *nlk, struct sk_buff *skb,\n\t\t\t     struct netlink_callback *cb,\n\t\t\t     struct netlink_ext_ack *extack)\n{\n\tstruct nlmsghdr *nlh;\n\n\tnlh = nlmsg_put_answer(skb, cb, NLMSG_DONE, sizeof(nlk->dump_done_errno),\n\t\t\t       NLM_F_MULTI | cb->answer_flags);\n\tif (WARN_ON(!nlh))\n\t\treturn -ENOBUFS;\n\n\tnl_dump_check_consistent(cb, nlh);\n\tmemcpy(nlmsg_data(nlh), &nlk->dump_done_errno, sizeof(nlk->dump_done_errno));\n\n\tif (extack->_msg && test_bit(NETLINK_F_EXT_ACK, &nlk->flags)) {\n\t\tnlh->nlmsg_flags |= NLM_F_ACK_TLVS;\n\t\tif (!nla_put_string(skb, NLMSGERR_ATTR_MSG, extack->_msg))\n\t\t\tnlmsg_end(skb, nlh);\n\t}\n\n\treturn 0;\n}\n\nstatic int netlink_dump(struct sock *sk)\n{\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tstruct netlink_ext_ack extack = {};\n\tstruct netlink_callback *cb;\n\tstruct sk_buff *skb = NULL;\n\tsize_t max_recvmsg_len;\n\tstruct module *module;\n\tint err = -ENOBUFS;\n\tint alloc_min_size;\n\tint alloc_size;\n\n\tmutex_lock(nlk->cb_mutex);\n\tif (!nlk->cb_running) {\n\t\terr = -EINVAL;\n\t\tgoto errout_skb;\n\t}\n\n\tif (atomic_read(&sk->sk_rmem_alloc) >= sk->sk_rcvbuf)\n\t\tgoto errout_skb;\n\n\t \n\tcb = &nlk->cb;\n\talloc_min_size = max_t(int, cb->min_dump_alloc, NLMSG_GOODSIZE);\n\n\tmax_recvmsg_len = READ_ONCE(nlk->max_recvmsg_len);\n\tif (alloc_min_size < max_recvmsg_len) {\n\t\talloc_size = max_recvmsg_len;\n\t\tskb = alloc_skb(alloc_size,\n\t\t\t\t(GFP_KERNEL & ~__GFP_DIRECT_RECLAIM) |\n\t\t\t\t__GFP_NOWARN | __GFP_NORETRY);\n\t}\n\tif (!skb) {\n\t\talloc_size = alloc_min_size;\n\t\tskb = alloc_skb(alloc_size, GFP_KERNEL);\n\t}\n\tif (!skb)\n\t\tgoto errout_skb;\n\n\t \n\tskb_reserve(skb, skb_tailroom(skb) - alloc_size);\n\n\t \n\tskb_reset_network_header(skb);\n\tskb_reset_mac_header(skb);\n\n\tnetlink_skb_set_owner_r(skb, sk);\n\n\tif (nlk->dump_done_errno > 0) {\n\t\tcb->extack = &extack;\n\t\tnlk->dump_done_errno = cb->dump(skb, cb);\n\t\tcb->extack = NULL;\n\t}\n\n\tif (nlk->dump_done_errno > 0 ||\n\t    skb_tailroom(skb) < nlmsg_total_size(sizeof(nlk->dump_done_errno))) {\n\t\tmutex_unlock(nlk->cb_mutex);\n\n\t\tif (sk_filter(sk, skb))\n\t\t\tkfree_skb(skb);\n\t\telse\n\t\t\t__netlink_sendskb(sk, skb);\n\t\treturn 0;\n\t}\n\n\tif (netlink_dump_done(nlk, skb, cb, &extack))\n\t\tgoto errout_skb;\n\n#ifdef CONFIG_COMPAT_NETLINK_MESSAGES\n\t \n\tif (unlikely(skb_shinfo(skb)->frag_list)) {\n\t\tif (netlink_dump_done(nlk, skb_shinfo(skb)->frag_list, cb, &extack))\n\t\t\tgoto errout_skb;\n\t}\n#endif\n\n\tif (sk_filter(sk, skb))\n\t\tkfree_skb(skb);\n\telse\n\t\t__netlink_sendskb(sk, skb);\n\n\tif (cb->done)\n\t\tcb->done(cb);\n\n\tWRITE_ONCE(nlk->cb_running, false);\n\tmodule = cb->module;\n\tskb = cb->skb;\n\tmutex_unlock(nlk->cb_mutex);\n\tmodule_put(module);\n\tconsume_skb(skb);\n\treturn 0;\n\nerrout_skb:\n\tmutex_unlock(nlk->cb_mutex);\n\tkfree_skb(skb);\n\treturn err;\n}\n\nint __netlink_dump_start(struct sock *ssk, struct sk_buff *skb,\n\t\t\t const struct nlmsghdr *nlh,\n\t\t\t struct netlink_dump_control *control)\n{\n\tstruct netlink_callback *cb;\n\tstruct netlink_sock *nlk;\n\tstruct sock *sk;\n\tint ret;\n\n\trefcount_inc(&skb->users);\n\n\tsk = netlink_lookup(sock_net(ssk), ssk->sk_protocol, NETLINK_CB(skb).portid);\n\tif (sk == NULL) {\n\t\tret = -ECONNREFUSED;\n\t\tgoto error_free;\n\t}\n\n\tnlk = nlk_sk(sk);\n\tmutex_lock(nlk->cb_mutex);\n\t \n\tif (nlk->cb_running) {\n\t\tret = -EBUSY;\n\t\tgoto error_unlock;\n\t}\n\t \n\tif (!try_module_get(control->module)) {\n\t\tret = -EPROTONOSUPPORT;\n\t\tgoto error_unlock;\n\t}\n\n\tcb = &nlk->cb;\n\tmemset(cb, 0, sizeof(*cb));\n\tcb->dump = control->dump;\n\tcb->done = control->done;\n\tcb->nlh = nlh;\n\tcb->data = control->data;\n\tcb->module = control->module;\n\tcb->min_dump_alloc = control->min_dump_alloc;\n\tcb->skb = skb;\n\n\tcb->strict_check = nlk_test_bit(STRICT_CHK, NETLINK_CB(skb).sk);\n\n\tif (control->start) {\n\t\tcb->extack = control->extack;\n\t\tret = control->start(cb);\n\t\tcb->extack = NULL;\n\t\tif (ret)\n\t\t\tgoto error_put;\n\t}\n\n\tWRITE_ONCE(nlk->cb_running, true);\n\tnlk->dump_done_errno = INT_MAX;\n\n\tmutex_unlock(nlk->cb_mutex);\n\n\tret = netlink_dump(sk);\n\n\tsock_put(sk);\n\n\tif (ret)\n\t\treturn ret;\n\n\t \n\treturn -EINTR;\n\nerror_put:\n\tmodule_put(control->module);\nerror_unlock:\n\tsock_put(sk);\n\tmutex_unlock(nlk->cb_mutex);\nerror_free:\n\tkfree_skb(skb);\n\treturn ret;\n}\nEXPORT_SYMBOL(__netlink_dump_start);\n\nstatic size_t\nnetlink_ack_tlv_len(struct netlink_sock *nlk, int err,\n\t\t    const struct netlink_ext_ack *extack)\n{\n\tsize_t tlvlen;\n\n\tif (!extack || !test_bit(NETLINK_F_EXT_ACK, &nlk->flags))\n\t\treturn 0;\n\n\ttlvlen = 0;\n\tif (extack->_msg)\n\t\ttlvlen += nla_total_size(strlen(extack->_msg) + 1);\n\tif (extack->cookie_len)\n\t\ttlvlen += nla_total_size(extack->cookie_len);\n\n\t \n\tif (!err)\n\t\treturn tlvlen;\n\n\tif (extack->bad_attr)\n\t\ttlvlen += nla_total_size(sizeof(u32));\n\tif (extack->policy)\n\t\ttlvlen += netlink_policy_dump_attr_size_estimate(extack->policy);\n\tif (extack->miss_type)\n\t\ttlvlen += nla_total_size(sizeof(u32));\n\tif (extack->miss_nest)\n\t\ttlvlen += nla_total_size(sizeof(u32));\n\n\treturn tlvlen;\n}\n\nstatic void\nnetlink_ack_tlv_fill(struct sk_buff *in_skb, struct sk_buff *skb,\n\t\t     struct nlmsghdr *nlh, int err,\n\t\t     const struct netlink_ext_ack *extack)\n{\n\tif (extack->_msg)\n\t\tWARN_ON(nla_put_string(skb, NLMSGERR_ATTR_MSG, extack->_msg));\n\tif (extack->cookie_len)\n\t\tWARN_ON(nla_put(skb, NLMSGERR_ATTR_COOKIE,\n\t\t\t\textack->cookie_len, extack->cookie));\n\n\tif (!err)\n\t\treturn;\n\n\tif (extack->bad_attr &&\n\t    !WARN_ON((u8 *)extack->bad_attr < in_skb->data ||\n\t\t     (u8 *)extack->bad_attr >= in_skb->data + in_skb->len))\n\t\tWARN_ON(nla_put_u32(skb, NLMSGERR_ATTR_OFFS,\n\t\t\t\t    (u8 *)extack->bad_attr - (u8 *)nlh));\n\tif (extack->policy)\n\t\tnetlink_policy_dump_write_attr(skb, extack->policy,\n\t\t\t\t\t       NLMSGERR_ATTR_POLICY);\n\tif (extack->miss_type)\n\t\tWARN_ON(nla_put_u32(skb, NLMSGERR_ATTR_MISS_TYPE,\n\t\t\t\t    extack->miss_type));\n\tif (extack->miss_nest &&\n\t    !WARN_ON((u8 *)extack->miss_nest < in_skb->data ||\n\t\t     (u8 *)extack->miss_nest > in_skb->data + in_skb->len))\n\t\tWARN_ON(nla_put_u32(skb, NLMSGERR_ATTR_MISS_NEST,\n\t\t\t\t    (u8 *)extack->miss_nest - (u8 *)nlh));\n}\n\nvoid netlink_ack(struct sk_buff *in_skb, struct nlmsghdr *nlh, int err,\n\t\t const struct netlink_ext_ack *extack)\n{\n\tstruct sk_buff *skb;\n\tstruct nlmsghdr *rep;\n\tstruct nlmsgerr *errmsg;\n\tsize_t payload = sizeof(*errmsg);\n\tstruct netlink_sock *nlk = nlk_sk(NETLINK_CB(in_skb).sk);\n\tunsigned int flags = 0;\n\tsize_t tlvlen;\n\n\t \n\tif (err && !test_bit(NETLINK_F_CAP_ACK, &nlk->flags))\n\t\tpayload += nlmsg_len(nlh);\n\telse\n\t\tflags |= NLM_F_CAPPED;\n\n\ttlvlen = netlink_ack_tlv_len(nlk, err, extack);\n\tif (tlvlen)\n\t\tflags |= NLM_F_ACK_TLVS;\n\n\tskb = nlmsg_new(payload + tlvlen, GFP_KERNEL);\n\tif (!skb)\n\t\tgoto err_skb;\n\n\trep = nlmsg_put(skb, NETLINK_CB(in_skb).portid, nlh->nlmsg_seq,\n\t\t\tNLMSG_ERROR, sizeof(*errmsg), flags);\n\tif (!rep)\n\t\tgoto err_bad_put;\n\terrmsg = nlmsg_data(rep);\n\terrmsg->error = err;\n\terrmsg->msg = *nlh;\n\n\tif (!(flags & NLM_F_CAPPED)) {\n\t\tif (!nlmsg_append(skb, nlmsg_len(nlh)))\n\t\t\tgoto err_bad_put;\n\n\t\tmemcpy(nlmsg_data(&errmsg->msg), nlmsg_data(nlh),\n\t\t       nlmsg_len(nlh));\n\t}\n\n\tif (tlvlen)\n\t\tnetlink_ack_tlv_fill(in_skb, skb, nlh, err, extack);\n\n\tnlmsg_end(skb, rep);\n\n\tnlmsg_unicast(in_skb->sk, skb, NETLINK_CB(in_skb).portid);\n\n\treturn;\n\nerr_bad_put:\n\tnlmsg_free(skb);\nerr_skb:\n\tWRITE_ONCE(NETLINK_CB(in_skb).sk->sk_err, ENOBUFS);\n\tsk_error_report(NETLINK_CB(in_skb).sk);\n}\nEXPORT_SYMBOL(netlink_ack);\n\nint netlink_rcv_skb(struct sk_buff *skb, int (*cb)(struct sk_buff *,\n\t\t\t\t\t\t   struct nlmsghdr *,\n\t\t\t\t\t\t   struct netlink_ext_ack *))\n{\n\tstruct netlink_ext_ack extack;\n\tstruct nlmsghdr *nlh;\n\tint err;\n\n\twhile (skb->len >= nlmsg_total_size(0)) {\n\t\tint msglen;\n\n\t\tmemset(&extack, 0, sizeof(extack));\n\t\tnlh = nlmsg_hdr(skb);\n\t\terr = 0;\n\n\t\tif (nlh->nlmsg_len < NLMSG_HDRLEN || skb->len < nlh->nlmsg_len)\n\t\t\treturn 0;\n\n\t\t \n\t\tif (!(nlh->nlmsg_flags & NLM_F_REQUEST))\n\t\t\tgoto ack;\n\n\t\t \n\t\tif (nlh->nlmsg_type < NLMSG_MIN_TYPE)\n\t\t\tgoto ack;\n\n\t\terr = cb(skb, nlh, &extack);\n\t\tif (err == -EINTR)\n\t\t\tgoto skip;\n\nack:\n\t\tif (nlh->nlmsg_flags & NLM_F_ACK || err)\n\t\t\tnetlink_ack(skb, nlh, err, &extack);\n\nskip:\n\t\tmsglen = NLMSG_ALIGN(nlh->nlmsg_len);\n\t\tif (msglen > skb->len)\n\t\t\tmsglen = skb->len;\n\t\tskb_pull(skb, msglen);\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(netlink_rcv_skb);\n\n \nint nlmsg_notify(struct sock *sk, struct sk_buff *skb, u32 portid,\n\t\t unsigned int group, int report, gfp_t flags)\n{\n\tint err = 0;\n\n\tif (group) {\n\t\tint exclude_portid = 0;\n\n\t\tif (report) {\n\t\t\trefcount_inc(&skb->users);\n\t\t\texclude_portid = portid;\n\t\t}\n\n\t\t \n\t\terr = nlmsg_multicast(sk, skb, exclude_portid, group, flags);\n\t\tif (err == -ESRCH)\n\t\t\terr = 0;\n\t}\n\n\tif (report) {\n\t\tint err2;\n\n\t\terr2 = nlmsg_unicast(sk, skb, portid);\n\t\tif (!err)\n\t\t\terr = err2;\n\t}\n\n\treturn err;\n}\nEXPORT_SYMBOL(nlmsg_notify);\n\n#ifdef CONFIG_PROC_FS\nstruct nl_seq_iter {\n\tstruct seq_net_private p;\n\tstruct rhashtable_iter hti;\n\tint link;\n};\n\nstatic void netlink_walk_start(struct nl_seq_iter *iter)\n{\n\trhashtable_walk_enter(&nl_table[iter->link].hash, &iter->hti);\n\trhashtable_walk_start(&iter->hti);\n}\n\nstatic void netlink_walk_stop(struct nl_seq_iter *iter)\n{\n\trhashtable_walk_stop(&iter->hti);\n\trhashtable_walk_exit(&iter->hti);\n}\n\nstatic void *__netlink_seq_next(struct seq_file *seq)\n{\n\tstruct nl_seq_iter *iter = seq->private;\n\tstruct netlink_sock *nlk;\n\n\tdo {\n\t\tfor (;;) {\n\t\t\tnlk = rhashtable_walk_next(&iter->hti);\n\n\t\t\tif (IS_ERR(nlk)) {\n\t\t\t\tif (PTR_ERR(nlk) == -EAGAIN)\n\t\t\t\t\tcontinue;\n\n\t\t\t\treturn nlk;\n\t\t\t}\n\n\t\t\tif (nlk)\n\t\t\t\tbreak;\n\n\t\t\tnetlink_walk_stop(iter);\n\t\t\tif (++iter->link >= MAX_LINKS)\n\t\t\t\treturn NULL;\n\n\t\t\tnetlink_walk_start(iter);\n\t\t}\n\t} while (sock_net(&nlk->sk) != seq_file_net(seq));\n\n\treturn nlk;\n}\n\nstatic void *netlink_seq_start(struct seq_file *seq, loff_t *posp)\n\t__acquires(RCU)\n{\n\tstruct nl_seq_iter *iter = seq->private;\n\tvoid *obj = SEQ_START_TOKEN;\n\tloff_t pos;\n\n\titer->link = 0;\n\n\tnetlink_walk_start(iter);\n\n\tfor (pos = *posp; pos && obj && !IS_ERR(obj); pos--)\n\t\tobj = __netlink_seq_next(seq);\n\n\treturn obj;\n}\n\nstatic void *netlink_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\t++*pos;\n\treturn __netlink_seq_next(seq);\n}\n\nstatic void netlink_native_seq_stop(struct seq_file *seq, void *v)\n{\n\tstruct nl_seq_iter *iter = seq->private;\n\n\tif (iter->link >= MAX_LINKS)\n\t\treturn;\n\n\tnetlink_walk_stop(iter);\n}\n\n\nstatic int netlink_native_seq_show(struct seq_file *seq, void *v)\n{\n\tif (v == SEQ_START_TOKEN) {\n\t\tseq_puts(seq,\n\t\t\t \"sk               Eth Pid        Groups   \"\n\t\t\t \"Rmem     Wmem     Dump  Locks    Drops    Inode\\n\");\n\t} else {\n\t\tstruct sock *s = v;\n\t\tstruct netlink_sock *nlk = nlk_sk(s);\n\n\t\tseq_printf(seq, \"%pK %-3d %-10u %08x %-8d %-8d %-5d %-8d %-8u %-8lu\\n\",\n\t\t\t   s,\n\t\t\t   s->sk_protocol,\n\t\t\t   nlk->portid,\n\t\t\t   nlk->groups ? (u32)nlk->groups[0] : 0,\n\t\t\t   sk_rmem_alloc_get(s),\n\t\t\t   sk_wmem_alloc_get(s),\n\t\t\t   READ_ONCE(nlk->cb_running),\n\t\t\t   refcount_read(&s->sk_refcnt),\n\t\t\t   atomic_read(&s->sk_drops),\n\t\t\t   sock_i_ino(s)\n\t\t\t);\n\n\t}\n\treturn 0;\n}\n\n#ifdef CONFIG_BPF_SYSCALL\nstruct bpf_iter__netlink {\n\t__bpf_md_ptr(struct bpf_iter_meta *, meta);\n\t__bpf_md_ptr(struct netlink_sock *, sk);\n};\n\nDEFINE_BPF_ITER_FUNC(netlink, struct bpf_iter_meta *meta, struct netlink_sock *sk)\n\nstatic int netlink_prog_seq_show(struct bpf_prog *prog,\n\t\t\t\t  struct bpf_iter_meta *meta,\n\t\t\t\t  void *v)\n{\n\tstruct bpf_iter__netlink ctx;\n\n\tmeta->seq_num--;   \n\tctx.meta = meta;\n\tctx.sk = nlk_sk((struct sock *)v);\n\treturn bpf_iter_run_prog(prog, &ctx);\n}\n\nstatic int netlink_seq_show(struct seq_file *seq, void *v)\n{\n\tstruct bpf_iter_meta meta;\n\tstruct bpf_prog *prog;\n\n\tmeta.seq = seq;\n\tprog = bpf_iter_get_info(&meta, false);\n\tif (!prog)\n\t\treturn netlink_native_seq_show(seq, v);\n\n\tif (v != SEQ_START_TOKEN)\n\t\treturn netlink_prog_seq_show(prog, &meta, v);\n\n\treturn 0;\n}\n\nstatic void netlink_seq_stop(struct seq_file *seq, void *v)\n{\n\tstruct bpf_iter_meta meta;\n\tstruct bpf_prog *prog;\n\n\tif (!v) {\n\t\tmeta.seq = seq;\n\t\tprog = bpf_iter_get_info(&meta, true);\n\t\tif (prog)\n\t\t\t(void)netlink_prog_seq_show(prog, &meta, v);\n\t}\n\n\tnetlink_native_seq_stop(seq, v);\n}\n#else\nstatic int netlink_seq_show(struct seq_file *seq, void *v)\n{\n\treturn netlink_native_seq_show(seq, v);\n}\n\nstatic void netlink_seq_stop(struct seq_file *seq, void *v)\n{\n\tnetlink_native_seq_stop(seq, v);\n}\n#endif\n\nstatic const struct seq_operations netlink_seq_ops = {\n\t.start  = netlink_seq_start,\n\t.next   = netlink_seq_next,\n\t.stop   = netlink_seq_stop,\n\t.show   = netlink_seq_show,\n};\n#endif\n\nint netlink_register_notifier(struct notifier_block *nb)\n{\n\treturn blocking_notifier_chain_register(&netlink_chain, nb);\n}\nEXPORT_SYMBOL(netlink_register_notifier);\n\nint netlink_unregister_notifier(struct notifier_block *nb)\n{\n\treturn blocking_notifier_chain_unregister(&netlink_chain, nb);\n}\nEXPORT_SYMBOL(netlink_unregister_notifier);\n\nstatic const struct proto_ops netlink_ops = {\n\t.family =\tPF_NETLINK,\n\t.owner =\tTHIS_MODULE,\n\t.release =\tnetlink_release,\n\t.bind =\t\tnetlink_bind,\n\t.connect =\tnetlink_connect,\n\t.socketpair =\tsock_no_socketpair,\n\t.accept =\tsock_no_accept,\n\t.getname =\tnetlink_getname,\n\t.poll =\t\tdatagram_poll,\n\t.ioctl =\tnetlink_ioctl,\n\t.listen =\tsock_no_listen,\n\t.shutdown =\tsock_no_shutdown,\n\t.setsockopt =\tnetlink_setsockopt,\n\t.getsockopt =\tnetlink_getsockopt,\n\t.sendmsg =\tnetlink_sendmsg,\n\t.recvmsg =\tnetlink_recvmsg,\n\t.mmap =\t\tsock_no_mmap,\n};\n\nstatic const struct net_proto_family netlink_family_ops = {\n\t.family = PF_NETLINK,\n\t.create = netlink_create,\n\t.owner\t= THIS_MODULE,\t \n};\n\nstatic int __net_init netlink_net_init(struct net *net)\n{\n#ifdef CONFIG_PROC_FS\n\tif (!proc_create_net(\"netlink\", 0, net->proc_net, &netlink_seq_ops,\n\t\t\tsizeof(struct nl_seq_iter)))\n\t\treturn -ENOMEM;\n#endif\n\treturn 0;\n}\n\nstatic void __net_exit netlink_net_exit(struct net *net)\n{\n#ifdef CONFIG_PROC_FS\n\tremove_proc_entry(\"netlink\", net->proc_net);\n#endif\n}\n\nstatic void __init netlink_add_usersock_entry(void)\n{\n\tstruct listeners *listeners;\n\tint groups = 32;\n\n\tlisteners = kzalloc(sizeof(*listeners) + NLGRPSZ(groups), GFP_KERNEL);\n\tif (!listeners)\n\t\tpanic(\"netlink_add_usersock_entry: Cannot allocate listeners\\n\");\n\n\tnetlink_table_grab();\n\n\tnl_table[NETLINK_USERSOCK].groups = groups;\n\trcu_assign_pointer(nl_table[NETLINK_USERSOCK].listeners, listeners);\n\tnl_table[NETLINK_USERSOCK].module = THIS_MODULE;\n\tnl_table[NETLINK_USERSOCK].registered = 1;\n\tnl_table[NETLINK_USERSOCK].flags = NL_CFG_F_NONROOT_SEND;\n\n\tnetlink_table_ungrab();\n}\n\nstatic struct pernet_operations __net_initdata netlink_net_ops = {\n\t.init = netlink_net_init,\n\t.exit = netlink_net_exit,\n};\n\nstatic inline u32 netlink_hash(const void *data, u32 len, u32 seed)\n{\n\tconst struct netlink_sock *nlk = data;\n\tstruct netlink_compare_arg arg;\n\n\tnetlink_compare_arg_init(&arg, sock_net(&nlk->sk), nlk->portid);\n\treturn jhash2((u32 *)&arg, netlink_compare_arg_len / sizeof(u32), seed);\n}\n\nstatic const struct rhashtable_params netlink_rhashtable_params = {\n\t.head_offset = offsetof(struct netlink_sock, node),\n\t.key_len = netlink_compare_arg_len,\n\t.obj_hashfn = netlink_hash,\n\t.obj_cmpfn = netlink_compare,\n\t.automatic_shrinking = true,\n};\n\n#if defined(CONFIG_BPF_SYSCALL) && defined(CONFIG_PROC_FS)\nBTF_ID_LIST(btf_netlink_sock_id)\nBTF_ID(struct, netlink_sock)\n\nstatic const struct bpf_iter_seq_info netlink_seq_info = {\n\t.seq_ops\t\t= &netlink_seq_ops,\n\t.init_seq_private\t= bpf_iter_init_seq_net,\n\t.fini_seq_private\t= bpf_iter_fini_seq_net,\n\t.seq_priv_size\t\t= sizeof(struct nl_seq_iter),\n};\n\nstatic struct bpf_iter_reg netlink_reg_info = {\n\t.target\t\t\t= \"netlink\",\n\t.ctx_arg_info_size\t= 1,\n\t.ctx_arg_info\t\t= {\n\t\t{ offsetof(struct bpf_iter__netlink, sk),\n\t\t  PTR_TO_BTF_ID_OR_NULL },\n\t},\n\t.seq_info\t\t= &netlink_seq_info,\n};\n\nstatic int __init bpf_iter_register(void)\n{\n\tnetlink_reg_info.ctx_arg_info[0].btf_id = *btf_netlink_sock_id;\n\treturn bpf_iter_reg_target(&netlink_reg_info);\n}\n#endif\n\nstatic int __init netlink_proto_init(void)\n{\n\tint i;\n\tint err = proto_register(&netlink_proto, 0);\n\n\tif (err != 0)\n\t\tgoto out;\n\n#if defined(CONFIG_BPF_SYSCALL) && defined(CONFIG_PROC_FS)\n\terr = bpf_iter_register();\n\tif (err)\n\t\tgoto out;\n#endif\n\n\tBUILD_BUG_ON(sizeof(struct netlink_skb_parms) > sizeof_field(struct sk_buff, cb));\n\n\tnl_table = kcalloc(MAX_LINKS, sizeof(*nl_table), GFP_KERNEL);\n\tif (!nl_table)\n\t\tgoto panic;\n\n\tfor (i = 0; i < MAX_LINKS; i++) {\n\t\tif (rhashtable_init(&nl_table[i].hash,\n\t\t\t\t    &netlink_rhashtable_params) < 0) {\n\t\t\twhile (--i > 0)\n\t\t\t\trhashtable_destroy(&nl_table[i].hash);\n\t\t\tkfree(nl_table);\n\t\t\tgoto panic;\n\t\t}\n\t}\n\n\tnetlink_add_usersock_entry();\n\n\tsock_register(&netlink_family_ops);\n\tregister_pernet_subsys(&netlink_net_ops);\n\tregister_pernet_subsys(&netlink_tap_net_ops);\n\t \n\trtnetlink_init();\nout:\n\treturn err;\npanic:\n\tpanic(\"netlink_init: Cannot allocate nl_table\\n\");\n}\n\ncore_initcall(netlink_proto_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}