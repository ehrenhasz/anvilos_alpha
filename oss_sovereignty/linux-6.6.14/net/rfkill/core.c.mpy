{
  "module_name": "core.c",
  "hash_id": "19e7419c5e63b4ada591186f2b59751bed49b37a3269d189c4fc7a58f2ed0ceb",
  "original_prompt": "Ingested from linux-6.6.14/net/rfkill/core.c",
  "human_readable_source": "\n \n\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/workqueue.h>\n#include <linux/capability.h>\n#include <linux/list.h>\n#include <linux/mutex.h>\n#include <linux/rfkill.h>\n#include <linux/sched.h>\n#include <linux/spinlock.h>\n#include <linux/device.h>\n#include <linux/miscdevice.h>\n#include <linux/wait.h>\n#include <linux/poll.h>\n#include <linux/fs.h>\n#include <linux/slab.h>\n\n#include \"rfkill.h\"\n\n#define POLL_INTERVAL\t\t(5 * HZ)\n\n#define RFKILL_BLOCK_HW\t\tBIT(0)\n#define RFKILL_BLOCK_SW\t\tBIT(1)\n#define RFKILL_BLOCK_SW_PREV\tBIT(2)\n#define RFKILL_BLOCK_ANY\t(RFKILL_BLOCK_HW |\\\n\t\t\t\t RFKILL_BLOCK_SW |\\\n\t\t\t\t RFKILL_BLOCK_SW_PREV)\n#define RFKILL_BLOCK_SW_SETCALL\tBIT(31)\n\nstruct rfkill {\n\tspinlock_t\t\tlock;\n\n\tenum rfkill_type\ttype;\n\n\tunsigned long\t\tstate;\n\tunsigned long\t\thard_block_reasons;\n\n\tu32\t\t\tidx;\n\n\tbool\t\t\tregistered;\n\tbool\t\t\tpersistent;\n\tbool\t\t\tpolling_paused;\n\tbool\t\t\tsuspended;\n\tbool\t\t\tneed_sync;\n\n\tconst struct rfkill_ops\t*ops;\n\tvoid\t\t\t*data;\n\n#ifdef CONFIG_RFKILL_LEDS\n\tstruct led_trigger\tled_trigger;\n\tconst char\t\t*ledtrigname;\n#endif\n\n\tstruct device\t\tdev;\n\tstruct list_head\tnode;\n\n\tstruct delayed_work\tpoll_work;\n\tstruct work_struct\tuevent_work;\n\tstruct work_struct\tsync_work;\n\tchar\t\t\tname[];\n};\n#define to_rfkill(d)\tcontainer_of(d, struct rfkill, dev)\n\nstruct rfkill_int_event {\n\tstruct list_head\tlist;\n\tstruct rfkill_event_ext\tev;\n};\n\nstruct rfkill_data {\n\tstruct list_head\tlist;\n\tstruct list_head\tevents;\n\tstruct mutex\t\tmtx;\n\twait_queue_head_t\tread_wait;\n\tbool\t\t\tinput_handler;\n\tu8\t\t\tmax_size;\n};\n\n\nMODULE_AUTHOR(\"Ivo van Doorn <IvDoorn@gmail.com>\");\nMODULE_AUTHOR(\"Johannes Berg <johannes@sipsolutions.net>\");\nMODULE_DESCRIPTION(\"RF switch support\");\nMODULE_LICENSE(\"GPL\");\n\n\n \nstatic LIST_HEAD(rfkill_list);\t \nstatic DEFINE_MUTEX(rfkill_global_mutex);\nstatic LIST_HEAD(rfkill_fds);\t \n\nstatic unsigned int rfkill_default_state = 1;\nmodule_param_named(default_state, rfkill_default_state, uint, 0444);\nMODULE_PARM_DESC(default_state,\n\t\t \"Default initial state for all radio types, 0 = radio off\");\n\nstatic struct {\n\tbool cur, sav;\n} rfkill_global_states[NUM_RFKILL_TYPES];\n\nstatic bool rfkill_epo_lock_active;\n\n\n#ifdef CONFIG_RFKILL_LEDS\nstatic void rfkill_led_trigger_event(struct rfkill *rfkill)\n{\n\tstruct led_trigger *trigger;\n\n\tif (!rfkill->registered)\n\t\treturn;\n\n\ttrigger = &rfkill->led_trigger;\n\n\tif (rfkill->state & RFKILL_BLOCK_ANY)\n\t\tled_trigger_event(trigger, LED_OFF);\n\telse\n\t\tled_trigger_event(trigger, LED_FULL);\n}\n\nstatic int rfkill_led_trigger_activate(struct led_classdev *led)\n{\n\tstruct rfkill *rfkill;\n\n\trfkill = container_of(led->trigger, struct rfkill, led_trigger);\n\n\trfkill_led_trigger_event(rfkill);\n\n\treturn 0;\n}\n\nconst char *rfkill_get_led_trigger_name(struct rfkill *rfkill)\n{\n\treturn rfkill->led_trigger.name;\n}\nEXPORT_SYMBOL(rfkill_get_led_trigger_name);\n\nvoid rfkill_set_led_trigger_name(struct rfkill *rfkill, const char *name)\n{\n\tBUG_ON(!rfkill);\n\n\trfkill->ledtrigname = name;\n}\nEXPORT_SYMBOL(rfkill_set_led_trigger_name);\n\nstatic int rfkill_led_trigger_register(struct rfkill *rfkill)\n{\n\trfkill->led_trigger.name = rfkill->ledtrigname\n\t\t\t\t\t? : dev_name(&rfkill->dev);\n\trfkill->led_trigger.activate = rfkill_led_trigger_activate;\n\treturn led_trigger_register(&rfkill->led_trigger);\n}\n\nstatic void rfkill_led_trigger_unregister(struct rfkill *rfkill)\n{\n\tled_trigger_unregister(&rfkill->led_trigger);\n}\n\nstatic struct led_trigger rfkill_any_led_trigger;\nstatic struct led_trigger rfkill_none_led_trigger;\nstatic struct work_struct rfkill_global_led_trigger_work;\n\nstatic void rfkill_global_led_trigger_worker(struct work_struct *work)\n{\n\tenum led_brightness brightness = LED_OFF;\n\tstruct rfkill *rfkill;\n\n\tmutex_lock(&rfkill_global_mutex);\n\tlist_for_each_entry(rfkill, &rfkill_list, node) {\n\t\tif (!(rfkill->state & RFKILL_BLOCK_ANY)) {\n\t\t\tbrightness = LED_FULL;\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&rfkill_global_mutex);\n\n\tled_trigger_event(&rfkill_any_led_trigger, brightness);\n\tled_trigger_event(&rfkill_none_led_trigger,\n\t\t\t  brightness == LED_OFF ? LED_FULL : LED_OFF);\n}\n\nstatic void rfkill_global_led_trigger_event(void)\n{\n\tschedule_work(&rfkill_global_led_trigger_work);\n}\n\nstatic int rfkill_global_led_trigger_register(void)\n{\n\tint ret;\n\n\tINIT_WORK(&rfkill_global_led_trigger_work,\n\t\t\trfkill_global_led_trigger_worker);\n\n\trfkill_any_led_trigger.name = \"rfkill-any\";\n\tret = led_trigger_register(&rfkill_any_led_trigger);\n\tif (ret)\n\t\treturn ret;\n\n\trfkill_none_led_trigger.name = \"rfkill-none\";\n\tret = led_trigger_register(&rfkill_none_led_trigger);\n\tif (ret)\n\t\tled_trigger_unregister(&rfkill_any_led_trigger);\n\telse\n\t\t \n\t\trfkill_global_led_trigger_event();\n\n\treturn ret;\n}\n\nstatic void rfkill_global_led_trigger_unregister(void)\n{\n\tled_trigger_unregister(&rfkill_none_led_trigger);\n\tled_trigger_unregister(&rfkill_any_led_trigger);\n\tcancel_work_sync(&rfkill_global_led_trigger_work);\n}\n#else\nstatic void rfkill_led_trigger_event(struct rfkill *rfkill)\n{\n}\n\nstatic inline int rfkill_led_trigger_register(struct rfkill *rfkill)\n{\n\treturn 0;\n}\n\nstatic inline void rfkill_led_trigger_unregister(struct rfkill *rfkill)\n{\n}\n\nstatic void rfkill_global_led_trigger_event(void)\n{\n}\n\nstatic int rfkill_global_led_trigger_register(void)\n{\n\treturn 0;\n}\n\nstatic void rfkill_global_led_trigger_unregister(void)\n{\n}\n#endif  \n\nstatic void rfkill_fill_event(struct rfkill_event_ext *ev,\n\t\t\t      struct rfkill *rfkill,\n\t\t\t      enum rfkill_operation op)\n{\n\tunsigned long flags;\n\n\tev->idx = rfkill->idx;\n\tev->type = rfkill->type;\n\tev->op = op;\n\n\tspin_lock_irqsave(&rfkill->lock, flags);\n\tev->hard = !!(rfkill->state & RFKILL_BLOCK_HW);\n\tev->soft = !!(rfkill->state & (RFKILL_BLOCK_SW |\n\t\t\t\t\tRFKILL_BLOCK_SW_PREV));\n\tev->hard_block_reasons = rfkill->hard_block_reasons;\n\tspin_unlock_irqrestore(&rfkill->lock, flags);\n}\n\nstatic void rfkill_send_events(struct rfkill *rfkill, enum rfkill_operation op)\n{\n\tstruct rfkill_data *data;\n\tstruct rfkill_int_event *ev;\n\n\tlist_for_each_entry(data, &rfkill_fds, list) {\n\t\tev = kzalloc(sizeof(*ev), GFP_KERNEL);\n\t\tif (!ev)\n\t\t\tcontinue;\n\t\trfkill_fill_event(&ev->ev, rfkill, op);\n\t\tmutex_lock(&data->mtx);\n\t\tlist_add_tail(&ev->list, &data->events);\n\t\tmutex_unlock(&data->mtx);\n\t\twake_up_interruptible(&data->read_wait);\n\t}\n}\n\nstatic void rfkill_event(struct rfkill *rfkill)\n{\n\tif (!rfkill->registered)\n\t\treturn;\n\n\tkobject_uevent(&rfkill->dev.kobj, KOBJ_CHANGE);\n\n\t \n\trfkill_send_events(rfkill, RFKILL_OP_CHANGE);\n}\n\n \nstatic void rfkill_set_block(struct rfkill *rfkill, bool blocked)\n{\n\tunsigned long flags;\n\tbool prev, curr;\n\tint err;\n\n\tif (unlikely(rfkill->dev.power.power_state.event & PM_EVENT_SLEEP))\n\t\treturn;\n\n\t \n\tif (rfkill->ops->query)\n\t\trfkill->ops->query(rfkill, rfkill->data);\n\n\tspin_lock_irqsave(&rfkill->lock, flags);\n\tprev = rfkill->state & RFKILL_BLOCK_SW;\n\n\tif (prev)\n\t\trfkill->state |= RFKILL_BLOCK_SW_PREV;\n\telse\n\t\trfkill->state &= ~RFKILL_BLOCK_SW_PREV;\n\n\tif (blocked)\n\t\trfkill->state |= RFKILL_BLOCK_SW;\n\telse\n\t\trfkill->state &= ~RFKILL_BLOCK_SW;\n\n\trfkill->state |= RFKILL_BLOCK_SW_SETCALL;\n\tspin_unlock_irqrestore(&rfkill->lock, flags);\n\n\terr = rfkill->ops->set_block(rfkill->data, blocked);\n\n\tspin_lock_irqsave(&rfkill->lock, flags);\n\tif (err) {\n\t\t \n\t\tif (rfkill->state & RFKILL_BLOCK_SW_PREV)\n\t\t\trfkill->state |= RFKILL_BLOCK_SW;\n\t\telse\n\t\t\trfkill->state &= ~RFKILL_BLOCK_SW;\n\t}\n\trfkill->state &= ~RFKILL_BLOCK_SW_SETCALL;\n\trfkill->state &= ~RFKILL_BLOCK_SW_PREV;\n\tcurr = rfkill->state & RFKILL_BLOCK_SW;\n\tspin_unlock_irqrestore(&rfkill->lock, flags);\n\n\trfkill_led_trigger_event(rfkill);\n\trfkill_global_led_trigger_event();\n\n\tif (prev != curr)\n\t\trfkill_event(rfkill);\n}\n\nstatic void rfkill_sync(struct rfkill *rfkill)\n{\n\tlockdep_assert_held(&rfkill_global_mutex);\n\n\tif (!rfkill->need_sync)\n\t\treturn;\n\n\trfkill_set_block(rfkill, rfkill_global_states[rfkill->type].cur);\n\trfkill->need_sync = false;\n}\n\nstatic void rfkill_update_global_state(enum rfkill_type type, bool blocked)\n{\n\tint i;\n\n\tif (type != RFKILL_TYPE_ALL) {\n\t\trfkill_global_states[type].cur = blocked;\n\t\treturn;\n\t}\n\n\tfor (i = 0; i < NUM_RFKILL_TYPES; i++)\n\t\trfkill_global_states[i].cur = blocked;\n}\n\n#ifdef CONFIG_RFKILL_INPUT\nstatic atomic_t rfkill_input_disabled = ATOMIC_INIT(0);\n\n \nstatic void __rfkill_switch_all(const enum rfkill_type type, bool blocked)\n{\n\tstruct rfkill *rfkill;\n\n\trfkill_update_global_state(type, blocked);\n\tlist_for_each_entry(rfkill, &rfkill_list, node) {\n\t\tif (rfkill->type != type && type != RFKILL_TYPE_ALL)\n\t\t\tcontinue;\n\n\t\trfkill_set_block(rfkill, blocked);\n\t}\n}\n\n \nvoid rfkill_switch_all(enum rfkill_type type, bool blocked)\n{\n\tif (atomic_read(&rfkill_input_disabled))\n\t\treturn;\n\n\tmutex_lock(&rfkill_global_mutex);\n\n\tif (!rfkill_epo_lock_active)\n\t\t__rfkill_switch_all(type, blocked);\n\n\tmutex_unlock(&rfkill_global_mutex);\n}\n\n \nvoid rfkill_epo(void)\n{\n\tstruct rfkill *rfkill;\n\tint i;\n\n\tif (atomic_read(&rfkill_input_disabled))\n\t\treturn;\n\n\tmutex_lock(&rfkill_global_mutex);\n\n\trfkill_epo_lock_active = true;\n\tlist_for_each_entry(rfkill, &rfkill_list, node)\n\t\trfkill_set_block(rfkill, true);\n\n\tfor (i = 0; i < NUM_RFKILL_TYPES; i++) {\n\t\trfkill_global_states[i].sav = rfkill_global_states[i].cur;\n\t\trfkill_global_states[i].cur = true;\n\t}\n\n\tmutex_unlock(&rfkill_global_mutex);\n}\n\n \nvoid rfkill_restore_states(void)\n{\n\tint i;\n\n\tif (atomic_read(&rfkill_input_disabled))\n\t\treturn;\n\n\tmutex_lock(&rfkill_global_mutex);\n\n\trfkill_epo_lock_active = false;\n\tfor (i = 0; i < NUM_RFKILL_TYPES; i++)\n\t\t__rfkill_switch_all(i, rfkill_global_states[i].sav);\n\tmutex_unlock(&rfkill_global_mutex);\n}\n\n \nvoid rfkill_remove_epo_lock(void)\n{\n\tif (atomic_read(&rfkill_input_disabled))\n\t\treturn;\n\n\tmutex_lock(&rfkill_global_mutex);\n\trfkill_epo_lock_active = false;\n\tmutex_unlock(&rfkill_global_mutex);\n}\n\n \nbool rfkill_is_epo_lock_active(void)\n{\n\treturn rfkill_epo_lock_active;\n}\n\n \nbool rfkill_get_global_sw_state(const enum rfkill_type type)\n{\n\treturn rfkill_global_states[type].cur;\n}\n#endif\n\nbool rfkill_set_hw_state_reason(struct rfkill *rfkill,\n\t\t\t\tbool blocked, unsigned long reason)\n{\n\tunsigned long flags;\n\tbool ret, prev;\n\n\tBUG_ON(!rfkill);\n\n\tif (WARN(reason &\n\t    ~(RFKILL_HARD_BLOCK_SIGNAL | RFKILL_HARD_BLOCK_NOT_OWNER),\n\t    \"hw_state reason not supported: 0x%lx\", reason))\n\t\treturn blocked;\n\n\tspin_lock_irqsave(&rfkill->lock, flags);\n\tprev = !!(rfkill->hard_block_reasons & reason);\n\tif (blocked) {\n\t\trfkill->state |= RFKILL_BLOCK_HW;\n\t\trfkill->hard_block_reasons |= reason;\n\t} else {\n\t\trfkill->hard_block_reasons &= ~reason;\n\t\tif (!rfkill->hard_block_reasons)\n\t\t\trfkill->state &= ~RFKILL_BLOCK_HW;\n\t}\n\tret = !!(rfkill->state & RFKILL_BLOCK_ANY);\n\tspin_unlock_irqrestore(&rfkill->lock, flags);\n\n\trfkill_led_trigger_event(rfkill);\n\trfkill_global_led_trigger_event();\n\n\tif (rfkill->registered && prev != blocked)\n\t\tschedule_work(&rfkill->uevent_work);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(rfkill_set_hw_state_reason);\n\nstatic void __rfkill_set_sw_state(struct rfkill *rfkill, bool blocked)\n{\n\tu32 bit = RFKILL_BLOCK_SW;\n\n\t \n\tif (rfkill->state & RFKILL_BLOCK_SW_SETCALL)\n\t\tbit = RFKILL_BLOCK_SW_PREV;\n\n\tif (blocked)\n\t\trfkill->state |= bit;\n\telse\n\t\trfkill->state &= ~bit;\n}\n\nbool rfkill_set_sw_state(struct rfkill *rfkill, bool blocked)\n{\n\tunsigned long flags;\n\tbool prev, hwblock;\n\n\tBUG_ON(!rfkill);\n\n\tspin_lock_irqsave(&rfkill->lock, flags);\n\tprev = !!(rfkill->state & RFKILL_BLOCK_SW);\n\t__rfkill_set_sw_state(rfkill, blocked);\n\thwblock = !!(rfkill->state & RFKILL_BLOCK_HW);\n\tblocked = blocked || hwblock;\n\tspin_unlock_irqrestore(&rfkill->lock, flags);\n\n\tif (!rfkill->registered)\n\t\treturn blocked;\n\n\tif (prev != blocked && !hwblock)\n\t\tschedule_work(&rfkill->uevent_work);\n\n\trfkill_led_trigger_event(rfkill);\n\trfkill_global_led_trigger_event();\n\n\treturn blocked;\n}\nEXPORT_SYMBOL(rfkill_set_sw_state);\n\nvoid rfkill_init_sw_state(struct rfkill *rfkill, bool blocked)\n{\n\tunsigned long flags;\n\n\tBUG_ON(!rfkill);\n\tBUG_ON(rfkill->registered);\n\n\tspin_lock_irqsave(&rfkill->lock, flags);\n\t__rfkill_set_sw_state(rfkill, blocked);\n\trfkill->persistent = true;\n\tspin_unlock_irqrestore(&rfkill->lock, flags);\n}\nEXPORT_SYMBOL(rfkill_init_sw_state);\n\nvoid rfkill_set_states(struct rfkill *rfkill, bool sw, bool hw)\n{\n\tunsigned long flags;\n\tbool swprev, hwprev;\n\n\tBUG_ON(!rfkill);\n\n\tspin_lock_irqsave(&rfkill->lock, flags);\n\n\t \n\tswprev = !!(rfkill->state & RFKILL_BLOCK_SW);\n\thwprev = !!(rfkill->state & RFKILL_BLOCK_HW);\n\t__rfkill_set_sw_state(rfkill, sw);\n\tif (hw)\n\t\trfkill->state |= RFKILL_BLOCK_HW;\n\telse\n\t\trfkill->state &= ~RFKILL_BLOCK_HW;\n\n\tspin_unlock_irqrestore(&rfkill->lock, flags);\n\n\tif (!rfkill->registered) {\n\t\trfkill->persistent = true;\n\t} else {\n\t\tif (swprev != sw || hwprev != hw)\n\t\t\tschedule_work(&rfkill->uevent_work);\n\n\t\trfkill_led_trigger_event(rfkill);\n\t\trfkill_global_led_trigger_event();\n\t}\n}\nEXPORT_SYMBOL(rfkill_set_states);\n\nstatic const char * const rfkill_types[] = {\n\tNULL,  \n\t\"wlan\",\n\t\"bluetooth\",\n\t\"ultrawideband\",\n\t\"wimax\",\n\t\"wwan\",\n\t\"gps\",\n\t\"fm\",\n\t\"nfc\",\n};\n\nenum rfkill_type rfkill_find_type(const char *name)\n{\n\tint i;\n\n\tBUILD_BUG_ON(ARRAY_SIZE(rfkill_types) != NUM_RFKILL_TYPES);\n\n\tif (!name)\n\t\treturn RFKILL_TYPE_ALL;\n\n\tfor (i = 1; i < NUM_RFKILL_TYPES; i++)\n\t\tif (!strcmp(name, rfkill_types[i]))\n\t\t\treturn i;\n\treturn RFKILL_TYPE_ALL;\n}\nEXPORT_SYMBOL(rfkill_find_type);\n\nstatic ssize_t name_show(struct device *dev, struct device_attribute *attr,\n\t\t\t char *buf)\n{\n\tstruct rfkill *rfkill = to_rfkill(dev);\n\n\treturn sysfs_emit(buf, \"%s\\n\", rfkill->name);\n}\nstatic DEVICE_ATTR_RO(name);\n\nstatic ssize_t type_show(struct device *dev, struct device_attribute *attr,\n\t\t\t char *buf)\n{\n\tstruct rfkill *rfkill = to_rfkill(dev);\n\n\treturn sysfs_emit(buf, \"%s\\n\", rfkill_types[rfkill->type]);\n}\nstatic DEVICE_ATTR_RO(type);\n\nstatic ssize_t index_show(struct device *dev, struct device_attribute *attr,\n\t\t\t  char *buf)\n{\n\tstruct rfkill *rfkill = to_rfkill(dev);\n\n\treturn sysfs_emit(buf, \"%d\\n\", rfkill->idx);\n}\nstatic DEVICE_ATTR_RO(index);\n\nstatic ssize_t persistent_show(struct device *dev,\n\t\t\t       struct device_attribute *attr, char *buf)\n{\n\tstruct rfkill *rfkill = to_rfkill(dev);\n\n\treturn sysfs_emit(buf, \"%d\\n\", rfkill->persistent);\n}\nstatic DEVICE_ATTR_RO(persistent);\n\nstatic ssize_t hard_show(struct device *dev, struct device_attribute *attr,\n\t\t\t char *buf)\n{\n\tstruct rfkill *rfkill = to_rfkill(dev);\n\n\treturn sysfs_emit(buf, \"%d\\n\", (rfkill->state & RFKILL_BLOCK_HW) ? 1 : 0);\n}\nstatic DEVICE_ATTR_RO(hard);\n\nstatic ssize_t soft_show(struct device *dev, struct device_attribute *attr,\n\t\t\t char *buf)\n{\n\tstruct rfkill *rfkill = to_rfkill(dev);\n\n\tmutex_lock(&rfkill_global_mutex);\n\trfkill_sync(rfkill);\n\tmutex_unlock(&rfkill_global_mutex);\n\n\treturn sysfs_emit(buf, \"%d\\n\", (rfkill->state & RFKILL_BLOCK_SW) ? 1 : 0);\n}\n\nstatic ssize_t soft_store(struct device *dev, struct device_attribute *attr,\n\t\t\t  const char *buf, size_t count)\n{\n\tstruct rfkill *rfkill = to_rfkill(dev);\n\tunsigned long state;\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = kstrtoul(buf, 0, &state);\n\tif (err)\n\t\treturn err;\n\n\tif (state > 1 )\n\t\treturn -EINVAL;\n\n\tmutex_lock(&rfkill_global_mutex);\n\trfkill_sync(rfkill);\n\trfkill_set_block(rfkill, state);\n\tmutex_unlock(&rfkill_global_mutex);\n\n\treturn count;\n}\nstatic DEVICE_ATTR_RW(soft);\n\nstatic ssize_t hard_block_reasons_show(struct device *dev,\n\t\t\t\t       struct device_attribute *attr,\n\t\t\t\t       char *buf)\n{\n\tstruct rfkill *rfkill = to_rfkill(dev);\n\n\treturn sysfs_emit(buf, \"0x%lx\\n\", rfkill->hard_block_reasons);\n}\nstatic DEVICE_ATTR_RO(hard_block_reasons);\n\nstatic u8 user_state_from_blocked(unsigned long state)\n{\n\tif (state & RFKILL_BLOCK_HW)\n\t\treturn RFKILL_USER_STATE_HARD_BLOCKED;\n\tif (state & RFKILL_BLOCK_SW)\n\t\treturn RFKILL_USER_STATE_SOFT_BLOCKED;\n\n\treturn RFKILL_USER_STATE_UNBLOCKED;\n}\n\nstatic ssize_t state_show(struct device *dev, struct device_attribute *attr,\n\t\t\t  char *buf)\n{\n\tstruct rfkill *rfkill = to_rfkill(dev);\n\n\tmutex_lock(&rfkill_global_mutex);\n\trfkill_sync(rfkill);\n\tmutex_unlock(&rfkill_global_mutex);\n\n\treturn sysfs_emit(buf, \"%d\\n\", user_state_from_blocked(rfkill->state));\n}\n\nstatic ssize_t state_store(struct device *dev, struct device_attribute *attr,\n\t\t\t   const char *buf, size_t count)\n{\n\tstruct rfkill *rfkill = to_rfkill(dev);\n\tunsigned long state;\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = kstrtoul(buf, 0, &state);\n\tif (err)\n\t\treturn err;\n\n\tif (state != RFKILL_USER_STATE_SOFT_BLOCKED &&\n\t    state != RFKILL_USER_STATE_UNBLOCKED)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&rfkill_global_mutex);\n\trfkill_sync(rfkill);\n\trfkill_set_block(rfkill, state == RFKILL_USER_STATE_SOFT_BLOCKED);\n\tmutex_unlock(&rfkill_global_mutex);\n\n\treturn count;\n}\nstatic DEVICE_ATTR_RW(state);\n\nstatic struct attribute *rfkill_dev_attrs[] = {\n\t&dev_attr_name.attr,\n\t&dev_attr_type.attr,\n\t&dev_attr_index.attr,\n\t&dev_attr_persistent.attr,\n\t&dev_attr_state.attr,\n\t&dev_attr_soft.attr,\n\t&dev_attr_hard.attr,\n\t&dev_attr_hard_block_reasons.attr,\n\tNULL,\n};\nATTRIBUTE_GROUPS(rfkill_dev);\n\nstatic void rfkill_release(struct device *dev)\n{\n\tstruct rfkill *rfkill = to_rfkill(dev);\n\n\tkfree(rfkill);\n}\n\nstatic int rfkill_dev_uevent(const struct device *dev, struct kobj_uevent_env *env)\n{\n\tstruct rfkill *rfkill = to_rfkill(dev);\n\tunsigned long flags;\n\tunsigned long reasons;\n\tu32 state;\n\tint error;\n\n\terror = add_uevent_var(env, \"RFKILL_NAME=%s\", rfkill->name);\n\tif (error)\n\t\treturn error;\n\terror = add_uevent_var(env, \"RFKILL_TYPE=%s\",\n\t\t\t       rfkill_types[rfkill->type]);\n\tif (error)\n\t\treturn error;\n\tspin_lock_irqsave(&rfkill->lock, flags);\n\tstate = rfkill->state;\n\treasons = rfkill->hard_block_reasons;\n\tspin_unlock_irqrestore(&rfkill->lock, flags);\n\terror = add_uevent_var(env, \"RFKILL_STATE=%d\",\n\t\t\t       user_state_from_blocked(state));\n\tif (error)\n\t\treturn error;\n\treturn add_uevent_var(env, \"RFKILL_HW_BLOCK_REASON=0x%lx\", reasons);\n}\n\nvoid rfkill_pause_polling(struct rfkill *rfkill)\n{\n\tBUG_ON(!rfkill);\n\n\tif (!rfkill->ops->poll)\n\t\treturn;\n\n\trfkill->polling_paused = true;\n\tcancel_delayed_work_sync(&rfkill->poll_work);\n}\nEXPORT_SYMBOL(rfkill_pause_polling);\n\nvoid rfkill_resume_polling(struct rfkill *rfkill)\n{\n\tBUG_ON(!rfkill);\n\n\tif (!rfkill->ops->poll)\n\t\treturn;\n\n\trfkill->polling_paused = false;\n\n\tif (rfkill->suspended)\n\t\treturn;\n\n\tqueue_delayed_work(system_power_efficient_wq,\n\t\t\t   &rfkill->poll_work, 0);\n}\nEXPORT_SYMBOL(rfkill_resume_polling);\n\n#ifdef CONFIG_PM_SLEEP\nstatic int rfkill_suspend(struct device *dev)\n{\n\tstruct rfkill *rfkill = to_rfkill(dev);\n\n\trfkill->suspended = true;\n\tcancel_delayed_work_sync(&rfkill->poll_work);\n\n\treturn 0;\n}\n\nstatic int rfkill_resume(struct device *dev)\n{\n\tstruct rfkill *rfkill = to_rfkill(dev);\n\tbool cur;\n\n\trfkill->suspended = false;\n\n\tif (!rfkill->registered)\n\t\treturn 0;\n\n\tif (!rfkill->persistent) {\n\t\tcur = !!(rfkill->state & RFKILL_BLOCK_SW);\n\t\trfkill_set_block(rfkill, cur);\n\t}\n\n\tif (rfkill->ops->poll && !rfkill->polling_paused)\n\t\tqueue_delayed_work(system_power_efficient_wq,\n\t\t\t\t   &rfkill->poll_work, 0);\n\n\treturn 0;\n}\n\nstatic SIMPLE_DEV_PM_OPS(rfkill_pm_ops, rfkill_suspend, rfkill_resume);\n#define RFKILL_PM_OPS (&rfkill_pm_ops)\n#else\n#define RFKILL_PM_OPS NULL\n#endif\n\nstatic struct class rfkill_class = {\n\t.name\t\t= \"rfkill\",\n\t.dev_release\t= rfkill_release,\n\t.dev_groups\t= rfkill_dev_groups,\n\t.dev_uevent\t= rfkill_dev_uevent,\n\t.pm\t\t= RFKILL_PM_OPS,\n};\n\nbool rfkill_blocked(struct rfkill *rfkill)\n{\n\tunsigned long flags;\n\tu32 state;\n\n\tspin_lock_irqsave(&rfkill->lock, flags);\n\tstate = rfkill->state;\n\tspin_unlock_irqrestore(&rfkill->lock, flags);\n\n\treturn !!(state & RFKILL_BLOCK_ANY);\n}\nEXPORT_SYMBOL(rfkill_blocked);\n\nbool rfkill_soft_blocked(struct rfkill *rfkill)\n{\n\tunsigned long flags;\n\tu32 state;\n\n\tspin_lock_irqsave(&rfkill->lock, flags);\n\tstate = rfkill->state;\n\tspin_unlock_irqrestore(&rfkill->lock, flags);\n\n\treturn !!(state & RFKILL_BLOCK_SW);\n}\nEXPORT_SYMBOL(rfkill_soft_blocked);\n\nstruct rfkill * __must_check rfkill_alloc(const char *name,\n\t\t\t\t\t  struct device *parent,\n\t\t\t\t\t  const enum rfkill_type type,\n\t\t\t\t\t  const struct rfkill_ops *ops,\n\t\t\t\t\t  void *ops_data)\n{\n\tstruct rfkill *rfkill;\n\tstruct device *dev;\n\n\tif (WARN_ON(!ops))\n\t\treturn NULL;\n\n\tif (WARN_ON(!ops->set_block))\n\t\treturn NULL;\n\n\tif (WARN_ON(!name))\n\t\treturn NULL;\n\n\tif (WARN_ON(type == RFKILL_TYPE_ALL || type >= NUM_RFKILL_TYPES))\n\t\treturn NULL;\n\n\trfkill = kzalloc(sizeof(*rfkill) + strlen(name) + 1, GFP_KERNEL);\n\tif (!rfkill)\n\t\treturn NULL;\n\n\tspin_lock_init(&rfkill->lock);\n\tINIT_LIST_HEAD(&rfkill->node);\n\trfkill->type = type;\n\tstrcpy(rfkill->name, name);\n\trfkill->ops = ops;\n\trfkill->data = ops_data;\n\n\tdev = &rfkill->dev;\n\tdev->class = &rfkill_class;\n\tdev->parent = parent;\n\tdevice_initialize(dev);\n\n\treturn rfkill;\n}\nEXPORT_SYMBOL(rfkill_alloc);\n\nstatic void rfkill_poll(struct work_struct *work)\n{\n\tstruct rfkill *rfkill;\n\n\trfkill = container_of(work, struct rfkill, poll_work.work);\n\n\t \n\trfkill->ops->poll(rfkill, rfkill->data);\n\n\tqueue_delayed_work(system_power_efficient_wq,\n\t\t&rfkill->poll_work,\n\t\tround_jiffies_relative(POLL_INTERVAL));\n}\n\nstatic void rfkill_uevent_work(struct work_struct *work)\n{\n\tstruct rfkill *rfkill;\n\n\trfkill = container_of(work, struct rfkill, uevent_work);\n\n\tmutex_lock(&rfkill_global_mutex);\n\trfkill_event(rfkill);\n\tmutex_unlock(&rfkill_global_mutex);\n}\n\nstatic void rfkill_sync_work(struct work_struct *work)\n{\n\tstruct rfkill *rfkill = container_of(work, struct rfkill, sync_work);\n\n\tmutex_lock(&rfkill_global_mutex);\n\trfkill_sync(rfkill);\n\tmutex_unlock(&rfkill_global_mutex);\n}\n\nint __must_check rfkill_register(struct rfkill *rfkill)\n{\n\tstatic unsigned long rfkill_no;\n\tstruct device *dev;\n\tint error;\n\n\tif (!rfkill)\n\t\treturn -EINVAL;\n\n\tdev = &rfkill->dev;\n\n\tmutex_lock(&rfkill_global_mutex);\n\n\tif (rfkill->registered) {\n\t\terror = -EALREADY;\n\t\tgoto unlock;\n\t}\n\n\trfkill->idx = rfkill_no;\n\tdev_set_name(dev, \"rfkill%lu\", rfkill_no);\n\trfkill_no++;\n\n\tlist_add_tail(&rfkill->node, &rfkill_list);\n\n\terror = device_add(dev);\n\tif (error)\n\t\tgoto remove;\n\n\terror = rfkill_led_trigger_register(rfkill);\n\tif (error)\n\t\tgoto devdel;\n\n\trfkill->registered = true;\n\n\tINIT_DELAYED_WORK(&rfkill->poll_work, rfkill_poll);\n\tINIT_WORK(&rfkill->uevent_work, rfkill_uevent_work);\n\tINIT_WORK(&rfkill->sync_work, rfkill_sync_work);\n\n\tif (rfkill->ops->poll)\n\t\tqueue_delayed_work(system_power_efficient_wq,\n\t\t\t&rfkill->poll_work,\n\t\t\tround_jiffies_relative(POLL_INTERVAL));\n\n\tif (!rfkill->persistent || rfkill_epo_lock_active) {\n\t\trfkill->need_sync = true;\n\t\tschedule_work(&rfkill->sync_work);\n\t} else {\n#ifdef CONFIG_RFKILL_INPUT\n\t\tbool soft_blocked = !!(rfkill->state & RFKILL_BLOCK_SW);\n\n\t\tif (!atomic_read(&rfkill_input_disabled))\n\t\t\t__rfkill_switch_all(rfkill->type, soft_blocked);\n#endif\n\t}\n\n\trfkill_global_led_trigger_event();\n\trfkill_send_events(rfkill, RFKILL_OP_ADD);\n\n\tmutex_unlock(&rfkill_global_mutex);\n\treturn 0;\n\n devdel:\n\tdevice_del(&rfkill->dev);\n remove:\n\tlist_del_init(&rfkill->node);\n unlock:\n\tmutex_unlock(&rfkill_global_mutex);\n\treturn error;\n}\nEXPORT_SYMBOL(rfkill_register);\n\nvoid rfkill_unregister(struct rfkill *rfkill)\n{\n\tBUG_ON(!rfkill);\n\n\tif (rfkill->ops->poll)\n\t\tcancel_delayed_work_sync(&rfkill->poll_work);\n\n\tcancel_work_sync(&rfkill->uevent_work);\n\tcancel_work_sync(&rfkill->sync_work);\n\n\trfkill->registered = false;\n\n\tdevice_del(&rfkill->dev);\n\n\tmutex_lock(&rfkill_global_mutex);\n\trfkill_send_events(rfkill, RFKILL_OP_DEL);\n\tlist_del_init(&rfkill->node);\n\trfkill_global_led_trigger_event();\n\tmutex_unlock(&rfkill_global_mutex);\n\n\trfkill_led_trigger_unregister(rfkill);\n}\nEXPORT_SYMBOL(rfkill_unregister);\n\nvoid rfkill_destroy(struct rfkill *rfkill)\n{\n\tif (rfkill)\n\t\tput_device(&rfkill->dev);\n}\nEXPORT_SYMBOL(rfkill_destroy);\n\nstatic int rfkill_fop_open(struct inode *inode, struct file *file)\n{\n\tstruct rfkill_data *data;\n\tstruct rfkill *rfkill;\n\tstruct rfkill_int_event *ev, *tmp;\n\n\tdata = kzalloc(sizeof(*data), GFP_KERNEL);\n\tif (!data)\n\t\treturn -ENOMEM;\n\n\tdata->max_size = RFKILL_EVENT_SIZE_V1;\n\n\tINIT_LIST_HEAD(&data->events);\n\tmutex_init(&data->mtx);\n\tinit_waitqueue_head(&data->read_wait);\n\n\tmutex_lock(&rfkill_global_mutex);\n\t \n\n\tlist_for_each_entry(rfkill, &rfkill_list, node) {\n\t\tev = kzalloc(sizeof(*ev), GFP_KERNEL);\n\t\tif (!ev)\n\t\t\tgoto free;\n\t\trfkill_sync(rfkill);\n\t\trfkill_fill_event(&ev->ev, rfkill, RFKILL_OP_ADD);\n\t\tmutex_lock(&data->mtx);\n\t\tlist_add_tail(&ev->list, &data->events);\n\t\tmutex_unlock(&data->mtx);\n\t}\n\tlist_add(&data->list, &rfkill_fds);\n\tmutex_unlock(&rfkill_global_mutex);\n\n\tfile->private_data = data;\n\n\treturn stream_open(inode, file);\n\n free:\n\tmutex_unlock(&rfkill_global_mutex);\n\tmutex_destroy(&data->mtx);\n\tlist_for_each_entry_safe(ev, tmp, &data->events, list)\n\t\tkfree(ev);\n\tkfree(data);\n\treturn -ENOMEM;\n}\n\nstatic __poll_t rfkill_fop_poll(struct file *file, poll_table *wait)\n{\n\tstruct rfkill_data *data = file->private_data;\n\t__poll_t res = EPOLLOUT | EPOLLWRNORM;\n\n\tpoll_wait(file, &data->read_wait, wait);\n\n\tmutex_lock(&data->mtx);\n\tif (!list_empty(&data->events))\n\t\tres = EPOLLIN | EPOLLRDNORM;\n\tmutex_unlock(&data->mtx);\n\n\treturn res;\n}\n\nstatic ssize_t rfkill_fop_read(struct file *file, char __user *buf,\n\t\t\t       size_t count, loff_t *pos)\n{\n\tstruct rfkill_data *data = file->private_data;\n\tstruct rfkill_int_event *ev;\n\tunsigned long sz;\n\tint ret;\n\n\tmutex_lock(&data->mtx);\n\n\twhile (list_empty(&data->events)) {\n\t\tif (file->f_flags & O_NONBLOCK) {\n\t\t\tret = -EAGAIN;\n\t\t\tgoto out;\n\t\t}\n\t\tmutex_unlock(&data->mtx);\n\t\t \n\t\tret = wait_event_interruptible(data->read_wait,\n\t\t\t\t\t       !list_empty(&data->events));\n\t\tmutex_lock(&data->mtx);\n\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tev = list_first_entry(&data->events, struct rfkill_int_event,\n\t\t\t\tlist);\n\n\tsz = min_t(unsigned long, sizeof(ev->ev), count);\n\tsz = min_t(unsigned long, sz, data->max_size);\n\tret = sz;\n\tif (copy_to_user(buf, &ev->ev, sz))\n\t\tret = -EFAULT;\n\n\tlist_del(&ev->list);\n\tkfree(ev);\n out:\n\tmutex_unlock(&data->mtx);\n\treturn ret;\n}\n\nstatic ssize_t rfkill_fop_write(struct file *file, const char __user *buf,\n\t\t\t\tsize_t count, loff_t *pos)\n{\n\tstruct rfkill_data *data = file->private_data;\n\tstruct rfkill *rfkill;\n\tstruct rfkill_event_ext ev;\n\tint ret;\n\n\t \n\tif (count < RFKILL_EVENT_SIZE_V1 - 1)\n\t\treturn -EINVAL;\n\n\t \n\tcount = min(count, sizeof(ev));\n\tcount = min_t(size_t, count, data->max_size);\n\tif (copy_from_user(&ev, buf, count))\n\t\treturn -EFAULT;\n\n\tif (ev.type >= NUM_RFKILL_TYPES)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&rfkill_global_mutex);\n\n\tswitch (ev.op) {\n\tcase RFKILL_OP_CHANGE_ALL:\n\t\trfkill_update_global_state(ev.type, ev.soft);\n\t\tlist_for_each_entry(rfkill, &rfkill_list, node)\n\t\t\tif (rfkill->type == ev.type ||\n\t\t\t    ev.type == RFKILL_TYPE_ALL)\n\t\t\t\trfkill_set_block(rfkill, ev.soft);\n\t\tret = 0;\n\t\tbreak;\n\tcase RFKILL_OP_CHANGE:\n\t\tlist_for_each_entry(rfkill, &rfkill_list, node)\n\t\t\tif (rfkill->idx == ev.idx &&\n\t\t\t    (rfkill->type == ev.type ||\n\t\t\t     ev.type == RFKILL_TYPE_ALL))\n\t\t\t\trfkill_set_block(rfkill, ev.soft);\n\t\tret = 0;\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\tmutex_unlock(&rfkill_global_mutex);\n\n\treturn ret ?: count;\n}\n\nstatic int rfkill_fop_release(struct inode *inode, struct file *file)\n{\n\tstruct rfkill_data *data = file->private_data;\n\tstruct rfkill_int_event *ev, *tmp;\n\n\tmutex_lock(&rfkill_global_mutex);\n\tlist_del(&data->list);\n\tmutex_unlock(&rfkill_global_mutex);\n\n\tmutex_destroy(&data->mtx);\n\tlist_for_each_entry_safe(ev, tmp, &data->events, list)\n\t\tkfree(ev);\n\n#ifdef CONFIG_RFKILL_INPUT\n\tif (data->input_handler)\n\t\tif (atomic_dec_return(&rfkill_input_disabled) == 0)\n\t\t\tprintk(KERN_DEBUG \"rfkill: input handler enabled\\n\");\n#endif\n\n\tkfree(data);\n\n\treturn 0;\n}\n\nstatic long rfkill_fop_ioctl(struct file *file, unsigned int cmd,\n\t\t\t     unsigned long arg)\n{\n\tstruct rfkill_data *data = file->private_data;\n\tint ret = -ENOSYS;\n\tu32 size;\n\n\tif (_IOC_TYPE(cmd) != RFKILL_IOC_MAGIC)\n\t\treturn -ENOSYS;\n\n\tmutex_lock(&data->mtx);\n\tswitch (_IOC_NR(cmd)) {\n#ifdef CONFIG_RFKILL_INPUT\n\tcase RFKILL_IOC_NOINPUT:\n\t\tif (!data->input_handler) {\n\t\t\tif (atomic_inc_return(&rfkill_input_disabled) == 1)\n\t\t\t\tprintk(KERN_DEBUG \"rfkill: input handler disabled\\n\");\n\t\t\tdata->input_handler = true;\n\t\t}\n\t\tret = 0;\n\t\tbreak;\n#endif\n\tcase RFKILL_IOC_MAX_SIZE:\n\t\tif (get_user(size, (__u32 __user *)arg)) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (size < RFKILL_EVENT_SIZE_V1 || size > U8_MAX) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tdata->max_size = size;\n\t\tret = 0;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\tmutex_unlock(&data->mtx);\n\n\treturn ret;\n}\n\nstatic const struct file_operations rfkill_fops = {\n\t.owner\t\t= THIS_MODULE,\n\t.open\t\t= rfkill_fop_open,\n\t.read\t\t= rfkill_fop_read,\n\t.write\t\t= rfkill_fop_write,\n\t.poll\t\t= rfkill_fop_poll,\n\t.release\t= rfkill_fop_release,\n\t.unlocked_ioctl\t= rfkill_fop_ioctl,\n\t.compat_ioctl\t= compat_ptr_ioctl,\n\t.llseek\t\t= no_llseek,\n};\n\n#define RFKILL_NAME \"rfkill\"\n\nstatic struct miscdevice rfkill_miscdev = {\n\t.fops\t= &rfkill_fops,\n\t.name\t= RFKILL_NAME,\n\t.minor\t= RFKILL_MINOR,\n};\n\nstatic int __init rfkill_init(void)\n{\n\tint error;\n\n\trfkill_update_global_state(RFKILL_TYPE_ALL, !rfkill_default_state);\n\n\terror = class_register(&rfkill_class);\n\tif (error)\n\t\tgoto error_class;\n\n\terror = misc_register(&rfkill_miscdev);\n\tif (error)\n\t\tgoto error_misc;\n\n\terror = rfkill_global_led_trigger_register();\n\tif (error)\n\t\tgoto error_led_trigger;\n\n#ifdef CONFIG_RFKILL_INPUT\n\terror = rfkill_handler_init();\n\tif (error)\n\t\tgoto error_input;\n#endif\n\n\treturn 0;\n\n#ifdef CONFIG_RFKILL_INPUT\nerror_input:\n\trfkill_global_led_trigger_unregister();\n#endif\nerror_led_trigger:\n\tmisc_deregister(&rfkill_miscdev);\nerror_misc:\n\tclass_unregister(&rfkill_class);\nerror_class:\n\treturn error;\n}\nsubsys_initcall(rfkill_init);\n\nstatic void __exit rfkill_exit(void)\n{\n#ifdef CONFIG_RFKILL_INPUT\n\trfkill_handler_exit();\n#endif\n\trfkill_global_led_trigger_unregister();\n\tmisc_deregister(&rfkill_miscdev);\n\tclass_unregister(&rfkill_class);\n}\nmodule_exit(rfkill_exit);\n\nMODULE_ALIAS_MISCDEV(RFKILL_MINOR);\nMODULE_ALIAS(\"devname:\" RFKILL_NAME);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}