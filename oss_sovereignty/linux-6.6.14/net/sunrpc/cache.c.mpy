{
  "module_name": "cache.c",
  "hash_id": "516b60e12ed4af4e8f2a3820db2238be4794d059a81c9f3faae556ddcf8d93d1",
  "original_prompt": "Ingested from linux-6.6.14/net/sunrpc/cache.c",
  "human_readable_source": "\n \n\n#include <linux/types.h>\n#include <linux/fs.h>\n#include <linux/file.h>\n#include <linux/slab.h>\n#include <linux/signal.h>\n#include <linux/sched.h>\n#include <linux/kmod.h>\n#include <linux/list.h>\n#include <linux/module.h>\n#include <linux/ctype.h>\n#include <linux/string_helpers.h>\n#include <linux/uaccess.h>\n#include <linux/poll.h>\n#include <linux/seq_file.h>\n#include <linux/proc_fs.h>\n#include <linux/net.h>\n#include <linux/workqueue.h>\n#include <linux/mutex.h>\n#include <linux/pagemap.h>\n#include <asm/ioctls.h>\n#include <linux/sunrpc/types.h>\n#include <linux/sunrpc/cache.h>\n#include <linux/sunrpc/stats.h>\n#include <linux/sunrpc/rpc_pipe_fs.h>\n#include <trace/events/sunrpc.h>\n\n#include \"netns.h\"\n#include \"fail.h\"\n\n#define\t RPCDBG_FACILITY RPCDBG_CACHE\n\nstatic bool cache_defer_req(struct cache_req *req, struct cache_head *item);\nstatic void cache_revisit_request(struct cache_head *item);\n\nstatic void cache_init(struct cache_head *h, struct cache_detail *detail)\n{\n\ttime64_t now = seconds_since_boot();\n\tINIT_HLIST_NODE(&h->cache_list);\n\th->flags = 0;\n\tkref_init(&h->ref);\n\th->expiry_time = now + CACHE_NEW_EXPIRY;\n\tif (now <= detail->flush_time)\n\t\t \n\t\tnow = detail->flush_time + 1;\n\th->last_refresh = now;\n}\n\nstatic void cache_fresh_unlocked(struct cache_head *head,\n\t\t\t\tstruct cache_detail *detail);\n\nstatic struct cache_head *sunrpc_cache_find_rcu(struct cache_detail *detail,\n\t\t\t\t\t\tstruct cache_head *key,\n\t\t\t\t\t\tint hash)\n{\n\tstruct hlist_head *head = &detail->hash_table[hash];\n\tstruct cache_head *tmp;\n\n\trcu_read_lock();\n\thlist_for_each_entry_rcu(tmp, head, cache_list) {\n\t\tif (!detail->match(tmp, key))\n\t\t\tcontinue;\n\t\tif (test_bit(CACHE_VALID, &tmp->flags) &&\n\t\t    cache_is_expired(detail, tmp))\n\t\t\tcontinue;\n\t\ttmp = cache_get_rcu(tmp);\n\t\trcu_read_unlock();\n\t\treturn tmp;\n\t}\n\trcu_read_unlock();\n\treturn NULL;\n}\n\nstatic void sunrpc_begin_cache_remove_entry(struct cache_head *ch,\n\t\t\t\t\t    struct cache_detail *cd)\n{\n\t \n\thlist_del_init_rcu(&ch->cache_list);\n\tset_bit(CACHE_CLEANED, &ch->flags);\n\tcd->entries --;\n}\n\nstatic void sunrpc_end_cache_remove_entry(struct cache_head *ch,\n\t\t\t\t\t  struct cache_detail *cd)\n{\n\tcache_fresh_unlocked(ch, cd);\n\tcache_put(ch, cd);\n}\n\nstatic struct cache_head *sunrpc_cache_add_entry(struct cache_detail *detail,\n\t\t\t\t\t\t struct cache_head *key,\n\t\t\t\t\t\t int hash)\n{\n\tstruct cache_head *new, *tmp, *freeme = NULL;\n\tstruct hlist_head *head = &detail->hash_table[hash];\n\n\tnew = detail->alloc();\n\tif (!new)\n\t\treturn NULL;\n\t \n\tcache_init(new, detail);\n\tdetail->init(new, key);\n\n\tspin_lock(&detail->hash_lock);\n\n\t \n\thlist_for_each_entry_rcu(tmp, head, cache_list,\n\t\t\t\t lockdep_is_held(&detail->hash_lock)) {\n\t\tif (!detail->match(tmp, key))\n\t\t\tcontinue;\n\t\tif (test_bit(CACHE_VALID, &tmp->flags) &&\n\t\t    cache_is_expired(detail, tmp)) {\n\t\t\tsunrpc_begin_cache_remove_entry(tmp, detail);\n\t\t\ttrace_cache_entry_expired(detail, tmp);\n\t\t\tfreeme = tmp;\n\t\t\tbreak;\n\t\t}\n\t\tcache_get(tmp);\n\t\tspin_unlock(&detail->hash_lock);\n\t\tcache_put(new, detail);\n\t\treturn tmp;\n\t}\n\n\thlist_add_head_rcu(&new->cache_list, head);\n\tdetail->entries++;\n\tcache_get(new);\n\tspin_unlock(&detail->hash_lock);\n\n\tif (freeme)\n\t\tsunrpc_end_cache_remove_entry(freeme, detail);\n\treturn new;\n}\n\nstruct cache_head *sunrpc_cache_lookup_rcu(struct cache_detail *detail,\n\t\t\t\t\t   struct cache_head *key, int hash)\n{\n\tstruct cache_head *ret;\n\n\tret = sunrpc_cache_find_rcu(detail, key, hash);\n\tif (ret)\n\t\treturn ret;\n\t \n\treturn sunrpc_cache_add_entry(detail, key, hash);\n}\nEXPORT_SYMBOL_GPL(sunrpc_cache_lookup_rcu);\n\nstatic void cache_dequeue(struct cache_detail *detail, struct cache_head *ch);\n\nstatic void cache_fresh_locked(struct cache_head *head, time64_t expiry,\n\t\t\t       struct cache_detail *detail)\n{\n\ttime64_t now = seconds_since_boot();\n\tif (now <= detail->flush_time)\n\t\t \n\t\tnow = detail->flush_time + 1;\n\thead->expiry_time = expiry;\n\thead->last_refresh = now;\n\tsmp_wmb();  \n\tset_bit(CACHE_VALID, &head->flags);\n}\n\nstatic void cache_fresh_unlocked(struct cache_head *head,\n\t\t\t\t struct cache_detail *detail)\n{\n\tif (test_and_clear_bit(CACHE_PENDING, &head->flags)) {\n\t\tcache_revisit_request(head);\n\t\tcache_dequeue(detail, head);\n\t}\n}\n\nstatic void cache_make_negative(struct cache_detail *detail,\n\t\t\t\tstruct cache_head *h)\n{\n\tset_bit(CACHE_NEGATIVE, &h->flags);\n\ttrace_cache_entry_make_negative(detail, h);\n}\n\nstatic void cache_entry_update(struct cache_detail *detail,\n\t\t\t       struct cache_head *h,\n\t\t\t       struct cache_head *new)\n{\n\tif (!test_bit(CACHE_NEGATIVE, &new->flags)) {\n\t\tdetail->update(h, new);\n\t\ttrace_cache_entry_update(detail, h);\n\t} else {\n\t\tcache_make_negative(detail, h);\n\t}\n}\n\nstruct cache_head *sunrpc_cache_update(struct cache_detail *detail,\n\t\t\t\t       struct cache_head *new, struct cache_head *old, int hash)\n{\n\t \n\tstruct cache_head *tmp;\n\n\tif (!test_bit(CACHE_VALID, &old->flags)) {\n\t\tspin_lock(&detail->hash_lock);\n\t\tif (!test_bit(CACHE_VALID, &old->flags)) {\n\t\t\tcache_entry_update(detail, old, new);\n\t\t\tcache_fresh_locked(old, new->expiry_time, detail);\n\t\t\tspin_unlock(&detail->hash_lock);\n\t\t\tcache_fresh_unlocked(old, detail);\n\t\t\treturn old;\n\t\t}\n\t\tspin_unlock(&detail->hash_lock);\n\t}\n\t \n\ttmp = detail->alloc();\n\tif (!tmp) {\n\t\tcache_put(old, detail);\n\t\treturn NULL;\n\t}\n\tcache_init(tmp, detail);\n\tdetail->init(tmp, old);\n\n\tspin_lock(&detail->hash_lock);\n\tcache_entry_update(detail, tmp, new);\n\thlist_add_head(&tmp->cache_list, &detail->hash_table[hash]);\n\tdetail->entries++;\n\tcache_get(tmp);\n\tcache_fresh_locked(tmp, new->expiry_time, detail);\n\tcache_fresh_locked(old, 0, detail);\n\tspin_unlock(&detail->hash_lock);\n\tcache_fresh_unlocked(tmp, detail);\n\tcache_fresh_unlocked(old, detail);\n\tcache_put(old, detail);\n\treturn tmp;\n}\nEXPORT_SYMBOL_GPL(sunrpc_cache_update);\n\nstatic inline int cache_is_valid(struct cache_head *h)\n{\n\tif (!test_bit(CACHE_VALID, &h->flags))\n\t\treturn -EAGAIN;\n\telse {\n\t\t \n\t\tif (test_bit(CACHE_NEGATIVE, &h->flags))\n\t\t\treturn -ENOENT;\n\t\telse {\n\t\t\t \n\t\t\tsmp_rmb();\n\t\t\treturn 0;\n\t\t}\n\t}\n}\n\nstatic int try_to_negate_entry(struct cache_detail *detail, struct cache_head *h)\n{\n\tint rv;\n\n\tspin_lock(&detail->hash_lock);\n\trv = cache_is_valid(h);\n\tif (rv == -EAGAIN) {\n\t\tcache_make_negative(detail, h);\n\t\tcache_fresh_locked(h, seconds_since_boot()+CACHE_NEW_EXPIRY,\n\t\t\t\t   detail);\n\t\trv = -ENOENT;\n\t}\n\tspin_unlock(&detail->hash_lock);\n\tcache_fresh_unlocked(h, detail);\n\treturn rv;\n}\n\n \nint cache_check(struct cache_detail *detail,\n\t\t    struct cache_head *h, struct cache_req *rqstp)\n{\n\tint rv;\n\ttime64_t refresh_age, age;\n\n\t \n\trv = cache_is_valid(h);\n\n\t \n\trefresh_age = (h->expiry_time - h->last_refresh);\n\tage = seconds_since_boot() - h->last_refresh;\n\n\tif (rqstp == NULL) {\n\t\tif (rv == -EAGAIN)\n\t\t\trv = -ENOENT;\n\t} else if (rv == -EAGAIN ||\n\t\t   (h->expiry_time != 0 && age > refresh_age/2)) {\n\t\tdprintk(\"RPC:       Want update, refage=%lld, age=%lld\\n\",\n\t\t\t\trefresh_age, age);\n\t\tswitch (detail->cache_upcall(detail, h)) {\n\t\tcase -EINVAL:\n\t\t\trv = try_to_negate_entry(detail, h);\n\t\t\tbreak;\n\t\tcase -EAGAIN:\n\t\t\tcache_fresh_unlocked(h, detail);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (rv == -EAGAIN) {\n\t\tif (!cache_defer_req(rqstp, h)) {\n\t\t\t \n\t\t\trv = cache_is_valid(h);\n\t\t\tif (rv == -EAGAIN)\n\t\t\t\trv = -ETIMEDOUT;\n\t\t}\n\t}\n\tif (rv)\n\t\tcache_put(h, detail);\n\treturn rv;\n}\nEXPORT_SYMBOL_GPL(cache_check);\n\n \n\nstatic LIST_HEAD(cache_list);\nstatic DEFINE_SPINLOCK(cache_list_lock);\nstatic struct cache_detail *current_detail;\nstatic int current_index;\n\nstatic void do_cache_clean(struct work_struct *work);\nstatic struct delayed_work cache_cleaner;\n\nvoid sunrpc_init_cache_detail(struct cache_detail *cd)\n{\n\tspin_lock_init(&cd->hash_lock);\n\tINIT_LIST_HEAD(&cd->queue);\n\tspin_lock(&cache_list_lock);\n\tcd->nextcheck = 0;\n\tcd->entries = 0;\n\tatomic_set(&cd->writers, 0);\n\tcd->last_close = 0;\n\tcd->last_warn = -1;\n\tlist_add(&cd->others, &cache_list);\n\tspin_unlock(&cache_list_lock);\n\n\t \n\tqueue_delayed_work(system_power_efficient_wq, &cache_cleaner, 0);\n}\nEXPORT_SYMBOL_GPL(sunrpc_init_cache_detail);\n\nvoid sunrpc_destroy_cache_detail(struct cache_detail *cd)\n{\n\tcache_purge(cd);\n\tspin_lock(&cache_list_lock);\n\tspin_lock(&cd->hash_lock);\n\tif (current_detail == cd)\n\t\tcurrent_detail = NULL;\n\tlist_del_init(&cd->others);\n\tspin_unlock(&cd->hash_lock);\n\tspin_unlock(&cache_list_lock);\n\tif (list_empty(&cache_list)) {\n\t\t \n\t\tcancel_delayed_work_sync(&cache_cleaner);\n\t}\n}\nEXPORT_SYMBOL_GPL(sunrpc_destroy_cache_detail);\n\n \nstatic int cache_clean(void)\n{\n\tint rv = 0;\n\tstruct list_head *next;\n\n\tspin_lock(&cache_list_lock);\n\n\t \n\twhile (current_detail == NULL ||\n\t    current_index >= current_detail->hash_size) {\n\t\tif (current_detail)\n\t\t\tnext = current_detail->others.next;\n\t\telse\n\t\t\tnext = cache_list.next;\n\t\tif (next == &cache_list) {\n\t\t\tcurrent_detail = NULL;\n\t\t\tspin_unlock(&cache_list_lock);\n\t\t\treturn -1;\n\t\t}\n\t\tcurrent_detail = list_entry(next, struct cache_detail, others);\n\t\tif (current_detail->nextcheck > seconds_since_boot())\n\t\t\tcurrent_index = current_detail->hash_size;\n\t\telse {\n\t\t\tcurrent_index = 0;\n\t\t\tcurrent_detail->nextcheck = seconds_since_boot()+30*60;\n\t\t}\n\t}\n\n\t \n\twhile (current_detail &&\n\t       current_index < current_detail->hash_size &&\n\t       hlist_empty(&current_detail->hash_table[current_index]))\n\t\tcurrent_index++;\n\n\t \n\n\tif (current_detail && current_index < current_detail->hash_size) {\n\t\tstruct cache_head *ch = NULL;\n\t\tstruct cache_detail *d;\n\t\tstruct hlist_head *head;\n\t\tstruct hlist_node *tmp;\n\n\t\tspin_lock(&current_detail->hash_lock);\n\n\t\t \n\n\t\thead = &current_detail->hash_table[current_index];\n\t\thlist_for_each_entry_safe(ch, tmp, head, cache_list) {\n\t\t\tif (current_detail->nextcheck > ch->expiry_time)\n\t\t\t\tcurrent_detail->nextcheck = ch->expiry_time+1;\n\t\t\tif (!cache_is_expired(current_detail, ch))\n\t\t\t\tcontinue;\n\n\t\t\tsunrpc_begin_cache_remove_entry(ch, current_detail);\n\t\t\ttrace_cache_entry_expired(current_detail, ch);\n\t\t\trv = 1;\n\t\t\tbreak;\n\t\t}\n\n\t\tspin_unlock(&current_detail->hash_lock);\n\t\td = current_detail;\n\t\tif (!ch)\n\t\t\tcurrent_index ++;\n\t\tspin_unlock(&cache_list_lock);\n\t\tif (ch)\n\t\t\tsunrpc_end_cache_remove_entry(ch, d);\n\t} else\n\t\tspin_unlock(&cache_list_lock);\n\n\treturn rv;\n}\n\n \nstatic void do_cache_clean(struct work_struct *work)\n{\n\tint delay;\n\n\tif (list_empty(&cache_list))\n\t\treturn;\n\n\tif (cache_clean() == -1)\n\t\tdelay = round_jiffies_relative(30*HZ);\n\telse\n\t\tdelay = 5;\n\n\tqueue_delayed_work(system_power_efficient_wq, &cache_cleaner, delay);\n}\n\n\n \nvoid cache_flush(void)\n{\n\twhile (cache_clean() != -1)\n\t\tcond_resched();\n\twhile (cache_clean() != -1)\n\t\tcond_resched();\n}\nEXPORT_SYMBOL_GPL(cache_flush);\n\nvoid cache_purge(struct cache_detail *detail)\n{\n\tstruct cache_head *ch = NULL;\n\tstruct hlist_head *head = NULL;\n\tint i = 0;\n\n\tspin_lock(&detail->hash_lock);\n\tif (!detail->entries) {\n\t\tspin_unlock(&detail->hash_lock);\n\t\treturn;\n\t}\n\n\tdprintk(\"RPC: %d entries in %s cache\\n\", detail->entries, detail->name);\n\tfor (i = 0; i < detail->hash_size; i++) {\n\t\thead = &detail->hash_table[i];\n\t\twhile (!hlist_empty(head)) {\n\t\t\tch = hlist_entry(head->first, struct cache_head,\n\t\t\t\t\t cache_list);\n\t\t\tsunrpc_begin_cache_remove_entry(ch, detail);\n\t\t\tspin_unlock(&detail->hash_lock);\n\t\t\tsunrpc_end_cache_remove_entry(ch, detail);\n\t\t\tspin_lock(&detail->hash_lock);\n\t\t}\n\t}\n\tspin_unlock(&detail->hash_lock);\n}\nEXPORT_SYMBOL_GPL(cache_purge);\n\n\n \n\n#define\tDFR_HASHSIZE\t(PAGE_SIZE/sizeof(struct list_head))\n#define\tDFR_HASH(item)\t((((long)item)>>4 ^ (((long)item)>>13)) % DFR_HASHSIZE)\n\n#define\tDFR_MAX\t300\t \n\nstatic DEFINE_SPINLOCK(cache_defer_lock);\nstatic LIST_HEAD(cache_defer_list);\nstatic struct hlist_head cache_defer_hash[DFR_HASHSIZE];\nstatic int cache_defer_cnt;\n\nstatic void __unhash_deferred_req(struct cache_deferred_req *dreq)\n{\n\thlist_del_init(&dreq->hash);\n\tif (!list_empty(&dreq->recent)) {\n\t\tlist_del_init(&dreq->recent);\n\t\tcache_defer_cnt--;\n\t}\n}\n\nstatic void __hash_deferred_req(struct cache_deferred_req *dreq, struct cache_head *item)\n{\n\tint hash = DFR_HASH(item);\n\n\tINIT_LIST_HEAD(&dreq->recent);\n\thlist_add_head(&dreq->hash, &cache_defer_hash[hash]);\n}\n\nstatic void setup_deferral(struct cache_deferred_req *dreq,\n\t\t\t   struct cache_head *item,\n\t\t\t   int count_me)\n{\n\n\tdreq->item = item;\n\n\tspin_lock(&cache_defer_lock);\n\n\t__hash_deferred_req(dreq, item);\n\n\tif (count_me) {\n\t\tcache_defer_cnt++;\n\t\tlist_add(&dreq->recent, &cache_defer_list);\n\t}\n\n\tspin_unlock(&cache_defer_lock);\n\n}\n\nstruct thread_deferred_req {\n\tstruct cache_deferred_req handle;\n\tstruct completion completion;\n};\n\nstatic void cache_restart_thread(struct cache_deferred_req *dreq, int too_many)\n{\n\tstruct thread_deferred_req *dr =\n\t\tcontainer_of(dreq, struct thread_deferred_req, handle);\n\tcomplete(&dr->completion);\n}\n\nstatic void cache_wait_req(struct cache_req *req, struct cache_head *item)\n{\n\tstruct thread_deferred_req sleeper;\n\tstruct cache_deferred_req *dreq = &sleeper.handle;\n\n\tsleeper.completion = COMPLETION_INITIALIZER_ONSTACK(sleeper.completion);\n\tdreq->revisit = cache_restart_thread;\n\n\tsetup_deferral(dreq, item, 0);\n\n\tif (!test_bit(CACHE_PENDING, &item->flags) ||\n\t    wait_for_completion_interruptible_timeout(\n\t\t    &sleeper.completion, req->thread_wait) <= 0) {\n\t\t \n\t\tspin_lock(&cache_defer_lock);\n\t\tif (!hlist_unhashed(&sleeper.handle.hash)) {\n\t\t\t__unhash_deferred_req(&sleeper.handle);\n\t\t\tspin_unlock(&cache_defer_lock);\n\t\t} else {\n\t\t\t \n\t\t\tspin_unlock(&cache_defer_lock);\n\t\t\twait_for_completion(&sleeper.completion);\n\t\t}\n\t}\n}\n\nstatic void cache_limit_defers(void)\n{\n\t \n\tstruct cache_deferred_req *discard = NULL;\n\n\tif (cache_defer_cnt <= DFR_MAX)\n\t\treturn;\n\n\tspin_lock(&cache_defer_lock);\n\n\t \n\tif (cache_defer_cnt > DFR_MAX) {\n\t\tif (get_random_u32_below(2))\n\t\t\tdiscard = list_entry(cache_defer_list.next,\n\t\t\t\t\t     struct cache_deferred_req, recent);\n\t\telse\n\t\t\tdiscard = list_entry(cache_defer_list.prev,\n\t\t\t\t\t     struct cache_deferred_req, recent);\n\t\t__unhash_deferred_req(discard);\n\t}\n\tspin_unlock(&cache_defer_lock);\n\tif (discard)\n\t\tdiscard->revisit(discard, 1);\n}\n\n#if IS_ENABLED(CONFIG_FAIL_SUNRPC)\nstatic inline bool cache_defer_immediately(void)\n{\n\treturn !fail_sunrpc.ignore_cache_wait &&\n\t\tshould_fail(&fail_sunrpc.attr, 1);\n}\n#else\nstatic inline bool cache_defer_immediately(void)\n{\n\treturn false;\n}\n#endif\n\n \nstatic bool cache_defer_req(struct cache_req *req, struct cache_head *item)\n{\n\tstruct cache_deferred_req *dreq;\n\n\tif (!cache_defer_immediately()) {\n\t\tcache_wait_req(req, item);\n\t\tif (!test_bit(CACHE_PENDING, &item->flags))\n\t\t\treturn false;\n\t}\n\n\tdreq = req->defer(req);\n\tif (dreq == NULL)\n\t\treturn false;\n\tsetup_deferral(dreq, item, 1);\n\tif (!test_bit(CACHE_PENDING, &item->flags))\n\t\t \n\t\tcache_revisit_request(item);\n\n\tcache_limit_defers();\n\treturn true;\n}\n\nstatic void cache_revisit_request(struct cache_head *item)\n{\n\tstruct cache_deferred_req *dreq;\n\tstruct list_head pending;\n\tstruct hlist_node *tmp;\n\tint hash = DFR_HASH(item);\n\n\tINIT_LIST_HEAD(&pending);\n\tspin_lock(&cache_defer_lock);\n\n\thlist_for_each_entry_safe(dreq, tmp, &cache_defer_hash[hash], hash)\n\t\tif (dreq->item == item) {\n\t\t\t__unhash_deferred_req(dreq);\n\t\t\tlist_add(&dreq->recent, &pending);\n\t\t}\n\n\tspin_unlock(&cache_defer_lock);\n\n\twhile (!list_empty(&pending)) {\n\t\tdreq = list_entry(pending.next, struct cache_deferred_req, recent);\n\t\tlist_del_init(&dreq->recent);\n\t\tdreq->revisit(dreq, 0);\n\t}\n}\n\nvoid cache_clean_deferred(void *owner)\n{\n\tstruct cache_deferred_req *dreq, *tmp;\n\tstruct list_head pending;\n\n\n\tINIT_LIST_HEAD(&pending);\n\tspin_lock(&cache_defer_lock);\n\n\tlist_for_each_entry_safe(dreq, tmp, &cache_defer_list, recent) {\n\t\tif (dreq->owner == owner) {\n\t\t\t__unhash_deferred_req(dreq);\n\t\t\tlist_add(&dreq->recent, &pending);\n\t\t}\n\t}\n\tspin_unlock(&cache_defer_lock);\n\n\twhile (!list_empty(&pending)) {\n\t\tdreq = list_entry(pending.next, struct cache_deferred_req, recent);\n\t\tlist_del_init(&dreq->recent);\n\t\tdreq->revisit(dreq, 1);\n\t}\n}\n\n \n\nstatic DEFINE_SPINLOCK(queue_lock);\n\nstruct cache_queue {\n\tstruct list_head\tlist;\n\tint\t\t\treader;\t \n};\nstruct cache_request {\n\tstruct cache_queue\tq;\n\tstruct cache_head\t*item;\n\tchar\t\t\t* buf;\n\tint\t\t\tlen;\n\tint\t\t\treaders;\n};\nstruct cache_reader {\n\tstruct cache_queue\tq;\n\tint\t\t\toffset;\t \n};\n\nstatic int cache_request(struct cache_detail *detail,\n\t\t\t       struct cache_request *crq)\n{\n\tchar *bp = crq->buf;\n\tint len = PAGE_SIZE;\n\n\tdetail->cache_request(detail, crq->item, &bp, &len);\n\tif (len < 0)\n\t\treturn -E2BIG;\n\treturn PAGE_SIZE - len;\n}\n\nstatic ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n\t\t\t  loff_t *ppos, struct cache_detail *cd)\n{\n\tstruct cache_reader *rp = filp->private_data;\n\tstruct cache_request *rq;\n\tstruct inode *inode = file_inode(filp);\n\tint err;\n\n\tif (count == 0)\n\t\treturn 0;\n\n\tinode_lock(inode);  \n again:\n\tspin_lock(&queue_lock);\n\t \n\twhile (rp->q.list.next != &cd->queue &&\n\t       list_entry(rp->q.list.next, struct cache_queue, list)\n\t       ->reader) {\n\t\tstruct list_head *next = rp->q.list.next;\n\t\tlist_move(&rp->q.list, next);\n\t}\n\tif (rp->q.list.next == &cd->queue) {\n\t\tspin_unlock(&queue_lock);\n\t\tinode_unlock(inode);\n\t\tWARN_ON_ONCE(rp->offset);\n\t\treturn 0;\n\t}\n\trq = container_of(rp->q.list.next, struct cache_request, q.list);\n\tWARN_ON_ONCE(rq->q.reader);\n\tif (rp->offset == 0)\n\t\trq->readers++;\n\tspin_unlock(&queue_lock);\n\n\tif (rq->len == 0) {\n\t\terr = cache_request(cd, rq);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t\trq->len = err;\n\t}\n\n\tif (rp->offset == 0 && !test_bit(CACHE_PENDING, &rq->item->flags)) {\n\t\terr = -EAGAIN;\n\t\tspin_lock(&queue_lock);\n\t\tlist_move(&rp->q.list, &rq->q.list);\n\t\tspin_unlock(&queue_lock);\n\t} else {\n\t\tif (rp->offset + count > rq->len)\n\t\t\tcount = rq->len - rp->offset;\n\t\terr = -EFAULT;\n\t\tif (copy_to_user(buf, rq->buf + rp->offset, count))\n\t\t\tgoto out;\n\t\trp->offset += count;\n\t\tif (rp->offset >= rq->len) {\n\t\t\trp->offset = 0;\n\t\t\tspin_lock(&queue_lock);\n\t\t\tlist_move(&rp->q.list, &rq->q.list);\n\t\t\tspin_unlock(&queue_lock);\n\t\t}\n\t\terr = 0;\n\t}\n out:\n\tif (rp->offset == 0) {\n\t\t \n\t\tspin_lock(&queue_lock);\n\t\trq->readers--;\n\t\tif (rq->readers == 0 &&\n\t\t    !test_bit(CACHE_PENDING, &rq->item->flags)) {\n\t\t\tlist_del(&rq->q.list);\n\t\t\tspin_unlock(&queue_lock);\n\t\t\tcache_put(rq->item, cd);\n\t\t\tkfree(rq->buf);\n\t\t\tkfree(rq);\n\t\t} else\n\t\t\tspin_unlock(&queue_lock);\n\t}\n\tif (err == -EAGAIN)\n\t\tgoto again;\n\tinode_unlock(inode);\n\treturn err ? err :  count;\n}\n\nstatic ssize_t cache_do_downcall(char *kaddr, const char __user *buf,\n\t\t\t\t size_t count, struct cache_detail *cd)\n{\n\tssize_t ret;\n\n\tif (count == 0)\n\t\treturn -EINVAL;\n\tif (copy_from_user(kaddr, buf, count))\n\t\treturn -EFAULT;\n\tkaddr[count] = '\\0';\n\tret = cd->cache_parse(cd, kaddr, count);\n\tif (!ret)\n\t\tret = count;\n\treturn ret;\n}\n\nstatic ssize_t cache_downcall(struct address_space *mapping,\n\t\t\t      const char __user *buf,\n\t\t\t      size_t count, struct cache_detail *cd)\n{\n\tchar *write_buf;\n\tssize_t ret = -ENOMEM;\n\n\tif (count >= 32768) {  \n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\twrite_buf = kvmalloc(count + 1, GFP_KERNEL);\n\tif (!write_buf)\n\t\tgoto out;\n\n\tret = cache_do_downcall(write_buf, buf, count, cd);\n\tkvfree(write_buf);\nout:\n\treturn ret;\n}\n\nstatic ssize_t cache_write(struct file *filp, const char __user *buf,\n\t\t\t   size_t count, loff_t *ppos,\n\t\t\t   struct cache_detail *cd)\n{\n\tstruct address_space *mapping = filp->f_mapping;\n\tstruct inode *inode = file_inode(filp);\n\tssize_t ret = -EINVAL;\n\n\tif (!cd->cache_parse)\n\t\tgoto out;\n\n\tinode_lock(inode);\n\tret = cache_downcall(mapping, buf, count, cd);\n\tinode_unlock(inode);\nout:\n\treturn ret;\n}\n\nstatic DECLARE_WAIT_QUEUE_HEAD(queue_wait);\n\nstatic __poll_t cache_poll(struct file *filp, poll_table *wait,\n\t\t\t       struct cache_detail *cd)\n{\n\t__poll_t mask;\n\tstruct cache_reader *rp = filp->private_data;\n\tstruct cache_queue *cq;\n\n\tpoll_wait(filp, &queue_wait, wait);\n\n\t \n\tmask = EPOLLOUT | EPOLLWRNORM;\n\n\tif (!rp)\n\t\treturn mask;\n\n\tspin_lock(&queue_lock);\n\n\tfor (cq= &rp->q; &cq->list != &cd->queue;\n\t     cq = list_entry(cq->list.next, struct cache_queue, list))\n\t\tif (!cq->reader) {\n\t\t\tmask |= EPOLLIN | EPOLLRDNORM;\n\t\t\tbreak;\n\t\t}\n\tspin_unlock(&queue_lock);\n\treturn mask;\n}\n\nstatic int cache_ioctl(struct inode *ino, struct file *filp,\n\t\t       unsigned int cmd, unsigned long arg,\n\t\t       struct cache_detail *cd)\n{\n\tint len = 0;\n\tstruct cache_reader *rp = filp->private_data;\n\tstruct cache_queue *cq;\n\n\tif (cmd != FIONREAD || !rp)\n\t\treturn -EINVAL;\n\n\tspin_lock(&queue_lock);\n\n\t \n\tfor (cq= &rp->q; &cq->list != &cd->queue;\n\t     cq = list_entry(cq->list.next, struct cache_queue, list))\n\t\tif (!cq->reader) {\n\t\t\tstruct cache_request *cr =\n\t\t\t\tcontainer_of(cq, struct cache_request, q);\n\t\t\tlen = cr->len - rp->offset;\n\t\t\tbreak;\n\t\t}\n\tspin_unlock(&queue_lock);\n\n\treturn put_user(len, (int __user *)arg);\n}\n\nstatic int cache_open(struct inode *inode, struct file *filp,\n\t\t      struct cache_detail *cd)\n{\n\tstruct cache_reader *rp = NULL;\n\n\tif (!cd || !try_module_get(cd->owner))\n\t\treturn -EACCES;\n\tnonseekable_open(inode, filp);\n\tif (filp->f_mode & FMODE_READ) {\n\t\trp = kmalloc(sizeof(*rp), GFP_KERNEL);\n\t\tif (!rp) {\n\t\t\tmodule_put(cd->owner);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\trp->offset = 0;\n\t\trp->q.reader = 1;\n\n\t\tspin_lock(&queue_lock);\n\t\tlist_add(&rp->q.list, &cd->queue);\n\t\tspin_unlock(&queue_lock);\n\t}\n\tif (filp->f_mode & FMODE_WRITE)\n\t\tatomic_inc(&cd->writers);\n\tfilp->private_data = rp;\n\treturn 0;\n}\n\nstatic int cache_release(struct inode *inode, struct file *filp,\n\t\t\t struct cache_detail *cd)\n{\n\tstruct cache_reader *rp = filp->private_data;\n\n\tif (rp) {\n\t\tspin_lock(&queue_lock);\n\t\tif (rp->offset) {\n\t\t\tstruct cache_queue *cq;\n\t\t\tfor (cq= &rp->q; &cq->list != &cd->queue;\n\t\t\t     cq = list_entry(cq->list.next, struct cache_queue, list))\n\t\t\t\tif (!cq->reader) {\n\t\t\t\t\tcontainer_of(cq, struct cache_request, q)\n\t\t\t\t\t\t->readers--;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\trp->offset = 0;\n\t\t}\n\t\tlist_del(&rp->q.list);\n\t\tspin_unlock(&queue_lock);\n\n\t\tfilp->private_data = NULL;\n\t\tkfree(rp);\n\n\t}\n\tif (filp->f_mode & FMODE_WRITE) {\n\t\tatomic_dec(&cd->writers);\n\t\tcd->last_close = seconds_since_boot();\n\t}\n\tmodule_put(cd->owner);\n\treturn 0;\n}\n\n\n\nstatic void cache_dequeue(struct cache_detail *detail, struct cache_head *ch)\n{\n\tstruct cache_queue *cq, *tmp;\n\tstruct cache_request *cr;\n\tstruct list_head dequeued;\n\n\tINIT_LIST_HEAD(&dequeued);\n\tspin_lock(&queue_lock);\n\tlist_for_each_entry_safe(cq, tmp, &detail->queue, list)\n\t\tif (!cq->reader) {\n\t\t\tcr = container_of(cq, struct cache_request, q);\n\t\t\tif (cr->item != ch)\n\t\t\t\tcontinue;\n\t\t\tif (test_bit(CACHE_PENDING, &ch->flags))\n\t\t\t\t \n\t\t\t\tbreak;\n\t\t\tif (cr->readers != 0)\n\t\t\t\tcontinue;\n\t\t\tlist_move(&cr->q.list, &dequeued);\n\t\t}\n\tspin_unlock(&queue_lock);\n\twhile (!list_empty(&dequeued)) {\n\t\tcr = list_entry(dequeued.next, struct cache_request, q.list);\n\t\tlist_del(&cr->q.list);\n\t\tcache_put(cr->item, detail);\n\t\tkfree(cr->buf);\n\t\tkfree(cr);\n\t}\n}\n\n \n\nvoid qword_add(char **bpp, int *lp, char *str)\n{\n\tchar *bp = *bpp;\n\tint len = *lp;\n\tint ret;\n\n\tif (len < 0) return;\n\n\tret = string_escape_str(str, bp, len, ESCAPE_OCTAL, \"\\\\ \\n\\t\");\n\tif (ret >= len) {\n\t\tbp += len;\n\t\tlen = -1;\n\t} else {\n\t\tbp += ret;\n\t\tlen -= ret;\n\t\t*bp++ = ' ';\n\t\tlen--;\n\t}\n\t*bpp = bp;\n\t*lp = len;\n}\nEXPORT_SYMBOL_GPL(qword_add);\n\nvoid qword_addhex(char **bpp, int *lp, char *buf, int blen)\n{\n\tchar *bp = *bpp;\n\tint len = *lp;\n\n\tif (len < 0) return;\n\n\tif (len > 2) {\n\t\t*bp++ = '\\\\';\n\t\t*bp++ = 'x';\n\t\tlen -= 2;\n\t\twhile (blen && len >= 2) {\n\t\t\tbp = hex_byte_pack(bp, *buf++);\n\t\t\tlen -= 2;\n\t\t\tblen--;\n\t\t}\n\t}\n\tif (blen || len<1) len = -1;\n\telse {\n\t\t*bp++ = ' ';\n\t\tlen--;\n\t}\n\t*bpp = bp;\n\t*lp = len;\n}\nEXPORT_SYMBOL_GPL(qword_addhex);\n\nstatic void warn_no_listener(struct cache_detail *detail)\n{\n\tif (detail->last_warn != detail->last_close) {\n\t\tdetail->last_warn = detail->last_close;\n\t\tif (detail->warn_no_listener)\n\t\t\tdetail->warn_no_listener(detail, detail->last_close != 0);\n\t}\n}\n\nstatic bool cache_listeners_exist(struct cache_detail *detail)\n{\n\tif (atomic_read(&detail->writers))\n\t\treturn true;\n\tif (detail->last_close == 0)\n\t\t \n\t\treturn false;\n\tif (detail->last_close < seconds_since_boot() - 30)\n\t\t \n\t\t return false;\n\treturn true;\n}\n\n \nstatic int cache_pipe_upcall(struct cache_detail *detail, struct cache_head *h)\n{\n\tchar *buf;\n\tstruct cache_request *crq;\n\tint ret = 0;\n\n\tif (test_bit(CACHE_CLEANED, &h->flags))\n\t\t \n\t\treturn -EAGAIN;\n\n\tbuf = kmalloc(PAGE_SIZE, GFP_KERNEL);\n\tif (!buf)\n\t\treturn -EAGAIN;\n\n\tcrq = kmalloc(sizeof (*crq), GFP_KERNEL);\n\tif (!crq) {\n\t\tkfree(buf);\n\t\treturn -EAGAIN;\n\t}\n\n\tcrq->q.reader = 0;\n\tcrq->buf = buf;\n\tcrq->len = 0;\n\tcrq->readers = 0;\n\tspin_lock(&queue_lock);\n\tif (test_bit(CACHE_PENDING, &h->flags)) {\n\t\tcrq->item = cache_get(h);\n\t\tlist_add_tail(&crq->q.list, &detail->queue);\n\t\ttrace_cache_entry_upcall(detail, h);\n\t} else\n\t\t \n\t\tret = -EAGAIN;\n\tspin_unlock(&queue_lock);\n\twake_up(&queue_wait);\n\tif (ret == -EAGAIN) {\n\t\tkfree(buf);\n\t\tkfree(crq);\n\t}\n\treturn ret;\n}\n\nint sunrpc_cache_pipe_upcall(struct cache_detail *detail, struct cache_head *h)\n{\n\tif (test_and_set_bit(CACHE_PENDING, &h->flags))\n\t\treturn 0;\n\treturn cache_pipe_upcall(detail, h);\n}\nEXPORT_SYMBOL_GPL(sunrpc_cache_pipe_upcall);\n\nint sunrpc_cache_pipe_upcall_timeout(struct cache_detail *detail,\n\t\t\t\t     struct cache_head *h)\n{\n\tif (!cache_listeners_exist(detail)) {\n\t\twarn_no_listener(detail);\n\t\ttrace_cache_entry_no_listener(detail, h);\n\t\treturn -EINVAL;\n\t}\n\treturn sunrpc_cache_pipe_upcall(detail, h);\n}\nEXPORT_SYMBOL_GPL(sunrpc_cache_pipe_upcall_timeout);\n\n \n\nint qword_get(char **bpp, char *dest, int bufsize)\n{\n\t \n\tchar *bp = *bpp;\n\tint len = 0;\n\n\twhile (*bp == ' ') bp++;\n\n\tif (bp[0] == '\\\\' && bp[1] == 'x') {\n\t\t \n\t\tbp += 2;\n\t\twhile (len < bufsize - 1) {\n\t\t\tint h, l;\n\n\t\t\th = hex_to_bin(bp[0]);\n\t\t\tif (h < 0)\n\t\t\t\tbreak;\n\n\t\t\tl = hex_to_bin(bp[1]);\n\t\t\tif (l < 0)\n\t\t\t\tbreak;\n\n\t\t\t*dest++ = (h << 4) | l;\n\t\t\tbp += 2;\n\t\t\tlen++;\n\t\t}\n\t} else {\n\t\t \n\t\twhile (*bp != ' ' && *bp != '\\n' && *bp && len < bufsize-1) {\n\t\t\tif (*bp == '\\\\' &&\n\t\t\t    isodigit(bp[1]) && (bp[1] <= '3') &&\n\t\t\t    isodigit(bp[2]) &&\n\t\t\t    isodigit(bp[3])) {\n\t\t\t\tint byte = (*++bp -'0');\n\t\t\t\tbp++;\n\t\t\t\tbyte = (byte << 3) | (*bp++ - '0');\n\t\t\t\tbyte = (byte << 3) | (*bp++ - '0');\n\t\t\t\t*dest++ = byte;\n\t\t\t\tlen++;\n\t\t\t} else {\n\t\t\t\t*dest++ = *bp++;\n\t\t\t\tlen++;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (*bp != ' ' && *bp != '\\n' && *bp != '\\0')\n\t\treturn -1;\n\twhile (*bp == ' ') bp++;\n\t*bpp = bp;\n\t*dest = '\\0';\n\treturn len;\n}\nEXPORT_SYMBOL_GPL(qword_get);\n\n\n \n\nstatic void *__cache_seq_start(struct seq_file *m, loff_t *pos)\n{\n\tloff_t n = *pos;\n\tunsigned int hash, entry;\n\tstruct cache_head *ch;\n\tstruct cache_detail *cd = m->private;\n\n\tif (!n--)\n\t\treturn SEQ_START_TOKEN;\n\thash = n >> 32;\n\tentry = n & ((1LL<<32) - 1);\n\n\thlist_for_each_entry_rcu(ch, &cd->hash_table[hash], cache_list)\n\t\tif (!entry--)\n\t\t\treturn ch;\n\tn &= ~((1LL<<32) - 1);\n\tdo {\n\t\thash++;\n\t\tn += 1LL<<32;\n\t} while(hash < cd->hash_size &&\n\t\thlist_empty(&cd->hash_table[hash]));\n\tif (hash >= cd->hash_size)\n\t\treturn NULL;\n\t*pos = n+1;\n\treturn hlist_entry_safe(rcu_dereference_raw(\n\t\t\t\thlist_first_rcu(&cd->hash_table[hash])),\n\t\t\t\tstruct cache_head, cache_list);\n}\n\nstatic void *cache_seq_next(struct seq_file *m, void *p, loff_t *pos)\n{\n\tstruct cache_head *ch = p;\n\tint hash = (*pos >> 32);\n\tstruct cache_detail *cd = m->private;\n\n\tif (p == SEQ_START_TOKEN)\n\t\thash = 0;\n\telse if (ch->cache_list.next == NULL) {\n\t\thash++;\n\t\t*pos += 1LL<<32;\n\t} else {\n\t\t++*pos;\n\t\treturn hlist_entry_safe(rcu_dereference_raw(\n\t\t\t\t\thlist_next_rcu(&ch->cache_list)),\n\t\t\t\t\tstruct cache_head, cache_list);\n\t}\n\t*pos &= ~((1LL<<32) - 1);\n\twhile (hash < cd->hash_size &&\n\t       hlist_empty(&cd->hash_table[hash])) {\n\t\thash++;\n\t\t*pos += 1LL<<32;\n\t}\n\tif (hash >= cd->hash_size)\n\t\treturn NULL;\n\t++*pos;\n\treturn hlist_entry_safe(rcu_dereference_raw(\n\t\t\t\thlist_first_rcu(&cd->hash_table[hash])),\n\t\t\t\tstruct cache_head, cache_list);\n}\n\nvoid *cache_seq_start_rcu(struct seq_file *m, loff_t *pos)\n\t__acquires(RCU)\n{\n\trcu_read_lock();\n\treturn __cache_seq_start(m, pos);\n}\nEXPORT_SYMBOL_GPL(cache_seq_start_rcu);\n\nvoid *cache_seq_next_rcu(struct seq_file *file, void *p, loff_t *pos)\n{\n\treturn cache_seq_next(file, p, pos);\n}\nEXPORT_SYMBOL_GPL(cache_seq_next_rcu);\n\nvoid cache_seq_stop_rcu(struct seq_file *m, void *p)\n\t__releases(RCU)\n{\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL_GPL(cache_seq_stop_rcu);\n\nstatic int c_show(struct seq_file *m, void *p)\n{\n\tstruct cache_head *cp = p;\n\tstruct cache_detail *cd = m->private;\n\n\tif (p == SEQ_START_TOKEN)\n\t\treturn cd->cache_show(m, cd, NULL);\n\n\tifdebug(CACHE)\n\t\tseq_printf(m, \"# expiry=%lld refcnt=%d flags=%lx\\n\",\n\t\t\t   convert_to_wallclock(cp->expiry_time),\n\t\t\t   kref_read(&cp->ref), cp->flags);\n\tcache_get(cp);\n\tif (cache_check(cd, cp, NULL))\n\t\t \n\t\tseq_puts(m, \"# \");\n\telse {\n\t\tif (cache_is_expired(cd, cp))\n\t\t\tseq_puts(m, \"# \");\n\t\tcache_put(cp, cd);\n\t}\n\n\treturn cd->cache_show(m, cd, cp);\n}\n\nstatic const struct seq_operations cache_content_op = {\n\t.start\t= cache_seq_start_rcu,\n\t.next\t= cache_seq_next_rcu,\n\t.stop\t= cache_seq_stop_rcu,\n\t.show\t= c_show,\n};\n\nstatic int content_open(struct inode *inode, struct file *file,\n\t\t\tstruct cache_detail *cd)\n{\n\tstruct seq_file *seq;\n\tint err;\n\n\tif (!cd || !try_module_get(cd->owner))\n\t\treturn -EACCES;\n\n\terr = seq_open(file, &cache_content_op);\n\tif (err) {\n\t\tmodule_put(cd->owner);\n\t\treturn err;\n\t}\n\n\tseq = file->private_data;\n\tseq->private = cd;\n\treturn 0;\n}\n\nstatic int content_release(struct inode *inode, struct file *file,\n\t\tstruct cache_detail *cd)\n{\n\tint ret = seq_release(inode, file);\n\tmodule_put(cd->owner);\n\treturn ret;\n}\n\nstatic int open_flush(struct inode *inode, struct file *file,\n\t\t\tstruct cache_detail *cd)\n{\n\tif (!cd || !try_module_get(cd->owner))\n\t\treturn -EACCES;\n\treturn nonseekable_open(inode, file);\n}\n\nstatic int release_flush(struct inode *inode, struct file *file,\n\t\t\tstruct cache_detail *cd)\n{\n\tmodule_put(cd->owner);\n\treturn 0;\n}\n\nstatic ssize_t read_flush(struct file *file, char __user *buf,\n\t\t\t  size_t count, loff_t *ppos,\n\t\t\t  struct cache_detail *cd)\n{\n\tchar tbuf[22];\n\tsize_t len;\n\n\tlen = snprintf(tbuf, sizeof(tbuf), \"%llu\\n\",\n\t\t\tconvert_to_wallclock(cd->flush_time));\n\treturn simple_read_from_buffer(buf, count, ppos, tbuf, len);\n}\n\nstatic ssize_t write_flush(struct file *file, const char __user *buf,\n\t\t\t   size_t count, loff_t *ppos,\n\t\t\t   struct cache_detail *cd)\n{\n\tchar tbuf[20];\n\tchar *ep;\n\ttime64_t now;\n\n\tif (*ppos || count > sizeof(tbuf)-1)\n\t\treturn -EINVAL;\n\tif (copy_from_user(tbuf, buf, count))\n\t\treturn -EFAULT;\n\ttbuf[count] = 0;\n\tsimple_strtoul(tbuf, &ep, 0);\n\tif (*ep && *ep != '\\n')\n\t\treturn -EINVAL;\n\t \n\n\tnow = seconds_since_boot();\n\t \n\n\tif (cd->flush_time >= now)\n\t\tnow = cd->flush_time + 1;\n\n\tcd->flush_time = now;\n\tcd->nextcheck = now;\n\tcache_flush();\n\n\tif (cd->flush)\n\t\tcd->flush();\n\n\t*ppos += count;\n\treturn count;\n}\n\nstatic ssize_t cache_read_procfs(struct file *filp, char __user *buf,\n\t\t\t\t size_t count, loff_t *ppos)\n{\n\tstruct cache_detail *cd = pde_data(file_inode(filp));\n\n\treturn cache_read(filp, buf, count, ppos, cd);\n}\n\nstatic ssize_t cache_write_procfs(struct file *filp, const char __user *buf,\n\t\t\t\t  size_t count, loff_t *ppos)\n{\n\tstruct cache_detail *cd = pde_data(file_inode(filp));\n\n\treturn cache_write(filp, buf, count, ppos, cd);\n}\n\nstatic __poll_t cache_poll_procfs(struct file *filp, poll_table *wait)\n{\n\tstruct cache_detail *cd = pde_data(file_inode(filp));\n\n\treturn cache_poll(filp, wait, cd);\n}\n\nstatic long cache_ioctl_procfs(struct file *filp,\n\t\t\t       unsigned int cmd, unsigned long arg)\n{\n\tstruct inode *inode = file_inode(filp);\n\tstruct cache_detail *cd = pde_data(inode);\n\n\treturn cache_ioctl(inode, filp, cmd, arg, cd);\n}\n\nstatic int cache_open_procfs(struct inode *inode, struct file *filp)\n{\n\tstruct cache_detail *cd = pde_data(inode);\n\n\treturn cache_open(inode, filp, cd);\n}\n\nstatic int cache_release_procfs(struct inode *inode, struct file *filp)\n{\n\tstruct cache_detail *cd = pde_data(inode);\n\n\treturn cache_release(inode, filp, cd);\n}\n\nstatic const struct proc_ops cache_channel_proc_ops = {\n\t.proc_lseek\t= no_llseek,\n\t.proc_read\t= cache_read_procfs,\n\t.proc_write\t= cache_write_procfs,\n\t.proc_poll\t= cache_poll_procfs,\n\t.proc_ioctl\t= cache_ioctl_procfs,  \n\t.proc_open\t= cache_open_procfs,\n\t.proc_release\t= cache_release_procfs,\n};\n\nstatic int content_open_procfs(struct inode *inode, struct file *filp)\n{\n\tstruct cache_detail *cd = pde_data(inode);\n\n\treturn content_open(inode, filp, cd);\n}\n\nstatic int content_release_procfs(struct inode *inode, struct file *filp)\n{\n\tstruct cache_detail *cd = pde_data(inode);\n\n\treturn content_release(inode, filp, cd);\n}\n\nstatic const struct proc_ops content_proc_ops = {\n\t.proc_open\t= content_open_procfs,\n\t.proc_read\t= seq_read,\n\t.proc_lseek\t= seq_lseek,\n\t.proc_release\t= content_release_procfs,\n};\n\nstatic int open_flush_procfs(struct inode *inode, struct file *filp)\n{\n\tstruct cache_detail *cd = pde_data(inode);\n\n\treturn open_flush(inode, filp, cd);\n}\n\nstatic int release_flush_procfs(struct inode *inode, struct file *filp)\n{\n\tstruct cache_detail *cd = pde_data(inode);\n\n\treturn release_flush(inode, filp, cd);\n}\n\nstatic ssize_t read_flush_procfs(struct file *filp, char __user *buf,\n\t\t\t    size_t count, loff_t *ppos)\n{\n\tstruct cache_detail *cd = pde_data(file_inode(filp));\n\n\treturn read_flush(filp, buf, count, ppos, cd);\n}\n\nstatic ssize_t write_flush_procfs(struct file *filp,\n\t\t\t\t  const char __user *buf,\n\t\t\t\t  size_t count, loff_t *ppos)\n{\n\tstruct cache_detail *cd = pde_data(file_inode(filp));\n\n\treturn write_flush(filp, buf, count, ppos, cd);\n}\n\nstatic const struct proc_ops cache_flush_proc_ops = {\n\t.proc_open\t= open_flush_procfs,\n\t.proc_read\t= read_flush_procfs,\n\t.proc_write\t= write_flush_procfs,\n\t.proc_release\t= release_flush_procfs,\n\t.proc_lseek\t= no_llseek,\n};\n\nstatic void remove_cache_proc_entries(struct cache_detail *cd)\n{\n\tif (cd->procfs) {\n\t\tproc_remove(cd->procfs);\n\t\tcd->procfs = NULL;\n\t}\n}\n\n#ifdef CONFIG_PROC_FS\nstatic int create_cache_proc_entries(struct cache_detail *cd, struct net *net)\n{\n\tstruct proc_dir_entry *p;\n\tstruct sunrpc_net *sn;\n\n\tsn = net_generic(net, sunrpc_net_id);\n\tcd->procfs = proc_mkdir(cd->name, sn->proc_net_rpc);\n\tif (cd->procfs == NULL)\n\t\tgoto out_nomem;\n\n\tp = proc_create_data(\"flush\", S_IFREG | 0600,\n\t\t\t     cd->procfs, &cache_flush_proc_ops, cd);\n\tif (p == NULL)\n\t\tgoto out_nomem;\n\n\tif (cd->cache_request || cd->cache_parse) {\n\t\tp = proc_create_data(\"channel\", S_IFREG | 0600, cd->procfs,\n\t\t\t\t     &cache_channel_proc_ops, cd);\n\t\tif (p == NULL)\n\t\t\tgoto out_nomem;\n\t}\n\tif (cd->cache_show) {\n\t\tp = proc_create_data(\"content\", S_IFREG | 0400, cd->procfs,\n\t\t\t\t     &content_proc_ops, cd);\n\t\tif (p == NULL)\n\t\t\tgoto out_nomem;\n\t}\n\treturn 0;\nout_nomem:\n\tremove_cache_proc_entries(cd);\n\treturn -ENOMEM;\n}\n#else  \nstatic int create_cache_proc_entries(struct cache_detail *cd, struct net *net)\n{\n\treturn 0;\n}\n#endif\n\nvoid __init cache_initialize(void)\n{\n\tINIT_DEFERRABLE_WORK(&cache_cleaner, do_cache_clean);\n}\n\nint cache_register_net(struct cache_detail *cd, struct net *net)\n{\n\tint ret;\n\n\tsunrpc_init_cache_detail(cd);\n\tret = create_cache_proc_entries(cd, net);\n\tif (ret)\n\t\tsunrpc_destroy_cache_detail(cd);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(cache_register_net);\n\nvoid cache_unregister_net(struct cache_detail *cd, struct net *net)\n{\n\tremove_cache_proc_entries(cd);\n\tsunrpc_destroy_cache_detail(cd);\n}\nEXPORT_SYMBOL_GPL(cache_unregister_net);\n\nstruct cache_detail *cache_create_net(const struct cache_detail *tmpl, struct net *net)\n{\n\tstruct cache_detail *cd;\n\tint i;\n\n\tcd = kmemdup(tmpl, sizeof(struct cache_detail), GFP_KERNEL);\n\tif (cd == NULL)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tcd->hash_table = kcalloc(cd->hash_size, sizeof(struct hlist_head),\n\t\t\t\t GFP_KERNEL);\n\tif (cd->hash_table == NULL) {\n\t\tkfree(cd);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tfor (i = 0; i < cd->hash_size; i++)\n\t\tINIT_HLIST_HEAD(&cd->hash_table[i]);\n\tcd->net = net;\n\treturn cd;\n}\nEXPORT_SYMBOL_GPL(cache_create_net);\n\nvoid cache_destroy_net(struct cache_detail *cd, struct net *net)\n{\n\tkfree(cd->hash_table);\n\tkfree(cd);\n}\nEXPORT_SYMBOL_GPL(cache_destroy_net);\n\nstatic ssize_t cache_read_pipefs(struct file *filp, char __user *buf,\n\t\t\t\t size_t count, loff_t *ppos)\n{\n\tstruct cache_detail *cd = RPC_I(file_inode(filp))->private;\n\n\treturn cache_read(filp, buf, count, ppos, cd);\n}\n\nstatic ssize_t cache_write_pipefs(struct file *filp, const char __user *buf,\n\t\t\t\t  size_t count, loff_t *ppos)\n{\n\tstruct cache_detail *cd = RPC_I(file_inode(filp))->private;\n\n\treturn cache_write(filp, buf, count, ppos, cd);\n}\n\nstatic __poll_t cache_poll_pipefs(struct file *filp, poll_table *wait)\n{\n\tstruct cache_detail *cd = RPC_I(file_inode(filp))->private;\n\n\treturn cache_poll(filp, wait, cd);\n}\n\nstatic long cache_ioctl_pipefs(struct file *filp,\n\t\t\t      unsigned int cmd, unsigned long arg)\n{\n\tstruct inode *inode = file_inode(filp);\n\tstruct cache_detail *cd = RPC_I(inode)->private;\n\n\treturn cache_ioctl(inode, filp, cmd, arg, cd);\n}\n\nstatic int cache_open_pipefs(struct inode *inode, struct file *filp)\n{\n\tstruct cache_detail *cd = RPC_I(inode)->private;\n\n\treturn cache_open(inode, filp, cd);\n}\n\nstatic int cache_release_pipefs(struct inode *inode, struct file *filp)\n{\n\tstruct cache_detail *cd = RPC_I(inode)->private;\n\n\treturn cache_release(inode, filp, cd);\n}\n\nconst struct file_operations cache_file_operations_pipefs = {\n\t.owner\t\t= THIS_MODULE,\n\t.llseek\t\t= no_llseek,\n\t.read\t\t= cache_read_pipefs,\n\t.write\t\t= cache_write_pipefs,\n\t.poll\t\t= cache_poll_pipefs,\n\t.unlocked_ioctl\t= cache_ioctl_pipefs,  \n\t.open\t\t= cache_open_pipefs,\n\t.release\t= cache_release_pipefs,\n};\n\nstatic int content_open_pipefs(struct inode *inode, struct file *filp)\n{\n\tstruct cache_detail *cd = RPC_I(inode)->private;\n\n\treturn content_open(inode, filp, cd);\n}\n\nstatic int content_release_pipefs(struct inode *inode, struct file *filp)\n{\n\tstruct cache_detail *cd = RPC_I(inode)->private;\n\n\treturn content_release(inode, filp, cd);\n}\n\nconst struct file_operations content_file_operations_pipefs = {\n\t.open\t\t= content_open_pipefs,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= content_release_pipefs,\n};\n\nstatic int open_flush_pipefs(struct inode *inode, struct file *filp)\n{\n\tstruct cache_detail *cd = RPC_I(inode)->private;\n\n\treturn open_flush(inode, filp, cd);\n}\n\nstatic int release_flush_pipefs(struct inode *inode, struct file *filp)\n{\n\tstruct cache_detail *cd = RPC_I(inode)->private;\n\n\treturn release_flush(inode, filp, cd);\n}\n\nstatic ssize_t read_flush_pipefs(struct file *filp, char __user *buf,\n\t\t\t    size_t count, loff_t *ppos)\n{\n\tstruct cache_detail *cd = RPC_I(file_inode(filp))->private;\n\n\treturn read_flush(filp, buf, count, ppos, cd);\n}\n\nstatic ssize_t write_flush_pipefs(struct file *filp,\n\t\t\t\t  const char __user *buf,\n\t\t\t\t  size_t count, loff_t *ppos)\n{\n\tstruct cache_detail *cd = RPC_I(file_inode(filp))->private;\n\n\treturn write_flush(filp, buf, count, ppos, cd);\n}\n\nconst struct file_operations cache_flush_operations_pipefs = {\n\t.open\t\t= open_flush_pipefs,\n\t.read\t\t= read_flush_pipefs,\n\t.write\t\t= write_flush_pipefs,\n\t.release\t= release_flush_pipefs,\n\t.llseek\t\t= no_llseek,\n};\n\nint sunrpc_cache_register_pipefs(struct dentry *parent,\n\t\t\t\t const char *name, umode_t umode,\n\t\t\t\t struct cache_detail *cd)\n{\n\tstruct dentry *dir = rpc_create_cache_dir(parent, name, umode, cd);\n\tif (IS_ERR(dir))\n\t\treturn PTR_ERR(dir);\n\tcd->pipefs = dir;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(sunrpc_cache_register_pipefs);\n\nvoid sunrpc_cache_unregister_pipefs(struct cache_detail *cd)\n{\n\tif (cd->pipefs) {\n\t\trpc_remove_cache_dir(cd->pipefs);\n\t\tcd->pipefs = NULL;\n\t}\n}\nEXPORT_SYMBOL_GPL(sunrpc_cache_unregister_pipefs);\n\nvoid sunrpc_cache_unhash(struct cache_detail *cd, struct cache_head *h)\n{\n\tspin_lock(&cd->hash_lock);\n\tif (!hlist_unhashed(&h->cache_list)){\n\t\tsunrpc_begin_cache_remove_entry(h, cd);\n\t\tspin_unlock(&cd->hash_lock);\n\t\tsunrpc_end_cache_remove_entry(h, cd);\n\t} else\n\t\tspin_unlock(&cd->hash_lock);\n}\nEXPORT_SYMBOL_GPL(sunrpc_cache_unhash);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}