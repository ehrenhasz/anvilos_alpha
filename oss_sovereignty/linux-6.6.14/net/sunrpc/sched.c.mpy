{
  "module_name": "sched.c",
  "hash_id": "574312842165229ec505b7180c5329e8e148a92d9399d5ee36a4e8f12b871ef5",
  "original_prompt": "Ingested from linux-6.6.14/net/sunrpc/sched.c",
  "human_readable_source": "\n \n\n#include <linux/module.h>\n\n#include <linux/sched.h>\n#include <linux/interrupt.h>\n#include <linux/slab.h>\n#include <linux/mempool.h>\n#include <linux/smp.h>\n#include <linux/spinlock.h>\n#include <linux/mutex.h>\n#include <linux/freezer.h>\n#include <linux/sched/mm.h>\n\n#include <linux/sunrpc/clnt.h>\n#include <linux/sunrpc/metrics.h>\n\n#include \"sunrpc.h\"\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/sunrpc.h>\n\n \n#define RPC_BUFFER_MAXSIZE\t(2048)\n#define RPC_BUFFER_POOLSIZE\t(8)\n#define RPC_TASK_POOLSIZE\t(8)\nstatic struct kmem_cache\t*rpc_task_slabp __read_mostly;\nstatic struct kmem_cache\t*rpc_buffer_slabp __read_mostly;\nstatic mempool_t\t*rpc_task_mempool __read_mostly;\nstatic mempool_t\t*rpc_buffer_mempool __read_mostly;\n\nstatic void\t\t\trpc_async_schedule(struct work_struct *);\nstatic void\t\t\t rpc_release_task(struct rpc_task *task);\nstatic void __rpc_queue_timer_fn(struct work_struct *);\n\n \nstatic struct rpc_wait_queue delay_queue;\n\n \nstruct workqueue_struct *rpciod_workqueue __read_mostly;\nstruct workqueue_struct *xprtiod_workqueue __read_mostly;\nEXPORT_SYMBOL_GPL(xprtiod_workqueue);\n\ngfp_t rpc_task_gfp_mask(void)\n{\n\tif (current->flags & PF_WQ_WORKER)\n\t\treturn GFP_KERNEL | __GFP_NORETRY | __GFP_NOWARN;\n\treturn GFP_KERNEL;\n}\nEXPORT_SYMBOL_GPL(rpc_task_gfp_mask);\n\nbool rpc_task_set_rpc_status(struct rpc_task *task, int rpc_status)\n{\n\tif (cmpxchg(&task->tk_rpc_status, 0, rpc_status) == 0)\n\t\treturn true;\n\treturn false;\n}\n\nunsigned long\nrpc_task_timeout(const struct rpc_task *task)\n{\n\tunsigned long timeout = READ_ONCE(task->tk_timeout);\n\n\tif (timeout != 0) {\n\t\tunsigned long now = jiffies;\n\t\tif (time_before(now, timeout))\n\t\t\treturn timeout - now;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(rpc_task_timeout);\n\n \nstatic void\n__rpc_disable_timer(struct rpc_wait_queue *queue, struct rpc_task *task)\n{\n\tif (list_empty(&task->u.tk_wait.timer_list))\n\t\treturn;\n\ttask->tk_timeout = 0;\n\tlist_del(&task->u.tk_wait.timer_list);\n\tif (list_empty(&queue->timer_list.list))\n\t\tcancel_delayed_work(&queue->timer_list.dwork);\n}\n\nstatic void\nrpc_set_queue_timer(struct rpc_wait_queue *queue, unsigned long expires)\n{\n\tunsigned long now = jiffies;\n\tqueue->timer_list.expires = expires;\n\tif (time_before_eq(expires, now))\n\t\texpires = 0;\n\telse\n\t\texpires -= now;\n\tmod_delayed_work(rpciod_workqueue, &queue->timer_list.dwork, expires);\n}\n\n \nstatic void\n__rpc_add_timer(struct rpc_wait_queue *queue, struct rpc_task *task,\n\t\tunsigned long timeout)\n{\n\ttask->tk_timeout = timeout;\n\tif (list_empty(&queue->timer_list.list) || time_before(timeout, queue->timer_list.expires))\n\t\trpc_set_queue_timer(queue, timeout);\n\tlist_add(&task->u.tk_wait.timer_list, &queue->timer_list.list);\n}\n\nstatic void rpc_set_waitqueue_priority(struct rpc_wait_queue *queue, int priority)\n{\n\tif (queue->priority != priority) {\n\t\tqueue->priority = priority;\n\t\tqueue->nr = 1U << priority;\n\t}\n}\n\nstatic void rpc_reset_waitqueue_priority(struct rpc_wait_queue *queue)\n{\n\trpc_set_waitqueue_priority(queue, queue->maxpriority);\n}\n\n \nstatic void\n__rpc_list_enqueue_task(struct list_head *q, struct rpc_task *task)\n{\n\tstruct rpc_task *t;\n\n\tlist_for_each_entry(t, q, u.tk_wait.list) {\n\t\tif (t->tk_owner == task->tk_owner) {\n\t\t\tlist_add_tail(&task->u.tk_wait.links,\n\t\t\t\t\t&t->u.tk_wait.links);\n\t\t\t \n\t\t\ttask->u.tk_wait.list.next = q;\n\t\t\ttask->u.tk_wait.list.prev = NULL;\n\t\t\treturn;\n\t\t}\n\t}\n\tINIT_LIST_HEAD(&task->u.tk_wait.links);\n\tlist_add_tail(&task->u.tk_wait.list, q);\n}\n\n \nstatic void\n__rpc_list_dequeue_task(struct rpc_task *task)\n{\n\tstruct list_head *q;\n\tstruct rpc_task *t;\n\n\tif (task->u.tk_wait.list.prev == NULL) {\n\t\tlist_del(&task->u.tk_wait.links);\n\t\treturn;\n\t}\n\tif (!list_empty(&task->u.tk_wait.links)) {\n\t\tt = list_first_entry(&task->u.tk_wait.links,\n\t\t\t\tstruct rpc_task,\n\t\t\t\tu.tk_wait.links);\n\t\t \n\t\tq = t->u.tk_wait.list.next;\n\t\tlist_add_tail(&t->u.tk_wait.list, q);\n\t\tlist_del(&task->u.tk_wait.links);\n\t}\n\tlist_del(&task->u.tk_wait.list);\n}\n\n \nstatic void __rpc_add_wait_queue_priority(struct rpc_wait_queue *queue,\n\t\tstruct rpc_task *task,\n\t\tunsigned char queue_priority)\n{\n\tif (unlikely(queue_priority > queue->maxpriority))\n\t\tqueue_priority = queue->maxpriority;\n\t__rpc_list_enqueue_task(&queue->tasks[queue_priority], task);\n}\n\n \nstatic void __rpc_add_wait_queue(struct rpc_wait_queue *queue,\n\t\tstruct rpc_task *task,\n\t\tunsigned char queue_priority)\n{\n\tINIT_LIST_HEAD(&task->u.tk_wait.timer_list);\n\tif (RPC_IS_PRIORITY(queue))\n\t\t__rpc_add_wait_queue_priority(queue, task, queue_priority);\n\telse\n\t\tlist_add_tail(&task->u.tk_wait.list, &queue->tasks[0]);\n\ttask->tk_waitqueue = queue;\n\tqueue->qlen++;\n\t \n\tsmp_wmb();\n\trpc_set_queued(task);\n}\n\n \nstatic void __rpc_remove_wait_queue_priority(struct rpc_task *task)\n{\n\t__rpc_list_dequeue_task(task);\n}\n\n \nstatic void __rpc_remove_wait_queue(struct rpc_wait_queue *queue, struct rpc_task *task)\n{\n\t__rpc_disable_timer(queue, task);\n\tif (RPC_IS_PRIORITY(queue))\n\t\t__rpc_remove_wait_queue_priority(task);\n\telse\n\t\tlist_del(&task->u.tk_wait.list);\n\tqueue->qlen--;\n}\n\nstatic void __rpc_init_priority_wait_queue(struct rpc_wait_queue *queue, const char *qname, unsigned char nr_queues)\n{\n\tint i;\n\n\tspin_lock_init(&queue->lock);\n\tfor (i = 0; i < ARRAY_SIZE(queue->tasks); i++)\n\t\tINIT_LIST_HEAD(&queue->tasks[i]);\n\tqueue->maxpriority = nr_queues - 1;\n\trpc_reset_waitqueue_priority(queue);\n\tqueue->qlen = 0;\n\tqueue->timer_list.expires = 0;\n\tINIT_DELAYED_WORK(&queue->timer_list.dwork, __rpc_queue_timer_fn);\n\tINIT_LIST_HEAD(&queue->timer_list.list);\n\trpc_assign_waitqueue_name(queue, qname);\n}\n\nvoid rpc_init_priority_wait_queue(struct rpc_wait_queue *queue, const char *qname)\n{\n\t__rpc_init_priority_wait_queue(queue, qname, RPC_NR_PRIORITY);\n}\nEXPORT_SYMBOL_GPL(rpc_init_priority_wait_queue);\n\nvoid rpc_init_wait_queue(struct rpc_wait_queue *queue, const char *qname)\n{\n\t__rpc_init_priority_wait_queue(queue, qname, 1);\n}\nEXPORT_SYMBOL_GPL(rpc_init_wait_queue);\n\nvoid rpc_destroy_wait_queue(struct rpc_wait_queue *queue)\n{\n\tcancel_delayed_work_sync(&queue->timer_list.dwork);\n}\nEXPORT_SYMBOL_GPL(rpc_destroy_wait_queue);\n\nstatic int rpc_wait_bit_killable(struct wait_bit_key *key, int mode)\n{\n\tschedule();\n\tif (signal_pending_state(mode, current))\n\t\treturn -ERESTARTSYS;\n\treturn 0;\n}\n\n#if IS_ENABLED(CONFIG_SUNRPC_DEBUG) || IS_ENABLED(CONFIG_TRACEPOINTS)\nstatic void rpc_task_set_debuginfo(struct rpc_task *task)\n{\n\tstruct rpc_clnt *clnt = task->tk_client;\n\n\t \n\tif (!clnt) {\n\t\tstatic atomic_t rpc_pid;\n\n\t\ttask->tk_pid = atomic_inc_return(&rpc_pid);\n\t\treturn;\n\t}\n\n\ttask->tk_pid = atomic_inc_return(&clnt->cl_pid);\n}\n#else\nstatic inline void rpc_task_set_debuginfo(struct rpc_task *task)\n{\n}\n#endif\n\nstatic void rpc_set_active(struct rpc_task *task)\n{\n\trpc_task_set_debuginfo(task);\n\tset_bit(RPC_TASK_ACTIVE, &task->tk_runstate);\n\ttrace_rpc_task_begin(task, NULL);\n}\n\n \nstatic int rpc_complete_task(struct rpc_task *task)\n{\n\tvoid *m = &task->tk_runstate;\n\twait_queue_head_t *wq = bit_waitqueue(m, RPC_TASK_ACTIVE);\n\tstruct wait_bit_key k = __WAIT_BIT_KEY_INITIALIZER(m, RPC_TASK_ACTIVE);\n\tunsigned long flags;\n\tint ret;\n\n\ttrace_rpc_task_complete(task, NULL);\n\n\tspin_lock_irqsave(&wq->lock, flags);\n\tclear_bit(RPC_TASK_ACTIVE, &task->tk_runstate);\n\tret = atomic_dec_and_test(&task->tk_count);\n\tif (waitqueue_active(wq))\n\t\t__wake_up_locked_key(wq, TASK_NORMAL, &k);\n\tspin_unlock_irqrestore(&wq->lock, flags);\n\treturn ret;\n}\n\n \nint rpc_wait_for_completion_task(struct rpc_task *task)\n{\n\treturn out_of_line_wait_on_bit(&task->tk_runstate, RPC_TASK_ACTIVE,\n\t\t\trpc_wait_bit_killable, TASK_KILLABLE|TASK_FREEZABLE_UNSAFE);\n}\nEXPORT_SYMBOL_GPL(rpc_wait_for_completion_task);\n\n \nstatic void rpc_make_runnable(struct workqueue_struct *wq,\n\t\tstruct rpc_task *task)\n{\n\tbool need_wakeup = !rpc_test_and_set_running(task);\n\n\trpc_clear_queued(task);\n\tif (!need_wakeup)\n\t\treturn;\n\tif (RPC_IS_ASYNC(task)) {\n\t\tINIT_WORK(&task->u.tk_work, rpc_async_schedule);\n\t\tqueue_work(wq, &task->u.tk_work);\n\t} else\n\t\twake_up_bit(&task->tk_runstate, RPC_TASK_QUEUED);\n}\n\n \nstatic void __rpc_do_sleep_on_priority(struct rpc_wait_queue *q,\n\t\tstruct rpc_task *task,\n\t\tunsigned char queue_priority)\n{\n\ttrace_rpc_task_sleep(task, q);\n\n\t__rpc_add_wait_queue(q, task, queue_priority);\n}\n\nstatic void __rpc_sleep_on_priority(struct rpc_wait_queue *q,\n\t\tstruct rpc_task *task,\n\t\tunsigned char queue_priority)\n{\n\tif (WARN_ON_ONCE(RPC_IS_QUEUED(task)))\n\t\treturn;\n\t__rpc_do_sleep_on_priority(q, task, queue_priority);\n}\n\nstatic void __rpc_sleep_on_priority_timeout(struct rpc_wait_queue *q,\n\t\tstruct rpc_task *task, unsigned long timeout,\n\t\tunsigned char queue_priority)\n{\n\tif (WARN_ON_ONCE(RPC_IS_QUEUED(task)))\n\t\treturn;\n\tif (time_is_after_jiffies(timeout)) {\n\t\t__rpc_do_sleep_on_priority(q, task, queue_priority);\n\t\t__rpc_add_timer(q, task, timeout);\n\t} else\n\t\ttask->tk_status = -ETIMEDOUT;\n}\n\nstatic void rpc_set_tk_callback(struct rpc_task *task, rpc_action action)\n{\n\tif (action && !WARN_ON_ONCE(task->tk_callback != NULL))\n\t\ttask->tk_callback = action;\n}\n\nstatic bool rpc_sleep_check_activated(struct rpc_task *task)\n{\n\t \n\tif (WARN_ON_ONCE(!RPC_IS_ACTIVATED(task))) {\n\t\ttask->tk_status = -EIO;\n\t\trpc_put_task_async(task);\n\t\treturn false;\n\t}\n\treturn true;\n}\n\nvoid rpc_sleep_on_timeout(struct rpc_wait_queue *q, struct rpc_task *task,\n\t\t\t\trpc_action action, unsigned long timeout)\n{\n\tif (!rpc_sleep_check_activated(task))\n\t\treturn;\n\n\trpc_set_tk_callback(task, action);\n\n\t \n\tspin_lock(&q->lock);\n\t__rpc_sleep_on_priority_timeout(q, task, timeout, task->tk_priority);\n\tspin_unlock(&q->lock);\n}\nEXPORT_SYMBOL_GPL(rpc_sleep_on_timeout);\n\nvoid rpc_sleep_on(struct rpc_wait_queue *q, struct rpc_task *task,\n\t\t\t\trpc_action action)\n{\n\tif (!rpc_sleep_check_activated(task))\n\t\treturn;\n\n\trpc_set_tk_callback(task, action);\n\n\tWARN_ON_ONCE(task->tk_timeout != 0);\n\t \n\tspin_lock(&q->lock);\n\t__rpc_sleep_on_priority(q, task, task->tk_priority);\n\tspin_unlock(&q->lock);\n}\nEXPORT_SYMBOL_GPL(rpc_sleep_on);\n\nvoid rpc_sleep_on_priority_timeout(struct rpc_wait_queue *q,\n\t\tstruct rpc_task *task, unsigned long timeout, int priority)\n{\n\tif (!rpc_sleep_check_activated(task))\n\t\treturn;\n\n\tpriority -= RPC_PRIORITY_LOW;\n\t \n\tspin_lock(&q->lock);\n\t__rpc_sleep_on_priority_timeout(q, task, timeout, priority);\n\tspin_unlock(&q->lock);\n}\nEXPORT_SYMBOL_GPL(rpc_sleep_on_priority_timeout);\n\nvoid rpc_sleep_on_priority(struct rpc_wait_queue *q, struct rpc_task *task,\n\t\tint priority)\n{\n\tif (!rpc_sleep_check_activated(task))\n\t\treturn;\n\n\tWARN_ON_ONCE(task->tk_timeout != 0);\n\tpriority -= RPC_PRIORITY_LOW;\n\t \n\tspin_lock(&q->lock);\n\t__rpc_sleep_on_priority(q, task, priority);\n\tspin_unlock(&q->lock);\n}\nEXPORT_SYMBOL_GPL(rpc_sleep_on_priority);\n\n \nstatic void __rpc_do_wake_up_task_on_wq(struct workqueue_struct *wq,\n\t\tstruct rpc_wait_queue *queue,\n\t\tstruct rpc_task *task)\n{\n\t \n\tif (!RPC_IS_ACTIVATED(task)) {\n\t\tprintk(KERN_ERR \"RPC: Inactive task (%p) being woken up!\\n\", task);\n\t\treturn;\n\t}\n\n\ttrace_rpc_task_wakeup(task, queue);\n\n\t__rpc_remove_wait_queue(queue, task);\n\n\trpc_make_runnable(wq, task);\n}\n\n \nstatic struct rpc_task *\nrpc_wake_up_task_on_wq_queue_action_locked(struct workqueue_struct *wq,\n\t\tstruct rpc_wait_queue *queue, struct rpc_task *task,\n\t\tbool (*action)(struct rpc_task *, void *), void *data)\n{\n\tif (RPC_IS_QUEUED(task)) {\n\t\tsmp_rmb();\n\t\tif (task->tk_waitqueue == queue) {\n\t\t\tif (action == NULL || action(task, data)) {\n\t\t\t\t__rpc_do_wake_up_task_on_wq(wq, queue, task);\n\t\t\t\treturn task;\n\t\t\t}\n\t\t}\n\t}\n\treturn NULL;\n}\n\n \nstatic void rpc_wake_up_task_queue_locked(struct rpc_wait_queue *queue,\n\t\t\t\t\t  struct rpc_task *task)\n{\n\trpc_wake_up_task_on_wq_queue_action_locked(rpciod_workqueue, queue,\n\t\t\t\t\t\t   task, NULL, NULL);\n}\n\n \nvoid rpc_wake_up_queued_task(struct rpc_wait_queue *queue, struct rpc_task *task)\n{\n\tif (!RPC_IS_QUEUED(task))\n\t\treturn;\n\tspin_lock(&queue->lock);\n\trpc_wake_up_task_queue_locked(queue, task);\n\tspin_unlock(&queue->lock);\n}\nEXPORT_SYMBOL_GPL(rpc_wake_up_queued_task);\n\nstatic bool rpc_task_action_set_status(struct rpc_task *task, void *status)\n{\n\ttask->tk_status = *(int *)status;\n\treturn true;\n}\n\nstatic void\nrpc_wake_up_task_queue_set_status_locked(struct rpc_wait_queue *queue,\n\t\tstruct rpc_task *task, int status)\n{\n\trpc_wake_up_task_on_wq_queue_action_locked(rpciod_workqueue, queue,\n\t\t\ttask, rpc_task_action_set_status, &status);\n}\n\n \nvoid\nrpc_wake_up_queued_task_set_status(struct rpc_wait_queue *queue,\n\t\tstruct rpc_task *task, int status)\n{\n\tif (!RPC_IS_QUEUED(task))\n\t\treturn;\n\tspin_lock(&queue->lock);\n\trpc_wake_up_task_queue_set_status_locked(queue, task, status);\n\tspin_unlock(&queue->lock);\n}\n\n \nstatic struct rpc_task *__rpc_find_next_queued_priority(struct rpc_wait_queue *queue)\n{\n\tstruct list_head *q;\n\tstruct rpc_task *task;\n\n\t \n\tq = &queue->tasks[RPC_NR_PRIORITY - 1];\n\tif (queue->maxpriority > RPC_PRIORITY_PRIVILEGED && !list_empty(q)) {\n\t\ttask = list_first_entry(q, struct rpc_task, u.tk_wait.list);\n\t\tgoto out;\n\t}\n\n\t \n\tq = &queue->tasks[queue->priority];\n\tif (!list_empty(q) && queue->nr) {\n\t\tqueue->nr--;\n\t\ttask = list_first_entry(q, struct rpc_task, u.tk_wait.list);\n\t\tgoto out;\n\t}\n\n\t \n\tdo {\n\t\tif (q == &queue->tasks[0])\n\t\t\tq = &queue->tasks[queue->maxpriority];\n\t\telse\n\t\t\tq = q - 1;\n\t\tif (!list_empty(q)) {\n\t\t\ttask = list_first_entry(q, struct rpc_task, u.tk_wait.list);\n\t\t\tgoto new_queue;\n\t\t}\n\t} while (q != &queue->tasks[queue->priority]);\n\n\trpc_reset_waitqueue_priority(queue);\n\treturn NULL;\n\nnew_queue:\n\trpc_set_waitqueue_priority(queue, (unsigned int)(q - &queue->tasks[0]));\nout:\n\treturn task;\n}\n\nstatic struct rpc_task *__rpc_find_next_queued(struct rpc_wait_queue *queue)\n{\n\tif (RPC_IS_PRIORITY(queue))\n\t\treturn __rpc_find_next_queued_priority(queue);\n\tif (!list_empty(&queue->tasks[0]))\n\t\treturn list_first_entry(&queue->tasks[0], struct rpc_task, u.tk_wait.list);\n\treturn NULL;\n}\n\n \nstruct rpc_task *rpc_wake_up_first_on_wq(struct workqueue_struct *wq,\n\t\tstruct rpc_wait_queue *queue,\n\t\tbool (*func)(struct rpc_task *, void *), void *data)\n{\n\tstruct rpc_task\t*task = NULL;\n\n\tspin_lock(&queue->lock);\n\ttask = __rpc_find_next_queued(queue);\n\tif (task != NULL)\n\t\ttask = rpc_wake_up_task_on_wq_queue_action_locked(wq, queue,\n\t\t\t\ttask, func, data);\n\tspin_unlock(&queue->lock);\n\n\treturn task;\n}\n\n \nstruct rpc_task *rpc_wake_up_first(struct rpc_wait_queue *queue,\n\t\tbool (*func)(struct rpc_task *, void *), void *data)\n{\n\treturn rpc_wake_up_first_on_wq(rpciod_workqueue, queue, func, data);\n}\nEXPORT_SYMBOL_GPL(rpc_wake_up_first);\n\nstatic bool rpc_wake_up_next_func(struct rpc_task *task, void *data)\n{\n\treturn true;\n}\n\n \nstruct rpc_task *rpc_wake_up_next(struct rpc_wait_queue *queue)\n{\n\treturn rpc_wake_up_first(queue, rpc_wake_up_next_func, NULL);\n}\nEXPORT_SYMBOL_GPL(rpc_wake_up_next);\n\n \nstatic void rpc_wake_up_locked(struct rpc_wait_queue *queue)\n{\n\tstruct rpc_task *task;\n\n\tfor (;;) {\n\t\ttask = __rpc_find_next_queued(queue);\n\t\tif (task == NULL)\n\t\t\tbreak;\n\t\trpc_wake_up_task_queue_locked(queue, task);\n\t}\n}\n\n \nvoid rpc_wake_up(struct rpc_wait_queue *queue)\n{\n\tspin_lock(&queue->lock);\n\trpc_wake_up_locked(queue);\n\tspin_unlock(&queue->lock);\n}\nEXPORT_SYMBOL_GPL(rpc_wake_up);\n\n \nstatic void rpc_wake_up_status_locked(struct rpc_wait_queue *queue, int status)\n{\n\tstruct rpc_task *task;\n\n\tfor (;;) {\n\t\ttask = __rpc_find_next_queued(queue);\n\t\tif (task == NULL)\n\t\t\tbreak;\n\t\trpc_wake_up_task_queue_set_status_locked(queue, task, status);\n\t}\n}\n\n \nvoid rpc_wake_up_status(struct rpc_wait_queue *queue, int status)\n{\n\tspin_lock(&queue->lock);\n\trpc_wake_up_status_locked(queue, status);\n\tspin_unlock(&queue->lock);\n}\nEXPORT_SYMBOL_GPL(rpc_wake_up_status);\n\nstatic void __rpc_queue_timer_fn(struct work_struct *work)\n{\n\tstruct rpc_wait_queue *queue = container_of(work,\n\t\t\tstruct rpc_wait_queue,\n\t\t\ttimer_list.dwork.work);\n\tstruct rpc_task *task, *n;\n\tunsigned long expires, now, timeo;\n\n\tspin_lock(&queue->lock);\n\texpires = now = jiffies;\n\tlist_for_each_entry_safe(task, n, &queue->timer_list.list, u.tk_wait.timer_list) {\n\t\ttimeo = task->tk_timeout;\n\t\tif (time_after_eq(now, timeo)) {\n\t\t\ttrace_rpc_task_timeout(task, task->tk_action);\n\t\t\ttask->tk_status = -ETIMEDOUT;\n\t\t\trpc_wake_up_task_queue_locked(queue, task);\n\t\t\tcontinue;\n\t\t}\n\t\tif (expires == now || time_after(expires, timeo))\n\t\t\texpires = timeo;\n\t}\n\tif (!list_empty(&queue->timer_list.list))\n\t\trpc_set_queue_timer(queue, expires);\n\tspin_unlock(&queue->lock);\n}\n\nstatic void __rpc_atrun(struct rpc_task *task)\n{\n\tif (task->tk_status == -ETIMEDOUT)\n\t\ttask->tk_status = 0;\n}\n\n \nvoid rpc_delay(struct rpc_task *task, unsigned long delay)\n{\n\trpc_sleep_on_timeout(&delay_queue, task, __rpc_atrun, jiffies + delay);\n}\nEXPORT_SYMBOL_GPL(rpc_delay);\n\n \nvoid rpc_prepare_task(struct rpc_task *task)\n{\n\ttask->tk_ops->rpc_call_prepare(task, task->tk_calldata);\n}\n\nstatic void\nrpc_init_task_statistics(struct rpc_task *task)\n{\n\t \n\ttask->tk_garb_retry = 2;\n\ttask->tk_cred_retry = 2;\n\n\t \n\ttask->tk_start = ktime_get();\n}\n\nstatic void\nrpc_reset_task_statistics(struct rpc_task *task)\n{\n\ttask->tk_timeouts = 0;\n\ttask->tk_flags &= ~(RPC_CALL_MAJORSEEN|RPC_TASK_SENT);\n\trpc_init_task_statistics(task);\n}\n\n \nvoid rpc_exit_task(struct rpc_task *task)\n{\n\ttrace_rpc_task_end(task, task->tk_action);\n\ttask->tk_action = NULL;\n\tif (task->tk_ops->rpc_count_stats)\n\t\ttask->tk_ops->rpc_count_stats(task, task->tk_calldata);\n\telse if (task->tk_client)\n\t\trpc_count_iostats(task, task->tk_client->cl_metrics);\n\tif (task->tk_ops->rpc_call_done != NULL) {\n\t\ttrace_rpc_task_call_done(task, task->tk_ops->rpc_call_done);\n\t\ttask->tk_ops->rpc_call_done(task, task->tk_calldata);\n\t\tif (task->tk_action != NULL) {\n\t\t\t \n\t\t\txprt_release(task);\n\t\t\trpc_reset_task_statistics(task);\n\t\t}\n\t}\n}\n\nvoid rpc_signal_task(struct rpc_task *task)\n{\n\tstruct rpc_wait_queue *queue;\n\n\tif (!RPC_IS_ACTIVATED(task))\n\t\treturn;\n\n\tif (!rpc_task_set_rpc_status(task, -ERESTARTSYS))\n\t\treturn;\n\ttrace_rpc_task_signalled(task, task->tk_action);\n\tset_bit(RPC_TASK_SIGNALLED, &task->tk_runstate);\n\tsmp_mb__after_atomic();\n\tqueue = READ_ONCE(task->tk_waitqueue);\n\tif (queue)\n\t\trpc_wake_up_queued_task(queue, task);\n}\n\nvoid rpc_task_try_cancel(struct rpc_task *task, int error)\n{\n\tstruct rpc_wait_queue *queue;\n\n\tif (!rpc_task_set_rpc_status(task, error))\n\t\treturn;\n\tqueue = READ_ONCE(task->tk_waitqueue);\n\tif (queue)\n\t\trpc_wake_up_queued_task(queue, task);\n}\n\nvoid rpc_exit(struct rpc_task *task, int status)\n{\n\ttask->tk_status = status;\n\ttask->tk_action = rpc_exit_task;\n\trpc_wake_up_queued_task(task->tk_waitqueue, task);\n}\nEXPORT_SYMBOL_GPL(rpc_exit);\n\nvoid rpc_release_calldata(const struct rpc_call_ops *ops, void *calldata)\n{\n\tif (ops->rpc_release != NULL)\n\t\tops->rpc_release(calldata);\n}\n\nstatic bool xprt_needs_memalloc(struct rpc_xprt *xprt, struct rpc_task *tk)\n{\n\tif (!xprt)\n\t\treturn false;\n\tif (!atomic_read(&xprt->swapper))\n\t\treturn false;\n\treturn test_bit(XPRT_LOCKED, &xprt->state) && xprt->snd_task == tk;\n}\n\n \nstatic void __rpc_execute(struct rpc_task *task)\n{\n\tstruct rpc_wait_queue *queue;\n\tint task_is_async = RPC_IS_ASYNC(task);\n\tint status = 0;\n\tunsigned long pflags = current->flags;\n\n\tWARN_ON_ONCE(RPC_IS_QUEUED(task));\n\tif (RPC_IS_QUEUED(task))\n\t\treturn;\n\n\tfor (;;) {\n\t\tvoid (*do_action)(struct rpc_task *);\n\n\t\t \n\t\tdo_action = task->tk_action;\n\t\t \n\t\tif (do_action && do_action != rpc_exit_task &&\n\t\t    (status = READ_ONCE(task->tk_rpc_status)) != 0) {\n\t\t\ttask->tk_status = status;\n\t\t\tdo_action = rpc_exit_task;\n\t\t}\n\t\t \n\t\tif (task->tk_callback) {\n\t\t\tdo_action = task->tk_callback;\n\t\t\ttask->tk_callback = NULL;\n\t\t}\n\t\tif (!do_action)\n\t\t\tbreak;\n\t\tif (RPC_IS_SWAPPER(task) ||\n\t\t    xprt_needs_memalloc(task->tk_xprt, task))\n\t\t\tcurrent->flags |= PF_MEMALLOC;\n\n\t\ttrace_rpc_task_run_action(task, do_action);\n\t\tdo_action(task);\n\n\t\t \n\t\tif (!RPC_IS_QUEUED(task)) {\n\t\t\tcond_resched();\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tqueue = task->tk_waitqueue;\n\t\tspin_lock(&queue->lock);\n\t\tif (!RPC_IS_QUEUED(task)) {\n\t\t\tspin_unlock(&queue->lock);\n\t\t\tcontinue;\n\t\t}\n\t\t \n\t\tif (READ_ONCE(task->tk_rpc_status) != 0) {\n\t\t\trpc_wake_up_task_queue_locked(queue, task);\n\t\t\tspin_unlock(&queue->lock);\n\t\t\tcontinue;\n\t\t}\n\t\trpc_clear_running(task);\n\t\tspin_unlock(&queue->lock);\n\t\tif (task_is_async)\n\t\t\tgoto out;\n\n\t\t \n\t\ttrace_rpc_task_sync_sleep(task, task->tk_action);\n\t\tstatus = out_of_line_wait_on_bit(&task->tk_runstate,\n\t\t\t\tRPC_TASK_QUEUED, rpc_wait_bit_killable,\n\t\t\t\tTASK_KILLABLE|TASK_FREEZABLE);\n\t\tif (status < 0) {\n\t\t\t \n\t\t\trpc_signal_task(task);\n\t\t}\n\t\ttrace_rpc_task_sync_wake(task, task->tk_action);\n\t}\n\n\t \n\trpc_release_task(task);\nout:\n\tcurrent_restore_flags(pflags, PF_MEMALLOC);\n}\n\n \nvoid rpc_execute(struct rpc_task *task)\n{\n\tbool is_async = RPC_IS_ASYNC(task);\n\n\trpc_set_active(task);\n\trpc_make_runnable(rpciod_workqueue, task);\n\tif (!is_async) {\n\t\tunsigned int pflags = memalloc_nofs_save();\n\t\t__rpc_execute(task);\n\t\tmemalloc_nofs_restore(pflags);\n\t}\n}\n\nstatic void rpc_async_schedule(struct work_struct *work)\n{\n\tunsigned int pflags = memalloc_nofs_save();\n\n\t__rpc_execute(container_of(work, struct rpc_task, u.tk_work));\n\tmemalloc_nofs_restore(pflags);\n}\n\n \nint rpc_malloc(struct rpc_task *task)\n{\n\tstruct rpc_rqst *rqst = task->tk_rqstp;\n\tsize_t size = rqst->rq_callsize + rqst->rq_rcvsize;\n\tstruct rpc_buffer *buf;\n\tgfp_t gfp = rpc_task_gfp_mask();\n\n\tsize += sizeof(struct rpc_buffer);\n\tif (size <= RPC_BUFFER_MAXSIZE) {\n\t\tbuf = kmem_cache_alloc(rpc_buffer_slabp, gfp);\n\t\t \n\t\tif (!buf && RPC_IS_ASYNC(task))\n\t\t\tbuf = mempool_alloc(rpc_buffer_mempool, GFP_NOWAIT);\n\t} else\n\t\tbuf = kmalloc(size, gfp);\n\n\tif (!buf)\n\t\treturn -ENOMEM;\n\n\tbuf->len = size;\n\trqst->rq_buffer = buf->data;\n\trqst->rq_rbuffer = (char *)rqst->rq_buffer + rqst->rq_callsize;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(rpc_malloc);\n\n \nvoid rpc_free(struct rpc_task *task)\n{\n\tvoid *buffer = task->tk_rqstp->rq_buffer;\n\tsize_t size;\n\tstruct rpc_buffer *buf;\n\n\tbuf = container_of(buffer, struct rpc_buffer, data);\n\tsize = buf->len;\n\n\tif (size <= RPC_BUFFER_MAXSIZE)\n\t\tmempool_free(buf, rpc_buffer_mempool);\n\telse\n\t\tkfree(buf);\n}\nEXPORT_SYMBOL_GPL(rpc_free);\n\n \nstatic void rpc_init_task(struct rpc_task *task, const struct rpc_task_setup *task_setup_data)\n{\n\tmemset(task, 0, sizeof(*task));\n\tatomic_set(&task->tk_count, 1);\n\ttask->tk_flags  = task_setup_data->flags;\n\ttask->tk_ops = task_setup_data->callback_ops;\n\ttask->tk_calldata = task_setup_data->callback_data;\n\tINIT_LIST_HEAD(&task->tk_task);\n\n\ttask->tk_priority = task_setup_data->priority - RPC_PRIORITY_LOW;\n\ttask->tk_owner = current->tgid;\n\n\t \n\ttask->tk_workqueue = task_setup_data->workqueue;\n\n\ttask->tk_xprt = rpc_task_get_xprt(task_setup_data->rpc_client,\n\t\t\txprt_get(task_setup_data->rpc_xprt));\n\n\ttask->tk_op_cred = get_rpccred(task_setup_data->rpc_op_cred);\n\n\tif (task->tk_ops->rpc_call_prepare != NULL)\n\t\ttask->tk_action = rpc_prepare_task;\n\n\trpc_init_task_statistics(task);\n}\n\nstatic struct rpc_task *rpc_alloc_task(void)\n{\n\tstruct rpc_task *task;\n\n\ttask = kmem_cache_alloc(rpc_task_slabp, rpc_task_gfp_mask());\n\tif (task)\n\t\treturn task;\n\treturn mempool_alloc(rpc_task_mempool, GFP_NOWAIT);\n}\n\n \nstruct rpc_task *rpc_new_task(const struct rpc_task_setup *setup_data)\n{\n\tstruct rpc_task\t*task = setup_data->task;\n\tunsigned short flags = 0;\n\n\tif (task == NULL) {\n\t\ttask = rpc_alloc_task();\n\t\tif (task == NULL) {\n\t\t\trpc_release_calldata(setup_data->callback_ops,\n\t\t\t\t\t     setup_data->callback_data);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\tflags = RPC_TASK_DYNAMIC;\n\t}\n\n\trpc_init_task(task, setup_data);\n\ttask->tk_flags |= flags;\n\treturn task;\n}\n\n \nstatic void rpc_free_task(struct rpc_task *task)\n{\n\tunsigned short tk_flags = task->tk_flags;\n\n\tput_rpccred(task->tk_op_cred);\n\trpc_release_calldata(task->tk_ops, task->tk_calldata);\n\n\tif (tk_flags & RPC_TASK_DYNAMIC)\n\t\tmempool_free(task, rpc_task_mempool);\n}\n\nstatic void rpc_async_release(struct work_struct *work)\n{\n\tunsigned int pflags = memalloc_nofs_save();\n\n\trpc_free_task(container_of(work, struct rpc_task, u.tk_work));\n\tmemalloc_nofs_restore(pflags);\n}\n\nstatic void rpc_release_resources_task(struct rpc_task *task)\n{\n\txprt_release(task);\n\tif (task->tk_msg.rpc_cred) {\n\t\tif (!(task->tk_flags & RPC_TASK_CRED_NOREF))\n\t\t\tput_cred(task->tk_msg.rpc_cred);\n\t\ttask->tk_msg.rpc_cred = NULL;\n\t}\n\trpc_task_release_client(task);\n}\n\nstatic void rpc_final_put_task(struct rpc_task *task,\n\t\tstruct workqueue_struct *q)\n{\n\tif (q != NULL) {\n\t\tINIT_WORK(&task->u.tk_work, rpc_async_release);\n\t\tqueue_work(q, &task->u.tk_work);\n\t} else\n\t\trpc_free_task(task);\n}\n\nstatic void rpc_do_put_task(struct rpc_task *task, struct workqueue_struct *q)\n{\n\tif (atomic_dec_and_test(&task->tk_count)) {\n\t\trpc_release_resources_task(task);\n\t\trpc_final_put_task(task, q);\n\t}\n}\n\nvoid rpc_put_task(struct rpc_task *task)\n{\n\trpc_do_put_task(task, NULL);\n}\nEXPORT_SYMBOL_GPL(rpc_put_task);\n\nvoid rpc_put_task_async(struct rpc_task *task)\n{\n\trpc_do_put_task(task, task->tk_workqueue);\n}\nEXPORT_SYMBOL_GPL(rpc_put_task_async);\n\nstatic void rpc_release_task(struct rpc_task *task)\n{\n\tWARN_ON_ONCE(RPC_IS_QUEUED(task));\n\n\trpc_release_resources_task(task);\n\n\t \n\tif (atomic_read(&task->tk_count) != 1 + !RPC_IS_ASYNC(task)) {\n\t\t \n\t\tif (!rpc_complete_task(task))\n\t\t\treturn;\n\t} else {\n\t\tif (!atomic_dec_and_test(&task->tk_count))\n\t\t\treturn;\n\t}\n\trpc_final_put_task(task, task->tk_workqueue);\n}\n\nint rpciod_up(void)\n{\n\treturn try_module_get(THIS_MODULE) ? 0 : -EINVAL;\n}\n\nvoid rpciod_down(void)\n{\n\tmodule_put(THIS_MODULE);\n}\n\n \nstatic int rpciod_start(void)\n{\n\tstruct workqueue_struct *wq;\n\n\t \n\twq = alloc_workqueue(\"rpciod\", WQ_MEM_RECLAIM | WQ_UNBOUND, 0);\n\tif (!wq)\n\t\tgoto out_failed;\n\trpciod_workqueue = wq;\n\twq = alloc_workqueue(\"xprtiod\", WQ_UNBOUND | WQ_MEM_RECLAIM, 0);\n\tif (!wq)\n\t\tgoto free_rpciod;\n\txprtiod_workqueue = wq;\n\treturn 1;\nfree_rpciod:\n\twq = rpciod_workqueue;\n\trpciod_workqueue = NULL;\n\tdestroy_workqueue(wq);\nout_failed:\n\treturn 0;\n}\n\nstatic void rpciod_stop(void)\n{\n\tstruct workqueue_struct *wq = NULL;\n\n\tif (rpciod_workqueue == NULL)\n\t\treturn;\n\n\twq = rpciod_workqueue;\n\trpciod_workqueue = NULL;\n\tdestroy_workqueue(wq);\n\twq = xprtiod_workqueue;\n\txprtiod_workqueue = NULL;\n\tdestroy_workqueue(wq);\n}\n\nvoid\nrpc_destroy_mempool(void)\n{\n\trpciod_stop();\n\tmempool_destroy(rpc_buffer_mempool);\n\tmempool_destroy(rpc_task_mempool);\n\tkmem_cache_destroy(rpc_task_slabp);\n\tkmem_cache_destroy(rpc_buffer_slabp);\n\trpc_destroy_wait_queue(&delay_queue);\n}\n\nint\nrpc_init_mempool(void)\n{\n\t \n\trpc_init_wait_queue(&delay_queue, \"delayq\");\n\tif (!rpciod_start())\n\t\tgoto err_nomem;\n\n\trpc_task_slabp = kmem_cache_create(\"rpc_tasks\",\n\t\t\t\t\t     sizeof(struct rpc_task),\n\t\t\t\t\t     0, SLAB_HWCACHE_ALIGN,\n\t\t\t\t\t     NULL);\n\tif (!rpc_task_slabp)\n\t\tgoto err_nomem;\n\trpc_buffer_slabp = kmem_cache_create(\"rpc_buffers\",\n\t\t\t\t\t     RPC_BUFFER_MAXSIZE,\n\t\t\t\t\t     0, SLAB_HWCACHE_ALIGN,\n\t\t\t\t\t     NULL);\n\tif (!rpc_buffer_slabp)\n\t\tgoto err_nomem;\n\trpc_task_mempool = mempool_create_slab_pool(RPC_TASK_POOLSIZE,\n\t\t\t\t\t\t    rpc_task_slabp);\n\tif (!rpc_task_mempool)\n\t\tgoto err_nomem;\n\trpc_buffer_mempool = mempool_create_slab_pool(RPC_BUFFER_POOLSIZE,\n\t\t\t\t\t\t      rpc_buffer_slabp);\n\tif (!rpc_buffer_mempool)\n\t\tgoto err_nomem;\n\treturn 0;\nerr_nomem:\n\trpc_destroy_mempool();\n\treturn -ENOMEM;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}