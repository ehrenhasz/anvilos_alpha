{
  "module_name": "rpc_rdma.c",
  "hash_id": "ffed490679bfdf25307cc8fa3fc4675f61204d233020bcf871257e4591f39df8",
  "original_prompt": "Ingested from linux-6.6.14/net/sunrpc/xprtrdma/rpc_rdma.c",
  "human_readable_source": "\n \n\n \n\n#include <linux/highmem.h>\n\n#include <linux/sunrpc/svc_rdma.h>\n\n#include \"xprt_rdma.h\"\n#include <trace/events/rpcrdma.h>\n\n \nstatic unsigned int rpcrdma_max_call_header_size(unsigned int maxsegs)\n{\n\tunsigned int size;\n\n\t \n\tsize = RPCRDMA_HDRLEN_MIN;\n\n\t \n\tsize += maxsegs * rpcrdma_readchunk_maxsz * sizeof(__be32);\n\n\t \n\tsize += sizeof(__be32);\t \n\tsize += rpcrdma_segment_maxsz * sizeof(__be32);\n\tsize += sizeof(__be32);\t \n\n\treturn size;\n}\n\n \nstatic unsigned int rpcrdma_max_reply_header_size(unsigned int maxsegs)\n{\n\tunsigned int size;\n\n\t \n\tsize = RPCRDMA_HDRLEN_MIN;\n\n\t \n\tsize += sizeof(__be32);\t\t \n\tsize += maxsegs * rpcrdma_segment_maxsz * sizeof(__be32);\n\tsize += sizeof(__be32);\t \n\n\treturn size;\n}\n\n \nvoid rpcrdma_set_max_header_sizes(struct rpcrdma_ep *ep)\n{\n\tunsigned int maxsegs = ep->re_max_rdma_segs;\n\n\tep->re_max_inline_send =\n\t\tep->re_inline_send - rpcrdma_max_call_header_size(maxsegs);\n\tep->re_max_inline_recv =\n\t\tep->re_inline_recv - rpcrdma_max_reply_header_size(maxsegs);\n}\n\n \nstatic bool rpcrdma_args_inline(struct rpcrdma_xprt *r_xprt,\n\t\t\t\tstruct rpc_rqst *rqst)\n{\n\tstruct xdr_buf *xdr = &rqst->rq_snd_buf;\n\tstruct rpcrdma_ep *ep = r_xprt->rx_ep;\n\tunsigned int count, remaining, offset;\n\n\tif (xdr->len > ep->re_max_inline_send)\n\t\treturn false;\n\n\tif (xdr->page_len) {\n\t\tremaining = xdr->page_len;\n\t\toffset = offset_in_page(xdr->page_base);\n\t\tcount = RPCRDMA_MIN_SEND_SGES;\n\t\twhile (remaining) {\n\t\t\tremaining -= min_t(unsigned int,\n\t\t\t\t\t   PAGE_SIZE - offset, remaining);\n\t\t\toffset = 0;\n\t\t\tif (++count > ep->re_attr.cap.max_send_sge)\n\t\t\t\treturn false;\n\t\t}\n\t}\n\n\treturn true;\n}\n\n \nstatic bool rpcrdma_results_inline(struct rpcrdma_xprt *r_xprt,\n\t\t\t\t   struct rpc_rqst *rqst)\n{\n\treturn rqst->rq_rcv_buf.buflen <= r_xprt->rx_ep->re_max_inline_recv;\n}\n\n \nstatic bool\nrpcrdma_nonpayload_inline(const struct rpcrdma_xprt *r_xprt,\n\t\t\t  const struct rpc_rqst *rqst)\n{\n\tconst struct xdr_buf *buf = &rqst->rq_rcv_buf;\n\n\treturn (buf->head[0].iov_len + buf->tail[0].iov_len) <\n\t\tr_xprt->rx_ep->re_max_inline_recv;\n}\n\n \nstatic noinline int\nrpcrdma_alloc_sparse_pages(struct xdr_buf *buf)\n{\n\tstruct page **ppages;\n\tint len;\n\n\tlen = buf->page_len;\n\tppages = buf->pages + (buf->page_base >> PAGE_SHIFT);\n\twhile (len > 0) {\n\t\tif (!*ppages)\n\t\t\t*ppages = alloc_page(GFP_NOWAIT | __GFP_NOWARN);\n\t\tif (!*ppages)\n\t\t\treturn -ENOBUFS;\n\t\tppages++;\n\t\tlen -= PAGE_SIZE;\n\t}\n\n\treturn 0;\n}\n\n \nstatic struct rpcrdma_mr_seg *\nrpcrdma_convert_kvec(struct kvec *vec, struct rpcrdma_mr_seg *seg,\n\t\t     unsigned int *n)\n{\n\tseg->mr_page = virt_to_page(vec->iov_base);\n\tseg->mr_offset = offset_in_page(vec->iov_base);\n\tseg->mr_len = vec->iov_len;\n\t++seg;\n\t++(*n);\n\treturn seg;\n}\n\n \n\nstatic int\nrpcrdma_convert_iovs(struct rpcrdma_xprt *r_xprt, struct xdr_buf *xdrbuf,\n\t\t     unsigned int pos, enum rpcrdma_chunktype type,\n\t\t     struct rpcrdma_mr_seg *seg)\n{\n\tunsigned long page_base;\n\tunsigned int len, n;\n\tstruct page **ppages;\n\n\tn = 0;\n\tif (pos == 0)\n\t\tseg = rpcrdma_convert_kvec(&xdrbuf->head[0], seg, &n);\n\n\tlen = xdrbuf->page_len;\n\tppages = xdrbuf->pages + (xdrbuf->page_base >> PAGE_SHIFT);\n\tpage_base = offset_in_page(xdrbuf->page_base);\n\twhile (len) {\n\t\tseg->mr_page = *ppages;\n\t\tseg->mr_offset = page_base;\n\t\tseg->mr_len = min_t(u32, PAGE_SIZE - page_base, len);\n\t\tlen -= seg->mr_len;\n\t\t++ppages;\n\t\t++seg;\n\t\t++n;\n\t\tpage_base = 0;\n\t}\n\n\tif (type == rpcrdma_readch || type == rpcrdma_writech)\n\t\tgoto out;\n\n\tif (xdrbuf->tail[0].iov_len)\n\t\trpcrdma_convert_kvec(&xdrbuf->tail[0], seg, &n);\n\nout:\n\tif (unlikely(n > RPCRDMA_MAX_SEGS))\n\t\treturn -EIO;\n\treturn n;\n}\n\nstatic int\nencode_rdma_segment(struct xdr_stream *xdr, struct rpcrdma_mr *mr)\n{\n\t__be32 *p;\n\n\tp = xdr_reserve_space(xdr, 4 * sizeof(*p));\n\tif (unlikely(!p))\n\t\treturn -EMSGSIZE;\n\n\txdr_encode_rdma_segment(p, mr->mr_handle, mr->mr_length, mr->mr_offset);\n\treturn 0;\n}\n\nstatic int\nencode_read_segment(struct xdr_stream *xdr, struct rpcrdma_mr *mr,\n\t\t    u32 position)\n{\n\t__be32 *p;\n\n\tp = xdr_reserve_space(xdr, 6 * sizeof(*p));\n\tif (unlikely(!p))\n\t\treturn -EMSGSIZE;\n\n\t*p++ = xdr_one;\t\t\t \n\txdr_encode_read_segment(p, position, mr->mr_handle, mr->mr_length,\n\t\t\t\tmr->mr_offset);\n\treturn 0;\n}\n\nstatic struct rpcrdma_mr_seg *rpcrdma_mr_prepare(struct rpcrdma_xprt *r_xprt,\n\t\t\t\t\t\t struct rpcrdma_req *req,\n\t\t\t\t\t\t struct rpcrdma_mr_seg *seg,\n\t\t\t\t\t\t int nsegs, bool writing,\n\t\t\t\t\t\t struct rpcrdma_mr **mr)\n{\n\t*mr = rpcrdma_mr_pop(&req->rl_free_mrs);\n\tif (!*mr) {\n\t\t*mr = rpcrdma_mr_get(r_xprt);\n\t\tif (!*mr)\n\t\t\tgoto out_getmr_err;\n\t\t(*mr)->mr_req = req;\n\t}\n\n\trpcrdma_mr_push(*mr, &req->rl_registered);\n\treturn frwr_map(r_xprt, seg, nsegs, writing, req->rl_slot.rq_xid, *mr);\n\nout_getmr_err:\n\ttrace_xprtrdma_nomrs_err(r_xprt, req);\n\txprt_wait_for_buffer_space(&r_xprt->rx_xprt);\n\trpcrdma_mrs_refresh(r_xprt);\n\treturn ERR_PTR(-EAGAIN);\n}\n\n \nstatic int rpcrdma_encode_read_list(struct rpcrdma_xprt *r_xprt,\n\t\t\t\t    struct rpcrdma_req *req,\n\t\t\t\t    struct rpc_rqst *rqst,\n\t\t\t\t    enum rpcrdma_chunktype rtype)\n{\n\tstruct xdr_stream *xdr = &req->rl_stream;\n\tstruct rpcrdma_mr_seg *seg;\n\tstruct rpcrdma_mr *mr;\n\tunsigned int pos;\n\tint nsegs;\n\n\tif (rtype == rpcrdma_noch_pullup || rtype == rpcrdma_noch_mapped)\n\t\tgoto done;\n\n\tpos = rqst->rq_snd_buf.head[0].iov_len;\n\tif (rtype == rpcrdma_areadch)\n\t\tpos = 0;\n\tseg = req->rl_segments;\n\tnsegs = rpcrdma_convert_iovs(r_xprt, &rqst->rq_snd_buf, pos,\n\t\t\t\t     rtype, seg);\n\tif (nsegs < 0)\n\t\treturn nsegs;\n\n\tdo {\n\t\tseg = rpcrdma_mr_prepare(r_xprt, req, seg, nsegs, false, &mr);\n\t\tif (IS_ERR(seg))\n\t\t\treturn PTR_ERR(seg);\n\n\t\tif (encode_read_segment(xdr, mr, pos) < 0)\n\t\t\treturn -EMSGSIZE;\n\n\t\ttrace_xprtrdma_chunk_read(rqst->rq_task, pos, mr, nsegs);\n\t\tr_xprt->rx_stats.read_chunk_count++;\n\t\tnsegs -= mr->mr_nents;\n\t} while (nsegs);\n\ndone:\n\tif (xdr_stream_encode_item_absent(xdr) < 0)\n\t\treturn -EMSGSIZE;\n\treturn 0;\n}\n\n \nstatic int rpcrdma_encode_write_list(struct rpcrdma_xprt *r_xprt,\n\t\t\t\t     struct rpcrdma_req *req,\n\t\t\t\t     struct rpc_rqst *rqst,\n\t\t\t\t     enum rpcrdma_chunktype wtype)\n{\n\tstruct xdr_stream *xdr = &req->rl_stream;\n\tstruct rpcrdma_ep *ep = r_xprt->rx_ep;\n\tstruct rpcrdma_mr_seg *seg;\n\tstruct rpcrdma_mr *mr;\n\tint nsegs, nchunks;\n\t__be32 *segcount;\n\n\tif (wtype != rpcrdma_writech)\n\t\tgoto done;\n\n\tseg = req->rl_segments;\n\tnsegs = rpcrdma_convert_iovs(r_xprt, &rqst->rq_rcv_buf,\n\t\t\t\t     rqst->rq_rcv_buf.head[0].iov_len,\n\t\t\t\t     wtype, seg);\n\tif (nsegs < 0)\n\t\treturn nsegs;\n\n\tif (xdr_stream_encode_item_present(xdr) < 0)\n\t\treturn -EMSGSIZE;\n\tsegcount = xdr_reserve_space(xdr, sizeof(*segcount));\n\tif (unlikely(!segcount))\n\t\treturn -EMSGSIZE;\n\t \n\n\tnchunks = 0;\n\tdo {\n\t\tseg = rpcrdma_mr_prepare(r_xprt, req, seg, nsegs, true, &mr);\n\t\tif (IS_ERR(seg))\n\t\t\treturn PTR_ERR(seg);\n\n\t\tif (encode_rdma_segment(xdr, mr) < 0)\n\t\t\treturn -EMSGSIZE;\n\n\t\ttrace_xprtrdma_chunk_write(rqst->rq_task, mr, nsegs);\n\t\tr_xprt->rx_stats.write_chunk_count++;\n\t\tr_xprt->rx_stats.total_rdma_request += mr->mr_length;\n\t\tnchunks++;\n\t\tnsegs -= mr->mr_nents;\n\t} while (nsegs);\n\n\tif (xdr_pad_size(rqst->rq_rcv_buf.page_len)) {\n\t\tif (encode_rdma_segment(xdr, ep->re_write_pad_mr) < 0)\n\t\t\treturn -EMSGSIZE;\n\n\t\ttrace_xprtrdma_chunk_wp(rqst->rq_task, ep->re_write_pad_mr,\n\t\t\t\t\tnsegs);\n\t\tr_xprt->rx_stats.write_chunk_count++;\n\t\tr_xprt->rx_stats.total_rdma_request += mr->mr_length;\n\t\tnchunks++;\n\t\tnsegs -= mr->mr_nents;\n\t}\n\n\t \n\t*segcount = cpu_to_be32(nchunks);\n\ndone:\n\tif (xdr_stream_encode_item_absent(xdr) < 0)\n\t\treturn -EMSGSIZE;\n\treturn 0;\n}\n\n \nstatic int rpcrdma_encode_reply_chunk(struct rpcrdma_xprt *r_xprt,\n\t\t\t\t      struct rpcrdma_req *req,\n\t\t\t\t      struct rpc_rqst *rqst,\n\t\t\t\t      enum rpcrdma_chunktype wtype)\n{\n\tstruct xdr_stream *xdr = &req->rl_stream;\n\tstruct rpcrdma_mr_seg *seg;\n\tstruct rpcrdma_mr *mr;\n\tint nsegs, nchunks;\n\t__be32 *segcount;\n\n\tif (wtype != rpcrdma_replych) {\n\t\tif (xdr_stream_encode_item_absent(xdr) < 0)\n\t\t\treturn -EMSGSIZE;\n\t\treturn 0;\n\t}\n\n\tseg = req->rl_segments;\n\tnsegs = rpcrdma_convert_iovs(r_xprt, &rqst->rq_rcv_buf, 0, wtype, seg);\n\tif (nsegs < 0)\n\t\treturn nsegs;\n\n\tif (xdr_stream_encode_item_present(xdr) < 0)\n\t\treturn -EMSGSIZE;\n\tsegcount = xdr_reserve_space(xdr, sizeof(*segcount));\n\tif (unlikely(!segcount))\n\t\treturn -EMSGSIZE;\n\t \n\n\tnchunks = 0;\n\tdo {\n\t\tseg = rpcrdma_mr_prepare(r_xprt, req, seg, nsegs, true, &mr);\n\t\tif (IS_ERR(seg))\n\t\t\treturn PTR_ERR(seg);\n\n\t\tif (encode_rdma_segment(xdr, mr) < 0)\n\t\t\treturn -EMSGSIZE;\n\n\t\ttrace_xprtrdma_chunk_reply(rqst->rq_task, mr, nsegs);\n\t\tr_xprt->rx_stats.reply_chunk_count++;\n\t\tr_xprt->rx_stats.total_rdma_request += mr->mr_length;\n\t\tnchunks++;\n\t\tnsegs -= mr->mr_nents;\n\t} while (nsegs);\n\n\t \n\t*segcount = cpu_to_be32(nchunks);\n\n\treturn 0;\n}\n\nstatic void rpcrdma_sendctx_done(struct kref *kref)\n{\n\tstruct rpcrdma_req *req =\n\t\tcontainer_of(kref, struct rpcrdma_req, rl_kref);\n\tstruct rpcrdma_rep *rep = req->rl_reply;\n\n\trpcrdma_complete_rqst(rep);\n\trep->rr_rxprt->rx_stats.reply_waits_for_send++;\n}\n\n \nvoid rpcrdma_sendctx_unmap(struct rpcrdma_sendctx *sc)\n{\n\tstruct rpcrdma_regbuf *rb = sc->sc_req->rl_sendbuf;\n\tstruct ib_sge *sge;\n\n\tif (!sc->sc_unmap_count)\n\t\treturn;\n\n\t \n\tfor (sge = &sc->sc_sges[2]; sc->sc_unmap_count;\n\t     ++sge, --sc->sc_unmap_count)\n\t\tib_dma_unmap_page(rdmab_device(rb), sge->addr, sge->length,\n\t\t\t\t  DMA_TO_DEVICE);\n\n\tkref_put(&sc->sc_req->rl_kref, rpcrdma_sendctx_done);\n}\n\n \nstatic void rpcrdma_prepare_hdr_sge(struct rpcrdma_xprt *r_xprt,\n\t\t\t\t    struct rpcrdma_req *req, u32 len)\n{\n\tstruct rpcrdma_sendctx *sc = req->rl_sendctx;\n\tstruct rpcrdma_regbuf *rb = req->rl_rdmabuf;\n\tstruct ib_sge *sge = &sc->sc_sges[req->rl_wr.num_sge++];\n\n\tsge->addr = rdmab_addr(rb);\n\tsge->length = len;\n\tsge->lkey = rdmab_lkey(rb);\n\n\tib_dma_sync_single_for_device(rdmab_device(rb), sge->addr, sge->length,\n\t\t\t\t      DMA_TO_DEVICE);\n}\n\n \nstatic bool rpcrdma_prepare_head_iov(struct rpcrdma_xprt *r_xprt,\n\t\t\t\t     struct rpcrdma_req *req, unsigned int len)\n{\n\tstruct rpcrdma_sendctx *sc = req->rl_sendctx;\n\tstruct ib_sge *sge = &sc->sc_sges[req->rl_wr.num_sge++];\n\tstruct rpcrdma_regbuf *rb = req->rl_sendbuf;\n\n\tif (!rpcrdma_regbuf_dma_map(r_xprt, rb))\n\t\treturn false;\n\n\tsge->addr = rdmab_addr(rb);\n\tsge->length = len;\n\tsge->lkey = rdmab_lkey(rb);\n\n\tib_dma_sync_single_for_device(rdmab_device(rb), sge->addr, sge->length,\n\t\t\t\t      DMA_TO_DEVICE);\n\treturn true;\n}\n\n \nstatic bool rpcrdma_prepare_pagelist(struct rpcrdma_req *req,\n\t\t\t\t     struct xdr_buf *xdr)\n{\n\tstruct rpcrdma_sendctx *sc = req->rl_sendctx;\n\tstruct rpcrdma_regbuf *rb = req->rl_sendbuf;\n\tunsigned int page_base, len, remaining;\n\tstruct page **ppages;\n\tstruct ib_sge *sge;\n\n\tppages = xdr->pages + (xdr->page_base >> PAGE_SHIFT);\n\tpage_base = offset_in_page(xdr->page_base);\n\tremaining = xdr->page_len;\n\twhile (remaining) {\n\t\tsge = &sc->sc_sges[req->rl_wr.num_sge++];\n\t\tlen = min_t(unsigned int, PAGE_SIZE - page_base, remaining);\n\t\tsge->addr = ib_dma_map_page(rdmab_device(rb), *ppages,\n\t\t\t\t\t    page_base, len, DMA_TO_DEVICE);\n\t\tif (ib_dma_mapping_error(rdmab_device(rb), sge->addr))\n\t\t\tgoto out_mapping_err;\n\n\t\tsge->length = len;\n\t\tsge->lkey = rdmab_lkey(rb);\n\n\t\tsc->sc_unmap_count++;\n\t\tppages++;\n\t\tremaining -= len;\n\t\tpage_base = 0;\n\t}\n\n\treturn true;\n\nout_mapping_err:\n\ttrace_xprtrdma_dma_maperr(sge->addr);\n\treturn false;\n}\n\n \nstatic bool rpcrdma_prepare_tail_iov(struct rpcrdma_req *req,\n\t\t\t\t     struct xdr_buf *xdr,\n\t\t\t\t     unsigned int page_base, unsigned int len)\n{\n\tstruct rpcrdma_sendctx *sc = req->rl_sendctx;\n\tstruct ib_sge *sge = &sc->sc_sges[req->rl_wr.num_sge++];\n\tstruct rpcrdma_regbuf *rb = req->rl_sendbuf;\n\tstruct page *page = virt_to_page(xdr->tail[0].iov_base);\n\n\tsge->addr = ib_dma_map_page(rdmab_device(rb), page, page_base, len,\n\t\t\t\t    DMA_TO_DEVICE);\n\tif (ib_dma_mapping_error(rdmab_device(rb), sge->addr))\n\t\tgoto out_mapping_err;\n\n\tsge->length = len;\n\tsge->lkey = rdmab_lkey(rb);\n\t++sc->sc_unmap_count;\n\treturn true;\n\nout_mapping_err:\n\ttrace_xprtrdma_dma_maperr(sge->addr);\n\treturn false;\n}\n\n \nstatic void rpcrdma_pullup_tail_iov(struct rpcrdma_xprt *r_xprt,\n\t\t\t\t    struct rpcrdma_req *req,\n\t\t\t\t    struct xdr_buf *xdr)\n{\n\tunsigned char *dst;\n\n\tdst = (unsigned char *)xdr->head[0].iov_base;\n\tdst += xdr->head[0].iov_len + xdr->page_len;\n\tmemmove(dst, xdr->tail[0].iov_base, xdr->tail[0].iov_len);\n\tr_xprt->rx_stats.pullup_copy_count += xdr->tail[0].iov_len;\n}\n\n \nstatic void rpcrdma_pullup_pagelist(struct rpcrdma_xprt *r_xprt,\n\t\t\t\t    struct rpcrdma_req *req,\n\t\t\t\t    struct xdr_buf *xdr)\n{\n\tunsigned int len, page_base, remaining;\n\tstruct page **ppages;\n\tunsigned char *src, *dst;\n\n\tdst = (unsigned char *)xdr->head[0].iov_base;\n\tdst += xdr->head[0].iov_len;\n\tppages = xdr->pages + (xdr->page_base >> PAGE_SHIFT);\n\tpage_base = offset_in_page(xdr->page_base);\n\tremaining = xdr->page_len;\n\twhile (remaining) {\n\t\tsrc = page_address(*ppages);\n\t\tsrc += page_base;\n\t\tlen = min_t(unsigned int, PAGE_SIZE - page_base, remaining);\n\t\tmemcpy(dst, src, len);\n\t\tr_xprt->rx_stats.pullup_copy_count += len;\n\n\t\tppages++;\n\t\tdst += len;\n\t\tremaining -= len;\n\t\tpage_base = 0;\n\t}\n}\n\n \nstatic bool rpcrdma_prepare_noch_pullup(struct rpcrdma_xprt *r_xprt,\n\t\t\t\t\tstruct rpcrdma_req *req,\n\t\t\t\t\tstruct xdr_buf *xdr)\n{\n\tif (unlikely(xdr->tail[0].iov_len))\n\t\trpcrdma_pullup_tail_iov(r_xprt, req, xdr);\n\n\tif (unlikely(xdr->page_len))\n\t\trpcrdma_pullup_pagelist(r_xprt, req, xdr);\n\n\t \n\treturn rpcrdma_prepare_head_iov(r_xprt, req, xdr->len);\n}\n\nstatic bool rpcrdma_prepare_noch_mapped(struct rpcrdma_xprt *r_xprt,\n\t\t\t\t\tstruct rpcrdma_req *req,\n\t\t\t\t\tstruct xdr_buf *xdr)\n{\n\tstruct kvec *tail = &xdr->tail[0];\n\n\tif (!rpcrdma_prepare_head_iov(r_xprt, req, xdr->head[0].iov_len))\n\t\treturn false;\n\tif (xdr->page_len)\n\t\tif (!rpcrdma_prepare_pagelist(req, xdr))\n\t\t\treturn false;\n\tif (tail->iov_len)\n\t\tif (!rpcrdma_prepare_tail_iov(req, xdr,\n\t\t\t\t\t      offset_in_page(tail->iov_base),\n\t\t\t\t\t      tail->iov_len))\n\t\t\treturn false;\n\n\tif (req->rl_sendctx->sc_unmap_count)\n\t\tkref_get(&req->rl_kref);\n\treturn true;\n}\n\nstatic bool rpcrdma_prepare_readch(struct rpcrdma_xprt *r_xprt,\n\t\t\t\t   struct rpcrdma_req *req,\n\t\t\t\t   struct xdr_buf *xdr)\n{\n\tif (!rpcrdma_prepare_head_iov(r_xprt, req, xdr->head[0].iov_len))\n\t\treturn false;\n\n\t \n\n\t \n\tif (xdr->tail[0].iov_len > 3) {\n\t\tunsigned int page_base, len;\n\n\t\t \n\t\tpage_base = offset_in_page(xdr->tail[0].iov_base);\n\t\tlen = xdr->tail[0].iov_len;\n\t\tpage_base += len & 3;\n\t\tlen -= len & 3;\n\t\tif (!rpcrdma_prepare_tail_iov(req, xdr, page_base, len))\n\t\t\treturn false;\n\t\tkref_get(&req->rl_kref);\n\t}\n\n\treturn true;\n}\n\n \ninline int rpcrdma_prepare_send_sges(struct rpcrdma_xprt *r_xprt,\n\t\t\t\t     struct rpcrdma_req *req, u32 hdrlen,\n\t\t\t\t     struct xdr_buf *xdr,\n\t\t\t\t     enum rpcrdma_chunktype rtype)\n{\n\tint ret;\n\n\tret = -EAGAIN;\n\treq->rl_sendctx = rpcrdma_sendctx_get_locked(r_xprt);\n\tif (!req->rl_sendctx)\n\t\tgoto out_nosc;\n\treq->rl_sendctx->sc_unmap_count = 0;\n\treq->rl_sendctx->sc_req = req;\n\tkref_init(&req->rl_kref);\n\treq->rl_wr.wr_cqe = &req->rl_sendctx->sc_cqe;\n\treq->rl_wr.sg_list = req->rl_sendctx->sc_sges;\n\treq->rl_wr.num_sge = 0;\n\treq->rl_wr.opcode = IB_WR_SEND;\n\n\trpcrdma_prepare_hdr_sge(r_xprt, req, hdrlen);\n\n\tret = -EIO;\n\tswitch (rtype) {\n\tcase rpcrdma_noch_pullup:\n\t\tif (!rpcrdma_prepare_noch_pullup(r_xprt, req, xdr))\n\t\t\tgoto out_unmap;\n\t\tbreak;\n\tcase rpcrdma_noch_mapped:\n\t\tif (!rpcrdma_prepare_noch_mapped(r_xprt, req, xdr))\n\t\t\tgoto out_unmap;\n\t\tbreak;\n\tcase rpcrdma_readch:\n\t\tif (!rpcrdma_prepare_readch(r_xprt, req, xdr))\n\t\t\tgoto out_unmap;\n\t\tbreak;\n\tcase rpcrdma_areadch:\n\t\tbreak;\n\tdefault:\n\t\tgoto out_unmap;\n\t}\n\n\treturn 0;\n\nout_unmap:\n\trpcrdma_sendctx_unmap(req->rl_sendctx);\nout_nosc:\n\ttrace_xprtrdma_prepsend_failed(&req->rl_slot, ret);\n\treturn ret;\n}\n\n \nint\nrpcrdma_marshal_req(struct rpcrdma_xprt *r_xprt, struct rpc_rqst *rqst)\n{\n\tstruct rpcrdma_req *req = rpcr_to_rdmar(rqst);\n\tstruct xdr_stream *xdr = &req->rl_stream;\n\tenum rpcrdma_chunktype rtype, wtype;\n\tstruct xdr_buf *buf = &rqst->rq_snd_buf;\n\tbool ddp_allowed;\n\t__be32 *p;\n\tint ret;\n\n\tif (unlikely(rqst->rq_rcv_buf.flags & XDRBUF_SPARSE_PAGES)) {\n\t\tret = rpcrdma_alloc_sparse_pages(&rqst->rq_rcv_buf);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\trpcrdma_set_xdrlen(&req->rl_hdrbuf, 0);\n\txdr_init_encode(xdr, &req->rl_hdrbuf, rdmab_data(req->rl_rdmabuf),\n\t\t\trqst);\n\n\t \n\tret = -EMSGSIZE;\n\tp = xdr_reserve_space(xdr, 4 * sizeof(*p));\n\tif (!p)\n\t\tgoto out_err;\n\t*p++ = rqst->rq_xid;\n\t*p++ = rpcrdma_version;\n\t*p++ = r_xprt->rx_buf.rb_max_requests;\n\n\t \n\tddp_allowed = !test_bit(RPCAUTH_AUTH_DATATOUCH,\n\t\t\t\t&rqst->rq_cred->cr_auth->au_flags);\n\n\t \n\tif (rpcrdma_results_inline(r_xprt, rqst))\n\t\twtype = rpcrdma_noch;\n\telse if ((ddp_allowed && rqst->rq_rcv_buf.flags & XDRBUF_READ) &&\n\t\t rpcrdma_nonpayload_inline(r_xprt, rqst))\n\t\twtype = rpcrdma_writech;\n\telse\n\t\twtype = rpcrdma_replych;\n\n\t \n\tif (rpcrdma_args_inline(r_xprt, rqst)) {\n\t\t*p++ = rdma_msg;\n\t\trtype = buf->len < rdmab_length(req->rl_sendbuf) ?\n\t\t\trpcrdma_noch_pullup : rpcrdma_noch_mapped;\n\t} else if (ddp_allowed && buf->flags & XDRBUF_WRITE) {\n\t\t*p++ = rdma_msg;\n\t\trtype = rpcrdma_readch;\n\t} else {\n\t\tr_xprt->rx_stats.nomsg_call_count++;\n\t\t*p++ = rdma_nomsg;\n\t\trtype = rpcrdma_areadch;\n\t}\n\n\t \n\tret = rpcrdma_encode_read_list(r_xprt, req, rqst, rtype);\n\tif (ret)\n\t\tgoto out_err;\n\tret = rpcrdma_encode_write_list(r_xprt, req, rqst, wtype);\n\tif (ret)\n\t\tgoto out_err;\n\tret = rpcrdma_encode_reply_chunk(r_xprt, req, rqst, wtype);\n\tif (ret)\n\t\tgoto out_err;\n\n\tret = rpcrdma_prepare_send_sges(r_xprt, req, req->rl_hdrbuf.len,\n\t\t\t\t\tbuf, rtype);\n\tif (ret)\n\t\tgoto out_err;\n\n\ttrace_xprtrdma_marshal(req, rtype, wtype);\n\treturn 0;\n\nout_err:\n\ttrace_xprtrdma_marshal_failed(rqst, ret);\n\tr_xprt->rx_stats.failed_marshal_count++;\n\tfrwr_reset(req);\n\treturn ret;\n}\n\nstatic void __rpcrdma_update_cwnd_locked(struct rpc_xprt *xprt,\n\t\t\t\t\t struct rpcrdma_buffer *buf,\n\t\t\t\t\t u32 grant)\n{\n\tbuf->rb_credits = grant;\n\txprt->cwnd = grant << RPC_CWNDSHIFT;\n}\n\nstatic void rpcrdma_update_cwnd(struct rpcrdma_xprt *r_xprt, u32 grant)\n{\n\tstruct rpc_xprt *xprt = &r_xprt->rx_xprt;\n\n\tspin_lock(&xprt->transport_lock);\n\t__rpcrdma_update_cwnd_locked(xprt, &r_xprt->rx_buf, grant);\n\tspin_unlock(&xprt->transport_lock);\n}\n\n \nvoid rpcrdma_reset_cwnd(struct rpcrdma_xprt *r_xprt)\n{\n\tstruct rpc_xprt *xprt = &r_xprt->rx_xprt;\n\n\tspin_lock(&xprt->transport_lock);\n\txprt->cong = 0;\n\t__rpcrdma_update_cwnd_locked(xprt, &r_xprt->rx_buf, 1);\n\tspin_unlock(&xprt->transport_lock);\n}\n\n \nstatic unsigned long\nrpcrdma_inline_fixup(struct rpc_rqst *rqst, char *srcp, int copy_len, int pad)\n{\n\tunsigned long fixup_copy_count;\n\tint i, npages, curlen;\n\tchar *destp;\n\tstruct page **ppages;\n\tint page_base;\n\n\t \n\trqst->rq_rcv_buf.head[0].iov_base = srcp;\n\trqst->rq_private_buf.head[0].iov_base = srcp;\n\n\t \n\tcurlen = rqst->rq_rcv_buf.head[0].iov_len;\n\tif (curlen > copy_len)\n\t\tcurlen = copy_len;\n\tsrcp += curlen;\n\tcopy_len -= curlen;\n\n\tppages = rqst->rq_rcv_buf.pages +\n\t\t(rqst->rq_rcv_buf.page_base >> PAGE_SHIFT);\n\tpage_base = offset_in_page(rqst->rq_rcv_buf.page_base);\n\tfixup_copy_count = 0;\n\tif (copy_len && rqst->rq_rcv_buf.page_len) {\n\t\tint pagelist_len;\n\n\t\tpagelist_len = rqst->rq_rcv_buf.page_len;\n\t\tif (pagelist_len > copy_len)\n\t\t\tpagelist_len = copy_len;\n\t\tnpages = PAGE_ALIGN(page_base + pagelist_len) >> PAGE_SHIFT;\n\t\tfor (i = 0; i < npages; i++) {\n\t\t\tcurlen = PAGE_SIZE - page_base;\n\t\t\tif (curlen > pagelist_len)\n\t\t\t\tcurlen = pagelist_len;\n\n\t\t\tdestp = kmap_atomic(ppages[i]);\n\t\t\tmemcpy(destp + page_base, srcp, curlen);\n\t\t\tflush_dcache_page(ppages[i]);\n\t\t\tkunmap_atomic(destp);\n\t\t\tsrcp += curlen;\n\t\t\tcopy_len -= curlen;\n\t\t\tfixup_copy_count += curlen;\n\t\t\tpagelist_len -= curlen;\n\t\t\tif (!pagelist_len)\n\t\t\t\tbreak;\n\t\t\tpage_base = 0;\n\t\t}\n\n\t\t \n\t\tif (pad)\n\t\t\tsrcp -= pad;\n\t}\n\n\t \n\tif (copy_len || pad) {\n\t\trqst->rq_rcv_buf.tail[0].iov_base = srcp;\n\t\trqst->rq_private_buf.tail[0].iov_base = srcp;\n\t}\n\n\tif (fixup_copy_count)\n\t\ttrace_xprtrdma_fixup(rqst, fixup_copy_count);\n\treturn fixup_copy_count;\n}\n\n \nstatic bool\nrpcrdma_is_bcall(struct rpcrdma_xprt *r_xprt, struct rpcrdma_rep *rep)\n#if defined(CONFIG_SUNRPC_BACKCHANNEL)\n{\n\tstruct rpc_xprt *xprt = &r_xprt->rx_xprt;\n\tstruct xdr_stream *xdr = &rep->rr_stream;\n\t__be32 *p;\n\n\tif (rep->rr_proc != rdma_msg)\n\t\treturn false;\n\n\t \n\tp = xdr_inline_decode(xdr, 0);\n\n\t \n\tif (xdr_item_is_present(p++))\n\t\treturn false;\n\tif (xdr_item_is_present(p++))\n\t\treturn false;\n\tif (xdr_item_is_present(p++))\n\t\treturn false;\n\n\t \n\tif (*p++ != rep->rr_xid)\n\t\treturn false;\n\tif (*p != cpu_to_be32(RPC_CALL))\n\t\treturn false;\n\n\t \n\tif (xprt->bc_serv == NULL)\n\t\treturn false;\n\n\t \n\tp = xdr_inline_decode(xdr, 3 * sizeof(*p));\n\tif (unlikely(!p))\n\t\treturn true;\n\n\trpcrdma_bc_receive_call(r_xprt, rep);\n\treturn true;\n}\n#else\t \n{\n\treturn false;\n}\n#endif\t \n\nstatic int decode_rdma_segment(struct xdr_stream *xdr, u32 *length)\n{\n\tu32 handle;\n\tu64 offset;\n\t__be32 *p;\n\n\tp = xdr_inline_decode(xdr, 4 * sizeof(*p));\n\tif (unlikely(!p))\n\t\treturn -EIO;\n\n\txdr_decode_rdma_segment(p, &handle, length, &offset);\n\ttrace_xprtrdma_decode_seg(handle, *length, offset);\n\treturn 0;\n}\n\nstatic int decode_write_chunk(struct xdr_stream *xdr, u32 *length)\n{\n\tu32 segcount, seglength;\n\t__be32 *p;\n\n\tp = xdr_inline_decode(xdr, sizeof(*p));\n\tif (unlikely(!p))\n\t\treturn -EIO;\n\n\t*length = 0;\n\tsegcount = be32_to_cpup(p);\n\twhile (segcount--) {\n\t\tif (decode_rdma_segment(xdr, &seglength))\n\t\t\treturn -EIO;\n\t\t*length += seglength;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int decode_read_list(struct xdr_stream *xdr)\n{\n\t__be32 *p;\n\n\tp = xdr_inline_decode(xdr, sizeof(*p));\n\tif (unlikely(!p))\n\t\treturn -EIO;\n\tif (unlikely(xdr_item_is_present(p)))\n\t\treturn -EIO;\n\treturn 0;\n}\n\n \nstatic int decode_write_list(struct xdr_stream *xdr, u32 *length)\n{\n\tu32 chunklen;\n\tbool first;\n\t__be32 *p;\n\n\t*length = 0;\n\tfirst = true;\n\tdo {\n\t\tp = xdr_inline_decode(xdr, sizeof(*p));\n\t\tif (unlikely(!p))\n\t\t\treturn -EIO;\n\t\tif (xdr_item_is_absent(p))\n\t\t\tbreak;\n\t\tif (!first)\n\t\t\treturn -EIO;\n\n\t\tif (decode_write_chunk(xdr, &chunklen))\n\t\t\treturn -EIO;\n\t\t*length += chunklen;\n\t\tfirst = false;\n\t} while (true);\n\treturn 0;\n}\n\nstatic int decode_reply_chunk(struct xdr_stream *xdr, u32 *length)\n{\n\t__be32 *p;\n\n\tp = xdr_inline_decode(xdr, sizeof(*p));\n\tif (unlikely(!p))\n\t\treturn -EIO;\n\n\t*length = 0;\n\tif (xdr_item_is_present(p))\n\t\tif (decode_write_chunk(xdr, length))\n\t\t\treturn -EIO;\n\treturn 0;\n}\n\nstatic int\nrpcrdma_decode_msg(struct rpcrdma_xprt *r_xprt, struct rpcrdma_rep *rep,\n\t\t   struct rpc_rqst *rqst)\n{\n\tstruct xdr_stream *xdr = &rep->rr_stream;\n\tu32 writelist, replychunk, rpclen;\n\tchar *base;\n\n\t \n\tif (decode_read_list(xdr))\n\t\treturn -EIO;\n\tif (decode_write_list(xdr, &writelist))\n\t\treturn -EIO;\n\tif (decode_reply_chunk(xdr, &replychunk))\n\t\treturn -EIO;\n\n\t \n\tif (unlikely(replychunk))\n\t\treturn -EIO;\n\n\t \n\tbase = (char *)xdr_inline_decode(xdr, 0);\n\trpclen = xdr_stream_remaining(xdr);\n\tr_xprt->rx_stats.fixup_copy_count +=\n\t\trpcrdma_inline_fixup(rqst, base, rpclen, writelist & 3);\n\n\tr_xprt->rx_stats.total_rdma_reply += writelist;\n\treturn rpclen + xdr_align_size(writelist);\n}\n\nstatic noinline int\nrpcrdma_decode_nomsg(struct rpcrdma_xprt *r_xprt, struct rpcrdma_rep *rep)\n{\n\tstruct xdr_stream *xdr = &rep->rr_stream;\n\tu32 writelist, replychunk;\n\n\t \n\tif (decode_read_list(xdr))\n\t\treturn -EIO;\n\tif (decode_write_list(xdr, &writelist))\n\t\treturn -EIO;\n\tif (decode_reply_chunk(xdr, &replychunk))\n\t\treturn -EIO;\n\n\t \n\tif (unlikely(writelist))\n\t\treturn -EIO;\n\tif (unlikely(!replychunk))\n\t\treturn -EIO;\n\n\t \n\tr_xprt->rx_stats.total_rdma_reply += replychunk;\n\treturn replychunk;\n}\n\nstatic noinline int\nrpcrdma_decode_error(struct rpcrdma_xprt *r_xprt, struct rpcrdma_rep *rep,\n\t\t     struct rpc_rqst *rqst)\n{\n\tstruct xdr_stream *xdr = &rep->rr_stream;\n\t__be32 *p;\n\n\tp = xdr_inline_decode(xdr, sizeof(*p));\n\tif (unlikely(!p))\n\t\treturn -EIO;\n\n\tswitch (*p) {\n\tcase err_vers:\n\t\tp = xdr_inline_decode(xdr, 2 * sizeof(*p));\n\t\tif (!p)\n\t\t\tbreak;\n\t\ttrace_xprtrdma_err_vers(rqst, p, p + 1);\n\t\tbreak;\n\tcase err_chunk:\n\t\ttrace_xprtrdma_err_chunk(rqst);\n\t\tbreak;\n\tdefault:\n\t\ttrace_xprtrdma_err_unrecognized(rqst, p);\n\t}\n\n\treturn -EIO;\n}\n\n \nvoid rpcrdma_unpin_rqst(struct rpcrdma_rep *rep)\n{\n\tstruct rpc_xprt *xprt = &rep->rr_rxprt->rx_xprt;\n\tstruct rpc_rqst *rqst = rep->rr_rqst;\n\tstruct rpcrdma_req *req = rpcr_to_rdmar(rqst);\n\n\treq->rl_reply = NULL;\n\trep->rr_rqst = NULL;\n\n\tspin_lock(&xprt->queue_lock);\n\txprt_unpin_rqst(rqst);\n\tspin_unlock(&xprt->queue_lock);\n}\n\n \nvoid rpcrdma_complete_rqst(struct rpcrdma_rep *rep)\n{\n\tstruct rpcrdma_xprt *r_xprt = rep->rr_rxprt;\n\tstruct rpc_xprt *xprt = &r_xprt->rx_xprt;\n\tstruct rpc_rqst *rqst = rep->rr_rqst;\n\tint status;\n\n\tswitch (rep->rr_proc) {\n\tcase rdma_msg:\n\t\tstatus = rpcrdma_decode_msg(r_xprt, rep, rqst);\n\t\tbreak;\n\tcase rdma_nomsg:\n\t\tstatus = rpcrdma_decode_nomsg(r_xprt, rep);\n\t\tbreak;\n\tcase rdma_error:\n\t\tstatus = rpcrdma_decode_error(r_xprt, rep, rqst);\n\t\tbreak;\n\tdefault:\n\t\tstatus = -EIO;\n\t}\n\tif (status < 0)\n\t\tgoto out_badheader;\n\nout:\n\tspin_lock(&xprt->queue_lock);\n\txprt_complete_rqst(rqst->rq_task, status);\n\txprt_unpin_rqst(rqst);\n\tspin_unlock(&xprt->queue_lock);\n\treturn;\n\nout_badheader:\n\ttrace_xprtrdma_reply_hdr_err(rep);\n\tr_xprt->rx_stats.bad_reply_count++;\n\trqst->rq_task->tk_status = status;\n\tstatus = 0;\n\tgoto out;\n}\n\nstatic void rpcrdma_reply_done(struct kref *kref)\n{\n\tstruct rpcrdma_req *req =\n\t\tcontainer_of(kref, struct rpcrdma_req, rl_kref);\n\n\trpcrdma_complete_rqst(req->rl_reply);\n}\n\n \nvoid rpcrdma_reply_handler(struct rpcrdma_rep *rep)\n{\n\tstruct rpcrdma_xprt *r_xprt = rep->rr_rxprt;\n\tstruct rpc_xprt *xprt = &r_xprt->rx_xprt;\n\tstruct rpcrdma_buffer *buf = &r_xprt->rx_buf;\n\tstruct rpcrdma_req *req;\n\tstruct rpc_rqst *rqst;\n\tu32 credits;\n\t__be32 *p;\n\n\t \n\tif (xprt->reestablish_timeout)\n\t\txprt->reestablish_timeout = 0;\n\n\t \n\txdr_init_decode(&rep->rr_stream, &rep->rr_hdrbuf,\n\t\t\trep->rr_hdrbuf.head[0].iov_base, NULL);\n\tp = xdr_inline_decode(&rep->rr_stream, 4 * sizeof(*p));\n\tif (unlikely(!p))\n\t\tgoto out_shortreply;\n\trep->rr_xid = *p++;\n\trep->rr_vers = *p++;\n\tcredits = be32_to_cpu(*p++);\n\trep->rr_proc = *p++;\n\n\tif (rep->rr_vers != rpcrdma_version)\n\t\tgoto out_badversion;\n\n\tif (rpcrdma_is_bcall(r_xprt, rep))\n\t\treturn;\n\n\t \n\tspin_lock(&xprt->queue_lock);\n\trqst = xprt_lookup_rqst(xprt, rep->rr_xid);\n\tif (!rqst)\n\t\tgoto out_norqst;\n\txprt_pin_rqst(rqst);\n\tspin_unlock(&xprt->queue_lock);\n\n\tif (credits == 0)\n\t\tcredits = 1;\t \n\telse if (credits > r_xprt->rx_ep->re_max_requests)\n\t\tcredits = r_xprt->rx_ep->re_max_requests;\n\trpcrdma_post_recvs(r_xprt, credits + (buf->rb_bc_srv_max_requests << 1),\n\t\t\t   false);\n\tif (buf->rb_credits != credits)\n\t\trpcrdma_update_cwnd(r_xprt, credits);\n\n\treq = rpcr_to_rdmar(rqst);\n\tif (unlikely(req->rl_reply))\n\t\trpcrdma_rep_put(buf, req->rl_reply);\n\treq->rl_reply = rep;\n\trep->rr_rqst = rqst;\n\n\ttrace_xprtrdma_reply(rqst->rq_task, rep, credits);\n\n\tif (rep->rr_wc_flags & IB_WC_WITH_INVALIDATE)\n\t\tfrwr_reminv(rep, &req->rl_registered);\n\tif (!list_empty(&req->rl_registered))\n\t\tfrwr_unmap_async(r_xprt, req);\n\t\t \n\telse\n\t\tkref_put(&req->rl_kref, rpcrdma_reply_done);\n\treturn;\n\nout_badversion:\n\ttrace_xprtrdma_reply_vers_err(rep);\n\tgoto out;\n\nout_norqst:\n\tspin_unlock(&xprt->queue_lock);\n\ttrace_xprtrdma_reply_rqst_err(rep);\n\tgoto out;\n\nout_shortreply:\n\ttrace_xprtrdma_reply_short_err(rep);\n\nout:\n\trpcrdma_rep_put(buf, rep);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}