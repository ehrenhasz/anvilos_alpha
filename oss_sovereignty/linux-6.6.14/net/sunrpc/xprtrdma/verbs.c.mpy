{
  "module_name": "verbs.c",
  "hash_id": "75b82f673c4f95860badca3a09273c7aba12e429d4d1b98c405992ea18888b5e",
  "original_prompt": "Ingested from linux-6.6.14/net/sunrpc/xprtrdma/verbs.c",
  "human_readable_source": "\n \n\n \n\n#include <linux/interrupt.h>\n#include <linux/slab.h>\n#include <linux/sunrpc/addr.h>\n#include <linux/sunrpc/svc_rdma.h>\n#include <linux/log2.h>\n\n#include <asm-generic/barrier.h>\n#include <asm/bitops.h>\n\n#include <rdma/ib_cm.h>\n\n#include \"xprt_rdma.h\"\n#include <trace/events/rpcrdma.h>\n\nstatic int rpcrdma_sendctxs_create(struct rpcrdma_xprt *r_xprt);\nstatic void rpcrdma_sendctxs_destroy(struct rpcrdma_xprt *r_xprt);\nstatic void rpcrdma_sendctx_put_locked(struct rpcrdma_xprt *r_xprt,\n\t\t\t\t       struct rpcrdma_sendctx *sc);\nstatic int rpcrdma_reqs_setup(struct rpcrdma_xprt *r_xprt);\nstatic void rpcrdma_reqs_reset(struct rpcrdma_xprt *r_xprt);\nstatic void rpcrdma_rep_destroy(struct rpcrdma_rep *rep);\nstatic void rpcrdma_reps_unmap(struct rpcrdma_xprt *r_xprt);\nstatic void rpcrdma_mrs_create(struct rpcrdma_xprt *r_xprt);\nstatic void rpcrdma_mrs_destroy(struct rpcrdma_xprt *r_xprt);\nstatic void rpcrdma_ep_get(struct rpcrdma_ep *ep);\nstatic int rpcrdma_ep_put(struct rpcrdma_ep *ep);\nstatic struct rpcrdma_regbuf *\nrpcrdma_regbuf_alloc(size_t size, enum dma_data_direction direction);\nstatic void rpcrdma_regbuf_dma_unmap(struct rpcrdma_regbuf *rb);\nstatic void rpcrdma_regbuf_free(struct rpcrdma_regbuf *rb);\n\n \nstatic void rpcrdma_xprt_drain(struct rpcrdma_xprt *r_xprt)\n{\n\tstruct rpcrdma_ep *ep = r_xprt->rx_ep;\n\tstruct rdma_cm_id *id = ep->re_id;\n\n\t \n\tif (atomic_inc_return(&ep->re_receiving) > 1)\n\t\twait_for_completion(&ep->re_done);\n\n\t \n\tib_drain_rq(id->qp);\n\n\t \n\tib_drain_sq(id->qp);\n\n\trpcrdma_ep_put(ep);\n}\n\n \nvoid rpcrdma_force_disconnect(struct rpcrdma_ep *ep)\n{\n\tif (atomic_add_unless(&ep->re_force_disconnect, 1, 1))\n\t\txprt_force_disconnect(ep->re_xprt);\n}\n\n \nvoid rpcrdma_flush_disconnect(struct rpcrdma_xprt *r_xprt, struct ib_wc *wc)\n{\n\tif (wc->status != IB_WC_SUCCESS)\n\t\trpcrdma_force_disconnect(r_xprt->rx_ep);\n}\n\n \nstatic void rpcrdma_wc_send(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct ib_cqe *cqe = wc->wr_cqe;\n\tstruct rpcrdma_sendctx *sc =\n\t\tcontainer_of(cqe, struct rpcrdma_sendctx, sc_cqe);\n\tstruct rpcrdma_xprt *r_xprt = cq->cq_context;\n\n\t \n\ttrace_xprtrdma_wc_send(wc, &sc->sc_cid);\n\trpcrdma_sendctx_put_locked(r_xprt, sc);\n\trpcrdma_flush_disconnect(r_xprt, wc);\n}\n\n \nstatic void rpcrdma_wc_receive(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct ib_cqe *cqe = wc->wr_cqe;\n\tstruct rpcrdma_rep *rep = container_of(cqe, struct rpcrdma_rep,\n\t\t\t\t\t       rr_cqe);\n\tstruct rpcrdma_xprt *r_xprt = cq->cq_context;\n\n\t \n\ttrace_xprtrdma_wc_receive(wc, &rep->rr_cid);\n\t--r_xprt->rx_ep->re_receive_count;\n\tif (wc->status != IB_WC_SUCCESS)\n\t\tgoto out_flushed;\n\n\t \n\trpcrdma_set_xdrlen(&rep->rr_hdrbuf, wc->byte_len);\n\trep->rr_wc_flags = wc->wc_flags;\n\trep->rr_inv_rkey = wc->ex.invalidate_rkey;\n\n\tib_dma_sync_single_for_cpu(rdmab_device(rep->rr_rdmabuf),\n\t\t\t\t   rdmab_addr(rep->rr_rdmabuf),\n\t\t\t\t   wc->byte_len, DMA_FROM_DEVICE);\n\n\trpcrdma_reply_handler(rep);\n\treturn;\n\nout_flushed:\n\trpcrdma_flush_disconnect(r_xprt, wc);\n\trpcrdma_rep_put(&r_xprt->rx_buf, rep);\n}\n\nstatic void rpcrdma_update_cm_private(struct rpcrdma_ep *ep,\n\t\t\t\t      struct rdma_conn_param *param)\n{\n\tconst struct rpcrdma_connect_private *pmsg = param->private_data;\n\tunsigned int rsize, wsize;\n\n\t \n\trsize = RPCRDMA_V1_DEF_INLINE_SIZE;\n\twsize = RPCRDMA_V1_DEF_INLINE_SIZE;\n\n\tif (pmsg &&\n\t    pmsg->cp_magic == rpcrdma_cmp_magic &&\n\t    pmsg->cp_version == RPCRDMA_CMP_VERSION) {\n\t\trsize = rpcrdma_decode_buffer_size(pmsg->cp_send_size);\n\t\twsize = rpcrdma_decode_buffer_size(pmsg->cp_recv_size);\n\t}\n\n\tif (rsize < ep->re_inline_recv)\n\t\tep->re_inline_recv = rsize;\n\tif (wsize < ep->re_inline_send)\n\t\tep->re_inline_send = wsize;\n\n\trpcrdma_set_max_header_sizes(ep);\n}\n\n \nstatic int\nrpcrdma_cm_event_handler(struct rdma_cm_id *id, struct rdma_cm_event *event)\n{\n\tstruct sockaddr *sap = (struct sockaddr *)&id->route.addr.dst_addr;\n\tstruct rpcrdma_ep *ep = id->context;\n\n\tmight_sleep();\n\n\tswitch (event->event) {\n\tcase RDMA_CM_EVENT_ADDR_RESOLVED:\n\tcase RDMA_CM_EVENT_ROUTE_RESOLVED:\n\t\tep->re_async_rc = 0;\n\t\tcomplete(&ep->re_done);\n\t\treturn 0;\n\tcase RDMA_CM_EVENT_ADDR_ERROR:\n\t\tep->re_async_rc = -EPROTO;\n\t\tcomplete(&ep->re_done);\n\t\treturn 0;\n\tcase RDMA_CM_EVENT_ROUTE_ERROR:\n\t\tep->re_async_rc = -ENETUNREACH;\n\t\tcomplete(&ep->re_done);\n\t\treturn 0;\n\tcase RDMA_CM_EVENT_DEVICE_REMOVAL:\n\t\tpr_info(\"rpcrdma: removing device %s for %pISpc\\n\",\n\t\t\tep->re_id->device->name, sap);\n\t\tfallthrough;\n\tcase RDMA_CM_EVENT_ADDR_CHANGE:\n\t\tep->re_connect_status = -ENODEV;\n\t\tgoto disconnected;\n\tcase RDMA_CM_EVENT_ESTABLISHED:\n\t\trpcrdma_ep_get(ep);\n\t\tep->re_connect_status = 1;\n\t\trpcrdma_update_cm_private(ep, &event->param.conn);\n\t\ttrace_xprtrdma_inline_thresh(ep);\n\t\twake_up_all(&ep->re_connect_wait);\n\t\tbreak;\n\tcase RDMA_CM_EVENT_CONNECT_ERROR:\n\t\tep->re_connect_status = -ENOTCONN;\n\t\tgoto wake_connect_worker;\n\tcase RDMA_CM_EVENT_UNREACHABLE:\n\t\tep->re_connect_status = -ENETUNREACH;\n\t\tgoto wake_connect_worker;\n\tcase RDMA_CM_EVENT_REJECTED:\n\t\tep->re_connect_status = -ECONNREFUSED;\n\t\tif (event->status == IB_CM_REJ_STALE_CONN)\n\t\t\tep->re_connect_status = -ENOTCONN;\nwake_connect_worker:\n\t\twake_up_all(&ep->re_connect_wait);\n\t\treturn 0;\n\tcase RDMA_CM_EVENT_DISCONNECTED:\n\t\tep->re_connect_status = -ECONNABORTED;\ndisconnected:\n\t\trpcrdma_force_disconnect(ep);\n\t\treturn rpcrdma_ep_put(ep);\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic struct rdma_cm_id *rpcrdma_create_id(struct rpcrdma_xprt *r_xprt,\n\t\t\t\t\t    struct rpcrdma_ep *ep)\n{\n\tunsigned long wtimeout = msecs_to_jiffies(RDMA_RESOLVE_TIMEOUT) + 1;\n\tstruct rpc_xprt *xprt = &r_xprt->rx_xprt;\n\tstruct rdma_cm_id *id;\n\tint rc;\n\n\tinit_completion(&ep->re_done);\n\n\tid = rdma_create_id(xprt->xprt_net, rpcrdma_cm_event_handler, ep,\n\t\t\t    RDMA_PS_TCP, IB_QPT_RC);\n\tif (IS_ERR(id))\n\t\treturn id;\n\n\tep->re_async_rc = -ETIMEDOUT;\n\trc = rdma_resolve_addr(id, NULL, (struct sockaddr *)&xprt->addr,\n\t\t\t       RDMA_RESOLVE_TIMEOUT);\n\tif (rc)\n\t\tgoto out;\n\trc = wait_for_completion_interruptible_timeout(&ep->re_done, wtimeout);\n\tif (rc < 0)\n\t\tgoto out;\n\n\trc = ep->re_async_rc;\n\tif (rc)\n\t\tgoto out;\n\n\tep->re_async_rc = -ETIMEDOUT;\n\trc = rdma_resolve_route(id, RDMA_RESOLVE_TIMEOUT);\n\tif (rc)\n\t\tgoto out;\n\trc = wait_for_completion_interruptible_timeout(&ep->re_done, wtimeout);\n\tif (rc < 0)\n\t\tgoto out;\n\trc = ep->re_async_rc;\n\tif (rc)\n\t\tgoto out;\n\n\treturn id;\n\nout:\n\trdma_destroy_id(id);\n\treturn ERR_PTR(rc);\n}\n\nstatic void rpcrdma_ep_destroy(struct kref *kref)\n{\n\tstruct rpcrdma_ep *ep = container_of(kref, struct rpcrdma_ep, re_kref);\n\n\tif (ep->re_id->qp) {\n\t\trdma_destroy_qp(ep->re_id);\n\t\tep->re_id->qp = NULL;\n\t}\n\n\tif (ep->re_attr.recv_cq)\n\t\tib_free_cq(ep->re_attr.recv_cq);\n\tep->re_attr.recv_cq = NULL;\n\tif (ep->re_attr.send_cq)\n\t\tib_free_cq(ep->re_attr.send_cq);\n\tep->re_attr.send_cq = NULL;\n\n\tif (ep->re_pd)\n\t\tib_dealloc_pd(ep->re_pd);\n\tep->re_pd = NULL;\n\n\tkfree(ep);\n\tmodule_put(THIS_MODULE);\n}\n\nstatic noinline void rpcrdma_ep_get(struct rpcrdma_ep *ep)\n{\n\tkref_get(&ep->re_kref);\n}\n\n \nstatic noinline int rpcrdma_ep_put(struct rpcrdma_ep *ep)\n{\n\treturn kref_put(&ep->re_kref, rpcrdma_ep_destroy);\n}\n\nstatic int rpcrdma_ep_create(struct rpcrdma_xprt *r_xprt)\n{\n\tstruct rpcrdma_connect_private *pmsg;\n\tstruct ib_device *device;\n\tstruct rdma_cm_id *id;\n\tstruct rpcrdma_ep *ep;\n\tint rc;\n\n\tep = kzalloc(sizeof(*ep), XPRTRDMA_GFP_FLAGS);\n\tif (!ep)\n\t\treturn -ENOTCONN;\n\tep->re_xprt = &r_xprt->rx_xprt;\n\tkref_init(&ep->re_kref);\n\n\tid = rpcrdma_create_id(r_xprt, ep);\n\tif (IS_ERR(id)) {\n\t\tkfree(ep);\n\t\treturn PTR_ERR(id);\n\t}\n\t__module_get(THIS_MODULE);\n\tdevice = id->device;\n\tep->re_id = id;\n\treinit_completion(&ep->re_done);\n\n\tep->re_max_requests = r_xprt->rx_xprt.max_reqs;\n\tep->re_inline_send = xprt_rdma_max_inline_write;\n\tep->re_inline_recv = xprt_rdma_max_inline_read;\n\trc = frwr_query_device(ep, device);\n\tif (rc)\n\t\tgoto out_destroy;\n\n\tr_xprt->rx_buf.rb_max_requests = cpu_to_be32(ep->re_max_requests);\n\n\tep->re_attr.srq = NULL;\n\tep->re_attr.cap.max_inline_data = 0;\n\tep->re_attr.sq_sig_type = IB_SIGNAL_REQ_WR;\n\tep->re_attr.qp_type = IB_QPT_RC;\n\tep->re_attr.port_num = ~0;\n\n\tep->re_send_batch = ep->re_max_requests >> 3;\n\tep->re_send_count = ep->re_send_batch;\n\tinit_waitqueue_head(&ep->re_connect_wait);\n\n\tep->re_attr.send_cq = ib_alloc_cq_any(device, r_xprt,\n\t\t\t\t\t      ep->re_attr.cap.max_send_wr,\n\t\t\t\t\t      IB_POLL_WORKQUEUE);\n\tif (IS_ERR(ep->re_attr.send_cq)) {\n\t\trc = PTR_ERR(ep->re_attr.send_cq);\n\t\tep->re_attr.send_cq = NULL;\n\t\tgoto out_destroy;\n\t}\n\n\tep->re_attr.recv_cq = ib_alloc_cq_any(device, r_xprt,\n\t\t\t\t\t      ep->re_attr.cap.max_recv_wr,\n\t\t\t\t\t      IB_POLL_WORKQUEUE);\n\tif (IS_ERR(ep->re_attr.recv_cq)) {\n\t\trc = PTR_ERR(ep->re_attr.recv_cq);\n\t\tep->re_attr.recv_cq = NULL;\n\t\tgoto out_destroy;\n\t}\n\tep->re_receive_count = 0;\n\n\t \n\tmemset(&ep->re_remote_cma, 0, sizeof(ep->re_remote_cma));\n\n\t \n\tpmsg = &ep->re_cm_private;\n\tpmsg->cp_magic = rpcrdma_cmp_magic;\n\tpmsg->cp_version = RPCRDMA_CMP_VERSION;\n\tpmsg->cp_flags |= RPCRDMA_CMP_F_SND_W_INV_OK;\n\tpmsg->cp_send_size = rpcrdma_encode_buffer_size(ep->re_inline_send);\n\tpmsg->cp_recv_size = rpcrdma_encode_buffer_size(ep->re_inline_recv);\n\tep->re_remote_cma.private_data = pmsg;\n\tep->re_remote_cma.private_data_len = sizeof(*pmsg);\n\n\t \n\tep->re_remote_cma.initiator_depth = 0;\n\tep->re_remote_cma.responder_resources =\n\t\tmin_t(int, U8_MAX, device->attrs.max_qp_rd_atom);\n\n\t \n\tep->re_remote_cma.retry_count = 6;\n\n\t \n\tep->re_remote_cma.flow_control = 0;\n\tep->re_remote_cma.rnr_retry_count = 0;\n\n\tep->re_pd = ib_alloc_pd(device, 0);\n\tif (IS_ERR(ep->re_pd)) {\n\t\trc = PTR_ERR(ep->re_pd);\n\t\tep->re_pd = NULL;\n\t\tgoto out_destroy;\n\t}\n\n\trc = rdma_create_qp(id, ep->re_pd, &ep->re_attr);\n\tif (rc)\n\t\tgoto out_destroy;\n\n\tr_xprt->rx_ep = ep;\n\treturn 0;\n\nout_destroy:\n\trpcrdma_ep_put(ep);\n\trdma_destroy_id(id);\n\treturn rc;\n}\n\n \nint rpcrdma_xprt_connect(struct rpcrdma_xprt *r_xprt)\n{\n\tstruct rpc_xprt *xprt = &r_xprt->rx_xprt;\n\tstruct rpcrdma_ep *ep;\n\tint rc;\n\n\trc = rpcrdma_ep_create(r_xprt);\n\tif (rc)\n\t\treturn rc;\n\tep = r_xprt->rx_ep;\n\n\txprt_clear_connected(xprt);\n\trpcrdma_reset_cwnd(r_xprt);\n\n\t \n\trpcrdma_ep_get(ep);\n\trpcrdma_post_recvs(r_xprt, 1, true);\n\n\trc = rdma_connect(ep->re_id, &ep->re_remote_cma);\n\tif (rc)\n\t\tgoto out;\n\n\tif (xprt->reestablish_timeout < RPCRDMA_INIT_REEST_TO)\n\t\txprt->reestablish_timeout = RPCRDMA_INIT_REEST_TO;\n\twait_event_interruptible(ep->re_connect_wait,\n\t\t\t\t ep->re_connect_status != 0);\n\tif (ep->re_connect_status <= 0) {\n\t\trc = ep->re_connect_status;\n\t\tgoto out;\n\t}\n\n\trc = rpcrdma_sendctxs_create(r_xprt);\n\tif (rc) {\n\t\trc = -ENOTCONN;\n\t\tgoto out;\n\t}\n\n\trc = rpcrdma_reqs_setup(r_xprt);\n\tif (rc) {\n\t\trc = -ENOTCONN;\n\t\tgoto out;\n\t}\n\trpcrdma_mrs_create(r_xprt);\n\tfrwr_wp_create(r_xprt);\n\nout:\n\ttrace_xprtrdma_connect(r_xprt, rc);\n\treturn rc;\n}\n\n \nvoid rpcrdma_xprt_disconnect(struct rpcrdma_xprt *r_xprt)\n{\n\tstruct rpcrdma_ep *ep = r_xprt->rx_ep;\n\tstruct rdma_cm_id *id;\n\tint rc;\n\n\tif (!ep)\n\t\treturn;\n\n\tid = ep->re_id;\n\trc = rdma_disconnect(id);\n\ttrace_xprtrdma_disconnect(r_xprt, rc);\n\n\trpcrdma_xprt_drain(r_xprt);\n\trpcrdma_reps_unmap(r_xprt);\n\trpcrdma_reqs_reset(r_xprt);\n\trpcrdma_mrs_destroy(r_xprt);\n\trpcrdma_sendctxs_destroy(r_xprt);\n\n\tif (rpcrdma_ep_put(ep))\n\t\trdma_destroy_id(id);\n\n\tr_xprt->rx_ep = NULL;\n}\n\n \n\n \nstatic void rpcrdma_sendctxs_destroy(struct rpcrdma_xprt *r_xprt)\n{\n\tstruct rpcrdma_buffer *buf = &r_xprt->rx_buf;\n\tunsigned long i;\n\n\tif (!buf->rb_sc_ctxs)\n\t\treturn;\n\tfor (i = 0; i <= buf->rb_sc_last; i++)\n\t\tkfree(buf->rb_sc_ctxs[i]);\n\tkfree(buf->rb_sc_ctxs);\n\tbuf->rb_sc_ctxs = NULL;\n}\n\nstatic struct rpcrdma_sendctx *rpcrdma_sendctx_create(struct rpcrdma_ep *ep)\n{\n\tstruct rpcrdma_sendctx *sc;\n\n\tsc = kzalloc(struct_size(sc, sc_sges, ep->re_attr.cap.max_send_sge),\n\t\t     XPRTRDMA_GFP_FLAGS);\n\tif (!sc)\n\t\treturn NULL;\n\n\tsc->sc_cqe.done = rpcrdma_wc_send;\n\tsc->sc_cid.ci_queue_id = ep->re_attr.send_cq->res.id;\n\tsc->sc_cid.ci_completion_id =\n\t\tatomic_inc_return(&ep->re_completion_ids);\n\treturn sc;\n}\n\nstatic int rpcrdma_sendctxs_create(struct rpcrdma_xprt *r_xprt)\n{\n\tstruct rpcrdma_buffer *buf = &r_xprt->rx_buf;\n\tstruct rpcrdma_sendctx *sc;\n\tunsigned long i;\n\n\t \n\ti = r_xprt->rx_ep->re_max_requests + RPCRDMA_MAX_BC_REQUESTS;\n\tbuf->rb_sc_ctxs = kcalloc(i, sizeof(sc), XPRTRDMA_GFP_FLAGS);\n\tif (!buf->rb_sc_ctxs)\n\t\treturn -ENOMEM;\n\n\tbuf->rb_sc_last = i - 1;\n\tfor (i = 0; i <= buf->rb_sc_last; i++) {\n\t\tsc = rpcrdma_sendctx_create(r_xprt->rx_ep);\n\t\tif (!sc)\n\t\t\treturn -ENOMEM;\n\n\t\tbuf->rb_sc_ctxs[i] = sc;\n\t}\n\n\tbuf->rb_sc_head = 0;\n\tbuf->rb_sc_tail = 0;\n\treturn 0;\n}\n\n \nstatic unsigned long rpcrdma_sendctx_next(struct rpcrdma_buffer *buf,\n\t\t\t\t\t  unsigned long item)\n{\n\treturn likely(item < buf->rb_sc_last) ? item + 1 : 0;\n}\n\n \nstruct rpcrdma_sendctx *rpcrdma_sendctx_get_locked(struct rpcrdma_xprt *r_xprt)\n{\n\tstruct rpcrdma_buffer *buf = &r_xprt->rx_buf;\n\tstruct rpcrdma_sendctx *sc;\n\tunsigned long next_head;\n\n\tnext_head = rpcrdma_sendctx_next(buf, buf->rb_sc_head);\n\n\tif (next_head == READ_ONCE(buf->rb_sc_tail))\n\t\tgoto out_emptyq;\n\n\t \n\tsc = buf->rb_sc_ctxs[next_head];\n\n\t \n\tbuf->rb_sc_head = next_head;\n\n\treturn sc;\n\nout_emptyq:\n\t \n\txprt_wait_for_buffer_space(&r_xprt->rx_xprt);\n\tr_xprt->rx_stats.empty_sendctx_q++;\n\treturn NULL;\n}\n\n \nstatic void rpcrdma_sendctx_put_locked(struct rpcrdma_xprt *r_xprt,\n\t\t\t\t       struct rpcrdma_sendctx *sc)\n{\n\tstruct rpcrdma_buffer *buf = &r_xprt->rx_buf;\n\tunsigned long next_tail;\n\n\t \n\tnext_tail = buf->rb_sc_tail;\n\tdo {\n\t\tnext_tail = rpcrdma_sendctx_next(buf, next_tail);\n\n\t\t \n\t\trpcrdma_sendctx_unmap(buf->rb_sc_ctxs[next_tail]);\n\n\t} while (buf->rb_sc_ctxs[next_tail] != sc);\n\n\t \n\tsmp_store_release(&buf->rb_sc_tail, next_tail);\n\n\txprt_write_space(&r_xprt->rx_xprt);\n}\n\nstatic void\nrpcrdma_mrs_create(struct rpcrdma_xprt *r_xprt)\n{\n\tstruct rpcrdma_buffer *buf = &r_xprt->rx_buf;\n\tstruct rpcrdma_ep *ep = r_xprt->rx_ep;\n\tstruct ib_device *device = ep->re_id->device;\n\tunsigned int count;\n\n\t \n\tfor (count = 0; count < ep->re_max_rdma_segs; count++) {\n\t\tstruct rpcrdma_mr *mr;\n\t\tint rc;\n\n\t\tmr = kzalloc_node(sizeof(*mr), XPRTRDMA_GFP_FLAGS,\n\t\t\t\t  ibdev_to_node(device));\n\t\tif (!mr)\n\t\t\tbreak;\n\n\t\trc = frwr_mr_init(r_xprt, mr);\n\t\tif (rc) {\n\t\t\tkfree(mr);\n\t\t\tbreak;\n\t\t}\n\n\t\tspin_lock(&buf->rb_lock);\n\t\trpcrdma_mr_push(mr, &buf->rb_mrs);\n\t\tlist_add(&mr->mr_all, &buf->rb_all_mrs);\n\t\tspin_unlock(&buf->rb_lock);\n\t}\n\n\tr_xprt->rx_stats.mrs_allocated += count;\n\ttrace_xprtrdma_createmrs(r_xprt, count);\n}\n\nstatic void\nrpcrdma_mr_refresh_worker(struct work_struct *work)\n{\n\tstruct rpcrdma_buffer *buf = container_of(work, struct rpcrdma_buffer,\n\t\t\t\t\t\t  rb_refresh_worker);\n\tstruct rpcrdma_xprt *r_xprt = container_of(buf, struct rpcrdma_xprt,\n\t\t\t\t\t\t   rx_buf);\n\n\trpcrdma_mrs_create(r_xprt);\n\txprt_write_space(&r_xprt->rx_xprt);\n}\n\n \nvoid rpcrdma_mrs_refresh(struct rpcrdma_xprt *r_xprt)\n{\n\tstruct rpcrdma_buffer *buf = &r_xprt->rx_buf;\n\tstruct rpcrdma_ep *ep = r_xprt->rx_ep;\n\n\t \n\tif (ep->re_connect_status != 1)\n\t\treturn;\n\tqueue_work(system_highpri_wq, &buf->rb_refresh_worker);\n}\n\n \nstruct rpcrdma_req *rpcrdma_req_create(struct rpcrdma_xprt *r_xprt,\n\t\t\t\t       size_t size)\n{\n\tstruct rpcrdma_buffer *buffer = &r_xprt->rx_buf;\n\tstruct rpcrdma_req *req;\n\n\treq = kzalloc(sizeof(*req), XPRTRDMA_GFP_FLAGS);\n\tif (req == NULL)\n\t\tgoto out1;\n\n\treq->rl_sendbuf = rpcrdma_regbuf_alloc(size, DMA_TO_DEVICE);\n\tif (!req->rl_sendbuf)\n\t\tgoto out2;\n\n\treq->rl_recvbuf = rpcrdma_regbuf_alloc(size, DMA_NONE);\n\tif (!req->rl_recvbuf)\n\t\tgoto out3;\n\n\tINIT_LIST_HEAD(&req->rl_free_mrs);\n\tINIT_LIST_HEAD(&req->rl_registered);\n\tspin_lock(&buffer->rb_lock);\n\tlist_add(&req->rl_all, &buffer->rb_allreqs);\n\tspin_unlock(&buffer->rb_lock);\n\treturn req;\n\nout3:\n\trpcrdma_regbuf_free(req->rl_sendbuf);\nout2:\n\tkfree(req);\nout1:\n\treturn NULL;\n}\n\n \nint rpcrdma_req_setup(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req)\n{\n\tstruct rpcrdma_regbuf *rb;\n\tsize_t maxhdrsize;\n\n\t \n\tmaxhdrsize = rpcrdma_fixed_maxsz + 3 +\n\t\t     r_xprt->rx_ep->re_max_rdma_segs * rpcrdma_readchunk_maxsz;\n\tmaxhdrsize *= sizeof(__be32);\n\trb = rpcrdma_regbuf_alloc(__roundup_pow_of_two(maxhdrsize),\n\t\t\t\t  DMA_TO_DEVICE);\n\tif (!rb)\n\t\tgoto out;\n\n\tif (!__rpcrdma_regbuf_dma_map(r_xprt, rb))\n\t\tgoto out_free;\n\n\treq->rl_rdmabuf = rb;\n\txdr_buf_init(&req->rl_hdrbuf, rdmab_data(rb), rdmab_length(rb));\n\treturn 0;\n\nout_free:\n\trpcrdma_regbuf_free(rb);\nout:\n\treturn -ENOMEM;\n}\n\n \nstatic int rpcrdma_reqs_setup(struct rpcrdma_xprt *r_xprt)\n{\n\tstruct rpcrdma_buffer *buf = &r_xprt->rx_buf;\n\tstruct rpcrdma_req *req;\n\tint rc;\n\n\tlist_for_each_entry(req, &buf->rb_allreqs, rl_all) {\n\t\trc = rpcrdma_req_setup(r_xprt, req);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\treturn 0;\n}\n\nstatic void rpcrdma_req_reset(struct rpcrdma_req *req)\n{\n\t \n\treq->rl_slot.rq_cong = 0;\n\n\trpcrdma_regbuf_free(req->rl_rdmabuf);\n\treq->rl_rdmabuf = NULL;\n\n\trpcrdma_regbuf_dma_unmap(req->rl_sendbuf);\n\trpcrdma_regbuf_dma_unmap(req->rl_recvbuf);\n\n\tfrwr_reset(req);\n}\n\n \nstatic void rpcrdma_reqs_reset(struct rpcrdma_xprt *r_xprt)\n{\n\tstruct rpcrdma_buffer *buf = &r_xprt->rx_buf;\n\tstruct rpcrdma_req *req;\n\n\tlist_for_each_entry(req, &buf->rb_allreqs, rl_all)\n\t\trpcrdma_req_reset(req);\n}\n\nstatic noinline\nstruct rpcrdma_rep *rpcrdma_rep_create(struct rpcrdma_xprt *r_xprt,\n\t\t\t\t       bool temp)\n{\n\tstruct rpcrdma_buffer *buf = &r_xprt->rx_buf;\n\tstruct rpcrdma_rep *rep;\n\n\trep = kzalloc(sizeof(*rep), XPRTRDMA_GFP_FLAGS);\n\tif (rep == NULL)\n\t\tgoto out;\n\n\trep->rr_rdmabuf = rpcrdma_regbuf_alloc(r_xprt->rx_ep->re_inline_recv,\n\t\t\t\t\t       DMA_FROM_DEVICE);\n\tif (!rep->rr_rdmabuf)\n\t\tgoto out_free;\n\n\trep->rr_cid.ci_completion_id =\n\t\tatomic_inc_return(&r_xprt->rx_ep->re_completion_ids);\n\n\txdr_buf_init(&rep->rr_hdrbuf, rdmab_data(rep->rr_rdmabuf),\n\t\t     rdmab_length(rep->rr_rdmabuf));\n\trep->rr_cqe.done = rpcrdma_wc_receive;\n\trep->rr_rxprt = r_xprt;\n\trep->rr_recv_wr.next = NULL;\n\trep->rr_recv_wr.wr_cqe = &rep->rr_cqe;\n\trep->rr_recv_wr.sg_list = &rep->rr_rdmabuf->rg_iov;\n\trep->rr_recv_wr.num_sge = 1;\n\trep->rr_temp = temp;\n\n\tspin_lock(&buf->rb_lock);\n\tlist_add(&rep->rr_all, &buf->rb_all_reps);\n\tspin_unlock(&buf->rb_lock);\n\treturn rep;\n\nout_free:\n\tkfree(rep);\nout:\n\treturn NULL;\n}\n\nstatic void rpcrdma_rep_free(struct rpcrdma_rep *rep)\n{\n\trpcrdma_regbuf_free(rep->rr_rdmabuf);\n\tkfree(rep);\n}\n\nstatic void rpcrdma_rep_destroy(struct rpcrdma_rep *rep)\n{\n\tstruct rpcrdma_buffer *buf = &rep->rr_rxprt->rx_buf;\n\n\tspin_lock(&buf->rb_lock);\n\tlist_del(&rep->rr_all);\n\tspin_unlock(&buf->rb_lock);\n\n\trpcrdma_rep_free(rep);\n}\n\nstatic struct rpcrdma_rep *rpcrdma_rep_get_locked(struct rpcrdma_buffer *buf)\n{\n\tstruct llist_node *node;\n\n\t \n\tnode = llist_del_first(&buf->rb_free_reps);\n\tif (!node)\n\t\treturn NULL;\n\treturn llist_entry(node, struct rpcrdma_rep, rr_node);\n}\n\n \nvoid rpcrdma_rep_put(struct rpcrdma_buffer *buf, struct rpcrdma_rep *rep)\n{\n\tllist_add(&rep->rr_node, &buf->rb_free_reps);\n}\n\n \nstatic void rpcrdma_reps_unmap(struct rpcrdma_xprt *r_xprt)\n{\n\tstruct rpcrdma_buffer *buf = &r_xprt->rx_buf;\n\tstruct rpcrdma_rep *rep;\n\n\tlist_for_each_entry(rep, &buf->rb_all_reps, rr_all) {\n\t\trpcrdma_regbuf_dma_unmap(rep->rr_rdmabuf);\n\t\trep->rr_temp = true;\t \n\t}\n}\n\nstatic void rpcrdma_reps_destroy(struct rpcrdma_buffer *buf)\n{\n\tstruct rpcrdma_rep *rep;\n\n\tspin_lock(&buf->rb_lock);\n\twhile ((rep = list_first_entry_or_null(&buf->rb_all_reps,\n\t\t\t\t\t       struct rpcrdma_rep,\n\t\t\t\t\t       rr_all)) != NULL) {\n\t\tlist_del(&rep->rr_all);\n\t\tspin_unlock(&buf->rb_lock);\n\n\t\trpcrdma_rep_free(rep);\n\n\t\tspin_lock(&buf->rb_lock);\n\t}\n\tspin_unlock(&buf->rb_lock);\n}\n\n \nint rpcrdma_buffer_create(struct rpcrdma_xprt *r_xprt)\n{\n\tstruct rpcrdma_buffer *buf = &r_xprt->rx_buf;\n\tint i, rc;\n\n\tbuf->rb_bc_srv_max_requests = 0;\n\tspin_lock_init(&buf->rb_lock);\n\tINIT_LIST_HEAD(&buf->rb_mrs);\n\tINIT_LIST_HEAD(&buf->rb_all_mrs);\n\tINIT_WORK(&buf->rb_refresh_worker, rpcrdma_mr_refresh_worker);\n\n\tINIT_LIST_HEAD(&buf->rb_send_bufs);\n\tINIT_LIST_HEAD(&buf->rb_allreqs);\n\tINIT_LIST_HEAD(&buf->rb_all_reps);\n\n\trc = -ENOMEM;\n\tfor (i = 0; i < r_xprt->rx_xprt.max_reqs; i++) {\n\t\tstruct rpcrdma_req *req;\n\n\t\treq = rpcrdma_req_create(r_xprt,\n\t\t\t\t\t RPCRDMA_V1_DEF_INLINE_SIZE * 2);\n\t\tif (!req)\n\t\t\tgoto out;\n\t\tlist_add(&req->rl_list, &buf->rb_send_bufs);\n\t}\n\n\tinit_llist_head(&buf->rb_free_reps);\n\n\treturn 0;\nout:\n\trpcrdma_buffer_destroy(buf);\n\treturn rc;\n}\n\n \nvoid rpcrdma_req_destroy(struct rpcrdma_req *req)\n{\n\tstruct rpcrdma_mr *mr;\n\n\tlist_del(&req->rl_all);\n\n\twhile ((mr = rpcrdma_mr_pop(&req->rl_free_mrs))) {\n\t\tstruct rpcrdma_buffer *buf = &mr->mr_xprt->rx_buf;\n\n\t\tspin_lock(&buf->rb_lock);\n\t\tlist_del(&mr->mr_all);\n\t\tspin_unlock(&buf->rb_lock);\n\n\t\tfrwr_mr_release(mr);\n\t}\n\n\trpcrdma_regbuf_free(req->rl_recvbuf);\n\trpcrdma_regbuf_free(req->rl_sendbuf);\n\trpcrdma_regbuf_free(req->rl_rdmabuf);\n\tkfree(req);\n}\n\n \nstatic void rpcrdma_mrs_destroy(struct rpcrdma_xprt *r_xprt)\n{\n\tstruct rpcrdma_buffer *buf = &r_xprt->rx_buf;\n\tstruct rpcrdma_mr *mr;\n\n\tcancel_work_sync(&buf->rb_refresh_worker);\n\n\tspin_lock(&buf->rb_lock);\n\twhile ((mr = list_first_entry_or_null(&buf->rb_all_mrs,\n\t\t\t\t\t      struct rpcrdma_mr,\n\t\t\t\t\t      mr_all)) != NULL) {\n\t\tlist_del(&mr->mr_list);\n\t\tlist_del(&mr->mr_all);\n\t\tspin_unlock(&buf->rb_lock);\n\n\t\tfrwr_mr_release(mr);\n\n\t\tspin_lock(&buf->rb_lock);\n\t}\n\tspin_unlock(&buf->rb_lock);\n}\n\n \nvoid\nrpcrdma_buffer_destroy(struct rpcrdma_buffer *buf)\n{\n\trpcrdma_reps_destroy(buf);\n\n\twhile (!list_empty(&buf->rb_send_bufs)) {\n\t\tstruct rpcrdma_req *req;\n\n\t\treq = list_first_entry(&buf->rb_send_bufs,\n\t\t\t\t       struct rpcrdma_req, rl_list);\n\t\tlist_del(&req->rl_list);\n\t\trpcrdma_req_destroy(req);\n\t}\n}\n\n \nstruct rpcrdma_mr *\nrpcrdma_mr_get(struct rpcrdma_xprt *r_xprt)\n{\n\tstruct rpcrdma_buffer *buf = &r_xprt->rx_buf;\n\tstruct rpcrdma_mr *mr;\n\n\tspin_lock(&buf->rb_lock);\n\tmr = rpcrdma_mr_pop(&buf->rb_mrs);\n\tspin_unlock(&buf->rb_lock);\n\treturn mr;\n}\n\n \nvoid rpcrdma_reply_put(struct rpcrdma_buffer *buffers, struct rpcrdma_req *req)\n{\n\tif (req->rl_reply) {\n\t\trpcrdma_rep_put(buffers, req->rl_reply);\n\t\treq->rl_reply = NULL;\n\t}\n}\n\n \nstruct rpcrdma_req *\nrpcrdma_buffer_get(struct rpcrdma_buffer *buffers)\n{\n\tstruct rpcrdma_req *req;\n\n\tspin_lock(&buffers->rb_lock);\n\treq = list_first_entry_or_null(&buffers->rb_send_bufs,\n\t\t\t\t       struct rpcrdma_req, rl_list);\n\tif (req)\n\t\tlist_del_init(&req->rl_list);\n\tspin_unlock(&buffers->rb_lock);\n\treturn req;\n}\n\n \nvoid rpcrdma_buffer_put(struct rpcrdma_buffer *buffers, struct rpcrdma_req *req)\n{\n\trpcrdma_reply_put(buffers, req);\n\n\tspin_lock(&buffers->rb_lock);\n\tlist_add(&req->rl_list, &buffers->rb_send_bufs);\n\tspin_unlock(&buffers->rb_lock);\n}\n\n \nstatic struct rpcrdma_regbuf *\nrpcrdma_regbuf_alloc(size_t size, enum dma_data_direction direction)\n{\n\tstruct rpcrdma_regbuf *rb;\n\n\trb = kmalloc(sizeof(*rb), XPRTRDMA_GFP_FLAGS);\n\tif (!rb)\n\t\treturn NULL;\n\trb->rg_data = kmalloc(size, XPRTRDMA_GFP_FLAGS);\n\tif (!rb->rg_data) {\n\t\tkfree(rb);\n\t\treturn NULL;\n\t}\n\n\trb->rg_device = NULL;\n\trb->rg_direction = direction;\n\trb->rg_iov.length = size;\n\treturn rb;\n}\n\n \nbool rpcrdma_regbuf_realloc(struct rpcrdma_regbuf *rb, size_t size, gfp_t flags)\n{\n\tvoid *buf;\n\n\tbuf = kmalloc(size, flags);\n\tif (!buf)\n\t\treturn false;\n\n\trpcrdma_regbuf_dma_unmap(rb);\n\tkfree(rb->rg_data);\n\n\trb->rg_data = buf;\n\trb->rg_iov.length = size;\n\treturn true;\n}\n\n \nbool __rpcrdma_regbuf_dma_map(struct rpcrdma_xprt *r_xprt,\n\t\t\t      struct rpcrdma_regbuf *rb)\n{\n\tstruct ib_device *device = r_xprt->rx_ep->re_id->device;\n\n\tif (rb->rg_direction == DMA_NONE)\n\t\treturn false;\n\n\trb->rg_iov.addr = ib_dma_map_single(device, rdmab_data(rb),\n\t\t\t\t\t    rdmab_length(rb), rb->rg_direction);\n\tif (ib_dma_mapping_error(device, rdmab_addr(rb))) {\n\t\ttrace_xprtrdma_dma_maperr(rdmab_addr(rb));\n\t\treturn false;\n\t}\n\n\trb->rg_device = device;\n\trb->rg_iov.lkey = r_xprt->rx_ep->re_pd->local_dma_lkey;\n\treturn true;\n}\n\nstatic void rpcrdma_regbuf_dma_unmap(struct rpcrdma_regbuf *rb)\n{\n\tif (!rb)\n\t\treturn;\n\n\tif (!rpcrdma_regbuf_is_mapped(rb))\n\t\treturn;\n\n\tib_dma_unmap_single(rb->rg_device, rdmab_addr(rb), rdmab_length(rb),\n\t\t\t    rb->rg_direction);\n\trb->rg_device = NULL;\n}\n\nstatic void rpcrdma_regbuf_free(struct rpcrdma_regbuf *rb)\n{\n\trpcrdma_regbuf_dma_unmap(rb);\n\tif (rb)\n\t\tkfree(rb->rg_data);\n\tkfree(rb);\n}\n\n \nvoid rpcrdma_post_recvs(struct rpcrdma_xprt *r_xprt, int needed, bool temp)\n{\n\tstruct rpcrdma_buffer *buf = &r_xprt->rx_buf;\n\tstruct rpcrdma_ep *ep = r_xprt->rx_ep;\n\tstruct ib_recv_wr *wr, *bad_wr;\n\tstruct rpcrdma_rep *rep;\n\tint count, rc;\n\n\trc = 0;\n\tcount = 0;\n\n\tif (likely(ep->re_receive_count > needed))\n\t\tgoto out;\n\tneeded -= ep->re_receive_count;\n\tif (!temp)\n\t\tneeded += RPCRDMA_MAX_RECV_BATCH;\n\n\tif (atomic_inc_return(&ep->re_receiving) > 1)\n\t\tgoto out;\n\n\t \n\twr = NULL;\n\twhile (needed) {\n\t\trep = rpcrdma_rep_get_locked(buf);\n\t\tif (rep && rep->rr_temp) {\n\t\t\trpcrdma_rep_destroy(rep);\n\t\t\tcontinue;\n\t\t}\n\t\tif (!rep)\n\t\t\trep = rpcrdma_rep_create(r_xprt, temp);\n\t\tif (!rep)\n\t\t\tbreak;\n\t\tif (!rpcrdma_regbuf_dma_map(r_xprt, rep->rr_rdmabuf)) {\n\t\t\trpcrdma_rep_put(buf, rep);\n\t\t\tbreak;\n\t\t}\n\n\t\trep->rr_cid.ci_queue_id = ep->re_attr.recv_cq->res.id;\n\t\ttrace_xprtrdma_post_recv(rep);\n\t\trep->rr_recv_wr.next = wr;\n\t\twr = &rep->rr_recv_wr;\n\t\t--needed;\n\t\t++count;\n\t}\n\tif (!wr)\n\t\tgoto out;\n\n\trc = ib_post_recv(ep->re_id->qp, wr,\n\t\t\t  (const struct ib_recv_wr **)&bad_wr);\n\tif (rc) {\n\t\ttrace_xprtrdma_post_recvs_err(r_xprt, rc);\n\t\tfor (wr = bad_wr; wr;) {\n\t\t\tstruct rpcrdma_rep *rep;\n\n\t\t\trep = container_of(wr, struct rpcrdma_rep, rr_recv_wr);\n\t\t\twr = wr->next;\n\t\t\trpcrdma_rep_put(buf, rep);\n\t\t\t--count;\n\t\t}\n\t}\n\tif (atomic_dec_return(&ep->re_receiving) > 0)\n\t\tcomplete(&ep->re_done);\n\nout:\n\ttrace_xprtrdma_post_recvs(r_xprt, count);\n\tep->re_receive_count += count;\n\treturn;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}