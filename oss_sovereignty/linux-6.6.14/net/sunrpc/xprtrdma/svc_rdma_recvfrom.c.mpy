{
  "module_name": "svc_rdma_recvfrom.c",
  "hash_id": "feb98f6e792eb4b385ca02d560348f119648b0b523f8156dcd8c5e74883bdc64",
  "original_prompt": "Ingested from linux-6.6.14/net/sunrpc/xprtrdma/svc_rdma_recvfrom.c",
  "human_readable_source": "\n \n\n \n\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n#include <asm/unaligned.h>\n#include <rdma/ib_verbs.h>\n#include <rdma/rdma_cm.h>\n\n#include <linux/sunrpc/xdr.h>\n#include <linux/sunrpc/debug.h>\n#include <linux/sunrpc/rpc_rdma.h>\n#include <linux/sunrpc/svc_rdma.h>\n\n#include \"xprt_rdma.h\"\n#include <trace/events/rpcrdma.h>\n\nstatic void svc_rdma_wc_receive(struct ib_cq *cq, struct ib_wc *wc);\n\nstatic inline struct svc_rdma_recv_ctxt *\nsvc_rdma_next_recv_ctxt(struct list_head *list)\n{\n\treturn list_first_entry_or_null(list, struct svc_rdma_recv_ctxt,\n\t\t\t\t\trc_list);\n}\n\nstatic void svc_rdma_recv_cid_init(struct svcxprt_rdma *rdma,\n\t\t\t\t   struct rpc_rdma_cid *cid)\n{\n\tcid->ci_queue_id = rdma->sc_rq_cq->res.id;\n\tcid->ci_completion_id = atomic_inc_return(&rdma->sc_completion_ids);\n}\n\nstatic struct svc_rdma_recv_ctxt *\nsvc_rdma_recv_ctxt_alloc(struct svcxprt_rdma *rdma)\n{\n\tint node = ibdev_to_node(rdma->sc_cm_id->device);\n\tstruct svc_rdma_recv_ctxt *ctxt;\n\tdma_addr_t addr;\n\tvoid *buffer;\n\n\tctxt = kmalloc_node(sizeof(*ctxt), GFP_KERNEL, node);\n\tif (!ctxt)\n\t\tgoto fail0;\n\tbuffer = kmalloc_node(rdma->sc_max_req_size, GFP_KERNEL, node);\n\tif (!buffer)\n\t\tgoto fail1;\n\taddr = ib_dma_map_single(rdma->sc_pd->device, buffer,\n\t\t\t\t rdma->sc_max_req_size, DMA_FROM_DEVICE);\n\tif (ib_dma_mapping_error(rdma->sc_pd->device, addr))\n\t\tgoto fail2;\n\n\tsvc_rdma_recv_cid_init(rdma, &ctxt->rc_cid);\n\tpcl_init(&ctxt->rc_call_pcl);\n\tpcl_init(&ctxt->rc_read_pcl);\n\tpcl_init(&ctxt->rc_write_pcl);\n\tpcl_init(&ctxt->rc_reply_pcl);\n\n\tctxt->rc_recv_wr.next = NULL;\n\tctxt->rc_recv_wr.wr_cqe = &ctxt->rc_cqe;\n\tctxt->rc_recv_wr.sg_list = &ctxt->rc_recv_sge;\n\tctxt->rc_recv_wr.num_sge = 1;\n\tctxt->rc_cqe.done = svc_rdma_wc_receive;\n\tctxt->rc_recv_sge.addr = addr;\n\tctxt->rc_recv_sge.length = rdma->sc_max_req_size;\n\tctxt->rc_recv_sge.lkey = rdma->sc_pd->local_dma_lkey;\n\tctxt->rc_recv_buf = buffer;\n\treturn ctxt;\n\nfail2:\n\tkfree(buffer);\nfail1:\n\tkfree(ctxt);\nfail0:\n\treturn NULL;\n}\n\nstatic void svc_rdma_recv_ctxt_destroy(struct svcxprt_rdma *rdma,\n\t\t\t\t       struct svc_rdma_recv_ctxt *ctxt)\n{\n\tib_dma_unmap_single(rdma->sc_pd->device, ctxt->rc_recv_sge.addr,\n\t\t\t    ctxt->rc_recv_sge.length, DMA_FROM_DEVICE);\n\tkfree(ctxt->rc_recv_buf);\n\tkfree(ctxt);\n}\n\n \nvoid svc_rdma_recv_ctxts_destroy(struct svcxprt_rdma *rdma)\n{\n\tstruct svc_rdma_recv_ctxt *ctxt;\n\tstruct llist_node *node;\n\n\twhile ((node = llist_del_first(&rdma->sc_recv_ctxts))) {\n\t\tctxt = llist_entry(node, struct svc_rdma_recv_ctxt, rc_node);\n\t\tsvc_rdma_recv_ctxt_destroy(rdma, ctxt);\n\t}\n}\n\n \nstruct svc_rdma_recv_ctxt *svc_rdma_recv_ctxt_get(struct svcxprt_rdma *rdma)\n{\n\tstruct svc_rdma_recv_ctxt *ctxt;\n\tstruct llist_node *node;\n\n\tnode = llist_del_first(&rdma->sc_recv_ctxts);\n\tif (!node)\n\t\tgoto out_empty;\n\tctxt = llist_entry(node, struct svc_rdma_recv_ctxt, rc_node);\n\nout:\n\tctxt->rc_page_count = 0;\n\treturn ctxt;\n\nout_empty:\n\tctxt = svc_rdma_recv_ctxt_alloc(rdma);\n\tif (!ctxt)\n\t\treturn NULL;\n\tgoto out;\n}\n\n \nvoid svc_rdma_recv_ctxt_put(struct svcxprt_rdma *rdma,\n\t\t\t    struct svc_rdma_recv_ctxt *ctxt)\n{\n\tpcl_free(&ctxt->rc_call_pcl);\n\tpcl_free(&ctxt->rc_read_pcl);\n\tpcl_free(&ctxt->rc_write_pcl);\n\tpcl_free(&ctxt->rc_reply_pcl);\n\n\tllist_add(&ctxt->rc_node, &rdma->sc_recv_ctxts);\n}\n\n \nvoid svc_rdma_release_ctxt(struct svc_xprt *xprt, void *vctxt)\n{\n\tstruct svc_rdma_recv_ctxt *ctxt = vctxt;\n\tstruct svcxprt_rdma *rdma =\n\t\tcontainer_of(xprt, struct svcxprt_rdma, sc_xprt);\n\n\tif (ctxt)\n\t\tsvc_rdma_recv_ctxt_put(rdma, ctxt);\n}\n\nstatic bool svc_rdma_refresh_recvs(struct svcxprt_rdma *rdma,\n\t\t\t\t   unsigned int wanted)\n{\n\tconst struct ib_recv_wr *bad_wr = NULL;\n\tstruct svc_rdma_recv_ctxt *ctxt;\n\tstruct ib_recv_wr *recv_chain;\n\tint ret;\n\n\tif (test_bit(XPT_CLOSE, &rdma->sc_xprt.xpt_flags))\n\t\treturn false;\n\n\trecv_chain = NULL;\n\twhile (wanted--) {\n\t\tctxt = svc_rdma_recv_ctxt_get(rdma);\n\t\tif (!ctxt)\n\t\t\tbreak;\n\n\t\ttrace_svcrdma_post_recv(ctxt);\n\t\tctxt->rc_recv_wr.next = recv_chain;\n\t\trecv_chain = &ctxt->rc_recv_wr;\n\t\trdma->sc_pending_recvs++;\n\t}\n\tif (!recv_chain)\n\t\treturn false;\n\n\tret = ib_post_recv(rdma->sc_qp, recv_chain, &bad_wr);\n\tif (ret)\n\t\tgoto err_free;\n\treturn true;\n\nerr_free:\n\ttrace_svcrdma_rq_post_err(rdma, ret);\n\twhile (bad_wr) {\n\t\tctxt = container_of(bad_wr, struct svc_rdma_recv_ctxt,\n\t\t\t\t    rc_recv_wr);\n\t\tbad_wr = bad_wr->next;\n\t\tsvc_rdma_recv_ctxt_put(rdma, ctxt);\n\t}\n\t \n\treturn false;\n}\n\n \nbool svc_rdma_post_recvs(struct svcxprt_rdma *rdma)\n{\n\treturn svc_rdma_refresh_recvs(rdma, rdma->sc_max_requests);\n}\n\n \nstatic void svc_rdma_wc_receive(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct svcxprt_rdma *rdma = cq->cq_context;\n\tstruct ib_cqe *cqe = wc->wr_cqe;\n\tstruct svc_rdma_recv_ctxt *ctxt;\n\n\trdma->sc_pending_recvs--;\n\n\t \n\tctxt = container_of(cqe, struct svc_rdma_recv_ctxt, rc_cqe);\n\n\tif (wc->status != IB_WC_SUCCESS)\n\t\tgoto flushed;\n\ttrace_svcrdma_wc_recv(wc, &ctxt->rc_cid);\n\n\t \n\tif (rdma->sc_pending_recvs < rdma->sc_max_requests)\n\t\tif (!svc_rdma_refresh_recvs(rdma, rdma->sc_recv_batch))\n\t\t\tgoto dropped;\n\n\t \n\tctxt->rc_byte_len = wc->byte_len;\n\n\tspin_lock(&rdma->sc_rq_dto_lock);\n\tlist_add_tail(&ctxt->rc_list, &rdma->sc_rq_dto_q);\n\t \n\tset_bit(XPT_DATA, &rdma->sc_xprt.xpt_flags);\n\tspin_unlock(&rdma->sc_rq_dto_lock);\n\tif (!test_bit(RDMAXPRT_CONN_PENDING, &rdma->sc_flags))\n\t\tsvc_xprt_enqueue(&rdma->sc_xprt);\n\treturn;\n\nflushed:\n\tif (wc->status == IB_WC_WR_FLUSH_ERR)\n\t\ttrace_svcrdma_wc_recv_flush(wc, &ctxt->rc_cid);\n\telse\n\t\ttrace_svcrdma_wc_recv_err(wc, &ctxt->rc_cid);\ndropped:\n\tsvc_rdma_recv_ctxt_put(rdma, ctxt);\n\tsvc_xprt_deferred_close(&rdma->sc_xprt);\n}\n\n \nvoid svc_rdma_flush_recv_queues(struct svcxprt_rdma *rdma)\n{\n\tstruct svc_rdma_recv_ctxt *ctxt;\n\n\twhile ((ctxt = svc_rdma_next_recv_ctxt(&rdma->sc_rq_dto_q))) {\n\t\tlist_del(&ctxt->rc_list);\n\t\tsvc_rdma_recv_ctxt_put(rdma, ctxt);\n\t}\n}\n\nstatic void svc_rdma_build_arg_xdr(struct svc_rqst *rqstp,\n\t\t\t\t   struct svc_rdma_recv_ctxt *ctxt)\n{\n\tstruct xdr_buf *arg = &rqstp->rq_arg;\n\n\targ->head[0].iov_base = ctxt->rc_recv_buf;\n\targ->head[0].iov_len = ctxt->rc_byte_len;\n\targ->tail[0].iov_base = NULL;\n\targ->tail[0].iov_len = 0;\n\targ->page_len = 0;\n\targ->page_base = 0;\n\targ->buflen = ctxt->rc_byte_len;\n\targ->len = ctxt->rc_byte_len;\n}\n\n \nstatic bool xdr_count_read_segments(struct svc_rdma_recv_ctxt *rctxt, __be32 *p)\n{\n\trctxt->rc_call_pcl.cl_count = 0;\n\trctxt->rc_read_pcl.cl_count = 0;\n\twhile (xdr_item_is_present(p)) {\n\t\tu32 position, handle, length;\n\t\tu64 offset;\n\n\t\tp = xdr_inline_decode(&rctxt->rc_stream,\n\t\t\t\t      rpcrdma_readseg_maxsz * sizeof(*p));\n\t\tif (!p)\n\t\t\treturn false;\n\n\t\txdr_decode_read_segment(p, &position, &handle,\n\t\t\t\t\t    &length, &offset);\n\t\tif (position) {\n\t\t\tif (position & 3)\n\t\t\t\treturn false;\n\t\t\t++rctxt->rc_read_pcl.cl_count;\n\t\t} else {\n\t\t\t++rctxt->rc_call_pcl.cl_count;\n\t\t}\n\n\t\tp = xdr_inline_decode(&rctxt->rc_stream, sizeof(*p));\n\t\tif (!p)\n\t\t\treturn false;\n\t}\n\treturn true;\n}\n\n \nstatic bool xdr_check_read_list(struct svc_rdma_recv_ctxt *rctxt)\n{\n\t__be32 *p;\n\n\tp = xdr_inline_decode(&rctxt->rc_stream, sizeof(*p));\n\tif (!p)\n\t\treturn false;\n\tif (!xdr_count_read_segments(rctxt, p))\n\t\treturn false;\n\tif (!pcl_alloc_call(rctxt, p))\n\t\treturn false;\n\treturn pcl_alloc_read(rctxt, p);\n}\n\nstatic bool xdr_check_write_chunk(struct svc_rdma_recv_ctxt *rctxt)\n{\n\tu32 segcount;\n\t__be32 *p;\n\n\tif (xdr_stream_decode_u32(&rctxt->rc_stream, &segcount))\n\t\treturn false;\n\n\t \n\tp = xdr_inline_decode(&rctxt->rc_stream,\n\t\t\t      segcount * rpcrdma_segment_maxsz * sizeof(*p));\n\treturn p != NULL;\n}\n\n \nstatic bool xdr_count_write_chunks(struct svc_rdma_recv_ctxt *rctxt, __be32 *p)\n{\n\trctxt->rc_write_pcl.cl_count = 0;\n\twhile (xdr_item_is_present(p)) {\n\t\tif (!xdr_check_write_chunk(rctxt))\n\t\t\treturn false;\n\t\t++rctxt->rc_write_pcl.cl_count;\n\t\tp = xdr_inline_decode(&rctxt->rc_stream, sizeof(*p));\n\t\tif (!p)\n\t\t\treturn false;\n\t}\n\treturn true;\n}\n\n \nstatic bool xdr_check_write_list(struct svc_rdma_recv_ctxt *rctxt)\n{\n\t__be32 *p;\n\n\tp = xdr_inline_decode(&rctxt->rc_stream, sizeof(*p));\n\tif (!p)\n\t\treturn false;\n\tif (!xdr_count_write_chunks(rctxt, p))\n\t\treturn false;\n\tif (!pcl_alloc_write(rctxt, &rctxt->rc_write_pcl, p))\n\t\treturn false;\n\n\trctxt->rc_cur_result_payload = pcl_first_chunk(&rctxt->rc_write_pcl);\n\treturn true;\n}\n\n \nstatic bool xdr_check_reply_chunk(struct svc_rdma_recv_ctxt *rctxt)\n{\n\t__be32 *p;\n\n\tp = xdr_inline_decode(&rctxt->rc_stream, sizeof(*p));\n\tif (!p)\n\t\treturn false;\n\n\tif (!xdr_item_is_present(p))\n\t\treturn true;\n\tif (!xdr_check_write_chunk(rctxt))\n\t\treturn false;\n\n\trctxt->rc_reply_pcl.cl_count = 1;\n\treturn pcl_alloc_write(rctxt, &rctxt->rc_reply_pcl, p);\n}\n\n \nstatic void svc_rdma_get_inv_rkey(struct svcxprt_rdma *rdma,\n\t\t\t\t  struct svc_rdma_recv_ctxt *ctxt)\n{\n\tstruct svc_rdma_segment *segment;\n\tstruct svc_rdma_chunk *chunk;\n\tu32 inv_rkey;\n\n\tctxt->rc_inv_rkey = 0;\n\n\tif (!rdma->sc_snd_w_inv)\n\t\treturn;\n\n\tinv_rkey = 0;\n\tpcl_for_each_chunk(chunk, &ctxt->rc_call_pcl) {\n\t\tpcl_for_each_segment(segment, chunk) {\n\t\t\tif (inv_rkey == 0)\n\t\t\t\tinv_rkey = segment->rs_handle;\n\t\t\telse if (inv_rkey != segment->rs_handle)\n\t\t\t\treturn;\n\t\t}\n\t}\n\tpcl_for_each_chunk(chunk, &ctxt->rc_read_pcl) {\n\t\tpcl_for_each_segment(segment, chunk) {\n\t\t\tif (inv_rkey == 0)\n\t\t\t\tinv_rkey = segment->rs_handle;\n\t\t\telse if (inv_rkey != segment->rs_handle)\n\t\t\t\treturn;\n\t\t}\n\t}\n\tpcl_for_each_chunk(chunk, &ctxt->rc_write_pcl) {\n\t\tpcl_for_each_segment(segment, chunk) {\n\t\t\tif (inv_rkey == 0)\n\t\t\t\tinv_rkey = segment->rs_handle;\n\t\t\telse if (inv_rkey != segment->rs_handle)\n\t\t\t\treturn;\n\t\t}\n\t}\n\tpcl_for_each_chunk(chunk, &ctxt->rc_reply_pcl) {\n\t\tpcl_for_each_segment(segment, chunk) {\n\t\t\tif (inv_rkey == 0)\n\t\t\t\tinv_rkey = segment->rs_handle;\n\t\t\telse if (inv_rkey != segment->rs_handle)\n\t\t\t\treturn;\n\t\t}\n\t}\n\tctxt->rc_inv_rkey = inv_rkey;\n}\n\n \nstatic int svc_rdma_xdr_decode_req(struct xdr_buf *rq_arg,\n\t\t\t\t   struct svc_rdma_recv_ctxt *rctxt)\n{\n\t__be32 *p, *rdma_argp;\n\tunsigned int hdr_len;\n\n\trdma_argp = rq_arg->head[0].iov_base;\n\txdr_init_decode(&rctxt->rc_stream, rq_arg, rdma_argp, NULL);\n\n\tp = xdr_inline_decode(&rctxt->rc_stream,\n\t\t\t      rpcrdma_fixed_maxsz * sizeof(*p));\n\tif (unlikely(!p))\n\t\tgoto out_short;\n\tp++;\n\tif (*p != rpcrdma_version)\n\t\tgoto out_version;\n\tp += 2;\n\trctxt->rc_msgtype = *p;\n\tswitch (rctxt->rc_msgtype) {\n\tcase rdma_msg:\n\t\tbreak;\n\tcase rdma_nomsg:\n\t\tbreak;\n\tcase rdma_done:\n\t\tgoto out_drop;\n\tcase rdma_error:\n\t\tgoto out_drop;\n\tdefault:\n\t\tgoto out_proc;\n\t}\n\n\tif (!xdr_check_read_list(rctxt))\n\t\tgoto out_inval;\n\tif (!xdr_check_write_list(rctxt))\n\t\tgoto out_inval;\n\tif (!xdr_check_reply_chunk(rctxt))\n\t\tgoto out_inval;\n\n\trq_arg->head[0].iov_base = rctxt->rc_stream.p;\n\thdr_len = xdr_stream_pos(&rctxt->rc_stream);\n\trq_arg->head[0].iov_len -= hdr_len;\n\trq_arg->len -= hdr_len;\n\ttrace_svcrdma_decode_rqst(rctxt, rdma_argp, hdr_len);\n\treturn hdr_len;\n\nout_short:\n\ttrace_svcrdma_decode_short_err(rctxt, rq_arg->len);\n\treturn -EINVAL;\n\nout_version:\n\ttrace_svcrdma_decode_badvers_err(rctxt, rdma_argp);\n\treturn -EPROTONOSUPPORT;\n\nout_drop:\n\ttrace_svcrdma_decode_drop_err(rctxt, rdma_argp);\n\treturn 0;\n\nout_proc:\n\ttrace_svcrdma_decode_badproc_err(rctxt, rdma_argp);\n\treturn -EINVAL;\n\nout_inval:\n\ttrace_svcrdma_decode_parse_err(rctxt, rdma_argp);\n\treturn -EINVAL;\n}\n\nstatic void svc_rdma_send_error(struct svcxprt_rdma *rdma,\n\t\t\t\tstruct svc_rdma_recv_ctxt *rctxt,\n\t\t\t\tint status)\n{\n\tstruct svc_rdma_send_ctxt *sctxt;\n\n\tsctxt = svc_rdma_send_ctxt_get(rdma);\n\tif (!sctxt)\n\t\treturn;\n\tsvc_rdma_send_error_msg(rdma, sctxt, rctxt, status);\n}\n\n \nstatic bool svc_rdma_is_reverse_direction_reply(struct svc_xprt *xprt,\n\t\t\t\t\t\tstruct svc_rdma_recv_ctxt *rctxt)\n{\n\t__be32 *p = rctxt->rc_recv_buf;\n\n\tif (!xprt->xpt_bc_xprt)\n\t\treturn false;\n\n\tif (rctxt->rc_msgtype != rdma_msg)\n\t\treturn false;\n\n\tif (!pcl_is_empty(&rctxt->rc_call_pcl))\n\t\treturn false;\n\tif (!pcl_is_empty(&rctxt->rc_read_pcl))\n\t\treturn false;\n\tif (!pcl_is_empty(&rctxt->rc_write_pcl))\n\t\treturn false;\n\tif (!pcl_is_empty(&rctxt->rc_reply_pcl))\n\t\treturn false;\n\n\t \n\tif (*(p + 8) == cpu_to_be32(RPC_CALL))\n\t\treturn false;\n\n\treturn true;\n}\n\n \nint svc_rdma_recvfrom(struct svc_rqst *rqstp)\n{\n\tstruct svc_xprt *xprt = rqstp->rq_xprt;\n\tstruct svcxprt_rdma *rdma_xprt =\n\t\tcontainer_of(xprt, struct svcxprt_rdma, sc_xprt);\n\tstruct svc_rdma_recv_ctxt *ctxt;\n\tint ret;\n\n\t \n\trqstp->rq_respages = rqstp->rq_pages;\n\trqstp->rq_next_page = rqstp->rq_respages;\n\n\trqstp->rq_xprt_ctxt = NULL;\n\n\tctxt = NULL;\n\tspin_lock(&rdma_xprt->sc_rq_dto_lock);\n\tctxt = svc_rdma_next_recv_ctxt(&rdma_xprt->sc_rq_dto_q);\n\tif (ctxt)\n\t\tlist_del(&ctxt->rc_list);\n\telse\n\t\t \n\t\tclear_bit(XPT_DATA, &xprt->xpt_flags);\n\tspin_unlock(&rdma_xprt->sc_rq_dto_lock);\n\n\t \n\tsvc_xprt_received(xprt);\n\tif (!ctxt)\n\t\treturn 0;\n\n\tpercpu_counter_inc(&svcrdma_stat_recv);\n\tib_dma_sync_single_for_cpu(rdma_xprt->sc_pd->device,\n\t\t\t\t   ctxt->rc_recv_sge.addr, ctxt->rc_byte_len,\n\t\t\t\t   DMA_FROM_DEVICE);\n\tsvc_rdma_build_arg_xdr(rqstp, ctxt);\n\n\tret = svc_rdma_xdr_decode_req(&rqstp->rq_arg, ctxt);\n\tif (ret < 0)\n\t\tgoto out_err;\n\tif (ret == 0)\n\t\tgoto out_drop;\n\n\tif (svc_rdma_is_reverse_direction_reply(xprt, ctxt))\n\t\tgoto out_backchannel;\n\n\tsvc_rdma_get_inv_rkey(rdma_xprt, ctxt);\n\n\tif (!pcl_is_empty(&ctxt->rc_read_pcl) ||\n\t    !pcl_is_empty(&ctxt->rc_call_pcl)) {\n\t\tret = svc_rdma_process_read_list(rdma_xprt, rqstp, ctxt);\n\t\tif (ret < 0)\n\t\t\tgoto out_readfail;\n\t}\n\n\trqstp->rq_xprt_ctxt = ctxt;\n\trqstp->rq_prot = IPPROTO_MAX;\n\tsvc_xprt_copy_addrs(rqstp, xprt);\n\tset_bit(RQ_SECURE, &rqstp->rq_flags);\n\treturn rqstp->rq_arg.len;\n\nout_err:\n\tsvc_rdma_send_error(rdma_xprt, ctxt, ret);\n\tsvc_rdma_recv_ctxt_put(rdma_xprt, ctxt);\n\treturn 0;\n\nout_readfail:\n\tif (ret == -EINVAL)\n\t\tsvc_rdma_send_error(rdma_xprt, ctxt, ret);\n\tsvc_rdma_recv_ctxt_put(rdma_xprt, ctxt);\n\tsvc_xprt_deferred_close(xprt);\n\treturn -ENOTCONN;\n\nout_backchannel:\n\tsvc_rdma_handle_bc_reply(rqstp, ctxt);\nout_drop:\n\tsvc_rdma_recv_ctxt_put(rdma_xprt, ctxt);\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}