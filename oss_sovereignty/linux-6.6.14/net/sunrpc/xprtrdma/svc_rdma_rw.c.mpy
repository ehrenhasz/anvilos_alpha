{
  "module_name": "svc_rdma_rw.c",
  "hash_id": "5e6d3a839580aa9660ca319bae19aa94ceefeb2e5e95154d25a804e75a5a4065",
  "original_prompt": "Ingested from linux-6.6.14/net/sunrpc/xprtrdma/svc_rdma_rw.c",
  "human_readable_source": "\n \n\n#include <rdma/rw.h>\n\n#include <linux/sunrpc/xdr.h>\n#include <linux/sunrpc/rpc_rdma.h>\n#include <linux/sunrpc/svc_rdma.h>\n\n#include \"xprt_rdma.h\"\n#include <trace/events/rpcrdma.h>\n\nstatic void svc_rdma_write_done(struct ib_cq *cq, struct ib_wc *wc);\nstatic void svc_rdma_wc_read_done(struct ib_cq *cq, struct ib_wc *wc);\n\n \nstruct svc_rdma_rw_ctxt {\n\tstruct llist_node\trw_node;\n\tstruct list_head\trw_list;\n\tstruct rdma_rw_ctx\trw_ctx;\n\tunsigned int\t\trw_nents;\n\tstruct sg_table\t\trw_sg_table;\n\tstruct scatterlist\trw_first_sgl[];\n};\n\nstatic inline struct svc_rdma_rw_ctxt *\nsvc_rdma_next_ctxt(struct list_head *list)\n{\n\treturn list_first_entry_or_null(list, struct svc_rdma_rw_ctxt,\n\t\t\t\t\trw_list);\n}\n\nstatic struct svc_rdma_rw_ctxt *\nsvc_rdma_get_rw_ctxt(struct svcxprt_rdma *rdma, unsigned int sges)\n{\n\tstruct svc_rdma_rw_ctxt *ctxt;\n\tstruct llist_node *node;\n\n\tspin_lock(&rdma->sc_rw_ctxt_lock);\n\tnode = llist_del_first(&rdma->sc_rw_ctxts);\n\tspin_unlock(&rdma->sc_rw_ctxt_lock);\n\tif (node) {\n\t\tctxt = llist_entry(node, struct svc_rdma_rw_ctxt, rw_node);\n\t} else {\n\t\tctxt = kmalloc_node(struct_size(ctxt, rw_first_sgl, SG_CHUNK_SIZE),\n\t\t\t\t    GFP_KERNEL, ibdev_to_node(rdma->sc_cm_id->device));\n\t\tif (!ctxt)\n\t\t\tgoto out_noctx;\n\n\t\tINIT_LIST_HEAD(&ctxt->rw_list);\n\t}\n\n\tctxt->rw_sg_table.sgl = ctxt->rw_first_sgl;\n\tif (sg_alloc_table_chained(&ctxt->rw_sg_table, sges,\n\t\t\t\t   ctxt->rw_sg_table.sgl,\n\t\t\t\t   SG_CHUNK_SIZE))\n\t\tgoto out_free;\n\treturn ctxt;\n\nout_free:\n\tkfree(ctxt);\nout_noctx:\n\ttrace_svcrdma_no_rwctx_err(rdma, sges);\n\treturn NULL;\n}\n\nstatic void __svc_rdma_put_rw_ctxt(struct svc_rdma_rw_ctxt *ctxt,\n\t\t\t\t   struct llist_head *list)\n{\n\tsg_free_table_chained(&ctxt->rw_sg_table, SG_CHUNK_SIZE);\n\tllist_add(&ctxt->rw_node, list);\n}\n\nstatic void svc_rdma_put_rw_ctxt(struct svcxprt_rdma *rdma,\n\t\t\t\t struct svc_rdma_rw_ctxt *ctxt)\n{\n\t__svc_rdma_put_rw_ctxt(ctxt, &rdma->sc_rw_ctxts);\n}\n\n \nvoid svc_rdma_destroy_rw_ctxts(struct svcxprt_rdma *rdma)\n{\n\tstruct svc_rdma_rw_ctxt *ctxt;\n\tstruct llist_node *node;\n\n\twhile ((node = llist_del_first(&rdma->sc_rw_ctxts)) != NULL) {\n\t\tctxt = llist_entry(node, struct svc_rdma_rw_ctxt, rw_node);\n\t\tkfree(ctxt);\n\t}\n}\n\n \nstatic int svc_rdma_rw_ctx_init(struct svcxprt_rdma *rdma,\n\t\t\t\tstruct svc_rdma_rw_ctxt *ctxt,\n\t\t\t\tu64 offset, u32 handle,\n\t\t\t\tenum dma_data_direction direction)\n{\n\tint ret;\n\n\tret = rdma_rw_ctx_init(&ctxt->rw_ctx, rdma->sc_qp, rdma->sc_port_num,\n\t\t\t       ctxt->rw_sg_table.sgl, ctxt->rw_nents,\n\t\t\t       0, offset, handle, direction);\n\tif (unlikely(ret < 0)) {\n\t\tsvc_rdma_put_rw_ctxt(rdma, ctxt);\n\t\ttrace_svcrdma_dma_map_rw_err(rdma, ctxt->rw_nents, ret);\n\t}\n\treturn ret;\n}\n\n \nstruct svc_rdma_chunk_ctxt {\n\tstruct rpc_rdma_cid\tcc_cid;\n\tstruct ib_cqe\t\tcc_cqe;\n\tstruct svcxprt_rdma\t*cc_rdma;\n\tstruct list_head\tcc_rwctxts;\n\tktime_t\t\t\tcc_posttime;\n\tint\t\t\tcc_sqecount;\n\tenum ib_wc_status\tcc_status;\n\tstruct completion\tcc_done;\n};\n\nstatic void svc_rdma_cc_cid_init(struct svcxprt_rdma *rdma,\n\t\t\t\t struct rpc_rdma_cid *cid)\n{\n\tcid->ci_queue_id = rdma->sc_sq_cq->res.id;\n\tcid->ci_completion_id = atomic_inc_return(&rdma->sc_completion_ids);\n}\n\nstatic void svc_rdma_cc_init(struct svcxprt_rdma *rdma,\n\t\t\t     struct svc_rdma_chunk_ctxt *cc)\n{\n\tsvc_rdma_cc_cid_init(rdma, &cc->cc_cid);\n\tcc->cc_rdma = rdma;\n\n\tINIT_LIST_HEAD(&cc->cc_rwctxts);\n\tcc->cc_sqecount = 0;\n}\n\n \nstatic void svc_rdma_cc_release(struct svc_rdma_chunk_ctxt *cc,\n\t\t\t\tenum dma_data_direction dir)\n{\n\tstruct svcxprt_rdma *rdma = cc->cc_rdma;\n\tstruct llist_node *first, *last;\n\tstruct svc_rdma_rw_ctxt *ctxt;\n\tLLIST_HEAD(free);\n\n\ttrace_svcrdma_cc_release(&cc->cc_cid, cc->cc_sqecount);\n\n\tfirst = last = NULL;\n\twhile ((ctxt = svc_rdma_next_ctxt(&cc->cc_rwctxts)) != NULL) {\n\t\tlist_del(&ctxt->rw_list);\n\n\t\trdma_rw_ctx_destroy(&ctxt->rw_ctx, rdma->sc_qp,\n\t\t\t\t    rdma->sc_port_num, ctxt->rw_sg_table.sgl,\n\t\t\t\t    ctxt->rw_nents, dir);\n\t\t__svc_rdma_put_rw_ctxt(ctxt, &free);\n\n\t\tctxt->rw_node.next = first;\n\t\tfirst = &ctxt->rw_node;\n\t\tif (!last)\n\t\t\tlast = first;\n\t}\n\tif (first)\n\t\tllist_add_batch(first, last, &rdma->sc_rw_ctxts);\n}\n\n \nstruct svc_rdma_write_info {\n\tconst struct svc_rdma_chunk\t*wi_chunk;\n\n\t \n\tunsigned int\t\twi_seg_off;\n\tunsigned int\t\twi_seg_no;\n\n\t \n\tconst struct xdr_buf\t*wi_xdr;\n\tunsigned char\t\t*wi_base;\n\tunsigned int\t\twi_next_off;\n\n\tstruct svc_rdma_chunk_ctxt\twi_cc;\n};\n\nstatic struct svc_rdma_write_info *\nsvc_rdma_write_info_alloc(struct svcxprt_rdma *rdma,\n\t\t\t  const struct svc_rdma_chunk *chunk)\n{\n\tstruct svc_rdma_write_info *info;\n\n\tinfo = kmalloc_node(sizeof(*info), GFP_KERNEL,\n\t\t\t    ibdev_to_node(rdma->sc_cm_id->device));\n\tif (!info)\n\t\treturn info;\n\n\tinfo->wi_chunk = chunk;\n\tinfo->wi_seg_off = 0;\n\tinfo->wi_seg_no = 0;\n\tsvc_rdma_cc_init(rdma, &info->wi_cc);\n\tinfo->wi_cc.cc_cqe.done = svc_rdma_write_done;\n\treturn info;\n}\n\nstatic void svc_rdma_write_info_free(struct svc_rdma_write_info *info)\n{\n\tsvc_rdma_cc_release(&info->wi_cc, DMA_TO_DEVICE);\n\tkfree(info);\n}\n\n \nstatic void svc_rdma_write_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct ib_cqe *cqe = wc->wr_cqe;\n\tstruct svc_rdma_chunk_ctxt *cc =\n\t\t\tcontainer_of(cqe, struct svc_rdma_chunk_ctxt, cc_cqe);\n\tstruct svcxprt_rdma *rdma = cc->cc_rdma;\n\tstruct svc_rdma_write_info *info =\n\t\t\tcontainer_of(cc, struct svc_rdma_write_info, wi_cc);\n\n\tswitch (wc->status) {\n\tcase IB_WC_SUCCESS:\n\t\ttrace_svcrdma_wc_write(wc, &cc->cc_cid);\n\t\tbreak;\n\tcase IB_WC_WR_FLUSH_ERR:\n\t\ttrace_svcrdma_wc_write_flush(wc, &cc->cc_cid);\n\t\tbreak;\n\tdefault:\n\t\ttrace_svcrdma_wc_write_err(wc, &cc->cc_cid);\n\t}\n\n\tsvc_rdma_wake_send_waiters(rdma, cc->cc_sqecount);\n\n\tif (unlikely(wc->status != IB_WC_SUCCESS))\n\t\tsvc_xprt_deferred_close(&rdma->sc_xprt);\n\n\tsvc_rdma_write_info_free(info);\n}\n\n \nstruct svc_rdma_read_info {\n\tstruct svc_rqst\t\t\t*ri_rqst;\n\tstruct svc_rdma_recv_ctxt\t*ri_readctxt;\n\tunsigned int\t\t\tri_pageno;\n\tunsigned int\t\t\tri_pageoff;\n\tunsigned int\t\t\tri_totalbytes;\n\n\tstruct svc_rdma_chunk_ctxt\tri_cc;\n};\n\nstatic struct svc_rdma_read_info *\nsvc_rdma_read_info_alloc(struct svcxprt_rdma *rdma)\n{\n\tstruct svc_rdma_read_info *info;\n\n\tinfo = kmalloc_node(sizeof(*info), GFP_KERNEL,\n\t\t\t    ibdev_to_node(rdma->sc_cm_id->device));\n\tif (!info)\n\t\treturn info;\n\n\tsvc_rdma_cc_init(rdma, &info->ri_cc);\n\tinfo->ri_cc.cc_cqe.done = svc_rdma_wc_read_done;\n\treturn info;\n}\n\nstatic void svc_rdma_read_info_free(struct svc_rdma_read_info *info)\n{\n\tsvc_rdma_cc_release(&info->ri_cc, DMA_FROM_DEVICE);\n\tkfree(info);\n}\n\n \nstatic void svc_rdma_wc_read_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct ib_cqe *cqe = wc->wr_cqe;\n\tstruct svc_rdma_chunk_ctxt *cc =\n\t\t\tcontainer_of(cqe, struct svc_rdma_chunk_ctxt, cc_cqe);\n\tstruct svc_rdma_read_info *info;\n\n\tswitch (wc->status) {\n\tcase IB_WC_SUCCESS:\n\t\tinfo = container_of(cc, struct svc_rdma_read_info, ri_cc);\n\t\ttrace_svcrdma_wc_read(wc, &cc->cc_cid, info->ri_totalbytes,\n\t\t\t\t      cc->cc_posttime);\n\t\tbreak;\n\tcase IB_WC_WR_FLUSH_ERR:\n\t\ttrace_svcrdma_wc_read_flush(wc, &cc->cc_cid);\n\t\tbreak;\n\tdefault:\n\t\ttrace_svcrdma_wc_read_err(wc, &cc->cc_cid);\n\t}\n\n\tsvc_rdma_wake_send_waiters(cc->cc_rdma, cc->cc_sqecount);\n\tcc->cc_status = wc->status;\n\tcomplete(&cc->cc_done);\n\treturn;\n}\n\n \nstatic int svc_rdma_post_chunk_ctxt(struct svc_rdma_chunk_ctxt *cc)\n{\n\tstruct svcxprt_rdma *rdma = cc->cc_rdma;\n\tstruct ib_send_wr *first_wr;\n\tconst struct ib_send_wr *bad_wr;\n\tstruct list_head *tmp;\n\tstruct ib_cqe *cqe;\n\tint ret;\n\n\tmight_sleep();\n\n\tif (cc->cc_sqecount > rdma->sc_sq_depth)\n\t\treturn -EINVAL;\n\n\tfirst_wr = NULL;\n\tcqe = &cc->cc_cqe;\n\tlist_for_each(tmp, &cc->cc_rwctxts) {\n\t\tstruct svc_rdma_rw_ctxt *ctxt;\n\n\t\tctxt = list_entry(tmp, struct svc_rdma_rw_ctxt, rw_list);\n\t\tfirst_wr = rdma_rw_ctx_wrs(&ctxt->rw_ctx, rdma->sc_qp,\n\t\t\t\t\t   rdma->sc_port_num, cqe, first_wr);\n\t\tcqe = NULL;\n\t}\n\n\tdo {\n\t\tif (atomic_sub_return(cc->cc_sqecount,\n\t\t\t\t      &rdma->sc_sq_avail) > 0) {\n\t\t\tcc->cc_posttime = ktime_get();\n\t\t\tret = ib_post_send(rdma->sc_qp, first_wr, &bad_wr);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t\treturn 0;\n\t\t}\n\n\t\tpercpu_counter_inc(&svcrdma_stat_sq_starve);\n\t\ttrace_svcrdma_sq_full(rdma);\n\t\tatomic_add(cc->cc_sqecount, &rdma->sc_sq_avail);\n\t\twait_event(rdma->sc_send_wait,\n\t\t\t   atomic_read(&rdma->sc_sq_avail) > cc->cc_sqecount);\n\t\ttrace_svcrdma_sq_retry(rdma);\n\t} while (1);\n\n\ttrace_svcrdma_sq_post_err(rdma, ret);\n\tsvc_xprt_deferred_close(&rdma->sc_xprt);\n\n\t \n\tif (bad_wr != first_wr)\n\t\treturn 0;\n\n\tatomic_add(cc->cc_sqecount, &rdma->sc_sq_avail);\n\twake_up(&rdma->sc_send_wait);\n\treturn -ENOTCONN;\n}\n\n \nstatic void svc_rdma_vec_to_sg(struct svc_rdma_write_info *info,\n\t\t\t       unsigned int len,\n\t\t\t       struct svc_rdma_rw_ctxt *ctxt)\n{\n\tstruct scatterlist *sg = ctxt->rw_sg_table.sgl;\n\n\tsg_set_buf(&sg[0], info->wi_base, len);\n\tinfo->wi_base += len;\n\n\tctxt->rw_nents = 1;\n}\n\n \nstatic void svc_rdma_pagelist_to_sg(struct svc_rdma_write_info *info,\n\t\t\t\t    unsigned int remaining,\n\t\t\t\t    struct svc_rdma_rw_ctxt *ctxt)\n{\n\tunsigned int sge_no, sge_bytes, page_off, page_no;\n\tconst struct xdr_buf *xdr = info->wi_xdr;\n\tstruct scatterlist *sg;\n\tstruct page **page;\n\n\tpage_off = info->wi_next_off + xdr->page_base;\n\tpage_no = page_off >> PAGE_SHIFT;\n\tpage_off = offset_in_page(page_off);\n\tpage = xdr->pages + page_no;\n\tinfo->wi_next_off += remaining;\n\tsg = ctxt->rw_sg_table.sgl;\n\tsge_no = 0;\n\tdo {\n\t\tsge_bytes = min_t(unsigned int, remaining,\n\t\t\t\t  PAGE_SIZE - page_off);\n\t\tsg_set_page(sg, *page, sge_bytes, page_off);\n\n\t\tremaining -= sge_bytes;\n\t\tsg = sg_next(sg);\n\t\tpage_off = 0;\n\t\tsge_no++;\n\t\tpage++;\n\t} while (remaining);\n\n\tctxt->rw_nents = sge_no;\n}\n\n \nstatic int\nsvc_rdma_build_writes(struct svc_rdma_write_info *info,\n\t\t      void (*constructor)(struct svc_rdma_write_info *info,\n\t\t\t\t\t  unsigned int len,\n\t\t\t\t\t  struct svc_rdma_rw_ctxt *ctxt),\n\t\t      unsigned int remaining)\n{\n\tstruct svc_rdma_chunk_ctxt *cc = &info->wi_cc;\n\tstruct svcxprt_rdma *rdma = cc->cc_rdma;\n\tconst struct svc_rdma_segment *seg;\n\tstruct svc_rdma_rw_ctxt *ctxt;\n\tint ret;\n\n\tdo {\n\t\tunsigned int write_len;\n\t\tu64 offset;\n\n\t\tif (info->wi_seg_no >= info->wi_chunk->ch_segcount)\n\t\t\tgoto out_overflow;\n\n\t\tseg = &info->wi_chunk->ch_segments[info->wi_seg_no];\n\t\twrite_len = min(remaining, seg->rs_length - info->wi_seg_off);\n\t\tif (!write_len)\n\t\t\tgoto out_overflow;\n\t\tctxt = svc_rdma_get_rw_ctxt(rdma,\n\t\t\t\t\t    (write_len >> PAGE_SHIFT) + 2);\n\t\tif (!ctxt)\n\t\t\treturn -ENOMEM;\n\n\t\tconstructor(info, write_len, ctxt);\n\t\toffset = seg->rs_offset + info->wi_seg_off;\n\t\tret = svc_rdma_rw_ctx_init(rdma, ctxt, offset, seg->rs_handle,\n\t\t\t\t\t   DMA_TO_DEVICE);\n\t\tif (ret < 0)\n\t\t\treturn -EIO;\n\t\tpercpu_counter_inc(&svcrdma_stat_write);\n\n\t\tlist_add(&ctxt->rw_list, &cc->cc_rwctxts);\n\t\tcc->cc_sqecount += ret;\n\t\tif (write_len == seg->rs_length - info->wi_seg_off) {\n\t\t\tinfo->wi_seg_no++;\n\t\t\tinfo->wi_seg_off = 0;\n\t\t} else {\n\t\t\tinfo->wi_seg_off += write_len;\n\t\t}\n\t\tremaining -= write_len;\n\t} while (remaining);\n\n\treturn 0;\n\nout_overflow:\n\ttrace_svcrdma_small_wrch_err(rdma, remaining, info->wi_seg_no,\n\t\t\t\t     info->wi_chunk->ch_segcount);\n\treturn -E2BIG;\n}\n\n \nstatic int svc_rdma_iov_write(struct svc_rdma_write_info *info,\n\t\t\t      const struct kvec *iov)\n{\n\tinfo->wi_base = iov->iov_base;\n\treturn svc_rdma_build_writes(info, svc_rdma_vec_to_sg,\n\t\t\t\t     iov->iov_len);\n}\n\n \nstatic int svc_rdma_pages_write(struct svc_rdma_write_info *info,\n\t\t\t\tconst struct xdr_buf *xdr,\n\t\t\t\tunsigned int offset,\n\t\t\t\tunsigned long length)\n{\n\tinfo->wi_xdr = xdr;\n\tinfo->wi_next_off = offset - xdr->head[0].iov_len;\n\treturn svc_rdma_build_writes(info, svc_rdma_pagelist_to_sg,\n\t\t\t\t     length);\n}\n\n \nstatic int svc_rdma_xb_write(const struct xdr_buf *xdr, void *data)\n{\n\tstruct svc_rdma_write_info *info = data;\n\tint ret;\n\n\tif (xdr->head[0].iov_len) {\n\t\tret = svc_rdma_iov_write(info, &xdr->head[0]);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\tif (xdr->page_len) {\n\t\tret = svc_rdma_pages_write(info, xdr, xdr->head[0].iov_len,\n\t\t\t\t\t   xdr->page_len);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\tif (xdr->tail[0].iov_len) {\n\t\tret = svc_rdma_iov_write(info, &xdr->tail[0]);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\treturn xdr->len;\n}\n\n \nint svc_rdma_send_write_chunk(struct svcxprt_rdma *rdma,\n\t\t\t      const struct svc_rdma_chunk *chunk,\n\t\t\t      const struct xdr_buf *xdr)\n{\n\tstruct svc_rdma_write_info *info;\n\tstruct svc_rdma_chunk_ctxt *cc;\n\tint ret;\n\n\tinfo = svc_rdma_write_info_alloc(rdma, chunk);\n\tif (!info)\n\t\treturn -ENOMEM;\n\tcc = &info->wi_cc;\n\n\tret = svc_rdma_xb_write(xdr, info);\n\tif (ret != xdr->len)\n\t\tgoto out_err;\n\n\ttrace_svcrdma_post_write_chunk(&cc->cc_cid, cc->cc_sqecount);\n\tret = svc_rdma_post_chunk_ctxt(cc);\n\tif (ret < 0)\n\t\tgoto out_err;\n\treturn xdr->len;\n\nout_err:\n\tsvc_rdma_write_info_free(info);\n\treturn ret;\n}\n\n \nint svc_rdma_send_reply_chunk(struct svcxprt_rdma *rdma,\n\t\t\t      const struct svc_rdma_recv_ctxt *rctxt,\n\t\t\t      const struct xdr_buf *xdr)\n{\n\tstruct svc_rdma_write_info *info;\n\tstruct svc_rdma_chunk_ctxt *cc;\n\tstruct svc_rdma_chunk *chunk;\n\tint ret;\n\n\tif (pcl_is_empty(&rctxt->rc_reply_pcl))\n\t\treturn 0;\n\n\tchunk = pcl_first_chunk(&rctxt->rc_reply_pcl);\n\tinfo = svc_rdma_write_info_alloc(rdma, chunk);\n\tif (!info)\n\t\treturn -ENOMEM;\n\tcc = &info->wi_cc;\n\n\tret = pcl_process_nonpayloads(&rctxt->rc_write_pcl, xdr,\n\t\t\t\t      svc_rdma_xb_write, info);\n\tif (ret < 0)\n\t\tgoto out_err;\n\n\ttrace_svcrdma_post_reply_chunk(&cc->cc_cid, cc->cc_sqecount);\n\tret = svc_rdma_post_chunk_ctxt(cc);\n\tif (ret < 0)\n\t\tgoto out_err;\n\n\treturn xdr->len;\n\nout_err:\n\tsvc_rdma_write_info_free(info);\n\treturn ret;\n}\n\n \nstatic int svc_rdma_build_read_segment(struct svc_rdma_read_info *info,\n\t\t\t\t       const struct svc_rdma_segment *segment)\n{\n\tstruct svc_rdma_recv_ctxt *head = info->ri_readctxt;\n\tstruct svc_rdma_chunk_ctxt *cc = &info->ri_cc;\n\tstruct svc_rqst *rqstp = info->ri_rqst;\n\tunsigned int sge_no, seg_len, len;\n\tstruct svc_rdma_rw_ctxt *ctxt;\n\tstruct scatterlist *sg;\n\tint ret;\n\n\tlen = segment->rs_length;\n\tsge_no = PAGE_ALIGN(info->ri_pageoff + len) >> PAGE_SHIFT;\n\tctxt = svc_rdma_get_rw_ctxt(cc->cc_rdma, sge_no);\n\tif (!ctxt)\n\t\treturn -ENOMEM;\n\tctxt->rw_nents = sge_no;\n\n\tsg = ctxt->rw_sg_table.sgl;\n\tfor (sge_no = 0; sge_no < ctxt->rw_nents; sge_no++) {\n\t\tseg_len = min_t(unsigned int, len,\n\t\t\t\tPAGE_SIZE - info->ri_pageoff);\n\n\t\tif (!info->ri_pageoff)\n\t\t\thead->rc_page_count++;\n\n\t\tsg_set_page(sg, rqstp->rq_pages[info->ri_pageno],\n\t\t\t    seg_len, info->ri_pageoff);\n\t\tsg = sg_next(sg);\n\n\t\tinfo->ri_pageoff += seg_len;\n\t\tif (info->ri_pageoff == PAGE_SIZE) {\n\t\t\tinfo->ri_pageno++;\n\t\t\tinfo->ri_pageoff = 0;\n\t\t}\n\t\tlen -= seg_len;\n\n\t\t \n\t\tif (len &&\n\t\t    &rqstp->rq_pages[info->ri_pageno + 1] > rqstp->rq_page_end)\n\t\t\tgoto out_overrun;\n\t}\n\n\tret = svc_rdma_rw_ctx_init(cc->cc_rdma, ctxt, segment->rs_offset,\n\t\t\t\t   segment->rs_handle, DMA_FROM_DEVICE);\n\tif (ret < 0)\n\t\treturn -EIO;\n\tpercpu_counter_inc(&svcrdma_stat_read);\n\n\tlist_add(&ctxt->rw_list, &cc->cc_rwctxts);\n\tcc->cc_sqecount += ret;\n\treturn 0;\n\nout_overrun:\n\ttrace_svcrdma_page_overrun_err(cc->cc_rdma, rqstp, info->ri_pageno);\n\treturn -EINVAL;\n}\n\n \nstatic int svc_rdma_build_read_chunk(struct svc_rdma_read_info *info,\n\t\t\t\t     const struct svc_rdma_chunk *chunk)\n{\n\tconst struct svc_rdma_segment *segment;\n\tint ret;\n\n\tret = -EINVAL;\n\tpcl_for_each_segment(segment, chunk) {\n\t\tret = svc_rdma_build_read_segment(info, segment);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\t\tinfo->ri_totalbytes += segment->rs_length;\n\t}\n\treturn ret;\n}\n\n \nstatic int svc_rdma_copy_inline_range(struct svc_rdma_read_info *info,\n\t\t\t\t      unsigned int offset,\n\t\t\t\t      unsigned int remaining)\n{\n\tstruct svc_rdma_recv_ctxt *head = info->ri_readctxt;\n\tunsigned char *dst, *src = head->rc_recv_buf;\n\tstruct svc_rqst *rqstp = info->ri_rqst;\n\tunsigned int page_no, numpages;\n\n\tnumpages = PAGE_ALIGN(info->ri_pageoff + remaining) >> PAGE_SHIFT;\n\tfor (page_no = 0; page_no < numpages; page_no++) {\n\t\tunsigned int page_len;\n\n\t\tpage_len = min_t(unsigned int, remaining,\n\t\t\t\t PAGE_SIZE - info->ri_pageoff);\n\n\t\tif (!info->ri_pageoff)\n\t\t\thead->rc_page_count++;\n\n\t\tdst = page_address(rqstp->rq_pages[info->ri_pageno]);\n\t\tmemcpy(dst + info->ri_pageno, src + offset, page_len);\n\n\t\tinfo->ri_totalbytes += page_len;\n\t\tinfo->ri_pageoff += page_len;\n\t\tif (info->ri_pageoff == PAGE_SIZE) {\n\t\t\tinfo->ri_pageno++;\n\t\t\tinfo->ri_pageoff = 0;\n\t\t}\n\t\tremaining -= page_len;\n\t\toffset += page_len;\n\t}\n\n\treturn -EINVAL;\n}\n\n \nstatic noinline int svc_rdma_read_multiple_chunks(struct svc_rdma_read_info *info)\n{\n\tstruct svc_rdma_recv_ctxt *head = info->ri_readctxt;\n\tconst struct svc_rdma_pcl *pcl = &head->rc_read_pcl;\n\tstruct xdr_buf *buf = &info->ri_rqst->rq_arg;\n\tstruct svc_rdma_chunk *chunk, *next;\n\tunsigned int start, length;\n\tint ret;\n\n\tstart = 0;\n\tchunk = pcl_first_chunk(pcl);\n\tlength = chunk->ch_position;\n\tret = svc_rdma_copy_inline_range(info, start, length);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tpcl_for_each_chunk(chunk, pcl) {\n\t\tret = svc_rdma_build_read_chunk(info, chunk);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tnext = pcl_next_chunk(pcl, chunk);\n\t\tif (!next)\n\t\t\tbreak;\n\n\t\tstart += length;\n\t\tlength = next->ch_position - info->ri_totalbytes;\n\t\tret = svc_rdma_copy_inline_range(info, start, length);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\tstart += length;\n\tlength = head->rc_byte_len - start;\n\tret = svc_rdma_copy_inline_range(info, start, length);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tbuf->len += info->ri_totalbytes;\n\tbuf->buflen += info->ri_totalbytes;\n\n\tbuf->head[0].iov_base = page_address(info->ri_rqst->rq_pages[0]);\n\tbuf->head[0].iov_len = min_t(size_t, PAGE_SIZE, info->ri_totalbytes);\n\tbuf->pages = &info->ri_rqst->rq_pages[1];\n\tbuf->page_len = info->ri_totalbytes - buf->head[0].iov_len;\n\treturn 0;\n}\n\n \nstatic int svc_rdma_read_data_item(struct svc_rdma_read_info *info)\n{\n\tstruct svc_rdma_recv_ctxt *head = info->ri_readctxt;\n\tstruct xdr_buf *buf = &info->ri_rqst->rq_arg;\n\tstruct svc_rdma_chunk *chunk;\n\tunsigned int length;\n\tint ret;\n\n\tchunk = pcl_first_chunk(&head->rc_read_pcl);\n\tret = svc_rdma_build_read_chunk(info, chunk);\n\tif (ret < 0)\n\t\tgoto out;\n\n\t \n\tbuf->tail[0].iov_base = buf->head[0].iov_base + chunk->ch_position;\n\tbuf->tail[0].iov_len = buf->head[0].iov_len - chunk->ch_position;\n\tbuf->head[0].iov_len = chunk->ch_position;\n\n\t \n\tbuf->pages = &info->ri_rqst->rq_pages[0];\n\tlength = xdr_align_size(chunk->ch_length);\n\tbuf->page_len = length;\n\tbuf->len += length;\n\tbuf->buflen += length;\n\nout:\n\treturn ret;\n}\n\n \nstatic int svc_rdma_read_chunk_range(struct svc_rdma_read_info *info,\n\t\t\t\t     const struct svc_rdma_chunk *chunk,\n\t\t\t\t     unsigned int offset, unsigned int length)\n{\n\tconst struct svc_rdma_segment *segment;\n\tint ret;\n\n\tret = -EINVAL;\n\tpcl_for_each_segment(segment, chunk) {\n\t\tstruct svc_rdma_segment dummy;\n\n\t\tif (offset > segment->rs_length) {\n\t\t\toffset -= segment->rs_length;\n\t\t\tcontinue;\n\t\t}\n\n\t\tdummy.rs_handle = segment->rs_handle;\n\t\tdummy.rs_length = min_t(u32, length, segment->rs_length) - offset;\n\t\tdummy.rs_offset = segment->rs_offset + offset;\n\n\t\tret = svc_rdma_build_read_segment(info, &dummy);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\n\t\tinfo->ri_totalbytes += dummy.rs_length;\n\t\tlength -= dummy.rs_length;\n\t\toffset = 0;\n\t}\n\treturn ret;\n}\n\n \nstatic int svc_rdma_read_call_chunk(struct svc_rdma_read_info *info)\n{\n\tstruct svc_rdma_recv_ctxt *head = info->ri_readctxt;\n\tconst struct svc_rdma_chunk *call_chunk =\n\t\t\tpcl_first_chunk(&head->rc_call_pcl);\n\tconst struct svc_rdma_pcl *pcl = &head->rc_read_pcl;\n\tstruct svc_rdma_chunk *chunk, *next;\n\tunsigned int start, length;\n\tint ret;\n\n\tif (pcl_is_empty(pcl))\n\t\treturn svc_rdma_build_read_chunk(info, call_chunk);\n\n\tstart = 0;\n\tchunk = pcl_first_chunk(pcl);\n\tlength = chunk->ch_position;\n\tret = svc_rdma_read_chunk_range(info, call_chunk, start, length);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tpcl_for_each_chunk(chunk, pcl) {\n\t\tret = svc_rdma_build_read_chunk(info, chunk);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tnext = pcl_next_chunk(pcl, chunk);\n\t\tif (!next)\n\t\t\tbreak;\n\n\t\tstart += length;\n\t\tlength = next->ch_position - info->ri_totalbytes;\n\t\tret = svc_rdma_read_chunk_range(info, call_chunk,\n\t\t\t\t\t\tstart, length);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\tstart += length;\n\tlength = call_chunk->ch_length - start;\n\treturn svc_rdma_read_chunk_range(info, call_chunk, start, length);\n}\n\n \nstatic noinline int svc_rdma_read_special(struct svc_rdma_read_info *info)\n{\n\tstruct xdr_buf *buf = &info->ri_rqst->rq_arg;\n\tint ret;\n\n\tret = svc_rdma_read_call_chunk(info);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tbuf->len += info->ri_totalbytes;\n\tbuf->buflen += info->ri_totalbytes;\n\n\tbuf->head[0].iov_base = page_address(info->ri_rqst->rq_pages[0]);\n\tbuf->head[0].iov_len = min_t(size_t, PAGE_SIZE, info->ri_totalbytes);\n\tbuf->pages = &info->ri_rqst->rq_pages[1];\n\tbuf->page_len = info->ri_totalbytes - buf->head[0].iov_len;\n\nout:\n\treturn ret;\n}\n\n \nint svc_rdma_process_read_list(struct svcxprt_rdma *rdma,\n\t\t\t       struct svc_rqst *rqstp,\n\t\t\t       struct svc_rdma_recv_ctxt *head)\n{\n\tstruct svc_rdma_read_info *info;\n\tstruct svc_rdma_chunk_ctxt *cc;\n\tint ret;\n\n\tinfo = svc_rdma_read_info_alloc(rdma);\n\tif (!info)\n\t\treturn -ENOMEM;\n\tcc = &info->ri_cc;\n\tinfo->ri_rqst = rqstp;\n\tinfo->ri_readctxt = head;\n\tinfo->ri_pageno = 0;\n\tinfo->ri_pageoff = 0;\n\tinfo->ri_totalbytes = 0;\n\n\tif (pcl_is_empty(&head->rc_call_pcl)) {\n\t\tif (head->rc_read_pcl.cl_count == 1)\n\t\t\tret = svc_rdma_read_data_item(info);\n\t\telse\n\t\t\tret = svc_rdma_read_multiple_chunks(info);\n\t} else\n\t\tret = svc_rdma_read_special(info);\n\tif (ret < 0)\n\t\tgoto out_err;\n\n\ttrace_svcrdma_post_read_chunk(&cc->cc_cid, cc->cc_sqecount);\n\tinit_completion(&cc->cc_done);\n\tret = svc_rdma_post_chunk_ctxt(cc);\n\tif (ret < 0)\n\t\tgoto out_err;\n\n\tret = 1;\n\twait_for_completion(&cc->cc_done);\n\tif (cc->cc_status != IB_WC_SUCCESS)\n\t\tret = -EIO;\n\n\t \n\trqstp->rq_respages = &rqstp->rq_pages[head->rc_page_count];\n\trqstp->rq_next_page = rqstp->rq_respages + 1;\n\n\t \n\thead->rc_page_count = 0;\n\nout_err:\n\tsvc_rdma_read_info_free(info);\n\treturn ret;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}