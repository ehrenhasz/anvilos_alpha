{
  "module_name": "frwr_ops.c",
  "hash_id": "d83b25c456b7285a44dc90dcb2295afbe77a01f4ed33b52832d277f59b72dc93",
  "original_prompt": "Ingested from linux-6.6.14/net/sunrpc/xprtrdma/frwr_ops.c",
  "human_readable_source": "\n \n\n \n\n \n\n \n\n#include <linux/sunrpc/svc_rdma.h>\n\n#include \"xprt_rdma.h\"\n#include <trace/events/rpcrdma.h>\n\nstatic void frwr_cid_init(struct rpcrdma_ep *ep,\n\t\t\t  struct rpcrdma_mr *mr)\n{\n\tstruct rpc_rdma_cid *cid = &mr->mr_cid;\n\n\tcid->ci_queue_id = ep->re_attr.send_cq->res.id;\n\tcid->ci_completion_id = mr->mr_ibmr->res.id;\n}\n\nstatic void frwr_mr_unmap(struct rpcrdma_xprt *r_xprt, struct rpcrdma_mr *mr)\n{\n\tif (mr->mr_device) {\n\t\ttrace_xprtrdma_mr_unmap(mr);\n\t\tib_dma_unmap_sg(mr->mr_device, mr->mr_sg, mr->mr_nents,\n\t\t\t\tmr->mr_dir);\n\t\tmr->mr_device = NULL;\n\t}\n}\n\n \nvoid frwr_mr_release(struct rpcrdma_mr *mr)\n{\n\tint rc;\n\n\tfrwr_mr_unmap(mr->mr_xprt, mr);\n\n\trc = ib_dereg_mr(mr->mr_ibmr);\n\tif (rc)\n\t\ttrace_xprtrdma_frwr_dereg(mr, rc);\n\tkfree(mr->mr_sg);\n\tkfree(mr);\n}\n\nstatic void frwr_mr_put(struct rpcrdma_mr *mr)\n{\n\tfrwr_mr_unmap(mr->mr_xprt, mr);\n\n\t \n\trpcrdma_mr_push(mr, &mr->mr_req->rl_free_mrs);\n}\n\n \nvoid frwr_reset(struct rpcrdma_req *req)\n{\n\tstruct rpcrdma_mr *mr;\n\n\twhile ((mr = rpcrdma_mr_pop(&req->rl_registered)))\n\t\tfrwr_mr_put(mr);\n}\n\n \nint frwr_mr_init(struct rpcrdma_xprt *r_xprt, struct rpcrdma_mr *mr)\n{\n\tstruct rpcrdma_ep *ep = r_xprt->rx_ep;\n\tunsigned int depth = ep->re_max_fr_depth;\n\tstruct scatterlist *sg;\n\tstruct ib_mr *frmr;\n\n\tsg = kcalloc_node(depth, sizeof(*sg), XPRTRDMA_GFP_FLAGS,\n\t\t\t  ibdev_to_node(ep->re_id->device));\n\tif (!sg)\n\t\treturn -ENOMEM;\n\n\tfrmr = ib_alloc_mr(ep->re_pd, ep->re_mrtype, depth);\n\tif (IS_ERR(frmr))\n\t\tgoto out_mr_err;\n\n\tmr->mr_xprt = r_xprt;\n\tmr->mr_ibmr = frmr;\n\tmr->mr_device = NULL;\n\tINIT_LIST_HEAD(&mr->mr_list);\n\tinit_completion(&mr->mr_linv_done);\n\tfrwr_cid_init(ep, mr);\n\n\tsg_init_table(sg, depth);\n\tmr->mr_sg = sg;\n\treturn 0;\n\nout_mr_err:\n\tkfree(sg);\n\ttrace_xprtrdma_frwr_alloc(mr, PTR_ERR(frmr));\n\treturn PTR_ERR(frmr);\n}\n\n \nint frwr_query_device(struct rpcrdma_ep *ep, const struct ib_device *device)\n{\n\tconst struct ib_device_attr *attrs = &device->attrs;\n\tint max_qp_wr, depth, delta;\n\tunsigned int max_sge;\n\n\tif (!(attrs->device_cap_flags & IB_DEVICE_MEM_MGT_EXTENSIONS) ||\n\t    attrs->max_fast_reg_page_list_len == 0) {\n\t\tpr_err(\"rpcrdma: 'frwr' mode is not supported by device %s\\n\",\n\t\t       device->name);\n\t\treturn -EINVAL;\n\t}\n\n\tmax_sge = min_t(unsigned int, attrs->max_send_sge,\n\t\t\tRPCRDMA_MAX_SEND_SGES);\n\tif (max_sge < RPCRDMA_MIN_SEND_SGES) {\n\t\tpr_err(\"rpcrdma: HCA provides only %u send SGEs\\n\", max_sge);\n\t\treturn -ENOMEM;\n\t}\n\tep->re_attr.cap.max_send_sge = max_sge;\n\tep->re_attr.cap.max_recv_sge = 1;\n\n\tep->re_mrtype = IB_MR_TYPE_MEM_REG;\n\tif (attrs->kernel_cap_flags & IBK_SG_GAPS_REG)\n\t\tep->re_mrtype = IB_MR_TYPE_SG_GAPS;\n\n\t \n\tif (attrs->max_sge_rd > RPCRDMA_MAX_HDR_SEGS)\n\t\tep->re_max_fr_depth = attrs->max_sge_rd;\n\telse\n\t\tep->re_max_fr_depth = attrs->max_fast_reg_page_list_len;\n\tif (ep->re_max_fr_depth > RPCRDMA_MAX_DATA_SEGS)\n\t\tep->re_max_fr_depth = RPCRDMA_MAX_DATA_SEGS;\n\n\t \n\tdepth = 7;\n\n\t \n\tif (ep->re_max_fr_depth < RPCRDMA_MAX_DATA_SEGS) {\n\t\tdelta = RPCRDMA_MAX_DATA_SEGS - ep->re_max_fr_depth;\n\t\tdo {\n\t\t\tdepth += 2;  \n\t\t\tdelta -= ep->re_max_fr_depth;\n\t\t} while (delta > 0);\n\t}\n\n\tmax_qp_wr = attrs->max_qp_wr;\n\tmax_qp_wr -= RPCRDMA_BACKWARD_WRS;\n\tmax_qp_wr -= 1;\n\tif (max_qp_wr < RPCRDMA_MIN_SLOT_TABLE)\n\t\treturn -ENOMEM;\n\tif (ep->re_max_requests > max_qp_wr)\n\t\tep->re_max_requests = max_qp_wr;\n\tep->re_attr.cap.max_send_wr = ep->re_max_requests * depth;\n\tif (ep->re_attr.cap.max_send_wr > max_qp_wr) {\n\t\tep->re_max_requests = max_qp_wr / depth;\n\t\tif (!ep->re_max_requests)\n\t\t\treturn -ENOMEM;\n\t\tep->re_attr.cap.max_send_wr = ep->re_max_requests * depth;\n\t}\n\tep->re_attr.cap.max_send_wr += RPCRDMA_BACKWARD_WRS;\n\tep->re_attr.cap.max_send_wr += 1;  \n\tep->re_attr.cap.max_recv_wr = ep->re_max_requests;\n\tep->re_attr.cap.max_recv_wr += RPCRDMA_BACKWARD_WRS;\n\tep->re_attr.cap.max_recv_wr += RPCRDMA_MAX_RECV_BATCH;\n\tep->re_attr.cap.max_recv_wr += 1;  \n\n\tep->re_max_rdma_segs =\n\t\tDIV_ROUND_UP(RPCRDMA_MAX_DATA_SEGS, ep->re_max_fr_depth);\n\t \n\tep->re_max_rdma_segs += 2;\n\tif (ep->re_max_rdma_segs > RPCRDMA_MAX_HDR_SEGS)\n\t\tep->re_max_rdma_segs = RPCRDMA_MAX_HDR_SEGS;\n\n\t \n\tif ((ep->re_max_rdma_segs * ep->re_max_fr_depth) < RPCRDMA_MAX_SEGS)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\n \nstruct rpcrdma_mr_seg *frwr_map(struct rpcrdma_xprt *r_xprt,\n\t\t\t\tstruct rpcrdma_mr_seg *seg,\n\t\t\t\tint nsegs, bool writing, __be32 xid,\n\t\t\t\tstruct rpcrdma_mr *mr)\n{\n\tstruct rpcrdma_ep *ep = r_xprt->rx_ep;\n\tstruct ib_reg_wr *reg_wr;\n\tint i, n, dma_nents;\n\tstruct ib_mr *ibmr;\n\tu8 key;\n\n\tif (nsegs > ep->re_max_fr_depth)\n\t\tnsegs = ep->re_max_fr_depth;\n\tfor (i = 0; i < nsegs;) {\n\t\tsg_set_page(&mr->mr_sg[i], seg->mr_page,\n\t\t\t    seg->mr_len, seg->mr_offset);\n\n\t\t++seg;\n\t\t++i;\n\t\tif (ep->re_mrtype == IB_MR_TYPE_SG_GAPS)\n\t\t\tcontinue;\n\t\tif ((i < nsegs && seg->mr_offset) ||\n\t\t    offset_in_page((seg-1)->mr_offset + (seg-1)->mr_len))\n\t\t\tbreak;\n\t}\n\tmr->mr_dir = rpcrdma_data_dir(writing);\n\tmr->mr_nents = i;\n\n\tdma_nents = ib_dma_map_sg(ep->re_id->device, mr->mr_sg, mr->mr_nents,\n\t\t\t\t  mr->mr_dir);\n\tif (!dma_nents)\n\t\tgoto out_dmamap_err;\n\tmr->mr_device = ep->re_id->device;\n\n\tibmr = mr->mr_ibmr;\n\tn = ib_map_mr_sg(ibmr, mr->mr_sg, dma_nents, NULL, PAGE_SIZE);\n\tif (n != dma_nents)\n\t\tgoto out_mapmr_err;\n\n\tibmr->iova &= 0x00000000ffffffff;\n\tibmr->iova |= ((u64)be32_to_cpu(xid)) << 32;\n\tkey = (u8)(ibmr->rkey & 0x000000FF);\n\tib_update_fast_reg_key(ibmr, ++key);\n\n\treg_wr = &mr->mr_regwr;\n\treg_wr->mr = ibmr;\n\treg_wr->key = ibmr->rkey;\n\treg_wr->access = writing ?\n\t\t\t IB_ACCESS_REMOTE_WRITE | IB_ACCESS_LOCAL_WRITE :\n\t\t\t IB_ACCESS_REMOTE_READ;\n\n\tmr->mr_handle = ibmr->rkey;\n\tmr->mr_length = ibmr->length;\n\tmr->mr_offset = ibmr->iova;\n\ttrace_xprtrdma_mr_map(mr);\n\n\treturn seg;\n\nout_dmamap_err:\n\ttrace_xprtrdma_frwr_sgerr(mr, i);\n\treturn ERR_PTR(-EIO);\n\nout_mapmr_err:\n\ttrace_xprtrdma_frwr_maperr(mr, n);\n\treturn ERR_PTR(-EIO);\n}\n\n \nstatic void frwr_wc_fastreg(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct ib_cqe *cqe = wc->wr_cqe;\n\tstruct rpcrdma_mr *mr = container_of(cqe, struct rpcrdma_mr, mr_cqe);\n\n\t \n\ttrace_xprtrdma_wc_fastreg(wc, &mr->mr_cid);\n\n\trpcrdma_flush_disconnect(cq->cq_context, wc);\n}\n\n \nint frwr_send(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req)\n{\n\tstruct ib_send_wr *post_wr, *send_wr = &req->rl_wr;\n\tstruct rpcrdma_ep *ep = r_xprt->rx_ep;\n\tstruct rpcrdma_mr *mr;\n\tunsigned int num_wrs;\n\tint ret;\n\n\tnum_wrs = 1;\n\tpost_wr = send_wr;\n\tlist_for_each_entry(mr, &req->rl_registered, mr_list) {\n\t\ttrace_xprtrdma_mr_fastreg(mr);\n\n\t\tmr->mr_cqe.done = frwr_wc_fastreg;\n\t\tmr->mr_regwr.wr.next = post_wr;\n\t\tmr->mr_regwr.wr.wr_cqe = &mr->mr_cqe;\n\t\tmr->mr_regwr.wr.num_sge = 0;\n\t\tmr->mr_regwr.wr.opcode = IB_WR_REG_MR;\n\t\tmr->mr_regwr.wr.send_flags = 0;\n\t\tpost_wr = &mr->mr_regwr.wr;\n\t\t++num_wrs;\n\t}\n\n\tif ((kref_read(&req->rl_kref) > 1) || num_wrs > ep->re_send_count) {\n\t\tsend_wr->send_flags |= IB_SEND_SIGNALED;\n\t\tep->re_send_count = min_t(unsigned int, ep->re_send_batch,\n\t\t\t\t\t  num_wrs - ep->re_send_count);\n\t} else {\n\t\tsend_wr->send_flags &= ~IB_SEND_SIGNALED;\n\t\tep->re_send_count -= num_wrs;\n\t}\n\n\ttrace_xprtrdma_post_send(req);\n\tret = ib_post_send(ep->re_id->qp, post_wr, NULL);\n\tif (ret)\n\t\ttrace_xprtrdma_post_send_err(r_xprt, req, ret);\n\treturn ret;\n}\n\n \nvoid frwr_reminv(struct rpcrdma_rep *rep, struct list_head *mrs)\n{\n\tstruct rpcrdma_mr *mr;\n\n\tlist_for_each_entry(mr, mrs, mr_list)\n\t\tif (mr->mr_handle == rep->rr_inv_rkey) {\n\t\t\tlist_del_init(&mr->mr_list);\n\t\t\ttrace_xprtrdma_mr_reminv(mr);\n\t\t\tfrwr_mr_put(mr);\n\t\t\tbreak;\t \n\t\t}\n}\n\nstatic void frwr_mr_done(struct ib_wc *wc, struct rpcrdma_mr *mr)\n{\n\tif (likely(wc->status == IB_WC_SUCCESS))\n\t\tfrwr_mr_put(mr);\n}\n\n \nstatic void frwr_wc_localinv(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct ib_cqe *cqe = wc->wr_cqe;\n\tstruct rpcrdma_mr *mr = container_of(cqe, struct rpcrdma_mr, mr_cqe);\n\n\t \n\ttrace_xprtrdma_wc_li(wc, &mr->mr_cid);\n\tfrwr_mr_done(wc, mr);\n\n\trpcrdma_flush_disconnect(cq->cq_context, wc);\n}\n\n \nstatic void frwr_wc_localinv_wake(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct ib_cqe *cqe = wc->wr_cqe;\n\tstruct rpcrdma_mr *mr = container_of(cqe, struct rpcrdma_mr, mr_cqe);\n\n\t \n\ttrace_xprtrdma_wc_li_wake(wc, &mr->mr_cid);\n\tfrwr_mr_done(wc, mr);\n\tcomplete(&mr->mr_linv_done);\n\n\trpcrdma_flush_disconnect(cq->cq_context, wc);\n}\n\n \nvoid frwr_unmap_sync(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req)\n{\n\tstruct ib_send_wr *first, **prev, *last;\n\tstruct rpcrdma_ep *ep = r_xprt->rx_ep;\n\tconst struct ib_send_wr *bad_wr;\n\tstruct rpcrdma_mr *mr;\n\tint rc;\n\n\t \n\tprev = &first;\n\tmr = rpcrdma_mr_pop(&req->rl_registered);\n\tdo {\n\t\ttrace_xprtrdma_mr_localinv(mr);\n\t\tr_xprt->rx_stats.local_inv_needed++;\n\n\t\tlast = &mr->mr_invwr;\n\t\tlast->next = NULL;\n\t\tlast->wr_cqe = &mr->mr_cqe;\n\t\tlast->sg_list = NULL;\n\t\tlast->num_sge = 0;\n\t\tlast->opcode = IB_WR_LOCAL_INV;\n\t\tlast->send_flags = IB_SEND_SIGNALED;\n\t\tlast->ex.invalidate_rkey = mr->mr_handle;\n\n\t\tlast->wr_cqe->done = frwr_wc_localinv;\n\n\t\t*prev = last;\n\t\tprev = &last->next;\n\t} while ((mr = rpcrdma_mr_pop(&req->rl_registered)));\n\n\tmr = container_of(last, struct rpcrdma_mr, mr_invwr);\n\n\t \n\tlast->wr_cqe->done = frwr_wc_localinv_wake;\n\treinit_completion(&mr->mr_linv_done);\n\n\t \n\tbad_wr = NULL;\n\trc = ib_post_send(ep->re_id->qp, first, &bad_wr);\n\n\t \n\tif (bad_wr != first)\n\t\twait_for_completion(&mr->mr_linv_done);\n\tif (!rc)\n\t\treturn;\n\n\t \n\ttrace_xprtrdma_post_linv_err(req, rc);\n\n\t \n\trpcrdma_force_disconnect(ep);\n}\n\n \nstatic void frwr_wc_localinv_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct ib_cqe *cqe = wc->wr_cqe;\n\tstruct rpcrdma_mr *mr = container_of(cqe, struct rpcrdma_mr, mr_cqe);\n\tstruct rpcrdma_rep *rep;\n\n\t \n\ttrace_xprtrdma_wc_li_done(wc, &mr->mr_cid);\n\n\t \n\trep = mr->mr_req->rl_reply;\n\tsmp_rmb();\n\n\tif (wc->status != IB_WC_SUCCESS) {\n\t\tif (rep)\n\t\t\trpcrdma_unpin_rqst(rep);\n\t\trpcrdma_flush_disconnect(cq->cq_context, wc);\n\t\treturn;\n\t}\n\tfrwr_mr_put(mr);\n\trpcrdma_complete_rqst(rep);\n}\n\n \nvoid frwr_unmap_async(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req)\n{\n\tstruct ib_send_wr *first, *last, **prev;\n\tstruct rpcrdma_ep *ep = r_xprt->rx_ep;\n\tstruct rpcrdma_mr *mr;\n\tint rc;\n\n\t \n\tprev = &first;\n\tmr = rpcrdma_mr_pop(&req->rl_registered);\n\tdo {\n\t\ttrace_xprtrdma_mr_localinv(mr);\n\t\tr_xprt->rx_stats.local_inv_needed++;\n\n\t\tlast = &mr->mr_invwr;\n\t\tlast->next = NULL;\n\t\tlast->wr_cqe = &mr->mr_cqe;\n\t\tlast->sg_list = NULL;\n\t\tlast->num_sge = 0;\n\t\tlast->opcode = IB_WR_LOCAL_INV;\n\t\tlast->send_flags = IB_SEND_SIGNALED;\n\t\tlast->ex.invalidate_rkey = mr->mr_handle;\n\n\t\tlast->wr_cqe->done = frwr_wc_localinv;\n\n\t\t*prev = last;\n\t\tprev = &last->next;\n\t} while ((mr = rpcrdma_mr_pop(&req->rl_registered)));\n\n\t \n\tlast->wr_cqe->done = frwr_wc_localinv_done;\n\n\t \n\trc = ib_post_send(ep->re_id->qp, first, NULL);\n\tif (!rc)\n\t\treturn;\n\n\t \n\ttrace_xprtrdma_post_linv_err(req, rc);\n\n\t \n\trpcrdma_unpin_rqst(req->rl_reply);\n\n\t \n\trpcrdma_force_disconnect(ep);\n}\n\n \nint frwr_wp_create(struct rpcrdma_xprt *r_xprt)\n{\n\tstruct rpcrdma_ep *ep = r_xprt->rx_ep;\n\tstruct rpcrdma_mr_seg seg;\n\tstruct rpcrdma_mr *mr;\n\n\tmr = rpcrdma_mr_get(r_xprt);\n\tif (!mr)\n\t\treturn -EAGAIN;\n\tmr->mr_req = NULL;\n\tep->re_write_pad_mr = mr;\n\n\tseg.mr_len = XDR_UNIT;\n\tseg.mr_page = virt_to_page(ep->re_write_pad);\n\tseg.mr_offset = offset_in_page(ep->re_write_pad);\n\tif (IS_ERR(frwr_map(r_xprt, &seg, 1, true, xdr_zero, mr)))\n\t\treturn -EIO;\n\ttrace_xprtrdma_mr_fastreg(mr);\n\n\tmr->mr_cqe.done = frwr_wc_fastreg;\n\tmr->mr_regwr.wr.next = NULL;\n\tmr->mr_regwr.wr.wr_cqe = &mr->mr_cqe;\n\tmr->mr_regwr.wr.num_sge = 0;\n\tmr->mr_regwr.wr.opcode = IB_WR_REG_MR;\n\tmr->mr_regwr.wr.send_flags = 0;\n\n\treturn ib_post_send(ep->re_id->qp, &mr->mr_regwr.wr, NULL);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}