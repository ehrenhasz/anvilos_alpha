{
  "module_name": "svc_rdma_sendto.c",
  "hash_id": "3c94a0904fa4896f56ea5b639eeb0747ac8bbf006eeef7480d2ab6ec4c091378",
  "original_prompt": "Ingested from linux-6.6.14/net/sunrpc/xprtrdma/svc_rdma_sendto.c",
  "human_readable_source": "\n \n\n \n\n#include <linux/spinlock.h>\n#include <asm/unaligned.h>\n\n#include <rdma/ib_verbs.h>\n#include <rdma/rdma_cm.h>\n\n#include <linux/sunrpc/debug.h>\n#include <linux/sunrpc/svc_rdma.h>\n\n#include \"xprt_rdma.h\"\n#include <trace/events/rpcrdma.h>\n\nstatic void svc_rdma_wc_send(struct ib_cq *cq, struct ib_wc *wc);\n\nstatic void svc_rdma_send_cid_init(struct svcxprt_rdma *rdma,\n\t\t\t\t   struct rpc_rdma_cid *cid)\n{\n\tcid->ci_queue_id = rdma->sc_sq_cq->res.id;\n\tcid->ci_completion_id = atomic_inc_return(&rdma->sc_completion_ids);\n}\n\nstatic struct svc_rdma_send_ctxt *\nsvc_rdma_send_ctxt_alloc(struct svcxprt_rdma *rdma)\n{\n\tint node = ibdev_to_node(rdma->sc_cm_id->device);\n\tstruct svc_rdma_send_ctxt *ctxt;\n\tdma_addr_t addr;\n\tvoid *buffer;\n\tint i;\n\n\tctxt = kmalloc_node(struct_size(ctxt, sc_sges, rdma->sc_max_send_sges),\n\t\t\t    GFP_KERNEL, node);\n\tif (!ctxt)\n\t\tgoto fail0;\n\tbuffer = kmalloc_node(rdma->sc_max_req_size, GFP_KERNEL, node);\n\tif (!buffer)\n\t\tgoto fail1;\n\taddr = ib_dma_map_single(rdma->sc_pd->device, buffer,\n\t\t\t\t rdma->sc_max_req_size, DMA_TO_DEVICE);\n\tif (ib_dma_mapping_error(rdma->sc_pd->device, addr))\n\t\tgoto fail2;\n\n\tsvc_rdma_send_cid_init(rdma, &ctxt->sc_cid);\n\n\tctxt->sc_send_wr.next = NULL;\n\tctxt->sc_send_wr.wr_cqe = &ctxt->sc_cqe;\n\tctxt->sc_send_wr.sg_list = ctxt->sc_sges;\n\tctxt->sc_send_wr.send_flags = IB_SEND_SIGNALED;\n\tctxt->sc_cqe.done = svc_rdma_wc_send;\n\tctxt->sc_xprt_buf = buffer;\n\txdr_buf_init(&ctxt->sc_hdrbuf, ctxt->sc_xprt_buf,\n\t\t     rdma->sc_max_req_size);\n\tctxt->sc_sges[0].addr = addr;\n\n\tfor (i = 0; i < rdma->sc_max_send_sges; i++)\n\t\tctxt->sc_sges[i].lkey = rdma->sc_pd->local_dma_lkey;\n\treturn ctxt;\n\nfail2:\n\tkfree(buffer);\nfail1:\n\tkfree(ctxt);\nfail0:\n\treturn NULL;\n}\n\n \nvoid svc_rdma_send_ctxts_destroy(struct svcxprt_rdma *rdma)\n{\n\tstruct svc_rdma_send_ctxt *ctxt;\n\tstruct llist_node *node;\n\n\twhile ((node = llist_del_first(&rdma->sc_send_ctxts)) != NULL) {\n\t\tctxt = llist_entry(node, struct svc_rdma_send_ctxt, sc_node);\n\t\tib_dma_unmap_single(rdma->sc_pd->device,\n\t\t\t\t    ctxt->sc_sges[0].addr,\n\t\t\t\t    rdma->sc_max_req_size,\n\t\t\t\t    DMA_TO_DEVICE);\n\t\tkfree(ctxt->sc_xprt_buf);\n\t\tkfree(ctxt);\n\t}\n}\n\n \nstruct svc_rdma_send_ctxt *svc_rdma_send_ctxt_get(struct svcxprt_rdma *rdma)\n{\n\tstruct svc_rdma_send_ctxt *ctxt;\n\tstruct llist_node *node;\n\n\tspin_lock(&rdma->sc_send_lock);\n\tnode = llist_del_first(&rdma->sc_send_ctxts);\n\tif (!node)\n\t\tgoto out_empty;\n\tctxt = llist_entry(node, struct svc_rdma_send_ctxt, sc_node);\n\tspin_unlock(&rdma->sc_send_lock);\n\nout:\n\trpcrdma_set_xdrlen(&ctxt->sc_hdrbuf, 0);\n\txdr_init_encode(&ctxt->sc_stream, &ctxt->sc_hdrbuf,\n\t\t\tctxt->sc_xprt_buf, NULL);\n\n\tctxt->sc_send_wr.num_sge = 0;\n\tctxt->sc_cur_sge_no = 0;\n\tctxt->sc_page_count = 0;\n\treturn ctxt;\n\nout_empty:\n\tspin_unlock(&rdma->sc_send_lock);\n\tctxt = svc_rdma_send_ctxt_alloc(rdma);\n\tif (!ctxt)\n\t\treturn NULL;\n\tgoto out;\n}\n\n \nvoid svc_rdma_send_ctxt_put(struct svcxprt_rdma *rdma,\n\t\t\t    struct svc_rdma_send_ctxt *ctxt)\n{\n\tstruct ib_device *device = rdma->sc_cm_id->device;\n\tunsigned int i;\n\n\tif (ctxt->sc_page_count)\n\t\trelease_pages(ctxt->sc_pages, ctxt->sc_page_count);\n\n\t \n\tfor (i = 1; i < ctxt->sc_send_wr.num_sge; i++) {\n\t\tib_dma_unmap_page(device,\n\t\t\t\t  ctxt->sc_sges[i].addr,\n\t\t\t\t  ctxt->sc_sges[i].length,\n\t\t\t\t  DMA_TO_DEVICE);\n\t\ttrace_svcrdma_dma_unmap_page(rdma,\n\t\t\t\t\t     ctxt->sc_sges[i].addr,\n\t\t\t\t\t     ctxt->sc_sges[i].length);\n\t}\n\n\tllist_add(&ctxt->sc_node, &rdma->sc_send_ctxts);\n}\n\n \nvoid svc_rdma_wake_send_waiters(struct svcxprt_rdma *rdma, int avail)\n{\n\tatomic_add(avail, &rdma->sc_sq_avail);\n\tsmp_mb__after_atomic();\n\tif (unlikely(waitqueue_active(&rdma->sc_send_wait)))\n\t\twake_up(&rdma->sc_send_wait);\n}\n\n \nstatic void svc_rdma_wc_send(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct svcxprt_rdma *rdma = cq->cq_context;\n\tstruct ib_cqe *cqe = wc->wr_cqe;\n\tstruct svc_rdma_send_ctxt *ctxt =\n\t\tcontainer_of(cqe, struct svc_rdma_send_ctxt, sc_cqe);\n\n\tsvc_rdma_wake_send_waiters(rdma, 1);\n\n\tif (unlikely(wc->status != IB_WC_SUCCESS))\n\t\tgoto flushed;\n\n\ttrace_svcrdma_wc_send(wc, &ctxt->sc_cid);\n\tsvc_rdma_send_ctxt_put(rdma, ctxt);\n\treturn;\n\nflushed:\n\tif (wc->status != IB_WC_WR_FLUSH_ERR)\n\t\ttrace_svcrdma_wc_send_err(wc, &ctxt->sc_cid);\n\telse\n\t\ttrace_svcrdma_wc_send_flush(wc, &ctxt->sc_cid);\n\tsvc_rdma_send_ctxt_put(rdma, ctxt);\n\tsvc_xprt_deferred_close(&rdma->sc_xprt);\n}\n\n \nint svc_rdma_send(struct svcxprt_rdma *rdma, struct svc_rdma_send_ctxt *ctxt)\n{\n\tstruct ib_send_wr *wr = &ctxt->sc_send_wr;\n\tint ret;\n\n\tmight_sleep();\n\n\t \n\tib_dma_sync_single_for_device(rdma->sc_pd->device,\n\t\t\t\t      wr->sg_list[0].addr,\n\t\t\t\t      wr->sg_list[0].length,\n\t\t\t\t      DMA_TO_DEVICE);\n\n\t \n\twhile (1) {\n\t\tif ((atomic_dec_return(&rdma->sc_sq_avail) < 0)) {\n\t\t\tpercpu_counter_inc(&svcrdma_stat_sq_starve);\n\t\t\ttrace_svcrdma_sq_full(rdma);\n\t\t\tatomic_inc(&rdma->sc_sq_avail);\n\t\t\twait_event(rdma->sc_send_wait,\n\t\t\t\t   atomic_read(&rdma->sc_sq_avail) > 1);\n\t\t\tif (test_bit(XPT_CLOSE, &rdma->sc_xprt.xpt_flags))\n\t\t\t\treturn -ENOTCONN;\n\t\t\ttrace_svcrdma_sq_retry(rdma);\n\t\t\tcontinue;\n\t\t}\n\n\t\ttrace_svcrdma_post_send(ctxt);\n\t\tret = ib_post_send(rdma->sc_qp, wr, NULL);\n\t\tif (ret)\n\t\t\tbreak;\n\t\treturn 0;\n\t}\n\n\ttrace_svcrdma_sq_post_err(rdma, ret);\n\tsvc_xprt_deferred_close(&rdma->sc_xprt);\n\twake_up(&rdma->sc_send_wait);\n\treturn ret;\n}\n\n \nstatic ssize_t svc_rdma_encode_read_list(struct svc_rdma_send_ctxt *sctxt)\n{\n\t \n\treturn xdr_stream_encode_item_absent(&sctxt->sc_stream);\n}\n\n \nstatic ssize_t svc_rdma_encode_write_segment(struct svc_rdma_send_ctxt *sctxt,\n\t\t\t\t\t     const struct svc_rdma_chunk *chunk,\n\t\t\t\t\t     u32 *remaining, unsigned int segno)\n{\n\tconst struct svc_rdma_segment *segment = &chunk->ch_segments[segno];\n\tconst size_t len = rpcrdma_segment_maxsz * sizeof(__be32);\n\tu32 length;\n\t__be32 *p;\n\n\tp = xdr_reserve_space(&sctxt->sc_stream, len);\n\tif (!p)\n\t\treturn -EMSGSIZE;\n\n\tlength = min_t(u32, *remaining, segment->rs_length);\n\t*remaining -= length;\n\txdr_encode_rdma_segment(p, segment->rs_handle, length,\n\t\t\t\tsegment->rs_offset);\n\ttrace_svcrdma_encode_wseg(sctxt, segno, segment->rs_handle, length,\n\t\t\t\t  segment->rs_offset);\n\treturn len;\n}\n\n \nstatic ssize_t svc_rdma_encode_write_chunk(struct svc_rdma_send_ctxt *sctxt,\n\t\t\t\t\t   const struct svc_rdma_chunk *chunk)\n{\n\tu32 remaining = chunk->ch_payload_length;\n\tunsigned int segno;\n\tssize_t len, ret;\n\n\tlen = 0;\n\tret = xdr_stream_encode_item_present(&sctxt->sc_stream);\n\tif (ret < 0)\n\t\treturn ret;\n\tlen += ret;\n\n\tret = xdr_stream_encode_u32(&sctxt->sc_stream, chunk->ch_segcount);\n\tif (ret < 0)\n\t\treturn ret;\n\tlen += ret;\n\n\tfor (segno = 0; segno < chunk->ch_segcount; segno++) {\n\t\tret = svc_rdma_encode_write_segment(sctxt, chunk, &remaining, segno);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\tlen += ret;\n\t}\n\n\treturn len;\n}\n\n \nstatic ssize_t svc_rdma_encode_write_list(struct svc_rdma_recv_ctxt *rctxt,\n\t\t\t\t\t  struct svc_rdma_send_ctxt *sctxt)\n{\n\tstruct svc_rdma_chunk *chunk;\n\tssize_t len, ret;\n\n\tlen = 0;\n\tpcl_for_each_chunk(chunk, &rctxt->rc_write_pcl) {\n\t\tret = svc_rdma_encode_write_chunk(sctxt, chunk);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\tlen += ret;\n\t}\n\n\t \n\tret = xdr_stream_encode_item_absent(&sctxt->sc_stream);\n\tif (ret < 0)\n\t\treturn ret;\n\n\treturn len + ret;\n}\n\n \nstatic ssize_t\nsvc_rdma_encode_reply_chunk(struct svc_rdma_recv_ctxt *rctxt,\n\t\t\t    struct svc_rdma_send_ctxt *sctxt,\n\t\t\t    unsigned int length)\n{\n\tstruct svc_rdma_chunk *chunk;\n\n\tif (pcl_is_empty(&rctxt->rc_reply_pcl))\n\t\treturn xdr_stream_encode_item_absent(&sctxt->sc_stream);\n\n\tchunk = pcl_first_chunk(&rctxt->rc_reply_pcl);\n\tif (length > chunk->ch_length)\n\t\treturn -E2BIG;\n\n\tchunk->ch_payload_length = length;\n\treturn svc_rdma_encode_write_chunk(sctxt, chunk);\n}\n\nstruct svc_rdma_map_data {\n\tstruct svcxprt_rdma\t\t*md_rdma;\n\tstruct svc_rdma_send_ctxt\t*md_ctxt;\n};\n\n \nstatic int svc_rdma_page_dma_map(void *data, struct page *page,\n\t\t\t\t unsigned long offset, unsigned int len)\n{\n\tstruct svc_rdma_map_data *args = data;\n\tstruct svcxprt_rdma *rdma = args->md_rdma;\n\tstruct svc_rdma_send_ctxt *ctxt = args->md_ctxt;\n\tstruct ib_device *dev = rdma->sc_cm_id->device;\n\tdma_addr_t dma_addr;\n\n\t++ctxt->sc_cur_sge_no;\n\n\tdma_addr = ib_dma_map_page(dev, page, offset, len, DMA_TO_DEVICE);\n\tif (ib_dma_mapping_error(dev, dma_addr))\n\t\tgoto out_maperr;\n\n\ttrace_svcrdma_dma_map_page(rdma, dma_addr, len);\n\tctxt->sc_sges[ctxt->sc_cur_sge_no].addr = dma_addr;\n\tctxt->sc_sges[ctxt->sc_cur_sge_no].length = len;\n\tctxt->sc_send_wr.num_sge++;\n\treturn 0;\n\nout_maperr:\n\ttrace_svcrdma_dma_map_err(rdma, dma_addr, len);\n\treturn -EIO;\n}\n\n \nstatic int svc_rdma_iov_dma_map(void *data, const struct kvec *iov)\n{\n\tif (!iov->iov_len)\n\t\treturn 0;\n\treturn svc_rdma_page_dma_map(data, virt_to_page(iov->iov_base),\n\t\t\t\t     offset_in_page(iov->iov_base),\n\t\t\t\t     iov->iov_len);\n}\n\n \nstatic int svc_rdma_xb_dma_map(const struct xdr_buf *xdr, void *data)\n{\n\tunsigned int len, remaining;\n\tunsigned long pageoff;\n\tstruct page **ppages;\n\tint ret;\n\n\tret = svc_rdma_iov_dma_map(data, &xdr->head[0]);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tppages = xdr->pages + (xdr->page_base >> PAGE_SHIFT);\n\tpageoff = offset_in_page(xdr->page_base);\n\tremaining = xdr->page_len;\n\twhile (remaining) {\n\t\tlen = min_t(u32, PAGE_SIZE - pageoff, remaining);\n\n\t\tret = svc_rdma_page_dma_map(data, *ppages++, pageoff, len);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tremaining -= len;\n\t\tpageoff = 0;\n\t}\n\n\tret = svc_rdma_iov_dma_map(data, &xdr->tail[0]);\n\tif (ret < 0)\n\t\treturn ret;\n\n\treturn xdr->len;\n}\n\nstruct svc_rdma_pullup_data {\n\tu8\t\t*pd_dest;\n\tunsigned int\tpd_length;\n\tunsigned int\tpd_num_sges;\n};\n\n \nstatic int svc_rdma_xb_count_sges(const struct xdr_buf *xdr,\n\t\t\t\t  void *data)\n{\n\tstruct svc_rdma_pullup_data *args = data;\n\tunsigned int remaining;\n\tunsigned long offset;\n\n\tif (xdr->head[0].iov_len)\n\t\t++args->pd_num_sges;\n\n\toffset = offset_in_page(xdr->page_base);\n\tremaining = xdr->page_len;\n\twhile (remaining) {\n\t\t++args->pd_num_sges;\n\t\tremaining -= min_t(u32, PAGE_SIZE - offset, remaining);\n\t\toffset = 0;\n\t}\n\n\tif (xdr->tail[0].iov_len)\n\t\t++args->pd_num_sges;\n\n\targs->pd_length += xdr->len;\n\treturn 0;\n}\n\n \nstatic bool svc_rdma_pull_up_needed(const struct svcxprt_rdma *rdma,\n\t\t\t\t    const struct svc_rdma_send_ctxt *sctxt,\n\t\t\t\t    const struct svc_rdma_recv_ctxt *rctxt,\n\t\t\t\t    const struct xdr_buf *xdr)\n{\n\t \n\tstruct svc_rdma_pullup_data args = {\n\t\t.pd_length\t= sctxt->sc_hdrbuf.len,\n\t\t.pd_num_sges\t= 1,\n\t};\n\tint ret;\n\n\tret = pcl_process_nonpayloads(&rctxt->rc_write_pcl, xdr,\n\t\t\t\t      svc_rdma_xb_count_sges, &args);\n\tif (ret < 0)\n\t\treturn false;\n\n\tif (args.pd_length < RPCRDMA_PULLUP_THRESH)\n\t\treturn true;\n\treturn args.pd_num_sges >= rdma->sc_max_send_sges;\n}\n\n \nstatic int svc_rdma_xb_linearize(const struct xdr_buf *xdr,\n\t\t\t\t void *data)\n{\n\tstruct svc_rdma_pullup_data *args = data;\n\tunsigned int len, remaining;\n\tunsigned long pageoff;\n\tstruct page **ppages;\n\n\tif (xdr->head[0].iov_len) {\n\t\tmemcpy(args->pd_dest, xdr->head[0].iov_base, xdr->head[0].iov_len);\n\t\targs->pd_dest += xdr->head[0].iov_len;\n\t}\n\n\tppages = xdr->pages + (xdr->page_base >> PAGE_SHIFT);\n\tpageoff = offset_in_page(xdr->page_base);\n\tremaining = xdr->page_len;\n\twhile (remaining) {\n\t\tlen = min_t(u32, PAGE_SIZE - pageoff, remaining);\n\t\tmemcpy(args->pd_dest, page_address(*ppages) + pageoff, len);\n\t\tremaining -= len;\n\t\targs->pd_dest += len;\n\t\tpageoff = 0;\n\t\tppages++;\n\t}\n\n\tif (xdr->tail[0].iov_len) {\n\t\tmemcpy(args->pd_dest, xdr->tail[0].iov_base, xdr->tail[0].iov_len);\n\t\targs->pd_dest += xdr->tail[0].iov_len;\n\t}\n\n\targs->pd_length += xdr->len;\n\treturn 0;\n}\n\n \nstatic int svc_rdma_pull_up_reply_msg(const struct svcxprt_rdma *rdma,\n\t\t\t\t      struct svc_rdma_send_ctxt *sctxt,\n\t\t\t\t      const struct svc_rdma_recv_ctxt *rctxt,\n\t\t\t\t      const struct xdr_buf *xdr)\n{\n\tstruct svc_rdma_pullup_data args = {\n\t\t.pd_dest\t= sctxt->sc_xprt_buf + sctxt->sc_hdrbuf.len,\n\t};\n\tint ret;\n\n\tret = pcl_process_nonpayloads(&rctxt->rc_write_pcl, xdr,\n\t\t\t\t      svc_rdma_xb_linearize, &args);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tsctxt->sc_sges[0].length = sctxt->sc_hdrbuf.len + args.pd_length;\n\ttrace_svcrdma_send_pullup(sctxt, args.pd_length);\n\treturn 0;\n}\n\n \nint svc_rdma_map_reply_msg(struct svcxprt_rdma *rdma,\n\t\t\t   struct svc_rdma_send_ctxt *sctxt,\n\t\t\t   const struct svc_rdma_recv_ctxt *rctxt,\n\t\t\t   const struct xdr_buf *xdr)\n{\n\tstruct svc_rdma_map_data args = {\n\t\t.md_rdma\t= rdma,\n\t\t.md_ctxt\t= sctxt,\n\t};\n\n\t \n\tsctxt->sc_send_wr.num_sge = 1;\n\tsctxt->sc_sges[0].length = sctxt->sc_hdrbuf.len;\n\n\t \n\tif (!pcl_is_empty(&rctxt->rc_reply_pcl))\n\t\treturn 0;\n\n\t \n\tif (svc_rdma_pull_up_needed(rdma, sctxt, rctxt, xdr))\n\t\treturn svc_rdma_pull_up_reply_msg(rdma, sctxt, rctxt, xdr);\n\n\treturn pcl_process_nonpayloads(&rctxt->rc_write_pcl, xdr,\n\t\t\t\t       svc_rdma_xb_dma_map, &args);\n}\n\n \nstatic void svc_rdma_save_io_pages(struct svc_rqst *rqstp,\n\t\t\t\t   struct svc_rdma_send_ctxt *ctxt)\n{\n\tint i, pages = rqstp->rq_next_page - rqstp->rq_respages;\n\n\tctxt->sc_page_count += pages;\n\tfor (i = 0; i < pages; i++) {\n\t\tctxt->sc_pages[i] = rqstp->rq_respages[i];\n\t\trqstp->rq_respages[i] = NULL;\n\t}\n\n\t \n\trqstp->rq_next_page = rqstp->rq_respages;\n}\n\n \nstatic int svc_rdma_send_reply_msg(struct svcxprt_rdma *rdma,\n\t\t\t\t   struct svc_rdma_send_ctxt *sctxt,\n\t\t\t\t   const struct svc_rdma_recv_ctxt *rctxt,\n\t\t\t\t   struct svc_rqst *rqstp)\n{\n\tint ret;\n\n\tret = svc_rdma_map_reply_msg(rdma, sctxt, rctxt, &rqstp->rq_res);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tsvc_rdma_save_io_pages(rqstp, sctxt);\n\n\tif (rctxt->rc_inv_rkey) {\n\t\tsctxt->sc_send_wr.opcode = IB_WR_SEND_WITH_INV;\n\t\tsctxt->sc_send_wr.ex.invalidate_rkey = rctxt->rc_inv_rkey;\n\t} else {\n\t\tsctxt->sc_send_wr.opcode = IB_WR_SEND;\n\t}\n\n\treturn svc_rdma_send(rdma, sctxt);\n}\n\n \nvoid svc_rdma_send_error_msg(struct svcxprt_rdma *rdma,\n\t\t\t     struct svc_rdma_send_ctxt *sctxt,\n\t\t\t     struct svc_rdma_recv_ctxt *rctxt,\n\t\t\t     int status)\n{\n\t__be32 *rdma_argp = rctxt->rc_recv_buf;\n\t__be32 *p;\n\n\trpcrdma_set_xdrlen(&sctxt->sc_hdrbuf, 0);\n\txdr_init_encode(&sctxt->sc_stream, &sctxt->sc_hdrbuf,\n\t\t\tsctxt->sc_xprt_buf, NULL);\n\n\tp = xdr_reserve_space(&sctxt->sc_stream,\n\t\t\t      rpcrdma_fixed_maxsz * sizeof(*p));\n\tif (!p)\n\t\tgoto put_ctxt;\n\n\t*p++ = *rdma_argp;\n\t*p++ = *(rdma_argp + 1);\n\t*p++ = rdma->sc_fc_credits;\n\t*p = rdma_error;\n\n\tswitch (status) {\n\tcase -EPROTONOSUPPORT:\n\t\tp = xdr_reserve_space(&sctxt->sc_stream, 3 * sizeof(*p));\n\t\tif (!p)\n\t\t\tgoto put_ctxt;\n\n\t\t*p++ = err_vers;\n\t\t*p++ = rpcrdma_version;\n\t\t*p = rpcrdma_version;\n\t\ttrace_svcrdma_err_vers(*rdma_argp);\n\t\tbreak;\n\tdefault:\n\t\tp = xdr_reserve_space(&sctxt->sc_stream, sizeof(*p));\n\t\tif (!p)\n\t\t\tgoto put_ctxt;\n\n\t\t*p = err_chunk;\n\t\ttrace_svcrdma_err_chunk(*rdma_argp);\n\t}\n\n\t \n\tsctxt->sc_send_wr.num_sge = 1;\n\tsctxt->sc_send_wr.opcode = IB_WR_SEND;\n\tsctxt->sc_sges[0].length = sctxt->sc_hdrbuf.len;\n\tif (svc_rdma_send(rdma, sctxt))\n\t\tgoto put_ctxt;\n\treturn;\n\nput_ctxt:\n\tsvc_rdma_send_ctxt_put(rdma, sctxt);\n}\n\n \nint svc_rdma_sendto(struct svc_rqst *rqstp)\n{\n\tstruct svc_xprt *xprt = rqstp->rq_xprt;\n\tstruct svcxprt_rdma *rdma =\n\t\tcontainer_of(xprt, struct svcxprt_rdma, sc_xprt);\n\tstruct svc_rdma_recv_ctxt *rctxt = rqstp->rq_xprt_ctxt;\n\t__be32 *rdma_argp = rctxt->rc_recv_buf;\n\tstruct svc_rdma_send_ctxt *sctxt;\n\tunsigned int rc_size;\n\t__be32 *p;\n\tint ret;\n\n\tret = -ENOTCONN;\n\tif (svc_xprt_is_dead(xprt))\n\t\tgoto drop_connection;\n\n\tret = -ENOMEM;\n\tsctxt = svc_rdma_send_ctxt_get(rdma);\n\tif (!sctxt)\n\t\tgoto drop_connection;\n\n\tret = -EMSGSIZE;\n\tp = xdr_reserve_space(&sctxt->sc_stream,\n\t\t\t      rpcrdma_fixed_maxsz * sizeof(*p));\n\tif (!p)\n\t\tgoto put_ctxt;\n\n\tret = svc_rdma_send_reply_chunk(rdma, rctxt, &rqstp->rq_res);\n\tif (ret < 0)\n\t\tgoto reply_chunk;\n\trc_size = ret;\n\n\t*p++ = *rdma_argp;\n\t*p++ = *(rdma_argp + 1);\n\t*p++ = rdma->sc_fc_credits;\n\t*p = pcl_is_empty(&rctxt->rc_reply_pcl) ? rdma_msg : rdma_nomsg;\n\n\tret = svc_rdma_encode_read_list(sctxt);\n\tif (ret < 0)\n\t\tgoto put_ctxt;\n\tret = svc_rdma_encode_write_list(rctxt, sctxt);\n\tif (ret < 0)\n\t\tgoto put_ctxt;\n\tret = svc_rdma_encode_reply_chunk(rctxt, sctxt, rc_size);\n\tif (ret < 0)\n\t\tgoto put_ctxt;\n\n\tret = svc_rdma_send_reply_msg(rdma, sctxt, rctxt, rqstp);\n\tif (ret < 0)\n\t\tgoto put_ctxt;\n\treturn 0;\n\nreply_chunk:\n\tif (ret != -E2BIG && ret != -EINVAL)\n\t\tgoto put_ctxt;\n\n\t \n\tsvc_rdma_save_io_pages(rqstp, sctxt);\n\tsvc_rdma_send_error_msg(rdma, sctxt, rctxt, ret);\n\treturn 0;\n\nput_ctxt:\n\tsvc_rdma_send_ctxt_put(rdma, sctxt);\ndrop_connection:\n\ttrace_svcrdma_send_err(rqstp, ret);\n\tsvc_xprt_deferred_close(&rdma->sc_xprt);\n\treturn -ENOTCONN;\n}\n\n \nint svc_rdma_result_payload(struct svc_rqst *rqstp, unsigned int offset,\n\t\t\t    unsigned int length)\n{\n\tstruct svc_rdma_recv_ctxt *rctxt = rqstp->rq_xprt_ctxt;\n\tstruct svc_rdma_chunk *chunk;\n\tstruct svcxprt_rdma *rdma;\n\tstruct xdr_buf subbuf;\n\tint ret;\n\n\tchunk = rctxt->rc_cur_result_payload;\n\tif (!length || !chunk)\n\t\treturn 0;\n\trctxt->rc_cur_result_payload =\n\t\tpcl_next_chunk(&rctxt->rc_write_pcl, chunk);\n\tif (length > chunk->ch_length)\n\t\treturn -E2BIG;\n\n\tchunk->ch_position = offset;\n\tchunk->ch_payload_length = length;\n\n\tif (xdr_buf_subsegment(&rqstp->rq_res, &subbuf, offset, length))\n\t\treturn -EMSGSIZE;\n\n\trdma = container_of(rqstp->rq_xprt, struct svcxprt_rdma, sc_xprt);\n\tret = svc_rdma_send_write_chunk(rdma, chunk, &subbuf);\n\tif (ret < 0)\n\t\treturn ret;\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}