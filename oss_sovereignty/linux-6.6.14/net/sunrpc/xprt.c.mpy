{
  "module_name": "xprt.c",
  "hash_id": "6956ed93587923d25cbd0c76814b5713118fe3f79111ca0fe21c856c9d3f5688",
  "original_prompt": "Ingested from linux-6.6.14/net/sunrpc/xprt.c",
  "human_readable_source": "\n \n\n#include <linux/module.h>\n\n#include <linux/types.h>\n#include <linux/interrupt.h>\n#include <linux/workqueue.h>\n#include <linux/net.h>\n#include <linux/ktime.h>\n\n#include <linux/sunrpc/clnt.h>\n#include <linux/sunrpc/metrics.h>\n#include <linux/sunrpc/bc_xprt.h>\n#include <linux/rcupdate.h>\n#include <linux/sched/mm.h>\n\n#include <trace/events/sunrpc.h>\n\n#include \"sunrpc.h\"\n#include \"sysfs.h\"\n#include \"fail.h\"\n\n \n\n#if IS_ENABLED(CONFIG_SUNRPC_DEBUG)\n# define RPCDBG_FACILITY\tRPCDBG_XPRT\n#endif\n\n \nstatic void\txprt_init(struct rpc_xprt *xprt, struct net *net);\nstatic __be32\txprt_alloc_xid(struct rpc_xprt *xprt);\nstatic void\txprt_destroy(struct rpc_xprt *xprt);\nstatic void\txprt_request_init(struct rpc_task *task);\nstatic int\txprt_request_prepare(struct rpc_rqst *req, struct xdr_buf *buf);\n\nstatic DEFINE_SPINLOCK(xprt_list_lock);\nstatic LIST_HEAD(xprt_list);\n\nstatic unsigned long xprt_request_timeout(const struct rpc_rqst *req)\n{\n\tunsigned long timeout = jiffies + req->rq_timeout;\n\n\tif (time_before(timeout, req->rq_majortimeo))\n\t\treturn timeout;\n\treturn req->rq_majortimeo;\n}\n\n \nint xprt_register_transport(struct xprt_class *transport)\n{\n\tstruct xprt_class *t;\n\tint result;\n\n\tresult = -EEXIST;\n\tspin_lock(&xprt_list_lock);\n\tlist_for_each_entry(t, &xprt_list, list) {\n\t\t \n\t\tif (t->ident == transport->ident)\n\t\t\tgoto out;\n\t}\n\n\tlist_add_tail(&transport->list, &xprt_list);\n\tprintk(KERN_INFO \"RPC: Registered %s transport module.\\n\",\n\t       transport->name);\n\tresult = 0;\n\nout:\n\tspin_unlock(&xprt_list_lock);\n\treturn result;\n}\nEXPORT_SYMBOL_GPL(xprt_register_transport);\n\n \nint xprt_unregister_transport(struct xprt_class *transport)\n{\n\tstruct xprt_class *t;\n\tint result;\n\n\tresult = 0;\n\tspin_lock(&xprt_list_lock);\n\tlist_for_each_entry(t, &xprt_list, list) {\n\t\tif (t == transport) {\n\t\t\tprintk(KERN_INFO\n\t\t\t\t\"RPC: Unregistered %s transport module.\\n\",\n\t\t\t\ttransport->name);\n\t\t\tlist_del_init(&transport->list);\n\t\t\tgoto out;\n\t\t}\n\t}\n\tresult = -ENOENT;\n\nout:\n\tspin_unlock(&xprt_list_lock);\n\treturn result;\n}\nEXPORT_SYMBOL_GPL(xprt_unregister_transport);\n\nstatic void\nxprt_class_release(const struct xprt_class *t)\n{\n\tmodule_put(t->owner);\n}\n\nstatic const struct xprt_class *\nxprt_class_find_by_ident_locked(int ident)\n{\n\tconst struct xprt_class *t;\n\n\tlist_for_each_entry(t, &xprt_list, list) {\n\t\tif (t->ident != ident)\n\t\t\tcontinue;\n\t\tif (!try_module_get(t->owner))\n\t\t\tcontinue;\n\t\treturn t;\n\t}\n\treturn NULL;\n}\n\nstatic const struct xprt_class *\nxprt_class_find_by_ident(int ident)\n{\n\tconst struct xprt_class *t;\n\n\tspin_lock(&xprt_list_lock);\n\tt = xprt_class_find_by_ident_locked(ident);\n\tspin_unlock(&xprt_list_lock);\n\treturn t;\n}\n\nstatic const struct xprt_class *\nxprt_class_find_by_netid_locked(const char *netid)\n{\n\tconst struct xprt_class *t;\n\tunsigned int i;\n\n\tlist_for_each_entry(t, &xprt_list, list) {\n\t\tfor (i = 0; t->netid[i][0] != '\\0'; i++) {\n\t\t\tif (strcmp(t->netid[i], netid) != 0)\n\t\t\t\tcontinue;\n\t\t\tif (!try_module_get(t->owner))\n\t\t\t\tcontinue;\n\t\t\treturn t;\n\t\t}\n\t}\n\treturn NULL;\n}\n\nstatic const struct xprt_class *\nxprt_class_find_by_netid(const char *netid)\n{\n\tconst struct xprt_class *t;\n\n\tspin_lock(&xprt_list_lock);\n\tt = xprt_class_find_by_netid_locked(netid);\n\tif (!t) {\n\t\tspin_unlock(&xprt_list_lock);\n\t\trequest_module(\"rpc%s\", netid);\n\t\tspin_lock(&xprt_list_lock);\n\t\tt = xprt_class_find_by_netid_locked(netid);\n\t}\n\tspin_unlock(&xprt_list_lock);\n\treturn t;\n}\n\n \nint xprt_find_transport_ident(const char *netid)\n{\n\tconst struct xprt_class *t;\n\tint ret;\n\n\tt = xprt_class_find_by_netid(netid);\n\tif (!t)\n\t\treturn -ENOENT;\n\tret = t->ident;\n\txprt_class_release(t);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(xprt_find_transport_ident);\n\nstatic void xprt_clear_locked(struct rpc_xprt *xprt)\n{\n\txprt->snd_task = NULL;\n\tif (!test_bit(XPRT_CLOSE_WAIT, &xprt->state))\n\t\tclear_bit_unlock(XPRT_LOCKED, &xprt->state);\n\telse\n\t\tqueue_work(xprtiod_workqueue, &xprt->task_cleanup);\n}\n\n \nint xprt_reserve_xprt(struct rpc_xprt *xprt, struct rpc_task *task)\n{\n\tstruct rpc_rqst *req = task->tk_rqstp;\n\n\tif (test_and_set_bit(XPRT_LOCKED, &xprt->state)) {\n\t\tif (task == xprt->snd_task)\n\t\t\tgoto out_locked;\n\t\tgoto out_sleep;\n\t}\n\tif (test_bit(XPRT_WRITE_SPACE, &xprt->state))\n\t\tgoto out_unlock;\n\txprt->snd_task = task;\n\nout_locked:\n\ttrace_xprt_reserve_xprt(xprt, task);\n\treturn 1;\n\nout_unlock:\n\txprt_clear_locked(xprt);\nout_sleep:\n\ttask->tk_status = -EAGAIN;\n\tif  (RPC_IS_SOFT(task))\n\t\trpc_sleep_on_timeout(&xprt->sending, task, NULL,\n\t\t\t\txprt_request_timeout(req));\n\telse\n\t\trpc_sleep_on(&xprt->sending, task, NULL);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(xprt_reserve_xprt);\n\nstatic bool\nxprt_need_congestion_window_wait(struct rpc_xprt *xprt)\n{\n\treturn test_bit(XPRT_CWND_WAIT, &xprt->state);\n}\n\nstatic void\nxprt_set_congestion_window_wait(struct rpc_xprt *xprt)\n{\n\tif (!list_empty(&xprt->xmit_queue)) {\n\t\t \n\t\tif (list_first_entry(&xprt->xmit_queue, struct rpc_rqst,\n\t\t\t\t\trq_xmit)->rq_cong)\n\t\t\treturn;\n\t}\n\tset_bit(XPRT_CWND_WAIT, &xprt->state);\n}\n\nstatic void\nxprt_test_and_clear_congestion_window_wait(struct rpc_xprt *xprt)\n{\n\tif (!RPCXPRT_CONGESTED(xprt))\n\t\tclear_bit(XPRT_CWND_WAIT, &xprt->state);\n}\n\n \nint xprt_reserve_xprt_cong(struct rpc_xprt *xprt, struct rpc_task *task)\n{\n\tstruct rpc_rqst *req = task->tk_rqstp;\n\n\tif (test_and_set_bit(XPRT_LOCKED, &xprt->state)) {\n\t\tif (task == xprt->snd_task)\n\t\t\tgoto out_locked;\n\t\tgoto out_sleep;\n\t}\n\tif (req == NULL) {\n\t\txprt->snd_task = task;\n\t\tgoto out_locked;\n\t}\n\tif (test_bit(XPRT_WRITE_SPACE, &xprt->state))\n\t\tgoto out_unlock;\n\tif (!xprt_need_congestion_window_wait(xprt)) {\n\t\txprt->snd_task = task;\n\t\tgoto out_locked;\n\t}\nout_unlock:\n\txprt_clear_locked(xprt);\nout_sleep:\n\ttask->tk_status = -EAGAIN;\n\tif (RPC_IS_SOFT(task))\n\t\trpc_sleep_on_timeout(&xprt->sending, task, NULL,\n\t\t\t\txprt_request_timeout(req));\n\telse\n\t\trpc_sleep_on(&xprt->sending, task, NULL);\n\treturn 0;\nout_locked:\n\ttrace_xprt_reserve_cong(xprt, task);\n\treturn 1;\n}\nEXPORT_SYMBOL_GPL(xprt_reserve_xprt_cong);\n\nstatic inline int xprt_lock_write(struct rpc_xprt *xprt, struct rpc_task *task)\n{\n\tint retval;\n\n\tif (test_bit(XPRT_LOCKED, &xprt->state) && xprt->snd_task == task)\n\t\treturn 1;\n\tspin_lock(&xprt->transport_lock);\n\tretval = xprt->ops->reserve_xprt(xprt, task);\n\tspin_unlock(&xprt->transport_lock);\n\treturn retval;\n}\n\nstatic bool __xprt_lock_write_func(struct rpc_task *task, void *data)\n{\n\tstruct rpc_xprt *xprt = data;\n\n\txprt->snd_task = task;\n\treturn true;\n}\n\nstatic void __xprt_lock_write_next(struct rpc_xprt *xprt)\n{\n\tif (test_and_set_bit(XPRT_LOCKED, &xprt->state))\n\t\treturn;\n\tif (test_bit(XPRT_WRITE_SPACE, &xprt->state))\n\t\tgoto out_unlock;\n\tif (rpc_wake_up_first_on_wq(xprtiod_workqueue, &xprt->sending,\n\t\t\t\t__xprt_lock_write_func, xprt))\n\t\treturn;\nout_unlock:\n\txprt_clear_locked(xprt);\n}\n\nstatic void __xprt_lock_write_next_cong(struct rpc_xprt *xprt)\n{\n\tif (test_and_set_bit(XPRT_LOCKED, &xprt->state))\n\t\treturn;\n\tif (test_bit(XPRT_WRITE_SPACE, &xprt->state))\n\t\tgoto out_unlock;\n\tif (xprt_need_congestion_window_wait(xprt))\n\t\tgoto out_unlock;\n\tif (rpc_wake_up_first_on_wq(xprtiod_workqueue, &xprt->sending,\n\t\t\t\t__xprt_lock_write_func, xprt))\n\t\treturn;\nout_unlock:\n\txprt_clear_locked(xprt);\n}\n\n \nvoid xprt_release_xprt(struct rpc_xprt *xprt, struct rpc_task *task)\n{\n\tif (xprt->snd_task == task) {\n\t\txprt_clear_locked(xprt);\n\t\t__xprt_lock_write_next(xprt);\n\t}\n\ttrace_xprt_release_xprt(xprt, task);\n}\nEXPORT_SYMBOL_GPL(xprt_release_xprt);\n\n \nvoid xprt_release_xprt_cong(struct rpc_xprt *xprt, struct rpc_task *task)\n{\n\tif (xprt->snd_task == task) {\n\t\txprt_clear_locked(xprt);\n\t\t__xprt_lock_write_next_cong(xprt);\n\t}\n\ttrace_xprt_release_cong(xprt, task);\n}\nEXPORT_SYMBOL_GPL(xprt_release_xprt_cong);\n\nvoid xprt_release_write(struct rpc_xprt *xprt, struct rpc_task *task)\n{\n\tif (xprt->snd_task != task)\n\t\treturn;\n\tspin_lock(&xprt->transport_lock);\n\txprt->ops->release_xprt(xprt, task);\n\tspin_unlock(&xprt->transport_lock);\n}\n\n \nstatic int\n__xprt_get_cong(struct rpc_xprt *xprt, struct rpc_rqst *req)\n{\n\tif (req->rq_cong)\n\t\treturn 1;\n\ttrace_xprt_get_cong(xprt, req->rq_task);\n\tif (RPCXPRT_CONGESTED(xprt)) {\n\t\txprt_set_congestion_window_wait(xprt);\n\t\treturn 0;\n\t}\n\treq->rq_cong = 1;\n\txprt->cong += RPC_CWNDSCALE;\n\treturn 1;\n}\n\n \nstatic void\n__xprt_put_cong(struct rpc_xprt *xprt, struct rpc_rqst *req)\n{\n\tif (!req->rq_cong)\n\t\treturn;\n\treq->rq_cong = 0;\n\txprt->cong -= RPC_CWNDSCALE;\n\txprt_test_and_clear_congestion_window_wait(xprt);\n\ttrace_xprt_put_cong(xprt, req->rq_task);\n\t__xprt_lock_write_next_cong(xprt);\n}\n\n \nbool\nxprt_request_get_cong(struct rpc_xprt *xprt, struct rpc_rqst *req)\n{\n\tbool ret = false;\n\n\tif (req->rq_cong)\n\t\treturn true;\n\tspin_lock(&xprt->transport_lock);\n\tret = __xprt_get_cong(xprt, req) != 0;\n\tspin_unlock(&xprt->transport_lock);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(xprt_request_get_cong);\n\n \nvoid xprt_release_rqst_cong(struct rpc_task *task)\n{\n\tstruct rpc_rqst *req = task->tk_rqstp;\n\n\t__xprt_put_cong(req->rq_xprt, req);\n}\nEXPORT_SYMBOL_GPL(xprt_release_rqst_cong);\n\nstatic void xprt_clear_congestion_window_wait_locked(struct rpc_xprt *xprt)\n{\n\tif (test_and_clear_bit(XPRT_CWND_WAIT, &xprt->state))\n\t\t__xprt_lock_write_next_cong(xprt);\n}\n\n \nstatic void\nxprt_clear_congestion_window_wait(struct rpc_xprt *xprt)\n{\n\tif (test_and_clear_bit(XPRT_CWND_WAIT, &xprt->state)) {\n\t\tspin_lock(&xprt->transport_lock);\n\t\t__xprt_lock_write_next_cong(xprt);\n\t\tspin_unlock(&xprt->transport_lock);\n\t}\n}\n\n \nvoid xprt_adjust_cwnd(struct rpc_xprt *xprt, struct rpc_task *task, int result)\n{\n\tstruct rpc_rqst *req = task->tk_rqstp;\n\tunsigned long cwnd = xprt->cwnd;\n\n\tif (result >= 0 && cwnd <= xprt->cong) {\n\t\t \n\t\tcwnd += (RPC_CWNDSCALE * RPC_CWNDSCALE + (cwnd >> 1)) / cwnd;\n\t\tif (cwnd > RPC_MAXCWND(xprt))\n\t\t\tcwnd = RPC_MAXCWND(xprt);\n\t\t__xprt_lock_write_next_cong(xprt);\n\t} else if (result == -ETIMEDOUT) {\n\t\tcwnd >>= 1;\n\t\tif (cwnd < RPC_CWNDSCALE)\n\t\t\tcwnd = RPC_CWNDSCALE;\n\t}\n\tdprintk(\"RPC:       cong %ld, cwnd was %ld, now %ld\\n\",\n\t\t\txprt->cong, xprt->cwnd, cwnd);\n\txprt->cwnd = cwnd;\n\t__xprt_put_cong(xprt, req);\n}\nEXPORT_SYMBOL_GPL(xprt_adjust_cwnd);\n\n \nvoid xprt_wake_pending_tasks(struct rpc_xprt *xprt, int status)\n{\n\tif (status < 0)\n\t\trpc_wake_up_status(&xprt->pending, status);\n\telse\n\t\trpc_wake_up(&xprt->pending);\n}\nEXPORT_SYMBOL_GPL(xprt_wake_pending_tasks);\n\n \nvoid xprt_wait_for_buffer_space(struct rpc_xprt *xprt)\n{\n\tset_bit(XPRT_WRITE_SPACE, &xprt->state);\n}\nEXPORT_SYMBOL_GPL(xprt_wait_for_buffer_space);\n\nstatic bool\nxprt_clear_write_space_locked(struct rpc_xprt *xprt)\n{\n\tif (test_and_clear_bit(XPRT_WRITE_SPACE, &xprt->state)) {\n\t\t__xprt_lock_write_next(xprt);\n\t\tdprintk(\"RPC:       write space: waking waiting task on \"\n\t\t\t\t\"xprt %p\\n\", xprt);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n \nbool xprt_write_space(struct rpc_xprt *xprt)\n{\n\tbool ret;\n\n\tif (!test_bit(XPRT_WRITE_SPACE, &xprt->state))\n\t\treturn false;\n\tspin_lock(&xprt->transport_lock);\n\tret = xprt_clear_write_space_locked(xprt);\n\tspin_unlock(&xprt->transport_lock);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(xprt_write_space);\n\nstatic unsigned long xprt_abs_ktime_to_jiffies(ktime_t abstime)\n{\n\ts64 delta = ktime_to_ns(ktime_get() - abstime);\n\treturn likely(delta >= 0) ?\n\t\tjiffies - nsecs_to_jiffies(delta) :\n\t\tjiffies + nsecs_to_jiffies(-delta);\n}\n\nstatic unsigned long xprt_calc_majortimeo(struct rpc_rqst *req)\n{\n\tconst struct rpc_timeout *to = req->rq_task->tk_client->cl_timeout;\n\tunsigned long majortimeo = req->rq_timeout;\n\n\tif (to->to_exponential)\n\t\tmajortimeo <<= to->to_retries;\n\telse\n\t\tmajortimeo += to->to_increment * to->to_retries;\n\tif (majortimeo > to->to_maxval || majortimeo == 0)\n\t\tmajortimeo = to->to_maxval;\n\treturn majortimeo;\n}\n\nstatic void xprt_reset_majortimeo(struct rpc_rqst *req)\n{\n\treq->rq_majortimeo += xprt_calc_majortimeo(req);\n}\n\nstatic void xprt_reset_minortimeo(struct rpc_rqst *req)\n{\n\treq->rq_minortimeo += req->rq_timeout;\n}\n\nstatic void xprt_init_majortimeo(struct rpc_task *task, struct rpc_rqst *req)\n{\n\tunsigned long time_init;\n\tstruct rpc_xprt *xprt = req->rq_xprt;\n\n\tif (likely(xprt && xprt_connected(xprt)))\n\t\ttime_init = jiffies;\n\telse\n\t\ttime_init = xprt_abs_ktime_to_jiffies(task->tk_start);\n\treq->rq_timeout = task->tk_client->cl_timeout->to_initval;\n\treq->rq_majortimeo = time_init + xprt_calc_majortimeo(req);\n\treq->rq_minortimeo = time_init + req->rq_timeout;\n}\n\n \nint xprt_adjust_timeout(struct rpc_rqst *req)\n{\n\tstruct rpc_xprt *xprt = req->rq_xprt;\n\tconst struct rpc_timeout *to = req->rq_task->tk_client->cl_timeout;\n\tint status = 0;\n\n\tif (time_before(jiffies, req->rq_majortimeo)) {\n\t\tif (time_before(jiffies, req->rq_minortimeo))\n\t\t\treturn status;\n\t\tif (to->to_exponential)\n\t\t\treq->rq_timeout <<= 1;\n\t\telse\n\t\t\treq->rq_timeout += to->to_increment;\n\t\tif (to->to_maxval && req->rq_timeout >= to->to_maxval)\n\t\t\treq->rq_timeout = to->to_maxval;\n\t\treq->rq_retries++;\n\t} else {\n\t\treq->rq_timeout = to->to_initval;\n\t\treq->rq_retries = 0;\n\t\txprt_reset_majortimeo(req);\n\t\t \n\t\tspin_lock(&xprt->transport_lock);\n\t\trpc_init_rtt(req->rq_task->tk_client->cl_rtt, to->to_initval);\n\t\tspin_unlock(&xprt->transport_lock);\n\t\tstatus = -ETIMEDOUT;\n\t}\n\txprt_reset_minortimeo(req);\n\n\tif (req->rq_timeout == 0) {\n\t\tprintk(KERN_WARNING \"xprt_adjust_timeout: rq_timeout = 0!\\n\");\n\t\treq->rq_timeout = 5 * HZ;\n\t}\n\treturn status;\n}\n\nstatic void xprt_autoclose(struct work_struct *work)\n{\n\tstruct rpc_xprt *xprt =\n\t\tcontainer_of(work, struct rpc_xprt, task_cleanup);\n\tunsigned int pflags = memalloc_nofs_save();\n\n\ttrace_xprt_disconnect_auto(xprt);\n\txprt->connect_cookie++;\n\tsmp_mb__before_atomic();\n\tclear_bit(XPRT_CLOSE_WAIT, &xprt->state);\n\txprt->ops->close(xprt);\n\txprt_release_write(xprt, NULL);\n\twake_up_bit(&xprt->state, XPRT_LOCKED);\n\tmemalloc_nofs_restore(pflags);\n}\n\n \nvoid xprt_disconnect_done(struct rpc_xprt *xprt)\n{\n\ttrace_xprt_disconnect_done(xprt);\n\tspin_lock(&xprt->transport_lock);\n\txprt_clear_connected(xprt);\n\txprt_clear_write_space_locked(xprt);\n\txprt_clear_congestion_window_wait_locked(xprt);\n\txprt_wake_pending_tasks(xprt, -ENOTCONN);\n\tspin_unlock(&xprt->transport_lock);\n}\nEXPORT_SYMBOL_GPL(xprt_disconnect_done);\n\n \nstatic void xprt_schedule_autoclose_locked(struct rpc_xprt *xprt)\n{\n\tif (test_and_set_bit(XPRT_CLOSE_WAIT, &xprt->state))\n\t\treturn;\n\tif (test_and_set_bit(XPRT_LOCKED, &xprt->state) == 0)\n\t\tqueue_work(xprtiod_workqueue, &xprt->task_cleanup);\n\telse if (xprt->snd_task && !test_bit(XPRT_SND_IS_COOKIE, &xprt->state))\n\t\trpc_wake_up_queued_task_set_status(&xprt->pending,\n\t\t\t\t\t\t   xprt->snd_task, -ENOTCONN);\n}\n\n \nvoid xprt_force_disconnect(struct rpc_xprt *xprt)\n{\n\ttrace_xprt_disconnect_force(xprt);\n\n\t \n\tspin_lock(&xprt->transport_lock);\n\txprt_schedule_autoclose_locked(xprt);\n\tspin_unlock(&xprt->transport_lock);\n}\nEXPORT_SYMBOL_GPL(xprt_force_disconnect);\n\nstatic unsigned int\nxprt_connect_cookie(struct rpc_xprt *xprt)\n{\n\treturn READ_ONCE(xprt->connect_cookie);\n}\n\nstatic bool\nxprt_request_retransmit_after_disconnect(struct rpc_task *task)\n{\n\tstruct rpc_rqst *req = task->tk_rqstp;\n\tstruct rpc_xprt *xprt = req->rq_xprt;\n\n\treturn req->rq_connect_cookie != xprt_connect_cookie(xprt) ||\n\t\t!xprt_connected(xprt);\n}\n\n \nvoid xprt_conditional_disconnect(struct rpc_xprt *xprt, unsigned int cookie)\n{\n\t \n\tspin_lock(&xprt->transport_lock);\n\tif (cookie != xprt->connect_cookie)\n\t\tgoto out;\n\tif (test_bit(XPRT_CLOSING, &xprt->state))\n\t\tgoto out;\n\txprt_schedule_autoclose_locked(xprt);\nout:\n\tspin_unlock(&xprt->transport_lock);\n}\n\nstatic bool\nxprt_has_timer(const struct rpc_xprt *xprt)\n{\n\treturn xprt->idle_timeout != 0;\n}\n\nstatic void\nxprt_schedule_autodisconnect(struct rpc_xprt *xprt)\n\t__must_hold(&xprt->transport_lock)\n{\n\txprt->last_used = jiffies;\n\tif (RB_EMPTY_ROOT(&xprt->recv_queue) && xprt_has_timer(xprt))\n\t\tmod_timer(&xprt->timer, xprt->last_used + xprt->idle_timeout);\n}\n\nstatic void\nxprt_init_autodisconnect(struct timer_list *t)\n{\n\tstruct rpc_xprt *xprt = from_timer(xprt, t, timer);\n\n\tif (!RB_EMPTY_ROOT(&xprt->recv_queue))\n\t\treturn;\n\t \n\txprt->last_used = jiffies;\n\tif (test_and_set_bit(XPRT_LOCKED, &xprt->state))\n\t\treturn;\n\tqueue_work(xprtiod_workqueue, &xprt->task_cleanup);\n}\n\n#if IS_ENABLED(CONFIG_FAIL_SUNRPC)\nstatic void xprt_inject_disconnect(struct rpc_xprt *xprt)\n{\n\tif (!fail_sunrpc.ignore_client_disconnect &&\n\t    should_fail(&fail_sunrpc.attr, 1))\n\t\txprt->ops->inject_disconnect(xprt);\n}\n#else\nstatic inline void xprt_inject_disconnect(struct rpc_xprt *xprt)\n{\n}\n#endif\n\nbool xprt_lock_connect(struct rpc_xprt *xprt,\n\t\tstruct rpc_task *task,\n\t\tvoid *cookie)\n{\n\tbool ret = false;\n\n\tspin_lock(&xprt->transport_lock);\n\tif (!test_bit(XPRT_LOCKED, &xprt->state))\n\t\tgoto out;\n\tif (xprt->snd_task != task)\n\t\tgoto out;\n\tset_bit(XPRT_SND_IS_COOKIE, &xprt->state);\n\txprt->snd_task = cookie;\n\tret = true;\nout:\n\tspin_unlock(&xprt->transport_lock);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(xprt_lock_connect);\n\nvoid xprt_unlock_connect(struct rpc_xprt *xprt, void *cookie)\n{\n\tspin_lock(&xprt->transport_lock);\n\tif (xprt->snd_task != cookie)\n\t\tgoto out;\n\tif (!test_bit(XPRT_LOCKED, &xprt->state))\n\t\tgoto out;\n\txprt->snd_task =NULL;\n\tclear_bit(XPRT_SND_IS_COOKIE, &xprt->state);\n\txprt->ops->release_xprt(xprt, NULL);\n\txprt_schedule_autodisconnect(xprt);\nout:\n\tspin_unlock(&xprt->transport_lock);\n\twake_up_bit(&xprt->state, XPRT_LOCKED);\n}\nEXPORT_SYMBOL_GPL(xprt_unlock_connect);\n\n \nvoid xprt_connect(struct rpc_task *task)\n{\n\tstruct rpc_xprt\t*xprt = task->tk_rqstp->rq_xprt;\n\n\ttrace_xprt_connect(xprt);\n\n\tif (!xprt_bound(xprt)) {\n\t\ttask->tk_status = -EAGAIN;\n\t\treturn;\n\t}\n\tif (!xprt_lock_write(xprt, task))\n\t\treturn;\n\n\tif (!xprt_connected(xprt) && !test_bit(XPRT_CLOSE_WAIT, &xprt->state)) {\n\t\ttask->tk_rqstp->rq_connect_cookie = xprt->connect_cookie;\n\t\trpc_sleep_on_timeout(&xprt->pending, task, NULL,\n\t\t\t\txprt_request_timeout(task->tk_rqstp));\n\n\t\tif (test_bit(XPRT_CLOSING, &xprt->state))\n\t\t\treturn;\n\t\tif (xprt_test_and_set_connecting(xprt))\n\t\t\treturn;\n\t\t \n\t\tif (!xprt_connected(xprt)) {\n\t\t\txprt->stat.connect_start = jiffies;\n\t\t\txprt->ops->connect(xprt, task);\n\t\t} else {\n\t\t\txprt_clear_connecting(xprt);\n\t\t\ttask->tk_status = 0;\n\t\t\trpc_wake_up_queued_task(&xprt->pending, task);\n\t\t}\n\t}\n\txprt_release_write(xprt, task);\n}\n\n \nunsigned long xprt_reconnect_delay(const struct rpc_xprt *xprt)\n{\n\tunsigned long start, now = jiffies;\n\n\tstart = xprt->stat.connect_start + xprt->reestablish_timeout;\n\tif (time_after(start, now))\n\t\treturn start - now;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(xprt_reconnect_delay);\n\n \nvoid xprt_reconnect_backoff(struct rpc_xprt *xprt, unsigned long init_to)\n{\n\txprt->reestablish_timeout <<= 1;\n\tif (xprt->reestablish_timeout > xprt->max_reconnect_timeout)\n\t\txprt->reestablish_timeout = xprt->max_reconnect_timeout;\n\tif (xprt->reestablish_timeout < init_to)\n\t\txprt->reestablish_timeout = init_to;\n}\nEXPORT_SYMBOL_GPL(xprt_reconnect_backoff);\n\nenum xprt_xid_rb_cmp {\n\tXID_RB_EQUAL,\n\tXID_RB_LEFT,\n\tXID_RB_RIGHT,\n};\nstatic enum xprt_xid_rb_cmp\nxprt_xid_cmp(__be32 xid1, __be32 xid2)\n{\n\tif (xid1 == xid2)\n\t\treturn XID_RB_EQUAL;\n\tif ((__force u32)xid1 < (__force u32)xid2)\n\t\treturn XID_RB_LEFT;\n\treturn XID_RB_RIGHT;\n}\n\nstatic struct rpc_rqst *\nxprt_request_rb_find(struct rpc_xprt *xprt, __be32 xid)\n{\n\tstruct rb_node *n = xprt->recv_queue.rb_node;\n\tstruct rpc_rqst *req;\n\n\twhile (n != NULL) {\n\t\treq = rb_entry(n, struct rpc_rqst, rq_recv);\n\t\tswitch (xprt_xid_cmp(xid, req->rq_xid)) {\n\t\tcase XID_RB_LEFT:\n\t\t\tn = n->rb_left;\n\t\t\tbreak;\n\t\tcase XID_RB_RIGHT:\n\t\t\tn = n->rb_right;\n\t\t\tbreak;\n\t\tcase XID_RB_EQUAL:\n\t\t\treturn req;\n\t\t}\n\t}\n\treturn NULL;\n}\n\nstatic void\nxprt_request_rb_insert(struct rpc_xprt *xprt, struct rpc_rqst *new)\n{\n\tstruct rb_node **p = &xprt->recv_queue.rb_node;\n\tstruct rb_node *n = NULL;\n\tstruct rpc_rqst *req;\n\n\twhile (*p != NULL) {\n\t\tn = *p;\n\t\treq = rb_entry(n, struct rpc_rqst, rq_recv);\n\t\tswitch(xprt_xid_cmp(new->rq_xid, req->rq_xid)) {\n\t\tcase XID_RB_LEFT:\n\t\t\tp = &n->rb_left;\n\t\t\tbreak;\n\t\tcase XID_RB_RIGHT:\n\t\t\tp = &n->rb_right;\n\t\t\tbreak;\n\t\tcase XID_RB_EQUAL:\n\t\t\tWARN_ON_ONCE(new != req);\n\t\t\treturn;\n\t\t}\n\t}\n\trb_link_node(&new->rq_recv, n, p);\n\trb_insert_color(&new->rq_recv, &xprt->recv_queue);\n}\n\nstatic void\nxprt_request_rb_remove(struct rpc_xprt *xprt, struct rpc_rqst *req)\n{\n\trb_erase(&req->rq_recv, &xprt->recv_queue);\n}\n\n \nstruct rpc_rqst *xprt_lookup_rqst(struct rpc_xprt *xprt, __be32 xid)\n{\n\tstruct rpc_rqst *entry;\n\n\tentry = xprt_request_rb_find(xprt, xid);\n\tif (entry != NULL) {\n\t\ttrace_xprt_lookup_rqst(xprt, xid, 0);\n\t\tentry->rq_rtt = ktime_sub(ktime_get(), entry->rq_xtime);\n\t\treturn entry;\n\t}\n\n\tdprintk(\"RPC:       xprt_lookup_rqst did not find xid %08x\\n\",\n\t\t\tntohl(xid));\n\ttrace_xprt_lookup_rqst(xprt, xid, -ENOENT);\n\txprt->stat.bad_xids++;\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(xprt_lookup_rqst);\n\nstatic bool\nxprt_is_pinned_rqst(struct rpc_rqst *req)\n{\n\treturn atomic_read(&req->rq_pin) != 0;\n}\n\n \nvoid xprt_pin_rqst(struct rpc_rqst *req)\n{\n\tatomic_inc(&req->rq_pin);\n}\nEXPORT_SYMBOL_GPL(xprt_pin_rqst);\n\n \nvoid xprt_unpin_rqst(struct rpc_rqst *req)\n{\n\tif (!test_bit(RPC_TASK_MSG_PIN_WAIT, &req->rq_task->tk_runstate)) {\n\t\tatomic_dec(&req->rq_pin);\n\t\treturn;\n\t}\n\tif (atomic_dec_and_test(&req->rq_pin))\n\t\twake_up_var(&req->rq_pin);\n}\nEXPORT_SYMBOL_GPL(xprt_unpin_rqst);\n\nstatic void xprt_wait_on_pinned_rqst(struct rpc_rqst *req)\n{\n\twait_var_event(&req->rq_pin, !xprt_is_pinned_rqst(req));\n}\n\nstatic bool\nxprt_request_data_received(struct rpc_task *task)\n{\n\treturn !test_bit(RPC_TASK_NEED_RECV, &task->tk_runstate) &&\n\t\tREAD_ONCE(task->tk_rqstp->rq_reply_bytes_recvd) != 0;\n}\n\nstatic bool\nxprt_request_need_enqueue_receive(struct rpc_task *task, struct rpc_rqst *req)\n{\n\treturn !test_bit(RPC_TASK_NEED_RECV, &task->tk_runstate) &&\n\t\tREAD_ONCE(task->tk_rqstp->rq_reply_bytes_recvd) == 0;\n}\n\n \nint\nxprt_request_enqueue_receive(struct rpc_task *task)\n{\n\tstruct rpc_rqst *req = task->tk_rqstp;\n\tstruct rpc_xprt *xprt = req->rq_xprt;\n\tint ret;\n\n\tif (!xprt_request_need_enqueue_receive(task, req))\n\t\treturn 0;\n\n\tret = xprt_request_prepare(task->tk_rqstp, &req->rq_rcv_buf);\n\tif (ret)\n\t\treturn ret;\n\tspin_lock(&xprt->queue_lock);\n\n\t \n\tmemcpy(&req->rq_private_buf, &req->rq_rcv_buf,\n\t\t\tsizeof(req->rq_private_buf));\n\n\t \n\txprt_request_rb_insert(xprt, req);\n\tset_bit(RPC_TASK_NEED_RECV, &task->tk_runstate);\n\tspin_unlock(&xprt->queue_lock);\n\n\t \n\tdel_timer_sync(&xprt->timer);\n\treturn 0;\n}\n\n \nstatic void\nxprt_request_dequeue_receive_locked(struct rpc_task *task)\n{\n\tstruct rpc_rqst *req = task->tk_rqstp;\n\n\tif (test_and_clear_bit(RPC_TASK_NEED_RECV, &task->tk_runstate))\n\t\txprt_request_rb_remove(req->rq_xprt, req);\n}\n\n \nvoid xprt_update_rtt(struct rpc_task *task)\n{\n\tstruct rpc_rqst *req = task->tk_rqstp;\n\tstruct rpc_rtt *rtt = task->tk_client->cl_rtt;\n\tunsigned int timer = task->tk_msg.rpc_proc->p_timer;\n\tlong m = usecs_to_jiffies(ktime_to_us(req->rq_rtt));\n\n\tif (timer) {\n\t\tif (req->rq_ntrans == 1)\n\t\t\trpc_update_rtt(rtt, timer, m);\n\t\trpc_set_timeo(rtt, timer, req->rq_ntrans - 1);\n\t}\n}\nEXPORT_SYMBOL_GPL(xprt_update_rtt);\n\n \nvoid xprt_complete_rqst(struct rpc_task *task, int copied)\n{\n\tstruct rpc_rqst *req = task->tk_rqstp;\n\tstruct rpc_xprt *xprt = req->rq_xprt;\n\n\txprt->stat.recvs++;\n\n\txdr_free_bvec(&req->rq_rcv_buf);\n\treq->rq_private_buf.bvec = NULL;\n\treq->rq_private_buf.len = copied;\n\t \n\t \n\tsmp_wmb();\n\treq->rq_reply_bytes_recvd = copied;\n\txprt_request_dequeue_receive_locked(task);\n\trpc_wake_up_queued_task(&xprt->pending, task);\n}\nEXPORT_SYMBOL_GPL(xprt_complete_rqst);\n\nstatic void xprt_timer(struct rpc_task *task)\n{\n\tstruct rpc_rqst *req = task->tk_rqstp;\n\tstruct rpc_xprt *xprt = req->rq_xprt;\n\n\tif (task->tk_status != -ETIMEDOUT)\n\t\treturn;\n\n\ttrace_xprt_timer(xprt, req->rq_xid, task->tk_status);\n\tif (!req->rq_reply_bytes_recvd) {\n\t\tif (xprt->ops->timer)\n\t\t\txprt->ops->timer(xprt, task);\n\t} else\n\t\ttask->tk_status = 0;\n}\n\n \nvoid xprt_wait_for_reply_request_def(struct rpc_task *task)\n{\n\tstruct rpc_rqst *req = task->tk_rqstp;\n\n\trpc_sleep_on_timeout(&req->rq_xprt->pending, task, xprt_timer,\n\t\t\txprt_request_timeout(req));\n}\nEXPORT_SYMBOL_GPL(xprt_wait_for_reply_request_def);\n\n \nvoid xprt_wait_for_reply_request_rtt(struct rpc_task *task)\n{\n\tint timer = task->tk_msg.rpc_proc->p_timer;\n\tstruct rpc_clnt *clnt = task->tk_client;\n\tstruct rpc_rtt *rtt = clnt->cl_rtt;\n\tstruct rpc_rqst *req = task->tk_rqstp;\n\tunsigned long max_timeout = clnt->cl_timeout->to_maxval;\n\tunsigned long timeout;\n\n\ttimeout = rpc_calc_rto(rtt, timer);\n\ttimeout <<= rpc_ntimeo(rtt, timer) + req->rq_retries;\n\tif (timeout > max_timeout || timeout == 0)\n\t\ttimeout = max_timeout;\n\trpc_sleep_on_timeout(&req->rq_xprt->pending, task, xprt_timer,\n\t\t\tjiffies + timeout);\n}\nEXPORT_SYMBOL_GPL(xprt_wait_for_reply_request_rtt);\n\n \nvoid xprt_request_wait_receive(struct rpc_task *task)\n{\n\tstruct rpc_rqst *req = task->tk_rqstp;\n\tstruct rpc_xprt *xprt = req->rq_xprt;\n\n\tif (!test_bit(RPC_TASK_NEED_RECV, &task->tk_runstate))\n\t\treturn;\n\t \n\tspin_lock(&xprt->queue_lock);\n\tif (test_bit(RPC_TASK_NEED_RECV, &task->tk_runstate)) {\n\t\txprt->ops->wait_for_reply_request(task);\n\t\t \n\t\tif (xprt_request_retransmit_after_disconnect(task))\n\t\t\trpc_wake_up_queued_task_set_status(&xprt->pending,\n\t\t\t\t\ttask, -ENOTCONN);\n\t}\n\tspin_unlock(&xprt->queue_lock);\n}\n\nstatic bool\nxprt_request_need_enqueue_transmit(struct rpc_task *task, struct rpc_rqst *req)\n{\n\treturn !test_bit(RPC_TASK_NEED_XMIT, &task->tk_runstate);\n}\n\n \nvoid\nxprt_request_enqueue_transmit(struct rpc_task *task)\n{\n\tstruct rpc_rqst *pos, *req = task->tk_rqstp;\n\tstruct rpc_xprt *xprt = req->rq_xprt;\n\tint ret;\n\n\tif (xprt_request_need_enqueue_transmit(task, req)) {\n\t\tret = xprt_request_prepare(task->tk_rqstp, &req->rq_snd_buf);\n\t\tif (ret) {\n\t\t\ttask->tk_status = ret;\n\t\t\treturn;\n\t\t}\n\t\treq->rq_bytes_sent = 0;\n\t\tspin_lock(&xprt->queue_lock);\n\t\t \n\t\tif (req->rq_cong) {\n\t\t\txprt_clear_congestion_window_wait(xprt);\n\t\t\tlist_for_each_entry(pos, &xprt->xmit_queue, rq_xmit) {\n\t\t\t\tif (pos->rq_cong)\n\t\t\t\t\tcontinue;\n\t\t\t\t \n\t\t\t\tlist_add_tail(&req->rq_xmit, &pos->rq_xmit);\n\t\t\t\tINIT_LIST_HEAD(&req->rq_xmit2);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t} else if (!req->rq_seqno) {\n\t\t\tlist_for_each_entry(pos, &xprt->xmit_queue, rq_xmit) {\n\t\t\t\tif (pos->rq_task->tk_owner != task->tk_owner)\n\t\t\t\t\tcontinue;\n\t\t\t\tlist_add_tail(&req->rq_xmit2, &pos->rq_xmit2);\n\t\t\t\tINIT_LIST_HEAD(&req->rq_xmit);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\tlist_add_tail(&req->rq_xmit, &xprt->xmit_queue);\n\t\tINIT_LIST_HEAD(&req->rq_xmit2);\nout:\n\t\tatomic_long_inc(&xprt->xmit_queuelen);\n\t\tset_bit(RPC_TASK_NEED_XMIT, &task->tk_runstate);\n\t\tspin_unlock(&xprt->queue_lock);\n\t}\n}\n\n \nstatic void\nxprt_request_dequeue_transmit_locked(struct rpc_task *task)\n{\n\tstruct rpc_rqst *req = task->tk_rqstp;\n\n\tif (!test_and_clear_bit(RPC_TASK_NEED_XMIT, &task->tk_runstate))\n\t\treturn;\n\tif (!list_empty(&req->rq_xmit)) {\n\t\tlist_del(&req->rq_xmit);\n\t\tif (!list_empty(&req->rq_xmit2)) {\n\t\t\tstruct rpc_rqst *next = list_first_entry(&req->rq_xmit2,\n\t\t\t\t\tstruct rpc_rqst, rq_xmit2);\n\t\t\tlist_del(&req->rq_xmit2);\n\t\t\tlist_add_tail(&next->rq_xmit, &next->rq_xprt->xmit_queue);\n\t\t}\n\t} else\n\t\tlist_del(&req->rq_xmit2);\n\tatomic_long_dec(&req->rq_xprt->xmit_queuelen);\n\txdr_free_bvec(&req->rq_snd_buf);\n}\n\n \nstatic void\nxprt_request_dequeue_transmit(struct rpc_task *task)\n{\n\tstruct rpc_rqst *req = task->tk_rqstp;\n\tstruct rpc_xprt *xprt = req->rq_xprt;\n\n\tspin_lock(&xprt->queue_lock);\n\txprt_request_dequeue_transmit_locked(task);\n\tspin_unlock(&xprt->queue_lock);\n}\n\n \nvoid\nxprt_request_dequeue_xprt(struct rpc_task *task)\n{\n\tstruct rpc_rqst\t*req = task->tk_rqstp;\n\tstruct rpc_xprt *xprt = req->rq_xprt;\n\n\tif (test_bit(RPC_TASK_NEED_XMIT, &task->tk_runstate) ||\n\t    test_bit(RPC_TASK_NEED_RECV, &task->tk_runstate) ||\n\t    xprt_is_pinned_rqst(req)) {\n\t\tspin_lock(&xprt->queue_lock);\n\t\twhile (xprt_is_pinned_rqst(req)) {\n\t\t\tset_bit(RPC_TASK_MSG_PIN_WAIT, &task->tk_runstate);\n\t\t\tspin_unlock(&xprt->queue_lock);\n\t\t\txprt_wait_on_pinned_rqst(req);\n\t\t\tspin_lock(&xprt->queue_lock);\n\t\t\tclear_bit(RPC_TASK_MSG_PIN_WAIT, &task->tk_runstate);\n\t\t}\n\t\txprt_request_dequeue_transmit_locked(task);\n\t\txprt_request_dequeue_receive_locked(task);\n\t\tspin_unlock(&xprt->queue_lock);\n\t\txdr_free_bvec(&req->rq_rcv_buf);\n\t}\n}\n\n \nstatic int\nxprt_request_prepare(struct rpc_rqst *req, struct xdr_buf *buf)\n{\n\tstruct rpc_xprt *xprt = req->rq_xprt;\n\n\tif (xprt->ops->prepare_request)\n\t\treturn xprt->ops->prepare_request(req, buf);\n\treturn 0;\n}\n\n \nbool\nxprt_request_need_retransmit(struct rpc_task *task)\n{\n\treturn xprt_request_retransmit_after_disconnect(task);\n}\n\n \nbool xprt_prepare_transmit(struct rpc_task *task)\n{\n\tstruct rpc_rqst\t*req = task->tk_rqstp;\n\tstruct rpc_xprt\t*xprt = req->rq_xprt;\n\n\tif (!xprt_lock_write(xprt, task)) {\n\t\t \n\t\tif (!test_bit(RPC_TASK_NEED_XMIT, &task->tk_runstate))\n\t\t\trpc_wake_up_queued_task_set_status(&xprt->sending,\n\t\t\t\t\ttask, 0);\n\t\treturn false;\n\n\t}\n\tif (atomic_read(&xprt->swapper))\n\t\t \n\t\tcurrent->flags |= PF_MEMALLOC;\n\treturn true;\n}\n\nvoid xprt_end_transmit(struct rpc_task *task)\n{\n\tstruct rpc_xprt\t*xprt = task->tk_rqstp->rq_xprt;\n\n\txprt_inject_disconnect(xprt);\n\txprt_release_write(xprt, task);\n}\n\n \nstatic int\nxprt_request_transmit(struct rpc_rqst *req, struct rpc_task *snd_task)\n{\n\tstruct rpc_xprt *xprt = req->rq_xprt;\n\tstruct rpc_task *task = req->rq_task;\n\tunsigned int connect_cookie;\n\tint is_retrans = RPC_WAS_SENT(task);\n\tint status;\n\n\tif (!req->rq_bytes_sent) {\n\t\tif (xprt_request_data_received(task)) {\n\t\t\tstatus = 0;\n\t\t\tgoto out_dequeue;\n\t\t}\n\t\t \n\t\tif (rpcauth_xmit_need_reencode(task)) {\n\t\t\tstatus = -EBADMSG;\n\t\t\tgoto out_dequeue;\n\t\t}\n\t\tif (RPC_SIGNALLED(task)) {\n\t\t\tstatus = -ERESTARTSYS;\n\t\t\tgoto out_dequeue;\n\t\t}\n\t}\n\n\t \n\treq->rq_ntrans++;\n\n\ttrace_rpc_xdr_sendto(task, &req->rq_snd_buf);\n\tconnect_cookie = xprt->connect_cookie;\n\tstatus = xprt->ops->send_request(req);\n\tif (status != 0) {\n\t\treq->rq_ntrans--;\n\t\ttrace_xprt_transmit(req, status);\n\t\treturn status;\n\t}\n\n\tif (is_retrans) {\n\t\ttask->tk_client->cl_stats->rpcretrans++;\n\t\ttrace_xprt_retransmit(req);\n\t}\n\n\txprt_inject_disconnect(xprt);\n\n\ttask->tk_flags |= RPC_TASK_SENT;\n\tspin_lock(&xprt->transport_lock);\n\n\txprt->stat.sends++;\n\txprt->stat.req_u += xprt->stat.sends - xprt->stat.recvs;\n\txprt->stat.bklog_u += xprt->backlog.qlen;\n\txprt->stat.sending_u += xprt->sending.qlen;\n\txprt->stat.pending_u += xprt->pending.qlen;\n\tspin_unlock(&xprt->transport_lock);\n\n\treq->rq_connect_cookie = connect_cookie;\nout_dequeue:\n\ttrace_xprt_transmit(req, status);\n\txprt_request_dequeue_transmit(task);\n\trpc_wake_up_queued_task_set_status(&xprt->sending, task, status);\n\treturn status;\n}\n\n \nvoid\nxprt_transmit(struct rpc_task *task)\n{\n\tstruct rpc_rqst *next, *req = task->tk_rqstp;\n\tstruct rpc_xprt\t*xprt = req->rq_xprt;\n\tint status;\n\n\tspin_lock(&xprt->queue_lock);\n\tfor (;;) {\n\t\tnext = list_first_entry_or_null(&xprt->xmit_queue,\n\t\t\t\t\t\tstruct rpc_rqst, rq_xmit);\n\t\tif (!next)\n\t\t\tbreak;\n\t\txprt_pin_rqst(next);\n\t\tspin_unlock(&xprt->queue_lock);\n\t\tstatus = xprt_request_transmit(next, task);\n\t\tif (status == -EBADMSG && next != req)\n\t\t\tstatus = 0;\n\t\tspin_lock(&xprt->queue_lock);\n\t\txprt_unpin_rqst(next);\n\t\tif (status < 0) {\n\t\t\tif (test_bit(RPC_TASK_NEED_XMIT, &task->tk_runstate))\n\t\t\t\ttask->tk_status = status;\n\t\t\tbreak;\n\t\t}\n\t\t \n\t\tif (xprt_request_data_received(task) &&\n\t\t    !test_bit(RPC_TASK_NEED_XMIT, &task->tk_runstate))\n\t\t\tbreak;\n\t\tcond_resched_lock(&xprt->queue_lock);\n\t}\n\tspin_unlock(&xprt->queue_lock);\n}\n\nstatic void xprt_complete_request_init(struct rpc_task *task)\n{\n\tif (task->tk_rqstp)\n\t\txprt_request_init(task);\n}\n\nvoid xprt_add_backlog(struct rpc_xprt *xprt, struct rpc_task *task)\n{\n\tset_bit(XPRT_CONGESTED, &xprt->state);\n\trpc_sleep_on(&xprt->backlog, task, xprt_complete_request_init);\n}\nEXPORT_SYMBOL_GPL(xprt_add_backlog);\n\nstatic bool __xprt_set_rq(struct rpc_task *task, void *data)\n{\n\tstruct rpc_rqst *req = data;\n\n\tif (task->tk_rqstp == NULL) {\n\t\tmemset(req, 0, sizeof(*req));\t \n\t\ttask->tk_rqstp = req;\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nbool xprt_wake_up_backlog(struct rpc_xprt *xprt, struct rpc_rqst *req)\n{\n\tif (rpc_wake_up_first(&xprt->backlog, __xprt_set_rq, req) == NULL) {\n\t\tclear_bit(XPRT_CONGESTED, &xprt->state);\n\t\treturn false;\n\t}\n\treturn true;\n}\nEXPORT_SYMBOL_GPL(xprt_wake_up_backlog);\n\nstatic bool xprt_throttle_congested(struct rpc_xprt *xprt, struct rpc_task *task)\n{\n\tbool ret = false;\n\n\tif (!test_bit(XPRT_CONGESTED, &xprt->state))\n\t\tgoto out;\n\tspin_lock(&xprt->reserve_lock);\n\tif (test_bit(XPRT_CONGESTED, &xprt->state)) {\n\t\txprt_add_backlog(xprt, task);\n\t\tret = true;\n\t}\n\tspin_unlock(&xprt->reserve_lock);\nout:\n\treturn ret;\n}\n\nstatic struct rpc_rqst *xprt_dynamic_alloc_slot(struct rpc_xprt *xprt)\n{\n\tstruct rpc_rqst *req = ERR_PTR(-EAGAIN);\n\n\tif (xprt->num_reqs >= xprt->max_reqs)\n\t\tgoto out;\n\t++xprt->num_reqs;\n\tspin_unlock(&xprt->reserve_lock);\n\treq = kzalloc(sizeof(*req), rpc_task_gfp_mask());\n\tspin_lock(&xprt->reserve_lock);\n\tif (req != NULL)\n\t\tgoto out;\n\t--xprt->num_reqs;\n\treq = ERR_PTR(-ENOMEM);\nout:\n\treturn req;\n}\n\nstatic bool xprt_dynamic_free_slot(struct rpc_xprt *xprt, struct rpc_rqst *req)\n{\n\tif (xprt->num_reqs > xprt->min_reqs) {\n\t\t--xprt->num_reqs;\n\t\tkfree(req);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nvoid xprt_alloc_slot(struct rpc_xprt *xprt, struct rpc_task *task)\n{\n\tstruct rpc_rqst *req;\n\n\tspin_lock(&xprt->reserve_lock);\n\tif (!list_empty(&xprt->free)) {\n\t\treq = list_entry(xprt->free.next, struct rpc_rqst, rq_list);\n\t\tlist_del(&req->rq_list);\n\t\tgoto out_init_req;\n\t}\n\treq = xprt_dynamic_alloc_slot(xprt);\n\tif (!IS_ERR(req))\n\t\tgoto out_init_req;\n\tswitch (PTR_ERR(req)) {\n\tcase -ENOMEM:\n\t\tdprintk(\"RPC:       dynamic allocation of request slot \"\n\t\t\t\t\"failed! Retrying\\n\");\n\t\ttask->tk_status = -ENOMEM;\n\t\tbreak;\n\tcase -EAGAIN:\n\t\txprt_add_backlog(xprt, task);\n\t\tdprintk(\"RPC:       waiting for request slot\\n\");\n\t\tfallthrough;\n\tdefault:\n\t\ttask->tk_status = -EAGAIN;\n\t}\n\tspin_unlock(&xprt->reserve_lock);\n\treturn;\nout_init_req:\n\txprt->stat.max_slots = max_t(unsigned int, xprt->stat.max_slots,\n\t\t\t\t     xprt->num_reqs);\n\tspin_unlock(&xprt->reserve_lock);\n\n\ttask->tk_status = 0;\n\ttask->tk_rqstp = req;\n}\nEXPORT_SYMBOL_GPL(xprt_alloc_slot);\n\nvoid xprt_free_slot(struct rpc_xprt *xprt, struct rpc_rqst *req)\n{\n\tspin_lock(&xprt->reserve_lock);\n\tif (!xprt_wake_up_backlog(xprt, req) &&\n\t    !xprt_dynamic_free_slot(xprt, req)) {\n\t\tmemset(req, 0, sizeof(*req));\t \n\t\tlist_add(&req->rq_list, &xprt->free);\n\t}\n\tspin_unlock(&xprt->reserve_lock);\n}\nEXPORT_SYMBOL_GPL(xprt_free_slot);\n\nstatic void xprt_free_all_slots(struct rpc_xprt *xprt)\n{\n\tstruct rpc_rqst *req;\n\twhile (!list_empty(&xprt->free)) {\n\t\treq = list_first_entry(&xprt->free, struct rpc_rqst, rq_list);\n\t\tlist_del(&req->rq_list);\n\t\tkfree(req);\n\t}\n}\n\nstatic DEFINE_IDA(rpc_xprt_ids);\n\nvoid xprt_cleanup_ids(void)\n{\n\tida_destroy(&rpc_xprt_ids);\n}\n\nstatic int xprt_alloc_id(struct rpc_xprt *xprt)\n{\n\tint id;\n\n\tid = ida_alloc(&rpc_xprt_ids, GFP_KERNEL);\n\tif (id < 0)\n\t\treturn id;\n\n\txprt->id = id;\n\treturn 0;\n}\n\nstatic void xprt_free_id(struct rpc_xprt *xprt)\n{\n\tida_free(&rpc_xprt_ids, xprt->id);\n}\n\nstruct rpc_xprt *xprt_alloc(struct net *net, size_t size,\n\t\tunsigned int num_prealloc,\n\t\tunsigned int max_alloc)\n{\n\tstruct rpc_xprt *xprt;\n\tstruct rpc_rqst *req;\n\tint i;\n\n\txprt = kzalloc(size, GFP_KERNEL);\n\tif (xprt == NULL)\n\t\tgoto out;\n\n\txprt_alloc_id(xprt);\n\txprt_init(xprt, net);\n\n\tfor (i = 0; i < num_prealloc; i++) {\n\t\treq = kzalloc(sizeof(struct rpc_rqst), GFP_KERNEL);\n\t\tif (!req)\n\t\t\tgoto out_free;\n\t\tlist_add(&req->rq_list, &xprt->free);\n\t}\n\txprt->max_reqs = max_t(unsigned int, max_alloc, num_prealloc);\n\txprt->min_reqs = num_prealloc;\n\txprt->num_reqs = num_prealloc;\n\n\treturn xprt;\n\nout_free:\n\txprt_free(xprt);\nout:\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(xprt_alloc);\n\nvoid xprt_free(struct rpc_xprt *xprt)\n{\n\tput_net_track(xprt->xprt_net, &xprt->ns_tracker);\n\txprt_free_all_slots(xprt);\n\txprt_free_id(xprt);\n\trpc_sysfs_xprt_destroy(xprt);\n\tkfree_rcu(xprt, rcu);\n}\nEXPORT_SYMBOL_GPL(xprt_free);\n\nstatic void\nxprt_init_connect_cookie(struct rpc_rqst *req, struct rpc_xprt *xprt)\n{\n\treq->rq_connect_cookie = xprt_connect_cookie(xprt) - 1;\n}\n\nstatic __be32\nxprt_alloc_xid(struct rpc_xprt *xprt)\n{\n\t__be32 xid;\n\n\tspin_lock(&xprt->reserve_lock);\n\txid = (__force __be32)xprt->xid++;\n\tspin_unlock(&xprt->reserve_lock);\n\treturn xid;\n}\n\nstatic void\nxprt_init_xid(struct rpc_xprt *xprt)\n{\n\txprt->xid = get_random_u32();\n}\n\nstatic void\nxprt_request_init(struct rpc_task *task)\n{\n\tstruct rpc_xprt *xprt = task->tk_xprt;\n\tstruct rpc_rqst\t*req = task->tk_rqstp;\n\n\treq->rq_task\t= task;\n\treq->rq_xprt    = xprt;\n\treq->rq_buffer  = NULL;\n\treq->rq_xid\t= xprt_alloc_xid(xprt);\n\txprt_init_connect_cookie(req, xprt);\n\treq->rq_snd_buf.len = 0;\n\treq->rq_snd_buf.buflen = 0;\n\treq->rq_rcv_buf.len = 0;\n\treq->rq_rcv_buf.buflen = 0;\n\treq->rq_snd_buf.bvec = NULL;\n\treq->rq_rcv_buf.bvec = NULL;\n\treq->rq_release_snd_buf = NULL;\n\txprt_init_majortimeo(task, req);\n\n\ttrace_xprt_reserve(req);\n}\n\nstatic void\nxprt_do_reserve(struct rpc_xprt *xprt, struct rpc_task *task)\n{\n\txprt->ops->alloc_slot(xprt, task);\n\tif (task->tk_rqstp != NULL)\n\t\txprt_request_init(task);\n}\n\n \nvoid xprt_reserve(struct rpc_task *task)\n{\n\tstruct rpc_xprt *xprt = task->tk_xprt;\n\n\ttask->tk_status = 0;\n\tif (task->tk_rqstp != NULL)\n\t\treturn;\n\n\ttask->tk_status = -EAGAIN;\n\tif (!xprt_throttle_congested(xprt, task))\n\t\txprt_do_reserve(xprt, task);\n}\n\n \nvoid xprt_retry_reserve(struct rpc_task *task)\n{\n\tstruct rpc_xprt *xprt = task->tk_xprt;\n\n\ttask->tk_status = 0;\n\tif (task->tk_rqstp != NULL)\n\t\treturn;\n\n\ttask->tk_status = -EAGAIN;\n\txprt_do_reserve(xprt, task);\n}\n\n \nvoid xprt_release(struct rpc_task *task)\n{\n\tstruct rpc_xprt\t*xprt;\n\tstruct rpc_rqst\t*req = task->tk_rqstp;\n\n\tif (req == NULL) {\n\t\tif (task->tk_client) {\n\t\t\txprt = task->tk_xprt;\n\t\t\txprt_release_write(xprt, task);\n\t\t}\n\t\treturn;\n\t}\n\n\txprt = req->rq_xprt;\n\txprt_request_dequeue_xprt(task);\n\tspin_lock(&xprt->transport_lock);\n\txprt->ops->release_xprt(xprt, task);\n\tif (xprt->ops->release_request)\n\t\txprt->ops->release_request(task);\n\txprt_schedule_autodisconnect(xprt);\n\tspin_unlock(&xprt->transport_lock);\n\tif (req->rq_buffer)\n\t\txprt->ops->buf_free(task);\n\tif (req->rq_cred != NULL)\n\t\tput_rpccred(req->rq_cred);\n\tif (req->rq_release_snd_buf)\n\t\treq->rq_release_snd_buf(req);\n\n\ttask->tk_rqstp = NULL;\n\tif (likely(!bc_prealloc(req)))\n\t\txprt->ops->free_slot(xprt, req);\n\telse\n\t\txprt_free_bc_request(req);\n}\n\n#ifdef CONFIG_SUNRPC_BACKCHANNEL\nvoid\nxprt_init_bc_request(struct rpc_rqst *req, struct rpc_task *task)\n{\n\tstruct xdr_buf *xbufp = &req->rq_snd_buf;\n\n\ttask->tk_rqstp = req;\n\treq->rq_task = task;\n\txprt_init_connect_cookie(req, req->rq_xprt);\n\t \n\txbufp->len = xbufp->head[0].iov_len + xbufp->page_len +\n\t\txbufp->tail[0].iov_len;\n}\n#endif\n\nstatic void xprt_init(struct rpc_xprt *xprt, struct net *net)\n{\n\tkref_init(&xprt->kref);\n\n\tspin_lock_init(&xprt->transport_lock);\n\tspin_lock_init(&xprt->reserve_lock);\n\tspin_lock_init(&xprt->queue_lock);\n\n\tINIT_LIST_HEAD(&xprt->free);\n\txprt->recv_queue = RB_ROOT;\n\tINIT_LIST_HEAD(&xprt->xmit_queue);\n#if defined(CONFIG_SUNRPC_BACKCHANNEL)\n\tspin_lock_init(&xprt->bc_pa_lock);\n\tINIT_LIST_HEAD(&xprt->bc_pa_list);\n#endif  \n\tINIT_LIST_HEAD(&xprt->xprt_switch);\n\n\txprt->last_used = jiffies;\n\txprt->cwnd = RPC_INITCWND;\n\txprt->bind_index = 0;\n\n\trpc_init_wait_queue(&xprt->binding, \"xprt_binding\");\n\trpc_init_wait_queue(&xprt->pending, \"xprt_pending\");\n\trpc_init_wait_queue(&xprt->sending, \"xprt_sending\");\n\trpc_init_priority_wait_queue(&xprt->backlog, \"xprt_backlog\");\n\n\txprt_init_xid(xprt);\n\n\txprt->xprt_net = get_net_track(net, &xprt->ns_tracker, GFP_KERNEL);\n}\n\n \nstruct rpc_xprt *xprt_create_transport(struct xprt_create *args)\n{\n\tstruct rpc_xprt\t*xprt;\n\tconst struct xprt_class *t;\n\n\tt = xprt_class_find_by_ident(args->ident);\n\tif (!t) {\n\t\tdprintk(\"RPC: transport (%d) not supported\\n\", args->ident);\n\t\treturn ERR_PTR(-EIO);\n\t}\n\n\txprt = t->setup(args);\n\txprt_class_release(t);\n\n\tif (IS_ERR(xprt))\n\t\tgoto out;\n\tif (args->flags & XPRT_CREATE_NO_IDLE_TIMEOUT)\n\t\txprt->idle_timeout = 0;\n\tINIT_WORK(&xprt->task_cleanup, xprt_autoclose);\n\tif (xprt_has_timer(xprt))\n\t\ttimer_setup(&xprt->timer, xprt_init_autodisconnect, 0);\n\telse\n\t\ttimer_setup(&xprt->timer, NULL, 0);\n\n\tif (strlen(args->servername) > RPC_MAXNETNAMELEN) {\n\t\txprt_destroy(xprt);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\txprt->servername = kstrdup(args->servername, GFP_KERNEL);\n\tif (xprt->servername == NULL) {\n\t\txprt_destroy(xprt);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\trpc_xprt_debugfs_register(xprt);\n\n\ttrace_xprt_create(xprt);\nout:\n\treturn xprt;\n}\n\nstatic void xprt_destroy_cb(struct work_struct *work)\n{\n\tstruct rpc_xprt *xprt =\n\t\tcontainer_of(work, struct rpc_xprt, task_cleanup);\n\n\ttrace_xprt_destroy(xprt);\n\n\trpc_xprt_debugfs_unregister(xprt);\n\trpc_destroy_wait_queue(&xprt->binding);\n\trpc_destroy_wait_queue(&xprt->pending);\n\trpc_destroy_wait_queue(&xprt->sending);\n\trpc_destroy_wait_queue(&xprt->backlog);\n\tkfree(xprt->servername);\n\t \n\txprt_destroy_backchannel(xprt, UINT_MAX);\n\n\t \n\txprt->ops->destroy(xprt);\n}\n\n \nstatic void xprt_destroy(struct rpc_xprt *xprt)\n{\n\t \n\twait_on_bit_lock(&xprt->state, XPRT_LOCKED, TASK_UNINTERRUPTIBLE);\n\n\t \n\tspin_lock(&xprt->transport_lock);\n\tdel_timer_sync(&xprt->timer);\n\tspin_unlock(&xprt->transport_lock);\n\n\t \n\tINIT_WORK(&xprt->task_cleanup, xprt_destroy_cb);\n\tschedule_work(&xprt->task_cleanup);\n}\n\nstatic void xprt_destroy_kref(struct kref *kref)\n{\n\txprt_destroy(container_of(kref, struct rpc_xprt, kref));\n}\n\n \nstruct rpc_xprt *xprt_get(struct rpc_xprt *xprt)\n{\n\tif (xprt != NULL && kref_get_unless_zero(&xprt->kref))\n\t\treturn xprt;\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(xprt_get);\n\n \nvoid xprt_put(struct rpc_xprt *xprt)\n{\n\tif (xprt != NULL)\n\t\tkref_put(&xprt->kref, xprt_destroy_kref);\n}\nEXPORT_SYMBOL_GPL(xprt_put);\n\nvoid xprt_set_offline_locked(struct rpc_xprt *xprt, struct rpc_xprt_switch *xps)\n{\n\tif (!test_and_set_bit(XPRT_OFFLINE, &xprt->state)) {\n\t\tspin_lock(&xps->xps_lock);\n\t\txps->xps_nactive--;\n\t\tspin_unlock(&xps->xps_lock);\n\t}\n}\n\nvoid xprt_set_online_locked(struct rpc_xprt *xprt, struct rpc_xprt_switch *xps)\n{\n\tif (test_and_clear_bit(XPRT_OFFLINE, &xprt->state)) {\n\t\tspin_lock(&xps->xps_lock);\n\t\txps->xps_nactive++;\n\t\tspin_unlock(&xps->xps_lock);\n\t}\n}\n\nvoid xprt_delete_locked(struct rpc_xprt *xprt, struct rpc_xprt_switch *xps)\n{\n\tif (test_and_set_bit(XPRT_REMOVE, &xprt->state))\n\t\treturn;\n\n\txprt_force_disconnect(xprt);\n\tif (!test_bit(XPRT_CONNECTED, &xprt->state))\n\t\treturn;\n\n\tif (!xprt->sending.qlen && !xprt->pending.qlen &&\n\t    !xprt->backlog.qlen && !atomic_long_read(&xprt->queuelen))\n\t\trpc_xprt_switch_remove_xprt(xps, xprt, true);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}