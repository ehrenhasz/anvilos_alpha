{
  "module_name": "svc_xprt.c",
  "hash_id": "ab549b62213db1a7998fb1d03d7a2a4750c1273476afb02d6a182ec1f1a5205f",
  "original_prompt": "Ingested from linux-6.6.14/net/sunrpc/svc_xprt.c",
  "human_readable_source": "\n \n\n#include <linux/sched.h>\n#include <linux/sched/mm.h>\n#include <linux/errno.h>\n#include <linux/freezer.h>\n#include <linux/kthread.h>\n#include <linux/slab.h>\n#include <net/sock.h>\n#include <linux/sunrpc/addr.h>\n#include <linux/sunrpc/stats.h>\n#include <linux/sunrpc/svc_xprt.h>\n#include <linux/sunrpc/svcsock.h>\n#include <linux/sunrpc/xprt.h>\n#include <linux/module.h>\n#include <linux/netdevice.h>\n#include <trace/events/sunrpc.h>\n\n#define RPCDBG_FACILITY\tRPCDBG_SVCXPRT\n\nstatic unsigned int svc_rpc_per_connection_limit __read_mostly;\nmodule_param(svc_rpc_per_connection_limit, uint, 0644);\n\n\nstatic struct svc_deferred_req *svc_deferred_dequeue(struct svc_xprt *xprt);\nstatic int svc_deferred_recv(struct svc_rqst *rqstp);\nstatic struct cache_deferred_req *svc_defer(struct cache_req *req);\nstatic void svc_age_temp_xprts(struct timer_list *t);\nstatic void svc_delete_xprt(struct svc_xprt *xprt);\n\n \nstatic int svc_conn_age_period = 6*60;\n\n \nstatic DEFINE_SPINLOCK(svc_xprt_class_lock);\nstatic LIST_HEAD(svc_xprt_class_list);\n\n \n\n \nint svc_reg_xprt_class(struct svc_xprt_class *xcl)\n{\n\tstruct svc_xprt_class *cl;\n\tint res = -EEXIST;\n\n\tINIT_LIST_HEAD(&xcl->xcl_list);\n\tspin_lock(&svc_xprt_class_lock);\n\t \n\tlist_for_each_entry(cl, &svc_xprt_class_list, xcl_list) {\n\t\tif (strcmp(xcl->xcl_name, cl->xcl_name) == 0)\n\t\t\tgoto out;\n\t}\n\tlist_add_tail(&xcl->xcl_list, &svc_xprt_class_list);\n\tres = 0;\nout:\n\tspin_unlock(&svc_xprt_class_lock);\n\treturn res;\n}\nEXPORT_SYMBOL_GPL(svc_reg_xprt_class);\n\n \nvoid svc_unreg_xprt_class(struct svc_xprt_class *xcl)\n{\n\tspin_lock(&svc_xprt_class_lock);\n\tlist_del_init(&xcl->xcl_list);\n\tspin_unlock(&svc_xprt_class_lock);\n}\nEXPORT_SYMBOL_GPL(svc_unreg_xprt_class);\n\n \nint svc_print_xprts(char *buf, int maxlen)\n{\n\tstruct svc_xprt_class *xcl;\n\tchar tmpstr[80];\n\tint len = 0;\n\tbuf[0] = '\\0';\n\n\tspin_lock(&svc_xprt_class_lock);\n\tlist_for_each_entry(xcl, &svc_xprt_class_list, xcl_list) {\n\t\tint slen;\n\n\t\tslen = snprintf(tmpstr, sizeof(tmpstr), \"%s %d\\n\",\n\t\t\t\txcl->xcl_name, xcl->xcl_max_payload);\n\t\tif (slen >= sizeof(tmpstr) || len + slen >= maxlen)\n\t\t\tbreak;\n\t\tlen += slen;\n\t\tstrcat(buf, tmpstr);\n\t}\n\tspin_unlock(&svc_xprt_class_lock);\n\n\treturn len;\n}\n\n \nvoid svc_xprt_deferred_close(struct svc_xprt *xprt)\n{\n\tif (!test_and_set_bit(XPT_CLOSE, &xprt->xpt_flags))\n\t\tsvc_xprt_enqueue(xprt);\n}\nEXPORT_SYMBOL_GPL(svc_xprt_deferred_close);\n\nstatic void svc_xprt_free(struct kref *kref)\n{\n\tstruct svc_xprt *xprt =\n\t\tcontainer_of(kref, struct svc_xprt, xpt_ref);\n\tstruct module *owner = xprt->xpt_class->xcl_owner;\n\tif (test_bit(XPT_CACHE_AUTH, &xprt->xpt_flags))\n\t\tsvcauth_unix_info_release(xprt);\n\tput_cred(xprt->xpt_cred);\n\tput_net_track(xprt->xpt_net, &xprt->ns_tracker);\n\t \n\tif (xprt->xpt_bc_xprt)\n\t\txprt_put(xprt->xpt_bc_xprt);\n\tif (xprt->xpt_bc_xps)\n\t\txprt_switch_put(xprt->xpt_bc_xps);\n\ttrace_svc_xprt_free(xprt);\n\txprt->xpt_ops->xpo_free(xprt);\n\tmodule_put(owner);\n}\n\nvoid svc_xprt_put(struct svc_xprt *xprt)\n{\n\tkref_put(&xprt->xpt_ref, svc_xprt_free);\n}\nEXPORT_SYMBOL_GPL(svc_xprt_put);\n\n \nvoid svc_xprt_init(struct net *net, struct svc_xprt_class *xcl,\n\t\t   struct svc_xprt *xprt, struct svc_serv *serv)\n{\n\tmemset(xprt, 0, sizeof(*xprt));\n\txprt->xpt_class = xcl;\n\txprt->xpt_ops = xcl->xcl_ops;\n\tkref_init(&xprt->xpt_ref);\n\txprt->xpt_server = serv;\n\tINIT_LIST_HEAD(&xprt->xpt_list);\n\tINIT_LIST_HEAD(&xprt->xpt_ready);\n\tINIT_LIST_HEAD(&xprt->xpt_deferred);\n\tINIT_LIST_HEAD(&xprt->xpt_users);\n\tmutex_init(&xprt->xpt_mutex);\n\tspin_lock_init(&xprt->xpt_lock);\n\tset_bit(XPT_BUSY, &xprt->xpt_flags);\n\txprt->xpt_net = get_net_track(net, &xprt->ns_tracker, GFP_ATOMIC);\n\tstrcpy(xprt->xpt_remotebuf, \"uninitialized\");\n}\nEXPORT_SYMBOL_GPL(svc_xprt_init);\n\nstatic struct svc_xprt *__svc_xpo_create(struct svc_xprt_class *xcl,\n\t\t\t\t\t struct svc_serv *serv,\n\t\t\t\t\t struct net *net,\n\t\t\t\t\t const int family,\n\t\t\t\t\t const unsigned short port,\n\t\t\t\t\t int flags)\n{\n\tstruct sockaddr_in sin = {\n\t\t.sin_family\t\t= AF_INET,\n\t\t.sin_addr.s_addr\t= htonl(INADDR_ANY),\n\t\t.sin_port\t\t= htons(port),\n\t};\n#if IS_ENABLED(CONFIG_IPV6)\n\tstruct sockaddr_in6 sin6 = {\n\t\t.sin6_family\t\t= AF_INET6,\n\t\t.sin6_addr\t\t= IN6ADDR_ANY_INIT,\n\t\t.sin6_port\t\t= htons(port),\n\t};\n#endif\n\tstruct svc_xprt *xprt;\n\tstruct sockaddr *sap;\n\tsize_t len;\n\n\tswitch (family) {\n\tcase PF_INET:\n\t\tsap = (struct sockaddr *)&sin;\n\t\tlen = sizeof(sin);\n\t\tbreak;\n#if IS_ENABLED(CONFIG_IPV6)\n\tcase PF_INET6:\n\t\tsap = (struct sockaddr *)&sin6;\n\t\tlen = sizeof(sin6);\n\t\tbreak;\n#endif\n\tdefault:\n\t\treturn ERR_PTR(-EAFNOSUPPORT);\n\t}\n\n\txprt = xcl->xcl_ops->xpo_create(serv, net, sap, len, flags);\n\tif (IS_ERR(xprt))\n\t\ttrace_svc_xprt_create_err(serv->sv_program->pg_name,\n\t\t\t\t\t  xcl->xcl_name, sap, len, xprt);\n\treturn xprt;\n}\n\n \nvoid svc_xprt_received(struct svc_xprt *xprt)\n{\n\tif (!test_bit(XPT_BUSY, &xprt->xpt_flags)) {\n\t\tWARN_ONCE(1, \"xprt=0x%p already busy!\", xprt);\n\t\treturn;\n\t}\n\n\t \n\tsvc_xprt_get(xprt);\n\tsmp_mb__before_atomic();\n\tclear_bit(XPT_BUSY, &xprt->xpt_flags);\n\tsvc_xprt_enqueue(xprt);\n\tsvc_xprt_put(xprt);\n}\nEXPORT_SYMBOL_GPL(svc_xprt_received);\n\nvoid svc_add_new_perm_xprt(struct svc_serv *serv, struct svc_xprt *new)\n{\n\tclear_bit(XPT_TEMP, &new->xpt_flags);\n\tspin_lock_bh(&serv->sv_lock);\n\tlist_add(&new->xpt_list, &serv->sv_permsocks);\n\tspin_unlock_bh(&serv->sv_lock);\n\tsvc_xprt_received(new);\n}\n\nstatic int _svc_xprt_create(struct svc_serv *serv, const char *xprt_name,\n\t\t\t    struct net *net, const int family,\n\t\t\t    const unsigned short port, int flags,\n\t\t\t    const struct cred *cred)\n{\n\tstruct svc_xprt_class *xcl;\n\n\tspin_lock(&svc_xprt_class_lock);\n\tlist_for_each_entry(xcl, &svc_xprt_class_list, xcl_list) {\n\t\tstruct svc_xprt *newxprt;\n\t\tunsigned short newport;\n\n\t\tif (strcmp(xprt_name, xcl->xcl_name))\n\t\t\tcontinue;\n\n\t\tif (!try_module_get(xcl->xcl_owner))\n\t\t\tgoto err;\n\n\t\tspin_unlock(&svc_xprt_class_lock);\n\t\tnewxprt = __svc_xpo_create(xcl, serv, net, family, port, flags);\n\t\tif (IS_ERR(newxprt)) {\n\t\t\tmodule_put(xcl->xcl_owner);\n\t\t\treturn PTR_ERR(newxprt);\n\t\t}\n\t\tnewxprt->xpt_cred = get_cred(cred);\n\t\tsvc_add_new_perm_xprt(serv, newxprt);\n\t\tnewport = svc_xprt_local_port(newxprt);\n\t\treturn newport;\n\t}\n err:\n\tspin_unlock(&svc_xprt_class_lock);\n\t \n\treturn -EPROTONOSUPPORT;\n}\n\n \nint svc_xprt_create(struct svc_serv *serv, const char *xprt_name,\n\t\t    struct net *net, const int family,\n\t\t    const unsigned short port, int flags,\n\t\t    const struct cred *cred)\n{\n\tint err;\n\n\terr = _svc_xprt_create(serv, xprt_name, net, family, port, flags, cred);\n\tif (err == -EPROTONOSUPPORT) {\n\t\trequest_module(\"svc%s\", xprt_name);\n\t\terr = _svc_xprt_create(serv, xprt_name, net, family, port, flags, cred);\n\t}\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(svc_xprt_create);\n\n \nvoid svc_xprt_copy_addrs(struct svc_rqst *rqstp, struct svc_xprt *xprt)\n{\n\tmemcpy(&rqstp->rq_addr, &xprt->xpt_remote, xprt->xpt_remotelen);\n\trqstp->rq_addrlen = xprt->xpt_remotelen;\n\n\t \n\tmemcpy(&rqstp->rq_daddr, &xprt->xpt_local, xprt->xpt_locallen);\n\trqstp->rq_daddrlen = xprt->xpt_locallen;\n}\nEXPORT_SYMBOL_GPL(svc_xprt_copy_addrs);\n\n \nchar *svc_print_addr(struct svc_rqst *rqstp, char *buf, size_t len)\n{\n\treturn __svc_print_addr(svc_addr(rqstp), buf, len);\n}\nEXPORT_SYMBOL_GPL(svc_print_addr);\n\nstatic bool svc_xprt_slots_in_range(struct svc_xprt *xprt)\n{\n\tunsigned int limit = svc_rpc_per_connection_limit;\n\tint nrqsts = atomic_read(&xprt->xpt_nr_rqsts);\n\n\treturn limit == 0 || (nrqsts >= 0 && nrqsts < limit);\n}\n\nstatic bool svc_xprt_reserve_slot(struct svc_rqst *rqstp, struct svc_xprt *xprt)\n{\n\tif (!test_bit(RQ_DATA, &rqstp->rq_flags)) {\n\t\tif (!svc_xprt_slots_in_range(xprt))\n\t\t\treturn false;\n\t\tatomic_inc(&xprt->xpt_nr_rqsts);\n\t\tset_bit(RQ_DATA, &rqstp->rq_flags);\n\t}\n\treturn true;\n}\n\nstatic void svc_xprt_release_slot(struct svc_rqst *rqstp)\n{\n\tstruct svc_xprt\t*xprt = rqstp->rq_xprt;\n\tif (test_and_clear_bit(RQ_DATA, &rqstp->rq_flags)) {\n\t\tatomic_dec(&xprt->xpt_nr_rqsts);\n\t\tsmp_wmb();  \n\t\tsvc_xprt_enqueue(xprt);\n\t}\n}\n\nstatic bool svc_xprt_ready(struct svc_xprt *xprt)\n{\n\tunsigned long xpt_flags;\n\n\t \n\tsmp_rmb();\n\txpt_flags = READ_ONCE(xprt->xpt_flags);\n\n\ttrace_svc_xprt_enqueue(xprt, xpt_flags);\n\tif (xpt_flags & BIT(XPT_BUSY))\n\t\treturn false;\n\tif (xpt_flags & (BIT(XPT_CONN) | BIT(XPT_CLOSE) | BIT(XPT_HANDSHAKE)))\n\t\treturn true;\n\tif (xpt_flags & (BIT(XPT_DATA) | BIT(XPT_DEFERRED))) {\n\t\tif (xprt->xpt_ops->xpo_has_wspace(xprt) &&\n\t\t    svc_xprt_slots_in_range(xprt))\n\t\t\treturn true;\n\t\ttrace_svc_xprt_no_write_space(xprt);\n\t\treturn false;\n\t}\n\treturn false;\n}\n\n \nvoid svc_xprt_enqueue(struct svc_xprt *xprt)\n{\n\tstruct svc_pool *pool;\n\n\tif (!svc_xprt_ready(xprt))\n\t\treturn;\n\n\t \n\tif (test_and_set_bit(XPT_BUSY, &xprt->xpt_flags))\n\t\treturn;\n\n\tpool = svc_pool_for_cpu(xprt->xpt_server);\n\n\tpercpu_counter_inc(&pool->sp_sockets_queued);\n\tspin_lock_bh(&pool->sp_lock);\n\tlist_add_tail(&xprt->xpt_ready, &pool->sp_sockets);\n\tspin_unlock_bh(&pool->sp_lock);\n\n\tsvc_pool_wake_idle_thread(pool);\n}\nEXPORT_SYMBOL_GPL(svc_xprt_enqueue);\n\n \nstatic struct svc_xprt *svc_xprt_dequeue(struct svc_pool *pool)\n{\n\tstruct svc_xprt\t*xprt = NULL;\n\n\tif (list_empty(&pool->sp_sockets))\n\t\tgoto out;\n\n\tspin_lock_bh(&pool->sp_lock);\n\tif (likely(!list_empty(&pool->sp_sockets))) {\n\t\txprt = list_first_entry(&pool->sp_sockets,\n\t\t\t\t\tstruct svc_xprt, xpt_ready);\n\t\tlist_del_init(&xprt->xpt_ready);\n\t\tsvc_xprt_get(xprt);\n\t}\n\tspin_unlock_bh(&pool->sp_lock);\nout:\n\treturn xprt;\n}\n\n \nvoid svc_reserve(struct svc_rqst *rqstp, int space)\n{\n\tstruct svc_xprt *xprt = rqstp->rq_xprt;\n\n\tspace += rqstp->rq_res.head[0].iov_len;\n\n\tif (xprt && space < rqstp->rq_reserved) {\n\t\tatomic_sub((rqstp->rq_reserved - space), &xprt->xpt_reserved);\n\t\trqstp->rq_reserved = space;\n\t\tsmp_wmb();  \n\t\tsvc_xprt_enqueue(xprt);\n\t}\n}\nEXPORT_SYMBOL_GPL(svc_reserve);\n\nstatic void free_deferred(struct svc_xprt *xprt, struct svc_deferred_req *dr)\n{\n\tif (!dr)\n\t\treturn;\n\n\txprt->xpt_ops->xpo_release_ctxt(xprt, dr->xprt_ctxt);\n\tkfree(dr);\n}\n\nstatic void svc_xprt_release(struct svc_rqst *rqstp)\n{\n\tstruct svc_xprt\t*xprt = rqstp->rq_xprt;\n\n\txprt->xpt_ops->xpo_release_ctxt(xprt, rqstp->rq_xprt_ctxt);\n\trqstp->rq_xprt_ctxt = NULL;\n\n\tfree_deferred(xprt, rqstp->rq_deferred);\n\trqstp->rq_deferred = NULL;\n\n\tsvc_rqst_release_pages(rqstp);\n\trqstp->rq_res.page_len = 0;\n\trqstp->rq_res.page_base = 0;\n\n\t \n\tif ((rqstp->rq_res.len) >  rqstp->rq_reserved)\n\t\tprintk(KERN_ERR \"RPC request reserved %d but used %d\\n\",\n\t\t       rqstp->rq_reserved,\n\t\t       rqstp->rq_res.len);\n\n\trqstp->rq_res.head[0].iov_len = 0;\n\tsvc_reserve(rqstp, 0);\n\tsvc_xprt_release_slot(rqstp);\n\trqstp->rq_xprt = NULL;\n\tsvc_xprt_put(xprt);\n}\n\n \nvoid svc_wake_up(struct svc_serv *serv)\n{\n\tstruct svc_pool *pool = &serv->sv_pools[0];\n\n\tset_bit(SP_TASK_PENDING, &pool->sp_flags);\n\tsvc_pool_wake_idle_thread(pool);\n}\nEXPORT_SYMBOL_GPL(svc_wake_up);\n\nint svc_port_is_privileged(struct sockaddr *sin)\n{\n\tswitch (sin->sa_family) {\n\tcase AF_INET:\n\t\treturn ntohs(((struct sockaddr_in *)sin)->sin_port)\n\t\t\t< PROT_SOCK;\n\tcase AF_INET6:\n\t\treturn ntohs(((struct sockaddr_in6 *)sin)->sin6_port)\n\t\t\t< PROT_SOCK;\n\tdefault:\n\t\treturn 0;\n\t}\n}\n\n \nstatic void svc_check_conn_limits(struct svc_serv *serv)\n{\n\tunsigned int limit = serv->sv_maxconn ? serv->sv_maxconn :\n\t\t\t\t(serv->sv_nrthreads+3) * 20;\n\n\tif (serv->sv_tmpcnt > limit) {\n\t\tstruct svc_xprt *xprt = NULL;\n\t\tspin_lock_bh(&serv->sv_lock);\n\t\tif (!list_empty(&serv->sv_tempsocks)) {\n\t\t\t \n\t\t\tnet_notice_ratelimited(\"%s: too many open connections, consider increasing the %s\\n\",\n\t\t\t\t\t       serv->sv_name, serv->sv_maxconn ?\n\t\t\t\t\t       \"max number of connections\" :\n\t\t\t\t\t       \"number of threads\");\n\t\t\t \n\t\t\txprt = list_entry(serv->sv_tempsocks.prev,\n\t\t\t\t\t  struct svc_xprt,\n\t\t\t\t\t  xpt_list);\n\t\t\tset_bit(XPT_CLOSE, &xprt->xpt_flags);\n\t\t\tsvc_xprt_get(xprt);\n\t\t}\n\t\tspin_unlock_bh(&serv->sv_lock);\n\n\t\tif (xprt) {\n\t\t\tsvc_xprt_enqueue(xprt);\n\t\t\tsvc_xprt_put(xprt);\n\t\t}\n\t}\n}\n\nstatic bool svc_alloc_arg(struct svc_rqst *rqstp)\n{\n\tstruct svc_serv *serv = rqstp->rq_server;\n\tstruct xdr_buf *arg = &rqstp->rq_arg;\n\tunsigned long pages, filled, ret;\n\n\tpages = (serv->sv_max_mesg + 2 * PAGE_SIZE) >> PAGE_SHIFT;\n\tif (pages > RPCSVC_MAXPAGES) {\n\t\tpr_warn_once(\"svc: warning: pages=%lu > RPCSVC_MAXPAGES=%lu\\n\",\n\t\t\t     pages, RPCSVC_MAXPAGES);\n\t\t \n\t\tpages = RPCSVC_MAXPAGES;\n\t}\n\n\tfor (filled = 0; filled < pages; filled = ret) {\n\t\tret = alloc_pages_bulk_array(GFP_KERNEL, pages,\n\t\t\t\t\t     rqstp->rq_pages);\n\t\tif (ret > filled)\n\t\t\t \n\t\t\tcontinue;\n\n\t\tset_current_state(TASK_IDLE);\n\t\tif (kthread_should_stop()) {\n\t\t\tset_current_state(TASK_RUNNING);\n\t\t\treturn false;\n\t\t}\n\t\ttrace_svc_alloc_arg_err(pages, ret);\n\t\tmemalloc_retry_wait(GFP_KERNEL);\n\t}\n\trqstp->rq_page_end = &rqstp->rq_pages[pages];\n\trqstp->rq_pages[pages] = NULL;  \n\n\t \n\targ->head[0].iov_base = page_address(rqstp->rq_pages[0]);\n\targ->head[0].iov_len = PAGE_SIZE;\n\targ->pages = rqstp->rq_pages + 1;\n\targ->page_base = 0;\n\t \n\targ->page_len = (pages-2)*PAGE_SIZE;\n\targ->len = (pages-1)*PAGE_SIZE;\n\targ->tail[0].iov_len = 0;\n\n\trqstp->rq_xid = xdr_zero;\n\treturn true;\n}\n\nstatic bool\nrqst_should_sleep(struct svc_rqst *rqstp)\n{\n\tstruct svc_pool\t\t*pool = rqstp->rq_pool;\n\n\t \n\tif (test_bit(SP_TASK_PENDING, &pool->sp_flags))\n\t\treturn false;\n\n\t \n\tif (!list_empty(&pool->sp_sockets))\n\t\treturn false;\n\n\t \n\tif (kthread_should_stop())\n\t\treturn false;\n\n\t \n\tif (freezing(current))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic struct svc_xprt *svc_get_next_xprt(struct svc_rqst *rqstp)\n{\n\tstruct svc_pool\t\t*pool = rqstp->rq_pool;\n\n\t \n\tWARN_ON_ONCE(rqstp->rq_xprt);\n\n\trqstp->rq_xprt = svc_xprt_dequeue(pool);\n\tif (rqstp->rq_xprt)\n\t\tgoto out_found;\n\n\tset_current_state(TASK_IDLE);\n\tsmp_mb__before_atomic();\n\tclear_bit(SP_CONGESTED, &pool->sp_flags);\n\tclear_bit(RQ_BUSY, &rqstp->rq_flags);\n\tsmp_mb__after_atomic();\n\n\tif (likely(rqst_should_sleep(rqstp)))\n\t\tschedule();\n\telse\n\t\t__set_current_state(TASK_RUNNING);\n\n\ttry_to_freeze();\n\n\tset_bit(RQ_BUSY, &rqstp->rq_flags);\n\tsmp_mb__after_atomic();\n\tclear_bit(SP_TASK_PENDING, &pool->sp_flags);\n\trqstp->rq_xprt = svc_xprt_dequeue(pool);\n\tif (rqstp->rq_xprt)\n\t\tgoto out_found;\n\n\tif (kthread_should_stop())\n\t\treturn NULL;\n\treturn NULL;\nout_found:\n\tclear_bit(SP_TASK_PENDING, &pool->sp_flags);\n\t \n\tif (!test_bit(SP_CONGESTED, &pool->sp_flags))\n\t\trqstp->rq_chandle.thread_wait = 5*HZ;\n\telse\n\t\trqstp->rq_chandle.thread_wait = 1*HZ;\n\ttrace_svc_xprt_dequeue(rqstp);\n\treturn rqstp->rq_xprt;\n}\n\nstatic void svc_add_new_temp_xprt(struct svc_serv *serv, struct svc_xprt *newxpt)\n{\n\tspin_lock_bh(&serv->sv_lock);\n\tset_bit(XPT_TEMP, &newxpt->xpt_flags);\n\tlist_add(&newxpt->xpt_list, &serv->sv_tempsocks);\n\tserv->sv_tmpcnt++;\n\tif (serv->sv_temptimer.function == NULL) {\n\t\t \n\t\tserv->sv_temptimer.function = svc_age_temp_xprts;\n\t\tmod_timer(&serv->sv_temptimer,\n\t\t\t  jiffies + svc_conn_age_period * HZ);\n\t}\n\tspin_unlock_bh(&serv->sv_lock);\n\tsvc_xprt_received(newxpt);\n}\n\nstatic int svc_handle_xprt(struct svc_rqst *rqstp, struct svc_xprt *xprt)\n{\n\tstruct svc_serv *serv = rqstp->rq_server;\n\tint len = 0;\n\n\tif (test_bit(XPT_CLOSE, &xprt->xpt_flags)) {\n\t\tif (test_and_clear_bit(XPT_KILL_TEMP, &xprt->xpt_flags))\n\t\t\txprt->xpt_ops->xpo_kill_temp_xprt(xprt);\n\t\tsvc_delete_xprt(xprt);\n\t\t \n\t\tgoto out;\n\t}\n\tif (test_bit(XPT_LISTENER, &xprt->xpt_flags)) {\n\t\tstruct svc_xprt *newxpt;\n\t\t \n\t\t__module_get(xprt->xpt_class->xcl_owner);\n\t\tsvc_check_conn_limits(xprt->xpt_server);\n\t\tnewxpt = xprt->xpt_ops->xpo_accept(xprt);\n\t\tif (newxpt) {\n\t\t\tnewxpt->xpt_cred = get_cred(xprt->xpt_cred);\n\t\t\tsvc_add_new_temp_xprt(serv, newxpt);\n\t\t\ttrace_svc_xprt_accept(newxpt, serv->sv_name);\n\t\t} else {\n\t\t\tmodule_put(xprt->xpt_class->xcl_owner);\n\t\t}\n\t\tsvc_xprt_received(xprt);\n\t} else if (test_bit(XPT_HANDSHAKE, &xprt->xpt_flags)) {\n\t\txprt->xpt_ops->xpo_handshake(xprt);\n\t\tsvc_xprt_received(xprt);\n\t} else if (svc_xprt_reserve_slot(rqstp, xprt)) {\n\t\t \n\t\trqstp->rq_deferred = svc_deferred_dequeue(xprt);\n\t\tif (rqstp->rq_deferred)\n\t\t\tlen = svc_deferred_recv(rqstp);\n\t\telse\n\t\t\tlen = xprt->xpt_ops->xpo_recvfrom(rqstp);\n\t\trqstp->rq_reserved = serv->sv_max_mesg;\n\t\tatomic_add(rqstp->rq_reserved, &xprt->xpt_reserved);\n\t} else\n\t\tsvc_xprt_received(xprt);\n\nout:\n\treturn len;\n}\n\n \nvoid svc_recv(struct svc_rqst *rqstp)\n{\n\tstruct svc_xprt\t\t*xprt = NULL;\n\tstruct svc_serv\t\t*serv = rqstp->rq_server;\n\tint\t\t\tlen;\n\n\tif (!svc_alloc_arg(rqstp))\n\t\tgoto out;\n\n\ttry_to_freeze();\n\tcond_resched();\n\tif (kthread_should_stop())\n\t\tgoto out;\n\n\txprt = svc_get_next_xprt(rqstp);\n\tif (!xprt)\n\t\tgoto out;\n\n\tlen = svc_handle_xprt(rqstp, xprt);\n\n\t \n\tif (len <= 0)\n\t\tgoto out_release;\n\n\ttrace_svc_xdr_recvfrom(&rqstp->rq_arg);\n\n\tclear_bit(XPT_OLD, &xprt->xpt_flags);\n\n\trqstp->rq_chandle.defer = svc_defer;\n\n\tif (serv->sv_stats)\n\t\tserv->sv_stats->netcnt++;\n\tpercpu_counter_inc(&rqstp->rq_pool->sp_messages_arrived);\n\trqstp->rq_stime = ktime_get();\n\tsvc_process(rqstp);\nout:\n\treturn;\nout_release:\n\trqstp->rq_res.len = 0;\n\tsvc_xprt_release(rqstp);\n}\nEXPORT_SYMBOL_GPL(svc_recv);\n\n \nvoid svc_drop(struct svc_rqst *rqstp)\n{\n\ttrace_svc_drop(rqstp);\n\tsvc_xprt_release(rqstp);\n}\nEXPORT_SYMBOL_GPL(svc_drop);\n\n \nvoid svc_send(struct svc_rqst *rqstp)\n{\n\tstruct svc_xprt\t*xprt;\n\tstruct xdr_buf\t*xb;\n\tint status;\n\n\txprt = rqstp->rq_xprt;\n\tif (!xprt)\n\t\treturn;\n\n\t \n\txb = &rqstp->rq_res;\n\txb->len = xb->head[0].iov_len +\n\t\txb->page_len +\n\t\txb->tail[0].iov_len;\n\ttrace_svc_xdr_sendto(rqstp->rq_xid, xb);\n\ttrace_svc_stats_latency(rqstp);\n\n\tstatus = xprt->xpt_ops->xpo_sendto(rqstp);\n\n\ttrace_svc_send(rqstp, status);\n\tsvc_xprt_release(rqstp);\n}\n\n \nstatic void svc_age_temp_xprts(struct timer_list *t)\n{\n\tstruct svc_serv *serv = from_timer(serv, t, sv_temptimer);\n\tstruct svc_xprt *xprt;\n\tstruct list_head *le, *next;\n\n\tdprintk(\"svc_age_temp_xprts\\n\");\n\n\tif (!spin_trylock_bh(&serv->sv_lock)) {\n\t\t \n\t\tdprintk(\"svc_age_temp_xprts: busy\\n\");\n\t\tmod_timer(&serv->sv_temptimer, jiffies + HZ);\n\t\treturn;\n\t}\n\n\tlist_for_each_safe(le, next, &serv->sv_tempsocks) {\n\t\txprt = list_entry(le, struct svc_xprt, xpt_list);\n\n\t\t \n\t\tif (!test_and_set_bit(XPT_OLD, &xprt->xpt_flags))\n\t\t\tcontinue;\n\t\tif (kref_read(&xprt->xpt_ref) > 1 ||\n\t\t    test_bit(XPT_BUSY, &xprt->xpt_flags))\n\t\t\tcontinue;\n\t\tlist_del_init(le);\n\t\tset_bit(XPT_CLOSE, &xprt->xpt_flags);\n\t\tdprintk(\"queuing xprt %p for closing\\n\", xprt);\n\n\t\t \n\t\tsvc_xprt_enqueue(xprt);\n\t}\n\tspin_unlock_bh(&serv->sv_lock);\n\n\tmod_timer(&serv->sv_temptimer, jiffies + svc_conn_age_period * HZ);\n}\n\n \nvoid svc_age_temp_xprts_now(struct svc_serv *serv, struct sockaddr *server_addr)\n{\n\tstruct svc_xprt *xprt;\n\tstruct list_head *le, *next;\n\tLIST_HEAD(to_be_closed);\n\n\tspin_lock_bh(&serv->sv_lock);\n\tlist_for_each_safe(le, next, &serv->sv_tempsocks) {\n\t\txprt = list_entry(le, struct svc_xprt, xpt_list);\n\t\tif (rpc_cmp_addr(server_addr, (struct sockaddr *)\n\t\t\t\t&xprt->xpt_local)) {\n\t\t\tdprintk(\"svc_age_temp_xprts_now: found %p\\n\", xprt);\n\t\t\tlist_move(le, &to_be_closed);\n\t\t}\n\t}\n\tspin_unlock_bh(&serv->sv_lock);\n\n\twhile (!list_empty(&to_be_closed)) {\n\t\tle = to_be_closed.next;\n\t\tlist_del_init(le);\n\t\txprt = list_entry(le, struct svc_xprt, xpt_list);\n\t\tset_bit(XPT_CLOSE, &xprt->xpt_flags);\n\t\tset_bit(XPT_KILL_TEMP, &xprt->xpt_flags);\n\t\tdprintk(\"svc_age_temp_xprts_now: queuing xprt %p for closing\\n\",\n\t\t\t\txprt);\n\t\tsvc_xprt_enqueue(xprt);\n\t}\n}\nEXPORT_SYMBOL_GPL(svc_age_temp_xprts_now);\n\nstatic void call_xpt_users(struct svc_xprt *xprt)\n{\n\tstruct svc_xpt_user *u;\n\n\tspin_lock(&xprt->xpt_lock);\n\twhile (!list_empty(&xprt->xpt_users)) {\n\t\tu = list_first_entry(&xprt->xpt_users, struct svc_xpt_user, list);\n\t\tlist_del_init(&u->list);\n\t\tu->callback(u);\n\t}\n\tspin_unlock(&xprt->xpt_lock);\n}\n\n \nstatic void svc_delete_xprt(struct svc_xprt *xprt)\n{\n\tstruct svc_serv\t*serv = xprt->xpt_server;\n\tstruct svc_deferred_req *dr;\n\n\tif (test_and_set_bit(XPT_DEAD, &xprt->xpt_flags))\n\t\treturn;\n\n\ttrace_svc_xprt_detach(xprt);\n\txprt->xpt_ops->xpo_detach(xprt);\n\tif (xprt->xpt_bc_xprt)\n\t\txprt->xpt_bc_xprt->ops->close(xprt->xpt_bc_xprt);\n\n\tspin_lock_bh(&serv->sv_lock);\n\tlist_del_init(&xprt->xpt_list);\n\tWARN_ON_ONCE(!list_empty(&xprt->xpt_ready));\n\tif (test_bit(XPT_TEMP, &xprt->xpt_flags))\n\t\tserv->sv_tmpcnt--;\n\tspin_unlock_bh(&serv->sv_lock);\n\n\twhile ((dr = svc_deferred_dequeue(xprt)) != NULL)\n\t\tfree_deferred(xprt, dr);\n\n\tcall_xpt_users(xprt);\n\tsvc_xprt_put(xprt);\n}\n\n \nvoid svc_xprt_close(struct svc_xprt *xprt)\n{\n\ttrace_svc_xprt_close(xprt);\n\tset_bit(XPT_CLOSE, &xprt->xpt_flags);\n\tif (test_and_set_bit(XPT_BUSY, &xprt->xpt_flags))\n\t\t \n\t\treturn;\n\t \n\tsvc_delete_xprt(xprt);\n}\nEXPORT_SYMBOL_GPL(svc_xprt_close);\n\nstatic int svc_close_list(struct svc_serv *serv, struct list_head *xprt_list, struct net *net)\n{\n\tstruct svc_xprt *xprt;\n\tint ret = 0;\n\n\tspin_lock_bh(&serv->sv_lock);\n\tlist_for_each_entry(xprt, xprt_list, xpt_list) {\n\t\tif (xprt->xpt_net != net)\n\t\t\tcontinue;\n\t\tret++;\n\t\tset_bit(XPT_CLOSE, &xprt->xpt_flags);\n\t\tsvc_xprt_enqueue(xprt);\n\t}\n\tspin_unlock_bh(&serv->sv_lock);\n\treturn ret;\n}\n\nstatic struct svc_xprt *svc_dequeue_net(struct svc_serv *serv, struct net *net)\n{\n\tstruct svc_pool *pool;\n\tstruct svc_xprt *xprt;\n\tstruct svc_xprt *tmp;\n\tint i;\n\n\tfor (i = 0; i < serv->sv_nrpools; i++) {\n\t\tpool = &serv->sv_pools[i];\n\n\t\tspin_lock_bh(&pool->sp_lock);\n\t\tlist_for_each_entry_safe(xprt, tmp, &pool->sp_sockets, xpt_ready) {\n\t\t\tif (xprt->xpt_net != net)\n\t\t\t\tcontinue;\n\t\t\tlist_del_init(&xprt->xpt_ready);\n\t\t\tspin_unlock_bh(&pool->sp_lock);\n\t\t\treturn xprt;\n\t\t}\n\t\tspin_unlock_bh(&pool->sp_lock);\n\t}\n\treturn NULL;\n}\n\nstatic void svc_clean_up_xprts(struct svc_serv *serv, struct net *net)\n{\n\tstruct svc_xprt *xprt;\n\n\twhile ((xprt = svc_dequeue_net(serv, net))) {\n\t\tset_bit(XPT_CLOSE, &xprt->xpt_flags);\n\t\tsvc_delete_xprt(xprt);\n\t}\n}\n\n \nvoid svc_xprt_destroy_all(struct svc_serv *serv, struct net *net)\n{\n\tint delay = 0;\n\n\twhile (svc_close_list(serv, &serv->sv_permsocks, net) +\n\t       svc_close_list(serv, &serv->sv_tempsocks, net)) {\n\n\t\tsvc_clean_up_xprts(serv, net);\n\t\tmsleep(delay++);\n\t}\n}\nEXPORT_SYMBOL_GPL(svc_xprt_destroy_all);\n\n \n\nstatic void svc_revisit(struct cache_deferred_req *dreq, int too_many)\n{\n\tstruct svc_deferred_req *dr =\n\t\tcontainer_of(dreq, struct svc_deferred_req, handle);\n\tstruct svc_xprt *xprt = dr->xprt;\n\n\tspin_lock(&xprt->xpt_lock);\n\tset_bit(XPT_DEFERRED, &xprt->xpt_flags);\n\tif (too_many || test_bit(XPT_DEAD, &xprt->xpt_flags)) {\n\t\tspin_unlock(&xprt->xpt_lock);\n\t\ttrace_svc_defer_drop(dr);\n\t\tfree_deferred(xprt, dr);\n\t\tsvc_xprt_put(xprt);\n\t\treturn;\n\t}\n\tdr->xprt = NULL;\n\tlist_add(&dr->handle.recent, &xprt->xpt_deferred);\n\tspin_unlock(&xprt->xpt_lock);\n\ttrace_svc_defer_queue(dr);\n\tsvc_xprt_enqueue(xprt);\n\tsvc_xprt_put(xprt);\n}\n\n \nstatic struct cache_deferred_req *svc_defer(struct cache_req *req)\n{\n\tstruct svc_rqst *rqstp = container_of(req, struct svc_rqst, rq_chandle);\n\tstruct svc_deferred_req *dr;\n\n\tif (rqstp->rq_arg.page_len || !test_bit(RQ_USEDEFERRAL, &rqstp->rq_flags))\n\t\treturn NULL;  \n\tif (rqstp->rq_deferred) {\n\t\tdr = rqstp->rq_deferred;\n\t\trqstp->rq_deferred = NULL;\n\t} else {\n\t\tsize_t skip;\n\t\tsize_t size;\n\t\t \n\t\tsize = sizeof(struct svc_deferred_req) + rqstp->rq_arg.len;\n\t\tdr = kmalloc(size, GFP_KERNEL);\n\t\tif (dr == NULL)\n\t\t\treturn NULL;\n\n\t\tdr->handle.owner = rqstp->rq_server;\n\t\tdr->prot = rqstp->rq_prot;\n\t\tmemcpy(&dr->addr, &rqstp->rq_addr, rqstp->rq_addrlen);\n\t\tdr->addrlen = rqstp->rq_addrlen;\n\t\tdr->daddr = rqstp->rq_daddr;\n\t\tdr->argslen = rqstp->rq_arg.len >> 2;\n\n\t\t \n\t\tskip = rqstp->rq_arg.len - rqstp->rq_arg.head[0].iov_len;\n\t\tmemcpy(dr->args, rqstp->rq_arg.head[0].iov_base - skip,\n\t\t       dr->argslen << 2);\n\t}\n\tdr->xprt_ctxt = rqstp->rq_xprt_ctxt;\n\trqstp->rq_xprt_ctxt = NULL;\n\ttrace_svc_defer(rqstp);\n\tsvc_xprt_get(rqstp->rq_xprt);\n\tdr->xprt = rqstp->rq_xprt;\n\tset_bit(RQ_DROPME, &rqstp->rq_flags);\n\n\tdr->handle.revisit = svc_revisit;\n\treturn &dr->handle;\n}\n\n \nstatic noinline int svc_deferred_recv(struct svc_rqst *rqstp)\n{\n\tstruct svc_deferred_req *dr = rqstp->rq_deferred;\n\n\ttrace_svc_defer_recv(dr);\n\n\t \n\trqstp->rq_arg.head[0].iov_base = dr->args;\n\t \n\trqstp->rq_arg.head[0].iov_len = dr->argslen << 2;\n\trqstp->rq_arg.page_len = 0;\n\t \n\trqstp->rq_arg.len     = dr->argslen << 2;\n\trqstp->rq_prot        = dr->prot;\n\tmemcpy(&rqstp->rq_addr, &dr->addr, dr->addrlen);\n\trqstp->rq_addrlen     = dr->addrlen;\n\t \n\trqstp->rq_daddr       = dr->daddr;\n\trqstp->rq_respages    = rqstp->rq_pages;\n\trqstp->rq_xprt_ctxt   = dr->xprt_ctxt;\n\n\tdr->xprt_ctxt = NULL;\n\tsvc_xprt_received(rqstp->rq_xprt);\n\treturn dr->argslen << 2;\n}\n\n\nstatic struct svc_deferred_req *svc_deferred_dequeue(struct svc_xprt *xprt)\n{\n\tstruct svc_deferred_req *dr = NULL;\n\n\tif (!test_bit(XPT_DEFERRED, &xprt->xpt_flags))\n\t\treturn NULL;\n\tspin_lock(&xprt->xpt_lock);\n\tif (!list_empty(&xprt->xpt_deferred)) {\n\t\tdr = list_entry(xprt->xpt_deferred.next,\n\t\t\t\tstruct svc_deferred_req,\n\t\t\t\thandle.recent);\n\t\tlist_del_init(&dr->handle.recent);\n\t} else\n\t\tclear_bit(XPT_DEFERRED, &xprt->xpt_flags);\n\tspin_unlock(&xprt->xpt_lock);\n\treturn dr;\n}\n\n \nstruct svc_xprt *svc_find_xprt(struct svc_serv *serv, const char *xcl_name,\n\t\t\t       struct net *net, const sa_family_t af,\n\t\t\t       const unsigned short port)\n{\n\tstruct svc_xprt *xprt;\n\tstruct svc_xprt *found = NULL;\n\n\t \n\tif (serv == NULL || xcl_name == NULL)\n\t\treturn found;\n\n\tspin_lock_bh(&serv->sv_lock);\n\tlist_for_each_entry(xprt, &serv->sv_permsocks, xpt_list) {\n\t\tif (xprt->xpt_net != net)\n\t\t\tcontinue;\n\t\tif (strcmp(xprt->xpt_class->xcl_name, xcl_name))\n\t\t\tcontinue;\n\t\tif (af != AF_UNSPEC && af != xprt->xpt_local.ss_family)\n\t\t\tcontinue;\n\t\tif (port != 0 && port != svc_xprt_local_port(xprt))\n\t\t\tcontinue;\n\t\tfound = xprt;\n\t\tsvc_xprt_get(xprt);\n\t\tbreak;\n\t}\n\tspin_unlock_bh(&serv->sv_lock);\n\treturn found;\n}\nEXPORT_SYMBOL_GPL(svc_find_xprt);\n\nstatic int svc_one_xprt_name(const struct svc_xprt *xprt,\n\t\t\t     char *pos, int remaining)\n{\n\tint len;\n\n\tlen = snprintf(pos, remaining, \"%s %u\\n\",\n\t\t\txprt->xpt_class->xcl_name,\n\t\t\tsvc_xprt_local_port(xprt));\n\tif (len >= remaining)\n\t\treturn -ENAMETOOLONG;\n\treturn len;\n}\n\n \nint svc_xprt_names(struct svc_serv *serv, char *buf, const int buflen)\n{\n\tstruct svc_xprt *xprt;\n\tint len, totlen;\n\tchar *pos;\n\n\t \n\tif (!serv)\n\t\treturn 0;\n\n\tspin_lock_bh(&serv->sv_lock);\n\n\tpos = buf;\n\ttotlen = 0;\n\tlist_for_each_entry(xprt, &serv->sv_permsocks, xpt_list) {\n\t\tlen = svc_one_xprt_name(xprt, pos, buflen - totlen);\n\t\tif (len < 0) {\n\t\t\t*buf = '\\0';\n\t\t\ttotlen = len;\n\t\t}\n\t\tif (len <= 0)\n\t\t\tbreak;\n\n\t\tpos += len;\n\t\ttotlen += len;\n\t}\n\n\tspin_unlock_bh(&serv->sv_lock);\n\treturn totlen;\n}\nEXPORT_SYMBOL_GPL(svc_xprt_names);\n\n\n \n\nstatic void *svc_pool_stats_start(struct seq_file *m, loff_t *pos)\n{\n\tunsigned int pidx = (unsigned int)*pos;\n\tstruct svc_serv *serv = m->private;\n\n\tdprintk(\"svc_pool_stats_start, *pidx=%u\\n\", pidx);\n\n\tif (!pidx)\n\t\treturn SEQ_START_TOKEN;\n\treturn (pidx > serv->sv_nrpools ? NULL : &serv->sv_pools[pidx-1]);\n}\n\nstatic void *svc_pool_stats_next(struct seq_file *m, void *p, loff_t *pos)\n{\n\tstruct svc_pool *pool = p;\n\tstruct svc_serv *serv = m->private;\n\n\tdprintk(\"svc_pool_stats_next, *pos=%llu\\n\", *pos);\n\n\tif (p == SEQ_START_TOKEN) {\n\t\tpool = &serv->sv_pools[0];\n\t} else {\n\t\tunsigned int pidx = (pool - &serv->sv_pools[0]);\n\t\tif (pidx < serv->sv_nrpools-1)\n\t\t\tpool = &serv->sv_pools[pidx+1];\n\t\telse\n\t\t\tpool = NULL;\n\t}\n\t++*pos;\n\treturn pool;\n}\n\nstatic void svc_pool_stats_stop(struct seq_file *m, void *p)\n{\n}\n\nstatic int svc_pool_stats_show(struct seq_file *m, void *p)\n{\n\tstruct svc_pool *pool = p;\n\n\tif (p == SEQ_START_TOKEN) {\n\t\tseq_puts(m, \"# pool packets-arrived sockets-enqueued threads-woken threads-timedout\\n\");\n\t\treturn 0;\n\t}\n\n\tseq_printf(m, \"%u %llu %llu %llu 0\\n\",\n\t\t   pool->sp_id,\n\t\t   percpu_counter_sum_positive(&pool->sp_messages_arrived),\n\t\t   percpu_counter_sum_positive(&pool->sp_sockets_queued),\n\t\t   percpu_counter_sum_positive(&pool->sp_threads_woken));\n\n\treturn 0;\n}\n\nstatic const struct seq_operations svc_pool_stats_seq_ops = {\n\t.start\t= svc_pool_stats_start,\n\t.next\t= svc_pool_stats_next,\n\t.stop\t= svc_pool_stats_stop,\n\t.show\t= svc_pool_stats_show,\n};\n\nint svc_pool_stats_open(struct svc_serv *serv, struct file *file)\n{\n\tint err;\n\n\terr = seq_open(file, &svc_pool_stats_seq_ops);\n\tif (!err)\n\t\t((struct seq_file *) file->private_data)->private = serv;\n\treturn err;\n}\nEXPORT_SYMBOL(svc_pool_stats_open);\n\n \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}