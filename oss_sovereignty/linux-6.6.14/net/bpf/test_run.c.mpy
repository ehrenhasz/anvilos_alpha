{
  "module_name": "test_run.c",
  "hash_id": "3f2d8d24a649a391e505b5275f0c1445c50c723e90b20c2449b130f7a3736113",
  "original_prompt": "Ingested from linux-6.6.14/net/bpf/test_run.c",
  "human_readable_source": "\n \n#include <linux/bpf.h>\n#include <linux/btf.h>\n#include <linux/btf_ids.h>\n#include <linux/slab.h>\n#include <linux/init.h>\n#include <linux/vmalloc.h>\n#include <linux/etherdevice.h>\n#include <linux/filter.h>\n#include <linux/rcupdate_trace.h>\n#include <linux/sched/signal.h>\n#include <net/bpf_sk_storage.h>\n#include <net/sock.h>\n#include <net/tcp.h>\n#include <net/net_namespace.h>\n#include <net/page_pool/helpers.h>\n#include <linux/error-injection.h>\n#include <linux/smp.h>\n#include <linux/sock_diag.h>\n#include <linux/netfilter.h>\n#include <net/netdev_rx_queue.h>\n#include <net/xdp.h>\n#include <net/netfilter/nf_bpf_link.h>\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/bpf_test_run.h>\n\nstruct bpf_test_timer {\n\tenum { NO_PREEMPT, NO_MIGRATE } mode;\n\tu32 i;\n\tu64 time_start, time_spent;\n};\n\nstatic void bpf_test_timer_enter(struct bpf_test_timer *t)\n\t__acquires(rcu)\n{\n\trcu_read_lock();\n\tif (t->mode == NO_PREEMPT)\n\t\tpreempt_disable();\n\telse\n\t\tmigrate_disable();\n\n\tt->time_start = ktime_get_ns();\n}\n\nstatic void bpf_test_timer_leave(struct bpf_test_timer *t)\n\t__releases(rcu)\n{\n\tt->time_start = 0;\n\n\tif (t->mode == NO_PREEMPT)\n\t\tpreempt_enable();\n\telse\n\t\tmigrate_enable();\n\trcu_read_unlock();\n}\n\nstatic bool bpf_test_timer_continue(struct bpf_test_timer *t, int iterations,\n\t\t\t\t    u32 repeat, int *err, u32 *duration)\n\t__must_hold(rcu)\n{\n\tt->i += iterations;\n\tif (t->i >= repeat) {\n\t\t \n\t\tt->time_spent += ktime_get_ns() - t->time_start;\n\t\tdo_div(t->time_spent, t->i);\n\t\t*duration = t->time_spent > U32_MAX ? U32_MAX : (u32)t->time_spent;\n\t\t*err = 0;\n\t\tgoto reset;\n\t}\n\n\tif (signal_pending(current)) {\n\t\t \n\t\t*err = -EINTR;\n\t\tgoto reset;\n\t}\n\n\tif (need_resched()) {\n\t\t \n\t\tt->time_spent += ktime_get_ns() - t->time_start;\n\t\tbpf_test_timer_leave(t);\n\t\tcond_resched();\n\t\tbpf_test_timer_enter(t);\n\t}\n\n\t \n\treturn true;\n\nreset:\n\tt->i = 0;\n\treturn false;\n}\n\n \nstruct xdp_page_head {\n\tstruct xdp_buff orig_ctx;\n\tstruct xdp_buff ctx;\n\tunion {\n\t\t \n\t\tDECLARE_FLEX_ARRAY(struct xdp_frame, frame);\n\t\tDECLARE_FLEX_ARRAY(u8, data);\n\t};\n};\n\nstruct xdp_test_data {\n\tstruct xdp_buff *orig_ctx;\n\tstruct xdp_rxq_info rxq;\n\tstruct net_device *dev;\n\tstruct page_pool *pp;\n\tstruct xdp_frame **frames;\n\tstruct sk_buff **skbs;\n\tstruct xdp_mem_info mem;\n\tu32 batch_size;\n\tu32 frame_cnt;\n};\n\n \n#define TEST_XDP_FRAME_SIZE (PAGE_SIZE - sizeof(struct xdp_page_head))\n#define TEST_XDP_MAX_BATCH 256\n\nstatic void xdp_test_run_init_page(struct page *page, void *arg)\n{\n\tstruct xdp_page_head *head = phys_to_virt(page_to_phys(page));\n\tstruct xdp_buff *new_ctx, *orig_ctx;\n\tu32 headroom = XDP_PACKET_HEADROOM;\n\tstruct xdp_test_data *xdp = arg;\n\tsize_t frm_len, meta_len;\n\tstruct xdp_frame *frm;\n\tvoid *data;\n\n\torig_ctx = xdp->orig_ctx;\n\tfrm_len = orig_ctx->data_end - orig_ctx->data_meta;\n\tmeta_len = orig_ctx->data - orig_ctx->data_meta;\n\theadroom -= meta_len;\n\n\tnew_ctx = &head->ctx;\n\tfrm = head->frame;\n\tdata = head->data;\n\tmemcpy(data + headroom, orig_ctx->data_meta, frm_len);\n\n\txdp_init_buff(new_ctx, TEST_XDP_FRAME_SIZE, &xdp->rxq);\n\txdp_prepare_buff(new_ctx, data, headroom, frm_len, true);\n\tnew_ctx->data = new_ctx->data_meta + meta_len;\n\n\txdp_update_frame_from_buff(new_ctx, frm);\n\tfrm->mem = new_ctx->rxq->mem;\n\n\tmemcpy(&head->orig_ctx, new_ctx, sizeof(head->orig_ctx));\n}\n\nstatic int xdp_test_run_setup(struct xdp_test_data *xdp, struct xdp_buff *orig_ctx)\n{\n\tstruct page_pool *pp;\n\tint err = -ENOMEM;\n\tstruct page_pool_params pp_params = {\n\t\t.order = 0,\n\t\t.flags = 0,\n\t\t.pool_size = xdp->batch_size,\n\t\t.nid = NUMA_NO_NODE,\n\t\t.init_callback = xdp_test_run_init_page,\n\t\t.init_arg = xdp,\n\t};\n\n\txdp->frames = kvmalloc_array(xdp->batch_size, sizeof(void *), GFP_KERNEL);\n\tif (!xdp->frames)\n\t\treturn -ENOMEM;\n\n\txdp->skbs = kvmalloc_array(xdp->batch_size, sizeof(void *), GFP_KERNEL);\n\tif (!xdp->skbs)\n\t\tgoto err_skbs;\n\n\tpp = page_pool_create(&pp_params);\n\tif (IS_ERR(pp)) {\n\t\terr = PTR_ERR(pp);\n\t\tgoto err_pp;\n\t}\n\n\t \n\terr = xdp_reg_mem_model(&xdp->mem, MEM_TYPE_PAGE_POOL, pp);\n\tif (err)\n\t\tgoto err_mmodel;\n\n\txdp->pp = pp;\n\n\t \n\txdp_rxq_info_reg(&xdp->rxq, orig_ctx->rxq->dev, 0, 0);\n\txdp->rxq.mem.type = MEM_TYPE_PAGE_POOL;\n\txdp->rxq.mem.id = pp->xdp_mem_id;\n\txdp->dev = orig_ctx->rxq->dev;\n\txdp->orig_ctx = orig_ctx;\n\n\treturn 0;\n\nerr_mmodel:\n\tpage_pool_destroy(pp);\nerr_pp:\n\tkvfree(xdp->skbs);\nerr_skbs:\n\tkvfree(xdp->frames);\n\treturn err;\n}\n\nstatic void xdp_test_run_teardown(struct xdp_test_data *xdp)\n{\n\txdp_unreg_mem_model(&xdp->mem);\n\tpage_pool_destroy(xdp->pp);\n\tkfree(xdp->frames);\n\tkfree(xdp->skbs);\n}\n\nstatic bool frame_was_changed(const struct xdp_page_head *head)\n{\n\t \n\treturn head->frame->data != head->orig_ctx.data ||\n\t       head->frame->flags != head->orig_ctx.flags;\n}\n\nstatic bool ctx_was_changed(struct xdp_page_head *head)\n{\n\treturn head->orig_ctx.data != head->ctx.data ||\n\t\thead->orig_ctx.data_meta != head->ctx.data_meta ||\n\t\thead->orig_ctx.data_end != head->ctx.data_end;\n}\n\nstatic void reset_ctx(struct xdp_page_head *head)\n{\n\tif (likely(!frame_was_changed(head) && !ctx_was_changed(head)))\n\t\treturn;\n\n\thead->ctx.data = head->orig_ctx.data;\n\thead->ctx.data_meta = head->orig_ctx.data_meta;\n\thead->ctx.data_end = head->orig_ctx.data_end;\n\txdp_update_frame_from_buff(&head->ctx, head->frame);\n}\n\nstatic int xdp_recv_frames(struct xdp_frame **frames, int nframes,\n\t\t\t   struct sk_buff **skbs,\n\t\t\t   struct net_device *dev)\n{\n\tgfp_t gfp = __GFP_ZERO | GFP_ATOMIC;\n\tint i, n;\n\tLIST_HEAD(list);\n\n\tn = kmem_cache_alloc_bulk(skbuff_cache, gfp, nframes, (void **)skbs);\n\tif (unlikely(n == 0)) {\n\t\tfor (i = 0; i < nframes; i++)\n\t\t\txdp_return_frame(frames[i]);\n\t\treturn -ENOMEM;\n\t}\n\n\tfor (i = 0; i < nframes; i++) {\n\t\tstruct xdp_frame *xdpf = frames[i];\n\t\tstruct sk_buff *skb = skbs[i];\n\n\t\tskb = __xdp_build_skb_from_frame(xdpf, skb, dev);\n\t\tif (!skb) {\n\t\t\txdp_return_frame(xdpf);\n\t\t\tcontinue;\n\t\t}\n\n\t\tlist_add_tail(&skb->list, &list);\n\t}\n\tnetif_receive_skb_list(&list);\n\n\treturn 0;\n}\n\nstatic int xdp_test_run_batch(struct xdp_test_data *xdp, struct bpf_prog *prog,\n\t\t\t      u32 repeat)\n{\n\tstruct bpf_redirect_info *ri = this_cpu_ptr(&bpf_redirect_info);\n\tint err = 0, act, ret, i, nframes = 0, batch_sz;\n\tstruct xdp_frame **frames = xdp->frames;\n\tstruct xdp_page_head *head;\n\tstruct xdp_frame *frm;\n\tbool redirect = false;\n\tstruct xdp_buff *ctx;\n\tstruct page *page;\n\n\tbatch_sz = min_t(u32, repeat, xdp->batch_size);\n\n\tlocal_bh_disable();\n\txdp_set_return_frame_no_direct();\n\n\tfor (i = 0; i < batch_sz; i++) {\n\t\tpage = page_pool_dev_alloc_pages(xdp->pp);\n\t\tif (!page) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\n\t\thead = phys_to_virt(page_to_phys(page));\n\t\treset_ctx(head);\n\t\tctx = &head->ctx;\n\t\tfrm = head->frame;\n\t\txdp->frame_cnt++;\n\n\t\tact = bpf_prog_run_xdp(prog, ctx);\n\n\t\t \n\t\tif (unlikely(ctx_was_changed(head))) {\n\t\t\tret = xdp_update_frame_from_buff(ctx, frm);\n\t\t\tif (ret) {\n\t\t\t\txdp_return_buff(ctx);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\tswitch (act) {\n\t\tcase XDP_TX:\n\t\t\t \n\t\t\tri->tgt_index = xdp->dev->ifindex;\n\t\t\tri->map_id = INT_MAX;\n\t\t\tri->map_type = BPF_MAP_TYPE_UNSPEC;\n\t\t\tfallthrough;\n\t\tcase XDP_REDIRECT:\n\t\t\tredirect = true;\n\t\t\tret = xdp_do_redirect_frame(xdp->dev, ctx, frm, prog);\n\t\t\tif (ret)\n\t\t\t\txdp_return_buff(ctx);\n\t\t\tbreak;\n\t\tcase XDP_PASS:\n\t\t\tframes[nframes++] = frm;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbpf_warn_invalid_xdp_action(NULL, prog, act);\n\t\t\tfallthrough;\n\t\tcase XDP_DROP:\n\t\t\txdp_return_buff(ctx);\n\t\t\tbreak;\n\t\t}\n\t}\n\nout:\n\tif (redirect)\n\t\txdp_do_flush();\n\tif (nframes) {\n\t\tret = xdp_recv_frames(frames, nframes, xdp->skbs, xdp->dev);\n\t\tif (ret)\n\t\t\terr = ret;\n\t}\n\n\txdp_clear_return_frame_no_direct();\n\tlocal_bh_enable();\n\treturn err;\n}\n\nstatic int bpf_test_run_xdp_live(struct bpf_prog *prog, struct xdp_buff *ctx,\n\t\t\t\t u32 repeat, u32 batch_size, u32 *time)\n\n{\n\tstruct xdp_test_data xdp = { .batch_size = batch_size };\n\tstruct bpf_test_timer t = { .mode = NO_MIGRATE };\n\tint ret;\n\n\tif (!repeat)\n\t\trepeat = 1;\n\n\tret = xdp_test_run_setup(&xdp, ctx);\n\tif (ret)\n\t\treturn ret;\n\n\tbpf_test_timer_enter(&t);\n\tdo {\n\t\txdp.frame_cnt = 0;\n\t\tret = xdp_test_run_batch(&xdp, prog, repeat - t.i);\n\t\tif (unlikely(ret < 0))\n\t\t\tbreak;\n\t} while (bpf_test_timer_continue(&t, xdp.frame_cnt, repeat, &ret, time));\n\tbpf_test_timer_leave(&t);\n\n\txdp_test_run_teardown(&xdp);\n\treturn ret;\n}\n\nstatic int bpf_test_run(struct bpf_prog *prog, void *ctx, u32 repeat,\n\t\t\tu32 *retval, u32 *time, bool xdp)\n{\n\tstruct bpf_prog_array_item item = {.prog = prog};\n\tstruct bpf_run_ctx *old_ctx;\n\tstruct bpf_cg_run_ctx run_ctx;\n\tstruct bpf_test_timer t = { NO_MIGRATE };\n\tenum bpf_cgroup_storage_type stype;\n\tint ret;\n\n\tfor_each_cgroup_storage_type(stype) {\n\t\titem.cgroup_storage[stype] = bpf_cgroup_storage_alloc(prog, stype);\n\t\tif (IS_ERR(item.cgroup_storage[stype])) {\n\t\t\titem.cgroup_storage[stype] = NULL;\n\t\t\tfor_each_cgroup_storage_type(stype)\n\t\t\t\tbpf_cgroup_storage_free(item.cgroup_storage[stype]);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\tif (!repeat)\n\t\trepeat = 1;\n\n\tbpf_test_timer_enter(&t);\n\told_ctx = bpf_set_run_ctx(&run_ctx.run_ctx);\n\tdo {\n\t\trun_ctx.prog_item = &item;\n\t\tlocal_bh_disable();\n\t\tif (xdp)\n\t\t\t*retval = bpf_prog_run_xdp(prog, ctx);\n\t\telse\n\t\t\t*retval = bpf_prog_run(prog, ctx);\n\t\tlocal_bh_enable();\n\t} while (bpf_test_timer_continue(&t, 1, repeat, &ret, time));\n\tbpf_reset_run_ctx(old_ctx);\n\tbpf_test_timer_leave(&t);\n\n\tfor_each_cgroup_storage_type(stype)\n\t\tbpf_cgroup_storage_free(item.cgroup_storage[stype]);\n\n\treturn ret;\n}\n\nstatic int bpf_test_finish(const union bpf_attr *kattr,\n\t\t\t   union bpf_attr __user *uattr, const void *data,\n\t\t\t   struct skb_shared_info *sinfo, u32 size,\n\t\t\t   u32 retval, u32 duration)\n{\n\tvoid __user *data_out = u64_to_user_ptr(kattr->test.data_out);\n\tint err = -EFAULT;\n\tu32 copy_size = size;\n\n\t \n\tif (kattr->test.data_size_out &&\n\t    copy_size > kattr->test.data_size_out) {\n\t\tcopy_size = kattr->test.data_size_out;\n\t\terr = -ENOSPC;\n\t}\n\n\tif (data_out) {\n\t\tint len = sinfo ? copy_size - sinfo->xdp_frags_size : copy_size;\n\n\t\tif (len < 0) {\n\t\t\terr = -ENOSPC;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (copy_to_user(data_out, data, len))\n\t\t\tgoto out;\n\n\t\tif (sinfo) {\n\t\t\tint i, offset = len;\n\t\t\tu32 data_len;\n\n\t\t\tfor (i = 0; i < sinfo->nr_frags; i++) {\n\t\t\t\tskb_frag_t *frag = &sinfo->frags[i];\n\n\t\t\t\tif (offset >= copy_size) {\n\t\t\t\t\terr = -ENOSPC;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\tdata_len = min_t(u32, copy_size - offset,\n\t\t\t\t\t\t skb_frag_size(frag));\n\n\t\t\t\tif (copy_to_user(data_out + offset,\n\t\t\t\t\t\t skb_frag_address(frag),\n\t\t\t\t\t\t data_len))\n\t\t\t\t\tgoto out;\n\n\t\t\t\toffset += data_len;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (copy_to_user(&uattr->test.data_size_out, &size, sizeof(size)))\n\t\tgoto out;\n\tif (copy_to_user(&uattr->test.retval, &retval, sizeof(retval)))\n\t\tgoto out;\n\tif (copy_to_user(&uattr->test.duration, &duration, sizeof(duration)))\n\t\tgoto out;\n\tif (err != -ENOSPC)\n\t\terr = 0;\nout:\n\ttrace_bpf_test_finish(&err);\n\treturn err;\n}\n\n \n__diag_push();\n__diag_ignore_all(\"-Wmissing-prototypes\",\n\t\t  \"Global functions as their definitions will be in vmlinux BTF\");\n__bpf_kfunc int bpf_fentry_test1(int a)\n{\n\treturn a + 1;\n}\nEXPORT_SYMBOL_GPL(bpf_fentry_test1);\n\nint noinline bpf_fentry_test2(int a, u64 b)\n{\n\treturn a + b;\n}\n\nint noinline bpf_fentry_test3(char a, int b, u64 c)\n{\n\treturn a + b + c;\n}\n\nint noinline bpf_fentry_test4(void *a, char b, int c, u64 d)\n{\n\treturn (long)a + b + c + d;\n}\n\nint noinline bpf_fentry_test5(u64 a, void *b, short c, int d, u64 e)\n{\n\treturn a + (long)b + c + d + e;\n}\n\nint noinline bpf_fentry_test6(u64 a, void *b, short c, int d, void *e, u64 f)\n{\n\treturn a + (long)b + c + d + (long)e + f;\n}\n\nstruct bpf_fentry_test_t {\n\tstruct bpf_fentry_test_t *a;\n};\n\nint noinline bpf_fentry_test7(struct bpf_fentry_test_t *arg)\n{\n\tasm volatile (\"\");\n\treturn (long)arg;\n}\n\nint noinline bpf_fentry_test8(struct bpf_fentry_test_t *arg)\n{\n\treturn (long)arg->a;\n}\n\n__bpf_kfunc u32 bpf_fentry_test9(u32 *a)\n{\n\treturn *a;\n}\n\nvoid noinline bpf_fentry_test_sinfo(struct skb_shared_info *sinfo)\n{\n}\n\n__bpf_kfunc int bpf_modify_return_test(int a, int *b)\n{\n\t*b += 1;\n\treturn a + *b;\n}\n\n__bpf_kfunc int bpf_modify_return_test2(int a, int *b, short c, int d,\n\t\t\t\t\tvoid *e, char f, int g)\n{\n\t*b += 1;\n\treturn a + *b + c + d + (long)e + f + g;\n}\n\nint noinline bpf_fentry_shadow_test(int a)\n{\n\treturn a + 1;\n}\n\nstruct prog_test_member1 {\n\tint a;\n};\n\nstruct prog_test_member {\n\tstruct prog_test_member1 m;\n\tint c;\n};\n\nstruct prog_test_ref_kfunc {\n\tint a;\n\tint b;\n\tstruct prog_test_member memb;\n\tstruct prog_test_ref_kfunc *next;\n\trefcount_t cnt;\n};\n\n__bpf_kfunc void bpf_kfunc_call_test_release(struct prog_test_ref_kfunc *p)\n{\n\trefcount_dec(&p->cnt);\n}\n\n__bpf_kfunc void bpf_kfunc_call_memb_release(struct prog_test_member *p)\n{\n}\n\n__diag_pop();\n\nBTF_SET8_START(bpf_test_modify_return_ids)\nBTF_ID_FLAGS(func, bpf_modify_return_test)\nBTF_ID_FLAGS(func, bpf_modify_return_test2)\nBTF_ID_FLAGS(func, bpf_fentry_test1, KF_SLEEPABLE)\nBTF_SET8_END(bpf_test_modify_return_ids)\n\nstatic const struct btf_kfunc_id_set bpf_test_modify_return_set = {\n\t.owner = THIS_MODULE,\n\t.set   = &bpf_test_modify_return_ids,\n};\n\nBTF_SET8_START(test_sk_check_kfunc_ids)\nBTF_ID_FLAGS(func, bpf_kfunc_call_test_release, KF_RELEASE)\nBTF_ID_FLAGS(func, bpf_kfunc_call_memb_release, KF_RELEASE)\nBTF_SET8_END(test_sk_check_kfunc_ids)\n\nstatic void *bpf_test_init(const union bpf_attr *kattr, u32 user_size,\n\t\t\t   u32 size, u32 headroom, u32 tailroom)\n{\n\tvoid __user *data_in = u64_to_user_ptr(kattr->test.data_in);\n\tvoid *data;\n\n\tif (size < ETH_HLEN || size > PAGE_SIZE - headroom - tailroom)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (user_size > size)\n\t\treturn ERR_PTR(-EMSGSIZE);\n\n\tsize = SKB_DATA_ALIGN(size);\n\tdata = kzalloc(size + headroom + tailroom, GFP_USER);\n\tif (!data)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (copy_from_user(data + headroom, data_in, user_size)) {\n\t\tkfree(data);\n\t\treturn ERR_PTR(-EFAULT);\n\t}\n\n\treturn data;\n}\n\nint bpf_prog_test_run_tracing(struct bpf_prog *prog,\n\t\t\t      const union bpf_attr *kattr,\n\t\t\t      union bpf_attr __user *uattr)\n{\n\tstruct bpf_fentry_test_t arg = {};\n\tu16 side_effect = 0, ret = 0;\n\tint b = 2, err = -EFAULT;\n\tu32 retval = 0;\n\n\tif (kattr->test.flags || kattr->test.cpu || kattr->test.batch_size)\n\t\treturn -EINVAL;\n\n\tswitch (prog->expected_attach_type) {\n\tcase BPF_TRACE_FENTRY:\n\tcase BPF_TRACE_FEXIT:\n\t\tif (bpf_fentry_test1(1) != 2 ||\n\t\t    bpf_fentry_test2(2, 3) != 5 ||\n\t\t    bpf_fentry_test3(4, 5, 6) != 15 ||\n\t\t    bpf_fentry_test4((void *)7, 8, 9, 10) != 34 ||\n\t\t    bpf_fentry_test5(11, (void *)12, 13, 14, 15) != 65 ||\n\t\t    bpf_fentry_test6(16, (void *)17, 18, 19, (void *)20, 21) != 111 ||\n\t\t    bpf_fentry_test7((struct bpf_fentry_test_t *)0) != 0 ||\n\t\t    bpf_fentry_test8(&arg) != 0 ||\n\t\t    bpf_fentry_test9(&retval) != 0)\n\t\t\tgoto out;\n\t\tbreak;\n\tcase BPF_MODIFY_RETURN:\n\t\tret = bpf_modify_return_test(1, &b);\n\t\tif (b != 2)\n\t\t\tside_effect++;\n\t\tb = 2;\n\t\tret += bpf_modify_return_test2(1, &b, 3, 4, (void *)5, 6, 7);\n\t\tif (b != 2)\n\t\t\tside_effect++;\n\t\tbreak;\n\tdefault:\n\t\tgoto out;\n\t}\n\n\tretval = ((u32)side_effect << 16) | ret;\n\tif (copy_to_user(&uattr->test.retval, &retval, sizeof(retval)))\n\t\tgoto out;\n\n\terr = 0;\nout:\n\ttrace_bpf_test_finish(&err);\n\treturn err;\n}\n\nstruct bpf_raw_tp_test_run_info {\n\tstruct bpf_prog *prog;\n\tvoid *ctx;\n\tu32 retval;\n};\n\nstatic void\n__bpf_prog_test_run_raw_tp(void *data)\n{\n\tstruct bpf_raw_tp_test_run_info *info = data;\n\n\trcu_read_lock();\n\tinfo->retval = bpf_prog_run(info->prog, info->ctx);\n\trcu_read_unlock();\n}\n\nint bpf_prog_test_run_raw_tp(struct bpf_prog *prog,\n\t\t\t     const union bpf_attr *kattr,\n\t\t\t     union bpf_attr __user *uattr)\n{\n\tvoid __user *ctx_in = u64_to_user_ptr(kattr->test.ctx_in);\n\t__u32 ctx_size_in = kattr->test.ctx_size_in;\n\tstruct bpf_raw_tp_test_run_info info;\n\tint cpu = kattr->test.cpu, err = 0;\n\tint current_cpu;\n\n\t \n\tif (kattr->test.data_in || kattr->test.data_out ||\n\t    kattr->test.ctx_out || kattr->test.duration ||\n\t    kattr->test.repeat || kattr->test.batch_size)\n\t\treturn -EINVAL;\n\n\tif (ctx_size_in < prog->aux->max_ctx_offset ||\n\t    ctx_size_in > MAX_BPF_FUNC_ARGS * sizeof(u64))\n\t\treturn -EINVAL;\n\n\tif ((kattr->test.flags & BPF_F_TEST_RUN_ON_CPU) == 0 && cpu != 0)\n\t\treturn -EINVAL;\n\n\tif (ctx_size_in) {\n\t\tinfo.ctx = memdup_user(ctx_in, ctx_size_in);\n\t\tif (IS_ERR(info.ctx))\n\t\t\treturn PTR_ERR(info.ctx);\n\t} else {\n\t\tinfo.ctx = NULL;\n\t}\n\n\tinfo.prog = prog;\n\n\tcurrent_cpu = get_cpu();\n\tif ((kattr->test.flags & BPF_F_TEST_RUN_ON_CPU) == 0 ||\n\t    cpu == current_cpu) {\n\t\t__bpf_prog_test_run_raw_tp(&info);\n\t} else if (cpu >= nr_cpu_ids || !cpu_online(cpu)) {\n\t\t \n\t\terr = -ENXIO;\n\t} else {\n\t\terr = smp_call_function_single(cpu, __bpf_prog_test_run_raw_tp,\n\t\t\t\t\t       &info, 1);\n\t}\n\tput_cpu();\n\n\tif (!err &&\n\t    copy_to_user(&uattr->test.retval, &info.retval, sizeof(u32)))\n\t\terr = -EFAULT;\n\n\tkfree(info.ctx);\n\treturn err;\n}\n\nstatic void *bpf_ctx_init(const union bpf_attr *kattr, u32 max_size)\n{\n\tvoid __user *data_in = u64_to_user_ptr(kattr->test.ctx_in);\n\tvoid __user *data_out = u64_to_user_ptr(kattr->test.ctx_out);\n\tu32 size = kattr->test.ctx_size_in;\n\tvoid *data;\n\tint err;\n\n\tif (!data_in && !data_out)\n\t\treturn NULL;\n\n\tdata = kzalloc(max_size, GFP_USER);\n\tif (!data)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (data_in) {\n\t\terr = bpf_check_uarg_tail_zero(USER_BPFPTR(data_in), max_size, size);\n\t\tif (err) {\n\t\t\tkfree(data);\n\t\t\treturn ERR_PTR(err);\n\t\t}\n\n\t\tsize = min_t(u32, max_size, size);\n\t\tif (copy_from_user(data, data_in, size)) {\n\t\t\tkfree(data);\n\t\t\treturn ERR_PTR(-EFAULT);\n\t\t}\n\t}\n\treturn data;\n}\n\nstatic int bpf_ctx_finish(const union bpf_attr *kattr,\n\t\t\t  union bpf_attr __user *uattr, const void *data,\n\t\t\t  u32 size)\n{\n\tvoid __user *data_out = u64_to_user_ptr(kattr->test.ctx_out);\n\tint err = -EFAULT;\n\tu32 copy_size = size;\n\n\tif (!data || !data_out)\n\t\treturn 0;\n\n\tif (copy_size > kattr->test.ctx_size_out) {\n\t\tcopy_size = kattr->test.ctx_size_out;\n\t\terr = -ENOSPC;\n\t}\n\n\tif (copy_to_user(data_out, data, copy_size))\n\t\tgoto out;\n\tif (copy_to_user(&uattr->test.ctx_size_out, &size, sizeof(size)))\n\t\tgoto out;\n\tif (err != -ENOSPC)\n\t\terr = 0;\nout:\n\treturn err;\n}\n\n \nstatic inline bool range_is_zero(void *buf, size_t from, size_t to)\n{\n\treturn !memchr_inv((u8 *)buf + from, 0, to - from);\n}\n\nstatic int convert___skb_to_skb(struct sk_buff *skb, struct __sk_buff *__skb)\n{\n\tstruct qdisc_skb_cb *cb = (struct qdisc_skb_cb *)skb->cb;\n\n\tif (!__skb)\n\t\treturn 0;\n\n\t \n\tif (!range_is_zero(__skb, 0, offsetof(struct __sk_buff, mark)))\n\t\treturn -EINVAL;\n\n\t \n\n\tif (!range_is_zero(__skb, offsetofend(struct __sk_buff, mark),\n\t\t\t   offsetof(struct __sk_buff, priority)))\n\t\treturn -EINVAL;\n\n\t \n\t \n\t \n\n\tif (!range_is_zero(__skb, offsetofend(struct __sk_buff, ifindex),\n\t\t\t   offsetof(struct __sk_buff, cb)))\n\t\treturn -EINVAL;\n\n\t \n\n\tif (!range_is_zero(__skb, offsetofend(struct __sk_buff, cb),\n\t\t\t   offsetof(struct __sk_buff, tstamp)))\n\t\treturn -EINVAL;\n\n\t \n\t \n\t \n\n\tif (!range_is_zero(__skb, offsetofend(struct __sk_buff, gso_segs),\n\t\t\t   offsetof(struct __sk_buff, gso_size)))\n\t\treturn -EINVAL;\n\n\t \n\n\tif (!range_is_zero(__skb, offsetofend(struct __sk_buff, gso_size),\n\t\t\t   offsetof(struct __sk_buff, hwtstamp)))\n\t\treturn -EINVAL;\n\n\t \n\n\tif (!range_is_zero(__skb, offsetofend(struct __sk_buff, hwtstamp),\n\t\t\t   sizeof(struct __sk_buff)))\n\t\treturn -EINVAL;\n\n\tskb->mark = __skb->mark;\n\tskb->priority = __skb->priority;\n\tskb->skb_iif = __skb->ingress_ifindex;\n\tskb->tstamp = __skb->tstamp;\n\tmemcpy(&cb->data, __skb->cb, QDISC_CB_PRIV_LEN);\n\n\tif (__skb->wire_len == 0) {\n\t\tcb->pkt_len = skb->len;\n\t} else {\n\t\tif (__skb->wire_len < skb->len ||\n\t\t    __skb->wire_len > GSO_LEGACY_MAX_SIZE)\n\t\t\treturn -EINVAL;\n\t\tcb->pkt_len = __skb->wire_len;\n\t}\n\n\tif (__skb->gso_segs > GSO_MAX_SEGS)\n\t\treturn -EINVAL;\n\tskb_shinfo(skb)->gso_segs = __skb->gso_segs;\n\tskb_shinfo(skb)->gso_size = __skb->gso_size;\n\tskb_shinfo(skb)->hwtstamps.hwtstamp = __skb->hwtstamp;\n\n\treturn 0;\n}\n\nstatic void convert_skb_to___skb(struct sk_buff *skb, struct __sk_buff *__skb)\n{\n\tstruct qdisc_skb_cb *cb = (struct qdisc_skb_cb *)skb->cb;\n\n\tif (!__skb)\n\t\treturn;\n\n\t__skb->mark = skb->mark;\n\t__skb->priority = skb->priority;\n\t__skb->ingress_ifindex = skb->skb_iif;\n\t__skb->ifindex = skb->dev->ifindex;\n\t__skb->tstamp = skb->tstamp;\n\tmemcpy(__skb->cb, &cb->data, QDISC_CB_PRIV_LEN);\n\t__skb->wire_len = cb->pkt_len;\n\t__skb->gso_segs = skb_shinfo(skb)->gso_segs;\n\t__skb->hwtstamp = skb_shinfo(skb)->hwtstamps.hwtstamp;\n}\n\nstatic struct proto bpf_dummy_proto = {\n\t.name   = \"bpf_dummy\",\n\t.owner  = THIS_MODULE,\n\t.obj_size = sizeof(struct sock),\n};\n\nint bpf_prog_test_run_skb(struct bpf_prog *prog, const union bpf_attr *kattr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tbool is_l2 = false, is_direct_pkt_access = false;\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct net_device *dev = net->loopback_dev;\n\tu32 size = kattr->test.data_size_in;\n\tu32 repeat = kattr->test.repeat;\n\tstruct __sk_buff *ctx = NULL;\n\tu32 retval, duration;\n\tint hh_len = ETH_HLEN;\n\tstruct sk_buff *skb;\n\tstruct sock *sk;\n\tvoid *data;\n\tint ret;\n\n\tif (kattr->test.flags || kattr->test.cpu || kattr->test.batch_size)\n\t\treturn -EINVAL;\n\n\tdata = bpf_test_init(kattr, kattr->test.data_size_in,\n\t\t\t     size, NET_SKB_PAD + NET_IP_ALIGN,\n\t\t\t     SKB_DATA_ALIGN(sizeof(struct skb_shared_info)));\n\tif (IS_ERR(data))\n\t\treturn PTR_ERR(data);\n\n\tctx = bpf_ctx_init(kattr, sizeof(struct __sk_buff));\n\tif (IS_ERR(ctx)) {\n\t\tkfree(data);\n\t\treturn PTR_ERR(ctx);\n\t}\n\n\tswitch (prog->type) {\n\tcase BPF_PROG_TYPE_SCHED_CLS:\n\tcase BPF_PROG_TYPE_SCHED_ACT:\n\t\tis_l2 = true;\n\t\tfallthrough;\n\tcase BPF_PROG_TYPE_LWT_IN:\n\tcase BPF_PROG_TYPE_LWT_OUT:\n\tcase BPF_PROG_TYPE_LWT_XMIT:\n\t\tis_direct_pkt_access = true;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tsk = sk_alloc(net, AF_UNSPEC, GFP_USER, &bpf_dummy_proto, 1);\n\tif (!sk) {\n\t\tkfree(data);\n\t\tkfree(ctx);\n\t\treturn -ENOMEM;\n\t}\n\tsock_init_data(NULL, sk);\n\n\tskb = slab_build_skb(data);\n\tif (!skb) {\n\t\tkfree(data);\n\t\tkfree(ctx);\n\t\tsk_free(sk);\n\t\treturn -ENOMEM;\n\t}\n\tskb->sk = sk;\n\n\tskb_reserve(skb, NET_SKB_PAD + NET_IP_ALIGN);\n\t__skb_put(skb, size);\n\tif (ctx && ctx->ifindex > 1) {\n\t\tdev = dev_get_by_index(net, ctx->ifindex);\n\t\tif (!dev) {\n\t\t\tret = -ENODEV;\n\t\t\tgoto out;\n\t\t}\n\t}\n\tskb->protocol = eth_type_trans(skb, dev);\n\tskb_reset_network_header(skb);\n\n\tswitch (skb->protocol) {\n\tcase htons(ETH_P_IP):\n\t\tsk->sk_family = AF_INET;\n\t\tif (sizeof(struct iphdr) <= skb_headlen(skb)) {\n\t\t\tsk->sk_rcv_saddr = ip_hdr(skb)->saddr;\n\t\t\tsk->sk_daddr = ip_hdr(skb)->daddr;\n\t\t}\n\t\tbreak;\n#if IS_ENABLED(CONFIG_IPV6)\n\tcase htons(ETH_P_IPV6):\n\t\tsk->sk_family = AF_INET6;\n\t\tif (sizeof(struct ipv6hdr) <= skb_headlen(skb)) {\n\t\t\tsk->sk_v6_rcv_saddr = ipv6_hdr(skb)->saddr;\n\t\t\tsk->sk_v6_daddr = ipv6_hdr(skb)->daddr;\n\t\t}\n\t\tbreak;\n#endif\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (is_l2)\n\t\t__skb_push(skb, hh_len);\n\tif (is_direct_pkt_access)\n\t\tbpf_compute_data_pointers(skb);\n\tret = convert___skb_to_skb(skb, ctx);\n\tif (ret)\n\t\tgoto out;\n\tret = bpf_test_run(prog, skb, repeat, &retval, &duration, false);\n\tif (ret)\n\t\tgoto out;\n\tif (!is_l2) {\n\t\tif (skb_headroom(skb) < hh_len) {\n\t\t\tint nhead = HH_DATA_ALIGN(hh_len - skb_headroom(skb));\n\n\t\t\tif (pskb_expand_head(skb, nhead, 0, GFP_USER)) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\tmemset(__skb_push(skb, hh_len), 0, hh_len);\n\t}\n\tconvert_skb_to___skb(skb, ctx);\n\n\tsize = skb->len;\n\t \n\tif (WARN_ON_ONCE(skb_is_nonlinear(skb)))\n\t\tsize = skb_headlen(skb);\n\tret = bpf_test_finish(kattr, uattr, skb->data, NULL, size, retval,\n\t\t\t      duration);\n\tif (!ret)\n\t\tret = bpf_ctx_finish(kattr, uattr, ctx,\n\t\t\t\t     sizeof(struct __sk_buff));\nout:\n\tif (dev && dev != net->loopback_dev)\n\t\tdev_put(dev);\n\tkfree_skb(skb);\n\tsk_free(sk);\n\tkfree(ctx);\n\treturn ret;\n}\n\nstatic int xdp_convert_md_to_buff(struct xdp_md *xdp_md, struct xdp_buff *xdp)\n{\n\tunsigned int ingress_ifindex, rx_queue_index;\n\tstruct netdev_rx_queue *rxqueue;\n\tstruct net_device *device;\n\n\tif (!xdp_md)\n\t\treturn 0;\n\n\tif (xdp_md->egress_ifindex != 0)\n\t\treturn -EINVAL;\n\n\tingress_ifindex = xdp_md->ingress_ifindex;\n\trx_queue_index = xdp_md->rx_queue_index;\n\n\tif (!ingress_ifindex && rx_queue_index)\n\t\treturn -EINVAL;\n\n\tif (ingress_ifindex) {\n\t\tdevice = dev_get_by_index(current->nsproxy->net_ns,\n\t\t\t\t\t  ingress_ifindex);\n\t\tif (!device)\n\t\t\treturn -ENODEV;\n\n\t\tif (rx_queue_index >= device->real_num_rx_queues)\n\t\t\tgoto free_dev;\n\n\t\trxqueue = __netif_get_rx_queue(device, rx_queue_index);\n\n\t\tif (!xdp_rxq_info_is_reg(&rxqueue->xdp_rxq))\n\t\t\tgoto free_dev;\n\n\t\txdp->rxq = &rxqueue->xdp_rxq;\n\t\t \n\t}\n\n\txdp->data = xdp->data_meta + xdp_md->data;\n\treturn 0;\n\nfree_dev:\n\tdev_put(device);\n\treturn -EINVAL;\n}\n\nstatic void xdp_convert_buff_to_md(struct xdp_buff *xdp, struct xdp_md *xdp_md)\n{\n\tif (!xdp_md)\n\t\treturn;\n\n\txdp_md->data = xdp->data - xdp->data_meta;\n\txdp_md->data_end = xdp->data_end - xdp->data_meta;\n\n\tif (xdp_md->ingress_ifindex)\n\t\tdev_put(xdp->rxq->dev);\n}\n\nint bpf_prog_test_run_xdp(struct bpf_prog *prog, const union bpf_attr *kattr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tbool do_live = (kattr->test.flags & BPF_F_TEST_XDP_LIVE_FRAMES);\n\tu32 tailroom = SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\tu32 batch_size = kattr->test.batch_size;\n\tu32 retval = 0, duration, max_data_sz;\n\tu32 size = kattr->test.data_size_in;\n\tu32 headroom = XDP_PACKET_HEADROOM;\n\tu32 repeat = kattr->test.repeat;\n\tstruct netdev_rx_queue *rxqueue;\n\tstruct skb_shared_info *sinfo;\n\tstruct xdp_buff xdp = {};\n\tint i, ret = -EINVAL;\n\tstruct xdp_md *ctx;\n\tvoid *data;\n\n\tif (prog->expected_attach_type == BPF_XDP_DEVMAP ||\n\t    prog->expected_attach_type == BPF_XDP_CPUMAP)\n\t\treturn -EINVAL;\n\n\tif (kattr->test.flags & ~BPF_F_TEST_XDP_LIVE_FRAMES)\n\t\treturn -EINVAL;\n\n\tif (bpf_prog_is_dev_bound(prog->aux))\n\t\treturn -EINVAL;\n\n\tif (do_live) {\n\t\tif (!batch_size)\n\t\t\tbatch_size = NAPI_POLL_WEIGHT;\n\t\telse if (batch_size > TEST_XDP_MAX_BATCH)\n\t\t\treturn -E2BIG;\n\n\t\theadroom += sizeof(struct xdp_page_head);\n\t} else if (batch_size) {\n\t\treturn -EINVAL;\n\t}\n\n\tctx = bpf_ctx_init(kattr, sizeof(struct xdp_md));\n\tif (IS_ERR(ctx))\n\t\treturn PTR_ERR(ctx);\n\n\tif (ctx) {\n\t\t \n\t\tif (ctx->data_meta || ctx->data_end != size ||\n\t\t    ctx->data > ctx->data_end ||\n\t\t    unlikely(xdp_metalen_invalid(ctx->data)) ||\n\t\t    (do_live && (kattr->test.data_out || kattr->test.ctx_out)))\n\t\t\tgoto free_ctx;\n\t\t \n\t\theadroom -= ctx->data;\n\t}\n\n\tmax_data_sz = 4096 - headroom - tailroom;\n\tif (size > max_data_sz) {\n\t\t \n\t\tif (do_live)\n\t\t\tgoto free_ctx;\n\t\tsize = max_data_sz;\n\t}\n\n\tdata = bpf_test_init(kattr, size, max_data_sz, headroom, tailroom);\n\tif (IS_ERR(data)) {\n\t\tret = PTR_ERR(data);\n\t\tgoto free_ctx;\n\t}\n\n\trxqueue = __netif_get_rx_queue(current->nsproxy->net_ns->loopback_dev, 0);\n\trxqueue->xdp_rxq.frag_size = headroom + max_data_sz + tailroom;\n\txdp_init_buff(&xdp, rxqueue->xdp_rxq.frag_size, &rxqueue->xdp_rxq);\n\txdp_prepare_buff(&xdp, data, headroom, size, true);\n\tsinfo = xdp_get_shared_info_from_buff(&xdp);\n\n\tret = xdp_convert_md_to_buff(ctx, &xdp);\n\tif (ret)\n\t\tgoto free_data;\n\n\tif (unlikely(kattr->test.data_size_in > size)) {\n\t\tvoid __user *data_in = u64_to_user_ptr(kattr->test.data_in);\n\n\t\twhile (size < kattr->test.data_size_in) {\n\t\t\tstruct page *page;\n\t\t\tskb_frag_t *frag;\n\t\t\tu32 data_len;\n\n\t\t\tif (sinfo->nr_frags == MAX_SKB_FRAGS) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tpage = alloc_page(GFP_KERNEL);\n\t\t\tif (!page) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tfrag = &sinfo->frags[sinfo->nr_frags++];\n\n\t\t\tdata_len = min_t(u32, kattr->test.data_size_in - size,\n\t\t\t\t\t PAGE_SIZE);\n\t\t\tskb_frag_fill_page_desc(frag, page, 0, data_len);\n\n\t\t\tif (copy_from_user(page_address(page), data_in + size,\n\t\t\t\t\t   data_len)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tsinfo->xdp_frags_size += data_len;\n\t\t\tsize += data_len;\n\t\t}\n\t\txdp_buff_set_frags_flag(&xdp);\n\t}\n\n\tif (repeat > 1)\n\t\tbpf_prog_change_xdp(NULL, prog);\n\n\tif (do_live)\n\t\tret = bpf_test_run_xdp_live(prog, &xdp, repeat, batch_size, &duration);\n\telse\n\t\tret = bpf_test_run(prog, &xdp, repeat, &retval, &duration, true);\n\t \n\txdp_convert_buff_to_md(&xdp, ctx);\n\tif (ret)\n\t\tgoto out;\n\n\tsize = xdp.data_end - xdp.data_meta + sinfo->xdp_frags_size;\n\tret = bpf_test_finish(kattr, uattr, xdp.data_meta, sinfo, size,\n\t\t\t      retval, duration);\n\tif (!ret)\n\t\tret = bpf_ctx_finish(kattr, uattr, ctx,\n\t\t\t\t     sizeof(struct xdp_md));\n\nout:\n\tif (repeat > 1)\n\t\tbpf_prog_change_xdp(prog, NULL);\nfree_data:\n\tfor (i = 0; i < sinfo->nr_frags; i++)\n\t\t__free_page(skb_frag_page(&sinfo->frags[i]));\n\tkfree(data);\nfree_ctx:\n\tkfree(ctx);\n\treturn ret;\n}\n\nstatic int verify_user_bpf_flow_keys(struct bpf_flow_keys *ctx)\n{\n\t \n\tif (!range_is_zero(ctx, 0, offsetof(struct bpf_flow_keys, flags)))\n\t\treturn -EINVAL;\n\n\t \n\n\tif (!range_is_zero(ctx, offsetofend(struct bpf_flow_keys, flags),\n\t\t\t   sizeof(struct bpf_flow_keys)))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nint bpf_prog_test_run_flow_dissector(struct bpf_prog *prog,\n\t\t\t\t     const union bpf_attr *kattr,\n\t\t\t\t     union bpf_attr __user *uattr)\n{\n\tstruct bpf_test_timer t = { NO_PREEMPT };\n\tu32 size = kattr->test.data_size_in;\n\tstruct bpf_flow_dissector ctx = {};\n\tu32 repeat = kattr->test.repeat;\n\tstruct bpf_flow_keys *user_ctx;\n\tstruct bpf_flow_keys flow_keys;\n\tconst struct ethhdr *eth;\n\tunsigned int flags = 0;\n\tu32 retval, duration;\n\tvoid *data;\n\tint ret;\n\n\tif (kattr->test.flags || kattr->test.cpu || kattr->test.batch_size)\n\t\treturn -EINVAL;\n\n\tif (size < ETH_HLEN)\n\t\treturn -EINVAL;\n\n\tdata = bpf_test_init(kattr, kattr->test.data_size_in, size, 0, 0);\n\tif (IS_ERR(data))\n\t\treturn PTR_ERR(data);\n\n\teth = (struct ethhdr *)data;\n\n\tif (!repeat)\n\t\trepeat = 1;\n\n\tuser_ctx = bpf_ctx_init(kattr, sizeof(struct bpf_flow_keys));\n\tif (IS_ERR(user_ctx)) {\n\t\tkfree(data);\n\t\treturn PTR_ERR(user_ctx);\n\t}\n\tif (user_ctx) {\n\t\tret = verify_user_bpf_flow_keys(user_ctx);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\tflags = user_ctx->flags;\n\t}\n\n\tctx.flow_keys = &flow_keys;\n\tctx.data = data;\n\tctx.data_end = (__u8 *)data + size;\n\n\tbpf_test_timer_enter(&t);\n\tdo {\n\t\tretval = bpf_flow_dissect(prog, &ctx, eth->h_proto, ETH_HLEN,\n\t\t\t\t\t  size, flags);\n\t} while (bpf_test_timer_continue(&t, 1, repeat, &ret, &duration));\n\tbpf_test_timer_leave(&t);\n\n\tif (ret < 0)\n\t\tgoto out;\n\n\tret = bpf_test_finish(kattr, uattr, &flow_keys, NULL,\n\t\t\t      sizeof(flow_keys), retval, duration);\n\tif (!ret)\n\t\tret = bpf_ctx_finish(kattr, uattr, user_ctx,\n\t\t\t\t     sizeof(struct bpf_flow_keys));\n\nout:\n\tkfree(user_ctx);\n\tkfree(data);\n\treturn ret;\n}\n\nint bpf_prog_test_run_sk_lookup(struct bpf_prog *prog, const union bpf_attr *kattr,\n\t\t\t\tunion bpf_attr __user *uattr)\n{\n\tstruct bpf_test_timer t = { NO_PREEMPT };\n\tstruct bpf_prog_array *progs = NULL;\n\tstruct bpf_sk_lookup_kern ctx = {};\n\tu32 repeat = kattr->test.repeat;\n\tstruct bpf_sk_lookup *user_ctx;\n\tu32 retval, duration;\n\tint ret = -EINVAL;\n\n\tif (kattr->test.flags || kattr->test.cpu || kattr->test.batch_size)\n\t\treturn -EINVAL;\n\n\tif (kattr->test.data_in || kattr->test.data_size_in || kattr->test.data_out ||\n\t    kattr->test.data_size_out)\n\t\treturn -EINVAL;\n\n\tif (!repeat)\n\t\trepeat = 1;\n\n\tuser_ctx = bpf_ctx_init(kattr, sizeof(*user_ctx));\n\tif (IS_ERR(user_ctx))\n\t\treturn PTR_ERR(user_ctx);\n\n\tif (!user_ctx)\n\t\treturn -EINVAL;\n\n\tif (user_ctx->sk)\n\t\tgoto out;\n\n\tif (!range_is_zero(user_ctx, offsetofend(typeof(*user_ctx), local_port), sizeof(*user_ctx)))\n\t\tgoto out;\n\n\tif (user_ctx->local_port > U16_MAX) {\n\t\tret = -ERANGE;\n\t\tgoto out;\n\t}\n\n\tctx.family = (u16)user_ctx->family;\n\tctx.protocol = (u16)user_ctx->protocol;\n\tctx.dport = (u16)user_ctx->local_port;\n\tctx.sport = user_ctx->remote_port;\n\n\tswitch (ctx.family) {\n\tcase AF_INET:\n\t\tctx.v4.daddr = (__force __be32)user_ctx->local_ip4;\n\t\tctx.v4.saddr = (__force __be32)user_ctx->remote_ip4;\n\t\tbreak;\n\n#if IS_ENABLED(CONFIG_IPV6)\n\tcase AF_INET6:\n\t\tctx.v6.daddr = (struct in6_addr *)user_ctx->local_ip6;\n\t\tctx.v6.saddr = (struct in6_addr *)user_ctx->remote_ip6;\n\t\tbreak;\n#endif\n\n\tdefault:\n\t\tret = -EAFNOSUPPORT;\n\t\tgoto out;\n\t}\n\n\tprogs = bpf_prog_array_alloc(1, GFP_KERNEL);\n\tif (!progs) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tprogs->items[0].prog = prog;\n\n\tbpf_test_timer_enter(&t);\n\tdo {\n\t\tctx.selected_sk = NULL;\n\t\tretval = BPF_PROG_SK_LOOKUP_RUN_ARRAY(progs, ctx, bpf_prog_run);\n\t} while (bpf_test_timer_continue(&t, 1, repeat, &ret, &duration));\n\tbpf_test_timer_leave(&t);\n\n\tif (ret < 0)\n\t\tgoto out;\n\n\tuser_ctx->cookie = 0;\n\tif (ctx.selected_sk) {\n\t\tif (ctx.selected_sk->sk_reuseport && !ctx.no_reuseport) {\n\t\t\tret = -EOPNOTSUPP;\n\t\t\tgoto out;\n\t\t}\n\n\t\tuser_ctx->cookie = sock_gen_cookie(ctx.selected_sk);\n\t}\n\n\tret = bpf_test_finish(kattr, uattr, NULL, NULL, 0, retval, duration);\n\tif (!ret)\n\t\tret = bpf_ctx_finish(kattr, uattr, user_ctx, sizeof(*user_ctx));\n\nout:\n\tbpf_prog_array_free(progs);\n\tkfree(user_ctx);\n\treturn ret;\n}\n\nint bpf_prog_test_run_syscall(struct bpf_prog *prog,\n\t\t\t      const union bpf_attr *kattr,\n\t\t\t      union bpf_attr __user *uattr)\n{\n\tvoid __user *ctx_in = u64_to_user_ptr(kattr->test.ctx_in);\n\t__u32 ctx_size_in = kattr->test.ctx_size_in;\n\tvoid *ctx = NULL;\n\tu32 retval;\n\tint err = 0;\n\n\t \n\tif (kattr->test.data_in || kattr->test.data_out ||\n\t    kattr->test.ctx_out || kattr->test.duration ||\n\t    kattr->test.repeat || kattr->test.flags ||\n\t    kattr->test.batch_size)\n\t\treturn -EINVAL;\n\n\tif (ctx_size_in < prog->aux->max_ctx_offset ||\n\t    ctx_size_in > U16_MAX)\n\t\treturn -EINVAL;\n\n\tif (ctx_size_in) {\n\t\tctx = memdup_user(ctx_in, ctx_size_in);\n\t\tif (IS_ERR(ctx))\n\t\t\treturn PTR_ERR(ctx);\n\t}\n\n\trcu_read_lock_trace();\n\tretval = bpf_prog_run_pin_on_cpu(prog, ctx);\n\trcu_read_unlock_trace();\n\n\tif (copy_to_user(&uattr->test.retval, &retval, sizeof(u32))) {\n\t\terr = -EFAULT;\n\t\tgoto out;\n\t}\n\tif (ctx_size_in)\n\t\tif (copy_to_user(ctx_in, ctx, ctx_size_in))\n\t\t\terr = -EFAULT;\nout:\n\tkfree(ctx);\n\treturn err;\n}\n\nstatic int verify_and_copy_hook_state(struct nf_hook_state *state,\n\t\t\t\t      const struct nf_hook_state *user,\n\t\t\t\t      struct net_device *dev)\n{\n\tif (user->in || user->out)\n\t\treturn -EINVAL;\n\n\tif (user->net || user->sk || user->okfn)\n\t\treturn -EINVAL;\n\n\tswitch (user->pf) {\n\tcase NFPROTO_IPV4:\n\tcase NFPROTO_IPV6:\n\t\tswitch (state->hook) {\n\t\tcase NF_INET_PRE_ROUTING:\n\t\t\tstate->in = dev;\n\t\t\tbreak;\n\t\tcase NF_INET_LOCAL_IN:\n\t\t\tstate->in = dev;\n\t\t\tbreak;\n\t\tcase NF_INET_FORWARD:\n\t\t\tstate->in = dev;\n\t\t\tstate->out = dev;\n\t\t\tbreak;\n\t\tcase NF_INET_LOCAL_OUT:\n\t\t\tstate->out = dev;\n\t\t\tbreak;\n\t\tcase NF_INET_POST_ROUTING:\n\t\t\tstate->out = dev;\n\t\t\tbreak;\n\t\t}\n\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tstate->pf = user->pf;\n\tstate->hook = user->hook;\n\n\treturn 0;\n}\n\nstatic __be16 nfproto_eth(int nfproto)\n{\n\tswitch (nfproto) {\n\tcase NFPROTO_IPV4:\n\t\treturn htons(ETH_P_IP);\n\tcase NFPROTO_IPV6:\n\t\tbreak;\n\t}\n\n\treturn htons(ETH_P_IPV6);\n}\n\nint bpf_prog_test_run_nf(struct bpf_prog *prog,\n\t\t\t const union bpf_attr *kattr,\n\t\t\t union bpf_attr __user *uattr)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct net_device *dev = net->loopback_dev;\n\tstruct nf_hook_state *user_ctx, hook_state = {\n\t\t.pf = NFPROTO_IPV4,\n\t\t.hook = NF_INET_LOCAL_OUT,\n\t};\n\tu32 size = kattr->test.data_size_in;\n\tu32 repeat = kattr->test.repeat;\n\tstruct bpf_nf_ctx ctx = {\n\t\t.state = &hook_state,\n\t};\n\tstruct sk_buff *skb = NULL;\n\tu32 retval, duration;\n\tvoid *data;\n\tint ret;\n\n\tif (kattr->test.flags || kattr->test.cpu || kattr->test.batch_size)\n\t\treturn -EINVAL;\n\n\tif (size < sizeof(struct iphdr))\n\t\treturn -EINVAL;\n\n\tdata = bpf_test_init(kattr, kattr->test.data_size_in, size,\n\t\t\t     NET_SKB_PAD + NET_IP_ALIGN,\n\t\t\t     SKB_DATA_ALIGN(sizeof(struct skb_shared_info)));\n\tif (IS_ERR(data))\n\t\treturn PTR_ERR(data);\n\n\tif (!repeat)\n\t\trepeat = 1;\n\n\tuser_ctx = bpf_ctx_init(kattr, sizeof(struct nf_hook_state));\n\tif (IS_ERR(user_ctx)) {\n\t\tkfree(data);\n\t\treturn PTR_ERR(user_ctx);\n\t}\n\n\tif (user_ctx) {\n\t\tret = verify_and_copy_hook_state(&hook_state, user_ctx, dev);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tskb = slab_build_skb(data);\n\tif (!skb) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tdata = NULL;  \n\n\tskb_reserve(skb, NET_SKB_PAD + NET_IP_ALIGN);\n\t__skb_put(skb, size);\n\n\tret = -EINVAL;\n\n\tif (hook_state.hook != NF_INET_LOCAL_OUT) {\n\t\tif (size < ETH_HLEN + sizeof(struct iphdr))\n\t\t\tgoto out;\n\n\t\tskb->protocol = eth_type_trans(skb, dev);\n\t\tswitch (skb->protocol) {\n\t\tcase htons(ETH_P_IP):\n\t\t\tif (hook_state.pf == NFPROTO_IPV4)\n\t\t\t\tbreak;\n\t\t\tgoto out;\n\t\tcase htons(ETH_P_IPV6):\n\t\t\tif (size < ETH_HLEN + sizeof(struct ipv6hdr))\n\t\t\t\tgoto out;\n\t\t\tif (hook_state.pf == NFPROTO_IPV6)\n\t\t\t\tbreak;\n\t\t\tgoto out;\n\t\tdefault:\n\t\t\tret = -EPROTO;\n\t\t\tgoto out;\n\t\t}\n\n\t\tskb_reset_network_header(skb);\n\t} else {\n\t\tskb->protocol = nfproto_eth(hook_state.pf);\n\t}\n\n\tctx.skb = skb;\n\n\tret = bpf_test_run(prog, &ctx, repeat, &retval, &duration, false);\n\tif (ret)\n\t\tgoto out;\n\n\tret = bpf_test_finish(kattr, uattr, NULL, NULL, 0, retval, duration);\n\nout:\n\tkfree(user_ctx);\n\tkfree_skb(skb);\n\tkfree(data);\n\treturn ret;\n}\n\nstatic const struct btf_kfunc_id_set bpf_prog_test_kfunc_set = {\n\t.owner = THIS_MODULE,\n\t.set   = &test_sk_check_kfunc_ids,\n};\n\nBTF_ID_LIST(bpf_prog_test_dtor_kfunc_ids)\nBTF_ID(struct, prog_test_ref_kfunc)\nBTF_ID(func, bpf_kfunc_call_test_release)\nBTF_ID(struct, prog_test_member)\nBTF_ID(func, bpf_kfunc_call_memb_release)\n\nstatic int __init bpf_prog_test_run_init(void)\n{\n\tconst struct btf_id_dtor_kfunc bpf_prog_test_dtor_kfunc[] = {\n\t\t{\n\t\t  .btf_id       = bpf_prog_test_dtor_kfunc_ids[0],\n\t\t  .kfunc_btf_id = bpf_prog_test_dtor_kfunc_ids[1]\n\t\t},\n\t\t{\n\t\t  .btf_id\t= bpf_prog_test_dtor_kfunc_ids[2],\n\t\t  .kfunc_btf_id = bpf_prog_test_dtor_kfunc_ids[3],\n\t\t},\n\t};\n\tint ret;\n\n\tret = register_btf_fmodret_id_set(&bpf_test_modify_return_set);\n\tret = ret ?: register_btf_kfunc_id_set(BPF_PROG_TYPE_SCHED_CLS, &bpf_prog_test_kfunc_set);\n\tret = ret ?: register_btf_kfunc_id_set(BPF_PROG_TYPE_TRACING, &bpf_prog_test_kfunc_set);\n\tret = ret ?: register_btf_kfunc_id_set(BPF_PROG_TYPE_SYSCALL, &bpf_prog_test_kfunc_set);\n\treturn ret ?: register_btf_id_dtor_kfuncs(bpf_prog_test_dtor_kfunc,\n\t\t\t\t\t\t  ARRAY_SIZE(bpf_prog_test_dtor_kfunc),\n\t\t\t\t\t\t  THIS_MODULE);\n}\nlate_initcall(bpf_prog_test_run_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}