{
  "module_name": "bridge_loop_avoidance.c",
  "hash_id": "509c8f587f1de3da1d186454468a2c835703b908cb07c16cf5c239497af57efd",
  "original_prompt": "Ingested from linux-6.6.14/net/batman-adv/bridge_loop_avoidance.c",
  "human_readable_source": "\n \n\n#include \"bridge_loop_avoidance.h\"\n#include \"main.h\"\n\n#include <linux/atomic.h>\n#include <linux/byteorder/generic.h>\n#include <linux/compiler.h>\n#include <linux/container_of.h>\n#include <linux/crc16.h>\n#include <linux/errno.h>\n#include <linux/etherdevice.h>\n#include <linux/gfp.h>\n#include <linux/if_arp.h>\n#include <linux/if_ether.h>\n#include <linux/if_vlan.h>\n#include <linux/jhash.h>\n#include <linux/jiffies.h>\n#include <linux/kernel.h>\n#include <linux/kref.h>\n#include <linux/list.h>\n#include <linux/lockdep.h>\n#include <linux/netdevice.h>\n#include <linux/netlink.h>\n#include <linux/rculist.h>\n#include <linux/rcupdate.h>\n#include <linux/skbuff.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n#include <linux/stddef.h>\n#include <linux/string.h>\n#include <linux/workqueue.h>\n#include <net/arp.h>\n#include <net/genetlink.h>\n#include <net/netlink.h>\n#include <net/sock.h>\n#include <uapi/linux/batadv_packet.h>\n#include <uapi/linux/batman_adv.h>\n\n#include \"hard-interface.h\"\n#include \"hash.h\"\n#include \"log.h\"\n#include \"netlink.h\"\n#include \"originator.h\"\n#include \"soft-interface.h\"\n#include \"translation-table.h\"\n\nstatic const u8 batadv_announce_mac[4] = {0x43, 0x05, 0x43, 0x05};\n\nstatic void batadv_bla_periodic_work(struct work_struct *work);\nstatic void\nbatadv_bla_send_announce(struct batadv_priv *bat_priv,\n\t\t\t struct batadv_bla_backbone_gw *backbone_gw);\n\n \nstatic inline u32 batadv_choose_claim(const void *data, u32 size)\n{\n\tconst struct batadv_bla_claim *claim = data;\n\tu32 hash = 0;\n\n\thash = jhash(&claim->addr, sizeof(claim->addr), hash);\n\thash = jhash(&claim->vid, sizeof(claim->vid), hash);\n\n\treturn hash % size;\n}\n\n \nstatic inline u32 batadv_choose_backbone_gw(const void *data, u32 size)\n{\n\tconst struct batadv_bla_backbone_gw *gw;\n\tu32 hash = 0;\n\n\tgw = data;\n\thash = jhash(&gw->orig, sizeof(gw->orig), hash);\n\thash = jhash(&gw->vid, sizeof(gw->vid), hash);\n\n\treturn hash % size;\n}\n\n \nstatic bool batadv_compare_backbone_gw(const struct hlist_node *node,\n\t\t\t\t       const void *data2)\n{\n\tconst void *data1 = container_of(node, struct batadv_bla_backbone_gw,\n\t\t\t\t\t hash_entry);\n\tconst struct batadv_bla_backbone_gw *gw1 = data1;\n\tconst struct batadv_bla_backbone_gw *gw2 = data2;\n\n\tif (!batadv_compare_eth(gw1->orig, gw2->orig))\n\t\treturn false;\n\n\tif (gw1->vid != gw2->vid)\n\t\treturn false;\n\n\treturn true;\n}\n\n \nstatic bool batadv_compare_claim(const struct hlist_node *node,\n\t\t\t\t const void *data2)\n{\n\tconst void *data1 = container_of(node, struct batadv_bla_claim,\n\t\t\t\t\t hash_entry);\n\tconst struct batadv_bla_claim *cl1 = data1;\n\tconst struct batadv_bla_claim *cl2 = data2;\n\n\tif (!batadv_compare_eth(cl1->addr, cl2->addr))\n\t\treturn false;\n\n\tif (cl1->vid != cl2->vid)\n\t\treturn false;\n\n\treturn true;\n}\n\n \nstatic void batadv_backbone_gw_release(struct kref *ref)\n{\n\tstruct batadv_bla_backbone_gw *backbone_gw;\n\n\tbackbone_gw = container_of(ref, struct batadv_bla_backbone_gw,\n\t\t\t\t   refcount);\n\n\tkfree_rcu(backbone_gw, rcu);\n}\n\n \nstatic void batadv_backbone_gw_put(struct batadv_bla_backbone_gw *backbone_gw)\n{\n\tif (!backbone_gw)\n\t\treturn;\n\n\tkref_put(&backbone_gw->refcount, batadv_backbone_gw_release);\n}\n\n \nstatic void batadv_claim_release(struct kref *ref)\n{\n\tstruct batadv_bla_claim *claim;\n\tstruct batadv_bla_backbone_gw *old_backbone_gw;\n\n\tclaim = container_of(ref, struct batadv_bla_claim, refcount);\n\n\tspin_lock_bh(&claim->backbone_lock);\n\told_backbone_gw = claim->backbone_gw;\n\tclaim->backbone_gw = NULL;\n\tspin_unlock_bh(&claim->backbone_lock);\n\n\tspin_lock_bh(&old_backbone_gw->crc_lock);\n\told_backbone_gw->crc ^= crc16(0, claim->addr, ETH_ALEN);\n\tspin_unlock_bh(&old_backbone_gw->crc_lock);\n\n\tbatadv_backbone_gw_put(old_backbone_gw);\n\n\tkfree_rcu(claim, rcu);\n}\n\n \nstatic void batadv_claim_put(struct batadv_bla_claim *claim)\n{\n\tif (!claim)\n\t\treturn;\n\n\tkref_put(&claim->refcount, batadv_claim_release);\n}\n\n \nstatic struct batadv_bla_claim *\nbatadv_claim_hash_find(struct batadv_priv *bat_priv,\n\t\t       struct batadv_bla_claim *data)\n{\n\tstruct batadv_hashtable *hash = bat_priv->bla.claim_hash;\n\tstruct hlist_head *head;\n\tstruct batadv_bla_claim *claim;\n\tstruct batadv_bla_claim *claim_tmp = NULL;\n\tint index;\n\n\tif (!hash)\n\t\treturn NULL;\n\n\tindex = batadv_choose_claim(data, hash->size);\n\thead = &hash->table[index];\n\n\trcu_read_lock();\n\thlist_for_each_entry_rcu(claim, head, hash_entry) {\n\t\tif (!batadv_compare_claim(&claim->hash_entry, data))\n\t\t\tcontinue;\n\n\t\tif (!kref_get_unless_zero(&claim->refcount))\n\t\t\tcontinue;\n\n\t\tclaim_tmp = claim;\n\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\treturn claim_tmp;\n}\n\n \nstatic struct batadv_bla_backbone_gw *\nbatadv_backbone_hash_find(struct batadv_priv *bat_priv, const u8 *addr,\n\t\t\t  unsigned short vid)\n{\n\tstruct batadv_hashtable *hash = bat_priv->bla.backbone_hash;\n\tstruct hlist_head *head;\n\tstruct batadv_bla_backbone_gw search_entry, *backbone_gw;\n\tstruct batadv_bla_backbone_gw *backbone_gw_tmp = NULL;\n\tint index;\n\n\tif (!hash)\n\t\treturn NULL;\n\n\tether_addr_copy(search_entry.orig, addr);\n\tsearch_entry.vid = vid;\n\n\tindex = batadv_choose_backbone_gw(&search_entry, hash->size);\n\thead = &hash->table[index];\n\n\trcu_read_lock();\n\thlist_for_each_entry_rcu(backbone_gw, head, hash_entry) {\n\t\tif (!batadv_compare_backbone_gw(&backbone_gw->hash_entry,\n\t\t\t\t\t\t&search_entry))\n\t\t\tcontinue;\n\n\t\tif (!kref_get_unless_zero(&backbone_gw->refcount))\n\t\t\tcontinue;\n\n\t\tbackbone_gw_tmp = backbone_gw;\n\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\treturn backbone_gw_tmp;\n}\n\n \nstatic void\nbatadv_bla_del_backbone_claims(struct batadv_bla_backbone_gw *backbone_gw)\n{\n\tstruct batadv_hashtable *hash;\n\tstruct hlist_node *node_tmp;\n\tstruct hlist_head *head;\n\tstruct batadv_bla_claim *claim;\n\tint i;\n\tspinlock_t *list_lock;\t \n\n\thash = backbone_gw->bat_priv->bla.claim_hash;\n\tif (!hash)\n\t\treturn;\n\n\tfor (i = 0; i < hash->size; i++) {\n\t\thead = &hash->table[i];\n\t\tlist_lock = &hash->list_locks[i];\n\n\t\tspin_lock_bh(list_lock);\n\t\thlist_for_each_entry_safe(claim, node_tmp,\n\t\t\t\t\t  head, hash_entry) {\n\t\t\tif (claim->backbone_gw != backbone_gw)\n\t\t\t\tcontinue;\n\n\t\t\tbatadv_claim_put(claim);\n\t\t\thlist_del_rcu(&claim->hash_entry);\n\t\t}\n\t\tspin_unlock_bh(list_lock);\n\t}\n\n\t \n\tspin_lock_bh(&backbone_gw->crc_lock);\n\tbackbone_gw->crc = BATADV_BLA_CRC_INIT;\n\tspin_unlock_bh(&backbone_gw->crc_lock);\n}\n\n \nstatic void batadv_bla_send_claim(struct batadv_priv *bat_priv, const u8 *mac,\n\t\t\t\t  unsigned short vid, int claimtype)\n{\n\tstruct sk_buff *skb;\n\tstruct ethhdr *ethhdr;\n\tstruct batadv_hard_iface *primary_if;\n\tstruct net_device *soft_iface;\n\tu8 *hw_src;\n\tstruct batadv_bla_claim_dst local_claim_dest;\n\t__be32 zeroip = 0;\n\n\tprimary_if = batadv_primary_if_get_selected(bat_priv);\n\tif (!primary_if)\n\t\treturn;\n\n\tmemcpy(&local_claim_dest, &bat_priv->bla.claim_dest,\n\t       sizeof(local_claim_dest));\n\tlocal_claim_dest.type = claimtype;\n\n\tsoft_iface = primary_if->soft_iface;\n\n\tskb = arp_create(ARPOP_REPLY, ETH_P_ARP,\n\t\t\t  \n\t\t\t zeroip,\n\t\t\t primary_if->soft_iface,\n\t\t\t  \n\t\t\t zeroip,\n\t\t\t  \n\t\t\t NULL,\n\t\t\t  \n\t\t\t primary_if->net_dev->dev_addr,\n\t\t\t  \n\t\t\t (u8 *)&local_claim_dest);\n\n\tif (!skb)\n\t\tgoto out;\n\n\tethhdr = (struct ethhdr *)skb->data;\n\thw_src = (u8 *)ethhdr + ETH_HLEN + sizeof(struct arphdr);\n\n\t \n\tswitch (claimtype) {\n\tcase BATADV_CLAIM_TYPE_CLAIM:\n\t\t \n\t\tether_addr_copy(ethhdr->h_source, mac);\n\t\tbatadv_dbg(BATADV_DBG_BLA, bat_priv,\n\t\t\t   \"%s(): CLAIM %pM on vid %d\\n\", __func__, mac,\n\t\t\t   batadv_print_vid(vid));\n\t\tbreak;\n\tcase BATADV_CLAIM_TYPE_UNCLAIM:\n\t\t \n\t\tether_addr_copy(hw_src, mac);\n\t\tbatadv_dbg(BATADV_DBG_BLA, bat_priv,\n\t\t\t   \"%s(): UNCLAIM %pM on vid %d\\n\", __func__, mac,\n\t\t\t   batadv_print_vid(vid));\n\t\tbreak;\n\tcase BATADV_CLAIM_TYPE_ANNOUNCE:\n\t\t \n\t\tether_addr_copy(hw_src, mac);\n\t\tbatadv_dbg(BATADV_DBG_BLA, bat_priv,\n\t\t\t   \"%s(): ANNOUNCE of %pM on vid %d\\n\", __func__,\n\t\t\t   ethhdr->h_source, batadv_print_vid(vid));\n\t\tbreak;\n\tcase BATADV_CLAIM_TYPE_REQUEST:\n\t\t \n\t\tether_addr_copy(hw_src, mac);\n\t\tether_addr_copy(ethhdr->h_dest, mac);\n\t\tbatadv_dbg(BATADV_DBG_BLA, bat_priv,\n\t\t\t   \"%s(): REQUEST of %pM to %pM on vid %d\\n\", __func__,\n\t\t\t   ethhdr->h_source, ethhdr->h_dest,\n\t\t\t   batadv_print_vid(vid));\n\t\tbreak;\n\tcase BATADV_CLAIM_TYPE_LOOPDETECT:\n\t\tether_addr_copy(ethhdr->h_source, mac);\n\t\tbatadv_dbg(BATADV_DBG_BLA, bat_priv,\n\t\t\t   \"%s(): LOOPDETECT of %pM to %pM on vid %d\\n\",\n\t\t\t   __func__, ethhdr->h_source, ethhdr->h_dest,\n\t\t\t   batadv_print_vid(vid));\n\n\t\tbreak;\n\t}\n\n\tif (vid & BATADV_VLAN_HAS_TAG) {\n\t\tskb = vlan_insert_tag(skb, htons(ETH_P_8021Q),\n\t\t\t\t      vid & VLAN_VID_MASK);\n\t\tif (!skb)\n\t\t\tgoto out;\n\t}\n\n\tskb_reset_mac_header(skb);\n\tskb->protocol = eth_type_trans(skb, soft_iface);\n\tbatadv_inc_counter(bat_priv, BATADV_CNT_RX);\n\tbatadv_add_counter(bat_priv, BATADV_CNT_RX_BYTES,\n\t\t\t   skb->len + ETH_HLEN);\n\n\tnetif_rx(skb);\nout:\n\tbatadv_hardif_put(primary_if);\n}\n\n \nstatic void batadv_bla_loopdetect_report(struct work_struct *work)\n{\n\tstruct batadv_bla_backbone_gw *backbone_gw;\n\tstruct batadv_priv *bat_priv;\n\tchar vid_str[6] = { '\\0' };\n\n\tbackbone_gw = container_of(work, struct batadv_bla_backbone_gw,\n\t\t\t\t   report_work);\n\tbat_priv = backbone_gw->bat_priv;\n\n\tbatadv_info(bat_priv->soft_iface,\n\t\t    \"Possible loop on VLAN %d detected which can't be handled by BLA - please check your network setup!\\n\",\n\t\t    batadv_print_vid(backbone_gw->vid));\n\tsnprintf(vid_str, sizeof(vid_str), \"%d\",\n\t\t batadv_print_vid(backbone_gw->vid));\n\tvid_str[sizeof(vid_str) - 1] = 0;\n\n\tbatadv_throw_uevent(bat_priv, BATADV_UEV_BLA, BATADV_UEV_LOOPDETECT,\n\t\t\t    vid_str);\n\n\tbatadv_backbone_gw_put(backbone_gw);\n}\n\n \nstatic struct batadv_bla_backbone_gw *\nbatadv_bla_get_backbone_gw(struct batadv_priv *bat_priv, const u8 *orig,\n\t\t\t   unsigned short vid, bool own_backbone)\n{\n\tstruct batadv_bla_backbone_gw *entry;\n\tstruct batadv_orig_node *orig_node;\n\tint hash_added;\n\n\tentry = batadv_backbone_hash_find(bat_priv, orig, vid);\n\n\tif (entry)\n\t\treturn entry;\n\n\tbatadv_dbg(BATADV_DBG_BLA, bat_priv,\n\t\t   \"%s(): not found (%pM, %d), creating new entry\\n\", __func__,\n\t\t   orig, batadv_print_vid(vid));\n\n\tentry = kzalloc(sizeof(*entry), GFP_ATOMIC);\n\tif (!entry)\n\t\treturn NULL;\n\n\tentry->vid = vid;\n\tentry->lasttime = jiffies;\n\tentry->crc = BATADV_BLA_CRC_INIT;\n\tentry->bat_priv = bat_priv;\n\tspin_lock_init(&entry->crc_lock);\n\tatomic_set(&entry->request_sent, 0);\n\tatomic_set(&entry->wait_periods, 0);\n\tether_addr_copy(entry->orig, orig);\n\tINIT_WORK(&entry->report_work, batadv_bla_loopdetect_report);\n\tkref_init(&entry->refcount);\n\n\tkref_get(&entry->refcount);\n\thash_added = batadv_hash_add(bat_priv->bla.backbone_hash,\n\t\t\t\t     batadv_compare_backbone_gw,\n\t\t\t\t     batadv_choose_backbone_gw, entry,\n\t\t\t\t     &entry->hash_entry);\n\n\tif (unlikely(hash_added != 0)) {\n\t\t \n\t\tkfree(entry);\n\t\treturn NULL;\n\t}\n\n\t \n\torig_node = batadv_orig_hash_find(bat_priv, orig);\n\tif (orig_node) {\n\t\tbatadv_tt_global_del_orig(bat_priv, orig_node, vid,\n\t\t\t\t\t  \"became a backbone gateway\");\n\t\tbatadv_orig_node_put(orig_node);\n\t}\n\n\tif (own_backbone) {\n\t\tbatadv_bla_send_announce(bat_priv, entry);\n\n\t\t \n\t\tatomic_inc(&entry->request_sent);\n\t\tatomic_set(&entry->wait_periods, BATADV_BLA_WAIT_PERIODS);\n\t\tatomic_inc(&bat_priv->bla.num_requests);\n\t}\n\n\treturn entry;\n}\n\n \nstatic void\nbatadv_bla_update_own_backbone_gw(struct batadv_priv *bat_priv,\n\t\t\t\t  struct batadv_hard_iface *primary_if,\n\t\t\t\t  unsigned short vid)\n{\n\tstruct batadv_bla_backbone_gw *backbone_gw;\n\n\tbackbone_gw = batadv_bla_get_backbone_gw(bat_priv,\n\t\t\t\t\t\t primary_if->net_dev->dev_addr,\n\t\t\t\t\t\t vid, true);\n\tif (unlikely(!backbone_gw))\n\t\treturn;\n\n\tbackbone_gw->lasttime = jiffies;\n\tbatadv_backbone_gw_put(backbone_gw);\n}\n\n \nstatic void batadv_bla_answer_request(struct batadv_priv *bat_priv,\n\t\t\t\t      struct batadv_hard_iface *primary_if,\n\t\t\t\t      unsigned short vid)\n{\n\tstruct hlist_head *head;\n\tstruct batadv_hashtable *hash;\n\tstruct batadv_bla_claim *claim;\n\tstruct batadv_bla_backbone_gw *backbone_gw;\n\tint i;\n\n\tbatadv_dbg(BATADV_DBG_BLA, bat_priv,\n\t\t   \"%s(): received a claim request, send all of our own claims again\\n\",\n\t\t   __func__);\n\n\tbackbone_gw = batadv_backbone_hash_find(bat_priv,\n\t\t\t\t\t\tprimary_if->net_dev->dev_addr,\n\t\t\t\t\t\tvid);\n\tif (!backbone_gw)\n\t\treturn;\n\n\thash = bat_priv->bla.claim_hash;\n\tfor (i = 0; i < hash->size; i++) {\n\t\thead = &hash->table[i];\n\n\t\trcu_read_lock();\n\t\thlist_for_each_entry_rcu(claim, head, hash_entry) {\n\t\t\t \n\t\t\tif (claim->backbone_gw != backbone_gw)\n\t\t\t\tcontinue;\n\n\t\t\tbatadv_bla_send_claim(bat_priv, claim->addr, claim->vid,\n\t\t\t\t\t      BATADV_CLAIM_TYPE_CLAIM);\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\n\t \n\tbatadv_bla_send_announce(bat_priv, backbone_gw);\n\tbatadv_backbone_gw_put(backbone_gw);\n}\n\n \nstatic void batadv_bla_send_request(struct batadv_bla_backbone_gw *backbone_gw)\n{\n\t \n\tbatadv_bla_del_backbone_claims(backbone_gw);\n\n\tbatadv_dbg(BATADV_DBG_BLA, backbone_gw->bat_priv,\n\t\t   \"Sending REQUEST to %pM\\n\", backbone_gw->orig);\n\n\t \n\tbatadv_bla_send_claim(backbone_gw->bat_priv, backbone_gw->orig,\n\t\t\t      backbone_gw->vid, BATADV_CLAIM_TYPE_REQUEST);\n\n\t \n\tif (!atomic_read(&backbone_gw->request_sent)) {\n\t\tatomic_inc(&backbone_gw->bat_priv->bla.num_requests);\n\t\tatomic_set(&backbone_gw->request_sent, 1);\n\t}\n}\n\n \nstatic void batadv_bla_send_announce(struct batadv_priv *bat_priv,\n\t\t\t\t     struct batadv_bla_backbone_gw *backbone_gw)\n{\n\tu8 mac[ETH_ALEN];\n\t__be16 crc;\n\n\tmemcpy(mac, batadv_announce_mac, 4);\n\tspin_lock_bh(&backbone_gw->crc_lock);\n\tcrc = htons(backbone_gw->crc);\n\tspin_unlock_bh(&backbone_gw->crc_lock);\n\tmemcpy(&mac[4], &crc, 2);\n\n\tbatadv_bla_send_claim(bat_priv, mac, backbone_gw->vid,\n\t\t\t      BATADV_CLAIM_TYPE_ANNOUNCE);\n}\n\n \nstatic void batadv_bla_add_claim(struct batadv_priv *bat_priv,\n\t\t\t\t const u8 *mac, const unsigned short vid,\n\t\t\t\t struct batadv_bla_backbone_gw *backbone_gw)\n{\n\tstruct batadv_bla_backbone_gw *old_backbone_gw;\n\tstruct batadv_bla_claim *claim;\n\tstruct batadv_bla_claim search_claim;\n\tbool remove_crc = false;\n\tint hash_added;\n\n\tether_addr_copy(search_claim.addr, mac);\n\tsearch_claim.vid = vid;\n\tclaim = batadv_claim_hash_find(bat_priv, &search_claim);\n\n\t \n\tif (!claim) {\n\t\tclaim = kzalloc(sizeof(*claim), GFP_ATOMIC);\n\t\tif (!claim)\n\t\t\treturn;\n\n\t\tether_addr_copy(claim->addr, mac);\n\t\tspin_lock_init(&claim->backbone_lock);\n\t\tclaim->vid = vid;\n\t\tclaim->lasttime = jiffies;\n\t\tkref_get(&backbone_gw->refcount);\n\t\tclaim->backbone_gw = backbone_gw;\n\t\tkref_init(&claim->refcount);\n\n\t\tbatadv_dbg(BATADV_DBG_BLA, bat_priv,\n\t\t\t   \"%s(): adding new entry %pM, vid %d to hash ...\\n\",\n\t\t\t   __func__, mac, batadv_print_vid(vid));\n\n\t\tkref_get(&claim->refcount);\n\t\thash_added = batadv_hash_add(bat_priv->bla.claim_hash,\n\t\t\t\t\t     batadv_compare_claim,\n\t\t\t\t\t     batadv_choose_claim, claim,\n\t\t\t\t\t     &claim->hash_entry);\n\n\t\tif (unlikely(hash_added != 0)) {\n\t\t\t \n\t\t\tkfree(claim);\n\t\t\treturn;\n\t\t}\n\t} else {\n\t\tclaim->lasttime = jiffies;\n\t\tif (claim->backbone_gw == backbone_gw)\n\t\t\t \n\t\t\tgoto claim_free_ref;\n\n\t\tbatadv_dbg(BATADV_DBG_BLA, bat_priv,\n\t\t\t   \"%s(): changing ownership for %pM, vid %d to gw %pM\\n\",\n\t\t\t   __func__, mac, batadv_print_vid(vid),\n\t\t\t   backbone_gw->orig);\n\n\t\tremove_crc = true;\n\t}\n\n\t \n\tspin_lock_bh(&claim->backbone_lock);\n\told_backbone_gw = claim->backbone_gw;\n\tkref_get(&backbone_gw->refcount);\n\tclaim->backbone_gw = backbone_gw;\n\tspin_unlock_bh(&claim->backbone_lock);\n\n\tif (remove_crc) {\n\t\t \n\t\tspin_lock_bh(&old_backbone_gw->crc_lock);\n\t\told_backbone_gw->crc ^= crc16(0, claim->addr, ETH_ALEN);\n\t\tspin_unlock_bh(&old_backbone_gw->crc_lock);\n\t}\n\n\tbatadv_backbone_gw_put(old_backbone_gw);\n\n\t \n\tspin_lock_bh(&backbone_gw->crc_lock);\n\tbackbone_gw->crc ^= crc16(0, claim->addr, ETH_ALEN);\n\tspin_unlock_bh(&backbone_gw->crc_lock);\n\tbackbone_gw->lasttime = jiffies;\n\nclaim_free_ref:\n\tbatadv_claim_put(claim);\n}\n\n \nstatic struct batadv_bla_backbone_gw *\nbatadv_bla_claim_get_backbone_gw(struct batadv_bla_claim *claim)\n{\n\tstruct batadv_bla_backbone_gw *backbone_gw;\n\n\tspin_lock_bh(&claim->backbone_lock);\n\tbackbone_gw = claim->backbone_gw;\n\tkref_get(&backbone_gw->refcount);\n\tspin_unlock_bh(&claim->backbone_lock);\n\n\treturn backbone_gw;\n}\n\n \nstatic void batadv_bla_del_claim(struct batadv_priv *bat_priv,\n\t\t\t\t const u8 *mac, const unsigned short vid)\n{\n\tstruct batadv_bla_claim search_claim, *claim;\n\tstruct batadv_bla_claim *claim_removed_entry;\n\tstruct hlist_node *claim_removed_node;\n\n\tether_addr_copy(search_claim.addr, mac);\n\tsearch_claim.vid = vid;\n\tclaim = batadv_claim_hash_find(bat_priv, &search_claim);\n\tif (!claim)\n\t\treturn;\n\n\tbatadv_dbg(BATADV_DBG_BLA, bat_priv, \"%s(): %pM, vid %d\\n\", __func__,\n\t\t   mac, batadv_print_vid(vid));\n\n\tclaim_removed_node = batadv_hash_remove(bat_priv->bla.claim_hash,\n\t\t\t\t\t\tbatadv_compare_claim,\n\t\t\t\t\t\tbatadv_choose_claim, claim);\n\tif (!claim_removed_node)\n\t\tgoto free_claim;\n\n\t \n\tclaim_removed_entry = hlist_entry(claim_removed_node,\n\t\t\t\t\t  struct batadv_bla_claim, hash_entry);\n\tbatadv_claim_put(claim_removed_entry);\n\nfree_claim:\n\t \n\tbatadv_claim_put(claim);\n}\n\n \nstatic bool batadv_handle_announce(struct batadv_priv *bat_priv, u8 *an_addr,\n\t\t\t\t   u8 *backbone_addr, unsigned short vid)\n{\n\tstruct batadv_bla_backbone_gw *backbone_gw;\n\tu16 backbone_crc, crc;\n\n\tif (memcmp(an_addr, batadv_announce_mac, 4) != 0)\n\t\treturn false;\n\n\tbackbone_gw = batadv_bla_get_backbone_gw(bat_priv, backbone_addr, vid,\n\t\t\t\t\t\t false);\n\n\tif (unlikely(!backbone_gw))\n\t\treturn true;\n\n\t \n\tbackbone_gw->lasttime = jiffies;\n\tcrc = ntohs(*((__force __be16 *)(&an_addr[4])));\n\n\tbatadv_dbg(BATADV_DBG_BLA, bat_priv,\n\t\t   \"%s(): ANNOUNCE vid %d (sent by %pM)... CRC = %#.4x\\n\",\n\t\t   __func__, batadv_print_vid(vid), backbone_gw->orig, crc);\n\n\tspin_lock_bh(&backbone_gw->crc_lock);\n\tbackbone_crc = backbone_gw->crc;\n\tspin_unlock_bh(&backbone_gw->crc_lock);\n\n\tif (backbone_crc != crc) {\n\t\tbatadv_dbg(BATADV_DBG_BLA, backbone_gw->bat_priv,\n\t\t\t   \"%s(): CRC FAILED for %pM/%d (my = %#.4x, sent = %#.4x)\\n\",\n\t\t\t   __func__, backbone_gw->orig,\n\t\t\t   batadv_print_vid(backbone_gw->vid),\n\t\t\t   backbone_crc, crc);\n\n\t\tbatadv_bla_send_request(backbone_gw);\n\t} else {\n\t\t \n\t\tif (atomic_read(&backbone_gw->request_sent)) {\n\t\t\tatomic_dec(&backbone_gw->bat_priv->bla.num_requests);\n\t\t\tatomic_set(&backbone_gw->request_sent, 0);\n\t\t}\n\t}\n\n\tbatadv_backbone_gw_put(backbone_gw);\n\treturn true;\n}\n\n \nstatic bool batadv_handle_request(struct batadv_priv *bat_priv,\n\t\t\t\t  struct batadv_hard_iface *primary_if,\n\t\t\t\t  u8 *backbone_addr, struct ethhdr *ethhdr,\n\t\t\t\t  unsigned short vid)\n{\n\t \n\tif (!batadv_compare_eth(backbone_addr, ethhdr->h_dest))\n\t\treturn false;\n\n\t \n\tif (!batadv_compare_eth(ethhdr->h_dest, primary_if->net_dev->dev_addr))\n\t\treturn true;\n\n\tbatadv_dbg(BATADV_DBG_BLA, bat_priv,\n\t\t   \"%s(): REQUEST vid %d (sent by %pM)...\\n\",\n\t\t   __func__, batadv_print_vid(vid), ethhdr->h_source);\n\n\tbatadv_bla_answer_request(bat_priv, primary_if, vid);\n\treturn true;\n}\n\n \nstatic bool batadv_handle_unclaim(struct batadv_priv *bat_priv,\n\t\t\t\t  struct batadv_hard_iface *primary_if,\n\t\t\t\t  const u8 *backbone_addr, const u8 *claim_addr,\n\t\t\t\t  unsigned short vid)\n{\n\tstruct batadv_bla_backbone_gw *backbone_gw;\n\n\t \n\tif (primary_if && batadv_compare_eth(backbone_addr,\n\t\t\t\t\t     primary_if->net_dev->dev_addr))\n\t\tbatadv_bla_send_claim(bat_priv, claim_addr, vid,\n\t\t\t\t      BATADV_CLAIM_TYPE_UNCLAIM);\n\n\tbackbone_gw = batadv_backbone_hash_find(bat_priv, backbone_addr, vid);\n\n\tif (!backbone_gw)\n\t\treturn true;\n\n\t \n\tbatadv_dbg(BATADV_DBG_BLA, bat_priv,\n\t\t   \"%s(): UNCLAIM %pM on vid %d (sent by %pM)...\\n\", __func__,\n\t\t   claim_addr, batadv_print_vid(vid), backbone_gw->orig);\n\n\tbatadv_bla_del_claim(bat_priv, claim_addr, vid);\n\tbatadv_backbone_gw_put(backbone_gw);\n\treturn true;\n}\n\n \nstatic bool batadv_handle_claim(struct batadv_priv *bat_priv,\n\t\t\t\tstruct batadv_hard_iface *primary_if,\n\t\t\t\tconst u8 *backbone_addr, const u8 *claim_addr,\n\t\t\t\tunsigned short vid)\n{\n\tstruct batadv_bla_backbone_gw *backbone_gw;\n\n\t \n\n\tbackbone_gw = batadv_bla_get_backbone_gw(bat_priv, backbone_addr, vid,\n\t\t\t\t\t\t false);\n\n\tif (unlikely(!backbone_gw))\n\t\treturn true;\n\n\t \n\tbatadv_bla_add_claim(bat_priv, claim_addr, vid, backbone_gw);\n\tif (batadv_compare_eth(backbone_addr, primary_if->net_dev->dev_addr))\n\t\tbatadv_bla_send_claim(bat_priv, claim_addr, vid,\n\t\t\t\t      BATADV_CLAIM_TYPE_CLAIM);\n\n\t \n\n\tbatadv_backbone_gw_put(backbone_gw);\n\treturn true;\n}\n\n \nstatic int batadv_check_claim_group(struct batadv_priv *bat_priv,\n\t\t\t\t    struct batadv_hard_iface *primary_if,\n\t\t\t\t    u8 *hw_src, u8 *hw_dst,\n\t\t\t\t    struct ethhdr *ethhdr)\n{\n\tu8 *backbone_addr;\n\tstruct batadv_orig_node *orig_node;\n\tstruct batadv_bla_claim_dst *bla_dst, *bla_dst_own;\n\n\tbla_dst = (struct batadv_bla_claim_dst *)hw_dst;\n\tbla_dst_own = &bat_priv->bla.claim_dest;\n\n\t \n\tswitch (bla_dst->type) {\n\tcase BATADV_CLAIM_TYPE_CLAIM:\n\t\tbackbone_addr = hw_src;\n\t\tbreak;\n\tcase BATADV_CLAIM_TYPE_REQUEST:\n\tcase BATADV_CLAIM_TYPE_ANNOUNCE:\n\tcase BATADV_CLAIM_TYPE_UNCLAIM:\n\t\tbackbone_addr = ethhdr->h_source;\n\t\tbreak;\n\tdefault:\n\t\treturn 0;\n\t}\n\n\t \n\tif (batadv_compare_eth(backbone_addr, primary_if->net_dev->dev_addr))\n\t\treturn 0;\n\n\t \n\tif (bla_dst->group == bla_dst_own->group)\n\t\treturn 2;\n\n\t \n\torig_node = batadv_orig_hash_find(bat_priv, backbone_addr);\n\n\t \n\tif (!orig_node)\n\t\treturn 1;\n\n\t \n\tif (ntohs(bla_dst->group) > ntohs(bla_dst_own->group)) {\n\t\tbatadv_dbg(BATADV_DBG_BLA, bat_priv,\n\t\t\t   \"taking other backbones claim group: %#.4x\\n\",\n\t\t\t   ntohs(bla_dst->group));\n\t\tbla_dst_own->group = bla_dst->group;\n\t}\n\n\tbatadv_orig_node_put(orig_node);\n\n\treturn 2;\n}\n\n \nstatic bool batadv_bla_process_claim(struct batadv_priv *bat_priv,\n\t\t\t\t     struct batadv_hard_iface *primary_if,\n\t\t\t\t     struct sk_buff *skb)\n{\n\tstruct batadv_bla_claim_dst *bla_dst, *bla_dst_own;\n\tu8 *hw_src, *hw_dst;\n\tstruct vlan_hdr *vhdr, vhdr_buf;\n\tstruct ethhdr *ethhdr;\n\tstruct arphdr *arphdr;\n\tunsigned short vid;\n\tint vlan_depth = 0;\n\t__be16 proto;\n\tint headlen;\n\tint ret;\n\n\tvid = batadv_get_vid(skb, 0);\n\tethhdr = eth_hdr(skb);\n\n\tproto = ethhdr->h_proto;\n\theadlen = ETH_HLEN;\n\tif (vid & BATADV_VLAN_HAS_TAG) {\n\t\t \n\t\tdo {\n\t\t\tvhdr = skb_header_pointer(skb, headlen, VLAN_HLEN,\n\t\t\t\t\t\t  &vhdr_buf);\n\t\t\tif (!vhdr)\n\t\t\t\treturn false;\n\n\t\t\tproto = vhdr->h_vlan_encapsulated_proto;\n\t\t\theadlen += VLAN_HLEN;\n\t\t\tvlan_depth++;\n\t\t} while (proto == htons(ETH_P_8021Q));\n\t}\n\n\tif (proto != htons(ETH_P_ARP))\n\t\treturn false;  \n\n\t \n\n\tif (unlikely(!pskb_may_pull(skb, headlen + arp_hdr_len(skb->dev))))\n\t\treturn false;\n\n\t \n\tethhdr = eth_hdr(skb);\n\tarphdr = (struct arphdr *)((u8 *)ethhdr + headlen);\n\n\t \n\tif (arphdr->ar_hrd != htons(ARPHRD_ETHER))\n\t\treturn false;\n\tif (arphdr->ar_pro != htons(ETH_P_IP))\n\t\treturn false;\n\tif (arphdr->ar_hln != ETH_ALEN)\n\t\treturn false;\n\tif (arphdr->ar_pln != 4)\n\t\treturn false;\n\n\thw_src = (u8 *)arphdr + sizeof(struct arphdr);\n\thw_dst = hw_src + ETH_ALEN + 4;\n\tbla_dst = (struct batadv_bla_claim_dst *)hw_dst;\n\tbla_dst_own = &bat_priv->bla.claim_dest;\n\n\t \n\tif (memcmp(bla_dst->magic, bla_dst_own->magic,\n\t\t   sizeof(bla_dst->magic)) != 0)\n\t\treturn false;\n\n\t \n\tif (vlan_depth > 1)\n\t\treturn true;\n\n\t \n\tif (bla_dst->type == BATADV_CLAIM_TYPE_LOOPDETECT)\n\t\treturn false;\n\n\t \n\tret = batadv_check_claim_group(bat_priv, primary_if, hw_src, hw_dst,\n\t\t\t\t       ethhdr);\n\tif (ret == 1)\n\t\tbatadv_dbg(BATADV_DBG_BLA, bat_priv,\n\t\t\t   \"%s(): received a claim frame from another group. From: %pM on vid %d ...(hw_src %pM, hw_dst %pM)\\n\",\n\t\t\t   __func__, ethhdr->h_source, batadv_print_vid(vid),\n\t\t\t   hw_src, hw_dst);\n\n\tif (ret < 2)\n\t\treturn !!ret;\n\n\t \n\tbatadv_bla_update_own_backbone_gw(bat_priv, primary_if, vid);\n\n\t \n\tswitch (bla_dst->type) {\n\tcase BATADV_CLAIM_TYPE_CLAIM:\n\t\tif (batadv_handle_claim(bat_priv, primary_if, hw_src,\n\t\t\t\t\tethhdr->h_source, vid))\n\t\t\treturn true;\n\t\tbreak;\n\tcase BATADV_CLAIM_TYPE_UNCLAIM:\n\t\tif (batadv_handle_unclaim(bat_priv, primary_if,\n\t\t\t\t\t  ethhdr->h_source, hw_src, vid))\n\t\t\treturn true;\n\t\tbreak;\n\n\tcase BATADV_CLAIM_TYPE_ANNOUNCE:\n\t\tif (batadv_handle_announce(bat_priv, hw_src, ethhdr->h_source,\n\t\t\t\t\t   vid))\n\t\t\treturn true;\n\t\tbreak;\n\tcase BATADV_CLAIM_TYPE_REQUEST:\n\t\tif (batadv_handle_request(bat_priv, primary_if, hw_src, ethhdr,\n\t\t\t\t\t  vid))\n\t\t\treturn true;\n\t\tbreak;\n\t}\n\n\tbatadv_dbg(BATADV_DBG_BLA, bat_priv,\n\t\t   \"%s(): ERROR - this looks like a claim frame, but is useless. eth src %pM on vid %d ...(hw_src %pM, hw_dst %pM)\\n\",\n\t\t   __func__, ethhdr->h_source, batadv_print_vid(vid), hw_src,\n\t\t   hw_dst);\n\treturn true;\n}\n\n \nstatic void batadv_bla_purge_backbone_gw(struct batadv_priv *bat_priv, int now)\n{\n\tstruct batadv_bla_backbone_gw *backbone_gw;\n\tstruct hlist_node *node_tmp;\n\tstruct hlist_head *head;\n\tstruct batadv_hashtable *hash;\n\tspinlock_t *list_lock;\t \n\tint i;\n\n\thash = bat_priv->bla.backbone_hash;\n\tif (!hash)\n\t\treturn;\n\n\tfor (i = 0; i < hash->size; i++) {\n\t\thead = &hash->table[i];\n\t\tlist_lock = &hash->list_locks[i];\n\n\t\tspin_lock_bh(list_lock);\n\t\thlist_for_each_entry_safe(backbone_gw, node_tmp,\n\t\t\t\t\t  head, hash_entry) {\n\t\t\tif (now)\n\t\t\t\tgoto purge_now;\n\t\t\tif (!batadv_has_timed_out(backbone_gw->lasttime,\n\t\t\t\t\t\t  BATADV_BLA_BACKBONE_TIMEOUT))\n\t\t\t\tcontinue;\n\n\t\t\tbatadv_dbg(BATADV_DBG_BLA, backbone_gw->bat_priv,\n\t\t\t\t   \"%s(): backbone gw %pM timed out\\n\",\n\t\t\t\t   __func__, backbone_gw->orig);\n\npurge_now:\n\t\t\t \n\t\t\tif (atomic_read(&backbone_gw->request_sent))\n\t\t\t\tatomic_dec(&bat_priv->bla.num_requests);\n\n\t\t\tbatadv_bla_del_backbone_claims(backbone_gw);\n\n\t\t\thlist_del_rcu(&backbone_gw->hash_entry);\n\t\t\tbatadv_backbone_gw_put(backbone_gw);\n\t\t}\n\t\tspin_unlock_bh(list_lock);\n\t}\n}\n\n \nstatic void batadv_bla_purge_claims(struct batadv_priv *bat_priv,\n\t\t\t\t    struct batadv_hard_iface *primary_if,\n\t\t\t\t    int now)\n{\n\tstruct batadv_bla_backbone_gw *backbone_gw;\n\tstruct batadv_bla_claim *claim;\n\tstruct hlist_head *head;\n\tstruct batadv_hashtable *hash;\n\tint i;\n\n\thash = bat_priv->bla.claim_hash;\n\tif (!hash)\n\t\treturn;\n\n\tfor (i = 0; i < hash->size; i++) {\n\t\thead = &hash->table[i];\n\n\t\trcu_read_lock();\n\t\thlist_for_each_entry_rcu(claim, head, hash_entry) {\n\t\t\tbackbone_gw = batadv_bla_claim_get_backbone_gw(claim);\n\t\t\tif (now)\n\t\t\t\tgoto purge_now;\n\n\t\t\tif (!batadv_compare_eth(backbone_gw->orig,\n\t\t\t\t\t\tprimary_if->net_dev->dev_addr))\n\t\t\t\tgoto skip;\n\n\t\t\tif (!batadv_has_timed_out(claim->lasttime,\n\t\t\t\t\t\t  BATADV_BLA_CLAIM_TIMEOUT))\n\t\t\t\tgoto skip;\n\n\t\t\tbatadv_dbg(BATADV_DBG_BLA, bat_priv,\n\t\t\t\t   \"%s(): timed out.\\n\", __func__);\n\npurge_now:\n\t\t\tbatadv_dbg(BATADV_DBG_BLA, bat_priv,\n\t\t\t\t   \"%s(): %pM, vid %d\\n\", __func__,\n\t\t\t\t   claim->addr, claim->vid);\n\n\t\t\tbatadv_handle_unclaim(bat_priv, primary_if,\n\t\t\t\t\t      backbone_gw->orig,\n\t\t\t\t\t      claim->addr, claim->vid);\nskip:\n\t\t\tbatadv_backbone_gw_put(backbone_gw);\n\t\t}\n\t\trcu_read_unlock();\n\t}\n}\n\n \nvoid batadv_bla_update_orig_address(struct batadv_priv *bat_priv,\n\t\t\t\t    struct batadv_hard_iface *primary_if,\n\t\t\t\t    struct batadv_hard_iface *oldif)\n{\n\tstruct batadv_bla_backbone_gw *backbone_gw;\n\tstruct hlist_head *head;\n\tstruct batadv_hashtable *hash;\n\t__be16 group;\n\tint i;\n\n\t \n\tgroup = htons(crc16(0, primary_if->net_dev->dev_addr, ETH_ALEN));\n\tbat_priv->bla.claim_dest.group = group;\n\n\t \n\tif (!atomic_read(&bat_priv->bridge_loop_avoidance))\n\t\toldif = NULL;\n\n\tif (!oldif) {\n\t\tbatadv_bla_purge_claims(bat_priv, NULL, 1);\n\t\tbatadv_bla_purge_backbone_gw(bat_priv, 1);\n\t\treturn;\n\t}\n\n\thash = bat_priv->bla.backbone_hash;\n\tif (!hash)\n\t\treturn;\n\n\tfor (i = 0; i < hash->size; i++) {\n\t\thead = &hash->table[i];\n\n\t\trcu_read_lock();\n\t\thlist_for_each_entry_rcu(backbone_gw, head, hash_entry) {\n\t\t\t \n\t\t\tif (!batadv_compare_eth(backbone_gw->orig,\n\t\t\t\t\t\toldif->net_dev->dev_addr))\n\t\t\t\tcontinue;\n\n\t\t\tether_addr_copy(backbone_gw->orig,\n\t\t\t\t\tprimary_if->net_dev->dev_addr);\n\t\t\t \n\t\t\tbatadv_bla_send_announce(bat_priv, backbone_gw);\n\t\t}\n\t\trcu_read_unlock();\n\t}\n}\n\n \nstatic void\nbatadv_bla_send_loopdetect(struct batadv_priv *bat_priv,\n\t\t\t   struct batadv_bla_backbone_gw *backbone_gw)\n{\n\tbatadv_dbg(BATADV_DBG_BLA, bat_priv, \"Send loopdetect frame for vid %d\\n\",\n\t\t   backbone_gw->vid);\n\tbatadv_bla_send_claim(bat_priv, bat_priv->bla.loopdetect_addr,\n\t\t\t      backbone_gw->vid, BATADV_CLAIM_TYPE_LOOPDETECT);\n}\n\n \nvoid batadv_bla_status_update(struct net_device *net_dev)\n{\n\tstruct batadv_priv *bat_priv = netdev_priv(net_dev);\n\tstruct batadv_hard_iface *primary_if;\n\n\tprimary_if = batadv_primary_if_get_selected(bat_priv);\n\tif (!primary_if)\n\t\treturn;\n\n\t \n\tbatadv_bla_update_orig_address(bat_priv, primary_if, primary_if);\n\tbatadv_hardif_put(primary_if);\n}\n\n \nstatic void batadv_bla_periodic_work(struct work_struct *work)\n{\n\tstruct delayed_work *delayed_work;\n\tstruct batadv_priv *bat_priv;\n\tstruct batadv_priv_bla *priv_bla;\n\tstruct hlist_head *head;\n\tstruct batadv_bla_backbone_gw *backbone_gw;\n\tstruct batadv_hashtable *hash;\n\tstruct batadv_hard_iface *primary_if;\n\tbool send_loopdetect = false;\n\tint i;\n\n\tdelayed_work = to_delayed_work(work);\n\tpriv_bla = container_of(delayed_work, struct batadv_priv_bla, work);\n\tbat_priv = container_of(priv_bla, struct batadv_priv, bla);\n\tprimary_if = batadv_primary_if_get_selected(bat_priv);\n\tif (!primary_if)\n\t\tgoto out;\n\n\tbatadv_bla_purge_claims(bat_priv, primary_if, 0);\n\tbatadv_bla_purge_backbone_gw(bat_priv, 0);\n\n\tif (!atomic_read(&bat_priv->bridge_loop_avoidance))\n\t\tgoto out;\n\n\tif (atomic_dec_and_test(&bat_priv->bla.loopdetect_next)) {\n\t\t \n\t\teth_random_addr(bat_priv->bla.loopdetect_addr);\n\t\tbat_priv->bla.loopdetect_addr[0] = 0xba;\n\t\tbat_priv->bla.loopdetect_addr[1] = 0xbe;\n\t\tbat_priv->bla.loopdetect_lasttime = jiffies;\n\t\tatomic_set(&bat_priv->bla.loopdetect_next,\n\t\t\t   BATADV_BLA_LOOPDETECT_PERIODS);\n\n\t\t \n\t\tsend_loopdetect = true;\n\t}\n\n\thash = bat_priv->bla.backbone_hash;\n\tif (!hash)\n\t\tgoto out;\n\n\tfor (i = 0; i < hash->size; i++) {\n\t\thead = &hash->table[i];\n\n\t\trcu_read_lock();\n\t\thlist_for_each_entry_rcu(backbone_gw, head, hash_entry) {\n\t\t\tif (!batadv_compare_eth(backbone_gw->orig,\n\t\t\t\t\t\tprimary_if->net_dev->dev_addr))\n\t\t\t\tcontinue;\n\n\t\t\tbackbone_gw->lasttime = jiffies;\n\n\t\t\tbatadv_bla_send_announce(bat_priv, backbone_gw);\n\t\t\tif (send_loopdetect)\n\t\t\t\tbatadv_bla_send_loopdetect(bat_priv,\n\t\t\t\t\t\t\t   backbone_gw);\n\n\t\t\t \n\n\t\t\tif (atomic_read(&backbone_gw->request_sent) == 0)\n\t\t\t\tcontinue;\n\n\t\t\tif (!atomic_dec_and_test(&backbone_gw->wait_periods))\n\t\t\t\tcontinue;\n\n\t\t\tatomic_dec(&backbone_gw->bat_priv->bla.num_requests);\n\t\t\tatomic_set(&backbone_gw->request_sent, 0);\n\t\t}\n\t\trcu_read_unlock();\n\t}\nout:\n\tbatadv_hardif_put(primary_if);\n\n\tqueue_delayed_work(batadv_event_workqueue, &bat_priv->bla.work,\n\t\t\t   msecs_to_jiffies(BATADV_BLA_PERIOD_LENGTH));\n}\n\n \nstatic struct lock_class_key batadv_claim_hash_lock_class_key;\nstatic struct lock_class_key batadv_backbone_hash_lock_class_key;\n\n \nint batadv_bla_init(struct batadv_priv *bat_priv)\n{\n\tint i;\n\tu8 claim_dest[ETH_ALEN] = {0xff, 0x43, 0x05, 0x00, 0x00, 0x00};\n\tstruct batadv_hard_iface *primary_if;\n\tu16 crc;\n\tunsigned long entrytime;\n\n\tspin_lock_init(&bat_priv->bla.bcast_duplist_lock);\n\n\tbatadv_dbg(BATADV_DBG_BLA, bat_priv, \"bla hash registering\\n\");\n\n\t \n\tmemcpy(&bat_priv->bla.claim_dest.magic, claim_dest, 3);\n\tbat_priv->bla.claim_dest.type = 0;\n\tprimary_if = batadv_primary_if_get_selected(bat_priv);\n\tif (primary_if) {\n\t\tcrc = crc16(0, primary_if->net_dev->dev_addr, ETH_ALEN);\n\t\tbat_priv->bla.claim_dest.group = htons(crc);\n\t\tbatadv_hardif_put(primary_if);\n\t} else {\n\t\tbat_priv->bla.claim_dest.group = 0;  \n\t}\n\n\t \n\tentrytime = jiffies - msecs_to_jiffies(BATADV_DUPLIST_TIMEOUT);\n\tfor (i = 0; i < BATADV_DUPLIST_SIZE; i++)\n\t\tbat_priv->bla.bcast_duplist[i].entrytime = entrytime;\n\tbat_priv->bla.bcast_duplist_curr = 0;\n\n\tatomic_set(&bat_priv->bla.loopdetect_next,\n\t\t   BATADV_BLA_LOOPDETECT_PERIODS);\n\n\tif (bat_priv->bla.claim_hash)\n\t\treturn 0;\n\n\tbat_priv->bla.claim_hash = batadv_hash_new(128);\n\tif (!bat_priv->bla.claim_hash)\n\t\treturn -ENOMEM;\n\n\tbat_priv->bla.backbone_hash = batadv_hash_new(32);\n\tif (!bat_priv->bla.backbone_hash) {\n\t\tbatadv_hash_destroy(bat_priv->bla.claim_hash);\n\t\treturn -ENOMEM;\n\t}\n\n\tbatadv_hash_set_lock_class(bat_priv->bla.claim_hash,\n\t\t\t\t   &batadv_claim_hash_lock_class_key);\n\tbatadv_hash_set_lock_class(bat_priv->bla.backbone_hash,\n\t\t\t\t   &batadv_backbone_hash_lock_class_key);\n\n\tbatadv_dbg(BATADV_DBG_BLA, bat_priv, \"bla hashes initialized\\n\");\n\n\tINIT_DELAYED_WORK(&bat_priv->bla.work, batadv_bla_periodic_work);\n\n\tqueue_delayed_work(batadv_event_workqueue, &bat_priv->bla.work,\n\t\t\t   msecs_to_jiffies(BATADV_BLA_PERIOD_LENGTH));\n\treturn 0;\n}\n\n \nstatic bool batadv_bla_check_duplist(struct batadv_priv *bat_priv,\n\t\t\t\t     struct sk_buff *skb, u8 *payload_ptr,\n\t\t\t\t     const u8 *orig)\n{\n\tstruct batadv_bcast_duplist_entry *entry;\n\tbool ret = false;\n\tint i, curr;\n\t__be32 crc;\n\n\t \n\tcrc = batadv_skb_crc32(skb, payload_ptr);\n\n\tspin_lock_bh(&bat_priv->bla.bcast_duplist_lock);\n\n\tfor (i = 0; i < BATADV_DUPLIST_SIZE; i++) {\n\t\tcurr = (bat_priv->bla.bcast_duplist_curr + i);\n\t\tcurr %= BATADV_DUPLIST_SIZE;\n\t\tentry = &bat_priv->bla.bcast_duplist[curr];\n\n\t\t \n\t\tif (batadv_has_timed_out(entry->entrytime,\n\t\t\t\t\t BATADV_DUPLIST_TIMEOUT))\n\t\t\tbreak;\n\n\t\tif (entry->crc != crc)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (orig && !is_zero_ether_addr(orig) &&\n\t\t    !is_zero_ether_addr(entry->orig)) {\n\t\t\t \n\t\t\tif (batadv_compare_eth(entry->orig, orig))\n\t\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tret = true;\n\t\tgoto out;\n\t}\n\t \n\tcurr = (bat_priv->bla.bcast_duplist_curr + BATADV_DUPLIST_SIZE - 1);\n\tcurr %= BATADV_DUPLIST_SIZE;\n\tentry = &bat_priv->bla.bcast_duplist[curr];\n\tentry->crc = crc;\n\tentry->entrytime = jiffies;\n\n\t \n\tif (orig)\n\t\tether_addr_copy(entry->orig, orig);\n\t \n\telse\n\t\teth_zero_addr(entry->orig);\n\n\tbat_priv->bla.bcast_duplist_curr = curr;\n\nout:\n\tspin_unlock_bh(&bat_priv->bla.bcast_duplist_lock);\n\n\treturn ret;\n}\n\n \nstatic bool batadv_bla_check_ucast_duplist(struct batadv_priv *bat_priv,\n\t\t\t\t\t   struct sk_buff *skb)\n{\n\treturn batadv_bla_check_duplist(bat_priv, skb, (u8 *)skb->data, NULL);\n}\n\n \nbool batadv_bla_check_bcast_duplist(struct batadv_priv *bat_priv,\n\t\t\t\t    struct sk_buff *skb)\n{\n\tstruct batadv_bcast_packet *bcast_packet;\n\tu8 *payload_ptr;\n\n\tbcast_packet = (struct batadv_bcast_packet *)skb->data;\n\tpayload_ptr = (u8 *)(bcast_packet + 1);\n\n\treturn batadv_bla_check_duplist(bat_priv, skb, payload_ptr,\n\t\t\t\t\tbcast_packet->orig);\n}\n\n \nbool batadv_bla_is_backbone_gw_orig(struct batadv_priv *bat_priv, u8 *orig,\n\t\t\t\t    unsigned short vid)\n{\n\tstruct batadv_hashtable *hash = bat_priv->bla.backbone_hash;\n\tstruct hlist_head *head;\n\tstruct batadv_bla_backbone_gw *backbone_gw;\n\tint i;\n\n\tif (!atomic_read(&bat_priv->bridge_loop_avoidance))\n\t\treturn false;\n\n\tif (!hash)\n\t\treturn false;\n\n\tfor (i = 0; i < hash->size; i++) {\n\t\thead = &hash->table[i];\n\n\t\trcu_read_lock();\n\t\thlist_for_each_entry_rcu(backbone_gw, head, hash_entry) {\n\t\t\tif (batadv_compare_eth(backbone_gw->orig, orig) &&\n\t\t\t    backbone_gw->vid == vid) {\n\t\t\t\trcu_read_unlock();\n\t\t\t\treturn true;\n\t\t\t}\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\n\treturn false;\n}\n\n \nbool batadv_bla_is_backbone_gw(struct sk_buff *skb,\n\t\t\t       struct batadv_orig_node *orig_node, int hdr_size)\n{\n\tstruct batadv_bla_backbone_gw *backbone_gw;\n\tunsigned short vid;\n\n\tif (!atomic_read(&orig_node->bat_priv->bridge_loop_avoidance))\n\t\treturn false;\n\n\t \n\tif (!pskb_may_pull(skb, hdr_size + ETH_HLEN))\n\t\treturn false;\n\n\tvid = batadv_get_vid(skb, hdr_size);\n\n\t \n\tbackbone_gw = batadv_backbone_hash_find(orig_node->bat_priv,\n\t\t\t\t\t\torig_node->orig, vid);\n\tif (!backbone_gw)\n\t\treturn false;\n\n\tbatadv_backbone_gw_put(backbone_gw);\n\treturn true;\n}\n\n \nvoid batadv_bla_free(struct batadv_priv *bat_priv)\n{\n\tstruct batadv_hard_iface *primary_if;\n\n\tcancel_delayed_work_sync(&bat_priv->bla.work);\n\tprimary_if = batadv_primary_if_get_selected(bat_priv);\n\n\tif (bat_priv->bla.claim_hash) {\n\t\tbatadv_bla_purge_claims(bat_priv, primary_if, 1);\n\t\tbatadv_hash_destroy(bat_priv->bla.claim_hash);\n\t\tbat_priv->bla.claim_hash = NULL;\n\t}\n\tif (bat_priv->bla.backbone_hash) {\n\t\tbatadv_bla_purge_backbone_gw(bat_priv, 1);\n\t\tbatadv_hash_destroy(bat_priv->bla.backbone_hash);\n\t\tbat_priv->bla.backbone_hash = NULL;\n\t}\n\tbatadv_hardif_put(primary_if);\n}\n\n \nstatic bool\nbatadv_bla_loopdetect_check(struct batadv_priv *bat_priv, struct sk_buff *skb,\n\t\t\t    struct batadv_hard_iface *primary_if,\n\t\t\t    unsigned short vid)\n{\n\tstruct batadv_bla_backbone_gw *backbone_gw;\n\tstruct ethhdr *ethhdr;\n\tbool ret;\n\n\tethhdr = eth_hdr(skb);\n\n\t \n\tif (!batadv_compare_eth(ethhdr->h_source,\n\t\t\t\tbat_priv->bla.loopdetect_addr))\n\t\treturn false;\n\n\t \n\tif (batadv_has_timed_out(bat_priv->bla.loopdetect_lasttime,\n\t\t\t\t BATADV_BLA_LOOPDETECT_TIMEOUT))\n\t\treturn true;\n\n\tbackbone_gw = batadv_bla_get_backbone_gw(bat_priv,\n\t\t\t\t\t\t primary_if->net_dev->dev_addr,\n\t\t\t\t\t\t vid, true);\n\tif (unlikely(!backbone_gw))\n\t\treturn true;\n\n\tret = queue_work(batadv_event_workqueue, &backbone_gw->report_work);\n\n\t \n\tif (!ret)\n\t\tbatadv_backbone_gw_put(backbone_gw);\n\n\treturn true;\n}\n\n \nbool batadv_bla_rx(struct batadv_priv *bat_priv, struct sk_buff *skb,\n\t\t   unsigned short vid, int packet_type)\n{\n\tstruct batadv_bla_backbone_gw *backbone_gw;\n\tstruct ethhdr *ethhdr;\n\tstruct batadv_bla_claim search_claim, *claim = NULL;\n\tstruct batadv_hard_iface *primary_if;\n\tbool own_claim;\n\tbool ret;\n\n\tethhdr = eth_hdr(skb);\n\n\tprimary_if = batadv_primary_if_get_selected(bat_priv);\n\tif (!primary_if)\n\t\tgoto handled;\n\n\tif (!atomic_read(&bat_priv->bridge_loop_avoidance))\n\t\tgoto allow;\n\n\tif (batadv_bla_loopdetect_check(bat_priv, skb, primary_if, vid))\n\t\tgoto handled;\n\n\tif (unlikely(atomic_read(&bat_priv->bla.num_requests)))\n\t\t \n\t\tif (is_multicast_ether_addr(ethhdr->h_dest))\n\t\t\t \n\t\t\tif (packet_type == BATADV_BCAST ||\n\t\t\t    packet_type == BATADV_UNICAST)\n\t\t\t\tgoto handled;\n\n\t \n\tif (is_multicast_ether_addr(ethhdr->h_dest) &&\n\t    packet_type == BATADV_UNICAST &&\n\t    batadv_bla_check_ucast_duplist(bat_priv, skb))\n\t\tgoto handled;\n\n\tether_addr_copy(search_claim.addr, ethhdr->h_source);\n\tsearch_claim.vid = vid;\n\tclaim = batadv_claim_hash_find(bat_priv, &search_claim);\n\n\tif (!claim) {\n\t\t \n\t\t \n\n\t\tbatadv_dbg(BATADV_DBG_BLA, bat_priv,\n\t\t\t   \"%s(): Unclaimed MAC %pM found. Claim it. Local: %s\\n\",\n\t\t\t   __func__, ethhdr->h_source,\n\t\t\t   batadv_is_my_client(bat_priv,\n\t\t\t\t\t       ethhdr->h_source, vid) ?\n\t\t\t   \"yes\" : \"no\");\n\t\tbatadv_handle_claim(bat_priv, primary_if,\n\t\t\t\t    primary_if->net_dev->dev_addr,\n\t\t\t\t    ethhdr->h_source, vid);\n\t\tgoto allow;\n\t}\n\n\t \n\tbackbone_gw = batadv_bla_claim_get_backbone_gw(claim);\n\town_claim = batadv_compare_eth(backbone_gw->orig,\n\t\t\t\t       primary_if->net_dev->dev_addr);\n\tbatadv_backbone_gw_put(backbone_gw);\n\n\tif (own_claim) {\n\t\t \n\t\tclaim->lasttime = jiffies;\n\t\tgoto allow;\n\t}\n\n\t \n\tif (is_multicast_ether_addr(ethhdr->h_dest) &&\n\t    (packet_type == BATADV_BCAST || packet_type == BATADV_UNICAST)) {\n\t\t \n\t\tgoto handled;\n\t} else {\n\t\t \n\t\tbatadv_handle_claim(bat_priv, primary_if,\n\t\t\t\t    primary_if->net_dev->dev_addr,\n\t\t\t\t    ethhdr->h_source, vid);\n\t\tgoto allow;\n\t}\nallow:\n\tbatadv_bla_update_own_backbone_gw(bat_priv, primary_if, vid);\n\tret = false;\n\tgoto out;\n\nhandled:\n\tkfree_skb(skb);\n\tret = true;\n\nout:\n\tbatadv_hardif_put(primary_if);\n\tbatadv_claim_put(claim);\n\treturn ret;\n}\n\n \nbool batadv_bla_tx(struct batadv_priv *bat_priv, struct sk_buff *skb,\n\t\t   unsigned short vid)\n{\n\tstruct ethhdr *ethhdr;\n\tstruct batadv_bla_claim search_claim, *claim = NULL;\n\tstruct batadv_bla_backbone_gw *backbone_gw;\n\tstruct batadv_hard_iface *primary_if;\n\tbool client_roamed;\n\tbool ret = false;\n\n\tprimary_if = batadv_primary_if_get_selected(bat_priv);\n\tif (!primary_if)\n\t\tgoto out;\n\n\tif (!atomic_read(&bat_priv->bridge_loop_avoidance))\n\t\tgoto allow;\n\n\tif (batadv_bla_process_claim(bat_priv, primary_if, skb))\n\t\tgoto handled;\n\n\tethhdr = eth_hdr(skb);\n\n\tif (unlikely(atomic_read(&bat_priv->bla.num_requests)))\n\t\t \n\t\tif (is_multicast_ether_addr(ethhdr->h_dest))\n\t\t\tgoto handled;\n\n\tether_addr_copy(search_claim.addr, ethhdr->h_source);\n\tsearch_claim.vid = vid;\n\n\tclaim = batadv_claim_hash_find(bat_priv, &search_claim);\n\n\t \n\tif (!claim)\n\t\tgoto allow;\n\n\t \n\tbackbone_gw = batadv_bla_claim_get_backbone_gw(claim);\n\tclient_roamed = batadv_compare_eth(backbone_gw->orig,\n\t\t\t\t\t   primary_if->net_dev->dev_addr);\n\tbatadv_backbone_gw_put(backbone_gw);\n\n\tif (client_roamed) {\n\t\t \n\t\tif (batadv_has_timed_out(claim->lasttime, 100)) {\n\t\t\t \n\t\t\tbatadv_dbg(BATADV_DBG_BLA, bat_priv, \"%s(): Roaming client %pM detected. Unclaim it.\\n\",\n\t\t\t\t   __func__, ethhdr->h_source);\n\t\t\tbatadv_handle_unclaim(bat_priv, primary_if,\n\t\t\t\t\t      primary_if->net_dev->dev_addr,\n\t\t\t\t\t      ethhdr->h_source, vid);\n\t\t\tgoto allow;\n\t\t} else {\n\t\t\tbatadv_dbg(BATADV_DBG_BLA, bat_priv, \"%s(): Race for claim %pM detected. Drop packet.\\n\",\n\t\t\t\t   __func__, ethhdr->h_source);\n\t\t\tgoto handled;\n\t\t}\n\t}\n\n\t \n\tif (is_multicast_ether_addr(ethhdr->h_dest)) {\n\t\t \n\t\tgoto handled;\n\t} else {\n\t\t \n\t\tgoto allow;\n\t}\nallow:\n\tbatadv_bla_update_own_backbone_gw(bat_priv, primary_if, vid);\n\tret = false;\n\tgoto out;\nhandled:\n\tret = true;\nout:\n\tbatadv_hardif_put(primary_if);\n\tbatadv_claim_put(claim);\n\treturn ret;\n}\n\n \nstatic int\nbatadv_bla_claim_dump_entry(struct sk_buff *msg, u32 portid,\n\t\t\t    struct netlink_callback *cb,\n\t\t\t    struct batadv_hard_iface *primary_if,\n\t\t\t    struct batadv_bla_claim *claim)\n{\n\tconst u8 *primary_addr = primary_if->net_dev->dev_addr;\n\tu16 backbone_crc;\n\tbool is_own;\n\tvoid *hdr;\n\tint ret = -EINVAL;\n\n\thdr = genlmsg_put(msg, portid, cb->nlh->nlmsg_seq,\n\t\t\t  &batadv_netlink_family, NLM_F_MULTI,\n\t\t\t  BATADV_CMD_GET_BLA_CLAIM);\n\tif (!hdr) {\n\t\tret = -ENOBUFS;\n\t\tgoto out;\n\t}\n\n\tgenl_dump_check_consistent(cb, hdr);\n\n\tis_own = batadv_compare_eth(claim->backbone_gw->orig,\n\t\t\t\t    primary_addr);\n\n\tspin_lock_bh(&claim->backbone_gw->crc_lock);\n\tbackbone_crc = claim->backbone_gw->crc;\n\tspin_unlock_bh(&claim->backbone_gw->crc_lock);\n\n\tif (is_own)\n\t\tif (nla_put_flag(msg, BATADV_ATTR_BLA_OWN)) {\n\t\t\tgenlmsg_cancel(msg, hdr);\n\t\t\tgoto out;\n\t\t}\n\n\tif (nla_put(msg, BATADV_ATTR_BLA_ADDRESS, ETH_ALEN, claim->addr) ||\n\t    nla_put_u16(msg, BATADV_ATTR_BLA_VID, claim->vid) ||\n\t    nla_put(msg, BATADV_ATTR_BLA_BACKBONE, ETH_ALEN,\n\t\t    claim->backbone_gw->orig) ||\n\t    nla_put_u16(msg, BATADV_ATTR_BLA_CRC,\n\t\t\tbackbone_crc)) {\n\t\tgenlmsg_cancel(msg, hdr);\n\t\tgoto out;\n\t}\n\n\tgenlmsg_end(msg, hdr);\n\tret = 0;\n\nout:\n\treturn ret;\n}\n\n \nstatic int\nbatadv_bla_claim_dump_bucket(struct sk_buff *msg, u32 portid,\n\t\t\t     struct netlink_callback *cb,\n\t\t\t     struct batadv_hard_iface *primary_if,\n\t\t\t     struct batadv_hashtable *hash, unsigned int bucket,\n\t\t\t     int *idx_skip)\n{\n\tstruct batadv_bla_claim *claim;\n\tint idx = 0;\n\tint ret = 0;\n\n\tspin_lock_bh(&hash->list_locks[bucket]);\n\tcb->seq = atomic_read(&hash->generation) << 1 | 1;\n\n\thlist_for_each_entry(claim, &hash->table[bucket], hash_entry) {\n\t\tif (idx++ < *idx_skip)\n\t\t\tcontinue;\n\n\t\tret = batadv_bla_claim_dump_entry(msg, portid, cb,\n\t\t\t\t\t\t  primary_if, claim);\n\t\tif (ret) {\n\t\t\t*idx_skip = idx - 1;\n\t\t\tgoto unlock;\n\t\t}\n\t}\n\n\t*idx_skip = 0;\nunlock:\n\tspin_unlock_bh(&hash->list_locks[bucket]);\n\treturn ret;\n}\n\n \nint batadv_bla_claim_dump(struct sk_buff *msg, struct netlink_callback *cb)\n{\n\tstruct batadv_hard_iface *primary_if = NULL;\n\tint portid = NETLINK_CB(cb->skb).portid;\n\tstruct net *net = sock_net(cb->skb->sk);\n\tstruct net_device *soft_iface;\n\tstruct batadv_hashtable *hash;\n\tstruct batadv_priv *bat_priv;\n\tint bucket = cb->args[0];\n\tint idx = cb->args[1];\n\tint ifindex;\n\tint ret = 0;\n\n\tifindex = batadv_netlink_get_ifindex(cb->nlh,\n\t\t\t\t\t     BATADV_ATTR_MESH_IFINDEX);\n\tif (!ifindex)\n\t\treturn -EINVAL;\n\n\tsoft_iface = dev_get_by_index(net, ifindex);\n\tif (!soft_iface || !batadv_softif_is_valid(soft_iface)) {\n\t\tret = -ENODEV;\n\t\tgoto out;\n\t}\n\n\tbat_priv = netdev_priv(soft_iface);\n\thash = bat_priv->bla.claim_hash;\n\n\tprimary_if = batadv_primary_if_get_selected(bat_priv);\n\tif (!primary_if || primary_if->if_status != BATADV_IF_ACTIVE) {\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\twhile (bucket < hash->size) {\n\t\tif (batadv_bla_claim_dump_bucket(msg, portid, cb, primary_if,\n\t\t\t\t\t\t hash, bucket, &idx))\n\t\t\tbreak;\n\t\tbucket++;\n\t}\n\n\tcb->args[0] = bucket;\n\tcb->args[1] = idx;\n\n\tret = msg->len;\n\nout:\n\tbatadv_hardif_put(primary_if);\n\n\tdev_put(soft_iface);\n\n\treturn ret;\n}\n\n \nstatic int\nbatadv_bla_backbone_dump_entry(struct sk_buff *msg, u32 portid,\n\t\t\t       struct netlink_callback *cb,\n\t\t\t       struct batadv_hard_iface *primary_if,\n\t\t\t       struct batadv_bla_backbone_gw *backbone_gw)\n{\n\tconst u8 *primary_addr = primary_if->net_dev->dev_addr;\n\tu16 backbone_crc;\n\tbool is_own;\n\tint msecs;\n\tvoid *hdr;\n\tint ret = -EINVAL;\n\n\thdr = genlmsg_put(msg, portid, cb->nlh->nlmsg_seq,\n\t\t\t  &batadv_netlink_family, NLM_F_MULTI,\n\t\t\t  BATADV_CMD_GET_BLA_BACKBONE);\n\tif (!hdr) {\n\t\tret = -ENOBUFS;\n\t\tgoto out;\n\t}\n\n\tgenl_dump_check_consistent(cb, hdr);\n\n\tis_own = batadv_compare_eth(backbone_gw->orig, primary_addr);\n\n\tspin_lock_bh(&backbone_gw->crc_lock);\n\tbackbone_crc = backbone_gw->crc;\n\tspin_unlock_bh(&backbone_gw->crc_lock);\n\n\tmsecs = jiffies_to_msecs(jiffies - backbone_gw->lasttime);\n\n\tif (is_own)\n\t\tif (nla_put_flag(msg, BATADV_ATTR_BLA_OWN)) {\n\t\t\tgenlmsg_cancel(msg, hdr);\n\t\t\tgoto out;\n\t\t}\n\n\tif (nla_put(msg, BATADV_ATTR_BLA_BACKBONE, ETH_ALEN,\n\t\t    backbone_gw->orig) ||\n\t    nla_put_u16(msg, BATADV_ATTR_BLA_VID, backbone_gw->vid) ||\n\t    nla_put_u16(msg, BATADV_ATTR_BLA_CRC,\n\t\t\tbackbone_crc) ||\n\t    nla_put_u32(msg, BATADV_ATTR_LAST_SEEN_MSECS, msecs)) {\n\t\tgenlmsg_cancel(msg, hdr);\n\t\tgoto out;\n\t}\n\n\tgenlmsg_end(msg, hdr);\n\tret = 0;\n\nout:\n\treturn ret;\n}\n\n \nstatic int\nbatadv_bla_backbone_dump_bucket(struct sk_buff *msg, u32 portid,\n\t\t\t\tstruct netlink_callback *cb,\n\t\t\t\tstruct batadv_hard_iface *primary_if,\n\t\t\t\tstruct batadv_hashtable *hash,\n\t\t\t\tunsigned int bucket, int *idx_skip)\n{\n\tstruct batadv_bla_backbone_gw *backbone_gw;\n\tint idx = 0;\n\tint ret = 0;\n\n\tspin_lock_bh(&hash->list_locks[bucket]);\n\tcb->seq = atomic_read(&hash->generation) << 1 | 1;\n\n\thlist_for_each_entry(backbone_gw, &hash->table[bucket], hash_entry) {\n\t\tif (idx++ < *idx_skip)\n\t\t\tcontinue;\n\n\t\tret = batadv_bla_backbone_dump_entry(msg, portid, cb,\n\t\t\t\t\t\t     primary_if, backbone_gw);\n\t\tif (ret) {\n\t\t\t*idx_skip = idx - 1;\n\t\t\tgoto unlock;\n\t\t}\n\t}\n\n\t*idx_skip = 0;\nunlock:\n\tspin_unlock_bh(&hash->list_locks[bucket]);\n\treturn ret;\n}\n\n \nint batadv_bla_backbone_dump(struct sk_buff *msg, struct netlink_callback *cb)\n{\n\tstruct batadv_hard_iface *primary_if = NULL;\n\tint portid = NETLINK_CB(cb->skb).portid;\n\tstruct net *net = sock_net(cb->skb->sk);\n\tstruct net_device *soft_iface;\n\tstruct batadv_hashtable *hash;\n\tstruct batadv_priv *bat_priv;\n\tint bucket = cb->args[0];\n\tint idx = cb->args[1];\n\tint ifindex;\n\tint ret = 0;\n\n\tifindex = batadv_netlink_get_ifindex(cb->nlh,\n\t\t\t\t\t     BATADV_ATTR_MESH_IFINDEX);\n\tif (!ifindex)\n\t\treturn -EINVAL;\n\n\tsoft_iface = dev_get_by_index(net, ifindex);\n\tif (!soft_iface || !batadv_softif_is_valid(soft_iface)) {\n\t\tret = -ENODEV;\n\t\tgoto out;\n\t}\n\n\tbat_priv = netdev_priv(soft_iface);\n\thash = bat_priv->bla.backbone_hash;\n\n\tprimary_if = batadv_primary_if_get_selected(bat_priv);\n\tif (!primary_if || primary_if->if_status != BATADV_IF_ACTIVE) {\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\twhile (bucket < hash->size) {\n\t\tif (batadv_bla_backbone_dump_bucket(msg, portid, cb, primary_if,\n\t\t\t\t\t\t    hash, bucket, &idx))\n\t\t\tbreak;\n\t\tbucket++;\n\t}\n\n\tcb->args[0] = bucket;\n\tcb->args[1] = idx;\n\n\tret = msg->len;\n\nout:\n\tbatadv_hardif_put(primary_if);\n\n\tdev_put(soft_iface);\n\n\treturn ret;\n}\n\n#ifdef CONFIG_BATMAN_ADV_DAT\n \nbool batadv_bla_check_claim(struct batadv_priv *bat_priv,\n\t\t\t    u8 *addr, unsigned short vid)\n{\n\tstruct batadv_bla_claim search_claim;\n\tstruct batadv_bla_claim *claim = NULL;\n\tstruct batadv_hard_iface *primary_if = NULL;\n\tbool ret = true;\n\n\tif (!atomic_read(&bat_priv->bridge_loop_avoidance))\n\t\treturn ret;\n\n\tprimary_if = batadv_primary_if_get_selected(bat_priv);\n\tif (!primary_if)\n\t\treturn ret;\n\n\t \n\tether_addr_copy(search_claim.addr, addr);\n\tsearch_claim.vid = vid;\n\n\tclaim = batadv_claim_hash_find(bat_priv, &search_claim);\n\n\t \n\tif (claim) {\n\t\tif (!batadv_compare_eth(claim->backbone_gw->orig,\n\t\t\t\t\tprimary_if->net_dev->dev_addr))\n\t\t\tret = false;\n\t\tbatadv_claim_put(claim);\n\t}\n\n\tbatadv_hardif_put(primary_if);\n\treturn ret;\n}\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}