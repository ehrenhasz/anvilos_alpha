{
  "module_name": "tcp_fastopen.c",
  "hash_id": "613b4000f46dc0e1824e2731e6257ce86895f9135774764ad1ceb02dd4b0a8d4",
  "original_prompt": "Ingested from linux-6.6.14/net/ipv4/tcp_fastopen.c",
  "human_readable_source": "\n#include <linux/kernel.h>\n#include <linux/tcp.h>\n#include <linux/rcupdate.h>\n#include <net/tcp.h>\n\nvoid tcp_fastopen_init_key_once(struct net *net)\n{\n\tu8 key[TCP_FASTOPEN_KEY_LENGTH];\n\tstruct tcp_fastopen_context *ctxt;\n\n\trcu_read_lock();\n\tctxt = rcu_dereference(net->ipv4.tcp_fastopen_ctx);\n\tif (ctxt) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\trcu_read_unlock();\n\n\t \n\tget_random_bytes(key, sizeof(key));\n\ttcp_fastopen_reset_cipher(net, NULL, key, NULL);\n}\n\nstatic void tcp_fastopen_ctx_free(struct rcu_head *head)\n{\n\tstruct tcp_fastopen_context *ctx =\n\t    container_of(head, struct tcp_fastopen_context, rcu);\n\n\tkfree_sensitive(ctx);\n}\n\nvoid tcp_fastopen_destroy_cipher(struct sock *sk)\n{\n\tstruct tcp_fastopen_context *ctx;\n\n\tctx = rcu_dereference_protected(\n\t\t\tinet_csk(sk)->icsk_accept_queue.fastopenq.ctx, 1);\n\tif (ctx)\n\t\tcall_rcu(&ctx->rcu, tcp_fastopen_ctx_free);\n}\n\nvoid tcp_fastopen_ctx_destroy(struct net *net)\n{\n\tstruct tcp_fastopen_context *ctxt;\n\n\tctxt = xchg((__force struct tcp_fastopen_context **)&net->ipv4.tcp_fastopen_ctx, NULL);\n\n\tif (ctxt)\n\t\tcall_rcu(&ctxt->rcu, tcp_fastopen_ctx_free);\n}\n\nint tcp_fastopen_reset_cipher(struct net *net, struct sock *sk,\n\t\t\t      void *primary_key, void *backup_key)\n{\n\tstruct tcp_fastopen_context *ctx, *octx;\n\tstruct fastopen_queue *q;\n\tint err = 0;\n\n\tctx = kmalloc(sizeof(*ctx), GFP_KERNEL);\n\tif (!ctx) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tctx->key[0].key[0] = get_unaligned_le64(primary_key);\n\tctx->key[0].key[1] = get_unaligned_le64(primary_key + 8);\n\tif (backup_key) {\n\t\tctx->key[1].key[0] = get_unaligned_le64(backup_key);\n\t\tctx->key[1].key[1] = get_unaligned_le64(backup_key + 8);\n\t\tctx->num = 2;\n\t} else {\n\t\tctx->num = 1;\n\t}\n\n\tif (sk) {\n\t\tq = &inet_csk(sk)->icsk_accept_queue.fastopenq;\n\t\toctx = xchg((__force struct tcp_fastopen_context **)&q->ctx, ctx);\n\t} else {\n\t\toctx = xchg((__force struct tcp_fastopen_context **)&net->ipv4.tcp_fastopen_ctx, ctx);\n\t}\n\n\tif (octx)\n\t\tcall_rcu(&octx->rcu, tcp_fastopen_ctx_free);\nout:\n\treturn err;\n}\n\nint tcp_fastopen_get_cipher(struct net *net, struct inet_connection_sock *icsk,\n\t\t\t    u64 *key)\n{\n\tstruct tcp_fastopen_context *ctx;\n\tint n_keys = 0, i;\n\n\trcu_read_lock();\n\tif (icsk)\n\t\tctx = rcu_dereference(icsk->icsk_accept_queue.fastopenq.ctx);\n\telse\n\t\tctx = rcu_dereference(net->ipv4.tcp_fastopen_ctx);\n\tif (ctx) {\n\t\tn_keys = tcp_fastopen_context_len(ctx);\n\t\tfor (i = 0; i < n_keys; i++) {\n\t\t\tput_unaligned_le64(ctx->key[i].key[0], key + (i * 2));\n\t\t\tput_unaligned_le64(ctx->key[i].key[1], key + (i * 2) + 1);\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\treturn n_keys;\n}\n\nstatic bool __tcp_fastopen_cookie_gen_cipher(struct request_sock *req,\n\t\t\t\t\t     struct sk_buff *syn,\n\t\t\t\t\t     const siphash_key_t *key,\n\t\t\t\t\t     struct tcp_fastopen_cookie *foc)\n{\n\tBUILD_BUG_ON(TCP_FASTOPEN_COOKIE_SIZE != sizeof(u64));\n\n\tif (req->rsk_ops->family == AF_INET) {\n\t\tconst struct iphdr *iph = ip_hdr(syn);\n\n\t\tfoc->val[0] = cpu_to_le64(siphash(&iph->saddr,\n\t\t\t\t\t  sizeof(iph->saddr) +\n\t\t\t\t\t  sizeof(iph->daddr),\n\t\t\t\t\t  key));\n\t\tfoc->len = TCP_FASTOPEN_COOKIE_SIZE;\n\t\treturn true;\n\t}\n#if IS_ENABLED(CONFIG_IPV6)\n\tif (req->rsk_ops->family == AF_INET6) {\n\t\tconst struct ipv6hdr *ip6h = ipv6_hdr(syn);\n\n\t\tfoc->val[0] = cpu_to_le64(siphash(&ip6h->saddr,\n\t\t\t\t\t  sizeof(ip6h->saddr) +\n\t\t\t\t\t  sizeof(ip6h->daddr),\n\t\t\t\t\t  key));\n\t\tfoc->len = TCP_FASTOPEN_COOKIE_SIZE;\n\t\treturn true;\n\t}\n#endif\n\treturn false;\n}\n\n \nstatic void tcp_fastopen_cookie_gen(struct sock *sk,\n\t\t\t\t    struct request_sock *req,\n\t\t\t\t    struct sk_buff *syn,\n\t\t\t\t    struct tcp_fastopen_cookie *foc)\n{\n\tstruct tcp_fastopen_context *ctx;\n\n\trcu_read_lock();\n\tctx = tcp_fastopen_get_ctx(sk);\n\tif (ctx)\n\t\t__tcp_fastopen_cookie_gen_cipher(req, syn, &ctx->key[0], foc);\n\trcu_read_unlock();\n}\n\n \nvoid tcp_fastopen_add_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (TCP_SKB_CB(skb)->end_seq == tp->rcv_nxt)\n\t\treturn;\n\n\tskb = skb_clone(skb, GFP_ATOMIC);\n\tif (!skb)\n\t\treturn;\n\n\tskb_dst_drop(skb);\n\t \n\ttp->segs_in = 0;\n\ttcp_segs_in(tp, skb);\n\t__skb_pull(skb, tcp_hdrlen(skb));\n\tsk_forced_mem_schedule(sk, skb->truesize);\n\tskb_set_owner_r(skb, sk);\n\n\tTCP_SKB_CB(skb)->seq++;\n\tTCP_SKB_CB(skb)->tcp_flags &= ~TCPHDR_SYN;\n\n\ttp->rcv_nxt = TCP_SKB_CB(skb)->end_seq;\n\t__skb_queue_tail(&sk->sk_receive_queue, skb);\n\ttp->syn_data_acked = 1;\n\n\t \n\ttp->bytes_received = skb->len;\n\n\tif (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN)\n\t\ttcp_fin(sk);\n}\n\n \nstatic int tcp_fastopen_cookie_gen_check(struct sock *sk,\n\t\t\t\t\t struct request_sock *req,\n\t\t\t\t\t struct sk_buff *syn,\n\t\t\t\t\t struct tcp_fastopen_cookie *orig,\n\t\t\t\t\t struct tcp_fastopen_cookie *valid_foc)\n{\n\tstruct tcp_fastopen_cookie search_foc = { .len = -1 };\n\tstruct tcp_fastopen_cookie *foc = valid_foc;\n\tstruct tcp_fastopen_context *ctx;\n\tint i, ret = 0;\n\n\trcu_read_lock();\n\tctx = tcp_fastopen_get_ctx(sk);\n\tif (!ctx)\n\t\tgoto out;\n\tfor (i = 0; i < tcp_fastopen_context_len(ctx); i++) {\n\t\t__tcp_fastopen_cookie_gen_cipher(req, syn, &ctx->key[i], foc);\n\t\tif (tcp_fastopen_cookie_match(foc, orig)) {\n\t\t\tret = i + 1;\n\t\t\tgoto out;\n\t\t}\n\t\tfoc = &search_foc;\n\t}\nout:\n\trcu_read_unlock();\n\treturn ret;\n}\n\nstatic struct sock *tcp_fastopen_create_child(struct sock *sk,\n\t\t\t\t\t      struct sk_buff *skb,\n\t\t\t\t\t      struct request_sock *req)\n{\n\tstruct tcp_sock *tp;\n\tstruct request_sock_queue *queue = &inet_csk(sk)->icsk_accept_queue;\n\tstruct sock *child;\n\tbool own_req;\n\n\tchild = inet_csk(sk)->icsk_af_ops->syn_recv_sock(sk, skb, req, NULL,\n\t\t\t\t\t\t\t NULL, &own_req);\n\tif (!child)\n\t\treturn NULL;\n\n\tspin_lock(&queue->fastopenq.lock);\n\tqueue->fastopenq.qlen++;\n\tspin_unlock(&queue->fastopenq.lock);\n\n\t \n\ttp = tcp_sk(child);\n\n\trcu_assign_pointer(tp->fastopen_rsk, req);\n\ttcp_rsk(req)->tfo_listener = true;\n\n\t \n\ttp->snd_wnd = ntohs(tcp_hdr(skb)->window);\n\ttp->max_window = tp->snd_wnd;\n\n\t \n\treq->timeout = tcp_timeout_init(child);\n\tinet_csk_reset_xmit_timer(child, ICSK_TIME_RETRANS,\n\t\t\t\t  req->timeout, TCP_RTO_MAX);\n\n\trefcount_set(&req->rsk_refcnt, 2);\n\n\t \n\ttcp_init_transfer(child, BPF_SOCK_OPS_PASSIVE_ESTABLISHED_CB, skb);\n\n\ttp->rcv_nxt = TCP_SKB_CB(skb)->seq + 1;\n\n\ttcp_fastopen_add_skb(child, skb);\n\n\ttcp_rsk(req)->rcv_nxt = tp->rcv_nxt;\n\ttp->rcv_wup = tp->rcv_nxt;\n\t \n\treturn child;\n}\n\nstatic bool tcp_fastopen_queue_check(struct sock *sk)\n{\n\tstruct fastopen_queue *fastopenq;\n\tint max_qlen;\n\n\t \n\tfastopenq = &inet_csk(sk)->icsk_accept_queue.fastopenq;\n\tmax_qlen = READ_ONCE(fastopenq->max_qlen);\n\tif (max_qlen == 0)\n\t\treturn false;\n\n\tif (fastopenq->qlen >= max_qlen) {\n\t\tstruct request_sock *req1;\n\t\tspin_lock(&fastopenq->lock);\n\t\treq1 = fastopenq->rskq_rst_head;\n\t\tif (!req1 || time_after(req1->rsk_timer.expires, jiffies)) {\n\t\t\t__NET_INC_STATS(sock_net(sk),\n\t\t\t\t\tLINUX_MIB_TCPFASTOPENLISTENOVERFLOW);\n\t\t\tspin_unlock(&fastopenq->lock);\n\t\t\treturn false;\n\t\t}\n\t\tfastopenq->rskq_rst_head = req1->dl_next;\n\t\tfastopenq->qlen--;\n\t\tspin_unlock(&fastopenq->lock);\n\t\treqsk_put(req1);\n\t}\n\treturn true;\n}\n\nstatic bool tcp_fastopen_no_cookie(const struct sock *sk,\n\t\t\t\t   const struct dst_entry *dst,\n\t\t\t\t   int flag)\n{\n\treturn (READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_fastopen) & flag) ||\n\t       tcp_sk(sk)->fastopen_no_cookie ||\n\t       (dst && dst_metric(dst, RTAX_FASTOPEN_NO_COOKIE));\n}\n\n \nstruct sock *tcp_try_fastopen(struct sock *sk, struct sk_buff *skb,\n\t\t\t      struct request_sock *req,\n\t\t\t      struct tcp_fastopen_cookie *foc,\n\t\t\t      const struct dst_entry *dst)\n{\n\tbool syn_data = TCP_SKB_CB(skb)->end_seq != TCP_SKB_CB(skb)->seq + 1;\n\tint tcp_fastopen = READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_fastopen);\n\tstruct tcp_fastopen_cookie valid_foc = { .len = -1 };\n\tstruct sock *child;\n\tint ret = 0;\n\n\tif (foc->len == 0)  \n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPFASTOPENCOOKIEREQD);\n\n\tif (!((tcp_fastopen & TFO_SERVER_ENABLE) &&\n\t      (syn_data || foc->len >= 0) &&\n\t      tcp_fastopen_queue_check(sk))) {\n\t\tfoc->len = -1;\n\t\treturn NULL;\n\t}\n\n\tif (tcp_fastopen_no_cookie(sk, dst, TFO_SERVER_COOKIE_NOT_REQD))\n\t\tgoto fastopen;\n\n\tif (foc->len == 0) {\n\t\t \n\t\ttcp_fastopen_cookie_gen(sk, req, skb, &valid_foc);\n\t} else if (foc->len > 0) {\n\t\tret = tcp_fastopen_cookie_gen_check(sk, req, skb, foc,\n\t\t\t\t\t\t    &valid_foc);\n\t\tif (!ret) {\n\t\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\t      LINUX_MIB_TCPFASTOPENPASSIVEFAIL);\n\t\t} else {\n\t\t\t \nfastopen:\n\t\t\tchild = tcp_fastopen_create_child(sk, skb, req);\n\t\t\tif (child) {\n\t\t\t\tif (ret == 2) {\n\t\t\t\t\tvalid_foc.exp = foc->exp;\n\t\t\t\t\t*foc = valid_foc;\n\t\t\t\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\t\t\t      LINUX_MIB_TCPFASTOPENPASSIVEALTKEY);\n\t\t\t\t} else {\n\t\t\t\t\tfoc->len = -1;\n\t\t\t\t}\n\t\t\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\t\t      LINUX_MIB_TCPFASTOPENPASSIVE);\n\t\t\t\treturn child;\n\t\t\t}\n\t\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\t      LINUX_MIB_TCPFASTOPENPASSIVEFAIL);\n\t\t}\n\t}\n\tvalid_foc.exp = foc->exp;\n\t*foc = valid_foc;\n\treturn NULL;\n}\n\nbool tcp_fastopen_cookie_check(struct sock *sk, u16 *mss,\n\t\t\t       struct tcp_fastopen_cookie *cookie)\n{\n\tconst struct dst_entry *dst;\n\n\ttcp_fastopen_cache_get(sk, mss, cookie);\n\n\t \n\tif (tcp_fastopen_active_should_disable(sk)) {\n\t\tcookie->len = -1;\n\t\treturn false;\n\t}\n\n\tdst = __sk_dst_get(sk);\n\n\tif (tcp_fastopen_no_cookie(sk, dst, TFO_CLIENT_NO_COOKIE)) {\n\t\tcookie->len = -1;\n\t\treturn true;\n\t}\n\tif (cookie->len > 0)\n\t\treturn true;\n\ttcp_sk(sk)->fastopen_client_fail = TFO_COOKIE_UNAVAILABLE;\n\treturn false;\n}\n\n \nbool tcp_fastopen_defer_connect(struct sock *sk, int *err)\n{\n\tstruct tcp_fastopen_cookie cookie = { .len = 0 };\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu16 mss;\n\n\tif (tp->fastopen_connect && !tp->fastopen_req) {\n\t\tif (tcp_fastopen_cookie_check(sk, &mss, &cookie)) {\n\t\t\tinet_set_bit(DEFER_CONNECT, sk);\n\t\t\treturn true;\n\t\t}\n\n\t\t \n\t\ttp->fastopen_req = kzalloc(sizeof(*tp->fastopen_req),\n\t\t\t\t\t   sk->sk_allocation);\n\t\tif (tp->fastopen_req)\n\t\t\ttp->fastopen_req->cookie = cookie;\n\t\telse\n\t\t\t*err = -ENOBUFS;\n\t}\n\treturn false;\n}\nEXPORT_SYMBOL(tcp_fastopen_defer_connect);\n\n \n\n \nvoid tcp_fastopen_active_disable(struct sock *sk)\n{\n\tstruct net *net = sock_net(sk);\n\n\tif (!READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_fastopen_blackhole_timeout))\n\t\treturn;\n\n\t \n\tWRITE_ONCE(net->ipv4.tfo_active_disable_stamp, jiffies);\n\n\t \n\tsmp_mb__before_atomic();\n\tatomic_inc(&net->ipv4.tfo_active_disable_times);\n\n\tNET_INC_STATS(net, LINUX_MIB_TCPFASTOPENBLACKHOLE);\n}\n\n \nbool tcp_fastopen_active_should_disable(struct sock *sk)\n{\n\tunsigned int tfo_bh_timeout =\n\t\tREAD_ONCE(sock_net(sk)->ipv4.sysctl_tcp_fastopen_blackhole_timeout);\n\tunsigned long timeout;\n\tint tfo_da_times;\n\tint multiplier;\n\n\tif (!tfo_bh_timeout)\n\t\treturn false;\n\n\ttfo_da_times = atomic_read(&sock_net(sk)->ipv4.tfo_active_disable_times);\n\tif (!tfo_da_times)\n\t\treturn false;\n\n\t \n\tsmp_rmb();\n\n\t \n\tmultiplier = 1 << min(tfo_da_times - 1, 6);\n\n\t \n\ttimeout = READ_ONCE(sock_net(sk)->ipv4.tfo_active_disable_stamp) +\n\t\t  multiplier * tfo_bh_timeout * HZ;\n\tif (time_before(jiffies, timeout))\n\t\treturn true;\n\n\t \n\ttcp_sk(sk)->syn_fastopen_ch = 1;\n\treturn false;\n}\n\n \nvoid tcp_fastopen_active_disable_ofo_check(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct dst_entry *dst;\n\tstruct sk_buff *skb;\n\n\tif (!tp->syn_fastopen)\n\t\treturn;\n\n\tif (!tp->data_segs_in) {\n\t\tskb = skb_rb_first(&tp->out_of_order_queue);\n\t\tif (skb && !skb_rb_next(skb)) {\n\t\t\tif (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN) {\n\t\t\t\ttcp_fastopen_active_disable(sk);\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t} else if (tp->syn_fastopen_ch &&\n\t\t   atomic_read(&sock_net(sk)->ipv4.tfo_active_disable_times)) {\n\t\tdst = sk_dst_get(sk);\n\t\tif (!(dst && dst->dev && (dst->dev->flags & IFF_LOOPBACK)))\n\t\t\tatomic_set(&sock_net(sk)->ipv4.tfo_active_disable_times, 0);\n\t\tdst_release(dst);\n\t}\n}\n\nvoid tcp_fastopen_active_detect_blackhole(struct sock *sk, bool expired)\n{\n\tu32 timeouts = inet_csk(sk)->icsk_retransmits;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t \n\tif ((tp->syn_fastopen || tp->syn_data || tp->syn_data_acked) &&\n\t    (timeouts == 2 || (timeouts < 2 && expired))) {\n\t\ttcp_fastopen_active_disable(sk);\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPFASTOPENACTIVEFAIL);\n\t}\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}