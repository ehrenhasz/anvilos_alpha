{
  "module_name": "tcp_input.c",
  "hash_id": "6dea6cebe2ea901634fe87f09ef6ffabdf98ad2db4984ff01802e8b272755856",
  "original_prompt": "Ingested from linux-6.6.14/net/ipv4/tcp_input.c",
  "human_readable_source": "\n \n\n \n\n#define pr_fmt(fmt) \"TCP: \" fmt\n\n#include <linux/mm.h>\n#include <linux/slab.h>\n#include <linux/module.h>\n#include <linux/sysctl.h>\n#include <linux/kernel.h>\n#include <linux/prefetch.h>\n#include <net/dst.h>\n#include <net/tcp.h>\n#include <net/inet_common.h>\n#include <linux/ipsec.h>\n#include <asm/unaligned.h>\n#include <linux/errqueue.h>\n#include <trace/events/tcp.h>\n#include <linux/jump_label_ratelimit.h>\n#include <net/busy_poll.h>\n#include <net/mptcp.h>\n\nint sysctl_tcp_max_orphans __read_mostly = NR_FILE;\n\n#define FLAG_DATA\t\t0x01  \n#define FLAG_WIN_UPDATE\t\t0x02  \n#define FLAG_DATA_ACKED\t\t0x04  \n#define FLAG_RETRANS_DATA_ACKED\t0x08  \n#define FLAG_SYN_ACKED\t\t0x10  \n#define FLAG_DATA_SACKED\t0x20  \n#define FLAG_ECE\t\t0x40  \n#define FLAG_LOST_RETRANS\t0x80  \n#define FLAG_SLOWPATH\t\t0x100  \n#define FLAG_ORIG_SACK_ACKED\t0x200  \n#define FLAG_SND_UNA_ADVANCED\t0x400  \n#define FLAG_DSACKING_ACK\t0x800  \n#define FLAG_SET_XMIT_TIMER\t0x1000  \n#define FLAG_SACK_RENEGING\t0x2000  \n#define FLAG_UPDATE_TS_RECENT\t0x4000  \n#define FLAG_NO_CHALLENGE_ACK\t0x8000  \n#define FLAG_ACK_MAYBE_DELAYED\t0x10000  \n#define FLAG_DSACK_TLP\t\t0x20000  \n\n#define FLAG_ACKED\t\t(FLAG_DATA_ACKED|FLAG_SYN_ACKED)\n#define FLAG_NOT_DUP\t\t(FLAG_DATA|FLAG_WIN_UPDATE|FLAG_ACKED)\n#define FLAG_CA_ALERT\t\t(FLAG_DATA_SACKED|FLAG_ECE|FLAG_DSACKING_ACK)\n#define FLAG_FORWARD_PROGRESS\t(FLAG_ACKED|FLAG_DATA_SACKED)\n\n#define TCP_REMNANT (TCP_FLAG_FIN|TCP_FLAG_URG|TCP_FLAG_SYN|TCP_FLAG_PSH)\n#define TCP_HP_BITS (~(TCP_RESERVED_BITS|TCP_FLAG_PSH))\n\n#define REXMIT_NONE\t0  \n#define REXMIT_LOST\t1  \n#define REXMIT_NEW\t2  \n\n#if IS_ENABLED(CONFIG_TLS_DEVICE)\nstatic DEFINE_STATIC_KEY_DEFERRED_FALSE(clean_acked_data_enabled, HZ);\n\nvoid clean_acked_data_enable(struct inet_connection_sock *icsk,\n\t\t\t     void (*cad)(struct sock *sk, u32 ack_seq))\n{\n\ticsk->icsk_clean_acked = cad;\n\tstatic_branch_deferred_inc(&clean_acked_data_enabled);\n}\nEXPORT_SYMBOL_GPL(clean_acked_data_enable);\n\nvoid clean_acked_data_disable(struct inet_connection_sock *icsk)\n{\n\tstatic_branch_slow_dec_deferred(&clean_acked_data_enabled);\n\ticsk->icsk_clean_acked = NULL;\n}\nEXPORT_SYMBOL_GPL(clean_acked_data_disable);\n\nvoid clean_acked_data_flush(void)\n{\n\tstatic_key_deferred_flush(&clean_acked_data_enabled);\n}\nEXPORT_SYMBOL_GPL(clean_acked_data_flush);\n#endif\n\n#ifdef CONFIG_CGROUP_BPF\nstatic void bpf_skops_parse_hdr(struct sock *sk, struct sk_buff *skb)\n{\n\tbool unknown_opt = tcp_sk(sk)->rx_opt.saw_unknown &&\n\t\tBPF_SOCK_OPS_TEST_FLAG(tcp_sk(sk),\n\t\t\t\t       BPF_SOCK_OPS_PARSE_UNKNOWN_HDR_OPT_CB_FLAG);\n\tbool parse_all_opt = BPF_SOCK_OPS_TEST_FLAG(tcp_sk(sk),\n\t\t\t\t\t\t    BPF_SOCK_OPS_PARSE_ALL_HDR_OPT_CB_FLAG);\n\tstruct bpf_sock_ops_kern sock_ops;\n\n\tif (likely(!unknown_opt && !parse_all_opt))\n\t\treturn;\n\n\t \n\tswitch (sk->sk_state) {\n\tcase TCP_SYN_RECV:\n\tcase TCP_SYN_SENT:\n\tcase TCP_LISTEN:\n\t\treturn;\n\t}\n\n\tsock_owned_by_me(sk);\n\n\tmemset(&sock_ops, 0, offsetof(struct bpf_sock_ops_kern, temp));\n\tsock_ops.op = BPF_SOCK_OPS_PARSE_HDR_OPT_CB;\n\tsock_ops.is_fullsock = 1;\n\tsock_ops.sk = sk;\n\tbpf_skops_init_skb(&sock_ops, skb, tcp_hdrlen(skb));\n\n\tBPF_CGROUP_RUN_PROG_SOCK_OPS(&sock_ops);\n}\n\nstatic void bpf_skops_established(struct sock *sk, int bpf_op,\n\t\t\t\t  struct sk_buff *skb)\n{\n\tstruct bpf_sock_ops_kern sock_ops;\n\n\tsock_owned_by_me(sk);\n\n\tmemset(&sock_ops, 0, offsetof(struct bpf_sock_ops_kern, temp));\n\tsock_ops.op = bpf_op;\n\tsock_ops.is_fullsock = 1;\n\tsock_ops.sk = sk;\n\t \n\tif (skb)\n\t\tbpf_skops_init_skb(&sock_ops, skb, tcp_hdrlen(skb));\n\n\tBPF_CGROUP_RUN_PROG_SOCK_OPS(&sock_ops);\n}\n#else\nstatic void bpf_skops_parse_hdr(struct sock *sk, struct sk_buff *skb)\n{\n}\n\nstatic void bpf_skops_established(struct sock *sk, int bpf_op,\n\t\t\t\t  struct sk_buff *skb)\n{\n}\n#endif\n\nstatic void tcp_gro_dev_warn(struct sock *sk, const struct sk_buff *skb,\n\t\t\t     unsigned int len)\n{\n\tstatic bool __once __read_mostly;\n\n\tif (!__once) {\n\t\tstruct net_device *dev;\n\n\t\t__once = true;\n\n\t\trcu_read_lock();\n\t\tdev = dev_get_by_index_rcu(sock_net(sk), skb->skb_iif);\n\t\tif (!dev || len >= dev->mtu)\n\t\t\tpr_warn(\"%s: Driver has suspect GRO implementation, TCP performance may be compromised.\\n\",\n\t\t\t\tdev ? dev->name : \"Unknown driver\");\n\t\trcu_read_unlock();\n\t}\n}\n\n \nstatic void tcp_measure_rcv_mss(struct sock *sk, const struct sk_buff *skb)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tconst unsigned int lss = icsk->icsk_ack.last_seg_size;\n\tunsigned int len;\n\n\ticsk->icsk_ack.last_seg_size = 0;\n\n\t \n\tlen = skb_shinfo(skb)->gso_size ? : skb->len;\n\tif (len >= icsk->icsk_ack.rcv_mss) {\n\t\t \n\t\tif (unlikely(len != icsk->icsk_ack.rcv_mss)) {\n\t\t\tu64 val = (u64)skb->len << TCP_RMEM_TO_WIN_SCALE;\n\n\t\t\tdo_div(val, skb->truesize);\n\t\t\ttcp_sk(sk)->scaling_ratio = val ? val : 1;\n\t\t}\n\t\ticsk->icsk_ack.rcv_mss = min_t(unsigned int, len,\n\t\t\t\t\t       tcp_sk(sk)->advmss);\n\t\t \n\t\tif (unlikely(len > icsk->icsk_ack.rcv_mss +\n\t\t\t\t   MAX_TCP_OPTION_SPACE))\n\t\t\ttcp_gro_dev_warn(sk, skb, len);\n\t\t \n\t\tif (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_PSH)\n\t\t\ticsk->icsk_ack.pending |= ICSK_ACK_PUSHED;\n\t} else {\n\t\t \n\t\tlen += skb->data - skb_transport_header(skb);\n\t\tif (len >= TCP_MSS_DEFAULT + sizeof(struct tcphdr) ||\n\t\t     \n\t\t    (len >= TCP_MIN_MSS + sizeof(struct tcphdr) &&\n\t\t     !(tcp_flag_word(tcp_hdr(skb)) & TCP_REMNANT))) {\n\t\t\t \n\t\t\tlen -= tcp_sk(sk)->tcp_header_len;\n\t\t\ticsk->icsk_ack.last_seg_size = len;\n\t\t\tif (len == lss) {\n\t\t\t\ticsk->icsk_ack.rcv_mss = len;\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t\tif (icsk->icsk_ack.pending & ICSK_ACK_PUSHED)\n\t\t\ticsk->icsk_ack.pending |= ICSK_ACK_PUSHED2;\n\t\ticsk->icsk_ack.pending |= ICSK_ACK_PUSHED;\n\t}\n}\n\nstatic void tcp_incr_quickack(struct sock *sk, unsigned int max_quickacks)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tunsigned int quickacks = tcp_sk(sk)->rcv_wnd / (2 * icsk->icsk_ack.rcv_mss);\n\n\tif (quickacks == 0)\n\t\tquickacks = 2;\n\tquickacks = min(quickacks, max_quickacks);\n\tif (quickacks > icsk->icsk_ack.quick)\n\t\ticsk->icsk_ack.quick = quickacks;\n}\n\nstatic void tcp_enter_quickack_mode(struct sock *sk, unsigned int max_quickacks)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\ttcp_incr_quickack(sk, max_quickacks);\n\tinet_csk_exit_pingpong_mode(sk);\n\ticsk->icsk_ack.ato = TCP_ATO_MIN;\n}\n\n \n\nstatic bool tcp_in_quickack_mode(struct sock *sk)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tconst struct dst_entry *dst = __sk_dst_get(sk);\n\n\treturn (dst && dst_metric(dst, RTAX_QUICKACK)) ||\n\t\t(icsk->icsk_ack.quick && !inet_csk_in_pingpong_mode(sk));\n}\n\nstatic void tcp_ecn_queue_cwr(struct tcp_sock *tp)\n{\n\tif (tp->ecn_flags & TCP_ECN_OK)\n\t\ttp->ecn_flags |= TCP_ECN_QUEUE_CWR;\n}\n\nstatic void tcp_ecn_accept_cwr(struct sock *sk, const struct sk_buff *skb)\n{\n\tif (tcp_hdr(skb)->cwr) {\n\t\ttcp_sk(sk)->ecn_flags &= ~TCP_ECN_DEMAND_CWR;\n\n\t\t \n\t\tif (TCP_SKB_CB(skb)->seq != TCP_SKB_CB(skb)->end_seq)\n\t\t\tinet_csk(sk)->icsk_ack.pending |= ICSK_ACK_NOW;\n\t}\n}\n\nstatic void tcp_ecn_withdraw_cwr(struct tcp_sock *tp)\n{\n\ttp->ecn_flags &= ~TCP_ECN_QUEUE_CWR;\n}\n\nstatic void __tcp_ecn_check_ce(struct sock *sk, const struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tswitch (TCP_SKB_CB(skb)->ip_dsfield & INET_ECN_MASK) {\n\tcase INET_ECN_NOT_ECT:\n\t\t \n\t\tif (tp->ecn_flags & TCP_ECN_SEEN)\n\t\t\ttcp_enter_quickack_mode(sk, 2);\n\t\tbreak;\n\tcase INET_ECN_CE:\n\t\tif (tcp_ca_needs_ecn(sk))\n\t\t\ttcp_ca_event(sk, CA_EVENT_ECN_IS_CE);\n\n\t\tif (!(tp->ecn_flags & TCP_ECN_DEMAND_CWR)) {\n\t\t\t \n\t\t\ttcp_enter_quickack_mode(sk, 2);\n\t\t\ttp->ecn_flags |= TCP_ECN_DEMAND_CWR;\n\t\t}\n\t\ttp->ecn_flags |= TCP_ECN_SEEN;\n\t\tbreak;\n\tdefault:\n\t\tif (tcp_ca_needs_ecn(sk))\n\t\t\ttcp_ca_event(sk, CA_EVENT_ECN_NO_CE);\n\t\ttp->ecn_flags |= TCP_ECN_SEEN;\n\t\tbreak;\n\t}\n}\n\nstatic void tcp_ecn_check_ce(struct sock *sk, const struct sk_buff *skb)\n{\n\tif (tcp_sk(sk)->ecn_flags & TCP_ECN_OK)\n\t\t__tcp_ecn_check_ce(sk, skb);\n}\n\nstatic void tcp_ecn_rcv_synack(struct tcp_sock *tp, const struct tcphdr *th)\n{\n\tif ((tp->ecn_flags & TCP_ECN_OK) && (!th->ece || th->cwr))\n\t\ttp->ecn_flags &= ~TCP_ECN_OK;\n}\n\nstatic void tcp_ecn_rcv_syn(struct tcp_sock *tp, const struct tcphdr *th)\n{\n\tif ((tp->ecn_flags & TCP_ECN_OK) && (!th->ece || !th->cwr))\n\t\ttp->ecn_flags &= ~TCP_ECN_OK;\n}\n\nstatic bool tcp_ecn_rcv_ecn_echo(const struct tcp_sock *tp, const struct tcphdr *th)\n{\n\tif (th->ece && !th->syn && (tp->ecn_flags & TCP_ECN_OK))\n\t\treturn true;\n\treturn false;\n}\n\n \n\nstatic void tcp_sndbuf_expand(struct sock *sk)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tconst struct tcp_congestion_ops *ca_ops = inet_csk(sk)->icsk_ca_ops;\n\tint sndmem, per_mss;\n\tu32 nr_segs;\n\n\t \n\tper_mss = max_t(u32, tp->rx_opt.mss_clamp, tp->mss_cache) +\n\t\t  MAX_TCP_HEADER +\n\t\t  SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\n\tper_mss = roundup_pow_of_two(per_mss) +\n\t\t  SKB_DATA_ALIGN(sizeof(struct sk_buff));\n\n\tnr_segs = max_t(u32, TCP_INIT_CWND, tcp_snd_cwnd(tp));\n\tnr_segs = max_t(u32, nr_segs, tp->reordering + 1);\n\n\t \n\tsndmem = ca_ops->sndbuf_expand ? ca_ops->sndbuf_expand(sk) : 2;\n\tsndmem *= nr_segs * per_mss;\n\n\tif (sk->sk_sndbuf < sndmem)\n\t\tWRITE_ONCE(sk->sk_sndbuf,\n\t\t\t   min(sndmem, READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_wmem[2])));\n}\n\n \n\n \nstatic int __tcp_grow_window(const struct sock *sk, const struct sk_buff *skb,\n\t\t\t     unsigned int skbtruesize)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\t \n\tint truesize = tcp_win_from_space(sk, skbtruesize) >> 1;\n\tint window = tcp_win_from_space(sk, READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_rmem[2])) >> 1;\n\n\twhile (tp->rcv_ssthresh <= window) {\n\t\tif (truesize <= skb->len)\n\t\t\treturn 2 * inet_csk(sk)->icsk_ack.rcv_mss;\n\n\t\ttruesize >>= 1;\n\t\twindow >>= 1;\n\t}\n\treturn 0;\n}\n\n \nstatic u32 truesize_adjust(bool adjust, const struct sk_buff *skb)\n{\n\tu32 truesize = skb->truesize;\n\n\tif (adjust && !skb_headlen(skb)) {\n\t\ttruesize -= SKB_TRUESIZE(skb_end_offset(skb));\n\t\t \n\t\tif (unlikely((int)truesize < (int)skb->len))\n\t\t\ttruesize = skb->truesize;\n\t}\n\treturn truesize;\n}\n\nstatic void tcp_grow_window(struct sock *sk, const struct sk_buff *skb,\n\t\t\t    bool adjust)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint room;\n\n\troom = min_t(int, tp->window_clamp, tcp_space(sk)) - tp->rcv_ssthresh;\n\n\tif (room <= 0)\n\t\treturn;\n\n\t \n\tif (!tcp_under_memory_pressure(sk)) {\n\t\tunsigned int truesize = truesize_adjust(adjust, skb);\n\t\tint incr;\n\n\t\t \n\t\tif (tcp_win_from_space(sk, truesize) <= skb->len)\n\t\t\tincr = 2 * tp->advmss;\n\t\telse\n\t\t\tincr = __tcp_grow_window(sk, skb, truesize);\n\n\t\tif (incr) {\n\t\t\tincr = max_t(int, incr, 2 * skb->len);\n\t\t\ttp->rcv_ssthresh += min(room, incr);\n\t\t\tinet_csk(sk)->icsk_ack.quick |= 1;\n\t\t}\n\t} else {\n\t\t \n\t\ttcp_adjust_rcv_ssthresh(sk);\n\t}\n}\n\n \nstatic void tcp_init_buffer_space(struct sock *sk)\n{\n\tint tcp_app_win = READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_app_win);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint maxwin;\n\n\tif (!(sk->sk_userlocks & SOCK_SNDBUF_LOCK))\n\t\ttcp_sndbuf_expand(sk);\n\n\ttcp_mstamp_refresh(tp);\n\ttp->rcvq_space.time = tp->tcp_mstamp;\n\ttp->rcvq_space.seq = tp->copied_seq;\n\n\tmaxwin = tcp_full_space(sk);\n\n\tif (tp->window_clamp >= maxwin) {\n\t\ttp->window_clamp = maxwin;\n\n\t\tif (tcp_app_win && maxwin > 4 * tp->advmss)\n\t\t\ttp->window_clamp = max(maxwin -\n\t\t\t\t\t       (maxwin >> tcp_app_win),\n\t\t\t\t\t       4 * tp->advmss);\n\t}\n\n\t \n\tif (tcp_app_win &&\n\t    tp->window_clamp > 2 * tp->advmss &&\n\t    tp->window_clamp + tp->advmss > maxwin)\n\t\ttp->window_clamp = max(2 * tp->advmss, maxwin - tp->advmss);\n\n\ttp->rcv_ssthresh = min(tp->rcv_ssthresh, tp->window_clamp);\n\ttp->snd_cwnd_stamp = tcp_jiffies32;\n\ttp->rcvq_space.space = min3(tp->rcv_ssthresh, tp->rcv_wnd,\n\t\t\t\t    (u32)TCP_INIT_CWND * tp->advmss);\n}\n\n \nstatic void tcp_clamp_window(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct net *net = sock_net(sk);\n\tint rmem2;\n\n\ticsk->icsk_ack.quick = 0;\n\trmem2 = READ_ONCE(net->ipv4.sysctl_tcp_rmem[2]);\n\n\tif (sk->sk_rcvbuf < rmem2 &&\n\t    !(sk->sk_userlocks & SOCK_RCVBUF_LOCK) &&\n\t    !tcp_under_memory_pressure(sk) &&\n\t    sk_memory_allocated(sk) < sk_prot_mem_limits(sk, 0)) {\n\t\tWRITE_ONCE(sk->sk_rcvbuf,\n\t\t\t   min(atomic_read(&sk->sk_rmem_alloc), rmem2));\n\t}\n\tif (atomic_read(&sk->sk_rmem_alloc) > sk->sk_rcvbuf)\n\t\ttp->rcv_ssthresh = min(tp->window_clamp, 2U * tp->advmss);\n}\n\n \nvoid tcp_initialize_rcv_mss(struct sock *sk)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tunsigned int hint = min_t(unsigned int, tp->advmss, tp->mss_cache);\n\n\thint = min(hint, tp->rcv_wnd / 2);\n\thint = min(hint, TCP_MSS_DEFAULT);\n\thint = max(hint, TCP_MIN_MSS);\n\n\tinet_csk(sk)->icsk_ack.rcv_mss = hint;\n}\nEXPORT_SYMBOL(tcp_initialize_rcv_mss);\n\n \nstatic void tcp_rcv_rtt_update(struct tcp_sock *tp, u32 sample, int win_dep)\n{\n\tu32 new_sample = tp->rcv_rtt_est.rtt_us;\n\tlong m = sample;\n\n\tif (new_sample != 0) {\n\t\t \n\t\tif (!win_dep) {\n\t\t\tm -= (new_sample >> 3);\n\t\t\tnew_sample += m;\n\t\t} else {\n\t\t\tm <<= 3;\n\t\t\tif (m < new_sample)\n\t\t\t\tnew_sample = m;\n\t\t}\n\t} else {\n\t\t \n\t\tnew_sample = m << 3;\n\t}\n\n\ttp->rcv_rtt_est.rtt_us = new_sample;\n}\n\nstatic inline void tcp_rcv_rtt_measure(struct tcp_sock *tp)\n{\n\tu32 delta_us;\n\n\tif (tp->rcv_rtt_est.time == 0)\n\t\tgoto new_measure;\n\tif (before(tp->rcv_nxt, tp->rcv_rtt_est.seq))\n\t\treturn;\n\tdelta_us = tcp_stamp_us_delta(tp->tcp_mstamp, tp->rcv_rtt_est.time);\n\tif (!delta_us)\n\t\tdelta_us = 1;\n\ttcp_rcv_rtt_update(tp, delta_us, 1);\n\nnew_measure:\n\ttp->rcv_rtt_est.seq = tp->rcv_nxt + tp->rcv_wnd;\n\ttp->rcv_rtt_est.time = tp->tcp_mstamp;\n}\n\nstatic inline void tcp_rcv_rtt_measure_ts(struct sock *sk,\n\t\t\t\t\t  const struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (tp->rx_opt.rcv_tsecr == tp->rcv_rtt_last_tsecr)\n\t\treturn;\n\ttp->rcv_rtt_last_tsecr = tp->rx_opt.rcv_tsecr;\n\n\tif (TCP_SKB_CB(skb)->end_seq -\n\t    TCP_SKB_CB(skb)->seq >= inet_csk(sk)->icsk_ack.rcv_mss) {\n\t\tu32 delta = tcp_time_stamp(tp) - tp->rx_opt.rcv_tsecr;\n\t\tu32 delta_us;\n\n\t\tif (likely(delta < INT_MAX / (USEC_PER_SEC / TCP_TS_HZ))) {\n\t\t\tif (!delta)\n\t\t\t\tdelta = 1;\n\t\t\tdelta_us = delta * (USEC_PER_SEC / TCP_TS_HZ);\n\t\t\ttcp_rcv_rtt_update(tp, delta_us, 0);\n\t\t}\n\t}\n}\n\n \nvoid tcp_rcv_space_adjust(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 copied;\n\tint time;\n\n\ttrace_tcp_rcv_space_adjust(sk);\n\n\ttcp_mstamp_refresh(tp);\n\ttime = tcp_stamp_us_delta(tp->tcp_mstamp, tp->rcvq_space.time);\n\tif (time < (tp->rcv_rtt_est.rtt_us >> 3) || tp->rcv_rtt_est.rtt_us == 0)\n\t\treturn;\n\n\t \n\tcopied = tp->copied_seq - tp->rcvq_space.seq;\n\tif (copied <= tp->rcvq_space.space)\n\t\tgoto new_measure;\n\n\t \n\n\tif (READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_moderate_rcvbuf) &&\n\t    !(sk->sk_userlocks & SOCK_RCVBUF_LOCK)) {\n\t\tu64 rcvwin, grow;\n\t\tint rcvbuf;\n\n\t\t \n\t\trcvwin = ((u64)copied << 1) + 16 * tp->advmss;\n\n\t\t \n\t\tgrow = rcvwin * (copied - tp->rcvq_space.space);\n\t\tdo_div(grow, tp->rcvq_space.space);\n\t\trcvwin += (grow << 1);\n\n\t\trcvbuf = min_t(u64, tcp_space_from_win(sk, rcvwin),\n\t\t\t       READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_rmem[2]));\n\t\tif (rcvbuf > sk->sk_rcvbuf) {\n\t\t\tWRITE_ONCE(sk->sk_rcvbuf, rcvbuf);\n\n\t\t\t \n\t\t\ttp->window_clamp = tcp_win_from_space(sk, rcvbuf);\n\t\t}\n\t}\n\ttp->rcvq_space.space = copied;\n\nnew_measure:\n\ttp->rcvq_space.seq = tp->copied_seq;\n\ttp->rcvq_space.time = tp->tcp_mstamp;\n}\n\n \nstatic void tcp_event_data_recv(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tu32 now;\n\n\tinet_csk_schedule_ack(sk);\n\n\ttcp_measure_rcv_mss(sk, skb);\n\n\ttcp_rcv_rtt_measure(tp);\n\n\tnow = tcp_jiffies32;\n\n\tif (!icsk->icsk_ack.ato) {\n\t\t \n\t\ttcp_incr_quickack(sk, TCP_MAX_QUICKACKS);\n\t\ticsk->icsk_ack.ato = TCP_ATO_MIN;\n\t} else {\n\t\tint m = now - icsk->icsk_ack.lrcvtime;\n\n\t\tif (m <= TCP_ATO_MIN / 2) {\n\t\t\t \n\t\t\ticsk->icsk_ack.ato = (icsk->icsk_ack.ato >> 1) + TCP_ATO_MIN / 2;\n\t\t} else if (m < icsk->icsk_ack.ato) {\n\t\t\ticsk->icsk_ack.ato = (icsk->icsk_ack.ato >> 1) + m;\n\t\t\tif (icsk->icsk_ack.ato > icsk->icsk_rto)\n\t\t\t\ticsk->icsk_ack.ato = icsk->icsk_rto;\n\t\t} else if (m > icsk->icsk_rto) {\n\t\t\t \n\t\t\ttcp_incr_quickack(sk, TCP_MAX_QUICKACKS);\n\t\t}\n\t}\n\ticsk->icsk_ack.lrcvtime = now;\n\n\ttcp_ecn_check_ce(sk, skb);\n\n\tif (skb->len >= 128)\n\t\ttcp_grow_window(sk, skb, true);\n}\n\n \nstatic void tcp_rtt_estimator(struct sock *sk, long mrtt_us)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tlong m = mrtt_us;  \n\tu32 srtt = tp->srtt_us;\n\n\t \n\tif (srtt != 0) {\n\t\tm -= (srtt >> 3);\t \n\t\tsrtt += m;\t\t \n\t\tif (m < 0) {\n\t\t\tm = -m;\t\t \n\t\t\tm -= (tp->mdev_us >> 2);    \n\t\t\t \n\t\t\tif (m > 0)\n\t\t\t\tm >>= 3;\n\t\t} else {\n\t\t\tm -= (tp->mdev_us >> 2);    \n\t\t}\n\t\ttp->mdev_us += m;\t\t \n\t\tif (tp->mdev_us > tp->mdev_max_us) {\n\t\t\ttp->mdev_max_us = tp->mdev_us;\n\t\t\tif (tp->mdev_max_us > tp->rttvar_us)\n\t\t\t\ttp->rttvar_us = tp->mdev_max_us;\n\t\t}\n\t\tif (after(tp->snd_una, tp->rtt_seq)) {\n\t\t\tif (tp->mdev_max_us < tp->rttvar_us)\n\t\t\t\ttp->rttvar_us -= (tp->rttvar_us - tp->mdev_max_us) >> 2;\n\t\t\ttp->rtt_seq = tp->snd_nxt;\n\t\t\ttp->mdev_max_us = tcp_rto_min_us(sk);\n\n\t\t\ttcp_bpf_rtt(sk);\n\t\t}\n\t} else {\n\t\t \n\t\tsrtt = m << 3;\t\t \n\t\ttp->mdev_us = m << 1;\t \n\t\ttp->rttvar_us = max(tp->mdev_us, tcp_rto_min_us(sk));\n\t\ttp->mdev_max_us = tp->rttvar_us;\n\t\ttp->rtt_seq = tp->snd_nxt;\n\n\t\ttcp_bpf_rtt(sk);\n\t}\n\ttp->srtt_us = max(1U, srtt);\n}\n\nstatic void tcp_update_pacing_rate(struct sock *sk)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tu64 rate;\n\n\t \n\trate = (u64)tp->mss_cache * ((USEC_PER_SEC / 100) << 3);\n\n\t \n\tif (tcp_snd_cwnd(tp) < tp->snd_ssthresh / 2)\n\t\trate *= READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_pacing_ss_ratio);\n\telse\n\t\trate *= READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_pacing_ca_ratio);\n\n\trate *= max(tcp_snd_cwnd(tp), tp->packets_out);\n\n\tif (likely(tp->srtt_us))\n\t\tdo_div(rate, tp->srtt_us);\n\n\t \n\tWRITE_ONCE(sk->sk_pacing_rate, min_t(u64, rate,\n\t\t\t\t\t     sk->sk_max_pacing_rate));\n}\n\n \nstatic void tcp_set_rto(struct sock *sk)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\t \n\tinet_csk(sk)->icsk_rto = __tcp_set_rto(tp);\n\n\t \n\n\t \n\ttcp_bound_rto(sk);\n}\n\n__u32 tcp_init_cwnd(const struct tcp_sock *tp, const struct dst_entry *dst)\n{\n\t__u32 cwnd = (dst ? dst_metric(dst, RTAX_INITCWND) : 0);\n\n\tif (!cwnd)\n\t\tcwnd = TCP_INIT_CWND;\n\treturn min_t(__u32, cwnd, tp->snd_cwnd_clamp);\n}\n\nstruct tcp_sacktag_state {\n\t \n\tu64\tfirst_sackt;\n\tu64\tlast_sackt;\n\tu32\treord;\n\tu32\tsack_delivered;\n\tint\tflag;\n\tunsigned int mss_now;\n\tstruct rate_sample *rate;\n};\n\n \nstatic u32 tcp_dsack_seen(struct tcp_sock *tp, u32 start_seq,\n\t\t\t  u32 end_seq, struct tcp_sacktag_state *state)\n{\n\tu32 seq_len, dup_segs = 1;\n\n\tif (!before(start_seq, end_seq))\n\t\treturn 0;\n\n\tseq_len = end_seq - start_seq;\n\t \n\tif (seq_len > tp->max_window)\n\t\treturn 0;\n\tif (seq_len > tp->mss_cache)\n\t\tdup_segs = DIV_ROUND_UP(seq_len, tp->mss_cache);\n\telse if (tp->tlp_high_seq && tp->tlp_high_seq == end_seq)\n\t\tstate->flag |= FLAG_DSACK_TLP;\n\n\ttp->dsack_dups += dup_segs;\n\t \n\tif (tp->dsack_dups > tp->total_retrans)\n\t\treturn 0;\n\n\ttp->rx_opt.sack_ok |= TCP_DSACK_SEEN;\n\t \n\tif (tp->reord_seen && !(state->flag & FLAG_DSACK_TLP))\n\t\ttp->rack.dsack_seen = 1;\n\n\tstate->flag |= FLAG_DSACKING_ACK;\n\t \n\tstate->sack_delivered += dup_segs;\n\n\treturn dup_segs;\n}\n\n \nstatic void tcp_check_sack_reordering(struct sock *sk, const u32 low_seq,\n\t\t\t\t      const int ts)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tconst u32 mss = tp->mss_cache;\n\tu32 fack, metric;\n\n\tfack = tcp_highest_sack_seq(tp);\n\tif (!before(low_seq, fack))\n\t\treturn;\n\n\tmetric = fack - low_seq;\n\tif ((metric > tp->reordering * mss) && mss) {\n#if FASTRETRANS_DEBUG > 1\n\t\tpr_debug(\"Disorder%d %d %u f%u s%u rr%d\\n\",\n\t\t\t tp->rx_opt.sack_ok, inet_csk(sk)->icsk_ca_state,\n\t\t\t tp->reordering,\n\t\t\t 0,\n\t\t\t tp->sacked_out,\n\t\t\t tp->undo_marker ? tp->undo_retrans : 0);\n#endif\n\t\ttp->reordering = min_t(u32, (metric + mss - 1) / mss,\n\t\t\t\t       READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_max_reordering));\n\t}\n\n\t \n\ttp->reord_seen++;\n\tNET_INC_STATS(sock_net(sk),\n\t\t      ts ? LINUX_MIB_TCPTSREORDER : LINUX_MIB_TCPSACKREORDER);\n}\n\n  \nstatic void tcp_verify_retransmit_hint(struct tcp_sock *tp, struct sk_buff *skb)\n{\n\tif ((!tp->retransmit_skb_hint && tp->retrans_out >= tp->lost_out) ||\n\t    (tp->retransmit_skb_hint &&\n\t     before(TCP_SKB_CB(skb)->seq,\n\t\t    TCP_SKB_CB(tp->retransmit_skb_hint)->seq)))\n\t\ttp->retransmit_skb_hint = skb;\n}\n\n \nstatic void tcp_notify_skb_loss_event(struct tcp_sock *tp, const struct sk_buff *skb)\n{\n\ttp->lost += tcp_skb_pcount(skb);\n}\n\nvoid tcp_mark_skb_lost(struct sock *sk, struct sk_buff *skb)\n{\n\t__u8 sacked = TCP_SKB_CB(skb)->sacked;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (sacked & TCPCB_SACKED_ACKED)\n\t\treturn;\n\n\ttcp_verify_retransmit_hint(tp, skb);\n\tif (sacked & TCPCB_LOST) {\n\t\tif (sacked & TCPCB_SACKED_RETRANS) {\n\t\t\t \n\t\t\tTCP_SKB_CB(skb)->sacked &= ~TCPCB_SACKED_RETRANS;\n\t\t\ttp->retrans_out -= tcp_skb_pcount(skb);\n\t\t\tNET_ADD_STATS(sock_net(sk), LINUX_MIB_TCPLOSTRETRANSMIT,\n\t\t\t\t      tcp_skb_pcount(skb));\n\t\t\ttcp_notify_skb_loss_event(tp, skb);\n\t\t}\n\t} else {\n\t\ttp->lost_out += tcp_skb_pcount(skb);\n\t\tTCP_SKB_CB(skb)->sacked |= TCPCB_LOST;\n\t\ttcp_notify_skb_loss_event(tp, skb);\n\t}\n}\n\n \nstatic void tcp_count_delivered(struct tcp_sock *tp, u32 delivered,\n\t\t\t\tbool ece_ack)\n{\n\ttp->delivered += delivered;\n\tif (ece_ack)\n\t\ttp->delivered_ce += delivered;\n}\n\n \nstatic bool tcp_is_sackblock_valid(struct tcp_sock *tp, bool is_dsack,\n\t\t\t\t   u32 start_seq, u32 end_seq)\n{\n\t \n\tif (after(end_seq, tp->snd_nxt) || !before(start_seq, end_seq))\n\t\treturn false;\n\n\t \n\tif (!before(start_seq, tp->snd_nxt))\n\t\treturn false;\n\n\t \n\tif (after(start_seq, tp->snd_una))\n\t\treturn true;\n\n\tif (!is_dsack || !tp->undo_marker)\n\t\treturn false;\n\n\t \n\tif (after(end_seq, tp->snd_una))\n\t\treturn false;\n\n\tif (!before(start_seq, tp->undo_marker))\n\t\treturn true;\n\n\t \n\tif (!after(end_seq, tp->undo_marker))\n\t\treturn false;\n\n\t \n\treturn !before(start_seq, end_seq - tp->max_window);\n}\n\nstatic bool tcp_check_dsack(struct sock *sk, const struct sk_buff *ack_skb,\n\t\t\t    struct tcp_sack_block_wire *sp, int num_sacks,\n\t\t\t    u32 prior_snd_una, struct tcp_sacktag_state *state)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 start_seq_0 = get_unaligned_be32(&sp[0].start_seq);\n\tu32 end_seq_0 = get_unaligned_be32(&sp[0].end_seq);\n\tu32 dup_segs;\n\n\tif (before(start_seq_0, TCP_SKB_CB(ack_skb)->ack_seq)) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPDSACKRECV);\n\t} else if (num_sacks > 1) {\n\t\tu32 end_seq_1 = get_unaligned_be32(&sp[1].end_seq);\n\t\tu32 start_seq_1 = get_unaligned_be32(&sp[1].start_seq);\n\n\t\tif (after(end_seq_0, end_seq_1) || before(start_seq_0, start_seq_1))\n\t\t\treturn false;\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPDSACKOFORECV);\n\t} else {\n\t\treturn false;\n\t}\n\n\tdup_segs = tcp_dsack_seen(tp, start_seq_0, end_seq_0, state);\n\tif (!dup_segs) {\t \n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPDSACKIGNOREDDUBIOUS);\n\t\treturn false;\n\t}\n\n\tNET_ADD_STATS(sock_net(sk), LINUX_MIB_TCPDSACKRECVSEGS, dup_segs);\n\n\t \n\tif (tp->undo_marker && tp->undo_retrans > 0 &&\n\t    !after(end_seq_0, prior_snd_una) &&\n\t    after(end_seq_0, tp->undo_marker))\n\t\ttp->undo_retrans = max_t(int, 0, tp->undo_retrans - dup_segs);\n\n\treturn true;\n}\n\n \nstatic int tcp_match_skb_to_sack(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t  u32 start_seq, u32 end_seq)\n{\n\tint err;\n\tbool in_sack;\n\tunsigned int pkt_len;\n\tunsigned int mss;\n\n\tin_sack = !after(start_seq, TCP_SKB_CB(skb)->seq) &&\n\t\t  !before(end_seq, TCP_SKB_CB(skb)->end_seq);\n\n\tif (tcp_skb_pcount(skb) > 1 && !in_sack &&\n\t    after(TCP_SKB_CB(skb)->end_seq, start_seq)) {\n\t\tmss = tcp_skb_mss(skb);\n\t\tin_sack = !after(start_seq, TCP_SKB_CB(skb)->seq);\n\n\t\tif (!in_sack) {\n\t\t\tpkt_len = start_seq - TCP_SKB_CB(skb)->seq;\n\t\t\tif (pkt_len < mss)\n\t\t\t\tpkt_len = mss;\n\t\t} else {\n\t\t\tpkt_len = end_seq - TCP_SKB_CB(skb)->seq;\n\t\t\tif (pkt_len < mss)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t \n\t\tif (pkt_len > mss) {\n\t\t\tunsigned int new_len = (pkt_len / mss) * mss;\n\t\t\tif (!in_sack && new_len < pkt_len)\n\t\t\t\tnew_len += mss;\n\t\t\tpkt_len = new_len;\n\t\t}\n\n\t\tif (pkt_len >= skb->len && !in_sack)\n\t\t\treturn 0;\n\n\t\terr = tcp_fragment(sk, TCP_FRAG_IN_RTX_QUEUE, skb,\n\t\t\t\t   pkt_len, mss, GFP_ATOMIC);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\n\treturn in_sack;\n}\n\n \nstatic u8 tcp_sacktag_one(struct sock *sk,\n\t\t\t  struct tcp_sacktag_state *state, u8 sacked,\n\t\t\t  u32 start_seq, u32 end_seq,\n\t\t\t  int dup_sack, int pcount,\n\t\t\t  u64 xmit_time)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t \n\tif (dup_sack && (sacked & TCPCB_RETRANS)) {\n\t\tif (tp->undo_marker && tp->undo_retrans > 0 &&\n\t\t    after(end_seq, tp->undo_marker))\n\t\t\ttp->undo_retrans = max_t(int, 0, tp->undo_retrans - pcount);\n\t\tif ((sacked & TCPCB_SACKED_ACKED) &&\n\t\t    before(start_seq, state->reord))\n\t\t\t\tstate->reord = start_seq;\n\t}\n\n\t \n\tif (!after(end_seq, tp->snd_una))\n\t\treturn sacked;\n\n\tif (!(sacked & TCPCB_SACKED_ACKED)) {\n\t\ttcp_rack_advance(tp, sacked, end_seq, xmit_time);\n\n\t\tif (sacked & TCPCB_SACKED_RETRANS) {\n\t\t\t \n\t\t\tif (sacked & TCPCB_LOST) {\n\t\t\t\tsacked &= ~(TCPCB_LOST|TCPCB_SACKED_RETRANS);\n\t\t\t\ttp->lost_out -= pcount;\n\t\t\t\ttp->retrans_out -= pcount;\n\t\t\t}\n\t\t} else {\n\t\t\tif (!(sacked & TCPCB_RETRANS)) {\n\t\t\t\t \n\t\t\t\tif (before(start_seq,\n\t\t\t\t\t   tcp_highest_sack_seq(tp)) &&\n\t\t\t\t    before(start_seq, state->reord))\n\t\t\t\t\tstate->reord = start_seq;\n\n\t\t\t\tif (!after(end_seq, tp->high_seq))\n\t\t\t\t\tstate->flag |= FLAG_ORIG_SACK_ACKED;\n\t\t\t\tif (state->first_sackt == 0)\n\t\t\t\t\tstate->first_sackt = xmit_time;\n\t\t\t\tstate->last_sackt = xmit_time;\n\t\t\t}\n\n\t\t\tif (sacked & TCPCB_LOST) {\n\t\t\t\tsacked &= ~TCPCB_LOST;\n\t\t\t\ttp->lost_out -= pcount;\n\t\t\t}\n\t\t}\n\n\t\tsacked |= TCPCB_SACKED_ACKED;\n\t\tstate->flag |= FLAG_DATA_SACKED;\n\t\ttp->sacked_out += pcount;\n\t\t \n\t\tstate->sack_delivered += pcount;\n\n\t\t \n\t\tif (tp->lost_skb_hint &&\n\t\t    before(start_seq, TCP_SKB_CB(tp->lost_skb_hint)->seq))\n\t\t\ttp->lost_cnt_hint += pcount;\n\t}\n\n\t \n\tif (dup_sack && (sacked & TCPCB_SACKED_RETRANS)) {\n\t\tsacked &= ~TCPCB_SACKED_RETRANS;\n\t\ttp->retrans_out -= pcount;\n\t}\n\n\treturn sacked;\n}\n\n \nstatic bool tcp_shifted_skb(struct sock *sk, struct sk_buff *prev,\n\t\t\t    struct sk_buff *skb,\n\t\t\t    struct tcp_sacktag_state *state,\n\t\t\t    unsigned int pcount, int shifted, int mss,\n\t\t\t    bool dup_sack)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 start_seq = TCP_SKB_CB(skb)->seq;\t \n\tu32 end_seq = start_seq + shifted;\t \n\n\tBUG_ON(!pcount);\n\n\t \n\ttcp_sacktag_one(sk, state, TCP_SKB_CB(skb)->sacked,\n\t\t\tstart_seq, end_seq, dup_sack, pcount,\n\t\t\ttcp_skb_timestamp_us(skb));\n\ttcp_rate_skb_delivered(sk, skb, state->rate);\n\n\tif (skb == tp->lost_skb_hint)\n\t\ttp->lost_cnt_hint += pcount;\n\n\tTCP_SKB_CB(prev)->end_seq += shifted;\n\tTCP_SKB_CB(skb)->seq += shifted;\n\n\ttcp_skb_pcount_add(prev, pcount);\n\tWARN_ON_ONCE(tcp_skb_pcount(skb) < pcount);\n\ttcp_skb_pcount_add(skb, -pcount);\n\n\t \n\tif (!TCP_SKB_CB(prev)->tcp_gso_size)\n\t\tTCP_SKB_CB(prev)->tcp_gso_size = mss;\n\n\t \n\tif (tcp_skb_pcount(skb) <= 1)\n\t\tTCP_SKB_CB(skb)->tcp_gso_size = 0;\n\n\t \n\tTCP_SKB_CB(prev)->sacked |= (TCP_SKB_CB(skb)->sacked & TCPCB_EVER_RETRANS);\n\n\tif (skb->len > 0) {\n\t\tBUG_ON(!tcp_skb_pcount(skb));\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_SACKSHIFTED);\n\t\treturn false;\n\t}\n\n\t \n\n\tif (skb == tp->retransmit_skb_hint)\n\t\ttp->retransmit_skb_hint = prev;\n\tif (skb == tp->lost_skb_hint) {\n\t\ttp->lost_skb_hint = prev;\n\t\ttp->lost_cnt_hint -= tcp_skb_pcount(prev);\n\t}\n\n\tTCP_SKB_CB(prev)->tcp_flags |= TCP_SKB_CB(skb)->tcp_flags;\n\tTCP_SKB_CB(prev)->eor = TCP_SKB_CB(skb)->eor;\n\tif (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN)\n\t\tTCP_SKB_CB(prev)->end_seq++;\n\n\tif (skb == tcp_highest_sack(sk))\n\t\ttcp_advance_highest_sack(sk, skb);\n\n\ttcp_skb_collapse_tstamp(prev, skb);\n\tif (unlikely(TCP_SKB_CB(prev)->tx.delivered_mstamp))\n\t\tTCP_SKB_CB(prev)->tx.delivered_mstamp = 0;\n\n\ttcp_rtx_queue_unlink_and_free(skb, sk);\n\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_SACKMERGED);\n\n\treturn true;\n}\n\n \nstatic int tcp_skb_seglen(const struct sk_buff *skb)\n{\n\treturn tcp_skb_pcount(skb) == 1 ? skb->len : tcp_skb_mss(skb);\n}\n\n \nstatic int skb_can_shift(const struct sk_buff *skb)\n{\n\treturn !skb_headlen(skb) && skb_is_nonlinear(skb);\n}\n\nint tcp_skb_shift(struct sk_buff *to, struct sk_buff *from,\n\t\t  int pcount, int shiftlen)\n{\n\t \n\tif (unlikely(to->len + shiftlen >= 65535 * TCP_MIN_GSO_SIZE))\n\t\treturn 0;\n\tif (unlikely(tcp_skb_pcount(to) + pcount > 65535))\n\t\treturn 0;\n\treturn skb_shift(to, from, shiftlen);\n}\n\n \nstatic struct sk_buff *tcp_shift_skb_data(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t\t  struct tcp_sacktag_state *state,\n\t\t\t\t\t  u32 start_seq, u32 end_seq,\n\t\t\t\t\t  bool dup_sack)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *prev;\n\tint mss;\n\tint pcount = 0;\n\tint len;\n\tint in_sack;\n\n\t \n\tif (!dup_sack &&\n\t    (TCP_SKB_CB(skb)->sacked & (TCPCB_LOST|TCPCB_SACKED_RETRANS)) == TCPCB_SACKED_RETRANS)\n\t\tgoto fallback;\n\tif (!skb_can_shift(skb))\n\t\tgoto fallback;\n\t \n\tif (!after(TCP_SKB_CB(skb)->end_seq, tp->snd_una))\n\t\tgoto fallback;\n\n\t \n\tprev = skb_rb_prev(skb);\n\tif (!prev)\n\t\tgoto fallback;\n\n\tif ((TCP_SKB_CB(prev)->sacked & TCPCB_TAGBITS) != TCPCB_SACKED_ACKED)\n\t\tgoto fallback;\n\n\tif (!tcp_skb_can_collapse(prev, skb))\n\t\tgoto fallback;\n\n\tin_sack = !after(start_seq, TCP_SKB_CB(skb)->seq) &&\n\t\t  !before(end_seq, TCP_SKB_CB(skb)->end_seq);\n\n\tif (in_sack) {\n\t\tlen = skb->len;\n\t\tpcount = tcp_skb_pcount(skb);\n\t\tmss = tcp_skb_seglen(skb);\n\n\t\t \n\t\tif (mss != tcp_skb_seglen(prev))\n\t\t\tgoto fallback;\n\t} else {\n\t\tif (!after(TCP_SKB_CB(skb)->end_seq, start_seq))\n\t\t\tgoto noop;\n\t\t \n\t\tif (tcp_skb_pcount(skb) <= 1)\n\t\t\tgoto noop;\n\n\t\tin_sack = !after(start_seq, TCP_SKB_CB(skb)->seq);\n\t\tif (!in_sack) {\n\t\t\t \n\t\t\tgoto fallback;\n\t\t}\n\n\t\tlen = end_seq - TCP_SKB_CB(skb)->seq;\n\t\tBUG_ON(len < 0);\n\t\tBUG_ON(len > skb->len);\n\n\t\t \n\t\tmss = tcp_skb_mss(skb);\n\n\t\t \n\t\tif (mss != tcp_skb_seglen(prev))\n\t\t\tgoto fallback;\n\n\t\tif (len == mss) {\n\t\t\tpcount = 1;\n\t\t} else if (len < mss) {\n\t\t\tgoto noop;\n\t\t} else {\n\t\t\tpcount = len / mss;\n\t\t\tlen = pcount * mss;\n\t\t}\n\t}\n\n\t \n\tif (!after(TCP_SKB_CB(skb)->seq + len, tp->snd_una))\n\t\tgoto fallback;\n\n\tif (!tcp_skb_shift(prev, skb, pcount, len))\n\t\tgoto fallback;\n\tif (!tcp_shifted_skb(sk, prev, skb, state, pcount, len, mss, dup_sack))\n\t\tgoto out;\n\n\t \n\tskb = skb_rb_next(prev);\n\tif (!skb)\n\t\tgoto out;\n\n\tif (!skb_can_shift(skb) ||\n\t    ((TCP_SKB_CB(skb)->sacked & TCPCB_TAGBITS) != TCPCB_SACKED_ACKED) ||\n\t    (mss != tcp_skb_seglen(skb)))\n\t\tgoto out;\n\n\tif (!tcp_skb_can_collapse(prev, skb))\n\t\tgoto out;\n\tlen = skb->len;\n\tpcount = tcp_skb_pcount(skb);\n\tif (tcp_skb_shift(prev, skb, pcount, len))\n\t\ttcp_shifted_skb(sk, prev, skb, state, pcount,\n\t\t\t\tlen, mss, 0);\n\nout:\n\treturn prev;\n\nnoop:\n\treturn skb;\n\nfallback:\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_SACKSHIFTFALLBACK);\n\treturn NULL;\n}\n\nstatic struct sk_buff *tcp_sacktag_walk(struct sk_buff *skb, struct sock *sk,\n\t\t\t\t\tstruct tcp_sack_block *next_dup,\n\t\t\t\t\tstruct tcp_sacktag_state *state,\n\t\t\t\t\tu32 start_seq, u32 end_seq,\n\t\t\t\t\tbool dup_sack_in)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *tmp;\n\n\tskb_rbtree_walk_from(skb) {\n\t\tint in_sack = 0;\n\t\tbool dup_sack = dup_sack_in;\n\n\t\t \n\t\tif (!before(TCP_SKB_CB(skb)->seq, end_seq))\n\t\t\tbreak;\n\n\t\tif (next_dup  &&\n\t\t    before(TCP_SKB_CB(skb)->seq, next_dup->end_seq)) {\n\t\t\tin_sack = tcp_match_skb_to_sack(sk, skb,\n\t\t\t\t\t\t\tnext_dup->start_seq,\n\t\t\t\t\t\t\tnext_dup->end_seq);\n\t\t\tif (in_sack > 0)\n\t\t\t\tdup_sack = true;\n\t\t}\n\n\t\t \n\t\tif (in_sack <= 0) {\n\t\t\ttmp = tcp_shift_skb_data(sk, skb, state,\n\t\t\t\t\t\t start_seq, end_seq, dup_sack);\n\t\t\tif (tmp) {\n\t\t\t\tif (tmp != skb) {\n\t\t\t\t\tskb = tmp;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\tin_sack = 0;\n\t\t\t} else {\n\t\t\t\tin_sack = tcp_match_skb_to_sack(sk, skb,\n\t\t\t\t\t\t\t\tstart_seq,\n\t\t\t\t\t\t\t\tend_seq);\n\t\t\t}\n\t\t}\n\n\t\tif (unlikely(in_sack < 0))\n\t\t\tbreak;\n\n\t\tif (in_sack) {\n\t\t\tTCP_SKB_CB(skb)->sacked =\n\t\t\t\ttcp_sacktag_one(sk,\n\t\t\t\t\t\tstate,\n\t\t\t\t\t\tTCP_SKB_CB(skb)->sacked,\n\t\t\t\t\t\tTCP_SKB_CB(skb)->seq,\n\t\t\t\t\t\tTCP_SKB_CB(skb)->end_seq,\n\t\t\t\t\t\tdup_sack,\n\t\t\t\t\t\ttcp_skb_pcount(skb),\n\t\t\t\t\t\ttcp_skb_timestamp_us(skb));\n\t\t\ttcp_rate_skb_delivered(sk, skb, state->rate);\n\t\t\tif (TCP_SKB_CB(skb)->sacked & TCPCB_SACKED_ACKED)\n\t\t\t\tlist_del_init(&skb->tcp_tsorted_anchor);\n\n\t\t\tif (!before(TCP_SKB_CB(skb)->seq,\n\t\t\t\t    tcp_highest_sack_seq(tp)))\n\t\t\t\ttcp_advance_highest_sack(sk, skb);\n\t\t}\n\t}\n\treturn skb;\n}\n\nstatic struct sk_buff *tcp_sacktag_bsearch(struct sock *sk, u32 seq)\n{\n\tstruct rb_node *parent, **p = &sk->tcp_rtx_queue.rb_node;\n\tstruct sk_buff *skb;\n\n\twhile (*p) {\n\t\tparent = *p;\n\t\tskb = rb_to_skb(parent);\n\t\tif (before(seq, TCP_SKB_CB(skb)->seq)) {\n\t\t\tp = &parent->rb_left;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!before(seq, TCP_SKB_CB(skb)->end_seq)) {\n\t\t\tp = &parent->rb_right;\n\t\t\tcontinue;\n\t\t}\n\t\treturn skb;\n\t}\n\treturn NULL;\n}\n\nstatic struct sk_buff *tcp_sacktag_skip(struct sk_buff *skb, struct sock *sk,\n\t\t\t\t\tu32 skip_to_seq)\n{\n\tif (skb && after(TCP_SKB_CB(skb)->seq, skip_to_seq))\n\t\treturn skb;\n\n\treturn tcp_sacktag_bsearch(sk, skip_to_seq);\n}\n\nstatic struct sk_buff *tcp_maybe_skipping_dsack(struct sk_buff *skb,\n\t\t\t\t\t\tstruct sock *sk,\n\t\t\t\t\t\tstruct tcp_sack_block *next_dup,\n\t\t\t\t\t\tstruct tcp_sacktag_state *state,\n\t\t\t\t\t\tu32 skip_to_seq)\n{\n\tif (!next_dup)\n\t\treturn skb;\n\n\tif (before(next_dup->start_seq, skip_to_seq)) {\n\t\tskb = tcp_sacktag_skip(skb, sk, next_dup->start_seq);\n\t\tskb = tcp_sacktag_walk(skb, sk, NULL, state,\n\t\t\t\t       next_dup->start_seq, next_dup->end_seq,\n\t\t\t\t       1);\n\t}\n\n\treturn skb;\n}\n\nstatic int tcp_sack_cache_ok(const struct tcp_sock *tp, const struct tcp_sack_block *cache)\n{\n\treturn cache < tp->recv_sack_cache + ARRAY_SIZE(tp->recv_sack_cache);\n}\n\nstatic int\ntcp_sacktag_write_queue(struct sock *sk, const struct sk_buff *ack_skb,\n\t\t\tu32 prior_snd_una, struct tcp_sacktag_state *state)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tconst unsigned char *ptr = (skb_transport_header(ack_skb) +\n\t\t\t\t    TCP_SKB_CB(ack_skb)->sacked);\n\tstruct tcp_sack_block_wire *sp_wire = (struct tcp_sack_block_wire *)(ptr+2);\n\tstruct tcp_sack_block sp[TCP_NUM_SACKS];\n\tstruct tcp_sack_block *cache;\n\tstruct sk_buff *skb;\n\tint num_sacks = min(TCP_NUM_SACKS, (ptr[1] - TCPOLEN_SACK_BASE) >> 3);\n\tint used_sacks;\n\tbool found_dup_sack = false;\n\tint i, j;\n\tint first_sack_index;\n\n\tstate->flag = 0;\n\tstate->reord = tp->snd_nxt;\n\n\tif (!tp->sacked_out)\n\t\ttcp_highest_sack_reset(sk);\n\n\tfound_dup_sack = tcp_check_dsack(sk, ack_skb, sp_wire,\n\t\t\t\t\t num_sacks, prior_snd_una, state);\n\n\t \n\tif (before(TCP_SKB_CB(ack_skb)->ack_seq, prior_snd_una - tp->max_window))\n\t\treturn 0;\n\n\tif (!tp->packets_out)\n\t\tgoto out;\n\n\tused_sacks = 0;\n\tfirst_sack_index = 0;\n\tfor (i = 0; i < num_sacks; i++) {\n\t\tbool dup_sack = !i && found_dup_sack;\n\n\t\tsp[used_sacks].start_seq = get_unaligned_be32(&sp_wire[i].start_seq);\n\t\tsp[used_sacks].end_seq = get_unaligned_be32(&sp_wire[i].end_seq);\n\n\t\tif (!tcp_is_sackblock_valid(tp, dup_sack,\n\t\t\t\t\t    sp[used_sacks].start_seq,\n\t\t\t\t\t    sp[used_sacks].end_seq)) {\n\t\t\tint mib_idx;\n\n\t\t\tif (dup_sack) {\n\t\t\t\tif (!tp->undo_marker)\n\t\t\t\t\tmib_idx = LINUX_MIB_TCPDSACKIGNOREDNOUNDO;\n\t\t\t\telse\n\t\t\t\t\tmib_idx = LINUX_MIB_TCPDSACKIGNOREDOLD;\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tif ((TCP_SKB_CB(ack_skb)->ack_seq != tp->snd_una) &&\n\t\t\t\t    !after(sp[used_sacks].end_seq, tp->snd_una))\n\t\t\t\t\tcontinue;\n\t\t\t\tmib_idx = LINUX_MIB_TCPSACKDISCARD;\n\t\t\t}\n\n\t\t\tNET_INC_STATS(sock_net(sk), mib_idx);\n\t\t\tif (i == 0)\n\t\t\t\tfirst_sack_index = -1;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (!after(sp[used_sacks].end_seq, prior_snd_una)) {\n\t\t\tif (i == 0)\n\t\t\t\tfirst_sack_index = -1;\n\t\t\tcontinue;\n\t\t}\n\n\t\tused_sacks++;\n\t}\n\n\t \n\tfor (i = used_sacks - 1; i > 0; i--) {\n\t\tfor (j = 0; j < i; j++) {\n\t\t\tif (after(sp[j].start_seq, sp[j + 1].start_seq)) {\n\t\t\t\tswap(sp[j], sp[j + 1]);\n\n\t\t\t\t \n\t\t\t\tif (j == first_sack_index)\n\t\t\t\t\tfirst_sack_index = j + 1;\n\t\t\t}\n\t\t}\n\t}\n\n\tstate->mss_now = tcp_current_mss(sk);\n\tskb = NULL;\n\ti = 0;\n\n\tif (!tp->sacked_out) {\n\t\t \n\t\tcache = tp->recv_sack_cache + ARRAY_SIZE(tp->recv_sack_cache);\n\t} else {\n\t\tcache = tp->recv_sack_cache;\n\t\t \n\t\twhile (tcp_sack_cache_ok(tp, cache) && !cache->start_seq &&\n\t\t       !cache->end_seq)\n\t\t\tcache++;\n\t}\n\n\twhile (i < used_sacks) {\n\t\tu32 start_seq = sp[i].start_seq;\n\t\tu32 end_seq = sp[i].end_seq;\n\t\tbool dup_sack = (found_dup_sack && (i == first_sack_index));\n\t\tstruct tcp_sack_block *next_dup = NULL;\n\n\t\tif (found_dup_sack && ((i + 1) == first_sack_index))\n\t\t\tnext_dup = &sp[i + 1];\n\n\t\t \n\t\twhile (tcp_sack_cache_ok(tp, cache) &&\n\t\t       !before(start_seq, cache->end_seq))\n\t\t\tcache++;\n\n\t\t \n\t\tif (tcp_sack_cache_ok(tp, cache) && !dup_sack &&\n\t\t    after(end_seq, cache->start_seq)) {\n\n\t\t\t \n\t\t\tif (before(start_seq, cache->start_seq)) {\n\t\t\t\tskb = tcp_sacktag_skip(skb, sk, start_seq);\n\t\t\t\tskb = tcp_sacktag_walk(skb, sk, next_dup,\n\t\t\t\t\t\t       state,\n\t\t\t\t\t\t       start_seq,\n\t\t\t\t\t\t       cache->start_seq,\n\t\t\t\t\t\t       dup_sack);\n\t\t\t}\n\n\t\t\t \n\t\t\tif (!after(end_seq, cache->end_seq))\n\t\t\t\tgoto advance_sp;\n\n\t\t\tskb = tcp_maybe_skipping_dsack(skb, sk, next_dup,\n\t\t\t\t\t\t       state,\n\t\t\t\t\t\t       cache->end_seq);\n\n\t\t\t \n\t\t\tif (tcp_highest_sack_seq(tp) == cache->end_seq) {\n\t\t\t\t \n\t\t\t\tskb = tcp_highest_sack(sk);\n\t\t\t\tif (!skb)\n\t\t\t\t\tbreak;\n\t\t\t\tcache++;\n\t\t\t\tgoto walk;\n\t\t\t}\n\n\t\t\tskb = tcp_sacktag_skip(skb, sk, cache->end_seq);\n\t\t\t \n\t\t\tcache++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!before(start_seq, tcp_highest_sack_seq(tp))) {\n\t\t\tskb = tcp_highest_sack(sk);\n\t\t\tif (!skb)\n\t\t\t\tbreak;\n\t\t}\n\t\tskb = tcp_sacktag_skip(skb, sk, start_seq);\n\nwalk:\n\t\tskb = tcp_sacktag_walk(skb, sk, next_dup, state,\n\t\t\t\t       start_seq, end_seq, dup_sack);\n\nadvance_sp:\n\t\ti++;\n\t}\n\n\t \n\tfor (i = 0; i < ARRAY_SIZE(tp->recv_sack_cache) - used_sacks; i++) {\n\t\ttp->recv_sack_cache[i].start_seq = 0;\n\t\ttp->recv_sack_cache[i].end_seq = 0;\n\t}\n\tfor (j = 0; j < used_sacks; j++)\n\t\ttp->recv_sack_cache[i++] = sp[j];\n\n\tif (inet_csk(sk)->icsk_ca_state != TCP_CA_Loss || tp->undo_marker)\n\t\ttcp_check_sack_reordering(sk, state->reord, 0);\n\n\ttcp_verify_left_out(tp);\nout:\n\n#if FASTRETRANS_DEBUG > 0\n\tWARN_ON((int)tp->sacked_out < 0);\n\tWARN_ON((int)tp->lost_out < 0);\n\tWARN_ON((int)tp->retrans_out < 0);\n\tWARN_ON((int)tcp_packets_in_flight(tp) < 0);\n#endif\n\treturn state->flag;\n}\n\n \nstatic bool tcp_limit_reno_sacked(struct tcp_sock *tp)\n{\n\tu32 holes;\n\n\tholes = max(tp->lost_out, 1U);\n\tholes = min(holes, tp->packets_out);\n\n\tif ((tp->sacked_out + holes) > tp->packets_out) {\n\t\ttp->sacked_out = tp->packets_out - holes;\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n \nstatic void tcp_check_reno_reordering(struct sock *sk, const int addend)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (!tcp_limit_reno_sacked(tp))\n\t\treturn;\n\n\ttp->reordering = min_t(u32, tp->packets_out + addend,\n\t\t\t       READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_max_reordering));\n\ttp->reord_seen++;\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPRENOREORDER);\n}\n\n \n\nstatic void tcp_add_reno_sack(struct sock *sk, int num_dupack, bool ece_ack)\n{\n\tif (num_dupack) {\n\t\tstruct tcp_sock *tp = tcp_sk(sk);\n\t\tu32 prior_sacked = tp->sacked_out;\n\t\ts32 delivered;\n\n\t\ttp->sacked_out += num_dupack;\n\t\ttcp_check_reno_reordering(sk, 0);\n\t\tdelivered = tp->sacked_out - prior_sacked;\n\t\tif (delivered > 0)\n\t\t\ttcp_count_delivered(tp, delivered, ece_ack);\n\t\ttcp_verify_left_out(tp);\n\t}\n}\n\n \n\nstatic void tcp_remove_reno_sacks(struct sock *sk, int acked, bool ece_ack)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (acked > 0) {\n\t\t \n\t\ttcp_count_delivered(tp, max_t(int, acked - tp->sacked_out, 1),\n\t\t\t\t    ece_ack);\n\t\tif (acked - 1 >= tp->sacked_out)\n\t\t\ttp->sacked_out = 0;\n\t\telse\n\t\t\ttp->sacked_out -= acked - 1;\n\t}\n\ttcp_check_reno_reordering(sk, acked);\n\ttcp_verify_left_out(tp);\n}\n\nstatic inline void tcp_reset_reno_sack(struct tcp_sock *tp)\n{\n\ttp->sacked_out = 0;\n}\n\nvoid tcp_clear_retrans(struct tcp_sock *tp)\n{\n\ttp->retrans_out = 0;\n\ttp->lost_out = 0;\n\ttp->undo_marker = 0;\n\ttp->undo_retrans = -1;\n\ttp->sacked_out = 0;\n}\n\nstatic inline void tcp_init_undo(struct tcp_sock *tp)\n{\n\ttp->undo_marker = tp->snd_una;\n\t \n\ttp->undo_retrans = tp->retrans_out ? : -1;\n}\n\nstatic bool tcp_is_rack(const struct sock *sk)\n{\n\treturn READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_recovery) &\n\t\tTCP_RACK_LOSS_DETECTION;\n}\n\n \nstatic void tcp_timeout_mark_lost(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb, *head;\n\tbool is_reneg;\t\t\t \n\n\thead = tcp_rtx_queue_head(sk);\n\tis_reneg = head && (TCP_SKB_CB(head)->sacked & TCPCB_SACKED_ACKED);\n\tif (is_reneg) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPSACKRENEGING);\n\t\ttp->sacked_out = 0;\n\t\t \n\t\ttp->is_sack_reneg = 1;\n\t} else if (tcp_is_reno(tp)) {\n\t\ttcp_reset_reno_sack(tp);\n\t}\n\n\tskb = head;\n\tskb_rbtree_walk_from(skb) {\n\t\tif (is_reneg)\n\t\t\tTCP_SKB_CB(skb)->sacked &= ~TCPCB_SACKED_ACKED;\n\t\telse if (tcp_is_rack(sk) && skb != head &&\n\t\t\t tcp_rack_skb_timeout(tp, skb, 0) > 0)\n\t\t\tcontinue;  \n\t\ttcp_mark_skb_lost(sk, skb);\n\t}\n\ttcp_verify_left_out(tp);\n\ttcp_clear_all_retrans_hints(tp);\n}\n\n \nvoid tcp_enter_loss(struct sock *sk)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tbool new_recovery = icsk->icsk_ca_state < TCP_CA_Recovery;\n\tu8 reordering;\n\n\ttcp_timeout_mark_lost(sk);\n\n\t \n\tif (icsk->icsk_ca_state <= TCP_CA_Disorder ||\n\t    !after(tp->high_seq, tp->snd_una) ||\n\t    (icsk->icsk_ca_state == TCP_CA_Loss && !icsk->icsk_retransmits)) {\n\t\ttp->prior_ssthresh = tcp_current_ssthresh(sk);\n\t\ttp->prior_cwnd = tcp_snd_cwnd(tp);\n\t\ttp->snd_ssthresh = icsk->icsk_ca_ops->ssthresh(sk);\n\t\ttcp_ca_event(sk, CA_EVENT_LOSS);\n\t\ttcp_init_undo(tp);\n\t}\n\ttcp_snd_cwnd_set(tp, tcp_packets_in_flight(tp) + 1);\n\ttp->snd_cwnd_cnt   = 0;\n\ttp->snd_cwnd_stamp = tcp_jiffies32;\n\n\t \n\treordering = READ_ONCE(net->ipv4.sysctl_tcp_reordering);\n\tif (icsk->icsk_ca_state <= TCP_CA_Disorder &&\n\t    tp->sacked_out >= reordering)\n\t\ttp->reordering = min_t(unsigned int, tp->reordering,\n\t\t\t\t       reordering);\n\n\ttcp_set_ca_state(sk, TCP_CA_Loss);\n\ttp->high_seq = tp->snd_nxt;\n\ttcp_ecn_queue_cwr(tp);\n\n\t \n\ttp->frto = READ_ONCE(net->ipv4.sysctl_tcp_frto) &&\n\t\t   (new_recovery || icsk->icsk_retransmits) &&\n\t\t   !inet_csk(sk)->icsk_mtup.probe_size;\n}\n\n \nstatic bool tcp_check_sack_reneging(struct sock *sk, int *ack_flag)\n{\n\tif (*ack_flag & FLAG_SACK_RENEGING &&\n\t    *ack_flag & FLAG_SND_UNA_ADVANCED) {\n\t\tstruct tcp_sock *tp = tcp_sk(sk);\n\t\tunsigned long delay = max(usecs_to_jiffies(tp->srtt_us >> 4),\n\t\t\t\t\t  msecs_to_jiffies(10));\n\n\t\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,\n\t\t\t\t\t  delay, TCP_RTO_MAX);\n\t\t*ack_flag &= ~FLAG_SET_XMIT_TIMER;\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n \nstatic inline int tcp_dupack_heuristics(const struct tcp_sock *tp)\n{\n\treturn tp->sacked_out + 1;\n}\n\n \n\n \nstatic bool tcp_time_to_recover(struct sock *sk, int flag)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t \n\tif (tp->lost_out)\n\t\treturn true;\n\n\t \n\tif (!tcp_is_rack(sk) && tcp_dupack_heuristics(tp) > tp->reordering)\n\t\treturn true;\n\n\treturn false;\n}\n\n \nstatic void tcp_mark_head_lost(struct sock *sk, int packets, int mark_head)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb;\n\tint cnt;\n\t \n\tconst u32 loss_high = tp->snd_nxt;\n\n\tWARN_ON(packets > tp->packets_out);\n\tskb = tp->lost_skb_hint;\n\tif (skb) {\n\t\t \n\t\tif (mark_head && after(TCP_SKB_CB(skb)->seq, tp->snd_una))\n\t\t\treturn;\n\t\tcnt = tp->lost_cnt_hint;\n\t} else {\n\t\tskb = tcp_rtx_queue_head(sk);\n\t\tcnt = 0;\n\t}\n\n\tskb_rbtree_walk_from(skb) {\n\t\t \n\t\t \n\t\ttp->lost_skb_hint = skb;\n\t\ttp->lost_cnt_hint = cnt;\n\n\t\tif (after(TCP_SKB_CB(skb)->end_seq, loss_high))\n\t\t\tbreak;\n\n\t\tif (TCP_SKB_CB(skb)->sacked & TCPCB_SACKED_ACKED)\n\t\t\tcnt += tcp_skb_pcount(skb);\n\n\t\tif (cnt > packets)\n\t\t\tbreak;\n\n\t\tif (!(TCP_SKB_CB(skb)->sacked & TCPCB_LOST))\n\t\t\ttcp_mark_skb_lost(sk, skb);\n\n\t\tif (mark_head)\n\t\t\tbreak;\n\t}\n\ttcp_verify_left_out(tp);\n}\n\n \n\nstatic void tcp_update_scoreboard(struct sock *sk, int fast_rexmit)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (tcp_is_sack(tp)) {\n\t\tint sacked_upto = tp->sacked_out - tp->reordering;\n\t\tif (sacked_upto >= 0)\n\t\t\ttcp_mark_head_lost(sk, sacked_upto, 0);\n\t\telse if (fast_rexmit)\n\t\t\ttcp_mark_head_lost(sk, 1, 1);\n\t}\n}\n\nstatic bool tcp_tsopt_ecr_before(const struct tcp_sock *tp, u32 when)\n{\n\treturn tp->rx_opt.saw_tstamp && tp->rx_opt.rcv_tsecr &&\n\t       before(tp->rx_opt.rcv_tsecr, when);\n}\n\n \nstatic bool tcp_skb_spurious_retrans(const struct tcp_sock *tp,\n\t\t\t\t     const struct sk_buff *skb)\n{\n\treturn (TCP_SKB_CB(skb)->sacked & TCPCB_RETRANS) &&\n\t       tcp_tsopt_ecr_before(tp, tcp_skb_timestamp(skb));\n}\n\n \nstatic inline bool tcp_packet_delayed(const struct tcp_sock *tp)\n{\n\treturn tp->retrans_stamp &&\n\t       tcp_tsopt_ecr_before(tp, tp->retrans_stamp);\n}\n\n \n\n \nstatic bool tcp_any_retrans_done(const struct sock *sk)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb;\n\n\tif (tp->retrans_out)\n\t\treturn true;\n\n\tskb = tcp_rtx_queue_head(sk);\n\tif (unlikely(skb && TCP_SKB_CB(skb)->sacked & TCPCB_EVER_RETRANS))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic void DBGUNDO(struct sock *sk, const char *msg)\n{\n#if FASTRETRANS_DEBUG > 1\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\n\tif (sk->sk_family == AF_INET) {\n\t\tpr_debug(\"Undo %s %pI4/%u c%u l%u ss%u/%u p%u\\n\",\n\t\t\t msg,\n\t\t\t &inet->inet_daddr, ntohs(inet->inet_dport),\n\t\t\t tcp_snd_cwnd(tp), tcp_left_out(tp),\n\t\t\t tp->snd_ssthresh, tp->prior_ssthresh,\n\t\t\t tp->packets_out);\n\t}\n#if IS_ENABLED(CONFIG_IPV6)\n\telse if (sk->sk_family == AF_INET6) {\n\t\tpr_debug(\"Undo %s %pI6/%u c%u l%u ss%u/%u p%u\\n\",\n\t\t\t msg,\n\t\t\t &sk->sk_v6_daddr, ntohs(inet->inet_dport),\n\t\t\t tcp_snd_cwnd(tp), tcp_left_out(tp),\n\t\t\t tp->snd_ssthresh, tp->prior_ssthresh,\n\t\t\t tp->packets_out);\n\t}\n#endif\n#endif\n}\n\nstatic void tcp_undo_cwnd_reduction(struct sock *sk, bool unmark_loss)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (unmark_loss) {\n\t\tstruct sk_buff *skb;\n\n\t\tskb_rbtree_walk(skb, &sk->tcp_rtx_queue) {\n\t\t\tTCP_SKB_CB(skb)->sacked &= ~TCPCB_LOST;\n\t\t}\n\t\ttp->lost_out = 0;\n\t\ttcp_clear_all_retrans_hints(tp);\n\t}\n\n\tif (tp->prior_ssthresh) {\n\t\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\n\t\ttcp_snd_cwnd_set(tp, icsk->icsk_ca_ops->undo_cwnd(sk));\n\n\t\tif (tp->prior_ssthresh > tp->snd_ssthresh) {\n\t\t\ttp->snd_ssthresh = tp->prior_ssthresh;\n\t\t\ttcp_ecn_withdraw_cwr(tp);\n\t\t}\n\t}\n\ttp->snd_cwnd_stamp = tcp_jiffies32;\n\ttp->undo_marker = 0;\n\ttp->rack.advanced = 1;  \n}\n\nstatic inline bool tcp_may_undo(const struct tcp_sock *tp)\n{\n\treturn tp->undo_marker && (!tp->undo_retrans || tcp_packet_delayed(tp));\n}\n\nstatic bool tcp_is_non_sack_preventing_reopen(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (tp->snd_una == tp->high_seq && tcp_is_reno(tp)) {\n\t\t \n\t\tif (!tcp_any_retrans_done(sk))\n\t\t\ttp->retrans_stamp = 0;\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n \nstatic bool tcp_try_undo_recovery(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (tcp_may_undo(tp)) {\n\t\tint mib_idx;\n\n\t\t \n\t\tDBGUNDO(sk, inet_csk(sk)->icsk_ca_state == TCP_CA_Loss ? \"loss\" : \"retrans\");\n\t\ttcp_undo_cwnd_reduction(sk, false);\n\t\tif (inet_csk(sk)->icsk_ca_state == TCP_CA_Loss)\n\t\t\tmib_idx = LINUX_MIB_TCPLOSSUNDO;\n\t\telse\n\t\t\tmib_idx = LINUX_MIB_TCPFULLUNDO;\n\n\t\tNET_INC_STATS(sock_net(sk), mib_idx);\n\t} else if (tp->rack.reo_wnd_persist) {\n\t\ttp->rack.reo_wnd_persist--;\n\t}\n\tif (tcp_is_non_sack_preventing_reopen(sk))\n\t\treturn true;\n\ttcp_set_ca_state(sk, TCP_CA_Open);\n\ttp->is_sack_reneg = 0;\n\treturn false;\n}\n\n \nstatic bool tcp_try_undo_dsack(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (tp->undo_marker && !tp->undo_retrans) {\n\t\ttp->rack.reo_wnd_persist = min(TCP_RACK_RECOVERY_THRESH,\n\t\t\t\t\t       tp->rack.reo_wnd_persist + 1);\n\t\tDBGUNDO(sk, \"D-SACK\");\n\t\ttcp_undo_cwnd_reduction(sk, false);\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPDSACKUNDO);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n \nstatic bool tcp_try_undo_loss(struct sock *sk, bool frto_undo)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (frto_undo || tcp_may_undo(tp)) {\n\t\ttcp_undo_cwnd_reduction(sk, true);\n\n\t\tDBGUNDO(sk, \"partial loss\");\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPLOSSUNDO);\n\t\tif (frto_undo)\n\t\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\t\tLINUX_MIB_TCPSPURIOUSRTOS);\n\t\tinet_csk(sk)->icsk_retransmits = 0;\n\t\tif (tcp_is_non_sack_preventing_reopen(sk))\n\t\t\treturn true;\n\t\tif (frto_undo || tcp_is_sack(tp)) {\n\t\t\ttcp_set_ca_state(sk, TCP_CA_Open);\n\t\t\ttp->is_sack_reneg = 0;\n\t\t}\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n \nstatic void tcp_init_cwnd_reduction(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\ttp->high_seq = tp->snd_nxt;\n\ttp->tlp_high_seq = 0;\n\ttp->snd_cwnd_cnt = 0;\n\ttp->prior_cwnd = tcp_snd_cwnd(tp);\n\ttp->prr_delivered = 0;\n\ttp->prr_out = 0;\n\ttp->snd_ssthresh = inet_csk(sk)->icsk_ca_ops->ssthresh(sk);\n\ttcp_ecn_queue_cwr(tp);\n}\n\nvoid tcp_cwnd_reduction(struct sock *sk, int newly_acked_sacked, int newly_lost, int flag)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint sndcnt = 0;\n\tint delta = tp->snd_ssthresh - tcp_packets_in_flight(tp);\n\n\tif (newly_acked_sacked <= 0 || WARN_ON_ONCE(!tp->prior_cwnd))\n\t\treturn;\n\n\ttp->prr_delivered += newly_acked_sacked;\n\tif (delta < 0) {\n\t\tu64 dividend = (u64)tp->snd_ssthresh * tp->prr_delivered +\n\t\t\t       tp->prior_cwnd - 1;\n\t\tsndcnt = div_u64(dividend, tp->prior_cwnd) - tp->prr_out;\n\t} else {\n\t\tsndcnt = max_t(int, tp->prr_delivered - tp->prr_out,\n\t\t\t       newly_acked_sacked);\n\t\tif (flag & FLAG_SND_UNA_ADVANCED && !newly_lost)\n\t\t\tsndcnt++;\n\t\tsndcnt = min(delta, sndcnt);\n\t}\n\t \n\tsndcnt = max(sndcnt, (tp->prr_out ? 0 : 1));\n\ttcp_snd_cwnd_set(tp, tcp_packets_in_flight(tp) + sndcnt);\n}\n\nstatic inline void tcp_end_cwnd_reduction(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (inet_csk(sk)->icsk_ca_ops->cong_control)\n\t\treturn;\n\n\t \n\tif (tp->snd_ssthresh < TCP_INFINITE_SSTHRESH &&\n\t    (inet_csk(sk)->icsk_ca_state == TCP_CA_CWR || tp->undo_marker)) {\n\t\ttcp_snd_cwnd_set(tp, tp->snd_ssthresh);\n\t\ttp->snd_cwnd_stamp = tcp_jiffies32;\n\t}\n\ttcp_ca_event(sk, CA_EVENT_COMPLETE_CWR);\n}\n\n \nvoid tcp_enter_cwr(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\ttp->prior_ssthresh = 0;\n\tif (inet_csk(sk)->icsk_ca_state < TCP_CA_CWR) {\n\t\ttp->undo_marker = 0;\n\t\ttcp_init_cwnd_reduction(sk);\n\t\ttcp_set_ca_state(sk, TCP_CA_CWR);\n\t}\n}\nEXPORT_SYMBOL(tcp_enter_cwr);\n\nstatic void tcp_try_keep_open(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint state = TCP_CA_Open;\n\n\tif (tcp_left_out(tp) || tcp_any_retrans_done(sk))\n\t\tstate = TCP_CA_Disorder;\n\n\tif (inet_csk(sk)->icsk_ca_state != state) {\n\t\ttcp_set_ca_state(sk, state);\n\t\ttp->high_seq = tp->snd_nxt;\n\t}\n}\n\nstatic void tcp_try_to_open(struct sock *sk, int flag)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\ttcp_verify_left_out(tp);\n\n\tif (!tcp_any_retrans_done(sk))\n\t\ttp->retrans_stamp = 0;\n\n\tif (flag & FLAG_ECE)\n\t\ttcp_enter_cwr(sk);\n\n\tif (inet_csk(sk)->icsk_ca_state != TCP_CA_CWR) {\n\t\ttcp_try_keep_open(sk);\n\t}\n}\n\nstatic void tcp_mtup_probe_failed(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\ticsk->icsk_mtup.search_high = icsk->icsk_mtup.probe_size - 1;\n\ticsk->icsk_mtup.probe_size = 0;\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPMTUPFAIL);\n}\n\nstatic void tcp_mtup_probe_success(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tu64 val;\n\n\ttp->prior_ssthresh = tcp_current_ssthresh(sk);\n\n\tval = (u64)tcp_snd_cwnd(tp) * tcp_mss_to_mtu(sk, tp->mss_cache);\n\tdo_div(val, icsk->icsk_mtup.probe_size);\n\tDEBUG_NET_WARN_ON_ONCE((u32)val != val);\n\ttcp_snd_cwnd_set(tp, max_t(u32, 1U, val));\n\n\ttp->snd_cwnd_cnt = 0;\n\ttp->snd_cwnd_stamp = tcp_jiffies32;\n\ttp->snd_ssthresh = tcp_current_ssthresh(sk);\n\n\ticsk->icsk_mtup.search_low = icsk->icsk_mtup.probe_size;\n\ticsk->icsk_mtup.probe_size = 0;\n\ttcp_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPMTUPSUCCESS);\n}\n\n \nvoid tcp_simple_retransmit(struct sock *sk)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb;\n\tint mss;\n\n\t \n\tif (tp->syn_data && sk->sk_state == TCP_SYN_SENT)\n\t\tmss = -1;\n\telse\n\t\tmss = tcp_current_mss(sk);\n\n\tskb_rbtree_walk(skb, &sk->tcp_rtx_queue) {\n\t\tif (tcp_skb_seglen(skb) > mss)\n\t\t\ttcp_mark_skb_lost(sk, skb);\n\t}\n\n\ttcp_clear_retrans_hints_partial(tp);\n\n\tif (!tp->lost_out)\n\t\treturn;\n\n\tif (tcp_is_reno(tp))\n\t\ttcp_limit_reno_sacked(tp);\n\n\ttcp_verify_left_out(tp);\n\n\t \n\tif (icsk->icsk_ca_state != TCP_CA_Loss) {\n\t\ttp->high_seq = tp->snd_nxt;\n\t\ttp->snd_ssthresh = tcp_current_ssthresh(sk);\n\t\ttp->prior_ssthresh = 0;\n\t\ttp->undo_marker = 0;\n\t\ttcp_set_ca_state(sk, TCP_CA_Loss);\n\t}\n\ttcp_xmit_retransmit_queue(sk);\n}\nEXPORT_SYMBOL(tcp_simple_retransmit);\n\nvoid tcp_enter_recovery(struct sock *sk, bool ece_ack)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint mib_idx;\n\n\tif (tcp_is_reno(tp))\n\t\tmib_idx = LINUX_MIB_TCPRENORECOVERY;\n\telse\n\t\tmib_idx = LINUX_MIB_TCPSACKRECOVERY;\n\n\tNET_INC_STATS(sock_net(sk), mib_idx);\n\n\ttp->prior_ssthresh = 0;\n\ttcp_init_undo(tp);\n\n\tif (!tcp_in_cwnd_reduction(sk)) {\n\t\tif (!ece_ack)\n\t\t\ttp->prior_ssthresh = tcp_current_ssthresh(sk);\n\t\ttcp_init_cwnd_reduction(sk);\n\t}\n\ttcp_set_ca_state(sk, TCP_CA_Recovery);\n}\n\n \nstatic void tcp_process_loss(struct sock *sk, int flag, int num_dupack,\n\t\t\t     int *rexmit)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tbool recovered = !before(tp->snd_una, tp->high_seq);\n\n\tif ((flag & FLAG_SND_UNA_ADVANCED || rcu_access_pointer(tp->fastopen_rsk)) &&\n\t    tcp_try_undo_loss(sk, false))\n\t\treturn;\n\n\tif (tp->frto) {  \n\t\t \n\t\tif ((flag & FLAG_ORIG_SACK_ACKED) &&\n\t\t    tcp_try_undo_loss(sk, true))\n\t\t\treturn;\n\n\t\tif (after(tp->snd_nxt, tp->high_seq)) {\n\t\t\tif (flag & FLAG_DATA_SACKED || num_dupack)\n\t\t\t\ttp->frto = 0;  \n\t\t} else if (flag & FLAG_SND_UNA_ADVANCED && !recovered) {\n\t\t\ttp->high_seq = tp->snd_nxt;\n\t\t\t \n\t\t\tif (!tcp_write_queue_empty(sk) &&\n\t\t\t    after(tcp_wnd_end(tp), tp->snd_nxt)) {\n\t\t\t\t*rexmit = REXMIT_NEW;\n\t\t\t\treturn;\n\t\t\t}\n\t\t\ttp->frto = 0;\n\t\t}\n\t}\n\n\tif (recovered) {\n\t\t \n\t\ttcp_try_undo_recovery(sk);\n\t\treturn;\n\t}\n\tif (tcp_is_reno(tp)) {\n\t\t \n\t\tif (after(tp->snd_nxt, tp->high_seq) && num_dupack)\n\t\t\ttcp_add_reno_sack(sk, num_dupack, flag & FLAG_ECE);\n\t\telse if (flag & FLAG_SND_UNA_ADVANCED)\n\t\t\ttcp_reset_reno_sack(tp);\n\t}\n\t*rexmit = REXMIT_LOST;\n}\n\nstatic bool tcp_force_fast_retransmit(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\treturn after(tcp_highest_sack_seq(tp),\n\t\t     tp->snd_una + tp->reordering * tp->mss_cache);\n}\n\n \nstatic bool tcp_try_undo_partial(struct sock *sk, u32 prior_snd_una,\n\t\t\t\t bool *do_lost)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (tp->undo_marker && tcp_packet_delayed(tp)) {\n\t\t \n\t\ttcp_check_sack_reordering(sk, prior_snd_una, 1);\n\n\t\t \n\t\tif (tp->retrans_out)\n\t\t\treturn true;\n\n\t\tif (!tcp_any_retrans_done(sk))\n\t\t\ttp->retrans_stamp = 0;\n\n\t\tDBGUNDO(sk, \"partial recovery\");\n\t\ttcp_undo_cwnd_reduction(sk, true);\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPPARTIALUNDO);\n\t\ttcp_try_keep_open(sk);\n\t} else {\n\t\t \n\t\t*do_lost = tcp_force_fast_retransmit(sk);\n\t}\n\treturn false;\n}\n\nstatic void tcp_identify_packet_loss(struct sock *sk, int *ack_flag)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (tcp_rtx_queue_empty(sk))\n\t\treturn;\n\n\tif (unlikely(tcp_is_reno(tp))) {\n\t\ttcp_newreno_mark_lost(sk, *ack_flag & FLAG_SND_UNA_ADVANCED);\n\t} else if (tcp_is_rack(sk)) {\n\t\tu32 prior_retrans = tp->retrans_out;\n\n\t\tif (tcp_rack_mark_lost(sk))\n\t\t\t*ack_flag &= ~FLAG_SET_XMIT_TIMER;\n\t\tif (prior_retrans > tp->retrans_out)\n\t\t\t*ack_flag |= FLAG_LOST_RETRANS;\n\t}\n}\n\n \nstatic void tcp_fastretrans_alert(struct sock *sk, const u32 prior_snd_una,\n\t\t\t\t  int num_dupack, int *ack_flag, int *rexmit)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint fast_rexmit = 0, flag = *ack_flag;\n\tbool ece_ack = flag & FLAG_ECE;\n\tbool do_lost = num_dupack || ((flag & FLAG_DATA_SACKED) &&\n\t\t\t\t      tcp_force_fast_retransmit(sk));\n\n\tif (!tp->packets_out && tp->sacked_out)\n\t\ttp->sacked_out = 0;\n\n\t \n\tif (ece_ack)\n\t\ttp->prior_ssthresh = 0;\n\n\t \n\tif (tcp_check_sack_reneging(sk, ack_flag))\n\t\treturn;\n\n\t \n\ttcp_verify_left_out(tp);\n\n\t \n\tif (icsk->icsk_ca_state == TCP_CA_Open) {\n\t\tWARN_ON(tp->retrans_out != 0 && !tp->syn_data);\n\t\ttp->retrans_stamp = 0;\n\t} else if (!before(tp->snd_una, tp->high_seq)) {\n\t\tswitch (icsk->icsk_ca_state) {\n\t\tcase TCP_CA_CWR:\n\t\t\t \n\t\t\tif (tp->snd_una != tp->high_seq) {\n\t\t\t\ttcp_end_cwnd_reduction(sk);\n\t\t\t\ttcp_set_ca_state(sk, TCP_CA_Open);\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase TCP_CA_Recovery:\n\t\t\tif (tcp_is_reno(tp))\n\t\t\t\ttcp_reset_reno_sack(tp);\n\t\t\tif (tcp_try_undo_recovery(sk))\n\t\t\t\treturn;\n\t\t\ttcp_end_cwnd_reduction(sk);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\tswitch (icsk->icsk_ca_state) {\n\tcase TCP_CA_Recovery:\n\t\tif (!(flag & FLAG_SND_UNA_ADVANCED)) {\n\t\t\tif (tcp_is_reno(tp))\n\t\t\t\ttcp_add_reno_sack(sk, num_dupack, ece_ack);\n\t\t} else if (tcp_try_undo_partial(sk, prior_snd_una, &do_lost))\n\t\t\treturn;\n\n\t\tif (tcp_try_undo_dsack(sk))\n\t\t\ttcp_try_keep_open(sk);\n\n\t\ttcp_identify_packet_loss(sk, ack_flag);\n\t\tif (icsk->icsk_ca_state != TCP_CA_Recovery) {\n\t\t\tif (!tcp_time_to_recover(sk, flag))\n\t\t\t\treturn;\n\t\t\t \n\t\t\ttcp_enter_recovery(sk, ece_ack);\n\t\t}\n\t\tbreak;\n\tcase TCP_CA_Loss:\n\t\ttcp_process_loss(sk, flag, num_dupack, rexmit);\n\t\ttcp_identify_packet_loss(sk, ack_flag);\n\t\tif (!(icsk->icsk_ca_state == TCP_CA_Open ||\n\t\t      (*ack_flag & FLAG_LOST_RETRANS)))\n\t\t\treturn;\n\t\t \n\t\tfallthrough;\n\tdefault:\n\t\tif (tcp_is_reno(tp)) {\n\t\t\tif (flag & FLAG_SND_UNA_ADVANCED)\n\t\t\t\ttcp_reset_reno_sack(tp);\n\t\t\ttcp_add_reno_sack(sk, num_dupack, ece_ack);\n\t\t}\n\n\t\tif (icsk->icsk_ca_state <= TCP_CA_Disorder)\n\t\t\ttcp_try_undo_dsack(sk);\n\n\t\ttcp_identify_packet_loss(sk, ack_flag);\n\t\tif (!tcp_time_to_recover(sk, flag)) {\n\t\t\ttcp_try_to_open(sk, flag);\n\t\t\treturn;\n\t\t}\n\n\t\t \n\t\tif (icsk->icsk_ca_state < TCP_CA_CWR &&\n\t\t    icsk->icsk_mtup.probe_size &&\n\t\t    tp->snd_una == tp->mtu_probe.probe_seq_start) {\n\t\t\ttcp_mtup_probe_failed(sk);\n\t\t\t \n\t\t\ttcp_snd_cwnd_set(tp, tcp_snd_cwnd(tp) + 1);\n\t\t\ttcp_simple_retransmit(sk);\n\t\t\treturn;\n\t\t}\n\n\t\t \n\t\ttcp_enter_recovery(sk, ece_ack);\n\t\tfast_rexmit = 1;\n\t}\n\n\tif (!tcp_is_rack(sk) && do_lost)\n\t\ttcp_update_scoreboard(sk, fast_rexmit);\n\t*rexmit = REXMIT_LOST;\n}\n\nstatic void tcp_update_rtt_min(struct sock *sk, u32 rtt_us, const int flag)\n{\n\tu32 wlen = READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_min_rtt_wlen) * HZ;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif ((flag & FLAG_ACK_MAYBE_DELAYED) && rtt_us > tcp_min_rtt(tp)) {\n\t\t \n\t\treturn;\n\t}\n\tminmax_running_min(&tp->rtt_min, wlen, tcp_jiffies32,\n\t\t\t   rtt_us ? : jiffies_to_usecs(1));\n}\n\nstatic bool tcp_ack_update_rtt(struct sock *sk, const int flag,\n\t\t\t       long seq_rtt_us, long sack_rtt_us,\n\t\t\t       long ca_rtt_us, struct rate_sample *rs)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\n\t \n\tif (seq_rtt_us < 0)\n\t\tseq_rtt_us = sack_rtt_us;\n\n\t \n\tif (seq_rtt_us < 0 && tp->rx_opt.saw_tstamp && tp->rx_opt.rcv_tsecr &&\n\t    flag & FLAG_ACKED) {\n\t\tu32 delta = tcp_time_stamp(tp) - tp->rx_opt.rcv_tsecr;\n\n\t\tif (likely(delta < INT_MAX / (USEC_PER_SEC / TCP_TS_HZ))) {\n\t\t\tif (!delta)\n\t\t\t\tdelta = 1;\n\t\t\tseq_rtt_us = delta * (USEC_PER_SEC / TCP_TS_HZ);\n\t\t\tca_rtt_us = seq_rtt_us;\n\t\t}\n\t}\n\trs->rtt_us = ca_rtt_us;  \n\tif (seq_rtt_us < 0)\n\t\treturn false;\n\n\t \n\ttcp_update_rtt_min(sk, ca_rtt_us, flag);\n\ttcp_rtt_estimator(sk, seq_rtt_us);\n\ttcp_set_rto(sk);\n\n\t \n\tinet_csk(sk)->icsk_backoff = 0;\n\treturn true;\n}\n\n \nvoid tcp_synack_rtt_meas(struct sock *sk, struct request_sock *req)\n{\n\tstruct rate_sample rs;\n\tlong rtt_us = -1L;\n\n\tif (req && !req->num_retrans && tcp_rsk(req)->snt_synack)\n\t\trtt_us = tcp_stamp_us_delta(tcp_clock_us(), tcp_rsk(req)->snt_synack);\n\n\ttcp_ack_update_rtt(sk, FLAG_SYN_ACKED, rtt_us, -1L, rtt_us, &rs);\n}\n\n\nstatic void tcp_cong_avoid(struct sock *sk, u32 ack, u32 acked)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\n\ticsk->icsk_ca_ops->cong_avoid(sk, ack, acked);\n\ttcp_sk(sk)->snd_cwnd_stamp = tcp_jiffies32;\n}\n\n \nvoid tcp_rearm_rto(struct sock *sk)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t \n\tif (rcu_access_pointer(tp->fastopen_rsk))\n\t\treturn;\n\n\tif (!tp->packets_out) {\n\t\tinet_csk_clear_xmit_timer(sk, ICSK_TIME_RETRANS);\n\t} else {\n\t\tu32 rto = inet_csk(sk)->icsk_rto;\n\t\t \n\t\tif (icsk->icsk_pending == ICSK_TIME_REO_TIMEOUT ||\n\t\t    icsk->icsk_pending == ICSK_TIME_LOSS_PROBE) {\n\t\t\ts64 delta_us = tcp_rto_delta_us(sk);\n\t\t\t \n\t\t\trto = usecs_to_jiffies(max_t(int, delta_us, 1));\n\t\t}\n\t\ttcp_reset_xmit_timer(sk, ICSK_TIME_RETRANS, rto,\n\t\t\t\t     TCP_RTO_MAX);\n\t}\n}\n\n \nstatic void tcp_set_xmit_timer(struct sock *sk)\n{\n\tif (!tcp_schedule_loss_probe(sk, true))\n\t\ttcp_rearm_rto(sk);\n}\n\n \nstatic u32 tcp_tso_acked(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 packets_acked;\n\n\tBUG_ON(!after(TCP_SKB_CB(skb)->end_seq, tp->snd_una));\n\n\tpackets_acked = tcp_skb_pcount(skb);\n\tif (tcp_trim_head(sk, skb, tp->snd_una - TCP_SKB_CB(skb)->seq))\n\t\treturn 0;\n\tpackets_acked -= tcp_skb_pcount(skb);\n\n\tif (packets_acked) {\n\t\tBUG_ON(tcp_skb_pcount(skb) == 0);\n\t\tBUG_ON(!before(TCP_SKB_CB(skb)->seq, TCP_SKB_CB(skb)->end_seq));\n\t}\n\n\treturn packets_acked;\n}\n\nstatic void tcp_ack_tstamp(struct sock *sk, struct sk_buff *skb,\n\t\t\t   const struct sk_buff *ack_skb, u32 prior_snd_una)\n{\n\tconst struct skb_shared_info *shinfo;\n\n\t \n\tif (likely(!TCP_SKB_CB(skb)->txstamp_ack))\n\t\treturn;\n\n\tshinfo = skb_shinfo(skb);\n\tif (!before(shinfo->tskey, prior_snd_una) &&\n\t    before(shinfo->tskey, tcp_sk(sk)->snd_una)) {\n\t\ttcp_skb_tsorted_save(skb) {\n\t\t\t__skb_tstamp_tx(skb, ack_skb, NULL, sk, SCM_TSTAMP_ACK);\n\t\t} tcp_skb_tsorted_restore(skb);\n\t}\n}\n\n \nstatic int tcp_clean_rtx_queue(struct sock *sk, const struct sk_buff *ack_skb,\n\t\t\t       u32 prior_fack, u32 prior_snd_una,\n\t\t\t       struct tcp_sacktag_state *sack, bool ece_ack)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tu64 first_ackt, last_ackt;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 prior_sacked = tp->sacked_out;\n\tu32 reord = tp->snd_nxt;  \n\tstruct sk_buff *skb, *next;\n\tbool fully_acked = true;\n\tlong sack_rtt_us = -1L;\n\tlong seq_rtt_us = -1L;\n\tlong ca_rtt_us = -1L;\n\tu32 pkts_acked = 0;\n\tbool rtt_update;\n\tint flag = 0;\n\n\tfirst_ackt = 0;\n\n\tfor (skb = skb_rb_first(&sk->tcp_rtx_queue); skb; skb = next) {\n\t\tstruct tcp_skb_cb *scb = TCP_SKB_CB(skb);\n\t\tconst u32 start_seq = scb->seq;\n\t\tu8 sacked = scb->sacked;\n\t\tu32 acked_pcount;\n\n\t\t \n\t\tif (after(scb->end_seq, tp->snd_una)) {\n\t\t\tif (tcp_skb_pcount(skb) == 1 ||\n\t\t\t    !after(tp->snd_una, scb->seq))\n\t\t\t\tbreak;\n\n\t\t\tacked_pcount = tcp_tso_acked(sk, skb);\n\t\t\tif (!acked_pcount)\n\t\t\t\tbreak;\n\t\t\tfully_acked = false;\n\t\t} else {\n\t\t\tacked_pcount = tcp_skb_pcount(skb);\n\t\t}\n\n\t\tif (unlikely(sacked & TCPCB_RETRANS)) {\n\t\t\tif (sacked & TCPCB_SACKED_RETRANS)\n\t\t\t\ttp->retrans_out -= acked_pcount;\n\t\t\tflag |= FLAG_RETRANS_DATA_ACKED;\n\t\t} else if (!(sacked & TCPCB_SACKED_ACKED)) {\n\t\t\tlast_ackt = tcp_skb_timestamp_us(skb);\n\t\t\tWARN_ON_ONCE(last_ackt == 0);\n\t\t\tif (!first_ackt)\n\t\t\t\tfirst_ackt = last_ackt;\n\n\t\t\tif (before(start_seq, reord))\n\t\t\t\treord = start_seq;\n\t\t\tif (!after(scb->end_seq, tp->high_seq))\n\t\t\t\tflag |= FLAG_ORIG_SACK_ACKED;\n\t\t}\n\n\t\tif (sacked & TCPCB_SACKED_ACKED) {\n\t\t\ttp->sacked_out -= acked_pcount;\n\t\t} else if (tcp_is_sack(tp)) {\n\t\t\ttcp_count_delivered(tp, acked_pcount, ece_ack);\n\t\t\tif (!tcp_skb_spurious_retrans(tp, skb))\n\t\t\t\ttcp_rack_advance(tp, sacked, scb->end_seq,\n\t\t\t\t\t\t tcp_skb_timestamp_us(skb));\n\t\t}\n\t\tif (sacked & TCPCB_LOST)\n\t\t\ttp->lost_out -= acked_pcount;\n\n\t\ttp->packets_out -= acked_pcount;\n\t\tpkts_acked += acked_pcount;\n\t\ttcp_rate_skb_delivered(sk, skb, sack->rate);\n\n\t\t \n\t\tif (likely(!(scb->tcp_flags & TCPHDR_SYN))) {\n\t\t\tflag |= FLAG_DATA_ACKED;\n\t\t} else {\n\t\t\tflag |= FLAG_SYN_ACKED;\n\t\t\ttp->retrans_stamp = 0;\n\t\t}\n\n\t\tif (!fully_acked)\n\t\t\tbreak;\n\n\t\ttcp_ack_tstamp(sk, skb, ack_skb, prior_snd_una);\n\n\t\tnext = skb_rb_next(skb);\n\t\tif (unlikely(skb == tp->retransmit_skb_hint))\n\t\t\ttp->retransmit_skb_hint = NULL;\n\t\tif (unlikely(skb == tp->lost_skb_hint))\n\t\t\ttp->lost_skb_hint = NULL;\n\t\ttcp_highest_sack_replace(sk, skb, next);\n\t\ttcp_rtx_queue_unlink_and_free(skb, sk);\n\t}\n\n\tif (!skb)\n\t\ttcp_chrono_stop(sk, TCP_CHRONO_BUSY);\n\n\tif (likely(between(tp->snd_up, prior_snd_una, tp->snd_una)))\n\t\ttp->snd_up = tp->snd_una;\n\n\tif (skb) {\n\t\ttcp_ack_tstamp(sk, skb, ack_skb, prior_snd_una);\n\t\tif (TCP_SKB_CB(skb)->sacked & TCPCB_SACKED_ACKED)\n\t\t\tflag |= FLAG_SACK_RENEGING;\n\t}\n\n\tif (likely(first_ackt) && !(flag & FLAG_RETRANS_DATA_ACKED)) {\n\t\tseq_rtt_us = tcp_stamp_us_delta(tp->tcp_mstamp, first_ackt);\n\t\tca_rtt_us = tcp_stamp_us_delta(tp->tcp_mstamp, last_ackt);\n\n\t\tif (pkts_acked == 1 && fully_acked && !prior_sacked &&\n\t\t    (tp->snd_una - prior_snd_una) < tp->mss_cache &&\n\t\t    sack->rate->prior_delivered + 1 == tp->delivered &&\n\t\t    !(flag & (FLAG_CA_ALERT | FLAG_SYN_ACKED))) {\n\t\t\t \n\t\t\tflag |= FLAG_ACK_MAYBE_DELAYED;\n\t\t}\n\t}\n\tif (sack->first_sackt) {\n\t\tsack_rtt_us = tcp_stamp_us_delta(tp->tcp_mstamp, sack->first_sackt);\n\t\tca_rtt_us = tcp_stamp_us_delta(tp->tcp_mstamp, sack->last_sackt);\n\t}\n\trtt_update = tcp_ack_update_rtt(sk, flag, seq_rtt_us, sack_rtt_us,\n\t\t\t\t\tca_rtt_us, sack->rate);\n\n\tif (flag & FLAG_ACKED) {\n\t\tflag |= FLAG_SET_XMIT_TIMER;   \n\t\tif (unlikely(icsk->icsk_mtup.probe_size &&\n\t\t\t     !after(tp->mtu_probe.probe_seq_end, tp->snd_una))) {\n\t\t\ttcp_mtup_probe_success(sk);\n\t\t}\n\n\t\tif (tcp_is_reno(tp)) {\n\t\t\ttcp_remove_reno_sacks(sk, pkts_acked, ece_ack);\n\n\t\t\t \n\t\t\tif (flag & FLAG_RETRANS_DATA_ACKED)\n\t\t\t\tflag &= ~FLAG_ORIG_SACK_ACKED;\n\t\t} else {\n\t\t\tint delta;\n\n\t\t\t \n\t\t\tif (before(reord, prior_fack))\n\t\t\t\ttcp_check_sack_reordering(sk, reord, 0);\n\n\t\t\tdelta = prior_sacked - tp->sacked_out;\n\t\t\ttp->lost_cnt_hint -= min(tp->lost_cnt_hint, delta);\n\t\t}\n\t} else if (skb && rtt_update && sack_rtt_us >= 0 &&\n\t\t   sack_rtt_us > tcp_stamp_us_delta(tp->tcp_mstamp,\n\t\t\t\t\t\t    tcp_skb_timestamp_us(skb))) {\n\t\t \n\t\tflag |= FLAG_SET_XMIT_TIMER;   \n\t}\n\n\tif (icsk->icsk_ca_ops->pkts_acked) {\n\t\tstruct ack_sample sample = { .pkts_acked = pkts_acked,\n\t\t\t\t\t     .rtt_us = sack->rate->rtt_us };\n\n\t\tsample.in_flight = tp->mss_cache *\n\t\t\t(tp->delivered - sack->rate->prior_delivered);\n\t\ticsk->icsk_ca_ops->pkts_acked(sk, &sample);\n\t}\n\n#if FASTRETRANS_DEBUG > 0\n\tWARN_ON((int)tp->sacked_out < 0);\n\tWARN_ON((int)tp->lost_out < 0);\n\tWARN_ON((int)tp->retrans_out < 0);\n\tif (!tp->packets_out && tcp_is_sack(tp)) {\n\t\ticsk = inet_csk(sk);\n\t\tif (tp->lost_out) {\n\t\t\tpr_debug(\"Leak l=%u %d\\n\",\n\t\t\t\t tp->lost_out, icsk->icsk_ca_state);\n\t\t\ttp->lost_out = 0;\n\t\t}\n\t\tif (tp->sacked_out) {\n\t\t\tpr_debug(\"Leak s=%u %d\\n\",\n\t\t\t\t tp->sacked_out, icsk->icsk_ca_state);\n\t\t\ttp->sacked_out = 0;\n\t\t}\n\t\tif (tp->retrans_out) {\n\t\t\tpr_debug(\"Leak r=%u %d\\n\",\n\t\t\t\t tp->retrans_out, icsk->icsk_ca_state);\n\t\t\ttp->retrans_out = 0;\n\t\t}\n\t}\n#endif\n\treturn flag;\n}\n\nstatic void tcp_ack_probe(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct sk_buff *head = tcp_send_head(sk);\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\n\t \n\tif (!head)\n\t\treturn;\n\tif (!after(TCP_SKB_CB(head)->end_seq, tcp_wnd_end(tp))) {\n\t\ticsk->icsk_backoff = 0;\n\t\ticsk->icsk_probes_tstamp = 0;\n\t\tinet_csk_clear_xmit_timer(sk, ICSK_TIME_PROBE0);\n\t\t \n\t} else {\n\t\tunsigned long when = tcp_probe0_when(sk, TCP_RTO_MAX);\n\n\t\twhen = tcp_clamp_probe0_to_user_timeout(sk, when);\n\t\ttcp_reset_xmit_timer(sk, ICSK_TIME_PROBE0, when, TCP_RTO_MAX);\n\t}\n}\n\nstatic inline bool tcp_ack_is_dubious(const struct sock *sk, const int flag)\n{\n\treturn !(flag & FLAG_NOT_DUP) || (flag & FLAG_CA_ALERT) ||\n\t\tinet_csk(sk)->icsk_ca_state != TCP_CA_Open;\n}\n\n \nstatic inline bool tcp_may_raise_cwnd(const struct sock *sk, const int flag)\n{\n\t \n\tif (tcp_sk(sk)->reordering >\n\t    READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_reordering))\n\t\treturn flag & FLAG_FORWARD_PROGRESS;\n\n\treturn flag & FLAG_DATA_ACKED;\n}\n\n \nstatic void tcp_cong_control(struct sock *sk, u32 ack, u32 acked_sacked,\n\t\t\t     int flag, const struct rate_sample *rs)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\n\tif (icsk->icsk_ca_ops->cong_control) {\n\t\ticsk->icsk_ca_ops->cong_control(sk, rs);\n\t\treturn;\n\t}\n\n\tif (tcp_in_cwnd_reduction(sk)) {\n\t\t \n\t\ttcp_cwnd_reduction(sk, acked_sacked, rs->losses, flag);\n\t} else if (tcp_may_raise_cwnd(sk, flag)) {\n\t\t \n\t\ttcp_cong_avoid(sk, ack, acked_sacked);\n\t}\n\ttcp_update_pacing_rate(sk);\n}\n\n \nstatic inline bool tcp_may_update_window(const struct tcp_sock *tp,\n\t\t\t\t\tconst u32 ack, const u32 ack_seq,\n\t\t\t\t\tconst u32 nwin)\n{\n\treturn\tafter(ack, tp->snd_una) ||\n\t\tafter(ack_seq, tp->snd_wl1) ||\n\t\t(ack_seq == tp->snd_wl1 && (nwin > tp->snd_wnd || !nwin));\n}\n\n \nstatic void tcp_snd_una_update(struct tcp_sock *tp, u32 ack)\n{\n\tu32 delta = ack - tp->snd_una;\n\n\tsock_owned_by_me((struct sock *)tp);\n\ttp->bytes_acked += delta;\n\ttp->snd_una = ack;\n}\n\n \nstatic void tcp_rcv_nxt_update(struct tcp_sock *tp, u32 seq)\n{\n\tu32 delta = seq - tp->rcv_nxt;\n\n\tsock_owned_by_me((struct sock *)tp);\n\ttp->bytes_received += delta;\n\tWRITE_ONCE(tp->rcv_nxt, seq);\n}\n\n \nstatic int tcp_ack_update_window(struct sock *sk, const struct sk_buff *skb, u32 ack,\n\t\t\t\t u32 ack_seq)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint flag = 0;\n\tu32 nwin = ntohs(tcp_hdr(skb)->window);\n\n\tif (likely(!tcp_hdr(skb)->syn))\n\t\tnwin <<= tp->rx_opt.snd_wscale;\n\n\tif (tcp_may_update_window(tp, ack, ack_seq, nwin)) {\n\t\tflag |= FLAG_WIN_UPDATE;\n\t\ttcp_update_wl(tp, ack_seq);\n\n\t\tif (tp->snd_wnd != nwin) {\n\t\t\ttp->snd_wnd = nwin;\n\n\t\t\t \n\t\t\ttp->pred_flags = 0;\n\t\t\ttcp_fast_path_check(sk);\n\n\t\t\tif (!tcp_write_queue_empty(sk))\n\t\t\t\ttcp_slow_start_after_idle_check(sk);\n\n\t\t\tif (nwin > tp->max_window) {\n\t\t\t\ttp->max_window = nwin;\n\t\t\t\ttcp_sync_mss(sk, inet_csk(sk)->icsk_pmtu_cookie);\n\t\t\t}\n\t\t}\n\t}\n\n\ttcp_snd_una_update(tp, ack);\n\n\treturn flag;\n}\n\nstatic bool __tcp_oow_rate_limited(struct net *net, int mib_idx,\n\t\t\t\t   u32 *last_oow_ack_time)\n{\n\t \n\tu32 val = READ_ONCE(*last_oow_ack_time);\n\n\tif (val) {\n\t\ts32 elapsed = (s32)(tcp_jiffies32 - val);\n\n\t\tif (0 <= elapsed &&\n\t\t    elapsed < READ_ONCE(net->ipv4.sysctl_tcp_invalid_ratelimit)) {\n\t\t\tNET_INC_STATS(net, mib_idx);\n\t\t\treturn true;\t \n\t\t}\n\t}\n\n\t \n\tWRITE_ONCE(*last_oow_ack_time, tcp_jiffies32);\n\n\treturn false;\t \n}\n\n \nbool tcp_oow_rate_limited(struct net *net, const struct sk_buff *skb,\n\t\t\t  int mib_idx, u32 *last_oow_ack_time)\n{\n\t \n\tif ((TCP_SKB_CB(skb)->seq != TCP_SKB_CB(skb)->end_seq) &&\n\t    !tcp_hdr(skb)->syn)\n\t\treturn false;\n\n\treturn __tcp_oow_rate_limited(net, mib_idx, last_oow_ack_time);\n}\n\n \nstatic void tcp_send_challenge_ack(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tu32 count, now, ack_limit;\n\n\t \n\tif (__tcp_oow_rate_limited(net,\n\t\t\t\t   LINUX_MIB_TCPACKSKIPPEDCHALLENGE,\n\t\t\t\t   &tp->last_oow_ack_time))\n\t\treturn;\n\n\tack_limit = READ_ONCE(net->ipv4.sysctl_tcp_challenge_ack_limit);\n\tif (ack_limit == INT_MAX)\n\t\tgoto send_ack;\n\n\t \n\tnow = jiffies / HZ;\n\tif (now != READ_ONCE(net->ipv4.tcp_challenge_timestamp)) {\n\t\tu32 half = (ack_limit + 1) >> 1;\n\n\t\tWRITE_ONCE(net->ipv4.tcp_challenge_timestamp, now);\n\t\tWRITE_ONCE(net->ipv4.tcp_challenge_count,\n\t\t\t   get_random_u32_inclusive(half, ack_limit + half - 1));\n\t}\n\tcount = READ_ONCE(net->ipv4.tcp_challenge_count);\n\tif (count > 0) {\n\t\tWRITE_ONCE(net->ipv4.tcp_challenge_count, count - 1);\nsend_ack:\n\t\tNET_INC_STATS(net, LINUX_MIB_TCPCHALLENGEACK);\n\t\ttcp_send_ack(sk);\n\t}\n}\n\nstatic void tcp_store_ts_recent(struct tcp_sock *tp)\n{\n\ttp->rx_opt.ts_recent = tp->rx_opt.rcv_tsval;\n\ttp->rx_opt.ts_recent_stamp = ktime_get_seconds();\n}\n\nstatic void tcp_replace_ts_recent(struct tcp_sock *tp, u32 seq)\n{\n\tif (tp->rx_opt.saw_tstamp && !after(seq, tp->rcv_wup)) {\n\t\t \n\n\t\tif (tcp_paws_check(&tp->rx_opt, 0))\n\t\t\ttcp_store_ts_recent(tp);\n\t}\n}\n\n \nstatic void tcp_process_tlp_ack(struct sock *sk, u32 ack, int flag)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (before(ack, tp->tlp_high_seq))\n\t\treturn;\n\n\tif (!tp->tlp_retrans) {\n\t\t \n\t\ttp->tlp_high_seq = 0;\n\t} else if (flag & FLAG_DSACK_TLP) {\n\t\t \n\t\ttp->tlp_high_seq = 0;\n\t} else if (after(ack, tp->tlp_high_seq)) {\n\t\t \n\t\ttcp_init_cwnd_reduction(sk);\n\t\ttcp_set_ca_state(sk, TCP_CA_CWR);\n\t\ttcp_end_cwnd_reduction(sk);\n\t\ttcp_try_keep_open(sk);\n\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\tLINUX_MIB_TCPLOSSPROBERECOVERY);\n\t} else if (!(flag & (FLAG_SND_UNA_ADVANCED |\n\t\t\t     FLAG_NOT_DUP | FLAG_DATA_SACKED))) {\n\t\t \n\t\ttp->tlp_high_seq = 0;\n\t}\n}\n\nstatic inline void tcp_in_ack_event(struct sock *sk, u32 flags)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\n\tif (icsk->icsk_ca_ops->in_ack_event)\n\t\ticsk->icsk_ca_ops->in_ack_event(sk, flags);\n}\n\n \nstatic void tcp_xmit_recovery(struct sock *sk, int rexmit)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (rexmit == REXMIT_NONE || sk->sk_state == TCP_SYN_SENT)\n\t\treturn;\n\n\tif (unlikely(rexmit == REXMIT_NEW)) {\n\t\t__tcp_push_pending_frames(sk, tcp_current_mss(sk),\n\t\t\t\t\t  TCP_NAGLE_OFF);\n\t\tif (after(tp->snd_nxt, tp->high_seq))\n\t\t\treturn;\n\t\ttp->frto = 0;\n\t}\n\ttcp_xmit_retransmit_queue(sk);\n}\n\n \nstatic u32 tcp_newly_delivered(struct sock *sk, u32 prior_delivered, int flag)\n{\n\tconst struct net *net = sock_net(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 delivered;\n\n\tdelivered = tp->delivered - prior_delivered;\n\tNET_ADD_STATS(net, LINUX_MIB_TCPDELIVERED, delivered);\n\tif (flag & FLAG_ECE)\n\t\tNET_ADD_STATS(net, LINUX_MIB_TCPDELIVEREDCE, delivered);\n\n\treturn delivered;\n}\n\n \nstatic int tcp_ack(struct sock *sk, const struct sk_buff *skb, int flag)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct tcp_sacktag_state sack_state;\n\tstruct rate_sample rs = { .prior_delivered = 0 };\n\tu32 prior_snd_una = tp->snd_una;\n\tbool is_sack_reneg = tp->is_sack_reneg;\n\tu32 ack_seq = TCP_SKB_CB(skb)->seq;\n\tu32 ack = TCP_SKB_CB(skb)->ack_seq;\n\tint num_dupack = 0;\n\tint prior_packets = tp->packets_out;\n\tu32 delivered = tp->delivered;\n\tu32 lost = tp->lost;\n\tint rexmit = REXMIT_NONE;  \n\tu32 prior_fack;\n\n\tsack_state.first_sackt = 0;\n\tsack_state.rate = &rs;\n\tsack_state.sack_delivered = 0;\n\n\t \n\tprefetch(sk->tcp_rtx_queue.rb_node);\n\n\t \n\tif (before(ack, prior_snd_una)) {\n\t\tu32 max_window;\n\n\t\t \n\t\tmax_window = min_t(u64, tp->max_window, tp->bytes_acked);\n\t\t \n\t\tif (before(ack, prior_snd_una - max_window)) {\n\t\t\tif (!(flag & FLAG_NO_CHALLENGE_ACK))\n\t\t\t\ttcp_send_challenge_ack(sk);\n\t\t\treturn -SKB_DROP_REASON_TCP_TOO_OLD_ACK;\n\t\t}\n\t\tgoto old_ack;\n\t}\n\n\t \n\tif (after(ack, tp->snd_nxt))\n\t\treturn -SKB_DROP_REASON_TCP_ACK_UNSENT_DATA;\n\n\tif (after(ack, prior_snd_una)) {\n\t\tflag |= FLAG_SND_UNA_ADVANCED;\n\t\ticsk->icsk_retransmits = 0;\n\n#if IS_ENABLED(CONFIG_TLS_DEVICE)\n\t\tif (static_branch_unlikely(&clean_acked_data_enabled.key))\n\t\t\tif (icsk->icsk_clean_acked)\n\t\t\t\ticsk->icsk_clean_acked(sk, ack);\n#endif\n\t}\n\n\tprior_fack = tcp_is_sack(tp) ? tcp_highest_sack_seq(tp) : tp->snd_una;\n\trs.prior_in_flight = tcp_packets_in_flight(tp);\n\n\t \n\tif (flag & FLAG_UPDATE_TS_RECENT)\n\t\ttcp_replace_ts_recent(tp, TCP_SKB_CB(skb)->seq);\n\n\tif ((flag & (FLAG_SLOWPATH | FLAG_SND_UNA_ADVANCED)) ==\n\t    FLAG_SND_UNA_ADVANCED) {\n\t\t \n\t\ttcp_update_wl(tp, ack_seq);\n\t\ttcp_snd_una_update(tp, ack);\n\t\tflag |= FLAG_WIN_UPDATE;\n\n\t\ttcp_in_ack_event(sk, CA_ACK_WIN_UPDATE);\n\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPHPACKS);\n\t} else {\n\t\tu32 ack_ev_flags = CA_ACK_SLOWPATH;\n\n\t\tif (ack_seq != TCP_SKB_CB(skb)->end_seq)\n\t\t\tflag |= FLAG_DATA;\n\t\telse\n\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPPUREACKS);\n\n\t\tflag |= tcp_ack_update_window(sk, skb, ack, ack_seq);\n\n\t\tif (TCP_SKB_CB(skb)->sacked)\n\t\t\tflag |= tcp_sacktag_write_queue(sk, skb, prior_snd_una,\n\t\t\t\t\t\t\t&sack_state);\n\n\t\tif (tcp_ecn_rcv_ecn_echo(tp, tcp_hdr(skb))) {\n\t\t\tflag |= FLAG_ECE;\n\t\t\tack_ev_flags |= CA_ACK_ECE;\n\t\t}\n\n\t\tif (sack_state.sack_delivered)\n\t\t\ttcp_count_delivered(tp, sack_state.sack_delivered,\n\t\t\t\t\t    flag & FLAG_ECE);\n\n\t\tif (flag & FLAG_WIN_UPDATE)\n\t\t\tack_ev_flags |= CA_ACK_WIN_UPDATE;\n\n\t\ttcp_in_ack_event(sk, ack_ev_flags);\n\t}\n\n\t \n\ttcp_ecn_accept_cwr(sk, skb);\n\n\t \n\tWRITE_ONCE(sk->sk_err_soft, 0);\n\ticsk->icsk_probes_out = 0;\n\ttp->rcv_tstamp = tcp_jiffies32;\n\tif (!prior_packets)\n\t\tgoto no_queue;\n\n\t \n\tflag |= tcp_clean_rtx_queue(sk, skb, prior_fack, prior_snd_una,\n\t\t\t\t    &sack_state, flag & FLAG_ECE);\n\n\ttcp_rack_update_reo_wnd(sk, &rs);\n\n\tif (tp->tlp_high_seq)\n\t\ttcp_process_tlp_ack(sk, ack, flag);\n\n\tif (tcp_ack_is_dubious(sk, flag)) {\n\t\tif (!(flag & (FLAG_SND_UNA_ADVANCED |\n\t\t\t      FLAG_NOT_DUP | FLAG_DSACKING_ACK))) {\n\t\t\tnum_dupack = 1;\n\t\t\t \n\t\t\tif (!(flag & FLAG_DATA))\n\t\t\t\tnum_dupack = max_t(u16, 1, skb_shinfo(skb)->gso_segs);\n\t\t}\n\t\ttcp_fastretrans_alert(sk, prior_snd_una, num_dupack, &flag,\n\t\t\t\t      &rexmit);\n\t}\n\n\t \n\tif (flag & FLAG_SET_XMIT_TIMER)\n\t\ttcp_set_xmit_timer(sk);\n\n\tif ((flag & FLAG_FORWARD_PROGRESS) || !(flag & FLAG_NOT_DUP))\n\t\tsk_dst_confirm(sk);\n\n\tdelivered = tcp_newly_delivered(sk, delivered, flag);\n\tlost = tp->lost - lost;\t\t\t \n\trs.is_ack_delayed = !!(flag & FLAG_ACK_MAYBE_DELAYED);\n\ttcp_rate_gen(sk, delivered, lost, is_sack_reneg, sack_state.rate);\n\ttcp_cong_control(sk, ack, delivered, flag, sack_state.rate);\n\ttcp_xmit_recovery(sk, rexmit);\n\treturn 1;\n\nno_queue:\n\t \n\tif (flag & FLAG_DSACKING_ACK) {\n\t\ttcp_fastretrans_alert(sk, prior_snd_una, num_dupack, &flag,\n\t\t\t\t      &rexmit);\n\t\ttcp_newly_delivered(sk, delivered, flag);\n\t}\n\t \n\ttcp_ack_probe(sk);\n\n\tif (tp->tlp_high_seq)\n\t\ttcp_process_tlp_ack(sk, ack, flag);\n\treturn 1;\n\nold_ack:\n\t \n\tif (TCP_SKB_CB(skb)->sacked) {\n\t\tflag |= tcp_sacktag_write_queue(sk, skb, prior_snd_una,\n\t\t\t\t\t\t&sack_state);\n\t\ttcp_fastretrans_alert(sk, prior_snd_una, num_dupack, &flag,\n\t\t\t\t      &rexmit);\n\t\ttcp_newly_delivered(sk, delivered, flag);\n\t\ttcp_xmit_recovery(sk, rexmit);\n\t}\n\n\treturn 0;\n}\n\nstatic void tcp_parse_fastopen_option(int len, const unsigned char *cookie,\n\t\t\t\t      bool syn, struct tcp_fastopen_cookie *foc,\n\t\t\t\t      bool exp_opt)\n{\n\t \n\tif (!foc || !syn || len < 0 || (len & 1))\n\t\treturn;\n\n\tif (len >= TCP_FASTOPEN_COOKIE_MIN &&\n\t    len <= TCP_FASTOPEN_COOKIE_MAX)\n\t\tmemcpy(foc->val, cookie, len);\n\telse if (len != 0)\n\t\tlen = -1;\n\tfoc->len = len;\n\tfoc->exp = exp_opt;\n}\n\nstatic bool smc_parse_options(const struct tcphdr *th,\n\t\t\t      struct tcp_options_received *opt_rx,\n\t\t\t      const unsigned char *ptr,\n\t\t\t      int opsize)\n{\n#if IS_ENABLED(CONFIG_SMC)\n\tif (static_branch_unlikely(&tcp_have_smc)) {\n\t\tif (th->syn && !(opsize & 1) &&\n\t\t    opsize >= TCPOLEN_EXP_SMC_BASE &&\n\t\t    get_unaligned_be32(ptr) == TCPOPT_SMC_MAGIC) {\n\t\t\topt_rx->smc_ok = 1;\n\t\t\treturn true;\n\t\t}\n\t}\n#endif\n\treturn false;\n}\n\n \nu16 tcp_parse_mss_option(const struct tcphdr *th, u16 user_mss)\n{\n\tconst unsigned char *ptr = (const unsigned char *)(th + 1);\n\tint length = (th->doff * 4) - sizeof(struct tcphdr);\n\tu16 mss = 0;\n\n\twhile (length > 0) {\n\t\tint opcode = *ptr++;\n\t\tint opsize;\n\n\t\tswitch (opcode) {\n\t\tcase TCPOPT_EOL:\n\t\t\treturn mss;\n\t\tcase TCPOPT_NOP:\t \n\t\t\tlength--;\n\t\t\tcontinue;\n\t\tdefault:\n\t\t\tif (length < 2)\n\t\t\t\treturn mss;\n\t\t\topsize = *ptr++;\n\t\t\tif (opsize < 2)  \n\t\t\t\treturn mss;\n\t\t\tif (opsize > length)\n\t\t\t\treturn mss;\t \n\t\t\tif (opcode == TCPOPT_MSS && opsize == TCPOLEN_MSS) {\n\t\t\t\tu16 in_mss = get_unaligned_be16(ptr);\n\n\t\t\t\tif (in_mss) {\n\t\t\t\t\tif (user_mss && user_mss < in_mss)\n\t\t\t\t\t\tin_mss = user_mss;\n\t\t\t\t\tmss = in_mss;\n\t\t\t\t}\n\t\t\t}\n\t\t\tptr += opsize - 2;\n\t\t\tlength -= opsize;\n\t\t}\n\t}\n\treturn mss;\n}\nEXPORT_SYMBOL_GPL(tcp_parse_mss_option);\n\n \nvoid tcp_parse_options(const struct net *net,\n\t\t       const struct sk_buff *skb,\n\t\t       struct tcp_options_received *opt_rx, int estab,\n\t\t       struct tcp_fastopen_cookie *foc)\n{\n\tconst unsigned char *ptr;\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\tint length = (th->doff * 4) - sizeof(struct tcphdr);\n\n\tptr = (const unsigned char *)(th + 1);\n\topt_rx->saw_tstamp = 0;\n\topt_rx->saw_unknown = 0;\n\n\twhile (length > 0) {\n\t\tint opcode = *ptr++;\n\t\tint opsize;\n\n\t\tswitch (opcode) {\n\t\tcase TCPOPT_EOL:\n\t\t\treturn;\n\t\tcase TCPOPT_NOP:\t \n\t\t\tlength--;\n\t\t\tcontinue;\n\t\tdefault:\n\t\t\tif (length < 2)\n\t\t\t\treturn;\n\t\t\topsize = *ptr++;\n\t\t\tif (opsize < 2)  \n\t\t\t\treturn;\n\t\t\tif (opsize > length)\n\t\t\t\treturn;\t \n\t\t\tswitch (opcode) {\n\t\t\tcase TCPOPT_MSS:\n\t\t\t\tif (opsize == TCPOLEN_MSS && th->syn && !estab) {\n\t\t\t\t\tu16 in_mss = get_unaligned_be16(ptr);\n\t\t\t\t\tif (in_mss) {\n\t\t\t\t\t\tif (opt_rx->user_mss &&\n\t\t\t\t\t\t    opt_rx->user_mss < in_mss)\n\t\t\t\t\t\t\tin_mss = opt_rx->user_mss;\n\t\t\t\t\t\topt_rx->mss_clamp = in_mss;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase TCPOPT_WINDOW:\n\t\t\t\tif (opsize == TCPOLEN_WINDOW && th->syn &&\n\t\t\t\t    !estab && READ_ONCE(net->ipv4.sysctl_tcp_window_scaling)) {\n\t\t\t\t\t__u8 snd_wscale = *(__u8 *)ptr;\n\t\t\t\t\topt_rx->wscale_ok = 1;\n\t\t\t\t\tif (snd_wscale > TCP_MAX_WSCALE) {\n\t\t\t\t\t\tnet_info_ratelimited(\"%s: Illegal window scaling value %d > %u received\\n\",\n\t\t\t\t\t\t\t\t     __func__,\n\t\t\t\t\t\t\t\t     snd_wscale,\n\t\t\t\t\t\t\t\t     TCP_MAX_WSCALE);\n\t\t\t\t\t\tsnd_wscale = TCP_MAX_WSCALE;\n\t\t\t\t\t}\n\t\t\t\t\topt_rx->snd_wscale = snd_wscale;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase TCPOPT_TIMESTAMP:\n\t\t\t\tif ((opsize == TCPOLEN_TIMESTAMP) &&\n\t\t\t\t    ((estab && opt_rx->tstamp_ok) ||\n\t\t\t\t     (!estab && READ_ONCE(net->ipv4.sysctl_tcp_timestamps)))) {\n\t\t\t\t\topt_rx->saw_tstamp = 1;\n\t\t\t\t\topt_rx->rcv_tsval = get_unaligned_be32(ptr);\n\t\t\t\t\topt_rx->rcv_tsecr = get_unaligned_be32(ptr + 4);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase TCPOPT_SACK_PERM:\n\t\t\t\tif (opsize == TCPOLEN_SACK_PERM && th->syn &&\n\t\t\t\t    !estab && READ_ONCE(net->ipv4.sysctl_tcp_sack)) {\n\t\t\t\t\topt_rx->sack_ok = TCP_SACK_SEEN;\n\t\t\t\t\ttcp_sack_reset(opt_rx);\n\t\t\t\t}\n\t\t\t\tbreak;\n\n\t\t\tcase TCPOPT_SACK:\n\t\t\t\tif ((opsize >= (TCPOLEN_SACK_BASE + TCPOLEN_SACK_PERBLOCK)) &&\n\t\t\t\t   !((opsize - TCPOLEN_SACK_BASE) % TCPOLEN_SACK_PERBLOCK) &&\n\t\t\t\t   opt_rx->sack_ok) {\n\t\t\t\t\tTCP_SKB_CB(skb)->sacked = (ptr - 2) - (unsigned char *)th;\n\t\t\t\t}\n\t\t\t\tbreak;\n#ifdef CONFIG_TCP_MD5SIG\n\t\t\tcase TCPOPT_MD5SIG:\n\t\t\t\t \n\t\t\t\tbreak;\n#endif\n\t\t\tcase TCPOPT_FASTOPEN:\n\t\t\t\ttcp_parse_fastopen_option(\n\t\t\t\t\topsize - TCPOLEN_FASTOPEN_BASE,\n\t\t\t\t\tptr, th->syn, foc, false);\n\t\t\t\tbreak;\n\n\t\t\tcase TCPOPT_EXP:\n\t\t\t\t \n\t\t\t\tif (opsize >= TCPOLEN_EXP_FASTOPEN_BASE &&\n\t\t\t\t    get_unaligned_be16(ptr) ==\n\t\t\t\t    TCPOPT_FASTOPEN_MAGIC) {\n\t\t\t\t\ttcp_parse_fastopen_option(opsize -\n\t\t\t\t\t\tTCPOLEN_EXP_FASTOPEN_BASE,\n\t\t\t\t\t\tptr + 2, th->syn, foc, true);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\tif (smc_parse_options(th, opt_rx, ptr, opsize))\n\t\t\t\t\tbreak;\n\n\t\t\t\topt_rx->saw_unknown = 1;\n\t\t\t\tbreak;\n\n\t\t\tdefault:\n\t\t\t\topt_rx->saw_unknown = 1;\n\t\t\t}\n\t\t\tptr += opsize-2;\n\t\t\tlength -= opsize;\n\t\t}\n\t}\n}\nEXPORT_SYMBOL(tcp_parse_options);\n\nstatic bool tcp_parse_aligned_timestamp(struct tcp_sock *tp, const struct tcphdr *th)\n{\n\tconst __be32 *ptr = (const __be32 *)(th + 1);\n\n\tif (*ptr == htonl((TCPOPT_NOP << 24) | (TCPOPT_NOP << 16)\n\t\t\t  | (TCPOPT_TIMESTAMP << 8) | TCPOLEN_TIMESTAMP)) {\n\t\ttp->rx_opt.saw_tstamp = 1;\n\t\t++ptr;\n\t\ttp->rx_opt.rcv_tsval = ntohl(*ptr);\n\t\t++ptr;\n\t\tif (*ptr)\n\t\t\ttp->rx_opt.rcv_tsecr = ntohl(*ptr) - tp->tsoffset;\n\t\telse\n\t\t\ttp->rx_opt.rcv_tsecr = 0;\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n \nstatic bool tcp_fast_parse_options(const struct net *net,\n\t\t\t\t   const struct sk_buff *skb,\n\t\t\t\t   const struct tcphdr *th, struct tcp_sock *tp)\n{\n\t \n\tif (th->doff == (sizeof(*th) / 4)) {\n\t\ttp->rx_opt.saw_tstamp = 0;\n\t\treturn false;\n\t} else if (tp->rx_opt.tstamp_ok &&\n\t\t   th->doff == ((sizeof(*th) + TCPOLEN_TSTAMP_ALIGNED) / 4)) {\n\t\tif (tcp_parse_aligned_timestamp(tp, th))\n\t\t\treturn true;\n\t}\n\n\ttcp_parse_options(net, skb, &tp->rx_opt, 1, NULL);\n\tif (tp->rx_opt.saw_tstamp && tp->rx_opt.rcv_tsecr)\n\t\ttp->rx_opt.rcv_tsecr -= tp->tsoffset;\n\n\treturn true;\n}\n\n#ifdef CONFIG_TCP_MD5SIG\n \nconst u8 *tcp_parse_md5sig_option(const struct tcphdr *th)\n{\n\tint length = (th->doff << 2) - sizeof(*th);\n\tconst u8 *ptr = (const u8 *)(th + 1);\n\n\t \n\twhile (length >= TCPOLEN_MD5SIG) {\n\t\tint opcode = *ptr++;\n\t\tint opsize;\n\n\t\tswitch (opcode) {\n\t\tcase TCPOPT_EOL:\n\t\t\treturn NULL;\n\t\tcase TCPOPT_NOP:\n\t\t\tlength--;\n\t\t\tcontinue;\n\t\tdefault:\n\t\t\topsize = *ptr++;\n\t\t\tif (opsize < 2 || opsize > length)\n\t\t\t\treturn NULL;\n\t\t\tif (opcode == TCPOPT_MD5SIG)\n\t\t\t\treturn opsize == TCPOLEN_MD5SIG ? ptr : NULL;\n\t\t}\n\t\tptr += opsize - 2;\n\t\tlength -= opsize;\n\t}\n\treturn NULL;\n}\nEXPORT_SYMBOL(tcp_parse_md5sig_option);\n#endif\n\n \n\nstatic int tcp_disordered_ack(const struct sock *sk, const struct sk_buff *skb)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\tu32 seq = TCP_SKB_CB(skb)->seq;\n\tu32 ack = TCP_SKB_CB(skb)->ack_seq;\n\n\treturn ( \n\t\t(th->ack && seq == TCP_SKB_CB(skb)->end_seq && seq == tp->rcv_nxt) &&\n\n\t\t \n\t\tack == tp->snd_una &&\n\n\t\t \n\t\t!tcp_may_update_window(tp, ack, seq, ntohs(th->window) << tp->rx_opt.snd_wscale) &&\n\n\t\t \n\t\t(s32)(tp->rx_opt.ts_recent - tp->rx_opt.rcv_tsval) <= (inet_csk(sk)->icsk_rto * 1024) / HZ);\n}\n\nstatic inline bool tcp_paws_discard(const struct sock *sk,\n\t\t\t\t   const struct sk_buff *skb)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\n\treturn !tcp_paws_check(&tp->rx_opt, TCP_PAWS_WINDOW) &&\n\t       !tcp_disordered_ack(sk, skb);\n}\n\n \n\nstatic enum skb_drop_reason tcp_sequence(const struct tcp_sock *tp,\n\t\t\t\t\t u32 seq, u32 end_seq)\n{\n\tif (before(end_seq, tp->rcv_wup))\n\t\treturn SKB_DROP_REASON_TCP_OLD_SEQUENCE;\n\n\tif (after(seq, tp->rcv_nxt + tcp_receive_window(tp)))\n\t\treturn SKB_DROP_REASON_TCP_INVALID_SEQUENCE;\n\n\treturn SKB_NOT_DROPPED_YET;\n}\n\n \nvoid tcp_reset(struct sock *sk, struct sk_buff *skb)\n{\n\ttrace_tcp_receive_reset(sk);\n\n\t \n\tif (sk_is_mptcp(sk))\n\t\tmptcp_incoming_options(sk, skb);\n\n\t \n\tswitch (sk->sk_state) {\n\tcase TCP_SYN_SENT:\n\t\tWRITE_ONCE(sk->sk_err, ECONNREFUSED);\n\t\tbreak;\n\tcase TCP_CLOSE_WAIT:\n\t\tWRITE_ONCE(sk->sk_err, EPIPE);\n\t\tbreak;\n\tcase TCP_CLOSE:\n\t\treturn;\n\tdefault:\n\t\tWRITE_ONCE(sk->sk_err, ECONNRESET);\n\t}\n\t \n\tsmp_wmb();\n\n\ttcp_write_queue_purge(sk);\n\ttcp_done(sk);\n\n\tif (!sock_flag(sk, SOCK_DEAD))\n\t\tsk_error_report(sk);\n}\n\n \nvoid tcp_fin(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tinet_csk_schedule_ack(sk);\n\n\tWRITE_ONCE(sk->sk_shutdown, sk->sk_shutdown | RCV_SHUTDOWN);\n\tsock_set_flag(sk, SOCK_DONE);\n\n\tswitch (sk->sk_state) {\n\tcase TCP_SYN_RECV:\n\tcase TCP_ESTABLISHED:\n\t\t \n\t\ttcp_set_state(sk, TCP_CLOSE_WAIT);\n\t\tinet_csk_enter_pingpong_mode(sk);\n\t\tbreak;\n\n\tcase TCP_CLOSE_WAIT:\n\tcase TCP_CLOSING:\n\t\t \n\t\tbreak;\n\tcase TCP_LAST_ACK:\n\t\t \n\t\tbreak;\n\n\tcase TCP_FIN_WAIT1:\n\t\t \n\t\ttcp_send_ack(sk);\n\t\ttcp_set_state(sk, TCP_CLOSING);\n\t\tbreak;\n\tcase TCP_FIN_WAIT2:\n\t\t \n\t\ttcp_send_ack(sk);\n\t\ttcp_time_wait(sk, TCP_TIME_WAIT, 0);\n\t\tbreak;\n\tdefault:\n\t\t \n\t\tpr_err(\"%s: Impossible, sk->sk_state=%d\\n\",\n\t\t       __func__, sk->sk_state);\n\t\tbreak;\n\t}\n\n\t \n\tskb_rbtree_purge(&tp->out_of_order_queue);\n\tif (tcp_is_sack(tp))\n\t\ttcp_sack_reset(&tp->rx_opt);\n\n\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\tsk->sk_state_change(sk);\n\n\t\t \n\t\tif (sk->sk_shutdown == SHUTDOWN_MASK ||\n\t\t    sk->sk_state == TCP_CLOSE)\n\t\t\tsk_wake_async(sk, SOCK_WAKE_WAITD, POLL_HUP);\n\t\telse\n\t\t\tsk_wake_async(sk, SOCK_WAKE_WAITD, POLL_IN);\n\t}\n}\n\nstatic inline bool tcp_sack_extend(struct tcp_sack_block *sp, u32 seq,\n\t\t\t\t  u32 end_seq)\n{\n\tif (!after(seq, sp->end_seq) && !after(sp->start_seq, end_seq)) {\n\t\tif (before(seq, sp->start_seq))\n\t\t\tsp->start_seq = seq;\n\t\tif (after(end_seq, sp->end_seq))\n\t\t\tsp->end_seq = end_seq;\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic void tcp_dsack_set(struct sock *sk, u32 seq, u32 end_seq)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (tcp_is_sack(tp) && READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_dsack)) {\n\t\tint mib_idx;\n\n\t\tif (before(seq, tp->rcv_nxt))\n\t\t\tmib_idx = LINUX_MIB_TCPDSACKOLDSENT;\n\t\telse\n\t\t\tmib_idx = LINUX_MIB_TCPDSACKOFOSENT;\n\n\t\tNET_INC_STATS(sock_net(sk), mib_idx);\n\n\t\ttp->rx_opt.dsack = 1;\n\t\ttp->duplicate_sack[0].start_seq = seq;\n\t\ttp->duplicate_sack[0].end_seq = end_seq;\n\t}\n}\n\nstatic void tcp_dsack_extend(struct sock *sk, u32 seq, u32 end_seq)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (!tp->rx_opt.dsack)\n\t\ttcp_dsack_set(sk, seq, end_seq);\n\telse\n\t\ttcp_sack_extend(tp->duplicate_sack, seq, end_seq);\n}\n\nstatic void tcp_rcv_spurious_retrans(struct sock *sk, const struct sk_buff *skb)\n{\n\t \n\tif (TCP_SKB_CB(skb)->seq == tcp_sk(sk)->duplicate_sack[0].start_seq &&\n\t    sk_rethink_txhash(sk))\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPDUPLICATEDATAREHASH);\n}\n\nstatic void tcp_send_dupack(struct sock *sk, const struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (TCP_SKB_CB(skb)->end_seq != TCP_SKB_CB(skb)->seq &&\n\t    before(TCP_SKB_CB(skb)->seq, tp->rcv_nxt)) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_DELAYEDACKLOST);\n\t\ttcp_enter_quickack_mode(sk, TCP_MAX_QUICKACKS);\n\n\t\tif (tcp_is_sack(tp) && READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_dsack)) {\n\t\t\tu32 end_seq = TCP_SKB_CB(skb)->end_seq;\n\n\t\t\ttcp_rcv_spurious_retrans(sk, skb);\n\t\t\tif (after(TCP_SKB_CB(skb)->end_seq, tp->rcv_nxt))\n\t\t\t\tend_seq = tp->rcv_nxt;\n\t\t\ttcp_dsack_set(sk, TCP_SKB_CB(skb)->seq, end_seq);\n\t\t}\n\t}\n\n\ttcp_send_ack(sk);\n}\n\n \nstatic void tcp_sack_maybe_coalesce(struct tcp_sock *tp)\n{\n\tint this_sack;\n\tstruct tcp_sack_block *sp = &tp->selective_acks[0];\n\tstruct tcp_sack_block *swalk = sp + 1;\n\n\t \n\tfor (this_sack = 1; this_sack < tp->rx_opt.num_sacks;) {\n\t\tif (tcp_sack_extend(sp, swalk->start_seq, swalk->end_seq)) {\n\t\t\tint i;\n\n\t\t\t \n\t\t\ttp->rx_opt.num_sacks--;\n\t\t\tfor (i = this_sack; i < tp->rx_opt.num_sacks; i++)\n\t\t\t\tsp[i] = sp[i + 1];\n\t\t\tcontinue;\n\t\t}\n\t\tthis_sack++;\n\t\tswalk++;\n\t}\n}\n\nvoid tcp_sack_compress_send_ack(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (!tp->compressed_ack)\n\t\treturn;\n\n\tif (hrtimer_try_to_cancel(&tp->compressed_ack_timer) == 1)\n\t\t__sock_put(sk);\n\n\t \n\tNET_ADD_STATS(sock_net(sk), LINUX_MIB_TCPACKCOMPRESSED,\n\t\t      tp->compressed_ack - 1);\n\n\ttp->compressed_ack = 0;\n\ttcp_send_ack(sk);\n}\n\n \n#define TCP_SACK_BLOCKS_EXPECTED 2\n\nstatic void tcp_sack_new_ofo_skb(struct sock *sk, u32 seq, u32 end_seq)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct tcp_sack_block *sp = &tp->selective_acks[0];\n\tint cur_sacks = tp->rx_opt.num_sacks;\n\tint this_sack;\n\n\tif (!cur_sacks)\n\t\tgoto new_sack;\n\n\tfor (this_sack = 0; this_sack < cur_sacks; this_sack++, sp++) {\n\t\tif (tcp_sack_extend(sp, seq, end_seq)) {\n\t\t\tif (this_sack >= TCP_SACK_BLOCKS_EXPECTED)\n\t\t\t\ttcp_sack_compress_send_ack(sk);\n\t\t\t \n\t\t\tfor (; this_sack > 0; this_sack--, sp--)\n\t\t\t\tswap(*sp, *(sp - 1));\n\t\t\tif (cur_sacks > 1)\n\t\t\t\ttcp_sack_maybe_coalesce(tp);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tif (this_sack >= TCP_SACK_BLOCKS_EXPECTED)\n\t\ttcp_sack_compress_send_ack(sk);\n\n\t \n\tif (this_sack >= TCP_NUM_SACKS) {\n\t\tthis_sack--;\n\t\ttp->rx_opt.num_sacks--;\n\t\tsp--;\n\t}\n\tfor (; this_sack > 0; this_sack--, sp--)\n\t\t*sp = *(sp - 1);\n\nnew_sack:\n\t \n\tsp->start_seq = seq;\n\tsp->end_seq = end_seq;\n\ttp->rx_opt.num_sacks++;\n}\n\n \n\nstatic void tcp_sack_remove(struct tcp_sock *tp)\n{\n\tstruct tcp_sack_block *sp = &tp->selective_acks[0];\n\tint num_sacks = tp->rx_opt.num_sacks;\n\tint this_sack;\n\n\t \n\tif (RB_EMPTY_ROOT(&tp->out_of_order_queue)) {\n\t\ttp->rx_opt.num_sacks = 0;\n\t\treturn;\n\t}\n\n\tfor (this_sack = 0; this_sack < num_sacks;) {\n\t\t \n\t\tif (!before(tp->rcv_nxt, sp->start_seq)) {\n\t\t\tint i;\n\n\t\t\t \n\t\t\tWARN_ON(before(tp->rcv_nxt, sp->end_seq));\n\n\t\t\t \n\t\t\tfor (i = this_sack+1; i < num_sacks; i++)\n\t\t\t\ttp->selective_acks[i-1] = tp->selective_acks[i];\n\t\t\tnum_sacks--;\n\t\t\tcontinue;\n\t\t}\n\t\tthis_sack++;\n\t\tsp++;\n\t}\n\ttp->rx_opt.num_sacks = num_sacks;\n}\n\n \nstatic bool tcp_try_coalesce(struct sock *sk,\n\t\t\t     struct sk_buff *to,\n\t\t\t     struct sk_buff *from,\n\t\t\t     bool *fragstolen)\n{\n\tint delta;\n\n\t*fragstolen = false;\n\n\t \n\tif (TCP_SKB_CB(from)->seq != TCP_SKB_CB(to)->end_seq)\n\t\treturn false;\n\n\tif (!mptcp_skb_can_collapse(to, from))\n\t\treturn false;\n\n#ifdef CONFIG_TLS_DEVICE\n\tif (from->decrypted != to->decrypted)\n\t\treturn false;\n#endif\n\n\tif (!skb_try_coalesce(to, from, fragstolen, &delta))\n\t\treturn false;\n\n\tatomic_add(delta, &sk->sk_rmem_alloc);\n\tsk_mem_charge(sk, delta);\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPRCVCOALESCE);\n\tTCP_SKB_CB(to)->end_seq = TCP_SKB_CB(from)->end_seq;\n\tTCP_SKB_CB(to)->ack_seq = TCP_SKB_CB(from)->ack_seq;\n\tTCP_SKB_CB(to)->tcp_flags |= TCP_SKB_CB(from)->tcp_flags;\n\n\tif (TCP_SKB_CB(from)->has_rxtstamp) {\n\t\tTCP_SKB_CB(to)->has_rxtstamp = true;\n\t\tto->tstamp = from->tstamp;\n\t\tskb_hwtstamps(to)->hwtstamp = skb_hwtstamps(from)->hwtstamp;\n\t}\n\n\treturn true;\n}\n\nstatic bool tcp_ooo_try_coalesce(struct sock *sk,\n\t\t\t     struct sk_buff *to,\n\t\t\t     struct sk_buff *from,\n\t\t\t     bool *fragstolen)\n{\n\tbool res = tcp_try_coalesce(sk, to, from, fragstolen);\n\n\t \n\tif (res) {\n\t\tu32 gso_segs = max_t(u16, 1, skb_shinfo(to)->gso_segs) +\n\t\t\t       max_t(u16, 1, skb_shinfo(from)->gso_segs);\n\n\t\tskb_shinfo(to)->gso_segs = min_t(u32, gso_segs, 0xFFFF);\n\t}\n\treturn res;\n}\n\nstatic void tcp_drop_reason(struct sock *sk, struct sk_buff *skb,\n\t\t\t    enum skb_drop_reason reason)\n{\n\tsk_drops_add(sk, skb);\n\tkfree_skb_reason(skb, reason);\n}\n\n \nstatic void tcp_ofo_queue(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\t__u32 dsack_high = tp->rcv_nxt;\n\tbool fin, fragstolen, eaten;\n\tstruct sk_buff *skb, *tail;\n\tstruct rb_node *p;\n\n\tp = rb_first(&tp->out_of_order_queue);\n\twhile (p) {\n\t\tskb = rb_to_skb(p);\n\t\tif (after(TCP_SKB_CB(skb)->seq, tp->rcv_nxt))\n\t\t\tbreak;\n\n\t\tif (before(TCP_SKB_CB(skb)->seq, dsack_high)) {\n\t\t\t__u32 dsack = dsack_high;\n\t\t\tif (before(TCP_SKB_CB(skb)->end_seq, dsack_high))\n\t\t\t\tdsack_high = TCP_SKB_CB(skb)->end_seq;\n\t\t\ttcp_dsack_extend(sk, TCP_SKB_CB(skb)->seq, dsack);\n\t\t}\n\t\tp = rb_next(p);\n\t\trb_erase(&skb->rbnode, &tp->out_of_order_queue);\n\n\t\tif (unlikely(!after(TCP_SKB_CB(skb)->end_seq, tp->rcv_nxt))) {\n\t\t\ttcp_drop_reason(sk, skb, SKB_DROP_REASON_TCP_OFO_DROP);\n\t\t\tcontinue;\n\t\t}\n\n\t\ttail = skb_peek_tail(&sk->sk_receive_queue);\n\t\teaten = tail && tcp_try_coalesce(sk, tail, skb, &fragstolen);\n\t\ttcp_rcv_nxt_update(tp, TCP_SKB_CB(skb)->end_seq);\n\t\tfin = TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN;\n\t\tif (!eaten)\n\t\t\t__skb_queue_tail(&sk->sk_receive_queue, skb);\n\t\telse\n\t\t\tkfree_skb_partial(skb, fragstolen);\n\n\t\tif (unlikely(fin)) {\n\t\t\ttcp_fin(sk);\n\t\t\t \n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic bool tcp_prune_ofo_queue(struct sock *sk, const struct sk_buff *in_skb);\nstatic int tcp_prune_queue(struct sock *sk, const struct sk_buff *in_skb);\n\nstatic int tcp_try_rmem_schedule(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t unsigned int size)\n{\n\tif (atomic_read(&sk->sk_rmem_alloc) > sk->sk_rcvbuf ||\n\t    !sk_rmem_schedule(sk, skb, size)) {\n\n\t\tif (tcp_prune_queue(sk, skb) < 0)\n\t\t\treturn -1;\n\n\t\twhile (!sk_rmem_schedule(sk, skb, size)) {\n\t\t\tif (!tcp_prune_ofo_queue(sk, skb))\n\t\t\t\treturn -1;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic void tcp_data_queue_ofo(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct rb_node **p, *parent;\n\tstruct sk_buff *skb1;\n\tu32 seq, end_seq;\n\tbool fragstolen;\n\n\ttcp_ecn_check_ce(sk, skb);\n\n\tif (unlikely(tcp_try_rmem_schedule(sk, skb, skb->truesize))) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPOFODROP);\n\t\tsk->sk_data_ready(sk);\n\t\ttcp_drop_reason(sk, skb, SKB_DROP_REASON_PROTO_MEM);\n\t\treturn;\n\t}\n\n\t \n\ttp->pred_flags = 0;\n\tinet_csk_schedule_ack(sk);\n\n\ttp->rcv_ooopack += max_t(u16, 1, skb_shinfo(skb)->gso_segs);\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPOFOQUEUE);\n\tseq = TCP_SKB_CB(skb)->seq;\n\tend_seq = TCP_SKB_CB(skb)->end_seq;\n\n\tp = &tp->out_of_order_queue.rb_node;\n\tif (RB_EMPTY_ROOT(&tp->out_of_order_queue)) {\n\t\t \n\t\tif (tcp_is_sack(tp)) {\n\t\t\ttp->rx_opt.num_sacks = 1;\n\t\t\ttp->selective_acks[0].start_seq = seq;\n\t\t\ttp->selective_acks[0].end_seq = end_seq;\n\t\t}\n\t\trb_link_node(&skb->rbnode, NULL, p);\n\t\trb_insert_color(&skb->rbnode, &tp->out_of_order_queue);\n\t\ttp->ooo_last_skb = skb;\n\t\tgoto end;\n\t}\n\n\t \n\tif (tcp_ooo_try_coalesce(sk, tp->ooo_last_skb,\n\t\t\t\t skb, &fragstolen)) {\ncoalesce_done:\n\t\t \n\t\tif (tcp_is_sack(tp))\n\t\t\ttcp_grow_window(sk, skb, true);\n\t\tkfree_skb_partial(skb, fragstolen);\n\t\tskb = NULL;\n\t\tgoto add_sack;\n\t}\n\t \n\tif (!before(seq, TCP_SKB_CB(tp->ooo_last_skb)->end_seq)) {\n\t\tparent = &tp->ooo_last_skb->rbnode;\n\t\tp = &parent->rb_right;\n\t\tgoto insert;\n\t}\n\n\t \n\tparent = NULL;\n\twhile (*p) {\n\t\tparent = *p;\n\t\tskb1 = rb_to_skb(parent);\n\t\tif (before(seq, TCP_SKB_CB(skb1)->seq)) {\n\t\t\tp = &parent->rb_left;\n\t\t\tcontinue;\n\t\t}\n\t\tif (before(seq, TCP_SKB_CB(skb1)->end_seq)) {\n\t\t\tif (!after(end_seq, TCP_SKB_CB(skb1)->end_seq)) {\n\t\t\t\t \n\t\t\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\t\t      LINUX_MIB_TCPOFOMERGE);\n\t\t\t\ttcp_drop_reason(sk, skb,\n\t\t\t\t\t\tSKB_DROP_REASON_TCP_OFOMERGE);\n\t\t\t\tskb = NULL;\n\t\t\t\ttcp_dsack_set(sk, seq, end_seq);\n\t\t\t\tgoto add_sack;\n\t\t\t}\n\t\t\tif (after(seq, TCP_SKB_CB(skb1)->seq)) {\n\t\t\t\t \n\t\t\t\ttcp_dsack_set(sk, seq, TCP_SKB_CB(skb1)->end_seq);\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\trb_replace_node(&skb1->rbnode, &skb->rbnode,\n\t\t\t\t\t\t&tp->out_of_order_queue);\n\t\t\t\ttcp_dsack_extend(sk,\n\t\t\t\t\t\t TCP_SKB_CB(skb1)->seq,\n\t\t\t\t\t\t TCP_SKB_CB(skb1)->end_seq);\n\t\t\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\t\t      LINUX_MIB_TCPOFOMERGE);\n\t\t\t\ttcp_drop_reason(sk, skb1,\n\t\t\t\t\t\tSKB_DROP_REASON_TCP_OFOMERGE);\n\t\t\t\tgoto merge_right;\n\t\t\t}\n\t\t} else if (tcp_ooo_try_coalesce(sk, skb1,\n\t\t\t\t\t\tskb, &fragstolen)) {\n\t\t\tgoto coalesce_done;\n\t\t}\n\t\tp = &parent->rb_right;\n\t}\ninsert:\n\t \n\trb_link_node(&skb->rbnode, parent, p);\n\trb_insert_color(&skb->rbnode, &tp->out_of_order_queue);\n\nmerge_right:\n\t \n\twhile ((skb1 = skb_rb_next(skb)) != NULL) {\n\t\tif (!after(end_seq, TCP_SKB_CB(skb1)->seq))\n\t\t\tbreak;\n\t\tif (before(end_seq, TCP_SKB_CB(skb1)->end_seq)) {\n\t\t\ttcp_dsack_extend(sk, TCP_SKB_CB(skb1)->seq,\n\t\t\t\t\t end_seq);\n\t\t\tbreak;\n\t\t}\n\t\trb_erase(&skb1->rbnode, &tp->out_of_order_queue);\n\t\ttcp_dsack_extend(sk, TCP_SKB_CB(skb1)->seq,\n\t\t\t\t TCP_SKB_CB(skb1)->end_seq);\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPOFOMERGE);\n\t\ttcp_drop_reason(sk, skb1, SKB_DROP_REASON_TCP_OFOMERGE);\n\t}\n\t \n\tif (!skb1)\n\t\ttp->ooo_last_skb = skb;\n\nadd_sack:\n\tif (tcp_is_sack(tp))\n\t\ttcp_sack_new_ofo_skb(sk, seq, end_seq);\nend:\n\tif (skb) {\n\t\t \n\t\tif (tcp_is_sack(tp))\n\t\t\ttcp_grow_window(sk, skb, false);\n\t\tskb_condense(skb);\n\t\tskb_set_owner_r(skb, sk);\n\t}\n}\n\nstatic int __must_check tcp_queue_rcv(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t      bool *fragstolen)\n{\n\tint eaten;\n\tstruct sk_buff *tail = skb_peek_tail(&sk->sk_receive_queue);\n\n\teaten = (tail &&\n\t\t tcp_try_coalesce(sk, tail,\n\t\t\t\t  skb, fragstolen)) ? 1 : 0;\n\ttcp_rcv_nxt_update(tcp_sk(sk), TCP_SKB_CB(skb)->end_seq);\n\tif (!eaten) {\n\t\t__skb_queue_tail(&sk->sk_receive_queue, skb);\n\t\tskb_set_owner_r(skb, sk);\n\t}\n\treturn eaten;\n}\n\nint tcp_send_rcvq(struct sock *sk, struct msghdr *msg, size_t size)\n{\n\tstruct sk_buff *skb;\n\tint err = -ENOMEM;\n\tint data_len = 0;\n\tbool fragstolen;\n\n\tif (size == 0)\n\t\treturn 0;\n\n\tif (size > PAGE_SIZE) {\n\t\tint npages = min_t(size_t, size >> PAGE_SHIFT, MAX_SKB_FRAGS);\n\n\t\tdata_len = npages << PAGE_SHIFT;\n\t\tsize = data_len + (size & ~PAGE_MASK);\n\t}\n\tskb = alloc_skb_with_frags(size - data_len, data_len,\n\t\t\t\t   PAGE_ALLOC_COSTLY_ORDER,\n\t\t\t\t   &err, sk->sk_allocation);\n\tif (!skb)\n\t\tgoto err;\n\n\tskb_put(skb, size - data_len);\n\tskb->data_len = data_len;\n\tskb->len = size;\n\n\tif (tcp_try_rmem_schedule(sk, skb, skb->truesize)) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPRCVQDROP);\n\t\tgoto err_free;\n\t}\n\n\terr = skb_copy_datagram_from_iter(skb, 0, &msg->msg_iter, size);\n\tif (err)\n\t\tgoto err_free;\n\n\tTCP_SKB_CB(skb)->seq = tcp_sk(sk)->rcv_nxt;\n\tTCP_SKB_CB(skb)->end_seq = TCP_SKB_CB(skb)->seq + size;\n\tTCP_SKB_CB(skb)->ack_seq = tcp_sk(sk)->snd_una - 1;\n\n\tif (tcp_queue_rcv(sk, skb, &fragstolen)) {\n\t\tWARN_ON_ONCE(fragstolen);  \n\t\t__kfree_skb(skb);\n\t}\n\treturn size;\n\nerr_free:\n\tkfree_skb(skb);\nerr:\n\treturn err;\n\n}\n\nvoid tcp_data_ready(struct sock *sk)\n{\n\tif (tcp_epollin_ready(sk, sk->sk_rcvlowat) || sock_flag(sk, SOCK_DONE))\n\t\tsk->sk_data_ready(sk);\n}\n\nstatic void tcp_data_queue(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tenum skb_drop_reason reason;\n\tbool fragstolen;\n\tint eaten;\n\n\t \n\tif (sk_is_mptcp(sk) && !mptcp_incoming_options(sk, skb)) {\n\t\t__kfree_skb(skb);\n\t\treturn;\n\t}\n\n\tif (TCP_SKB_CB(skb)->seq == TCP_SKB_CB(skb)->end_seq) {\n\t\t__kfree_skb(skb);\n\t\treturn;\n\t}\n\tskb_dst_drop(skb);\n\t__skb_pull(skb, tcp_hdr(skb)->doff * 4);\n\n\treason = SKB_DROP_REASON_NOT_SPECIFIED;\n\ttp->rx_opt.dsack = 0;\n\n\t \n\tif (TCP_SKB_CB(skb)->seq == tp->rcv_nxt) {\n\t\tif (tcp_receive_window(tp) == 0) {\n\t\t\treason = SKB_DROP_REASON_TCP_ZEROWINDOW;\n\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPZEROWINDOWDROP);\n\t\t\tgoto out_of_window;\n\t\t}\n\n\t\t \nqueue_and_out:\n\t\tif (tcp_try_rmem_schedule(sk, skb, skb->truesize)) {\n\t\t\t \n\t\t\tinet_csk(sk)->icsk_ack.pending |=\n\t\t\t\t\t(ICSK_ACK_NOMEM | ICSK_ACK_NOW);\n\t\t\tinet_csk_schedule_ack(sk);\n\t\t\tsk->sk_data_ready(sk);\n\n\t\t\tif (skb_queue_len(&sk->sk_receive_queue)) {\n\t\t\t\treason = SKB_DROP_REASON_PROTO_MEM;\n\t\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPRCVQDROP);\n\t\t\t\tgoto drop;\n\t\t\t}\n\t\t\tsk_forced_mem_schedule(sk, skb->truesize);\n\t\t}\n\n\t\teaten = tcp_queue_rcv(sk, skb, &fragstolen);\n\t\tif (skb->len)\n\t\t\ttcp_event_data_recv(sk, skb);\n\t\tif (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN)\n\t\t\ttcp_fin(sk);\n\n\t\tif (!RB_EMPTY_ROOT(&tp->out_of_order_queue)) {\n\t\t\ttcp_ofo_queue(sk);\n\n\t\t\t \n\t\t\tif (RB_EMPTY_ROOT(&tp->out_of_order_queue))\n\t\t\t\tinet_csk(sk)->icsk_ack.pending |= ICSK_ACK_NOW;\n\t\t}\n\n\t\tif (tp->rx_opt.num_sacks)\n\t\t\ttcp_sack_remove(tp);\n\n\t\ttcp_fast_path_check(sk);\n\n\t\tif (eaten > 0)\n\t\t\tkfree_skb_partial(skb, fragstolen);\n\t\tif (!sock_flag(sk, SOCK_DEAD))\n\t\t\ttcp_data_ready(sk);\n\t\treturn;\n\t}\n\n\tif (!after(TCP_SKB_CB(skb)->end_seq, tp->rcv_nxt)) {\n\t\ttcp_rcv_spurious_retrans(sk, skb);\n\t\t \n\t\treason = SKB_DROP_REASON_TCP_OLD_DATA;\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_DELAYEDACKLOST);\n\t\ttcp_dsack_set(sk, TCP_SKB_CB(skb)->seq, TCP_SKB_CB(skb)->end_seq);\n\nout_of_window:\n\t\ttcp_enter_quickack_mode(sk, TCP_MAX_QUICKACKS);\n\t\tinet_csk_schedule_ack(sk);\ndrop:\n\t\ttcp_drop_reason(sk, skb, reason);\n\t\treturn;\n\t}\n\n\t \n\tif (!before(TCP_SKB_CB(skb)->seq,\n\t\t    tp->rcv_nxt + tcp_receive_window(tp))) {\n\t\treason = SKB_DROP_REASON_TCP_OVERWINDOW;\n\t\tgoto out_of_window;\n\t}\n\n\tif (before(TCP_SKB_CB(skb)->seq, tp->rcv_nxt)) {\n\t\t \n\t\ttcp_dsack_set(sk, TCP_SKB_CB(skb)->seq, tp->rcv_nxt);\n\n\t\t \n\t\tif (!tcp_receive_window(tp)) {\n\t\t\treason = SKB_DROP_REASON_TCP_ZEROWINDOW;\n\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPZEROWINDOWDROP);\n\t\t\tgoto out_of_window;\n\t\t}\n\t\tgoto queue_and_out;\n\t}\n\n\ttcp_data_queue_ofo(sk, skb);\n}\n\nstatic struct sk_buff *tcp_skb_next(struct sk_buff *skb, struct sk_buff_head *list)\n{\n\tif (list)\n\t\treturn !skb_queue_is_last(list, skb) ? skb->next : NULL;\n\n\treturn skb_rb_next(skb);\n}\n\nstatic struct sk_buff *tcp_collapse_one(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t\tstruct sk_buff_head *list,\n\t\t\t\t\tstruct rb_root *root)\n{\n\tstruct sk_buff *next = tcp_skb_next(skb, list);\n\n\tif (list)\n\t\t__skb_unlink(skb, list);\n\telse\n\t\trb_erase(&skb->rbnode, root);\n\n\t__kfree_skb(skb);\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPRCVCOLLAPSED);\n\n\treturn next;\n}\n\n \nvoid tcp_rbtree_insert(struct rb_root *root, struct sk_buff *skb)\n{\n\tstruct rb_node **p = &root->rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct sk_buff *skb1;\n\n\twhile (*p) {\n\t\tparent = *p;\n\t\tskb1 = rb_to_skb(parent);\n\t\tif (before(TCP_SKB_CB(skb)->seq, TCP_SKB_CB(skb1)->seq))\n\t\t\tp = &parent->rb_left;\n\t\telse\n\t\t\tp = &parent->rb_right;\n\t}\n\trb_link_node(&skb->rbnode, parent, p);\n\trb_insert_color(&skb->rbnode, root);\n}\n\n \nstatic void\ntcp_collapse(struct sock *sk, struct sk_buff_head *list, struct rb_root *root,\n\t     struct sk_buff *head, struct sk_buff *tail, u32 start, u32 end)\n{\n\tstruct sk_buff *skb = head, *n;\n\tstruct sk_buff_head tmp;\n\tbool end_of_skbs;\n\n\t \nrestart:\n\tfor (end_of_skbs = true; skb != NULL && skb != tail; skb = n) {\n\t\tn = tcp_skb_next(skb, list);\n\n\t\t \n\t\tif (!before(start, TCP_SKB_CB(skb)->end_seq)) {\n\t\t\tskb = tcp_collapse_one(sk, skb, list, root);\n\t\t\tif (!skb)\n\t\t\t\tbreak;\n\t\t\tgoto restart;\n\t\t}\n\n\t\t \n\t\tif (!(TCP_SKB_CB(skb)->tcp_flags & (TCPHDR_SYN | TCPHDR_FIN)) &&\n\t\t    (tcp_win_from_space(sk, skb->truesize) > skb->len ||\n\t\t     before(TCP_SKB_CB(skb)->seq, start))) {\n\t\t\tend_of_skbs = false;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (n && n != tail && mptcp_skb_can_collapse(skb, n) &&\n\t\t    TCP_SKB_CB(skb)->end_seq != TCP_SKB_CB(n)->seq) {\n\t\t\tend_of_skbs = false;\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tstart = TCP_SKB_CB(skb)->end_seq;\n\t}\n\tif (end_of_skbs ||\n\t    (TCP_SKB_CB(skb)->tcp_flags & (TCPHDR_SYN | TCPHDR_FIN)))\n\t\treturn;\n\n\t__skb_queue_head_init(&tmp);\n\n\twhile (before(start, end)) {\n\t\tint copy = min_t(int, SKB_MAX_ORDER(0, 0), end - start);\n\t\tstruct sk_buff *nskb;\n\n\t\tnskb = alloc_skb(copy, GFP_ATOMIC);\n\t\tif (!nskb)\n\t\t\tbreak;\n\n\t\tmemcpy(nskb->cb, skb->cb, sizeof(skb->cb));\n#ifdef CONFIG_TLS_DEVICE\n\t\tnskb->decrypted = skb->decrypted;\n#endif\n\t\tTCP_SKB_CB(nskb)->seq = TCP_SKB_CB(nskb)->end_seq = start;\n\t\tif (list)\n\t\t\t__skb_queue_before(list, skb, nskb);\n\t\telse\n\t\t\t__skb_queue_tail(&tmp, nskb);  \n\t\tskb_set_owner_r(nskb, sk);\n\t\tmptcp_skb_ext_move(nskb, skb);\n\n\t\t \n\t\twhile (copy > 0) {\n\t\t\tint offset = start - TCP_SKB_CB(skb)->seq;\n\t\t\tint size = TCP_SKB_CB(skb)->end_seq - start;\n\n\t\t\tBUG_ON(offset < 0);\n\t\t\tif (size > 0) {\n\t\t\t\tsize = min(copy, size);\n\t\t\t\tif (skb_copy_bits(skb, offset, skb_put(nskb, size), size))\n\t\t\t\t\tBUG();\n\t\t\t\tTCP_SKB_CB(nskb)->end_seq += size;\n\t\t\t\tcopy -= size;\n\t\t\t\tstart += size;\n\t\t\t}\n\t\t\tif (!before(start, TCP_SKB_CB(skb)->end_seq)) {\n\t\t\t\tskb = tcp_collapse_one(sk, skb, list, root);\n\t\t\t\tif (!skb ||\n\t\t\t\t    skb == tail ||\n\t\t\t\t    !mptcp_skb_can_collapse(nskb, skb) ||\n\t\t\t\t    (TCP_SKB_CB(skb)->tcp_flags & (TCPHDR_SYN | TCPHDR_FIN)))\n\t\t\t\t\tgoto end;\n#ifdef CONFIG_TLS_DEVICE\n\t\t\t\tif (skb->decrypted != nskb->decrypted)\n\t\t\t\t\tgoto end;\n#endif\n\t\t\t}\n\t\t}\n\t}\nend:\n\tskb_queue_walk_safe(&tmp, skb, n)\n\t\ttcp_rbtree_insert(root, skb);\n}\n\n \nstatic void tcp_collapse_ofo_queue(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 range_truesize, sum_tiny = 0;\n\tstruct sk_buff *skb, *head;\n\tu32 start, end;\n\n\tskb = skb_rb_first(&tp->out_of_order_queue);\nnew_range:\n\tif (!skb) {\n\t\ttp->ooo_last_skb = skb_rb_last(&tp->out_of_order_queue);\n\t\treturn;\n\t}\n\tstart = TCP_SKB_CB(skb)->seq;\n\tend = TCP_SKB_CB(skb)->end_seq;\n\trange_truesize = skb->truesize;\n\n\tfor (head = skb;;) {\n\t\tskb = skb_rb_next(skb);\n\n\t\t \n\t\tif (!skb ||\n\t\t    after(TCP_SKB_CB(skb)->seq, end) ||\n\t\t    before(TCP_SKB_CB(skb)->end_seq, start)) {\n\t\t\t \n\t\t\tif (range_truesize != head->truesize ||\n\t\t\t    end - start >= SKB_WITH_OVERHEAD(PAGE_SIZE)) {\n\t\t\t\ttcp_collapse(sk, NULL, &tp->out_of_order_queue,\n\t\t\t\t\t     head, skb, start, end);\n\t\t\t} else {\n\t\t\t\tsum_tiny += range_truesize;\n\t\t\t\tif (sum_tiny > sk->sk_rcvbuf >> 3)\n\t\t\t\t\treturn;\n\t\t\t}\n\t\t\tgoto new_range;\n\t\t}\n\n\t\trange_truesize += skb->truesize;\n\t\tif (unlikely(before(TCP_SKB_CB(skb)->seq, start)))\n\t\t\tstart = TCP_SKB_CB(skb)->seq;\n\t\tif (after(TCP_SKB_CB(skb)->end_seq, end))\n\t\t\tend = TCP_SKB_CB(skb)->end_seq;\n\t}\n}\n\n \nstatic bool tcp_prune_ofo_queue(struct sock *sk, const struct sk_buff *in_skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct rb_node *node, *prev;\n\tbool pruned = false;\n\tint goal;\n\n\tif (RB_EMPTY_ROOT(&tp->out_of_order_queue))\n\t\treturn false;\n\n\tgoal = sk->sk_rcvbuf >> 3;\n\tnode = &tp->ooo_last_skb->rbnode;\n\n\tdo {\n\t\tstruct sk_buff *skb = rb_to_skb(node);\n\n\t\t \n\t\tif (after(TCP_SKB_CB(in_skb)->seq, TCP_SKB_CB(skb)->seq))\n\t\t\tbreak;\n\t\tpruned = true;\n\t\tprev = rb_prev(node);\n\t\trb_erase(node, &tp->out_of_order_queue);\n\t\tgoal -= skb->truesize;\n\t\ttcp_drop_reason(sk, skb, SKB_DROP_REASON_TCP_OFO_QUEUE_PRUNE);\n\t\ttp->ooo_last_skb = rb_to_skb(prev);\n\t\tif (!prev || goal <= 0) {\n\t\t\tif (atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf &&\n\t\t\t    !tcp_under_memory_pressure(sk))\n\t\t\t\tbreak;\n\t\t\tgoal = sk->sk_rcvbuf >> 3;\n\t\t}\n\t\tnode = prev;\n\t} while (node);\n\n\tif (pruned) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_OFOPRUNED);\n\t\t \n\t\tif (tp->rx_opt.sack_ok)\n\t\t\ttcp_sack_reset(&tp->rx_opt);\n\t}\n\treturn pruned;\n}\n\n \nstatic int tcp_prune_queue(struct sock *sk, const struct sk_buff *in_skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_PRUNECALLED);\n\n\tif (atomic_read(&sk->sk_rmem_alloc) >= sk->sk_rcvbuf)\n\t\ttcp_clamp_window(sk);\n\telse if (tcp_under_memory_pressure(sk))\n\t\ttcp_adjust_rcv_ssthresh(sk);\n\n\tif (atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf)\n\t\treturn 0;\n\n\ttcp_collapse_ofo_queue(sk);\n\tif (!skb_queue_empty(&sk->sk_receive_queue))\n\t\ttcp_collapse(sk, &sk->sk_receive_queue, NULL,\n\t\t\t     skb_peek(&sk->sk_receive_queue),\n\t\t\t     NULL,\n\t\t\t     tp->copied_seq, tp->rcv_nxt);\n\n\tif (atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf)\n\t\treturn 0;\n\n\t \n\n\ttcp_prune_ofo_queue(sk, in_skb);\n\n\tif (atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf)\n\t\treturn 0;\n\n\t \n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_RCVPRUNED);\n\n\t \n\ttp->pred_flags = 0;\n\treturn -1;\n}\n\nstatic bool tcp_should_expand_sndbuf(struct sock *sk)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\n\t \n\tif (sk->sk_userlocks & SOCK_SNDBUF_LOCK)\n\t\treturn false;\n\n\t \n\tif (tcp_under_memory_pressure(sk)) {\n\t\tint unused_mem = sk_unused_reserved_mem(sk);\n\n\t\t \n\t\tif (unused_mem > SOCK_MIN_SNDBUF)\n\t\t\tWRITE_ONCE(sk->sk_sndbuf, unused_mem);\n\n\t\treturn false;\n\t}\n\n\t \n\tif (sk_memory_allocated(sk) >= sk_prot_mem_limits(sk, 0))\n\t\treturn false;\n\n\t \n\tif (tcp_packets_in_flight(tp) >= tcp_snd_cwnd(tp))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic void tcp_new_space(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (tcp_should_expand_sndbuf(sk)) {\n\t\ttcp_sndbuf_expand(sk);\n\t\ttp->snd_cwnd_stamp = tcp_jiffies32;\n\t}\n\n\tINDIRECT_CALL_1(sk->sk_write_space, sk_stream_write_space, sk);\n}\n\n \nvoid tcp_check_space(struct sock *sk)\n{\n\t \n\tsmp_mb();\n\tif (sk->sk_socket &&\n\t    test_bit(SOCK_NOSPACE, &sk->sk_socket->flags)) {\n\t\ttcp_new_space(sk);\n\t\tif (!test_bit(SOCK_NOSPACE, &sk->sk_socket->flags))\n\t\t\ttcp_chrono_stop(sk, TCP_CHRONO_SNDBUF_LIMITED);\n\t}\n}\n\nstatic inline void tcp_data_snd_check(struct sock *sk)\n{\n\ttcp_push_pending_frames(sk);\n\ttcp_check_space(sk);\n}\n\n \nstatic void __tcp_ack_snd_check(struct sock *sk, int ofo_possible)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tunsigned long rtt, delay;\n\n\t     \n\tif (((tp->rcv_nxt - tp->rcv_wup) > inet_csk(sk)->icsk_ack.rcv_mss &&\n\t      \n\t    (tp->rcv_nxt - tp->copied_seq < sk->sk_rcvlowat ||\n\t     __tcp_select_window(sk) >= tp->rcv_wnd)) ||\n\t     \n\t    tcp_in_quickack_mode(sk) ||\n\t     \n\t    inet_csk(sk)->icsk_ack.pending & ICSK_ACK_NOW) {\nsend_now:\n\t\ttcp_send_ack(sk);\n\t\treturn;\n\t}\n\n\tif (!ofo_possible || RB_EMPTY_ROOT(&tp->out_of_order_queue)) {\n\t\ttcp_send_delayed_ack(sk);\n\t\treturn;\n\t}\n\n\tif (!tcp_is_sack(tp) ||\n\t    tp->compressed_ack >= READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_comp_sack_nr))\n\t\tgoto send_now;\n\n\tif (tp->compressed_ack_rcv_nxt != tp->rcv_nxt) {\n\t\ttp->compressed_ack_rcv_nxt = tp->rcv_nxt;\n\t\ttp->dup_ack_counter = 0;\n\t}\n\tif (tp->dup_ack_counter < TCP_FASTRETRANS_THRESH) {\n\t\ttp->dup_ack_counter++;\n\t\tgoto send_now;\n\t}\n\ttp->compressed_ack++;\n\tif (hrtimer_is_queued(&tp->compressed_ack_timer))\n\t\treturn;\n\n\t \n\n\trtt = tp->rcv_rtt_est.rtt_us;\n\tif (tp->srtt_us && tp->srtt_us < rtt)\n\t\trtt = tp->srtt_us;\n\n\tdelay = min_t(unsigned long,\n\t\t      READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_comp_sack_delay_ns),\n\t\t      rtt * (NSEC_PER_USEC >> 3)/20);\n\tsock_hold(sk);\n\thrtimer_start_range_ns(&tp->compressed_ack_timer, ns_to_ktime(delay),\n\t\t\t       READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_comp_sack_slack_ns),\n\t\t\t       HRTIMER_MODE_REL_PINNED_SOFT);\n}\n\nstatic inline void tcp_ack_snd_check(struct sock *sk)\n{\n\tif (!inet_csk_ack_scheduled(sk)) {\n\t\t \n\t\treturn;\n\t}\n\t__tcp_ack_snd_check(sk, 1);\n}\n\n \n\nstatic void tcp_check_urg(struct sock *sk, const struct tcphdr *th)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 ptr = ntohs(th->urg_ptr);\n\n\tif (ptr && !READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_stdurg))\n\t\tptr--;\n\tptr += ntohl(th->seq);\n\n\t \n\tif (after(tp->copied_seq, ptr))\n\t\treturn;\n\n\t \n\tif (before(ptr, tp->rcv_nxt))\n\t\treturn;\n\n\t \n\tif (tp->urg_data && !after(ptr, tp->urg_seq))\n\t\treturn;\n\n\t \n\tsk_send_sigurg(sk);\n\n\t \n\tif (tp->urg_seq == tp->copied_seq && tp->urg_data &&\n\t    !sock_flag(sk, SOCK_URGINLINE) && tp->copied_seq != tp->rcv_nxt) {\n\t\tstruct sk_buff *skb = skb_peek(&sk->sk_receive_queue);\n\t\ttp->copied_seq++;\n\t\tif (skb && !before(tp->copied_seq, TCP_SKB_CB(skb)->end_seq)) {\n\t\t\t__skb_unlink(skb, &sk->sk_receive_queue);\n\t\t\t__kfree_skb(skb);\n\t\t}\n\t}\n\n\tWRITE_ONCE(tp->urg_data, TCP_URG_NOTYET);\n\tWRITE_ONCE(tp->urg_seq, ptr);\n\n\t \n\ttp->pred_flags = 0;\n}\n\n \nstatic void tcp_urg(struct sock *sk, struct sk_buff *skb, const struct tcphdr *th)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t \n\tif (unlikely(th->urg))\n\t\ttcp_check_urg(sk, th);\n\n\t \n\tif (unlikely(tp->urg_data == TCP_URG_NOTYET)) {\n\t\tu32 ptr = tp->urg_seq - ntohl(th->seq) + (th->doff * 4) -\n\t\t\t  th->syn;\n\n\t\t \n\t\tif (ptr < skb->len) {\n\t\t\tu8 tmp;\n\t\t\tif (skb_copy_bits(skb, ptr, &tmp, 1))\n\t\t\t\tBUG();\n\t\t\tWRITE_ONCE(tp->urg_data, TCP_URG_VALID | tmp);\n\t\t\tif (!sock_flag(sk, SOCK_DEAD))\n\t\t\t\tsk->sk_data_ready(sk);\n\t\t}\n\t}\n}\n\n \nstatic bool tcp_reset_check(const struct sock *sk, const struct sk_buff *skb)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\n\treturn unlikely(TCP_SKB_CB(skb)->seq == (tp->rcv_nxt - 1) &&\n\t\t\t(1 << sk->sk_state) & (TCPF_CLOSE_WAIT | TCPF_LAST_ACK |\n\t\t\t\t\t       TCPF_CLOSING));\n}\n\n \nstatic bool tcp_validate_incoming(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t  const struct tcphdr *th, int syn_inerr)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tSKB_DR(reason);\n\n\t \n\tif (tcp_fast_parse_options(sock_net(sk), skb, th, tp) &&\n\t    tp->rx_opt.saw_tstamp &&\n\t    tcp_paws_discard(sk, skb)) {\n\t\tif (!th->rst) {\n\t\t\tif (unlikely(th->syn))\n\t\t\t\tgoto syn_challenge;\n\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_PAWSESTABREJECTED);\n\t\t\tif (!tcp_oow_rate_limited(sock_net(sk), skb,\n\t\t\t\t\t\t  LINUX_MIB_TCPACKSKIPPEDPAWS,\n\t\t\t\t\t\t  &tp->last_oow_ack_time))\n\t\t\t\ttcp_send_dupack(sk, skb);\n\t\t\tSKB_DR_SET(reason, TCP_RFC7323_PAWS);\n\t\t\tgoto discard;\n\t\t}\n\t\t \n\t}\n\n\t \n\treason = tcp_sequence(tp, TCP_SKB_CB(skb)->seq, TCP_SKB_CB(skb)->end_seq);\n\tif (reason) {\n\t\t \n\t\tif (!th->rst) {\n\t\t\tif (th->syn)\n\t\t\t\tgoto syn_challenge;\n\t\t\tif (!tcp_oow_rate_limited(sock_net(sk), skb,\n\t\t\t\t\t\t  LINUX_MIB_TCPACKSKIPPEDSEQ,\n\t\t\t\t\t\t  &tp->last_oow_ack_time))\n\t\t\t\ttcp_send_dupack(sk, skb);\n\t\t} else if (tcp_reset_check(sk, skb)) {\n\t\t\tgoto reset;\n\t\t}\n\t\tgoto discard;\n\t}\n\n\t \n\tif (th->rst) {\n\t\t \n\t\tif (TCP_SKB_CB(skb)->seq == tp->rcv_nxt ||\n\t\t    tcp_reset_check(sk, skb))\n\t\t\tgoto reset;\n\n\t\tif (tcp_is_sack(tp) && tp->rx_opt.num_sacks > 0) {\n\t\t\tstruct tcp_sack_block *sp = &tp->selective_acks[0];\n\t\t\tint max_sack = sp[0].end_seq;\n\t\t\tint this_sack;\n\n\t\t\tfor (this_sack = 1; this_sack < tp->rx_opt.num_sacks;\n\t\t\t     ++this_sack) {\n\t\t\t\tmax_sack = after(sp[this_sack].end_seq,\n\t\t\t\t\t\t max_sack) ?\n\t\t\t\t\tsp[this_sack].end_seq : max_sack;\n\t\t\t}\n\n\t\t\tif (TCP_SKB_CB(skb)->seq == max_sack)\n\t\t\t\tgoto reset;\n\t\t}\n\n\t\t \n\t\tif (tp->syn_fastopen && !tp->data_segs_in &&\n\t\t    sk->sk_state == TCP_ESTABLISHED)\n\t\t\ttcp_fastopen_active_disable(sk);\n\t\ttcp_send_challenge_ack(sk);\n\t\tSKB_DR_SET(reason, TCP_RESET);\n\t\tgoto discard;\n\t}\n\n\t \n\n\t \n\tif (th->syn) {\nsyn_challenge:\n\t\tif (syn_inerr)\n\t\t\tTCP_INC_STATS(sock_net(sk), TCP_MIB_INERRS);\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPSYNCHALLENGE);\n\t\ttcp_send_challenge_ack(sk);\n\t\tSKB_DR_SET(reason, TCP_INVALID_SYN);\n\t\tgoto discard;\n\t}\n\n\tbpf_skops_parse_hdr(sk, skb);\n\n\treturn true;\n\ndiscard:\n\ttcp_drop_reason(sk, skb, reason);\n\treturn false;\n\nreset:\n\ttcp_reset(sk, skb);\n\t__kfree_skb(skb);\n\treturn false;\n}\n\n \nvoid tcp_rcv_established(struct sock *sk, struct sk_buff *skb)\n{\n\tenum skb_drop_reason reason = SKB_DROP_REASON_NOT_SPECIFIED;\n\tconst struct tcphdr *th = (const struct tcphdr *)skb->data;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tunsigned int len = skb->len;\n\n\t \n\ttrace_tcp_probe(sk, skb);\n\n\ttcp_mstamp_refresh(tp);\n\tif (unlikely(!rcu_access_pointer(sk->sk_rx_dst)))\n\t\tinet_csk(sk)->icsk_af_ops->sk_rx_dst_set(sk, skb);\n\t \n\n\ttp->rx_opt.saw_tstamp = 0;\n\n\t \n\n\tif ((tcp_flag_word(th) & TCP_HP_BITS) == tp->pred_flags &&\n\t    TCP_SKB_CB(skb)->seq == tp->rcv_nxt &&\n\t    !after(TCP_SKB_CB(skb)->ack_seq, tp->snd_nxt)) {\n\t\tint tcp_header_len = tp->tcp_header_len;\n\n\t\t \n\n\t\t \n\t\tif (tcp_header_len == sizeof(struct tcphdr) + TCPOLEN_TSTAMP_ALIGNED) {\n\t\t\t \n\t\t\tif (!tcp_parse_aligned_timestamp(tp, th))\n\t\t\t\tgoto slow_path;\n\n\t\t\t \n\t\t\tif ((s32)(tp->rx_opt.rcv_tsval - tp->rx_opt.ts_recent) < 0)\n\t\t\t\tgoto slow_path;\n\n\t\t\t \n\t\t}\n\n\t\tif (len <= tcp_header_len) {\n\t\t\t \n\t\t\tif (len == tcp_header_len) {\n\t\t\t\t \n\t\t\t\tif (tcp_header_len ==\n\t\t\t\t    (sizeof(struct tcphdr) + TCPOLEN_TSTAMP_ALIGNED) &&\n\t\t\t\t    tp->rcv_nxt == tp->rcv_wup)\n\t\t\t\t\ttcp_store_ts_recent(tp);\n\n\t\t\t\t \n\t\t\t\ttcp_ack(sk, skb, 0);\n\t\t\t\t__kfree_skb(skb);\n\t\t\t\ttcp_data_snd_check(sk);\n\t\t\t\t \n\t\t\t\ttp->rcv_rtt_last_tsecr = tp->rx_opt.rcv_tsecr;\n\t\t\t\treturn;\n\t\t\t} else {  \n\t\t\t\treason = SKB_DROP_REASON_PKT_TOO_SMALL;\n\t\t\t\tTCP_INC_STATS(sock_net(sk), TCP_MIB_INERRS);\n\t\t\t\tgoto discard;\n\t\t\t}\n\t\t} else {\n\t\t\tint eaten = 0;\n\t\t\tbool fragstolen = false;\n\n\t\t\tif (tcp_checksum_complete(skb))\n\t\t\t\tgoto csum_error;\n\n\t\t\tif ((int)skb->truesize > sk->sk_forward_alloc)\n\t\t\t\tgoto step5;\n\n\t\t\t \n\t\t\tif (tcp_header_len ==\n\t\t\t    (sizeof(struct tcphdr) + TCPOLEN_TSTAMP_ALIGNED) &&\n\t\t\t    tp->rcv_nxt == tp->rcv_wup)\n\t\t\t\ttcp_store_ts_recent(tp);\n\n\t\t\ttcp_rcv_rtt_measure_ts(sk, skb);\n\n\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPHPHITS);\n\n\t\t\t \n\t\t\tskb_dst_drop(skb);\n\t\t\t__skb_pull(skb, tcp_header_len);\n\t\t\teaten = tcp_queue_rcv(sk, skb, &fragstolen);\n\n\t\t\ttcp_event_data_recv(sk, skb);\n\n\t\t\tif (TCP_SKB_CB(skb)->ack_seq != tp->snd_una) {\n\t\t\t\t \n\t\t\t\ttcp_ack(sk, skb, FLAG_DATA);\n\t\t\t\ttcp_data_snd_check(sk);\n\t\t\t\tif (!inet_csk_ack_scheduled(sk))\n\t\t\t\t\tgoto no_ack;\n\t\t\t} else {\n\t\t\t\ttcp_update_wl(tp, TCP_SKB_CB(skb)->seq);\n\t\t\t}\n\n\t\t\t__tcp_ack_snd_check(sk, 0);\nno_ack:\n\t\t\tif (eaten)\n\t\t\t\tkfree_skb_partial(skb, fragstolen);\n\t\t\ttcp_data_ready(sk);\n\t\t\treturn;\n\t\t}\n\t}\n\nslow_path:\n\tif (len < (th->doff << 2) || tcp_checksum_complete(skb))\n\t\tgoto csum_error;\n\n\tif (!th->ack && !th->rst && !th->syn) {\n\t\treason = SKB_DROP_REASON_TCP_FLAGS;\n\t\tgoto discard;\n\t}\n\n\t \n\n\tif (!tcp_validate_incoming(sk, skb, th, 1))\n\t\treturn;\n\nstep5:\n\treason = tcp_ack(sk, skb, FLAG_SLOWPATH | FLAG_UPDATE_TS_RECENT);\n\tif ((int)reason < 0) {\n\t\treason = -reason;\n\t\tgoto discard;\n\t}\n\ttcp_rcv_rtt_measure_ts(sk, skb);\n\n\t \n\ttcp_urg(sk, skb, th);\n\n\t \n\ttcp_data_queue(sk, skb);\n\n\ttcp_data_snd_check(sk);\n\ttcp_ack_snd_check(sk);\n\treturn;\n\ncsum_error:\n\treason = SKB_DROP_REASON_TCP_CSUM;\n\ttrace_tcp_bad_csum(skb);\n\tTCP_INC_STATS(sock_net(sk), TCP_MIB_CSUMERRORS);\n\tTCP_INC_STATS(sock_net(sk), TCP_MIB_INERRS);\n\ndiscard:\n\ttcp_drop_reason(sk, skb, reason);\n}\nEXPORT_SYMBOL(tcp_rcv_established);\n\nvoid tcp_init_transfer(struct sock *sk, int bpf_op, struct sk_buff *skb)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\ttcp_mtup_init(sk);\n\ticsk->icsk_af_ops->rebuild_header(sk);\n\ttcp_init_metrics(sk);\n\n\t \n\tif (tp->total_retrans > 1 && tp->undo_marker)\n\t\ttcp_snd_cwnd_set(tp, 1);\n\telse\n\t\ttcp_snd_cwnd_set(tp, tcp_init_cwnd(tp, __sk_dst_get(sk)));\n\ttp->snd_cwnd_stamp = tcp_jiffies32;\n\n\tbpf_skops_established(sk, bpf_op, skb);\n\t \n\tif (!icsk->icsk_ca_initialized)\n\t\ttcp_init_congestion_control(sk);\n\ttcp_init_buffer_space(sk);\n}\n\nvoid tcp_finish_connect(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\ttcp_set_state(sk, TCP_ESTABLISHED);\n\ticsk->icsk_ack.lrcvtime = tcp_jiffies32;\n\n\tif (skb) {\n\t\ticsk->icsk_af_ops->sk_rx_dst_set(sk, skb);\n\t\tsecurity_inet_conn_established(sk, skb);\n\t\tsk_mark_napi_id(sk, skb);\n\t}\n\n\ttcp_init_transfer(sk, BPF_SOCK_OPS_ACTIVE_ESTABLISHED_CB, skb);\n\n\t \n\ttp->lsndtime = tcp_jiffies32;\n\n\tif (sock_flag(sk, SOCK_KEEPOPEN))\n\t\tinet_csk_reset_keepalive_timer(sk, keepalive_time_when(tp));\n\n\tif (!tp->rx_opt.snd_wscale)\n\t\t__tcp_fast_path_on(tp, tp->snd_wnd);\n\telse\n\t\ttp->pred_flags = 0;\n}\n\nstatic bool tcp_rcv_fastopen_synack(struct sock *sk, struct sk_buff *synack,\n\t\t\t\t    struct tcp_fastopen_cookie *cookie)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *data = tp->syn_data ? tcp_rtx_queue_head(sk) : NULL;\n\tu16 mss = tp->rx_opt.mss_clamp, try_exp = 0;\n\tbool syn_drop = false;\n\n\tif (mss == tp->rx_opt.user_mss) {\n\t\tstruct tcp_options_received opt;\n\n\t\t \n\t\ttcp_clear_options(&opt);\n\t\topt.user_mss = opt.mss_clamp = 0;\n\t\ttcp_parse_options(sock_net(sk), synack, &opt, 0, NULL);\n\t\tmss = opt.mss_clamp;\n\t}\n\n\tif (!tp->syn_fastopen) {\n\t\t \n\t\tcookie->len = -1;\n\t} else if (tp->total_retrans) {\n\t\t \n\t\tsyn_drop = (cookie->len < 0 && data);\n\t} else if (cookie->len < 0 && !tp->syn_data) {\n\t\t \n\t\ttry_exp = tp->syn_fastopen_exp ? 2 : 1;\n\t}\n\n\ttcp_fastopen_cache_set(sk, mss, cookie, syn_drop, try_exp);\n\n\tif (data) {  \n\t\tif (tp->total_retrans)\n\t\t\ttp->fastopen_client_fail = TFO_SYN_RETRANSMITTED;\n\t\telse\n\t\t\ttp->fastopen_client_fail = TFO_DATA_NOT_ACKED;\n\t\tskb_rbtree_walk_from(data)\n\t\t\t tcp_mark_skb_lost(sk, data);\n\t\ttcp_xmit_retransmit_queue(sk);\n\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\tLINUX_MIB_TCPFASTOPENACTIVEFAIL);\n\t\treturn true;\n\t}\n\ttp->syn_data_acked = tp->syn_data;\n\tif (tp->syn_data_acked) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPFASTOPENACTIVE);\n\t\t \n\t\tif (tp->delivered > 1)\n\t\t\t--tp->delivered;\n\t}\n\n\ttcp_fastopen_add_skb(sk, synack);\n\n\treturn false;\n}\n\nstatic void smc_check_reset_syn(struct tcp_sock *tp)\n{\n#if IS_ENABLED(CONFIG_SMC)\n\tif (static_branch_unlikely(&tcp_have_smc)) {\n\t\tif (tp->syn_smc && !tp->rx_opt.smc_ok)\n\t\t\ttp->syn_smc = 0;\n\t}\n#endif\n}\n\nstatic void tcp_try_undo_spurious_syn(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 syn_stamp;\n\n\t \n\tsyn_stamp = tp->retrans_stamp;\n\tif (tp->undo_marker && syn_stamp && tp->rx_opt.saw_tstamp &&\n\t    syn_stamp == tp->rx_opt.rcv_tsecr)\n\t\ttp->undo_marker = 0;\n}\n\nstatic int tcp_rcv_synsent_state_process(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t\t const struct tcphdr *th)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct tcp_fastopen_cookie foc = { .len = -1 };\n\tint saved_clamp = tp->rx_opt.mss_clamp;\n\tbool fastopen_fail;\n\tSKB_DR(reason);\n\n\ttcp_parse_options(sock_net(sk), skb, &tp->rx_opt, 0, &foc);\n\tif (tp->rx_opt.saw_tstamp && tp->rx_opt.rcv_tsecr)\n\t\ttp->rx_opt.rcv_tsecr -= tp->tsoffset;\n\n\tif (th->ack) {\n\t\t \n\t\tif (!after(TCP_SKB_CB(skb)->ack_seq, tp->snd_una) ||\n\t\t    after(TCP_SKB_CB(skb)->ack_seq, tp->snd_nxt)) {\n\t\t\t \n\t\t\tif (icsk->icsk_retransmits == 0)\n\t\t\t\tinet_csk_reset_xmit_timer(sk,\n\t\t\t\t\t\tICSK_TIME_RETRANS,\n\t\t\t\t\t\tTCP_TIMEOUT_MIN, TCP_RTO_MAX);\n\t\t\tgoto reset_and_undo;\n\t\t}\n\n\t\tif (tp->rx_opt.saw_tstamp && tp->rx_opt.rcv_tsecr &&\n\t\t    !between(tp->rx_opt.rcv_tsecr, tp->retrans_stamp,\n\t\t\t     tcp_time_stamp(tp))) {\n\t\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\t\tLINUX_MIB_PAWSACTIVEREJECTED);\n\t\t\tgoto reset_and_undo;\n\t\t}\n\n\t\t \n\n\t\tif (th->rst) {\n\t\t\ttcp_reset(sk, skb);\nconsume:\n\t\t\t__kfree_skb(skb);\n\t\t\treturn 0;\n\t\t}\n\n\t\t \n\t\tif (!th->syn) {\n\t\t\tSKB_DR_SET(reason, TCP_FLAGS);\n\t\t\tgoto discard_and_undo;\n\t\t}\n\t\t \n\n\t\ttcp_ecn_rcv_synack(tp, th);\n\n\t\ttcp_init_wl(tp, TCP_SKB_CB(skb)->seq);\n\t\ttcp_try_undo_spurious_syn(sk);\n\t\ttcp_ack(sk, skb, FLAG_SLOWPATH);\n\n\t\t \n\t\tWRITE_ONCE(tp->rcv_nxt, TCP_SKB_CB(skb)->seq + 1);\n\t\ttp->rcv_wup = TCP_SKB_CB(skb)->seq + 1;\n\n\t\t \n\t\ttp->snd_wnd = ntohs(th->window);\n\n\t\tif (!tp->rx_opt.wscale_ok) {\n\t\t\ttp->rx_opt.snd_wscale = tp->rx_opt.rcv_wscale = 0;\n\t\t\ttp->window_clamp = min(tp->window_clamp, 65535U);\n\t\t}\n\n\t\tif (tp->rx_opt.saw_tstamp) {\n\t\t\ttp->rx_opt.tstamp_ok\t   = 1;\n\t\t\ttp->tcp_header_len =\n\t\t\t\tsizeof(struct tcphdr) + TCPOLEN_TSTAMP_ALIGNED;\n\t\t\ttp->advmss\t    -= TCPOLEN_TSTAMP_ALIGNED;\n\t\t\ttcp_store_ts_recent(tp);\n\t\t} else {\n\t\t\ttp->tcp_header_len = sizeof(struct tcphdr);\n\t\t}\n\n\t\ttcp_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\t\ttcp_initialize_rcv_mss(sk);\n\n\t\t \n\t\tWRITE_ONCE(tp->copied_seq, tp->rcv_nxt);\n\n\t\tsmc_check_reset_syn(tp);\n\n\t\tsmp_mb();\n\n\t\ttcp_finish_connect(sk, skb);\n\n\t\tfastopen_fail = (tp->syn_fastopen || tp->syn_data) &&\n\t\t\t\ttcp_rcv_fastopen_synack(sk, skb, &foc);\n\n\t\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\t\tsk->sk_state_change(sk);\n\t\t\tsk_wake_async(sk, SOCK_WAKE_IO, POLL_OUT);\n\t\t}\n\t\tif (fastopen_fail)\n\t\t\treturn -1;\n\t\tif (sk->sk_write_pending ||\n\t\t    READ_ONCE(icsk->icsk_accept_queue.rskq_defer_accept) ||\n\t\t    inet_csk_in_pingpong_mode(sk)) {\n\t\t\t \n\t\t\tinet_csk_schedule_ack(sk);\n\t\t\ttcp_enter_quickack_mode(sk, TCP_MAX_QUICKACKS);\n\t\t\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_DACK,\n\t\t\t\t\t\t  TCP_DELACK_MAX, TCP_RTO_MAX);\n\t\t\tgoto consume;\n\t\t}\n\t\ttcp_send_ack(sk);\n\t\treturn -1;\n\t}\n\n\t \n\n\tif (th->rst) {\n\t\t \n\t\tSKB_DR_SET(reason, TCP_RESET);\n\t\tgoto discard_and_undo;\n\t}\n\n\t \n\tif (tp->rx_opt.ts_recent_stamp && tp->rx_opt.saw_tstamp &&\n\t    tcp_paws_reject(&tp->rx_opt, 0)) {\n\t\tSKB_DR_SET(reason, TCP_RFC7323_PAWS);\n\t\tgoto discard_and_undo;\n\t}\n\tif (th->syn) {\n\t\t \n\t\ttcp_set_state(sk, TCP_SYN_RECV);\n\n\t\tif (tp->rx_opt.saw_tstamp) {\n\t\t\ttp->rx_opt.tstamp_ok = 1;\n\t\t\ttcp_store_ts_recent(tp);\n\t\t\ttp->tcp_header_len =\n\t\t\t\tsizeof(struct tcphdr) + TCPOLEN_TSTAMP_ALIGNED;\n\t\t} else {\n\t\t\ttp->tcp_header_len = sizeof(struct tcphdr);\n\t\t}\n\n\t\tWRITE_ONCE(tp->rcv_nxt, TCP_SKB_CB(skb)->seq + 1);\n\t\tWRITE_ONCE(tp->copied_seq, tp->rcv_nxt);\n\t\ttp->rcv_wup = TCP_SKB_CB(skb)->seq + 1;\n\n\t\t \n\t\ttp->snd_wnd    = ntohs(th->window);\n\t\ttp->snd_wl1    = TCP_SKB_CB(skb)->seq;\n\t\ttp->max_window = tp->snd_wnd;\n\n\t\ttcp_ecn_rcv_syn(tp, th);\n\n\t\ttcp_mtup_init(sk);\n\t\ttcp_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\t\ttcp_initialize_rcv_mss(sk);\n\n\t\ttcp_send_synack(sk);\n#if 0\n\t\t \n\t\treturn -1;\n#else\n\t\tgoto consume;\n#endif\n\t}\n\t \n\ndiscard_and_undo:\n\ttcp_clear_options(&tp->rx_opt);\n\ttp->rx_opt.mss_clamp = saved_clamp;\n\ttcp_drop_reason(sk, skb, reason);\n\treturn 0;\n\nreset_and_undo:\n\ttcp_clear_options(&tp->rx_opt);\n\ttp->rx_opt.mss_clamp = saved_clamp;\n\treturn 1;\n}\n\nstatic void tcp_rcv_synrecv_state_fastopen(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct request_sock *req;\n\n\t \n\tif (inet_csk(sk)->icsk_ca_state == TCP_CA_Loss && !tp->packets_out)\n\t\ttcp_try_undo_recovery(sk);\n\n\t \n\ttp->retrans_stamp = 0;\n\tinet_csk(sk)->icsk_retransmits = 0;\n\n\t \n\treq = rcu_dereference_protected(tp->fastopen_rsk,\n\t\t\t\t\tlockdep_sock_is_held(sk));\n\treqsk_fastopen_remove(sk, req, false);\n\n\t \n\ttcp_rearm_rto(sk);\n}\n\n \n\nint tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\tstruct request_sock *req;\n\tint queued = 0;\n\tbool acceptable;\n\tSKB_DR(reason);\n\n\tswitch (sk->sk_state) {\n\tcase TCP_CLOSE:\n\t\tSKB_DR_SET(reason, TCP_CLOSE);\n\t\tgoto discard;\n\n\tcase TCP_LISTEN:\n\t\tif (th->ack)\n\t\t\treturn 1;\n\n\t\tif (th->rst) {\n\t\t\tSKB_DR_SET(reason, TCP_RESET);\n\t\t\tgoto discard;\n\t\t}\n\t\tif (th->syn) {\n\t\t\tif (th->fin) {\n\t\t\t\tSKB_DR_SET(reason, TCP_FLAGS);\n\t\t\t\tgoto discard;\n\t\t\t}\n\t\t\t \n\t\t\trcu_read_lock();\n\t\t\tlocal_bh_disable();\n\t\t\tacceptable = icsk->icsk_af_ops->conn_request(sk, skb) >= 0;\n\t\t\tlocal_bh_enable();\n\t\t\trcu_read_unlock();\n\n\t\t\tif (!acceptable)\n\t\t\t\treturn 1;\n\t\t\tconsume_skb(skb);\n\t\t\treturn 0;\n\t\t}\n\t\tSKB_DR_SET(reason, TCP_FLAGS);\n\t\tgoto discard;\n\n\tcase TCP_SYN_SENT:\n\t\ttp->rx_opt.saw_tstamp = 0;\n\t\ttcp_mstamp_refresh(tp);\n\t\tqueued = tcp_rcv_synsent_state_process(sk, skb, th);\n\t\tif (queued >= 0)\n\t\t\treturn queued;\n\n\t\t \n\t\ttcp_urg(sk, skb, th);\n\t\t__kfree_skb(skb);\n\t\ttcp_data_snd_check(sk);\n\t\treturn 0;\n\t}\n\n\ttcp_mstamp_refresh(tp);\n\ttp->rx_opt.saw_tstamp = 0;\n\treq = rcu_dereference_protected(tp->fastopen_rsk,\n\t\t\t\t\tlockdep_sock_is_held(sk));\n\tif (req) {\n\t\tbool req_stolen;\n\n\t\tWARN_ON_ONCE(sk->sk_state != TCP_SYN_RECV &&\n\t\t    sk->sk_state != TCP_FIN_WAIT1);\n\n\t\tif (!tcp_check_req(sk, skb, req, true, &req_stolen)) {\n\t\t\tSKB_DR_SET(reason, TCP_FASTOPEN);\n\t\t\tgoto discard;\n\t\t}\n\t}\n\n\tif (!th->ack && !th->rst && !th->syn) {\n\t\tSKB_DR_SET(reason, TCP_FLAGS);\n\t\tgoto discard;\n\t}\n\tif (!tcp_validate_incoming(sk, skb, th, 0))\n\t\treturn 0;\n\n\t \n\tacceptable = tcp_ack(sk, skb, FLAG_SLOWPATH |\n\t\t\t\t      FLAG_UPDATE_TS_RECENT |\n\t\t\t\t      FLAG_NO_CHALLENGE_ACK) > 0;\n\n\tif (!acceptable) {\n\t\tif (sk->sk_state == TCP_SYN_RECV)\n\t\t\treturn 1;\t \n\t\ttcp_send_challenge_ack(sk);\n\t\tSKB_DR_SET(reason, TCP_OLD_ACK);\n\t\tgoto discard;\n\t}\n\tswitch (sk->sk_state) {\n\tcase TCP_SYN_RECV:\n\t\ttp->delivered++;  \n\t\tif (!tp->srtt_us)\n\t\t\ttcp_synack_rtt_meas(sk, req);\n\n\t\tif (req) {\n\t\t\ttcp_rcv_synrecv_state_fastopen(sk);\n\t\t} else {\n\t\t\ttcp_try_undo_spurious_syn(sk);\n\t\t\ttp->retrans_stamp = 0;\n\t\t\ttcp_init_transfer(sk, BPF_SOCK_OPS_PASSIVE_ESTABLISHED_CB,\n\t\t\t\t\t  skb);\n\t\t\tWRITE_ONCE(tp->copied_seq, tp->rcv_nxt);\n\t\t}\n\t\tsmp_mb();\n\t\ttcp_set_state(sk, TCP_ESTABLISHED);\n\t\tsk->sk_state_change(sk);\n\n\t\t \n\t\tif (sk->sk_socket)\n\t\t\tsk_wake_async(sk, SOCK_WAKE_IO, POLL_OUT);\n\n\t\ttp->snd_una = TCP_SKB_CB(skb)->ack_seq;\n\t\ttp->snd_wnd = ntohs(th->window) << tp->rx_opt.snd_wscale;\n\t\ttcp_init_wl(tp, TCP_SKB_CB(skb)->seq);\n\n\t\tif (tp->rx_opt.tstamp_ok)\n\t\t\ttp->advmss -= TCPOLEN_TSTAMP_ALIGNED;\n\n\t\tif (!inet_csk(sk)->icsk_ca_ops->cong_control)\n\t\t\ttcp_update_pacing_rate(sk);\n\n\t\t \n\t\ttp->lsndtime = tcp_jiffies32;\n\n\t\ttcp_initialize_rcv_mss(sk);\n\t\ttcp_fast_path_on(tp);\n\t\tbreak;\n\n\tcase TCP_FIN_WAIT1: {\n\t\tint tmo;\n\n\t\tif (req)\n\t\t\ttcp_rcv_synrecv_state_fastopen(sk);\n\n\t\tif (tp->snd_una != tp->write_seq)\n\t\t\tbreak;\n\n\t\ttcp_set_state(sk, TCP_FIN_WAIT2);\n\t\tWRITE_ONCE(sk->sk_shutdown, sk->sk_shutdown | SEND_SHUTDOWN);\n\n\t\tsk_dst_confirm(sk);\n\n\t\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\t\t \n\t\t\tsk->sk_state_change(sk);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (READ_ONCE(tp->linger2) < 0) {\n\t\t\ttcp_done(sk);\n\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTONDATA);\n\t\t\treturn 1;\n\t\t}\n\t\tif (TCP_SKB_CB(skb)->end_seq != TCP_SKB_CB(skb)->seq &&\n\t\t    after(TCP_SKB_CB(skb)->end_seq - th->fin, tp->rcv_nxt)) {\n\t\t\t \n\t\t\tif (tp->syn_fastopen && th->fin)\n\t\t\t\ttcp_fastopen_active_disable(sk);\n\t\t\ttcp_done(sk);\n\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTONDATA);\n\t\t\treturn 1;\n\t\t}\n\n\t\ttmo = tcp_fin_time(sk);\n\t\tif (tmo > TCP_TIMEWAIT_LEN) {\n\t\t\tinet_csk_reset_keepalive_timer(sk, tmo - TCP_TIMEWAIT_LEN);\n\t\t} else if (th->fin || sock_owned_by_user(sk)) {\n\t\t\t \n\t\t\tinet_csk_reset_keepalive_timer(sk, tmo);\n\t\t} else {\n\t\t\ttcp_time_wait(sk, TCP_FIN_WAIT2, tmo);\n\t\t\tgoto consume;\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase TCP_CLOSING:\n\t\tif (tp->snd_una == tp->write_seq) {\n\t\t\ttcp_time_wait(sk, TCP_TIME_WAIT, 0);\n\t\t\tgoto consume;\n\t\t}\n\t\tbreak;\n\n\tcase TCP_LAST_ACK:\n\t\tif (tp->snd_una == tp->write_seq) {\n\t\t\ttcp_update_metrics(sk);\n\t\t\ttcp_done(sk);\n\t\t\tgoto consume;\n\t\t}\n\t\tbreak;\n\t}\n\n\t \n\ttcp_urg(sk, skb, th);\n\n\t \n\tswitch (sk->sk_state) {\n\tcase TCP_CLOSE_WAIT:\n\tcase TCP_CLOSING:\n\tcase TCP_LAST_ACK:\n\t\tif (!before(TCP_SKB_CB(skb)->seq, tp->rcv_nxt)) {\n\t\t\t \n\t\t\tif (sk_is_mptcp(sk) && !mptcp_incoming_options(sk, skb))\n\t\t\t\tgoto discard;\n\t\t\tbreak;\n\t\t}\n\t\tfallthrough;\n\tcase TCP_FIN_WAIT1:\n\tcase TCP_FIN_WAIT2:\n\t\t \n\t\tif (sk->sk_shutdown & RCV_SHUTDOWN) {\n\t\t\tif (TCP_SKB_CB(skb)->end_seq != TCP_SKB_CB(skb)->seq &&\n\t\t\t    after(TCP_SKB_CB(skb)->end_seq - th->fin, tp->rcv_nxt)) {\n\t\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTONDATA);\n\t\t\t\ttcp_reset(sk, skb);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t}\n\t\tfallthrough;\n\tcase TCP_ESTABLISHED:\n\t\ttcp_data_queue(sk, skb);\n\t\tqueued = 1;\n\t\tbreak;\n\t}\n\n\t \n\tif (sk->sk_state != TCP_CLOSE) {\n\t\ttcp_data_snd_check(sk);\n\t\ttcp_ack_snd_check(sk);\n\t}\n\n\tif (!queued) {\ndiscard:\n\t\ttcp_drop_reason(sk, skb, reason);\n\t}\n\treturn 0;\n\nconsume:\n\t__kfree_skb(skb);\n\treturn 0;\n}\nEXPORT_SYMBOL(tcp_rcv_state_process);\n\nstatic inline void pr_drop_req(struct request_sock *req, __u16 port, int family)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\n\tif (family == AF_INET)\n\t\tnet_dbg_ratelimited(\"drop open request from %pI4/%u\\n\",\n\t\t\t\t    &ireq->ir_rmt_addr, port);\n#if IS_ENABLED(CONFIG_IPV6)\n\telse if (family == AF_INET6)\n\t\tnet_dbg_ratelimited(\"drop open request from %pI6/%u\\n\",\n\t\t\t\t    &ireq->ir_v6_rmt_addr, port);\n#endif\n}\n\n \nstatic void tcp_ecn_create_request(struct request_sock *req,\n\t\t\t\t   const struct sk_buff *skb,\n\t\t\t\t   const struct sock *listen_sk,\n\t\t\t\t   const struct dst_entry *dst)\n{\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\tconst struct net *net = sock_net(listen_sk);\n\tbool th_ecn = th->ece && th->cwr;\n\tbool ect, ecn_ok;\n\tu32 ecn_ok_dst;\n\n\tif (!th_ecn)\n\t\treturn;\n\n\tect = !INET_ECN_is_not_ect(TCP_SKB_CB(skb)->ip_dsfield);\n\tecn_ok_dst = dst_feature(dst, DST_FEATURE_ECN_MASK);\n\tecn_ok = READ_ONCE(net->ipv4.sysctl_tcp_ecn) || ecn_ok_dst;\n\n\tif (((!ect || th->res1) && ecn_ok) || tcp_ca_needs_ecn(listen_sk) ||\n\t    (ecn_ok_dst & DST_FEATURE_ECN_CA) ||\n\t    tcp_bpf_ca_needs_ecn((struct sock *)req))\n\t\tinet_rsk(req)->ecn_ok = 1;\n}\n\nstatic void tcp_openreq_init(struct request_sock *req,\n\t\t\t     const struct tcp_options_received *rx_opt,\n\t\t\t     struct sk_buff *skb, const struct sock *sk)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\n\treq->rsk_rcv_wnd = 0;\t\t \n\ttcp_rsk(req)->rcv_isn = TCP_SKB_CB(skb)->seq;\n\ttcp_rsk(req)->rcv_nxt = TCP_SKB_CB(skb)->seq + 1;\n\ttcp_rsk(req)->snt_synack = 0;\n\ttcp_rsk(req)->last_oow_ack_time = 0;\n\treq->mss = rx_opt->mss_clamp;\n\treq->ts_recent = rx_opt->saw_tstamp ? rx_opt->rcv_tsval : 0;\n\tireq->tstamp_ok = rx_opt->tstamp_ok;\n\tireq->sack_ok = rx_opt->sack_ok;\n\tireq->snd_wscale = rx_opt->snd_wscale;\n\tireq->wscale_ok = rx_opt->wscale_ok;\n\tireq->acked = 0;\n\tireq->ecn_ok = 0;\n\tireq->ir_rmt_port = tcp_hdr(skb)->source;\n\tireq->ir_num = ntohs(tcp_hdr(skb)->dest);\n\tireq->ir_mark = inet_request_mark(sk, skb);\n#if IS_ENABLED(CONFIG_SMC)\n\tireq->smc_ok = rx_opt->smc_ok && !(tcp_sk(sk)->smc_hs_congested &&\n\t\t\ttcp_sk(sk)->smc_hs_congested(sk));\n#endif\n}\n\nstruct request_sock *inet_reqsk_alloc(const struct request_sock_ops *ops,\n\t\t\t\t      struct sock *sk_listener,\n\t\t\t\t      bool attach_listener)\n{\n\tstruct request_sock *req = reqsk_alloc(ops, sk_listener,\n\t\t\t\t\t       attach_listener);\n\n\tif (req) {\n\t\tstruct inet_request_sock *ireq = inet_rsk(req);\n\n\t\tireq->ireq_opt = NULL;\n#if IS_ENABLED(CONFIG_IPV6)\n\t\tireq->pktopts = NULL;\n#endif\n\t\tatomic64_set(&ireq->ir_cookie, 0);\n\t\tireq->ireq_state = TCP_NEW_SYN_RECV;\n\t\twrite_pnet(&ireq->ireq_net, sock_net(sk_listener));\n\t\tireq->ireq_family = sk_listener->sk_family;\n\t\treq->timeout = TCP_TIMEOUT_INIT;\n\t}\n\n\treturn req;\n}\nEXPORT_SYMBOL(inet_reqsk_alloc);\n\n \nstatic bool tcp_syn_flood_action(const struct sock *sk, const char *proto)\n{\n\tstruct request_sock_queue *queue = &inet_csk(sk)->icsk_accept_queue;\n\tconst char *msg = \"Dropping request\";\n\tstruct net *net = sock_net(sk);\n\tbool want_cookie = false;\n\tu8 syncookies;\n\n\tsyncookies = READ_ONCE(net->ipv4.sysctl_tcp_syncookies);\n\n#ifdef CONFIG_SYN_COOKIES\n\tif (syncookies) {\n\t\tmsg = \"Sending cookies\";\n\t\twant_cookie = true;\n\t\t__NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPREQQFULLDOCOOKIES);\n\t} else\n#endif\n\t\t__NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPREQQFULLDROP);\n\n\tif (!READ_ONCE(queue->synflood_warned) && syncookies != 2 &&\n\t    xchg(&queue->synflood_warned, 1) == 0) {\n\t\tif (IS_ENABLED(CONFIG_IPV6) && sk->sk_family == AF_INET6) {\n\t\t\tnet_info_ratelimited(\"%s: Possible SYN flooding on port [%pI6c]:%u. %s.\\n\",\n\t\t\t\t\tproto, inet6_rcv_saddr(sk),\n\t\t\t\t\tsk->sk_num, msg);\n\t\t} else {\n\t\t\tnet_info_ratelimited(\"%s: Possible SYN flooding on port %pI4:%u. %s.\\n\",\n\t\t\t\t\tproto, &sk->sk_rcv_saddr,\n\t\t\t\t\tsk->sk_num, msg);\n\t\t}\n\t}\n\n\treturn want_cookie;\n}\n\nstatic void tcp_reqsk_record_syn(const struct sock *sk,\n\t\t\t\t struct request_sock *req,\n\t\t\t\t const struct sk_buff *skb)\n{\n\tif (tcp_sk(sk)->save_syn) {\n\t\tu32 len = skb_network_header_len(skb) + tcp_hdrlen(skb);\n\t\tstruct saved_syn *saved_syn;\n\t\tu32 mac_hdrlen;\n\t\tvoid *base;\n\n\t\tif (tcp_sk(sk)->save_syn == 2) {   \n\t\t\tbase = skb_mac_header(skb);\n\t\t\tmac_hdrlen = skb_mac_header_len(skb);\n\t\t\tlen += mac_hdrlen;\n\t\t} else {\n\t\t\tbase = skb_network_header(skb);\n\t\t\tmac_hdrlen = 0;\n\t\t}\n\n\t\tsaved_syn = kmalloc(struct_size(saved_syn, data, len),\n\t\t\t\t    GFP_ATOMIC);\n\t\tif (saved_syn) {\n\t\t\tsaved_syn->mac_hdrlen = mac_hdrlen;\n\t\t\tsaved_syn->network_hdrlen = skb_network_header_len(skb);\n\t\t\tsaved_syn->tcp_hdrlen = tcp_hdrlen(skb);\n\t\t\tmemcpy(saved_syn->data, base, len);\n\t\t\treq->saved_syn = saved_syn;\n\t\t}\n\t}\n}\n\n \nu16 tcp_get_syncookie_mss(struct request_sock_ops *rsk_ops,\n\t\t\t  const struct tcp_request_sock_ops *af_ops,\n\t\t\t  struct sock *sk, struct tcphdr *th)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu16 mss;\n\n\tif (READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_syncookies) != 2 &&\n\t    !inet_csk_reqsk_queue_is_full(sk))\n\t\treturn 0;\n\n\tif (!tcp_syn_flood_action(sk, rsk_ops->slab_name))\n\t\treturn 0;\n\n\tif (sk_acceptq_is_full(sk)) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\n\t\treturn 0;\n\t}\n\n\tmss = tcp_parse_mss_option(th, tp->rx_opt.user_mss);\n\tif (!mss)\n\t\tmss = af_ops->mss_clamp;\n\n\treturn mss;\n}\nEXPORT_SYMBOL_GPL(tcp_get_syncookie_mss);\n\nint tcp_conn_request(struct request_sock_ops *rsk_ops,\n\t\t     const struct tcp_request_sock_ops *af_ops,\n\t\t     struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_fastopen_cookie foc = { .len = -1 };\n\t__u32 isn = TCP_SKB_CB(skb)->tcp_tw_isn;\n\tstruct tcp_options_received tmp_opt;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tstruct sock *fastopen_sk = NULL;\n\tstruct request_sock *req;\n\tbool want_cookie = false;\n\tstruct dst_entry *dst;\n\tstruct flowi fl;\n\tu8 syncookies;\n\n\tsyncookies = READ_ONCE(net->ipv4.sysctl_tcp_syncookies);\n\n\t \n\tif ((syncookies == 2 || inet_csk_reqsk_queue_is_full(sk)) && !isn) {\n\t\twant_cookie = tcp_syn_flood_action(sk, rsk_ops->slab_name);\n\t\tif (!want_cookie)\n\t\t\tgoto drop;\n\t}\n\n\tif (sk_acceptq_is_full(sk)) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\n\t\tgoto drop;\n\t}\n\n\treq = inet_reqsk_alloc(rsk_ops, sk, !want_cookie);\n\tif (!req)\n\t\tgoto drop;\n\n\treq->syncookie = want_cookie;\n\ttcp_rsk(req)->af_specific = af_ops;\n\ttcp_rsk(req)->ts_off = 0;\n#if IS_ENABLED(CONFIG_MPTCP)\n\ttcp_rsk(req)->is_mptcp = 0;\n#endif\n\n\ttcp_clear_options(&tmp_opt);\n\ttmp_opt.mss_clamp = af_ops->mss_clamp;\n\ttmp_opt.user_mss  = tp->rx_opt.user_mss;\n\ttcp_parse_options(sock_net(sk), skb, &tmp_opt, 0,\n\t\t\t  want_cookie ? NULL : &foc);\n\n\tif (want_cookie && !tmp_opt.saw_tstamp)\n\t\ttcp_clear_options(&tmp_opt);\n\n\tif (IS_ENABLED(CONFIG_SMC) && want_cookie)\n\t\ttmp_opt.smc_ok = 0;\n\n\ttmp_opt.tstamp_ok = tmp_opt.saw_tstamp;\n\ttcp_openreq_init(req, &tmp_opt, skb, sk);\n\tinet_rsk(req)->no_srccheck = inet_test_bit(TRANSPARENT, sk);\n\n\t \n\tinet_rsk(req)->ir_iif = inet_request_bound_dev_if(sk, skb);\n\n\tdst = af_ops->route_req(sk, skb, &fl, req);\n\tif (!dst)\n\t\tgoto drop_and_free;\n\n\tif (tmp_opt.tstamp_ok)\n\t\ttcp_rsk(req)->ts_off = af_ops->init_ts_off(net, skb);\n\n\tif (!want_cookie && !isn) {\n\t\tint max_syn_backlog = READ_ONCE(net->ipv4.sysctl_max_syn_backlog);\n\n\t\t \n\t\tif (!syncookies &&\n\t\t    (max_syn_backlog - inet_csk_reqsk_queue_len(sk) <\n\t\t     (max_syn_backlog >> 2)) &&\n\t\t    !tcp_peer_is_proven(req, dst)) {\n\t\t\t \n\t\t\tpr_drop_req(req, ntohs(tcp_hdr(skb)->source),\n\t\t\t\t    rsk_ops->family);\n\t\t\tgoto drop_and_release;\n\t\t}\n\n\t\tisn = af_ops->init_seq(skb);\n\t}\n\n\ttcp_ecn_create_request(req, skb, sk, dst);\n\n\tif (want_cookie) {\n\t\tisn = cookie_init_sequence(af_ops, sk, skb, &req->mss);\n\t\tif (!tmp_opt.tstamp_ok)\n\t\t\tinet_rsk(req)->ecn_ok = 0;\n\t}\n\n\ttcp_rsk(req)->snt_isn = isn;\n\ttcp_rsk(req)->txhash = net_tx_rndhash();\n\ttcp_rsk(req)->syn_tos = TCP_SKB_CB(skb)->ip_dsfield;\n\ttcp_openreq_init_rwin(req, sk, dst);\n\tsk_rx_queue_set(req_to_sk(req), skb);\n\tif (!want_cookie) {\n\t\ttcp_reqsk_record_syn(sk, req, skb);\n\t\tfastopen_sk = tcp_try_fastopen(sk, skb, req, &foc, dst);\n\t}\n\tif (fastopen_sk) {\n\t\taf_ops->send_synack(fastopen_sk, dst, &fl, req,\n\t\t\t\t    &foc, TCP_SYNACK_FASTOPEN, skb);\n\t\t \n\t\tif (!inet_csk_reqsk_queue_add(sk, req, fastopen_sk)) {\n\t\t\treqsk_fastopen_remove(fastopen_sk, req, false);\n\t\t\tbh_unlock_sock(fastopen_sk);\n\t\t\tsock_put(fastopen_sk);\n\t\t\tgoto drop_and_free;\n\t\t}\n\t\tsk->sk_data_ready(sk);\n\t\tbh_unlock_sock(fastopen_sk);\n\t\tsock_put(fastopen_sk);\n\t} else {\n\t\ttcp_rsk(req)->tfo_listener = false;\n\t\tif (!want_cookie) {\n\t\t\treq->timeout = tcp_timeout_init((struct sock *)req);\n\t\t\tinet_csk_reqsk_queue_hash_add(sk, req, req->timeout);\n\t\t}\n\t\taf_ops->send_synack(sk, dst, &fl, req, &foc,\n\t\t\t\t    !want_cookie ? TCP_SYNACK_NORMAL :\n\t\t\t\t\t\t   TCP_SYNACK_COOKIE,\n\t\t\t\t    skb);\n\t\tif (want_cookie) {\n\t\t\treqsk_free(req);\n\t\t\treturn 0;\n\t\t}\n\t}\n\treqsk_put(req);\n\treturn 0;\n\ndrop_and_release:\n\tdst_release(dst);\ndrop_and_free:\n\t__reqsk_free(req);\ndrop:\n\ttcp_listendrop(sk);\n\treturn 0;\n}\nEXPORT_SYMBOL(tcp_conn_request);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}