{
  "module_name": "tcp.c",
  "hash_id": "8c45a306ab0542e27688c823802723c80f8f7715f7db5af2b59e664eb4d13b29",
  "original_prompt": "Ingested from linux-6.6.14/net/ipv4/tcp.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) \"TCP: \" fmt\n\n#include <crypto/hash.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/types.h>\n#include <linux/fcntl.h>\n#include <linux/poll.h>\n#include <linux/inet_diag.h>\n#include <linux/init.h>\n#include <linux/fs.h>\n#include <linux/skbuff.h>\n#include <linux/scatterlist.h>\n#include <linux/splice.h>\n#include <linux/net.h>\n#include <linux/socket.h>\n#include <linux/random.h>\n#include <linux/memblock.h>\n#include <linux/highmem.h>\n#include <linux/cache.h>\n#include <linux/err.h>\n#include <linux/time.h>\n#include <linux/slab.h>\n#include <linux/errqueue.h>\n#include <linux/static_key.h>\n#include <linux/btf.h>\n\n#include <net/icmp.h>\n#include <net/inet_common.h>\n#include <net/tcp.h>\n#include <net/mptcp.h>\n#include <net/xfrm.h>\n#include <net/ip.h>\n#include <net/sock.h>\n\n#include <linux/uaccess.h>\n#include <asm/ioctls.h>\n#include <net/busy_poll.h>\n\n \nenum {\n\tTCP_CMSG_INQ = 1,\n\tTCP_CMSG_TS = 2\n};\n\nDEFINE_PER_CPU(unsigned int, tcp_orphan_count);\nEXPORT_PER_CPU_SYMBOL_GPL(tcp_orphan_count);\n\nlong sysctl_tcp_mem[3] __read_mostly;\nEXPORT_SYMBOL(sysctl_tcp_mem);\n\natomic_long_t tcp_memory_allocated ____cacheline_aligned_in_smp;\t \nEXPORT_SYMBOL(tcp_memory_allocated);\nDEFINE_PER_CPU(int, tcp_memory_per_cpu_fw_alloc);\nEXPORT_PER_CPU_SYMBOL_GPL(tcp_memory_per_cpu_fw_alloc);\n\n#if IS_ENABLED(CONFIG_SMC)\nDEFINE_STATIC_KEY_FALSE(tcp_have_smc);\nEXPORT_SYMBOL(tcp_have_smc);\n#endif\n\n \nstruct percpu_counter tcp_sockets_allocated ____cacheline_aligned_in_smp;\nEXPORT_SYMBOL(tcp_sockets_allocated);\n\n \nstruct tcp_splice_state {\n\tstruct pipe_inode_info *pipe;\n\tsize_t len;\n\tunsigned int flags;\n};\n\n \nunsigned long tcp_memory_pressure __read_mostly;\nEXPORT_SYMBOL_GPL(tcp_memory_pressure);\n\nvoid tcp_enter_memory_pressure(struct sock *sk)\n{\n\tunsigned long val;\n\n\tif (READ_ONCE(tcp_memory_pressure))\n\t\treturn;\n\tval = jiffies;\n\n\tif (!val)\n\t\tval--;\n\tif (!cmpxchg(&tcp_memory_pressure, 0, val))\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPMEMORYPRESSURES);\n}\nEXPORT_SYMBOL_GPL(tcp_enter_memory_pressure);\n\nvoid tcp_leave_memory_pressure(struct sock *sk)\n{\n\tunsigned long val;\n\n\tif (!READ_ONCE(tcp_memory_pressure))\n\t\treturn;\n\tval = xchg(&tcp_memory_pressure, 0);\n\tif (val)\n\t\tNET_ADD_STATS(sock_net(sk), LINUX_MIB_TCPMEMORYPRESSURESCHRONO,\n\t\t\t      jiffies_to_msecs(jiffies - val));\n}\nEXPORT_SYMBOL_GPL(tcp_leave_memory_pressure);\n\n \nstatic u8 secs_to_retrans(int seconds, int timeout, int rto_max)\n{\n\tu8 res = 0;\n\n\tif (seconds > 0) {\n\t\tint period = timeout;\n\n\t\tres = 1;\n\t\twhile (seconds > period && res < 255) {\n\t\t\tres++;\n\t\t\ttimeout <<= 1;\n\t\t\tif (timeout > rto_max)\n\t\t\t\ttimeout = rto_max;\n\t\t\tperiod += timeout;\n\t\t}\n\t}\n\treturn res;\n}\n\n \nstatic int retrans_to_secs(u8 retrans, int timeout, int rto_max)\n{\n\tint period = 0;\n\n\tif (retrans > 0) {\n\t\tperiod = timeout;\n\t\twhile (--retrans) {\n\t\t\ttimeout <<= 1;\n\t\t\tif (timeout > rto_max)\n\t\t\t\ttimeout = rto_max;\n\t\t\tperiod += timeout;\n\t\t}\n\t}\n\treturn period;\n}\n\nstatic u64 tcp_compute_delivery_rate(const struct tcp_sock *tp)\n{\n\tu32 rate = READ_ONCE(tp->rate_delivered);\n\tu32 intv = READ_ONCE(tp->rate_interval_us);\n\tu64 rate64 = 0;\n\n\tif (rate && intv) {\n\t\trate64 = (u64)rate * tp->mss_cache * USEC_PER_SEC;\n\t\tdo_div(rate64, intv);\n\t}\n\treturn rate64;\n}\n\n \nvoid tcp_init_sock(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\ttp->out_of_order_queue = RB_ROOT;\n\tsk->tcp_rtx_queue = RB_ROOT;\n\ttcp_init_xmit_timers(sk);\n\tINIT_LIST_HEAD(&tp->tsq_node);\n\tINIT_LIST_HEAD(&tp->tsorted_sent_queue);\n\n\ticsk->icsk_rto = TCP_TIMEOUT_INIT;\n\ticsk->icsk_rto_min = TCP_RTO_MIN;\n\ticsk->icsk_delack_max = TCP_DELACK_MAX;\n\ttp->mdev_us = jiffies_to_usecs(TCP_TIMEOUT_INIT);\n\tminmax_reset(&tp->rtt_min, tcp_jiffies32, ~0U);\n\n\t \n\ttcp_snd_cwnd_set(tp, TCP_INIT_CWND);\n\n\t \n\ttp->app_limited = ~0U;\n\ttp->rate_app_limited = 1;\n\n\t \n\ttp->snd_ssthresh = TCP_INFINITE_SSTHRESH;\n\ttp->snd_cwnd_clamp = ~0;\n\ttp->mss_cache = TCP_MSS_DEFAULT;\n\n\ttp->reordering = READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_reordering);\n\ttcp_assign_congestion_control(sk);\n\n\ttp->tsoffset = 0;\n\ttp->rack.reo_wnd_steps = 1;\n\n\tsk->sk_write_space = sk_stream_write_space;\n\tsock_set_flag(sk, SOCK_USE_WRITE_QUEUE);\n\n\ticsk->icsk_sync_mss = tcp_sync_mss;\n\n\tWRITE_ONCE(sk->sk_sndbuf, READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_wmem[1]));\n\tWRITE_ONCE(sk->sk_rcvbuf, READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_rmem[1]));\n\ttcp_scaling_ratio_init(sk);\n\n\tset_bit(SOCK_SUPPORT_ZC, &sk->sk_socket->flags);\n\tsk_sockets_allocated_inc(sk);\n}\nEXPORT_SYMBOL(tcp_init_sock);\n\nstatic void tcp_tx_timestamp(struct sock *sk, u16 tsflags)\n{\n\tstruct sk_buff *skb = tcp_write_queue_tail(sk);\n\n\tif (tsflags && skb) {\n\t\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\t\tstruct tcp_skb_cb *tcb = TCP_SKB_CB(skb);\n\n\t\tsock_tx_timestamp(sk, tsflags, &shinfo->tx_flags);\n\t\tif (tsflags & SOF_TIMESTAMPING_TX_ACK)\n\t\t\ttcb->txstamp_ack = 1;\n\t\tif (tsflags & SOF_TIMESTAMPING_TX_RECORD_MASK)\n\t\t\tshinfo->tskey = TCP_SKB_CB(skb)->seq + skb->len - 1;\n\t}\n}\n\nstatic bool tcp_stream_is_readable(struct sock *sk, int target)\n{\n\tif (tcp_epollin_ready(sk, target))\n\t\treturn true;\n\treturn sk_is_readable(sk);\n}\n\n \n__poll_t tcp_poll(struct file *file, struct socket *sock, poll_table *wait)\n{\n\t__poll_t mask;\n\tstruct sock *sk = sock->sk;\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tu8 shutdown;\n\tint state;\n\n\tsock_poll_wait(file, sock, wait);\n\n\tstate = inet_sk_state_load(sk);\n\tif (state == TCP_LISTEN)\n\t\treturn inet_csk_listen_poll(sk);\n\n\t \n\n\tmask = 0;\n\n\t \n\tshutdown = READ_ONCE(sk->sk_shutdown);\n\tif (shutdown == SHUTDOWN_MASK || state == TCP_CLOSE)\n\t\tmask |= EPOLLHUP;\n\tif (shutdown & RCV_SHUTDOWN)\n\t\tmask |= EPOLLIN | EPOLLRDNORM | EPOLLRDHUP;\n\n\t \n\tif (state != TCP_SYN_SENT &&\n\t    (state != TCP_SYN_RECV || rcu_access_pointer(tp->fastopen_rsk))) {\n\t\tint target = sock_rcvlowat(sk, 0, INT_MAX);\n\t\tu16 urg_data = READ_ONCE(tp->urg_data);\n\n\t\tif (unlikely(urg_data) &&\n\t\t    READ_ONCE(tp->urg_seq) == READ_ONCE(tp->copied_seq) &&\n\t\t    !sock_flag(sk, SOCK_URGINLINE))\n\t\t\ttarget++;\n\n\t\tif (tcp_stream_is_readable(sk, target))\n\t\t\tmask |= EPOLLIN | EPOLLRDNORM;\n\n\t\tif (!(shutdown & SEND_SHUTDOWN)) {\n\t\t\tif (__sk_stream_is_writeable(sk, 1)) {\n\t\t\t\tmask |= EPOLLOUT | EPOLLWRNORM;\n\t\t\t} else {   \n\t\t\t\tsk_set_bit(SOCKWQ_ASYNC_NOSPACE, sk);\n\t\t\t\tset_bit(SOCK_NOSPACE, &sk->sk_socket->flags);\n\n\t\t\t\t \n\t\t\t\tsmp_mb__after_atomic();\n\t\t\t\tif (__sk_stream_is_writeable(sk, 1))\n\t\t\t\t\tmask |= EPOLLOUT | EPOLLWRNORM;\n\t\t\t}\n\t\t} else\n\t\t\tmask |= EPOLLOUT | EPOLLWRNORM;\n\n\t\tif (urg_data & TCP_URG_VALID)\n\t\t\tmask |= EPOLLPRI;\n\t} else if (state == TCP_SYN_SENT &&\n\t\t   inet_test_bit(DEFER_CONNECT, sk)) {\n\t\t \n\t\tmask |= EPOLLOUT | EPOLLWRNORM;\n\t}\n\t \n\tsmp_rmb();\n\tif (READ_ONCE(sk->sk_err) ||\n\t    !skb_queue_empty_lockless(&sk->sk_error_queue))\n\t\tmask |= EPOLLERR;\n\n\treturn mask;\n}\nEXPORT_SYMBOL(tcp_poll);\n\nint tcp_ioctl(struct sock *sk, int cmd, int *karg)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint answ;\n\tbool slow;\n\n\tswitch (cmd) {\n\tcase SIOCINQ:\n\t\tif (sk->sk_state == TCP_LISTEN)\n\t\t\treturn -EINVAL;\n\n\t\tslow = lock_sock_fast(sk);\n\t\tansw = tcp_inq(sk);\n\t\tunlock_sock_fast(sk, slow);\n\t\tbreak;\n\tcase SIOCATMARK:\n\t\tansw = READ_ONCE(tp->urg_data) &&\n\t\t       READ_ONCE(tp->urg_seq) == READ_ONCE(tp->copied_seq);\n\t\tbreak;\n\tcase SIOCOUTQ:\n\t\tif (sk->sk_state == TCP_LISTEN)\n\t\t\treturn -EINVAL;\n\n\t\tif ((1 << sk->sk_state) & (TCPF_SYN_SENT | TCPF_SYN_RECV))\n\t\t\tansw = 0;\n\t\telse\n\t\t\tansw = READ_ONCE(tp->write_seq) - tp->snd_una;\n\t\tbreak;\n\tcase SIOCOUTQNSD:\n\t\tif (sk->sk_state == TCP_LISTEN)\n\t\t\treturn -EINVAL;\n\n\t\tif ((1 << sk->sk_state) & (TCPF_SYN_SENT | TCPF_SYN_RECV))\n\t\t\tansw = 0;\n\t\telse\n\t\t\tansw = READ_ONCE(tp->write_seq) -\n\t\t\t       READ_ONCE(tp->snd_nxt);\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOIOCTLCMD;\n\t}\n\n\t*karg = answ;\n\treturn 0;\n}\nEXPORT_SYMBOL(tcp_ioctl);\n\nvoid tcp_mark_push(struct tcp_sock *tp, struct sk_buff *skb)\n{\n\tTCP_SKB_CB(skb)->tcp_flags |= TCPHDR_PSH;\n\ttp->pushed_seq = tp->write_seq;\n}\n\nstatic inline bool forced_push(const struct tcp_sock *tp)\n{\n\treturn after(tp->write_seq, tp->pushed_seq + (tp->max_window >> 1));\n}\n\nvoid tcp_skb_entail(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct tcp_skb_cb *tcb = TCP_SKB_CB(skb);\n\n\ttcb->seq     = tcb->end_seq = tp->write_seq;\n\ttcb->tcp_flags = TCPHDR_ACK;\n\t__skb_header_release(skb);\n\ttcp_add_write_queue_tail(sk, skb);\n\tsk_wmem_queued_add(sk, skb->truesize);\n\tsk_mem_charge(sk, skb->truesize);\n\tif (tp->nonagle & TCP_NAGLE_PUSH)\n\t\ttp->nonagle &= ~TCP_NAGLE_PUSH;\n\n\ttcp_slow_start_after_idle_check(sk);\n}\n\nstatic inline void tcp_mark_urg(struct tcp_sock *tp, int flags)\n{\n\tif (flags & MSG_OOB)\n\t\ttp->snd_up = tp->write_seq;\n}\n\n \nstatic bool tcp_should_autocork(struct sock *sk, struct sk_buff *skb,\n\t\t\t\tint size_goal)\n{\n\treturn skb->len < size_goal &&\n\t       READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_autocorking) &&\n\t       !tcp_rtx_queue_empty(sk) &&\n\t       refcount_read(&sk->sk_wmem_alloc) > skb->truesize &&\n\t       tcp_skb_can_collapse_to(skb);\n}\n\nvoid tcp_push(struct sock *sk, int flags, int mss_now,\n\t      int nonagle, int size_goal)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb;\n\n\tskb = tcp_write_queue_tail(sk);\n\tif (!skb)\n\t\treturn;\n\tif (!(flags & MSG_MORE) || forced_push(tp))\n\t\ttcp_mark_push(tp, skb);\n\n\ttcp_mark_urg(tp, flags);\n\n\tif (tcp_should_autocork(sk, skb, size_goal)) {\n\n\t\t \n\t\tif (!test_bit(TSQ_THROTTLED, &sk->sk_tsq_flags)) {\n\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPAUTOCORKING);\n\t\t\tset_bit(TSQ_THROTTLED, &sk->sk_tsq_flags);\n\t\t}\n\t\t \n\t\tif (refcount_read(&sk->sk_wmem_alloc) > skb->truesize)\n\t\t\treturn;\n\t}\n\n\tif (flags & MSG_MORE)\n\t\tnonagle = TCP_NAGLE_CORK;\n\n\t__tcp_push_pending_frames(sk, mss_now, nonagle);\n}\n\nstatic int tcp_splice_data_recv(read_descriptor_t *rd_desc, struct sk_buff *skb,\n\t\t\t\tunsigned int offset, size_t len)\n{\n\tstruct tcp_splice_state *tss = rd_desc->arg.data;\n\tint ret;\n\n\tret = skb_splice_bits(skb, skb->sk, offset, tss->pipe,\n\t\t\t      min(rd_desc->count, len), tss->flags);\n\tif (ret > 0)\n\t\trd_desc->count -= ret;\n\treturn ret;\n}\n\nstatic int __tcp_splice_read(struct sock *sk, struct tcp_splice_state *tss)\n{\n\t \n\tread_descriptor_t rd_desc = {\n\t\t.arg.data = tss,\n\t\t.count\t  = tss->len,\n\t};\n\n\treturn tcp_read_sock(sk, &rd_desc, tcp_splice_data_recv);\n}\n\n \nssize_t tcp_splice_read(struct socket *sock, loff_t *ppos,\n\t\t\tstruct pipe_inode_info *pipe, size_t len,\n\t\t\tunsigned int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct tcp_splice_state tss = {\n\t\t.pipe = pipe,\n\t\t.len = len,\n\t\t.flags = flags,\n\t};\n\tlong timeo;\n\tssize_t spliced;\n\tint ret;\n\n\tsock_rps_record_flow(sk);\n\t \n\tif (unlikely(*ppos))\n\t\treturn -ESPIPE;\n\n\tret = spliced = 0;\n\n\tlock_sock(sk);\n\n\ttimeo = sock_rcvtimeo(sk, sock->file->f_flags & O_NONBLOCK);\n\twhile (tss.len) {\n\t\tret = __tcp_splice_read(sk, &tss);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\t\telse if (!ret) {\n\t\t\tif (spliced)\n\t\t\t\tbreak;\n\t\t\tif (sock_flag(sk, SOCK_DONE))\n\t\t\t\tbreak;\n\t\t\tif (sk->sk_err) {\n\t\t\t\tret = sock_error(sk);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t\tbreak;\n\t\t\tif (sk->sk_state == TCP_CLOSE) {\n\t\t\t\t \n\t\t\t\tret = -ENOTCONN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (!timeo) {\n\t\t\t\tret = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t \n\t\t\tif (!skb_queue_empty(&sk->sk_receive_queue))\n\t\t\t\tbreak;\n\t\t\tret = sk_wait_data(sk, &timeo, NULL);\n\t\t\tif (ret < 0)\n\t\t\t\tbreak;\n\t\t\tif (signal_pending(current)) {\n\t\t\t\tret = sock_intr_errno(timeo);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\t\ttss.len -= ret;\n\t\tspliced += ret;\n\n\t\tif (!tss.len || !timeo)\n\t\t\tbreak;\n\t\trelease_sock(sk);\n\t\tlock_sock(sk);\n\n\t\tif (sk->sk_err || sk->sk_state == TCP_CLOSE ||\n\t\t    (sk->sk_shutdown & RCV_SHUTDOWN) ||\n\t\t    signal_pending(current))\n\t\t\tbreak;\n\t}\n\n\trelease_sock(sk);\n\n\tif (spliced)\n\t\treturn spliced;\n\n\treturn ret;\n}\nEXPORT_SYMBOL(tcp_splice_read);\n\nstruct sk_buff *tcp_stream_alloc_skb(struct sock *sk, gfp_t gfp,\n\t\t\t\t     bool force_schedule)\n{\n\tstruct sk_buff *skb;\n\n\tskb = alloc_skb_fclone(MAX_TCP_HEADER, gfp);\n\tif (likely(skb)) {\n\t\tbool mem_scheduled;\n\n\t\tskb->truesize = SKB_TRUESIZE(skb_end_offset(skb));\n\t\tif (force_schedule) {\n\t\t\tmem_scheduled = true;\n\t\t\tsk_forced_mem_schedule(sk, skb->truesize);\n\t\t} else {\n\t\t\tmem_scheduled = sk_wmem_schedule(sk, skb->truesize);\n\t\t}\n\t\tif (likely(mem_scheduled)) {\n\t\t\tskb_reserve(skb, MAX_TCP_HEADER);\n\t\t\tskb->ip_summed = CHECKSUM_PARTIAL;\n\t\t\tINIT_LIST_HEAD(&skb->tcp_tsorted_anchor);\n\t\t\treturn skb;\n\t\t}\n\t\t__kfree_skb(skb);\n\t} else {\n\t\tsk->sk_prot->enter_memory_pressure(sk);\n\t\tsk_stream_moderate_sndbuf(sk);\n\t}\n\treturn NULL;\n}\n\nstatic unsigned int tcp_xmit_size_goal(struct sock *sk, u32 mss_now,\n\t\t\t\t       int large_allowed)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 new_size_goal, size_goal;\n\n\tif (!large_allowed)\n\t\treturn mss_now;\n\n\t \n\tnew_size_goal = tcp_bound_to_half_wnd(tp, sk->sk_gso_max_size);\n\n\t \n\tsize_goal = tp->gso_segs * mss_now;\n\tif (unlikely(new_size_goal < size_goal ||\n\t\t     new_size_goal >= size_goal + mss_now)) {\n\t\ttp->gso_segs = min_t(u16, new_size_goal / mss_now,\n\t\t\t\t     sk->sk_gso_max_segs);\n\t\tsize_goal = tp->gso_segs * mss_now;\n\t}\n\n\treturn max(size_goal, mss_now);\n}\n\nint tcp_send_mss(struct sock *sk, int *size_goal, int flags)\n{\n\tint mss_now;\n\n\tmss_now = tcp_current_mss(sk);\n\t*size_goal = tcp_xmit_size_goal(sk, mss_now, !(flags & MSG_OOB));\n\n\treturn mss_now;\n}\n\n \nvoid tcp_remove_empty_skb(struct sock *sk)\n{\n\tstruct sk_buff *skb = tcp_write_queue_tail(sk);\n\n\tif (skb && TCP_SKB_CB(skb)->seq == TCP_SKB_CB(skb)->end_seq) {\n\t\ttcp_unlink_write_queue(skb, sk);\n\t\tif (tcp_write_queue_empty(sk))\n\t\t\ttcp_chrono_stop(sk, TCP_CHRONO_BUSY);\n\t\ttcp_wmem_free_skb(sk, skb);\n\t}\n}\n\n \nstatic int tcp_downgrade_zcopy_pure(struct sock *sk, struct sk_buff *skb)\n{\n\tif (unlikely(skb_zcopy_pure(skb))) {\n\t\tu32 extra = skb->truesize -\n\t\t\t    SKB_TRUESIZE(skb_end_offset(skb));\n\n\t\tif (!sk_wmem_schedule(sk, extra))\n\t\t\treturn -ENOMEM;\n\n\t\tsk_mem_charge(sk, extra);\n\t\tskb_shinfo(skb)->flags &= ~SKBFL_PURE_ZEROCOPY;\n\t}\n\treturn 0;\n}\n\n\nint tcp_wmem_schedule(struct sock *sk, int copy)\n{\n\tint left;\n\n\tif (likely(sk_wmem_schedule(sk, copy)))\n\t\treturn copy;\n\n\t \n\tleft = sock_net(sk)->ipv4.sysctl_tcp_wmem[0] - sk->sk_wmem_queued;\n\tif (left > 0)\n\t\tsk_forced_mem_schedule(sk, min(left, copy));\n\treturn min(copy, sk->sk_forward_alloc);\n}\n\nvoid tcp_free_fastopen_req(struct tcp_sock *tp)\n{\n\tif (tp->fastopen_req) {\n\t\tkfree(tp->fastopen_req);\n\t\ttp->fastopen_req = NULL;\n\t}\n}\n\nint tcp_sendmsg_fastopen(struct sock *sk, struct msghdr *msg, int *copied,\n\t\t\t size_t size, struct ubuf_info *uarg)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sockaddr *uaddr = msg->msg_name;\n\tint err, flags;\n\n\tif (!(READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_fastopen) &\n\t      TFO_CLIENT_ENABLE) ||\n\t    (uaddr && msg->msg_namelen >= sizeof(uaddr->sa_family) &&\n\t     uaddr->sa_family == AF_UNSPEC))\n\t\treturn -EOPNOTSUPP;\n\tif (tp->fastopen_req)\n\t\treturn -EALREADY;  \n\n\ttp->fastopen_req = kzalloc(sizeof(struct tcp_fastopen_request),\n\t\t\t\t   sk->sk_allocation);\n\tif (unlikely(!tp->fastopen_req))\n\t\treturn -ENOBUFS;\n\ttp->fastopen_req->data = msg;\n\ttp->fastopen_req->size = size;\n\ttp->fastopen_req->uarg = uarg;\n\n\tif (inet_test_bit(DEFER_CONNECT, sk)) {\n\t\terr = tcp_connect(sk);\n\t\t \n\t\tif (err) {\n\t\t\ttcp_set_state(sk, TCP_CLOSE);\n\t\t\tinet->inet_dport = 0;\n\t\t\tsk->sk_route_caps = 0;\n\t\t}\n\t}\n\tflags = (msg->msg_flags & MSG_DONTWAIT) ? O_NONBLOCK : 0;\n\terr = __inet_stream_connect(sk->sk_socket, uaddr,\n\t\t\t\t    msg->msg_namelen, flags, 1);\n\t \n\tif (tp->fastopen_req) {\n\t\t*copied = tp->fastopen_req->copied;\n\t\ttcp_free_fastopen_req(tp);\n\t\tinet_clear_bit(DEFER_CONNECT, sk);\n\t}\n\treturn err;\n}\n\nint tcp_sendmsg_locked(struct sock *sk, struct msghdr *msg, size_t size)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct ubuf_info *uarg = NULL;\n\tstruct sk_buff *skb;\n\tstruct sockcm_cookie sockc;\n\tint flags, err, copied = 0;\n\tint mss_now = 0, size_goal, copied_syn = 0;\n\tint process_backlog = 0;\n\tint zc = 0;\n\tlong timeo;\n\n\tflags = msg->msg_flags;\n\n\tif ((flags & MSG_ZEROCOPY) && size) {\n\t\tif (msg->msg_ubuf) {\n\t\t\tuarg = msg->msg_ubuf;\n\t\t\tif (sk->sk_route_caps & NETIF_F_SG)\n\t\t\t\tzc = MSG_ZEROCOPY;\n\t\t} else if (sock_flag(sk, SOCK_ZEROCOPY)) {\n\t\t\tskb = tcp_write_queue_tail(sk);\n\t\t\tuarg = msg_zerocopy_realloc(sk, size, skb_zcopy(skb));\n\t\t\tif (!uarg) {\n\t\t\t\terr = -ENOBUFS;\n\t\t\t\tgoto out_err;\n\t\t\t}\n\t\t\tif (sk->sk_route_caps & NETIF_F_SG)\n\t\t\t\tzc = MSG_ZEROCOPY;\n\t\t\telse\n\t\t\t\tuarg_to_msgzc(uarg)->zerocopy = 0;\n\t\t}\n\t} else if (unlikely(msg->msg_flags & MSG_SPLICE_PAGES) && size) {\n\t\tif (sk->sk_route_caps & NETIF_F_SG)\n\t\t\tzc = MSG_SPLICE_PAGES;\n\t}\n\n\tif (unlikely(flags & MSG_FASTOPEN ||\n\t\t     inet_test_bit(DEFER_CONNECT, sk)) &&\n\t    !tp->repair) {\n\t\terr = tcp_sendmsg_fastopen(sk, msg, &copied_syn, size, uarg);\n\t\tif (err == -EINPROGRESS && copied_syn > 0)\n\t\t\tgoto out;\n\t\telse if (err)\n\t\t\tgoto out_err;\n\t}\n\n\ttimeo = sock_sndtimeo(sk, flags & MSG_DONTWAIT);\n\n\ttcp_rate_check_app_limited(sk);   \n\n\t \n\tif (((1 << sk->sk_state) & ~(TCPF_ESTABLISHED | TCPF_CLOSE_WAIT)) &&\n\t    !tcp_passive_fastopen(sk)) {\n\t\terr = sk_stream_wait_connect(sk, &timeo);\n\t\tif (err != 0)\n\t\t\tgoto do_error;\n\t}\n\n\tif (unlikely(tp->repair)) {\n\t\tif (tp->repair_queue == TCP_RECV_QUEUE) {\n\t\t\tcopied = tcp_send_rcvq(sk, msg, size);\n\t\t\tgoto out_nopush;\n\t\t}\n\n\t\terr = -EINVAL;\n\t\tif (tp->repair_queue == TCP_NO_QUEUE)\n\t\t\tgoto out_err;\n\n\t\t \n\t}\n\n\tsockcm_init(&sockc, sk);\n\tif (msg->msg_controllen) {\n\t\terr = sock_cmsg_send(sk, msg, &sockc);\n\t\tif (unlikely(err)) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out_err;\n\t\t}\n\t}\n\n\t \n\tsk_clear_bit(SOCKWQ_ASYNC_NOSPACE, sk);\n\n\t \n\tcopied = 0;\n\nrestart:\n\tmss_now = tcp_send_mss(sk, &size_goal, flags);\n\n\terr = -EPIPE;\n\tif (sk->sk_err || (sk->sk_shutdown & SEND_SHUTDOWN))\n\t\tgoto do_error;\n\n\twhile (msg_data_left(msg)) {\n\t\tssize_t copy = 0;\n\n\t\tskb = tcp_write_queue_tail(sk);\n\t\tif (skb)\n\t\t\tcopy = size_goal - skb->len;\n\n\t\tif (copy <= 0 || !tcp_skb_can_collapse_to(skb)) {\n\t\t\tbool first_skb;\n\nnew_segment:\n\t\t\tif (!sk_stream_memory_free(sk))\n\t\t\t\tgoto wait_for_space;\n\n\t\t\tif (unlikely(process_backlog >= 16)) {\n\t\t\t\tprocess_backlog = 0;\n\t\t\t\tif (sk_flush_backlog(sk))\n\t\t\t\t\tgoto restart;\n\t\t\t}\n\t\t\tfirst_skb = tcp_rtx_and_write_queues_empty(sk);\n\t\t\tskb = tcp_stream_alloc_skb(sk, sk->sk_allocation,\n\t\t\t\t\t\t   first_skb);\n\t\t\tif (!skb)\n\t\t\t\tgoto wait_for_space;\n\n\t\t\tprocess_backlog++;\n\n\t\t\ttcp_skb_entail(sk, skb);\n\t\t\tcopy = size_goal;\n\n\t\t\t \n\t\t\tif (tp->repair)\n\t\t\t\tTCP_SKB_CB(skb)->sacked |= TCPCB_REPAIRED;\n\t\t}\n\n\t\t \n\t\tif (copy > msg_data_left(msg))\n\t\t\tcopy = msg_data_left(msg);\n\n\t\tif (zc == 0) {\n\t\t\tbool merge = true;\n\t\t\tint i = skb_shinfo(skb)->nr_frags;\n\t\t\tstruct page_frag *pfrag = sk_page_frag(sk);\n\n\t\t\tif (!sk_page_frag_refill(sk, pfrag))\n\t\t\t\tgoto wait_for_space;\n\n\t\t\tif (!skb_can_coalesce(skb, i, pfrag->page,\n\t\t\t\t\t      pfrag->offset)) {\n\t\t\t\tif (i >= READ_ONCE(sysctl_max_skb_frags)) {\n\t\t\t\t\ttcp_mark_push(tp, skb);\n\t\t\t\t\tgoto new_segment;\n\t\t\t\t}\n\t\t\t\tmerge = false;\n\t\t\t}\n\n\t\t\tcopy = min_t(int, copy, pfrag->size - pfrag->offset);\n\n\t\t\tif (unlikely(skb_zcopy_pure(skb) || skb_zcopy_managed(skb))) {\n\t\t\t\tif (tcp_downgrade_zcopy_pure(sk, skb))\n\t\t\t\t\tgoto wait_for_space;\n\t\t\t\tskb_zcopy_downgrade_managed(skb);\n\t\t\t}\n\n\t\t\tcopy = tcp_wmem_schedule(sk, copy);\n\t\t\tif (!copy)\n\t\t\t\tgoto wait_for_space;\n\n\t\t\terr = skb_copy_to_page_nocache(sk, &msg->msg_iter, skb,\n\t\t\t\t\t\t       pfrag->page,\n\t\t\t\t\t\t       pfrag->offset,\n\t\t\t\t\t\t       copy);\n\t\t\tif (err)\n\t\t\t\tgoto do_error;\n\n\t\t\t \n\t\t\tif (merge) {\n\t\t\t\tskb_frag_size_add(&skb_shinfo(skb)->frags[i - 1], copy);\n\t\t\t} else {\n\t\t\t\tskb_fill_page_desc(skb, i, pfrag->page,\n\t\t\t\t\t\t   pfrag->offset, copy);\n\t\t\t\tpage_ref_inc(pfrag->page);\n\t\t\t}\n\t\t\tpfrag->offset += copy;\n\t\t} else if (zc == MSG_ZEROCOPY)  {\n\t\t\t \n\t\t\tif (!skb->len)\n\t\t\t\tskb_shinfo(skb)->flags |= SKBFL_PURE_ZEROCOPY;\n\n\t\t\tif (!skb_zcopy_pure(skb)) {\n\t\t\t\tcopy = tcp_wmem_schedule(sk, copy);\n\t\t\t\tif (!copy)\n\t\t\t\t\tgoto wait_for_space;\n\t\t\t}\n\n\t\t\terr = skb_zerocopy_iter_stream(sk, skb, msg, copy, uarg);\n\t\t\tif (err == -EMSGSIZE || err == -EEXIST) {\n\t\t\t\ttcp_mark_push(tp, skb);\n\t\t\t\tgoto new_segment;\n\t\t\t}\n\t\t\tif (err < 0)\n\t\t\t\tgoto do_error;\n\t\t\tcopy = err;\n\t\t} else if (zc == MSG_SPLICE_PAGES) {\n\t\t\t \n\t\t\tif (tcp_downgrade_zcopy_pure(sk, skb))\n\t\t\t\tgoto wait_for_space;\n\t\t\tcopy = tcp_wmem_schedule(sk, copy);\n\t\t\tif (!copy)\n\t\t\t\tgoto wait_for_space;\n\n\t\t\terr = skb_splice_from_iter(skb, &msg->msg_iter, copy,\n\t\t\t\t\t\t   sk->sk_allocation);\n\t\t\tif (err < 0) {\n\t\t\t\tif (err == -EMSGSIZE) {\n\t\t\t\t\ttcp_mark_push(tp, skb);\n\t\t\t\t\tgoto new_segment;\n\t\t\t\t}\n\t\t\t\tgoto do_error;\n\t\t\t}\n\t\t\tcopy = err;\n\n\t\t\tif (!(flags & MSG_NO_SHARED_FRAGS))\n\t\t\t\tskb_shinfo(skb)->flags |= SKBFL_SHARED_FRAG;\n\n\t\t\tsk_wmem_queued_add(sk, copy);\n\t\t\tsk_mem_charge(sk, copy);\n\t\t}\n\n\t\tif (!copied)\n\t\t\tTCP_SKB_CB(skb)->tcp_flags &= ~TCPHDR_PSH;\n\n\t\tWRITE_ONCE(tp->write_seq, tp->write_seq + copy);\n\t\tTCP_SKB_CB(skb)->end_seq += copy;\n\t\ttcp_skb_pcount_set(skb, 0);\n\n\t\tcopied += copy;\n\t\tif (!msg_data_left(msg)) {\n\t\t\tif (unlikely(flags & MSG_EOR))\n\t\t\t\tTCP_SKB_CB(skb)->eor = 1;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (skb->len < size_goal || (flags & MSG_OOB) || unlikely(tp->repair))\n\t\t\tcontinue;\n\n\t\tif (forced_push(tp)) {\n\t\t\ttcp_mark_push(tp, skb);\n\t\t\t__tcp_push_pending_frames(sk, mss_now, TCP_NAGLE_PUSH);\n\t\t} else if (skb == tcp_send_head(sk))\n\t\t\ttcp_push_one(sk, mss_now);\n\t\tcontinue;\n\nwait_for_space:\n\t\tset_bit(SOCK_NOSPACE, &sk->sk_socket->flags);\n\t\ttcp_remove_empty_skb(sk);\n\t\tif (copied)\n\t\t\ttcp_push(sk, flags & ~MSG_MORE, mss_now,\n\t\t\t\t TCP_NAGLE_PUSH, size_goal);\n\n\t\terr = sk_stream_wait_memory(sk, &timeo);\n\t\tif (err != 0)\n\t\t\tgoto do_error;\n\n\t\tmss_now = tcp_send_mss(sk, &size_goal, flags);\n\t}\n\nout:\n\tif (copied) {\n\t\ttcp_tx_timestamp(sk, sockc.tsflags);\n\t\ttcp_push(sk, flags, mss_now, tp->nonagle, size_goal);\n\t}\nout_nopush:\n\t \n\tif (uarg && !msg->msg_ubuf)\n\t\tnet_zcopy_put(uarg);\n\treturn copied + copied_syn;\n\ndo_error:\n\ttcp_remove_empty_skb(sk);\n\n\tif (copied + copied_syn)\n\t\tgoto out;\nout_err:\n\t \n\tif (uarg && !msg->msg_ubuf)\n\t\tnet_zcopy_put_abort(uarg, true);\n\terr = sk_stream_error(sk, flags, err);\n\t \n\tif (unlikely(tcp_rtx_and_write_queues_empty(sk) && err == -EAGAIN)) {\n\t\tsk->sk_write_space(sk);\n\t\ttcp_chrono_stop(sk, TCP_CHRONO_SNDBUF_LIMITED);\n\t}\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(tcp_sendmsg_locked);\n\nint tcp_sendmsg(struct sock *sk, struct msghdr *msg, size_t size)\n{\n\tint ret;\n\n\tlock_sock(sk);\n\tret = tcp_sendmsg_locked(sk, msg, size);\n\trelease_sock(sk);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(tcp_sendmsg);\n\nvoid tcp_splice_eof(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint mss_now, size_goal;\n\n\tif (!tcp_write_queue_tail(sk))\n\t\treturn;\n\n\tlock_sock(sk);\n\tmss_now = tcp_send_mss(sk, &size_goal, 0);\n\ttcp_push(sk, 0, mss_now, tp->nonagle, size_goal);\n\trelease_sock(sk);\n}\nEXPORT_SYMBOL_GPL(tcp_splice_eof);\n\n \n\nstatic int tcp_recv_urg(struct sock *sk, struct msghdr *msg, int len, int flags)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t \n\tif (sock_flag(sk, SOCK_URGINLINE) || !tp->urg_data ||\n\t    tp->urg_data == TCP_URG_READ)\n\t\treturn -EINVAL;\t \n\n\tif (sk->sk_state == TCP_CLOSE && !sock_flag(sk, SOCK_DONE))\n\t\treturn -ENOTCONN;\n\n\tif (tp->urg_data & TCP_URG_VALID) {\n\t\tint err = 0;\n\t\tchar c = tp->urg_data;\n\n\t\tif (!(flags & MSG_PEEK))\n\t\t\tWRITE_ONCE(tp->urg_data, TCP_URG_READ);\n\n\t\t \n\t\tmsg->msg_flags |= MSG_OOB;\n\n\t\tif (len > 0) {\n\t\t\tif (!(flags & MSG_TRUNC))\n\t\t\t\terr = memcpy_to_msg(msg, &c, 1);\n\t\t\tlen = 1;\n\t\t} else\n\t\t\tmsg->msg_flags |= MSG_TRUNC;\n\n\t\treturn err ? -EFAULT : len;\n\t}\n\n\tif (sk->sk_state == TCP_CLOSE || (sk->sk_shutdown & RCV_SHUTDOWN))\n\t\treturn 0;\n\n\t \n\treturn -EAGAIN;\n}\n\nstatic int tcp_peek_sndq(struct sock *sk, struct msghdr *msg, int len)\n{\n\tstruct sk_buff *skb;\n\tint copied = 0, err = 0;\n\n\t \n\n\tskb_rbtree_walk(skb, &sk->tcp_rtx_queue) {\n\t\terr = skb_copy_datagram_msg(skb, 0, msg, skb->len);\n\t\tif (err)\n\t\t\treturn err;\n\t\tcopied += skb->len;\n\t}\n\n\tskb_queue_walk(&sk->sk_write_queue, skb) {\n\t\terr = skb_copy_datagram_msg(skb, 0, msg, skb->len);\n\t\tif (err)\n\t\t\tbreak;\n\n\t\tcopied += skb->len;\n\t}\n\n\treturn err ?: copied;\n}\n\n \nvoid __tcp_cleanup_rbuf(struct sock *sk, int copied)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tbool time_to_ack = false;\n\n\tif (inet_csk_ack_scheduled(sk)) {\n\t\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\n\t\tif ( \n\t\t    tp->rcv_nxt - tp->rcv_wup > icsk->icsk_ack.rcv_mss ||\n\t\t     \n\t\t    (copied > 0 &&\n\t\t     ((icsk->icsk_ack.pending & ICSK_ACK_PUSHED2) ||\n\t\t      ((icsk->icsk_ack.pending & ICSK_ACK_PUSHED) &&\n\t\t       !inet_csk_in_pingpong_mode(sk))) &&\n\t\t      !atomic_read(&sk->sk_rmem_alloc)))\n\t\t\ttime_to_ack = true;\n\t}\n\n\t \n\tif (copied > 0 && !time_to_ack && !(sk->sk_shutdown & RCV_SHUTDOWN)) {\n\t\t__u32 rcv_window_now = tcp_receive_window(tp);\n\n\t\t \n\t\tif (2*rcv_window_now <= tp->window_clamp) {\n\t\t\t__u32 new_window = __tcp_select_window(sk);\n\n\t\t\t \n\t\t\tif (new_window && new_window >= 2 * rcv_window_now)\n\t\t\t\ttime_to_ack = true;\n\t\t}\n\t}\n\tif (time_to_ack)\n\t\ttcp_send_ack(sk);\n}\n\nvoid tcp_cleanup_rbuf(struct sock *sk, int copied)\n{\n\tstruct sk_buff *skb = skb_peek(&sk->sk_receive_queue);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tWARN(skb && !before(tp->copied_seq, TCP_SKB_CB(skb)->end_seq),\n\t     \"cleanup rbuf bug: copied %X seq %X rcvnxt %X\\n\",\n\t     tp->copied_seq, TCP_SKB_CB(skb)->end_seq, tp->rcv_nxt);\n\t__tcp_cleanup_rbuf(sk, copied);\n}\n\nstatic void tcp_eat_recv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\t__skb_unlink(skb, &sk->sk_receive_queue);\n\tif (likely(skb->destructor == sock_rfree)) {\n\t\tsock_rfree(skb);\n\t\tskb->destructor = NULL;\n\t\tskb->sk = NULL;\n\t\treturn skb_attempt_defer_free(skb);\n\t}\n\t__kfree_skb(skb);\n}\n\nstruct sk_buff *tcp_recv_skb(struct sock *sk, u32 seq, u32 *off)\n{\n\tstruct sk_buff *skb;\n\tu32 offset;\n\n\twhile ((skb = skb_peek(&sk->sk_receive_queue)) != NULL) {\n\t\toffset = seq - TCP_SKB_CB(skb)->seq;\n\t\tif (unlikely(TCP_SKB_CB(skb)->tcp_flags & TCPHDR_SYN)) {\n\t\t\tpr_err_once(\"%s: found a SYN, please report !\\n\", __func__);\n\t\t\toffset--;\n\t\t}\n\t\tif (offset < skb->len || (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN)) {\n\t\t\t*off = offset;\n\t\t\treturn skb;\n\t\t}\n\t\t \n\t\ttcp_eat_recv_skb(sk, skb);\n\t}\n\treturn NULL;\n}\nEXPORT_SYMBOL(tcp_recv_skb);\n\n \nint tcp_read_sock(struct sock *sk, read_descriptor_t *desc,\n\t\t  sk_read_actor_t recv_actor)\n{\n\tstruct sk_buff *skb;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 seq = tp->copied_seq;\n\tu32 offset;\n\tint copied = 0;\n\n\tif (sk->sk_state == TCP_LISTEN)\n\t\treturn -ENOTCONN;\n\twhile ((skb = tcp_recv_skb(sk, seq, &offset)) != NULL) {\n\t\tif (offset < skb->len) {\n\t\t\tint used;\n\t\t\tsize_t len;\n\n\t\t\tlen = skb->len - offset;\n\t\t\t \n\t\t\tif (unlikely(tp->urg_data)) {\n\t\t\t\tu32 urg_offset = tp->urg_seq - seq;\n\t\t\t\tif (urg_offset < len)\n\t\t\t\t\tlen = urg_offset;\n\t\t\t\tif (!len)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tused = recv_actor(desc, skb, offset, len);\n\t\t\tif (used <= 0) {\n\t\t\t\tif (!copied)\n\t\t\t\t\tcopied = used;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (WARN_ON_ONCE(used > len))\n\t\t\t\tused = len;\n\t\t\tseq += used;\n\t\t\tcopied += used;\n\t\t\toffset += used;\n\n\t\t\t \n\t\t\tskb = tcp_recv_skb(sk, seq - 1, &offset);\n\t\t\tif (!skb)\n\t\t\t\tbreak;\n\t\t\t \n\t\t\tif (offset + 1 != skb->len)\n\t\t\t\tcontinue;\n\t\t}\n\t\tif (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN) {\n\t\t\ttcp_eat_recv_skb(sk, skb);\n\t\t\t++seq;\n\t\t\tbreak;\n\t\t}\n\t\ttcp_eat_recv_skb(sk, skb);\n\t\tif (!desc->count)\n\t\t\tbreak;\n\t\tWRITE_ONCE(tp->copied_seq, seq);\n\t}\n\tWRITE_ONCE(tp->copied_seq, seq);\n\n\ttcp_rcv_space_adjust(sk);\n\n\t \n\tif (copied > 0) {\n\t\ttcp_recv_skb(sk, seq, &offset);\n\t\ttcp_cleanup_rbuf(sk, copied);\n\t}\n\treturn copied;\n}\nEXPORT_SYMBOL(tcp_read_sock);\n\nint tcp_read_skb(struct sock *sk, skb_read_actor_t recv_actor)\n{\n\tstruct sk_buff *skb;\n\tint copied = 0;\n\n\tif (sk->sk_state == TCP_LISTEN)\n\t\treturn -ENOTCONN;\n\n\twhile ((skb = skb_peek(&sk->sk_receive_queue)) != NULL) {\n\t\tu8 tcp_flags;\n\t\tint used;\n\n\t\t__skb_unlink(skb, &sk->sk_receive_queue);\n\t\tWARN_ON_ONCE(!skb_set_owner_sk_safe(skb, sk));\n\t\ttcp_flags = TCP_SKB_CB(skb)->tcp_flags;\n\t\tused = recv_actor(sk, skb);\n\t\tif (used < 0) {\n\t\t\tif (!copied)\n\t\t\t\tcopied = used;\n\t\t\tbreak;\n\t\t}\n\t\tcopied += used;\n\n\t\tif (tcp_flags & TCPHDR_FIN)\n\t\t\tbreak;\n\t}\n\treturn copied;\n}\nEXPORT_SYMBOL(tcp_read_skb);\n\nvoid tcp_read_done(struct sock *sk, size_t len)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 seq = tp->copied_seq;\n\tstruct sk_buff *skb;\n\tsize_t left;\n\tu32 offset;\n\n\tif (sk->sk_state == TCP_LISTEN)\n\t\treturn;\n\n\tleft = len;\n\twhile (left && (skb = tcp_recv_skb(sk, seq, &offset)) != NULL) {\n\t\tint used;\n\n\t\tused = min_t(size_t, skb->len - offset, left);\n\t\tseq += used;\n\t\tleft -= used;\n\n\t\tif (skb->len > offset + used)\n\t\t\tbreak;\n\n\t\tif (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN) {\n\t\t\ttcp_eat_recv_skb(sk, skb);\n\t\t\t++seq;\n\t\t\tbreak;\n\t\t}\n\t\ttcp_eat_recv_skb(sk, skb);\n\t}\n\tWRITE_ONCE(tp->copied_seq, seq);\n\n\ttcp_rcv_space_adjust(sk);\n\n\t \n\tif (left != len)\n\t\ttcp_cleanup_rbuf(sk, len - left);\n}\nEXPORT_SYMBOL(tcp_read_done);\n\nint tcp_peek_len(struct socket *sock)\n{\n\treturn tcp_inq(sock->sk);\n}\nEXPORT_SYMBOL(tcp_peek_len);\n\n \nint tcp_set_rcvlowat(struct sock *sk, int val)\n{\n\tint space, cap;\n\n\tif (sk->sk_userlocks & SOCK_RCVBUF_LOCK)\n\t\tcap = sk->sk_rcvbuf >> 1;\n\telse\n\t\tcap = READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_rmem[2]) >> 1;\n\tval = min(val, cap);\n\tWRITE_ONCE(sk->sk_rcvlowat, val ? : 1);\n\n\t \n\ttcp_data_ready(sk);\n\n\tif (sk->sk_userlocks & SOCK_RCVBUF_LOCK)\n\t\treturn 0;\n\n\tspace = tcp_space_from_win(sk, val);\n\tif (space > sk->sk_rcvbuf) {\n\t\tWRITE_ONCE(sk->sk_rcvbuf, space);\n\t\ttcp_sk(sk)->window_clamp = val;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(tcp_set_rcvlowat);\n\nvoid tcp_update_recv_tstamps(struct sk_buff *skb,\n\t\t\t     struct scm_timestamping_internal *tss)\n{\n\tif (skb->tstamp)\n\t\ttss->ts[0] = ktime_to_timespec64(skb->tstamp);\n\telse\n\t\ttss->ts[0] = (struct timespec64) {0};\n\n\tif (skb_hwtstamps(skb)->hwtstamp)\n\t\ttss->ts[2] = ktime_to_timespec64(skb_hwtstamps(skb)->hwtstamp);\n\telse\n\t\ttss->ts[2] = (struct timespec64) {0};\n}\n\n#ifdef CONFIG_MMU\nstatic const struct vm_operations_struct tcp_vm_ops = {\n};\n\nint tcp_mmap(struct file *file, struct socket *sock,\n\t     struct vm_area_struct *vma)\n{\n\tif (vma->vm_flags & (VM_WRITE | VM_EXEC))\n\t\treturn -EPERM;\n\tvm_flags_clear(vma, VM_MAYWRITE | VM_MAYEXEC);\n\n\t \n\tvm_flags_set(vma, VM_MIXEDMAP);\n\n\tvma->vm_ops = &tcp_vm_ops;\n\treturn 0;\n}\nEXPORT_SYMBOL(tcp_mmap);\n\nstatic skb_frag_t *skb_advance_to_frag(struct sk_buff *skb, u32 offset_skb,\n\t\t\t\t       u32 *offset_frag)\n{\n\tskb_frag_t *frag;\n\n\tif (unlikely(offset_skb >= skb->len))\n\t\treturn NULL;\n\n\toffset_skb -= skb_headlen(skb);\n\tif ((int)offset_skb < 0 || skb_has_frag_list(skb))\n\t\treturn NULL;\n\n\tfrag = skb_shinfo(skb)->frags;\n\twhile (offset_skb) {\n\t\tif (skb_frag_size(frag) > offset_skb) {\n\t\t\t*offset_frag = offset_skb;\n\t\t\treturn frag;\n\t\t}\n\t\toffset_skb -= skb_frag_size(frag);\n\t\t++frag;\n\t}\n\t*offset_frag = 0;\n\treturn frag;\n}\n\nstatic bool can_map_frag(const skb_frag_t *frag)\n{\n\treturn skb_frag_size(frag) == PAGE_SIZE && !skb_frag_off(frag);\n}\n\nstatic int find_next_mappable_frag(const skb_frag_t *frag,\n\t\t\t\t   int remaining_in_skb)\n{\n\tint offset = 0;\n\n\tif (likely(can_map_frag(frag)))\n\t\treturn 0;\n\n\twhile (offset < remaining_in_skb && !can_map_frag(frag)) {\n\t\toffset += skb_frag_size(frag);\n\t\t++frag;\n\t}\n\treturn offset;\n}\n\nstatic void tcp_zerocopy_set_hint_for_skb(struct sock *sk,\n\t\t\t\t\t  struct tcp_zerocopy_receive *zc,\n\t\t\t\t\t  struct sk_buff *skb, u32 offset)\n{\n\tu32 frag_offset, partial_frag_remainder = 0;\n\tint mappable_offset;\n\tskb_frag_t *frag;\n\n\t \n\tzc->recv_skip_hint = skb->len - offset;\n\n\t \n\tfrag = skb_advance_to_frag(skb, offset, &frag_offset);\n\tif (!frag)\n\t\treturn;\n\n\tif (frag_offset) {\n\t\tstruct skb_shared_info *info = skb_shinfo(skb);\n\n\t\t \n\t\tif (frag == &info->frags[info->nr_frags - 1])\n\t\t\treturn;\n\n\t\t \n\t\tpartial_frag_remainder = skb_frag_size(frag) - frag_offset;\n\t\tzc->recv_skip_hint -= partial_frag_remainder;\n\t\t++frag;\n\t}\n\n\t \n\tmappable_offset = find_next_mappable_frag(frag, zc->recv_skip_hint);\n\tzc->recv_skip_hint = mappable_offset + partial_frag_remainder;\n}\n\nstatic int tcp_recvmsg_locked(struct sock *sk, struct msghdr *msg, size_t len,\n\t\t\t      int flags, struct scm_timestamping_internal *tss,\n\t\t\t      int *cmsg_flags);\nstatic int receive_fallback_to_copy(struct sock *sk,\n\t\t\t\t    struct tcp_zerocopy_receive *zc, int inq,\n\t\t\t\t    struct scm_timestamping_internal *tss)\n{\n\tunsigned long copy_address = (unsigned long)zc->copybuf_address;\n\tstruct msghdr msg = {};\n\tstruct iovec iov;\n\tint err;\n\n\tzc->length = 0;\n\tzc->recv_skip_hint = 0;\n\n\tif (copy_address != zc->copybuf_address)\n\t\treturn -EINVAL;\n\n\terr = import_single_range(ITER_DEST, (void __user *)copy_address,\n\t\t\t\t  inq, &iov, &msg.msg_iter);\n\tif (err)\n\t\treturn err;\n\n\terr = tcp_recvmsg_locked(sk, &msg, inq, MSG_DONTWAIT,\n\t\t\t\t tss, &zc->msg_flags);\n\tif (err < 0)\n\t\treturn err;\n\n\tzc->copybuf_len = err;\n\tif (likely(zc->copybuf_len)) {\n\t\tstruct sk_buff *skb;\n\t\tu32 offset;\n\n\t\tskb = tcp_recv_skb(sk, tcp_sk(sk)->copied_seq, &offset);\n\t\tif (skb)\n\t\t\ttcp_zerocopy_set_hint_for_skb(sk, zc, skb, offset);\n\t}\n\treturn 0;\n}\n\nstatic int tcp_copy_straggler_data(struct tcp_zerocopy_receive *zc,\n\t\t\t\t   struct sk_buff *skb, u32 copylen,\n\t\t\t\t   u32 *offset, u32 *seq)\n{\n\tunsigned long copy_address = (unsigned long)zc->copybuf_address;\n\tstruct msghdr msg = {};\n\tstruct iovec iov;\n\tint err;\n\n\tif (copy_address != zc->copybuf_address)\n\t\treturn -EINVAL;\n\n\terr = import_single_range(ITER_DEST, (void __user *)copy_address,\n\t\t\t\t  copylen, &iov, &msg.msg_iter);\n\tif (err)\n\t\treturn err;\n\terr = skb_copy_datagram_msg(skb, *offset, &msg, copylen);\n\tif (err)\n\t\treturn err;\n\tzc->recv_skip_hint -= copylen;\n\t*offset += copylen;\n\t*seq += copylen;\n\treturn (__s32)copylen;\n}\n\nstatic int tcp_zc_handle_leftover(struct tcp_zerocopy_receive *zc,\n\t\t\t\t  struct sock *sk,\n\t\t\t\t  struct sk_buff *skb,\n\t\t\t\t  u32 *seq,\n\t\t\t\t  s32 copybuf_len,\n\t\t\t\t  struct scm_timestamping_internal *tss)\n{\n\tu32 offset, copylen = min_t(u32, copybuf_len, zc->recv_skip_hint);\n\n\tif (!copylen)\n\t\treturn 0;\n\t \n\tif (skb) {\n\t\toffset = *seq - TCP_SKB_CB(skb)->seq;\n\t} else {\n\t\tskb = tcp_recv_skb(sk, *seq, &offset);\n\t\tif (TCP_SKB_CB(skb)->has_rxtstamp) {\n\t\t\ttcp_update_recv_tstamps(skb, tss);\n\t\t\tzc->msg_flags |= TCP_CMSG_TS;\n\t\t}\n\t}\n\n\tzc->copybuf_len = tcp_copy_straggler_data(zc, skb, copylen, &offset,\n\t\t\t\t\t\t  seq);\n\treturn zc->copybuf_len < 0 ? 0 : copylen;\n}\n\nstatic int tcp_zerocopy_vm_insert_batch_error(struct vm_area_struct *vma,\n\t\t\t\t\t      struct page **pending_pages,\n\t\t\t\t\t      unsigned long pages_remaining,\n\t\t\t\t\t      unsigned long *address,\n\t\t\t\t\t      u32 *length,\n\t\t\t\t\t      u32 *seq,\n\t\t\t\t\t      struct tcp_zerocopy_receive *zc,\n\t\t\t\t\t      u32 total_bytes_to_map,\n\t\t\t\t\t      int err)\n{\n\t \n\tif (err == -EBUSY &&\n\t    zc->flags & TCP_RECEIVE_ZEROCOPY_FLAG_TLB_CLEAN_HINT) {\n\t\tu32 maybe_zap_len;\n\n\t\tmaybe_zap_len = total_bytes_to_map -   \n\t\t\t\t*length +  \n\t\t\t\t(pages_remaining * PAGE_SIZE);  \n\t\tzap_page_range_single(vma, *address, maybe_zap_len, NULL);\n\t\terr = 0;\n\t}\n\n\tif (!err) {\n\t\tunsigned long leftover_pages = pages_remaining;\n\t\tint bytes_mapped;\n\n\t\t \n\t\terr = vm_insert_pages(vma, *address,\n\t\t\t\t      pending_pages,\n\t\t\t\t      &pages_remaining);\n\t\tbytes_mapped = PAGE_SIZE * (leftover_pages - pages_remaining);\n\t\t*seq += bytes_mapped;\n\t\t*address += bytes_mapped;\n\t}\n\tif (err) {\n\t\t \n\t\tconst int bytes_not_mapped = PAGE_SIZE * pages_remaining;\n\n\t\t*length -= bytes_not_mapped;\n\t\tzc->recv_skip_hint += bytes_not_mapped;\n\t}\n\treturn err;\n}\n\nstatic int tcp_zerocopy_vm_insert_batch(struct vm_area_struct *vma,\n\t\t\t\t\tstruct page **pages,\n\t\t\t\t\tunsigned int pages_to_map,\n\t\t\t\t\tunsigned long *address,\n\t\t\t\t\tu32 *length,\n\t\t\t\t\tu32 *seq,\n\t\t\t\t\tstruct tcp_zerocopy_receive *zc,\n\t\t\t\t\tu32 total_bytes_to_map)\n{\n\tunsigned long pages_remaining = pages_to_map;\n\tunsigned int pages_mapped;\n\tunsigned int bytes_mapped;\n\tint err;\n\n\terr = vm_insert_pages(vma, *address, pages, &pages_remaining);\n\tpages_mapped = pages_to_map - (unsigned int)pages_remaining;\n\tbytes_mapped = PAGE_SIZE * pages_mapped;\n\t \n\t*seq += bytes_mapped;\n\t*address += bytes_mapped;\n\n\tif (likely(!err))\n\t\treturn 0;\n\n\t \n\treturn tcp_zerocopy_vm_insert_batch_error(vma, pages + pages_mapped,\n\t\tpages_remaining, address, length, seq, zc, total_bytes_to_map,\n\t\terr);\n}\n\n#define TCP_VALID_ZC_MSG_FLAGS   (TCP_CMSG_TS)\nstatic void tcp_zc_finalize_rx_tstamp(struct sock *sk,\n\t\t\t\t      struct tcp_zerocopy_receive *zc,\n\t\t\t\t      struct scm_timestamping_internal *tss)\n{\n\tunsigned long msg_control_addr;\n\tstruct msghdr cmsg_dummy;\n\n\tmsg_control_addr = (unsigned long)zc->msg_control;\n\tcmsg_dummy.msg_control_user = (void __user *)msg_control_addr;\n\tcmsg_dummy.msg_controllen =\n\t\t(__kernel_size_t)zc->msg_controllen;\n\tcmsg_dummy.msg_flags = in_compat_syscall()\n\t\t? MSG_CMSG_COMPAT : 0;\n\tcmsg_dummy.msg_control_is_user = true;\n\tzc->msg_flags = 0;\n\tif (zc->msg_control == msg_control_addr &&\n\t    zc->msg_controllen == cmsg_dummy.msg_controllen) {\n\t\ttcp_recv_timestamp(&cmsg_dummy, sk, tss);\n\t\tzc->msg_control = (__u64)\n\t\t\t((uintptr_t)cmsg_dummy.msg_control_user);\n\t\tzc->msg_controllen =\n\t\t\t(__u64)cmsg_dummy.msg_controllen;\n\t\tzc->msg_flags = (__u32)cmsg_dummy.msg_flags;\n\t}\n}\n\nstatic struct vm_area_struct *find_tcp_vma(struct mm_struct *mm,\n\t\t\t\t\t   unsigned long address,\n\t\t\t\t\t   bool *mmap_locked)\n{\n\tstruct vm_area_struct *vma = lock_vma_under_rcu(mm, address);\n\n\tif (vma) {\n\t\tif (vma->vm_ops != &tcp_vm_ops) {\n\t\t\tvma_end_read(vma);\n\t\t\treturn NULL;\n\t\t}\n\t\t*mmap_locked = false;\n\t\treturn vma;\n\t}\n\n\tmmap_read_lock(mm);\n\tvma = vma_lookup(mm, address);\n\tif (!vma || vma->vm_ops != &tcp_vm_ops) {\n\t\tmmap_read_unlock(mm);\n\t\treturn NULL;\n\t}\n\t*mmap_locked = true;\n\treturn vma;\n}\n\n#define TCP_ZEROCOPY_PAGE_BATCH_SIZE 32\nstatic int tcp_zerocopy_receive(struct sock *sk,\n\t\t\t\tstruct tcp_zerocopy_receive *zc,\n\t\t\t\tstruct scm_timestamping_internal *tss)\n{\n\tu32 length = 0, offset, vma_len, avail_len, copylen = 0;\n\tunsigned long address = (unsigned long)zc->address;\n\tstruct page *pages[TCP_ZEROCOPY_PAGE_BATCH_SIZE];\n\ts32 copybuf_len = zc->copybuf_len;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tconst skb_frag_t *frags = NULL;\n\tunsigned int pages_to_map = 0;\n\tstruct vm_area_struct *vma;\n\tstruct sk_buff *skb = NULL;\n\tu32 seq = tp->copied_seq;\n\tu32 total_bytes_to_map;\n\tint inq = tcp_inq(sk);\n\tbool mmap_locked;\n\tint ret;\n\n\tzc->copybuf_len = 0;\n\tzc->msg_flags = 0;\n\n\tif (address & (PAGE_SIZE - 1) || address != zc->address)\n\t\treturn -EINVAL;\n\n\tif (sk->sk_state == TCP_LISTEN)\n\t\treturn -ENOTCONN;\n\n\tsock_rps_record_flow(sk);\n\n\tif (inq && inq <= copybuf_len)\n\t\treturn receive_fallback_to_copy(sk, zc, inq, tss);\n\n\tif (inq < PAGE_SIZE) {\n\t\tzc->length = 0;\n\t\tzc->recv_skip_hint = inq;\n\t\tif (!inq && sock_flag(sk, SOCK_DONE))\n\t\t\treturn -EIO;\n\t\treturn 0;\n\t}\n\n\tvma = find_tcp_vma(current->mm, address, &mmap_locked);\n\tif (!vma)\n\t\treturn -EINVAL;\n\n\tvma_len = min_t(unsigned long, zc->length, vma->vm_end - address);\n\tavail_len = min_t(u32, vma_len, inq);\n\ttotal_bytes_to_map = avail_len & ~(PAGE_SIZE - 1);\n\tif (total_bytes_to_map) {\n\t\tif (!(zc->flags & TCP_RECEIVE_ZEROCOPY_FLAG_TLB_CLEAN_HINT))\n\t\t\tzap_page_range_single(vma, address, total_bytes_to_map,\n\t\t\t\t\t      NULL);\n\t\tzc->length = total_bytes_to_map;\n\t\tzc->recv_skip_hint = 0;\n\t} else {\n\t\tzc->length = avail_len;\n\t\tzc->recv_skip_hint = avail_len;\n\t}\n\tret = 0;\n\twhile (length + PAGE_SIZE <= zc->length) {\n\t\tint mappable_offset;\n\t\tstruct page *page;\n\n\t\tif (zc->recv_skip_hint < PAGE_SIZE) {\n\t\t\tu32 offset_frag;\n\n\t\t\tif (skb) {\n\t\t\t\tif (zc->recv_skip_hint > 0)\n\t\t\t\t\tbreak;\n\t\t\t\tskb = skb->next;\n\t\t\t\toffset = seq - TCP_SKB_CB(skb)->seq;\n\t\t\t} else {\n\t\t\t\tskb = tcp_recv_skb(sk, seq, &offset);\n\t\t\t}\n\n\t\t\tif (TCP_SKB_CB(skb)->has_rxtstamp) {\n\t\t\t\ttcp_update_recv_tstamps(skb, tss);\n\t\t\t\tzc->msg_flags |= TCP_CMSG_TS;\n\t\t\t}\n\t\t\tzc->recv_skip_hint = skb->len - offset;\n\t\t\tfrags = skb_advance_to_frag(skb, offset, &offset_frag);\n\t\t\tif (!frags || offset_frag)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tmappable_offset = find_next_mappable_frag(frags,\n\t\t\t\t\t\t\t  zc->recv_skip_hint);\n\t\tif (mappable_offset) {\n\t\t\tzc->recv_skip_hint = mappable_offset;\n\t\t\tbreak;\n\t\t}\n\t\tpage = skb_frag_page(frags);\n\t\tprefetchw(page);\n\t\tpages[pages_to_map++] = page;\n\t\tlength += PAGE_SIZE;\n\t\tzc->recv_skip_hint -= PAGE_SIZE;\n\t\tfrags++;\n\t\tif (pages_to_map == TCP_ZEROCOPY_PAGE_BATCH_SIZE ||\n\t\t    zc->recv_skip_hint < PAGE_SIZE) {\n\t\t\t \n\t\t\tret = tcp_zerocopy_vm_insert_batch(vma, pages,\n\t\t\t\t\t\t\t   pages_to_map,\n\t\t\t\t\t\t\t   &address, &length,\n\t\t\t\t\t\t\t   &seq, zc,\n\t\t\t\t\t\t\t   total_bytes_to_map);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t\tpages_to_map = 0;\n\t\t}\n\t}\n\tif (pages_to_map) {\n\t\tret = tcp_zerocopy_vm_insert_batch(vma, pages, pages_to_map,\n\t\t\t\t\t\t   &address, &length, &seq,\n\t\t\t\t\t\t   zc, total_bytes_to_map);\n\t}\nout:\n\tif (mmap_locked)\n\t\tmmap_read_unlock(current->mm);\n\telse\n\t\tvma_end_read(vma);\n\t \n\tif (!ret)\n\t\tcopylen = tcp_zc_handle_leftover(zc, sk, skb, &seq, copybuf_len, tss);\n\n\tif (length + copylen) {\n\t\tWRITE_ONCE(tp->copied_seq, seq);\n\t\ttcp_rcv_space_adjust(sk);\n\n\t\t \n\t\ttcp_recv_skb(sk, seq, &offset);\n\t\ttcp_cleanup_rbuf(sk, length + copylen);\n\t\tret = 0;\n\t\tif (length == zc->length)\n\t\t\tzc->recv_skip_hint = 0;\n\t} else {\n\t\tif (!zc->recv_skip_hint && sock_flag(sk, SOCK_DONE))\n\t\t\tret = -EIO;\n\t}\n\tzc->length = length;\n\treturn ret;\n}\n#endif\n\n \nvoid tcp_recv_timestamp(struct msghdr *msg, const struct sock *sk,\n\t\t\tstruct scm_timestamping_internal *tss)\n{\n\tint new_tstamp = sock_flag(sk, SOCK_TSTAMP_NEW);\n\tbool has_timestamping = false;\n\n\tif (tss->ts[0].tv_sec || tss->ts[0].tv_nsec) {\n\t\tif (sock_flag(sk, SOCK_RCVTSTAMP)) {\n\t\t\tif (sock_flag(sk, SOCK_RCVTSTAMPNS)) {\n\t\t\t\tif (new_tstamp) {\n\t\t\t\t\tstruct __kernel_timespec kts = {\n\t\t\t\t\t\t.tv_sec = tss->ts[0].tv_sec,\n\t\t\t\t\t\t.tv_nsec = tss->ts[0].tv_nsec,\n\t\t\t\t\t};\n\t\t\t\t\tput_cmsg(msg, SOL_SOCKET, SO_TIMESTAMPNS_NEW,\n\t\t\t\t\t\t sizeof(kts), &kts);\n\t\t\t\t} else {\n\t\t\t\t\tstruct __kernel_old_timespec ts_old = {\n\t\t\t\t\t\t.tv_sec = tss->ts[0].tv_sec,\n\t\t\t\t\t\t.tv_nsec = tss->ts[0].tv_nsec,\n\t\t\t\t\t};\n\t\t\t\t\tput_cmsg(msg, SOL_SOCKET, SO_TIMESTAMPNS_OLD,\n\t\t\t\t\t\t sizeof(ts_old), &ts_old);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (new_tstamp) {\n\t\t\t\t\tstruct __kernel_sock_timeval stv = {\n\t\t\t\t\t\t.tv_sec = tss->ts[0].tv_sec,\n\t\t\t\t\t\t.tv_usec = tss->ts[0].tv_nsec / 1000,\n\t\t\t\t\t};\n\t\t\t\t\tput_cmsg(msg, SOL_SOCKET, SO_TIMESTAMP_NEW,\n\t\t\t\t\t\t sizeof(stv), &stv);\n\t\t\t\t} else {\n\t\t\t\t\tstruct __kernel_old_timeval tv = {\n\t\t\t\t\t\t.tv_sec = tss->ts[0].tv_sec,\n\t\t\t\t\t\t.tv_usec = tss->ts[0].tv_nsec / 1000,\n\t\t\t\t\t};\n\t\t\t\t\tput_cmsg(msg, SOL_SOCKET, SO_TIMESTAMP_OLD,\n\t\t\t\t\t\t sizeof(tv), &tv);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif (READ_ONCE(sk->sk_tsflags) & SOF_TIMESTAMPING_SOFTWARE)\n\t\t\thas_timestamping = true;\n\t\telse\n\t\t\ttss->ts[0] = (struct timespec64) {0};\n\t}\n\n\tif (tss->ts[2].tv_sec || tss->ts[2].tv_nsec) {\n\t\tif (READ_ONCE(sk->sk_tsflags) & SOF_TIMESTAMPING_RAW_HARDWARE)\n\t\t\thas_timestamping = true;\n\t\telse\n\t\t\ttss->ts[2] = (struct timespec64) {0};\n\t}\n\n\tif (has_timestamping) {\n\t\ttss->ts[1] = (struct timespec64) {0};\n\t\tif (sock_flag(sk, SOCK_TSTAMP_NEW))\n\t\t\tput_cmsg_scm_timestamping64(msg, tss);\n\t\telse\n\t\t\tput_cmsg_scm_timestamping(msg, tss);\n\t}\n}\n\nstatic int tcp_inq_hint(struct sock *sk)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tu32 copied_seq = READ_ONCE(tp->copied_seq);\n\tu32 rcv_nxt = READ_ONCE(tp->rcv_nxt);\n\tint inq;\n\n\tinq = rcv_nxt - copied_seq;\n\tif (unlikely(inq < 0 || copied_seq != READ_ONCE(tp->copied_seq))) {\n\t\tlock_sock(sk);\n\t\tinq = tp->rcv_nxt - tp->copied_seq;\n\t\trelease_sock(sk);\n\t}\n\t \n\tif (inq == 0 && sock_flag(sk, SOCK_DONE))\n\t\tinq = 1;\n\treturn inq;\n}\n\n \n\nstatic int tcp_recvmsg_locked(struct sock *sk, struct msghdr *msg, size_t len,\n\t\t\t      int flags, struct scm_timestamping_internal *tss,\n\t\t\t      int *cmsg_flags)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint copied = 0;\n\tu32 peek_seq;\n\tu32 *seq;\n\tunsigned long used;\n\tint err;\n\tint target;\t\t \n\tlong timeo;\n\tstruct sk_buff *skb, *last;\n\tu32 urg_hole = 0;\n\n\terr = -ENOTCONN;\n\tif (sk->sk_state == TCP_LISTEN)\n\t\tgoto out;\n\n\tif (tp->recvmsg_inq) {\n\t\t*cmsg_flags = TCP_CMSG_INQ;\n\t\tmsg->msg_get_inq = 1;\n\t}\n\ttimeo = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);\n\n\t \n\tif (flags & MSG_OOB)\n\t\tgoto recv_urg;\n\n\tif (unlikely(tp->repair)) {\n\t\terr = -EPERM;\n\t\tif (!(flags & MSG_PEEK))\n\t\t\tgoto out;\n\n\t\tif (tp->repair_queue == TCP_SEND_QUEUE)\n\t\t\tgoto recv_sndq;\n\n\t\terr = -EINVAL;\n\t\tif (tp->repair_queue == TCP_NO_QUEUE)\n\t\t\tgoto out;\n\n\t\t \n\t}\n\n\tseq = &tp->copied_seq;\n\tif (flags & MSG_PEEK) {\n\t\tpeek_seq = tp->copied_seq;\n\t\tseq = &peek_seq;\n\t}\n\n\ttarget = sock_rcvlowat(sk, flags & MSG_WAITALL, len);\n\n\tdo {\n\t\tu32 offset;\n\n\t\t \n\t\tif (unlikely(tp->urg_data) && tp->urg_seq == *seq) {\n\t\t\tif (copied)\n\t\t\t\tbreak;\n\t\t\tif (signal_pending(current)) {\n\t\t\t\tcopied = timeo ? sock_intr_errno(timeo) : -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\t \n\n\t\tlast = skb_peek_tail(&sk->sk_receive_queue);\n\t\tskb_queue_walk(&sk->sk_receive_queue, skb) {\n\t\t\tlast = skb;\n\t\t\t \n\t\t\tif (WARN(before(*seq, TCP_SKB_CB(skb)->seq),\n\t\t\t\t \"TCP recvmsg seq # bug: copied %X, seq %X, rcvnxt %X, fl %X\\n\",\n\t\t\t\t *seq, TCP_SKB_CB(skb)->seq, tp->rcv_nxt,\n\t\t\t\t flags))\n\t\t\t\tbreak;\n\n\t\t\toffset = *seq - TCP_SKB_CB(skb)->seq;\n\t\t\tif (unlikely(TCP_SKB_CB(skb)->tcp_flags & TCPHDR_SYN)) {\n\t\t\t\tpr_err_once(\"%s: found a SYN, please report !\\n\", __func__);\n\t\t\t\toffset--;\n\t\t\t}\n\t\t\tif (offset < skb->len)\n\t\t\t\tgoto found_ok_skb;\n\t\t\tif (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN)\n\t\t\t\tgoto found_fin_ok;\n\t\t\tWARN(!(flags & MSG_PEEK),\n\t\t\t     \"TCP recvmsg seq # bug 2: copied %X, seq %X, rcvnxt %X, fl %X\\n\",\n\t\t\t     *seq, TCP_SKB_CB(skb)->seq, tp->rcv_nxt, flags);\n\t\t}\n\n\t\t \n\n\t\tif (copied >= target && !READ_ONCE(sk->sk_backlog.tail))\n\t\t\tbreak;\n\n\t\tif (copied) {\n\t\t\tif (!timeo ||\n\t\t\t    sk->sk_err ||\n\t\t\t    sk->sk_state == TCP_CLOSE ||\n\t\t\t    (sk->sk_shutdown & RCV_SHUTDOWN) ||\n\t\t\t    signal_pending(current))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tif (sock_flag(sk, SOCK_DONE))\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_err) {\n\t\t\t\tcopied = sock_error(sk);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_state == TCP_CLOSE) {\n\t\t\t\t \n\t\t\t\tcopied = -ENOTCONN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (!timeo) {\n\t\t\t\tcopied = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (signal_pending(current)) {\n\t\t\t\tcopied = sock_intr_errno(timeo);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (copied >= target) {\n\t\t\t \n\t\t\t__sk_flush_backlog(sk);\n\t\t} else {\n\t\t\ttcp_cleanup_rbuf(sk, copied);\n\t\t\terr = sk_wait_data(sk, &timeo, last);\n\t\t\tif (err < 0) {\n\t\t\t\terr = copied ? : err;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\tif ((flags & MSG_PEEK) &&\n\t\t    (peek_seq - copied - urg_hole != tp->copied_seq)) {\n\t\t\tnet_dbg_ratelimited(\"TCP(%s:%d): Application bug, race in MSG_PEEK\\n\",\n\t\t\t\t\t    current->comm,\n\t\t\t\t\t    task_pid_nr(current));\n\t\t\tpeek_seq = tp->copied_seq;\n\t\t}\n\t\tcontinue;\n\nfound_ok_skb:\n\t\t \n\t\tused = skb->len - offset;\n\t\tif (len < used)\n\t\t\tused = len;\n\n\t\t \n\t\tif (unlikely(tp->urg_data)) {\n\t\t\tu32 urg_offset = tp->urg_seq - *seq;\n\t\t\tif (urg_offset < used) {\n\t\t\t\tif (!urg_offset) {\n\t\t\t\t\tif (!sock_flag(sk, SOCK_URGINLINE)) {\n\t\t\t\t\t\tWRITE_ONCE(*seq, *seq + 1);\n\t\t\t\t\t\turg_hole++;\n\t\t\t\t\t\toffset++;\n\t\t\t\t\t\tused--;\n\t\t\t\t\t\tif (!used)\n\t\t\t\t\t\t\tgoto skip_copy;\n\t\t\t\t\t}\n\t\t\t\t} else\n\t\t\t\t\tused = urg_offset;\n\t\t\t}\n\t\t}\n\n\t\tif (!(flags & MSG_TRUNC)) {\n\t\t\terr = skb_copy_datagram_msg(skb, offset, msg, used);\n\t\t\tif (err) {\n\t\t\t\t \n\t\t\t\tif (!copied)\n\t\t\t\t\tcopied = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tWRITE_ONCE(*seq, *seq + used);\n\t\tcopied += used;\n\t\tlen -= used;\n\n\t\ttcp_rcv_space_adjust(sk);\n\nskip_copy:\n\t\tif (unlikely(tp->urg_data) && after(tp->copied_seq, tp->urg_seq)) {\n\t\t\tWRITE_ONCE(tp->urg_data, 0);\n\t\t\ttcp_fast_path_check(sk);\n\t\t}\n\n\t\tif (TCP_SKB_CB(skb)->has_rxtstamp) {\n\t\t\ttcp_update_recv_tstamps(skb, tss);\n\t\t\t*cmsg_flags |= TCP_CMSG_TS;\n\t\t}\n\n\t\tif (used + offset < skb->len)\n\t\t\tcontinue;\n\n\t\tif (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN)\n\t\t\tgoto found_fin_ok;\n\t\tif (!(flags & MSG_PEEK))\n\t\t\ttcp_eat_recv_skb(sk, skb);\n\t\tcontinue;\n\nfound_fin_ok:\n\t\t \n\t\tWRITE_ONCE(*seq, *seq + 1);\n\t\tif (!(flags & MSG_PEEK))\n\t\t\ttcp_eat_recv_skb(sk, skb);\n\t\tbreak;\n\t} while (len > 0);\n\n\t \n\n\t \n\ttcp_cleanup_rbuf(sk, copied);\n\treturn copied;\n\nout:\n\treturn err;\n\nrecv_urg:\n\terr = tcp_recv_urg(sk, msg, len, flags);\n\tgoto out;\n\nrecv_sndq:\n\terr = tcp_peek_sndq(sk, msg, len);\n\tgoto out;\n}\n\nint tcp_recvmsg(struct sock *sk, struct msghdr *msg, size_t len, int flags,\n\t\tint *addr_len)\n{\n\tint cmsg_flags = 0, ret;\n\tstruct scm_timestamping_internal tss;\n\n\tif (unlikely(flags & MSG_ERRQUEUE))\n\t\treturn inet_recv_error(sk, msg, len, addr_len);\n\n\tif (sk_can_busy_loop(sk) &&\n\t    skb_queue_empty_lockless(&sk->sk_receive_queue) &&\n\t    sk->sk_state == TCP_ESTABLISHED)\n\t\tsk_busy_loop(sk, flags & MSG_DONTWAIT);\n\n\tlock_sock(sk);\n\tret = tcp_recvmsg_locked(sk, msg, len, flags, &tss, &cmsg_flags);\n\trelease_sock(sk);\n\n\tif ((cmsg_flags || msg->msg_get_inq) && ret >= 0) {\n\t\tif (cmsg_flags & TCP_CMSG_TS)\n\t\t\ttcp_recv_timestamp(msg, sk, &tss);\n\t\tif (msg->msg_get_inq) {\n\t\t\tmsg->msg_inq = tcp_inq_hint(sk);\n\t\t\tif (cmsg_flags & TCP_CMSG_INQ)\n\t\t\t\tput_cmsg(msg, SOL_TCP, TCP_CM_INQ,\n\t\t\t\t\t sizeof(msg->msg_inq), &msg->msg_inq);\n\t\t}\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL(tcp_recvmsg);\n\nvoid tcp_set_state(struct sock *sk, int state)\n{\n\tint oldstate = sk->sk_state;\n\n\t \n\tBUILD_BUG_ON((int)BPF_TCP_ESTABLISHED != (int)TCP_ESTABLISHED);\n\tBUILD_BUG_ON((int)BPF_TCP_SYN_SENT != (int)TCP_SYN_SENT);\n\tBUILD_BUG_ON((int)BPF_TCP_SYN_RECV != (int)TCP_SYN_RECV);\n\tBUILD_BUG_ON((int)BPF_TCP_FIN_WAIT1 != (int)TCP_FIN_WAIT1);\n\tBUILD_BUG_ON((int)BPF_TCP_FIN_WAIT2 != (int)TCP_FIN_WAIT2);\n\tBUILD_BUG_ON((int)BPF_TCP_TIME_WAIT != (int)TCP_TIME_WAIT);\n\tBUILD_BUG_ON((int)BPF_TCP_CLOSE != (int)TCP_CLOSE);\n\tBUILD_BUG_ON((int)BPF_TCP_CLOSE_WAIT != (int)TCP_CLOSE_WAIT);\n\tBUILD_BUG_ON((int)BPF_TCP_LAST_ACK != (int)TCP_LAST_ACK);\n\tBUILD_BUG_ON((int)BPF_TCP_LISTEN != (int)TCP_LISTEN);\n\tBUILD_BUG_ON((int)BPF_TCP_CLOSING != (int)TCP_CLOSING);\n\tBUILD_BUG_ON((int)BPF_TCP_NEW_SYN_RECV != (int)TCP_NEW_SYN_RECV);\n\tBUILD_BUG_ON((int)BPF_TCP_MAX_STATES != (int)TCP_MAX_STATES);\n\n\t \n\tBTF_TYPE_EMIT_ENUM(BPF_TCP_ESTABLISHED);\n\n\tif (BPF_SOCK_OPS_TEST_FLAG(tcp_sk(sk), BPF_SOCK_OPS_STATE_CB_FLAG))\n\t\ttcp_call_bpf_2arg(sk, BPF_SOCK_OPS_STATE_CB, oldstate, state);\n\n\tswitch (state) {\n\tcase TCP_ESTABLISHED:\n\t\tif (oldstate != TCP_ESTABLISHED)\n\t\t\tTCP_INC_STATS(sock_net(sk), TCP_MIB_CURRESTAB);\n\t\tbreak;\n\n\tcase TCP_CLOSE:\n\t\tif (oldstate == TCP_CLOSE_WAIT || oldstate == TCP_ESTABLISHED)\n\t\t\tTCP_INC_STATS(sock_net(sk), TCP_MIB_ESTABRESETS);\n\n\t\tsk->sk_prot->unhash(sk);\n\t\tif (inet_csk(sk)->icsk_bind_hash &&\n\t\t    !(sk->sk_userlocks & SOCK_BINDPORT_LOCK))\n\t\t\tinet_put_port(sk);\n\t\tfallthrough;\n\tdefault:\n\t\tif (oldstate == TCP_ESTABLISHED)\n\t\t\tTCP_DEC_STATS(sock_net(sk), TCP_MIB_CURRESTAB);\n\t}\n\n\t \n\tinet_sk_state_store(sk, state);\n}\nEXPORT_SYMBOL_GPL(tcp_set_state);\n\n \n\nstatic const unsigned char new_state[16] = {\n   \n  [0  ]\t= TCP_CLOSE,\n  [TCP_ESTABLISHED]\t= TCP_FIN_WAIT1 | TCP_ACTION_FIN,\n  [TCP_SYN_SENT]\t= TCP_CLOSE,\n  [TCP_SYN_RECV]\t= TCP_FIN_WAIT1 | TCP_ACTION_FIN,\n  [TCP_FIN_WAIT1]\t= TCP_FIN_WAIT1,\n  [TCP_FIN_WAIT2]\t= TCP_FIN_WAIT2,\n  [TCP_TIME_WAIT]\t= TCP_CLOSE,\n  [TCP_CLOSE]\t\t= TCP_CLOSE,\n  [TCP_CLOSE_WAIT]\t= TCP_LAST_ACK  | TCP_ACTION_FIN,\n  [TCP_LAST_ACK]\t= TCP_LAST_ACK,\n  [TCP_LISTEN]\t\t= TCP_CLOSE,\n  [TCP_CLOSING]\t\t= TCP_CLOSING,\n  [TCP_NEW_SYN_RECV]\t= TCP_CLOSE,\t \n};\n\nstatic int tcp_close_state(struct sock *sk)\n{\n\tint next = (int)new_state[sk->sk_state];\n\tint ns = next & TCP_STATE_MASK;\n\n\ttcp_set_state(sk, ns);\n\n\treturn next & TCP_ACTION_FIN;\n}\n\n \n\nvoid tcp_shutdown(struct sock *sk, int how)\n{\n\t \n\tif (!(how & SEND_SHUTDOWN))\n\t\treturn;\n\n\t \n\tif ((1 << sk->sk_state) &\n\t    (TCPF_ESTABLISHED | TCPF_SYN_SENT |\n\t     TCPF_SYN_RECV | TCPF_CLOSE_WAIT)) {\n\t\t \n\t\tif (tcp_close_state(sk))\n\t\t\ttcp_send_fin(sk);\n\t}\n}\nEXPORT_SYMBOL(tcp_shutdown);\n\nint tcp_orphan_count_sum(void)\n{\n\tint i, total = 0;\n\n\tfor_each_possible_cpu(i)\n\t\ttotal += per_cpu(tcp_orphan_count, i);\n\n\treturn max(total, 0);\n}\n\nstatic int tcp_orphan_cache;\nstatic struct timer_list tcp_orphan_timer;\n#define TCP_ORPHAN_TIMER_PERIOD msecs_to_jiffies(100)\n\nstatic void tcp_orphan_update(struct timer_list *unused)\n{\n\tWRITE_ONCE(tcp_orphan_cache, tcp_orphan_count_sum());\n\tmod_timer(&tcp_orphan_timer, jiffies + TCP_ORPHAN_TIMER_PERIOD);\n}\n\nstatic bool tcp_too_many_orphans(int shift)\n{\n\treturn READ_ONCE(tcp_orphan_cache) << shift >\n\t\tREAD_ONCE(sysctl_tcp_max_orphans);\n}\n\nbool tcp_check_oom(struct sock *sk, int shift)\n{\n\tbool too_many_orphans, out_of_socket_memory;\n\n\ttoo_many_orphans = tcp_too_many_orphans(shift);\n\tout_of_socket_memory = tcp_out_of_memory(sk);\n\n\tif (too_many_orphans)\n\t\tnet_info_ratelimited(\"too many orphaned sockets\\n\");\n\tif (out_of_socket_memory)\n\t\tnet_info_ratelimited(\"out of memory -- consider tuning tcp_mem\\n\");\n\treturn too_many_orphans || out_of_socket_memory;\n}\n\nvoid __tcp_close(struct sock *sk, long timeout)\n{\n\tstruct sk_buff *skb;\n\tint data_was_unread = 0;\n\tint state;\n\n\tWRITE_ONCE(sk->sk_shutdown, SHUTDOWN_MASK);\n\n\tif (sk->sk_state == TCP_LISTEN) {\n\t\ttcp_set_state(sk, TCP_CLOSE);\n\n\t\t \n\t\tinet_csk_listen_stop(sk);\n\n\t\tgoto adjudge_to_death;\n\t}\n\n\t \n\twhile ((skb = __skb_dequeue(&sk->sk_receive_queue)) != NULL) {\n\t\tu32 len = TCP_SKB_CB(skb)->end_seq - TCP_SKB_CB(skb)->seq;\n\n\t\tif (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN)\n\t\t\tlen--;\n\t\tdata_was_unread += len;\n\t\t__kfree_skb(skb);\n\t}\n\n\t \n\tif (sk->sk_state == TCP_CLOSE)\n\t\tgoto adjudge_to_death;\n\n\t \n\tif (unlikely(tcp_sk(sk)->repair)) {\n\t\tsk->sk_prot->disconnect(sk, 0);\n\t} else if (data_was_unread) {\n\t\t \n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTONCLOSE);\n\t\ttcp_set_state(sk, TCP_CLOSE);\n\t\ttcp_send_active_reset(sk, sk->sk_allocation);\n\t} else if (sock_flag(sk, SOCK_LINGER) && !sk->sk_lingertime) {\n\t\t \n\t\tsk->sk_prot->disconnect(sk, 0);\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTONDATA);\n\t} else if (tcp_close_state(sk)) {\n\t\t \n\n\t\t \n\t\ttcp_send_fin(sk);\n\t}\n\n\tsk_stream_wait_close(sk, timeout);\n\nadjudge_to_death:\n\tstate = sk->sk_state;\n\tsock_hold(sk);\n\tsock_orphan(sk);\n\n\tlocal_bh_disable();\n\tbh_lock_sock(sk);\n\t \n\t__release_sock(sk);\n\n\tthis_cpu_inc(tcp_orphan_count);\n\n\t \n\tif (state != TCP_CLOSE && sk->sk_state == TCP_CLOSE)\n\t\tgoto out;\n\n\t \n\n\tif (sk->sk_state == TCP_FIN_WAIT2) {\n\t\tstruct tcp_sock *tp = tcp_sk(sk);\n\t\tif (READ_ONCE(tp->linger2) < 0) {\n\t\t\ttcp_set_state(sk, TCP_CLOSE);\n\t\t\ttcp_send_active_reset(sk, GFP_ATOMIC);\n\t\t\t__NET_INC_STATS(sock_net(sk),\n\t\t\t\t\tLINUX_MIB_TCPABORTONLINGER);\n\t\t} else {\n\t\t\tconst int tmo = tcp_fin_time(sk);\n\n\t\t\tif (tmo > TCP_TIMEWAIT_LEN) {\n\t\t\t\tinet_csk_reset_keepalive_timer(sk,\n\t\t\t\t\t\ttmo - TCP_TIMEWAIT_LEN);\n\t\t\t} else {\n\t\t\t\ttcp_time_wait(sk, TCP_FIN_WAIT2, tmo);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\tif (sk->sk_state != TCP_CLOSE) {\n\t\tif (tcp_check_oom(sk, 0)) {\n\t\t\ttcp_set_state(sk, TCP_CLOSE);\n\t\t\ttcp_send_active_reset(sk, GFP_ATOMIC);\n\t\t\t__NET_INC_STATS(sock_net(sk),\n\t\t\t\t\tLINUX_MIB_TCPABORTONMEMORY);\n\t\t} else if (!check_net(sock_net(sk))) {\n\t\t\t \n\t\t\ttcp_set_state(sk, TCP_CLOSE);\n\t\t}\n\t}\n\n\tif (sk->sk_state == TCP_CLOSE) {\n\t\tstruct request_sock *req;\n\n\t\treq = rcu_dereference_protected(tcp_sk(sk)->fastopen_rsk,\n\t\t\t\t\t\tlockdep_sock_is_held(sk));\n\t\t \n\t\tif (req)\n\t\t\treqsk_fastopen_remove(sk, req, false);\n\t\tinet_csk_destroy_sock(sk);\n\t}\n\t \n\nout:\n\tbh_unlock_sock(sk);\n\tlocal_bh_enable();\n}\n\nvoid tcp_close(struct sock *sk, long timeout)\n{\n\tlock_sock(sk);\n\t__tcp_close(sk, timeout);\n\trelease_sock(sk);\n\tsock_put(sk);\n}\nEXPORT_SYMBOL(tcp_close);\n\n \n\nstatic inline bool tcp_need_reset(int state)\n{\n\treturn (1 << state) &\n\t       (TCPF_ESTABLISHED | TCPF_CLOSE_WAIT | TCPF_FIN_WAIT1 |\n\t\tTCPF_FIN_WAIT2 | TCPF_SYN_RECV);\n}\n\nstatic void tcp_rtx_queue_purge(struct sock *sk)\n{\n\tstruct rb_node *p = rb_first(&sk->tcp_rtx_queue);\n\n\ttcp_sk(sk)->highest_sack = NULL;\n\twhile (p) {\n\t\tstruct sk_buff *skb = rb_to_skb(p);\n\n\t\tp = rb_next(p);\n\t\t \n\t\ttcp_rtx_queue_unlink(skb, sk);\n\t\ttcp_wmem_free_skb(sk, skb);\n\t}\n}\n\nvoid tcp_write_queue_purge(struct sock *sk)\n{\n\tstruct sk_buff *skb;\n\n\ttcp_chrono_stop(sk, TCP_CHRONO_BUSY);\n\twhile ((skb = __skb_dequeue(&sk->sk_write_queue)) != NULL) {\n\t\ttcp_skb_tsorted_anchor_cleanup(skb);\n\t\ttcp_wmem_free_skb(sk, skb);\n\t}\n\ttcp_rtx_queue_purge(sk);\n\tINIT_LIST_HEAD(&tcp_sk(sk)->tsorted_sent_queue);\n\ttcp_clear_all_retrans_hints(tcp_sk(sk));\n\ttcp_sk(sk)->packets_out = 0;\n\tinet_csk(sk)->icsk_backoff = 0;\n}\n\nint tcp_disconnect(struct sock *sk, int flags)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint old_state = sk->sk_state;\n\tu32 seq;\n\n\tif (old_state != TCP_CLOSE)\n\t\ttcp_set_state(sk, TCP_CLOSE);\n\n\t \n\tif (old_state == TCP_LISTEN) {\n\t\tinet_csk_listen_stop(sk);\n\t} else if (unlikely(tp->repair)) {\n\t\tWRITE_ONCE(sk->sk_err, ECONNABORTED);\n\t} else if (tcp_need_reset(old_state) ||\n\t\t   (tp->snd_nxt != tp->write_seq &&\n\t\t    (1 << old_state) & (TCPF_CLOSING | TCPF_LAST_ACK))) {\n\t\t \n\t\ttcp_send_active_reset(sk, gfp_any());\n\t\tWRITE_ONCE(sk->sk_err, ECONNRESET);\n\t} else if (old_state == TCP_SYN_SENT)\n\t\tWRITE_ONCE(sk->sk_err, ECONNRESET);\n\n\ttcp_clear_xmit_timers(sk);\n\t__skb_queue_purge(&sk->sk_receive_queue);\n\tWRITE_ONCE(tp->copied_seq, tp->rcv_nxt);\n\tWRITE_ONCE(tp->urg_data, 0);\n\ttcp_write_queue_purge(sk);\n\ttcp_fastopen_active_disable_ofo_check(sk);\n\tskb_rbtree_purge(&tp->out_of_order_queue);\n\n\tinet->inet_dport = 0;\n\n\tinet_bhash2_reset_saddr(sk);\n\n\tWRITE_ONCE(sk->sk_shutdown, 0);\n\tsock_reset_flag(sk, SOCK_DONE);\n\ttp->srtt_us = 0;\n\ttp->mdev_us = jiffies_to_usecs(TCP_TIMEOUT_INIT);\n\ttp->rcv_rtt_last_tsecr = 0;\n\n\tseq = tp->write_seq + tp->max_window + 2;\n\tif (!seq)\n\t\tseq = 1;\n\tWRITE_ONCE(tp->write_seq, seq);\n\n\ticsk->icsk_backoff = 0;\n\ticsk->icsk_probes_out = 0;\n\ticsk->icsk_probes_tstamp = 0;\n\ticsk->icsk_rto = TCP_TIMEOUT_INIT;\n\ticsk->icsk_rto_min = TCP_RTO_MIN;\n\ticsk->icsk_delack_max = TCP_DELACK_MAX;\n\ttp->snd_ssthresh = TCP_INFINITE_SSTHRESH;\n\ttcp_snd_cwnd_set(tp, TCP_INIT_CWND);\n\ttp->snd_cwnd_cnt = 0;\n\ttp->is_cwnd_limited = 0;\n\ttp->max_packets_out = 0;\n\ttp->window_clamp = 0;\n\ttp->delivered = 0;\n\ttp->delivered_ce = 0;\n\tif (icsk->icsk_ca_ops->release)\n\t\ticsk->icsk_ca_ops->release(sk);\n\tmemset(icsk->icsk_ca_priv, 0, sizeof(icsk->icsk_ca_priv));\n\ticsk->icsk_ca_initialized = 0;\n\ttcp_set_ca_state(sk, TCP_CA_Open);\n\ttp->is_sack_reneg = 0;\n\ttcp_clear_retrans(tp);\n\ttp->total_retrans = 0;\n\tinet_csk_delack_init(sk);\n\t \n\ticsk->icsk_ack.rcv_mss = TCP_MIN_MSS;\n\tmemset(&tp->rx_opt, 0, sizeof(tp->rx_opt));\n\t__sk_dst_reset(sk);\n\tdst_release(xchg((__force struct dst_entry **)&sk->sk_rx_dst, NULL));\n\ttcp_saved_syn_free(tp);\n\ttp->compressed_ack = 0;\n\ttp->segs_in = 0;\n\ttp->segs_out = 0;\n\ttp->bytes_sent = 0;\n\ttp->bytes_acked = 0;\n\ttp->bytes_received = 0;\n\ttp->bytes_retrans = 0;\n\ttp->data_segs_in = 0;\n\ttp->data_segs_out = 0;\n\ttp->duplicate_sack[0].start_seq = 0;\n\ttp->duplicate_sack[0].end_seq = 0;\n\ttp->dsack_dups = 0;\n\ttp->reord_seen = 0;\n\ttp->retrans_out = 0;\n\ttp->sacked_out = 0;\n\ttp->tlp_high_seq = 0;\n\ttp->last_oow_ack_time = 0;\n\ttp->plb_rehash = 0;\n\t \n\ttp->app_limited = ~0U;\n\ttp->rate_app_limited = 1;\n\ttp->rack.mstamp = 0;\n\ttp->rack.advanced = 0;\n\ttp->rack.reo_wnd_steps = 1;\n\ttp->rack.last_delivered = 0;\n\ttp->rack.reo_wnd_persist = 0;\n\ttp->rack.dsack_seen = 0;\n\ttp->syn_data_acked = 0;\n\ttp->rx_opt.saw_tstamp = 0;\n\ttp->rx_opt.dsack = 0;\n\ttp->rx_opt.num_sacks = 0;\n\ttp->rcv_ooopack = 0;\n\n\n\t \n\ttcp_free_fastopen_req(tp);\n\tinet_clear_bit(DEFER_CONNECT, sk);\n\ttp->fastopen_client_fail = 0;\n\n\tWARN_ON(inet->inet_num && !icsk->icsk_bind_hash);\n\n\tif (sk->sk_frag.page) {\n\t\tput_page(sk->sk_frag.page);\n\t\tsk->sk_frag.page = NULL;\n\t\tsk->sk_frag.offset = 0;\n\t}\n\tsk_error_report(sk);\n\treturn 0;\n}\nEXPORT_SYMBOL(tcp_disconnect);\n\nstatic inline bool tcp_can_repair_sock(const struct sock *sk)\n{\n\treturn sockopt_ns_capable(sock_net(sk)->user_ns, CAP_NET_ADMIN) &&\n\t\t(sk->sk_state != TCP_LISTEN);\n}\n\nstatic int tcp_repair_set_window(struct tcp_sock *tp, sockptr_t optbuf, int len)\n{\n\tstruct tcp_repair_window opt;\n\n\tif (!tp->repair)\n\t\treturn -EPERM;\n\n\tif (len != sizeof(opt))\n\t\treturn -EINVAL;\n\n\tif (copy_from_sockptr(&opt, optbuf, sizeof(opt)))\n\t\treturn -EFAULT;\n\n\tif (opt.max_window < opt.snd_wnd)\n\t\treturn -EINVAL;\n\n\tif (after(opt.snd_wl1, tp->rcv_nxt + opt.rcv_wnd))\n\t\treturn -EINVAL;\n\n\tif (after(opt.rcv_wup, tp->rcv_nxt))\n\t\treturn -EINVAL;\n\n\ttp->snd_wl1\t= opt.snd_wl1;\n\ttp->snd_wnd\t= opt.snd_wnd;\n\ttp->max_window\t= opt.max_window;\n\n\ttp->rcv_wnd\t= opt.rcv_wnd;\n\ttp->rcv_wup\t= opt.rcv_wup;\n\n\treturn 0;\n}\n\nstatic int tcp_repair_options_est(struct sock *sk, sockptr_t optbuf,\n\t\tunsigned int len)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct tcp_repair_opt opt;\n\tsize_t offset = 0;\n\n\twhile (len >= sizeof(opt)) {\n\t\tif (copy_from_sockptr_offset(&opt, optbuf, offset, sizeof(opt)))\n\t\t\treturn -EFAULT;\n\n\t\toffset += sizeof(opt);\n\t\tlen -= sizeof(opt);\n\n\t\tswitch (opt.opt_code) {\n\t\tcase TCPOPT_MSS:\n\t\t\ttp->rx_opt.mss_clamp = opt.opt_val;\n\t\t\ttcp_mtup_init(sk);\n\t\t\tbreak;\n\t\tcase TCPOPT_WINDOW:\n\t\t\t{\n\t\t\t\tu16 snd_wscale = opt.opt_val & 0xFFFF;\n\t\t\t\tu16 rcv_wscale = opt.opt_val >> 16;\n\n\t\t\t\tif (snd_wscale > TCP_MAX_WSCALE || rcv_wscale > TCP_MAX_WSCALE)\n\t\t\t\t\treturn -EFBIG;\n\n\t\t\t\ttp->rx_opt.snd_wscale = snd_wscale;\n\t\t\t\ttp->rx_opt.rcv_wscale = rcv_wscale;\n\t\t\t\ttp->rx_opt.wscale_ok = 1;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase TCPOPT_SACK_PERM:\n\t\t\tif (opt.opt_val != 0)\n\t\t\t\treturn -EINVAL;\n\n\t\t\ttp->rx_opt.sack_ok |= TCP_SACK_SEEN;\n\t\t\tbreak;\n\t\tcase TCPOPT_TIMESTAMP:\n\t\t\tif (opt.opt_val != 0)\n\t\t\t\treturn -EINVAL;\n\n\t\t\ttp->rx_opt.tstamp_ok = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nDEFINE_STATIC_KEY_FALSE(tcp_tx_delay_enabled);\nEXPORT_SYMBOL(tcp_tx_delay_enabled);\n\nstatic void tcp_enable_tx_delay(void)\n{\n\tif (!static_branch_unlikely(&tcp_tx_delay_enabled)) {\n\t\tstatic int __tcp_tx_delay_enabled = 0;\n\n\t\tif (cmpxchg(&__tcp_tx_delay_enabled, 0, 1) == 0) {\n\t\t\tstatic_branch_enable(&tcp_tx_delay_enabled);\n\t\t\tpr_info(\"TCP_TX_DELAY enabled\\n\");\n\t\t}\n\t}\n}\n\n \nvoid __tcp_sock_set_cork(struct sock *sk, bool on)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (on) {\n\t\ttp->nonagle |= TCP_NAGLE_CORK;\n\t} else {\n\t\ttp->nonagle &= ~TCP_NAGLE_CORK;\n\t\tif (tp->nonagle & TCP_NAGLE_OFF)\n\t\t\ttp->nonagle |= TCP_NAGLE_PUSH;\n\t\ttcp_push_pending_frames(sk);\n\t}\n}\n\nvoid tcp_sock_set_cork(struct sock *sk, bool on)\n{\n\tlock_sock(sk);\n\t__tcp_sock_set_cork(sk, on);\n\trelease_sock(sk);\n}\nEXPORT_SYMBOL(tcp_sock_set_cork);\n\n \nvoid __tcp_sock_set_nodelay(struct sock *sk, bool on)\n{\n\tif (on) {\n\t\ttcp_sk(sk)->nonagle |= TCP_NAGLE_OFF|TCP_NAGLE_PUSH;\n\t\ttcp_push_pending_frames(sk);\n\t} else {\n\t\ttcp_sk(sk)->nonagle &= ~TCP_NAGLE_OFF;\n\t}\n}\n\nvoid tcp_sock_set_nodelay(struct sock *sk)\n{\n\tlock_sock(sk);\n\t__tcp_sock_set_nodelay(sk, true);\n\trelease_sock(sk);\n}\nEXPORT_SYMBOL(tcp_sock_set_nodelay);\n\nstatic void __tcp_sock_set_quickack(struct sock *sk, int val)\n{\n\tif (!val) {\n\t\tinet_csk_enter_pingpong_mode(sk);\n\t\treturn;\n\t}\n\n\tinet_csk_exit_pingpong_mode(sk);\n\tif ((1 << sk->sk_state) & (TCPF_ESTABLISHED | TCPF_CLOSE_WAIT) &&\n\t    inet_csk_ack_scheduled(sk)) {\n\t\tinet_csk(sk)->icsk_ack.pending |= ICSK_ACK_PUSHED;\n\t\ttcp_cleanup_rbuf(sk, 1);\n\t\tif (!(val & 1))\n\t\t\tinet_csk_enter_pingpong_mode(sk);\n\t}\n}\n\nvoid tcp_sock_set_quickack(struct sock *sk, int val)\n{\n\tlock_sock(sk);\n\t__tcp_sock_set_quickack(sk, val);\n\trelease_sock(sk);\n}\nEXPORT_SYMBOL(tcp_sock_set_quickack);\n\nint tcp_sock_set_syncnt(struct sock *sk, int val)\n{\n\tif (val < 1 || val > MAX_TCP_SYNCNT)\n\t\treturn -EINVAL;\n\n\tWRITE_ONCE(inet_csk(sk)->icsk_syn_retries, val);\n\treturn 0;\n}\nEXPORT_SYMBOL(tcp_sock_set_syncnt);\n\nint tcp_sock_set_user_timeout(struct sock *sk, int val)\n{\n\t \n\tif (val < 0)\n\t\treturn -EINVAL;\n\n\tWRITE_ONCE(inet_csk(sk)->icsk_user_timeout, val);\n\treturn 0;\n}\nEXPORT_SYMBOL(tcp_sock_set_user_timeout);\n\nint tcp_sock_set_keepidle_locked(struct sock *sk, int val)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (val < 1 || val > MAX_TCP_KEEPIDLE)\n\t\treturn -EINVAL;\n\n\t \n\tWRITE_ONCE(tp->keepalive_time, val * HZ);\n\tif (sock_flag(sk, SOCK_KEEPOPEN) &&\n\t    !((1 << sk->sk_state) & (TCPF_CLOSE | TCPF_LISTEN))) {\n\t\tu32 elapsed = keepalive_time_elapsed(tp);\n\n\t\tif (tp->keepalive_time > elapsed)\n\t\t\telapsed = tp->keepalive_time - elapsed;\n\t\telse\n\t\t\telapsed = 0;\n\t\tinet_csk_reset_keepalive_timer(sk, elapsed);\n\t}\n\n\treturn 0;\n}\n\nint tcp_sock_set_keepidle(struct sock *sk, int val)\n{\n\tint err;\n\n\tlock_sock(sk);\n\terr = tcp_sock_set_keepidle_locked(sk, val);\n\trelease_sock(sk);\n\treturn err;\n}\nEXPORT_SYMBOL(tcp_sock_set_keepidle);\n\nint tcp_sock_set_keepintvl(struct sock *sk, int val)\n{\n\tif (val < 1 || val > MAX_TCP_KEEPINTVL)\n\t\treturn -EINVAL;\n\n\tWRITE_ONCE(tcp_sk(sk)->keepalive_intvl, val * HZ);\n\treturn 0;\n}\nEXPORT_SYMBOL(tcp_sock_set_keepintvl);\n\nint tcp_sock_set_keepcnt(struct sock *sk, int val)\n{\n\tif (val < 1 || val > MAX_TCP_KEEPCNT)\n\t\treturn -EINVAL;\n\n\t \n\tWRITE_ONCE(tcp_sk(sk)->keepalive_probes, val);\n\treturn 0;\n}\nEXPORT_SYMBOL(tcp_sock_set_keepcnt);\n\nint tcp_set_window_clamp(struct sock *sk, int val)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (!val) {\n\t\tif (sk->sk_state != TCP_CLOSE)\n\t\t\treturn -EINVAL;\n\t\ttp->window_clamp = 0;\n\t} else {\n\t\tu32 new_rcv_ssthresh, old_window_clamp = tp->window_clamp;\n\t\tu32 new_window_clamp = val < SOCK_MIN_RCVBUF / 2 ?\n\t\t\t\t\t\tSOCK_MIN_RCVBUF / 2 : val;\n\n\t\tif (new_window_clamp == old_window_clamp)\n\t\t\treturn 0;\n\n\t\ttp->window_clamp = new_window_clamp;\n\t\tif (new_window_clamp < old_window_clamp) {\n\t\t\t \n\t\t\t__tcp_adjust_rcv_ssthresh(sk, tp->window_clamp);\n\n\t\t} else {\n\t\t\tnew_rcv_ssthresh = min(tp->rcv_wnd, tp->window_clamp);\n\t\t\ttp->rcv_ssthresh = max(new_rcv_ssthresh,\n\t\t\t\t\t       tp->rcv_ssthresh);\n\t\t}\n\t}\n\treturn 0;\n}\n\n \nint do_tcp_setsockopt(struct sock *sk, int level, int optname,\n\t\t      sockptr_t optval, unsigned int optlen)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct net *net = sock_net(sk);\n\tint val;\n\tint err = 0;\n\n\t \n\tswitch (optname) {\n\tcase TCP_CONGESTION: {\n\t\tchar name[TCP_CA_NAME_MAX];\n\n\t\tif (optlen < 1)\n\t\t\treturn -EINVAL;\n\n\t\tval = strncpy_from_sockptr(name, optval,\n\t\t\t\t\tmin_t(long, TCP_CA_NAME_MAX-1, optlen));\n\t\tif (val < 0)\n\t\t\treturn -EFAULT;\n\t\tname[val] = 0;\n\n\t\tsockopt_lock_sock(sk);\n\t\terr = tcp_set_congestion_control(sk, name, !has_current_bpf_ctx(),\n\t\t\t\t\t\t sockopt_ns_capable(sock_net(sk)->user_ns,\n\t\t\t\t\t\t\t\t    CAP_NET_ADMIN));\n\t\tsockopt_release_sock(sk);\n\t\treturn err;\n\t}\n\tcase TCP_ULP: {\n\t\tchar name[TCP_ULP_NAME_MAX];\n\n\t\tif (optlen < 1)\n\t\t\treturn -EINVAL;\n\n\t\tval = strncpy_from_sockptr(name, optval,\n\t\t\t\t\tmin_t(long, TCP_ULP_NAME_MAX - 1,\n\t\t\t\t\t      optlen));\n\t\tif (val < 0)\n\t\t\treturn -EFAULT;\n\t\tname[val] = 0;\n\n\t\tsockopt_lock_sock(sk);\n\t\terr = tcp_set_ulp(sk, name);\n\t\tsockopt_release_sock(sk);\n\t\treturn err;\n\t}\n\tcase TCP_FASTOPEN_KEY: {\n\t\t__u8 key[TCP_FASTOPEN_KEY_BUF_LENGTH];\n\t\t__u8 *backup_key = NULL;\n\n\t\t \n\t\tif (optlen != TCP_FASTOPEN_KEY_LENGTH &&\n\t\t    optlen != TCP_FASTOPEN_KEY_BUF_LENGTH)\n\t\t\treturn -EINVAL;\n\n\t\tif (copy_from_sockptr(key, optval, optlen))\n\t\t\treturn -EFAULT;\n\n\t\tif (optlen == TCP_FASTOPEN_KEY_BUF_LENGTH)\n\t\t\tbackup_key = key + TCP_FASTOPEN_KEY_LENGTH;\n\n\t\treturn tcp_fastopen_reset_cipher(net, sk, key, backup_key);\n\t}\n\tdefault:\n\t\t \n\t\tbreak;\n\t}\n\n\tif (optlen < sizeof(int))\n\t\treturn -EINVAL;\n\n\tif (copy_from_sockptr(&val, optval, sizeof(val)))\n\t\treturn -EFAULT;\n\n\t \n\tswitch (optname) {\n\tcase TCP_SYNCNT:\n\t\treturn tcp_sock_set_syncnt(sk, val);\n\tcase TCP_USER_TIMEOUT:\n\t\treturn tcp_sock_set_user_timeout(sk, val);\n\tcase TCP_KEEPINTVL:\n\t\treturn tcp_sock_set_keepintvl(sk, val);\n\tcase TCP_KEEPCNT:\n\t\treturn tcp_sock_set_keepcnt(sk, val);\n\tcase TCP_LINGER2:\n\t\tif (val < 0)\n\t\t\tWRITE_ONCE(tp->linger2, -1);\n\t\telse if (val > TCP_FIN_TIMEOUT_MAX / HZ)\n\t\t\tWRITE_ONCE(tp->linger2, TCP_FIN_TIMEOUT_MAX);\n\t\telse\n\t\t\tWRITE_ONCE(tp->linger2, val * HZ);\n\t\treturn 0;\n\tcase TCP_DEFER_ACCEPT:\n\t\t \n\t\tWRITE_ONCE(icsk->icsk_accept_queue.rskq_defer_accept,\n\t\t\t   secs_to_retrans(val, TCP_TIMEOUT_INIT / HZ,\n\t\t\t\t\t   TCP_RTO_MAX / HZ));\n\t\treturn 0;\n\t}\n\n\tsockopt_lock_sock(sk);\n\n\tswitch (optname) {\n\tcase TCP_MAXSEG:\n\t\t \n\t\tif (val && (val < TCP_MIN_MSS || val > MAX_TCP_WINDOW)) {\n\t\t\terr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\ttp->rx_opt.user_mss = val;\n\t\tbreak;\n\n\tcase TCP_NODELAY:\n\t\t__tcp_sock_set_nodelay(sk, val);\n\t\tbreak;\n\n\tcase TCP_THIN_LINEAR_TIMEOUTS:\n\t\tif (val < 0 || val > 1)\n\t\t\terr = -EINVAL;\n\t\telse\n\t\t\ttp->thin_lto = val;\n\t\tbreak;\n\n\tcase TCP_THIN_DUPACK:\n\t\tif (val < 0 || val > 1)\n\t\t\terr = -EINVAL;\n\t\tbreak;\n\n\tcase TCP_REPAIR:\n\t\tif (!tcp_can_repair_sock(sk))\n\t\t\terr = -EPERM;\n\t\telse if (val == TCP_REPAIR_ON) {\n\t\t\ttp->repair = 1;\n\t\t\tsk->sk_reuse = SK_FORCE_REUSE;\n\t\t\ttp->repair_queue = TCP_NO_QUEUE;\n\t\t} else if (val == TCP_REPAIR_OFF) {\n\t\t\ttp->repair = 0;\n\t\t\tsk->sk_reuse = SK_NO_REUSE;\n\t\t\ttcp_send_window_probe(sk);\n\t\t} else if (val == TCP_REPAIR_OFF_NO_WP) {\n\t\t\ttp->repair = 0;\n\t\t\tsk->sk_reuse = SK_NO_REUSE;\n\t\t} else\n\t\t\terr = -EINVAL;\n\n\t\tbreak;\n\n\tcase TCP_REPAIR_QUEUE:\n\t\tif (!tp->repair)\n\t\t\terr = -EPERM;\n\t\telse if ((unsigned int)val < TCP_QUEUES_NR)\n\t\t\ttp->repair_queue = val;\n\t\telse\n\t\t\terr = -EINVAL;\n\t\tbreak;\n\n\tcase TCP_QUEUE_SEQ:\n\t\tif (sk->sk_state != TCP_CLOSE) {\n\t\t\terr = -EPERM;\n\t\t} else if (tp->repair_queue == TCP_SEND_QUEUE) {\n\t\t\tif (!tcp_rtx_queue_empty(sk))\n\t\t\t\terr = -EPERM;\n\t\t\telse\n\t\t\t\tWRITE_ONCE(tp->write_seq, val);\n\t\t} else if (tp->repair_queue == TCP_RECV_QUEUE) {\n\t\t\tif (tp->rcv_nxt != tp->copied_seq) {\n\t\t\t\terr = -EPERM;\n\t\t\t} else {\n\t\t\t\tWRITE_ONCE(tp->rcv_nxt, val);\n\t\t\t\tWRITE_ONCE(tp->copied_seq, val);\n\t\t\t}\n\t\t} else {\n\t\t\terr = -EINVAL;\n\t\t}\n\t\tbreak;\n\n\tcase TCP_REPAIR_OPTIONS:\n\t\tif (!tp->repair)\n\t\t\terr = -EINVAL;\n\t\telse if (sk->sk_state == TCP_ESTABLISHED && !tp->bytes_sent)\n\t\t\terr = tcp_repair_options_est(sk, optval, optlen);\n\t\telse\n\t\t\terr = -EPERM;\n\t\tbreak;\n\n\tcase TCP_CORK:\n\t\t__tcp_sock_set_cork(sk, val);\n\t\tbreak;\n\n\tcase TCP_KEEPIDLE:\n\t\terr = tcp_sock_set_keepidle_locked(sk, val);\n\t\tbreak;\n\tcase TCP_SAVE_SYN:\n\t\t \n\t\tif (val < 0 || val > 2)\n\t\t\terr = -EINVAL;\n\t\telse\n\t\t\ttp->save_syn = val;\n\t\tbreak;\n\n\tcase TCP_WINDOW_CLAMP:\n\t\terr = tcp_set_window_clamp(sk, val);\n\t\tbreak;\n\n\tcase TCP_QUICKACK:\n\t\t__tcp_sock_set_quickack(sk, val);\n\t\tbreak;\n\n#ifdef CONFIG_TCP_MD5SIG\n\tcase TCP_MD5SIG:\n\tcase TCP_MD5SIG_EXT:\n\t\terr = tp->af_specific->md5_parse(sk, optname, optval, optlen);\n\t\tbreak;\n#endif\n\tcase TCP_FASTOPEN:\n\t\tif (val >= 0 && ((1 << sk->sk_state) & (TCPF_CLOSE |\n\t\t    TCPF_LISTEN))) {\n\t\t\ttcp_fastopen_init_key_once(net);\n\n\t\t\tfastopen_queue_tune(sk, val);\n\t\t} else {\n\t\t\terr = -EINVAL;\n\t\t}\n\t\tbreak;\n\tcase TCP_FASTOPEN_CONNECT:\n\t\tif (val > 1 || val < 0) {\n\t\t\terr = -EINVAL;\n\t\t} else if (READ_ONCE(net->ipv4.sysctl_tcp_fastopen) &\n\t\t\t   TFO_CLIENT_ENABLE) {\n\t\t\tif (sk->sk_state == TCP_CLOSE)\n\t\t\t\ttp->fastopen_connect = val;\n\t\t\telse\n\t\t\t\terr = -EINVAL;\n\t\t} else {\n\t\t\terr = -EOPNOTSUPP;\n\t\t}\n\t\tbreak;\n\tcase TCP_FASTOPEN_NO_COOKIE:\n\t\tif (val > 1 || val < 0)\n\t\t\terr = -EINVAL;\n\t\telse if (!((1 << sk->sk_state) & (TCPF_CLOSE | TCPF_LISTEN)))\n\t\t\terr = -EINVAL;\n\t\telse\n\t\t\ttp->fastopen_no_cookie = val;\n\t\tbreak;\n\tcase TCP_TIMESTAMP:\n\t\tif (!tp->repair)\n\t\t\terr = -EPERM;\n\t\telse\n\t\t\tWRITE_ONCE(tp->tsoffset, val - tcp_time_stamp_raw());\n\t\tbreak;\n\tcase TCP_REPAIR_WINDOW:\n\t\terr = tcp_repair_set_window(tp, optval, optlen);\n\t\tbreak;\n\tcase TCP_NOTSENT_LOWAT:\n\t\tWRITE_ONCE(tp->notsent_lowat, val);\n\t\tsk->sk_write_space(sk);\n\t\tbreak;\n\tcase TCP_INQ:\n\t\tif (val > 1 || val < 0)\n\t\t\terr = -EINVAL;\n\t\telse\n\t\t\ttp->recvmsg_inq = val;\n\t\tbreak;\n\tcase TCP_TX_DELAY:\n\t\tif (val)\n\t\t\ttcp_enable_tx_delay();\n\t\tWRITE_ONCE(tp->tcp_tx_delay, val);\n\t\tbreak;\n\tdefault:\n\t\terr = -ENOPROTOOPT;\n\t\tbreak;\n\t}\n\n\tsockopt_release_sock(sk);\n\treturn err;\n}\n\nint tcp_setsockopt(struct sock *sk, int level, int optname, sockptr_t optval,\n\t\t   unsigned int optlen)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\n\tif (level != SOL_TCP)\n\t\t \n\t\treturn READ_ONCE(icsk->icsk_af_ops)->setsockopt(sk, level, optname,\n\t\t\t\t\t\t\t\toptval, optlen);\n\treturn do_tcp_setsockopt(sk, level, optname, optval, optlen);\n}\nEXPORT_SYMBOL(tcp_setsockopt);\n\nstatic void tcp_get_info_chrono_stats(const struct tcp_sock *tp,\n\t\t\t\t      struct tcp_info *info)\n{\n\tu64 stats[__TCP_CHRONO_MAX], total = 0;\n\tenum tcp_chrono i;\n\n\tfor (i = TCP_CHRONO_BUSY; i < __TCP_CHRONO_MAX; ++i) {\n\t\tstats[i] = tp->chrono_stat[i - 1];\n\t\tif (i == tp->chrono_type)\n\t\t\tstats[i] += tcp_jiffies32 - tp->chrono_start;\n\t\tstats[i] *= USEC_PER_SEC / HZ;\n\t\ttotal += stats[i];\n\t}\n\n\tinfo->tcpi_busy_time = total;\n\tinfo->tcpi_rwnd_limited = stats[TCP_CHRONO_RWND_LIMITED];\n\tinfo->tcpi_sndbuf_limited = stats[TCP_CHRONO_SNDBUF_LIMITED];\n}\n\n \nvoid tcp_get_info(struct sock *sk, struct tcp_info *info)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);  \n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tunsigned long rate;\n\tu32 now;\n\tu64 rate64;\n\tbool slow;\n\n\tmemset(info, 0, sizeof(*info));\n\tif (sk->sk_type != SOCK_STREAM)\n\t\treturn;\n\n\tinfo->tcpi_state = inet_sk_state_load(sk);\n\n\t \n\trate = READ_ONCE(sk->sk_pacing_rate);\n\trate64 = (rate != ~0UL) ? rate : ~0ULL;\n\tinfo->tcpi_pacing_rate = rate64;\n\n\trate = READ_ONCE(sk->sk_max_pacing_rate);\n\trate64 = (rate != ~0UL) ? rate : ~0ULL;\n\tinfo->tcpi_max_pacing_rate = rate64;\n\n\tinfo->tcpi_reordering = tp->reordering;\n\tinfo->tcpi_snd_cwnd = tcp_snd_cwnd(tp);\n\n\tif (info->tcpi_state == TCP_LISTEN) {\n\t\t \n\t\tinfo->tcpi_unacked = READ_ONCE(sk->sk_ack_backlog);\n\t\tinfo->tcpi_sacked = READ_ONCE(sk->sk_max_ack_backlog);\n\t\treturn;\n\t}\n\n\tslow = lock_sock_fast(sk);\n\n\tinfo->tcpi_ca_state = icsk->icsk_ca_state;\n\tinfo->tcpi_retransmits = icsk->icsk_retransmits;\n\tinfo->tcpi_probes = icsk->icsk_probes_out;\n\tinfo->tcpi_backoff = icsk->icsk_backoff;\n\n\tif (tp->rx_opt.tstamp_ok)\n\t\tinfo->tcpi_options |= TCPI_OPT_TIMESTAMPS;\n\tif (tcp_is_sack(tp))\n\t\tinfo->tcpi_options |= TCPI_OPT_SACK;\n\tif (tp->rx_opt.wscale_ok) {\n\t\tinfo->tcpi_options |= TCPI_OPT_WSCALE;\n\t\tinfo->tcpi_snd_wscale = tp->rx_opt.snd_wscale;\n\t\tinfo->tcpi_rcv_wscale = tp->rx_opt.rcv_wscale;\n\t}\n\n\tif (tp->ecn_flags & TCP_ECN_OK)\n\t\tinfo->tcpi_options |= TCPI_OPT_ECN;\n\tif (tp->ecn_flags & TCP_ECN_SEEN)\n\t\tinfo->tcpi_options |= TCPI_OPT_ECN_SEEN;\n\tif (tp->syn_data_acked)\n\t\tinfo->tcpi_options |= TCPI_OPT_SYN_DATA;\n\n\tinfo->tcpi_rto = jiffies_to_usecs(icsk->icsk_rto);\n\tinfo->tcpi_ato = jiffies_to_usecs(min(icsk->icsk_ack.ato,\n\t\t\t\t\t      tcp_delack_max(sk)));\n\tinfo->tcpi_snd_mss = tp->mss_cache;\n\tinfo->tcpi_rcv_mss = icsk->icsk_ack.rcv_mss;\n\n\tinfo->tcpi_unacked = tp->packets_out;\n\tinfo->tcpi_sacked = tp->sacked_out;\n\n\tinfo->tcpi_lost = tp->lost_out;\n\tinfo->tcpi_retrans = tp->retrans_out;\n\n\tnow = tcp_jiffies32;\n\tinfo->tcpi_last_data_sent = jiffies_to_msecs(now - tp->lsndtime);\n\tinfo->tcpi_last_data_recv = jiffies_to_msecs(now - icsk->icsk_ack.lrcvtime);\n\tinfo->tcpi_last_ack_recv = jiffies_to_msecs(now - tp->rcv_tstamp);\n\n\tinfo->tcpi_pmtu = icsk->icsk_pmtu_cookie;\n\tinfo->tcpi_rcv_ssthresh = tp->rcv_ssthresh;\n\tinfo->tcpi_rtt = tp->srtt_us >> 3;\n\tinfo->tcpi_rttvar = tp->mdev_us >> 2;\n\tinfo->tcpi_snd_ssthresh = tp->snd_ssthresh;\n\tinfo->tcpi_advmss = tp->advmss;\n\n\tinfo->tcpi_rcv_rtt = tp->rcv_rtt_est.rtt_us >> 3;\n\tinfo->tcpi_rcv_space = tp->rcvq_space.space;\n\n\tinfo->tcpi_total_retrans = tp->total_retrans;\n\n\tinfo->tcpi_bytes_acked = tp->bytes_acked;\n\tinfo->tcpi_bytes_received = tp->bytes_received;\n\tinfo->tcpi_notsent_bytes = max_t(int, 0, tp->write_seq - tp->snd_nxt);\n\ttcp_get_info_chrono_stats(tp, info);\n\n\tinfo->tcpi_segs_out = tp->segs_out;\n\n\t \n\tinfo->tcpi_segs_in = READ_ONCE(tp->segs_in);\n\tinfo->tcpi_data_segs_in = READ_ONCE(tp->data_segs_in);\n\n\tinfo->tcpi_min_rtt = tcp_min_rtt(tp);\n\tinfo->tcpi_data_segs_out = tp->data_segs_out;\n\n\tinfo->tcpi_delivery_rate_app_limited = tp->rate_app_limited ? 1 : 0;\n\trate64 = tcp_compute_delivery_rate(tp);\n\tif (rate64)\n\t\tinfo->tcpi_delivery_rate = rate64;\n\tinfo->tcpi_delivered = tp->delivered;\n\tinfo->tcpi_delivered_ce = tp->delivered_ce;\n\tinfo->tcpi_bytes_sent = tp->bytes_sent;\n\tinfo->tcpi_bytes_retrans = tp->bytes_retrans;\n\tinfo->tcpi_dsack_dups = tp->dsack_dups;\n\tinfo->tcpi_reord_seen = tp->reord_seen;\n\tinfo->tcpi_rcv_ooopack = tp->rcv_ooopack;\n\tinfo->tcpi_snd_wnd = tp->snd_wnd;\n\tinfo->tcpi_rcv_wnd = tp->rcv_wnd;\n\tinfo->tcpi_rehash = tp->plb_rehash + tp->timeout_rehash;\n\tinfo->tcpi_fastopen_client_fail = tp->fastopen_client_fail;\n\tunlock_sock_fast(sk, slow);\n}\nEXPORT_SYMBOL_GPL(tcp_get_info);\n\nstatic size_t tcp_opt_stats_get_size(void)\n{\n\treturn\n\t\tnla_total_size_64bit(sizeof(u64)) +  \n\t\tnla_total_size_64bit(sizeof(u64)) +  \n\t\tnla_total_size_64bit(sizeof(u64)) +  \n\t\tnla_total_size_64bit(sizeof(u64)) +  \n\t\tnla_total_size_64bit(sizeof(u64)) +  \n\t\tnla_total_size_64bit(sizeof(u64)) +  \n\t\tnla_total_size_64bit(sizeof(u64)) +  \n\t\tnla_total_size(sizeof(u32)) +  \n\t\tnla_total_size(sizeof(u32)) +  \n\t\tnla_total_size(sizeof(u32)) +  \n\t\tnla_total_size(sizeof(u8)) +  \n\t\tnla_total_size(sizeof(u8)) +  \n\t\tnla_total_size(sizeof(u32)) +  \n\t\tnla_total_size(sizeof(u8)) +  \n\t\tnla_total_size(sizeof(u32)) +  \n\t\tnla_total_size(sizeof(u32)) +  \n\t\tnla_total_size(sizeof(u32)) +  \n\t\tnla_total_size_64bit(sizeof(u64)) +  \n\t\tnla_total_size_64bit(sizeof(u64)) +  \n\t\tnla_total_size(sizeof(u32)) +  \n\t\tnla_total_size(sizeof(u32)) +  \n\t\tnla_total_size(sizeof(u32)) +  \n\t\tnla_total_size(sizeof(u16)) +  \n\t\tnla_total_size(sizeof(u32)) +  \n\t\tnla_total_size_64bit(sizeof(u64)) +  \n\t\tnla_total_size(sizeof(u8)) +  \n\t\tnla_total_size(sizeof(u32)) +  \n\t\t0;\n}\n\n \nstatic u8 tcp_skb_ttl_or_hop_limit(const struct sk_buff *skb)\n{\n\tif (skb->protocol == htons(ETH_P_IP))\n\t\treturn ip_hdr(skb)->ttl;\n\telse if (skb->protocol == htons(ETH_P_IPV6))\n\t\treturn ipv6_hdr(skb)->hop_limit;\n\telse\n\t\treturn 0;\n}\n\nstruct sk_buff *tcp_get_timestamping_opt_stats(const struct sock *sk,\n\t\t\t\t\t       const struct sk_buff *orig_skb,\n\t\t\t\t\t       const struct sk_buff *ack_skb)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *stats;\n\tstruct tcp_info info;\n\tunsigned long rate;\n\tu64 rate64;\n\n\tstats = alloc_skb(tcp_opt_stats_get_size(), GFP_ATOMIC);\n\tif (!stats)\n\t\treturn NULL;\n\n\ttcp_get_info_chrono_stats(tp, &info);\n\tnla_put_u64_64bit(stats, TCP_NLA_BUSY,\n\t\t\t  info.tcpi_busy_time, TCP_NLA_PAD);\n\tnla_put_u64_64bit(stats, TCP_NLA_RWND_LIMITED,\n\t\t\t  info.tcpi_rwnd_limited, TCP_NLA_PAD);\n\tnla_put_u64_64bit(stats, TCP_NLA_SNDBUF_LIMITED,\n\t\t\t  info.tcpi_sndbuf_limited, TCP_NLA_PAD);\n\tnla_put_u64_64bit(stats, TCP_NLA_DATA_SEGS_OUT,\n\t\t\t  tp->data_segs_out, TCP_NLA_PAD);\n\tnla_put_u64_64bit(stats, TCP_NLA_TOTAL_RETRANS,\n\t\t\t  tp->total_retrans, TCP_NLA_PAD);\n\n\trate = READ_ONCE(sk->sk_pacing_rate);\n\trate64 = (rate != ~0UL) ? rate : ~0ULL;\n\tnla_put_u64_64bit(stats, TCP_NLA_PACING_RATE, rate64, TCP_NLA_PAD);\n\n\trate64 = tcp_compute_delivery_rate(tp);\n\tnla_put_u64_64bit(stats, TCP_NLA_DELIVERY_RATE, rate64, TCP_NLA_PAD);\n\n\tnla_put_u32(stats, TCP_NLA_SND_CWND, tcp_snd_cwnd(tp));\n\tnla_put_u32(stats, TCP_NLA_REORDERING, tp->reordering);\n\tnla_put_u32(stats, TCP_NLA_MIN_RTT, tcp_min_rtt(tp));\n\n\tnla_put_u8(stats, TCP_NLA_RECUR_RETRANS, inet_csk(sk)->icsk_retransmits);\n\tnla_put_u8(stats, TCP_NLA_DELIVERY_RATE_APP_LMT, !!tp->rate_app_limited);\n\tnla_put_u32(stats, TCP_NLA_SND_SSTHRESH, tp->snd_ssthresh);\n\tnla_put_u32(stats, TCP_NLA_DELIVERED, tp->delivered);\n\tnla_put_u32(stats, TCP_NLA_DELIVERED_CE, tp->delivered_ce);\n\n\tnla_put_u32(stats, TCP_NLA_SNDQ_SIZE, tp->write_seq - tp->snd_una);\n\tnla_put_u8(stats, TCP_NLA_CA_STATE, inet_csk(sk)->icsk_ca_state);\n\n\tnla_put_u64_64bit(stats, TCP_NLA_BYTES_SENT, tp->bytes_sent,\n\t\t\t  TCP_NLA_PAD);\n\tnla_put_u64_64bit(stats, TCP_NLA_BYTES_RETRANS, tp->bytes_retrans,\n\t\t\t  TCP_NLA_PAD);\n\tnla_put_u32(stats, TCP_NLA_DSACK_DUPS, tp->dsack_dups);\n\tnla_put_u32(stats, TCP_NLA_REORD_SEEN, tp->reord_seen);\n\tnla_put_u32(stats, TCP_NLA_SRTT, tp->srtt_us >> 3);\n\tnla_put_u16(stats, TCP_NLA_TIMEOUT_REHASH, tp->timeout_rehash);\n\tnla_put_u32(stats, TCP_NLA_BYTES_NOTSENT,\n\t\t    max_t(int, 0, tp->write_seq - tp->snd_nxt));\n\tnla_put_u64_64bit(stats, TCP_NLA_EDT, orig_skb->skb_mstamp_ns,\n\t\t\t  TCP_NLA_PAD);\n\tif (ack_skb)\n\t\tnla_put_u8(stats, TCP_NLA_TTL,\n\t\t\t   tcp_skb_ttl_or_hop_limit(ack_skb));\n\n\tnla_put_u32(stats, TCP_NLA_REHASH, tp->plb_rehash + tp->timeout_rehash);\n\treturn stats;\n}\n\nint do_tcp_getsockopt(struct sock *sk, int level,\n\t\t      int optname, sockptr_t optval, sockptr_t optlen)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tint val, len;\n\n\tif (copy_from_sockptr(&len, optlen, sizeof(int)))\n\t\treturn -EFAULT;\n\n\tlen = min_t(unsigned int, len, sizeof(int));\n\n\tif (len < 0)\n\t\treturn -EINVAL;\n\n\tswitch (optname) {\n\tcase TCP_MAXSEG:\n\t\tval = tp->mss_cache;\n\t\tif (tp->rx_opt.user_mss &&\n\t\t    ((1 << sk->sk_state) & (TCPF_CLOSE | TCPF_LISTEN)))\n\t\t\tval = tp->rx_opt.user_mss;\n\t\tif (tp->repair)\n\t\t\tval = tp->rx_opt.mss_clamp;\n\t\tbreak;\n\tcase TCP_NODELAY:\n\t\tval = !!(tp->nonagle&TCP_NAGLE_OFF);\n\t\tbreak;\n\tcase TCP_CORK:\n\t\tval = !!(tp->nonagle&TCP_NAGLE_CORK);\n\t\tbreak;\n\tcase TCP_KEEPIDLE:\n\t\tval = keepalive_time_when(tp) / HZ;\n\t\tbreak;\n\tcase TCP_KEEPINTVL:\n\t\tval = keepalive_intvl_when(tp) / HZ;\n\t\tbreak;\n\tcase TCP_KEEPCNT:\n\t\tval = keepalive_probes(tp);\n\t\tbreak;\n\tcase TCP_SYNCNT:\n\t\tval = READ_ONCE(icsk->icsk_syn_retries) ? :\n\t\t\tREAD_ONCE(net->ipv4.sysctl_tcp_syn_retries);\n\t\tbreak;\n\tcase TCP_LINGER2:\n\t\tval = READ_ONCE(tp->linger2);\n\t\tif (val >= 0)\n\t\t\tval = (val ? : READ_ONCE(net->ipv4.sysctl_tcp_fin_timeout)) / HZ;\n\t\tbreak;\n\tcase TCP_DEFER_ACCEPT:\n\t\tval = READ_ONCE(icsk->icsk_accept_queue.rskq_defer_accept);\n\t\tval = retrans_to_secs(val, TCP_TIMEOUT_INIT / HZ,\n\t\t\t\t      TCP_RTO_MAX / HZ);\n\t\tbreak;\n\tcase TCP_WINDOW_CLAMP:\n\t\tval = tp->window_clamp;\n\t\tbreak;\n\tcase TCP_INFO: {\n\t\tstruct tcp_info info;\n\n\t\tif (copy_from_sockptr(&len, optlen, sizeof(int)))\n\t\t\treturn -EFAULT;\n\n\t\ttcp_get_info(sk, &info);\n\n\t\tlen = min_t(unsigned int, len, sizeof(info));\n\t\tif (copy_to_sockptr(optlen, &len, sizeof(int)))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_sockptr(optval, &info, len))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\t}\n\tcase TCP_CC_INFO: {\n\t\tconst struct tcp_congestion_ops *ca_ops;\n\t\tunion tcp_cc_info info;\n\t\tsize_t sz = 0;\n\t\tint attr;\n\n\t\tif (copy_from_sockptr(&len, optlen, sizeof(int)))\n\t\t\treturn -EFAULT;\n\n\t\tca_ops = icsk->icsk_ca_ops;\n\t\tif (ca_ops && ca_ops->get_info)\n\t\t\tsz = ca_ops->get_info(sk, ~0U, &attr, &info);\n\n\t\tlen = min_t(unsigned int, len, sz);\n\t\tif (copy_to_sockptr(optlen, &len, sizeof(int)))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_sockptr(optval, &info, len))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\t}\n\tcase TCP_QUICKACK:\n\t\tval = !inet_csk_in_pingpong_mode(sk);\n\t\tbreak;\n\n\tcase TCP_CONGESTION:\n\t\tif (copy_from_sockptr(&len, optlen, sizeof(int)))\n\t\t\treturn -EFAULT;\n\t\tlen = min_t(unsigned int, len, TCP_CA_NAME_MAX);\n\t\tif (copy_to_sockptr(optlen, &len, sizeof(int)))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_sockptr(optval, icsk->icsk_ca_ops->name, len))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\n\tcase TCP_ULP:\n\t\tif (copy_from_sockptr(&len, optlen, sizeof(int)))\n\t\t\treturn -EFAULT;\n\t\tlen = min_t(unsigned int, len, TCP_ULP_NAME_MAX);\n\t\tif (!icsk->icsk_ulp_ops) {\n\t\t\tlen = 0;\n\t\t\tif (copy_to_sockptr(optlen, &len, sizeof(int)))\n\t\t\t\treturn -EFAULT;\n\t\t\treturn 0;\n\t\t}\n\t\tif (copy_to_sockptr(optlen, &len, sizeof(int)))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_sockptr(optval, icsk->icsk_ulp_ops->name, len))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\n\tcase TCP_FASTOPEN_KEY: {\n\t\tu64 key[TCP_FASTOPEN_KEY_BUF_LENGTH / sizeof(u64)];\n\t\tunsigned int key_len;\n\n\t\tif (copy_from_sockptr(&len, optlen, sizeof(int)))\n\t\t\treturn -EFAULT;\n\n\t\tkey_len = tcp_fastopen_get_cipher(net, icsk, key) *\n\t\t\t\tTCP_FASTOPEN_KEY_LENGTH;\n\t\tlen = min_t(unsigned int, len, key_len);\n\t\tif (copy_to_sockptr(optlen, &len, sizeof(int)))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_sockptr(optval, key, len))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\t}\n\tcase TCP_THIN_LINEAR_TIMEOUTS:\n\t\tval = tp->thin_lto;\n\t\tbreak;\n\n\tcase TCP_THIN_DUPACK:\n\t\tval = 0;\n\t\tbreak;\n\n\tcase TCP_REPAIR:\n\t\tval = tp->repair;\n\t\tbreak;\n\n\tcase TCP_REPAIR_QUEUE:\n\t\tif (tp->repair)\n\t\t\tval = tp->repair_queue;\n\t\telse\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\n\tcase TCP_REPAIR_WINDOW: {\n\t\tstruct tcp_repair_window opt;\n\n\t\tif (copy_from_sockptr(&len, optlen, sizeof(int)))\n\t\t\treturn -EFAULT;\n\n\t\tif (len != sizeof(opt))\n\t\t\treturn -EINVAL;\n\n\t\tif (!tp->repair)\n\t\t\treturn -EPERM;\n\n\t\topt.snd_wl1\t= tp->snd_wl1;\n\t\topt.snd_wnd\t= tp->snd_wnd;\n\t\topt.max_window\t= tp->max_window;\n\t\topt.rcv_wnd\t= tp->rcv_wnd;\n\t\topt.rcv_wup\t= tp->rcv_wup;\n\n\t\tif (copy_to_sockptr(optval, &opt, len))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\t}\n\tcase TCP_QUEUE_SEQ:\n\t\tif (tp->repair_queue == TCP_SEND_QUEUE)\n\t\t\tval = tp->write_seq;\n\t\telse if (tp->repair_queue == TCP_RECV_QUEUE)\n\t\t\tval = tp->rcv_nxt;\n\t\telse\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\n\tcase TCP_USER_TIMEOUT:\n\t\tval = READ_ONCE(icsk->icsk_user_timeout);\n\t\tbreak;\n\n\tcase TCP_FASTOPEN:\n\t\tval = READ_ONCE(icsk->icsk_accept_queue.fastopenq.max_qlen);\n\t\tbreak;\n\n\tcase TCP_FASTOPEN_CONNECT:\n\t\tval = tp->fastopen_connect;\n\t\tbreak;\n\n\tcase TCP_FASTOPEN_NO_COOKIE:\n\t\tval = tp->fastopen_no_cookie;\n\t\tbreak;\n\n\tcase TCP_TX_DELAY:\n\t\tval = READ_ONCE(tp->tcp_tx_delay);\n\t\tbreak;\n\n\tcase TCP_TIMESTAMP:\n\t\tval = tcp_time_stamp_raw() + READ_ONCE(tp->tsoffset);\n\t\tbreak;\n\tcase TCP_NOTSENT_LOWAT:\n\t\tval = READ_ONCE(tp->notsent_lowat);\n\t\tbreak;\n\tcase TCP_INQ:\n\t\tval = tp->recvmsg_inq;\n\t\tbreak;\n\tcase TCP_SAVE_SYN:\n\t\tval = tp->save_syn;\n\t\tbreak;\n\tcase TCP_SAVED_SYN: {\n\t\tif (copy_from_sockptr(&len, optlen, sizeof(int)))\n\t\t\treturn -EFAULT;\n\n\t\tsockopt_lock_sock(sk);\n\t\tif (tp->saved_syn) {\n\t\t\tif (len < tcp_saved_syn_len(tp->saved_syn)) {\n\t\t\t\tlen = tcp_saved_syn_len(tp->saved_syn);\n\t\t\t\tif (copy_to_sockptr(optlen, &len, sizeof(int))) {\n\t\t\t\t\tsockopt_release_sock(sk);\n\t\t\t\t\treturn -EFAULT;\n\t\t\t\t}\n\t\t\t\tsockopt_release_sock(sk);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tlen = tcp_saved_syn_len(tp->saved_syn);\n\t\t\tif (copy_to_sockptr(optlen, &len, sizeof(int))) {\n\t\t\t\tsockopt_release_sock(sk);\n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t\tif (copy_to_sockptr(optval, tp->saved_syn->data, len)) {\n\t\t\t\tsockopt_release_sock(sk);\n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t\ttcp_saved_syn_free(tp);\n\t\t\tsockopt_release_sock(sk);\n\t\t} else {\n\t\t\tsockopt_release_sock(sk);\n\t\t\tlen = 0;\n\t\t\tif (copy_to_sockptr(optlen, &len, sizeof(int)))\n\t\t\t\treturn -EFAULT;\n\t\t}\n\t\treturn 0;\n\t}\n#ifdef CONFIG_MMU\n\tcase TCP_ZEROCOPY_RECEIVE: {\n\t\tstruct scm_timestamping_internal tss;\n\t\tstruct tcp_zerocopy_receive zc = {};\n\t\tint err;\n\n\t\tif (copy_from_sockptr(&len, optlen, sizeof(int)))\n\t\t\treturn -EFAULT;\n\t\tif (len < 0 ||\n\t\t    len < offsetofend(struct tcp_zerocopy_receive, length))\n\t\t\treturn -EINVAL;\n\t\tif (unlikely(len > sizeof(zc))) {\n\t\t\terr = check_zeroed_sockptr(optval, sizeof(zc),\n\t\t\t\t\t\t   len - sizeof(zc));\n\t\t\tif (err < 1)\n\t\t\t\treturn err == 0 ? -EINVAL : err;\n\t\t\tlen = sizeof(zc);\n\t\t\tif (copy_to_sockptr(optlen, &len, sizeof(int)))\n\t\t\t\treturn -EFAULT;\n\t\t}\n\t\tif (copy_from_sockptr(&zc, optval, len))\n\t\t\treturn -EFAULT;\n\t\tif (zc.reserved)\n\t\t\treturn -EINVAL;\n\t\tif (zc.msg_flags &  ~(TCP_VALID_ZC_MSG_FLAGS))\n\t\t\treturn -EINVAL;\n\t\tsockopt_lock_sock(sk);\n\t\terr = tcp_zerocopy_receive(sk, &zc, &tss);\n\t\terr = BPF_CGROUP_RUN_PROG_GETSOCKOPT_KERN(sk, level, optname,\n\t\t\t\t\t\t\t  &zc, &len, err);\n\t\tsockopt_release_sock(sk);\n\t\tif (len >= offsetofend(struct tcp_zerocopy_receive, msg_flags))\n\t\t\tgoto zerocopy_rcv_cmsg;\n\t\tswitch (len) {\n\t\tcase offsetofend(struct tcp_zerocopy_receive, msg_flags):\n\t\t\tgoto zerocopy_rcv_cmsg;\n\t\tcase offsetofend(struct tcp_zerocopy_receive, msg_controllen):\n\t\tcase offsetofend(struct tcp_zerocopy_receive, msg_control):\n\t\tcase offsetofend(struct tcp_zerocopy_receive, flags):\n\t\tcase offsetofend(struct tcp_zerocopy_receive, copybuf_len):\n\t\tcase offsetofend(struct tcp_zerocopy_receive, copybuf_address):\n\t\tcase offsetofend(struct tcp_zerocopy_receive, err):\n\t\t\tgoto zerocopy_rcv_sk_err;\n\t\tcase offsetofend(struct tcp_zerocopy_receive, inq):\n\t\t\tgoto zerocopy_rcv_inq;\n\t\tcase offsetofend(struct tcp_zerocopy_receive, length):\n\t\tdefault:\n\t\t\tgoto zerocopy_rcv_out;\n\t\t}\nzerocopy_rcv_cmsg:\n\t\tif (zc.msg_flags & TCP_CMSG_TS)\n\t\t\ttcp_zc_finalize_rx_tstamp(sk, &zc, &tss);\n\t\telse\n\t\t\tzc.msg_flags = 0;\nzerocopy_rcv_sk_err:\n\t\tif (!err)\n\t\t\tzc.err = sock_error(sk);\nzerocopy_rcv_inq:\n\t\tzc.inq = tcp_inq_hint(sk);\nzerocopy_rcv_out:\n\t\tif (!err && copy_to_sockptr(optval, &zc, len))\n\t\t\terr = -EFAULT;\n\t\treturn err;\n\t}\n#endif\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\tif (copy_to_sockptr(optlen, &len, sizeof(int)))\n\t\treturn -EFAULT;\n\tif (copy_to_sockptr(optval, &val, len))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nbool tcp_bpf_bypass_getsockopt(int level, int optname)\n{\n\t \n\tif (level == SOL_TCP && optname == TCP_ZEROCOPY_RECEIVE)\n\t\treturn true;\n\n\treturn false;\n}\nEXPORT_SYMBOL(tcp_bpf_bypass_getsockopt);\n\nint tcp_getsockopt(struct sock *sk, int level, int optname, char __user *optval,\n\t\t   int __user *optlen)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\tif (level != SOL_TCP)\n\t\t \n\t\treturn READ_ONCE(icsk->icsk_af_ops)->getsockopt(sk, level, optname,\n\t\t\t\t\t\t\t\toptval, optlen);\n\treturn do_tcp_getsockopt(sk, level, optname, USER_SOCKPTR(optval),\n\t\t\t\t USER_SOCKPTR(optlen));\n}\nEXPORT_SYMBOL(tcp_getsockopt);\n\n#ifdef CONFIG_TCP_MD5SIG\nstatic DEFINE_PER_CPU(struct tcp_md5sig_pool, tcp_md5sig_pool);\nstatic DEFINE_MUTEX(tcp_md5sig_mutex);\nstatic bool tcp_md5sig_pool_populated = false;\n\nstatic void __tcp_alloc_md5sig_pool(void)\n{\n\tstruct crypto_ahash *hash;\n\tint cpu;\n\n\thash = crypto_alloc_ahash(\"md5\", 0, CRYPTO_ALG_ASYNC);\n\tif (IS_ERR(hash))\n\t\treturn;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tvoid *scratch = per_cpu(tcp_md5sig_pool, cpu).scratch;\n\t\tstruct ahash_request *req;\n\n\t\tif (!scratch) {\n\t\t\tscratch = kmalloc_node(sizeof(union tcp_md5sum_block) +\n\t\t\t\t\t       sizeof(struct tcphdr),\n\t\t\t\t\t       GFP_KERNEL,\n\t\t\t\t\t       cpu_to_node(cpu));\n\t\t\tif (!scratch)\n\t\t\t\treturn;\n\t\t\tper_cpu(tcp_md5sig_pool, cpu).scratch = scratch;\n\t\t}\n\t\tif (per_cpu(tcp_md5sig_pool, cpu).md5_req)\n\t\t\tcontinue;\n\n\t\treq = ahash_request_alloc(hash, GFP_KERNEL);\n\t\tif (!req)\n\t\t\treturn;\n\n\t\tahash_request_set_callback(req, 0, NULL, NULL);\n\n\t\tper_cpu(tcp_md5sig_pool, cpu).md5_req = req;\n\t}\n\t \n\tsmp_wmb();\n\t \n\tWRITE_ONCE(tcp_md5sig_pool_populated, true);\n}\n\nbool tcp_alloc_md5sig_pool(void)\n{\n\t \n\tif (unlikely(!READ_ONCE(tcp_md5sig_pool_populated))) {\n\t\tmutex_lock(&tcp_md5sig_mutex);\n\n\t\tif (!tcp_md5sig_pool_populated)\n\t\t\t__tcp_alloc_md5sig_pool();\n\n\t\tmutex_unlock(&tcp_md5sig_mutex);\n\t}\n\t \n\treturn READ_ONCE(tcp_md5sig_pool_populated);\n}\nEXPORT_SYMBOL(tcp_alloc_md5sig_pool);\n\n\n \nstruct tcp_md5sig_pool *tcp_get_md5sig_pool(void)\n{\n\tlocal_bh_disable();\n\n\t \n\tif (READ_ONCE(tcp_md5sig_pool_populated)) {\n\t\t \n\t\tsmp_rmb();\n\t\treturn this_cpu_ptr(&tcp_md5sig_pool);\n\t}\n\tlocal_bh_enable();\n\treturn NULL;\n}\nEXPORT_SYMBOL(tcp_get_md5sig_pool);\n\nint tcp_md5_hash_skb_data(struct tcp_md5sig_pool *hp,\n\t\t\t  const struct sk_buff *skb, unsigned int header_len)\n{\n\tstruct scatterlist sg;\n\tconst struct tcphdr *tp = tcp_hdr(skb);\n\tstruct ahash_request *req = hp->md5_req;\n\tunsigned int i;\n\tconst unsigned int head_data_len = skb_headlen(skb) > header_len ?\n\t\t\t\t\t   skb_headlen(skb) - header_len : 0;\n\tconst struct skb_shared_info *shi = skb_shinfo(skb);\n\tstruct sk_buff *frag_iter;\n\n\tsg_init_table(&sg, 1);\n\n\tsg_set_buf(&sg, ((u8 *) tp) + header_len, head_data_len);\n\tahash_request_set_crypt(req, &sg, NULL, head_data_len);\n\tif (crypto_ahash_update(req))\n\t\treturn 1;\n\n\tfor (i = 0; i < shi->nr_frags; ++i) {\n\t\tconst skb_frag_t *f = &shi->frags[i];\n\t\tunsigned int offset = skb_frag_off(f);\n\t\tstruct page *page = skb_frag_page(f) + (offset >> PAGE_SHIFT);\n\n\t\tsg_set_page(&sg, page, skb_frag_size(f),\n\t\t\t    offset_in_page(offset));\n\t\tahash_request_set_crypt(req, &sg, NULL, skb_frag_size(f));\n\t\tif (crypto_ahash_update(req))\n\t\t\treturn 1;\n\t}\n\n\tskb_walk_frags(skb, frag_iter)\n\t\tif (tcp_md5_hash_skb_data(hp, frag_iter, 0))\n\t\t\treturn 1;\n\n\treturn 0;\n}\nEXPORT_SYMBOL(tcp_md5_hash_skb_data);\n\nint tcp_md5_hash_key(struct tcp_md5sig_pool *hp, const struct tcp_md5sig_key *key)\n{\n\tu8 keylen = READ_ONCE(key->keylen);  \n\tstruct scatterlist sg;\n\n\tsg_init_one(&sg, key->key, keylen);\n\tahash_request_set_crypt(hp->md5_req, &sg, NULL, keylen);\n\n\t \n\treturn data_race(crypto_ahash_update(hp->md5_req));\n}\nEXPORT_SYMBOL(tcp_md5_hash_key);\n\n \nenum skb_drop_reason\ntcp_inbound_md5_hash(const struct sock *sk, const struct sk_buff *skb,\n\t\t     const void *saddr, const void *daddr,\n\t\t     int family, int dif, int sdif)\n{\n\t \n\tconst __u8 *hash_location = NULL;\n\tstruct tcp_md5sig_key *hash_expected;\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tint genhash, l3index;\n\tu8 newhash[16];\n\n\t \n\tl3index = sdif ? dif : 0;\n\n\thash_expected = tcp_md5_do_lookup(sk, l3index, saddr, family);\n\thash_location = tcp_parse_md5sig_option(th);\n\n\t \n\tif (!hash_expected && !hash_location)\n\t\treturn SKB_NOT_DROPPED_YET;\n\n\tif (hash_expected && !hash_location) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPMD5NOTFOUND);\n\t\treturn SKB_DROP_REASON_TCP_MD5NOTFOUND;\n\t}\n\n\tif (!hash_expected && hash_location) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPMD5UNEXPECTED);\n\t\treturn SKB_DROP_REASON_TCP_MD5UNEXPECTED;\n\t}\n\n\t \n\tif (family == AF_INET)\n\t\tgenhash = tcp_v4_md5_hash_skb(newhash,\n\t\t\t\t\t      hash_expected,\n\t\t\t\t\t      NULL, skb);\n\telse\n\t\tgenhash = tp->af_specific->calc_md5_hash(newhash,\n\t\t\t\t\t\t\t hash_expected,\n\t\t\t\t\t\t\t NULL, skb);\n\n\tif (genhash || memcmp(hash_location, newhash, 16) != 0) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPMD5FAILURE);\n\t\tif (family == AF_INET) {\n\t\t\tnet_info_ratelimited(\"MD5 Hash failed for (%pI4, %d)->(%pI4, %d)%s L3 index %d\\n\",\n\t\t\t\t\tsaddr, ntohs(th->source),\n\t\t\t\t\tdaddr, ntohs(th->dest),\n\t\t\t\t\tgenhash ? \" tcp_v4_calc_md5_hash failed\"\n\t\t\t\t\t: \"\", l3index);\n\t\t} else {\n\t\t\tnet_info_ratelimited(\"MD5 Hash %s for [%pI6c]:%u->[%pI6c]:%u L3 index %d\\n\",\n\t\t\t\t\tgenhash ? \"failed\" : \"mismatch\",\n\t\t\t\t\tsaddr, ntohs(th->source),\n\t\t\t\t\tdaddr, ntohs(th->dest), l3index);\n\t\t}\n\t\treturn SKB_DROP_REASON_TCP_MD5FAILURE;\n\t}\n\treturn SKB_NOT_DROPPED_YET;\n}\nEXPORT_SYMBOL(tcp_inbound_md5_hash);\n\n#endif\n\nvoid tcp_done(struct sock *sk)\n{\n\tstruct request_sock *req;\n\n\t \n\treq = rcu_dereference_protected(tcp_sk(sk)->fastopen_rsk, 1);\n\n\tif (sk->sk_state == TCP_SYN_SENT || sk->sk_state == TCP_SYN_RECV)\n\t\tTCP_INC_STATS(sock_net(sk), TCP_MIB_ATTEMPTFAILS);\n\n\ttcp_set_state(sk, TCP_CLOSE);\n\ttcp_clear_xmit_timers(sk);\n\tif (req)\n\t\treqsk_fastopen_remove(sk, req, false);\n\n\tWRITE_ONCE(sk->sk_shutdown, SHUTDOWN_MASK);\n\n\tif (!sock_flag(sk, SOCK_DEAD))\n\t\tsk->sk_state_change(sk);\n\telse\n\t\tinet_csk_destroy_sock(sk);\n}\nEXPORT_SYMBOL_GPL(tcp_done);\n\nint tcp_abort(struct sock *sk, int err)\n{\n\tint state = inet_sk_state_load(sk);\n\n\tif (state == TCP_NEW_SYN_RECV) {\n\t\tstruct request_sock *req = inet_reqsk(sk);\n\n\t\tlocal_bh_disable();\n\t\tinet_csk_reqsk_queue_drop(req->rsk_listener, req);\n\t\tlocal_bh_enable();\n\t\treturn 0;\n\t}\n\tif (state == TCP_TIME_WAIT) {\n\t\tstruct inet_timewait_sock *tw = inet_twsk(sk);\n\n\t\trefcount_inc(&tw->tw_refcnt);\n\t\tlocal_bh_disable();\n\t\tinet_twsk_deschedule_put(tw);\n\t\tlocal_bh_enable();\n\t\treturn 0;\n\t}\n\n\t \n\tif (!has_current_bpf_ctx())\n\t\t \n\t\tlock_sock(sk);\n\n\tif (sk->sk_state == TCP_LISTEN) {\n\t\ttcp_set_state(sk, TCP_CLOSE);\n\t\tinet_csk_listen_stop(sk);\n\t}\n\n\t \n\tlocal_bh_disable();\n\tbh_lock_sock(sk);\n\n\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\tWRITE_ONCE(sk->sk_err, err);\n\t\t \n\t\tsmp_wmb();\n\t\tsk_error_report(sk);\n\t\tif (tcp_need_reset(sk->sk_state))\n\t\t\ttcp_send_active_reset(sk, GFP_ATOMIC);\n\t\ttcp_done(sk);\n\t}\n\n\tbh_unlock_sock(sk);\n\tlocal_bh_enable();\n\ttcp_write_queue_purge(sk);\n\tif (!has_current_bpf_ctx())\n\t\trelease_sock(sk);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(tcp_abort);\n\nextern struct tcp_congestion_ops tcp_reno;\n\nstatic __initdata unsigned long thash_entries;\nstatic int __init set_thash_entries(char *str)\n{\n\tssize_t ret;\n\n\tif (!str)\n\t\treturn 0;\n\n\tret = kstrtoul(str, 0, &thash_entries);\n\tif (ret)\n\t\treturn 0;\n\n\treturn 1;\n}\n__setup(\"thash_entries=\", set_thash_entries);\n\nstatic void __init tcp_init_mem(void)\n{\n\tunsigned long limit = nr_free_buffer_pages() / 16;\n\n\tlimit = max(limit, 128UL);\n\tsysctl_tcp_mem[0] = limit / 4 * 3;\t\t \n\tsysctl_tcp_mem[1] = limit;\t\t\t \n\tsysctl_tcp_mem[2] = sysctl_tcp_mem[0] * 2;\t \n}\n\nvoid __init tcp_init(void)\n{\n\tint max_rshare, max_wshare, cnt;\n\tunsigned long limit;\n\tunsigned int i;\n\n\tBUILD_BUG_ON(TCP_MIN_SND_MSS <= MAX_TCP_OPTION_SPACE);\n\tBUILD_BUG_ON(sizeof(struct tcp_skb_cb) >\n\t\t     sizeof_field(struct sk_buff, cb));\n\n\tpercpu_counter_init(&tcp_sockets_allocated, 0, GFP_KERNEL);\n\n\ttimer_setup(&tcp_orphan_timer, tcp_orphan_update, TIMER_DEFERRABLE);\n\tmod_timer(&tcp_orphan_timer, jiffies + TCP_ORPHAN_TIMER_PERIOD);\n\n\tinet_hashinfo2_init(&tcp_hashinfo, \"tcp_listen_portaddr_hash\",\n\t\t\t    thash_entries, 21,   \n\t\t\t    0, 64 * 1024);\n\ttcp_hashinfo.bind_bucket_cachep =\n\t\tkmem_cache_create(\"tcp_bind_bucket\",\n\t\t\t\t  sizeof(struct inet_bind_bucket), 0,\n\t\t\t\t  SLAB_HWCACHE_ALIGN | SLAB_PANIC |\n\t\t\t\t  SLAB_ACCOUNT,\n\t\t\t\t  NULL);\n\ttcp_hashinfo.bind2_bucket_cachep =\n\t\tkmem_cache_create(\"tcp_bind2_bucket\",\n\t\t\t\t  sizeof(struct inet_bind2_bucket), 0,\n\t\t\t\t  SLAB_HWCACHE_ALIGN | SLAB_PANIC |\n\t\t\t\t  SLAB_ACCOUNT,\n\t\t\t\t  NULL);\n\n\t \n\ttcp_hashinfo.ehash =\n\t\talloc_large_system_hash(\"TCP established\",\n\t\t\t\t\tsizeof(struct inet_ehash_bucket),\n\t\t\t\t\tthash_entries,\n\t\t\t\t\t17,  \n\t\t\t\t\t0,\n\t\t\t\t\tNULL,\n\t\t\t\t\t&tcp_hashinfo.ehash_mask,\n\t\t\t\t\t0,\n\t\t\t\t\tthash_entries ? 0 : 512 * 1024);\n\tfor (i = 0; i <= tcp_hashinfo.ehash_mask; i++)\n\t\tINIT_HLIST_NULLS_HEAD(&tcp_hashinfo.ehash[i].chain, i);\n\n\tif (inet_ehash_locks_alloc(&tcp_hashinfo))\n\t\tpanic(\"TCP: failed to alloc ehash_locks\");\n\ttcp_hashinfo.bhash =\n\t\talloc_large_system_hash(\"TCP bind\",\n\t\t\t\t\t2 * sizeof(struct inet_bind_hashbucket),\n\t\t\t\t\ttcp_hashinfo.ehash_mask + 1,\n\t\t\t\t\t17,  \n\t\t\t\t\t0,\n\t\t\t\t\t&tcp_hashinfo.bhash_size,\n\t\t\t\t\tNULL,\n\t\t\t\t\t0,\n\t\t\t\t\t64 * 1024);\n\ttcp_hashinfo.bhash_size = 1U << tcp_hashinfo.bhash_size;\n\ttcp_hashinfo.bhash2 = tcp_hashinfo.bhash + tcp_hashinfo.bhash_size;\n\tfor (i = 0; i < tcp_hashinfo.bhash_size; i++) {\n\t\tspin_lock_init(&tcp_hashinfo.bhash[i].lock);\n\t\tINIT_HLIST_HEAD(&tcp_hashinfo.bhash[i].chain);\n\t\tspin_lock_init(&tcp_hashinfo.bhash2[i].lock);\n\t\tINIT_HLIST_HEAD(&tcp_hashinfo.bhash2[i].chain);\n\t}\n\n\ttcp_hashinfo.pernet = false;\n\n\tcnt = tcp_hashinfo.ehash_mask + 1;\n\tsysctl_tcp_max_orphans = cnt / 2;\n\n\ttcp_init_mem();\n\t \n\tlimit = nr_free_buffer_pages() << (PAGE_SHIFT - 7);\n\tmax_wshare = min(4UL*1024*1024, limit);\n\tmax_rshare = min(6UL*1024*1024, limit);\n\n\tinit_net.ipv4.sysctl_tcp_wmem[0] = PAGE_SIZE;\n\tinit_net.ipv4.sysctl_tcp_wmem[1] = 16*1024;\n\tinit_net.ipv4.sysctl_tcp_wmem[2] = max(64*1024, max_wshare);\n\n\tinit_net.ipv4.sysctl_tcp_rmem[0] = PAGE_SIZE;\n\tinit_net.ipv4.sysctl_tcp_rmem[1] = 131072;\n\tinit_net.ipv4.sysctl_tcp_rmem[2] = max(131072, max_rshare);\n\n\tpr_info(\"Hash tables configured (established %u bind %u)\\n\",\n\t\ttcp_hashinfo.ehash_mask + 1, tcp_hashinfo.bhash_size);\n\n\ttcp_v4_init();\n\ttcp_metrics_init();\n\tBUG_ON(tcp_register_congestion_control(&tcp_reno) != 0);\n\ttcp_tasklet_init();\n\tmptcp_init();\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}