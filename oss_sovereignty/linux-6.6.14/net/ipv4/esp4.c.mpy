{
  "module_name": "esp4.c",
  "hash_id": "8d5cec1aec866db3e12529fcc98e0ee704149715eda2fcd001e1b6b543f8240c",
  "original_prompt": "Ingested from linux-6.6.14/net/ipv4/esp4.c",
  "human_readable_source": "\n#define pr_fmt(fmt) \"IPsec: \" fmt\n\n#include <crypto/aead.h>\n#include <crypto/authenc.h>\n#include <linux/err.h>\n#include <linux/module.h>\n#include <net/ip.h>\n#include <net/xfrm.h>\n#include <net/esp.h>\n#include <linux/scatterlist.h>\n#include <linux/kernel.h>\n#include <linux/pfkeyv2.h>\n#include <linux/rtnetlink.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n#include <linux/in6.h>\n#include <net/icmp.h>\n#include <net/protocol.h>\n#include <net/udp.h>\n#include <net/tcp.h>\n#include <net/espintcp.h>\n\n#include <linux/highmem.h>\n\nstruct esp_skb_cb {\n\tstruct xfrm_skb_cb xfrm;\n\tvoid *tmp;\n};\n\nstruct esp_output_extra {\n\t__be32 seqhi;\n\tu32 esphoff;\n};\n\n#define ESP_SKB_CB(__skb) ((struct esp_skb_cb *)&((__skb)->cb[0]))\n\n \nstatic void *esp_alloc_tmp(struct crypto_aead *aead, int nfrags, int extralen)\n{\n\tunsigned int len;\n\n\tlen = extralen;\n\n\tlen += crypto_aead_ivsize(aead);\n\n\tif (len) {\n\t\tlen += crypto_aead_alignmask(aead) &\n\t\t       ~(crypto_tfm_ctx_alignment() - 1);\n\t\tlen = ALIGN(len, crypto_tfm_ctx_alignment());\n\t}\n\n\tlen += sizeof(struct aead_request) + crypto_aead_reqsize(aead);\n\tlen = ALIGN(len, __alignof__(struct scatterlist));\n\n\tlen += sizeof(struct scatterlist) * nfrags;\n\n\treturn kmalloc(len, GFP_ATOMIC);\n}\n\nstatic inline void *esp_tmp_extra(void *tmp)\n{\n\treturn PTR_ALIGN(tmp, __alignof__(struct esp_output_extra));\n}\n\nstatic inline u8 *esp_tmp_iv(struct crypto_aead *aead, void *tmp, int extralen)\n{\n\treturn crypto_aead_ivsize(aead) ?\n\t       PTR_ALIGN((u8 *)tmp + extralen,\n\t\t\t crypto_aead_alignmask(aead) + 1) : tmp + extralen;\n}\n\nstatic inline struct aead_request *esp_tmp_req(struct crypto_aead *aead, u8 *iv)\n{\n\tstruct aead_request *req;\n\n\treq = (void *)PTR_ALIGN(iv + crypto_aead_ivsize(aead),\n\t\t\t\tcrypto_tfm_ctx_alignment());\n\taead_request_set_tfm(req, aead);\n\treturn req;\n}\n\nstatic inline struct scatterlist *esp_req_sg(struct crypto_aead *aead,\n\t\t\t\t\t     struct aead_request *req)\n{\n\treturn (void *)ALIGN((unsigned long)(req + 1) +\n\t\t\t     crypto_aead_reqsize(aead),\n\t\t\t     __alignof__(struct scatterlist));\n}\n\nstatic void esp_ssg_unref(struct xfrm_state *x, void *tmp)\n{\n\tstruct crypto_aead *aead = x->data;\n\tint extralen = 0;\n\tu8 *iv;\n\tstruct aead_request *req;\n\tstruct scatterlist *sg;\n\n\tif (x->props.flags & XFRM_STATE_ESN)\n\t\textralen += sizeof(struct esp_output_extra);\n\n\tiv = esp_tmp_iv(aead, tmp, extralen);\n\treq = esp_tmp_req(aead, iv);\n\n\t \n\tif (req->src != req->dst)\n\t\tfor (sg = sg_next(req->src); sg; sg = sg_next(sg))\n\t\t\tput_page(sg_page(sg));\n}\n\n#ifdef CONFIG_INET_ESPINTCP\nstruct esp_tcp_sk {\n\tstruct sock *sk;\n\tstruct rcu_head rcu;\n};\n\nstatic void esp_free_tcp_sk(struct rcu_head *head)\n{\n\tstruct esp_tcp_sk *esk = container_of(head, struct esp_tcp_sk, rcu);\n\n\tsock_put(esk->sk);\n\tkfree(esk);\n}\n\nstatic struct sock *esp_find_tcp_sk(struct xfrm_state *x)\n{\n\tstruct xfrm_encap_tmpl *encap = x->encap;\n\tstruct net *net = xs_net(x);\n\tstruct esp_tcp_sk *esk;\n\t__be16 sport, dport;\n\tstruct sock *nsk;\n\tstruct sock *sk;\n\n\tsk = rcu_dereference(x->encap_sk);\n\tif (sk && sk->sk_state == TCP_ESTABLISHED)\n\t\treturn sk;\n\n\tspin_lock_bh(&x->lock);\n\tsport = encap->encap_sport;\n\tdport = encap->encap_dport;\n\tnsk = rcu_dereference_protected(x->encap_sk,\n\t\t\t\t\tlockdep_is_held(&x->lock));\n\tif (sk && sk == nsk) {\n\t\tesk = kmalloc(sizeof(*esk), GFP_ATOMIC);\n\t\tif (!esk) {\n\t\t\tspin_unlock_bh(&x->lock);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\tRCU_INIT_POINTER(x->encap_sk, NULL);\n\t\tesk->sk = sk;\n\t\tcall_rcu(&esk->rcu, esp_free_tcp_sk);\n\t}\n\tspin_unlock_bh(&x->lock);\n\n\tsk = inet_lookup_established(net, net->ipv4.tcp_death_row.hashinfo, x->id.daddr.a4,\n\t\t\t\t     dport, x->props.saddr.a4, sport, 0);\n\tif (!sk)\n\t\treturn ERR_PTR(-ENOENT);\n\n\tif (!tcp_is_ulp_esp(sk)) {\n\t\tsock_put(sk);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tspin_lock_bh(&x->lock);\n\tnsk = rcu_dereference_protected(x->encap_sk,\n\t\t\t\t\tlockdep_is_held(&x->lock));\n\tif (encap->encap_sport != sport ||\n\t    encap->encap_dport != dport) {\n\t\tsock_put(sk);\n\t\tsk = nsk ?: ERR_PTR(-EREMCHG);\n\t} else if (sk == nsk) {\n\t\tsock_put(sk);\n\t} else {\n\t\trcu_assign_pointer(x->encap_sk, sk);\n\t}\n\tspin_unlock_bh(&x->lock);\n\n\treturn sk;\n}\n\nstatic int esp_output_tcp_finish(struct xfrm_state *x, struct sk_buff *skb)\n{\n\tstruct sock *sk;\n\tint err;\n\n\trcu_read_lock();\n\n\tsk = esp_find_tcp_sk(x);\n\terr = PTR_ERR_OR_ZERO(sk);\n\tif (err)\n\t\tgoto out;\n\n\tbh_lock_sock(sk);\n\tif (sock_owned_by_user(sk))\n\t\terr = espintcp_queue_out(sk, skb);\n\telse\n\t\terr = espintcp_push_skb(sk, skb);\n\tbh_unlock_sock(sk);\n\nout:\n\trcu_read_unlock();\n\treturn err;\n}\n\nstatic int esp_output_tcp_encap_cb(struct net *net, struct sock *sk,\n\t\t\t\t   struct sk_buff *skb)\n{\n\tstruct dst_entry *dst = skb_dst(skb);\n\tstruct xfrm_state *x = dst->xfrm;\n\n\treturn esp_output_tcp_finish(x, skb);\n}\n\nstatic int esp_output_tail_tcp(struct xfrm_state *x, struct sk_buff *skb)\n{\n\tint err;\n\n\tlocal_bh_disable();\n\terr = xfrm_trans_queue_net(xs_net(x), skb, esp_output_tcp_encap_cb);\n\tlocal_bh_enable();\n\n\t \n\treturn err ?: -EINPROGRESS;\n}\n#else\nstatic int esp_output_tail_tcp(struct xfrm_state *x, struct sk_buff *skb)\n{\n\tkfree_skb(skb);\n\n\treturn -EOPNOTSUPP;\n}\n#endif\n\nstatic void esp_output_done(void *data, int err)\n{\n\tstruct sk_buff *skb = data;\n\tstruct xfrm_offload *xo = xfrm_offload(skb);\n\tvoid *tmp;\n\tstruct xfrm_state *x;\n\n\tif (xo && (xo->flags & XFRM_DEV_RESUME)) {\n\t\tstruct sec_path *sp = skb_sec_path(skb);\n\n\t\tx = sp->xvec[sp->len - 1];\n\t} else {\n\t\tx = skb_dst(skb)->xfrm;\n\t}\n\n\ttmp = ESP_SKB_CB(skb)->tmp;\n\tesp_ssg_unref(x, tmp);\n\tkfree(tmp);\n\n\tif (xo && (xo->flags & XFRM_DEV_RESUME)) {\n\t\tif (err) {\n\t\t\tXFRM_INC_STATS(xs_net(x), LINUX_MIB_XFRMOUTSTATEPROTOERROR);\n\t\t\tkfree_skb(skb);\n\t\t\treturn;\n\t\t}\n\n\t\tskb_push(skb, skb->data - skb_mac_header(skb));\n\t\tsecpath_reset(skb);\n\t\txfrm_dev_resume(skb);\n\t} else {\n\t\tif (!err &&\n\t\t    x->encap && x->encap->encap_type == TCP_ENCAP_ESPINTCP)\n\t\t\tesp_output_tail_tcp(x, skb);\n\t\telse\n\t\t\txfrm_output_resume(skb->sk, skb, err);\n\t}\n}\n\n \nstatic void esp_restore_header(struct sk_buff *skb, unsigned int offset)\n{\n\tstruct ip_esp_hdr *esph = (void *)(skb->data + offset);\n\tvoid *tmp = ESP_SKB_CB(skb)->tmp;\n\t__be32 *seqhi = esp_tmp_extra(tmp);\n\n\tesph->seq_no = esph->spi;\n\tesph->spi = *seqhi;\n}\n\nstatic void esp_output_restore_header(struct sk_buff *skb)\n{\n\tvoid *tmp = ESP_SKB_CB(skb)->tmp;\n\tstruct esp_output_extra *extra = esp_tmp_extra(tmp);\n\n\tesp_restore_header(skb, skb_transport_offset(skb) + extra->esphoff -\n\t\t\t\tsizeof(__be32));\n}\n\nstatic struct ip_esp_hdr *esp_output_set_extra(struct sk_buff *skb,\n\t\t\t\t\t       struct xfrm_state *x,\n\t\t\t\t\t       struct ip_esp_hdr *esph,\n\t\t\t\t\t       struct esp_output_extra *extra)\n{\n\t \n\tif ((x->props.flags & XFRM_STATE_ESN)) {\n\t\t__u32 seqhi;\n\t\tstruct xfrm_offload *xo = xfrm_offload(skb);\n\n\t\tif (xo)\n\t\t\tseqhi = xo->seq.hi;\n\t\telse\n\t\t\tseqhi = XFRM_SKB_CB(skb)->seq.output.hi;\n\n\t\textra->esphoff = (unsigned char *)esph -\n\t\t\t\t skb_transport_header(skb);\n\t\tesph = (struct ip_esp_hdr *)((unsigned char *)esph - 4);\n\t\textra->seqhi = esph->spi;\n\t\tesph->seq_no = htonl(seqhi);\n\t}\n\n\tesph->spi = x->id.spi;\n\n\treturn esph;\n}\n\nstatic void esp_output_done_esn(void *data, int err)\n{\n\tstruct sk_buff *skb = data;\n\n\tesp_output_restore_header(skb);\n\tesp_output_done(data, err);\n}\n\nstatic struct ip_esp_hdr *esp_output_udp_encap(struct sk_buff *skb,\n\t\t\t\t\t       int encap_type,\n\t\t\t\t\t       struct esp_info *esp,\n\t\t\t\t\t       __be16 sport,\n\t\t\t\t\t       __be16 dport)\n{\n\tstruct udphdr *uh;\n\t__be32 *udpdata32;\n\tunsigned int len;\n\n\tlen = skb->len + esp->tailen - skb_transport_offset(skb);\n\tif (len + sizeof(struct iphdr) > IP_MAX_MTU)\n\t\treturn ERR_PTR(-EMSGSIZE);\n\n\tuh = (struct udphdr *)esp->esph;\n\tuh->source = sport;\n\tuh->dest = dport;\n\tuh->len = htons(len);\n\tuh->check = 0;\n\n\t*skb_mac_header(skb) = IPPROTO_UDP;\n\n\tif (encap_type == UDP_ENCAP_ESPINUDP_NON_IKE) {\n\t\tudpdata32 = (__be32 *)(uh + 1);\n\t\tudpdata32[0] = udpdata32[1] = 0;\n\t\treturn (struct ip_esp_hdr *)(udpdata32 + 2);\n\t}\n\n\treturn (struct ip_esp_hdr *)(uh + 1);\n}\n\n#ifdef CONFIG_INET_ESPINTCP\nstatic struct ip_esp_hdr *esp_output_tcp_encap(struct xfrm_state *x,\n\t\t\t\t\t\t    struct sk_buff *skb,\n\t\t\t\t\t\t    struct esp_info *esp)\n{\n\t__be16 *lenp = (void *)esp->esph;\n\tstruct ip_esp_hdr *esph;\n\tunsigned int len;\n\tstruct sock *sk;\n\n\tlen = skb->len + esp->tailen - skb_transport_offset(skb);\n\tif (len > IP_MAX_MTU)\n\t\treturn ERR_PTR(-EMSGSIZE);\n\n\trcu_read_lock();\n\tsk = esp_find_tcp_sk(x);\n\trcu_read_unlock();\n\n\tif (IS_ERR(sk))\n\t\treturn ERR_CAST(sk);\n\n\t*lenp = htons(len);\n\tesph = (struct ip_esp_hdr *)(lenp + 1);\n\n\treturn esph;\n}\n#else\nstatic struct ip_esp_hdr *esp_output_tcp_encap(struct xfrm_state *x,\n\t\t\t\t\t\t    struct sk_buff *skb,\n\t\t\t\t\t\t    struct esp_info *esp)\n{\n\treturn ERR_PTR(-EOPNOTSUPP);\n}\n#endif\n\nstatic int esp_output_encap(struct xfrm_state *x, struct sk_buff *skb,\n\t\t\t    struct esp_info *esp)\n{\n\tstruct xfrm_encap_tmpl *encap = x->encap;\n\tstruct ip_esp_hdr *esph;\n\t__be16 sport, dport;\n\tint encap_type;\n\n\tspin_lock_bh(&x->lock);\n\tsport = encap->encap_sport;\n\tdport = encap->encap_dport;\n\tencap_type = encap->encap_type;\n\tspin_unlock_bh(&x->lock);\n\n\tswitch (encap_type) {\n\tdefault:\n\tcase UDP_ENCAP_ESPINUDP:\n\tcase UDP_ENCAP_ESPINUDP_NON_IKE:\n\t\tesph = esp_output_udp_encap(skb, encap_type, esp, sport, dport);\n\t\tbreak;\n\tcase TCP_ENCAP_ESPINTCP:\n\t\tesph = esp_output_tcp_encap(x, skb, esp);\n\t\tbreak;\n\t}\n\n\tif (IS_ERR(esph))\n\t\treturn PTR_ERR(esph);\n\n\tesp->esph = esph;\n\n\treturn 0;\n}\n\nint esp_output_head(struct xfrm_state *x, struct sk_buff *skb, struct esp_info *esp)\n{\n\tu8 *tail;\n\tint nfrags;\n\tint esph_offset;\n\tstruct page *page;\n\tstruct sk_buff *trailer;\n\tint tailen = esp->tailen;\n\n\t \n\tif (x->encap) {\n\t\tint err = esp_output_encap(x, skb, esp);\n\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\n\tif (ALIGN(tailen, L1_CACHE_BYTES) > PAGE_SIZE ||\n\t    ALIGN(skb->data_len, L1_CACHE_BYTES) > PAGE_SIZE)\n\t\tgoto cow;\n\n\tif (!skb_cloned(skb)) {\n\t\tif (tailen <= skb_tailroom(skb)) {\n\t\t\tnfrags = 1;\n\t\t\ttrailer = skb;\n\t\t\ttail = skb_tail_pointer(trailer);\n\n\t\t\tgoto skip_cow;\n\t\t} else if ((skb_shinfo(skb)->nr_frags < MAX_SKB_FRAGS)\n\t\t\t   && !skb_has_frag_list(skb)) {\n\t\t\tint allocsize;\n\t\t\tstruct sock *sk = skb->sk;\n\t\t\tstruct page_frag *pfrag = &x->xfrag;\n\n\t\t\tesp->inplace = false;\n\n\t\t\tallocsize = ALIGN(tailen, L1_CACHE_BYTES);\n\n\t\t\tspin_lock_bh(&x->lock);\n\n\t\t\tif (unlikely(!skb_page_frag_refill(allocsize, pfrag, GFP_ATOMIC))) {\n\t\t\t\tspin_unlock_bh(&x->lock);\n\t\t\t\tgoto cow;\n\t\t\t}\n\n\t\t\tpage = pfrag->page;\n\t\t\tget_page(page);\n\n\t\t\ttail = page_address(page) + pfrag->offset;\n\n\t\t\tesp_output_fill_trailer(tail, esp->tfclen, esp->plen, esp->proto);\n\n\t\t\tnfrags = skb_shinfo(skb)->nr_frags;\n\n\t\t\t__skb_fill_page_desc(skb, nfrags, page, pfrag->offset,\n\t\t\t\t\t     tailen);\n\t\t\tskb_shinfo(skb)->nr_frags = ++nfrags;\n\n\t\t\tpfrag->offset = pfrag->offset + allocsize;\n\n\t\t\tspin_unlock_bh(&x->lock);\n\n\t\t\tnfrags++;\n\n\t\t\tskb_len_add(skb, tailen);\n\t\t\tif (sk && sk_fullsock(sk))\n\t\t\t\trefcount_add(tailen, &sk->sk_wmem_alloc);\n\n\t\t\tgoto out;\n\t\t}\n\t}\n\ncow:\n\tesph_offset = (unsigned char *)esp->esph - skb_transport_header(skb);\n\n\tnfrags = skb_cow_data(skb, tailen, &trailer);\n\tif (nfrags < 0)\n\t\tgoto out;\n\ttail = skb_tail_pointer(trailer);\n\tesp->esph = (struct ip_esp_hdr *)(skb_transport_header(skb) + esph_offset);\n\nskip_cow:\n\tesp_output_fill_trailer(tail, esp->tfclen, esp->plen, esp->proto);\n\tpskb_put(skb, trailer, tailen);\n\nout:\n\treturn nfrags;\n}\nEXPORT_SYMBOL_GPL(esp_output_head);\n\nint esp_output_tail(struct xfrm_state *x, struct sk_buff *skb, struct esp_info *esp)\n{\n\tu8 *iv;\n\tint alen;\n\tvoid *tmp;\n\tint ivlen;\n\tint assoclen;\n\tint extralen;\n\tstruct page *page;\n\tstruct ip_esp_hdr *esph;\n\tstruct crypto_aead *aead;\n\tstruct aead_request *req;\n\tstruct scatterlist *sg, *dsg;\n\tstruct esp_output_extra *extra;\n\tint err = -ENOMEM;\n\n\tassoclen = sizeof(struct ip_esp_hdr);\n\textralen = 0;\n\n\tif (x->props.flags & XFRM_STATE_ESN) {\n\t\textralen += sizeof(*extra);\n\t\tassoclen += sizeof(__be32);\n\t}\n\n\taead = x->data;\n\talen = crypto_aead_authsize(aead);\n\tivlen = crypto_aead_ivsize(aead);\n\n\ttmp = esp_alloc_tmp(aead, esp->nfrags + 2, extralen);\n\tif (!tmp)\n\t\tgoto error;\n\n\textra = esp_tmp_extra(tmp);\n\tiv = esp_tmp_iv(aead, tmp, extralen);\n\treq = esp_tmp_req(aead, iv);\n\tsg = esp_req_sg(aead, req);\n\n\tif (esp->inplace)\n\t\tdsg = sg;\n\telse\n\t\tdsg = &sg[esp->nfrags];\n\n\tesph = esp_output_set_extra(skb, x, esp->esph, extra);\n\tesp->esph = esph;\n\n\tsg_init_table(sg, esp->nfrags);\n\terr = skb_to_sgvec(skb, sg,\n\t\t           (unsigned char *)esph - skb->data,\n\t\t           assoclen + ivlen + esp->clen + alen);\n\tif (unlikely(err < 0))\n\t\tgoto error_free;\n\n\tif (!esp->inplace) {\n\t\tint allocsize;\n\t\tstruct page_frag *pfrag = &x->xfrag;\n\n\t\tallocsize = ALIGN(skb->data_len, L1_CACHE_BYTES);\n\n\t\tspin_lock_bh(&x->lock);\n\t\tif (unlikely(!skb_page_frag_refill(allocsize, pfrag, GFP_ATOMIC))) {\n\t\t\tspin_unlock_bh(&x->lock);\n\t\t\tgoto error_free;\n\t\t}\n\n\t\tskb_shinfo(skb)->nr_frags = 1;\n\n\t\tpage = pfrag->page;\n\t\tget_page(page);\n\t\t \n\t\t__skb_fill_page_desc(skb, 0, page, pfrag->offset, skb->data_len);\n\t\tpfrag->offset = pfrag->offset + allocsize;\n\t\tspin_unlock_bh(&x->lock);\n\n\t\tsg_init_table(dsg, skb_shinfo(skb)->nr_frags + 1);\n\t\terr = skb_to_sgvec(skb, dsg,\n\t\t\t           (unsigned char *)esph - skb->data,\n\t\t\t           assoclen + ivlen + esp->clen + alen);\n\t\tif (unlikely(err < 0))\n\t\t\tgoto error_free;\n\t}\n\n\tif ((x->props.flags & XFRM_STATE_ESN))\n\t\taead_request_set_callback(req, 0, esp_output_done_esn, skb);\n\telse\n\t\taead_request_set_callback(req, 0, esp_output_done, skb);\n\n\taead_request_set_crypt(req, sg, dsg, ivlen + esp->clen, iv);\n\taead_request_set_ad(req, assoclen);\n\n\tmemset(iv, 0, ivlen);\n\tmemcpy(iv + ivlen - min(ivlen, 8), (u8 *)&esp->seqno + 8 - min(ivlen, 8),\n\t       min(ivlen, 8));\n\n\tESP_SKB_CB(skb)->tmp = tmp;\n\terr = crypto_aead_encrypt(req);\n\n\tswitch (err) {\n\tcase -EINPROGRESS:\n\t\tgoto error;\n\n\tcase -ENOSPC:\n\t\terr = NET_XMIT_DROP;\n\t\tbreak;\n\n\tcase 0:\n\t\tif ((x->props.flags & XFRM_STATE_ESN))\n\t\t\tesp_output_restore_header(skb);\n\t}\n\n\tif (sg != dsg)\n\t\tesp_ssg_unref(x, tmp);\n\n\tif (!err && x->encap && x->encap->encap_type == TCP_ENCAP_ESPINTCP)\n\t\terr = esp_output_tail_tcp(x, skb);\n\nerror_free:\n\tkfree(tmp);\nerror:\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(esp_output_tail);\n\nstatic int esp_output(struct xfrm_state *x, struct sk_buff *skb)\n{\n\tint alen;\n\tint blksize;\n\tstruct ip_esp_hdr *esph;\n\tstruct crypto_aead *aead;\n\tstruct esp_info esp;\n\n\tesp.inplace = true;\n\n\tesp.proto = *skb_mac_header(skb);\n\t*skb_mac_header(skb) = IPPROTO_ESP;\n\n\t \n\n\taead = x->data;\n\talen = crypto_aead_authsize(aead);\n\n\tesp.tfclen = 0;\n\tif (x->tfcpad) {\n\t\tstruct xfrm_dst *dst = (struct xfrm_dst *)skb_dst(skb);\n\t\tu32 padto;\n\n\t\tpadto = min(x->tfcpad, xfrm_state_mtu(x, dst->child_mtu_cached));\n\t\tif (skb->len < padto)\n\t\t\tesp.tfclen = padto - skb->len;\n\t}\n\tblksize = ALIGN(crypto_aead_blocksize(aead), 4);\n\tesp.clen = ALIGN(skb->len + 2 + esp.tfclen, blksize);\n\tesp.plen = esp.clen - skb->len - esp.tfclen;\n\tesp.tailen = esp.tfclen + esp.plen + alen;\n\n\tesp.esph = ip_esp_hdr(skb);\n\n\tesp.nfrags = esp_output_head(x, skb, &esp);\n\tif (esp.nfrags < 0)\n\t\treturn esp.nfrags;\n\n\tesph = esp.esph;\n\tesph->spi = x->id.spi;\n\n\tesph->seq_no = htonl(XFRM_SKB_CB(skb)->seq.output.low);\n\tesp.seqno = cpu_to_be64(XFRM_SKB_CB(skb)->seq.output.low +\n\t\t\t\t ((u64)XFRM_SKB_CB(skb)->seq.output.hi << 32));\n\n\tskb_push(skb, -skb_network_offset(skb));\n\n\treturn esp_output_tail(x, skb, &esp);\n}\n\nstatic inline int esp_remove_trailer(struct sk_buff *skb)\n{\n\tstruct xfrm_state *x = xfrm_input_state(skb);\n\tstruct crypto_aead *aead = x->data;\n\tint alen, hlen, elen;\n\tint padlen, trimlen;\n\t__wsum csumdiff;\n\tu8 nexthdr[2];\n\tint ret;\n\n\talen = crypto_aead_authsize(aead);\n\thlen = sizeof(struct ip_esp_hdr) + crypto_aead_ivsize(aead);\n\telen = skb->len - hlen;\n\n\tif (skb_copy_bits(skb, skb->len - alen - 2, nexthdr, 2))\n\t\tBUG();\n\n\tret = -EINVAL;\n\tpadlen = nexthdr[0];\n\tif (padlen + 2 + alen >= elen) {\n\t\tnet_dbg_ratelimited(\"ipsec esp packet is garbage padlen=%d, elen=%d\\n\",\n\t\t\t\t    padlen + 2, elen - alen);\n\t\tgoto out;\n\t}\n\n\ttrimlen = alen + padlen + 2;\n\tif (skb->ip_summed == CHECKSUM_COMPLETE) {\n\t\tcsumdiff = skb_checksum(skb, skb->len - trimlen, trimlen, 0);\n\t\tskb->csum = csum_block_sub(skb->csum, csumdiff,\n\t\t\t\t\t   skb->len - trimlen);\n\t}\n\tret = pskb_trim(skb, skb->len - trimlen);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tret = nexthdr[1];\n\nout:\n\treturn ret;\n}\n\nint esp_input_done2(struct sk_buff *skb, int err)\n{\n\tconst struct iphdr *iph;\n\tstruct xfrm_state *x = xfrm_input_state(skb);\n\tstruct xfrm_offload *xo = xfrm_offload(skb);\n\tstruct crypto_aead *aead = x->data;\n\tint hlen = sizeof(struct ip_esp_hdr) + crypto_aead_ivsize(aead);\n\tint ihl;\n\n\tif (!xo || !(xo->flags & CRYPTO_DONE))\n\t\tkfree(ESP_SKB_CB(skb)->tmp);\n\n\tif (unlikely(err))\n\t\tgoto out;\n\n\terr = esp_remove_trailer(skb);\n\tif (unlikely(err < 0))\n\t\tgoto out;\n\n\tiph = ip_hdr(skb);\n\tihl = iph->ihl * 4;\n\n\tif (x->encap) {\n\t\tstruct xfrm_encap_tmpl *encap = x->encap;\n\t\tstruct tcphdr *th = (void *)(skb_network_header(skb) + ihl);\n\t\tstruct udphdr *uh = (void *)(skb_network_header(skb) + ihl);\n\t\t__be16 source;\n\n\t\tswitch (x->encap->encap_type) {\n\t\tcase TCP_ENCAP_ESPINTCP:\n\t\t\tsource = th->source;\n\t\t\tbreak;\n\t\tcase UDP_ENCAP_ESPINUDP:\n\t\tcase UDP_ENCAP_ESPINUDP_NON_IKE:\n\t\t\tsource = uh->source;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tWARN_ON_ONCE(1);\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\t \n\t\tif (iph->saddr != x->props.saddr.a4 ||\n\t\t    source != encap->encap_sport) {\n\t\t\txfrm_address_t ipaddr;\n\n\t\t\tipaddr.a4 = iph->saddr;\n\t\t\tkm_new_mapping(x, &ipaddr, source);\n\n\t\t\t \n\t\t}\n\n\t\t \n\t\tif (x->props.mode == XFRM_MODE_TRANSPORT)\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t}\n\n\tskb_pull_rcsum(skb, hlen);\n\tif (x->props.mode == XFRM_MODE_TUNNEL)\n\t\tskb_reset_transport_header(skb);\n\telse\n\t\tskb_set_transport_header(skb, -ihl);\n\n\t \n\tif (err == IPPROTO_NONE)\n\t\terr = -EINVAL;\n\nout:\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(esp_input_done2);\n\nstatic void esp_input_done(void *data, int err)\n{\n\tstruct sk_buff *skb = data;\n\n\txfrm_input_resume(skb, esp_input_done2(skb, err));\n}\n\nstatic void esp_input_restore_header(struct sk_buff *skb)\n{\n\tesp_restore_header(skb, 0);\n\t__skb_pull(skb, 4);\n}\n\nstatic void esp_input_set_header(struct sk_buff *skb, __be32 *seqhi)\n{\n\tstruct xfrm_state *x = xfrm_input_state(skb);\n\tstruct ip_esp_hdr *esph;\n\n\t \n\tif ((x->props.flags & XFRM_STATE_ESN)) {\n\t\tesph = skb_push(skb, 4);\n\t\t*seqhi = esph->spi;\n\t\tesph->spi = esph->seq_no;\n\t\tesph->seq_no = XFRM_SKB_CB(skb)->seq.input.hi;\n\t}\n}\n\nstatic void esp_input_done_esn(void *data, int err)\n{\n\tstruct sk_buff *skb = data;\n\n\tesp_input_restore_header(skb);\n\tesp_input_done(data, err);\n}\n\n \nstatic int esp_input(struct xfrm_state *x, struct sk_buff *skb)\n{\n\tstruct crypto_aead *aead = x->data;\n\tstruct aead_request *req;\n\tstruct sk_buff *trailer;\n\tint ivlen = crypto_aead_ivsize(aead);\n\tint elen = skb->len - sizeof(struct ip_esp_hdr) - ivlen;\n\tint nfrags;\n\tint assoclen;\n\tint seqhilen;\n\t__be32 *seqhi;\n\tvoid *tmp;\n\tu8 *iv;\n\tstruct scatterlist *sg;\n\tint err = -EINVAL;\n\n\tif (!pskb_may_pull(skb, sizeof(struct ip_esp_hdr) + ivlen))\n\t\tgoto out;\n\n\tif (elen <= 0)\n\t\tgoto out;\n\n\tassoclen = sizeof(struct ip_esp_hdr);\n\tseqhilen = 0;\n\n\tif (x->props.flags & XFRM_STATE_ESN) {\n\t\tseqhilen += sizeof(__be32);\n\t\tassoclen += seqhilen;\n\t}\n\n\tif (!skb_cloned(skb)) {\n\t\tif (!skb_is_nonlinear(skb)) {\n\t\t\tnfrags = 1;\n\n\t\t\tgoto skip_cow;\n\t\t} else if (!skb_has_frag_list(skb)) {\n\t\t\tnfrags = skb_shinfo(skb)->nr_frags;\n\t\t\tnfrags++;\n\n\t\t\tgoto skip_cow;\n\t\t}\n\t}\n\n\terr = skb_cow_data(skb, 0, &trailer);\n\tif (err < 0)\n\t\tgoto out;\n\n\tnfrags = err;\n\nskip_cow:\n\terr = -ENOMEM;\n\ttmp = esp_alloc_tmp(aead, nfrags, seqhilen);\n\tif (!tmp)\n\t\tgoto out;\n\n\tESP_SKB_CB(skb)->tmp = tmp;\n\tseqhi = esp_tmp_extra(tmp);\n\tiv = esp_tmp_iv(aead, tmp, seqhilen);\n\treq = esp_tmp_req(aead, iv);\n\tsg = esp_req_sg(aead, req);\n\n\tesp_input_set_header(skb, seqhi);\n\n\tsg_init_table(sg, nfrags);\n\terr = skb_to_sgvec(skb, sg, 0, skb->len);\n\tif (unlikely(err < 0)) {\n\t\tkfree(tmp);\n\t\tgoto out;\n\t}\n\n\tskb->ip_summed = CHECKSUM_NONE;\n\n\tif ((x->props.flags & XFRM_STATE_ESN))\n\t\taead_request_set_callback(req, 0, esp_input_done_esn, skb);\n\telse\n\t\taead_request_set_callback(req, 0, esp_input_done, skb);\n\n\taead_request_set_crypt(req, sg, sg, elen + ivlen, iv);\n\taead_request_set_ad(req, assoclen);\n\n\terr = crypto_aead_decrypt(req);\n\tif (err == -EINPROGRESS)\n\t\tgoto out;\n\n\tif ((x->props.flags & XFRM_STATE_ESN))\n\t\tesp_input_restore_header(skb);\n\n\terr = esp_input_done2(skb, err);\n\nout:\n\treturn err;\n}\n\nstatic int esp4_err(struct sk_buff *skb, u32 info)\n{\n\tstruct net *net = dev_net(skb->dev);\n\tconst struct iphdr *iph = (const struct iphdr *)skb->data;\n\tstruct ip_esp_hdr *esph = (struct ip_esp_hdr *)(skb->data+(iph->ihl<<2));\n\tstruct xfrm_state *x;\n\n\tswitch (icmp_hdr(skb)->type) {\n\tcase ICMP_DEST_UNREACH:\n\t\tif (icmp_hdr(skb)->code != ICMP_FRAG_NEEDED)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase ICMP_REDIRECT:\n\t\tbreak;\n\tdefault:\n\t\treturn 0;\n\t}\n\n\tx = xfrm_state_lookup(net, skb->mark, (const xfrm_address_t *)&iph->daddr,\n\t\t\t      esph->spi, IPPROTO_ESP, AF_INET);\n\tif (!x)\n\t\treturn 0;\n\n\tif (icmp_hdr(skb)->type == ICMP_DEST_UNREACH)\n\t\tipv4_update_pmtu(skb, net, info, 0, IPPROTO_ESP);\n\telse\n\t\tipv4_redirect(skb, net, 0, IPPROTO_ESP);\n\txfrm_state_put(x);\n\n\treturn 0;\n}\n\nstatic void esp_destroy(struct xfrm_state *x)\n{\n\tstruct crypto_aead *aead = x->data;\n\n\tif (!aead)\n\t\treturn;\n\n\tcrypto_free_aead(aead);\n}\n\nstatic int esp_init_aead(struct xfrm_state *x, struct netlink_ext_ack *extack)\n{\n\tchar aead_name[CRYPTO_MAX_ALG_NAME];\n\tstruct crypto_aead *aead;\n\tint err;\n\n\tif (snprintf(aead_name, CRYPTO_MAX_ALG_NAME, \"%s(%s)\",\n\t\t     x->geniv, x->aead->alg_name) >= CRYPTO_MAX_ALG_NAME) {\n\t\tNL_SET_ERR_MSG(extack, \"Algorithm name is too long\");\n\t\treturn -ENAMETOOLONG;\n\t}\n\n\taead = crypto_alloc_aead(aead_name, 0, 0);\n\terr = PTR_ERR(aead);\n\tif (IS_ERR(aead))\n\t\tgoto error;\n\n\tx->data = aead;\n\n\terr = crypto_aead_setkey(aead, x->aead->alg_key,\n\t\t\t\t (x->aead->alg_key_len + 7) / 8);\n\tif (err)\n\t\tgoto error;\n\n\terr = crypto_aead_setauthsize(aead, x->aead->alg_icv_len / 8);\n\tif (err)\n\t\tgoto error;\n\n\treturn 0;\n\nerror:\n\tNL_SET_ERR_MSG(extack, \"Kernel was unable to initialize cryptographic operations\");\n\treturn err;\n}\n\nstatic int esp_init_authenc(struct xfrm_state *x,\n\t\t\t    struct netlink_ext_ack *extack)\n{\n\tstruct crypto_aead *aead;\n\tstruct crypto_authenc_key_param *param;\n\tstruct rtattr *rta;\n\tchar *key;\n\tchar *p;\n\tchar authenc_name[CRYPTO_MAX_ALG_NAME];\n\tunsigned int keylen;\n\tint err;\n\n\terr = -ENAMETOOLONG;\n\n\tif ((x->props.flags & XFRM_STATE_ESN)) {\n\t\tif (snprintf(authenc_name, CRYPTO_MAX_ALG_NAME,\n\t\t\t     \"%s%sauthencesn(%s,%s)%s\",\n\t\t\t     x->geniv ?: \"\", x->geniv ? \"(\" : \"\",\n\t\t\t     x->aalg ? x->aalg->alg_name : \"digest_null\",\n\t\t\t     x->ealg->alg_name,\n\t\t\t     x->geniv ? \")\" : \"\") >= CRYPTO_MAX_ALG_NAME) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Algorithm name is too long\");\n\t\t\tgoto error;\n\t\t}\n\t} else {\n\t\tif (snprintf(authenc_name, CRYPTO_MAX_ALG_NAME,\n\t\t\t     \"%s%sauthenc(%s,%s)%s\",\n\t\t\t     x->geniv ?: \"\", x->geniv ? \"(\" : \"\",\n\t\t\t     x->aalg ? x->aalg->alg_name : \"digest_null\",\n\t\t\t     x->ealg->alg_name,\n\t\t\t     x->geniv ? \")\" : \"\") >= CRYPTO_MAX_ALG_NAME) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Algorithm name is too long\");\n\t\t\tgoto error;\n\t\t}\n\t}\n\n\taead = crypto_alloc_aead(authenc_name, 0, 0);\n\terr = PTR_ERR(aead);\n\tif (IS_ERR(aead)) {\n\t\tNL_SET_ERR_MSG(extack, \"Kernel was unable to initialize cryptographic operations\");\n\t\tgoto error;\n\t}\n\n\tx->data = aead;\n\n\tkeylen = (x->aalg ? (x->aalg->alg_key_len + 7) / 8 : 0) +\n\t\t (x->ealg->alg_key_len + 7) / 8 + RTA_SPACE(sizeof(*param));\n\terr = -ENOMEM;\n\tkey = kmalloc(keylen, GFP_KERNEL);\n\tif (!key)\n\t\tgoto error;\n\n\tp = key;\n\trta = (void *)p;\n\trta->rta_type = CRYPTO_AUTHENC_KEYA_PARAM;\n\trta->rta_len = RTA_LENGTH(sizeof(*param));\n\tparam = RTA_DATA(rta);\n\tp += RTA_SPACE(sizeof(*param));\n\n\tif (x->aalg) {\n\t\tstruct xfrm_algo_desc *aalg_desc;\n\n\t\tmemcpy(p, x->aalg->alg_key, (x->aalg->alg_key_len + 7) / 8);\n\t\tp += (x->aalg->alg_key_len + 7) / 8;\n\n\t\taalg_desc = xfrm_aalg_get_byname(x->aalg->alg_name, 0);\n\t\tBUG_ON(!aalg_desc);\n\n\t\terr = -EINVAL;\n\t\tif (aalg_desc->uinfo.auth.icv_fullbits / 8 !=\n\t\t    crypto_aead_authsize(aead)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Kernel was unable to initialize cryptographic operations\");\n\t\t\tgoto free_key;\n\t\t}\n\n\t\terr = crypto_aead_setauthsize(\n\t\t\taead, x->aalg->alg_trunc_len / 8);\n\t\tif (err) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Kernel was unable to initialize cryptographic operations\");\n\t\t\tgoto free_key;\n\t\t}\n\t}\n\n\tparam->enckeylen = cpu_to_be32((x->ealg->alg_key_len + 7) / 8);\n\tmemcpy(p, x->ealg->alg_key, (x->ealg->alg_key_len + 7) / 8);\n\n\terr = crypto_aead_setkey(aead, key, keylen);\n\nfree_key:\n\tkfree_sensitive(key);\n\nerror:\n\treturn err;\n}\n\nstatic int esp_init_state(struct xfrm_state *x, struct netlink_ext_ack *extack)\n{\n\tstruct crypto_aead *aead;\n\tu32 align;\n\tint err;\n\n\tx->data = NULL;\n\n\tif (x->aead) {\n\t\terr = esp_init_aead(x, extack);\n\t} else if (x->ealg) {\n\t\terr = esp_init_authenc(x, extack);\n\t} else {\n\t\tNL_SET_ERR_MSG(extack, \"ESP: AEAD or CRYPT must be provided\");\n\t\terr = -EINVAL;\n\t}\n\n\tif (err)\n\t\tgoto error;\n\n\taead = x->data;\n\n\tx->props.header_len = sizeof(struct ip_esp_hdr) +\n\t\t\t      crypto_aead_ivsize(aead);\n\tif (x->props.mode == XFRM_MODE_TUNNEL)\n\t\tx->props.header_len += sizeof(struct iphdr);\n\telse if (x->props.mode == XFRM_MODE_BEET && x->sel.family != AF_INET6)\n\t\tx->props.header_len += IPV4_BEET_PHMAXLEN;\n\tif (x->encap) {\n\t\tstruct xfrm_encap_tmpl *encap = x->encap;\n\n\t\tswitch (encap->encap_type) {\n\t\tdefault:\n\t\t\tNL_SET_ERR_MSG(extack, \"Unsupported encapsulation type for ESP\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto error;\n\t\tcase UDP_ENCAP_ESPINUDP:\n\t\t\tx->props.header_len += sizeof(struct udphdr);\n\t\t\tbreak;\n\t\tcase UDP_ENCAP_ESPINUDP_NON_IKE:\n\t\t\tx->props.header_len += sizeof(struct udphdr) + 2 * sizeof(u32);\n\t\t\tbreak;\n#ifdef CONFIG_INET_ESPINTCP\n\t\tcase TCP_ENCAP_ESPINTCP:\n\t\t\t \n\t\t\tx->props.header_len += 2;\n\t\t\tbreak;\n#endif\n\t\t}\n\t}\n\n\talign = ALIGN(crypto_aead_blocksize(aead), 4);\n\tx->props.trailer_len = align + 1 + crypto_aead_authsize(aead);\n\nerror:\n\treturn err;\n}\n\nstatic int esp4_rcv_cb(struct sk_buff *skb, int err)\n{\n\treturn 0;\n}\n\nstatic const struct xfrm_type esp_type =\n{\n\t.owner\t\t= THIS_MODULE,\n\t.proto\t     \t= IPPROTO_ESP,\n\t.flags\t\t= XFRM_TYPE_REPLAY_PROT,\n\t.init_state\t= esp_init_state,\n\t.destructor\t= esp_destroy,\n\t.input\t\t= esp_input,\n\t.output\t\t= esp_output,\n};\n\nstatic struct xfrm4_protocol esp4_protocol = {\n\t.handler\t=\txfrm4_rcv,\n\t.input_handler\t=\txfrm_input,\n\t.cb_handler\t=\tesp4_rcv_cb,\n\t.err_handler\t=\tesp4_err,\n\t.priority\t=\t0,\n};\n\nstatic int __init esp4_init(void)\n{\n\tif (xfrm_register_type(&esp_type, AF_INET) < 0) {\n\t\tpr_info(\"%s: can't add xfrm type\\n\", __func__);\n\t\treturn -EAGAIN;\n\t}\n\tif (xfrm4_protocol_register(&esp4_protocol, IPPROTO_ESP) < 0) {\n\t\tpr_info(\"%s: can't add protocol\\n\", __func__);\n\t\txfrm_unregister_type(&esp_type, AF_INET);\n\t\treturn -EAGAIN;\n\t}\n\treturn 0;\n}\n\nstatic void __exit esp4_fini(void)\n{\n\tif (xfrm4_protocol_deregister(&esp4_protocol, IPPROTO_ESP) < 0)\n\t\tpr_info(\"%s: can't remove protocol\\n\", __func__);\n\txfrm_unregister_type(&esp_type, AF_INET);\n}\n\nmodule_init(esp4_init);\nmodule_exit(esp4_fini);\nMODULE_LICENSE(\"GPL\");\nMODULE_ALIAS_XFRM_TYPE(AF_INET, XFRM_PROTO_ESP);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}