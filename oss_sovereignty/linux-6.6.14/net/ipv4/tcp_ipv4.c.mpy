{
  "module_name": "tcp_ipv4.c",
  "hash_id": "7ec8b8a2c799f49cb2dc1b4c288d924fdb6ced419c636aed67478566363c1495",
  "original_prompt": "Ingested from linux-6.6.14/net/ipv4/tcp_ipv4.c",
  "human_readable_source": "\n \n\n \n\n#define pr_fmt(fmt) \"TCP: \" fmt\n\n#include <linux/bottom_half.h>\n#include <linux/types.h>\n#include <linux/fcntl.h>\n#include <linux/module.h>\n#include <linux/random.h>\n#include <linux/cache.h>\n#include <linux/jhash.h>\n#include <linux/init.h>\n#include <linux/times.h>\n#include <linux/slab.h>\n#include <linux/sched.h>\n\n#include <net/net_namespace.h>\n#include <net/icmp.h>\n#include <net/inet_hashtables.h>\n#include <net/tcp.h>\n#include <net/transp_v6.h>\n#include <net/ipv6.h>\n#include <net/inet_common.h>\n#include <net/timewait_sock.h>\n#include <net/xfrm.h>\n#include <net/secure_seq.h>\n#include <net/busy_poll.h>\n\n#include <linux/inet.h>\n#include <linux/ipv6.h>\n#include <linux/stddef.h>\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <linux/inetdevice.h>\n#include <linux/btf_ids.h>\n\n#include <crypto/hash.h>\n#include <linux/scatterlist.h>\n\n#include <trace/events/tcp.h>\n\n#ifdef CONFIG_TCP_MD5SIG\nstatic int tcp_v4_md5_hash_hdr(char *md5_hash, const struct tcp_md5sig_key *key,\n\t\t\t       __be32 daddr, __be32 saddr, const struct tcphdr *th);\n#endif\n\nstruct inet_hashinfo tcp_hashinfo;\nEXPORT_SYMBOL(tcp_hashinfo);\n\nstatic DEFINE_PER_CPU(struct sock *, ipv4_tcp_sk);\n\nstatic u32 tcp_v4_init_seq(const struct sk_buff *skb)\n{\n\treturn secure_tcp_seq(ip_hdr(skb)->daddr,\n\t\t\t      ip_hdr(skb)->saddr,\n\t\t\t      tcp_hdr(skb)->dest,\n\t\t\t      tcp_hdr(skb)->source);\n}\n\nstatic u32 tcp_v4_init_ts_off(const struct net *net, const struct sk_buff *skb)\n{\n\treturn secure_tcp_ts_off(net, ip_hdr(skb)->daddr, ip_hdr(skb)->saddr);\n}\n\nint tcp_twsk_unique(struct sock *sk, struct sock *sktw, void *twp)\n{\n\tint reuse = READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_tw_reuse);\n\tconst struct inet_timewait_sock *tw = inet_twsk(sktw);\n\tconst struct tcp_timewait_sock *tcptw = tcp_twsk(sktw);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (reuse == 2) {\n\t\t \n\t\tbool loopback = false;\n\t\tif (tw->tw_bound_dev_if == LOOPBACK_IFINDEX)\n\t\t\tloopback = true;\n#if IS_ENABLED(CONFIG_IPV6)\n\t\tif (tw->tw_family == AF_INET6) {\n\t\t\tif (ipv6_addr_loopback(&tw->tw_v6_daddr) ||\n\t\t\t    ipv6_addr_v4mapped_loopback(&tw->tw_v6_daddr) ||\n\t\t\t    ipv6_addr_loopback(&tw->tw_v6_rcv_saddr) ||\n\t\t\t    ipv6_addr_v4mapped_loopback(&tw->tw_v6_rcv_saddr))\n\t\t\t\tloopback = true;\n\t\t} else\n#endif\n\t\t{\n\t\t\tif (ipv4_is_loopback(tw->tw_daddr) ||\n\t\t\t    ipv4_is_loopback(tw->tw_rcv_saddr))\n\t\t\t\tloopback = true;\n\t\t}\n\t\tif (!loopback)\n\t\t\treuse = 0;\n\t}\n\n\t \n\tif (tcptw->tw_ts_recent_stamp &&\n\t    (!twp || (reuse && time_after32(ktime_get_seconds(),\n\t\t\t\t\t    tcptw->tw_ts_recent_stamp)))) {\n\t\t \n\t\tif (likely(!tp->repair)) {\n\t\t\tu32 seq = tcptw->tw_snd_nxt + 65535 + 2;\n\n\t\t\tif (!seq)\n\t\t\t\tseq = 1;\n\t\t\tWRITE_ONCE(tp->write_seq, seq);\n\t\t\ttp->rx_opt.ts_recent\t   = tcptw->tw_ts_recent;\n\t\t\ttp->rx_opt.ts_recent_stamp = tcptw->tw_ts_recent_stamp;\n\t\t}\n\t\tsock_hold(sktw);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(tcp_twsk_unique);\n\nstatic int tcp_v4_pre_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t      int addr_len)\n{\n\t \n\tif (addr_len < sizeof(struct sockaddr_in))\n\t\treturn -EINVAL;\n\n\tsock_owned_by_me(sk);\n\n\treturn BPF_CGROUP_RUN_PROG_INET4_CONNECT(sk, uaddr);\n}\n\n \nint tcp_v4_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_in *usin = (struct sockaddr_in *)uaddr;\n\tstruct inet_timewait_death_row *tcp_death_row;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct ip_options_rcu *inet_opt;\n\tstruct net *net = sock_net(sk);\n\t__be16 orig_sport, orig_dport;\n\t__be32 daddr, nexthop;\n\tstruct flowi4 *fl4;\n\tstruct rtable *rt;\n\tint err;\n\n\tif (addr_len < sizeof(struct sockaddr_in))\n\t\treturn -EINVAL;\n\n\tif (usin->sin_family != AF_INET)\n\t\treturn -EAFNOSUPPORT;\n\n\tnexthop = daddr = usin->sin_addr.s_addr;\n\tinet_opt = rcu_dereference_protected(inet->inet_opt,\n\t\t\t\t\t     lockdep_sock_is_held(sk));\n\tif (inet_opt && inet_opt->opt.srr) {\n\t\tif (!daddr)\n\t\t\treturn -EINVAL;\n\t\tnexthop = inet_opt->opt.faddr;\n\t}\n\n\torig_sport = inet->inet_sport;\n\torig_dport = usin->sin_port;\n\tfl4 = &inet->cork.fl.u.ip4;\n\trt = ip_route_connect(fl4, nexthop, inet->inet_saddr,\n\t\t\t      sk->sk_bound_dev_if, IPPROTO_TCP, orig_sport,\n\t\t\t      orig_dport, sk);\n\tif (IS_ERR(rt)) {\n\t\terr = PTR_ERR(rt);\n\t\tif (err == -ENETUNREACH)\n\t\t\tIP_INC_STATS(net, IPSTATS_MIB_OUTNOROUTES);\n\t\treturn err;\n\t}\n\n\tif (rt->rt_flags & (RTCF_MULTICAST | RTCF_BROADCAST)) {\n\t\tip_rt_put(rt);\n\t\treturn -ENETUNREACH;\n\t}\n\n\tif (!inet_opt || !inet_opt->opt.srr)\n\t\tdaddr = fl4->daddr;\n\n\ttcp_death_row = &sock_net(sk)->ipv4.tcp_death_row;\n\n\tif (!inet->inet_saddr) {\n\t\terr = inet_bhash2_update_saddr(sk,  &fl4->saddr, AF_INET);\n\t\tif (err) {\n\t\t\tip_rt_put(rt);\n\t\t\treturn err;\n\t\t}\n\t} else {\n\t\tsk_rcv_saddr_set(sk, inet->inet_saddr);\n\t}\n\n\tif (tp->rx_opt.ts_recent_stamp && inet->inet_daddr != daddr) {\n\t\t \n\t\ttp->rx_opt.ts_recent\t   = 0;\n\t\ttp->rx_opt.ts_recent_stamp = 0;\n\t\tif (likely(!tp->repair))\n\t\t\tWRITE_ONCE(tp->write_seq, 0);\n\t}\n\n\tinet->inet_dport = usin->sin_port;\n\tsk_daddr_set(sk, daddr);\n\n\tinet_csk(sk)->icsk_ext_hdr_len = 0;\n\tif (inet_opt)\n\t\tinet_csk(sk)->icsk_ext_hdr_len = inet_opt->opt.optlen;\n\n\ttp->rx_opt.mss_clamp = TCP_MSS_DEFAULT;\n\n\t \n\ttcp_set_state(sk, TCP_SYN_SENT);\n\terr = inet_hash_connect(tcp_death_row, sk);\n\tif (err)\n\t\tgoto failure;\n\n\tsk_set_txhash(sk);\n\n\trt = ip_route_newports(fl4, rt, orig_sport, orig_dport,\n\t\t\t       inet->inet_sport, inet->inet_dport, sk);\n\tif (IS_ERR(rt)) {\n\t\terr = PTR_ERR(rt);\n\t\trt = NULL;\n\t\tgoto failure;\n\t}\n\t \n\tsk->sk_gso_type = SKB_GSO_TCPV4;\n\tsk_setup_caps(sk, &rt->dst);\n\trt = NULL;\n\n\tif (likely(!tp->repair)) {\n\t\tif (!tp->write_seq)\n\t\t\tWRITE_ONCE(tp->write_seq,\n\t\t\t\t   secure_tcp_seq(inet->inet_saddr,\n\t\t\t\t\t\t  inet->inet_daddr,\n\t\t\t\t\t\t  inet->inet_sport,\n\t\t\t\t\t\t  usin->sin_port));\n\t\tWRITE_ONCE(tp->tsoffset,\n\t\t\t   secure_tcp_ts_off(net, inet->inet_saddr,\n\t\t\t\t\t     inet->inet_daddr));\n\t}\n\n\tatomic_set(&inet->inet_id, get_random_u16());\n\n\tif (tcp_fastopen_defer_connect(sk, &err))\n\t\treturn err;\n\tif (err)\n\t\tgoto failure;\n\n\terr = tcp_connect(sk);\n\n\tif (err)\n\t\tgoto failure;\n\n\treturn 0;\n\nfailure:\n\t \n\ttcp_set_state(sk, TCP_CLOSE);\n\tinet_bhash2_reset_saddr(sk);\n\tip_rt_put(rt);\n\tsk->sk_route_caps = 0;\n\tinet->inet_dport = 0;\n\treturn err;\n}\nEXPORT_SYMBOL(tcp_v4_connect);\n\n \nvoid tcp_v4_mtu_reduced(struct sock *sk)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct dst_entry *dst;\n\tu32 mtu;\n\n\tif ((1 << sk->sk_state) & (TCPF_LISTEN | TCPF_CLOSE))\n\t\treturn;\n\tmtu = READ_ONCE(tcp_sk(sk)->mtu_info);\n\tdst = inet_csk_update_pmtu(sk, mtu);\n\tif (!dst)\n\t\treturn;\n\n\t \n\tif (mtu < dst_mtu(dst) && ip_dont_fragment(sk, dst))\n\t\tWRITE_ONCE(sk->sk_err_soft, EMSGSIZE);\n\n\tmtu = dst_mtu(dst);\n\n\tif (inet->pmtudisc != IP_PMTUDISC_DONT &&\n\t    ip_sk_accept_pmtu(sk) &&\n\t    inet_csk(sk)->icsk_pmtu_cookie > mtu) {\n\t\ttcp_sync_mss(sk, mtu);\n\n\t\t \n\t\ttcp_simple_retransmit(sk);\n\t}  \n}\nEXPORT_SYMBOL(tcp_v4_mtu_reduced);\n\nstatic void do_redirect(struct sk_buff *skb, struct sock *sk)\n{\n\tstruct dst_entry *dst = __sk_dst_check(sk, 0);\n\n\tif (dst)\n\t\tdst->ops->redirect(dst, sk, skb);\n}\n\n\n \nvoid tcp_req_err(struct sock *sk, u32 seq, bool abort)\n{\n\tstruct request_sock *req = inet_reqsk(sk);\n\tstruct net *net = sock_net(sk);\n\n\t \n\tif (seq != tcp_rsk(req)->snt_isn) {\n\t\t__NET_INC_STATS(net, LINUX_MIB_OUTOFWINDOWICMPS);\n\t} else if (abort) {\n\t\t \n\t\tinet_csk_reqsk_queue_drop(req->rsk_listener, req);\n\t\ttcp_listendrop(req->rsk_listener);\n\t}\n\treqsk_put(req);\n}\nEXPORT_SYMBOL(tcp_req_err);\n\n \nvoid tcp_ld_RTO_revert(struct sock *sk, u32 seq)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb;\n\ts32 remaining;\n\tu32 delta_us;\n\n\tif (sock_owned_by_user(sk))\n\t\treturn;\n\n\tif (seq != tp->snd_una  || !icsk->icsk_retransmits ||\n\t    !icsk->icsk_backoff)\n\t\treturn;\n\n\tskb = tcp_rtx_queue_head(sk);\n\tif (WARN_ON_ONCE(!skb))\n\t\treturn;\n\n\ticsk->icsk_backoff--;\n\ticsk->icsk_rto = tp->srtt_us ? __tcp_set_rto(tp) : TCP_TIMEOUT_INIT;\n\ticsk->icsk_rto = inet_csk_rto_backoff(icsk, TCP_RTO_MAX);\n\n\ttcp_mstamp_refresh(tp);\n\tdelta_us = (u32)(tp->tcp_mstamp - tcp_skb_timestamp_us(skb));\n\tremaining = icsk->icsk_rto - usecs_to_jiffies(delta_us);\n\n\tif (remaining > 0) {\n\t\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,\n\t\t\t\t\t  remaining, TCP_RTO_MAX);\n\t} else {\n\t\t \n\t\ttcp_retransmit_timer(sk);\n\t}\n}\nEXPORT_SYMBOL(tcp_ld_RTO_revert);\n\n \n\nint tcp_v4_err(struct sk_buff *skb, u32 info)\n{\n\tconst struct iphdr *iph = (const struct iphdr *)skb->data;\n\tstruct tcphdr *th = (struct tcphdr *)(skb->data + (iph->ihl << 2));\n\tstruct tcp_sock *tp;\n\tconst int type = icmp_hdr(skb)->type;\n\tconst int code = icmp_hdr(skb)->code;\n\tstruct sock *sk;\n\tstruct request_sock *fastopen;\n\tu32 seq, snd_una;\n\tint err;\n\tstruct net *net = dev_net(skb->dev);\n\n\tsk = __inet_lookup_established(net, net->ipv4.tcp_death_row.hashinfo,\n\t\t\t\t       iph->daddr, th->dest, iph->saddr,\n\t\t\t\t       ntohs(th->source), inet_iif(skb), 0);\n\tif (!sk) {\n\t\t__ICMP_INC_STATS(net, ICMP_MIB_INERRORS);\n\t\treturn -ENOENT;\n\t}\n\tif (sk->sk_state == TCP_TIME_WAIT) {\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\treturn 0;\n\t}\n\tseq = ntohl(th->seq);\n\tif (sk->sk_state == TCP_NEW_SYN_RECV) {\n\t\ttcp_req_err(sk, seq, type == ICMP_PARAMETERPROB ||\n\t\t\t\t     type == ICMP_TIME_EXCEEDED ||\n\t\t\t\t     (type == ICMP_DEST_UNREACH &&\n\t\t\t\t      (code == ICMP_NET_UNREACH ||\n\t\t\t\t       code == ICMP_HOST_UNREACH)));\n\t\treturn 0;\n\t}\n\n\tbh_lock_sock(sk);\n\t \n\tif (sock_owned_by_user(sk)) {\n\t\tif (!(type == ICMP_DEST_UNREACH && code == ICMP_FRAG_NEEDED))\n\t\t\t__NET_INC_STATS(net, LINUX_MIB_LOCKDROPPEDICMPS);\n\t}\n\tif (sk->sk_state == TCP_CLOSE)\n\t\tgoto out;\n\n\tif (static_branch_unlikely(&ip4_min_ttl)) {\n\t\t \n\t\tif (unlikely(iph->ttl < READ_ONCE(inet_sk(sk)->min_ttl))) {\n\t\t\t__NET_INC_STATS(net, LINUX_MIB_TCPMINTTLDROP);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\ttp = tcp_sk(sk);\n\t \n\tfastopen = rcu_dereference(tp->fastopen_rsk);\n\tsnd_una = fastopen ? tcp_rsk(fastopen)->snt_isn : tp->snd_una;\n\tif (sk->sk_state != TCP_LISTEN &&\n\t    !between(seq, snd_una, tp->snd_nxt)) {\n\t\t__NET_INC_STATS(net, LINUX_MIB_OUTOFWINDOWICMPS);\n\t\tgoto out;\n\t}\n\n\tswitch (type) {\n\tcase ICMP_REDIRECT:\n\t\tif (!sock_owned_by_user(sk))\n\t\t\tdo_redirect(skb, sk);\n\t\tgoto out;\n\tcase ICMP_SOURCE_QUENCH:\n\t\t \n\t\tgoto out;\n\tcase ICMP_PARAMETERPROB:\n\t\terr = EPROTO;\n\t\tbreak;\n\tcase ICMP_DEST_UNREACH:\n\t\tif (code > NR_ICMP_UNREACH)\n\t\t\tgoto out;\n\n\t\tif (code == ICMP_FRAG_NEEDED) {  \n\t\t\t \n\t\t\tif (sk->sk_state == TCP_LISTEN)\n\t\t\t\tgoto out;\n\n\t\t\tWRITE_ONCE(tp->mtu_info, info);\n\t\t\tif (!sock_owned_by_user(sk)) {\n\t\t\t\ttcp_v4_mtu_reduced(sk);\n\t\t\t} else {\n\t\t\t\tif (!test_and_set_bit(TCP_MTU_REDUCED_DEFERRED, &sk->sk_tsq_flags))\n\t\t\t\t\tsock_hold(sk);\n\t\t\t}\n\t\t\tgoto out;\n\t\t}\n\n\t\terr = icmp_err_convert[code].errno;\n\t\t \n\t\tif (!fastopen &&\n\t\t    (code == ICMP_NET_UNREACH || code == ICMP_HOST_UNREACH))\n\t\t\ttcp_ld_RTO_revert(sk, seq);\n\t\tbreak;\n\tcase ICMP_TIME_EXCEEDED:\n\t\terr = EHOSTUNREACH;\n\t\tbreak;\n\tdefault:\n\t\tgoto out;\n\t}\n\n\tswitch (sk->sk_state) {\n\tcase TCP_SYN_SENT:\n\tcase TCP_SYN_RECV:\n\t\t \n\t\tif (fastopen && !fastopen->sk)\n\t\t\tbreak;\n\n\t\tip_icmp_error(sk, skb, err, th->dest, info, (u8 *)th);\n\n\t\tif (!sock_owned_by_user(sk)) {\n\t\t\tWRITE_ONCE(sk->sk_err, err);\n\n\t\t\tsk_error_report(sk);\n\n\t\t\ttcp_done(sk);\n\t\t} else {\n\t\t\tWRITE_ONCE(sk->sk_err_soft, err);\n\t\t}\n\t\tgoto out;\n\t}\n\n\t \n\n\tif (!sock_owned_by_user(sk) &&\n\t    inet_test_bit(RECVERR, sk)) {\n\t\tWRITE_ONCE(sk->sk_err, err);\n\t\tsk_error_report(sk);\n\t} else\t{  \n\t\tWRITE_ONCE(sk->sk_err_soft, err);\n\t}\n\nout:\n\tbh_unlock_sock(sk);\n\tsock_put(sk);\n\treturn 0;\n}\n\nvoid __tcp_v4_send_check(struct sk_buff *skb, __be32 saddr, __be32 daddr)\n{\n\tstruct tcphdr *th = tcp_hdr(skb);\n\n\tth->check = ~tcp_v4_check(skb->len, saddr, daddr, 0);\n\tskb->csum_start = skb_transport_header(skb) - skb->head;\n\tskb->csum_offset = offsetof(struct tcphdr, check);\n}\n\n \nvoid tcp_v4_send_check(struct sock *sk, struct sk_buff *skb)\n{\n\tconst struct inet_sock *inet = inet_sk(sk);\n\n\t__tcp_v4_send_check(skb, inet->inet_saddr, inet->inet_daddr);\n}\nEXPORT_SYMBOL(tcp_v4_send_check);\n\n \n\n#ifdef CONFIG_TCP_MD5SIG\n#define OPTION_BYTES TCPOLEN_MD5SIG_ALIGNED\n#else\n#define OPTION_BYTES sizeof(__be32)\n#endif\n\nstatic void tcp_v4_send_reset(const struct sock *sk, struct sk_buff *skb)\n{\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\tstruct {\n\t\tstruct tcphdr th;\n\t\t__be32 opt[OPTION_BYTES / sizeof(__be32)];\n\t} rep;\n\tstruct ip_reply_arg arg;\n#ifdef CONFIG_TCP_MD5SIG\n\tstruct tcp_md5sig_key *key = NULL;\n\tconst __u8 *hash_location = NULL;\n\tunsigned char newhash[16];\n\tint genhash;\n\tstruct sock *sk1 = NULL;\n#endif\n\tu64 transmit_time = 0;\n\tstruct sock *ctl_sk;\n\tstruct net *net;\n\tu32 txhash = 0;\n\n\t \n\tif (th->rst)\n\t\treturn;\n\n\t \n\tif (!sk && skb_rtable(skb)->rt_type != RTN_LOCAL)\n\t\treturn;\n\n\t \n\tmemset(&rep, 0, sizeof(rep));\n\trep.th.dest   = th->source;\n\trep.th.source = th->dest;\n\trep.th.doff   = sizeof(struct tcphdr) / 4;\n\trep.th.rst    = 1;\n\n\tif (th->ack) {\n\t\trep.th.seq = th->ack_seq;\n\t} else {\n\t\trep.th.ack = 1;\n\t\trep.th.ack_seq = htonl(ntohl(th->seq) + th->syn + th->fin +\n\t\t\t\t       skb->len - (th->doff << 2));\n\t}\n\n\tmemset(&arg, 0, sizeof(arg));\n\targ.iov[0].iov_base = (unsigned char *)&rep;\n\targ.iov[0].iov_len  = sizeof(rep.th);\n\n\tnet = sk ? sock_net(sk) : dev_net(skb_dst(skb)->dev);\n#ifdef CONFIG_TCP_MD5SIG\n\trcu_read_lock();\n\thash_location = tcp_parse_md5sig_option(th);\n\tif (sk && sk_fullsock(sk)) {\n\t\tconst union tcp_md5_addr *addr;\n\t\tint l3index;\n\n\t\t \n\t\tl3index = tcp_v4_sdif(skb) ? inet_iif(skb) : 0;\n\t\taddr = (union tcp_md5_addr *)&ip_hdr(skb)->saddr;\n\t\tkey = tcp_md5_do_lookup(sk, l3index, addr, AF_INET);\n\t} else if (hash_location) {\n\t\tconst union tcp_md5_addr *addr;\n\t\tint sdif = tcp_v4_sdif(skb);\n\t\tint dif = inet_iif(skb);\n\t\tint l3index;\n\n\t\t \n\t\tsk1 = __inet_lookup_listener(net, net->ipv4.tcp_death_row.hashinfo,\n\t\t\t\t\t     NULL, 0, ip_hdr(skb)->saddr,\n\t\t\t\t\t     th->source, ip_hdr(skb)->daddr,\n\t\t\t\t\t     ntohs(th->source), dif, sdif);\n\t\t \n\t\tif (!sk1)\n\t\t\tgoto out;\n\n\t\t \n\t\tl3index = sdif ? dif : 0;\n\t\taddr = (union tcp_md5_addr *)&ip_hdr(skb)->saddr;\n\t\tkey = tcp_md5_do_lookup(sk1, l3index, addr, AF_INET);\n\t\tif (!key)\n\t\t\tgoto out;\n\n\n\t\tgenhash = tcp_v4_md5_hash_skb(newhash, key, NULL, skb);\n\t\tif (genhash || memcmp(hash_location, newhash, 16) != 0)\n\t\t\tgoto out;\n\n\t}\n\n\tif (key) {\n\t\trep.opt[0] = htonl((TCPOPT_NOP << 24) |\n\t\t\t\t   (TCPOPT_NOP << 16) |\n\t\t\t\t   (TCPOPT_MD5SIG << 8) |\n\t\t\t\t   TCPOLEN_MD5SIG);\n\t\t \n\t\targ.iov[0].iov_len += TCPOLEN_MD5SIG_ALIGNED;\n\t\trep.th.doff = arg.iov[0].iov_len / 4;\n\n\t\ttcp_v4_md5_hash_hdr((__u8 *) &rep.opt[1],\n\t\t\t\t     key, ip_hdr(skb)->saddr,\n\t\t\t\t     ip_hdr(skb)->daddr, &rep.th);\n\t}\n#endif\n\t \n\tif (rep.opt[0] == 0) {\n\t\t__be32 mrst = mptcp_reset_option(skb);\n\n\t\tif (mrst) {\n\t\t\trep.opt[0] = mrst;\n\t\t\targ.iov[0].iov_len += sizeof(mrst);\n\t\t\trep.th.doff = arg.iov[0].iov_len / 4;\n\t\t}\n\t}\n\n\targ.csum = csum_tcpudp_nofold(ip_hdr(skb)->daddr,\n\t\t\t\t      ip_hdr(skb)->saddr,  \n\t\t\t\t      arg.iov[0].iov_len, IPPROTO_TCP, 0);\n\targ.csumoffset = offsetof(struct tcphdr, check) / 2;\n\targ.flags = (sk && inet_sk_transparent(sk)) ? IP_REPLY_ARG_NOSRCCHECK : 0;\n\n\t \n\tif (sk) {\n\t\targ.bound_dev_if = sk->sk_bound_dev_if;\n\t\tif (sk_fullsock(sk))\n\t\t\ttrace_tcp_send_reset(sk, skb);\n\t}\n\n\tBUILD_BUG_ON(offsetof(struct sock, sk_bound_dev_if) !=\n\t\t     offsetof(struct inet_timewait_sock, tw_bound_dev_if));\n\n\targ.tos = ip_hdr(skb)->tos;\n\targ.uid = sock_net_uid(net, sk && sk_fullsock(sk) ? sk : NULL);\n\tlocal_bh_disable();\n\tctl_sk = this_cpu_read(ipv4_tcp_sk);\n\tsock_net_set(ctl_sk, net);\n\tif (sk) {\n\t\tctl_sk->sk_mark = (sk->sk_state == TCP_TIME_WAIT) ?\n\t\t\t\t   inet_twsk(sk)->tw_mark : sk->sk_mark;\n\t\tctl_sk->sk_priority = (sk->sk_state == TCP_TIME_WAIT) ?\n\t\t\t\t   inet_twsk(sk)->tw_priority : sk->sk_priority;\n\t\ttransmit_time = tcp_transmit_time(sk);\n\t\txfrm_sk_clone_policy(ctl_sk, sk);\n\t\ttxhash = (sk->sk_state == TCP_TIME_WAIT) ?\n\t\t\t inet_twsk(sk)->tw_txhash : sk->sk_txhash;\n\t} else {\n\t\tctl_sk->sk_mark = 0;\n\t\tctl_sk->sk_priority = 0;\n\t}\n\tip_send_unicast_reply(ctl_sk,\n\t\t\t      skb, &TCP_SKB_CB(skb)->header.h4.opt,\n\t\t\t      ip_hdr(skb)->saddr, ip_hdr(skb)->daddr,\n\t\t\t      &arg, arg.iov[0].iov_len,\n\t\t\t      transmit_time, txhash);\n\n\txfrm_sk_free_policy(ctl_sk);\n\tsock_net_set(ctl_sk, &init_net);\n\t__TCP_INC_STATS(net, TCP_MIB_OUTSEGS);\n\t__TCP_INC_STATS(net, TCP_MIB_OUTRSTS);\n\tlocal_bh_enable();\n\n#ifdef CONFIG_TCP_MD5SIG\nout:\n\trcu_read_unlock();\n#endif\n}\n\n \n\nstatic void tcp_v4_send_ack(const struct sock *sk,\n\t\t\t    struct sk_buff *skb, u32 seq, u32 ack,\n\t\t\t    u32 win, u32 tsval, u32 tsecr, int oif,\n\t\t\t    struct tcp_md5sig_key *key,\n\t\t\t    int reply_flags, u8 tos, u32 txhash)\n{\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\tstruct {\n\t\tstruct tcphdr th;\n\t\t__be32 opt[(TCPOLEN_TSTAMP_ALIGNED >> 2)\n#ifdef CONFIG_TCP_MD5SIG\n\t\t\t   + (TCPOLEN_MD5SIG_ALIGNED >> 2)\n#endif\n\t\t\t];\n\t} rep;\n\tstruct net *net = sock_net(sk);\n\tstruct ip_reply_arg arg;\n\tstruct sock *ctl_sk;\n\tu64 transmit_time;\n\n\tmemset(&rep.th, 0, sizeof(struct tcphdr));\n\tmemset(&arg, 0, sizeof(arg));\n\n\targ.iov[0].iov_base = (unsigned char *)&rep;\n\targ.iov[0].iov_len  = sizeof(rep.th);\n\tif (tsecr) {\n\t\trep.opt[0] = htonl((TCPOPT_NOP << 24) | (TCPOPT_NOP << 16) |\n\t\t\t\t   (TCPOPT_TIMESTAMP << 8) |\n\t\t\t\t   TCPOLEN_TIMESTAMP);\n\t\trep.opt[1] = htonl(tsval);\n\t\trep.opt[2] = htonl(tsecr);\n\t\targ.iov[0].iov_len += TCPOLEN_TSTAMP_ALIGNED;\n\t}\n\n\t \n\trep.th.dest    = th->source;\n\trep.th.source  = th->dest;\n\trep.th.doff    = arg.iov[0].iov_len / 4;\n\trep.th.seq     = htonl(seq);\n\trep.th.ack_seq = htonl(ack);\n\trep.th.ack     = 1;\n\trep.th.window  = htons(win);\n\n#ifdef CONFIG_TCP_MD5SIG\n\tif (key) {\n\t\tint offset = (tsecr) ? 3 : 0;\n\n\t\trep.opt[offset++] = htonl((TCPOPT_NOP << 24) |\n\t\t\t\t\t  (TCPOPT_NOP << 16) |\n\t\t\t\t\t  (TCPOPT_MD5SIG << 8) |\n\t\t\t\t\t  TCPOLEN_MD5SIG);\n\t\targ.iov[0].iov_len += TCPOLEN_MD5SIG_ALIGNED;\n\t\trep.th.doff = arg.iov[0].iov_len/4;\n\n\t\ttcp_v4_md5_hash_hdr((__u8 *) &rep.opt[offset],\n\t\t\t\t    key, ip_hdr(skb)->saddr,\n\t\t\t\t    ip_hdr(skb)->daddr, &rep.th);\n\t}\n#endif\n\targ.flags = reply_flags;\n\targ.csum = csum_tcpudp_nofold(ip_hdr(skb)->daddr,\n\t\t\t\t      ip_hdr(skb)->saddr,  \n\t\t\t\t      arg.iov[0].iov_len, IPPROTO_TCP, 0);\n\targ.csumoffset = offsetof(struct tcphdr, check) / 2;\n\tif (oif)\n\t\targ.bound_dev_if = oif;\n\targ.tos = tos;\n\targ.uid = sock_net_uid(net, sk_fullsock(sk) ? sk : NULL);\n\tlocal_bh_disable();\n\tctl_sk = this_cpu_read(ipv4_tcp_sk);\n\tsock_net_set(ctl_sk, net);\n\tctl_sk->sk_mark = (sk->sk_state == TCP_TIME_WAIT) ?\n\t\t\t   inet_twsk(sk)->tw_mark : READ_ONCE(sk->sk_mark);\n\tctl_sk->sk_priority = (sk->sk_state == TCP_TIME_WAIT) ?\n\t\t\t   inet_twsk(sk)->tw_priority : READ_ONCE(sk->sk_priority);\n\ttransmit_time = tcp_transmit_time(sk);\n\tip_send_unicast_reply(ctl_sk,\n\t\t\t      skb, &TCP_SKB_CB(skb)->header.h4.opt,\n\t\t\t      ip_hdr(skb)->saddr, ip_hdr(skb)->daddr,\n\t\t\t      &arg, arg.iov[0].iov_len,\n\t\t\t      transmit_time, txhash);\n\n\tsock_net_set(ctl_sk, &init_net);\n\t__TCP_INC_STATS(net, TCP_MIB_OUTSEGS);\n\tlocal_bh_enable();\n}\n\nstatic void tcp_v4_timewait_ack(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct inet_timewait_sock *tw = inet_twsk(sk);\n\tstruct tcp_timewait_sock *tcptw = tcp_twsk(sk);\n\n\ttcp_v4_send_ack(sk, skb,\n\t\t\ttcptw->tw_snd_nxt, tcptw->tw_rcv_nxt,\n\t\t\ttcptw->tw_rcv_wnd >> tw->tw_rcv_wscale,\n\t\t\ttcp_time_stamp_raw() + tcptw->tw_ts_offset,\n\t\t\ttcptw->tw_ts_recent,\n\t\t\ttw->tw_bound_dev_if,\n\t\t\ttcp_twsk_md5_key(tcptw),\n\t\t\ttw->tw_transparent ? IP_REPLY_ARG_NOSRCCHECK : 0,\n\t\t\ttw->tw_tos,\n\t\t\ttw->tw_txhash\n\t\t\t);\n\n\tinet_twsk_put(tw);\n}\n\nstatic void tcp_v4_reqsk_send_ack(const struct sock *sk, struct sk_buff *skb,\n\t\t\t\t  struct request_sock *req)\n{\n\tconst union tcp_md5_addr *addr;\n\tint l3index;\n\n\t \n\tu32 seq = (sk->sk_state == TCP_LISTEN) ? tcp_rsk(req)->snt_isn + 1 :\n\t\t\t\t\t     tcp_sk(sk)->snd_nxt;\n\n\t \n\taddr = (union tcp_md5_addr *)&ip_hdr(skb)->saddr;\n\tl3index = tcp_v4_sdif(skb) ? inet_iif(skb) : 0;\n\ttcp_v4_send_ack(sk, skb, seq,\n\t\t\ttcp_rsk(req)->rcv_nxt,\n\t\t\treq->rsk_rcv_wnd >> inet_rsk(req)->rcv_wscale,\n\t\t\ttcp_time_stamp_raw() + tcp_rsk(req)->ts_off,\n\t\t\tREAD_ONCE(req->ts_recent),\n\t\t\t0,\n\t\t\ttcp_md5_do_lookup(sk, l3index, addr, AF_INET),\n\t\t\tinet_rsk(req)->no_srccheck ? IP_REPLY_ARG_NOSRCCHECK : 0,\n\t\t\tip_hdr(skb)->tos,\n\t\t\tREAD_ONCE(tcp_rsk(req)->txhash));\n}\n\n \nstatic int tcp_v4_send_synack(const struct sock *sk, struct dst_entry *dst,\n\t\t\t      struct flowi *fl,\n\t\t\t      struct request_sock *req,\n\t\t\t      struct tcp_fastopen_cookie *foc,\n\t\t\t      enum tcp_synack_type synack_type,\n\t\t\t      struct sk_buff *syn_skb)\n{\n\tconst struct inet_request_sock *ireq = inet_rsk(req);\n\tstruct flowi4 fl4;\n\tint err = -1;\n\tstruct sk_buff *skb;\n\tu8 tos;\n\n\t \n\tif (!dst && (dst = inet_csk_route_req(sk, &fl4, req)) == NULL)\n\t\treturn -1;\n\n\tskb = tcp_make_synack(sk, dst, req, foc, synack_type, syn_skb);\n\n\tif (skb) {\n\t\t__tcp_v4_send_check(skb, ireq->ir_loc_addr, ireq->ir_rmt_addr);\n\n\t\ttos = READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_reflect_tos) ?\n\t\t\t\t(tcp_rsk(req)->syn_tos & ~INET_ECN_MASK) |\n\t\t\t\t(inet_sk(sk)->tos & INET_ECN_MASK) :\n\t\t\t\tinet_sk(sk)->tos;\n\n\t\tif (!INET_ECN_is_capable(tos) &&\n\t\t    tcp_bpf_ca_needs_ecn((struct sock *)req))\n\t\t\ttos |= INET_ECN_ECT_0;\n\n\t\trcu_read_lock();\n\t\terr = ip_build_and_send_pkt(skb, sk, ireq->ir_loc_addr,\n\t\t\t\t\t    ireq->ir_rmt_addr,\n\t\t\t\t\t    rcu_dereference(ireq->ireq_opt),\n\t\t\t\t\t    tos);\n\t\trcu_read_unlock();\n\t\terr = net_xmit_eval(err);\n\t}\n\n\treturn err;\n}\n\n \nstatic void tcp_v4_reqsk_destructor(struct request_sock *req)\n{\n\tkfree(rcu_dereference_protected(inet_rsk(req)->ireq_opt, 1));\n}\n\n#ifdef CONFIG_TCP_MD5SIG\n \n\nDEFINE_STATIC_KEY_DEFERRED_FALSE(tcp_md5_needed, HZ);\nEXPORT_SYMBOL(tcp_md5_needed);\n\nstatic bool better_md5_match(struct tcp_md5sig_key *old, struct tcp_md5sig_key *new)\n{\n\tif (!old)\n\t\treturn true;\n\n\t \n\tif (old->l3index && new->l3index == 0)\n\t\treturn false;\n\tif (old->l3index == 0 && new->l3index)\n\t\treturn true;\n\n\treturn old->prefixlen < new->prefixlen;\n}\n\n \nstruct tcp_md5sig_key *__tcp_md5_do_lookup(const struct sock *sk, int l3index,\n\t\t\t\t\t   const union tcp_md5_addr *addr,\n\t\t\t\t\t   int family)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tstruct tcp_md5sig_key *key;\n\tconst struct tcp_md5sig_info *md5sig;\n\t__be32 mask;\n\tstruct tcp_md5sig_key *best_match = NULL;\n\tbool match;\n\n\t \n\tmd5sig = rcu_dereference_check(tp->md5sig_info,\n\t\t\t\t       lockdep_sock_is_held(sk));\n\tif (!md5sig)\n\t\treturn NULL;\n\n\thlist_for_each_entry_rcu(key, &md5sig->head, node,\n\t\t\t\t lockdep_sock_is_held(sk)) {\n\t\tif (key->family != family)\n\t\t\tcontinue;\n\t\tif (key->flags & TCP_MD5SIG_FLAG_IFINDEX && key->l3index != l3index)\n\t\t\tcontinue;\n\t\tif (family == AF_INET) {\n\t\t\tmask = inet_make_mask(key->prefixlen);\n\t\t\tmatch = (key->addr.a4.s_addr & mask) ==\n\t\t\t\t(addr->a4.s_addr & mask);\n#if IS_ENABLED(CONFIG_IPV6)\n\t\t} else if (family == AF_INET6) {\n\t\t\tmatch = ipv6_prefix_equal(&key->addr.a6, &addr->a6,\n\t\t\t\t\t\t  key->prefixlen);\n#endif\n\t\t} else {\n\t\t\tmatch = false;\n\t\t}\n\n\t\tif (match && better_md5_match(best_match, key))\n\t\t\tbest_match = key;\n\t}\n\treturn best_match;\n}\nEXPORT_SYMBOL(__tcp_md5_do_lookup);\n\nstatic struct tcp_md5sig_key *tcp_md5_do_lookup_exact(const struct sock *sk,\n\t\t\t\t\t\t      const union tcp_md5_addr *addr,\n\t\t\t\t\t\t      int family, u8 prefixlen,\n\t\t\t\t\t\t      int l3index, u8 flags)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tstruct tcp_md5sig_key *key;\n\tunsigned int size = sizeof(struct in_addr);\n\tconst struct tcp_md5sig_info *md5sig;\n\n\t \n\tmd5sig = rcu_dereference_check(tp->md5sig_info,\n\t\t\t\t       lockdep_sock_is_held(sk));\n\tif (!md5sig)\n\t\treturn NULL;\n#if IS_ENABLED(CONFIG_IPV6)\n\tif (family == AF_INET6)\n\t\tsize = sizeof(struct in6_addr);\n#endif\n\thlist_for_each_entry_rcu(key, &md5sig->head, node,\n\t\t\t\t lockdep_sock_is_held(sk)) {\n\t\tif (key->family != family)\n\t\t\tcontinue;\n\t\tif ((key->flags & TCP_MD5SIG_FLAG_IFINDEX) != (flags & TCP_MD5SIG_FLAG_IFINDEX))\n\t\t\tcontinue;\n\t\tif (key->l3index != l3index)\n\t\t\tcontinue;\n\t\tif (!memcmp(&key->addr, addr, size) &&\n\t\t    key->prefixlen == prefixlen)\n\t\t\treturn key;\n\t}\n\treturn NULL;\n}\n\nstruct tcp_md5sig_key *tcp_v4_md5_lookup(const struct sock *sk,\n\t\t\t\t\t const struct sock *addr_sk)\n{\n\tconst union tcp_md5_addr *addr;\n\tint l3index;\n\n\tl3index = l3mdev_master_ifindex_by_index(sock_net(sk),\n\t\t\t\t\t\t addr_sk->sk_bound_dev_if);\n\taddr = (const union tcp_md5_addr *)&addr_sk->sk_daddr;\n\treturn tcp_md5_do_lookup(sk, l3index, addr, AF_INET);\n}\nEXPORT_SYMBOL(tcp_v4_md5_lookup);\n\nstatic int tcp_md5sig_info_add(struct sock *sk, gfp_t gfp)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct tcp_md5sig_info *md5sig;\n\n\tmd5sig = kmalloc(sizeof(*md5sig), gfp);\n\tif (!md5sig)\n\t\treturn -ENOMEM;\n\n\tsk_gso_disable(sk);\n\tINIT_HLIST_HEAD(&md5sig->head);\n\trcu_assign_pointer(tp->md5sig_info, md5sig);\n\treturn 0;\n}\n\n \nstatic int __tcp_md5_do_add(struct sock *sk, const union tcp_md5_addr *addr,\n\t\t\t    int family, u8 prefixlen, int l3index, u8 flags,\n\t\t\t    const u8 *newkey, u8 newkeylen, gfp_t gfp)\n{\n\t \n\tstruct tcp_md5sig_key *key;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct tcp_md5sig_info *md5sig;\n\n\tkey = tcp_md5_do_lookup_exact(sk, addr, family, prefixlen, l3index, flags);\n\tif (key) {\n\t\t \n\t\tdata_race(memcpy(key->key, newkey, newkeylen));\n\n\t\t \n\t\tWRITE_ONCE(key->keylen, newkeylen);\n\n\t\treturn 0;\n\t}\n\n\tmd5sig = rcu_dereference_protected(tp->md5sig_info,\n\t\t\t\t\t   lockdep_sock_is_held(sk));\n\n\tkey = sock_kmalloc(sk, sizeof(*key), gfp | __GFP_ZERO);\n\tif (!key)\n\t\treturn -ENOMEM;\n\tif (!tcp_alloc_md5sig_pool()) {\n\t\tsock_kfree_s(sk, key, sizeof(*key));\n\t\treturn -ENOMEM;\n\t}\n\n\tmemcpy(key->key, newkey, newkeylen);\n\tkey->keylen = newkeylen;\n\tkey->family = family;\n\tkey->prefixlen = prefixlen;\n\tkey->l3index = l3index;\n\tkey->flags = flags;\n\tmemcpy(&key->addr, addr,\n\t       (IS_ENABLED(CONFIG_IPV6) && family == AF_INET6) ? sizeof(struct in6_addr) :\n\t\t\t\t\t\t\t\t sizeof(struct in_addr));\n\thlist_add_head_rcu(&key->node, &md5sig->head);\n\treturn 0;\n}\n\nint tcp_md5_do_add(struct sock *sk, const union tcp_md5_addr *addr,\n\t\t   int family, u8 prefixlen, int l3index, u8 flags,\n\t\t   const u8 *newkey, u8 newkeylen)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (!rcu_dereference_protected(tp->md5sig_info, lockdep_sock_is_held(sk))) {\n\t\tif (tcp_md5sig_info_add(sk, GFP_KERNEL))\n\t\t\treturn -ENOMEM;\n\n\t\tif (!static_branch_inc(&tcp_md5_needed.key)) {\n\t\t\tstruct tcp_md5sig_info *md5sig;\n\n\t\t\tmd5sig = rcu_dereference_protected(tp->md5sig_info, lockdep_sock_is_held(sk));\n\t\t\trcu_assign_pointer(tp->md5sig_info, NULL);\n\t\t\tkfree_rcu(md5sig, rcu);\n\t\t\treturn -EUSERS;\n\t\t}\n\t}\n\n\treturn __tcp_md5_do_add(sk, addr, family, prefixlen, l3index, flags,\n\t\t\t\tnewkey, newkeylen, GFP_KERNEL);\n}\nEXPORT_SYMBOL(tcp_md5_do_add);\n\nint tcp_md5_key_copy(struct sock *sk, const union tcp_md5_addr *addr,\n\t\t     int family, u8 prefixlen, int l3index,\n\t\t     struct tcp_md5sig_key *key)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (!rcu_dereference_protected(tp->md5sig_info, lockdep_sock_is_held(sk))) {\n\t\tif (tcp_md5sig_info_add(sk, sk_gfp_mask(sk, GFP_ATOMIC)))\n\t\t\treturn -ENOMEM;\n\n\t\tif (!static_key_fast_inc_not_disabled(&tcp_md5_needed.key.key)) {\n\t\t\tstruct tcp_md5sig_info *md5sig;\n\n\t\t\tmd5sig = rcu_dereference_protected(tp->md5sig_info, lockdep_sock_is_held(sk));\n\t\t\tnet_warn_ratelimited(\"Too many TCP-MD5 keys in the system\\n\");\n\t\t\trcu_assign_pointer(tp->md5sig_info, NULL);\n\t\t\tkfree_rcu(md5sig, rcu);\n\t\t\treturn -EUSERS;\n\t\t}\n\t}\n\n\treturn __tcp_md5_do_add(sk, addr, family, prefixlen, l3index,\n\t\t\t\tkey->flags, key->key, key->keylen,\n\t\t\t\tsk_gfp_mask(sk, GFP_ATOMIC));\n}\nEXPORT_SYMBOL(tcp_md5_key_copy);\n\nint tcp_md5_do_del(struct sock *sk, const union tcp_md5_addr *addr, int family,\n\t\t   u8 prefixlen, int l3index, u8 flags)\n{\n\tstruct tcp_md5sig_key *key;\n\n\tkey = tcp_md5_do_lookup_exact(sk, addr, family, prefixlen, l3index, flags);\n\tif (!key)\n\t\treturn -ENOENT;\n\thlist_del_rcu(&key->node);\n\tatomic_sub(sizeof(*key), &sk->sk_omem_alloc);\n\tkfree_rcu(key, rcu);\n\treturn 0;\n}\nEXPORT_SYMBOL(tcp_md5_do_del);\n\nstatic void tcp_clear_md5_list(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct tcp_md5sig_key *key;\n\tstruct hlist_node *n;\n\tstruct tcp_md5sig_info *md5sig;\n\n\tmd5sig = rcu_dereference_protected(tp->md5sig_info, 1);\n\n\thlist_for_each_entry_safe(key, n, &md5sig->head, node) {\n\t\thlist_del_rcu(&key->node);\n\t\tatomic_sub(sizeof(*key), &sk->sk_omem_alloc);\n\t\tkfree_rcu(key, rcu);\n\t}\n}\n\nstatic int tcp_v4_parse_md5_keys(struct sock *sk, int optname,\n\t\t\t\t sockptr_t optval, int optlen)\n{\n\tstruct tcp_md5sig cmd;\n\tstruct sockaddr_in *sin = (struct sockaddr_in *)&cmd.tcpm_addr;\n\tconst union tcp_md5_addr *addr;\n\tu8 prefixlen = 32;\n\tint l3index = 0;\n\tu8 flags;\n\n\tif (optlen < sizeof(cmd))\n\t\treturn -EINVAL;\n\n\tif (copy_from_sockptr(&cmd, optval, sizeof(cmd)))\n\t\treturn -EFAULT;\n\n\tif (sin->sin_family != AF_INET)\n\t\treturn -EINVAL;\n\n\tflags = cmd.tcpm_flags & TCP_MD5SIG_FLAG_IFINDEX;\n\n\tif (optname == TCP_MD5SIG_EXT &&\n\t    cmd.tcpm_flags & TCP_MD5SIG_FLAG_PREFIX) {\n\t\tprefixlen = cmd.tcpm_prefixlen;\n\t\tif (prefixlen > 32)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (optname == TCP_MD5SIG_EXT && cmd.tcpm_ifindex &&\n\t    cmd.tcpm_flags & TCP_MD5SIG_FLAG_IFINDEX) {\n\t\tstruct net_device *dev;\n\n\t\trcu_read_lock();\n\t\tdev = dev_get_by_index_rcu(sock_net(sk), cmd.tcpm_ifindex);\n\t\tif (dev && netif_is_l3_master(dev))\n\t\t\tl3index = dev->ifindex;\n\n\t\trcu_read_unlock();\n\n\t\t \n\t\tif (!dev || !l3index)\n\t\t\treturn -EINVAL;\n\t}\n\n\taddr = (union tcp_md5_addr *)&sin->sin_addr.s_addr;\n\n\tif (!cmd.tcpm_keylen)\n\t\treturn tcp_md5_do_del(sk, addr, AF_INET, prefixlen, l3index, flags);\n\n\tif (cmd.tcpm_keylen > TCP_MD5SIG_MAXKEYLEN)\n\t\treturn -EINVAL;\n\n\treturn tcp_md5_do_add(sk, addr, AF_INET, prefixlen, l3index, flags,\n\t\t\t      cmd.tcpm_key, cmd.tcpm_keylen);\n}\n\nstatic int tcp_v4_md5_hash_headers(struct tcp_md5sig_pool *hp,\n\t\t\t\t   __be32 daddr, __be32 saddr,\n\t\t\t\t   const struct tcphdr *th, int nbytes)\n{\n\tstruct tcp4_pseudohdr *bp;\n\tstruct scatterlist sg;\n\tstruct tcphdr *_th;\n\n\tbp = hp->scratch;\n\tbp->saddr = saddr;\n\tbp->daddr = daddr;\n\tbp->pad = 0;\n\tbp->protocol = IPPROTO_TCP;\n\tbp->len = cpu_to_be16(nbytes);\n\n\t_th = (struct tcphdr *)(bp + 1);\n\tmemcpy(_th, th, sizeof(*th));\n\t_th->check = 0;\n\n\tsg_init_one(&sg, bp, sizeof(*bp) + sizeof(*th));\n\tahash_request_set_crypt(hp->md5_req, &sg, NULL,\n\t\t\t\tsizeof(*bp) + sizeof(*th));\n\treturn crypto_ahash_update(hp->md5_req);\n}\n\nstatic int tcp_v4_md5_hash_hdr(char *md5_hash, const struct tcp_md5sig_key *key,\n\t\t\t       __be32 daddr, __be32 saddr, const struct tcphdr *th)\n{\n\tstruct tcp_md5sig_pool *hp;\n\tstruct ahash_request *req;\n\n\thp = tcp_get_md5sig_pool();\n\tif (!hp)\n\t\tgoto clear_hash_noput;\n\treq = hp->md5_req;\n\n\tif (crypto_ahash_init(req))\n\t\tgoto clear_hash;\n\tif (tcp_v4_md5_hash_headers(hp, daddr, saddr, th, th->doff << 2))\n\t\tgoto clear_hash;\n\tif (tcp_md5_hash_key(hp, key))\n\t\tgoto clear_hash;\n\tahash_request_set_crypt(req, NULL, md5_hash, 0);\n\tif (crypto_ahash_final(req))\n\t\tgoto clear_hash;\n\n\ttcp_put_md5sig_pool();\n\treturn 0;\n\nclear_hash:\n\ttcp_put_md5sig_pool();\nclear_hash_noput:\n\tmemset(md5_hash, 0, 16);\n\treturn 1;\n}\n\nint tcp_v4_md5_hash_skb(char *md5_hash, const struct tcp_md5sig_key *key,\n\t\t\tconst struct sock *sk,\n\t\t\tconst struct sk_buff *skb)\n{\n\tstruct tcp_md5sig_pool *hp;\n\tstruct ahash_request *req;\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\t__be32 saddr, daddr;\n\n\tif (sk) {  \n\t\tsaddr = sk->sk_rcv_saddr;\n\t\tdaddr = sk->sk_daddr;\n\t} else {\n\t\tconst struct iphdr *iph = ip_hdr(skb);\n\t\tsaddr = iph->saddr;\n\t\tdaddr = iph->daddr;\n\t}\n\n\thp = tcp_get_md5sig_pool();\n\tif (!hp)\n\t\tgoto clear_hash_noput;\n\treq = hp->md5_req;\n\n\tif (crypto_ahash_init(req))\n\t\tgoto clear_hash;\n\n\tif (tcp_v4_md5_hash_headers(hp, daddr, saddr, th, skb->len))\n\t\tgoto clear_hash;\n\tif (tcp_md5_hash_skb_data(hp, skb, th->doff << 2))\n\t\tgoto clear_hash;\n\tif (tcp_md5_hash_key(hp, key))\n\t\tgoto clear_hash;\n\tahash_request_set_crypt(req, NULL, md5_hash, 0);\n\tif (crypto_ahash_final(req))\n\t\tgoto clear_hash;\n\n\ttcp_put_md5sig_pool();\n\treturn 0;\n\nclear_hash:\n\ttcp_put_md5sig_pool();\nclear_hash_noput:\n\tmemset(md5_hash, 0, 16);\n\treturn 1;\n}\nEXPORT_SYMBOL(tcp_v4_md5_hash_skb);\n\n#endif\n\nstatic void tcp_v4_init_req(struct request_sock *req,\n\t\t\t    const struct sock *sk_listener,\n\t\t\t    struct sk_buff *skb)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tstruct net *net = sock_net(sk_listener);\n\n\tsk_rcv_saddr_set(req_to_sk(req), ip_hdr(skb)->daddr);\n\tsk_daddr_set(req_to_sk(req), ip_hdr(skb)->saddr);\n\tRCU_INIT_POINTER(ireq->ireq_opt, tcp_v4_save_options(net, skb));\n}\n\nstatic struct dst_entry *tcp_v4_route_req(const struct sock *sk,\n\t\t\t\t\t  struct sk_buff *skb,\n\t\t\t\t\t  struct flowi *fl,\n\t\t\t\t\t  struct request_sock *req)\n{\n\ttcp_v4_init_req(req, sk, skb);\n\n\tif (security_inet_conn_request(sk, skb, req))\n\t\treturn NULL;\n\n\treturn inet_csk_route_req(sk, &fl->u.ip4, req);\n}\n\nstruct request_sock_ops tcp_request_sock_ops __read_mostly = {\n\t.family\t\t=\tPF_INET,\n\t.obj_size\t=\tsizeof(struct tcp_request_sock),\n\t.rtx_syn_ack\t=\ttcp_rtx_synack,\n\t.send_ack\t=\ttcp_v4_reqsk_send_ack,\n\t.destructor\t=\ttcp_v4_reqsk_destructor,\n\t.send_reset\t=\ttcp_v4_send_reset,\n\t.syn_ack_timeout =\ttcp_syn_ack_timeout,\n};\n\nconst struct tcp_request_sock_ops tcp_request_sock_ipv4_ops = {\n\t.mss_clamp\t=\tTCP_MSS_DEFAULT,\n#ifdef CONFIG_TCP_MD5SIG\n\t.req_md5_lookup\t=\ttcp_v4_md5_lookup,\n\t.calc_md5_hash\t=\ttcp_v4_md5_hash_skb,\n#endif\n#ifdef CONFIG_SYN_COOKIES\n\t.cookie_init_seq =\tcookie_v4_init_sequence,\n#endif\n\t.route_req\t=\ttcp_v4_route_req,\n\t.init_seq\t=\ttcp_v4_init_seq,\n\t.init_ts_off\t=\ttcp_v4_init_ts_off,\n\t.send_synack\t=\ttcp_v4_send_synack,\n};\n\nint tcp_v4_conn_request(struct sock *sk, struct sk_buff *skb)\n{\n\t \n\tif (skb_rtable(skb)->rt_flags & (RTCF_BROADCAST | RTCF_MULTICAST))\n\t\tgoto drop;\n\n\treturn tcp_conn_request(&tcp_request_sock_ops,\n\t\t\t\t&tcp_request_sock_ipv4_ops, sk, skb);\n\ndrop:\n\ttcp_listendrop(sk);\n\treturn 0;\n}\nEXPORT_SYMBOL(tcp_v4_conn_request);\n\n\n \nstruct sock *tcp_v4_syn_recv_sock(const struct sock *sk, struct sk_buff *skb,\n\t\t\t\t  struct request_sock *req,\n\t\t\t\t  struct dst_entry *dst,\n\t\t\t\t  struct request_sock *req_unhash,\n\t\t\t\t  bool *own_req)\n{\n\tstruct inet_request_sock *ireq;\n\tbool found_dup_sk = false;\n\tstruct inet_sock *newinet;\n\tstruct tcp_sock *newtp;\n\tstruct sock *newsk;\n#ifdef CONFIG_TCP_MD5SIG\n\tconst union tcp_md5_addr *addr;\n\tstruct tcp_md5sig_key *key;\n\tint l3index;\n#endif\n\tstruct ip_options_rcu *inet_opt;\n\n\tif (sk_acceptq_is_full(sk))\n\t\tgoto exit_overflow;\n\n\tnewsk = tcp_create_openreq_child(sk, req, skb);\n\tif (!newsk)\n\t\tgoto exit_nonewsk;\n\n\tnewsk->sk_gso_type = SKB_GSO_TCPV4;\n\tinet_sk_rx_dst_set(newsk, skb);\n\n\tnewtp\t\t      = tcp_sk(newsk);\n\tnewinet\t\t      = inet_sk(newsk);\n\tireq\t\t      = inet_rsk(req);\n\tsk_daddr_set(newsk, ireq->ir_rmt_addr);\n\tsk_rcv_saddr_set(newsk, ireq->ir_loc_addr);\n\tnewsk->sk_bound_dev_if = ireq->ir_iif;\n\tnewinet->inet_saddr   = ireq->ir_loc_addr;\n\tinet_opt\t      = rcu_dereference(ireq->ireq_opt);\n\tRCU_INIT_POINTER(newinet->inet_opt, inet_opt);\n\tnewinet->mc_index     = inet_iif(skb);\n\tnewinet->mc_ttl\t      = ip_hdr(skb)->ttl;\n\tnewinet->rcv_tos      = ip_hdr(skb)->tos;\n\tinet_csk(newsk)->icsk_ext_hdr_len = 0;\n\tif (inet_opt)\n\t\tinet_csk(newsk)->icsk_ext_hdr_len = inet_opt->opt.optlen;\n\tatomic_set(&newinet->inet_id, get_random_u16());\n\n\t \n\tif (READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_reflect_tos))\n\t\tnewinet->tos = tcp_rsk(req)->syn_tos & ~INET_ECN_MASK;\n\n\tif (!dst) {\n\t\tdst = inet_csk_route_child_sock(sk, newsk, req);\n\t\tif (!dst)\n\t\t\tgoto put_and_exit;\n\t} else {\n\t\t \n\t}\n\tsk_setup_caps(newsk, dst);\n\n\ttcp_ca_openreq_child(newsk, dst);\n\n\ttcp_sync_mss(newsk, dst_mtu(dst));\n\tnewtp->advmss = tcp_mss_clamp(tcp_sk(sk), dst_metric_advmss(dst));\n\n\ttcp_initialize_rcv_mss(newsk);\n\n#ifdef CONFIG_TCP_MD5SIG\n\tl3index = l3mdev_master_ifindex_by_index(sock_net(sk), ireq->ir_iif);\n\t \n\taddr = (union tcp_md5_addr *)&newinet->inet_daddr;\n\tkey = tcp_md5_do_lookup(sk, l3index, addr, AF_INET);\n\tif (key) {\n\t\tif (tcp_md5_key_copy(newsk, addr, AF_INET, 32, l3index, key))\n\t\t\tgoto put_and_exit;\n\t\tsk_gso_disable(newsk);\n\t}\n#endif\n\n\tif (__inet_inherit_port(sk, newsk) < 0)\n\t\tgoto put_and_exit;\n\t*own_req = inet_ehash_nolisten(newsk, req_to_sk(req_unhash),\n\t\t\t\t       &found_dup_sk);\n\tif (likely(*own_req)) {\n\t\ttcp_move_syn(newtp, req);\n\t\tireq->ireq_opt = NULL;\n\t} else {\n\t\tnewinet->inet_opt = NULL;\n\n\t\tif (!req_unhash && found_dup_sk) {\n\t\t\t \n\t\t\tbh_unlock_sock(newsk);\n\t\t\tsock_put(newsk);\n\t\t\tnewsk = NULL;\n\t\t}\n\t}\n\treturn newsk;\n\nexit_overflow:\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\nexit_nonewsk:\n\tdst_release(dst);\nexit:\n\ttcp_listendrop(sk);\n\treturn NULL;\nput_and_exit:\n\tnewinet->inet_opt = NULL;\n\tinet_csk_prepare_forced_close(newsk);\n\ttcp_done(newsk);\n\tgoto exit;\n}\nEXPORT_SYMBOL(tcp_v4_syn_recv_sock);\n\nstatic struct sock *tcp_v4_cookie_check(struct sock *sk, struct sk_buff *skb)\n{\n#ifdef CONFIG_SYN_COOKIES\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\n\tif (!th->syn)\n\t\tsk = cookie_v4_check(sk, skb);\n#endif\n\treturn sk;\n}\n\nu16 tcp_v4_get_syncookie(struct sock *sk, struct iphdr *iph,\n\t\t\t struct tcphdr *th, u32 *cookie)\n{\n\tu16 mss = 0;\n#ifdef CONFIG_SYN_COOKIES\n\tmss = tcp_get_syncookie_mss(&tcp_request_sock_ops,\n\t\t\t\t    &tcp_request_sock_ipv4_ops, sk, th);\n\tif (mss) {\n\t\t*cookie = __cookie_v4_init_sequence(iph, th, &mss);\n\t\ttcp_synq_overflow(sk);\n\t}\n#endif\n\treturn mss;\n}\n\nINDIRECT_CALLABLE_DECLARE(struct dst_entry *ipv4_dst_check(struct dst_entry *,\n\t\t\t\t\t\t\t   u32));\n \nint tcp_v4_do_rcv(struct sock *sk, struct sk_buff *skb)\n{\n\tenum skb_drop_reason reason;\n\tstruct sock *rsk;\n\n\tif (sk->sk_state == TCP_ESTABLISHED) {  \n\t\tstruct dst_entry *dst;\n\n\t\tdst = rcu_dereference_protected(sk->sk_rx_dst,\n\t\t\t\t\t\tlockdep_sock_is_held(sk));\n\n\t\tsock_rps_save_rxhash(sk, skb);\n\t\tsk_mark_napi_id(sk, skb);\n\t\tif (dst) {\n\t\t\tif (sk->sk_rx_dst_ifindex != skb->skb_iif ||\n\t\t\t    !INDIRECT_CALL_1(dst->ops->check, ipv4_dst_check,\n\t\t\t\t\t     dst, 0)) {\n\t\t\t\tRCU_INIT_POINTER(sk->sk_rx_dst, NULL);\n\t\t\t\tdst_release(dst);\n\t\t\t}\n\t\t}\n\t\ttcp_rcv_established(sk, skb);\n\t\treturn 0;\n\t}\n\n\treason = SKB_DROP_REASON_NOT_SPECIFIED;\n\tif (tcp_checksum_complete(skb))\n\t\tgoto csum_err;\n\n\tif (sk->sk_state == TCP_LISTEN) {\n\t\tstruct sock *nsk = tcp_v4_cookie_check(sk, skb);\n\n\t\tif (!nsk)\n\t\t\tgoto discard;\n\t\tif (nsk != sk) {\n\t\t\tif (tcp_child_process(sk, nsk, skb)) {\n\t\t\t\trsk = nsk;\n\t\t\t\tgoto reset;\n\t\t\t}\n\t\t\treturn 0;\n\t\t}\n\t} else\n\t\tsock_rps_save_rxhash(sk, skb);\n\n\tif (tcp_rcv_state_process(sk, skb)) {\n\t\trsk = sk;\n\t\tgoto reset;\n\t}\n\treturn 0;\n\nreset:\n\ttcp_v4_send_reset(rsk, skb);\ndiscard:\n\tkfree_skb_reason(skb, reason);\n\t \n\treturn 0;\n\ncsum_err:\n\treason = SKB_DROP_REASON_TCP_CSUM;\n\ttrace_tcp_bad_csum(skb);\n\tTCP_INC_STATS(sock_net(sk), TCP_MIB_CSUMERRORS);\n\tTCP_INC_STATS(sock_net(sk), TCP_MIB_INERRS);\n\tgoto discard;\n}\nEXPORT_SYMBOL(tcp_v4_do_rcv);\n\nint tcp_v4_early_demux(struct sk_buff *skb)\n{\n\tstruct net *net = dev_net(skb->dev);\n\tconst struct iphdr *iph;\n\tconst struct tcphdr *th;\n\tstruct sock *sk;\n\n\tif (skb->pkt_type != PACKET_HOST)\n\t\treturn 0;\n\n\tif (!pskb_may_pull(skb, skb_transport_offset(skb) + sizeof(struct tcphdr)))\n\t\treturn 0;\n\n\tiph = ip_hdr(skb);\n\tth = tcp_hdr(skb);\n\n\tif (th->doff < sizeof(struct tcphdr) / 4)\n\t\treturn 0;\n\n\tsk = __inet_lookup_established(net, net->ipv4.tcp_death_row.hashinfo,\n\t\t\t\t       iph->saddr, th->source,\n\t\t\t\t       iph->daddr, ntohs(th->dest),\n\t\t\t\t       skb->skb_iif, inet_sdif(skb));\n\tif (sk) {\n\t\tskb->sk = sk;\n\t\tskb->destructor = sock_edemux;\n\t\tif (sk_fullsock(sk)) {\n\t\t\tstruct dst_entry *dst = rcu_dereference(sk->sk_rx_dst);\n\n\t\t\tif (dst)\n\t\t\t\tdst = dst_check(dst, 0);\n\t\t\tif (dst &&\n\t\t\t    sk->sk_rx_dst_ifindex == skb->skb_iif)\n\t\t\t\tskb_dst_set_noref(skb, dst);\n\t\t}\n\t}\n\treturn 0;\n}\n\nbool tcp_add_backlog(struct sock *sk, struct sk_buff *skb,\n\t\t     enum skb_drop_reason *reason)\n{\n\tu32 limit, tail_gso_size, tail_gso_segs;\n\tstruct skb_shared_info *shinfo;\n\tconst struct tcphdr *th;\n\tstruct tcphdr *thtail;\n\tstruct sk_buff *tail;\n\tunsigned int hdrlen;\n\tbool fragstolen;\n\tu32 gso_segs;\n\tu32 gso_size;\n\tint delta;\n\n\t \n\tskb_condense(skb);\n\n\tskb_dst_drop(skb);\n\n\tif (unlikely(tcp_checksum_complete(skb))) {\n\t\tbh_unlock_sock(sk);\n\t\ttrace_tcp_bad_csum(skb);\n\t\t*reason = SKB_DROP_REASON_TCP_CSUM;\n\t\t__TCP_INC_STATS(sock_net(sk), TCP_MIB_CSUMERRORS);\n\t\t__TCP_INC_STATS(sock_net(sk), TCP_MIB_INERRS);\n\t\treturn true;\n\t}\n\n\t \n\tth = (const struct tcphdr *)skb->data;\n\thdrlen = th->doff * 4;\n\n\ttail = sk->sk_backlog.tail;\n\tif (!tail)\n\t\tgoto no_coalesce;\n\tthtail = (struct tcphdr *)tail->data;\n\n\tif (TCP_SKB_CB(tail)->end_seq != TCP_SKB_CB(skb)->seq ||\n\t    TCP_SKB_CB(tail)->ip_dsfield != TCP_SKB_CB(skb)->ip_dsfield ||\n\t    ((TCP_SKB_CB(tail)->tcp_flags |\n\t      TCP_SKB_CB(skb)->tcp_flags) & (TCPHDR_SYN | TCPHDR_RST | TCPHDR_URG)) ||\n\t    !((TCP_SKB_CB(tail)->tcp_flags &\n\t      TCP_SKB_CB(skb)->tcp_flags) & TCPHDR_ACK) ||\n\t    ((TCP_SKB_CB(tail)->tcp_flags ^\n\t      TCP_SKB_CB(skb)->tcp_flags) & (TCPHDR_ECE | TCPHDR_CWR)) ||\n#ifdef CONFIG_TLS_DEVICE\n\t    tail->decrypted != skb->decrypted ||\n#endif\n\t    !mptcp_skb_can_collapse(tail, skb) ||\n\t    thtail->doff != th->doff ||\n\t    memcmp(thtail + 1, th + 1, hdrlen - sizeof(*th)))\n\t\tgoto no_coalesce;\n\n\t__skb_pull(skb, hdrlen);\n\n\tshinfo = skb_shinfo(skb);\n\tgso_size = shinfo->gso_size ?: skb->len;\n\tgso_segs = shinfo->gso_segs ?: 1;\n\n\tshinfo = skb_shinfo(tail);\n\ttail_gso_size = shinfo->gso_size ?: (tail->len - hdrlen);\n\ttail_gso_segs = shinfo->gso_segs ?: 1;\n\n\tif (skb_try_coalesce(tail, skb, &fragstolen, &delta)) {\n\t\tTCP_SKB_CB(tail)->end_seq = TCP_SKB_CB(skb)->end_seq;\n\n\t\tif (likely(!before(TCP_SKB_CB(skb)->ack_seq, TCP_SKB_CB(tail)->ack_seq))) {\n\t\t\tTCP_SKB_CB(tail)->ack_seq = TCP_SKB_CB(skb)->ack_seq;\n\t\t\tthtail->window = th->window;\n\t\t}\n\n\t\t \n\t\tthtail->fin |= th->fin;\n\t\tTCP_SKB_CB(tail)->tcp_flags |= TCP_SKB_CB(skb)->tcp_flags;\n\n\t\tif (TCP_SKB_CB(skb)->has_rxtstamp) {\n\t\t\tTCP_SKB_CB(tail)->has_rxtstamp = true;\n\t\t\ttail->tstamp = skb->tstamp;\n\t\t\tskb_hwtstamps(tail)->hwtstamp = skb_hwtstamps(skb)->hwtstamp;\n\t\t}\n\n\t\t \n\t\tshinfo->gso_size = max(gso_size, tail_gso_size);\n\t\tshinfo->gso_segs = min_t(u32, gso_segs + tail_gso_segs, 0xFFFF);\n\n\t\tsk->sk_backlog.len += delta;\n\t\t__NET_INC_STATS(sock_net(sk),\n\t\t\t\tLINUX_MIB_TCPBACKLOGCOALESCE);\n\t\tkfree_skb_partial(skb, fragstolen);\n\t\treturn false;\n\t}\n\t__skb_push(skb, hdrlen);\n\nno_coalesce:\n\tlimit = (u32)READ_ONCE(sk->sk_rcvbuf) + (u32)(READ_ONCE(sk->sk_sndbuf) >> 1);\n\n\t \n\tlimit += 64 * 1024;\n\n\tif (unlikely(sk_add_backlog(sk, skb, limit))) {\n\t\tbh_unlock_sock(sk);\n\t\t*reason = SKB_DROP_REASON_SOCKET_BACKLOG;\n\t\t__NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPBACKLOGDROP);\n\t\treturn true;\n\t}\n\treturn false;\n}\nEXPORT_SYMBOL(tcp_add_backlog);\n\nint tcp_filter(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcphdr *th = (struct tcphdr *)skb->data;\n\n\treturn sk_filter_trim_cap(sk, skb, th->doff * 4);\n}\nEXPORT_SYMBOL(tcp_filter);\n\nstatic void tcp_v4_restore_cb(struct sk_buff *skb)\n{\n\tmemmove(IPCB(skb), &TCP_SKB_CB(skb)->header.h4,\n\t\tsizeof(struct inet_skb_parm));\n}\n\nstatic void tcp_v4_fill_cb(struct sk_buff *skb, const struct iphdr *iph,\n\t\t\t   const struct tcphdr *th)\n{\n\t \n\tmemmove(&TCP_SKB_CB(skb)->header.h4, IPCB(skb),\n\t\tsizeof(struct inet_skb_parm));\n\tbarrier();\n\n\tTCP_SKB_CB(skb)->seq = ntohl(th->seq);\n\tTCP_SKB_CB(skb)->end_seq = (TCP_SKB_CB(skb)->seq + th->syn + th->fin +\n\t\t\t\t    skb->len - th->doff * 4);\n\tTCP_SKB_CB(skb)->ack_seq = ntohl(th->ack_seq);\n\tTCP_SKB_CB(skb)->tcp_flags = tcp_flag_byte(th);\n\tTCP_SKB_CB(skb)->tcp_tw_isn = 0;\n\tTCP_SKB_CB(skb)->ip_dsfield = ipv4_get_dsfield(iph);\n\tTCP_SKB_CB(skb)->sacked\t = 0;\n\tTCP_SKB_CB(skb)->has_rxtstamp =\n\t\t\tskb->tstamp || skb_hwtstamps(skb)->hwtstamp;\n}\n\n \n\nint tcp_v4_rcv(struct sk_buff *skb)\n{\n\tstruct net *net = dev_net(skb->dev);\n\tenum skb_drop_reason drop_reason;\n\tint sdif = inet_sdif(skb);\n\tint dif = inet_iif(skb);\n\tconst struct iphdr *iph;\n\tconst struct tcphdr *th;\n\tbool refcounted;\n\tstruct sock *sk;\n\tint ret;\n\n\tdrop_reason = SKB_DROP_REASON_NOT_SPECIFIED;\n\tif (skb->pkt_type != PACKET_HOST)\n\t\tgoto discard_it;\n\n\t \n\t__TCP_INC_STATS(net, TCP_MIB_INSEGS);\n\n\tif (!pskb_may_pull(skb, sizeof(struct tcphdr)))\n\t\tgoto discard_it;\n\n\tth = (const struct tcphdr *)skb->data;\n\n\tif (unlikely(th->doff < sizeof(struct tcphdr) / 4)) {\n\t\tdrop_reason = SKB_DROP_REASON_PKT_TOO_SMALL;\n\t\tgoto bad_packet;\n\t}\n\tif (!pskb_may_pull(skb, th->doff * 4))\n\t\tgoto discard_it;\n\n\t \n\n\tif (skb_checksum_init(skb, IPPROTO_TCP, inet_compute_pseudo))\n\t\tgoto csum_error;\n\n\tth = (const struct tcphdr *)skb->data;\n\tiph = ip_hdr(skb);\nlookup:\n\tsk = __inet_lookup_skb(net->ipv4.tcp_death_row.hashinfo,\n\t\t\t       skb, __tcp_hdrlen(th), th->source,\n\t\t\t       th->dest, sdif, &refcounted);\n\tif (!sk)\n\t\tgoto no_tcp_socket;\n\nprocess:\n\tif (sk->sk_state == TCP_TIME_WAIT)\n\t\tgoto do_time_wait;\n\n\tif (sk->sk_state == TCP_NEW_SYN_RECV) {\n\t\tstruct request_sock *req = inet_reqsk(sk);\n\t\tbool req_stolen = false;\n\t\tstruct sock *nsk;\n\n\t\tsk = req->rsk_listener;\n\t\tif (!xfrm4_policy_check(sk, XFRM_POLICY_IN, skb))\n\t\t\tdrop_reason = SKB_DROP_REASON_XFRM_POLICY;\n\t\telse\n\t\t\tdrop_reason = tcp_inbound_md5_hash(sk, skb,\n\t\t\t\t\t\t   &iph->saddr, &iph->daddr,\n\t\t\t\t\t\t   AF_INET, dif, sdif);\n\t\tif (unlikely(drop_reason)) {\n\t\t\tsk_drops_add(sk, skb);\n\t\t\treqsk_put(req);\n\t\t\tgoto discard_it;\n\t\t}\n\t\tif (tcp_checksum_complete(skb)) {\n\t\t\treqsk_put(req);\n\t\t\tgoto csum_error;\n\t\t}\n\t\tif (unlikely(sk->sk_state != TCP_LISTEN)) {\n\t\t\tnsk = reuseport_migrate_sock(sk, req_to_sk(req), skb);\n\t\t\tif (!nsk) {\n\t\t\t\tinet_csk_reqsk_queue_drop_and_put(sk, req);\n\t\t\t\tgoto lookup;\n\t\t\t}\n\t\t\tsk = nsk;\n\t\t\t \n\t\t} else {\n\t\t\t \n\t\t\tsock_hold(sk);\n\t\t}\n\t\trefcounted = true;\n\t\tnsk = NULL;\n\t\tif (!tcp_filter(sk, skb)) {\n\t\t\tth = (const struct tcphdr *)skb->data;\n\t\t\tiph = ip_hdr(skb);\n\t\t\ttcp_v4_fill_cb(skb, iph, th);\n\t\t\tnsk = tcp_check_req(sk, skb, req, false, &req_stolen);\n\t\t} else {\n\t\t\tdrop_reason = SKB_DROP_REASON_SOCKET_FILTER;\n\t\t}\n\t\tif (!nsk) {\n\t\t\treqsk_put(req);\n\t\t\tif (req_stolen) {\n\t\t\t\t \n\t\t\t\ttcp_v4_restore_cb(skb);\n\t\t\t\tsock_put(sk);\n\t\t\t\tgoto lookup;\n\t\t\t}\n\t\t\tgoto discard_and_relse;\n\t\t}\n\t\tnf_reset_ct(skb);\n\t\tif (nsk == sk) {\n\t\t\treqsk_put(req);\n\t\t\ttcp_v4_restore_cb(skb);\n\t\t} else if (tcp_child_process(sk, nsk, skb)) {\n\t\t\ttcp_v4_send_reset(nsk, skb);\n\t\t\tgoto discard_and_relse;\n\t\t} else {\n\t\t\tsock_put(sk);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tif (static_branch_unlikely(&ip4_min_ttl)) {\n\t\t \n\t\tif (unlikely(iph->ttl < READ_ONCE(inet_sk(sk)->min_ttl))) {\n\t\t\t__NET_INC_STATS(net, LINUX_MIB_TCPMINTTLDROP);\n\t\t\tdrop_reason = SKB_DROP_REASON_TCP_MINTTL;\n\t\t\tgoto discard_and_relse;\n\t\t}\n\t}\n\n\tif (!xfrm4_policy_check(sk, XFRM_POLICY_IN, skb)) {\n\t\tdrop_reason = SKB_DROP_REASON_XFRM_POLICY;\n\t\tgoto discard_and_relse;\n\t}\n\n\tdrop_reason = tcp_inbound_md5_hash(sk, skb, &iph->saddr,\n\t\t\t\t\t   &iph->daddr, AF_INET, dif, sdif);\n\tif (drop_reason)\n\t\tgoto discard_and_relse;\n\n\tnf_reset_ct(skb);\n\n\tif (tcp_filter(sk, skb)) {\n\t\tdrop_reason = SKB_DROP_REASON_SOCKET_FILTER;\n\t\tgoto discard_and_relse;\n\t}\n\tth = (const struct tcphdr *)skb->data;\n\tiph = ip_hdr(skb);\n\ttcp_v4_fill_cb(skb, iph, th);\n\n\tskb->dev = NULL;\n\n\tif (sk->sk_state == TCP_LISTEN) {\n\t\tret = tcp_v4_do_rcv(sk, skb);\n\t\tgoto put_and_return;\n\t}\n\n\tsk_incoming_cpu_update(sk);\n\n\tbh_lock_sock_nested(sk);\n\ttcp_segs_in(tcp_sk(sk), skb);\n\tret = 0;\n\tif (!sock_owned_by_user(sk)) {\n\t\tret = tcp_v4_do_rcv(sk, skb);\n\t} else {\n\t\tif (tcp_add_backlog(sk, skb, &drop_reason))\n\t\t\tgoto discard_and_relse;\n\t}\n\tbh_unlock_sock(sk);\n\nput_and_return:\n\tif (refcounted)\n\t\tsock_put(sk);\n\n\treturn ret;\n\nno_tcp_socket:\n\tdrop_reason = SKB_DROP_REASON_NO_SOCKET;\n\tif (!xfrm4_policy_check(NULL, XFRM_POLICY_IN, skb))\n\t\tgoto discard_it;\n\n\ttcp_v4_fill_cb(skb, iph, th);\n\n\tif (tcp_checksum_complete(skb)) {\ncsum_error:\n\t\tdrop_reason = SKB_DROP_REASON_TCP_CSUM;\n\t\ttrace_tcp_bad_csum(skb);\n\t\t__TCP_INC_STATS(net, TCP_MIB_CSUMERRORS);\nbad_packet:\n\t\t__TCP_INC_STATS(net, TCP_MIB_INERRS);\n\t} else {\n\t\ttcp_v4_send_reset(NULL, skb);\n\t}\n\ndiscard_it:\n\tSKB_DR_OR(drop_reason, NOT_SPECIFIED);\n\t \n\tkfree_skb_reason(skb, drop_reason);\n\treturn 0;\n\ndiscard_and_relse:\n\tsk_drops_add(sk, skb);\n\tif (refcounted)\n\t\tsock_put(sk);\n\tgoto discard_it;\n\ndo_time_wait:\n\tif (!xfrm4_policy_check(NULL, XFRM_POLICY_IN, skb)) {\n\t\tdrop_reason = SKB_DROP_REASON_XFRM_POLICY;\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\tgoto discard_it;\n\t}\n\n\ttcp_v4_fill_cb(skb, iph, th);\n\n\tif (tcp_checksum_complete(skb)) {\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\tgoto csum_error;\n\t}\n\tswitch (tcp_timewait_state_process(inet_twsk(sk), skb, th)) {\n\tcase TCP_TW_SYN: {\n\t\tstruct sock *sk2 = inet_lookup_listener(net,\n\t\t\t\t\t\t\tnet->ipv4.tcp_death_row.hashinfo,\n\t\t\t\t\t\t\tskb, __tcp_hdrlen(th),\n\t\t\t\t\t\t\tiph->saddr, th->source,\n\t\t\t\t\t\t\tiph->daddr, th->dest,\n\t\t\t\t\t\t\tinet_iif(skb),\n\t\t\t\t\t\t\tsdif);\n\t\tif (sk2) {\n\t\t\tinet_twsk_deschedule_put(inet_twsk(sk));\n\t\t\tsk = sk2;\n\t\t\ttcp_v4_restore_cb(skb);\n\t\t\trefcounted = false;\n\t\t\tgoto process;\n\t\t}\n\t}\n\t\t \n\t\tfallthrough;\n\tcase TCP_TW_ACK:\n\t\ttcp_v4_timewait_ack(sk, skb);\n\t\tbreak;\n\tcase TCP_TW_RST:\n\t\ttcp_v4_send_reset(sk, skb);\n\t\tinet_twsk_deschedule_put(inet_twsk(sk));\n\t\tgoto discard_it;\n\tcase TCP_TW_SUCCESS:;\n\t}\n\tgoto discard_it;\n}\n\nstatic struct timewait_sock_ops tcp_timewait_sock_ops = {\n\t.twsk_obj_size\t= sizeof(struct tcp_timewait_sock),\n\t.twsk_unique\t= tcp_twsk_unique,\n\t.twsk_destructor= tcp_twsk_destructor,\n};\n\nvoid inet_sk_rx_dst_set(struct sock *sk, const struct sk_buff *skb)\n{\n\tstruct dst_entry *dst = skb_dst(skb);\n\n\tif (dst && dst_hold_safe(dst)) {\n\t\trcu_assign_pointer(sk->sk_rx_dst, dst);\n\t\tsk->sk_rx_dst_ifindex = skb->skb_iif;\n\t}\n}\nEXPORT_SYMBOL(inet_sk_rx_dst_set);\n\nconst struct inet_connection_sock_af_ops ipv4_specific = {\n\t.queue_xmit\t   = ip_queue_xmit,\n\t.send_check\t   = tcp_v4_send_check,\n\t.rebuild_header\t   = inet_sk_rebuild_header,\n\t.sk_rx_dst_set\t   = inet_sk_rx_dst_set,\n\t.conn_request\t   = tcp_v4_conn_request,\n\t.syn_recv_sock\t   = tcp_v4_syn_recv_sock,\n\t.net_header_len\t   = sizeof(struct iphdr),\n\t.setsockopt\t   = ip_setsockopt,\n\t.getsockopt\t   = ip_getsockopt,\n\t.addr2sockaddr\t   = inet_csk_addr2sockaddr,\n\t.sockaddr_len\t   = sizeof(struct sockaddr_in),\n\t.mtu_reduced\t   = tcp_v4_mtu_reduced,\n};\nEXPORT_SYMBOL(ipv4_specific);\n\n#ifdef CONFIG_TCP_MD5SIG\nstatic const struct tcp_sock_af_ops tcp_sock_ipv4_specific = {\n\t.md5_lookup\t\t= tcp_v4_md5_lookup,\n\t.calc_md5_hash\t\t= tcp_v4_md5_hash_skb,\n\t.md5_parse\t\t= tcp_v4_parse_md5_keys,\n};\n#endif\n\n \nstatic int tcp_v4_init_sock(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\ttcp_init_sock(sk);\n\n\ticsk->icsk_af_ops = &ipv4_specific;\n\n#ifdef CONFIG_TCP_MD5SIG\n\ttcp_sk(sk)->af_specific = &tcp_sock_ipv4_specific;\n#endif\n\n\treturn 0;\n}\n\nvoid tcp_v4_destroy_sock(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\ttrace_tcp_destroy_sock(sk);\n\n\ttcp_clear_xmit_timers(sk);\n\n\ttcp_cleanup_congestion_control(sk);\n\n\ttcp_cleanup_ulp(sk);\n\n\t \n\ttcp_write_queue_purge(sk);\n\n\t \n\ttcp_fastopen_active_disable_ofo_check(sk);\n\n\t \n\tskb_rbtree_purge(&tp->out_of_order_queue);\n\n#ifdef CONFIG_TCP_MD5SIG\n\t \n\tif (tp->md5sig_info) {\n\t\ttcp_clear_md5_list(sk);\n\t\tkfree_rcu(rcu_dereference_protected(tp->md5sig_info, 1), rcu);\n\t\ttp->md5sig_info = NULL;\n\t\tstatic_branch_slow_dec_deferred(&tcp_md5_needed);\n\t}\n#endif\n\n\t \n\tif (inet_csk(sk)->icsk_bind_hash)\n\t\tinet_put_port(sk);\n\n\tBUG_ON(rcu_access_pointer(tp->fastopen_rsk));\n\n\t \n\ttcp_free_fastopen_req(tp);\n\ttcp_fastopen_destroy_cipher(sk);\n\ttcp_saved_syn_free(tp);\n\n\tsk_sockets_allocated_dec(sk);\n}\nEXPORT_SYMBOL(tcp_v4_destroy_sock);\n\n#ifdef CONFIG_PROC_FS\n \n\nstatic unsigned short seq_file_family(const struct seq_file *seq);\n\nstatic bool seq_sk_match(struct seq_file *seq, const struct sock *sk)\n{\n\tunsigned short family = seq_file_family(seq);\n\n\t \n\treturn ((family == AF_UNSPEC || family == sk->sk_family) &&\n\t\tnet_eq(sock_net(sk), seq_file_net(seq)));\n}\n\n \nstatic void *listening_get_first(struct seq_file *seq)\n{\n\tstruct inet_hashinfo *hinfo = seq_file_net(seq)->ipv4.tcp_death_row.hashinfo;\n\tstruct tcp_iter_state *st = seq->private;\n\n\tst->offset = 0;\n\tfor (; st->bucket <= hinfo->lhash2_mask; st->bucket++) {\n\t\tstruct inet_listen_hashbucket *ilb2;\n\t\tstruct hlist_nulls_node *node;\n\t\tstruct sock *sk;\n\n\t\tilb2 = &hinfo->lhash2[st->bucket];\n\t\tif (hlist_nulls_empty(&ilb2->nulls_head))\n\t\t\tcontinue;\n\n\t\tspin_lock(&ilb2->lock);\n\t\tsk_nulls_for_each(sk, node, &ilb2->nulls_head) {\n\t\t\tif (seq_sk_match(seq, sk))\n\t\t\t\treturn sk;\n\t\t}\n\t\tspin_unlock(&ilb2->lock);\n\t}\n\n\treturn NULL;\n}\n\n \nstatic void *listening_get_next(struct seq_file *seq, void *cur)\n{\n\tstruct tcp_iter_state *st = seq->private;\n\tstruct inet_listen_hashbucket *ilb2;\n\tstruct hlist_nulls_node *node;\n\tstruct inet_hashinfo *hinfo;\n\tstruct sock *sk = cur;\n\n\t++st->num;\n\t++st->offset;\n\n\tsk = sk_nulls_next(sk);\n\tsk_nulls_for_each_from(sk, node) {\n\t\tif (seq_sk_match(seq, sk))\n\t\t\treturn sk;\n\t}\n\n\thinfo = seq_file_net(seq)->ipv4.tcp_death_row.hashinfo;\n\tilb2 = &hinfo->lhash2[st->bucket];\n\tspin_unlock(&ilb2->lock);\n\t++st->bucket;\n\treturn listening_get_first(seq);\n}\n\nstatic void *listening_get_idx(struct seq_file *seq, loff_t *pos)\n{\n\tstruct tcp_iter_state *st = seq->private;\n\tvoid *rc;\n\n\tst->bucket = 0;\n\tst->offset = 0;\n\trc = listening_get_first(seq);\n\n\twhile (rc && *pos) {\n\t\trc = listening_get_next(seq, rc);\n\t\t--*pos;\n\t}\n\treturn rc;\n}\n\nstatic inline bool empty_bucket(struct inet_hashinfo *hinfo,\n\t\t\t\tconst struct tcp_iter_state *st)\n{\n\treturn hlist_nulls_empty(&hinfo->ehash[st->bucket].chain);\n}\n\n \nstatic void *established_get_first(struct seq_file *seq)\n{\n\tstruct inet_hashinfo *hinfo = seq_file_net(seq)->ipv4.tcp_death_row.hashinfo;\n\tstruct tcp_iter_state *st = seq->private;\n\n\tst->offset = 0;\n\tfor (; st->bucket <= hinfo->ehash_mask; ++st->bucket) {\n\t\tstruct sock *sk;\n\t\tstruct hlist_nulls_node *node;\n\t\tspinlock_t *lock = inet_ehash_lockp(hinfo, st->bucket);\n\n\t\tcond_resched();\n\n\t\t \n\t\tif (empty_bucket(hinfo, st))\n\t\t\tcontinue;\n\n\t\tspin_lock_bh(lock);\n\t\tsk_nulls_for_each(sk, node, &hinfo->ehash[st->bucket].chain) {\n\t\t\tif (seq_sk_match(seq, sk))\n\t\t\t\treturn sk;\n\t\t}\n\t\tspin_unlock_bh(lock);\n\t}\n\n\treturn NULL;\n}\n\nstatic void *established_get_next(struct seq_file *seq, void *cur)\n{\n\tstruct inet_hashinfo *hinfo = seq_file_net(seq)->ipv4.tcp_death_row.hashinfo;\n\tstruct tcp_iter_state *st = seq->private;\n\tstruct hlist_nulls_node *node;\n\tstruct sock *sk = cur;\n\n\t++st->num;\n\t++st->offset;\n\n\tsk = sk_nulls_next(sk);\n\n\tsk_nulls_for_each_from(sk, node) {\n\t\tif (seq_sk_match(seq, sk))\n\t\t\treturn sk;\n\t}\n\n\tspin_unlock_bh(inet_ehash_lockp(hinfo, st->bucket));\n\t++st->bucket;\n\treturn established_get_first(seq);\n}\n\nstatic void *established_get_idx(struct seq_file *seq, loff_t pos)\n{\n\tstruct tcp_iter_state *st = seq->private;\n\tvoid *rc;\n\n\tst->bucket = 0;\n\trc = established_get_first(seq);\n\n\twhile (rc && pos) {\n\t\trc = established_get_next(seq, rc);\n\t\t--pos;\n\t}\n\treturn rc;\n}\n\nstatic void *tcp_get_idx(struct seq_file *seq, loff_t pos)\n{\n\tvoid *rc;\n\tstruct tcp_iter_state *st = seq->private;\n\n\tst->state = TCP_SEQ_STATE_LISTENING;\n\trc\t  = listening_get_idx(seq, &pos);\n\n\tif (!rc) {\n\t\tst->state = TCP_SEQ_STATE_ESTABLISHED;\n\t\trc\t  = established_get_idx(seq, pos);\n\t}\n\n\treturn rc;\n}\n\nstatic void *tcp_seek_last_pos(struct seq_file *seq)\n{\n\tstruct inet_hashinfo *hinfo = seq_file_net(seq)->ipv4.tcp_death_row.hashinfo;\n\tstruct tcp_iter_state *st = seq->private;\n\tint bucket = st->bucket;\n\tint offset = st->offset;\n\tint orig_num = st->num;\n\tvoid *rc = NULL;\n\n\tswitch (st->state) {\n\tcase TCP_SEQ_STATE_LISTENING:\n\t\tif (st->bucket > hinfo->lhash2_mask)\n\t\t\tbreak;\n\t\trc = listening_get_first(seq);\n\t\twhile (offset-- && rc && bucket == st->bucket)\n\t\t\trc = listening_get_next(seq, rc);\n\t\tif (rc)\n\t\t\tbreak;\n\t\tst->bucket = 0;\n\t\tst->state = TCP_SEQ_STATE_ESTABLISHED;\n\t\tfallthrough;\n\tcase TCP_SEQ_STATE_ESTABLISHED:\n\t\tif (st->bucket > hinfo->ehash_mask)\n\t\t\tbreak;\n\t\trc = established_get_first(seq);\n\t\twhile (offset-- && rc && bucket == st->bucket)\n\t\t\trc = established_get_next(seq, rc);\n\t}\n\n\tst->num = orig_num;\n\n\treturn rc;\n}\n\nvoid *tcp_seq_start(struct seq_file *seq, loff_t *pos)\n{\n\tstruct tcp_iter_state *st = seq->private;\n\tvoid *rc;\n\n\tif (*pos && *pos == st->last_pos) {\n\t\trc = tcp_seek_last_pos(seq);\n\t\tif (rc)\n\t\t\tgoto out;\n\t}\n\n\tst->state = TCP_SEQ_STATE_LISTENING;\n\tst->num = 0;\n\tst->bucket = 0;\n\tst->offset = 0;\n\trc = *pos ? tcp_get_idx(seq, *pos - 1) : SEQ_START_TOKEN;\n\nout:\n\tst->last_pos = *pos;\n\treturn rc;\n}\nEXPORT_SYMBOL(tcp_seq_start);\n\nvoid *tcp_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\tstruct tcp_iter_state *st = seq->private;\n\tvoid *rc = NULL;\n\n\tif (v == SEQ_START_TOKEN) {\n\t\trc = tcp_get_idx(seq, 0);\n\t\tgoto out;\n\t}\n\n\tswitch (st->state) {\n\tcase TCP_SEQ_STATE_LISTENING:\n\t\trc = listening_get_next(seq, v);\n\t\tif (!rc) {\n\t\t\tst->state = TCP_SEQ_STATE_ESTABLISHED;\n\t\t\tst->bucket = 0;\n\t\t\tst->offset = 0;\n\t\t\trc\t  = established_get_first(seq);\n\t\t}\n\t\tbreak;\n\tcase TCP_SEQ_STATE_ESTABLISHED:\n\t\trc = established_get_next(seq, v);\n\t\tbreak;\n\t}\nout:\n\t++*pos;\n\tst->last_pos = *pos;\n\treturn rc;\n}\nEXPORT_SYMBOL(tcp_seq_next);\n\nvoid tcp_seq_stop(struct seq_file *seq, void *v)\n{\n\tstruct inet_hashinfo *hinfo = seq_file_net(seq)->ipv4.tcp_death_row.hashinfo;\n\tstruct tcp_iter_state *st = seq->private;\n\n\tswitch (st->state) {\n\tcase TCP_SEQ_STATE_LISTENING:\n\t\tif (v != SEQ_START_TOKEN)\n\t\t\tspin_unlock(&hinfo->lhash2[st->bucket].lock);\n\t\tbreak;\n\tcase TCP_SEQ_STATE_ESTABLISHED:\n\t\tif (v)\n\t\t\tspin_unlock_bh(inet_ehash_lockp(hinfo, st->bucket));\n\t\tbreak;\n\t}\n}\nEXPORT_SYMBOL(tcp_seq_stop);\n\nstatic void get_openreq4(const struct request_sock *req,\n\t\t\t struct seq_file *f, int i)\n{\n\tconst struct inet_request_sock *ireq = inet_rsk(req);\n\tlong delta = req->rsk_timer.expires - jiffies;\n\n\tseq_printf(f, \"%4d: %08X:%04X %08X:%04X\"\n\t\t\" %02X %08X:%08X %02X:%08lX %08X %5u %8d %u %d %pK\",\n\t\ti,\n\t\tireq->ir_loc_addr,\n\t\tireq->ir_num,\n\t\tireq->ir_rmt_addr,\n\t\tntohs(ireq->ir_rmt_port),\n\t\tTCP_SYN_RECV,\n\t\t0, 0,  \n\t\t1,     \n\t\tjiffies_delta_to_clock_t(delta),\n\t\treq->num_timeout,\n\t\tfrom_kuid_munged(seq_user_ns(f),\n\t\t\t\t sock_i_uid(req->rsk_listener)),\n\t\t0,   \n\t\t0,  \n\t\t0,\n\t\treq);\n}\n\nstatic void get_tcp4_sock(struct sock *sk, struct seq_file *f, int i)\n{\n\tint timer_active;\n\tunsigned long timer_expires;\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tconst struct inet_sock *inet = inet_sk(sk);\n\tconst struct fastopen_queue *fastopenq = &icsk->icsk_accept_queue.fastopenq;\n\t__be32 dest = inet->inet_daddr;\n\t__be32 src = inet->inet_rcv_saddr;\n\t__u16 destp = ntohs(inet->inet_dport);\n\t__u16 srcp = ntohs(inet->inet_sport);\n\tint rx_queue;\n\tint state;\n\n\tif (icsk->icsk_pending == ICSK_TIME_RETRANS ||\n\t    icsk->icsk_pending == ICSK_TIME_REO_TIMEOUT ||\n\t    icsk->icsk_pending == ICSK_TIME_LOSS_PROBE) {\n\t\ttimer_active\t= 1;\n\t\ttimer_expires\t= icsk->icsk_timeout;\n\t} else if (icsk->icsk_pending == ICSK_TIME_PROBE0) {\n\t\ttimer_active\t= 4;\n\t\ttimer_expires\t= icsk->icsk_timeout;\n\t} else if (timer_pending(&sk->sk_timer)) {\n\t\ttimer_active\t= 2;\n\t\ttimer_expires\t= sk->sk_timer.expires;\n\t} else {\n\t\ttimer_active\t= 0;\n\t\ttimer_expires = jiffies;\n\t}\n\n\tstate = inet_sk_state_load(sk);\n\tif (state == TCP_LISTEN)\n\t\trx_queue = READ_ONCE(sk->sk_ack_backlog);\n\telse\n\t\t \n\t\trx_queue = max_t(int, READ_ONCE(tp->rcv_nxt) -\n\t\t\t\t      READ_ONCE(tp->copied_seq), 0);\n\n\tseq_printf(f, \"%4d: %08X:%04X %08X:%04X %02X %08X:%08X %02X:%08lX \"\n\t\t\t\"%08X %5u %8d %lu %d %pK %lu %lu %u %u %d\",\n\t\ti, src, srcp, dest, destp, state,\n\t\tREAD_ONCE(tp->write_seq) - tp->snd_una,\n\t\trx_queue,\n\t\ttimer_active,\n\t\tjiffies_delta_to_clock_t(timer_expires - jiffies),\n\t\ticsk->icsk_retransmits,\n\t\tfrom_kuid_munged(seq_user_ns(f), sock_i_uid(sk)),\n\t\ticsk->icsk_probes_out,\n\t\tsock_i_ino(sk),\n\t\trefcount_read(&sk->sk_refcnt), sk,\n\t\tjiffies_to_clock_t(icsk->icsk_rto),\n\t\tjiffies_to_clock_t(icsk->icsk_ack.ato),\n\t\t(icsk->icsk_ack.quick << 1) | inet_csk_in_pingpong_mode(sk),\n\t\ttcp_snd_cwnd(tp),\n\t\tstate == TCP_LISTEN ?\n\t\t    fastopenq->max_qlen :\n\t\t    (tcp_in_initial_slowstart(tp) ? -1 : tp->snd_ssthresh));\n}\n\nstatic void get_timewait4_sock(const struct inet_timewait_sock *tw,\n\t\t\t       struct seq_file *f, int i)\n{\n\tlong delta = tw->tw_timer.expires - jiffies;\n\t__be32 dest, src;\n\t__u16 destp, srcp;\n\n\tdest  = tw->tw_daddr;\n\tsrc   = tw->tw_rcv_saddr;\n\tdestp = ntohs(tw->tw_dport);\n\tsrcp  = ntohs(tw->tw_sport);\n\n\tseq_printf(f, \"%4d: %08X:%04X %08X:%04X\"\n\t\t\" %02X %08X:%08X %02X:%08lX %08X %5d %8d %d %d %pK\",\n\t\ti, src, srcp, dest, destp, tw->tw_substate, 0, 0,\n\t\t3, jiffies_delta_to_clock_t(delta), 0, 0, 0, 0,\n\t\trefcount_read(&tw->tw_refcnt), tw);\n}\n\n#define TMPSZ 150\n\nstatic int tcp4_seq_show(struct seq_file *seq, void *v)\n{\n\tstruct tcp_iter_state *st;\n\tstruct sock *sk = v;\n\n\tseq_setwidth(seq, TMPSZ - 1);\n\tif (v == SEQ_START_TOKEN) {\n\t\tseq_puts(seq, \"  sl  local_address rem_address   st tx_queue \"\n\t\t\t   \"rx_queue tr tm->when retrnsmt   uid  timeout \"\n\t\t\t   \"inode\");\n\t\tgoto out;\n\t}\n\tst = seq->private;\n\n\tif (sk->sk_state == TCP_TIME_WAIT)\n\t\tget_timewait4_sock(v, seq, st->num);\n\telse if (sk->sk_state == TCP_NEW_SYN_RECV)\n\t\tget_openreq4(v, seq, st->num);\n\telse\n\t\tget_tcp4_sock(v, seq, st->num);\nout:\n\tseq_pad(seq, '\\n');\n\treturn 0;\n}\n\n#ifdef CONFIG_BPF_SYSCALL\nstruct bpf_tcp_iter_state {\n\tstruct tcp_iter_state state;\n\tunsigned int cur_sk;\n\tunsigned int end_sk;\n\tunsigned int max_sk;\n\tstruct sock **batch;\n\tbool st_bucket_done;\n};\n\nstruct bpf_iter__tcp {\n\t__bpf_md_ptr(struct bpf_iter_meta *, meta);\n\t__bpf_md_ptr(struct sock_common *, sk_common);\n\tuid_t uid __aligned(8);\n};\n\nstatic int tcp_prog_seq_show(struct bpf_prog *prog, struct bpf_iter_meta *meta,\n\t\t\t     struct sock_common *sk_common, uid_t uid)\n{\n\tstruct bpf_iter__tcp ctx;\n\n\tmeta->seq_num--;   \n\tctx.meta = meta;\n\tctx.sk_common = sk_common;\n\tctx.uid = uid;\n\treturn bpf_iter_run_prog(prog, &ctx);\n}\n\nstatic void bpf_iter_tcp_put_batch(struct bpf_tcp_iter_state *iter)\n{\n\twhile (iter->cur_sk < iter->end_sk)\n\t\tsock_gen_put(iter->batch[iter->cur_sk++]);\n}\n\nstatic int bpf_iter_tcp_realloc_batch(struct bpf_tcp_iter_state *iter,\n\t\t\t\t      unsigned int new_batch_sz)\n{\n\tstruct sock **new_batch;\n\n\tnew_batch = kvmalloc(sizeof(*new_batch) * new_batch_sz,\n\t\t\t     GFP_USER | __GFP_NOWARN);\n\tif (!new_batch)\n\t\treturn -ENOMEM;\n\n\tbpf_iter_tcp_put_batch(iter);\n\tkvfree(iter->batch);\n\titer->batch = new_batch;\n\titer->max_sk = new_batch_sz;\n\n\treturn 0;\n}\n\nstatic unsigned int bpf_iter_tcp_listening_batch(struct seq_file *seq,\n\t\t\t\t\t\t struct sock *start_sk)\n{\n\tstruct inet_hashinfo *hinfo = seq_file_net(seq)->ipv4.tcp_death_row.hashinfo;\n\tstruct bpf_tcp_iter_state *iter = seq->private;\n\tstruct tcp_iter_state *st = &iter->state;\n\tstruct hlist_nulls_node *node;\n\tunsigned int expected = 1;\n\tstruct sock *sk;\n\n\tsock_hold(start_sk);\n\titer->batch[iter->end_sk++] = start_sk;\n\n\tsk = sk_nulls_next(start_sk);\n\tsk_nulls_for_each_from(sk, node) {\n\t\tif (seq_sk_match(seq, sk)) {\n\t\t\tif (iter->end_sk < iter->max_sk) {\n\t\t\t\tsock_hold(sk);\n\t\t\t\titer->batch[iter->end_sk++] = sk;\n\t\t\t}\n\t\t\texpected++;\n\t\t}\n\t}\n\tspin_unlock(&hinfo->lhash2[st->bucket].lock);\n\n\treturn expected;\n}\n\nstatic unsigned int bpf_iter_tcp_established_batch(struct seq_file *seq,\n\t\t\t\t\t\t   struct sock *start_sk)\n{\n\tstruct inet_hashinfo *hinfo = seq_file_net(seq)->ipv4.tcp_death_row.hashinfo;\n\tstruct bpf_tcp_iter_state *iter = seq->private;\n\tstruct tcp_iter_state *st = &iter->state;\n\tstruct hlist_nulls_node *node;\n\tunsigned int expected = 1;\n\tstruct sock *sk;\n\n\tsock_hold(start_sk);\n\titer->batch[iter->end_sk++] = start_sk;\n\n\tsk = sk_nulls_next(start_sk);\n\tsk_nulls_for_each_from(sk, node) {\n\t\tif (seq_sk_match(seq, sk)) {\n\t\t\tif (iter->end_sk < iter->max_sk) {\n\t\t\t\tsock_hold(sk);\n\t\t\t\titer->batch[iter->end_sk++] = sk;\n\t\t\t}\n\t\t\texpected++;\n\t\t}\n\t}\n\tspin_unlock_bh(inet_ehash_lockp(hinfo, st->bucket));\n\n\treturn expected;\n}\n\nstatic struct sock *bpf_iter_tcp_batch(struct seq_file *seq)\n{\n\tstruct inet_hashinfo *hinfo = seq_file_net(seq)->ipv4.tcp_death_row.hashinfo;\n\tstruct bpf_tcp_iter_state *iter = seq->private;\n\tstruct tcp_iter_state *st = &iter->state;\n\tunsigned int expected;\n\tbool resized = false;\n\tstruct sock *sk;\n\n\t \n\tif (iter->st_bucket_done) {\n\t\tst->offset = 0;\n\t\tst->bucket++;\n\t\tif (st->state == TCP_SEQ_STATE_LISTENING &&\n\t\t    st->bucket > hinfo->lhash2_mask) {\n\t\t\tst->state = TCP_SEQ_STATE_ESTABLISHED;\n\t\t\tst->bucket = 0;\n\t\t}\n\t}\n\nagain:\n\t \n\titer->cur_sk = 0;\n\titer->end_sk = 0;\n\titer->st_bucket_done = false;\n\n\tsk = tcp_seek_last_pos(seq);\n\tif (!sk)\n\t\treturn NULL;  \n\n\tif (st->state == TCP_SEQ_STATE_LISTENING)\n\t\texpected = bpf_iter_tcp_listening_batch(seq, sk);\n\telse\n\t\texpected = bpf_iter_tcp_established_batch(seq, sk);\n\n\tif (iter->end_sk == expected) {\n\t\titer->st_bucket_done = true;\n\t\treturn sk;\n\t}\n\n\tif (!resized && !bpf_iter_tcp_realloc_batch(iter, expected * 3 / 2)) {\n\t\tresized = true;\n\t\tgoto again;\n\t}\n\n\treturn sk;\n}\n\nstatic void *bpf_iter_tcp_seq_start(struct seq_file *seq, loff_t *pos)\n{\n\t \n\tif (*pos)\n\t\treturn bpf_iter_tcp_batch(seq);\n\n\treturn SEQ_START_TOKEN;\n}\n\nstatic void *bpf_iter_tcp_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\tstruct bpf_tcp_iter_state *iter = seq->private;\n\tstruct tcp_iter_state *st = &iter->state;\n\tstruct sock *sk;\n\n\t \n\tif (iter->cur_sk < iter->end_sk) {\n\t\t \n\t\tst->num++;\n\t\t \n\t\tst->offset++;\n\t\tsock_gen_put(iter->batch[iter->cur_sk++]);\n\t}\n\n\tif (iter->cur_sk < iter->end_sk)\n\t\tsk = iter->batch[iter->cur_sk];\n\telse\n\t\tsk = bpf_iter_tcp_batch(seq);\n\n\t++*pos;\n\t \n\tst->last_pos = *pos;\n\treturn sk;\n}\n\nstatic int bpf_iter_tcp_seq_show(struct seq_file *seq, void *v)\n{\n\tstruct bpf_iter_meta meta;\n\tstruct bpf_prog *prog;\n\tstruct sock *sk = v;\n\tuid_t uid;\n\tint ret;\n\n\tif (v == SEQ_START_TOKEN)\n\t\treturn 0;\n\n\tif (sk_fullsock(sk))\n\t\tlock_sock(sk);\n\n\tif (unlikely(sk_unhashed(sk))) {\n\t\tret = SEQ_SKIP;\n\t\tgoto unlock;\n\t}\n\n\tif (sk->sk_state == TCP_TIME_WAIT) {\n\t\tuid = 0;\n\t} else if (sk->sk_state == TCP_NEW_SYN_RECV) {\n\t\tconst struct request_sock *req = v;\n\n\t\tuid = from_kuid_munged(seq_user_ns(seq),\n\t\t\t\t       sock_i_uid(req->rsk_listener));\n\t} else {\n\t\tuid = from_kuid_munged(seq_user_ns(seq), sock_i_uid(sk));\n\t}\n\n\tmeta.seq = seq;\n\tprog = bpf_iter_get_info(&meta, false);\n\tret = tcp_prog_seq_show(prog, &meta, v, uid);\n\nunlock:\n\tif (sk_fullsock(sk))\n\t\trelease_sock(sk);\n\treturn ret;\n\n}\n\nstatic void bpf_iter_tcp_seq_stop(struct seq_file *seq, void *v)\n{\n\tstruct bpf_tcp_iter_state *iter = seq->private;\n\tstruct bpf_iter_meta meta;\n\tstruct bpf_prog *prog;\n\n\tif (!v) {\n\t\tmeta.seq = seq;\n\t\tprog = bpf_iter_get_info(&meta, true);\n\t\tif (prog)\n\t\t\t(void)tcp_prog_seq_show(prog, &meta, v, 0);\n\t}\n\n\tif (iter->cur_sk < iter->end_sk) {\n\t\tbpf_iter_tcp_put_batch(iter);\n\t\titer->st_bucket_done = false;\n\t}\n}\n\nstatic const struct seq_operations bpf_iter_tcp_seq_ops = {\n\t.show\t\t= bpf_iter_tcp_seq_show,\n\t.start\t\t= bpf_iter_tcp_seq_start,\n\t.next\t\t= bpf_iter_tcp_seq_next,\n\t.stop\t\t= bpf_iter_tcp_seq_stop,\n};\n#endif\nstatic unsigned short seq_file_family(const struct seq_file *seq)\n{\n\tconst struct tcp_seq_afinfo *afinfo;\n\n#ifdef CONFIG_BPF_SYSCALL\n\t \n\tif (seq->op == &bpf_iter_tcp_seq_ops)\n\t\treturn AF_UNSPEC;\n#endif\n\n\t \n\tafinfo = pde_data(file_inode(seq->file));\n\treturn afinfo->family;\n}\n\nstatic const struct seq_operations tcp4_seq_ops = {\n\t.show\t\t= tcp4_seq_show,\n\t.start\t\t= tcp_seq_start,\n\t.next\t\t= tcp_seq_next,\n\t.stop\t\t= tcp_seq_stop,\n};\n\nstatic struct tcp_seq_afinfo tcp4_seq_afinfo = {\n\t.family\t\t= AF_INET,\n};\n\nstatic int __net_init tcp4_proc_init_net(struct net *net)\n{\n\tif (!proc_create_net_data(\"tcp\", 0444, net->proc_net, &tcp4_seq_ops,\n\t\t\tsizeof(struct tcp_iter_state), &tcp4_seq_afinfo))\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nstatic void __net_exit tcp4_proc_exit_net(struct net *net)\n{\n\tremove_proc_entry(\"tcp\", net->proc_net);\n}\n\nstatic struct pernet_operations tcp4_net_ops = {\n\t.init = tcp4_proc_init_net,\n\t.exit = tcp4_proc_exit_net,\n};\n\nint __init tcp4_proc_init(void)\n{\n\treturn register_pernet_subsys(&tcp4_net_ops);\n}\n\nvoid tcp4_proc_exit(void)\n{\n\tunregister_pernet_subsys(&tcp4_net_ops);\n}\n#endif  \n\n \nbool tcp_stream_memory_free(const struct sock *sk, int wake)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tu32 notsent_bytes = READ_ONCE(tp->write_seq) -\n\t\t\t    READ_ONCE(tp->snd_nxt);\n\n\treturn (notsent_bytes << wake) < tcp_notsent_lowat(tp);\n}\nEXPORT_SYMBOL(tcp_stream_memory_free);\n\nstruct proto tcp_prot = {\n\t.name\t\t\t= \"TCP\",\n\t.owner\t\t\t= THIS_MODULE,\n\t.close\t\t\t= tcp_close,\n\t.pre_connect\t\t= tcp_v4_pre_connect,\n\t.connect\t\t= tcp_v4_connect,\n\t.disconnect\t\t= tcp_disconnect,\n\t.accept\t\t\t= inet_csk_accept,\n\t.ioctl\t\t\t= tcp_ioctl,\n\t.init\t\t\t= tcp_v4_init_sock,\n\t.destroy\t\t= tcp_v4_destroy_sock,\n\t.shutdown\t\t= tcp_shutdown,\n\t.setsockopt\t\t= tcp_setsockopt,\n\t.getsockopt\t\t= tcp_getsockopt,\n\t.bpf_bypass_getsockopt\t= tcp_bpf_bypass_getsockopt,\n\t.keepalive\t\t= tcp_set_keepalive,\n\t.recvmsg\t\t= tcp_recvmsg,\n\t.sendmsg\t\t= tcp_sendmsg,\n\t.splice_eof\t\t= tcp_splice_eof,\n\t.backlog_rcv\t\t= tcp_v4_do_rcv,\n\t.release_cb\t\t= tcp_release_cb,\n\t.hash\t\t\t= inet_hash,\n\t.unhash\t\t\t= inet_unhash,\n\t.get_port\t\t= inet_csk_get_port,\n\t.put_port\t\t= inet_put_port,\n#ifdef CONFIG_BPF_SYSCALL\n\t.psock_update_sk_prot\t= tcp_bpf_update_proto,\n#endif\n\t.enter_memory_pressure\t= tcp_enter_memory_pressure,\n\t.leave_memory_pressure\t= tcp_leave_memory_pressure,\n\t.stream_memory_free\t= tcp_stream_memory_free,\n\t.sockets_allocated\t= &tcp_sockets_allocated,\n\t.orphan_count\t\t= &tcp_orphan_count,\n\n\t.memory_allocated\t= &tcp_memory_allocated,\n\t.per_cpu_fw_alloc\t= &tcp_memory_per_cpu_fw_alloc,\n\n\t.memory_pressure\t= &tcp_memory_pressure,\n\t.sysctl_mem\t\t= sysctl_tcp_mem,\n\t.sysctl_wmem_offset\t= offsetof(struct net, ipv4.sysctl_tcp_wmem),\n\t.sysctl_rmem_offset\t= offsetof(struct net, ipv4.sysctl_tcp_rmem),\n\t.max_header\t\t= MAX_TCP_HEADER,\n\t.obj_size\t\t= sizeof(struct tcp_sock),\n\t.slab_flags\t\t= SLAB_TYPESAFE_BY_RCU,\n\t.twsk_prot\t\t= &tcp_timewait_sock_ops,\n\t.rsk_prot\t\t= &tcp_request_sock_ops,\n\t.h.hashinfo\t\t= NULL,\n\t.no_autobind\t\t= true,\n\t.diag_destroy\t\t= tcp_abort,\n};\nEXPORT_SYMBOL(tcp_prot);\n\nstatic void __net_exit tcp_sk_exit(struct net *net)\n{\n\tif (net->ipv4.tcp_congestion_control)\n\t\tbpf_module_put(net->ipv4.tcp_congestion_control,\n\t\t\t       net->ipv4.tcp_congestion_control->owner);\n}\n\nstatic void __net_init tcp_set_hashinfo(struct net *net)\n{\n\tstruct inet_hashinfo *hinfo;\n\tunsigned int ehash_entries;\n\tstruct net *old_net;\n\n\tif (net_eq(net, &init_net))\n\t\tgoto fallback;\n\n\told_net = current->nsproxy->net_ns;\n\tehash_entries = READ_ONCE(old_net->ipv4.sysctl_tcp_child_ehash_entries);\n\tif (!ehash_entries)\n\t\tgoto fallback;\n\n\tehash_entries = roundup_pow_of_two(ehash_entries);\n\thinfo = inet_pernet_hashinfo_alloc(&tcp_hashinfo, ehash_entries);\n\tif (!hinfo) {\n\t\tpr_warn(\"Failed to allocate TCP ehash (entries: %u) \"\n\t\t\t\"for a netns, fallback to the global one\\n\",\n\t\t\tehash_entries);\nfallback:\n\t\thinfo = &tcp_hashinfo;\n\t\tehash_entries = tcp_hashinfo.ehash_mask + 1;\n\t}\n\n\tnet->ipv4.tcp_death_row.hashinfo = hinfo;\n\tnet->ipv4.tcp_death_row.sysctl_max_tw_buckets = ehash_entries / 2;\n\tnet->ipv4.sysctl_max_syn_backlog = max(128U, ehash_entries / 128);\n}\n\nstatic int __net_init tcp_sk_init(struct net *net)\n{\n\tnet->ipv4.sysctl_tcp_ecn = 2;\n\tnet->ipv4.sysctl_tcp_ecn_fallback = 1;\n\n\tnet->ipv4.sysctl_tcp_base_mss = TCP_BASE_MSS;\n\tnet->ipv4.sysctl_tcp_min_snd_mss = TCP_MIN_SND_MSS;\n\tnet->ipv4.sysctl_tcp_probe_threshold = TCP_PROBE_THRESHOLD;\n\tnet->ipv4.sysctl_tcp_probe_interval = TCP_PROBE_INTERVAL;\n\tnet->ipv4.sysctl_tcp_mtu_probe_floor = TCP_MIN_SND_MSS;\n\n\tnet->ipv4.sysctl_tcp_keepalive_time = TCP_KEEPALIVE_TIME;\n\tnet->ipv4.sysctl_tcp_keepalive_probes = TCP_KEEPALIVE_PROBES;\n\tnet->ipv4.sysctl_tcp_keepalive_intvl = TCP_KEEPALIVE_INTVL;\n\n\tnet->ipv4.sysctl_tcp_syn_retries = TCP_SYN_RETRIES;\n\tnet->ipv4.sysctl_tcp_synack_retries = TCP_SYNACK_RETRIES;\n\tnet->ipv4.sysctl_tcp_syncookies = 1;\n\tnet->ipv4.sysctl_tcp_reordering = TCP_FASTRETRANS_THRESH;\n\tnet->ipv4.sysctl_tcp_retries1 = TCP_RETR1;\n\tnet->ipv4.sysctl_tcp_retries2 = TCP_RETR2;\n\tnet->ipv4.sysctl_tcp_orphan_retries = 0;\n\tnet->ipv4.sysctl_tcp_fin_timeout = TCP_FIN_TIMEOUT;\n\tnet->ipv4.sysctl_tcp_notsent_lowat = UINT_MAX;\n\tnet->ipv4.sysctl_tcp_tw_reuse = 2;\n\tnet->ipv4.sysctl_tcp_no_ssthresh_metrics_save = 1;\n\n\trefcount_set(&net->ipv4.tcp_death_row.tw_refcount, 1);\n\ttcp_set_hashinfo(net);\n\n\tnet->ipv4.sysctl_tcp_sack = 1;\n\tnet->ipv4.sysctl_tcp_window_scaling = 1;\n\tnet->ipv4.sysctl_tcp_timestamps = 1;\n\tnet->ipv4.sysctl_tcp_early_retrans = 3;\n\tnet->ipv4.sysctl_tcp_recovery = TCP_RACK_LOSS_DETECTION;\n\tnet->ipv4.sysctl_tcp_slow_start_after_idle = 1;  \n\tnet->ipv4.sysctl_tcp_retrans_collapse = 1;\n\tnet->ipv4.sysctl_tcp_max_reordering = 300;\n\tnet->ipv4.sysctl_tcp_dsack = 1;\n\tnet->ipv4.sysctl_tcp_app_win = 31;\n\tnet->ipv4.sysctl_tcp_adv_win_scale = 1;\n\tnet->ipv4.sysctl_tcp_frto = 2;\n\tnet->ipv4.sysctl_tcp_moderate_rcvbuf = 1;\n\t \n\tnet->ipv4.sysctl_tcp_tso_win_divisor = 3;\n\t \n\tnet->ipv4.sysctl_tcp_limit_output_bytes = 16 * 65536;\n\n\t \n\tnet->ipv4.sysctl_tcp_challenge_ack_limit = INT_MAX;\n\n\tnet->ipv4.sysctl_tcp_min_tso_segs = 2;\n\tnet->ipv4.sysctl_tcp_tso_rtt_log = 9;   \n\tnet->ipv4.sysctl_tcp_min_rtt_wlen = 300;\n\tnet->ipv4.sysctl_tcp_autocorking = 1;\n\tnet->ipv4.sysctl_tcp_invalid_ratelimit = HZ/2;\n\tnet->ipv4.sysctl_tcp_pacing_ss_ratio = 200;\n\tnet->ipv4.sysctl_tcp_pacing_ca_ratio = 120;\n\tif (net != &init_net) {\n\t\tmemcpy(net->ipv4.sysctl_tcp_rmem,\n\t\t       init_net.ipv4.sysctl_tcp_rmem,\n\t\t       sizeof(init_net.ipv4.sysctl_tcp_rmem));\n\t\tmemcpy(net->ipv4.sysctl_tcp_wmem,\n\t\t       init_net.ipv4.sysctl_tcp_wmem,\n\t\t       sizeof(init_net.ipv4.sysctl_tcp_wmem));\n\t}\n\tnet->ipv4.sysctl_tcp_comp_sack_delay_ns = NSEC_PER_MSEC;\n\tnet->ipv4.sysctl_tcp_comp_sack_slack_ns = 100 * NSEC_PER_USEC;\n\tnet->ipv4.sysctl_tcp_comp_sack_nr = 44;\n\tnet->ipv4.sysctl_tcp_fastopen = TFO_CLIENT_ENABLE;\n\tnet->ipv4.sysctl_tcp_fastopen_blackhole_timeout = 0;\n\tatomic_set(&net->ipv4.tfo_active_disable_times, 0);\n\n\t \n\tnet->ipv4.sysctl_tcp_plb_enabled = 0;  \n\tnet->ipv4.sysctl_tcp_plb_idle_rehash_rounds = 3;\n\tnet->ipv4.sysctl_tcp_plb_rehash_rounds = 12;\n\tnet->ipv4.sysctl_tcp_plb_suspend_rto_sec = 60;\n\t \n\tnet->ipv4.sysctl_tcp_plb_cong_thresh = (1 << TCP_PLB_SCALE) / 2;\n\n\t \n\tif (!net_eq(net, &init_net) &&\n\t    bpf_try_module_get(init_net.ipv4.tcp_congestion_control,\n\t\t\t       init_net.ipv4.tcp_congestion_control->owner))\n\t\tnet->ipv4.tcp_congestion_control = init_net.ipv4.tcp_congestion_control;\n\telse\n\t\tnet->ipv4.tcp_congestion_control = &tcp_reno;\n\n\tnet->ipv4.sysctl_tcp_syn_linear_timeouts = 4;\n\tnet->ipv4.sysctl_tcp_shrink_window = 0;\n\n\treturn 0;\n}\n\nstatic void __net_exit tcp_sk_exit_batch(struct list_head *net_exit_list)\n{\n\tstruct net *net;\n\n\ttcp_twsk_purge(net_exit_list, AF_INET);\n\n\tlist_for_each_entry(net, net_exit_list, exit_list) {\n\t\tinet_pernet_hashinfo_free(net->ipv4.tcp_death_row.hashinfo);\n\t\tWARN_ON_ONCE(!refcount_dec_and_test(&net->ipv4.tcp_death_row.tw_refcount));\n\t\ttcp_fastopen_ctx_destroy(net);\n\t}\n}\n\nstatic struct pernet_operations __net_initdata tcp_sk_ops = {\n       .init\t   = tcp_sk_init,\n       .exit\t   = tcp_sk_exit,\n       .exit_batch = tcp_sk_exit_batch,\n};\n\n#if defined(CONFIG_BPF_SYSCALL) && defined(CONFIG_PROC_FS)\nDEFINE_BPF_ITER_FUNC(tcp, struct bpf_iter_meta *meta,\n\t\t     struct sock_common *sk_common, uid_t uid)\n\n#define INIT_BATCH_SZ 16\n\nstatic int bpf_iter_init_tcp(void *priv_data, struct bpf_iter_aux_info *aux)\n{\n\tstruct bpf_tcp_iter_state *iter = priv_data;\n\tint err;\n\n\terr = bpf_iter_init_seq_net(priv_data, aux);\n\tif (err)\n\t\treturn err;\n\n\terr = bpf_iter_tcp_realloc_batch(iter, INIT_BATCH_SZ);\n\tif (err) {\n\t\tbpf_iter_fini_seq_net(priv_data);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic void bpf_iter_fini_tcp(void *priv_data)\n{\n\tstruct bpf_tcp_iter_state *iter = priv_data;\n\n\tbpf_iter_fini_seq_net(priv_data);\n\tkvfree(iter->batch);\n}\n\nstatic const struct bpf_iter_seq_info tcp_seq_info = {\n\t.seq_ops\t\t= &bpf_iter_tcp_seq_ops,\n\t.init_seq_private\t= bpf_iter_init_tcp,\n\t.fini_seq_private\t= bpf_iter_fini_tcp,\n\t.seq_priv_size\t\t= sizeof(struct bpf_tcp_iter_state),\n};\n\nstatic const struct bpf_func_proto *\nbpf_iter_tcp_get_func_proto(enum bpf_func_id func_id,\n\t\t\t    const struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n\tcase BPF_FUNC_setsockopt:\n\t\treturn &bpf_sk_setsockopt_proto;\n\tcase BPF_FUNC_getsockopt:\n\t\treturn &bpf_sk_getsockopt_proto;\n\tdefault:\n\t\treturn NULL;\n\t}\n}\n\nstatic struct bpf_iter_reg tcp_reg_info = {\n\t.target\t\t\t= \"tcp\",\n\t.ctx_arg_info_size\t= 1,\n\t.ctx_arg_info\t\t= {\n\t\t{ offsetof(struct bpf_iter__tcp, sk_common),\n\t\t  PTR_TO_BTF_ID_OR_NULL | PTR_TRUSTED },\n\t},\n\t.get_func_proto\t\t= bpf_iter_tcp_get_func_proto,\n\t.seq_info\t\t= &tcp_seq_info,\n};\n\nstatic void __init bpf_iter_register(void)\n{\n\ttcp_reg_info.ctx_arg_info[0].btf_id = btf_sock_ids[BTF_SOCK_TYPE_SOCK_COMMON];\n\tif (bpf_iter_reg_target(&tcp_reg_info))\n\t\tpr_warn(\"Warning: could not register bpf iterator tcp\\n\");\n}\n\n#endif\n\nvoid __init tcp_v4_init(void)\n{\n\tint cpu, res;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct sock *sk;\n\n\t\tres = inet_ctl_sock_create(&sk, PF_INET, SOCK_RAW,\n\t\t\t\t\t   IPPROTO_TCP, &init_net);\n\t\tif (res)\n\t\t\tpanic(\"Failed to create the TCP control socket.\\n\");\n\t\tsock_set_flag(sk, SOCK_USE_WRITE_QUEUE);\n\n\t\t \n\t\tinet_sk(sk)->pmtudisc = IP_PMTUDISC_DO;\n\n\t\tper_cpu(ipv4_tcp_sk, cpu) = sk;\n\t}\n\tif (register_pernet_subsys(&tcp_sk_ops))\n\t\tpanic(\"Failed to create the TCP control socket.\\n\");\n\n#if defined(CONFIG_BPF_SYSCALL) && defined(CONFIG_PROC_FS)\n\tbpf_iter_register();\n#endif\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}