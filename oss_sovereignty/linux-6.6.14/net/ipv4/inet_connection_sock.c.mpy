{
  "module_name": "inet_connection_sock.c",
  "hash_id": "5a177e168184c672520711a58d1d03256f83cfc40f69b12c458f588080faea35",
  "original_prompt": "Ingested from linux-6.6.14/net/ipv4/inet_connection_sock.c",
  "human_readable_source": "\n \n\n#include <linux/module.h>\n#include <linux/jhash.h>\n\n#include <net/inet_connection_sock.h>\n#include <net/inet_hashtables.h>\n#include <net/inet_timewait_sock.h>\n#include <net/ip.h>\n#include <net/route.h>\n#include <net/tcp_states.h>\n#include <net/xfrm.h>\n#include <net/tcp.h>\n#include <net/sock_reuseport.h>\n#include <net/addrconf.h>\n\n#if IS_ENABLED(CONFIG_IPV6)\n \nstatic bool ipv6_rcv_saddr_equal(const struct in6_addr *sk1_rcv_saddr6,\n\t\t\t\t const struct in6_addr *sk2_rcv_saddr6,\n\t\t\t\t __be32 sk1_rcv_saddr, __be32 sk2_rcv_saddr,\n\t\t\t\t bool sk1_ipv6only, bool sk2_ipv6only,\n\t\t\t\t bool match_sk1_wildcard,\n\t\t\t\t bool match_sk2_wildcard)\n{\n\tint addr_type = ipv6_addr_type(sk1_rcv_saddr6);\n\tint addr_type2 = sk2_rcv_saddr6 ? ipv6_addr_type(sk2_rcv_saddr6) : IPV6_ADDR_MAPPED;\n\n\t \n\tif (addr_type == IPV6_ADDR_MAPPED && addr_type2 == IPV6_ADDR_MAPPED) {\n\t\tif (!sk2_ipv6only) {\n\t\t\tif (sk1_rcv_saddr == sk2_rcv_saddr)\n\t\t\t\treturn true;\n\t\t\treturn (match_sk1_wildcard && !sk1_rcv_saddr) ||\n\t\t\t\t(match_sk2_wildcard && !sk2_rcv_saddr);\n\t\t}\n\t\treturn false;\n\t}\n\n\tif (addr_type == IPV6_ADDR_ANY && addr_type2 == IPV6_ADDR_ANY)\n\t\treturn true;\n\n\tif (addr_type2 == IPV6_ADDR_ANY && match_sk2_wildcard &&\n\t    !(sk2_ipv6only && addr_type == IPV6_ADDR_MAPPED))\n\t\treturn true;\n\n\tif (addr_type == IPV6_ADDR_ANY && match_sk1_wildcard &&\n\t    !(sk1_ipv6only && addr_type2 == IPV6_ADDR_MAPPED))\n\t\treturn true;\n\n\tif (sk2_rcv_saddr6 &&\n\t    ipv6_addr_equal(sk1_rcv_saddr6, sk2_rcv_saddr6))\n\t\treturn true;\n\n\treturn false;\n}\n#endif\n\n \nstatic bool ipv4_rcv_saddr_equal(__be32 sk1_rcv_saddr, __be32 sk2_rcv_saddr,\n\t\t\t\t bool sk2_ipv6only, bool match_sk1_wildcard,\n\t\t\t\t bool match_sk2_wildcard)\n{\n\tif (!sk2_ipv6only) {\n\t\tif (sk1_rcv_saddr == sk2_rcv_saddr)\n\t\t\treturn true;\n\t\treturn (match_sk1_wildcard && !sk1_rcv_saddr) ||\n\t\t\t(match_sk2_wildcard && !sk2_rcv_saddr);\n\t}\n\treturn false;\n}\n\nbool inet_rcv_saddr_equal(const struct sock *sk, const struct sock *sk2,\n\t\t\t  bool match_wildcard)\n{\n#if IS_ENABLED(CONFIG_IPV6)\n\tif (sk->sk_family == AF_INET6)\n\t\treturn ipv6_rcv_saddr_equal(&sk->sk_v6_rcv_saddr,\n\t\t\t\t\t    inet6_rcv_saddr(sk2),\n\t\t\t\t\t    sk->sk_rcv_saddr,\n\t\t\t\t\t    sk2->sk_rcv_saddr,\n\t\t\t\t\t    ipv6_only_sock(sk),\n\t\t\t\t\t    ipv6_only_sock(sk2),\n\t\t\t\t\t    match_wildcard,\n\t\t\t\t\t    match_wildcard);\n#endif\n\treturn ipv4_rcv_saddr_equal(sk->sk_rcv_saddr, sk2->sk_rcv_saddr,\n\t\t\t\t    ipv6_only_sock(sk2), match_wildcard,\n\t\t\t\t    match_wildcard);\n}\nEXPORT_SYMBOL(inet_rcv_saddr_equal);\n\nbool inet_rcv_saddr_any(const struct sock *sk)\n{\n#if IS_ENABLED(CONFIG_IPV6)\n\tif (sk->sk_family == AF_INET6)\n\t\treturn ipv6_addr_any(&sk->sk_v6_rcv_saddr);\n#endif\n\treturn !sk->sk_rcv_saddr;\n}\n\nvoid inet_get_local_port_range(const struct net *net, int *low, int *high)\n{\n\tunsigned int seq;\n\n\tdo {\n\t\tseq = read_seqbegin(&net->ipv4.ip_local_ports.lock);\n\n\t\t*low = net->ipv4.ip_local_ports.range[0];\n\t\t*high = net->ipv4.ip_local_ports.range[1];\n\t} while (read_seqretry(&net->ipv4.ip_local_ports.lock, seq));\n}\nEXPORT_SYMBOL(inet_get_local_port_range);\n\nvoid inet_sk_get_local_port_range(const struct sock *sk, int *low, int *high)\n{\n\tconst struct inet_sock *inet = inet_sk(sk);\n\tconst struct net *net = sock_net(sk);\n\tint lo, hi, sk_lo, sk_hi;\n\n\tinet_get_local_port_range(net, &lo, &hi);\n\n\tsk_lo = inet->local_port_range.lo;\n\tsk_hi = inet->local_port_range.hi;\n\n\tif (unlikely(lo <= sk_lo && sk_lo <= hi))\n\t\tlo = sk_lo;\n\tif (unlikely(lo <= sk_hi && sk_hi <= hi))\n\t\thi = sk_hi;\n\n\t*low = lo;\n\t*high = hi;\n}\nEXPORT_SYMBOL(inet_sk_get_local_port_range);\n\nstatic bool inet_use_bhash2_on_bind(const struct sock *sk)\n{\n#if IS_ENABLED(CONFIG_IPV6)\n\tif (sk->sk_family == AF_INET6) {\n\t\tint addr_type = ipv6_addr_type(&sk->sk_v6_rcv_saddr);\n\n\t\treturn addr_type != IPV6_ADDR_ANY &&\n\t\t\taddr_type != IPV6_ADDR_MAPPED;\n\t}\n#endif\n\treturn sk->sk_rcv_saddr != htonl(INADDR_ANY);\n}\n\nstatic bool inet_bind_conflict(const struct sock *sk, struct sock *sk2,\n\t\t\t       kuid_t sk_uid, bool relax,\n\t\t\t       bool reuseport_cb_ok, bool reuseport_ok)\n{\n\tint bound_dev_if2;\n\n\tif (sk == sk2)\n\t\treturn false;\n\n\tbound_dev_if2 = READ_ONCE(sk2->sk_bound_dev_if);\n\n\tif (!sk->sk_bound_dev_if || !bound_dev_if2 ||\n\t    sk->sk_bound_dev_if == bound_dev_if2) {\n\t\tif (sk->sk_reuse && sk2->sk_reuse &&\n\t\t    sk2->sk_state != TCP_LISTEN) {\n\t\t\tif (!relax || (!reuseport_ok && sk->sk_reuseport &&\n\t\t\t\t       sk2->sk_reuseport && reuseport_cb_ok &&\n\t\t\t\t       (sk2->sk_state == TCP_TIME_WAIT ||\n\t\t\t\t\tuid_eq(sk_uid, sock_i_uid(sk2)))))\n\t\t\t\treturn true;\n\t\t} else if (!reuseport_ok || !sk->sk_reuseport ||\n\t\t\t   !sk2->sk_reuseport || !reuseport_cb_ok ||\n\t\t\t   (sk2->sk_state != TCP_TIME_WAIT &&\n\t\t\t    !uid_eq(sk_uid, sock_i_uid(sk2)))) {\n\t\t\treturn true;\n\t\t}\n\t}\n\treturn false;\n}\n\nstatic bool __inet_bhash2_conflict(const struct sock *sk, struct sock *sk2,\n\t\t\t\t   kuid_t sk_uid, bool relax,\n\t\t\t\t   bool reuseport_cb_ok, bool reuseport_ok)\n{\n\tif (sk->sk_family == AF_INET && ipv6_only_sock(sk2))\n\t\treturn false;\n\n\treturn inet_bind_conflict(sk, sk2, sk_uid, relax,\n\t\t\t\t  reuseport_cb_ok, reuseport_ok);\n}\n\nstatic bool inet_bhash2_conflict(const struct sock *sk,\n\t\t\t\t const struct inet_bind2_bucket *tb2,\n\t\t\t\t kuid_t sk_uid,\n\t\t\t\t bool relax, bool reuseport_cb_ok,\n\t\t\t\t bool reuseport_ok)\n{\n\tstruct inet_timewait_sock *tw2;\n\tstruct sock *sk2;\n\n\tsk_for_each_bound_bhash2(sk2, &tb2->owners) {\n\t\tif (__inet_bhash2_conflict(sk, sk2, sk_uid, relax,\n\t\t\t\t\t   reuseport_cb_ok, reuseport_ok))\n\t\t\treturn true;\n\t}\n\n\ttwsk_for_each_bound_bhash2(tw2, &tb2->deathrow) {\n\t\tsk2 = (struct sock *)tw2;\n\n\t\tif (__inet_bhash2_conflict(sk, sk2, sk_uid, relax,\n\t\t\t\t\t   reuseport_cb_ok, reuseport_ok))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nstatic int inet_csk_bind_conflict(const struct sock *sk,\n\t\t\t\t  const struct inet_bind_bucket *tb,\n\t\t\t\t  const struct inet_bind2_bucket *tb2,  \n\t\t\t\t  bool relax, bool reuseport_ok)\n{\n\tbool reuseport_cb_ok;\n\tstruct sock_reuseport *reuseport_cb;\n\tkuid_t uid = sock_i_uid((struct sock *)sk);\n\n\trcu_read_lock();\n\treuseport_cb = rcu_dereference(sk->sk_reuseport_cb);\n\t \n\treuseport_cb_ok = !reuseport_cb || READ_ONCE(reuseport_cb->num_closed_socks);\n\trcu_read_unlock();\n\n\t \n\n\tif (!inet_use_bhash2_on_bind(sk)) {\n\t\tstruct sock *sk2;\n\n\t\tsk_for_each_bound(sk2, &tb->owners)\n\t\t\tif (inet_bind_conflict(sk, sk2, uid, relax,\n\t\t\t\t\t       reuseport_cb_ok, reuseport_ok) &&\n\t\t\t    inet_rcv_saddr_equal(sk, sk2, true))\n\t\t\t\treturn true;\n\n\t\treturn false;\n\t}\n\n\t \n\treturn tb2 && inet_bhash2_conflict(sk, tb2, uid, relax, reuseport_cb_ok,\n\t\t\t\t\t   reuseport_ok);\n}\n\n \nstatic bool inet_bhash2_addr_any_conflict(const struct sock *sk, int port, int l3mdev,\n\t\t\t\t\t  bool relax, bool reuseport_ok)\n{\n\tkuid_t uid = sock_i_uid((struct sock *)sk);\n\tconst struct net *net = sock_net(sk);\n\tstruct sock_reuseport *reuseport_cb;\n\tstruct inet_bind_hashbucket *head2;\n\tstruct inet_bind2_bucket *tb2;\n\tbool reuseport_cb_ok;\n\n\trcu_read_lock();\n\treuseport_cb = rcu_dereference(sk->sk_reuseport_cb);\n\t \n\treuseport_cb_ok = !reuseport_cb || READ_ONCE(reuseport_cb->num_closed_socks);\n\trcu_read_unlock();\n\n\thead2 = inet_bhash2_addr_any_hashbucket(sk, net, port);\n\n\tspin_lock(&head2->lock);\n\n\tinet_bind_bucket_for_each(tb2, &head2->chain)\n\t\tif (inet_bind2_bucket_match_addr_any(tb2, net, port, l3mdev, sk))\n\t\t\tbreak;\n\n\tif (tb2 && inet_bhash2_conflict(sk, tb2, uid, relax, reuseport_cb_ok,\n\t\t\t\t\treuseport_ok)) {\n\t\tspin_unlock(&head2->lock);\n\t\treturn true;\n\t}\n\n\tspin_unlock(&head2->lock);\n\treturn false;\n}\n\n \nstatic struct inet_bind_hashbucket *\ninet_csk_find_open_port(const struct sock *sk, struct inet_bind_bucket **tb_ret,\n\t\t\tstruct inet_bind2_bucket **tb2_ret,\n\t\t\tstruct inet_bind_hashbucket **head2_ret, int *port_ret)\n{\n\tstruct inet_hashinfo *hinfo = tcp_or_dccp_get_hashinfo(sk);\n\tint i, low, high, attempt_half, port, l3mdev;\n\tstruct inet_bind_hashbucket *head, *head2;\n\tstruct net *net = sock_net(sk);\n\tstruct inet_bind2_bucket *tb2;\n\tstruct inet_bind_bucket *tb;\n\tu32 remaining, offset;\n\tbool relax = false;\n\n\tl3mdev = inet_sk_bound_l3mdev(sk);\nports_exhausted:\n\tattempt_half = (sk->sk_reuse == SK_CAN_REUSE) ? 1 : 0;\nother_half_scan:\n\tinet_sk_get_local_port_range(sk, &low, &high);\n\thigh++;  \n\tif (high - low < 4)\n\t\tattempt_half = 0;\n\tif (attempt_half) {\n\t\tint half = low + (((high - low) >> 2) << 1);\n\n\t\tif (attempt_half == 1)\n\t\t\thigh = half;\n\t\telse\n\t\t\tlow = half;\n\t}\n\tremaining = high - low;\n\tif (likely(remaining > 1))\n\t\tremaining &= ~1U;\n\n\toffset = get_random_u32_below(remaining);\n\t \n\toffset |= 1U;\n\nother_parity_scan:\n\tport = low + offset;\n\tfor (i = 0; i < remaining; i += 2, port += 2) {\n\t\tif (unlikely(port >= high))\n\t\t\tport -= remaining;\n\t\tif (inet_is_local_reserved_port(net, port))\n\t\t\tcontinue;\n\t\thead = &hinfo->bhash[inet_bhashfn(net, port,\n\t\t\t\t\t\t  hinfo->bhash_size)];\n\t\tspin_lock_bh(&head->lock);\n\t\tif (inet_use_bhash2_on_bind(sk)) {\n\t\t\tif (inet_bhash2_addr_any_conflict(sk, port, l3mdev, relax, false))\n\t\t\t\tgoto next_port;\n\t\t}\n\n\t\thead2 = inet_bhashfn_portaddr(hinfo, sk, net, port);\n\t\tspin_lock(&head2->lock);\n\t\ttb2 = inet_bind2_bucket_find(head2, net, port, l3mdev, sk);\n\t\tinet_bind_bucket_for_each(tb, &head->chain)\n\t\t\tif (inet_bind_bucket_match(tb, net, port, l3mdev)) {\n\t\t\t\tif (!inet_csk_bind_conflict(sk, tb, tb2,\n\t\t\t\t\t\t\t    relax, false))\n\t\t\t\t\tgoto success;\n\t\t\t\tspin_unlock(&head2->lock);\n\t\t\t\tgoto next_port;\n\t\t\t}\n\t\ttb = NULL;\n\t\tgoto success;\nnext_port:\n\t\tspin_unlock_bh(&head->lock);\n\t\tcond_resched();\n\t}\n\n\toffset--;\n\tif (!(offset & 1))\n\t\tgoto other_parity_scan;\n\n\tif (attempt_half == 1) {\n\t\t \n\t\tattempt_half = 2;\n\t\tgoto other_half_scan;\n\t}\n\n\tif (READ_ONCE(net->ipv4.sysctl_ip_autobind_reuse) && !relax) {\n\t\t \n\t\trelax = true;\n\t\tgoto ports_exhausted;\n\t}\n\treturn NULL;\nsuccess:\n\t*port_ret = port;\n\t*tb_ret = tb;\n\t*tb2_ret = tb2;\n\t*head2_ret = head2;\n\treturn head;\n}\n\nstatic inline int sk_reuseport_match(struct inet_bind_bucket *tb,\n\t\t\t\t     struct sock *sk)\n{\n\tkuid_t uid = sock_i_uid(sk);\n\n\tif (tb->fastreuseport <= 0)\n\t\treturn 0;\n\tif (!sk->sk_reuseport)\n\t\treturn 0;\n\tif (rcu_access_pointer(sk->sk_reuseport_cb))\n\t\treturn 0;\n\tif (!uid_eq(tb->fastuid, uid))\n\t\treturn 0;\n\t \n\tif (tb->fastreuseport == FASTREUSEPORT_ANY)\n\t\treturn 1;\n#if IS_ENABLED(CONFIG_IPV6)\n\tif (tb->fast_sk_family == AF_INET6)\n\t\treturn ipv6_rcv_saddr_equal(&tb->fast_v6_rcv_saddr,\n\t\t\t\t\t    inet6_rcv_saddr(sk),\n\t\t\t\t\t    tb->fast_rcv_saddr,\n\t\t\t\t\t    sk->sk_rcv_saddr,\n\t\t\t\t\t    tb->fast_ipv6_only,\n\t\t\t\t\t    ipv6_only_sock(sk), true, false);\n#endif\n\treturn ipv4_rcv_saddr_equal(tb->fast_rcv_saddr, sk->sk_rcv_saddr,\n\t\t\t\t    ipv6_only_sock(sk), true, false);\n}\n\nvoid inet_csk_update_fastreuse(struct inet_bind_bucket *tb,\n\t\t\t       struct sock *sk)\n{\n\tkuid_t uid = sock_i_uid(sk);\n\tbool reuse = sk->sk_reuse && sk->sk_state != TCP_LISTEN;\n\n\tif (hlist_empty(&tb->owners)) {\n\t\ttb->fastreuse = reuse;\n\t\tif (sk->sk_reuseport) {\n\t\t\ttb->fastreuseport = FASTREUSEPORT_ANY;\n\t\t\ttb->fastuid = uid;\n\t\t\ttb->fast_rcv_saddr = sk->sk_rcv_saddr;\n\t\t\ttb->fast_ipv6_only = ipv6_only_sock(sk);\n\t\t\ttb->fast_sk_family = sk->sk_family;\n#if IS_ENABLED(CONFIG_IPV6)\n\t\t\ttb->fast_v6_rcv_saddr = sk->sk_v6_rcv_saddr;\n#endif\n\t\t} else {\n\t\t\ttb->fastreuseport = 0;\n\t\t}\n\t} else {\n\t\tif (!reuse)\n\t\t\ttb->fastreuse = 0;\n\t\tif (sk->sk_reuseport) {\n\t\t\t \n\t\t\tif (!sk_reuseport_match(tb, sk)) {\n\t\t\t\ttb->fastreuseport = FASTREUSEPORT_STRICT;\n\t\t\t\ttb->fastuid = uid;\n\t\t\t\ttb->fast_rcv_saddr = sk->sk_rcv_saddr;\n\t\t\t\ttb->fast_ipv6_only = ipv6_only_sock(sk);\n\t\t\t\ttb->fast_sk_family = sk->sk_family;\n#if IS_ENABLED(CONFIG_IPV6)\n\t\t\t\ttb->fast_v6_rcv_saddr = sk->sk_v6_rcv_saddr;\n#endif\n\t\t\t}\n\t\t} else {\n\t\t\ttb->fastreuseport = 0;\n\t\t}\n\t}\n}\n\n \nint inet_csk_get_port(struct sock *sk, unsigned short snum)\n{\n\tstruct inet_hashinfo *hinfo = tcp_or_dccp_get_hashinfo(sk);\n\tbool reuse = sk->sk_reuse && sk->sk_state != TCP_LISTEN;\n\tbool found_port = false, check_bind_conflict = true;\n\tbool bhash_created = false, bhash2_created = false;\n\tint ret = -EADDRINUSE, port = snum, l3mdev;\n\tstruct inet_bind_hashbucket *head, *head2;\n\tstruct inet_bind2_bucket *tb2 = NULL;\n\tstruct inet_bind_bucket *tb = NULL;\n\tbool head2_lock_acquired = false;\n\tstruct net *net = sock_net(sk);\n\n\tl3mdev = inet_sk_bound_l3mdev(sk);\n\n\tif (!port) {\n\t\thead = inet_csk_find_open_port(sk, &tb, &tb2, &head2, &port);\n\t\tif (!head)\n\t\t\treturn ret;\n\n\t\thead2_lock_acquired = true;\n\n\t\tif (tb && tb2)\n\t\t\tgoto success;\n\t\tfound_port = true;\n\t} else {\n\t\thead = &hinfo->bhash[inet_bhashfn(net, port,\n\t\t\t\t\t\t  hinfo->bhash_size)];\n\t\tspin_lock_bh(&head->lock);\n\t\tinet_bind_bucket_for_each(tb, &head->chain)\n\t\t\tif (inet_bind_bucket_match(tb, net, port, l3mdev))\n\t\t\t\tbreak;\n\t}\n\n\tif (!tb) {\n\t\ttb = inet_bind_bucket_create(hinfo->bind_bucket_cachep, net,\n\t\t\t\t\t     head, port, l3mdev);\n\t\tif (!tb)\n\t\t\tgoto fail_unlock;\n\t\tbhash_created = true;\n\t}\n\n\tif (!found_port) {\n\t\tif (!hlist_empty(&tb->owners)) {\n\t\t\tif (sk->sk_reuse == SK_FORCE_REUSE ||\n\t\t\t    (tb->fastreuse > 0 && reuse) ||\n\t\t\t    sk_reuseport_match(tb, sk))\n\t\t\t\tcheck_bind_conflict = false;\n\t\t}\n\n\t\tif (check_bind_conflict && inet_use_bhash2_on_bind(sk)) {\n\t\t\tif (inet_bhash2_addr_any_conflict(sk, port, l3mdev, true, true))\n\t\t\t\tgoto fail_unlock;\n\t\t}\n\n\t\thead2 = inet_bhashfn_portaddr(hinfo, sk, net, port);\n\t\tspin_lock(&head2->lock);\n\t\thead2_lock_acquired = true;\n\t\ttb2 = inet_bind2_bucket_find(head2, net, port, l3mdev, sk);\n\t}\n\n\tif (!tb2) {\n\t\ttb2 = inet_bind2_bucket_create(hinfo->bind2_bucket_cachep,\n\t\t\t\t\t       net, head2, port, l3mdev, sk);\n\t\tif (!tb2)\n\t\t\tgoto fail_unlock;\n\t\tbhash2_created = true;\n\t}\n\n\tif (!found_port && check_bind_conflict) {\n\t\tif (inet_csk_bind_conflict(sk, tb, tb2, true, true))\n\t\t\tgoto fail_unlock;\n\t}\n\nsuccess:\n\tinet_csk_update_fastreuse(tb, sk);\n\n\tif (!inet_csk(sk)->icsk_bind_hash)\n\t\tinet_bind_hash(sk, tb, tb2, port);\n\tWARN_ON(inet_csk(sk)->icsk_bind_hash != tb);\n\tWARN_ON(inet_csk(sk)->icsk_bind2_hash != tb2);\n\tret = 0;\n\nfail_unlock:\n\tif (ret) {\n\t\tif (bhash_created)\n\t\t\tinet_bind_bucket_destroy(hinfo->bind_bucket_cachep, tb);\n\t\tif (bhash2_created)\n\t\t\tinet_bind2_bucket_destroy(hinfo->bind2_bucket_cachep,\n\t\t\t\t\t\t  tb2);\n\t}\n\tif (head2_lock_acquired)\n\t\tspin_unlock(&head2->lock);\n\tspin_unlock_bh(&head->lock);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(inet_csk_get_port);\n\n \nstatic int inet_csk_wait_for_connect(struct sock *sk, long timeo)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tDEFINE_WAIT(wait);\n\tint err;\n\n\t \n\tfor (;;) {\n\t\tprepare_to_wait_exclusive(sk_sleep(sk), &wait,\n\t\t\t\t\t  TASK_INTERRUPTIBLE);\n\t\trelease_sock(sk);\n\t\tif (reqsk_queue_empty(&icsk->icsk_accept_queue))\n\t\t\ttimeo = schedule_timeout(timeo);\n\t\tsched_annotate_sleep();\n\t\tlock_sock(sk);\n\t\terr = 0;\n\t\tif (!reqsk_queue_empty(&icsk->icsk_accept_queue))\n\t\t\tbreak;\n\t\terr = -EINVAL;\n\t\tif (sk->sk_state != TCP_LISTEN)\n\t\t\tbreak;\n\t\terr = sock_intr_errno(timeo);\n\t\tif (signal_pending(current))\n\t\t\tbreak;\n\t\terr = -EAGAIN;\n\t\tif (!timeo)\n\t\t\tbreak;\n\t}\n\tfinish_wait(sk_sleep(sk), &wait);\n\treturn err;\n}\n\n \nstruct sock *inet_csk_accept(struct sock *sk, int flags, int *err, bool kern)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct request_sock_queue *queue = &icsk->icsk_accept_queue;\n\tstruct request_sock *req;\n\tstruct sock *newsk;\n\tint error;\n\n\tlock_sock(sk);\n\n\t \n\terror = -EINVAL;\n\tif (sk->sk_state != TCP_LISTEN)\n\t\tgoto out_err;\n\n\t \n\tif (reqsk_queue_empty(queue)) {\n\t\tlong timeo = sock_rcvtimeo(sk, flags & O_NONBLOCK);\n\n\t\t \n\t\terror = -EAGAIN;\n\t\tif (!timeo)\n\t\t\tgoto out_err;\n\n\t\terror = inet_csk_wait_for_connect(sk, timeo);\n\t\tif (error)\n\t\t\tgoto out_err;\n\t}\n\treq = reqsk_queue_remove(queue, sk);\n\tnewsk = req->sk;\n\n\tif (sk->sk_protocol == IPPROTO_TCP &&\n\t    tcp_rsk(req)->tfo_listener) {\n\t\tspin_lock_bh(&queue->fastopenq.lock);\n\t\tif (tcp_rsk(req)->tfo_listener) {\n\t\t\t \n\t\t\treq->sk = NULL;\n\t\t\treq = NULL;\n\t\t}\n\t\tspin_unlock_bh(&queue->fastopenq.lock);\n\t}\n\nout:\n\trelease_sock(sk);\n\tif (newsk && mem_cgroup_sockets_enabled) {\n\t\tint amt = 0;\n\n\t\t \n\t\tlock_sock(newsk);\n\n\t\tmem_cgroup_sk_alloc(newsk);\n\t\tif (newsk->sk_memcg) {\n\t\t\t \n\t\t\tamt = sk_mem_pages(newsk->sk_forward_alloc +\n\t\t\t\t\t   atomic_read(&newsk->sk_rmem_alloc));\n\t\t}\n\n\t\tif (amt)\n\t\t\tmem_cgroup_charge_skmem(newsk->sk_memcg, amt,\n\t\t\t\t\t\tGFP_KERNEL | __GFP_NOFAIL);\n\n\t\trelease_sock(newsk);\n\t}\n\tif (req)\n\t\treqsk_put(req);\n\treturn newsk;\nout_err:\n\tnewsk = NULL;\n\treq = NULL;\n\t*err = error;\n\tgoto out;\n}\nEXPORT_SYMBOL(inet_csk_accept);\n\n \nvoid inet_csk_init_xmit_timers(struct sock *sk,\n\t\t\t       void (*retransmit_handler)(struct timer_list *t),\n\t\t\t       void (*delack_handler)(struct timer_list *t),\n\t\t\t       void (*keepalive_handler)(struct timer_list *t))\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\ttimer_setup(&icsk->icsk_retransmit_timer, retransmit_handler, 0);\n\ttimer_setup(&icsk->icsk_delack_timer, delack_handler, 0);\n\ttimer_setup(&sk->sk_timer, keepalive_handler, 0);\n\ticsk->icsk_pending = icsk->icsk_ack.pending = 0;\n}\nEXPORT_SYMBOL(inet_csk_init_xmit_timers);\n\nvoid inet_csk_clear_xmit_timers(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\ticsk->icsk_pending = icsk->icsk_ack.pending = 0;\n\n\tsk_stop_timer(sk, &icsk->icsk_retransmit_timer);\n\tsk_stop_timer(sk, &icsk->icsk_delack_timer);\n\tsk_stop_timer(sk, &sk->sk_timer);\n}\nEXPORT_SYMBOL(inet_csk_clear_xmit_timers);\n\nvoid inet_csk_delete_keepalive_timer(struct sock *sk)\n{\n\tsk_stop_timer(sk, &sk->sk_timer);\n}\nEXPORT_SYMBOL(inet_csk_delete_keepalive_timer);\n\nvoid inet_csk_reset_keepalive_timer(struct sock *sk, unsigned long len)\n{\n\tsk_reset_timer(sk, &sk->sk_timer, jiffies + len);\n}\nEXPORT_SYMBOL(inet_csk_reset_keepalive_timer);\n\nstruct dst_entry *inet_csk_route_req(const struct sock *sk,\n\t\t\t\t     struct flowi4 *fl4,\n\t\t\t\t     const struct request_sock *req)\n{\n\tconst struct inet_request_sock *ireq = inet_rsk(req);\n\tstruct net *net = read_pnet(&ireq->ireq_net);\n\tstruct ip_options_rcu *opt;\n\tstruct rtable *rt;\n\n\trcu_read_lock();\n\topt = rcu_dereference(ireq->ireq_opt);\n\n\tflowi4_init_output(fl4, ireq->ir_iif, ireq->ir_mark,\n\t\t\t   ip_sock_rt_tos(sk), ip_sock_rt_scope(sk),\n\t\t\t   sk->sk_protocol, inet_sk_flowi_flags(sk),\n\t\t\t   (opt && opt->opt.srr) ? opt->opt.faddr : ireq->ir_rmt_addr,\n\t\t\t   ireq->ir_loc_addr, ireq->ir_rmt_port,\n\t\t\t   htons(ireq->ir_num), sk->sk_uid);\n\tsecurity_req_classify_flow(req, flowi4_to_flowi_common(fl4));\n\trt = ip_route_output_flow(net, fl4, sk);\n\tif (IS_ERR(rt))\n\t\tgoto no_route;\n\tif (opt && opt->opt.is_strictroute && rt->rt_uses_gateway)\n\t\tgoto route_err;\n\trcu_read_unlock();\n\treturn &rt->dst;\n\nroute_err:\n\tip_rt_put(rt);\nno_route:\n\trcu_read_unlock();\n\t__IP_INC_STATS(net, IPSTATS_MIB_OUTNOROUTES);\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(inet_csk_route_req);\n\nstruct dst_entry *inet_csk_route_child_sock(const struct sock *sk,\n\t\t\t\t\t    struct sock *newsk,\n\t\t\t\t\t    const struct request_sock *req)\n{\n\tconst struct inet_request_sock *ireq = inet_rsk(req);\n\tstruct net *net = read_pnet(&ireq->ireq_net);\n\tstruct inet_sock *newinet = inet_sk(newsk);\n\tstruct ip_options_rcu *opt;\n\tstruct flowi4 *fl4;\n\tstruct rtable *rt;\n\n\topt = rcu_dereference(ireq->ireq_opt);\n\tfl4 = &newinet->cork.fl.u.ip4;\n\n\tflowi4_init_output(fl4, ireq->ir_iif, ireq->ir_mark,\n\t\t\t   ip_sock_rt_tos(sk), ip_sock_rt_scope(sk),\n\t\t\t   sk->sk_protocol, inet_sk_flowi_flags(sk),\n\t\t\t   (opt && opt->opt.srr) ? opt->opt.faddr : ireq->ir_rmt_addr,\n\t\t\t   ireq->ir_loc_addr, ireq->ir_rmt_port,\n\t\t\t   htons(ireq->ir_num), sk->sk_uid);\n\tsecurity_req_classify_flow(req, flowi4_to_flowi_common(fl4));\n\trt = ip_route_output_flow(net, fl4, sk);\n\tif (IS_ERR(rt))\n\t\tgoto no_route;\n\tif (opt && opt->opt.is_strictroute && rt->rt_uses_gateway)\n\t\tgoto route_err;\n\treturn &rt->dst;\n\nroute_err:\n\tip_rt_put(rt);\nno_route:\n\t__IP_INC_STATS(net, IPSTATS_MIB_OUTNOROUTES);\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(inet_csk_route_child_sock);\n\n \nstatic void syn_ack_recalc(struct request_sock *req,\n\t\t\t   const int max_syn_ack_retries,\n\t\t\t   const u8 rskq_defer_accept,\n\t\t\t   int *expire, int *resend)\n{\n\tif (!rskq_defer_accept) {\n\t\t*expire = req->num_timeout >= max_syn_ack_retries;\n\t\t*resend = 1;\n\t\treturn;\n\t}\n\t*expire = req->num_timeout >= max_syn_ack_retries &&\n\t\t  (!inet_rsk(req)->acked || req->num_timeout >= rskq_defer_accept);\n\t \n\t*resend = !inet_rsk(req)->acked ||\n\t\t  req->num_timeout >= rskq_defer_accept - 1;\n}\n\nint inet_rtx_syn_ack(const struct sock *parent, struct request_sock *req)\n{\n\tint err = req->rsk_ops->rtx_syn_ack(parent, req);\n\n\tif (!err)\n\t\treq->num_retrans++;\n\treturn err;\n}\nEXPORT_SYMBOL(inet_rtx_syn_ack);\n\nstatic struct request_sock *inet_reqsk_clone(struct request_sock *req,\n\t\t\t\t\t     struct sock *sk)\n{\n\tstruct sock *req_sk, *nreq_sk;\n\tstruct request_sock *nreq;\n\n\tnreq = kmem_cache_alloc(req->rsk_ops->slab, GFP_ATOMIC | __GFP_NOWARN);\n\tif (!nreq) {\n\t\t__NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPMIGRATEREQFAILURE);\n\n\t\t \n\t\tsock_put(sk);\n\t\treturn NULL;\n\t}\n\n\treq_sk = req_to_sk(req);\n\tnreq_sk = req_to_sk(nreq);\n\n\tmemcpy(nreq_sk, req_sk,\n\t       offsetof(struct sock, sk_dontcopy_begin));\n\tmemcpy(&nreq_sk->sk_dontcopy_end, &req_sk->sk_dontcopy_end,\n\t       req->rsk_ops->obj_size - offsetof(struct sock, sk_dontcopy_end));\n\n\tsk_node_init(&nreq_sk->sk_node);\n\tnreq_sk->sk_tx_queue_mapping = req_sk->sk_tx_queue_mapping;\n#ifdef CONFIG_SOCK_RX_QUEUE_MAPPING\n\tnreq_sk->sk_rx_queue_mapping = req_sk->sk_rx_queue_mapping;\n#endif\n\tnreq_sk->sk_incoming_cpu = req_sk->sk_incoming_cpu;\n\n\tnreq->rsk_listener = sk;\n\n\t \n\tif (sk->sk_protocol == IPPROTO_TCP && tcp_rsk(nreq)->tfo_listener)\n\t\trcu_assign_pointer(tcp_sk(nreq->sk)->fastopen_rsk, nreq);\n\n\treturn nreq;\n}\n\nstatic void reqsk_queue_migrated(struct request_sock_queue *queue,\n\t\t\t\t const struct request_sock *req)\n{\n\tif (req->num_timeout == 0)\n\t\tatomic_inc(&queue->young);\n\tatomic_inc(&queue->qlen);\n}\n\nstatic void reqsk_migrate_reset(struct request_sock *req)\n{\n\treq->saved_syn = NULL;\n#if IS_ENABLED(CONFIG_IPV6)\n\tinet_rsk(req)->ipv6_opt = NULL;\n\tinet_rsk(req)->pktopts = NULL;\n#else\n\tinet_rsk(req)->ireq_opt = NULL;\n#endif\n}\n\n \nstatic bool reqsk_queue_unlink(struct request_sock *req)\n{\n\tstruct sock *sk = req_to_sk(req);\n\tbool found = false;\n\n\tif (sk_hashed(sk)) {\n\t\tstruct inet_hashinfo *hashinfo = tcp_or_dccp_get_hashinfo(sk);\n\t\tspinlock_t *lock = inet_ehash_lockp(hashinfo, req->rsk_hash);\n\n\t\tspin_lock(lock);\n\t\tfound = __sk_nulls_del_node_init_rcu(sk);\n\t\tspin_unlock(lock);\n\t}\n\tif (timer_pending(&req->rsk_timer) && del_timer_sync(&req->rsk_timer))\n\t\treqsk_put(req);\n\treturn found;\n}\n\nbool inet_csk_reqsk_queue_drop(struct sock *sk, struct request_sock *req)\n{\n\tbool unlinked = reqsk_queue_unlink(req);\n\n\tif (unlinked) {\n\t\treqsk_queue_removed(&inet_csk(sk)->icsk_accept_queue, req);\n\t\treqsk_put(req);\n\t}\n\treturn unlinked;\n}\nEXPORT_SYMBOL(inet_csk_reqsk_queue_drop);\n\nvoid inet_csk_reqsk_queue_drop_and_put(struct sock *sk, struct request_sock *req)\n{\n\tinet_csk_reqsk_queue_drop(sk, req);\n\treqsk_put(req);\n}\nEXPORT_SYMBOL(inet_csk_reqsk_queue_drop_and_put);\n\nstatic void reqsk_timer_handler(struct timer_list *t)\n{\n\tstruct request_sock *req = from_timer(req, t, rsk_timer);\n\tstruct request_sock *nreq = NULL, *oreq = req;\n\tstruct sock *sk_listener = req->rsk_listener;\n\tstruct inet_connection_sock *icsk;\n\tstruct request_sock_queue *queue;\n\tstruct net *net;\n\tint max_syn_ack_retries, qlen, expire = 0, resend = 0;\n\n\tif (inet_sk_state_load(sk_listener) != TCP_LISTEN) {\n\t\tstruct sock *nsk;\n\n\t\tnsk = reuseport_migrate_sock(sk_listener, req_to_sk(req), NULL);\n\t\tif (!nsk)\n\t\t\tgoto drop;\n\n\t\tnreq = inet_reqsk_clone(req, nsk);\n\t\tif (!nreq)\n\t\t\tgoto drop;\n\n\t\t \n\t\trefcount_set(&nreq->rsk_refcnt, 2 + 1);\n\t\ttimer_setup(&nreq->rsk_timer, reqsk_timer_handler, TIMER_PINNED);\n\t\treqsk_queue_migrated(&inet_csk(nsk)->icsk_accept_queue, req);\n\n\t\treq = nreq;\n\t\tsk_listener = nsk;\n\t}\n\n\ticsk = inet_csk(sk_listener);\n\tnet = sock_net(sk_listener);\n\tmax_syn_ack_retries = READ_ONCE(icsk->icsk_syn_retries) ? :\n\t\tREAD_ONCE(net->ipv4.sysctl_tcp_synack_retries);\n\t \n\tqueue = &icsk->icsk_accept_queue;\n\tqlen = reqsk_queue_len(queue);\n\tif ((qlen << 1) > max(8U, READ_ONCE(sk_listener->sk_max_ack_backlog))) {\n\t\tint young = reqsk_queue_len_young(queue) << 1;\n\n\t\twhile (max_syn_ack_retries > 2) {\n\t\t\tif (qlen < young)\n\t\t\t\tbreak;\n\t\t\tmax_syn_ack_retries--;\n\t\t\tyoung <<= 1;\n\t\t}\n\t}\n\tsyn_ack_recalc(req, max_syn_ack_retries, READ_ONCE(queue->rskq_defer_accept),\n\t\t       &expire, &resend);\n\treq->rsk_ops->syn_ack_timeout(req);\n\tif (!expire &&\n\t    (!resend ||\n\t     !inet_rtx_syn_ack(sk_listener, req) ||\n\t     inet_rsk(req)->acked)) {\n\t\tif (req->num_timeout++ == 0)\n\t\t\tatomic_dec(&queue->young);\n\t\tmod_timer(&req->rsk_timer, jiffies + reqsk_timeout(req, TCP_RTO_MAX));\n\n\t\tif (!nreq)\n\t\t\treturn;\n\n\t\tif (!inet_ehash_insert(req_to_sk(nreq), req_to_sk(oreq), NULL)) {\n\t\t\t \n\t\t\tinet_csk_reqsk_queue_drop(sk_listener, nreq);\n\t\t\tgoto no_ownership;\n\t\t}\n\n\t\t__NET_INC_STATS(net, LINUX_MIB_TCPMIGRATEREQSUCCESS);\n\t\treqsk_migrate_reset(oreq);\n\t\treqsk_queue_removed(&inet_csk(oreq->rsk_listener)->icsk_accept_queue, oreq);\n\t\treqsk_put(oreq);\n\n\t\treqsk_put(nreq);\n\t\treturn;\n\t}\n\n\t \n\tif (nreq) {\n\t\t__NET_INC_STATS(net, LINUX_MIB_TCPMIGRATEREQFAILURE);\nno_ownership:\n\t\treqsk_migrate_reset(nreq);\n\t\treqsk_queue_removed(queue, nreq);\n\t\t__reqsk_free(nreq);\n\t}\n\ndrop:\n\tinet_csk_reqsk_queue_drop_and_put(oreq->rsk_listener, oreq);\n}\n\nstatic void reqsk_queue_hash_req(struct request_sock *req,\n\t\t\t\t unsigned long timeout)\n{\n\ttimer_setup(&req->rsk_timer, reqsk_timer_handler, TIMER_PINNED);\n\tmod_timer(&req->rsk_timer, jiffies + timeout);\n\n\tinet_ehash_insert(req_to_sk(req), NULL, NULL);\n\t \n\tsmp_wmb();\n\trefcount_set(&req->rsk_refcnt, 2 + 1);\n}\n\nvoid inet_csk_reqsk_queue_hash_add(struct sock *sk, struct request_sock *req,\n\t\t\t\t   unsigned long timeout)\n{\n\treqsk_queue_hash_req(req, timeout);\n\tinet_csk_reqsk_queue_added(sk);\n}\nEXPORT_SYMBOL_GPL(inet_csk_reqsk_queue_hash_add);\n\nstatic void inet_clone_ulp(const struct request_sock *req, struct sock *newsk,\n\t\t\t   const gfp_t priority)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(newsk);\n\n\tif (!icsk->icsk_ulp_ops)\n\t\treturn;\n\n\ticsk->icsk_ulp_ops->clone(req, newsk, priority);\n}\n\n \nstruct sock *inet_csk_clone_lock(const struct sock *sk,\n\t\t\t\t const struct request_sock *req,\n\t\t\t\t const gfp_t priority)\n{\n\tstruct sock *newsk = sk_clone_lock(sk, priority);\n\n\tif (newsk) {\n\t\tstruct inet_connection_sock *newicsk = inet_csk(newsk);\n\n\t\tinet_sk_set_state(newsk, TCP_SYN_RECV);\n\t\tnewicsk->icsk_bind_hash = NULL;\n\t\tnewicsk->icsk_bind2_hash = NULL;\n\n\t\tinet_sk(newsk)->inet_dport = inet_rsk(req)->ir_rmt_port;\n\t\tinet_sk(newsk)->inet_num = inet_rsk(req)->ir_num;\n\t\tinet_sk(newsk)->inet_sport = htons(inet_rsk(req)->ir_num);\n\n\t\t \n\t\tsock_reset_flag(newsk, SOCK_RCU_FREE);\n\n\t\tinet_sk(newsk)->mc_list = NULL;\n\n\t\tnewsk->sk_mark = inet_rsk(req)->ir_mark;\n\t\tatomic64_set(&newsk->sk_cookie,\n\t\t\t     atomic64_read(&inet_rsk(req)->ir_cookie));\n\n\t\tnewicsk->icsk_retransmits = 0;\n\t\tnewicsk->icsk_backoff\t  = 0;\n\t\tnewicsk->icsk_probes_out  = 0;\n\t\tnewicsk->icsk_probes_tstamp = 0;\n\n\t\t \n\t\tmemset(&newicsk->icsk_accept_queue, 0, sizeof(newicsk->icsk_accept_queue));\n\n\t\tinet_clone_ulp(req, newsk, priority);\n\n\t\tsecurity_inet_csk_clone(newsk, req);\n\t}\n\treturn newsk;\n}\nEXPORT_SYMBOL_GPL(inet_csk_clone_lock);\n\n \nvoid inet_csk_destroy_sock(struct sock *sk)\n{\n\tWARN_ON(sk->sk_state != TCP_CLOSE);\n\tWARN_ON(!sock_flag(sk, SOCK_DEAD));\n\n\t \n\tWARN_ON(!sk_unhashed(sk));\n\n\t \n\tWARN_ON(inet_sk(sk)->inet_num && !inet_csk(sk)->icsk_bind_hash);\n\n\tsk->sk_prot->destroy(sk);\n\n\tsk_stream_kill_queues(sk);\n\n\txfrm_sk_free_policy(sk);\n\n\tthis_cpu_dec(*sk->sk_prot->orphan_count);\n\n\tsock_put(sk);\n}\nEXPORT_SYMBOL(inet_csk_destroy_sock);\n\n \nvoid inet_csk_prepare_forced_close(struct sock *sk)\n\t__releases(&sk->sk_lock.slock)\n{\n\t \n\tbh_unlock_sock(sk);\n\tsock_put(sk);\n\tinet_csk_prepare_for_destroy_sock(sk);\n\tinet_sk(sk)->inet_num = 0;\n}\nEXPORT_SYMBOL(inet_csk_prepare_forced_close);\n\nstatic int inet_ulp_can_listen(const struct sock *sk)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\n\tif (icsk->icsk_ulp_ops && !icsk->icsk_ulp_ops->clone)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nint inet_csk_listen_start(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tint err;\n\n\terr = inet_ulp_can_listen(sk);\n\tif (unlikely(err))\n\t\treturn err;\n\n\treqsk_queue_alloc(&icsk->icsk_accept_queue);\n\n\tsk->sk_ack_backlog = 0;\n\tinet_csk_delack_init(sk);\n\n\t \n\tinet_sk_state_store(sk, TCP_LISTEN);\n\terr = sk->sk_prot->get_port(sk, inet->inet_num);\n\tif (!err) {\n\t\tinet->inet_sport = htons(inet->inet_num);\n\n\t\tsk_dst_reset(sk);\n\t\terr = sk->sk_prot->hash(sk);\n\n\t\tif (likely(!err))\n\t\t\treturn 0;\n\t}\n\n\tinet_sk_set_state(sk, TCP_CLOSE);\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(inet_csk_listen_start);\n\nstatic void inet_child_forget(struct sock *sk, struct request_sock *req,\n\t\t\t      struct sock *child)\n{\n\tsk->sk_prot->disconnect(child, O_NONBLOCK);\n\n\tsock_orphan(child);\n\n\tthis_cpu_inc(*sk->sk_prot->orphan_count);\n\n\tif (sk->sk_protocol == IPPROTO_TCP && tcp_rsk(req)->tfo_listener) {\n\t\tBUG_ON(rcu_access_pointer(tcp_sk(child)->fastopen_rsk) != req);\n\t\tBUG_ON(sk != req->rsk_listener);\n\n\t\t \n\t\tRCU_INIT_POINTER(tcp_sk(child)->fastopen_rsk, NULL);\n\t}\n\tinet_csk_destroy_sock(child);\n}\n\nstruct sock *inet_csk_reqsk_queue_add(struct sock *sk,\n\t\t\t\t      struct request_sock *req,\n\t\t\t\t      struct sock *child)\n{\n\tstruct request_sock_queue *queue = &inet_csk(sk)->icsk_accept_queue;\n\n\tspin_lock(&queue->rskq_lock);\n\tif (unlikely(sk->sk_state != TCP_LISTEN)) {\n\t\tinet_child_forget(sk, req, child);\n\t\tchild = NULL;\n\t} else {\n\t\treq->sk = child;\n\t\treq->dl_next = NULL;\n\t\tif (queue->rskq_accept_head == NULL)\n\t\t\tWRITE_ONCE(queue->rskq_accept_head, req);\n\t\telse\n\t\t\tqueue->rskq_accept_tail->dl_next = req;\n\t\tqueue->rskq_accept_tail = req;\n\t\tsk_acceptq_added(sk);\n\t}\n\tspin_unlock(&queue->rskq_lock);\n\treturn child;\n}\nEXPORT_SYMBOL(inet_csk_reqsk_queue_add);\n\nstruct sock *inet_csk_complete_hashdance(struct sock *sk, struct sock *child,\n\t\t\t\t\t struct request_sock *req, bool own_req)\n{\n\tif (own_req) {\n\t\tinet_csk_reqsk_queue_drop(req->rsk_listener, req);\n\t\treqsk_queue_removed(&inet_csk(req->rsk_listener)->icsk_accept_queue, req);\n\n\t\tif (sk != req->rsk_listener) {\n\t\t\t \n\t\t\tstruct request_sock *nreq;\n\n\t\t\t \n\t\t\tsock_hold(sk);\n\t\t\tnreq = inet_reqsk_clone(req, sk);\n\t\t\tif (!nreq) {\n\t\t\t\tinet_child_forget(sk, req, child);\n\t\t\t\tgoto child_put;\n\t\t\t}\n\n\t\t\trefcount_set(&nreq->rsk_refcnt, 1);\n\t\t\tif (inet_csk_reqsk_queue_add(sk, nreq, child)) {\n\t\t\t\t__NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPMIGRATEREQSUCCESS);\n\t\t\t\treqsk_migrate_reset(req);\n\t\t\t\treqsk_put(req);\n\t\t\t\treturn child;\n\t\t\t}\n\n\t\t\t__NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPMIGRATEREQFAILURE);\n\t\t\treqsk_migrate_reset(nreq);\n\t\t\t__reqsk_free(nreq);\n\t\t} else if (inet_csk_reqsk_queue_add(sk, req, child)) {\n\t\t\treturn child;\n\t\t}\n\t}\n\t \nchild_put:\n\tbh_unlock_sock(child);\n\tsock_put(child);\n\treturn NULL;\n}\nEXPORT_SYMBOL(inet_csk_complete_hashdance);\n\n \nvoid inet_csk_listen_stop(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct request_sock_queue *queue = &icsk->icsk_accept_queue;\n\tstruct request_sock *next, *req;\n\n\t \n\twhile ((req = reqsk_queue_remove(queue, sk)) != NULL) {\n\t\tstruct sock *child = req->sk, *nsk;\n\t\tstruct request_sock *nreq;\n\n\t\tlocal_bh_disable();\n\t\tbh_lock_sock(child);\n\t\tWARN_ON(sock_owned_by_user(child));\n\t\tsock_hold(child);\n\n\t\tnsk = reuseport_migrate_sock(sk, child, NULL);\n\t\tif (nsk) {\n\t\t\tnreq = inet_reqsk_clone(req, nsk);\n\t\t\tif (nreq) {\n\t\t\t\trefcount_set(&nreq->rsk_refcnt, 1);\n\n\t\t\t\tif (inet_csk_reqsk_queue_add(nsk, nreq, child)) {\n\t\t\t\t\t__NET_INC_STATS(sock_net(nsk),\n\t\t\t\t\t\t\tLINUX_MIB_TCPMIGRATEREQSUCCESS);\n\t\t\t\t\treqsk_migrate_reset(req);\n\t\t\t\t} else {\n\t\t\t\t\t__NET_INC_STATS(sock_net(nsk),\n\t\t\t\t\t\t\tLINUX_MIB_TCPMIGRATEREQFAILURE);\n\t\t\t\t\treqsk_migrate_reset(nreq);\n\t\t\t\t\t__reqsk_free(nreq);\n\t\t\t\t}\n\n\t\t\t\t \n\t\t\t\tgoto skip_child_forget;\n\t\t\t}\n\t\t}\n\n\t\tinet_child_forget(sk, req, child);\nskip_child_forget:\n\t\treqsk_put(req);\n\t\tbh_unlock_sock(child);\n\t\tlocal_bh_enable();\n\t\tsock_put(child);\n\n\t\tcond_resched();\n\t}\n\tif (queue->fastopenq.rskq_rst_head) {\n\t\t \n\t\tspin_lock_bh(&queue->fastopenq.lock);\n\t\treq = queue->fastopenq.rskq_rst_head;\n\t\tqueue->fastopenq.rskq_rst_head = NULL;\n\t\tspin_unlock_bh(&queue->fastopenq.lock);\n\t\twhile (req != NULL) {\n\t\t\tnext = req->dl_next;\n\t\t\treqsk_put(req);\n\t\t\treq = next;\n\t\t}\n\t}\n\tWARN_ON_ONCE(sk->sk_ack_backlog);\n}\nEXPORT_SYMBOL_GPL(inet_csk_listen_stop);\n\nvoid inet_csk_addr2sockaddr(struct sock *sk, struct sockaddr *uaddr)\n{\n\tstruct sockaddr_in *sin = (struct sockaddr_in *)uaddr;\n\tconst struct inet_sock *inet = inet_sk(sk);\n\n\tsin->sin_family\t\t= AF_INET;\n\tsin->sin_addr.s_addr\t= inet->inet_daddr;\n\tsin->sin_port\t\t= inet->inet_dport;\n}\nEXPORT_SYMBOL_GPL(inet_csk_addr2sockaddr);\n\nstatic struct dst_entry *inet_csk_rebuild_route(struct sock *sk, struct flowi *fl)\n{\n\tconst struct inet_sock *inet = inet_sk(sk);\n\tconst struct ip_options_rcu *inet_opt;\n\t__be32 daddr = inet->inet_daddr;\n\tstruct flowi4 *fl4;\n\tstruct rtable *rt;\n\n\trcu_read_lock();\n\tinet_opt = rcu_dereference(inet->inet_opt);\n\tif (inet_opt && inet_opt->opt.srr)\n\t\tdaddr = inet_opt->opt.faddr;\n\tfl4 = &fl->u.ip4;\n\trt = ip_route_output_ports(sock_net(sk), fl4, sk, daddr,\n\t\t\t\t   inet->inet_saddr, inet->inet_dport,\n\t\t\t\t   inet->inet_sport, sk->sk_protocol,\n\t\t\t\t   RT_CONN_FLAGS(sk), sk->sk_bound_dev_if);\n\tif (IS_ERR(rt))\n\t\trt = NULL;\n\tif (rt)\n\t\tsk_setup_caps(sk, &rt->dst);\n\trcu_read_unlock();\n\n\treturn &rt->dst;\n}\n\nstruct dst_entry *inet_csk_update_pmtu(struct sock *sk, u32 mtu)\n{\n\tstruct dst_entry *dst = __sk_dst_check(sk, 0);\n\tstruct inet_sock *inet = inet_sk(sk);\n\n\tif (!dst) {\n\t\tdst = inet_csk_rebuild_route(sk, &inet->cork.fl);\n\t\tif (!dst)\n\t\t\tgoto out;\n\t}\n\tdst->ops->update_pmtu(dst, sk, NULL, mtu, true);\n\n\tdst = __sk_dst_check(sk, 0);\n\tif (!dst)\n\t\tdst = inet_csk_rebuild_route(sk, &inet->cork.fl);\nout:\n\treturn dst;\n}\nEXPORT_SYMBOL_GPL(inet_csk_update_pmtu);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}