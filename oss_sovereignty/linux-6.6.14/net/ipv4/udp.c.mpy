{
  "module_name": "udp.c",
  "hash_id": "fcfefe4394552ee04fc6fa479027fe14b9250803f1ec2a0e3664657ae196e0bb",
  "original_prompt": "Ingested from linux-6.6.14/net/ipv4/udp.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) \"UDP: \" fmt\n\n#include <linux/bpf-cgroup.h>\n#include <linux/uaccess.h>\n#include <asm/ioctls.h>\n#include <linux/memblock.h>\n#include <linux/highmem.h>\n#include <linux/types.h>\n#include <linux/fcntl.h>\n#include <linux/module.h>\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/igmp.h>\n#include <linux/inetdevice.h>\n#include <linux/in.h>\n#include <linux/errno.h>\n#include <linux/timer.h>\n#include <linux/mm.h>\n#include <linux/inet.h>\n#include <linux/netdevice.h>\n#include <linux/slab.h>\n#include <net/tcp_states.h>\n#include <linux/skbuff.h>\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <net/net_namespace.h>\n#include <net/icmp.h>\n#include <net/inet_hashtables.h>\n#include <net/ip_tunnels.h>\n#include <net/route.h>\n#include <net/checksum.h>\n#include <net/gso.h>\n#include <net/xfrm.h>\n#include <trace/events/udp.h>\n#include <linux/static_key.h>\n#include <linux/btf_ids.h>\n#include <trace/events/skb.h>\n#include <net/busy_poll.h>\n#include \"udp_impl.h\"\n#include <net/sock_reuseport.h>\n#include <net/addrconf.h>\n#include <net/udp_tunnel.h>\n#include <net/gro.h>\n#if IS_ENABLED(CONFIG_IPV6)\n#include <net/ipv6_stubs.h>\n#endif\n\nstruct udp_table udp_table __read_mostly;\nEXPORT_SYMBOL(udp_table);\n\nlong sysctl_udp_mem[3] __read_mostly;\nEXPORT_SYMBOL(sysctl_udp_mem);\n\natomic_long_t udp_memory_allocated ____cacheline_aligned_in_smp;\nEXPORT_SYMBOL(udp_memory_allocated);\nDEFINE_PER_CPU(int, udp_memory_per_cpu_fw_alloc);\nEXPORT_PER_CPU_SYMBOL_GPL(udp_memory_per_cpu_fw_alloc);\n\n#define MAX_UDP_PORTS 65536\n#define PORTS_PER_CHAIN (MAX_UDP_PORTS / UDP_HTABLE_SIZE_MIN_PERNET)\n\nstatic struct udp_table *udp_get_table_prot(struct sock *sk)\n{\n\treturn sk->sk_prot->h.udp_table ? : sock_net(sk)->ipv4.udp_table;\n}\n\nstatic int udp_lib_lport_inuse(struct net *net, __u16 num,\n\t\t\t       const struct udp_hslot *hslot,\n\t\t\t       unsigned long *bitmap,\n\t\t\t       struct sock *sk, unsigned int log)\n{\n\tstruct sock *sk2;\n\tkuid_t uid = sock_i_uid(sk);\n\n\tsk_for_each(sk2, &hslot->head) {\n\t\tif (net_eq(sock_net(sk2), net) &&\n\t\t    sk2 != sk &&\n\t\t    (bitmap || udp_sk(sk2)->udp_port_hash == num) &&\n\t\t    (!sk2->sk_reuse || !sk->sk_reuse) &&\n\t\t    (!sk2->sk_bound_dev_if || !sk->sk_bound_dev_if ||\n\t\t     sk2->sk_bound_dev_if == sk->sk_bound_dev_if) &&\n\t\t    inet_rcv_saddr_equal(sk, sk2, true)) {\n\t\t\tif (sk2->sk_reuseport && sk->sk_reuseport &&\n\t\t\t    !rcu_access_pointer(sk->sk_reuseport_cb) &&\n\t\t\t    uid_eq(uid, sock_i_uid(sk2))) {\n\t\t\t\tif (!bitmap)\n\t\t\t\t\treturn 0;\n\t\t\t} else {\n\t\t\t\tif (!bitmap)\n\t\t\t\t\treturn 1;\n\t\t\t\t__set_bit(udp_sk(sk2)->udp_port_hash >> log,\n\t\t\t\t\t  bitmap);\n\t\t\t}\n\t\t}\n\t}\n\treturn 0;\n}\n\n \nstatic int udp_lib_lport_inuse2(struct net *net, __u16 num,\n\t\t\t\tstruct udp_hslot *hslot2,\n\t\t\t\tstruct sock *sk)\n{\n\tstruct sock *sk2;\n\tkuid_t uid = sock_i_uid(sk);\n\tint res = 0;\n\n\tspin_lock(&hslot2->lock);\n\tudp_portaddr_for_each_entry(sk2, &hslot2->head) {\n\t\tif (net_eq(sock_net(sk2), net) &&\n\t\t    sk2 != sk &&\n\t\t    (udp_sk(sk2)->udp_port_hash == num) &&\n\t\t    (!sk2->sk_reuse || !sk->sk_reuse) &&\n\t\t    (!sk2->sk_bound_dev_if || !sk->sk_bound_dev_if ||\n\t\t     sk2->sk_bound_dev_if == sk->sk_bound_dev_if) &&\n\t\t    inet_rcv_saddr_equal(sk, sk2, true)) {\n\t\t\tif (sk2->sk_reuseport && sk->sk_reuseport &&\n\t\t\t    !rcu_access_pointer(sk->sk_reuseport_cb) &&\n\t\t\t    uid_eq(uid, sock_i_uid(sk2))) {\n\t\t\t\tres = 0;\n\t\t\t} else {\n\t\t\t\tres = 1;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&hslot2->lock);\n\treturn res;\n}\n\nstatic int udp_reuseport_add_sock(struct sock *sk, struct udp_hslot *hslot)\n{\n\tstruct net *net = sock_net(sk);\n\tkuid_t uid = sock_i_uid(sk);\n\tstruct sock *sk2;\n\n\tsk_for_each(sk2, &hslot->head) {\n\t\tif (net_eq(sock_net(sk2), net) &&\n\t\t    sk2 != sk &&\n\t\t    sk2->sk_family == sk->sk_family &&\n\t\t    ipv6_only_sock(sk2) == ipv6_only_sock(sk) &&\n\t\t    (udp_sk(sk2)->udp_port_hash == udp_sk(sk)->udp_port_hash) &&\n\t\t    (sk2->sk_bound_dev_if == sk->sk_bound_dev_if) &&\n\t\t    sk2->sk_reuseport && uid_eq(uid, sock_i_uid(sk2)) &&\n\t\t    inet_rcv_saddr_equal(sk, sk2, false)) {\n\t\t\treturn reuseport_add_sock(sk, sk2,\n\t\t\t\t\t\t  inet_rcv_saddr_any(sk));\n\t\t}\n\t}\n\n\treturn reuseport_alloc(sk, inet_rcv_saddr_any(sk));\n}\n\n \nint udp_lib_get_port(struct sock *sk, unsigned short snum,\n\t\t     unsigned int hash2_nulladdr)\n{\n\tstruct udp_table *udptable = udp_get_table_prot(sk);\n\tstruct udp_hslot *hslot, *hslot2;\n\tstruct net *net = sock_net(sk);\n\tint error = -EADDRINUSE;\n\n\tif (!snum) {\n\t\tDECLARE_BITMAP(bitmap, PORTS_PER_CHAIN);\n\t\tunsigned short first, last;\n\t\tint low, high, remaining;\n\t\tunsigned int rand;\n\n\t\tinet_sk_get_local_port_range(sk, &low, &high);\n\t\tremaining = (high - low) + 1;\n\n\t\trand = get_random_u32();\n\t\tfirst = reciprocal_scale(rand, remaining) + low;\n\t\t \n\t\trand = (rand | 1) * (udptable->mask + 1);\n\t\tlast = first + udptable->mask + 1;\n\t\tdo {\n\t\t\thslot = udp_hashslot(udptable, net, first);\n\t\t\tbitmap_zero(bitmap, PORTS_PER_CHAIN);\n\t\t\tspin_lock_bh(&hslot->lock);\n\t\t\tudp_lib_lport_inuse(net, snum, hslot, bitmap, sk,\n\t\t\t\t\t    udptable->log);\n\n\t\t\tsnum = first;\n\t\t\t \n\t\t\tdo {\n\t\t\t\tif (low <= snum && snum <= high &&\n\t\t\t\t    !test_bit(snum >> udptable->log, bitmap) &&\n\t\t\t\t    !inet_is_local_reserved_port(net, snum))\n\t\t\t\t\tgoto found;\n\t\t\t\tsnum += rand;\n\t\t\t} while (snum != first);\n\t\t\tspin_unlock_bh(&hslot->lock);\n\t\t\tcond_resched();\n\t\t} while (++first != last);\n\t\tgoto fail;\n\t} else {\n\t\thslot = udp_hashslot(udptable, net, snum);\n\t\tspin_lock_bh(&hslot->lock);\n\t\tif (hslot->count > 10) {\n\t\t\tint exist;\n\t\t\tunsigned int slot2 = udp_sk(sk)->udp_portaddr_hash ^ snum;\n\n\t\t\tslot2          &= udptable->mask;\n\t\t\thash2_nulladdr &= udptable->mask;\n\n\t\t\thslot2 = udp_hashslot2(udptable, slot2);\n\t\t\tif (hslot->count < hslot2->count)\n\t\t\t\tgoto scan_primary_hash;\n\n\t\t\texist = udp_lib_lport_inuse2(net, snum, hslot2, sk);\n\t\t\tif (!exist && (hash2_nulladdr != slot2)) {\n\t\t\t\thslot2 = udp_hashslot2(udptable, hash2_nulladdr);\n\t\t\t\texist = udp_lib_lport_inuse2(net, snum, hslot2,\n\t\t\t\t\t\t\t     sk);\n\t\t\t}\n\t\t\tif (exist)\n\t\t\t\tgoto fail_unlock;\n\t\t\telse\n\t\t\t\tgoto found;\n\t\t}\nscan_primary_hash:\n\t\tif (udp_lib_lport_inuse(net, snum, hslot, NULL, sk, 0))\n\t\t\tgoto fail_unlock;\n\t}\nfound:\n\tinet_sk(sk)->inet_num = snum;\n\tudp_sk(sk)->udp_port_hash = snum;\n\tudp_sk(sk)->udp_portaddr_hash ^= snum;\n\tif (sk_unhashed(sk)) {\n\t\tif (sk->sk_reuseport &&\n\t\t    udp_reuseport_add_sock(sk, hslot)) {\n\t\t\tinet_sk(sk)->inet_num = 0;\n\t\t\tudp_sk(sk)->udp_port_hash = 0;\n\t\t\tudp_sk(sk)->udp_portaddr_hash ^= snum;\n\t\t\tgoto fail_unlock;\n\t\t}\n\n\t\tsk_add_node_rcu(sk, &hslot->head);\n\t\thslot->count++;\n\t\tsock_prot_inuse_add(sock_net(sk), sk->sk_prot, 1);\n\n\t\thslot2 = udp_hashslot2(udptable, udp_sk(sk)->udp_portaddr_hash);\n\t\tspin_lock(&hslot2->lock);\n\t\tif (IS_ENABLED(CONFIG_IPV6) && sk->sk_reuseport &&\n\t\t    sk->sk_family == AF_INET6)\n\t\t\thlist_add_tail_rcu(&udp_sk(sk)->udp_portaddr_node,\n\t\t\t\t\t   &hslot2->head);\n\t\telse\n\t\t\thlist_add_head_rcu(&udp_sk(sk)->udp_portaddr_node,\n\t\t\t\t\t   &hslot2->head);\n\t\thslot2->count++;\n\t\tspin_unlock(&hslot2->lock);\n\t}\n\tsock_set_flag(sk, SOCK_RCU_FREE);\n\terror = 0;\nfail_unlock:\n\tspin_unlock_bh(&hslot->lock);\nfail:\n\treturn error;\n}\nEXPORT_SYMBOL(udp_lib_get_port);\n\nint udp_v4_get_port(struct sock *sk, unsigned short snum)\n{\n\tunsigned int hash2_nulladdr =\n\t\tipv4_portaddr_hash(sock_net(sk), htonl(INADDR_ANY), snum);\n\tunsigned int hash2_partial =\n\t\tipv4_portaddr_hash(sock_net(sk), inet_sk(sk)->inet_rcv_saddr, 0);\n\n\t \n\tudp_sk(sk)->udp_portaddr_hash = hash2_partial;\n\treturn udp_lib_get_port(sk, snum, hash2_nulladdr);\n}\n\nstatic int compute_score(struct sock *sk, struct net *net,\n\t\t\t __be32 saddr, __be16 sport,\n\t\t\t __be32 daddr, unsigned short hnum,\n\t\t\t int dif, int sdif)\n{\n\tint score;\n\tstruct inet_sock *inet;\n\tbool dev_match;\n\n\tif (!net_eq(sock_net(sk), net) ||\n\t    udp_sk(sk)->udp_port_hash != hnum ||\n\t    ipv6_only_sock(sk))\n\t\treturn -1;\n\n\tif (sk->sk_rcv_saddr != daddr)\n\t\treturn -1;\n\n\tscore = (sk->sk_family == PF_INET) ? 2 : 1;\n\n\tinet = inet_sk(sk);\n\tif (inet->inet_daddr) {\n\t\tif (inet->inet_daddr != saddr)\n\t\t\treturn -1;\n\t\tscore += 4;\n\t}\n\n\tif (inet->inet_dport) {\n\t\tif (inet->inet_dport != sport)\n\t\t\treturn -1;\n\t\tscore += 4;\n\t}\n\n\tdev_match = udp_sk_bound_dev_eq(net, sk->sk_bound_dev_if,\n\t\t\t\t\tdif, sdif);\n\tif (!dev_match)\n\t\treturn -1;\n\tif (sk->sk_bound_dev_if)\n\t\tscore += 4;\n\n\tif (READ_ONCE(sk->sk_incoming_cpu) == raw_smp_processor_id())\n\t\tscore++;\n\treturn score;\n}\n\nINDIRECT_CALLABLE_SCOPE\nu32 udp_ehashfn(const struct net *net, const __be32 laddr, const __u16 lport,\n\t\tconst __be32 faddr, const __be16 fport)\n{\n\tstatic u32 udp_ehash_secret __read_mostly;\n\n\tnet_get_random_once(&udp_ehash_secret, sizeof(udp_ehash_secret));\n\n\treturn __inet_ehashfn(laddr, lport, faddr, fport,\n\t\t\t      udp_ehash_secret + net_hash_mix(net));\n}\n\n \nstatic struct sock *udp4_lib_lookup2(struct net *net,\n\t\t\t\t     __be32 saddr, __be16 sport,\n\t\t\t\t     __be32 daddr, unsigned int hnum,\n\t\t\t\t     int dif, int sdif,\n\t\t\t\t     struct udp_hslot *hslot2,\n\t\t\t\t     struct sk_buff *skb)\n{\n\tstruct sock *sk, *result;\n\tint score, badness;\n\n\tresult = NULL;\n\tbadness = 0;\n\tudp_portaddr_for_each_entry_rcu(sk, &hslot2->head) {\n\t\tscore = compute_score(sk, net, saddr, sport,\n\t\t\t\t      daddr, hnum, dif, sdif);\n\t\tif (score > badness) {\n\t\t\tbadness = score;\n\n\t\t\tif (sk->sk_state == TCP_ESTABLISHED) {\n\t\t\t\tresult = sk;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tresult = inet_lookup_reuseport(net, sk, skb, sizeof(struct udphdr),\n\t\t\t\t\t\t       saddr, sport, daddr, hnum, udp_ehashfn);\n\t\t\tif (!result) {\n\t\t\t\tresult = sk;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (!reuseport_has_conns(sk))\n\t\t\t\treturn result;\n\n\t\t\t \n\t\t\tif (IS_ERR(result))\n\t\t\t\tcontinue;\n\n\t\t\tbadness = compute_score(result, net, saddr, sport,\n\t\t\t\t\t\tdaddr, hnum, dif, sdif);\n\n\t\t}\n\t}\n\treturn result;\n}\n\n \nstruct sock *__udp4_lib_lookup(struct net *net, __be32 saddr,\n\t\t__be16 sport, __be32 daddr, __be16 dport, int dif,\n\t\tint sdif, struct udp_table *udptable, struct sk_buff *skb)\n{\n\tunsigned short hnum = ntohs(dport);\n\tunsigned int hash2, slot2;\n\tstruct udp_hslot *hslot2;\n\tstruct sock *result, *sk;\n\n\thash2 = ipv4_portaddr_hash(net, daddr, hnum);\n\tslot2 = hash2 & udptable->mask;\n\thslot2 = &udptable->hash2[slot2];\n\n\t \n\tresult = udp4_lib_lookup2(net, saddr, sport,\n\t\t\t\t  daddr, hnum, dif, sdif,\n\t\t\t\t  hslot2, skb);\n\tif (!IS_ERR_OR_NULL(result) && result->sk_state == TCP_ESTABLISHED)\n\t\tgoto done;\n\n\t \n\tif (static_branch_unlikely(&bpf_sk_lookup_enabled) &&\n\t    udptable == net->ipv4.udp_table) {\n\t\tsk = inet_lookup_run_sk_lookup(net, IPPROTO_UDP, skb, sizeof(struct udphdr),\n\t\t\t\t\t       saddr, sport, daddr, hnum, dif,\n\t\t\t\t\t       udp_ehashfn);\n\t\tif (sk) {\n\t\t\tresult = sk;\n\t\t\tgoto done;\n\t\t}\n\t}\n\n\t \n\tif (result)\n\t\tgoto done;\n\n\t \n\thash2 = ipv4_portaddr_hash(net, htonl(INADDR_ANY), hnum);\n\tslot2 = hash2 & udptable->mask;\n\thslot2 = &udptable->hash2[slot2];\n\n\tresult = udp4_lib_lookup2(net, saddr, sport,\n\t\t\t\t  htonl(INADDR_ANY), hnum, dif, sdif,\n\t\t\t\t  hslot2, skb);\ndone:\n\tif (IS_ERR(result))\n\t\treturn NULL;\n\treturn result;\n}\nEXPORT_SYMBOL_GPL(__udp4_lib_lookup);\n\nstatic inline struct sock *__udp4_lib_lookup_skb(struct sk_buff *skb,\n\t\t\t\t\t\t __be16 sport, __be16 dport,\n\t\t\t\t\t\t struct udp_table *udptable)\n{\n\tconst struct iphdr *iph = ip_hdr(skb);\n\n\treturn __udp4_lib_lookup(dev_net(skb->dev), iph->saddr, sport,\n\t\t\t\t iph->daddr, dport, inet_iif(skb),\n\t\t\t\t inet_sdif(skb), udptable, skb);\n}\n\nstruct sock *udp4_lib_lookup_skb(const struct sk_buff *skb,\n\t\t\t\t __be16 sport, __be16 dport)\n{\n\tconst struct iphdr *iph = ip_hdr(skb);\n\tstruct net *net = dev_net(skb->dev);\n\tint iif, sdif;\n\n\tinet_get_iif_sdif(skb, &iif, &sdif);\n\n\treturn __udp4_lib_lookup(net, iph->saddr, sport,\n\t\t\t\t iph->daddr, dport, iif,\n\t\t\t\t sdif, net->ipv4.udp_table, NULL);\n}\n\n \n#if IS_ENABLED(CONFIG_NF_TPROXY_IPV4) || IS_ENABLED(CONFIG_NF_SOCKET_IPV4)\nstruct sock *udp4_lib_lookup(struct net *net, __be32 saddr, __be16 sport,\n\t\t\t     __be32 daddr, __be16 dport, int dif)\n{\n\tstruct sock *sk;\n\n\tsk = __udp4_lib_lookup(net, saddr, sport, daddr, dport,\n\t\t\t       dif, 0, net->ipv4.udp_table, NULL);\n\tif (sk && !refcount_inc_not_zero(&sk->sk_refcnt))\n\t\tsk = NULL;\n\treturn sk;\n}\nEXPORT_SYMBOL_GPL(udp4_lib_lookup);\n#endif\n\nstatic inline bool __udp_is_mcast_sock(struct net *net, const struct sock *sk,\n\t\t\t\t       __be16 loc_port, __be32 loc_addr,\n\t\t\t\t       __be16 rmt_port, __be32 rmt_addr,\n\t\t\t\t       int dif, int sdif, unsigned short hnum)\n{\n\tconst struct inet_sock *inet = inet_sk(sk);\n\n\tif (!net_eq(sock_net(sk), net) ||\n\t    udp_sk(sk)->udp_port_hash != hnum ||\n\t    (inet->inet_daddr && inet->inet_daddr != rmt_addr) ||\n\t    (inet->inet_dport != rmt_port && inet->inet_dport) ||\n\t    (inet->inet_rcv_saddr && inet->inet_rcv_saddr != loc_addr) ||\n\t    ipv6_only_sock(sk) ||\n\t    !udp_sk_bound_dev_eq(net, sk->sk_bound_dev_if, dif, sdif))\n\t\treturn false;\n\tif (!ip_mc_sf_allow(sk, loc_addr, rmt_addr, dif, sdif))\n\t\treturn false;\n\treturn true;\n}\n\nDEFINE_STATIC_KEY_FALSE(udp_encap_needed_key);\nvoid udp_encap_enable(void)\n{\n\tstatic_branch_inc(&udp_encap_needed_key);\n}\nEXPORT_SYMBOL(udp_encap_enable);\n\nvoid udp_encap_disable(void)\n{\n\tstatic_branch_dec(&udp_encap_needed_key);\n}\nEXPORT_SYMBOL(udp_encap_disable);\n\n \nstatic int __udp4_lib_err_encap_no_sk(struct sk_buff *skb, u32 info)\n{\n\tint i;\n\n\tfor (i = 0; i < MAX_IPTUN_ENCAP_OPS; i++) {\n\t\tint (*handler)(struct sk_buff *skb, u32 info);\n\t\tconst struct ip_tunnel_encap_ops *encap;\n\n\t\tencap = rcu_dereference(iptun_encaps[i]);\n\t\tif (!encap)\n\t\t\tcontinue;\n\t\thandler = encap->err_handler;\n\t\tif (handler && !handler(skb, info))\n\t\t\treturn 0;\n\t}\n\n\treturn -ENOENT;\n}\n\n \nstatic struct sock *__udp4_lib_err_encap(struct net *net,\n\t\t\t\t\t const struct iphdr *iph,\n\t\t\t\t\t struct udphdr *uh,\n\t\t\t\t\t struct udp_table *udptable,\n\t\t\t\t\t struct sock *sk,\n\t\t\t\t\t struct sk_buff *skb, u32 info)\n{\n\tint (*lookup)(struct sock *sk, struct sk_buff *skb);\n\tint network_offset, transport_offset;\n\tstruct udp_sock *up;\n\n\tnetwork_offset = skb_network_offset(skb);\n\ttransport_offset = skb_transport_offset(skb);\n\n\t \n\tskb_reset_network_header(skb);\n\n\t \n\tskb_set_transport_header(skb, iph->ihl << 2);\n\n\tif (sk) {\n\t\tup = udp_sk(sk);\n\n\t\tlookup = READ_ONCE(up->encap_err_lookup);\n\t\tif (lookup && lookup(sk, skb))\n\t\t\tsk = NULL;\n\n\t\tgoto out;\n\t}\n\n\tsk = __udp4_lib_lookup(net, iph->daddr, uh->source,\n\t\t\t       iph->saddr, uh->dest, skb->dev->ifindex, 0,\n\t\t\t       udptable, NULL);\n\tif (sk) {\n\t\tup = udp_sk(sk);\n\n\t\tlookup = READ_ONCE(up->encap_err_lookup);\n\t\tif (!lookup || lookup(sk, skb))\n\t\t\tsk = NULL;\n\t}\n\nout:\n\tif (!sk)\n\t\tsk = ERR_PTR(__udp4_lib_err_encap_no_sk(skb, info));\n\n\tskb_set_transport_header(skb, transport_offset);\n\tskb_set_network_header(skb, network_offset);\n\n\treturn sk;\n}\n\n \n\nint __udp4_lib_err(struct sk_buff *skb, u32 info, struct udp_table *udptable)\n{\n\tstruct inet_sock *inet;\n\tconst struct iphdr *iph = (const struct iphdr *)skb->data;\n\tstruct udphdr *uh = (struct udphdr *)(skb->data+(iph->ihl<<2));\n\tconst int type = icmp_hdr(skb)->type;\n\tconst int code = icmp_hdr(skb)->code;\n\tbool tunnel = false;\n\tstruct sock *sk;\n\tint harderr;\n\tint err;\n\tstruct net *net = dev_net(skb->dev);\n\n\tsk = __udp4_lib_lookup(net, iph->daddr, uh->dest,\n\t\t\t       iph->saddr, uh->source, skb->dev->ifindex,\n\t\t\t       inet_sdif(skb), udptable, NULL);\n\n\tif (!sk || READ_ONCE(udp_sk(sk)->encap_type)) {\n\t\t \n\t\tif (static_branch_unlikely(&udp_encap_needed_key)) {\n\t\t\tsk = __udp4_lib_err_encap(net, iph, uh, udptable, sk, skb,\n\t\t\t\t\t\t  info);\n\t\t\tif (!sk)\n\t\t\t\treturn 0;\n\t\t} else\n\t\t\tsk = ERR_PTR(-ENOENT);\n\n\t\tif (IS_ERR(sk)) {\n\t\t\t__ICMP_INC_STATS(net, ICMP_MIB_INERRORS);\n\t\t\treturn PTR_ERR(sk);\n\t\t}\n\n\t\ttunnel = true;\n\t}\n\n\terr = 0;\n\tharderr = 0;\n\tinet = inet_sk(sk);\n\n\tswitch (type) {\n\tdefault:\n\tcase ICMP_TIME_EXCEEDED:\n\t\terr = EHOSTUNREACH;\n\t\tbreak;\n\tcase ICMP_SOURCE_QUENCH:\n\t\tgoto out;\n\tcase ICMP_PARAMETERPROB:\n\t\terr = EPROTO;\n\t\tharderr = 1;\n\t\tbreak;\n\tcase ICMP_DEST_UNREACH:\n\t\tif (code == ICMP_FRAG_NEEDED) {  \n\t\t\tipv4_sk_update_pmtu(skb, sk, info);\n\t\t\tif (inet->pmtudisc != IP_PMTUDISC_DONT) {\n\t\t\t\terr = EMSGSIZE;\n\t\t\t\tharderr = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tgoto out;\n\t\t}\n\t\terr = EHOSTUNREACH;\n\t\tif (code <= NR_ICMP_UNREACH) {\n\t\t\tharderr = icmp_err_convert[code].fatal;\n\t\t\terr = icmp_err_convert[code].errno;\n\t\t}\n\t\tbreak;\n\tcase ICMP_REDIRECT:\n\t\tipv4_sk_redirect(skb, sk);\n\t\tgoto out;\n\t}\n\n\t \n\tif (tunnel) {\n\t\t \n\t\tif (udp_sk(sk)->encap_err_rcv)\n\t\t\tudp_sk(sk)->encap_err_rcv(sk, skb, err, uh->dest, info,\n\t\t\t\t\t\t  (u8 *)(uh+1));\n\t\tgoto out;\n\t}\n\tif (!inet_test_bit(RECVERR, sk)) {\n\t\tif (!harderr || sk->sk_state != TCP_ESTABLISHED)\n\t\t\tgoto out;\n\t} else\n\t\tip_icmp_error(sk, skb, err, uh->dest, info, (u8 *)(uh+1));\n\n\tsk->sk_err = err;\n\tsk_error_report(sk);\nout:\n\treturn 0;\n}\n\nint udp_err(struct sk_buff *skb, u32 info)\n{\n\treturn __udp4_lib_err(skb, info, dev_net(skb->dev)->ipv4.udp_table);\n}\n\n \nvoid udp_flush_pending_frames(struct sock *sk)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\n\tif (up->pending) {\n\t\tup->len = 0;\n\t\tWRITE_ONCE(up->pending, 0);\n\t\tip_flush_pending_frames(sk);\n\t}\n}\nEXPORT_SYMBOL(udp_flush_pending_frames);\n\n \nvoid udp4_hwcsum(struct sk_buff *skb, __be32 src, __be32 dst)\n{\n\tstruct udphdr *uh = udp_hdr(skb);\n\tint offset = skb_transport_offset(skb);\n\tint len = skb->len - offset;\n\tint hlen = len;\n\t__wsum csum = 0;\n\n\tif (!skb_has_frag_list(skb)) {\n\t\t \n\t\tskb->csum_start = skb_transport_header(skb) - skb->head;\n\t\tskb->csum_offset = offsetof(struct udphdr, check);\n\t\tuh->check = ~csum_tcpudp_magic(src, dst, len,\n\t\t\t\t\t       IPPROTO_UDP, 0);\n\t} else {\n\t\tstruct sk_buff *frags;\n\n\t\t \n\t\tskb_walk_frags(skb, frags) {\n\t\t\tcsum = csum_add(csum, frags->csum);\n\t\t\thlen -= frags->len;\n\t\t}\n\n\t\tcsum = skb_checksum(skb, offset, hlen, csum);\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\n\t\tuh->check = csum_tcpudp_magic(src, dst, len, IPPROTO_UDP, csum);\n\t\tif (uh->check == 0)\n\t\t\tuh->check = CSUM_MANGLED_0;\n\t}\n}\nEXPORT_SYMBOL_GPL(udp4_hwcsum);\n\n \nvoid udp_set_csum(bool nocheck, struct sk_buff *skb,\n\t\t  __be32 saddr, __be32 daddr, int len)\n{\n\tstruct udphdr *uh = udp_hdr(skb);\n\n\tif (nocheck) {\n\t\tuh->check = 0;\n\t} else if (skb_is_gso(skb)) {\n\t\tuh->check = ~udp_v4_check(len, saddr, daddr, 0);\n\t} else if (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\tuh->check = 0;\n\t\tuh->check = udp_v4_check(len, saddr, daddr, lco_csum(skb));\n\t\tif (uh->check == 0)\n\t\t\tuh->check = CSUM_MANGLED_0;\n\t} else {\n\t\tskb->ip_summed = CHECKSUM_PARTIAL;\n\t\tskb->csum_start = skb_transport_header(skb) - skb->head;\n\t\tskb->csum_offset = offsetof(struct udphdr, check);\n\t\tuh->check = ~udp_v4_check(len, saddr, daddr, 0);\n\t}\n}\nEXPORT_SYMBOL(udp_set_csum);\n\nstatic int udp_send_skb(struct sk_buff *skb, struct flowi4 *fl4,\n\t\t\tstruct inet_cork *cork)\n{\n\tstruct sock *sk = skb->sk;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct udphdr *uh;\n\tint err;\n\tint is_udplite = IS_UDPLITE(sk);\n\tint offset = skb_transport_offset(skb);\n\tint len = skb->len - offset;\n\tint datalen = len - sizeof(*uh);\n\t__wsum csum = 0;\n\n\t \n\tuh = udp_hdr(skb);\n\tuh->source = inet->inet_sport;\n\tuh->dest = fl4->fl4_dport;\n\tuh->len = htons(len);\n\tuh->check = 0;\n\n\tif (cork->gso_size) {\n\t\tconst int hlen = skb_network_header_len(skb) +\n\t\t\t\t sizeof(struct udphdr);\n\n\t\tif (hlen + cork->gso_size > cork->fragsize) {\n\t\t\tkfree_skb(skb);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (datalen > cork->gso_size * UDP_MAX_SEGMENTS) {\n\t\t\tkfree_skb(skb);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (sk->sk_no_check_tx) {\n\t\t\tkfree_skb(skb);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (skb->ip_summed != CHECKSUM_PARTIAL || is_udplite ||\n\t\t    dst_xfrm(skb_dst(skb))) {\n\t\t\tkfree_skb(skb);\n\t\t\treturn -EIO;\n\t\t}\n\n\t\tif (datalen > cork->gso_size) {\n\t\t\tskb_shinfo(skb)->gso_size = cork->gso_size;\n\t\t\tskb_shinfo(skb)->gso_type = SKB_GSO_UDP_L4;\n\t\t\tskb_shinfo(skb)->gso_segs = DIV_ROUND_UP(datalen,\n\t\t\t\t\t\t\t\t cork->gso_size);\n\t\t}\n\t\tgoto csum_partial;\n\t}\n\n\tif (is_udplite)  \t\t\t\t  \n\t\tcsum = udplite_csum(skb);\n\n\telse if (sk->sk_no_check_tx) {\t\t\t  \n\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\tgoto send;\n\n\t} else if (skb->ip_summed == CHECKSUM_PARTIAL) {  \ncsum_partial:\n\n\t\tudp4_hwcsum(skb, fl4->saddr, fl4->daddr);\n\t\tgoto send;\n\n\t} else\n\t\tcsum = udp_csum(skb);\n\n\t \n\tuh->check = csum_tcpudp_magic(fl4->saddr, fl4->daddr, len,\n\t\t\t\t      sk->sk_protocol, csum);\n\tif (uh->check == 0)\n\t\tuh->check = CSUM_MANGLED_0;\n\nsend:\n\terr = ip_send_skb(sock_net(sk), skb);\n\tif (err) {\n\t\tif (err == -ENOBUFS &&\n\t\t    !inet_test_bit(RECVERR, sk)) {\n\t\t\tUDP_INC_STATS(sock_net(sk),\n\t\t\t\t      UDP_MIB_SNDBUFERRORS, is_udplite);\n\t\t\terr = 0;\n\t\t}\n\t} else\n\t\tUDP_INC_STATS(sock_net(sk),\n\t\t\t      UDP_MIB_OUTDATAGRAMS, is_udplite);\n\treturn err;\n}\n\n \nint udp_push_pending_frames(struct sock *sk)\n{\n\tstruct udp_sock  *up = udp_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct flowi4 *fl4 = &inet->cork.fl.u.ip4;\n\tstruct sk_buff *skb;\n\tint err = 0;\n\n\tskb = ip_finish_skb(sk, fl4);\n\tif (!skb)\n\t\tgoto out;\n\n\terr = udp_send_skb(skb, fl4, &inet->cork.base);\n\nout:\n\tup->len = 0;\n\tWRITE_ONCE(up->pending, 0);\n\treturn err;\n}\nEXPORT_SYMBOL(udp_push_pending_frames);\n\nstatic int __udp_cmsg_send(struct cmsghdr *cmsg, u16 *gso_size)\n{\n\tswitch (cmsg->cmsg_type) {\n\tcase UDP_SEGMENT:\n\t\tif (cmsg->cmsg_len != CMSG_LEN(sizeof(__u16)))\n\t\t\treturn -EINVAL;\n\t\t*gso_size = *(__u16 *)CMSG_DATA(cmsg);\n\t\treturn 0;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nint udp_cmsg_send(struct sock *sk, struct msghdr *msg, u16 *gso_size)\n{\n\tstruct cmsghdr *cmsg;\n\tbool need_ip = false;\n\tint err;\n\n\tfor_each_cmsghdr(cmsg, msg) {\n\t\tif (!CMSG_OK(msg, cmsg))\n\t\t\treturn -EINVAL;\n\n\t\tif (cmsg->cmsg_level != SOL_UDP) {\n\t\t\tneed_ip = true;\n\t\t\tcontinue;\n\t\t}\n\n\t\terr = __udp_cmsg_send(cmsg, gso_size);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn need_ip;\n}\nEXPORT_SYMBOL_GPL(udp_cmsg_send);\n\nint udp_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct udp_sock *up = udp_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);\n\tstruct flowi4 fl4_stack;\n\tstruct flowi4 *fl4;\n\tint ulen = len;\n\tstruct ipcm_cookie ipc;\n\tstruct rtable *rt = NULL;\n\tint free = 0;\n\tint connected = 0;\n\t__be32 daddr, faddr, saddr;\n\tu8 tos, scope;\n\t__be16 dport;\n\tint err, is_udplite = IS_UDPLITE(sk);\n\tint corkreq = udp_test_bit(CORK, sk) || msg->msg_flags & MSG_MORE;\n\tint (*getfrag)(void *, char *, int, int, int, struct sk_buff *);\n\tstruct sk_buff *skb;\n\tstruct ip_options_data opt_copy;\n\n\tif (len > 0xFFFF)\n\t\treturn -EMSGSIZE;\n\n\t \n\n\tif (msg->msg_flags & MSG_OOB)  \n\t\treturn -EOPNOTSUPP;\n\n\tgetfrag = is_udplite ? udplite_getfrag : ip_generic_getfrag;\n\n\tfl4 = &inet->cork.fl.u.ip4;\n\tif (READ_ONCE(up->pending)) {\n\t\t \n\t\tlock_sock(sk);\n\t\tif (likely(up->pending)) {\n\t\t\tif (unlikely(up->pending != AF_INET)) {\n\t\t\t\trelease_sock(sk);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tgoto do_append_data;\n\t\t}\n\t\trelease_sock(sk);\n\t}\n\tulen += sizeof(struct udphdr);\n\n\t \n\tif (usin) {\n\t\tif (msg->msg_namelen < sizeof(*usin))\n\t\t\treturn -EINVAL;\n\t\tif (usin->sin_family != AF_INET) {\n\t\t\tif (usin->sin_family != AF_UNSPEC)\n\t\t\t\treturn -EAFNOSUPPORT;\n\t\t}\n\n\t\tdaddr = usin->sin_addr.s_addr;\n\t\tdport = usin->sin_port;\n\t\tif (dport == 0)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\t\tdaddr = inet->inet_daddr;\n\t\tdport = inet->inet_dport;\n\t\t \n\t\tconnected = 1;\n\t}\n\n\tipcm_init_sk(&ipc, inet);\n\tipc.gso_size = READ_ONCE(up->gso_size);\n\n\tif (msg->msg_controllen) {\n\t\terr = udp_cmsg_send(sk, msg, &ipc.gso_size);\n\t\tif (err > 0)\n\t\t\terr = ip_cmsg_send(sk, msg, &ipc,\n\t\t\t\t\t   sk->sk_family == AF_INET6);\n\t\tif (unlikely(err < 0)) {\n\t\t\tkfree(ipc.opt);\n\t\t\treturn err;\n\t\t}\n\t\tif (ipc.opt)\n\t\t\tfree = 1;\n\t\tconnected = 0;\n\t}\n\tif (!ipc.opt) {\n\t\tstruct ip_options_rcu *inet_opt;\n\n\t\trcu_read_lock();\n\t\tinet_opt = rcu_dereference(inet->inet_opt);\n\t\tif (inet_opt) {\n\t\t\tmemcpy(&opt_copy, inet_opt,\n\t\t\t       sizeof(*inet_opt) + inet_opt->opt.optlen);\n\t\t\tipc.opt = &opt_copy.opt;\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\n\tif (cgroup_bpf_enabled(CGROUP_UDP4_SENDMSG) && !connected) {\n\t\terr = BPF_CGROUP_RUN_PROG_UDP4_SENDMSG_LOCK(sk,\n\t\t\t\t\t    (struct sockaddr *)usin, &ipc.addr);\n\t\tif (err)\n\t\t\tgoto out_free;\n\t\tif (usin) {\n\t\t\tif (usin->sin_port == 0) {\n\t\t\t\t \n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t\tdaddr = usin->sin_addr.s_addr;\n\t\t\tdport = usin->sin_port;\n\t\t}\n\t}\n\n\tsaddr = ipc.addr;\n\tipc.addr = faddr = daddr;\n\n\tif (ipc.opt && ipc.opt->opt.srr) {\n\t\tif (!daddr) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t\tfaddr = ipc.opt->opt.faddr;\n\t\tconnected = 0;\n\t}\n\ttos = get_rttos(&ipc, inet);\n\tscope = ip_sendmsg_scope(inet, &ipc, msg);\n\tif (scope == RT_SCOPE_LINK)\n\t\tconnected = 0;\n\n\tif (ipv4_is_multicast(daddr)) {\n\t\tif (!ipc.oif || netif_index_is_l3_master(sock_net(sk), ipc.oif))\n\t\t\tipc.oif = inet->mc_index;\n\t\tif (!saddr)\n\t\t\tsaddr = inet->mc_addr;\n\t\tconnected = 0;\n\t} else if (!ipc.oif) {\n\t\tipc.oif = inet->uc_index;\n\t} else if (ipv4_is_lbcast(daddr) && inet->uc_index) {\n\t\t \n\t\tif (ipc.oif != inet->uc_index &&\n\t\t    ipc.oif == l3mdev_master_ifindex_by_index(sock_net(sk),\n\t\t\t\t\t\t\t      inet->uc_index)) {\n\t\t\tipc.oif = inet->uc_index;\n\t\t}\n\t}\n\n\tif (connected)\n\t\trt = (struct rtable *)sk_dst_check(sk, 0);\n\n\tif (!rt) {\n\t\tstruct net *net = sock_net(sk);\n\t\t__u8 flow_flags = inet_sk_flowi_flags(sk);\n\n\t\tfl4 = &fl4_stack;\n\n\t\tflowi4_init_output(fl4, ipc.oif, ipc.sockc.mark, tos, scope,\n\t\t\t\t   sk->sk_protocol, flow_flags, faddr, saddr,\n\t\t\t\t   dport, inet->inet_sport, sk->sk_uid);\n\n\t\tsecurity_sk_classify_flow(sk, flowi4_to_flowi_common(fl4));\n\t\trt = ip_route_output_flow(net, fl4, sk);\n\t\tif (IS_ERR(rt)) {\n\t\t\terr = PTR_ERR(rt);\n\t\t\trt = NULL;\n\t\t\tif (err == -ENETUNREACH)\n\t\t\t\tIP_INC_STATS(net, IPSTATS_MIB_OUTNOROUTES);\n\t\t\tgoto out;\n\t\t}\n\n\t\terr = -EACCES;\n\t\tif ((rt->rt_flags & RTCF_BROADCAST) &&\n\t\t    !sock_flag(sk, SOCK_BROADCAST))\n\t\t\tgoto out;\n\t\tif (connected)\n\t\t\tsk_dst_set(sk, dst_clone(&rt->dst));\n\t}\n\n\tif (msg->msg_flags&MSG_CONFIRM)\n\t\tgoto do_confirm;\nback_from_confirm:\n\n\tsaddr = fl4->saddr;\n\tif (!ipc.addr)\n\t\tdaddr = ipc.addr = fl4->daddr;\n\n\t \n\tif (!corkreq) {\n\t\tstruct inet_cork cork;\n\n\t\tskb = ip_make_skb(sk, fl4, getfrag, msg, ulen,\n\t\t\t\t  sizeof(struct udphdr), &ipc, &rt,\n\t\t\t\t  &cork, msg->msg_flags);\n\t\terr = PTR_ERR(skb);\n\t\tif (!IS_ERR_OR_NULL(skb))\n\t\t\terr = udp_send_skb(skb, fl4, &cork);\n\t\tgoto out;\n\t}\n\n\tlock_sock(sk);\n\tif (unlikely(up->pending)) {\n\t\t \n\t\t \n\t\trelease_sock(sk);\n\n\t\tnet_dbg_ratelimited(\"socket already corked\\n\");\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\t \n\tfl4 = &inet->cork.fl.u.ip4;\n\tfl4->daddr = daddr;\n\tfl4->saddr = saddr;\n\tfl4->fl4_dport = dport;\n\tfl4->fl4_sport = inet->inet_sport;\n\tWRITE_ONCE(up->pending, AF_INET);\n\ndo_append_data:\n\tup->len += ulen;\n\terr = ip_append_data(sk, fl4, getfrag, msg, ulen,\n\t\t\t     sizeof(struct udphdr), &ipc, &rt,\n\t\t\t     corkreq ? msg->msg_flags|MSG_MORE : msg->msg_flags);\n\tif (err)\n\t\tudp_flush_pending_frames(sk);\n\telse if (!corkreq)\n\t\terr = udp_push_pending_frames(sk);\n\telse if (unlikely(skb_queue_empty(&sk->sk_write_queue)))\n\t\tWRITE_ONCE(up->pending, 0);\n\trelease_sock(sk);\n\nout:\n\tip_rt_put(rt);\nout_free:\n\tif (free)\n\t\tkfree(ipc.opt);\n\tif (!err)\n\t\treturn len;\n\t \n\tif (err == -ENOBUFS || test_bit(SOCK_NOSPACE, &sk->sk_socket->flags)) {\n\t\tUDP_INC_STATS(sock_net(sk),\n\t\t\t      UDP_MIB_SNDBUFERRORS, is_udplite);\n\t}\n\treturn err;\n\ndo_confirm:\n\tif (msg->msg_flags & MSG_PROBE)\n\t\tdst_confirm_neigh(&rt->dst, &fl4->daddr);\n\tif (!(msg->msg_flags&MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto out;\n}\nEXPORT_SYMBOL(udp_sendmsg);\n\nvoid udp_splice_eof(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct udp_sock *up = udp_sk(sk);\n\n\tif (!READ_ONCE(up->pending) || udp_test_bit(CORK, sk))\n\t\treturn;\n\n\tlock_sock(sk);\n\tif (up->pending && !udp_test_bit(CORK, sk))\n\t\tudp_push_pending_frames(sk);\n\trelease_sock(sk);\n}\nEXPORT_SYMBOL_GPL(udp_splice_eof);\n\n#define UDP_SKB_IS_STATELESS 0x80000000\n\n \nstatic bool udp_try_make_stateless(struct sk_buff *skb)\n{\n\tif (!skb_has_extensions(skb))\n\t\treturn true;\n\n\tif (!secpath_exists(skb)) {\n\t\tskb_ext_reset(skb);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic void udp_set_dev_scratch(struct sk_buff *skb)\n{\n\tstruct udp_dev_scratch *scratch = udp_skb_scratch(skb);\n\n\tBUILD_BUG_ON(sizeof(struct udp_dev_scratch) > sizeof(long));\n\tscratch->_tsize_state = skb->truesize;\n#if BITS_PER_LONG == 64\n\tscratch->len = skb->len;\n\tscratch->csum_unnecessary = !!skb_csum_unnecessary(skb);\n\tscratch->is_linear = !skb_is_nonlinear(skb);\n#endif\n\tif (udp_try_make_stateless(skb))\n\t\tscratch->_tsize_state |= UDP_SKB_IS_STATELESS;\n}\n\nstatic void udp_skb_csum_unnecessary_set(struct sk_buff *skb)\n{\n\t \n#if BITS_PER_LONG == 64\n\tif (!skb_shared(skb))\n\t\tudp_skb_scratch(skb)->csum_unnecessary = true;\n#endif\n}\n\nstatic int udp_skb_truesize(struct sk_buff *skb)\n{\n\treturn udp_skb_scratch(skb)->_tsize_state & ~UDP_SKB_IS_STATELESS;\n}\n\nstatic bool udp_skb_has_head_state(struct sk_buff *skb)\n{\n\treturn !(udp_skb_scratch(skb)->_tsize_state & UDP_SKB_IS_STATELESS);\n}\n\n \nstatic void udp_rmem_release(struct sock *sk, int size, int partial,\n\t\t\t     bool rx_queue_lock_held)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\tstruct sk_buff_head *sk_queue;\n\tint amt;\n\n\tif (likely(partial)) {\n\t\tup->forward_deficit += size;\n\t\tsize = up->forward_deficit;\n\t\tif (size < READ_ONCE(up->forward_threshold) &&\n\t\t    !skb_queue_empty(&up->reader_queue))\n\t\t\treturn;\n\t} else {\n\t\tsize += up->forward_deficit;\n\t}\n\tup->forward_deficit = 0;\n\n\t \n\tsk_queue = &sk->sk_receive_queue;\n\tif (!rx_queue_lock_held)\n\t\tspin_lock(&sk_queue->lock);\n\n\n\tsk_forward_alloc_add(sk, size);\n\tamt = (sk->sk_forward_alloc - partial) & ~(PAGE_SIZE - 1);\n\tsk_forward_alloc_add(sk, -amt);\n\n\tif (amt)\n\t\t__sk_mem_reduce_allocated(sk, amt >> PAGE_SHIFT);\n\n\tatomic_sub(size, &sk->sk_rmem_alloc);\n\n\t \n\tskb_queue_splice_tail_init(sk_queue, &up->reader_queue);\n\n\tif (!rx_queue_lock_held)\n\t\tspin_unlock(&sk_queue->lock);\n}\n\n \nvoid udp_skb_destructor(struct sock *sk, struct sk_buff *skb)\n{\n\tprefetch(&skb->data);\n\tudp_rmem_release(sk, udp_skb_truesize(skb), 1, false);\n}\nEXPORT_SYMBOL(udp_skb_destructor);\n\n \nstatic void udp_skb_dtor_locked(struct sock *sk, struct sk_buff *skb)\n{\n\tprefetch(&skb->data);\n\tudp_rmem_release(sk, udp_skb_truesize(skb), 1, true);\n}\n\n \nstatic int udp_busylocks_log __read_mostly;\nstatic spinlock_t *udp_busylocks __read_mostly;\n\nstatic spinlock_t *busylock_acquire(void *ptr)\n{\n\tspinlock_t *busy;\n\n\tbusy = udp_busylocks + hash_ptr(ptr, udp_busylocks_log);\n\tspin_lock(busy);\n\treturn busy;\n}\n\nstatic void busylock_release(spinlock_t *busy)\n{\n\tif (busy)\n\t\tspin_unlock(busy);\n}\n\nstatic int udp_rmem_schedule(struct sock *sk, int size)\n{\n\tint delta;\n\n\tdelta = size - sk->sk_forward_alloc;\n\tif (delta > 0 && !__sk_mem_schedule(sk, delta, SK_MEM_RECV))\n\t\treturn -ENOBUFS;\n\n\treturn 0;\n}\n\nint __udp_enqueue_schedule_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct sk_buff_head *list = &sk->sk_receive_queue;\n\tint rmem, err = -ENOMEM;\n\tspinlock_t *busy = NULL;\n\tint size;\n\n\t \n\trmem = atomic_read(&sk->sk_rmem_alloc);\n\tif (rmem > sk->sk_rcvbuf)\n\t\tgoto drop;\n\n\t \n\tif (rmem > (sk->sk_rcvbuf >> 1)) {\n\t\tskb_condense(skb);\n\n\t\tbusy = busylock_acquire(sk);\n\t}\n\tsize = skb->truesize;\n\tudp_set_dev_scratch(skb);\n\n\t \n\trmem = atomic_add_return(size, &sk->sk_rmem_alloc);\n\tif (rmem > (size + (unsigned int)sk->sk_rcvbuf))\n\t\tgoto uncharge_drop;\n\n\tspin_lock(&list->lock);\n\terr = udp_rmem_schedule(sk, size);\n\tif (err) {\n\t\tspin_unlock(&list->lock);\n\t\tgoto uncharge_drop;\n\t}\n\n\tsk_forward_alloc_add(sk, -size);\n\n\t \n\tsock_skb_set_dropcount(sk, skb);\n\n\t__skb_queue_tail(list, skb);\n\tspin_unlock(&list->lock);\n\n\tif (!sock_flag(sk, SOCK_DEAD))\n\t\tINDIRECT_CALL_1(sk->sk_data_ready, sock_def_readable, sk);\n\n\tbusylock_release(busy);\n\treturn 0;\n\nuncharge_drop:\n\tatomic_sub(skb->truesize, &sk->sk_rmem_alloc);\n\ndrop:\n\tatomic_inc(&sk->sk_drops);\n\tbusylock_release(busy);\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(__udp_enqueue_schedule_skb);\n\nvoid udp_destruct_common(struct sock *sk)\n{\n\t \n\tstruct udp_sock *up = udp_sk(sk);\n\tunsigned int total = 0;\n\tstruct sk_buff *skb;\n\n\tskb_queue_splice_tail_init(&sk->sk_receive_queue, &up->reader_queue);\n\twhile ((skb = __skb_dequeue(&up->reader_queue)) != NULL) {\n\t\ttotal += skb->truesize;\n\t\tkfree_skb(skb);\n\t}\n\tudp_rmem_release(sk, total, 0, true);\n}\nEXPORT_SYMBOL_GPL(udp_destruct_common);\n\nstatic void udp_destruct_sock(struct sock *sk)\n{\n\tudp_destruct_common(sk);\n\tinet_sock_destruct(sk);\n}\n\nint udp_init_sock(struct sock *sk)\n{\n\tudp_lib_init_sock(sk);\n\tsk->sk_destruct = udp_destruct_sock;\n\tset_bit(SOCK_SUPPORT_ZC, &sk->sk_socket->flags);\n\treturn 0;\n}\n\nvoid skb_consume_udp(struct sock *sk, struct sk_buff *skb, int len)\n{\n\tif (unlikely(READ_ONCE(sk->sk_peek_off) >= 0)) {\n\t\tbool slow = lock_sock_fast(sk);\n\n\t\tsk_peek_offset_bwd(sk, len);\n\t\tunlock_sock_fast(sk, slow);\n\t}\n\n\tif (!skb_unref(skb))\n\t\treturn;\n\n\t \n\tif (unlikely(udp_skb_has_head_state(skb)))\n\t\tskb_release_head_state(skb);\n\t__consume_stateless_skb(skb);\n}\nEXPORT_SYMBOL_GPL(skb_consume_udp);\n\nstatic struct sk_buff *__first_packet_length(struct sock *sk,\n\t\t\t\t\t     struct sk_buff_head *rcvq,\n\t\t\t\t\t     int *total)\n{\n\tstruct sk_buff *skb;\n\n\twhile ((skb = skb_peek(rcvq)) != NULL) {\n\t\tif (udp_lib_checksum_complete(skb)) {\n\t\t\t__UDP_INC_STATS(sock_net(sk), UDP_MIB_CSUMERRORS,\n\t\t\t\t\tIS_UDPLITE(sk));\n\t\t\t__UDP_INC_STATS(sock_net(sk), UDP_MIB_INERRORS,\n\t\t\t\t\tIS_UDPLITE(sk));\n\t\t\tatomic_inc(&sk->sk_drops);\n\t\t\t__skb_unlink(skb, rcvq);\n\t\t\t*total += skb->truesize;\n\t\t\tkfree_skb(skb);\n\t\t} else {\n\t\t\tudp_skb_csum_unnecessary_set(skb);\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn skb;\n}\n\n \nstatic int first_packet_length(struct sock *sk)\n{\n\tstruct sk_buff_head *rcvq = &udp_sk(sk)->reader_queue;\n\tstruct sk_buff_head *sk_queue = &sk->sk_receive_queue;\n\tstruct sk_buff *skb;\n\tint total = 0;\n\tint res;\n\n\tspin_lock_bh(&rcvq->lock);\n\tskb = __first_packet_length(sk, rcvq, &total);\n\tif (!skb && !skb_queue_empty_lockless(sk_queue)) {\n\t\tspin_lock(&sk_queue->lock);\n\t\tskb_queue_splice_tail_init(sk_queue, rcvq);\n\t\tspin_unlock(&sk_queue->lock);\n\n\t\tskb = __first_packet_length(sk, rcvq, &total);\n\t}\n\tres = skb ? skb->len : -1;\n\tif (total)\n\t\tudp_rmem_release(sk, total, 1, false);\n\tspin_unlock_bh(&rcvq->lock);\n\treturn res;\n}\n\n \n\nint udp_ioctl(struct sock *sk, int cmd, int *karg)\n{\n\tswitch (cmd) {\n\tcase SIOCOUTQ:\n\t{\n\t\t*karg = sk_wmem_alloc_get(sk);\n\t\treturn 0;\n\t}\n\n\tcase SIOCINQ:\n\t{\n\t\t*karg = max_t(int, 0, first_packet_length(sk));\n\t\treturn 0;\n\t}\n\n\tdefault:\n\t\treturn -ENOIOCTLCMD;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(udp_ioctl);\n\nstruct sk_buff *__skb_recv_udp(struct sock *sk, unsigned int flags,\n\t\t\t       int *off, int *err)\n{\n\tstruct sk_buff_head *sk_queue = &sk->sk_receive_queue;\n\tstruct sk_buff_head *queue;\n\tstruct sk_buff *last;\n\tlong timeo;\n\tint error;\n\n\tqueue = &udp_sk(sk)->reader_queue;\n\ttimeo = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);\n\tdo {\n\t\tstruct sk_buff *skb;\n\n\t\terror = sock_error(sk);\n\t\tif (error)\n\t\t\tbreak;\n\n\t\terror = -EAGAIN;\n\t\tdo {\n\t\t\tspin_lock_bh(&queue->lock);\n\t\t\tskb = __skb_try_recv_from_queue(sk, queue, flags, off,\n\t\t\t\t\t\t\terr, &last);\n\t\t\tif (skb) {\n\t\t\t\tif (!(flags & MSG_PEEK))\n\t\t\t\t\tudp_skb_destructor(sk, skb);\n\t\t\t\tspin_unlock_bh(&queue->lock);\n\t\t\t\treturn skb;\n\t\t\t}\n\n\t\t\tif (skb_queue_empty_lockless(sk_queue)) {\n\t\t\t\tspin_unlock_bh(&queue->lock);\n\t\t\t\tgoto busy_check;\n\t\t\t}\n\n\t\t\t \n\t\t\tspin_lock(&sk_queue->lock);\n\t\t\tskb_queue_splice_tail_init(sk_queue, queue);\n\n\t\t\tskb = __skb_try_recv_from_queue(sk, queue, flags, off,\n\t\t\t\t\t\t\terr, &last);\n\t\t\tif (skb && !(flags & MSG_PEEK))\n\t\t\t\tudp_skb_dtor_locked(sk, skb);\n\t\t\tspin_unlock(&sk_queue->lock);\n\t\t\tspin_unlock_bh(&queue->lock);\n\t\t\tif (skb)\n\t\t\t\treturn skb;\n\nbusy_check:\n\t\t\tif (!sk_can_busy_loop(sk))\n\t\t\t\tbreak;\n\n\t\t\tsk_busy_loop(sk, flags & MSG_DONTWAIT);\n\t\t} while (!skb_queue_empty_lockless(sk_queue));\n\n\t\t \n\t} while (timeo &&\n\t\t !__skb_wait_for_more_packets(sk, &sk->sk_receive_queue,\n\t\t\t\t\t      &error, &timeo,\n\t\t\t\t\t      (struct sk_buff *)sk_queue));\n\n\t*err = error;\n\treturn NULL;\n}\nEXPORT_SYMBOL(__skb_recv_udp);\n\nint udp_read_skb(struct sock *sk, skb_read_actor_t recv_actor)\n{\n\tstruct sk_buff *skb;\n\tint err;\n\ntry_again:\n\tskb = skb_recv_udp(sk, MSG_DONTWAIT, &err);\n\tif (!skb)\n\t\treturn err;\n\n\tif (udp_lib_checksum_complete(skb)) {\n\t\tint is_udplite = IS_UDPLITE(sk);\n\t\tstruct net *net = sock_net(sk);\n\n\t\t__UDP_INC_STATS(net, UDP_MIB_CSUMERRORS, is_udplite);\n\t\t__UDP_INC_STATS(net, UDP_MIB_INERRORS, is_udplite);\n\t\tatomic_inc(&sk->sk_drops);\n\t\tkfree_skb(skb);\n\t\tgoto try_again;\n\t}\n\n\tWARN_ON_ONCE(!skb_set_owner_sk_safe(skb, sk));\n\treturn recv_actor(sk, skb);\n}\nEXPORT_SYMBOL(udp_read_skb);\n\n \n\nint udp_recvmsg(struct sock *sk, struct msghdr *msg, size_t len, int flags,\n\t\tint *addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_in *, sin, msg->msg_name);\n\tstruct sk_buff *skb;\n\tunsigned int ulen, copied;\n\tint off, err, peeking = flags & MSG_PEEK;\n\tint is_udplite = IS_UDPLITE(sk);\n\tbool checksum_valid = false;\n\n\tif (flags & MSG_ERRQUEUE)\n\t\treturn ip_recv_error(sk, msg, len, addr_len);\n\ntry_again:\n\toff = sk_peek_offset(sk, flags);\n\tskb = __skb_recv_udp(sk, flags, &off, &err);\n\tif (!skb)\n\t\treturn err;\n\n\tulen = udp_skb_len(skb);\n\tcopied = len;\n\tif (copied > ulen - off)\n\t\tcopied = ulen - off;\n\telse if (copied < ulen)\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\n\t \n\n\tif (copied < ulen || peeking ||\n\t    (is_udplite && UDP_SKB_CB(skb)->partial_cov)) {\n\t\tchecksum_valid = udp_skb_csum_unnecessary(skb) ||\n\t\t\t\t!__udp_lib_checksum_complete(skb);\n\t\tif (!checksum_valid)\n\t\t\tgoto csum_copy_err;\n\t}\n\n\tif (checksum_valid || udp_skb_csum_unnecessary(skb)) {\n\t\tif (udp_skb_is_linear(skb))\n\t\t\terr = copy_linear_skb(skb, copied, off, &msg->msg_iter);\n\t\telse\n\t\t\terr = skb_copy_datagram_msg(skb, off, msg, copied);\n\t} else {\n\t\terr = skb_copy_and_csum_datagram_msg(skb, off, msg);\n\n\t\tif (err == -EINVAL)\n\t\t\tgoto csum_copy_err;\n\t}\n\n\tif (unlikely(err)) {\n\t\tif (!peeking) {\n\t\t\tatomic_inc(&sk->sk_drops);\n\t\t\tUDP_INC_STATS(sock_net(sk),\n\t\t\t\t      UDP_MIB_INERRORS, is_udplite);\n\t\t}\n\t\tkfree_skb(skb);\n\t\treturn err;\n\t}\n\n\tif (!peeking)\n\t\tUDP_INC_STATS(sock_net(sk),\n\t\t\t      UDP_MIB_INDATAGRAMS, is_udplite);\n\n\tsock_recv_cmsgs(msg, sk, skb);\n\n\t \n\tif (sin) {\n\t\tsin->sin_family = AF_INET;\n\t\tsin->sin_port = udp_hdr(skb)->source;\n\t\tsin->sin_addr.s_addr = ip_hdr(skb)->saddr;\n\t\tmemset(sin->sin_zero, 0, sizeof(sin->sin_zero));\n\t\t*addr_len = sizeof(*sin);\n\n\t\tBPF_CGROUP_RUN_PROG_UDP4_RECVMSG_LOCK(sk,\n\t\t\t\t\t\t      (struct sockaddr *)sin);\n\t}\n\n\tif (udp_test_bit(GRO_ENABLED, sk))\n\t\tudp_cmsg_recv(msg, sk, skb);\n\n\tif (inet_cmsg_flags(inet))\n\t\tip_cmsg_recv_offset(msg, sk, skb, sizeof(struct udphdr), off);\n\n\terr = copied;\n\tif (flags & MSG_TRUNC)\n\t\terr = ulen;\n\n\tskb_consume_udp(sk, skb, peeking ? -err : err);\n\treturn err;\n\ncsum_copy_err:\n\tif (!__sk_queue_drop_skb(sk, &udp_sk(sk)->reader_queue, skb, flags,\n\t\t\t\t udp_skb_destructor)) {\n\t\tUDP_INC_STATS(sock_net(sk), UDP_MIB_CSUMERRORS, is_udplite);\n\t\tUDP_INC_STATS(sock_net(sk), UDP_MIB_INERRORS, is_udplite);\n\t}\n\tkfree_skb(skb);\n\n\t \n\tcond_resched();\n\tmsg->msg_flags &= ~MSG_TRUNC;\n\tgoto try_again;\n}\n\nint udp_pre_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\t \n\tif (addr_len < sizeof(struct sockaddr_in))\n\t\treturn -EINVAL;\n\n\treturn BPF_CGROUP_RUN_PROG_INET4_CONNECT_LOCK(sk, uaddr);\n}\nEXPORT_SYMBOL(udp_pre_connect);\n\nint __udp_disconnect(struct sock *sk, int flags)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\t \n\n\tsk->sk_state = TCP_CLOSE;\n\tinet->inet_daddr = 0;\n\tinet->inet_dport = 0;\n\tsock_rps_reset_rxhash(sk);\n\tsk->sk_bound_dev_if = 0;\n\tif (!(sk->sk_userlocks & SOCK_BINDADDR_LOCK)) {\n\t\tinet_reset_saddr(sk);\n\t\tif (sk->sk_prot->rehash &&\n\t\t    (sk->sk_userlocks & SOCK_BINDPORT_LOCK))\n\t\t\tsk->sk_prot->rehash(sk);\n\t}\n\n\tif (!(sk->sk_userlocks & SOCK_BINDPORT_LOCK)) {\n\t\tsk->sk_prot->unhash(sk);\n\t\tinet->inet_sport = 0;\n\t}\n\tsk_dst_reset(sk);\n\treturn 0;\n}\nEXPORT_SYMBOL(__udp_disconnect);\n\nint udp_disconnect(struct sock *sk, int flags)\n{\n\tlock_sock(sk);\n\t__udp_disconnect(sk, flags);\n\trelease_sock(sk);\n\treturn 0;\n}\nEXPORT_SYMBOL(udp_disconnect);\n\nvoid udp_lib_unhash(struct sock *sk)\n{\n\tif (sk_hashed(sk)) {\n\t\tstruct udp_table *udptable = udp_get_table_prot(sk);\n\t\tstruct udp_hslot *hslot, *hslot2;\n\n\t\thslot  = udp_hashslot(udptable, sock_net(sk),\n\t\t\t\t      udp_sk(sk)->udp_port_hash);\n\t\thslot2 = udp_hashslot2(udptable, udp_sk(sk)->udp_portaddr_hash);\n\n\t\tspin_lock_bh(&hslot->lock);\n\t\tif (rcu_access_pointer(sk->sk_reuseport_cb))\n\t\t\treuseport_detach_sock(sk);\n\t\tif (sk_del_node_init_rcu(sk)) {\n\t\t\thslot->count--;\n\t\t\tinet_sk(sk)->inet_num = 0;\n\t\t\tsock_prot_inuse_add(sock_net(sk), sk->sk_prot, -1);\n\n\t\t\tspin_lock(&hslot2->lock);\n\t\t\thlist_del_init_rcu(&udp_sk(sk)->udp_portaddr_node);\n\t\t\thslot2->count--;\n\t\t\tspin_unlock(&hslot2->lock);\n\t\t}\n\t\tspin_unlock_bh(&hslot->lock);\n\t}\n}\nEXPORT_SYMBOL(udp_lib_unhash);\n\n \nvoid udp_lib_rehash(struct sock *sk, u16 newhash)\n{\n\tif (sk_hashed(sk)) {\n\t\tstruct udp_table *udptable = udp_get_table_prot(sk);\n\t\tstruct udp_hslot *hslot, *hslot2, *nhslot2;\n\n\t\thslot2 = udp_hashslot2(udptable, udp_sk(sk)->udp_portaddr_hash);\n\t\tnhslot2 = udp_hashslot2(udptable, newhash);\n\t\tudp_sk(sk)->udp_portaddr_hash = newhash;\n\n\t\tif (hslot2 != nhslot2 ||\n\t\t    rcu_access_pointer(sk->sk_reuseport_cb)) {\n\t\t\thslot = udp_hashslot(udptable, sock_net(sk),\n\t\t\t\t\t     udp_sk(sk)->udp_port_hash);\n\t\t\t \n\t\t\tspin_lock_bh(&hslot->lock);\n\t\t\tif (rcu_access_pointer(sk->sk_reuseport_cb))\n\t\t\t\treuseport_detach_sock(sk);\n\n\t\t\tif (hslot2 != nhslot2) {\n\t\t\t\tspin_lock(&hslot2->lock);\n\t\t\t\thlist_del_init_rcu(&udp_sk(sk)->udp_portaddr_node);\n\t\t\t\thslot2->count--;\n\t\t\t\tspin_unlock(&hslot2->lock);\n\n\t\t\t\tspin_lock(&nhslot2->lock);\n\t\t\t\thlist_add_head_rcu(&udp_sk(sk)->udp_portaddr_node,\n\t\t\t\t\t\t\t &nhslot2->head);\n\t\t\t\tnhslot2->count++;\n\t\t\t\tspin_unlock(&nhslot2->lock);\n\t\t\t}\n\n\t\t\tspin_unlock_bh(&hslot->lock);\n\t\t}\n\t}\n}\nEXPORT_SYMBOL(udp_lib_rehash);\n\nvoid udp_v4_rehash(struct sock *sk)\n{\n\tu16 new_hash = ipv4_portaddr_hash(sock_net(sk),\n\t\t\t\t\t  inet_sk(sk)->inet_rcv_saddr,\n\t\t\t\t\t  inet_sk(sk)->inet_num);\n\tudp_lib_rehash(sk, new_hash);\n}\n\nstatic int __udp_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tint rc;\n\n\tif (inet_sk(sk)->inet_daddr) {\n\t\tsock_rps_save_rxhash(sk, skb);\n\t\tsk_mark_napi_id(sk, skb);\n\t\tsk_incoming_cpu_update(sk);\n\t} else {\n\t\tsk_mark_napi_id_once(sk, skb);\n\t}\n\n\trc = __udp_enqueue_schedule_skb(sk, skb);\n\tif (rc < 0) {\n\t\tint is_udplite = IS_UDPLITE(sk);\n\t\tint drop_reason;\n\n\t\t \n\t\tif (rc == -ENOMEM) {\n\t\t\tUDP_INC_STATS(sock_net(sk), UDP_MIB_RCVBUFERRORS,\n\t\t\t\t\tis_udplite);\n\t\t\tdrop_reason = SKB_DROP_REASON_SOCKET_RCVBUFF;\n\t\t} else {\n\t\t\tUDP_INC_STATS(sock_net(sk), UDP_MIB_MEMERRORS,\n\t\t\t\t      is_udplite);\n\t\t\tdrop_reason = SKB_DROP_REASON_PROTO_MEM;\n\t\t}\n\t\tUDP_INC_STATS(sock_net(sk), UDP_MIB_INERRORS, is_udplite);\n\t\tkfree_skb_reason(skb, drop_reason);\n\t\ttrace_udp_fail_queue_rcv_skb(rc, sk);\n\t\treturn -1;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int udp_queue_rcv_one_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tint drop_reason = SKB_DROP_REASON_NOT_SPECIFIED;\n\tstruct udp_sock *up = udp_sk(sk);\n\tint is_udplite = IS_UDPLITE(sk);\n\n\t \n\tif (!xfrm4_policy_check(sk, XFRM_POLICY_IN, skb)) {\n\t\tdrop_reason = SKB_DROP_REASON_XFRM_POLICY;\n\t\tgoto drop;\n\t}\n\tnf_reset_ct(skb);\n\n\tif (static_branch_unlikely(&udp_encap_needed_key) &&\n\t    READ_ONCE(up->encap_type)) {\n\t\tint (*encap_rcv)(struct sock *sk, struct sk_buff *skb);\n\n\t\t \n\n\t\t \n\t\tencap_rcv = READ_ONCE(up->encap_rcv);\n\t\tif (encap_rcv) {\n\t\t\tint ret;\n\n\t\t\t \n\t\t\tif (udp_lib_checksum_complete(skb))\n\t\t\t\tgoto csum_error;\n\n\t\t\tret = encap_rcv(sk, skb);\n\t\t\tif (ret <= 0) {\n\t\t\t\t__UDP_INC_STATS(sock_net(sk),\n\t\t\t\t\t\tUDP_MIB_INDATAGRAMS,\n\t\t\t\t\t\tis_udplite);\n\t\t\t\treturn -ret;\n\t\t\t}\n\t\t}\n\n\t\t \n\t}\n\n\t \n\tif (udp_test_bit(UDPLITE_RECV_CC, sk) && UDP_SKB_CB(skb)->partial_cov) {\n\t\tu16 pcrlen = READ_ONCE(up->pcrlen);\n\n\t\t \n\t\tif (pcrlen == 0) {           \n\t\t\tnet_dbg_ratelimited(\"UDPLite: partial coverage %d while full coverage %d requested\\n\",\n\t\t\t\t\t    UDP_SKB_CB(skb)->cscov, skb->len);\n\t\t\tgoto drop;\n\t\t}\n\t\t \n\t\tif (UDP_SKB_CB(skb)->cscov < pcrlen) {\n\t\t\tnet_dbg_ratelimited(\"UDPLite: coverage %d too small, need min %d\\n\",\n\t\t\t\t\t    UDP_SKB_CB(skb)->cscov, pcrlen);\n\t\t\tgoto drop;\n\t\t}\n\t}\n\n\tprefetch(&sk->sk_rmem_alloc);\n\tif (rcu_access_pointer(sk->sk_filter) &&\n\t    udp_lib_checksum_complete(skb))\n\t\t\tgoto csum_error;\n\n\tif (sk_filter_trim_cap(sk, skb, sizeof(struct udphdr))) {\n\t\tdrop_reason = SKB_DROP_REASON_SOCKET_FILTER;\n\t\tgoto drop;\n\t}\n\n\tudp_csum_pull_header(skb);\n\n\tipv4_pktinfo_prepare(sk, skb);\n\treturn __udp_queue_rcv_skb(sk, skb);\n\ncsum_error:\n\tdrop_reason = SKB_DROP_REASON_UDP_CSUM;\n\t__UDP_INC_STATS(sock_net(sk), UDP_MIB_CSUMERRORS, is_udplite);\ndrop:\n\t__UDP_INC_STATS(sock_net(sk), UDP_MIB_INERRORS, is_udplite);\n\tatomic_inc(&sk->sk_drops);\n\tkfree_skb_reason(skb, drop_reason);\n\treturn -1;\n}\n\nstatic int udp_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct sk_buff *next, *segs;\n\tint ret;\n\n\tif (likely(!udp_unexpected_gso(sk, skb)))\n\t\treturn udp_queue_rcv_one_skb(sk, skb);\n\n\tBUILD_BUG_ON(sizeof(struct udp_skb_cb) > SKB_GSO_CB_OFFSET);\n\t__skb_push(skb, -skb_mac_offset(skb));\n\tsegs = udp_rcv_segment(sk, skb, true);\n\tskb_list_walk_safe(segs, skb, next) {\n\t\t__skb_pull(skb, skb_transport_offset(skb));\n\n\t\tudp_post_segment_fix_csum(skb);\n\t\tret = udp_queue_rcv_one_skb(sk, skb);\n\t\tif (ret > 0)\n\t\t\tip_protocol_deliver_rcu(dev_net(skb->dev), skb, ret);\n\t}\n\treturn 0;\n}\n\n \nbool udp_sk_rx_dst_set(struct sock *sk, struct dst_entry *dst)\n{\n\tstruct dst_entry *old;\n\n\tif (dst_hold_safe(dst)) {\n\t\told = xchg((__force struct dst_entry **)&sk->sk_rx_dst, dst);\n\t\tdst_release(old);\n\t\treturn old != dst;\n\t}\n\treturn false;\n}\nEXPORT_SYMBOL(udp_sk_rx_dst_set);\n\n \nstatic int __udp4_lib_mcast_deliver(struct net *net, struct sk_buff *skb,\n\t\t\t\t    struct udphdr  *uh,\n\t\t\t\t    __be32 saddr, __be32 daddr,\n\t\t\t\t    struct udp_table *udptable,\n\t\t\t\t    int proto)\n{\n\tstruct sock *sk, *first = NULL;\n\tunsigned short hnum = ntohs(uh->dest);\n\tstruct udp_hslot *hslot = udp_hashslot(udptable, net, hnum);\n\tunsigned int hash2 = 0, hash2_any = 0, use_hash2 = (hslot->count > 10);\n\tunsigned int offset = offsetof(typeof(*sk), sk_node);\n\tint dif = skb->dev->ifindex;\n\tint sdif = inet_sdif(skb);\n\tstruct hlist_node *node;\n\tstruct sk_buff *nskb;\n\n\tif (use_hash2) {\n\t\thash2_any = ipv4_portaddr_hash(net, htonl(INADDR_ANY), hnum) &\n\t\t\t    udptable->mask;\n\t\thash2 = ipv4_portaddr_hash(net, daddr, hnum) & udptable->mask;\nstart_lookup:\n\t\thslot = &udptable->hash2[hash2];\n\t\toffset = offsetof(typeof(*sk), __sk_common.skc_portaddr_node);\n\t}\n\n\tsk_for_each_entry_offset_rcu(sk, node, &hslot->head, offset) {\n\t\tif (!__udp_is_mcast_sock(net, sk, uh->dest, daddr,\n\t\t\t\t\t uh->source, saddr, dif, sdif, hnum))\n\t\t\tcontinue;\n\n\t\tif (!first) {\n\t\t\tfirst = sk;\n\t\t\tcontinue;\n\t\t}\n\t\tnskb = skb_clone(skb, GFP_ATOMIC);\n\n\t\tif (unlikely(!nskb)) {\n\t\t\tatomic_inc(&sk->sk_drops);\n\t\t\t__UDP_INC_STATS(net, UDP_MIB_RCVBUFERRORS,\n\t\t\t\t\tIS_UDPLITE(sk));\n\t\t\t__UDP_INC_STATS(net, UDP_MIB_INERRORS,\n\t\t\t\t\tIS_UDPLITE(sk));\n\t\t\tcontinue;\n\t\t}\n\t\tif (udp_queue_rcv_skb(sk, nskb) > 0)\n\t\t\tconsume_skb(nskb);\n\t}\n\n\t \n\tif (use_hash2 && hash2 != hash2_any) {\n\t\thash2 = hash2_any;\n\t\tgoto start_lookup;\n\t}\n\n\tif (first) {\n\t\tif (udp_queue_rcv_skb(first, skb) > 0)\n\t\t\tconsume_skb(skb);\n\t} else {\n\t\tkfree_skb(skb);\n\t\t__UDP_INC_STATS(net, UDP_MIB_IGNOREDMULTI,\n\t\t\t\tproto == IPPROTO_UDPLITE);\n\t}\n\treturn 0;\n}\n\n \nstatic inline int udp4_csum_init(struct sk_buff *skb, struct udphdr *uh,\n\t\t\t\t int proto)\n{\n\tint err;\n\n\tUDP_SKB_CB(skb)->partial_cov = 0;\n\tUDP_SKB_CB(skb)->cscov = skb->len;\n\n\tif (proto == IPPROTO_UDPLITE) {\n\t\terr = udplite_checksum_init(skb, uh);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (UDP_SKB_CB(skb)->partial_cov) {\n\t\t\tskb->csum = inet_compute_pseudo(skb, proto);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t \n\terr = (__force int)skb_checksum_init_zero_check(skb, proto, uh->check,\n\t\t\t\t\t\t\tinet_compute_pseudo);\n\tif (err)\n\t\treturn err;\n\n\tif (skb->ip_summed == CHECKSUM_COMPLETE && !skb->csum_valid) {\n\t\t \n\t\tif (skb->csum_complete_sw)\n\t\t\treturn 1;\n\n\t\t \n\t\tskb_checksum_complete_unset(skb);\n\t}\n\n\treturn 0;\n}\n\n \nstatic int udp_unicast_rcv_skb(struct sock *sk, struct sk_buff *skb,\n\t\t\t       struct udphdr *uh)\n{\n\tint ret;\n\n\tif (inet_get_convert_csum(sk) && uh->check && !IS_UDPLITE(sk))\n\t\tskb_checksum_try_convert(skb, IPPROTO_UDP, inet_compute_pseudo);\n\n\tret = udp_queue_rcv_skb(sk, skb);\n\n\t \n\tif (ret > 0)\n\t\treturn -ret;\n\treturn 0;\n}\n\n \n\nint __udp4_lib_rcv(struct sk_buff *skb, struct udp_table *udptable,\n\t\t   int proto)\n{\n\tstruct sock *sk;\n\tstruct udphdr *uh;\n\tunsigned short ulen;\n\tstruct rtable *rt = skb_rtable(skb);\n\t__be32 saddr, daddr;\n\tstruct net *net = dev_net(skb->dev);\n\tbool refcounted;\n\tint drop_reason;\n\n\tdrop_reason = SKB_DROP_REASON_NOT_SPECIFIED;\n\n\t \n\tif (!pskb_may_pull(skb, sizeof(struct udphdr)))\n\t\tgoto drop;\t\t \n\n\tuh   = udp_hdr(skb);\n\tulen = ntohs(uh->len);\n\tsaddr = ip_hdr(skb)->saddr;\n\tdaddr = ip_hdr(skb)->daddr;\n\n\tif (ulen > skb->len)\n\t\tgoto short_packet;\n\n\tif (proto == IPPROTO_UDP) {\n\t\t \n\t\tif (ulen < sizeof(*uh) || pskb_trim_rcsum(skb, ulen))\n\t\t\tgoto short_packet;\n\t\tuh = udp_hdr(skb);\n\t}\n\n\tif (udp4_csum_init(skb, uh, proto))\n\t\tgoto csum_error;\n\n\tsk = inet_steal_sock(net, skb, sizeof(struct udphdr), saddr, uh->source, daddr, uh->dest,\n\t\t\t     &refcounted, udp_ehashfn);\n\tif (IS_ERR(sk))\n\t\tgoto no_sk;\n\n\tif (sk) {\n\t\tstruct dst_entry *dst = skb_dst(skb);\n\t\tint ret;\n\n\t\tif (unlikely(rcu_dereference(sk->sk_rx_dst) != dst))\n\t\t\tudp_sk_rx_dst_set(sk, dst);\n\n\t\tret = udp_unicast_rcv_skb(sk, skb, uh);\n\t\tif (refcounted)\n\t\t\tsock_put(sk);\n\t\treturn ret;\n\t}\n\n\tif (rt->rt_flags & (RTCF_BROADCAST|RTCF_MULTICAST))\n\t\treturn __udp4_lib_mcast_deliver(net, skb, uh,\n\t\t\t\t\t\tsaddr, daddr, udptable, proto);\n\n\tsk = __udp4_lib_lookup_skb(skb, uh->source, uh->dest, udptable);\n\tif (sk)\n\t\treturn udp_unicast_rcv_skb(sk, skb, uh);\nno_sk:\n\tif (!xfrm4_policy_check(NULL, XFRM_POLICY_IN, skb))\n\t\tgoto drop;\n\tnf_reset_ct(skb);\n\n\t \n\tif (udp_lib_checksum_complete(skb))\n\t\tgoto csum_error;\n\n\tdrop_reason = SKB_DROP_REASON_NO_SOCKET;\n\t__UDP_INC_STATS(net, UDP_MIB_NOPORTS, proto == IPPROTO_UDPLITE);\n\ticmp_send(skb, ICMP_DEST_UNREACH, ICMP_PORT_UNREACH, 0);\n\n\t \n\tkfree_skb_reason(skb, drop_reason);\n\treturn 0;\n\nshort_packet:\n\tdrop_reason = SKB_DROP_REASON_PKT_TOO_SMALL;\n\tnet_dbg_ratelimited(\"UDP%s: short packet: From %pI4:%u %d/%d to %pI4:%u\\n\",\n\t\t\t    proto == IPPROTO_UDPLITE ? \"Lite\" : \"\",\n\t\t\t    &saddr, ntohs(uh->source),\n\t\t\t    ulen, skb->len,\n\t\t\t    &daddr, ntohs(uh->dest));\n\tgoto drop;\n\ncsum_error:\n\t \n\tdrop_reason = SKB_DROP_REASON_UDP_CSUM;\n\tnet_dbg_ratelimited(\"UDP%s: bad checksum. From %pI4:%u to %pI4:%u ulen %d\\n\",\n\t\t\t    proto == IPPROTO_UDPLITE ? \"Lite\" : \"\",\n\t\t\t    &saddr, ntohs(uh->source), &daddr, ntohs(uh->dest),\n\t\t\t    ulen);\n\t__UDP_INC_STATS(net, UDP_MIB_CSUMERRORS, proto == IPPROTO_UDPLITE);\ndrop:\n\t__UDP_INC_STATS(net, UDP_MIB_INERRORS, proto == IPPROTO_UDPLITE);\n\tkfree_skb_reason(skb, drop_reason);\n\treturn 0;\n}\n\n \nstatic struct sock *__udp4_lib_mcast_demux_lookup(struct net *net,\n\t\t\t\t\t\t  __be16 loc_port, __be32 loc_addr,\n\t\t\t\t\t\t  __be16 rmt_port, __be32 rmt_addr,\n\t\t\t\t\t\t  int dif, int sdif)\n{\n\tstruct udp_table *udptable = net->ipv4.udp_table;\n\tunsigned short hnum = ntohs(loc_port);\n\tstruct sock *sk, *result;\n\tstruct udp_hslot *hslot;\n\tunsigned int slot;\n\n\tslot = udp_hashfn(net, hnum, udptable->mask);\n\thslot = &udptable->hash[slot];\n\n\t \n\tif (hslot->count > 10)\n\t\treturn NULL;\n\n\tresult = NULL;\n\tsk_for_each_rcu(sk, &hslot->head) {\n\t\tif (__udp_is_mcast_sock(net, sk, loc_port, loc_addr,\n\t\t\t\t\trmt_port, rmt_addr, dif, sdif, hnum)) {\n\t\t\tif (result)\n\t\t\t\treturn NULL;\n\t\t\tresult = sk;\n\t\t}\n\t}\n\n\treturn result;\n}\n\n \nstatic struct sock *__udp4_lib_demux_lookup(struct net *net,\n\t\t\t\t\t    __be16 loc_port, __be32 loc_addr,\n\t\t\t\t\t    __be16 rmt_port, __be32 rmt_addr,\n\t\t\t\t\t    int dif, int sdif)\n{\n\tstruct udp_table *udptable = net->ipv4.udp_table;\n\tINET_ADDR_COOKIE(acookie, rmt_addr, loc_addr);\n\tunsigned short hnum = ntohs(loc_port);\n\tunsigned int hash2, slot2;\n\tstruct udp_hslot *hslot2;\n\t__portpair ports;\n\tstruct sock *sk;\n\n\thash2 = ipv4_portaddr_hash(net, loc_addr, hnum);\n\tslot2 = hash2 & udptable->mask;\n\thslot2 = &udptable->hash2[slot2];\n\tports = INET_COMBINED_PORTS(rmt_port, hnum);\n\n\tudp_portaddr_for_each_entry_rcu(sk, &hslot2->head) {\n\t\tif (inet_match(net, sk, acookie, ports, dif, sdif))\n\t\t\treturn sk;\n\t\t \n\t\tbreak;\n\t}\n\treturn NULL;\n}\n\nint udp_v4_early_demux(struct sk_buff *skb)\n{\n\tstruct net *net = dev_net(skb->dev);\n\tstruct in_device *in_dev = NULL;\n\tconst struct iphdr *iph;\n\tconst struct udphdr *uh;\n\tstruct sock *sk = NULL;\n\tstruct dst_entry *dst;\n\tint dif = skb->dev->ifindex;\n\tint sdif = inet_sdif(skb);\n\tint ours;\n\n\t \n\tif (!pskb_may_pull(skb, skb_transport_offset(skb) + sizeof(struct udphdr)))\n\t\treturn 0;\n\n\tiph = ip_hdr(skb);\n\tuh = udp_hdr(skb);\n\n\tif (skb->pkt_type == PACKET_MULTICAST) {\n\t\tin_dev = __in_dev_get_rcu(skb->dev);\n\n\t\tif (!in_dev)\n\t\t\treturn 0;\n\n\t\tours = ip_check_mc_rcu(in_dev, iph->daddr, iph->saddr,\n\t\t\t\t       iph->protocol);\n\t\tif (!ours)\n\t\t\treturn 0;\n\n\t\tsk = __udp4_lib_mcast_demux_lookup(net, uh->dest, iph->daddr,\n\t\t\t\t\t\t   uh->source, iph->saddr,\n\t\t\t\t\t\t   dif, sdif);\n\t} else if (skb->pkt_type == PACKET_HOST) {\n\t\tsk = __udp4_lib_demux_lookup(net, uh->dest, iph->daddr,\n\t\t\t\t\t     uh->source, iph->saddr, dif, sdif);\n\t}\n\n\tif (!sk || !refcount_inc_not_zero(&sk->sk_refcnt))\n\t\treturn 0;\n\n\tskb->sk = sk;\n\tskb->destructor = sock_efree;\n\tdst = rcu_dereference(sk->sk_rx_dst);\n\n\tif (dst)\n\t\tdst = dst_check(dst, 0);\n\tif (dst) {\n\t\tu32 itag = 0;\n\n\t\t \n\t\tskb_dst_set_noref(skb, dst);\n\n\t\t \n\t\tif (!inet_sk(sk)->inet_daddr && in_dev)\n\t\t\treturn ip_mc_validate_source(skb, iph->daddr,\n\t\t\t\t\t\t     iph->saddr,\n\t\t\t\t\t\t     iph->tos & IPTOS_RT_MASK,\n\t\t\t\t\t\t     skb->dev, in_dev, &itag);\n\t}\n\treturn 0;\n}\n\nint udp_rcv(struct sk_buff *skb)\n{\n\treturn __udp4_lib_rcv(skb, dev_net(skb->dev)->ipv4.udp_table, IPPROTO_UDP);\n}\n\nvoid udp_destroy_sock(struct sock *sk)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\tbool slow = lock_sock_fast(sk);\n\n\t \n\tsock_set_flag(sk, SOCK_DEAD);\n\tudp_flush_pending_frames(sk);\n\tunlock_sock_fast(sk, slow);\n\tif (static_branch_unlikely(&udp_encap_needed_key)) {\n\t\tif (up->encap_type) {\n\t\t\tvoid (*encap_destroy)(struct sock *sk);\n\t\t\tencap_destroy = READ_ONCE(up->encap_destroy);\n\t\t\tif (encap_destroy)\n\t\t\t\tencap_destroy(sk);\n\t\t}\n\t\tif (udp_test_bit(ENCAP_ENABLED, sk))\n\t\t\tstatic_branch_dec(&udp_encap_needed_key);\n\t}\n}\n\n \nint udp_lib_setsockopt(struct sock *sk, int level, int optname,\n\t\t       sockptr_t optval, unsigned int optlen,\n\t\t       int (*push_pending_frames)(struct sock *))\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\tint val, valbool;\n\tint err = 0;\n\tint is_udplite = IS_UDPLITE(sk);\n\n\tif (level == SOL_SOCKET) {\n\t\terr = sk_setsockopt(sk, level, optname, optval, optlen);\n\n\t\tif (optname == SO_RCVBUF || optname == SO_RCVBUFFORCE) {\n\t\t\tsockopt_lock_sock(sk);\n\t\t\t \n\t\t\tWRITE_ONCE(up->forward_threshold, sk->sk_rcvbuf >> 2);\n\t\t\tsockopt_release_sock(sk);\n\t\t}\n\t\treturn err;\n\t}\n\n\tif (optlen < sizeof(int))\n\t\treturn -EINVAL;\n\n\tif (copy_from_sockptr(&val, optval, sizeof(val)))\n\t\treturn -EFAULT;\n\n\tvalbool = val ? 1 : 0;\n\n\tswitch (optname) {\n\tcase UDP_CORK:\n\t\tif (val != 0) {\n\t\t\tudp_set_bit(CORK, sk);\n\t\t} else {\n\t\t\tudp_clear_bit(CORK, sk);\n\t\t\tlock_sock(sk);\n\t\t\tpush_pending_frames(sk);\n\t\t\trelease_sock(sk);\n\t\t}\n\t\tbreak;\n\n\tcase UDP_ENCAP:\n\t\tswitch (val) {\n\t\tcase 0:\n#ifdef CONFIG_XFRM\n\t\tcase UDP_ENCAP_ESPINUDP:\n\t\tcase UDP_ENCAP_ESPINUDP_NON_IKE:\n#if IS_ENABLED(CONFIG_IPV6)\n\t\t\tif (sk->sk_family == AF_INET6)\n\t\t\t\tWRITE_ONCE(up->encap_rcv,\n\t\t\t\t\t   ipv6_stub->xfrm6_udp_encap_rcv);\n\t\t\telse\n#endif\n\t\t\t\tWRITE_ONCE(up->encap_rcv,\n\t\t\t\t\t   xfrm4_udp_encap_rcv);\n#endif\n\t\t\tfallthrough;\n\t\tcase UDP_ENCAP_L2TPINUDP:\n\t\t\tWRITE_ONCE(up->encap_type, val);\n\t\t\tudp_tunnel_encap_enable(sk);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\terr = -ENOPROTOOPT;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase UDP_NO_CHECK6_TX:\n\t\tudp_set_no_check6_tx(sk, valbool);\n\t\tbreak;\n\n\tcase UDP_NO_CHECK6_RX:\n\t\tudp_set_no_check6_rx(sk, valbool);\n\t\tbreak;\n\n\tcase UDP_SEGMENT:\n\t\tif (val < 0 || val > USHRT_MAX)\n\t\t\treturn -EINVAL;\n\t\tWRITE_ONCE(up->gso_size, val);\n\t\tbreak;\n\n\tcase UDP_GRO:\n\n\t\t \n\t\tif (valbool)\n\t\t\tudp_tunnel_encap_enable(sk);\n\t\tudp_assign_bit(GRO_ENABLED, sk, valbool);\n\t\tudp_assign_bit(ACCEPT_L4, sk, valbool);\n\t\tbreak;\n\n\t \n\t \n\tcase UDPLITE_SEND_CSCOV:\n\t\tif (!is_udplite)          \n\t\t\treturn -ENOPROTOOPT;\n\t\tif (val != 0 && val < 8)  \n\t\t\tval = 8;\n\t\telse if (val > USHRT_MAX)\n\t\t\tval = USHRT_MAX;\n\t\tWRITE_ONCE(up->pcslen, val);\n\t\tudp_set_bit(UDPLITE_SEND_CC, sk);\n\t\tbreak;\n\n\t \n\tcase UDPLITE_RECV_CSCOV:\n\t\tif (!is_udplite)          \n\t\t\treturn -ENOPROTOOPT;\n\t\tif (val != 0 && val < 8)  \n\t\t\tval = 8;\n\t\telse if (val > USHRT_MAX)\n\t\t\tval = USHRT_MAX;\n\t\tWRITE_ONCE(up->pcrlen, val);\n\t\tudp_set_bit(UDPLITE_RECV_CC, sk);\n\t\tbreak;\n\n\tdefault:\n\t\terr = -ENOPROTOOPT;\n\t\tbreak;\n\t}\n\n\treturn err;\n}\nEXPORT_SYMBOL(udp_lib_setsockopt);\n\nint udp_setsockopt(struct sock *sk, int level, int optname, sockptr_t optval,\n\t\t   unsigned int optlen)\n{\n\tif (level == SOL_UDP  ||  level == SOL_UDPLITE || level == SOL_SOCKET)\n\t\treturn udp_lib_setsockopt(sk, level, optname,\n\t\t\t\t\t  optval, optlen,\n\t\t\t\t\t  udp_push_pending_frames);\n\treturn ip_setsockopt(sk, level, optname, optval, optlen);\n}\n\nint udp_lib_getsockopt(struct sock *sk, int level, int optname,\n\t\t       char __user *optval, int __user *optlen)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\tint val, len;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\n\tlen = min_t(unsigned int, len, sizeof(int));\n\n\tif (len < 0)\n\t\treturn -EINVAL;\n\n\tswitch (optname) {\n\tcase UDP_CORK:\n\t\tval = udp_test_bit(CORK, sk);\n\t\tbreak;\n\n\tcase UDP_ENCAP:\n\t\tval = READ_ONCE(up->encap_type);\n\t\tbreak;\n\n\tcase UDP_NO_CHECK6_TX:\n\t\tval = udp_get_no_check6_tx(sk);\n\t\tbreak;\n\n\tcase UDP_NO_CHECK6_RX:\n\t\tval = udp_get_no_check6_rx(sk);\n\t\tbreak;\n\n\tcase UDP_SEGMENT:\n\t\tval = READ_ONCE(up->gso_size);\n\t\tbreak;\n\n\tcase UDP_GRO:\n\t\tval = udp_test_bit(GRO_ENABLED, sk);\n\t\tbreak;\n\n\t \n\tcase UDPLITE_SEND_CSCOV:\n\t\tval = READ_ONCE(up->pcslen);\n\t\tbreak;\n\n\tcase UDPLITE_RECV_CSCOV:\n\t\tval = READ_ONCE(up->pcrlen);\n\t\tbreak;\n\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (copy_to_user(optval, &val, len))\n\t\treturn -EFAULT;\n\treturn 0;\n}\nEXPORT_SYMBOL(udp_lib_getsockopt);\n\nint udp_getsockopt(struct sock *sk, int level, int optname,\n\t\t   char __user *optval, int __user *optlen)\n{\n\tif (level == SOL_UDP  ||  level == SOL_UDPLITE)\n\t\treturn udp_lib_getsockopt(sk, level, optname, optval, optlen);\n\treturn ip_getsockopt(sk, level, optname, optval, optlen);\n}\n\n \n__poll_t udp_poll(struct file *file, struct socket *sock, poll_table *wait)\n{\n\t__poll_t mask = datagram_poll(file, sock, wait);\n\tstruct sock *sk = sock->sk;\n\n\tif (!skb_queue_empty_lockless(&udp_sk(sk)->reader_queue))\n\t\tmask |= EPOLLIN | EPOLLRDNORM;\n\n\t \n\tif ((mask & EPOLLRDNORM) && !(file->f_flags & O_NONBLOCK) &&\n\t    !(sk->sk_shutdown & RCV_SHUTDOWN) && first_packet_length(sk) == -1)\n\t\tmask &= ~(EPOLLIN | EPOLLRDNORM);\n\n\t \n\tif (sk_is_readable(sk))\n\t\tmask |= EPOLLIN | EPOLLRDNORM;\n\treturn mask;\n\n}\nEXPORT_SYMBOL(udp_poll);\n\nint udp_abort(struct sock *sk, int err)\n{\n\tif (!has_current_bpf_ctx())\n\t\tlock_sock(sk);\n\n\t \n\tif (sock_flag(sk, SOCK_DEAD))\n\t\tgoto out;\n\n\tsk->sk_err = err;\n\tsk_error_report(sk);\n\t__udp_disconnect(sk, 0);\n\nout:\n\tif (!has_current_bpf_ctx())\n\t\trelease_sock(sk);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(udp_abort);\n\nstruct proto udp_prot = {\n\t.name\t\t\t= \"UDP\",\n\t.owner\t\t\t= THIS_MODULE,\n\t.close\t\t\t= udp_lib_close,\n\t.pre_connect\t\t= udp_pre_connect,\n\t.connect\t\t= ip4_datagram_connect,\n\t.disconnect\t\t= udp_disconnect,\n\t.ioctl\t\t\t= udp_ioctl,\n\t.init\t\t\t= udp_init_sock,\n\t.destroy\t\t= udp_destroy_sock,\n\t.setsockopt\t\t= udp_setsockopt,\n\t.getsockopt\t\t= udp_getsockopt,\n\t.sendmsg\t\t= udp_sendmsg,\n\t.recvmsg\t\t= udp_recvmsg,\n\t.splice_eof\t\t= udp_splice_eof,\n\t.release_cb\t\t= ip4_datagram_release_cb,\n\t.hash\t\t\t= udp_lib_hash,\n\t.unhash\t\t\t= udp_lib_unhash,\n\t.rehash\t\t\t= udp_v4_rehash,\n\t.get_port\t\t= udp_v4_get_port,\n\t.put_port\t\t= udp_lib_unhash,\n#ifdef CONFIG_BPF_SYSCALL\n\t.psock_update_sk_prot\t= udp_bpf_update_proto,\n#endif\n\t.memory_allocated\t= &udp_memory_allocated,\n\t.per_cpu_fw_alloc\t= &udp_memory_per_cpu_fw_alloc,\n\n\t.sysctl_mem\t\t= sysctl_udp_mem,\n\t.sysctl_wmem_offset\t= offsetof(struct net, ipv4.sysctl_udp_wmem_min),\n\t.sysctl_rmem_offset\t= offsetof(struct net, ipv4.sysctl_udp_rmem_min),\n\t.obj_size\t\t= sizeof(struct udp_sock),\n\t.h.udp_table\t\t= NULL,\n\t.diag_destroy\t\t= udp_abort,\n};\nEXPORT_SYMBOL(udp_prot);\n\n \n#ifdef CONFIG_PROC_FS\n\nstatic unsigned short seq_file_family(const struct seq_file *seq);\nstatic bool seq_sk_match(struct seq_file *seq, const struct sock *sk)\n{\n\tunsigned short family = seq_file_family(seq);\n\n\t \n\treturn ((family == AF_UNSPEC || family == sk->sk_family) &&\n\t\tnet_eq(sock_net(sk), seq_file_net(seq)));\n}\n\n#ifdef CONFIG_BPF_SYSCALL\nstatic const struct seq_operations bpf_iter_udp_seq_ops;\n#endif\nstatic struct udp_table *udp_get_table_seq(struct seq_file *seq,\n\t\t\t\t\t   struct net *net)\n{\n\tconst struct udp_seq_afinfo *afinfo;\n\n#ifdef CONFIG_BPF_SYSCALL\n\tif (seq->op == &bpf_iter_udp_seq_ops)\n\t\treturn net->ipv4.udp_table;\n#endif\n\n\tafinfo = pde_data(file_inode(seq->file));\n\treturn afinfo->udp_table ? : net->ipv4.udp_table;\n}\n\nstatic struct sock *udp_get_first(struct seq_file *seq, int start)\n{\n\tstruct udp_iter_state *state = seq->private;\n\tstruct net *net = seq_file_net(seq);\n\tstruct udp_table *udptable;\n\tstruct sock *sk;\n\n\tudptable = udp_get_table_seq(seq, net);\n\n\tfor (state->bucket = start; state->bucket <= udptable->mask;\n\t     ++state->bucket) {\n\t\tstruct udp_hslot *hslot = &udptable->hash[state->bucket];\n\n\t\tif (hlist_empty(&hslot->head))\n\t\t\tcontinue;\n\n\t\tspin_lock_bh(&hslot->lock);\n\t\tsk_for_each(sk, &hslot->head) {\n\t\t\tif (seq_sk_match(seq, sk))\n\t\t\t\tgoto found;\n\t\t}\n\t\tspin_unlock_bh(&hslot->lock);\n\t}\n\tsk = NULL;\nfound:\n\treturn sk;\n}\n\nstatic struct sock *udp_get_next(struct seq_file *seq, struct sock *sk)\n{\n\tstruct udp_iter_state *state = seq->private;\n\tstruct net *net = seq_file_net(seq);\n\tstruct udp_table *udptable;\n\n\tdo {\n\t\tsk = sk_next(sk);\n\t} while (sk && !seq_sk_match(seq, sk));\n\n\tif (!sk) {\n\t\tudptable = udp_get_table_seq(seq, net);\n\n\t\tif (state->bucket <= udptable->mask)\n\t\t\tspin_unlock_bh(&udptable->hash[state->bucket].lock);\n\n\t\treturn udp_get_first(seq, state->bucket + 1);\n\t}\n\treturn sk;\n}\n\nstatic struct sock *udp_get_idx(struct seq_file *seq, loff_t pos)\n{\n\tstruct sock *sk = udp_get_first(seq, 0);\n\n\tif (sk)\n\t\twhile (pos && (sk = udp_get_next(seq, sk)) != NULL)\n\t\t\t--pos;\n\treturn pos ? NULL : sk;\n}\n\nvoid *udp_seq_start(struct seq_file *seq, loff_t *pos)\n{\n\tstruct udp_iter_state *state = seq->private;\n\tstate->bucket = MAX_UDP_PORTS;\n\n\treturn *pos ? udp_get_idx(seq, *pos-1) : SEQ_START_TOKEN;\n}\nEXPORT_SYMBOL(udp_seq_start);\n\nvoid *udp_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\tstruct sock *sk;\n\n\tif (v == SEQ_START_TOKEN)\n\t\tsk = udp_get_idx(seq, 0);\n\telse\n\t\tsk = udp_get_next(seq, v);\n\n\t++*pos;\n\treturn sk;\n}\nEXPORT_SYMBOL(udp_seq_next);\n\nvoid udp_seq_stop(struct seq_file *seq, void *v)\n{\n\tstruct udp_iter_state *state = seq->private;\n\tstruct udp_table *udptable;\n\n\tudptable = udp_get_table_seq(seq, seq_file_net(seq));\n\n\tif (state->bucket <= udptable->mask)\n\t\tspin_unlock_bh(&udptable->hash[state->bucket].lock);\n}\nEXPORT_SYMBOL(udp_seq_stop);\n\n \nstatic void udp4_format_sock(struct sock *sp, struct seq_file *f,\n\t\tint bucket)\n{\n\tstruct inet_sock *inet = inet_sk(sp);\n\t__be32 dest = inet->inet_daddr;\n\t__be32 src  = inet->inet_rcv_saddr;\n\t__u16 destp\t  = ntohs(inet->inet_dport);\n\t__u16 srcp\t  = ntohs(inet->inet_sport);\n\n\tseq_printf(f, \"%5d: %08X:%04X %08X:%04X\"\n\t\t\" %02X %08X:%08X %02X:%08lX %08X %5u %8d %lu %d %pK %u\",\n\t\tbucket, src, srcp, dest, destp, sp->sk_state,\n\t\tsk_wmem_alloc_get(sp),\n\t\tudp_rqueue_get(sp),\n\t\t0, 0L, 0,\n\t\tfrom_kuid_munged(seq_user_ns(f), sock_i_uid(sp)),\n\t\t0, sock_i_ino(sp),\n\t\trefcount_read(&sp->sk_refcnt), sp,\n\t\tatomic_read(&sp->sk_drops));\n}\n\nint udp4_seq_show(struct seq_file *seq, void *v)\n{\n\tseq_setwidth(seq, 127);\n\tif (v == SEQ_START_TOKEN)\n\t\tseq_puts(seq, \"   sl  local_address rem_address   st tx_queue \"\n\t\t\t   \"rx_queue tr tm->when retrnsmt   uid  timeout \"\n\t\t\t   \"inode ref pointer drops\");\n\telse {\n\t\tstruct udp_iter_state *state = seq->private;\n\n\t\tudp4_format_sock(v, seq, state->bucket);\n\t}\n\tseq_pad(seq, '\\n');\n\treturn 0;\n}\n\n#ifdef CONFIG_BPF_SYSCALL\nstruct bpf_iter__udp {\n\t__bpf_md_ptr(struct bpf_iter_meta *, meta);\n\t__bpf_md_ptr(struct udp_sock *, udp_sk);\n\tuid_t uid __aligned(8);\n\tint bucket __aligned(8);\n};\n\nstruct bpf_udp_iter_state {\n\tstruct udp_iter_state state;\n\tunsigned int cur_sk;\n\tunsigned int end_sk;\n\tunsigned int max_sk;\n\tint offset;\n\tstruct sock **batch;\n\tbool st_bucket_done;\n};\n\nstatic int bpf_iter_udp_realloc_batch(struct bpf_udp_iter_state *iter,\n\t\t\t\t      unsigned int new_batch_sz);\nstatic struct sock *bpf_iter_udp_batch(struct seq_file *seq)\n{\n\tstruct bpf_udp_iter_state *iter = seq->private;\n\tstruct udp_iter_state *state = &iter->state;\n\tstruct net *net = seq_file_net(seq);\n\tint resume_bucket, resume_offset;\n\tstruct udp_table *udptable;\n\tunsigned int batch_sks = 0;\n\tbool resized = false;\n\tstruct sock *sk;\n\n\tresume_bucket = state->bucket;\n\tresume_offset = iter->offset;\n\n\t \n\tif (iter->st_bucket_done)\n\t\tstate->bucket++;\n\n\tudptable = udp_get_table_seq(seq, net);\n\nagain:\n\t \n\titer->cur_sk = 0;\n\titer->end_sk = 0;\n\titer->st_bucket_done = false;\n\tbatch_sks = 0;\n\n\tfor (; state->bucket <= udptable->mask; state->bucket++) {\n\t\tstruct udp_hslot *hslot2 = &udptable->hash2[state->bucket];\n\n\t\tif (hlist_empty(&hslot2->head))\n\t\t\tcontinue;\n\n\t\titer->offset = 0;\n\t\tspin_lock_bh(&hslot2->lock);\n\t\tudp_portaddr_for_each_entry(sk, &hslot2->head) {\n\t\t\tif (seq_sk_match(seq, sk)) {\n\t\t\t\t \n\t\t\t\tif (state->bucket == resume_bucket &&\n\t\t\t\t    iter->offset < resume_offset) {\n\t\t\t\t\t++iter->offset;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tif (iter->end_sk < iter->max_sk) {\n\t\t\t\t\tsock_hold(sk);\n\t\t\t\t\titer->batch[iter->end_sk++] = sk;\n\t\t\t\t}\n\t\t\t\tbatch_sks++;\n\t\t\t}\n\t\t}\n\t\tspin_unlock_bh(&hslot2->lock);\n\n\t\tif (iter->end_sk)\n\t\t\tbreak;\n\t}\n\n\t \n\tif (!iter->end_sk)\n\t\treturn NULL;\n\n\tif (iter->end_sk == batch_sks) {\n\t\t \n\t\titer->st_bucket_done = true;\n\t\tgoto done;\n\t}\n\tif (!resized && !bpf_iter_udp_realloc_batch(iter, batch_sks * 3 / 2)) {\n\t\tresized = true;\n\t\t \n\t\tgoto again;\n\t}\ndone:\n\treturn iter->batch[0];\n}\n\nstatic void *bpf_iter_udp_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\tstruct bpf_udp_iter_state *iter = seq->private;\n\tstruct sock *sk;\n\n\t \n\tif (iter->cur_sk < iter->end_sk) {\n\t\tsock_put(iter->batch[iter->cur_sk++]);\n\t\t++iter->offset;\n\t}\n\n\t \n\tif (iter->cur_sk < iter->end_sk)\n\t\tsk = iter->batch[iter->cur_sk];\n\telse\n\t\t \n\t\tsk = bpf_iter_udp_batch(seq);\n\n\t++*pos;\n\treturn sk;\n}\n\nstatic void *bpf_iter_udp_seq_start(struct seq_file *seq, loff_t *pos)\n{\n\t \n\tif (*pos)\n\t\treturn bpf_iter_udp_batch(seq);\n\n\treturn SEQ_START_TOKEN;\n}\n\nstatic int udp_prog_seq_show(struct bpf_prog *prog, struct bpf_iter_meta *meta,\n\t\t\t     struct udp_sock *udp_sk, uid_t uid, int bucket)\n{\n\tstruct bpf_iter__udp ctx;\n\n\tmeta->seq_num--;   \n\tctx.meta = meta;\n\tctx.udp_sk = udp_sk;\n\tctx.uid = uid;\n\tctx.bucket = bucket;\n\treturn bpf_iter_run_prog(prog, &ctx);\n}\n\nstatic int bpf_iter_udp_seq_show(struct seq_file *seq, void *v)\n{\n\tstruct udp_iter_state *state = seq->private;\n\tstruct bpf_iter_meta meta;\n\tstruct bpf_prog *prog;\n\tstruct sock *sk = v;\n\tuid_t uid;\n\tint ret;\n\n\tif (v == SEQ_START_TOKEN)\n\t\treturn 0;\n\n\tlock_sock(sk);\n\n\tif (unlikely(sk_unhashed(sk))) {\n\t\tret = SEQ_SKIP;\n\t\tgoto unlock;\n\t}\n\n\tuid = from_kuid_munged(seq_user_ns(seq), sock_i_uid(sk));\n\tmeta.seq = seq;\n\tprog = bpf_iter_get_info(&meta, false);\n\tret = udp_prog_seq_show(prog, &meta, v, uid, state->bucket);\n\nunlock:\n\trelease_sock(sk);\n\treturn ret;\n}\n\nstatic void bpf_iter_udp_put_batch(struct bpf_udp_iter_state *iter)\n{\n\twhile (iter->cur_sk < iter->end_sk)\n\t\tsock_put(iter->batch[iter->cur_sk++]);\n}\n\nstatic void bpf_iter_udp_seq_stop(struct seq_file *seq, void *v)\n{\n\tstruct bpf_udp_iter_state *iter = seq->private;\n\tstruct bpf_iter_meta meta;\n\tstruct bpf_prog *prog;\n\n\tif (!v) {\n\t\tmeta.seq = seq;\n\t\tprog = bpf_iter_get_info(&meta, true);\n\t\tif (prog)\n\t\t\t(void)udp_prog_seq_show(prog, &meta, v, 0, 0);\n\t}\n\n\tif (iter->cur_sk < iter->end_sk) {\n\t\tbpf_iter_udp_put_batch(iter);\n\t\titer->st_bucket_done = false;\n\t}\n}\n\nstatic const struct seq_operations bpf_iter_udp_seq_ops = {\n\t.start\t\t= bpf_iter_udp_seq_start,\n\t.next\t\t= bpf_iter_udp_seq_next,\n\t.stop\t\t= bpf_iter_udp_seq_stop,\n\t.show\t\t= bpf_iter_udp_seq_show,\n};\n#endif\n\nstatic unsigned short seq_file_family(const struct seq_file *seq)\n{\n\tconst struct udp_seq_afinfo *afinfo;\n\n#ifdef CONFIG_BPF_SYSCALL\n\t \n\tif (seq->op == &bpf_iter_udp_seq_ops)\n\t\treturn AF_UNSPEC;\n#endif\n\n\t \n\tafinfo = pde_data(file_inode(seq->file));\n\treturn afinfo->family;\n}\n\nconst struct seq_operations udp_seq_ops = {\n\t.start\t\t= udp_seq_start,\n\t.next\t\t= udp_seq_next,\n\t.stop\t\t= udp_seq_stop,\n\t.show\t\t= udp4_seq_show,\n};\nEXPORT_SYMBOL(udp_seq_ops);\n\nstatic struct udp_seq_afinfo udp4_seq_afinfo = {\n\t.family\t\t= AF_INET,\n\t.udp_table\t= NULL,\n};\n\nstatic int __net_init udp4_proc_init_net(struct net *net)\n{\n\tif (!proc_create_net_data(\"udp\", 0444, net->proc_net, &udp_seq_ops,\n\t\t\tsizeof(struct udp_iter_state), &udp4_seq_afinfo))\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nstatic void __net_exit udp4_proc_exit_net(struct net *net)\n{\n\tremove_proc_entry(\"udp\", net->proc_net);\n}\n\nstatic struct pernet_operations udp4_net_ops = {\n\t.init = udp4_proc_init_net,\n\t.exit = udp4_proc_exit_net,\n};\n\nint __init udp4_proc_init(void)\n{\n\treturn register_pernet_subsys(&udp4_net_ops);\n}\n\nvoid udp4_proc_exit(void)\n{\n\tunregister_pernet_subsys(&udp4_net_ops);\n}\n#endif  \n\nstatic __initdata unsigned long uhash_entries;\nstatic int __init set_uhash_entries(char *str)\n{\n\tssize_t ret;\n\n\tif (!str)\n\t\treturn 0;\n\n\tret = kstrtoul(str, 0, &uhash_entries);\n\tif (ret)\n\t\treturn 0;\n\n\tif (uhash_entries && uhash_entries < UDP_HTABLE_SIZE_MIN)\n\t\tuhash_entries = UDP_HTABLE_SIZE_MIN;\n\treturn 1;\n}\n__setup(\"uhash_entries=\", set_uhash_entries);\n\nvoid __init udp_table_init(struct udp_table *table, const char *name)\n{\n\tunsigned int i;\n\n\ttable->hash = alloc_large_system_hash(name,\n\t\t\t\t\t      2 * sizeof(struct udp_hslot),\n\t\t\t\t\t      uhash_entries,\n\t\t\t\t\t      21,  \n\t\t\t\t\t      0,\n\t\t\t\t\t      &table->log,\n\t\t\t\t\t      &table->mask,\n\t\t\t\t\t      UDP_HTABLE_SIZE_MIN,\n\t\t\t\t\t      UDP_HTABLE_SIZE_MAX);\n\n\ttable->hash2 = table->hash + (table->mask + 1);\n\tfor (i = 0; i <= table->mask; i++) {\n\t\tINIT_HLIST_HEAD(&table->hash[i].head);\n\t\ttable->hash[i].count = 0;\n\t\tspin_lock_init(&table->hash[i].lock);\n\t}\n\tfor (i = 0; i <= table->mask; i++) {\n\t\tINIT_HLIST_HEAD(&table->hash2[i].head);\n\t\ttable->hash2[i].count = 0;\n\t\tspin_lock_init(&table->hash2[i].lock);\n\t}\n}\n\nu32 udp_flow_hashrnd(void)\n{\n\tstatic u32 hashrnd __read_mostly;\n\n\tnet_get_random_once(&hashrnd, sizeof(hashrnd));\n\n\treturn hashrnd;\n}\nEXPORT_SYMBOL(udp_flow_hashrnd);\n\nstatic void __net_init udp_sysctl_init(struct net *net)\n{\n\tnet->ipv4.sysctl_udp_rmem_min = PAGE_SIZE;\n\tnet->ipv4.sysctl_udp_wmem_min = PAGE_SIZE;\n\n#ifdef CONFIG_NET_L3_MASTER_DEV\n\tnet->ipv4.sysctl_udp_l3mdev_accept = 0;\n#endif\n}\n\nstatic struct udp_table __net_init *udp_pernet_table_alloc(unsigned int hash_entries)\n{\n\tstruct udp_table *udptable;\n\tint i;\n\n\tudptable = kmalloc(sizeof(*udptable), GFP_KERNEL);\n\tif (!udptable)\n\t\tgoto out;\n\n\tudptable->hash = vmalloc_huge(hash_entries * 2 * sizeof(struct udp_hslot),\n\t\t\t\t      GFP_KERNEL_ACCOUNT);\n\tif (!udptable->hash)\n\t\tgoto free_table;\n\n\tudptable->hash2 = udptable->hash + hash_entries;\n\tudptable->mask = hash_entries - 1;\n\tudptable->log = ilog2(hash_entries);\n\n\tfor (i = 0; i < hash_entries; i++) {\n\t\tINIT_HLIST_HEAD(&udptable->hash[i].head);\n\t\tudptable->hash[i].count = 0;\n\t\tspin_lock_init(&udptable->hash[i].lock);\n\n\t\tINIT_HLIST_HEAD(&udptable->hash2[i].head);\n\t\tudptable->hash2[i].count = 0;\n\t\tspin_lock_init(&udptable->hash2[i].lock);\n\t}\n\n\treturn udptable;\n\nfree_table:\n\tkfree(udptable);\nout:\n\treturn NULL;\n}\n\nstatic void __net_exit udp_pernet_table_free(struct net *net)\n{\n\tstruct udp_table *udptable = net->ipv4.udp_table;\n\n\tif (udptable == &udp_table)\n\t\treturn;\n\n\tkvfree(udptable->hash);\n\tkfree(udptable);\n}\n\nstatic void __net_init udp_set_table(struct net *net)\n{\n\tstruct udp_table *udptable;\n\tunsigned int hash_entries;\n\tstruct net *old_net;\n\n\tif (net_eq(net, &init_net))\n\t\tgoto fallback;\n\n\told_net = current->nsproxy->net_ns;\n\thash_entries = READ_ONCE(old_net->ipv4.sysctl_udp_child_hash_entries);\n\tif (!hash_entries)\n\t\tgoto fallback;\n\n\t \n\tif (hash_entries < UDP_HTABLE_SIZE_MIN_PERNET)\n\t\thash_entries = UDP_HTABLE_SIZE_MIN_PERNET;\n\telse\n\t\thash_entries = roundup_pow_of_two(hash_entries);\n\n\tudptable = udp_pernet_table_alloc(hash_entries);\n\tif (udptable) {\n\t\tnet->ipv4.udp_table = udptable;\n\t} else {\n\t\tpr_warn(\"Failed to allocate UDP hash table (entries: %u) \"\n\t\t\t\"for a netns, fallback to the global one\\n\",\n\t\t\thash_entries);\nfallback:\n\t\tnet->ipv4.udp_table = &udp_table;\n\t}\n}\n\nstatic int __net_init udp_pernet_init(struct net *net)\n{\n\tudp_sysctl_init(net);\n\tudp_set_table(net);\n\n\treturn 0;\n}\n\nstatic void __net_exit udp_pernet_exit(struct net *net)\n{\n\tudp_pernet_table_free(net);\n}\n\nstatic struct pernet_operations __net_initdata udp_sysctl_ops = {\n\t.init\t= udp_pernet_init,\n\t.exit\t= udp_pernet_exit,\n};\n\n#if defined(CONFIG_BPF_SYSCALL) && defined(CONFIG_PROC_FS)\nDEFINE_BPF_ITER_FUNC(udp, struct bpf_iter_meta *meta,\n\t\t     struct udp_sock *udp_sk, uid_t uid, int bucket)\n\nstatic int bpf_iter_udp_realloc_batch(struct bpf_udp_iter_state *iter,\n\t\t\t\t      unsigned int new_batch_sz)\n{\n\tstruct sock **new_batch;\n\n\tnew_batch = kvmalloc_array(new_batch_sz, sizeof(*new_batch),\n\t\t\t\t   GFP_USER | __GFP_NOWARN);\n\tif (!new_batch)\n\t\treturn -ENOMEM;\n\n\tbpf_iter_udp_put_batch(iter);\n\tkvfree(iter->batch);\n\titer->batch = new_batch;\n\titer->max_sk = new_batch_sz;\n\n\treturn 0;\n}\n\n#define INIT_BATCH_SZ 16\n\nstatic int bpf_iter_init_udp(void *priv_data, struct bpf_iter_aux_info *aux)\n{\n\tstruct bpf_udp_iter_state *iter = priv_data;\n\tint ret;\n\n\tret = bpf_iter_init_seq_net(priv_data, aux);\n\tif (ret)\n\t\treturn ret;\n\n\tret = bpf_iter_udp_realloc_batch(iter, INIT_BATCH_SZ);\n\tif (ret)\n\t\tbpf_iter_fini_seq_net(priv_data);\n\n\treturn ret;\n}\n\nstatic void bpf_iter_fini_udp(void *priv_data)\n{\n\tstruct bpf_udp_iter_state *iter = priv_data;\n\n\tbpf_iter_fini_seq_net(priv_data);\n\tkvfree(iter->batch);\n}\n\nstatic const struct bpf_iter_seq_info udp_seq_info = {\n\t.seq_ops\t\t= &bpf_iter_udp_seq_ops,\n\t.init_seq_private\t= bpf_iter_init_udp,\n\t.fini_seq_private\t= bpf_iter_fini_udp,\n\t.seq_priv_size\t\t= sizeof(struct bpf_udp_iter_state),\n};\n\nstatic struct bpf_iter_reg udp_reg_info = {\n\t.target\t\t\t= \"udp\",\n\t.ctx_arg_info_size\t= 1,\n\t.ctx_arg_info\t\t= {\n\t\t{ offsetof(struct bpf_iter__udp, udp_sk),\n\t\t  PTR_TO_BTF_ID_OR_NULL | PTR_TRUSTED },\n\t},\n\t.seq_info\t\t= &udp_seq_info,\n};\n\nstatic void __init bpf_iter_register(void)\n{\n\tudp_reg_info.ctx_arg_info[0].btf_id = btf_sock_ids[BTF_SOCK_TYPE_UDP];\n\tif (bpf_iter_reg_target(&udp_reg_info))\n\t\tpr_warn(\"Warning: could not register bpf iterator udp\\n\");\n}\n#endif\n\nvoid __init udp_init(void)\n{\n\tunsigned long limit;\n\tunsigned int i;\n\n\tudp_table_init(&udp_table, \"UDP\");\n\tlimit = nr_free_buffer_pages() / 8;\n\tlimit = max(limit, 128UL);\n\tsysctl_udp_mem[0] = limit / 4 * 3;\n\tsysctl_udp_mem[1] = limit;\n\tsysctl_udp_mem[2] = sysctl_udp_mem[0] * 2;\n\n\t \n\tudp_busylocks_log = ilog2(nr_cpu_ids) + 4;\n\tudp_busylocks = kmalloc(sizeof(spinlock_t) << udp_busylocks_log,\n\t\t\t\tGFP_KERNEL);\n\tif (!udp_busylocks)\n\t\tpanic(\"UDP: failed to alloc udp_busylocks\\n\");\n\tfor (i = 0; i < (1U << udp_busylocks_log); i++)\n\t\tspin_lock_init(udp_busylocks + i);\n\n\tif (register_pernet_subsys(&udp_sysctl_ops))\n\t\tpanic(\"UDP: failed to init sysctl parameters.\\n\");\n\n#if defined(CONFIG_BPF_SYSCALL) && defined(CONFIG_PROC_FS)\n\tbpf_iter_register();\n#endif\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}