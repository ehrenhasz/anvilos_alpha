{
  "module_name": "tcp_bbr.c",
  "hash_id": "fc754c6b73e820ad73152462bafc1014fa412480dc74f5fb0b47c276799cdd92",
  "original_prompt": "Ingested from linux-6.6.14/net/ipv4/tcp_bbr.c",
  "human_readable_source": " \n#include <linux/btf.h>\n#include <linux/btf_ids.h>\n#include <linux/module.h>\n#include <net/tcp.h>\n#include <linux/inet_diag.h>\n#include <linux/inet.h>\n#include <linux/random.h>\n#include <linux/win_minmax.h>\n\n \n#define BW_SCALE 24\n#define BW_UNIT (1 << BW_SCALE)\n\n#define BBR_SCALE 8\t \n#define BBR_UNIT (1 << BBR_SCALE)\n\n \nenum bbr_mode {\n\tBBR_STARTUP,\t \n\tBBR_DRAIN,\t \n\tBBR_PROBE_BW,\t \n\tBBR_PROBE_RTT,\t \n};\n\n \nstruct bbr {\n\tu32\tmin_rtt_us;\t         \n\tu32\tmin_rtt_stamp;\t         \n\tu32\tprobe_rtt_done_stamp;    \n\tstruct minmax bw;\t \n\tu32\trtt_cnt;\t     \n\tu32     next_rtt_delivered;  \n\tu64\tcycle_mstamp;\t      \n\tu32     mode:3,\t\t      \n\t\tprev_ca_state:3,      \n\t\tpacket_conservation:1,   \n\t\tround_start:1,\t      \n\t\tidle_restart:1,\t      \n\t\tprobe_rtt_round_done:1,   \n\t\tunused:13,\n\t\tlt_is_sampling:1,     \n\t\tlt_rtt_cnt:7,\t      \n\t\tlt_use_bw:1;\t      \n\tu32\tlt_bw;\t\t      \n\tu32\tlt_last_delivered;    \n\tu32\tlt_last_stamp;\t      \n\tu32\tlt_last_lost;\t      \n\tu32\tpacing_gain:10,\t \n\t\tcwnd_gain:10,\t \n\t\tfull_bw_reached:1,    \n\t\tfull_bw_cnt:2,\t \n\t\tcycle_idx:3,\t \n\t\thas_seen_rtt:1,  \n\t\tunused_b:5;\n\tu32\tprior_cwnd;\t \n\tu32\tfull_bw;\t \n\n\t \n\tu64\tack_epoch_mstamp;\t \n\tu16\textra_acked[2];\t\t \n\tu32\tack_epoch_acked:20,\t \n\t\textra_acked_win_rtts:5,\t \n\t\textra_acked_win_idx:1,\t \n\t\tunused_c:6;\n};\n\n#define CYCLE_LEN\t8\t \n\n \nstatic const int bbr_bw_rtts = CYCLE_LEN + 2;\n \nstatic const u32 bbr_min_rtt_win_sec = 10;\n \nstatic const u32 bbr_probe_rtt_mode_ms = 200;\n \nstatic const int bbr_min_tso_rate = 1200000;\n\n \nstatic const int bbr_pacing_margin_percent = 1;\n\n \nstatic const int bbr_high_gain  = BBR_UNIT * 2885 / 1000 + 1;\n \nstatic const int bbr_drain_gain = BBR_UNIT * 1000 / 2885;\n \nstatic const int bbr_cwnd_gain  = BBR_UNIT * 2;\n \nstatic const int bbr_pacing_gain[] = {\n\tBBR_UNIT * 5 / 4,\t \n\tBBR_UNIT * 3 / 4,\t \n\tBBR_UNIT, BBR_UNIT, BBR_UNIT,\t \n\tBBR_UNIT, BBR_UNIT, BBR_UNIT\t \n};\n \nstatic const u32 bbr_cycle_rand = 7;\n\n \nstatic const u32 bbr_cwnd_min_target = 4;\n\n \n \nstatic const u32 bbr_full_bw_thresh = BBR_UNIT * 5 / 4;\n \nstatic const u32 bbr_full_bw_cnt = 3;\n\n \n \nstatic const u32 bbr_lt_intvl_min_rtts = 4;\n \nstatic const u32 bbr_lt_loss_thresh = 50;\n \nstatic const u32 bbr_lt_bw_ratio = BBR_UNIT / 8;\n \nstatic const u32 bbr_lt_bw_diff = 4000 / 8;\n \nstatic const u32 bbr_lt_bw_max_rtts = 48;\n\n \nstatic const int bbr_extra_acked_gain = BBR_UNIT;\n \nstatic const u32 bbr_extra_acked_win_rtts = 5;\n \nstatic const u32 bbr_ack_epoch_acked_reset_thresh = 1U << 20;\n \nstatic const u32 bbr_extra_acked_max_us = 100 * 1000;\n\nstatic void bbr_check_probe_rtt_done(struct sock *sk);\n\n \nstatic bool bbr_full_bw_reached(const struct sock *sk)\n{\n\tconst struct bbr *bbr = inet_csk_ca(sk);\n\n\treturn bbr->full_bw_reached;\n}\n\n \nstatic u32 bbr_max_bw(const struct sock *sk)\n{\n\tstruct bbr *bbr = inet_csk_ca(sk);\n\n\treturn minmax_get(&bbr->bw);\n}\n\n \nstatic u32 bbr_bw(const struct sock *sk)\n{\n\tstruct bbr *bbr = inet_csk_ca(sk);\n\n\treturn bbr->lt_use_bw ? bbr->lt_bw : bbr_max_bw(sk);\n}\n\n \nstatic u16 bbr_extra_acked(const struct sock *sk)\n{\n\tstruct bbr *bbr = inet_csk_ca(sk);\n\n\treturn max(bbr->extra_acked[0], bbr->extra_acked[1]);\n}\n\n \nstatic u64 bbr_rate_bytes_per_sec(struct sock *sk, u64 rate, int gain)\n{\n\tunsigned int mss = tcp_sk(sk)->mss_cache;\n\n\trate *= mss;\n\trate *= gain;\n\trate >>= BBR_SCALE;\n\trate *= USEC_PER_SEC / 100 * (100 - bbr_pacing_margin_percent);\n\treturn rate >> BW_SCALE;\n}\n\n \nstatic unsigned long bbr_bw_to_pacing_rate(struct sock *sk, u32 bw, int gain)\n{\n\tu64 rate = bw;\n\n\trate = bbr_rate_bytes_per_sec(sk, rate, gain);\n\trate = min_t(u64, rate, sk->sk_max_pacing_rate);\n\treturn rate;\n}\n\n \nstatic void bbr_init_pacing_rate_from_rtt(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct bbr *bbr = inet_csk_ca(sk);\n\tu64 bw;\n\tu32 rtt_us;\n\n\tif (tp->srtt_us) {\t\t \n\t\trtt_us = max(tp->srtt_us >> 3, 1U);\n\t\tbbr->has_seen_rtt = 1;\n\t} else {\t\t\t  \n\t\trtt_us = USEC_PER_MSEC;\t  \n\t}\n\tbw = (u64)tcp_snd_cwnd(tp) * BW_UNIT;\n\tdo_div(bw, rtt_us);\n\tsk->sk_pacing_rate = bbr_bw_to_pacing_rate(sk, bw, bbr_high_gain);\n}\n\n \nstatic void bbr_set_pacing_rate(struct sock *sk, u32 bw, int gain)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct bbr *bbr = inet_csk_ca(sk);\n\tunsigned long rate = bbr_bw_to_pacing_rate(sk, bw, gain);\n\n\tif (unlikely(!bbr->has_seen_rtt && tp->srtt_us))\n\t\tbbr_init_pacing_rate_from_rtt(sk);\n\tif (bbr_full_bw_reached(sk) || rate > sk->sk_pacing_rate)\n\t\tsk->sk_pacing_rate = rate;\n}\n\n \n__bpf_kfunc static u32 bbr_min_tso_segs(struct sock *sk)\n{\n\treturn sk->sk_pacing_rate < (bbr_min_tso_rate >> 3) ? 1 : 2;\n}\n\nstatic u32 bbr_tso_segs_goal(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 segs, bytes;\n\n\t \n\tbytes = min_t(unsigned long,\n\t\t      sk->sk_pacing_rate >> READ_ONCE(sk->sk_pacing_shift),\n\t\t      GSO_LEGACY_MAX_SIZE - 1 - MAX_TCP_HEADER);\n\tsegs = max_t(u32, bytes / tp->mss_cache, bbr_min_tso_segs(sk));\n\n\treturn min(segs, 0x7FU);\n}\n\n \nstatic void bbr_save_cwnd(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct bbr *bbr = inet_csk_ca(sk);\n\n\tif (bbr->prev_ca_state < TCP_CA_Recovery && bbr->mode != BBR_PROBE_RTT)\n\t\tbbr->prior_cwnd = tcp_snd_cwnd(tp);   \n\telse   \n\t\tbbr->prior_cwnd = max(bbr->prior_cwnd, tcp_snd_cwnd(tp));\n}\n\n__bpf_kfunc static void bbr_cwnd_event(struct sock *sk, enum tcp_ca_event event)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct bbr *bbr = inet_csk_ca(sk);\n\n\tif (event == CA_EVENT_TX_START && tp->app_limited) {\n\t\tbbr->idle_restart = 1;\n\t\tbbr->ack_epoch_mstamp = tp->tcp_mstamp;\n\t\tbbr->ack_epoch_acked = 0;\n\t\t \n\t\tif (bbr->mode == BBR_PROBE_BW)\n\t\t\tbbr_set_pacing_rate(sk, bbr_bw(sk), BBR_UNIT);\n\t\telse if (bbr->mode == BBR_PROBE_RTT)\n\t\t\tbbr_check_probe_rtt_done(sk);\n\t}\n}\n\n \nstatic u32 bbr_bdp(struct sock *sk, u32 bw, int gain)\n{\n\tstruct bbr *bbr = inet_csk_ca(sk);\n\tu32 bdp;\n\tu64 w;\n\n\t \n\tif (unlikely(bbr->min_rtt_us == ~0U))\t  \n\t\treturn TCP_INIT_CWND;   \n\n\tw = (u64)bw * bbr->min_rtt_us;\n\n\t \n\tbdp = (((w * gain) >> BBR_SCALE) + BW_UNIT - 1) / BW_UNIT;\n\n\treturn bdp;\n}\n\n \nstatic u32 bbr_quantization_budget(struct sock *sk, u32 cwnd)\n{\n\tstruct bbr *bbr = inet_csk_ca(sk);\n\n\t \n\tcwnd += 3 * bbr_tso_segs_goal(sk);\n\n\t \n\tcwnd = (cwnd + 1) & ~1U;\n\n\t \n\tif (bbr->mode == BBR_PROBE_BW && bbr->cycle_idx == 0)\n\t\tcwnd += 2;\n\n\treturn cwnd;\n}\n\n \nstatic u32 bbr_inflight(struct sock *sk, u32 bw, int gain)\n{\n\tu32 inflight;\n\n\tinflight = bbr_bdp(sk, bw, gain);\n\tinflight = bbr_quantization_budget(sk, inflight);\n\n\treturn inflight;\n}\n\n \nstatic u32 bbr_packets_in_net_at_edt(struct sock *sk, u32 inflight_now)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct bbr *bbr = inet_csk_ca(sk);\n\tu64 now_ns, edt_ns, interval_us;\n\tu32 interval_delivered, inflight_at_edt;\n\n\tnow_ns = tp->tcp_clock_cache;\n\tedt_ns = max(tp->tcp_wstamp_ns, now_ns);\n\tinterval_us = div_u64(edt_ns - now_ns, NSEC_PER_USEC);\n\tinterval_delivered = (u64)bbr_bw(sk) * interval_us >> BW_SCALE;\n\tinflight_at_edt = inflight_now;\n\tif (bbr->pacing_gain > BBR_UNIT)               \n\t\tinflight_at_edt += bbr_tso_segs_goal(sk);   \n\tif (interval_delivered >= inflight_at_edt)\n\t\treturn 0;\n\treturn inflight_at_edt - interval_delivered;\n}\n\n \nstatic u32 bbr_ack_aggregation_cwnd(struct sock *sk)\n{\n\tu32 max_aggr_cwnd, aggr_cwnd = 0;\n\n\tif (bbr_extra_acked_gain && bbr_full_bw_reached(sk)) {\n\t\tmax_aggr_cwnd = ((u64)bbr_bw(sk) * bbr_extra_acked_max_us)\n\t\t\t\t/ BW_UNIT;\n\t\taggr_cwnd = (bbr_extra_acked_gain * bbr_extra_acked(sk))\n\t\t\t     >> BBR_SCALE;\n\t\taggr_cwnd = min(aggr_cwnd, max_aggr_cwnd);\n\t}\n\n\treturn aggr_cwnd;\n}\n\n \nstatic bool bbr_set_cwnd_to_recover_or_restore(\n\tstruct sock *sk, const struct rate_sample *rs, u32 acked, u32 *new_cwnd)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct bbr *bbr = inet_csk_ca(sk);\n\tu8 prev_state = bbr->prev_ca_state, state = inet_csk(sk)->icsk_ca_state;\n\tu32 cwnd = tcp_snd_cwnd(tp);\n\n\t \n\tif (rs->losses > 0)\n\t\tcwnd = max_t(s32, cwnd - rs->losses, 1);\n\n\tif (state == TCP_CA_Recovery && prev_state != TCP_CA_Recovery) {\n\t\t \n\t\tbbr->packet_conservation = 1;\n\t\tbbr->next_rtt_delivered = tp->delivered;   \n\t\t \n\t\tcwnd = tcp_packets_in_flight(tp) + acked;\n\t} else if (prev_state >= TCP_CA_Recovery && state < TCP_CA_Recovery) {\n\t\t \n\t\tcwnd = max(cwnd, bbr->prior_cwnd);\n\t\tbbr->packet_conservation = 0;\n\t}\n\tbbr->prev_ca_state = state;\n\n\tif (bbr->packet_conservation) {\n\t\t*new_cwnd = max(cwnd, tcp_packets_in_flight(tp) + acked);\n\t\treturn true;\t \n\t}\n\t*new_cwnd = cwnd;\n\treturn false;\n}\n\n \nstatic void bbr_set_cwnd(struct sock *sk, const struct rate_sample *rs,\n\t\t\t u32 acked, u32 bw, int gain)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct bbr *bbr = inet_csk_ca(sk);\n\tu32 cwnd = tcp_snd_cwnd(tp), target_cwnd = 0;\n\n\tif (!acked)\n\t\tgoto done;   \n\n\tif (bbr_set_cwnd_to_recover_or_restore(sk, rs, acked, &cwnd))\n\t\tgoto done;\n\n\ttarget_cwnd = bbr_bdp(sk, bw, gain);\n\n\t \n\ttarget_cwnd += bbr_ack_aggregation_cwnd(sk);\n\ttarget_cwnd = bbr_quantization_budget(sk, target_cwnd);\n\n\t \n\tif (bbr_full_bw_reached(sk))   \n\t\tcwnd = min(cwnd + acked, target_cwnd);\n\telse if (cwnd < target_cwnd || tp->delivered < TCP_INIT_CWND)\n\t\tcwnd = cwnd + acked;\n\tcwnd = max(cwnd, bbr_cwnd_min_target);\n\ndone:\n\ttcp_snd_cwnd_set(tp, min(cwnd, tp->snd_cwnd_clamp));\t \n\tif (bbr->mode == BBR_PROBE_RTT)   \n\t\ttcp_snd_cwnd_set(tp, min(tcp_snd_cwnd(tp), bbr_cwnd_min_target));\n}\n\n \nstatic bool bbr_is_next_cycle_phase(struct sock *sk,\n\t\t\t\t    const struct rate_sample *rs)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct bbr *bbr = inet_csk_ca(sk);\n\tbool is_full_length =\n\t\ttcp_stamp_us_delta(tp->delivered_mstamp, bbr->cycle_mstamp) >\n\t\tbbr->min_rtt_us;\n\tu32 inflight, bw;\n\n\t \n\tif (bbr->pacing_gain == BBR_UNIT)\n\t\treturn is_full_length;\t\t \n\n\tinflight = bbr_packets_in_net_at_edt(sk, rs->prior_in_flight);\n\tbw = bbr_max_bw(sk);\n\n\t \n\tif (bbr->pacing_gain > BBR_UNIT)\n\t\treturn is_full_length &&\n\t\t\t(rs->losses ||   \n\t\t\t inflight >= bbr_inflight(sk, bw, bbr->pacing_gain));\n\n\t \n\treturn is_full_length ||\n\t\tinflight <= bbr_inflight(sk, bw, BBR_UNIT);\n}\n\nstatic void bbr_advance_cycle_phase(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct bbr *bbr = inet_csk_ca(sk);\n\n\tbbr->cycle_idx = (bbr->cycle_idx + 1) & (CYCLE_LEN - 1);\n\tbbr->cycle_mstamp = tp->delivered_mstamp;\n}\n\n \nstatic void bbr_update_cycle_phase(struct sock *sk,\n\t\t\t\t   const struct rate_sample *rs)\n{\n\tstruct bbr *bbr = inet_csk_ca(sk);\n\n\tif (bbr->mode == BBR_PROBE_BW && bbr_is_next_cycle_phase(sk, rs))\n\t\tbbr_advance_cycle_phase(sk);\n}\n\nstatic void bbr_reset_startup_mode(struct sock *sk)\n{\n\tstruct bbr *bbr = inet_csk_ca(sk);\n\n\tbbr->mode = BBR_STARTUP;\n}\n\nstatic void bbr_reset_probe_bw_mode(struct sock *sk)\n{\n\tstruct bbr *bbr = inet_csk_ca(sk);\n\n\tbbr->mode = BBR_PROBE_BW;\n\tbbr->cycle_idx = CYCLE_LEN - 1 - get_random_u32_below(bbr_cycle_rand);\n\tbbr_advance_cycle_phase(sk);\t \n}\n\nstatic void bbr_reset_mode(struct sock *sk)\n{\n\tif (!bbr_full_bw_reached(sk))\n\t\tbbr_reset_startup_mode(sk);\n\telse\n\t\tbbr_reset_probe_bw_mode(sk);\n}\n\n \nstatic void bbr_reset_lt_bw_sampling_interval(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct bbr *bbr = inet_csk_ca(sk);\n\n\tbbr->lt_last_stamp = div_u64(tp->delivered_mstamp, USEC_PER_MSEC);\n\tbbr->lt_last_delivered = tp->delivered;\n\tbbr->lt_last_lost = tp->lost;\n\tbbr->lt_rtt_cnt = 0;\n}\n\n \nstatic void bbr_reset_lt_bw_sampling(struct sock *sk)\n{\n\tstruct bbr *bbr = inet_csk_ca(sk);\n\n\tbbr->lt_bw = 0;\n\tbbr->lt_use_bw = 0;\n\tbbr->lt_is_sampling = false;\n\tbbr_reset_lt_bw_sampling_interval(sk);\n}\n\n \nstatic void bbr_lt_bw_interval_done(struct sock *sk, u32 bw)\n{\n\tstruct bbr *bbr = inet_csk_ca(sk);\n\tu32 diff;\n\n\tif (bbr->lt_bw) {   \n\t\t \n\t\tdiff = abs(bw - bbr->lt_bw);\n\t\tif ((diff * BBR_UNIT <= bbr_lt_bw_ratio * bbr->lt_bw) ||\n\t\t    (bbr_rate_bytes_per_sec(sk, diff, BBR_UNIT) <=\n\t\t     bbr_lt_bw_diff)) {\n\t\t\t \n\t\t\tbbr->lt_bw = (bw + bbr->lt_bw) >> 1;   \n\t\t\tbbr->lt_use_bw = 1;\n\t\t\tbbr->pacing_gain = BBR_UNIT;   \n\t\t\tbbr->lt_rtt_cnt = 0;\n\t\t\treturn;\n\t\t}\n\t}\n\tbbr->lt_bw = bw;\n\tbbr_reset_lt_bw_sampling_interval(sk);\n}\n\n \nstatic void bbr_lt_bw_sampling(struct sock *sk, const struct rate_sample *rs)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct bbr *bbr = inet_csk_ca(sk);\n\tu32 lost, delivered;\n\tu64 bw;\n\tu32 t;\n\n\tif (bbr->lt_use_bw) {\t \n\t\tif (bbr->mode == BBR_PROBE_BW && bbr->round_start &&\n\t\t    ++bbr->lt_rtt_cnt >= bbr_lt_bw_max_rtts) {\n\t\t\tbbr_reset_lt_bw_sampling(sk);     \n\t\t\tbbr_reset_probe_bw_mode(sk);   \n\t\t}\n\t\treturn;\n\t}\n\n\t \n\tif (!bbr->lt_is_sampling) {\n\t\tif (!rs->losses)\n\t\t\treturn;\n\t\tbbr_reset_lt_bw_sampling_interval(sk);\n\t\tbbr->lt_is_sampling = true;\n\t}\n\n\t \n\tif (rs->is_app_limited) {\n\t\tbbr_reset_lt_bw_sampling(sk);\n\t\treturn;\n\t}\n\n\tif (bbr->round_start)\n\t\tbbr->lt_rtt_cnt++;\t \n\tif (bbr->lt_rtt_cnt < bbr_lt_intvl_min_rtts)\n\t\treturn;\t\t \n\tif (bbr->lt_rtt_cnt > 4 * bbr_lt_intvl_min_rtts) {\n\t\tbbr_reset_lt_bw_sampling(sk);   \n\t\treturn;\n\t}\n\n\t \n\tif (!rs->losses)\n\t\treturn;\n\n\t \n\tlost = tp->lost - bbr->lt_last_lost;\n\tdelivered = tp->delivered - bbr->lt_last_delivered;\n\t \n\tif (!delivered || (lost << BBR_SCALE) < bbr_lt_loss_thresh * delivered)\n\t\treturn;\n\n\t \n\tt = div_u64(tp->delivered_mstamp, USEC_PER_MSEC) - bbr->lt_last_stamp;\n\tif ((s32)t < 1)\n\t\treturn;\t\t \n\t \n\tif (t >= ~0U / USEC_PER_MSEC) {\n\t\tbbr_reset_lt_bw_sampling(sk);   \n\t\treturn;\n\t}\n\tt *= USEC_PER_MSEC;\n\tbw = (u64)delivered * BW_UNIT;\n\tdo_div(bw, t);\n\tbbr_lt_bw_interval_done(sk, bw);\n}\n\n \nstatic void bbr_update_bw(struct sock *sk, const struct rate_sample *rs)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct bbr *bbr = inet_csk_ca(sk);\n\tu64 bw;\n\n\tbbr->round_start = 0;\n\tif (rs->delivered < 0 || rs->interval_us <= 0)\n\t\treturn;  \n\n\t \n\tif (!before(rs->prior_delivered, bbr->next_rtt_delivered)) {\n\t\tbbr->next_rtt_delivered = tp->delivered;\n\t\tbbr->rtt_cnt++;\n\t\tbbr->round_start = 1;\n\t\tbbr->packet_conservation = 0;\n\t}\n\n\tbbr_lt_bw_sampling(sk, rs);\n\n\t \n\tbw = div64_long((u64)rs->delivered * BW_UNIT, rs->interval_us);\n\n\t \n\tif (!rs->is_app_limited || bw >= bbr_max_bw(sk)) {\n\t\t \n\t\tminmax_running_max(&bbr->bw, bbr_bw_rtts, bbr->rtt_cnt, bw);\n\t}\n}\n\n \nstatic void bbr_update_ack_aggregation(struct sock *sk,\n\t\t\t\t       const struct rate_sample *rs)\n{\n\tu32 epoch_us, expected_acked, extra_acked;\n\tstruct bbr *bbr = inet_csk_ca(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (!bbr_extra_acked_gain || rs->acked_sacked <= 0 ||\n\t    rs->delivered < 0 || rs->interval_us <= 0)\n\t\treturn;\n\n\tif (bbr->round_start) {\n\t\tbbr->extra_acked_win_rtts = min(0x1F,\n\t\t\t\t\t\tbbr->extra_acked_win_rtts + 1);\n\t\tif (bbr->extra_acked_win_rtts >= bbr_extra_acked_win_rtts) {\n\t\t\tbbr->extra_acked_win_rtts = 0;\n\t\t\tbbr->extra_acked_win_idx = bbr->extra_acked_win_idx ?\n\t\t\t\t\t\t   0 : 1;\n\t\t\tbbr->extra_acked[bbr->extra_acked_win_idx] = 0;\n\t\t}\n\t}\n\n\t \n\tepoch_us = tcp_stamp_us_delta(tp->delivered_mstamp,\n\t\t\t\t      bbr->ack_epoch_mstamp);\n\texpected_acked = ((u64)bbr_bw(sk) * epoch_us) / BW_UNIT;\n\n\t \n\tif (bbr->ack_epoch_acked <= expected_acked ||\n\t    (bbr->ack_epoch_acked + rs->acked_sacked >=\n\t     bbr_ack_epoch_acked_reset_thresh)) {\n\t\tbbr->ack_epoch_acked = 0;\n\t\tbbr->ack_epoch_mstamp = tp->delivered_mstamp;\n\t\texpected_acked = 0;\n\t}\n\n\t \n\tbbr->ack_epoch_acked = min_t(u32, 0xFFFFF,\n\t\t\t\t     bbr->ack_epoch_acked + rs->acked_sacked);\n\textra_acked = bbr->ack_epoch_acked - expected_acked;\n\textra_acked = min(extra_acked, tcp_snd_cwnd(tp));\n\tif (extra_acked > bbr->extra_acked[bbr->extra_acked_win_idx])\n\t\tbbr->extra_acked[bbr->extra_acked_win_idx] = extra_acked;\n}\n\n \nstatic void bbr_check_full_bw_reached(struct sock *sk,\n\t\t\t\t      const struct rate_sample *rs)\n{\n\tstruct bbr *bbr = inet_csk_ca(sk);\n\tu32 bw_thresh;\n\n\tif (bbr_full_bw_reached(sk) || !bbr->round_start || rs->is_app_limited)\n\t\treturn;\n\n\tbw_thresh = (u64)bbr->full_bw * bbr_full_bw_thresh >> BBR_SCALE;\n\tif (bbr_max_bw(sk) >= bw_thresh) {\n\t\tbbr->full_bw = bbr_max_bw(sk);\n\t\tbbr->full_bw_cnt = 0;\n\t\treturn;\n\t}\n\t++bbr->full_bw_cnt;\n\tbbr->full_bw_reached = bbr->full_bw_cnt >= bbr_full_bw_cnt;\n}\n\n \nstatic void bbr_check_drain(struct sock *sk, const struct rate_sample *rs)\n{\n\tstruct bbr *bbr = inet_csk_ca(sk);\n\n\tif (bbr->mode == BBR_STARTUP && bbr_full_bw_reached(sk)) {\n\t\tbbr->mode = BBR_DRAIN;\t \n\t\ttcp_sk(sk)->snd_ssthresh =\n\t\t\t\tbbr_inflight(sk, bbr_max_bw(sk), BBR_UNIT);\n\t}\t \n\tif (bbr->mode == BBR_DRAIN &&\n\t    bbr_packets_in_net_at_edt(sk, tcp_packets_in_flight(tcp_sk(sk))) <=\n\t    bbr_inflight(sk, bbr_max_bw(sk), BBR_UNIT))\n\t\tbbr_reset_probe_bw_mode(sk);   \n}\n\nstatic void bbr_check_probe_rtt_done(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct bbr *bbr = inet_csk_ca(sk);\n\n\tif (!(bbr->probe_rtt_done_stamp &&\n\t      after(tcp_jiffies32, bbr->probe_rtt_done_stamp)))\n\t\treturn;\n\n\tbbr->min_rtt_stamp = tcp_jiffies32;   \n\ttcp_snd_cwnd_set(tp, max(tcp_snd_cwnd(tp), bbr->prior_cwnd));\n\tbbr_reset_mode(sk);\n}\n\n \nstatic void bbr_update_min_rtt(struct sock *sk, const struct rate_sample *rs)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct bbr *bbr = inet_csk_ca(sk);\n\tbool filter_expired;\n\n\t \n\tfilter_expired = after(tcp_jiffies32,\n\t\t\t       bbr->min_rtt_stamp + bbr_min_rtt_win_sec * HZ);\n\tif (rs->rtt_us >= 0 &&\n\t    (rs->rtt_us < bbr->min_rtt_us ||\n\t     (filter_expired && !rs->is_ack_delayed))) {\n\t\tbbr->min_rtt_us = rs->rtt_us;\n\t\tbbr->min_rtt_stamp = tcp_jiffies32;\n\t}\n\n\tif (bbr_probe_rtt_mode_ms > 0 && filter_expired &&\n\t    !bbr->idle_restart && bbr->mode != BBR_PROBE_RTT) {\n\t\tbbr->mode = BBR_PROBE_RTT;   \n\t\tbbr_save_cwnd(sk);   \n\t\tbbr->probe_rtt_done_stamp = 0;\n\t}\n\n\tif (bbr->mode == BBR_PROBE_RTT) {\n\t\t \n\t\ttp->app_limited =\n\t\t\t(tp->delivered + tcp_packets_in_flight(tp)) ? : 1;\n\t\t \n\t\tif (!bbr->probe_rtt_done_stamp &&\n\t\t    tcp_packets_in_flight(tp) <= bbr_cwnd_min_target) {\n\t\t\tbbr->probe_rtt_done_stamp = tcp_jiffies32 +\n\t\t\t\tmsecs_to_jiffies(bbr_probe_rtt_mode_ms);\n\t\t\tbbr->probe_rtt_round_done = 0;\n\t\t\tbbr->next_rtt_delivered = tp->delivered;\n\t\t} else if (bbr->probe_rtt_done_stamp) {\n\t\t\tif (bbr->round_start)\n\t\t\t\tbbr->probe_rtt_round_done = 1;\n\t\t\tif (bbr->probe_rtt_round_done)\n\t\t\t\tbbr_check_probe_rtt_done(sk);\n\t\t}\n\t}\n\t \n\tif (rs->delivered > 0)\n\t\tbbr->idle_restart = 0;\n}\n\nstatic void bbr_update_gains(struct sock *sk)\n{\n\tstruct bbr *bbr = inet_csk_ca(sk);\n\n\tswitch (bbr->mode) {\n\tcase BBR_STARTUP:\n\t\tbbr->pacing_gain = bbr_high_gain;\n\t\tbbr->cwnd_gain\t = bbr_high_gain;\n\t\tbreak;\n\tcase BBR_DRAIN:\n\t\tbbr->pacing_gain = bbr_drain_gain;\t \n\t\tbbr->cwnd_gain\t = bbr_high_gain;\t \n\t\tbreak;\n\tcase BBR_PROBE_BW:\n\t\tbbr->pacing_gain = (bbr->lt_use_bw ?\n\t\t\t\t    BBR_UNIT :\n\t\t\t\t    bbr_pacing_gain[bbr->cycle_idx]);\n\t\tbbr->cwnd_gain\t = bbr_cwnd_gain;\n\t\tbreak;\n\tcase BBR_PROBE_RTT:\n\t\tbbr->pacing_gain = BBR_UNIT;\n\t\tbbr->cwnd_gain\t = BBR_UNIT;\n\t\tbreak;\n\tdefault:\n\t\tWARN_ONCE(1, \"BBR bad mode: %u\\n\", bbr->mode);\n\t\tbreak;\n\t}\n}\n\nstatic void bbr_update_model(struct sock *sk, const struct rate_sample *rs)\n{\n\tbbr_update_bw(sk, rs);\n\tbbr_update_ack_aggregation(sk, rs);\n\tbbr_update_cycle_phase(sk, rs);\n\tbbr_check_full_bw_reached(sk, rs);\n\tbbr_check_drain(sk, rs);\n\tbbr_update_min_rtt(sk, rs);\n\tbbr_update_gains(sk);\n}\n\n__bpf_kfunc static void bbr_main(struct sock *sk, const struct rate_sample *rs)\n{\n\tstruct bbr *bbr = inet_csk_ca(sk);\n\tu32 bw;\n\n\tbbr_update_model(sk, rs);\n\n\tbw = bbr_bw(sk);\n\tbbr_set_pacing_rate(sk, bw, bbr->pacing_gain);\n\tbbr_set_cwnd(sk, rs, rs->acked_sacked, bw, bbr->cwnd_gain);\n}\n\n__bpf_kfunc static void bbr_init(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct bbr *bbr = inet_csk_ca(sk);\n\n\tbbr->prior_cwnd = 0;\n\ttp->snd_ssthresh = TCP_INFINITE_SSTHRESH;\n\tbbr->rtt_cnt = 0;\n\tbbr->next_rtt_delivered = tp->delivered;\n\tbbr->prev_ca_state = TCP_CA_Open;\n\tbbr->packet_conservation = 0;\n\n\tbbr->probe_rtt_done_stamp = 0;\n\tbbr->probe_rtt_round_done = 0;\n\tbbr->min_rtt_us = tcp_min_rtt(tp);\n\tbbr->min_rtt_stamp = tcp_jiffies32;\n\n\tminmax_reset(&bbr->bw, bbr->rtt_cnt, 0);   \n\n\tbbr->has_seen_rtt = 0;\n\tbbr_init_pacing_rate_from_rtt(sk);\n\n\tbbr->round_start = 0;\n\tbbr->idle_restart = 0;\n\tbbr->full_bw_reached = 0;\n\tbbr->full_bw = 0;\n\tbbr->full_bw_cnt = 0;\n\tbbr->cycle_mstamp = 0;\n\tbbr->cycle_idx = 0;\n\tbbr_reset_lt_bw_sampling(sk);\n\tbbr_reset_startup_mode(sk);\n\n\tbbr->ack_epoch_mstamp = tp->tcp_mstamp;\n\tbbr->ack_epoch_acked = 0;\n\tbbr->extra_acked_win_rtts = 0;\n\tbbr->extra_acked_win_idx = 0;\n\tbbr->extra_acked[0] = 0;\n\tbbr->extra_acked[1] = 0;\n\n\tcmpxchg(&sk->sk_pacing_status, SK_PACING_NONE, SK_PACING_NEEDED);\n}\n\n__bpf_kfunc static u32 bbr_sndbuf_expand(struct sock *sk)\n{\n\t \n\treturn 3;\n}\n\n \n__bpf_kfunc static u32 bbr_undo_cwnd(struct sock *sk)\n{\n\tstruct bbr *bbr = inet_csk_ca(sk);\n\n\tbbr->full_bw = 0;    \n\tbbr->full_bw_cnt = 0;\n\tbbr_reset_lt_bw_sampling(sk);\n\treturn tcp_snd_cwnd(tcp_sk(sk));\n}\n\n \n__bpf_kfunc static u32 bbr_ssthresh(struct sock *sk)\n{\n\tbbr_save_cwnd(sk);\n\treturn tcp_sk(sk)->snd_ssthresh;\n}\n\nstatic size_t bbr_get_info(struct sock *sk, u32 ext, int *attr,\n\t\t\t   union tcp_cc_info *info)\n{\n\tif (ext & (1 << (INET_DIAG_BBRINFO - 1)) ||\n\t    ext & (1 << (INET_DIAG_VEGASINFO - 1))) {\n\t\tstruct tcp_sock *tp = tcp_sk(sk);\n\t\tstruct bbr *bbr = inet_csk_ca(sk);\n\t\tu64 bw = bbr_bw(sk);\n\n\t\tbw = bw * tp->mss_cache * USEC_PER_SEC >> BW_SCALE;\n\t\tmemset(&info->bbr, 0, sizeof(info->bbr));\n\t\tinfo->bbr.bbr_bw_lo\t\t= (u32)bw;\n\t\tinfo->bbr.bbr_bw_hi\t\t= (u32)(bw >> 32);\n\t\tinfo->bbr.bbr_min_rtt\t\t= bbr->min_rtt_us;\n\t\tinfo->bbr.bbr_pacing_gain\t= bbr->pacing_gain;\n\t\tinfo->bbr.bbr_cwnd_gain\t\t= bbr->cwnd_gain;\n\t\t*attr = INET_DIAG_BBRINFO;\n\t\treturn sizeof(info->bbr);\n\t}\n\treturn 0;\n}\n\n__bpf_kfunc static void bbr_set_state(struct sock *sk, u8 new_state)\n{\n\tstruct bbr *bbr = inet_csk_ca(sk);\n\n\tif (new_state == TCP_CA_Loss) {\n\t\tstruct rate_sample rs = { .losses = 1 };\n\n\t\tbbr->prev_ca_state = TCP_CA_Loss;\n\t\tbbr->full_bw = 0;\n\t\tbbr->round_start = 1;\t \n\t\tbbr_lt_bw_sampling(sk, &rs);\n\t}\n}\n\nstatic struct tcp_congestion_ops tcp_bbr_cong_ops __read_mostly = {\n\t.flags\t\t= TCP_CONG_NON_RESTRICTED,\n\t.name\t\t= \"bbr\",\n\t.owner\t\t= THIS_MODULE,\n\t.init\t\t= bbr_init,\n\t.cong_control\t= bbr_main,\n\t.sndbuf_expand\t= bbr_sndbuf_expand,\n\t.undo_cwnd\t= bbr_undo_cwnd,\n\t.cwnd_event\t= bbr_cwnd_event,\n\t.ssthresh\t= bbr_ssthresh,\n\t.min_tso_segs\t= bbr_min_tso_segs,\n\t.get_info\t= bbr_get_info,\n\t.set_state\t= bbr_set_state,\n};\n\nBTF_SET8_START(tcp_bbr_check_kfunc_ids)\n#ifdef CONFIG_X86\n#ifdef CONFIG_DYNAMIC_FTRACE\nBTF_ID_FLAGS(func, bbr_init)\nBTF_ID_FLAGS(func, bbr_main)\nBTF_ID_FLAGS(func, bbr_sndbuf_expand)\nBTF_ID_FLAGS(func, bbr_undo_cwnd)\nBTF_ID_FLAGS(func, bbr_cwnd_event)\nBTF_ID_FLAGS(func, bbr_ssthresh)\nBTF_ID_FLAGS(func, bbr_min_tso_segs)\nBTF_ID_FLAGS(func, bbr_set_state)\n#endif\n#endif\nBTF_SET8_END(tcp_bbr_check_kfunc_ids)\n\nstatic const struct btf_kfunc_id_set tcp_bbr_kfunc_set = {\n\t.owner = THIS_MODULE,\n\t.set   = &tcp_bbr_check_kfunc_ids,\n};\n\nstatic int __init bbr_register(void)\n{\n\tint ret;\n\n\tBUILD_BUG_ON(sizeof(struct bbr) > ICSK_CA_PRIV_SIZE);\n\n\tret = register_btf_kfunc_id_set(BPF_PROG_TYPE_STRUCT_OPS, &tcp_bbr_kfunc_set);\n\tif (ret < 0)\n\t\treturn ret;\n\treturn tcp_register_congestion_control(&tcp_bbr_cong_ops);\n}\n\nstatic void __exit bbr_unregister(void)\n{\n\ttcp_unregister_congestion_control(&tcp_bbr_cong_ops);\n}\n\nmodule_init(bbr_register);\nmodule_exit(bbr_unregister);\n\nMODULE_AUTHOR(\"Van Jacobson <vanj@google.com>\");\nMODULE_AUTHOR(\"Neal Cardwell <ncardwell@google.com>\");\nMODULE_AUTHOR(\"Yuchung Cheng <ycheng@google.com>\");\nMODULE_AUTHOR(\"Soheil Hassas Yeganeh <soheil@google.com>\");\nMODULE_LICENSE(\"Dual BSD/GPL\");\nMODULE_DESCRIPTION(\"TCP BBR (Bottleneck Bandwidth and RTT)\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}