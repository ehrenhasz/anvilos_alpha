{
  "module_name": "tcp_recovery.c",
  "hash_id": "da1a7aea4fde705382a4ae6f5a29ef13db50e6d78bb81801d65d1db44e05af4a",
  "original_prompt": "Ingested from linux-6.6.14/net/ipv4/tcp_recovery.c",
  "human_readable_source": "\n#include <linux/tcp.h>\n#include <net/tcp.h>\n\nstatic u32 tcp_rack_reo_wnd(const struct sock *sk)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\n\tif (!tp->reord_seen) {\n\t\t \n\t\tif (inet_csk(sk)->icsk_ca_state >= TCP_CA_Recovery)\n\t\t\treturn 0;\n\n\t\tif (tp->sacked_out >= tp->reordering &&\n\t\t    !(READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_recovery) &\n\t\t      TCP_RACK_NO_DUPTHRESH))\n\t\t\treturn 0;\n\t}\n\n\t \n\treturn min((tcp_min_rtt(tp) >> 2) * tp->rack.reo_wnd_steps,\n\t\t   tp->srtt_us >> 3);\n}\n\ns32 tcp_rack_skb_timeout(struct tcp_sock *tp, struct sk_buff *skb, u32 reo_wnd)\n{\n\treturn tp->rack.rtt_us + reo_wnd -\n\t       tcp_stamp_us_delta(tp->tcp_mstamp, tcp_skb_timestamp_us(skb));\n}\n\n \nstatic void tcp_rack_detect_loss(struct sock *sk, u32 *reo_timeout)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb, *n;\n\tu32 reo_wnd;\n\n\t*reo_timeout = 0;\n\treo_wnd = tcp_rack_reo_wnd(sk);\n\tlist_for_each_entry_safe(skb, n, &tp->tsorted_sent_queue,\n\t\t\t\t tcp_tsorted_anchor) {\n\t\tstruct tcp_skb_cb *scb = TCP_SKB_CB(skb);\n\t\ts32 remaining;\n\n\t\t \n\t\tif ((scb->sacked & TCPCB_LOST) &&\n\t\t    !(scb->sacked & TCPCB_SACKED_RETRANS))\n\t\t\tcontinue;\n\n\t\tif (!tcp_skb_sent_after(tp->rack.mstamp,\n\t\t\t\t\ttcp_skb_timestamp_us(skb),\n\t\t\t\t\ttp->rack.end_seq, scb->end_seq))\n\t\t\tbreak;\n\n\t\t \n\t\tremaining = tcp_rack_skb_timeout(tp, skb, reo_wnd);\n\t\tif (remaining <= 0) {\n\t\t\ttcp_mark_skb_lost(sk, skb);\n\t\t\tlist_del_init(&skb->tcp_tsorted_anchor);\n\t\t} else {\n\t\t\t \n\t\t\t*reo_timeout = max_t(u32, *reo_timeout, remaining);\n\t\t}\n\t}\n}\n\nbool tcp_rack_mark_lost(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 timeout;\n\n\tif (!tp->rack.advanced)\n\t\treturn false;\n\n\t \n\ttp->rack.advanced = 0;\n\ttcp_rack_detect_loss(sk, &timeout);\n\tif (timeout) {\n\t\ttimeout = usecs_to_jiffies(timeout + TCP_TIMEOUT_MIN_US);\n\t\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_REO_TIMEOUT,\n\t\t\t\t\t  timeout, inet_csk(sk)->icsk_rto);\n\t}\n\treturn !!timeout;\n}\n\n \nvoid tcp_rack_advance(struct tcp_sock *tp, u8 sacked, u32 end_seq,\n\t\t      u64 xmit_time)\n{\n\tu32 rtt_us;\n\n\trtt_us = tcp_stamp_us_delta(tp->tcp_mstamp, xmit_time);\n\tif (rtt_us < tcp_min_rtt(tp) && (sacked & TCPCB_RETRANS)) {\n\t\t \n\t\treturn;\n\t}\n\ttp->rack.advanced = 1;\n\ttp->rack.rtt_us = rtt_us;\n\tif (tcp_skb_sent_after(xmit_time, tp->rack.mstamp,\n\t\t\t       end_seq, tp->rack.end_seq)) {\n\t\ttp->rack.mstamp = xmit_time;\n\t\ttp->rack.end_seq = end_seq;\n\t}\n}\n\n \nvoid tcp_rack_reo_timeout(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 timeout, prior_inflight;\n\tu32 lost = tp->lost;\n\n\tprior_inflight = tcp_packets_in_flight(tp);\n\ttcp_rack_detect_loss(sk, &timeout);\n\tif (prior_inflight != tcp_packets_in_flight(tp)) {\n\t\tif (inet_csk(sk)->icsk_ca_state != TCP_CA_Recovery) {\n\t\t\ttcp_enter_recovery(sk, false);\n\t\t\tif (!inet_csk(sk)->icsk_ca_ops->cong_control)\n\t\t\t\ttcp_cwnd_reduction(sk, 1, tp->lost - lost, 0);\n\t\t}\n\t\ttcp_xmit_retransmit_queue(sk);\n\t}\n\tif (inet_csk(sk)->icsk_pending != ICSK_TIME_RETRANS)\n\t\ttcp_rearm_rto(sk);\n}\n\n \nvoid tcp_rack_update_reo_wnd(struct sock *sk, struct rate_sample *rs)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif ((READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_recovery) &\n\t     TCP_RACK_STATIC_REO_WND) ||\n\t    !rs->prior_delivered)\n\t\treturn;\n\n\t \n\tif (before(rs->prior_delivered, tp->rack.last_delivered))\n\t\ttp->rack.dsack_seen = 0;\n\n\t \n\tif (tp->rack.dsack_seen) {\n\t\ttp->rack.reo_wnd_steps = min_t(u32, 0xFF,\n\t\t\t\t\t       tp->rack.reo_wnd_steps + 1);\n\t\ttp->rack.dsack_seen = 0;\n\t\ttp->rack.last_delivered = tp->delivered;\n\t\ttp->rack.reo_wnd_persist = TCP_RACK_RECOVERY_THRESH;\n\t} else if (!tp->rack.reo_wnd_persist) {\n\t\ttp->rack.reo_wnd_steps = 1;\n\t}\n}\n\n \nvoid tcp_newreno_mark_lost(struct sock *sk, bool snd_una_advanced)\n{\n\tconst u8 state = inet_csk(sk)->icsk_ca_state;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif ((state < TCP_CA_Recovery && tp->sacked_out >= tp->reordering) ||\n\t    (state == TCP_CA_Recovery && snd_una_advanced)) {\n\t\tstruct sk_buff *skb = tcp_rtx_queue_head(sk);\n\t\tu32 mss;\n\n\t\tif (TCP_SKB_CB(skb)->sacked & TCPCB_LOST)\n\t\t\treturn;\n\n\t\tmss = tcp_skb_mss(skb);\n\t\tif (tcp_skb_pcount(skb) > 1 && skb->len > mss)\n\t\t\ttcp_fragment(sk, TCP_FRAG_IN_RTX_QUEUE, skb,\n\t\t\t\t     mss, mss, GFP_ATOMIC);\n\n\t\ttcp_mark_skb_lost(sk, skb);\n\t}\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}