{
  "module_name": "tcp_output.c",
  "hash_id": "b1ced567e4cf38e6fd7ead387f04e483b49de33b22501bc0864ca40d33a883e4",
  "original_prompt": "Ingested from linux-6.6.14/net/ipv4/tcp_output.c",
  "human_readable_source": "\n \n\n \n\n#define pr_fmt(fmt) \"TCP: \" fmt\n\n#include <net/tcp.h>\n#include <net/mptcp.h>\n\n#include <linux/compiler.h>\n#include <linux/gfp.h>\n#include <linux/module.h>\n#include <linux/static_key.h>\n\n#include <trace/events/tcp.h>\n\n \nvoid tcp_mstamp_refresh(struct tcp_sock *tp)\n{\n\tu64 val = tcp_clock_ns();\n\n\ttp->tcp_clock_cache = val;\n\ttp->tcp_mstamp = div_u64(val, NSEC_PER_USEC);\n}\n\nstatic bool tcp_write_xmit(struct sock *sk, unsigned int mss_now, int nonagle,\n\t\t\t   int push_one, gfp_t gfp);\n\n \nstatic void tcp_event_new_data_sent(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tunsigned int prior_packets = tp->packets_out;\n\n\tWRITE_ONCE(tp->snd_nxt, TCP_SKB_CB(skb)->end_seq);\n\n\t__skb_unlink(skb, &sk->sk_write_queue);\n\ttcp_rbtree_insert(&sk->tcp_rtx_queue, skb);\n\n\tif (tp->highest_sack == NULL)\n\t\ttp->highest_sack = skb;\n\n\ttp->packets_out += tcp_skb_pcount(skb);\n\tif (!prior_packets || icsk->icsk_pending == ICSK_TIME_LOSS_PROBE)\n\t\ttcp_rearm_rto(sk);\n\n\tNET_ADD_STATS(sock_net(sk), LINUX_MIB_TCPORIGDATASENT,\n\t\t      tcp_skb_pcount(skb));\n\ttcp_check_space(sk);\n}\n\n \nstatic inline __u32 tcp_acceptable_seq(const struct sock *sk)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\n\tif (!before(tcp_wnd_end(tp), tp->snd_nxt) ||\n\t    (tp->rx_opt.wscale_ok &&\n\t     ((tp->snd_nxt - tcp_wnd_end(tp)) < (1 << tp->rx_opt.rcv_wscale))))\n\t\treturn tp->snd_nxt;\n\telse\n\t\treturn tcp_wnd_end(tp);\n}\n\n \nstatic __u16 tcp_advertise_mss(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tconst struct dst_entry *dst = __sk_dst_get(sk);\n\tint mss = tp->advmss;\n\n\tif (dst) {\n\t\tunsigned int metric = dst_metric_advmss(dst);\n\n\t\tif (metric < mss) {\n\t\t\tmss = metric;\n\t\t\ttp->advmss = mss;\n\t\t}\n\t}\n\n\treturn (__u16)mss;\n}\n\n \nvoid tcp_cwnd_restart(struct sock *sk, s32 delta)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 restart_cwnd = tcp_init_cwnd(tp, __sk_dst_get(sk));\n\tu32 cwnd = tcp_snd_cwnd(tp);\n\n\ttcp_ca_event(sk, CA_EVENT_CWND_RESTART);\n\n\ttp->snd_ssthresh = tcp_current_ssthresh(sk);\n\trestart_cwnd = min(restart_cwnd, cwnd);\n\n\twhile ((delta -= inet_csk(sk)->icsk_rto) > 0 && cwnd > restart_cwnd)\n\t\tcwnd >>= 1;\n\ttcp_snd_cwnd_set(tp, max(cwnd, restart_cwnd));\n\ttp->snd_cwnd_stamp = tcp_jiffies32;\n\ttp->snd_cwnd_used = 0;\n}\n\n \nstatic void tcp_event_data_sent(struct tcp_sock *tp,\n\t\t\t\tstruct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tconst u32 now = tcp_jiffies32;\n\n\tif (tcp_packets_in_flight(tp) == 0)\n\t\ttcp_ca_event(sk, CA_EVENT_TX_START);\n\n\ttp->lsndtime = now;\n\n\t \n\tif ((u32)(now - icsk->icsk_ack.lrcvtime) < icsk->icsk_ack.ato)\n\t\tinet_csk_enter_pingpong_mode(sk);\n}\n\n \nstatic inline void tcp_event_ack_sent(struct sock *sk, u32 rcv_nxt)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (unlikely(tp->compressed_ack)) {\n\t\tNET_ADD_STATS(sock_net(sk), LINUX_MIB_TCPACKCOMPRESSED,\n\t\t\t      tp->compressed_ack);\n\t\ttp->compressed_ack = 0;\n\t\tif (hrtimer_try_to_cancel(&tp->compressed_ack_timer) == 1)\n\t\t\t__sock_put(sk);\n\t}\n\n\tif (unlikely(rcv_nxt != tp->rcv_nxt))\n\t\treturn;   \n\ttcp_dec_quickack_mode(sk);\n\tinet_csk_clear_xmit_timer(sk, ICSK_TIME_DACK);\n}\n\n \nvoid tcp_select_initial_window(const struct sock *sk, int __space, __u32 mss,\n\t\t\t       __u32 *rcv_wnd, __u32 *window_clamp,\n\t\t\t       int wscale_ok, __u8 *rcv_wscale,\n\t\t\t       __u32 init_rcv_wnd)\n{\n\tunsigned int space = (__space < 0 ? 0 : __space);\n\n\t \n\tif (*window_clamp == 0)\n\t\t(*window_clamp) = (U16_MAX << TCP_MAX_WSCALE);\n\tspace = min(*window_clamp, space);\n\n\t \n\tif (space > mss)\n\t\tspace = rounddown(space, mss);\n\n\t \n\tif (READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_workaround_signed_windows))\n\t\t(*rcv_wnd) = min(space, MAX_TCP_WINDOW);\n\telse\n\t\t(*rcv_wnd) = min_t(u32, space, U16_MAX);\n\n\tif (init_rcv_wnd)\n\t\t*rcv_wnd = min(*rcv_wnd, init_rcv_wnd * mss);\n\n\t*rcv_wscale = 0;\n\tif (wscale_ok) {\n\t\t \n\t\tspace = max_t(u32, space, READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_rmem[2]));\n\t\tspace = max_t(u32, space, READ_ONCE(sysctl_rmem_max));\n\t\tspace = min_t(u32, space, *window_clamp);\n\t\t*rcv_wscale = clamp_t(int, ilog2(space) - 15,\n\t\t\t\t      0, TCP_MAX_WSCALE);\n\t}\n\t \n\t(*window_clamp) = min_t(__u32, U16_MAX << (*rcv_wscale), *window_clamp);\n}\nEXPORT_SYMBOL(tcp_select_initial_window);\n\n \nstatic u16 tcp_select_window(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tu32 old_win = tp->rcv_wnd;\n\tu32 cur_win, new_win;\n\n\t \n\tif (unlikely(inet_csk(sk)->icsk_ack.pending & ICSK_ACK_NOMEM))\n\t\treturn 0;\n\n\tcur_win = tcp_receive_window(tp);\n\tnew_win = __tcp_select_window(sk);\n\tif (new_win < cur_win) {\n\t\t \n\t\tif (!READ_ONCE(net->ipv4.sysctl_tcp_shrink_window) || !tp->rx_opt.rcv_wscale) {\n\t\t\t \n\t\t\tif (new_win == 0)\n\t\t\t\tNET_INC_STATS(net, LINUX_MIB_TCPWANTZEROWINDOWADV);\n\t\t\tnew_win = ALIGN(cur_win, 1 << tp->rx_opt.rcv_wscale);\n\t\t}\n\t}\n\n\ttp->rcv_wnd = new_win;\n\ttp->rcv_wup = tp->rcv_nxt;\n\n\t \n\tif (!tp->rx_opt.rcv_wscale &&\n\t    READ_ONCE(net->ipv4.sysctl_tcp_workaround_signed_windows))\n\t\tnew_win = min(new_win, MAX_TCP_WINDOW);\n\telse\n\t\tnew_win = min(new_win, (65535U << tp->rx_opt.rcv_wscale));\n\n\t \n\tnew_win >>= tp->rx_opt.rcv_wscale;\n\n\t \n\tif (new_win == 0) {\n\t\ttp->pred_flags = 0;\n\t\tif (old_win)\n\t\t\tNET_INC_STATS(net, LINUX_MIB_TCPTOZEROWINDOWADV);\n\t} else if (old_win == 0) {\n\t\tNET_INC_STATS(net, LINUX_MIB_TCPFROMZEROWINDOWADV);\n\t}\n\n\treturn new_win;\n}\n\n \nstatic void tcp_ecn_send_synack(struct sock *sk, struct sk_buff *skb)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\n\tTCP_SKB_CB(skb)->tcp_flags &= ~TCPHDR_CWR;\n\tif (!(tp->ecn_flags & TCP_ECN_OK))\n\t\tTCP_SKB_CB(skb)->tcp_flags &= ~TCPHDR_ECE;\n\telse if (tcp_ca_needs_ecn(sk) ||\n\t\t tcp_bpf_ca_needs_ecn(sk))\n\t\tINET_ECN_xmit(sk);\n}\n\n \nstatic void tcp_ecn_send_syn(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tbool bpf_needs_ecn = tcp_bpf_ca_needs_ecn(sk);\n\tbool use_ecn = READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_ecn) == 1 ||\n\t\ttcp_ca_needs_ecn(sk) || bpf_needs_ecn;\n\n\tif (!use_ecn) {\n\t\tconst struct dst_entry *dst = __sk_dst_get(sk);\n\n\t\tif (dst && dst_feature(dst, RTAX_FEATURE_ECN))\n\t\t\tuse_ecn = true;\n\t}\n\n\ttp->ecn_flags = 0;\n\n\tif (use_ecn) {\n\t\tTCP_SKB_CB(skb)->tcp_flags |= TCPHDR_ECE | TCPHDR_CWR;\n\t\ttp->ecn_flags = TCP_ECN_OK;\n\t\tif (tcp_ca_needs_ecn(sk) || bpf_needs_ecn)\n\t\t\tINET_ECN_xmit(sk);\n\t}\n}\n\nstatic void tcp_ecn_clear_syn(struct sock *sk, struct sk_buff *skb)\n{\n\tif (READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_ecn_fallback))\n\t\t \n\t\tTCP_SKB_CB(skb)->tcp_flags &= ~(TCPHDR_ECE | TCPHDR_CWR);\n}\n\nstatic void\ntcp_ecn_make_synack(const struct request_sock *req, struct tcphdr *th)\n{\n\tif (inet_rsk(req)->ecn_ok)\n\t\tth->ece = 1;\n}\n\n \nstatic void tcp_ecn_send(struct sock *sk, struct sk_buff *skb,\n\t\t\t struct tcphdr *th, int tcp_header_len)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (tp->ecn_flags & TCP_ECN_OK) {\n\t\t \n\t\tif (skb->len != tcp_header_len &&\n\t\t    !before(TCP_SKB_CB(skb)->seq, tp->snd_nxt)) {\n\t\t\tINET_ECN_xmit(sk);\n\t\t\tif (tp->ecn_flags & TCP_ECN_QUEUE_CWR) {\n\t\t\t\ttp->ecn_flags &= ~TCP_ECN_QUEUE_CWR;\n\t\t\t\tth->cwr = 1;\n\t\t\t\tskb_shinfo(skb)->gso_type |= SKB_GSO_TCP_ECN;\n\t\t\t}\n\t\t} else if (!tcp_ca_needs_ecn(sk)) {\n\t\t\t \n\t\t\tINET_ECN_dontxmit(sk);\n\t\t}\n\t\tif (tp->ecn_flags & TCP_ECN_DEMAND_CWR)\n\t\t\tth->ece = 1;\n\t}\n}\n\n \nstatic void tcp_init_nondata_skb(struct sk_buff *skb, u32 seq, u8 flags)\n{\n\tskb->ip_summed = CHECKSUM_PARTIAL;\n\n\tTCP_SKB_CB(skb)->tcp_flags = flags;\n\n\ttcp_skb_pcount_set(skb, 1);\n\n\tTCP_SKB_CB(skb)->seq = seq;\n\tif (flags & (TCPHDR_SYN | TCPHDR_FIN))\n\t\tseq++;\n\tTCP_SKB_CB(skb)->end_seq = seq;\n}\n\nstatic inline bool tcp_urg_mode(const struct tcp_sock *tp)\n{\n\treturn tp->snd_una != tp->snd_up;\n}\n\n#define OPTION_SACK_ADVERTISE\tBIT(0)\n#define OPTION_TS\t\tBIT(1)\n#define OPTION_MD5\t\tBIT(2)\n#define OPTION_WSCALE\t\tBIT(3)\n#define OPTION_FAST_OPEN_COOKIE\tBIT(8)\n#define OPTION_SMC\t\tBIT(9)\n#define OPTION_MPTCP\t\tBIT(10)\n\nstatic void smc_options_write(__be32 *ptr, u16 *options)\n{\n#if IS_ENABLED(CONFIG_SMC)\n\tif (static_branch_unlikely(&tcp_have_smc)) {\n\t\tif (unlikely(OPTION_SMC & *options)) {\n\t\t\t*ptr++ = htonl((TCPOPT_NOP  << 24) |\n\t\t\t\t       (TCPOPT_NOP  << 16) |\n\t\t\t\t       (TCPOPT_EXP <<  8) |\n\t\t\t\t       (TCPOLEN_EXP_SMC_BASE));\n\t\t\t*ptr++ = htonl(TCPOPT_SMC_MAGIC);\n\t\t}\n\t}\n#endif\n}\n\nstruct tcp_out_options {\n\tu16 options;\t\t \n\tu16 mss;\t\t \n\tu8 ws;\t\t\t \n\tu8 num_sack_blocks;\t \n\tu8 hash_size;\t\t \n\tu8 bpf_opt_len;\t\t \n\t__u8 *hash_location;\t \n\t__u32 tsval, tsecr;\t \n\tstruct tcp_fastopen_cookie *fastopen_cookie;\t \n\tstruct mptcp_out_options mptcp;\n};\n\nstatic void mptcp_options_write(struct tcphdr *th, __be32 *ptr,\n\t\t\t\tstruct tcp_sock *tp,\n\t\t\t\tstruct tcp_out_options *opts)\n{\n#if IS_ENABLED(CONFIG_MPTCP)\n\tif (unlikely(OPTION_MPTCP & opts->options))\n\t\tmptcp_write_options(th, ptr, tp, &opts->mptcp);\n#endif\n}\n\n#ifdef CONFIG_CGROUP_BPF\nstatic int bpf_skops_write_hdr_opt_arg0(struct sk_buff *skb,\n\t\t\t\t\tenum tcp_synack_type synack_type)\n{\n\tif (unlikely(!skb))\n\t\treturn BPF_WRITE_HDR_TCP_CURRENT_MSS;\n\n\tif (unlikely(synack_type == TCP_SYNACK_COOKIE))\n\t\treturn BPF_WRITE_HDR_TCP_SYNACK_COOKIE;\n\n\treturn 0;\n}\n\n \nstatic void bpf_skops_hdr_opt_len(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t  struct request_sock *req,\n\t\t\t\t  struct sk_buff *syn_skb,\n\t\t\t\t  enum tcp_synack_type synack_type,\n\t\t\t\t  struct tcp_out_options *opts,\n\t\t\t\t  unsigned int *remaining)\n{\n\tstruct bpf_sock_ops_kern sock_ops;\n\tint err;\n\n\tif (likely(!BPF_SOCK_OPS_TEST_FLAG(tcp_sk(sk),\n\t\t\t\t\t   BPF_SOCK_OPS_WRITE_HDR_OPT_CB_FLAG)) ||\n\t    !*remaining)\n\t\treturn;\n\n\t \n\n\t \n\tmemset(&sock_ops, 0, offsetof(struct bpf_sock_ops_kern, temp));\n\n\tsock_ops.op = BPF_SOCK_OPS_HDR_OPT_LEN_CB;\n\n\tif (req) {\n\t\t \n\t\tsock_ops.sk = (struct sock *)req;\n\t\tsock_ops.syn_skb = syn_skb;\n\t} else {\n\t\tsock_owned_by_me(sk);\n\n\t\tsock_ops.is_fullsock = 1;\n\t\tsock_ops.sk = sk;\n\t}\n\n\tsock_ops.args[0] = bpf_skops_write_hdr_opt_arg0(skb, synack_type);\n\tsock_ops.remaining_opt_len = *remaining;\n\t \n\tif (skb)\n\t\tbpf_skops_init_skb(&sock_ops, skb, 0);\n\n\terr = BPF_CGROUP_RUN_PROG_SOCK_OPS_SK(&sock_ops, sk);\n\n\tif (err || sock_ops.remaining_opt_len == *remaining)\n\t\treturn;\n\n\topts->bpf_opt_len = *remaining - sock_ops.remaining_opt_len;\n\t \n\topts->bpf_opt_len = (opts->bpf_opt_len + 3) & ~3;\n\n\t*remaining -= opts->bpf_opt_len;\n}\n\nstatic void bpf_skops_write_hdr_opt(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t    struct request_sock *req,\n\t\t\t\t    struct sk_buff *syn_skb,\n\t\t\t\t    enum tcp_synack_type synack_type,\n\t\t\t\t    struct tcp_out_options *opts)\n{\n\tu8 first_opt_off, nr_written, max_opt_len = opts->bpf_opt_len;\n\tstruct bpf_sock_ops_kern sock_ops;\n\tint err;\n\n\tif (likely(!max_opt_len))\n\t\treturn;\n\n\tmemset(&sock_ops, 0, offsetof(struct bpf_sock_ops_kern, temp));\n\n\tsock_ops.op = BPF_SOCK_OPS_WRITE_HDR_OPT_CB;\n\n\tif (req) {\n\t\tsock_ops.sk = (struct sock *)req;\n\t\tsock_ops.syn_skb = syn_skb;\n\t} else {\n\t\tsock_owned_by_me(sk);\n\n\t\tsock_ops.is_fullsock = 1;\n\t\tsock_ops.sk = sk;\n\t}\n\n\tsock_ops.args[0] = bpf_skops_write_hdr_opt_arg0(skb, synack_type);\n\tsock_ops.remaining_opt_len = max_opt_len;\n\tfirst_opt_off = tcp_hdrlen(skb) - max_opt_len;\n\tbpf_skops_init_skb(&sock_ops, skb, first_opt_off);\n\n\terr = BPF_CGROUP_RUN_PROG_SOCK_OPS_SK(&sock_ops, sk);\n\n\tif (err)\n\t\tnr_written = 0;\n\telse\n\t\tnr_written = max_opt_len - sock_ops.remaining_opt_len;\n\n\tif (nr_written < max_opt_len)\n\t\tmemset(skb->data + first_opt_off + nr_written, TCPOPT_NOP,\n\t\t       max_opt_len - nr_written);\n}\n#else\nstatic void bpf_skops_hdr_opt_len(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t  struct request_sock *req,\n\t\t\t\t  struct sk_buff *syn_skb,\n\t\t\t\t  enum tcp_synack_type synack_type,\n\t\t\t\t  struct tcp_out_options *opts,\n\t\t\t\t  unsigned int *remaining)\n{\n}\n\nstatic void bpf_skops_write_hdr_opt(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t    struct request_sock *req,\n\t\t\t\t    struct sk_buff *syn_skb,\n\t\t\t\t    enum tcp_synack_type synack_type,\n\t\t\t\t    struct tcp_out_options *opts)\n{\n}\n#endif\n\n \nstatic void tcp_options_write(struct tcphdr *th, struct tcp_sock *tp,\n\t\t\t      struct tcp_out_options *opts)\n{\n\t__be32 *ptr = (__be32 *)(th + 1);\n\tu16 options = opts->options;\t \n\n\tif (unlikely(OPTION_MD5 & options)) {\n\t\t*ptr++ = htonl((TCPOPT_NOP << 24) | (TCPOPT_NOP << 16) |\n\t\t\t       (TCPOPT_MD5SIG << 8) | TCPOLEN_MD5SIG);\n\t\t \n\t\topts->hash_location = (__u8 *)ptr;\n\t\tptr += 4;\n\t}\n\n\tif (unlikely(opts->mss)) {\n\t\t*ptr++ = htonl((TCPOPT_MSS << 24) |\n\t\t\t       (TCPOLEN_MSS << 16) |\n\t\t\t       opts->mss);\n\t}\n\n\tif (likely(OPTION_TS & options)) {\n\t\tif (unlikely(OPTION_SACK_ADVERTISE & options)) {\n\t\t\t*ptr++ = htonl((TCPOPT_SACK_PERM << 24) |\n\t\t\t\t       (TCPOLEN_SACK_PERM << 16) |\n\t\t\t\t       (TCPOPT_TIMESTAMP << 8) |\n\t\t\t\t       TCPOLEN_TIMESTAMP);\n\t\t\toptions &= ~OPTION_SACK_ADVERTISE;\n\t\t} else {\n\t\t\t*ptr++ = htonl((TCPOPT_NOP << 24) |\n\t\t\t\t       (TCPOPT_NOP << 16) |\n\t\t\t\t       (TCPOPT_TIMESTAMP << 8) |\n\t\t\t\t       TCPOLEN_TIMESTAMP);\n\t\t}\n\t\t*ptr++ = htonl(opts->tsval);\n\t\t*ptr++ = htonl(opts->tsecr);\n\t}\n\n\tif (unlikely(OPTION_SACK_ADVERTISE & options)) {\n\t\t*ptr++ = htonl((TCPOPT_NOP << 24) |\n\t\t\t       (TCPOPT_NOP << 16) |\n\t\t\t       (TCPOPT_SACK_PERM << 8) |\n\t\t\t       TCPOLEN_SACK_PERM);\n\t}\n\n\tif (unlikely(OPTION_WSCALE & options)) {\n\t\t*ptr++ = htonl((TCPOPT_NOP << 24) |\n\t\t\t       (TCPOPT_WINDOW << 16) |\n\t\t\t       (TCPOLEN_WINDOW << 8) |\n\t\t\t       opts->ws);\n\t}\n\n\tif (unlikely(opts->num_sack_blocks)) {\n\t\tstruct tcp_sack_block *sp = tp->rx_opt.dsack ?\n\t\t\ttp->duplicate_sack : tp->selective_acks;\n\t\tint this_sack;\n\n\t\t*ptr++ = htonl((TCPOPT_NOP  << 24) |\n\t\t\t       (TCPOPT_NOP  << 16) |\n\t\t\t       (TCPOPT_SACK <<  8) |\n\t\t\t       (TCPOLEN_SACK_BASE + (opts->num_sack_blocks *\n\t\t\t\t\t\t     TCPOLEN_SACK_PERBLOCK)));\n\n\t\tfor (this_sack = 0; this_sack < opts->num_sack_blocks;\n\t\t     ++this_sack) {\n\t\t\t*ptr++ = htonl(sp[this_sack].start_seq);\n\t\t\t*ptr++ = htonl(sp[this_sack].end_seq);\n\t\t}\n\n\t\ttp->rx_opt.dsack = 0;\n\t}\n\n\tif (unlikely(OPTION_FAST_OPEN_COOKIE & options)) {\n\t\tstruct tcp_fastopen_cookie *foc = opts->fastopen_cookie;\n\t\tu8 *p = (u8 *)ptr;\n\t\tu32 len;  \n\n\t\tif (foc->exp) {\n\t\t\tlen = TCPOLEN_EXP_FASTOPEN_BASE + foc->len;\n\t\t\t*ptr = htonl((TCPOPT_EXP << 24) | (len << 16) |\n\t\t\t\t     TCPOPT_FASTOPEN_MAGIC);\n\t\t\tp += TCPOLEN_EXP_FASTOPEN_BASE;\n\t\t} else {\n\t\t\tlen = TCPOLEN_FASTOPEN_BASE + foc->len;\n\t\t\t*p++ = TCPOPT_FASTOPEN;\n\t\t\t*p++ = len;\n\t\t}\n\n\t\tmemcpy(p, foc->val, foc->len);\n\t\tif ((len & 3) == 2) {\n\t\t\tp[foc->len] = TCPOPT_NOP;\n\t\t\tp[foc->len + 1] = TCPOPT_NOP;\n\t\t}\n\t\tptr += (len + 3) >> 2;\n\t}\n\n\tsmc_options_write(ptr, &options);\n\n\tmptcp_options_write(th, ptr, tp, opts);\n}\n\nstatic void smc_set_option(const struct tcp_sock *tp,\n\t\t\t   struct tcp_out_options *opts,\n\t\t\t   unsigned int *remaining)\n{\n#if IS_ENABLED(CONFIG_SMC)\n\tif (static_branch_unlikely(&tcp_have_smc)) {\n\t\tif (tp->syn_smc) {\n\t\t\tif (*remaining >= TCPOLEN_EXP_SMC_BASE_ALIGNED) {\n\t\t\t\topts->options |= OPTION_SMC;\n\t\t\t\t*remaining -= TCPOLEN_EXP_SMC_BASE_ALIGNED;\n\t\t\t}\n\t\t}\n\t}\n#endif\n}\n\nstatic void smc_set_option_cond(const struct tcp_sock *tp,\n\t\t\t\tconst struct inet_request_sock *ireq,\n\t\t\t\tstruct tcp_out_options *opts,\n\t\t\t\tunsigned int *remaining)\n{\n#if IS_ENABLED(CONFIG_SMC)\n\tif (static_branch_unlikely(&tcp_have_smc)) {\n\t\tif (tp->syn_smc && ireq->smc_ok) {\n\t\t\tif (*remaining >= TCPOLEN_EXP_SMC_BASE_ALIGNED) {\n\t\t\t\topts->options |= OPTION_SMC;\n\t\t\t\t*remaining -= TCPOLEN_EXP_SMC_BASE_ALIGNED;\n\t\t\t}\n\t\t}\n\t}\n#endif\n}\n\nstatic void mptcp_set_option_cond(const struct request_sock *req,\n\t\t\t\t  struct tcp_out_options *opts,\n\t\t\t\t  unsigned int *remaining)\n{\n\tif (rsk_is_mptcp(req)) {\n\t\tunsigned int size;\n\n\t\tif (mptcp_synack_options(req, &size, &opts->mptcp)) {\n\t\t\tif (*remaining >= size) {\n\t\t\t\topts->options |= OPTION_MPTCP;\n\t\t\t\t*remaining -= size;\n\t\t\t}\n\t\t}\n\t}\n}\n\n \nstatic unsigned int tcp_syn_options(struct sock *sk, struct sk_buff *skb,\n\t\t\t\tstruct tcp_out_options *opts,\n\t\t\t\tstruct tcp_md5sig_key **md5)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tunsigned int remaining = MAX_TCP_OPTION_SPACE;\n\tstruct tcp_fastopen_request *fastopen = tp->fastopen_req;\n\n\t*md5 = NULL;\n#ifdef CONFIG_TCP_MD5SIG\n\tif (static_branch_unlikely(&tcp_md5_needed.key) &&\n\t    rcu_access_pointer(tp->md5sig_info)) {\n\t\t*md5 = tp->af_specific->md5_lookup(sk, sk);\n\t\tif (*md5) {\n\t\t\topts->options |= OPTION_MD5;\n\t\t\tremaining -= TCPOLEN_MD5SIG_ALIGNED;\n\t\t}\n\t}\n#endif\n\n\t \n\topts->mss = tcp_advertise_mss(sk);\n\tremaining -= TCPOLEN_MSS_ALIGNED;\n\n\tif (likely(READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_timestamps) && !*md5)) {\n\t\topts->options |= OPTION_TS;\n\t\topts->tsval = tcp_skb_timestamp(skb) + tp->tsoffset;\n\t\topts->tsecr = tp->rx_opt.ts_recent;\n\t\tremaining -= TCPOLEN_TSTAMP_ALIGNED;\n\t}\n\tif (likely(READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_window_scaling))) {\n\t\topts->ws = tp->rx_opt.rcv_wscale;\n\t\topts->options |= OPTION_WSCALE;\n\t\tremaining -= TCPOLEN_WSCALE_ALIGNED;\n\t}\n\tif (likely(READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_sack))) {\n\t\topts->options |= OPTION_SACK_ADVERTISE;\n\t\tif (unlikely(!(OPTION_TS & opts->options)))\n\t\t\tremaining -= TCPOLEN_SACKPERM_ALIGNED;\n\t}\n\n\tif (fastopen && fastopen->cookie.len >= 0) {\n\t\tu32 need = fastopen->cookie.len;\n\n\t\tneed += fastopen->cookie.exp ? TCPOLEN_EXP_FASTOPEN_BASE :\n\t\t\t\t\t       TCPOLEN_FASTOPEN_BASE;\n\t\tneed = (need + 3) & ~3U;   \n\t\tif (remaining >= need) {\n\t\t\topts->options |= OPTION_FAST_OPEN_COOKIE;\n\t\t\topts->fastopen_cookie = &fastopen->cookie;\n\t\t\tremaining -= need;\n\t\t\ttp->syn_fastopen = 1;\n\t\t\ttp->syn_fastopen_exp = fastopen->cookie.exp ? 1 : 0;\n\t\t}\n\t}\n\n\tsmc_set_option(tp, opts, &remaining);\n\n\tif (sk_is_mptcp(sk)) {\n\t\tunsigned int size;\n\n\t\tif (mptcp_syn_options(sk, skb, &size, &opts->mptcp)) {\n\t\t\topts->options |= OPTION_MPTCP;\n\t\t\tremaining -= size;\n\t\t}\n\t}\n\n\tbpf_skops_hdr_opt_len(sk, skb, NULL, NULL, 0, opts, &remaining);\n\n\treturn MAX_TCP_OPTION_SPACE - remaining;\n}\n\n \nstatic unsigned int tcp_synack_options(const struct sock *sk,\n\t\t\t\t       struct request_sock *req,\n\t\t\t\t       unsigned int mss, struct sk_buff *skb,\n\t\t\t\t       struct tcp_out_options *opts,\n\t\t\t\t       const struct tcp_md5sig_key *md5,\n\t\t\t\t       struct tcp_fastopen_cookie *foc,\n\t\t\t\t       enum tcp_synack_type synack_type,\n\t\t\t\t       struct sk_buff *syn_skb)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tunsigned int remaining = MAX_TCP_OPTION_SPACE;\n\n#ifdef CONFIG_TCP_MD5SIG\n\tif (md5) {\n\t\topts->options |= OPTION_MD5;\n\t\tremaining -= TCPOLEN_MD5SIG_ALIGNED;\n\n\t\t \n\t\tif (synack_type != TCP_SYNACK_COOKIE)\n\t\t\tireq->tstamp_ok &= !ireq->sack_ok;\n\t}\n#endif\n\n\t \n\topts->mss = mss;\n\tremaining -= TCPOLEN_MSS_ALIGNED;\n\n\tif (likely(ireq->wscale_ok)) {\n\t\topts->ws = ireq->rcv_wscale;\n\t\topts->options |= OPTION_WSCALE;\n\t\tremaining -= TCPOLEN_WSCALE_ALIGNED;\n\t}\n\tif (likely(ireq->tstamp_ok)) {\n\t\topts->options |= OPTION_TS;\n\t\topts->tsval = tcp_skb_timestamp(skb) + tcp_rsk(req)->ts_off;\n\t\topts->tsecr = READ_ONCE(req->ts_recent);\n\t\tremaining -= TCPOLEN_TSTAMP_ALIGNED;\n\t}\n\tif (likely(ireq->sack_ok)) {\n\t\topts->options |= OPTION_SACK_ADVERTISE;\n\t\tif (unlikely(!ireq->tstamp_ok))\n\t\t\tremaining -= TCPOLEN_SACKPERM_ALIGNED;\n\t}\n\tif (foc != NULL && foc->len >= 0) {\n\t\tu32 need = foc->len;\n\n\t\tneed += foc->exp ? TCPOLEN_EXP_FASTOPEN_BASE :\n\t\t\t\t   TCPOLEN_FASTOPEN_BASE;\n\t\tneed = (need + 3) & ~3U;   \n\t\tif (remaining >= need) {\n\t\t\topts->options |= OPTION_FAST_OPEN_COOKIE;\n\t\t\topts->fastopen_cookie = foc;\n\t\t\tremaining -= need;\n\t\t}\n\t}\n\n\tmptcp_set_option_cond(req, opts, &remaining);\n\n\tsmc_set_option_cond(tcp_sk(sk), ireq, opts, &remaining);\n\n\tbpf_skops_hdr_opt_len((struct sock *)sk, skb, req, syn_skb,\n\t\t\t      synack_type, opts, &remaining);\n\n\treturn MAX_TCP_OPTION_SPACE - remaining;\n}\n\n \nstatic unsigned int tcp_established_options(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t\tstruct tcp_out_options *opts,\n\t\t\t\t\tstruct tcp_md5sig_key **md5)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tunsigned int size = 0;\n\tunsigned int eff_sacks;\n\n\topts->options = 0;\n\n\t*md5 = NULL;\n#ifdef CONFIG_TCP_MD5SIG\n\tif (static_branch_unlikely(&tcp_md5_needed.key) &&\n\t    rcu_access_pointer(tp->md5sig_info)) {\n\t\t*md5 = tp->af_specific->md5_lookup(sk, sk);\n\t\tif (*md5) {\n\t\t\topts->options |= OPTION_MD5;\n\t\t\tsize += TCPOLEN_MD5SIG_ALIGNED;\n\t\t}\n\t}\n#endif\n\n\tif (likely(tp->rx_opt.tstamp_ok)) {\n\t\topts->options |= OPTION_TS;\n\t\topts->tsval = skb ? tcp_skb_timestamp(skb) + tp->tsoffset : 0;\n\t\topts->tsecr = tp->rx_opt.ts_recent;\n\t\tsize += TCPOLEN_TSTAMP_ALIGNED;\n\t}\n\n\t \n\tif (sk_is_mptcp(sk)) {\n\t\tunsigned int remaining = MAX_TCP_OPTION_SPACE - size;\n\t\tunsigned int opt_size = 0;\n\n\t\tif (mptcp_established_options(sk, skb, &opt_size, remaining,\n\t\t\t\t\t      &opts->mptcp)) {\n\t\t\topts->options |= OPTION_MPTCP;\n\t\t\tsize += opt_size;\n\t\t}\n\t}\n\n\teff_sacks = tp->rx_opt.num_sacks + tp->rx_opt.dsack;\n\tif (unlikely(eff_sacks)) {\n\t\tconst unsigned int remaining = MAX_TCP_OPTION_SPACE - size;\n\t\tif (unlikely(remaining < TCPOLEN_SACK_BASE_ALIGNED +\n\t\t\t\t\t TCPOLEN_SACK_PERBLOCK))\n\t\t\treturn size;\n\n\t\topts->num_sack_blocks =\n\t\t\tmin_t(unsigned int, eff_sacks,\n\t\t\t      (remaining - TCPOLEN_SACK_BASE_ALIGNED) /\n\t\t\t      TCPOLEN_SACK_PERBLOCK);\n\n\t\tsize += TCPOLEN_SACK_BASE_ALIGNED +\n\t\t\topts->num_sack_blocks * TCPOLEN_SACK_PERBLOCK;\n\t}\n\n\tif (unlikely(BPF_SOCK_OPS_TEST_FLAG(tp,\n\t\t\t\t\t    BPF_SOCK_OPS_WRITE_HDR_OPT_CB_FLAG))) {\n\t\tunsigned int remaining = MAX_TCP_OPTION_SPACE - size;\n\n\t\tbpf_skops_hdr_opt_len(sk, skb, NULL, NULL, 0, opts, &remaining);\n\n\t\tsize = MAX_TCP_OPTION_SPACE - remaining;\n\t}\n\n\treturn size;\n}\n\n\n \nstruct tsq_tasklet {\n\tstruct tasklet_struct\ttasklet;\n\tstruct list_head\thead;  \n};\nstatic DEFINE_PER_CPU(struct tsq_tasklet, tsq_tasklet);\n\nstatic void tcp_tsq_write(struct sock *sk)\n{\n\tif ((1 << sk->sk_state) &\n\t    (TCPF_ESTABLISHED | TCPF_FIN_WAIT1 | TCPF_CLOSING |\n\t     TCPF_CLOSE_WAIT  | TCPF_LAST_ACK)) {\n\t\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t\tif (tp->lost_out > tp->retrans_out &&\n\t\t    tcp_snd_cwnd(tp) > tcp_packets_in_flight(tp)) {\n\t\t\ttcp_mstamp_refresh(tp);\n\t\t\ttcp_xmit_retransmit_queue(sk);\n\t\t}\n\n\t\ttcp_write_xmit(sk, tcp_current_mss(sk), tp->nonagle,\n\t\t\t       0, GFP_ATOMIC);\n\t}\n}\n\nstatic void tcp_tsq_handler(struct sock *sk)\n{\n\tbh_lock_sock(sk);\n\tif (!sock_owned_by_user(sk))\n\t\ttcp_tsq_write(sk);\n\telse if (!test_and_set_bit(TCP_TSQ_DEFERRED, &sk->sk_tsq_flags))\n\t\tsock_hold(sk);\n\tbh_unlock_sock(sk);\n}\n \nstatic void tcp_tasklet_func(struct tasklet_struct *t)\n{\n\tstruct tsq_tasklet *tsq = from_tasklet(tsq,  t, tasklet);\n\tLIST_HEAD(list);\n\tunsigned long flags;\n\tstruct list_head *q, *n;\n\tstruct tcp_sock *tp;\n\tstruct sock *sk;\n\n\tlocal_irq_save(flags);\n\tlist_splice_init(&tsq->head, &list);\n\tlocal_irq_restore(flags);\n\n\tlist_for_each_safe(q, n, &list) {\n\t\ttp = list_entry(q, struct tcp_sock, tsq_node);\n\t\tlist_del(&tp->tsq_node);\n\n\t\tsk = (struct sock *)tp;\n\t\tsmp_mb__before_atomic();\n\t\tclear_bit(TSQ_QUEUED, &sk->sk_tsq_flags);\n\n\t\ttcp_tsq_handler(sk);\n\t\tsk_free(sk);\n\t}\n}\n\n#define TCP_DEFERRED_ALL (TCPF_TSQ_DEFERRED |\t\t\\\n\t\t\t  TCPF_WRITE_TIMER_DEFERRED |\t\\\n\t\t\t  TCPF_DELACK_TIMER_DEFERRED |\t\\\n\t\t\t  TCPF_MTU_REDUCED_DEFERRED)\n \nvoid tcp_release_cb(struct sock *sk)\n{\n\tunsigned long flags = smp_load_acquire(&sk->sk_tsq_flags);\n\tunsigned long nflags;\n\n\t \n\tdo {\n\t\tif (!(flags & TCP_DEFERRED_ALL))\n\t\t\treturn;\n\t\tnflags = flags & ~TCP_DEFERRED_ALL;\n\t} while (!try_cmpxchg(&sk->sk_tsq_flags, &flags, nflags));\n\n\tif (flags & TCPF_TSQ_DEFERRED) {\n\t\ttcp_tsq_write(sk);\n\t\t__sock_put(sk);\n\t}\n\t \n\tsock_release_ownership(sk);\n\n\tif (flags & TCPF_WRITE_TIMER_DEFERRED) {\n\t\ttcp_write_timer_handler(sk);\n\t\t__sock_put(sk);\n\t}\n\tif (flags & TCPF_DELACK_TIMER_DEFERRED) {\n\t\ttcp_delack_timer_handler(sk);\n\t\t__sock_put(sk);\n\t}\n\tif (flags & TCPF_MTU_REDUCED_DEFERRED) {\n\t\tinet_csk(sk)->icsk_af_ops->mtu_reduced(sk);\n\t\t__sock_put(sk);\n\t}\n}\nEXPORT_SYMBOL(tcp_release_cb);\n\nvoid __init tcp_tasklet_init(void)\n{\n\tint i;\n\n\tfor_each_possible_cpu(i) {\n\t\tstruct tsq_tasklet *tsq = &per_cpu(tsq_tasklet, i);\n\n\t\tINIT_LIST_HEAD(&tsq->head);\n\t\ttasklet_setup(&tsq->tasklet, tcp_tasklet_func);\n\t}\n}\n\n \nvoid tcp_wfree(struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tunsigned long flags, nval, oval;\n\tstruct tsq_tasklet *tsq;\n\tbool empty;\n\n\t \n\tWARN_ON(refcount_sub_and_test(skb->truesize - 1, &sk->sk_wmem_alloc));\n\n\t \n\tif (refcount_read(&sk->sk_wmem_alloc) >= SKB_TRUESIZE(1) && this_cpu_ksoftirqd() == current)\n\t\tgoto out;\n\n\toval = smp_load_acquire(&sk->sk_tsq_flags);\n\tdo {\n\t\tif (!(oval & TSQF_THROTTLED) || (oval & TSQF_QUEUED))\n\t\t\tgoto out;\n\n\t\tnval = (oval & ~TSQF_THROTTLED) | TSQF_QUEUED;\n\t} while (!try_cmpxchg(&sk->sk_tsq_flags, &oval, nval));\n\n\t \n\tlocal_irq_save(flags);\n\ttsq = this_cpu_ptr(&tsq_tasklet);\n\tempty = list_empty(&tsq->head);\n\tlist_add(&tp->tsq_node, &tsq->head);\n\tif (empty)\n\t\ttasklet_schedule(&tsq->tasklet);\n\tlocal_irq_restore(flags);\n\treturn;\nout:\n\tsk_free(sk);\n}\n\n \nenum hrtimer_restart tcp_pace_kick(struct hrtimer *timer)\n{\n\tstruct tcp_sock *tp = container_of(timer, struct tcp_sock, pacing_timer);\n\tstruct sock *sk = (struct sock *)tp;\n\n\ttcp_tsq_handler(sk);\n\tsock_put(sk);\n\n\treturn HRTIMER_NORESTART;\n}\n\nstatic void tcp_update_skb_after_send(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t      u64 prior_wstamp)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (sk->sk_pacing_status != SK_PACING_NONE) {\n\t\tunsigned long rate = sk->sk_pacing_rate;\n\n\t\t \n\t\tif (rate != ~0UL && rate && tp->data_segs_out >= 10) {\n\t\t\tu64 len_ns = div64_ul((u64)skb->len * NSEC_PER_SEC, rate);\n\t\t\tu64 credit = tp->tcp_wstamp_ns - prior_wstamp;\n\n\t\t\t \n\t\t\tlen_ns -= min_t(u64, len_ns / 2, credit);\n\t\t\ttp->tcp_wstamp_ns += len_ns;\n\t\t}\n\t}\n\tlist_move_tail(&skb->tcp_tsorted_anchor, &tp->tsorted_sent_queue);\n}\n\nINDIRECT_CALLABLE_DECLARE(int ip_queue_xmit(struct sock *sk, struct sk_buff *skb, struct flowi *fl));\nINDIRECT_CALLABLE_DECLARE(int inet6_csk_xmit(struct sock *sk, struct sk_buff *skb, struct flowi *fl));\nINDIRECT_CALLABLE_DECLARE(void tcp_v4_send_check(struct sock *sk, struct sk_buff *skb));\n\n \nstatic int __tcp_transmit_skb(struct sock *sk, struct sk_buff *skb,\n\t\t\t      int clone_it, gfp_t gfp_mask, u32 rcv_nxt)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct inet_sock *inet;\n\tstruct tcp_sock *tp;\n\tstruct tcp_skb_cb *tcb;\n\tstruct tcp_out_options opts;\n\tunsigned int tcp_options_size, tcp_header_size;\n\tstruct sk_buff *oskb = NULL;\n\tstruct tcp_md5sig_key *md5;\n\tstruct tcphdr *th;\n\tu64 prior_wstamp;\n\tint err;\n\n\tBUG_ON(!skb || !tcp_skb_pcount(skb));\n\ttp = tcp_sk(sk);\n\tprior_wstamp = tp->tcp_wstamp_ns;\n\ttp->tcp_wstamp_ns = max(tp->tcp_wstamp_ns, tp->tcp_clock_cache);\n\tskb_set_delivery_time(skb, tp->tcp_wstamp_ns, true);\n\tif (clone_it) {\n\t\toskb = skb;\n\n\t\ttcp_skb_tsorted_save(oskb) {\n\t\t\tif (unlikely(skb_cloned(oskb)))\n\t\t\t\tskb = pskb_copy(oskb, gfp_mask);\n\t\t\telse\n\t\t\t\tskb = skb_clone(oskb, gfp_mask);\n\t\t} tcp_skb_tsorted_restore(oskb);\n\n\t\tif (unlikely(!skb))\n\t\t\treturn -ENOBUFS;\n\t\t \n\t\tskb->dev = NULL;\n\t}\n\n\tinet = inet_sk(sk);\n\ttcb = TCP_SKB_CB(skb);\n\tmemset(&opts, 0, sizeof(opts));\n\n\tif (unlikely(tcb->tcp_flags & TCPHDR_SYN)) {\n\t\ttcp_options_size = tcp_syn_options(sk, skb, &opts, &md5);\n\t} else {\n\t\ttcp_options_size = tcp_established_options(sk, skb, &opts,\n\t\t\t\t\t\t\t   &md5);\n\t\t \n\t\tif (tcp_skb_pcount(skb) > 1)\n\t\t\ttcb->tcp_flags |= TCPHDR_PSH;\n\t}\n\ttcp_header_size = tcp_options_size + sizeof(struct tcphdr);\n\n\t \n\tskb->ooo_okay = sk_wmem_alloc_get(sk) < SKB_TRUESIZE(1) ||\n\t\t\ttcp_rtx_queue_empty(sk);\n\n\t \n\tskb->pfmemalloc = 0;\n\n\tskb_push(skb, tcp_header_size);\n\tskb_reset_transport_header(skb);\n\n\tskb_orphan(skb);\n\tskb->sk = sk;\n\tskb->destructor = skb_is_tcp_pure_ack(skb) ? __sock_wfree : tcp_wfree;\n\trefcount_add(skb->truesize, &sk->sk_wmem_alloc);\n\n\tskb_set_dst_pending_confirm(skb, READ_ONCE(sk->sk_dst_pending_confirm));\n\n\t \n\tth = (struct tcphdr *)skb->data;\n\tth->source\t\t= inet->inet_sport;\n\tth->dest\t\t= inet->inet_dport;\n\tth->seq\t\t\t= htonl(tcb->seq);\n\tth->ack_seq\t\t= htonl(rcv_nxt);\n\t*(((__be16 *)th) + 6)\t= htons(((tcp_header_size >> 2) << 12) |\n\t\t\t\t\ttcb->tcp_flags);\n\n\tth->check\t\t= 0;\n\tth->urg_ptr\t\t= 0;\n\n\t \n\tif (unlikely(tcp_urg_mode(tp) && before(tcb->seq, tp->snd_up))) {\n\t\tif (before(tp->snd_up, tcb->seq + 0x10000)) {\n\t\t\tth->urg_ptr = htons(tp->snd_up - tcb->seq);\n\t\t\tth->urg = 1;\n\t\t} else if (after(tcb->seq + 0xFFFF, tp->snd_nxt)) {\n\t\t\tth->urg_ptr = htons(0xFFFF);\n\t\t\tth->urg = 1;\n\t\t}\n\t}\n\n\tskb_shinfo(skb)->gso_type = sk->sk_gso_type;\n\tif (likely(!(tcb->tcp_flags & TCPHDR_SYN))) {\n\t\tth->window      = htons(tcp_select_window(sk));\n\t\ttcp_ecn_send(sk, skb, th, tcp_header_size);\n\t} else {\n\t\t \n\t\tth->window\t= htons(min(tp->rcv_wnd, 65535U));\n\t}\n\n\ttcp_options_write(th, tp, &opts);\n\n#ifdef CONFIG_TCP_MD5SIG\n\t \n\tif (md5) {\n\t\tsk_gso_disable(sk);\n\t\ttp->af_specific->calc_md5_hash(opts.hash_location,\n\t\t\t\t\t       md5, sk, skb);\n\t}\n#endif\n\n\t \n\tbpf_skops_write_hdr_opt(sk, skb, NULL, NULL, 0, &opts);\n\n\tINDIRECT_CALL_INET(icsk->icsk_af_ops->send_check,\n\t\t\t   tcp_v6_send_check, tcp_v4_send_check,\n\t\t\t   sk, skb);\n\n\tif (likely(tcb->tcp_flags & TCPHDR_ACK))\n\t\ttcp_event_ack_sent(sk, rcv_nxt);\n\n\tif (skb->len != tcp_header_size) {\n\t\ttcp_event_data_sent(tp, sk);\n\t\ttp->data_segs_out += tcp_skb_pcount(skb);\n\t\ttp->bytes_sent += skb->len - tcp_header_size;\n\t}\n\n\tif (after(tcb->end_seq, tp->snd_nxt) || tcb->seq == tcb->end_seq)\n\t\tTCP_ADD_STATS(sock_net(sk), TCP_MIB_OUTSEGS,\n\t\t\t      tcp_skb_pcount(skb));\n\n\ttp->segs_out += tcp_skb_pcount(skb);\n\tskb_set_hash_from_sk(skb, sk);\n\t \n\tskb_shinfo(skb)->gso_segs = tcp_skb_pcount(skb);\n\tskb_shinfo(skb)->gso_size = tcp_skb_mss(skb);\n\n\t \n\n\t \n\tmemset(skb->cb, 0, max(sizeof(struct inet_skb_parm),\n\t\t\t       sizeof(struct inet6_skb_parm)));\n\n\ttcp_add_tx_delay(skb, tp);\n\n\terr = INDIRECT_CALL_INET(icsk->icsk_af_ops->queue_xmit,\n\t\t\t\t inet6_csk_xmit, ip_queue_xmit,\n\t\t\t\t sk, skb, &inet->cork.fl);\n\n\tif (unlikely(err > 0)) {\n\t\ttcp_enter_cwr(sk);\n\t\terr = net_xmit_eval(err);\n\t}\n\tif (!err && oskb) {\n\t\ttcp_update_skb_after_send(sk, oskb, prior_wstamp);\n\t\ttcp_rate_skb_sent(sk, oskb);\n\t}\n\treturn err;\n}\n\nstatic int tcp_transmit_skb(struct sock *sk, struct sk_buff *skb, int clone_it,\n\t\t\t    gfp_t gfp_mask)\n{\n\treturn __tcp_transmit_skb(sk, skb, clone_it, gfp_mask,\n\t\t\t\t  tcp_sk(sk)->rcv_nxt);\n}\n\n \nstatic void tcp_queue_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t \n\tWRITE_ONCE(tp->write_seq, TCP_SKB_CB(skb)->end_seq);\n\t__skb_header_release(skb);\n\ttcp_add_write_queue_tail(sk, skb);\n\tsk_wmem_queued_add(sk, skb->truesize);\n\tsk_mem_charge(sk, skb->truesize);\n}\n\n \nstatic void tcp_set_skb_tso_segs(struct sk_buff *skb, unsigned int mss_now)\n{\n\tif (skb->len <= mss_now) {\n\t\t \n\t\ttcp_skb_pcount_set(skb, 1);\n\t\tTCP_SKB_CB(skb)->tcp_gso_size = 0;\n\t} else {\n\t\ttcp_skb_pcount_set(skb, DIV_ROUND_UP(skb->len, mss_now));\n\t\tTCP_SKB_CB(skb)->tcp_gso_size = mss_now;\n\t}\n}\n\n \nstatic void tcp_adjust_pcount(struct sock *sk, const struct sk_buff *skb, int decr)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\ttp->packets_out -= decr;\n\n\tif (TCP_SKB_CB(skb)->sacked & TCPCB_SACKED_ACKED)\n\t\ttp->sacked_out -= decr;\n\tif (TCP_SKB_CB(skb)->sacked & TCPCB_SACKED_RETRANS)\n\t\ttp->retrans_out -= decr;\n\tif (TCP_SKB_CB(skb)->sacked & TCPCB_LOST)\n\t\ttp->lost_out -= decr;\n\n\t \n\tif (tcp_is_reno(tp) && decr > 0)\n\t\ttp->sacked_out -= min_t(u32, tp->sacked_out, decr);\n\n\tif (tp->lost_skb_hint &&\n\t    before(TCP_SKB_CB(skb)->seq, TCP_SKB_CB(tp->lost_skb_hint)->seq) &&\n\t    (TCP_SKB_CB(skb)->sacked & TCPCB_SACKED_ACKED))\n\t\ttp->lost_cnt_hint -= decr;\n\n\ttcp_verify_left_out(tp);\n}\n\nstatic bool tcp_has_tx_tstamp(const struct sk_buff *skb)\n{\n\treturn TCP_SKB_CB(skb)->txstamp_ack ||\n\t\t(skb_shinfo(skb)->tx_flags & SKBTX_ANY_TSTAMP);\n}\n\nstatic void tcp_fragment_tstamp(struct sk_buff *skb, struct sk_buff *skb2)\n{\n\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\n\tif (unlikely(tcp_has_tx_tstamp(skb)) &&\n\t    !before(shinfo->tskey, TCP_SKB_CB(skb2)->seq)) {\n\t\tstruct skb_shared_info *shinfo2 = skb_shinfo(skb2);\n\t\tu8 tsflags = shinfo->tx_flags & SKBTX_ANY_TSTAMP;\n\n\t\tshinfo->tx_flags &= ~tsflags;\n\t\tshinfo2->tx_flags |= tsflags;\n\t\tswap(shinfo->tskey, shinfo2->tskey);\n\t\tTCP_SKB_CB(skb2)->txstamp_ack = TCP_SKB_CB(skb)->txstamp_ack;\n\t\tTCP_SKB_CB(skb)->txstamp_ack = 0;\n\t}\n}\n\nstatic void tcp_skb_fragment_eor(struct sk_buff *skb, struct sk_buff *skb2)\n{\n\tTCP_SKB_CB(skb2)->eor = TCP_SKB_CB(skb)->eor;\n\tTCP_SKB_CB(skb)->eor = 0;\n}\n\n \nstatic void tcp_insert_write_queue_after(struct sk_buff *skb,\n\t\t\t\t\t struct sk_buff *buff,\n\t\t\t\t\t struct sock *sk,\n\t\t\t\t\t enum tcp_queue tcp_queue)\n{\n\tif (tcp_queue == TCP_FRAG_IN_WRITE_QUEUE)\n\t\t__skb_queue_after(&sk->sk_write_queue, skb, buff);\n\telse\n\t\ttcp_rbtree_insert(&sk->tcp_rtx_queue, buff);\n}\n\n \nint tcp_fragment(struct sock *sk, enum tcp_queue tcp_queue,\n\t\t struct sk_buff *skb, u32 len,\n\t\t unsigned int mss_now, gfp_t gfp)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *buff;\n\tint old_factor;\n\tlong limit;\n\tint nlen;\n\tu8 flags;\n\n\tif (WARN_ON(len > skb->len))\n\t\treturn -EINVAL;\n\n\tDEBUG_NET_WARN_ON_ONCE(skb_headlen(skb));\n\n\t \n\tlimit = sk->sk_sndbuf + 2 * SKB_TRUESIZE(GSO_LEGACY_MAX_SIZE);\n\tif (unlikely((sk->sk_wmem_queued >> 1) > limit &&\n\t\t     tcp_queue != TCP_FRAG_IN_WRITE_QUEUE &&\n\t\t     skb != tcp_rtx_queue_head(sk) &&\n\t\t     skb != tcp_rtx_queue_tail(sk))) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPWQUEUETOOBIG);\n\t\treturn -ENOMEM;\n\t}\n\n\tif (skb_unclone_keeptruesize(skb, gfp))\n\t\treturn -ENOMEM;\n\n\t \n\tbuff = tcp_stream_alloc_skb(sk, gfp, true);\n\tif (!buff)\n\t\treturn -ENOMEM;  \n\tskb_copy_decrypted(buff, skb);\n\tmptcp_skb_ext_copy(buff, skb);\n\n\tsk_wmem_queued_add(sk, buff->truesize);\n\tsk_mem_charge(sk, buff->truesize);\n\tnlen = skb->len - len;\n\tbuff->truesize += nlen;\n\tskb->truesize -= nlen;\n\n\t \n\tTCP_SKB_CB(buff)->seq = TCP_SKB_CB(skb)->seq + len;\n\tTCP_SKB_CB(buff)->end_seq = TCP_SKB_CB(skb)->end_seq;\n\tTCP_SKB_CB(skb)->end_seq = TCP_SKB_CB(buff)->seq;\n\n\t \n\tflags = TCP_SKB_CB(skb)->tcp_flags;\n\tTCP_SKB_CB(skb)->tcp_flags = flags & ~(TCPHDR_FIN | TCPHDR_PSH);\n\tTCP_SKB_CB(buff)->tcp_flags = flags;\n\tTCP_SKB_CB(buff)->sacked = TCP_SKB_CB(skb)->sacked;\n\ttcp_skb_fragment_eor(skb, buff);\n\n\tskb_split(skb, buff, len);\n\n\tskb_set_delivery_time(buff, skb->tstamp, true);\n\ttcp_fragment_tstamp(skb, buff);\n\n\told_factor = tcp_skb_pcount(skb);\n\n\t \n\ttcp_set_skb_tso_segs(skb, mss_now);\n\ttcp_set_skb_tso_segs(buff, mss_now);\n\n\t \n\tTCP_SKB_CB(buff)->tx = TCP_SKB_CB(skb)->tx;\n\n\t \n\tif (!before(tp->snd_nxt, TCP_SKB_CB(buff)->end_seq)) {\n\t\tint diff = old_factor - tcp_skb_pcount(skb) -\n\t\t\ttcp_skb_pcount(buff);\n\n\t\tif (diff)\n\t\t\ttcp_adjust_pcount(sk, skb, diff);\n\t}\n\n\t \n\t__skb_header_release(buff);\n\ttcp_insert_write_queue_after(skb, buff, sk, tcp_queue);\n\tif (tcp_queue == TCP_FRAG_IN_RTX_QUEUE)\n\t\tlist_add(&buff->tcp_tsorted_anchor, &skb->tcp_tsorted_anchor);\n\n\treturn 0;\n}\n\n \nstatic int __pskb_trim_head(struct sk_buff *skb, int len)\n{\n\tstruct skb_shared_info *shinfo;\n\tint i, k, eat;\n\n\tDEBUG_NET_WARN_ON_ONCE(skb_headlen(skb));\n\teat = len;\n\tk = 0;\n\tshinfo = skb_shinfo(skb);\n\tfor (i = 0; i < shinfo->nr_frags; i++) {\n\t\tint size = skb_frag_size(&shinfo->frags[i]);\n\n\t\tif (size <= eat) {\n\t\t\tskb_frag_unref(skb, i);\n\t\t\teat -= size;\n\t\t} else {\n\t\t\tshinfo->frags[k] = shinfo->frags[i];\n\t\t\tif (eat) {\n\t\t\t\tskb_frag_off_add(&shinfo->frags[k], eat);\n\t\t\t\tskb_frag_size_sub(&shinfo->frags[k], eat);\n\t\t\t\teat = 0;\n\t\t\t}\n\t\t\tk++;\n\t\t}\n\t}\n\tshinfo->nr_frags = k;\n\n\tskb->data_len -= len;\n\tskb->len = skb->data_len;\n\treturn len;\n}\n\n \nint tcp_trim_head(struct sock *sk, struct sk_buff *skb, u32 len)\n{\n\tu32 delta_truesize;\n\n\tif (skb_unclone_keeptruesize(skb, GFP_ATOMIC))\n\t\treturn -ENOMEM;\n\n\tdelta_truesize = __pskb_trim_head(skb, len);\n\n\tTCP_SKB_CB(skb)->seq += len;\n\n\tskb->truesize\t   -= delta_truesize;\n\tsk_wmem_queued_add(sk, -delta_truesize);\n\tif (!skb_zcopy_pure(skb))\n\t\tsk_mem_uncharge(sk, delta_truesize);\n\n\t \n\tif (tcp_skb_pcount(skb) > 1)\n\t\ttcp_set_skb_tso_segs(skb, tcp_skb_mss(skb));\n\n\treturn 0;\n}\n\n \nstatic inline int __tcp_mtu_to_mss(struct sock *sk, int pmtu)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tint mss_now;\n\n\t \n\tmss_now = pmtu - icsk->icsk_af_ops->net_header_len - sizeof(struct tcphdr);\n\n\t \n\tif (icsk->icsk_af_ops->net_frag_header_len) {\n\t\tconst struct dst_entry *dst = __sk_dst_get(sk);\n\n\t\tif (dst && dst_allfrag(dst))\n\t\t\tmss_now -= icsk->icsk_af_ops->net_frag_header_len;\n\t}\n\n\t \n\tif (mss_now > tp->rx_opt.mss_clamp)\n\t\tmss_now = tp->rx_opt.mss_clamp;\n\n\t \n\tmss_now -= icsk->icsk_ext_hdr_len;\n\n\t \n\tmss_now = max(mss_now,\n\t\t      READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_min_snd_mss));\n\treturn mss_now;\n}\n\n \nint tcp_mtu_to_mss(struct sock *sk, int pmtu)\n{\n\t \n\treturn __tcp_mtu_to_mss(sk, pmtu) -\n\t       (tcp_sk(sk)->tcp_header_len - sizeof(struct tcphdr));\n}\nEXPORT_SYMBOL(tcp_mtu_to_mss);\n\n \nint tcp_mss_to_mtu(struct sock *sk, int mss)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tint mtu;\n\n\tmtu = mss +\n\t      tp->tcp_header_len +\n\t      icsk->icsk_ext_hdr_len +\n\t      icsk->icsk_af_ops->net_header_len;\n\n\t \n\tif (icsk->icsk_af_ops->net_frag_header_len) {\n\t\tconst struct dst_entry *dst = __sk_dst_get(sk);\n\n\t\tif (dst && dst_allfrag(dst))\n\t\t\tmtu += icsk->icsk_af_ops->net_frag_header_len;\n\t}\n\treturn mtu;\n}\nEXPORT_SYMBOL(tcp_mss_to_mtu);\n\n \nvoid tcp_mtup_init(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct net *net = sock_net(sk);\n\n\ticsk->icsk_mtup.enabled = READ_ONCE(net->ipv4.sysctl_tcp_mtu_probing) > 1;\n\ticsk->icsk_mtup.search_high = tp->rx_opt.mss_clamp + sizeof(struct tcphdr) +\n\t\t\t       icsk->icsk_af_ops->net_header_len;\n\ticsk->icsk_mtup.search_low = tcp_mss_to_mtu(sk, READ_ONCE(net->ipv4.sysctl_tcp_base_mss));\n\ticsk->icsk_mtup.probe_size = 0;\n\tif (icsk->icsk_mtup.enabled)\n\t\ticsk->icsk_mtup.probe_timestamp = tcp_jiffies32;\n}\nEXPORT_SYMBOL(tcp_mtup_init);\n\n \nunsigned int tcp_sync_mss(struct sock *sk, u32 pmtu)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tint mss_now;\n\n\tif (icsk->icsk_mtup.search_high > pmtu)\n\t\ticsk->icsk_mtup.search_high = pmtu;\n\n\tmss_now = tcp_mtu_to_mss(sk, pmtu);\n\tmss_now = tcp_bound_to_half_wnd(tp, mss_now);\n\n\t \n\ticsk->icsk_pmtu_cookie = pmtu;\n\tif (icsk->icsk_mtup.enabled)\n\t\tmss_now = min(mss_now, tcp_mtu_to_mss(sk, icsk->icsk_mtup.search_low));\n\ttp->mss_cache = mss_now;\n\n\treturn mss_now;\n}\nEXPORT_SYMBOL(tcp_sync_mss);\n\n \nunsigned int tcp_current_mss(struct sock *sk)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tconst struct dst_entry *dst = __sk_dst_get(sk);\n\tu32 mss_now;\n\tunsigned int header_len;\n\tstruct tcp_out_options opts;\n\tstruct tcp_md5sig_key *md5;\n\n\tmss_now = tp->mss_cache;\n\n\tif (dst) {\n\t\tu32 mtu = dst_mtu(dst);\n\t\tif (mtu != inet_csk(sk)->icsk_pmtu_cookie)\n\t\t\tmss_now = tcp_sync_mss(sk, mtu);\n\t}\n\n\theader_len = tcp_established_options(sk, NULL, &opts, &md5) +\n\t\t     sizeof(struct tcphdr);\n\t \n\tif (header_len != tp->tcp_header_len) {\n\t\tint delta = (int) header_len - tp->tcp_header_len;\n\t\tmss_now -= delta;\n\t}\n\n\treturn mss_now;\n}\n\n \nstatic void tcp_cwnd_application_limited(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (inet_csk(sk)->icsk_ca_state == TCP_CA_Open &&\n\t    sk->sk_socket && !test_bit(SOCK_NOSPACE, &sk->sk_socket->flags)) {\n\t\t \n\t\tu32 init_win = tcp_init_cwnd(tp, __sk_dst_get(sk));\n\t\tu32 win_used = max(tp->snd_cwnd_used, init_win);\n\t\tif (win_used < tcp_snd_cwnd(tp)) {\n\t\t\ttp->snd_ssthresh = tcp_current_ssthresh(sk);\n\t\t\ttcp_snd_cwnd_set(tp, (tcp_snd_cwnd(tp) + win_used) >> 1);\n\t\t}\n\t\ttp->snd_cwnd_used = 0;\n\t}\n\ttp->snd_cwnd_stamp = tcp_jiffies32;\n}\n\nstatic void tcp_cwnd_validate(struct sock *sk, bool is_cwnd_limited)\n{\n\tconst struct tcp_congestion_ops *ca_ops = inet_csk(sk)->icsk_ca_ops;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t \n\tif (!before(tp->snd_una, tp->cwnd_usage_seq) ||\n\t    is_cwnd_limited ||\n\t    (!tp->is_cwnd_limited &&\n\t     tp->packets_out > tp->max_packets_out)) {\n\t\ttp->is_cwnd_limited = is_cwnd_limited;\n\t\ttp->max_packets_out = tp->packets_out;\n\t\ttp->cwnd_usage_seq = tp->snd_nxt;\n\t}\n\n\tif (tcp_is_cwnd_limited(sk)) {\n\t\t \n\t\ttp->snd_cwnd_used = 0;\n\t\ttp->snd_cwnd_stamp = tcp_jiffies32;\n\t} else {\n\t\t \n\t\tif (tp->packets_out > tp->snd_cwnd_used)\n\t\t\ttp->snd_cwnd_used = tp->packets_out;\n\n\t\tif (READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_slow_start_after_idle) &&\n\t\t    (s32)(tcp_jiffies32 - tp->snd_cwnd_stamp) >= inet_csk(sk)->icsk_rto &&\n\t\t    !ca_ops->cong_control)\n\t\t\ttcp_cwnd_application_limited(sk);\n\n\t\t \n\t\tif (tcp_write_queue_empty(sk) && sk->sk_socket &&\n\t\t    test_bit(SOCK_NOSPACE, &sk->sk_socket->flags) &&\n\t\t    (1 << sk->sk_state) & (TCPF_ESTABLISHED | TCPF_CLOSE_WAIT))\n\t\t\ttcp_chrono_start(sk, TCP_CHRONO_SNDBUF_LIMITED);\n\t}\n}\n\n \nstatic bool tcp_minshall_check(const struct tcp_sock *tp)\n{\n\treturn after(tp->snd_sml, tp->snd_una) &&\n\t\t!after(tp->snd_sml, tp->snd_nxt);\n}\n\n \nstatic void tcp_minshall_update(struct tcp_sock *tp, unsigned int mss_now,\n\t\t\t\tconst struct sk_buff *skb)\n{\n\tif (skb->len < tcp_skb_pcount(skb) * mss_now)\n\t\ttp->snd_sml = TCP_SKB_CB(skb)->end_seq;\n}\n\n \nstatic bool tcp_nagle_check(bool partial, const struct tcp_sock *tp,\n\t\t\t    int nonagle)\n{\n\treturn partial &&\n\t\t((nonagle & TCP_NAGLE_CORK) ||\n\t\t (!nonagle && tp->packets_out && tcp_minshall_check(tp)));\n}\n\n \nstatic u32 tcp_tso_autosize(const struct sock *sk, unsigned int mss_now,\n\t\t\t    int min_tso_segs)\n{\n\tunsigned long bytes;\n\tu32 r;\n\n\tbytes = sk->sk_pacing_rate >> READ_ONCE(sk->sk_pacing_shift);\n\n\tr = tcp_min_rtt(tcp_sk(sk)) >> READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_tso_rtt_log);\n\tif (r < BITS_PER_TYPE(sk->sk_gso_max_size))\n\t\tbytes += sk->sk_gso_max_size >> r;\n\n\tbytes = min_t(unsigned long, bytes, sk->sk_gso_max_size);\n\n\treturn max_t(u32, bytes / mss_now, min_tso_segs);\n}\n\n \nstatic u32 tcp_tso_segs(struct sock *sk, unsigned int mss_now)\n{\n\tconst struct tcp_congestion_ops *ca_ops = inet_csk(sk)->icsk_ca_ops;\n\tu32 min_tso, tso_segs;\n\n\tmin_tso = ca_ops->min_tso_segs ?\n\t\t\tca_ops->min_tso_segs(sk) :\n\t\t\tREAD_ONCE(sock_net(sk)->ipv4.sysctl_tcp_min_tso_segs);\n\n\ttso_segs = tcp_tso_autosize(sk, mss_now, min_tso);\n\treturn min_t(u32, tso_segs, sk->sk_gso_max_segs);\n}\n\n \nstatic unsigned int tcp_mss_split_point(const struct sock *sk,\n\t\t\t\t\tconst struct sk_buff *skb,\n\t\t\t\t\tunsigned int mss_now,\n\t\t\t\t\tunsigned int max_segs,\n\t\t\t\t\tint nonagle)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tu32 partial, needed, window, max_len;\n\n\twindow = tcp_wnd_end(tp) - TCP_SKB_CB(skb)->seq;\n\tmax_len = mss_now * max_segs;\n\n\tif (likely(max_len <= window && skb != tcp_write_queue_tail(sk)))\n\t\treturn max_len;\n\n\tneeded = min(skb->len, window);\n\n\tif (max_len <= needed)\n\t\treturn max_len;\n\n\tpartial = needed % mss_now;\n\t \n\tif (tcp_nagle_check(partial != 0, tp, nonagle))\n\t\treturn needed - partial;\n\n\treturn needed;\n}\n\n \nstatic inline unsigned int tcp_cwnd_test(const struct tcp_sock *tp,\n\t\t\t\t\t const struct sk_buff *skb)\n{\n\tu32 in_flight, cwnd, halfcwnd;\n\n\t \n\tif ((TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN) &&\n\t    tcp_skb_pcount(skb) == 1)\n\t\treturn 1;\n\n\tin_flight = tcp_packets_in_flight(tp);\n\tcwnd = tcp_snd_cwnd(tp);\n\tif (in_flight >= cwnd)\n\t\treturn 0;\n\n\t \n\thalfcwnd = max(cwnd >> 1, 1U);\n\treturn min(halfcwnd, cwnd - in_flight);\n}\n\n \nstatic int tcp_init_tso_segs(struct sk_buff *skb, unsigned int mss_now)\n{\n\tint tso_segs = tcp_skb_pcount(skb);\n\n\tif (!tso_segs || (tso_segs > 1 && tcp_skb_mss(skb) != mss_now)) {\n\t\ttcp_set_skb_tso_segs(skb, mss_now);\n\t\ttso_segs = tcp_skb_pcount(skb);\n\t}\n\treturn tso_segs;\n}\n\n\n \nstatic inline bool tcp_nagle_test(const struct tcp_sock *tp, const struct sk_buff *skb,\n\t\t\t\t  unsigned int cur_mss, int nonagle)\n{\n\t \n\tif (nonagle & TCP_NAGLE_PUSH)\n\t\treturn true;\n\n\t \n\tif (tcp_urg_mode(tp) || (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN))\n\t\treturn true;\n\n\tif (!tcp_nagle_check(skb->len < cur_mss, tp, nonagle))\n\t\treturn true;\n\n\treturn false;\n}\n\n \nstatic bool tcp_snd_wnd_test(const struct tcp_sock *tp,\n\t\t\t     const struct sk_buff *skb,\n\t\t\t     unsigned int cur_mss)\n{\n\tu32 end_seq = TCP_SKB_CB(skb)->end_seq;\n\n\tif (skb->len > cur_mss)\n\t\tend_seq = TCP_SKB_CB(skb)->seq + cur_mss;\n\n\treturn !after(end_seq, tcp_wnd_end(tp));\n}\n\n \nstatic int tso_fragment(struct sock *sk, struct sk_buff *skb, unsigned int len,\n\t\t\tunsigned int mss_now, gfp_t gfp)\n{\n\tint nlen = skb->len - len;\n\tstruct sk_buff *buff;\n\tu8 flags;\n\n\t \n\tDEBUG_NET_WARN_ON_ONCE(skb->len != skb->data_len);\n\n\tbuff = tcp_stream_alloc_skb(sk, gfp, true);\n\tif (unlikely(!buff))\n\t\treturn -ENOMEM;\n\tskb_copy_decrypted(buff, skb);\n\tmptcp_skb_ext_copy(buff, skb);\n\n\tsk_wmem_queued_add(sk, buff->truesize);\n\tsk_mem_charge(sk, buff->truesize);\n\tbuff->truesize += nlen;\n\tskb->truesize -= nlen;\n\n\t \n\tTCP_SKB_CB(buff)->seq = TCP_SKB_CB(skb)->seq + len;\n\tTCP_SKB_CB(buff)->end_seq = TCP_SKB_CB(skb)->end_seq;\n\tTCP_SKB_CB(skb)->end_seq = TCP_SKB_CB(buff)->seq;\n\n\t \n\tflags = TCP_SKB_CB(skb)->tcp_flags;\n\tTCP_SKB_CB(skb)->tcp_flags = flags & ~(TCPHDR_FIN | TCPHDR_PSH);\n\tTCP_SKB_CB(buff)->tcp_flags = flags;\n\n\ttcp_skb_fragment_eor(skb, buff);\n\n\tskb_split(skb, buff, len);\n\ttcp_fragment_tstamp(skb, buff);\n\n\t \n\ttcp_set_skb_tso_segs(skb, mss_now);\n\ttcp_set_skb_tso_segs(buff, mss_now);\n\n\t \n\t__skb_header_release(buff);\n\ttcp_insert_write_queue_after(skb, buff, sk, TCP_FRAG_IN_WRITE_QUEUE);\n\n\treturn 0;\n}\n\n \nstatic bool tcp_tso_should_defer(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t bool *is_cwnd_limited,\n\t\t\t\t bool *is_rwnd_limited,\n\t\t\t\t u32 max_segs)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tu32 send_win, cong_win, limit, in_flight;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *head;\n\tint win_divisor;\n\ts64 delta;\n\n\tif (icsk->icsk_ca_state >= TCP_CA_Recovery)\n\t\tgoto send_now;\n\n\t \n\tdelta = tp->tcp_clock_cache - tp->tcp_wstamp_ns - NSEC_PER_MSEC;\n\tif (delta > 0)\n\t\tgoto send_now;\n\n\tin_flight = tcp_packets_in_flight(tp);\n\n\tBUG_ON(tcp_skb_pcount(skb) <= 1);\n\tBUG_ON(tcp_snd_cwnd(tp) <= in_flight);\n\n\tsend_win = tcp_wnd_end(tp) - TCP_SKB_CB(skb)->seq;\n\n\t \n\tcong_win = (tcp_snd_cwnd(tp) - in_flight) * tp->mss_cache;\n\n\tlimit = min(send_win, cong_win);\n\n\t \n\tif (limit >= max_segs * tp->mss_cache)\n\t\tgoto send_now;\n\n\t \n\tif ((skb != tcp_write_queue_tail(sk)) && (limit >= skb->len))\n\t\tgoto send_now;\n\n\twin_divisor = READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_tso_win_divisor);\n\tif (win_divisor) {\n\t\tu32 chunk = min(tp->snd_wnd, tcp_snd_cwnd(tp) * tp->mss_cache);\n\n\t\t \n\t\tchunk /= win_divisor;\n\t\tif (limit >= chunk)\n\t\t\tgoto send_now;\n\t} else {\n\t\t \n\t\tif (limit > tcp_max_tso_deferred_mss(tp) * tp->mss_cache)\n\t\t\tgoto send_now;\n\t}\n\n\t \n\thead = tcp_rtx_queue_head(sk);\n\tif (!head)\n\t\tgoto send_now;\n\tdelta = tp->tcp_clock_cache - head->tstamp;\n\t \n\tif ((s64)(delta - (u64)NSEC_PER_USEC * (tp->srtt_us >> 4)) < 0)\n\t\tgoto send_now;\n\n\t \n\tif (cong_win < send_win) {\n\t\tif (cong_win <= skb->len) {\n\t\t\t*is_cwnd_limited = true;\n\t\t\treturn true;\n\t\t}\n\t} else {\n\t\tif (send_win <= skb->len) {\n\t\t\t*is_rwnd_limited = true;\n\t\t\treturn true;\n\t\t}\n\t}\n\n\t \n\tif ((TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN) ||\n\t    TCP_SKB_CB(skb)->eor)\n\t\tgoto send_now;\n\n\treturn true;\n\nsend_now:\n\treturn false;\n}\n\nstatic inline void tcp_mtu_check_reprobe(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tu32 interval;\n\ts32 delta;\n\n\tinterval = READ_ONCE(net->ipv4.sysctl_tcp_probe_interval);\n\tdelta = tcp_jiffies32 - icsk->icsk_mtup.probe_timestamp;\n\tif (unlikely(delta >= interval * HZ)) {\n\t\tint mss = tcp_current_mss(sk);\n\n\t\t \n\t\ticsk->icsk_mtup.probe_size = 0;\n\t\ticsk->icsk_mtup.search_high = tp->rx_opt.mss_clamp +\n\t\t\tsizeof(struct tcphdr) +\n\t\t\ticsk->icsk_af_ops->net_header_len;\n\t\ticsk->icsk_mtup.search_low = tcp_mss_to_mtu(sk, mss);\n\n\t\t \n\t\ticsk->icsk_mtup.probe_timestamp = tcp_jiffies32;\n\t}\n}\n\nstatic bool tcp_can_coalesce_send_queue_head(struct sock *sk, int len)\n{\n\tstruct sk_buff *skb, *next;\n\n\tskb = tcp_send_head(sk);\n\ttcp_for_write_queue_from_safe(skb, next, sk) {\n\t\tif (len <= skb->len)\n\t\t\tbreak;\n\n\t\tif (unlikely(TCP_SKB_CB(skb)->eor) ||\n\t\t    tcp_has_tx_tstamp(skb) ||\n\t\t    !skb_pure_zcopy_same(skb, next))\n\t\t\treturn false;\n\n\t\tlen -= skb->len;\n\t}\n\n\treturn true;\n}\n\nstatic int tcp_clone_payload(struct sock *sk, struct sk_buff *to,\n\t\t\t     int probe_size)\n{\n\tskb_frag_t *lastfrag = NULL, *fragto = skb_shinfo(to)->frags;\n\tint i, todo, len = 0, nr_frags = 0;\n\tconst struct sk_buff *skb;\n\n\tif (!sk_wmem_schedule(sk, to->truesize + probe_size))\n\t\treturn -ENOMEM;\n\n\tskb_queue_walk(&sk->sk_write_queue, skb) {\n\t\tconst skb_frag_t *fragfrom = skb_shinfo(skb)->frags;\n\n\t\tif (skb_headlen(skb))\n\t\t\treturn -EINVAL;\n\n\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++, fragfrom++) {\n\t\t\tif (len >= probe_size)\n\t\t\t\tgoto commit;\n\t\t\ttodo = min_t(int, skb_frag_size(fragfrom),\n\t\t\t\t     probe_size - len);\n\t\t\tlen += todo;\n\t\t\tif (lastfrag &&\n\t\t\t    skb_frag_page(fragfrom) == skb_frag_page(lastfrag) &&\n\t\t\t    skb_frag_off(fragfrom) == skb_frag_off(lastfrag) +\n\t\t\t\t\t\t      skb_frag_size(lastfrag)) {\n\t\t\t\tskb_frag_size_add(lastfrag, todo);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (unlikely(nr_frags == MAX_SKB_FRAGS))\n\t\t\t\treturn -E2BIG;\n\t\t\tskb_frag_page_copy(fragto, fragfrom);\n\t\t\tskb_frag_off_copy(fragto, fragfrom);\n\t\t\tskb_frag_size_set(fragto, todo);\n\t\t\tnr_frags++;\n\t\t\tlastfrag = fragto++;\n\t\t}\n\t}\ncommit:\n\tWARN_ON_ONCE(len != probe_size);\n\tfor (i = 0; i < nr_frags; i++)\n\t\tskb_frag_ref(to, i);\n\n\tskb_shinfo(to)->nr_frags = nr_frags;\n\tto->truesize += probe_size;\n\tto->len += probe_size;\n\tto->data_len += probe_size;\n\t__skb_header_release(to);\n\treturn 0;\n}\n\n \nstatic int tcp_mtu_probe(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb, *nskb, *next;\n\tstruct net *net = sock_net(sk);\n\tint probe_size;\n\tint size_needed;\n\tint copy, len;\n\tint mss_now;\n\tint interval;\n\n\t \n\tif (likely(!icsk->icsk_mtup.enabled ||\n\t\t   icsk->icsk_mtup.probe_size ||\n\t\t   inet_csk(sk)->icsk_ca_state != TCP_CA_Open ||\n\t\t   tcp_snd_cwnd(tp) < 11 ||\n\t\t   tp->rx_opt.num_sacks || tp->rx_opt.dsack))\n\t\treturn -1;\n\n\t \n\tmss_now = tcp_current_mss(sk);\n\tprobe_size = tcp_mtu_to_mss(sk, (icsk->icsk_mtup.search_high +\n\t\t\t\t    icsk->icsk_mtup.search_low) >> 1);\n\tsize_needed = probe_size + (tp->reordering + 1) * tp->mss_cache;\n\tinterval = icsk->icsk_mtup.search_high - icsk->icsk_mtup.search_low;\n\t \n\tif (probe_size > tcp_mtu_to_mss(sk, icsk->icsk_mtup.search_high) ||\n\t    interval < READ_ONCE(net->ipv4.sysctl_tcp_probe_threshold)) {\n\t\t \n\t\ttcp_mtu_check_reprobe(sk);\n\t\treturn -1;\n\t}\n\n\t \n\tif (tp->write_seq - tp->snd_nxt < size_needed)\n\t\treturn -1;\n\n\tif (tp->snd_wnd < size_needed)\n\t\treturn -1;\n\tif (after(tp->snd_nxt + size_needed, tcp_wnd_end(tp)))\n\t\treturn 0;\n\n\t \n\tif (tcp_packets_in_flight(tp) + 2 > tcp_snd_cwnd(tp)) {\n\t\tif (!tcp_packets_in_flight(tp))\n\t\t\treturn -1;\n\t\telse\n\t\t\treturn 0;\n\t}\n\n\tif (!tcp_can_coalesce_send_queue_head(sk, probe_size))\n\t\treturn -1;\n\n\t \n\tnskb = tcp_stream_alloc_skb(sk, GFP_ATOMIC, false);\n\tif (!nskb)\n\t\treturn -1;\n\n\t \n\tif (tcp_clone_payload(sk, nskb, probe_size)) {\n\t\ttcp_skb_tsorted_anchor_cleanup(nskb);\n\t\tconsume_skb(nskb);\n\t\treturn -1;\n\t}\n\tsk_wmem_queued_add(sk, nskb->truesize);\n\tsk_mem_charge(sk, nskb->truesize);\n\n\tskb = tcp_send_head(sk);\n\tskb_copy_decrypted(nskb, skb);\n\tmptcp_skb_ext_copy(nskb, skb);\n\n\tTCP_SKB_CB(nskb)->seq = TCP_SKB_CB(skb)->seq;\n\tTCP_SKB_CB(nskb)->end_seq = TCP_SKB_CB(skb)->seq + probe_size;\n\tTCP_SKB_CB(nskb)->tcp_flags = TCPHDR_ACK;\n\n\ttcp_insert_write_queue_before(nskb, skb, sk);\n\ttcp_highest_sack_replace(sk, skb, nskb);\n\n\tlen = 0;\n\ttcp_for_write_queue_from_safe(skb, next, sk) {\n\t\tcopy = min_t(int, skb->len, probe_size - len);\n\n\t\tif (skb->len <= copy) {\n\t\t\t \n\t\t\tTCP_SKB_CB(nskb)->tcp_flags |= TCP_SKB_CB(skb)->tcp_flags;\n\t\t\t \n\t\t\tTCP_SKB_CB(nskb)->eor = TCP_SKB_CB(skb)->eor;\n\t\t\ttcp_skb_collapse_tstamp(nskb, skb);\n\t\t\ttcp_unlink_write_queue(skb, sk);\n\t\t\ttcp_wmem_free_skb(sk, skb);\n\t\t} else {\n\t\t\tTCP_SKB_CB(nskb)->tcp_flags |= TCP_SKB_CB(skb)->tcp_flags &\n\t\t\t\t\t\t   ~(TCPHDR_FIN|TCPHDR_PSH);\n\t\t\t__pskb_trim_head(skb, copy);\n\t\t\ttcp_set_skb_tso_segs(skb, mss_now);\n\t\t\tTCP_SKB_CB(skb)->seq += copy;\n\t\t}\n\n\t\tlen += copy;\n\n\t\tif (len >= probe_size)\n\t\t\tbreak;\n\t}\n\ttcp_init_tso_segs(nskb, nskb->len);\n\n\t \n\tif (!tcp_transmit_skb(sk, nskb, 1, GFP_ATOMIC)) {\n\t\t \n\t\ttcp_snd_cwnd_set(tp, tcp_snd_cwnd(tp) - 1);\n\t\ttcp_event_new_data_sent(sk, nskb);\n\n\t\ticsk->icsk_mtup.probe_size = tcp_mss_to_mtu(sk, nskb->len);\n\t\ttp->mtu_probe.probe_seq_start = TCP_SKB_CB(nskb)->seq;\n\t\ttp->mtu_probe.probe_seq_end = TCP_SKB_CB(nskb)->end_seq;\n\n\t\treturn 1;\n\t}\n\n\treturn -1;\n}\n\nstatic bool tcp_pacing_check(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (!tcp_needs_internal_pacing(sk))\n\t\treturn false;\n\n\tif (tp->tcp_wstamp_ns <= tp->tcp_clock_cache)\n\t\treturn false;\n\n\tif (!hrtimer_is_queued(&tp->pacing_timer)) {\n\t\thrtimer_start(&tp->pacing_timer,\n\t\t\t      ns_to_ktime(tp->tcp_wstamp_ns),\n\t\t\t      HRTIMER_MODE_ABS_PINNED_SOFT);\n\t\tsock_hold(sk);\n\t}\n\treturn true;\n}\n\nstatic bool tcp_rtx_queue_empty_or_single_skb(const struct sock *sk)\n{\n\tconst struct rb_node *node = sk->tcp_rtx_queue.rb_node;\n\n\t \n\tif (!node)\n\t\treturn true;\n\n\t \n\treturn !node->rb_left && !node->rb_right;\n}\n\n \nstatic bool tcp_small_queue_check(struct sock *sk, const struct sk_buff *skb,\n\t\t\t\t  unsigned int factor)\n{\n\tunsigned long limit;\n\n\tlimit = max_t(unsigned long,\n\t\t      2 * skb->truesize,\n\t\t      sk->sk_pacing_rate >> READ_ONCE(sk->sk_pacing_shift));\n\tif (sk->sk_pacing_status == SK_PACING_NONE)\n\t\tlimit = min_t(unsigned long, limit,\n\t\t\t      READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_limit_output_bytes));\n\tlimit <<= factor;\n\n\tif (static_branch_unlikely(&tcp_tx_delay_enabled) &&\n\t    tcp_sk(sk)->tcp_tx_delay) {\n\t\tu64 extra_bytes = (u64)sk->sk_pacing_rate * tcp_sk(sk)->tcp_tx_delay;\n\n\t\t \n\t\textra_bytes >>= (20 - 1);\n\t\tlimit += extra_bytes;\n\t}\n\tif (refcount_read(&sk->sk_wmem_alloc) > limit) {\n\t\t \n\t\tif (tcp_rtx_queue_empty_or_single_skb(sk))\n\t\t\treturn false;\n\n\t\tset_bit(TSQ_THROTTLED, &sk->sk_tsq_flags);\n\t\t \n\t\tsmp_mb__after_atomic();\n\t\tif (refcount_read(&sk->sk_wmem_alloc) > limit)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic void tcp_chrono_set(struct tcp_sock *tp, const enum tcp_chrono new)\n{\n\tconst u32 now = tcp_jiffies32;\n\tenum tcp_chrono old = tp->chrono_type;\n\n\tif (old > TCP_CHRONO_UNSPEC)\n\t\ttp->chrono_stat[old - 1] += now - tp->chrono_start;\n\ttp->chrono_start = now;\n\ttp->chrono_type = new;\n}\n\nvoid tcp_chrono_start(struct sock *sk, const enum tcp_chrono type)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t \n\tif (type > tp->chrono_type)\n\t\ttcp_chrono_set(tp, type);\n}\n\nvoid tcp_chrono_stop(struct sock *sk, const enum tcp_chrono type)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\n\t \n\tif (tcp_rtx_and_write_queues_empty(sk))\n\t\ttcp_chrono_set(tp, TCP_CHRONO_UNSPEC);\n\telse if (type == tp->chrono_type)\n\t\ttcp_chrono_set(tp, TCP_CHRONO_BUSY);\n}\n\n \nstatic bool tcp_write_xmit(struct sock *sk, unsigned int mss_now, int nonagle,\n\t\t\t   int push_one, gfp_t gfp)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb;\n\tunsigned int tso_segs, sent_pkts;\n\tint cwnd_quota;\n\tint result;\n\tbool is_cwnd_limited = false, is_rwnd_limited = false;\n\tu32 max_segs;\n\n\tsent_pkts = 0;\n\n\ttcp_mstamp_refresh(tp);\n\tif (!push_one) {\n\t\t \n\t\tresult = tcp_mtu_probe(sk);\n\t\tif (!result) {\n\t\t\treturn false;\n\t\t} else if (result > 0) {\n\t\t\tsent_pkts = 1;\n\t\t}\n\t}\n\n\tmax_segs = tcp_tso_segs(sk, mss_now);\n\twhile ((skb = tcp_send_head(sk))) {\n\t\tunsigned int limit;\n\n\t\tif (unlikely(tp->repair) && tp->repair_queue == TCP_SEND_QUEUE) {\n\t\t\t \n\t\t\ttp->tcp_wstamp_ns = tp->tcp_clock_cache;\n\t\t\tskb_set_delivery_time(skb, tp->tcp_wstamp_ns, true);\n\t\t\tlist_move_tail(&skb->tcp_tsorted_anchor, &tp->tsorted_sent_queue);\n\t\t\ttcp_init_tso_segs(skb, mss_now);\n\t\t\tgoto repair;  \n\t\t}\n\n\t\tif (tcp_pacing_check(sk))\n\t\t\tbreak;\n\n\t\ttso_segs = tcp_init_tso_segs(skb, mss_now);\n\t\tBUG_ON(!tso_segs);\n\n\t\tcwnd_quota = tcp_cwnd_test(tp, skb);\n\t\tif (!cwnd_quota) {\n\t\t\tif (push_one == 2)\n\t\t\t\t \n\t\t\t\tcwnd_quota = 1;\n\t\t\telse\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (unlikely(!tcp_snd_wnd_test(tp, skb, mss_now))) {\n\t\t\tis_rwnd_limited = true;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (tso_segs == 1) {\n\t\t\tif (unlikely(!tcp_nagle_test(tp, skb, mss_now,\n\t\t\t\t\t\t     (tcp_skb_is_last(sk, skb) ?\n\t\t\t\t\t\t      nonagle : TCP_NAGLE_PUSH))))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tif (!push_one &&\n\t\t\t    tcp_tso_should_defer(sk, skb, &is_cwnd_limited,\n\t\t\t\t\t\t &is_rwnd_limited, max_segs))\n\t\t\t\tbreak;\n\t\t}\n\n\t\tlimit = mss_now;\n\t\tif (tso_segs > 1 && !tcp_urg_mode(tp))\n\t\t\tlimit = tcp_mss_split_point(sk, skb, mss_now,\n\t\t\t\t\t\t    min_t(unsigned int,\n\t\t\t\t\t\t\t  cwnd_quota,\n\t\t\t\t\t\t\t  max_segs),\n\t\t\t\t\t\t    nonagle);\n\n\t\tif (skb->len > limit &&\n\t\t    unlikely(tso_fragment(sk, skb, limit, mss_now, gfp)))\n\t\t\tbreak;\n\n\t\tif (tcp_small_queue_check(sk, skb, 0))\n\t\t\tbreak;\n\n\t\t \n\t\tif (TCP_SKB_CB(skb)->end_seq == TCP_SKB_CB(skb)->seq)\n\t\t\tbreak;\n\n\t\tif (unlikely(tcp_transmit_skb(sk, skb, 1, gfp)))\n\t\t\tbreak;\n\nrepair:\n\t\t \n\t\ttcp_event_new_data_sent(sk, skb);\n\n\t\ttcp_minshall_update(tp, mss_now, skb);\n\t\tsent_pkts += tcp_skb_pcount(skb);\n\n\t\tif (push_one)\n\t\t\tbreak;\n\t}\n\n\tif (is_rwnd_limited)\n\t\ttcp_chrono_start(sk, TCP_CHRONO_RWND_LIMITED);\n\telse\n\t\ttcp_chrono_stop(sk, TCP_CHRONO_RWND_LIMITED);\n\n\tis_cwnd_limited |= (tcp_packets_in_flight(tp) >= tcp_snd_cwnd(tp));\n\tif (likely(sent_pkts || is_cwnd_limited))\n\t\ttcp_cwnd_validate(sk, is_cwnd_limited);\n\n\tif (likely(sent_pkts)) {\n\t\tif (tcp_in_cwnd_reduction(sk))\n\t\t\ttp->prr_out += sent_pkts;\n\n\t\t \n\t\tif (push_one != 2)\n\t\t\ttcp_schedule_loss_probe(sk, false);\n\t\treturn false;\n\t}\n\treturn !tp->packets_out && !tcp_write_queue_empty(sk);\n}\n\nbool tcp_schedule_loss_probe(struct sock *sk, bool advancing_rto)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 timeout, timeout_us, rto_delta_us;\n\tint early_retrans;\n\n\t \n\tif (rcu_access_pointer(tp->fastopen_rsk))\n\t\treturn false;\n\n\tearly_retrans = READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_early_retrans);\n\t \n\tif ((early_retrans != 3 && early_retrans != 4) ||\n\t    !tp->packets_out || !tcp_is_sack(tp) ||\n\t    (icsk->icsk_ca_state != TCP_CA_Open &&\n\t     icsk->icsk_ca_state != TCP_CA_CWR))\n\t\treturn false;\n\n\t \n\tif (tp->srtt_us) {\n\t\ttimeout_us = tp->srtt_us >> 2;\n\t\tif (tp->packets_out == 1)\n\t\t\ttimeout_us += tcp_rto_min_us(sk);\n\t\telse\n\t\t\ttimeout_us += TCP_TIMEOUT_MIN_US;\n\t\ttimeout = usecs_to_jiffies(timeout_us);\n\t} else {\n\t\ttimeout = TCP_TIMEOUT_INIT;\n\t}\n\n\t \n\trto_delta_us = advancing_rto ?\n\t\t\tjiffies_to_usecs(inet_csk(sk)->icsk_rto) :\n\t\t\ttcp_rto_delta_us(sk);   \n\tif (rto_delta_us > 0)\n\t\ttimeout = min_t(u32, timeout, usecs_to_jiffies(rto_delta_us));\n\n\ttcp_reset_xmit_timer(sk, ICSK_TIME_LOSS_PROBE, timeout, TCP_RTO_MAX);\n\treturn true;\n}\n\n \nstatic bool skb_still_in_host_queue(struct sock *sk,\n\t\t\t\t    const struct sk_buff *skb)\n{\n\tif (unlikely(skb_fclone_busy(sk, skb))) {\n\t\tset_bit(TSQ_THROTTLED, &sk->sk_tsq_flags);\n\t\tsmp_mb__after_atomic();\n\t\tif (skb_fclone_busy(sk, skb)) {\n\t\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\t      LINUX_MIB_TCPSPURIOUS_RTX_HOSTQUEUES);\n\t\t\treturn true;\n\t\t}\n\t}\n\treturn false;\n}\n\n \nvoid tcp_send_loss_probe(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb;\n\tint pcount;\n\tint mss = tcp_current_mss(sk);\n\n\t \n\tif (tp->tlp_high_seq)\n\t\tgoto rearm_timer;\n\n\ttp->tlp_retrans = 0;\n\tskb = tcp_send_head(sk);\n\tif (skb && tcp_snd_wnd_test(tp, skb, mss)) {\n\t\tpcount = tp->packets_out;\n\t\ttcp_write_xmit(sk, mss, TCP_NAGLE_OFF, 2, GFP_ATOMIC);\n\t\tif (tp->packets_out > pcount)\n\t\t\tgoto probe_sent;\n\t\tgoto rearm_timer;\n\t}\n\tskb = skb_rb_last(&sk->tcp_rtx_queue);\n\tif (unlikely(!skb)) {\n\t\tWARN_ONCE(tp->packets_out,\n\t\t\t  \"invalid inflight: %u state %u cwnd %u mss %d\\n\",\n\t\t\t  tp->packets_out, sk->sk_state, tcp_snd_cwnd(tp), mss);\n\t\tinet_csk(sk)->icsk_pending = 0;\n\t\treturn;\n\t}\n\n\tif (skb_still_in_host_queue(sk, skb))\n\t\tgoto rearm_timer;\n\n\tpcount = tcp_skb_pcount(skb);\n\tif (WARN_ON(!pcount))\n\t\tgoto rearm_timer;\n\n\tif ((pcount > 1) && (skb->len > (pcount - 1) * mss)) {\n\t\tif (unlikely(tcp_fragment(sk, TCP_FRAG_IN_RTX_QUEUE, skb,\n\t\t\t\t\t  (pcount - 1) * mss, mss,\n\t\t\t\t\t  GFP_ATOMIC)))\n\t\t\tgoto rearm_timer;\n\t\tskb = skb_rb_next(skb);\n\t}\n\n\tif (WARN_ON(!skb || !tcp_skb_pcount(skb)))\n\t\tgoto rearm_timer;\n\n\tif (__tcp_retransmit_skb(sk, skb, 1))\n\t\tgoto rearm_timer;\n\n\ttp->tlp_retrans = 1;\n\nprobe_sent:\n\t \n\ttp->tlp_high_seq = tp->snd_nxt;\n\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPLOSSPROBES);\n\t \n\tinet_csk(sk)->icsk_pending = 0;\nrearm_timer:\n\ttcp_rearm_rto(sk);\n}\n\n \nvoid __tcp_push_pending_frames(struct sock *sk, unsigned int cur_mss,\n\t\t\t       int nonagle)\n{\n\t \n\tif (unlikely(sk->sk_state == TCP_CLOSE))\n\t\treturn;\n\n\tif (tcp_write_xmit(sk, cur_mss, nonagle, 0,\n\t\t\t   sk_gfp_mask(sk, GFP_ATOMIC)))\n\t\ttcp_check_probe_timer(sk);\n}\n\n \nvoid tcp_push_one(struct sock *sk, unsigned int mss_now)\n{\n\tstruct sk_buff *skb = tcp_send_head(sk);\n\n\tBUG_ON(!skb || skb->len < mss_now);\n\n\ttcp_write_xmit(sk, mss_now, TCP_NAGLE_PUSH, 1, sk->sk_allocation);\n}\n\n \nu32 __tcp_select_window(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct net *net = sock_net(sk);\n\t \n\tint mss = icsk->icsk_ack.rcv_mss;\n\tint free_space = tcp_space(sk);\n\tint allowed_space = tcp_full_space(sk);\n\tint full_space, window;\n\n\tif (sk_is_mptcp(sk))\n\t\tmptcp_space(sk, &free_space, &allowed_space);\n\n\tfull_space = min_t(int, tp->window_clamp, allowed_space);\n\n\tif (unlikely(mss > full_space)) {\n\t\tmss = full_space;\n\t\tif (mss <= 0)\n\t\t\treturn 0;\n\t}\n\n\t \n\tif (READ_ONCE(net->ipv4.sysctl_tcp_shrink_window) && tp->rx_opt.rcv_wscale)\n\t\tgoto shrink_window_allowed;\n\n\t \n\n\tif (free_space < (full_space >> 1)) {\n\t\ticsk->icsk_ack.quick = 0;\n\n\t\tif (tcp_under_memory_pressure(sk))\n\t\t\ttcp_adjust_rcv_ssthresh(sk);\n\n\t\t \n\t\tfree_space = round_down(free_space, 1 << tp->rx_opt.rcv_wscale);\n\n\t\t \n\t\tif (free_space < (allowed_space >> 4) || free_space < mss)\n\t\t\treturn 0;\n\t}\n\n\tif (free_space > tp->rcv_ssthresh)\n\t\tfree_space = tp->rcv_ssthresh;\n\n\t \n\tif (tp->rx_opt.rcv_wscale) {\n\t\twindow = free_space;\n\n\t\t \n\t\twindow = ALIGN(window, (1 << tp->rx_opt.rcv_wscale));\n\t} else {\n\t\twindow = tp->rcv_wnd;\n\t\t \n\t\tif (window <= free_space - mss || window > free_space)\n\t\t\twindow = rounddown(free_space, mss);\n\t\telse if (mss == full_space &&\n\t\t\t free_space > window + (full_space >> 1))\n\t\t\twindow = free_space;\n\t}\n\n\treturn window;\n\nshrink_window_allowed:\n\t \n\tfree_space = round_down(free_space, 1 << tp->rx_opt.rcv_wscale);\n\n\tif (free_space < (full_space >> 1)) {\n\t\ticsk->icsk_ack.quick = 0;\n\n\t\tif (tcp_under_memory_pressure(sk))\n\t\t\ttcp_adjust_rcv_ssthresh(sk);\n\n\t\t \n\t\tif (free_space < (allowed_space >> 4) || free_space < mss ||\n\t\t\tfree_space < (1 << tp->rx_opt.rcv_wscale))\n\t\t\treturn 0;\n\t}\n\n\tif (free_space > tp->rcv_ssthresh) {\n\t\tfree_space = tp->rcv_ssthresh;\n\t\t \n\t\tfree_space = ALIGN(free_space, (1 << tp->rx_opt.rcv_wscale));\n\t}\n\n\treturn free_space;\n}\n\nvoid tcp_skb_collapse_tstamp(struct sk_buff *skb,\n\t\t\t     const struct sk_buff *next_skb)\n{\n\tif (unlikely(tcp_has_tx_tstamp(next_skb))) {\n\t\tconst struct skb_shared_info *next_shinfo =\n\t\t\tskb_shinfo(next_skb);\n\t\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\n\t\tshinfo->tx_flags |= next_shinfo->tx_flags & SKBTX_ANY_TSTAMP;\n\t\tshinfo->tskey = next_shinfo->tskey;\n\t\tTCP_SKB_CB(skb)->txstamp_ack |=\n\t\t\tTCP_SKB_CB(next_skb)->txstamp_ack;\n\t}\n}\n\n \nstatic bool tcp_collapse_retrans(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *next_skb = skb_rb_next(skb);\n\tint next_skb_size;\n\n\tnext_skb_size = next_skb->len;\n\n\tBUG_ON(tcp_skb_pcount(skb) != 1 || tcp_skb_pcount(next_skb) != 1);\n\n\tif (next_skb_size && !tcp_skb_shift(skb, next_skb, 1, next_skb_size))\n\t\treturn false;\n\n\ttcp_highest_sack_replace(sk, next_skb, skb);\n\n\t \n\tTCP_SKB_CB(skb)->end_seq = TCP_SKB_CB(next_skb)->end_seq;\n\n\t \n\tTCP_SKB_CB(skb)->tcp_flags |= TCP_SKB_CB(next_skb)->tcp_flags;\n\n\t \n\tTCP_SKB_CB(skb)->sacked |= TCP_SKB_CB(next_skb)->sacked & TCPCB_EVER_RETRANS;\n\tTCP_SKB_CB(skb)->eor = TCP_SKB_CB(next_skb)->eor;\n\n\t \n\ttcp_clear_retrans_hints_partial(tp);\n\tif (next_skb == tp->retransmit_skb_hint)\n\t\ttp->retransmit_skb_hint = skb;\n\n\ttcp_adjust_pcount(sk, next_skb, tcp_skb_pcount(next_skb));\n\n\ttcp_skb_collapse_tstamp(skb, next_skb);\n\n\ttcp_rtx_queue_unlink_and_free(next_skb, sk);\n\treturn true;\n}\n\n \nstatic bool tcp_can_collapse(const struct sock *sk, const struct sk_buff *skb)\n{\n\tif (tcp_skb_pcount(skb) > 1)\n\t\treturn false;\n\tif (skb_cloned(skb))\n\t\treturn false;\n\t \n\tif (TCP_SKB_CB(skb)->sacked & TCPCB_SACKED_ACKED)\n\t\treturn false;\n\n\treturn true;\n}\n\n \nstatic void tcp_retrans_try_collapse(struct sock *sk, struct sk_buff *to,\n\t\t\t\t     int space)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb = to, *tmp;\n\tbool first = true;\n\n\tif (!READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_retrans_collapse))\n\t\treturn;\n\tif (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_SYN)\n\t\treturn;\n\n\tskb_rbtree_walk_from_safe(skb, tmp) {\n\t\tif (!tcp_can_collapse(sk, skb))\n\t\t\tbreak;\n\n\t\tif (!tcp_skb_can_collapse(to, skb))\n\t\t\tbreak;\n\n\t\tspace -= skb->len;\n\n\t\tif (first) {\n\t\t\tfirst = false;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (space < 0)\n\t\t\tbreak;\n\n\t\tif (after(TCP_SKB_CB(skb)->end_seq, tcp_wnd_end(tp)))\n\t\t\tbreak;\n\n\t\tif (!tcp_collapse_retrans(sk, to))\n\t\t\tbreak;\n\t}\n}\n\n \nint __tcp_retransmit_skb(struct sock *sk, struct sk_buff *skb, int segs)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tunsigned int cur_mss;\n\tint diff, len, err;\n\tint avail_wnd;\n\n\t \n\tif (icsk->icsk_mtup.probe_size)\n\t\ticsk->icsk_mtup.probe_size = 0;\n\n\tif (skb_still_in_host_queue(sk, skb))\n\t\treturn -EBUSY;\n\nstart:\n\tif (before(TCP_SKB_CB(skb)->seq, tp->snd_una)) {\n\t\tif (unlikely(TCP_SKB_CB(skb)->tcp_flags & TCPHDR_SYN)) {\n\t\t\tTCP_SKB_CB(skb)->tcp_flags &= ~TCPHDR_SYN;\n\t\t\tTCP_SKB_CB(skb)->seq++;\n\t\t\tgoto start;\n\t\t}\n\t\tif (unlikely(before(TCP_SKB_CB(skb)->end_seq, tp->snd_una))) {\n\t\t\tWARN_ON_ONCE(1);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (tcp_trim_head(sk, skb, tp->snd_una - TCP_SKB_CB(skb)->seq))\n\t\t\treturn -ENOMEM;\n\t}\n\n\tif (inet_csk(sk)->icsk_af_ops->rebuild_header(sk))\n\t\treturn -EHOSTUNREACH;  \n\n\tcur_mss = tcp_current_mss(sk);\n\tavail_wnd = tcp_wnd_end(tp) - TCP_SKB_CB(skb)->seq;\n\n\t \n\tif (avail_wnd <= 0) {\n\t\tif (TCP_SKB_CB(skb)->seq != tp->snd_una)\n\t\t\treturn -EAGAIN;\n\t\tavail_wnd = cur_mss;\n\t}\n\n\tlen = cur_mss * segs;\n\tif (len > avail_wnd) {\n\t\tlen = rounddown(avail_wnd, cur_mss);\n\t\tif (!len)\n\t\t\tlen = avail_wnd;\n\t}\n\tif (skb->len > len) {\n\t\tif (tcp_fragment(sk, TCP_FRAG_IN_RTX_QUEUE, skb, len,\n\t\t\t\t cur_mss, GFP_ATOMIC))\n\t\t\treturn -ENOMEM;  \n\t} else {\n\t\tif (skb_unclone_keeptruesize(skb, GFP_ATOMIC))\n\t\t\treturn -ENOMEM;\n\n\t\tdiff = tcp_skb_pcount(skb);\n\t\ttcp_set_skb_tso_segs(skb, cur_mss);\n\t\tdiff -= tcp_skb_pcount(skb);\n\t\tif (diff)\n\t\t\ttcp_adjust_pcount(sk, skb, diff);\n\t\tavail_wnd = min_t(int, avail_wnd, cur_mss);\n\t\tif (skb->len < avail_wnd)\n\t\t\ttcp_retrans_try_collapse(sk, skb, avail_wnd);\n\t}\n\n\t \n\tif ((TCP_SKB_CB(skb)->tcp_flags & TCPHDR_SYN_ECN) == TCPHDR_SYN_ECN)\n\t\ttcp_ecn_clear_syn(sk, skb);\n\n\t \n\tsegs = tcp_skb_pcount(skb);\n\tTCP_ADD_STATS(sock_net(sk), TCP_MIB_RETRANSSEGS, segs);\n\tif (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_SYN)\n\t\t__NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPSYNRETRANS);\n\ttp->total_retrans += segs;\n\ttp->bytes_retrans += skb->len;\n\n\t \n\tif (unlikely((NET_IP_ALIGN && ((unsigned long)skb->data & 3)) ||\n\t\t     skb_headroom(skb) >= 0xFFFF)) {\n\t\tstruct sk_buff *nskb;\n\n\t\ttcp_skb_tsorted_save(skb) {\n\t\t\tnskb = __pskb_copy(skb, MAX_TCP_HEADER, GFP_ATOMIC);\n\t\t\tif (nskb) {\n\t\t\t\tnskb->dev = NULL;\n\t\t\t\terr = tcp_transmit_skb(sk, nskb, 0, GFP_ATOMIC);\n\t\t\t} else {\n\t\t\t\terr = -ENOBUFS;\n\t\t\t}\n\t\t} tcp_skb_tsorted_restore(skb);\n\n\t\tif (!err) {\n\t\t\ttcp_update_skb_after_send(sk, skb, tp->tcp_wstamp_ns);\n\t\t\ttcp_rate_skb_sent(sk, skb);\n\t\t}\n\t} else {\n\t\terr = tcp_transmit_skb(sk, skb, 1, GFP_ATOMIC);\n\t}\n\n\t \n\tTCP_SKB_CB(skb)->sacked |= TCPCB_EVER_RETRANS;\n\n\tif (BPF_SOCK_OPS_TEST_FLAG(tp, BPF_SOCK_OPS_RETRANS_CB_FLAG))\n\t\ttcp_call_bpf_3arg(sk, BPF_SOCK_OPS_RETRANS_CB,\n\t\t\t\t  TCP_SKB_CB(skb)->seq, segs, err);\n\n\tif (likely(!err)) {\n\t\ttrace_tcp_retransmit_skb(sk, skb);\n\t} else if (err != -EBUSY) {\n\t\tNET_ADD_STATS(sock_net(sk), LINUX_MIB_TCPRETRANSFAIL, segs);\n\t}\n\treturn err;\n}\n\nint tcp_retransmit_skb(struct sock *sk, struct sk_buff *skb, int segs)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint err = __tcp_retransmit_skb(sk, skb, segs);\n\n\tif (err == 0) {\n#if FASTRETRANS_DEBUG > 0\n\t\tif (TCP_SKB_CB(skb)->sacked & TCPCB_SACKED_RETRANS) {\n\t\t\tnet_dbg_ratelimited(\"retrans_out leaked\\n\");\n\t\t}\n#endif\n\t\tTCP_SKB_CB(skb)->sacked |= TCPCB_RETRANS;\n\t\ttp->retrans_out += tcp_skb_pcount(skb);\n\t}\n\n\t \n\tif (!tp->retrans_stamp)\n\t\ttp->retrans_stamp = tcp_skb_timestamp(skb);\n\n\tif (tp->undo_retrans < 0)\n\t\ttp->undo_retrans = 0;\n\ttp->undo_retrans += tcp_skb_pcount(skb);\n\treturn err;\n}\n\n \nvoid tcp_xmit_retransmit_queue(struct sock *sk)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct sk_buff *skb, *rtx_head, *hole = NULL;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tbool rearm_timer = false;\n\tu32 max_segs;\n\tint mib_idx;\n\n\tif (!tp->packets_out)\n\t\treturn;\n\n\trtx_head = tcp_rtx_queue_head(sk);\n\tskb = tp->retransmit_skb_hint ?: rtx_head;\n\tmax_segs = tcp_tso_segs(sk, tcp_current_mss(sk));\n\tskb_rbtree_walk_from(skb) {\n\t\t__u8 sacked;\n\t\tint segs;\n\n\t\tif (tcp_pacing_check(sk))\n\t\t\tbreak;\n\n\t\t \n\t\tif (!hole)\n\t\t\ttp->retransmit_skb_hint = skb;\n\n\t\tsegs = tcp_snd_cwnd(tp) - tcp_packets_in_flight(tp);\n\t\tif (segs <= 0)\n\t\t\tbreak;\n\t\tsacked = TCP_SKB_CB(skb)->sacked;\n\t\t \n\t\tsegs = min_t(int, segs, max_segs);\n\n\t\tif (tp->retrans_out >= tp->lost_out) {\n\t\t\tbreak;\n\t\t} else if (!(sacked & TCPCB_LOST)) {\n\t\t\tif (!hole && !(sacked & (TCPCB_SACKED_RETRANS|TCPCB_SACKED_ACKED)))\n\t\t\t\thole = skb;\n\t\t\tcontinue;\n\n\t\t} else {\n\t\t\tif (icsk->icsk_ca_state != TCP_CA_Loss)\n\t\t\t\tmib_idx = LINUX_MIB_TCPFASTRETRANS;\n\t\t\telse\n\t\t\t\tmib_idx = LINUX_MIB_TCPSLOWSTARTRETRANS;\n\t\t}\n\n\t\tif (sacked & (TCPCB_SACKED_ACKED|TCPCB_SACKED_RETRANS))\n\t\t\tcontinue;\n\n\t\tif (tcp_small_queue_check(sk, skb, 1))\n\t\t\tbreak;\n\n\t\tif (tcp_retransmit_skb(sk, skb, segs))\n\t\t\tbreak;\n\n\t\tNET_ADD_STATS(sock_net(sk), mib_idx, tcp_skb_pcount(skb));\n\n\t\tif (tcp_in_cwnd_reduction(sk))\n\t\t\ttp->prr_out += tcp_skb_pcount(skb);\n\n\t\tif (skb == rtx_head &&\n\t\t    icsk->icsk_pending != ICSK_TIME_REO_TIMEOUT)\n\t\t\trearm_timer = true;\n\n\t}\n\tif (rearm_timer)\n\t\ttcp_reset_xmit_timer(sk, ICSK_TIME_RETRANS,\n\t\t\t\t     inet_csk(sk)->icsk_rto,\n\t\t\t\t     TCP_RTO_MAX);\n}\n\n \nvoid sk_forced_mem_schedule(struct sock *sk, int size)\n{\n\tint delta, amt;\n\n\tdelta = size - sk->sk_forward_alloc;\n\tif (delta <= 0)\n\t\treturn;\n\tamt = sk_mem_pages(delta);\n\tsk_forward_alloc_add(sk, amt << PAGE_SHIFT);\n\tsk_memory_allocated_add(sk, amt);\n\n\tif (mem_cgroup_sockets_enabled && sk->sk_memcg)\n\t\tmem_cgroup_charge_skmem(sk->sk_memcg, amt,\n\t\t\t\t\tgfp_memcg_charge() | __GFP_NOFAIL);\n}\n\n \nvoid tcp_send_fin(struct sock *sk)\n{\n\tstruct sk_buff *skb, *tskb, *tail = tcp_write_queue_tail(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t \n\ttskb = tail;\n\tif (!tskb && tcp_under_memory_pressure(sk))\n\t\ttskb = skb_rb_last(&sk->tcp_rtx_queue);\n\n\tif (tskb) {\n\t\tTCP_SKB_CB(tskb)->tcp_flags |= TCPHDR_FIN;\n\t\tTCP_SKB_CB(tskb)->end_seq++;\n\t\ttp->write_seq++;\n\t\tif (!tail) {\n\t\t\t \n\t\t\tWRITE_ONCE(tp->snd_nxt, tp->snd_nxt + 1);\n\t\t\treturn;\n\t\t}\n\t} else {\n\t\tskb = alloc_skb_fclone(MAX_TCP_HEADER, sk->sk_allocation);\n\t\tif (unlikely(!skb))\n\t\t\treturn;\n\n\t\tINIT_LIST_HEAD(&skb->tcp_tsorted_anchor);\n\t\tskb_reserve(skb, MAX_TCP_HEADER);\n\t\tsk_forced_mem_schedule(sk, skb->truesize);\n\t\t \n\t\ttcp_init_nondata_skb(skb, tp->write_seq,\n\t\t\t\t     TCPHDR_ACK | TCPHDR_FIN);\n\t\ttcp_queue_skb(sk, skb);\n\t}\n\t__tcp_push_pending_frames(sk, tcp_current_mss(sk), TCP_NAGLE_OFF);\n}\n\n \nvoid tcp_send_active_reset(struct sock *sk, gfp_t priority)\n{\n\tstruct sk_buff *skb;\n\n\tTCP_INC_STATS(sock_net(sk), TCP_MIB_OUTRSTS);\n\n\t \n\tskb = alloc_skb(MAX_TCP_HEADER, priority);\n\tif (!skb) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTFAILED);\n\t\treturn;\n\t}\n\n\t \n\tskb_reserve(skb, MAX_TCP_HEADER);\n\ttcp_init_nondata_skb(skb, tcp_acceptable_seq(sk),\n\t\t\t     TCPHDR_ACK | TCPHDR_RST);\n\ttcp_mstamp_refresh(tcp_sk(sk));\n\t \n\tif (tcp_transmit_skb(sk, skb, 0, priority))\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTFAILED);\n\n\t \n\ttrace_tcp_send_reset(sk, NULL);\n}\n\n \nint tcp_send_synack(struct sock *sk)\n{\n\tstruct sk_buff *skb;\n\n\tskb = tcp_rtx_queue_head(sk);\n\tif (!skb || !(TCP_SKB_CB(skb)->tcp_flags & TCPHDR_SYN)) {\n\t\tpr_err(\"%s: wrong queue state\\n\", __func__);\n\t\treturn -EFAULT;\n\t}\n\tif (!(TCP_SKB_CB(skb)->tcp_flags & TCPHDR_ACK)) {\n\t\tif (skb_cloned(skb)) {\n\t\t\tstruct sk_buff *nskb;\n\n\t\t\ttcp_skb_tsorted_save(skb) {\n\t\t\t\tnskb = skb_copy(skb, GFP_ATOMIC);\n\t\t\t} tcp_skb_tsorted_restore(skb);\n\t\t\tif (!nskb)\n\t\t\t\treturn -ENOMEM;\n\t\t\tINIT_LIST_HEAD(&nskb->tcp_tsorted_anchor);\n\t\t\ttcp_highest_sack_replace(sk, skb, nskb);\n\t\t\ttcp_rtx_queue_unlink_and_free(skb, sk);\n\t\t\t__skb_header_release(nskb);\n\t\t\ttcp_rbtree_insert(&sk->tcp_rtx_queue, nskb);\n\t\t\tsk_wmem_queued_add(sk, nskb->truesize);\n\t\t\tsk_mem_charge(sk, nskb->truesize);\n\t\t\tskb = nskb;\n\t\t}\n\n\t\tTCP_SKB_CB(skb)->tcp_flags |= TCPHDR_ACK;\n\t\ttcp_ecn_send_synack(sk, skb);\n\t}\n\treturn tcp_transmit_skb(sk, skb, 1, GFP_ATOMIC);\n}\n\n \nstruct sk_buff *tcp_make_synack(const struct sock *sk, struct dst_entry *dst,\n\t\t\t\tstruct request_sock *req,\n\t\t\t\tstruct tcp_fastopen_cookie *foc,\n\t\t\t\tenum tcp_synack_type synack_type,\n\t\t\t\tstruct sk_buff *syn_skb)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tstruct tcp_md5sig_key *md5 = NULL;\n\tstruct tcp_out_options opts;\n\tstruct sk_buff *skb;\n\tint tcp_header_size;\n\tstruct tcphdr *th;\n\tint mss;\n\tu64 now;\n\n\tskb = alloc_skb(MAX_TCP_HEADER, GFP_ATOMIC);\n\tif (unlikely(!skb)) {\n\t\tdst_release(dst);\n\t\treturn NULL;\n\t}\n\t \n\tskb_reserve(skb, MAX_TCP_HEADER);\n\n\tswitch (synack_type) {\n\tcase TCP_SYNACK_NORMAL:\n\t\tskb_set_owner_w(skb, req_to_sk(req));\n\t\tbreak;\n\tcase TCP_SYNACK_COOKIE:\n\t\t \n\t\tbreak;\n\tcase TCP_SYNACK_FASTOPEN:\n\t\t \n\t\tskb_set_owner_w(skb, (struct sock *)sk);\n\t\tbreak;\n\t}\n\tskb_dst_set(skb, dst);\n\n\tmss = tcp_mss_clamp(tp, dst_metric_advmss(dst));\n\n\tmemset(&opts, 0, sizeof(opts));\n\tnow = tcp_clock_ns();\n#ifdef CONFIG_SYN_COOKIES\n\tif (unlikely(synack_type == TCP_SYNACK_COOKIE && ireq->tstamp_ok))\n\t\tskb_set_delivery_time(skb, cookie_init_timestamp(req, now),\n\t\t\t\t      true);\n\telse\n#endif\n\t{\n\t\tskb_set_delivery_time(skb, now, true);\n\t\tif (!tcp_rsk(req)->snt_synack)  \n\t\t\ttcp_rsk(req)->snt_synack = tcp_skb_timestamp_us(skb);\n\t}\n\n#ifdef CONFIG_TCP_MD5SIG\n\trcu_read_lock();\n\tmd5 = tcp_rsk(req)->af_specific->req_md5_lookup(sk, req_to_sk(req));\n#endif\n\tskb_set_hash(skb, READ_ONCE(tcp_rsk(req)->txhash), PKT_HASH_TYPE_L4);\n\t \n\tTCP_SKB_CB(skb)->tcp_flags = TCPHDR_SYN | TCPHDR_ACK;\n\ttcp_header_size = tcp_synack_options(sk, req, mss, skb, &opts, md5,\n\t\t\t\t\t     foc, synack_type,\n\t\t\t\t\t     syn_skb) + sizeof(*th);\n\n\tskb_push(skb, tcp_header_size);\n\tskb_reset_transport_header(skb);\n\n\tth = (struct tcphdr *)skb->data;\n\tmemset(th, 0, sizeof(struct tcphdr));\n\tth->syn = 1;\n\tth->ack = 1;\n\ttcp_ecn_make_synack(req, th);\n\tth->source = htons(ireq->ir_num);\n\tth->dest = ireq->ir_rmt_port;\n\tskb->mark = ireq->ir_mark;\n\tskb->ip_summed = CHECKSUM_PARTIAL;\n\tth->seq = htonl(tcp_rsk(req)->snt_isn);\n\t \n\tth->ack_seq = htonl(tcp_rsk(req)->rcv_nxt);\n\n\t \n\tth->window = htons(min(req->rsk_rcv_wnd, 65535U));\n\ttcp_options_write(th, NULL, &opts);\n\tth->doff = (tcp_header_size >> 2);\n\tTCP_INC_STATS(sock_net(sk), TCP_MIB_OUTSEGS);\n\n#ifdef CONFIG_TCP_MD5SIG\n\t \n\tif (md5)\n\t\ttcp_rsk(req)->af_specific->calc_md5_hash(opts.hash_location,\n\t\t\t\t\t       md5, req_to_sk(req), skb);\n\trcu_read_unlock();\n#endif\n\n\tbpf_skops_write_hdr_opt((struct sock *)sk, skb, req, syn_skb,\n\t\t\t\tsynack_type, &opts);\n\n\tskb_set_delivery_time(skb, now, true);\n\ttcp_add_tx_delay(skb, tp);\n\n\treturn skb;\n}\nEXPORT_SYMBOL(tcp_make_synack);\n\nstatic void tcp_ca_dst_init(struct sock *sk, const struct dst_entry *dst)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tconst struct tcp_congestion_ops *ca;\n\tu32 ca_key = dst_metric(dst, RTAX_CC_ALGO);\n\n\tif (ca_key == TCP_CA_UNSPEC)\n\t\treturn;\n\n\trcu_read_lock();\n\tca = tcp_ca_find_key(ca_key);\n\tif (likely(ca && bpf_try_module_get(ca, ca->owner))) {\n\t\tbpf_module_put(icsk->icsk_ca_ops, icsk->icsk_ca_ops->owner);\n\t\ticsk->icsk_ca_dst_locked = tcp_ca_dst_locked(dst);\n\t\ticsk->icsk_ca_ops = ca;\n\t}\n\trcu_read_unlock();\n}\n\n \nstatic void tcp_connect_init(struct sock *sk)\n{\n\tconst struct dst_entry *dst = __sk_dst_get(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\t__u8 rcv_wscale;\n\tu32 rcv_wnd;\n\n\t \n\ttp->tcp_header_len = sizeof(struct tcphdr);\n\tif (READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_timestamps))\n\t\ttp->tcp_header_len += TCPOLEN_TSTAMP_ALIGNED;\n\n\t \n\tif (tp->rx_opt.user_mss)\n\t\ttp->rx_opt.mss_clamp = tp->rx_opt.user_mss;\n\ttp->max_window = 0;\n\ttcp_mtup_init(sk);\n\ttcp_sync_mss(sk, dst_mtu(dst));\n\n\ttcp_ca_dst_init(sk, dst);\n\n\tif (!tp->window_clamp)\n\t\ttp->window_clamp = dst_metric(dst, RTAX_WINDOW);\n\ttp->advmss = tcp_mss_clamp(tp, dst_metric_advmss(dst));\n\n\ttcp_initialize_rcv_mss(sk);\n\n\t \n\tif (sk->sk_userlocks & SOCK_RCVBUF_LOCK &&\n\t    (tp->window_clamp > tcp_full_space(sk) || tp->window_clamp == 0))\n\t\ttp->window_clamp = tcp_full_space(sk);\n\n\trcv_wnd = tcp_rwnd_init_bpf(sk);\n\tif (rcv_wnd == 0)\n\t\trcv_wnd = dst_metric(dst, RTAX_INITRWND);\n\n\ttcp_select_initial_window(sk, tcp_full_space(sk),\n\t\t\t\t  tp->advmss - (tp->rx_opt.ts_recent_stamp ? tp->tcp_header_len - sizeof(struct tcphdr) : 0),\n\t\t\t\t  &tp->rcv_wnd,\n\t\t\t\t  &tp->window_clamp,\n\t\t\t\t  READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_window_scaling),\n\t\t\t\t  &rcv_wscale,\n\t\t\t\t  rcv_wnd);\n\n\ttp->rx_opt.rcv_wscale = rcv_wscale;\n\ttp->rcv_ssthresh = tp->rcv_wnd;\n\n\tWRITE_ONCE(sk->sk_err, 0);\n\tsock_reset_flag(sk, SOCK_DONE);\n\ttp->snd_wnd = 0;\n\ttcp_init_wl(tp, 0);\n\ttcp_write_queue_purge(sk);\n\ttp->snd_una = tp->write_seq;\n\ttp->snd_sml = tp->write_seq;\n\ttp->snd_up = tp->write_seq;\n\tWRITE_ONCE(tp->snd_nxt, tp->write_seq);\n\n\tif (likely(!tp->repair))\n\t\ttp->rcv_nxt = 0;\n\telse\n\t\ttp->rcv_tstamp = tcp_jiffies32;\n\ttp->rcv_wup = tp->rcv_nxt;\n\tWRITE_ONCE(tp->copied_seq, tp->rcv_nxt);\n\n\tinet_csk(sk)->icsk_rto = tcp_timeout_init(sk);\n\tinet_csk(sk)->icsk_retransmits = 0;\n\ttcp_clear_retrans(tp);\n}\n\nstatic void tcp_connect_queue_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct tcp_skb_cb *tcb = TCP_SKB_CB(skb);\n\n\ttcb->end_seq += skb->len;\n\t__skb_header_release(skb);\n\tsk_wmem_queued_add(sk, skb->truesize);\n\tsk_mem_charge(sk, skb->truesize);\n\tWRITE_ONCE(tp->write_seq, tcb->end_seq);\n\ttp->packets_out += tcp_skb_pcount(skb);\n}\n\n \nstatic int tcp_send_syn_data(struct sock *sk, struct sk_buff *syn)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct tcp_fastopen_request *fo = tp->fastopen_req;\n\tstruct page_frag *pfrag = sk_page_frag(sk);\n\tstruct sk_buff *syn_data;\n\tint space, err = 0;\n\n\ttp->rx_opt.mss_clamp = tp->advmss;   \n\tif (!tcp_fastopen_cookie_check(sk, &tp->rx_opt.mss_clamp, &fo->cookie))\n\t\tgoto fallback;\n\n\t \n\ttp->rx_opt.mss_clamp = tcp_mss_clamp(tp, tp->rx_opt.mss_clamp);\n\t \n\ttcp_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\n\tspace = __tcp_mtu_to_mss(sk, icsk->icsk_pmtu_cookie) -\n\t\tMAX_TCP_OPTION_SPACE;\n\n\tspace = min_t(size_t, space, fo->size);\n\n\tif (space &&\n\t    !skb_page_frag_refill(min_t(size_t, space, PAGE_SIZE),\n\t\t\t\t  pfrag, sk->sk_allocation))\n\t\tgoto fallback;\n\tsyn_data = tcp_stream_alloc_skb(sk, sk->sk_allocation, false);\n\tif (!syn_data)\n\t\tgoto fallback;\n\tmemcpy(syn_data->cb, syn->cb, sizeof(syn->cb));\n\tif (space) {\n\t\tspace = min_t(size_t, space, pfrag->size - pfrag->offset);\n\t\tspace = tcp_wmem_schedule(sk, space);\n\t}\n\tif (space) {\n\t\tspace = copy_page_from_iter(pfrag->page, pfrag->offset,\n\t\t\t\t\t    space, &fo->data->msg_iter);\n\t\tif (unlikely(!space)) {\n\t\t\ttcp_skb_tsorted_anchor_cleanup(syn_data);\n\t\t\tkfree_skb(syn_data);\n\t\t\tgoto fallback;\n\t\t}\n\t\tskb_fill_page_desc(syn_data, 0, pfrag->page,\n\t\t\t\t   pfrag->offset, space);\n\t\tpage_ref_inc(pfrag->page);\n\t\tpfrag->offset += space;\n\t\tskb_len_add(syn_data, space);\n\t\tskb_zcopy_set(syn_data, fo->uarg, NULL);\n\t}\n\t \n\tif (space == fo->size)\n\t\tfo->data = NULL;\n\tfo->copied = space;\n\n\ttcp_connect_queue_skb(sk, syn_data);\n\tif (syn_data->len)\n\t\ttcp_chrono_start(sk, TCP_CHRONO_BUSY);\n\n\terr = tcp_transmit_skb(sk, syn_data, 1, sk->sk_allocation);\n\n\tskb_set_delivery_time(syn, syn_data->skb_mstamp_ns, true);\n\n\t \n\tTCP_SKB_CB(syn_data)->seq++;\n\tTCP_SKB_CB(syn_data)->tcp_flags = TCPHDR_ACK | TCPHDR_PSH;\n\tif (!err) {\n\t\ttp->syn_data = (fo->copied > 0);\n\t\ttcp_rbtree_insert(&sk->tcp_rtx_queue, syn_data);\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPORIGDATASENT);\n\t\tgoto done;\n\t}\n\n\t \n\t__skb_queue_tail(&sk->sk_write_queue, syn_data);\n\ttp->packets_out -= tcp_skb_pcount(syn_data);\n\nfallback:\n\t \n\tif (fo->cookie.len > 0)\n\t\tfo->cookie.len = 0;\n\terr = tcp_transmit_skb(sk, syn, 1, sk->sk_allocation);\n\tif (err)\n\t\ttp->syn_fastopen = 0;\ndone:\n\tfo->cookie.len = -1;   \n\treturn err;\n}\n\n \nint tcp_connect(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *buff;\n\tint err;\n\n\ttcp_call_bpf(sk, BPF_SOCK_OPS_TCP_CONNECT_CB, 0, NULL);\n\n\tif (inet_csk(sk)->icsk_af_ops->rebuild_header(sk))\n\t\treturn -EHOSTUNREACH;  \n\n\ttcp_connect_init(sk);\n\n\tif (unlikely(tp->repair)) {\n\t\ttcp_finish_connect(sk, NULL);\n\t\treturn 0;\n\t}\n\n\tbuff = tcp_stream_alloc_skb(sk, sk->sk_allocation, true);\n\tif (unlikely(!buff))\n\t\treturn -ENOBUFS;\n\n\ttcp_init_nondata_skb(buff, tp->write_seq++, TCPHDR_SYN);\n\ttcp_mstamp_refresh(tp);\n\ttp->retrans_stamp = tcp_time_stamp(tp);\n\ttcp_connect_queue_skb(sk, buff);\n\ttcp_ecn_send_syn(sk, buff);\n\ttcp_rbtree_insert(&sk->tcp_rtx_queue, buff);\n\n\t \n\terr = tp->fastopen_req ? tcp_send_syn_data(sk, buff) :\n\t      tcp_transmit_skb(sk, buff, 1, sk->sk_allocation);\n\tif (err == -ECONNREFUSED)\n\t\treturn err;\n\n\t \n\tWRITE_ONCE(tp->snd_nxt, tp->write_seq);\n\ttp->pushed_seq = tp->write_seq;\n\tbuff = tcp_send_head(sk);\n\tif (unlikely(buff)) {\n\t\tWRITE_ONCE(tp->snd_nxt, TCP_SKB_CB(buff)->seq);\n\t\ttp->pushed_seq\t= TCP_SKB_CB(buff)->seq;\n\t}\n\tTCP_INC_STATS(sock_net(sk), TCP_MIB_ACTIVEOPENS);\n\n\t \n\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,\n\t\t\t\t  inet_csk(sk)->icsk_rto, TCP_RTO_MAX);\n\treturn 0;\n}\nEXPORT_SYMBOL(tcp_connect);\n\nu32 tcp_delack_max(const struct sock *sk)\n{\n\tconst struct dst_entry *dst = __sk_dst_get(sk);\n\tu32 delack_max = inet_csk(sk)->icsk_delack_max;\n\n\tif (dst && dst_metric_locked(dst, RTAX_RTO_MIN)) {\n\t\tu32 rto_min = dst_metric_rtt(dst, RTAX_RTO_MIN);\n\t\tu32 delack_from_rto_min = max_t(int, 1, rto_min - 1);\n\n\t\tdelack_max = min_t(u32, delack_max, delack_from_rto_min);\n\t}\n\treturn delack_max;\n}\n\n \nvoid tcp_send_delayed_ack(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tint ato = icsk->icsk_ack.ato;\n\tunsigned long timeout;\n\n\tif (ato > TCP_DELACK_MIN) {\n\t\tconst struct tcp_sock *tp = tcp_sk(sk);\n\t\tint max_ato = HZ / 2;\n\n\t\tif (inet_csk_in_pingpong_mode(sk) ||\n\t\t    (icsk->icsk_ack.pending & ICSK_ACK_PUSHED))\n\t\t\tmax_ato = TCP_DELACK_MAX;\n\n\t\t \n\n\t\t \n\t\tif (tp->srtt_us) {\n\t\t\tint rtt = max_t(int, usecs_to_jiffies(tp->srtt_us >> 3),\n\t\t\t\t\tTCP_DELACK_MIN);\n\n\t\t\tif (rtt < max_ato)\n\t\t\t\tmax_ato = rtt;\n\t\t}\n\n\t\tato = min(ato, max_ato);\n\t}\n\n\tato = min_t(u32, ato, tcp_delack_max(sk));\n\n\t \n\ttimeout = jiffies + ato;\n\n\t \n\tif (icsk->icsk_ack.pending & ICSK_ACK_TIMER) {\n\t\t \n\t\tif (time_before_eq(icsk->icsk_ack.timeout, jiffies + (ato >> 2))) {\n\t\t\ttcp_send_ack(sk);\n\t\t\treturn;\n\t\t}\n\n\t\tif (!time_before(timeout, icsk->icsk_ack.timeout))\n\t\t\ttimeout = icsk->icsk_ack.timeout;\n\t}\n\ticsk->icsk_ack.pending |= ICSK_ACK_SCHED | ICSK_ACK_TIMER;\n\ticsk->icsk_ack.timeout = timeout;\n\tsk_reset_timer(sk, &icsk->icsk_delack_timer, timeout);\n}\n\n \nvoid __tcp_send_ack(struct sock *sk, u32 rcv_nxt)\n{\n\tstruct sk_buff *buff;\n\n\t \n\tif (sk->sk_state == TCP_CLOSE)\n\t\treturn;\n\n\t \n\tbuff = alloc_skb(MAX_TCP_HEADER,\n\t\t\t sk_gfp_mask(sk, GFP_ATOMIC | __GFP_NOWARN));\n\tif (unlikely(!buff)) {\n\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\t\tunsigned long delay;\n\n\t\tdelay = TCP_DELACK_MAX << icsk->icsk_ack.retry;\n\t\tif (delay < TCP_RTO_MAX)\n\t\t\ticsk->icsk_ack.retry++;\n\t\tinet_csk_schedule_ack(sk);\n\t\ticsk->icsk_ack.ato = TCP_ATO_MIN;\n\t\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_DACK, delay, TCP_RTO_MAX);\n\t\treturn;\n\t}\n\n\t \n\tskb_reserve(buff, MAX_TCP_HEADER);\n\ttcp_init_nondata_skb(buff, tcp_acceptable_seq(sk), TCPHDR_ACK);\n\n\t \n\tskb_set_tcp_pure_ack(buff);\n\n\t \n\t__tcp_transmit_skb(sk, buff, 0, (__force gfp_t)0, rcv_nxt);\n}\nEXPORT_SYMBOL_GPL(__tcp_send_ack);\n\nvoid tcp_send_ack(struct sock *sk)\n{\n\t__tcp_send_ack(sk, tcp_sk(sk)->rcv_nxt);\n}\n\n \nstatic int tcp_xmit_probe_skb(struct sock *sk, int urgent, int mib)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb;\n\n\t \n\tskb = alloc_skb(MAX_TCP_HEADER,\n\t\t\tsk_gfp_mask(sk, GFP_ATOMIC | __GFP_NOWARN));\n\tif (!skb)\n\t\treturn -1;\n\n\t \n\tskb_reserve(skb, MAX_TCP_HEADER);\n\t \n\ttcp_init_nondata_skb(skb, tp->snd_una - !urgent, TCPHDR_ACK);\n\tNET_INC_STATS(sock_net(sk), mib);\n\treturn tcp_transmit_skb(sk, skb, 0, (__force gfp_t)0);\n}\n\n \nvoid tcp_send_window_probe(struct sock *sk)\n{\n\tif (sk->sk_state == TCP_ESTABLISHED) {\n\t\ttcp_sk(sk)->snd_wl1 = tcp_sk(sk)->rcv_nxt - 1;\n\t\ttcp_mstamp_refresh(tcp_sk(sk));\n\t\ttcp_xmit_probe_skb(sk, 0, LINUX_MIB_TCPWINPROBE);\n\t}\n}\n\n \nint tcp_write_wakeup(struct sock *sk, int mib)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb;\n\n\tif (sk->sk_state == TCP_CLOSE)\n\t\treturn -1;\n\n\tskb = tcp_send_head(sk);\n\tif (skb && before(TCP_SKB_CB(skb)->seq, tcp_wnd_end(tp))) {\n\t\tint err;\n\t\tunsigned int mss = tcp_current_mss(sk);\n\t\tunsigned int seg_size = tcp_wnd_end(tp) - TCP_SKB_CB(skb)->seq;\n\n\t\tif (before(tp->pushed_seq, TCP_SKB_CB(skb)->end_seq))\n\t\t\ttp->pushed_seq = TCP_SKB_CB(skb)->end_seq;\n\n\t\t \n\t\tif (seg_size < TCP_SKB_CB(skb)->end_seq - TCP_SKB_CB(skb)->seq ||\n\t\t    skb->len > mss) {\n\t\t\tseg_size = min(seg_size, mss);\n\t\t\tTCP_SKB_CB(skb)->tcp_flags |= TCPHDR_PSH;\n\t\t\tif (tcp_fragment(sk, TCP_FRAG_IN_WRITE_QUEUE,\n\t\t\t\t\t skb, seg_size, mss, GFP_ATOMIC))\n\t\t\t\treturn -1;\n\t\t} else if (!tcp_skb_pcount(skb))\n\t\t\ttcp_set_skb_tso_segs(skb, mss);\n\n\t\tTCP_SKB_CB(skb)->tcp_flags |= TCPHDR_PSH;\n\t\terr = tcp_transmit_skb(sk, skb, 1, GFP_ATOMIC);\n\t\tif (!err)\n\t\t\ttcp_event_new_data_sent(sk, skb);\n\t\treturn err;\n\t} else {\n\t\tif (between(tp->snd_up, tp->snd_una + 1, tp->snd_una + 0xFFFF))\n\t\t\ttcp_xmit_probe_skb(sk, 1, mib);\n\t\treturn tcp_xmit_probe_skb(sk, 0, mib);\n\t}\n}\n\n \nvoid tcp_send_probe0(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tunsigned long timeout;\n\tint err;\n\n\terr = tcp_write_wakeup(sk, LINUX_MIB_TCPWINPROBE);\n\n\tif (tp->packets_out || tcp_write_queue_empty(sk)) {\n\t\t \n\t\ticsk->icsk_probes_out = 0;\n\t\ticsk->icsk_backoff = 0;\n\t\ticsk->icsk_probes_tstamp = 0;\n\t\treturn;\n\t}\n\n\ticsk->icsk_probes_out++;\n\tif (err <= 0) {\n\t\tif (icsk->icsk_backoff < READ_ONCE(net->ipv4.sysctl_tcp_retries2))\n\t\t\ticsk->icsk_backoff++;\n\t\ttimeout = tcp_probe0_when(sk, TCP_RTO_MAX);\n\t} else {\n\t\t \n\t\ttimeout = TCP_RESOURCE_PROBE_INTERVAL;\n\t}\n\n\ttimeout = tcp_clamp_probe0_to_user_timeout(sk, timeout);\n\ttcp_reset_xmit_timer(sk, ICSK_TIME_PROBE0, timeout, TCP_RTO_MAX);\n}\n\nint tcp_rtx_synack(const struct sock *sk, struct request_sock *req)\n{\n\tconst struct tcp_request_sock_ops *af_ops = tcp_rsk(req)->af_specific;\n\tstruct flowi fl;\n\tint res;\n\n\t \n\tif (READ_ONCE(sk->sk_txrehash) == SOCK_TXREHASH_ENABLED)\n\t\tWRITE_ONCE(tcp_rsk(req)->txhash, net_tx_rndhash());\n\tres = af_ops->send_synack(sk, NULL, &fl, req, NULL, TCP_SYNACK_NORMAL,\n\t\t\t\t  NULL);\n\tif (!res) {\n\t\tTCP_INC_STATS(sock_net(sk), TCP_MIB_RETRANSSEGS);\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPSYNRETRANS);\n\t\tif (unlikely(tcp_passive_fastopen(sk))) {\n\t\t\t \n\t\t\ttcp_sk_rw(sk)->total_retrans++;\n\t\t}\n\t\ttrace_tcp_retransmit_synack(sk, req);\n\t}\n\treturn res;\n}\nEXPORT_SYMBOL(tcp_rtx_synack);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}