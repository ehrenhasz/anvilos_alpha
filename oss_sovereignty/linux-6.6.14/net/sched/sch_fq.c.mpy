{
  "module_name": "sch_fq.c",
  "hash_id": "3c63cea1d7352cc3038704bb1cf2275083ed195e1173c4da069a00993051c191",
  "original_prompt": "Ingested from linux-6.6.14/net/sched/sch_fq.c",
  "human_readable_source": "\n \n\n#include <linux/module.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/jiffies.h>\n#include <linux/string.h>\n#include <linux/in.h>\n#include <linux/errno.h>\n#include <linux/init.h>\n#include <linux/skbuff.h>\n#include <linux/slab.h>\n#include <linux/rbtree.h>\n#include <linux/hash.h>\n#include <linux/prefetch.h>\n#include <linux/vmalloc.h>\n#include <net/netlink.h>\n#include <net/pkt_sched.h>\n#include <net/sock.h>\n#include <net/tcp_states.h>\n#include <net/tcp.h>\n\nstruct fq_skb_cb {\n\tu64\t        time_to_send;\n};\n\nstatic inline struct fq_skb_cb *fq_skb_cb(struct sk_buff *skb)\n{\n\tqdisc_cb_private_validate(skb, sizeof(struct fq_skb_cb));\n\treturn (struct fq_skb_cb *)qdisc_skb_cb(skb)->data;\n}\n\n \nstruct fq_flow {\n \n\tstruct rb_root\tt_root;\n\tstruct sk_buff\t*head;\t\t \n\tunion {\n\t\tstruct sk_buff *tail;\t \n\t\tunsigned long  age;\t \n\t};\n\tstruct rb_node\tfq_node;\t \n\tstruct sock\t*sk;\n\tu32\t\tsocket_hash;\t \n\tint\t\tqlen;\t\t \n\n \n\tint\t\tcredit;\n\t \n\n\tstruct fq_flow *next;\t\t \n\n\tstruct rb_node  rate_node;\t \n\tu64\t\ttime_next_packet;\n} ____cacheline_aligned_in_smp;\n\nstruct fq_flow_head {\n\tstruct fq_flow *first;\n\tstruct fq_flow *last;\n};\n\nstruct fq_sched_data {\n\tstruct fq_flow_head new_flows;\n\n\tstruct fq_flow_head old_flows;\n\n\tstruct rb_root\tdelayed;\t \n\tu64\t\ttime_next_delayed_flow;\n\tu64\t\tktime_cache;\t \n\tunsigned long\tunthrottle_latency_ns;\n\n\tstruct fq_flow\tinternal;\t \n\tu32\t\tquantum;\n\tu32\t\tinitial_quantum;\n\tu32\t\tflow_refill_delay;\n\tu32\t\tflow_plimit;\t \n\tunsigned long\tflow_max_rate;\t \n\tu64\t\tce_threshold;\n\tu64\t\thorizon;\t \n\tu32\t\torphan_mask;\t \n\tu32\t\tlow_rate_threshold;\n\tstruct rb_root\t*fq_root;\n\tu8\t\trate_enable;\n\tu8\t\tfq_trees_log;\n\tu8\t\thorizon_drop;\n\tu32\t\tflows;\n\tu32\t\tinactive_flows;\n\tu32\t\tthrottled_flows;\n\n\tu64\t\tstat_gc_flows;\n\tu64\t\tstat_internal_packets;\n\tu64\t\tstat_throttled;\n\tu64\t\tstat_ce_mark;\n\tu64\t\tstat_horizon_drops;\n\tu64\t\tstat_horizon_caps;\n\tu64\t\tstat_flows_plimit;\n\tu64\t\tstat_pkts_too_long;\n\tu64\t\tstat_allocation_errors;\n\n\tu32\t\ttimer_slack;  \n\tstruct qdisc_watchdog watchdog;\n};\n\n \nstatic void fq_flow_set_detached(struct fq_flow *f)\n{\n\tf->age = jiffies | 1UL;\n}\n\nstatic bool fq_flow_is_detached(const struct fq_flow *f)\n{\n\treturn !!(f->age & 1UL);\n}\n\n \nstatic struct fq_flow throttled;\n\nstatic bool fq_flow_is_throttled(const struct fq_flow *f)\n{\n\treturn f->next == &throttled;\n}\n\nstatic void fq_flow_add_tail(struct fq_flow_head *head, struct fq_flow *flow)\n{\n\tif (head->first)\n\t\thead->last->next = flow;\n\telse\n\t\thead->first = flow;\n\thead->last = flow;\n\tflow->next = NULL;\n}\n\nstatic void fq_flow_unset_throttled(struct fq_sched_data *q, struct fq_flow *f)\n{\n\trb_erase(&f->rate_node, &q->delayed);\n\tq->throttled_flows--;\n\tfq_flow_add_tail(&q->old_flows, f);\n}\n\nstatic void fq_flow_set_throttled(struct fq_sched_data *q, struct fq_flow *f)\n{\n\tstruct rb_node **p = &q->delayed.rb_node, *parent = NULL;\n\n\twhile (*p) {\n\t\tstruct fq_flow *aux;\n\n\t\tparent = *p;\n\t\taux = rb_entry(parent, struct fq_flow, rate_node);\n\t\tif (f->time_next_packet >= aux->time_next_packet)\n\t\t\tp = &parent->rb_right;\n\t\telse\n\t\t\tp = &parent->rb_left;\n\t}\n\trb_link_node(&f->rate_node, parent, p);\n\trb_insert_color(&f->rate_node, &q->delayed);\n\tq->throttled_flows++;\n\tq->stat_throttled++;\n\n\tf->next = &throttled;\n\tif (q->time_next_delayed_flow > f->time_next_packet)\n\t\tq->time_next_delayed_flow = f->time_next_packet;\n}\n\n\nstatic struct kmem_cache *fq_flow_cachep __read_mostly;\n\n\n \n#define FQ_GC_MAX 8\n#define FQ_GC_AGE (3*HZ)\n\nstatic bool fq_gc_candidate(const struct fq_flow *f)\n{\n\treturn fq_flow_is_detached(f) &&\n\t       time_after(jiffies, f->age + FQ_GC_AGE);\n}\n\nstatic void fq_gc(struct fq_sched_data *q,\n\t\t  struct rb_root *root,\n\t\t  struct sock *sk)\n{\n\tstruct rb_node **p, *parent;\n\tvoid *tofree[FQ_GC_MAX];\n\tstruct fq_flow *f;\n\tint i, fcnt = 0;\n\n\tp = &root->rb_node;\n\tparent = NULL;\n\twhile (*p) {\n\t\tparent = *p;\n\n\t\tf = rb_entry(parent, struct fq_flow, fq_node);\n\t\tif (f->sk == sk)\n\t\t\tbreak;\n\n\t\tif (fq_gc_candidate(f)) {\n\t\t\ttofree[fcnt++] = f;\n\t\t\tif (fcnt == FQ_GC_MAX)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (f->sk > sk)\n\t\t\tp = &parent->rb_right;\n\t\telse\n\t\t\tp = &parent->rb_left;\n\t}\n\n\tif (!fcnt)\n\t\treturn;\n\n\tfor (i = fcnt; i > 0; ) {\n\t\tf = tofree[--i];\n\t\trb_erase(&f->fq_node, root);\n\t}\n\tq->flows -= fcnt;\n\tq->inactive_flows -= fcnt;\n\tq->stat_gc_flows += fcnt;\n\n\tkmem_cache_free_bulk(fq_flow_cachep, fcnt, tofree);\n}\n\nstatic struct fq_flow *fq_classify(struct sk_buff *skb, struct fq_sched_data *q)\n{\n\tstruct rb_node **p, *parent;\n\tstruct sock *sk = skb->sk;\n\tstruct rb_root *root;\n\tstruct fq_flow *f;\n\n\t \n\tif (unlikely((skb->priority & TC_PRIO_MAX) == TC_PRIO_CONTROL))\n\t\treturn &q->internal;\n\n\t \n\tif (!sk || sk_listener(sk)) {\n\t\tunsigned long hash = skb_get_hash(skb) & q->orphan_mask;\n\n\t\t \n\t\tsk = (struct sock *)((hash << 1) | 1UL);\n\t\tskb_orphan(skb);\n\t} else if (sk->sk_state == TCP_CLOSE) {\n\t\tunsigned long hash = skb_get_hash(skb) & q->orphan_mask;\n\t\t \n\t\tsk = (struct sock *)((hash << 1) | 1UL);\n\t}\n\n\troot = &q->fq_root[hash_ptr(sk, q->fq_trees_log)];\n\n\tif (q->flows >= (2U << q->fq_trees_log) &&\n\t    q->inactive_flows > q->flows/2)\n\t\tfq_gc(q, root, sk);\n\n\tp = &root->rb_node;\n\tparent = NULL;\n\twhile (*p) {\n\t\tparent = *p;\n\n\t\tf = rb_entry(parent, struct fq_flow, fq_node);\n\t\tif (f->sk == sk) {\n\t\t\t \n\t\t\tif (unlikely(skb->sk == sk &&\n\t\t\t\t     f->socket_hash != sk->sk_hash)) {\n\t\t\t\tf->credit = q->initial_quantum;\n\t\t\t\tf->socket_hash = sk->sk_hash;\n\t\t\t\tif (q->rate_enable)\n\t\t\t\t\tsmp_store_release(&sk->sk_pacing_status,\n\t\t\t\t\t\t\t  SK_PACING_FQ);\n\t\t\t\tif (fq_flow_is_throttled(f))\n\t\t\t\t\tfq_flow_unset_throttled(q, f);\n\t\t\t\tf->time_next_packet = 0ULL;\n\t\t\t}\n\t\t\treturn f;\n\t\t}\n\t\tif (f->sk > sk)\n\t\t\tp = &parent->rb_right;\n\t\telse\n\t\t\tp = &parent->rb_left;\n\t}\n\n\tf = kmem_cache_zalloc(fq_flow_cachep, GFP_ATOMIC | __GFP_NOWARN);\n\tif (unlikely(!f)) {\n\t\tq->stat_allocation_errors++;\n\t\treturn &q->internal;\n\t}\n\t \n\n\tfq_flow_set_detached(f);\n\tf->sk = sk;\n\tif (skb->sk == sk) {\n\t\tf->socket_hash = sk->sk_hash;\n\t\tif (q->rate_enable)\n\t\t\tsmp_store_release(&sk->sk_pacing_status,\n\t\t\t\t\t  SK_PACING_FQ);\n\t}\n\tf->credit = q->initial_quantum;\n\n\trb_link_node(&f->fq_node, parent, p);\n\trb_insert_color(&f->fq_node, root);\n\n\tq->flows++;\n\tq->inactive_flows++;\n\treturn f;\n}\n\nstatic struct sk_buff *fq_peek(struct fq_flow *flow)\n{\n\tstruct sk_buff *skb = skb_rb_first(&flow->t_root);\n\tstruct sk_buff *head = flow->head;\n\n\tif (!skb)\n\t\treturn head;\n\n\tif (!head)\n\t\treturn skb;\n\n\tif (fq_skb_cb(skb)->time_to_send < fq_skb_cb(head)->time_to_send)\n\t\treturn skb;\n\treturn head;\n}\n\nstatic void fq_erase_head(struct Qdisc *sch, struct fq_flow *flow,\n\t\t\t  struct sk_buff *skb)\n{\n\tif (skb == flow->head) {\n\t\tflow->head = skb->next;\n\t} else {\n\t\trb_erase(&skb->rbnode, &flow->t_root);\n\t\tskb->dev = qdisc_dev(sch);\n\t}\n}\n\n \nstatic void fq_dequeue_skb(struct Qdisc *sch, struct fq_flow *flow,\n\t\t\t   struct sk_buff *skb)\n{\n\tfq_erase_head(sch, flow, skb);\n\tskb_mark_not_on_list(skb);\n\tflow->qlen--;\n\tqdisc_qstats_backlog_dec(sch, skb);\n\tsch->q.qlen--;\n}\n\nstatic void flow_queue_add(struct fq_flow *flow, struct sk_buff *skb)\n{\n\tstruct rb_node **p, *parent;\n\tstruct sk_buff *head, *aux;\n\n\thead = flow->head;\n\tif (!head ||\n\t    fq_skb_cb(skb)->time_to_send >= fq_skb_cb(flow->tail)->time_to_send) {\n\t\tif (!head)\n\t\t\tflow->head = skb;\n\t\telse\n\t\t\tflow->tail->next = skb;\n\t\tflow->tail = skb;\n\t\tskb->next = NULL;\n\t\treturn;\n\t}\n\n\tp = &flow->t_root.rb_node;\n\tparent = NULL;\n\n\twhile (*p) {\n\t\tparent = *p;\n\t\taux = rb_to_skb(parent);\n\t\tif (fq_skb_cb(skb)->time_to_send >= fq_skb_cb(aux)->time_to_send)\n\t\t\tp = &parent->rb_right;\n\t\telse\n\t\t\tp = &parent->rb_left;\n\t}\n\trb_link_node(&skb->rbnode, parent, p);\n\trb_insert_color(&skb->rbnode, &flow->t_root);\n}\n\nstatic bool fq_packet_beyond_horizon(const struct sk_buff *skb,\n\t\t\t\t    const struct fq_sched_data *q)\n{\n\treturn unlikely((s64)skb->tstamp > (s64)(q->ktime_cache + q->horizon));\n}\n\nstatic int fq_enqueue(struct sk_buff *skb, struct Qdisc *sch,\n\t\t      struct sk_buff **to_free)\n{\n\tstruct fq_sched_data *q = qdisc_priv(sch);\n\tstruct fq_flow *f;\n\n\tif (unlikely(sch->q.qlen >= sch->limit))\n\t\treturn qdisc_drop(skb, sch, to_free);\n\n\tif (!skb->tstamp) {\n\t\tfq_skb_cb(skb)->time_to_send = q->ktime_cache = ktime_get_ns();\n\t} else {\n\t\t \n\t\tif (fq_packet_beyond_horizon(skb, q)) {\n\t\t\t \n\t\t\tq->ktime_cache = ktime_get_ns();\n\t\t\tif (fq_packet_beyond_horizon(skb, q)) {\n\t\t\t\tif (q->horizon_drop) {\n\t\t\t\t\tq->stat_horizon_drops++;\n\t\t\t\t\treturn qdisc_drop(skb, sch, to_free);\n\t\t\t\t}\n\t\t\t\tq->stat_horizon_caps++;\n\t\t\t\tskb->tstamp = q->ktime_cache + q->horizon;\n\t\t\t}\n\t\t}\n\t\tfq_skb_cb(skb)->time_to_send = skb->tstamp;\n\t}\n\n\tf = fq_classify(skb, q);\n\tif (unlikely(f->qlen >= q->flow_plimit && f != &q->internal)) {\n\t\tq->stat_flows_plimit++;\n\t\treturn qdisc_drop(skb, sch, to_free);\n\t}\n\n\tf->qlen++;\n\tqdisc_qstats_backlog_inc(sch, skb);\n\tif (fq_flow_is_detached(f)) {\n\t\tfq_flow_add_tail(&q->new_flows, f);\n\t\tif (time_after(jiffies, f->age + q->flow_refill_delay))\n\t\t\tf->credit = max_t(u32, f->credit, q->quantum);\n\t\tq->inactive_flows--;\n\t}\n\n\t \n\tflow_queue_add(f, skb);\n\n\tif (unlikely(f == &q->internal)) {\n\t\tq->stat_internal_packets++;\n\t}\n\tsch->q.qlen++;\n\n\treturn NET_XMIT_SUCCESS;\n}\n\nstatic void fq_check_throttled(struct fq_sched_data *q, u64 now)\n{\n\tunsigned long sample;\n\tstruct rb_node *p;\n\n\tif (q->time_next_delayed_flow > now)\n\t\treturn;\n\n\t \n\tsample = (unsigned long)(now - q->time_next_delayed_flow);\n\tq->unthrottle_latency_ns -= q->unthrottle_latency_ns >> 3;\n\tq->unthrottle_latency_ns += sample >> 3;\n\n\tq->time_next_delayed_flow = ~0ULL;\n\twhile ((p = rb_first(&q->delayed)) != NULL) {\n\t\tstruct fq_flow *f = rb_entry(p, struct fq_flow, rate_node);\n\n\t\tif (f->time_next_packet > now) {\n\t\t\tq->time_next_delayed_flow = f->time_next_packet;\n\t\t\tbreak;\n\t\t}\n\t\tfq_flow_unset_throttled(q, f);\n\t}\n}\n\nstatic struct sk_buff *fq_dequeue(struct Qdisc *sch)\n{\n\tstruct fq_sched_data *q = qdisc_priv(sch);\n\tstruct fq_flow_head *head;\n\tstruct sk_buff *skb;\n\tstruct fq_flow *f;\n\tunsigned long rate;\n\tu32 plen;\n\tu64 now;\n\n\tif (!sch->q.qlen)\n\t\treturn NULL;\n\n\tskb = fq_peek(&q->internal);\n\tif (unlikely(skb)) {\n\t\tfq_dequeue_skb(sch, &q->internal, skb);\n\t\tgoto out;\n\t}\n\n\tq->ktime_cache = now = ktime_get_ns();\n\tfq_check_throttled(q, now);\nbegin:\n\thead = &q->new_flows;\n\tif (!head->first) {\n\t\thead = &q->old_flows;\n\t\tif (!head->first) {\n\t\t\tif (q->time_next_delayed_flow != ~0ULL)\n\t\t\t\tqdisc_watchdog_schedule_range_ns(&q->watchdog,\n\t\t\t\t\t\t\tq->time_next_delayed_flow,\n\t\t\t\t\t\t\tq->timer_slack);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\tf = head->first;\n\n\tif (f->credit <= 0) {\n\t\tf->credit += q->quantum;\n\t\thead->first = f->next;\n\t\tfq_flow_add_tail(&q->old_flows, f);\n\t\tgoto begin;\n\t}\n\n\tskb = fq_peek(f);\n\tif (skb) {\n\t\tu64 time_next_packet = max_t(u64, fq_skb_cb(skb)->time_to_send,\n\t\t\t\t\t     f->time_next_packet);\n\n\t\tif (now < time_next_packet) {\n\t\t\thead->first = f->next;\n\t\t\tf->time_next_packet = time_next_packet;\n\t\t\tfq_flow_set_throttled(q, f);\n\t\t\tgoto begin;\n\t\t}\n\t\tprefetch(&skb->end);\n\t\tif ((s64)(now - time_next_packet - q->ce_threshold) > 0) {\n\t\t\tINET_ECN_set_ce(skb);\n\t\t\tq->stat_ce_mark++;\n\t\t}\n\t\tfq_dequeue_skb(sch, f, skb);\n\t} else {\n\t\thead->first = f->next;\n\t\t \n\t\tif ((head == &q->new_flows) && q->old_flows.first) {\n\t\t\tfq_flow_add_tail(&q->old_flows, f);\n\t\t} else {\n\t\t\tfq_flow_set_detached(f);\n\t\t\tq->inactive_flows++;\n\t\t}\n\t\tgoto begin;\n\t}\n\tplen = qdisc_pkt_len(skb);\n\tf->credit -= plen;\n\n\tif (!q->rate_enable)\n\t\tgoto out;\n\n\trate = q->flow_max_rate;\n\n\t \n\tif (!skb->tstamp) {\n\t\tif (skb->sk)\n\t\t\trate = min(skb->sk->sk_pacing_rate, rate);\n\n\t\tif (rate <= q->low_rate_threshold) {\n\t\t\tf->credit = 0;\n\t\t} else {\n\t\t\tplen = max(plen, q->quantum);\n\t\t\tif (f->credit > 0)\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\tif (rate != ~0UL) {\n\t\tu64 len = (u64)plen * NSEC_PER_SEC;\n\n\t\tif (likely(rate))\n\t\t\tlen = div64_ul(len, rate);\n\t\t \n\t\tif (unlikely(len > NSEC_PER_SEC)) {\n\t\t\tlen = NSEC_PER_SEC;\n\t\t\tq->stat_pkts_too_long++;\n\t\t}\n\t\t \n\t\tif (f->time_next_packet)\n\t\t\tlen -= min(len/2, now - f->time_next_packet);\n\t\tf->time_next_packet = now + len;\n\t}\nout:\n\tqdisc_bstats_update(sch, skb);\n\treturn skb;\n}\n\nstatic void fq_flow_purge(struct fq_flow *flow)\n{\n\tstruct rb_node *p = rb_first(&flow->t_root);\n\n\twhile (p) {\n\t\tstruct sk_buff *skb = rb_to_skb(p);\n\n\t\tp = rb_next(p);\n\t\trb_erase(&skb->rbnode, &flow->t_root);\n\t\trtnl_kfree_skbs(skb, skb);\n\t}\n\trtnl_kfree_skbs(flow->head, flow->tail);\n\tflow->head = NULL;\n\tflow->qlen = 0;\n}\n\nstatic void fq_reset(struct Qdisc *sch)\n{\n\tstruct fq_sched_data *q = qdisc_priv(sch);\n\tstruct rb_root *root;\n\tstruct rb_node *p;\n\tstruct fq_flow *f;\n\tunsigned int idx;\n\n\tsch->q.qlen = 0;\n\tsch->qstats.backlog = 0;\n\n\tfq_flow_purge(&q->internal);\n\n\tif (!q->fq_root)\n\t\treturn;\n\n\tfor (idx = 0; idx < (1U << q->fq_trees_log); idx++) {\n\t\troot = &q->fq_root[idx];\n\t\twhile ((p = rb_first(root)) != NULL) {\n\t\t\tf = rb_entry(p, struct fq_flow, fq_node);\n\t\t\trb_erase(p, root);\n\n\t\t\tfq_flow_purge(f);\n\n\t\t\tkmem_cache_free(fq_flow_cachep, f);\n\t\t}\n\t}\n\tq->new_flows.first\t= NULL;\n\tq->old_flows.first\t= NULL;\n\tq->delayed\t\t= RB_ROOT;\n\tq->flows\t\t= 0;\n\tq->inactive_flows\t= 0;\n\tq->throttled_flows\t= 0;\n}\n\nstatic void fq_rehash(struct fq_sched_data *q,\n\t\t      struct rb_root *old_array, u32 old_log,\n\t\t      struct rb_root *new_array, u32 new_log)\n{\n\tstruct rb_node *op, **np, *parent;\n\tstruct rb_root *oroot, *nroot;\n\tstruct fq_flow *of, *nf;\n\tint fcnt = 0;\n\tu32 idx;\n\n\tfor (idx = 0; idx < (1U << old_log); idx++) {\n\t\toroot = &old_array[idx];\n\t\twhile ((op = rb_first(oroot)) != NULL) {\n\t\t\trb_erase(op, oroot);\n\t\t\tof = rb_entry(op, struct fq_flow, fq_node);\n\t\t\tif (fq_gc_candidate(of)) {\n\t\t\t\tfcnt++;\n\t\t\t\tkmem_cache_free(fq_flow_cachep, of);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tnroot = &new_array[hash_ptr(of->sk, new_log)];\n\n\t\t\tnp = &nroot->rb_node;\n\t\t\tparent = NULL;\n\t\t\twhile (*np) {\n\t\t\t\tparent = *np;\n\n\t\t\t\tnf = rb_entry(parent, struct fq_flow, fq_node);\n\t\t\t\tBUG_ON(nf->sk == of->sk);\n\n\t\t\t\tif (nf->sk > of->sk)\n\t\t\t\t\tnp = &parent->rb_right;\n\t\t\t\telse\n\t\t\t\t\tnp = &parent->rb_left;\n\t\t\t}\n\n\t\t\trb_link_node(&of->fq_node, parent, np);\n\t\t\trb_insert_color(&of->fq_node, nroot);\n\t\t}\n\t}\n\tq->flows -= fcnt;\n\tq->inactive_flows -= fcnt;\n\tq->stat_gc_flows += fcnt;\n}\n\nstatic void fq_free(void *addr)\n{\n\tkvfree(addr);\n}\n\nstatic int fq_resize(struct Qdisc *sch, u32 log)\n{\n\tstruct fq_sched_data *q = qdisc_priv(sch);\n\tstruct rb_root *array;\n\tvoid *old_fq_root;\n\tu32 idx;\n\n\tif (q->fq_root && log == q->fq_trees_log)\n\t\treturn 0;\n\n\t \n\tarray = kvmalloc_node(sizeof(struct rb_root) << log, GFP_KERNEL | __GFP_RETRY_MAYFAIL,\n\t\t\t      netdev_queue_numa_node_read(sch->dev_queue));\n\tif (!array)\n\t\treturn -ENOMEM;\n\n\tfor (idx = 0; idx < (1U << log); idx++)\n\t\tarray[idx] = RB_ROOT;\n\n\tsch_tree_lock(sch);\n\n\told_fq_root = q->fq_root;\n\tif (old_fq_root)\n\t\tfq_rehash(q, old_fq_root, q->fq_trees_log, array, log);\n\n\tq->fq_root = array;\n\tq->fq_trees_log = log;\n\n\tsch_tree_unlock(sch);\n\n\tfq_free(old_fq_root);\n\n\treturn 0;\n}\n\nstatic struct netlink_range_validation iq_range = {\n\t.max = INT_MAX,\n};\n\nstatic const struct nla_policy fq_policy[TCA_FQ_MAX + 1] = {\n\t[TCA_FQ_UNSPEC]\t\t\t= { .strict_start_type = TCA_FQ_TIMER_SLACK },\n\n\t[TCA_FQ_PLIMIT]\t\t\t= { .type = NLA_U32 },\n\t[TCA_FQ_FLOW_PLIMIT]\t\t= { .type = NLA_U32 },\n\t[TCA_FQ_QUANTUM]\t\t= { .type = NLA_U32 },\n\t[TCA_FQ_INITIAL_QUANTUM]\t= NLA_POLICY_FULL_RANGE(NLA_U32, &iq_range),\n\t[TCA_FQ_RATE_ENABLE]\t\t= { .type = NLA_U32 },\n\t[TCA_FQ_FLOW_DEFAULT_RATE]\t= { .type = NLA_U32 },\n\t[TCA_FQ_FLOW_MAX_RATE]\t\t= { .type = NLA_U32 },\n\t[TCA_FQ_BUCKETS_LOG]\t\t= { .type = NLA_U32 },\n\t[TCA_FQ_FLOW_REFILL_DELAY]\t= { .type = NLA_U32 },\n\t[TCA_FQ_ORPHAN_MASK]\t\t= { .type = NLA_U32 },\n\t[TCA_FQ_LOW_RATE_THRESHOLD]\t= { .type = NLA_U32 },\n\t[TCA_FQ_CE_THRESHOLD]\t\t= { .type = NLA_U32 },\n\t[TCA_FQ_TIMER_SLACK]\t\t= { .type = NLA_U32 },\n\t[TCA_FQ_HORIZON]\t\t= { .type = NLA_U32 },\n\t[TCA_FQ_HORIZON_DROP]\t\t= { .type = NLA_U8 },\n};\n\nstatic int fq_change(struct Qdisc *sch, struct nlattr *opt,\n\t\t     struct netlink_ext_ack *extack)\n{\n\tstruct fq_sched_data *q = qdisc_priv(sch);\n\tstruct nlattr *tb[TCA_FQ_MAX + 1];\n\tint err, drop_count = 0;\n\tunsigned drop_len = 0;\n\tu32 fq_log;\n\n\terr = nla_parse_nested_deprecated(tb, TCA_FQ_MAX, opt, fq_policy,\n\t\t\t\t\t  NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tsch_tree_lock(sch);\n\n\tfq_log = q->fq_trees_log;\n\n\tif (tb[TCA_FQ_BUCKETS_LOG]) {\n\t\tu32 nval = nla_get_u32(tb[TCA_FQ_BUCKETS_LOG]);\n\n\t\tif (nval >= 1 && nval <= ilog2(256*1024))\n\t\t\tfq_log = nval;\n\t\telse\n\t\t\terr = -EINVAL;\n\t}\n\tif (tb[TCA_FQ_PLIMIT])\n\t\tsch->limit = nla_get_u32(tb[TCA_FQ_PLIMIT]);\n\n\tif (tb[TCA_FQ_FLOW_PLIMIT])\n\t\tq->flow_plimit = nla_get_u32(tb[TCA_FQ_FLOW_PLIMIT]);\n\n\tif (tb[TCA_FQ_QUANTUM]) {\n\t\tu32 quantum = nla_get_u32(tb[TCA_FQ_QUANTUM]);\n\n\t\tif (quantum > 0 && quantum <= (1 << 20)) {\n\t\t\tq->quantum = quantum;\n\t\t} else {\n\t\t\tNL_SET_ERR_MSG_MOD(extack, \"invalid quantum\");\n\t\t\terr = -EINVAL;\n\t\t}\n\t}\n\n\tif (tb[TCA_FQ_INITIAL_QUANTUM])\n\t\tq->initial_quantum = nla_get_u32(tb[TCA_FQ_INITIAL_QUANTUM]);\n\n\tif (tb[TCA_FQ_FLOW_DEFAULT_RATE])\n\t\tpr_warn_ratelimited(\"sch_fq: defrate %u ignored.\\n\",\n\t\t\t\t    nla_get_u32(tb[TCA_FQ_FLOW_DEFAULT_RATE]));\n\n\tif (tb[TCA_FQ_FLOW_MAX_RATE]) {\n\t\tu32 rate = nla_get_u32(tb[TCA_FQ_FLOW_MAX_RATE]);\n\n\t\tq->flow_max_rate = (rate == ~0U) ? ~0UL : rate;\n\t}\n\tif (tb[TCA_FQ_LOW_RATE_THRESHOLD])\n\t\tq->low_rate_threshold =\n\t\t\tnla_get_u32(tb[TCA_FQ_LOW_RATE_THRESHOLD]);\n\n\tif (tb[TCA_FQ_RATE_ENABLE]) {\n\t\tu32 enable = nla_get_u32(tb[TCA_FQ_RATE_ENABLE]);\n\n\t\tif (enable <= 1)\n\t\t\tq->rate_enable = enable;\n\t\telse\n\t\t\terr = -EINVAL;\n\t}\n\n\tif (tb[TCA_FQ_FLOW_REFILL_DELAY]) {\n\t\tu32 usecs_delay = nla_get_u32(tb[TCA_FQ_FLOW_REFILL_DELAY]) ;\n\n\t\tq->flow_refill_delay = usecs_to_jiffies(usecs_delay);\n\t}\n\n\tif (tb[TCA_FQ_ORPHAN_MASK])\n\t\tq->orphan_mask = nla_get_u32(tb[TCA_FQ_ORPHAN_MASK]);\n\n\tif (tb[TCA_FQ_CE_THRESHOLD])\n\t\tq->ce_threshold = (u64)NSEC_PER_USEC *\n\t\t\t\t  nla_get_u32(tb[TCA_FQ_CE_THRESHOLD]);\n\n\tif (tb[TCA_FQ_TIMER_SLACK])\n\t\tq->timer_slack = nla_get_u32(tb[TCA_FQ_TIMER_SLACK]);\n\n\tif (tb[TCA_FQ_HORIZON])\n\t\tq->horizon = (u64)NSEC_PER_USEC *\n\t\t\t\t  nla_get_u32(tb[TCA_FQ_HORIZON]);\n\n\tif (tb[TCA_FQ_HORIZON_DROP])\n\t\tq->horizon_drop = nla_get_u8(tb[TCA_FQ_HORIZON_DROP]);\n\n\tif (!err) {\n\n\t\tsch_tree_unlock(sch);\n\t\terr = fq_resize(sch, fq_log);\n\t\tsch_tree_lock(sch);\n\t}\n\twhile (sch->q.qlen > sch->limit) {\n\t\tstruct sk_buff *skb = fq_dequeue(sch);\n\n\t\tif (!skb)\n\t\t\tbreak;\n\t\tdrop_len += qdisc_pkt_len(skb);\n\t\trtnl_kfree_skbs(skb, skb);\n\t\tdrop_count++;\n\t}\n\tqdisc_tree_reduce_backlog(sch, drop_count, drop_len);\n\n\tsch_tree_unlock(sch);\n\treturn err;\n}\n\nstatic void fq_destroy(struct Qdisc *sch)\n{\n\tstruct fq_sched_data *q = qdisc_priv(sch);\n\n\tfq_reset(sch);\n\tfq_free(q->fq_root);\n\tqdisc_watchdog_cancel(&q->watchdog);\n}\n\nstatic int fq_init(struct Qdisc *sch, struct nlattr *opt,\n\t\t   struct netlink_ext_ack *extack)\n{\n\tstruct fq_sched_data *q = qdisc_priv(sch);\n\tint err;\n\n\tsch->limit\t\t= 10000;\n\tq->flow_plimit\t\t= 100;\n\tq->quantum\t\t= 2 * psched_mtu(qdisc_dev(sch));\n\tq->initial_quantum\t= 10 * psched_mtu(qdisc_dev(sch));\n\tq->flow_refill_delay\t= msecs_to_jiffies(40);\n\tq->flow_max_rate\t= ~0UL;\n\tq->time_next_delayed_flow = ~0ULL;\n\tq->rate_enable\t\t= 1;\n\tq->new_flows.first\t= NULL;\n\tq->old_flows.first\t= NULL;\n\tq->delayed\t\t= RB_ROOT;\n\tq->fq_root\t\t= NULL;\n\tq->fq_trees_log\t\t= ilog2(1024);\n\tq->orphan_mask\t\t= 1024 - 1;\n\tq->low_rate_threshold\t= 550000 / 8;\n\n\tq->timer_slack = 10 * NSEC_PER_USEC;  \n\n\tq->horizon = 10ULL * NSEC_PER_SEC;  \n\tq->horizon_drop = 1;  \n\n\t \n\tq->ce_threshold\t\t= (u64)NSEC_PER_USEC * ~0U;\n\n\tqdisc_watchdog_init_clockid(&q->watchdog, sch, CLOCK_MONOTONIC);\n\n\tif (opt)\n\t\terr = fq_change(sch, opt, extack);\n\telse\n\t\terr = fq_resize(sch, q->fq_trees_log);\n\n\treturn err;\n}\n\nstatic int fq_dump(struct Qdisc *sch, struct sk_buff *skb)\n{\n\tstruct fq_sched_data *q = qdisc_priv(sch);\n\tu64 ce_threshold = q->ce_threshold;\n\tu64 horizon = q->horizon;\n\tstruct nlattr *opts;\n\n\topts = nla_nest_start_noflag(skb, TCA_OPTIONS);\n\tif (opts == NULL)\n\t\tgoto nla_put_failure;\n\n\t \n\n\tdo_div(ce_threshold, NSEC_PER_USEC);\n\tdo_div(horizon, NSEC_PER_USEC);\n\n\tif (nla_put_u32(skb, TCA_FQ_PLIMIT, sch->limit) ||\n\t    nla_put_u32(skb, TCA_FQ_FLOW_PLIMIT, q->flow_plimit) ||\n\t    nla_put_u32(skb, TCA_FQ_QUANTUM, q->quantum) ||\n\t    nla_put_u32(skb, TCA_FQ_INITIAL_QUANTUM, q->initial_quantum) ||\n\t    nla_put_u32(skb, TCA_FQ_RATE_ENABLE, q->rate_enable) ||\n\t    nla_put_u32(skb, TCA_FQ_FLOW_MAX_RATE,\n\t\t\tmin_t(unsigned long, q->flow_max_rate, ~0U)) ||\n\t    nla_put_u32(skb, TCA_FQ_FLOW_REFILL_DELAY,\n\t\t\tjiffies_to_usecs(q->flow_refill_delay)) ||\n\t    nla_put_u32(skb, TCA_FQ_ORPHAN_MASK, q->orphan_mask) ||\n\t    nla_put_u32(skb, TCA_FQ_LOW_RATE_THRESHOLD,\n\t\t\tq->low_rate_threshold) ||\n\t    nla_put_u32(skb, TCA_FQ_CE_THRESHOLD, (u32)ce_threshold) ||\n\t    nla_put_u32(skb, TCA_FQ_BUCKETS_LOG, q->fq_trees_log) ||\n\t    nla_put_u32(skb, TCA_FQ_TIMER_SLACK, q->timer_slack) ||\n\t    nla_put_u32(skb, TCA_FQ_HORIZON, (u32)horizon) ||\n\t    nla_put_u8(skb, TCA_FQ_HORIZON_DROP, q->horizon_drop))\n\t\tgoto nla_put_failure;\n\n\treturn nla_nest_end(skb, opts);\n\nnla_put_failure:\n\treturn -1;\n}\n\nstatic int fq_dump_stats(struct Qdisc *sch, struct gnet_dump *d)\n{\n\tstruct fq_sched_data *q = qdisc_priv(sch);\n\tstruct tc_fq_qd_stats st;\n\n\tsch_tree_lock(sch);\n\n\tst.gc_flows\t\t  = q->stat_gc_flows;\n\tst.highprio_packets\t  = q->stat_internal_packets;\n\tst.tcp_retrans\t\t  = 0;\n\tst.throttled\t\t  = q->stat_throttled;\n\tst.flows_plimit\t\t  = q->stat_flows_plimit;\n\tst.pkts_too_long\t  = q->stat_pkts_too_long;\n\tst.allocation_errors\t  = q->stat_allocation_errors;\n\tst.time_next_delayed_flow = q->time_next_delayed_flow + q->timer_slack -\n\t\t\t\t    ktime_get_ns();\n\tst.flows\t\t  = q->flows;\n\tst.inactive_flows\t  = q->inactive_flows;\n\tst.throttled_flows\t  = q->throttled_flows;\n\tst.unthrottle_latency_ns  = min_t(unsigned long,\n\t\t\t\t\t  q->unthrottle_latency_ns, ~0U);\n\tst.ce_mark\t\t  = q->stat_ce_mark;\n\tst.horizon_drops\t  = q->stat_horizon_drops;\n\tst.horizon_caps\t\t  = q->stat_horizon_caps;\n\tsch_tree_unlock(sch);\n\n\treturn gnet_stats_copy_app(d, &st, sizeof(st));\n}\n\nstatic struct Qdisc_ops fq_qdisc_ops __read_mostly = {\n\t.id\t\t=\t\"fq\",\n\t.priv_size\t=\tsizeof(struct fq_sched_data),\n\n\t.enqueue\t=\tfq_enqueue,\n\t.dequeue\t=\tfq_dequeue,\n\t.peek\t\t=\tqdisc_peek_dequeued,\n\t.init\t\t=\tfq_init,\n\t.reset\t\t=\tfq_reset,\n\t.destroy\t=\tfq_destroy,\n\t.change\t\t=\tfq_change,\n\t.dump\t\t=\tfq_dump,\n\t.dump_stats\t=\tfq_dump_stats,\n\t.owner\t\t=\tTHIS_MODULE,\n};\n\nstatic int __init fq_module_init(void)\n{\n\tint ret;\n\n\tfq_flow_cachep = kmem_cache_create(\"fq_flow_cache\",\n\t\t\t\t\t   sizeof(struct fq_flow),\n\t\t\t\t\t   0, 0, NULL);\n\tif (!fq_flow_cachep)\n\t\treturn -ENOMEM;\n\n\tret = register_qdisc(&fq_qdisc_ops);\n\tif (ret)\n\t\tkmem_cache_destroy(fq_flow_cachep);\n\treturn ret;\n}\n\nstatic void __exit fq_module_exit(void)\n{\n\tunregister_qdisc(&fq_qdisc_ops);\n\tkmem_cache_destroy(fq_flow_cachep);\n}\n\nmodule_init(fq_module_init)\nmodule_exit(fq_module_exit)\nMODULE_AUTHOR(\"Eric Dumazet\");\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"Fair Queue Packet Scheduler\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}