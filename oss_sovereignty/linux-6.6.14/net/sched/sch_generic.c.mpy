{
  "module_name": "sch_generic.c",
  "hash_id": "f423acf00694316e4d86bc61d218d1fa799666a37c4e7137a2610df1043e4ebf",
  "original_prompt": "Ingested from linux-6.6.14/net/sched/sch_generic.c",
  "human_readable_source": "\n \n\n#include <linux/bitops.h>\n#include <linux/module.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/sched.h>\n#include <linux/string.h>\n#include <linux/errno.h>\n#include <linux/netdevice.h>\n#include <linux/skbuff.h>\n#include <linux/rtnetlink.h>\n#include <linux/init.h>\n#include <linux/rcupdate.h>\n#include <linux/list.h>\n#include <linux/slab.h>\n#include <linux/if_vlan.h>\n#include <linux/skb_array.h>\n#include <linux/if_macvlan.h>\n#include <net/sch_generic.h>\n#include <net/pkt_sched.h>\n#include <net/dst.h>\n#include <trace/events/qdisc.h>\n#include <trace/events/net.h>\n#include <net/xfrm.h>\n\n \nconst struct Qdisc_ops *default_qdisc_ops = &pfifo_fast_ops;\nEXPORT_SYMBOL(default_qdisc_ops);\n\nstatic void qdisc_maybe_clear_missed(struct Qdisc *q,\n\t\t\t\t     const struct netdev_queue *txq)\n{\n\tclear_bit(__QDISC_STATE_MISSED, &q->state);\n\n\t \n\tsmp_mb__after_atomic();\n\n\t \n\tif (!netif_xmit_frozen_or_stopped(txq))\n\t\tset_bit(__QDISC_STATE_MISSED, &q->state);\n\telse\n\t\tset_bit(__QDISC_STATE_DRAINING, &q->state);\n}\n\n \n\n \n\n#define SKB_XOFF_MAGIC ((struct sk_buff *)1UL)\n\nstatic inline struct sk_buff *__skb_dequeue_bad_txq(struct Qdisc *q)\n{\n\tconst struct netdev_queue *txq = q->dev_queue;\n\tspinlock_t *lock = NULL;\n\tstruct sk_buff *skb;\n\n\tif (q->flags & TCQ_F_NOLOCK) {\n\t\tlock = qdisc_lock(q);\n\t\tspin_lock(lock);\n\t}\n\n\tskb = skb_peek(&q->skb_bad_txq);\n\tif (skb) {\n\t\t \n\t\ttxq = skb_get_tx_queue(txq->dev, skb);\n\t\tif (!netif_xmit_frozen_or_stopped(txq)) {\n\t\t\tskb = __skb_dequeue(&q->skb_bad_txq);\n\t\t\tif (qdisc_is_percpu_stats(q)) {\n\t\t\t\tqdisc_qstats_cpu_backlog_dec(q, skb);\n\t\t\t\tqdisc_qstats_cpu_qlen_dec(q);\n\t\t\t} else {\n\t\t\t\tqdisc_qstats_backlog_dec(q, skb);\n\t\t\t\tq->q.qlen--;\n\t\t\t}\n\t\t} else {\n\t\t\tskb = SKB_XOFF_MAGIC;\n\t\t\tqdisc_maybe_clear_missed(q, txq);\n\t\t}\n\t}\n\n\tif (lock)\n\t\tspin_unlock(lock);\n\n\treturn skb;\n}\n\nstatic inline struct sk_buff *qdisc_dequeue_skb_bad_txq(struct Qdisc *q)\n{\n\tstruct sk_buff *skb = skb_peek(&q->skb_bad_txq);\n\n\tif (unlikely(skb))\n\t\tskb = __skb_dequeue_bad_txq(q);\n\n\treturn skb;\n}\n\nstatic inline void qdisc_enqueue_skb_bad_txq(struct Qdisc *q,\n\t\t\t\t\t     struct sk_buff *skb)\n{\n\tspinlock_t *lock = NULL;\n\n\tif (q->flags & TCQ_F_NOLOCK) {\n\t\tlock = qdisc_lock(q);\n\t\tspin_lock(lock);\n\t}\n\n\t__skb_queue_tail(&q->skb_bad_txq, skb);\n\n\tif (qdisc_is_percpu_stats(q)) {\n\t\tqdisc_qstats_cpu_backlog_inc(q, skb);\n\t\tqdisc_qstats_cpu_qlen_inc(q);\n\t} else {\n\t\tqdisc_qstats_backlog_inc(q, skb);\n\t\tq->q.qlen++;\n\t}\n\n\tif (lock)\n\t\tspin_unlock(lock);\n}\n\nstatic inline void dev_requeue_skb(struct sk_buff *skb, struct Qdisc *q)\n{\n\tspinlock_t *lock = NULL;\n\n\tif (q->flags & TCQ_F_NOLOCK) {\n\t\tlock = qdisc_lock(q);\n\t\tspin_lock(lock);\n\t}\n\n\twhile (skb) {\n\t\tstruct sk_buff *next = skb->next;\n\n\t\t__skb_queue_tail(&q->gso_skb, skb);\n\n\t\t \n\t\tif (qdisc_is_percpu_stats(q)) {\n\t\t\tqdisc_qstats_cpu_requeues_inc(q);\n\t\t\tqdisc_qstats_cpu_backlog_inc(q, skb);\n\t\t\tqdisc_qstats_cpu_qlen_inc(q);\n\t\t} else {\n\t\t\tq->qstats.requeues++;\n\t\t\tqdisc_qstats_backlog_inc(q, skb);\n\t\t\tq->q.qlen++;\n\t\t}\n\n\t\tskb = next;\n\t}\n\n\tif (lock) {\n\t\tspin_unlock(lock);\n\t\tset_bit(__QDISC_STATE_MISSED, &q->state);\n\t} else {\n\t\t__netif_schedule(q);\n\t}\n}\n\nstatic void try_bulk_dequeue_skb(struct Qdisc *q,\n\t\t\t\t struct sk_buff *skb,\n\t\t\t\t const struct netdev_queue *txq,\n\t\t\t\t int *packets)\n{\n\tint bytelimit = qdisc_avail_bulklimit(txq) - skb->len;\n\n\twhile (bytelimit > 0) {\n\t\tstruct sk_buff *nskb = q->dequeue(q);\n\n\t\tif (!nskb)\n\t\t\tbreak;\n\n\t\tbytelimit -= nskb->len;  \n\t\tskb->next = nskb;\n\t\tskb = nskb;\n\t\t(*packets)++;  \n\t}\n\tskb_mark_not_on_list(skb);\n}\n\n \nstatic void try_bulk_dequeue_skb_slow(struct Qdisc *q,\n\t\t\t\t      struct sk_buff *skb,\n\t\t\t\t      int *packets)\n{\n\tint mapping = skb_get_queue_mapping(skb);\n\tstruct sk_buff *nskb;\n\tint cnt = 0;\n\n\tdo {\n\t\tnskb = q->dequeue(q);\n\t\tif (!nskb)\n\t\t\tbreak;\n\t\tif (unlikely(skb_get_queue_mapping(nskb) != mapping)) {\n\t\t\tqdisc_enqueue_skb_bad_txq(q, nskb);\n\t\t\tbreak;\n\t\t}\n\t\tskb->next = nskb;\n\t\tskb = nskb;\n\t} while (++cnt < 8);\n\t(*packets) += cnt;\n\tskb_mark_not_on_list(skb);\n}\n\n \nstatic struct sk_buff *dequeue_skb(struct Qdisc *q, bool *validate,\n\t\t\t\t   int *packets)\n{\n\tconst struct netdev_queue *txq = q->dev_queue;\n\tstruct sk_buff *skb = NULL;\n\n\t*packets = 1;\n\tif (unlikely(!skb_queue_empty(&q->gso_skb))) {\n\t\tspinlock_t *lock = NULL;\n\n\t\tif (q->flags & TCQ_F_NOLOCK) {\n\t\t\tlock = qdisc_lock(q);\n\t\t\tspin_lock(lock);\n\t\t}\n\n\t\tskb = skb_peek(&q->gso_skb);\n\n\t\t \n\t\tif (!skb) {\n\t\t\tif (lock)\n\t\t\t\tspin_unlock(lock);\n\t\t\tgoto validate;\n\t\t}\n\n\t\t \n\t\t*validate = false;\n\t\tif (xfrm_offload(skb))\n\t\t\t*validate = true;\n\t\t \n\t\ttxq = skb_get_tx_queue(txq->dev, skb);\n\t\tif (!netif_xmit_frozen_or_stopped(txq)) {\n\t\t\tskb = __skb_dequeue(&q->gso_skb);\n\t\t\tif (qdisc_is_percpu_stats(q)) {\n\t\t\t\tqdisc_qstats_cpu_backlog_dec(q, skb);\n\t\t\t\tqdisc_qstats_cpu_qlen_dec(q);\n\t\t\t} else {\n\t\t\t\tqdisc_qstats_backlog_dec(q, skb);\n\t\t\t\tq->q.qlen--;\n\t\t\t}\n\t\t} else {\n\t\t\tskb = NULL;\n\t\t\tqdisc_maybe_clear_missed(q, txq);\n\t\t}\n\t\tif (lock)\n\t\t\tspin_unlock(lock);\n\t\tgoto trace;\n\t}\nvalidate:\n\t*validate = true;\n\n\tif ((q->flags & TCQ_F_ONETXQUEUE) &&\n\t    netif_xmit_frozen_or_stopped(txq)) {\n\t\tqdisc_maybe_clear_missed(q, txq);\n\t\treturn skb;\n\t}\n\n\tskb = qdisc_dequeue_skb_bad_txq(q);\n\tif (unlikely(skb)) {\n\t\tif (skb == SKB_XOFF_MAGIC)\n\t\t\treturn NULL;\n\t\tgoto bulk;\n\t}\n\tskb = q->dequeue(q);\n\tif (skb) {\nbulk:\n\t\tif (qdisc_may_bulk(q))\n\t\t\ttry_bulk_dequeue_skb(q, skb, txq, packets);\n\t\telse\n\t\t\ttry_bulk_dequeue_skb_slow(q, skb, packets);\n\t}\ntrace:\n\ttrace_qdisc_dequeue(q, txq, *packets, skb);\n\treturn skb;\n}\n\n \nbool sch_direct_xmit(struct sk_buff *skb, struct Qdisc *q,\n\t\t     struct net_device *dev, struct netdev_queue *txq,\n\t\t     spinlock_t *root_lock, bool validate)\n{\n\tint ret = NETDEV_TX_BUSY;\n\tbool again = false;\n\n\t \n\tif (root_lock)\n\t\tspin_unlock(root_lock);\n\n\t \n\tif (validate)\n\t\tskb = validate_xmit_skb_list(skb, dev, &again);\n\n#ifdef CONFIG_XFRM_OFFLOAD\n\tif (unlikely(again)) {\n\t\tif (root_lock)\n\t\t\tspin_lock(root_lock);\n\n\t\tdev_requeue_skb(skb, q);\n\t\treturn false;\n\t}\n#endif\n\n\tif (likely(skb)) {\n\t\tHARD_TX_LOCK(dev, txq, smp_processor_id());\n\t\tif (!netif_xmit_frozen_or_stopped(txq))\n\t\t\tskb = dev_hard_start_xmit(skb, dev, txq, &ret);\n\t\telse\n\t\t\tqdisc_maybe_clear_missed(q, txq);\n\n\t\tHARD_TX_UNLOCK(dev, txq);\n\t} else {\n\t\tif (root_lock)\n\t\t\tspin_lock(root_lock);\n\t\treturn true;\n\t}\n\n\tif (root_lock)\n\t\tspin_lock(root_lock);\n\n\tif (!dev_xmit_complete(ret)) {\n\t\t \n\t\tif (unlikely(ret != NETDEV_TX_BUSY))\n\t\t\tnet_warn_ratelimited(\"BUG %s code %d qlen %d\\n\",\n\t\t\t\t\t     dev->name, ret, q->q.qlen);\n\n\t\tdev_requeue_skb(skb, q);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\n \nstatic inline bool qdisc_restart(struct Qdisc *q, int *packets)\n{\n\tspinlock_t *root_lock = NULL;\n\tstruct netdev_queue *txq;\n\tstruct net_device *dev;\n\tstruct sk_buff *skb;\n\tbool validate;\n\n\t \n\tskb = dequeue_skb(q, &validate, packets);\n\tif (unlikely(!skb))\n\t\treturn false;\n\n\tif (!(q->flags & TCQ_F_NOLOCK))\n\t\troot_lock = qdisc_lock(q);\n\n\tdev = qdisc_dev(q);\n\ttxq = skb_get_tx_queue(dev, skb);\n\n\treturn sch_direct_xmit(skb, q, dev, txq, root_lock, validate);\n}\n\nvoid __qdisc_run(struct Qdisc *q)\n{\n\tint quota = READ_ONCE(dev_tx_weight);\n\tint packets;\n\n\twhile (qdisc_restart(q, &packets)) {\n\t\tquota -= packets;\n\t\tif (quota <= 0) {\n\t\t\tif (q->flags & TCQ_F_NOLOCK)\n\t\t\t\tset_bit(__QDISC_STATE_MISSED, &q->state);\n\t\t\telse\n\t\t\t\t__netif_schedule(q);\n\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nunsigned long dev_trans_start(struct net_device *dev)\n{\n\tunsigned long res = READ_ONCE(netdev_get_tx_queue(dev, 0)->trans_start);\n\tunsigned long val;\n\tunsigned int i;\n\n\tfor (i = 1; i < dev->num_tx_queues; i++) {\n\t\tval = READ_ONCE(netdev_get_tx_queue(dev, i)->trans_start);\n\t\tif (val && time_after(val, res))\n\t\t\tres = val;\n\t}\n\n\treturn res;\n}\nEXPORT_SYMBOL(dev_trans_start);\n\nstatic void netif_freeze_queues(struct net_device *dev)\n{\n\tunsigned int i;\n\tint cpu;\n\n\tcpu = smp_processor_id();\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\n\t\t \n\t\t__netif_tx_lock(txq, cpu);\n\t\tset_bit(__QUEUE_STATE_FROZEN, &txq->state);\n\t\t__netif_tx_unlock(txq);\n\t}\n}\n\nvoid netif_tx_lock(struct net_device *dev)\n{\n\tspin_lock(&dev->tx_global_lock);\n\tnetif_freeze_queues(dev);\n}\nEXPORT_SYMBOL(netif_tx_lock);\n\nstatic void netif_unfreeze_queues(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\n\t\t \n\t\tclear_bit(__QUEUE_STATE_FROZEN, &txq->state);\n\t\tnetif_schedule_queue(txq);\n\t}\n}\n\nvoid netif_tx_unlock(struct net_device *dev)\n{\n\tnetif_unfreeze_queues(dev);\n\tspin_unlock(&dev->tx_global_lock);\n}\nEXPORT_SYMBOL(netif_tx_unlock);\n\nstatic void dev_watchdog(struct timer_list *t)\n{\n\tstruct net_device *dev = from_timer(dev, t, watchdog_timer);\n\tbool release = true;\n\n\tspin_lock(&dev->tx_global_lock);\n\tif (!qdisc_tx_is_noop(dev)) {\n\t\tif (netif_device_present(dev) &&\n\t\t    netif_running(dev) &&\n\t\t    netif_carrier_ok(dev)) {\n\t\t\tunsigned int timedout_ms = 0;\n\t\t\tunsigned int i;\n\t\t\tunsigned long trans_start;\n\n\t\t\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\t\t\tstruct netdev_queue *txq;\n\n\t\t\t\ttxq = netdev_get_tx_queue(dev, i);\n\t\t\t\ttrans_start = READ_ONCE(txq->trans_start);\n\t\t\t\tif (netif_xmit_stopped(txq) &&\n\t\t\t\t    time_after(jiffies, (trans_start +\n\t\t\t\t\t\t\t dev->watchdog_timeo))) {\n\t\t\t\t\ttimedout_ms = jiffies_to_msecs(jiffies - trans_start);\n\t\t\t\t\tatomic_long_inc(&txq->trans_timeout);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (unlikely(timedout_ms)) {\n\t\t\t\ttrace_net_dev_xmit_timeout(dev, i);\n\t\t\t\tWARN_ONCE(1, \"NETDEV WATCHDOG: %s (%s): transmit queue %u timed out %u ms\\n\",\n\t\t\t\t\t  dev->name, netdev_drivername(dev), i, timedout_ms);\n\t\t\t\tnetif_freeze_queues(dev);\n\t\t\t\tdev->netdev_ops->ndo_tx_timeout(dev, i);\n\t\t\t\tnetif_unfreeze_queues(dev);\n\t\t\t}\n\t\t\tif (!mod_timer(&dev->watchdog_timer,\n\t\t\t\t       round_jiffies(jiffies +\n\t\t\t\t\t\t     dev->watchdog_timeo)))\n\t\t\t\trelease = false;\n\t\t}\n\t}\n\tspin_unlock(&dev->tx_global_lock);\n\n\tif (release)\n\t\tnetdev_put(dev, &dev->watchdog_dev_tracker);\n}\n\nvoid __netdev_watchdog_up(struct net_device *dev)\n{\n\tif (dev->netdev_ops->ndo_tx_timeout) {\n\t\tif (dev->watchdog_timeo <= 0)\n\t\t\tdev->watchdog_timeo = 5*HZ;\n\t\tif (!mod_timer(&dev->watchdog_timer,\n\t\t\t       round_jiffies(jiffies + dev->watchdog_timeo)))\n\t\t\tnetdev_hold(dev, &dev->watchdog_dev_tracker,\n\t\t\t\t    GFP_ATOMIC);\n\t}\n}\nEXPORT_SYMBOL_GPL(__netdev_watchdog_up);\n\nstatic void dev_watchdog_up(struct net_device *dev)\n{\n\t__netdev_watchdog_up(dev);\n}\n\nstatic void dev_watchdog_down(struct net_device *dev)\n{\n\tnetif_tx_lock_bh(dev);\n\tif (del_timer(&dev->watchdog_timer))\n\t\tnetdev_put(dev, &dev->watchdog_dev_tracker);\n\tnetif_tx_unlock_bh(dev);\n}\n\n \nvoid netif_carrier_on(struct net_device *dev)\n{\n\tif (test_and_clear_bit(__LINK_STATE_NOCARRIER, &dev->state)) {\n\t\tif (dev->reg_state == NETREG_UNINITIALIZED)\n\t\t\treturn;\n\t\tatomic_inc(&dev->carrier_up_count);\n\t\tlinkwatch_fire_event(dev);\n\t\tif (netif_running(dev))\n\t\t\t__netdev_watchdog_up(dev);\n\t}\n}\nEXPORT_SYMBOL(netif_carrier_on);\n\n \nvoid netif_carrier_off(struct net_device *dev)\n{\n\tif (!test_and_set_bit(__LINK_STATE_NOCARRIER, &dev->state)) {\n\t\tif (dev->reg_state == NETREG_UNINITIALIZED)\n\t\t\treturn;\n\t\tatomic_inc(&dev->carrier_down_count);\n\t\tlinkwatch_fire_event(dev);\n\t}\n}\nEXPORT_SYMBOL(netif_carrier_off);\n\n \nvoid netif_carrier_event(struct net_device *dev)\n{\n\tif (dev->reg_state == NETREG_UNINITIALIZED)\n\t\treturn;\n\tatomic_inc(&dev->carrier_up_count);\n\tatomic_inc(&dev->carrier_down_count);\n\tlinkwatch_fire_event(dev);\n}\nEXPORT_SYMBOL_GPL(netif_carrier_event);\n\n \n\nstatic int noop_enqueue(struct sk_buff *skb, struct Qdisc *qdisc,\n\t\t\tstruct sk_buff **to_free)\n{\n\t__qdisc_drop(skb, to_free);\n\treturn NET_XMIT_CN;\n}\n\nstatic struct sk_buff *noop_dequeue(struct Qdisc *qdisc)\n{\n\treturn NULL;\n}\n\nstruct Qdisc_ops noop_qdisc_ops __read_mostly = {\n\t.id\t\t=\t\"noop\",\n\t.priv_size\t=\t0,\n\t.enqueue\t=\tnoop_enqueue,\n\t.dequeue\t=\tnoop_dequeue,\n\t.peek\t\t=\tnoop_dequeue,\n\t.owner\t\t=\tTHIS_MODULE,\n};\n\nstatic struct netdev_queue noop_netdev_queue = {\n\tRCU_POINTER_INITIALIZER(qdisc, &noop_qdisc),\n\tRCU_POINTER_INITIALIZER(qdisc_sleeping, &noop_qdisc),\n};\n\nstruct Qdisc noop_qdisc = {\n\t.enqueue\t=\tnoop_enqueue,\n\t.dequeue\t=\tnoop_dequeue,\n\t.flags\t\t=\tTCQ_F_BUILTIN,\n\t.ops\t\t=\t&noop_qdisc_ops,\n\t.q.lock\t\t=\t__SPIN_LOCK_UNLOCKED(noop_qdisc.q.lock),\n\t.dev_queue\t=\t&noop_netdev_queue,\n\t.busylock\t=\t__SPIN_LOCK_UNLOCKED(noop_qdisc.busylock),\n\t.gso_skb = {\n\t\t.next = (struct sk_buff *)&noop_qdisc.gso_skb,\n\t\t.prev = (struct sk_buff *)&noop_qdisc.gso_skb,\n\t\t.qlen = 0,\n\t\t.lock = __SPIN_LOCK_UNLOCKED(noop_qdisc.gso_skb.lock),\n\t},\n\t.skb_bad_txq = {\n\t\t.next = (struct sk_buff *)&noop_qdisc.skb_bad_txq,\n\t\t.prev = (struct sk_buff *)&noop_qdisc.skb_bad_txq,\n\t\t.qlen = 0,\n\t\t.lock = __SPIN_LOCK_UNLOCKED(noop_qdisc.skb_bad_txq.lock),\n\t},\n};\nEXPORT_SYMBOL(noop_qdisc);\n\nstatic int noqueue_init(struct Qdisc *qdisc, struct nlattr *opt,\n\t\t\tstruct netlink_ext_ack *extack)\n{\n\t \n\tqdisc->enqueue = NULL;\n\treturn 0;\n}\n\nstruct Qdisc_ops noqueue_qdisc_ops __read_mostly = {\n\t.id\t\t=\t\"noqueue\",\n\t.priv_size\t=\t0,\n\t.init\t\t=\tnoqueue_init,\n\t.enqueue\t=\tnoop_enqueue,\n\t.dequeue\t=\tnoop_dequeue,\n\t.peek\t\t=\tnoop_dequeue,\n\t.owner\t\t=\tTHIS_MODULE,\n};\n\nstatic const u8 prio2band[TC_PRIO_MAX + 1] = {\n\t1, 2, 2, 2, 1, 2, 0, 0 , 1, 1, 1, 1, 1, 1, 1, 1\n};\n\n \n\n#define PFIFO_FAST_BANDS 3\n\n \nstruct pfifo_fast_priv {\n\tstruct skb_array q[PFIFO_FAST_BANDS];\n};\n\nstatic inline struct skb_array *band2list(struct pfifo_fast_priv *priv,\n\t\t\t\t\t  int band)\n{\n\treturn &priv->q[band];\n}\n\nstatic int pfifo_fast_enqueue(struct sk_buff *skb, struct Qdisc *qdisc,\n\t\t\t      struct sk_buff **to_free)\n{\n\tint band = prio2band[skb->priority & TC_PRIO_MAX];\n\tstruct pfifo_fast_priv *priv = qdisc_priv(qdisc);\n\tstruct skb_array *q = band2list(priv, band);\n\tunsigned int pkt_len = qdisc_pkt_len(skb);\n\tint err;\n\n\terr = skb_array_produce(q, skb);\n\n\tif (unlikely(err)) {\n\t\tif (qdisc_is_percpu_stats(qdisc))\n\t\t\treturn qdisc_drop_cpu(skb, qdisc, to_free);\n\t\telse\n\t\t\treturn qdisc_drop(skb, qdisc, to_free);\n\t}\n\n\tqdisc_update_stats_at_enqueue(qdisc, pkt_len);\n\treturn NET_XMIT_SUCCESS;\n}\n\nstatic struct sk_buff *pfifo_fast_dequeue(struct Qdisc *qdisc)\n{\n\tstruct pfifo_fast_priv *priv = qdisc_priv(qdisc);\n\tstruct sk_buff *skb = NULL;\n\tbool need_retry = true;\n\tint band;\n\nretry:\n\tfor (band = 0; band < PFIFO_FAST_BANDS && !skb; band++) {\n\t\tstruct skb_array *q = band2list(priv, band);\n\n\t\tif (__skb_array_empty(q))\n\t\t\tcontinue;\n\n\t\tskb = __skb_array_consume(q);\n\t}\n\tif (likely(skb)) {\n\t\tqdisc_update_stats_at_dequeue(qdisc, skb);\n\t} else if (need_retry &&\n\t\t   READ_ONCE(qdisc->state) & QDISC_STATE_NON_EMPTY) {\n\t\t \n\t\tclear_bit(__QDISC_STATE_MISSED, &qdisc->state);\n\t\tclear_bit(__QDISC_STATE_DRAINING, &qdisc->state);\n\n\t\t \n\t\tsmp_mb__after_atomic();\n\n\t\tneed_retry = false;\n\n\t\tgoto retry;\n\t}\n\n\treturn skb;\n}\n\nstatic struct sk_buff *pfifo_fast_peek(struct Qdisc *qdisc)\n{\n\tstruct pfifo_fast_priv *priv = qdisc_priv(qdisc);\n\tstruct sk_buff *skb = NULL;\n\tint band;\n\n\tfor (band = 0; band < PFIFO_FAST_BANDS && !skb; band++) {\n\t\tstruct skb_array *q = band2list(priv, band);\n\n\t\tskb = __skb_array_peek(q);\n\t}\n\n\treturn skb;\n}\n\nstatic void pfifo_fast_reset(struct Qdisc *qdisc)\n{\n\tint i, band;\n\tstruct pfifo_fast_priv *priv = qdisc_priv(qdisc);\n\n\tfor (band = 0; band < PFIFO_FAST_BANDS; band++) {\n\t\tstruct skb_array *q = band2list(priv, band);\n\t\tstruct sk_buff *skb;\n\n\t\t \n\t\tif (!q->ring.queue)\n\t\t\tcontinue;\n\n\t\twhile ((skb = __skb_array_consume(q)) != NULL)\n\t\t\tkfree_skb(skb);\n\t}\n\n\tif (qdisc_is_percpu_stats(qdisc)) {\n\t\tfor_each_possible_cpu(i) {\n\t\t\tstruct gnet_stats_queue *q;\n\n\t\t\tq = per_cpu_ptr(qdisc->cpu_qstats, i);\n\t\t\tq->backlog = 0;\n\t\t\tq->qlen = 0;\n\t\t}\n\t}\n}\n\nstatic int pfifo_fast_dump(struct Qdisc *qdisc, struct sk_buff *skb)\n{\n\tstruct tc_prio_qopt opt = { .bands = PFIFO_FAST_BANDS };\n\n\tmemcpy(&opt.priomap, prio2band, TC_PRIO_MAX + 1);\n\tif (nla_put(skb, TCA_OPTIONS, sizeof(opt), &opt))\n\t\tgoto nla_put_failure;\n\treturn skb->len;\n\nnla_put_failure:\n\treturn -1;\n}\n\nstatic int pfifo_fast_init(struct Qdisc *qdisc, struct nlattr *opt,\n\t\t\t   struct netlink_ext_ack *extack)\n{\n\tunsigned int qlen = qdisc_dev(qdisc)->tx_queue_len;\n\tstruct pfifo_fast_priv *priv = qdisc_priv(qdisc);\n\tint prio;\n\n\t \n\tif (!qlen)\n\t\treturn -EINVAL;\n\n\tfor (prio = 0; prio < PFIFO_FAST_BANDS; prio++) {\n\t\tstruct skb_array *q = band2list(priv, prio);\n\t\tint err;\n\n\t\terr = skb_array_init(q, qlen, GFP_KERNEL);\n\t\tif (err)\n\t\t\treturn -ENOMEM;\n\t}\n\n\t \n\tqdisc->flags |= TCQ_F_CAN_BYPASS;\n\treturn 0;\n}\n\nstatic void pfifo_fast_destroy(struct Qdisc *sch)\n{\n\tstruct pfifo_fast_priv *priv = qdisc_priv(sch);\n\tint prio;\n\n\tfor (prio = 0; prio < PFIFO_FAST_BANDS; prio++) {\n\t\tstruct skb_array *q = band2list(priv, prio);\n\n\t\t \n\t\tif (!q->ring.queue)\n\t\t\tcontinue;\n\t\t \n\t\tptr_ring_cleanup(&q->ring, NULL);\n\t}\n}\n\nstatic int pfifo_fast_change_tx_queue_len(struct Qdisc *sch,\n\t\t\t\t\t  unsigned int new_len)\n{\n\tstruct pfifo_fast_priv *priv = qdisc_priv(sch);\n\tstruct skb_array *bands[PFIFO_FAST_BANDS];\n\tint prio;\n\n\tfor (prio = 0; prio < PFIFO_FAST_BANDS; prio++) {\n\t\tstruct skb_array *q = band2list(priv, prio);\n\n\t\tbands[prio] = q;\n\t}\n\n\treturn skb_array_resize_multiple(bands, PFIFO_FAST_BANDS, new_len,\n\t\t\t\t\t GFP_KERNEL);\n}\n\nstruct Qdisc_ops pfifo_fast_ops __read_mostly = {\n\t.id\t\t=\t\"pfifo_fast\",\n\t.priv_size\t=\tsizeof(struct pfifo_fast_priv),\n\t.enqueue\t=\tpfifo_fast_enqueue,\n\t.dequeue\t=\tpfifo_fast_dequeue,\n\t.peek\t\t=\tpfifo_fast_peek,\n\t.init\t\t=\tpfifo_fast_init,\n\t.destroy\t=\tpfifo_fast_destroy,\n\t.reset\t\t=\tpfifo_fast_reset,\n\t.dump\t\t=\tpfifo_fast_dump,\n\t.change_tx_queue_len =  pfifo_fast_change_tx_queue_len,\n\t.owner\t\t=\tTHIS_MODULE,\n\t.static_flags\t=\tTCQ_F_NOLOCK | TCQ_F_CPUSTATS,\n};\nEXPORT_SYMBOL(pfifo_fast_ops);\n\nstatic struct lock_class_key qdisc_tx_busylock;\n\nstruct Qdisc *qdisc_alloc(struct netdev_queue *dev_queue,\n\t\t\t  const struct Qdisc_ops *ops,\n\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct Qdisc *sch;\n\tunsigned int size = sizeof(*sch) + ops->priv_size;\n\tint err = -ENOBUFS;\n\tstruct net_device *dev;\n\n\tif (!dev_queue) {\n\t\tNL_SET_ERR_MSG(extack, \"No device queue given\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\n\tdev = dev_queue->dev;\n\tsch = kzalloc_node(size, GFP_KERNEL, netdev_queue_numa_node_read(dev_queue));\n\n\tif (!sch)\n\t\tgoto errout;\n\t__skb_queue_head_init(&sch->gso_skb);\n\t__skb_queue_head_init(&sch->skb_bad_txq);\n\tgnet_stats_basic_sync_init(&sch->bstats);\n\tspin_lock_init(&sch->q.lock);\n\n\tif (ops->static_flags & TCQ_F_CPUSTATS) {\n\t\tsch->cpu_bstats =\n\t\t\tnetdev_alloc_pcpu_stats(struct gnet_stats_basic_sync);\n\t\tif (!sch->cpu_bstats)\n\t\t\tgoto errout1;\n\n\t\tsch->cpu_qstats = alloc_percpu(struct gnet_stats_queue);\n\t\tif (!sch->cpu_qstats) {\n\t\t\tfree_percpu(sch->cpu_bstats);\n\t\t\tgoto errout1;\n\t\t}\n\t}\n\n\tspin_lock_init(&sch->busylock);\n\tlockdep_set_class(&sch->busylock,\n\t\t\t  dev->qdisc_tx_busylock ?: &qdisc_tx_busylock);\n\n\t \n\tspin_lock_init(&sch->seqlock);\n\tlockdep_set_class(&sch->seqlock,\n\t\t\t  dev->qdisc_tx_busylock ?: &qdisc_tx_busylock);\n\n\tsch->ops = ops;\n\tsch->flags = ops->static_flags;\n\tsch->enqueue = ops->enqueue;\n\tsch->dequeue = ops->dequeue;\n\tsch->dev_queue = dev_queue;\n\tnetdev_hold(dev, &sch->dev_tracker, GFP_KERNEL);\n\trefcount_set(&sch->refcnt, 1);\n\n\treturn sch;\nerrout1:\n\tkfree(sch);\nerrout:\n\treturn ERR_PTR(err);\n}\n\nstruct Qdisc *qdisc_create_dflt(struct netdev_queue *dev_queue,\n\t\t\t\tconst struct Qdisc_ops *ops,\n\t\t\t\tunsigned int parentid,\n\t\t\t\tstruct netlink_ext_ack *extack)\n{\n\tstruct Qdisc *sch;\n\n\tif (!try_module_get(ops->owner)) {\n\t\tNL_SET_ERR_MSG(extack, \"Failed to increase module reference counter\");\n\t\treturn NULL;\n\t}\n\n\tsch = qdisc_alloc(dev_queue, ops, extack);\n\tif (IS_ERR(sch)) {\n\t\tmodule_put(ops->owner);\n\t\treturn NULL;\n\t}\n\tsch->parent = parentid;\n\n\tif (!ops->init || ops->init(sch, NULL, extack) == 0) {\n\t\ttrace_qdisc_create(ops, dev_queue->dev, parentid);\n\t\treturn sch;\n\t}\n\n\tqdisc_put(sch);\n\treturn NULL;\n}\nEXPORT_SYMBOL(qdisc_create_dflt);\n\n \n\nvoid qdisc_reset(struct Qdisc *qdisc)\n{\n\tconst struct Qdisc_ops *ops = qdisc->ops;\n\n\ttrace_qdisc_reset(qdisc);\n\n\tif (ops->reset)\n\t\tops->reset(qdisc);\n\n\t__skb_queue_purge(&qdisc->gso_skb);\n\t__skb_queue_purge(&qdisc->skb_bad_txq);\n\n\tqdisc->q.qlen = 0;\n\tqdisc->qstats.backlog = 0;\n}\nEXPORT_SYMBOL(qdisc_reset);\n\nvoid qdisc_free(struct Qdisc *qdisc)\n{\n\tif (qdisc_is_percpu_stats(qdisc)) {\n\t\tfree_percpu(qdisc->cpu_bstats);\n\t\tfree_percpu(qdisc->cpu_qstats);\n\t}\n\n\tkfree(qdisc);\n}\n\nstatic void qdisc_free_cb(struct rcu_head *head)\n{\n\tstruct Qdisc *q = container_of(head, struct Qdisc, rcu);\n\n\tqdisc_free(q);\n}\n\nstatic void __qdisc_destroy(struct Qdisc *qdisc)\n{\n\tconst struct Qdisc_ops  *ops = qdisc->ops;\n\n#ifdef CONFIG_NET_SCHED\n\tqdisc_hash_del(qdisc);\n\n\tqdisc_put_stab(rtnl_dereference(qdisc->stab));\n#endif\n\tgen_kill_estimator(&qdisc->rate_est);\n\n\tqdisc_reset(qdisc);\n\n\tif (ops->destroy)\n\t\tops->destroy(qdisc);\n\n\tmodule_put(ops->owner);\n\tnetdev_put(qdisc_dev(qdisc), &qdisc->dev_tracker);\n\n\ttrace_qdisc_destroy(qdisc);\n\n\tcall_rcu(&qdisc->rcu, qdisc_free_cb);\n}\n\nvoid qdisc_destroy(struct Qdisc *qdisc)\n{\n\tif (qdisc->flags & TCQ_F_BUILTIN)\n\t\treturn;\n\n\t__qdisc_destroy(qdisc);\n}\n\nvoid qdisc_put(struct Qdisc *qdisc)\n{\n\tif (!qdisc)\n\t\treturn;\n\n\tif (qdisc->flags & TCQ_F_BUILTIN ||\n\t    !refcount_dec_and_test(&qdisc->refcnt))\n\t\treturn;\n\n\t__qdisc_destroy(qdisc);\n}\nEXPORT_SYMBOL(qdisc_put);\n\n \n\nvoid qdisc_put_unlocked(struct Qdisc *qdisc)\n{\n\tif (qdisc->flags & TCQ_F_BUILTIN ||\n\t    !refcount_dec_and_rtnl_lock(&qdisc->refcnt))\n\t\treturn;\n\n\t__qdisc_destroy(qdisc);\n\trtnl_unlock();\n}\nEXPORT_SYMBOL(qdisc_put_unlocked);\n\n \nstruct Qdisc *dev_graft_qdisc(struct netdev_queue *dev_queue,\n\t\t\t      struct Qdisc *qdisc)\n{\n\tstruct Qdisc *oqdisc = rtnl_dereference(dev_queue->qdisc_sleeping);\n\tspinlock_t *root_lock;\n\n\troot_lock = qdisc_lock(oqdisc);\n\tspin_lock_bh(root_lock);\n\n\t \n\tif (qdisc == NULL)\n\t\tqdisc = &noop_qdisc;\n\trcu_assign_pointer(dev_queue->qdisc_sleeping, qdisc);\n\trcu_assign_pointer(dev_queue->qdisc, &noop_qdisc);\n\n\tspin_unlock_bh(root_lock);\n\n\treturn oqdisc;\n}\nEXPORT_SYMBOL(dev_graft_qdisc);\n\nstatic void shutdown_scheduler_queue(struct net_device *dev,\n\t\t\t\t     struct netdev_queue *dev_queue,\n\t\t\t\t     void *_qdisc_default)\n{\n\tstruct Qdisc *qdisc = rtnl_dereference(dev_queue->qdisc_sleeping);\n\tstruct Qdisc *qdisc_default = _qdisc_default;\n\n\tif (qdisc) {\n\t\trcu_assign_pointer(dev_queue->qdisc, qdisc_default);\n\t\trcu_assign_pointer(dev_queue->qdisc_sleeping, qdisc_default);\n\n\t\tqdisc_put(qdisc);\n\t}\n}\n\nstatic void attach_one_default_qdisc(struct net_device *dev,\n\t\t\t\t     struct netdev_queue *dev_queue,\n\t\t\t\t     void *_unused)\n{\n\tstruct Qdisc *qdisc;\n\tconst struct Qdisc_ops *ops = default_qdisc_ops;\n\n\tif (dev->priv_flags & IFF_NO_QUEUE)\n\t\tops = &noqueue_qdisc_ops;\n\telse if(dev->type == ARPHRD_CAN)\n\t\tops = &pfifo_fast_ops;\n\n\tqdisc = qdisc_create_dflt(dev_queue, ops, TC_H_ROOT, NULL);\n\tif (!qdisc)\n\t\treturn;\n\n\tif (!netif_is_multiqueue(dev))\n\t\tqdisc->flags |= TCQ_F_ONETXQUEUE | TCQ_F_NOPARENT;\n\trcu_assign_pointer(dev_queue->qdisc_sleeping, qdisc);\n}\n\nstatic void attach_default_qdiscs(struct net_device *dev)\n{\n\tstruct netdev_queue *txq;\n\tstruct Qdisc *qdisc;\n\n\ttxq = netdev_get_tx_queue(dev, 0);\n\n\tif (!netif_is_multiqueue(dev) ||\n\t    dev->priv_flags & IFF_NO_QUEUE) {\n\t\tnetdev_for_each_tx_queue(dev, attach_one_default_qdisc, NULL);\n\t\tqdisc = rtnl_dereference(txq->qdisc_sleeping);\n\t\trcu_assign_pointer(dev->qdisc, qdisc);\n\t\tqdisc_refcount_inc(qdisc);\n\t} else {\n\t\tqdisc = qdisc_create_dflt(txq, &mq_qdisc_ops, TC_H_ROOT, NULL);\n\t\tif (qdisc) {\n\t\t\trcu_assign_pointer(dev->qdisc, qdisc);\n\t\t\tqdisc->ops->attach(qdisc);\n\t\t}\n\t}\n\tqdisc = rtnl_dereference(dev->qdisc);\n\n\t \n\tif (qdisc == &noop_qdisc) {\n\t\tnetdev_warn(dev, \"default qdisc (%s) fail, fallback to %s\\n\",\n\t\t\t    default_qdisc_ops->id, noqueue_qdisc_ops.id);\n\t\tnetdev_for_each_tx_queue(dev, shutdown_scheduler_queue, &noop_qdisc);\n\t\tdev->priv_flags |= IFF_NO_QUEUE;\n\t\tnetdev_for_each_tx_queue(dev, attach_one_default_qdisc, NULL);\n\t\tqdisc = rtnl_dereference(txq->qdisc_sleeping);\n\t\trcu_assign_pointer(dev->qdisc, qdisc);\n\t\tqdisc_refcount_inc(qdisc);\n\t\tdev->priv_flags ^= IFF_NO_QUEUE;\n\t}\n\n#ifdef CONFIG_NET_SCHED\n\tif (qdisc != &noop_qdisc)\n\t\tqdisc_hash_add(qdisc, false);\n#endif\n}\n\nstatic void transition_one_qdisc(struct net_device *dev,\n\t\t\t\t struct netdev_queue *dev_queue,\n\t\t\t\t void *_need_watchdog)\n{\n\tstruct Qdisc *new_qdisc = rtnl_dereference(dev_queue->qdisc_sleeping);\n\tint *need_watchdog_p = _need_watchdog;\n\n\tif (!(new_qdisc->flags & TCQ_F_BUILTIN))\n\t\tclear_bit(__QDISC_STATE_DEACTIVATED, &new_qdisc->state);\n\n\trcu_assign_pointer(dev_queue->qdisc, new_qdisc);\n\tif (need_watchdog_p) {\n\t\tWRITE_ONCE(dev_queue->trans_start, 0);\n\t\t*need_watchdog_p = 1;\n\t}\n}\n\nvoid dev_activate(struct net_device *dev)\n{\n\tint need_watchdog;\n\n\t \n\n\tif (rtnl_dereference(dev->qdisc) == &noop_qdisc)\n\t\tattach_default_qdiscs(dev);\n\n\tif (!netif_carrier_ok(dev))\n\t\t \n\t\treturn;\n\n\tneed_watchdog = 0;\n\tnetdev_for_each_tx_queue(dev, transition_one_qdisc, &need_watchdog);\n\tif (dev_ingress_queue(dev))\n\t\ttransition_one_qdisc(dev, dev_ingress_queue(dev), NULL);\n\n\tif (need_watchdog) {\n\t\tnetif_trans_update(dev);\n\t\tdev_watchdog_up(dev);\n\t}\n}\nEXPORT_SYMBOL(dev_activate);\n\nstatic void qdisc_deactivate(struct Qdisc *qdisc)\n{\n\tif (qdisc->flags & TCQ_F_BUILTIN)\n\t\treturn;\n\n\tset_bit(__QDISC_STATE_DEACTIVATED, &qdisc->state);\n}\n\nstatic void dev_deactivate_queue(struct net_device *dev,\n\t\t\t\t struct netdev_queue *dev_queue,\n\t\t\t\t void *_qdisc_default)\n{\n\tstruct Qdisc *qdisc_default = _qdisc_default;\n\tstruct Qdisc *qdisc;\n\n\tqdisc = rtnl_dereference(dev_queue->qdisc);\n\tif (qdisc) {\n\t\tqdisc_deactivate(qdisc);\n\t\trcu_assign_pointer(dev_queue->qdisc, qdisc_default);\n\t}\n}\n\nstatic void dev_reset_queue(struct net_device *dev,\n\t\t\t    struct netdev_queue *dev_queue,\n\t\t\t    void *_unused)\n{\n\tstruct Qdisc *qdisc;\n\tbool nolock;\n\n\tqdisc = rtnl_dereference(dev_queue->qdisc_sleeping);\n\tif (!qdisc)\n\t\treturn;\n\n\tnolock = qdisc->flags & TCQ_F_NOLOCK;\n\n\tif (nolock)\n\t\tspin_lock_bh(&qdisc->seqlock);\n\tspin_lock_bh(qdisc_lock(qdisc));\n\n\tqdisc_reset(qdisc);\n\n\tspin_unlock_bh(qdisc_lock(qdisc));\n\tif (nolock) {\n\t\tclear_bit(__QDISC_STATE_MISSED, &qdisc->state);\n\t\tclear_bit(__QDISC_STATE_DRAINING, &qdisc->state);\n\t\tspin_unlock_bh(&qdisc->seqlock);\n\t}\n}\n\nstatic bool some_qdisc_is_busy(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *dev_queue;\n\t\tspinlock_t *root_lock;\n\t\tstruct Qdisc *q;\n\t\tint val;\n\n\t\tdev_queue = netdev_get_tx_queue(dev, i);\n\t\tq = rtnl_dereference(dev_queue->qdisc_sleeping);\n\n\t\troot_lock = qdisc_lock(q);\n\t\tspin_lock_bh(root_lock);\n\n\t\tval = (qdisc_is_running(q) ||\n\t\t       test_bit(__QDISC_STATE_SCHED, &q->state));\n\n\t\tspin_unlock_bh(root_lock);\n\n\t\tif (val)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\n \nvoid dev_deactivate_many(struct list_head *head)\n{\n\tstruct net_device *dev;\n\n\tlist_for_each_entry(dev, head, close_list) {\n\t\tnetdev_for_each_tx_queue(dev, dev_deactivate_queue,\n\t\t\t\t\t &noop_qdisc);\n\t\tif (dev_ingress_queue(dev))\n\t\t\tdev_deactivate_queue(dev, dev_ingress_queue(dev),\n\t\t\t\t\t     &noop_qdisc);\n\n\t\tdev_watchdog_down(dev);\n\t}\n\n\t \n\tsynchronize_net();\n\n\tlist_for_each_entry(dev, head, close_list) {\n\t\tnetdev_for_each_tx_queue(dev, dev_reset_queue, NULL);\n\n\t\tif (dev_ingress_queue(dev))\n\t\t\tdev_reset_queue(dev, dev_ingress_queue(dev), NULL);\n\t}\n\n\t \n\tlist_for_each_entry(dev, head, close_list) {\n\t\twhile (some_qdisc_is_busy(dev)) {\n\t\t\t \n\t\t\tschedule_timeout_uninterruptible(1);\n\t\t}\n\t}\n}\n\nvoid dev_deactivate(struct net_device *dev)\n{\n\tLIST_HEAD(single);\n\n\tlist_add(&dev->close_list, &single);\n\tdev_deactivate_many(&single);\n\tlist_del(&single);\n}\nEXPORT_SYMBOL(dev_deactivate);\n\nstatic int qdisc_change_tx_queue_len(struct net_device *dev,\n\t\t\t\t     struct netdev_queue *dev_queue)\n{\n\tstruct Qdisc *qdisc = rtnl_dereference(dev_queue->qdisc_sleeping);\n\tconst struct Qdisc_ops *ops = qdisc->ops;\n\n\tif (ops->change_tx_queue_len)\n\t\treturn ops->change_tx_queue_len(qdisc, dev->tx_queue_len);\n\treturn 0;\n}\n\nvoid dev_qdisc_change_real_num_tx(struct net_device *dev,\n\t\t\t\t  unsigned int new_real_tx)\n{\n\tstruct Qdisc *qdisc = rtnl_dereference(dev->qdisc);\n\n\tif (qdisc->ops->change_real_num_tx)\n\t\tqdisc->ops->change_real_num_tx(qdisc, new_real_tx);\n}\n\nvoid mq_change_real_num_tx(struct Qdisc *sch, unsigned int new_real_tx)\n{\n#ifdef CONFIG_NET_SCHED\n\tstruct net_device *dev = qdisc_dev(sch);\n\tstruct Qdisc *qdisc;\n\tunsigned int i;\n\n\tfor (i = new_real_tx; i < dev->real_num_tx_queues; i++) {\n\t\tqdisc = rtnl_dereference(netdev_get_tx_queue(dev, i)->qdisc_sleeping);\n\t\t \n\t\tif (qdisc != &noop_qdisc && !qdisc->handle)\n\t\t\tqdisc_hash_del(qdisc);\n\t}\n\tfor (i = dev->real_num_tx_queues; i < new_real_tx; i++) {\n\t\tqdisc = rtnl_dereference(netdev_get_tx_queue(dev, i)->qdisc_sleeping);\n\t\tif (qdisc != &noop_qdisc && !qdisc->handle)\n\t\t\tqdisc_hash_add(qdisc, false);\n\t}\n#endif\n}\nEXPORT_SYMBOL(mq_change_real_num_tx);\n\nint dev_qdisc_change_tx_queue_len(struct net_device *dev)\n{\n\tbool up = dev->flags & IFF_UP;\n\tunsigned int i;\n\tint ret = 0;\n\n\tif (up)\n\t\tdev_deactivate(dev);\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tret = qdisc_change_tx_queue_len(dev, &dev->_tx[i]);\n\n\t\t \n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\n\tif (up)\n\t\tdev_activate(dev);\n\treturn ret;\n}\n\nstatic void dev_init_scheduler_queue(struct net_device *dev,\n\t\t\t\t     struct netdev_queue *dev_queue,\n\t\t\t\t     void *_qdisc)\n{\n\tstruct Qdisc *qdisc = _qdisc;\n\n\trcu_assign_pointer(dev_queue->qdisc, qdisc);\n\trcu_assign_pointer(dev_queue->qdisc_sleeping, qdisc);\n}\n\nvoid dev_init_scheduler(struct net_device *dev)\n{\n\trcu_assign_pointer(dev->qdisc, &noop_qdisc);\n\tnetdev_for_each_tx_queue(dev, dev_init_scheduler_queue, &noop_qdisc);\n\tif (dev_ingress_queue(dev))\n\t\tdev_init_scheduler_queue(dev, dev_ingress_queue(dev), &noop_qdisc);\n\n\ttimer_setup(&dev->watchdog_timer, dev_watchdog, 0);\n}\n\nvoid dev_shutdown(struct net_device *dev)\n{\n\tnetdev_for_each_tx_queue(dev, shutdown_scheduler_queue, &noop_qdisc);\n\tif (dev_ingress_queue(dev))\n\t\tshutdown_scheduler_queue(dev, dev_ingress_queue(dev), &noop_qdisc);\n\tqdisc_put(rtnl_dereference(dev->qdisc));\n\trcu_assign_pointer(dev->qdisc, &noop_qdisc);\n\n\tWARN_ON(timer_pending(&dev->watchdog_timer));\n}\n\n \nstatic void psched_ratecfg_precompute__(u64 rate, u32 *mult, u8 *shift)\n{\n\tu64 factor = NSEC_PER_SEC;\n\n\t*mult = 1;\n\t*shift = 0;\n\n\tif (rate <= 0)\n\t\treturn;\n\n\tfor (;;) {\n\t\t*mult = div64_u64(factor, rate);\n\t\tif (*mult & (1U << 31) || factor & (1ULL << 63))\n\t\t\tbreak;\n\t\tfactor <<= 1;\n\t\t(*shift)++;\n\t}\n}\n\nvoid psched_ratecfg_precompute(struct psched_ratecfg *r,\n\t\t\t       const struct tc_ratespec *conf,\n\t\t\t       u64 rate64)\n{\n\tmemset(r, 0, sizeof(*r));\n\tr->overhead = conf->overhead;\n\tr->mpu = conf->mpu;\n\tr->rate_bytes_ps = max_t(u64, conf->rate, rate64);\n\tr->linklayer = (conf->linklayer & TC_LINKLAYER_MASK);\n\tpsched_ratecfg_precompute__(r->rate_bytes_ps, &r->mult, &r->shift);\n}\nEXPORT_SYMBOL(psched_ratecfg_precompute);\n\nvoid psched_ppscfg_precompute(struct psched_pktrate *r, u64 pktrate64)\n{\n\tr->rate_pkts_ps = pktrate64;\n\tpsched_ratecfg_precompute__(r->rate_pkts_ps, &r->mult, &r->shift);\n}\nEXPORT_SYMBOL(psched_ppscfg_precompute);\n\nvoid mini_qdisc_pair_swap(struct mini_Qdisc_pair *miniqp,\n\t\t\t  struct tcf_proto *tp_head)\n{\n\t \n\tstruct mini_Qdisc *miniq_old =\n\t\trcu_dereference_protected(*miniqp->p_miniq, 1);\n\tstruct mini_Qdisc *miniq;\n\n\tif (!tp_head) {\n\t\tRCU_INIT_POINTER(*miniqp->p_miniq, NULL);\n\t} else {\n\t\tminiq = miniq_old != &miniqp->miniq1 ?\n\t\t\t&miniqp->miniq1 : &miniqp->miniq2;\n\n\t\t \n\t\tif (IS_ENABLED(CONFIG_PREEMPT_RT))\n\t\t\tcond_synchronize_rcu(miniq->rcu_state);\n\t\telse if (!poll_state_synchronize_rcu(miniq->rcu_state))\n\t\t\tsynchronize_rcu_expedited();\n\n\t\tminiq->filter_list = tp_head;\n\t\trcu_assign_pointer(*miniqp->p_miniq, miniq);\n\t}\n\n\tif (miniq_old)\n\t\t \n\t\tminiq_old->rcu_state = start_poll_synchronize_rcu();\n}\nEXPORT_SYMBOL(mini_qdisc_pair_swap);\n\nvoid mini_qdisc_pair_block_init(struct mini_Qdisc_pair *miniqp,\n\t\t\t\tstruct tcf_block *block)\n{\n\tminiqp->miniq1.block = block;\n\tminiqp->miniq2.block = block;\n}\nEXPORT_SYMBOL(mini_qdisc_pair_block_init);\n\nvoid mini_qdisc_pair_init(struct mini_Qdisc_pair *miniqp, struct Qdisc *qdisc,\n\t\t\t  struct mini_Qdisc __rcu **p_miniq)\n{\n\tminiqp->miniq1.cpu_bstats = qdisc->cpu_bstats;\n\tminiqp->miniq1.cpu_qstats = qdisc->cpu_qstats;\n\tminiqp->miniq2.cpu_bstats = qdisc->cpu_bstats;\n\tminiqp->miniq2.cpu_qstats = qdisc->cpu_qstats;\n\tminiqp->miniq1.rcu_state = get_state_synchronize_rcu();\n\tminiqp->miniq2.rcu_state = miniqp->miniq1.rcu_state;\n\tminiqp->p_miniq = p_miniq;\n}\nEXPORT_SYMBOL(mini_qdisc_pair_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}