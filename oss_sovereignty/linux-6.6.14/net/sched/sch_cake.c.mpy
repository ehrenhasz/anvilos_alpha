{
  "module_name": "sch_cake.c",
  "hash_id": "2b2b6c68980684e92e5dd1e2dc60df5c95d1757588499d64960126b7b6462c0d",
  "original_prompt": "Ingested from linux-6.6.14/net/sched/sch_cake.c",
  "human_readable_source": "\n\n \n\n#include <linux/module.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/jiffies.h>\n#include <linux/string.h>\n#include <linux/in.h>\n#include <linux/errno.h>\n#include <linux/init.h>\n#include <linux/skbuff.h>\n#include <linux/jhash.h>\n#include <linux/slab.h>\n#include <linux/vmalloc.h>\n#include <linux/reciprocal_div.h>\n#include <net/netlink.h>\n#include <linux/if_vlan.h>\n#include <net/gso.h>\n#include <net/pkt_sched.h>\n#include <net/pkt_cls.h>\n#include <net/tcp.h>\n#include <net/flow_dissector.h>\n\n#if IS_ENABLED(CONFIG_NF_CONNTRACK)\n#include <net/netfilter/nf_conntrack_core.h>\n#endif\n\n#define CAKE_SET_WAYS (8)\n#define CAKE_MAX_TINS (8)\n#define CAKE_QUEUES (1024)\n#define CAKE_FLOW_MASK 63\n#define CAKE_FLOW_NAT_FLAG 64\n\n \nstruct cobalt_params {\n\tu64\tinterval;\n\tu64\ttarget;\n\tu64\tmtu_time;\n\tu32\tp_inc;\n\tu32\tp_dec;\n};\n\n \nstruct cobalt_vars {\n\tu32\tcount;\n\tu32\trec_inv_sqrt;\n\tktime_t\tdrop_next;\n\tktime_t\tblue_timer;\n\tu32     p_drop;\n\tbool\tdropping;\n\tbool    ecn_marked;\n};\n\nenum {\n\tCAKE_SET_NONE = 0,\n\tCAKE_SET_SPARSE,\n\tCAKE_SET_SPARSE_WAIT,  \n\tCAKE_SET_BULK,\n\tCAKE_SET_DECAYING\n};\n\nstruct cake_flow {\n\t \n\tstruct sk_buff\t  *head;\n\tstruct sk_buff\t  *tail;\n\tstruct list_head  flowchain;\n\ts32\t\t  deficit;\n\tu32\t\t  dropped;\n\tstruct cobalt_vars cvars;\n\tu16\t\t  srchost;  \n\tu16\t\t  dsthost;\n\tu8\t\t  set;\n};  \n\nstruct cake_host {\n\tu32 srchost_tag;\n\tu32 dsthost_tag;\n\tu16 srchost_bulk_flow_count;\n\tu16 dsthost_bulk_flow_count;\n};\n\nstruct cake_heap_entry {\n\tu16 t:3, b:10;\n};\n\nstruct cake_tin_data {\n\tstruct cake_flow flows[CAKE_QUEUES];\n\tu32\tbacklogs[CAKE_QUEUES];\n\tu32\ttags[CAKE_QUEUES];  \n\tu16\toverflow_idx[CAKE_QUEUES];\n\tstruct cake_host hosts[CAKE_QUEUES];  \n\tu16\tflow_quantum;\n\n\tstruct cobalt_params cparams;\n\tu32\tdrop_overlimit;\n\tu16\tbulk_flow_count;\n\tu16\tsparse_flow_count;\n\tu16\tdecaying_flow_count;\n\tu16\tunresponsive_flow_count;\n\n\tu32\tmax_skblen;\n\n\tstruct list_head new_flows;\n\tstruct list_head old_flows;\n\tstruct list_head decaying_flows;\n\n\t \n\tktime_t\ttime_next_packet;\n\tu64\ttin_rate_ns;\n\tu64\ttin_rate_bps;\n\tu16\ttin_rate_shft;\n\n\tu16\ttin_quantum;\n\ts32\ttin_deficit;\n\tu32\ttin_backlog;\n\tu32\ttin_dropped;\n\tu32\ttin_ecn_mark;\n\n\tu32\tpackets;\n\tu64\tbytes;\n\n\tu32\tack_drops;\n\n\t \n\tu64 avge_delay;\n\tu64 peak_delay;\n\tu64 base_delay;\n\n\t \n\tu32\tway_directs;\n\tu32\tway_hits;\n\tu32\tway_misses;\n\tu32\tway_collisions;\n};  \n\nstruct cake_sched_data {\n\tstruct tcf_proto __rcu *filter_list;  \n\tstruct tcf_block *block;\n\tstruct cake_tin_data *tins;\n\n\tstruct cake_heap_entry overflow_heap[CAKE_QUEUES * CAKE_MAX_TINS];\n\tu16\t\toverflow_timeout;\n\n\tu16\t\ttin_cnt;\n\tu8\t\ttin_mode;\n\tu8\t\tflow_mode;\n\tu8\t\tack_filter;\n\tu8\t\tatm_mode;\n\n\tu32\t\tfwmark_mask;\n\tu16\t\tfwmark_shft;\n\n\t \n\tu16\t\trate_shft;\n\tktime_t\t\ttime_next_packet;\n\tktime_t\t\tfailsafe_next_packet;\n\tu64\t\trate_ns;\n\tu64\t\trate_bps;\n\tu16\t\trate_flags;\n\ts16\t\trate_overhead;\n\tu16\t\trate_mpu;\n\tu64\t\tinterval;\n\tu64\t\ttarget;\n\n\t \n\tu32\t\tbuffer_used;\n\tu32\t\tbuffer_max_used;\n\tu32\t\tbuffer_limit;\n\tu32\t\tbuffer_config_limit;\n\n\t \n\tu16\t\tcur_tin;\n\tu16\t\tcur_flow;\n\n\tstruct qdisc_watchdog watchdog;\n\tconst u8\t*tin_index;\n\tconst u8\t*tin_order;\n\n\t \n\tktime_t\t\tlast_packet_time;\n\tktime_t\t\tavg_window_begin;\n\tu64\t\tavg_packet_interval;\n\tu64\t\tavg_window_bytes;\n\tu64\t\tavg_peak_bandwidth;\n\tktime_t\t\tlast_reconfig_time;\n\n\t \n\tu32\t\tavg_netoff;\n\tu16\t\tmax_netlen;\n\tu16\t\tmax_adjlen;\n\tu16\t\tmin_netlen;\n\tu16\t\tmin_adjlen;\n};\n\nenum {\n\tCAKE_FLAG_OVERHEAD\t   = BIT(0),\n\tCAKE_FLAG_AUTORATE_INGRESS = BIT(1),\n\tCAKE_FLAG_INGRESS\t   = BIT(2),\n\tCAKE_FLAG_WASH\t\t   = BIT(3),\n\tCAKE_FLAG_SPLIT_GSO\t   = BIT(4)\n};\n\n \n\nstruct cobalt_skb_cb {\n\tktime_t enqueue_time;\n\tu32     adjusted_len;\n};\n\nstatic u64 us_to_ns(u64 us)\n{\n\treturn us * NSEC_PER_USEC;\n}\n\nstatic struct cobalt_skb_cb *get_cobalt_cb(const struct sk_buff *skb)\n{\n\tqdisc_cb_private_validate(skb, sizeof(struct cobalt_skb_cb));\n\treturn (struct cobalt_skb_cb *)qdisc_skb_cb(skb)->data;\n}\n\nstatic ktime_t cobalt_get_enqueue_time(const struct sk_buff *skb)\n{\n\treturn get_cobalt_cb(skb)->enqueue_time;\n}\n\nstatic void cobalt_set_enqueue_time(struct sk_buff *skb,\n\t\t\t\t    ktime_t now)\n{\n\tget_cobalt_cb(skb)->enqueue_time = now;\n}\n\nstatic u16 quantum_div[CAKE_QUEUES + 1] = {0};\n\n \n\nstatic const u8 precedence[] = {\n\t0, 0, 0, 0, 0, 0, 0, 0,\n\t1, 1, 1, 1, 1, 1, 1, 1,\n\t2, 2, 2, 2, 2, 2, 2, 2,\n\t3, 3, 3, 3, 3, 3, 3, 3,\n\t4, 4, 4, 4, 4, 4, 4, 4,\n\t5, 5, 5, 5, 5, 5, 5, 5,\n\t6, 6, 6, 6, 6, 6, 6, 6,\n\t7, 7, 7, 7, 7, 7, 7, 7,\n};\n\nstatic const u8 diffserv8[] = {\n\t2, 0, 1, 2, 4, 2, 2, 2,\n\t1, 2, 1, 2, 1, 2, 1, 2,\n\t5, 2, 4, 2, 4, 2, 4, 2,\n\t3, 2, 3, 2, 3, 2, 3, 2,\n\t6, 2, 3, 2, 3, 2, 3, 2,\n\t6, 2, 2, 2, 6, 2, 6, 2,\n\t7, 2, 2, 2, 2, 2, 2, 2,\n\t7, 2, 2, 2, 2, 2, 2, 2,\n};\n\nstatic const u8 diffserv4[] = {\n\t0, 1, 0, 0, 2, 0, 0, 0,\n\t1, 0, 0, 0, 0, 0, 0, 0,\n\t2, 0, 2, 0, 2, 0, 2, 0,\n\t2, 0, 2, 0, 2, 0, 2, 0,\n\t3, 0, 2, 0, 2, 0, 2, 0,\n\t3, 0, 0, 0, 3, 0, 3, 0,\n\t3, 0, 0, 0, 0, 0, 0, 0,\n\t3, 0, 0, 0, 0, 0, 0, 0,\n};\n\nstatic const u8 diffserv3[] = {\n\t0, 1, 0, 0, 2, 0, 0, 0,\n\t1, 0, 0, 0, 0, 0, 0, 0,\n\t0, 0, 0, 0, 0, 0, 0, 0,\n\t0, 0, 0, 0, 0, 0, 0, 0,\n\t0, 0, 0, 0, 0, 0, 0, 0,\n\t0, 0, 0, 0, 2, 0, 2, 0,\n\t2, 0, 0, 0, 0, 0, 0, 0,\n\t2, 0, 0, 0, 0, 0, 0, 0,\n};\n\nstatic const u8 besteffort[] = {\n\t0, 0, 0, 0, 0, 0, 0, 0,\n\t0, 0, 0, 0, 0, 0, 0, 0,\n\t0, 0, 0, 0, 0, 0, 0, 0,\n\t0, 0, 0, 0, 0, 0, 0, 0,\n\t0, 0, 0, 0, 0, 0, 0, 0,\n\t0, 0, 0, 0, 0, 0, 0, 0,\n\t0, 0, 0, 0, 0, 0, 0, 0,\n\t0, 0, 0, 0, 0, 0, 0, 0,\n};\n\n \n\nstatic const u8 normal_order[] = {0, 1, 2, 3, 4, 5, 6, 7};\nstatic const u8 bulk_order[] = {1, 0, 2, 3};\n\n#define REC_INV_SQRT_CACHE (16)\nstatic u32 cobalt_rec_inv_sqrt_cache[REC_INV_SQRT_CACHE] = {0};\n\n \n\nstatic void cobalt_newton_step(struct cobalt_vars *vars)\n{\n\tu32 invsqrt, invsqrt2;\n\tu64 val;\n\n\tinvsqrt = vars->rec_inv_sqrt;\n\tinvsqrt2 = ((u64)invsqrt * invsqrt) >> 32;\n\tval = (3LL << 32) - ((u64)vars->count * invsqrt2);\n\n\tval >>= 2;  \n\tval = (val * invsqrt) >> (32 - 2 + 1);\n\n\tvars->rec_inv_sqrt = val;\n}\n\nstatic void cobalt_invsqrt(struct cobalt_vars *vars)\n{\n\tif (vars->count < REC_INV_SQRT_CACHE)\n\t\tvars->rec_inv_sqrt = cobalt_rec_inv_sqrt_cache[vars->count];\n\telse\n\t\tcobalt_newton_step(vars);\n}\n\n \n\nstatic void cobalt_cache_init(void)\n{\n\tstruct cobalt_vars v;\n\n\tmemset(&v, 0, sizeof(v));\n\tv.rec_inv_sqrt = ~0U;\n\tcobalt_rec_inv_sqrt_cache[0] = v.rec_inv_sqrt;\n\n\tfor (v.count = 1; v.count < REC_INV_SQRT_CACHE; v.count++) {\n\t\tcobalt_newton_step(&v);\n\t\tcobalt_newton_step(&v);\n\t\tcobalt_newton_step(&v);\n\t\tcobalt_newton_step(&v);\n\n\t\tcobalt_rec_inv_sqrt_cache[v.count] = v.rec_inv_sqrt;\n\t}\n}\n\nstatic void cobalt_vars_init(struct cobalt_vars *vars)\n{\n\tmemset(vars, 0, sizeof(*vars));\n\n\tif (!cobalt_rec_inv_sqrt_cache[0]) {\n\t\tcobalt_cache_init();\n\t\tcobalt_rec_inv_sqrt_cache[0] = ~0;\n\t}\n}\n\n \nstatic ktime_t cobalt_control(ktime_t t,\n\t\t\t      u64 interval,\n\t\t\t      u32 rec_inv_sqrt)\n{\n\treturn ktime_add_ns(t, reciprocal_scale(interval,\n\t\t\t\t\t\trec_inv_sqrt));\n}\n\n \nstatic bool cobalt_queue_full(struct cobalt_vars *vars,\n\t\t\t      struct cobalt_params *p,\n\t\t\t      ktime_t now)\n{\n\tbool up = false;\n\n\tif (ktime_to_ns(ktime_sub(now, vars->blue_timer)) > p->target) {\n\t\tup = !vars->p_drop;\n\t\tvars->p_drop += p->p_inc;\n\t\tif (vars->p_drop < p->p_inc)\n\t\t\tvars->p_drop = ~0;\n\t\tvars->blue_timer = now;\n\t}\n\tvars->dropping = true;\n\tvars->drop_next = now;\n\tif (!vars->count)\n\t\tvars->count = 1;\n\n\treturn up;\n}\n\n \nstatic bool cobalt_queue_empty(struct cobalt_vars *vars,\n\t\t\t       struct cobalt_params *p,\n\t\t\t       ktime_t now)\n{\n\tbool down = false;\n\n\tif (vars->p_drop &&\n\t    ktime_to_ns(ktime_sub(now, vars->blue_timer)) > p->target) {\n\t\tif (vars->p_drop < p->p_dec)\n\t\t\tvars->p_drop = 0;\n\t\telse\n\t\t\tvars->p_drop -= p->p_dec;\n\t\tvars->blue_timer = now;\n\t\tdown = !vars->p_drop;\n\t}\n\tvars->dropping = false;\n\n\tif (vars->count && ktime_to_ns(ktime_sub(now, vars->drop_next)) >= 0) {\n\t\tvars->count--;\n\t\tcobalt_invsqrt(vars);\n\t\tvars->drop_next = cobalt_control(vars->drop_next,\n\t\t\t\t\t\t p->interval,\n\t\t\t\t\t\t vars->rec_inv_sqrt);\n\t}\n\n\treturn down;\n}\n\n \nstatic bool cobalt_should_drop(struct cobalt_vars *vars,\n\t\t\t       struct cobalt_params *p,\n\t\t\t       ktime_t now,\n\t\t\t       struct sk_buff *skb,\n\t\t\t       u32 bulk_flows)\n{\n\tbool next_due, over_target, drop = false;\n\tktime_t schedule;\n\tu64 sojourn;\n\n \n\n\tsojourn = ktime_to_ns(ktime_sub(now, cobalt_get_enqueue_time(skb)));\n\tschedule = ktime_sub(now, vars->drop_next);\n\tover_target = sojourn > p->target &&\n\t\t      sojourn > p->mtu_time * bulk_flows * 2 &&\n\t\t      sojourn > p->mtu_time * 4;\n\tnext_due = vars->count && ktime_to_ns(schedule) >= 0;\n\n\tvars->ecn_marked = false;\n\n\tif (over_target) {\n\t\tif (!vars->dropping) {\n\t\t\tvars->dropping = true;\n\t\t\tvars->drop_next = cobalt_control(now,\n\t\t\t\t\t\t\t p->interval,\n\t\t\t\t\t\t\t vars->rec_inv_sqrt);\n\t\t}\n\t\tif (!vars->count)\n\t\t\tvars->count = 1;\n\t} else if (vars->dropping) {\n\t\tvars->dropping = false;\n\t}\n\n\tif (next_due && vars->dropping) {\n\t\t \n\t\tdrop = !(vars->ecn_marked = INET_ECN_set_ce(skb));\n\n\t\tvars->count++;\n\t\tif (!vars->count)\n\t\t\tvars->count--;\n\t\tcobalt_invsqrt(vars);\n\t\tvars->drop_next = cobalt_control(vars->drop_next,\n\t\t\t\t\t\t p->interval,\n\t\t\t\t\t\t vars->rec_inv_sqrt);\n\t\tschedule = ktime_sub(now, vars->drop_next);\n\t} else {\n\t\twhile (next_due) {\n\t\t\tvars->count--;\n\t\t\tcobalt_invsqrt(vars);\n\t\t\tvars->drop_next = cobalt_control(vars->drop_next,\n\t\t\t\t\t\t\t p->interval,\n\t\t\t\t\t\t\t vars->rec_inv_sqrt);\n\t\t\tschedule = ktime_sub(now, vars->drop_next);\n\t\t\tnext_due = vars->count && ktime_to_ns(schedule) >= 0;\n\t\t}\n\t}\n\n\t \n\tif (vars->p_drop)\n\t\tdrop |= (get_random_u32() < vars->p_drop);\n\n\t \n\tif (!vars->count)\n\t\tvars->drop_next = ktime_add_ns(now, p->interval);\n\telse if (ktime_to_ns(schedule) > 0 && !drop)\n\t\tvars->drop_next = now;\n\n\treturn drop;\n}\n\nstatic bool cake_update_flowkeys(struct flow_keys *keys,\n\t\t\t\t const struct sk_buff *skb)\n{\n#if IS_ENABLED(CONFIG_NF_CONNTRACK)\n\tstruct nf_conntrack_tuple tuple = {};\n\tbool rev = !skb->_nfct, upd = false;\n\t__be32 ip;\n\n\tif (skb_protocol(skb, true) != htons(ETH_P_IP))\n\t\treturn false;\n\n\tif (!nf_ct_get_tuple_skb(&tuple, skb))\n\t\treturn false;\n\n\tip = rev ? tuple.dst.u3.ip : tuple.src.u3.ip;\n\tif (ip != keys->addrs.v4addrs.src) {\n\t\tkeys->addrs.v4addrs.src = ip;\n\t\tupd = true;\n\t}\n\tip = rev ? tuple.src.u3.ip : tuple.dst.u3.ip;\n\tif (ip != keys->addrs.v4addrs.dst) {\n\t\tkeys->addrs.v4addrs.dst = ip;\n\t\tupd = true;\n\t}\n\n\tif (keys->ports.ports) {\n\t\t__be16 port;\n\n\t\tport = rev ? tuple.dst.u.all : tuple.src.u.all;\n\t\tif (port != keys->ports.src) {\n\t\t\tkeys->ports.src = port;\n\t\t\tupd = true;\n\t\t}\n\t\tport = rev ? tuple.src.u.all : tuple.dst.u.all;\n\t\tif (port != keys->ports.dst) {\n\t\t\tport = keys->ports.dst;\n\t\t\tupd = true;\n\t\t}\n\t}\n\treturn upd;\n#else\n\treturn false;\n#endif\n}\n\n \n\nstatic bool cake_dsrc(int flow_mode)\n{\n\treturn (flow_mode & CAKE_FLOW_DUAL_SRC) == CAKE_FLOW_DUAL_SRC;\n}\n\nstatic bool cake_ddst(int flow_mode)\n{\n\treturn (flow_mode & CAKE_FLOW_DUAL_DST) == CAKE_FLOW_DUAL_DST;\n}\n\nstatic u32 cake_hash(struct cake_tin_data *q, const struct sk_buff *skb,\n\t\t     int flow_mode, u16 flow_override, u16 host_override)\n{\n\tbool hash_flows = (!flow_override && !!(flow_mode & CAKE_FLOW_FLOWS));\n\tbool hash_hosts = (!host_override && !!(flow_mode & CAKE_FLOW_HOSTS));\n\tbool nat_enabled = !!(flow_mode & CAKE_FLOW_NAT_FLAG);\n\tu32 flow_hash = 0, srchost_hash = 0, dsthost_hash = 0;\n\tu16 reduced_hash, srchost_idx, dsthost_idx;\n\tstruct flow_keys keys, host_keys;\n\tbool use_skbhash = skb->l4_hash;\n\n\tif (unlikely(flow_mode == CAKE_FLOW_NONE))\n\t\treturn 0;\n\n\t \n\tif ((!hash_flows || (use_skbhash && !nat_enabled)) && !hash_hosts)\n\t\tgoto skip_hash;\n\n\tskb_flow_dissect_flow_keys(skb, &keys,\n\t\t\t\t   FLOW_DISSECTOR_F_STOP_AT_FLOW_LABEL);\n\n\t \n\tif (nat_enabled && cake_update_flowkeys(&keys, skb))\n\t\tuse_skbhash = false;\n\n\t \n\tif (use_skbhash && !hash_hosts)\n\t\tgoto skip_hash;\n\n\t \n\thost_keys = keys;\n\thost_keys.ports.ports     = 0;\n\thost_keys.basic.ip_proto  = 0;\n\thost_keys.keyid.keyid     = 0;\n\thost_keys.tags.flow_label = 0;\n\n\tswitch (host_keys.control.addr_type) {\n\tcase FLOW_DISSECTOR_KEY_IPV4_ADDRS:\n\t\thost_keys.addrs.v4addrs.src = 0;\n\t\tdsthost_hash = flow_hash_from_keys(&host_keys);\n\t\thost_keys.addrs.v4addrs.src = keys.addrs.v4addrs.src;\n\t\thost_keys.addrs.v4addrs.dst = 0;\n\t\tsrchost_hash = flow_hash_from_keys(&host_keys);\n\t\tbreak;\n\n\tcase FLOW_DISSECTOR_KEY_IPV6_ADDRS:\n\t\tmemset(&host_keys.addrs.v6addrs.src, 0,\n\t\t       sizeof(host_keys.addrs.v6addrs.src));\n\t\tdsthost_hash = flow_hash_from_keys(&host_keys);\n\t\thost_keys.addrs.v6addrs.src = keys.addrs.v6addrs.src;\n\t\tmemset(&host_keys.addrs.v6addrs.dst, 0,\n\t\t       sizeof(host_keys.addrs.v6addrs.dst));\n\t\tsrchost_hash = flow_hash_from_keys(&host_keys);\n\t\tbreak;\n\n\tdefault:\n\t\tdsthost_hash = 0;\n\t\tsrchost_hash = 0;\n\t}\n\n\t \n\tif (hash_flows && !use_skbhash)\n\t\tflow_hash = flow_hash_from_keys(&keys);\n\nskip_hash:\n\tif (flow_override)\n\t\tflow_hash = flow_override - 1;\n\telse if (use_skbhash && (flow_mode & CAKE_FLOW_FLOWS))\n\t\tflow_hash = skb->hash;\n\tif (host_override) {\n\t\tdsthost_hash = host_override - 1;\n\t\tsrchost_hash = host_override - 1;\n\t}\n\n\tif (!(flow_mode & CAKE_FLOW_FLOWS)) {\n\t\tif (flow_mode & CAKE_FLOW_SRC_IP)\n\t\t\tflow_hash ^= srchost_hash;\n\n\t\tif (flow_mode & CAKE_FLOW_DST_IP)\n\t\t\tflow_hash ^= dsthost_hash;\n\t}\n\n\treduced_hash = flow_hash % CAKE_QUEUES;\n\n\t \n\t \n\tif (likely(q->tags[reduced_hash] == flow_hash &&\n\t\t   q->flows[reduced_hash].set)) {\n\t\tq->way_directs++;\n\t} else {\n\t\tu32 inner_hash = reduced_hash % CAKE_SET_WAYS;\n\t\tu32 outer_hash = reduced_hash - inner_hash;\n\t\tbool allocate_src = false;\n\t\tbool allocate_dst = false;\n\t\tu32 i, k;\n\n\t\t \n\t\tfor (i = 0, k = inner_hash; i < CAKE_SET_WAYS;\n\t\t     i++, k = (k + 1) % CAKE_SET_WAYS) {\n\t\t\tif (q->tags[outer_hash + k] == flow_hash) {\n\t\t\t\tif (i)\n\t\t\t\t\tq->way_hits++;\n\n\t\t\t\tif (!q->flows[outer_hash + k].set) {\n\t\t\t\t\t \n\t\t\t\t\tallocate_src = cake_dsrc(flow_mode);\n\t\t\t\t\tallocate_dst = cake_ddst(flow_mode);\n\t\t\t\t}\n\n\t\t\t\tgoto found;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tfor (i = 0; i < CAKE_SET_WAYS;\n\t\t\t i++, k = (k + 1) % CAKE_SET_WAYS) {\n\t\t\tif (!q->flows[outer_hash + k].set) {\n\t\t\t\tq->way_misses++;\n\t\t\t\tallocate_src = cake_dsrc(flow_mode);\n\t\t\t\tallocate_dst = cake_ddst(flow_mode);\n\t\t\t\tgoto found;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tq->way_collisions++;\n\t\tif (q->flows[outer_hash + k].set == CAKE_SET_BULK) {\n\t\t\tq->hosts[q->flows[reduced_hash].srchost].srchost_bulk_flow_count--;\n\t\t\tq->hosts[q->flows[reduced_hash].dsthost].dsthost_bulk_flow_count--;\n\t\t}\n\t\tallocate_src = cake_dsrc(flow_mode);\n\t\tallocate_dst = cake_ddst(flow_mode);\nfound:\n\t\t \n\t\treduced_hash = outer_hash + k;\n\t\tq->tags[reduced_hash] = flow_hash;\n\n\t\tif (allocate_src) {\n\t\t\tsrchost_idx = srchost_hash % CAKE_QUEUES;\n\t\t\tinner_hash = srchost_idx % CAKE_SET_WAYS;\n\t\t\touter_hash = srchost_idx - inner_hash;\n\t\t\tfor (i = 0, k = inner_hash; i < CAKE_SET_WAYS;\n\t\t\t\ti++, k = (k + 1) % CAKE_SET_WAYS) {\n\t\t\t\tif (q->hosts[outer_hash + k].srchost_tag ==\n\t\t\t\t    srchost_hash)\n\t\t\t\t\tgoto found_src;\n\t\t\t}\n\t\t\tfor (i = 0; i < CAKE_SET_WAYS;\n\t\t\t\ti++, k = (k + 1) % CAKE_SET_WAYS) {\n\t\t\t\tif (!q->hosts[outer_hash + k].srchost_bulk_flow_count)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tq->hosts[outer_hash + k].srchost_tag = srchost_hash;\nfound_src:\n\t\t\tsrchost_idx = outer_hash + k;\n\t\t\tif (q->flows[reduced_hash].set == CAKE_SET_BULK)\n\t\t\t\tq->hosts[srchost_idx].srchost_bulk_flow_count++;\n\t\t\tq->flows[reduced_hash].srchost = srchost_idx;\n\t\t}\n\n\t\tif (allocate_dst) {\n\t\t\tdsthost_idx = dsthost_hash % CAKE_QUEUES;\n\t\t\tinner_hash = dsthost_idx % CAKE_SET_WAYS;\n\t\t\touter_hash = dsthost_idx - inner_hash;\n\t\t\tfor (i = 0, k = inner_hash; i < CAKE_SET_WAYS;\n\t\t\t     i++, k = (k + 1) % CAKE_SET_WAYS) {\n\t\t\t\tif (q->hosts[outer_hash + k].dsthost_tag ==\n\t\t\t\t    dsthost_hash)\n\t\t\t\t\tgoto found_dst;\n\t\t\t}\n\t\t\tfor (i = 0; i < CAKE_SET_WAYS;\n\t\t\t     i++, k = (k + 1) % CAKE_SET_WAYS) {\n\t\t\t\tif (!q->hosts[outer_hash + k].dsthost_bulk_flow_count)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tq->hosts[outer_hash + k].dsthost_tag = dsthost_hash;\nfound_dst:\n\t\t\tdsthost_idx = outer_hash + k;\n\t\t\tif (q->flows[reduced_hash].set == CAKE_SET_BULK)\n\t\t\t\tq->hosts[dsthost_idx].dsthost_bulk_flow_count++;\n\t\t\tq->flows[reduced_hash].dsthost = dsthost_idx;\n\t\t}\n\t}\n\n\treturn reduced_hash;\n}\n\n \n \n\nstatic struct sk_buff *dequeue_head(struct cake_flow *flow)\n{\n\tstruct sk_buff *skb = flow->head;\n\n\tif (skb) {\n\t\tflow->head = skb->next;\n\t\tskb_mark_not_on_list(skb);\n\t}\n\n\treturn skb;\n}\n\n \n\nstatic void flow_queue_add(struct cake_flow *flow, struct sk_buff *skb)\n{\n\tif (!flow->head)\n\t\tflow->head = skb;\n\telse\n\t\tflow->tail->next = skb;\n\tflow->tail = skb;\n\tskb->next = NULL;\n}\n\nstatic struct iphdr *cake_get_iphdr(const struct sk_buff *skb,\n\t\t\t\t    struct ipv6hdr *buf)\n{\n\tunsigned int offset = skb_network_offset(skb);\n\tstruct iphdr *iph;\n\n\tiph = skb_header_pointer(skb, offset, sizeof(struct iphdr), buf);\n\n\tif (!iph)\n\t\treturn NULL;\n\n\tif (iph->version == 4 && iph->protocol == IPPROTO_IPV6)\n\t\treturn skb_header_pointer(skb, offset + iph->ihl * 4,\n\t\t\t\t\t  sizeof(struct ipv6hdr), buf);\n\n\telse if (iph->version == 4)\n\t\treturn iph;\n\n\telse if (iph->version == 6)\n\t\treturn skb_header_pointer(skb, offset, sizeof(struct ipv6hdr),\n\t\t\t\t\t  buf);\n\n\treturn NULL;\n}\n\nstatic struct tcphdr *cake_get_tcphdr(const struct sk_buff *skb,\n\t\t\t\t      void *buf, unsigned int bufsize)\n{\n\tunsigned int offset = skb_network_offset(skb);\n\tconst struct ipv6hdr *ipv6h;\n\tconst struct tcphdr *tcph;\n\tconst struct iphdr *iph;\n\tstruct ipv6hdr _ipv6h;\n\tstruct tcphdr _tcph;\n\n\tipv6h = skb_header_pointer(skb, offset, sizeof(_ipv6h), &_ipv6h);\n\n\tif (!ipv6h)\n\t\treturn NULL;\n\n\tif (ipv6h->version == 4) {\n\t\tiph = (struct iphdr *)ipv6h;\n\t\toffset += iph->ihl * 4;\n\n\t\t \n\t\tif (iph->protocol == IPPROTO_IPV6) {\n\t\t\tipv6h = skb_header_pointer(skb, offset,\n\t\t\t\t\t\t   sizeof(_ipv6h), &_ipv6h);\n\n\t\t\tif (!ipv6h || ipv6h->nexthdr != IPPROTO_TCP)\n\t\t\t\treturn NULL;\n\n\t\t\toffset += sizeof(struct ipv6hdr);\n\n\t\t} else if (iph->protocol != IPPROTO_TCP) {\n\t\t\treturn NULL;\n\t\t}\n\n\t} else if (ipv6h->version == 6) {\n\t\tif (ipv6h->nexthdr != IPPROTO_TCP)\n\t\t\treturn NULL;\n\n\t\toffset += sizeof(struct ipv6hdr);\n\t} else {\n\t\treturn NULL;\n\t}\n\n\ttcph = skb_header_pointer(skb, offset, sizeof(_tcph), &_tcph);\n\tif (!tcph || tcph->doff < 5)\n\t\treturn NULL;\n\n\treturn skb_header_pointer(skb, offset,\n\t\t\t\t  min(__tcp_hdrlen(tcph), bufsize), buf);\n}\n\nstatic const void *cake_get_tcpopt(const struct tcphdr *tcph,\n\t\t\t\t   int code, int *oplen)\n{\n\t \n\tint length = __tcp_hdrlen(tcph) - sizeof(struct tcphdr);\n\tconst u8 *ptr = (const u8 *)(tcph + 1);\n\n\twhile (length > 0) {\n\t\tint opcode = *ptr++;\n\t\tint opsize;\n\n\t\tif (opcode == TCPOPT_EOL)\n\t\t\tbreak;\n\t\tif (opcode == TCPOPT_NOP) {\n\t\t\tlength--;\n\t\t\tcontinue;\n\t\t}\n\t\tif (length < 2)\n\t\t\tbreak;\n\t\topsize = *ptr++;\n\t\tif (opsize < 2 || opsize > length)\n\t\t\tbreak;\n\n\t\tif (opcode == code) {\n\t\t\t*oplen = opsize;\n\t\t\treturn ptr;\n\t\t}\n\n\t\tptr += opsize - 2;\n\t\tlength -= opsize;\n\t}\n\n\treturn NULL;\n}\n\n \nstatic int cake_tcph_sack_compare(const struct tcphdr *tcph_a,\n\t\t\t\t  const struct tcphdr *tcph_b)\n{\n\tconst struct tcp_sack_block_wire *sack_a, *sack_b;\n\tu32 ack_seq_a = ntohl(tcph_a->ack_seq);\n\tu32 bytes_a = 0, bytes_b = 0;\n\tint oplen_a, oplen_b;\n\tbool first = true;\n\n\tsack_a = cake_get_tcpopt(tcph_a, TCPOPT_SACK, &oplen_a);\n\tsack_b = cake_get_tcpopt(tcph_b, TCPOPT_SACK, &oplen_b);\n\n\t \n\toplen_a -= TCPOLEN_SACK_BASE;\n\toplen_b -= TCPOLEN_SACK_BASE;\n\n\tif (sack_a && oplen_a >= sizeof(*sack_a) &&\n\t    (!sack_b || oplen_b < sizeof(*sack_b)))\n\t\treturn -1;\n\telse if (sack_b && oplen_b >= sizeof(*sack_b) &&\n\t\t (!sack_a || oplen_a < sizeof(*sack_a)))\n\t\treturn 1;\n\telse if ((!sack_a || oplen_a < sizeof(*sack_a)) &&\n\t\t (!sack_b || oplen_b < sizeof(*sack_b)))\n\t\treturn 0;\n\n\twhile (oplen_a >= sizeof(*sack_a)) {\n\t\tconst struct tcp_sack_block_wire *sack_tmp = sack_b;\n\t\tu32 start_a = get_unaligned_be32(&sack_a->start_seq);\n\t\tu32 end_a = get_unaligned_be32(&sack_a->end_seq);\n\t\tint oplen_tmp = oplen_b;\n\t\tbool found = false;\n\n\t\t \n\t\tif (before(start_a, ack_seq_a))\n\t\t\treturn -1;\n\n\t\tbytes_a += end_a - start_a;\n\n\t\twhile (oplen_tmp >= sizeof(*sack_tmp)) {\n\t\t\tu32 start_b = get_unaligned_be32(&sack_tmp->start_seq);\n\t\t\tu32 end_b = get_unaligned_be32(&sack_tmp->end_seq);\n\n\t\t\t \n\t\t\tif (first)\n\t\t\t\tbytes_b += end_b - start_b;\n\n\t\t\tif (!after(start_b, start_a) && !before(end_b, end_a)) {\n\t\t\t\tfound = true;\n\t\t\t\tif (!first)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\toplen_tmp -= sizeof(*sack_tmp);\n\t\t\tsack_tmp++;\n\t\t}\n\n\t\tif (!found)\n\t\t\treturn -1;\n\n\t\toplen_a -= sizeof(*sack_a);\n\t\tsack_a++;\n\t\tfirst = false;\n\t}\n\n\t \n\treturn bytes_b > bytes_a ? 1 : 0;\n}\n\nstatic void cake_tcph_get_tstamp(const struct tcphdr *tcph,\n\t\t\t\t u32 *tsval, u32 *tsecr)\n{\n\tconst u8 *ptr;\n\tint opsize;\n\n\tptr = cake_get_tcpopt(tcph, TCPOPT_TIMESTAMP, &opsize);\n\n\tif (ptr && opsize == TCPOLEN_TIMESTAMP) {\n\t\t*tsval = get_unaligned_be32(ptr);\n\t\t*tsecr = get_unaligned_be32(ptr + 4);\n\t}\n}\n\nstatic bool cake_tcph_may_drop(const struct tcphdr *tcph,\n\t\t\t       u32 tstamp_new, u32 tsecr_new)\n{\n\t \n\tint length = __tcp_hdrlen(tcph) - sizeof(struct tcphdr);\n\tconst u8 *ptr = (const u8 *)(tcph + 1);\n\tu32 tstamp, tsecr;\n\n\t \n\tif (((tcp_flag_word(tcph) &\n\t      cpu_to_be32(0x0F3F0000)) != TCP_FLAG_ACK))\n\t\treturn false;\n\n\twhile (length > 0) {\n\t\tint opcode = *ptr++;\n\t\tint opsize;\n\n\t\tif (opcode == TCPOPT_EOL)\n\t\t\tbreak;\n\t\tif (opcode == TCPOPT_NOP) {\n\t\t\tlength--;\n\t\t\tcontinue;\n\t\t}\n\t\tif (length < 2)\n\t\t\tbreak;\n\t\topsize = *ptr++;\n\t\tif (opsize < 2 || opsize > length)\n\t\t\tbreak;\n\n\t\tswitch (opcode) {\n\t\tcase TCPOPT_MD5SIG:  \n\t\t\tbreak;\n\n\t\tcase TCPOPT_SACK:  \n\t\t\tif (opsize % 8 != 2)\n\t\t\t\treturn false;\n\t\t\tbreak;\n\n\t\tcase TCPOPT_TIMESTAMP:\n\t\t\t \n\t\t\tif (opsize != TCPOLEN_TIMESTAMP)\n\t\t\t\treturn false;\n\t\t\ttstamp = get_unaligned_be32(ptr);\n\t\t\ttsecr = get_unaligned_be32(ptr + 4);\n\t\t\tif (after(tstamp, tstamp_new) ||\n\t\t\t    after(tsecr, tsecr_new))\n\t\t\t\treturn false;\n\t\t\tbreak;\n\n\t\tcase TCPOPT_MSS:   \n\t\tcase TCPOPT_WINDOW:\n\t\tcase TCPOPT_SACK_PERM:\n\t\tcase TCPOPT_FASTOPEN:\n\t\tcase TCPOPT_EXP:\n\t\tdefault:  \n\t\t\treturn false;\n\t\t}\n\n\t\tptr += opsize - 2;\n\t\tlength -= opsize;\n\t}\n\n\treturn true;\n}\n\nstatic struct sk_buff *cake_ack_filter(struct cake_sched_data *q,\n\t\t\t\t       struct cake_flow *flow)\n{\n\tbool aggressive = q->ack_filter == CAKE_ACK_AGGRESSIVE;\n\tstruct sk_buff *elig_ack = NULL, *elig_ack_prev = NULL;\n\tstruct sk_buff *skb_check, *skb_prev = NULL;\n\tconst struct ipv6hdr *ipv6h, *ipv6h_check;\n\tunsigned char _tcph[64], _tcph_check[64];\n\tconst struct tcphdr *tcph, *tcph_check;\n\tconst struct iphdr *iph, *iph_check;\n\tstruct ipv6hdr _iph, _iph_check;\n\tconst struct sk_buff *skb;\n\tint seglen, num_found = 0;\n\tu32 tstamp = 0, tsecr = 0;\n\t__be32 elig_flags = 0;\n\tint sack_comp;\n\n\t \n\tif (flow->head == flow->tail)\n\t\treturn NULL;\n\n\tskb = flow->tail;\n\ttcph = cake_get_tcphdr(skb, _tcph, sizeof(_tcph));\n\tiph = cake_get_iphdr(skb, &_iph);\n\tif (!tcph)\n\t\treturn NULL;\n\n\tcake_tcph_get_tstamp(tcph, &tstamp, &tsecr);\n\n\t \n\tif ((tcp_flag_word(tcph) &\n\t     (TCP_FLAG_ACK | TCP_FLAG_SYN)) != TCP_FLAG_ACK)\n\t\treturn NULL;\n\n\t \n\tfor (skb_check = flow->head;\n\t     skb_check && skb_check != skb;\n\t     skb_prev = skb_check, skb_check = skb_check->next) {\n\t\tiph_check = cake_get_iphdr(skb_check, &_iph_check);\n\t\ttcph_check = cake_get_tcphdr(skb_check, &_tcph_check,\n\t\t\t\t\t     sizeof(_tcph_check));\n\n\t\t \n\t\tif (!tcph_check || iph->version != iph_check->version ||\n\t\t    tcph_check->source != tcph->source ||\n\t\t    tcph_check->dest != tcph->dest)\n\t\t\tcontinue;\n\n\t\tif (iph_check->version == 4) {\n\t\t\tif (iph_check->saddr != iph->saddr ||\n\t\t\t    iph_check->daddr != iph->daddr)\n\t\t\t\tcontinue;\n\n\t\t\tseglen = iph_totlen(skb, iph_check) -\n\t\t\t\t       (4 * iph_check->ihl);\n\t\t} else if (iph_check->version == 6) {\n\t\t\tipv6h = (struct ipv6hdr *)iph;\n\t\t\tipv6h_check = (struct ipv6hdr *)iph_check;\n\n\t\t\tif (ipv6_addr_cmp(&ipv6h_check->saddr, &ipv6h->saddr) ||\n\t\t\t    ipv6_addr_cmp(&ipv6h_check->daddr, &ipv6h->daddr))\n\t\t\t\tcontinue;\n\n\t\t\tseglen = ntohs(ipv6h_check->payload_len);\n\t\t} else {\n\t\t\tWARN_ON(1);   \n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (elig_ack && (tcp_flag_word(tcph_check) &\n\t\t\t\t (TCP_FLAG_ECE | TCP_FLAG_CWR)) != elig_flags) {\n\t\t\telig_ack = NULL;\n\t\t\telig_ack_prev = NULL;\n\t\t\tnum_found--;\n\t\t}\n\n\t\t \n\t\tif (!cake_tcph_may_drop(tcph_check, tstamp, tsecr) ||\n\t\t    (seglen - __tcp_hdrlen(tcph_check)) != 0 ||\n\t\t    after(ntohl(tcph_check->ack_seq), ntohl(tcph->ack_seq)))\n\t\t\tcontinue;\n\n\t\t \n\t\tsack_comp = cake_tcph_sack_compare(tcph_check, tcph);\n\n\t\tif (sack_comp < 0 ||\n\t\t    (ntohl(tcph_check->ack_seq) == ntohl(tcph->ack_seq) &&\n\t\t     sack_comp == 0))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (!elig_ack) {\n\t\t\telig_ack = skb_check;\n\t\t\telig_ack_prev = skb_prev;\n\t\t\telig_flags = (tcp_flag_word(tcph_check)\n\t\t\t\t      & (TCP_FLAG_ECE | TCP_FLAG_CWR));\n\t\t}\n\n\t\tif (num_found++ > 0)\n\t\t\tgoto found;\n\t}\n\n\t \n\tif (elig_ack && aggressive && elig_ack->next == skb &&\n\t    (elig_flags == (tcp_flag_word(tcph) &\n\t\t\t    (TCP_FLAG_ECE | TCP_FLAG_CWR))))\n\t\tgoto found;\n\n\treturn NULL;\n\nfound:\n\tif (elig_ack_prev)\n\t\telig_ack_prev->next = elig_ack->next;\n\telse\n\t\tflow->head = elig_ack->next;\n\n\tskb_mark_not_on_list(elig_ack);\n\n\treturn elig_ack;\n}\n\nstatic u64 cake_ewma(u64 avg, u64 sample, u32 shift)\n{\n\tavg -= avg >> shift;\n\tavg += sample >> shift;\n\treturn avg;\n}\n\nstatic u32 cake_calc_overhead(struct cake_sched_data *q, u32 len, u32 off)\n{\n\tif (q->rate_flags & CAKE_FLAG_OVERHEAD)\n\t\tlen -= off;\n\n\tif (q->max_netlen < len)\n\t\tq->max_netlen = len;\n\tif (q->min_netlen > len)\n\t\tq->min_netlen = len;\n\n\tlen += q->rate_overhead;\n\n\tif (len < q->rate_mpu)\n\t\tlen = q->rate_mpu;\n\n\tif (q->atm_mode == CAKE_ATM_ATM) {\n\t\tlen += 47;\n\t\tlen /= 48;\n\t\tlen *= 53;\n\t} else if (q->atm_mode == CAKE_ATM_PTM) {\n\t\t \n\t\tlen += (len + 63) / 64;\n\t}\n\n\tif (q->max_adjlen < len)\n\t\tq->max_adjlen = len;\n\tif (q->min_adjlen > len)\n\t\tq->min_adjlen = len;\n\n\treturn len;\n}\n\nstatic u32 cake_overhead(struct cake_sched_data *q, const struct sk_buff *skb)\n{\n\tconst struct skb_shared_info *shinfo = skb_shinfo(skb);\n\tunsigned int hdr_len, last_len = 0;\n\tu32 off = skb_network_offset(skb);\n\tu32 len = qdisc_pkt_len(skb);\n\tu16 segs = 1;\n\n\tq->avg_netoff = cake_ewma(q->avg_netoff, off << 16, 8);\n\n\tif (!shinfo->gso_size)\n\t\treturn cake_calc_overhead(q, len, off);\n\n\t \n\thdr_len = skb_transport_offset(skb);\n\n\t \n\tif (likely(shinfo->gso_type & (SKB_GSO_TCPV4 |\n\t\t\t\t\t\tSKB_GSO_TCPV6))) {\n\t\tconst struct tcphdr *th;\n\t\tstruct tcphdr _tcphdr;\n\n\t\tth = skb_header_pointer(skb, hdr_len,\n\t\t\t\t\tsizeof(_tcphdr), &_tcphdr);\n\t\tif (likely(th))\n\t\t\thdr_len += __tcp_hdrlen(th);\n\t} else {\n\t\tstruct udphdr _udphdr;\n\n\t\tif (skb_header_pointer(skb, hdr_len,\n\t\t\t\t       sizeof(_udphdr), &_udphdr))\n\t\t\thdr_len += sizeof(struct udphdr);\n\t}\n\n\tif (unlikely(shinfo->gso_type & SKB_GSO_DODGY))\n\t\tsegs = DIV_ROUND_UP(skb->len - hdr_len,\n\t\t\t\t    shinfo->gso_size);\n\telse\n\t\tsegs = shinfo->gso_segs;\n\n\tlen = shinfo->gso_size + hdr_len;\n\tlast_len = skb->len - shinfo->gso_size * (segs - 1);\n\n\treturn (cake_calc_overhead(q, len, off) * (segs - 1) +\n\t\tcake_calc_overhead(q, last_len, off));\n}\n\nstatic void cake_heap_swap(struct cake_sched_data *q, u16 i, u16 j)\n{\n\tstruct cake_heap_entry ii = q->overflow_heap[i];\n\tstruct cake_heap_entry jj = q->overflow_heap[j];\n\n\tq->overflow_heap[i] = jj;\n\tq->overflow_heap[j] = ii;\n\n\tq->tins[ii.t].overflow_idx[ii.b] = j;\n\tq->tins[jj.t].overflow_idx[jj.b] = i;\n}\n\nstatic u32 cake_heap_get_backlog(const struct cake_sched_data *q, u16 i)\n{\n\tstruct cake_heap_entry ii = q->overflow_heap[i];\n\n\treturn q->tins[ii.t].backlogs[ii.b];\n}\n\nstatic void cake_heapify(struct cake_sched_data *q, u16 i)\n{\n\tstatic const u32 a = CAKE_MAX_TINS * CAKE_QUEUES;\n\tu32 mb = cake_heap_get_backlog(q, i);\n\tu32 m = i;\n\n\twhile (m < a) {\n\t\tu32 l = m + m + 1;\n\t\tu32 r = l + 1;\n\n\t\tif (l < a) {\n\t\t\tu32 lb = cake_heap_get_backlog(q, l);\n\n\t\t\tif (lb > mb) {\n\t\t\t\tm  = l;\n\t\t\t\tmb = lb;\n\t\t\t}\n\t\t}\n\n\t\tif (r < a) {\n\t\t\tu32 rb = cake_heap_get_backlog(q, r);\n\n\t\t\tif (rb > mb) {\n\t\t\t\tm  = r;\n\t\t\t\tmb = rb;\n\t\t\t}\n\t\t}\n\n\t\tif (m != i) {\n\t\t\tcake_heap_swap(q, i, m);\n\t\t\ti = m;\n\t\t} else {\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic void cake_heapify_up(struct cake_sched_data *q, u16 i)\n{\n\twhile (i > 0 && i < CAKE_MAX_TINS * CAKE_QUEUES) {\n\t\tu16 p = (i - 1) >> 1;\n\t\tu32 ib = cake_heap_get_backlog(q, i);\n\t\tu32 pb = cake_heap_get_backlog(q, p);\n\n\t\tif (ib > pb) {\n\t\t\tcake_heap_swap(q, i, p);\n\t\t\ti = p;\n\t\t} else {\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic int cake_advance_shaper(struct cake_sched_data *q,\n\t\t\t       struct cake_tin_data *b,\n\t\t\t       struct sk_buff *skb,\n\t\t\t       ktime_t now, bool drop)\n{\n\tu32 len = get_cobalt_cb(skb)->adjusted_len;\n\n\t \n\tif (q->rate_ns) {\n\t\tu64 tin_dur = (len * b->tin_rate_ns) >> b->tin_rate_shft;\n\t\tu64 global_dur = (len * q->rate_ns) >> q->rate_shft;\n\t\tu64 failsafe_dur = global_dur + (global_dur >> 1);\n\n\t\tif (ktime_before(b->time_next_packet, now))\n\t\t\tb->time_next_packet = ktime_add_ns(b->time_next_packet,\n\t\t\t\t\t\t\t   tin_dur);\n\n\t\telse if (ktime_before(b->time_next_packet,\n\t\t\t\t      ktime_add_ns(now, tin_dur)))\n\t\t\tb->time_next_packet = ktime_add_ns(now, tin_dur);\n\n\t\tq->time_next_packet = ktime_add_ns(q->time_next_packet,\n\t\t\t\t\t\t   global_dur);\n\t\tif (!drop)\n\t\t\tq->failsafe_next_packet = \\\n\t\t\t\tktime_add_ns(q->failsafe_next_packet,\n\t\t\t\t\t     failsafe_dur);\n\t}\n\treturn len;\n}\n\nstatic unsigned int cake_drop(struct Qdisc *sch, struct sk_buff **to_free)\n{\n\tstruct cake_sched_data *q = qdisc_priv(sch);\n\tktime_t now = ktime_get();\n\tu32 idx = 0, tin = 0, len;\n\tstruct cake_heap_entry qq;\n\tstruct cake_tin_data *b;\n\tstruct cake_flow *flow;\n\tstruct sk_buff *skb;\n\n\tif (!q->overflow_timeout) {\n\t\tint i;\n\t\t \n\t\tfor (i = CAKE_MAX_TINS * CAKE_QUEUES / 2; i >= 0; i--)\n\t\t\tcake_heapify(q, i);\n\t}\n\tq->overflow_timeout = 65535;\n\n\t \n\tqq  = q->overflow_heap[0];\n\ttin = qq.t;\n\tidx = qq.b;\n\n\tb = &q->tins[tin];\n\tflow = &b->flows[idx];\n\tskb = dequeue_head(flow);\n\tif (unlikely(!skb)) {\n\t\t \n\t\tq->overflow_timeout = 0;\n\t\treturn idx + (tin << 16);\n\t}\n\n\tif (cobalt_queue_full(&flow->cvars, &b->cparams, now))\n\t\tb->unresponsive_flow_count++;\n\n\tlen = qdisc_pkt_len(skb);\n\tq->buffer_used      -= skb->truesize;\n\tb->backlogs[idx]    -= len;\n\tb->tin_backlog      -= len;\n\tsch->qstats.backlog -= len;\n\tqdisc_tree_reduce_backlog(sch, 1, len);\n\n\tflow->dropped++;\n\tb->tin_dropped++;\n\tsch->qstats.drops++;\n\n\tif (q->rate_flags & CAKE_FLAG_INGRESS)\n\t\tcake_advance_shaper(q, b, skb, now, true);\n\n\t__qdisc_drop(skb, to_free);\n\tsch->q.qlen--;\n\n\tcake_heapify(q, 0);\n\n\treturn idx + (tin << 16);\n}\n\nstatic u8 cake_handle_diffserv(struct sk_buff *skb, bool wash)\n{\n\tconst int offset = skb_network_offset(skb);\n\tu16 *buf, buf_;\n\tu8 dscp;\n\n\tswitch (skb_protocol(skb, true)) {\n\tcase htons(ETH_P_IP):\n\t\tbuf = skb_header_pointer(skb, offset, sizeof(buf_), &buf_);\n\t\tif (unlikely(!buf))\n\t\t\treturn 0;\n\n\t\t \n\t\tdscp = ipv4_get_dsfield((struct iphdr *)buf) >> 2;\n\n\t\tif (wash && dscp) {\n\t\t\tconst int wlen = offset + sizeof(struct iphdr);\n\n\t\t\tif (!pskb_may_pull(skb, wlen) ||\n\t\t\t    skb_try_make_writable(skb, wlen))\n\t\t\t\treturn 0;\n\n\t\t\tipv4_change_dsfield(ip_hdr(skb), INET_ECN_MASK, 0);\n\t\t}\n\n\t\treturn dscp;\n\n\tcase htons(ETH_P_IPV6):\n\t\tbuf = skb_header_pointer(skb, offset, sizeof(buf_), &buf_);\n\t\tif (unlikely(!buf))\n\t\t\treturn 0;\n\n\t\t \n\t\tdscp = ipv6_get_dsfield((struct ipv6hdr *)buf) >> 2;\n\n\t\tif (wash && dscp) {\n\t\t\tconst int wlen = offset + sizeof(struct ipv6hdr);\n\n\t\t\tif (!pskb_may_pull(skb, wlen) ||\n\t\t\t    skb_try_make_writable(skb, wlen))\n\t\t\t\treturn 0;\n\n\t\t\tipv6_change_dsfield(ipv6_hdr(skb), INET_ECN_MASK, 0);\n\t\t}\n\n\t\treturn dscp;\n\n\tcase htons(ETH_P_ARP):\n\t\treturn 0x38;   \n\n\tdefault:\n\t\t \n\t\treturn 0;\n\t}\n}\n\nstatic struct cake_tin_data *cake_select_tin(struct Qdisc *sch,\n\t\t\t\t\t     struct sk_buff *skb)\n{\n\tstruct cake_sched_data *q = qdisc_priv(sch);\n\tu32 tin, mark;\n\tbool wash;\n\tu8 dscp;\n\n\t \n\tmark = (skb->mark & q->fwmark_mask) >> q->fwmark_shft;\n\twash = !!(q->rate_flags & CAKE_FLAG_WASH);\n\tif (wash)\n\t\tdscp = cake_handle_diffserv(skb, wash);\n\n\tif (q->tin_mode == CAKE_DIFFSERV_BESTEFFORT)\n\t\ttin = 0;\n\n\telse if (mark && mark <= q->tin_cnt)\n\t\ttin = q->tin_order[mark - 1];\n\n\telse if (TC_H_MAJ(skb->priority) == sch->handle &&\n\t\t TC_H_MIN(skb->priority) > 0 &&\n\t\t TC_H_MIN(skb->priority) <= q->tin_cnt)\n\t\ttin = q->tin_order[TC_H_MIN(skb->priority) - 1];\n\n\telse {\n\t\tif (!wash)\n\t\t\tdscp = cake_handle_diffserv(skb, wash);\n\t\ttin = q->tin_index[dscp];\n\n\t\tif (unlikely(tin >= q->tin_cnt))\n\t\t\ttin = 0;\n\t}\n\n\treturn &q->tins[tin];\n}\n\nstatic u32 cake_classify(struct Qdisc *sch, struct cake_tin_data **t,\n\t\t\t struct sk_buff *skb, int flow_mode, int *qerr)\n{\n\tstruct cake_sched_data *q = qdisc_priv(sch);\n\tstruct tcf_proto *filter;\n\tstruct tcf_result res;\n\tu16 flow = 0, host = 0;\n\tint result;\n\n\tfilter = rcu_dereference_bh(q->filter_list);\n\tif (!filter)\n\t\tgoto hash;\n\n\t*qerr = NET_XMIT_SUCCESS | __NET_XMIT_BYPASS;\n\tresult = tcf_classify(skb, NULL, filter, &res, false);\n\n\tif (result >= 0) {\n#ifdef CONFIG_NET_CLS_ACT\n\t\tswitch (result) {\n\t\tcase TC_ACT_STOLEN:\n\t\tcase TC_ACT_QUEUED:\n\t\tcase TC_ACT_TRAP:\n\t\t\t*qerr = NET_XMIT_SUCCESS | __NET_XMIT_STOLEN;\n\t\t\tfallthrough;\n\t\tcase TC_ACT_SHOT:\n\t\t\treturn 0;\n\t\t}\n#endif\n\t\tif (TC_H_MIN(res.classid) <= CAKE_QUEUES)\n\t\t\tflow = TC_H_MIN(res.classid);\n\t\tif (TC_H_MAJ(res.classid) <= (CAKE_QUEUES << 16))\n\t\t\thost = TC_H_MAJ(res.classid) >> 16;\n\t}\nhash:\n\t*t = cake_select_tin(sch, skb);\n\treturn cake_hash(*t, skb, flow_mode, flow, host) + 1;\n}\n\nstatic void cake_reconfigure(struct Qdisc *sch);\n\nstatic s32 cake_enqueue(struct sk_buff *skb, struct Qdisc *sch,\n\t\t\tstruct sk_buff **to_free)\n{\n\tstruct cake_sched_data *q = qdisc_priv(sch);\n\tint len = qdisc_pkt_len(skb);\n\tint ret;\n\tstruct sk_buff *ack = NULL;\n\tktime_t now = ktime_get();\n\tstruct cake_tin_data *b;\n\tstruct cake_flow *flow;\n\tu32 idx;\n\n\t \n\tidx = cake_classify(sch, &b, skb, q->flow_mode, &ret);\n\tif (idx == 0) {\n\t\tif (ret & __NET_XMIT_BYPASS)\n\t\t\tqdisc_qstats_drop(sch);\n\t\t__qdisc_drop(skb, to_free);\n\t\treturn ret;\n\t}\n\tidx--;\n\tflow = &b->flows[idx];\n\n\t \n\tif (!b->tin_backlog) {\n\t\tif (ktime_before(b->time_next_packet, now))\n\t\t\tb->time_next_packet = now;\n\n\t\tif (!sch->q.qlen) {\n\t\t\tif (ktime_before(q->time_next_packet, now)) {\n\t\t\t\tq->failsafe_next_packet = now;\n\t\t\t\tq->time_next_packet = now;\n\t\t\t} else if (ktime_after(q->time_next_packet, now) &&\n\t\t\t\t   ktime_after(q->failsafe_next_packet, now)) {\n\t\t\t\tu64 next = \\\n\t\t\t\t\tmin(ktime_to_ns(q->time_next_packet),\n\t\t\t\t\t    ktime_to_ns(\n\t\t\t\t\t\t   q->failsafe_next_packet));\n\t\t\t\tsch->qstats.overlimits++;\n\t\t\t\tqdisc_watchdog_schedule_ns(&q->watchdog, next);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (unlikely(len > b->max_skblen))\n\t\tb->max_skblen = len;\n\n\tif (skb_is_gso(skb) && q->rate_flags & CAKE_FLAG_SPLIT_GSO) {\n\t\tstruct sk_buff *segs, *nskb;\n\t\tnetdev_features_t features = netif_skb_features(skb);\n\t\tunsigned int slen = 0, numsegs = 0;\n\n\t\tsegs = skb_gso_segment(skb, features & ~NETIF_F_GSO_MASK);\n\t\tif (IS_ERR_OR_NULL(segs))\n\t\t\treturn qdisc_drop(skb, sch, to_free);\n\n\t\tskb_list_walk_safe(segs, segs, nskb) {\n\t\t\tskb_mark_not_on_list(segs);\n\t\t\tqdisc_skb_cb(segs)->pkt_len = segs->len;\n\t\t\tcobalt_set_enqueue_time(segs, now);\n\t\t\tget_cobalt_cb(segs)->adjusted_len = cake_overhead(q,\n\t\t\t\t\t\t\t\t\t  segs);\n\t\t\tflow_queue_add(flow, segs);\n\n\t\t\tsch->q.qlen++;\n\t\t\tnumsegs++;\n\t\t\tslen += segs->len;\n\t\t\tq->buffer_used += segs->truesize;\n\t\t\tb->packets++;\n\t\t}\n\n\t\t \n\t\tb->bytes\t    += slen;\n\t\tb->backlogs[idx]    += slen;\n\t\tb->tin_backlog      += slen;\n\t\tsch->qstats.backlog += slen;\n\t\tq->avg_window_bytes += slen;\n\n\t\tqdisc_tree_reduce_backlog(sch, 1-numsegs, len-slen);\n\t\tconsume_skb(skb);\n\t} else {\n\t\t \n\t\tcobalt_set_enqueue_time(skb, now);\n\t\tget_cobalt_cb(skb)->adjusted_len = cake_overhead(q, skb);\n\t\tflow_queue_add(flow, skb);\n\n\t\tif (q->ack_filter)\n\t\t\tack = cake_ack_filter(q, flow);\n\n\t\tif (ack) {\n\t\t\tb->ack_drops++;\n\t\t\tsch->qstats.drops++;\n\t\t\tb->bytes += qdisc_pkt_len(ack);\n\t\t\tlen -= qdisc_pkt_len(ack);\n\t\t\tq->buffer_used += skb->truesize - ack->truesize;\n\t\t\tif (q->rate_flags & CAKE_FLAG_INGRESS)\n\t\t\t\tcake_advance_shaper(q, b, ack, now, true);\n\n\t\t\tqdisc_tree_reduce_backlog(sch, 1, qdisc_pkt_len(ack));\n\t\t\tconsume_skb(ack);\n\t\t} else {\n\t\t\tsch->q.qlen++;\n\t\t\tq->buffer_used      += skb->truesize;\n\t\t}\n\n\t\t \n\t\tb->packets++;\n\t\tb->bytes\t    += len;\n\t\tb->backlogs[idx]    += len;\n\t\tb->tin_backlog      += len;\n\t\tsch->qstats.backlog += len;\n\t\tq->avg_window_bytes += len;\n\t}\n\n\tif (q->overflow_timeout)\n\t\tcake_heapify_up(q, b->overflow_idx[idx]);\n\n\t \n\tif (q->rate_flags & CAKE_FLAG_AUTORATE_INGRESS) {\n\t\tu64 packet_interval = \\\n\t\t\tktime_to_ns(ktime_sub(now, q->last_packet_time));\n\n\t\tif (packet_interval > NSEC_PER_SEC)\n\t\t\tpacket_interval = NSEC_PER_SEC;\n\n\t\t \n\t\tq->avg_packet_interval = \\\n\t\t\tcake_ewma(q->avg_packet_interval,\n\t\t\t\t  packet_interval,\n\t\t\t\t  (packet_interval > q->avg_packet_interval ?\n\t\t\t\t\t  2 : 8));\n\n\t\tq->last_packet_time = now;\n\n\t\tif (packet_interval > q->avg_packet_interval) {\n\t\t\tu64 window_interval = \\\n\t\t\t\tktime_to_ns(ktime_sub(now,\n\t\t\t\t\t\t      q->avg_window_begin));\n\t\t\tu64 b = q->avg_window_bytes * (u64)NSEC_PER_SEC;\n\n\t\t\tb = div64_u64(b, window_interval);\n\t\t\tq->avg_peak_bandwidth =\n\t\t\t\tcake_ewma(q->avg_peak_bandwidth, b,\n\t\t\t\t\t  b > q->avg_peak_bandwidth ? 2 : 8);\n\t\t\tq->avg_window_bytes = 0;\n\t\t\tq->avg_window_begin = now;\n\n\t\t\tif (ktime_after(now,\n\t\t\t\t\tktime_add_ms(q->last_reconfig_time,\n\t\t\t\t\t\t     250))) {\n\t\t\t\tq->rate_bps = (q->avg_peak_bandwidth * 15) >> 4;\n\t\t\t\tcake_reconfigure(sch);\n\t\t\t}\n\t\t}\n\t} else {\n\t\tq->avg_window_bytes = 0;\n\t\tq->last_packet_time = now;\n\t}\n\n\t \n\tif (!flow->set || flow->set == CAKE_SET_DECAYING) {\n\t\tstruct cake_host *srchost = &b->hosts[flow->srchost];\n\t\tstruct cake_host *dsthost = &b->hosts[flow->dsthost];\n\t\tu16 host_load = 1;\n\n\t\tif (!flow->set) {\n\t\t\tlist_add_tail(&flow->flowchain, &b->new_flows);\n\t\t} else {\n\t\t\tb->decaying_flow_count--;\n\t\t\tlist_move_tail(&flow->flowchain, &b->new_flows);\n\t\t}\n\t\tflow->set = CAKE_SET_SPARSE;\n\t\tb->sparse_flow_count++;\n\n\t\tif (cake_dsrc(q->flow_mode))\n\t\t\thost_load = max(host_load, srchost->srchost_bulk_flow_count);\n\n\t\tif (cake_ddst(q->flow_mode))\n\t\t\thost_load = max(host_load, dsthost->dsthost_bulk_flow_count);\n\n\t\tflow->deficit = (b->flow_quantum *\n\t\t\t\t quantum_div[host_load]) >> 16;\n\t} else if (flow->set == CAKE_SET_SPARSE_WAIT) {\n\t\tstruct cake_host *srchost = &b->hosts[flow->srchost];\n\t\tstruct cake_host *dsthost = &b->hosts[flow->dsthost];\n\n\t\t \n\t\tflow->set = CAKE_SET_BULK;\n\t\tb->sparse_flow_count--;\n\t\tb->bulk_flow_count++;\n\n\t\tif (cake_dsrc(q->flow_mode))\n\t\t\tsrchost->srchost_bulk_flow_count++;\n\n\t\tif (cake_ddst(q->flow_mode))\n\t\t\tdsthost->dsthost_bulk_flow_count++;\n\n\t}\n\n\tif (q->buffer_used > q->buffer_max_used)\n\t\tq->buffer_max_used = q->buffer_used;\n\n\tif (q->buffer_used > q->buffer_limit) {\n\t\tu32 dropped = 0;\n\n\t\twhile (q->buffer_used > q->buffer_limit) {\n\t\t\tdropped++;\n\t\t\tcake_drop(sch, to_free);\n\t\t}\n\t\tb->drop_overlimit += dropped;\n\t}\n\treturn NET_XMIT_SUCCESS;\n}\n\nstatic struct sk_buff *cake_dequeue_one(struct Qdisc *sch)\n{\n\tstruct cake_sched_data *q = qdisc_priv(sch);\n\tstruct cake_tin_data *b = &q->tins[q->cur_tin];\n\tstruct cake_flow *flow = &b->flows[q->cur_flow];\n\tstruct sk_buff *skb = NULL;\n\tu32 len;\n\n\tif (flow->head) {\n\t\tskb = dequeue_head(flow);\n\t\tlen = qdisc_pkt_len(skb);\n\t\tb->backlogs[q->cur_flow] -= len;\n\t\tb->tin_backlog\t\t -= len;\n\t\tsch->qstats.backlog      -= len;\n\t\tq->buffer_used\t\t -= skb->truesize;\n\t\tsch->q.qlen--;\n\n\t\tif (q->overflow_timeout)\n\t\t\tcake_heapify(q, b->overflow_idx[q->cur_flow]);\n\t}\n\treturn skb;\n}\n\n \nstatic void cake_clear_tin(struct Qdisc *sch, u16 tin)\n{\n\tstruct cake_sched_data *q = qdisc_priv(sch);\n\tstruct sk_buff *skb;\n\n\tq->cur_tin = tin;\n\tfor (q->cur_flow = 0; q->cur_flow < CAKE_QUEUES; q->cur_flow++)\n\t\twhile (!!(skb = cake_dequeue_one(sch)))\n\t\t\tkfree_skb(skb);\n}\n\nstatic struct sk_buff *cake_dequeue(struct Qdisc *sch)\n{\n\tstruct cake_sched_data *q = qdisc_priv(sch);\n\tstruct cake_tin_data *b = &q->tins[q->cur_tin];\n\tstruct cake_host *srchost, *dsthost;\n\tktime_t now = ktime_get();\n\tstruct cake_flow *flow;\n\tstruct list_head *head;\n\tbool first_flow = true;\n\tstruct sk_buff *skb;\n\tu16 host_load;\n\tu64 delay;\n\tu32 len;\n\nbegin:\n\tif (!sch->q.qlen)\n\t\treturn NULL;\n\n\t \n\tif (ktime_after(q->time_next_packet, now) &&\n\t    ktime_after(q->failsafe_next_packet, now)) {\n\t\tu64 next = min(ktime_to_ns(q->time_next_packet),\n\t\t\t       ktime_to_ns(q->failsafe_next_packet));\n\n\t\tsch->qstats.overlimits++;\n\t\tqdisc_watchdog_schedule_ns(&q->watchdog, next);\n\t\treturn NULL;\n\t}\n\n\t \n\tif (!q->rate_ns) {\n\t\t \n\t\tbool wrapped = false, empty = true;\n\n\t\twhile (b->tin_deficit < 0 ||\n\t\t       !(b->sparse_flow_count + b->bulk_flow_count)) {\n\t\t\tif (b->tin_deficit <= 0)\n\t\t\t\tb->tin_deficit += b->tin_quantum;\n\t\t\tif (b->sparse_flow_count + b->bulk_flow_count)\n\t\t\t\tempty = false;\n\n\t\t\tq->cur_tin++;\n\t\t\tb++;\n\t\t\tif (q->cur_tin >= q->tin_cnt) {\n\t\t\t\tq->cur_tin = 0;\n\t\t\t\tb = q->tins;\n\n\t\t\t\tif (wrapped) {\n\t\t\t\t\t \n\t\t\t\t\tif (empty)\n\t\t\t\t\t\treturn NULL;\n\t\t\t\t} else {\n\t\t\t\t\twrapped = true;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t} else {\n\t\t \n\t\tktime_t best_time = KTIME_MAX;\n\t\tint tin, best_tin = 0;\n\n\t\tfor (tin = 0; tin < q->tin_cnt; tin++) {\n\t\t\tb = q->tins + tin;\n\t\t\tif ((b->sparse_flow_count + b->bulk_flow_count) > 0) {\n\t\t\t\tktime_t time_to_pkt = \\\n\t\t\t\t\tktime_sub(b->time_next_packet, now);\n\n\t\t\t\tif (ktime_to_ns(time_to_pkt) <= 0 ||\n\t\t\t\t    ktime_compare(time_to_pkt,\n\t\t\t\t\t\t  best_time) <= 0) {\n\t\t\t\t\tbest_time = time_to_pkt;\n\t\t\t\t\tbest_tin = tin;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tq->cur_tin = best_tin;\n\t\tb = q->tins + best_tin;\n\n\t\t \n\t\tif (unlikely(!(b->sparse_flow_count + b->bulk_flow_count)))\n\t\t\treturn NULL;\n\t}\n\nretry:\n\t \n\thead = &b->decaying_flows;\n\tif (!first_flow || list_empty(head)) {\n\t\thead = &b->new_flows;\n\t\tif (list_empty(head)) {\n\t\t\thead = &b->old_flows;\n\t\t\tif (unlikely(list_empty(head))) {\n\t\t\t\thead = &b->decaying_flows;\n\t\t\t\tif (unlikely(list_empty(head)))\n\t\t\t\t\tgoto begin;\n\t\t\t}\n\t\t}\n\t}\n\tflow = list_first_entry(head, struct cake_flow, flowchain);\n\tq->cur_flow = flow - b->flows;\n\tfirst_flow = false;\n\n\t \n\tsrchost = &b->hosts[flow->srchost];\n\tdsthost = &b->hosts[flow->dsthost];\n\thost_load = 1;\n\n\t \n\tif (flow->deficit <= 0) {\n\t\t \n\t\tif (flow->set == CAKE_SET_SPARSE) {\n\t\t\tif (flow->head) {\n\t\t\t\tb->sparse_flow_count--;\n\t\t\t\tb->bulk_flow_count++;\n\n\t\t\t\tif (cake_dsrc(q->flow_mode))\n\t\t\t\t\tsrchost->srchost_bulk_flow_count++;\n\n\t\t\t\tif (cake_ddst(q->flow_mode))\n\t\t\t\t\tdsthost->dsthost_bulk_flow_count++;\n\n\t\t\t\tflow->set = CAKE_SET_BULK;\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tflow->set = CAKE_SET_SPARSE_WAIT;\n\t\t\t}\n\t\t}\n\n\t\tif (cake_dsrc(q->flow_mode))\n\t\t\thost_load = max(host_load, srchost->srchost_bulk_flow_count);\n\n\t\tif (cake_ddst(q->flow_mode))\n\t\t\thost_load = max(host_load, dsthost->dsthost_bulk_flow_count);\n\n\t\tWARN_ON(host_load > CAKE_QUEUES);\n\n\t\t \n\t\tflow->deficit += (b->flow_quantum * quantum_div[host_load] +\n\t\t\t\t  get_random_u16()) >> 16;\n\t\tlist_move_tail(&flow->flowchain, &b->old_flows);\n\n\t\tgoto retry;\n\t}\n\n\t \n\twhile (1) {\n\t\tskb = cake_dequeue_one(sch);\n\t\tif (!skb) {\n\t\t\t \n\t\t\tif (cobalt_queue_empty(&flow->cvars, &b->cparams, now))\n\t\t\t\tb->unresponsive_flow_count--;\n\n\t\t\tif (flow->cvars.p_drop || flow->cvars.count ||\n\t\t\t    ktime_before(now, flow->cvars.drop_next)) {\n\t\t\t\t \n\t\t\t\tlist_move_tail(&flow->flowchain,\n\t\t\t\t\t       &b->decaying_flows);\n\t\t\t\tif (flow->set == CAKE_SET_BULK) {\n\t\t\t\t\tb->bulk_flow_count--;\n\n\t\t\t\t\tif (cake_dsrc(q->flow_mode))\n\t\t\t\t\t\tsrchost->srchost_bulk_flow_count--;\n\n\t\t\t\t\tif (cake_ddst(q->flow_mode))\n\t\t\t\t\t\tdsthost->dsthost_bulk_flow_count--;\n\n\t\t\t\t\tb->decaying_flow_count++;\n\t\t\t\t} else if (flow->set == CAKE_SET_SPARSE ||\n\t\t\t\t\t   flow->set == CAKE_SET_SPARSE_WAIT) {\n\t\t\t\t\tb->sparse_flow_count--;\n\t\t\t\t\tb->decaying_flow_count++;\n\t\t\t\t}\n\t\t\t\tflow->set = CAKE_SET_DECAYING;\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tlist_del_init(&flow->flowchain);\n\t\t\t\tif (flow->set == CAKE_SET_SPARSE ||\n\t\t\t\t    flow->set == CAKE_SET_SPARSE_WAIT)\n\t\t\t\t\tb->sparse_flow_count--;\n\t\t\t\telse if (flow->set == CAKE_SET_BULK) {\n\t\t\t\t\tb->bulk_flow_count--;\n\n\t\t\t\t\tif (cake_dsrc(q->flow_mode))\n\t\t\t\t\t\tsrchost->srchost_bulk_flow_count--;\n\n\t\t\t\t\tif (cake_ddst(q->flow_mode))\n\t\t\t\t\t\tdsthost->dsthost_bulk_flow_count--;\n\n\t\t\t\t} else\n\t\t\t\t\tb->decaying_flow_count--;\n\n\t\t\t\tflow->set = CAKE_SET_NONE;\n\t\t\t}\n\t\t\tgoto begin;\n\t\t}\n\n\t\t \n\t\tif (!cobalt_should_drop(&flow->cvars, &b->cparams, now, skb,\n\t\t\t\t\t(b->bulk_flow_count *\n\t\t\t\t\t !!(q->rate_flags &\n\t\t\t\t\t    CAKE_FLAG_INGRESS))) ||\n\t\t    !flow->head)\n\t\t\tbreak;\n\n\t\t \n\t\tif (q->rate_flags & CAKE_FLAG_INGRESS) {\n\t\t\tlen = cake_advance_shaper(q, b, skb,\n\t\t\t\t\t\t  now, true);\n\t\t\tflow->deficit -= len;\n\t\t\tb->tin_deficit -= len;\n\t\t}\n\t\tflow->dropped++;\n\t\tb->tin_dropped++;\n\t\tqdisc_tree_reduce_backlog(sch, 1, qdisc_pkt_len(skb));\n\t\tqdisc_qstats_drop(sch);\n\t\tkfree_skb(skb);\n\t\tif (q->rate_flags & CAKE_FLAG_INGRESS)\n\t\t\tgoto retry;\n\t}\n\n\tb->tin_ecn_mark += !!flow->cvars.ecn_marked;\n\tqdisc_bstats_update(sch, skb);\n\n\t \n\tdelay = ktime_to_ns(ktime_sub(now, cobalt_get_enqueue_time(skb)));\n\tb->avge_delay = cake_ewma(b->avge_delay, delay, 8);\n\tb->peak_delay = cake_ewma(b->peak_delay, delay,\n\t\t\t\t  delay > b->peak_delay ? 2 : 8);\n\tb->base_delay = cake_ewma(b->base_delay, delay,\n\t\t\t\t  delay < b->base_delay ? 2 : 8);\n\n\tlen = cake_advance_shaper(q, b, skb, now, false);\n\tflow->deficit -= len;\n\tb->tin_deficit -= len;\n\n\tif (ktime_after(q->time_next_packet, now) && sch->q.qlen) {\n\t\tu64 next = min(ktime_to_ns(q->time_next_packet),\n\t\t\t       ktime_to_ns(q->failsafe_next_packet));\n\n\t\tqdisc_watchdog_schedule_ns(&q->watchdog, next);\n\t} else if (!sch->q.qlen) {\n\t\tint i;\n\n\t\tfor (i = 0; i < q->tin_cnt; i++) {\n\t\t\tif (q->tins[i].decaying_flow_count) {\n\t\t\t\tktime_t next = \\\n\t\t\t\t\tktime_add_ns(now,\n\t\t\t\t\t\t     q->tins[i].cparams.target);\n\n\t\t\t\tqdisc_watchdog_schedule_ns(&q->watchdog,\n\t\t\t\t\t\t\t   ktime_to_ns(next));\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (q->overflow_timeout)\n\t\tq->overflow_timeout--;\n\n\treturn skb;\n}\n\nstatic void cake_reset(struct Qdisc *sch)\n{\n\tstruct cake_sched_data *q = qdisc_priv(sch);\n\tu32 c;\n\n\tif (!q->tins)\n\t\treturn;\n\n\tfor (c = 0; c < CAKE_MAX_TINS; c++)\n\t\tcake_clear_tin(sch, c);\n}\n\nstatic const struct nla_policy cake_policy[TCA_CAKE_MAX + 1] = {\n\t[TCA_CAKE_BASE_RATE64]   = { .type = NLA_U64 },\n\t[TCA_CAKE_DIFFSERV_MODE] = { .type = NLA_U32 },\n\t[TCA_CAKE_ATM]\t\t = { .type = NLA_U32 },\n\t[TCA_CAKE_FLOW_MODE]     = { .type = NLA_U32 },\n\t[TCA_CAKE_OVERHEAD]      = { .type = NLA_S32 },\n\t[TCA_CAKE_RTT]\t\t = { .type = NLA_U32 },\n\t[TCA_CAKE_TARGET]\t = { .type = NLA_U32 },\n\t[TCA_CAKE_AUTORATE]      = { .type = NLA_U32 },\n\t[TCA_CAKE_MEMORY]\t = { .type = NLA_U32 },\n\t[TCA_CAKE_NAT]\t\t = { .type = NLA_U32 },\n\t[TCA_CAKE_RAW]\t\t = { .type = NLA_U32 },\n\t[TCA_CAKE_WASH]\t\t = { .type = NLA_U32 },\n\t[TCA_CAKE_MPU]\t\t = { .type = NLA_U32 },\n\t[TCA_CAKE_INGRESS]\t = { .type = NLA_U32 },\n\t[TCA_CAKE_ACK_FILTER]\t = { .type = NLA_U32 },\n\t[TCA_CAKE_SPLIT_GSO]\t = { .type = NLA_U32 },\n\t[TCA_CAKE_FWMARK]\t = { .type = NLA_U32 },\n};\n\nstatic void cake_set_rate(struct cake_tin_data *b, u64 rate, u32 mtu,\n\t\t\t  u64 target_ns, u64 rtt_est_ns)\n{\n\t \n\tstatic const u64 MIN_RATE = 64;\n\tu32 byte_target = mtu;\n\tu64 byte_target_ns;\n\tu8  rate_shft = 0;\n\tu64 rate_ns = 0;\n\n\tb->flow_quantum = 1514;\n\tif (rate) {\n\t\tb->flow_quantum = max(min(rate >> 12, 1514ULL), 300ULL);\n\t\trate_shft = 34;\n\t\trate_ns = ((u64)NSEC_PER_SEC) << rate_shft;\n\t\trate_ns = div64_u64(rate_ns, max(MIN_RATE, rate));\n\t\twhile (!!(rate_ns >> 34)) {\n\t\t\trate_ns >>= 1;\n\t\t\trate_shft--;\n\t\t}\n\t}  \n\n\tb->tin_rate_bps  = rate;\n\tb->tin_rate_ns   = rate_ns;\n\tb->tin_rate_shft = rate_shft;\n\n\tbyte_target_ns = (byte_target * rate_ns) >> rate_shft;\n\n\tb->cparams.target = max((byte_target_ns * 3) / 2, target_ns);\n\tb->cparams.interval = max(rtt_est_ns +\n\t\t\t\t     b->cparams.target - target_ns,\n\t\t\t\t     b->cparams.target * 2);\n\tb->cparams.mtu_time = byte_target_ns;\n\tb->cparams.p_inc = 1 << 24;  \n\tb->cparams.p_dec = 1 << 20;  \n}\n\nstatic int cake_config_besteffort(struct Qdisc *sch)\n{\n\tstruct cake_sched_data *q = qdisc_priv(sch);\n\tstruct cake_tin_data *b = &q->tins[0];\n\tu32 mtu = psched_mtu(qdisc_dev(sch));\n\tu64 rate = q->rate_bps;\n\n\tq->tin_cnt = 1;\n\n\tq->tin_index = besteffort;\n\tq->tin_order = normal_order;\n\n\tcake_set_rate(b, rate, mtu,\n\t\t      us_to_ns(q->target), us_to_ns(q->interval));\n\tb->tin_quantum = 65535;\n\n\treturn 0;\n}\n\nstatic int cake_config_precedence(struct Qdisc *sch)\n{\n\t \n\tstruct cake_sched_data *q = qdisc_priv(sch);\n\tu32 mtu = psched_mtu(qdisc_dev(sch));\n\tu64 rate = q->rate_bps;\n\tu32 quantum = 256;\n\tu32 i;\n\n\tq->tin_cnt = 8;\n\tq->tin_index = precedence;\n\tq->tin_order = normal_order;\n\n\tfor (i = 0; i < q->tin_cnt; i++) {\n\t\tstruct cake_tin_data *b = &q->tins[i];\n\n\t\tcake_set_rate(b, rate, mtu, us_to_ns(q->target),\n\t\t\t      us_to_ns(q->interval));\n\n\t\tb->tin_quantum = max_t(u16, 1U, quantum);\n\n\t\t \n\t\trate  *= 7;\n\t\trate >>= 3;\n\n\t\tquantum  *= 7;\n\t\tquantum >>= 3;\n\t}\n\n\treturn 0;\n}\n\n \n\n \n\nstatic int cake_config_diffserv8(struct Qdisc *sch)\n{\n \n\n\tstruct cake_sched_data *q = qdisc_priv(sch);\n\tu32 mtu = psched_mtu(qdisc_dev(sch));\n\tu64 rate = q->rate_bps;\n\tu32 quantum = 256;\n\tu32 i;\n\n\tq->tin_cnt = 8;\n\n\t \n\tq->tin_index = diffserv8;\n\tq->tin_order = normal_order;\n\n\t \n\tfor (i = 0; i < q->tin_cnt; i++) {\n\t\tstruct cake_tin_data *b = &q->tins[i];\n\n\t\tcake_set_rate(b, rate, mtu, us_to_ns(q->target),\n\t\t\t      us_to_ns(q->interval));\n\n\t\tb->tin_quantum = max_t(u16, 1U, quantum);\n\n\t\t \n\t\trate  *= 7;\n\t\trate >>= 3;\n\n\t\tquantum  *= 7;\n\t\tquantum >>= 3;\n\t}\n\n\treturn 0;\n}\n\nstatic int cake_config_diffserv4(struct Qdisc *sch)\n{\n \n\n\tstruct cake_sched_data *q = qdisc_priv(sch);\n\tu32 mtu = psched_mtu(qdisc_dev(sch));\n\tu64 rate = q->rate_bps;\n\tu32 quantum = 1024;\n\n\tq->tin_cnt = 4;\n\n\t \n\tq->tin_index = diffserv4;\n\tq->tin_order = bulk_order;\n\n\t \n\tcake_set_rate(&q->tins[0], rate, mtu,\n\t\t      us_to_ns(q->target), us_to_ns(q->interval));\n\tcake_set_rate(&q->tins[1], rate >> 4, mtu,\n\t\t      us_to_ns(q->target), us_to_ns(q->interval));\n\tcake_set_rate(&q->tins[2], rate >> 1, mtu,\n\t\t      us_to_ns(q->target), us_to_ns(q->interval));\n\tcake_set_rate(&q->tins[3], rate >> 2, mtu,\n\t\t      us_to_ns(q->target), us_to_ns(q->interval));\n\n\t \n\tq->tins[0].tin_quantum = quantum;\n\tq->tins[1].tin_quantum = quantum >> 4;\n\tq->tins[2].tin_quantum = quantum >> 1;\n\tq->tins[3].tin_quantum = quantum >> 2;\n\n\treturn 0;\n}\n\nstatic int cake_config_diffserv3(struct Qdisc *sch)\n{\n \n\tstruct cake_sched_data *q = qdisc_priv(sch);\n\tu32 mtu = psched_mtu(qdisc_dev(sch));\n\tu64 rate = q->rate_bps;\n\tu32 quantum = 1024;\n\n\tq->tin_cnt = 3;\n\n\t \n\tq->tin_index = diffserv3;\n\tq->tin_order = bulk_order;\n\n\t \n\tcake_set_rate(&q->tins[0], rate, mtu,\n\t\t      us_to_ns(q->target), us_to_ns(q->interval));\n\tcake_set_rate(&q->tins[1], rate >> 4, mtu,\n\t\t      us_to_ns(q->target), us_to_ns(q->interval));\n\tcake_set_rate(&q->tins[2], rate >> 2, mtu,\n\t\t      us_to_ns(q->target), us_to_ns(q->interval));\n\n\t \n\tq->tins[0].tin_quantum = quantum;\n\tq->tins[1].tin_quantum = quantum >> 4;\n\tq->tins[2].tin_quantum = quantum >> 2;\n\n\treturn 0;\n}\n\nstatic void cake_reconfigure(struct Qdisc *sch)\n{\n\tstruct cake_sched_data *q = qdisc_priv(sch);\n\tint c, ft;\n\n\tswitch (q->tin_mode) {\n\tcase CAKE_DIFFSERV_BESTEFFORT:\n\t\tft = cake_config_besteffort(sch);\n\t\tbreak;\n\n\tcase CAKE_DIFFSERV_PRECEDENCE:\n\t\tft = cake_config_precedence(sch);\n\t\tbreak;\n\n\tcase CAKE_DIFFSERV_DIFFSERV8:\n\t\tft = cake_config_diffserv8(sch);\n\t\tbreak;\n\n\tcase CAKE_DIFFSERV_DIFFSERV4:\n\t\tft = cake_config_diffserv4(sch);\n\t\tbreak;\n\n\tcase CAKE_DIFFSERV_DIFFSERV3:\n\tdefault:\n\t\tft = cake_config_diffserv3(sch);\n\t\tbreak;\n\t}\n\n\tfor (c = q->tin_cnt; c < CAKE_MAX_TINS; c++) {\n\t\tcake_clear_tin(sch, c);\n\t\tq->tins[c].cparams.mtu_time = q->tins[ft].cparams.mtu_time;\n\t}\n\n\tq->rate_ns   = q->tins[ft].tin_rate_ns;\n\tq->rate_shft = q->tins[ft].tin_rate_shft;\n\n\tif (q->buffer_config_limit) {\n\t\tq->buffer_limit = q->buffer_config_limit;\n\t} else if (q->rate_bps) {\n\t\tu64 t = q->rate_bps * q->interval;\n\n\t\tdo_div(t, USEC_PER_SEC / 4);\n\t\tq->buffer_limit = max_t(u32, t, 4U << 20);\n\t} else {\n\t\tq->buffer_limit = ~0;\n\t}\n\n\tsch->flags &= ~TCQ_F_CAN_BYPASS;\n\n\tq->buffer_limit = min(q->buffer_limit,\n\t\t\t      max(sch->limit * psched_mtu(qdisc_dev(sch)),\n\t\t\t\t  q->buffer_config_limit));\n}\n\nstatic int cake_change(struct Qdisc *sch, struct nlattr *opt,\n\t\t       struct netlink_ext_ack *extack)\n{\n\tstruct cake_sched_data *q = qdisc_priv(sch);\n\tstruct nlattr *tb[TCA_CAKE_MAX + 1];\n\tint err;\n\n\terr = nla_parse_nested_deprecated(tb, TCA_CAKE_MAX, opt, cake_policy,\n\t\t\t\t\t  extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[TCA_CAKE_NAT]) {\n#if IS_ENABLED(CONFIG_NF_CONNTRACK)\n\t\tq->flow_mode &= ~CAKE_FLOW_NAT_FLAG;\n\t\tq->flow_mode |= CAKE_FLOW_NAT_FLAG *\n\t\t\t!!nla_get_u32(tb[TCA_CAKE_NAT]);\n#else\n\t\tNL_SET_ERR_MSG_ATTR(extack, tb[TCA_CAKE_NAT],\n\t\t\t\t    \"No conntrack support in kernel\");\n\t\treturn -EOPNOTSUPP;\n#endif\n\t}\n\n\tif (tb[TCA_CAKE_BASE_RATE64])\n\t\tq->rate_bps = nla_get_u64(tb[TCA_CAKE_BASE_RATE64]);\n\n\tif (tb[TCA_CAKE_DIFFSERV_MODE])\n\t\tq->tin_mode = nla_get_u32(tb[TCA_CAKE_DIFFSERV_MODE]);\n\n\tif (tb[TCA_CAKE_WASH]) {\n\t\tif (!!nla_get_u32(tb[TCA_CAKE_WASH]))\n\t\t\tq->rate_flags |= CAKE_FLAG_WASH;\n\t\telse\n\t\t\tq->rate_flags &= ~CAKE_FLAG_WASH;\n\t}\n\n\tif (tb[TCA_CAKE_FLOW_MODE])\n\t\tq->flow_mode = ((q->flow_mode & CAKE_FLOW_NAT_FLAG) |\n\t\t\t\t(nla_get_u32(tb[TCA_CAKE_FLOW_MODE]) &\n\t\t\t\t\tCAKE_FLOW_MASK));\n\n\tif (tb[TCA_CAKE_ATM])\n\t\tq->atm_mode = nla_get_u32(tb[TCA_CAKE_ATM]);\n\n\tif (tb[TCA_CAKE_OVERHEAD]) {\n\t\tq->rate_overhead = nla_get_s32(tb[TCA_CAKE_OVERHEAD]);\n\t\tq->rate_flags |= CAKE_FLAG_OVERHEAD;\n\n\t\tq->max_netlen = 0;\n\t\tq->max_adjlen = 0;\n\t\tq->min_netlen = ~0;\n\t\tq->min_adjlen = ~0;\n\t}\n\n\tif (tb[TCA_CAKE_RAW]) {\n\t\tq->rate_flags &= ~CAKE_FLAG_OVERHEAD;\n\n\t\tq->max_netlen = 0;\n\t\tq->max_adjlen = 0;\n\t\tq->min_netlen = ~0;\n\t\tq->min_adjlen = ~0;\n\t}\n\n\tif (tb[TCA_CAKE_MPU])\n\t\tq->rate_mpu = nla_get_u32(tb[TCA_CAKE_MPU]);\n\n\tif (tb[TCA_CAKE_RTT]) {\n\t\tq->interval = nla_get_u32(tb[TCA_CAKE_RTT]);\n\n\t\tif (!q->interval)\n\t\t\tq->interval = 1;\n\t}\n\n\tif (tb[TCA_CAKE_TARGET]) {\n\t\tq->target = nla_get_u32(tb[TCA_CAKE_TARGET]);\n\n\t\tif (!q->target)\n\t\t\tq->target = 1;\n\t}\n\n\tif (tb[TCA_CAKE_AUTORATE]) {\n\t\tif (!!nla_get_u32(tb[TCA_CAKE_AUTORATE]))\n\t\t\tq->rate_flags |= CAKE_FLAG_AUTORATE_INGRESS;\n\t\telse\n\t\t\tq->rate_flags &= ~CAKE_FLAG_AUTORATE_INGRESS;\n\t}\n\n\tif (tb[TCA_CAKE_INGRESS]) {\n\t\tif (!!nla_get_u32(tb[TCA_CAKE_INGRESS]))\n\t\t\tq->rate_flags |= CAKE_FLAG_INGRESS;\n\t\telse\n\t\t\tq->rate_flags &= ~CAKE_FLAG_INGRESS;\n\t}\n\n\tif (tb[TCA_CAKE_ACK_FILTER])\n\t\tq->ack_filter = nla_get_u32(tb[TCA_CAKE_ACK_FILTER]);\n\n\tif (tb[TCA_CAKE_MEMORY])\n\t\tq->buffer_config_limit = nla_get_u32(tb[TCA_CAKE_MEMORY]);\n\n\tif (tb[TCA_CAKE_SPLIT_GSO]) {\n\t\tif (!!nla_get_u32(tb[TCA_CAKE_SPLIT_GSO]))\n\t\t\tq->rate_flags |= CAKE_FLAG_SPLIT_GSO;\n\t\telse\n\t\t\tq->rate_flags &= ~CAKE_FLAG_SPLIT_GSO;\n\t}\n\n\tif (tb[TCA_CAKE_FWMARK]) {\n\t\tq->fwmark_mask = nla_get_u32(tb[TCA_CAKE_FWMARK]);\n\t\tq->fwmark_shft = q->fwmark_mask ? __ffs(q->fwmark_mask) : 0;\n\t}\n\n\tif (q->tins) {\n\t\tsch_tree_lock(sch);\n\t\tcake_reconfigure(sch);\n\t\tsch_tree_unlock(sch);\n\t}\n\n\treturn 0;\n}\n\nstatic void cake_destroy(struct Qdisc *sch)\n{\n\tstruct cake_sched_data *q = qdisc_priv(sch);\n\n\tqdisc_watchdog_cancel(&q->watchdog);\n\ttcf_block_put(q->block);\n\tkvfree(q->tins);\n}\n\nstatic int cake_init(struct Qdisc *sch, struct nlattr *opt,\n\t\t     struct netlink_ext_ack *extack)\n{\n\tstruct cake_sched_data *q = qdisc_priv(sch);\n\tint i, j, err;\n\n\tsch->limit = 10240;\n\tq->tin_mode = CAKE_DIFFSERV_DIFFSERV3;\n\tq->flow_mode  = CAKE_FLOW_TRIPLE;\n\n\tq->rate_bps = 0;  \n\n\tq->interval = 100000;  \n\tq->target   =   5000;  \n\tq->rate_flags |= CAKE_FLAG_SPLIT_GSO;\n\tq->cur_tin = 0;\n\tq->cur_flow  = 0;\n\n\tqdisc_watchdog_init(&q->watchdog, sch);\n\n\tif (opt) {\n\t\terr = cake_change(sch, opt, extack);\n\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\terr = tcf_block_get(&q->block, &q->filter_list, sch, extack);\n\tif (err)\n\t\treturn err;\n\n\tquantum_div[0] = ~0;\n\tfor (i = 1; i <= CAKE_QUEUES; i++)\n\t\tquantum_div[i] = 65535 / i;\n\n\tq->tins = kvcalloc(CAKE_MAX_TINS, sizeof(struct cake_tin_data),\n\t\t\t   GFP_KERNEL);\n\tif (!q->tins)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < CAKE_MAX_TINS; i++) {\n\t\tstruct cake_tin_data *b = q->tins + i;\n\n\t\tINIT_LIST_HEAD(&b->new_flows);\n\t\tINIT_LIST_HEAD(&b->old_flows);\n\t\tINIT_LIST_HEAD(&b->decaying_flows);\n\t\tb->sparse_flow_count = 0;\n\t\tb->bulk_flow_count = 0;\n\t\tb->decaying_flow_count = 0;\n\n\t\tfor (j = 0; j < CAKE_QUEUES; j++) {\n\t\t\tstruct cake_flow *flow = b->flows + j;\n\t\t\tu32 k = j * CAKE_MAX_TINS + i;\n\n\t\t\tINIT_LIST_HEAD(&flow->flowchain);\n\t\t\tcobalt_vars_init(&flow->cvars);\n\n\t\t\tq->overflow_heap[k].t = i;\n\t\t\tq->overflow_heap[k].b = j;\n\t\t\tb->overflow_idx[j] = k;\n\t\t}\n\t}\n\n\tcake_reconfigure(sch);\n\tq->avg_peak_bandwidth = q->rate_bps;\n\tq->min_netlen = ~0;\n\tq->min_adjlen = ~0;\n\treturn 0;\n}\n\nstatic int cake_dump(struct Qdisc *sch, struct sk_buff *skb)\n{\n\tstruct cake_sched_data *q = qdisc_priv(sch);\n\tstruct nlattr *opts;\n\n\topts = nla_nest_start_noflag(skb, TCA_OPTIONS);\n\tif (!opts)\n\t\tgoto nla_put_failure;\n\n\tif (nla_put_u64_64bit(skb, TCA_CAKE_BASE_RATE64, q->rate_bps,\n\t\t\t      TCA_CAKE_PAD))\n\t\tgoto nla_put_failure;\n\n\tif (nla_put_u32(skb, TCA_CAKE_FLOW_MODE,\n\t\t\tq->flow_mode & CAKE_FLOW_MASK))\n\t\tgoto nla_put_failure;\n\n\tif (nla_put_u32(skb, TCA_CAKE_RTT, q->interval))\n\t\tgoto nla_put_failure;\n\n\tif (nla_put_u32(skb, TCA_CAKE_TARGET, q->target))\n\t\tgoto nla_put_failure;\n\n\tif (nla_put_u32(skb, TCA_CAKE_MEMORY, q->buffer_config_limit))\n\t\tgoto nla_put_failure;\n\n\tif (nla_put_u32(skb, TCA_CAKE_AUTORATE,\n\t\t\t!!(q->rate_flags & CAKE_FLAG_AUTORATE_INGRESS)))\n\t\tgoto nla_put_failure;\n\n\tif (nla_put_u32(skb, TCA_CAKE_INGRESS,\n\t\t\t!!(q->rate_flags & CAKE_FLAG_INGRESS)))\n\t\tgoto nla_put_failure;\n\n\tif (nla_put_u32(skb, TCA_CAKE_ACK_FILTER, q->ack_filter))\n\t\tgoto nla_put_failure;\n\n\tif (nla_put_u32(skb, TCA_CAKE_NAT,\n\t\t\t!!(q->flow_mode & CAKE_FLOW_NAT_FLAG)))\n\t\tgoto nla_put_failure;\n\n\tif (nla_put_u32(skb, TCA_CAKE_DIFFSERV_MODE, q->tin_mode))\n\t\tgoto nla_put_failure;\n\n\tif (nla_put_u32(skb, TCA_CAKE_WASH,\n\t\t\t!!(q->rate_flags & CAKE_FLAG_WASH)))\n\t\tgoto nla_put_failure;\n\n\tif (nla_put_u32(skb, TCA_CAKE_OVERHEAD, q->rate_overhead))\n\t\tgoto nla_put_failure;\n\n\tif (!(q->rate_flags & CAKE_FLAG_OVERHEAD))\n\t\tif (nla_put_u32(skb, TCA_CAKE_RAW, 0))\n\t\t\tgoto nla_put_failure;\n\n\tif (nla_put_u32(skb, TCA_CAKE_ATM, q->atm_mode))\n\t\tgoto nla_put_failure;\n\n\tif (nla_put_u32(skb, TCA_CAKE_MPU, q->rate_mpu))\n\t\tgoto nla_put_failure;\n\n\tif (nla_put_u32(skb, TCA_CAKE_SPLIT_GSO,\n\t\t\t!!(q->rate_flags & CAKE_FLAG_SPLIT_GSO)))\n\t\tgoto nla_put_failure;\n\n\tif (nla_put_u32(skb, TCA_CAKE_FWMARK, q->fwmark_mask))\n\t\tgoto nla_put_failure;\n\n\treturn nla_nest_end(skb, opts);\n\nnla_put_failure:\n\treturn -1;\n}\n\nstatic int cake_dump_stats(struct Qdisc *sch, struct gnet_dump *d)\n{\n\tstruct nlattr *stats = nla_nest_start_noflag(d->skb, TCA_STATS_APP);\n\tstruct cake_sched_data *q = qdisc_priv(sch);\n\tstruct nlattr *tstats, *ts;\n\tint i;\n\n\tif (!stats)\n\t\treturn -1;\n\n#define PUT_STAT_U32(attr, data) do {\t\t\t\t       \\\n\t\tif (nla_put_u32(d->skb, TCA_CAKE_STATS_ ## attr, data)) \\\n\t\t\tgoto nla_put_failure;\t\t\t       \\\n\t} while (0)\n#define PUT_STAT_U64(attr, data) do {\t\t\t\t       \\\n\t\tif (nla_put_u64_64bit(d->skb, TCA_CAKE_STATS_ ## attr, \\\n\t\t\t\t\tdata, TCA_CAKE_STATS_PAD)) \\\n\t\t\tgoto nla_put_failure;\t\t\t       \\\n\t} while (0)\n\n\tPUT_STAT_U64(CAPACITY_ESTIMATE64, q->avg_peak_bandwidth);\n\tPUT_STAT_U32(MEMORY_LIMIT, q->buffer_limit);\n\tPUT_STAT_U32(MEMORY_USED, q->buffer_max_used);\n\tPUT_STAT_U32(AVG_NETOFF, ((q->avg_netoff + 0x8000) >> 16));\n\tPUT_STAT_U32(MAX_NETLEN, q->max_netlen);\n\tPUT_STAT_U32(MAX_ADJLEN, q->max_adjlen);\n\tPUT_STAT_U32(MIN_NETLEN, q->min_netlen);\n\tPUT_STAT_U32(MIN_ADJLEN, q->min_adjlen);\n\n#undef PUT_STAT_U32\n#undef PUT_STAT_U64\n\n\ttstats = nla_nest_start_noflag(d->skb, TCA_CAKE_STATS_TIN_STATS);\n\tif (!tstats)\n\t\tgoto nla_put_failure;\n\n#define PUT_TSTAT_U32(attr, data) do {\t\t\t\t\t\\\n\t\tif (nla_put_u32(d->skb, TCA_CAKE_TIN_STATS_ ## attr, data)) \\\n\t\t\tgoto nla_put_failure;\t\t\t\t\\\n\t} while (0)\n#define PUT_TSTAT_U64(attr, data) do {\t\t\t\t\t\\\n\t\tif (nla_put_u64_64bit(d->skb, TCA_CAKE_TIN_STATS_ ## attr, \\\n\t\t\t\t\tdata, TCA_CAKE_TIN_STATS_PAD))\t\\\n\t\t\tgoto nla_put_failure;\t\t\t\t\\\n\t} while (0)\n\n\tfor (i = 0; i < q->tin_cnt; i++) {\n\t\tstruct cake_tin_data *b = &q->tins[q->tin_order[i]];\n\n\t\tts = nla_nest_start_noflag(d->skb, i + 1);\n\t\tif (!ts)\n\t\t\tgoto nla_put_failure;\n\n\t\tPUT_TSTAT_U64(THRESHOLD_RATE64, b->tin_rate_bps);\n\t\tPUT_TSTAT_U64(SENT_BYTES64, b->bytes);\n\t\tPUT_TSTAT_U32(BACKLOG_BYTES, b->tin_backlog);\n\n\t\tPUT_TSTAT_U32(TARGET_US,\n\t\t\t      ktime_to_us(ns_to_ktime(b->cparams.target)));\n\t\tPUT_TSTAT_U32(INTERVAL_US,\n\t\t\t      ktime_to_us(ns_to_ktime(b->cparams.interval)));\n\n\t\tPUT_TSTAT_U32(SENT_PACKETS, b->packets);\n\t\tPUT_TSTAT_U32(DROPPED_PACKETS, b->tin_dropped);\n\t\tPUT_TSTAT_U32(ECN_MARKED_PACKETS, b->tin_ecn_mark);\n\t\tPUT_TSTAT_U32(ACKS_DROPPED_PACKETS, b->ack_drops);\n\n\t\tPUT_TSTAT_U32(PEAK_DELAY_US,\n\t\t\t      ktime_to_us(ns_to_ktime(b->peak_delay)));\n\t\tPUT_TSTAT_U32(AVG_DELAY_US,\n\t\t\t      ktime_to_us(ns_to_ktime(b->avge_delay)));\n\t\tPUT_TSTAT_U32(BASE_DELAY_US,\n\t\t\t      ktime_to_us(ns_to_ktime(b->base_delay)));\n\n\t\tPUT_TSTAT_U32(WAY_INDIRECT_HITS, b->way_hits);\n\t\tPUT_TSTAT_U32(WAY_MISSES, b->way_misses);\n\t\tPUT_TSTAT_U32(WAY_COLLISIONS, b->way_collisions);\n\n\t\tPUT_TSTAT_U32(SPARSE_FLOWS, b->sparse_flow_count +\n\t\t\t\t\t    b->decaying_flow_count);\n\t\tPUT_TSTAT_U32(BULK_FLOWS, b->bulk_flow_count);\n\t\tPUT_TSTAT_U32(UNRESPONSIVE_FLOWS, b->unresponsive_flow_count);\n\t\tPUT_TSTAT_U32(MAX_SKBLEN, b->max_skblen);\n\n\t\tPUT_TSTAT_U32(FLOW_QUANTUM, b->flow_quantum);\n\t\tnla_nest_end(d->skb, ts);\n\t}\n\n#undef PUT_TSTAT_U32\n#undef PUT_TSTAT_U64\n\n\tnla_nest_end(d->skb, tstats);\n\treturn nla_nest_end(d->skb, stats);\n\nnla_put_failure:\n\tnla_nest_cancel(d->skb, stats);\n\treturn -1;\n}\n\nstatic struct Qdisc *cake_leaf(struct Qdisc *sch, unsigned long arg)\n{\n\treturn NULL;\n}\n\nstatic unsigned long cake_find(struct Qdisc *sch, u32 classid)\n{\n\treturn 0;\n}\n\nstatic unsigned long cake_bind(struct Qdisc *sch, unsigned long parent,\n\t\t\t       u32 classid)\n{\n\treturn 0;\n}\n\nstatic void cake_unbind(struct Qdisc *q, unsigned long cl)\n{\n}\n\nstatic struct tcf_block *cake_tcf_block(struct Qdisc *sch, unsigned long cl,\n\t\t\t\t\tstruct netlink_ext_ack *extack)\n{\n\tstruct cake_sched_data *q = qdisc_priv(sch);\n\n\tif (cl)\n\t\treturn NULL;\n\treturn q->block;\n}\n\nstatic int cake_dump_class(struct Qdisc *sch, unsigned long cl,\n\t\t\t   struct sk_buff *skb, struct tcmsg *tcm)\n{\n\ttcm->tcm_handle |= TC_H_MIN(cl);\n\treturn 0;\n}\n\nstatic int cake_dump_class_stats(struct Qdisc *sch, unsigned long cl,\n\t\t\t\t struct gnet_dump *d)\n{\n\tstruct cake_sched_data *q = qdisc_priv(sch);\n\tconst struct cake_flow *flow = NULL;\n\tstruct gnet_stats_queue qs = { 0 };\n\tstruct nlattr *stats;\n\tu32 idx = cl - 1;\n\n\tif (idx < CAKE_QUEUES * q->tin_cnt) {\n\t\tconst struct cake_tin_data *b = \\\n\t\t\t&q->tins[q->tin_order[idx / CAKE_QUEUES]];\n\t\tconst struct sk_buff *skb;\n\n\t\tflow = &b->flows[idx % CAKE_QUEUES];\n\n\t\tif (flow->head) {\n\t\t\tsch_tree_lock(sch);\n\t\t\tskb = flow->head;\n\t\t\twhile (skb) {\n\t\t\t\tqs.qlen++;\n\t\t\t\tskb = skb->next;\n\t\t\t}\n\t\t\tsch_tree_unlock(sch);\n\t\t}\n\t\tqs.backlog = b->backlogs[idx % CAKE_QUEUES];\n\t\tqs.drops = flow->dropped;\n\t}\n\tif (gnet_stats_copy_queue(d, NULL, &qs, qs.qlen) < 0)\n\t\treturn -1;\n\tif (flow) {\n\t\tktime_t now = ktime_get();\n\n\t\tstats = nla_nest_start_noflag(d->skb, TCA_STATS_APP);\n\t\tif (!stats)\n\t\t\treturn -1;\n\n#define PUT_STAT_U32(attr, data) do {\t\t\t\t       \\\n\t\tif (nla_put_u32(d->skb, TCA_CAKE_STATS_ ## attr, data)) \\\n\t\t\tgoto nla_put_failure;\t\t\t       \\\n\t} while (0)\n#define PUT_STAT_S32(attr, data) do {\t\t\t\t       \\\n\t\tif (nla_put_s32(d->skb, TCA_CAKE_STATS_ ## attr, data)) \\\n\t\t\tgoto nla_put_failure;\t\t\t       \\\n\t} while (0)\n\n\t\tPUT_STAT_S32(DEFICIT, flow->deficit);\n\t\tPUT_STAT_U32(DROPPING, flow->cvars.dropping);\n\t\tPUT_STAT_U32(COBALT_COUNT, flow->cvars.count);\n\t\tPUT_STAT_U32(P_DROP, flow->cvars.p_drop);\n\t\tif (flow->cvars.p_drop) {\n\t\t\tPUT_STAT_S32(BLUE_TIMER_US,\n\t\t\t\t     ktime_to_us(\n\t\t\t\t\t     ktime_sub(now,\n\t\t\t\t\t\t       flow->cvars.blue_timer)));\n\t\t}\n\t\tif (flow->cvars.dropping) {\n\t\t\tPUT_STAT_S32(DROP_NEXT_US,\n\t\t\t\t     ktime_to_us(\n\t\t\t\t\t     ktime_sub(now,\n\t\t\t\t\t\t       flow->cvars.drop_next)));\n\t\t}\n\n\t\tif (nla_nest_end(d->skb, stats) < 0)\n\t\t\treturn -1;\n\t}\n\n\treturn 0;\n\nnla_put_failure:\n\tnla_nest_cancel(d->skb, stats);\n\treturn -1;\n}\n\nstatic void cake_walk(struct Qdisc *sch, struct qdisc_walker *arg)\n{\n\tstruct cake_sched_data *q = qdisc_priv(sch);\n\tunsigned int i, j;\n\n\tif (arg->stop)\n\t\treturn;\n\n\tfor (i = 0; i < q->tin_cnt; i++) {\n\t\tstruct cake_tin_data *b = &q->tins[q->tin_order[i]];\n\n\t\tfor (j = 0; j < CAKE_QUEUES; j++) {\n\t\t\tif (list_empty(&b->flows[j].flowchain)) {\n\t\t\t\targ->count++;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (!tc_qdisc_stats_dump(sch, i * CAKE_QUEUES + j + 1,\n\t\t\t\t\t\t arg))\n\t\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic const struct Qdisc_class_ops cake_class_ops = {\n\t.leaf\t\t=\tcake_leaf,\n\t.find\t\t=\tcake_find,\n\t.tcf_block\t=\tcake_tcf_block,\n\t.bind_tcf\t=\tcake_bind,\n\t.unbind_tcf\t=\tcake_unbind,\n\t.dump\t\t=\tcake_dump_class,\n\t.dump_stats\t=\tcake_dump_class_stats,\n\t.walk\t\t=\tcake_walk,\n};\n\nstatic struct Qdisc_ops cake_qdisc_ops __read_mostly = {\n\t.cl_ops\t\t=\t&cake_class_ops,\n\t.id\t\t=\t\"cake\",\n\t.priv_size\t=\tsizeof(struct cake_sched_data),\n\t.enqueue\t=\tcake_enqueue,\n\t.dequeue\t=\tcake_dequeue,\n\t.peek\t\t=\tqdisc_peek_dequeued,\n\t.init\t\t=\tcake_init,\n\t.reset\t\t=\tcake_reset,\n\t.destroy\t=\tcake_destroy,\n\t.change\t\t=\tcake_change,\n\t.dump\t\t=\tcake_dump,\n\t.dump_stats\t=\tcake_dump_stats,\n\t.owner\t\t=\tTHIS_MODULE,\n};\n\nstatic int __init cake_module_init(void)\n{\n\treturn register_qdisc(&cake_qdisc_ops);\n}\n\nstatic void __exit cake_module_exit(void)\n{\n\tunregister_qdisc(&cake_qdisc_ops);\n}\n\nmodule_init(cake_module_init)\nmodule_exit(cake_module_exit)\nMODULE_AUTHOR(\"Jonathan Morton\");\nMODULE_LICENSE(\"Dual BSD/GPL\");\nMODULE_DESCRIPTION(\"The CAKE shaper.\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}