{
  "module_name": "sch_htb.c",
  "hash_id": "2cc2fd02f6a5f9a152e6ad4bdbab198cb1ea701c3227162e7f819389704b95c7",
  "original_prompt": "Ingested from linux-6.6.14/net/sched/sch_htb.c",
  "human_readable_source": "\n \n#include <linux/module.h>\n#include <linux/moduleparam.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/string.h>\n#include <linux/errno.h>\n#include <linux/skbuff.h>\n#include <linux/list.h>\n#include <linux/compiler.h>\n#include <linux/rbtree.h>\n#include <linux/workqueue.h>\n#include <linux/slab.h>\n#include <net/netlink.h>\n#include <net/sch_generic.h>\n#include <net/pkt_sched.h>\n#include <net/pkt_cls.h>\n\n \n\nstatic int htb_hysteresis __read_mostly = 0;  \n#define HTB_VER 0x30011\t\t \n\n#if HTB_VER >> 16 != TC_HTB_PROTOVER\n#error \"Mismatched sch_htb.c and pkt_sch.h\"\n#endif\n\n \nmodule_param    (htb_hysteresis, int, 0640);\nMODULE_PARM_DESC(htb_hysteresis, \"Hysteresis mode, less CPU load, less accurate\");\n\nstatic int htb_rate_est = 0;  \nmodule_param(htb_rate_est, int, 0640);\nMODULE_PARM_DESC(htb_rate_est, \"setup a default rate estimator (4sec 16sec) for htb classes\");\n\n \nenum htb_cmode {\n\tHTB_CANT_SEND,\t\t \n\tHTB_MAY_BORROW,\t\t \n\tHTB_CAN_SEND\t\t \n};\n\nstruct htb_prio {\n\tunion {\n\t\tstruct rb_root\trow;\n\t\tstruct rb_root\tfeed;\n\t};\n\tstruct rb_node\t*ptr;\n\t \n\tu32\t\tlast_ptr_id;\n};\n\n \nstruct htb_class {\n\tstruct Qdisc_class_common common;\n\tstruct psched_ratecfg\trate;\n\tstruct psched_ratecfg\tceil;\n\ts64\t\t\tbuffer, cbuffer; \n\ts64\t\t\tmbuffer;\t \n\tu32\t\t\tprio;\t\t \n\tint\t\t\tquantum;\t \n\n\tstruct tcf_proto __rcu\t*filter_list;\t \n\tstruct tcf_block\t*block;\n\n\tint\t\t\tlevel;\t\t \n\tunsigned int\t\tchildren;\n\tstruct htb_class\t*parent;\t \n\n\tstruct net_rate_estimator __rcu *rate_est;\n\n\t \n\tstruct gnet_stats_basic_sync bstats;\n\tstruct gnet_stats_basic_sync bstats_bias;\n\tstruct tc_htb_xstats\txstats;\t \n\n\t \n\ts64\t\t\ttokens, ctokens; \n\ts64\t\t\tt_c;\t\t \n\n\tunion {\n\t\tstruct htb_class_leaf {\n\t\t\tint\t\tdeficit[TC_HTB_MAXDEPTH];\n\t\t\tstruct Qdisc\t*q;\n\t\t\tstruct netdev_queue *offload_queue;\n\t\t} leaf;\n\t\tstruct htb_class_inner {\n\t\t\tstruct htb_prio clprio[TC_HTB_NUMPRIO];\n\t\t} inner;\n\t};\n\ts64\t\t\tpq_key;\n\n\tint\t\t\tprio_activity;\t \n\tenum htb_cmode\t\tcmode;\t\t \n\tstruct rb_node\t\tpq_node;\t \n\tstruct rb_node\t\tnode[TC_HTB_NUMPRIO];\t \n\n\tunsigned int drops ____cacheline_aligned_in_smp;\n\tunsigned int\t\toverlimits;\n};\n\nstruct htb_level {\n\tstruct rb_root\twait_pq;\n\tstruct htb_prio hprio[TC_HTB_NUMPRIO];\n};\n\nstruct htb_sched {\n\tstruct Qdisc_class_hash clhash;\n\tint\t\t\tdefcls;\t\t \n\tint\t\t\trate2quantum;\t \n\n\t \n\tstruct tcf_proto __rcu\t*filter_list;\n\tstruct tcf_block\t*block;\n\n#define HTB_WARN_TOOMANYEVENTS\t0x1\n\tunsigned int\t\twarned;\t \n\tint\t\t\tdirect_qlen;\n\tstruct work_struct\twork;\n\n\t \n\tstruct qdisc_skb_head\tdirect_queue;\n\tu32\t\t\tdirect_pkts;\n\tu32\t\t\toverlimits;\n\n\tstruct qdisc_watchdog\twatchdog;\n\n\ts64\t\t\tnow;\t \n\n\t \n\ts64\t\t\tnear_ev_cache[TC_HTB_MAXDEPTH];\n\n\tint\t\t\trow_mask[TC_HTB_MAXDEPTH];\n\n\tstruct htb_level\thlevel[TC_HTB_MAXDEPTH];\n\n\tstruct Qdisc\t\t**direct_qdiscs;\n\tunsigned int            num_direct_qdiscs;\n\n\tbool\t\t\toffload;\n};\n\n \nstatic inline struct htb_class *htb_find(u32 handle, struct Qdisc *sch)\n{\n\tstruct htb_sched *q = qdisc_priv(sch);\n\tstruct Qdisc_class_common *clc;\n\n\tclc = qdisc_class_find(&q->clhash, handle);\n\tif (clc == NULL)\n\t\treturn NULL;\n\treturn container_of(clc, struct htb_class, common);\n}\n\nstatic unsigned long htb_search(struct Qdisc *sch, u32 handle)\n{\n\treturn (unsigned long)htb_find(handle, sch);\n}\n\n#define HTB_DIRECT ((struct htb_class *)-1L)\n\n \nstatic struct htb_class *htb_classify(struct sk_buff *skb, struct Qdisc *sch,\n\t\t\t\t      int *qerr)\n{\n\tstruct htb_sched *q = qdisc_priv(sch);\n\tstruct htb_class *cl;\n\tstruct tcf_result res;\n\tstruct tcf_proto *tcf;\n\tint result;\n\n\t \n\tif (skb->priority == sch->handle)\n\t\treturn HTB_DIRECT;\t \n\tcl = htb_find(skb->priority, sch);\n\tif (cl) {\n\t\tif (cl->level == 0)\n\t\t\treturn cl;\n\t\t \n\t\ttcf = rcu_dereference_bh(cl->filter_list);\n\t} else {\n\t\ttcf = rcu_dereference_bh(q->filter_list);\n\t}\n\n\t*qerr = NET_XMIT_SUCCESS | __NET_XMIT_BYPASS;\n\twhile (tcf && (result = tcf_classify(skb, NULL, tcf, &res, false)) >= 0) {\n#ifdef CONFIG_NET_CLS_ACT\n\t\tswitch (result) {\n\t\tcase TC_ACT_QUEUED:\n\t\tcase TC_ACT_STOLEN:\n\t\tcase TC_ACT_TRAP:\n\t\t\t*qerr = NET_XMIT_SUCCESS | __NET_XMIT_STOLEN;\n\t\t\tfallthrough;\n\t\tcase TC_ACT_SHOT:\n\t\t\treturn NULL;\n\t\t}\n#endif\n\t\tcl = (void *)res.class;\n\t\tif (!cl) {\n\t\t\tif (res.classid == sch->handle)\n\t\t\t\treturn HTB_DIRECT;\t \n\t\t\tcl = htb_find(res.classid, sch);\n\t\t\tif (!cl)\n\t\t\t\tbreak;\t \n\t\t}\n\t\tif (!cl->level)\n\t\t\treturn cl;\t \n\n\t\t \n\t\ttcf = rcu_dereference_bh(cl->filter_list);\n\t}\n\t \n\tcl = htb_find(TC_H_MAKE(TC_H_MAJ(sch->handle), q->defcls), sch);\n\tif (!cl || cl->level)\n\t\treturn HTB_DIRECT;\t \n\treturn cl;\n}\n\n \nstatic void htb_add_to_id_tree(struct rb_root *root,\n\t\t\t       struct htb_class *cl, int prio)\n{\n\tstruct rb_node **p = &root->rb_node, *parent = NULL;\n\n\twhile (*p) {\n\t\tstruct htb_class *c;\n\t\tparent = *p;\n\t\tc = rb_entry(parent, struct htb_class, node[prio]);\n\n\t\tif (cl->common.classid > c->common.classid)\n\t\t\tp = &parent->rb_right;\n\t\telse\n\t\t\tp = &parent->rb_left;\n\t}\n\trb_link_node(&cl->node[prio], parent, p);\n\trb_insert_color(&cl->node[prio], root);\n}\n\n \nstatic void htb_add_to_wait_tree(struct htb_sched *q,\n\t\t\t\t struct htb_class *cl, s64 delay)\n{\n\tstruct rb_node **p = &q->hlevel[cl->level].wait_pq.rb_node, *parent = NULL;\n\n\tcl->pq_key = q->now + delay;\n\tif (cl->pq_key == q->now)\n\t\tcl->pq_key++;\n\n\t \n\tif (q->near_ev_cache[cl->level] > cl->pq_key)\n\t\tq->near_ev_cache[cl->level] = cl->pq_key;\n\n\twhile (*p) {\n\t\tstruct htb_class *c;\n\t\tparent = *p;\n\t\tc = rb_entry(parent, struct htb_class, pq_node);\n\t\tif (cl->pq_key >= c->pq_key)\n\t\t\tp = &parent->rb_right;\n\t\telse\n\t\t\tp = &parent->rb_left;\n\t}\n\trb_link_node(&cl->pq_node, parent, p);\n\trb_insert_color(&cl->pq_node, &q->hlevel[cl->level].wait_pq);\n}\n\n \nstatic inline void htb_next_rb_node(struct rb_node **n)\n{\n\t*n = rb_next(*n);\n}\n\n \nstatic inline void htb_add_class_to_row(struct htb_sched *q,\n\t\t\t\t\tstruct htb_class *cl, int mask)\n{\n\tq->row_mask[cl->level] |= mask;\n\twhile (mask) {\n\t\tint prio = ffz(~mask);\n\t\tmask &= ~(1 << prio);\n\t\thtb_add_to_id_tree(&q->hlevel[cl->level].hprio[prio].row, cl, prio);\n\t}\n}\n\n \nstatic void htb_safe_rb_erase(struct rb_node *rb, struct rb_root *root)\n{\n\tif (RB_EMPTY_NODE(rb)) {\n\t\tWARN_ON(1);\n\t} else {\n\t\trb_erase(rb, root);\n\t\tRB_CLEAR_NODE(rb);\n\t}\n}\n\n\n \nstatic inline void htb_remove_class_from_row(struct htb_sched *q,\n\t\t\t\t\t\t struct htb_class *cl, int mask)\n{\n\tint m = 0;\n\tstruct htb_level *hlevel = &q->hlevel[cl->level];\n\n\twhile (mask) {\n\t\tint prio = ffz(~mask);\n\t\tstruct htb_prio *hprio = &hlevel->hprio[prio];\n\n\t\tmask &= ~(1 << prio);\n\t\tif (hprio->ptr == cl->node + prio)\n\t\t\thtb_next_rb_node(&hprio->ptr);\n\n\t\thtb_safe_rb_erase(cl->node + prio, &hprio->row);\n\t\tif (!hprio->row.rb_node)\n\t\t\tm |= 1 << prio;\n\t}\n\tq->row_mask[cl->level] &= ~m;\n}\n\n \nstatic void htb_activate_prios(struct htb_sched *q, struct htb_class *cl)\n{\n\tstruct htb_class *p = cl->parent;\n\tlong m, mask = cl->prio_activity;\n\n\twhile (cl->cmode == HTB_MAY_BORROW && p && mask) {\n\t\tm = mask;\n\t\twhile (m) {\n\t\t\tunsigned int prio = ffz(~m);\n\n\t\t\tif (WARN_ON_ONCE(prio >= ARRAY_SIZE(p->inner.clprio)))\n\t\t\t\tbreak;\n\t\t\tm &= ~(1 << prio);\n\n\t\t\tif (p->inner.clprio[prio].feed.rb_node)\n\t\t\t\t \n\t\t\t\tmask &= ~(1 << prio);\n\n\t\t\thtb_add_to_id_tree(&p->inner.clprio[prio].feed, cl, prio);\n\t\t}\n\t\tp->prio_activity |= mask;\n\t\tcl = p;\n\t\tp = cl->parent;\n\n\t}\n\tif (cl->cmode == HTB_CAN_SEND && mask)\n\t\thtb_add_class_to_row(q, cl, mask);\n}\n\n \nstatic void htb_deactivate_prios(struct htb_sched *q, struct htb_class *cl)\n{\n\tstruct htb_class *p = cl->parent;\n\tlong m, mask = cl->prio_activity;\n\n\twhile (cl->cmode == HTB_MAY_BORROW && p && mask) {\n\t\tm = mask;\n\t\tmask = 0;\n\t\twhile (m) {\n\t\t\tint prio = ffz(~m);\n\t\t\tm &= ~(1 << prio);\n\n\t\t\tif (p->inner.clprio[prio].ptr == cl->node + prio) {\n\t\t\t\t \n\t\t\t\tp->inner.clprio[prio].last_ptr_id = cl->common.classid;\n\t\t\t\tp->inner.clprio[prio].ptr = NULL;\n\t\t\t}\n\n\t\t\thtb_safe_rb_erase(cl->node + prio,\n\t\t\t\t\t  &p->inner.clprio[prio].feed);\n\n\t\t\tif (!p->inner.clprio[prio].feed.rb_node)\n\t\t\t\tmask |= 1 << prio;\n\t\t}\n\n\t\tp->prio_activity &= ~mask;\n\t\tcl = p;\n\t\tp = cl->parent;\n\n\t}\n\tif (cl->cmode == HTB_CAN_SEND && mask)\n\t\thtb_remove_class_from_row(q, cl, mask);\n}\n\nstatic inline s64 htb_lowater(const struct htb_class *cl)\n{\n\tif (htb_hysteresis)\n\t\treturn cl->cmode != HTB_CANT_SEND ? -cl->cbuffer : 0;\n\telse\n\t\treturn 0;\n}\nstatic inline s64 htb_hiwater(const struct htb_class *cl)\n{\n\tif (htb_hysteresis)\n\t\treturn cl->cmode == HTB_CAN_SEND ? -cl->buffer : 0;\n\telse\n\t\treturn 0;\n}\n\n\n \nstatic inline enum htb_cmode\nhtb_class_mode(struct htb_class *cl, s64 *diff)\n{\n\ts64 toks;\n\n\tif ((toks = (cl->ctokens + *diff)) < htb_lowater(cl)) {\n\t\t*diff = -toks;\n\t\treturn HTB_CANT_SEND;\n\t}\n\n\tif ((toks = (cl->tokens + *diff)) >= htb_hiwater(cl))\n\t\treturn HTB_CAN_SEND;\n\n\t*diff = -toks;\n\treturn HTB_MAY_BORROW;\n}\n\n \nstatic void\nhtb_change_class_mode(struct htb_sched *q, struct htb_class *cl, s64 *diff)\n{\n\tenum htb_cmode new_mode = htb_class_mode(cl, diff);\n\n\tif (new_mode == cl->cmode)\n\t\treturn;\n\n\tif (new_mode == HTB_CANT_SEND) {\n\t\tcl->overlimits++;\n\t\tq->overlimits++;\n\t}\n\n\tif (cl->prio_activity) {\t \n\t\tif (cl->cmode != HTB_CANT_SEND)\n\t\t\thtb_deactivate_prios(q, cl);\n\t\tcl->cmode = new_mode;\n\t\tif (new_mode != HTB_CANT_SEND)\n\t\t\thtb_activate_prios(q, cl);\n\t} else\n\t\tcl->cmode = new_mode;\n}\n\n \nstatic inline void htb_activate(struct htb_sched *q, struct htb_class *cl)\n{\n\tWARN_ON(cl->level || !cl->leaf.q || !cl->leaf.q->q.qlen);\n\n\tif (!cl->prio_activity) {\n\t\tcl->prio_activity = 1 << cl->prio;\n\t\thtb_activate_prios(q, cl);\n\t}\n}\n\n \nstatic inline void htb_deactivate(struct htb_sched *q, struct htb_class *cl)\n{\n\tWARN_ON(!cl->prio_activity);\n\n\thtb_deactivate_prios(q, cl);\n\tcl->prio_activity = 0;\n}\n\nstatic int htb_enqueue(struct sk_buff *skb, struct Qdisc *sch,\n\t\t       struct sk_buff **to_free)\n{\n\tint ret;\n\tunsigned int len = qdisc_pkt_len(skb);\n\tstruct htb_sched *q = qdisc_priv(sch);\n\tstruct htb_class *cl = htb_classify(skb, sch, &ret);\n\n\tif (cl == HTB_DIRECT) {\n\t\t \n\t\tif (q->direct_queue.qlen < q->direct_qlen) {\n\t\t\t__qdisc_enqueue_tail(skb, &q->direct_queue);\n\t\t\tq->direct_pkts++;\n\t\t} else {\n\t\t\treturn qdisc_drop(skb, sch, to_free);\n\t\t}\n#ifdef CONFIG_NET_CLS_ACT\n\t} else if (!cl) {\n\t\tif (ret & __NET_XMIT_BYPASS)\n\t\t\tqdisc_qstats_drop(sch);\n\t\t__qdisc_drop(skb, to_free);\n\t\treturn ret;\n#endif\n\t} else if ((ret = qdisc_enqueue(skb, cl->leaf.q,\n\t\t\t\t\tto_free)) != NET_XMIT_SUCCESS) {\n\t\tif (net_xmit_drop_count(ret)) {\n\t\t\tqdisc_qstats_drop(sch);\n\t\t\tcl->drops++;\n\t\t}\n\t\treturn ret;\n\t} else {\n\t\thtb_activate(q, cl);\n\t}\n\n\tsch->qstats.backlog += len;\n\tsch->q.qlen++;\n\treturn NET_XMIT_SUCCESS;\n}\n\nstatic inline void htb_accnt_tokens(struct htb_class *cl, int bytes, s64 diff)\n{\n\ts64 toks = diff + cl->tokens;\n\n\tif (toks > cl->buffer)\n\t\ttoks = cl->buffer;\n\ttoks -= (s64) psched_l2t_ns(&cl->rate, bytes);\n\tif (toks <= -cl->mbuffer)\n\t\ttoks = 1 - cl->mbuffer;\n\n\tcl->tokens = toks;\n}\n\nstatic inline void htb_accnt_ctokens(struct htb_class *cl, int bytes, s64 diff)\n{\n\ts64 toks = diff + cl->ctokens;\n\n\tif (toks > cl->cbuffer)\n\t\ttoks = cl->cbuffer;\n\ttoks -= (s64) psched_l2t_ns(&cl->ceil, bytes);\n\tif (toks <= -cl->mbuffer)\n\t\ttoks = 1 - cl->mbuffer;\n\n\tcl->ctokens = toks;\n}\n\n \nstatic void htb_charge_class(struct htb_sched *q, struct htb_class *cl,\n\t\t\t     int level, struct sk_buff *skb)\n{\n\tint bytes = qdisc_pkt_len(skb);\n\tenum htb_cmode old_mode;\n\ts64 diff;\n\n\twhile (cl) {\n\t\tdiff = min_t(s64, q->now - cl->t_c, cl->mbuffer);\n\t\tif (cl->level >= level) {\n\t\t\tif (cl->level == level)\n\t\t\t\tcl->xstats.lends++;\n\t\t\thtb_accnt_tokens(cl, bytes, diff);\n\t\t} else {\n\t\t\tcl->xstats.borrows++;\n\t\t\tcl->tokens += diff;\t \n\t\t}\n\t\thtb_accnt_ctokens(cl, bytes, diff);\n\t\tcl->t_c = q->now;\n\n\t\told_mode = cl->cmode;\n\t\tdiff = 0;\n\t\thtb_change_class_mode(q, cl, &diff);\n\t\tif (old_mode != cl->cmode) {\n\t\t\tif (old_mode != HTB_CAN_SEND)\n\t\t\t\thtb_safe_rb_erase(&cl->pq_node, &q->hlevel[cl->level].wait_pq);\n\t\t\tif (cl->cmode != HTB_CAN_SEND)\n\t\t\t\thtb_add_to_wait_tree(q, cl, diff);\n\t\t}\n\n\t\t \n\t\tif (cl->level)\n\t\t\tbstats_update(&cl->bstats, skb);\n\n\t\tcl = cl->parent;\n\t}\n}\n\n \nstatic s64 htb_do_events(struct htb_sched *q, const int level,\n\t\t\t unsigned long start)\n{\n\t \n\tunsigned long stop_at = start + 2;\n\tstruct rb_root *wait_pq = &q->hlevel[level].wait_pq;\n\n\twhile (time_before(jiffies, stop_at)) {\n\t\tstruct htb_class *cl;\n\t\ts64 diff;\n\t\tstruct rb_node *p = rb_first(wait_pq);\n\n\t\tif (!p)\n\t\t\treturn 0;\n\n\t\tcl = rb_entry(p, struct htb_class, pq_node);\n\t\tif (cl->pq_key > q->now)\n\t\t\treturn cl->pq_key;\n\n\t\thtb_safe_rb_erase(p, wait_pq);\n\t\tdiff = min_t(s64, q->now - cl->t_c, cl->mbuffer);\n\t\thtb_change_class_mode(q, cl, &diff);\n\t\tif (cl->cmode != HTB_CAN_SEND)\n\t\t\thtb_add_to_wait_tree(q, cl, diff);\n\t}\n\n\t \n\tif (!(q->warned & HTB_WARN_TOOMANYEVENTS)) {\n\t\tpr_warn(\"htb: too many events!\\n\");\n\t\tq->warned |= HTB_WARN_TOOMANYEVENTS;\n\t}\n\n\treturn q->now;\n}\n\n \nstatic struct rb_node *htb_id_find_next_upper(int prio, struct rb_node *n,\n\t\t\t\t\t      u32 id)\n{\n\tstruct rb_node *r = NULL;\n\twhile (n) {\n\t\tstruct htb_class *cl =\n\t\t    rb_entry(n, struct htb_class, node[prio]);\n\n\t\tif (id > cl->common.classid) {\n\t\t\tn = n->rb_right;\n\t\t} else if (id < cl->common.classid) {\n\t\t\tr = n;\n\t\t\tn = n->rb_left;\n\t\t} else {\n\t\t\treturn n;\n\t\t}\n\t}\n\treturn r;\n}\n\n \nstatic struct htb_class *htb_lookup_leaf(struct htb_prio *hprio, const int prio)\n{\n\tint i;\n\tstruct {\n\t\tstruct rb_node *root;\n\t\tstruct rb_node **pptr;\n\t\tu32 *pid;\n\t} stk[TC_HTB_MAXDEPTH], *sp = stk;\n\n\tBUG_ON(!hprio->row.rb_node);\n\tsp->root = hprio->row.rb_node;\n\tsp->pptr = &hprio->ptr;\n\tsp->pid = &hprio->last_ptr_id;\n\n\tfor (i = 0; i < 65535; i++) {\n\t\tif (!*sp->pptr && *sp->pid) {\n\t\t\t \n\t\t\t*sp->pptr =\n\t\t\t    htb_id_find_next_upper(prio, sp->root, *sp->pid);\n\t\t}\n\t\t*sp->pid = 0;\t \n\t\tif (!*sp->pptr) {\t \n\t\t\t*sp->pptr = sp->root;\n\t\t\twhile ((*sp->pptr)->rb_left)\n\t\t\t\t*sp->pptr = (*sp->pptr)->rb_left;\n\t\t\tif (sp > stk) {\n\t\t\t\tsp--;\n\t\t\t\tif (!*sp->pptr) {\n\t\t\t\t\tWARN_ON(1);\n\t\t\t\t\treturn NULL;\n\t\t\t\t}\n\t\t\t\thtb_next_rb_node(sp->pptr);\n\t\t\t}\n\t\t} else {\n\t\t\tstruct htb_class *cl;\n\t\t\tstruct htb_prio *clp;\n\n\t\t\tcl = rb_entry(*sp->pptr, struct htb_class, node[prio]);\n\t\t\tif (!cl->level)\n\t\t\t\treturn cl;\n\t\t\tclp = &cl->inner.clprio[prio];\n\t\t\t(++sp)->root = clp->feed.rb_node;\n\t\t\tsp->pptr = &clp->ptr;\n\t\t\tsp->pid = &clp->last_ptr_id;\n\t\t}\n\t}\n\tWARN_ON(1);\n\treturn NULL;\n}\n\n \nstatic struct sk_buff *htb_dequeue_tree(struct htb_sched *q, const int prio,\n\t\t\t\t\tconst int level)\n{\n\tstruct sk_buff *skb = NULL;\n\tstruct htb_class *cl, *start;\n\tstruct htb_level *hlevel = &q->hlevel[level];\n\tstruct htb_prio *hprio = &hlevel->hprio[prio];\n\n\t \n\tstart = cl = htb_lookup_leaf(hprio, prio);\n\n\tdo {\nnext:\n\t\tif (unlikely(!cl))\n\t\t\treturn NULL;\n\n\t\t \n\t\tif (unlikely(cl->leaf.q->q.qlen == 0)) {\n\t\t\tstruct htb_class *next;\n\t\t\thtb_deactivate(q, cl);\n\n\t\t\t \n\t\t\tif ((q->row_mask[level] & (1 << prio)) == 0)\n\t\t\t\treturn NULL;\n\n\t\t\tnext = htb_lookup_leaf(hprio, prio);\n\n\t\t\tif (cl == start)\t \n\t\t\t\tstart = next;\n\t\t\tcl = next;\n\t\t\tgoto next;\n\t\t}\n\n\t\tskb = cl->leaf.q->dequeue(cl->leaf.q);\n\t\tif (likely(skb != NULL))\n\t\t\tbreak;\n\n\t\tqdisc_warn_nonwc(\"htb\", cl->leaf.q);\n\t\thtb_next_rb_node(level ? &cl->parent->inner.clprio[prio].ptr:\n\t\t\t\t\t &q->hlevel[0].hprio[prio].ptr);\n\t\tcl = htb_lookup_leaf(hprio, prio);\n\n\t} while (cl != start);\n\n\tif (likely(skb != NULL)) {\n\t\tbstats_update(&cl->bstats, skb);\n\t\tcl->leaf.deficit[level] -= qdisc_pkt_len(skb);\n\t\tif (cl->leaf.deficit[level] < 0) {\n\t\t\tcl->leaf.deficit[level] += cl->quantum;\n\t\t\thtb_next_rb_node(level ? &cl->parent->inner.clprio[prio].ptr :\n\t\t\t\t\t\t &q->hlevel[0].hprio[prio].ptr);\n\t\t}\n\t\t \n\t\tif (!cl->leaf.q->q.qlen)\n\t\t\thtb_deactivate(q, cl);\n\t\thtb_charge_class(q, cl, level, skb);\n\t}\n\treturn skb;\n}\n\nstatic struct sk_buff *htb_dequeue(struct Qdisc *sch)\n{\n\tstruct sk_buff *skb;\n\tstruct htb_sched *q = qdisc_priv(sch);\n\tint level;\n\ts64 next_event;\n\tunsigned long start_at;\n\n\t \n\tskb = __qdisc_dequeue_head(&q->direct_queue);\n\tif (skb != NULL) {\nok:\n\t\tqdisc_bstats_update(sch, skb);\n\t\tqdisc_qstats_backlog_dec(sch, skb);\n\t\tsch->q.qlen--;\n\t\treturn skb;\n\t}\n\n\tif (!sch->q.qlen)\n\t\tgoto fin;\n\tq->now = ktime_get_ns();\n\tstart_at = jiffies;\n\n\tnext_event = q->now + 5LLU * NSEC_PER_SEC;\n\n\tfor (level = 0; level < TC_HTB_MAXDEPTH; level++) {\n\t\t \n\t\tint m;\n\t\ts64 event = q->near_ev_cache[level];\n\n\t\tif (q->now >= event) {\n\t\t\tevent = htb_do_events(q, level, start_at);\n\t\t\tif (!event)\n\t\t\t\tevent = q->now + NSEC_PER_SEC;\n\t\t\tq->near_ev_cache[level] = event;\n\t\t}\n\n\t\tif (next_event > event)\n\t\t\tnext_event = event;\n\n\t\tm = ~q->row_mask[level];\n\t\twhile (m != (int)(-1)) {\n\t\t\tint prio = ffz(m);\n\n\t\t\tm |= 1 << prio;\n\t\t\tskb = htb_dequeue_tree(q, prio, level);\n\t\t\tif (likely(skb != NULL))\n\t\t\t\tgoto ok;\n\t\t}\n\t}\n\tif (likely(next_event > q->now))\n\t\tqdisc_watchdog_schedule_ns(&q->watchdog, next_event);\n\telse\n\t\tschedule_work(&q->work);\nfin:\n\treturn skb;\n}\n\n \n \nstatic void htb_reset(struct Qdisc *sch)\n{\n\tstruct htb_sched *q = qdisc_priv(sch);\n\tstruct htb_class *cl;\n\tunsigned int i;\n\n\tfor (i = 0; i < q->clhash.hashsize; i++) {\n\t\thlist_for_each_entry(cl, &q->clhash.hash[i], common.hnode) {\n\t\t\tif (cl->level)\n\t\t\t\tmemset(&cl->inner, 0, sizeof(cl->inner));\n\t\t\telse {\n\t\t\t\tif (cl->leaf.q && !q->offload)\n\t\t\t\t\tqdisc_reset(cl->leaf.q);\n\t\t\t}\n\t\t\tcl->prio_activity = 0;\n\t\t\tcl->cmode = HTB_CAN_SEND;\n\t\t}\n\t}\n\tqdisc_watchdog_cancel(&q->watchdog);\n\t__qdisc_reset_queue(&q->direct_queue);\n\tmemset(q->hlevel, 0, sizeof(q->hlevel));\n\tmemset(q->row_mask, 0, sizeof(q->row_mask));\n}\n\nstatic const struct nla_policy htb_policy[TCA_HTB_MAX + 1] = {\n\t[TCA_HTB_PARMS]\t= { .len = sizeof(struct tc_htb_opt) },\n\t[TCA_HTB_INIT]\t= { .len = sizeof(struct tc_htb_glob) },\n\t[TCA_HTB_CTAB]\t= { .type = NLA_BINARY, .len = TC_RTAB_SIZE },\n\t[TCA_HTB_RTAB]\t= { .type = NLA_BINARY, .len = TC_RTAB_SIZE },\n\t[TCA_HTB_DIRECT_QLEN] = { .type = NLA_U32 },\n\t[TCA_HTB_RATE64] = { .type = NLA_U64 },\n\t[TCA_HTB_CEIL64] = { .type = NLA_U64 },\n\t[TCA_HTB_OFFLOAD] = { .type = NLA_FLAG },\n};\n\nstatic void htb_work_func(struct work_struct *work)\n{\n\tstruct htb_sched *q = container_of(work, struct htb_sched, work);\n\tstruct Qdisc *sch = q->watchdog.qdisc;\n\n\trcu_read_lock();\n\t__netif_schedule(qdisc_root(sch));\n\trcu_read_unlock();\n}\n\nstatic void htb_set_lockdep_class_child(struct Qdisc *q)\n{\n\tstatic struct lock_class_key child_key;\n\n\tlockdep_set_class(qdisc_lock(q), &child_key);\n}\n\nstatic int htb_offload(struct net_device *dev, struct tc_htb_qopt_offload *opt)\n{\n\treturn dev->netdev_ops->ndo_setup_tc(dev, TC_SETUP_QDISC_HTB, opt);\n}\n\nstatic int htb_init(struct Qdisc *sch, struct nlattr *opt,\n\t\t    struct netlink_ext_ack *extack)\n{\n\tstruct net_device *dev = qdisc_dev(sch);\n\tstruct tc_htb_qopt_offload offload_opt;\n\tstruct htb_sched *q = qdisc_priv(sch);\n\tstruct nlattr *tb[TCA_HTB_MAX + 1];\n\tstruct tc_htb_glob *gopt;\n\tunsigned int ntx;\n\tbool offload;\n\tint err;\n\n\tqdisc_watchdog_init(&q->watchdog, sch);\n\tINIT_WORK(&q->work, htb_work_func);\n\n\tif (!opt)\n\t\treturn -EINVAL;\n\n\terr = tcf_block_get(&q->block, &q->filter_list, sch, extack);\n\tif (err)\n\t\treturn err;\n\n\terr = nla_parse_nested_deprecated(tb, TCA_HTB_MAX, opt, htb_policy,\n\t\t\t\t\t  NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (!tb[TCA_HTB_INIT])\n\t\treturn -EINVAL;\n\n\tgopt = nla_data(tb[TCA_HTB_INIT]);\n\tif (gopt->version != HTB_VER >> 16)\n\t\treturn -EINVAL;\n\n\toffload = nla_get_flag(tb[TCA_HTB_OFFLOAD]);\n\n\tif (offload) {\n\t\tif (sch->parent != TC_H_ROOT) {\n\t\t\tNL_SET_ERR_MSG(extack, \"HTB must be the root qdisc to use offload\");\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!tc_can_offload(dev) || !dev->netdev_ops->ndo_setup_tc) {\n\t\t\tNL_SET_ERR_MSG(extack, \"hw-tc-offload ethtool feature flag must be on\");\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tq->num_direct_qdiscs = dev->real_num_tx_queues;\n\t\tq->direct_qdiscs = kcalloc(q->num_direct_qdiscs,\n\t\t\t\t\t   sizeof(*q->direct_qdiscs),\n\t\t\t\t\t   GFP_KERNEL);\n\t\tif (!q->direct_qdiscs)\n\t\t\treturn -ENOMEM;\n\t}\n\n\terr = qdisc_class_hash_init(&q->clhash);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[TCA_HTB_DIRECT_QLEN])\n\t\tq->direct_qlen = nla_get_u32(tb[TCA_HTB_DIRECT_QLEN]);\n\telse\n\t\tq->direct_qlen = qdisc_dev(sch)->tx_queue_len;\n\n\tif ((q->rate2quantum = gopt->rate2quantum) < 1)\n\t\tq->rate2quantum = 1;\n\tq->defcls = gopt->defcls;\n\n\tif (!offload)\n\t\treturn 0;\n\n\tfor (ntx = 0; ntx < q->num_direct_qdiscs; ntx++) {\n\t\tstruct netdev_queue *dev_queue = netdev_get_tx_queue(dev, ntx);\n\t\tstruct Qdisc *qdisc;\n\n\t\tqdisc = qdisc_create_dflt(dev_queue, &pfifo_qdisc_ops,\n\t\t\t\t\t  TC_H_MAKE(sch->handle, 0), extack);\n\t\tif (!qdisc) {\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\thtb_set_lockdep_class_child(qdisc);\n\t\tq->direct_qdiscs[ntx] = qdisc;\n\t\tqdisc->flags |= TCQ_F_ONETXQUEUE | TCQ_F_NOPARENT;\n\t}\n\n\tsch->flags |= TCQ_F_MQROOT;\n\n\toffload_opt = (struct tc_htb_qopt_offload) {\n\t\t.command = TC_HTB_CREATE,\n\t\t.parent_classid = TC_H_MAJ(sch->handle) >> 16,\n\t\t.classid = TC_H_MIN(q->defcls),\n\t\t.extack = extack,\n\t};\n\terr = htb_offload(dev, &offload_opt);\n\tif (err)\n\t\treturn err;\n\n\t \n\tq->offload = true;\n\n\treturn 0;\n}\n\nstatic void htb_attach_offload(struct Qdisc *sch)\n{\n\tstruct net_device *dev = qdisc_dev(sch);\n\tstruct htb_sched *q = qdisc_priv(sch);\n\tunsigned int ntx;\n\n\tfor (ntx = 0; ntx < q->num_direct_qdiscs; ntx++) {\n\t\tstruct Qdisc *old, *qdisc = q->direct_qdiscs[ntx];\n\n\t\told = dev_graft_qdisc(qdisc->dev_queue, qdisc);\n\t\tqdisc_put(old);\n\t\tqdisc_hash_add(qdisc, false);\n\t}\n\tfor (ntx = q->num_direct_qdiscs; ntx < dev->num_tx_queues; ntx++) {\n\t\tstruct netdev_queue *dev_queue = netdev_get_tx_queue(dev, ntx);\n\t\tstruct Qdisc *old = dev_graft_qdisc(dev_queue, NULL);\n\n\t\tqdisc_put(old);\n\t}\n\n\tkfree(q->direct_qdiscs);\n\tq->direct_qdiscs = NULL;\n}\n\nstatic void htb_attach_software(struct Qdisc *sch)\n{\n\tstruct net_device *dev = qdisc_dev(sch);\n\tunsigned int ntx;\n\n\t \n\tfor (ntx = 0; ntx < dev->num_tx_queues; ntx++) {\n\t\tstruct netdev_queue *dev_queue = netdev_get_tx_queue(dev, ntx);\n\t\tstruct Qdisc *old = dev_graft_qdisc(dev_queue, sch);\n\n\t\tqdisc_refcount_inc(sch);\n\n\t\tqdisc_put(old);\n\t}\n}\n\nstatic void htb_attach(struct Qdisc *sch)\n{\n\tstruct htb_sched *q = qdisc_priv(sch);\n\n\tif (q->offload)\n\t\thtb_attach_offload(sch);\n\telse\n\t\thtb_attach_software(sch);\n}\n\nstatic int htb_dump(struct Qdisc *sch, struct sk_buff *skb)\n{\n\tstruct htb_sched *q = qdisc_priv(sch);\n\tstruct nlattr *nest;\n\tstruct tc_htb_glob gopt;\n\n\tif (q->offload)\n\t\tsch->flags |= TCQ_F_OFFLOADED;\n\telse\n\t\tsch->flags &= ~TCQ_F_OFFLOADED;\n\n\tsch->qstats.overlimits = q->overlimits;\n\t \n\n\tgopt.direct_pkts = q->direct_pkts;\n\tgopt.version = HTB_VER;\n\tgopt.rate2quantum = q->rate2quantum;\n\tgopt.defcls = q->defcls;\n\tgopt.debug = 0;\n\n\tnest = nla_nest_start_noflag(skb, TCA_OPTIONS);\n\tif (nest == NULL)\n\t\tgoto nla_put_failure;\n\tif (nla_put(skb, TCA_HTB_INIT, sizeof(gopt), &gopt) ||\n\t    nla_put_u32(skb, TCA_HTB_DIRECT_QLEN, q->direct_qlen))\n\t\tgoto nla_put_failure;\n\tif (q->offload && nla_put_flag(skb, TCA_HTB_OFFLOAD))\n\t\tgoto nla_put_failure;\n\n\treturn nla_nest_end(skb, nest);\n\nnla_put_failure:\n\tnla_nest_cancel(skb, nest);\n\treturn -1;\n}\n\nstatic int htb_dump_class(struct Qdisc *sch, unsigned long arg,\n\t\t\t  struct sk_buff *skb, struct tcmsg *tcm)\n{\n\tstruct htb_class *cl = (struct htb_class *)arg;\n\tstruct htb_sched *q = qdisc_priv(sch);\n\tstruct nlattr *nest;\n\tstruct tc_htb_opt opt;\n\n\t \n\ttcm->tcm_parent = cl->parent ? cl->parent->common.classid : TC_H_ROOT;\n\ttcm->tcm_handle = cl->common.classid;\n\tif (!cl->level && cl->leaf.q)\n\t\ttcm->tcm_info = cl->leaf.q->handle;\n\n\tnest = nla_nest_start_noflag(skb, TCA_OPTIONS);\n\tif (nest == NULL)\n\t\tgoto nla_put_failure;\n\n\tmemset(&opt, 0, sizeof(opt));\n\n\tpsched_ratecfg_getrate(&opt.rate, &cl->rate);\n\topt.buffer = PSCHED_NS2TICKS(cl->buffer);\n\tpsched_ratecfg_getrate(&opt.ceil, &cl->ceil);\n\topt.cbuffer = PSCHED_NS2TICKS(cl->cbuffer);\n\topt.quantum = cl->quantum;\n\topt.prio = cl->prio;\n\topt.level = cl->level;\n\tif (nla_put(skb, TCA_HTB_PARMS, sizeof(opt), &opt))\n\t\tgoto nla_put_failure;\n\tif (q->offload && nla_put_flag(skb, TCA_HTB_OFFLOAD))\n\t\tgoto nla_put_failure;\n\tif ((cl->rate.rate_bytes_ps >= (1ULL << 32)) &&\n\t    nla_put_u64_64bit(skb, TCA_HTB_RATE64, cl->rate.rate_bytes_ps,\n\t\t\t      TCA_HTB_PAD))\n\t\tgoto nla_put_failure;\n\tif ((cl->ceil.rate_bytes_ps >= (1ULL << 32)) &&\n\t    nla_put_u64_64bit(skb, TCA_HTB_CEIL64, cl->ceil.rate_bytes_ps,\n\t\t\t      TCA_HTB_PAD))\n\t\tgoto nla_put_failure;\n\n\treturn nla_nest_end(skb, nest);\n\nnla_put_failure:\n\tnla_nest_cancel(skb, nest);\n\treturn -1;\n}\n\nstatic void htb_offload_aggregate_stats(struct htb_sched *q,\n\t\t\t\t\tstruct htb_class *cl)\n{\n\tu64 bytes = 0, packets = 0;\n\tstruct htb_class *c;\n\tunsigned int i;\n\n\tgnet_stats_basic_sync_init(&cl->bstats);\n\n\tfor (i = 0; i < q->clhash.hashsize; i++) {\n\t\thlist_for_each_entry(c, &q->clhash.hash[i], common.hnode) {\n\t\t\tstruct htb_class *p = c;\n\n\t\t\twhile (p && p->level < cl->level)\n\t\t\t\tp = p->parent;\n\n\t\t\tif (p != cl)\n\t\t\t\tcontinue;\n\n\t\t\tbytes += u64_stats_read(&c->bstats_bias.bytes);\n\t\t\tpackets += u64_stats_read(&c->bstats_bias.packets);\n\t\t\tif (c->level == 0) {\n\t\t\t\tbytes += u64_stats_read(&c->leaf.q->bstats.bytes);\n\t\t\t\tpackets += u64_stats_read(&c->leaf.q->bstats.packets);\n\t\t\t}\n\t\t}\n\t}\n\t_bstats_update(&cl->bstats, bytes, packets);\n}\n\nstatic int\nhtb_dump_class_stats(struct Qdisc *sch, unsigned long arg, struct gnet_dump *d)\n{\n\tstruct htb_class *cl = (struct htb_class *)arg;\n\tstruct htb_sched *q = qdisc_priv(sch);\n\tstruct gnet_stats_queue qs = {\n\t\t.drops = cl->drops,\n\t\t.overlimits = cl->overlimits,\n\t};\n\t__u32 qlen = 0;\n\n\tif (!cl->level && cl->leaf.q)\n\t\tqdisc_qstats_qlen_backlog(cl->leaf.q, &qlen, &qs.backlog);\n\n\tcl->xstats.tokens = clamp_t(s64, PSCHED_NS2TICKS(cl->tokens),\n\t\t\t\t    INT_MIN, INT_MAX);\n\tcl->xstats.ctokens = clamp_t(s64, PSCHED_NS2TICKS(cl->ctokens),\n\t\t\t\t     INT_MIN, INT_MAX);\n\n\tif (q->offload) {\n\t\tif (!cl->level) {\n\t\t\tif (cl->leaf.q)\n\t\t\t\tcl->bstats = cl->leaf.q->bstats;\n\t\t\telse\n\t\t\t\tgnet_stats_basic_sync_init(&cl->bstats);\n\t\t\t_bstats_update(&cl->bstats,\n\t\t\t\t       u64_stats_read(&cl->bstats_bias.bytes),\n\t\t\t\t       u64_stats_read(&cl->bstats_bias.packets));\n\t\t} else {\n\t\t\thtb_offload_aggregate_stats(q, cl);\n\t\t}\n\t}\n\n\tif (gnet_stats_copy_basic(d, NULL, &cl->bstats, true) < 0 ||\n\t    gnet_stats_copy_rate_est(d, &cl->rate_est) < 0 ||\n\t    gnet_stats_copy_queue(d, NULL, &qs, qlen) < 0)\n\t\treturn -1;\n\n\treturn gnet_stats_copy_app(d, &cl->xstats, sizeof(cl->xstats));\n}\n\nstatic struct netdev_queue *\nhtb_select_queue(struct Qdisc *sch, struct tcmsg *tcm)\n{\n\tstruct net_device *dev = qdisc_dev(sch);\n\tstruct tc_htb_qopt_offload offload_opt;\n\tstruct htb_sched *q = qdisc_priv(sch);\n\tint err;\n\n\tif (!q->offload)\n\t\treturn sch->dev_queue;\n\n\toffload_opt = (struct tc_htb_qopt_offload) {\n\t\t.command = TC_HTB_LEAF_QUERY_QUEUE,\n\t\t.classid = TC_H_MIN(tcm->tcm_parent),\n\t};\n\terr = htb_offload(dev, &offload_opt);\n\tif (err || offload_opt.qid >= dev->num_tx_queues)\n\t\treturn NULL;\n\treturn netdev_get_tx_queue(dev, offload_opt.qid);\n}\n\nstatic struct Qdisc *\nhtb_graft_helper(struct netdev_queue *dev_queue, struct Qdisc *new_q)\n{\n\tstruct net_device *dev = dev_queue->dev;\n\tstruct Qdisc *old_q;\n\n\tif (dev->flags & IFF_UP)\n\t\tdev_deactivate(dev);\n\told_q = dev_graft_qdisc(dev_queue, new_q);\n\tif (new_q)\n\t\tnew_q->flags |= TCQ_F_ONETXQUEUE | TCQ_F_NOPARENT;\n\tif (dev->flags & IFF_UP)\n\t\tdev_activate(dev);\n\n\treturn old_q;\n}\n\nstatic struct netdev_queue *htb_offload_get_queue(struct htb_class *cl)\n{\n\tstruct netdev_queue *queue;\n\n\tqueue = cl->leaf.offload_queue;\n\tif (!(cl->leaf.q->flags & TCQ_F_BUILTIN))\n\t\tWARN_ON(cl->leaf.q->dev_queue != queue);\n\n\treturn queue;\n}\n\nstatic void htb_offload_move_qdisc(struct Qdisc *sch, struct htb_class *cl_old,\n\t\t\t\t   struct htb_class *cl_new, bool destroying)\n{\n\tstruct netdev_queue *queue_old, *queue_new;\n\tstruct net_device *dev = qdisc_dev(sch);\n\n\tqueue_old = htb_offload_get_queue(cl_old);\n\tqueue_new = htb_offload_get_queue(cl_new);\n\n\tif (!destroying) {\n\t\tstruct Qdisc *qdisc;\n\n\t\tif (dev->flags & IFF_UP)\n\t\t\tdev_deactivate(dev);\n\t\tqdisc = dev_graft_qdisc(queue_old, NULL);\n\t\tWARN_ON(qdisc != cl_old->leaf.q);\n\t}\n\n\tif (!(cl_old->leaf.q->flags & TCQ_F_BUILTIN))\n\t\tcl_old->leaf.q->dev_queue = queue_new;\n\tcl_old->leaf.offload_queue = queue_new;\n\n\tif (!destroying) {\n\t\tstruct Qdisc *qdisc;\n\n\t\tqdisc = dev_graft_qdisc(queue_new, cl_old->leaf.q);\n\t\tif (dev->flags & IFF_UP)\n\t\t\tdev_activate(dev);\n\t\tWARN_ON(!(qdisc->flags & TCQ_F_BUILTIN));\n\t}\n}\n\nstatic int htb_graft(struct Qdisc *sch, unsigned long arg, struct Qdisc *new,\n\t\t     struct Qdisc **old, struct netlink_ext_ack *extack)\n{\n\tstruct netdev_queue *dev_queue = sch->dev_queue;\n\tstruct htb_class *cl = (struct htb_class *)arg;\n\tstruct htb_sched *q = qdisc_priv(sch);\n\tstruct Qdisc *old_q;\n\n\tif (cl->level)\n\t\treturn -EINVAL;\n\n\tif (q->offload)\n\t\tdev_queue = htb_offload_get_queue(cl);\n\n\tif (!new) {\n\t\tnew = qdisc_create_dflt(dev_queue, &pfifo_qdisc_ops,\n\t\t\t\t\tcl->common.classid, extack);\n\t\tif (!new)\n\t\t\treturn -ENOBUFS;\n\t}\n\n\tif (q->offload) {\n\t\thtb_set_lockdep_class_child(new);\n\t\t \n\t\tqdisc_refcount_inc(new);\n\t\told_q = htb_graft_helper(dev_queue, new);\n\t}\n\n\t*old = qdisc_replace(sch, new, &cl->leaf.q);\n\n\tif (q->offload) {\n\t\tWARN_ON(old_q != *old);\n\t\tqdisc_put(old_q);\n\t}\n\n\treturn 0;\n}\n\nstatic struct Qdisc *htb_leaf(struct Qdisc *sch, unsigned long arg)\n{\n\tstruct htb_class *cl = (struct htb_class *)arg;\n\treturn !cl->level ? cl->leaf.q : NULL;\n}\n\nstatic void htb_qlen_notify(struct Qdisc *sch, unsigned long arg)\n{\n\tstruct htb_class *cl = (struct htb_class *)arg;\n\n\thtb_deactivate(qdisc_priv(sch), cl);\n}\n\nstatic inline int htb_parent_last_child(struct htb_class *cl)\n{\n\tif (!cl->parent)\n\t\t \n\t\treturn 0;\n\tif (cl->parent->children > 1)\n\t\t \n\t\treturn 0;\n\treturn 1;\n}\n\nstatic void htb_parent_to_leaf(struct Qdisc *sch, struct htb_class *cl,\n\t\t\t       struct Qdisc *new_q)\n{\n\tstruct htb_sched *q = qdisc_priv(sch);\n\tstruct htb_class *parent = cl->parent;\n\n\tWARN_ON(cl->level || !cl->leaf.q || cl->prio_activity);\n\n\tif (parent->cmode != HTB_CAN_SEND)\n\t\thtb_safe_rb_erase(&parent->pq_node,\n\t\t\t\t  &q->hlevel[parent->level].wait_pq);\n\n\tparent->level = 0;\n\tmemset(&parent->inner, 0, sizeof(parent->inner));\n\tparent->leaf.q = new_q ? new_q : &noop_qdisc;\n\tparent->tokens = parent->buffer;\n\tparent->ctokens = parent->cbuffer;\n\tparent->t_c = ktime_get_ns();\n\tparent->cmode = HTB_CAN_SEND;\n\tif (q->offload)\n\t\tparent->leaf.offload_queue = cl->leaf.offload_queue;\n}\n\nstatic void htb_parent_to_leaf_offload(struct Qdisc *sch,\n\t\t\t\t       struct netdev_queue *dev_queue,\n\t\t\t\t       struct Qdisc *new_q)\n{\n\tstruct Qdisc *old_q;\n\n\t \n\tif (new_q)\n\t\tqdisc_refcount_inc(new_q);\n\told_q = htb_graft_helper(dev_queue, new_q);\n\tWARN_ON(!(old_q->flags & TCQ_F_BUILTIN));\n}\n\nstatic int htb_destroy_class_offload(struct Qdisc *sch, struct htb_class *cl,\n\t\t\t\t     bool last_child, bool destroying,\n\t\t\t\t     struct netlink_ext_ack *extack)\n{\n\tstruct tc_htb_qopt_offload offload_opt;\n\tstruct netdev_queue *dev_queue;\n\tstruct Qdisc *q = cl->leaf.q;\n\tstruct Qdisc *old;\n\tint err;\n\n\tif (cl->level)\n\t\treturn -EINVAL;\n\n\tWARN_ON(!q);\n\tdev_queue = htb_offload_get_queue(cl);\n\t \n\tif (!destroying) {\n\t\told = htb_graft_helper(dev_queue, NULL);\n\t\t \n\t\tWARN_ON(old != q);\n\t}\n\n\tif (cl->parent) {\n\t\t_bstats_update(&cl->parent->bstats_bias,\n\t\t\t       u64_stats_read(&q->bstats.bytes),\n\t\t\t       u64_stats_read(&q->bstats.packets));\n\t}\n\n\toffload_opt = (struct tc_htb_qopt_offload) {\n\t\t.command = !last_child ? TC_HTB_LEAF_DEL :\n\t\t\t   destroying ? TC_HTB_LEAF_DEL_LAST_FORCE :\n\t\t\t   TC_HTB_LEAF_DEL_LAST,\n\t\t.classid = cl->common.classid,\n\t\t.extack = extack,\n\t};\n\terr = htb_offload(qdisc_dev(sch), &offload_opt);\n\n\tif (!destroying) {\n\t\tif (!err)\n\t\t\tqdisc_put(old);\n\t\telse\n\t\t\thtb_graft_helper(dev_queue, old);\n\t}\n\n\tif (last_child)\n\t\treturn err;\n\n\tif (!err && offload_opt.classid != TC_H_MIN(cl->common.classid)) {\n\t\tu32 classid = TC_H_MAJ(sch->handle) |\n\t\t\t      TC_H_MIN(offload_opt.classid);\n\t\tstruct htb_class *moved_cl = htb_find(classid, sch);\n\n\t\thtb_offload_move_qdisc(sch, moved_cl, cl, destroying);\n\t}\n\n\treturn err;\n}\n\nstatic void htb_destroy_class(struct Qdisc *sch, struct htb_class *cl)\n{\n\tif (!cl->level) {\n\t\tWARN_ON(!cl->leaf.q);\n\t\tqdisc_put(cl->leaf.q);\n\t}\n\tgen_kill_estimator(&cl->rate_est);\n\ttcf_block_put(cl->block);\n\tkfree(cl);\n}\n\nstatic void htb_destroy(struct Qdisc *sch)\n{\n\tstruct net_device *dev = qdisc_dev(sch);\n\tstruct tc_htb_qopt_offload offload_opt;\n\tstruct htb_sched *q = qdisc_priv(sch);\n\tstruct hlist_node *next;\n\tbool nonempty, changed;\n\tstruct htb_class *cl;\n\tunsigned int i;\n\n\tcancel_work_sync(&q->work);\n\tqdisc_watchdog_cancel(&q->watchdog);\n\t \n\ttcf_block_put(q->block);\n\n\tfor (i = 0; i < q->clhash.hashsize; i++) {\n\t\thlist_for_each_entry(cl, &q->clhash.hash[i], common.hnode) {\n\t\t\ttcf_block_put(cl->block);\n\t\t\tcl->block = NULL;\n\t\t}\n\t}\n\n\tdo {\n\t\tnonempty = false;\n\t\tchanged = false;\n\t\tfor (i = 0; i < q->clhash.hashsize; i++) {\n\t\t\thlist_for_each_entry_safe(cl, next, &q->clhash.hash[i],\n\t\t\t\t\t\t  common.hnode) {\n\t\t\t\tbool last_child;\n\n\t\t\t\tif (!q->offload) {\n\t\t\t\t\thtb_destroy_class(sch, cl);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\tnonempty = true;\n\n\t\t\t\tif (cl->level)\n\t\t\t\t\tcontinue;\n\n\t\t\t\tchanged = true;\n\n\t\t\t\tlast_child = htb_parent_last_child(cl);\n\t\t\t\thtb_destroy_class_offload(sch, cl, last_child,\n\t\t\t\t\t\t\t  true, NULL);\n\t\t\t\tqdisc_class_hash_remove(&q->clhash,\n\t\t\t\t\t\t\t&cl->common);\n\t\t\t\tif (cl->parent)\n\t\t\t\t\tcl->parent->children--;\n\t\t\t\tif (last_child)\n\t\t\t\t\thtb_parent_to_leaf(sch, cl, NULL);\n\t\t\t\thtb_destroy_class(sch, cl);\n\t\t\t}\n\t\t}\n\t} while (changed);\n\tWARN_ON(nonempty);\n\n\tqdisc_class_hash_destroy(&q->clhash);\n\t__qdisc_reset_queue(&q->direct_queue);\n\n\tif (q->offload) {\n\t\toffload_opt = (struct tc_htb_qopt_offload) {\n\t\t\t.command = TC_HTB_DESTROY,\n\t\t};\n\t\thtb_offload(dev, &offload_opt);\n\t}\n\n\tif (!q->direct_qdiscs)\n\t\treturn;\n\tfor (i = 0; i < q->num_direct_qdiscs && q->direct_qdiscs[i]; i++)\n\t\tqdisc_put(q->direct_qdiscs[i]);\n\tkfree(q->direct_qdiscs);\n}\n\nstatic int htb_delete(struct Qdisc *sch, unsigned long arg,\n\t\t      struct netlink_ext_ack *extack)\n{\n\tstruct htb_sched *q = qdisc_priv(sch);\n\tstruct htb_class *cl = (struct htb_class *)arg;\n\tstruct Qdisc *new_q = NULL;\n\tint last_child = 0;\n\tint err;\n\n\t \n\tif (cl->children || qdisc_class_in_use(&cl->common)) {\n\t\tNL_SET_ERR_MSG(extack, \"HTB class in use\");\n\t\treturn -EBUSY;\n\t}\n\n\tif (!cl->level && htb_parent_last_child(cl))\n\t\tlast_child = 1;\n\n\tif (q->offload) {\n\t\terr = htb_destroy_class_offload(sch, cl, last_child, false,\n\t\t\t\t\t\textack);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (last_child) {\n\t\tstruct netdev_queue *dev_queue = sch->dev_queue;\n\n\t\tif (q->offload)\n\t\t\tdev_queue = htb_offload_get_queue(cl);\n\n\t\tnew_q = qdisc_create_dflt(dev_queue, &pfifo_qdisc_ops,\n\t\t\t\t\t  cl->parent->common.classid,\n\t\t\t\t\t  NULL);\n\t\tif (q->offload) {\n\t\t\tif (new_q)\n\t\t\t\thtb_set_lockdep_class_child(new_q);\n\t\t\thtb_parent_to_leaf_offload(sch, dev_queue, new_q);\n\t\t}\n\t}\n\n\tsch_tree_lock(sch);\n\n\tif (!cl->level)\n\t\tqdisc_purge_queue(cl->leaf.q);\n\n\t \n\tqdisc_class_hash_remove(&q->clhash, &cl->common);\n\tif (cl->parent)\n\t\tcl->parent->children--;\n\n\tif (cl->prio_activity)\n\t\thtb_deactivate(q, cl);\n\n\tif (cl->cmode != HTB_CAN_SEND)\n\t\thtb_safe_rb_erase(&cl->pq_node,\n\t\t\t\t  &q->hlevel[cl->level].wait_pq);\n\n\tif (last_child)\n\t\thtb_parent_to_leaf(sch, cl, new_q);\n\n\tsch_tree_unlock(sch);\n\n\thtb_destroy_class(sch, cl);\n\treturn 0;\n}\n\nstatic int htb_change_class(struct Qdisc *sch, u32 classid,\n\t\t\t    u32 parentid, struct nlattr **tca,\n\t\t\t    unsigned long *arg, struct netlink_ext_ack *extack)\n{\n\tint err = -EINVAL;\n\tstruct htb_sched *q = qdisc_priv(sch);\n\tstruct htb_class *cl = (struct htb_class *)*arg, *parent;\n\tstruct tc_htb_qopt_offload offload_opt;\n\tstruct nlattr *opt = tca[TCA_OPTIONS];\n\tstruct nlattr *tb[TCA_HTB_MAX + 1];\n\tstruct Qdisc *parent_qdisc = NULL;\n\tstruct netdev_queue *dev_queue;\n\tstruct tc_htb_opt *hopt;\n\tu64 rate64, ceil64;\n\tint warn = 0;\n\n\t \n\tif (!opt)\n\t\tgoto failure;\n\n\terr = nla_parse_nested_deprecated(tb, TCA_HTB_MAX, opt, htb_policy,\n\t\t\t\t\t  extack);\n\tif (err < 0)\n\t\tgoto failure;\n\n\terr = -EINVAL;\n\tif (tb[TCA_HTB_PARMS] == NULL)\n\t\tgoto failure;\n\n\tparent = parentid == TC_H_ROOT ? NULL : htb_find(parentid, sch);\n\n\thopt = nla_data(tb[TCA_HTB_PARMS]);\n\tif (!hopt->rate.rate || !hopt->ceil.rate)\n\t\tgoto failure;\n\n\tif (q->offload) {\n\t\t \n\t\tif (hopt->rate.overhead || hopt->ceil.overhead) {\n\t\t\tNL_SET_ERR_MSG(extack, \"HTB offload doesn't support the overhead parameter\");\n\t\t\tgoto failure;\n\t\t}\n\t\tif (hopt->rate.mpu || hopt->ceil.mpu) {\n\t\t\tNL_SET_ERR_MSG(extack, \"HTB offload doesn't support the mpu parameter\");\n\t\t\tgoto failure;\n\t\t}\n\t}\n\n\t \n\tif (hopt->rate.linklayer == TC_LINKLAYER_UNAWARE)\n\t\tqdisc_put_rtab(qdisc_get_rtab(&hopt->rate, tb[TCA_HTB_RTAB],\n\t\t\t\t\t      NULL));\n\n\tif (hopt->ceil.linklayer == TC_LINKLAYER_UNAWARE)\n\t\tqdisc_put_rtab(qdisc_get_rtab(&hopt->ceil, tb[TCA_HTB_CTAB],\n\t\t\t\t\t      NULL));\n\n\trate64 = tb[TCA_HTB_RATE64] ? nla_get_u64(tb[TCA_HTB_RATE64]) : 0;\n\tceil64 = tb[TCA_HTB_CEIL64] ? nla_get_u64(tb[TCA_HTB_CEIL64]) : 0;\n\n\tif (!cl) {\t\t \n\t\tstruct net_device *dev = qdisc_dev(sch);\n\t\tstruct Qdisc *new_q, *old_q;\n\t\tint prio;\n\t\tstruct {\n\t\t\tstruct nlattr\t\tnla;\n\t\t\tstruct gnet_estimator\topt;\n\t\t} est = {\n\t\t\t.nla = {\n\t\t\t\t.nla_len\t= nla_attr_size(sizeof(est.opt)),\n\t\t\t\t.nla_type\t= TCA_RATE,\n\t\t\t},\n\t\t\t.opt = {\n\t\t\t\t \n\t\t\t\t.interval\t= 2,\n\t\t\t\t.ewma_log\t= 2,\n\t\t\t},\n\t\t};\n\n\t\t \n\t\tif (!classid || TC_H_MAJ(classid ^ sch->handle) ||\n\t\t    htb_find(classid, sch))\n\t\t\tgoto failure;\n\n\t\t \n\t\tif (parent && parent->parent && parent->parent->level < 2) {\n\t\t\tNL_SET_ERR_MSG_MOD(extack, \"tree is too deep\");\n\t\t\tgoto failure;\n\t\t}\n\t\terr = -ENOBUFS;\n\t\tcl = kzalloc(sizeof(*cl), GFP_KERNEL);\n\t\tif (!cl)\n\t\t\tgoto failure;\n\n\t\tgnet_stats_basic_sync_init(&cl->bstats);\n\t\tgnet_stats_basic_sync_init(&cl->bstats_bias);\n\n\t\terr = tcf_block_get(&cl->block, &cl->filter_list, sch, extack);\n\t\tif (err) {\n\t\t\tkfree(cl);\n\t\t\tgoto failure;\n\t\t}\n\t\tif (htb_rate_est || tca[TCA_RATE]) {\n\t\t\terr = gen_new_estimator(&cl->bstats, NULL,\n\t\t\t\t\t\t&cl->rate_est,\n\t\t\t\t\t\tNULL,\n\t\t\t\t\t\ttrue,\n\t\t\t\t\t\ttca[TCA_RATE] ? : &est.nla);\n\t\t\tif (err)\n\t\t\t\tgoto err_block_put;\n\t\t}\n\n\t\tcl->children = 0;\n\t\tRB_CLEAR_NODE(&cl->pq_node);\n\n\t\tfor (prio = 0; prio < TC_HTB_NUMPRIO; prio++)\n\t\t\tRB_CLEAR_NODE(&cl->node[prio]);\n\n\t\tcl->common.classid = classid;\n\n\t\t \n\t\tASSERT_RTNL();\n\n\t\t \n\t\tif (!q->offload) {\n\t\t\tdev_queue = sch->dev_queue;\n\t\t} else if (!(parent && !parent->level)) {\n\t\t\t \n\t\t\toffload_opt = (struct tc_htb_qopt_offload) {\n\t\t\t\t.command = TC_HTB_LEAF_ALLOC_QUEUE,\n\t\t\t\t.classid = cl->common.classid,\n\t\t\t\t.parent_classid = parent ?\n\t\t\t\t\tTC_H_MIN(parent->common.classid) :\n\t\t\t\t\tTC_HTB_CLASSID_ROOT,\n\t\t\t\t.rate = max_t(u64, hopt->rate.rate, rate64),\n\t\t\t\t.ceil = max_t(u64, hopt->ceil.rate, ceil64),\n\t\t\t\t.prio = hopt->prio,\n\t\t\t\t.quantum = hopt->quantum,\n\t\t\t\t.extack = extack,\n\t\t\t};\n\t\t\terr = htb_offload(dev, &offload_opt);\n\t\t\tif (err) {\n\t\t\t\tNL_SET_ERR_MSG_WEAK(extack,\n\t\t\t\t\t\t    \"Failed to offload TC_HTB_LEAF_ALLOC_QUEUE\");\n\t\t\t\tgoto err_kill_estimator;\n\t\t\t}\n\t\t\tdev_queue = netdev_get_tx_queue(dev, offload_opt.qid);\n\t\t} else {  \n\t\t\tdev_queue = htb_offload_get_queue(parent);\n\t\t\told_q = htb_graft_helper(dev_queue, NULL);\n\t\t\tWARN_ON(old_q != parent->leaf.q);\n\t\t\toffload_opt = (struct tc_htb_qopt_offload) {\n\t\t\t\t.command = TC_HTB_LEAF_TO_INNER,\n\t\t\t\t.classid = cl->common.classid,\n\t\t\t\t.parent_classid =\n\t\t\t\t\tTC_H_MIN(parent->common.classid),\n\t\t\t\t.rate = max_t(u64, hopt->rate.rate, rate64),\n\t\t\t\t.ceil = max_t(u64, hopt->ceil.rate, ceil64),\n\t\t\t\t.prio = hopt->prio,\n\t\t\t\t.quantum = hopt->quantum,\n\t\t\t\t.extack = extack,\n\t\t\t};\n\t\t\terr = htb_offload(dev, &offload_opt);\n\t\t\tif (err) {\n\t\t\t\tNL_SET_ERR_MSG_WEAK(extack,\n\t\t\t\t\t\t    \"Failed to offload TC_HTB_LEAF_TO_INNER\");\n\t\t\t\thtb_graft_helper(dev_queue, old_q);\n\t\t\t\tgoto err_kill_estimator;\n\t\t\t}\n\t\t\t_bstats_update(&parent->bstats_bias,\n\t\t\t\t       u64_stats_read(&old_q->bstats.bytes),\n\t\t\t\t       u64_stats_read(&old_q->bstats.packets));\n\t\t\tqdisc_put(old_q);\n\t\t}\n\t\tnew_q = qdisc_create_dflt(dev_queue, &pfifo_qdisc_ops,\n\t\t\t\t\t  classid, NULL);\n\t\tif (q->offload) {\n\t\t\tif (new_q) {\n\t\t\t\thtb_set_lockdep_class_child(new_q);\n\t\t\t\t \n\t\t\t\tqdisc_refcount_inc(new_q);\n\t\t\t}\n\t\t\told_q = htb_graft_helper(dev_queue, new_q);\n\t\t\t \n\t\t\tWARN_ON(!(old_q->flags & TCQ_F_BUILTIN));\n\t\t}\n\t\tsch_tree_lock(sch);\n\t\tif (parent && !parent->level) {\n\t\t\t \n\t\t\tqdisc_purge_queue(parent->leaf.q);\n\t\t\tparent_qdisc = parent->leaf.q;\n\t\t\tif (parent->prio_activity)\n\t\t\t\thtb_deactivate(q, parent);\n\n\t\t\t \n\t\t\tif (parent->cmode != HTB_CAN_SEND) {\n\t\t\t\thtb_safe_rb_erase(&parent->pq_node, &q->hlevel[0].wait_pq);\n\t\t\t\tparent->cmode = HTB_CAN_SEND;\n\t\t\t}\n\t\t\tparent->level = (parent->parent ? parent->parent->level\n\t\t\t\t\t : TC_HTB_MAXDEPTH) - 1;\n\t\t\tmemset(&parent->inner, 0, sizeof(parent->inner));\n\t\t}\n\n\t\t \n\t\tcl->leaf.q = new_q ? new_q : &noop_qdisc;\n\t\tif (q->offload)\n\t\t\tcl->leaf.offload_queue = dev_queue;\n\n\t\tcl->parent = parent;\n\n\t\t \n\t\tcl->tokens = PSCHED_TICKS2NS(hopt->buffer);\n\t\tcl->ctokens = PSCHED_TICKS2NS(hopt->cbuffer);\n\t\tcl->mbuffer = 60ULL * NSEC_PER_SEC;\t \n\t\tcl->t_c = ktime_get_ns();\n\t\tcl->cmode = HTB_CAN_SEND;\n\n\t\t \n\t\tqdisc_class_hash_insert(&q->clhash, &cl->common);\n\t\tif (parent)\n\t\t\tparent->children++;\n\t\tif (cl->leaf.q != &noop_qdisc)\n\t\t\tqdisc_hash_add(cl->leaf.q, true);\n\t} else {\n\t\tif (tca[TCA_RATE]) {\n\t\t\terr = gen_replace_estimator(&cl->bstats, NULL,\n\t\t\t\t\t\t    &cl->rate_est,\n\t\t\t\t\t\t    NULL,\n\t\t\t\t\t\t    true,\n\t\t\t\t\t\t    tca[TCA_RATE]);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\tif (q->offload) {\n\t\t\tstruct net_device *dev = qdisc_dev(sch);\n\n\t\t\toffload_opt = (struct tc_htb_qopt_offload) {\n\t\t\t\t.command = TC_HTB_NODE_MODIFY,\n\t\t\t\t.classid = cl->common.classid,\n\t\t\t\t.rate = max_t(u64, hopt->rate.rate, rate64),\n\t\t\t\t.ceil = max_t(u64, hopt->ceil.rate, ceil64),\n\t\t\t\t.prio = hopt->prio,\n\t\t\t\t.quantum = hopt->quantum,\n\t\t\t\t.extack = extack,\n\t\t\t};\n\t\t\terr = htb_offload(dev, &offload_opt);\n\t\t\tif (err)\n\t\t\t\t \n\t\t\t\treturn err;\n\t\t}\n\n\t\tsch_tree_lock(sch);\n\t}\n\n\tpsched_ratecfg_precompute(&cl->rate, &hopt->rate, rate64);\n\tpsched_ratecfg_precompute(&cl->ceil, &hopt->ceil, ceil64);\n\n\t \n\tif (!cl->level) {\n\t\tu64 quantum = cl->rate.rate_bytes_ps;\n\n\t\tdo_div(quantum, q->rate2quantum);\n\t\tcl->quantum = min_t(u64, quantum, INT_MAX);\n\n\t\tif (!hopt->quantum && cl->quantum < 1000) {\n\t\t\twarn = -1;\n\t\t\tcl->quantum = 1000;\n\t\t}\n\t\tif (!hopt->quantum && cl->quantum > 200000) {\n\t\t\twarn = 1;\n\t\t\tcl->quantum = 200000;\n\t\t}\n\t\tif (hopt->quantum)\n\t\t\tcl->quantum = hopt->quantum;\n\t\tif ((cl->prio = hopt->prio) >= TC_HTB_NUMPRIO)\n\t\t\tcl->prio = TC_HTB_NUMPRIO - 1;\n\t}\n\n\tcl->buffer = PSCHED_TICKS2NS(hopt->buffer);\n\tcl->cbuffer = PSCHED_TICKS2NS(hopt->cbuffer);\n\n\tsch_tree_unlock(sch);\n\tqdisc_put(parent_qdisc);\n\n\tif (warn)\n\t\tNL_SET_ERR_MSG_FMT_MOD(extack,\n\t\t\t\t       \"quantum of class %X is %s. Consider r2q change.\",\n\t\t\t\t       cl->common.classid, (warn == -1 ? \"small\" : \"big\"));\n\n\tqdisc_class_hash_grow(sch, &q->clhash);\n\n\t*arg = (unsigned long)cl;\n\treturn 0;\n\nerr_kill_estimator:\n\tgen_kill_estimator(&cl->rate_est);\nerr_block_put:\n\ttcf_block_put(cl->block);\n\tkfree(cl);\nfailure:\n\treturn err;\n}\n\nstatic struct tcf_block *htb_tcf_block(struct Qdisc *sch, unsigned long arg,\n\t\t\t\t       struct netlink_ext_ack *extack)\n{\n\tstruct htb_sched *q = qdisc_priv(sch);\n\tstruct htb_class *cl = (struct htb_class *)arg;\n\n\treturn cl ? cl->block : q->block;\n}\n\nstatic unsigned long htb_bind_filter(struct Qdisc *sch, unsigned long parent,\n\t\t\t\t     u32 classid)\n{\n\tstruct htb_class *cl = htb_find(classid, sch);\n\n\t \n\tif (cl)\n\t\tqdisc_class_get(&cl->common);\n\treturn (unsigned long)cl;\n}\n\nstatic void htb_unbind_filter(struct Qdisc *sch, unsigned long arg)\n{\n\tstruct htb_class *cl = (struct htb_class *)arg;\n\n\tqdisc_class_put(&cl->common);\n}\n\nstatic void htb_walk(struct Qdisc *sch, struct qdisc_walker *arg)\n{\n\tstruct htb_sched *q = qdisc_priv(sch);\n\tstruct htb_class *cl;\n\tunsigned int i;\n\n\tif (arg->stop)\n\t\treturn;\n\n\tfor (i = 0; i < q->clhash.hashsize; i++) {\n\t\thlist_for_each_entry(cl, &q->clhash.hash[i], common.hnode) {\n\t\t\tif (!tc_qdisc_stats_dump(sch, (unsigned long)cl, arg))\n\t\t\t\treturn;\n\t\t}\n\t}\n}\n\nstatic const struct Qdisc_class_ops htb_class_ops = {\n\t.select_queue\t=\thtb_select_queue,\n\t.graft\t\t=\thtb_graft,\n\t.leaf\t\t=\thtb_leaf,\n\t.qlen_notify\t=\thtb_qlen_notify,\n\t.find\t\t=\thtb_search,\n\t.change\t\t=\thtb_change_class,\n\t.delete\t\t=\thtb_delete,\n\t.walk\t\t=\thtb_walk,\n\t.tcf_block\t=\thtb_tcf_block,\n\t.bind_tcf\t=\thtb_bind_filter,\n\t.unbind_tcf\t=\thtb_unbind_filter,\n\t.dump\t\t=\thtb_dump_class,\n\t.dump_stats\t=\thtb_dump_class_stats,\n};\n\nstatic struct Qdisc_ops htb_qdisc_ops __read_mostly = {\n\t.cl_ops\t\t=\t&htb_class_ops,\n\t.id\t\t=\t\"htb\",\n\t.priv_size\t=\tsizeof(struct htb_sched),\n\t.enqueue\t=\thtb_enqueue,\n\t.dequeue\t=\thtb_dequeue,\n\t.peek\t\t=\tqdisc_peek_dequeued,\n\t.init\t\t=\thtb_init,\n\t.attach\t\t=\thtb_attach,\n\t.reset\t\t=\thtb_reset,\n\t.destroy\t=\thtb_destroy,\n\t.dump\t\t=\thtb_dump,\n\t.owner\t\t=\tTHIS_MODULE,\n};\n\nstatic int __init htb_module_init(void)\n{\n\treturn register_qdisc(&htb_qdisc_ops);\n}\nstatic void __exit htb_module_exit(void)\n{\n\tunregister_qdisc(&htb_qdisc_ops);\n}\n\nmodule_init(htb_module_init)\nmodule_exit(htb_module_exit)\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}