{
  "module_name": "sch_fq_codel.c",
  "hash_id": "545cbf97d03aa73c63c5ac1645451d94b519af2d698da533bf0b35287e3a69bf",
  "original_prompt": "Ingested from linux-6.6.14/net/sched/sch_fq_codel.c",
  "human_readable_source": "\n \n\n#include <linux/module.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/jiffies.h>\n#include <linux/string.h>\n#include <linux/in.h>\n#include <linux/errno.h>\n#include <linux/init.h>\n#include <linux/skbuff.h>\n#include <linux/slab.h>\n#include <linux/vmalloc.h>\n#include <net/netlink.h>\n#include <net/pkt_sched.h>\n#include <net/pkt_cls.h>\n#include <net/codel.h>\n#include <net/codel_impl.h>\n#include <net/codel_qdisc.h>\n\n \n\nstruct fq_codel_flow {\n\tstruct sk_buff\t  *head;\n\tstruct sk_buff\t  *tail;\n\tstruct list_head  flowchain;\n\tint\t\t  deficit;\n\tstruct codel_vars cvars;\n};  \n\nstruct fq_codel_sched_data {\n\tstruct tcf_proto __rcu *filter_list;  \n\tstruct tcf_block *block;\n\tstruct fq_codel_flow *flows;\t \n\tu32\t\t*backlogs;\t \n\tu32\t\tflows_cnt;\t \n\tu32\t\tquantum;\t \n\tu32\t\tdrop_batch_size;\n\tu32\t\tmemory_limit;\n\tstruct codel_params cparams;\n\tstruct codel_stats cstats;\n\tu32\t\tmemory_usage;\n\tu32\t\tdrop_overmemory;\n\tu32\t\tdrop_overlimit;\n\tu32\t\tnew_flow_count;\n\n\tstruct list_head new_flows;\t \n\tstruct list_head old_flows;\t \n};\n\nstatic unsigned int fq_codel_hash(const struct fq_codel_sched_data *q,\n\t\t\t\t  struct sk_buff *skb)\n{\n\treturn reciprocal_scale(skb_get_hash(skb), q->flows_cnt);\n}\n\nstatic unsigned int fq_codel_classify(struct sk_buff *skb, struct Qdisc *sch,\n\t\t\t\t      int *qerr)\n{\n\tstruct fq_codel_sched_data *q = qdisc_priv(sch);\n\tstruct tcf_proto *filter;\n\tstruct tcf_result res;\n\tint result;\n\n\tif (TC_H_MAJ(skb->priority) == sch->handle &&\n\t    TC_H_MIN(skb->priority) > 0 &&\n\t    TC_H_MIN(skb->priority) <= q->flows_cnt)\n\t\treturn TC_H_MIN(skb->priority);\n\n\tfilter = rcu_dereference_bh(q->filter_list);\n\tif (!filter)\n\t\treturn fq_codel_hash(q, skb) + 1;\n\n\t*qerr = NET_XMIT_SUCCESS | __NET_XMIT_BYPASS;\n\tresult = tcf_classify(skb, NULL, filter, &res, false);\n\tif (result >= 0) {\n#ifdef CONFIG_NET_CLS_ACT\n\t\tswitch (result) {\n\t\tcase TC_ACT_STOLEN:\n\t\tcase TC_ACT_QUEUED:\n\t\tcase TC_ACT_TRAP:\n\t\t\t*qerr = NET_XMIT_SUCCESS | __NET_XMIT_STOLEN;\n\t\t\tfallthrough;\n\t\tcase TC_ACT_SHOT:\n\t\t\treturn 0;\n\t\t}\n#endif\n\t\tif (TC_H_MIN(res.classid) <= q->flows_cnt)\n\t\t\treturn TC_H_MIN(res.classid);\n\t}\n\treturn 0;\n}\n\n \n\n \nstatic inline struct sk_buff *dequeue_head(struct fq_codel_flow *flow)\n{\n\tstruct sk_buff *skb = flow->head;\n\n\tflow->head = skb->next;\n\tskb_mark_not_on_list(skb);\n\treturn skb;\n}\n\n \nstatic inline void flow_queue_add(struct fq_codel_flow *flow,\n\t\t\t\t  struct sk_buff *skb)\n{\n\tif (flow->head == NULL)\n\t\tflow->head = skb;\n\telse\n\t\tflow->tail->next = skb;\n\tflow->tail = skb;\n\tskb->next = NULL;\n}\n\nstatic unsigned int fq_codel_drop(struct Qdisc *sch, unsigned int max_packets,\n\t\t\t\t  struct sk_buff **to_free)\n{\n\tstruct fq_codel_sched_data *q = qdisc_priv(sch);\n\tstruct sk_buff *skb;\n\tunsigned int maxbacklog = 0, idx = 0, i, len;\n\tstruct fq_codel_flow *flow;\n\tunsigned int threshold;\n\tunsigned int mem = 0;\n\n\t \n\tfor (i = 0; i < q->flows_cnt; i++) {\n\t\tif (q->backlogs[i] > maxbacklog) {\n\t\t\tmaxbacklog = q->backlogs[i];\n\t\t\tidx = i;\n\t\t}\n\t}\n\n\t \n\tthreshold = maxbacklog >> 1;\n\n\tflow = &q->flows[idx];\n\tlen = 0;\n\ti = 0;\n\tdo {\n\t\tskb = dequeue_head(flow);\n\t\tlen += qdisc_pkt_len(skb);\n\t\tmem += get_codel_cb(skb)->mem_usage;\n\t\t__qdisc_drop(skb, to_free);\n\t} while (++i < max_packets && len < threshold);\n\n\t \n\tflow->cvars.count += i;\n\tq->backlogs[idx] -= len;\n\tq->memory_usage -= mem;\n\tsch->qstats.drops += i;\n\tsch->qstats.backlog -= len;\n\tsch->q.qlen -= i;\n\treturn idx;\n}\n\nstatic int fq_codel_enqueue(struct sk_buff *skb, struct Qdisc *sch,\n\t\t\t    struct sk_buff **to_free)\n{\n\tstruct fq_codel_sched_data *q = qdisc_priv(sch);\n\tunsigned int idx, prev_backlog, prev_qlen;\n\tstruct fq_codel_flow *flow;\n\tint ret;\n\tunsigned int pkt_len;\n\tbool memory_limited;\n\n\tidx = fq_codel_classify(skb, sch, &ret);\n\tif (idx == 0) {\n\t\tif (ret & __NET_XMIT_BYPASS)\n\t\t\tqdisc_qstats_drop(sch);\n\t\t__qdisc_drop(skb, to_free);\n\t\treturn ret;\n\t}\n\tidx--;\n\n\tcodel_set_enqueue_time(skb);\n\tflow = &q->flows[idx];\n\tflow_queue_add(flow, skb);\n\tq->backlogs[idx] += qdisc_pkt_len(skb);\n\tqdisc_qstats_backlog_inc(sch, skb);\n\n\tif (list_empty(&flow->flowchain)) {\n\t\tlist_add_tail(&flow->flowchain, &q->new_flows);\n\t\tq->new_flow_count++;\n\t\tflow->deficit = q->quantum;\n\t}\n\tget_codel_cb(skb)->mem_usage = skb->truesize;\n\tq->memory_usage += get_codel_cb(skb)->mem_usage;\n\tmemory_limited = q->memory_usage > q->memory_limit;\n\tif (++sch->q.qlen <= sch->limit && !memory_limited)\n\t\treturn NET_XMIT_SUCCESS;\n\n\tprev_backlog = sch->qstats.backlog;\n\tprev_qlen = sch->q.qlen;\n\n\t \n\tpkt_len = qdisc_pkt_len(skb);\n\t \n\tret = fq_codel_drop(sch, q->drop_batch_size, to_free);\n\n\tprev_qlen -= sch->q.qlen;\n\tprev_backlog -= sch->qstats.backlog;\n\tq->drop_overlimit += prev_qlen;\n\tif (memory_limited)\n\t\tq->drop_overmemory += prev_qlen;\n\n\t \n\tif (ret == idx) {\n\t\tqdisc_tree_reduce_backlog(sch, prev_qlen - 1,\n\t\t\t\t\t  prev_backlog - pkt_len);\n\t\treturn NET_XMIT_CN;\n\t}\n\tqdisc_tree_reduce_backlog(sch, prev_qlen, prev_backlog);\n\treturn NET_XMIT_SUCCESS;\n}\n\n \nstatic struct sk_buff *dequeue_func(struct codel_vars *vars, void *ctx)\n{\n\tstruct Qdisc *sch = ctx;\n\tstruct fq_codel_sched_data *q = qdisc_priv(sch);\n\tstruct fq_codel_flow *flow;\n\tstruct sk_buff *skb = NULL;\n\n\tflow = container_of(vars, struct fq_codel_flow, cvars);\n\tif (flow->head) {\n\t\tskb = dequeue_head(flow);\n\t\tq->backlogs[flow - q->flows] -= qdisc_pkt_len(skb);\n\t\tq->memory_usage -= get_codel_cb(skb)->mem_usage;\n\t\tsch->q.qlen--;\n\t\tsch->qstats.backlog -= qdisc_pkt_len(skb);\n\t}\n\treturn skb;\n}\n\nstatic void drop_func(struct sk_buff *skb, void *ctx)\n{\n\tstruct Qdisc *sch = ctx;\n\n\tkfree_skb(skb);\n\tqdisc_qstats_drop(sch);\n}\n\nstatic struct sk_buff *fq_codel_dequeue(struct Qdisc *sch)\n{\n\tstruct fq_codel_sched_data *q = qdisc_priv(sch);\n\tstruct sk_buff *skb;\n\tstruct fq_codel_flow *flow;\n\tstruct list_head *head;\n\nbegin:\n\thead = &q->new_flows;\n\tif (list_empty(head)) {\n\t\thead = &q->old_flows;\n\t\tif (list_empty(head))\n\t\t\treturn NULL;\n\t}\n\tflow = list_first_entry(head, struct fq_codel_flow, flowchain);\n\n\tif (flow->deficit <= 0) {\n\t\tflow->deficit += q->quantum;\n\t\tlist_move_tail(&flow->flowchain, &q->old_flows);\n\t\tgoto begin;\n\t}\n\n\tskb = codel_dequeue(sch, &sch->qstats.backlog, &q->cparams,\n\t\t\t    &flow->cvars, &q->cstats, qdisc_pkt_len,\n\t\t\t    codel_get_enqueue_time, drop_func, dequeue_func);\n\n\tif (!skb) {\n\t\t \n\t\tif ((head == &q->new_flows) && !list_empty(&q->old_flows))\n\t\t\tlist_move_tail(&flow->flowchain, &q->old_flows);\n\t\telse\n\t\t\tlist_del_init(&flow->flowchain);\n\t\tgoto begin;\n\t}\n\tqdisc_bstats_update(sch, skb);\n\tflow->deficit -= qdisc_pkt_len(skb);\n\t \n\tif (q->cstats.drop_count && sch->q.qlen) {\n\t\tqdisc_tree_reduce_backlog(sch, q->cstats.drop_count,\n\t\t\t\t\t  q->cstats.drop_len);\n\t\tq->cstats.drop_count = 0;\n\t\tq->cstats.drop_len = 0;\n\t}\n\treturn skb;\n}\n\nstatic void fq_codel_flow_purge(struct fq_codel_flow *flow)\n{\n\trtnl_kfree_skbs(flow->head, flow->tail);\n\tflow->head = NULL;\n}\n\nstatic void fq_codel_reset(struct Qdisc *sch)\n{\n\tstruct fq_codel_sched_data *q = qdisc_priv(sch);\n\tint i;\n\n\tINIT_LIST_HEAD(&q->new_flows);\n\tINIT_LIST_HEAD(&q->old_flows);\n\tfor (i = 0; i < q->flows_cnt; i++) {\n\t\tstruct fq_codel_flow *flow = q->flows + i;\n\n\t\tfq_codel_flow_purge(flow);\n\t\tINIT_LIST_HEAD(&flow->flowchain);\n\t\tcodel_vars_init(&flow->cvars);\n\t}\n\tmemset(q->backlogs, 0, q->flows_cnt * sizeof(u32));\n\tq->memory_usage = 0;\n}\n\nstatic const struct nla_policy fq_codel_policy[TCA_FQ_CODEL_MAX + 1] = {\n\t[TCA_FQ_CODEL_TARGET]\t= { .type = NLA_U32 },\n\t[TCA_FQ_CODEL_LIMIT]\t= { .type = NLA_U32 },\n\t[TCA_FQ_CODEL_INTERVAL]\t= { .type = NLA_U32 },\n\t[TCA_FQ_CODEL_ECN]\t= { .type = NLA_U32 },\n\t[TCA_FQ_CODEL_FLOWS]\t= { .type = NLA_U32 },\n\t[TCA_FQ_CODEL_QUANTUM]\t= { .type = NLA_U32 },\n\t[TCA_FQ_CODEL_CE_THRESHOLD] = { .type = NLA_U32 },\n\t[TCA_FQ_CODEL_DROP_BATCH_SIZE] = { .type = NLA_U32 },\n\t[TCA_FQ_CODEL_MEMORY_LIMIT] = { .type = NLA_U32 },\n\t[TCA_FQ_CODEL_CE_THRESHOLD_SELECTOR] = { .type = NLA_U8 },\n\t[TCA_FQ_CODEL_CE_THRESHOLD_MASK] = { .type = NLA_U8 },\n};\n\nstatic int fq_codel_change(struct Qdisc *sch, struct nlattr *opt,\n\t\t\t   struct netlink_ext_ack *extack)\n{\n\tstruct fq_codel_sched_data *q = qdisc_priv(sch);\n\tstruct nlattr *tb[TCA_FQ_CODEL_MAX + 1];\n\tu32 quantum = 0;\n\tint err;\n\n\terr = nla_parse_nested_deprecated(tb, TCA_FQ_CODEL_MAX, opt,\n\t\t\t\t\t  fq_codel_policy, NULL);\n\tif (err < 0)\n\t\treturn err;\n\tif (tb[TCA_FQ_CODEL_FLOWS]) {\n\t\tif (q->flows)\n\t\t\treturn -EINVAL;\n\t\tq->flows_cnt = nla_get_u32(tb[TCA_FQ_CODEL_FLOWS]);\n\t\tif (!q->flows_cnt ||\n\t\t    q->flows_cnt > 65536)\n\t\t\treturn -EINVAL;\n\t}\n\tif (tb[TCA_FQ_CODEL_QUANTUM]) {\n\t\tquantum = max(256U, nla_get_u32(tb[TCA_FQ_CODEL_QUANTUM]));\n\t\tif (quantum > FQ_CODEL_QUANTUM_MAX) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Invalid quantum\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\tsch_tree_lock(sch);\n\n\tif (tb[TCA_FQ_CODEL_TARGET]) {\n\t\tu64 target = nla_get_u32(tb[TCA_FQ_CODEL_TARGET]);\n\n\t\tq->cparams.target = (target * NSEC_PER_USEC) >> CODEL_SHIFT;\n\t}\n\n\tif (tb[TCA_FQ_CODEL_CE_THRESHOLD]) {\n\t\tu64 val = nla_get_u32(tb[TCA_FQ_CODEL_CE_THRESHOLD]);\n\n\t\tq->cparams.ce_threshold = (val * NSEC_PER_USEC) >> CODEL_SHIFT;\n\t}\n\n\tif (tb[TCA_FQ_CODEL_CE_THRESHOLD_SELECTOR])\n\t\tq->cparams.ce_threshold_selector = nla_get_u8(tb[TCA_FQ_CODEL_CE_THRESHOLD_SELECTOR]);\n\tif (tb[TCA_FQ_CODEL_CE_THRESHOLD_MASK])\n\t\tq->cparams.ce_threshold_mask = nla_get_u8(tb[TCA_FQ_CODEL_CE_THRESHOLD_MASK]);\n\n\tif (tb[TCA_FQ_CODEL_INTERVAL]) {\n\t\tu64 interval = nla_get_u32(tb[TCA_FQ_CODEL_INTERVAL]);\n\n\t\tq->cparams.interval = (interval * NSEC_PER_USEC) >> CODEL_SHIFT;\n\t}\n\n\tif (tb[TCA_FQ_CODEL_LIMIT])\n\t\tsch->limit = nla_get_u32(tb[TCA_FQ_CODEL_LIMIT]);\n\n\tif (tb[TCA_FQ_CODEL_ECN])\n\t\tq->cparams.ecn = !!nla_get_u32(tb[TCA_FQ_CODEL_ECN]);\n\n\tif (quantum)\n\t\tq->quantum = quantum;\n\n\tif (tb[TCA_FQ_CODEL_DROP_BATCH_SIZE])\n\t\tq->drop_batch_size = max(1U, nla_get_u32(tb[TCA_FQ_CODEL_DROP_BATCH_SIZE]));\n\n\tif (tb[TCA_FQ_CODEL_MEMORY_LIMIT])\n\t\tq->memory_limit = min(1U << 31, nla_get_u32(tb[TCA_FQ_CODEL_MEMORY_LIMIT]));\n\n\twhile (sch->q.qlen > sch->limit ||\n\t       q->memory_usage > q->memory_limit) {\n\t\tstruct sk_buff *skb = fq_codel_dequeue(sch);\n\n\t\tq->cstats.drop_len += qdisc_pkt_len(skb);\n\t\trtnl_kfree_skbs(skb, skb);\n\t\tq->cstats.drop_count++;\n\t}\n\tqdisc_tree_reduce_backlog(sch, q->cstats.drop_count, q->cstats.drop_len);\n\tq->cstats.drop_count = 0;\n\tq->cstats.drop_len = 0;\n\n\tsch_tree_unlock(sch);\n\treturn 0;\n}\n\nstatic void fq_codel_destroy(struct Qdisc *sch)\n{\n\tstruct fq_codel_sched_data *q = qdisc_priv(sch);\n\n\ttcf_block_put(q->block);\n\tkvfree(q->backlogs);\n\tkvfree(q->flows);\n}\n\nstatic int fq_codel_init(struct Qdisc *sch, struct nlattr *opt,\n\t\t\t struct netlink_ext_ack *extack)\n{\n\tstruct fq_codel_sched_data *q = qdisc_priv(sch);\n\tint i;\n\tint err;\n\n\tsch->limit = 10*1024;\n\tq->flows_cnt = 1024;\n\tq->memory_limit = 32 << 20;  \n\tq->drop_batch_size = 64;\n\tq->quantum = psched_mtu(qdisc_dev(sch));\n\tINIT_LIST_HEAD(&q->new_flows);\n\tINIT_LIST_HEAD(&q->old_flows);\n\tcodel_params_init(&q->cparams);\n\tcodel_stats_init(&q->cstats);\n\tq->cparams.ecn = true;\n\tq->cparams.mtu = psched_mtu(qdisc_dev(sch));\n\n\tif (opt) {\n\t\terr = fq_codel_change(sch, opt, extack);\n\t\tif (err)\n\t\t\tgoto init_failure;\n\t}\n\n\terr = tcf_block_get(&q->block, &q->filter_list, sch, extack);\n\tif (err)\n\t\tgoto init_failure;\n\n\tif (!q->flows) {\n\t\tq->flows = kvcalloc(q->flows_cnt,\n\t\t\t\t    sizeof(struct fq_codel_flow),\n\t\t\t\t    GFP_KERNEL);\n\t\tif (!q->flows) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto init_failure;\n\t\t}\n\t\tq->backlogs = kvcalloc(q->flows_cnt, sizeof(u32), GFP_KERNEL);\n\t\tif (!q->backlogs) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto alloc_failure;\n\t\t}\n\t\tfor (i = 0; i < q->flows_cnt; i++) {\n\t\t\tstruct fq_codel_flow *flow = q->flows + i;\n\n\t\t\tINIT_LIST_HEAD(&flow->flowchain);\n\t\t\tcodel_vars_init(&flow->cvars);\n\t\t}\n\t}\n\tif (sch->limit >= 1)\n\t\tsch->flags |= TCQ_F_CAN_BYPASS;\n\telse\n\t\tsch->flags &= ~TCQ_F_CAN_BYPASS;\n\treturn 0;\n\nalloc_failure:\n\tkvfree(q->flows);\n\tq->flows = NULL;\ninit_failure:\n\tq->flows_cnt = 0;\n\treturn err;\n}\n\nstatic int fq_codel_dump(struct Qdisc *sch, struct sk_buff *skb)\n{\n\tstruct fq_codel_sched_data *q = qdisc_priv(sch);\n\tstruct nlattr *opts;\n\n\topts = nla_nest_start_noflag(skb, TCA_OPTIONS);\n\tif (opts == NULL)\n\t\tgoto nla_put_failure;\n\n\tif (nla_put_u32(skb, TCA_FQ_CODEL_TARGET,\n\t\t\tcodel_time_to_us(q->cparams.target)) ||\n\t    nla_put_u32(skb, TCA_FQ_CODEL_LIMIT,\n\t\t\tsch->limit) ||\n\t    nla_put_u32(skb, TCA_FQ_CODEL_INTERVAL,\n\t\t\tcodel_time_to_us(q->cparams.interval)) ||\n\t    nla_put_u32(skb, TCA_FQ_CODEL_ECN,\n\t\t\tq->cparams.ecn) ||\n\t    nla_put_u32(skb, TCA_FQ_CODEL_QUANTUM,\n\t\t\tq->quantum) ||\n\t    nla_put_u32(skb, TCA_FQ_CODEL_DROP_BATCH_SIZE,\n\t\t\tq->drop_batch_size) ||\n\t    nla_put_u32(skb, TCA_FQ_CODEL_MEMORY_LIMIT,\n\t\t\tq->memory_limit) ||\n\t    nla_put_u32(skb, TCA_FQ_CODEL_FLOWS,\n\t\t\tq->flows_cnt))\n\t\tgoto nla_put_failure;\n\n\tif (q->cparams.ce_threshold != CODEL_DISABLED_THRESHOLD) {\n\t\tif (nla_put_u32(skb, TCA_FQ_CODEL_CE_THRESHOLD,\n\t\t\t\tcodel_time_to_us(q->cparams.ce_threshold)))\n\t\t\tgoto nla_put_failure;\n\t\tif (nla_put_u8(skb, TCA_FQ_CODEL_CE_THRESHOLD_SELECTOR, q->cparams.ce_threshold_selector))\n\t\t\tgoto nla_put_failure;\n\t\tif (nla_put_u8(skb, TCA_FQ_CODEL_CE_THRESHOLD_MASK, q->cparams.ce_threshold_mask))\n\t\t\tgoto nla_put_failure;\n\t}\n\n\treturn nla_nest_end(skb, opts);\n\nnla_put_failure:\n\treturn -1;\n}\n\nstatic int fq_codel_dump_stats(struct Qdisc *sch, struct gnet_dump *d)\n{\n\tstruct fq_codel_sched_data *q = qdisc_priv(sch);\n\tstruct tc_fq_codel_xstats st = {\n\t\t.type\t\t\t\t= TCA_FQ_CODEL_XSTATS_QDISC,\n\t};\n\tstruct list_head *pos;\n\n\tst.qdisc_stats.maxpacket = q->cstats.maxpacket;\n\tst.qdisc_stats.drop_overlimit = q->drop_overlimit;\n\tst.qdisc_stats.ecn_mark = q->cstats.ecn_mark;\n\tst.qdisc_stats.new_flow_count = q->new_flow_count;\n\tst.qdisc_stats.ce_mark = q->cstats.ce_mark;\n\tst.qdisc_stats.memory_usage  = q->memory_usage;\n\tst.qdisc_stats.drop_overmemory = q->drop_overmemory;\n\n\tsch_tree_lock(sch);\n\tlist_for_each(pos, &q->new_flows)\n\t\tst.qdisc_stats.new_flows_len++;\n\n\tlist_for_each(pos, &q->old_flows)\n\t\tst.qdisc_stats.old_flows_len++;\n\tsch_tree_unlock(sch);\n\n\treturn gnet_stats_copy_app(d, &st, sizeof(st));\n}\n\nstatic struct Qdisc *fq_codel_leaf(struct Qdisc *sch, unsigned long arg)\n{\n\treturn NULL;\n}\n\nstatic unsigned long fq_codel_find(struct Qdisc *sch, u32 classid)\n{\n\treturn 0;\n}\n\nstatic unsigned long fq_codel_bind(struct Qdisc *sch, unsigned long parent,\n\t\t\t      u32 classid)\n{\n\treturn 0;\n}\n\nstatic void fq_codel_unbind(struct Qdisc *q, unsigned long cl)\n{\n}\n\nstatic struct tcf_block *fq_codel_tcf_block(struct Qdisc *sch, unsigned long cl,\n\t\t\t\t\t    struct netlink_ext_ack *extack)\n{\n\tstruct fq_codel_sched_data *q = qdisc_priv(sch);\n\n\tif (cl)\n\t\treturn NULL;\n\treturn q->block;\n}\n\nstatic int fq_codel_dump_class(struct Qdisc *sch, unsigned long cl,\n\t\t\t  struct sk_buff *skb, struct tcmsg *tcm)\n{\n\ttcm->tcm_handle |= TC_H_MIN(cl);\n\treturn 0;\n}\n\nstatic int fq_codel_dump_class_stats(struct Qdisc *sch, unsigned long cl,\n\t\t\t\t     struct gnet_dump *d)\n{\n\tstruct fq_codel_sched_data *q = qdisc_priv(sch);\n\tu32 idx = cl - 1;\n\tstruct gnet_stats_queue qs = { 0 };\n\tstruct tc_fq_codel_xstats xstats;\n\n\tif (idx < q->flows_cnt) {\n\t\tconst struct fq_codel_flow *flow = &q->flows[idx];\n\t\tconst struct sk_buff *skb;\n\n\t\tmemset(&xstats, 0, sizeof(xstats));\n\t\txstats.type = TCA_FQ_CODEL_XSTATS_CLASS;\n\t\txstats.class_stats.deficit = flow->deficit;\n\t\txstats.class_stats.ldelay =\n\t\t\tcodel_time_to_us(flow->cvars.ldelay);\n\t\txstats.class_stats.count = flow->cvars.count;\n\t\txstats.class_stats.lastcount = flow->cvars.lastcount;\n\t\txstats.class_stats.dropping = flow->cvars.dropping;\n\t\tif (flow->cvars.dropping) {\n\t\t\tcodel_tdiff_t delta = flow->cvars.drop_next -\n\t\t\t\t\t      codel_get_time();\n\n\t\t\txstats.class_stats.drop_next = (delta >= 0) ?\n\t\t\t\tcodel_time_to_us(delta) :\n\t\t\t\t-codel_time_to_us(-delta);\n\t\t}\n\t\tif (flow->head) {\n\t\t\tsch_tree_lock(sch);\n\t\t\tskb = flow->head;\n\t\t\twhile (skb) {\n\t\t\t\tqs.qlen++;\n\t\t\t\tskb = skb->next;\n\t\t\t}\n\t\t\tsch_tree_unlock(sch);\n\t\t}\n\t\tqs.backlog = q->backlogs[idx];\n\t\tqs.drops = 0;\n\t}\n\tif (gnet_stats_copy_queue(d, NULL, &qs, qs.qlen) < 0)\n\t\treturn -1;\n\tif (idx < q->flows_cnt)\n\t\treturn gnet_stats_copy_app(d, &xstats, sizeof(xstats));\n\treturn 0;\n}\n\nstatic void fq_codel_walk(struct Qdisc *sch, struct qdisc_walker *arg)\n{\n\tstruct fq_codel_sched_data *q = qdisc_priv(sch);\n\tunsigned int i;\n\n\tif (arg->stop)\n\t\treturn;\n\n\tfor (i = 0; i < q->flows_cnt; i++) {\n\t\tif (list_empty(&q->flows[i].flowchain)) {\n\t\t\targ->count++;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!tc_qdisc_stats_dump(sch, i + 1, arg))\n\t\t\tbreak;\n\t}\n}\n\nstatic const struct Qdisc_class_ops fq_codel_class_ops = {\n\t.leaf\t\t=\tfq_codel_leaf,\n\t.find\t\t=\tfq_codel_find,\n\t.tcf_block\t=\tfq_codel_tcf_block,\n\t.bind_tcf\t=\tfq_codel_bind,\n\t.unbind_tcf\t=\tfq_codel_unbind,\n\t.dump\t\t=\tfq_codel_dump_class,\n\t.dump_stats\t=\tfq_codel_dump_class_stats,\n\t.walk\t\t=\tfq_codel_walk,\n};\n\nstatic struct Qdisc_ops fq_codel_qdisc_ops __read_mostly = {\n\t.cl_ops\t\t=\t&fq_codel_class_ops,\n\t.id\t\t=\t\"fq_codel\",\n\t.priv_size\t=\tsizeof(struct fq_codel_sched_data),\n\t.enqueue\t=\tfq_codel_enqueue,\n\t.dequeue\t=\tfq_codel_dequeue,\n\t.peek\t\t=\tqdisc_peek_dequeued,\n\t.init\t\t=\tfq_codel_init,\n\t.reset\t\t=\tfq_codel_reset,\n\t.destroy\t=\tfq_codel_destroy,\n\t.change\t\t=\tfq_codel_change,\n\t.dump\t\t=\tfq_codel_dump,\n\t.dump_stats =\tfq_codel_dump_stats,\n\t.owner\t\t=\tTHIS_MODULE,\n};\n\nstatic int __init fq_codel_module_init(void)\n{\n\treturn register_qdisc(&fq_codel_qdisc_ops);\n}\n\nstatic void __exit fq_codel_module_exit(void)\n{\n\tunregister_qdisc(&fq_codel_qdisc_ops);\n}\n\nmodule_init(fq_codel_module_init)\nmodule_exit(fq_codel_module_exit)\nMODULE_AUTHOR(\"Eric Dumazet\");\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"Fair Queue CoDel discipline\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}