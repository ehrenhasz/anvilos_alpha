{
  "module_name": "sch_taprio.c",
  "hash_id": "6d172e985efcee4a81f77d4fee7cb79e324703292311e663d574ad2ac1c4e371",
  "original_prompt": "Ingested from linux-6.6.14/net/sched/sch_taprio.c",
  "human_readable_source": "\n\n \n\n#include <linux/ethtool.h>\n#include <linux/ethtool_netlink.h>\n#include <linux/types.h>\n#include <linux/slab.h>\n#include <linux/kernel.h>\n#include <linux/string.h>\n#include <linux/list.h>\n#include <linux/errno.h>\n#include <linux/skbuff.h>\n#include <linux/math64.h>\n#include <linux/module.h>\n#include <linux/spinlock.h>\n#include <linux/rcupdate.h>\n#include <linux/time.h>\n#include <net/gso.h>\n#include <net/netlink.h>\n#include <net/pkt_sched.h>\n#include <net/pkt_cls.h>\n#include <net/sch_generic.h>\n#include <net/sock.h>\n#include <net/tcp.h>\n\n#define TAPRIO_STAT_NOT_SET\t(~0ULL)\n\n#include \"sch_mqprio_lib.h\"\n\nstatic LIST_HEAD(taprio_list);\nstatic struct static_key_false taprio_have_broken_mqprio;\nstatic struct static_key_false taprio_have_working_mqprio;\n\n#define TAPRIO_ALL_GATES_OPEN -1\n\n#define TXTIME_ASSIST_IS_ENABLED(flags) ((flags) & TCA_TAPRIO_ATTR_FLAG_TXTIME_ASSIST)\n#define FULL_OFFLOAD_IS_ENABLED(flags) ((flags) & TCA_TAPRIO_ATTR_FLAG_FULL_OFFLOAD)\n#define TAPRIO_FLAGS_INVALID U32_MAX\n\nstruct sched_entry {\n\t \n\tu64 gate_duration[TC_MAX_QUEUE];\n\tatomic_t budget[TC_MAX_QUEUE];\n\t \n\tktime_t gate_close_time[TC_MAX_QUEUE];\n\tstruct list_head list;\n\t \n\tktime_t end_time;\n\tktime_t next_txtime;\n\tint index;\n\tu32 gate_mask;\n\tu32 interval;\n\tu8 command;\n};\n\nstruct sched_gate_list {\n\t \n\tu64 max_open_gate_duration[TC_MAX_QUEUE];\n\tu32 max_frm_len[TC_MAX_QUEUE];  \n\tu32 max_sdu[TC_MAX_QUEUE];  \n\tstruct rcu_head rcu;\n\tstruct list_head entries;\n\tsize_t num_entries;\n\tktime_t cycle_end_time;\n\ts64 cycle_time;\n\ts64 cycle_time_extension;\n\ts64 base_time;\n};\n\nstruct taprio_sched {\n\tstruct Qdisc **qdiscs;\n\tstruct Qdisc *root;\n\tu32 flags;\n\tenum tk_offsets tk_offset;\n\tint clockid;\n\tbool offloaded;\n\tbool detected_mqprio;\n\tbool broken_mqprio;\n\tatomic64_t picos_per_byte;  \n\n\t \n\tspinlock_t current_entry_lock;\n\tstruct sched_entry __rcu *current_entry;\n\tstruct sched_gate_list __rcu *oper_sched;\n\tstruct sched_gate_list __rcu *admin_sched;\n\tstruct hrtimer advance_timer;\n\tstruct list_head taprio_list;\n\tint cur_txq[TC_MAX_QUEUE];\n\tu32 max_sdu[TC_MAX_QUEUE];  \n\tu32 fp[TC_QOPT_MAX_QUEUE];  \n\tu32 txtime_delay;\n};\n\nstruct __tc_taprio_qopt_offload {\n\trefcount_t users;\n\tstruct tc_taprio_qopt_offload offload;\n};\n\nstatic void taprio_calculate_gate_durations(struct taprio_sched *q,\n\t\t\t\t\t    struct sched_gate_list *sched)\n{\n\tstruct net_device *dev = qdisc_dev(q->root);\n\tint num_tc = netdev_get_num_tc(dev);\n\tstruct sched_entry *entry, *cur;\n\tint tc;\n\n\tlist_for_each_entry(entry, &sched->entries, list) {\n\t\tu32 gates_still_open = entry->gate_mask;\n\n\t\t \n\t\tcur = entry;\n\n\t\tdo {\n\t\t\tif (!gates_still_open)\n\t\t\t\tbreak;\n\n\t\t\tfor (tc = 0; tc < num_tc; tc++) {\n\t\t\t\tif (!(gates_still_open & BIT(tc)))\n\t\t\t\t\tcontinue;\n\n\t\t\t\tif (cur->gate_mask & BIT(tc))\n\t\t\t\t\tentry->gate_duration[tc] += cur->interval;\n\t\t\t\telse\n\t\t\t\t\tgates_still_open &= ~BIT(tc);\n\t\t\t}\n\n\t\t\tcur = list_next_entry_circular(cur, &sched->entries, list);\n\t\t} while (cur != entry);\n\n\t\t \n\t\tfor (tc = 0; tc < num_tc; tc++)\n\t\t\tif (entry->gate_duration[tc] &&\n\t\t\t    sched->max_open_gate_duration[tc] < entry->gate_duration[tc])\n\t\t\t\tsched->max_open_gate_duration[tc] = entry->gate_duration[tc];\n\t}\n}\n\nstatic bool taprio_entry_allows_tx(ktime_t skb_end_time,\n\t\t\t\t   struct sched_entry *entry, int tc)\n{\n\treturn ktime_before(skb_end_time, entry->gate_close_time[tc]);\n}\n\nstatic ktime_t sched_base_time(const struct sched_gate_list *sched)\n{\n\tif (!sched)\n\t\treturn KTIME_MAX;\n\n\treturn ns_to_ktime(sched->base_time);\n}\n\nstatic ktime_t taprio_mono_to_any(const struct taprio_sched *q, ktime_t mono)\n{\n\t \n\tenum tk_offsets tk_offset = READ_ONCE(q->tk_offset);\n\n\tswitch (tk_offset) {\n\tcase TK_OFFS_MAX:\n\t\treturn mono;\n\tdefault:\n\t\treturn ktime_mono_to_any(mono, tk_offset);\n\t}\n}\n\nstatic ktime_t taprio_get_time(const struct taprio_sched *q)\n{\n\treturn taprio_mono_to_any(q, ktime_get());\n}\n\nstatic void taprio_free_sched_cb(struct rcu_head *head)\n{\n\tstruct sched_gate_list *sched = container_of(head, struct sched_gate_list, rcu);\n\tstruct sched_entry *entry, *n;\n\n\tlist_for_each_entry_safe(entry, n, &sched->entries, list) {\n\t\tlist_del(&entry->list);\n\t\tkfree(entry);\n\t}\n\n\tkfree(sched);\n}\n\nstatic void switch_schedules(struct taprio_sched *q,\n\t\t\t     struct sched_gate_list **admin,\n\t\t\t     struct sched_gate_list **oper)\n{\n\trcu_assign_pointer(q->oper_sched, *admin);\n\trcu_assign_pointer(q->admin_sched, NULL);\n\n\tif (*oper)\n\t\tcall_rcu(&(*oper)->rcu, taprio_free_sched_cb);\n\n\t*oper = *admin;\n\t*admin = NULL;\n}\n\n \nstatic s32 get_cycle_time_elapsed(struct sched_gate_list *sched, ktime_t time)\n{\n\tktime_t time_since_sched_start;\n\ts32 time_elapsed;\n\n\ttime_since_sched_start = ktime_sub(time, sched->base_time);\n\tdiv_s64_rem(time_since_sched_start, sched->cycle_time, &time_elapsed);\n\n\treturn time_elapsed;\n}\n\nstatic ktime_t get_interval_end_time(struct sched_gate_list *sched,\n\t\t\t\t     struct sched_gate_list *admin,\n\t\t\t\t     struct sched_entry *entry,\n\t\t\t\t     ktime_t intv_start)\n{\n\ts32 cycle_elapsed = get_cycle_time_elapsed(sched, intv_start);\n\tktime_t intv_end, cycle_ext_end, cycle_end;\n\n\tcycle_end = ktime_add_ns(intv_start, sched->cycle_time - cycle_elapsed);\n\tintv_end = ktime_add_ns(intv_start, entry->interval);\n\tcycle_ext_end = ktime_add(cycle_end, sched->cycle_time_extension);\n\n\tif (ktime_before(intv_end, cycle_end))\n\t\treturn intv_end;\n\telse if (admin && admin != sched &&\n\t\t ktime_after(admin->base_time, cycle_end) &&\n\t\t ktime_before(admin->base_time, cycle_ext_end))\n\t\treturn admin->base_time;\n\telse\n\t\treturn cycle_end;\n}\n\nstatic int length_to_duration(struct taprio_sched *q, int len)\n{\n\treturn div_u64(len * atomic64_read(&q->picos_per_byte), PSEC_PER_NSEC);\n}\n\nstatic int duration_to_length(struct taprio_sched *q, u64 duration)\n{\n\treturn div_u64(duration * PSEC_PER_NSEC, atomic64_read(&q->picos_per_byte));\n}\n\n \nstatic void taprio_update_queue_max_sdu(struct taprio_sched *q,\n\t\t\t\t\tstruct sched_gate_list *sched,\n\t\t\t\t\tstruct qdisc_size_table *stab)\n{\n\tstruct net_device *dev = qdisc_dev(q->root);\n\tint num_tc = netdev_get_num_tc(dev);\n\tu32 max_sdu_from_user;\n\tu32 max_sdu_dynamic;\n\tu32 max_sdu;\n\tint tc;\n\n\tfor (tc = 0; tc < num_tc; tc++) {\n\t\tmax_sdu_from_user = q->max_sdu[tc] ?: U32_MAX;\n\n\t\t \n\t\tif (sched->max_open_gate_duration[tc] == sched->cycle_time) {\n\t\t\tmax_sdu_dynamic = U32_MAX;\n\t\t} else {\n\t\t\tu32 max_frm_len;\n\n\t\t\tmax_frm_len = duration_to_length(q, sched->max_open_gate_duration[tc]);\n\t\t\t \n\t\t\tif (stab) {\n\t\t\t\tmax_frm_len -= stab->szopts.overhead;\n\t\t\t\tmax_frm_len = max_t(int, max_frm_len,\n\t\t\t\t\t\t    dev->hard_header_len + 1);\n\t\t\t}\n\t\t\tmax_sdu_dynamic = max_frm_len - dev->hard_header_len;\n\t\t\tif (max_sdu_dynamic > dev->max_mtu)\n\t\t\t\tmax_sdu_dynamic = U32_MAX;\n\t\t}\n\n\t\tmax_sdu = min(max_sdu_dynamic, max_sdu_from_user);\n\n\t\tif (max_sdu != U32_MAX) {\n\t\t\tsched->max_frm_len[tc] = max_sdu + dev->hard_header_len;\n\t\t\tsched->max_sdu[tc] = max_sdu;\n\t\t} else {\n\t\t\tsched->max_frm_len[tc] = U32_MAX;  \n\t\t\tsched->max_sdu[tc] = 0;\n\t\t}\n\t}\n}\n\n \nstatic struct sched_entry *find_entry_to_transmit(struct sk_buff *skb,\n\t\t\t\t\t\t  struct Qdisc *sch,\n\t\t\t\t\t\t  struct sched_gate_list *sched,\n\t\t\t\t\t\t  struct sched_gate_list *admin,\n\t\t\t\t\t\t  ktime_t time,\n\t\t\t\t\t\t  ktime_t *interval_start,\n\t\t\t\t\t\t  ktime_t *interval_end,\n\t\t\t\t\t\t  bool validate_interval)\n{\n\tktime_t curr_intv_start, curr_intv_end, cycle_end, packet_transmit_time;\n\tktime_t earliest_txtime = KTIME_MAX, txtime, cycle, transmit_end_time;\n\tstruct sched_entry *entry = NULL, *entry_found = NULL;\n\tstruct taprio_sched *q = qdisc_priv(sch);\n\tstruct net_device *dev = qdisc_dev(sch);\n\tbool entry_available = false;\n\ts32 cycle_elapsed;\n\tint tc, n;\n\n\ttc = netdev_get_prio_tc_map(dev, skb->priority);\n\tpacket_transmit_time = length_to_duration(q, qdisc_pkt_len(skb));\n\n\t*interval_start = 0;\n\t*interval_end = 0;\n\n\tif (!sched)\n\t\treturn NULL;\n\n\tcycle = sched->cycle_time;\n\tcycle_elapsed = get_cycle_time_elapsed(sched, time);\n\tcurr_intv_end = ktime_sub_ns(time, cycle_elapsed);\n\tcycle_end = ktime_add_ns(curr_intv_end, cycle);\n\n\tlist_for_each_entry(entry, &sched->entries, list) {\n\t\tcurr_intv_start = curr_intv_end;\n\t\tcurr_intv_end = get_interval_end_time(sched, admin, entry,\n\t\t\t\t\t\t      curr_intv_start);\n\n\t\tif (ktime_after(curr_intv_start, cycle_end))\n\t\t\tbreak;\n\n\t\tif (!(entry->gate_mask & BIT(tc)) ||\n\t\t    packet_transmit_time > entry->interval)\n\t\t\tcontinue;\n\n\t\ttxtime = entry->next_txtime;\n\n\t\tif (ktime_before(txtime, time) || validate_interval) {\n\t\t\ttransmit_end_time = ktime_add_ns(time, packet_transmit_time);\n\t\t\tif ((ktime_before(curr_intv_start, time) &&\n\t\t\t     ktime_before(transmit_end_time, curr_intv_end)) ||\n\t\t\t    (ktime_after(curr_intv_start, time) && !validate_interval)) {\n\t\t\t\tentry_found = entry;\n\t\t\t\t*interval_start = curr_intv_start;\n\t\t\t\t*interval_end = curr_intv_end;\n\t\t\t\tbreak;\n\t\t\t} else if (!entry_available && !validate_interval) {\n\t\t\t\t \n\t\t\t\tentry_available = true;\n\t\t\t\tentry_found = entry;\n\t\t\t\t*interval_start = ktime_add_ns(curr_intv_start, cycle);\n\t\t\t\t*interval_end = ktime_add_ns(curr_intv_end, cycle);\n\t\t\t}\n\t\t} else if (ktime_before(txtime, earliest_txtime) &&\n\t\t\t   !entry_available) {\n\t\t\tearliest_txtime = txtime;\n\t\t\tentry_found = entry;\n\t\t\tn = div_s64(ktime_sub(txtime, curr_intv_start), cycle);\n\t\t\t*interval_start = ktime_add(curr_intv_start, n * cycle);\n\t\t\t*interval_end = ktime_add(curr_intv_end, n * cycle);\n\t\t}\n\t}\n\n\treturn entry_found;\n}\n\nstatic bool is_valid_interval(struct sk_buff *skb, struct Qdisc *sch)\n{\n\tstruct taprio_sched *q = qdisc_priv(sch);\n\tstruct sched_gate_list *sched, *admin;\n\tktime_t interval_start, interval_end;\n\tstruct sched_entry *entry;\n\n\trcu_read_lock();\n\tsched = rcu_dereference(q->oper_sched);\n\tadmin = rcu_dereference(q->admin_sched);\n\n\tentry = find_entry_to_transmit(skb, sch, sched, admin, skb->tstamp,\n\t\t\t\t       &interval_start, &interval_end, true);\n\trcu_read_unlock();\n\n\treturn entry;\n}\n\nstatic bool taprio_flags_valid(u32 flags)\n{\n\t \n\tif (flags & ~(TCA_TAPRIO_ATTR_FLAG_TXTIME_ASSIST |\n\t\t      TCA_TAPRIO_ATTR_FLAG_FULL_OFFLOAD))\n\t\treturn false;\n\t \n\tif ((flags & TCA_TAPRIO_ATTR_FLAG_TXTIME_ASSIST) &&\n\t    (flags & TCA_TAPRIO_ATTR_FLAG_FULL_OFFLOAD))\n\t\treturn false;\n\treturn true;\n}\n\n \nstatic ktime_t get_tcp_tstamp(struct taprio_sched *q, struct sk_buff *skb)\n{\n\tunsigned int offset = skb_network_offset(skb);\n\tconst struct ipv6hdr *ipv6h;\n\tconst struct iphdr *iph;\n\tstruct ipv6hdr _ipv6h;\n\n\tipv6h = skb_header_pointer(skb, offset, sizeof(_ipv6h), &_ipv6h);\n\tif (!ipv6h)\n\t\treturn 0;\n\n\tif (ipv6h->version == 4) {\n\t\tiph = (struct iphdr *)ipv6h;\n\t\toffset += iph->ihl * 4;\n\n\t\t \n\t\tif (iph->protocol == IPPROTO_IPV6) {\n\t\t\tipv6h = skb_header_pointer(skb, offset,\n\t\t\t\t\t\t   sizeof(_ipv6h), &_ipv6h);\n\n\t\t\tif (!ipv6h || ipv6h->nexthdr != IPPROTO_TCP)\n\t\t\t\treturn 0;\n\t\t} else if (iph->protocol != IPPROTO_TCP) {\n\t\t\treturn 0;\n\t\t}\n\t} else if (ipv6h->version == 6 && ipv6h->nexthdr != IPPROTO_TCP) {\n\t\treturn 0;\n\t}\n\n\treturn taprio_mono_to_any(q, skb->skb_mstamp_ns);\n}\n\n \nstatic long get_packet_txtime(struct sk_buff *skb, struct Qdisc *sch)\n{\n\tktime_t transmit_end_time, interval_end, interval_start, tcp_tstamp;\n\tstruct taprio_sched *q = qdisc_priv(sch);\n\tstruct sched_gate_list *sched, *admin;\n\tktime_t minimum_time, now, txtime;\n\tint len, packet_transmit_time;\n\tstruct sched_entry *entry;\n\tbool sched_changed;\n\n\tnow = taprio_get_time(q);\n\tminimum_time = ktime_add_ns(now, q->txtime_delay);\n\n\ttcp_tstamp = get_tcp_tstamp(q, skb);\n\tminimum_time = max_t(ktime_t, minimum_time, tcp_tstamp);\n\n\trcu_read_lock();\n\tadmin = rcu_dereference(q->admin_sched);\n\tsched = rcu_dereference(q->oper_sched);\n\tif (admin && ktime_after(minimum_time, admin->base_time))\n\t\tswitch_schedules(q, &admin, &sched);\n\n\t \n\tif (!sched || ktime_before(minimum_time, sched->base_time)) {\n\t\ttxtime = minimum_time;\n\t\tgoto done;\n\t}\n\n\tlen = qdisc_pkt_len(skb);\n\tpacket_transmit_time = length_to_duration(q, len);\n\n\tdo {\n\t\tsched_changed = false;\n\n\t\tentry = find_entry_to_transmit(skb, sch, sched, admin,\n\t\t\t\t\t       minimum_time,\n\t\t\t\t\t       &interval_start, &interval_end,\n\t\t\t\t\t       false);\n\t\tif (!entry) {\n\t\t\ttxtime = 0;\n\t\t\tgoto done;\n\t\t}\n\n\t\ttxtime = entry->next_txtime;\n\t\ttxtime = max_t(ktime_t, txtime, minimum_time);\n\t\ttxtime = max_t(ktime_t, txtime, interval_start);\n\n\t\tif (admin && admin != sched &&\n\t\t    ktime_after(txtime, admin->base_time)) {\n\t\t\tsched = admin;\n\t\t\tsched_changed = true;\n\t\t\tcontinue;\n\t\t}\n\n\t\ttransmit_end_time = ktime_add(txtime, packet_transmit_time);\n\t\tminimum_time = transmit_end_time;\n\n\t\t \n\t\tif (ktime_after(transmit_end_time, interval_end))\n\t\t\tentry->next_txtime = ktime_add(interval_start, sched->cycle_time);\n\t} while (sched_changed || ktime_after(transmit_end_time, interval_end));\n\n\tentry->next_txtime = transmit_end_time;\n\ndone:\n\trcu_read_unlock();\n\treturn txtime;\n}\n\n \nstatic bool taprio_skb_exceeds_queue_max_sdu(struct Qdisc *sch,\n\t\t\t\t\t     struct sk_buff *skb)\n{\n\tstruct taprio_sched *q = qdisc_priv(sch);\n\tstruct net_device *dev = qdisc_dev(sch);\n\tstruct sched_gate_list *sched;\n\tint prio = skb->priority;\n\tbool exceeds = false;\n\tu8 tc;\n\n\ttc = netdev_get_prio_tc_map(dev, prio);\n\n\trcu_read_lock();\n\tsched = rcu_dereference(q->oper_sched);\n\tif (sched && skb->len > sched->max_frm_len[tc])\n\t\texceeds = true;\n\trcu_read_unlock();\n\n\treturn exceeds;\n}\n\nstatic int taprio_enqueue_one(struct sk_buff *skb, struct Qdisc *sch,\n\t\t\t      struct Qdisc *child, struct sk_buff **to_free)\n{\n\tstruct taprio_sched *q = qdisc_priv(sch);\n\n\t \n\tif (skb->sk && sk_fullsock(skb->sk) && sock_flag(skb->sk, SOCK_TXTIME)) {\n\t\tif (!is_valid_interval(skb, sch))\n\t\t\treturn qdisc_drop(skb, sch, to_free);\n\t} else if (TXTIME_ASSIST_IS_ENABLED(q->flags)) {\n\t\tskb->tstamp = get_packet_txtime(skb, sch);\n\t\tif (!skb->tstamp)\n\t\t\treturn qdisc_drop(skb, sch, to_free);\n\t}\n\n\tqdisc_qstats_backlog_inc(sch, skb);\n\tsch->q.qlen++;\n\n\treturn qdisc_enqueue(skb, child, to_free);\n}\n\nstatic int taprio_enqueue_segmented(struct sk_buff *skb, struct Qdisc *sch,\n\t\t\t\t    struct Qdisc *child,\n\t\t\t\t    struct sk_buff **to_free)\n{\n\tunsigned int slen = 0, numsegs = 0, len = qdisc_pkt_len(skb);\n\tnetdev_features_t features = netif_skb_features(skb);\n\tstruct sk_buff *segs, *nskb;\n\tint ret;\n\n\tsegs = skb_gso_segment(skb, features & ~NETIF_F_GSO_MASK);\n\tif (IS_ERR_OR_NULL(segs))\n\t\treturn qdisc_drop(skb, sch, to_free);\n\n\tskb_list_walk_safe(segs, segs, nskb) {\n\t\tskb_mark_not_on_list(segs);\n\t\tqdisc_skb_cb(segs)->pkt_len = segs->len;\n\t\tslen += segs->len;\n\n\t\t \n\t\tif (taprio_skb_exceeds_queue_max_sdu(sch, segs))\n\t\t\tret = qdisc_drop(segs, sch, to_free);\n\t\telse\n\t\t\tret = taprio_enqueue_one(segs, sch, child, to_free);\n\n\t\tif (ret != NET_XMIT_SUCCESS) {\n\t\t\tif (net_xmit_drop_count(ret))\n\t\t\t\tqdisc_qstats_drop(sch);\n\t\t} else {\n\t\t\tnumsegs++;\n\t\t}\n\t}\n\n\tif (numsegs > 1)\n\t\tqdisc_tree_reduce_backlog(sch, 1 - numsegs, len - slen);\n\tconsume_skb(skb);\n\n\treturn numsegs > 0 ? NET_XMIT_SUCCESS : NET_XMIT_DROP;\n}\n\n \nstatic int taprio_enqueue(struct sk_buff *skb, struct Qdisc *sch,\n\t\t\t  struct sk_buff **to_free)\n{\n\tstruct taprio_sched *q = qdisc_priv(sch);\n\tstruct Qdisc *child;\n\tint queue;\n\n\tqueue = skb_get_queue_mapping(skb);\n\n\tchild = q->qdiscs[queue];\n\tif (unlikely(!child))\n\t\treturn qdisc_drop(skb, sch, to_free);\n\n\tif (taprio_skb_exceeds_queue_max_sdu(sch, skb)) {\n\t\t \n\t\tif (skb_is_gso(skb))\n\t\t\treturn taprio_enqueue_segmented(skb, sch, child,\n\t\t\t\t\t\t\tto_free);\n\n\t\treturn qdisc_drop(skb, sch, to_free);\n\t}\n\n\treturn taprio_enqueue_one(skb, sch, child, to_free);\n}\n\nstatic struct sk_buff *taprio_peek(struct Qdisc *sch)\n{\n\tWARN_ONCE(1, \"taprio only supports operating as root qdisc, peek() not implemented\");\n\treturn NULL;\n}\n\nstatic void taprio_set_budgets(struct taprio_sched *q,\n\t\t\t       struct sched_gate_list *sched,\n\t\t\t       struct sched_entry *entry)\n{\n\tstruct net_device *dev = qdisc_dev(q->root);\n\tint num_tc = netdev_get_num_tc(dev);\n\tint tc, budget;\n\n\tfor (tc = 0; tc < num_tc; tc++) {\n\t\t \n\t\tif (entry->gate_duration[tc] == sched->cycle_time)\n\t\t\tbudget = INT_MAX;\n\t\telse\n\t\t\tbudget = div64_u64((u64)entry->gate_duration[tc] * PSEC_PER_NSEC,\n\t\t\t\t\t   atomic64_read(&q->picos_per_byte));\n\n\t\tatomic_set(&entry->budget[tc], budget);\n\t}\n}\n\n \nstatic int taprio_update_budgets(struct sched_entry *entry, size_t len,\n\t\t\t\t int tc_consumed, int num_tc)\n{\n\tint tc, budget, new_budget = 0;\n\n\tfor (tc = 0; tc < num_tc; tc++) {\n\t\tbudget = atomic_read(&entry->budget[tc]);\n\t\t \n\t\tif (budget == INT_MAX) {\n\t\t\tif (tc == tc_consumed)\n\t\t\t\tnew_budget = budget;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (tc == tc_consumed)\n\t\t\tnew_budget = atomic_sub_return(len, &entry->budget[tc]);\n\t\telse\n\t\t\tatomic_sub(len, &entry->budget[tc]);\n\t}\n\n\treturn new_budget;\n}\n\nstatic struct sk_buff *taprio_dequeue_from_txq(struct Qdisc *sch, int txq,\n\t\t\t\t\t       struct sched_entry *entry,\n\t\t\t\t\t       u32 gate_mask)\n{\n\tstruct taprio_sched *q = qdisc_priv(sch);\n\tstruct net_device *dev = qdisc_dev(sch);\n\tstruct Qdisc *child = q->qdiscs[txq];\n\tint num_tc = netdev_get_num_tc(dev);\n\tstruct sk_buff *skb;\n\tktime_t guard;\n\tint prio;\n\tint len;\n\tu8 tc;\n\n\tif (unlikely(!child))\n\t\treturn NULL;\n\n\tif (TXTIME_ASSIST_IS_ENABLED(q->flags))\n\t\tgoto skip_peek_checks;\n\n\tskb = child->ops->peek(child);\n\tif (!skb)\n\t\treturn NULL;\n\n\tprio = skb->priority;\n\ttc = netdev_get_prio_tc_map(dev, prio);\n\n\tif (!(gate_mask & BIT(tc)))\n\t\treturn NULL;\n\n\tlen = qdisc_pkt_len(skb);\n\tguard = ktime_add_ns(taprio_get_time(q), length_to_duration(q, len));\n\n\t \n\tif (gate_mask != TAPRIO_ALL_GATES_OPEN &&\n\t    !taprio_entry_allows_tx(guard, entry, tc))\n\t\treturn NULL;\n\n\t \n\tif (gate_mask != TAPRIO_ALL_GATES_OPEN &&\n\t    taprio_update_budgets(entry, len, tc, num_tc) < 0)\n\t\treturn NULL;\n\nskip_peek_checks:\n\tskb = child->ops->dequeue(child);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\tqdisc_bstats_update(sch, skb);\n\tqdisc_qstats_backlog_dec(sch, skb);\n\tsch->q.qlen--;\n\n\treturn skb;\n}\n\nstatic void taprio_next_tc_txq(struct net_device *dev, int tc, int *txq)\n{\n\tint offset = dev->tc_to_txq[tc].offset;\n\tint count = dev->tc_to_txq[tc].count;\n\n\t(*txq)++;\n\tif (*txq == offset + count)\n\t\t*txq = offset;\n}\n\n \nstatic struct sk_buff *taprio_dequeue_tc_priority(struct Qdisc *sch,\n\t\t\t\t\t\t  struct sched_entry *entry,\n\t\t\t\t\t\t  u32 gate_mask)\n{\n\tstruct taprio_sched *q = qdisc_priv(sch);\n\tstruct net_device *dev = qdisc_dev(sch);\n\tint num_tc = netdev_get_num_tc(dev);\n\tstruct sk_buff *skb;\n\tint tc;\n\n\tfor (tc = num_tc - 1; tc >= 0; tc--) {\n\t\tint first_txq = q->cur_txq[tc];\n\n\t\tif (!(gate_mask & BIT(tc)))\n\t\t\tcontinue;\n\n\t\tdo {\n\t\t\tskb = taprio_dequeue_from_txq(sch, q->cur_txq[tc],\n\t\t\t\t\t\t      entry, gate_mask);\n\n\t\t\ttaprio_next_tc_txq(dev, tc, &q->cur_txq[tc]);\n\n\t\t\tif (q->cur_txq[tc] >= dev->num_tx_queues)\n\t\t\t\tq->cur_txq[tc] = first_txq;\n\n\t\t\tif (skb)\n\t\t\t\treturn skb;\n\t\t} while (q->cur_txq[tc] != first_txq);\n\t}\n\n\treturn NULL;\n}\n\n \nstatic struct sk_buff *taprio_dequeue_txq_priority(struct Qdisc *sch,\n\t\t\t\t\t\t   struct sched_entry *entry,\n\t\t\t\t\t\t   u32 gate_mask)\n{\n\tstruct net_device *dev = qdisc_dev(sch);\n\tstruct sk_buff *skb;\n\tint i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tskb = taprio_dequeue_from_txq(sch, i, entry, gate_mask);\n\t\tif (skb)\n\t\t\treturn skb;\n\t}\n\n\treturn NULL;\n}\n\n \nstatic struct sk_buff *taprio_dequeue(struct Qdisc *sch)\n{\n\tstruct taprio_sched *q = qdisc_priv(sch);\n\tstruct sk_buff *skb = NULL;\n\tstruct sched_entry *entry;\n\tu32 gate_mask;\n\n\trcu_read_lock();\n\tentry = rcu_dereference(q->current_entry);\n\t \n\tgate_mask = entry ? entry->gate_mask : TAPRIO_ALL_GATES_OPEN;\n\tif (!gate_mask)\n\t\tgoto done;\n\n\tif (static_branch_unlikely(&taprio_have_broken_mqprio) &&\n\t    !static_branch_likely(&taprio_have_working_mqprio)) {\n\t\t \n\t\tskb = taprio_dequeue_txq_priority(sch, entry, gate_mask);\n\t} else if (static_branch_likely(&taprio_have_working_mqprio) &&\n\t\t   !static_branch_unlikely(&taprio_have_broken_mqprio)) {\n\t\t \n\t\tskb = taprio_dequeue_tc_priority(sch, entry, gate_mask);\n\t} else {\n\t\t \n\t\tif (q->broken_mqprio)\n\t\t\tskb = taprio_dequeue_txq_priority(sch, entry, gate_mask);\n\t\telse\n\t\t\tskb = taprio_dequeue_tc_priority(sch, entry, gate_mask);\n\t}\n\ndone:\n\trcu_read_unlock();\n\n\treturn skb;\n}\n\nstatic bool should_restart_cycle(const struct sched_gate_list *oper,\n\t\t\t\t const struct sched_entry *entry)\n{\n\tif (list_is_last(&entry->list, &oper->entries))\n\t\treturn true;\n\n\tif (ktime_compare(entry->end_time, oper->cycle_end_time) == 0)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic bool should_change_schedules(const struct sched_gate_list *admin,\n\t\t\t\t    const struct sched_gate_list *oper,\n\t\t\t\t    ktime_t end_time)\n{\n\tktime_t next_base_time, extension_time;\n\n\tif (!admin)\n\t\treturn false;\n\n\tnext_base_time = sched_base_time(admin);\n\n\t \n\tif (ktime_compare(next_base_time, end_time) <= 0)\n\t\treturn true;\n\n\t \n\textension_time = ktime_add_ns(end_time, oper->cycle_time_extension);\n\n\t \n\tif (ktime_compare(next_base_time, extension_time) <= 0)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic enum hrtimer_restart advance_sched(struct hrtimer *timer)\n{\n\tstruct taprio_sched *q = container_of(timer, struct taprio_sched,\n\t\t\t\t\t      advance_timer);\n\tstruct net_device *dev = qdisc_dev(q->root);\n\tstruct sched_gate_list *oper, *admin;\n\tint num_tc = netdev_get_num_tc(dev);\n\tstruct sched_entry *entry, *next;\n\tstruct Qdisc *sch = q->root;\n\tktime_t end_time;\n\tint tc;\n\n\tspin_lock(&q->current_entry_lock);\n\tentry = rcu_dereference_protected(q->current_entry,\n\t\t\t\t\t  lockdep_is_held(&q->current_entry_lock));\n\toper = rcu_dereference_protected(q->oper_sched,\n\t\t\t\t\t lockdep_is_held(&q->current_entry_lock));\n\tadmin = rcu_dereference_protected(q->admin_sched,\n\t\t\t\t\t  lockdep_is_held(&q->current_entry_lock));\n\n\tif (!oper)\n\t\tswitch_schedules(q, &admin, &oper);\n\n\t \n\tif (unlikely(!entry || entry->end_time == oper->base_time)) {\n\t\tnext = list_first_entry(&oper->entries, struct sched_entry,\n\t\t\t\t\tlist);\n\t\tend_time = next->end_time;\n\t\tgoto first_run;\n\t}\n\n\tif (should_restart_cycle(oper, entry)) {\n\t\tnext = list_first_entry(&oper->entries, struct sched_entry,\n\t\t\t\t\tlist);\n\t\toper->cycle_end_time = ktime_add_ns(oper->cycle_end_time,\n\t\t\t\t\t\t    oper->cycle_time);\n\t} else {\n\t\tnext = list_next_entry(entry, list);\n\t}\n\n\tend_time = ktime_add_ns(entry->end_time, next->interval);\n\tend_time = min_t(ktime_t, end_time, oper->cycle_end_time);\n\n\tfor (tc = 0; tc < num_tc; tc++) {\n\t\tif (next->gate_duration[tc] == oper->cycle_time)\n\t\t\tnext->gate_close_time[tc] = KTIME_MAX;\n\t\telse\n\t\t\tnext->gate_close_time[tc] = ktime_add_ns(entry->end_time,\n\t\t\t\t\t\t\t\t next->gate_duration[tc]);\n\t}\n\n\tif (should_change_schedules(admin, oper, end_time)) {\n\t\t \n\t\tend_time = sched_base_time(admin);\n\t\tswitch_schedules(q, &admin, &oper);\n\t}\n\n\tnext->end_time = end_time;\n\ttaprio_set_budgets(q, oper, next);\n\nfirst_run:\n\trcu_assign_pointer(q->current_entry, next);\n\tspin_unlock(&q->current_entry_lock);\n\n\thrtimer_set_expires(&q->advance_timer, end_time);\n\n\trcu_read_lock();\n\t__netif_schedule(sch);\n\trcu_read_unlock();\n\n\treturn HRTIMER_RESTART;\n}\n\nstatic const struct nla_policy entry_policy[TCA_TAPRIO_SCHED_ENTRY_MAX + 1] = {\n\t[TCA_TAPRIO_SCHED_ENTRY_INDEX]\t   = { .type = NLA_U32 },\n\t[TCA_TAPRIO_SCHED_ENTRY_CMD]\t   = { .type = NLA_U8 },\n\t[TCA_TAPRIO_SCHED_ENTRY_GATE_MASK] = { .type = NLA_U32 },\n\t[TCA_TAPRIO_SCHED_ENTRY_INTERVAL]  = { .type = NLA_U32 },\n};\n\nstatic const struct nla_policy taprio_tc_policy[TCA_TAPRIO_TC_ENTRY_MAX + 1] = {\n\t[TCA_TAPRIO_TC_ENTRY_INDEX]\t   = { .type = NLA_U32 },\n\t[TCA_TAPRIO_TC_ENTRY_MAX_SDU]\t   = { .type = NLA_U32 },\n\t[TCA_TAPRIO_TC_ENTRY_FP]\t   = NLA_POLICY_RANGE(NLA_U32,\n\t\t\t\t\t\t\t      TC_FP_EXPRESS,\n\t\t\t\t\t\t\t      TC_FP_PREEMPTIBLE),\n};\n\nstatic struct netlink_range_validation_signed taprio_cycle_time_range = {\n\t.min = 0,\n\t.max = INT_MAX,\n};\n\nstatic const struct nla_policy taprio_policy[TCA_TAPRIO_ATTR_MAX + 1] = {\n\t[TCA_TAPRIO_ATTR_PRIOMAP]\t       = {\n\t\t.len = sizeof(struct tc_mqprio_qopt)\n\t},\n\t[TCA_TAPRIO_ATTR_SCHED_ENTRY_LIST]           = { .type = NLA_NESTED },\n\t[TCA_TAPRIO_ATTR_SCHED_BASE_TIME]            = { .type = NLA_S64 },\n\t[TCA_TAPRIO_ATTR_SCHED_SINGLE_ENTRY]         = { .type = NLA_NESTED },\n\t[TCA_TAPRIO_ATTR_SCHED_CLOCKID]              = { .type = NLA_S32 },\n\t[TCA_TAPRIO_ATTR_SCHED_CYCLE_TIME]           =\n\t\tNLA_POLICY_FULL_RANGE_SIGNED(NLA_S64, &taprio_cycle_time_range),\n\t[TCA_TAPRIO_ATTR_SCHED_CYCLE_TIME_EXTENSION] = { .type = NLA_S64 },\n\t[TCA_TAPRIO_ATTR_FLAGS]                      = { .type = NLA_U32 },\n\t[TCA_TAPRIO_ATTR_TXTIME_DELAY]\t\t     = { .type = NLA_U32 },\n\t[TCA_TAPRIO_ATTR_TC_ENTRY]\t\t     = { .type = NLA_NESTED },\n};\n\nstatic int fill_sched_entry(struct taprio_sched *q, struct nlattr **tb,\n\t\t\t    struct sched_entry *entry,\n\t\t\t    struct netlink_ext_ack *extack)\n{\n\tint min_duration = length_to_duration(q, ETH_ZLEN);\n\tu32 interval = 0;\n\n\tif (tb[TCA_TAPRIO_SCHED_ENTRY_CMD])\n\t\tentry->command = nla_get_u8(\n\t\t\ttb[TCA_TAPRIO_SCHED_ENTRY_CMD]);\n\n\tif (tb[TCA_TAPRIO_SCHED_ENTRY_GATE_MASK])\n\t\tentry->gate_mask = nla_get_u32(\n\t\t\ttb[TCA_TAPRIO_SCHED_ENTRY_GATE_MASK]);\n\n\tif (tb[TCA_TAPRIO_SCHED_ENTRY_INTERVAL])\n\t\tinterval = nla_get_u32(\n\t\t\ttb[TCA_TAPRIO_SCHED_ENTRY_INTERVAL]);\n\n\t \n\tif (interval < min_duration) {\n\t\tNL_SET_ERR_MSG(extack, \"Invalid interval for schedule entry\");\n\t\treturn -EINVAL;\n\t}\n\n\tentry->interval = interval;\n\n\treturn 0;\n}\n\nstatic int parse_sched_entry(struct taprio_sched *q, struct nlattr *n,\n\t\t\t     struct sched_entry *entry, int index,\n\t\t\t     struct netlink_ext_ack *extack)\n{\n\tstruct nlattr *tb[TCA_TAPRIO_SCHED_ENTRY_MAX + 1] = { };\n\tint err;\n\n\terr = nla_parse_nested_deprecated(tb, TCA_TAPRIO_SCHED_ENTRY_MAX, n,\n\t\t\t\t\t  entry_policy, NULL);\n\tif (err < 0) {\n\t\tNL_SET_ERR_MSG(extack, \"Could not parse nested entry\");\n\t\treturn -EINVAL;\n\t}\n\n\tentry->index = index;\n\n\treturn fill_sched_entry(q, tb, entry, extack);\n}\n\nstatic int parse_sched_list(struct taprio_sched *q, struct nlattr *list,\n\t\t\t    struct sched_gate_list *sched,\n\t\t\t    struct netlink_ext_ack *extack)\n{\n\tstruct nlattr *n;\n\tint err, rem;\n\tint i = 0;\n\n\tif (!list)\n\t\treturn -EINVAL;\n\n\tnla_for_each_nested(n, list, rem) {\n\t\tstruct sched_entry *entry;\n\n\t\tif (nla_type(n) != TCA_TAPRIO_SCHED_ENTRY) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Attribute is not of type 'entry'\");\n\t\t\tcontinue;\n\t\t}\n\n\t\tentry = kzalloc(sizeof(*entry), GFP_KERNEL);\n\t\tif (!entry) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Not enough memory for entry\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\terr = parse_sched_entry(q, n, entry, i, extack);\n\t\tif (err < 0) {\n\t\t\tkfree(entry);\n\t\t\treturn err;\n\t\t}\n\n\t\tlist_add_tail(&entry->list, &sched->entries);\n\t\ti++;\n\t}\n\n\tsched->num_entries = i;\n\n\treturn i;\n}\n\nstatic int parse_taprio_schedule(struct taprio_sched *q, struct nlattr **tb,\n\t\t\t\t struct sched_gate_list *new,\n\t\t\t\t struct netlink_ext_ack *extack)\n{\n\tint err = 0;\n\n\tif (tb[TCA_TAPRIO_ATTR_SCHED_SINGLE_ENTRY]) {\n\t\tNL_SET_ERR_MSG(extack, \"Adding a single entry is not supported\");\n\t\treturn -ENOTSUPP;\n\t}\n\n\tif (tb[TCA_TAPRIO_ATTR_SCHED_BASE_TIME])\n\t\tnew->base_time = nla_get_s64(tb[TCA_TAPRIO_ATTR_SCHED_BASE_TIME]);\n\n\tif (tb[TCA_TAPRIO_ATTR_SCHED_CYCLE_TIME_EXTENSION])\n\t\tnew->cycle_time_extension = nla_get_s64(tb[TCA_TAPRIO_ATTR_SCHED_CYCLE_TIME_EXTENSION]);\n\n\tif (tb[TCA_TAPRIO_ATTR_SCHED_CYCLE_TIME])\n\t\tnew->cycle_time = nla_get_s64(tb[TCA_TAPRIO_ATTR_SCHED_CYCLE_TIME]);\n\n\tif (tb[TCA_TAPRIO_ATTR_SCHED_ENTRY_LIST])\n\t\terr = parse_sched_list(q, tb[TCA_TAPRIO_ATTR_SCHED_ENTRY_LIST],\n\t\t\t\t       new, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (!new->cycle_time) {\n\t\tstruct sched_entry *entry;\n\t\tktime_t cycle = 0;\n\n\t\tlist_for_each_entry(entry, &new->entries, list)\n\t\t\tcycle = ktime_add_ns(cycle, entry->interval);\n\n\t\tif (!cycle) {\n\t\t\tNL_SET_ERR_MSG(extack, \"'cycle_time' can never be 0\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (cycle < 0 || cycle > INT_MAX) {\n\t\t\tNL_SET_ERR_MSG(extack, \"'cycle_time' is too big\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tnew->cycle_time = cycle;\n\t}\n\n\ttaprio_calculate_gate_durations(q, new);\n\n\treturn 0;\n}\n\nstatic int taprio_parse_mqprio_opt(struct net_device *dev,\n\t\t\t\t   struct tc_mqprio_qopt *qopt,\n\t\t\t\t   struct netlink_ext_ack *extack,\n\t\t\t\t   u32 taprio_flags)\n{\n\tbool allow_overlapping_txqs = TXTIME_ASSIST_IS_ENABLED(taprio_flags);\n\n\tif (!qopt && !dev->num_tc) {\n\t\tNL_SET_ERR_MSG(extack, \"'mqprio' configuration is necessary\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (dev->num_tc)\n\t\treturn 0;\n\n\t \n\tif (qopt->num_tc > dev->num_tx_queues) {\n\t\tNL_SET_ERR_MSG(extack, \"Number of traffic classes is greater than number of HW queues\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\treturn mqprio_validate_qopt(dev, qopt, true, allow_overlapping_txqs,\n\t\t\t\t    extack);\n}\n\nstatic int taprio_get_start_time(struct Qdisc *sch,\n\t\t\t\t struct sched_gate_list *sched,\n\t\t\t\t ktime_t *start)\n{\n\tstruct taprio_sched *q = qdisc_priv(sch);\n\tktime_t now, base, cycle;\n\ts64 n;\n\n\tbase = sched_base_time(sched);\n\tnow = taprio_get_time(q);\n\n\tif (ktime_after(base, now)) {\n\t\t*start = base;\n\t\treturn 0;\n\t}\n\n\tcycle = sched->cycle_time;\n\n\t \n\tif (WARN_ON(!cycle))\n\t\treturn -EFAULT;\n\n\t \n\tn = div64_s64(ktime_sub_ns(now, base), cycle);\n\t*start = ktime_add_ns(base, (n + 1) * cycle);\n\treturn 0;\n}\n\nstatic void setup_first_end_time(struct taprio_sched *q,\n\t\t\t\t struct sched_gate_list *sched, ktime_t base)\n{\n\tstruct net_device *dev = qdisc_dev(q->root);\n\tint num_tc = netdev_get_num_tc(dev);\n\tstruct sched_entry *first;\n\tktime_t cycle;\n\tint tc;\n\n\tfirst = list_first_entry(&sched->entries,\n\t\t\t\t struct sched_entry, list);\n\n\tcycle = sched->cycle_time;\n\n\t \n\tsched->cycle_end_time = ktime_add_ns(base, cycle);\n\n\tfirst->end_time = ktime_add_ns(base, first->interval);\n\ttaprio_set_budgets(q, sched, first);\n\n\tfor (tc = 0; tc < num_tc; tc++) {\n\t\tif (first->gate_duration[tc] == sched->cycle_time)\n\t\t\tfirst->gate_close_time[tc] = KTIME_MAX;\n\t\telse\n\t\t\tfirst->gate_close_time[tc] = ktime_add_ns(base, first->gate_duration[tc]);\n\t}\n\n\trcu_assign_pointer(q->current_entry, NULL);\n}\n\nstatic void taprio_start_sched(struct Qdisc *sch,\n\t\t\t       ktime_t start, struct sched_gate_list *new)\n{\n\tstruct taprio_sched *q = qdisc_priv(sch);\n\tktime_t expires;\n\n\tif (FULL_OFFLOAD_IS_ENABLED(q->flags))\n\t\treturn;\n\n\texpires = hrtimer_get_expires(&q->advance_timer);\n\tif (expires == 0)\n\t\texpires = KTIME_MAX;\n\n\t \n\tstart = min_t(ktime_t, start, expires);\n\n\thrtimer_start(&q->advance_timer, start, HRTIMER_MODE_ABS);\n}\n\nstatic void taprio_set_picos_per_byte(struct net_device *dev,\n\t\t\t\t      struct taprio_sched *q)\n{\n\tstruct ethtool_link_ksettings ecmd;\n\tint speed = SPEED_10;\n\tint picos_per_byte;\n\tint err;\n\n\terr = __ethtool_get_link_ksettings(dev, &ecmd);\n\tif (err < 0)\n\t\tgoto skip;\n\n\tif (ecmd.base.speed && ecmd.base.speed != SPEED_UNKNOWN)\n\t\tspeed = ecmd.base.speed;\n\nskip:\n\tpicos_per_byte = (USEC_PER_SEC * 8) / speed;\n\n\tatomic64_set(&q->picos_per_byte, picos_per_byte);\n\tnetdev_dbg(dev, \"taprio: set %s's picos_per_byte to: %lld, linkspeed: %d\\n\",\n\t\t   dev->name, (long long)atomic64_read(&q->picos_per_byte),\n\t\t   ecmd.base.speed);\n}\n\nstatic int taprio_dev_notifier(struct notifier_block *nb, unsigned long event,\n\t\t\t       void *ptr)\n{\n\tstruct net_device *dev = netdev_notifier_info_to_dev(ptr);\n\tstruct sched_gate_list *oper, *admin;\n\tstruct qdisc_size_table *stab;\n\tstruct taprio_sched *q;\n\n\tASSERT_RTNL();\n\n\tif (event != NETDEV_UP && event != NETDEV_CHANGE)\n\t\treturn NOTIFY_DONE;\n\n\tlist_for_each_entry(q, &taprio_list, taprio_list) {\n\t\tif (dev != qdisc_dev(q->root))\n\t\t\tcontinue;\n\n\t\ttaprio_set_picos_per_byte(dev, q);\n\n\t\tstab = rtnl_dereference(q->root->stab);\n\n\t\toper = rtnl_dereference(q->oper_sched);\n\t\tif (oper)\n\t\t\ttaprio_update_queue_max_sdu(q, oper, stab);\n\n\t\tadmin = rtnl_dereference(q->admin_sched);\n\t\tif (admin)\n\t\t\ttaprio_update_queue_max_sdu(q, admin, stab);\n\n\t\tbreak;\n\t}\n\n\treturn NOTIFY_DONE;\n}\n\nstatic void setup_txtime(struct taprio_sched *q,\n\t\t\t struct sched_gate_list *sched, ktime_t base)\n{\n\tstruct sched_entry *entry;\n\tu64 interval = 0;\n\n\tlist_for_each_entry(entry, &sched->entries, list) {\n\t\tentry->next_txtime = ktime_add_ns(base, interval);\n\t\tinterval += entry->interval;\n\t}\n}\n\nstatic struct tc_taprio_qopt_offload *taprio_offload_alloc(int num_entries)\n{\n\tstruct __tc_taprio_qopt_offload *__offload;\n\n\t__offload = kzalloc(struct_size(__offload, offload.entries, num_entries),\n\t\t\t    GFP_KERNEL);\n\tif (!__offload)\n\t\treturn NULL;\n\n\trefcount_set(&__offload->users, 1);\n\n\treturn &__offload->offload;\n}\n\nstruct tc_taprio_qopt_offload *taprio_offload_get(struct tc_taprio_qopt_offload\n\t\t\t\t\t\t  *offload)\n{\n\tstruct __tc_taprio_qopt_offload *__offload;\n\n\t__offload = container_of(offload, struct __tc_taprio_qopt_offload,\n\t\t\t\t offload);\n\n\trefcount_inc(&__offload->users);\n\n\treturn offload;\n}\nEXPORT_SYMBOL_GPL(taprio_offload_get);\n\nvoid taprio_offload_free(struct tc_taprio_qopt_offload *offload)\n{\n\tstruct __tc_taprio_qopt_offload *__offload;\n\n\t__offload = container_of(offload, struct __tc_taprio_qopt_offload,\n\t\t\t\t offload);\n\n\tif (!refcount_dec_and_test(&__offload->users))\n\t\treturn;\n\n\tkfree(__offload);\n}\nEXPORT_SYMBOL_GPL(taprio_offload_free);\n\n \nstatic void taprio_offload_config_changed(struct taprio_sched *q)\n{\n\tstruct sched_gate_list *oper, *admin;\n\n\toper = rtnl_dereference(q->oper_sched);\n\tadmin = rtnl_dereference(q->admin_sched);\n\n\tswitch_schedules(q, &admin, &oper);\n}\n\nstatic u32 tc_map_to_queue_mask(struct net_device *dev, u32 tc_mask)\n{\n\tu32 i, queue_mask = 0;\n\n\tfor (i = 0; i < dev->num_tc; i++) {\n\t\tu32 offset, count;\n\n\t\tif (!(tc_mask & BIT(i)))\n\t\t\tcontinue;\n\n\t\toffset = dev->tc_to_txq[i].offset;\n\t\tcount = dev->tc_to_txq[i].count;\n\n\t\tqueue_mask |= GENMASK(offset + count - 1, offset);\n\t}\n\n\treturn queue_mask;\n}\n\nstatic void taprio_sched_to_offload(struct net_device *dev,\n\t\t\t\t    struct sched_gate_list *sched,\n\t\t\t\t    struct tc_taprio_qopt_offload *offload,\n\t\t\t\t    const struct tc_taprio_caps *caps)\n{\n\tstruct sched_entry *entry;\n\tint i = 0;\n\n\toffload->base_time = sched->base_time;\n\toffload->cycle_time = sched->cycle_time;\n\toffload->cycle_time_extension = sched->cycle_time_extension;\n\n\tlist_for_each_entry(entry, &sched->entries, list) {\n\t\tstruct tc_taprio_sched_entry *e = &offload->entries[i];\n\n\t\te->command = entry->command;\n\t\te->interval = entry->interval;\n\t\tif (caps->gate_mask_per_txq)\n\t\t\te->gate_mask = tc_map_to_queue_mask(dev,\n\t\t\t\t\t\t\t    entry->gate_mask);\n\t\telse\n\t\t\te->gate_mask = entry->gate_mask;\n\n\t\ti++;\n\t}\n\n\toffload->num_entries = i;\n}\n\nstatic void taprio_detect_broken_mqprio(struct taprio_sched *q)\n{\n\tstruct net_device *dev = qdisc_dev(q->root);\n\tstruct tc_taprio_caps caps;\n\n\tqdisc_offload_query_caps(dev, TC_SETUP_QDISC_TAPRIO,\n\t\t\t\t &caps, sizeof(caps));\n\n\tq->broken_mqprio = caps.broken_mqprio;\n\tif (q->broken_mqprio)\n\t\tstatic_branch_inc(&taprio_have_broken_mqprio);\n\telse\n\t\tstatic_branch_inc(&taprio_have_working_mqprio);\n\n\tq->detected_mqprio = true;\n}\n\nstatic void taprio_cleanup_broken_mqprio(struct taprio_sched *q)\n{\n\tif (!q->detected_mqprio)\n\t\treturn;\n\n\tif (q->broken_mqprio)\n\t\tstatic_branch_dec(&taprio_have_broken_mqprio);\n\telse\n\t\tstatic_branch_dec(&taprio_have_working_mqprio);\n}\n\nstatic int taprio_enable_offload(struct net_device *dev,\n\t\t\t\t struct taprio_sched *q,\n\t\t\t\t struct sched_gate_list *sched,\n\t\t\t\t struct netlink_ext_ack *extack)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tstruct tc_taprio_qopt_offload *offload;\n\tstruct tc_taprio_caps caps;\n\tint tc, err = 0;\n\n\tif (!ops->ndo_setup_tc) {\n\t\tNL_SET_ERR_MSG(extack,\n\t\t\t       \"Device does not support taprio offload\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tqdisc_offload_query_caps(dev, TC_SETUP_QDISC_TAPRIO,\n\t\t\t\t &caps, sizeof(caps));\n\n\tif (!caps.supports_queue_max_sdu) {\n\t\tfor (tc = 0; tc < TC_MAX_QUEUE; tc++) {\n\t\t\tif (q->max_sdu[tc]) {\n\t\t\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t\t\t   \"Device does not handle queueMaxSDU\");\n\t\t\t\treturn -EOPNOTSUPP;\n\t\t\t}\n\t\t}\n\t}\n\n\toffload = taprio_offload_alloc(sched->num_entries);\n\tif (!offload) {\n\t\tNL_SET_ERR_MSG(extack,\n\t\t\t       \"Not enough memory for enabling offload mode\");\n\t\treturn -ENOMEM;\n\t}\n\toffload->cmd = TAPRIO_CMD_REPLACE;\n\toffload->extack = extack;\n\tmqprio_qopt_reconstruct(dev, &offload->mqprio.qopt);\n\toffload->mqprio.extack = extack;\n\ttaprio_sched_to_offload(dev, sched, offload, &caps);\n\tmqprio_fp_to_offload(q->fp, &offload->mqprio);\n\n\tfor (tc = 0; tc < TC_MAX_QUEUE; tc++)\n\t\toffload->max_sdu[tc] = q->max_sdu[tc];\n\n\terr = ops->ndo_setup_tc(dev, TC_SETUP_QDISC_TAPRIO, offload);\n\tif (err < 0) {\n\t\tNL_SET_ERR_MSG_WEAK(extack,\n\t\t\t\t    \"Device failed to setup taprio offload\");\n\t\tgoto done;\n\t}\n\n\tq->offloaded = true;\n\ndone:\n\t \n\toffload->extack = NULL;\n\toffload->mqprio.extack = NULL;\n\ttaprio_offload_free(offload);\n\n\treturn err;\n}\n\nstatic int taprio_disable_offload(struct net_device *dev,\n\t\t\t\t  struct taprio_sched *q,\n\t\t\t\t  struct netlink_ext_ack *extack)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tstruct tc_taprio_qopt_offload *offload;\n\tint err;\n\n\tif (!q->offloaded)\n\t\treturn 0;\n\n\toffload = taprio_offload_alloc(0);\n\tif (!offload) {\n\t\tNL_SET_ERR_MSG(extack,\n\t\t\t       \"Not enough memory to disable offload mode\");\n\t\treturn -ENOMEM;\n\t}\n\toffload->cmd = TAPRIO_CMD_DESTROY;\n\n\terr = ops->ndo_setup_tc(dev, TC_SETUP_QDISC_TAPRIO, offload);\n\tif (err < 0) {\n\t\tNL_SET_ERR_MSG(extack,\n\t\t\t       \"Device failed to disable offload\");\n\t\tgoto out;\n\t}\n\n\tq->offloaded = false;\n\nout:\n\ttaprio_offload_free(offload);\n\n\treturn err;\n}\n\n \nstatic int taprio_parse_clockid(struct Qdisc *sch, struct nlattr **tb,\n\t\t\t\tstruct netlink_ext_ack *extack)\n{\n\tstruct taprio_sched *q = qdisc_priv(sch);\n\tstruct net_device *dev = qdisc_dev(sch);\n\tint err = -EINVAL;\n\n\tif (FULL_OFFLOAD_IS_ENABLED(q->flags)) {\n\t\tconst struct ethtool_ops *ops = dev->ethtool_ops;\n\t\tstruct ethtool_ts_info info = {\n\t\t\t.cmd = ETHTOOL_GET_TS_INFO,\n\t\t\t.phc_index = -1,\n\t\t};\n\n\t\tif (tb[TCA_TAPRIO_ATTR_SCHED_CLOCKID]) {\n\t\t\tNL_SET_ERR_MSG(extack,\n\t\t\t\t       \"The 'clockid' cannot be specified for full offload\");\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (ops && ops->get_ts_info)\n\t\t\terr = ops->get_ts_info(dev, &info);\n\n\t\tif (err || info.phc_index < 0) {\n\t\t\tNL_SET_ERR_MSG(extack,\n\t\t\t\t       \"Device does not have a PTP clock\");\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto out;\n\t\t}\n\t} else if (tb[TCA_TAPRIO_ATTR_SCHED_CLOCKID]) {\n\t\tint clockid = nla_get_s32(tb[TCA_TAPRIO_ATTR_SCHED_CLOCKID]);\n\t\tenum tk_offsets tk_offset;\n\n\t\t \n\t\tif (clockid < 0 ||\n\t\t    (q->clockid != -1 && q->clockid != clockid)) {\n\t\t\tNL_SET_ERR_MSG(extack,\n\t\t\t\t       \"Changing the 'clockid' of a running schedule is not supported\");\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto out;\n\t\t}\n\n\t\tswitch (clockid) {\n\t\tcase CLOCK_REALTIME:\n\t\t\ttk_offset = TK_OFFS_REAL;\n\t\t\tbreak;\n\t\tcase CLOCK_MONOTONIC:\n\t\t\ttk_offset = TK_OFFS_MAX;\n\t\t\tbreak;\n\t\tcase CLOCK_BOOTTIME:\n\t\t\ttk_offset = TK_OFFS_BOOT;\n\t\t\tbreak;\n\t\tcase CLOCK_TAI:\n\t\t\ttk_offset = TK_OFFS_TAI;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tNL_SET_ERR_MSG(extack, \"Invalid 'clockid'\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\t \n\t\tWRITE_ONCE(q->tk_offset, tk_offset);\n\n\t\tq->clockid = clockid;\n\t} else {\n\t\tNL_SET_ERR_MSG(extack, \"Specifying a 'clockid' is mandatory\");\n\t\tgoto out;\n\t}\n\n\t \n\terr = 0;\n\nout:\n\treturn err;\n}\n\nstatic int taprio_parse_tc_entry(struct Qdisc *sch,\n\t\t\t\t struct nlattr *opt,\n\t\t\t\t u32 max_sdu[TC_QOPT_MAX_QUEUE],\n\t\t\t\t u32 fp[TC_QOPT_MAX_QUEUE],\n\t\t\t\t unsigned long *seen_tcs,\n\t\t\t\t struct netlink_ext_ack *extack)\n{\n\tstruct nlattr *tb[TCA_TAPRIO_TC_ENTRY_MAX + 1] = { };\n\tstruct net_device *dev = qdisc_dev(sch);\n\tint err, tc;\n\tu32 val;\n\n\terr = nla_parse_nested(tb, TCA_TAPRIO_TC_ENTRY_MAX, opt,\n\t\t\t       taprio_tc_policy, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (!tb[TCA_TAPRIO_TC_ENTRY_INDEX]) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"TC entry index missing\");\n\t\treturn -EINVAL;\n\t}\n\n\ttc = nla_get_u32(tb[TCA_TAPRIO_TC_ENTRY_INDEX]);\n\tif (tc >= TC_QOPT_MAX_QUEUE) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"TC entry index out of range\");\n\t\treturn -ERANGE;\n\t}\n\n\tif (*seen_tcs & BIT(tc)) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Duplicate TC entry\");\n\t\treturn -EINVAL;\n\t}\n\n\t*seen_tcs |= BIT(tc);\n\n\tif (tb[TCA_TAPRIO_TC_ENTRY_MAX_SDU]) {\n\t\tval = nla_get_u32(tb[TCA_TAPRIO_TC_ENTRY_MAX_SDU]);\n\t\tif (val > dev->max_mtu) {\n\t\t\tNL_SET_ERR_MSG_MOD(extack, \"TC max SDU exceeds device max MTU\");\n\t\t\treturn -ERANGE;\n\t\t}\n\n\t\tmax_sdu[tc] = val;\n\t}\n\n\tif (tb[TCA_TAPRIO_TC_ENTRY_FP])\n\t\tfp[tc] = nla_get_u32(tb[TCA_TAPRIO_TC_ENTRY_FP]);\n\n\treturn 0;\n}\n\nstatic int taprio_parse_tc_entries(struct Qdisc *sch,\n\t\t\t\t   struct nlattr *opt,\n\t\t\t\t   struct netlink_ext_ack *extack)\n{\n\tstruct taprio_sched *q = qdisc_priv(sch);\n\tstruct net_device *dev = qdisc_dev(sch);\n\tu32 max_sdu[TC_QOPT_MAX_QUEUE];\n\tbool have_preemption = false;\n\tunsigned long seen_tcs = 0;\n\tu32 fp[TC_QOPT_MAX_QUEUE];\n\tstruct nlattr *n;\n\tint tc, rem;\n\tint err = 0;\n\n\tfor (tc = 0; tc < TC_QOPT_MAX_QUEUE; tc++) {\n\t\tmax_sdu[tc] = q->max_sdu[tc];\n\t\tfp[tc] = q->fp[tc];\n\t}\n\n\tnla_for_each_nested(n, opt, rem) {\n\t\tif (nla_type(n) != TCA_TAPRIO_ATTR_TC_ENTRY)\n\t\t\tcontinue;\n\n\t\terr = taprio_parse_tc_entry(sch, n, max_sdu, fp, &seen_tcs,\n\t\t\t\t\t    extack);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tfor (tc = 0; tc < TC_QOPT_MAX_QUEUE; tc++) {\n\t\tq->max_sdu[tc] = max_sdu[tc];\n\t\tq->fp[tc] = fp[tc];\n\t\tif (fp[tc] != TC_FP_EXPRESS)\n\t\t\thave_preemption = true;\n\t}\n\n\tif (have_preemption) {\n\t\tif (!FULL_OFFLOAD_IS_ENABLED(q->flags)) {\n\t\t\tNL_SET_ERR_MSG(extack,\n\t\t\t\t       \"Preemption only supported with full offload\");\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ethtool_dev_mm_supported(dev)) {\n\t\t\tNL_SET_ERR_MSG(extack,\n\t\t\t\t       \"Device does not support preemption\");\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\t}\n\n\treturn err;\n}\n\nstatic int taprio_mqprio_cmp(const struct net_device *dev,\n\t\t\t     const struct tc_mqprio_qopt *mqprio)\n{\n\tint i;\n\n\tif (!mqprio || mqprio->num_tc != dev->num_tc)\n\t\treturn -1;\n\n\tfor (i = 0; i < mqprio->num_tc; i++)\n\t\tif (dev->tc_to_txq[i].count != mqprio->count[i] ||\n\t\t    dev->tc_to_txq[i].offset != mqprio->offset[i])\n\t\t\treturn -1;\n\n\tfor (i = 0; i <= TC_BITMASK; i++)\n\t\tif (dev->prio_tc_map[i] != mqprio->prio_tc_map[i])\n\t\t\treturn -1;\n\n\treturn 0;\n}\n\n \nstatic int taprio_new_flags(const struct nlattr *attr, u32 old,\n\t\t\t    struct netlink_ext_ack *extack)\n{\n\tu32 new = 0;\n\n\tif (attr)\n\t\tnew = nla_get_u32(attr);\n\n\tif (old != TAPRIO_FLAGS_INVALID && old != new) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Changing 'flags' of a running schedule is not supported\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (!taprio_flags_valid(new)) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Specified 'flags' are not valid\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn new;\n}\n\nstatic int taprio_change(struct Qdisc *sch, struct nlattr *opt,\n\t\t\t struct netlink_ext_ack *extack)\n{\n\tstruct qdisc_size_table *stab = rtnl_dereference(sch->stab);\n\tstruct nlattr *tb[TCA_TAPRIO_ATTR_MAX + 1] = { };\n\tstruct sched_gate_list *oper, *admin, *new_admin;\n\tstruct taprio_sched *q = qdisc_priv(sch);\n\tstruct net_device *dev = qdisc_dev(sch);\n\tstruct tc_mqprio_qopt *mqprio = NULL;\n\tunsigned long flags;\n\tktime_t start;\n\tint i, err;\n\n\terr = nla_parse_nested_deprecated(tb, TCA_TAPRIO_ATTR_MAX, opt,\n\t\t\t\t\t  taprio_policy, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[TCA_TAPRIO_ATTR_PRIOMAP])\n\t\tmqprio = nla_data(tb[TCA_TAPRIO_ATTR_PRIOMAP]);\n\n\terr = taprio_new_flags(tb[TCA_TAPRIO_ATTR_FLAGS],\n\t\t\t       q->flags, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tq->flags = err;\n\n\terr = taprio_parse_mqprio_opt(dev, mqprio, extack, q->flags);\n\tif (err < 0)\n\t\treturn err;\n\n\terr = taprio_parse_tc_entries(sch, opt, extack);\n\tif (err)\n\t\treturn err;\n\n\tnew_admin = kzalloc(sizeof(*new_admin), GFP_KERNEL);\n\tif (!new_admin) {\n\t\tNL_SET_ERR_MSG(extack, \"Not enough memory for a new schedule\");\n\t\treturn -ENOMEM;\n\t}\n\tINIT_LIST_HEAD(&new_admin->entries);\n\n\toper = rtnl_dereference(q->oper_sched);\n\tadmin = rtnl_dereference(q->admin_sched);\n\n\t \n\tif (!taprio_mqprio_cmp(dev, mqprio))\n\t\tmqprio = NULL;\n\n\tif (mqprio && (oper || admin)) {\n\t\tNL_SET_ERR_MSG(extack, \"Changing the traffic mapping of a running schedule is not supported\");\n\t\terr = -ENOTSUPP;\n\t\tgoto free_sched;\n\t}\n\n\tif (mqprio) {\n\t\terr = netdev_set_num_tc(dev, mqprio->num_tc);\n\t\tif (err)\n\t\t\tgoto free_sched;\n\t\tfor (i = 0; i < mqprio->num_tc; i++) {\n\t\t\tnetdev_set_tc_queue(dev, i,\n\t\t\t\t\t    mqprio->count[i],\n\t\t\t\t\t    mqprio->offset[i]);\n\t\t\tq->cur_txq[i] = mqprio->offset[i];\n\t\t}\n\n\t\t \n\t\tfor (i = 0; i <= TC_BITMASK; i++)\n\t\t\tnetdev_set_prio_tc_map(dev, i,\n\t\t\t\t\t       mqprio->prio_tc_map[i]);\n\t}\n\n\terr = parse_taprio_schedule(q, tb, new_admin, extack);\n\tif (err < 0)\n\t\tgoto free_sched;\n\n\tif (new_admin->num_entries == 0) {\n\t\tNL_SET_ERR_MSG(extack, \"There should be at least one entry in the schedule\");\n\t\terr = -EINVAL;\n\t\tgoto free_sched;\n\t}\n\n\terr = taprio_parse_clockid(sch, tb, extack);\n\tif (err < 0)\n\t\tgoto free_sched;\n\n\ttaprio_set_picos_per_byte(dev, q);\n\ttaprio_update_queue_max_sdu(q, new_admin, stab);\n\n\tif (FULL_OFFLOAD_IS_ENABLED(q->flags))\n\t\terr = taprio_enable_offload(dev, q, new_admin, extack);\n\telse\n\t\terr = taprio_disable_offload(dev, q, extack);\n\tif (err)\n\t\tgoto free_sched;\n\n\t \n\tspin_lock_bh(qdisc_lock(sch));\n\n\tif (tb[TCA_TAPRIO_ATTR_TXTIME_DELAY]) {\n\t\tif (!TXTIME_ASSIST_IS_ENABLED(q->flags)) {\n\t\t\tNL_SET_ERR_MSG_MOD(extack, \"txtime-delay can only be set when txtime-assist mode is enabled\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto unlock;\n\t\t}\n\n\t\tq->txtime_delay = nla_get_u32(tb[TCA_TAPRIO_ATTR_TXTIME_DELAY]);\n\t}\n\n\tif (!TXTIME_ASSIST_IS_ENABLED(q->flags) &&\n\t    !FULL_OFFLOAD_IS_ENABLED(q->flags) &&\n\t    !hrtimer_active(&q->advance_timer)) {\n\t\thrtimer_init(&q->advance_timer, q->clockid, HRTIMER_MODE_ABS);\n\t\tq->advance_timer.function = advance_sched;\n\t}\n\n\terr = taprio_get_start_time(sch, new_admin, &start);\n\tif (err < 0) {\n\t\tNL_SET_ERR_MSG(extack, \"Internal error: failed get start time\");\n\t\tgoto unlock;\n\t}\n\n\tsetup_txtime(q, new_admin, start);\n\n\tif (TXTIME_ASSIST_IS_ENABLED(q->flags)) {\n\t\tif (!oper) {\n\t\t\trcu_assign_pointer(q->oper_sched, new_admin);\n\t\t\terr = 0;\n\t\t\tnew_admin = NULL;\n\t\t\tgoto unlock;\n\t\t}\n\n\t\trcu_assign_pointer(q->admin_sched, new_admin);\n\t\tif (admin)\n\t\t\tcall_rcu(&admin->rcu, taprio_free_sched_cb);\n\t} else {\n\t\tsetup_first_end_time(q, new_admin, start);\n\n\t\t \n\t\tspin_lock_irqsave(&q->current_entry_lock, flags);\n\n\t\ttaprio_start_sched(sch, start, new_admin);\n\n\t\trcu_assign_pointer(q->admin_sched, new_admin);\n\t\tif (admin)\n\t\t\tcall_rcu(&admin->rcu, taprio_free_sched_cb);\n\n\t\tspin_unlock_irqrestore(&q->current_entry_lock, flags);\n\n\t\tif (FULL_OFFLOAD_IS_ENABLED(q->flags))\n\t\t\ttaprio_offload_config_changed(q);\n\t}\n\n\tnew_admin = NULL;\n\terr = 0;\n\n\tif (!stab)\n\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t   \"Size table not specified, frame length estimations may be inaccurate\");\n\nunlock:\n\tspin_unlock_bh(qdisc_lock(sch));\n\nfree_sched:\n\tif (new_admin)\n\t\tcall_rcu(&new_admin->rcu, taprio_free_sched_cb);\n\n\treturn err;\n}\n\nstatic void taprio_reset(struct Qdisc *sch)\n{\n\tstruct taprio_sched *q = qdisc_priv(sch);\n\tstruct net_device *dev = qdisc_dev(sch);\n\tint i;\n\n\thrtimer_cancel(&q->advance_timer);\n\n\tif (q->qdiscs) {\n\t\tfor (i = 0; i < dev->num_tx_queues; i++)\n\t\t\tif (q->qdiscs[i])\n\t\t\t\tqdisc_reset(q->qdiscs[i]);\n\t}\n}\n\nstatic void taprio_destroy(struct Qdisc *sch)\n{\n\tstruct taprio_sched *q = qdisc_priv(sch);\n\tstruct net_device *dev = qdisc_dev(sch);\n\tstruct sched_gate_list *oper, *admin;\n\tunsigned int i;\n\n\tlist_del(&q->taprio_list);\n\n\t \n\thrtimer_cancel(&q->advance_timer);\n\tqdisc_synchronize(sch);\n\n\ttaprio_disable_offload(dev, q, NULL);\n\n\tif (q->qdiscs) {\n\t\tfor (i = 0; i < dev->num_tx_queues; i++)\n\t\t\tqdisc_put(q->qdiscs[i]);\n\n\t\tkfree(q->qdiscs);\n\t}\n\tq->qdiscs = NULL;\n\n\tnetdev_reset_tc(dev);\n\n\toper = rtnl_dereference(q->oper_sched);\n\tadmin = rtnl_dereference(q->admin_sched);\n\n\tif (oper)\n\t\tcall_rcu(&oper->rcu, taprio_free_sched_cb);\n\n\tif (admin)\n\t\tcall_rcu(&admin->rcu, taprio_free_sched_cb);\n\n\ttaprio_cleanup_broken_mqprio(q);\n}\n\nstatic int taprio_init(struct Qdisc *sch, struct nlattr *opt,\n\t\t       struct netlink_ext_ack *extack)\n{\n\tstruct taprio_sched *q = qdisc_priv(sch);\n\tstruct net_device *dev = qdisc_dev(sch);\n\tint i, tc;\n\n\tspin_lock_init(&q->current_entry_lock);\n\n\thrtimer_init(&q->advance_timer, CLOCK_TAI, HRTIMER_MODE_ABS);\n\tq->advance_timer.function = advance_sched;\n\n\tq->root = sch;\n\n\t \n\tq->clockid = -1;\n\tq->flags = TAPRIO_FLAGS_INVALID;\n\n\tlist_add(&q->taprio_list, &taprio_list);\n\n\tif (sch->parent != TC_H_ROOT) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Can only be attached as root qdisc\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (!netif_is_multiqueue(dev)) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Multi-queue device is required\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tq->qdiscs = kcalloc(dev->num_tx_queues, sizeof(q->qdiscs[0]),\n\t\t\t    GFP_KERNEL);\n\tif (!q->qdiscs)\n\t\treturn -ENOMEM;\n\n\tif (!opt)\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *dev_queue;\n\t\tstruct Qdisc *qdisc;\n\n\t\tdev_queue = netdev_get_tx_queue(dev, i);\n\t\tqdisc = qdisc_create_dflt(dev_queue,\n\t\t\t\t\t  &pfifo_qdisc_ops,\n\t\t\t\t\t  TC_H_MAKE(TC_H_MAJ(sch->handle),\n\t\t\t\t\t\t    TC_H_MIN(i + 1)),\n\t\t\t\t\t  extack);\n\t\tif (!qdisc)\n\t\t\treturn -ENOMEM;\n\n\t\tif (i < dev->real_num_tx_queues)\n\t\t\tqdisc_hash_add(qdisc, false);\n\n\t\tq->qdiscs[i] = qdisc;\n\t}\n\n\tfor (tc = 0; tc < TC_QOPT_MAX_QUEUE; tc++)\n\t\tq->fp[tc] = TC_FP_EXPRESS;\n\n\ttaprio_detect_broken_mqprio(q);\n\n\treturn taprio_change(sch, opt, extack);\n}\n\nstatic void taprio_attach(struct Qdisc *sch)\n{\n\tstruct taprio_sched *q = qdisc_priv(sch);\n\tstruct net_device *dev = qdisc_dev(sch);\n\tunsigned int ntx;\n\n\t \n\tfor (ntx = 0; ntx < dev->num_tx_queues; ntx++) {\n\t\tstruct netdev_queue *dev_queue = netdev_get_tx_queue(dev, ntx);\n\t\tstruct Qdisc *old, *dev_queue_qdisc;\n\n\t\tif (FULL_OFFLOAD_IS_ENABLED(q->flags)) {\n\t\t\tstruct Qdisc *qdisc = q->qdiscs[ntx];\n\n\t\t\t \n\t\t\tqdisc->flags |= TCQ_F_ONETXQUEUE | TCQ_F_NOPARENT;\n\t\t\tdev_queue_qdisc = qdisc;\n\t\t} else {\n\t\t\t \n\t\t\tdev_queue_qdisc = sch;\n\t\t}\n\t\told = dev_graft_qdisc(dev_queue, dev_queue_qdisc);\n\t\t \n\t\tqdisc_refcount_inc(dev_queue_qdisc);\n\t\tif (old)\n\t\t\tqdisc_put(old);\n\t}\n}\n\nstatic struct netdev_queue *taprio_queue_get(struct Qdisc *sch,\n\t\t\t\t\t     unsigned long cl)\n{\n\tstruct net_device *dev = qdisc_dev(sch);\n\tunsigned long ntx = cl - 1;\n\n\tif (ntx >= dev->num_tx_queues)\n\t\treturn NULL;\n\n\treturn netdev_get_tx_queue(dev, ntx);\n}\n\nstatic int taprio_graft(struct Qdisc *sch, unsigned long cl,\n\t\t\tstruct Qdisc *new, struct Qdisc **old,\n\t\t\tstruct netlink_ext_ack *extack)\n{\n\tstruct taprio_sched *q = qdisc_priv(sch);\n\tstruct net_device *dev = qdisc_dev(sch);\n\tstruct netdev_queue *dev_queue = taprio_queue_get(sch, cl);\n\n\tif (!dev_queue)\n\t\treturn -EINVAL;\n\n\tif (dev->flags & IFF_UP)\n\t\tdev_deactivate(dev);\n\n\t \n\t*old = q->qdiscs[cl - 1];\n\tif (FULL_OFFLOAD_IS_ENABLED(q->flags)) {\n\t\tWARN_ON_ONCE(dev_graft_qdisc(dev_queue, new) != *old);\n\t\tif (new)\n\t\t\tqdisc_refcount_inc(new);\n\t\tif (*old)\n\t\t\tqdisc_put(*old);\n\t}\n\n\tq->qdiscs[cl - 1] = new;\n\tif (new)\n\t\tnew->flags |= TCQ_F_ONETXQUEUE | TCQ_F_NOPARENT;\n\n\tif (dev->flags & IFF_UP)\n\t\tdev_activate(dev);\n\n\treturn 0;\n}\n\nstatic int dump_entry(struct sk_buff *msg,\n\t\t      const struct sched_entry *entry)\n{\n\tstruct nlattr *item;\n\n\titem = nla_nest_start_noflag(msg, TCA_TAPRIO_SCHED_ENTRY);\n\tif (!item)\n\t\treturn -ENOSPC;\n\n\tif (nla_put_u32(msg, TCA_TAPRIO_SCHED_ENTRY_INDEX, entry->index))\n\t\tgoto nla_put_failure;\n\n\tif (nla_put_u8(msg, TCA_TAPRIO_SCHED_ENTRY_CMD, entry->command))\n\t\tgoto nla_put_failure;\n\n\tif (nla_put_u32(msg, TCA_TAPRIO_SCHED_ENTRY_GATE_MASK,\n\t\t\tentry->gate_mask))\n\t\tgoto nla_put_failure;\n\n\tif (nla_put_u32(msg, TCA_TAPRIO_SCHED_ENTRY_INTERVAL,\n\t\t\tentry->interval))\n\t\tgoto nla_put_failure;\n\n\treturn nla_nest_end(msg, item);\n\nnla_put_failure:\n\tnla_nest_cancel(msg, item);\n\treturn -1;\n}\n\nstatic int dump_schedule(struct sk_buff *msg,\n\t\t\t const struct sched_gate_list *root)\n{\n\tstruct nlattr *entry_list;\n\tstruct sched_entry *entry;\n\n\tif (nla_put_s64(msg, TCA_TAPRIO_ATTR_SCHED_BASE_TIME,\n\t\t\troot->base_time, TCA_TAPRIO_PAD))\n\t\treturn -1;\n\n\tif (nla_put_s64(msg, TCA_TAPRIO_ATTR_SCHED_CYCLE_TIME,\n\t\t\troot->cycle_time, TCA_TAPRIO_PAD))\n\t\treturn -1;\n\n\tif (nla_put_s64(msg, TCA_TAPRIO_ATTR_SCHED_CYCLE_TIME_EXTENSION,\n\t\t\troot->cycle_time_extension, TCA_TAPRIO_PAD))\n\t\treturn -1;\n\n\tentry_list = nla_nest_start_noflag(msg,\n\t\t\t\t\t   TCA_TAPRIO_ATTR_SCHED_ENTRY_LIST);\n\tif (!entry_list)\n\t\tgoto error_nest;\n\n\tlist_for_each_entry(entry, &root->entries, list) {\n\t\tif (dump_entry(msg, entry) < 0)\n\t\t\tgoto error_nest;\n\t}\n\n\tnla_nest_end(msg, entry_list);\n\treturn 0;\n\nerror_nest:\n\tnla_nest_cancel(msg, entry_list);\n\treturn -1;\n}\n\nstatic int taprio_dump_tc_entries(struct sk_buff *skb,\n\t\t\t\t  struct taprio_sched *q,\n\t\t\t\t  struct sched_gate_list *sched)\n{\n\tstruct nlattr *n;\n\tint tc;\n\n\tfor (tc = 0; tc < TC_MAX_QUEUE; tc++) {\n\t\tn = nla_nest_start(skb, TCA_TAPRIO_ATTR_TC_ENTRY);\n\t\tif (!n)\n\t\t\treturn -EMSGSIZE;\n\n\t\tif (nla_put_u32(skb, TCA_TAPRIO_TC_ENTRY_INDEX, tc))\n\t\t\tgoto nla_put_failure;\n\n\t\tif (nla_put_u32(skb, TCA_TAPRIO_TC_ENTRY_MAX_SDU,\n\t\t\t\tsched->max_sdu[tc]))\n\t\t\tgoto nla_put_failure;\n\n\t\tif (nla_put_u32(skb, TCA_TAPRIO_TC_ENTRY_FP, q->fp[tc]))\n\t\t\tgoto nla_put_failure;\n\n\t\tnla_nest_end(skb, n);\n\t}\n\n\treturn 0;\n\nnla_put_failure:\n\tnla_nest_cancel(skb, n);\n\treturn -EMSGSIZE;\n}\n\nstatic int taprio_put_stat(struct sk_buff *skb, u64 val, u16 attrtype)\n{\n\tif (val == TAPRIO_STAT_NOT_SET)\n\t\treturn 0;\n\tif (nla_put_u64_64bit(skb, attrtype, val, TCA_TAPRIO_OFFLOAD_STATS_PAD))\n\t\treturn -EMSGSIZE;\n\treturn 0;\n}\n\nstatic int taprio_dump_xstats(struct Qdisc *sch, struct gnet_dump *d,\n\t\t\t      struct tc_taprio_qopt_offload *offload,\n\t\t\t      struct tc_taprio_qopt_stats *stats)\n{\n\tstruct net_device *dev = qdisc_dev(sch);\n\tconst struct net_device_ops *ops;\n\tstruct sk_buff *skb = d->skb;\n\tstruct nlattr *xstats;\n\tint err;\n\n\tops = qdisc_dev(sch)->netdev_ops;\n\n\t \n\tif (!ops->ndo_setup_tc)\n\t\treturn 0;\n\n\tmemset(stats, 0xff, sizeof(*stats));\n\n\terr = ops->ndo_setup_tc(dev, TC_SETUP_QDISC_TAPRIO, offload);\n\tif (err == -EOPNOTSUPP)\n\t\treturn 0;\n\tif (err)\n\t\treturn err;\n\n\txstats = nla_nest_start(skb, TCA_STATS_APP);\n\tif (!xstats)\n\t\tgoto err;\n\n\tif (taprio_put_stat(skb, stats->window_drops,\n\t\t\t    TCA_TAPRIO_OFFLOAD_STATS_WINDOW_DROPS) ||\n\t    taprio_put_stat(skb, stats->tx_overruns,\n\t\t\t    TCA_TAPRIO_OFFLOAD_STATS_TX_OVERRUNS))\n\t\tgoto err_cancel;\n\n\tnla_nest_end(skb, xstats);\n\n\treturn 0;\n\nerr_cancel:\n\tnla_nest_cancel(skb, xstats);\nerr:\n\treturn -EMSGSIZE;\n}\n\nstatic int taprio_dump_stats(struct Qdisc *sch, struct gnet_dump *d)\n{\n\tstruct tc_taprio_qopt_offload offload = {\n\t\t.cmd = TAPRIO_CMD_STATS,\n\t};\n\n\treturn taprio_dump_xstats(sch, d, &offload, &offload.stats);\n}\n\nstatic int taprio_dump(struct Qdisc *sch, struct sk_buff *skb)\n{\n\tstruct taprio_sched *q = qdisc_priv(sch);\n\tstruct net_device *dev = qdisc_dev(sch);\n\tstruct sched_gate_list *oper, *admin;\n\tstruct tc_mqprio_qopt opt = { 0 };\n\tstruct nlattr *nest, *sched_nest;\n\n\toper = rtnl_dereference(q->oper_sched);\n\tadmin = rtnl_dereference(q->admin_sched);\n\n\tmqprio_qopt_reconstruct(dev, &opt);\n\n\tnest = nla_nest_start_noflag(skb, TCA_OPTIONS);\n\tif (!nest)\n\t\tgoto start_error;\n\n\tif (nla_put(skb, TCA_TAPRIO_ATTR_PRIOMAP, sizeof(opt), &opt))\n\t\tgoto options_error;\n\n\tif (!FULL_OFFLOAD_IS_ENABLED(q->flags) &&\n\t    nla_put_s32(skb, TCA_TAPRIO_ATTR_SCHED_CLOCKID, q->clockid))\n\t\tgoto options_error;\n\n\tif (q->flags && nla_put_u32(skb, TCA_TAPRIO_ATTR_FLAGS, q->flags))\n\t\tgoto options_error;\n\n\tif (q->txtime_delay &&\n\t    nla_put_u32(skb, TCA_TAPRIO_ATTR_TXTIME_DELAY, q->txtime_delay))\n\t\tgoto options_error;\n\n\tif (oper && taprio_dump_tc_entries(skb, q, oper))\n\t\tgoto options_error;\n\n\tif (oper && dump_schedule(skb, oper))\n\t\tgoto options_error;\n\n\tif (!admin)\n\t\tgoto done;\n\n\tsched_nest = nla_nest_start_noflag(skb, TCA_TAPRIO_ATTR_ADMIN_SCHED);\n\tif (!sched_nest)\n\t\tgoto options_error;\n\n\tif (dump_schedule(skb, admin))\n\t\tgoto admin_error;\n\n\tnla_nest_end(skb, sched_nest);\n\ndone:\n\treturn nla_nest_end(skb, nest);\n\nadmin_error:\n\tnla_nest_cancel(skb, sched_nest);\n\noptions_error:\n\tnla_nest_cancel(skb, nest);\n\nstart_error:\n\treturn -ENOSPC;\n}\n\nstatic struct Qdisc *taprio_leaf(struct Qdisc *sch, unsigned long cl)\n{\n\tstruct taprio_sched *q = qdisc_priv(sch);\n\tstruct net_device *dev = qdisc_dev(sch);\n\tunsigned int ntx = cl - 1;\n\n\tif (ntx >= dev->num_tx_queues)\n\t\treturn NULL;\n\n\treturn q->qdiscs[ntx];\n}\n\nstatic unsigned long taprio_find(struct Qdisc *sch, u32 classid)\n{\n\tunsigned int ntx = TC_H_MIN(classid);\n\n\tif (!taprio_queue_get(sch, ntx))\n\t\treturn 0;\n\treturn ntx;\n}\n\nstatic int taprio_dump_class(struct Qdisc *sch, unsigned long cl,\n\t\t\t     struct sk_buff *skb, struct tcmsg *tcm)\n{\n\tstruct Qdisc *child = taprio_leaf(sch, cl);\n\n\ttcm->tcm_parent = TC_H_ROOT;\n\ttcm->tcm_handle |= TC_H_MIN(cl);\n\ttcm->tcm_info = child->handle;\n\n\treturn 0;\n}\n\nstatic int taprio_dump_class_stats(struct Qdisc *sch, unsigned long cl,\n\t\t\t\t   struct gnet_dump *d)\n\t__releases(d->lock)\n\t__acquires(d->lock)\n{\n\tstruct Qdisc *child = taprio_leaf(sch, cl);\n\tstruct tc_taprio_qopt_offload offload = {\n\t\t.cmd = TAPRIO_CMD_QUEUE_STATS,\n\t\t.queue_stats = {\n\t\t\t.queue = cl - 1,\n\t\t},\n\t};\n\n\tif (gnet_stats_copy_basic(d, NULL, &child->bstats, true) < 0 ||\n\t    qdisc_qstats_copy(d, child) < 0)\n\t\treturn -1;\n\n\treturn taprio_dump_xstats(sch, d, &offload, &offload.queue_stats.stats);\n}\n\nstatic void taprio_walk(struct Qdisc *sch, struct qdisc_walker *arg)\n{\n\tstruct net_device *dev = qdisc_dev(sch);\n\tunsigned long ntx;\n\n\tif (arg->stop)\n\t\treturn;\n\n\targ->count = arg->skip;\n\tfor (ntx = arg->skip; ntx < dev->num_tx_queues; ntx++) {\n\t\tif (!tc_qdisc_stats_dump(sch, ntx + 1, arg))\n\t\t\tbreak;\n\t}\n}\n\nstatic struct netdev_queue *taprio_select_queue(struct Qdisc *sch,\n\t\t\t\t\t\tstruct tcmsg *tcm)\n{\n\treturn taprio_queue_get(sch, TC_H_MIN(tcm->tcm_parent));\n}\n\nstatic const struct Qdisc_class_ops taprio_class_ops = {\n\t.graft\t\t= taprio_graft,\n\t.leaf\t\t= taprio_leaf,\n\t.find\t\t= taprio_find,\n\t.walk\t\t= taprio_walk,\n\t.dump\t\t= taprio_dump_class,\n\t.dump_stats\t= taprio_dump_class_stats,\n\t.select_queue\t= taprio_select_queue,\n};\n\nstatic struct Qdisc_ops taprio_qdisc_ops __read_mostly = {\n\t.cl_ops\t\t= &taprio_class_ops,\n\t.id\t\t= \"taprio\",\n\t.priv_size\t= sizeof(struct taprio_sched),\n\t.init\t\t= taprio_init,\n\t.change\t\t= taprio_change,\n\t.destroy\t= taprio_destroy,\n\t.reset\t\t= taprio_reset,\n\t.attach\t\t= taprio_attach,\n\t.peek\t\t= taprio_peek,\n\t.dequeue\t= taprio_dequeue,\n\t.enqueue\t= taprio_enqueue,\n\t.dump\t\t= taprio_dump,\n\t.dump_stats\t= taprio_dump_stats,\n\t.owner\t\t= THIS_MODULE,\n};\n\nstatic struct notifier_block taprio_device_notifier = {\n\t.notifier_call = taprio_dev_notifier,\n};\n\nstatic int __init taprio_module_init(void)\n{\n\tint err = register_netdevice_notifier(&taprio_device_notifier);\n\n\tif (err)\n\t\treturn err;\n\n\treturn register_qdisc(&taprio_qdisc_ops);\n}\n\nstatic void __exit taprio_module_exit(void)\n{\n\tunregister_qdisc(&taprio_qdisc_ops);\n\tunregister_netdevice_notifier(&taprio_device_notifier);\n}\n\nmodule_init(taprio_module_init);\nmodule_exit(taprio_module_exit);\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}