{
  "module_name": "sch_gred.c",
  "hash_id": "6426ab3970e1f1d27791e7ae6cef477a5ce3e262006a8eb42d03cffb4f688f3d",
  "original_prompt": "Ingested from linux-6.6.14/net/sched/sch_gred.c",
  "human_readable_source": "\n \n\n#include <linux/slab.h>\n#include <linux/module.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/skbuff.h>\n#include <net/pkt_cls.h>\n#include <net/pkt_sched.h>\n#include <net/red.h>\n\n#define GRED_DEF_PRIO (MAX_DPs / 2)\n#define GRED_VQ_MASK (MAX_DPs - 1)\n\n#define GRED_VQ_RED_FLAGS\t(TC_RED_ECN | TC_RED_HARDDROP)\n\nstruct gred_sched_data;\nstruct gred_sched;\n\nstruct gred_sched_data {\n\tu32\t\tlimit;\t\t \n\tu32\t\tDP;\t\t \n\tu32\t\tred_flags;\t \n\tu64\t\tbytesin;\t \n\tu32\t\tpacketsin;\t \n\tu32\t\tbacklog;\t \n\tu8\t\tprio;\t\t \n\n\tstruct red_parms parms;\n\tstruct red_vars  vars;\n\tstruct red_stats stats;\n};\n\nenum {\n\tGRED_WRED_MODE = 1,\n\tGRED_RIO_MODE,\n};\n\nstruct gred_sched {\n\tstruct gred_sched_data *tab[MAX_DPs];\n\tunsigned long\tflags;\n\tu32\t\tred_flags;\n\tu32 \t\tDPs;\n\tu32 \t\tdef;\n\tstruct red_vars wred_set;\n\tstruct tc_gred_qopt_offload *opt;\n};\n\nstatic inline int gred_wred_mode(struct gred_sched *table)\n{\n\treturn test_bit(GRED_WRED_MODE, &table->flags);\n}\n\nstatic inline void gred_enable_wred_mode(struct gred_sched *table)\n{\n\t__set_bit(GRED_WRED_MODE, &table->flags);\n}\n\nstatic inline void gred_disable_wred_mode(struct gred_sched *table)\n{\n\t__clear_bit(GRED_WRED_MODE, &table->flags);\n}\n\nstatic inline int gred_rio_mode(struct gred_sched *table)\n{\n\treturn test_bit(GRED_RIO_MODE, &table->flags);\n}\n\nstatic inline void gred_enable_rio_mode(struct gred_sched *table)\n{\n\t__set_bit(GRED_RIO_MODE, &table->flags);\n}\n\nstatic inline void gred_disable_rio_mode(struct gred_sched *table)\n{\n\t__clear_bit(GRED_RIO_MODE, &table->flags);\n}\n\nstatic inline int gred_wred_mode_check(struct Qdisc *sch)\n{\n\tstruct gred_sched *table = qdisc_priv(sch);\n\tint i;\n\n\t \n\tfor (i = 0; i < table->DPs; i++) {\n\t\tstruct gred_sched_data *q = table->tab[i];\n\t\tint n;\n\n\t\tif (q == NULL)\n\t\t\tcontinue;\n\n\t\tfor (n = i + 1; n < table->DPs; n++)\n\t\t\tif (table->tab[n] && table->tab[n]->prio == q->prio)\n\t\t\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic inline unsigned int gred_backlog(struct gred_sched *table,\n\t\t\t\t\tstruct gred_sched_data *q,\n\t\t\t\t\tstruct Qdisc *sch)\n{\n\tif (gred_wred_mode(table))\n\t\treturn sch->qstats.backlog;\n\telse\n\t\treturn q->backlog;\n}\n\nstatic inline u16 tc_index_to_dp(struct sk_buff *skb)\n{\n\treturn skb->tc_index & GRED_VQ_MASK;\n}\n\nstatic inline void gred_load_wred_set(const struct gred_sched *table,\n\t\t\t\t      struct gred_sched_data *q)\n{\n\tq->vars.qavg = table->wred_set.qavg;\n\tq->vars.qidlestart = table->wred_set.qidlestart;\n}\n\nstatic inline void gred_store_wred_set(struct gred_sched *table,\n\t\t\t\t       struct gred_sched_data *q)\n{\n\ttable->wred_set.qavg = q->vars.qavg;\n\ttable->wred_set.qidlestart = q->vars.qidlestart;\n}\n\nstatic int gred_use_ecn(struct gred_sched_data *q)\n{\n\treturn q->red_flags & TC_RED_ECN;\n}\n\nstatic int gred_use_harddrop(struct gred_sched_data *q)\n{\n\treturn q->red_flags & TC_RED_HARDDROP;\n}\n\nstatic bool gred_per_vq_red_flags_used(struct gred_sched *table)\n{\n\tunsigned int i;\n\n\t \n\tif (table->red_flags)\n\t\treturn false;\n\tfor (i = 0; i < MAX_DPs; i++)\n\t\tif (table->tab[i] && table->tab[i]->red_flags)\n\t\t\treturn true;\n\treturn false;\n}\n\nstatic int gred_enqueue(struct sk_buff *skb, struct Qdisc *sch,\n\t\t\tstruct sk_buff **to_free)\n{\n\tstruct gred_sched_data *q = NULL;\n\tstruct gred_sched *t = qdisc_priv(sch);\n\tunsigned long qavg = 0;\n\tu16 dp = tc_index_to_dp(skb);\n\n\tif (dp >= t->DPs || (q = t->tab[dp]) == NULL) {\n\t\tdp = t->def;\n\n\t\tq = t->tab[dp];\n\t\tif (!q) {\n\t\t\t \n\t\t\tif (likely(sch->qstats.backlog + qdisc_pkt_len(skb) <=\n\t\t\t\t\tsch->limit))\n\t\t\t\treturn qdisc_enqueue_tail(skb, sch);\n\t\t\telse\n\t\t\t\tgoto drop;\n\t\t}\n\n\t\t \n\t\tskb->tc_index = (skb->tc_index & ~GRED_VQ_MASK) | dp;\n\t}\n\n\t \n\tif (!gred_wred_mode(t) && gred_rio_mode(t)) {\n\t\tint i;\n\n\t\tfor (i = 0; i < t->DPs; i++) {\n\t\t\tif (t->tab[i] && t->tab[i]->prio < q->prio &&\n\t\t\t    !red_is_idling(&t->tab[i]->vars))\n\t\t\t\tqavg += t->tab[i]->vars.qavg;\n\t\t}\n\n\t}\n\n\tq->packetsin++;\n\tq->bytesin += qdisc_pkt_len(skb);\n\n\tif (gred_wred_mode(t))\n\t\tgred_load_wred_set(t, q);\n\n\tq->vars.qavg = red_calc_qavg(&q->parms,\n\t\t\t\t     &q->vars,\n\t\t\t\t     gred_backlog(t, q, sch));\n\n\tif (red_is_idling(&q->vars))\n\t\tred_end_of_idle_period(&q->vars);\n\n\tif (gred_wred_mode(t))\n\t\tgred_store_wred_set(t, q);\n\n\tswitch (red_action(&q->parms, &q->vars, q->vars.qavg + qavg)) {\n\tcase RED_DONT_MARK:\n\t\tbreak;\n\n\tcase RED_PROB_MARK:\n\t\tqdisc_qstats_overlimit(sch);\n\t\tif (!gred_use_ecn(q) || !INET_ECN_set_ce(skb)) {\n\t\t\tq->stats.prob_drop++;\n\t\t\tgoto congestion_drop;\n\t\t}\n\n\t\tq->stats.prob_mark++;\n\t\tbreak;\n\n\tcase RED_HARD_MARK:\n\t\tqdisc_qstats_overlimit(sch);\n\t\tif (gred_use_harddrop(q) || !gred_use_ecn(q) ||\n\t\t    !INET_ECN_set_ce(skb)) {\n\t\t\tq->stats.forced_drop++;\n\t\t\tgoto congestion_drop;\n\t\t}\n\t\tq->stats.forced_mark++;\n\t\tbreak;\n\t}\n\n\tif (gred_backlog(t, q, sch) + qdisc_pkt_len(skb) <= q->limit) {\n\t\tq->backlog += qdisc_pkt_len(skb);\n\t\treturn qdisc_enqueue_tail(skb, sch);\n\t}\n\n\tq->stats.pdrop++;\ndrop:\n\treturn qdisc_drop(skb, sch, to_free);\n\ncongestion_drop:\n\tqdisc_drop(skb, sch, to_free);\n\treturn NET_XMIT_CN;\n}\n\nstatic struct sk_buff *gred_dequeue(struct Qdisc *sch)\n{\n\tstruct sk_buff *skb;\n\tstruct gred_sched *t = qdisc_priv(sch);\n\n\tskb = qdisc_dequeue_head(sch);\n\n\tif (skb) {\n\t\tstruct gred_sched_data *q;\n\t\tu16 dp = tc_index_to_dp(skb);\n\n\t\tif (dp >= t->DPs || (q = t->tab[dp]) == NULL) {\n\t\t\tnet_warn_ratelimited(\"GRED: Unable to relocate VQ 0x%x after dequeue, screwing up backlog\\n\",\n\t\t\t\t\t     tc_index_to_dp(skb));\n\t\t} else {\n\t\t\tq->backlog -= qdisc_pkt_len(skb);\n\n\t\t\tif (gred_wred_mode(t)) {\n\t\t\t\tif (!sch->qstats.backlog)\n\t\t\t\t\tred_start_of_idle_period(&t->wred_set);\n\t\t\t} else {\n\t\t\t\tif (!q->backlog)\n\t\t\t\t\tred_start_of_idle_period(&q->vars);\n\t\t\t}\n\t\t}\n\n\t\treturn skb;\n\t}\n\n\treturn NULL;\n}\n\nstatic void gred_reset(struct Qdisc *sch)\n{\n\tint i;\n\tstruct gred_sched *t = qdisc_priv(sch);\n\n\tqdisc_reset_queue(sch);\n\n\tfor (i = 0; i < t->DPs; i++) {\n\t\tstruct gred_sched_data *q = t->tab[i];\n\n\t\tif (!q)\n\t\t\tcontinue;\n\n\t\tred_restart(&q->vars);\n\t\tq->backlog = 0;\n\t}\n}\n\nstatic void gred_offload(struct Qdisc *sch, enum tc_gred_command command)\n{\n\tstruct gred_sched *table = qdisc_priv(sch);\n\tstruct net_device *dev = qdisc_dev(sch);\n\tstruct tc_gred_qopt_offload *opt = table->opt;\n\n\tif (!tc_can_offload(dev) || !dev->netdev_ops->ndo_setup_tc)\n\t\treturn;\n\n\tmemset(opt, 0, sizeof(*opt));\n\topt->command = command;\n\topt->handle = sch->handle;\n\topt->parent = sch->parent;\n\n\tif (command == TC_GRED_REPLACE) {\n\t\tunsigned int i;\n\n\t\topt->set.grio_on = gred_rio_mode(table);\n\t\topt->set.wred_on = gred_wred_mode(table);\n\t\topt->set.dp_cnt = table->DPs;\n\t\topt->set.dp_def = table->def;\n\n\t\tfor (i = 0; i < table->DPs; i++) {\n\t\t\tstruct gred_sched_data *q = table->tab[i];\n\n\t\t\tif (!q)\n\t\t\t\tcontinue;\n\t\t\topt->set.tab[i].present = true;\n\t\t\topt->set.tab[i].limit = q->limit;\n\t\t\topt->set.tab[i].prio = q->prio;\n\t\t\topt->set.tab[i].min = q->parms.qth_min >> q->parms.Wlog;\n\t\t\topt->set.tab[i].max = q->parms.qth_max >> q->parms.Wlog;\n\t\t\topt->set.tab[i].is_ecn = gred_use_ecn(q);\n\t\t\topt->set.tab[i].is_harddrop = gred_use_harddrop(q);\n\t\t\topt->set.tab[i].probability = q->parms.max_P;\n\t\t\topt->set.tab[i].backlog = &q->backlog;\n\t\t}\n\t\topt->set.qstats = &sch->qstats;\n\t}\n\n\tdev->netdev_ops->ndo_setup_tc(dev, TC_SETUP_QDISC_GRED, opt);\n}\n\nstatic int gred_offload_dump_stats(struct Qdisc *sch)\n{\n\tstruct gred_sched *table = qdisc_priv(sch);\n\tstruct tc_gred_qopt_offload *hw_stats;\n\tu64 bytes = 0, packets = 0;\n\tunsigned int i;\n\tint ret;\n\n\thw_stats = kzalloc(sizeof(*hw_stats), GFP_KERNEL);\n\tif (!hw_stats)\n\t\treturn -ENOMEM;\n\n\thw_stats->command = TC_GRED_STATS;\n\thw_stats->handle = sch->handle;\n\thw_stats->parent = sch->parent;\n\n\tfor (i = 0; i < MAX_DPs; i++) {\n\t\tgnet_stats_basic_sync_init(&hw_stats->stats.bstats[i]);\n\t\tif (table->tab[i])\n\t\t\thw_stats->stats.xstats[i] = &table->tab[i]->stats;\n\t}\n\n\tret = qdisc_offload_dump_helper(sch, TC_SETUP_QDISC_GRED, hw_stats);\n\t \n\tsch_tree_lock(sch);\n\tfor (i = 0; i < MAX_DPs; i++) {\n\t\tif (!table->tab[i])\n\t\t\tcontinue;\n\t\ttable->tab[i]->packetsin += u64_stats_read(&hw_stats->stats.bstats[i].packets);\n\t\ttable->tab[i]->bytesin += u64_stats_read(&hw_stats->stats.bstats[i].bytes);\n\t\ttable->tab[i]->backlog += hw_stats->stats.qstats[i].backlog;\n\n\t\tbytes += u64_stats_read(&hw_stats->stats.bstats[i].bytes);\n\t\tpackets += u64_stats_read(&hw_stats->stats.bstats[i].packets);\n\t\tsch->qstats.qlen += hw_stats->stats.qstats[i].qlen;\n\t\tsch->qstats.backlog += hw_stats->stats.qstats[i].backlog;\n\t\tsch->qstats.drops += hw_stats->stats.qstats[i].drops;\n\t\tsch->qstats.requeues += hw_stats->stats.qstats[i].requeues;\n\t\tsch->qstats.overlimits += hw_stats->stats.qstats[i].overlimits;\n\t}\n\t_bstats_update(&sch->bstats, bytes, packets);\n\tsch_tree_unlock(sch);\n\n\tkfree(hw_stats);\n\treturn ret;\n}\n\nstatic inline void gred_destroy_vq(struct gred_sched_data *q)\n{\n\tkfree(q);\n}\n\nstatic int gred_change_table_def(struct Qdisc *sch, struct nlattr *dps,\n\t\t\t\t struct netlink_ext_ack *extack)\n{\n\tstruct gred_sched *table = qdisc_priv(sch);\n\tstruct tc_gred_sopt *sopt;\n\tbool red_flags_changed;\n\tint i;\n\n\tif (!dps)\n\t\treturn -EINVAL;\n\n\tsopt = nla_data(dps);\n\n\tif (sopt->DPs > MAX_DPs) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"number of virtual queues too high\");\n\t\treturn -EINVAL;\n\t}\n\tif (sopt->DPs == 0) {\n\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t   \"number of virtual queues can't be 0\");\n\t\treturn -EINVAL;\n\t}\n\tif (sopt->def_DP >= sopt->DPs) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"default virtual queue above virtual queue count\");\n\t\treturn -EINVAL;\n\t}\n\tif (sopt->flags && gred_per_vq_red_flags_used(table)) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"can't set per-Qdisc RED flags when per-virtual queue flags are used\");\n\t\treturn -EINVAL;\n\t}\n\n\tsch_tree_lock(sch);\n\ttable->DPs = sopt->DPs;\n\ttable->def = sopt->def_DP;\n\tred_flags_changed = table->red_flags != sopt->flags;\n\ttable->red_flags = sopt->flags;\n\n\t \n\tsch_tree_unlock(sch);\n\n\tif (sopt->grio) {\n\t\tgred_enable_rio_mode(table);\n\t\tgred_disable_wred_mode(table);\n\t\tif (gred_wred_mode_check(sch))\n\t\t\tgred_enable_wred_mode(table);\n\t} else {\n\t\tgred_disable_rio_mode(table);\n\t\tgred_disable_wred_mode(table);\n\t}\n\n\tif (red_flags_changed)\n\t\tfor (i = 0; i < table->DPs; i++)\n\t\t\tif (table->tab[i])\n\t\t\t\ttable->tab[i]->red_flags =\n\t\t\t\t\ttable->red_flags & GRED_VQ_RED_FLAGS;\n\n\tfor (i = table->DPs; i < MAX_DPs; i++) {\n\t\tif (table->tab[i]) {\n\t\t\tpr_warn(\"GRED: Warning: Destroying shadowed VQ 0x%x\\n\",\n\t\t\t\ti);\n\t\t\tgred_destroy_vq(table->tab[i]);\n\t\t\ttable->tab[i] = NULL;\n\t\t}\n\t}\n\n\tgred_offload(sch, TC_GRED_REPLACE);\n\treturn 0;\n}\n\nstatic inline int gred_change_vq(struct Qdisc *sch, int dp,\n\t\t\t\t struct tc_gred_qopt *ctl, int prio,\n\t\t\t\t u8 *stab, u32 max_P,\n\t\t\t\t struct gred_sched_data **prealloc,\n\t\t\t\t struct netlink_ext_ack *extack)\n{\n\tstruct gred_sched *table = qdisc_priv(sch);\n\tstruct gred_sched_data *q = table->tab[dp];\n\n\tif (!red_check_params(ctl->qth_min, ctl->qth_max, ctl->Wlog, ctl->Scell_log, stab)) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"invalid RED parameters\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!q) {\n\t\ttable->tab[dp] = q = *prealloc;\n\t\t*prealloc = NULL;\n\t\tif (!q)\n\t\t\treturn -ENOMEM;\n\t\tq->red_flags = table->red_flags & GRED_VQ_RED_FLAGS;\n\t}\n\n\tq->DP = dp;\n\tq->prio = prio;\n\tif (ctl->limit > sch->limit)\n\t\tq->limit = sch->limit;\n\telse\n\t\tq->limit = ctl->limit;\n\n\tif (q->backlog == 0)\n\t\tred_end_of_idle_period(&q->vars);\n\n\tred_set_parms(&q->parms,\n\t\t      ctl->qth_min, ctl->qth_max, ctl->Wlog, ctl->Plog,\n\t\t      ctl->Scell_log, stab, max_P);\n\tred_set_vars(&q->vars);\n\treturn 0;\n}\n\nstatic const struct nla_policy gred_vq_policy[TCA_GRED_VQ_MAX + 1] = {\n\t[TCA_GRED_VQ_DP]\t= { .type = NLA_U32 },\n\t[TCA_GRED_VQ_FLAGS]\t= { .type = NLA_U32 },\n};\n\nstatic const struct nla_policy gred_vqe_policy[TCA_GRED_VQ_ENTRY_MAX + 1] = {\n\t[TCA_GRED_VQ_ENTRY]\t= { .type = NLA_NESTED },\n};\n\nstatic const struct nla_policy gred_policy[TCA_GRED_MAX + 1] = {\n\t[TCA_GRED_PARMS]\t= { .len = sizeof(struct tc_gred_qopt) },\n\t[TCA_GRED_STAB]\t\t= { .len = 256 },\n\t[TCA_GRED_DPS]\t\t= { .len = sizeof(struct tc_gred_sopt) },\n\t[TCA_GRED_MAX_P]\t= { .type = NLA_U32 },\n\t[TCA_GRED_LIMIT]\t= { .type = NLA_U32 },\n\t[TCA_GRED_VQ_LIST]\t= { .type = NLA_NESTED },\n};\n\nstatic void gred_vq_apply(struct gred_sched *table, const struct nlattr *entry)\n{\n\tstruct nlattr *tb[TCA_GRED_VQ_MAX + 1];\n\tu32 dp;\n\n\tnla_parse_nested_deprecated(tb, TCA_GRED_VQ_MAX, entry,\n\t\t\t\t    gred_vq_policy, NULL);\n\n\tdp = nla_get_u32(tb[TCA_GRED_VQ_DP]);\n\n\tif (tb[TCA_GRED_VQ_FLAGS])\n\t\ttable->tab[dp]->red_flags = nla_get_u32(tb[TCA_GRED_VQ_FLAGS]);\n}\n\nstatic void gred_vqs_apply(struct gred_sched *table, struct nlattr *vqs)\n{\n\tconst struct nlattr *attr;\n\tint rem;\n\n\tnla_for_each_nested(attr, vqs, rem) {\n\t\tswitch (nla_type(attr)) {\n\t\tcase TCA_GRED_VQ_ENTRY:\n\t\t\tgred_vq_apply(table, attr);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic int gred_vq_validate(struct gred_sched *table, u32 cdp,\n\t\t\t    const struct nlattr *entry,\n\t\t\t    struct netlink_ext_ack *extack)\n{\n\tstruct nlattr *tb[TCA_GRED_VQ_MAX + 1];\n\tint err;\n\tu32 dp;\n\n\terr = nla_parse_nested_deprecated(tb, TCA_GRED_VQ_MAX, entry,\n\t\t\t\t\t  gred_vq_policy, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (!tb[TCA_GRED_VQ_DP]) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Virtual queue with no index specified\");\n\t\treturn -EINVAL;\n\t}\n\tdp = nla_get_u32(tb[TCA_GRED_VQ_DP]);\n\tif (dp >= table->DPs) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Virtual queue with index out of bounds\");\n\t\treturn -EINVAL;\n\t}\n\tif (dp != cdp && !table->tab[dp]) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Virtual queue not yet instantiated\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (tb[TCA_GRED_VQ_FLAGS]) {\n\t\tu32 red_flags = nla_get_u32(tb[TCA_GRED_VQ_FLAGS]);\n\n\t\tif (table->red_flags && table->red_flags != red_flags) {\n\t\t\tNL_SET_ERR_MSG_MOD(extack, \"can't change per-virtual queue RED flags when per-Qdisc flags are used\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (red_flags & ~GRED_VQ_RED_FLAGS) {\n\t\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t\t   \"invalid RED flags specified\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int gred_vqs_validate(struct gred_sched *table, u32 cdp,\n\t\t\t     struct nlattr *vqs, struct netlink_ext_ack *extack)\n{\n\tconst struct nlattr *attr;\n\tint rem, err;\n\n\terr = nla_validate_nested_deprecated(vqs, TCA_GRED_VQ_ENTRY_MAX,\n\t\t\t\t\t     gred_vqe_policy, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tnla_for_each_nested(attr, vqs, rem) {\n\t\tswitch (nla_type(attr)) {\n\t\tcase TCA_GRED_VQ_ENTRY:\n\t\t\terr = gred_vq_validate(table, cdp, attr, extack);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tNL_SET_ERR_MSG_MOD(extack, \"GRED_VQ_LIST can contain only entry attributes\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tif (rem > 0) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Trailing data after parsing virtual queue list\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int gred_change(struct Qdisc *sch, struct nlattr *opt,\n\t\t       struct netlink_ext_ack *extack)\n{\n\tstruct gred_sched *table = qdisc_priv(sch);\n\tstruct tc_gred_qopt *ctl;\n\tstruct nlattr *tb[TCA_GRED_MAX + 1];\n\tint err, prio = GRED_DEF_PRIO;\n\tu8 *stab;\n\tu32 max_P;\n\tstruct gred_sched_data *prealloc;\n\n\terr = nla_parse_nested_deprecated(tb, TCA_GRED_MAX, opt, gred_policy,\n\t\t\t\t\t  extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[TCA_GRED_PARMS] == NULL && tb[TCA_GRED_STAB] == NULL) {\n\t\tif (tb[TCA_GRED_LIMIT] != NULL)\n\t\t\tsch->limit = nla_get_u32(tb[TCA_GRED_LIMIT]);\n\t\treturn gred_change_table_def(sch, tb[TCA_GRED_DPS], extack);\n\t}\n\n\tif (tb[TCA_GRED_PARMS] == NULL ||\n\t    tb[TCA_GRED_STAB] == NULL ||\n\t    tb[TCA_GRED_LIMIT] != NULL) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"can't configure Qdisc and virtual queue at the same time\");\n\t\treturn -EINVAL;\n\t}\n\n\tmax_P = tb[TCA_GRED_MAX_P] ? nla_get_u32(tb[TCA_GRED_MAX_P]) : 0;\n\n\tctl = nla_data(tb[TCA_GRED_PARMS]);\n\tstab = nla_data(tb[TCA_GRED_STAB]);\n\n\tif (ctl->DP >= table->DPs) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"virtual queue index above virtual queue count\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (tb[TCA_GRED_VQ_LIST]) {\n\t\terr = gred_vqs_validate(table, ctl->DP, tb[TCA_GRED_VQ_LIST],\n\t\t\t\t\textack);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (gred_rio_mode(table)) {\n\t\tif (ctl->prio == 0) {\n\t\t\tint def_prio = GRED_DEF_PRIO;\n\n\t\t\tif (table->tab[table->def])\n\t\t\t\tdef_prio = table->tab[table->def]->prio;\n\n\t\t\tprintk(KERN_DEBUG \"GRED: DP %u does not have a prio \"\n\t\t\t       \"setting default to %d\\n\", ctl->DP, def_prio);\n\n\t\t\tprio = def_prio;\n\t\t} else\n\t\t\tprio = ctl->prio;\n\t}\n\n\tprealloc = kzalloc(sizeof(*prealloc), GFP_KERNEL);\n\tsch_tree_lock(sch);\n\n\terr = gred_change_vq(sch, ctl->DP, ctl, prio, stab, max_P, &prealloc,\n\t\t\t     extack);\n\tif (err < 0)\n\t\tgoto err_unlock_free;\n\n\tif (tb[TCA_GRED_VQ_LIST])\n\t\tgred_vqs_apply(table, tb[TCA_GRED_VQ_LIST]);\n\n\tif (gred_rio_mode(table)) {\n\t\tgred_disable_wred_mode(table);\n\t\tif (gred_wred_mode_check(sch))\n\t\t\tgred_enable_wred_mode(table);\n\t}\n\n\tsch_tree_unlock(sch);\n\tkfree(prealloc);\n\n\tgred_offload(sch, TC_GRED_REPLACE);\n\treturn 0;\n\nerr_unlock_free:\n\tsch_tree_unlock(sch);\n\tkfree(prealloc);\n\treturn err;\n}\n\nstatic int gred_init(struct Qdisc *sch, struct nlattr *opt,\n\t\t     struct netlink_ext_ack *extack)\n{\n\tstruct gred_sched *table = qdisc_priv(sch);\n\tstruct nlattr *tb[TCA_GRED_MAX + 1];\n\tint err;\n\n\tif (!opt)\n\t\treturn -EINVAL;\n\n\terr = nla_parse_nested_deprecated(tb, TCA_GRED_MAX, opt, gred_policy,\n\t\t\t\t\t  extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[TCA_GRED_PARMS] || tb[TCA_GRED_STAB]) {\n\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t   \"virtual queue configuration can't be specified at initialization time\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (tb[TCA_GRED_LIMIT])\n\t\tsch->limit = nla_get_u32(tb[TCA_GRED_LIMIT]);\n\telse\n\t\tsch->limit = qdisc_dev(sch)->tx_queue_len\n\t\t             * psched_mtu(qdisc_dev(sch));\n\n\tif (qdisc_dev(sch)->netdev_ops->ndo_setup_tc) {\n\t\ttable->opt = kzalloc(sizeof(*table->opt), GFP_KERNEL);\n\t\tif (!table->opt)\n\t\t\treturn -ENOMEM;\n\t}\n\n\treturn gred_change_table_def(sch, tb[TCA_GRED_DPS], extack);\n}\n\nstatic int gred_dump(struct Qdisc *sch, struct sk_buff *skb)\n{\n\tstruct gred_sched *table = qdisc_priv(sch);\n\tstruct nlattr *parms, *vqs, *opts = NULL;\n\tint i;\n\tu32 max_p[MAX_DPs];\n\tstruct tc_gred_sopt sopt = {\n\t\t.DPs\t= table->DPs,\n\t\t.def_DP\t= table->def,\n\t\t.grio\t= gred_rio_mode(table),\n\t\t.flags\t= table->red_flags,\n\t};\n\n\tif (gred_offload_dump_stats(sch))\n\t\tgoto nla_put_failure;\n\n\topts = nla_nest_start_noflag(skb, TCA_OPTIONS);\n\tif (opts == NULL)\n\t\tgoto nla_put_failure;\n\tif (nla_put(skb, TCA_GRED_DPS, sizeof(sopt), &sopt))\n\t\tgoto nla_put_failure;\n\n\tfor (i = 0; i < MAX_DPs; i++) {\n\t\tstruct gred_sched_data *q = table->tab[i];\n\n\t\tmax_p[i] = q ? q->parms.max_P : 0;\n\t}\n\tif (nla_put(skb, TCA_GRED_MAX_P, sizeof(max_p), max_p))\n\t\tgoto nla_put_failure;\n\n\tif (nla_put_u32(skb, TCA_GRED_LIMIT, sch->limit))\n\t\tgoto nla_put_failure;\n\n\t \n\tparms = nla_nest_start_noflag(skb, TCA_GRED_PARMS);\n\tif (parms == NULL)\n\t\tgoto nla_put_failure;\n\n\tfor (i = 0; i < MAX_DPs; i++) {\n\t\tstruct gred_sched_data *q = table->tab[i];\n\t\tstruct tc_gred_qopt opt;\n\t\tunsigned long qavg;\n\n\t\tmemset(&opt, 0, sizeof(opt));\n\n\t\tif (!q) {\n\t\t\t \n\n\t\t\topt.DP = MAX_DPs + i;\n\t\t\tgoto append_opt;\n\t\t}\n\n\t\topt.limit\t= q->limit;\n\t\topt.DP\t\t= q->DP;\n\t\topt.backlog\t= gred_backlog(table, q, sch);\n\t\topt.prio\t= q->prio;\n\t\topt.qth_min\t= q->parms.qth_min >> q->parms.Wlog;\n\t\topt.qth_max\t= q->parms.qth_max >> q->parms.Wlog;\n\t\topt.Wlog\t= q->parms.Wlog;\n\t\topt.Plog\t= q->parms.Plog;\n\t\topt.Scell_log\t= q->parms.Scell_log;\n\t\topt.early\t= q->stats.prob_drop;\n\t\topt.forced\t= q->stats.forced_drop;\n\t\topt.pdrop\t= q->stats.pdrop;\n\t\topt.packets\t= q->packetsin;\n\t\topt.bytesin\t= q->bytesin;\n\n\t\tif (gred_wred_mode(table))\n\t\t\tgred_load_wred_set(table, q);\n\n\t\tqavg = red_calc_qavg(&q->parms, &q->vars,\n\t\t\t\t     q->vars.qavg >> q->parms.Wlog);\n\t\topt.qave = qavg >> q->parms.Wlog;\n\nappend_opt:\n\t\tif (nla_append(skb, sizeof(opt), &opt) < 0)\n\t\t\tgoto nla_put_failure;\n\t}\n\n\tnla_nest_end(skb, parms);\n\n\t \n\tvqs = nla_nest_start_noflag(skb, TCA_GRED_VQ_LIST);\n\tif (!vqs)\n\t\tgoto nla_put_failure;\n\n\tfor (i = 0; i < MAX_DPs; i++) {\n\t\tstruct gred_sched_data *q = table->tab[i];\n\t\tstruct nlattr *vq;\n\n\t\tif (!q)\n\t\t\tcontinue;\n\n\t\tvq = nla_nest_start_noflag(skb, TCA_GRED_VQ_ENTRY);\n\t\tif (!vq)\n\t\t\tgoto nla_put_failure;\n\n\t\tif (nla_put_u32(skb, TCA_GRED_VQ_DP, q->DP))\n\t\t\tgoto nla_put_failure;\n\n\t\tif (nla_put_u32(skb, TCA_GRED_VQ_FLAGS, q->red_flags))\n\t\t\tgoto nla_put_failure;\n\n\t\t \n\t\tif (nla_put_u64_64bit(skb, TCA_GRED_VQ_STAT_BYTES, q->bytesin,\n\t\t\t\t      TCA_GRED_VQ_PAD))\n\t\t\tgoto nla_put_failure;\n\t\tif (nla_put_u32(skb, TCA_GRED_VQ_STAT_PACKETS, q->packetsin))\n\t\t\tgoto nla_put_failure;\n\t\tif (nla_put_u32(skb, TCA_GRED_VQ_STAT_BACKLOG,\n\t\t\t\tgred_backlog(table, q, sch)))\n\t\t\tgoto nla_put_failure;\n\t\tif (nla_put_u32(skb, TCA_GRED_VQ_STAT_PROB_DROP,\n\t\t\t\tq->stats.prob_drop))\n\t\t\tgoto nla_put_failure;\n\t\tif (nla_put_u32(skb, TCA_GRED_VQ_STAT_PROB_MARK,\n\t\t\t\tq->stats.prob_mark))\n\t\t\tgoto nla_put_failure;\n\t\tif (nla_put_u32(skb, TCA_GRED_VQ_STAT_FORCED_DROP,\n\t\t\t\tq->stats.forced_drop))\n\t\t\tgoto nla_put_failure;\n\t\tif (nla_put_u32(skb, TCA_GRED_VQ_STAT_FORCED_MARK,\n\t\t\t\tq->stats.forced_mark))\n\t\t\tgoto nla_put_failure;\n\t\tif (nla_put_u32(skb, TCA_GRED_VQ_STAT_PDROP, q->stats.pdrop))\n\t\t\tgoto nla_put_failure;\n\n\t\tnla_nest_end(skb, vq);\n\t}\n\tnla_nest_end(skb, vqs);\n\n\treturn nla_nest_end(skb, opts);\n\nnla_put_failure:\n\tnla_nest_cancel(skb, opts);\n\treturn -EMSGSIZE;\n}\n\nstatic void gred_destroy(struct Qdisc *sch)\n{\n\tstruct gred_sched *table = qdisc_priv(sch);\n\tint i;\n\n\tfor (i = 0; i < table->DPs; i++)\n\t\tgred_destroy_vq(table->tab[i]);\n\n\tgred_offload(sch, TC_GRED_DESTROY);\n\tkfree(table->opt);\n}\n\nstatic struct Qdisc_ops gred_qdisc_ops __read_mostly = {\n\t.id\t\t=\t\"gred\",\n\t.priv_size\t=\tsizeof(struct gred_sched),\n\t.enqueue\t=\tgred_enqueue,\n\t.dequeue\t=\tgred_dequeue,\n\t.peek\t\t=\tqdisc_peek_head,\n\t.init\t\t=\tgred_init,\n\t.reset\t\t=\tgred_reset,\n\t.destroy\t=\tgred_destroy,\n\t.change\t\t=\tgred_change,\n\t.dump\t\t=\tgred_dump,\n\t.owner\t\t=\tTHIS_MODULE,\n};\n\nstatic int __init gred_module_init(void)\n{\n\treturn register_qdisc(&gred_qdisc_ops);\n}\n\nstatic void __exit gred_module_exit(void)\n{\n\tunregister_qdisc(&gred_qdisc_ops);\n}\n\nmodule_init(gred_module_init)\nmodule_exit(gred_module_exit)\n\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}