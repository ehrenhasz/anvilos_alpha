{
  "module_name": "sch_netem.c",
  "hash_id": "5ed279059427b4f6b5bca3e1121e9b3eb0d386b35759f748579f0f22728225ce",
  "original_prompt": "Ingested from linux-6.6.14/net/sched/sch_netem.c",
  "human_readable_source": "\n \n\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/errno.h>\n#include <linux/skbuff.h>\n#include <linux/vmalloc.h>\n#include <linux/rtnetlink.h>\n#include <linux/reciprocal_div.h>\n#include <linux/rbtree.h>\n\n#include <net/gso.h>\n#include <net/netlink.h>\n#include <net/pkt_sched.h>\n#include <net/inet_ecn.h>\n\n#define VERSION \"1.3\"\n\n \n\nstruct disttable {\n\tu32  size;\n\ts16 table[];\n};\n\nstruct netem_sched_data {\n\t \n\tstruct rb_root t_root;\n\n\t \n\tstruct sk_buff\t*t_head;\n\tstruct sk_buff\t*t_tail;\n\n\t \n\tstruct Qdisc\t*qdisc;\n\n\tstruct qdisc_watchdog watchdog;\n\n\ts64 latency;\n\ts64 jitter;\n\n\tu32 loss;\n\tu32 ecn;\n\tu32 limit;\n\tu32 counter;\n\tu32 gap;\n\tu32 duplicate;\n\tu32 reorder;\n\tu32 corrupt;\n\tu64 rate;\n\ts32 packet_overhead;\n\tu32 cell_size;\n\tstruct reciprocal_value cell_size_reciprocal;\n\ts32 cell_overhead;\n\n\tstruct crndstate {\n\t\tu32 last;\n\t\tu32 rho;\n\t} delay_cor, loss_cor, dup_cor, reorder_cor, corrupt_cor;\n\n\tstruct prng  {\n\t\tu64 seed;\n\t\tstruct rnd_state prng_state;\n\t} prng;\n\n\tstruct disttable *delay_dist;\n\n\tenum  {\n\t\tCLG_RANDOM,\n\t\tCLG_4_STATES,\n\t\tCLG_GILB_ELL,\n\t} loss_model;\n\n\tenum {\n\t\tTX_IN_GAP_PERIOD = 1,\n\t\tTX_IN_BURST_PERIOD,\n\t\tLOST_IN_GAP_PERIOD,\n\t\tLOST_IN_BURST_PERIOD,\n\t} _4_state_model;\n\n\tenum {\n\t\tGOOD_STATE = 1,\n\t\tBAD_STATE,\n\t} GE_state_model;\n\n\t \n\tstruct clgstate {\n\t\t \n\t\tu8 state;\n\n\t\t \n\t\tu32 a1;\t \n\t\tu32 a2;\t \n\t\tu32 a3;\t \n\t\tu32 a4;\t \n\t\tu32 a5;  \n\t} clg;\n\n\tstruct tc_netem_slot slot_config;\n\tstruct slotstate {\n\t\tu64 slot_next;\n\t\ts32 packets_left;\n\t\ts32 bytes_left;\n\t} slot;\n\n\tstruct disttable *slot_dist;\n};\n\n \nstruct netem_skb_cb {\n\tu64\t        time_to_send;\n};\n\nstatic inline struct netem_skb_cb *netem_skb_cb(struct sk_buff *skb)\n{\n\t \n\tqdisc_cb_private_validate(skb, sizeof(struct netem_skb_cb));\n\treturn (struct netem_skb_cb *)qdisc_skb_cb(skb)->data;\n}\n\n \nstatic void init_crandom(struct crndstate *state, unsigned long rho)\n{\n\tstate->rho = rho;\n\tstate->last = get_random_u32();\n}\n\n \nstatic u32 get_crandom(struct crndstate *state, struct prng *p)\n{\n\tu64 value, rho;\n\tunsigned long answer;\n\tstruct rnd_state *s = &p->prng_state;\n\n\tif (!state || state->rho == 0)\t \n\t\treturn prandom_u32_state(s);\n\n\tvalue = prandom_u32_state(s);\n\trho = (u64)state->rho + 1;\n\tanswer = (value * ((1ull<<32) - rho) + state->last * rho) >> 32;\n\tstate->last = answer;\n\treturn answer;\n}\n\n \nstatic bool loss_4state(struct netem_sched_data *q)\n{\n\tstruct clgstate *clg = &q->clg;\n\tu32 rnd = prandom_u32_state(&q->prng.prng_state);\n\n\t \n\tswitch (clg->state) {\n\tcase TX_IN_GAP_PERIOD:\n\t\tif (rnd < clg->a4) {\n\t\t\tclg->state = LOST_IN_GAP_PERIOD;\n\t\t\treturn true;\n\t\t} else if (clg->a4 < rnd && rnd < clg->a1 + clg->a4) {\n\t\t\tclg->state = LOST_IN_BURST_PERIOD;\n\t\t\treturn true;\n\t\t} else if (clg->a1 + clg->a4 < rnd) {\n\t\t\tclg->state = TX_IN_GAP_PERIOD;\n\t\t}\n\n\t\tbreak;\n\tcase TX_IN_BURST_PERIOD:\n\t\tif (rnd < clg->a5) {\n\t\t\tclg->state = LOST_IN_BURST_PERIOD;\n\t\t\treturn true;\n\t\t} else {\n\t\t\tclg->state = TX_IN_BURST_PERIOD;\n\t\t}\n\n\t\tbreak;\n\tcase LOST_IN_BURST_PERIOD:\n\t\tif (rnd < clg->a3)\n\t\t\tclg->state = TX_IN_BURST_PERIOD;\n\t\telse if (clg->a3 < rnd && rnd < clg->a2 + clg->a3) {\n\t\t\tclg->state = TX_IN_GAP_PERIOD;\n\t\t} else if (clg->a2 + clg->a3 < rnd) {\n\t\t\tclg->state = LOST_IN_BURST_PERIOD;\n\t\t\treturn true;\n\t\t}\n\t\tbreak;\n\tcase LOST_IN_GAP_PERIOD:\n\t\tclg->state = TX_IN_GAP_PERIOD;\n\t\tbreak;\n\t}\n\n\treturn false;\n}\n\n \nstatic bool loss_gilb_ell(struct netem_sched_data *q)\n{\n\tstruct clgstate *clg = &q->clg;\n\tstruct rnd_state *s = &q->prng.prng_state;\n\n\tswitch (clg->state) {\n\tcase GOOD_STATE:\n\t\tif (prandom_u32_state(s) < clg->a1)\n\t\t\tclg->state = BAD_STATE;\n\t\tif (prandom_u32_state(s) < clg->a4)\n\t\t\treturn true;\n\t\tbreak;\n\tcase BAD_STATE:\n\t\tif (prandom_u32_state(s) < clg->a2)\n\t\t\tclg->state = GOOD_STATE;\n\t\tif (prandom_u32_state(s) > clg->a3)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic bool loss_event(struct netem_sched_data *q)\n{\n\tswitch (q->loss_model) {\n\tcase CLG_RANDOM:\n\t\t \n\t\treturn q->loss && q->loss >= get_crandom(&q->loss_cor, &q->prng);\n\n\tcase CLG_4_STATES:\n\t\t \n\t\treturn loss_4state(q);\n\n\tcase CLG_GILB_ELL:\n\t\t \n\t\treturn loss_gilb_ell(q);\n\t}\n\n\treturn false;\t \n}\n\n\n \nstatic s64 tabledist(s64 mu, s32 sigma,\n\t\t     struct crndstate *state,\n\t\t     struct prng *prng,\n\t\t     const struct disttable *dist)\n{\n\ts64 x;\n\tlong t;\n\tu32 rnd;\n\n\tif (sigma == 0)\n\t\treturn mu;\n\n\trnd = get_crandom(state, prng);\n\n\t \n\tif (dist == NULL)\n\t\treturn ((rnd % (2 * (u32)sigma)) + mu) - sigma;\n\n\tt = dist->table[rnd % dist->size];\n\tx = (sigma % NETEM_DIST_SCALE) * t;\n\tif (x >= 0)\n\t\tx += NETEM_DIST_SCALE/2;\n\telse\n\t\tx -= NETEM_DIST_SCALE/2;\n\n\treturn  x / NETEM_DIST_SCALE + (sigma / NETEM_DIST_SCALE) * t + mu;\n}\n\nstatic u64 packet_time_ns(u64 len, const struct netem_sched_data *q)\n{\n\tlen += q->packet_overhead;\n\n\tif (q->cell_size) {\n\t\tu32 cells = reciprocal_divide(len, q->cell_size_reciprocal);\n\n\t\tif (len > cells * q->cell_size)\t \n\t\t\tcells++;\n\t\tlen = cells * (q->cell_size + q->cell_overhead);\n\t}\n\n\treturn div64_u64(len * NSEC_PER_SEC, q->rate);\n}\n\nstatic void tfifo_reset(struct Qdisc *sch)\n{\n\tstruct netem_sched_data *q = qdisc_priv(sch);\n\tstruct rb_node *p = rb_first(&q->t_root);\n\n\twhile (p) {\n\t\tstruct sk_buff *skb = rb_to_skb(p);\n\n\t\tp = rb_next(p);\n\t\trb_erase(&skb->rbnode, &q->t_root);\n\t\trtnl_kfree_skbs(skb, skb);\n\t}\n\n\trtnl_kfree_skbs(q->t_head, q->t_tail);\n\tq->t_head = NULL;\n\tq->t_tail = NULL;\n}\n\nstatic void tfifo_enqueue(struct sk_buff *nskb, struct Qdisc *sch)\n{\n\tstruct netem_sched_data *q = qdisc_priv(sch);\n\tu64 tnext = netem_skb_cb(nskb)->time_to_send;\n\n\tif (!q->t_tail || tnext >= netem_skb_cb(q->t_tail)->time_to_send) {\n\t\tif (q->t_tail)\n\t\t\tq->t_tail->next = nskb;\n\t\telse\n\t\t\tq->t_head = nskb;\n\t\tq->t_tail = nskb;\n\t} else {\n\t\tstruct rb_node **p = &q->t_root.rb_node, *parent = NULL;\n\n\t\twhile (*p) {\n\t\t\tstruct sk_buff *skb;\n\n\t\t\tparent = *p;\n\t\t\tskb = rb_to_skb(parent);\n\t\t\tif (tnext >= netem_skb_cb(skb)->time_to_send)\n\t\t\t\tp = &parent->rb_right;\n\t\t\telse\n\t\t\t\tp = &parent->rb_left;\n\t\t}\n\t\trb_link_node(&nskb->rbnode, parent, p);\n\t\trb_insert_color(&nskb->rbnode, &q->t_root);\n\t}\n\tsch->q.qlen++;\n}\n\n \nstatic struct sk_buff *netem_segment(struct sk_buff *skb, struct Qdisc *sch,\n\t\t\t\t     struct sk_buff **to_free)\n{\n\tstruct sk_buff *segs;\n\tnetdev_features_t features = netif_skb_features(skb);\n\n\tsegs = skb_gso_segment(skb, features & ~NETIF_F_GSO_MASK);\n\n\tif (IS_ERR_OR_NULL(segs)) {\n\t\tqdisc_drop(skb, sch, to_free);\n\t\treturn NULL;\n\t}\n\tconsume_skb(skb);\n\treturn segs;\n}\n\n \nstatic int netem_enqueue(struct sk_buff *skb, struct Qdisc *sch,\n\t\t\t struct sk_buff **to_free)\n{\n\tstruct netem_sched_data *q = qdisc_priv(sch);\n\t \n\tstruct netem_skb_cb *cb;\n\tstruct sk_buff *skb2;\n\tstruct sk_buff *segs = NULL;\n\tunsigned int prev_len = qdisc_pkt_len(skb);\n\tint count = 1;\n\tint rc = NET_XMIT_SUCCESS;\n\tint rc_drop = NET_XMIT_DROP;\n\n\t \n\tskb->prev = NULL;\n\n\t \n\tif (q->duplicate && q->duplicate >= get_crandom(&q->dup_cor, &q->prng))\n\t\t++count;\n\n\t \n\tif (loss_event(q)) {\n\t\tif (q->ecn && INET_ECN_set_ce(skb))\n\t\t\tqdisc_qstats_drop(sch);  \n\t\telse\n\t\t\t--count;\n\t}\n\tif (count == 0) {\n\t\tqdisc_qstats_drop(sch);\n\t\t__qdisc_drop(skb, to_free);\n\t\treturn NET_XMIT_SUCCESS | __NET_XMIT_BYPASS;\n\t}\n\n\t \n\tif (q->latency || q->jitter || q->rate)\n\t\tskb_orphan_partial(skb);\n\n\t \n\tif (count > 1 && (skb2 = skb_clone(skb, GFP_ATOMIC)) != NULL) {\n\t\tstruct Qdisc *rootq = qdisc_root_bh(sch);\n\t\tu32 dupsave = q->duplicate;  \n\n\t\tq->duplicate = 0;\n\t\trootq->enqueue(skb2, rootq, to_free);\n\t\tq->duplicate = dupsave;\n\t\trc_drop = NET_XMIT_SUCCESS;\n\t}\n\n\t \n\tif (q->corrupt && q->corrupt >= get_crandom(&q->corrupt_cor, &q->prng)) {\n\t\tif (skb_is_gso(skb)) {\n\t\t\tskb = netem_segment(skb, sch, to_free);\n\t\t\tif (!skb)\n\t\t\t\treturn rc_drop;\n\t\t\tsegs = skb->next;\n\t\t\tskb_mark_not_on_list(skb);\n\t\t\tqdisc_skb_cb(skb)->pkt_len = skb->len;\n\t\t}\n\n\t\tskb = skb_unshare(skb, GFP_ATOMIC);\n\t\tif (unlikely(!skb)) {\n\t\t\tqdisc_qstats_drop(sch);\n\t\t\tgoto finish_segs;\n\t\t}\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL &&\n\t\t    skb_checksum_help(skb)) {\n\t\t\tqdisc_drop(skb, sch, to_free);\n\t\t\tskb = NULL;\n\t\t\tgoto finish_segs;\n\t\t}\n\n\t\tskb->data[get_random_u32_below(skb_headlen(skb))] ^=\n\t\t\t1<<get_random_u32_below(8);\n\t}\n\n\tif (unlikely(sch->q.qlen >= sch->limit)) {\n\t\t \n\t\tskb->next = segs;\n\t\tqdisc_drop_all(skb, sch, to_free);\n\t\treturn rc_drop;\n\t}\n\n\tqdisc_qstats_backlog_inc(sch, skb);\n\n\tcb = netem_skb_cb(skb);\n\tif (q->gap == 0 ||\t\t \n\t    q->counter < q->gap - 1 ||\t \n\t    q->reorder < get_crandom(&q->reorder_cor, &q->prng)) {\n\t\tu64 now;\n\t\ts64 delay;\n\n\t\tdelay = tabledist(q->latency, q->jitter,\n\t\t\t\t  &q->delay_cor, &q->prng, q->delay_dist);\n\n\t\tnow = ktime_get_ns();\n\n\t\tif (q->rate) {\n\t\t\tstruct netem_skb_cb *last = NULL;\n\n\t\t\tif (sch->q.tail)\n\t\t\t\tlast = netem_skb_cb(sch->q.tail);\n\t\t\tif (q->t_root.rb_node) {\n\t\t\t\tstruct sk_buff *t_skb;\n\t\t\t\tstruct netem_skb_cb *t_last;\n\n\t\t\t\tt_skb = skb_rb_last(&q->t_root);\n\t\t\t\tt_last = netem_skb_cb(t_skb);\n\t\t\t\tif (!last ||\n\t\t\t\t    t_last->time_to_send > last->time_to_send)\n\t\t\t\t\tlast = t_last;\n\t\t\t}\n\t\t\tif (q->t_tail) {\n\t\t\t\tstruct netem_skb_cb *t_last =\n\t\t\t\t\tnetem_skb_cb(q->t_tail);\n\n\t\t\t\tif (!last ||\n\t\t\t\t    t_last->time_to_send > last->time_to_send)\n\t\t\t\t\tlast = t_last;\n\t\t\t}\n\n\t\t\tif (last) {\n\t\t\t\t \n\t\t\t\tdelay -= last->time_to_send - now;\n\t\t\t\tdelay = max_t(s64, 0, delay);\n\t\t\t\tnow = last->time_to_send;\n\t\t\t}\n\n\t\t\tdelay += packet_time_ns(qdisc_pkt_len(skb), q);\n\t\t}\n\n\t\tcb->time_to_send = now + delay;\n\t\t++q->counter;\n\t\ttfifo_enqueue(skb, sch);\n\t} else {\n\t\t \n\t\tcb->time_to_send = ktime_get_ns();\n\t\tq->counter = 0;\n\n\t\t__qdisc_enqueue_head(skb, &sch->q);\n\t\tsch->qstats.requeues++;\n\t}\n\nfinish_segs:\n\tif (segs) {\n\t\tunsigned int len, last_len;\n\t\tint nb;\n\n\t\tlen = skb ? skb->len : 0;\n\t\tnb = skb ? 1 : 0;\n\n\t\twhile (segs) {\n\t\t\tskb2 = segs->next;\n\t\t\tskb_mark_not_on_list(segs);\n\t\t\tqdisc_skb_cb(segs)->pkt_len = segs->len;\n\t\t\tlast_len = segs->len;\n\t\t\trc = qdisc_enqueue(segs, sch, to_free);\n\t\t\tif (rc != NET_XMIT_SUCCESS) {\n\t\t\t\tif (net_xmit_drop_count(rc))\n\t\t\t\t\tqdisc_qstats_drop(sch);\n\t\t\t} else {\n\t\t\t\tnb++;\n\t\t\t\tlen += last_len;\n\t\t\t}\n\t\t\tsegs = skb2;\n\t\t}\n\t\t \n\t\tqdisc_tree_reduce_backlog(sch, -(nb - 1), -(len - prev_len));\n\t} else if (!skb) {\n\t\treturn NET_XMIT_DROP;\n\t}\n\treturn NET_XMIT_SUCCESS;\n}\n\n \n\nstatic void get_slot_next(struct netem_sched_data *q, u64 now)\n{\n\ts64 next_delay;\n\n\tif (!q->slot_dist)\n\t\tnext_delay = q->slot_config.min_delay +\n\t\t\t\t(get_random_u32() *\n\t\t\t\t (q->slot_config.max_delay -\n\t\t\t\t  q->slot_config.min_delay) >> 32);\n\telse\n\t\tnext_delay = tabledist(q->slot_config.dist_delay,\n\t\t\t\t       (s32)(q->slot_config.dist_jitter),\n\t\t\t\t       NULL, &q->prng, q->slot_dist);\n\n\tq->slot.slot_next = now + next_delay;\n\tq->slot.packets_left = q->slot_config.max_packets;\n\tq->slot.bytes_left = q->slot_config.max_bytes;\n}\n\nstatic struct sk_buff *netem_peek(struct netem_sched_data *q)\n{\n\tstruct sk_buff *skb = skb_rb_first(&q->t_root);\n\tu64 t1, t2;\n\n\tif (!skb)\n\t\treturn q->t_head;\n\tif (!q->t_head)\n\t\treturn skb;\n\n\tt1 = netem_skb_cb(skb)->time_to_send;\n\tt2 = netem_skb_cb(q->t_head)->time_to_send;\n\tif (t1 < t2)\n\t\treturn skb;\n\treturn q->t_head;\n}\n\nstatic void netem_erase_head(struct netem_sched_data *q, struct sk_buff *skb)\n{\n\tif (skb == q->t_head) {\n\t\tq->t_head = skb->next;\n\t\tif (!q->t_head)\n\t\t\tq->t_tail = NULL;\n\t} else {\n\t\trb_erase(&skb->rbnode, &q->t_root);\n\t}\n}\n\nstatic struct sk_buff *netem_dequeue(struct Qdisc *sch)\n{\n\tstruct netem_sched_data *q = qdisc_priv(sch);\n\tstruct sk_buff *skb;\n\ntfifo_dequeue:\n\tskb = __qdisc_dequeue_head(&sch->q);\n\tif (skb) {\n\t\tqdisc_qstats_backlog_dec(sch, skb);\ndeliver:\n\t\tqdisc_bstats_update(sch, skb);\n\t\treturn skb;\n\t}\n\tskb = netem_peek(q);\n\tif (skb) {\n\t\tu64 time_to_send;\n\t\tu64 now = ktime_get_ns();\n\n\t\t \n\t\ttime_to_send = netem_skb_cb(skb)->time_to_send;\n\t\tif (q->slot.slot_next && q->slot.slot_next < time_to_send)\n\t\t\tget_slot_next(q, now);\n\n\t\tif (time_to_send <= now && q->slot.slot_next <= now) {\n\t\t\tnetem_erase_head(q, skb);\n\t\t\tsch->q.qlen--;\n\t\t\tqdisc_qstats_backlog_dec(sch, skb);\n\t\t\tskb->next = NULL;\n\t\t\tskb->prev = NULL;\n\t\t\t \n\t\t\tskb->dev = qdisc_dev(sch);\n\n\t\t\tif (q->slot.slot_next) {\n\t\t\t\tq->slot.packets_left--;\n\t\t\t\tq->slot.bytes_left -= qdisc_pkt_len(skb);\n\t\t\t\tif (q->slot.packets_left <= 0 ||\n\t\t\t\t    q->slot.bytes_left <= 0)\n\t\t\t\t\tget_slot_next(q, now);\n\t\t\t}\n\n\t\t\tif (q->qdisc) {\n\t\t\t\tunsigned int pkt_len = qdisc_pkt_len(skb);\n\t\t\t\tstruct sk_buff *to_free = NULL;\n\t\t\t\tint err;\n\n\t\t\t\terr = qdisc_enqueue(skb, q->qdisc, &to_free);\n\t\t\t\tkfree_skb_list(to_free);\n\t\t\t\tif (err != NET_XMIT_SUCCESS &&\n\t\t\t\t    net_xmit_drop_count(err)) {\n\t\t\t\t\tqdisc_qstats_drop(sch);\n\t\t\t\t\tqdisc_tree_reduce_backlog(sch, 1,\n\t\t\t\t\t\t\t\t  pkt_len);\n\t\t\t\t}\n\t\t\t\tgoto tfifo_dequeue;\n\t\t\t}\n\t\t\tgoto deliver;\n\t\t}\n\n\t\tif (q->qdisc) {\n\t\t\tskb = q->qdisc->ops->dequeue(q->qdisc);\n\t\t\tif (skb)\n\t\t\t\tgoto deliver;\n\t\t}\n\n\t\tqdisc_watchdog_schedule_ns(&q->watchdog,\n\t\t\t\t\t   max(time_to_send,\n\t\t\t\t\t       q->slot.slot_next));\n\t}\n\n\tif (q->qdisc) {\n\t\tskb = q->qdisc->ops->dequeue(q->qdisc);\n\t\tif (skb)\n\t\t\tgoto deliver;\n\t}\n\treturn NULL;\n}\n\nstatic void netem_reset(struct Qdisc *sch)\n{\n\tstruct netem_sched_data *q = qdisc_priv(sch);\n\n\tqdisc_reset_queue(sch);\n\ttfifo_reset(sch);\n\tif (q->qdisc)\n\t\tqdisc_reset(q->qdisc);\n\tqdisc_watchdog_cancel(&q->watchdog);\n}\n\nstatic void dist_free(struct disttable *d)\n{\n\tkvfree(d);\n}\n\n \n\nstatic int get_dist_table(struct disttable **tbl, const struct nlattr *attr)\n{\n\tsize_t n = nla_len(attr)/sizeof(__s16);\n\tconst __s16 *data = nla_data(attr);\n\tstruct disttable *d;\n\tint i;\n\n\tif (!n || n > NETEM_DIST_MAX)\n\t\treturn -EINVAL;\n\n\td = kvmalloc(struct_size(d, table, n), GFP_KERNEL);\n\tif (!d)\n\t\treturn -ENOMEM;\n\n\td->size = n;\n\tfor (i = 0; i < n; i++)\n\t\td->table[i] = data[i];\n\n\t*tbl = d;\n\treturn 0;\n}\n\nstatic void get_slot(struct netem_sched_data *q, const struct nlattr *attr)\n{\n\tconst struct tc_netem_slot *c = nla_data(attr);\n\n\tq->slot_config = *c;\n\tif (q->slot_config.max_packets == 0)\n\t\tq->slot_config.max_packets = INT_MAX;\n\tif (q->slot_config.max_bytes == 0)\n\t\tq->slot_config.max_bytes = INT_MAX;\n\n\t \n\tq->slot_config.dist_jitter = min_t(__s64, INT_MAX, abs(q->slot_config.dist_jitter));\n\n\tq->slot.packets_left = q->slot_config.max_packets;\n\tq->slot.bytes_left = q->slot_config.max_bytes;\n\tif (q->slot_config.min_delay | q->slot_config.max_delay |\n\t    q->slot_config.dist_jitter)\n\t\tq->slot.slot_next = ktime_get_ns();\n\telse\n\t\tq->slot.slot_next = 0;\n}\n\nstatic void get_correlation(struct netem_sched_data *q, const struct nlattr *attr)\n{\n\tconst struct tc_netem_corr *c = nla_data(attr);\n\n\tinit_crandom(&q->delay_cor, c->delay_corr);\n\tinit_crandom(&q->loss_cor, c->loss_corr);\n\tinit_crandom(&q->dup_cor, c->dup_corr);\n}\n\nstatic void get_reorder(struct netem_sched_data *q, const struct nlattr *attr)\n{\n\tconst struct tc_netem_reorder *r = nla_data(attr);\n\n\tq->reorder = r->probability;\n\tinit_crandom(&q->reorder_cor, r->correlation);\n}\n\nstatic void get_corrupt(struct netem_sched_data *q, const struct nlattr *attr)\n{\n\tconst struct tc_netem_corrupt *r = nla_data(attr);\n\n\tq->corrupt = r->probability;\n\tinit_crandom(&q->corrupt_cor, r->correlation);\n}\n\nstatic void get_rate(struct netem_sched_data *q, const struct nlattr *attr)\n{\n\tconst struct tc_netem_rate *r = nla_data(attr);\n\n\tq->rate = r->rate;\n\tq->packet_overhead = r->packet_overhead;\n\tq->cell_size = r->cell_size;\n\tq->cell_overhead = r->cell_overhead;\n\tif (q->cell_size)\n\t\tq->cell_size_reciprocal = reciprocal_value(q->cell_size);\n\telse\n\t\tq->cell_size_reciprocal = (struct reciprocal_value) { 0 };\n}\n\nstatic int get_loss_clg(struct netem_sched_data *q, const struct nlattr *attr)\n{\n\tconst struct nlattr *la;\n\tint rem;\n\n\tnla_for_each_nested(la, attr, rem) {\n\t\tu16 type = nla_type(la);\n\n\t\tswitch (type) {\n\t\tcase NETEM_LOSS_GI: {\n\t\t\tconst struct tc_netem_gimodel *gi = nla_data(la);\n\n\t\t\tif (nla_len(la) < sizeof(struct tc_netem_gimodel)) {\n\t\t\t\tpr_info(\"netem: incorrect gi model size\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tq->loss_model = CLG_4_STATES;\n\n\t\t\tq->clg.state = TX_IN_GAP_PERIOD;\n\t\t\tq->clg.a1 = gi->p13;\n\t\t\tq->clg.a2 = gi->p31;\n\t\t\tq->clg.a3 = gi->p32;\n\t\t\tq->clg.a4 = gi->p14;\n\t\t\tq->clg.a5 = gi->p23;\n\t\t\tbreak;\n\t\t}\n\n\t\tcase NETEM_LOSS_GE: {\n\t\t\tconst struct tc_netem_gemodel *ge = nla_data(la);\n\n\t\t\tif (nla_len(la) < sizeof(struct tc_netem_gemodel)) {\n\t\t\t\tpr_info(\"netem: incorrect ge model size\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tq->loss_model = CLG_GILB_ELL;\n\t\t\tq->clg.state = GOOD_STATE;\n\t\t\tq->clg.a1 = ge->p;\n\t\t\tq->clg.a2 = ge->r;\n\t\t\tq->clg.a3 = ge->h;\n\t\t\tq->clg.a4 = ge->k1;\n\t\t\tbreak;\n\t\t}\n\n\t\tdefault:\n\t\t\tpr_info(\"netem: unknown loss type %u\\n\", type);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic const struct nla_policy netem_policy[TCA_NETEM_MAX + 1] = {\n\t[TCA_NETEM_CORR]\t= { .len = sizeof(struct tc_netem_corr) },\n\t[TCA_NETEM_REORDER]\t= { .len = sizeof(struct tc_netem_reorder) },\n\t[TCA_NETEM_CORRUPT]\t= { .len = sizeof(struct tc_netem_corrupt) },\n\t[TCA_NETEM_RATE]\t= { .len = sizeof(struct tc_netem_rate) },\n\t[TCA_NETEM_LOSS]\t= { .type = NLA_NESTED },\n\t[TCA_NETEM_ECN]\t\t= { .type = NLA_U32 },\n\t[TCA_NETEM_RATE64]\t= { .type = NLA_U64 },\n\t[TCA_NETEM_LATENCY64]\t= { .type = NLA_S64 },\n\t[TCA_NETEM_JITTER64]\t= { .type = NLA_S64 },\n\t[TCA_NETEM_SLOT]\t= { .len = sizeof(struct tc_netem_slot) },\n\t[TCA_NETEM_PRNG_SEED]\t= { .type = NLA_U64 },\n};\n\nstatic int parse_attr(struct nlattr *tb[], int maxtype, struct nlattr *nla,\n\t\t      const struct nla_policy *policy, int len)\n{\n\tint nested_len = nla_len(nla) - NLA_ALIGN(len);\n\n\tif (nested_len < 0) {\n\t\tpr_info(\"netem: invalid attributes len %d\\n\", nested_len);\n\t\treturn -EINVAL;\n\t}\n\n\tif (nested_len >= nla_attr_size(0))\n\t\treturn nla_parse_deprecated(tb, maxtype,\n\t\t\t\t\t    nla_data(nla) + NLA_ALIGN(len),\n\t\t\t\t\t    nested_len, policy, NULL);\n\n\tmemset(tb, 0, sizeof(struct nlattr *) * (maxtype + 1));\n\treturn 0;\n}\n\n \nstatic int netem_change(struct Qdisc *sch, struct nlattr *opt,\n\t\t\tstruct netlink_ext_ack *extack)\n{\n\tstruct netem_sched_data *q = qdisc_priv(sch);\n\tstruct nlattr *tb[TCA_NETEM_MAX + 1];\n\tstruct disttable *delay_dist = NULL;\n\tstruct disttable *slot_dist = NULL;\n\tstruct tc_netem_qopt *qopt;\n\tstruct clgstate old_clg;\n\tint old_loss_model = CLG_RANDOM;\n\tint ret;\n\n\tqopt = nla_data(opt);\n\tret = parse_attr(tb, TCA_NETEM_MAX, opt, netem_policy, sizeof(*qopt));\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (tb[TCA_NETEM_DELAY_DIST]) {\n\t\tret = get_dist_table(&delay_dist, tb[TCA_NETEM_DELAY_DIST]);\n\t\tif (ret)\n\t\t\tgoto table_free;\n\t}\n\n\tif (tb[TCA_NETEM_SLOT_DIST]) {\n\t\tret = get_dist_table(&slot_dist, tb[TCA_NETEM_SLOT_DIST]);\n\t\tif (ret)\n\t\t\tgoto table_free;\n\t}\n\n\tsch_tree_lock(sch);\n\t \n\told_clg = q->clg;\n\told_loss_model = q->loss_model;\n\n\tif (tb[TCA_NETEM_LOSS]) {\n\t\tret = get_loss_clg(q, tb[TCA_NETEM_LOSS]);\n\t\tif (ret) {\n\t\t\tq->loss_model = old_loss_model;\n\t\t\tq->clg = old_clg;\n\t\t\tgoto unlock;\n\t\t}\n\t} else {\n\t\tq->loss_model = CLG_RANDOM;\n\t}\n\n\tif (delay_dist)\n\t\tswap(q->delay_dist, delay_dist);\n\tif (slot_dist)\n\t\tswap(q->slot_dist, slot_dist);\n\tsch->limit = qopt->limit;\n\n\tq->latency = PSCHED_TICKS2NS(qopt->latency);\n\tq->jitter = PSCHED_TICKS2NS(qopt->jitter);\n\tq->limit = qopt->limit;\n\tq->gap = qopt->gap;\n\tq->counter = 0;\n\tq->loss = qopt->loss;\n\tq->duplicate = qopt->duplicate;\n\n\t \n\tif (q->gap)\n\t\tq->reorder = ~0;\n\n\tif (tb[TCA_NETEM_CORR])\n\t\tget_correlation(q, tb[TCA_NETEM_CORR]);\n\n\tif (tb[TCA_NETEM_REORDER])\n\t\tget_reorder(q, tb[TCA_NETEM_REORDER]);\n\n\tif (tb[TCA_NETEM_CORRUPT])\n\t\tget_corrupt(q, tb[TCA_NETEM_CORRUPT]);\n\n\tif (tb[TCA_NETEM_RATE])\n\t\tget_rate(q, tb[TCA_NETEM_RATE]);\n\n\tif (tb[TCA_NETEM_RATE64])\n\t\tq->rate = max_t(u64, q->rate,\n\t\t\t\tnla_get_u64(tb[TCA_NETEM_RATE64]));\n\n\tif (tb[TCA_NETEM_LATENCY64])\n\t\tq->latency = nla_get_s64(tb[TCA_NETEM_LATENCY64]);\n\n\tif (tb[TCA_NETEM_JITTER64])\n\t\tq->jitter = nla_get_s64(tb[TCA_NETEM_JITTER64]);\n\n\tif (tb[TCA_NETEM_ECN])\n\t\tq->ecn = nla_get_u32(tb[TCA_NETEM_ECN]);\n\n\tif (tb[TCA_NETEM_SLOT])\n\t\tget_slot(q, tb[TCA_NETEM_SLOT]);\n\n\t \n\tq->jitter = min_t(s64, abs(q->jitter), INT_MAX);\n\n\tif (tb[TCA_NETEM_PRNG_SEED])\n\t\tq->prng.seed = nla_get_u64(tb[TCA_NETEM_PRNG_SEED]);\n\telse\n\t\tq->prng.seed = get_random_u64();\n\tprandom_seed_state(&q->prng.prng_state, q->prng.seed);\n\nunlock:\n\tsch_tree_unlock(sch);\n\ntable_free:\n\tdist_free(delay_dist);\n\tdist_free(slot_dist);\n\treturn ret;\n}\n\nstatic int netem_init(struct Qdisc *sch, struct nlattr *opt,\n\t\t      struct netlink_ext_ack *extack)\n{\n\tstruct netem_sched_data *q = qdisc_priv(sch);\n\tint ret;\n\n\tqdisc_watchdog_init(&q->watchdog, sch);\n\n\tif (!opt)\n\t\treturn -EINVAL;\n\n\tq->loss_model = CLG_RANDOM;\n\tret = netem_change(sch, opt, extack);\n\tif (ret)\n\t\tpr_info(\"netem: change failed\\n\");\n\treturn ret;\n}\n\nstatic void netem_destroy(struct Qdisc *sch)\n{\n\tstruct netem_sched_data *q = qdisc_priv(sch);\n\n\tqdisc_watchdog_cancel(&q->watchdog);\n\tif (q->qdisc)\n\t\tqdisc_put(q->qdisc);\n\tdist_free(q->delay_dist);\n\tdist_free(q->slot_dist);\n}\n\nstatic int dump_loss_model(const struct netem_sched_data *q,\n\t\t\t   struct sk_buff *skb)\n{\n\tstruct nlattr *nest;\n\n\tnest = nla_nest_start_noflag(skb, TCA_NETEM_LOSS);\n\tif (nest == NULL)\n\t\tgoto nla_put_failure;\n\n\tswitch (q->loss_model) {\n\tcase CLG_RANDOM:\n\t\t \n\t\tnla_nest_cancel(skb, nest);\n\t\treturn 0;\t \n\n\tcase CLG_4_STATES: {\n\t\tstruct tc_netem_gimodel gi = {\n\t\t\t.p13 = q->clg.a1,\n\t\t\t.p31 = q->clg.a2,\n\t\t\t.p32 = q->clg.a3,\n\t\t\t.p14 = q->clg.a4,\n\t\t\t.p23 = q->clg.a5,\n\t\t};\n\n\t\tif (nla_put(skb, NETEM_LOSS_GI, sizeof(gi), &gi))\n\t\t\tgoto nla_put_failure;\n\t\tbreak;\n\t}\n\tcase CLG_GILB_ELL: {\n\t\tstruct tc_netem_gemodel ge = {\n\t\t\t.p = q->clg.a1,\n\t\t\t.r = q->clg.a2,\n\t\t\t.h = q->clg.a3,\n\t\t\t.k1 = q->clg.a4,\n\t\t};\n\n\t\tif (nla_put(skb, NETEM_LOSS_GE, sizeof(ge), &ge))\n\t\t\tgoto nla_put_failure;\n\t\tbreak;\n\t}\n\t}\n\n\tnla_nest_end(skb, nest);\n\treturn 0;\n\nnla_put_failure:\n\tnla_nest_cancel(skb, nest);\n\treturn -1;\n}\n\nstatic int netem_dump(struct Qdisc *sch, struct sk_buff *skb)\n{\n\tconst struct netem_sched_data *q = qdisc_priv(sch);\n\tstruct nlattr *nla = (struct nlattr *) skb_tail_pointer(skb);\n\tstruct tc_netem_qopt qopt;\n\tstruct tc_netem_corr cor;\n\tstruct tc_netem_reorder reorder;\n\tstruct tc_netem_corrupt corrupt;\n\tstruct tc_netem_rate rate;\n\tstruct tc_netem_slot slot;\n\n\tqopt.latency = min_t(psched_time_t, PSCHED_NS2TICKS(q->latency),\n\t\t\t     UINT_MAX);\n\tqopt.jitter = min_t(psched_time_t, PSCHED_NS2TICKS(q->jitter),\n\t\t\t    UINT_MAX);\n\tqopt.limit = q->limit;\n\tqopt.loss = q->loss;\n\tqopt.gap = q->gap;\n\tqopt.duplicate = q->duplicate;\n\tif (nla_put(skb, TCA_OPTIONS, sizeof(qopt), &qopt))\n\t\tgoto nla_put_failure;\n\n\tif (nla_put(skb, TCA_NETEM_LATENCY64, sizeof(q->latency), &q->latency))\n\t\tgoto nla_put_failure;\n\n\tif (nla_put(skb, TCA_NETEM_JITTER64, sizeof(q->jitter), &q->jitter))\n\t\tgoto nla_put_failure;\n\n\tcor.delay_corr = q->delay_cor.rho;\n\tcor.loss_corr = q->loss_cor.rho;\n\tcor.dup_corr = q->dup_cor.rho;\n\tif (nla_put(skb, TCA_NETEM_CORR, sizeof(cor), &cor))\n\t\tgoto nla_put_failure;\n\n\treorder.probability = q->reorder;\n\treorder.correlation = q->reorder_cor.rho;\n\tif (nla_put(skb, TCA_NETEM_REORDER, sizeof(reorder), &reorder))\n\t\tgoto nla_put_failure;\n\n\tcorrupt.probability = q->corrupt;\n\tcorrupt.correlation = q->corrupt_cor.rho;\n\tif (nla_put(skb, TCA_NETEM_CORRUPT, sizeof(corrupt), &corrupt))\n\t\tgoto nla_put_failure;\n\n\tif (q->rate >= (1ULL << 32)) {\n\t\tif (nla_put_u64_64bit(skb, TCA_NETEM_RATE64, q->rate,\n\t\t\t\t      TCA_NETEM_PAD))\n\t\t\tgoto nla_put_failure;\n\t\trate.rate = ~0U;\n\t} else {\n\t\trate.rate = q->rate;\n\t}\n\trate.packet_overhead = q->packet_overhead;\n\trate.cell_size = q->cell_size;\n\trate.cell_overhead = q->cell_overhead;\n\tif (nla_put(skb, TCA_NETEM_RATE, sizeof(rate), &rate))\n\t\tgoto nla_put_failure;\n\n\tif (q->ecn && nla_put_u32(skb, TCA_NETEM_ECN, q->ecn))\n\t\tgoto nla_put_failure;\n\n\tif (dump_loss_model(q, skb) != 0)\n\t\tgoto nla_put_failure;\n\n\tif (q->slot_config.min_delay | q->slot_config.max_delay |\n\t    q->slot_config.dist_jitter) {\n\t\tslot = q->slot_config;\n\t\tif (slot.max_packets == INT_MAX)\n\t\t\tslot.max_packets = 0;\n\t\tif (slot.max_bytes == INT_MAX)\n\t\t\tslot.max_bytes = 0;\n\t\tif (nla_put(skb, TCA_NETEM_SLOT, sizeof(slot), &slot))\n\t\t\tgoto nla_put_failure;\n\t}\n\n\tif (nla_put_u64_64bit(skb, TCA_NETEM_PRNG_SEED, q->prng.seed,\n\t\t\t      TCA_NETEM_PAD))\n\t\tgoto nla_put_failure;\n\n\treturn nla_nest_end(skb, nla);\n\nnla_put_failure:\n\tnlmsg_trim(skb, nla);\n\treturn -1;\n}\n\nstatic int netem_dump_class(struct Qdisc *sch, unsigned long cl,\n\t\t\t  struct sk_buff *skb, struct tcmsg *tcm)\n{\n\tstruct netem_sched_data *q = qdisc_priv(sch);\n\n\tif (cl != 1 || !q->qdisc) \t \n\t\treturn -ENOENT;\n\n\ttcm->tcm_handle |= TC_H_MIN(1);\n\ttcm->tcm_info = q->qdisc->handle;\n\n\treturn 0;\n}\n\nstatic int netem_graft(struct Qdisc *sch, unsigned long arg, struct Qdisc *new,\n\t\t     struct Qdisc **old, struct netlink_ext_ack *extack)\n{\n\tstruct netem_sched_data *q = qdisc_priv(sch);\n\n\t*old = qdisc_replace(sch, new, &q->qdisc);\n\treturn 0;\n}\n\nstatic struct Qdisc *netem_leaf(struct Qdisc *sch, unsigned long arg)\n{\n\tstruct netem_sched_data *q = qdisc_priv(sch);\n\treturn q->qdisc;\n}\n\nstatic unsigned long netem_find(struct Qdisc *sch, u32 classid)\n{\n\treturn 1;\n}\n\nstatic void netem_walk(struct Qdisc *sch, struct qdisc_walker *walker)\n{\n\tif (!walker->stop) {\n\t\tif (!tc_qdisc_stats_dump(sch, 1, walker))\n\t\t\treturn;\n\t}\n}\n\nstatic const struct Qdisc_class_ops netem_class_ops = {\n\t.graft\t\t=\tnetem_graft,\n\t.leaf\t\t=\tnetem_leaf,\n\t.find\t\t=\tnetem_find,\n\t.walk\t\t=\tnetem_walk,\n\t.dump\t\t=\tnetem_dump_class,\n};\n\nstatic struct Qdisc_ops netem_qdisc_ops __read_mostly = {\n\t.id\t\t=\t\"netem\",\n\t.cl_ops\t\t=\t&netem_class_ops,\n\t.priv_size\t=\tsizeof(struct netem_sched_data),\n\t.enqueue\t=\tnetem_enqueue,\n\t.dequeue\t=\tnetem_dequeue,\n\t.peek\t\t=\tqdisc_peek_dequeued,\n\t.init\t\t=\tnetem_init,\n\t.reset\t\t=\tnetem_reset,\n\t.destroy\t=\tnetem_destroy,\n\t.change\t\t=\tnetem_change,\n\t.dump\t\t=\tnetem_dump,\n\t.owner\t\t=\tTHIS_MODULE,\n};\n\n\nstatic int __init netem_module_init(void)\n{\n\tpr_info(\"netem: version \" VERSION \"\\n\");\n\treturn register_qdisc(&netem_qdisc_ops);\n}\nstatic void __exit netem_module_exit(void)\n{\n\tunregister_qdisc(&netem_qdisc_ops);\n}\nmodule_init(netem_module_init)\nmodule_exit(netem_module_exit)\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}