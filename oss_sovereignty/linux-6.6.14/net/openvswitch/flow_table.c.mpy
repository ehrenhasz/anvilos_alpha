{
  "module_name": "flow_table.c",
  "hash_id": "abedd770094db98981acc6fc584d14c27dd92a147f09439ce9c08807d8b4f68c",
  "original_prompt": "Ingested from linux-6.6.14/net/openvswitch/flow_table.c",
  "human_readable_source": "\n \n\n#include \"flow.h\"\n#include \"datapath.h\"\n#include \"flow_netlink.h\"\n#include <linux/uaccess.h>\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/if_ether.h>\n#include <linux/if_vlan.h>\n#include <net/llc_pdu.h>\n#include <linux/kernel.h>\n#include <linux/jhash.h>\n#include <linux/jiffies.h>\n#include <linux/llc.h>\n#include <linux/module.h>\n#include <linux/in.h>\n#include <linux/rcupdate.h>\n#include <linux/cpumask.h>\n#include <linux/if_arp.h>\n#include <linux/ip.h>\n#include <linux/ipv6.h>\n#include <linux/sctp.h>\n#include <linux/tcp.h>\n#include <linux/udp.h>\n#include <linux/icmp.h>\n#include <linux/icmpv6.h>\n#include <linux/rculist.h>\n#include <linux/sort.h>\n#include <net/ip.h>\n#include <net/ipv6.h>\n#include <net/ndisc.h>\n\n#define TBL_MIN_BUCKETS\t\t1024\n#define MASK_ARRAY_SIZE_MIN\t16\n#define REHASH_INTERVAL\t\t(10 * 60 * HZ)\n\n#define MC_DEFAULT_HASH_ENTRIES\t256\n#define MC_HASH_SHIFT\t\t8\n#define MC_HASH_SEGS\t\t((sizeof(uint32_t) * 8) / MC_HASH_SHIFT)\n\nstatic struct kmem_cache *flow_cache;\nstruct kmem_cache *flow_stats_cache __read_mostly;\n\nstatic u16 range_n_bytes(const struct sw_flow_key_range *range)\n{\n\treturn range->end - range->start;\n}\n\nvoid ovs_flow_mask_key(struct sw_flow_key *dst, const struct sw_flow_key *src,\n\t\t       bool full, const struct sw_flow_mask *mask)\n{\n\tint start = full ? 0 : mask->range.start;\n\tint len = full ? sizeof *dst : range_n_bytes(&mask->range);\n\tconst long *m = (const long *)((const u8 *)&mask->key + start);\n\tconst long *s = (const long *)((const u8 *)src + start);\n\tlong *d = (long *)((u8 *)dst + start);\n\tint i;\n\n\t \n\tfor (i = 0; i < len; i += sizeof(long))\n\t\t*d++ = *s++ & *m++;\n}\n\nstruct sw_flow *ovs_flow_alloc(void)\n{\n\tstruct sw_flow *flow;\n\tstruct sw_flow_stats *stats;\n\n\tflow = kmem_cache_zalloc(flow_cache, GFP_KERNEL);\n\tif (!flow)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tflow->stats_last_writer = -1;\n\tflow->cpu_used_mask = (struct cpumask *)&flow->stats[nr_cpu_ids];\n\n\t \n\tstats = kmem_cache_alloc_node(flow_stats_cache,\n\t\t\t\t      GFP_KERNEL | __GFP_ZERO,\n\t\t\t\t      node_online(0) ? 0 : NUMA_NO_NODE);\n\tif (!stats)\n\t\tgoto err;\n\n\tspin_lock_init(&stats->lock);\n\n\tRCU_INIT_POINTER(flow->stats[0], stats);\n\n\tcpumask_set_cpu(0, flow->cpu_used_mask);\n\n\treturn flow;\nerr:\n\tkmem_cache_free(flow_cache, flow);\n\treturn ERR_PTR(-ENOMEM);\n}\n\nint ovs_flow_tbl_count(const struct flow_table *table)\n{\n\treturn table->count;\n}\n\nstatic void flow_free(struct sw_flow *flow)\n{\n\tint cpu;\n\n\tif (ovs_identifier_is_key(&flow->id))\n\t\tkfree(flow->id.unmasked_key);\n\tif (flow->sf_acts)\n\t\tovs_nla_free_flow_actions((struct sw_flow_actions __force *)\n\t\t\t\t\t  flow->sf_acts);\n\t \n\tfor (cpu = 0; cpu < nr_cpu_ids;\n\t     cpu = cpumask_next(cpu, flow->cpu_used_mask)) {\n\t\tif (flow->stats[cpu])\n\t\t\tkmem_cache_free(flow_stats_cache,\n\t\t\t\t\t(struct sw_flow_stats __force *)flow->stats[cpu]);\n\t}\n\n\tkmem_cache_free(flow_cache, flow);\n}\n\nstatic void rcu_free_flow_callback(struct rcu_head *rcu)\n{\n\tstruct sw_flow *flow = container_of(rcu, struct sw_flow, rcu);\n\n\tflow_free(flow);\n}\n\nvoid ovs_flow_free(struct sw_flow *flow, bool deferred)\n{\n\tif (!flow)\n\t\treturn;\n\n\tif (deferred)\n\t\tcall_rcu(&flow->rcu, rcu_free_flow_callback);\n\telse\n\t\tflow_free(flow);\n}\n\nstatic void __table_instance_destroy(struct table_instance *ti)\n{\n\tkvfree(ti->buckets);\n\tkfree(ti);\n}\n\nstatic struct table_instance *table_instance_alloc(int new_size)\n{\n\tstruct table_instance *ti = kmalloc(sizeof(*ti), GFP_KERNEL);\n\tint i;\n\n\tif (!ti)\n\t\treturn NULL;\n\n\tti->buckets = kvmalloc_array(new_size, sizeof(struct hlist_head),\n\t\t\t\t     GFP_KERNEL);\n\tif (!ti->buckets) {\n\t\tkfree(ti);\n\t\treturn NULL;\n\t}\n\n\tfor (i = 0; i < new_size; i++)\n\t\tINIT_HLIST_HEAD(&ti->buckets[i]);\n\n\tti->n_buckets = new_size;\n\tti->node_ver = 0;\n\tget_random_bytes(&ti->hash_seed, sizeof(u32));\n\n\treturn ti;\n}\n\nstatic void __mask_array_destroy(struct mask_array *ma)\n{\n\tfree_percpu(ma->masks_usage_stats);\n\tkfree(ma);\n}\n\nstatic void mask_array_rcu_cb(struct rcu_head *rcu)\n{\n\tstruct mask_array *ma = container_of(rcu, struct mask_array, rcu);\n\n\t__mask_array_destroy(ma);\n}\n\nstatic void tbl_mask_array_reset_counters(struct mask_array *ma)\n{\n\tint i, cpu;\n\n\t \n\tfor (i = 0; i < ma->max; i++) {\n\t\tma->masks_usage_zero_cntr[i] = 0;\n\n\t\tfor_each_possible_cpu(cpu) {\n\t\t\tstruct mask_array_stats *stats;\n\t\t\tunsigned int start;\n\t\t\tu64 counter;\n\n\t\t\tstats = per_cpu_ptr(ma->masks_usage_stats, cpu);\n\t\t\tdo {\n\t\t\t\tstart = u64_stats_fetch_begin(&stats->syncp);\n\t\t\t\tcounter = stats->usage_cntrs[i];\n\t\t\t} while (u64_stats_fetch_retry(&stats->syncp, start));\n\n\t\t\tma->masks_usage_zero_cntr[i] += counter;\n\t\t}\n\t}\n}\n\nstatic struct mask_array *tbl_mask_array_alloc(int size)\n{\n\tstruct mask_array *new;\n\n\tsize = max(MASK_ARRAY_SIZE_MIN, size);\n\tnew = kzalloc(sizeof(struct mask_array) +\n\t\t      sizeof(struct sw_flow_mask *) * size +\n\t\t      sizeof(u64) * size, GFP_KERNEL);\n\tif (!new)\n\t\treturn NULL;\n\n\tnew->masks_usage_zero_cntr = (u64 *)((u8 *)new +\n\t\t\t\t\t     sizeof(struct mask_array) +\n\t\t\t\t\t     sizeof(struct sw_flow_mask *) *\n\t\t\t\t\t     size);\n\n\tnew->masks_usage_stats = __alloc_percpu(sizeof(struct mask_array_stats) +\n\t\t\t\t\t\tsizeof(u64) * size,\n\t\t\t\t\t\t__alignof__(u64));\n\tif (!new->masks_usage_stats) {\n\t\tkfree(new);\n\t\treturn NULL;\n\t}\n\n\tnew->count = 0;\n\tnew->max = size;\n\n\treturn new;\n}\n\nstatic int tbl_mask_array_realloc(struct flow_table *tbl, int size)\n{\n\tstruct mask_array *old;\n\tstruct mask_array *new;\n\n\tnew = tbl_mask_array_alloc(size);\n\tif (!new)\n\t\treturn -ENOMEM;\n\n\told = ovsl_dereference(tbl->mask_array);\n\tif (old) {\n\t\tint i;\n\n\t\tfor (i = 0; i < old->max; i++) {\n\t\t\tif (ovsl_dereference(old->masks[i]))\n\t\t\t\tnew->masks[new->count++] = old->masks[i];\n\t\t}\n\t\tcall_rcu(&old->rcu, mask_array_rcu_cb);\n\t}\n\n\trcu_assign_pointer(tbl->mask_array, new);\n\n\treturn 0;\n}\n\nstatic int tbl_mask_array_add_mask(struct flow_table *tbl,\n\t\t\t\t   struct sw_flow_mask *new)\n{\n\tstruct mask_array *ma = ovsl_dereference(tbl->mask_array);\n\tint err, ma_count = READ_ONCE(ma->count);\n\n\tif (ma_count >= ma->max) {\n\t\terr = tbl_mask_array_realloc(tbl, ma->max +\n\t\t\t\t\t\t  MASK_ARRAY_SIZE_MIN);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tma = ovsl_dereference(tbl->mask_array);\n\t} else {\n\t\t \n\t\ttbl_mask_array_reset_counters(ma);\n\t}\n\n\tBUG_ON(ovsl_dereference(ma->masks[ma_count]));\n\n\trcu_assign_pointer(ma->masks[ma_count], new);\n\tWRITE_ONCE(ma->count, ma_count + 1);\n\n\treturn 0;\n}\n\nstatic void tbl_mask_array_del_mask(struct flow_table *tbl,\n\t\t\t\t    struct sw_flow_mask *mask)\n{\n\tstruct mask_array *ma = ovsl_dereference(tbl->mask_array);\n\tint i, ma_count = READ_ONCE(ma->count);\n\n\t \n\tfor (i = 0; i < ma_count; i++) {\n\t\tif (mask == ovsl_dereference(ma->masks[i]))\n\t\t\tgoto found;\n\t}\n\n\tBUG();\n\treturn;\n\nfound:\n\tWRITE_ONCE(ma->count, ma_count - 1);\n\n\trcu_assign_pointer(ma->masks[i], ma->masks[ma_count - 1]);\n\tRCU_INIT_POINTER(ma->masks[ma_count - 1], NULL);\n\n\tkfree_rcu(mask, rcu);\n\n\t \n\tif (ma->max >= (MASK_ARRAY_SIZE_MIN * 2) &&\n\t    ma_count <= (ma->max / 3))\n\t\ttbl_mask_array_realloc(tbl, ma->max / 2);\n\telse\n\t\ttbl_mask_array_reset_counters(ma);\n\n}\n\n \nstatic void flow_mask_remove(struct flow_table *tbl, struct sw_flow_mask *mask)\n{\n\tif (mask) {\n\t\t \n\t\tASSERT_OVSL();\n\t\tBUG_ON(!mask->ref_count);\n\t\tmask->ref_count--;\n\n\t\tif (!mask->ref_count)\n\t\t\ttbl_mask_array_del_mask(tbl, mask);\n\t}\n}\n\nstatic void __mask_cache_destroy(struct mask_cache *mc)\n{\n\tfree_percpu(mc->mask_cache);\n\tkfree(mc);\n}\n\nstatic void mask_cache_rcu_cb(struct rcu_head *rcu)\n{\n\tstruct mask_cache *mc = container_of(rcu, struct mask_cache, rcu);\n\n\t__mask_cache_destroy(mc);\n}\n\nstatic struct mask_cache *tbl_mask_cache_alloc(u32 size)\n{\n\tstruct mask_cache_entry __percpu *cache = NULL;\n\tstruct mask_cache *new;\n\n\t \n\tif ((!is_power_of_2(size) && size != 0) ||\n\t    (size * sizeof(struct mask_cache_entry)) > PCPU_MIN_UNIT_SIZE)\n\t\treturn NULL;\n\n\tnew = kzalloc(sizeof(*new), GFP_KERNEL);\n\tif (!new)\n\t\treturn NULL;\n\n\tnew->cache_size = size;\n\tif (new->cache_size > 0) {\n\t\tcache = __alloc_percpu(array_size(sizeof(struct mask_cache_entry),\n\t\t\t\t\t\t  new->cache_size),\n\t\t\t\t       __alignof__(struct mask_cache_entry));\n\t\tif (!cache) {\n\t\t\tkfree(new);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\n\tnew->mask_cache = cache;\n\treturn new;\n}\nint ovs_flow_tbl_masks_cache_resize(struct flow_table *table, u32 size)\n{\n\tstruct mask_cache *mc = rcu_dereference_ovsl(table->mask_cache);\n\tstruct mask_cache *new;\n\n\tif (size == mc->cache_size)\n\t\treturn 0;\n\n\tif ((!is_power_of_2(size) && size != 0) ||\n\t    (size * sizeof(struct mask_cache_entry)) > PCPU_MIN_UNIT_SIZE)\n\t\treturn -EINVAL;\n\n\tnew = tbl_mask_cache_alloc(size);\n\tif (!new)\n\t\treturn -ENOMEM;\n\n\trcu_assign_pointer(table->mask_cache, new);\n\tcall_rcu(&mc->rcu, mask_cache_rcu_cb);\n\n\treturn 0;\n}\n\nint ovs_flow_tbl_init(struct flow_table *table)\n{\n\tstruct table_instance *ti, *ufid_ti;\n\tstruct mask_cache *mc;\n\tstruct mask_array *ma;\n\n\tmc = tbl_mask_cache_alloc(MC_DEFAULT_HASH_ENTRIES);\n\tif (!mc)\n\t\treturn -ENOMEM;\n\n\tma = tbl_mask_array_alloc(MASK_ARRAY_SIZE_MIN);\n\tif (!ma)\n\t\tgoto free_mask_cache;\n\n\tti = table_instance_alloc(TBL_MIN_BUCKETS);\n\tif (!ti)\n\t\tgoto free_mask_array;\n\n\tufid_ti = table_instance_alloc(TBL_MIN_BUCKETS);\n\tif (!ufid_ti)\n\t\tgoto free_ti;\n\n\trcu_assign_pointer(table->ti, ti);\n\trcu_assign_pointer(table->ufid_ti, ufid_ti);\n\trcu_assign_pointer(table->mask_array, ma);\n\trcu_assign_pointer(table->mask_cache, mc);\n\ttable->last_rehash = jiffies;\n\ttable->count = 0;\n\ttable->ufid_count = 0;\n\treturn 0;\n\nfree_ti:\n\t__table_instance_destroy(ti);\nfree_mask_array:\n\t__mask_array_destroy(ma);\nfree_mask_cache:\n\t__mask_cache_destroy(mc);\n\treturn -ENOMEM;\n}\n\nstatic void flow_tbl_destroy_rcu_cb(struct rcu_head *rcu)\n{\n\tstruct table_instance *ti;\n\n\tti = container_of(rcu, struct table_instance, rcu);\n\t__table_instance_destroy(ti);\n}\n\nstatic void table_instance_flow_free(struct flow_table *table,\n\t\t\t\t     struct table_instance *ti,\n\t\t\t\t     struct table_instance *ufid_ti,\n\t\t\t\t     struct sw_flow *flow)\n{\n\thlist_del_rcu(&flow->flow_table.node[ti->node_ver]);\n\ttable->count--;\n\n\tif (ovs_identifier_is_ufid(&flow->id)) {\n\t\thlist_del_rcu(&flow->ufid_table.node[ufid_ti->node_ver]);\n\t\ttable->ufid_count--;\n\t}\n\n\tflow_mask_remove(table, flow->mask);\n}\n\n \nvoid table_instance_flow_flush(struct flow_table *table,\n\t\t\t       struct table_instance *ti,\n\t\t\t       struct table_instance *ufid_ti)\n{\n\tint i;\n\n\tfor (i = 0; i < ti->n_buckets; i++) {\n\t\tstruct hlist_head *head = &ti->buckets[i];\n\t\tstruct hlist_node *n;\n\t\tstruct sw_flow *flow;\n\n\t\thlist_for_each_entry_safe(flow, n, head,\n\t\t\t\t\t  flow_table.node[ti->node_ver]) {\n\n\t\t\ttable_instance_flow_free(table, ti, ufid_ti,\n\t\t\t\t\t\t flow);\n\t\t\tovs_flow_free(flow, true);\n\t\t}\n\t}\n\n\tif (WARN_ON(table->count != 0 ||\n\t\t    table->ufid_count != 0)) {\n\t\ttable->count = 0;\n\t\ttable->ufid_count = 0;\n\t}\n}\n\nstatic void table_instance_destroy(struct table_instance *ti,\n\t\t\t\t   struct table_instance *ufid_ti)\n{\n\tcall_rcu(&ti->rcu, flow_tbl_destroy_rcu_cb);\n\tcall_rcu(&ufid_ti->rcu, flow_tbl_destroy_rcu_cb);\n}\n\n \nvoid ovs_flow_tbl_destroy(struct flow_table *table)\n{\n\tstruct table_instance *ti = rcu_dereference_raw(table->ti);\n\tstruct table_instance *ufid_ti = rcu_dereference_raw(table->ufid_ti);\n\tstruct mask_cache *mc = rcu_dereference_raw(table->mask_cache);\n\tstruct mask_array *ma = rcu_dereference_raw(table->mask_array);\n\n\tcall_rcu(&mc->rcu, mask_cache_rcu_cb);\n\tcall_rcu(&ma->rcu, mask_array_rcu_cb);\n\ttable_instance_destroy(ti, ufid_ti);\n}\n\nstruct sw_flow *ovs_flow_tbl_dump_next(struct table_instance *ti,\n\t\t\t\t       u32 *bucket, u32 *last)\n{\n\tstruct sw_flow *flow;\n\tstruct hlist_head *head;\n\tint ver;\n\tint i;\n\n\tver = ti->node_ver;\n\twhile (*bucket < ti->n_buckets) {\n\t\ti = 0;\n\t\thead = &ti->buckets[*bucket];\n\t\thlist_for_each_entry_rcu(flow, head, flow_table.node[ver]) {\n\t\t\tif (i < *last) {\n\t\t\t\ti++;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\t*last = i + 1;\n\t\t\treturn flow;\n\t\t}\n\t\t(*bucket)++;\n\t\t*last = 0;\n\t}\n\n\treturn NULL;\n}\n\nstatic struct hlist_head *find_bucket(struct table_instance *ti, u32 hash)\n{\n\thash = jhash_1word(hash, ti->hash_seed);\n\treturn &ti->buckets[hash & (ti->n_buckets - 1)];\n}\n\nstatic void table_instance_insert(struct table_instance *ti,\n\t\t\t\t  struct sw_flow *flow)\n{\n\tstruct hlist_head *head;\n\n\thead = find_bucket(ti, flow->flow_table.hash);\n\thlist_add_head_rcu(&flow->flow_table.node[ti->node_ver], head);\n}\n\nstatic void ufid_table_instance_insert(struct table_instance *ti,\n\t\t\t\t       struct sw_flow *flow)\n{\n\tstruct hlist_head *head;\n\n\thead = find_bucket(ti, flow->ufid_table.hash);\n\thlist_add_head_rcu(&flow->ufid_table.node[ti->node_ver], head);\n}\n\nstatic void flow_table_copy_flows(struct table_instance *old,\n\t\t\t\t  struct table_instance *new, bool ufid)\n{\n\tint old_ver;\n\tint i;\n\n\told_ver = old->node_ver;\n\tnew->node_ver = !old_ver;\n\n\t \n\tfor (i = 0; i < old->n_buckets; i++) {\n\t\tstruct sw_flow *flow;\n\t\tstruct hlist_head *head = &old->buckets[i];\n\n\t\tif (ufid)\n\t\t\thlist_for_each_entry_rcu(flow, head,\n\t\t\t\t\t\t ufid_table.node[old_ver],\n\t\t\t\t\t\t lockdep_ovsl_is_held())\n\t\t\t\tufid_table_instance_insert(new, flow);\n\t\telse\n\t\t\thlist_for_each_entry_rcu(flow, head,\n\t\t\t\t\t\t flow_table.node[old_ver],\n\t\t\t\t\t\t lockdep_ovsl_is_held())\n\t\t\t\ttable_instance_insert(new, flow);\n\t}\n}\n\nstatic struct table_instance *table_instance_rehash(struct table_instance *ti,\n\t\t\t\t\t\t    int n_buckets, bool ufid)\n{\n\tstruct table_instance *new_ti;\n\n\tnew_ti = table_instance_alloc(n_buckets);\n\tif (!new_ti)\n\t\treturn NULL;\n\n\tflow_table_copy_flows(ti, new_ti, ufid);\n\n\treturn new_ti;\n}\n\nint ovs_flow_tbl_flush(struct flow_table *flow_table)\n{\n\tstruct table_instance *old_ti, *new_ti;\n\tstruct table_instance *old_ufid_ti, *new_ufid_ti;\n\n\tnew_ti = table_instance_alloc(TBL_MIN_BUCKETS);\n\tif (!new_ti)\n\t\treturn -ENOMEM;\n\tnew_ufid_ti = table_instance_alloc(TBL_MIN_BUCKETS);\n\tif (!new_ufid_ti)\n\t\tgoto err_free_ti;\n\n\told_ti = ovsl_dereference(flow_table->ti);\n\told_ufid_ti = ovsl_dereference(flow_table->ufid_ti);\n\n\trcu_assign_pointer(flow_table->ti, new_ti);\n\trcu_assign_pointer(flow_table->ufid_ti, new_ufid_ti);\n\tflow_table->last_rehash = jiffies;\n\n\ttable_instance_flow_flush(flow_table, old_ti, old_ufid_ti);\n\ttable_instance_destroy(old_ti, old_ufid_ti);\n\treturn 0;\n\nerr_free_ti:\n\t__table_instance_destroy(new_ti);\n\treturn -ENOMEM;\n}\n\nstatic u32 flow_hash(const struct sw_flow_key *key,\n\t\t     const struct sw_flow_key_range *range)\n{\n\tconst u32 *hash_key = (const u32 *)((const u8 *)key + range->start);\n\n\t \n\tint hash_u32s = range_n_bytes(range) >> 2;\n\n\treturn jhash2(hash_key, hash_u32s, 0);\n}\n\nstatic int flow_key_start(const struct sw_flow_key *key)\n{\n\tif (key->tun_proto)\n\t\treturn 0;\n\telse\n\t\treturn rounddown(offsetof(struct sw_flow_key, phy),\n\t\t\t\t sizeof(long));\n}\n\nstatic bool cmp_key(const struct sw_flow_key *key1,\n\t\t    const struct sw_flow_key *key2,\n\t\t    int key_start, int key_end)\n{\n\tconst long *cp1 = (const long *)((const u8 *)key1 + key_start);\n\tconst long *cp2 = (const long *)((const u8 *)key2 + key_start);\n\tint i;\n\n\tfor (i = key_start; i < key_end; i += sizeof(long))\n\t\tif (*cp1++ ^ *cp2++)\n\t\t\treturn false;\n\n\treturn true;\n}\n\nstatic bool flow_cmp_masked_key(const struct sw_flow *flow,\n\t\t\t\tconst struct sw_flow_key *key,\n\t\t\t\tconst struct sw_flow_key_range *range)\n{\n\treturn cmp_key(&flow->key, key, range->start, range->end);\n}\n\nstatic bool ovs_flow_cmp_unmasked_key(const struct sw_flow *flow,\n\t\t\t\t      const struct sw_flow_match *match)\n{\n\tstruct sw_flow_key *key = match->key;\n\tint key_start = flow_key_start(key);\n\tint key_end = match->range.end;\n\n\tBUG_ON(ovs_identifier_is_ufid(&flow->id));\n\treturn cmp_key(flow->id.unmasked_key, key, key_start, key_end);\n}\n\nstatic struct sw_flow *masked_flow_lookup(struct table_instance *ti,\n\t\t\t\t\t  const struct sw_flow_key *unmasked,\n\t\t\t\t\t  const struct sw_flow_mask *mask,\n\t\t\t\t\t  u32 *n_mask_hit)\n{\n\tstruct sw_flow *flow;\n\tstruct hlist_head *head;\n\tu32 hash;\n\tstruct sw_flow_key masked_key;\n\n\tovs_flow_mask_key(&masked_key, unmasked, false, mask);\n\thash = flow_hash(&masked_key, &mask->range);\n\thead = find_bucket(ti, hash);\n\t(*n_mask_hit)++;\n\n\thlist_for_each_entry_rcu(flow, head, flow_table.node[ti->node_ver],\n\t\t\t\t lockdep_ovsl_is_held()) {\n\t\tif (flow->mask == mask && flow->flow_table.hash == hash &&\n\t\t    flow_cmp_masked_key(flow, &masked_key, &mask->range))\n\t\t\treturn flow;\n\t}\n\treturn NULL;\n}\n\n \nstatic struct sw_flow *flow_lookup(struct flow_table *tbl,\n\t\t\t\t   struct table_instance *ti,\n\t\t\t\t   struct mask_array *ma,\n\t\t\t\t   const struct sw_flow_key *key,\n\t\t\t\t   u32 *n_mask_hit,\n\t\t\t\t   u32 *n_cache_hit,\n\t\t\t\t   u32 *index)\n{\n\tstruct mask_array_stats *stats = this_cpu_ptr(ma->masks_usage_stats);\n\tstruct sw_flow *flow;\n\tstruct sw_flow_mask *mask;\n\tint i;\n\n\tif (likely(*index < ma->max)) {\n\t\tmask = rcu_dereference_ovsl(ma->masks[*index]);\n\t\tif (mask) {\n\t\t\tflow = masked_flow_lookup(ti, key, mask, n_mask_hit);\n\t\t\tif (flow) {\n\t\t\t\tu64_stats_update_begin(&stats->syncp);\n\t\t\t\tstats->usage_cntrs[*index]++;\n\t\t\t\tu64_stats_update_end(&stats->syncp);\n\t\t\t\t(*n_cache_hit)++;\n\t\t\t\treturn flow;\n\t\t\t}\n\t\t}\n\t}\n\n\tfor (i = 0; i < ma->max; i++)  {\n\n\t\tif (i == *index)\n\t\t\tcontinue;\n\n\t\tmask = rcu_dereference_ovsl(ma->masks[i]);\n\t\tif (unlikely(!mask))\n\t\t\tbreak;\n\n\t\tflow = masked_flow_lookup(ti, key, mask, n_mask_hit);\n\t\tif (flow) {  \n\t\t\t*index = i;\n\t\t\tu64_stats_update_begin(&stats->syncp);\n\t\t\tstats->usage_cntrs[*index]++;\n\t\t\tu64_stats_update_end(&stats->syncp);\n\t\t\treturn flow;\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\n \nstruct sw_flow *ovs_flow_tbl_lookup_stats(struct flow_table *tbl,\n\t\t\t\t\t  const struct sw_flow_key *key,\n\t\t\t\t\t  u32 skb_hash,\n\t\t\t\t\t  u32 *n_mask_hit,\n\t\t\t\t\t  u32 *n_cache_hit)\n{\n\tstruct mask_cache *mc = rcu_dereference(tbl->mask_cache);\n\tstruct mask_array *ma = rcu_dereference(tbl->mask_array);\n\tstruct table_instance *ti = rcu_dereference(tbl->ti);\n\tstruct mask_cache_entry *entries, *ce;\n\tstruct sw_flow *flow;\n\tu32 hash;\n\tint seg;\n\n\t*n_mask_hit = 0;\n\t*n_cache_hit = 0;\n\tif (unlikely(!skb_hash || mc->cache_size == 0)) {\n\t\tu32 mask_index = 0;\n\t\tu32 cache = 0;\n\n\t\treturn flow_lookup(tbl, ti, ma, key, n_mask_hit, &cache,\n\t\t\t\t   &mask_index);\n\t}\n\n\t \n\tif (key->recirc_id)\n\t\tskb_hash = jhash_1word(skb_hash, key->recirc_id);\n\n\tce = NULL;\n\thash = skb_hash;\n\tentries = this_cpu_ptr(mc->mask_cache);\n\n\t \n\tfor (seg = 0; seg < MC_HASH_SEGS; seg++) {\n\t\tint index = hash & (mc->cache_size - 1);\n\t\tstruct mask_cache_entry *e;\n\n\t\te = &entries[index];\n\t\tif (e->skb_hash == skb_hash) {\n\t\t\tflow = flow_lookup(tbl, ti, ma, key, n_mask_hit,\n\t\t\t\t\t   n_cache_hit, &e->mask_index);\n\t\t\tif (!flow)\n\t\t\t\te->skb_hash = 0;\n\t\t\treturn flow;\n\t\t}\n\n\t\tif (!ce || e->skb_hash < ce->skb_hash)\n\t\t\tce = e;   \n\n\t\thash >>= MC_HASH_SHIFT;\n\t}\n\n\t \n\tflow = flow_lookup(tbl, ti, ma, key, n_mask_hit, n_cache_hit,\n\t\t\t   &ce->mask_index);\n\tif (flow)\n\t\tce->skb_hash = skb_hash;\n\n\t*n_cache_hit = 0;\n\treturn flow;\n}\n\nstruct sw_flow *ovs_flow_tbl_lookup(struct flow_table *tbl,\n\t\t\t\t    const struct sw_flow_key *key)\n{\n\tstruct table_instance *ti = rcu_dereference_ovsl(tbl->ti);\n\tstruct mask_array *ma = rcu_dereference_ovsl(tbl->mask_array);\n\tu32 __always_unused n_mask_hit;\n\tu32 __always_unused n_cache_hit;\n\tstruct sw_flow *flow;\n\tu32 index = 0;\n\n\t \n\tlocal_bh_disable();\n\tflow = flow_lookup(tbl, ti, ma, key, &n_mask_hit, &n_cache_hit, &index);\n\tlocal_bh_enable();\n\treturn flow;\n}\n\nstruct sw_flow *ovs_flow_tbl_lookup_exact(struct flow_table *tbl,\n\t\t\t\t\t  const struct sw_flow_match *match)\n{\n\tstruct mask_array *ma = ovsl_dereference(tbl->mask_array);\n\tint i;\n\n\t \n\tfor (i = 0; i < ma->max; i++) {\n\t\tstruct table_instance *ti = rcu_dereference_ovsl(tbl->ti);\n\t\tu32 __always_unused n_mask_hit;\n\t\tstruct sw_flow_mask *mask;\n\t\tstruct sw_flow *flow;\n\n\t\tmask = ovsl_dereference(ma->masks[i]);\n\t\tif (!mask)\n\t\t\tcontinue;\n\n\t\tflow = masked_flow_lookup(ti, match->key, mask, &n_mask_hit);\n\t\tif (flow && ovs_identifier_is_key(&flow->id) &&\n\t\t    ovs_flow_cmp_unmasked_key(flow, match)) {\n\t\t\treturn flow;\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\nstatic u32 ufid_hash(const struct sw_flow_id *sfid)\n{\n\treturn jhash(sfid->ufid, sfid->ufid_len, 0);\n}\n\nstatic bool ovs_flow_cmp_ufid(const struct sw_flow *flow,\n\t\t\t      const struct sw_flow_id *sfid)\n{\n\tif (flow->id.ufid_len != sfid->ufid_len)\n\t\treturn false;\n\n\treturn !memcmp(flow->id.ufid, sfid->ufid, sfid->ufid_len);\n}\n\nbool ovs_flow_cmp(const struct sw_flow *flow,\n\t\t  const struct sw_flow_match *match)\n{\n\tif (ovs_identifier_is_ufid(&flow->id))\n\t\treturn flow_cmp_masked_key(flow, match->key, &match->range);\n\n\treturn ovs_flow_cmp_unmasked_key(flow, match);\n}\n\nstruct sw_flow *ovs_flow_tbl_lookup_ufid(struct flow_table *tbl,\n\t\t\t\t\t const struct sw_flow_id *ufid)\n{\n\tstruct table_instance *ti = rcu_dereference_ovsl(tbl->ufid_ti);\n\tstruct sw_flow *flow;\n\tstruct hlist_head *head;\n\tu32 hash;\n\n\thash = ufid_hash(ufid);\n\thead = find_bucket(ti, hash);\n\thlist_for_each_entry_rcu(flow, head, ufid_table.node[ti->node_ver],\n\t\t\t\t lockdep_ovsl_is_held()) {\n\t\tif (flow->ufid_table.hash == hash &&\n\t\t    ovs_flow_cmp_ufid(flow, ufid))\n\t\t\treturn flow;\n\t}\n\treturn NULL;\n}\n\nint ovs_flow_tbl_num_masks(const struct flow_table *table)\n{\n\tstruct mask_array *ma = rcu_dereference_ovsl(table->mask_array);\n\treturn READ_ONCE(ma->count);\n}\n\nu32 ovs_flow_tbl_masks_cache_size(const struct flow_table *table)\n{\n\tstruct mask_cache *mc = rcu_dereference_ovsl(table->mask_cache);\n\n\treturn READ_ONCE(mc->cache_size);\n}\n\nstatic struct table_instance *table_instance_expand(struct table_instance *ti,\n\t\t\t\t\t\t    bool ufid)\n{\n\treturn table_instance_rehash(ti, ti->n_buckets * 2, ufid);\n}\n\n \nvoid ovs_flow_tbl_remove(struct flow_table *table, struct sw_flow *flow)\n{\n\tstruct table_instance *ti = ovsl_dereference(table->ti);\n\tstruct table_instance *ufid_ti = ovsl_dereference(table->ufid_ti);\n\n\tBUG_ON(table->count == 0);\n\ttable_instance_flow_free(table, ti, ufid_ti, flow);\n}\n\nstatic struct sw_flow_mask *mask_alloc(void)\n{\n\tstruct sw_flow_mask *mask;\n\n\tmask = kmalloc(sizeof(*mask), GFP_KERNEL);\n\tif (mask)\n\t\tmask->ref_count = 1;\n\n\treturn mask;\n}\n\nstatic bool mask_equal(const struct sw_flow_mask *a,\n\t\t       const struct sw_flow_mask *b)\n{\n\tconst u8 *a_ = (const u8 *)&a->key + a->range.start;\n\tconst u8 *b_ = (const u8 *)&b->key + b->range.start;\n\n\treturn  (a->range.end == b->range.end)\n\t\t&& (a->range.start == b->range.start)\n\t\t&& (memcmp(a_, b_, range_n_bytes(&a->range)) == 0);\n}\n\nstatic struct sw_flow_mask *flow_mask_find(const struct flow_table *tbl,\n\t\t\t\t\t   const struct sw_flow_mask *mask)\n{\n\tstruct mask_array *ma;\n\tint i;\n\n\tma = ovsl_dereference(tbl->mask_array);\n\tfor (i = 0; i < ma->max; i++) {\n\t\tstruct sw_flow_mask *t;\n\t\tt = ovsl_dereference(ma->masks[i]);\n\n\t\tif (t && mask_equal(mask, t))\n\t\t\treturn t;\n\t}\n\n\treturn NULL;\n}\n\n \nstatic int flow_mask_insert(struct flow_table *tbl, struct sw_flow *flow,\n\t\t\t    const struct sw_flow_mask *new)\n{\n\tstruct sw_flow_mask *mask;\n\n\tmask = flow_mask_find(tbl, new);\n\tif (!mask) {\n\t\t \n\t\tmask = mask_alloc();\n\t\tif (!mask)\n\t\t\treturn -ENOMEM;\n\t\tmask->key = new->key;\n\t\tmask->range = new->range;\n\n\t\t \n\t\tif (tbl_mask_array_add_mask(tbl, mask)) {\n\t\t\tkfree(mask);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t} else {\n\t\tBUG_ON(!mask->ref_count);\n\t\tmask->ref_count++;\n\t}\n\n\tflow->mask = mask;\n\treturn 0;\n}\n\n \nstatic void flow_key_insert(struct flow_table *table, struct sw_flow *flow)\n{\n\tstruct table_instance *new_ti = NULL;\n\tstruct table_instance *ti;\n\n\tflow->flow_table.hash = flow_hash(&flow->key, &flow->mask->range);\n\tti = ovsl_dereference(table->ti);\n\ttable_instance_insert(ti, flow);\n\ttable->count++;\n\n\t \n\tif (table->count > ti->n_buckets)\n\t\tnew_ti = table_instance_expand(ti, false);\n\telse if (time_after(jiffies, table->last_rehash + REHASH_INTERVAL))\n\t\tnew_ti = table_instance_rehash(ti, ti->n_buckets, false);\n\n\tif (new_ti) {\n\t\trcu_assign_pointer(table->ti, new_ti);\n\t\tcall_rcu(&ti->rcu, flow_tbl_destroy_rcu_cb);\n\t\ttable->last_rehash = jiffies;\n\t}\n}\n\n \nstatic void flow_ufid_insert(struct flow_table *table, struct sw_flow *flow)\n{\n\tstruct table_instance *ti;\n\n\tflow->ufid_table.hash = ufid_hash(&flow->id);\n\tti = ovsl_dereference(table->ufid_ti);\n\tufid_table_instance_insert(ti, flow);\n\ttable->ufid_count++;\n\n\t \n\tif (table->ufid_count > ti->n_buckets) {\n\t\tstruct table_instance *new_ti;\n\n\t\tnew_ti = table_instance_expand(ti, true);\n\t\tif (new_ti) {\n\t\t\trcu_assign_pointer(table->ufid_ti, new_ti);\n\t\t\tcall_rcu(&ti->rcu, flow_tbl_destroy_rcu_cb);\n\t\t}\n\t}\n}\n\n \nint ovs_flow_tbl_insert(struct flow_table *table, struct sw_flow *flow,\n\t\t\tconst struct sw_flow_mask *mask)\n{\n\tint err;\n\n\terr = flow_mask_insert(table, flow, mask);\n\tif (err)\n\t\treturn err;\n\tflow_key_insert(table, flow);\n\tif (ovs_identifier_is_ufid(&flow->id))\n\t\tflow_ufid_insert(table, flow);\n\n\treturn 0;\n}\n\nstatic int compare_mask_and_count(const void *a, const void *b)\n{\n\tconst struct mask_count *mc_a = a;\n\tconst struct mask_count *mc_b = b;\n\n\treturn (s64)mc_b->counter - (s64)mc_a->counter;\n}\n\n \nvoid ovs_flow_masks_rebalance(struct flow_table *table)\n{\n\tstruct mask_array *ma = rcu_dereference_ovsl(table->mask_array);\n\tstruct mask_count *masks_and_count;\n\tstruct mask_array *new;\n\tint masks_entries = 0;\n\tint i;\n\n\t \n\tmasks_and_count = kmalloc_array(ma->max, sizeof(*masks_and_count),\n\t\t\t\t\tGFP_KERNEL);\n\tif (!masks_and_count)\n\t\treturn;\n\n\tfor (i = 0; i < ma->max; i++) {\n\t\tstruct sw_flow_mask *mask;\n\t\tint cpu;\n\n\t\tmask = rcu_dereference_ovsl(ma->masks[i]);\n\t\tif (unlikely(!mask))\n\t\t\tbreak;\n\n\t\tmasks_and_count[i].index = i;\n\t\tmasks_and_count[i].counter = 0;\n\n\t\tfor_each_possible_cpu(cpu) {\n\t\t\tstruct mask_array_stats *stats;\n\t\t\tunsigned int start;\n\t\t\tu64 counter;\n\n\t\t\tstats = per_cpu_ptr(ma->masks_usage_stats, cpu);\n\t\t\tdo {\n\t\t\t\tstart = u64_stats_fetch_begin(&stats->syncp);\n\t\t\t\tcounter = stats->usage_cntrs[i];\n\t\t\t} while (u64_stats_fetch_retry(&stats->syncp, start));\n\n\t\t\tmasks_and_count[i].counter += counter;\n\t\t}\n\n\t\t \n\t\tmasks_and_count[i].counter -= ma->masks_usage_zero_cntr[i];\n\n\t\t \n\t\tma->masks_usage_zero_cntr[i] += masks_and_count[i].counter;\n\t}\n\n\tif (i == 0)\n\t\tgoto free_mask_entries;\n\n\t \n\tmasks_entries = i;\n\tsort(masks_and_count, masks_entries, sizeof(*masks_and_count),\n\t     compare_mask_and_count, NULL);\n\n\t \n\tfor (i = 0; i < masks_entries; i++) {\n\t\tif (i != masks_and_count[i].index)\n\t\t\tbreak;\n\t}\n\tif (i == masks_entries)\n\t\tgoto free_mask_entries;\n\n\t \n\tnew = tbl_mask_array_alloc(ma->max);\n\tif (!new)\n\t\tgoto free_mask_entries;\n\n\tfor (i = 0; i < masks_entries; i++) {\n\t\tint index = masks_and_count[i].index;\n\n\t\tif (ovsl_dereference(ma->masks[index]))\n\t\t\tnew->masks[new->count++] = ma->masks[index];\n\t}\n\n\trcu_assign_pointer(table->mask_array, new);\n\tcall_rcu(&ma->rcu, mask_array_rcu_cb);\n\nfree_mask_entries:\n\tkfree(masks_and_count);\n}\n\n \nint ovs_flow_init(void)\n{\n\tBUILD_BUG_ON(__alignof__(struct sw_flow_key) % __alignof__(long));\n\tBUILD_BUG_ON(sizeof(struct sw_flow_key) % sizeof(long));\n\n\tflow_cache = kmem_cache_create(\"sw_flow\", sizeof(struct sw_flow)\n\t\t\t\t       + (nr_cpu_ids\n\t\t\t\t\t  * sizeof(struct sw_flow_stats *))\n\t\t\t\t       + cpumask_size(),\n\t\t\t\t       0, 0, NULL);\n\tif (flow_cache == NULL)\n\t\treturn -ENOMEM;\n\n\tflow_stats_cache\n\t\t= kmem_cache_create(\"sw_flow_stats\", sizeof(struct sw_flow_stats),\n\t\t\t\t    0, SLAB_HWCACHE_ALIGN, NULL);\n\tif (flow_stats_cache == NULL) {\n\t\tkmem_cache_destroy(flow_cache);\n\t\tflow_cache = NULL;\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\n \nvoid ovs_flow_exit(void)\n{\n\tkmem_cache_destroy(flow_stats_cache);\n\tkmem_cache_destroy(flow_cache);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}