{
  "module_name": "xsk_buff_pool.c",
  "hash_id": "75bf6b544765b6d2b32529a1f450045fe8557603c527cae90923580fc845c048",
  "original_prompt": "Ingested from linux-6.6.14/net/xdp/xsk_buff_pool.c",
  "human_readable_source": "\n\n#include <net/xsk_buff_pool.h>\n#include <net/xdp_sock.h>\n#include <net/xdp_sock_drv.h>\n\n#include \"xsk_queue.h\"\n#include \"xdp_umem.h\"\n#include \"xsk.h\"\n\nvoid xp_add_xsk(struct xsk_buff_pool *pool, struct xdp_sock *xs)\n{\n\tunsigned long flags;\n\n\tif (!xs->tx)\n\t\treturn;\n\n\tspin_lock_irqsave(&pool->xsk_tx_list_lock, flags);\n\tlist_add_rcu(&xs->tx_list, &pool->xsk_tx_list);\n\tspin_unlock_irqrestore(&pool->xsk_tx_list_lock, flags);\n}\n\nvoid xp_del_xsk(struct xsk_buff_pool *pool, struct xdp_sock *xs)\n{\n\tunsigned long flags;\n\n\tif (!xs->tx)\n\t\treturn;\n\n\tspin_lock_irqsave(&pool->xsk_tx_list_lock, flags);\n\tlist_del_rcu(&xs->tx_list);\n\tspin_unlock_irqrestore(&pool->xsk_tx_list_lock, flags);\n}\n\nvoid xp_destroy(struct xsk_buff_pool *pool)\n{\n\tif (!pool)\n\t\treturn;\n\n\tkvfree(pool->tx_descs);\n\tkvfree(pool->heads);\n\tkvfree(pool);\n}\n\nint xp_alloc_tx_descs(struct xsk_buff_pool *pool, struct xdp_sock *xs)\n{\n\tpool->tx_descs = kvcalloc(xs->tx->nentries, sizeof(*pool->tx_descs),\n\t\t\t\t  GFP_KERNEL);\n\tif (!pool->tx_descs)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstruct xsk_buff_pool *xp_create_and_assign_umem(struct xdp_sock *xs,\n\t\t\t\t\t\tstruct xdp_umem *umem)\n{\n\tbool unaligned = umem->flags & XDP_UMEM_UNALIGNED_CHUNK_FLAG;\n\tstruct xsk_buff_pool *pool;\n\tstruct xdp_buff_xsk *xskb;\n\tu32 i, entries;\n\n\tentries = unaligned ? umem->chunks : 0;\n\tpool = kvzalloc(struct_size(pool, free_heads, entries),\tGFP_KERNEL);\n\tif (!pool)\n\t\tgoto out;\n\n\tpool->heads = kvcalloc(umem->chunks, sizeof(*pool->heads), GFP_KERNEL);\n\tif (!pool->heads)\n\t\tgoto out;\n\n\tif (xs->tx)\n\t\tif (xp_alloc_tx_descs(pool, xs))\n\t\t\tgoto out;\n\n\tpool->chunk_mask = ~((u64)umem->chunk_size - 1);\n\tpool->addrs_cnt = umem->size;\n\tpool->heads_cnt = umem->chunks;\n\tpool->free_heads_cnt = umem->chunks;\n\tpool->headroom = umem->headroom;\n\tpool->chunk_size = umem->chunk_size;\n\tpool->chunk_shift = ffs(umem->chunk_size) - 1;\n\tpool->unaligned = unaligned;\n\tpool->frame_len = umem->chunk_size - umem->headroom -\n\t\tXDP_PACKET_HEADROOM;\n\tpool->umem = umem;\n\tpool->addrs = umem->addrs;\n\tINIT_LIST_HEAD(&pool->free_list);\n\tINIT_LIST_HEAD(&pool->xskb_list);\n\tINIT_LIST_HEAD(&pool->xsk_tx_list);\n\tspin_lock_init(&pool->xsk_tx_list_lock);\n\tspin_lock_init(&pool->cq_lock);\n\trefcount_set(&pool->users, 1);\n\n\tpool->fq = xs->fq_tmp;\n\tpool->cq = xs->cq_tmp;\n\n\tfor (i = 0; i < pool->free_heads_cnt; i++) {\n\t\txskb = &pool->heads[i];\n\t\txskb->pool = pool;\n\t\txskb->xdp.frame_sz = umem->chunk_size - umem->headroom;\n\t\tINIT_LIST_HEAD(&xskb->free_list_node);\n\t\tINIT_LIST_HEAD(&xskb->xskb_list_node);\n\t\tif (pool->unaligned)\n\t\t\tpool->free_heads[i] = xskb;\n\t\telse\n\t\t\txp_init_xskb_addr(xskb, pool, i * pool->chunk_size);\n\t}\n\n\treturn pool;\n\nout:\n\txp_destroy(pool);\n\treturn NULL;\n}\n\nvoid xp_set_rxq_info(struct xsk_buff_pool *pool, struct xdp_rxq_info *rxq)\n{\n\tu32 i;\n\n\tfor (i = 0; i < pool->heads_cnt; i++)\n\t\tpool->heads[i].xdp.rxq = rxq;\n}\nEXPORT_SYMBOL(xp_set_rxq_info);\n\nstatic void xp_disable_drv_zc(struct xsk_buff_pool *pool)\n{\n\tstruct netdev_bpf bpf;\n\tint err;\n\n\tASSERT_RTNL();\n\n\tif (pool->umem->zc) {\n\t\tbpf.command = XDP_SETUP_XSK_POOL;\n\t\tbpf.xsk.pool = NULL;\n\t\tbpf.xsk.queue_id = pool->queue_id;\n\n\t\terr = pool->netdev->netdev_ops->ndo_bpf(pool->netdev, &bpf);\n\n\t\tif (err)\n\t\t\tWARN(1, \"Failed to disable zero-copy!\\n\");\n\t}\n}\n\n#define NETDEV_XDP_ACT_ZC\t(NETDEV_XDP_ACT_BASIC |\t\t\\\n\t\t\t\t NETDEV_XDP_ACT_REDIRECT |\t\\\n\t\t\t\t NETDEV_XDP_ACT_XSK_ZEROCOPY)\n\nint xp_assign_dev(struct xsk_buff_pool *pool,\n\t\t  struct net_device *netdev, u16 queue_id, u16 flags)\n{\n\tbool force_zc, force_copy;\n\tstruct netdev_bpf bpf;\n\tint err = 0;\n\n\tASSERT_RTNL();\n\n\tforce_zc = flags & XDP_ZEROCOPY;\n\tforce_copy = flags & XDP_COPY;\n\n\tif (force_zc && force_copy)\n\t\treturn -EINVAL;\n\n\tif (xsk_get_pool_from_qid(netdev, queue_id))\n\t\treturn -EBUSY;\n\n\tpool->netdev = netdev;\n\tpool->queue_id = queue_id;\n\terr = xsk_reg_pool_at_qid(netdev, pool, queue_id);\n\tif (err)\n\t\treturn err;\n\n\tif (flags & XDP_USE_SG)\n\t\tpool->umem->flags |= XDP_UMEM_SG_FLAG;\n\n\tif (flags & XDP_USE_NEED_WAKEUP)\n\t\tpool->uses_need_wakeup = true;\n\t \n\tpool->cached_need_wakeup = XDP_WAKEUP_TX;\n\n\tdev_hold(netdev);\n\n\tif (force_copy)\n\t\t \n\t\treturn 0;\n\n\tif ((netdev->xdp_features & NETDEV_XDP_ACT_ZC) != NETDEV_XDP_ACT_ZC) {\n\t\terr = -EOPNOTSUPP;\n\t\tgoto err_unreg_pool;\n\t}\n\n\tif (netdev->xdp_zc_max_segs == 1 && (flags & XDP_USE_SG)) {\n\t\terr = -EOPNOTSUPP;\n\t\tgoto err_unreg_pool;\n\t}\n\n\tbpf.command = XDP_SETUP_XSK_POOL;\n\tbpf.xsk.pool = pool;\n\tbpf.xsk.queue_id = queue_id;\n\n\terr = netdev->netdev_ops->ndo_bpf(netdev, &bpf);\n\tif (err)\n\t\tgoto err_unreg_pool;\n\n\tif (!pool->dma_pages) {\n\t\tWARN(1, \"Driver did not DMA map zero-copy buffers\");\n\t\terr = -EINVAL;\n\t\tgoto err_unreg_xsk;\n\t}\n\tpool->umem->zc = true;\n\treturn 0;\n\nerr_unreg_xsk:\n\txp_disable_drv_zc(pool);\nerr_unreg_pool:\n\tif (!force_zc)\n\t\terr = 0;  \n\tif (err) {\n\t\txsk_clear_pool_at_qid(netdev, queue_id);\n\t\tdev_put(netdev);\n\t}\n\treturn err;\n}\n\nint xp_assign_dev_shared(struct xsk_buff_pool *pool, struct xdp_sock *umem_xs,\n\t\t\t struct net_device *dev, u16 queue_id)\n{\n\tu16 flags;\n\tstruct xdp_umem *umem = umem_xs->umem;\n\n\t \n\tif (!pool->fq || !pool->cq)\n\t\treturn -EINVAL;\n\n\tflags = umem->zc ? XDP_ZEROCOPY : XDP_COPY;\n\tif (umem_xs->pool->uses_need_wakeup)\n\t\tflags |= XDP_USE_NEED_WAKEUP;\n\n\treturn xp_assign_dev(pool, dev, queue_id, flags);\n}\n\nvoid xp_clear_dev(struct xsk_buff_pool *pool)\n{\n\tif (!pool->netdev)\n\t\treturn;\n\n\txp_disable_drv_zc(pool);\n\txsk_clear_pool_at_qid(pool->netdev, pool->queue_id);\n\tdev_put(pool->netdev);\n\tpool->netdev = NULL;\n}\n\nstatic void xp_release_deferred(struct work_struct *work)\n{\n\tstruct xsk_buff_pool *pool = container_of(work, struct xsk_buff_pool,\n\t\t\t\t\t\t  work);\n\n\trtnl_lock();\n\txp_clear_dev(pool);\n\trtnl_unlock();\n\n\tif (pool->fq) {\n\t\txskq_destroy(pool->fq);\n\t\tpool->fq = NULL;\n\t}\n\n\tif (pool->cq) {\n\t\txskq_destroy(pool->cq);\n\t\tpool->cq = NULL;\n\t}\n\n\txdp_put_umem(pool->umem, false);\n\txp_destroy(pool);\n}\n\nvoid xp_get_pool(struct xsk_buff_pool *pool)\n{\n\trefcount_inc(&pool->users);\n}\n\nbool xp_put_pool(struct xsk_buff_pool *pool)\n{\n\tif (!pool)\n\t\treturn false;\n\n\tif (refcount_dec_and_test(&pool->users)) {\n\t\tINIT_WORK(&pool->work, xp_release_deferred);\n\t\tschedule_work(&pool->work);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic struct xsk_dma_map *xp_find_dma_map(struct xsk_buff_pool *pool)\n{\n\tstruct xsk_dma_map *dma_map;\n\n\tlist_for_each_entry(dma_map, &pool->umem->xsk_dma_list, list) {\n\t\tif (dma_map->netdev == pool->netdev)\n\t\t\treturn dma_map;\n\t}\n\n\treturn NULL;\n}\n\nstatic struct xsk_dma_map *xp_create_dma_map(struct device *dev, struct net_device *netdev,\n\t\t\t\t\t     u32 nr_pages, struct xdp_umem *umem)\n{\n\tstruct xsk_dma_map *dma_map;\n\n\tdma_map = kzalloc(sizeof(*dma_map), GFP_KERNEL);\n\tif (!dma_map)\n\t\treturn NULL;\n\n\tdma_map->dma_pages = kvcalloc(nr_pages, sizeof(*dma_map->dma_pages), GFP_KERNEL);\n\tif (!dma_map->dma_pages) {\n\t\tkfree(dma_map);\n\t\treturn NULL;\n\t}\n\n\tdma_map->netdev = netdev;\n\tdma_map->dev = dev;\n\tdma_map->dma_need_sync = false;\n\tdma_map->dma_pages_cnt = nr_pages;\n\trefcount_set(&dma_map->users, 1);\n\tlist_add(&dma_map->list, &umem->xsk_dma_list);\n\treturn dma_map;\n}\n\nstatic void xp_destroy_dma_map(struct xsk_dma_map *dma_map)\n{\n\tlist_del(&dma_map->list);\n\tkvfree(dma_map->dma_pages);\n\tkfree(dma_map);\n}\n\nstatic void __xp_dma_unmap(struct xsk_dma_map *dma_map, unsigned long attrs)\n{\n\tdma_addr_t *dma;\n\tu32 i;\n\n\tfor (i = 0; i < dma_map->dma_pages_cnt; i++) {\n\t\tdma = &dma_map->dma_pages[i];\n\t\tif (*dma) {\n\t\t\t*dma &= ~XSK_NEXT_PG_CONTIG_MASK;\n\t\t\tdma_unmap_page_attrs(dma_map->dev, *dma, PAGE_SIZE,\n\t\t\t\t\t     DMA_BIDIRECTIONAL, attrs);\n\t\t\t*dma = 0;\n\t\t}\n\t}\n\n\txp_destroy_dma_map(dma_map);\n}\n\nvoid xp_dma_unmap(struct xsk_buff_pool *pool, unsigned long attrs)\n{\n\tstruct xsk_dma_map *dma_map;\n\n\tif (!pool->dma_pages)\n\t\treturn;\n\n\tdma_map = xp_find_dma_map(pool);\n\tif (!dma_map) {\n\t\tWARN(1, \"Could not find dma_map for device\");\n\t\treturn;\n\t}\n\n\tif (!refcount_dec_and_test(&dma_map->users))\n\t\treturn;\n\n\t__xp_dma_unmap(dma_map, attrs);\n\tkvfree(pool->dma_pages);\n\tpool->dma_pages = NULL;\n\tpool->dma_pages_cnt = 0;\n\tpool->dev = NULL;\n}\nEXPORT_SYMBOL(xp_dma_unmap);\n\nstatic void xp_check_dma_contiguity(struct xsk_dma_map *dma_map)\n{\n\tu32 i;\n\n\tfor (i = 0; i < dma_map->dma_pages_cnt - 1; i++) {\n\t\tif (dma_map->dma_pages[i] + PAGE_SIZE == dma_map->dma_pages[i + 1])\n\t\t\tdma_map->dma_pages[i] |= XSK_NEXT_PG_CONTIG_MASK;\n\t\telse\n\t\t\tdma_map->dma_pages[i] &= ~XSK_NEXT_PG_CONTIG_MASK;\n\t}\n}\n\nstatic int xp_init_dma_info(struct xsk_buff_pool *pool, struct xsk_dma_map *dma_map)\n{\n\tif (!pool->unaligned) {\n\t\tu32 i;\n\n\t\tfor (i = 0; i < pool->heads_cnt; i++) {\n\t\t\tstruct xdp_buff_xsk *xskb = &pool->heads[i];\n\n\t\t\txp_init_xskb_dma(xskb, pool, dma_map->dma_pages, xskb->orig_addr);\n\t\t}\n\t}\n\n\tpool->dma_pages = kvcalloc(dma_map->dma_pages_cnt, sizeof(*pool->dma_pages), GFP_KERNEL);\n\tif (!pool->dma_pages)\n\t\treturn -ENOMEM;\n\n\tpool->dev = dma_map->dev;\n\tpool->dma_pages_cnt = dma_map->dma_pages_cnt;\n\tpool->dma_need_sync = dma_map->dma_need_sync;\n\tmemcpy(pool->dma_pages, dma_map->dma_pages,\n\t       pool->dma_pages_cnt * sizeof(*pool->dma_pages));\n\n\treturn 0;\n}\n\nint xp_dma_map(struct xsk_buff_pool *pool, struct device *dev,\n\t       unsigned long attrs, struct page **pages, u32 nr_pages)\n{\n\tstruct xsk_dma_map *dma_map;\n\tdma_addr_t dma;\n\tint err;\n\tu32 i;\n\n\tdma_map = xp_find_dma_map(pool);\n\tif (dma_map) {\n\t\terr = xp_init_dma_info(pool, dma_map);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\trefcount_inc(&dma_map->users);\n\t\treturn 0;\n\t}\n\n\tdma_map = xp_create_dma_map(dev, pool->netdev, nr_pages, pool->umem);\n\tif (!dma_map)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < dma_map->dma_pages_cnt; i++) {\n\t\tdma = dma_map_page_attrs(dev, pages[i], 0, PAGE_SIZE,\n\t\t\t\t\t DMA_BIDIRECTIONAL, attrs);\n\t\tif (dma_mapping_error(dev, dma)) {\n\t\t\t__xp_dma_unmap(dma_map, attrs);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tif (dma_need_sync(dev, dma))\n\t\t\tdma_map->dma_need_sync = true;\n\t\tdma_map->dma_pages[i] = dma;\n\t}\n\n\tif (pool->unaligned)\n\t\txp_check_dma_contiguity(dma_map);\n\n\terr = xp_init_dma_info(pool, dma_map);\n\tif (err) {\n\t\t__xp_dma_unmap(dma_map, attrs);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(xp_dma_map);\n\nstatic bool xp_addr_crosses_non_contig_pg(struct xsk_buff_pool *pool,\n\t\t\t\t\t  u64 addr)\n{\n\treturn xp_desc_crosses_non_contig_pg(pool, addr, pool->chunk_size);\n}\n\nstatic bool xp_check_unaligned(struct xsk_buff_pool *pool, u64 *addr)\n{\n\t*addr = xp_unaligned_extract_addr(*addr);\n\tif (*addr >= pool->addrs_cnt ||\n\t    *addr + pool->chunk_size > pool->addrs_cnt ||\n\t    xp_addr_crosses_non_contig_pg(pool, *addr))\n\t\treturn false;\n\treturn true;\n}\n\nstatic bool xp_check_aligned(struct xsk_buff_pool *pool, u64 *addr)\n{\n\t*addr = xp_aligned_extract_addr(pool, *addr);\n\treturn *addr < pool->addrs_cnt;\n}\n\nstatic struct xdp_buff_xsk *__xp_alloc(struct xsk_buff_pool *pool)\n{\n\tstruct xdp_buff_xsk *xskb;\n\tu64 addr;\n\tbool ok;\n\n\tif (pool->free_heads_cnt == 0)\n\t\treturn NULL;\n\n\tfor (;;) {\n\t\tif (!xskq_cons_peek_addr_unchecked(pool->fq, &addr)) {\n\t\t\tpool->fq->queue_empty_descs++;\n\t\t\treturn NULL;\n\t\t}\n\n\t\tok = pool->unaligned ? xp_check_unaligned(pool, &addr) :\n\t\t     xp_check_aligned(pool, &addr);\n\t\tif (!ok) {\n\t\t\tpool->fq->invalid_descs++;\n\t\t\txskq_cons_release(pool->fq);\n\t\t\tcontinue;\n\t\t}\n\t\tbreak;\n\t}\n\n\tif (pool->unaligned) {\n\t\txskb = pool->free_heads[--pool->free_heads_cnt];\n\t\txp_init_xskb_addr(xskb, pool, addr);\n\t\tif (pool->dma_pages)\n\t\t\txp_init_xskb_dma(xskb, pool, pool->dma_pages, addr);\n\t} else {\n\t\txskb = &pool->heads[xp_aligned_extract_idx(pool, addr)];\n\t}\n\n\txskq_cons_release(pool->fq);\n\treturn xskb;\n}\n\nstruct xdp_buff *xp_alloc(struct xsk_buff_pool *pool)\n{\n\tstruct xdp_buff_xsk *xskb;\n\n\tif (!pool->free_list_cnt) {\n\t\txskb = __xp_alloc(pool);\n\t\tif (!xskb)\n\t\t\treturn NULL;\n\t} else {\n\t\tpool->free_list_cnt--;\n\t\txskb = list_first_entry(&pool->free_list, struct xdp_buff_xsk,\n\t\t\t\t\tfree_list_node);\n\t\tlist_del_init(&xskb->free_list_node);\n\t}\n\n\txskb->xdp.data = xskb->xdp.data_hard_start + XDP_PACKET_HEADROOM;\n\txskb->xdp.data_meta = xskb->xdp.data;\n\n\tif (pool->dma_need_sync) {\n\t\tdma_sync_single_range_for_device(pool->dev, xskb->dma, 0,\n\t\t\t\t\t\t pool->frame_len,\n\t\t\t\t\t\t DMA_BIDIRECTIONAL);\n\t}\n\treturn &xskb->xdp;\n}\nEXPORT_SYMBOL(xp_alloc);\n\nstatic u32 xp_alloc_new_from_fq(struct xsk_buff_pool *pool, struct xdp_buff **xdp, u32 max)\n{\n\tu32 i, cached_cons, nb_entries;\n\n\tif (max > pool->free_heads_cnt)\n\t\tmax = pool->free_heads_cnt;\n\tmax = xskq_cons_nb_entries(pool->fq, max);\n\n\tcached_cons = pool->fq->cached_cons;\n\tnb_entries = max;\n\ti = max;\n\twhile (i--) {\n\t\tstruct xdp_buff_xsk *xskb;\n\t\tu64 addr;\n\t\tbool ok;\n\n\t\t__xskq_cons_read_addr_unchecked(pool->fq, cached_cons++, &addr);\n\n\t\tok = pool->unaligned ? xp_check_unaligned(pool, &addr) :\n\t\t\txp_check_aligned(pool, &addr);\n\t\tif (unlikely(!ok)) {\n\t\t\tpool->fq->invalid_descs++;\n\t\t\tnb_entries--;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (pool->unaligned) {\n\t\t\txskb = pool->free_heads[--pool->free_heads_cnt];\n\t\t\txp_init_xskb_addr(xskb, pool, addr);\n\t\t\tif (pool->dma_pages)\n\t\t\t\txp_init_xskb_dma(xskb, pool, pool->dma_pages, addr);\n\t\t} else {\n\t\t\txskb = &pool->heads[xp_aligned_extract_idx(pool, addr)];\n\t\t}\n\n\t\t*xdp = &xskb->xdp;\n\t\txdp++;\n\t}\n\n\txskq_cons_release_n(pool->fq, max);\n\treturn nb_entries;\n}\n\nstatic u32 xp_alloc_reused(struct xsk_buff_pool *pool, struct xdp_buff **xdp, u32 nb_entries)\n{\n\tstruct xdp_buff_xsk *xskb;\n\tu32 i;\n\n\tnb_entries = min_t(u32, nb_entries, pool->free_list_cnt);\n\n\ti = nb_entries;\n\twhile (i--) {\n\t\txskb = list_first_entry(&pool->free_list, struct xdp_buff_xsk, free_list_node);\n\t\tlist_del_init(&xskb->free_list_node);\n\n\t\t*xdp = &xskb->xdp;\n\t\txdp++;\n\t}\n\tpool->free_list_cnt -= nb_entries;\n\n\treturn nb_entries;\n}\n\nu32 xp_alloc_batch(struct xsk_buff_pool *pool, struct xdp_buff **xdp, u32 max)\n{\n\tu32 nb_entries1 = 0, nb_entries2;\n\n\tif (unlikely(pool->dma_need_sync)) {\n\t\tstruct xdp_buff *buff;\n\n\t\t \n\t\tbuff = xp_alloc(pool);\n\t\tif (buff)\n\t\t\t*xdp = buff;\n\t\treturn !!buff;\n\t}\n\n\tif (unlikely(pool->free_list_cnt)) {\n\t\tnb_entries1 = xp_alloc_reused(pool, xdp, max);\n\t\tif (nb_entries1 == max)\n\t\t\treturn nb_entries1;\n\n\t\tmax -= nb_entries1;\n\t\txdp += nb_entries1;\n\t}\n\n\tnb_entries2 = xp_alloc_new_from_fq(pool, xdp, max);\n\tif (!nb_entries2)\n\t\tpool->fq->queue_empty_descs++;\n\n\treturn nb_entries1 + nb_entries2;\n}\nEXPORT_SYMBOL(xp_alloc_batch);\n\nbool xp_can_alloc(struct xsk_buff_pool *pool, u32 count)\n{\n\tif (pool->free_list_cnt >= count)\n\t\treturn true;\n\treturn xskq_cons_has_entries(pool->fq, count - pool->free_list_cnt);\n}\nEXPORT_SYMBOL(xp_can_alloc);\n\nvoid xp_free(struct xdp_buff_xsk *xskb)\n{\n\tif (!list_empty(&xskb->free_list_node))\n\t\treturn;\n\n\txskb->pool->free_list_cnt++;\n\tlist_add(&xskb->free_list_node, &xskb->pool->free_list);\n}\nEXPORT_SYMBOL(xp_free);\n\nvoid *xp_raw_get_data(struct xsk_buff_pool *pool, u64 addr)\n{\n\taddr = pool->unaligned ? xp_unaligned_add_offset_to_addr(addr) : addr;\n\treturn pool->addrs + addr;\n}\nEXPORT_SYMBOL(xp_raw_get_data);\n\ndma_addr_t xp_raw_get_dma(struct xsk_buff_pool *pool, u64 addr)\n{\n\taddr = pool->unaligned ? xp_unaligned_add_offset_to_addr(addr) : addr;\n\treturn (pool->dma_pages[addr >> PAGE_SHIFT] &\n\t\t~XSK_NEXT_PG_CONTIG_MASK) +\n\t\t(addr & ~PAGE_MASK);\n}\nEXPORT_SYMBOL(xp_raw_get_dma);\n\nvoid xp_dma_sync_for_cpu_slow(struct xdp_buff_xsk *xskb)\n{\n\tdma_sync_single_range_for_cpu(xskb->pool->dev, xskb->dma, 0,\n\t\t\t\t      xskb->pool->frame_len, DMA_BIDIRECTIONAL);\n}\nEXPORT_SYMBOL(xp_dma_sync_for_cpu_slow);\n\nvoid xp_dma_sync_for_device_slow(struct xsk_buff_pool *pool, dma_addr_t dma,\n\t\t\t\t size_t size)\n{\n\tdma_sync_single_range_for_device(pool->dev, dma, 0,\n\t\t\t\t\t size, DMA_BIDIRECTIONAL);\n}\nEXPORT_SYMBOL(xp_dma_sync_for_device_slow);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}