{
  "module_name": "xsk_queue.h",
  "hash_id": "e0fa77048b9a0708f144a393b00bc24d842f613184d633fb5af1e3caff24065a",
  "original_prompt": "Ingested from linux-6.6.14/net/xdp/xsk_queue.h",
  "human_readable_source": " \n \n\n#ifndef _LINUX_XSK_QUEUE_H\n#define _LINUX_XSK_QUEUE_H\n\n#include <linux/types.h>\n#include <linux/if_xdp.h>\n#include <net/xdp_sock.h>\n#include <net/xsk_buff_pool.h>\n\n#include \"xsk.h\"\n\nstruct xdp_ring {\n\tu32 producer ____cacheline_aligned_in_smp;\n\t \n\tu32 pad1 ____cacheline_aligned_in_smp;\n\tu32 consumer ____cacheline_aligned_in_smp;\n\tu32 pad2 ____cacheline_aligned_in_smp;\n\tu32 flags;\n\tu32 pad3 ____cacheline_aligned_in_smp;\n};\n\n \nstruct xdp_rxtx_ring {\n\tstruct xdp_ring ptrs;\n\tstruct xdp_desc desc[] ____cacheline_aligned_in_smp;\n};\n\n \nstruct xdp_umem_ring {\n\tstruct xdp_ring ptrs;\n\tu64 desc[] ____cacheline_aligned_in_smp;\n};\n\nstruct xsk_queue {\n\tu32 ring_mask;\n\tu32 nentries;\n\tu32 cached_prod;\n\tu32 cached_cons;\n\tstruct xdp_ring *ring;\n\tu64 invalid_descs;\n\tu64 queue_empty_descs;\n\tsize_t ring_vmalloc_size;\n};\n\nstruct parsed_desc {\n\tu32 mb;\n\tu32 valid;\n};\n\n \n\n \n\n \n\nstatic inline void __xskq_cons_read_addr_unchecked(struct xsk_queue *q, u32 cached_cons, u64 *addr)\n{\n\tstruct xdp_umem_ring *ring = (struct xdp_umem_ring *)q->ring;\n\tu32 idx = cached_cons & q->ring_mask;\n\n\t*addr = ring->desc[idx];\n}\n\nstatic inline bool xskq_cons_read_addr_unchecked(struct xsk_queue *q, u64 *addr)\n{\n\tif (q->cached_cons != q->cached_prod) {\n\t\t__xskq_cons_read_addr_unchecked(q, q->cached_cons, addr);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic inline bool xp_unused_options_set(u32 options)\n{\n\treturn options & ~XDP_PKT_CONTD;\n}\n\nstatic inline bool xp_aligned_validate_desc(struct xsk_buff_pool *pool,\n\t\t\t\t\t    struct xdp_desc *desc)\n{\n\tu64 offset = desc->addr & (pool->chunk_size - 1);\n\n\tif (!desc->len)\n\t\treturn false;\n\n\tif (offset + desc->len > pool->chunk_size)\n\t\treturn false;\n\n\tif (desc->addr >= pool->addrs_cnt)\n\t\treturn false;\n\n\tif (xp_unused_options_set(desc->options))\n\t\treturn false;\n\treturn true;\n}\n\nstatic inline bool xp_unaligned_validate_desc(struct xsk_buff_pool *pool,\n\t\t\t\t\t      struct xdp_desc *desc)\n{\n\tu64 addr = xp_unaligned_add_offset_to_addr(desc->addr);\n\n\tif (!desc->len)\n\t\treturn false;\n\n\tif (desc->len > pool->chunk_size)\n\t\treturn false;\n\n\tif (addr >= pool->addrs_cnt || addr + desc->len > pool->addrs_cnt ||\n\t    xp_desc_crosses_non_contig_pg(pool, addr, desc->len))\n\t\treturn false;\n\n\tif (xp_unused_options_set(desc->options))\n\t\treturn false;\n\treturn true;\n}\n\nstatic inline bool xp_validate_desc(struct xsk_buff_pool *pool,\n\t\t\t\t    struct xdp_desc *desc)\n{\n\treturn pool->unaligned ? xp_unaligned_validate_desc(pool, desc) :\n\t\txp_aligned_validate_desc(pool, desc);\n}\n\nstatic inline bool xskq_has_descs(struct xsk_queue *q)\n{\n\treturn q->cached_cons != q->cached_prod;\n}\n\nstatic inline bool xskq_cons_is_valid_desc(struct xsk_queue *q,\n\t\t\t\t\t   struct xdp_desc *d,\n\t\t\t\t\t   struct xsk_buff_pool *pool)\n{\n\tif (!xp_validate_desc(pool, d)) {\n\t\tq->invalid_descs++;\n\t\treturn false;\n\t}\n\treturn true;\n}\n\nstatic inline bool xskq_cons_read_desc(struct xsk_queue *q,\n\t\t\t\t       struct xdp_desc *desc,\n\t\t\t\t       struct xsk_buff_pool *pool)\n{\n\tif (q->cached_cons != q->cached_prod) {\n\t\tstruct xdp_rxtx_ring *ring = (struct xdp_rxtx_ring *)q->ring;\n\t\tu32 idx = q->cached_cons & q->ring_mask;\n\n\t\t*desc = ring->desc[idx];\n\t\treturn xskq_cons_is_valid_desc(q, desc, pool);\n\t}\n\n\tq->queue_empty_descs++;\n\treturn false;\n}\n\nstatic inline void xskq_cons_release_n(struct xsk_queue *q, u32 cnt)\n{\n\tq->cached_cons += cnt;\n}\n\nstatic inline void parse_desc(struct xsk_queue *q, struct xsk_buff_pool *pool,\n\t\t\t      struct xdp_desc *desc, struct parsed_desc *parsed)\n{\n\tparsed->valid = xskq_cons_is_valid_desc(q, desc, pool);\n\tparsed->mb = xp_mb_desc(desc);\n}\n\nstatic inline\nu32 xskq_cons_read_desc_batch(struct xsk_queue *q, struct xsk_buff_pool *pool,\n\t\t\t      u32 max)\n{\n\tu32 cached_cons = q->cached_cons, nb_entries = 0;\n\tstruct xdp_desc *descs = pool->tx_descs;\n\tu32 total_descs = 0, nr_frags = 0;\n\n\t \n\twhile (cached_cons != q->cached_prod && nb_entries < max) {\n\t\tstruct xdp_rxtx_ring *ring = (struct xdp_rxtx_ring *)q->ring;\n\t\tu32 idx = cached_cons & q->ring_mask;\n\t\tstruct parsed_desc parsed;\n\n\t\tdescs[nb_entries] = ring->desc[idx];\n\t\tcached_cons++;\n\t\tparse_desc(q, pool, &descs[nb_entries], &parsed);\n\t\tif (unlikely(!parsed.valid))\n\t\t\tbreak;\n\n\t\tif (likely(!parsed.mb)) {\n\t\t\ttotal_descs += (nr_frags + 1);\n\t\t\tnr_frags = 0;\n\t\t} else {\n\t\t\tnr_frags++;\n\t\t\tif (nr_frags == pool->netdev->xdp_zc_max_segs) {\n\t\t\t\tnr_frags = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tnb_entries++;\n\t}\n\n\tcached_cons -= nr_frags;\n\t \n\txskq_cons_release_n(q, cached_cons - q->cached_cons);\n\treturn total_descs;\n}\n\n \n\nstatic inline void __xskq_cons_release(struct xsk_queue *q)\n{\n\tsmp_store_release(&q->ring->consumer, q->cached_cons);  \n}\n\nstatic inline void __xskq_cons_peek(struct xsk_queue *q)\n{\n\t \n\tq->cached_prod = smp_load_acquire(&q->ring->producer);   \n}\n\nstatic inline void xskq_cons_get_entries(struct xsk_queue *q)\n{\n\t__xskq_cons_release(q);\n\t__xskq_cons_peek(q);\n}\n\nstatic inline u32 xskq_cons_nb_entries(struct xsk_queue *q, u32 max)\n{\n\tu32 entries = q->cached_prod - q->cached_cons;\n\n\tif (entries >= max)\n\t\treturn max;\n\n\t__xskq_cons_peek(q);\n\tentries = q->cached_prod - q->cached_cons;\n\n\treturn entries >= max ? max : entries;\n}\n\nstatic inline bool xskq_cons_has_entries(struct xsk_queue *q, u32 cnt)\n{\n\treturn xskq_cons_nb_entries(q, cnt) >= cnt;\n}\n\nstatic inline bool xskq_cons_peek_addr_unchecked(struct xsk_queue *q, u64 *addr)\n{\n\tif (q->cached_prod == q->cached_cons)\n\t\txskq_cons_get_entries(q);\n\treturn xskq_cons_read_addr_unchecked(q, addr);\n}\n\nstatic inline bool xskq_cons_peek_desc(struct xsk_queue *q,\n\t\t\t\t       struct xdp_desc *desc,\n\t\t\t\t       struct xsk_buff_pool *pool)\n{\n\tif (q->cached_prod == q->cached_cons)\n\t\txskq_cons_get_entries(q);\n\treturn xskq_cons_read_desc(q, desc, pool);\n}\n\n \nstatic inline void xskq_cons_release(struct xsk_queue *q)\n{\n\tq->cached_cons++;\n}\n\nstatic inline void xskq_cons_cancel_n(struct xsk_queue *q, u32 cnt)\n{\n\tq->cached_cons -= cnt;\n}\n\nstatic inline u32 xskq_cons_present_entries(struct xsk_queue *q)\n{\n\t \n\treturn READ_ONCE(q->ring->producer) - READ_ONCE(q->ring->consumer);\n}\n\n \n\nstatic inline u32 xskq_prod_nb_free(struct xsk_queue *q, u32 max)\n{\n\tu32 free_entries = q->nentries - (q->cached_prod - q->cached_cons);\n\n\tif (free_entries >= max)\n\t\treturn max;\n\n\t \n\tq->cached_cons = READ_ONCE(q->ring->consumer);\n\tfree_entries = q->nentries - (q->cached_prod - q->cached_cons);\n\n\treturn free_entries >= max ? max : free_entries;\n}\n\nstatic inline bool xskq_prod_is_full(struct xsk_queue *q)\n{\n\treturn xskq_prod_nb_free(q, 1) ? false : true;\n}\n\nstatic inline void xskq_prod_cancel_n(struct xsk_queue *q, u32 cnt)\n{\n\tq->cached_prod -= cnt;\n}\n\nstatic inline int xskq_prod_reserve(struct xsk_queue *q)\n{\n\tif (xskq_prod_is_full(q))\n\t\treturn -ENOSPC;\n\n\t \n\tq->cached_prod++;\n\treturn 0;\n}\n\nstatic inline int xskq_prod_reserve_addr(struct xsk_queue *q, u64 addr)\n{\n\tstruct xdp_umem_ring *ring = (struct xdp_umem_ring *)q->ring;\n\n\tif (xskq_prod_is_full(q))\n\t\treturn -ENOSPC;\n\n\t \n\tring->desc[q->cached_prod++ & q->ring_mask] = addr;\n\treturn 0;\n}\n\nstatic inline void xskq_prod_write_addr_batch(struct xsk_queue *q, struct xdp_desc *descs,\n\t\t\t\t\t      u32 nb_entries)\n{\n\tstruct xdp_umem_ring *ring = (struct xdp_umem_ring *)q->ring;\n\tu32 i, cached_prod;\n\n\t \n\tcached_prod = q->cached_prod;\n\tfor (i = 0; i < nb_entries; i++)\n\t\tring->desc[cached_prod++ & q->ring_mask] = descs[i].addr;\n\tq->cached_prod = cached_prod;\n}\n\nstatic inline int xskq_prod_reserve_desc(struct xsk_queue *q,\n\t\t\t\t\t u64 addr, u32 len, u32 flags)\n{\n\tstruct xdp_rxtx_ring *ring = (struct xdp_rxtx_ring *)q->ring;\n\tu32 idx;\n\n\tif (xskq_prod_is_full(q))\n\t\treturn -ENOBUFS;\n\n\t \n\tidx = q->cached_prod++ & q->ring_mask;\n\tring->desc[idx].addr = addr;\n\tring->desc[idx].len = len;\n\tring->desc[idx].options = flags;\n\n\treturn 0;\n}\n\nstatic inline void __xskq_prod_submit(struct xsk_queue *q, u32 idx)\n{\n\tsmp_store_release(&q->ring->producer, idx);  \n}\n\nstatic inline void xskq_prod_submit(struct xsk_queue *q)\n{\n\t__xskq_prod_submit(q, q->cached_prod);\n}\n\nstatic inline void xskq_prod_submit_n(struct xsk_queue *q, u32 nb_entries)\n{\n\t__xskq_prod_submit(q, q->ring->producer + nb_entries);\n}\n\nstatic inline bool xskq_prod_is_empty(struct xsk_queue *q)\n{\n\t \n\treturn READ_ONCE(q->ring->consumer) == READ_ONCE(q->ring->producer);\n}\n\n \n\nstatic inline u64 xskq_nb_invalid_descs(struct xsk_queue *q)\n{\n\treturn q ? q->invalid_descs : 0;\n}\n\nstatic inline u64 xskq_nb_queue_empty_descs(struct xsk_queue *q)\n{\n\treturn q ? q->queue_empty_descs : 0;\n}\n\nstruct xsk_queue *xskq_create(u32 nentries, bool umem_queue);\nvoid xskq_destroy(struct xsk_queue *q_ops);\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}