{
  "module_name": "xsk.c",
  "hash_id": "4f3b366ac9df1e160fbc0d052325fb2bf1f3ebde01a73ec9d886a736d3f1fb25",
  "original_prompt": "Ingested from linux-6.6.14/net/xdp/xsk.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) \"AF_XDP: %s: \" fmt, __func__\n\n#include <linux/if_xdp.h>\n#include <linux/init.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/task.h>\n#include <linux/socket.h>\n#include <linux/file.h>\n#include <linux/uaccess.h>\n#include <linux/net.h>\n#include <linux/netdevice.h>\n#include <linux/rculist.h>\n#include <linux/vmalloc.h>\n#include <net/xdp_sock_drv.h>\n#include <net/busy_poll.h>\n#include <net/netdev_rx_queue.h>\n#include <net/xdp.h>\n\n#include \"xsk_queue.h\"\n#include \"xdp_umem.h\"\n#include \"xsk.h\"\n\n#define TX_BATCH_SIZE 32\n\nstatic DEFINE_PER_CPU(struct list_head, xskmap_flush_list);\n\nvoid xsk_set_rx_need_wakeup(struct xsk_buff_pool *pool)\n{\n\tif (pool->cached_need_wakeup & XDP_WAKEUP_RX)\n\t\treturn;\n\n\tpool->fq->ring->flags |= XDP_RING_NEED_WAKEUP;\n\tpool->cached_need_wakeup |= XDP_WAKEUP_RX;\n}\nEXPORT_SYMBOL(xsk_set_rx_need_wakeup);\n\nvoid xsk_set_tx_need_wakeup(struct xsk_buff_pool *pool)\n{\n\tstruct xdp_sock *xs;\n\n\tif (pool->cached_need_wakeup & XDP_WAKEUP_TX)\n\t\treturn;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(xs, &pool->xsk_tx_list, tx_list) {\n\t\txs->tx->ring->flags |= XDP_RING_NEED_WAKEUP;\n\t}\n\trcu_read_unlock();\n\n\tpool->cached_need_wakeup |= XDP_WAKEUP_TX;\n}\nEXPORT_SYMBOL(xsk_set_tx_need_wakeup);\n\nvoid xsk_clear_rx_need_wakeup(struct xsk_buff_pool *pool)\n{\n\tif (!(pool->cached_need_wakeup & XDP_WAKEUP_RX))\n\t\treturn;\n\n\tpool->fq->ring->flags &= ~XDP_RING_NEED_WAKEUP;\n\tpool->cached_need_wakeup &= ~XDP_WAKEUP_RX;\n}\nEXPORT_SYMBOL(xsk_clear_rx_need_wakeup);\n\nvoid xsk_clear_tx_need_wakeup(struct xsk_buff_pool *pool)\n{\n\tstruct xdp_sock *xs;\n\n\tif (!(pool->cached_need_wakeup & XDP_WAKEUP_TX))\n\t\treturn;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(xs, &pool->xsk_tx_list, tx_list) {\n\t\txs->tx->ring->flags &= ~XDP_RING_NEED_WAKEUP;\n\t}\n\trcu_read_unlock();\n\n\tpool->cached_need_wakeup &= ~XDP_WAKEUP_TX;\n}\nEXPORT_SYMBOL(xsk_clear_tx_need_wakeup);\n\nbool xsk_uses_need_wakeup(struct xsk_buff_pool *pool)\n{\n\treturn pool->uses_need_wakeup;\n}\nEXPORT_SYMBOL(xsk_uses_need_wakeup);\n\nstruct xsk_buff_pool *xsk_get_pool_from_qid(struct net_device *dev,\n\t\t\t\t\t    u16 queue_id)\n{\n\tif (queue_id < dev->real_num_rx_queues)\n\t\treturn dev->_rx[queue_id].pool;\n\tif (queue_id < dev->real_num_tx_queues)\n\t\treturn dev->_tx[queue_id].pool;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(xsk_get_pool_from_qid);\n\nvoid xsk_clear_pool_at_qid(struct net_device *dev, u16 queue_id)\n{\n\tif (queue_id < dev->num_rx_queues)\n\t\tdev->_rx[queue_id].pool = NULL;\n\tif (queue_id < dev->num_tx_queues)\n\t\tdev->_tx[queue_id].pool = NULL;\n}\n\n \nint xsk_reg_pool_at_qid(struct net_device *dev, struct xsk_buff_pool *pool,\n\t\t\tu16 queue_id)\n{\n\tif (queue_id >= max_t(unsigned int,\n\t\t\t      dev->real_num_rx_queues,\n\t\t\t      dev->real_num_tx_queues))\n\t\treturn -EINVAL;\n\n\tif (queue_id < dev->real_num_rx_queues)\n\t\tdev->_rx[queue_id].pool = pool;\n\tif (queue_id < dev->real_num_tx_queues)\n\t\tdev->_tx[queue_id].pool = pool;\n\n\treturn 0;\n}\n\nstatic int __xsk_rcv_zc(struct xdp_sock *xs, struct xdp_buff_xsk *xskb, u32 len,\n\t\t\tu32 flags)\n{\n\tu64 addr;\n\tint err;\n\n\taddr = xp_get_handle(xskb);\n\terr = xskq_prod_reserve_desc(xs->rx, addr, len, flags);\n\tif (err) {\n\t\txs->rx_queue_full++;\n\t\treturn err;\n\t}\n\n\txp_release(xskb);\n\treturn 0;\n}\n\nstatic int xsk_rcv_zc(struct xdp_sock *xs, struct xdp_buff *xdp, u32 len)\n{\n\tstruct xdp_buff_xsk *xskb = container_of(xdp, struct xdp_buff_xsk, xdp);\n\tu32 frags = xdp_buff_has_frags(xdp);\n\tstruct xdp_buff_xsk *pos, *tmp;\n\tstruct list_head *xskb_list;\n\tu32 contd = 0;\n\tint err;\n\n\tif (frags)\n\t\tcontd = XDP_PKT_CONTD;\n\n\terr = __xsk_rcv_zc(xs, xskb, len, contd);\n\tif (err || likely(!frags))\n\t\tgoto out;\n\n\txskb_list = &xskb->pool->xskb_list;\n\tlist_for_each_entry_safe(pos, tmp, xskb_list, xskb_list_node) {\n\t\tif (list_is_singular(xskb_list))\n\t\t\tcontd = 0;\n\t\tlen = pos->xdp.data_end - pos->xdp.data;\n\t\terr = __xsk_rcv_zc(xs, pos, len, contd);\n\t\tif (err)\n\t\t\treturn err;\n\t\tlist_del(&pos->xskb_list_node);\n\t}\n\nout:\n\treturn err;\n}\n\nstatic void *xsk_copy_xdp_start(struct xdp_buff *from)\n{\n\tif (unlikely(xdp_data_meta_unsupported(from)))\n\t\treturn from->data;\n\telse\n\t\treturn from->data_meta;\n}\n\nstatic u32 xsk_copy_xdp(void *to, void **from, u32 to_len,\n\t\t\tu32 *from_len, skb_frag_t **frag, u32 rem)\n{\n\tu32 copied = 0;\n\n\twhile (1) {\n\t\tu32 copy_len = min_t(u32, *from_len, to_len);\n\n\t\tmemcpy(to, *from, copy_len);\n\t\tcopied += copy_len;\n\t\tif (rem == copied)\n\t\t\treturn copied;\n\n\t\tif (*from_len == copy_len) {\n\t\t\t*from = skb_frag_address(*frag);\n\t\t\t*from_len = skb_frag_size((*frag)++);\n\t\t} else {\n\t\t\t*from += copy_len;\n\t\t\t*from_len -= copy_len;\n\t\t}\n\t\tif (to_len == copy_len)\n\t\t\treturn copied;\n\n\t\tto_len -= copy_len;\n\t\tto += copy_len;\n\t}\n}\n\nstatic int __xsk_rcv(struct xdp_sock *xs, struct xdp_buff *xdp, u32 len)\n{\n\tu32 frame_size = xsk_pool_get_rx_frame_size(xs->pool);\n\tvoid *copy_from = xsk_copy_xdp_start(xdp), *copy_to;\n\tu32 from_len, meta_len, rem, num_desc;\n\tstruct xdp_buff_xsk *xskb;\n\tstruct xdp_buff *xsk_xdp;\n\tskb_frag_t *frag;\n\n\tfrom_len = xdp->data_end - copy_from;\n\tmeta_len = xdp->data - copy_from;\n\trem = len + meta_len;\n\n\tif (len <= frame_size && !xdp_buff_has_frags(xdp)) {\n\t\tint err;\n\n\t\txsk_xdp = xsk_buff_alloc(xs->pool);\n\t\tif (!xsk_xdp) {\n\t\t\txs->rx_dropped++;\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tmemcpy(xsk_xdp->data - meta_len, copy_from, rem);\n\t\txskb = container_of(xsk_xdp, struct xdp_buff_xsk, xdp);\n\t\terr = __xsk_rcv_zc(xs, xskb, len, 0);\n\t\tif (err) {\n\t\t\txsk_buff_free(xsk_xdp);\n\t\t\treturn err;\n\t\t}\n\n\t\treturn 0;\n\t}\n\n\tnum_desc = (len - 1) / frame_size + 1;\n\n\tif (!xsk_buff_can_alloc(xs->pool, num_desc)) {\n\t\txs->rx_dropped++;\n\t\treturn -ENOMEM;\n\t}\n\tif (xskq_prod_nb_free(xs->rx, num_desc) < num_desc) {\n\t\txs->rx_queue_full++;\n\t\treturn -ENOBUFS;\n\t}\n\n\tif (xdp_buff_has_frags(xdp)) {\n\t\tstruct skb_shared_info *sinfo;\n\n\t\tsinfo = xdp_get_shared_info_from_buff(xdp);\n\t\tfrag =  &sinfo->frags[0];\n\t}\n\n\tdo {\n\t\tu32 to_len = frame_size + meta_len;\n\t\tu32 copied;\n\n\t\txsk_xdp = xsk_buff_alloc(xs->pool);\n\t\tcopy_to = xsk_xdp->data - meta_len;\n\n\t\tcopied = xsk_copy_xdp(copy_to, &copy_from, to_len, &from_len, &frag, rem);\n\t\trem -= copied;\n\n\t\txskb = container_of(xsk_xdp, struct xdp_buff_xsk, xdp);\n\t\t__xsk_rcv_zc(xs, xskb, copied - meta_len, rem ? XDP_PKT_CONTD : 0);\n\t\tmeta_len = 0;\n\t} while (rem);\n\n\treturn 0;\n}\n\nstatic bool xsk_tx_writeable(struct xdp_sock *xs)\n{\n\tif (xskq_cons_present_entries(xs->tx) > xs->tx->nentries / 2)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic bool xsk_is_bound(struct xdp_sock *xs)\n{\n\tif (READ_ONCE(xs->state) == XSK_BOUND) {\n\t\t \n\t\tsmp_rmb();\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic int xsk_rcv_check(struct xdp_sock *xs, struct xdp_buff *xdp, u32 len)\n{\n\tif (!xsk_is_bound(xs))\n\t\treturn -ENXIO;\n\n\tif (xs->dev != xdp->rxq->dev || xs->queue_id != xdp->rxq->queue_index)\n\t\treturn -EINVAL;\n\n\tif (len > xsk_pool_get_rx_frame_size(xs->pool) && !xs->sg) {\n\t\txs->rx_dropped++;\n\t\treturn -ENOSPC;\n\t}\n\n\tsk_mark_napi_id_once_xdp(&xs->sk, xdp);\n\treturn 0;\n}\n\nstatic void xsk_flush(struct xdp_sock *xs)\n{\n\txskq_prod_submit(xs->rx);\n\t__xskq_cons_release(xs->pool->fq);\n\tsock_def_readable(&xs->sk);\n}\n\nint xsk_generic_rcv(struct xdp_sock *xs, struct xdp_buff *xdp)\n{\n\tu32 len = xdp_get_buff_len(xdp);\n\tint err;\n\n\tspin_lock_bh(&xs->rx_lock);\n\terr = xsk_rcv_check(xs, xdp, len);\n\tif (!err) {\n\t\terr = __xsk_rcv(xs, xdp, len);\n\t\txsk_flush(xs);\n\t}\n\tspin_unlock_bh(&xs->rx_lock);\n\treturn err;\n}\n\nstatic int xsk_rcv(struct xdp_sock *xs, struct xdp_buff *xdp)\n{\n\tu32 len = xdp_get_buff_len(xdp);\n\tint err;\n\n\terr = xsk_rcv_check(xs, xdp, len);\n\tif (err)\n\t\treturn err;\n\n\tif (xdp->rxq->mem.type == MEM_TYPE_XSK_BUFF_POOL) {\n\t\tlen = xdp->data_end - xdp->data;\n\t\treturn xsk_rcv_zc(xs, xdp, len);\n\t}\n\n\terr = __xsk_rcv(xs, xdp, len);\n\tif (!err)\n\t\txdp_return_buff(xdp);\n\treturn err;\n}\n\nint __xsk_map_redirect(struct xdp_sock *xs, struct xdp_buff *xdp)\n{\n\tstruct list_head *flush_list = this_cpu_ptr(&xskmap_flush_list);\n\tint err;\n\n\terr = xsk_rcv(xs, xdp);\n\tif (err)\n\t\treturn err;\n\n\tif (!xs->flush_node.prev)\n\t\tlist_add(&xs->flush_node, flush_list);\n\n\treturn 0;\n}\n\nvoid __xsk_map_flush(void)\n{\n\tstruct list_head *flush_list = this_cpu_ptr(&xskmap_flush_list);\n\tstruct xdp_sock *xs, *tmp;\n\n\tlist_for_each_entry_safe(xs, tmp, flush_list, flush_node) {\n\t\txsk_flush(xs);\n\t\t__list_del_clearprev(&xs->flush_node);\n\t}\n}\n\nvoid xsk_tx_completed(struct xsk_buff_pool *pool, u32 nb_entries)\n{\n\txskq_prod_submit_n(pool->cq, nb_entries);\n}\nEXPORT_SYMBOL(xsk_tx_completed);\n\nvoid xsk_tx_release(struct xsk_buff_pool *pool)\n{\n\tstruct xdp_sock *xs;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(xs, &pool->xsk_tx_list, tx_list) {\n\t\t__xskq_cons_release(xs->tx);\n\t\tif (xsk_tx_writeable(xs))\n\t\t\txs->sk.sk_write_space(&xs->sk);\n\t}\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL(xsk_tx_release);\n\nbool xsk_tx_peek_desc(struct xsk_buff_pool *pool, struct xdp_desc *desc)\n{\n\tstruct xdp_sock *xs;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(xs, &pool->xsk_tx_list, tx_list) {\n\t\tif (!xskq_cons_peek_desc(xs->tx, desc, pool)) {\n\t\t\tif (xskq_has_descs(xs->tx))\n\t\t\t\txskq_cons_release(xs->tx);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (xskq_prod_reserve_addr(pool->cq, desc->addr))\n\t\t\tgoto out;\n\n\t\txskq_cons_release(xs->tx);\n\t\trcu_read_unlock();\n\t\treturn true;\n\t}\n\nout:\n\trcu_read_unlock();\n\treturn false;\n}\nEXPORT_SYMBOL(xsk_tx_peek_desc);\n\nstatic u32 xsk_tx_peek_release_fallback(struct xsk_buff_pool *pool, u32 max_entries)\n{\n\tstruct xdp_desc *descs = pool->tx_descs;\n\tu32 nb_pkts = 0;\n\n\twhile (nb_pkts < max_entries && xsk_tx_peek_desc(pool, &descs[nb_pkts]))\n\t\tnb_pkts++;\n\n\txsk_tx_release(pool);\n\treturn nb_pkts;\n}\n\nu32 xsk_tx_peek_release_desc_batch(struct xsk_buff_pool *pool, u32 nb_pkts)\n{\n\tstruct xdp_sock *xs;\n\n\trcu_read_lock();\n\tif (!list_is_singular(&pool->xsk_tx_list)) {\n\t\t \n\t\trcu_read_unlock();\n\t\treturn xsk_tx_peek_release_fallback(pool, nb_pkts);\n\t}\n\n\txs = list_first_or_null_rcu(&pool->xsk_tx_list, struct xdp_sock, tx_list);\n\tif (!xs) {\n\t\tnb_pkts = 0;\n\t\tgoto out;\n\t}\n\n\tnb_pkts = xskq_cons_nb_entries(xs->tx, nb_pkts);\n\n\t \n\tnb_pkts = xskq_prod_nb_free(pool->cq, nb_pkts);\n\tif (!nb_pkts)\n\t\tgoto out;\n\n\tnb_pkts = xskq_cons_read_desc_batch(xs->tx, pool, nb_pkts);\n\tif (!nb_pkts) {\n\t\txs->tx->queue_empty_descs++;\n\t\tgoto out;\n\t}\n\n\t__xskq_cons_release(xs->tx);\n\txskq_prod_write_addr_batch(pool->cq, pool->tx_descs, nb_pkts);\n\txs->sk.sk_write_space(&xs->sk);\n\nout:\n\trcu_read_unlock();\n\treturn nb_pkts;\n}\nEXPORT_SYMBOL(xsk_tx_peek_release_desc_batch);\n\nstatic int xsk_wakeup(struct xdp_sock *xs, u8 flags)\n{\n\tstruct net_device *dev = xs->dev;\n\n\treturn dev->netdev_ops->ndo_xsk_wakeup(dev, xs->queue_id, flags);\n}\n\nstatic int xsk_cq_reserve_addr_locked(struct xdp_sock *xs, u64 addr)\n{\n\tunsigned long flags;\n\tint ret;\n\n\tspin_lock_irqsave(&xs->pool->cq_lock, flags);\n\tret = xskq_prod_reserve_addr(xs->pool->cq, addr);\n\tspin_unlock_irqrestore(&xs->pool->cq_lock, flags);\n\n\treturn ret;\n}\n\nstatic void xsk_cq_submit_locked(struct xdp_sock *xs, u32 n)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&xs->pool->cq_lock, flags);\n\txskq_prod_submit_n(xs->pool->cq, n);\n\tspin_unlock_irqrestore(&xs->pool->cq_lock, flags);\n}\n\nstatic void xsk_cq_cancel_locked(struct xdp_sock *xs, u32 n)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&xs->pool->cq_lock, flags);\n\txskq_prod_cancel_n(xs->pool->cq, n);\n\tspin_unlock_irqrestore(&xs->pool->cq_lock, flags);\n}\n\nstatic u32 xsk_get_num_desc(struct sk_buff *skb)\n{\n\treturn skb ? (long)skb_shinfo(skb)->destructor_arg : 0;\n}\n\nstatic void xsk_destruct_skb(struct sk_buff *skb)\n{\n\txsk_cq_submit_locked(xdp_sk(skb->sk), xsk_get_num_desc(skb));\n\tsock_wfree(skb);\n}\n\nstatic void xsk_set_destructor_arg(struct sk_buff *skb)\n{\n\tlong num = xsk_get_num_desc(xdp_sk(skb->sk)->skb) + 1;\n\n\tskb_shinfo(skb)->destructor_arg = (void *)num;\n}\n\nstatic void xsk_consume_skb(struct sk_buff *skb)\n{\n\tstruct xdp_sock *xs = xdp_sk(skb->sk);\n\n\tskb->destructor = sock_wfree;\n\txsk_cq_cancel_locked(xs, xsk_get_num_desc(skb));\n\t \n\tconsume_skb(skb);\n\txs->skb = NULL;\n}\n\nstatic void xsk_drop_skb(struct sk_buff *skb)\n{\n\txdp_sk(skb->sk)->tx->invalid_descs += xsk_get_num_desc(skb);\n\txsk_consume_skb(skb);\n}\n\nstatic struct sk_buff *xsk_build_skb_zerocopy(struct xdp_sock *xs,\n\t\t\t\t\t      struct xdp_desc *desc)\n{\n\tstruct xsk_buff_pool *pool = xs->pool;\n\tu32 hr, len, ts, offset, copy, copied;\n\tstruct sk_buff *skb = xs->skb;\n\tstruct page *page;\n\tvoid *buffer;\n\tint err, i;\n\tu64 addr;\n\n\tif (!skb) {\n\t\thr = max(NET_SKB_PAD, L1_CACHE_ALIGN(xs->dev->needed_headroom));\n\n\t\tskb = sock_alloc_send_skb(&xs->sk, hr, 1, &err);\n\t\tif (unlikely(!skb))\n\t\t\treturn ERR_PTR(err);\n\n\t\tskb_reserve(skb, hr);\n\t}\n\n\taddr = desc->addr;\n\tlen = desc->len;\n\tts = pool->unaligned ? len : pool->chunk_size;\n\n\tbuffer = xsk_buff_raw_get_data(pool, addr);\n\toffset = offset_in_page(buffer);\n\taddr = buffer - pool->addrs;\n\n\tfor (copied = 0, i = skb_shinfo(skb)->nr_frags; copied < len; i++) {\n\t\tif (unlikely(i >= MAX_SKB_FRAGS))\n\t\t\treturn ERR_PTR(-EOVERFLOW);\n\n\t\tpage = pool->umem->pgs[addr >> PAGE_SHIFT];\n\t\tget_page(page);\n\n\t\tcopy = min_t(u32, PAGE_SIZE - offset, len - copied);\n\t\tskb_fill_page_desc(skb, i, page, offset, copy);\n\n\t\tcopied += copy;\n\t\taddr += copy;\n\t\toffset = 0;\n\t}\n\n\tskb->len += len;\n\tskb->data_len += len;\n\tskb->truesize += ts;\n\n\trefcount_add(ts, &xs->sk.sk_wmem_alloc);\n\n\treturn skb;\n}\n\nstatic struct sk_buff *xsk_build_skb(struct xdp_sock *xs,\n\t\t\t\t     struct xdp_desc *desc)\n{\n\tstruct net_device *dev = xs->dev;\n\tstruct sk_buff *skb = xs->skb;\n\tint err;\n\n\tif (dev->priv_flags & IFF_TX_SKB_NO_LINEAR) {\n\t\tskb = xsk_build_skb_zerocopy(xs, desc);\n\t\tif (IS_ERR(skb)) {\n\t\t\terr = PTR_ERR(skb);\n\t\t\tgoto free_err;\n\t\t}\n\t} else {\n\t\tu32 hr, tr, len;\n\t\tvoid *buffer;\n\n\t\tbuffer = xsk_buff_raw_get_data(xs->pool, desc->addr);\n\t\tlen = desc->len;\n\n\t\tif (!skb) {\n\t\t\thr = max(NET_SKB_PAD, L1_CACHE_ALIGN(dev->needed_headroom));\n\t\t\ttr = dev->needed_tailroom;\n\t\t\tskb = sock_alloc_send_skb(&xs->sk, hr + len + tr, 1, &err);\n\t\t\tif (unlikely(!skb))\n\t\t\t\tgoto free_err;\n\n\t\t\tskb_reserve(skb, hr);\n\t\t\tskb_put(skb, len);\n\n\t\t\terr = skb_store_bits(skb, 0, buffer, len);\n\t\t\tif (unlikely(err)) {\n\t\t\t\tkfree_skb(skb);\n\t\t\t\tgoto free_err;\n\t\t\t}\n\t\t} else {\n\t\t\tint nr_frags = skb_shinfo(skb)->nr_frags;\n\t\t\tstruct page *page;\n\t\t\tu8 *vaddr;\n\n\t\t\tif (unlikely(nr_frags == (MAX_SKB_FRAGS - 1) && xp_mb_desc(desc))) {\n\t\t\t\terr = -EOVERFLOW;\n\t\t\t\tgoto free_err;\n\t\t\t}\n\n\t\t\tpage = alloc_page(xs->sk.sk_allocation);\n\t\t\tif (unlikely(!page)) {\n\t\t\t\terr = -EAGAIN;\n\t\t\t\tgoto free_err;\n\t\t\t}\n\n\t\t\tvaddr = kmap_local_page(page);\n\t\t\tmemcpy(vaddr, buffer, len);\n\t\t\tkunmap_local(vaddr);\n\n\t\t\tskb_add_rx_frag(skb, nr_frags, page, 0, len, 0);\n\t\t}\n\t}\n\n\tskb->dev = dev;\n\tskb->priority = xs->sk.sk_priority;\n\tskb->mark = READ_ONCE(xs->sk.sk_mark);\n\tskb->destructor = xsk_destruct_skb;\n\txsk_set_destructor_arg(skb);\n\n\treturn skb;\n\nfree_err:\n\tif (err == -EOVERFLOW) {\n\t\t \n\t\txsk_set_destructor_arg(xs->skb);\n\t\txsk_drop_skb(xs->skb);\n\t\txskq_cons_release(xs->tx);\n\t} else {\n\t\t \n\t\txsk_cq_cancel_locked(xs, 1);\n\t}\n\n\treturn ERR_PTR(err);\n}\n\nstatic int __xsk_generic_xmit(struct sock *sk)\n{\n\tstruct xdp_sock *xs = xdp_sk(sk);\n\tu32 max_batch = TX_BATCH_SIZE;\n\tbool sent_frame = false;\n\tstruct xdp_desc desc;\n\tstruct sk_buff *skb;\n\tint err = 0;\n\n\tmutex_lock(&xs->mutex);\n\n\t \n\tif (unlikely(!xsk_is_bound(xs))) {\n\t\terr = -ENXIO;\n\t\tgoto out;\n\t}\n\n\tif (xs->queue_id >= xs->dev->real_num_tx_queues)\n\t\tgoto out;\n\n\twhile (xskq_cons_peek_desc(xs->tx, &desc, xs->pool)) {\n\t\tif (max_batch-- == 0) {\n\t\t\terr = -EAGAIN;\n\t\t\tgoto out;\n\t\t}\n\n\t\t \n\t\tif (xsk_cq_reserve_addr_locked(xs, desc.addr))\n\t\t\tgoto out;\n\n\t\tskb = xsk_build_skb(xs, &desc);\n\t\tif (IS_ERR(skb)) {\n\t\t\terr = PTR_ERR(skb);\n\t\t\tif (err != -EOVERFLOW)\n\t\t\t\tgoto out;\n\t\t\terr = 0;\n\t\t\tcontinue;\n\t\t}\n\n\t\txskq_cons_release(xs->tx);\n\n\t\tif (xp_mb_desc(&desc)) {\n\t\t\txs->skb = skb;\n\t\t\tcontinue;\n\t\t}\n\n\t\terr = __dev_direct_xmit(skb, xs->queue_id);\n\t\tif  (err == NETDEV_TX_BUSY) {\n\t\t\t \n\t\t\txskq_cons_cancel_n(xs->tx, xsk_get_num_desc(skb));\n\t\t\txsk_consume_skb(skb);\n\t\t\terr = -EAGAIN;\n\t\t\tgoto out;\n\t\t}\n\n\t\t \n\t\tif (err == NET_XMIT_DROP) {\n\t\t\t \n\t\t\terr = -EBUSY;\n\t\t\txs->skb = NULL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tsent_frame = true;\n\t\txs->skb = NULL;\n\t}\n\n\tif (xskq_has_descs(xs->tx)) {\n\t\tif (xs->skb)\n\t\t\txsk_drop_skb(xs->skb);\n\t\txskq_cons_release(xs->tx);\n\t}\n\nout:\n\tif (sent_frame)\n\t\tif (xsk_tx_writeable(xs))\n\t\t\tsk->sk_write_space(sk);\n\n\tmutex_unlock(&xs->mutex);\n\treturn err;\n}\n\nstatic int xsk_generic_xmit(struct sock *sk)\n{\n\tint ret;\n\n\t \n\trcu_read_unlock();\n\tret = __xsk_generic_xmit(sk);\n\t \n\trcu_read_lock();\n\n\treturn ret;\n}\n\nstatic bool xsk_no_wakeup(struct sock *sk)\n{\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\t \n\treturn READ_ONCE(sk->sk_prefer_busy_poll) && READ_ONCE(sk->sk_ll_usec) &&\n\t\tREAD_ONCE(sk->sk_napi_id) >= MIN_NAPI_ID;\n#else\n\treturn false;\n#endif\n}\n\nstatic int xsk_check_common(struct xdp_sock *xs)\n{\n\tif (unlikely(!xsk_is_bound(xs)))\n\t\treturn -ENXIO;\n\tif (unlikely(!(xs->dev->flags & IFF_UP)))\n\t\treturn -ENETDOWN;\n\n\treturn 0;\n}\n\nstatic int __xsk_sendmsg(struct socket *sock, struct msghdr *m, size_t total_len)\n{\n\tbool need_wait = !(m->msg_flags & MSG_DONTWAIT);\n\tstruct sock *sk = sock->sk;\n\tstruct xdp_sock *xs = xdp_sk(sk);\n\tstruct xsk_buff_pool *pool;\n\tint err;\n\n\terr = xsk_check_common(xs);\n\tif (err)\n\t\treturn err;\n\tif (unlikely(need_wait))\n\t\treturn -EOPNOTSUPP;\n\tif (unlikely(!xs->tx))\n\t\treturn -ENOBUFS;\n\n\tif (sk_can_busy_loop(sk)) {\n\t\tif (xs->zc)\n\t\t\t__sk_mark_napi_id_once(sk, xsk_pool_get_napi_id(xs->pool));\n\t\tsk_busy_loop(sk, 1);  \n\t}\n\n\tif (xs->zc && xsk_no_wakeup(sk))\n\t\treturn 0;\n\n\tpool = xs->pool;\n\tif (pool->cached_need_wakeup & XDP_WAKEUP_TX) {\n\t\tif (xs->zc)\n\t\t\treturn xsk_wakeup(xs, XDP_WAKEUP_TX);\n\t\treturn xsk_generic_xmit(sk);\n\t}\n\treturn 0;\n}\n\nstatic int xsk_sendmsg(struct socket *sock, struct msghdr *m, size_t total_len)\n{\n\tint ret;\n\n\trcu_read_lock();\n\tret = __xsk_sendmsg(sock, m, total_len);\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\nstatic int __xsk_recvmsg(struct socket *sock, struct msghdr *m, size_t len, int flags)\n{\n\tbool need_wait = !(flags & MSG_DONTWAIT);\n\tstruct sock *sk = sock->sk;\n\tstruct xdp_sock *xs = xdp_sk(sk);\n\tint err;\n\n\terr = xsk_check_common(xs);\n\tif (err)\n\t\treturn err;\n\tif (unlikely(!xs->rx))\n\t\treturn -ENOBUFS;\n\tif (unlikely(need_wait))\n\t\treturn -EOPNOTSUPP;\n\n\tif (sk_can_busy_loop(sk))\n\t\tsk_busy_loop(sk, 1);  \n\n\tif (xsk_no_wakeup(sk))\n\t\treturn 0;\n\n\tif (xs->pool->cached_need_wakeup & XDP_WAKEUP_RX && xs->zc)\n\t\treturn xsk_wakeup(xs, XDP_WAKEUP_RX);\n\treturn 0;\n}\n\nstatic int xsk_recvmsg(struct socket *sock, struct msghdr *m, size_t len, int flags)\n{\n\tint ret;\n\n\trcu_read_lock();\n\tret = __xsk_recvmsg(sock, m, len, flags);\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\nstatic __poll_t xsk_poll(struct file *file, struct socket *sock,\n\t\t\t     struct poll_table_struct *wait)\n{\n\t__poll_t mask = 0;\n\tstruct sock *sk = sock->sk;\n\tstruct xdp_sock *xs = xdp_sk(sk);\n\tstruct xsk_buff_pool *pool;\n\n\tsock_poll_wait(file, sock, wait);\n\n\trcu_read_lock();\n\tif (xsk_check_common(xs))\n\t\tgoto out;\n\n\tpool = xs->pool;\n\n\tif (pool->cached_need_wakeup) {\n\t\tif (xs->zc)\n\t\t\txsk_wakeup(xs, pool->cached_need_wakeup);\n\t\telse if (xs->tx)\n\t\t\t \n\t\t\txsk_generic_xmit(sk);\n\t}\n\n\tif (xs->rx && !xskq_prod_is_empty(xs->rx))\n\t\tmask |= EPOLLIN | EPOLLRDNORM;\n\tif (xs->tx && xsk_tx_writeable(xs))\n\t\tmask |= EPOLLOUT | EPOLLWRNORM;\nout:\n\trcu_read_unlock();\n\treturn mask;\n}\n\nstatic int xsk_init_queue(u32 entries, struct xsk_queue **queue,\n\t\t\t  bool umem_queue)\n{\n\tstruct xsk_queue *q;\n\n\tif (entries == 0 || *queue || !is_power_of_2(entries))\n\t\treturn -EINVAL;\n\n\tq = xskq_create(entries, umem_queue);\n\tif (!q)\n\t\treturn -ENOMEM;\n\n\t \n\tsmp_wmb();\n\tWRITE_ONCE(*queue, q);\n\treturn 0;\n}\n\nstatic void xsk_unbind_dev(struct xdp_sock *xs)\n{\n\tstruct net_device *dev = xs->dev;\n\n\tif (xs->state != XSK_BOUND)\n\t\treturn;\n\tWRITE_ONCE(xs->state, XSK_UNBOUND);\n\n\t \n\txp_del_xsk(xs->pool, xs);\n\tsynchronize_net();\n\tdev_put(dev);\n}\n\nstatic struct xsk_map *xsk_get_map_list_entry(struct xdp_sock *xs,\n\t\t\t\t\t      struct xdp_sock __rcu ***map_entry)\n{\n\tstruct xsk_map *map = NULL;\n\tstruct xsk_map_node *node;\n\n\t*map_entry = NULL;\n\n\tspin_lock_bh(&xs->map_list_lock);\n\tnode = list_first_entry_or_null(&xs->map_list, struct xsk_map_node,\n\t\t\t\t\tnode);\n\tif (node) {\n\t\tbpf_map_inc(&node->map->map);\n\t\tmap = node->map;\n\t\t*map_entry = node->map_entry;\n\t}\n\tspin_unlock_bh(&xs->map_list_lock);\n\treturn map;\n}\n\nstatic void xsk_delete_from_maps(struct xdp_sock *xs)\n{\n\t \n\tstruct xdp_sock __rcu **map_entry = NULL;\n\tstruct xsk_map *map;\n\n\twhile ((map = xsk_get_map_list_entry(xs, &map_entry))) {\n\t\txsk_map_try_sock_delete(map, xs, map_entry);\n\t\tbpf_map_put(&map->map);\n\t}\n}\n\nstatic int xsk_release(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct xdp_sock *xs = xdp_sk(sk);\n\tstruct net *net;\n\n\tif (!sk)\n\t\treturn 0;\n\n\tnet = sock_net(sk);\n\n\tif (xs->skb)\n\t\txsk_drop_skb(xs->skb);\n\n\tmutex_lock(&net->xdp.lock);\n\tsk_del_node_init_rcu(sk);\n\tmutex_unlock(&net->xdp.lock);\n\n\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\n\txsk_delete_from_maps(xs);\n\tmutex_lock(&xs->mutex);\n\txsk_unbind_dev(xs);\n\tmutex_unlock(&xs->mutex);\n\n\txskq_destroy(xs->rx);\n\txskq_destroy(xs->tx);\n\txskq_destroy(xs->fq_tmp);\n\txskq_destroy(xs->cq_tmp);\n\n\tsock_orphan(sk);\n\tsock->sk = NULL;\n\n\tsock_put(sk);\n\n\treturn 0;\n}\n\nstatic struct socket *xsk_lookup_xsk_from_fd(int fd)\n{\n\tstruct socket *sock;\n\tint err;\n\n\tsock = sockfd_lookup(fd, &err);\n\tif (!sock)\n\t\treturn ERR_PTR(-ENOTSOCK);\n\n\tif (sock->sk->sk_family != PF_XDP) {\n\t\tsockfd_put(sock);\n\t\treturn ERR_PTR(-ENOPROTOOPT);\n\t}\n\n\treturn sock;\n}\n\nstatic bool xsk_validate_queues(struct xdp_sock *xs)\n{\n\treturn xs->fq_tmp && xs->cq_tmp;\n}\n\nstatic int xsk_bind(struct socket *sock, struct sockaddr *addr, int addr_len)\n{\n\tstruct sockaddr_xdp *sxdp = (struct sockaddr_xdp *)addr;\n\tstruct sock *sk = sock->sk;\n\tstruct xdp_sock *xs = xdp_sk(sk);\n\tstruct net_device *dev;\n\tint bound_dev_if;\n\tu32 flags, qid;\n\tint err = 0;\n\n\tif (addr_len < sizeof(struct sockaddr_xdp))\n\t\treturn -EINVAL;\n\tif (sxdp->sxdp_family != AF_XDP)\n\t\treturn -EINVAL;\n\n\tflags = sxdp->sxdp_flags;\n\tif (flags & ~(XDP_SHARED_UMEM | XDP_COPY | XDP_ZEROCOPY |\n\t\t      XDP_USE_NEED_WAKEUP | XDP_USE_SG))\n\t\treturn -EINVAL;\n\n\tbound_dev_if = READ_ONCE(sk->sk_bound_dev_if);\n\tif (bound_dev_if && bound_dev_if != sxdp->sxdp_ifindex)\n\t\treturn -EINVAL;\n\n\trtnl_lock();\n\tmutex_lock(&xs->mutex);\n\tif (xs->state != XSK_READY) {\n\t\terr = -EBUSY;\n\t\tgoto out_release;\n\t}\n\n\tdev = dev_get_by_index(sock_net(sk), sxdp->sxdp_ifindex);\n\tif (!dev) {\n\t\terr = -ENODEV;\n\t\tgoto out_release;\n\t}\n\n\tif (!xs->rx && !xs->tx) {\n\t\terr = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\tqid = sxdp->sxdp_queue_id;\n\n\tif (flags & XDP_SHARED_UMEM) {\n\t\tstruct xdp_sock *umem_xs;\n\t\tstruct socket *sock;\n\n\t\tif ((flags & XDP_COPY) || (flags & XDP_ZEROCOPY) ||\n\t\t    (flags & XDP_USE_NEED_WAKEUP) || (flags & XDP_USE_SG)) {\n\t\t\t \n\t\t\terr = -EINVAL;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tif (xs->umem) {\n\t\t\t \n\t\t\terr = -EINVAL;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tsock = xsk_lookup_xsk_from_fd(sxdp->sxdp_shared_umem_fd);\n\t\tif (IS_ERR(sock)) {\n\t\t\terr = PTR_ERR(sock);\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tumem_xs = xdp_sk(sock->sk);\n\t\tif (!xsk_is_bound(umem_xs)) {\n\t\t\terr = -EBADF;\n\t\t\tsockfd_put(sock);\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tif (umem_xs->queue_id != qid || umem_xs->dev != dev) {\n\t\t\t \n\t\t\txs->pool = xp_create_and_assign_umem(xs,\n\t\t\t\t\t\t\t     umem_xs->umem);\n\t\t\tif (!xs->pool) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tsockfd_put(sock);\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\n\t\t\terr = xp_assign_dev_shared(xs->pool, umem_xs, dev,\n\t\t\t\t\t\t   qid);\n\t\t\tif (err) {\n\t\t\t\txp_destroy(xs->pool);\n\t\t\t\txs->pool = NULL;\n\t\t\t\tsockfd_put(sock);\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t} else {\n\t\t\t \n\t\t\tif (xs->fq_tmp || xs->cq_tmp) {\n\t\t\t\t \n\t\t\t\terr = -EINVAL;\n\t\t\t\tsockfd_put(sock);\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\n\t\t\txp_get_pool(umem_xs->pool);\n\t\t\txs->pool = umem_xs->pool;\n\n\t\t\t \n\t\t\tif (xs->tx && !xs->pool->tx_descs) {\n\t\t\t\terr = xp_alloc_tx_descs(xs->pool, xs);\n\t\t\t\tif (err) {\n\t\t\t\t\txp_put_pool(xs->pool);\n\t\t\t\t\txs->pool = NULL;\n\t\t\t\t\tsockfd_put(sock);\n\t\t\t\t\tgoto out_unlock;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\txdp_get_umem(umem_xs->umem);\n\t\tWRITE_ONCE(xs->umem, umem_xs->umem);\n\t\tsockfd_put(sock);\n\t} else if (!xs->umem || !xsk_validate_queues(xs)) {\n\t\terr = -EINVAL;\n\t\tgoto out_unlock;\n\t} else {\n\t\t \n\t\txs->pool = xp_create_and_assign_umem(xs, xs->umem);\n\t\tif (!xs->pool) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\terr = xp_assign_dev(xs->pool, dev, qid, flags);\n\t\tif (err) {\n\t\t\txp_destroy(xs->pool);\n\t\t\txs->pool = NULL;\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\t \n\txs->fq_tmp = NULL;\n\txs->cq_tmp = NULL;\n\n\txs->dev = dev;\n\txs->zc = xs->umem->zc;\n\txs->sg = !!(xs->umem->flags & XDP_UMEM_SG_FLAG);\n\txs->queue_id = qid;\n\txp_add_xsk(xs->pool, xs);\n\nout_unlock:\n\tif (err) {\n\t\tdev_put(dev);\n\t} else {\n\t\t \n\t\tsmp_wmb();\n\t\tWRITE_ONCE(xs->state, XSK_BOUND);\n\t}\nout_release:\n\tmutex_unlock(&xs->mutex);\n\trtnl_unlock();\n\treturn err;\n}\n\nstruct xdp_umem_reg_v1 {\n\t__u64 addr;  \n\t__u64 len;  \n\t__u32 chunk_size;\n\t__u32 headroom;\n};\n\nstatic int xsk_setsockopt(struct socket *sock, int level, int optname,\n\t\t\t  sockptr_t optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct xdp_sock *xs = xdp_sk(sk);\n\tint err;\n\n\tif (level != SOL_XDP)\n\t\treturn -ENOPROTOOPT;\n\n\tswitch (optname) {\n\tcase XDP_RX_RING:\n\tcase XDP_TX_RING:\n\t{\n\t\tstruct xsk_queue **q;\n\t\tint entries;\n\n\t\tif (optlen < sizeof(entries))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_sockptr(&entries, optval, sizeof(entries)))\n\t\t\treturn -EFAULT;\n\n\t\tmutex_lock(&xs->mutex);\n\t\tif (xs->state != XSK_READY) {\n\t\t\tmutex_unlock(&xs->mutex);\n\t\t\treturn -EBUSY;\n\t\t}\n\t\tq = (optname == XDP_TX_RING) ? &xs->tx : &xs->rx;\n\t\terr = xsk_init_queue(entries, q, false);\n\t\tif (!err && optname == XDP_TX_RING)\n\t\t\t \n\t\t\txs->tx->ring->flags |= XDP_RING_NEED_WAKEUP;\n\t\tmutex_unlock(&xs->mutex);\n\t\treturn err;\n\t}\n\tcase XDP_UMEM_REG:\n\t{\n\t\tsize_t mr_size = sizeof(struct xdp_umem_reg);\n\t\tstruct xdp_umem_reg mr = {};\n\t\tstruct xdp_umem *umem;\n\n\t\tif (optlen < sizeof(struct xdp_umem_reg_v1))\n\t\t\treturn -EINVAL;\n\t\telse if (optlen < sizeof(mr))\n\t\t\tmr_size = sizeof(struct xdp_umem_reg_v1);\n\n\t\tif (copy_from_sockptr(&mr, optval, mr_size))\n\t\t\treturn -EFAULT;\n\n\t\tmutex_lock(&xs->mutex);\n\t\tif (xs->state != XSK_READY || xs->umem) {\n\t\t\tmutex_unlock(&xs->mutex);\n\t\t\treturn -EBUSY;\n\t\t}\n\n\t\tumem = xdp_umem_create(&mr);\n\t\tif (IS_ERR(umem)) {\n\t\t\tmutex_unlock(&xs->mutex);\n\t\t\treturn PTR_ERR(umem);\n\t\t}\n\n\t\t \n\t\tsmp_wmb();\n\t\tWRITE_ONCE(xs->umem, umem);\n\t\tmutex_unlock(&xs->mutex);\n\t\treturn 0;\n\t}\n\tcase XDP_UMEM_FILL_RING:\n\tcase XDP_UMEM_COMPLETION_RING:\n\t{\n\t\tstruct xsk_queue **q;\n\t\tint entries;\n\n\t\tif (copy_from_sockptr(&entries, optval, sizeof(entries)))\n\t\t\treturn -EFAULT;\n\n\t\tmutex_lock(&xs->mutex);\n\t\tif (xs->state != XSK_READY) {\n\t\t\tmutex_unlock(&xs->mutex);\n\t\t\treturn -EBUSY;\n\t\t}\n\n\t\tq = (optname == XDP_UMEM_FILL_RING) ? &xs->fq_tmp :\n\t\t\t&xs->cq_tmp;\n\t\terr = xsk_init_queue(entries, q, true);\n\t\tmutex_unlock(&xs->mutex);\n\t\treturn err;\n\t}\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn -ENOPROTOOPT;\n}\n\nstatic void xsk_enter_rxtx_offsets(struct xdp_ring_offset_v1 *ring)\n{\n\tring->producer = offsetof(struct xdp_rxtx_ring, ptrs.producer);\n\tring->consumer = offsetof(struct xdp_rxtx_ring, ptrs.consumer);\n\tring->desc = offsetof(struct xdp_rxtx_ring, desc);\n}\n\nstatic void xsk_enter_umem_offsets(struct xdp_ring_offset_v1 *ring)\n{\n\tring->producer = offsetof(struct xdp_umem_ring, ptrs.producer);\n\tring->consumer = offsetof(struct xdp_umem_ring, ptrs.consumer);\n\tring->desc = offsetof(struct xdp_umem_ring, desc);\n}\n\nstruct xdp_statistics_v1 {\n\t__u64 rx_dropped;\n\t__u64 rx_invalid_descs;\n\t__u64 tx_invalid_descs;\n};\n\nstatic int xsk_getsockopt(struct socket *sock, int level, int optname,\n\t\t\t  char __user *optval, int __user *optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct xdp_sock *xs = xdp_sk(sk);\n\tint len;\n\n\tif (level != SOL_XDP)\n\t\treturn -ENOPROTOOPT;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (len < 0)\n\t\treturn -EINVAL;\n\n\tswitch (optname) {\n\tcase XDP_STATISTICS:\n\t{\n\t\tstruct xdp_statistics stats = {};\n\t\tbool extra_stats = true;\n\t\tsize_t stats_size;\n\n\t\tif (len < sizeof(struct xdp_statistics_v1)) {\n\t\t\treturn -EINVAL;\n\t\t} else if (len < sizeof(stats)) {\n\t\t\textra_stats = false;\n\t\t\tstats_size = sizeof(struct xdp_statistics_v1);\n\t\t} else {\n\t\t\tstats_size = sizeof(stats);\n\t\t}\n\n\t\tmutex_lock(&xs->mutex);\n\t\tstats.rx_dropped = xs->rx_dropped;\n\t\tif (extra_stats) {\n\t\t\tstats.rx_ring_full = xs->rx_queue_full;\n\t\t\tstats.rx_fill_ring_empty_descs =\n\t\t\t\txs->pool ? xskq_nb_queue_empty_descs(xs->pool->fq) : 0;\n\t\t\tstats.tx_ring_empty_descs = xskq_nb_queue_empty_descs(xs->tx);\n\t\t} else {\n\t\t\tstats.rx_dropped += xs->rx_queue_full;\n\t\t}\n\t\tstats.rx_invalid_descs = xskq_nb_invalid_descs(xs->rx);\n\t\tstats.tx_invalid_descs = xskq_nb_invalid_descs(xs->tx);\n\t\tmutex_unlock(&xs->mutex);\n\n\t\tif (copy_to_user(optval, &stats, stats_size))\n\t\t\treturn -EFAULT;\n\t\tif (put_user(stats_size, optlen))\n\t\t\treturn -EFAULT;\n\n\t\treturn 0;\n\t}\n\tcase XDP_MMAP_OFFSETS:\n\t{\n\t\tstruct xdp_mmap_offsets off;\n\t\tstruct xdp_mmap_offsets_v1 off_v1;\n\t\tbool flags_supported = true;\n\t\tvoid *to_copy;\n\n\t\tif (len < sizeof(off_v1))\n\t\t\treturn -EINVAL;\n\t\telse if (len < sizeof(off))\n\t\t\tflags_supported = false;\n\n\t\tif (flags_supported) {\n\t\t\t \n\t\t\txsk_enter_rxtx_offsets((struct xdp_ring_offset_v1 *)\n\t\t\t\t\t       &off.rx);\n\t\t\txsk_enter_rxtx_offsets((struct xdp_ring_offset_v1 *)\n\t\t\t\t\t       &off.tx);\n\t\t\txsk_enter_umem_offsets((struct xdp_ring_offset_v1 *)\n\t\t\t\t\t       &off.fr);\n\t\t\txsk_enter_umem_offsets((struct xdp_ring_offset_v1 *)\n\t\t\t\t\t       &off.cr);\n\t\t\toff.rx.flags = offsetof(struct xdp_rxtx_ring,\n\t\t\t\t\t\tptrs.flags);\n\t\t\toff.tx.flags = offsetof(struct xdp_rxtx_ring,\n\t\t\t\t\t\tptrs.flags);\n\t\t\toff.fr.flags = offsetof(struct xdp_umem_ring,\n\t\t\t\t\t\tptrs.flags);\n\t\t\toff.cr.flags = offsetof(struct xdp_umem_ring,\n\t\t\t\t\t\tptrs.flags);\n\n\t\t\tlen = sizeof(off);\n\t\t\tto_copy = &off;\n\t\t} else {\n\t\t\txsk_enter_rxtx_offsets(&off_v1.rx);\n\t\t\txsk_enter_rxtx_offsets(&off_v1.tx);\n\t\t\txsk_enter_umem_offsets(&off_v1.fr);\n\t\t\txsk_enter_umem_offsets(&off_v1.cr);\n\n\t\t\tlen = sizeof(off_v1);\n\t\t\tto_copy = &off_v1;\n\t\t}\n\n\t\tif (copy_to_user(optval, to_copy, len))\n\t\t\treturn -EFAULT;\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\n\t\treturn 0;\n\t}\n\tcase XDP_OPTIONS:\n\t{\n\t\tstruct xdp_options opts = {};\n\n\t\tif (len < sizeof(opts))\n\t\t\treturn -EINVAL;\n\n\t\tmutex_lock(&xs->mutex);\n\t\tif (xs->zc)\n\t\t\topts.flags |= XDP_OPTIONS_ZEROCOPY;\n\t\tmutex_unlock(&xs->mutex);\n\n\t\tlen = sizeof(opts);\n\t\tif (copy_to_user(optval, &opts, len))\n\t\t\treturn -EFAULT;\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\n\t\treturn 0;\n\t}\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn -EOPNOTSUPP;\n}\n\nstatic int xsk_mmap(struct file *file, struct socket *sock,\n\t\t    struct vm_area_struct *vma)\n{\n\tloff_t offset = (loff_t)vma->vm_pgoff << PAGE_SHIFT;\n\tunsigned long size = vma->vm_end - vma->vm_start;\n\tstruct xdp_sock *xs = xdp_sk(sock->sk);\n\tint state = READ_ONCE(xs->state);\n\tstruct xsk_queue *q = NULL;\n\n\tif (state != XSK_READY && state != XSK_BOUND)\n\t\treturn -EBUSY;\n\n\tif (offset == XDP_PGOFF_RX_RING) {\n\t\tq = READ_ONCE(xs->rx);\n\t} else if (offset == XDP_PGOFF_TX_RING) {\n\t\tq = READ_ONCE(xs->tx);\n\t} else {\n\t\t \n\t\tsmp_rmb();\n\t\tif (offset == XDP_UMEM_PGOFF_FILL_RING)\n\t\t\tq = state == XSK_READY ? READ_ONCE(xs->fq_tmp) :\n\t\t\t\t\t\t READ_ONCE(xs->pool->fq);\n\t\telse if (offset == XDP_UMEM_PGOFF_COMPLETION_RING)\n\t\t\tq = state == XSK_READY ? READ_ONCE(xs->cq_tmp) :\n\t\t\t\t\t\t READ_ONCE(xs->pool->cq);\n\t}\n\n\tif (!q)\n\t\treturn -EINVAL;\n\n\t \n\tsmp_rmb();\n\tif (size > q->ring_vmalloc_size)\n\t\treturn -EINVAL;\n\n\treturn remap_vmalloc_range(vma, q->ring, 0);\n}\n\nstatic int xsk_notifier(struct notifier_block *this,\n\t\t\tunsigned long msg, void *ptr)\n{\n\tstruct net_device *dev = netdev_notifier_info_to_dev(ptr);\n\tstruct net *net = dev_net(dev);\n\tstruct sock *sk;\n\n\tswitch (msg) {\n\tcase NETDEV_UNREGISTER:\n\t\tmutex_lock(&net->xdp.lock);\n\t\tsk_for_each(sk, &net->xdp.list) {\n\t\t\tstruct xdp_sock *xs = xdp_sk(sk);\n\n\t\t\tmutex_lock(&xs->mutex);\n\t\t\tif (xs->dev == dev) {\n\t\t\t\tsk->sk_err = ENETDOWN;\n\t\t\t\tif (!sock_flag(sk, SOCK_DEAD))\n\t\t\t\t\tsk_error_report(sk);\n\n\t\t\t\txsk_unbind_dev(xs);\n\n\t\t\t\t \n\t\t\t\txp_clear_dev(xs->pool);\n\t\t\t}\n\t\t\tmutex_unlock(&xs->mutex);\n\t\t}\n\t\tmutex_unlock(&net->xdp.lock);\n\t\tbreak;\n\t}\n\treturn NOTIFY_DONE;\n}\n\nstatic struct proto xsk_proto = {\n\t.name =\t\t\"XDP\",\n\t.owner =\tTHIS_MODULE,\n\t.obj_size =\tsizeof(struct xdp_sock),\n};\n\nstatic const struct proto_ops xsk_proto_ops = {\n\t.family\t\t= PF_XDP,\n\t.owner\t\t= THIS_MODULE,\n\t.release\t= xsk_release,\n\t.bind\t\t= xsk_bind,\n\t.connect\t= sock_no_connect,\n\t.socketpair\t= sock_no_socketpair,\n\t.accept\t\t= sock_no_accept,\n\t.getname\t= sock_no_getname,\n\t.poll\t\t= xsk_poll,\n\t.ioctl\t\t= sock_no_ioctl,\n\t.listen\t\t= sock_no_listen,\n\t.shutdown\t= sock_no_shutdown,\n\t.setsockopt\t= xsk_setsockopt,\n\t.getsockopt\t= xsk_getsockopt,\n\t.sendmsg\t= xsk_sendmsg,\n\t.recvmsg\t= xsk_recvmsg,\n\t.mmap\t\t= xsk_mmap,\n};\n\nstatic void xsk_destruct(struct sock *sk)\n{\n\tstruct xdp_sock *xs = xdp_sk(sk);\n\n\tif (!sock_flag(sk, SOCK_DEAD))\n\t\treturn;\n\n\tif (!xp_put_pool(xs->pool))\n\t\txdp_put_umem(xs->umem, !xs->pool);\n}\n\nstatic int xsk_create(struct net *net, struct socket *sock, int protocol,\n\t\t      int kern)\n{\n\tstruct xdp_sock *xs;\n\tstruct sock *sk;\n\n\tif (!ns_capable(net->user_ns, CAP_NET_RAW))\n\t\treturn -EPERM;\n\tif (sock->type != SOCK_RAW)\n\t\treturn -ESOCKTNOSUPPORT;\n\n\tif (protocol)\n\t\treturn -EPROTONOSUPPORT;\n\n\tsock->state = SS_UNCONNECTED;\n\n\tsk = sk_alloc(net, PF_XDP, GFP_KERNEL, &xsk_proto, kern);\n\tif (!sk)\n\t\treturn -ENOBUFS;\n\n\tsock->ops = &xsk_proto_ops;\n\n\tsock_init_data(sock, sk);\n\n\tsk->sk_family = PF_XDP;\n\n\tsk->sk_destruct = xsk_destruct;\n\n\tsock_set_flag(sk, SOCK_RCU_FREE);\n\n\txs = xdp_sk(sk);\n\txs->state = XSK_READY;\n\tmutex_init(&xs->mutex);\n\tspin_lock_init(&xs->rx_lock);\n\n\tINIT_LIST_HEAD(&xs->map_list);\n\tspin_lock_init(&xs->map_list_lock);\n\n\tmutex_lock(&net->xdp.lock);\n\tsk_add_node_rcu(sk, &net->xdp.list);\n\tmutex_unlock(&net->xdp.lock);\n\n\tsock_prot_inuse_add(net, &xsk_proto, 1);\n\n\treturn 0;\n}\n\nstatic const struct net_proto_family xsk_family_ops = {\n\t.family = PF_XDP,\n\t.create = xsk_create,\n\t.owner\t= THIS_MODULE,\n};\n\nstatic struct notifier_block xsk_netdev_notifier = {\n\t.notifier_call\t= xsk_notifier,\n};\n\nstatic int __net_init xsk_net_init(struct net *net)\n{\n\tmutex_init(&net->xdp.lock);\n\tINIT_HLIST_HEAD(&net->xdp.list);\n\treturn 0;\n}\n\nstatic void __net_exit xsk_net_exit(struct net *net)\n{\n\tWARN_ON_ONCE(!hlist_empty(&net->xdp.list));\n}\n\nstatic struct pernet_operations xsk_net_ops = {\n\t.init = xsk_net_init,\n\t.exit = xsk_net_exit,\n};\n\nstatic int __init xsk_init(void)\n{\n\tint err, cpu;\n\n\terr = proto_register(&xsk_proto, 0  );\n\tif (err)\n\t\tgoto out;\n\n\terr = sock_register(&xsk_family_ops);\n\tif (err)\n\t\tgoto out_proto;\n\n\terr = register_pernet_subsys(&xsk_net_ops);\n\tif (err)\n\t\tgoto out_sk;\n\n\terr = register_netdevice_notifier(&xsk_netdev_notifier);\n\tif (err)\n\t\tgoto out_pernet;\n\n\tfor_each_possible_cpu(cpu)\n\t\tINIT_LIST_HEAD(&per_cpu(xskmap_flush_list, cpu));\n\treturn 0;\n\nout_pernet:\n\tunregister_pernet_subsys(&xsk_net_ops);\nout_sk:\n\tsock_unregister(PF_XDP);\nout_proto:\n\tproto_unregister(&xsk_proto);\nout:\n\treturn err;\n}\n\nfs_initcall(xsk_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}