{
  "module_name": "xdp_umem.c",
  "hash_id": "8eab9b33ca01d9dcd8e6ef6727cf99792eafbfb1de5f97ac6d89a0b39707a505",
  "original_prompt": "Ingested from linux-6.6.14/net/xdp/xdp_umem.c",
  "human_readable_source": "\n \n\n#include <linux/init.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/task.h>\n#include <linux/uaccess.h>\n#include <linux/slab.h>\n#include <linux/bpf.h>\n#include <linux/mm.h>\n#include <linux/netdevice.h>\n#include <linux/rtnetlink.h>\n#include <linux/idr.h>\n#include <linux/vmalloc.h>\n\n#include \"xdp_umem.h\"\n#include \"xsk_queue.h\"\n\nstatic DEFINE_IDA(umem_ida);\n\nstatic void xdp_umem_unpin_pages(struct xdp_umem *umem)\n{\n\tunpin_user_pages_dirty_lock(umem->pgs, umem->npgs, true);\n\n\tkvfree(umem->pgs);\n\tumem->pgs = NULL;\n}\n\nstatic void xdp_umem_unaccount_pages(struct xdp_umem *umem)\n{\n\tif (umem->user) {\n\t\tatomic_long_sub(umem->npgs, &umem->user->locked_vm);\n\t\tfree_uid(umem->user);\n\t}\n}\n\nstatic void xdp_umem_addr_unmap(struct xdp_umem *umem)\n{\n\tvunmap(umem->addrs);\n\tumem->addrs = NULL;\n}\n\nstatic int xdp_umem_addr_map(struct xdp_umem *umem, struct page **pages,\n\t\t\t     u32 nr_pages)\n{\n\tumem->addrs = vmap(pages, nr_pages, VM_MAP, PAGE_KERNEL);\n\tif (!umem->addrs)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nstatic void xdp_umem_release(struct xdp_umem *umem)\n{\n\tumem->zc = false;\n\tida_free(&umem_ida, umem->id);\n\n\txdp_umem_addr_unmap(umem);\n\txdp_umem_unpin_pages(umem);\n\n\txdp_umem_unaccount_pages(umem);\n\tkfree(umem);\n}\n\nstatic void xdp_umem_release_deferred(struct work_struct *work)\n{\n\tstruct xdp_umem *umem = container_of(work, struct xdp_umem, work);\n\n\txdp_umem_release(umem);\n}\n\nvoid xdp_get_umem(struct xdp_umem *umem)\n{\n\trefcount_inc(&umem->users);\n}\n\nvoid xdp_put_umem(struct xdp_umem *umem, bool defer_cleanup)\n{\n\tif (!umem)\n\t\treturn;\n\n\tif (refcount_dec_and_test(&umem->users)) {\n\t\tif (defer_cleanup) {\n\t\t\tINIT_WORK(&umem->work, xdp_umem_release_deferred);\n\t\t\tschedule_work(&umem->work);\n\t\t} else {\n\t\t\txdp_umem_release(umem);\n\t\t}\n\t}\n}\n\nstatic int xdp_umem_pin_pages(struct xdp_umem *umem, unsigned long address)\n{\n\tunsigned int gup_flags = FOLL_WRITE;\n\tlong npgs;\n\tint err;\n\n\tumem->pgs = kvcalloc(umem->npgs, sizeof(*umem->pgs), GFP_KERNEL | __GFP_NOWARN);\n\tif (!umem->pgs)\n\t\treturn -ENOMEM;\n\n\tmmap_read_lock(current->mm);\n\tnpgs = pin_user_pages(address, umem->npgs,\n\t\t\t      gup_flags | FOLL_LONGTERM, &umem->pgs[0]);\n\tmmap_read_unlock(current->mm);\n\n\tif (npgs != umem->npgs) {\n\t\tif (npgs >= 0) {\n\t\t\tumem->npgs = npgs;\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out_pin;\n\t\t}\n\t\terr = npgs;\n\t\tgoto out_pgs;\n\t}\n\treturn 0;\n\nout_pin:\n\txdp_umem_unpin_pages(umem);\nout_pgs:\n\tkvfree(umem->pgs);\n\tumem->pgs = NULL;\n\treturn err;\n}\n\nstatic int xdp_umem_account_pages(struct xdp_umem *umem)\n{\n\tunsigned long lock_limit, new_npgs, old_npgs;\n\n\tif (capable(CAP_IPC_LOCK))\n\t\treturn 0;\n\n\tlock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;\n\tumem->user = get_uid(current_user());\n\n\tdo {\n\t\told_npgs = atomic_long_read(&umem->user->locked_vm);\n\t\tnew_npgs = old_npgs + umem->npgs;\n\t\tif (new_npgs > lock_limit) {\n\t\t\tfree_uid(umem->user);\n\t\t\tumem->user = NULL;\n\t\t\treturn -ENOBUFS;\n\t\t}\n\t} while (atomic_long_cmpxchg(&umem->user->locked_vm, old_npgs,\n\t\t\t\t     new_npgs) != old_npgs);\n\treturn 0;\n}\n\nstatic int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)\n{\n\tbool unaligned_chunks = mr->flags & XDP_UMEM_UNALIGNED_CHUNK_FLAG;\n\tu32 chunk_size = mr->chunk_size, headroom = mr->headroom;\n\tu64 addr = mr->addr, size = mr->len;\n\tu32 chunks_rem, npgs_rem;\n\tu64 chunks, npgs;\n\tint err;\n\n\tif (chunk_size < XDP_UMEM_MIN_CHUNK_SIZE || chunk_size > PAGE_SIZE) {\n\t\t \n\t\treturn -EINVAL;\n\t}\n\n\tif (mr->flags & ~XDP_UMEM_UNALIGNED_CHUNK_FLAG)\n\t\treturn -EINVAL;\n\n\tif (!unaligned_chunks && !is_power_of_2(chunk_size))\n\t\treturn -EINVAL;\n\n\tif (!PAGE_ALIGNED(addr)) {\n\t\t \n\t\treturn -EINVAL;\n\t}\n\n\tif ((addr + size) < addr)\n\t\treturn -EINVAL;\n\n\tnpgs = div_u64_rem(size, PAGE_SIZE, &npgs_rem);\n\tif (npgs_rem)\n\t\tnpgs++;\n\tif (npgs > U32_MAX)\n\t\treturn -EINVAL;\n\n\tchunks = div_u64_rem(size, chunk_size, &chunks_rem);\n\tif (!chunks || chunks > U32_MAX)\n\t\treturn -EINVAL;\n\n\tif (!unaligned_chunks && chunks_rem)\n\t\treturn -EINVAL;\n\n\tif (headroom >= chunk_size - XDP_PACKET_HEADROOM)\n\t\treturn -EINVAL;\n\n\tumem->size = size;\n\tumem->headroom = headroom;\n\tumem->chunk_size = chunk_size;\n\tumem->chunks = chunks;\n\tumem->npgs = npgs;\n\tumem->pgs = NULL;\n\tumem->user = NULL;\n\tumem->flags = mr->flags;\n\n\tINIT_LIST_HEAD(&umem->xsk_dma_list);\n\trefcount_set(&umem->users, 1);\n\n\terr = xdp_umem_account_pages(umem);\n\tif (err)\n\t\treturn err;\n\n\terr = xdp_umem_pin_pages(umem, (unsigned long)addr);\n\tif (err)\n\t\tgoto out_account;\n\n\terr = xdp_umem_addr_map(umem, umem->pgs, umem->npgs);\n\tif (err)\n\t\tgoto out_unpin;\n\n\treturn 0;\n\nout_unpin:\n\txdp_umem_unpin_pages(umem);\nout_account:\n\txdp_umem_unaccount_pages(umem);\n\treturn err;\n}\n\nstruct xdp_umem *xdp_umem_create(struct xdp_umem_reg *mr)\n{\n\tstruct xdp_umem *umem;\n\tint err;\n\n\tumem = kzalloc(sizeof(*umem), GFP_KERNEL);\n\tif (!umem)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\terr = ida_alloc(&umem_ida, GFP_KERNEL);\n\tif (err < 0) {\n\t\tkfree(umem);\n\t\treturn ERR_PTR(err);\n\t}\n\tumem->id = err;\n\n\terr = xdp_umem_reg(umem, mr);\n\tif (err) {\n\t\tida_free(&umem_ida, umem->id);\n\t\tkfree(umem);\n\t\treturn ERR_PTR(err);\n\t}\n\n\treturn umem;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}