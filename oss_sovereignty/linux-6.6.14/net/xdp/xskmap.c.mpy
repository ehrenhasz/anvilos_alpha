{
  "module_name": "xskmap.c",
  "hash_id": "691f611ab5825a51f9fe817f1079bf952528ce7fe9653a4ac7dfc7020538bd30",
  "original_prompt": "Ingested from linux-6.6.14/net/xdp/xskmap.c",
  "human_readable_source": "\n \n\n#include <linux/bpf.h>\n#include <linux/filter.h>\n#include <net/xdp_sock.h>\n#include <linux/slab.h>\n#include <linux/sched.h>\n#include <linux/btf_ids.h>\n\n#include \"xsk.h\"\n\nstatic struct xsk_map_node *xsk_map_node_alloc(struct xsk_map *map,\n\t\t\t\t\t       struct xdp_sock __rcu **map_entry)\n{\n\tstruct xsk_map_node *node;\n\n\tnode = bpf_map_kzalloc(&map->map, sizeof(*node),\n\t\t\t       GFP_ATOMIC | __GFP_NOWARN);\n\tif (!node)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tbpf_map_inc(&map->map);\n\tatomic_inc(&map->count);\n\n\tnode->map = map;\n\tnode->map_entry = map_entry;\n\treturn node;\n}\n\nstatic void xsk_map_node_free(struct xsk_map_node *node)\n{\n\tstruct xsk_map *map = node->map;\n\n\tbpf_map_put(&node->map->map);\n\tkfree(node);\n\tatomic_dec(&map->count);\n}\n\nstatic void xsk_map_sock_add(struct xdp_sock *xs, struct xsk_map_node *node)\n{\n\tspin_lock_bh(&xs->map_list_lock);\n\tlist_add_tail(&node->node, &xs->map_list);\n\tspin_unlock_bh(&xs->map_list_lock);\n}\n\nstatic void xsk_map_sock_delete(struct xdp_sock *xs,\n\t\t\t\tstruct xdp_sock __rcu **map_entry)\n{\n\tstruct xsk_map_node *n, *tmp;\n\n\tspin_lock_bh(&xs->map_list_lock);\n\tlist_for_each_entry_safe(n, tmp, &xs->map_list, node) {\n\t\tif (map_entry == n->map_entry) {\n\t\t\tlist_del(&n->node);\n\t\t\txsk_map_node_free(n);\n\t\t}\n\t}\n\tspin_unlock_bh(&xs->map_list_lock);\n}\n\nstatic struct bpf_map *xsk_map_alloc(union bpf_attr *attr)\n{\n\tstruct xsk_map *m;\n\tint numa_node;\n\tu64 size;\n\n\tif (attr->max_entries == 0 || attr->key_size != 4 ||\n\t    attr->value_size != 4 ||\n\t    attr->map_flags & ~(BPF_F_NUMA_NODE | BPF_F_RDONLY | BPF_F_WRONLY))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tnuma_node = bpf_map_attr_numa_node(attr);\n\tsize = struct_size(m, xsk_map, attr->max_entries);\n\n\tm = bpf_map_area_alloc(size, numa_node);\n\tif (!m)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tbpf_map_init_from_attr(&m->map, attr);\n\tspin_lock_init(&m->lock);\n\n\treturn &m->map;\n}\n\nstatic u64 xsk_map_mem_usage(const struct bpf_map *map)\n{\n\tstruct xsk_map *m = container_of(map, struct xsk_map, map);\n\n\treturn struct_size(m, xsk_map, map->max_entries) +\n\t\t   (u64)atomic_read(&m->count) * sizeof(struct xsk_map_node);\n}\n\nstatic void xsk_map_free(struct bpf_map *map)\n{\n\tstruct xsk_map *m = container_of(map, struct xsk_map, map);\n\n\tsynchronize_net();\n\tbpf_map_area_free(m);\n}\n\nstatic int xsk_map_get_next_key(struct bpf_map *map, void *key, void *next_key)\n{\n\tstruct xsk_map *m = container_of(map, struct xsk_map, map);\n\tu32 index = key ? *(u32 *)key : U32_MAX;\n\tu32 *next = next_key;\n\n\tif (index >= m->map.max_entries) {\n\t\t*next = 0;\n\t\treturn 0;\n\t}\n\n\tif (index == m->map.max_entries - 1)\n\t\treturn -ENOENT;\n\t*next = index + 1;\n\treturn 0;\n}\n\nstatic int xsk_map_gen_lookup(struct bpf_map *map, struct bpf_insn *insn_buf)\n{\n\tconst int ret = BPF_REG_0, mp = BPF_REG_1, index = BPF_REG_2;\n\tstruct bpf_insn *insn = insn_buf;\n\n\t*insn++ = BPF_LDX_MEM(BPF_W, ret, index, 0);\n\t*insn++ = BPF_JMP_IMM(BPF_JGE, ret, map->max_entries, 5);\n\t*insn++ = BPF_ALU64_IMM(BPF_LSH, ret, ilog2(sizeof(struct xsk_sock *)));\n\t*insn++ = BPF_ALU64_IMM(BPF_ADD, mp, offsetof(struct xsk_map, xsk_map));\n\t*insn++ = BPF_ALU64_REG(BPF_ADD, ret, mp);\n\t*insn++ = BPF_LDX_MEM(BPF_SIZEOF(struct xsk_sock *), ret, ret, 0);\n\t*insn++ = BPF_JMP_IMM(BPF_JA, 0, 0, 1);\n\t*insn++ = BPF_MOV64_IMM(ret, 0);\n\treturn insn - insn_buf;\n}\n\n \nstatic void *__xsk_map_lookup_elem(struct bpf_map *map, u32 key)\n{\n\tstruct xsk_map *m = container_of(map, struct xsk_map, map);\n\n\tif (key >= map->max_entries)\n\t\treturn NULL;\n\n\treturn rcu_dereference_check(m->xsk_map[key], rcu_read_lock_bh_held());\n}\n\nstatic void *xsk_map_lookup_elem(struct bpf_map *map, void *key)\n{\n\treturn __xsk_map_lookup_elem(map, *(u32 *)key);\n}\n\nstatic void *xsk_map_lookup_elem_sys_only(struct bpf_map *map, void *key)\n{\n\treturn ERR_PTR(-EOPNOTSUPP);\n}\n\nstatic long xsk_map_update_elem(struct bpf_map *map, void *key, void *value,\n\t\t\t\tu64 map_flags)\n{\n\tstruct xsk_map *m = container_of(map, struct xsk_map, map);\n\tstruct xdp_sock __rcu **map_entry;\n\tstruct xdp_sock *xs, *old_xs;\n\tu32 i = *(u32 *)key, fd = *(u32 *)value;\n\tstruct xsk_map_node *node;\n\tstruct socket *sock;\n\tint err;\n\n\tif (unlikely(map_flags > BPF_EXIST))\n\t\treturn -EINVAL;\n\tif (unlikely(i >= m->map.max_entries))\n\t\treturn -E2BIG;\n\n\tsock = sockfd_lookup(fd, &err);\n\tif (!sock)\n\t\treturn err;\n\n\tif (sock->sk->sk_family != PF_XDP) {\n\t\tsockfd_put(sock);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\txs = (struct xdp_sock *)sock->sk;\n\n\tmap_entry = &m->xsk_map[i];\n\tnode = xsk_map_node_alloc(m, map_entry);\n\tif (IS_ERR(node)) {\n\t\tsockfd_put(sock);\n\t\treturn PTR_ERR(node);\n\t}\n\n\tspin_lock_bh(&m->lock);\n\told_xs = rcu_dereference_protected(*map_entry, lockdep_is_held(&m->lock));\n\tif (old_xs == xs) {\n\t\terr = 0;\n\t\tgoto out;\n\t} else if (old_xs && map_flags == BPF_NOEXIST) {\n\t\terr = -EEXIST;\n\t\tgoto out;\n\t} else if (!old_xs && map_flags == BPF_EXIST) {\n\t\terr = -ENOENT;\n\t\tgoto out;\n\t}\n\txsk_map_sock_add(xs, node);\n\trcu_assign_pointer(*map_entry, xs);\n\tif (old_xs)\n\t\txsk_map_sock_delete(old_xs, map_entry);\n\tspin_unlock_bh(&m->lock);\n\tsockfd_put(sock);\n\treturn 0;\n\nout:\n\tspin_unlock_bh(&m->lock);\n\tsockfd_put(sock);\n\txsk_map_node_free(node);\n\treturn err;\n}\n\nstatic long xsk_map_delete_elem(struct bpf_map *map, void *key)\n{\n\tstruct xsk_map *m = container_of(map, struct xsk_map, map);\n\tstruct xdp_sock __rcu **map_entry;\n\tstruct xdp_sock *old_xs;\n\tint k = *(u32 *)key;\n\n\tif (k >= map->max_entries)\n\t\treturn -EINVAL;\n\n\tspin_lock_bh(&m->lock);\n\tmap_entry = &m->xsk_map[k];\n\told_xs = unrcu_pointer(xchg(map_entry, NULL));\n\tif (old_xs)\n\t\txsk_map_sock_delete(old_xs, map_entry);\n\tspin_unlock_bh(&m->lock);\n\n\treturn 0;\n}\n\nstatic long xsk_map_redirect(struct bpf_map *map, u64 index, u64 flags)\n{\n\treturn __bpf_xdp_redirect_map(map, index, flags, 0,\n\t\t\t\t      __xsk_map_lookup_elem);\n}\n\nvoid xsk_map_try_sock_delete(struct xsk_map *map, struct xdp_sock *xs,\n\t\t\t     struct xdp_sock __rcu **map_entry)\n{\n\tspin_lock_bh(&map->lock);\n\tif (rcu_access_pointer(*map_entry) == xs) {\n\t\trcu_assign_pointer(*map_entry, NULL);\n\t\txsk_map_sock_delete(xs, map_entry);\n\t}\n\tspin_unlock_bh(&map->lock);\n}\n\nstatic bool xsk_map_meta_equal(const struct bpf_map *meta0,\n\t\t\t       const struct bpf_map *meta1)\n{\n\treturn meta0->max_entries == meta1->max_entries &&\n\t\tbpf_map_meta_equal(meta0, meta1);\n}\n\nBTF_ID_LIST_SINGLE(xsk_map_btf_ids, struct, xsk_map)\nconst struct bpf_map_ops xsk_map_ops = {\n\t.map_meta_equal = xsk_map_meta_equal,\n\t.map_alloc = xsk_map_alloc,\n\t.map_free = xsk_map_free,\n\t.map_get_next_key = xsk_map_get_next_key,\n\t.map_lookup_elem = xsk_map_lookup_elem,\n\t.map_gen_lookup = xsk_map_gen_lookup,\n\t.map_lookup_elem_sys_only = xsk_map_lookup_elem_sys_only,\n\t.map_update_elem = xsk_map_update_elem,\n\t.map_delete_elem = xsk_map_delete_elem,\n\t.map_check_btf = map_check_no_btf,\n\t.map_mem_usage = xsk_map_mem_usage,\n\t.map_btf_id = &xsk_map_btf_ids[0],\n\t.map_redirect = xsk_map_redirect,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}