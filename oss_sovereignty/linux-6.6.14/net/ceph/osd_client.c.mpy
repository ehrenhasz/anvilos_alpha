{
  "module_name": "osd_client.c",
  "hash_id": "028dd7c6291a6bb86cd4f2ba0c2848817e56b4a587021677ec4b18cfb20bc885",
  "original_prompt": "Ingested from linux-6.6.14/net/ceph/osd_client.c",
  "human_readable_source": "\n\n#include <linux/ceph/ceph_debug.h>\n\n#include <linux/module.h>\n#include <linux/err.h>\n#include <linux/highmem.h>\n#include <linux/mm.h>\n#include <linux/pagemap.h>\n#include <linux/slab.h>\n#include <linux/uaccess.h>\n#ifdef CONFIG_BLOCK\n#include <linux/bio.h>\n#endif\n\n#include <linux/ceph/ceph_features.h>\n#include <linux/ceph/libceph.h>\n#include <linux/ceph/osd_client.h>\n#include <linux/ceph/messenger.h>\n#include <linux/ceph/decode.h>\n#include <linux/ceph/auth.h>\n#include <linux/ceph/pagelist.h>\n#include <linux/ceph/striper.h>\n\n#define OSD_OPREPLY_FRONT_LEN\t512\n\nstatic struct kmem_cache\t*ceph_osd_request_cache;\n\nstatic const struct ceph_connection_operations osd_con_ops;\n\n \n\nstatic void link_request(struct ceph_osd *osd, struct ceph_osd_request *req);\nstatic void unlink_request(struct ceph_osd *osd, struct ceph_osd_request *req);\nstatic void link_linger(struct ceph_osd *osd,\n\t\t\tstruct ceph_osd_linger_request *lreq);\nstatic void unlink_linger(struct ceph_osd *osd,\n\t\t\t  struct ceph_osd_linger_request *lreq);\nstatic void clear_backoffs(struct ceph_osd *osd);\n\n#if 1\nstatic inline bool rwsem_is_wrlocked(struct rw_semaphore *sem)\n{\n\tbool wrlocked = true;\n\n\tif (unlikely(down_read_trylock(sem))) {\n\t\twrlocked = false;\n\t\tup_read(sem);\n\t}\n\n\treturn wrlocked;\n}\nstatic inline void verify_osdc_locked(struct ceph_osd_client *osdc)\n{\n\tWARN_ON(!rwsem_is_locked(&osdc->lock));\n}\nstatic inline void verify_osdc_wrlocked(struct ceph_osd_client *osdc)\n{\n\tWARN_ON(!rwsem_is_wrlocked(&osdc->lock));\n}\nstatic inline void verify_osd_locked(struct ceph_osd *osd)\n{\n\tstruct ceph_osd_client *osdc = osd->o_osdc;\n\n\tWARN_ON(!(mutex_is_locked(&osd->lock) &&\n\t\t  rwsem_is_locked(&osdc->lock)) &&\n\t\t!rwsem_is_wrlocked(&osdc->lock));\n}\nstatic inline void verify_lreq_locked(struct ceph_osd_linger_request *lreq)\n{\n\tWARN_ON(!mutex_is_locked(&lreq->lock));\n}\n#else\nstatic inline void verify_osdc_locked(struct ceph_osd_client *osdc) { }\nstatic inline void verify_osdc_wrlocked(struct ceph_osd_client *osdc) { }\nstatic inline void verify_osd_locked(struct ceph_osd *osd) { }\nstatic inline void verify_lreq_locked(struct ceph_osd_linger_request *lreq) { }\n#endif\n\n \nstatic int calc_layout(struct ceph_file_layout *layout, u64 off, u64 *plen,\n\t\t\tu64 *objnum, u64 *objoff, u64 *objlen)\n{\n\tu64 orig_len = *plen;\n\tu32 xlen;\n\n\t \n\tceph_calc_file_object_mapping(layout, off, orig_len, objnum,\n\t\t\t\t\t  objoff, &xlen);\n\t*objlen = xlen;\n\tif (*objlen < orig_len) {\n\t\t*plen = *objlen;\n\t\tdout(\" skipping last %llu, final file extent %llu~%llu\\n\",\n\t\t     orig_len - *plen, off, *plen);\n\t}\n\n\tdout(\"calc_layout objnum=%llx %llu~%llu\\n\", *objnum, *objoff, *objlen);\n\treturn 0;\n}\n\nstatic void ceph_osd_data_init(struct ceph_osd_data *osd_data)\n{\n\tmemset(osd_data, 0, sizeof (*osd_data));\n\tosd_data->type = CEPH_OSD_DATA_TYPE_NONE;\n}\n\n \nstatic void ceph_osd_data_pages_init(struct ceph_osd_data *osd_data,\n\t\t\tstruct page **pages, u64 length, u32 alignment,\n\t\t\tbool pages_from_pool, bool own_pages)\n{\n\tosd_data->type = CEPH_OSD_DATA_TYPE_PAGES;\n\tosd_data->pages = pages;\n\tosd_data->length = length;\n\tosd_data->alignment = alignment;\n\tosd_data->pages_from_pool = pages_from_pool;\n\tosd_data->own_pages = own_pages;\n}\n\n \nstatic void ceph_osd_data_pagelist_init(struct ceph_osd_data *osd_data,\n\t\t\tstruct ceph_pagelist *pagelist)\n{\n\tosd_data->type = CEPH_OSD_DATA_TYPE_PAGELIST;\n\tosd_data->pagelist = pagelist;\n}\n\n#ifdef CONFIG_BLOCK\nstatic void ceph_osd_data_bio_init(struct ceph_osd_data *osd_data,\n\t\t\t\t   struct ceph_bio_iter *bio_pos,\n\t\t\t\t   u32 bio_length)\n{\n\tosd_data->type = CEPH_OSD_DATA_TYPE_BIO;\n\tosd_data->bio_pos = *bio_pos;\n\tosd_data->bio_length = bio_length;\n}\n#endif  \n\nstatic void ceph_osd_data_bvecs_init(struct ceph_osd_data *osd_data,\n\t\t\t\t     struct ceph_bvec_iter *bvec_pos,\n\t\t\t\t     u32 num_bvecs)\n{\n\tosd_data->type = CEPH_OSD_DATA_TYPE_BVECS;\n\tosd_data->bvec_pos = *bvec_pos;\n\tosd_data->num_bvecs = num_bvecs;\n}\n\nstatic void ceph_osd_iter_init(struct ceph_osd_data *osd_data,\n\t\t\t       struct iov_iter *iter)\n{\n\tosd_data->type = CEPH_OSD_DATA_TYPE_ITER;\n\tosd_data->iter = *iter;\n}\n\nstatic struct ceph_osd_data *\nosd_req_op_raw_data_in(struct ceph_osd_request *osd_req, unsigned int which)\n{\n\tBUG_ON(which >= osd_req->r_num_ops);\n\n\treturn &osd_req->r_ops[which].raw_data_in;\n}\n\nstruct ceph_osd_data *\nosd_req_op_extent_osd_data(struct ceph_osd_request *osd_req,\n\t\t\tunsigned int which)\n{\n\treturn osd_req_op_data(osd_req, which, extent, osd_data);\n}\nEXPORT_SYMBOL(osd_req_op_extent_osd_data);\n\nvoid osd_req_op_raw_data_in_pages(struct ceph_osd_request *osd_req,\n\t\t\tunsigned int which, struct page **pages,\n\t\t\tu64 length, u32 alignment,\n\t\t\tbool pages_from_pool, bool own_pages)\n{\n\tstruct ceph_osd_data *osd_data;\n\n\tosd_data = osd_req_op_raw_data_in(osd_req, which);\n\tceph_osd_data_pages_init(osd_data, pages, length, alignment,\n\t\t\t\tpages_from_pool, own_pages);\n}\nEXPORT_SYMBOL(osd_req_op_raw_data_in_pages);\n\nvoid osd_req_op_extent_osd_data_pages(struct ceph_osd_request *osd_req,\n\t\t\tunsigned int which, struct page **pages,\n\t\t\tu64 length, u32 alignment,\n\t\t\tbool pages_from_pool, bool own_pages)\n{\n\tstruct ceph_osd_data *osd_data;\n\n\tosd_data = osd_req_op_data(osd_req, which, extent, osd_data);\n\tceph_osd_data_pages_init(osd_data, pages, length, alignment,\n\t\t\t\tpages_from_pool, own_pages);\n}\nEXPORT_SYMBOL(osd_req_op_extent_osd_data_pages);\n\nvoid osd_req_op_extent_osd_data_pagelist(struct ceph_osd_request *osd_req,\n\t\t\tunsigned int which, struct ceph_pagelist *pagelist)\n{\n\tstruct ceph_osd_data *osd_data;\n\n\tosd_data = osd_req_op_data(osd_req, which, extent, osd_data);\n\tceph_osd_data_pagelist_init(osd_data, pagelist);\n}\nEXPORT_SYMBOL(osd_req_op_extent_osd_data_pagelist);\n\n#ifdef CONFIG_BLOCK\nvoid osd_req_op_extent_osd_data_bio(struct ceph_osd_request *osd_req,\n\t\t\t\t    unsigned int which,\n\t\t\t\t    struct ceph_bio_iter *bio_pos,\n\t\t\t\t    u32 bio_length)\n{\n\tstruct ceph_osd_data *osd_data;\n\n\tosd_data = osd_req_op_data(osd_req, which, extent, osd_data);\n\tceph_osd_data_bio_init(osd_data, bio_pos, bio_length);\n}\nEXPORT_SYMBOL(osd_req_op_extent_osd_data_bio);\n#endif  \n\nvoid osd_req_op_extent_osd_data_bvecs(struct ceph_osd_request *osd_req,\n\t\t\t\t      unsigned int which,\n\t\t\t\t      struct bio_vec *bvecs, u32 num_bvecs,\n\t\t\t\t      u32 bytes)\n{\n\tstruct ceph_osd_data *osd_data;\n\tstruct ceph_bvec_iter it = {\n\t\t.bvecs = bvecs,\n\t\t.iter = { .bi_size = bytes },\n\t};\n\n\tosd_data = osd_req_op_data(osd_req, which, extent, osd_data);\n\tceph_osd_data_bvecs_init(osd_data, &it, num_bvecs);\n}\nEXPORT_SYMBOL(osd_req_op_extent_osd_data_bvecs);\n\nvoid osd_req_op_extent_osd_data_bvec_pos(struct ceph_osd_request *osd_req,\n\t\t\t\t\t unsigned int which,\n\t\t\t\t\t struct ceph_bvec_iter *bvec_pos)\n{\n\tstruct ceph_osd_data *osd_data;\n\n\tosd_data = osd_req_op_data(osd_req, which, extent, osd_data);\n\tceph_osd_data_bvecs_init(osd_data, bvec_pos, 0);\n}\nEXPORT_SYMBOL(osd_req_op_extent_osd_data_bvec_pos);\n\n \nvoid osd_req_op_extent_osd_iter(struct ceph_osd_request *osd_req,\n\t\t\t\tunsigned int which, struct iov_iter *iter)\n{\n\tstruct ceph_osd_data *osd_data;\n\n\tosd_data = osd_req_op_data(osd_req, which, extent, osd_data);\n\tceph_osd_iter_init(osd_data, iter);\n}\nEXPORT_SYMBOL(osd_req_op_extent_osd_iter);\n\nstatic void osd_req_op_cls_request_info_pagelist(\n\t\t\tstruct ceph_osd_request *osd_req,\n\t\t\tunsigned int which, struct ceph_pagelist *pagelist)\n{\n\tstruct ceph_osd_data *osd_data;\n\n\tosd_data = osd_req_op_data(osd_req, which, cls, request_info);\n\tceph_osd_data_pagelist_init(osd_data, pagelist);\n}\n\nvoid osd_req_op_cls_request_data_pagelist(\n\t\t\tstruct ceph_osd_request *osd_req,\n\t\t\tunsigned int which, struct ceph_pagelist *pagelist)\n{\n\tstruct ceph_osd_data *osd_data;\n\n\tosd_data = osd_req_op_data(osd_req, which, cls, request_data);\n\tceph_osd_data_pagelist_init(osd_data, pagelist);\n\tosd_req->r_ops[which].cls.indata_len += pagelist->length;\n\tosd_req->r_ops[which].indata_len += pagelist->length;\n}\nEXPORT_SYMBOL(osd_req_op_cls_request_data_pagelist);\n\nvoid osd_req_op_cls_request_data_pages(struct ceph_osd_request *osd_req,\n\t\t\tunsigned int which, struct page **pages, u64 length,\n\t\t\tu32 alignment, bool pages_from_pool, bool own_pages)\n{\n\tstruct ceph_osd_data *osd_data;\n\n\tosd_data = osd_req_op_data(osd_req, which, cls, request_data);\n\tceph_osd_data_pages_init(osd_data, pages, length, alignment,\n\t\t\t\tpages_from_pool, own_pages);\n\tosd_req->r_ops[which].cls.indata_len += length;\n\tosd_req->r_ops[which].indata_len += length;\n}\nEXPORT_SYMBOL(osd_req_op_cls_request_data_pages);\n\nvoid osd_req_op_cls_request_data_bvecs(struct ceph_osd_request *osd_req,\n\t\t\t\t       unsigned int which,\n\t\t\t\t       struct bio_vec *bvecs, u32 num_bvecs,\n\t\t\t\t       u32 bytes)\n{\n\tstruct ceph_osd_data *osd_data;\n\tstruct ceph_bvec_iter it = {\n\t\t.bvecs = bvecs,\n\t\t.iter = { .bi_size = bytes },\n\t};\n\n\tosd_data = osd_req_op_data(osd_req, which, cls, request_data);\n\tceph_osd_data_bvecs_init(osd_data, &it, num_bvecs);\n\tosd_req->r_ops[which].cls.indata_len += bytes;\n\tosd_req->r_ops[which].indata_len += bytes;\n}\nEXPORT_SYMBOL(osd_req_op_cls_request_data_bvecs);\n\nvoid osd_req_op_cls_response_data_pages(struct ceph_osd_request *osd_req,\n\t\t\tunsigned int which, struct page **pages, u64 length,\n\t\t\tu32 alignment, bool pages_from_pool, bool own_pages)\n{\n\tstruct ceph_osd_data *osd_data;\n\n\tosd_data = osd_req_op_data(osd_req, which, cls, response_data);\n\tceph_osd_data_pages_init(osd_data, pages, length, alignment,\n\t\t\t\tpages_from_pool, own_pages);\n}\nEXPORT_SYMBOL(osd_req_op_cls_response_data_pages);\n\nstatic u64 ceph_osd_data_length(struct ceph_osd_data *osd_data)\n{\n\tswitch (osd_data->type) {\n\tcase CEPH_OSD_DATA_TYPE_NONE:\n\t\treturn 0;\n\tcase CEPH_OSD_DATA_TYPE_PAGES:\n\t\treturn osd_data->length;\n\tcase CEPH_OSD_DATA_TYPE_PAGELIST:\n\t\treturn (u64)osd_data->pagelist->length;\n#ifdef CONFIG_BLOCK\n\tcase CEPH_OSD_DATA_TYPE_BIO:\n\t\treturn (u64)osd_data->bio_length;\n#endif  \n\tcase CEPH_OSD_DATA_TYPE_BVECS:\n\t\treturn osd_data->bvec_pos.iter.bi_size;\n\tcase CEPH_OSD_DATA_TYPE_ITER:\n\t\treturn iov_iter_count(&osd_data->iter);\n\tdefault:\n\t\tWARN(true, \"unrecognized data type %d\\n\", (int)osd_data->type);\n\t\treturn 0;\n\t}\n}\n\nstatic void ceph_osd_data_release(struct ceph_osd_data *osd_data)\n{\n\tif (osd_data->type == CEPH_OSD_DATA_TYPE_PAGES && osd_data->own_pages) {\n\t\tint num_pages;\n\n\t\tnum_pages = calc_pages_for((u64)osd_data->alignment,\n\t\t\t\t\t\t(u64)osd_data->length);\n\t\tceph_release_page_vector(osd_data->pages, num_pages);\n\t} else if (osd_data->type == CEPH_OSD_DATA_TYPE_PAGELIST) {\n\t\tceph_pagelist_release(osd_data->pagelist);\n\t}\n\tceph_osd_data_init(osd_data);\n}\n\nstatic void osd_req_op_data_release(struct ceph_osd_request *osd_req,\n\t\t\tunsigned int which)\n{\n\tstruct ceph_osd_req_op *op;\n\n\tBUG_ON(which >= osd_req->r_num_ops);\n\top = &osd_req->r_ops[which];\n\n\tswitch (op->op) {\n\tcase CEPH_OSD_OP_READ:\n\tcase CEPH_OSD_OP_SPARSE_READ:\n\tcase CEPH_OSD_OP_WRITE:\n\tcase CEPH_OSD_OP_WRITEFULL:\n\t\tkfree(op->extent.sparse_ext);\n\t\tceph_osd_data_release(&op->extent.osd_data);\n\t\tbreak;\n\tcase CEPH_OSD_OP_CALL:\n\t\tceph_osd_data_release(&op->cls.request_info);\n\t\tceph_osd_data_release(&op->cls.request_data);\n\t\tceph_osd_data_release(&op->cls.response_data);\n\t\tbreak;\n\tcase CEPH_OSD_OP_SETXATTR:\n\tcase CEPH_OSD_OP_CMPXATTR:\n\t\tceph_osd_data_release(&op->xattr.osd_data);\n\t\tbreak;\n\tcase CEPH_OSD_OP_STAT:\n\t\tceph_osd_data_release(&op->raw_data_in);\n\t\tbreak;\n\tcase CEPH_OSD_OP_NOTIFY_ACK:\n\t\tceph_osd_data_release(&op->notify_ack.request_data);\n\t\tbreak;\n\tcase CEPH_OSD_OP_NOTIFY:\n\t\tceph_osd_data_release(&op->notify.request_data);\n\t\tceph_osd_data_release(&op->notify.response_data);\n\t\tbreak;\n\tcase CEPH_OSD_OP_LIST_WATCHERS:\n\t\tceph_osd_data_release(&op->list_watchers.response_data);\n\t\tbreak;\n\tcase CEPH_OSD_OP_COPY_FROM2:\n\t\tceph_osd_data_release(&op->copy_from.osd_data);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\n \nstatic void target_init(struct ceph_osd_request_target *t)\n{\n\tceph_oid_init(&t->base_oid);\n\tceph_oloc_init(&t->base_oloc);\n\tceph_oid_init(&t->target_oid);\n\tceph_oloc_init(&t->target_oloc);\n\n\tceph_osds_init(&t->acting);\n\tceph_osds_init(&t->up);\n\tt->size = -1;\n\tt->min_size = -1;\n\n\tt->osd = CEPH_HOMELESS_OSD;\n}\n\nstatic void target_copy(struct ceph_osd_request_target *dest,\n\t\t\tconst struct ceph_osd_request_target *src)\n{\n\tceph_oid_copy(&dest->base_oid, &src->base_oid);\n\tceph_oloc_copy(&dest->base_oloc, &src->base_oloc);\n\tceph_oid_copy(&dest->target_oid, &src->target_oid);\n\tceph_oloc_copy(&dest->target_oloc, &src->target_oloc);\n\n\tdest->pgid = src->pgid;  \n\tdest->spgid = src->spgid;  \n\tdest->pg_num = src->pg_num;\n\tdest->pg_num_mask = src->pg_num_mask;\n\tceph_osds_copy(&dest->acting, &src->acting);\n\tceph_osds_copy(&dest->up, &src->up);\n\tdest->size = src->size;\n\tdest->min_size = src->min_size;\n\tdest->sort_bitwise = src->sort_bitwise;\n\tdest->recovery_deletes = src->recovery_deletes;\n\n\tdest->flags = src->flags;\n\tdest->used_replica = src->used_replica;\n\tdest->paused = src->paused;\n\n\tdest->epoch = src->epoch;\n\tdest->last_force_resend = src->last_force_resend;\n\n\tdest->osd = src->osd;\n}\n\nstatic void target_destroy(struct ceph_osd_request_target *t)\n{\n\tceph_oid_destroy(&t->base_oid);\n\tceph_oloc_destroy(&t->base_oloc);\n\tceph_oid_destroy(&t->target_oid);\n\tceph_oloc_destroy(&t->target_oloc);\n}\n\n \nstatic void request_release_checks(struct ceph_osd_request *req)\n{\n\tWARN_ON(!RB_EMPTY_NODE(&req->r_node));\n\tWARN_ON(!RB_EMPTY_NODE(&req->r_mc_node));\n\tWARN_ON(!list_empty(&req->r_private_item));\n\tWARN_ON(req->r_osd);\n}\n\nstatic void ceph_osdc_release_request(struct kref *kref)\n{\n\tstruct ceph_osd_request *req = container_of(kref,\n\t\t\t\t\t    struct ceph_osd_request, r_kref);\n\tunsigned int which;\n\n\tdout(\"%s %p (r_request %p r_reply %p)\\n\", __func__, req,\n\t     req->r_request, req->r_reply);\n\trequest_release_checks(req);\n\n\tif (req->r_request)\n\t\tceph_msg_put(req->r_request);\n\tif (req->r_reply)\n\t\tceph_msg_put(req->r_reply);\n\n\tfor (which = 0; which < req->r_num_ops; which++)\n\t\tosd_req_op_data_release(req, which);\n\n\ttarget_destroy(&req->r_t);\n\tceph_put_snap_context(req->r_snapc);\n\n\tif (req->r_mempool)\n\t\tmempool_free(req, req->r_osdc->req_mempool);\n\telse if (req->r_num_ops <= CEPH_OSD_SLAB_OPS)\n\t\tkmem_cache_free(ceph_osd_request_cache, req);\n\telse\n\t\tkfree(req);\n}\n\nvoid ceph_osdc_get_request(struct ceph_osd_request *req)\n{\n\tdout(\"%s %p (was %d)\\n\", __func__, req,\n\t     kref_read(&req->r_kref));\n\tkref_get(&req->r_kref);\n}\nEXPORT_SYMBOL(ceph_osdc_get_request);\n\nvoid ceph_osdc_put_request(struct ceph_osd_request *req)\n{\n\tif (req) {\n\t\tdout(\"%s %p (was %d)\\n\", __func__, req,\n\t\t     kref_read(&req->r_kref));\n\t\tkref_put(&req->r_kref, ceph_osdc_release_request);\n\t}\n}\nEXPORT_SYMBOL(ceph_osdc_put_request);\n\nstatic void request_init(struct ceph_osd_request *req)\n{\n\t \n\tmemset(req, 0, sizeof(*req));\n\n\tkref_init(&req->r_kref);\n\tinit_completion(&req->r_completion);\n\tRB_CLEAR_NODE(&req->r_node);\n\tRB_CLEAR_NODE(&req->r_mc_node);\n\tINIT_LIST_HEAD(&req->r_private_item);\n\n\ttarget_init(&req->r_t);\n}\n\nstruct ceph_osd_request *ceph_osdc_alloc_request(struct ceph_osd_client *osdc,\n\t\t\t\t\t       struct ceph_snap_context *snapc,\n\t\t\t\t\t       unsigned int num_ops,\n\t\t\t\t\t       bool use_mempool,\n\t\t\t\t\t       gfp_t gfp_flags)\n{\n\tstruct ceph_osd_request *req;\n\n\tif (use_mempool) {\n\t\tBUG_ON(num_ops > CEPH_OSD_SLAB_OPS);\n\t\treq = mempool_alloc(osdc->req_mempool, gfp_flags);\n\t} else if (num_ops <= CEPH_OSD_SLAB_OPS) {\n\t\treq = kmem_cache_alloc(ceph_osd_request_cache, gfp_flags);\n\t} else {\n\t\tBUG_ON(num_ops > CEPH_OSD_MAX_OPS);\n\t\treq = kmalloc(struct_size(req, r_ops, num_ops), gfp_flags);\n\t}\n\tif (unlikely(!req))\n\t\treturn NULL;\n\n\trequest_init(req);\n\treq->r_osdc = osdc;\n\treq->r_mempool = use_mempool;\n\treq->r_num_ops = num_ops;\n\treq->r_snapid = CEPH_NOSNAP;\n\treq->r_snapc = ceph_get_snap_context(snapc);\n\n\tdout(\"%s req %p\\n\", __func__, req);\n\treturn req;\n}\nEXPORT_SYMBOL(ceph_osdc_alloc_request);\n\nstatic int ceph_oloc_encoding_size(const struct ceph_object_locator *oloc)\n{\n\treturn 8 + 4 + 4 + 4 + (oloc->pool_ns ? oloc->pool_ns->len : 0);\n}\n\nstatic int __ceph_osdc_alloc_messages(struct ceph_osd_request *req, gfp_t gfp,\n\t\t\t\t      int num_request_data_items,\n\t\t\t\t      int num_reply_data_items)\n{\n\tstruct ceph_osd_client *osdc = req->r_osdc;\n\tstruct ceph_msg *msg;\n\tint msg_size;\n\n\tWARN_ON(req->r_request || req->r_reply);\n\tWARN_ON(ceph_oid_empty(&req->r_base_oid));\n\tWARN_ON(ceph_oloc_empty(&req->r_base_oloc));\n\n\t \n\tmsg_size = CEPH_ENCODING_START_BLK_LEN +\n\t\t\tCEPH_PGID_ENCODING_LEN + 1;  \n\tmsg_size += 4 + 4 + 4;  \n\tmsg_size += CEPH_ENCODING_START_BLK_LEN +\n\t\t\tsizeof(struct ceph_osd_reqid);  \n\tmsg_size += sizeof(struct ceph_blkin_trace_info);  \n\tmsg_size += 4 + sizeof(struct ceph_timespec);  \n\tmsg_size += CEPH_ENCODING_START_BLK_LEN +\n\t\t\tceph_oloc_encoding_size(&req->r_base_oloc);  \n\tmsg_size += 4 + req->r_base_oid.name_len;  \n\tmsg_size += 2 + req->r_num_ops * sizeof(struct ceph_osd_op);\n\tmsg_size += 8;  \n\tmsg_size += 8;  \n\tmsg_size += 4 + 8 * (req->r_snapc ? req->r_snapc->num_snaps : 0);\n\tmsg_size += 4 + 8;  \n\n\tif (req->r_mempool)\n\t\tmsg = ceph_msgpool_get(&osdc->msgpool_op, msg_size,\n\t\t\t\t       num_request_data_items);\n\telse\n\t\tmsg = ceph_msg_new2(CEPH_MSG_OSD_OP, msg_size,\n\t\t\t\t    num_request_data_items, gfp, true);\n\tif (!msg)\n\t\treturn -ENOMEM;\n\n\tmemset(msg->front.iov_base, 0, msg->front.iov_len);\n\treq->r_request = msg;\n\n\t \n\tmsg_size = OSD_OPREPLY_FRONT_LEN;\n\tmsg_size += req->r_base_oid.name_len;\n\tmsg_size += req->r_num_ops * sizeof(struct ceph_osd_op);\n\n\tif (req->r_mempool)\n\t\tmsg = ceph_msgpool_get(&osdc->msgpool_op_reply, msg_size,\n\t\t\t\t       num_reply_data_items);\n\telse\n\t\tmsg = ceph_msg_new2(CEPH_MSG_OSD_OPREPLY, msg_size,\n\t\t\t\t    num_reply_data_items, gfp, true);\n\tif (!msg)\n\t\treturn -ENOMEM;\n\n\treq->r_reply = msg;\n\n\treturn 0;\n}\n\nstatic bool osd_req_opcode_valid(u16 opcode)\n{\n\tswitch (opcode) {\n#define GENERATE_CASE(op, opcode, str)\tcase CEPH_OSD_OP_##op: return true;\n__CEPH_FORALL_OSD_OPS(GENERATE_CASE)\n#undef GENERATE_CASE\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic void get_num_data_items(struct ceph_osd_request *req,\n\t\t\t       int *num_request_data_items,\n\t\t\t       int *num_reply_data_items)\n{\n\tstruct ceph_osd_req_op *op;\n\n\t*num_request_data_items = 0;\n\t*num_reply_data_items = 0;\n\n\tfor (op = req->r_ops; op != &req->r_ops[req->r_num_ops]; op++) {\n\t\tswitch (op->op) {\n\t\t \n\t\tcase CEPH_OSD_OP_WRITE:\n\t\tcase CEPH_OSD_OP_WRITEFULL:\n\t\tcase CEPH_OSD_OP_SETXATTR:\n\t\tcase CEPH_OSD_OP_CMPXATTR:\n\t\tcase CEPH_OSD_OP_NOTIFY_ACK:\n\t\tcase CEPH_OSD_OP_COPY_FROM2:\n\t\t\t*num_request_data_items += 1;\n\t\t\tbreak;\n\n\t\t \n\t\tcase CEPH_OSD_OP_STAT:\n\t\tcase CEPH_OSD_OP_READ:\n\t\tcase CEPH_OSD_OP_SPARSE_READ:\n\t\tcase CEPH_OSD_OP_LIST_WATCHERS:\n\t\t\t*num_reply_data_items += 1;\n\t\t\tbreak;\n\n\t\t \n\t\tcase CEPH_OSD_OP_NOTIFY:\n\t\t\t*num_request_data_items += 1;\n\t\t\t*num_reply_data_items += 1;\n\t\t\tbreak;\n\t\tcase CEPH_OSD_OP_CALL:\n\t\t\t*num_request_data_items += 2;\n\t\t\t*num_reply_data_items += 1;\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tWARN_ON(!osd_req_opcode_valid(op->op));\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\n \nint ceph_osdc_alloc_messages(struct ceph_osd_request *req, gfp_t gfp)\n{\n\tint num_request_data_items, num_reply_data_items;\n\n\tget_num_data_items(req, &num_request_data_items, &num_reply_data_items);\n\treturn __ceph_osdc_alloc_messages(req, gfp, num_request_data_items,\n\t\t\t\t\t  num_reply_data_items);\n}\nEXPORT_SYMBOL(ceph_osdc_alloc_messages);\n\n \nstruct ceph_osd_req_op *\nosd_req_op_init(struct ceph_osd_request *osd_req, unsigned int which,\n\t\t u16 opcode, u32 flags)\n{\n\tstruct ceph_osd_req_op *op;\n\n\tBUG_ON(which >= osd_req->r_num_ops);\n\tBUG_ON(!osd_req_opcode_valid(opcode));\n\n\top = &osd_req->r_ops[which];\n\tmemset(op, 0, sizeof (*op));\n\top->op = opcode;\n\top->flags = flags;\n\n\treturn op;\n}\nEXPORT_SYMBOL(osd_req_op_init);\n\nvoid osd_req_op_extent_init(struct ceph_osd_request *osd_req,\n\t\t\t\tunsigned int which, u16 opcode,\n\t\t\t\tu64 offset, u64 length,\n\t\t\t\tu64 truncate_size, u32 truncate_seq)\n{\n\tstruct ceph_osd_req_op *op = osd_req_op_init(osd_req, which,\n\t\t\t\t\t\t     opcode, 0);\n\tsize_t payload_len = 0;\n\n\tBUG_ON(opcode != CEPH_OSD_OP_READ && opcode != CEPH_OSD_OP_WRITE &&\n\t       opcode != CEPH_OSD_OP_WRITEFULL && opcode != CEPH_OSD_OP_ZERO &&\n\t       opcode != CEPH_OSD_OP_TRUNCATE && opcode != CEPH_OSD_OP_SPARSE_READ);\n\n\top->extent.offset = offset;\n\top->extent.length = length;\n\top->extent.truncate_size = truncate_size;\n\top->extent.truncate_seq = truncate_seq;\n\tif (opcode == CEPH_OSD_OP_WRITE || opcode == CEPH_OSD_OP_WRITEFULL)\n\t\tpayload_len += length;\n\n\top->indata_len = payload_len;\n}\nEXPORT_SYMBOL(osd_req_op_extent_init);\n\nvoid osd_req_op_extent_update(struct ceph_osd_request *osd_req,\n\t\t\t\tunsigned int which, u64 length)\n{\n\tstruct ceph_osd_req_op *op;\n\tu64 previous;\n\n\tBUG_ON(which >= osd_req->r_num_ops);\n\top = &osd_req->r_ops[which];\n\tprevious = op->extent.length;\n\n\tif (length == previous)\n\t\treturn;\t\t \n\tBUG_ON(length > previous);\n\n\top->extent.length = length;\n\tif (op->op == CEPH_OSD_OP_WRITE || op->op == CEPH_OSD_OP_WRITEFULL)\n\t\top->indata_len -= previous - length;\n}\nEXPORT_SYMBOL(osd_req_op_extent_update);\n\nvoid osd_req_op_extent_dup_last(struct ceph_osd_request *osd_req,\n\t\t\t\tunsigned int which, u64 offset_inc)\n{\n\tstruct ceph_osd_req_op *op, *prev_op;\n\n\tBUG_ON(which + 1 >= osd_req->r_num_ops);\n\n\tprev_op = &osd_req->r_ops[which];\n\top = osd_req_op_init(osd_req, which + 1, prev_op->op, prev_op->flags);\n\t \n\top->indata_len = prev_op->indata_len;\n\top->outdata_len = prev_op->outdata_len;\n\top->extent = prev_op->extent;\n\t \n\top->extent.offset += offset_inc;\n\top->extent.length -= offset_inc;\n\n\tif (op->op == CEPH_OSD_OP_WRITE || op->op == CEPH_OSD_OP_WRITEFULL)\n\t\top->indata_len -= offset_inc;\n}\nEXPORT_SYMBOL(osd_req_op_extent_dup_last);\n\nint osd_req_op_cls_init(struct ceph_osd_request *osd_req, unsigned int which,\n\t\t\tconst char *class, const char *method)\n{\n\tstruct ceph_osd_req_op *op;\n\tstruct ceph_pagelist *pagelist;\n\tsize_t payload_len = 0;\n\tsize_t size;\n\tint ret;\n\n\top = osd_req_op_init(osd_req, which, CEPH_OSD_OP_CALL, 0);\n\n\tpagelist = ceph_pagelist_alloc(GFP_NOFS);\n\tif (!pagelist)\n\t\treturn -ENOMEM;\n\n\top->cls.class_name = class;\n\tsize = strlen(class);\n\tBUG_ON(size > (size_t) U8_MAX);\n\top->cls.class_len = size;\n\tret = ceph_pagelist_append(pagelist, class, size);\n\tif (ret)\n\t\tgoto err_pagelist_free;\n\tpayload_len += size;\n\n\top->cls.method_name = method;\n\tsize = strlen(method);\n\tBUG_ON(size > (size_t) U8_MAX);\n\top->cls.method_len = size;\n\tret = ceph_pagelist_append(pagelist, method, size);\n\tif (ret)\n\t\tgoto err_pagelist_free;\n\tpayload_len += size;\n\n\tosd_req_op_cls_request_info_pagelist(osd_req, which, pagelist);\n\top->indata_len = payload_len;\n\treturn 0;\n\nerr_pagelist_free:\n\tceph_pagelist_release(pagelist);\n\treturn ret;\n}\nEXPORT_SYMBOL(osd_req_op_cls_init);\n\nint osd_req_op_xattr_init(struct ceph_osd_request *osd_req, unsigned int which,\n\t\t\t  u16 opcode, const char *name, const void *value,\n\t\t\t  size_t size, u8 cmp_op, u8 cmp_mode)\n{\n\tstruct ceph_osd_req_op *op = osd_req_op_init(osd_req, which,\n\t\t\t\t\t\t     opcode, 0);\n\tstruct ceph_pagelist *pagelist;\n\tsize_t payload_len;\n\tint ret;\n\n\tBUG_ON(opcode != CEPH_OSD_OP_SETXATTR && opcode != CEPH_OSD_OP_CMPXATTR);\n\n\tpagelist = ceph_pagelist_alloc(GFP_NOFS);\n\tif (!pagelist)\n\t\treturn -ENOMEM;\n\n\tpayload_len = strlen(name);\n\top->xattr.name_len = payload_len;\n\tret = ceph_pagelist_append(pagelist, name, payload_len);\n\tif (ret)\n\t\tgoto err_pagelist_free;\n\n\top->xattr.value_len = size;\n\tret = ceph_pagelist_append(pagelist, value, size);\n\tif (ret)\n\t\tgoto err_pagelist_free;\n\tpayload_len += size;\n\n\top->xattr.cmp_op = cmp_op;\n\top->xattr.cmp_mode = cmp_mode;\n\n\tceph_osd_data_pagelist_init(&op->xattr.osd_data, pagelist);\n\top->indata_len = payload_len;\n\treturn 0;\n\nerr_pagelist_free:\n\tceph_pagelist_release(pagelist);\n\treturn ret;\n}\nEXPORT_SYMBOL(osd_req_op_xattr_init);\n\n \nstatic void osd_req_op_watch_init(struct ceph_osd_request *req, int which,\n\t\t\t\t  u8 watch_opcode, u64 cookie, u32 gen)\n{\n\tstruct ceph_osd_req_op *op;\n\n\top = osd_req_op_init(req, which, CEPH_OSD_OP_WATCH, 0);\n\top->watch.cookie = cookie;\n\top->watch.op = watch_opcode;\n\top->watch.gen = gen;\n}\n\n \nstatic void osd_req_op_notify_init(struct ceph_osd_request *req, int which,\n\t\t\t\t   u64 cookie, struct ceph_pagelist *request_pl)\n{\n\tstruct ceph_osd_req_op *op;\n\n\top = osd_req_op_init(req, which, CEPH_OSD_OP_NOTIFY, 0);\n\top->notify.cookie = cookie;\n\n\tceph_osd_data_pagelist_init(&op->notify.request_data, request_pl);\n\top->indata_len = request_pl->length;\n}\n\n \nvoid osd_req_op_alloc_hint_init(struct ceph_osd_request *osd_req,\n\t\t\t\tunsigned int which,\n\t\t\t\tu64 expected_object_size,\n\t\t\t\tu64 expected_write_size,\n\t\t\t\tu32 flags)\n{\n\tstruct ceph_osd_req_op *op;\n\n\top = osd_req_op_init(osd_req, which, CEPH_OSD_OP_SETALLOCHINT, 0);\n\top->alloc_hint.expected_object_size = expected_object_size;\n\top->alloc_hint.expected_write_size = expected_write_size;\n\top->alloc_hint.flags = flags;\n\n\t \n\top->flags |= CEPH_OSD_OP_FLAG_FAILOK;\n}\nEXPORT_SYMBOL(osd_req_op_alloc_hint_init);\n\nstatic void ceph_osdc_msg_data_add(struct ceph_msg *msg,\n\t\t\t\tstruct ceph_osd_data *osd_data)\n{\n\tu64 length = ceph_osd_data_length(osd_data);\n\n\tif (osd_data->type == CEPH_OSD_DATA_TYPE_PAGES) {\n\t\tBUG_ON(length > (u64) SIZE_MAX);\n\t\tif (length)\n\t\t\tceph_msg_data_add_pages(msg, osd_data->pages,\n\t\t\t\t\tlength, osd_data->alignment, false);\n\t} else if (osd_data->type == CEPH_OSD_DATA_TYPE_PAGELIST) {\n\t\tBUG_ON(!length);\n\t\tceph_msg_data_add_pagelist(msg, osd_data->pagelist);\n#ifdef CONFIG_BLOCK\n\t} else if (osd_data->type == CEPH_OSD_DATA_TYPE_BIO) {\n\t\tceph_msg_data_add_bio(msg, &osd_data->bio_pos, length);\n#endif\n\t} else if (osd_data->type == CEPH_OSD_DATA_TYPE_BVECS) {\n\t\tceph_msg_data_add_bvecs(msg, &osd_data->bvec_pos);\n\t} else if (osd_data->type == CEPH_OSD_DATA_TYPE_ITER) {\n\t\tceph_msg_data_add_iter(msg, &osd_data->iter);\n\t} else {\n\t\tBUG_ON(osd_data->type != CEPH_OSD_DATA_TYPE_NONE);\n\t}\n}\n\nstatic u32 osd_req_encode_op(struct ceph_osd_op *dst,\n\t\t\t     const struct ceph_osd_req_op *src)\n{\n\tswitch (src->op) {\n\tcase CEPH_OSD_OP_STAT:\n\t\tbreak;\n\tcase CEPH_OSD_OP_READ:\n\tcase CEPH_OSD_OP_SPARSE_READ:\n\tcase CEPH_OSD_OP_WRITE:\n\tcase CEPH_OSD_OP_WRITEFULL:\n\tcase CEPH_OSD_OP_ZERO:\n\tcase CEPH_OSD_OP_TRUNCATE:\n\t\tdst->extent.offset = cpu_to_le64(src->extent.offset);\n\t\tdst->extent.length = cpu_to_le64(src->extent.length);\n\t\tdst->extent.truncate_size =\n\t\t\tcpu_to_le64(src->extent.truncate_size);\n\t\tdst->extent.truncate_seq =\n\t\t\tcpu_to_le32(src->extent.truncate_seq);\n\t\tbreak;\n\tcase CEPH_OSD_OP_CALL:\n\t\tdst->cls.class_len = src->cls.class_len;\n\t\tdst->cls.method_len = src->cls.method_len;\n\t\tdst->cls.indata_len = cpu_to_le32(src->cls.indata_len);\n\t\tbreak;\n\tcase CEPH_OSD_OP_WATCH:\n\t\tdst->watch.cookie = cpu_to_le64(src->watch.cookie);\n\t\tdst->watch.ver = cpu_to_le64(0);\n\t\tdst->watch.op = src->watch.op;\n\t\tdst->watch.gen = cpu_to_le32(src->watch.gen);\n\t\tbreak;\n\tcase CEPH_OSD_OP_NOTIFY_ACK:\n\t\tbreak;\n\tcase CEPH_OSD_OP_NOTIFY:\n\t\tdst->notify.cookie = cpu_to_le64(src->notify.cookie);\n\t\tbreak;\n\tcase CEPH_OSD_OP_LIST_WATCHERS:\n\t\tbreak;\n\tcase CEPH_OSD_OP_SETALLOCHINT:\n\t\tdst->alloc_hint.expected_object_size =\n\t\t    cpu_to_le64(src->alloc_hint.expected_object_size);\n\t\tdst->alloc_hint.expected_write_size =\n\t\t    cpu_to_le64(src->alloc_hint.expected_write_size);\n\t\tdst->alloc_hint.flags = cpu_to_le32(src->alloc_hint.flags);\n\t\tbreak;\n\tcase CEPH_OSD_OP_SETXATTR:\n\tcase CEPH_OSD_OP_CMPXATTR:\n\t\tdst->xattr.name_len = cpu_to_le32(src->xattr.name_len);\n\t\tdst->xattr.value_len = cpu_to_le32(src->xattr.value_len);\n\t\tdst->xattr.cmp_op = src->xattr.cmp_op;\n\t\tdst->xattr.cmp_mode = src->xattr.cmp_mode;\n\t\tbreak;\n\tcase CEPH_OSD_OP_CREATE:\n\tcase CEPH_OSD_OP_DELETE:\n\t\tbreak;\n\tcase CEPH_OSD_OP_COPY_FROM2:\n\t\tdst->copy_from.snapid = cpu_to_le64(src->copy_from.snapid);\n\t\tdst->copy_from.src_version =\n\t\t\tcpu_to_le64(src->copy_from.src_version);\n\t\tdst->copy_from.flags = src->copy_from.flags;\n\t\tdst->copy_from.src_fadvise_flags =\n\t\t\tcpu_to_le32(src->copy_from.src_fadvise_flags);\n\t\tbreak;\n\tcase CEPH_OSD_OP_ASSERT_VER:\n\t\tdst->assert_ver.unused = cpu_to_le64(0);\n\t\tdst->assert_ver.ver = cpu_to_le64(src->assert_ver.ver);\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"unsupported osd opcode %s\\n\",\n\t\t\tceph_osd_op_name(src->op));\n\t\tWARN_ON(1);\n\n\t\treturn 0;\n\t}\n\n\tdst->op = cpu_to_le16(src->op);\n\tdst->flags = cpu_to_le32(src->flags);\n\tdst->payload_len = cpu_to_le32(src->indata_len);\n\n\treturn src->indata_len;\n}\n\n \nstruct ceph_osd_request *ceph_osdc_new_request(struct ceph_osd_client *osdc,\n\t\t\t\t\t       struct ceph_file_layout *layout,\n\t\t\t\t\t       struct ceph_vino vino,\n\t\t\t\t\t       u64 off, u64 *plen,\n\t\t\t\t\t       unsigned int which, int num_ops,\n\t\t\t\t\t       int opcode, int flags,\n\t\t\t\t\t       struct ceph_snap_context *snapc,\n\t\t\t\t\t       u32 truncate_seq,\n\t\t\t\t\t       u64 truncate_size,\n\t\t\t\t\t       bool use_mempool)\n{\n\tstruct ceph_osd_request *req;\n\tu64 objnum = 0;\n\tu64 objoff = 0;\n\tu64 objlen = 0;\n\tint r;\n\n\tBUG_ON(opcode != CEPH_OSD_OP_READ && opcode != CEPH_OSD_OP_WRITE &&\n\t       opcode != CEPH_OSD_OP_ZERO && opcode != CEPH_OSD_OP_TRUNCATE &&\n\t       opcode != CEPH_OSD_OP_CREATE && opcode != CEPH_OSD_OP_DELETE &&\n\t       opcode != CEPH_OSD_OP_SPARSE_READ);\n\n\treq = ceph_osdc_alloc_request(osdc, snapc, num_ops, use_mempool,\n\t\t\t\t\tGFP_NOFS);\n\tif (!req) {\n\t\tr = -ENOMEM;\n\t\tgoto fail;\n\t}\n\n\t \n\tr = calc_layout(layout, off, plen, &objnum, &objoff, &objlen);\n\tif (r)\n\t\tgoto fail;\n\n\tif (opcode == CEPH_OSD_OP_CREATE || opcode == CEPH_OSD_OP_DELETE) {\n\t\tosd_req_op_init(req, which, opcode, 0);\n\t} else {\n\t\tu32 object_size = layout->object_size;\n\t\tu32 object_base = off - objoff;\n\t\tif (!(truncate_seq == 1 && truncate_size == -1ULL)) {\n\t\t\tif (truncate_size <= object_base) {\n\t\t\t\ttruncate_size = 0;\n\t\t\t} else {\n\t\t\t\ttruncate_size -= object_base;\n\t\t\t\tif (truncate_size > object_size)\n\t\t\t\t\ttruncate_size = object_size;\n\t\t\t}\n\t\t}\n\t\tosd_req_op_extent_init(req, which, opcode, objoff, objlen,\n\t\t\t\t       truncate_size, truncate_seq);\n\t}\n\n\treq->r_base_oloc.pool = layout->pool_id;\n\treq->r_base_oloc.pool_ns = ceph_try_get_string(layout->pool_ns);\n\tceph_oid_printf(&req->r_base_oid, \"%llx.%08llx\", vino.ino, objnum);\n\treq->r_flags = flags | osdc->client->options->read_from_replica;\n\n\treq->r_snapid = vino.snap;\n\tif (flags & CEPH_OSD_FLAG_WRITE)\n\t\treq->r_data_offset = off;\n\n\tif (num_ops > 1) {\n\t\tint num_req_ops, num_rep_ops;\n\n\t\t \n\t\tif (flags & CEPH_OSD_FLAG_WRITE) {\n\t\t\tnum_req_ops = num_ops;\n\t\t\tnum_rep_ops = 0;\n\t\t} else if (flags & CEPH_OSD_FLAG_READ) {\n\t\t\tnum_req_ops = 0;\n\t\t\tnum_rep_ops = num_ops;\n\t\t} else {\n\t\t\tr = -EINVAL;\n\t\t\tgoto fail;\n\t\t}\n\n\t\tr = __ceph_osdc_alloc_messages(req, GFP_NOFS, num_req_ops,\n\t\t\t\t\t       num_rep_ops);\n\t} else {\n\t\tr = ceph_osdc_alloc_messages(req, GFP_NOFS);\n\t}\n\tif (r)\n\t\tgoto fail;\n\n\treturn req;\n\nfail:\n\tceph_osdc_put_request(req);\n\treturn ERR_PTR(r);\n}\nEXPORT_SYMBOL(ceph_osdc_new_request);\n\nint __ceph_alloc_sparse_ext_map(struct ceph_osd_req_op *op, int cnt)\n{\n\top->extent.sparse_ext_cnt = cnt;\n\top->extent.sparse_ext = kmalloc_array(cnt,\n\t\t\t\t\t      sizeof(*op->extent.sparse_ext),\n\t\t\t\t\t      GFP_NOFS);\n\tif (!op->extent.sparse_ext)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\nEXPORT_SYMBOL(__ceph_alloc_sparse_ext_map);\n\n \nDEFINE_RB_FUNCS(request, struct ceph_osd_request, r_tid, r_node)\nDEFINE_RB_FUNCS(request_mc, struct ceph_osd_request, r_tid, r_mc_node)\n\n \nstatic void for_each_request(struct ceph_osd_client *osdc,\n\t\t\tint (*fn)(struct ceph_osd_request *req, void *arg),\n\t\t\tvoid *arg)\n{\n\tstruct rb_node *n, *p;\n\n\tfor (n = rb_first(&osdc->osds); n; n = rb_next(n)) {\n\t\tstruct ceph_osd *osd = rb_entry(n, struct ceph_osd, o_node);\n\n\t\tfor (p = rb_first(&osd->o_requests); p; ) {\n\t\t\tstruct ceph_osd_request *req =\n\t\t\t    rb_entry(p, struct ceph_osd_request, r_node);\n\n\t\t\tp = rb_next(p);\n\t\t\tif (fn(req, arg))\n\t\t\t\treturn;\n\t\t}\n\t}\n\n\tfor (p = rb_first(&osdc->homeless_osd.o_requests); p; ) {\n\t\tstruct ceph_osd_request *req =\n\t\t    rb_entry(p, struct ceph_osd_request, r_node);\n\n\t\tp = rb_next(p);\n\t\tif (fn(req, arg))\n\t\t\treturn;\n\t}\n}\n\nstatic bool osd_homeless(struct ceph_osd *osd)\n{\n\treturn osd->o_osd == CEPH_HOMELESS_OSD;\n}\n\nstatic bool osd_registered(struct ceph_osd *osd)\n{\n\tverify_osdc_locked(osd->o_osdc);\n\n\treturn !RB_EMPTY_NODE(&osd->o_node);\n}\n\n \nstatic void osd_init(struct ceph_osd *osd)\n{\n\trefcount_set(&osd->o_ref, 1);\n\tRB_CLEAR_NODE(&osd->o_node);\n\tspin_lock_init(&osd->o_requests_lock);\n\tosd->o_requests = RB_ROOT;\n\tosd->o_linger_requests = RB_ROOT;\n\tosd->o_backoff_mappings = RB_ROOT;\n\tosd->o_backoffs_by_id = RB_ROOT;\n\tINIT_LIST_HEAD(&osd->o_osd_lru);\n\tINIT_LIST_HEAD(&osd->o_keepalive_item);\n\tosd->o_incarnation = 1;\n\tmutex_init(&osd->lock);\n}\n\nstatic void ceph_init_sparse_read(struct ceph_sparse_read *sr)\n{\n\tkfree(sr->sr_extent);\n\tmemset(sr, '\\0', sizeof(*sr));\n\tsr->sr_state = CEPH_SPARSE_READ_HDR;\n}\n\nstatic void osd_cleanup(struct ceph_osd *osd)\n{\n\tWARN_ON(!RB_EMPTY_NODE(&osd->o_node));\n\tWARN_ON(!RB_EMPTY_ROOT(&osd->o_requests));\n\tWARN_ON(!RB_EMPTY_ROOT(&osd->o_linger_requests));\n\tWARN_ON(!RB_EMPTY_ROOT(&osd->o_backoff_mappings));\n\tWARN_ON(!RB_EMPTY_ROOT(&osd->o_backoffs_by_id));\n\tWARN_ON(!list_empty(&osd->o_osd_lru));\n\tWARN_ON(!list_empty(&osd->o_keepalive_item));\n\n\tceph_init_sparse_read(&osd->o_sparse_read);\n\n\tif (osd->o_auth.authorizer) {\n\t\tWARN_ON(osd_homeless(osd));\n\t\tceph_auth_destroy_authorizer(osd->o_auth.authorizer);\n\t}\n}\n\n \nstatic struct ceph_osd *create_osd(struct ceph_osd_client *osdc, int onum)\n{\n\tstruct ceph_osd *osd;\n\n\tWARN_ON(onum == CEPH_HOMELESS_OSD);\n\n\tosd = kzalloc(sizeof(*osd), GFP_NOIO | __GFP_NOFAIL);\n\tosd_init(osd);\n\tosd->o_osdc = osdc;\n\tosd->o_osd = onum;\n\tosd->o_sparse_op_idx = -1;\n\n\tceph_init_sparse_read(&osd->o_sparse_read);\n\n\tceph_con_init(&osd->o_con, osd, &osd_con_ops, &osdc->client->msgr);\n\n\treturn osd;\n}\n\nstatic struct ceph_osd *get_osd(struct ceph_osd *osd)\n{\n\tif (refcount_inc_not_zero(&osd->o_ref)) {\n\t\tdout(\"get_osd %p %d -> %d\\n\", osd, refcount_read(&osd->o_ref)-1,\n\t\t     refcount_read(&osd->o_ref));\n\t\treturn osd;\n\t} else {\n\t\tdout(\"get_osd %p FAIL\\n\", osd);\n\t\treturn NULL;\n\t}\n}\n\nstatic void put_osd(struct ceph_osd *osd)\n{\n\tdout(\"put_osd %p %d -> %d\\n\", osd, refcount_read(&osd->o_ref),\n\t     refcount_read(&osd->o_ref) - 1);\n\tif (refcount_dec_and_test(&osd->o_ref)) {\n\t\tosd_cleanup(osd);\n\t\tkfree(osd);\n\t}\n}\n\nDEFINE_RB_FUNCS(osd, struct ceph_osd, o_osd, o_node)\n\nstatic void __move_osd_to_lru(struct ceph_osd *osd)\n{\n\tstruct ceph_osd_client *osdc = osd->o_osdc;\n\n\tdout(\"%s osd %p osd%d\\n\", __func__, osd, osd->o_osd);\n\tBUG_ON(!list_empty(&osd->o_osd_lru));\n\n\tspin_lock(&osdc->osd_lru_lock);\n\tlist_add_tail(&osd->o_osd_lru, &osdc->osd_lru);\n\tspin_unlock(&osdc->osd_lru_lock);\n\n\tosd->lru_ttl = jiffies + osdc->client->options->osd_idle_ttl;\n}\n\nstatic void maybe_move_osd_to_lru(struct ceph_osd *osd)\n{\n\tif (RB_EMPTY_ROOT(&osd->o_requests) &&\n\t    RB_EMPTY_ROOT(&osd->o_linger_requests))\n\t\t__move_osd_to_lru(osd);\n}\n\nstatic void __remove_osd_from_lru(struct ceph_osd *osd)\n{\n\tstruct ceph_osd_client *osdc = osd->o_osdc;\n\n\tdout(\"%s osd %p osd%d\\n\", __func__, osd, osd->o_osd);\n\n\tspin_lock(&osdc->osd_lru_lock);\n\tif (!list_empty(&osd->o_osd_lru))\n\t\tlist_del_init(&osd->o_osd_lru);\n\tspin_unlock(&osdc->osd_lru_lock);\n}\n\n \nstatic void close_osd(struct ceph_osd *osd)\n{\n\tstruct ceph_osd_client *osdc = osd->o_osdc;\n\tstruct rb_node *n;\n\n\tverify_osdc_wrlocked(osdc);\n\tdout(\"%s osd %p osd%d\\n\", __func__, osd, osd->o_osd);\n\n\tceph_con_close(&osd->o_con);\n\n\tfor (n = rb_first(&osd->o_requests); n; ) {\n\t\tstruct ceph_osd_request *req =\n\t\t    rb_entry(n, struct ceph_osd_request, r_node);\n\n\t\tn = rb_next(n);  \n\n\t\tdout(\" reassigning req %p tid %llu\\n\", req, req->r_tid);\n\t\tunlink_request(osd, req);\n\t\tlink_request(&osdc->homeless_osd, req);\n\t}\n\tfor (n = rb_first(&osd->o_linger_requests); n; ) {\n\t\tstruct ceph_osd_linger_request *lreq =\n\t\t    rb_entry(n, struct ceph_osd_linger_request, node);\n\n\t\tn = rb_next(n);  \n\n\t\tdout(\" reassigning lreq %p linger_id %llu\\n\", lreq,\n\t\t     lreq->linger_id);\n\t\tunlink_linger(osd, lreq);\n\t\tlink_linger(&osdc->homeless_osd, lreq);\n\t}\n\tclear_backoffs(osd);\n\n\t__remove_osd_from_lru(osd);\n\terase_osd(&osdc->osds, osd);\n\tput_osd(osd);\n}\n\n \nstatic int reopen_osd(struct ceph_osd *osd)\n{\n\tstruct ceph_entity_addr *peer_addr;\n\n\tdout(\"%s osd %p osd%d\\n\", __func__, osd, osd->o_osd);\n\n\tif (RB_EMPTY_ROOT(&osd->o_requests) &&\n\t    RB_EMPTY_ROOT(&osd->o_linger_requests)) {\n\t\tclose_osd(osd);\n\t\treturn -ENODEV;\n\t}\n\n\tpeer_addr = &osd->o_osdc->osdmap->osd_addr[osd->o_osd];\n\tif (!memcmp(peer_addr, &osd->o_con.peer_addr, sizeof (*peer_addr)) &&\n\t\t\t!ceph_con_opened(&osd->o_con)) {\n\t\tstruct rb_node *n;\n\n\t\tdout(\"osd addr hasn't changed and connection never opened, \"\n\t\t     \"letting msgr retry\\n\");\n\t\t \n\t\tfor (n = rb_first(&osd->o_requests); n; n = rb_next(n)) {\n\t\t\tstruct ceph_osd_request *req =\n\t\t\t    rb_entry(n, struct ceph_osd_request, r_node);\n\t\t\treq->r_stamp = jiffies;\n\t\t}\n\n\t\treturn -EAGAIN;\n\t}\n\n\tceph_con_close(&osd->o_con);\n\tceph_con_open(&osd->o_con, CEPH_ENTITY_TYPE_OSD, osd->o_osd, peer_addr);\n\tosd->o_incarnation++;\n\n\treturn 0;\n}\n\nstatic struct ceph_osd *lookup_create_osd(struct ceph_osd_client *osdc, int o,\n\t\t\t\t\t  bool wrlocked)\n{\n\tstruct ceph_osd *osd;\n\n\tif (wrlocked)\n\t\tverify_osdc_wrlocked(osdc);\n\telse\n\t\tverify_osdc_locked(osdc);\n\n\tif (o != CEPH_HOMELESS_OSD)\n\t\tosd = lookup_osd(&osdc->osds, o);\n\telse\n\t\tosd = &osdc->homeless_osd;\n\tif (!osd) {\n\t\tif (!wrlocked)\n\t\t\treturn ERR_PTR(-EAGAIN);\n\n\t\tosd = create_osd(osdc, o);\n\t\tinsert_osd(&osdc->osds, osd);\n\t\tceph_con_open(&osd->o_con, CEPH_ENTITY_TYPE_OSD, osd->o_osd,\n\t\t\t      &osdc->osdmap->osd_addr[osd->o_osd]);\n\t}\n\n\tdout(\"%s osdc %p osd%d -> osd %p\\n\", __func__, osdc, o, osd);\n\treturn osd;\n}\n\n \nstatic void link_request(struct ceph_osd *osd, struct ceph_osd_request *req)\n{\n\tverify_osd_locked(osd);\n\tWARN_ON(!req->r_tid || req->r_osd);\n\tdout(\"%s osd %p osd%d req %p tid %llu\\n\", __func__, osd, osd->o_osd,\n\t     req, req->r_tid);\n\n\tif (!osd_homeless(osd))\n\t\t__remove_osd_from_lru(osd);\n\telse\n\t\tatomic_inc(&osd->o_osdc->num_homeless);\n\n\tget_osd(osd);\n\tspin_lock(&osd->o_requests_lock);\n\tinsert_request(&osd->o_requests, req);\n\tspin_unlock(&osd->o_requests_lock);\n\treq->r_osd = osd;\n}\n\nstatic void unlink_request(struct ceph_osd *osd, struct ceph_osd_request *req)\n{\n\tverify_osd_locked(osd);\n\tWARN_ON(req->r_osd != osd);\n\tdout(\"%s osd %p osd%d req %p tid %llu\\n\", __func__, osd, osd->o_osd,\n\t     req, req->r_tid);\n\n\treq->r_osd = NULL;\n\tspin_lock(&osd->o_requests_lock);\n\terase_request(&osd->o_requests, req);\n\tspin_unlock(&osd->o_requests_lock);\n\tput_osd(osd);\n\n\tif (!osd_homeless(osd))\n\t\tmaybe_move_osd_to_lru(osd);\n\telse\n\t\tatomic_dec(&osd->o_osdc->num_homeless);\n}\n\nstatic bool __pool_full(struct ceph_pg_pool_info *pi)\n{\n\treturn pi->flags & CEPH_POOL_FLAG_FULL;\n}\n\nstatic bool have_pool_full(struct ceph_osd_client *osdc)\n{\n\tstruct rb_node *n;\n\n\tfor (n = rb_first(&osdc->osdmap->pg_pools); n; n = rb_next(n)) {\n\t\tstruct ceph_pg_pool_info *pi =\n\t\t    rb_entry(n, struct ceph_pg_pool_info, node);\n\n\t\tif (__pool_full(pi))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic bool pool_full(struct ceph_osd_client *osdc, s64 pool_id)\n{\n\tstruct ceph_pg_pool_info *pi;\n\n\tpi = ceph_pg_pool_by_id(osdc->osdmap, pool_id);\n\tif (!pi)\n\t\treturn false;\n\n\treturn __pool_full(pi);\n}\n\n \nstatic bool target_should_be_paused(struct ceph_osd_client *osdc,\n\t\t\t\t    const struct ceph_osd_request_target *t,\n\t\t\t\t    struct ceph_pg_pool_info *pi)\n{\n\tbool pauserd = ceph_osdmap_flag(osdc, CEPH_OSDMAP_PAUSERD);\n\tbool pausewr = ceph_osdmap_flag(osdc, CEPH_OSDMAP_PAUSEWR) ||\n\t\t       ceph_osdmap_flag(osdc, CEPH_OSDMAP_FULL) ||\n\t\t       __pool_full(pi);\n\n\tWARN_ON(pi->id != t->target_oloc.pool);\n\treturn ((t->flags & CEPH_OSD_FLAG_READ) && pauserd) ||\n\t       ((t->flags & CEPH_OSD_FLAG_WRITE) && pausewr) ||\n\t       (osdc->osdmap->epoch < osdc->epoch_barrier);\n}\n\nstatic int pick_random_replica(const struct ceph_osds *acting)\n{\n\tint i = get_random_u32_below(acting->size);\n\n\tdout(\"%s picked osd%d, primary osd%d\\n\", __func__,\n\t     acting->osds[i], acting->primary);\n\treturn i;\n}\n\n \nstatic int pick_closest_replica(struct ceph_osd_client *osdc,\n\t\t\t\tconst struct ceph_osds *acting)\n{\n\tstruct ceph_options *opt = osdc->client->options;\n\tint best_i, best_locality;\n\tint i = 0, locality;\n\n\tdo {\n\t\tlocality = ceph_get_crush_locality(osdc->osdmap,\n\t\t\t\t\t\t   acting->osds[i],\n\t\t\t\t\t\t   &opt->crush_locs);\n\t\tif (i == 0 ||\n\t\t    (locality >= 0 && best_locality < 0) ||\n\t\t    (locality >= 0 && best_locality >= 0 &&\n\t\t     locality < best_locality)) {\n\t\t\tbest_i = i;\n\t\t\tbest_locality = locality;\n\t\t}\n\t} while (++i < acting->size);\n\n\tdout(\"%s picked osd%d with locality %d, primary osd%d\\n\", __func__,\n\t     acting->osds[best_i], best_locality, acting->primary);\n\treturn best_i;\n}\n\nenum calc_target_result {\n\tCALC_TARGET_NO_ACTION = 0,\n\tCALC_TARGET_NEED_RESEND,\n\tCALC_TARGET_POOL_DNE,\n};\n\nstatic enum calc_target_result calc_target(struct ceph_osd_client *osdc,\n\t\t\t\t\t   struct ceph_osd_request_target *t,\n\t\t\t\t\t   bool any_change)\n{\n\tstruct ceph_pg_pool_info *pi;\n\tstruct ceph_pg pgid, last_pgid;\n\tstruct ceph_osds up, acting;\n\tbool is_read = t->flags & CEPH_OSD_FLAG_READ;\n\tbool is_write = t->flags & CEPH_OSD_FLAG_WRITE;\n\tbool force_resend = false;\n\tbool unpaused = false;\n\tbool legacy_change = false;\n\tbool split = false;\n\tbool sort_bitwise = ceph_osdmap_flag(osdc, CEPH_OSDMAP_SORTBITWISE);\n\tbool recovery_deletes = ceph_osdmap_flag(osdc,\n\t\t\t\t\t\t CEPH_OSDMAP_RECOVERY_DELETES);\n\tenum calc_target_result ct_res;\n\n\tt->epoch = osdc->osdmap->epoch;\n\tpi = ceph_pg_pool_by_id(osdc->osdmap, t->base_oloc.pool);\n\tif (!pi) {\n\t\tt->osd = CEPH_HOMELESS_OSD;\n\t\tct_res = CALC_TARGET_POOL_DNE;\n\t\tgoto out;\n\t}\n\n\tif (osdc->osdmap->epoch == pi->last_force_request_resend) {\n\t\tif (t->last_force_resend < pi->last_force_request_resend) {\n\t\t\tt->last_force_resend = pi->last_force_request_resend;\n\t\t\tforce_resend = true;\n\t\t} else if (t->last_force_resend == 0) {\n\t\t\tforce_resend = true;\n\t\t}\n\t}\n\n\t \n\tceph_oid_copy(&t->target_oid, &t->base_oid);\n\tceph_oloc_copy(&t->target_oloc, &t->base_oloc);\n\tif ((t->flags & CEPH_OSD_FLAG_IGNORE_OVERLAY) == 0) {\n\t\tif (is_read && pi->read_tier >= 0)\n\t\t\tt->target_oloc.pool = pi->read_tier;\n\t\tif (is_write && pi->write_tier >= 0)\n\t\t\tt->target_oloc.pool = pi->write_tier;\n\n\t\tpi = ceph_pg_pool_by_id(osdc->osdmap, t->target_oloc.pool);\n\t\tif (!pi) {\n\t\t\tt->osd = CEPH_HOMELESS_OSD;\n\t\t\tct_res = CALC_TARGET_POOL_DNE;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t__ceph_object_locator_to_pg(pi, &t->target_oid, &t->target_oloc, &pgid);\n\tlast_pgid.pool = pgid.pool;\n\tlast_pgid.seed = ceph_stable_mod(pgid.seed, t->pg_num, t->pg_num_mask);\n\n\tceph_pg_to_up_acting_osds(osdc->osdmap, pi, &pgid, &up, &acting);\n\tif (any_change &&\n\t    ceph_is_new_interval(&t->acting,\n\t\t\t\t &acting,\n\t\t\t\t &t->up,\n\t\t\t\t &up,\n\t\t\t\t t->size,\n\t\t\t\t pi->size,\n\t\t\t\t t->min_size,\n\t\t\t\t pi->min_size,\n\t\t\t\t t->pg_num,\n\t\t\t\t pi->pg_num,\n\t\t\t\t t->sort_bitwise,\n\t\t\t\t sort_bitwise,\n\t\t\t\t t->recovery_deletes,\n\t\t\t\t recovery_deletes,\n\t\t\t\t &last_pgid))\n\t\tforce_resend = true;\n\n\tif (t->paused && !target_should_be_paused(osdc, t, pi)) {\n\t\tt->paused = false;\n\t\tunpaused = true;\n\t}\n\tlegacy_change = ceph_pg_compare(&t->pgid, &pgid) ||\n\t\t\tceph_osds_changed(&t->acting, &acting,\n\t\t\t\t\t  t->used_replica || any_change);\n\tif (t->pg_num)\n\t\tsplit = ceph_pg_is_split(&last_pgid, t->pg_num, pi->pg_num);\n\n\tif (legacy_change || force_resend || split) {\n\t\tt->pgid = pgid;  \n\t\tceph_pg_to_primary_shard(osdc->osdmap, pi, &pgid, &t->spgid);\n\t\tceph_osds_copy(&t->acting, &acting);\n\t\tceph_osds_copy(&t->up, &up);\n\t\tt->size = pi->size;\n\t\tt->min_size = pi->min_size;\n\t\tt->pg_num = pi->pg_num;\n\t\tt->pg_num_mask = pi->pg_num_mask;\n\t\tt->sort_bitwise = sort_bitwise;\n\t\tt->recovery_deletes = recovery_deletes;\n\n\t\tif ((t->flags & (CEPH_OSD_FLAG_BALANCE_READS |\n\t\t\t\t CEPH_OSD_FLAG_LOCALIZE_READS)) &&\n\t\t    !is_write && pi->type == CEPH_POOL_TYPE_REP &&\n\t\t    acting.size > 1) {\n\t\t\tint pos;\n\n\t\t\tWARN_ON(!is_read || acting.osds[0] != acting.primary);\n\t\t\tif (t->flags & CEPH_OSD_FLAG_BALANCE_READS) {\n\t\t\t\tpos = pick_random_replica(&acting);\n\t\t\t} else {\n\t\t\t\tpos = pick_closest_replica(osdc, &acting);\n\t\t\t}\n\t\t\tt->osd = acting.osds[pos];\n\t\t\tt->used_replica = pos > 0;\n\t\t} else {\n\t\t\tt->osd = acting.primary;\n\t\t\tt->used_replica = false;\n\t\t}\n\t}\n\n\tif (unpaused || legacy_change || force_resend || split)\n\t\tct_res = CALC_TARGET_NEED_RESEND;\n\telse\n\t\tct_res = CALC_TARGET_NO_ACTION;\n\nout:\n\tdout(\"%s t %p -> %d%d%d%d ct_res %d osd%d\\n\", __func__, t, unpaused,\n\t     legacy_change, force_resend, split, ct_res, t->osd);\n\treturn ct_res;\n}\n\nstatic struct ceph_spg_mapping *alloc_spg_mapping(void)\n{\n\tstruct ceph_spg_mapping *spg;\n\n\tspg = kmalloc(sizeof(*spg), GFP_NOIO);\n\tif (!spg)\n\t\treturn NULL;\n\n\tRB_CLEAR_NODE(&spg->node);\n\tspg->backoffs = RB_ROOT;\n\treturn spg;\n}\n\nstatic void free_spg_mapping(struct ceph_spg_mapping *spg)\n{\n\tWARN_ON(!RB_EMPTY_NODE(&spg->node));\n\tWARN_ON(!RB_EMPTY_ROOT(&spg->backoffs));\n\n\tkfree(spg);\n}\n\n \nDEFINE_RB_FUNCS2(spg_mapping, struct ceph_spg_mapping, spgid, ceph_spg_compare,\n\t\t RB_BYPTR, const struct ceph_spg *, node)\n\nstatic u64 hoid_get_bitwise_key(const struct ceph_hobject_id *hoid)\n{\n\treturn hoid->is_max ? 0x100000000ull : hoid->hash_reverse_bits;\n}\n\nstatic void hoid_get_effective_key(const struct ceph_hobject_id *hoid,\n\t\t\t\t   void **pkey, size_t *pkey_len)\n{\n\tif (hoid->key_len) {\n\t\t*pkey = hoid->key;\n\t\t*pkey_len = hoid->key_len;\n\t} else {\n\t\t*pkey = hoid->oid;\n\t\t*pkey_len = hoid->oid_len;\n\t}\n}\n\nstatic int compare_names(const void *name1, size_t name1_len,\n\t\t\t const void *name2, size_t name2_len)\n{\n\tint ret;\n\n\tret = memcmp(name1, name2, min(name1_len, name2_len));\n\tif (!ret) {\n\t\tif (name1_len < name2_len)\n\t\t\tret = -1;\n\t\telse if (name1_len > name2_len)\n\t\t\tret = 1;\n\t}\n\treturn ret;\n}\n\nstatic int hoid_compare(const struct ceph_hobject_id *lhs,\n\t\t\tconst struct ceph_hobject_id *rhs)\n{\n\tvoid *effective_key1, *effective_key2;\n\tsize_t effective_key1_len, effective_key2_len;\n\tint ret;\n\n\tif (lhs->is_max < rhs->is_max)\n\t\treturn -1;\n\tif (lhs->is_max > rhs->is_max)\n\t\treturn 1;\n\n\tif (lhs->pool < rhs->pool)\n\t\treturn -1;\n\tif (lhs->pool > rhs->pool)\n\t\treturn 1;\n\n\tif (hoid_get_bitwise_key(lhs) < hoid_get_bitwise_key(rhs))\n\t\treturn -1;\n\tif (hoid_get_bitwise_key(lhs) > hoid_get_bitwise_key(rhs))\n\t\treturn 1;\n\n\tret = compare_names(lhs->nspace, lhs->nspace_len,\n\t\t\t    rhs->nspace, rhs->nspace_len);\n\tif (ret)\n\t\treturn ret;\n\n\thoid_get_effective_key(lhs, &effective_key1, &effective_key1_len);\n\thoid_get_effective_key(rhs, &effective_key2, &effective_key2_len);\n\tret = compare_names(effective_key1, effective_key1_len,\n\t\t\t    effective_key2, effective_key2_len);\n\tif (ret)\n\t\treturn ret;\n\n\tret = compare_names(lhs->oid, lhs->oid_len, rhs->oid, rhs->oid_len);\n\tif (ret)\n\t\treturn ret;\n\n\tif (lhs->snapid < rhs->snapid)\n\t\treturn -1;\n\tif (lhs->snapid > rhs->snapid)\n\t\treturn 1;\n\n\treturn 0;\n}\n\n \nstatic int decode_hoid(void **p, void *end, struct ceph_hobject_id *hoid)\n{\n\tu8 struct_v;\n\tu32 struct_len;\n\tint ret;\n\n\tret = ceph_start_decoding(p, end, 4, \"hobject_t\", &struct_v,\n\t\t\t\t  &struct_len);\n\tif (ret)\n\t\treturn ret;\n\n\tif (struct_v < 4) {\n\t\tpr_err(\"got struct_v %d < 4 of hobject_t\\n\", struct_v);\n\t\tgoto e_inval;\n\t}\n\n\thoid->key = ceph_extract_encoded_string(p, end, &hoid->key_len,\n\t\t\t\t\t\tGFP_NOIO);\n\tif (IS_ERR(hoid->key)) {\n\t\tret = PTR_ERR(hoid->key);\n\t\thoid->key = NULL;\n\t\treturn ret;\n\t}\n\n\thoid->oid = ceph_extract_encoded_string(p, end, &hoid->oid_len,\n\t\t\t\t\t\tGFP_NOIO);\n\tif (IS_ERR(hoid->oid)) {\n\t\tret = PTR_ERR(hoid->oid);\n\t\thoid->oid = NULL;\n\t\treturn ret;\n\t}\n\n\tceph_decode_64_safe(p, end, hoid->snapid, e_inval);\n\tceph_decode_32_safe(p, end, hoid->hash, e_inval);\n\tceph_decode_8_safe(p, end, hoid->is_max, e_inval);\n\n\thoid->nspace = ceph_extract_encoded_string(p, end, &hoid->nspace_len,\n\t\t\t\t\t\t   GFP_NOIO);\n\tif (IS_ERR(hoid->nspace)) {\n\t\tret = PTR_ERR(hoid->nspace);\n\t\thoid->nspace = NULL;\n\t\treturn ret;\n\t}\n\n\tceph_decode_64_safe(p, end, hoid->pool, e_inval);\n\n\tceph_hoid_build_hash_cache(hoid);\n\treturn 0;\n\ne_inval:\n\treturn -EINVAL;\n}\n\nstatic int hoid_encoding_size(const struct ceph_hobject_id *hoid)\n{\n\treturn 8 + 4 + 1 + 8 +  \n\t       4 + hoid->key_len + 4 + hoid->oid_len + 4 + hoid->nspace_len;\n}\n\nstatic void encode_hoid(void **p, void *end, const struct ceph_hobject_id *hoid)\n{\n\tceph_start_encoding(p, 4, 3, hoid_encoding_size(hoid));\n\tceph_encode_string(p, end, hoid->key, hoid->key_len);\n\tceph_encode_string(p, end, hoid->oid, hoid->oid_len);\n\tceph_encode_64(p, hoid->snapid);\n\tceph_encode_32(p, hoid->hash);\n\tceph_encode_8(p, hoid->is_max);\n\tceph_encode_string(p, end, hoid->nspace, hoid->nspace_len);\n\tceph_encode_64(p, hoid->pool);\n}\n\nstatic void free_hoid(struct ceph_hobject_id *hoid)\n{\n\tif (hoid) {\n\t\tkfree(hoid->key);\n\t\tkfree(hoid->oid);\n\t\tkfree(hoid->nspace);\n\t\tkfree(hoid);\n\t}\n}\n\nstatic struct ceph_osd_backoff *alloc_backoff(void)\n{\n\tstruct ceph_osd_backoff *backoff;\n\n\tbackoff = kzalloc(sizeof(*backoff), GFP_NOIO);\n\tif (!backoff)\n\t\treturn NULL;\n\n\tRB_CLEAR_NODE(&backoff->spg_node);\n\tRB_CLEAR_NODE(&backoff->id_node);\n\treturn backoff;\n}\n\nstatic void free_backoff(struct ceph_osd_backoff *backoff)\n{\n\tWARN_ON(!RB_EMPTY_NODE(&backoff->spg_node));\n\tWARN_ON(!RB_EMPTY_NODE(&backoff->id_node));\n\n\tfree_hoid(backoff->begin);\n\tfree_hoid(backoff->end);\n\tkfree(backoff);\n}\n\n \nDEFINE_RB_INSDEL_FUNCS2(backoff, struct ceph_osd_backoff, begin, hoid_compare,\n\t\t\tRB_BYVAL, spg_node);\n\nstatic struct ceph_osd_backoff *lookup_containing_backoff(struct rb_root *root,\n\t\t\t\t\t    const struct ceph_hobject_id *hoid)\n{\n\tstruct rb_node *n = root->rb_node;\n\n\twhile (n) {\n\t\tstruct ceph_osd_backoff *cur =\n\t\t    rb_entry(n, struct ceph_osd_backoff, spg_node);\n\t\tint cmp;\n\n\t\tcmp = hoid_compare(hoid, cur->begin);\n\t\tif (cmp < 0) {\n\t\t\tn = n->rb_left;\n\t\t} else if (cmp > 0) {\n\t\t\tif (hoid_compare(hoid, cur->end) < 0)\n\t\t\t\treturn cur;\n\n\t\t\tn = n->rb_right;\n\t\t} else {\n\t\t\treturn cur;\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\n \nDEFINE_RB_FUNCS(backoff_by_id, struct ceph_osd_backoff, id, id_node)\n\nstatic void clear_backoffs(struct ceph_osd *osd)\n{\n\twhile (!RB_EMPTY_ROOT(&osd->o_backoff_mappings)) {\n\t\tstruct ceph_spg_mapping *spg =\n\t\t    rb_entry(rb_first(&osd->o_backoff_mappings),\n\t\t\t     struct ceph_spg_mapping, node);\n\n\t\twhile (!RB_EMPTY_ROOT(&spg->backoffs)) {\n\t\t\tstruct ceph_osd_backoff *backoff =\n\t\t\t    rb_entry(rb_first(&spg->backoffs),\n\t\t\t\t     struct ceph_osd_backoff, spg_node);\n\n\t\t\terase_backoff(&spg->backoffs, backoff);\n\t\t\terase_backoff_by_id(&osd->o_backoffs_by_id, backoff);\n\t\t\tfree_backoff(backoff);\n\t\t}\n\t\terase_spg_mapping(&osd->o_backoff_mappings, spg);\n\t\tfree_spg_mapping(spg);\n\t}\n}\n\n \nstatic void hoid_fill_from_target(struct ceph_hobject_id *hoid,\n\t\t\t\t  const struct ceph_osd_request_target *t)\n{\n\thoid->key = NULL;\n\thoid->key_len = 0;\n\thoid->oid = t->target_oid.name;\n\thoid->oid_len = t->target_oid.name_len;\n\thoid->snapid = CEPH_NOSNAP;\n\thoid->hash = t->pgid.seed;\n\thoid->is_max = false;\n\tif (t->target_oloc.pool_ns) {\n\t\thoid->nspace = t->target_oloc.pool_ns->str;\n\t\thoid->nspace_len = t->target_oloc.pool_ns->len;\n\t} else {\n\t\thoid->nspace = NULL;\n\t\thoid->nspace_len = 0;\n\t}\n\thoid->pool = t->target_oloc.pool;\n\tceph_hoid_build_hash_cache(hoid);\n}\n\nstatic bool should_plug_request(struct ceph_osd_request *req)\n{\n\tstruct ceph_osd *osd = req->r_osd;\n\tstruct ceph_spg_mapping *spg;\n\tstruct ceph_osd_backoff *backoff;\n\tstruct ceph_hobject_id hoid;\n\n\tspg = lookup_spg_mapping(&osd->o_backoff_mappings, &req->r_t.spgid);\n\tif (!spg)\n\t\treturn false;\n\n\thoid_fill_from_target(&hoid, &req->r_t);\n\tbackoff = lookup_containing_backoff(&spg->backoffs, &hoid);\n\tif (!backoff)\n\t\treturn false;\n\n\tdout(\"%s req %p tid %llu backoff osd%d spgid %llu.%xs%d id %llu\\n\",\n\t     __func__, req, req->r_tid, osd->o_osd, backoff->spgid.pgid.pool,\n\t     backoff->spgid.pgid.seed, backoff->spgid.shard, backoff->id);\n\treturn true;\n}\n\n \nstatic void setup_request_data(struct ceph_osd_request *req)\n{\n\tstruct ceph_msg *request_msg = req->r_request;\n\tstruct ceph_msg *reply_msg = req->r_reply;\n\tstruct ceph_osd_req_op *op;\n\n\tif (req->r_request->num_data_items || req->r_reply->num_data_items)\n\t\treturn;\n\n\tWARN_ON(request_msg->data_length || reply_msg->data_length);\n\tfor (op = req->r_ops; op != &req->r_ops[req->r_num_ops]; op++) {\n\t\tswitch (op->op) {\n\t\t \n\t\tcase CEPH_OSD_OP_WRITE:\n\t\tcase CEPH_OSD_OP_WRITEFULL:\n\t\t\tWARN_ON(op->indata_len != op->extent.length);\n\t\t\tceph_osdc_msg_data_add(request_msg,\n\t\t\t\t\t       &op->extent.osd_data);\n\t\t\tbreak;\n\t\tcase CEPH_OSD_OP_SETXATTR:\n\t\tcase CEPH_OSD_OP_CMPXATTR:\n\t\t\tWARN_ON(op->indata_len != op->xattr.name_len +\n\t\t\t\t\t\t  op->xattr.value_len);\n\t\t\tceph_osdc_msg_data_add(request_msg,\n\t\t\t\t\t       &op->xattr.osd_data);\n\t\t\tbreak;\n\t\tcase CEPH_OSD_OP_NOTIFY_ACK:\n\t\t\tceph_osdc_msg_data_add(request_msg,\n\t\t\t\t\t       &op->notify_ack.request_data);\n\t\t\tbreak;\n\t\tcase CEPH_OSD_OP_COPY_FROM2:\n\t\t\tceph_osdc_msg_data_add(request_msg,\n\t\t\t\t\t       &op->copy_from.osd_data);\n\t\t\tbreak;\n\n\t\t \n\t\tcase CEPH_OSD_OP_STAT:\n\t\t\tceph_osdc_msg_data_add(reply_msg,\n\t\t\t\t\t       &op->raw_data_in);\n\t\t\tbreak;\n\t\tcase CEPH_OSD_OP_READ:\n\t\tcase CEPH_OSD_OP_SPARSE_READ:\n\t\t\tceph_osdc_msg_data_add(reply_msg,\n\t\t\t\t\t       &op->extent.osd_data);\n\t\t\tbreak;\n\t\tcase CEPH_OSD_OP_LIST_WATCHERS:\n\t\t\tceph_osdc_msg_data_add(reply_msg,\n\t\t\t\t\t       &op->list_watchers.response_data);\n\t\t\tbreak;\n\n\t\t \n\t\tcase CEPH_OSD_OP_CALL:\n\t\t\tWARN_ON(op->indata_len != op->cls.class_len +\n\t\t\t\t\t\t  op->cls.method_len +\n\t\t\t\t\t\t  op->cls.indata_len);\n\t\t\tceph_osdc_msg_data_add(request_msg,\n\t\t\t\t\t       &op->cls.request_info);\n\t\t\t \n\t\t\tceph_osdc_msg_data_add(request_msg,\n\t\t\t\t\t       &op->cls.request_data);\n\t\t\t \n\t\t\tceph_osdc_msg_data_add(reply_msg,\n\t\t\t\t\t       &op->cls.response_data);\n\t\t\tbreak;\n\t\tcase CEPH_OSD_OP_NOTIFY:\n\t\t\tceph_osdc_msg_data_add(request_msg,\n\t\t\t\t\t       &op->notify.request_data);\n\t\t\tceph_osdc_msg_data_add(reply_msg,\n\t\t\t\t\t       &op->notify.response_data);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic void encode_pgid(void **p, const struct ceph_pg *pgid)\n{\n\tceph_encode_8(p, 1);\n\tceph_encode_64(p, pgid->pool);\n\tceph_encode_32(p, pgid->seed);\n\tceph_encode_32(p, -1);  \n}\n\nstatic void encode_spgid(void **p, const struct ceph_spg *spgid)\n{\n\tceph_start_encoding(p, 1, 1, CEPH_PGID_ENCODING_LEN + 1);\n\tencode_pgid(p, &spgid->pgid);\n\tceph_encode_8(p, spgid->shard);\n}\n\nstatic void encode_oloc(void **p, void *end,\n\t\t\tconst struct ceph_object_locator *oloc)\n{\n\tceph_start_encoding(p, 5, 4, ceph_oloc_encoding_size(oloc));\n\tceph_encode_64(p, oloc->pool);\n\tceph_encode_32(p, -1);  \n\tceph_encode_32(p, 0);   \n\tif (oloc->pool_ns)\n\t\tceph_encode_string(p, end, oloc->pool_ns->str,\n\t\t\t\t   oloc->pool_ns->len);\n\telse\n\t\tceph_encode_32(p, 0);\n}\n\nstatic void encode_request_partial(struct ceph_osd_request *req,\n\t\t\t\t   struct ceph_msg *msg)\n{\n\tvoid *p = msg->front.iov_base;\n\tvoid *const end = p + msg->front_alloc_len;\n\tu32 data_len = 0;\n\tint i;\n\n\tif (req->r_flags & CEPH_OSD_FLAG_WRITE) {\n\t\t \n\t\tWARN_ON(req->r_snapid != CEPH_NOSNAP);\n\t} else {\n\t\tWARN_ON(req->r_mtime.tv_sec || req->r_mtime.tv_nsec ||\n\t\t\treq->r_data_offset || req->r_snapc);\n\t}\n\n\tsetup_request_data(req);\n\n\tencode_spgid(&p, &req->r_t.spgid);  \n\tceph_encode_32(&p, req->r_t.pgid.seed);  \n\tceph_encode_32(&p, req->r_osdc->osdmap->epoch);\n\tceph_encode_32(&p, req->r_flags);\n\n\t \n\tceph_start_encoding(&p, 2, 2, sizeof(struct ceph_osd_reqid));\n\tmemset(p, 0, sizeof(struct ceph_osd_reqid));\n\tp += sizeof(struct ceph_osd_reqid);\n\n\t \n\tmemset(p, 0, sizeof(struct ceph_blkin_trace_info));\n\tp += sizeof(struct ceph_blkin_trace_info);\n\n\tceph_encode_32(&p, 0);  \n\tceph_encode_timespec64(p, &req->r_mtime);\n\tp += sizeof(struct ceph_timespec);\n\n\tencode_oloc(&p, end, &req->r_t.target_oloc);\n\tceph_encode_string(&p, end, req->r_t.target_oid.name,\n\t\t\t   req->r_t.target_oid.name_len);\n\n\t \n\tceph_encode_16(&p, req->r_num_ops);\n\tfor (i = 0; i < req->r_num_ops; i++) {\n\t\tdata_len += osd_req_encode_op(p, &req->r_ops[i]);\n\t\tp += sizeof(struct ceph_osd_op);\n\t}\n\n\tceph_encode_64(&p, req->r_snapid);  \n\tif (req->r_snapc) {\n\t\tceph_encode_64(&p, req->r_snapc->seq);\n\t\tceph_encode_32(&p, req->r_snapc->num_snaps);\n\t\tfor (i = 0; i < req->r_snapc->num_snaps; i++)\n\t\t\tceph_encode_64(&p, req->r_snapc->snaps[i]);\n\t} else {\n\t\tceph_encode_64(&p, 0);  \n\t\tceph_encode_32(&p, 0);  \n\t}\n\n\tceph_encode_32(&p, req->r_attempts);  \n\tBUG_ON(p > end - 8);  \n\n\tmsg->hdr.version = cpu_to_le16(8);  \n\t \n\tmsg->front.iov_len = p - msg->front.iov_base;\n\tmsg->hdr.front_len = cpu_to_le32(msg->front.iov_len);\n\tmsg->hdr.data_len = cpu_to_le32(data_len);\n\t \n\tmsg->hdr.data_off = cpu_to_le16(req->r_data_offset);\n\n\tdout(\"%s req %p msg %p oid %s oid_len %d\\n\", __func__, req, msg,\n\t     req->r_t.target_oid.name, req->r_t.target_oid.name_len);\n}\n\nstatic void encode_request_finish(struct ceph_msg *msg)\n{\n\tvoid *p = msg->front.iov_base;\n\tvoid *const partial_end = p + msg->front.iov_len;\n\tvoid *const end = p + msg->front_alloc_len;\n\n\tif (CEPH_HAVE_FEATURE(msg->con->peer_features, RESEND_ON_SPLIT)) {\n\t\t \n\t\tp = partial_end;\n\t\tceph_encode_64(&p, msg->con->peer_features);\n\t} else {\n\t\tstruct {\n\t\t\tchar spgid[CEPH_ENCODING_START_BLK_LEN +\n\t\t\t\t   CEPH_PGID_ENCODING_LEN + 1];\n\t\t\t__le32 hash;\n\t\t\t__le32 epoch;\n\t\t\t__le32 flags;\n\t\t\tchar reqid[CEPH_ENCODING_START_BLK_LEN +\n\t\t\t\t   sizeof(struct ceph_osd_reqid)];\n\t\t\tchar trace[sizeof(struct ceph_blkin_trace_info)];\n\t\t\t__le32 client_inc;\n\t\t\tstruct ceph_timespec mtime;\n\t\t} __packed head;\n\t\tstruct ceph_pg pgid;\n\t\tvoid *oloc, *oid, *tail;\n\t\tint oloc_len, oid_len, tail_len;\n\t\tint len;\n\n\t\t \n\t\tmemcpy(&head, p, sizeof(head));\n\t\tp += sizeof(head);\n\n\t\toloc = p;\n\t\tp += CEPH_ENCODING_START_BLK_LEN;\n\t\tpgid.pool = ceph_decode_64(&p);\n\t\tp += 4 + 4;  \n\t\tlen = ceph_decode_32(&p);\n\t\tp += len;    \n\t\toloc_len = p - oloc;\n\n\t\toid = p;\n\t\tlen = ceph_decode_32(&p);\n\t\tp += len;\n\t\toid_len = p - oid;\n\n\t\ttail = p;\n\t\ttail_len = partial_end - p;\n\n\t\tp = msg->front.iov_base;\n\t\tceph_encode_copy(&p, &head.client_inc, sizeof(head.client_inc));\n\t\tceph_encode_copy(&p, &head.epoch, sizeof(head.epoch));\n\t\tceph_encode_copy(&p, &head.flags, sizeof(head.flags));\n\t\tceph_encode_copy(&p, &head.mtime, sizeof(head.mtime));\n\n\t\t \n\t\tmemset(p, 0, sizeof(struct ceph_eversion));\n\t\tp += sizeof(struct ceph_eversion);\n\n\t\tBUG_ON(p >= oloc);\n\t\tmemmove(p, oloc, oloc_len);\n\t\tp += oloc_len;\n\n\t\tpgid.seed = le32_to_cpu(head.hash);\n\t\tencode_pgid(&p, &pgid);  \n\n\t\tBUG_ON(p >= oid);\n\t\tmemmove(p, oid, oid_len);\n\t\tp += oid_len;\n\n\t\t \n\t\tBUG_ON(p >= tail);\n\t\tmemmove(p, tail, tail_len);\n\t\tp += tail_len;\n\n\t\tmsg->hdr.version = cpu_to_le16(4);  \n\t}\n\n\tBUG_ON(p > end);\n\tmsg->front.iov_len = p - msg->front.iov_base;\n\tmsg->hdr.front_len = cpu_to_le32(msg->front.iov_len);\n\n\tdout(\"%s msg %p tid %llu %u+%u+%u v%d\\n\", __func__, msg,\n\t     le64_to_cpu(msg->hdr.tid), le32_to_cpu(msg->hdr.front_len),\n\t     le32_to_cpu(msg->hdr.middle_len), le32_to_cpu(msg->hdr.data_len),\n\t     le16_to_cpu(msg->hdr.version));\n}\n\n \nstatic void send_request(struct ceph_osd_request *req)\n{\n\tstruct ceph_osd *osd = req->r_osd;\n\n\tverify_osd_locked(osd);\n\tWARN_ON(osd->o_osd != req->r_t.osd);\n\n\t \n\tif (should_plug_request(req))\n\t\treturn;\n\n\t \n\tif (req->r_sent)\n\t\tceph_msg_revoke(req->r_request);\n\n\treq->r_flags |= CEPH_OSD_FLAG_KNOWN_REDIR;\n\tif (req->r_attempts)\n\t\treq->r_flags |= CEPH_OSD_FLAG_RETRY;\n\telse\n\t\tWARN_ON(req->r_flags & CEPH_OSD_FLAG_RETRY);\n\n\tencode_request_partial(req, req->r_request);\n\n\tdout(\"%s req %p tid %llu to pgid %llu.%x spgid %llu.%xs%d osd%d e%u flags 0x%x attempt %d\\n\",\n\t     __func__, req, req->r_tid, req->r_t.pgid.pool, req->r_t.pgid.seed,\n\t     req->r_t.spgid.pgid.pool, req->r_t.spgid.pgid.seed,\n\t     req->r_t.spgid.shard, osd->o_osd, req->r_t.epoch, req->r_flags,\n\t     req->r_attempts);\n\n\treq->r_t.paused = false;\n\treq->r_stamp = jiffies;\n\treq->r_attempts++;\n\n\treq->r_sent = osd->o_incarnation;\n\treq->r_request->hdr.tid = cpu_to_le64(req->r_tid);\n\tceph_con_send(&osd->o_con, ceph_msg_get(req->r_request));\n}\n\nstatic void maybe_request_map(struct ceph_osd_client *osdc)\n{\n\tbool continuous = false;\n\n\tverify_osdc_locked(osdc);\n\tWARN_ON(!osdc->osdmap->epoch);\n\n\tif (ceph_osdmap_flag(osdc, CEPH_OSDMAP_FULL) ||\n\t    ceph_osdmap_flag(osdc, CEPH_OSDMAP_PAUSERD) ||\n\t    ceph_osdmap_flag(osdc, CEPH_OSDMAP_PAUSEWR)) {\n\t\tdout(\"%s osdc %p continuous\\n\", __func__, osdc);\n\t\tcontinuous = true;\n\t} else {\n\t\tdout(\"%s osdc %p onetime\\n\", __func__, osdc);\n\t}\n\n\tif (ceph_monc_want_map(&osdc->client->monc, CEPH_SUB_OSDMAP,\n\t\t\t       osdc->osdmap->epoch + 1, continuous))\n\t\tceph_monc_renew_subs(&osdc->client->monc);\n}\n\nstatic void complete_request(struct ceph_osd_request *req, int err);\nstatic void send_map_check(struct ceph_osd_request *req);\n\nstatic void __submit_request(struct ceph_osd_request *req, bool wrlocked)\n{\n\tstruct ceph_osd_client *osdc = req->r_osdc;\n\tstruct ceph_osd *osd;\n\tenum calc_target_result ct_res;\n\tint err = 0;\n\tbool need_send = false;\n\tbool promoted = false;\n\n\tWARN_ON(req->r_tid);\n\tdout(\"%s req %p wrlocked %d\\n\", __func__, req, wrlocked);\n\nagain:\n\tct_res = calc_target(osdc, &req->r_t, false);\n\tif (ct_res == CALC_TARGET_POOL_DNE && !wrlocked)\n\t\tgoto promote;\n\n\tosd = lookup_create_osd(osdc, req->r_t.osd, wrlocked);\n\tif (IS_ERR(osd)) {\n\t\tWARN_ON(PTR_ERR(osd) != -EAGAIN || wrlocked);\n\t\tgoto promote;\n\t}\n\n\tif (osdc->abort_err) {\n\t\tdout(\"req %p abort_err %d\\n\", req, osdc->abort_err);\n\t\terr = osdc->abort_err;\n\t} else if (osdc->osdmap->epoch < osdc->epoch_barrier) {\n\t\tdout(\"req %p epoch %u barrier %u\\n\", req, osdc->osdmap->epoch,\n\t\t     osdc->epoch_barrier);\n\t\treq->r_t.paused = true;\n\t\tmaybe_request_map(osdc);\n\t} else if ((req->r_flags & CEPH_OSD_FLAG_WRITE) &&\n\t\t   ceph_osdmap_flag(osdc, CEPH_OSDMAP_PAUSEWR)) {\n\t\tdout(\"req %p pausewr\\n\", req);\n\t\treq->r_t.paused = true;\n\t\tmaybe_request_map(osdc);\n\t} else if ((req->r_flags & CEPH_OSD_FLAG_READ) &&\n\t\t   ceph_osdmap_flag(osdc, CEPH_OSDMAP_PAUSERD)) {\n\t\tdout(\"req %p pauserd\\n\", req);\n\t\treq->r_t.paused = true;\n\t\tmaybe_request_map(osdc);\n\t} else if ((req->r_flags & CEPH_OSD_FLAG_WRITE) &&\n\t\t   !(req->r_flags & (CEPH_OSD_FLAG_FULL_TRY |\n\t\t\t\t     CEPH_OSD_FLAG_FULL_FORCE)) &&\n\t\t   (ceph_osdmap_flag(osdc, CEPH_OSDMAP_FULL) ||\n\t\t    pool_full(osdc, req->r_t.base_oloc.pool))) {\n\t\tdout(\"req %p full/pool_full\\n\", req);\n\t\tif (ceph_test_opt(osdc->client, ABORT_ON_FULL)) {\n\t\t\terr = -ENOSPC;\n\t\t} else {\n\t\t\tif (ceph_osdmap_flag(osdc, CEPH_OSDMAP_FULL))\n\t\t\t\tpr_warn_ratelimited(\"cluster is full (osdmap FULL)\\n\");\n\t\t\telse\n\t\t\t\tpr_warn_ratelimited(\"pool %lld is full or reached quota\\n\",\n\t\t\t\t\t\t    req->r_t.base_oloc.pool);\n\t\t\treq->r_t.paused = true;\n\t\t\tmaybe_request_map(osdc);\n\t\t}\n\t} else if (!osd_homeless(osd)) {\n\t\tneed_send = true;\n\t} else {\n\t\tmaybe_request_map(osdc);\n\t}\n\n\tmutex_lock(&osd->lock);\n\t \n\treq->r_tid = atomic64_inc_return(&osdc->last_tid);\n\tlink_request(osd, req);\n\tif (need_send)\n\t\tsend_request(req);\n\telse if (err)\n\t\tcomplete_request(req, err);\n\tmutex_unlock(&osd->lock);\n\n\tif (!err && ct_res == CALC_TARGET_POOL_DNE)\n\t\tsend_map_check(req);\n\n\tif (promoted)\n\t\tdowngrade_write(&osdc->lock);\n\treturn;\n\npromote:\n\tup_read(&osdc->lock);\n\tdown_write(&osdc->lock);\n\twrlocked = true;\n\tpromoted = true;\n\tgoto again;\n}\n\nstatic void account_request(struct ceph_osd_request *req)\n{\n\tWARN_ON(req->r_flags & (CEPH_OSD_FLAG_ACK | CEPH_OSD_FLAG_ONDISK));\n\tWARN_ON(!(req->r_flags & (CEPH_OSD_FLAG_READ | CEPH_OSD_FLAG_WRITE)));\n\n\treq->r_flags |= CEPH_OSD_FLAG_ONDISK;\n\tatomic_inc(&req->r_osdc->num_requests);\n\n\treq->r_start_stamp = jiffies;\n\treq->r_start_latency = ktime_get();\n}\n\nstatic void submit_request(struct ceph_osd_request *req, bool wrlocked)\n{\n\tceph_osdc_get_request(req);\n\taccount_request(req);\n\t__submit_request(req, wrlocked);\n}\n\nstatic void finish_request(struct ceph_osd_request *req)\n{\n\tstruct ceph_osd_client *osdc = req->r_osdc;\n\n\tWARN_ON(lookup_request_mc(&osdc->map_checks, req->r_tid));\n\tdout(\"%s req %p tid %llu\\n\", __func__, req, req->r_tid);\n\n\treq->r_end_latency = ktime_get();\n\n\tif (req->r_osd) {\n\t\tceph_init_sparse_read(&req->r_osd->o_sparse_read);\n\t\tunlink_request(req->r_osd, req);\n\t}\n\tatomic_dec(&osdc->num_requests);\n\n\t \n\tceph_msg_revoke(req->r_request);\n\tceph_msg_revoke_incoming(req->r_reply);\n}\n\nstatic void __complete_request(struct ceph_osd_request *req)\n{\n\tdout(\"%s req %p tid %llu cb %ps result %d\\n\", __func__, req,\n\t     req->r_tid, req->r_callback, req->r_result);\n\n\tif (req->r_callback)\n\t\treq->r_callback(req);\n\tcomplete_all(&req->r_completion);\n\tceph_osdc_put_request(req);\n}\n\nstatic void complete_request_workfn(struct work_struct *work)\n{\n\tstruct ceph_osd_request *req =\n\t    container_of(work, struct ceph_osd_request, r_complete_work);\n\n\t__complete_request(req);\n}\n\n \nstatic void complete_request(struct ceph_osd_request *req, int err)\n{\n\tdout(\"%s req %p tid %llu err %d\\n\", __func__, req, req->r_tid, err);\n\n\treq->r_result = err;\n\tfinish_request(req);\n\n\tINIT_WORK(&req->r_complete_work, complete_request_workfn);\n\tqueue_work(req->r_osdc->completion_wq, &req->r_complete_work);\n}\n\nstatic void cancel_map_check(struct ceph_osd_request *req)\n{\n\tstruct ceph_osd_client *osdc = req->r_osdc;\n\tstruct ceph_osd_request *lookup_req;\n\n\tverify_osdc_wrlocked(osdc);\n\n\tlookup_req = lookup_request_mc(&osdc->map_checks, req->r_tid);\n\tif (!lookup_req)\n\t\treturn;\n\n\tWARN_ON(lookup_req != req);\n\terase_request_mc(&osdc->map_checks, req);\n\tceph_osdc_put_request(req);\n}\n\nstatic void cancel_request(struct ceph_osd_request *req)\n{\n\tdout(\"%s req %p tid %llu\\n\", __func__, req, req->r_tid);\n\n\tcancel_map_check(req);\n\tfinish_request(req);\n\tcomplete_all(&req->r_completion);\n\tceph_osdc_put_request(req);\n}\n\nstatic void abort_request(struct ceph_osd_request *req, int err)\n{\n\tdout(\"%s req %p tid %llu err %d\\n\", __func__, req, req->r_tid, err);\n\n\tcancel_map_check(req);\n\tcomplete_request(req, err);\n}\n\nstatic int abort_fn(struct ceph_osd_request *req, void *arg)\n{\n\tint err = *(int *)arg;\n\n\tabort_request(req, err);\n\treturn 0;  \n}\n\n \nvoid ceph_osdc_abort_requests(struct ceph_osd_client *osdc, int err)\n{\n\tdout(\"%s osdc %p err %d\\n\", __func__, osdc, err);\n\tdown_write(&osdc->lock);\n\tfor_each_request(osdc, abort_fn, &err);\n\tosdc->abort_err = err;\n\tup_write(&osdc->lock);\n}\nEXPORT_SYMBOL(ceph_osdc_abort_requests);\n\nvoid ceph_osdc_clear_abort_err(struct ceph_osd_client *osdc)\n{\n\tdown_write(&osdc->lock);\n\tosdc->abort_err = 0;\n\tup_write(&osdc->lock);\n}\nEXPORT_SYMBOL(ceph_osdc_clear_abort_err);\n\nstatic void update_epoch_barrier(struct ceph_osd_client *osdc, u32 eb)\n{\n\tif (likely(eb > osdc->epoch_barrier)) {\n\t\tdout(\"updating epoch_barrier from %u to %u\\n\",\n\t\t\t\tosdc->epoch_barrier, eb);\n\t\tosdc->epoch_barrier = eb;\n\t\t \n\t\tif (eb > osdc->osdmap->epoch)\n\t\t\tmaybe_request_map(osdc);\n\t}\n}\n\nvoid ceph_osdc_update_epoch_barrier(struct ceph_osd_client *osdc, u32 eb)\n{\n\tdown_read(&osdc->lock);\n\tif (unlikely(eb > osdc->epoch_barrier)) {\n\t\tup_read(&osdc->lock);\n\t\tdown_write(&osdc->lock);\n\t\tupdate_epoch_barrier(osdc, eb);\n\t\tup_write(&osdc->lock);\n\t} else {\n\t\tup_read(&osdc->lock);\n\t}\n}\nEXPORT_SYMBOL(ceph_osdc_update_epoch_barrier);\n\n \nstatic int abort_on_full_fn(struct ceph_osd_request *req, void *arg)\n{\n\tstruct ceph_osd_client *osdc = req->r_osdc;\n\tbool *victims = arg;\n\n\tif ((req->r_flags & CEPH_OSD_FLAG_WRITE) &&\n\t    (ceph_osdmap_flag(osdc, CEPH_OSDMAP_FULL) ||\n\t     pool_full(osdc, req->r_t.base_oloc.pool))) {\n\t\tif (!*victims) {\n\t\t\tupdate_epoch_barrier(osdc, osdc->osdmap->epoch);\n\t\t\t*victims = true;\n\t\t}\n\t\tabort_request(req, -ENOSPC);\n\t}\n\n\treturn 0;  \n}\n\n \nstatic void ceph_osdc_abort_on_full(struct ceph_osd_client *osdc)\n{\n\tbool victims = false;\n\n\tif (ceph_test_opt(osdc->client, ABORT_ON_FULL) &&\n\t    (ceph_osdmap_flag(osdc, CEPH_OSDMAP_FULL) || have_pool_full(osdc)))\n\t\tfor_each_request(osdc, abort_on_full_fn, &victims);\n}\n\nstatic void check_pool_dne(struct ceph_osd_request *req)\n{\n\tstruct ceph_osd_client *osdc = req->r_osdc;\n\tstruct ceph_osdmap *map = osdc->osdmap;\n\n\tverify_osdc_wrlocked(osdc);\n\tWARN_ON(!map->epoch);\n\n\tif (req->r_attempts) {\n\t\t \n\t\treq->r_map_dne_bound = map->epoch;\n\t\tdout(\"%s req %p tid %llu pool disappeared\\n\", __func__, req,\n\t\t     req->r_tid);\n\t} else {\n\t\tdout(\"%s req %p tid %llu map_dne_bound %u have %u\\n\", __func__,\n\t\t     req, req->r_tid, req->r_map_dne_bound, map->epoch);\n\t}\n\n\tif (req->r_map_dne_bound) {\n\t\tif (map->epoch >= req->r_map_dne_bound) {\n\t\t\t \n\t\t\tpr_info_ratelimited(\"tid %llu pool does not exist\\n\",\n\t\t\t\t\t    req->r_tid);\n\t\t\tcomplete_request(req, -ENOENT);\n\t\t}\n\t} else {\n\t\tsend_map_check(req);\n\t}\n}\n\nstatic void map_check_cb(struct ceph_mon_generic_request *greq)\n{\n\tstruct ceph_osd_client *osdc = &greq->monc->client->osdc;\n\tstruct ceph_osd_request *req;\n\tu64 tid = greq->private_data;\n\n\tWARN_ON(greq->result || !greq->u.newest);\n\n\tdown_write(&osdc->lock);\n\treq = lookup_request_mc(&osdc->map_checks, tid);\n\tif (!req) {\n\t\tdout(\"%s tid %llu dne\\n\", __func__, tid);\n\t\tgoto out_unlock;\n\t}\n\n\tdout(\"%s req %p tid %llu map_dne_bound %u newest %llu\\n\", __func__,\n\t     req, req->r_tid, req->r_map_dne_bound, greq->u.newest);\n\tif (!req->r_map_dne_bound)\n\t\treq->r_map_dne_bound = greq->u.newest;\n\terase_request_mc(&osdc->map_checks, req);\n\tcheck_pool_dne(req);\n\n\tceph_osdc_put_request(req);\nout_unlock:\n\tup_write(&osdc->lock);\n}\n\nstatic void send_map_check(struct ceph_osd_request *req)\n{\n\tstruct ceph_osd_client *osdc = req->r_osdc;\n\tstruct ceph_osd_request *lookup_req;\n\tint ret;\n\n\tverify_osdc_wrlocked(osdc);\n\n\tlookup_req = lookup_request_mc(&osdc->map_checks, req->r_tid);\n\tif (lookup_req) {\n\t\tWARN_ON(lookup_req != req);\n\t\treturn;\n\t}\n\n\tceph_osdc_get_request(req);\n\tinsert_request_mc(&osdc->map_checks, req);\n\tret = ceph_monc_get_version_async(&osdc->client->monc, \"osdmap\",\n\t\t\t\t\t  map_check_cb, req->r_tid);\n\tWARN_ON(ret);\n}\n\n \nstatic void linger_release(struct kref *kref)\n{\n\tstruct ceph_osd_linger_request *lreq =\n\t    container_of(kref, struct ceph_osd_linger_request, kref);\n\n\tdout(\"%s lreq %p reg_req %p ping_req %p\\n\", __func__, lreq,\n\t     lreq->reg_req, lreq->ping_req);\n\tWARN_ON(!RB_EMPTY_NODE(&lreq->node));\n\tWARN_ON(!RB_EMPTY_NODE(&lreq->osdc_node));\n\tWARN_ON(!RB_EMPTY_NODE(&lreq->mc_node));\n\tWARN_ON(!list_empty(&lreq->scan_item));\n\tWARN_ON(!list_empty(&lreq->pending_lworks));\n\tWARN_ON(lreq->osd);\n\n\tif (lreq->request_pl)\n\t\tceph_pagelist_release(lreq->request_pl);\n\tif (lreq->notify_id_pages)\n\t\tceph_release_page_vector(lreq->notify_id_pages, 1);\n\n\tceph_osdc_put_request(lreq->reg_req);\n\tceph_osdc_put_request(lreq->ping_req);\n\ttarget_destroy(&lreq->t);\n\tkfree(lreq);\n}\n\nstatic void linger_put(struct ceph_osd_linger_request *lreq)\n{\n\tif (lreq)\n\t\tkref_put(&lreq->kref, linger_release);\n}\n\nstatic struct ceph_osd_linger_request *\nlinger_get(struct ceph_osd_linger_request *lreq)\n{\n\tkref_get(&lreq->kref);\n\treturn lreq;\n}\n\nstatic struct ceph_osd_linger_request *\nlinger_alloc(struct ceph_osd_client *osdc)\n{\n\tstruct ceph_osd_linger_request *lreq;\n\n\tlreq = kzalloc(sizeof(*lreq), GFP_NOIO);\n\tif (!lreq)\n\t\treturn NULL;\n\n\tkref_init(&lreq->kref);\n\tmutex_init(&lreq->lock);\n\tRB_CLEAR_NODE(&lreq->node);\n\tRB_CLEAR_NODE(&lreq->osdc_node);\n\tRB_CLEAR_NODE(&lreq->mc_node);\n\tINIT_LIST_HEAD(&lreq->scan_item);\n\tINIT_LIST_HEAD(&lreq->pending_lworks);\n\tinit_completion(&lreq->reg_commit_wait);\n\tinit_completion(&lreq->notify_finish_wait);\n\n\tlreq->osdc = osdc;\n\ttarget_init(&lreq->t);\n\n\tdout(\"%s lreq %p\\n\", __func__, lreq);\n\treturn lreq;\n}\n\nDEFINE_RB_INSDEL_FUNCS(linger, struct ceph_osd_linger_request, linger_id, node)\nDEFINE_RB_FUNCS(linger_osdc, struct ceph_osd_linger_request, linger_id, osdc_node)\nDEFINE_RB_FUNCS(linger_mc, struct ceph_osd_linger_request, linger_id, mc_node)\n\n \nstatic void link_linger(struct ceph_osd *osd,\n\t\t\tstruct ceph_osd_linger_request *lreq)\n{\n\tverify_osd_locked(osd);\n\tWARN_ON(!lreq->linger_id || lreq->osd);\n\tdout(\"%s osd %p osd%d lreq %p linger_id %llu\\n\", __func__, osd,\n\t     osd->o_osd, lreq, lreq->linger_id);\n\n\tif (!osd_homeless(osd))\n\t\t__remove_osd_from_lru(osd);\n\telse\n\t\tatomic_inc(&osd->o_osdc->num_homeless);\n\n\tget_osd(osd);\n\tinsert_linger(&osd->o_linger_requests, lreq);\n\tlreq->osd = osd;\n}\n\nstatic void unlink_linger(struct ceph_osd *osd,\n\t\t\t  struct ceph_osd_linger_request *lreq)\n{\n\tverify_osd_locked(osd);\n\tWARN_ON(lreq->osd != osd);\n\tdout(\"%s osd %p osd%d lreq %p linger_id %llu\\n\", __func__, osd,\n\t     osd->o_osd, lreq, lreq->linger_id);\n\n\tlreq->osd = NULL;\n\terase_linger(&osd->o_linger_requests, lreq);\n\tput_osd(osd);\n\n\tif (!osd_homeless(osd))\n\t\tmaybe_move_osd_to_lru(osd);\n\telse\n\t\tatomic_dec(&osd->o_osdc->num_homeless);\n}\n\nstatic bool __linger_registered(struct ceph_osd_linger_request *lreq)\n{\n\tverify_osdc_locked(lreq->osdc);\n\n\treturn !RB_EMPTY_NODE(&lreq->osdc_node);\n}\n\nstatic bool linger_registered(struct ceph_osd_linger_request *lreq)\n{\n\tstruct ceph_osd_client *osdc = lreq->osdc;\n\tbool registered;\n\n\tdown_read(&osdc->lock);\n\tregistered = __linger_registered(lreq);\n\tup_read(&osdc->lock);\n\n\treturn registered;\n}\n\nstatic void linger_register(struct ceph_osd_linger_request *lreq)\n{\n\tstruct ceph_osd_client *osdc = lreq->osdc;\n\n\tverify_osdc_wrlocked(osdc);\n\tWARN_ON(lreq->linger_id);\n\n\tlinger_get(lreq);\n\tlreq->linger_id = ++osdc->last_linger_id;\n\tinsert_linger_osdc(&osdc->linger_requests, lreq);\n}\n\nstatic void linger_unregister(struct ceph_osd_linger_request *lreq)\n{\n\tstruct ceph_osd_client *osdc = lreq->osdc;\n\n\tverify_osdc_wrlocked(osdc);\n\n\terase_linger_osdc(&osdc->linger_requests, lreq);\n\tlinger_put(lreq);\n}\n\nstatic void cancel_linger_request(struct ceph_osd_request *req)\n{\n\tstruct ceph_osd_linger_request *lreq = req->r_priv;\n\n\tWARN_ON(!req->r_linger);\n\tcancel_request(req);\n\tlinger_put(lreq);\n}\n\nstruct linger_work {\n\tstruct work_struct work;\n\tstruct ceph_osd_linger_request *lreq;\n\tstruct list_head pending_item;\n\tunsigned long queued_stamp;\n\n\tunion {\n\t\tstruct {\n\t\t\tu64 notify_id;\n\t\t\tu64 notifier_id;\n\t\t\tvoid *payload;  \n\t\t\tsize_t payload_len;\n\n\t\t\tstruct ceph_msg *msg;  \n\t\t} notify;\n\t\tstruct {\n\t\t\tint err;\n\t\t} error;\n\t};\n};\n\nstatic struct linger_work *lwork_alloc(struct ceph_osd_linger_request *lreq,\n\t\t\t\t       work_func_t workfn)\n{\n\tstruct linger_work *lwork;\n\n\tlwork = kzalloc(sizeof(*lwork), GFP_NOIO);\n\tif (!lwork)\n\t\treturn NULL;\n\n\tINIT_WORK(&lwork->work, workfn);\n\tINIT_LIST_HEAD(&lwork->pending_item);\n\tlwork->lreq = linger_get(lreq);\n\n\treturn lwork;\n}\n\nstatic void lwork_free(struct linger_work *lwork)\n{\n\tstruct ceph_osd_linger_request *lreq = lwork->lreq;\n\n\tmutex_lock(&lreq->lock);\n\tlist_del(&lwork->pending_item);\n\tmutex_unlock(&lreq->lock);\n\n\tlinger_put(lreq);\n\tkfree(lwork);\n}\n\nstatic void lwork_queue(struct linger_work *lwork)\n{\n\tstruct ceph_osd_linger_request *lreq = lwork->lreq;\n\tstruct ceph_osd_client *osdc = lreq->osdc;\n\n\tverify_lreq_locked(lreq);\n\tWARN_ON(!list_empty(&lwork->pending_item));\n\n\tlwork->queued_stamp = jiffies;\n\tlist_add_tail(&lwork->pending_item, &lreq->pending_lworks);\n\tqueue_work(osdc->notify_wq, &lwork->work);\n}\n\nstatic void do_watch_notify(struct work_struct *w)\n{\n\tstruct linger_work *lwork = container_of(w, struct linger_work, work);\n\tstruct ceph_osd_linger_request *lreq = lwork->lreq;\n\n\tif (!linger_registered(lreq)) {\n\t\tdout(\"%s lreq %p not registered\\n\", __func__, lreq);\n\t\tgoto out;\n\t}\n\n\tWARN_ON(!lreq->is_watch);\n\tdout(\"%s lreq %p notify_id %llu notifier_id %llu payload_len %zu\\n\",\n\t     __func__, lreq, lwork->notify.notify_id, lwork->notify.notifier_id,\n\t     lwork->notify.payload_len);\n\tlreq->wcb(lreq->data, lwork->notify.notify_id, lreq->linger_id,\n\t\t  lwork->notify.notifier_id, lwork->notify.payload,\n\t\t  lwork->notify.payload_len);\n\nout:\n\tceph_msg_put(lwork->notify.msg);\n\tlwork_free(lwork);\n}\n\nstatic void do_watch_error(struct work_struct *w)\n{\n\tstruct linger_work *lwork = container_of(w, struct linger_work, work);\n\tstruct ceph_osd_linger_request *lreq = lwork->lreq;\n\n\tif (!linger_registered(lreq)) {\n\t\tdout(\"%s lreq %p not registered\\n\", __func__, lreq);\n\t\tgoto out;\n\t}\n\n\tdout(\"%s lreq %p err %d\\n\", __func__, lreq, lwork->error.err);\n\tlreq->errcb(lreq->data, lreq->linger_id, lwork->error.err);\n\nout:\n\tlwork_free(lwork);\n}\n\nstatic void queue_watch_error(struct ceph_osd_linger_request *lreq)\n{\n\tstruct linger_work *lwork;\n\n\tlwork = lwork_alloc(lreq, do_watch_error);\n\tif (!lwork) {\n\t\tpr_err(\"failed to allocate error-lwork\\n\");\n\t\treturn;\n\t}\n\n\tlwork->error.err = lreq->last_error;\n\tlwork_queue(lwork);\n}\n\nstatic void linger_reg_commit_complete(struct ceph_osd_linger_request *lreq,\n\t\t\t\t       int result)\n{\n\tif (!completion_done(&lreq->reg_commit_wait)) {\n\t\tlreq->reg_commit_error = (result <= 0 ? result : 0);\n\t\tcomplete_all(&lreq->reg_commit_wait);\n\t}\n}\n\nstatic void linger_commit_cb(struct ceph_osd_request *req)\n{\n\tstruct ceph_osd_linger_request *lreq = req->r_priv;\n\n\tmutex_lock(&lreq->lock);\n\tif (req != lreq->reg_req) {\n\t\tdout(\"%s lreq %p linger_id %llu unknown req (%p != %p)\\n\",\n\t\t     __func__, lreq, lreq->linger_id, req, lreq->reg_req);\n\t\tgoto out;\n\t}\n\n\tdout(\"%s lreq %p linger_id %llu result %d\\n\", __func__, lreq,\n\t     lreq->linger_id, req->r_result);\n\tlinger_reg_commit_complete(lreq, req->r_result);\n\tlreq->committed = true;\n\n\tif (!lreq->is_watch) {\n\t\tstruct ceph_osd_data *osd_data =\n\t\t    osd_req_op_data(req, 0, notify, response_data);\n\t\tvoid *p = page_address(osd_data->pages[0]);\n\n\t\tWARN_ON(req->r_ops[0].op != CEPH_OSD_OP_NOTIFY ||\n\t\t\tosd_data->type != CEPH_OSD_DATA_TYPE_PAGES);\n\n\t\t \n\t\tif (req->r_ops[0].outdata_len >= sizeof(u64)) {\n\t\t\tlreq->notify_id = ceph_decode_64(&p);\n\t\t\tdout(\"lreq %p notify_id %llu\\n\", lreq,\n\t\t\t     lreq->notify_id);\n\t\t} else {\n\t\t\tdout(\"lreq %p no notify_id\\n\", lreq);\n\t\t}\n\t}\n\nout:\n\tmutex_unlock(&lreq->lock);\n\tlinger_put(lreq);\n}\n\nstatic int normalize_watch_error(int err)\n{\n\t \n\tif (err == -ENOENT)\n\t\terr = -ENOTCONN;\n\n\treturn err;\n}\n\nstatic void linger_reconnect_cb(struct ceph_osd_request *req)\n{\n\tstruct ceph_osd_linger_request *lreq = req->r_priv;\n\n\tmutex_lock(&lreq->lock);\n\tif (req != lreq->reg_req) {\n\t\tdout(\"%s lreq %p linger_id %llu unknown req (%p != %p)\\n\",\n\t\t     __func__, lreq, lreq->linger_id, req, lreq->reg_req);\n\t\tgoto out;\n\t}\n\n\tdout(\"%s lreq %p linger_id %llu result %d last_error %d\\n\", __func__,\n\t     lreq, lreq->linger_id, req->r_result, lreq->last_error);\n\tif (req->r_result < 0) {\n\t\tif (!lreq->last_error) {\n\t\t\tlreq->last_error = normalize_watch_error(req->r_result);\n\t\t\tqueue_watch_error(lreq);\n\t\t}\n\t}\n\nout:\n\tmutex_unlock(&lreq->lock);\n\tlinger_put(lreq);\n}\n\nstatic void send_linger(struct ceph_osd_linger_request *lreq)\n{\n\tstruct ceph_osd_client *osdc = lreq->osdc;\n\tstruct ceph_osd_request *req;\n\tint ret;\n\n\tverify_osdc_wrlocked(osdc);\n\tmutex_lock(&lreq->lock);\n\tdout(\"%s lreq %p linger_id %llu\\n\", __func__, lreq, lreq->linger_id);\n\n\tif (lreq->reg_req) {\n\t\tif (lreq->reg_req->r_osd)\n\t\t\tcancel_linger_request(lreq->reg_req);\n\t\tceph_osdc_put_request(lreq->reg_req);\n\t}\n\n\treq = ceph_osdc_alloc_request(osdc, NULL, 1, true, GFP_NOIO);\n\tBUG_ON(!req);\n\n\ttarget_copy(&req->r_t, &lreq->t);\n\treq->r_mtime = lreq->mtime;\n\n\tif (lreq->is_watch && lreq->committed) {\n\t\tosd_req_op_watch_init(req, 0, CEPH_OSD_WATCH_OP_RECONNECT,\n\t\t\t\t      lreq->linger_id, ++lreq->register_gen);\n\t\tdout(\"lreq %p reconnect register_gen %u\\n\", lreq,\n\t\t     req->r_ops[0].watch.gen);\n\t\treq->r_callback = linger_reconnect_cb;\n\t} else {\n\t\tif (lreq->is_watch) {\n\t\t\tosd_req_op_watch_init(req, 0, CEPH_OSD_WATCH_OP_WATCH,\n\t\t\t\t\t      lreq->linger_id, 0);\n\t\t} else {\n\t\t\tlreq->notify_id = 0;\n\n\t\t\trefcount_inc(&lreq->request_pl->refcnt);\n\t\t\tosd_req_op_notify_init(req, 0, lreq->linger_id,\n\t\t\t\t\t       lreq->request_pl);\n\t\t\tceph_osd_data_pages_init(\n\t\t\t    osd_req_op_data(req, 0, notify, response_data),\n\t\t\t    lreq->notify_id_pages, PAGE_SIZE, 0, false, false);\n\t\t}\n\t\tdout(\"lreq %p register\\n\", lreq);\n\t\treq->r_callback = linger_commit_cb;\n\t}\n\n\tret = ceph_osdc_alloc_messages(req, GFP_NOIO);\n\tBUG_ON(ret);\n\n\treq->r_priv = linger_get(lreq);\n\treq->r_linger = true;\n\tlreq->reg_req = req;\n\tmutex_unlock(&lreq->lock);\n\n\tsubmit_request(req, true);\n}\n\nstatic void linger_ping_cb(struct ceph_osd_request *req)\n{\n\tstruct ceph_osd_linger_request *lreq = req->r_priv;\n\n\tmutex_lock(&lreq->lock);\n\tif (req != lreq->ping_req) {\n\t\tdout(\"%s lreq %p linger_id %llu unknown req (%p != %p)\\n\",\n\t\t     __func__, lreq, lreq->linger_id, req, lreq->ping_req);\n\t\tgoto out;\n\t}\n\n\tdout(\"%s lreq %p linger_id %llu result %d ping_sent %lu last_error %d\\n\",\n\t     __func__, lreq, lreq->linger_id, req->r_result, lreq->ping_sent,\n\t     lreq->last_error);\n\tif (lreq->register_gen == req->r_ops[0].watch.gen) {\n\t\tif (!req->r_result) {\n\t\t\tlreq->watch_valid_thru = lreq->ping_sent;\n\t\t} else if (!lreq->last_error) {\n\t\t\tlreq->last_error = normalize_watch_error(req->r_result);\n\t\t\tqueue_watch_error(lreq);\n\t\t}\n\t} else {\n\t\tdout(\"lreq %p register_gen %u ignoring old pong %u\\n\", lreq,\n\t\t     lreq->register_gen, req->r_ops[0].watch.gen);\n\t}\n\nout:\n\tmutex_unlock(&lreq->lock);\n\tlinger_put(lreq);\n}\n\nstatic void send_linger_ping(struct ceph_osd_linger_request *lreq)\n{\n\tstruct ceph_osd_client *osdc = lreq->osdc;\n\tstruct ceph_osd_request *req;\n\tint ret;\n\n\tif (ceph_osdmap_flag(osdc, CEPH_OSDMAP_PAUSERD)) {\n\t\tdout(\"%s PAUSERD\\n\", __func__);\n\t\treturn;\n\t}\n\n\tlreq->ping_sent = jiffies;\n\tdout(\"%s lreq %p linger_id %llu ping_sent %lu register_gen %u\\n\",\n\t     __func__, lreq, lreq->linger_id, lreq->ping_sent,\n\t     lreq->register_gen);\n\n\tif (lreq->ping_req) {\n\t\tif (lreq->ping_req->r_osd)\n\t\t\tcancel_linger_request(lreq->ping_req);\n\t\tceph_osdc_put_request(lreq->ping_req);\n\t}\n\n\treq = ceph_osdc_alloc_request(osdc, NULL, 1, true, GFP_NOIO);\n\tBUG_ON(!req);\n\n\ttarget_copy(&req->r_t, &lreq->t);\n\tosd_req_op_watch_init(req, 0, CEPH_OSD_WATCH_OP_PING, lreq->linger_id,\n\t\t\t      lreq->register_gen);\n\treq->r_callback = linger_ping_cb;\n\n\tret = ceph_osdc_alloc_messages(req, GFP_NOIO);\n\tBUG_ON(ret);\n\n\treq->r_priv = linger_get(lreq);\n\treq->r_linger = true;\n\tlreq->ping_req = req;\n\n\tceph_osdc_get_request(req);\n\taccount_request(req);\n\treq->r_tid = atomic64_inc_return(&osdc->last_tid);\n\tlink_request(lreq->osd, req);\n\tsend_request(req);\n}\n\nstatic void linger_submit(struct ceph_osd_linger_request *lreq)\n{\n\tstruct ceph_osd_client *osdc = lreq->osdc;\n\tstruct ceph_osd *osd;\n\n\tdown_write(&osdc->lock);\n\tlinger_register(lreq);\n\n\tcalc_target(osdc, &lreq->t, false);\n\tosd = lookup_create_osd(osdc, lreq->t.osd, true);\n\tlink_linger(osd, lreq);\n\n\tsend_linger(lreq);\n\tup_write(&osdc->lock);\n}\n\nstatic void cancel_linger_map_check(struct ceph_osd_linger_request *lreq)\n{\n\tstruct ceph_osd_client *osdc = lreq->osdc;\n\tstruct ceph_osd_linger_request *lookup_lreq;\n\n\tverify_osdc_wrlocked(osdc);\n\n\tlookup_lreq = lookup_linger_mc(&osdc->linger_map_checks,\n\t\t\t\t       lreq->linger_id);\n\tif (!lookup_lreq)\n\t\treturn;\n\n\tWARN_ON(lookup_lreq != lreq);\n\terase_linger_mc(&osdc->linger_map_checks, lreq);\n\tlinger_put(lreq);\n}\n\n \nstatic void __linger_cancel(struct ceph_osd_linger_request *lreq)\n{\n\tif (lreq->ping_req && lreq->ping_req->r_osd)\n\t\tcancel_linger_request(lreq->ping_req);\n\tif (lreq->reg_req && lreq->reg_req->r_osd)\n\t\tcancel_linger_request(lreq->reg_req);\n\tcancel_linger_map_check(lreq);\n\tunlink_linger(lreq->osd, lreq);\n\tlinger_unregister(lreq);\n}\n\nstatic void linger_cancel(struct ceph_osd_linger_request *lreq)\n{\n\tstruct ceph_osd_client *osdc = lreq->osdc;\n\n\tdown_write(&osdc->lock);\n\tif (__linger_registered(lreq))\n\t\t__linger_cancel(lreq);\n\tup_write(&osdc->lock);\n}\n\nstatic void send_linger_map_check(struct ceph_osd_linger_request *lreq);\n\nstatic void check_linger_pool_dne(struct ceph_osd_linger_request *lreq)\n{\n\tstruct ceph_osd_client *osdc = lreq->osdc;\n\tstruct ceph_osdmap *map = osdc->osdmap;\n\n\tverify_osdc_wrlocked(osdc);\n\tWARN_ON(!map->epoch);\n\n\tif (lreq->register_gen) {\n\t\tlreq->map_dne_bound = map->epoch;\n\t\tdout(\"%s lreq %p linger_id %llu pool disappeared\\n\", __func__,\n\t\t     lreq, lreq->linger_id);\n\t} else {\n\t\tdout(\"%s lreq %p linger_id %llu map_dne_bound %u have %u\\n\",\n\t\t     __func__, lreq, lreq->linger_id, lreq->map_dne_bound,\n\t\t     map->epoch);\n\t}\n\n\tif (lreq->map_dne_bound) {\n\t\tif (map->epoch >= lreq->map_dne_bound) {\n\t\t\t \n\t\t\tpr_info(\"linger_id %llu pool does not exist\\n\",\n\t\t\t\tlreq->linger_id);\n\t\t\tlinger_reg_commit_complete(lreq, -ENOENT);\n\t\t\t__linger_cancel(lreq);\n\t\t}\n\t} else {\n\t\tsend_linger_map_check(lreq);\n\t}\n}\n\nstatic void linger_map_check_cb(struct ceph_mon_generic_request *greq)\n{\n\tstruct ceph_osd_client *osdc = &greq->monc->client->osdc;\n\tstruct ceph_osd_linger_request *lreq;\n\tu64 linger_id = greq->private_data;\n\n\tWARN_ON(greq->result || !greq->u.newest);\n\n\tdown_write(&osdc->lock);\n\tlreq = lookup_linger_mc(&osdc->linger_map_checks, linger_id);\n\tif (!lreq) {\n\t\tdout(\"%s linger_id %llu dne\\n\", __func__, linger_id);\n\t\tgoto out_unlock;\n\t}\n\n\tdout(\"%s lreq %p linger_id %llu map_dne_bound %u newest %llu\\n\",\n\t     __func__, lreq, lreq->linger_id, lreq->map_dne_bound,\n\t     greq->u.newest);\n\tif (!lreq->map_dne_bound)\n\t\tlreq->map_dne_bound = greq->u.newest;\n\terase_linger_mc(&osdc->linger_map_checks, lreq);\n\tcheck_linger_pool_dne(lreq);\n\n\tlinger_put(lreq);\nout_unlock:\n\tup_write(&osdc->lock);\n}\n\nstatic void send_linger_map_check(struct ceph_osd_linger_request *lreq)\n{\n\tstruct ceph_osd_client *osdc = lreq->osdc;\n\tstruct ceph_osd_linger_request *lookup_lreq;\n\tint ret;\n\n\tverify_osdc_wrlocked(osdc);\n\n\tlookup_lreq = lookup_linger_mc(&osdc->linger_map_checks,\n\t\t\t\t       lreq->linger_id);\n\tif (lookup_lreq) {\n\t\tWARN_ON(lookup_lreq != lreq);\n\t\treturn;\n\t}\n\n\tlinger_get(lreq);\n\tinsert_linger_mc(&osdc->linger_map_checks, lreq);\n\tret = ceph_monc_get_version_async(&osdc->client->monc, \"osdmap\",\n\t\t\t\t\t  linger_map_check_cb, lreq->linger_id);\n\tWARN_ON(ret);\n}\n\nstatic int linger_reg_commit_wait(struct ceph_osd_linger_request *lreq)\n{\n\tint ret;\n\n\tdout(\"%s lreq %p linger_id %llu\\n\", __func__, lreq, lreq->linger_id);\n\tret = wait_for_completion_killable(&lreq->reg_commit_wait);\n\treturn ret ?: lreq->reg_commit_error;\n}\n\nstatic int linger_notify_finish_wait(struct ceph_osd_linger_request *lreq,\n\t\t\t\t     unsigned long timeout)\n{\n\tlong left;\n\n\tdout(\"%s lreq %p linger_id %llu\\n\", __func__, lreq, lreq->linger_id);\n\tleft = wait_for_completion_killable_timeout(&lreq->notify_finish_wait,\n\t\t\t\t\t\tceph_timeout_jiffies(timeout));\n\tif (left <= 0)\n\t\tleft = left ?: -ETIMEDOUT;\n\telse\n\t\tleft = lreq->notify_finish_error;  \n\n\treturn left;\n}\n\n \nstatic void handle_timeout(struct work_struct *work)\n{\n\tstruct ceph_osd_client *osdc =\n\t\tcontainer_of(work, struct ceph_osd_client, timeout_work.work);\n\tstruct ceph_options *opts = osdc->client->options;\n\tunsigned long cutoff = jiffies - opts->osd_keepalive_timeout;\n\tunsigned long expiry_cutoff = jiffies - opts->osd_request_timeout;\n\tLIST_HEAD(slow_osds);\n\tstruct rb_node *n, *p;\n\n\tdout(\"%s osdc %p\\n\", __func__, osdc);\n\tdown_write(&osdc->lock);\n\n\t \n\tfor (n = rb_first(&osdc->osds); n; n = rb_next(n)) {\n\t\tstruct ceph_osd *osd = rb_entry(n, struct ceph_osd, o_node);\n\t\tbool found = false;\n\n\t\tfor (p = rb_first(&osd->o_requests); p; ) {\n\t\t\tstruct ceph_osd_request *req =\n\t\t\t    rb_entry(p, struct ceph_osd_request, r_node);\n\n\t\t\tp = rb_next(p);  \n\n\t\t\tif (time_before(req->r_stamp, cutoff)) {\n\t\t\t\tdout(\" req %p tid %llu on osd%d is laggy\\n\",\n\t\t\t\t     req, req->r_tid, osd->o_osd);\n\t\t\t\tfound = true;\n\t\t\t}\n\t\t\tif (opts->osd_request_timeout &&\n\t\t\t    time_before(req->r_start_stamp, expiry_cutoff)) {\n\t\t\t\tpr_err_ratelimited(\"tid %llu on osd%d timeout\\n\",\n\t\t\t\t       req->r_tid, osd->o_osd);\n\t\t\t\tabort_request(req, -ETIMEDOUT);\n\t\t\t}\n\t\t}\n\t\tfor (p = rb_first(&osd->o_linger_requests); p; p = rb_next(p)) {\n\t\t\tstruct ceph_osd_linger_request *lreq =\n\t\t\t    rb_entry(p, struct ceph_osd_linger_request, node);\n\n\t\t\tdout(\" lreq %p linger_id %llu is served by osd%d\\n\",\n\t\t\t     lreq, lreq->linger_id, osd->o_osd);\n\t\t\tfound = true;\n\n\t\t\tmutex_lock(&lreq->lock);\n\t\t\tif (lreq->is_watch && lreq->committed && !lreq->last_error)\n\t\t\t\tsend_linger_ping(lreq);\n\t\t\tmutex_unlock(&lreq->lock);\n\t\t}\n\n\t\tif (found)\n\t\t\tlist_move_tail(&osd->o_keepalive_item, &slow_osds);\n\t}\n\n\tif (opts->osd_request_timeout) {\n\t\tfor (p = rb_first(&osdc->homeless_osd.o_requests); p; ) {\n\t\t\tstruct ceph_osd_request *req =\n\t\t\t    rb_entry(p, struct ceph_osd_request, r_node);\n\n\t\t\tp = rb_next(p);  \n\n\t\t\tif (time_before(req->r_start_stamp, expiry_cutoff)) {\n\t\t\t\tpr_err_ratelimited(\"tid %llu on osd%d timeout\\n\",\n\t\t\t\t       req->r_tid, osdc->homeless_osd.o_osd);\n\t\t\t\tabort_request(req, -ETIMEDOUT);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (atomic_read(&osdc->num_homeless) || !list_empty(&slow_osds))\n\t\tmaybe_request_map(osdc);\n\n\twhile (!list_empty(&slow_osds)) {\n\t\tstruct ceph_osd *osd = list_first_entry(&slow_osds,\n\t\t\t\t\t\t\tstruct ceph_osd,\n\t\t\t\t\t\t\to_keepalive_item);\n\t\tlist_del_init(&osd->o_keepalive_item);\n\t\tceph_con_keepalive(&osd->o_con);\n\t}\n\n\tup_write(&osdc->lock);\n\tschedule_delayed_work(&osdc->timeout_work,\n\t\t\t      osdc->client->options->osd_keepalive_timeout);\n}\n\nstatic void handle_osds_timeout(struct work_struct *work)\n{\n\tstruct ceph_osd_client *osdc =\n\t\tcontainer_of(work, struct ceph_osd_client,\n\t\t\t     osds_timeout_work.work);\n\tunsigned long delay = osdc->client->options->osd_idle_ttl / 4;\n\tstruct ceph_osd *osd, *nosd;\n\n\tdout(\"%s osdc %p\\n\", __func__, osdc);\n\tdown_write(&osdc->lock);\n\tlist_for_each_entry_safe(osd, nosd, &osdc->osd_lru, o_osd_lru) {\n\t\tif (time_before(jiffies, osd->lru_ttl))\n\t\t\tbreak;\n\n\t\tWARN_ON(!RB_EMPTY_ROOT(&osd->o_requests));\n\t\tWARN_ON(!RB_EMPTY_ROOT(&osd->o_linger_requests));\n\t\tclose_osd(osd);\n\t}\n\n\tup_write(&osdc->lock);\n\tschedule_delayed_work(&osdc->osds_timeout_work,\n\t\t\t      round_jiffies_relative(delay));\n}\n\nstatic int ceph_oloc_decode(void **p, void *end,\n\t\t\t    struct ceph_object_locator *oloc)\n{\n\tu8 struct_v, struct_cv;\n\tu32 len;\n\tvoid *struct_end;\n\tint ret = 0;\n\n\tceph_decode_need(p, end, 1 + 1 + 4, e_inval);\n\tstruct_v = ceph_decode_8(p);\n\tstruct_cv = ceph_decode_8(p);\n\tif (struct_v < 3) {\n\t\tpr_warn(\"got v %d < 3 cv %d of ceph_object_locator\\n\",\n\t\t\tstruct_v, struct_cv);\n\t\tgoto e_inval;\n\t}\n\tif (struct_cv > 6) {\n\t\tpr_warn(\"got v %d cv %d > 6 of ceph_object_locator\\n\",\n\t\t\tstruct_v, struct_cv);\n\t\tgoto e_inval;\n\t}\n\tlen = ceph_decode_32(p);\n\tceph_decode_need(p, end, len, e_inval);\n\tstruct_end = *p + len;\n\n\toloc->pool = ceph_decode_64(p);\n\t*p += 4;  \n\n\tlen = ceph_decode_32(p);\n\tif (len > 0) {\n\t\tpr_warn(\"ceph_object_locator::key is set\\n\");\n\t\tgoto e_inval;\n\t}\n\n\tif (struct_v >= 5) {\n\t\tbool changed = false;\n\n\t\tlen = ceph_decode_32(p);\n\t\tif (len > 0) {\n\t\t\tceph_decode_need(p, end, len, e_inval);\n\t\t\tif (!oloc->pool_ns ||\n\t\t\t    ceph_compare_string(oloc->pool_ns, *p, len))\n\t\t\t\tchanged = true;\n\t\t\t*p += len;\n\t\t} else {\n\t\t\tif (oloc->pool_ns)\n\t\t\t\tchanged = true;\n\t\t}\n\t\tif (changed) {\n\t\t\t \n\t\t\tpr_warn(\"ceph_object_locator::nspace is changed\\n\");\n\t\t\tgoto e_inval;\n\t\t}\n\t}\n\n\tif (struct_v >= 6) {\n\t\ts64 hash = ceph_decode_64(p);\n\t\tif (hash != -1) {\n\t\t\tpr_warn(\"ceph_object_locator::hash is set\\n\");\n\t\t\tgoto e_inval;\n\t\t}\n\t}\n\n\t \n\t*p = struct_end;\nout:\n\treturn ret;\n\ne_inval:\n\tret = -EINVAL;\n\tgoto out;\n}\n\nstatic int ceph_redirect_decode(void **p, void *end,\n\t\t\t\tstruct ceph_request_redirect *redir)\n{\n\tu8 struct_v, struct_cv;\n\tu32 len;\n\tvoid *struct_end;\n\tint ret;\n\n\tceph_decode_need(p, end, 1 + 1 + 4, e_inval);\n\tstruct_v = ceph_decode_8(p);\n\tstruct_cv = ceph_decode_8(p);\n\tif (struct_cv > 1) {\n\t\tpr_warn(\"got v %d cv %d > 1 of ceph_request_redirect\\n\",\n\t\t\tstruct_v, struct_cv);\n\t\tgoto e_inval;\n\t}\n\tlen = ceph_decode_32(p);\n\tceph_decode_need(p, end, len, e_inval);\n\tstruct_end = *p + len;\n\n\tret = ceph_oloc_decode(p, end, &redir->oloc);\n\tif (ret)\n\t\tgoto out;\n\n\tlen = ceph_decode_32(p);\n\tif (len > 0) {\n\t\tpr_warn(\"ceph_request_redirect::object_name is set\\n\");\n\t\tgoto e_inval;\n\t}\n\n\t \n\t*p = struct_end;\nout:\n\treturn ret;\n\ne_inval:\n\tret = -EINVAL;\n\tgoto out;\n}\n\nstruct MOSDOpReply {\n\tstruct ceph_pg pgid;\n\tu64 flags;\n\tint result;\n\tu32 epoch;\n\tint num_ops;\n\tu32 outdata_len[CEPH_OSD_MAX_OPS];\n\ts32 rval[CEPH_OSD_MAX_OPS];\n\tint retry_attempt;\n\tstruct ceph_eversion replay_version;\n\tu64 user_version;\n\tstruct ceph_request_redirect redirect;\n};\n\nstatic int decode_MOSDOpReply(const struct ceph_msg *msg, struct MOSDOpReply *m)\n{\n\tvoid *p = msg->front.iov_base;\n\tvoid *const end = p + msg->front.iov_len;\n\tu16 version = le16_to_cpu(msg->hdr.version);\n\tstruct ceph_eversion bad_replay_version;\n\tu8 decode_redir;\n\tu32 len;\n\tint ret;\n\tint i;\n\n\tceph_decode_32_safe(&p, end, len, e_inval);\n\tceph_decode_need(&p, end, len, e_inval);\n\tp += len;  \n\n\tret = ceph_decode_pgid(&p, end, &m->pgid);\n\tif (ret)\n\t\treturn ret;\n\n\tceph_decode_64_safe(&p, end, m->flags, e_inval);\n\tceph_decode_32_safe(&p, end, m->result, e_inval);\n\tceph_decode_need(&p, end, sizeof(bad_replay_version), e_inval);\n\tmemcpy(&bad_replay_version, p, sizeof(bad_replay_version));\n\tp += sizeof(bad_replay_version);\n\tceph_decode_32_safe(&p, end, m->epoch, e_inval);\n\n\tceph_decode_32_safe(&p, end, m->num_ops, e_inval);\n\tif (m->num_ops > ARRAY_SIZE(m->outdata_len))\n\t\tgoto e_inval;\n\n\tceph_decode_need(&p, end, m->num_ops * sizeof(struct ceph_osd_op),\n\t\t\t e_inval);\n\tfor (i = 0; i < m->num_ops; i++) {\n\t\tstruct ceph_osd_op *op = p;\n\n\t\tm->outdata_len[i] = le32_to_cpu(op->payload_len);\n\t\tp += sizeof(*op);\n\t}\n\n\tceph_decode_32_safe(&p, end, m->retry_attempt, e_inval);\n\tfor (i = 0; i < m->num_ops; i++)\n\t\tceph_decode_32_safe(&p, end, m->rval[i], e_inval);\n\n\tif (version >= 5) {\n\t\tceph_decode_need(&p, end, sizeof(m->replay_version), e_inval);\n\t\tmemcpy(&m->replay_version, p, sizeof(m->replay_version));\n\t\tp += sizeof(m->replay_version);\n\t\tceph_decode_64_safe(&p, end, m->user_version, e_inval);\n\t} else {\n\t\tm->replay_version = bad_replay_version;  \n\t\tm->user_version = le64_to_cpu(m->replay_version.version);\n\t}\n\n\tif (version >= 6) {\n\t\tif (version >= 7)\n\t\t\tceph_decode_8_safe(&p, end, decode_redir, e_inval);\n\t\telse\n\t\t\tdecode_redir = 1;\n\t} else {\n\t\tdecode_redir = 0;\n\t}\n\n\tif (decode_redir) {\n\t\tret = ceph_redirect_decode(&p, end, &m->redirect);\n\t\tif (ret)\n\t\t\treturn ret;\n\t} else {\n\t\tceph_oloc_init(&m->redirect.oloc);\n\t}\n\n\treturn 0;\n\ne_inval:\n\treturn -EINVAL;\n}\n\n \nstatic void handle_reply(struct ceph_osd *osd, struct ceph_msg *msg)\n{\n\tstruct ceph_osd_client *osdc = osd->o_osdc;\n\tstruct ceph_osd_request *req;\n\tstruct MOSDOpReply m;\n\tu64 tid = le64_to_cpu(msg->hdr.tid);\n\tu32 data_len = 0;\n\tint ret;\n\tint i;\n\n\tdout(\"%s msg %p tid %llu\\n\", __func__, msg, tid);\n\n\tdown_read(&osdc->lock);\n\tif (!osd_registered(osd)) {\n\t\tdout(\"%s osd%d unknown\\n\", __func__, osd->o_osd);\n\t\tgoto out_unlock_osdc;\n\t}\n\tWARN_ON(osd->o_osd != le64_to_cpu(msg->hdr.src.num));\n\n\tmutex_lock(&osd->lock);\n\treq = lookup_request(&osd->o_requests, tid);\n\tif (!req) {\n\t\tdout(\"%s osd%d tid %llu unknown\\n\", __func__, osd->o_osd, tid);\n\t\tgoto out_unlock_session;\n\t}\n\n\tm.redirect.oloc.pool_ns = req->r_t.target_oloc.pool_ns;\n\tret = decode_MOSDOpReply(msg, &m);\n\tm.redirect.oloc.pool_ns = NULL;\n\tif (ret) {\n\t\tpr_err(\"failed to decode MOSDOpReply for tid %llu: %d\\n\",\n\t\t       req->r_tid, ret);\n\t\tceph_msg_dump(msg);\n\t\tgoto fail_request;\n\t}\n\tdout(\"%s req %p tid %llu flags 0x%llx pgid %llu.%x epoch %u attempt %d v %u'%llu uv %llu\\n\",\n\t     __func__, req, req->r_tid, m.flags, m.pgid.pool, m.pgid.seed,\n\t     m.epoch, m.retry_attempt, le32_to_cpu(m.replay_version.epoch),\n\t     le64_to_cpu(m.replay_version.version), m.user_version);\n\n\tif (m.retry_attempt >= 0) {\n\t\tif (m.retry_attempt != req->r_attempts - 1) {\n\t\t\tdout(\"req %p tid %llu retry_attempt %d != %d, ignoring\\n\",\n\t\t\t     req, req->r_tid, m.retry_attempt,\n\t\t\t     req->r_attempts - 1);\n\t\t\tgoto out_unlock_session;\n\t\t}\n\t} else {\n\t\tWARN_ON(1);  \n\t}\n\n\tif (!ceph_oloc_empty(&m.redirect.oloc)) {\n\t\tdout(\"req %p tid %llu redirect pool %lld\\n\", req, req->r_tid,\n\t\t     m.redirect.oloc.pool);\n\t\tunlink_request(osd, req);\n\t\tmutex_unlock(&osd->lock);\n\n\t\t \n\t\treq->r_t.target_oloc.pool = m.redirect.oloc.pool;\n\t\treq->r_flags |= CEPH_OSD_FLAG_REDIRECTED |\n\t\t\t\tCEPH_OSD_FLAG_IGNORE_OVERLAY |\n\t\t\t\tCEPH_OSD_FLAG_IGNORE_CACHE;\n\t\treq->r_tid = 0;\n\t\t__submit_request(req, false);\n\t\tgoto out_unlock_osdc;\n\t}\n\n\tif (m.result == -EAGAIN) {\n\t\tdout(\"req %p tid %llu EAGAIN\\n\", req, req->r_tid);\n\t\tunlink_request(osd, req);\n\t\tmutex_unlock(&osd->lock);\n\n\t\t \n\t\treq->r_t.pgid.pool = 0;\n\t\treq->r_t.pgid.seed = 0;\n\t\tWARN_ON(!req->r_t.used_replica);\n\t\treq->r_flags &= ~(CEPH_OSD_FLAG_BALANCE_READS |\n\t\t\t\t  CEPH_OSD_FLAG_LOCALIZE_READS);\n\t\treq->r_tid = 0;\n\t\t__submit_request(req, false);\n\t\tgoto out_unlock_osdc;\n\t}\n\n\tif (m.num_ops != req->r_num_ops) {\n\t\tpr_err(\"num_ops %d != %d for tid %llu\\n\", m.num_ops,\n\t\t       req->r_num_ops, req->r_tid);\n\t\tgoto fail_request;\n\t}\n\tfor (i = 0; i < req->r_num_ops; i++) {\n\t\tdout(\" req %p tid %llu op %d rval %d len %u\\n\", req,\n\t\t     req->r_tid, i, m.rval[i], m.outdata_len[i]);\n\t\treq->r_ops[i].rval = m.rval[i];\n\t\treq->r_ops[i].outdata_len = m.outdata_len[i];\n\t\tdata_len += m.outdata_len[i];\n\t}\n\tif (data_len != le32_to_cpu(msg->hdr.data_len)) {\n\t\tpr_err(\"sum of lens %u != %u for tid %llu\\n\", data_len,\n\t\t       le32_to_cpu(msg->hdr.data_len), req->r_tid);\n\t\tgoto fail_request;\n\t}\n\tdout(\"%s req %p tid %llu result %d data_len %u\\n\", __func__,\n\t     req, req->r_tid, m.result, data_len);\n\n\t \n\tWARN_ON(!(m.flags & CEPH_OSD_FLAG_ONDISK));\n\treq->r_version = m.user_version;\n\treq->r_result = m.result ?: data_len;\n\tfinish_request(req);\n\tmutex_unlock(&osd->lock);\n\tup_read(&osdc->lock);\n\n\t__complete_request(req);\n\treturn;\n\nfail_request:\n\tcomplete_request(req, -EIO);\nout_unlock_session:\n\tmutex_unlock(&osd->lock);\nout_unlock_osdc:\n\tup_read(&osdc->lock);\n}\n\nstatic void set_pool_was_full(struct ceph_osd_client *osdc)\n{\n\tstruct rb_node *n;\n\n\tfor (n = rb_first(&osdc->osdmap->pg_pools); n; n = rb_next(n)) {\n\t\tstruct ceph_pg_pool_info *pi =\n\t\t    rb_entry(n, struct ceph_pg_pool_info, node);\n\n\t\tpi->was_full = __pool_full(pi);\n\t}\n}\n\nstatic bool pool_cleared_full(struct ceph_osd_client *osdc, s64 pool_id)\n{\n\tstruct ceph_pg_pool_info *pi;\n\n\tpi = ceph_pg_pool_by_id(osdc->osdmap, pool_id);\n\tif (!pi)\n\t\treturn false;\n\n\treturn pi->was_full && !__pool_full(pi);\n}\n\nstatic enum calc_target_result\nrecalc_linger_target(struct ceph_osd_linger_request *lreq)\n{\n\tstruct ceph_osd_client *osdc = lreq->osdc;\n\tenum calc_target_result ct_res;\n\n\tct_res = calc_target(osdc, &lreq->t, true);\n\tif (ct_res == CALC_TARGET_NEED_RESEND) {\n\t\tstruct ceph_osd *osd;\n\n\t\tosd = lookup_create_osd(osdc, lreq->t.osd, true);\n\t\tif (osd != lreq->osd) {\n\t\t\tunlink_linger(lreq->osd, lreq);\n\t\t\tlink_linger(osd, lreq);\n\t\t}\n\t}\n\n\treturn ct_res;\n}\n\n \nstatic void scan_requests(struct ceph_osd *osd,\n\t\t\t  bool force_resend,\n\t\t\t  bool cleared_full,\n\t\t\t  bool check_pool_cleared_full,\n\t\t\t  struct rb_root *need_resend,\n\t\t\t  struct list_head *need_resend_linger)\n{\n\tstruct ceph_osd_client *osdc = osd->o_osdc;\n\tstruct rb_node *n;\n\tbool force_resend_writes;\n\n\tfor (n = rb_first(&osd->o_linger_requests); n; ) {\n\t\tstruct ceph_osd_linger_request *lreq =\n\t\t    rb_entry(n, struct ceph_osd_linger_request, node);\n\t\tenum calc_target_result ct_res;\n\n\t\tn = rb_next(n);  \n\n\t\tdout(\"%s lreq %p linger_id %llu\\n\", __func__, lreq,\n\t\t     lreq->linger_id);\n\t\tct_res = recalc_linger_target(lreq);\n\t\tswitch (ct_res) {\n\t\tcase CALC_TARGET_NO_ACTION:\n\t\t\tforce_resend_writes = cleared_full ||\n\t\t\t    (check_pool_cleared_full &&\n\t\t\t     pool_cleared_full(osdc, lreq->t.base_oloc.pool));\n\t\t\tif (!force_resend && !force_resend_writes)\n\t\t\t\tbreak;\n\n\t\t\tfallthrough;\n\t\tcase CALC_TARGET_NEED_RESEND:\n\t\t\tcancel_linger_map_check(lreq);\n\t\t\t \n\t\t\tif (list_empty(&lreq->scan_item))\n\t\t\t\tlist_add_tail(&lreq->scan_item, need_resend_linger);\n\t\t\tbreak;\n\t\tcase CALC_TARGET_POOL_DNE:\n\t\t\tlist_del_init(&lreq->scan_item);\n\t\t\tcheck_linger_pool_dne(lreq);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tfor (n = rb_first(&osd->o_requests); n; ) {\n\t\tstruct ceph_osd_request *req =\n\t\t    rb_entry(n, struct ceph_osd_request, r_node);\n\t\tenum calc_target_result ct_res;\n\n\t\tn = rb_next(n);  \n\n\t\tdout(\"%s req %p tid %llu\\n\", __func__, req, req->r_tid);\n\t\tct_res = calc_target(osdc, &req->r_t, false);\n\t\tswitch (ct_res) {\n\t\tcase CALC_TARGET_NO_ACTION:\n\t\t\tforce_resend_writes = cleared_full ||\n\t\t\t    (check_pool_cleared_full &&\n\t\t\t     pool_cleared_full(osdc, req->r_t.base_oloc.pool));\n\t\t\tif (!force_resend &&\n\t\t\t    (!(req->r_flags & CEPH_OSD_FLAG_WRITE) ||\n\t\t\t     !force_resend_writes))\n\t\t\t\tbreak;\n\n\t\t\tfallthrough;\n\t\tcase CALC_TARGET_NEED_RESEND:\n\t\t\tcancel_map_check(req);\n\t\t\tunlink_request(osd, req);\n\t\t\tinsert_request(need_resend, req);\n\t\t\tbreak;\n\t\tcase CALC_TARGET_POOL_DNE:\n\t\t\tcheck_pool_dne(req);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic int handle_one_map(struct ceph_osd_client *osdc,\n\t\t\t  void *p, void *end, bool incremental,\n\t\t\t  struct rb_root *need_resend,\n\t\t\t  struct list_head *need_resend_linger)\n{\n\tstruct ceph_osdmap *newmap;\n\tstruct rb_node *n;\n\tbool skipped_map = false;\n\tbool was_full;\n\n\twas_full = ceph_osdmap_flag(osdc, CEPH_OSDMAP_FULL);\n\tset_pool_was_full(osdc);\n\n\tif (incremental)\n\t\tnewmap = osdmap_apply_incremental(&p, end,\n\t\t\t\t\t\t  ceph_msgr2(osdc->client),\n\t\t\t\t\t\t  osdc->osdmap);\n\telse\n\t\tnewmap = ceph_osdmap_decode(&p, end, ceph_msgr2(osdc->client));\n\tif (IS_ERR(newmap))\n\t\treturn PTR_ERR(newmap);\n\n\tif (newmap != osdc->osdmap) {\n\t\t \n\t\tfor (n = rb_first(&newmap->pg_pools); n; n = rb_next(n)) {\n\t\t\tstruct ceph_pg_pool_info *pi =\n\t\t\t    rb_entry(n, struct ceph_pg_pool_info, node);\n\t\t\tstruct ceph_pg_pool_info *old_pi;\n\n\t\t\told_pi = ceph_pg_pool_by_id(osdc->osdmap, pi->id);\n\t\t\tif (old_pi)\n\t\t\t\tpi->was_full = old_pi->was_full;\n\t\t\telse\n\t\t\t\tWARN_ON(pi->was_full);\n\t\t}\n\n\t\tif (osdc->osdmap->epoch &&\n\t\t    osdc->osdmap->epoch + 1 < newmap->epoch) {\n\t\t\tWARN_ON(incremental);\n\t\t\tskipped_map = true;\n\t\t}\n\n\t\tceph_osdmap_destroy(osdc->osdmap);\n\t\tosdc->osdmap = newmap;\n\t}\n\n\twas_full &= !ceph_osdmap_flag(osdc, CEPH_OSDMAP_FULL);\n\tscan_requests(&osdc->homeless_osd, skipped_map, was_full, true,\n\t\t      need_resend, need_resend_linger);\n\n\tfor (n = rb_first(&osdc->osds); n; ) {\n\t\tstruct ceph_osd *osd = rb_entry(n, struct ceph_osd, o_node);\n\n\t\tn = rb_next(n);  \n\n\t\tscan_requests(osd, skipped_map, was_full, true, need_resend,\n\t\t\t      need_resend_linger);\n\t\tif (!ceph_osd_is_up(osdc->osdmap, osd->o_osd) ||\n\t\t    memcmp(&osd->o_con.peer_addr,\n\t\t\t   ceph_osd_addr(osdc->osdmap, osd->o_osd),\n\t\t\t   sizeof(struct ceph_entity_addr)))\n\t\t\tclose_osd(osd);\n\t}\n\n\treturn 0;\n}\n\nstatic void kick_requests(struct ceph_osd_client *osdc,\n\t\t\t  struct rb_root *need_resend,\n\t\t\t  struct list_head *need_resend_linger)\n{\n\tstruct ceph_osd_linger_request *lreq, *nlreq;\n\tenum calc_target_result ct_res;\n\tstruct rb_node *n;\n\n\t \n\tfor (n = rb_first(need_resend); n; ) {\n\t\tstruct ceph_osd_request *req =\n\t\t    rb_entry(n, struct ceph_osd_request, r_node);\n\n\t\tn = rb_next(n);\n\n\t\tif (req->r_t.epoch < osdc->osdmap->epoch) {\n\t\t\tct_res = calc_target(osdc, &req->r_t, false);\n\t\t\tif (ct_res == CALC_TARGET_POOL_DNE) {\n\t\t\t\terase_request(need_resend, req);\n\t\t\t\tcheck_pool_dne(req);\n\t\t\t}\n\t\t}\n\t}\n\n\tfor (n = rb_first(need_resend); n; ) {\n\t\tstruct ceph_osd_request *req =\n\t\t    rb_entry(n, struct ceph_osd_request, r_node);\n\t\tstruct ceph_osd *osd;\n\n\t\tn = rb_next(n);\n\t\terase_request(need_resend, req);  \n\n\t\tosd = lookup_create_osd(osdc, req->r_t.osd, true);\n\t\tlink_request(osd, req);\n\t\tif (!req->r_linger) {\n\t\t\tif (!osd_homeless(osd) && !req->r_t.paused)\n\t\t\t\tsend_request(req);\n\t\t} else {\n\t\t\tcancel_linger_request(req);\n\t\t}\n\t}\n\n\tlist_for_each_entry_safe(lreq, nlreq, need_resend_linger, scan_item) {\n\t\tif (!osd_homeless(lreq->osd))\n\t\t\tsend_linger(lreq);\n\n\t\tlist_del_init(&lreq->scan_item);\n\t}\n}\n\n \nvoid ceph_osdc_handle_map(struct ceph_osd_client *osdc, struct ceph_msg *msg)\n{\n\tvoid *p = msg->front.iov_base;\n\tvoid *const end = p + msg->front.iov_len;\n\tu32 nr_maps, maplen;\n\tu32 epoch;\n\tstruct ceph_fsid fsid;\n\tstruct rb_root need_resend = RB_ROOT;\n\tLIST_HEAD(need_resend_linger);\n\tbool handled_incremental = false;\n\tbool was_pauserd, was_pausewr;\n\tbool pauserd, pausewr;\n\tint err;\n\n\tdout(\"%s have %u\\n\", __func__, osdc->osdmap->epoch);\n\tdown_write(&osdc->lock);\n\n\t \n\tceph_decode_need(&p, end, sizeof(fsid), bad);\n\tceph_decode_copy(&p, &fsid, sizeof(fsid));\n\tif (ceph_check_fsid(osdc->client, &fsid) < 0)\n\t\tgoto bad;\n\n\twas_pauserd = ceph_osdmap_flag(osdc, CEPH_OSDMAP_PAUSERD);\n\twas_pausewr = ceph_osdmap_flag(osdc, CEPH_OSDMAP_PAUSEWR) ||\n\t\t      ceph_osdmap_flag(osdc, CEPH_OSDMAP_FULL) ||\n\t\t      have_pool_full(osdc);\n\n\t \n\tceph_decode_32_safe(&p, end, nr_maps, bad);\n\tdout(\" %d inc maps\\n\", nr_maps);\n\twhile (nr_maps > 0) {\n\t\tceph_decode_need(&p, end, 2*sizeof(u32), bad);\n\t\tepoch = ceph_decode_32(&p);\n\t\tmaplen = ceph_decode_32(&p);\n\t\tceph_decode_need(&p, end, maplen, bad);\n\t\tif (osdc->osdmap->epoch &&\n\t\t    osdc->osdmap->epoch + 1 == epoch) {\n\t\t\tdout(\"applying incremental map %u len %d\\n\",\n\t\t\t     epoch, maplen);\n\t\t\terr = handle_one_map(osdc, p, p + maplen, true,\n\t\t\t\t\t     &need_resend, &need_resend_linger);\n\t\t\tif (err)\n\t\t\t\tgoto bad;\n\t\t\thandled_incremental = true;\n\t\t} else {\n\t\t\tdout(\"ignoring incremental map %u len %d\\n\",\n\t\t\t     epoch, maplen);\n\t\t}\n\t\tp += maplen;\n\t\tnr_maps--;\n\t}\n\tif (handled_incremental)\n\t\tgoto done;\n\n\t \n\tceph_decode_32_safe(&p, end, nr_maps, bad);\n\tdout(\" %d full maps\\n\", nr_maps);\n\twhile (nr_maps) {\n\t\tceph_decode_need(&p, end, 2*sizeof(u32), bad);\n\t\tepoch = ceph_decode_32(&p);\n\t\tmaplen = ceph_decode_32(&p);\n\t\tceph_decode_need(&p, end, maplen, bad);\n\t\tif (nr_maps > 1) {\n\t\t\tdout(\"skipping non-latest full map %u len %d\\n\",\n\t\t\t     epoch, maplen);\n\t\t} else if (osdc->osdmap->epoch >= epoch) {\n\t\t\tdout(\"skipping full map %u len %d, \"\n\t\t\t     \"older than our %u\\n\", epoch, maplen,\n\t\t\t     osdc->osdmap->epoch);\n\t\t} else {\n\t\t\tdout(\"taking full map %u len %d\\n\", epoch, maplen);\n\t\t\terr = handle_one_map(osdc, p, p + maplen, false,\n\t\t\t\t\t     &need_resend, &need_resend_linger);\n\t\t\tif (err)\n\t\t\t\tgoto bad;\n\t\t}\n\t\tp += maplen;\n\t\tnr_maps--;\n\t}\n\ndone:\n\t \n\tpauserd = ceph_osdmap_flag(osdc, CEPH_OSDMAP_PAUSERD);\n\tpausewr = ceph_osdmap_flag(osdc, CEPH_OSDMAP_PAUSEWR) ||\n\t\t  ceph_osdmap_flag(osdc, CEPH_OSDMAP_FULL) ||\n\t\t  have_pool_full(osdc);\n\tif (was_pauserd || was_pausewr || pauserd || pausewr ||\n\t    osdc->osdmap->epoch < osdc->epoch_barrier)\n\t\tmaybe_request_map(osdc);\n\n\tkick_requests(osdc, &need_resend, &need_resend_linger);\n\n\tceph_osdc_abort_on_full(osdc);\n\tceph_monc_got_map(&osdc->client->monc, CEPH_SUB_OSDMAP,\n\t\t\t  osdc->osdmap->epoch);\n\tup_write(&osdc->lock);\n\twake_up_all(&osdc->client->auth_wq);\n\treturn;\n\nbad:\n\tpr_err(\"osdc handle_map corrupt msg\\n\");\n\tceph_msg_dump(msg);\n\tup_write(&osdc->lock);\n}\n\n \nstatic void kick_osd_requests(struct ceph_osd *osd)\n{\n\tstruct rb_node *n;\n\n\tclear_backoffs(osd);\n\n\tfor (n = rb_first(&osd->o_requests); n; ) {\n\t\tstruct ceph_osd_request *req =\n\t\t    rb_entry(n, struct ceph_osd_request, r_node);\n\n\t\tn = rb_next(n);  \n\n\t\tif (!req->r_linger) {\n\t\t\tif (!req->r_t.paused)\n\t\t\t\tsend_request(req);\n\t\t} else {\n\t\t\tcancel_linger_request(req);\n\t\t}\n\t}\n\tfor (n = rb_first(&osd->o_linger_requests); n; n = rb_next(n)) {\n\t\tstruct ceph_osd_linger_request *lreq =\n\t\t    rb_entry(n, struct ceph_osd_linger_request, node);\n\n\t\tsend_linger(lreq);\n\t}\n}\n\n \nstatic void osd_fault(struct ceph_connection *con)\n{\n\tstruct ceph_osd *osd = con->private;\n\tstruct ceph_osd_client *osdc = osd->o_osdc;\n\n\tdout(\"%s osd %p osd%d\\n\", __func__, osd, osd->o_osd);\n\n\tdown_write(&osdc->lock);\n\tif (!osd_registered(osd)) {\n\t\tdout(\"%s osd%d unknown\\n\", __func__, osd->o_osd);\n\t\tgoto out_unlock;\n\t}\n\n\tif (!reopen_osd(osd))\n\t\tkick_osd_requests(osd);\n\tmaybe_request_map(osdc);\n\nout_unlock:\n\tup_write(&osdc->lock);\n}\n\nstruct MOSDBackoff {\n\tstruct ceph_spg spgid;\n\tu32 map_epoch;\n\tu8 op;\n\tu64 id;\n\tstruct ceph_hobject_id *begin;\n\tstruct ceph_hobject_id *end;\n};\n\nstatic int decode_MOSDBackoff(const struct ceph_msg *msg, struct MOSDBackoff *m)\n{\n\tvoid *p = msg->front.iov_base;\n\tvoid *const end = p + msg->front.iov_len;\n\tu8 struct_v;\n\tu32 struct_len;\n\tint ret;\n\n\tret = ceph_start_decoding(&p, end, 1, \"spg_t\", &struct_v, &struct_len);\n\tif (ret)\n\t\treturn ret;\n\n\tret = ceph_decode_pgid(&p, end, &m->spgid.pgid);\n\tif (ret)\n\t\treturn ret;\n\n\tceph_decode_8_safe(&p, end, m->spgid.shard, e_inval);\n\tceph_decode_32_safe(&p, end, m->map_epoch, e_inval);\n\tceph_decode_8_safe(&p, end, m->op, e_inval);\n\tceph_decode_64_safe(&p, end, m->id, e_inval);\n\n\tm->begin = kzalloc(sizeof(*m->begin), GFP_NOIO);\n\tif (!m->begin)\n\t\treturn -ENOMEM;\n\n\tret = decode_hoid(&p, end, m->begin);\n\tif (ret) {\n\t\tfree_hoid(m->begin);\n\t\treturn ret;\n\t}\n\n\tm->end = kzalloc(sizeof(*m->end), GFP_NOIO);\n\tif (!m->end) {\n\t\tfree_hoid(m->begin);\n\t\treturn -ENOMEM;\n\t}\n\n\tret = decode_hoid(&p, end, m->end);\n\tif (ret) {\n\t\tfree_hoid(m->begin);\n\t\tfree_hoid(m->end);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n\ne_inval:\n\treturn -EINVAL;\n}\n\nstatic struct ceph_msg *create_backoff_message(\n\t\t\t\tconst struct ceph_osd_backoff *backoff,\n\t\t\t\tu32 map_epoch)\n{\n\tstruct ceph_msg *msg;\n\tvoid *p, *end;\n\tint msg_size;\n\n\tmsg_size = CEPH_ENCODING_START_BLK_LEN +\n\t\t\tCEPH_PGID_ENCODING_LEN + 1;  \n\tmsg_size += 4 + 1 + 8;  \n\tmsg_size += CEPH_ENCODING_START_BLK_LEN +\n\t\t\thoid_encoding_size(backoff->begin);\n\tmsg_size += CEPH_ENCODING_START_BLK_LEN +\n\t\t\thoid_encoding_size(backoff->end);\n\n\tmsg = ceph_msg_new(CEPH_MSG_OSD_BACKOFF, msg_size, GFP_NOIO, true);\n\tif (!msg)\n\t\treturn NULL;\n\n\tp = msg->front.iov_base;\n\tend = p + msg->front_alloc_len;\n\n\tencode_spgid(&p, &backoff->spgid);\n\tceph_encode_32(&p, map_epoch);\n\tceph_encode_8(&p, CEPH_OSD_BACKOFF_OP_ACK_BLOCK);\n\tceph_encode_64(&p, backoff->id);\n\tencode_hoid(&p, end, backoff->begin);\n\tencode_hoid(&p, end, backoff->end);\n\tBUG_ON(p != end);\n\n\tmsg->front.iov_len = p - msg->front.iov_base;\n\tmsg->hdr.version = cpu_to_le16(1);  \n\tmsg->hdr.front_len = cpu_to_le32(msg->front.iov_len);\n\n\treturn msg;\n}\n\nstatic void handle_backoff_block(struct ceph_osd *osd, struct MOSDBackoff *m)\n{\n\tstruct ceph_spg_mapping *spg;\n\tstruct ceph_osd_backoff *backoff;\n\tstruct ceph_msg *msg;\n\n\tdout(\"%s osd%d spgid %llu.%xs%d id %llu\\n\", __func__, osd->o_osd,\n\t     m->spgid.pgid.pool, m->spgid.pgid.seed, m->spgid.shard, m->id);\n\n\tspg = lookup_spg_mapping(&osd->o_backoff_mappings, &m->spgid);\n\tif (!spg) {\n\t\tspg = alloc_spg_mapping();\n\t\tif (!spg) {\n\t\t\tpr_err(\"%s failed to allocate spg\\n\", __func__);\n\t\t\treturn;\n\t\t}\n\t\tspg->spgid = m->spgid;  \n\t\tinsert_spg_mapping(&osd->o_backoff_mappings, spg);\n\t}\n\n\tbackoff = alloc_backoff();\n\tif (!backoff) {\n\t\tpr_err(\"%s failed to allocate backoff\\n\", __func__);\n\t\treturn;\n\t}\n\tbackoff->spgid = m->spgid;  \n\tbackoff->id = m->id;\n\tbackoff->begin = m->begin;\n\tm->begin = NULL;  \n\tbackoff->end = m->end;\n\tm->end = NULL;    \n\n\tinsert_backoff(&spg->backoffs, backoff);\n\tinsert_backoff_by_id(&osd->o_backoffs_by_id, backoff);\n\n\t \n\tmsg = create_backoff_message(backoff, m->map_epoch);\n\tif (!msg) {\n\t\tpr_err(\"%s failed to allocate msg\\n\", __func__);\n\t\treturn;\n\t}\n\tceph_con_send(&osd->o_con, msg);\n}\n\nstatic bool target_contained_by(const struct ceph_osd_request_target *t,\n\t\t\t\tconst struct ceph_hobject_id *begin,\n\t\t\t\tconst struct ceph_hobject_id *end)\n{\n\tstruct ceph_hobject_id hoid;\n\tint cmp;\n\n\thoid_fill_from_target(&hoid, t);\n\tcmp = hoid_compare(&hoid, begin);\n\treturn !cmp || (cmp > 0 && hoid_compare(&hoid, end) < 0);\n}\n\nstatic void handle_backoff_unblock(struct ceph_osd *osd,\n\t\t\t\t   const struct MOSDBackoff *m)\n{\n\tstruct ceph_spg_mapping *spg;\n\tstruct ceph_osd_backoff *backoff;\n\tstruct rb_node *n;\n\n\tdout(\"%s osd%d spgid %llu.%xs%d id %llu\\n\", __func__, osd->o_osd,\n\t     m->spgid.pgid.pool, m->spgid.pgid.seed, m->spgid.shard, m->id);\n\n\tbackoff = lookup_backoff_by_id(&osd->o_backoffs_by_id, m->id);\n\tif (!backoff) {\n\t\tpr_err(\"%s osd%d spgid %llu.%xs%d id %llu backoff dne\\n\",\n\t\t       __func__, osd->o_osd, m->spgid.pgid.pool,\n\t\t       m->spgid.pgid.seed, m->spgid.shard, m->id);\n\t\treturn;\n\t}\n\n\tif (hoid_compare(backoff->begin, m->begin) &&\n\t    hoid_compare(backoff->end, m->end)) {\n\t\tpr_err(\"%s osd%d spgid %llu.%xs%d id %llu bad range?\\n\",\n\t\t       __func__, osd->o_osd, m->spgid.pgid.pool,\n\t\t       m->spgid.pgid.seed, m->spgid.shard, m->id);\n\t\t \n\t}\n\n\tspg = lookup_spg_mapping(&osd->o_backoff_mappings, &backoff->spgid);\n\tBUG_ON(!spg);\n\n\terase_backoff(&spg->backoffs, backoff);\n\terase_backoff_by_id(&osd->o_backoffs_by_id, backoff);\n\tfree_backoff(backoff);\n\n\tif (RB_EMPTY_ROOT(&spg->backoffs)) {\n\t\terase_spg_mapping(&osd->o_backoff_mappings, spg);\n\t\tfree_spg_mapping(spg);\n\t}\n\n\tfor (n = rb_first(&osd->o_requests); n; n = rb_next(n)) {\n\t\tstruct ceph_osd_request *req =\n\t\t    rb_entry(n, struct ceph_osd_request, r_node);\n\n\t\tif (!ceph_spg_compare(&req->r_t.spgid, &m->spgid)) {\n\t\t\t \n\t\t\tif (target_contained_by(&req->r_t, m->begin, m->end)) {\n\t\t\t\t \n\t\t\t\tsend_request(req);\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic void handle_backoff(struct ceph_osd *osd, struct ceph_msg *msg)\n{\n\tstruct ceph_osd_client *osdc = osd->o_osdc;\n\tstruct MOSDBackoff m;\n\tint ret;\n\n\tdown_read(&osdc->lock);\n\tif (!osd_registered(osd)) {\n\t\tdout(\"%s osd%d unknown\\n\", __func__, osd->o_osd);\n\t\tup_read(&osdc->lock);\n\t\treturn;\n\t}\n\tWARN_ON(osd->o_osd != le64_to_cpu(msg->hdr.src.num));\n\n\tmutex_lock(&osd->lock);\n\tret = decode_MOSDBackoff(msg, &m);\n\tif (ret) {\n\t\tpr_err(\"failed to decode MOSDBackoff: %d\\n\", ret);\n\t\tceph_msg_dump(msg);\n\t\tgoto out_unlock;\n\t}\n\n\tswitch (m.op) {\n\tcase CEPH_OSD_BACKOFF_OP_BLOCK:\n\t\thandle_backoff_block(osd, &m);\n\t\tbreak;\n\tcase CEPH_OSD_BACKOFF_OP_UNBLOCK:\n\t\thandle_backoff_unblock(osd, &m);\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"%s osd%d unknown op %d\\n\", __func__, osd->o_osd, m.op);\n\t}\n\n\tfree_hoid(m.begin);\n\tfree_hoid(m.end);\n\nout_unlock:\n\tmutex_unlock(&osd->lock);\n\tup_read(&osdc->lock);\n}\n\n \nstatic void handle_watch_notify(struct ceph_osd_client *osdc,\n\t\t\t\tstruct ceph_msg *msg)\n{\n\tvoid *p = msg->front.iov_base;\n\tvoid *const end = p + msg->front.iov_len;\n\tstruct ceph_osd_linger_request *lreq;\n\tstruct linger_work *lwork;\n\tu8 proto_ver, opcode;\n\tu64 cookie, notify_id;\n\tu64 notifier_id = 0;\n\ts32 return_code = 0;\n\tvoid *payload = NULL;\n\tu32 payload_len = 0;\n\n\tceph_decode_8_safe(&p, end, proto_ver, bad);\n\tceph_decode_8_safe(&p, end, opcode, bad);\n\tceph_decode_64_safe(&p, end, cookie, bad);\n\tp += 8;  \n\tceph_decode_64_safe(&p, end, notify_id, bad);\n\n\tif (proto_ver >= 1) {\n\t\tceph_decode_32_safe(&p, end, payload_len, bad);\n\t\tceph_decode_need(&p, end, payload_len, bad);\n\t\tpayload = p;\n\t\tp += payload_len;\n\t}\n\n\tif (le16_to_cpu(msg->hdr.version) >= 2)\n\t\tceph_decode_32_safe(&p, end, return_code, bad);\n\n\tif (le16_to_cpu(msg->hdr.version) >= 3)\n\t\tceph_decode_64_safe(&p, end, notifier_id, bad);\n\n\tdown_read(&osdc->lock);\n\tlreq = lookup_linger_osdc(&osdc->linger_requests, cookie);\n\tif (!lreq) {\n\t\tdout(\"%s opcode %d cookie %llu dne\\n\", __func__, opcode,\n\t\t     cookie);\n\t\tgoto out_unlock_osdc;\n\t}\n\n\tmutex_lock(&lreq->lock);\n\tdout(\"%s opcode %d cookie %llu lreq %p is_watch %d\\n\", __func__,\n\t     opcode, cookie, lreq, lreq->is_watch);\n\tif (opcode == CEPH_WATCH_EVENT_DISCONNECT) {\n\t\tif (!lreq->last_error) {\n\t\t\tlreq->last_error = -ENOTCONN;\n\t\t\tqueue_watch_error(lreq);\n\t\t}\n\t} else if (!lreq->is_watch) {\n\t\t \n\t\tif (lreq->notify_id && lreq->notify_id != notify_id) {\n\t\t\tdout(\"lreq %p notify_id %llu != %llu, ignoring\\n\", lreq,\n\t\t\t     lreq->notify_id, notify_id);\n\t\t} else if (!completion_done(&lreq->notify_finish_wait)) {\n\t\t\tstruct ceph_msg_data *data =\n\t\t\t    msg->num_data_items ? &msg->data[0] : NULL;\n\n\t\t\tif (data) {\n\t\t\t\tif (lreq->preply_pages) {\n\t\t\t\t\tWARN_ON(data->type !=\n\t\t\t\t\t\t\tCEPH_MSG_DATA_PAGES);\n\t\t\t\t\t*lreq->preply_pages = data->pages;\n\t\t\t\t\t*lreq->preply_len = data->length;\n\t\t\t\t\tdata->own_pages = false;\n\t\t\t\t}\n\t\t\t}\n\t\t\tlreq->notify_finish_error = return_code;\n\t\t\tcomplete_all(&lreq->notify_finish_wait);\n\t\t}\n\t} else {\n\t\t \n\t\tlwork = lwork_alloc(lreq, do_watch_notify);\n\t\tif (!lwork) {\n\t\t\tpr_err(\"failed to allocate notify-lwork\\n\");\n\t\t\tgoto out_unlock_lreq;\n\t\t}\n\n\t\tlwork->notify.notify_id = notify_id;\n\t\tlwork->notify.notifier_id = notifier_id;\n\t\tlwork->notify.payload = payload;\n\t\tlwork->notify.payload_len = payload_len;\n\t\tlwork->notify.msg = ceph_msg_get(msg);\n\t\tlwork_queue(lwork);\n\t}\n\nout_unlock_lreq:\n\tmutex_unlock(&lreq->lock);\nout_unlock_osdc:\n\tup_read(&osdc->lock);\n\treturn;\n\nbad:\n\tpr_err(\"osdc handle_watch_notify corrupt msg\\n\");\n}\n\n \nvoid ceph_osdc_start_request(struct ceph_osd_client *osdc,\n\t\t\t     struct ceph_osd_request *req)\n{\n\tdown_read(&osdc->lock);\n\tsubmit_request(req, false);\n\tup_read(&osdc->lock);\n}\nEXPORT_SYMBOL(ceph_osdc_start_request);\n\n \nvoid ceph_osdc_cancel_request(struct ceph_osd_request *req)\n{\n\tstruct ceph_osd_client *osdc = req->r_osdc;\n\n\tdown_write(&osdc->lock);\n\tif (req->r_osd)\n\t\tcancel_request(req);\n\tup_write(&osdc->lock);\n}\nEXPORT_SYMBOL(ceph_osdc_cancel_request);\n\n \nstatic int wait_request_timeout(struct ceph_osd_request *req,\n\t\t\t\tunsigned long timeout)\n{\n\tlong left;\n\n\tdout(\"%s req %p tid %llu\\n\", __func__, req, req->r_tid);\n\tleft = wait_for_completion_killable_timeout(&req->r_completion,\n\t\t\t\t\t\tceph_timeout_jiffies(timeout));\n\tif (left <= 0) {\n\t\tleft = left ?: -ETIMEDOUT;\n\t\tceph_osdc_cancel_request(req);\n\t} else {\n\t\tleft = req->r_result;  \n\t}\n\n\treturn left;\n}\n\n \nint ceph_osdc_wait_request(struct ceph_osd_client *osdc,\n\t\t\t   struct ceph_osd_request *req)\n{\n\treturn wait_request_timeout(req, 0);\n}\nEXPORT_SYMBOL(ceph_osdc_wait_request);\n\n \nvoid ceph_osdc_sync(struct ceph_osd_client *osdc)\n{\n\tstruct rb_node *n, *p;\n\tu64 last_tid = atomic64_read(&osdc->last_tid);\n\nagain:\n\tdown_read(&osdc->lock);\n\tfor (n = rb_first(&osdc->osds); n; n = rb_next(n)) {\n\t\tstruct ceph_osd *osd = rb_entry(n, struct ceph_osd, o_node);\n\n\t\tmutex_lock(&osd->lock);\n\t\tfor (p = rb_first(&osd->o_requests); p; p = rb_next(p)) {\n\t\t\tstruct ceph_osd_request *req =\n\t\t\t    rb_entry(p, struct ceph_osd_request, r_node);\n\n\t\t\tif (req->r_tid > last_tid)\n\t\t\t\tbreak;\n\n\t\t\tif (!(req->r_flags & CEPH_OSD_FLAG_WRITE))\n\t\t\t\tcontinue;\n\n\t\t\tceph_osdc_get_request(req);\n\t\t\tmutex_unlock(&osd->lock);\n\t\t\tup_read(&osdc->lock);\n\t\t\tdout(\"%s waiting on req %p tid %llu last_tid %llu\\n\",\n\t\t\t     __func__, req, req->r_tid, last_tid);\n\t\t\twait_for_completion(&req->r_completion);\n\t\t\tceph_osdc_put_request(req);\n\t\t\tgoto again;\n\t\t}\n\n\t\tmutex_unlock(&osd->lock);\n\t}\n\n\tup_read(&osdc->lock);\n\tdout(\"%s done last_tid %llu\\n\", __func__, last_tid);\n}\nEXPORT_SYMBOL(ceph_osdc_sync);\n\n \nstruct ceph_osd_linger_request *\nceph_osdc_watch(struct ceph_osd_client *osdc,\n\t\tstruct ceph_object_id *oid,\n\t\tstruct ceph_object_locator *oloc,\n\t\trados_watchcb2_t wcb,\n\t\trados_watcherrcb_t errcb,\n\t\tvoid *data)\n{\n\tstruct ceph_osd_linger_request *lreq;\n\tint ret;\n\n\tlreq = linger_alloc(osdc);\n\tif (!lreq)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tlreq->is_watch = true;\n\tlreq->wcb = wcb;\n\tlreq->errcb = errcb;\n\tlreq->data = data;\n\tlreq->watch_valid_thru = jiffies;\n\n\tceph_oid_copy(&lreq->t.base_oid, oid);\n\tceph_oloc_copy(&lreq->t.base_oloc, oloc);\n\tlreq->t.flags = CEPH_OSD_FLAG_WRITE;\n\tktime_get_real_ts64(&lreq->mtime);\n\n\tlinger_submit(lreq);\n\tret = linger_reg_commit_wait(lreq);\n\tif (ret) {\n\t\tlinger_cancel(lreq);\n\t\tgoto err_put_lreq;\n\t}\n\n\treturn lreq;\n\nerr_put_lreq:\n\tlinger_put(lreq);\n\treturn ERR_PTR(ret);\n}\nEXPORT_SYMBOL(ceph_osdc_watch);\n\n \nint ceph_osdc_unwatch(struct ceph_osd_client *osdc,\n\t\t      struct ceph_osd_linger_request *lreq)\n{\n\tstruct ceph_options *opts = osdc->client->options;\n\tstruct ceph_osd_request *req;\n\tint ret;\n\n\treq = ceph_osdc_alloc_request(osdc, NULL, 1, false, GFP_NOIO);\n\tif (!req)\n\t\treturn -ENOMEM;\n\n\tceph_oid_copy(&req->r_base_oid, &lreq->t.base_oid);\n\tceph_oloc_copy(&req->r_base_oloc, &lreq->t.base_oloc);\n\treq->r_flags = CEPH_OSD_FLAG_WRITE;\n\tktime_get_real_ts64(&req->r_mtime);\n\tosd_req_op_watch_init(req, 0, CEPH_OSD_WATCH_OP_UNWATCH,\n\t\t\t      lreq->linger_id, 0);\n\n\tret = ceph_osdc_alloc_messages(req, GFP_NOIO);\n\tif (ret)\n\t\tgoto out_put_req;\n\n\tceph_osdc_start_request(osdc, req);\n\tlinger_cancel(lreq);\n\tlinger_put(lreq);\n\tret = wait_request_timeout(req, opts->mount_timeout);\n\nout_put_req:\n\tceph_osdc_put_request(req);\n\treturn ret;\n}\nEXPORT_SYMBOL(ceph_osdc_unwatch);\n\nstatic int osd_req_op_notify_ack_init(struct ceph_osd_request *req, int which,\n\t\t\t\t      u64 notify_id, u64 cookie, void *payload,\n\t\t\t\t      u32 payload_len)\n{\n\tstruct ceph_osd_req_op *op;\n\tstruct ceph_pagelist *pl;\n\tint ret;\n\n\top = osd_req_op_init(req, which, CEPH_OSD_OP_NOTIFY_ACK, 0);\n\n\tpl = ceph_pagelist_alloc(GFP_NOIO);\n\tif (!pl)\n\t\treturn -ENOMEM;\n\n\tret = ceph_pagelist_encode_64(pl, notify_id);\n\tret |= ceph_pagelist_encode_64(pl, cookie);\n\tif (payload) {\n\t\tret |= ceph_pagelist_encode_32(pl, payload_len);\n\t\tret |= ceph_pagelist_append(pl, payload, payload_len);\n\t} else {\n\t\tret |= ceph_pagelist_encode_32(pl, 0);\n\t}\n\tif (ret) {\n\t\tceph_pagelist_release(pl);\n\t\treturn -ENOMEM;\n\t}\n\n\tceph_osd_data_pagelist_init(&op->notify_ack.request_data, pl);\n\top->indata_len = pl->length;\n\treturn 0;\n}\n\nint ceph_osdc_notify_ack(struct ceph_osd_client *osdc,\n\t\t\t struct ceph_object_id *oid,\n\t\t\t struct ceph_object_locator *oloc,\n\t\t\t u64 notify_id,\n\t\t\t u64 cookie,\n\t\t\t void *payload,\n\t\t\t u32 payload_len)\n{\n\tstruct ceph_osd_request *req;\n\tint ret;\n\n\treq = ceph_osdc_alloc_request(osdc, NULL, 1, false, GFP_NOIO);\n\tif (!req)\n\t\treturn -ENOMEM;\n\n\tceph_oid_copy(&req->r_base_oid, oid);\n\tceph_oloc_copy(&req->r_base_oloc, oloc);\n\treq->r_flags = CEPH_OSD_FLAG_READ;\n\n\tret = osd_req_op_notify_ack_init(req, 0, notify_id, cookie, payload,\n\t\t\t\t\t payload_len);\n\tif (ret)\n\t\tgoto out_put_req;\n\n\tret = ceph_osdc_alloc_messages(req, GFP_NOIO);\n\tif (ret)\n\t\tgoto out_put_req;\n\n\tceph_osdc_start_request(osdc, req);\n\tret = ceph_osdc_wait_request(osdc, req);\n\nout_put_req:\n\tceph_osdc_put_request(req);\n\treturn ret;\n}\nEXPORT_SYMBOL(ceph_osdc_notify_ack);\n\n \nint ceph_osdc_notify(struct ceph_osd_client *osdc,\n\t\t     struct ceph_object_id *oid,\n\t\t     struct ceph_object_locator *oloc,\n\t\t     void *payload,\n\t\t     u32 payload_len,\n\t\t     u32 timeout,\n\t\t     struct page ***preply_pages,\n\t\t     size_t *preply_len)\n{\n\tstruct ceph_osd_linger_request *lreq;\n\tint ret;\n\n\tWARN_ON(!timeout);\n\tif (preply_pages) {\n\t\t*preply_pages = NULL;\n\t\t*preply_len = 0;\n\t}\n\n\tlreq = linger_alloc(osdc);\n\tif (!lreq)\n\t\treturn -ENOMEM;\n\n\tlreq->request_pl = ceph_pagelist_alloc(GFP_NOIO);\n\tif (!lreq->request_pl) {\n\t\tret = -ENOMEM;\n\t\tgoto out_put_lreq;\n\t}\n\n\tret = ceph_pagelist_encode_32(lreq->request_pl, 1);  \n\tret |= ceph_pagelist_encode_32(lreq->request_pl, timeout);\n\tret |= ceph_pagelist_encode_32(lreq->request_pl, payload_len);\n\tret |= ceph_pagelist_append(lreq->request_pl, payload, payload_len);\n\tif (ret) {\n\t\tret = -ENOMEM;\n\t\tgoto out_put_lreq;\n\t}\n\n\t \n\tlreq->notify_id_pages = ceph_alloc_page_vector(1, GFP_NOIO);\n\tif (IS_ERR(lreq->notify_id_pages)) {\n\t\tret = PTR_ERR(lreq->notify_id_pages);\n\t\tlreq->notify_id_pages = NULL;\n\t\tgoto out_put_lreq;\n\t}\n\n\tlreq->preply_pages = preply_pages;\n\tlreq->preply_len = preply_len;\n\n\tceph_oid_copy(&lreq->t.base_oid, oid);\n\tceph_oloc_copy(&lreq->t.base_oloc, oloc);\n\tlreq->t.flags = CEPH_OSD_FLAG_READ;\n\n\tlinger_submit(lreq);\n\tret = linger_reg_commit_wait(lreq);\n\tif (!ret)\n\t\tret = linger_notify_finish_wait(lreq,\n\t\t\t\t msecs_to_jiffies(2 * timeout * MSEC_PER_SEC));\n\telse\n\t\tdout(\"lreq %p failed to initiate notify %d\\n\", lreq, ret);\n\n\tlinger_cancel(lreq);\nout_put_lreq:\n\tlinger_put(lreq);\n\treturn ret;\n}\nEXPORT_SYMBOL(ceph_osdc_notify);\n\n \nint ceph_osdc_watch_check(struct ceph_osd_client *osdc,\n\t\t\t  struct ceph_osd_linger_request *lreq)\n{\n\tunsigned long stamp, age;\n\tint ret;\n\n\tdown_read(&osdc->lock);\n\tmutex_lock(&lreq->lock);\n\tstamp = lreq->watch_valid_thru;\n\tif (!list_empty(&lreq->pending_lworks)) {\n\t\tstruct linger_work *lwork =\n\t\t    list_first_entry(&lreq->pending_lworks,\n\t\t\t\t     struct linger_work,\n\t\t\t\t     pending_item);\n\n\t\tif (time_before(lwork->queued_stamp, stamp))\n\t\t\tstamp = lwork->queued_stamp;\n\t}\n\tage = jiffies - stamp;\n\tdout(\"%s lreq %p linger_id %llu age %lu last_error %d\\n\", __func__,\n\t     lreq, lreq->linger_id, age, lreq->last_error);\n\t \n\tret = lreq->last_error ?: 1 + jiffies_to_msecs(age);\n\n\tmutex_unlock(&lreq->lock);\n\tup_read(&osdc->lock);\n\treturn ret;\n}\n\nstatic int decode_watcher(void **p, void *end, struct ceph_watch_item *item)\n{\n\tu8 struct_v;\n\tu32 struct_len;\n\tint ret;\n\n\tret = ceph_start_decoding(p, end, 2, \"watch_item_t\",\n\t\t\t\t  &struct_v, &struct_len);\n\tif (ret)\n\t\tgoto bad;\n\n\tret = -EINVAL;\n\tceph_decode_copy_safe(p, end, &item->name, sizeof(item->name), bad);\n\tceph_decode_64_safe(p, end, item->cookie, bad);\n\tceph_decode_skip_32(p, end, bad);  \n\n\tif (struct_v >= 2) {\n\t\tret = ceph_decode_entity_addr(p, end, &item->addr);\n\t\tif (ret)\n\t\t\tgoto bad;\n\t} else {\n\t\tret = 0;\n\t}\n\n\tdout(\"%s %s%llu cookie %llu addr %s\\n\", __func__,\n\t     ENTITY_NAME(item->name), item->cookie,\n\t     ceph_pr_addr(&item->addr));\nbad:\n\treturn ret;\n}\n\nstatic int decode_watchers(void **p, void *end,\n\t\t\t   struct ceph_watch_item **watchers,\n\t\t\t   u32 *num_watchers)\n{\n\tu8 struct_v;\n\tu32 struct_len;\n\tint i;\n\tint ret;\n\n\tret = ceph_start_decoding(p, end, 1, \"obj_list_watch_response_t\",\n\t\t\t\t  &struct_v, &struct_len);\n\tif (ret)\n\t\treturn ret;\n\n\t*num_watchers = ceph_decode_32(p);\n\t*watchers = kcalloc(*num_watchers, sizeof(**watchers), GFP_NOIO);\n\tif (!*watchers)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < *num_watchers; i++) {\n\t\tret = decode_watcher(p, end, *watchers + i);\n\t\tif (ret) {\n\t\t\tkfree(*watchers);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nint ceph_osdc_list_watchers(struct ceph_osd_client *osdc,\n\t\t\t    struct ceph_object_id *oid,\n\t\t\t    struct ceph_object_locator *oloc,\n\t\t\t    struct ceph_watch_item **watchers,\n\t\t\t    u32 *num_watchers)\n{\n\tstruct ceph_osd_request *req;\n\tstruct page **pages;\n\tint ret;\n\n\treq = ceph_osdc_alloc_request(osdc, NULL, 1, false, GFP_NOIO);\n\tif (!req)\n\t\treturn -ENOMEM;\n\n\tceph_oid_copy(&req->r_base_oid, oid);\n\tceph_oloc_copy(&req->r_base_oloc, oloc);\n\treq->r_flags = CEPH_OSD_FLAG_READ;\n\n\tpages = ceph_alloc_page_vector(1, GFP_NOIO);\n\tif (IS_ERR(pages)) {\n\t\tret = PTR_ERR(pages);\n\t\tgoto out_put_req;\n\t}\n\n\tosd_req_op_init(req, 0, CEPH_OSD_OP_LIST_WATCHERS, 0);\n\tceph_osd_data_pages_init(osd_req_op_data(req, 0, list_watchers,\n\t\t\t\t\t\t response_data),\n\t\t\t\t pages, PAGE_SIZE, 0, false, true);\n\n\tret = ceph_osdc_alloc_messages(req, GFP_NOIO);\n\tif (ret)\n\t\tgoto out_put_req;\n\n\tceph_osdc_start_request(osdc, req);\n\tret = ceph_osdc_wait_request(osdc, req);\n\tif (ret >= 0) {\n\t\tvoid *p = page_address(pages[0]);\n\t\tvoid *const end = p + req->r_ops[0].outdata_len;\n\n\t\tret = decode_watchers(&p, end, watchers, num_watchers);\n\t}\n\nout_put_req:\n\tceph_osdc_put_request(req);\n\treturn ret;\n}\nEXPORT_SYMBOL(ceph_osdc_list_watchers);\n\n \nvoid ceph_osdc_flush_notifies(struct ceph_osd_client *osdc)\n{\n\tdout(\"%s osdc %p\\n\", __func__, osdc);\n\tflush_workqueue(osdc->notify_wq);\n}\nEXPORT_SYMBOL(ceph_osdc_flush_notifies);\n\nvoid ceph_osdc_maybe_request_map(struct ceph_osd_client *osdc)\n{\n\tdown_read(&osdc->lock);\n\tmaybe_request_map(osdc);\n\tup_read(&osdc->lock);\n}\nEXPORT_SYMBOL(ceph_osdc_maybe_request_map);\n\n \nint ceph_osdc_call(struct ceph_osd_client *osdc,\n\t\t   struct ceph_object_id *oid,\n\t\t   struct ceph_object_locator *oloc,\n\t\t   const char *class, const char *method,\n\t\t   unsigned int flags,\n\t\t   struct page *req_page, size_t req_len,\n\t\t   struct page **resp_pages, size_t *resp_len)\n{\n\tstruct ceph_osd_request *req;\n\tint ret;\n\n\tif (req_len > PAGE_SIZE)\n\t\treturn -E2BIG;\n\n\treq = ceph_osdc_alloc_request(osdc, NULL, 1, false, GFP_NOIO);\n\tif (!req)\n\t\treturn -ENOMEM;\n\n\tceph_oid_copy(&req->r_base_oid, oid);\n\tceph_oloc_copy(&req->r_base_oloc, oloc);\n\treq->r_flags = flags;\n\n\tret = osd_req_op_cls_init(req, 0, class, method);\n\tif (ret)\n\t\tgoto out_put_req;\n\n\tif (req_page)\n\t\tosd_req_op_cls_request_data_pages(req, 0, &req_page, req_len,\n\t\t\t\t\t\t  0, false, false);\n\tif (resp_pages)\n\t\tosd_req_op_cls_response_data_pages(req, 0, resp_pages,\n\t\t\t\t\t\t   *resp_len, 0, false, false);\n\n\tret = ceph_osdc_alloc_messages(req, GFP_NOIO);\n\tif (ret)\n\t\tgoto out_put_req;\n\n\tceph_osdc_start_request(osdc, req);\n\tret = ceph_osdc_wait_request(osdc, req);\n\tif (ret >= 0) {\n\t\tret = req->r_ops[0].rval;\n\t\tif (resp_pages)\n\t\t\t*resp_len = req->r_ops[0].outdata_len;\n\t}\n\nout_put_req:\n\tceph_osdc_put_request(req);\n\treturn ret;\n}\nEXPORT_SYMBOL(ceph_osdc_call);\n\n \nvoid ceph_osdc_reopen_osds(struct ceph_osd_client *osdc)\n{\n\tstruct rb_node *n;\n\n\tdown_write(&osdc->lock);\n\tfor (n = rb_first(&osdc->osds); n; ) {\n\t\tstruct ceph_osd *osd = rb_entry(n, struct ceph_osd, o_node);\n\n\t\tn = rb_next(n);\n\t\tif (!reopen_osd(osd))\n\t\t\tkick_osd_requests(osd);\n\t}\n\tup_write(&osdc->lock);\n}\n\n \nint ceph_osdc_init(struct ceph_osd_client *osdc, struct ceph_client *client)\n{\n\tint err;\n\n\tdout(\"init\\n\");\n\tosdc->client = client;\n\tinit_rwsem(&osdc->lock);\n\tosdc->osds = RB_ROOT;\n\tINIT_LIST_HEAD(&osdc->osd_lru);\n\tspin_lock_init(&osdc->osd_lru_lock);\n\tosd_init(&osdc->homeless_osd);\n\tosdc->homeless_osd.o_osdc = osdc;\n\tosdc->homeless_osd.o_osd = CEPH_HOMELESS_OSD;\n\tosdc->last_linger_id = CEPH_LINGER_ID_START;\n\tosdc->linger_requests = RB_ROOT;\n\tosdc->map_checks = RB_ROOT;\n\tosdc->linger_map_checks = RB_ROOT;\n\tINIT_DELAYED_WORK(&osdc->timeout_work, handle_timeout);\n\tINIT_DELAYED_WORK(&osdc->osds_timeout_work, handle_osds_timeout);\n\n\terr = -ENOMEM;\n\tosdc->osdmap = ceph_osdmap_alloc();\n\tif (!osdc->osdmap)\n\t\tgoto out;\n\n\tosdc->req_mempool = mempool_create_slab_pool(10,\n\t\t\t\t\t\t     ceph_osd_request_cache);\n\tif (!osdc->req_mempool)\n\t\tgoto out_map;\n\n\terr = ceph_msgpool_init(&osdc->msgpool_op, CEPH_MSG_OSD_OP,\n\t\t\t\tPAGE_SIZE, CEPH_OSD_SLAB_OPS, 10, \"osd_op\");\n\tif (err < 0)\n\t\tgoto out_mempool;\n\terr = ceph_msgpool_init(&osdc->msgpool_op_reply, CEPH_MSG_OSD_OPREPLY,\n\t\t\t\tPAGE_SIZE, CEPH_OSD_SLAB_OPS, 10,\n\t\t\t\t\"osd_op_reply\");\n\tif (err < 0)\n\t\tgoto out_msgpool;\n\n\terr = -ENOMEM;\n\tosdc->notify_wq = create_singlethread_workqueue(\"ceph-watch-notify\");\n\tif (!osdc->notify_wq)\n\t\tgoto out_msgpool_reply;\n\n\tosdc->completion_wq = create_singlethread_workqueue(\"ceph-completion\");\n\tif (!osdc->completion_wq)\n\t\tgoto out_notify_wq;\n\n\tschedule_delayed_work(&osdc->timeout_work,\n\t\t\t      osdc->client->options->osd_keepalive_timeout);\n\tschedule_delayed_work(&osdc->osds_timeout_work,\n\t    round_jiffies_relative(osdc->client->options->osd_idle_ttl));\n\n\treturn 0;\n\nout_notify_wq:\n\tdestroy_workqueue(osdc->notify_wq);\nout_msgpool_reply:\n\tceph_msgpool_destroy(&osdc->msgpool_op_reply);\nout_msgpool:\n\tceph_msgpool_destroy(&osdc->msgpool_op);\nout_mempool:\n\tmempool_destroy(osdc->req_mempool);\nout_map:\n\tceph_osdmap_destroy(osdc->osdmap);\nout:\n\treturn err;\n}\n\nvoid ceph_osdc_stop(struct ceph_osd_client *osdc)\n{\n\tdestroy_workqueue(osdc->completion_wq);\n\tdestroy_workqueue(osdc->notify_wq);\n\tcancel_delayed_work_sync(&osdc->timeout_work);\n\tcancel_delayed_work_sync(&osdc->osds_timeout_work);\n\n\tdown_write(&osdc->lock);\n\twhile (!RB_EMPTY_ROOT(&osdc->osds)) {\n\t\tstruct ceph_osd *osd = rb_entry(rb_first(&osdc->osds),\n\t\t\t\t\t\tstruct ceph_osd, o_node);\n\t\tclose_osd(osd);\n\t}\n\tup_write(&osdc->lock);\n\tWARN_ON(refcount_read(&osdc->homeless_osd.o_ref) != 1);\n\tosd_cleanup(&osdc->homeless_osd);\n\n\tWARN_ON(!list_empty(&osdc->osd_lru));\n\tWARN_ON(!RB_EMPTY_ROOT(&osdc->linger_requests));\n\tWARN_ON(!RB_EMPTY_ROOT(&osdc->map_checks));\n\tWARN_ON(!RB_EMPTY_ROOT(&osdc->linger_map_checks));\n\tWARN_ON(atomic_read(&osdc->num_requests));\n\tWARN_ON(atomic_read(&osdc->num_homeless));\n\n\tceph_osdmap_destroy(osdc->osdmap);\n\tmempool_destroy(osdc->req_mempool);\n\tceph_msgpool_destroy(&osdc->msgpool_op);\n\tceph_msgpool_destroy(&osdc->msgpool_op_reply);\n}\n\nint osd_req_op_copy_from_init(struct ceph_osd_request *req,\n\t\t\t      u64 src_snapid, u64 src_version,\n\t\t\t      struct ceph_object_id *src_oid,\n\t\t\t      struct ceph_object_locator *src_oloc,\n\t\t\t      u32 src_fadvise_flags,\n\t\t\t      u32 dst_fadvise_flags,\n\t\t\t      u32 truncate_seq, u64 truncate_size,\n\t\t\t      u8 copy_from_flags)\n{\n\tstruct ceph_osd_req_op *op;\n\tstruct page **pages;\n\tvoid *p, *end;\n\n\tpages = ceph_alloc_page_vector(1, GFP_KERNEL);\n\tif (IS_ERR(pages))\n\t\treturn PTR_ERR(pages);\n\n\top = osd_req_op_init(req, 0, CEPH_OSD_OP_COPY_FROM2,\n\t\t\t     dst_fadvise_flags);\n\top->copy_from.snapid = src_snapid;\n\top->copy_from.src_version = src_version;\n\top->copy_from.flags = copy_from_flags;\n\top->copy_from.src_fadvise_flags = src_fadvise_flags;\n\n\tp = page_address(pages[0]);\n\tend = p + PAGE_SIZE;\n\tceph_encode_string(&p, end, src_oid->name, src_oid->name_len);\n\tencode_oloc(&p, end, src_oloc);\n\tceph_encode_32(&p, truncate_seq);\n\tceph_encode_64(&p, truncate_size);\n\top->indata_len = PAGE_SIZE - (end - p);\n\n\tceph_osd_data_pages_init(&op->copy_from.osd_data, pages,\n\t\t\t\t op->indata_len, 0, false, true);\n\treturn 0;\n}\nEXPORT_SYMBOL(osd_req_op_copy_from_init);\n\nint __init ceph_osdc_setup(void)\n{\n\tsize_t size = sizeof(struct ceph_osd_request) +\n\t    CEPH_OSD_SLAB_OPS * sizeof(struct ceph_osd_req_op);\n\n\tBUG_ON(ceph_osd_request_cache);\n\tceph_osd_request_cache = kmem_cache_create(\"ceph_osd_request\", size,\n\t\t\t\t\t\t   0, 0, NULL);\n\n\treturn ceph_osd_request_cache ? 0 : -ENOMEM;\n}\n\nvoid ceph_osdc_cleanup(void)\n{\n\tBUG_ON(!ceph_osd_request_cache);\n\tkmem_cache_destroy(ceph_osd_request_cache);\n\tceph_osd_request_cache = NULL;\n}\n\n \nstatic void osd_dispatch(struct ceph_connection *con, struct ceph_msg *msg)\n{\n\tstruct ceph_osd *osd = con->private;\n\tstruct ceph_osd_client *osdc = osd->o_osdc;\n\tint type = le16_to_cpu(msg->hdr.type);\n\n\tswitch (type) {\n\tcase CEPH_MSG_OSD_MAP:\n\t\tceph_osdc_handle_map(osdc, msg);\n\t\tbreak;\n\tcase CEPH_MSG_OSD_OPREPLY:\n\t\thandle_reply(osd, msg);\n\t\tbreak;\n\tcase CEPH_MSG_OSD_BACKOFF:\n\t\thandle_backoff(osd, msg);\n\t\tbreak;\n\tcase CEPH_MSG_WATCH_NOTIFY:\n\t\thandle_watch_notify(osdc, msg);\n\t\tbreak;\n\n\tdefault:\n\t\tpr_err(\"received unknown message type %d %s\\n\", type,\n\t\t       ceph_msg_type_name(type));\n\t}\n\n\tceph_msg_put(msg);\n}\n\n \nstatic u64 sparse_data_requested(struct ceph_osd_request *req)\n{\n\tu64 len = 0;\n\n\tif (req->r_flags & CEPH_OSD_FLAG_READ) {\n\t\tint i;\n\n\t\tfor (i = 0; i < req->r_num_ops; ++i) {\n\t\t\tstruct ceph_osd_req_op *op = &req->r_ops[i];\n\n\t\t\tif (op->op == CEPH_OSD_OP_SPARSE_READ)\n\t\t\t\tlen += op->extent.length;\n\t\t}\n\t}\n\treturn len;\n}\n\n \nstatic struct ceph_msg *get_reply(struct ceph_connection *con,\n\t\t\t\t  struct ceph_msg_header *hdr,\n\t\t\t\t  int *skip)\n{\n\tstruct ceph_osd *osd = con->private;\n\tstruct ceph_osd_client *osdc = osd->o_osdc;\n\tstruct ceph_msg *m = NULL;\n\tstruct ceph_osd_request *req;\n\tint front_len = le32_to_cpu(hdr->front_len);\n\tint data_len = le32_to_cpu(hdr->data_len);\n\tu64 tid = le64_to_cpu(hdr->tid);\n\tu64 srlen;\n\n\tdown_read(&osdc->lock);\n\tif (!osd_registered(osd)) {\n\t\tdout(\"%s osd%d unknown, skipping\\n\", __func__, osd->o_osd);\n\t\t*skip = 1;\n\t\tgoto out_unlock_osdc;\n\t}\n\tWARN_ON(osd->o_osd != le64_to_cpu(hdr->src.num));\n\n\tmutex_lock(&osd->lock);\n\treq = lookup_request(&osd->o_requests, tid);\n\tif (!req) {\n\t\tdout(\"%s osd%d tid %llu unknown, skipping\\n\", __func__,\n\t\t     osd->o_osd, tid);\n\t\t*skip = 1;\n\t\tgoto out_unlock_session;\n\t}\n\n\tceph_msg_revoke_incoming(req->r_reply);\n\n\tif (front_len > req->r_reply->front_alloc_len) {\n\t\tpr_warn(\"%s osd%d tid %llu front %d > preallocated %d\\n\",\n\t\t\t__func__, osd->o_osd, req->r_tid, front_len,\n\t\t\treq->r_reply->front_alloc_len);\n\t\tm = ceph_msg_new(CEPH_MSG_OSD_OPREPLY, front_len, GFP_NOFS,\n\t\t\t\t false);\n\t\tif (!m)\n\t\t\tgoto out_unlock_session;\n\t\tceph_msg_put(req->r_reply);\n\t\treq->r_reply = m;\n\t}\n\n\tsrlen = sparse_data_requested(req);\n\tif (!srlen && data_len > req->r_reply->data_length) {\n\t\tpr_warn(\"%s osd%d tid %llu data %d > preallocated %zu, skipping\\n\",\n\t\t\t__func__, osd->o_osd, req->r_tid, data_len,\n\t\t\treq->r_reply->data_length);\n\t\tm = NULL;\n\t\t*skip = 1;\n\t\tgoto out_unlock_session;\n\t}\n\n\tm = ceph_msg_get(req->r_reply);\n\tm->sparse_read = (bool)srlen;\n\n\tdout(\"get_reply tid %lld %p\\n\", tid, m);\n\nout_unlock_session:\n\tmutex_unlock(&osd->lock);\nout_unlock_osdc:\n\tup_read(&osdc->lock);\n\treturn m;\n}\n\nstatic struct ceph_msg *alloc_msg_with_page_vector(struct ceph_msg_header *hdr)\n{\n\tstruct ceph_msg *m;\n\tint type = le16_to_cpu(hdr->type);\n\tu32 front_len = le32_to_cpu(hdr->front_len);\n\tu32 data_len = le32_to_cpu(hdr->data_len);\n\n\tm = ceph_msg_new2(type, front_len, 1, GFP_NOIO, false);\n\tif (!m)\n\t\treturn NULL;\n\n\tif (data_len) {\n\t\tstruct page **pages;\n\n\t\tpages = ceph_alloc_page_vector(calc_pages_for(0, data_len),\n\t\t\t\t\t       GFP_NOIO);\n\t\tif (IS_ERR(pages)) {\n\t\t\tceph_msg_put(m);\n\t\t\treturn NULL;\n\t\t}\n\n\t\tceph_msg_data_add_pages(m, pages, data_len, 0, true);\n\t}\n\n\treturn m;\n}\n\nstatic struct ceph_msg *osd_alloc_msg(struct ceph_connection *con,\n\t\t\t\t      struct ceph_msg_header *hdr,\n\t\t\t\t      int *skip)\n{\n\tstruct ceph_osd *osd = con->private;\n\tint type = le16_to_cpu(hdr->type);\n\n\t*skip = 0;\n\tswitch (type) {\n\tcase CEPH_MSG_OSD_MAP:\n\tcase CEPH_MSG_OSD_BACKOFF:\n\tcase CEPH_MSG_WATCH_NOTIFY:\n\t\treturn alloc_msg_with_page_vector(hdr);\n\tcase CEPH_MSG_OSD_OPREPLY:\n\t\treturn get_reply(con, hdr, skip);\n\tdefault:\n\t\tpr_warn(\"%s osd%d unknown msg type %d, skipping\\n\", __func__,\n\t\t\tosd->o_osd, type);\n\t\t*skip = 1;\n\t\treturn NULL;\n\t}\n}\n\n \nstatic struct ceph_connection *osd_get_con(struct ceph_connection *con)\n{\n\tstruct ceph_osd *osd = con->private;\n\tif (get_osd(osd))\n\t\treturn con;\n\treturn NULL;\n}\n\nstatic void osd_put_con(struct ceph_connection *con)\n{\n\tstruct ceph_osd *osd = con->private;\n\tput_osd(osd);\n}\n\n \n\n \nstatic struct ceph_auth_handshake *\nosd_get_authorizer(struct ceph_connection *con, int *proto, int force_new)\n{\n\tstruct ceph_osd *o = con->private;\n\tstruct ceph_osd_client *osdc = o->o_osdc;\n\tstruct ceph_auth_client *ac = osdc->client->monc.auth;\n\tstruct ceph_auth_handshake *auth = &o->o_auth;\n\tint ret;\n\n\tret = __ceph_auth_get_authorizer(ac, auth, CEPH_ENTITY_TYPE_OSD,\n\t\t\t\t\t force_new, proto, NULL, NULL);\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\n\treturn auth;\n}\n\nstatic int osd_add_authorizer_challenge(struct ceph_connection *con,\n\t\t\t\t    void *challenge_buf, int challenge_buf_len)\n{\n\tstruct ceph_osd *o = con->private;\n\tstruct ceph_osd_client *osdc = o->o_osdc;\n\tstruct ceph_auth_client *ac = osdc->client->monc.auth;\n\n\treturn ceph_auth_add_authorizer_challenge(ac, o->o_auth.authorizer,\n\t\t\t\t\t    challenge_buf, challenge_buf_len);\n}\n\nstatic int osd_verify_authorizer_reply(struct ceph_connection *con)\n{\n\tstruct ceph_osd *o = con->private;\n\tstruct ceph_osd_client *osdc = o->o_osdc;\n\tstruct ceph_auth_client *ac = osdc->client->monc.auth;\n\tstruct ceph_auth_handshake *auth = &o->o_auth;\n\n\treturn ceph_auth_verify_authorizer_reply(ac, auth->authorizer,\n\t\tauth->authorizer_reply_buf, auth->authorizer_reply_buf_len,\n\t\tNULL, NULL, NULL, NULL);\n}\n\nstatic int osd_invalidate_authorizer(struct ceph_connection *con)\n{\n\tstruct ceph_osd *o = con->private;\n\tstruct ceph_osd_client *osdc = o->o_osdc;\n\tstruct ceph_auth_client *ac = osdc->client->monc.auth;\n\n\tceph_auth_invalidate_authorizer(ac, CEPH_ENTITY_TYPE_OSD);\n\treturn ceph_monc_validate_auth(&osdc->client->monc);\n}\n\nstatic int osd_get_auth_request(struct ceph_connection *con,\n\t\t\t\tvoid *buf, int *buf_len,\n\t\t\t\tvoid **authorizer, int *authorizer_len)\n{\n\tstruct ceph_osd *o = con->private;\n\tstruct ceph_auth_client *ac = o->o_osdc->client->monc.auth;\n\tstruct ceph_auth_handshake *auth = &o->o_auth;\n\tint ret;\n\n\tret = ceph_auth_get_authorizer(ac, auth, CEPH_ENTITY_TYPE_OSD,\n\t\t\t\t       buf, buf_len);\n\tif (ret)\n\t\treturn ret;\n\n\t*authorizer = auth->authorizer_buf;\n\t*authorizer_len = auth->authorizer_buf_len;\n\treturn 0;\n}\n\nstatic int osd_handle_auth_reply_more(struct ceph_connection *con,\n\t\t\t\t      void *reply, int reply_len,\n\t\t\t\t      void *buf, int *buf_len,\n\t\t\t\t      void **authorizer, int *authorizer_len)\n{\n\tstruct ceph_osd *o = con->private;\n\tstruct ceph_auth_client *ac = o->o_osdc->client->monc.auth;\n\tstruct ceph_auth_handshake *auth = &o->o_auth;\n\tint ret;\n\n\tret = ceph_auth_handle_svc_reply_more(ac, auth, reply, reply_len,\n\t\t\t\t\t      buf, buf_len);\n\tif (ret)\n\t\treturn ret;\n\n\t*authorizer = auth->authorizer_buf;\n\t*authorizer_len = auth->authorizer_buf_len;\n\treturn 0;\n}\n\nstatic int osd_handle_auth_done(struct ceph_connection *con,\n\t\t\t\tu64 global_id, void *reply, int reply_len,\n\t\t\t\tu8 *session_key, int *session_key_len,\n\t\t\t\tu8 *con_secret, int *con_secret_len)\n{\n\tstruct ceph_osd *o = con->private;\n\tstruct ceph_auth_client *ac = o->o_osdc->client->monc.auth;\n\tstruct ceph_auth_handshake *auth = &o->o_auth;\n\n\treturn ceph_auth_handle_svc_reply_done(ac, auth, reply, reply_len,\n\t\t\t\t\t       session_key, session_key_len,\n\t\t\t\t\t       con_secret, con_secret_len);\n}\n\nstatic int osd_handle_auth_bad_method(struct ceph_connection *con,\n\t\t\t\t      int used_proto, int result,\n\t\t\t\t      const int *allowed_protos, int proto_cnt,\n\t\t\t\t      const int *allowed_modes, int mode_cnt)\n{\n\tstruct ceph_osd *o = con->private;\n\tstruct ceph_mon_client *monc = &o->o_osdc->client->monc;\n\tint ret;\n\n\tif (ceph_auth_handle_bad_authorizer(monc->auth, CEPH_ENTITY_TYPE_OSD,\n\t\t\t\t\t    used_proto, result,\n\t\t\t\t\t    allowed_protos, proto_cnt,\n\t\t\t\t\t    allowed_modes, mode_cnt)) {\n\t\tret = ceph_monc_validate_auth(monc);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn -EACCES;\n}\n\nstatic void osd_reencode_message(struct ceph_msg *msg)\n{\n\tint type = le16_to_cpu(msg->hdr.type);\n\n\tif (type == CEPH_MSG_OSD_OP)\n\t\tencode_request_finish(msg);\n}\n\nstatic int osd_sign_message(struct ceph_msg *msg)\n{\n\tstruct ceph_osd *o = msg->con->private;\n\tstruct ceph_auth_handshake *auth = &o->o_auth;\n\n\treturn ceph_auth_sign_message(auth, msg);\n}\n\nstatic int osd_check_message_signature(struct ceph_msg *msg)\n{\n\tstruct ceph_osd *o = msg->con->private;\n\tstruct ceph_auth_handshake *auth = &o->o_auth;\n\n\treturn ceph_auth_check_message_signature(auth, msg);\n}\n\nstatic void advance_cursor(struct ceph_msg_data_cursor *cursor, size_t len,\n\t\t\t   bool zero)\n{\n\twhile (len) {\n\t\tstruct page *page;\n\t\tsize_t poff, plen;\n\n\t\tpage = ceph_msg_data_next(cursor, &poff, &plen);\n\t\tif (plen > len)\n\t\t\tplen = len;\n\t\tif (zero)\n\t\t\tzero_user_segment(page, poff, poff + plen);\n\t\tlen -= plen;\n\t\tceph_msg_data_advance(cursor, plen);\n\t}\n}\n\nstatic int prep_next_sparse_read(struct ceph_connection *con,\n\t\t\t\t struct ceph_msg_data_cursor *cursor)\n{\n\tstruct ceph_osd *o = con->private;\n\tstruct ceph_sparse_read *sr = &o->o_sparse_read;\n\tstruct ceph_osd_request *req;\n\tstruct ceph_osd_req_op *op;\n\n\tspin_lock(&o->o_requests_lock);\n\treq = lookup_request(&o->o_requests, le64_to_cpu(con->in_msg->hdr.tid));\n\tif (!req) {\n\t\tspin_unlock(&o->o_requests_lock);\n\t\treturn -EBADR;\n\t}\n\n\tif (o->o_sparse_op_idx < 0) {\n\t\tu64 srlen = sparse_data_requested(req);\n\n\t\tdout(\"%s: [%d] starting new sparse read req. srlen=0x%llx\\n\",\n\t\t     __func__, o->o_osd, srlen);\n\t\tceph_msg_data_cursor_init(cursor, con->in_msg, srlen);\n\t} else {\n\t\tu64 end;\n\n\t\top = &req->r_ops[o->o_sparse_op_idx];\n\n\t\tWARN_ON_ONCE(op->extent.sparse_ext);\n\n\t\t \n\t\top->extent.sparse_ext = sr->sr_extent;\n\t\tsr->sr_extent = NULL;\n\t\top->extent.sparse_ext_cnt = sr->sr_count;\n\t\tsr->sr_ext_len = 0;\n\t\tdout(\"%s: [%d] completed extent array len %d cursor->resid %zd\\n\",\n\t\t     __func__, o->o_osd, op->extent.sparse_ext_cnt, cursor->resid);\n\t\t \n\t\tend = ceph_sparse_ext_map_end(op);\n\t\tif (end < sr->sr_req_len)\n\t\t\tadvance_cursor(cursor, sr->sr_req_len - end, false);\n\t}\n\n\tceph_init_sparse_read(sr);\n\n\t \n\twhile (++o->o_sparse_op_idx < req->r_num_ops) {\n\t\top = &req->r_ops[o->o_sparse_op_idx];\n\t\tif (op->op == CEPH_OSD_OP_SPARSE_READ)\n\t\t\tgoto found;\n\t}\n\n\t \n\tspin_unlock(&o->o_requests_lock);\n\to->o_sparse_op_idx = -1;\n\treturn 0;\nfound:\n\tsr->sr_req_off = op->extent.offset;\n\tsr->sr_req_len = op->extent.length;\n\tsr->sr_pos = sr->sr_req_off;\n\tdout(\"%s: [%d] new sparse read op at idx %d 0x%llx~0x%llx\\n\", __func__,\n\t     o->o_osd, o->o_sparse_op_idx, sr->sr_req_off, sr->sr_req_len);\n\n\t \n\tsr->sr_ext_len = op->extent.sparse_ext_cnt;\n\top->extent.sparse_ext_cnt = 0;\n\tsr->sr_extent = op->extent.sparse_ext;\n\top->extent.sparse_ext = NULL;\n\n\tspin_unlock(&o->o_requests_lock);\n\treturn 1;\n}\n\n#ifdef __BIG_ENDIAN\nstatic inline void convert_extent_map(struct ceph_sparse_read *sr)\n{\n\tint i;\n\n\tfor (i = 0; i < sr->sr_count; i++) {\n\t\tstruct ceph_sparse_extent *ext = &sr->sr_extent[i];\n\n\t\text->off = le64_to_cpu((__force __le64)ext->off);\n\t\text->len = le64_to_cpu((__force __le64)ext->len);\n\t}\n}\n#else\nstatic inline void convert_extent_map(struct ceph_sparse_read *sr)\n{\n}\n#endif\n\n#define MAX_EXTENTS 4096\n\nstatic int osd_sparse_read(struct ceph_connection *con,\n\t\t\t   struct ceph_msg_data_cursor *cursor,\n\t\t\t   char **pbuf)\n{\n\tstruct ceph_osd *o = con->private;\n\tstruct ceph_sparse_read *sr = &o->o_sparse_read;\n\tu32 count = sr->sr_count;\n\tu64 eoff, elen;\n\tint ret;\n\n\tswitch (sr->sr_state) {\n\tcase CEPH_SPARSE_READ_HDR:\nnext_op:\n\t\tret = prep_next_sparse_read(con, cursor);\n\t\tif (ret <= 0)\n\t\t\treturn ret;\n\n\t\t \n\t\tret = sizeof(sr->sr_count);\n\t\t*pbuf = (char *)&sr->sr_count;\n\t\tsr->sr_state = CEPH_SPARSE_READ_EXTENTS;\n\t\tbreak;\n\tcase CEPH_SPARSE_READ_EXTENTS:\n\t\t \n\t\tcount = le32_to_cpu((__force __le32)sr->sr_count);\n\t\tsr->sr_count = count;\n\t\tdout(\"[%d] got %u extents\\n\", o->o_osd, count);\n\n\t\tif (count > 0) {\n\t\t\tif (!sr->sr_extent || count > sr->sr_ext_len) {\n\t\t\t\t \n\t\t\t\tif (count > MAX_EXTENTS) {\n\t\t\t\t\tdout(\"%s: OSD returned 0x%x extents in a single reply!\\n\",\n\t\t\t\t\t     __func__, count);\n\t\t\t\t\treturn -EREMOTEIO;\n\t\t\t\t}\n\n\t\t\t\t \n\t\t\t\tkfree(sr->sr_extent);\n\t\t\t\tsr->sr_extent = kmalloc_array(count,\n\t\t\t\t\t\t\t      sizeof(*sr->sr_extent),\n\t\t\t\t\t\t\t      GFP_NOIO);\n\t\t\t\tif (!sr->sr_extent)\n\t\t\t\t\treturn -ENOMEM;\n\t\t\t\tsr->sr_ext_len = count;\n\t\t\t}\n\t\t\tret = count * sizeof(*sr->sr_extent);\n\t\t\t*pbuf = (char *)sr->sr_extent;\n\t\t\tsr->sr_state = CEPH_SPARSE_READ_DATA_LEN;\n\t\t\tbreak;\n\t\t}\n\t\t \n\t\tfallthrough;\n\tcase CEPH_SPARSE_READ_DATA_LEN:\n\t\tconvert_extent_map(sr);\n\t\tret = sizeof(sr->sr_datalen);\n\t\t*pbuf = (char *)&sr->sr_datalen;\n\t\tsr->sr_state = CEPH_SPARSE_READ_DATA;\n\t\tbreak;\n\tcase CEPH_SPARSE_READ_DATA:\n\t\tif (sr->sr_index >= count) {\n\t\t\tsr->sr_state = CEPH_SPARSE_READ_HDR;\n\t\t\tgoto next_op;\n\t\t}\n\n\t\teoff = sr->sr_extent[sr->sr_index].off;\n\t\telen = sr->sr_extent[sr->sr_index].len;\n\n\t\tdout(\"[%d] ext %d off 0x%llx len 0x%llx\\n\",\n\t\t     o->o_osd, sr->sr_index, eoff, elen);\n\n\t\tif (elen > INT_MAX) {\n\t\t\tdout(\"Sparse read extent length too long (0x%llx)\\n\",\n\t\t\t     elen);\n\t\t\treturn -EREMOTEIO;\n\t\t}\n\n\t\t \n\t\tif (sr->sr_pos < eoff)\n\t\t\tadvance_cursor(cursor, eoff - sr->sr_pos, true);\n\n\t\t \n\t\tsr->sr_pos = eoff + elen;\n\n\t\t \n\t\tcursor->sr_resid = elen;\n\t\tret = elen;\n\t\t*pbuf = NULL;\n\n\t\t \n\t\t++sr->sr_index;\n\t\tbreak;\n\t}\n\treturn ret;\n}\n\nstatic const struct ceph_connection_operations osd_con_ops = {\n\t.get = osd_get_con,\n\t.put = osd_put_con,\n\t.sparse_read = osd_sparse_read,\n\t.alloc_msg = osd_alloc_msg,\n\t.dispatch = osd_dispatch,\n\t.fault = osd_fault,\n\t.reencode_message = osd_reencode_message,\n\t.get_authorizer = osd_get_authorizer,\n\t.add_authorizer_challenge = osd_add_authorizer_challenge,\n\t.verify_authorizer_reply = osd_verify_authorizer_reply,\n\t.invalidate_authorizer = osd_invalidate_authorizer,\n\t.sign_message = osd_sign_message,\n\t.check_message_signature = osd_check_message_signature,\n\t.get_auth_request = osd_get_auth_request,\n\t.handle_auth_reply_more = osd_handle_auth_reply_more,\n\t.handle_auth_done = osd_handle_auth_done,\n\t.handle_auth_bad_method = osd_handle_auth_bad_method,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}