{
  "module_name": "af_packet.c",
  "hash_id": "9a09ee704b49ebea19831b7137b05bae6a2a1b227f5b8034f8c1f3a64f1c1b8c",
  "original_prompt": "Ingested from linux-6.6.14/net/packet/af_packet.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/ethtool.h>\n#include <linux/filter.h>\n#include <linux/types.h>\n#include <linux/mm.h>\n#include <linux/capability.h>\n#include <linux/fcntl.h>\n#include <linux/socket.h>\n#include <linux/in.h>\n#include <linux/inet.h>\n#include <linux/netdevice.h>\n#include <linux/if_packet.h>\n#include <linux/wireless.h>\n#include <linux/kernel.h>\n#include <linux/kmod.h>\n#include <linux/slab.h>\n#include <linux/vmalloc.h>\n#include <net/net_namespace.h>\n#include <net/ip.h>\n#include <net/protocol.h>\n#include <linux/skbuff.h>\n#include <net/sock.h>\n#include <linux/errno.h>\n#include <linux/timer.h>\n#include <linux/uaccess.h>\n#include <asm/ioctls.h>\n#include <asm/page.h>\n#include <asm/cacheflush.h>\n#include <asm/io.h>\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <linux/poll.h>\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/mutex.h>\n#include <linux/if_vlan.h>\n#include <linux/virtio_net.h>\n#include <linux/errqueue.h>\n#include <linux/net_tstamp.h>\n#include <linux/percpu.h>\n#ifdef CONFIG_INET\n#include <net/inet_common.h>\n#endif\n#include <linux/bpf.h>\n#include <net/compat.h>\n#include <linux/netfilter_netdev.h>\n\n#include \"internal.h\"\n\n \n\n \n\n \nstruct packet_mreq_max {\n\tint\t\tmr_ifindex;\n\tunsigned short\tmr_type;\n\tunsigned short\tmr_alen;\n\tunsigned char\tmr_address[MAX_ADDR_LEN];\n};\n\nunion tpacket_uhdr {\n\tstruct tpacket_hdr  *h1;\n\tstruct tpacket2_hdr *h2;\n\tstruct tpacket3_hdr *h3;\n\tvoid *raw;\n};\n\nstatic int packet_set_ring(struct sock *sk, union tpacket_req_u *req_u,\n\t\tint closing, int tx_ring);\n\n#define V3_ALIGNMENT\t(8)\n\n#define BLK_HDR_LEN\t(ALIGN(sizeof(struct tpacket_block_desc), V3_ALIGNMENT))\n\n#define BLK_PLUS_PRIV(sz_of_priv) \\\n\t(BLK_HDR_LEN + ALIGN((sz_of_priv), V3_ALIGNMENT))\n\n#define BLOCK_STATUS(x)\t((x)->hdr.bh1.block_status)\n#define BLOCK_NUM_PKTS(x)\t((x)->hdr.bh1.num_pkts)\n#define BLOCK_O2FP(x)\t\t((x)->hdr.bh1.offset_to_first_pkt)\n#define BLOCK_LEN(x)\t\t((x)->hdr.bh1.blk_len)\n#define BLOCK_SNUM(x)\t\t((x)->hdr.bh1.seq_num)\n#define BLOCK_O2PRIV(x)\t((x)->offset_to_priv)\n\nstruct packet_sock;\nstatic int tpacket_rcv(struct sk_buff *skb, struct net_device *dev,\n\t\t       struct packet_type *pt, struct net_device *orig_dev);\n\nstatic void *packet_previous_frame(struct packet_sock *po,\n\t\tstruct packet_ring_buffer *rb,\n\t\tint status);\nstatic void packet_increment_head(struct packet_ring_buffer *buff);\nstatic int prb_curr_blk_in_use(struct tpacket_block_desc *);\nstatic void *prb_dispatch_next_block(struct tpacket_kbdq_core *,\n\t\t\tstruct packet_sock *);\nstatic void prb_retire_current_block(struct tpacket_kbdq_core *,\n\t\tstruct packet_sock *, unsigned int status);\nstatic int prb_queue_frozen(struct tpacket_kbdq_core *);\nstatic void prb_open_block(struct tpacket_kbdq_core *,\n\t\tstruct tpacket_block_desc *);\nstatic void prb_retire_rx_blk_timer_expired(struct timer_list *);\nstatic void _prb_refresh_rx_retire_blk_timer(struct tpacket_kbdq_core *);\nstatic void prb_fill_rxhash(struct tpacket_kbdq_core *, struct tpacket3_hdr *);\nstatic void prb_clear_rxhash(struct tpacket_kbdq_core *,\n\t\tstruct tpacket3_hdr *);\nstatic void prb_fill_vlan_info(struct tpacket_kbdq_core *,\n\t\tstruct tpacket3_hdr *);\nstatic void packet_flush_mclist(struct sock *sk);\nstatic u16 packet_pick_tx_queue(struct sk_buff *skb);\n\nstruct packet_skb_cb {\n\tunion {\n\t\tstruct sockaddr_pkt pkt;\n\t\tunion {\n\t\t\t \n\t\t\tunsigned int origlen;\n\t\t\tstruct sockaddr_ll ll;\n\t\t};\n\t} sa;\n};\n\n#define vio_le() virtio_legacy_is_little_endian()\n\n#define PACKET_SKB_CB(__skb)\t((struct packet_skb_cb *)((__skb)->cb))\n\n#define GET_PBDQC_FROM_RB(x)\t((struct tpacket_kbdq_core *)(&(x)->prb_bdqc))\n#define GET_PBLOCK_DESC(x, bid)\t\\\n\t((struct tpacket_block_desc *)((x)->pkbdq[(bid)].buffer))\n#define GET_CURR_PBLOCK_DESC_FROM_CORE(x)\t\\\n\t((struct tpacket_block_desc *)((x)->pkbdq[(x)->kactive_blk_num].buffer))\n#define GET_NEXT_PRB_BLK_NUM(x) \\\n\t(((x)->kactive_blk_num < ((x)->knum_blocks-1)) ? \\\n\t((x)->kactive_blk_num+1) : 0)\n\nstatic void __fanout_unlink(struct sock *sk, struct packet_sock *po);\nstatic void __fanout_link(struct sock *sk, struct packet_sock *po);\n\n#ifdef CONFIG_NETFILTER_EGRESS\nstatic noinline struct sk_buff *nf_hook_direct_egress(struct sk_buff *skb)\n{\n\tstruct sk_buff *next, *head = NULL, *tail;\n\tint rc;\n\n\trcu_read_lock();\n\tfor (; skb != NULL; skb = next) {\n\t\tnext = skb->next;\n\t\tskb_mark_not_on_list(skb);\n\n\t\tif (!nf_hook_egress(skb, &rc, skb->dev))\n\t\t\tcontinue;\n\n\t\tif (!head)\n\t\t\thead = skb;\n\t\telse\n\t\t\ttail->next = skb;\n\n\t\ttail = skb;\n\t}\n\trcu_read_unlock();\n\n\treturn head;\n}\n#endif\n\nstatic int packet_xmit(const struct packet_sock *po, struct sk_buff *skb)\n{\n\tif (!packet_sock_flag(po, PACKET_SOCK_QDISC_BYPASS))\n\t\treturn dev_queue_xmit(skb);\n\n#ifdef CONFIG_NETFILTER_EGRESS\n\tif (nf_hook_egress_active()) {\n\t\tskb = nf_hook_direct_egress(skb);\n\t\tif (!skb)\n\t\t\treturn NET_XMIT_DROP;\n\t}\n#endif\n\treturn dev_direct_xmit(skb, packet_pick_tx_queue(skb));\n}\n\nstatic struct net_device *packet_cached_dev_get(struct packet_sock *po)\n{\n\tstruct net_device *dev;\n\n\trcu_read_lock();\n\tdev = rcu_dereference(po->cached_dev);\n\tdev_hold(dev);\n\trcu_read_unlock();\n\n\treturn dev;\n}\n\nstatic void packet_cached_dev_assign(struct packet_sock *po,\n\t\t\t\t     struct net_device *dev)\n{\n\trcu_assign_pointer(po->cached_dev, dev);\n}\n\nstatic void packet_cached_dev_reset(struct packet_sock *po)\n{\n\tRCU_INIT_POINTER(po->cached_dev, NULL);\n}\n\nstatic u16 packet_pick_tx_queue(struct sk_buff *skb)\n{\n\tstruct net_device *dev = skb->dev;\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tint cpu = raw_smp_processor_id();\n\tu16 queue_index;\n\n#ifdef CONFIG_XPS\n\tskb->sender_cpu = cpu + 1;\n#endif\n\tskb_record_rx_queue(skb, cpu % dev->real_num_tx_queues);\n\tif (ops->ndo_select_queue) {\n\t\tqueue_index = ops->ndo_select_queue(dev, skb, NULL);\n\t\tqueue_index = netdev_cap_txqueue(dev, queue_index);\n\t} else {\n\t\tqueue_index = netdev_pick_tx(dev, skb, NULL);\n\t}\n\n\treturn queue_index;\n}\n\n \nstatic void __register_prot_hook(struct sock *sk)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\n\tif (!packet_sock_flag(po, PACKET_SOCK_RUNNING)) {\n\t\tif (po->fanout)\n\t\t\t__fanout_link(sk, po);\n\t\telse\n\t\t\tdev_add_pack(&po->prot_hook);\n\n\t\tsock_hold(sk);\n\t\tpacket_sock_flag_set(po, PACKET_SOCK_RUNNING, 1);\n\t}\n}\n\nstatic void register_prot_hook(struct sock *sk)\n{\n\tlockdep_assert_held_once(&pkt_sk(sk)->bind_lock);\n\t__register_prot_hook(sk);\n}\n\n \nstatic void __unregister_prot_hook(struct sock *sk, bool sync)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\n\tlockdep_assert_held_once(&po->bind_lock);\n\n\tpacket_sock_flag_set(po, PACKET_SOCK_RUNNING, 0);\n\n\tif (po->fanout)\n\t\t__fanout_unlink(sk, po);\n\telse\n\t\t__dev_remove_pack(&po->prot_hook);\n\n\t__sock_put(sk);\n\n\tif (sync) {\n\t\tspin_unlock(&po->bind_lock);\n\t\tsynchronize_net();\n\t\tspin_lock(&po->bind_lock);\n\t}\n}\n\nstatic void unregister_prot_hook(struct sock *sk, bool sync)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\n\tif (packet_sock_flag(po, PACKET_SOCK_RUNNING))\n\t\t__unregister_prot_hook(sk, sync);\n}\n\nstatic inline struct page * __pure pgv_to_page(void *addr)\n{\n\tif (is_vmalloc_addr(addr))\n\t\treturn vmalloc_to_page(addr);\n\treturn virt_to_page(addr);\n}\n\nstatic void __packet_set_status(struct packet_sock *po, void *frame, int status)\n{\n\tunion tpacket_uhdr h;\n\n\t \n\n\th.raw = frame;\n\tswitch (po->tp_version) {\n\tcase TPACKET_V1:\n\t\tWRITE_ONCE(h.h1->tp_status, status);\n\t\tflush_dcache_page(pgv_to_page(&h.h1->tp_status));\n\t\tbreak;\n\tcase TPACKET_V2:\n\t\tWRITE_ONCE(h.h2->tp_status, status);\n\t\tflush_dcache_page(pgv_to_page(&h.h2->tp_status));\n\t\tbreak;\n\tcase TPACKET_V3:\n\t\tWRITE_ONCE(h.h3->tp_status, status);\n\t\tflush_dcache_page(pgv_to_page(&h.h3->tp_status));\n\t\tbreak;\n\tdefault:\n\t\tWARN(1, \"TPACKET version not supported.\\n\");\n\t\tBUG();\n\t}\n\n\tsmp_wmb();\n}\n\nstatic int __packet_get_status(const struct packet_sock *po, void *frame)\n{\n\tunion tpacket_uhdr h;\n\n\tsmp_rmb();\n\n\t \n\n\th.raw = frame;\n\tswitch (po->tp_version) {\n\tcase TPACKET_V1:\n\t\tflush_dcache_page(pgv_to_page(&h.h1->tp_status));\n\t\treturn READ_ONCE(h.h1->tp_status);\n\tcase TPACKET_V2:\n\t\tflush_dcache_page(pgv_to_page(&h.h2->tp_status));\n\t\treturn READ_ONCE(h.h2->tp_status);\n\tcase TPACKET_V3:\n\t\tflush_dcache_page(pgv_to_page(&h.h3->tp_status));\n\t\treturn READ_ONCE(h.h3->tp_status);\n\tdefault:\n\t\tWARN(1, \"TPACKET version not supported.\\n\");\n\t\tBUG();\n\t\treturn 0;\n\t}\n}\n\nstatic __u32 tpacket_get_timestamp(struct sk_buff *skb, struct timespec64 *ts,\n\t\t\t\t   unsigned int flags)\n{\n\tstruct skb_shared_hwtstamps *shhwtstamps = skb_hwtstamps(skb);\n\n\tif (shhwtstamps &&\n\t    (flags & SOF_TIMESTAMPING_RAW_HARDWARE) &&\n\t    ktime_to_timespec64_cond(shhwtstamps->hwtstamp, ts))\n\t\treturn TP_STATUS_TS_RAW_HARDWARE;\n\n\tif ((flags & SOF_TIMESTAMPING_SOFTWARE) &&\n\t    ktime_to_timespec64_cond(skb_tstamp(skb), ts))\n\t\treturn TP_STATUS_TS_SOFTWARE;\n\n\treturn 0;\n}\n\nstatic __u32 __packet_set_timestamp(struct packet_sock *po, void *frame,\n\t\t\t\t    struct sk_buff *skb)\n{\n\tunion tpacket_uhdr h;\n\tstruct timespec64 ts;\n\t__u32 ts_status;\n\n\tif (!(ts_status = tpacket_get_timestamp(skb, &ts, READ_ONCE(po->tp_tstamp))))\n\t\treturn 0;\n\n\th.raw = frame;\n\t \n\tswitch (po->tp_version) {\n\tcase TPACKET_V1:\n\t\th.h1->tp_sec = ts.tv_sec;\n\t\th.h1->tp_usec = ts.tv_nsec / NSEC_PER_USEC;\n\t\tbreak;\n\tcase TPACKET_V2:\n\t\th.h2->tp_sec = ts.tv_sec;\n\t\th.h2->tp_nsec = ts.tv_nsec;\n\t\tbreak;\n\tcase TPACKET_V3:\n\t\th.h3->tp_sec = ts.tv_sec;\n\t\th.h3->tp_nsec = ts.tv_nsec;\n\t\tbreak;\n\tdefault:\n\t\tWARN(1, \"TPACKET version not supported.\\n\");\n\t\tBUG();\n\t}\n\n\t \n\tflush_dcache_page(pgv_to_page(&h.h1->tp_sec));\n\tsmp_wmb();\n\n\treturn ts_status;\n}\n\nstatic void *packet_lookup_frame(const struct packet_sock *po,\n\t\t\t\t const struct packet_ring_buffer *rb,\n\t\t\t\t unsigned int position,\n\t\t\t\t int status)\n{\n\tunsigned int pg_vec_pos, frame_offset;\n\tunion tpacket_uhdr h;\n\n\tpg_vec_pos = position / rb->frames_per_block;\n\tframe_offset = position % rb->frames_per_block;\n\n\th.raw = rb->pg_vec[pg_vec_pos].buffer +\n\t\t(frame_offset * rb->frame_size);\n\n\tif (status != __packet_get_status(po, h.raw))\n\t\treturn NULL;\n\n\treturn h.raw;\n}\n\nstatic void *packet_current_frame(struct packet_sock *po,\n\t\tstruct packet_ring_buffer *rb,\n\t\tint status)\n{\n\treturn packet_lookup_frame(po, rb, rb->head, status);\n}\n\nstatic void prb_del_retire_blk_timer(struct tpacket_kbdq_core *pkc)\n{\n\tdel_timer_sync(&pkc->retire_blk_timer);\n}\n\nstatic void prb_shutdown_retire_blk_timer(struct packet_sock *po,\n\t\tstruct sk_buff_head *rb_queue)\n{\n\tstruct tpacket_kbdq_core *pkc;\n\n\tpkc = GET_PBDQC_FROM_RB(&po->rx_ring);\n\n\tspin_lock_bh(&rb_queue->lock);\n\tpkc->delete_blk_timer = 1;\n\tspin_unlock_bh(&rb_queue->lock);\n\n\tprb_del_retire_blk_timer(pkc);\n}\n\nstatic void prb_setup_retire_blk_timer(struct packet_sock *po)\n{\n\tstruct tpacket_kbdq_core *pkc;\n\n\tpkc = GET_PBDQC_FROM_RB(&po->rx_ring);\n\ttimer_setup(&pkc->retire_blk_timer, prb_retire_rx_blk_timer_expired,\n\t\t    0);\n\tpkc->retire_blk_timer.expires = jiffies;\n}\n\nstatic int prb_calc_retire_blk_tmo(struct packet_sock *po,\n\t\t\t\tint blk_size_in_bytes)\n{\n\tstruct net_device *dev;\n\tunsigned int mbits, div;\n\tstruct ethtool_link_ksettings ecmd;\n\tint err;\n\n\trtnl_lock();\n\tdev = __dev_get_by_index(sock_net(&po->sk), po->ifindex);\n\tif (unlikely(!dev)) {\n\t\trtnl_unlock();\n\t\treturn DEFAULT_PRB_RETIRE_TOV;\n\t}\n\terr = __ethtool_get_link_ksettings(dev, &ecmd);\n\trtnl_unlock();\n\tif (err)\n\t\treturn DEFAULT_PRB_RETIRE_TOV;\n\n\t \n\tif (ecmd.base.speed < SPEED_1000 ||\n\t    ecmd.base.speed == SPEED_UNKNOWN)\n\t\treturn DEFAULT_PRB_RETIRE_TOV;\n\n\tdiv = ecmd.base.speed / 1000;\n\tmbits = (blk_size_in_bytes * 8) / (1024 * 1024);\n\n\tif (div)\n\t\tmbits /= div;\n\n\tif (div)\n\t\treturn mbits + 1;\n\treturn mbits;\n}\n\nstatic void prb_init_ft_ops(struct tpacket_kbdq_core *p1,\n\t\t\tunion tpacket_req_u *req_u)\n{\n\tp1->feature_req_word = req_u->req3.tp_feature_req_word;\n}\n\nstatic void init_prb_bdqc(struct packet_sock *po,\n\t\t\tstruct packet_ring_buffer *rb,\n\t\t\tstruct pgv *pg_vec,\n\t\t\tunion tpacket_req_u *req_u)\n{\n\tstruct tpacket_kbdq_core *p1 = GET_PBDQC_FROM_RB(rb);\n\tstruct tpacket_block_desc *pbd;\n\n\tmemset(p1, 0x0, sizeof(*p1));\n\n\tp1->knxt_seq_num = 1;\n\tp1->pkbdq = pg_vec;\n\tpbd = (struct tpacket_block_desc *)pg_vec[0].buffer;\n\tp1->pkblk_start\t= pg_vec[0].buffer;\n\tp1->kblk_size = req_u->req3.tp_block_size;\n\tp1->knum_blocks\t= req_u->req3.tp_block_nr;\n\tp1->hdrlen = po->tp_hdrlen;\n\tp1->version = po->tp_version;\n\tp1->last_kactive_blk_num = 0;\n\tpo->stats.stats3.tp_freeze_q_cnt = 0;\n\tif (req_u->req3.tp_retire_blk_tov)\n\t\tp1->retire_blk_tov = req_u->req3.tp_retire_blk_tov;\n\telse\n\t\tp1->retire_blk_tov = prb_calc_retire_blk_tmo(po,\n\t\t\t\t\t\treq_u->req3.tp_block_size);\n\tp1->tov_in_jiffies = msecs_to_jiffies(p1->retire_blk_tov);\n\tp1->blk_sizeof_priv = req_u->req3.tp_sizeof_priv;\n\trwlock_init(&p1->blk_fill_in_prog_lock);\n\n\tp1->max_frame_len = p1->kblk_size - BLK_PLUS_PRIV(p1->blk_sizeof_priv);\n\tprb_init_ft_ops(p1, req_u);\n\tprb_setup_retire_blk_timer(po);\n\tprb_open_block(p1, pbd);\n}\n\n \nstatic void _prb_refresh_rx_retire_blk_timer(struct tpacket_kbdq_core *pkc)\n{\n\tmod_timer(&pkc->retire_blk_timer,\n\t\t\tjiffies + pkc->tov_in_jiffies);\n\tpkc->last_kactive_blk_num = pkc->kactive_blk_num;\n}\n\n \nstatic void prb_retire_rx_blk_timer_expired(struct timer_list *t)\n{\n\tstruct packet_sock *po =\n\t\tfrom_timer(po, t, rx_ring.prb_bdqc.retire_blk_timer);\n\tstruct tpacket_kbdq_core *pkc = GET_PBDQC_FROM_RB(&po->rx_ring);\n\tunsigned int frozen;\n\tstruct tpacket_block_desc *pbd;\n\n\tspin_lock(&po->sk.sk_receive_queue.lock);\n\n\tfrozen = prb_queue_frozen(pkc);\n\tpbd = GET_CURR_PBLOCK_DESC_FROM_CORE(pkc);\n\n\tif (unlikely(pkc->delete_blk_timer))\n\t\tgoto out;\n\n\t \n\tif (BLOCK_NUM_PKTS(pbd)) {\n\t\t \n\t\twrite_lock(&pkc->blk_fill_in_prog_lock);\n\t\twrite_unlock(&pkc->blk_fill_in_prog_lock);\n\t}\n\n\tif (pkc->last_kactive_blk_num == pkc->kactive_blk_num) {\n\t\tif (!frozen) {\n\t\t\tif (!BLOCK_NUM_PKTS(pbd)) {\n\t\t\t\t \n\t\t\t\tgoto refresh_timer;\n\t\t\t}\n\t\t\tprb_retire_current_block(pkc, po, TP_STATUS_BLK_TMO);\n\t\t\tif (!prb_dispatch_next_block(pkc, po))\n\t\t\t\tgoto refresh_timer;\n\t\t\telse\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\t \n\t\t\tif (prb_curr_blk_in_use(pbd)) {\n\t\t\t\t \n\t\t\t\tgoto refresh_timer;\n\t\t\t} else {\n\t\t\t        \n\t\t\t\tprb_open_block(pkc, pbd);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\nrefresh_timer:\n\t_prb_refresh_rx_retire_blk_timer(pkc);\n\nout:\n\tspin_unlock(&po->sk.sk_receive_queue.lock);\n}\n\nstatic void prb_flush_block(struct tpacket_kbdq_core *pkc1,\n\t\tstruct tpacket_block_desc *pbd1, __u32 status)\n{\n\t \n\n#if ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE == 1\n\tu8 *start, *end;\n\n\tstart = (u8 *)pbd1;\n\n\t \n\tstart += PAGE_SIZE;\n\n\tend = (u8 *)PAGE_ALIGN((unsigned long)pkc1->pkblk_end);\n\tfor (; start < end; start += PAGE_SIZE)\n\t\tflush_dcache_page(pgv_to_page(start));\n\n\tsmp_wmb();\n#endif\n\n\t \n\n\tBLOCK_STATUS(pbd1) = status;\n\n\t \n\n#if ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE == 1\n\tstart = (u8 *)pbd1;\n\tflush_dcache_page(pgv_to_page(start));\n\n\tsmp_wmb();\n#endif\n}\n\n \nstatic void prb_close_block(struct tpacket_kbdq_core *pkc1,\n\t\tstruct tpacket_block_desc *pbd1,\n\t\tstruct packet_sock *po, unsigned int stat)\n{\n\t__u32 status = TP_STATUS_USER | stat;\n\n\tstruct tpacket3_hdr *last_pkt;\n\tstruct tpacket_hdr_v1 *h1 = &pbd1->hdr.bh1;\n\tstruct sock *sk = &po->sk;\n\n\tif (atomic_read(&po->tp_drops))\n\t\tstatus |= TP_STATUS_LOSING;\n\n\tlast_pkt = (struct tpacket3_hdr *)pkc1->prev;\n\tlast_pkt->tp_next_offset = 0;\n\n\t \n\tif (BLOCK_NUM_PKTS(pbd1)) {\n\t\th1->ts_last_pkt.ts_sec = last_pkt->tp_sec;\n\t\th1->ts_last_pkt.ts_nsec\t= last_pkt->tp_nsec;\n\t} else {\n\t\t \n\t\tstruct timespec64 ts;\n\t\tktime_get_real_ts64(&ts);\n\t\th1->ts_last_pkt.ts_sec = ts.tv_sec;\n\t\th1->ts_last_pkt.ts_nsec\t= ts.tv_nsec;\n\t}\n\n\tsmp_wmb();\n\n\t \n\tprb_flush_block(pkc1, pbd1, status);\n\n\tsk->sk_data_ready(sk);\n\n\tpkc1->kactive_blk_num = GET_NEXT_PRB_BLK_NUM(pkc1);\n}\n\nstatic void prb_thaw_queue(struct tpacket_kbdq_core *pkc)\n{\n\tpkc->reset_pending_on_curr_blk = 0;\n}\n\n \nstatic void prb_open_block(struct tpacket_kbdq_core *pkc1,\n\tstruct tpacket_block_desc *pbd1)\n{\n\tstruct timespec64 ts;\n\tstruct tpacket_hdr_v1 *h1 = &pbd1->hdr.bh1;\n\n\tsmp_rmb();\n\n\t \n\n\tBLOCK_SNUM(pbd1) = pkc1->knxt_seq_num++;\n\tBLOCK_NUM_PKTS(pbd1) = 0;\n\tBLOCK_LEN(pbd1) = BLK_PLUS_PRIV(pkc1->blk_sizeof_priv);\n\n\tktime_get_real_ts64(&ts);\n\n\th1->ts_first_pkt.ts_sec = ts.tv_sec;\n\th1->ts_first_pkt.ts_nsec = ts.tv_nsec;\n\n\tpkc1->pkblk_start = (char *)pbd1;\n\tpkc1->nxt_offset = pkc1->pkblk_start + BLK_PLUS_PRIV(pkc1->blk_sizeof_priv);\n\n\tBLOCK_O2FP(pbd1) = (__u32)BLK_PLUS_PRIV(pkc1->blk_sizeof_priv);\n\tBLOCK_O2PRIV(pbd1) = BLK_HDR_LEN;\n\n\tpbd1->version = pkc1->version;\n\tpkc1->prev = pkc1->nxt_offset;\n\tpkc1->pkblk_end = pkc1->pkblk_start + pkc1->kblk_size;\n\n\tprb_thaw_queue(pkc1);\n\t_prb_refresh_rx_retire_blk_timer(pkc1);\n\n\tsmp_wmb();\n}\n\n \nstatic void prb_freeze_queue(struct tpacket_kbdq_core *pkc,\n\t\t\t\t  struct packet_sock *po)\n{\n\tpkc->reset_pending_on_curr_blk = 1;\n\tpo->stats.stats3.tp_freeze_q_cnt++;\n}\n\n#define TOTAL_PKT_LEN_INCL_ALIGN(length) (ALIGN((length), V3_ALIGNMENT))\n\n \nstatic void *prb_dispatch_next_block(struct tpacket_kbdq_core *pkc,\n\t\tstruct packet_sock *po)\n{\n\tstruct tpacket_block_desc *pbd;\n\n\tsmp_rmb();\n\n\t \n\tpbd = GET_CURR_PBLOCK_DESC_FROM_CORE(pkc);\n\n\t \n\tif (TP_STATUS_USER & BLOCK_STATUS(pbd)) {\n\t\tprb_freeze_queue(pkc, po);\n\t\treturn NULL;\n\t}\n\n\t \n\tprb_open_block(pkc, pbd);\n\treturn (void *)pkc->nxt_offset;\n}\n\nstatic void prb_retire_current_block(struct tpacket_kbdq_core *pkc,\n\t\tstruct packet_sock *po, unsigned int status)\n{\n\tstruct tpacket_block_desc *pbd = GET_CURR_PBLOCK_DESC_FROM_CORE(pkc);\n\n\t \n\tif (likely(TP_STATUS_KERNEL == BLOCK_STATUS(pbd))) {\n\t\t \n\t\tif (!(status & TP_STATUS_BLK_TMO)) {\n\t\t\t \n\t\t\twrite_lock(&pkc->blk_fill_in_prog_lock);\n\t\t\twrite_unlock(&pkc->blk_fill_in_prog_lock);\n\t\t}\n\t\tprb_close_block(pkc, pbd, po, status);\n\t\treturn;\n\t}\n}\n\nstatic int prb_curr_blk_in_use(struct tpacket_block_desc *pbd)\n{\n\treturn TP_STATUS_USER & BLOCK_STATUS(pbd);\n}\n\nstatic int prb_queue_frozen(struct tpacket_kbdq_core *pkc)\n{\n\treturn pkc->reset_pending_on_curr_blk;\n}\n\nstatic void prb_clear_blk_fill_status(struct packet_ring_buffer *rb)\n\t__releases(&pkc->blk_fill_in_prog_lock)\n{\n\tstruct tpacket_kbdq_core *pkc  = GET_PBDQC_FROM_RB(rb);\n\n\tread_unlock(&pkc->blk_fill_in_prog_lock);\n}\n\nstatic void prb_fill_rxhash(struct tpacket_kbdq_core *pkc,\n\t\t\tstruct tpacket3_hdr *ppd)\n{\n\tppd->hv1.tp_rxhash = skb_get_hash(pkc->skb);\n}\n\nstatic void prb_clear_rxhash(struct tpacket_kbdq_core *pkc,\n\t\t\tstruct tpacket3_hdr *ppd)\n{\n\tppd->hv1.tp_rxhash = 0;\n}\n\nstatic void prb_fill_vlan_info(struct tpacket_kbdq_core *pkc,\n\t\t\tstruct tpacket3_hdr *ppd)\n{\n\tif (skb_vlan_tag_present(pkc->skb)) {\n\t\tppd->hv1.tp_vlan_tci = skb_vlan_tag_get(pkc->skb);\n\t\tppd->hv1.tp_vlan_tpid = ntohs(pkc->skb->vlan_proto);\n\t\tppd->tp_status = TP_STATUS_VLAN_VALID | TP_STATUS_VLAN_TPID_VALID;\n\t} else {\n\t\tppd->hv1.tp_vlan_tci = 0;\n\t\tppd->hv1.tp_vlan_tpid = 0;\n\t\tppd->tp_status = TP_STATUS_AVAILABLE;\n\t}\n}\n\nstatic void prb_run_all_ft_ops(struct tpacket_kbdq_core *pkc,\n\t\t\tstruct tpacket3_hdr *ppd)\n{\n\tppd->hv1.tp_padding = 0;\n\tprb_fill_vlan_info(pkc, ppd);\n\n\tif (pkc->feature_req_word & TP_FT_REQ_FILL_RXHASH)\n\t\tprb_fill_rxhash(pkc, ppd);\n\telse\n\t\tprb_clear_rxhash(pkc, ppd);\n}\n\nstatic void prb_fill_curr_block(char *curr,\n\t\t\t\tstruct tpacket_kbdq_core *pkc,\n\t\t\t\tstruct tpacket_block_desc *pbd,\n\t\t\t\tunsigned int len)\n\t__acquires(&pkc->blk_fill_in_prog_lock)\n{\n\tstruct tpacket3_hdr *ppd;\n\n\tppd  = (struct tpacket3_hdr *)curr;\n\tppd->tp_next_offset = TOTAL_PKT_LEN_INCL_ALIGN(len);\n\tpkc->prev = curr;\n\tpkc->nxt_offset += TOTAL_PKT_LEN_INCL_ALIGN(len);\n\tBLOCK_LEN(pbd) += TOTAL_PKT_LEN_INCL_ALIGN(len);\n\tBLOCK_NUM_PKTS(pbd) += 1;\n\tread_lock(&pkc->blk_fill_in_prog_lock);\n\tprb_run_all_ft_ops(pkc, ppd);\n}\n\n \nstatic void *__packet_lookup_frame_in_block(struct packet_sock *po,\n\t\t\t\t\t    struct sk_buff *skb,\n\t\t\t\t\t    unsigned int len\n\t\t\t\t\t    )\n{\n\tstruct tpacket_kbdq_core *pkc;\n\tstruct tpacket_block_desc *pbd;\n\tchar *curr, *end;\n\n\tpkc = GET_PBDQC_FROM_RB(&po->rx_ring);\n\tpbd = GET_CURR_PBLOCK_DESC_FROM_CORE(pkc);\n\n\t \n\tif (prb_queue_frozen(pkc)) {\n\t\t \n\t\tif (prb_curr_blk_in_use(pbd)) {\n\t\t\t \n\t\t\treturn NULL;\n\t\t} else {\n\t\t\t \n\t\t\tprb_open_block(pkc, pbd);\n\t\t}\n\t}\n\n\tsmp_mb();\n\tcurr = pkc->nxt_offset;\n\tpkc->skb = skb;\n\tend = (char *)pbd + pkc->kblk_size;\n\n\t \n\tif (curr+TOTAL_PKT_LEN_INCL_ALIGN(len) < end) {\n\t\tprb_fill_curr_block(curr, pkc, pbd, len);\n\t\treturn (void *)curr;\n\t}\n\n\t \n\tprb_retire_current_block(pkc, po, 0);\n\n\t \n\tcurr = (char *)prb_dispatch_next_block(pkc, po);\n\tif (curr) {\n\t\tpbd = GET_CURR_PBLOCK_DESC_FROM_CORE(pkc);\n\t\tprb_fill_curr_block(curr, pkc, pbd, len);\n\t\treturn (void *)curr;\n\t}\n\n\t \n\treturn NULL;\n}\n\nstatic void *packet_current_rx_frame(struct packet_sock *po,\n\t\t\t\t\t    struct sk_buff *skb,\n\t\t\t\t\t    int status, unsigned int len)\n{\n\tchar *curr = NULL;\n\tswitch (po->tp_version) {\n\tcase TPACKET_V1:\n\tcase TPACKET_V2:\n\t\tcurr = packet_lookup_frame(po, &po->rx_ring,\n\t\t\t\t\tpo->rx_ring.head, status);\n\t\treturn curr;\n\tcase TPACKET_V3:\n\t\treturn __packet_lookup_frame_in_block(po, skb, len);\n\tdefault:\n\t\tWARN(1, \"TPACKET version not supported\\n\");\n\t\tBUG();\n\t\treturn NULL;\n\t}\n}\n\nstatic void *prb_lookup_block(const struct packet_sock *po,\n\t\t\t      const struct packet_ring_buffer *rb,\n\t\t\t      unsigned int idx,\n\t\t\t      int status)\n{\n\tstruct tpacket_kbdq_core *pkc  = GET_PBDQC_FROM_RB(rb);\n\tstruct tpacket_block_desc *pbd = GET_PBLOCK_DESC(pkc, idx);\n\n\tif (status != BLOCK_STATUS(pbd))\n\t\treturn NULL;\n\treturn pbd;\n}\n\nstatic int prb_previous_blk_num(struct packet_ring_buffer *rb)\n{\n\tunsigned int prev;\n\tif (rb->prb_bdqc.kactive_blk_num)\n\t\tprev = rb->prb_bdqc.kactive_blk_num-1;\n\telse\n\t\tprev = rb->prb_bdqc.knum_blocks-1;\n\treturn prev;\n}\n\n \nstatic void *__prb_previous_block(struct packet_sock *po,\n\t\t\t\t\t struct packet_ring_buffer *rb,\n\t\t\t\t\t int status)\n{\n\tunsigned int previous = prb_previous_blk_num(rb);\n\treturn prb_lookup_block(po, rb, previous, status);\n}\n\nstatic void *packet_previous_rx_frame(struct packet_sock *po,\n\t\t\t\t\t     struct packet_ring_buffer *rb,\n\t\t\t\t\t     int status)\n{\n\tif (po->tp_version <= TPACKET_V2)\n\t\treturn packet_previous_frame(po, rb, status);\n\n\treturn __prb_previous_block(po, rb, status);\n}\n\nstatic void packet_increment_rx_head(struct packet_sock *po,\n\t\t\t\t\t    struct packet_ring_buffer *rb)\n{\n\tswitch (po->tp_version) {\n\tcase TPACKET_V1:\n\tcase TPACKET_V2:\n\t\treturn packet_increment_head(rb);\n\tcase TPACKET_V3:\n\tdefault:\n\t\tWARN(1, \"TPACKET version not supported.\\n\");\n\t\tBUG();\n\t\treturn;\n\t}\n}\n\nstatic void *packet_previous_frame(struct packet_sock *po,\n\t\tstruct packet_ring_buffer *rb,\n\t\tint status)\n{\n\tunsigned int previous = rb->head ? rb->head - 1 : rb->frame_max;\n\treturn packet_lookup_frame(po, rb, previous, status);\n}\n\nstatic void packet_increment_head(struct packet_ring_buffer *buff)\n{\n\tbuff->head = buff->head != buff->frame_max ? buff->head+1 : 0;\n}\n\nstatic void packet_inc_pending(struct packet_ring_buffer *rb)\n{\n\tthis_cpu_inc(*rb->pending_refcnt);\n}\n\nstatic void packet_dec_pending(struct packet_ring_buffer *rb)\n{\n\tthis_cpu_dec(*rb->pending_refcnt);\n}\n\nstatic unsigned int packet_read_pending(const struct packet_ring_buffer *rb)\n{\n\tunsigned int refcnt = 0;\n\tint cpu;\n\n\t \n\tif (rb->pending_refcnt == NULL)\n\t\treturn 0;\n\n\tfor_each_possible_cpu(cpu)\n\t\trefcnt += *per_cpu_ptr(rb->pending_refcnt, cpu);\n\n\treturn refcnt;\n}\n\nstatic int packet_alloc_pending(struct packet_sock *po)\n{\n\tpo->rx_ring.pending_refcnt = NULL;\n\n\tpo->tx_ring.pending_refcnt = alloc_percpu(unsigned int);\n\tif (unlikely(po->tx_ring.pending_refcnt == NULL))\n\t\treturn -ENOBUFS;\n\n\treturn 0;\n}\n\nstatic void packet_free_pending(struct packet_sock *po)\n{\n\tfree_percpu(po->tx_ring.pending_refcnt);\n}\n\n#define ROOM_POW_OFF\t2\n#define ROOM_NONE\t0x0\n#define ROOM_LOW\t0x1\n#define ROOM_NORMAL\t0x2\n\nstatic bool __tpacket_has_room(const struct packet_sock *po, int pow_off)\n{\n\tint idx, len;\n\n\tlen = READ_ONCE(po->rx_ring.frame_max) + 1;\n\tidx = READ_ONCE(po->rx_ring.head);\n\tif (pow_off)\n\t\tidx += len >> pow_off;\n\tif (idx >= len)\n\t\tidx -= len;\n\treturn packet_lookup_frame(po, &po->rx_ring, idx, TP_STATUS_KERNEL);\n}\n\nstatic bool __tpacket_v3_has_room(const struct packet_sock *po, int pow_off)\n{\n\tint idx, len;\n\n\tlen = READ_ONCE(po->rx_ring.prb_bdqc.knum_blocks);\n\tidx = READ_ONCE(po->rx_ring.prb_bdqc.kactive_blk_num);\n\tif (pow_off)\n\t\tidx += len >> pow_off;\n\tif (idx >= len)\n\t\tidx -= len;\n\treturn prb_lookup_block(po, &po->rx_ring, idx, TP_STATUS_KERNEL);\n}\n\nstatic int __packet_rcv_has_room(const struct packet_sock *po,\n\t\t\t\t const struct sk_buff *skb)\n{\n\tconst struct sock *sk = &po->sk;\n\tint ret = ROOM_NONE;\n\n\tif (po->prot_hook.func != tpacket_rcv) {\n\t\tint rcvbuf = READ_ONCE(sk->sk_rcvbuf);\n\t\tint avail = rcvbuf - atomic_read(&sk->sk_rmem_alloc)\n\t\t\t\t   - (skb ? skb->truesize : 0);\n\n\t\tif (avail > (rcvbuf >> ROOM_POW_OFF))\n\t\t\treturn ROOM_NORMAL;\n\t\telse if (avail > 0)\n\t\t\treturn ROOM_LOW;\n\t\telse\n\t\t\treturn ROOM_NONE;\n\t}\n\n\tif (po->tp_version == TPACKET_V3) {\n\t\tif (__tpacket_v3_has_room(po, ROOM_POW_OFF))\n\t\t\tret = ROOM_NORMAL;\n\t\telse if (__tpacket_v3_has_room(po, 0))\n\t\t\tret = ROOM_LOW;\n\t} else {\n\t\tif (__tpacket_has_room(po, ROOM_POW_OFF))\n\t\t\tret = ROOM_NORMAL;\n\t\telse if (__tpacket_has_room(po, 0))\n\t\t\tret = ROOM_LOW;\n\t}\n\n\treturn ret;\n}\n\nstatic int packet_rcv_has_room(struct packet_sock *po, struct sk_buff *skb)\n{\n\tbool pressure;\n\tint ret;\n\n\tret = __packet_rcv_has_room(po, skb);\n\tpressure = ret != ROOM_NORMAL;\n\n\tif (packet_sock_flag(po, PACKET_SOCK_PRESSURE) != pressure)\n\t\tpacket_sock_flag_set(po, PACKET_SOCK_PRESSURE, pressure);\n\n\treturn ret;\n}\n\nstatic void packet_rcv_try_clear_pressure(struct packet_sock *po)\n{\n\tif (packet_sock_flag(po, PACKET_SOCK_PRESSURE) &&\n\t    __packet_rcv_has_room(po, NULL) == ROOM_NORMAL)\n\t\tpacket_sock_flag_set(po, PACKET_SOCK_PRESSURE, false);\n}\n\nstatic void packet_sock_destruct(struct sock *sk)\n{\n\tskb_queue_purge(&sk->sk_error_queue);\n\n\tWARN_ON(atomic_read(&sk->sk_rmem_alloc));\n\tWARN_ON(refcount_read(&sk->sk_wmem_alloc));\n\n\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\tpr_err(\"Attempt to release alive packet socket: %p\\n\", sk);\n\t\treturn;\n\t}\n}\n\nstatic bool fanout_flow_is_huge(struct packet_sock *po, struct sk_buff *skb)\n{\n\tu32 *history = po->rollover->history;\n\tu32 victim, rxhash;\n\tint i, count = 0;\n\n\trxhash = skb_get_hash(skb);\n\tfor (i = 0; i < ROLLOVER_HLEN; i++)\n\t\tif (READ_ONCE(history[i]) == rxhash)\n\t\t\tcount++;\n\n\tvictim = get_random_u32_below(ROLLOVER_HLEN);\n\n\t \n\tif (READ_ONCE(history[victim]) != rxhash)\n\t\tWRITE_ONCE(history[victim], rxhash);\n\n\treturn count > (ROLLOVER_HLEN >> 1);\n}\n\nstatic unsigned int fanout_demux_hash(struct packet_fanout *f,\n\t\t\t\t      struct sk_buff *skb,\n\t\t\t\t      unsigned int num)\n{\n\treturn reciprocal_scale(__skb_get_hash_symmetric(skb), num);\n}\n\nstatic unsigned int fanout_demux_lb(struct packet_fanout *f,\n\t\t\t\t    struct sk_buff *skb,\n\t\t\t\t    unsigned int num)\n{\n\tunsigned int val = atomic_inc_return(&f->rr_cur);\n\n\treturn val % num;\n}\n\nstatic unsigned int fanout_demux_cpu(struct packet_fanout *f,\n\t\t\t\t     struct sk_buff *skb,\n\t\t\t\t     unsigned int num)\n{\n\treturn smp_processor_id() % num;\n}\n\nstatic unsigned int fanout_demux_rnd(struct packet_fanout *f,\n\t\t\t\t     struct sk_buff *skb,\n\t\t\t\t     unsigned int num)\n{\n\treturn get_random_u32_below(num);\n}\n\nstatic unsigned int fanout_demux_rollover(struct packet_fanout *f,\n\t\t\t\t\t  struct sk_buff *skb,\n\t\t\t\t\t  unsigned int idx, bool try_self,\n\t\t\t\t\t  unsigned int num)\n{\n\tstruct packet_sock *po, *po_next, *po_skip = NULL;\n\tunsigned int i, j, room = ROOM_NONE;\n\n\tpo = pkt_sk(rcu_dereference(f->arr[idx]));\n\n\tif (try_self) {\n\t\troom = packet_rcv_has_room(po, skb);\n\t\tif (room == ROOM_NORMAL ||\n\t\t    (room == ROOM_LOW && !fanout_flow_is_huge(po, skb)))\n\t\t\treturn idx;\n\t\tpo_skip = po;\n\t}\n\n\ti = j = min_t(int, po->rollover->sock, num - 1);\n\tdo {\n\t\tpo_next = pkt_sk(rcu_dereference(f->arr[i]));\n\t\tif (po_next != po_skip &&\n\t\t    !packet_sock_flag(po_next, PACKET_SOCK_PRESSURE) &&\n\t\t    packet_rcv_has_room(po_next, skb) == ROOM_NORMAL) {\n\t\t\tif (i != j)\n\t\t\t\tpo->rollover->sock = i;\n\t\t\tatomic_long_inc(&po->rollover->num);\n\t\t\tif (room == ROOM_LOW)\n\t\t\t\tatomic_long_inc(&po->rollover->num_huge);\n\t\t\treturn i;\n\t\t}\n\n\t\tif (++i == num)\n\t\t\ti = 0;\n\t} while (i != j);\n\n\tatomic_long_inc(&po->rollover->num_failed);\n\treturn idx;\n}\n\nstatic unsigned int fanout_demux_qm(struct packet_fanout *f,\n\t\t\t\t    struct sk_buff *skb,\n\t\t\t\t    unsigned int num)\n{\n\treturn skb_get_queue_mapping(skb) % num;\n}\n\nstatic unsigned int fanout_demux_bpf(struct packet_fanout *f,\n\t\t\t\t     struct sk_buff *skb,\n\t\t\t\t     unsigned int num)\n{\n\tstruct bpf_prog *prog;\n\tunsigned int ret = 0;\n\n\trcu_read_lock();\n\tprog = rcu_dereference(f->bpf_prog);\n\tif (prog)\n\t\tret = bpf_prog_run_clear_cb(prog, skb) % num;\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\nstatic bool fanout_has_flag(struct packet_fanout *f, u16 flag)\n{\n\treturn f->flags & (flag >> 8);\n}\n\nstatic int packet_rcv_fanout(struct sk_buff *skb, struct net_device *dev,\n\t\t\t     struct packet_type *pt, struct net_device *orig_dev)\n{\n\tstruct packet_fanout *f = pt->af_packet_priv;\n\tunsigned int num = READ_ONCE(f->num_members);\n\tstruct net *net = read_pnet(&f->net);\n\tstruct packet_sock *po;\n\tunsigned int idx;\n\n\tif (!net_eq(dev_net(dev), net) || !num) {\n\t\tkfree_skb(skb);\n\t\treturn 0;\n\t}\n\n\tif (fanout_has_flag(f, PACKET_FANOUT_FLAG_DEFRAG)) {\n\t\tskb = ip_check_defrag(net, skb, IP_DEFRAG_AF_PACKET);\n\t\tif (!skb)\n\t\t\treturn 0;\n\t}\n\tswitch (f->type) {\n\tcase PACKET_FANOUT_HASH:\n\tdefault:\n\t\tidx = fanout_demux_hash(f, skb, num);\n\t\tbreak;\n\tcase PACKET_FANOUT_LB:\n\t\tidx = fanout_demux_lb(f, skb, num);\n\t\tbreak;\n\tcase PACKET_FANOUT_CPU:\n\t\tidx = fanout_demux_cpu(f, skb, num);\n\t\tbreak;\n\tcase PACKET_FANOUT_RND:\n\t\tidx = fanout_demux_rnd(f, skb, num);\n\t\tbreak;\n\tcase PACKET_FANOUT_QM:\n\t\tidx = fanout_demux_qm(f, skb, num);\n\t\tbreak;\n\tcase PACKET_FANOUT_ROLLOVER:\n\t\tidx = fanout_demux_rollover(f, skb, 0, false, num);\n\t\tbreak;\n\tcase PACKET_FANOUT_CBPF:\n\tcase PACKET_FANOUT_EBPF:\n\t\tidx = fanout_demux_bpf(f, skb, num);\n\t\tbreak;\n\t}\n\n\tif (fanout_has_flag(f, PACKET_FANOUT_FLAG_ROLLOVER))\n\t\tidx = fanout_demux_rollover(f, skb, idx, true, num);\n\n\tpo = pkt_sk(rcu_dereference(f->arr[idx]));\n\treturn po->prot_hook.func(skb, dev, &po->prot_hook, orig_dev);\n}\n\nDEFINE_MUTEX(fanout_mutex);\nEXPORT_SYMBOL_GPL(fanout_mutex);\nstatic LIST_HEAD(fanout_list);\nstatic u16 fanout_next_id;\n\nstatic void __fanout_link(struct sock *sk, struct packet_sock *po)\n{\n\tstruct packet_fanout *f = po->fanout;\n\n\tspin_lock(&f->lock);\n\trcu_assign_pointer(f->arr[f->num_members], sk);\n\tsmp_wmb();\n\tf->num_members++;\n\tif (f->num_members == 1)\n\t\tdev_add_pack(&f->prot_hook);\n\tspin_unlock(&f->lock);\n}\n\nstatic void __fanout_unlink(struct sock *sk, struct packet_sock *po)\n{\n\tstruct packet_fanout *f = po->fanout;\n\tint i;\n\n\tspin_lock(&f->lock);\n\tfor (i = 0; i < f->num_members; i++) {\n\t\tif (rcu_dereference_protected(f->arr[i],\n\t\t\t\t\t      lockdep_is_held(&f->lock)) == sk)\n\t\t\tbreak;\n\t}\n\tBUG_ON(i >= f->num_members);\n\trcu_assign_pointer(f->arr[i],\n\t\t\t   rcu_dereference_protected(f->arr[f->num_members - 1],\n\t\t\t\t\t\t     lockdep_is_held(&f->lock)));\n\tf->num_members--;\n\tif (f->num_members == 0)\n\t\t__dev_remove_pack(&f->prot_hook);\n\tspin_unlock(&f->lock);\n}\n\nstatic bool match_fanout_group(struct packet_type *ptype, struct sock *sk)\n{\n\tif (sk->sk_family != PF_PACKET)\n\t\treturn false;\n\n\treturn ptype->af_packet_priv == pkt_sk(sk)->fanout;\n}\n\nstatic void fanout_init_data(struct packet_fanout *f)\n{\n\tswitch (f->type) {\n\tcase PACKET_FANOUT_LB:\n\t\tatomic_set(&f->rr_cur, 0);\n\t\tbreak;\n\tcase PACKET_FANOUT_CBPF:\n\tcase PACKET_FANOUT_EBPF:\n\t\tRCU_INIT_POINTER(f->bpf_prog, NULL);\n\t\tbreak;\n\t}\n}\n\nstatic void __fanout_set_data_bpf(struct packet_fanout *f, struct bpf_prog *new)\n{\n\tstruct bpf_prog *old;\n\n\tspin_lock(&f->lock);\n\told = rcu_dereference_protected(f->bpf_prog, lockdep_is_held(&f->lock));\n\trcu_assign_pointer(f->bpf_prog, new);\n\tspin_unlock(&f->lock);\n\n\tif (old) {\n\t\tsynchronize_net();\n\t\tbpf_prog_destroy(old);\n\t}\n}\n\nstatic int fanout_set_data_cbpf(struct packet_sock *po, sockptr_t data,\n\t\t\t\tunsigned int len)\n{\n\tstruct bpf_prog *new;\n\tstruct sock_fprog fprog;\n\tint ret;\n\n\tif (sock_flag(&po->sk, SOCK_FILTER_LOCKED))\n\t\treturn -EPERM;\n\n\tret = copy_bpf_fprog_from_user(&fprog, data, len);\n\tif (ret)\n\t\treturn ret;\n\n\tret = bpf_prog_create_from_user(&new, &fprog, NULL, false);\n\tif (ret)\n\t\treturn ret;\n\n\t__fanout_set_data_bpf(po->fanout, new);\n\treturn 0;\n}\n\nstatic int fanout_set_data_ebpf(struct packet_sock *po, sockptr_t data,\n\t\t\t\tunsigned int len)\n{\n\tstruct bpf_prog *new;\n\tu32 fd;\n\n\tif (sock_flag(&po->sk, SOCK_FILTER_LOCKED))\n\t\treturn -EPERM;\n\tif (len != sizeof(fd))\n\t\treturn -EINVAL;\n\tif (copy_from_sockptr(&fd, data, len))\n\t\treturn -EFAULT;\n\n\tnew = bpf_prog_get_type(fd, BPF_PROG_TYPE_SOCKET_FILTER);\n\tif (IS_ERR(new))\n\t\treturn PTR_ERR(new);\n\n\t__fanout_set_data_bpf(po->fanout, new);\n\treturn 0;\n}\n\nstatic int fanout_set_data(struct packet_sock *po, sockptr_t data,\n\t\t\t   unsigned int len)\n{\n\tswitch (po->fanout->type) {\n\tcase PACKET_FANOUT_CBPF:\n\t\treturn fanout_set_data_cbpf(po, data, len);\n\tcase PACKET_FANOUT_EBPF:\n\t\treturn fanout_set_data_ebpf(po, data, len);\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic void fanout_release_data(struct packet_fanout *f)\n{\n\tswitch (f->type) {\n\tcase PACKET_FANOUT_CBPF:\n\tcase PACKET_FANOUT_EBPF:\n\t\t__fanout_set_data_bpf(f, NULL);\n\t}\n}\n\nstatic bool __fanout_id_is_free(struct sock *sk, u16 candidate_id)\n{\n\tstruct packet_fanout *f;\n\n\tlist_for_each_entry(f, &fanout_list, list) {\n\t\tif (f->id == candidate_id &&\n\t\t    read_pnet(&f->net) == sock_net(sk)) {\n\t\t\treturn false;\n\t\t}\n\t}\n\treturn true;\n}\n\nstatic bool fanout_find_new_id(struct sock *sk, u16 *new_id)\n{\n\tu16 id = fanout_next_id;\n\n\tdo {\n\t\tif (__fanout_id_is_free(sk, id)) {\n\t\t\t*new_id = id;\n\t\t\tfanout_next_id = id + 1;\n\t\t\treturn true;\n\t\t}\n\n\t\tid++;\n\t} while (id != fanout_next_id);\n\n\treturn false;\n}\n\nstatic int fanout_add(struct sock *sk, struct fanout_args *args)\n{\n\tstruct packet_rollover *rollover = NULL;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tu16 type_flags = args->type_flags;\n\tstruct packet_fanout *f, *match;\n\tu8 type = type_flags & 0xff;\n\tu8 flags = type_flags >> 8;\n\tu16 id = args->id;\n\tint err;\n\n\tswitch (type) {\n\tcase PACKET_FANOUT_ROLLOVER:\n\t\tif (type_flags & PACKET_FANOUT_FLAG_ROLLOVER)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase PACKET_FANOUT_HASH:\n\tcase PACKET_FANOUT_LB:\n\tcase PACKET_FANOUT_CPU:\n\tcase PACKET_FANOUT_RND:\n\tcase PACKET_FANOUT_QM:\n\tcase PACKET_FANOUT_CBPF:\n\tcase PACKET_FANOUT_EBPF:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tmutex_lock(&fanout_mutex);\n\n\terr = -EALREADY;\n\tif (po->fanout)\n\t\tgoto out;\n\n\tif (type == PACKET_FANOUT_ROLLOVER ||\n\t    (type_flags & PACKET_FANOUT_FLAG_ROLLOVER)) {\n\t\terr = -ENOMEM;\n\t\trollover = kzalloc(sizeof(*rollover), GFP_KERNEL);\n\t\tif (!rollover)\n\t\t\tgoto out;\n\t\tatomic_long_set(&rollover->num, 0);\n\t\tatomic_long_set(&rollover->num_huge, 0);\n\t\tatomic_long_set(&rollover->num_failed, 0);\n\t}\n\n\tif (type_flags & PACKET_FANOUT_FLAG_UNIQUEID) {\n\t\tif (id != 0) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tif (!fanout_find_new_id(sk, &id)) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\t \n\t\tflags &= ~(PACKET_FANOUT_FLAG_UNIQUEID >> 8);\n\t}\n\n\tmatch = NULL;\n\tlist_for_each_entry(f, &fanout_list, list) {\n\t\tif (f->id == id &&\n\t\t    read_pnet(&f->net) == sock_net(sk)) {\n\t\t\tmatch = f;\n\t\t\tbreak;\n\t\t}\n\t}\n\terr = -EINVAL;\n\tif (match) {\n\t\tif (match->flags != flags)\n\t\t\tgoto out;\n\t\tif (args->max_num_members &&\n\t\t    args->max_num_members != match->max_num_members)\n\t\t\tgoto out;\n\t} else {\n\t\tif (args->max_num_members > PACKET_FANOUT_MAX)\n\t\t\tgoto out;\n\t\tif (!args->max_num_members)\n\t\t\t \n\t\t\targs->max_num_members = 256;\n\t\terr = -ENOMEM;\n\t\tmatch = kvzalloc(struct_size(match, arr, args->max_num_members),\n\t\t\t\t GFP_KERNEL);\n\t\tif (!match)\n\t\t\tgoto out;\n\t\twrite_pnet(&match->net, sock_net(sk));\n\t\tmatch->id = id;\n\t\tmatch->type = type;\n\t\tmatch->flags = flags;\n\t\tINIT_LIST_HEAD(&match->list);\n\t\tspin_lock_init(&match->lock);\n\t\trefcount_set(&match->sk_ref, 0);\n\t\tfanout_init_data(match);\n\t\tmatch->prot_hook.type = po->prot_hook.type;\n\t\tmatch->prot_hook.dev = po->prot_hook.dev;\n\t\tmatch->prot_hook.func = packet_rcv_fanout;\n\t\tmatch->prot_hook.af_packet_priv = match;\n\t\tmatch->prot_hook.af_packet_net = read_pnet(&match->net);\n\t\tmatch->prot_hook.id_match = match_fanout_group;\n\t\tmatch->max_num_members = args->max_num_members;\n\t\tmatch->prot_hook.ignore_outgoing = type_flags & PACKET_FANOUT_FLAG_IGNORE_OUTGOING;\n\t\tlist_add(&match->list, &fanout_list);\n\t}\n\terr = -EINVAL;\n\n\tspin_lock(&po->bind_lock);\n\tif (packet_sock_flag(po, PACKET_SOCK_RUNNING) &&\n\t    match->type == type &&\n\t    match->prot_hook.type == po->prot_hook.type &&\n\t    match->prot_hook.dev == po->prot_hook.dev) {\n\t\terr = -ENOSPC;\n\t\tif (refcount_read(&match->sk_ref) < match->max_num_members) {\n\t\t\t__dev_remove_pack(&po->prot_hook);\n\n\t\t\t \n\t\t\tWRITE_ONCE(po->fanout, match);\n\n\t\t\tpo->rollover = rollover;\n\t\t\trollover = NULL;\n\t\t\trefcount_set(&match->sk_ref, refcount_read(&match->sk_ref) + 1);\n\t\t\t__fanout_link(sk, po);\n\t\t\terr = 0;\n\t\t}\n\t}\n\tspin_unlock(&po->bind_lock);\n\n\tif (err && !refcount_read(&match->sk_ref)) {\n\t\tlist_del(&match->list);\n\t\tkvfree(match);\n\t}\n\nout:\n\tkfree(rollover);\n\tmutex_unlock(&fanout_mutex);\n\treturn err;\n}\n\n \nstatic struct packet_fanout *fanout_release(struct sock *sk)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct packet_fanout *f;\n\n\tmutex_lock(&fanout_mutex);\n\tf = po->fanout;\n\tif (f) {\n\t\tpo->fanout = NULL;\n\n\t\tif (refcount_dec_and_test(&f->sk_ref))\n\t\t\tlist_del(&f->list);\n\t\telse\n\t\t\tf = NULL;\n\t}\n\tmutex_unlock(&fanout_mutex);\n\n\treturn f;\n}\n\nstatic bool packet_extra_vlan_len_allowed(const struct net_device *dev,\n\t\t\t\t\t  struct sk_buff *skb)\n{\n\t \n\tif (unlikely(dev->type != ARPHRD_ETHER))\n\t\treturn false;\n\n\tskb_reset_mac_header(skb);\n\treturn likely(eth_hdr(skb)->h_proto == htons(ETH_P_8021Q));\n}\n\nstatic const struct proto_ops packet_ops;\n\nstatic const struct proto_ops packet_ops_spkt;\n\nstatic int packet_rcv_spkt(struct sk_buff *skb, struct net_device *dev,\n\t\t\t   struct packet_type *pt, struct net_device *orig_dev)\n{\n\tstruct sock *sk;\n\tstruct sockaddr_pkt *spkt;\n\n\t \n\n\tsk = pt->af_packet_priv;\n\n\t \n\n\tif (skb->pkt_type == PACKET_LOOPBACK)\n\t\tgoto out;\n\n\tif (!net_eq(dev_net(dev), sock_net(sk)))\n\t\tgoto out;\n\n\tskb = skb_share_check(skb, GFP_ATOMIC);\n\tif (skb == NULL)\n\t\tgoto oom;\n\n\t \n\tskb_dst_drop(skb);\n\n\t \n\tnf_reset_ct(skb);\n\n\tspkt = &PACKET_SKB_CB(skb)->sa.pkt;\n\n\tskb_push(skb, skb->data - skb_mac_header(skb));\n\n\t \n\n\tspkt->spkt_family = dev->type;\n\tstrscpy(spkt->spkt_device, dev->name, sizeof(spkt->spkt_device));\n\tspkt->spkt_protocol = skb->protocol;\n\n\t \n\n\tif (sock_queue_rcv_skb(sk, skb) == 0)\n\t\treturn 0;\n\nout:\n\tkfree_skb(skb);\noom:\n\treturn 0;\n}\n\nstatic void packet_parse_headers(struct sk_buff *skb, struct socket *sock)\n{\n\tint depth;\n\n\tif ((!skb->protocol || skb->protocol == htons(ETH_P_ALL)) &&\n\t    sock->type == SOCK_RAW) {\n\t\tskb_reset_mac_header(skb);\n\t\tskb->protocol = dev_parse_header_protocol(skb);\n\t}\n\n\t \n\tif (likely(skb->dev->type == ARPHRD_ETHER) &&\n\t    eth_type_vlan(skb->protocol) &&\n\t    vlan_get_protocol_and_depth(skb, skb->protocol, &depth) != 0)\n\t\tskb_set_network_header(skb, depth);\n\n\tskb_probe_transport_header(skb);\n}\n\n \n\nstatic int packet_sendmsg_spkt(struct socket *sock, struct msghdr *msg,\n\t\t\t       size_t len)\n{\n\tstruct sock *sk = sock->sk;\n\tDECLARE_SOCKADDR(struct sockaddr_pkt *, saddr, msg->msg_name);\n\tstruct sk_buff *skb = NULL;\n\tstruct net_device *dev;\n\tstruct sockcm_cookie sockc;\n\t__be16 proto = 0;\n\tint err;\n\tint extra_len = 0;\n\n\t \n\n\tif (saddr) {\n\t\tif (msg->msg_namelen < sizeof(struct sockaddr))\n\t\t\treturn -EINVAL;\n\t\tif (msg->msg_namelen == sizeof(struct sockaddr_pkt))\n\t\t\tproto = saddr->spkt_protocol;\n\t} else\n\t\treturn -ENOTCONN;\t \n\n\t \n\n\tsaddr->spkt_device[sizeof(saddr->spkt_device) - 1] = 0;\nretry:\n\trcu_read_lock();\n\tdev = dev_get_by_name_rcu(sock_net(sk), saddr->spkt_device);\n\terr = -ENODEV;\n\tif (dev == NULL)\n\t\tgoto out_unlock;\n\n\terr = -ENETDOWN;\n\tif (!(dev->flags & IFF_UP))\n\t\tgoto out_unlock;\n\n\t \n\n\tif (unlikely(sock_flag(sk, SOCK_NOFCS))) {\n\t\tif (!netif_supports_nofcs(dev)) {\n\t\t\terr = -EPROTONOSUPPORT;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\textra_len = 4;  \n\t}\n\n\terr = -EMSGSIZE;\n\tif (len > dev->mtu + dev->hard_header_len + VLAN_HLEN + extra_len)\n\t\tgoto out_unlock;\n\n\tif (!skb) {\n\t\tsize_t reserved = LL_RESERVED_SPACE(dev);\n\t\tint tlen = dev->needed_tailroom;\n\t\tunsigned int hhlen = dev->header_ops ? dev->hard_header_len : 0;\n\n\t\trcu_read_unlock();\n\t\tskb = sock_wmalloc(sk, len + reserved + tlen, 0, GFP_KERNEL);\n\t\tif (skb == NULL)\n\t\t\treturn -ENOBUFS;\n\t\t \n\t\tskb_reserve(skb, reserved);\n\t\tskb_reset_network_header(skb);\n\n\t\t \n\t\tif (hhlen) {\n\t\t\tskb->data -= hhlen;\n\t\t\tskb->tail -= hhlen;\n\t\t\tif (len < hhlen)\n\t\t\t\tskb_reset_network_header(skb);\n\t\t}\n\t\terr = memcpy_from_msg(skb_put(skb, len), msg, len);\n\t\tif (err)\n\t\t\tgoto out_free;\n\t\tgoto retry;\n\t}\n\n\tif (!dev_validate_header(dev, skb->data, len) || !skb->len) {\n\t\terr = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\tif (len > (dev->mtu + dev->hard_header_len + extra_len) &&\n\t    !packet_extra_vlan_len_allowed(dev, skb)) {\n\t\terr = -EMSGSIZE;\n\t\tgoto out_unlock;\n\t}\n\n\tsockcm_init(&sockc, sk);\n\tif (msg->msg_controllen) {\n\t\terr = sock_cmsg_send(sk, msg, &sockc);\n\t\tif (unlikely(err))\n\t\t\tgoto out_unlock;\n\t}\n\n\tskb->protocol = proto;\n\tskb->dev = dev;\n\tskb->priority = READ_ONCE(sk->sk_priority);\n\tskb->mark = READ_ONCE(sk->sk_mark);\n\tskb->tstamp = sockc.transmit_time;\n\n\tskb_setup_tx_timestamp(skb, sockc.tsflags);\n\n\tif (unlikely(extra_len == 4))\n\t\tskb->no_fcs = 1;\n\n\tpacket_parse_headers(skb, sock);\n\n\tdev_queue_xmit(skb);\n\trcu_read_unlock();\n\treturn len;\n\nout_unlock:\n\trcu_read_unlock();\nout_free:\n\tkfree_skb(skb);\n\treturn err;\n}\n\nstatic unsigned int run_filter(struct sk_buff *skb,\n\t\t\t       const struct sock *sk,\n\t\t\t       unsigned int res)\n{\n\tstruct sk_filter *filter;\n\n\trcu_read_lock();\n\tfilter = rcu_dereference(sk->sk_filter);\n\tif (filter != NULL)\n\t\tres = bpf_prog_run_clear_cb(filter->prog, skb);\n\trcu_read_unlock();\n\n\treturn res;\n}\n\nstatic int packet_rcv_vnet(struct msghdr *msg, const struct sk_buff *skb,\n\t\t\t   size_t *len, int vnet_hdr_sz)\n{\n\tstruct virtio_net_hdr_mrg_rxbuf vnet_hdr = { .num_buffers = 0 };\n\n\tif (*len < vnet_hdr_sz)\n\t\treturn -EINVAL;\n\t*len -= vnet_hdr_sz;\n\n\tif (virtio_net_hdr_from_skb(skb, (struct virtio_net_hdr *)&vnet_hdr, vio_le(), true, 0))\n\t\treturn -EINVAL;\n\n\treturn memcpy_to_msg(msg, (void *)&vnet_hdr, vnet_hdr_sz);\n}\n\n \n\nstatic int packet_rcv(struct sk_buff *skb, struct net_device *dev,\n\t\t      struct packet_type *pt, struct net_device *orig_dev)\n{\n\tstruct sock *sk;\n\tstruct sockaddr_ll *sll;\n\tstruct packet_sock *po;\n\tu8 *skb_head = skb->data;\n\tint skb_len = skb->len;\n\tunsigned int snaplen, res;\n\tbool is_drop_n_account = false;\n\n\tif (skb->pkt_type == PACKET_LOOPBACK)\n\t\tgoto drop;\n\n\tsk = pt->af_packet_priv;\n\tpo = pkt_sk(sk);\n\n\tif (!net_eq(dev_net(dev), sock_net(sk)))\n\t\tgoto drop;\n\n\tskb->dev = dev;\n\n\tif (dev_has_header(dev)) {\n\t\t \n\t\tif (sk->sk_type != SOCK_DGRAM)\n\t\t\tskb_push(skb, skb->data - skb_mac_header(skb));\n\t\telse if (skb->pkt_type == PACKET_OUTGOING) {\n\t\t\t \n\t\t\tskb_pull(skb, skb_network_offset(skb));\n\t\t}\n\t}\n\n\tsnaplen = skb->len;\n\n\tres = run_filter(skb, sk, snaplen);\n\tif (!res)\n\t\tgoto drop_n_restore;\n\tif (snaplen > res)\n\t\tsnaplen = res;\n\n\tif (atomic_read(&sk->sk_rmem_alloc) >= sk->sk_rcvbuf)\n\t\tgoto drop_n_acct;\n\n\tif (skb_shared(skb)) {\n\t\tstruct sk_buff *nskb = skb_clone(skb, GFP_ATOMIC);\n\t\tif (nskb == NULL)\n\t\t\tgoto drop_n_acct;\n\n\t\tif (skb_head != skb->data) {\n\t\t\tskb->data = skb_head;\n\t\t\tskb->len = skb_len;\n\t\t}\n\t\tconsume_skb(skb);\n\t\tskb = nskb;\n\t}\n\n\tsock_skb_cb_check_size(sizeof(*PACKET_SKB_CB(skb)) + MAX_ADDR_LEN - 8);\n\n\tsll = &PACKET_SKB_CB(skb)->sa.ll;\n\tsll->sll_hatype = dev->type;\n\tsll->sll_pkttype = skb->pkt_type;\n\tif (unlikely(packet_sock_flag(po, PACKET_SOCK_ORIGDEV)))\n\t\tsll->sll_ifindex = orig_dev->ifindex;\n\telse\n\t\tsll->sll_ifindex = dev->ifindex;\n\n\tsll->sll_halen = dev_parse_header(skb, sll->sll_addr);\n\n\t \n\tPACKET_SKB_CB(skb)->sa.origlen = skb->len;\n\n\tif (pskb_trim(skb, snaplen))\n\t\tgoto drop_n_acct;\n\n\tskb_set_owner_r(skb, sk);\n\tskb->dev = NULL;\n\tskb_dst_drop(skb);\n\n\t \n\tnf_reset_ct(skb);\n\n\tspin_lock(&sk->sk_receive_queue.lock);\n\tpo->stats.stats1.tp_packets++;\n\tsock_skb_set_dropcount(sk, skb);\n\tskb_clear_delivery_time(skb);\n\t__skb_queue_tail(&sk->sk_receive_queue, skb);\n\tspin_unlock(&sk->sk_receive_queue.lock);\n\tsk->sk_data_ready(sk);\n\treturn 0;\n\ndrop_n_acct:\n\tis_drop_n_account = true;\n\tatomic_inc(&po->tp_drops);\n\tatomic_inc(&sk->sk_drops);\n\ndrop_n_restore:\n\tif (skb_head != skb->data && skb_shared(skb)) {\n\t\tskb->data = skb_head;\n\t\tskb->len = skb_len;\n\t}\ndrop:\n\tif (!is_drop_n_account)\n\t\tconsume_skb(skb);\n\telse\n\t\tkfree_skb(skb);\n\treturn 0;\n}\n\nstatic int tpacket_rcv(struct sk_buff *skb, struct net_device *dev,\n\t\t       struct packet_type *pt, struct net_device *orig_dev)\n{\n\tstruct sock *sk;\n\tstruct packet_sock *po;\n\tstruct sockaddr_ll *sll;\n\tunion tpacket_uhdr h;\n\tu8 *skb_head = skb->data;\n\tint skb_len = skb->len;\n\tunsigned int snaplen, res;\n\tunsigned long status = TP_STATUS_USER;\n\tunsigned short macoff, hdrlen;\n\tunsigned int netoff;\n\tstruct sk_buff *copy_skb = NULL;\n\tstruct timespec64 ts;\n\t__u32 ts_status;\n\tbool is_drop_n_account = false;\n\tunsigned int slot_id = 0;\n\tint vnet_hdr_sz = 0;\n\n\t \n\tBUILD_BUG_ON(TPACKET_ALIGN(sizeof(*h.h2)) != 32);\n\tBUILD_BUG_ON(TPACKET_ALIGN(sizeof(*h.h3)) != 48);\n\n\tif (skb->pkt_type == PACKET_LOOPBACK)\n\t\tgoto drop;\n\n\tsk = pt->af_packet_priv;\n\tpo = pkt_sk(sk);\n\n\tif (!net_eq(dev_net(dev), sock_net(sk)))\n\t\tgoto drop;\n\n\tif (dev_has_header(dev)) {\n\t\tif (sk->sk_type != SOCK_DGRAM)\n\t\t\tskb_push(skb, skb->data - skb_mac_header(skb));\n\t\telse if (skb->pkt_type == PACKET_OUTGOING) {\n\t\t\t \n\t\t\tskb_pull(skb, skb_network_offset(skb));\n\t\t}\n\t}\n\n\tsnaplen = skb->len;\n\n\tres = run_filter(skb, sk, snaplen);\n\tif (!res)\n\t\tgoto drop_n_restore;\n\n\t \n\tif (__packet_rcv_has_room(po, skb) == ROOM_NONE) {\n\t\tatomic_inc(&po->tp_drops);\n\t\tgoto drop_n_restore;\n\t}\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\tstatus |= TP_STATUS_CSUMNOTREADY;\n\telse if (skb->pkt_type != PACKET_OUTGOING &&\n\t\t skb_csum_unnecessary(skb))\n\t\tstatus |= TP_STATUS_CSUM_VALID;\n\tif (skb_is_gso(skb) && skb_is_gso_tcp(skb))\n\t\tstatus |= TP_STATUS_GSO_TCP;\n\n\tif (snaplen > res)\n\t\tsnaplen = res;\n\n\tif (sk->sk_type == SOCK_DGRAM) {\n\t\tmacoff = netoff = TPACKET_ALIGN(po->tp_hdrlen) + 16 +\n\t\t\t\t  po->tp_reserve;\n\t} else {\n\t\tunsigned int maclen = skb_network_offset(skb);\n\t\tnetoff = TPACKET_ALIGN(po->tp_hdrlen +\n\t\t\t\t       (maclen < 16 ? 16 : maclen)) +\n\t\t\t\t       po->tp_reserve;\n\t\tvnet_hdr_sz = READ_ONCE(po->vnet_hdr_sz);\n\t\tif (vnet_hdr_sz)\n\t\t\tnetoff += vnet_hdr_sz;\n\t\tmacoff = netoff - maclen;\n\t}\n\tif (netoff > USHRT_MAX) {\n\t\tatomic_inc(&po->tp_drops);\n\t\tgoto drop_n_restore;\n\t}\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tif (macoff + snaplen > po->rx_ring.frame_size) {\n\t\t\tif (po->copy_thresh &&\n\t\t\t    atomic_read(&sk->sk_rmem_alloc) < sk->sk_rcvbuf) {\n\t\t\t\tif (skb_shared(skb)) {\n\t\t\t\t\tcopy_skb = skb_clone(skb, GFP_ATOMIC);\n\t\t\t\t} else {\n\t\t\t\t\tcopy_skb = skb_get(skb);\n\t\t\t\t\tskb_head = skb->data;\n\t\t\t\t}\n\t\t\t\tif (copy_skb) {\n\t\t\t\t\tmemset(&PACKET_SKB_CB(copy_skb)->sa.ll, 0,\n\t\t\t\t\t       sizeof(PACKET_SKB_CB(copy_skb)->sa.ll));\n\t\t\t\t\tskb_set_owner_r(copy_skb, sk);\n\t\t\t\t}\n\t\t\t}\n\t\t\tsnaplen = po->rx_ring.frame_size - macoff;\n\t\t\tif ((int)snaplen < 0) {\n\t\t\t\tsnaplen = 0;\n\t\t\t\tvnet_hdr_sz = 0;\n\t\t\t}\n\t\t}\n\t} else if (unlikely(macoff + snaplen >\n\t\t\t    GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len)) {\n\t\tu32 nval;\n\n\t\tnval = GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len - macoff;\n\t\tpr_err_once(\"tpacket_rcv: packet too big, clamped from %u to %u. macoff=%u\\n\",\n\t\t\t    snaplen, nval, macoff);\n\t\tsnaplen = nval;\n\t\tif (unlikely((int)snaplen < 0)) {\n\t\t\tsnaplen = 0;\n\t\t\tmacoff = GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len;\n\t\t\tvnet_hdr_sz = 0;\n\t\t}\n\t}\n\tspin_lock(&sk->sk_receive_queue.lock);\n\th.raw = packet_current_rx_frame(po, skb,\n\t\t\t\t\tTP_STATUS_KERNEL, (macoff+snaplen));\n\tif (!h.raw)\n\t\tgoto drop_n_account;\n\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tslot_id = po->rx_ring.head;\n\t\tif (test_bit(slot_id, po->rx_ring.rx_owner_map))\n\t\t\tgoto drop_n_account;\n\t\t__set_bit(slot_id, po->rx_ring.rx_owner_map);\n\t}\n\n\tif (vnet_hdr_sz &&\n\t    virtio_net_hdr_from_skb(skb, h.raw + macoff -\n\t\t\t\t    sizeof(struct virtio_net_hdr),\n\t\t\t\t    vio_le(), true, 0)) {\n\t\tif (po->tp_version == TPACKET_V3)\n\t\t\tprb_clear_blk_fill_status(&po->rx_ring);\n\t\tgoto drop_n_account;\n\t}\n\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tpacket_increment_rx_head(po, &po->rx_ring);\n\t \n\t\tif (atomic_read(&po->tp_drops))\n\t\t\tstatus |= TP_STATUS_LOSING;\n\t}\n\n\tpo->stats.stats1.tp_packets++;\n\tif (copy_skb) {\n\t\tstatus |= TP_STATUS_COPY;\n\t\tskb_clear_delivery_time(copy_skb);\n\t\t__skb_queue_tail(&sk->sk_receive_queue, copy_skb);\n\t}\n\tspin_unlock(&sk->sk_receive_queue.lock);\n\n\tskb_copy_bits(skb, 0, h.raw + macoff, snaplen);\n\n\t \n\tts_status = tpacket_get_timestamp(skb, &ts,\n\t\t\t\t\t  READ_ONCE(po->tp_tstamp) |\n\t\t\t\t\t  SOF_TIMESTAMPING_SOFTWARE);\n\tif (!ts_status)\n\t\tktime_get_real_ts64(&ts);\n\n\tstatus |= ts_status;\n\n\tswitch (po->tp_version) {\n\tcase TPACKET_V1:\n\t\th.h1->tp_len = skb->len;\n\t\th.h1->tp_snaplen = snaplen;\n\t\th.h1->tp_mac = macoff;\n\t\th.h1->tp_net = netoff;\n\t\th.h1->tp_sec = ts.tv_sec;\n\t\th.h1->tp_usec = ts.tv_nsec / NSEC_PER_USEC;\n\t\thdrlen = sizeof(*h.h1);\n\t\tbreak;\n\tcase TPACKET_V2:\n\t\th.h2->tp_len = skb->len;\n\t\th.h2->tp_snaplen = snaplen;\n\t\th.h2->tp_mac = macoff;\n\t\th.h2->tp_net = netoff;\n\t\th.h2->tp_sec = ts.tv_sec;\n\t\th.h2->tp_nsec = ts.tv_nsec;\n\t\tif (skb_vlan_tag_present(skb)) {\n\t\t\th.h2->tp_vlan_tci = skb_vlan_tag_get(skb);\n\t\t\th.h2->tp_vlan_tpid = ntohs(skb->vlan_proto);\n\t\t\tstatus |= TP_STATUS_VLAN_VALID | TP_STATUS_VLAN_TPID_VALID;\n\t\t} else {\n\t\t\th.h2->tp_vlan_tci = 0;\n\t\t\th.h2->tp_vlan_tpid = 0;\n\t\t}\n\t\tmemset(h.h2->tp_padding, 0, sizeof(h.h2->tp_padding));\n\t\thdrlen = sizeof(*h.h2);\n\t\tbreak;\n\tcase TPACKET_V3:\n\t\t \n\t\th.h3->tp_status |= status;\n\t\th.h3->tp_len = skb->len;\n\t\th.h3->tp_snaplen = snaplen;\n\t\th.h3->tp_mac = macoff;\n\t\th.h3->tp_net = netoff;\n\t\th.h3->tp_sec  = ts.tv_sec;\n\t\th.h3->tp_nsec = ts.tv_nsec;\n\t\tmemset(h.h3->tp_padding, 0, sizeof(h.h3->tp_padding));\n\t\thdrlen = sizeof(*h.h3);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\tsll = h.raw + TPACKET_ALIGN(hdrlen);\n\tsll->sll_halen = dev_parse_header(skb, sll->sll_addr);\n\tsll->sll_family = AF_PACKET;\n\tsll->sll_hatype = dev->type;\n\tsll->sll_protocol = skb->protocol;\n\tsll->sll_pkttype = skb->pkt_type;\n\tif (unlikely(packet_sock_flag(po, PACKET_SOCK_ORIGDEV)))\n\t\tsll->sll_ifindex = orig_dev->ifindex;\n\telse\n\t\tsll->sll_ifindex = dev->ifindex;\n\n\tsmp_mb();\n\n#if ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE == 1\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tu8 *start, *end;\n\n\t\tend = (u8 *) PAGE_ALIGN((unsigned long) h.raw +\n\t\t\t\t\tmacoff + snaplen);\n\n\t\tfor (start = h.raw; start < end; start += PAGE_SIZE)\n\t\t\tflush_dcache_page(pgv_to_page(start));\n\t}\n\tsmp_wmb();\n#endif\n\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tspin_lock(&sk->sk_receive_queue.lock);\n\t\t__packet_set_status(po, h.raw, status);\n\t\t__clear_bit(slot_id, po->rx_ring.rx_owner_map);\n\t\tspin_unlock(&sk->sk_receive_queue.lock);\n\t\tsk->sk_data_ready(sk);\n\t} else if (po->tp_version == TPACKET_V3) {\n\t\tprb_clear_blk_fill_status(&po->rx_ring);\n\t}\n\ndrop_n_restore:\n\tif (skb_head != skb->data && skb_shared(skb)) {\n\t\tskb->data = skb_head;\n\t\tskb->len = skb_len;\n\t}\ndrop:\n\tif (!is_drop_n_account)\n\t\tconsume_skb(skb);\n\telse\n\t\tkfree_skb(skb);\n\treturn 0;\n\ndrop_n_account:\n\tspin_unlock(&sk->sk_receive_queue.lock);\n\tatomic_inc(&po->tp_drops);\n\tis_drop_n_account = true;\n\n\tsk->sk_data_ready(sk);\n\tkfree_skb(copy_skb);\n\tgoto drop_n_restore;\n}\n\nstatic void tpacket_destruct_skb(struct sk_buff *skb)\n{\n\tstruct packet_sock *po = pkt_sk(skb->sk);\n\n\tif (likely(po->tx_ring.pg_vec)) {\n\t\tvoid *ph;\n\t\t__u32 ts;\n\n\t\tph = skb_zcopy_get_nouarg(skb);\n\t\tpacket_dec_pending(&po->tx_ring);\n\n\t\tts = __packet_set_timestamp(po, ph, skb);\n\t\t__packet_set_status(po, ph, TP_STATUS_AVAILABLE | ts);\n\n\t\tif (!packet_read_pending(&po->tx_ring))\n\t\t\tcomplete(&po->skb_completion);\n\t}\n\n\tsock_wfree(skb);\n}\n\nstatic int __packet_snd_vnet_parse(struct virtio_net_hdr *vnet_hdr, size_t len)\n{\n\tif ((vnet_hdr->flags & VIRTIO_NET_HDR_F_NEEDS_CSUM) &&\n\t    (__virtio16_to_cpu(vio_le(), vnet_hdr->csum_start) +\n\t     __virtio16_to_cpu(vio_le(), vnet_hdr->csum_offset) + 2 >\n\t      __virtio16_to_cpu(vio_le(), vnet_hdr->hdr_len)))\n\t\tvnet_hdr->hdr_len = __cpu_to_virtio16(vio_le(),\n\t\t\t __virtio16_to_cpu(vio_le(), vnet_hdr->csum_start) +\n\t\t\t__virtio16_to_cpu(vio_le(), vnet_hdr->csum_offset) + 2);\n\n\tif (__virtio16_to_cpu(vio_le(), vnet_hdr->hdr_len) > len)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int packet_snd_vnet_parse(struct msghdr *msg, size_t *len,\n\t\t\t\t struct virtio_net_hdr *vnet_hdr, int vnet_hdr_sz)\n{\n\tint ret;\n\n\tif (*len < vnet_hdr_sz)\n\t\treturn -EINVAL;\n\t*len -= vnet_hdr_sz;\n\n\tif (!copy_from_iter_full(vnet_hdr, sizeof(*vnet_hdr), &msg->msg_iter))\n\t\treturn -EFAULT;\n\n\tret = __packet_snd_vnet_parse(vnet_hdr, *len);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tif (vnet_hdr_sz != sizeof(struct virtio_net_hdr))\n\t\tiov_iter_advance(&msg->msg_iter, vnet_hdr_sz - sizeof(struct virtio_net_hdr));\n\n\treturn 0;\n}\n\nstatic int tpacket_fill_skb(struct packet_sock *po, struct sk_buff *skb,\n\t\tvoid *frame, struct net_device *dev, void *data, int tp_len,\n\t\t__be16 proto, unsigned char *addr, int hlen, int copylen,\n\t\tconst struct sockcm_cookie *sockc)\n{\n\tunion tpacket_uhdr ph;\n\tint to_write, offset, len, nr_frags, len_max;\n\tstruct socket *sock = po->sk.sk_socket;\n\tstruct page *page;\n\tint err;\n\n\tph.raw = frame;\n\n\tskb->protocol = proto;\n\tskb->dev = dev;\n\tskb->priority = READ_ONCE(po->sk.sk_priority);\n\tskb->mark = READ_ONCE(po->sk.sk_mark);\n\tskb->tstamp = sockc->transmit_time;\n\tskb_setup_tx_timestamp(skb, sockc->tsflags);\n\tskb_zcopy_set_nouarg(skb, ph.raw);\n\n\tskb_reserve(skb, hlen);\n\tskb_reset_network_header(skb);\n\n\tto_write = tp_len;\n\n\tif (sock->type == SOCK_DGRAM) {\n\t\terr = dev_hard_header(skb, dev, ntohs(proto), addr,\n\t\t\t\tNULL, tp_len);\n\t\tif (unlikely(err < 0))\n\t\t\treturn -EINVAL;\n\t} else if (copylen) {\n\t\tint hdrlen = min_t(int, copylen, tp_len);\n\n\t\tskb_push(skb, dev->hard_header_len);\n\t\tskb_put(skb, copylen - dev->hard_header_len);\n\t\terr = skb_store_bits(skb, 0, data, hdrlen);\n\t\tif (unlikely(err))\n\t\t\treturn err;\n\t\tif (!dev_validate_header(dev, skb->data, hdrlen))\n\t\t\treturn -EINVAL;\n\n\t\tdata += hdrlen;\n\t\tto_write -= hdrlen;\n\t}\n\n\toffset = offset_in_page(data);\n\tlen_max = PAGE_SIZE - offset;\n\tlen = ((to_write > len_max) ? len_max : to_write);\n\n\tskb->data_len = to_write;\n\tskb->len += to_write;\n\tskb->truesize += to_write;\n\trefcount_add(to_write, &po->sk.sk_wmem_alloc);\n\n\twhile (likely(to_write)) {\n\t\tnr_frags = skb_shinfo(skb)->nr_frags;\n\n\t\tif (unlikely(nr_frags >= MAX_SKB_FRAGS)) {\n\t\t\tpr_err(\"Packet exceed the number of skb frags(%u)\\n\",\n\t\t\t       (unsigned int)MAX_SKB_FRAGS);\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tpage = pgv_to_page(data);\n\t\tdata += len;\n\t\tflush_dcache_page(page);\n\t\tget_page(page);\n\t\tskb_fill_page_desc(skb, nr_frags, page, offset, len);\n\t\tto_write -= len;\n\t\toffset = 0;\n\t\tlen_max = PAGE_SIZE;\n\t\tlen = ((to_write > len_max) ? len_max : to_write);\n\t}\n\n\tpacket_parse_headers(skb, sock);\n\n\treturn tp_len;\n}\n\nstatic int tpacket_parse_header(struct packet_sock *po, void *frame,\n\t\t\t\tint size_max, void **data)\n{\n\tunion tpacket_uhdr ph;\n\tint tp_len, off;\n\n\tph.raw = frame;\n\n\tswitch (po->tp_version) {\n\tcase TPACKET_V3:\n\t\tif (ph.h3->tp_next_offset != 0) {\n\t\t\tpr_warn_once(\"variable sized slot not supported\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\ttp_len = ph.h3->tp_len;\n\t\tbreak;\n\tcase TPACKET_V2:\n\t\ttp_len = ph.h2->tp_len;\n\t\tbreak;\n\tdefault:\n\t\ttp_len = ph.h1->tp_len;\n\t\tbreak;\n\t}\n\tif (unlikely(tp_len > size_max)) {\n\t\tpr_err(\"packet size is too long (%d > %d)\\n\", tp_len, size_max);\n\t\treturn -EMSGSIZE;\n\t}\n\n\tif (unlikely(packet_sock_flag(po, PACKET_SOCK_TX_HAS_OFF))) {\n\t\tint off_min, off_max;\n\n\t\toff_min = po->tp_hdrlen - sizeof(struct sockaddr_ll);\n\t\toff_max = po->tx_ring.frame_size - tp_len;\n\t\tif (po->sk.sk_type == SOCK_DGRAM) {\n\t\t\tswitch (po->tp_version) {\n\t\t\tcase TPACKET_V3:\n\t\t\t\toff = ph.h3->tp_net;\n\t\t\t\tbreak;\n\t\t\tcase TPACKET_V2:\n\t\t\t\toff = ph.h2->tp_net;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\toff = ph.h1->tp_net;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} else {\n\t\t\tswitch (po->tp_version) {\n\t\t\tcase TPACKET_V3:\n\t\t\t\toff = ph.h3->tp_mac;\n\t\t\t\tbreak;\n\t\t\tcase TPACKET_V2:\n\t\t\t\toff = ph.h2->tp_mac;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\toff = ph.h1->tp_mac;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (unlikely((off < off_min) || (off_max < off)))\n\t\t\treturn -EINVAL;\n\t} else {\n\t\toff = po->tp_hdrlen - sizeof(struct sockaddr_ll);\n\t}\n\n\t*data = frame + off;\n\treturn tp_len;\n}\n\nstatic int tpacket_snd(struct packet_sock *po, struct msghdr *msg)\n{\n\tstruct sk_buff *skb = NULL;\n\tstruct net_device *dev;\n\tstruct virtio_net_hdr *vnet_hdr = NULL;\n\tstruct sockcm_cookie sockc;\n\t__be16 proto;\n\tint err, reserve = 0;\n\tvoid *ph;\n\tDECLARE_SOCKADDR(struct sockaddr_ll *, saddr, msg->msg_name);\n\tbool need_wait = !(msg->msg_flags & MSG_DONTWAIT);\n\tint vnet_hdr_sz = READ_ONCE(po->vnet_hdr_sz);\n\tunsigned char *addr = NULL;\n\tint tp_len, size_max;\n\tvoid *data;\n\tint len_sum = 0;\n\tint status = TP_STATUS_AVAILABLE;\n\tint hlen, tlen, copylen = 0;\n\tlong timeo = 0;\n\n\tmutex_lock(&po->pg_vec_lock);\n\n\t \n\tif (unlikely(!po->tx_ring.pg_vec)) {\n\t\terr = -EBUSY;\n\t\tgoto out;\n\t}\n\tif (likely(saddr == NULL)) {\n\t\tdev\t= packet_cached_dev_get(po);\n\t\tproto\t= READ_ONCE(po->num);\n\t} else {\n\t\terr = -EINVAL;\n\t\tif (msg->msg_namelen < sizeof(struct sockaddr_ll))\n\t\t\tgoto out;\n\t\tif (msg->msg_namelen < (saddr->sll_halen\n\t\t\t\t\t+ offsetof(struct sockaddr_ll,\n\t\t\t\t\t\tsll_addr)))\n\t\t\tgoto out;\n\t\tproto\t= saddr->sll_protocol;\n\t\tdev = dev_get_by_index(sock_net(&po->sk), saddr->sll_ifindex);\n\t\tif (po->sk.sk_socket->type == SOCK_DGRAM) {\n\t\t\tif (dev && msg->msg_namelen < dev->addr_len +\n\t\t\t\t   offsetof(struct sockaddr_ll, sll_addr))\n\t\t\t\tgoto out_put;\n\t\t\taddr = saddr->sll_addr;\n\t\t}\n\t}\n\n\terr = -ENXIO;\n\tif (unlikely(dev == NULL))\n\t\tgoto out;\n\terr = -ENETDOWN;\n\tif (unlikely(!(dev->flags & IFF_UP)))\n\t\tgoto out_put;\n\n\tsockcm_init(&sockc, &po->sk);\n\tif (msg->msg_controllen) {\n\t\terr = sock_cmsg_send(&po->sk, msg, &sockc);\n\t\tif (unlikely(err))\n\t\t\tgoto out_put;\n\t}\n\n\tif (po->sk.sk_socket->type == SOCK_RAW)\n\t\treserve = dev->hard_header_len;\n\tsize_max = po->tx_ring.frame_size\n\t\t- (po->tp_hdrlen - sizeof(struct sockaddr_ll));\n\n\tif ((size_max > dev->mtu + reserve + VLAN_HLEN) && !vnet_hdr_sz)\n\t\tsize_max = dev->mtu + reserve + VLAN_HLEN;\n\n\treinit_completion(&po->skb_completion);\n\n\tdo {\n\t\tph = packet_current_frame(po, &po->tx_ring,\n\t\t\t\t\t  TP_STATUS_SEND_REQUEST);\n\t\tif (unlikely(ph == NULL)) {\n\t\t\tif (need_wait && skb) {\n\t\t\t\ttimeo = sock_sndtimeo(&po->sk, msg->msg_flags & MSG_DONTWAIT);\n\t\t\t\ttimeo = wait_for_completion_interruptible_timeout(&po->skb_completion, timeo);\n\t\t\t\tif (timeo <= 0) {\n\t\t\t\t\terr = !timeo ? -ETIMEDOUT : -ERESTARTSYS;\n\t\t\t\t\tgoto out_put;\n\t\t\t\t}\n\t\t\t}\n\t\t\t \n\t\t\tcontinue;\n\t\t}\n\n\t\tskb = NULL;\n\t\ttp_len = tpacket_parse_header(po, ph, size_max, &data);\n\t\tif (tp_len < 0)\n\t\t\tgoto tpacket_error;\n\n\t\tstatus = TP_STATUS_SEND_REQUEST;\n\t\thlen = LL_RESERVED_SPACE(dev);\n\t\ttlen = dev->needed_tailroom;\n\t\tif (vnet_hdr_sz) {\n\t\t\tvnet_hdr = data;\n\t\t\tdata += vnet_hdr_sz;\n\t\t\ttp_len -= vnet_hdr_sz;\n\t\t\tif (tp_len < 0 ||\n\t\t\t    __packet_snd_vnet_parse(vnet_hdr, tp_len)) {\n\t\t\t\ttp_len = -EINVAL;\n\t\t\t\tgoto tpacket_error;\n\t\t\t}\n\t\t\tcopylen = __virtio16_to_cpu(vio_le(),\n\t\t\t\t\t\t    vnet_hdr->hdr_len);\n\t\t}\n\t\tcopylen = max_t(int, copylen, dev->hard_header_len);\n\t\tskb = sock_alloc_send_skb(&po->sk,\n\t\t\t\thlen + tlen + sizeof(struct sockaddr_ll) +\n\t\t\t\t(copylen - dev->hard_header_len),\n\t\t\t\t!need_wait, &err);\n\n\t\tif (unlikely(skb == NULL)) {\n\t\t\t \n\t\t\tif (likely(len_sum > 0))\n\t\t\t\terr = len_sum;\n\t\t\tgoto out_status;\n\t\t}\n\t\ttp_len = tpacket_fill_skb(po, skb, ph, dev, data, tp_len, proto,\n\t\t\t\t\t  addr, hlen, copylen, &sockc);\n\t\tif (likely(tp_len >= 0) &&\n\t\t    tp_len > dev->mtu + reserve &&\n\t\t    !vnet_hdr_sz &&\n\t\t    !packet_extra_vlan_len_allowed(dev, skb))\n\t\t\ttp_len = -EMSGSIZE;\n\n\t\tif (unlikely(tp_len < 0)) {\ntpacket_error:\n\t\t\tif (packet_sock_flag(po, PACKET_SOCK_TP_LOSS)) {\n\t\t\t\t__packet_set_status(po, ph,\n\t\t\t\t\t\tTP_STATUS_AVAILABLE);\n\t\t\t\tpacket_increment_head(&po->tx_ring);\n\t\t\t\tkfree_skb(skb);\n\t\t\t\tcontinue;\n\t\t\t} else {\n\t\t\t\tstatus = TP_STATUS_WRONG_FORMAT;\n\t\t\t\terr = tp_len;\n\t\t\t\tgoto out_status;\n\t\t\t}\n\t\t}\n\n\t\tif (vnet_hdr_sz) {\n\t\t\tif (virtio_net_hdr_to_skb(skb, vnet_hdr, vio_le())) {\n\t\t\t\ttp_len = -EINVAL;\n\t\t\t\tgoto tpacket_error;\n\t\t\t}\n\t\t\tvirtio_net_hdr_set_proto(skb, vnet_hdr);\n\t\t}\n\n\t\tskb->destructor = tpacket_destruct_skb;\n\t\t__packet_set_status(po, ph, TP_STATUS_SENDING);\n\t\tpacket_inc_pending(&po->tx_ring);\n\n\t\tstatus = TP_STATUS_SEND_REQUEST;\n\t\terr = packet_xmit(po, skb);\n\t\tif (unlikely(err != 0)) {\n\t\t\tif (err > 0)\n\t\t\t\terr = net_xmit_errno(err);\n\t\t\tif (err && __packet_get_status(po, ph) ==\n\t\t\t\t   TP_STATUS_AVAILABLE) {\n\t\t\t\t \n\t\t\t\tskb = NULL;\n\t\t\t\tgoto out_status;\n\t\t\t}\n\t\t\t \n\t\t\terr = 0;\n\t\t}\n\t\tpacket_increment_head(&po->tx_ring);\n\t\tlen_sum += tp_len;\n\t} while (likely((ph != NULL) ||\n\t\t \n\t\t (need_wait && packet_read_pending(&po->tx_ring))));\n\n\terr = len_sum;\n\tgoto out_put;\n\nout_status:\n\t__packet_set_status(po, ph, status);\n\tkfree_skb(skb);\nout_put:\n\tdev_put(dev);\nout:\n\tmutex_unlock(&po->pg_vec_lock);\n\treturn err;\n}\n\nstatic struct sk_buff *packet_alloc_skb(struct sock *sk, size_t prepad,\n\t\t\t\t        size_t reserve, size_t len,\n\t\t\t\t        size_t linear, int noblock,\n\t\t\t\t        int *err)\n{\n\tstruct sk_buff *skb;\n\n\t \n\tif (prepad + len < PAGE_SIZE || !linear)\n\t\tlinear = len;\n\n\tif (len - linear > MAX_SKB_FRAGS * (PAGE_SIZE << PAGE_ALLOC_COSTLY_ORDER))\n\t\tlinear = len - MAX_SKB_FRAGS * (PAGE_SIZE << PAGE_ALLOC_COSTLY_ORDER);\n\tskb = sock_alloc_send_pskb(sk, prepad + linear, len - linear, noblock,\n\t\t\t\t   err, PAGE_ALLOC_COSTLY_ORDER);\n\tif (!skb)\n\t\treturn NULL;\n\n\tskb_reserve(skb, reserve);\n\tskb_put(skb, linear);\n\tskb->data_len = len - linear;\n\tskb->len += len - linear;\n\n\treturn skb;\n}\n\nstatic int packet_snd(struct socket *sock, struct msghdr *msg, size_t len)\n{\n\tstruct sock *sk = sock->sk;\n\tDECLARE_SOCKADDR(struct sockaddr_ll *, saddr, msg->msg_name);\n\tstruct sk_buff *skb;\n\tstruct net_device *dev;\n\t__be16 proto;\n\tunsigned char *addr = NULL;\n\tint err, reserve = 0;\n\tstruct sockcm_cookie sockc;\n\tstruct virtio_net_hdr vnet_hdr = { 0 };\n\tint offset = 0;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tint vnet_hdr_sz = READ_ONCE(po->vnet_hdr_sz);\n\tint hlen, tlen, linear;\n\tint extra_len = 0;\n\n\t \n\n\tif (likely(saddr == NULL)) {\n\t\tdev\t= packet_cached_dev_get(po);\n\t\tproto\t= READ_ONCE(po->num);\n\t} else {\n\t\terr = -EINVAL;\n\t\tif (msg->msg_namelen < sizeof(struct sockaddr_ll))\n\t\t\tgoto out;\n\t\tif (msg->msg_namelen < (saddr->sll_halen + offsetof(struct sockaddr_ll, sll_addr)))\n\t\t\tgoto out;\n\t\tproto\t= saddr->sll_protocol;\n\t\tdev = dev_get_by_index(sock_net(sk), saddr->sll_ifindex);\n\t\tif (sock->type == SOCK_DGRAM) {\n\t\t\tif (dev && msg->msg_namelen < dev->addr_len +\n\t\t\t\t   offsetof(struct sockaddr_ll, sll_addr))\n\t\t\t\tgoto out_unlock;\n\t\t\taddr = saddr->sll_addr;\n\t\t}\n\t}\n\n\terr = -ENXIO;\n\tif (unlikely(dev == NULL))\n\t\tgoto out_unlock;\n\terr = -ENETDOWN;\n\tif (unlikely(!(dev->flags & IFF_UP)))\n\t\tgoto out_unlock;\n\n\tsockcm_init(&sockc, sk);\n\tsockc.mark = READ_ONCE(sk->sk_mark);\n\tif (msg->msg_controllen) {\n\t\terr = sock_cmsg_send(sk, msg, &sockc);\n\t\tif (unlikely(err))\n\t\t\tgoto out_unlock;\n\t}\n\n\tif (sock->type == SOCK_RAW)\n\t\treserve = dev->hard_header_len;\n\tif (vnet_hdr_sz) {\n\t\terr = packet_snd_vnet_parse(msg, &len, &vnet_hdr, vnet_hdr_sz);\n\t\tif (err)\n\t\t\tgoto out_unlock;\n\t}\n\n\tif (unlikely(sock_flag(sk, SOCK_NOFCS))) {\n\t\tif (!netif_supports_nofcs(dev)) {\n\t\t\terr = -EPROTONOSUPPORT;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\textra_len = 4;  \n\t}\n\n\terr = -EMSGSIZE;\n\tif (!vnet_hdr.gso_type &&\n\t    (len > dev->mtu + reserve + VLAN_HLEN + extra_len))\n\t\tgoto out_unlock;\n\n\terr = -ENOBUFS;\n\thlen = LL_RESERVED_SPACE(dev);\n\ttlen = dev->needed_tailroom;\n\tlinear = __virtio16_to_cpu(vio_le(), vnet_hdr.hdr_len);\n\tlinear = max(linear, min_t(int, len, dev->hard_header_len));\n\tskb = packet_alloc_skb(sk, hlen + tlen, hlen, len, linear,\n\t\t\t       msg->msg_flags & MSG_DONTWAIT, &err);\n\tif (skb == NULL)\n\t\tgoto out_unlock;\n\n\tskb_reset_network_header(skb);\n\n\terr = -EINVAL;\n\tif (sock->type == SOCK_DGRAM) {\n\t\toffset = dev_hard_header(skb, dev, ntohs(proto), addr, NULL, len);\n\t\tif (unlikely(offset < 0))\n\t\t\tgoto out_free;\n\t} else if (reserve) {\n\t\tskb_reserve(skb, -reserve);\n\t\tif (len < reserve + sizeof(struct ipv6hdr) &&\n\t\t    dev->min_header_len != dev->hard_header_len)\n\t\t\tskb_reset_network_header(skb);\n\t}\n\n\t \n\terr = skb_copy_datagram_from_iter(skb, offset, &msg->msg_iter, len);\n\tif (err)\n\t\tgoto out_free;\n\n\tif ((sock->type == SOCK_RAW &&\n\t     !dev_validate_header(dev, skb->data, len)) || !skb->len) {\n\t\terr = -EINVAL;\n\t\tgoto out_free;\n\t}\n\n\tskb_setup_tx_timestamp(skb, sockc.tsflags);\n\n\tif (!vnet_hdr.gso_type && (len > dev->mtu + reserve + extra_len) &&\n\t    !packet_extra_vlan_len_allowed(dev, skb)) {\n\t\terr = -EMSGSIZE;\n\t\tgoto out_free;\n\t}\n\n\tskb->protocol = proto;\n\tskb->dev = dev;\n\tskb->priority = READ_ONCE(sk->sk_priority);\n\tskb->mark = sockc.mark;\n\tskb->tstamp = sockc.transmit_time;\n\n\tif (unlikely(extra_len == 4))\n\t\tskb->no_fcs = 1;\n\n\tpacket_parse_headers(skb, sock);\n\n\tif (vnet_hdr_sz) {\n\t\terr = virtio_net_hdr_to_skb(skb, &vnet_hdr, vio_le());\n\t\tif (err)\n\t\t\tgoto out_free;\n\t\tlen += vnet_hdr_sz;\n\t\tvirtio_net_hdr_set_proto(skb, &vnet_hdr);\n\t}\n\n\terr = packet_xmit(po, skb);\n\n\tif (unlikely(err != 0)) {\n\t\tif (err > 0)\n\t\t\terr = net_xmit_errno(err);\n\t\tif (err)\n\t\t\tgoto out_unlock;\n\t}\n\n\tdev_put(dev);\n\n\treturn len;\n\nout_free:\n\tkfree_skb(skb);\nout_unlock:\n\tdev_put(dev);\nout:\n\treturn err;\n}\n\nstatic int packet_sendmsg(struct socket *sock, struct msghdr *msg, size_t len)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\n\t \n\tif (data_race(po->tx_ring.pg_vec))\n\t\treturn tpacket_snd(po, msg);\n\n\treturn packet_snd(sock, msg, len);\n}\n\n \n\nstatic int packet_release(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po;\n\tstruct packet_fanout *f;\n\tstruct net *net;\n\tunion tpacket_req_u req_u;\n\n\tif (!sk)\n\t\treturn 0;\n\n\tnet = sock_net(sk);\n\tpo = pkt_sk(sk);\n\n\tmutex_lock(&net->packet.sklist_lock);\n\tsk_del_node_init_rcu(sk);\n\tmutex_unlock(&net->packet.sklist_lock);\n\n\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\n\tspin_lock(&po->bind_lock);\n\tunregister_prot_hook(sk, false);\n\tpacket_cached_dev_reset(po);\n\n\tif (po->prot_hook.dev) {\n\t\tnetdev_put(po->prot_hook.dev, &po->prot_hook.dev_tracker);\n\t\tpo->prot_hook.dev = NULL;\n\t}\n\tspin_unlock(&po->bind_lock);\n\n\tpacket_flush_mclist(sk);\n\n\tlock_sock(sk);\n\tif (po->rx_ring.pg_vec) {\n\t\tmemset(&req_u, 0, sizeof(req_u));\n\t\tpacket_set_ring(sk, &req_u, 1, 0);\n\t}\n\n\tif (po->tx_ring.pg_vec) {\n\t\tmemset(&req_u, 0, sizeof(req_u));\n\t\tpacket_set_ring(sk, &req_u, 1, 1);\n\t}\n\trelease_sock(sk);\n\n\tf = fanout_release(sk);\n\n\tsynchronize_net();\n\n\tkfree(po->rollover);\n\tif (f) {\n\t\tfanout_release_data(f);\n\t\tkvfree(f);\n\t}\n\t \n\tsock_orphan(sk);\n\tsock->sk = NULL;\n\n\t \n\n\tskb_queue_purge(&sk->sk_receive_queue);\n\tpacket_free_pending(po);\n\n\tsock_put(sk);\n\treturn 0;\n}\n\n \n\nstatic int packet_do_bind(struct sock *sk, const char *name, int ifindex,\n\t\t\t  __be16 proto)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct net_device *dev = NULL;\n\tbool unlisted = false;\n\tbool need_rehook;\n\tint ret = 0;\n\n\tlock_sock(sk);\n\tspin_lock(&po->bind_lock);\n\tif (!proto)\n\t\tproto = po->num;\n\n\trcu_read_lock();\n\n\tif (po->fanout) {\n\t\tret = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\tif (name) {\n\t\tdev = dev_get_by_name_rcu(sock_net(sk), name);\n\t\tif (!dev) {\n\t\t\tret = -ENODEV;\n\t\t\tgoto out_unlock;\n\t\t}\n\t} else if (ifindex) {\n\t\tdev = dev_get_by_index_rcu(sock_net(sk), ifindex);\n\t\tif (!dev) {\n\t\t\tret = -ENODEV;\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\tneed_rehook = po->prot_hook.type != proto || po->prot_hook.dev != dev;\n\n\tif (need_rehook) {\n\t\tdev_hold(dev);\n\t\tif (packet_sock_flag(po, PACKET_SOCK_RUNNING)) {\n\t\t\trcu_read_unlock();\n\t\t\t \n\t\t\tWRITE_ONCE(po->num, 0);\n\t\t\t__unregister_prot_hook(sk, true);\n\t\t\trcu_read_lock();\n\t\t\tif (dev)\n\t\t\t\tunlisted = !dev_get_by_index_rcu(sock_net(sk),\n\t\t\t\t\t\t\t\t dev->ifindex);\n\t\t}\n\n\t\tBUG_ON(packet_sock_flag(po, PACKET_SOCK_RUNNING));\n\t\tWRITE_ONCE(po->num, proto);\n\t\tpo->prot_hook.type = proto;\n\n\t\tnetdev_put(po->prot_hook.dev, &po->prot_hook.dev_tracker);\n\n\t\tif (unlikely(unlisted)) {\n\t\t\tpo->prot_hook.dev = NULL;\n\t\t\tWRITE_ONCE(po->ifindex, -1);\n\t\t\tpacket_cached_dev_reset(po);\n\t\t} else {\n\t\t\tnetdev_hold(dev, &po->prot_hook.dev_tracker,\n\t\t\t\t    GFP_ATOMIC);\n\t\t\tpo->prot_hook.dev = dev;\n\t\t\tWRITE_ONCE(po->ifindex, dev ? dev->ifindex : 0);\n\t\t\tpacket_cached_dev_assign(po, dev);\n\t\t}\n\t\tdev_put(dev);\n\t}\n\n\tif (proto == 0 || !need_rehook)\n\t\tgoto out_unlock;\n\n\tif (!unlisted && (!dev || (dev->flags & IFF_UP))) {\n\t\tregister_prot_hook(sk);\n\t} else {\n\t\tsk->sk_err = ENETDOWN;\n\t\tif (!sock_flag(sk, SOCK_DEAD))\n\t\t\tsk_error_report(sk);\n\t}\n\nout_unlock:\n\trcu_read_unlock();\n\tspin_unlock(&po->bind_lock);\n\trelease_sock(sk);\n\treturn ret;\n}\n\n \n\nstatic int packet_bind_spkt(struct socket *sock, struct sockaddr *uaddr,\n\t\t\t    int addr_len)\n{\n\tstruct sock *sk = sock->sk;\n\tchar name[sizeof(uaddr->sa_data_min) + 1];\n\n\t \n\n\tif (addr_len != sizeof(struct sockaddr))\n\t\treturn -EINVAL;\n\t \n\tmemcpy(name, uaddr->sa_data, sizeof(uaddr->sa_data_min));\n\tname[sizeof(uaddr->sa_data_min)] = 0;\n\n\treturn packet_do_bind(sk, name, 0, 0);\n}\n\nstatic int packet_bind(struct socket *sock, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_ll *sll = (struct sockaddr_ll *)uaddr;\n\tstruct sock *sk = sock->sk;\n\n\t \n\n\tif (addr_len < sizeof(struct sockaddr_ll))\n\t\treturn -EINVAL;\n\tif (sll->sll_family != AF_PACKET)\n\t\treturn -EINVAL;\n\n\treturn packet_do_bind(sk, NULL, sll->sll_ifindex, sll->sll_protocol);\n}\n\nstatic struct proto packet_proto = {\n\t.name\t  = \"PACKET\",\n\t.owner\t  = THIS_MODULE,\n\t.obj_size = sizeof(struct packet_sock),\n};\n\n \n\nstatic int packet_create(struct net *net, struct socket *sock, int protocol,\n\t\t\t int kern)\n{\n\tstruct sock *sk;\n\tstruct packet_sock *po;\n\t__be16 proto = (__force __be16)protocol;  \n\tint err;\n\n\tif (!ns_capable(net->user_ns, CAP_NET_RAW))\n\t\treturn -EPERM;\n\tif (sock->type != SOCK_DGRAM && sock->type != SOCK_RAW &&\n\t    sock->type != SOCK_PACKET)\n\t\treturn -ESOCKTNOSUPPORT;\n\n\tsock->state = SS_UNCONNECTED;\n\n\terr = -ENOBUFS;\n\tsk = sk_alloc(net, PF_PACKET, GFP_KERNEL, &packet_proto, kern);\n\tif (sk == NULL)\n\t\tgoto out;\n\n\tsock->ops = &packet_ops;\n\tif (sock->type == SOCK_PACKET)\n\t\tsock->ops = &packet_ops_spkt;\n\n\tsock_init_data(sock, sk);\n\n\tpo = pkt_sk(sk);\n\tinit_completion(&po->skb_completion);\n\tsk->sk_family = PF_PACKET;\n\tpo->num = proto;\n\n\terr = packet_alloc_pending(po);\n\tif (err)\n\t\tgoto out2;\n\n\tpacket_cached_dev_reset(po);\n\n\tsk->sk_destruct = packet_sock_destruct;\n\n\t \n\n\tspin_lock_init(&po->bind_lock);\n\tmutex_init(&po->pg_vec_lock);\n\tpo->rollover = NULL;\n\tpo->prot_hook.func = packet_rcv;\n\n\tif (sock->type == SOCK_PACKET)\n\t\tpo->prot_hook.func = packet_rcv_spkt;\n\n\tpo->prot_hook.af_packet_priv = sk;\n\tpo->prot_hook.af_packet_net = sock_net(sk);\n\n\tif (proto) {\n\t\tpo->prot_hook.type = proto;\n\t\t__register_prot_hook(sk);\n\t}\n\n\tmutex_lock(&net->packet.sklist_lock);\n\tsk_add_node_tail_rcu(sk, &net->packet.sklist);\n\tmutex_unlock(&net->packet.sklist_lock);\n\n\tsock_prot_inuse_add(net, &packet_proto, 1);\n\n\treturn 0;\nout2:\n\tsk_free(sk);\nout:\n\treturn err;\n}\n\n \n\nstatic int packet_recvmsg(struct socket *sock, struct msghdr *msg, size_t len,\n\t\t\t  int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sk_buff *skb;\n\tint copied, err;\n\tint vnet_hdr_len = READ_ONCE(pkt_sk(sk)->vnet_hdr_sz);\n\tunsigned int origlen = 0;\n\n\terr = -EINVAL;\n\tif (flags & ~(MSG_PEEK|MSG_DONTWAIT|MSG_TRUNC|MSG_CMSG_COMPAT|MSG_ERRQUEUE))\n\t\tgoto out;\n\n#if 0\n\t \n\tif (pkt_sk(sk)->ifindex < 0)\n\t\treturn -ENODEV;\n#endif\n\n\tif (flags & MSG_ERRQUEUE) {\n\t\terr = sock_recv_errqueue(sk, msg, len,\n\t\t\t\t\t SOL_PACKET, PACKET_TX_TIMESTAMP);\n\t\tgoto out;\n\t}\n\n\t \n\n\tskb = skb_recv_datagram(sk, flags, &err);\n\n\t \n\n\tif (skb == NULL)\n\t\tgoto out;\n\n\tpacket_rcv_try_clear_pressure(pkt_sk(sk));\n\n\tif (vnet_hdr_len) {\n\t\terr = packet_rcv_vnet(msg, skb, &len, vnet_hdr_len);\n\t\tif (err)\n\t\t\tgoto out_free;\n\t}\n\n\t \n\tcopied = skb->len;\n\tif (copied > len) {\n\t\tcopied = len;\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t}\n\n\terr = skb_copy_datagram_msg(skb, 0, msg, copied);\n\tif (err)\n\t\tgoto out_free;\n\n\tif (sock->type != SOCK_PACKET) {\n\t\tstruct sockaddr_ll *sll = &PACKET_SKB_CB(skb)->sa.ll;\n\n\t\t \n\t\toriglen = PACKET_SKB_CB(skb)->sa.origlen;\n\t\tsll->sll_family = AF_PACKET;\n\t\tsll->sll_protocol = skb->protocol;\n\t}\n\n\tsock_recv_cmsgs(msg, sk, skb);\n\n\tif (msg->msg_name) {\n\t\tconst size_t max_len = min(sizeof(skb->cb),\n\t\t\t\t\t   sizeof(struct sockaddr_storage));\n\t\tint copy_len;\n\n\t\t \n\t\tif (sock->type == SOCK_PACKET) {\n\t\t\t__sockaddr_check_size(sizeof(struct sockaddr_pkt));\n\t\t\tmsg->msg_namelen = sizeof(struct sockaddr_pkt);\n\t\t\tcopy_len = msg->msg_namelen;\n\t\t} else {\n\t\t\tstruct sockaddr_ll *sll = &PACKET_SKB_CB(skb)->sa.ll;\n\n\t\t\tmsg->msg_namelen = sll->sll_halen +\n\t\t\t\toffsetof(struct sockaddr_ll, sll_addr);\n\t\t\tcopy_len = msg->msg_namelen;\n\t\t\tif (msg->msg_namelen < sizeof(struct sockaddr_ll)) {\n\t\t\t\tmemset(msg->msg_name +\n\t\t\t\t       offsetof(struct sockaddr_ll, sll_addr),\n\t\t\t\t       0, sizeof(sll->sll_addr));\n\t\t\t\tmsg->msg_namelen = sizeof(struct sockaddr_ll);\n\t\t\t}\n\t\t}\n\t\tif (WARN_ON_ONCE(copy_len > max_len)) {\n\t\t\tcopy_len = max_len;\n\t\t\tmsg->msg_namelen = copy_len;\n\t\t}\n\t\tmemcpy(msg->msg_name, &PACKET_SKB_CB(skb)->sa, copy_len);\n\t}\n\n\tif (packet_sock_flag(pkt_sk(sk), PACKET_SOCK_AUXDATA)) {\n\t\tstruct tpacket_auxdata aux;\n\n\t\taux.tp_status = TP_STATUS_USER;\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\t\taux.tp_status |= TP_STATUS_CSUMNOTREADY;\n\t\telse if (skb->pkt_type != PACKET_OUTGOING &&\n\t\t\t skb_csum_unnecessary(skb))\n\t\t\taux.tp_status |= TP_STATUS_CSUM_VALID;\n\t\tif (skb_is_gso(skb) && skb_is_gso_tcp(skb))\n\t\t\taux.tp_status |= TP_STATUS_GSO_TCP;\n\n\t\taux.tp_len = origlen;\n\t\taux.tp_snaplen = skb->len;\n\t\taux.tp_mac = 0;\n\t\taux.tp_net = skb_network_offset(skb);\n\t\tif (skb_vlan_tag_present(skb)) {\n\t\t\taux.tp_vlan_tci = skb_vlan_tag_get(skb);\n\t\t\taux.tp_vlan_tpid = ntohs(skb->vlan_proto);\n\t\t\taux.tp_status |= TP_STATUS_VLAN_VALID | TP_STATUS_VLAN_TPID_VALID;\n\t\t} else {\n\t\t\taux.tp_vlan_tci = 0;\n\t\t\taux.tp_vlan_tpid = 0;\n\t\t}\n\t\tput_cmsg(msg, SOL_PACKET, PACKET_AUXDATA, sizeof(aux), &aux);\n\t}\n\n\t \n\terr = vnet_hdr_len + ((flags&MSG_TRUNC) ? skb->len : copied);\n\nout_free:\n\tskb_free_datagram(sk, skb);\nout:\n\treturn err;\n}\n\nstatic int packet_getname_spkt(struct socket *sock, struct sockaddr *uaddr,\n\t\t\t       int peer)\n{\n\tstruct net_device *dev;\n\tstruct sock *sk\t= sock->sk;\n\n\tif (peer)\n\t\treturn -EOPNOTSUPP;\n\n\tuaddr->sa_family = AF_PACKET;\n\tmemset(uaddr->sa_data, 0, sizeof(uaddr->sa_data_min));\n\trcu_read_lock();\n\tdev = dev_get_by_index_rcu(sock_net(sk), READ_ONCE(pkt_sk(sk)->ifindex));\n\tif (dev)\n\t\tstrscpy(uaddr->sa_data, dev->name, sizeof(uaddr->sa_data_min));\n\trcu_read_unlock();\n\n\treturn sizeof(*uaddr);\n}\n\nstatic int packet_getname(struct socket *sock, struct sockaddr *uaddr,\n\t\t\t  int peer)\n{\n\tstruct net_device *dev;\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_ll *, sll, uaddr);\n\tint ifindex;\n\n\tif (peer)\n\t\treturn -EOPNOTSUPP;\n\n\tifindex = READ_ONCE(po->ifindex);\n\tsll->sll_family = AF_PACKET;\n\tsll->sll_ifindex = ifindex;\n\tsll->sll_protocol = READ_ONCE(po->num);\n\tsll->sll_pkttype = 0;\n\trcu_read_lock();\n\tdev = dev_get_by_index_rcu(sock_net(sk), ifindex);\n\tif (dev) {\n\t\tsll->sll_hatype = dev->type;\n\t\tsll->sll_halen = dev->addr_len;\n\n\t\t \n\t\tmemcpy(((struct sockaddr_storage *)sll)->__data +\n\t\t       offsetof(struct sockaddr_ll, sll_addr) -\n\t\t       offsetofend(struct sockaddr_ll, sll_family),\n\t\t       dev->dev_addr, dev->addr_len);\n\t} else {\n\t\tsll->sll_hatype = 0;\t \n\t\tsll->sll_halen = 0;\n\t}\n\trcu_read_unlock();\n\n\treturn offsetof(struct sockaddr_ll, sll_addr) + sll->sll_halen;\n}\n\nstatic int packet_dev_mc(struct net_device *dev, struct packet_mclist *i,\n\t\t\t int what)\n{\n\tswitch (i->type) {\n\tcase PACKET_MR_MULTICAST:\n\t\tif (i->alen != dev->addr_len)\n\t\t\treturn -EINVAL;\n\t\tif (what > 0)\n\t\t\treturn dev_mc_add(dev, i->addr);\n\t\telse\n\t\t\treturn dev_mc_del(dev, i->addr);\n\t\tbreak;\n\tcase PACKET_MR_PROMISC:\n\t\treturn dev_set_promiscuity(dev, what);\n\tcase PACKET_MR_ALLMULTI:\n\t\treturn dev_set_allmulti(dev, what);\n\tcase PACKET_MR_UNICAST:\n\t\tif (i->alen != dev->addr_len)\n\t\t\treturn -EINVAL;\n\t\tif (what > 0)\n\t\t\treturn dev_uc_add(dev, i->addr);\n\t\telse\n\t\t\treturn dev_uc_del(dev, i->addr);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic void packet_dev_mclist_delete(struct net_device *dev,\n\t\t\t\t     struct packet_mclist **mlp)\n{\n\tstruct packet_mclist *ml;\n\n\twhile ((ml = *mlp) != NULL) {\n\t\tif (ml->ifindex == dev->ifindex) {\n\t\t\tpacket_dev_mc(dev, ml, -1);\n\t\t\t*mlp = ml->next;\n\t\t\tkfree(ml);\n\t\t} else\n\t\t\tmlp = &ml->next;\n\t}\n}\n\nstatic int packet_mc_add(struct sock *sk, struct packet_mreq_max *mreq)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct packet_mclist *ml, *i;\n\tstruct net_device *dev;\n\tint err;\n\n\trtnl_lock();\n\n\terr = -ENODEV;\n\tdev = __dev_get_by_index(sock_net(sk), mreq->mr_ifindex);\n\tif (!dev)\n\t\tgoto done;\n\n\terr = -EINVAL;\n\tif (mreq->mr_alen > dev->addr_len)\n\t\tgoto done;\n\n\terr = -ENOBUFS;\n\ti = kmalloc(sizeof(*i), GFP_KERNEL);\n\tif (i == NULL)\n\t\tgoto done;\n\n\terr = 0;\n\tfor (ml = po->mclist; ml; ml = ml->next) {\n\t\tif (ml->ifindex == mreq->mr_ifindex &&\n\t\t    ml->type == mreq->mr_type &&\n\t\t    ml->alen == mreq->mr_alen &&\n\t\t    memcmp(ml->addr, mreq->mr_address, ml->alen) == 0) {\n\t\t\tml->count++;\n\t\t\t \n\t\t\tkfree(i);\n\t\t\tgoto done;\n\t\t}\n\t}\n\n\ti->type = mreq->mr_type;\n\ti->ifindex = mreq->mr_ifindex;\n\ti->alen = mreq->mr_alen;\n\tmemcpy(i->addr, mreq->mr_address, i->alen);\n\tmemset(i->addr + i->alen, 0, sizeof(i->addr) - i->alen);\n\ti->count = 1;\n\ti->next = po->mclist;\n\tpo->mclist = i;\n\terr = packet_dev_mc(dev, i, 1);\n\tif (err) {\n\t\tpo->mclist = i->next;\n\t\tkfree(i);\n\t}\n\ndone:\n\trtnl_unlock();\n\treturn err;\n}\n\nstatic int packet_mc_drop(struct sock *sk, struct packet_mreq_max *mreq)\n{\n\tstruct packet_mclist *ml, **mlp;\n\n\trtnl_lock();\n\n\tfor (mlp = &pkt_sk(sk)->mclist; (ml = *mlp) != NULL; mlp = &ml->next) {\n\t\tif (ml->ifindex == mreq->mr_ifindex &&\n\t\t    ml->type == mreq->mr_type &&\n\t\t    ml->alen == mreq->mr_alen &&\n\t\t    memcmp(ml->addr, mreq->mr_address, ml->alen) == 0) {\n\t\t\tif (--ml->count == 0) {\n\t\t\t\tstruct net_device *dev;\n\t\t\t\t*mlp = ml->next;\n\t\t\t\tdev = __dev_get_by_index(sock_net(sk), ml->ifindex);\n\t\t\t\tif (dev)\n\t\t\t\t\tpacket_dev_mc(dev, ml, -1);\n\t\t\t\tkfree(ml);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\trtnl_unlock();\n\treturn 0;\n}\n\nstatic void packet_flush_mclist(struct sock *sk)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct packet_mclist *ml;\n\n\tif (!po->mclist)\n\t\treturn;\n\n\trtnl_lock();\n\twhile ((ml = po->mclist) != NULL) {\n\t\tstruct net_device *dev;\n\n\t\tpo->mclist = ml->next;\n\t\tdev = __dev_get_by_index(sock_net(sk), ml->ifindex);\n\t\tif (dev != NULL)\n\t\t\tpacket_dev_mc(dev, ml, -1);\n\t\tkfree(ml);\n\t}\n\trtnl_unlock();\n}\n\nstatic int\npacket_setsockopt(struct socket *sock, int level, int optname, sockptr_t optval,\n\t\t  unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tint ret;\n\n\tif (level != SOL_PACKET)\n\t\treturn -ENOPROTOOPT;\n\n\tswitch (optname) {\n\tcase PACKET_ADD_MEMBERSHIP:\n\tcase PACKET_DROP_MEMBERSHIP:\n\t{\n\t\tstruct packet_mreq_max mreq;\n\t\tint len = optlen;\n\t\tmemset(&mreq, 0, sizeof(mreq));\n\t\tif (len < sizeof(struct packet_mreq))\n\t\t\treturn -EINVAL;\n\t\tif (len > sizeof(mreq))\n\t\t\tlen = sizeof(mreq);\n\t\tif (copy_from_sockptr(&mreq, optval, len))\n\t\t\treturn -EFAULT;\n\t\tif (len < (mreq.mr_alen + offsetof(struct packet_mreq, mr_address)))\n\t\t\treturn -EINVAL;\n\t\tif (optname == PACKET_ADD_MEMBERSHIP)\n\t\t\tret = packet_mc_add(sk, &mreq);\n\t\telse\n\t\t\tret = packet_mc_drop(sk, &mreq);\n\t\treturn ret;\n\t}\n\n\tcase PACKET_RX_RING:\n\tcase PACKET_TX_RING:\n\t{\n\t\tunion tpacket_req_u req_u;\n\t\tint len;\n\n\t\tlock_sock(sk);\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V1:\n\t\tcase TPACKET_V2:\n\t\t\tlen = sizeof(req_u.req);\n\t\t\tbreak;\n\t\tcase TPACKET_V3:\n\t\tdefault:\n\t\t\tlen = sizeof(req_u.req3);\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < len) {\n\t\t\tret = -EINVAL;\n\t\t} else {\n\t\t\tif (copy_from_sockptr(&req_u.req, optval, len))\n\t\t\t\tret = -EFAULT;\n\t\t\telse\n\t\t\t\tret = packet_set_ring(sk, &req_u, 0,\n\t\t\t\t\t\t    optname == PACKET_TX_RING);\n\t\t}\n\t\trelease_sock(sk);\n\t\treturn ret;\n\t}\n\tcase PACKET_COPY_THRESH:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_sockptr(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpkt_sk(sk)->copy_thresh = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_VERSION:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_sockptr(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tswitch (val) {\n\t\tcase TPACKET_V1:\n\t\tcase TPACKET_V2:\n\t\tcase TPACKET_V3:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tlock_sock(sk);\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec) {\n\t\t\tret = -EBUSY;\n\t\t} else {\n\t\t\tpo->tp_version = val;\n\t\t\tret = 0;\n\t\t}\n\t\trelease_sock(sk);\n\t\treturn ret;\n\t}\n\tcase PACKET_RESERVE:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_sockptr(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tif (val > INT_MAX)\n\t\t\treturn -EINVAL;\n\t\tlock_sock(sk);\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec) {\n\t\t\tret = -EBUSY;\n\t\t} else {\n\t\t\tpo->tp_reserve = val;\n\t\t\tret = 0;\n\t\t}\n\t\trelease_sock(sk);\n\t\treturn ret;\n\t}\n\tcase PACKET_LOSS:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_sockptr(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tlock_sock(sk);\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec) {\n\t\t\tret = -EBUSY;\n\t\t} else {\n\t\t\tpacket_sock_flag_set(po, PACKET_SOCK_TP_LOSS, val);\n\t\t\tret = 0;\n\t\t}\n\t\trelease_sock(sk);\n\t\treturn ret;\n\t}\n\tcase PACKET_AUXDATA:\n\t{\n\t\tint val;\n\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_sockptr(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpacket_sock_flag_set(po, PACKET_SOCK_AUXDATA, val);\n\t\treturn 0;\n\t}\n\tcase PACKET_ORIGDEV:\n\t{\n\t\tint val;\n\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_sockptr(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpacket_sock_flag_set(po, PACKET_SOCK_ORIGDEV, val);\n\t\treturn 0;\n\t}\n\tcase PACKET_VNET_HDR:\n\tcase PACKET_VNET_HDR_SZ:\n\t{\n\t\tint val, hdr_len;\n\n\t\tif (sock->type != SOCK_RAW)\n\t\t\treturn -EINVAL;\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_sockptr(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tif (optname == PACKET_VNET_HDR_SZ) {\n\t\t\tif (val && val != sizeof(struct virtio_net_hdr) &&\n\t\t\t    val != sizeof(struct virtio_net_hdr_mrg_rxbuf))\n\t\t\t\treturn -EINVAL;\n\t\t\thdr_len = val;\n\t\t} else {\n\t\t\thdr_len = val ? sizeof(struct virtio_net_hdr) : 0;\n\t\t}\n\t\tlock_sock(sk);\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec) {\n\t\t\tret = -EBUSY;\n\t\t} else {\n\t\t\tWRITE_ONCE(po->vnet_hdr_sz, hdr_len);\n\t\t\tret = 0;\n\t\t}\n\t\trelease_sock(sk);\n\t\treturn ret;\n\t}\n\tcase PACKET_TIMESTAMP:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_sockptr(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tWRITE_ONCE(po->tp_tstamp, val);\n\t\treturn 0;\n\t}\n\tcase PACKET_FANOUT:\n\t{\n\t\tstruct fanout_args args = { 0 };\n\n\t\tif (optlen != sizeof(int) && optlen != sizeof(args))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_sockptr(&args, optval, optlen))\n\t\t\treturn -EFAULT;\n\n\t\treturn fanout_add(sk, &args);\n\t}\n\tcase PACKET_FANOUT_DATA:\n\t{\n\t\t \n\t\tif (!READ_ONCE(po->fanout))\n\t\t\treturn -EINVAL;\n\n\t\treturn fanout_set_data(po, optval, optlen);\n\t}\n\tcase PACKET_IGNORE_OUTGOING:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_sockptr(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tif (val < 0 || val > 1)\n\t\t\treturn -EINVAL;\n\n\t\tpo->prot_hook.ignore_outgoing = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_TX_HAS_OFF:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_sockptr(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tlock_sock(sk);\n\t\tif (!po->rx_ring.pg_vec && !po->tx_ring.pg_vec)\n\t\t\tpacket_sock_flag_set(po, PACKET_SOCK_TX_HAS_OFF, val);\n\n\t\trelease_sock(sk);\n\t\treturn 0;\n\t}\n\tcase PACKET_QDISC_BYPASS:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_sockptr(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpacket_sock_flag_set(po, PACKET_SOCK_QDISC_BYPASS, val);\n\t\treturn 0;\n\t}\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n}\n\nstatic int packet_getsockopt(struct socket *sock, int level, int optname,\n\t\t\t     char __user *optval, int __user *optlen)\n{\n\tint len;\n\tint val, lv = sizeof(val);\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tvoid *data = &val;\n\tunion tpacket_stats_u st;\n\tstruct tpacket_rollover_stats rstats;\n\tint drops;\n\n\tif (level != SOL_PACKET)\n\t\treturn -ENOPROTOOPT;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\n\tif (len < 0)\n\t\treturn -EINVAL;\n\n\tswitch (optname) {\n\tcase PACKET_STATISTICS:\n\t\tspin_lock_bh(&sk->sk_receive_queue.lock);\n\t\tmemcpy(&st, &po->stats, sizeof(st));\n\t\tmemset(&po->stats, 0, sizeof(po->stats));\n\t\tspin_unlock_bh(&sk->sk_receive_queue.lock);\n\t\tdrops = atomic_xchg(&po->tp_drops, 0);\n\n\t\tif (po->tp_version == TPACKET_V3) {\n\t\t\tlv = sizeof(struct tpacket_stats_v3);\n\t\t\tst.stats3.tp_drops = drops;\n\t\t\tst.stats3.tp_packets += drops;\n\t\t\tdata = &st.stats3;\n\t\t} else {\n\t\t\tlv = sizeof(struct tpacket_stats);\n\t\t\tst.stats1.tp_drops = drops;\n\t\t\tst.stats1.tp_packets += drops;\n\t\t\tdata = &st.stats1;\n\t\t}\n\n\t\tbreak;\n\tcase PACKET_AUXDATA:\n\t\tval = packet_sock_flag(po, PACKET_SOCK_AUXDATA);\n\t\tbreak;\n\tcase PACKET_ORIGDEV:\n\t\tval = packet_sock_flag(po, PACKET_SOCK_ORIGDEV);\n\t\tbreak;\n\tcase PACKET_VNET_HDR:\n\t\tval = !!READ_ONCE(po->vnet_hdr_sz);\n\t\tbreak;\n\tcase PACKET_VNET_HDR_SZ:\n\t\tval = READ_ONCE(po->vnet_hdr_sz);\n\t\tbreak;\n\tcase PACKET_VERSION:\n\t\tval = po->tp_version;\n\t\tbreak;\n\tcase PACKET_HDRLEN:\n\t\tif (len > sizeof(int))\n\t\t\tlen = sizeof(int);\n\t\tif (len < sizeof(int))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, len))\n\t\t\treturn -EFAULT;\n\t\tswitch (val) {\n\t\tcase TPACKET_V1:\n\t\t\tval = sizeof(struct tpacket_hdr);\n\t\t\tbreak;\n\t\tcase TPACKET_V2:\n\t\t\tval = sizeof(struct tpacket2_hdr);\n\t\t\tbreak;\n\t\tcase TPACKET_V3:\n\t\t\tval = sizeof(struct tpacket3_hdr);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\tcase PACKET_RESERVE:\n\t\tval = po->tp_reserve;\n\t\tbreak;\n\tcase PACKET_LOSS:\n\t\tval = packet_sock_flag(po, PACKET_SOCK_TP_LOSS);\n\t\tbreak;\n\tcase PACKET_TIMESTAMP:\n\t\tval = READ_ONCE(po->tp_tstamp);\n\t\tbreak;\n\tcase PACKET_FANOUT:\n\t\tval = (po->fanout ?\n\t\t       ((u32)po->fanout->id |\n\t\t\t((u32)po->fanout->type << 16) |\n\t\t\t((u32)po->fanout->flags << 24)) :\n\t\t       0);\n\t\tbreak;\n\tcase PACKET_IGNORE_OUTGOING:\n\t\tval = po->prot_hook.ignore_outgoing;\n\t\tbreak;\n\tcase PACKET_ROLLOVER_STATS:\n\t\tif (!po->rollover)\n\t\t\treturn -EINVAL;\n\t\trstats.tp_all = atomic_long_read(&po->rollover->num);\n\t\trstats.tp_huge = atomic_long_read(&po->rollover->num_huge);\n\t\trstats.tp_failed = atomic_long_read(&po->rollover->num_failed);\n\t\tdata = &rstats;\n\t\tlv = sizeof(rstats);\n\t\tbreak;\n\tcase PACKET_TX_HAS_OFF:\n\t\tval = packet_sock_flag(po, PACKET_SOCK_TX_HAS_OFF);\n\t\tbreak;\n\tcase PACKET_QDISC_BYPASS:\n\t\tval = packet_sock_flag(po, PACKET_SOCK_QDISC_BYPASS);\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\tif (len > lv)\n\t\tlen = lv;\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (copy_to_user(optval, data, len))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic int packet_notifier(struct notifier_block *this,\n\t\t\t   unsigned long msg, void *ptr)\n{\n\tstruct sock *sk;\n\tstruct net_device *dev = netdev_notifier_info_to_dev(ptr);\n\tstruct net *net = dev_net(dev);\n\n\trcu_read_lock();\n\tsk_for_each_rcu(sk, &net->packet.sklist) {\n\t\tstruct packet_sock *po = pkt_sk(sk);\n\n\t\tswitch (msg) {\n\t\tcase NETDEV_UNREGISTER:\n\t\t\tif (po->mclist)\n\t\t\t\tpacket_dev_mclist_delete(dev, &po->mclist);\n\t\t\tfallthrough;\n\n\t\tcase NETDEV_DOWN:\n\t\t\tif (dev->ifindex == po->ifindex) {\n\t\t\t\tspin_lock(&po->bind_lock);\n\t\t\t\tif (packet_sock_flag(po, PACKET_SOCK_RUNNING)) {\n\t\t\t\t\t__unregister_prot_hook(sk, false);\n\t\t\t\t\tsk->sk_err = ENETDOWN;\n\t\t\t\t\tif (!sock_flag(sk, SOCK_DEAD))\n\t\t\t\t\t\tsk_error_report(sk);\n\t\t\t\t}\n\t\t\t\tif (msg == NETDEV_UNREGISTER) {\n\t\t\t\t\tpacket_cached_dev_reset(po);\n\t\t\t\t\tWRITE_ONCE(po->ifindex, -1);\n\t\t\t\t\tnetdev_put(po->prot_hook.dev,\n\t\t\t\t\t\t   &po->prot_hook.dev_tracker);\n\t\t\t\t\tpo->prot_hook.dev = NULL;\n\t\t\t\t}\n\t\t\t\tspin_unlock(&po->bind_lock);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NETDEV_UP:\n\t\t\tif (dev->ifindex == po->ifindex) {\n\t\t\t\tspin_lock(&po->bind_lock);\n\t\t\t\tif (po->num)\n\t\t\t\t\tregister_prot_hook(sk);\n\t\t\t\tspin_unlock(&po->bind_lock);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn NOTIFY_DONE;\n}\n\n\nstatic int packet_ioctl(struct socket *sock, unsigned int cmd,\n\t\t\tunsigned long arg)\n{\n\tstruct sock *sk = sock->sk;\n\n\tswitch (cmd) {\n\tcase SIOCOUTQ:\n\t{\n\t\tint amount = sk_wmem_alloc_get(sk);\n\n\t\treturn put_user(amount, (int __user *)arg);\n\t}\n\tcase SIOCINQ:\n\t{\n\t\tstruct sk_buff *skb;\n\t\tint amount = 0;\n\n\t\tspin_lock_bh(&sk->sk_receive_queue.lock);\n\t\tskb = skb_peek(&sk->sk_receive_queue);\n\t\tif (skb)\n\t\t\tamount = skb->len;\n\t\tspin_unlock_bh(&sk->sk_receive_queue.lock);\n\t\treturn put_user(amount, (int __user *)arg);\n\t}\n#ifdef CONFIG_INET\n\tcase SIOCADDRT:\n\tcase SIOCDELRT:\n\tcase SIOCDARP:\n\tcase SIOCGARP:\n\tcase SIOCSARP:\n\tcase SIOCGIFADDR:\n\tcase SIOCSIFADDR:\n\tcase SIOCGIFBRDADDR:\n\tcase SIOCSIFBRDADDR:\n\tcase SIOCGIFNETMASK:\n\tcase SIOCSIFNETMASK:\n\tcase SIOCGIFDSTADDR:\n\tcase SIOCSIFDSTADDR:\n\tcase SIOCSIFFLAGS:\n\t\treturn inet_dgram_ops.ioctl(sock, cmd, arg);\n#endif\n\n\tdefault:\n\t\treturn -ENOIOCTLCMD;\n\t}\n\treturn 0;\n}\n\nstatic __poll_t packet_poll(struct file *file, struct socket *sock,\n\t\t\t\tpoll_table *wait)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\t__poll_t mask = datagram_poll(file, sock, wait);\n\n\tspin_lock_bh(&sk->sk_receive_queue.lock);\n\tif (po->rx_ring.pg_vec) {\n\t\tif (!packet_previous_rx_frame(po, &po->rx_ring,\n\t\t\tTP_STATUS_KERNEL))\n\t\t\tmask |= EPOLLIN | EPOLLRDNORM;\n\t}\n\tpacket_rcv_try_clear_pressure(po);\n\tspin_unlock_bh(&sk->sk_receive_queue.lock);\n\tspin_lock_bh(&sk->sk_write_queue.lock);\n\tif (po->tx_ring.pg_vec) {\n\t\tif (packet_current_frame(po, &po->tx_ring, TP_STATUS_AVAILABLE))\n\t\t\tmask |= EPOLLOUT | EPOLLWRNORM;\n\t}\n\tspin_unlock_bh(&sk->sk_write_queue.lock);\n\treturn mask;\n}\n\n\n \n\nstatic void packet_mm_open(struct vm_area_struct *vma)\n{\n\tstruct file *file = vma->vm_file;\n\tstruct socket *sock = file->private_data;\n\tstruct sock *sk = sock->sk;\n\n\tif (sk)\n\t\tatomic_long_inc(&pkt_sk(sk)->mapped);\n}\n\nstatic void packet_mm_close(struct vm_area_struct *vma)\n{\n\tstruct file *file = vma->vm_file;\n\tstruct socket *sock = file->private_data;\n\tstruct sock *sk = sock->sk;\n\n\tif (sk)\n\t\tatomic_long_dec(&pkt_sk(sk)->mapped);\n}\n\nstatic const struct vm_operations_struct packet_mmap_ops = {\n\t.open\t=\tpacket_mm_open,\n\t.close\t=\tpacket_mm_close,\n};\n\nstatic void free_pg_vec(struct pgv *pg_vec, unsigned int order,\n\t\t\tunsigned int len)\n{\n\tint i;\n\n\tfor (i = 0; i < len; i++) {\n\t\tif (likely(pg_vec[i].buffer)) {\n\t\t\tif (is_vmalloc_addr(pg_vec[i].buffer))\n\t\t\t\tvfree(pg_vec[i].buffer);\n\t\t\telse\n\t\t\t\tfree_pages((unsigned long)pg_vec[i].buffer,\n\t\t\t\t\t   order);\n\t\t\tpg_vec[i].buffer = NULL;\n\t\t}\n\t}\n\tkfree(pg_vec);\n}\n\nstatic char *alloc_one_pg_vec_page(unsigned long order)\n{\n\tchar *buffer;\n\tgfp_t gfp_flags = GFP_KERNEL | __GFP_COMP |\n\t\t\t  __GFP_ZERO | __GFP_NOWARN | __GFP_NORETRY;\n\n\tbuffer = (char *) __get_free_pages(gfp_flags, order);\n\tif (buffer)\n\t\treturn buffer;\n\n\t \n\tbuffer = vzalloc(array_size((1 << order), PAGE_SIZE));\n\tif (buffer)\n\t\treturn buffer;\n\n\t \n\tgfp_flags &= ~__GFP_NORETRY;\n\tbuffer = (char *) __get_free_pages(gfp_flags, order);\n\tif (buffer)\n\t\treturn buffer;\n\n\t \n\treturn NULL;\n}\n\nstatic struct pgv *alloc_pg_vec(struct tpacket_req *req, int order)\n{\n\tunsigned int block_nr = req->tp_block_nr;\n\tstruct pgv *pg_vec;\n\tint i;\n\n\tpg_vec = kcalloc(block_nr, sizeof(struct pgv), GFP_KERNEL | __GFP_NOWARN);\n\tif (unlikely(!pg_vec))\n\t\tgoto out;\n\n\tfor (i = 0; i < block_nr; i++) {\n\t\tpg_vec[i].buffer = alloc_one_pg_vec_page(order);\n\t\tif (unlikely(!pg_vec[i].buffer))\n\t\t\tgoto out_free_pgvec;\n\t}\n\nout:\n\treturn pg_vec;\n\nout_free_pgvec:\n\tfree_pg_vec(pg_vec, order, block_nr);\n\tpg_vec = NULL;\n\tgoto out;\n}\n\nstatic int packet_set_ring(struct sock *sk, union tpacket_req_u *req_u,\n\t\tint closing, int tx_ring)\n{\n\tstruct pgv *pg_vec = NULL;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tunsigned long *rx_owner_map = NULL;\n\tint was_running, order = 0;\n\tstruct packet_ring_buffer *rb;\n\tstruct sk_buff_head *rb_queue;\n\t__be16 num;\n\tint err;\n\t \n\tstruct tpacket_req *req = &req_u->req;\n\n\trb = tx_ring ? &po->tx_ring : &po->rx_ring;\n\trb_queue = tx_ring ? &sk->sk_write_queue : &sk->sk_receive_queue;\n\n\terr = -EBUSY;\n\tif (!closing) {\n\t\tif (atomic_long_read(&po->mapped))\n\t\t\tgoto out;\n\t\tif (packet_read_pending(rb))\n\t\t\tgoto out;\n\t}\n\n\tif (req->tp_block_nr) {\n\t\tunsigned int min_frame_size;\n\n\t\t \n\t\terr = -EBUSY;\n\t\tif (unlikely(rb->pg_vec))\n\t\t\tgoto out;\n\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V1:\n\t\t\tpo->tp_hdrlen = TPACKET_HDRLEN;\n\t\t\tbreak;\n\t\tcase TPACKET_V2:\n\t\t\tpo->tp_hdrlen = TPACKET2_HDRLEN;\n\t\t\tbreak;\n\t\tcase TPACKET_V3:\n\t\t\tpo->tp_hdrlen = TPACKET3_HDRLEN;\n\t\t\tbreak;\n\t\t}\n\n\t\terr = -EINVAL;\n\t\tif (unlikely((int)req->tp_block_size <= 0))\n\t\t\tgoto out;\n\t\tif (unlikely(!PAGE_ALIGNED(req->tp_block_size)))\n\t\t\tgoto out;\n\t\tmin_frame_size = po->tp_hdrlen + po->tp_reserve;\n\t\tif (po->tp_version >= TPACKET_V3 &&\n\t\t    req->tp_block_size <\n\t\t    BLK_PLUS_PRIV((u64)req_u->req3.tp_sizeof_priv) + min_frame_size)\n\t\t\tgoto out;\n\t\tif (unlikely(req->tp_frame_size < min_frame_size))\n\t\t\tgoto out;\n\t\tif (unlikely(req->tp_frame_size & (TPACKET_ALIGNMENT - 1)))\n\t\t\tgoto out;\n\n\t\trb->frames_per_block = req->tp_block_size / req->tp_frame_size;\n\t\tif (unlikely(rb->frames_per_block == 0))\n\t\t\tgoto out;\n\t\tif (unlikely(rb->frames_per_block > UINT_MAX / req->tp_block_nr))\n\t\t\tgoto out;\n\t\tif (unlikely((rb->frames_per_block * req->tp_block_nr) !=\n\t\t\t\t\treq->tp_frame_nr))\n\t\t\tgoto out;\n\n\t\terr = -ENOMEM;\n\t\torder = get_order(req->tp_block_size);\n\t\tpg_vec = alloc_pg_vec(req, order);\n\t\tif (unlikely(!pg_vec))\n\t\t\tgoto out;\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V3:\n\t\t\t \n\t\t\tif (!tx_ring) {\n\t\t\t\tinit_prb_bdqc(po, rb, pg_vec, req_u);\n\t\t\t} else {\n\t\t\t\tstruct tpacket_req3 *req3 = &req_u->req3;\n\n\t\t\t\tif (req3->tp_retire_blk_tov ||\n\t\t\t\t    req3->tp_sizeof_priv ||\n\t\t\t\t    req3->tp_feature_req_word) {\n\t\t\t\t\terr = -EINVAL;\n\t\t\t\t\tgoto out_free_pg_vec;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tif (!tx_ring) {\n\t\t\t\trx_owner_map = bitmap_alloc(req->tp_frame_nr,\n\t\t\t\t\tGFP_KERNEL | __GFP_NOWARN | __GFP_ZERO);\n\t\t\t\tif (!rx_owner_map)\n\t\t\t\t\tgoto out_free_pg_vec;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\t \n\telse {\n\t\terr = -EINVAL;\n\t\tif (unlikely(req->tp_frame_nr))\n\t\t\tgoto out;\n\t}\n\n\n\t \n\tspin_lock(&po->bind_lock);\n\twas_running = packet_sock_flag(po, PACKET_SOCK_RUNNING);\n\tnum = po->num;\n\tif (was_running) {\n\t\tWRITE_ONCE(po->num, 0);\n\t\t__unregister_prot_hook(sk, false);\n\t}\n\tspin_unlock(&po->bind_lock);\n\n\tsynchronize_net();\n\n\terr = -EBUSY;\n\tmutex_lock(&po->pg_vec_lock);\n\tif (closing || atomic_long_read(&po->mapped) == 0) {\n\t\terr = 0;\n\t\tspin_lock_bh(&rb_queue->lock);\n\t\tswap(rb->pg_vec, pg_vec);\n\t\tif (po->tp_version <= TPACKET_V2)\n\t\t\tswap(rb->rx_owner_map, rx_owner_map);\n\t\trb->frame_max = (req->tp_frame_nr - 1);\n\t\trb->head = 0;\n\t\trb->frame_size = req->tp_frame_size;\n\t\tspin_unlock_bh(&rb_queue->lock);\n\n\t\tswap(rb->pg_vec_order, order);\n\t\tswap(rb->pg_vec_len, req->tp_block_nr);\n\n\t\trb->pg_vec_pages = req->tp_block_size/PAGE_SIZE;\n\t\tpo->prot_hook.func = (po->rx_ring.pg_vec) ?\n\t\t\t\t\t\ttpacket_rcv : packet_rcv;\n\t\tskb_queue_purge(rb_queue);\n\t\tif (atomic_long_read(&po->mapped))\n\t\t\tpr_err(\"packet_mmap: vma is busy: %ld\\n\",\n\t\t\t       atomic_long_read(&po->mapped));\n\t}\n\tmutex_unlock(&po->pg_vec_lock);\n\n\tspin_lock(&po->bind_lock);\n\tif (was_running) {\n\t\tWRITE_ONCE(po->num, num);\n\t\tregister_prot_hook(sk);\n\t}\n\tspin_unlock(&po->bind_lock);\n\tif (pg_vec && (po->tp_version > TPACKET_V2)) {\n\t\t \n\t\tif (!tx_ring)\n\t\t\tprb_shutdown_retire_blk_timer(po, rb_queue);\n\t}\n\nout_free_pg_vec:\n\tif (pg_vec) {\n\t\tbitmap_free(rx_owner_map);\n\t\tfree_pg_vec(pg_vec, order, req->tp_block_nr);\n\t}\nout:\n\treturn err;\n}\n\nstatic int packet_mmap(struct file *file, struct socket *sock,\n\t\tstruct vm_area_struct *vma)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tunsigned long size, expected_size;\n\tstruct packet_ring_buffer *rb;\n\tunsigned long start;\n\tint err = -EINVAL;\n\tint i;\n\n\tif (vma->vm_pgoff)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&po->pg_vec_lock);\n\n\texpected_size = 0;\n\tfor (rb = &po->rx_ring; rb <= &po->tx_ring; rb++) {\n\t\tif (rb->pg_vec) {\n\t\t\texpected_size += rb->pg_vec_len\n\t\t\t\t\t\t* rb->pg_vec_pages\n\t\t\t\t\t\t* PAGE_SIZE;\n\t\t}\n\t}\n\n\tif (expected_size == 0)\n\t\tgoto out;\n\n\tsize = vma->vm_end - vma->vm_start;\n\tif (size != expected_size)\n\t\tgoto out;\n\n\tstart = vma->vm_start;\n\tfor (rb = &po->rx_ring; rb <= &po->tx_ring; rb++) {\n\t\tif (rb->pg_vec == NULL)\n\t\t\tcontinue;\n\n\t\tfor (i = 0; i < rb->pg_vec_len; i++) {\n\t\t\tstruct page *page;\n\t\t\tvoid *kaddr = rb->pg_vec[i].buffer;\n\t\t\tint pg_num;\n\n\t\t\tfor (pg_num = 0; pg_num < rb->pg_vec_pages; pg_num++) {\n\t\t\t\tpage = pgv_to_page(kaddr);\n\t\t\t\terr = vm_insert_page(vma, start, page);\n\t\t\t\tif (unlikely(err))\n\t\t\t\t\tgoto out;\n\t\t\t\tstart += PAGE_SIZE;\n\t\t\t\tkaddr += PAGE_SIZE;\n\t\t\t}\n\t\t}\n\t}\n\n\tatomic_long_inc(&po->mapped);\n\tvma->vm_ops = &packet_mmap_ops;\n\terr = 0;\n\nout:\n\tmutex_unlock(&po->pg_vec_lock);\n\treturn err;\n}\n\nstatic const struct proto_ops packet_ops_spkt = {\n\t.family =\tPF_PACKET,\n\t.owner =\tTHIS_MODULE,\n\t.release =\tpacket_release,\n\t.bind =\t\tpacket_bind_spkt,\n\t.connect =\tsock_no_connect,\n\t.socketpair =\tsock_no_socketpair,\n\t.accept =\tsock_no_accept,\n\t.getname =\tpacket_getname_spkt,\n\t.poll =\t\tdatagram_poll,\n\t.ioctl =\tpacket_ioctl,\n\t.gettstamp =\tsock_gettstamp,\n\t.listen =\tsock_no_listen,\n\t.shutdown =\tsock_no_shutdown,\n\t.sendmsg =\tpacket_sendmsg_spkt,\n\t.recvmsg =\tpacket_recvmsg,\n\t.mmap =\t\tsock_no_mmap,\n};\n\nstatic const struct proto_ops packet_ops = {\n\t.family =\tPF_PACKET,\n\t.owner =\tTHIS_MODULE,\n\t.release =\tpacket_release,\n\t.bind =\t\tpacket_bind,\n\t.connect =\tsock_no_connect,\n\t.socketpair =\tsock_no_socketpair,\n\t.accept =\tsock_no_accept,\n\t.getname =\tpacket_getname,\n\t.poll =\t\tpacket_poll,\n\t.ioctl =\tpacket_ioctl,\n\t.gettstamp =\tsock_gettstamp,\n\t.listen =\tsock_no_listen,\n\t.shutdown =\tsock_no_shutdown,\n\t.setsockopt =\tpacket_setsockopt,\n\t.getsockopt =\tpacket_getsockopt,\n\t.sendmsg =\tpacket_sendmsg,\n\t.recvmsg =\tpacket_recvmsg,\n\t.mmap =\t\tpacket_mmap,\n};\n\nstatic const struct net_proto_family packet_family_ops = {\n\t.family =\tPF_PACKET,\n\t.create =\tpacket_create,\n\t.owner\t=\tTHIS_MODULE,\n};\n\nstatic struct notifier_block packet_netdev_notifier = {\n\t.notifier_call =\tpacket_notifier,\n};\n\n#ifdef CONFIG_PROC_FS\n\nstatic void *packet_seq_start(struct seq_file *seq, loff_t *pos)\n\t__acquires(RCU)\n{\n\tstruct net *net = seq_file_net(seq);\n\n\trcu_read_lock();\n\treturn seq_hlist_start_head_rcu(&net->packet.sklist, *pos);\n}\n\nstatic void *packet_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\tstruct net *net = seq_file_net(seq);\n\treturn seq_hlist_next_rcu(v, &net->packet.sklist, pos);\n}\n\nstatic void packet_seq_stop(struct seq_file *seq, void *v)\n\t__releases(RCU)\n{\n\trcu_read_unlock();\n}\n\nstatic int packet_seq_show(struct seq_file *seq, void *v)\n{\n\tif (v == SEQ_START_TOKEN)\n\t\tseq_printf(seq,\n\t\t\t   \"%*sRefCnt Type Proto  Iface R Rmem   User   Inode\\n\",\n\t\t\t   IS_ENABLED(CONFIG_64BIT) ? -17 : -9, \"sk\");\n\telse {\n\t\tstruct sock *s = sk_entry(v);\n\t\tconst struct packet_sock *po = pkt_sk(s);\n\n\t\tseq_printf(seq,\n\t\t\t   \"%pK %-6d %-4d %04x   %-5d %1d %-6u %-6u %-6lu\\n\",\n\t\t\t   s,\n\t\t\t   refcount_read(&s->sk_refcnt),\n\t\t\t   s->sk_type,\n\t\t\t   ntohs(READ_ONCE(po->num)),\n\t\t\t   READ_ONCE(po->ifindex),\n\t\t\t   packet_sock_flag(po, PACKET_SOCK_RUNNING),\n\t\t\t   atomic_read(&s->sk_rmem_alloc),\n\t\t\t   from_kuid_munged(seq_user_ns(seq), sock_i_uid(s)),\n\t\t\t   sock_i_ino(s));\n\t}\n\n\treturn 0;\n}\n\nstatic const struct seq_operations packet_seq_ops = {\n\t.start\t= packet_seq_start,\n\t.next\t= packet_seq_next,\n\t.stop\t= packet_seq_stop,\n\t.show\t= packet_seq_show,\n};\n#endif\n\nstatic int __net_init packet_net_init(struct net *net)\n{\n\tmutex_init(&net->packet.sklist_lock);\n\tINIT_HLIST_HEAD(&net->packet.sklist);\n\n#ifdef CONFIG_PROC_FS\n\tif (!proc_create_net(\"packet\", 0, net->proc_net, &packet_seq_ops,\n\t\t\tsizeof(struct seq_net_private)))\n\t\treturn -ENOMEM;\n#endif  \n\n\treturn 0;\n}\n\nstatic void __net_exit packet_net_exit(struct net *net)\n{\n\tremove_proc_entry(\"packet\", net->proc_net);\n\tWARN_ON_ONCE(!hlist_empty(&net->packet.sklist));\n}\n\nstatic struct pernet_operations packet_net_ops = {\n\t.init = packet_net_init,\n\t.exit = packet_net_exit,\n};\n\n\nstatic void __exit packet_exit(void)\n{\n\tsock_unregister(PF_PACKET);\n\tproto_unregister(&packet_proto);\n\tunregister_netdevice_notifier(&packet_netdev_notifier);\n\tunregister_pernet_subsys(&packet_net_ops);\n}\n\nstatic int __init packet_init(void)\n{\n\tint rc;\n\n\trc = register_pernet_subsys(&packet_net_ops);\n\tif (rc)\n\t\tgoto out;\n\trc = register_netdevice_notifier(&packet_netdev_notifier);\n\tif (rc)\n\t\tgoto out_pernet;\n\trc = proto_register(&packet_proto, 0);\n\tif (rc)\n\t\tgoto out_notifier;\n\trc = sock_register(&packet_family_ops);\n\tif (rc)\n\t\tgoto out_proto;\n\n\treturn 0;\n\nout_proto:\n\tproto_unregister(&packet_proto);\nout_notifier:\n\tunregister_netdevice_notifier(&packet_netdev_notifier);\nout_pernet:\n\tunregister_pernet_subsys(&packet_net_ops);\nout:\n\treturn rc;\n}\n\nmodule_init(packet_init);\nmodule_exit(packet_exit);\nMODULE_LICENSE(\"GPL\");\nMODULE_ALIAS_NETPROTO(PF_PACKET);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}