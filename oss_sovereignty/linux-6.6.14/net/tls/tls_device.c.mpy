{
  "module_name": "tls_device.c",
  "hash_id": "967e09c551e3e5b0f563f7aff15eab40bb39a639f1b09aeccfcf024593717393",
  "original_prompt": "Ingested from linux-6.6.14/net/tls/tls_device.c",
  "human_readable_source": " \n\n#include <crypto/aead.h>\n#include <linux/highmem.h>\n#include <linux/module.h>\n#include <linux/netdevice.h>\n#include <net/dst.h>\n#include <net/inet_connection_sock.h>\n#include <net/tcp.h>\n#include <net/tls.h>\n\n#include \"tls.h\"\n#include \"trace.h\"\n\n \nstatic DECLARE_RWSEM(device_offload_lock);\n\nstatic struct workqueue_struct *destruct_wq __read_mostly;\n\nstatic LIST_HEAD(tls_device_list);\nstatic LIST_HEAD(tls_device_down_list);\nstatic DEFINE_SPINLOCK(tls_device_lock);\n\nstatic struct page *dummy_page;\n\nstatic void tls_device_free_ctx(struct tls_context *ctx)\n{\n\tif (ctx->tx_conf == TLS_HW) {\n\t\tkfree(tls_offload_ctx_tx(ctx));\n\t\tkfree(ctx->tx.rec_seq);\n\t\tkfree(ctx->tx.iv);\n\t}\n\n\tif (ctx->rx_conf == TLS_HW)\n\t\tkfree(tls_offload_ctx_rx(ctx));\n\n\ttls_ctx_free(NULL, ctx);\n}\n\nstatic void tls_device_tx_del_task(struct work_struct *work)\n{\n\tstruct tls_offload_context_tx *offload_ctx =\n\t\tcontainer_of(work, struct tls_offload_context_tx, destruct_work);\n\tstruct tls_context *ctx = offload_ctx->ctx;\n\tstruct net_device *netdev;\n\n\t \n\tnetdev = rcu_dereference_protected(ctx->netdev,\n\t\t\t\t\t   !refcount_read(&ctx->refcount));\n\n\tnetdev->tlsdev_ops->tls_dev_del(netdev, ctx, TLS_OFFLOAD_CTX_DIR_TX);\n\tdev_put(netdev);\n\tctx->netdev = NULL;\n\ttls_device_free_ctx(ctx);\n}\n\nstatic void tls_device_queue_ctx_destruction(struct tls_context *ctx)\n{\n\tstruct net_device *netdev;\n\tunsigned long flags;\n\tbool async_cleanup;\n\n\tspin_lock_irqsave(&tls_device_lock, flags);\n\tif (unlikely(!refcount_dec_and_test(&ctx->refcount))) {\n\t\tspin_unlock_irqrestore(&tls_device_lock, flags);\n\t\treturn;\n\t}\n\n\tlist_del(&ctx->list);  \n\n\t \n\tnetdev = rcu_dereference_protected(ctx->netdev,\n\t\t\t\t\t   !refcount_read(&ctx->refcount));\n\n\tasync_cleanup = netdev && ctx->tx_conf == TLS_HW;\n\tif (async_cleanup) {\n\t\tstruct tls_offload_context_tx *offload_ctx = tls_offload_ctx_tx(ctx);\n\n\t\t \n\t\tqueue_work(destruct_wq, &offload_ctx->destruct_work);\n\t}\n\tspin_unlock_irqrestore(&tls_device_lock, flags);\n\n\tif (!async_cleanup)\n\t\ttls_device_free_ctx(ctx);\n}\n\n \nstatic struct net_device *get_netdev_for_sock(struct sock *sk)\n{\n\tstruct dst_entry *dst = sk_dst_get(sk);\n\tstruct net_device *netdev = NULL;\n\n\tif (likely(dst)) {\n\t\tnetdev = netdev_sk_get_lowest_dev(dst->dev, sk);\n\t\tdev_hold(netdev);\n\t}\n\n\tdst_release(dst);\n\n\treturn netdev;\n}\n\nstatic void destroy_record(struct tls_record_info *record)\n{\n\tint i;\n\n\tfor (i = 0; i < record->num_frags; i++)\n\t\t__skb_frag_unref(&record->frags[i], false);\n\tkfree(record);\n}\n\nstatic void delete_all_records(struct tls_offload_context_tx *offload_ctx)\n{\n\tstruct tls_record_info *info, *temp;\n\n\tlist_for_each_entry_safe(info, temp, &offload_ctx->records_list, list) {\n\t\tlist_del(&info->list);\n\t\tdestroy_record(info);\n\t}\n\n\toffload_ctx->retransmit_hint = NULL;\n}\n\nstatic void tls_icsk_clean_acked(struct sock *sk, u32 acked_seq)\n{\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\tstruct tls_record_info *info, *temp;\n\tstruct tls_offload_context_tx *ctx;\n\tu64 deleted_records = 0;\n\tunsigned long flags;\n\n\tif (!tls_ctx)\n\t\treturn;\n\n\tctx = tls_offload_ctx_tx(tls_ctx);\n\n\tspin_lock_irqsave(&ctx->lock, flags);\n\tinfo = ctx->retransmit_hint;\n\tif (info && !before(acked_seq, info->end_seq))\n\t\tctx->retransmit_hint = NULL;\n\n\tlist_for_each_entry_safe(info, temp, &ctx->records_list, list) {\n\t\tif (before(acked_seq, info->end_seq))\n\t\t\tbreak;\n\t\tlist_del(&info->list);\n\n\t\tdestroy_record(info);\n\t\tdeleted_records++;\n\t}\n\n\tctx->unacked_record_sn += deleted_records;\n\tspin_unlock_irqrestore(&ctx->lock, flags);\n}\n\n \nvoid tls_device_sk_destruct(struct sock *sk)\n{\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\tstruct tls_offload_context_tx *ctx = tls_offload_ctx_tx(tls_ctx);\n\n\ttls_ctx->sk_destruct(sk);\n\n\tif (tls_ctx->tx_conf == TLS_HW) {\n\t\tif (ctx->open_record)\n\t\t\tdestroy_record(ctx->open_record);\n\t\tdelete_all_records(ctx);\n\t\tcrypto_free_aead(ctx->aead_send);\n\t\tclean_acked_data_disable(inet_csk(sk));\n\t}\n\n\ttls_device_queue_ctx_destruction(tls_ctx);\n}\nEXPORT_SYMBOL_GPL(tls_device_sk_destruct);\n\nvoid tls_device_free_resources_tx(struct sock *sk)\n{\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\n\ttls_free_partial_record(sk, tls_ctx);\n}\n\nvoid tls_offload_tx_resync_request(struct sock *sk, u32 got_seq, u32 exp_seq)\n{\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\n\ttrace_tls_device_tx_resync_req(sk, got_seq, exp_seq);\n\tWARN_ON(test_and_set_bit(TLS_TX_SYNC_SCHED, &tls_ctx->flags));\n}\nEXPORT_SYMBOL_GPL(tls_offload_tx_resync_request);\n\nstatic void tls_device_resync_tx(struct sock *sk, struct tls_context *tls_ctx,\n\t\t\t\t u32 seq)\n{\n\tstruct net_device *netdev;\n\tstruct sk_buff *skb;\n\tint err = 0;\n\tu8 *rcd_sn;\n\n\tskb = tcp_write_queue_tail(sk);\n\tif (skb)\n\t\tTCP_SKB_CB(skb)->eor = 1;\n\n\trcd_sn = tls_ctx->tx.rec_seq;\n\n\ttrace_tls_device_tx_resync_send(sk, seq, rcd_sn);\n\tdown_read(&device_offload_lock);\n\tnetdev = rcu_dereference_protected(tls_ctx->netdev,\n\t\t\t\t\t   lockdep_is_held(&device_offload_lock));\n\tif (netdev)\n\t\terr = netdev->tlsdev_ops->tls_dev_resync(netdev, sk, seq,\n\t\t\t\t\t\t\t rcd_sn,\n\t\t\t\t\t\t\t TLS_OFFLOAD_CTX_DIR_TX);\n\tup_read(&device_offload_lock);\n\tif (err)\n\t\treturn;\n\n\tclear_bit_unlock(TLS_TX_SYNC_SCHED, &tls_ctx->flags);\n}\n\nstatic void tls_append_frag(struct tls_record_info *record,\n\t\t\t    struct page_frag *pfrag,\n\t\t\t    int size)\n{\n\tskb_frag_t *frag;\n\n\tfrag = &record->frags[record->num_frags - 1];\n\tif (skb_frag_page(frag) == pfrag->page &&\n\t    skb_frag_off(frag) + skb_frag_size(frag) == pfrag->offset) {\n\t\tskb_frag_size_add(frag, size);\n\t} else {\n\t\t++frag;\n\t\tskb_frag_fill_page_desc(frag, pfrag->page, pfrag->offset,\n\t\t\t\t\tsize);\n\t\t++record->num_frags;\n\t\tget_page(pfrag->page);\n\t}\n\n\tpfrag->offset += size;\n\trecord->len += size;\n}\n\nstatic int tls_push_record(struct sock *sk,\n\t\t\t   struct tls_context *ctx,\n\t\t\t   struct tls_offload_context_tx *offload_ctx,\n\t\t\t   struct tls_record_info *record,\n\t\t\t   int flags)\n{\n\tstruct tls_prot_info *prot = &ctx->prot_info;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tskb_frag_t *frag;\n\tint i;\n\n\trecord->end_seq = tp->write_seq + record->len;\n\tlist_add_tail_rcu(&record->list, &offload_ctx->records_list);\n\toffload_ctx->open_record = NULL;\n\n\tif (test_bit(TLS_TX_SYNC_SCHED, &ctx->flags))\n\t\ttls_device_resync_tx(sk, ctx, tp->write_seq);\n\n\ttls_advance_record_sn(sk, prot, &ctx->tx);\n\n\tfor (i = 0; i < record->num_frags; i++) {\n\t\tfrag = &record->frags[i];\n\t\tsg_unmark_end(&offload_ctx->sg_tx_data[i]);\n\t\tsg_set_page(&offload_ctx->sg_tx_data[i], skb_frag_page(frag),\n\t\t\t    skb_frag_size(frag), skb_frag_off(frag));\n\t\tsk_mem_charge(sk, skb_frag_size(frag));\n\t\tget_page(skb_frag_page(frag));\n\t}\n\tsg_mark_end(&offload_ctx->sg_tx_data[record->num_frags - 1]);\n\n\t \n\treturn tls_push_sg(sk, ctx, offload_ctx->sg_tx_data, 0, flags);\n}\n\nstatic void tls_device_record_close(struct sock *sk,\n\t\t\t\t    struct tls_context *ctx,\n\t\t\t\t    struct tls_record_info *record,\n\t\t\t\t    struct page_frag *pfrag,\n\t\t\t\t    unsigned char record_type)\n{\n\tstruct tls_prot_info *prot = &ctx->prot_info;\n\tstruct page_frag dummy_tag_frag;\n\n\t \n\tif (unlikely(pfrag->size - pfrag->offset < prot->tag_size) &&\n\t    !skb_page_frag_refill(prot->tag_size, pfrag, sk->sk_allocation)) {\n\t\tdummy_tag_frag.page = dummy_page;\n\t\tdummy_tag_frag.offset = 0;\n\t\tpfrag = &dummy_tag_frag;\n\t}\n\ttls_append_frag(record, pfrag, prot->tag_size);\n\n\t \n\ttls_fill_prepend(ctx, skb_frag_address(&record->frags[0]),\n\t\t\t record->len - prot->overhead_size,\n\t\t\t record_type);\n}\n\nstatic int tls_create_new_record(struct tls_offload_context_tx *offload_ctx,\n\t\t\t\t struct page_frag *pfrag,\n\t\t\t\t size_t prepend_size)\n{\n\tstruct tls_record_info *record;\n\tskb_frag_t *frag;\n\n\trecord = kmalloc(sizeof(*record), GFP_KERNEL);\n\tif (!record)\n\t\treturn -ENOMEM;\n\n\tfrag = &record->frags[0];\n\tskb_frag_fill_page_desc(frag, pfrag->page, pfrag->offset,\n\t\t\t\tprepend_size);\n\n\tget_page(pfrag->page);\n\tpfrag->offset += prepend_size;\n\n\trecord->num_frags = 1;\n\trecord->len = prepend_size;\n\toffload_ctx->open_record = record;\n\treturn 0;\n}\n\nstatic int tls_do_allocation(struct sock *sk,\n\t\t\t     struct tls_offload_context_tx *offload_ctx,\n\t\t\t     struct page_frag *pfrag,\n\t\t\t     size_t prepend_size)\n{\n\tint ret;\n\n\tif (!offload_ctx->open_record) {\n\t\tif (unlikely(!skb_page_frag_refill(prepend_size, pfrag,\n\t\t\t\t\t\t   sk->sk_allocation))) {\n\t\t\tREAD_ONCE(sk->sk_prot)->enter_memory_pressure(sk);\n\t\t\tsk_stream_moderate_sndbuf(sk);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tret = tls_create_new_record(offload_ctx, pfrag, prepend_size);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tif (pfrag->size > pfrag->offset)\n\t\t\treturn 0;\n\t}\n\n\tif (!sk_page_frag_refill(sk, pfrag))\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic int tls_device_copy_data(void *addr, size_t bytes, struct iov_iter *i)\n{\n\tsize_t pre_copy, nocache;\n\n\tpre_copy = ~((unsigned long)addr - 1) & (SMP_CACHE_BYTES - 1);\n\tif (pre_copy) {\n\t\tpre_copy = min(pre_copy, bytes);\n\t\tif (copy_from_iter(addr, pre_copy, i) != pre_copy)\n\t\t\treturn -EFAULT;\n\t\tbytes -= pre_copy;\n\t\taddr += pre_copy;\n\t}\n\n\tnocache = round_down(bytes, SMP_CACHE_BYTES);\n\tif (copy_from_iter_nocache(addr, nocache, i) != nocache)\n\t\treturn -EFAULT;\n\tbytes -= nocache;\n\taddr += nocache;\n\n\tif (bytes && copy_from_iter(addr, bytes, i) != bytes)\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic int tls_push_data(struct sock *sk,\n\t\t\t struct iov_iter *iter,\n\t\t\t size_t size, int flags,\n\t\t\t unsigned char record_type)\n{\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\tstruct tls_prot_info *prot = &tls_ctx->prot_info;\n\tstruct tls_offload_context_tx *ctx = tls_offload_ctx_tx(tls_ctx);\n\tstruct tls_record_info *record;\n\tint tls_push_record_flags;\n\tstruct page_frag *pfrag;\n\tsize_t orig_size = size;\n\tu32 max_open_record_len;\n\tbool more = false;\n\tbool done = false;\n\tint copy, rc = 0;\n\tlong timeo;\n\n\tif (flags &\n\t    ~(MSG_MORE | MSG_DONTWAIT | MSG_NOSIGNAL |\n\t      MSG_SPLICE_PAGES | MSG_EOR))\n\t\treturn -EOPNOTSUPP;\n\n\tif ((flags & (MSG_MORE | MSG_EOR)) == (MSG_MORE | MSG_EOR))\n\t\treturn -EINVAL;\n\n\tif (unlikely(sk->sk_err))\n\t\treturn -sk->sk_err;\n\n\tflags |= MSG_SENDPAGE_DECRYPTED;\n\ttls_push_record_flags = flags | MSG_MORE;\n\n\ttimeo = sock_sndtimeo(sk, flags & MSG_DONTWAIT);\n\tif (tls_is_partially_sent_record(tls_ctx)) {\n\t\trc = tls_push_partial_record(sk, tls_ctx, flags);\n\t\tif (rc < 0)\n\t\t\treturn rc;\n\t}\n\n\tpfrag = sk_page_frag(sk);\n\n\t \n\tmax_open_record_len = TLS_MAX_PAYLOAD_SIZE +\n\t\t\t      prot->prepend_size;\n\tdo {\n\t\trc = tls_do_allocation(sk, ctx, pfrag, prot->prepend_size);\n\t\tif (unlikely(rc)) {\n\t\t\trc = sk_stream_wait_memory(sk, &timeo);\n\t\t\tif (!rc)\n\t\t\t\tcontinue;\n\n\t\t\trecord = ctx->open_record;\n\t\t\tif (!record)\n\t\t\t\tbreak;\nhandle_error:\n\t\t\tif (record_type != TLS_RECORD_TYPE_DATA) {\n\t\t\t\t \n\t\t\t\tsize = orig_size;\n\t\t\t\tdestroy_record(record);\n\t\t\t\tctx->open_record = NULL;\n\t\t\t} else if (record->len > prot->prepend_size) {\n\t\t\t\tgoto last_record;\n\t\t\t}\n\n\t\t\tbreak;\n\t\t}\n\n\t\trecord = ctx->open_record;\n\n\t\tcopy = min_t(size_t, size, max_open_record_len - record->len);\n\t\tif (copy && (flags & MSG_SPLICE_PAGES)) {\n\t\t\tstruct page_frag zc_pfrag;\n\t\t\tstruct page **pages = &zc_pfrag.page;\n\t\t\tsize_t off;\n\n\t\t\trc = iov_iter_extract_pages(iter, &pages,\n\t\t\t\t\t\t    copy, 1, 0, &off);\n\t\t\tif (rc <= 0) {\n\t\t\t\tif (rc == 0)\n\t\t\t\t\trc = -EIO;\n\t\t\t\tgoto handle_error;\n\t\t\t}\n\t\t\tcopy = rc;\n\n\t\t\tif (WARN_ON_ONCE(!sendpage_ok(zc_pfrag.page))) {\n\t\t\t\tiov_iter_revert(iter, copy);\n\t\t\t\trc = -EIO;\n\t\t\t\tgoto handle_error;\n\t\t\t}\n\n\t\t\tzc_pfrag.offset = off;\n\t\t\tzc_pfrag.size = copy;\n\t\t\ttls_append_frag(record, &zc_pfrag, copy);\n\t\t} else if (copy) {\n\t\t\tcopy = min_t(size_t, copy, pfrag->size - pfrag->offset);\n\n\t\t\trc = tls_device_copy_data(page_address(pfrag->page) +\n\t\t\t\t\t\t  pfrag->offset, copy,\n\t\t\t\t\t\t  iter);\n\t\t\tif (rc)\n\t\t\t\tgoto handle_error;\n\t\t\ttls_append_frag(record, pfrag, copy);\n\t\t}\n\n\t\tsize -= copy;\n\t\tif (!size) {\nlast_record:\n\t\t\ttls_push_record_flags = flags;\n\t\t\tif (flags & MSG_MORE) {\n\t\t\t\tmore = true;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tdone = true;\n\t\t}\n\n\t\tif (done || record->len >= max_open_record_len ||\n\t\t    (record->num_frags >= MAX_SKB_FRAGS - 1)) {\n\t\t\ttls_device_record_close(sk, tls_ctx, record,\n\t\t\t\t\t\tpfrag, record_type);\n\n\t\t\trc = tls_push_record(sk,\n\t\t\t\t\t     tls_ctx,\n\t\t\t\t\t     ctx,\n\t\t\t\t\t     record,\n\t\t\t\t\t     tls_push_record_flags);\n\t\t\tif (rc < 0)\n\t\t\t\tbreak;\n\t\t}\n\t} while (!done);\n\n\ttls_ctx->pending_open_record_frags = more;\n\n\tif (orig_size - size > 0)\n\t\trc = orig_size - size;\n\n\treturn rc;\n}\n\nint tls_device_sendmsg(struct sock *sk, struct msghdr *msg, size_t size)\n{\n\tunsigned char record_type = TLS_RECORD_TYPE_DATA;\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\tint rc;\n\n\tif (!tls_ctx->zerocopy_sendfile)\n\t\tmsg->msg_flags &= ~MSG_SPLICE_PAGES;\n\n\tmutex_lock(&tls_ctx->tx_lock);\n\tlock_sock(sk);\n\n\tif (unlikely(msg->msg_controllen)) {\n\t\trc = tls_process_cmsg(sk, msg, &record_type);\n\t\tif (rc)\n\t\t\tgoto out;\n\t}\n\n\trc = tls_push_data(sk, &msg->msg_iter, size, msg->msg_flags,\n\t\t\t   record_type);\n\nout:\n\trelease_sock(sk);\n\tmutex_unlock(&tls_ctx->tx_lock);\n\treturn rc;\n}\n\nvoid tls_device_splice_eof(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\tstruct iov_iter iter = {};\n\n\tif (!tls_is_partially_sent_record(tls_ctx))\n\t\treturn;\n\n\tmutex_lock(&tls_ctx->tx_lock);\n\tlock_sock(sk);\n\n\tif (tls_is_partially_sent_record(tls_ctx)) {\n\t\tiov_iter_bvec(&iter, ITER_SOURCE, NULL, 0, 0);\n\t\ttls_push_data(sk, &iter, 0, 0, TLS_RECORD_TYPE_DATA);\n\t}\n\n\trelease_sock(sk);\n\tmutex_unlock(&tls_ctx->tx_lock);\n}\n\nstruct tls_record_info *tls_get_record(struct tls_offload_context_tx *context,\n\t\t\t\t       u32 seq, u64 *p_record_sn)\n{\n\tu64 record_sn = context->hint_record_sn;\n\tstruct tls_record_info *info, *last;\n\n\tinfo = context->retransmit_hint;\n\tif (!info ||\n\t    before(seq, info->end_seq - info->len)) {\n\t\t \n\t\tinfo = list_first_entry_or_null(&context->records_list,\n\t\t\t\t\t\tstruct tls_record_info, list);\n\t\tif (!info)\n\t\t\treturn NULL;\n\t\t \n\t\tif (likely(!tls_record_is_start_marker(info))) {\n\t\t\t \n\t\t\tlast = list_last_entry(&context->records_list,\n\t\t\t\t\t       struct tls_record_info, list);\n\n\t\t\tif (!between(seq, tls_record_start_seq(info),\n\t\t\t\t     last->end_seq))\n\t\t\t\treturn NULL;\n\t\t}\n\t\trecord_sn = context->unacked_record_sn;\n\t}\n\n\t \n\trcu_read_lock();\n\tlist_for_each_entry_from_rcu(info, &context->records_list, list) {\n\t\tif (before(seq, info->end_seq)) {\n\t\t\tif (!context->retransmit_hint ||\n\t\t\t    after(info->end_seq,\n\t\t\t\t  context->retransmit_hint->end_seq)) {\n\t\t\t\tcontext->hint_record_sn = record_sn;\n\t\t\t\tcontext->retransmit_hint = info;\n\t\t\t}\n\t\t\t*p_record_sn = record_sn;\n\t\t\tgoto exit_rcu_unlock;\n\t\t}\n\t\trecord_sn++;\n\t}\n\tinfo = NULL;\n\nexit_rcu_unlock:\n\trcu_read_unlock();\n\treturn info;\n}\nEXPORT_SYMBOL(tls_get_record);\n\nstatic int tls_device_push_pending_record(struct sock *sk, int flags)\n{\n\tstruct iov_iter iter;\n\n\tiov_iter_kvec(&iter, ITER_SOURCE, NULL, 0, 0);\n\treturn tls_push_data(sk, &iter, 0, flags, TLS_RECORD_TYPE_DATA);\n}\n\nvoid tls_device_write_space(struct sock *sk, struct tls_context *ctx)\n{\n\tif (tls_is_partially_sent_record(ctx)) {\n\t\tgfp_t sk_allocation = sk->sk_allocation;\n\n\t\tWARN_ON_ONCE(sk->sk_write_pending);\n\n\t\tsk->sk_allocation = GFP_ATOMIC;\n\t\ttls_push_partial_record(sk, ctx,\n\t\t\t\t\tMSG_DONTWAIT | MSG_NOSIGNAL |\n\t\t\t\t\tMSG_SENDPAGE_DECRYPTED);\n\t\tsk->sk_allocation = sk_allocation;\n\t}\n}\n\nstatic void tls_device_resync_rx(struct tls_context *tls_ctx,\n\t\t\t\t struct sock *sk, u32 seq, u8 *rcd_sn)\n{\n\tstruct tls_offload_context_rx *rx_ctx = tls_offload_ctx_rx(tls_ctx);\n\tstruct net_device *netdev;\n\n\ttrace_tls_device_rx_resync_send(sk, seq, rcd_sn, rx_ctx->resync_type);\n\trcu_read_lock();\n\tnetdev = rcu_dereference(tls_ctx->netdev);\n\tif (netdev)\n\t\tnetdev->tlsdev_ops->tls_dev_resync(netdev, sk, seq, rcd_sn,\n\t\t\t\t\t\t   TLS_OFFLOAD_CTX_DIR_RX);\n\trcu_read_unlock();\n\tTLS_INC_STATS(sock_net(sk), LINUX_MIB_TLSRXDEVICERESYNC);\n}\n\nstatic bool\ntls_device_rx_resync_async(struct tls_offload_resync_async *resync_async,\n\t\t\t   s64 resync_req, u32 *seq, u16 *rcd_delta)\n{\n\tu32 is_async = resync_req & RESYNC_REQ_ASYNC;\n\tu32 req_seq = resync_req >> 32;\n\tu32 req_end = req_seq + ((resync_req >> 16) & 0xffff);\n\tu16 i;\n\n\t*rcd_delta = 0;\n\n\tif (is_async) {\n\t\t \n\t\tif (WARN_ON_ONCE(resync_async->rcd_delta == USHRT_MAX))\n\t\t\treturn false;\n\n\t\t \n\t\tif (before(*seq, req_seq))\n\t\t\treturn false;\n\t\tif (!after(*seq, req_end) &&\n\t\t    resync_async->loglen < TLS_DEVICE_RESYNC_ASYNC_LOGMAX)\n\t\t\tresync_async->log[resync_async->loglen++] = *seq;\n\n\t\tresync_async->rcd_delta++;\n\n\t\treturn false;\n\t}\n\n\t \n\tfor (i = 0; i < resync_async->loglen; i++)\n\t\tif (req_seq == resync_async->log[i] &&\n\t\t    atomic64_try_cmpxchg(&resync_async->req, &resync_req, 0)) {\n\t\t\t*rcd_delta = resync_async->rcd_delta - i;\n\t\t\t*seq = req_seq;\n\t\t\tresync_async->loglen = 0;\n\t\t\tresync_async->rcd_delta = 0;\n\t\t\treturn true;\n\t\t}\n\n\tresync_async->loglen = 0;\n\tresync_async->rcd_delta = 0;\n\n\tif (req_seq == *seq &&\n\t    atomic64_try_cmpxchg(&resync_async->req,\n\t\t\t\t &resync_req, 0))\n\t\treturn true;\n\n\treturn false;\n}\n\nvoid tls_device_rx_resync_new_rec(struct sock *sk, u32 rcd_len, u32 seq)\n{\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\tstruct tls_offload_context_rx *rx_ctx;\n\tu8 rcd_sn[TLS_MAX_REC_SEQ_SIZE];\n\tu32 sock_data, is_req_pending;\n\tstruct tls_prot_info *prot;\n\ts64 resync_req;\n\tu16 rcd_delta;\n\tu32 req_seq;\n\n\tif (tls_ctx->rx_conf != TLS_HW)\n\t\treturn;\n\tif (unlikely(test_bit(TLS_RX_DEV_DEGRADED, &tls_ctx->flags)))\n\t\treturn;\n\n\tprot = &tls_ctx->prot_info;\n\trx_ctx = tls_offload_ctx_rx(tls_ctx);\n\tmemcpy(rcd_sn, tls_ctx->rx.rec_seq, prot->rec_seq_size);\n\n\tswitch (rx_ctx->resync_type) {\n\tcase TLS_OFFLOAD_SYNC_TYPE_DRIVER_REQ:\n\t\tresync_req = atomic64_read(&rx_ctx->resync_req);\n\t\treq_seq = resync_req >> 32;\n\t\tseq += TLS_HEADER_SIZE - 1;\n\t\tis_req_pending = resync_req;\n\n\t\tif (likely(!is_req_pending) || req_seq != seq ||\n\t\t    !atomic64_try_cmpxchg(&rx_ctx->resync_req, &resync_req, 0))\n\t\t\treturn;\n\t\tbreak;\n\tcase TLS_OFFLOAD_SYNC_TYPE_CORE_NEXT_HINT:\n\t\tif (likely(!rx_ctx->resync_nh_do_now))\n\t\t\treturn;\n\n\t\t \n\t\tsock_data = tcp_inq(sk);\n\t\tif (sock_data > rcd_len) {\n\t\t\ttrace_tls_device_rx_resync_nh_delay(sk, sock_data,\n\t\t\t\t\t\t\t    rcd_len);\n\t\t\treturn;\n\t\t}\n\n\t\trx_ctx->resync_nh_do_now = 0;\n\t\tseq += rcd_len;\n\t\ttls_bigint_increment(rcd_sn, prot->rec_seq_size);\n\t\tbreak;\n\tcase TLS_OFFLOAD_SYNC_TYPE_DRIVER_REQ_ASYNC:\n\t\tresync_req = atomic64_read(&rx_ctx->resync_async->req);\n\t\tis_req_pending = resync_req;\n\t\tif (likely(!is_req_pending))\n\t\t\treturn;\n\n\t\tif (!tls_device_rx_resync_async(rx_ctx->resync_async,\n\t\t\t\t\t\tresync_req, &seq, &rcd_delta))\n\t\t\treturn;\n\t\ttls_bigint_subtract(rcd_sn, rcd_delta);\n\t\tbreak;\n\t}\n\n\ttls_device_resync_rx(tls_ctx, sk, seq, rcd_sn);\n}\n\nstatic void tls_device_core_ctrl_rx_resync(struct tls_context *tls_ctx,\n\t\t\t\t\t   struct tls_offload_context_rx *ctx,\n\t\t\t\t\t   struct sock *sk, struct sk_buff *skb)\n{\n\tstruct strp_msg *rxm;\n\n\t \n\tif (ctx->resync_type != TLS_OFFLOAD_SYNC_TYPE_CORE_NEXT_HINT)\n\t\treturn;\n\t \n\tif (ctx->resync_nh_do_now)\n\t\treturn;\n\t \n\tif (ctx->resync_nh_reset) {\n\t\tctx->resync_nh_reset = 0;\n\t\tctx->resync_nh.decrypted_failed = 1;\n\t\tctx->resync_nh.decrypted_tgt = TLS_DEVICE_RESYNC_NH_START_IVAL;\n\t\treturn;\n\t}\n\n\tif (++ctx->resync_nh.decrypted_failed <= ctx->resync_nh.decrypted_tgt)\n\t\treturn;\n\n\t \n\tif (ctx->resync_nh.decrypted_tgt < TLS_DEVICE_RESYNC_NH_MAX_IVAL)\n\t\tctx->resync_nh.decrypted_tgt *= 2;\n\telse\n\t\tctx->resync_nh.decrypted_tgt += TLS_DEVICE_RESYNC_NH_MAX_IVAL;\n\n\trxm = strp_msg(skb);\n\n\t \n\tif (tcp_inq(sk) > rxm->full_len) {\n\t\ttrace_tls_device_rx_resync_nh_schedule(sk);\n\t\tctx->resync_nh_do_now = 1;\n\t} else {\n\t\tstruct tls_prot_info *prot = &tls_ctx->prot_info;\n\t\tu8 rcd_sn[TLS_MAX_REC_SEQ_SIZE];\n\n\t\tmemcpy(rcd_sn, tls_ctx->rx.rec_seq, prot->rec_seq_size);\n\t\ttls_bigint_increment(rcd_sn, prot->rec_seq_size);\n\n\t\ttls_device_resync_rx(tls_ctx, sk, tcp_sk(sk)->copied_seq,\n\t\t\t\t     rcd_sn);\n\t}\n}\n\nstatic int\ntls_device_reencrypt(struct sock *sk, struct tls_context *tls_ctx)\n{\n\tstruct tls_sw_context_rx *sw_ctx = tls_sw_ctx_rx(tls_ctx);\n\tconst struct tls_cipher_desc *cipher_desc;\n\tint err, offset, copy, data_len, pos;\n\tstruct sk_buff *skb, *skb_iter;\n\tstruct scatterlist sg[1];\n\tstruct strp_msg *rxm;\n\tchar *orig_buf, *buf;\n\n\tswitch (tls_ctx->crypto_recv.info.cipher_type) {\n\tcase TLS_CIPHER_AES_GCM_128:\n\tcase TLS_CIPHER_AES_GCM_256:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\tcipher_desc = get_cipher_desc(tls_ctx->crypto_recv.info.cipher_type);\n\n\trxm = strp_msg(tls_strp_msg(sw_ctx));\n\torig_buf = kmalloc(rxm->full_len + TLS_HEADER_SIZE + cipher_desc->iv,\n\t\t\t   sk->sk_allocation);\n\tif (!orig_buf)\n\t\treturn -ENOMEM;\n\tbuf = orig_buf;\n\n\terr = tls_strp_msg_cow(sw_ctx);\n\tif (unlikely(err))\n\t\tgoto free_buf;\n\n\tskb = tls_strp_msg(sw_ctx);\n\trxm = strp_msg(skb);\n\toffset = rxm->offset;\n\n\tsg_init_table(sg, 1);\n\tsg_set_buf(&sg[0], buf,\n\t\t   rxm->full_len + TLS_HEADER_SIZE + cipher_desc->iv);\n\terr = skb_copy_bits(skb, offset, buf, TLS_HEADER_SIZE + cipher_desc->iv);\n\tif (err)\n\t\tgoto free_buf;\n\n\t \n\terr = decrypt_skb(sk, sg);\n\tif (err != -EBADMSG)\n\t\tgoto free_buf;\n\telse\n\t\terr = 0;\n\n\tdata_len = rxm->full_len - cipher_desc->tag;\n\n\tif (skb_pagelen(skb) > offset) {\n\t\tcopy = min_t(int, skb_pagelen(skb) - offset, data_len);\n\n\t\tif (skb->decrypted) {\n\t\t\terr = skb_store_bits(skb, offset, buf, copy);\n\t\t\tif (err)\n\t\t\t\tgoto free_buf;\n\t\t}\n\n\t\toffset += copy;\n\t\tbuf += copy;\n\t}\n\n\tpos = skb_pagelen(skb);\n\tskb_walk_frags(skb, skb_iter) {\n\t\tint frag_pos;\n\n\t\t \n\t\tif (pos + skb_iter->len <= offset)\n\t\t\tgoto done_with_frag;\n\t\tif (pos >= data_len + rxm->offset)\n\t\t\tbreak;\n\n\t\tfrag_pos = offset - pos;\n\t\tcopy = min_t(int, skb_iter->len - frag_pos,\n\t\t\t     data_len + rxm->offset - offset);\n\n\t\tif (skb_iter->decrypted) {\n\t\t\terr = skb_store_bits(skb_iter, frag_pos, buf, copy);\n\t\t\tif (err)\n\t\t\t\tgoto free_buf;\n\t\t}\n\n\t\toffset += copy;\n\t\tbuf += copy;\ndone_with_frag:\n\t\tpos += skb_iter->len;\n\t}\n\nfree_buf:\n\tkfree(orig_buf);\n\treturn err;\n}\n\nint tls_device_decrypted(struct sock *sk, struct tls_context *tls_ctx)\n{\n\tstruct tls_offload_context_rx *ctx = tls_offload_ctx_rx(tls_ctx);\n\tstruct tls_sw_context_rx *sw_ctx = tls_sw_ctx_rx(tls_ctx);\n\tstruct sk_buff *skb = tls_strp_msg(sw_ctx);\n\tstruct strp_msg *rxm = strp_msg(skb);\n\tint is_decrypted, is_encrypted;\n\n\tif (!tls_strp_msg_mixed_decrypted(sw_ctx)) {\n\t\tis_decrypted = skb->decrypted;\n\t\tis_encrypted = !is_decrypted;\n\t} else {\n\t\tis_decrypted = 0;\n\t\tis_encrypted = 0;\n\t}\n\n\ttrace_tls_device_decrypted(sk, tcp_sk(sk)->copied_seq - rxm->full_len,\n\t\t\t\t   tls_ctx->rx.rec_seq, rxm->full_len,\n\t\t\t\t   is_encrypted, is_decrypted);\n\n\tif (unlikely(test_bit(TLS_RX_DEV_DEGRADED, &tls_ctx->flags))) {\n\t\tif (likely(is_encrypted || is_decrypted))\n\t\t\treturn is_decrypted;\n\n\t\t \n\t\treturn tls_device_reencrypt(sk, tls_ctx);\n\t}\n\n\t \n\tif (is_decrypted) {\n\t\tctx->resync_nh_reset = 1;\n\t\treturn is_decrypted;\n\t}\n\tif (is_encrypted) {\n\t\ttls_device_core_ctrl_rx_resync(tls_ctx, ctx, sk, skb);\n\t\treturn 0;\n\t}\n\n\tctx->resync_nh_reset = 1;\n\treturn tls_device_reencrypt(sk, tls_ctx);\n}\n\nstatic void tls_device_attach(struct tls_context *ctx, struct sock *sk,\n\t\t\t      struct net_device *netdev)\n{\n\tif (sk->sk_destruct != tls_device_sk_destruct) {\n\t\trefcount_set(&ctx->refcount, 1);\n\t\tdev_hold(netdev);\n\t\tRCU_INIT_POINTER(ctx->netdev, netdev);\n\t\tspin_lock_irq(&tls_device_lock);\n\t\tlist_add_tail(&ctx->list, &tls_device_list);\n\t\tspin_unlock_irq(&tls_device_lock);\n\n\t\tctx->sk_destruct = sk->sk_destruct;\n\t\tsmp_store_release(&sk->sk_destruct, tls_device_sk_destruct);\n\t}\n}\n\nint tls_set_device_offload(struct sock *sk, struct tls_context *ctx)\n{\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\tstruct tls_prot_info *prot = &tls_ctx->prot_info;\n\tconst struct tls_cipher_desc *cipher_desc;\n\tstruct tls_record_info *start_marker_record;\n\tstruct tls_offload_context_tx *offload_ctx;\n\tstruct tls_crypto_info *crypto_info;\n\tstruct net_device *netdev;\n\tchar *iv, *rec_seq;\n\tstruct sk_buff *skb;\n\t__be64 rcd_sn;\n\tint rc;\n\n\tif (!ctx)\n\t\treturn -EINVAL;\n\n\tif (ctx->priv_ctx_tx)\n\t\treturn -EEXIST;\n\n\tnetdev = get_netdev_for_sock(sk);\n\tif (!netdev) {\n\t\tpr_err_ratelimited(\"%s: netdev not found\\n\", __func__);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!(netdev->features & NETIF_F_HW_TLS_TX)) {\n\t\trc = -EOPNOTSUPP;\n\t\tgoto release_netdev;\n\t}\n\n\tcrypto_info = &ctx->crypto_send.info;\n\tif (crypto_info->version != TLS_1_2_VERSION) {\n\t\trc = -EOPNOTSUPP;\n\t\tgoto release_netdev;\n\t}\n\n\tcipher_desc = get_cipher_desc(crypto_info->cipher_type);\n\tif (!cipher_desc || !cipher_desc->offloadable) {\n\t\trc = -EINVAL;\n\t\tgoto release_netdev;\n\t}\n\n\tiv = crypto_info_iv(crypto_info, cipher_desc);\n\trec_seq = crypto_info_rec_seq(crypto_info, cipher_desc);\n\n\tprot->version = crypto_info->version;\n\tprot->cipher_type = crypto_info->cipher_type;\n\tprot->prepend_size = TLS_HEADER_SIZE + cipher_desc->iv;\n\tprot->tag_size = cipher_desc->tag;\n\tprot->overhead_size = prot->prepend_size + prot->tag_size;\n\tprot->iv_size = cipher_desc->iv;\n\tprot->salt_size = cipher_desc->salt;\n\tctx->tx.iv = kmalloc(cipher_desc->iv + cipher_desc->salt, GFP_KERNEL);\n\tif (!ctx->tx.iv) {\n\t\trc = -ENOMEM;\n\t\tgoto release_netdev;\n\t}\n\n\tmemcpy(ctx->tx.iv + cipher_desc->salt, iv, cipher_desc->iv);\n\n\tprot->rec_seq_size = cipher_desc->rec_seq;\n\tctx->tx.rec_seq = kmemdup(rec_seq, cipher_desc->rec_seq, GFP_KERNEL);\n\tif (!ctx->tx.rec_seq) {\n\t\trc = -ENOMEM;\n\t\tgoto free_iv;\n\t}\n\n\tstart_marker_record = kmalloc(sizeof(*start_marker_record), GFP_KERNEL);\n\tif (!start_marker_record) {\n\t\trc = -ENOMEM;\n\t\tgoto free_rec_seq;\n\t}\n\n\toffload_ctx = kzalloc(TLS_OFFLOAD_CONTEXT_SIZE_TX, GFP_KERNEL);\n\tif (!offload_ctx) {\n\t\trc = -ENOMEM;\n\t\tgoto free_marker_record;\n\t}\n\n\trc = tls_sw_fallback_init(sk, offload_ctx, crypto_info);\n\tif (rc)\n\t\tgoto free_offload_ctx;\n\n\t \n\tmemcpy(&rcd_sn, ctx->tx.rec_seq, sizeof(rcd_sn));\n\toffload_ctx->unacked_record_sn = be64_to_cpu(rcd_sn) - 1;\n\n\tstart_marker_record->end_seq = tcp_sk(sk)->write_seq;\n\tstart_marker_record->len = 0;\n\tstart_marker_record->num_frags = 0;\n\n\tINIT_WORK(&offload_ctx->destruct_work, tls_device_tx_del_task);\n\toffload_ctx->ctx = ctx;\n\n\tINIT_LIST_HEAD(&offload_ctx->records_list);\n\tlist_add_tail(&start_marker_record->list, &offload_ctx->records_list);\n\tspin_lock_init(&offload_ctx->lock);\n\tsg_init_table(offload_ctx->sg_tx_data,\n\t\t      ARRAY_SIZE(offload_ctx->sg_tx_data));\n\n\tclean_acked_data_enable(inet_csk(sk), &tls_icsk_clean_acked);\n\tctx->push_pending_record = tls_device_push_pending_record;\n\n\t \n\tskb = tcp_write_queue_tail(sk);\n\tif (skb)\n\t\tTCP_SKB_CB(skb)->eor = 1;\n\n\t \n\tdown_read(&device_offload_lock);\n\tif (!(netdev->flags & IFF_UP)) {\n\t\trc = -EINVAL;\n\t\tgoto release_lock;\n\t}\n\n\tctx->priv_ctx_tx = offload_ctx;\n\trc = netdev->tlsdev_ops->tls_dev_add(netdev, sk, TLS_OFFLOAD_CTX_DIR_TX,\n\t\t\t\t\t     &ctx->crypto_send.info,\n\t\t\t\t\t     tcp_sk(sk)->write_seq);\n\ttrace_tls_device_offload_set(sk, TLS_OFFLOAD_CTX_DIR_TX,\n\t\t\t\t     tcp_sk(sk)->write_seq, rec_seq, rc);\n\tif (rc)\n\t\tgoto release_lock;\n\n\ttls_device_attach(ctx, sk, netdev);\n\tup_read(&device_offload_lock);\n\n\t \n\tsmp_store_release(&sk->sk_validate_xmit_skb, tls_validate_xmit_skb);\n\tdev_put(netdev);\n\n\treturn 0;\n\nrelease_lock:\n\tup_read(&device_offload_lock);\n\tclean_acked_data_disable(inet_csk(sk));\n\tcrypto_free_aead(offload_ctx->aead_send);\nfree_offload_ctx:\n\tkfree(offload_ctx);\n\tctx->priv_ctx_tx = NULL;\nfree_marker_record:\n\tkfree(start_marker_record);\nfree_rec_seq:\n\tkfree(ctx->tx.rec_seq);\nfree_iv:\n\tkfree(ctx->tx.iv);\nrelease_netdev:\n\tdev_put(netdev);\n\treturn rc;\n}\n\nint tls_set_device_offload_rx(struct sock *sk, struct tls_context *ctx)\n{\n\tstruct tls12_crypto_info_aes_gcm_128 *info;\n\tstruct tls_offload_context_rx *context;\n\tstruct net_device *netdev;\n\tint rc = 0;\n\n\tif (ctx->crypto_recv.info.version != TLS_1_2_VERSION)\n\t\treturn -EOPNOTSUPP;\n\n\tnetdev = get_netdev_for_sock(sk);\n\tif (!netdev) {\n\t\tpr_err_ratelimited(\"%s: netdev not found\\n\", __func__);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!(netdev->features & NETIF_F_HW_TLS_RX)) {\n\t\trc = -EOPNOTSUPP;\n\t\tgoto release_netdev;\n\t}\n\n\t \n\tdown_read(&device_offload_lock);\n\tif (!(netdev->flags & IFF_UP)) {\n\t\trc = -EINVAL;\n\t\tgoto release_lock;\n\t}\n\n\tcontext = kzalloc(TLS_OFFLOAD_CONTEXT_SIZE_RX, GFP_KERNEL);\n\tif (!context) {\n\t\trc = -ENOMEM;\n\t\tgoto release_lock;\n\t}\n\tcontext->resync_nh_reset = 1;\n\n\tctx->priv_ctx_rx = context;\n\trc = tls_set_sw_offload(sk, ctx, 0);\n\tif (rc)\n\t\tgoto release_ctx;\n\n\trc = netdev->tlsdev_ops->tls_dev_add(netdev, sk, TLS_OFFLOAD_CTX_DIR_RX,\n\t\t\t\t\t     &ctx->crypto_recv.info,\n\t\t\t\t\t     tcp_sk(sk)->copied_seq);\n\tinfo = (void *)&ctx->crypto_recv.info;\n\ttrace_tls_device_offload_set(sk, TLS_OFFLOAD_CTX_DIR_RX,\n\t\t\t\t     tcp_sk(sk)->copied_seq, info->rec_seq, rc);\n\tif (rc)\n\t\tgoto free_sw_resources;\n\n\ttls_device_attach(ctx, sk, netdev);\n\tup_read(&device_offload_lock);\n\n\tdev_put(netdev);\n\n\treturn 0;\n\nfree_sw_resources:\n\tup_read(&device_offload_lock);\n\ttls_sw_free_resources_rx(sk);\n\tdown_read(&device_offload_lock);\nrelease_ctx:\n\tctx->priv_ctx_rx = NULL;\nrelease_lock:\n\tup_read(&device_offload_lock);\nrelease_netdev:\n\tdev_put(netdev);\n\treturn rc;\n}\n\nvoid tls_device_offload_cleanup_rx(struct sock *sk)\n{\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\tstruct net_device *netdev;\n\n\tdown_read(&device_offload_lock);\n\tnetdev = rcu_dereference_protected(tls_ctx->netdev,\n\t\t\t\t\t   lockdep_is_held(&device_offload_lock));\n\tif (!netdev)\n\t\tgoto out;\n\n\tnetdev->tlsdev_ops->tls_dev_del(netdev, tls_ctx,\n\t\t\t\t\tTLS_OFFLOAD_CTX_DIR_RX);\n\n\tif (tls_ctx->tx_conf != TLS_HW) {\n\t\tdev_put(netdev);\n\t\trcu_assign_pointer(tls_ctx->netdev, NULL);\n\t} else {\n\t\tset_bit(TLS_RX_DEV_CLOSED, &tls_ctx->flags);\n\t}\nout:\n\tup_read(&device_offload_lock);\n\ttls_sw_release_resources_rx(sk);\n}\n\nstatic int tls_device_down(struct net_device *netdev)\n{\n\tstruct tls_context *ctx, *tmp;\n\tunsigned long flags;\n\tLIST_HEAD(list);\n\n\t \n\tdown_write(&device_offload_lock);\n\n\tspin_lock_irqsave(&tls_device_lock, flags);\n\tlist_for_each_entry_safe(ctx, tmp, &tls_device_list, list) {\n\t\tstruct net_device *ctx_netdev =\n\t\t\trcu_dereference_protected(ctx->netdev,\n\t\t\t\t\t\t  lockdep_is_held(&device_offload_lock));\n\n\t\tif (ctx_netdev != netdev ||\n\t\t    !refcount_inc_not_zero(&ctx->refcount))\n\t\t\tcontinue;\n\n\t\tlist_move(&ctx->list, &list);\n\t}\n\tspin_unlock_irqrestore(&tls_device_lock, flags);\n\n\tlist_for_each_entry_safe(ctx, tmp, &list, list)\t{\n\t\t \n\t\tWRITE_ONCE(ctx->sk->sk_validate_xmit_skb, tls_validate_xmit_skb_sw);\n\n\t\t \n\t\trcu_assign_pointer(ctx->netdev, NULL);\n\n\t\t \n\t\tset_bit(TLS_RX_DEV_DEGRADED, &ctx->flags);\n\n\t\t \n\t\tsynchronize_net();\n\n\t\t \n\t\tif (ctx->tx_conf == TLS_HW)\n\t\t\tnetdev->tlsdev_ops->tls_dev_del(netdev, ctx,\n\t\t\t\t\t\t\tTLS_OFFLOAD_CTX_DIR_TX);\n\t\tif (ctx->rx_conf == TLS_HW &&\n\t\t    !test_bit(TLS_RX_DEV_CLOSED, &ctx->flags))\n\t\t\tnetdev->tlsdev_ops->tls_dev_del(netdev, ctx,\n\t\t\t\t\t\t\tTLS_OFFLOAD_CTX_DIR_RX);\n\n\t\tdev_put(netdev);\n\n\t\t \n\t\tspin_lock_irqsave(&tls_device_lock, flags);\n\t\tlist_move_tail(&ctx->list, &tls_device_down_list);\n\t\tspin_unlock_irqrestore(&tls_device_lock, flags);\n\n\t\t \n\t\tif (refcount_dec_and_test(&ctx->refcount)) {\n\t\t\t \n\t\t\tlist_del(&ctx->list);\n\t\t\ttls_device_free_ctx(ctx);\n\t\t}\n\t}\n\n\tup_write(&device_offload_lock);\n\n\tflush_workqueue(destruct_wq);\n\n\treturn NOTIFY_DONE;\n}\n\nstatic int tls_dev_event(struct notifier_block *this, unsigned long event,\n\t\t\t void *ptr)\n{\n\tstruct net_device *dev = netdev_notifier_info_to_dev(ptr);\n\n\tif (!dev->tlsdev_ops &&\n\t    !(dev->features & (NETIF_F_HW_TLS_RX | NETIF_F_HW_TLS_TX)))\n\t\treturn NOTIFY_DONE;\n\n\tswitch (event) {\n\tcase NETDEV_REGISTER:\n\tcase NETDEV_FEAT_CHANGE:\n\t\tif (netif_is_bond_master(dev))\n\t\t\treturn NOTIFY_DONE;\n\t\tif ((dev->features & NETIF_F_HW_TLS_RX) &&\n\t\t    !dev->tlsdev_ops->tls_dev_resync)\n\t\t\treturn NOTIFY_BAD;\n\n\t\tif  (dev->tlsdev_ops &&\n\t\t     dev->tlsdev_ops->tls_dev_add &&\n\t\t     dev->tlsdev_ops->tls_dev_del)\n\t\t\treturn NOTIFY_DONE;\n\t\telse\n\t\t\treturn NOTIFY_BAD;\n\tcase NETDEV_DOWN:\n\t\treturn tls_device_down(dev);\n\t}\n\treturn NOTIFY_DONE;\n}\n\nstatic struct notifier_block tls_dev_notifier = {\n\t.notifier_call\t= tls_dev_event,\n};\n\nint __init tls_device_init(void)\n{\n\tint err;\n\n\tdummy_page = alloc_page(GFP_KERNEL);\n\tif (!dummy_page)\n\t\treturn -ENOMEM;\n\n\tdestruct_wq = alloc_workqueue(\"ktls_device_destruct\", 0, 0);\n\tif (!destruct_wq) {\n\t\terr = -ENOMEM;\n\t\tgoto err_free_dummy;\n\t}\n\n\terr = register_netdevice_notifier(&tls_dev_notifier);\n\tif (err)\n\t\tgoto err_destroy_wq;\n\n\treturn 0;\n\nerr_destroy_wq:\n\tdestroy_workqueue(destruct_wq);\nerr_free_dummy:\n\tput_page(dummy_page);\n\treturn err;\n}\n\nvoid __exit tls_device_cleanup(void)\n{\n\tunregister_netdevice_notifier(&tls_dev_notifier);\n\tdestroy_workqueue(destruct_wq);\n\tclean_acked_data_flush();\n\tput_page(dummy_page);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}