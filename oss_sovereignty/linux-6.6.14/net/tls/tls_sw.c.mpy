{
  "module_name": "tls_sw.c",
  "hash_id": "e6c76dea08c355d721caf6d3e5f5a7704dc4b53b871138dc68a53b2d70211d4e",
  "original_prompt": "Ingested from linux-6.6.14/net/tls/tls_sw.c",
  "human_readable_source": " \n\n#include <linux/bug.h>\n#include <linux/sched/signal.h>\n#include <linux/module.h>\n#include <linux/kernel.h>\n#include <linux/splice.h>\n#include <crypto/aead.h>\n\n#include <net/strparser.h>\n#include <net/tls.h>\n#include <trace/events/sock.h>\n\n#include \"tls.h\"\n\nstruct tls_decrypt_arg {\n\tstruct_group(inargs,\n\tbool zc;\n\tbool async;\n\tu8 tail;\n\t);\n\n\tstruct sk_buff *skb;\n};\n\nstruct tls_decrypt_ctx {\n\tstruct sock *sk;\n\tu8 iv[MAX_IV_SIZE];\n\tu8 aad[TLS_MAX_AAD_SIZE];\n\tu8 tail;\n\tstruct scatterlist sg[];\n};\n\nnoinline void tls_err_abort(struct sock *sk, int err)\n{\n\tWARN_ON_ONCE(err >= 0);\n\t \n\tWRITE_ONCE(sk->sk_err, -err);\n\t \n\tsmp_wmb();\n\tsk_error_report(sk);\n}\n\nstatic int __skb_nsg(struct sk_buff *skb, int offset, int len,\n                     unsigned int recursion_level)\n{\n        int start = skb_headlen(skb);\n        int i, chunk = start - offset;\n        struct sk_buff *frag_iter;\n        int elt = 0;\n\n        if (unlikely(recursion_level >= 24))\n                return -EMSGSIZE;\n\n        if (chunk > 0) {\n                if (chunk > len)\n                        chunk = len;\n                elt++;\n                len -= chunk;\n                if (len == 0)\n                        return elt;\n                offset += chunk;\n        }\n\n        for (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n                int end;\n\n                WARN_ON(start > offset + len);\n\n                end = start + skb_frag_size(&skb_shinfo(skb)->frags[i]);\n                chunk = end - offset;\n                if (chunk > 0) {\n                        if (chunk > len)\n                                chunk = len;\n                        elt++;\n                        len -= chunk;\n                        if (len == 0)\n                                return elt;\n                        offset += chunk;\n                }\n                start = end;\n        }\n\n        if (unlikely(skb_has_frag_list(skb))) {\n                skb_walk_frags(skb, frag_iter) {\n                        int end, ret;\n\n                        WARN_ON(start > offset + len);\n\n                        end = start + frag_iter->len;\n                        chunk = end - offset;\n                        if (chunk > 0) {\n                                if (chunk > len)\n                                        chunk = len;\n                                ret = __skb_nsg(frag_iter, offset - start, chunk,\n                                                recursion_level + 1);\n                                if (unlikely(ret < 0))\n                                        return ret;\n                                elt += ret;\n                                len -= chunk;\n                                if (len == 0)\n                                        return elt;\n                                offset += chunk;\n                        }\n                        start = end;\n                }\n        }\n        BUG_ON(len);\n        return elt;\n}\n\n \nstatic int skb_nsg(struct sk_buff *skb, int offset, int len)\n{\n        return __skb_nsg(skb, offset, len, 0);\n}\n\nstatic int tls_padding_length(struct tls_prot_info *prot, struct sk_buff *skb,\n\t\t\t      struct tls_decrypt_arg *darg)\n{\n\tstruct strp_msg *rxm = strp_msg(skb);\n\tstruct tls_msg *tlm = tls_msg(skb);\n\tint sub = 0;\n\n\t \n\tif (prot->version == TLS_1_3_VERSION) {\n\t\tint offset = rxm->full_len - TLS_TAG_SIZE - 1;\n\t\tchar content_type = darg->zc ? darg->tail : 0;\n\t\tint err;\n\n\t\twhile (content_type == 0) {\n\t\t\tif (offset < prot->prepend_size)\n\t\t\t\treturn -EBADMSG;\n\t\t\terr = skb_copy_bits(skb, rxm->offset + offset,\n\t\t\t\t\t    &content_type, 1);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\tif (content_type)\n\t\t\t\tbreak;\n\t\t\tsub++;\n\t\t\toffset--;\n\t\t}\n\t\ttlm->control = content_type;\n\t}\n\treturn sub;\n}\n\nstatic void tls_decrypt_done(void *data, int err)\n{\n\tstruct aead_request *aead_req = data;\n\tstruct crypto_aead *aead = crypto_aead_reqtfm(aead_req);\n\tstruct scatterlist *sgout = aead_req->dst;\n\tstruct scatterlist *sgin = aead_req->src;\n\tstruct tls_sw_context_rx *ctx;\n\tstruct tls_decrypt_ctx *dctx;\n\tstruct tls_context *tls_ctx;\n\tstruct scatterlist *sg;\n\tunsigned int pages;\n\tstruct sock *sk;\n\tint aead_size;\n\n\taead_size = sizeof(*aead_req) + crypto_aead_reqsize(aead);\n\taead_size = ALIGN(aead_size, __alignof__(*dctx));\n\tdctx = (void *)((u8 *)aead_req + aead_size);\n\n\tsk = dctx->sk;\n\ttls_ctx = tls_get_ctx(sk);\n\tctx = tls_sw_ctx_rx(tls_ctx);\n\n\t \n\tif (err) {\n\t\tif (err == -EBADMSG)\n\t\t\tTLS_INC_STATS(sock_net(sk), LINUX_MIB_TLSDECRYPTERROR);\n\t\tctx->async_wait.err = err;\n\t\ttls_err_abort(sk, err);\n\t}\n\n\t \n\tif (sgout != sgin) {\n\t\t \n\t\tfor_each_sg(sg_next(sgout), sg, UINT_MAX, pages) {\n\t\t\tif (!sg)\n\t\t\t\tbreak;\n\t\t\tput_page(sg_page(sg));\n\t\t}\n\t}\n\n\tkfree(aead_req);\n\n\tspin_lock_bh(&ctx->decrypt_compl_lock);\n\tif (!atomic_dec_return(&ctx->decrypt_pending))\n\t\tcomplete(&ctx->async_wait.completion);\n\tspin_unlock_bh(&ctx->decrypt_compl_lock);\n}\n\nstatic int tls_do_decryption(struct sock *sk,\n\t\t\t     struct scatterlist *sgin,\n\t\t\t     struct scatterlist *sgout,\n\t\t\t     char *iv_recv,\n\t\t\t     size_t data_len,\n\t\t\t     struct aead_request *aead_req,\n\t\t\t     struct tls_decrypt_arg *darg)\n{\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\tstruct tls_prot_info *prot = &tls_ctx->prot_info;\n\tstruct tls_sw_context_rx *ctx = tls_sw_ctx_rx(tls_ctx);\n\tint ret;\n\n\taead_request_set_tfm(aead_req, ctx->aead_recv);\n\taead_request_set_ad(aead_req, prot->aad_size);\n\taead_request_set_crypt(aead_req, sgin, sgout,\n\t\t\t       data_len + prot->tag_size,\n\t\t\t       (u8 *)iv_recv);\n\n\tif (darg->async) {\n\t\taead_request_set_callback(aead_req,\n\t\t\t\t\t  CRYPTO_TFM_REQ_MAY_BACKLOG,\n\t\t\t\t\t  tls_decrypt_done, aead_req);\n\t\tatomic_inc(&ctx->decrypt_pending);\n\t} else {\n\t\taead_request_set_callback(aead_req,\n\t\t\t\t\t  CRYPTO_TFM_REQ_MAY_BACKLOG,\n\t\t\t\t\t  crypto_req_done, &ctx->async_wait);\n\t}\n\n\tret = crypto_aead_decrypt(aead_req);\n\tif (ret == -EINPROGRESS) {\n\t\tif (darg->async)\n\t\t\treturn 0;\n\n\t\tret = crypto_wait_req(ret, &ctx->async_wait);\n\t}\n\tdarg->async = false;\n\n\treturn ret;\n}\n\nstatic void tls_trim_both_msgs(struct sock *sk, int target_size)\n{\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\tstruct tls_prot_info *prot = &tls_ctx->prot_info;\n\tstruct tls_sw_context_tx *ctx = tls_sw_ctx_tx(tls_ctx);\n\tstruct tls_rec *rec = ctx->open_rec;\n\n\tsk_msg_trim(sk, &rec->msg_plaintext, target_size);\n\tif (target_size > 0)\n\t\ttarget_size += prot->overhead_size;\n\tsk_msg_trim(sk, &rec->msg_encrypted, target_size);\n}\n\nstatic int tls_alloc_encrypted_msg(struct sock *sk, int len)\n{\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\tstruct tls_sw_context_tx *ctx = tls_sw_ctx_tx(tls_ctx);\n\tstruct tls_rec *rec = ctx->open_rec;\n\tstruct sk_msg *msg_en = &rec->msg_encrypted;\n\n\treturn sk_msg_alloc(sk, msg_en, len, 0);\n}\n\nstatic int tls_clone_plaintext_msg(struct sock *sk, int required)\n{\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\tstruct tls_prot_info *prot = &tls_ctx->prot_info;\n\tstruct tls_sw_context_tx *ctx = tls_sw_ctx_tx(tls_ctx);\n\tstruct tls_rec *rec = ctx->open_rec;\n\tstruct sk_msg *msg_pl = &rec->msg_plaintext;\n\tstruct sk_msg *msg_en = &rec->msg_encrypted;\n\tint skip, len;\n\n\t \n\tlen = required - msg_pl->sg.size;\n\n\t \n\tskip = prot->prepend_size + msg_pl->sg.size;\n\n\treturn sk_msg_clone(sk, msg_pl, msg_en, skip, len);\n}\n\nstatic struct tls_rec *tls_get_rec(struct sock *sk)\n{\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\tstruct tls_prot_info *prot = &tls_ctx->prot_info;\n\tstruct tls_sw_context_tx *ctx = tls_sw_ctx_tx(tls_ctx);\n\tstruct sk_msg *msg_pl, *msg_en;\n\tstruct tls_rec *rec;\n\tint mem_size;\n\n\tmem_size = sizeof(struct tls_rec) + crypto_aead_reqsize(ctx->aead_send);\n\n\trec = kzalloc(mem_size, sk->sk_allocation);\n\tif (!rec)\n\t\treturn NULL;\n\n\tmsg_pl = &rec->msg_plaintext;\n\tmsg_en = &rec->msg_encrypted;\n\n\tsk_msg_init(msg_pl);\n\tsk_msg_init(msg_en);\n\n\tsg_init_table(rec->sg_aead_in, 2);\n\tsg_set_buf(&rec->sg_aead_in[0], rec->aad_space, prot->aad_size);\n\tsg_unmark_end(&rec->sg_aead_in[1]);\n\n\tsg_init_table(rec->sg_aead_out, 2);\n\tsg_set_buf(&rec->sg_aead_out[0], rec->aad_space, prot->aad_size);\n\tsg_unmark_end(&rec->sg_aead_out[1]);\n\n\trec->sk = sk;\n\n\treturn rec;\n}\n\nstatic void tls_free_rec(struct sock *sk, struct tls_rec *rec)\n{\n\tsk_msg_free(sk, &rec->msg_encrypted);\n\tsk_msg_free(sk, &rec->msg_plaintext);\n\tkfree(rec);\n}\n\nstatic void tls_free_open_rec(struct sock *sk)\n{\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\tstruct tls_sw_context_tx *ctx = tls_sw_ctx_tx(tls_ctx);\n\tstruct tls_rec *rec = ctx->open_rec;\n\n\tif (rec) {\n\t\ttls_free_rec(sk, rec);\n\t\tctx->open_rec = NULL;\n\t}\n}\n\nint tls_tx_records(struct sock *sk, int flags)\n{\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\tstruct tls_sw_context_tx *ctx = tls_sw_ctx_tx(tls_ctx);\n\tstruct tls_rec *rec, *tmp;\n\tstruct sk_msg *msg_en;\n\tint tx_flags, rc = 0;\n\n\tif (tls_is_partially_sent_record(tls_ctx)) {\n\t\trec = list_first_entry(&ctx->tx_list,\n\t\t\t\t       struct tls_rec, list);\n\n\t\tif (flags == -1)\n\t\t\ttx_flags = rec->tx_flags;\n\t\telse\n\t\t\ttx_flags = flags;\n\n\t\trc = tls_push_partial_record(sk, tls_ctx, tx_flags);\n\t\tif (rc)\n\t\t\tgoto tx_err;\n\n\t\t \n\t\tlist_del(&rec->list);\n\t\tsk_msg_free(sk, &rec->msg_plaintext);\n\t\tkfree(rec);\n\t}\n\n\t \n\tlist_for_each_entry_safe(rec, tmp, &ctx->tx_list, list) {\n\t\tif (READ_ONCE(rec->tx_ready)) {\n\t\t\tif (flags == -1)\n\t\t\t\ttx_flags = rec->tx_flags;\n\t\t\telse\n\t\t\t\ttx_flags = flags;\n\n\t\t\tmsg_en = &rec->msg_encrypted;\n\t\t\trc = tls_push_sg(sk, tls_ctx,\n\t\t\t\t\t &msg_en->sg.data[msg_en->sg.curr],\n\t\t\t\t\t 0, tx_flags);\n\t\t\tif (rc)\n\t\t\t\tgoto tx_err;\n\n\t\t\tlist_del(&rec->list);\n\t\t\tsk_msg_free(sk, &rec->msg_plaintext);\n\t\t\tkfree(rec);\n\t\t} else {\n\t\t\tbreak;\n\t\t}\n\t}\n\ntx_err:\n\tif (rc < 0 && rc != -EAGAIN)\n\t\ttls_err_abort(sk, -EBADMSG);\n\n\treturn rc;\n}\n\nstatic void tls_encrypt_done(void *data, int err)\n{\n\tstruct tls_sw_context_tx *ctx;\n\tstruct tls_context *tls_ctx;\n\tstruct tls_prot_info *prot;\n\tstruct tls_rec *rec = data;\n\tstruct scatterlist *sge;\n\tstruct sk_msg *msg_en;\n\tbool ready = false;\n\tstruct sock *sk;\n\tint pending;\n\n\tmsg_en = &rec->msg_encrypted;\n\n\tsk = rec->sk;\n\ttls_ctx = tls_get_ctx(sk);\n\tprot = &tls_ctx->prot_info;\n\tctx = tls_sw_ctx_tx(tls_ctx);\n\n\tsge = sk_msg_elem(msg_en, msg_en->sg.curr);\n\tsge->offset -= prot->prepend_size;\n\tsge->length += prot->prepend_size;\n\n\t \n\tif (err || sk->sk_err) {\n\t\trec = NULL;\n\n\t\t \n\t\tif (sk->sk_err) {\n\t\t\tctx->async_wait.err = -sk->sk_err;\n\t\t} else {\n\t\t\tctx->async_wait.err = err;\n\t\t\ttls_err_abort(sk, err);\n\t\t}\n\t}\n\n\tif (rec) {\n\t\tstruct tls_rec *first_rec;\n\n\t\t \n\t\tsmp_store_mb(rec->tx_ready, true);\n\n\t\t \n\t\tfirst_rec = list_first_entry(&ctx->tx_list,\n\t\t\t\t\t     struct tls_rec, list);\n\t\tif (rec == first_rec)\n\t\t\tready = true;\n\t}\n\n\tspin_lock_bh(&ctx->encrypt_compl_lock);\n\tpending = atomic_dec_return(&ctx->encrypt_pending);\n\n\tif (!pending && ctx->async_notify)\n\t\tcomplete(&ctx->async_wait.completion);\n\tspin_unlock_bh(&ctx->encrypt_compl_lock);\n\n\tif (!ready)\n\t\treturn;\n\n\t \n\tif (!test_and_set_bit(BIT_TX_SCHEDULED, &ctx->tx_bitmask))\n\t\tschedule_delayed_work(&ctx->tx_work.work, 1);\n}\n\nstatic int tls_do_encryption(struct sock *sk,\n\t\t\t     struct tls_context *tls_ctx,\n\t\t\t     struct tls_sw_context_tx *ctx,\n\t\t\t     struct aead_request *aead_req,\n\t\t\t     size_t data_len, u32 start)\n{\n\tstruct tls_prot_info *prot = &tls_ctx->prot_info;\n\tstruct tls_rec *rec = ctx->open_rec;\n\tstruct sk_msg *msg_en = &rec->msg_encrypted;\n\tstruct scatterlist *sge = sk_msg_elem(msg_en, start);\n\tint rc, iv_offset = 0;\n\n\t \n\tswitch (prot->cipher_type) {\n\tcase TLS_CIPHER_AES_CCM_128:\n\t\trec->iv_data[0] = TLS_AES_CCM_IV_B0_BYTE;\n\t\tiv_offset = 1;\n\t\tbreak;\n\tcase TLS_CIPHER_SM4_CCM:\n\t\trec->iv_data[0] = TLS_SM4_CCM_IV_B0_BYTE;\n\t\tiv_offset = 1;\n\t\tbreak;\n\t}\n\n\tmemcpy(&rec->iv_data[iv_offset], tls_ctx->tx.iv,\n\t       prot->iv_size + prot->salt_size);\n\n\ttls_xor_iv_with_seq(prot, rec->iv_data + iv_offset,\n\t\t\t    tls_ctx->tx.rec_seq);\n\n\tsge->offset += prot->prepend_size;\n\tsge->length -= prot->prepend_size;\n\n\tmsg_en->sg.curr = start;\n\n\taead_request_set_tfm(aead_req, ctx->aead_send);\n\taead_request_set_ad(aead_req, prot->aad_size);\n\taead_request_set_crypt(aead_req, rec->sg_aead_in,\n\t\t\t       rec->sg_aead_out,\n\t\t\t       data_len, rec->iv_data);\n\n\taead_request_set_callback(aead_req, CRYPTO_TFM_REQ_MAY_BACKLOG,\n\t\t\t\t  tls_encrypt_done, rec);\n\n\t \n\tlist_add_tail((struct list_head *)&rec->list, &ctx->tx_list);\n\tatomic_inc(&ctx->encrypt_pending);\n\n\trc = crypto_aead_encrypt(aead_req);\n\tif (!rc || rc != -EINPROGRESS) {\n\t\tatomic_dec(&ctx->encrypt_pending);\n\t\tsge->offset -= prot->prepend_size;\n\t\tsge->length += prot->prepend_size;\n\t}\n\n\tif (!rc) {\n\t\tWRITE_ONCE(rec->tx_ready, true);\n\t} else if (rc != -EINPROGRESS) {\n\t\tlist_del(&rec->list);\n\t\treturn rc;\n\t}\n\n\t \n\tctx->open_rec = NULL;\n\ttls_advance_record_sn(sk, prot, &tls_ctx->tx);\n\treturn rc;\n}\n\nstatic int tls_split_open_record(struct sock *sk, struct tls_rec *from,\n\t\t\t\t struct tls_rec **to, struct sk_msg *msg_opl,\n\t\t\t\t struct sk_msg *msg_oen, u32 split_point,\n\t\t\t\t u32 tx_overhead_size, u32 *orig_end)\n{\n\tu32 i, j, bytes = 0, apply = msg_opl->apply_bytes;\n\tstruct scatterlist *sge, *osge, *nsge;\n\tu32 orig_size = msg_opl->sg.size;\n\tstruct scatterlist tmp = { };\n\tstruct sk_msg *msg_npl;\n\tstruct tls_rec *new;\n\tint ret;\n\n\tnew = tls_get_rec(sk);\n\tif (!new)\n\t\treturn -ENOMEM;\n\tret = sk_msg_alloc(sk, &new->msg_encrypted, msg_opl->sg.size +\n\t\t\t   tx_overhead_size, 0);\n\tif (ret < 0) {\n\t\ttls_free_rec(sk, new);\n\t\treturn ret;\n\t}\n\n\t*orig_end = msg_opl->sg.end;\n\ti = msg_opl->sg.start;\n\tsge = sk_msg_elem(msg_opl, i);\n\twhile (apply && sge->length) {\n\t\tif (sge->length > apply) {\n\t\t\tu32 len = sge->length - apply;\n\n\t\t\tget_page(sg_page(sge));\n\t\t\tsg_set_page(&tmp, sg_page(sge), len,\n\t\t\t\t    sge->offset + apply);\n\t\t\tsge->length = apply;\n\t\t\tbytes += apply;\n\t\t\tapply = 0;\n\t\t} else {\n\t\t\tapply -= sge->length;\n\t\t\tbytes += sge->length;\n\t\t}\n\n\t\tsk_msg_iter_var_next(i);\n\t\tif (i == msg_opl->sg.end)\n\t\t\tbreak;\n\t\tsge = sk_msg_elem(msg_opl, i);\n\t}\n\n\tmsg_opl->sg.end = i;\n\tmsg_opl->sg.curr = i;\n\tmsg_opl->sg.copybreak = 0;\n\tmsg_opl->apply_bytes = 0;\n\tmsg_opl->sg.size = bytes;\n\n\tmsg_npl = &new->msg_plaintext;\n\tmsg_npl->apply_bytes = apply;\n\tmsg_npl->sg.size = orig_size - bytes;\n\n\tj = msg_npl->sg.start;\n\tnsge = sk_msg_elem(msg_npl, j);\n\tif (tmp.length) {\n\t\tmemcpy(nsge, &tmp, sizeof(*nsge));\n\t\tsk_msg_iter_var_next(j);\n\t\tnsge = sk_msg_elem(msg_npl, j);\n\t}\n\n\tosge = sk_msg_elem(msg_opl, i);\n\twhile (osge->length) {\n\t\tmemcpy(nsge, osge, sizeof(*nsge));\n\t\tsg_unmark_end(nsge);\n\t\tsk_msg_iter_var_next(i);\n\t\tsk_msg_iter_var_next(j);\n\t\tif (i == *orig_end)\n\t\t\tbreak;\n\t\tosge = sk_msg_elem(msg_opl, i);\n\t\tnsge = sk_msg_elem(msg_npl, j);\n\t}\n\n\tmsg_npl->sg.end = j;\n\tmsg_npl->sg.curr = j;\n\tmsg_npl->sg.copybreak = 0;\n\n\t*to = new;\n\treturn 0;\n}\n\nstatic void tls_merge_open_record(struct sock *sk, struct tls_rec *to,\n\t\t\t\t  struct tls_rec *from, u32 orig_end)\n{\n\tstruct sk_msg *msg_npl = &from->msg_plaintext;\n\tstruct sk_msg *msg_opl = &to->msg_plaintext;\n\tstruct scatterlist *osge, *nsge;\n\tu32 i, j;\n\n\ti = msg_opl->sg.end;\n\tsk_msg_iter_var_prev(i);\n\tj = msg_npl->sg.start;\n\n\tosge = sk_msg_elem(msg_opl, i);\n\tnsge = sk_msg_elem(msg_npl, j);\n\n\tif (sg_page(osge) == sg_page(nsge) &&\n\t    osge->offset + osge->length == nsge->offset) {\n\t\tosge->length += nsge->length;\n\t\tput_page(sg_page(nsge));\n\t}\n\n\tmsg_opl->sg.end = orig_end;\n\tmsg_opl->sg.curr = orig_end;\n\tmsg_opl->sg.copybreak = 0;\n\tmsg_opl->apply_bytes = msg_opl->sg.size + msg_npl->sg.size;\n\tmsg_opl->sg.size += msg_npl->sg.size;\n\n\tsk_msg_free(sk, &to->msg_encrypted);\n\tsk_msg_xfer_full(&to->msg_encrypted, &from->msg_encrypted);\n\n\tkfree(from);\n}\n\nstatic int tls_push_record(struct sock *sk, int flags,\n\t\t\t   unsigned char record_type)\n{\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\tstruct tls_prot_info *prot = &tls_ctx->prot_info;\n\tstruct tls_sw_context_tx *ctx = tls_sw_ctx_tx(tls_ctx);\n\tstruct tls_rec *rec = ctx->open_rec, *tmp = NULL;\n\tu32 i, split_point, orig_end;\n\tstruct sk_msg *msg_pl, *msg_en;\n\tstruct aead_request *req;\n\tbool split;\n\tint rc;\n\n\tif (!rec)\n\t\treturn 0;\n\n\tmsg_pl = &rec->msg_plaintext;\n\tmsg_en = &rec->msg_encrypted;\n\n\tsplit_point = msg_pl->apply_bytes;\n\tsplit = split_point && split_point < msg_pl->sg.size;\n\tif (unlikely((!split &&\n\t\t      msg_pl->sg.size +\n\t\t      prot->overhead_size > msg_en->sg.size) ||\n\t\t     (split &&\n\t\t      split_point +\n\t\t      prot->overhead_size > msg_en->sg.size))) {\n\t\tsplit = true;\n\t\tsplit_point = msg_en->sg.size;\n\t}\n\tif (split) {\n\t\trc = tls_split_open_record(sk, rec, &tmp, msg_pl, msg_en,\n\t\t\t\t\t   split_point, prot->overhead_size,\n\t\t\t\t\t   &orig_end);\n\t\tif (rc < 0)\n\t\t\treturn rc;\n\t\t \n\t\tif (!msg_pl->sg.size) {\n\t\t\ttls_merge_open_record(sk, rec, tmp, orig_end);\n\t\t\tmsg_pl = &rec->msg_plaintext;\n\t\t\tmsg_en = &rec->msg_encrypted;\n\t\t\tsplit = false;\n\t\t}\n\t\tsk_msg_trim(sk, msg_en, msg_pl->sg.size +\n\t\t\t    prot->overhead_size);\n\t}\n\n\trec->tx_flags = flags;\n\treq = &rec->aead_req;\n\n\ti = msg_pl->sg.end;\n\tsk_msg_iter_var_prev(i);\n\n\trec->content_type = record_type;\n\tif (prot->version == TLS_1_3_VERSION) {\n\t\t \n\t\tsg_set_buf(&rec->sg_content_type, &rec->content_type, 1);\n\t\tsg_mark_end(&rec->sg_content_type);\n\t\tsg_chain(msg_pl->sg.data, msg_pl->sg.end + 1,\n\t\t\t &rec->sg_content_type);\n\t} else {\n\t\tsg_mark_end(sk_msg_elem(msg_pl, i));\n\t}\n\n\tif (msg_pl->sg.end < msg_pl->sg.start) {\n\t\tsg_chain(&msg_pl->sg.data[msg_pl->sg.start],\n\t\t\t MAX_SKB_FRAGS - msg_pl->sg.start + 1,\n\t\t\t msg_pl->sg.data);\n\t}\n\n\ti = msg_pl->sg.start;\n\tsg_chain(rec->sg_aead_in, 2, &msg_pl->sg.data[i]);\n\n\ti = msg_en->sg.end;\n\tsk_msg_iter_var_prev(i);\n\tsg_mark_end(sk_msg_elem(msg_en, i));\n\n\ti = msg_en->sg.start;\n\tsg_chain(rec->sg_aead_out, 2, &msg_en->sg.data[i]);\n\n\ttls_make_aad(rec->aad_space, msg_pl->sg.size + prot->tail_size,\n\t\t     tls_ctx->tx.rec_seq, record_type, prot);\n\n\ttls_fill_prepend(tls_ctx,\n\t\t\t page_address(sg_page(&msg_en->sg.data[i])) +\n\t\t\t msg_en->sg.data[i].offset,\n\t\t\t msg_pl->sg.size + prot->tail_size,\n\t\t\t record_type);\n\n\ttls_ctx->pending_open_record_frags = false;\n\n\trc = tls_do_encryption(sk, tls_ctx, ctx, req,\n\t\t\t       msg_pl->sg.size + prot->tail_size, i);\n\tif (rc < 0) {\n\t\tif (rc != -EINPROGRESS) {\n\t\t\ttls_err_abort(sk, -EBADMSG);\n\t\t\tif (split) {\n\t\t\t\ttls_ctx->pending_open_record_frags = true;\n\t\t\t\ttls_merge_open_record(sk, rec, tmp, orig_end);\n\t\t\t}\n\t\t}\n\t\tctx->async_capable = 1;\n\t\treturn rc;\n\t} else if (split) {\n\t\tmsg_pl = &tmp->msg_plaintext;\n\t\tmsg_en = &tmp->msg_encrypted;\n\t\tsk_msg_trim(sk, msg_en, msg_pl->sg.size + prot->overhead_size);\n\t\ttls_ctx->pending_open_record_frags = true;\n\t\tctx->open_rec = tmp;\n\t}\n\n\treturn tls_tx_records(sk, flags);\n}\n\nstatic int bpf_exec_tx_verdict(struct sk_msg *msg, struct sock *sk,\n\t\t\t       bool full_record, u8 record_type,\n\t\t\t       ssize_t *copied, int flags)\n{\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\tstruct tls_sw_context_tx *ctx = tls_sw_ctx_tx(tls_ctx);\n\tstruct sk_msg msg_redir = { };\n\tstruct sk_psock *psock;\n\tstruct sock *sk_redir;\n\tstruct tls_rec *rec;\n\tbool enospc, policy, redir_ingress;\n\tint err = 0, send;\n\tu32 delta = 0;\n\n\tpolicy = !(flags & MSG_SENDPAGE_NOPOLICY);\n\tpsock = sk_psock_get(sk);\n\tif (!psock || !policy) {\n\t\terr = tls_push_record(sk, flags, record_type);\n\t\tif (err && err != -EINPROGRESS && sk->sk_err == EBADMSG) {\n\t\t\t*copied -= sk_msg_free(sk, msg);\n\t\t\ttls_free_open_rec(sk);\n\t\t\terr = -sk->sk_err;\n\t\t}\n\t\tif (psock)\n\t\t\tsk_psock_put(sk, psock);\n\t\treturn err;\n\t}\nmore_data:\n\tenospc = sk_msg_full(msg);\n\tif (psock->eval == __SK_NONE) {\n\t\tdelta = msg->sg.size;\n\t\tpsock->eval = sk_psock_msg_verdict(sk, psock, msg);\n\t\tdelta -= msg->sg.size;\n\t}\n\tif (msg->cork_bytes && msg->cork_bytes > msg->sg.size &&\n\t    !enospc && !full_record) {\n\t\terr = -ENOSPC;\n\t\tgoto out_err;\n\t}\n\tmsg->cork_bytes = 0;\n\tsend = msg->sg.size;\n\tif (msg->apply_bytes && msg->apply_bytes < send)\n\t\tsend = msg->apply_bytes;\n\n\tswitch (psock->eval) {\n\tcase __SK_PASS:\n\t\terr = tls_push_record(sk, flags, record_type);\n\t\tif (err && err != -EINPROGRESS && sk->sk_err == EBADMSG) {\n\t\t\t*copied -= sk_msg_free(sk, msg);\n\t\t\ttls_free_open_rec(sk);\n\t\t\terr = -sk->sk_err;\n\t\t\tgoto out_err;\n\t\t}\n\t\tbreak;\n\tcase __SK_REDIRECT:\n\t\tredir_ingress = psock->redir_ingress;\n\t\tsk_redir = psock->sk_redir;\n\t\tmemcpy(&msg_redir, msg, sizeof(*msg));\n\t\tif (msg->apply_bytes < send)\n\t\t\tmsg->apply_bytes = 0;\n\t\telse\n\t\t\tmsg->apply_bytes -= send;\n\t\tsk_msg_return_zero(sk, msg, send);\n\t\tmsg->sg.size -= send;\n\t\trelease_sock(sk);\n\t\terr = tcp_bpf_sendmsg_redir(sk_redir, redir_ingress,\n\t\t\t\t\t    &msg_redir, send, flags);\n\t\tlock_sock(sk);\n\t\tif (err < 0) {\n\t\t\t*copied -= sk_msg_free_nocharge(sk, &msg_redir);\n\t\t\tmsg->sg.size = 0;\n\t\t}\n\t\tif (msg->sg.size == 0)\n\t\t\ttls_free_open_rec(sk);\n\t\tbreak;\n\tcase __SK_DROP:\n\tdefault:\n\t\tsk_msg_free_partial(sk, msg, send);\n\t\tif (msg->apply_bytes < send)\n\t\t\tmsg->apply_bytes = 0;\n\t\telse\n\t\t\tmsg->apply_bytes -= send;\n\t\tif (msg->sg.size == 0)\n\t\t\ttls_free_open_rec(sk);\n\t\t*copied -= (send + delta);\n\t\terr = -EACCES;\n\t}\n\n\tif (likely(!err)) {\n\t\tbool reset_eval = !ctx->open_rec;\n\n\t\trec = ctx->open_rec;\n\t\tif (rec) {\n\t\t\tmsg = &rec->msg_plaintext;\n\t\t\tif (!msg->apply_bytes)\n\t\t\t\treset_eval = true;\n\t\t}\n\t\tif (reset_eval) {\n\t\t\tpsock->eval = __SK_NONE;\n\t\t\tif (psock->sk_redir) {\n\t\t\t\tsock_put(psock->sk_redir);\n\t\t\t\tpsock->sk_redir = NULL;\n\t\t\t}\n\t\t}\n\t\tif (rec)\n\t\t\tgoto more_data;\n\t}\n out_err:\n\tsk_psock_put(sk, psock);\n\treturn err;\n}\n\nstatic int tls_sw_push_pending_record(struct sock *sk, int flags)\n{\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\tstruct tls_sw_context_tx *ctx = tls_sw_ctx_tx(tls_ctx);\n\tstruct tls_rec *rec = ctx->open_rec;\n\tstruct sk_msg *msg_pl;\n\tsize_t copied;\n\n\tif (!rec)\n\t\treturn 0;\n\n\tmsg_pl = &rec->msg_plaintext;\n\tcopied = msg_pl->sg.size;\n\tif (!copied)\n\t\treturn 0;\n\n\treturn bpf_exec_tx_verdict(msg_pl, sk, true, TLS_RECORD_TYPE_DATA,\n\t\t\t\t   &copied, flags);\n}\n\nstatic int tls_sw_sendmsg_splice(struct sock *sk, struct msghdr *msg,\n\t\t\t\t struct sk_msg *msg_pl, size_t try_to_copy,\n\t\t\t\t ssize_t *copied)\n{\n\tstruct page *page = NULL, **pages = &page;\n\n\tdo {\n\t\tssize_t part;\n\t\tsize_t off;\n\n\t\tpart = iov_iter_extract_pages(&msg->msg_iter, &pages,\n\t\t\t\t\t      try_to_copy, 1, 0, &off);\n\t\tif (part <= 0)\n\t\t\treturn part ?: -EIO;\n\n\t\tif (WARN_ON_ONCE(!sendpage_ok(page))) {\n\t\t\tiov_iter_revert(&msg->msg_iter, part);\n\t\t\treturn -EIO;\n\t\t}\n\n\t\tsk_msg_page_add(msg_pl, page, part, off);\n\t\tmsg_pl->sg.copybreak = 0;\n\t\tmsg_pl->sg.curr = msg_pl->sg.end;\n\t\tsk_mem_charge(sk, part);\n\t\t*copied += part;\n\t\ttry_to_copy -= part;\n\t} while (try_to_copy && !sk_msg_full(msg_pl));\n\n\treturn 0;\n}\n\nstatic int tls_sw_sendmsg_locked(struct sock *sk, struct msghdr *msg,\n\t\t\t\t size_t size)\n{\n\tlong timeo = sock_sndtimeo(sk, msg->msg_flags & MSG_DONTWAIT);\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\tstruct tls_prot_info *prot = &tls_ctx->prot_info;\n\tstruct tls_sw_context_tx *ctx = tls_sw_ctx_tx(tls_ctx);\n\tbool async_capable = ctx->async_capable;\n\tunsigned char record_type = TLS_RECORD_TYPE_DATA;\n\tbool is_kvec = iov_iter_is_kvec(&msg->msg_iter);\n\tbool eor = !(msg->msg_flags & MSG_MORE);\n\tsize_t try_to_copy;\n\tssize_t copied = 0;\n\tstruct sk_msg *msg_pl, *msg_en;\n\tstruct tls_rec *rec;\n\tint required_size;\n\tint num_async = 0;\n\tbool full_record;\n\tint record_room;\n\tint num_zc = 0;\n\tint orig_size;\n\tint ret = 0;\n\tint pending;\n\n\tif (!eor && (msg->msg_flags & MSG_EOR))\n\t\treturn -EINVAL;\n\n\tif (unlikely(msg->msg_controllen)) {\n\t\tret = tls_process_cmsg(sk, msg, &record_type);\n\t\tif (ret) {\n\t\t\tif (ret == -EINPROGRESS)\n\t\t\t\tnum_async++;\n\t\t\telse if (ret != -EAGAIN)\n\t\t\t\tgoto send_end;\n\t\t}\n\t}\n\n\twhile (msg_data_left(msg)) {\n\t\tif (sk->sk_err) {\n\t\t\tret = -sk->sk_err;\n\t\t\tgoto send_end;\n\t\t}\n\n\t\tif (ctx->open_rec)\n\t\t\trec = ctx->open_rec;\n\t\telse\n\t\t\trec = ctx->open_rec = tls_get_rec(sk);\n\t\tif (!rec) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto send_end;\n\t\t}\n\n\t\tmsg_pl = &rec->msg_plaintext;\n\t\tmsg_en = &rec->msg_encrypted;\n\n\t\torig_size = msg_pl->sg.size;\n\t\tfull_record = false;\n\t\ttry_to_copy = msg_data_left(msg);\n\t\trecord_room = TLS_MAX_PAYLOAD_SIZE - msg_pl->sg.size;\n\t\tif (try_to_copy >= record_room) {\n\t\t\ttry_to_copy = record_room;\n\t\t\tfull_record = true;\n\t\t}\n\n\t\trequired_size = msg_pl->sg.size + try_to_copy +\n\t\t\t\tprot->overhead_size;\n\n\t\tif (!sk_stream_memory_free(sk))\n\t\t\tgoto wait_for_sndbuf;\n\nalloc_encrypted:\n\t\tret = tls_alloc_encrypted_msg(sk, required_size);\n\t\tif (ret) {\n\t\t\tif (ret != -ENOSPC)\n\t\t\t\tgoto wait_for_memory;\n\n\t\t\t \n\t\t\ttry_to_copy -= required_size - msg_en->sg.size;\n\t\t\tfull_record = true;\n\t\t}\n\n\t\tif (try_to_copy && (msg->msg_flags & MSG_SPLICE_PAGES)) {\n\t\t\tret = tls_sw_sendmsg_splice(sk, msg, msg_pl,\n\t\t\t\t\t\t    try_to_copy, &copied);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto send_end;\n\t\t\ttls_ctx->pending_open_record_frags = true;\n\n\t\t\tif (sk_msg_full(msg_pl))\n\t\t\t\tfull_record = true;\n\n\t\t\tif (full_record || eor)\n\t\t\t\tgoto copied;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!is_kvec && (full_record || eor) && !async_capable) {\n\t\t\tu32 first = msg_pl->sg.end;\n\n\t\t\tret = sk_msg_zerocopy_from_iter(sk, &msg->msg_iter,\n\t\t\t\t\t\t\tmsg_pl, try_to_copy);\n\t\t\tif (ret)\n\t\t\t\tgoto fallback_to_reg_send;\n\n\t\t\tnum_zc++;\n\t\t\tcopied += try_to_copy;\n\n\t\t\tsk_msg_sg_copy_set(msg_pl, first);\n\t\t\tret = bpf_exec_tx_verdict(msg_pl, sk, full_record,\n\t\t\t\t\t\t  record_type, &copied,\n\t\t\t\t\t\t  msg->msg_flags);\n\t\t\tif (ret) {\n\t\t\t\tif (ret == -EINPROGRESS)\n\t\t\t\t\tnum_async++;\n\t\t\t\telse if (ret == -ENOMEM)\n\t\t\t\t\tgoto wait_for_memory;\n\t\t\t\telse if (ctx->open_rec && ret == -ENOSPC)\n\t\t\t\t\tgoto rollback_iter;\n\t\t\t\telse if (ret != -EAGAIN)\n\t\t\t\t\tgoto send_end;\n\t\t\t}\n\t\t\tcontinue;\nrollback_iter:\n\t\t\tcopied -= try_to_copy;\n\t\t\tsk_msg_sg_copy_clear(msg_pl, first);\n\t\t\tiov_iter_revert(&msg->msg_iter,\n\t\t\t\t\tmsg_pl->sg.size - orig_size);\nfallback_to_reg_send:\n\t\t\tsk_msg_trim(sk, msg_pl, orig_size);\n\t\t}\n\n\t\trequired_size = msg_pl->sg.size + try_to_copy;\n\n\t\tret = tls_clone_plaintext_msg(sk, required_size);\n\t\tif (ret) {\n\t\t\tif (ret != -ENOSPC)\n\t\t\t\tgoto send_end;\n\n\t\t\t \n\t\t\ttry_to_copy -= required_size - msg_pl->sg.size;\n\t\t\tfull_record = true;\n\t\t\tsk_msg_trim(sk, msg_en,\n\t\t\t\t    msg_pl->sg.size + prot->overhead_size);\n\t\t}\n\n\t\tif (try_to_copy) {\n\t\t\tret = sk_msg_memcopy_from_iter(sk, &msg->msg_iter,\n\t\t\t\t\t\t       msg_pl, try_to_copy);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto trim_sgl;\n\t\t}\n\n\t\t \n\t\ttls_ctx->pending_open_record_frags = true;\n\t\tcopied += try_to_copy;\ncopied:\n\t\tif (full_record || eor) {\n\t\t\tret = bpf_exec_tx_verdict(msg_pl, sk, full_record,\n\t\t\t\t\t\t  record_type, &copied,\n\t\t\t\t\t\t  msg->msg_flags);\n\t\t\tif (ret) {\n\t\t\t\tif (ret == -EINPROGRESS)\n\t\t\t\t\tnum_async++;\n\t\t\t\telse if (ret == -ENOMEM)\n\t\t\t\t\tgoto wait_for_memory;\n\t\t\t\telse if (ret != -EAGAIN) {\n\t\t\t\t\tif (ret == -ENOSPC)\n\t\t\t\t\t\tret = 0;\n\t\t\t\t\tgoto send_end;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tcontinue;\n\nwait_for_sndbuf:\n\t\tset_bit(SOCK_NOSPACE, &sk->sk_socket->flags);\nwait_for_memory:\n\t\tret = sk_stream_wait_memory(sk, &timeo);\n\t\tif (ret) {\ntrim_sgl:\n\t\t\tif (ctx->open_rec)\n\t\t\t\ttls_trim_both_msgs(sk, orig_size);\n\t\t\tgoto send_end;\n\t\t}\n\n\t\tif (ctx->open_rec && msg_en->sg.size < required_size)\n\t\t\tgoto alloc_encrypted;\n\t}\n\n\tif (!num_async) {\n\t\tgoto send_end;\n\t} else if (num_zc) {\n\t\t \n\t\tspin_lock_bh(&ctx->encrypt_compl_lock);\n\t\tctx->async_notify = true;\n\n\t\tpending = atomic_read(&ctx->encrypt_pending);\n\t\tspin_unlock_bh(&ctx->encrypt_compl_lock);\n\t\tif (pending)\n\t\t\tcrypto_wait_req(-EINPROGRESS, &ctx->async_wait);\n\t\telse\n\t\t\treinit_completion(&ctx->async_wait.completion);\n\n\t\t \n\t\tWRITE_ONCE(ctx->async_notify, false);\n\n\t\tif (ctx->async_wait.err) {\n\t\t\tret = ctx->async_wait.err;\n\t\t\tcopied = 0;\n\t\t}\n\t}\n\n\t \n\tif (test_and_clear_bit(BIT_TX_SCHEDULED, &ctx->tx_bitmask)) {\n\t\tcancel_delayed_work(&ctx->tx_work.work);\n\t\ttls_tx_records(sk, msg->msg_flags);\n\t}\n\nsend_end:\n\tret = sk_stream_error(sk, msg->msg_flags, ret);\n\treturn copied > 0 ? copied : ret;\n}\n\nint tls_sw_sendmsg(struct sock *sk, struct msghdr *msg, size_t size)\n{\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\tint ret;\n\n\tif (msg->msg_flags & ~(MSG_MORE | MSG_DONTWAIT | MSG_NOSIGNAL |\n\t\t\t       MSG_CMSG_COMPAT | MSG_SPLICE_PAGES | MSG_EOR |\n\t\t\t       MSG_SENDPAGE_NOPOLICY))\n\t\treturn -EOPNOTSUPP;\n\n\tret = mutex_lock_interruptible(&tls_ctx->tx_lock);\n\tif (ret)\n\t\treturn ret;\n\tlock_sock(sk);\n\tret = tls_sw_sendmsg_locked(sk, msg, size);\n\trelease_sock(sk);\n\tmutex_unlock(&tls_ctx->tx_lock);\n\treturn ret;\n}\n\n \nvoid tls_sw_splice_eof(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\tstruct tls_sw_context_tx *ctx = tls_sw_ctx_tx(tls_ctx);\n\tstruct tls_rec *rec;\n\tstruct sk_msg *msg_pl;\n\tssize_t copied = 0;\n\tbool retrying = false;\n\tint ret = 0;\n\tint pending;\n\n\tif (!ctx->open_rec)\n\t\treturn;\n\n\tmutex_lock(&tls_ctx->tx_lock);\n\tlock_sock(sk);\n\nretry:\n\t \n\trec = ctx->open_rec;\n\tif (!rec)\n\t\tgoto unlock;\n\n\tmsg_pl = &rec->msg_plaintext;\n\tif (msg_pl->sg.size == 0)\n\t\tgoto unlock;\n\n\t \n\tret = bpf_exec_tx_verdict(msg_pl, sk, false, TLS_RECORD_TYPE_DATA,\n\t\t\t\t  &copied, 0);\n\tswitch (ret) {\n\tcase 0:\n\tcase -EAGAIN:\n\t\tif (retrying)\n\t\t\tgoto unlock;\n\t\tretrying = true;\n\t\tgoto retry;\n\tcase -EINPROGRESS:\n\t\tbreak;\n\tdefault:\n\t\tgoto unlock;\n\t}\n\n\t \n\tspin_lock_bh(&ctx->encrypt_compl_lock);\n\tctx->async_notify = true;\n\n\tpending = atomic_read(&ctx->encrypt_pending);\n\tspin_unlock_bh(&ctx->encrypt_compl_lock);\n\tif (pending)\n\t\tcrypto_wait_req(-EINPROGRESS, &ctx->async_wait);\n\telse\n\t\treinit_completion(&ctx->async_wait.completion);\n\n\t \n\tWRITE_ONCE(ctx->async_notify, false);\n\n\tif (ctx->async_wait.err)\n\t\tgoto unlock;\n\n\t \n\tif (test_and_clear_bit(BIT_TX_SCHEDULED, &ctx->tx_bitmask)) {\n\t\tcancel_delayed_work(&ctx->tx_work.work);\n\t\ttls_tx_records(sk, 0);\n\t}\n\nunlock:\n\trelease_sock(sk);\n\tmutex_unlock(&tls_ctx->tx_lock);\n}\n\nstatic int\ntls_rx_rec_wait(struct sock *sk, struct sk_psock *psock, bool nonblock,\n\t\tbool released)\n{\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\tstruct tls_sw_context_rx *ctx = tls_sw_ctx_rx(tls_ctx);\n\tDEFINE_WAIT_FUNC(wait, woken_wake_function);\n\tint ret = 0;\n\tlong timeo;\n\n\ttimeo = sock_rcvtimeo(sk, nonblock);\n\n\twhile (!tls_strp_msg_ready(ctx)) {\n\t\tif (!sk_psock_queue_empty(psock))\n\t\t\treturn 0;\n\n\t\tif (sk->sk_err)\n\t\t\treturn sock_error(sk);\n\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tif (!skb_queue_empty(&sk->sk_receive_queue)) {\n\t\t\ttls_strp_check_rcv(&ctx->strp);\n\t\t\tif (tls_strp_msg_ready(ctx))\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\treturn 0;\n\n\t\tif (sock_flag(sk, SOCK_DONE))\n\t\t\treturn 0;\n\n\t\tif (!timeo)\n\t\t\treturn -EAGAIN;\n\n\t\treleased = true;\n\t\tadd_wait_queue(sk_sleep(sk), &wait);\n\t\tsk_set_bit(SOCKWQ_ASYNC_WAITDATA, sk);\n\t\tret = sk_wait_event(sk, &timeo,\n\t\t\t\t    tls_strp_msg_ready(ctx) ||\n\t\t\t\t    !sk_psock_queue_empty(psock),\n\t\t\t\t    &wait);\n\t\tsk_clear_bit(SOCKWQ_ASYNC_WAITDATA, sk);\n\t\tremove_wait_queue(sk_sleep(sk), &wait);\n\n\t\t \n\t\tif (signal_pending(current))\n\t\t\treturn sock_intr_errno(timeo);\n\t}\n\n\ttls_strp_msg_load(&ctx->strp, released);\n\n\treturn 1;\n}\n\nstatic int tls_setup_from_iter(struct iov_iter *from,\n\t\t\t       int length, int *pages_used,\n\t\t\t       struct scatterlist *to,\n\t\t\t       int to_max_pages)\n{\n\tint rc = 0, i = 0, num_elem = *pages_used, maxpages;\n\tstruct page *pages[MAX_SKB_FRAGS];\n\tunsigned int size = 0;\n\tssize_t copied, use;\n\tsize_t offset;\n\n\twhile (length > 0) {\n\t\ti = 0;\n\t\tmaxpages = to_max_pages - num_elem;\n\t\tif (maxpages == 0) {\n\t\t\trc = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t\tcopied = iov_iter_get_pages2(from, pages,\n\t\t\t\t\t    length,\n\t\t\t\t\t    maxpages, &offset);\n\t\tif (copied <= 0) {\n\t\t\trc = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\tlength -= copied;\n\t\tsize += copied;\n\t\twhile (copied) {\n\t\t\tuse = min_t(int, copied, PAGE_SIZE - offset);\n\n\t\t\tsg_set_page(&to[num_elem],\n\t\t\t\t    pages[i], use, offset);\n\t\t\tsg_unmark_end(&to[num_elem]);\n\t\t\t \n\n\t\t\toffset = 0;\n\t\t\tcopied -= use;\n\n\t\t\ti++;\n\t\t\tnum_elem++;\n\t\t}\n\t}\n\t \n\tif (num_elem > *pages_used)\n\t\tsg_mark_end(&to[num_elem - 1]);\nout:\n\tif (rc)\n\t\tiov_iter_revert(from, size);\n\t*pages_used = num_elem;\n\n\treturn rc;\n}\n\nstatic struct sk_buff *\ntls_alloc_clrtxt_skb(struct sock *sk, struct sk_buff *skb,\n\t\t     unsigned int full_len)\n{\n\tstruct strp_msg *clr_rxm;\n\tstruct sk_buff *clr_skb;\n\tint err;\n\n\tclr_skb = alloc_skb_with_frags(0, full_len, TLS_PAGE_ORDER,\n\t\t\t\t       &err, sk->sk_allocation);\n\tif (!clr_skb)\n\t\treturn NULL;\n\n\tskb_copy_header(clr_skb, skb);\n\tclr_skb->len = full_len;\n\tclr_skb->data_len = full_len;\n\n\tclr_rxm = strp_msg(clr_skb);\n\tclr_rxm->offset = 0;\n\n\treturn clr_skb;\n}\n\n \n\n \nstatic int tls_decrypt_sg(struct sock *sk, struct iov_iter *out_iov,\n\t\t\t  struct scatterlist *out_sg,\n\t\t\t  struct tls_decrypt_arg *darg)\n{\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\tstruct tls_sw_context_rx *ctx = tls_sw_ctx_rx(tls_ctx);\n\tstruct tls_prot_info *prot = &tls_ctx->prot_info;\n\tint n_sgin, n_sgout, aead_size, err, pages = 0;\n\tstruct sk_buff *skb = tls_strp_msg(ctx);\n\tconst struct strp_msg *rxm = strp_msg(skb);\n\tconst struct tls_msg *tlm = tls_msg(skb);\n\tstruct aead_request *aead_req;\n\tstruct scatterlist *sgin = NULL;\n\tstruct scatterlist *sgout = NULL;\n\tconst int data_len = rxm->full_len - prot->overhead_size;\n\tint tail_pages = !!prot->tail_size;\n\tstruct tls_decrypt_ctx *dctx;\n\tstruct sk_buff *clear_skb;\n\tint iv_offset = 0;\n\tu8 *mem;\n\n\tn_sgin = skb_nsg(skb, rxm->offset + prot->prepend_size,\n\t\t\t rxm->full_len - prot->prepend_size);\n\tif (n_sgin < 1)\n\t\treturn n_sgin ?: -EBADMSG;\n\n\tif (darg->zc && (out_iov || out_sg)) {\n\t\tclear_skb = NULL;\n\n\t\tif (out_iov)\n\t\t\tn_sgout = 1 + tail_pages +\n\t\t\t\tiov_iter_npages_cap(out_iov, INT_MAX, data_len);\n\t\telse\n\t\t\tn_sgout = sg_nents(out_sg);\n\t} else {\n\t\tdarg->zc = false;\n\n\t\tclear_skb = tls_alloc_clrtxt_skb(sk, skb, rxm->full_len);\n\t\tif (!clear_skb)\n\t\t\treturn -ENOMEM;\n\n\t\tn_sgout = 1 + skb_shinfo(clear_skb)->nr_frags;\n\t}\n\n\t \n\tn_sgin = n_sgin + 1;\n\n\t \n\taead_size = sizeof(*aead_req) + crypto_aead_reqsize(ctx->aead_recv);\n\taead_size = ALIGN(aead_size, __alignof__(*dctx));\n\tmem = kmalloc(aead_size + struct_size(dctx, sg, size_add(n_sgin, n_sgout)),\n\t\t      sk->sk_allocation);\n\tif (!mem) {\n\t\terr = -ENOMEM;\n\t\tgoto exit_free_skb;\n\t}\n\n\t \n\taead_req = (struct aead_request *)mem;\n\tdctx = (struct tls_decrypt_ctx *)(mem + aead_size);\n\tdctx->sk = sk;\n\tsgin = &dctx->sg[0];\n\tsgout = &dctx->sg[n_sgin];\n\n\t \n\tswitch (prot->cipher_type) {\n\tcase TLS_CIPHER_AES_CCM_128:\n\t\tdctx->iv[0] = TLS_AES_CCM_IV_B0_BYTE;\n\t\tiv_offset = 1;\n\t\tbreak;\n\tcase TLS_CIPHER_SM4_CCM:\n\t\tdctx->iv[0] = TLS_SM4_CCM_IV_B0_BYTE;\n\t\tiv_offset = 1;\n\t\tbreak;\n\t}\n\n\t \n\tif (prot->version == TLS_1_3_VERSION ||\n\t    prot->cipher_type == TLS_CIPHER_CHACHA20_POLY1305) {\n\t\tmemcpy(&dctx->iv[iv_offset], tls_ctx->rx.iv,\n\t\t       prot->iv_size + prot->salt_size);\n\t} else {\n\t\terr = skb_copy_bits(skb, rxm->offset + TLS_HEADER_SIZE,\n\t\t\t\t    &dctx->iv[iv_offset] + prot->salt_size,\n\t\t\t\t    prot->iv_size);\n\t\tif (err < 0)\n\t\t\tgoto exit_free;\n\t\tmemcpy(&dctx->iv[iv_offset], tls_ctx->rx.iv, prot->salt_size);\n\t}\n\ttls_xor_iv_with_seq(prot, &dctx->iv[iv_offset], tls_ctx->rx.rec_seq);\n\n\t \n\ttls_make_aad(dctx->aad, rxm->full_len - prot->overhead_size +\n\t\t     prot->tail_size,\n\t\t     tls_ctx->rx.rec_seq, tlm->control, prot);\n\n\t \n\tsg_init_table(sgin, n_sgin);\n\tsg_set_buf(&sgin[0], dctx->aad, prot->aad_size);\n\terr = skb_to_sgvec(skb, &sgin[1],\n\t\t\t   rxm->offset + prot->prepend_size,\n\t\t\t   rxm->full_len - prot->prepend_size);\n\tif (err < 0)\n\t\tgoto exit_free;\n\n\tif (clear_skb) {\n\t\tsg_init_table(sgout, n_sgout);\n\t\tsg_set_buf(&sgout[0], dctx->aad, prot->aad_size);\n\n\t\terr = skb_to_sgvec(clear_skb, &sgout[1], prot->prepend_size,\n\t\t\t\t   data_len + prot->tail_size);\n\t\tif (err < 0)\n\t\t\tgoto exit_free;\n\t} else if (out_iov) {\n\t\tsg_init_table(sgout, n_sgout);\n\t\tsg_set_buf(&sgout[0], dctx->aad, prot->aad_size);\n\n\t\terr = tls_setup_from_iter(out_iov, data_len, &pages, &sgout[1],\n\t\t\t\t\t  (n_sgout - 1 - tail_pages));\n\t\tif (err < 0)\n\t\t\tgoto exit_free_pages;\n\n\t\tif (prot->tail_size) {\n\t\t\tsg_unmark_end(&sgout[pages]);\n\t\t\tsg_set_buf(&sgout[pages + 1], &dctx->tail,\n\t\t\t\t   prot->tail_size);\n\t\t\tsg_mark_end(&sgout[pages + 1]);\n\t\t}\n\t} else if (out_sg) {\n\t\tmemcpy(sgout, out_sg, n_sgout * sizeof(*sgout));\n\t}\n\n\t \n\terr = tls_do_decryption(sk, sgin, sgout, dctx->iv,\n\t\t\t\tdata_len + prot->tail_size, aead_req, darg);\n\tif (err)\n\t\tgoto exit_free_pages;\n\n\tdarg->skb = clear_skb ?: tls_strp_msg(ctx);\n\tclear_skb = NULL;\n\n\tif (unlikely(darg->async)) {\n\t\terr = tls_strp_msg_hold(&ctx->strp, &ctx->async_hold);\n\t\tif (err)\n\t\t\t__skb_queue_tail(&ctx->async_hold, darg->skb);\n\t\treturn err;\n\t}\n\n\tif (prot->tail_size)\n\t\tdarg->tail = dctx->tail;\n\nexit_free_pages:\n\t \n\tfor (; pages > 0; pages--)\n\t\tput_page(sg_page(&sgout[pages]));\nexit_free:\n\tkfree(mem);\nexit_free_skb:\n\tconsume_skb(clear_skb);\n\treturn err;\n}\n\nstatic int\ntls_decrypt_sw(struct sock *sk, struct tls_context *tls_ctx,\n\t       struct msghdr *msg, struct tls_decrypt_arg *darg)\n{\n\tstruct tls_sw_context_rx *ctx = tls_sw_ctx_rx(tls_ctx);\n\tstruct tls_prot_info *prot = &tls_ctx->prot_info;\n\tstruct strp_msg *rxm;\n\tint pad, err;\n\n\terr = tls_decrypt_sg(sk, &msg->msg_iter, NULL, darg);\n\tif (err < 0) {\n\t\tif (err == -EBADMSG)\n\t\t\tTLS_INC_STATS(sock_net(sk), LINUX_MIB_TLSDECRYPTERROR);\n\t\treturn err;\n\t}\n\t \n\n\t \n\tif (unlikely(darg->zc && prot->version == TLS_1_3_VERSION &&\n\t\t     darg->tail != TLS_RECORD_TYPE_DATA)) {\n\t\tdarg->zc = false;\n\t\tif (!darg->tail)\n\t\t\tTLS_INC_STATS(sock_net(sk), LINUX_MIB_TLSRXNOPADVIOL);\n\t\tTLS_INC_STATS(sock_net(sk), LINUX_MIB_TLSDECRYPTRETRY);\n\t\treturn tls_decrypt_sw(sk, tls_ctx, msg, darg);\n\t}\n\n\tpad = tls_padding_length(prot, darg->skb, darg);\n\tif (pad < 0) {\n\t\tif (darg->skb != tls_strp_msg(ctx))\n\t\t\tconsume_skb(darg->skb);\n\t\treturn pad;\n\t}\n\n\trxm = strp_msg(darg->skb);\n\trxm->full_len -= pad;\n\n\treturn 0;\n}\n\nstatic int\ntls_decrypt_device(struct sock *sk, struct msghdr *msg,\n\t\t   struct tls_context *tls_ctx, struct tls_decrypt_arg *darg)\n{\n\tstruct tls_sw_context_rx *ctx = tls_sw_ctx_rx(tls_ctx);\n\tstruct tls_prot_info *prot = &tls_ctx->prot_info;\n\tstruct strp_msg *rxm;\n\tint pad, err;\n\n\tif (tls_ctx->rx_conf != TLS_HW)\n\t\treturn 0;\n\n\terr = tls_device_decrypted(sk, tls_ctx);\n\tif (err <= 0)\n\t\treturn err;\n\n\tpad = tls_padding_length(prot, tls_strp_msg(ctx), darg);\n\tif (pad < 0)\n\t\treturn pad;\n\n\tdarg->async = false;\n\tdarg->skb = tls_strp_msg(ctx);\n\t \n\tdarg->zc &= !(prot->version == TLS_1_3_VERSION &&\n\t\t      tls_msg(darg->skb)->control != TLS_RECORD_TYPE_DATA);\n\n\trxm = strp_msg(darg->skb);\n\trxm->full_len -= pad;\n\n\tif (!darg->zc) {\n\t\t \n\t\tdarg->skb = tls_strp_msg_detach(ctx);\n\t\tif (!darg->skb)\n\t\t\treturn -ENOMEM;\n\t} else {\n\t\tunsigned int off, len;\n\n\t\t \n\t\toff = rxm->offset + prot->prepend_size;\n\t\tlen = rxm->full_len - prot->overhead_size;\n\n\t\terr = skb_copy_datagram_msg(darg->skb, off, msg, len);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\treturn 1;\n}\n\nstatic int tls_rx_one_record(struct sock *sk, struct msghdr *msg,\n\t\t\t     struct tls_decrypt_arg *darg)\n{\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\tstruct tls_prot_info *prot = &tls_ctx->prot_info;\n\tstruct strp_msg *rxm;\n\tint err;\n\n\terr = tls_decrypt_device(sk, msg, tls_ctx, darg);\n\tif (!err)\n\t\terr = tls_decrypt_sw(sk, tls_ctx, msg, darg);\n\tif (err < 0)\n\t\treturn err;\n\n\trxm = strp_msg(darg->skb);\n\trxm->offset += prot->prepend_size;\n\trxm->full_len -= prot->overhead_size;\n\ttls_advance_record_sn(sk, prot, &tls_ctx->rx);\n\n\treturn 0;\n}\n\nint decrypt_skb(struct sock *sk, struct scatterlist *sgout)\n{\n\tstruct tls_decrypt_arg darg = { .zc = true, };\n\n\treturn tls_decrypt_sg(sk, NULL, sgout, &darg);\n}\n\nstatic int tls_record_content_type(struct msghdr *msg, struct tls_msg *tlm,\n\t\t\t\t   u8 *control)\n{\n\tint err;\n\n\tif (!*control) {\n\t\t*control = tlm->control;\n\t\tif (!*control)\n\t\t\treturn -EBADMSG;\n\n\t\terr = put_cmsg(msg, SOL_TLS, TLS_GET_RECORD_TYPE,\n\t\t\t       sizeof(*control), control);\n\t\tif (*control != TLS_RECORD_TYPE_DATA) {\n\t\t\tif (err || msg->msg_flags & MSG_CTRUNC)\n\t\t\t\treturn -EIO;\n\t\t}\n\t} else if (*control != tlm->control) {\n\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\nstatic void tls_rx_rec_done(struct tls_sw_context_rx *ctx)\n{\n\ttls_strp_msg_done(&ctx->strp);\n}\n\n \nstatic int process_rx_list(struct tls_sw_context_rx *ctx,\n\t\t\t   struct msghdr *msg,\n\t\t\t   u8 *control,\n\t\t\t   size_t skip,\n\t\t\t   size_t len,\n\t\t\t   bool is_peek)\n{\n\tstruct sk_buff *skb = skb_peek(&ctx->rx_list);\n\tstruct tls_msg *tlm;\n\tssize_t copied = 0;\n\tint err;\n\n\twhile (skip && skb) {\n\t\tstruct strp_msg *rxm = strp_msg(skb);\n\t\ttlm = tls_msg(skb);\n\n\t\terr = tls_record_content_type(msg, tlm, control);\n\t\tif (err <= 0)\n\t\t\tgoto out;\n\n\t\tif (skip < rxm->full_len)\n\t\t\tbreak;\n\n\t\tskip = skip - rxm->full_len;\n\t\tskb = skb_peek_next(skb, &ctx->rx_list);\n\t}\n\n\twhile (len && skb) {\n\t\tstruct sk_buff *next_skb;\n\t\tstruct strp_msg *rxm = strp_msg(skb);\n\t\tint chunk = min_t(unsigned int, rxm->full_len - skip, len);\n\n\t\ttlm = tls_msg(skb);\n\n\t\terr = tls_record_content_type(msg, tlm, control);\n\t\tif (err <= 0)\n\t\t\tgoto out;\n\n\t\terr = skb_copy_datagram_msg(skb, rxm->offset + skip,\n\t\t\t\t\t    msg, chunk);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\n\t\tlen = len - chunk;\n\t\tcopied = copied + chunk;\n\n\t\t \n\t\tif (!is_peek) {\n\t\t\trxm->offset = rxm->offset + chunk;\n\t\t\trxm->full_len = rxm->full_len - chunk;\n\n\t\t\t \n\t\t\tif (rxm->full_len - skip)\n\t\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tskip = 0;\n\n\t\tif (msg)\n\t\t\tmsg->msg_flags |= MSG_EOR;\n\n\t\tnext_skb = skb_peek_next(skb, &ctx->rx_list);\n\n\t\tif (!is_peek) {\n\t\t\t__skb_unlink(skb, &ctx->rx_list);\n\t\t\tconsume_skb(skb);\n\t\t}\n\n\t\tskb = next_skb;\n\t}\n\terr = 0;\n\nout:\n\treturn copied ? : err;\n}\n\nstatic bool\ntls_read_flush_backlog(struct sock *sk, struct tls_prot_info *prot,\n\t\t       size_t len_left, size_t decrypted, ssize_t done,\n\t\t       size_t *flushed_at)\n{\n\tsize_t max_rec;\n\n\tif (len_left <= decrypted)\n\t\treturn false;\n\n\tmax_rec = prot->overhead_size - prot->tail_size + TLS_MAX_PAYLOAD_SIZE;\n\tif (done - *flushed_at < SZ_128K && tcp_inq(sk) > max_rec)\n\t\treturn false;\n\n\t*flushed_at = done;\n\treturn sk_flush_backlog(sk);\n}\n\nstatic int tls_rx_reader_acquire(struct sock *sk, struct tls_sw_context_rx *ctx,\n\t\t\t\t bool nonblock)\n{\n\tlong timeo;\n\tint ret;\n\n\ttimeo = sock_rcvtimeo(sk, nonblock);\n\n\twhile (unlikely(ctx->reader_present)) {\n\t\tDEFINE_WAIT_FUNC(wait, woken_wake_function);\n\n\t\tctx->reader_contended = 1;\n\n\t\tadd_wait_queue(&ctx->wq, &wait);\n\t\tret = sk_wait_event(sk, &timeo,\n\t\t\t\t    !READ_ONCE(ctx->reader_present), &wait);\n\t\tremove_wait_queue(&ctx->wq, &wait);\n\n\t\tif (timeo <= 0)\n\t\t\treturn -EAGAIN;\n\t\tif (signal_pending(current))\n\t\t\treturn sock_intr_errno(timeo);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\tWRITE_ONCE(ctx->reader_present, 1);\n\n\treturn 0;\n}\n\nstatic int tls_rx_reader_lock(struct sock *sk, struct tls_sw_context_rx *ctx,\n\t\t\t      bool nonblock)\n{\n\tint err;\n\n\tlock_sock(sk);\n\terr = tls_rx_reader_acquire(sk, ctx, nonblock);\n\tif (err)\n\t\trelease_sock(sk);\n\treturn err;\n}\n\nstatic void tls_rx_reader_release(struct sock *sk, struct tls_sw_context_rx *ctx)\n{\n\tif (unlikely(ctx->reader_contended)) {\n\t\tif (wq_has_sleeper(&ctx->wq))\n\t\t\twake_up(&ctx->wq);\n\t\telse\n\t\t\tctx->reader_contended = 0;\n\n\t\tWARN_ON_ONCE(!ctx->reader_present);\n\t}\n\n\tWRITE_ONCE(ctx->reader_present, 0);\n}\n\nstatic void tls_rx_reader_unlock(struct sock *sk, struct tls_sw_context_rx *ctx)\n{\n\ttls_rx_reader_release(sk, ctx);\n\trelease_sock(sk);\n}\n\nint tls_sw_recvmsg(struct sock *sk,\n\t\t   struct msghdr *msg,\n\t\t   size_t len,\n\t\t   int flags,\n\t\t   int *addr_len)\n{\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\tstruct tls_sw_context_rx *ctx = tls_sw_ctx_rx(tls_ctx);\n\tstruct tls_prot_info *prot = &tls_ctx->prot_info;\n\tssize_t decrypted = 0, async_copy_bytes = 0;\n\tstruct sk_psock *psock;\n\tunsigned char control = 0;\n\tsize_t flushed_at = 0;\n\tstruct strp_msg *rxm;\n\tstruct tls_msg *tlm;\n\tssize_t copied = 0;\n\tbool async = false;\n\tint target, err;\n\tbool is_kvec = iov_iter_is_kvec(&msg->msg_iter);\n\tbool is_peek = flags & MSG_PEEK;\n\tbool released = true;\n\tbool bpf_strp_enabled;\n\tbool zc_capable;\n\n\tif (unlikely(flags & MSG_ERRQUEUE))\n\t\treturn sock_recv_errqueue(sk, msg, len, SOL_IP, IP_RECVERR);\n\n\tpsock = sk_psock_get(sk);\n\terr = tls_rx_reader_lock(sk, ctx, flags & MSG_DONTWAIT);\n\tif (err < 0)\n\t\treturn err;\n\tbpf_strp_enabled = sk_psock_strp_enabled(psock);\n\n\t \n\terr = ctx->async_wait.err;\n\tif (err)\n\t\tgoto end;\n\n\t \n\terr = process_rx_list(ctx, msg, &control, 0, len, is_peek);\n\tif (err < 0)\n\t\tgoto end;\n\n\tcopied = err;\n\tif (len <= copied)\n\t\tgoto end;\n\n\ttarget = sock_rcvlowat(sk, flags & MSG_WAITALL, len);\n\tlen = len - copied;\n\n\tzc_capable = !bpf_strp_enabled && !is_kvec && !is_peek &&\n\t\tctx->zc_capable;\n\tdecrypted = 0;\n\twhile (len && (decrypted + copied < target || tls_strp_msg_ready(ctx))) {\n\t\tstruct tls_decrypt_arg darg;\n\t\tint to_decrypt, chunk;\n\n\t\terr = tls_rx_rec_wait(sk, psock, flags & MSG_DONTWAIT,\n\t\t\t\t      released);\n\t\tif (err <= 0) {\n\t\t\tif (psock) {\n\t\t\t\tchunk = sk_msg_recvmsg(sk, psock, msg, len,\n\t\t\t\t\t\t       flags);\n\t\t\t\tif (chunk > 0) {\n\t\t\t\t\tdecrypted += chunk;\n\t\t\t\t\tlen -= chunk;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t}\n\t\t\tgoto recv_end;\n\t\t}\n\n\t\tmemset(&darg.inargs, 0, sizeof(darg.inargs));\n\n\t\trxm = strp_msg(tls_strp_msg(ctx));\n\t\ttlm = tls_msg(tls_strp_msg(ctx));\n\n\t\tto_decrypt = rxm->full_len - prot->overhead_size;\n\n\t\tif (zc_capable && to_decrypt <= len &&\n\t\t    tlm->control == TLS_RECORD_TYPE_DATA)\n\t\t\tdarg.zc = true;\n\n\t\t \n\t\tif (tlm->control == TLS_RECORD_TYPE_DATA && !bpf_strp_enabled)\n\t\t\tdarg.async = ctx->async_capable;\n\t\telse\n\t\t\tdarg.async = false;\n\n\t\terr = tls_rx_one_record(sk, msg, &darg);\n\t\tif (err < 0) {\n\t\t\ttls_err_abort(sk, -EBADMSG);\n\t\t\tgoto recv_end;\n\t\t}\n\n\t\tasync |= darg.async;\n\n\t\t \n\t\terr = tls_record_content_type(msg, tls_msg(darg.skb), &control);\n\t\tif (err <= 0) {\n\t\t\tDEBUG_NET_WARN_ON_ONCE(darg.zc);\n\t\t\ttls_rx_rec_done(ctx);\nput_on_rx_list_err:\n\t\t\t__skb_queue_tail(&ctx->rx_list, darg.skb);\n\t\t\tgoto recv_end;\n\t\t}\n\n\t\t \n\t\treleased = tls_read_flush_backlog(sk, prot, len, to_decrypt,\n\t\t\t\t\t\t  decrypted + copied,\n\t\t\t\t\t\t  &flushed_at);\n\n\t\t \n\t\trxm = strp_msg(darg.skb);\n\t\tchunk = rxm->full_len;\n\t\ttls_rx_rec_done(ctx);\n\n\t\tif (!darg.zc) {\n\t\t\tbool partially_consumed = chunk > len;\n\t\t\tstruct sk_buff *skb = darg.skb;\n\n\t\t\tDEBUG_NET_WARN_ON_ONCE(darg.skb == ctx->strp.anchor);\n\n\t\t\tif (async) {\n\t\t\t\t \n\t\t\t\tchunk = min_t(int, to_decrypt, len);\n\t\t\t\tasync_copy_bytes += chunk;\nput_on_rx_list:\n\t\t\t\tdecrypted += chunk;\n\t\t\t\tlen -= chunk;\n\t\t\t\t__skb_queue_tail(&ctx->rx_list, skb);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (bpf_strp_enabled) {\n\t\t\t\treleased = true;\n\t\t\t\terr = sk_psock_tls_strp_read(psock, skb);\n\t\t\t\tif (err != __SK_PASS) {\n\t\t\t\t\trxm->offset = rxm->offset + rxm->full_len;\n\t\t\t\t\trxm->full_len = 0;\n\t\t\t\t\tif (err == __SK_DROP)\n\t\t\t\t\t\tconsume_skb(skb);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (partially_consumed)\n\t\t\t\tchunk = len;\n\n\t\t\terr = skb_copy_datagram_msg(skb, rxm->offset,\n\t\t\t\t\t\t    msg, chunk);\n\t\t\tif (err < 0)\n\t\t\t\tgoto put_on_rx_list_err;\n\n\t\t\tif (is_peek)\n\t\t\t\tgoto put_on_rx_list;\n\n\t\t\tif (partially_consumed) {\n\t\t\t\trxm->offset += chunk;\n\t\t\t\trxm->full_len -= chunk;\n\t\t\t\tgoto put_on_rx_list;\n\t\t\t}\n\n\t\t\tconsume_skb(skb);\n\t\t}\n\n\t\tdecrypted += chunk;\n\t\tlen -= chunk;\n\n\t\t \n\t\tmsg->msg_flags |= MSG_EOR;\n\t\tif (control != TLS_RECORD_TYPE_DATA)\n\t\t\tbreak;\n\t}\n\nrecv_end:\n\tif (async) {\n\t\tint ret, pending;\n\n\t\t \n\t\tspin_lock_bh(&ctx->decrypt_compl_lock);\n\t\treinit_completion(&ctx->async_wait.completion);\n\t\tpending = atomic_read(&ctx->decrypt_pending);\n\t\tspin_unlock_bh(&ctx->decrypt_compl_lock);\n\t\tret = 0;\n\t\tif (pending)\n\t\t\tret = crypto_wait_req(-EINPROGRESS, &ctx->async_wait);\n\t\t__skb_queue_purge(&ctx->async_hold);\n\n\t\tif (ret) {\n\t\t\tif (err >= 0 || err == -EINPROGRESS)\n\t\t\t\terr = ret;\n\t\t\tdecrypted = 0;\n\t\t\tgoto end;\n\t\t}\n\n\t\t \n\t\tif (is_peek || is_kvec)\n\t\t\terr = process_rx_list(ctx, msg, &control, copied,\n\t\t\t\t\t      decrypted, is_peek);\n\t\telse\n\t\t\terr = process_rx_list(ctx, msg, &control, 0,\n\t\t\t\t\t      async_copy_bytes, is_peek);\n\t\tdecrypted += max(err, 0);\n\t}\n\n\tcopied += decrypted;\n\nend:\n\ttls_rx_reader_unlock(sk, ctx);\n\tif (psock)\n\t\tsk_psock_put(sk, psock);\n\treturn copied ? : err;\n}\n\nssize_t tls_sw_splice_read(struct socket *sock,  loff_t *ppos,\n\t\t\t   struct pipe_inode_info *pipe,\n\t\t\t   size_t len, unsigned int flags)\n{\n\tstruct tls_context *tls_ctx = tls_get_ctx(sock->sk);\n\tstruct tls_sw_context_rx *ctx = tls_sw_ctx_rx(tls_ctx);\n\tstruct strp_msg *rxm = NULL;\n\tstruct sock *sk = sock->sk;\n\tstruct tls_msg *tlm;\n\tstruct sk_buff *skb;\n\tssize_t copied = 0;\n\tint chunk;\n\tint err;\n\n\terr = tls_rx_reader_lock(sk, ctx, flags & SPLICE_F_NONBLOCK);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (!skb_queue_empty(&ctx->rx_list)) {\n\t\tskb = __skb_dequeue(&ctx->rx_list);\n\t} else {\n\t\tstruct tls_decrypt_arg darg;\n\n\t\terr = tls_rx_rec_wait(sk, NULL, flags & SPLICE_F_NONBLOCK,\n\t\t\t\t      true);\n\t\tif (err <= 0)\n\t\t\tgoto splice_read_end;\n\n\t\tmemset(&darg.inargs, 0, sizeof(darg.inargs));\n\n\t\terr = tls_rx_one_record(sk, NULL, &darg);\n\t\tif (err < 0) {\n\t\t\ttls_err_abort(sk, -EBADMSG);\n\t\t\tgoto splice_read_end;\n\t\t}\n\n\t\ttls_rx_rec_done(ctx);\n\t\tskb = darg.skb;\n\t}\n\n\trxm = strp_msg(skb);\n\ttlm = tls_msg(skb);\n\n\t \n\tif (tlm->control != TLS_RECORD_TYPE_DATA) {\n\t\terr = -EINVAL;\n\t\tgoto splice_requeue;\n\t}\n\n\tchunk = min_t(unsigned int, rxm->full_len, len);\n\tcopied = skb_splice_bits(skb, sk, rxm->offset, pipe, chunk, flags);\n\tif (copied < 0)\n\t\tgoto splice_requeue;\n\n\tif (chunk < rxm->full_len) {\n\t\trxm->offset += len;\n\t\trxm->full_len -= len;\n\t\tgoto splice_requeue;\n\t}\n\n\tconsume_skb(skb);\n\nsplice_read_end:\n\ttls_rx_reader_unlock(sk, ctx);\n\treturn copied ? : err;\n\nsplice_requeue:\n\t__skb_queue_head(&ctx->rx_list, skb);\n\tgoto splice_read_end;\n}\n\nint tls_sw_read_sock(struct sock *sk, read_descriptor_t *desc,\n\t\t     sk_read_actor_t read_actor)\n{\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\tstruct tls_sw_context_rx *ctx = tls_sw_ctx_rx(tls_ctx);\n\tstruct tls_prot_info *prot = &tls_ctx->prot_info;\n\tstruct strp_msg *rxm = NULL;\n\tstruct sk_buff *skb = NULL;\n\tstruct sk_psock *psock;\n\tsize_t flushed_at = 0;\n\tbool released = true;\n\tstruct tls_msg *tlm;\n\tssize_t copied = 0;\n\tssize_t decrypted;\n\tint err, used;\n\n\tpsock = sk_psock_get(sk);\n\tif (psock) {\n\t\tsk_psock_put(sk, psock);\n\t\treturn -EINVAL;\n\t}\n\terr = tls_rx_reader_acquire(sk, ctx, true);\n\tif (err < 0)\n\t\treturn err;\n\n\t \n\terr = ctx->async_wait.err;\n\tif (err)\n\t\tgoto read_sock_end;\n\n\tdecrypted = 0;\n\tdo {\n\t\tif (!skb_queue_empty(&ctx->rx_list)) {\n\t\t\tskb = __skb_dequeue(&ctx->rx_list);\n\t\t\trxm = strp_msg(skb);\n\t\t\ttlm = tls_msg(skb);\n\t\t} else {\n\t\t\tstruct tls_decrypt_arg darg;\n\n\t\t\terr = tls_rx_rec_wait(sk, NULL, true, released);\n\t\t\tif (err <= 0)\n\t\t\t\tgoto read_sock_end;\n\n\t\t\tmemset(&darg.inargs, 0, sizeof(darg.inargs));\n\n\t\t\terr = tls_rx_one_record(sk, NULL, &darg);\n\t\t\tif (err < 0) {\n\t\t\t\ttls_err_abort(sk, -EBADMSG);\n\t\t\t\tgoto read_sock_end;\n\t\t\t}\n\n\t\t\treleased = tls_read_flush_backlog(sk, prot, INT_MAX,\n\t\t\t\t\t\t\t  0, decrypted,\n\t\t\t\t\t\t\t  &flushed_at);\n\t\t\tskb = darg.skb;\n\t\t\trxm = strp_msg(skb);\n\t\t\ttlm = tls_msg(skb);\n\t\t\tdecrypted += rxm->full_len;\n\n\t\t\ttls_rx_rec_done(ctx);\n\t\t}\n\n\t\t \n\t\tif (tlm->control != TLS_RECORD_TYPE_DATA) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto read_sock_requeue;\n\t\t}\n\n\t\tused = read_actor(desc, skb, rxm->offset, rxm->full_len);\n\t\tif (used <= 0) {\n\t\t\tif (!copied)\n\t\t\t\terr = used;\n\t\t\tgoto read_sock_requeue;\n\t\t}\n\t\tcopied += used;\n\t\tif (used < rxm->full_len) {\n\t\t\trxm->offset += used;\n\t\t\trxm->full_len -= used;\n\t\t\tif (!desc->count)\n\t\t\t\tgoto read_sock_requeue;\n\t\t} else {\n\t\t\tconsume_skb(skb);\n\t\t\tif (!desc->count)\n\t\t\t\tskb = NULL;\n\t\t}\n\t} while (skb);\n\nread_sock_end:\n\ttls_rx_reader_release(sk, ctx);\n\treturn copied ? : err;\n\nread_sock_requeue:\n\t__skb_queue_head(&ctx->rx_list, skb);\n\tgoto read_sock_end;\n}\n\nbool tls_sw_sock_is_readable(struct sock *sk)\n{\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\tstruct tls_sw_context_rx *ctx = tls_sw_ctx_rx(tls_ctx);\n\tbool ingress_empty = true;\n\tstruct sk_psock *psock;\n\n\trcu_read_lock();\n\tpsock = sk_psock(sk);\n\tif (psock)\n\t\tingress_empty = list_empty(&psock->ingress_msg);\n\trcu_read_unlock();\n\n\treturn !ingress_empty || tls_strp_msg_ready(ctx) ||\n\t\t!skb_queue_empty(&ctx->rx_list);\n}\n\nint tls_rx_msg_size(struct tls_strparser *strp, struct sk_buff *skb)\n{\n\tstruct tls_context *tls_ctx = tls_get_ctx(strp->sk);\n\tstruct tls_prot_info *prot = &tls_ctx->prot_info;\n\tchar header[TLS_HEADER_SIZE + MAX_IV_SIZE];\n\tsize_t cipher_overhead;\n\tsize_t data_len = 0;\n\tint ret;\n\n\t \n\tif (strp->stm.offset + prot->prepend_size > skb->len)\n\t\treturn 0;\n\n\t \n\tif (WARN_ON(prot->prepend_size > sizeof(header))) {\n\t\tret = -EINVAL;\n\t\tgoto read_failure;\n\t}\n\n\t \n\tret = skb_copy_bits(skb, strp->stm.offset, header, prot->prepend_size);\n\tif (ret < 0)\n\t\tgoto read_failure;\n\n\tstrp->mark = header[0];\n\n\tdata_len = ((header[4] & 0xFF) | (header[3] << 8));\n\n\tcipher_overhead = prot->tag_size;\n\tif (prot->version != TLS_1_3_VERSION &&\n\t    prot->cipher_type != TLS_CIPHER_CHACHA20_POLY1305)\n\t\tcipher_overhead += prot->iv_size;\n\n\tif (data_len > TLS_MAX_PAYLOAD_SIZE + cipher_overhead +\n\t    prot->tail_size) {\n\t\tret = -EMSGSIZE;\n\t\tgoto read_failure;\n\t}\n\tif (data_len < cipher_overhead) {\n\t\tret = -EBADMSG;\n\t\tgoto read_failure;\n\t}\n\n\t \n\tif (header[1] != TLS_1_2_VERSION_MINOR ||\n\t    header[2] != TLS_1_2_VERSION_MAJOR) {\n\t\tret = -EINVAL;\n\t\tgoto read_failure;\n\t}\n\n\ttls_device_rx_resync_new_rec(strp->sk, data_len + TLS_HEADER_SIZE,\n\t\t\t\t     TCP_SKB_CB(skb)->seq + strp->stm.offset);\n\treturn data_len + TLS_HEADER_SIZE;\n\nread_failure:\n\ttls_err_abort(strp->sk, ret);\n\n\treturn ret;\n}\n\nvoid tls_rx_msg_ready(struct tls_strparser *strp)\n{\n\tstruct tls_sw_context_rx *ctx;\n\n\tctx = container_of(strp, struct tls_sw_context_rx, strp);\n\tctx->saved_data_ready(strp->sk);\n}\n\nstatic void tls_data_ready(struct sock *sk)\n{\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\tstruct tls_sw_context_rx *ctx = tls_sw_ctx_rx(tls_ctx);\n\tstruct sk_psock *psock;\n\tgfp_t alloc_save;\n\n\ttrace_sk_data_ready(sk);\n\n\talloc_save = sk->sk_allocation;\n\tsk->sk_allocation = GFP_ATOMIC;\n\ttls_strp_data_ready(&ctx->strp);\n\tsk->sk_allocation = alloc_save;\n\n\tpsock = sk_psock_get(sk);\n\tif (psock) {\n\t\tif (!list_empty(&psock->ingress_msg))\n\t\t\tctx->saved_data_ready(sk);\n\t\tsk_psock_put(sk, psock);\n\t}\n}\n\nvoid tls_sw_cancel_work_tx(struct tls_context *tls_ctx)\n{\n\tstruct tls_sw_context_tx *ctx = tls_sw_ctx_tx(tls_ctx);\n\n\tset_bit(BIT_TX_CLOSING, &ctx->tx_bitmask);\n\tset_bit(BIT_TX_SCHEDULED, &ctx->tx_bitmask);\n\tcancel_delayed_work_sync(&ctx->tx_work.work);\n}\n\nvoid tls_sw_release_resources_tx(struct sock *sk)\n{\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\tstruct tls_sw_context_tx *ctx = tls_sw_ctx_tx(tls_ctx);\n\tstruct tls_rec *rec, *tmp;\n\tint pending;\n\n\t \n\tspin_lock_bh(&ctx->encrypt_compl_lock);\n\tctx->async_notify = true;\n\tpending = atomic_read(&ctx->encrypt_pending);\n\tspin_unlock_bh(&ctx->encrypt_compl_lock);\n\n\tif (pending)\n\t\tcrypto_wait_req(-EINPROGRESS, &ctx->async_wait);\n\n\ttls_tx_records(sk, -1);\n\n\t \n\tif (tls_ctx->partially_sent_record) {\n\t\ttls_free_partial_record(sk, tls_ctx);\n\t\trec = list_first_entry(&ctx->tx_list,\n\t\t\t\t       struct tls_rec, list);\n\t\tlist_del(&rec->list);\n\t\tsk_msg_free(sk, &rec->msg_plaintext);\n\t\tkfree(rec);\n\t}\n\n\tlist_for_each_entry_safe(rec, tmp, &ctx->tx_list, list) {\n\t\tlist_del(&rec->list);\n\t\tsk_msg_free(sk, &rec->msg_encrypted);\n\t\tsk_msg_free(sk, &rec->msg_plaintext);\n\t\tkfree(rec);\n\t}\n\n\tcrypto_free_aead(ctx->aead_send);\n\ttls_free_open_rec(sk);\n}\n\nvoid tls_sw_free_ctx_tx(struct tls_context *tls_ctx)\n{\n\tstruct tls_sw_context_tx *ctx = tls_sw_ctx_tx(tls_ctx);\n\n\tkfree(ctx);\n}\n\nvoid tls_sw_release_resources_rx(struct sock *sk)\n{\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\tstruct tls_sw_context_rx *ctx = tls_sw_ctx_rx(tls_ctx);\n\n\tkfree(tls_ctx->rx.rec_seq);\n\tkfree(tls_ctx->rx.iv);\n\n\tif (ctx->aead_recv) {\n\t\t__skb_queue_purge(&ctx->rx_list);\n\t\tcrypto_free_aead(ctx->aead_recv);\n\t\ttls_strp_stop(&ctx->strp);\n\t\t \n\t\tif (ctx->saved_data_ready) {\n\t\t\twrite_lock_bh(&sk->sk_callback_lock);\n\t\t\tsk->sk_data_ready = ctx->saved_data_ready;\n\t\t\twrite_unlock_bh(&sk->sk_callback_lock);\n\t\t}\n\t}\n}\n\nvoid tls_sw_strparser_done(struct tls_context *tls_ctx)\n{\n\tstruct tls_sw_context_rx *ctx = tls_sw_ctx_rx(tls_ctx);\n\n\ttls_strp_done(&ctx->strp);\n}\n\nvoid tls_sw_free_ctx_rx(struct tls_context *tls_ctx)\n{\n\tstruct tls_sw_context_rx *ctx = tls_sw_ctx_rx(tls_ctx);\n\n\tkfree(ctx);\n}\n\nvoid tls_sw_free_resources_rx(struct sock *sk)\n{\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\n\ttls_sw_release_resources_rx(sk);\n\ttls_sw_free_ctx_rx(tls_ctx);\n}\n\n \nstatic void tx_work_handler(struct work_struct *work)\n{\n\tstruct delayed_work *delayed_work = to_delayed_work(work);\n\tstruct tx_work *tx_work = container_of(delayed_work,\n\t\t\t\t\t       struct tx_work, work);\n\tstruct sock *sk = tx_work->sk;\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\tstruct tls_sw_context_tx *ctx;\n\n\tif (unlikely(!tls_ctx))\n\t\treturn;\n\n\tctx = tls_sw_ctx_tx(tls_ctx);\n\tif (test_bit(BIT_TX_CLOSING, &ctx->tx_bitmask))\n\t\treturn;\n\n\tif (!test_and_clear_bit(BIT_TX_SCHEDULED, &ctx->tx_bitmask))\n\t\treturn;\n\n\tif (mutex_trylock(&tls_ctx->tx_lock)) {\n\t\tlock_sock(sk);\n\t\ttls_tx_records(sk, -1);\n\t\trelease_sock(sk);\n\t\tmutex_unlock(&tls_ctx->tx_lock);\n\t} else if (!test_and_set_bit(BIT_TX_SCHEDULED, &ctx->tx_bitmask)) {\n\t\t \n\t\tschedule_delayed_work(&ctx->tx_work.work, msecs_to_jiffies(10));\n\t}\n}\n\nstatic bool tls_is_tx_ready(struct tls_sw_context_tx *ctx)\n{\n\tstruct tls_rec *rec;\n\n\trec = list_first_entry_or_null(&ctx->tx_list, struct tls_rec, list);\n\tif (!rec)\n\t\treturn false;\n\n\treturn READ_ONCE(rec->tx_ready);\n}\n\nvoid tls_sw_write_space(struct sock *sk, struct tls_context *ctx)\n{\n\tstruct tls_sw_context_tx *tx_ctx = tls_sw_ctx_tx(ctx);\n\n\t \n\tif (tls_is_tx_ready(tx_ctx) &&\n\t    !test_and_set_bit(BIT_TX_SCHEDULED, &tx_ctx->tx_bitmask))\n\t\tschedule_delayed_work(&tx_ctx->tx_work.work, 0);\n}\n\nvoid tls_sw_strparser_arm(struct sock *sk, struct tls_context *tls_ctx)\n{\n\tstruct tls_sw_context_rx *rx_ctx = tls_sw_ctx_rx(tls_ctx);\n\n\twrite_lock_bh(&sk->sk_callback_lock);\n\trx_ctx->saved_data_ready = sk->sk_data_ready;\n\tsk->sk_data_ready = tls_data_ready;\n\twrite_unlock_bh(&sk->sk_callback_lock);\n}\n\nvoid tls_update_rx_zc_capable(struct tls_context *tls_ctx)\n{\n\tstruct tls_sw_context_rx *rx_ctx = tls_sw_ctx_rx(tls_ctx);\n\n\trx_ctx->zc_capable = tls_ctx->rx_no_pad ||\n\t\ttls_ctx->prot_info.version != TLS_1_3_VERSION;\n}\n\nint tls_set_sw_offload(struct sock *sk, struct tls_context *ctx, int tx)\n{\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\tstruct tls_prot_info *prot = &tls_ctx->prot_info;\n\tstruct tls_crypto_info *crypto_info;\n\tstruct tls_sw_context_tx *sw_ctx_tx = NULL;\n\tstruct tls_sw_context_rx *sw_ctx_rx = NULL;\n\tstruct cipher_context *cctx;\n\tstruct crypto_aead **aead;\n\tstruct crypto_tfm *tfm;\n\tchar *iv, *rec_seq, *key, *salt;\n\tconst struct tls_cipher_desc *cipher_desc;\n\tu16 nonce_size;\n\tint rc = 0;\n\n\tif (!ctx) {\n\t\trc = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (tx) {\n\t\tif (!ctx->priv_ctx_tx) {\n\t\t\tsw_ctx_tx = kzalloc(sizeof(*sw_ctx_tx), GFP_KERNEL);\n\t\t\tif (!sw_ctx_tx) {\n\t\t\t\trc = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tctx->priv_ctx_tx = sw_ctx_tx;\n\t\t} else {\n\t\t\tsw_ctx_tx =\n\t\t\t\t(struct tls_sw_context_tx *)ctx->priv_ctx_tx;\n\t\t}\n\t} else {\n\t\tif (!ctx->priv_ctx_rx) {\n\t\t\tsw_ctx_rx = kzalloc(sizeof(*sw_ctx_rx), GFP_KERNEL);\n\t\t\tif (!sw_ctx_rx) {\n\t\t\t\trc = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tctx->priv_ctx_rx = sw_ctx_rx;\n\t\t} else {\n\t\t\tsw_ctx_rx =\n\t\t\t\t(struct tls_sw_context_rx *)ctx->priv_ctx_rx;\n\t\t}\n\t}\n\n\tif (tx) {\n\t\tcrypto_init_wait(&sw_ctx_tx->async_wait);\n\t\tspin_lock_init(&sw_ctx_tx->encrypt_compl_lock);\n\t\tcrypto_info = &ctx->crypto_send.info;\n\t\tcctx = &ctx->tx;\n\t\taead = &sw_ctx_tx->aead_send;\n\t\tINIT_LIST_HEAD(&sw_ctx_tx->tx_list);\n\t\tINIT_DELAYED_WORK(&sw_ctx_tx->tx_work.work, tx_work_handler);\n\t\tsw_ctx_tx->tx_work.sk = sk;\n\t} else {\n\t\tcrypto_init_wait(&sw_ctx_rx->async_wait);\n\t\tspin_lock_init(&sw_ctx_rx->decrypt_compl_lock);\n\t\tinit_waitqueue_head(&sw_ctx_rx->wq);\n\t\tcrypto_info = &ctx->crypto_recv.info;\n\t\tcctx = &ctx->rx;\n\t\tskb_queue_head_init(&sw_ctx_rx->rx_list);\n\t\tskb_queue_head_init(&sw_ctx_rx->async_hold);\n\t\taead = &sw_ctx_rx->aead_recv;\n\t}\n\n\tcipher_desc = get_cipher_desc(crypto_info->cipher_type);\n\tif (!cipher_desc) {\n\t\trc = -EINVAL;\n\t\tgoto free_priv;\n\t}\n\n\tnonce_size = cipher_desc->nonce;\n\n\tiv = crypto_info_iv(crypto_info, cipher_desc);\n\tkey = crypto_info_key(crypto_info, cipher_desc);\n\tsalt = crypto_info_salt(crypto_info, cipher_desc);\n\trec_seq = crypto_info_rec_seq(crypto_info, cipher_desc);\n\n\tif (crypto_info->version == TLS_1_3_VERSION) {\n\t\tnonce_size = 0;\n\t\tprot->aad_size = TLS_HEADER_SIZE;\n\t\tprot->tail_size = 1;\n\t} else {\n\t\tprot->aad_size = TLS_AAD_SPACE_SIZE;\n\t\tprot->tail_size = 0;\n\t}\n\n\t \n\tif (nonce_size > MAX_IV_SIZE || prot->aad_size > TLS_MAX_AAD_SIZE) {\n\t\trc = -EINVAL;\n\t\tgoto free_priv;\n\t}\n\n\tprot->version = crypto_info->version;\n\tprot->cipher_type = crypto_info->cipher_type;\n\tprot->prepend_size = TLS_HEADER_SIZE + nonce_size;\n\tprot->tag_size = cipher_desc->tag;\n\tprot->overhead_size = prot->prepend_size +\n\t\t\t      prot->tag_size + prot->tail_size;\n\tprot->iv_size = cipher_desc->iv;\n\tprot->salt_size = cipher_desc->salt;\n\tcctx->iv = kmalloc(cipher_desc->iv + cipher_desc->salt, GFP_KERNEL);\n\tif (!cctx->iv) {\n\t\trc = -ENOMEM;\n\t\tgoto free_priv;\n\t}\n\t \n\tprot->rec_seq_size = cipher_desc->rec_seq;\n\tmemcpy(cctx->iv, salt, cipher_desc->salt);\n\tmemcpy(cctx->iv + cipher_desc->salt, iv, cipher_desc->iv);\n\n\tcctx->rec_seq = kmemdup(rec_seq, cipher_desc->rec_seq, GFP_KERNEL);\n\tif (!cctx->rec_seq) {\n\t\trc = -ENOMEM;\n\t\tgoto free_iv;\n\t}\n\n\tif (!*aead) {\n\t\t*aead = crypto_alloc_aead(cipher_desc->cipher_name, 0, 0);\n\t\tif (IS_ERR(*aead)) {\n\t\t\trc = PTR_ERR(*aead);\n\t\t\t*aead = NULL;\n\t\t\tgoto free_rec_seq;\n\t\t}\n\t}\n\n\tctx->push_pending_record = tls_sw_push_pending_record;\n\n\trc = crypto_aead_setkey(*aead, key, cipher_desc->key);\n\tif (rc)\n\t\tgoto free_aead;\n\n\trc = crypto_aead_setauthsize(*aead, prot->tag_size);\n\tif (rc)\n\t\tgoto free_aead;\n\n\tif (sw_ctx_rx) {\n\t\ttfm = crypto_aead_tfm(sw_ctx_rx->aead_recv);\n\n\t\ttls_update_rx_zc_capable(ctx);\n\t\tsw_ctx_rx->async_capable =\n\t\t\tcrypto_info->version != TLS_1_3_VERSION &&\n\t\t\t!!(tfm->__crt_alg->cra_flags & CRYPTO_ALG_ASYNC);\n\n\t\trc = tls_strp_init(&sw_ctx_rx->strp, sk);\n\t\tif (rc)\n\t\t\tgoto free_aead;\n\t}\n\n\tgoto out;\n\nfree_aead:\n\tcrypto_free_aead(*aead);\n\t*aead = NULL;\nfree_rec_seq:\n\tkfree(cctx->rec_seq);\n\tcctx->rec_seq = NULL;\nfree_iv:\n\tkfree(cctx->iv);\n\tcctx->iv = NULL;\nfree_priv:\n\tif (tx) {\n\t\tkfree(ctx->priv_ctx_tx);\n\t\tctx->priv_ctx_tx = NULL;\n\t} else {\n\t\tkfree(ctx->priv_ctx_rx);\n\t\tctx->priv_ctx_rx = NULL;\n\t}\nout:\n\treturn rc;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}