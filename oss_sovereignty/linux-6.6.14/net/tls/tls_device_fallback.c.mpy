{
  "module_name": "tls_device_fallback.c",
  "hash_id": "588f4d88317140fb4004e27e27938d249934e9f155efba919051dfa19fd578b0",
  "original_prompt": "Ingested from linux-6.6.14/net/tls/tls_device_fallback.c",
  "human_readable_source": " \n\n#include <net/tls.h>\n#include <crypto/aead.h>\n#include <crypto/scatterwalk.h>\n#include <net/ip6_checksum.h>\n\n#include \"tls.h\"\n\nstatic void chain_to_walk(struct scatterlist *sg, struct scatter_walk *walk)\n{\n\tstruct scatterlist *src = walk->sg;\n\tint diff = walk->offset - src->offset;\n\n\tsg_set_page(sg, sg_page(src),\n\t\t    src->length - diff, walk->offset);\n\n\tscatterwalk_crypto_chain(sg, sg_next(src), 2);\n}\n\nstatic int tls_enc_record(struct aead_request *aead_req,\n\t\t\t  struct crypto_aead *aead, char *aad,\n\t\t\t  char *iv, __be64 rcd_sn,\n\t\t\t  struct scatter_walk *in,\n\t\t\t  struct scatter_walk *out, int *in_len,\n\t\t\t  struct tls_prot_info *prot)\n{\n\tunsigned char buf[TLS_HEADER_SIZE + MAX_IV_SIZE];\n\tconst struct tls_cipher_desc *cipher_desc;\n\tstruct scatterlist sg_in[3];\n\tstruct scatterlist sg_out[3];\n\tunsigned int buf_size;\n\tu16 len;\n\tint rc;\n\n\tswitch (prot->cipher_type) {\n\tcase TLS_CIPHER_AES_GCM_128:\n\tcase TLS_CIPHER_AES_GCM_256:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\tcipher_desc = get_cipher_desc(prot->cipher_type);\n\n\tbuf_size = TLS_HEADER_SIZE + cipher_desc->iv;\n\tlen = min_t(int, *in_len, buf_size);\n\n\tscatterwalk_copychunks(buf, in, len, 0);\n\tscatterwalk_copychunks(buf, out, len, 1);\n\n\t*in_len -= len;\n\tif (!*in_len)\n\t\treturn 0;\n\n\tscatterwalk_pagedone(in, 0, 1);\n\tscatterwalk_pagedone(out, 1, 1);\n\n\tlen = buf[4] | (buf[3] << 8);\n\tlen -= cipher_desc->iv;\n\n\ttls_make_aad(aad, len - cipher_desc->tag, (char *)&rcd_sn, buf[0], prot);\n\n\tmemcpy(iv + cipher_desc->salt, buf + TLS_HEADER_SIZE, cipher_desc->iv);\n\n\tsg_init_table(sg_in, ARRAY_SIZE(sg_in));\n\tsg_init_table(sg_out, ARRAY_SIZE(sg_out));\n\tsg_set_buf(sg_in, aad, TLS_AAD_SPACE_SIZE);\n\tsg_set_buf(sg_out, aad, TLS_AAD_SPACE_SIZE);\n\tchain_to_walk(sg_in + 1, in);\n\tchain_to_walk(sg_out + 1, out);\n\n\t*in_len -= len;\n\tif (*in_len < 0) {\n\t\t*in_len += cipher_desc->tag;\n\t\t \n\t\tif (*in_len < 0)\n\t\t\tlen += *in_len;\n\n\t\t*in_len = 0;\n\t}\n\n\tif (*in_len) {\n\t\tscatterwalk_copychunks(NULL, in, len, 2);\n\t\tscatterwalk_pagedone(in, 0, 1);\n\t\tscatterwalk_copychunks(NULL, out, len, 2);\n\t\tscatterwalk_pagedone(out, 1, 1);\n\t}\n\n\tlen -= cipher_desc->tag;\n\taead_request_set_crypt(aead_req, sg_in, sg_out, len, iv);\n\n\trc = crypto_aead_encrypt(aead_req);\n\n\treturn rc;\n}\n\nstatic void tls_init_aead_request(struct aead_request *aead_req,\n\t\t\t\t  struct crypto_aead *aead)\n{\n\taead_request_set_tfm(aead_req, aead);\n\taead_request_set_ad(aead_req, TLS_AAD_SPACE_SIZE);\n}\n\nstatic struct aead_request *tls_alloc_aead_request(struct crypto_aead *aead,\n\t\t\t\t\t\t   gfp_t flags)\n{\n\tunsigned int req_size = sizeof(struct aead_request) +\n\t\tcrypto_aead_reqsize(aead);\n\tstruct aead_request *aead_req;\n\n\taead_req = kzalloc(req_size, flags);\n\tif (aead_req)\n\t\ttls_init_aead_request(aead_req, aead);\n\treturn aead_req;\n}\n\nstatic int tls_enc_records(struct aead_request *aead_req,\n\t\t\t   struct crypto_aead *aead, struct scatterlist *sg_in,\n\t\t\t   struct scatterlist *sg_out, char *aad, char *iv,\n\t\t\t   u64 rcd_sn, int len, struct tls_prot_info *prot)\n{\n\tstruct scatter_walk out, in;\n\tint rc;\n\n\tscatterwalk_start(&in, sg_in);\n\tscatterwalk_start(&out, sg_out);\n\n\tdo {\n\t\trc = tls_enc_record(aead_req, aead, aad, iv,\n\t\t\t\t    cpu_to_be64(rcd_sn), &in, &out, &len, prot);\n\t\trcd_sn++;\n\n\t} while (rc == 0 && len);\n\n\tscatterwalk_done(&in, 0, 0);\n\tscatterwalk_done(&out, 1, 0);\n\n\treturn rc;\n}\n\n \nstatic void update_chksum(struct sk_buff *skb, int headln)\n{\n\tstruct tcphdr *th = tcp_hdr(skb);\n\tint datalen = skb->len - headln;\n\tconst struct ipv6hdr *ipv6h;\n\tconst struct iphdr *iph;\n\n\t \n\tif (likely(skb->ip_summed == CHECKSUM_PARTIAL))\n\t\treturn;\n\n\tskb->ip_summed = CHECKSUM_PARTIAL;\n\tskb->csum_start = skb_transport_header(skb) - skb->head;\n\tskb->csum_offset = offsetof(struct tcphdr, check);\n\n\tif (skb->sk->sk_family == AF_INET6) {\n\t\tipv6h = ipv6_hdr(skb);\n\t\tth->check = ~csum_ipv6_magic(&ipv6h->saddr, &ipv6h->daddr,\n\t\t\t\t\t     datalen, IPPROTO_TCP, 0);\n\t} else {\n\t\tiph = ip_hdr(skb);\n\t\tth->check = ~csum_tcpudp_magic(iph->saddr, iph->daddr, datalen,\n\t\t\t\t\t       IPPROTO_TCP, 0);\n\t}\n}\n\nstatic void complete_skb(struct sk_buff *nskb, struct sk_buff *skb, int headln)\n{\n\tstruct sock *sk = skb->sk;\n\tint delta;\n\n\tskb_copy_header(nskb, skb);\n\n\tskb_put(nskb, skb->len);\n\tmemcpy(nskb->data, skb->data, headln);\n\n\tnskb->destructor = skb->destructor;\n\tnskb->sk = sk;\n\tskb->destructor = NULL;\n\tskb->sk = NULL;\n\n\tupdate_chksum(nskb, headln);\n\n\t \n\tif (nskb->destructor == sock_efree)\n\t\treturn;\n\n\tdelta = nskb->truesize - skb->truesize;\n\tif (likely(delta < 0))\n\t\tWARN_ON_ONCE(refcount_sub_and_test(-delta, &sk->sk_wmem_alloc));\n\telse if (delta)\n\t\trefcount_add(delta, &sk->sk_wmem_alloc);\n}\n\n \n\nstatic int fill_sg_in(struct scatterlist *sg_in,\n\t\t      struct sk_buff *skb,\n\t\t      struct tls_offload_context_tx *ctx,\n\t\t      u64 *rcd_sn,\n\t\t      s32 *sync_size,\n\t\t      int *resync_sgs)\n{\n\tint tcp_payload_offset = skb_tcp_all_headers(skb);\n\tint payload_len = skb->len - tcp_payload_offset;\n\tu32 tcp_seq = ntohl(tcp_hdr(skb)->seq);\n\tstruct tls_record_info *record;\n\tunsigned long flags;\n\tint remaining;\n\tint i;\n\n\tspin_lock_irqsave(&ctx->lock, flags);\n\trecord = tls_get_record(ctx, tcp_seq, rcd_sn);\n\tif (!record) {\n\t\tspin_unlock_irqrestore(&ctx->lock, flags);\n\t\treturn -EINVAL;\n\t}\n\n\t*sync_size = tcp_seq - tls_record_start_seq(record);\n\tif (*sync_size < 0) {\n\t\tint is_start_marker = tls_record_is_start_marker(record);\n\n\t\tspin_unlock_irqrestore(&ctx->lock, flags);\n\t\t \n\t\tif (!is_start_marker)\n\t\t\t*sync_size = 0;\n\t\treturn -EINVAL;\n\t}\n\n\tremaining = *sync_size;\n\tfor (i = 0; remaining > 0; i++) {\n\t\tskb_frag_t *frag = &record->frags[i];\n\n\t\t__skb_frag_ref(frag);\n\t\tsg_set_page(sg_in + i, skb_frag_page(frag),\n\t\t\t    skb_frag_size(frag), skb_frag_off(frag));\n\n\t\tremaining -= skb_frag_size(frag);\n\n\t\tif (remaining < 0)\n\t\t\tsg_in[i].length += remaining;\n\t}\n\t*resync_sgs = i;\n\n\tspin_unlock_irqrestore(&ctx->lock, flags);\n\tif (skb_to_sgvec(skb, &sg_in[i], tcp_payload_offset, payload_len) < 0)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic void fill_sg_out(struct scatterlist sg_out[3], void *buf,\n\t\t\tstruct tls_context *tls_ctx,\n\t\t\tstruct sk_buff *nskb,\n\t\t\tint tcp_payload_offset,\n\t\t\tint payload_len,\n\t\t\tint sync_size,\n\t\t\tvoid *dummy_buf)\n{\n\tconst struct tls_cipher_desc *cipher_desc =\n\t\tget_cipher_desc(tls_ctx->crypto_send.info.cipher_type);\n\n\tsg_set_buf(&sg_out[0], dummy_buf, sync_size);\n\tsg_set_buf(&sg_out[1], nskb->data + tcp_payload_offset, payload_len);\n\t \n\tdummy_buf += sync_size;\n\tsg_set_buf(&sg_out[2], dummy_buf, cipher_desc->tag);\n}\n\nstatic struct sk_buff *tls_enc_skb(struct tls_context *tls_ctx,\n\t\t\t\t   struct scatterlist sg_out[3],\n\t\t\t\t   struct scatterlist *sg_in,\n\t\t\t\t   struct sk_buff *skb,\n\t\t\t\t   s32 sync_size, u64 rcd_sn)\n{\n\tstruct tls_offload_context_tx *ctx = tls_offload_ctx_tx(tls_ctx);\n\tint tcp_payload_offset = skb_tcp_all_headers(skb);\n\tint payload_len = skb->len - tcp_payload_offset;\n\tconst struct tls_cipher_desc *cipher_desc;\n\tvoid *buf, *iv, *aad, *dummy_buf, *salt;\n\tstruct aead_request *aead_req;\n\tstruct sk_buff *nskb = NULL;\n\tint buf_len;\n\n\taead_req = tls_alloc_aead_request(ctx->aead_send, GFP_ATOMIC);\n\tif (!aead_req)\n\t\treturn NULL;\n\n\tswitch (tls_ctx->crypto_send.info.cipher_type) {\n\tcase TLS_CIPHER_AES_GCM_128:\n\t\tsalt = tls_ctx->crypto_send.aes_gcm_128.salt;\n\t\tbreak;\n\tcase TLS_CIPHER_AES_GCM_256:\n\t\tsalt = tls_ctx->crypto_send.aes_gcm_256.salt;\n\t\tbreak;\n\tdefault:\n\t\tgoto free_req;\n\t}\n\tcipher_desc = get_cipher_desc(tls_ctx->crypto_send.info.cipher_type);\n\tbuf_len = cipher_desc->salt + cipher_desc->iv + TLS_AAD_SPACE_SIZE +\n\t\t  sync_size + cipher_desc->tag;\n\tbuf = kmalloc(buf_len, GFP_ATOMIC);\n\tif (!buf)\n\t\tgoto free_req;\n\n\tiv = buf;\n\tmemcpy(iv, salt, cipher_desc->salt);\n\taad = buf + cipher_desc->salt + cipher_desc->iv;\n\tdummy_buf = aad + TLS_AAD_SPACE_SIZE;\n\n\tnskb = alloc_skb(skb_headroom(skb) + skb->len, GFP_ATOMIC);\n\tif (!nskb)\n\t\tgoto free_buf;\n\n\tskb_reserve(nskb, skb_headroom(skb));\n\n\tfill_sg_out(sg_out, buf, tls_ctx, nskb, tcp_payload_offset,\n\t\t    payload_len, sync_size, dummy_buf);\n\n\tif (tls_enc_records(aead_req, ctx->aead_send, sg_in, sg_out, aad, iv,\n\t\t\t    rcd_sn, sync_size + payload_len,\n\t\t\t    &tls_ctx->prot_info) < 0)\n\t\tgoto free_nskb;\n\n\tcomplete_skb(nskb, skb, tcp_payload_offset);\n\n\t \n\tnskb->prev = nskb;\n\nfree_buf:\n\tkfree(buf);\nfree_req:\n\tkfree(aead_req);\n\treturn nskb;\nfree_nskb:\n\tkfree_skb(nskb);\n\tnskb = NULL;\n\tgoto free_buf;\n}\n\nstatic struct sk_buff *tls_sw_fallback(struct sock *sk, struct sk_buff *skb)\n{\n\tint tcp_payload_offset = skb_tcp_all_headers(skb);\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\tstruct tls_offload_context_tx *ctx = tls_offload_ctx_tx(tls_ctx);\n\tint payload_len = skb->len - tcp_payload_offset;\n\tstruct scatterlist *sg_in, sg_out[3];\n\tstruct sk_buff *nskb = NULL;\n\tint sg_in_max_elements;\n\tint resync_sgs = 0;\n\ts32 sync_size = 0;\n\tu64 rcd_sn;\n\n\t \n\tsg_in_max_elements = 2 * MAX_SKB_FRAGS + 1;\n\n\tif (!payload_len)\n\t\treturn skb;\n\n\tsg_in = kmalloc_array(sg_in_max_elements, sizeof(*sg_in), GFP_ATOMIC);\n\tif (!sg_in)\n\t\tgoto free_orig;\n\n\tsg_init_table(sg_in, sg_in_max_elements);\n\tsg_init_table(sg_out, ARRAY_SIZE(sg_out));\n\n\tif (fill_sg_in(sg_in, skb, ctx, &rcd_sn, &sync_size, &resync_sgs)) {\n\t\t \n\t\tif (sync_size < 0 && payload_len <= -sync_size)\n\t\t\tnskb = skb_get(skb);\n\t\tgoto put_sg;\n\t}\n\n\tnskb = tls_enc_skb(tls_ctx, sg_out, sg_in, skb, sync_size, rcd_sn);\n\nput_sg:\n\twhile (resync_sgs)\n\t\tput_page(sg_page(&sg_in[--resync_sgs]));\n\tkfree(sg_in);\nfree_orig:\n\tif (nskb)\n\t\tconsume_skb(skb);\n\telse\n\t\tkfree_skb(skb);\n\treturn nskb;\n}\n\nstruct sk_buff *tls_validate_xmit_skb(struct sock *sk,\n\t\t\t\t      struct net_device *dev,\n\t\t\t\t      struct sk_buff *skb)\n{\n\tif (dev == rcu_dereference_bh(tls_get_ctx(sk)->netdev) ||\n\t    netif_is_bond_master(dev))\n\t\treturn skb;\n\n\treturn tls_sw_fallback(sk, skb);\n}\nEXPORT_SYMBOL_GPL(tls_validate_xmit_skb);\n\nstruct sk_buff *tls_validate_xmit_skb_sw(struct sock *sk,\n\t\t\t\t\t struct net_device *dev,\n\t\t\t\t\t struct sk_buff *skb)\n{\n\treturn tls_sw_fallback(sk, skb);\n}\n\nstruct sk_buff *tls_encrypt_skb(struct sk_buff *skb)\n{\n\treturn tls_sw_fallback(skb->sk, skb);\n}\nEXPORT_SYMBOL_GPL(tls_encrypt_skb);\n\nint tls_sw_fallback_init(struct sock *sk,\n\t\t\t struct tls_offload_context_tx *offload_ctx,\n\t\t\t struct tls_crypto_info *crypto_info)\n{\n\tconst struct tls_cipher_desc *cipher_desc;\n\tint rc;\n\n\tcipher_desc = get_cipher_desc(crypto_info->cipher_type);\n\tif (!cipher_desc || !cipher_desc->offloadable)\n\t\treturn -EINVAL;\n\n\toffload_ctx->aead_send =\n\t    crypto_alloc_aead(cipher_desc->cipher_name, 0, CRYPTO_ALG_ASYNC);\n\tif (IS_ERR(offload_ctx->aead_send)) {\n\t\trc = PTR_ERR(offload_ctx->aead_send);\n\t\tpr_err_ratelimited(\"crypto_alloc_aead failed rc=%d\\n\", rc);\n\t\toffload_ctx->aead_send = NULL;\n\t\tgoto err_out;\n\t}\n\n\trc = crypto_aead_setkey(offload_ctx->aead_send,\n\t\t\t\tcrypto_info_key(crypto_info, cipher_desc),\n\t\t\t\tcipher_desc->key);\n\tif (rc)\n\t\tgoto free_aead;\n\n\trc = crypto_aead_setauthsize(offload_ctx->aead_send, cipher_desc->tag);\n\tif (rc)\n\t\tgoto free_aead;\n\n\treturn 0;\nfree_aead:\n\tcrypto_free_aead(offload_ctx->aead_send);\nerr_out:\n\treturn rc;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}