{
  "module_name": "smc_core.c",
  "hash_id": "e60fda52bff1b89a4079550ea04bf0d7c49a911267722edc9a387d268bd2545e",
  "original_prompt": "Ingested from linux-6.6.14/net/smc/smc_core.c",
  "human_readable_source": "\n \n\n#include <linux/socket.h>\n#include <linux/if_vlan.h>\n#include <linux/random.h>\n#include <linux/workqueue.h>\n#include <linux/wait.h>\n#include <linux/reboot.h>\n#include <linux/mutex.h>\n#include <linux/list.h>\n#include <linux/smc.h>\n#include <net/tcp.h>\n#include <net/sock.h>\n#include <rdma/ib_verbs.h>\n#include <rdma/ib_cache.h>\n\n#include \"smc.h\"\n#include \"smc_clc.h\"\n#include \"smc_core.h\"\n#include \"smc_ib.h\"\n#include \"smc_wr.h\"\n#include \"smc_llc.h\"\n#include \"smc_cdc.h\"\n#include \"smc_close.h\"\n#include \"smc_ism.h\"\n#include \"smc_netlink.h\"\n#include \"smc_stats.h\"\n#include \"smc_tracepoint.h\"\n\n#define SMC_LGR_NUM_INCR\t\t256\n#define SMC_LGR_FREE_DELAY_SERV\t\t(600 * HZ)\n#define SMC_LGR_FREE_DELAY_CLNT\t\t(SMC_LGR_FREE_DELAY_SERV + 10 * HZ)\n\nstruct smc_lgr_list smc_lgr_list = {\t \n\t.lock = __SPIN_LOCK_UNLOCKED(smc_lgr_list.lock),\n\t.list = LIST_HEAD_INIT(smc_lgr_list.list),\n\t.num = 0,\n};\n\nstatic atomic_t lgr_cnt = ATOMIC_INIT(0);  \nstatic DECLARE_WAIT_QUEUE_HEAD(lgrs_deleted);\n\nstatic void smc_buf_free(struct smc_link_group *lgr, bool is_rmb,\n\t\t\t struct smc_buf_desc *buf_desc);\nstatic void __smc_lgr_terminate(struct smc_link_group *lgr, bool soft);\n\nstatic void smc_link_down_work(struct work_struct *work);\n\n \nstatic inline struct list_head *smc_lgr_list_head(struct smc_link_group *lgr,\n\t\t\t\t\t\t  spinlock_t **lgr_lock)\n{\n\tif (lgr->is_smcd) {\n\t\t*lgr_lock = &lgr->smcd->lgr_lock;\n\t\treturn &lgr->smcd->lgr_list;\n\t}\n\n\t*lgr_lock = &smc_lgr_list.lock;\n\treturn &smc_lgr_list.list;\n}\n\nstatic void smc_ibdev_cnt_inc(struct smc_link *lnk)\n{\n\tatomic_inc(&lnk->smcibdev->lnk_cnt_by_port[lnk->ibport - 1]);\n}\n\nstatic void smc_ibdev_cnt_dec(struct smc_link *lnk)\n{\n\tatomic_dec(&lnk->smcibdev->lnk_cnt_by_port[lnk->ibport - 1]);\n}\n\nstatic void smc_lgr_schedule_free_work(struct smc_link_group *lgr)\n{\n\t \n\tif (!lgr->freeing) {\n\t\tmod_delayed_work(system_wq, &lgr->free_work,\n\t\t\t\t (!lgr->is_smcd && lgr->role == SMC_CLNT) ?\n\t\t\t\t\t\tSMC_LGR_FREE_DELAY_CLNT :\n\t\t\t\t\t\tSMC_LGR_FREE_DELAY_SERV);\n\t}\n}\n\n \nstatic void smc_lgr_add_alert_token(struct smc_connection *conn)\n{\n\tstruct rb_node **link, *parent = NULL;\n\tu32 token = conn->alert_token_local;\n\n\tlink = &conn->lgr->conns_all.rb_node;\n\twhile (*link) {\n\t\tstruct smc_connection *cur = rb_entry(*link,\n\t\t\t\t\tstruct smc_connection, alert_node);\n\n\t\tparent = *link;\n\t\tif (cur->alert_token_local > token)\n\t\t\tlink = &parent->rb_left;\n\t\telse\n\t\t\tlink = &parent->rb_right;\n\t}\n\t \n\trb_link_node(&conn->alert_node, parent, link);\n\trb_insert_color(&conn->alert_node, &conn->lgr->conns_all);\n}\n\n \nstatic int smcr_lgr_conn_assign_link(struct smc_connection *conn, bool first)\n{\n\tenum smc_link_state expected = first ? SMC_LNK_ACTIVATING :\n\t\t\t\t       SMC_LNK_ACTIVE;\n\tint i, j;\n\n\t \n\tconn->lnk = NULL;\t \n\tfor (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++) {\n\t\tstruct smc_link *lnk = &conn->lgr->lnk[i];\n\n\t\tif (lnk->state != expected || lnk->link_is_asym)\n\t\t\tcontinue;\n\t\tif (conn->lgr->role == SMC_CLNT) {\n\t\t\tconn->lnk = lnk;  \n\t\t\tbreak;\n\t\t}\n\t\tif (conn->lgr->conns_num % 2) {\n\t\t\tfor (j = i + 1; j < SMC_LINKS_PER_LGR_MAX; j++) {\n\t\t\t\tstruct smc_link *lnk2;\n\n\t\t\t\tlnk2 = &conn->lgr->lnk[j];\n\t\t\t\tif (lnk2->state == expected &&\n\t\t\t\t    !lnk2->link_is_asym) {\n\t\t\t\t\tconn->lnk = lnk2;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif (!conn->lnk)\n\t\t\tconn->lnk = lnk;\n\t\tbreak;\n\t}\n\tif (!conn->lnk)\n\t\treturn SMC_CLC_DECL_NOACTLINK;\n\tatomic_inc(&conn->lnk->conn_cnt);\n\treturn 0;\n}\n\n \nstatic int smc_lgr_register_conn(struct smc_connection *conn, bool first)\n{\n\tstruct smc_sock *smc = container_of(conn, struct smc_sock, conn);\n\tstatic atomic_t nexttoken = ATOMIC_INIT(0);\n\tint rc;\n\n\tif (!conn->lgr->is_smcd) {\n\t\trc = smcr_lgr_conn_assign_link(conn, first);\n\t\tif (rc) {\n\t\t\tconn->lgr = NULL;\n\t\t\treturn rc;\n\t\t}\n\t}\n\t \n\tsock_hold(&smc->sk);  \n\twhile (!conn->alert_token_local) {\n\t\tconn->alert_token_local = atomic_inc_return(&nexttoken);\n\t\tif (smc_lgr_find_conn(conn->alert_token_local, conn->lgr))\n\t\t\tconn->alert_token_local = 0;\n\t}\n\tsmc_lgr_add_alert_token(conn);\n\tconn->lgr->conns_num++;\n\treturn 0;\n}\n\n \nstatic void __smc_lgr_unregister_conn(struct smc_connection *conn)\n{\n\tstruct smc_sock *smc = container_of(conn, struct smc_sock, conn);\n\tstruct smc_link_group *lgr = conn->lgr;\n\n\trb_erase(&conn->alert_node, &lgr->conns_all);\n\tif (conn->lnk)\n\t\tatomic_dec(&conn->lnk->conn_cnt);\n\tlgr->conns_num--;\n\tconn->alert_token_local = 0;\n\tsock_put(&smc->sk);  \n}\n\n \nstatic void smc_lgr_unregister_conn(struct smc_connection *conn)\n{\n\tstruct smc_link_group *lgr = conn->lgr;\n\n\tif (!smc_conn_lgr_valid(conn))\n\t\treturn;\n\twrite_lock_bh(&lgr->conns_lock);\n\tif (conn->alert_token_local) {\n\t\t__smc_lgr_unregister_conn(conn);\n\t}\n\twrite_unlock_bh(&lgr->conns_lock);\n}\n\nint smc_nl_get_sys_info(struct sk_buff *skb, struct netlink_callback *cb)\n{\n\tstruct smc_nl_dmp_ctx *cb_ctx = smc_nl_dmp_ctx(cb);\n\tchar hostname[SMC_MAX_HOSTNAME_LEN + 1];\n\tchar smc_seid[SMC_MAX_EID_LEN + 1];\n\tstruct nlattr *attrs;\n\tu8 *seid = NULL;\n\tu8 *host = NULL;\n\tvoid *nlh;\n\n\tnlh = genlmsg_put(skb, NETLINK_CB(cb->skb).portid, cb->nlh->nlmsg_seq,\n\t\t\t  &smc_gen_nl_family, NLM_F_MULTI,\n\t\t\t  SMC_NETLINK_GET_SYS_INFO);\n\tif (!nlh)\n\t\tgoto errmsg;\n\tif (cb_ctx->pos[0])\n\t\tgoto errout;\n\tattrs = nla_nest_start(skb, SMC_GEN_SYS_INFO);\n\tif (!attrs)\n\t\tgoto errout;\n\tif (nla_put_u8(skb, SMC_NLA_SYS_VER, SMC_V2))\n\t\tgoto errattr;\n\tif (nla_put_u8(skb, SMC_NLA_SYS_REL, SMC_RELEASE))\n\t\tgoto errattr;\n\tif (nla_put_u8(skb, SMC_NLA_SYS_IS_ISM_V2, smc_ism_is_v2_capable()))\n\t\tgoto errattr;\n\tif (nla_put_u8(skb, SMC_NLA_SYS_IS_SMCR_V2, true))\n\t\tgoto errattr;\n\tsmc_clc_get_hostname(&host);\n\tif (host) {\n\t\tmemcpy(hostname, host, SMC_MAX_HOSTNAME_LEN);\n\t\thostname[SMC_MAX_HOSTNAME_LEN] = 0;\n\t\tif (nla_put_string(skb, SMC_NLA_SYS_LOCAL_HOST, hostname))\n\t\t\tgoto errattr;\n\t}\n\tif (smc_ism_is_v2_capable()) {\n\t\tsmc_ism_get_system_eid(&seid);\n\t\tmemcpy(smc_seid, seid, SMC_MAX_EID_LEN);\n\t\tsmc_seid[SMC_MAX_EID_LEN] = 0;\n\t\tif (nla_put_string(skb, SMC_NLA_SYS_SEID, smc_seid))\n\t\t\tgoto errattr;\n\t}\n\tnla_nest_end(skb, attrs);\n\tgenlmsg_end(skb, nlh);\n\tcb_ctx->pos[0] = 1;\n\treturn skb->len;\n\nerrattr:\n\tnla_nest_cancel(skb, attrs);\nerrout:\n\tgenlmsg_cancel(skb, nlh);\nerrmsg:\n\treturn skb->len;\n}\n\n \nstatic int smc_nl_fill_lgr_v2_common(struct smc_link_group *lgr,\n\t\t\t\t     struct sk_buff *skb,\n\t\t\t\t     struct netlink_callback *cb,\n\t\t\t\t     struct nlattr *v2_attrs)\n{\n\tchar smc_host[SMC_MAX_HOSTNAME_LEN + 1];\n\tchar smc_eid[SMC_MAX_EID_LEN + 1];\n\n\tif (nla_put_u8(skb, SMC_NLA_LGR_V2_VER, lgr->smc_version))\n\t\tgoto errv2attr;\n\tif (nla_put_u8(skb, SMC_NLA_LGR_V2_REL, lgr->peer_smc_release))\n\t\tgoto errv2attr;\n\tif (nla_put_u8(skb, SMC_NLA_LGR_V2_OS, lgr->peer_os))\n\t\tgoto errv2attr;\n\tmemcpy(smc_host, lgr->peer_hostname, SMC_MAX_HOSTNAME_LEN);\n\tsmc_host[SMC_MAX_HOSTNAME_LEN] = 0;\n\tif (nla_put_string(skb, SMC_NLA_LGR_V2_PEER_HOST, smc_host))\n\t\tgoto errv2attr;\n\tmemcpy(smc_eid, lgr->negotiated_eid, SMC_MAX_EID_LEN);\n\tsmc_eid[SMC_MAX_EID_LEN] = 0;\n\tif (nla_put_string(skb, SMC_NLA_LGR_V2_NEG_EID, smc_eid))\n\t\tgoto errv2attr;\n\n\tnla_nest_end(skb, v2_attrs);\n\treturn 0;\n\nerrv2attr:\n\tnla_nest_cancel(skb, v2_attrs);\n\treturn -EMSGSIZE;\n}\n\nstatic int smc_nl_fill_smcr_lgr_v2(struct smc_link_group *lgr,\n\t\t\t\t   struct sk_buff *skb,\n\t\t\t\t   struct netlink_callback *cb)\n{\n\tstruct nlattr *v2_attrs;\n\n\tv2_attrs = nla_nest_start(skb, SMC_NLA_LGR_R_V2);\n\tif (!v2_attrs)\n\t\tgoto errattr;\n\tif (nla_put_u8(skb, SMC_NLA_LGR_R_V2_DIRECT, !lgr->uses_gateway))\n\t\tgoto errv2attr;\n\tif (nla_put_u8(skb, SMC_NLA_LGR_R_V2_MAX_CONNS, lgr->max_conns))\n\t\tgoto errv2attr;\n\tif (nla_put_u8(skb, SMC_NLA_LGR_R_V2_MAX_LINKS, lgr->max_links))\n\t\tgoto errv2attr;\n\n\tnla_nest_end(skb, v2_attrs);\n\treturn 0;\n\nerrv2attr:\n\tnla_nest_cancel(skb, v2_attrs);\nerrattr:\n\treturn -EMSGSIZE;\n}\n\nstatic int smc_nl_fill_lgr(struct smc_link_group *lgr,\n\t\t\t   struct sk_buff *skb,\n\t\t\t   struct netlink_callback *cb)\n{\n\tchar smc_target[SMC_MAX_PNETID_LEN + 1];\n\tstruct nlattr *attrs, *v2_attrs;\n\n\tattrs = nla_nest_start(skb, SMC_GEN_LGR_SMCR);\n\tif (!attrs)\n\t\tgoto errout;\n\n\tif (nla_put_u32(skb, SMC_NLA_LGR_R_ID, *((u32 *)&lgr->id)))\n\t\tgoto errattr;\n\tif (nla_put_u32(skb, SMC_NLA_LGR_R_CONNS_NUM, lgr->conns_num))\n\t\tgoto errattr;\n\tif (nla_put_u8(skb, SMC_NLA_LGR_R_ROLE, lgr->role))\n\t\tgoto errattr;\n\tif (nla_put_u8(skb, SMC_NLA_LGR_R_TYPE, lgr->type))\n\t\tgoto errattr;\n\tif (nla_put_u8(skb, SMC_NLA_LGR_R_BUF_TYPE, lgr->buf_type))\n\t\tgoto errattr;\n\tif (nla_put_u8(skb, SMC_NLA_LGR_R_VLAN_ID, lgr->vlan_id))\n\t\tgoto errattr;\n\tif (nla_put_u64_64bit(skb, SMC_NLA_LGR_R_NET_COOKIE,\n\t\t\t      lgr->net->net_cookie, SMC_NLA_LGR_R_PAD))\n\t\tgoto errattr;\n\tmemcpy(smc_target, lgr->pnet_id, SMC_MAX_PNETID_LEN);\n\tsmc_target[SMC_MAX_PNETID_LEN] = 0;\n\tif (nla_put_string(skb, SMC_NLA_LGR_R_PNETID, smc_target))\n\t\tgoto errattr;\n\tif (lgr->smc_version > SMC_V1) {\n\t\tv2_attrs = nla_nest_start(skb, SMC_NLA_LGR_R_V2_COMMON);\n\t\tif (!v2_attrs)\n\t\t\tgoto errattr;\n\t\tif (smc_nl_fill_lgr_v2_common(lgr, skb, cb, v2_attrs))\n\t\t\tgoto errattr;\n\t\tif (smc_nl_fill_smcr_lgr_v2(lgr, skb, cb))\n\t\t\tgoto errattr;\n\t}\n\n\tnla_nest_end(skb, attrs);\n\treturn 0;\nerrattr:\n\tnla_nest_cancel(skb, attrs);\nerrout:\n\treturn -EMSGSIZE;\n}\n\nstatic int smc_nl_fill_lgr_link(struct smc_link_group *lgr,\n\t\t\t\tstruct smc_link *link,\n\t\t\t\tstruct sk_buff *skb,\n\t\t\t\tstruct netlink_callback *cb)\n{\n\tchar smc_ibname[IB_DEVICE_NAME_MAX];\n\tu8 smc_gid_target[41];\n\tstruct nlattr *attrs;\n\tu32 link_uid = 0;\n\tvoid *nlh;\n\n\tnlh = genlmsg_put(skb, NETLINK_CB(cb->skb).portid, cb->nlh->nlmsg_seq,\n\t\t\t  &smc_gen_nl_family, NLM_F_MULTI,\n\t\t\t  SMC_NETLINK_GET_LINK_SMCR);\n\tif (!nlh)\n\t\tgoto errmsg;\n\n\tattrs = nla_nest_start(skb, SMC_GEN_LINK_SMCR);\n\tif (!attrs)\n\t\tgoto errout;\n\n\tif (nla_put_u8(skb, SMC_NLA_LINK_ID, link->link_id))\n\t\tgoto errattr;\n\tif (nla_put_u32(skb, SMC_NLA_LINK_STATE, link->state))\n\t\tgoto errattr;\n\tif (nla_put_u32(skb, SMC_NLA_LINK_CONN_CNT,\n\t\t\tatomic_read(&link->conn_cnt)))\n\t\tgoto errattr;\n\tif (nla_put_u8(skb, SMC_NLA_LINK_IB_PORT, link->ibport))\n\t\tgoto errattr;\n\tif (nla_put_u32(skb, SMC_NLA_LINK_NET_DEV, link->ndev_ifidx))\n\t\tgoto errattr;\n\tsnprintf(smc_ibname, sizeof(smc_ibname), \"%s\", link->ibname);\n\tif (nla_put_string(skb, SMC_NLA_LINK_IB_DEV, smc_ibname))\n\t\tgoto errattr;\n\tmemcpy(&link_uid, link->link_uid, sizeof(link_uid));\n\tif (nla_put_u32(skb, SMC_NLA_LINK_UID, link_uid))\n\t\tgoto errattr;\n\tmemcpy(&link_uid, link->peer_link_uid, sizeof(link_uid));\n\tif (nla_put_u32(skb, SMC_NLA_LINK_PEER_UID, link_uid))\n\t\tgoto errattr;\n\tmemset(smc_gid_target, 0, sizeof(smc_gid_target));\n\tsmc_gid_be16_convert(smc_gid_target, link->gid);\n\tif (nla_put_string(skb, SMC_NLA_LINK_GID, smc_gid_target))\n\t\tgoto errattr;\n\tmemset(smc_gid_target, 0, sizeof(smc_gid_target));\n\tsmc_gid_be16_convert(smc_gid_target, link->peer_gid);\n\tif (nla_put_string(skb, SMC_NLA_LINK_PEER_GID, smc_gid_target))\n\t\tgoto errattr;\n\n\tnla_nest_end(skb, attrs);\n\tgenlmsg_end(skb, nlh);\n\treturn 0;\nerrattr:\n\tnla_nest_cancel(skb, attrs);\nerrout:\n\tgenlmsg_cancel(skb, nlh);\nerrmsg:\n\treturn -EMSGSIZE;\n}\n\nstatic int smc_nl_handle_lgr(struct smc_link_group *lgr,\n\t\t\t     struct sk_buff *skb,\n\t\t\t     struct netlink_callback *cb,\n\t\t\t     bool list_links)\n{\n\tvoid *nlh;\n\tint i;\n\n\tnlh = genlmsg_put(skb, NETLINK_CB(cb->skb).portid, cb->nlh->nlmsg_seq,\n\t\t\t  &smc_gen_nl_family, NLM_F_MULTI,\n\t\t\t  SMC_NETLINK_GET_LGR_SMCR);\n\tif (!nlh)\n\t\tgoto errmsg;\n\tif (smc_nl_fill_lgr(lgr, skb, cb))\n\t\tgoto errout;\n\n\tgenlmsg_end(skb, nlh);\n\tif (!list_links)\n\t\tgoto out;\n\tfor (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++) {\n\t\tif (!smc_link_usable(&lgr->lnk[i]))\n\t\t\tcontinue;\n\t\tif (smc_nl_fill_lgr_link(lgr, &lgr->lnk[i], skb, cb))\n\t\t\tgoto errout;\n\t}\nout:\n\treturn 0;\n\nerrout:\n\tgenlmsg_cancel(skb, nlh);\nerrmsg:\n\treturn -EMSGSIZE;\n}\n\nstatic void smc_nl_fill_lgr_list(struct smc_lgr_list *smc_lgr,\n\t\t\t\t struct sk_buff *skb,\n\t\t\t\t struct netlink_callback *cb,\n\t\t\t\t bool list_links)\n{\n\tstruct smc_nl_dmp_ctx *cb_ctx = smc_nl_dmp_ctx(cb);\n\tstruct smc_link_group *lgr;\n\tint snum = cb_ctx->pos[0];\n\tint num = 0;\n\n\tspin_lock_bh(&smc_lgr->lock);\n\tlist_for_each_entry(lgr, &smc_lgr->list, list) {\n\t\tif (num < snum)\n\t\t\tgoto next;\n\t\tif (smc_nl_handle_lgr(lgr, skb, cb, list_links))\n\t\t\tgoto errout;\nnext:\n\t\tnum++;\n\t}\nerrout:\n\tspin_unlock_bh(&smc_lgr->lock);\n\tcb_ctx->pos[0] = num;\n}\n\nstatic int smc_nl_fill_smcd_lgr(struct smc_link_group *lgr,\n\t\t\t\tstruct sk_buff *skb,\n\t\t\t\tstruct netlink_callback *cb)\n{\n\tchar smc_pnet[SMC_MAX_PNETID_LEN + 1];\n\tstruct smcd_dev *smcd = lgr->smcd;\n\tstruct nlattr *attrs;\n\tvoid *nlh;\n\n\tnlh = genlmsg_put(skb, NETLINK_CB(cb->skb).portid, cb->nlh->nlmsg_seq,\n\t\t\t  &smc_gen_nl_family, NLM_F_MULTI,\n\t\t\t  SMC_NETLINK_GET_LGR_SMCD);\n\tif (!nlh)\n\t\tgoto errmsg;\n\n\tattrs = nla_nest_start(skb, SMC_GEN_LGR_SMCD);\n\tif (!attrs)\n\t\tgoto errout;\n\n\tif (nla_put_u32(skb, SMC_NLA_LGR_D_ID, *((u32 *)&lgr->id)))\n\t\tgoto errattr;\n\tif (nla_put_u64_64bit(skb, SMC_NLA_LGR_D_GID,\n\t\t\t      smcd->ops->get_local_gid(smcd),\n\t\t\t\t  SMC_NLA_LGR_D_PAD))\n\t\tgoto errattr;\n\tif (nla_put_u64_64bit(skb, SMC_NLA_LGR_D_PEER_GID, lgr->peer_gid,\n\t\t\t      SMC_NLA_LGR_D_PAD))\n\t\tgoto errattr;\n\tif (nla_put_u8(skb, SMC_NLA_LGR_D_VLAN_ID, lgr->vlan_id))\n\t\tgoto errattr;\n\tif (nla_put_u32(skb, SMC_NLA_LGR_D_CONNS_NUM, lgr->conns_num))\n\t\tgoto errattr;\n\tif (nla_put_u32(skb, SMC_NLA_LGR_D_CHID, smc_ism_get_chid(lgr->smcd)))\n\t\tgoto errattr;\n\tmemcpy(smc_pnet, lgr->smcd->pnetid, SMC_MAX_PNETID_LEN);\n\tsmc_pnet[SMC_MAX_PNETID_LEN] = 0;\n\tif (nla_put_string(skb, SMC_NLA_LGR_D_PNETID, smc_pnet))\n\t\tgoto errattr;\n\tif (lgr->smc_version > SMC_V1) {\n\t\tstruct nlattr *v2_attrs;\n\n\t\tv2_attrs = nla_nest_start(skb, SMC_NLA_LGR_D_V2_COMMON);\n\t\tif (!v2_attrs)\n\t\t\tgoto errattr;\n\t\tif (smc_nl_fill_lgr_v2_common(lgr, skb, cb, v2_attrs))\n\t\t\tgoto errattr;\n\t}\n\tnla_nest_end(skb, attrs);\n\tgenlmsg_end(skb, nlh);\n\treturn 0;\n\nerrattr:\n\tnla_nest_cancel(skb, attrs);\nerrout:\n\tgenlmsg_cancel(skb, nlh);\nerrmsg:\n\treturn -EMSGSIZE;\n}\n\nstatic int smc_nl_handle_smcd_lgr(struct smcd_dev *dev,\n\t\t\t\t  struct sk_buff *skb,\n\t\t\t\t  struct netlink_callback *cb)\n{\n\tstruct smc_nl_dmp_ctx *cb_ctx = smc_nl_dmp_ctx(cb);\n\tstruct smc_link_group *lgr;\n\tint snum = cb_ctx->pos[1];\n\tint rc = 0, num = 0;\n\n\tspin_lock_bh(&dev->lgr_lock);\n\tlist_for_each_entry(lgr, &dev->lgr_list, list) {\n\t\tif (!lgr->is_smcd)\n\t\t\tcontinue;\n\t\tif (num < snum)\n\t\t\tgoto next;\n\t\trc = smc_nl_fill_smcd_lgr(lgr, skb, cb);\n\t\tif (rc)\n\t\t\tgoto errout;\nnext:\n\t\tnum++;\n\t}\nerrout:\n\tspin_unlock_bh(&dev->lgr_lock);\n\tcb_ctx->pos[1] = num;\n\treturn rc;\n}\n\nstatic int smc_nl_fill_smcd_dev(struct smcd_dev_list *dev_list,\n\t\t\t\tstruct sk_buff *skb,\n\t\t\t\tstruct netlink_callback *cb)\n{\n\tstruct smc_nl_dmp_ctx *cb_ctx = smc_nl_dmp_ctx(cb);\n\tstruct smcd_dev *smcd_dev;\n\tint snum = cb_ctx->pos[0];\n\tint rc = 0, num = 0;\n\n\tmutex_lock(&dev_list->mutex);\n\tlist_for_each_entry(smcd_dev, &dev_list->list, list) {\n\t\tif (list_empty(&smcd_dev->lgr_list))\n\t\t\tcontinue;\n\t\tif (num < snum)\n\t\t\tgoto next;\n\t\trc = smc_nl_handle_smcd_lgr(smcd_dev, skb, cb);\n\t\tif (rc)\n\t\t\tgoto errout;\nnext:\n\t\tnum++;\n\t}\nerrout:\n\tmutex_unlock(&dev_list->mutex);\n\tcb_ctx->pos[0] = num;\n\treturn rc;\n}\n\nint smcr_nl_get_lgr(struct sk_buff *skb, struct netlink_callback *cb)\n{\n\tbool list_links = false;\n\n\tsmc_nl_fill_lgr_list(&smc_lgr_list, skb, cb, list_links);\n\treturn skb->len;\n}\n\nint smcr_nl_get_link(struct sk_buff *skb, struct netlink_callback *cb)\n{\n\tbool list_links = true;\n\n\tsmc_nl_fill_lgr_list(&smc_lgr_list, skb, cb, list_links);\n\treturn skb->len;\n}\n\nint smcd_nl_get_lgr(struct sk_buff *skb, struct netlink_callback *cb)\n{\n\tsmc_nl_fill_smcd_dev(&smcd_dev_list, skb, cb);\n\treturn skb->len;\n}\n\nvoid smc_lgr_cleanup_early(struct smc_link_group *lgr)\n{\n\tspinlock_t *lgr_lock;\n\n\tif (!lgr)\n\t\treturn;\n\n\tsmc_lgr_list_head(lgr, &lgr_lock);\n\tspin_lock_bh(lgr_lock);\n\t \n\tif (!list_empty(&lgr->list))\n\t\tlist_del_init(&lgr->list);\n\tspin_unlock_bh(lgr_lock);\n\t__smc_lgr_terminate(lgr, true);\n}\n\nstatic void smcr_lgr_link_deactivate_all(struct smc_link_group *lgr)\n{\n\tint i;\n\n\tfor (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++) {\n\t\tstruct smc_link *lnk = &lgr->lnk[i];\n\n\t\tif (smc_link_sendable(lnk))\n\t\t\tlnk->state = SMC_LNK_INACTIVE;\n\t}\n\twake_up_all(&lgr->llc_msg_waiter);\n\twake_up_all(&lgr->llc_flow_waiter);\n}\n\nstatic void smc_lgr_free(struct smc_link_group *lgr);\n\nstatic void smc_lgr_free_work(struct work_struct *work)\n{\n\tstruct smc_link_group *lgr = container_of(to_delayed_work(work),\n\t\t\t\t\t\t  struct smc_link_group,\n\t\t\t\t\t\t  free_work);\n\tspinlock_t *lgr_lock;\n\tbool conns;\n\n\tsmc_lgr_list_head(lgr, &lgr_lock);\n\tspin_lock_bh(lgr_lock);\n\tif (lgr->freeing) {\n\t\tspin_unlock_bh(lgr_lock);\n\t\treturn;\n\t}\n\tread_lock_bh(&lgr->conns_lock);\n\tconns = RB_EMPTY_ROOT(&lgr->conns_all);\n\tread_unlock_bh(&lgr->conns_lock);\n\tif (!conns) {  \n\t\tspin_unlock_bh(lgr_lock);\n\t\treturn;\n\t}\n\tlist_del_init(&lgr->list);  \n\tlgr->freeing = 1;  \n\tspin_unlock_bh(lgr_lock);\n\tcancel_delayed_work(&lgr->free_work);\n\n\tif (!lgr->is_smcd && !lgr->terminating)\n\t\tsmc_llc_send_link_delete_all(lgr, true,\n\t\t\t\t\t     SMC_LLC_DEL_PROG_INIT_TERM);\n\tif (lgr->is_smcd && !lgr->terminating)\n\t\tsmc_ism_signal_shutdown(lgr);\n\tif (!lgr->is_smcd)\n\t\tsmcr_lgr_link_deactivate_all(lgr);\n\tsmc_lgr_free(lgr);\n}\n\nstatic void smc_lgr_terminate_work(struct work_struct *work)\n{\n\tstruct smc_link_group *lgr = container_of(work, struct smc_link_group,\n\t\t\t\t\t\t  terminate_work);\n\n\t__smc_lgr_terminate(lgr, true);\n}\n\n \nstatic u8 smcr_next_link_id(struct smc_link_group *lgr)\n{\n\tu8 link_id;\n\tint i;\n\n\twhile (1) {\nagain:\n\t\tlink_id = ++lgr->next_link_id;\n\t\tif (!link_id)\t \n\t\t\tlink_id = ++lgr->next_link_id;\n\t\tfor (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++) {\n\t\t\tif (smc_link_usable(&lgr->lnk[i]) &&\n\t\t\t    lgr->lnk[i].link_id == link_id)\n\t\t\t\tgoto again;\n\t\t}\n\t\tbreak;\n\t}\n\treturn link_id;\n}\n\nstatic void smcr_copy_dev_info_to_link(struct smc_link *link)\n{\n\tstruct smc_ib_device *smcibdev = link->smcibdev;\n\n\tsnprintf(link->ibname, sizeof(link->ibname), \"%s\",\n\t\t smcibdev->ibdev->name);\n\tlink->ndev_ifidx = smcibdev->ndev_ifidx[link->ibport - 1];\n}\n\nint smcr_link_init(struct smc_link_group *lgr, struct smc_link *lnk,\n\t\t   u8 link_idx, struct smc_init_info *ini)\n{\n\tstruct smc_ib_device *smcibdev;\n\tu8 rndvec[3];\n\tint rc;\n\n\tif (lgr->smc_version == SMC_V2) {\n\t\tlnk->smcibdev = ini->smcrv2.ib_dev_v2;\n\t\tlnk->ibport = ini->smcrv2.ib_port_v2;\n\t} else {\n\t\tlnk->smcibdev = ini->ib_dev;\n\t\tlnk->ibport = ini->ib_port;\n\t}\n\tget_device(&lnk->smcibdev->ibdev->dev);\n\tatomic_inc(&lnk->smcibdev->lnk_cnt);\n\trefcount_set(&lnk->refcnt, 1);  \n\tlnk->clearing = 0;\n\tlnk->path_mtu = lnk->smcibdev->pattr[lnk->ibport - 1].active_mtu;\n\tlnk->link_id = smcr_next_link_id(lgr);\n\tlnk->lgr = lgr;\n\tsmc_lgr_hold(lgr);  \n\tlnk->link_idx = link_idx;\n\tlnk->wr_rx_id_compl = 0;\n\tsmc_ibdev_cnt_inc(lnk);\n\tsmcr_copy_dev_info_to_link(lnk);\n\tatomic_set(&lnk->conn_cnt, 0);\n\tsmc_llc_link_set_uid(lnk);\n\tINIT_WORK(&lnk->link_down_wrk, smc_link_down_work);\n\tif (!lnk->smcibdev->initialized) {\n\t\trc = (int)smc_ib_setup_per_ibdev(lnk->smcibdev);\n\t\tif (rc)\n\t\t\tgoto out;\n\t}\n\tget_random_bytes(rndvec, sizeof(rndvec));\n\tlnk->psn_initial = rndvec[0] + (rndvec[1] << 8) +\n\t\t(rndvec[2] << 16);\n\trc = smc_ib_determine_gid(lnk->smcibdev, lnk->ibport,\n\t\t\t\t  ini->vlan_id, lnk->gid, &lnk->sgid_index,\n\t\t\t\t  lgr->smc_version == SMC_V2 ?\n\t\t\t\t\t\t  &ini->smcrv2 : NULL);\n\tif (rc)\n\t\tgoto out;\n\trc = smc_llc_link_init(lnk);\n\tif (rc)\n\t\tgoto out;\n\trc = smc_wr_alloc_link_mem(lnk);\n\tif (rc)\n\t\tgoto clear_llc_lnk;\n\trc = smc_ib_create_protection_domain(lnk);\n\tif (rc)\n\t\tgoto free_link_mem;\n\trc = smc_ib_create_queue_pair(lnk);\n\tif (rc)\n\t\tgoto dealloc_pd;\n\trc = smc_wr_create_link(lnk);\n\tif (rc)\n\t\tgoto destroy_qp;\n\tlnk->state = SMC_LNK_ACTIVATING;\n\treturn 0;\n\ndestroy_qp:\n\tsmc_ib_destroy_queue_pair(lnk);\ndealloc_pd:\n\tsmc_ib_dealloc_protection_domain(lnk);\nfree_link_mem:\n\tsmc_wr_free_link_mem(lnk);\nclear_llc_lnk:\n\tsmc_llc_link_clear(lnk, false);\nout:\n\tsmc_ibdev_cnt_dec(lnk);\n\tput_device(&lnk->smcibdev->ibdev->dev);\n\tsmcibdev = lnk->smcibdev;\n\tmemset(lnk, 0, sizeof(struct smc_link));\n\tlnk->state = SMC_LNK_UNUSED;\n\tif (!atomic_dec_return(&smcibdev->lnk_cnt))\n\t\twake_up(&smcibdev->lnks_deleted);\n\tsmc_lgr_put(lgr);  \n\treturn rc;\n}\n\n \nstatic int smc_lgr_create(struct smc_sock *smc, struct smc_init_info *ini)\n{\n\tstruct smc_link_group *lgr;\n\tstruct list_head *lgr_list;\n\tstruct smcd_dev *smcd;\n\tstruct smc_link *lnk;\n\tspinlock_t *lgr_lock;\n\tu8 link_idx;\n\tint rc = 0;\n\tint i;\n\n\tif (ini->is_smcd && ini->vlan_id) {\n\t\tif (smc_ism_get_vlan(ini->ism_dev[ini->ism_selected],\n\t\t\t\t     ini->vlan_id)) {\n\t\t\trc = SMC_CLC_DECL_ISMVLANERR;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tlgr = kzalloc(sizeof(*lgr), GFP_KERNEL);\n\tif (!lgr) {\n\t\trc = SMC_CLC_DECL_MEM;\n\t\tgoto ism_put_vlan;\n\t}\n\tlgr->tx_wq = alloc_workqueue(\"smc_tx_wq-%*phN\", 0, 0,\n\t\t\t\t     SMC_LGR_ID_SIZE, &lgr->id);\n\tif (!lgr->tx_wq) {\n\t\trc = -ENOMEM;\n\t\tgoto free_lgr;\n\t}\n\tlgr->is_smcd = ini->is_smcd;\n\tlgr->sync_err = 0;\n\tlgr->terminating = 0;\n\tlgr->freeing = 0;\n\tlgr->vlan_id = ini->vlan_id;\n\trefcount_set(&lgr->refcnt, 1);  \n\tinit_rwsem(&lgr->sndbufs_lock);\n\tinit_rwsem(&lgr->rmbs_lock);\n\trwlock_init(&lgr->conns_lock);\n\tfor (i = 0; i < SMC_RMBE_SIZES; i++) {\n\t\tINIT_LIST_HEAD(&lgr->sndbufs[i]);\n\t\tINIT_LIST_HEAD(&lgr->rmbs[i]);\n\t}\n\tlgr->next_link_id = 0;\n\tsmc_lgr_list.num += SMC_LGR_NUM_INCR;\n\tmemcpy(&lgr->id, (u8 *)&smc_lgr_list.num, SMC_LGR_ID_SIZE);\n\tINIT_DELAYED_WORK(&lgr->free_work, smc_lgr_free_work);\n\tINIT_WORK(&lgr->terminate_work, smc_lgr_terminate_work);\n\tlgr->conns_all = RB_ROOT;\n\tif (ini->is_smcd) {\n\t\t \n\t\tsmcd = ini->ism_dev[ini->ism_selected];\n\t\tget_device(smcd->ops->get_dev(smcd));\n\t\tlgr->peer_gid = ini->ism_peer_gid[ini->ism_selected];\n\t\tlgr->smcd = ini->ism_dev[ini->ism_selected];\n\t\tlgr_list = &ini->ism_dev[ini->ism_selected]->lgr_list;\n\t\tlgr_lock = &lgr->smcd->lgr_lock;\n\t\tlgr->smc_version = ini->smcd_version;\n\t\tlgr->peer_shutdown = 0;\n\t\tatomic_inc(&ini->ism_dev[ini->ism_selected]->lgr_cnt);\n\t} else {\n\t\t \n\t\tstruct smc_ib_device *ibdev;\n\t\tint ibport;\n\n\t\tlgr->role = smc->listen_smc ? SMC_SERV : SMC_CLNT;\n\t\tlgr->smc_version = ini->smcr_version;\n\t\tmemcpy(lgr->peer_systemid, ini->peer_systemid,\n\t\t       SMC_SYSTEMID_LEN);\n\t\tif (lgr->smc_version == SMC_V2) {\n\t\t\tibdev = ini->smcrv2.ib_dev_v2;\n\t\t\tibport = ini->smcrv2.ib_port_v2;\n\t\t\tlgr->saddr = ini->smcrv2.saddr;\n\t\t\tlgr->uses_gateway = ini->smcrv2.uses_gateway;\n\t\t\tmemcpy(lgr->nexthop_mac, ini->smcrv2.nexthop_mac,\n\t\t\t       ETH_ALEN);\n\t\t\tlgr->max_conns = ini->max_conns;\n\t\t\tlgr->max_links = ini->max_links;\n\t\t} else {\n\t\t\tibdev = ini->ib_dev;\n\t\t\tibport = ini->ib_port;\n\t\t\tlgr->max_conns = SMC_CONN_PER_LGR_MAX;\n\t\t\tlgr->max_links = SMC_LINKS_ADD_LNK_MAX;\n\t\t}\n\t\tmemcpy(lgr->pnet_id, ibdev->pnetid[ibport - 1],\n\t\t       SMC_MAX_PNETID_LEN);\n\t\trc = smc_wr_alloc_lgr_mem(lgr);\n\t\tif (rc)\n\t\t\tgoto free_wq;\n\t\tsmc_llc_lgr_init(lgr, smc);\n\n\t\tlink_idx = SMC_SINGLE_LINK;\n\t\tlnk = &lgr->lnk[link_idx];\n\t\trc = smcr_link_init(lgr, lnk, link_idx, ini);\n\t\tif (rc) {\n\t\t\tsmc_wr_free_lgr_mem(lgr);\n\t\t\tgoto free_wq;\n\t\t}\n\t\tlgr->net = smc_ib_net(lnk->smcibdev);\n\t\tlgr_list = &smc_lgr_list.list;\n\t\tlgr_lock = &smc_lgr_list.lock;\n\t\tlgr->buf_type = lgr->net->smc.sysctl_smcr_buf_type;\n\t\tatomic_inc(&lgr_cnt);\n\t}\n\tsmc->conn.lgr = lgr;\n\tspin_lock_bh(lgr_lock);\n\tlist_add_tail(&lgr->list, lgr_list);\n\tspin_unlock_bh(lgr_lock);\n\treturn 0;\n\nfree_wq:\n\tdestroy_workqueue(lgr->tx_wq);\nfree_lgr:\n\tkfree(lgr);\nism_put_vlan:\n\tif (ini->is_smcd && ini->vlan_id)\n\t\tsmc_ism_put_vlan(ini->ism_dev[ini->ism_selected], ini->vlan_id);\nout:\n\tif (rc < 0) {\n\t\tif (rc == -ENOMEM)\n\t\t\trc = SMC_CLC_DECL_MEM;\n\t\telse\n\t\t\trc = SMC_CLC_DECL_INTERR;\n\t}\n\treturn rc;\n}\n\nstatic int smc_write_space(struct smc_connection *conn)\n{\n\tint buffer_len = conn->peer_rmbe_size;\n\tunion smc_host_cursor prod;\n\tunion smc_host_cursor cons;\n\tint space;\n\n\tsmc_curs_copy(&prod, &conn->local_tx_ctrl.prod, conn);\n\tsmc_curs_copy(&cons, &conn->local_rx_ctrl.cons, conn);\n\t \n\tspace = buffer_len - smc_curs_diff(buffer_len, &cons, &prod);\n\treturn space;\n}\n\nstatic int smc_switch_cursor(struct smc_sock *smc, struct smc_cdc_tx_pend *pend,\n\t\t\t     struct smc_wr_buf *wr_buf)\n{\n\tstruct smc_connection *conn = &smc->conn;\n\tunion smc_host_cursor cons, fin;\n\tint rc = 0;\n\tint diff;\n\n\tsmc_curs_copy(&conn->tx_curs_sent, &conn->tx_curs_fin, conn);\n\tsmc_curs_copy(&fin, &conn->local_tx_ctrl_fin, conn);\n\t \n\tsmc_curs_copy(&conn->local_tx_ctrl.prod, &fin, conn);\n\tsmc_curs_copy(&cons, &conn->local_rx_ctrl.cons, conn);\n\n\tif (smc_curs_comp(conn->peer_rmbe_size, &cons, &fin) < 0) {\n\t\t \n\t\tdiff = smc_curs_diff(conn->peer_rmbe_size, &fin, &cons);\n\t\tsmc_curs_add(conn->sndbuf_desc->len,\n\t\t\t     &conn->tx_curs_sent, diff);\n\t\tsmc_curs_add(conn->sndbuf_desc->len,\n\t\t\t     &conn->tx_curs_fin, diff);\n\n\t\tsmp_mb__before_atomic();\n\t\tatomic_add(diff, &conn->sndbuf_space);\n\t\tsmp_mb__after_atomic();\n\n\t\tsmc_curs_add(conn->peer_rmbe_size,\n\t\t\t     &conn->local_tx_ctrl.prod, diff);\n\t\tsmc_curs_add(conn->peer_rmbe_size,\n\t\t\t     &conn->local_tx_ctrl_fin, diff);\n\t}\n\t \n\tatomic_set(&smc->conn.peer_rmbe_space, smc_write_space(conn));\n\n\tif (smc->sk.sk_state != SMC_INIT &&\n\t    smc->sk.sk_state != SMC_CLOSED) {\n\t\trc = smcr_cdc_msg_send_validation(conn, pend, wr_buf);\n\t\tif (!rc) {\n\t\t\tqueue_delayed_work(conn->lgr->tx_wq, &conn->tx_work, 0);\n\t\t\tsmc->sk.sk_data_ready(&smc->sk);\n\t\t}\n\t} else {\n\t\tsmc_wr_tx_put_slot(conn->lnk,\n\t\t\t\t   (struct smc_wr_tx_pend_priv *)pend);\n\t}\n\treturn rc;\n}\n\nvoid smc_switch_link_and_count(struct smc_connection *conn,\n\t\t\t       struct smc_link *to_lnk)\n{\n\tatomic_dec(&conn->lnk->conn_cnt);\n\t \n\tsmcr_link_put(conn->lnk);\n\tconn->lnk = to_lnk;\n\tatomic_inc(&conn->lnk->conn_cnt);\n\t \n\tsmcr_link_hold(conn->lnk);\n}\n\nstruct smc_link *smc_switch_conns(struct smc_link_group *lgr,\n\t\t\t\t  struct smc_link *from_lnk, bool is_dev_err)\n{\n\tstruct smc_link *to_lnk = NULL;\n\tstruct smc_cdc_tx_pend *pend;\n\tstruct smc_connection *conn;\n\tstruct smc_wr_buf *wr_buf;\n\tstruct smc_sock *smc;\n\tstruct rb_node *node;\n\tint i, rc = 0;\n\n\t \n\tsmc_wr_wakeup_tx_wait(from_lnk);\n\n\tfor (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++) {\n\t\tif (!smc_link_active(&lgr->lnk[i]) || i == from_lnk->link_idx)\n\t\t\tcontinue;\n\t\tif (is_dev_err && from_lnk->smcibdev == lgr->lnk[i].smcibdev &&\n\t\t    from_lnk->ibport == lgr->lnk[i].ibport) {\n\t\t\tcontinue;\n\t\t}\n\t\tto_lnk = &lgr->lnk[i];\n\t\tbreak;\n\t}\n\tif (!to_lnk || !smc_wr_tx_link_hold(to_lnk)) {\n\t\tsmc_lgr_terminate_sched(lgr);\n\t\treturn NULL;\n\t}\nagain:\n\tread_lock_bh(&lgr->conns_lock);\n\tfor (node = rb_first(&lgr->conns_all); node; node = rb_next(node)) {\n\t\tconn = rb_entry(node, struct smc_connection, alert_node);\n\t\tif (conn->lnk != from_lnk)\n\t\t\tcontinue;\n\t\tsmc = container_of(conn, struct smc_sock, conn);\n\t\t \n\t\tif (smc->sk.sk_state == SMC_INIT)\n\t\t\tcontinue;\n\t\tif (smc->sk.sk_state == SMC_CLOSED ||\n\t\t    smc->sk.sk_state == SMC_PEERCLOSEWAIT1 ||\n\t\t    smc->sk.sk_state == SMC_PEERCLOSEWAIT2 ||\n\t\t    smc->sk.sk_state == SMC_APPFINCLOSEWAIT ||\n\t\t    smc->sk.sk_state == SMC_APPCLOSEWAIT1 ||\n\t\t    smc->sk.sk_state == SMC_APPCLOSEWAIT2 ||\n\t\t    smc->sk.sk_state == SMC_PEERFINCLOSEWAIT ||\n\t\t    smc->sk.sk_state == SMC_PEERABORTWAIT ||\n\t\t    smc->sk.sk_state == SMC_PROCESSABORT) {\n\t\t\tspin_lock_bh(&conn->send_lock);\n\t\t\tsmc_switch_link_and_count(conn, to_lnk);\n\t\t\tspin_unlock_bh(&conn->send_lock);\n\t\t\tcontinue;\n\t\t}\n\t\tsock_hold(&smc->sk);\n\t\tread_unlock_bh(&lgr->conns_lock);\n\t\t \n\t\trc = smc_cdc_get_free_slot(conn, to_lnk, &wr_buf, NULL, &pend);\n\t\tif (rc)\n\t\t\tgoto err_out;\n\t\t \n\t\tspin_lock_bh(&conn->send_lock);\n\t\tsmc_switch_link_and_count(conn, to_lnk);\n\t\trc = smc_switch_cursor(smc, pend, wr_buf);\n\t\tspin_unlock_bh(&conn->send_lock);\n\t\tsock_put(&smc->sk);\n\t\tif (rc)\n\t\t\tgoto err_out;\n\t\tgoto again;\n\t}\n\tread_unlock_bh(&lgr->conns_lock);\n\tsmc_wr_tx_link_put(to_lnk);\n\treturn to_lnk;\n\nerr_out:\n\tsmcr_link_down_cond_sched(to_lnk);\n\tsmc_wr_tx_link_put(to_lnk);\n\treturn NULL;\n}\n\nstatic void smcr_buf_unuse(struct smc_buf_desc *buf_desc, bool is_rmb,\n\t\t\t   struct smc_link_group *lgr)\n{\n\tstruct rw_semaphore *lock;\t \n\tint rc;\n\n\tif (is_rmb && buf_desc->is_conf_rkey && !list_empty(&lgr->list)) {\n\t\t \n\t\trc = smc_llc_flow_initiate(lgr, SMC_LLC_FLOW_RKEY);\n\t\tif (!rc) {\n\t\t\t \n\t\t\tdown_read(&lgr->llc_conf_mutex);\n\t\t\tsmc_llc_do_delete_rkey(lgr, buf_desc);\n\t\t\tbuf_desc->is_conf_rkey = false;\n\t\t\tup_read(&lgr->llc_conf_mutex);\n\t\t\tsmc_llc_flow_stop(lgr, &lgr->llc_flow_lcl);\n\t\t}\n\t}\n\n\tif (buf_desc->is_reg_err) {\n\t\t \n\t\tlock = is_rmb ? &lgr->rmbs_lock :\n\t\t\t\t&lgr->sndbufs_lock;\n\t\tdown_write(lock);\n\t\tlist_del(&buf_desc->list);\n\t\tup_write(lock);\n\n\t\tsmc_buf_free(lgr, is_rmb, buf_desc);\n\t} else {\n\t\t \n\t\tmemzero_explicit(buf_desc->cpu_addr, buf_desc->len);\n\t\tWRITE_ONCE(buf_desc->used, 0);\n\t}\n}\n\nstatic void smc_buf_unuse(struct smc_connection *conn,\n\t\t\t  struct smc_link_group *lgr)\n{\n\tif (conn->sndbuf_desc) {\n\t\tif (!lgr->is_smcd && conn->sndbuf_desc->is_vm) {\n\t\t\tsmcr_buf_unuse(conn->sndbuf_desc, false, lgr);\n\t\t} else {\n\t\t\tmemzero_explicit(conn->sndbuf_desc->cpu_addr, conn->sndbuf_desc->len);\n\t\t\tWRITE_ONCE(conn->sndbuf_desc->used, 0);\n\t\t}\n\t}\n\tif (conn->rmb_desc) {\n\t\tif (!lgr->is_smcd) {\n\t\t\tsmcr_buf_unuse(conn->rmb_desc, true, lgr);\n\t\t} else {\n\t\t\tmemzero_explicit(conn->rmb_desc->cpu_addr,\n\t\t\t\t\t conn->rmb_desc->len + sizeof(struct smcd_cdc_msg));\n\t\t\tWRITE_ONCE(conn->rmb_desc->used, 0);\n\t\t}\n\t}\n}\n\n \nvoid smc_conn_free(struct smc_connection *conn)\n{\n\tstruct smc_link_group *lgr = conn->lgr;\n\n\tif (!lgr || conn->freed)\n\t\t \n\t\treturn;\n\n\tconn->freed = 1;\n\tif (!smc_conn_lgr_valid(conn))\n\t\t \n\t\tgoto lgr_put;\n\n\tif (lgr->is_smcd) {\n\t\tif (!list_empty(&lgr->list))\n\t\t\tsmc_ism_unset_conn(conn);\n\t\ttasklet_kill(&conn->rx_tsklet);\n\t} else {\n\t\tsmc_cdc_wait_pend_tx_wr(conn);\n\t\tif (current_work() != &conn->abort_work)\n\t\t\tcancel_work_sync(&conn->abort_work);\n\t}\n\tif (!list_empty(&lgr->list)) {\n\t\tsmc_buf_unuse(conn, lgr);  \n\t\tsmc_lgr_unregister_conn(conn);\n\t}\n\n\tif (!lgr->conns_num)\n\t\tsmc_lgr_schedule_free_work(lgr);\nlgr_put:\n\tif (!lgr->is_smcd)\n\t\tsmcr_link_put(conn->lnk);  \n\tsmc_lgr_put(lgr);  \n}\n\n \nstatic void smcr_buf_unmap_link(struct smc_buf_desc *buf_desc, bool is_rmb,\n\t\t\t\tstruct smc_link *lnk)\n{\n\tif (is_rmb || buf_desc->is_vm)\n\t\tbuf_desc->is_reg_mr[lnk->link_idx] = false;\n\tif (!buf_desc->is_map_ib[lnk->link_idx])\n\t\treturn;\n\n\tif ((is_rmb || buf_desc->is_vm) &&\n\t    buf_desc->mr[lnk->link_idx]) {\n\t\tsmc_ib_put_memory_region(buf_desc->mr[lnk->link_idx]);\n\t\tbuf_desc->mr[lnk->link_idx] = NULL;\n\t}\n\tif (is_rmb)\n\t\tsmc_ib_buf_unmap_sg(lnk, buf_desc, DMA_FROM_DEVICE);\n\telse\n\t\tsmc_ib_buf_unmap_sg(lnk, buf_desc, DMA_TO_DEVICE);\n\n\tsg_free_table(&buf_desc->sgt[lnk->link_idx]);\n\tbuf_desc->is_map_ib[lnk->link_idx] = false;\n}\n\n \nstatic void smcr_buf_unmap_lgr(struct smc_link *lnk)\n{\n\tstruct smc_link_group *lgr = lnk->lgr;\n\tstruct smc_buf_desc *buf_desc, *bf;\n\tint i;\n\n\tfor (i = 0; i < SMC_RMBE_SIZES; i++) {\n\t\tdown_write(&lgr->rmbs_lock);\n\t\tlist_for_each_entry_safe(buf_desc, bf, &lgr->rmbs[i], list)\n\t\t\tsmcr_buf_unmap_link(buf_desc, true, lnk);\n\t\tup_write(&lgr->rmbs_lock);\n\n\t\tdown_write(&lgr->sndbufs_lock);\n\t\tlist_for_each_entry_safe(buf_desc, bf, &lgr->sndbufs[i],\n\t\t\t\t\t list)\n\t\t\tsmcr_buf_unmap_link(buf_desc, false, lnk);\n\t\tup_write(&lgr->sndbufs_lock);\n\t}\n}\n\nstatic void smcr_rtoken_clear_link(struct smc_link *lnk)\n{\n\tstruct smc_link_group *lgr = lnk->lgr;\n\tint i;\n\n\tfor (i = 0; i < SMC_RMBS_PER_LGR_MAX; i++) {\n\t\tlgr->rtokens[i][lnk->link_idx].rkey = 0;\n\t\tlgr->rtokens[i][lnk->link_idx].dma_addr = 0;\n\t}\n}\n\nstatic void __smcr_link_clear(struct smc_link *lnk)\n{\n\tstruct smc_link_group *lgr = lnk->lgr;\n\tstruct smc_ib_device *smcibdev;\n\n\tsmc_wr_free_link_mem(lnk);\n\tsmc_ibdev_cnt_dec(lnk);\n\tput_device(&lnk->smcibdev->ibdev->dev);\n\tsmcibdev = lnk->smcibdev;\n\tmemset(lnk, 0, sizeof(struct smc_link));\n\tlnk->state = SMC_LNK_UNUSED;\n\tif (!atomic_dec_return(&smcibdev->lnk_cnt))\n\t\twake_up(&smcibdev->lnks_deleted);\n\tsmc_lgr_put(lgr);  \n}\n\n \nvoid smcr_link_clear(struct smc_link *lnk, bool log)\n{\n\tif (!lnk->lgr || lnk->clearing ||\n\t    lnk->state == SMC_LNK_UNUSED)\n\t\treturn;\n\tlnk->clearing = 1;\n\tlnk->peer_qpn = 0;\n\tsmc_llc_link_clear(lnk, log);\n\tsmcr_buf_unmap_lgr(lnk);\n\tsmcr_rtoken_clear_link(lnk);\n\tsmc_ib_modify_qp_error(lnk);\n\tsmc_wr_free_link(lnk);\n\tsmc_ib_destroy_queue_pair(lnk);\n\tsmc_ib_dealloc_protection_domain(lnk);\n\tsmcr_link_put(lnk);  \n}\n\nvoid smcr_link_hold(struct smc_link *lnk)\n{\n\trefcount_inc(&lnk->refcnt);\n}\n\nvoid smcr_link_put(struct smc_link *lnk)\n{\n\tif (refcount_dec_and_test(&lnk->refcnt))\n\t\t__smcr_link_clear(lnk);\n}\n\nstatic void smcr_buf_free(struct smc_link_group *lgr, bool is_rmb,\n\t\t\t  struct smc_buf_desc *buf_desc)\n{\n\tint i;\n\n\tfor (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++)\n\t\tsmcr_buf_unmap_link(buf_desc, is_rmb, &lgr->lnk[i]);\n\n\tif (!buf_desc->is_vm && buf_desc->pages)\n\t\t__free_pages(buf_desc->pages, buf_desc->order);\n\telse if (buf_desc->is_vm && buf_desc->cpu_addr)\n\t\tvfree(buf_desc->cpu_addr);\n\tkfree(buf_desc);\n}\n\nstatic void smcd_buf_free(struct smc_link_group *lgr, bool is_dmb,\n\t\t\t  struct smc_buf_desc *buf_desc)\n{\n\tif (is_dmb) {\n\t\t \n\t\tbuf_desc->len += sizeof(struct smcd_cdc_msg);\n\t\tsmc_ism_unregister_dmb(lgr->smcd, buf_desc);\n\t} else {\n\t\tkfree(buf_desc->cpu_addr);\n\t}\n\tkfree(buf_desc);\n}\n\nstatic void smc_buf_free(struct smc_link_group *lgr, bool is_rmb,\n\t\t\t struct smc_buf_desc *buf_desc)\n{\n\tif (lgr->is_smcd)\n\t\tsmcd_buf_free(lgr, is_rmb, buf_desc);\n\telse\n\t\tsmcr_buf_free(lgr, is_rmb, buf_desc);\n}\n\nstatic void __smc_lgr_free_bufs(struct smc_link_group *lgr, bool is_rmb)\n{\n\tstruct smc_buf_desc *buf_desc, *bf_desc;\n\tstruct list_head *buf_list;\n\tint i;\n\n\tfor (i = 0; i < SMC_RMBE_SIZES; i++) {\n\t\tif (is_rmb)\n\t\t\tbuf_list = &lgr->rmbs[i];\n\t\telse\n\t\t\tbuf_list = &lgr->sndbufs[i];\n\t\tlist_for_each_entry_safe(buf_desc, bf_desc, buf_list,\n\t\t\t\t\t list) {\n\t\t\tlist_del(&buf_desc->list);\n\t\t\tsmc_buf_free(lgr, is_rmb, buf_desc);\n\t\t}\n\t}\n}\n\nstatic void smc_lgr_free_bufs(struct smc_link_group *lgr)\n{\n\t \n\t__smc_lgr_free_bufs(lgr, false);\n\t \n\t__smc_lgr_free_bufs(lgr, true);\n}\n\n \nstatic void __smc_lgr_free(struct smc_link_group *lgr)\n{\n\tsmc_lgr_free_bufs(lgr);\n\tif (lgr->is_smcd) {\n\t\tif (!atomic_dec_return(&lgr->smcd->lgr_cnt))\n\t\t\twake_up(&lgr->smcd->lgrs_deleted);\n\t} else {\n\t\tsmc_wr_free_lgr_mem(lgr);\n\t\tif (!atomic_dec_return(&lgr_cnt))\n\t\t\twake_up(&lgrs_deleted);\n\t}\n\tkfree(lgr);\n}\n\n \nstatic void smc_lgr_free(struct smc_link_group *lgr)\n{\n\tint i;\n\n\tif (!lgr->is_smcd) {\n\t\tdown_write(&lgr->llc_conf_mutex);\n\t\tfor (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++) {\n\t\t\tif (lgr->lnk[i].state != SMC_LNK_UNUSED)\n\t\t\t\tsmcr_link_clear(&lgr->lnk[i], false);\n\t\t}\n\t\tup_write(&lgr->llc_conf_mutex);\n\t\tsmc_llc_lgr_clear(lgr);\n\t}\n\n\tdestroy_workqueue(lgr->tx_wq);\n\tif (lgr->is_smcd) {\n\t\tsmc_ism_put_vlan(lgr->smcd, lgr->vlan_id);\n\t\tput_device(lgr->smcd->ops->get_dev(lgr->smcd));\n\t}\n\tsmc_lgr_put(lgr);  \n}\n\nvoid smc_lgr_hold(struct smc_link_group *lgr)\n{\n\trefcount_inc(&lgr->refcnt);\n}\n\nvoid smc_lgr_put(struct smc_link_group *lgr)\n{\n\tif (refcount_dec_and_test(&lgr->refcnt))\n\t\t__smc_lgr_free(lgr);\n}\n\nstatic void smc_sk_wake_ups(struct smc_sock *smc)\n{\n\tsmc->sk.sk_write_space(&smc->sk);\n\tsmc->sk.sk_data_ready(&smc->sk);\n\tsmc->sk.sk_state_change(&smc->sk);\n}\n\n \nstatic void smc_conn_kill(struct smc_connection *conn, bool soft)\n{\n\tstruct smc_sock *smc = container_of(conn, struct smc_sock, conn);\n\n\tif (conn->lgr->is_smcd && conn->lgr->peer_shutdown)\n\t\tconn->local_tx_ctrl.conn_state_flags.peer_conn_abort = 1;\n\telse\n\t\tsmc_close_abort(conn);\n\tconn->killed = 1;\n\tsmc->sk.sk_err = ECONNABORTED;\n\tsmc_sk_wake_ups(smc);\n\tif (conn->lgr->is_smcd) {\n\t\tsmc_ism_unset_conn(conn);\n\t\tif (soft)\n\t\t\ttasklet_kill(&conn->rx_tsklet);\n\t\telse\n\t\t\ttasklet_unlock_wait(&conn->rx_tsklet);\n\t} else {\n\t\tsmc_cdc_wait_pend_tx_wr(conn);\n\t}\n\tsmc_lgr_unregister_conn(conn);\n\tsmc_close_active_abort(smc);\n}\n\nstatic void smc_lgr_cleanup(struct smc_link_group *lgr)\n{\n\tif (lgr->is_smcd) {\n\t\tsmc_ism_signal_shutdown(lgr);\n\t} else {\n\t\tu32 rsn = lgr->llc_termination_rsn;\n\n\t\tif (!rsn)\n\t\t\trsn = SMC_LLC_DEL_PROG_INIT_TERM;\n\t\tsmc_llc_send_link_delete_all(lgr, false, rsn);\n\t\tsmcr_lgr_link_deactivate_all(lgr);\n\t}\n}\n\n \nstatic void __smc_lgr_terminate(struct smc_link_group *lgr, bool soft)\n{\n\tstruct smc_connection *conn;\n\tstruct smc_sock *smc;\n\tstruct rb_node *node;\n\n\tif (lgr->terminating)\n\t\treturn;\t \n\t \n\tcancel_delayed_work(&lgr->free_work);\n\tlgr->terminating = 1;\n\n\t \n\tread_lock_bh(&lgr->conns_lock);\n\tnode = rb_first(&lgr->conns_all);\n\twhile (node) {\n\t\tread_unlock_bh(&lgr->conns_lock);\n\t\tconn = rb_entry(node, struct smc_connection, alert_node);\n\t\tsmc = container_of(conn, struct smc_sock, conn);\n\t\tsock_hold(&smc->sk);  \n\t\tlock_sock(&smc->sk);\n\t\tsmc_conn_kill(conn, soft);\n\t\trelease_sock(&smc->sk);\n\t\tsock_put(&smc->sk);  \n\t\tread_lock_bh(&lgr->conns_lock);\n\t\tnode = rb_first(&lgr->conns_all);\n\t}\n\tread_unlock_bh(&lgr->conns_lock);\n\tsmc_lgr_cleanup(lgr);\n\tsmc_lgr_free(lgr);\n}\n\n \nvoid smc_lgr_terminate_sched(struct smc_link_group *lgr)\n{\n\tspinlock_t *lgr_lock;\n\n\tsmc_lgr_list_head(lgr, &lgr_lock);\n\tspin_lock_bh(lgr_lock);\n\tif (list_empty(&lgr->list) || lgr->terminating || lgr->freeing) {\n\t\tspin_unlock_bh(lgr_lock);\n\t\treturn;\t \n\t}\n\tlist_del_init(&lgr->list);\n\tlgr->freeing = 1;\n\tspin_unlock_bh(lgr_lock);\n\tschedule_work(&lgr->terminate_work);\n}\n\n \nvoid smc_smcd_terminate(struct smcd_dev *dev, u64 peer_gid, unsigned short vlan)\n{\n\tstruct smc_link_group *lgr, *l;\n\tLIST_HEAD(lgr_free_list);\n\n\t \n\tspin_lock_bh(&dev->lgr_lock);\n\tlist_for_each_entry_safe(lgr, l, &dev->lgr_list, list) {\n\t\tif ((!peer_gid || lgr->peer_gid == peer_gid) &&\n\t\t    (vlan == VLAN_VID_MASK || lgr->vlan_id == vlan)) {\n\t\t\tif (peer_gid)  \n\t\t\t\tlgr->peer_shutdown = 1;\n\t\t\tlist_move(&lgr->list, &lgr_free_list);\n\t\t\tlgr->freeing = 1;\n\t\t}\n\t}\n\tspin_unlock_bh(&dev->lgr_lock);\n\n\t \n\tlist_for_each_entry_safe(lgr, l, &lgr_free_list, list) {\n\t\tlist_del_init(&lgr->list);\n\t\tschedule_work(&lgr->terminate_work);\n\t}\n}\n\n \nvoid smc_smcd_terminate_all(struct smcd_dev *smcd)\n{\n\tstruct smc_link_group *lgr, *lg;\n\tLIST_HEAD(lgr_free_list);\n\n\tspin_lock_bh(&smcd->lgr_lock);\n\tlist_splice_init(&smcd->lgr_list, &lgr_free_list);\n\tlist_for_each_entry(lgr, &lgr_free_list, list)\n\t\tlgr->freeing = 1;\n\tspin_unlock_bh(&smcd->lgr_lock);\n\n\tlist_for_each_entry_safe(lgr, lg, &lgr_free_list, list) {\n\t\tlist_del_init(&lgr->list);\n\t\t__smc_lgr_terminate(lgr, false);\n\t}\n\n\tif (atomic_read(&smcd->lgr_cnt))\n\t\twait_event(smcd->lgrs_deleted, !atomic_read(&smcd->lgr_cnt));\n}\n\n \nvoid smc_smcr_terminate_all(struct smc_ib_device *smcibdev)\n{\n\tstruct smc_link_group *lgr, *lg;\n\tLIST_HEAD(lgr_free_list);\n\tint i;\n\n\tspin_lock_bh(&smc_lgr_list.lock);\n\tif (!smcibdev) {\n\t\tlist_splice_init(&smc_lgr_list.list, &lgr_free_list);\n\t\tlist_for_each_entry(lgr, &lgr_free_list, list)\n\t\t\tlgr->freeing = 1;\n\t} else {\n\t\tlist_for_each_entry_safe(lgr, lg, &smc_lgr_list.list, list) {\n\t\t\tfor (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++) {\n\t\t\t\tif (lgr->lnk[i].smcibdev == smcibdev)\n\t\t\t\t\tsmcr_link_down_cond_sched(&lgr->lnk[i]);\n\t\t\t}\n\t\t}\n\t}\n\tspin_unlock_bh(&smc_lgr_list.lock);\n\n\tlist_for_each_entry_safe(lgr, lg, &lgr_free_list, list) {\n\t\tlist_del_init(&lgr->list);\n\t\tsmc_llc_set_termination_rsn(lgr, SMC_LLC_DEL_OP_INIT_TERM);\n\t\t__smc_lgr_terminate(lgr, false);\n\t}\n\n\tif (smcibdev) {\n\t\tif (atomic_read(&smcibdev->lnk_cnt))\n\t\t\twait_event(smcibdev->lnks_deleted,\n\t\t\t\t   !atomic_read(&smcibdev->lnk_cnt));\n\t} else {\n\t\tif (atomic_read(&lgr_cnt))\n\t\t\twait_event(lgrs_deleted, !atomic_read(&lgr_cnt));\n\t}\n}\n\n \nvoid smcr_lgr_set_type(struct smc_link_group *lgr, enum smc_lgr_type new_type)\n{\n\tchar *lgr_type = \"\";\n\tint i;\n\n\tfor (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++)\n\t\tif (smc_link_usable(&lgr->lnk[i]))\n\t\t\tlgr->lnk[i].link_is_asym = false;\n\tif (lgr->type == new_type)\n\t\treturn;\n\tlgr->type = new_type;\n\n\tswitch (lgr->type) {\n\tcase SMC_LGR_NONE:\n\t\tlgr_type = \"NONE\";\n\t\tbreak;\n\tcase SMC_LGR_SINGLE:\n\t\tlgr_type = \"SINGLE\";\n\t\tbreak;\n\tcase SMC_LGR_SYMMETRIC:\n\t\tlgr_type = \"SYMMETRIC\";\n\t\tbreak;\n\tcase SMC_LGR_ASYMMETRIC_PEER:\n\t\tlgr_type = \"ASYMMETRIC_PEER\";\n\t\tbreak;\n\tcase SMC_LGR_ASYMMETRIC_LOCAL:\n\t\tlgr_type = \"ASYMMETRIC_LOCAL\";\n\t\tbreak;\n\t}\n\tpr_warn_ratelimited(\"smc: SMC-R lg %*phN net %llu state changed: \"\n\t\t\t    \"%s, pnetid %.16s\\n\", SMC_LGR_ID_SIZE, &lgr->id,\n\t\t\t    lgr->net->net_cookie, lgr_type, lgr->pnet_id);\n}\n\n \nvoid smcr_lgr_set_type_asym(struct smc_link_group *lgr,\n\t\t\t    enum smc_lgr_type new_type, int asym_lnk_idx)\n{\n\tsmcr_lgr_set_type(lgr, new_type);\n\tlgr->lnk[asym_lnk_idx].link_is_asym = true;\n}\n\n \nstatic void smc_conn_abort_work(struct work_struct *work)\n{\n\tstruct smc_connection *conn = container_of(work,\n\t\t\t\t\t\t   struct smc_connection,\n\t\t\t\t\t\t   abort_work);\n\tstruct smc_sock *smc = container_of(conn, struct smc_sock, conn);\n\n\tlock_sock(&smc->sk);\n\tsmc_conn_kill(conn, true);\n\trelease_sock(&smc->sk);\n\tsock_put(&smc->sk);  \n}\n\nvoid smcr_port_add(struct smc_ib_device *smcibdev, u8 ibport)\n{\n\tstruct smc_link_group *lgr, *n;\n\n\tspin_lock_bh(&smc_lgr_list.lock);\n\tlist_for_each_entry_safe(lgr, n, &smc_lgr_list.list, list) {\n\t\tstruct smc_link *link;\n\n\t\tif (strncmp(smcibdev->pnetid[ibport - 1], lgr->pnet_id,\n\t\t\t    SMC_MAX_PNETID_LEN) ||\n\t\t    lgr->type == SMC_LGR_SYMMETRIC ||\n\t\t    lgr->type == SMC_LGR_ASYMMETRIC_PEER ||\n\t\t    !rdma_dev_access_netns(smcibdev->ibdev, lgr->net))\n\t\t\tcontinue;\n\n\t\tif (lgr->type == SMC_LGR_SINGLE && lgr->max_links <= 1)\n\t\t\tcontinue;\n\n\t\t \n\t\tlink = smc_llc_usable_link(lgr);\n\t\tif (link)\n\t\t\tsmc_llc_add_link_local(link);\n\t}\n\tspin_unlock_bh(&smc_lgr_list.lock);\n}\n\n \nstatic void smcr_link_down(struct smc_link *lnk)\n{\n\tstruct smc_link_group *lgr = lnk->lgr;\n\tstruct smc_link *to_lnk;\n\tint del_link_id;\n\n\tif (!lgr || lnk->state == SMC_LNK_UNUSED || list_empty(&lgr->list))\n\t\treturn;\n\n\tto_lnk = smc_switch_conns(lgr, lnk, true);\n\tif (!to_lnk) {  \n\t\tsmcr_link_clear(lnk, true);\n\t\treturn;\n\t}\n\tsmcr_lgr_set_type(lgr, SMC_LGR_SINGLE);\n\tdel_link_id = lnk->link_id;\n\n\tif (lgr->role == SMC_SERV) {\n\t\t \n\t\tsmc_llc_srv_delete_link_local(to_lnk, del_link_id);\n\t} else {\n\t\tif (lgr->llc_flow_lcl.type != SMC_LLC_FLOW_NONE) {\n\t\t\t \n\t\t\tup_write(&lgr->llc_conf_mutex);\n\t\t\twait_event_timeout(lgr->llc_flow_waiter,\n\t\t\t\t(list_empty(&lgr->list) ||\n\t\t\t\t lgr->llc_flow_lcl.type == SMC_LLC_FLOW_NONE),\n\t\t\t\tSMC_LLC_WAIT_TIME);\n\t\t\tdown_write(&lgr->llc_conf_mutex);\n\t\t}\n\t\tif (!list_empty(&lgr->list)) {\n\t\t\tsmc_llc_send_delete_link(to_lnk, del_link_id,\n\t\t\t\t\t\t SMC_LLC_REQ, true,\n\t\t\t\t\t\t SMC_LLC_DEL_LOST_PATH);\n\t\t\tsmcr_link_clear(lnk, true);\n\t\t}\n\t\twake_up(&lgr->llc_flow_waiter);\t \n\t}\n}\n\n \nvoid smcr_link_down_cond(struct smc_link *lnk)\n{\n\tif (smc_link_downing(&lnk->state)) {\n\t\ttrace_smcr_link_down(lnk, __builtin_return_address(0));\n\t\tsmcr_link_down(lnk);\n\t}\n}\n\n \nvoid smcr_link_down_cond_sched(struct smc_link *lnk)\n{\n\tif (smc_link_downing(&lnk->state)) {\n\t\ttrace_smcr_link_down(lnk, __builtin_return_address(0));\n\t\tschedule_work(&lnk->link_down_wrk);\n\t}\n}\n\nvoid smcr_port_err(struct smc_ib_device *smcibdev, u8 ibport)\n{\n\tstruct smc_link_group *lgr, *n;\n\tint i;\n\n\tlist_for_each_entry_safe(lgr, n, &smc_lgr_list.list, list) {\n\t\tif (strncmp(smcibdev->pnetid[ibport - 1], lgr->pnet_id,\n\t\t\t    SMC_MAX_PNETID_LEN))\n\t\t\tcontinue;  \n\t\tif (list_empty(&lgr->list))\n\t\t\tcontinue;\n\t\tfor (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++) {\n\t\t\tstruct smc_link *lnk = &lgr->lnk[i];\n\n\t\t\tif (smc_link_usable(lnk) &&\n\t\t\t    lnk->smcibdev == smcibdev && lnk->ibport == ibport)\n\t\t\t\tsmcr_link_down_cond_sched(lnk);\n\t\t}\n\t}\n}\n\nstatic void smc_link_down_work(struct work_struct *work)\n{\n\tstruct smc_link *link = container_of(work, struct smc_link,\n\t\t\t\t\t     link_down_wrk);\n\tstruct smc_link_group *lgr = link->lgr;\n\n\tif (list_empty(&lgr->list))\n\t\treturn;\n\twake_up_all(&lgr->llc_msg_waiter);\n\tdown_write(&lgr->llc_conf_mutex);\n\tsmcr_link_down(link);\n\tup_write(&lgr->llc_conf_mutex);\n}\n\nstatic int smc_vlan_by_tcpsk_walk(struct net_device *lower_dev,\n\t\t\t\t  struct netdev_nested_priv *priv)\n{\n\tunsigned short *vlan_id = (unsigned short *)priv->data;\n\n\tif (is_vlan_dev(lower_dev)) {\n\t\t*vlan_id = vlan_dev_vlan_id(lower_dev);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\n \nint smc_vlan_by_tcpsk(struct socket *clcsock, struct smc_init_info *ini)\n{\n\tstruct dst_entry *dst = sk_dst_get(clcsock->sk);\n\tstruct netdev_nested_priv priv;\n\tstruct net_device *ndev;\n\tint rc = 0;\n\n\tini->vlan_id = 0;\n\tif (!dst) {\n\t\trc = -ENOTCONN;\n\t\tgoto out;\n\t}\n\tif (!dst->dev) {\n\t\trc = -ENODEV;\n\t\tgoto out_rel;\n\t}\n\n\tndev = dst->dev;\n\tif (is_vlan_dev(ndev)) {\n\t\tini->vlan_id = vlan_dev_vlan_id(ndev);\n\t\tgoto out_rel;\n\t}\n\n\tpriv.data = (void *)&ini->vlan_id;\n\trtnl_lock();\n\tnetdev_walk_all_lower_dev(ndev, smc_vlan_by_tcpsk_walk, &priv);\n\trtnl_unlock();\n\nout_rel:\n\tdst_release(dst);\nout:\n\treturn rc;\n}\n\nstatic bool smcr_lgr_match(struct smc_link_group *lgr, u8 smcr_version,\n\t\t\t   u8 peer_systemid[],\n\t\t\t   u8 peer_gid[],\n\t\t\t   u8 peer_mac_v1[],\n\t\t\t   enum smc_lgr_role role, u32 clcqpn,\n\t\t\t   struct net *net)\n{\n\tstruct smc_link *lnk;\n\tint i;\n\n\tif (memcmp(lgr->peer_systemid, peer_systemid, SMC_SYSTEMID_LEN) ||\n\t    lgr->role != role)\n\t\treturn false;\n\n\tfor (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++) {\n\t\tlnk = &lgr->lnk[i];\n\n\t\tif (!smc_link_active(lnk))\n\t\t\tcontinue;\n\t\t \n\t\tif (!rdma_dev_access_netns(lnk->smcibdev->ibdev, net))\n\t\t\treturn false;\n\t\tif ((lgr->role == SMC_SERV || lnk->peer_qpn == clcqpn) &&\n\t\t    !memcmp(lnk->peer_gid, peer_gid, SMC_GID_SIZE) &&\n\t\t    (smcr_version == SMC_V2 ||\n\t\t     !memcmp(lnk->peer_mac, peer_mac_v1, ETH_ALEN)))\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic bool smcd_lgr_match(struct smc_link_group *lgr,\n\t\t\t   struct smcd_dev *smcismdev, u64 peer_gid)\n{\n\treturn lgr->peer_gid == peer_gid && lgr->smcd == smcismdev;\n}\n\n \nint smc_conn_create(struct smc_sock *smc, struct smc_init_info *ini)\n{\n\tstruct smc_connection *conn = &smc->conn;\n\tstruct net *net = sock_net(&smc->sk);\n\tstruct list_head *lgr_list;\n\tstruct smc_link_group *lgr;\n\tenum smc_lgr_role role;\n\tspinlock_t *lgr_lock;\n\tint rc = 0;\n\n\tlgr_list = ini->is_smcd ? &ini->ism_dev[ini->ism_selected]->lgr_list :\n\t\t\t\t  &smc_lgr_list.list;\n\tlgr_lock = ini->is_smcd ? &ini->ism_dev[ini->ism_selected]->lgr_lock :\n\t\t\t\t  &smc_lgr_list.lock;\n\tini->first_contact_local = 1;\n\trole = smc->listen_smc ? SMC_SERV : SMC_CLNT;\n\tif (role == SMC_CLNT && ini->first_contact_peer)\n\t\t \n\t\tgoto create;\n\n\t \n\tspin_lock_bh(lgr_lock);\n\tlist_for_each_entry(lgr, lgr_list, list) {\n\t\twrite_lock_bh(&lgr->conns_lock);\n\t\tif ((ini->is_smcd ?\n\t\t     smcd_lgr_match(lgr, ini->ism_dev[ini->ism_selected],\n\t\t\t\t    ini->ism_peer_gid[ini->ism_selected]) :\n\t\t     smcr_lgr_match(lgr, ini->smcr_version,\n\t\t\t\t    ini->peer_systemid,\n\t\t\t\t    ini->peer_gid, ini->peer_mac, role,\n\t\t\t\t    ini->ib_clcqpn, net)) &&\n\t\t    !lgr->sync_err &&\n\t\t    (ini->smcd_version == SMC_V2 ||\n\t\t     lgr->vlan_id == ini->vlan_id) &&\n\t\t    (role == SMC_CLNT || ini->is_smcd ||\n\t\t    (lgr->conns_num < lgr->max_conns &&\n\t\t      !bitmap_full(lgr->rtokens_used_mask, SMC_RMBS_PER_LGR_MAX)))) {\n\t\t\t \n\t\t\tini->first_contact_local = 0;\n\t\t\tconn->lgr = lgr;\n\t\t\trc = smc_lgr_register_conn(conn, false);\n\t\t\twrite_unlock_bh(&lgr->conns_lock);\n\t\t\tif (!rc && delayed_work_pending(&lgr->free_work))\n\t\t\t\tcancel_delayed_work(&lgr->free_work);\n\t\t\tbreak;\n\t\t}\n\t\twrite_unlock_bh(&lgr->conns_lock);\n\t}\n\tspin_unlock_bh(lgr_lock);\n\tif (rc)\n\t\treturn rc;\n\n\tif (role == SMC_CLNT && !ini->first_contact_peer &&\n\t    ini->first_contact_local) {\n\t\t \n\t\treturn SMC_CLC_DECL_SYNCERR;\n\t}\n\ncreate:\n\tif (ini->first_contact_local) {\n\t\trc = smc_lgr_create(smc, ini);\n\t\tif (rc)\n\t\t\tgoto out;\n\t\tlgr = conn->lgr;\n\t\twrite_lock_bh(&lgr->conns_lock);\n\t\trc = smc_lgr_register_conn(conn, true);\n\t\twrite_unlock_bh(&lgr->conns_lock);\n\t\tif (rc) {\n\t\t\tsmc_lgr_cleanup_early(lgr);\n\t\t\tgoto out;\n\t\t}\n\t}\n\tsmc_lgr_hold(conn->lgr);  \n\tif (!conn->lgr->is_smcd)\n\t\tsmcr_link_hold(conn->lnk);  \n\tconn->freed = 0;\n\tconn->local_tx_ctrl.common.type = SMC_CDC_MSG_TYPE;\n\tconn->local_tx_ctrl.len = SMC_WR_TX_SIZE;\n\tconn->urg_state = SMC_URG_READ;\n\tinit_waitqueue_head(&conn->cdc_pend_tx_wq);\n\tINIT_WORK(&smc->conn.abort_work, smc_conn_abort_work);\n\tif (ini->is_smcd) {\n\t\tconn->rx_off = sizeof(struct smcd_cdc_msg);\n\t\tsmcd_cdc_rx_init(conn);  \n\t} else {\n\t\tconn->rx_off = 0;\n\t}\n#ifndef KERNEL_HAS_ATOMIC64\n\tspin_lock_init(&conn->acurs_lock);\n#endif\n\nout:\n\treturn rc;\n}\n\n#define SMCD_DMBE_SIZES\t\t6  \n#define SMCR_RMBE_SIZES\t\t5  \n\n \nstatic u8 smc_compress_bufsize(int size, bool is_smcd, bool is_rmb)\n{\n\tconst unsigned int max_scat = SG_MAX_SINGLE_ALLOC * PAGE_SIZE;\n\tu8 compressed;\n\n\tif (size <= SMC_BUF_MIN_SIZE)\n\t\treturn 0;\n\n\tsize = (size - 1) >> 14;   \n\tcompressed = min_t(u8, ilog2(size) + 1,\n\t\t\t   is_smcd ? SMCD_DMBE_SIZES : SMCR_RMBE_SIZES);\n\n\tif (!is_smcd && is_rmb)\n\t\t \n\t\tcompressed = min_t(u8, compressed, ilog2(max_scat >> 14));\n\n\treturn compressed;\n}\n\n \nint smc_uncompress_bufsize(u8 compressed)\n{\n\tu32 size;\n\n\tsize = 0x00000001 << (((int)compressed) + 14);\n\treturn (int)size;\n}\n\n \nstatic struct smc_buf_desc *smc_buf_get_slot(int compressed_bufsize,\n\t\t\t\t\t     struct rw_semaphore *lock,\n\t\t\t\t\t     struct list_head *buf_list)\n{\n\tstruct smc_buf_desc *buf_slot;\n\n\tdown_read(lock);\n\tlist_for_each_entry(buf_slot, buf_list, list) {\n\t\tif (cmpxchg(&buf_slot->used, 0, 1) == 0) {\n\t\t\tup_read(lock);\n\t\t\treturn buf_slot;\n\t\t}\n\t}\n\tup_read(lock);\n\treturn NULL;\n}\n\n \nstatic inline int smc_rmb_wnd_update_limit(int rmbe_size)\n{\n\treturn max_t(int, rmbe_size / 10, SOCK_MIN_SNDBUF / 2);\n}\n\n \nstatic int smcr_buf_map_link(struct smc_buf_desc *buf_desc, bool is_rmb,\n\t\t\t     struct smc_link *lnk)\n{\n\tint rc, i, nents, offset, buf_size, size, access_flags;\n\tstruct scatterlist *sg;\n\tvoid *buf;\n\n\tif (buf_desc->is_map_ib[lnk->link_idx])\n\t\treturn 0;\n\n\tif (buf_desc->is_vm) {\n\t\tbuf = buf_desc->cpu_addr;\n\t\tbuf_size = buf_desc->len;\n\t\toffset = offset_in_page(buf_desc->cpu_addr);\n\t\tnents = PAGE_ALIGN(buf_size + offset) / PAGE_SIZE;\n\t} else {\n\t\tnents = 1;\n\t}\n\n\trc = sg_alloc_table(&buf_desc->sgt[lnk->link_idx], nents, GFP_KERNEL);\n\tif (rc)\n\t\treturn rc;\n\n\tif (buf_desc->is_vm) {\n\t\t \n\t\tfor_each_sg(buf_desc->sgt[lnk->link_idx].sgl, sg, nents, i) {\n\t\t\tsize = min_t(int, PAGE_SIZE - offset, buf_size);\n\t\t\tsg_set_page(sg, vmalloc_to_page(buf), size, offset);\n\t\t\tbuf += size / sizeof(*buf);\n\t\t\tbuf_size -= size;\n\t\t\toffset = 0;\n\t\t}\n\t} else {\n\t\t \n\t\tsg_set_buf(buf_desc->sgt[lnk->link_idx].sgl,\n\t\t\t   buf_desc->cpu_addr, buf_desc->len);\n\t}\n\n\t \n\trc = smc_ib_buf_map_sg(lnk, buf_desc,\n\t\t\t       is_rmb ? DMA_FROM_DEVICE : DMA_TO_DEVICE);\n\t \n\tif (rc != nents) {\n\t\trc = -EAGAIN;\n\t\tgoto free_table;\n\t}\n\n\tbuf_desc->is_dma_need_sync |=\n\t\tsmc_ib_is_sg_need_sync(lnk, buf_desc) << lnk->link_idx;\n\n\tif (is_rmb || buf_desc->is_vm) {\n\t\t \n\t\taccess_flags = is_rmb ?\n\t\t\t       IB_ACCESS_REMOTE_WRITE | IB_ACCESS_LOCAL_WRITE :\n\t\t\t       IB_ACCESS_LOCAL_WRITE;\n\n\t\trc = smc_ib_get_memory_region(lnk->roce_pd, access_flags,\n\t\t\t\t\t      buf_desc, lnk->link_idx);\n\t\tif (rc)\n\t\t\tgoto buf_unmap;\n\t\tsmc_ib_sync_sg_for_device(lnk, buf_desc,\n\t\t\t\t\t  is_rmb ? DMA_FROM_DEVICE : DMA_TO_DEVICE);\n\t}\n\tbuf_desc->is_map_ib[lnk->link_idx] = true;\n\treturn 0;\n\nbuf_unmap:\n\tsmc_ib_buf_unmap_sg(lnk, buf_desc,\n\t\t\t    is_rmb ? DMA_FROM_DEVICE : DMA_TO_DEVICE);\nfree_table:\n\tsg_free_table(&buf_desc->sgt[lnk->link_idx]);\n\treturn rc;\n}\n\n \nint smcr_link_reg_buf(struct smc_link *link, struct smc_buf_desc *buf_desc)\n{\n\tif (list_empty(&link->lgr->list))\n\t\treturn -ENOLINK;\n\tif (!buf_desc->is_reg_mr[link->link_idx]) {\n\t\t \n\t\tif (buf_desc->is_vm)\n\t\t\tbuf_desc->mr[link->link_idx]->iova =\n\t\t\t\t(uintptr_t)buf_desc->cpu_addr;\n\t\tif (smc_wr_reg_send(link, buf_desc->mr[link->link_idx])) {\n\t\t\tbuf_desc->is_reg_err = true;\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tbuf_desc->is_reg_mr[link->link_idx] = true;\n\t}\n\treturn 0;\n}\n\nstatic int _smcr_buf_map_lgr(struct smc_link *lnk, struct rw_semaphore *lock,\n\t\t\t     struct list_head *lst, bool is_rmb)\n{\n\tstruct smc_buf_desc *buf_desc, *bf;\n\tint rc = 0;\n\n\tdown_write(lock);\n\tlist_for_each_entry_safe(buf_desc, bf, lst, list) {\n\t\tif (!buf_desc->used)\n\t\t\tcontinue;\n\t\trc = smcr_buf_map_link(buf_desc, is_rmb, lnk);\n\t\tif (rc)\n\t\t\tgoto out;\n\t}\nout:\n\tup_write(lock);\n\treturn rc;\n}\n\n \nint smcr_buf_map_lgr(struct smc_link *lnk)\n{\n\tstruct smc_link_group *lgr = lnk->lgr;\n\tint i, rc = 0;\n\n\tfor (i = 0; i < SMC_RMBE_SIZES; i++) {\n\t\trc = _smcr_buf_map_lgr(lnk, &lgr->rmbs_lock,\n\t\t\t\t       &lgr->rmbs[i], true);\n\t\tif (rc)\n\t\t\treturn rc;\n\t\trc = _smcr_buf_map_lgr(lnk, &lgr->sndbufs_lock,\n\t\t\t\t       &lgr->sndbufs[i], false);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\treturn 0;\n}\n\n \nint smcr_buf_reg_lgr(struct smc_link *lnk)\n{\n\tstruct smc_link_group *lgr = lnk->lgr;\n\tstruct smc_buf_desc *buf_desc, *bf;\n\tint i, rc = 0;\n\n\t \n\tdown_write(&lgr->rmbs_lock);\n\tfor (i = 0; i < SMC_RMBE_SIZES; i++) {\n\t\tlist_for_each_entry_safe(buf_desc, bf, &lgr->rmbs[i], list) {\n\t\t\tif (!buf_desc->used)\n\t\t\t\tcontinue;\n\t\t\trc = smcr_link_reg_buf(lnk, buf_desc);\n\t\t\tif (rc) {\n\t\t\t\tup_write(&lgr->rmbs_lock);\n\t\t\t\treturn rc;\n\t\t\t}\n\t\t}\n\t}\n\tup_write(&lgr->rmbs_lock);\n\n\tif (lgr->buf_type == SMCR_PHYS_CONT_BUFS)\n\t\treturn rc;\n\n\t \n\tdown_write(&lgr->sndbufs_lock);\n\tfor (i = 0; i < SMC_RMBE_SIZES; i++) {\n\t\tlist_for_each_entry_safe(buf_desc, bf, &lgr->sndbufs[i], list) {\n\t\t\tif (!buf_desc->used || !buf_desc->is_vm)\n\t\t\t\tcontinue;\n\t\t\trc = smcr_link_reg_buf(lnk, buf_desc);\n\t\t\tif (rc) {\n\t\t\t\tup_write(&lgr->sndbufs_lock);\n\t\t\t\treturn rc;\n\t\t\t}\n\t\t}\n\t}\n\tup_write(&lgr->sndbufs_lock);\n\treturn rc;\n}\n\nstatic struct smc_buf_desc *smcr_new_buf_create(struct smc_link_group *lgr,\n\t\t\t\t\t\tbool is_rmb, int bufsize)\n{\n\tstruct smc_buf_desc *buf_desc;\n\n\t \n\tbuf_desc = kzalloc(sizeof(*buf_desc), GFP_KERNEL);\n\tif (!buf_desc)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tswitch (lgr->buf_type) {\n\tcase SMCR_PHYS_CONT_BUFS:\n\tcase SMCR_MIXED_BUFS:\n\t\tbuf_desc->order = get_order(bufsize);\n\t\tbuf_desc->pages = alloc_pages(GFP_KERNEL | __GFP_NOWARN |\n\t\t\t\t\t      __GFP_NOMEMALLOC | __GFP_COMP |\n\t\t\t\t\t      __GFP_NORETRY | __GFP_ZERO,\n\t\t\t\t\t      buf_desc->order);\n\t\tif (buf_desc->pages) {\n\t\t\tbuf_desc->cpu_addr =\n\t\t\t\t(void *)page_address(buf_desc->pages);\n\t\t\tbuf_desc->len = bufsize;\n\t\t\tbuf_desc->is_vm = false;\n\t\t\tbreak;\n\t\t}\n\t\tif (lgr->buf_type == SMCR_PHYS_CONT_BUFS)\n\t\t\tgoto out;\n\t\tfallthrough;\t\n\tcase SMCR_VIRT_CONT_BUFS:\n\t\tbuf_desc->order = get_order(bufsize);\n\t\tbuf_desc->cpu_addr = vzalloc(PAGE_SIZE << buf_desc->order);\n\t\tif (!buf_desc->cpu_addr)\n\t\t\tgoto out;\n\t\tbuf_desc->pages = NULL;\n\t\tbuf_desc->len = bufsize;\n\t\tbuf_desc->is_vm = true;\n\t\tbreak;\n\t}\n\treturn buf_desc;\n\nout:\n\tkfree(buf_desc);\n\treturn ERR_PTR(-EAGAIN);\n}\n\n \nstatic int smcr_buf_map_usable_links(struct smc_link_group *lgr,\n\t\t\t\t     struct smc_buf_desc *buf_desc, bool is_rmb)\n{\n\tint i, rc = 0, cnt = 0;\n\n\t \n\tdown_read(&lgr->llc_conf_mutex);\n\tfor (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++) {\n\t\tstruct smc_link *lnk = &lgr->lnk[i];\n\n\t\tif (!smc_link_usable(lnk))\n\t\t\tcontinue;\n\t\tif (smcr_buf_map_link(buf_desc, is_rmb, lnk)) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tcnt++;\n\t}\nout:\n\tup_read(&lgr->llc_conf_mutex);\n\tif (!rc && !cnt)\n\t\trc = -EINVAL;\n\treturn rc;\n}\n\nstatic struct smc_buf_desc *smcd_new_buf_create(struct smc_link_group *lgr,\n\t\t\t\t\t\tbool is_dmb, int bufsize)\n{\n\tstruct smc_buf_desc *buf_desc;\n\tint rc;\n\n\t \n\tbuf_desc = kzalloc(sizeof(*buf_desc), GFP_KERNEL);\n\tif (!buf_desc)\n\t\treturn ERR_PTR(-ENOMEM);\n\tif (is_dmb) {\n\t\trc = smc_ism_register_dmb(lgr, bufsize, buf_desc);\n\t\tif (rc) {\n\t\t\tkfree(buf_desc);\n\t\t\tif (rc == -ENOMEM)\n\t\t\t\treturn ERR_PTR(-EAGAIN);\n\t\t\tif (rc == -ENOSPC)\n\t\t\t\treturn ERR_PTR(-ENOSPC);\n\t\t\treturn ERR_PTR(-EIO);\n\t\t}\n\t\tbuf_desc->pages = virt_to_page(buf_desc->cpu_addr);\n\t\t \n\t\tbuf_desc->len = bufsize - sizeof(struct smcd_cdc_msg);\n\t} else {\n\t\tbuf_desc->cpu_addr = kzalloc(bufsize, GFP_KERNEL |\n\t\t\t\t\t     __GFP_NOWARN | __GFP_NORETRY |\n\t\t\t\t\t     __GFP_NOMEMALLOC);\n\t\tif (!buf_desc->cpu_addr) {\n\t\t\tkfree(buf_desc);\n\t\t\treturn ERR_PTR(-EAGAIN);\n\t\t}\n\t\tbuf_desc->len = bufsize;\n\t}\n\treturn buf_desc;\n}\n\nstatic int __smc_buf_create(struct smc_sock *smc, bool is_smcd, bool is_rmb)\n{\n\tstruct smc_buf_desc *buf_desc = ERR_PTR(-ENOMEM);\n\tstruct smc_connection *conn = &smc->conn;\n\tstruct smc_link_group *lgr = conn->lgr;\n\tstruct list_head *buf_list;\n\tint bufsize, bufsize_comp;\n\tstruct rw_semaphore *lock;\t \n\tbool is_dgraded = false;\n\n\tif (is_rmb)\n\t\t \n\t\tbufsize = smc->sk.sk_rcvbuf / 2;\n\telse\n\t\t \n\t\tbufsize = smc->sk.sk_sndbuf / 2;\n\n\tfor (bufsize_comp = smc_compress_bufsize(bufsize, is_smcd, is_rmb);\n\t     bufsize_comp >= 0; bufsize_comp--) {\n\t\tif (is_rmb) {\n\t\t\tlock = &lgr->rmbs_lock;\n\t\t\tbuf_list = &lgr->rmbs[bufsize_comp];\n\t\t} else {\n\t\t\tlock = &lgr->sndbufs_lock;\n\t\t\tbuf_list = &lgr->sndbufs[bufsize_comp];\n\t\t}\n\t\tbufsize = smc_uncompress_bufsize(bufsize_comp);\n\n\t\t \n\t\tbuf_desc = smc_buf_get_slot(bufsize_comp, lock, buf_list);\n\t\tif (buf_desc) {\n\t\t\tbuf_desc->is_dma_need_sync = 0;\n\t\t\tSMC_STAT_RMB_SIZE(smc, is_smcd, is_rmb, bufsize);\n\t\t\tSMC_STAT_BUF_REUSE(smc, is_smcd, is_rmb);\n\t\t\tbreak;  \n\t\t}\n\n\t\tif (is_smcd)\n\t\t\tbuf_desc = smcd_new_buf_create(lgr, is_rmb, bufsize);\n\t\telse\n\t\t\tbuf_desc = smcr_new_buf_create(lgr, is_rmb, bufsize);\n\n\t\tif (PTR_ERR(buf_desc) == -ENOMEM)\n\t\t\tbreak;\n\t\tif (IS_ERR(buf_desc)) {\n\t\t\tif (!is_dgraded) {\n\t\t\t\tis_dgraded = true;\n\t\t\t\tSMC_STAT_RMB_DOWNGRADED(smc, is_smcd, is_rmb);\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\tSMC_STAT_RMB_ALLOC(smc, is_smcd, is_rmb);\n\t\tSMC_STAT_RMB_SIZE(smc, is_smcd, is_rmb, bufsize);\n\t\tbuf_desc->used = 1;\n\t\tdown_write(lock);\n\t\tlist_add(&buf_desc->list, buf_list);\n\t\tup_write(lock);\n\t\tbreak;  \n\t}\n\n\tif (IS_ERR(buf_desc))\n\t\treturn PTR_ERR(buf_desc);\n\n\tif (!is_smcd) {\n\t\tif (smcr_buf_map_usable_links(lgr, buf_desc, is_rmb)) {\n\t\t\tsmcr_buf_unuse(buf_desc, is_rmb, lgr);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\tif (is_rmb) {\n\t\tconn->rmb_desc = buf_desc;\n\t\tconn->rmbe_size_comp = bufsize_comp;\n\t\tsmc->sk.sk_rcvbuf = bufsize * 2;\n\t\tatomic_set(&conn->bytes_to_rcv, 0);\n\t\tconn->rmbe_update_limit =\n\t\t\tsmc_rmb_wnd_update_limit(buf_desc->len);\n\t\tif (is_smcd)\n\t\t\tsmc_ism_set_conn(conn);  \n\t} else {\n\t\tconn->sndbuf_desc = buf_desc;\n\t\tsmc->sk.sk_sndbuf = bufsize * 2;\n\t\tatomic_set(&conn->sndbuf_space, bufsize);\n\t}\n\treturn 0;\n}\n\nvoid smc_sndbuf_sync_sg_for_device(struct smc_connection *conn)\n{\n\tif (!conn->sndbuf_desc->is_dma_need_sync)\n\t\treturn;\n\tif (!smc_conn_lgr_valid(conn) || conn->lgr->is_smcd ||\n\t    !smc_link_active(conn->lnk))\n\t\treturn;\n\tsmc_ib_sync_sg_for_device(conn->lnk, conn->sndbuf_desc, DMA_TO_DEVICE);\n}\n\nvoid smc_rmb_sync_sg_for_cpu(struct smc_connection *conn)\n{\n\tint i;\n\n\tif (!conn->rmb_desc->is_dma_need_sync)\n\t\treturn;\n\tif (!smc_conn_lgr_valid(conn) || conn->lgr->is_smcd)\n\t\treturn;\n\tfor (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++) {\n\t\tif (!smc_link_active(&conn->lgr->lnk[i]))\n\t\t\tcontinue;\n\t\tsmc_ib_sync_sg_for_cpu(&conn->lgr->lnk[i], conn->rmb_desc,\n\t\t\t\t       DMA_FROM_DEVICE);\n\t}\n}\n\n \nint smc_buf_create(struct smc_sock *smc, bool is_smcd)\n{\n\tint rc;\n\n\t \n\trc = __smc_buf_create(smc, is_smcd, false);\n\tif (rc)\n\t\treturn rc;\n\t \n\trc = __smc_buf_create(smc, is_smcd, true);\n\tif (rc) {\n\t\tdown_write(&smc->conn.lgr->sndbufs_lock);\n\t\tlist_del(&smc->conn.sndbuf_desc->list);\n\t\tup_write(&smc->conn.lgr->sndbufs_lock);\n\t\tsmc_buf_free(smc->conn.lgr, false, smc->conn.sndbuf_desc);\n\t\tsmc->conn.sndbuf_desc = NULL;\n\t}\n\treturn rc;\n}\n\nstatic inline int smc_rmb_reserve_rtoken_idx(struct smc_link_group *lgr)\n{\n\tint i;\n\n\tfor_each_clear_bit(i, lgr->rtokens_used_mask, SMC_RMBS_PER_LGR_MAX) {\n\t\tif (!test_and_set_bit(i, lgr->rtokens_used_mask))\n\t\t\treturn i;\n\t}\n\treturn -ENOSPC;\n}\n\nstatic int smc_rtoken_find_by_link(struct smc_link_group *lgr, int lnk_idx,\n\t\t\t\t   u32 rkey)\n{\n\tint i;\n\n\tfor (i = 0; i < SMC_RMBS_PER_LGR_MAX; i++) {\n\t\tif (test_bit(i, lgr->rtokens_used_mask) &&\n\t\t    lgr->rtokens[i][lnk_idx].rkey == rkey)\n\t\t\treturn i;\n\t}\n\treturn -ENOENT;\n}\n\n \nvoid smc_rtoken_set(struct smc_link_group *lgr, int link_idx, int link_idx_new,\n\t\t    __be32 nw_rkey_known, __be64 nw_vaddr, __be32 nw_rkey)\n{\n\tint rtok_idx;\n\n\trtok_idx = smc_rtoken_find_by_link(lgr, link_idx, ntohl(nw_rkey_known));\n\tif (rtok_idx == -ENOENT)\n\t\treturn;\n\tlgr->rtokens[rtok_idx][link_idx_new].rkey = ntohl(nw_rkey);\n\tlgr->rtokens[rtok_idx][link_idx_new].dma_addr = be64_to_cpu(nw_vaddr);\n}\n\n \nvoid smc_rtoken_set2(struct smc_link_group *lgr, int rtok_idx, int link_id,\n\t\t     __be64 nw_vaddr, __be32 nw_rkey)\n{\n\tu64 dma_addr = be64_to_cpu(nw_vaddr);\n\tu32 rkey = ntohl(nw_rkey);\n\tbool found = false;\n\tint link_idx;\n\n\tfor (link_idx = 0; link_idx < SMC_LINKS_PER_LGR_MAX; link_idx++) {\n\t\tif (lgr->lnk[link_idx].link_id == link_id) {\n\t\t\tfound = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!found)\n\t\treturn;\n\tlgr->rtokens[rtok_idx][link_idx].rkey = rkey;\n\tlgr->rtokens[rtok_idx][link_idx].dma_addr = dma_addr;\n}\n\n \nint smc_rtoken_add(struct smc_link *lnk, __be64 nw_vaddr, __be32 nw_rkey)\n{\n\tstruct smc_link_group *lgr = smc_get_lgr(lnk);\n\tu64 dma_addr = be64_to_cpu(nw_vaddr);\n\tu32 rkey = ntohl(nw_rkey);\n\tint i;\n\n\tfor (i = 0; i < SMC_RMBS_PER_LGR_MAX; i++) {\n\t\tif (lgr->rtokens[i][lnk->link_idx].rkey == rkey &&\n\t\t    lgr->rtokens[i][lnk->link_idx].dma_addr == dma_addr &&\n\t\t    test_bit(i, lgr->rtokens_used_mask)) {\n\t\t\t \n\t\t\treturn i;\n\t\t}\n\t}\n\ti = smc_rmb_reserve_rtoken_idx(lgr);\n\tif (i < 0)\n\t\treturn i;\n\tlgr->rtokens[i][lnk->link_idx].rkey = rkey;\n\tlgr->rtokens[i][lnk->link_idx].dma_addr = dma_addr;\n\treturn i;\n}\n\n \nint smc_rtoken_delete(struct smc_link *lnk, __be32 nw_rkey)\n{\n\tstruct smc_link_group *lgr = smc_get_lgr(lnk);\n\tu32 rkey = ntohl(nw_rkey);\n\tint i, j;\n\n\tfor (i = 0; i < SMC_RMBS_PER_LGR_MAX; i++) {\n\t\tif (lgr->rtokens[i][lnk->link_idx].rkey == rkey &&\n\t\t    test_bit(i, lgr->rtokens_used_mask)) {\n\t\t\tfor (j = 0; j < SMC_LINKS_PER_LGR_MAX; j++) {\n\t\t\t\tlgr->rtokens[i][j].rkey = 0;\n\t\t\t\tlgr->rtokens[i][j].dma_addr = 0;\n\t\t\t}\n\t\t\tclear_bit(i, lgr->rtokens_used_mask);\n\t\t\treturn 0;\n\t\t}\n\t}\n\treturn -ENOENT;\n}\n\n \nint smc_rmb_rtoken_handling(struct smc_connection *conn,\n\t\t\t    struct smc_link *lnk,\n\t\t\t    struct smc_clc_msg_accept_confirm *clc)\n{\n\tconn->rtoken_idx = smc_rtoken_add(lnk, clc->r0.rmb_dma_addr,\n\t\t\t\t\t  clc->r0.rmb_rkey);\n\tif (conn->rtoken_idx < 0)\n\t\treturn conn->rtoken_idx;\n\treturn 0;\n}\n\nstatic void smc_core_going_away(void)\n{\n\tstruct smc_ib_device *smcibdev;\n\tstruct smcd_dev *smcd;\n\n\tmutex_lock(&smc_ib_devices.mutex);\n\tlist_for_each_entry(smcibdev, &smc_ib_devices.list, list) {\n\t\tint i;\n\n\t\tfor (i = 0; i < SMC_MAX_PORTS; i++)\n\t\t\tset_bit(i, smcibdev->ports_going_away);\n\t}\n\tmutex_unlock(&smc_ib_devices.mutex);\n\n\tmutex_lock(&smcd_dev_list.mutex);\n\tlist_for_each_entry(smcd, &smcd_dev_list.list, list) {\n\t\tsmcd->going_away = 1;\n\t}\n\tmutex_unlock(&smcd_dev_list.mutex);\n}\n\n \nstatic void smc_lgrs_shutdown(void)\n{\n\tstruct smcd_dev *smcd;\n\n\tsmc_core_going_away();\n\n\tsmc_smcr_terminate_all(NULL);\n\n\tmutex_lock(&smcd_dev_list.mutex);\n\tlist_for_each_entry(smcd, &smcd_dev_list.list, list)\n\t\tsmc_smcd_terminate_all(smcd);\n\tmutex_unlock(&smcd_dev_list.mutex);\n}\n\nstatic int smc_core_reboot_event(struct notifier_block *this,\n\t\t\t\t unsigned long event, void *ptr)\n{\n\tsmc_lgrs_shutdown();\n\tsmc_ib_unregister_client();\n\tsmc_ism_exit();\n\treturn 0;\n}\n\nstatic struct notifier_block smc_reboot_notifier = {\n\t.notifier_call = smc_core_reboot_event,\n};\n\nint __init smc_core_init(void)\n{\n\treturn register_reboot_notifier(&smc_reboot_notifier);\n}\n\n \nvoid smc_core_exit(void)\n{\n\tunregister_reboot_notifier(&smc_reboot_notifier);\n\tsmc_lgrs_shutdown();\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}