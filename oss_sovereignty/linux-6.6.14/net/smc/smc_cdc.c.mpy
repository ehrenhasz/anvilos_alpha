{
  "module_name": "smc_cdc.c",
  "hash_id": "c3dc6206fedfa030ca67cb71bdf5fefca72c72bd89e50287413a1213b90a756c",
  "original_prompt": "Ingested from linux-6.6.14/net/smc/smc_cdc.c",
  "human_readable_source": "\n \n\n#include <linux/spinlock.h>\n\n#include \"smc.h\"\n#include \"smc_wr.h\"\n#include \"smc_cdc.h\"\n#include \"smc_tx.h\"\n#include \"smc_rx.h\"\n#include \"smc_close.h\"\n\n \n\n \nstatic void smc_cdc_tx_handler(struct smc_wr_tx_pend_priv *pnd_snd,\n\t\t\t       struct smc_link *link,\n\t\t\t       enum ib_wc_status wc_status)\n{\n\tstruct smc_cdc_tx_pend *cdcpend = (struct smc_cdc_tx_pend *)pnd_snd;\n\tstruct smc_connection *conn = cdcpend->conn;\n\tstruct smc_buf_desc *sndbuf_desc;\n\tstruct smc_sock *smc;\n\tint diff;\n\n\tsndbuf_desc = conn->sndbuf_desc;\n\tsmc = container_of(conn, struct smc_sock, conn);\n\tbh_lock_sock(&smc->sk);\n\tif (!wc_status && sndbuf_desc) {\n\t\tdiff = smc_curs_diff(sndbuf_desc->len,\n\t\t\t\t     &cdcpend->conn->tx_curs_fin,\n\t\t\t\t     &cdcpend->cursor);\n\t\t \n\t\tsmp_mb__before_atomic();\n\t\tatomic_add(diff, &cdcpend->conn->sndbuf_space);\n\t\t \n\t\tsmp_mb__after_atomic();\n\t\tsmc_curs_copy(&conn->tx_curs_fin, &cdcpend->cursor, conn);\n\t\tsmc_curs_copy(&conn->local_tx_ctrl_fin, &cdcpend->p_cursor,\n\t\t\t      conn);\n\t\tconn->tx_cdc_seq_fin = cdcpend->ctrl_seq;\n\t}\n\n\tif (atomic_dec_and_test(&conn->cdc_pend_tx_wr)) {\n\t\t \n\t\tif (sock_owned_by_user(&smc->sk))\n\t\t\tconn->tx_in_release_sock = true;\n\t\telse\n\t\t\tsmc_tx_pending(conn);\n\n\t\tif (unlikely(wq_has_sleeper(&conn->cdc_pend_tx_wq)))\n\t\t\twake_up(&conn->cdc_pend_tx_wq);\n\t}\n\tWARN_ON(atomic_read(&conn->cdc_pend_tx_wr) < 0);\n\n\tsmc_tx_sndbuf_nonfull(smc);\n\tbh_unlock_sock(&smc->sk);\n}\n\nint smc_cdc_get_free_slot(struct smc_connection *conn,\n\t\t\t  struct smc_link *link,\n\t\t\t  struct smc_wr_buf **wr_buf,\n\t\t\t  struct smc_rdma_wr **wr_rdma_buf,\n\t\t\t  struct smc_cdc_tx_pend **pend)\n{\n\tint rc;\n\n\trc = smc_wr_tx_get_free_slot(link, smc_cdc_tx_handler, wr_buf,\n\t\t\t\t     wr_rdma_buf,\n\t\t\t\t     (struct smc_wr_tx_pend_priv **)pend);\n\tif (conn->killed) {\n\t\t \n\t\tif (!rc)\n\t\t\tsmc_wr_tx_put_slot(link,\n\t\t\t\t\t   (struct smc_wr_tx_pend_priv *)(*pend));\n\t\trc = -EPIPE;\n\t}\n\treturn rc;\n}\n\nstatic inline void smc_cdc_add_pending_send(struct smc_connection *conn,\n\t\t\t\t\t    struct smc_cdc_tx_pend *pend)\n{\n\tBUILD_BUG_ON_MSG(\n\t\tsizeof(struct smc_cdc_msg) > SMC_WR_BUF_SIZE,\n\t\t\"must increase SMC_WR_BUF_SIZE to at least sizeof(struct smc_cdc_msg)\");\n\tBUILD_BUG_ON_MSG(\n\t\toffsetofend(struct smc_cdc_msg, reserved) > SMC_WR_TX_SIZE,\n\t\t\"must adapt SMC_WR_TX_SIZE to sizeof(struct smc_cdc_msg); if not all smc_wr upper layer protocols use the same message size any more, must start to set link->wr_tx_sges[i].length on each individual smc_wr_tx_send()\");\n\tBUILD_BUG_ON_MSG(\n\t\tsizeof(struct smc_cdc_tx_pend) > SMC_WR_TX_PEND_PRIV_SIZE,\n\t\t\"must increase SMC_WR_TX_PEND_PRIV_SIZE to at least sizeof(struct smc_cdc_tx_pend)\");\n\tpend->conn = conn;\n\tpend->cursor = conn->tx_curs_sent;\n\tpend->p_cursor = conn->local_tx_ctrl.prod;\n\tpend->ctrl_seq = conn->tx_cdc_seq;\n}\n\nint smc_cdc_msg_send(struct smc_connection *conn,\n\t\t     struct smc_wr_buf *wr_buf,\n\t\t     struct smc_cdc_tx_pend *pend)\n{\n\tstruct smc_link *link = conn->lnk;\n\tunion smc_host_cursor cfed;\n\tint rc;\n\n\tsmc_cdc_add_pending_send(conn, pend);\n\n\tconn->tx_cdc_seq++;\n\tconn->local_tx_ctrl.seqno = conn->tx_cdc_seq;\n\tsmc_host_msg_to_cdc((struct smc_cdc_msg *)wr_buf, conn, &cfed);\n\n\tatomic_inc(&conn->cdc_pend_tx_wr);\n\tsmp_mb__after_atomic();  \n\n\trc = smc_wr_tx_send(link, (struct smc_wr_tx_pend_priv *)pend);\n\tif (!rc) {\n\t\tsmc_curs_copy(&conn->rx_curs_confirmed, &cfed, conn);\n\t\tconn->local_rx_ctrl.prod_flags.cons_curs_upd_req = 0;\n\t} else {\n\t\tconn->tx_cdc_seq--;\n\t\tconn->local_tx_ctrl.seqno = conn->tx_cdc_seq;\n\t\tatomic_dec(&conn->cdc_pend_tx_wr);\n\t}\n\n\treturn rc;\n}\n\n \nint smcr_cdc_msg_send_validation(struct smc_connection *conn,\n\t\t\t\t struct smc_cdc_tx_pend *pend,\n\t\t\t\t struct smc_wr_buf *wr_buf)\n{\n\tstruct smc_host_cdc_msg *local = &conn->local_tx_ctrl;\n\tstruct smc_link *link = conn->lnk;\n\tstruct smc_cdc_msg *peer;\n\tint rc;\n\n\tpeer = (struct smc_cdc_msg *)wr_buf;\n\tpeer->common.type = local->common.type;\n\tpeer->len = local->len;\n\tpeer->seqno = htons(conn->tx_cdc_seq_fin);  \n\tpeer->token = htonl(local->token);\n\tpeer->prod_flags.failover_validation = 1;\n\n\t \n\tsmc_cdc_add_pending_send(conn, pend);\n\n\tatomic_inc(&conn->cdc_pend_tx_wr);\n\tsmp_mb__after_atomic();  \n\n\trc = smc_wr_tx_send(link, (struct smc_wr_tx_pend_priv *)pend);\n\tif (unlikely(rc))\n\t\tatomic_dec(&conn->cdc_pend_tx_wr);\n\n\treturn rc;\n}\n\nstatic int smcr_cdc_get_slot_and_msg_send(struct smc_connection *conn)\n{\n\tstruct smc_cdc_tx_pend *pend;\n\tstruct smc_wr_buf *wr_buf;\n\tstruct smc_link *link;\n\tbool again = false;\n\tint rc;\n\nagain:\n\tlink = conn->lnk;\n\tif (!smc_wr_tx_link_hold(link))\n\t\treturn -ENOLINK;\n\trc = smc_cdc_get_free_slot(conn, link, &wr_buf, NULL, &pend);\n\tif (rc)\n\t\tgoto put_out;\n\n\tspin_lock_bh(&conn->send_lock);\n\tif (link != conn->lnk) {\n\t\t \n\t\tspin_unlock_bh(&conn->send_lock);\n\t\tsmc_wr_tx_put_slot(link,\n\t\t\t\t   (struct smc_wr_tx_pend_priv *)pend);\n\t\tsmc_wr_tx_link_put(link);\n\t\tif (again)\n\t\t\treturn -ENOLINK;\n\t\tagain = true;\n\t\tgoto again;\n\t}\n\trc = smc_cdc_msg_send(conn, wr_buf, pend);\n\tspin_unlock_bh(&conn->send_lock);\nput_out:\n\tsmc_wr_tx_link_put(link);\n\treturn rc;\n}\n\nint smc_cdc_get_slot_and_msg_send(struct smc_connection *conn)\n{\n\tint rc;\n\n\tif (!smc_conn_lgr_valid(conn) ||\n\t    (conn->lgr->is_smcd && conn->lgr->peer_shutdown))\n\t\treturn -EPIPE;\n\n\tif (conn->lgr->is_smcd) {\n\t\tspin_lock_bh(&conn->send_lock);\n\t\trc = smcd_cdc_msg_send(conn);\n\t\tspin_unlock_bh(&conn->send_lock);\n\t} else {\n\t\trc = smcr_cdc_get_slot_and_msg_send(conn);\n\t}\n\n\treturn rc;\n}\n\nvoid smc_cdc_wait_pend_tx_wr(struct smc_connection *conn)\n{\n\twait_event(conn->cdc_pend_tx_wq, !atomic_read(&conn->cdc_pend_tx_wr));\n}\n\n \nint smcd_cdc_msg_send(struct smc_connection *conn)\n{\n\tstruct smc_sock *smc = container_of(conn, struct smc_sock, conn);\n\tunion smc_host_cursor curs;\n\tstruct smcd_cdc_msg cdc;\n\tint rc, diff;\n\n\tmemset(&cdc, 0, sizeof(cdc));\n\tcdc.common.type = SMC_CDC_MSG_TYPE;\n\tcurs.acurs.counter = atomic64_read(&conn->local_tx_ctrl.prod.acurs);\n\tcdc.prod.wrap = curs.wrap;\n\tcdc.prod.count = curs.count;\n\tcurs.acurs.counter = atomic64_read(&conn->local_tx_ctrl.cons.acurs);\n\tcdc.cons.wrap = curs.wrap;\n\tcdc.cons.count = curs.count;\n\tcdc.cons.prod_flags = conn->local_tx_ctrl.prod_flags;\n\tcdc.cons.conn_state_flags = conn->local_tx_ctrl.conn_state_flags;\n\trc = smcd_tx_ism_write(conn, &cdc, sizeof(cdc), 0, 1);\n\tif (rc)\n\t\treturn rc;\n\tsmc_curs_copy(&conn->rx_curs_confirmed, &curs, conn);\n\tconn->local_rx_ctrl.prod_flags.cons_curs_upd_req = 0;\n\t \n\tdiff = smc_curs_diff(conn->sndbuf_desc->len, &conn->tx_curs_fin,\n\t\t\t     &conn->tx_curs_sent);\n\t \n\tsmp_mb__before_atomic();\n\tatomic_add(diff, &conn->sndbuf_space);\n\t \n\tsmp_mb__after_atomic();\n\tsmc_curs_copy(&conn->tx_curs_fin, &conn->tx_curs_sent, conn);\n\n\tsmc_tx_sndbuf_nonfull(smc);\n\treturn rc;\n}\n\n \n\nstatic inline bool smc_cdc_before(u16 seq1, u16 seq2)\n{\n\treturn (s16)(seq1 - seq2) < 0;\n}\n\nstatic void smc_cdc_handle_urg_data_arrival(struct smc_sock *smc,\n\t\t\t\t\t    int *diff_prod)\n{\n\tstruct smc_connection *conn = &smc->conn;\n\tchar *base;\n\n\t \n\tsmc_curs_copy(&conn->urg_curs, &conn->local_rx_ctrl.prod, conn);\n\tconn->urg_state = SMC_URG_VALID;\n\tif (!sock_flag(&smc->sk, SOCK_URGINLINE))\n\t\t \n\t\t(*diff_prod)--;\n\tbase = (char *)conn->rmb_desc->cpu_addr + conn->rx_off;\n\tif (conn->urg_curs.count)\n\t\tconn->urg_rx_byte = *(base + conn->urg_curs.count - 1);\n\telse\n\t\tconn->urg_rx_byte = *(base + conn->rmb_desc->len - 1);\n\tsk_send_sigurg(&smc->sk);\n}\n\nstatic void smc_cdc_msg_validate(struct smc_sock *smc, struct smc_cdc_msg *cdc,\n\t\t\t\t struct smc_link *link)\n{\n\tstruct smc_connection *conn = &smc->conn;\n\tu16 recv_seq = ntohs(cdc->seqno);\n\ts16 diff;\n\n\t \n\tdiff = conn->local_rx_ctrl.seqno - recv_seq;\n\tif (diff < 0) {  \n\t\t \n\t\tconn->out_of_sync = 1;\t \n\t\tspin_lock_bh(&conn->send_lock);\n\t\tconn->local_tx_ctrl.conn_state_flags.peer_conn_abort = 1;\n\t\tconn->lnk = link;\n\t\tspin_unlock_bh(&conn->send_lock);\n\t\tsock_hold(&smc->sk);  \n\t\tif (!queue_work(smc_close_wq, &conn->abort_work))\n\t\t\tsock_put(&smc->sk);\n\t}\n}\n\nstatic void smc_cdc_msg_recv_action(struct smc_sock *smc,\n\t\t\t\t    struct smc_cdc_msg *cdc)\n{\n\tunion smc_host_cursor cons_old, prod_old;\n\tstruct smc_connection *conn = &smc->conn;\n\tint diff_cons, diff_prod;\n\n\tsmc_curs_copy(&prod_old, &conn->local_rx_ctrl.prod, conn);\n\tsmc_curs_copy(&cons_old, &conn->local_rx_ctrl.cons, conn);\n\tsmc_cdc_msg_to_host(&conn->local_rx_ctrl, cdc, conn);\n\n\tdiff_cons = smc_curs_diff(conn->peer_rmbe_size, &cons_old,\n\t\t\t\t  &conn->local_rx_ctrl.cons);\n\tif (diff_cons) {\n\t\t \n\t\tsmp_mb__before_atomic();\n\t\tatomic_add(diff_cons, &conn->peer_rmbe_space);\n\t\t \n\t\tsmp_mb__after_atomic();\n\t}\n\n\tdiff_prod = smc_curs_diff(conn->rmb_desc->len, &prod_old,\n\t\t\t\t  &conn->local_rx_ctrl.prod);\n\tif (diff_prod) {\n\t\tif (conn->local_rx_ctrl.prod_flags.urg_data_present)\n\t\t\tsmc_cdc_handle_urg_data_arrival(smc, &diff_prod);\n\t\t \n\t\tsmp_mb__before_atomic();\n\t\tatomic_add(diff_prod, &conn->bytes_to_rcv);\n\t\t \n\t\tsmp_mb__after_atomic();\n\t\tsmc->sk.sk_data_ready(&smc->sk);\n\t} else {\n\t\tif (conn->local_rx_ctrl.prod_flags.write_blocked)\n\t\t\tsmc->sk.sk_data_ready(&smc->sk);\n\t\tif (conn->local_rx_ctrl.prod_flags.urg_data_pending)\n\t\t\tconn->urg_state = SMC_URG_NOTYET;\n\t}\n\n\t \n\tif ((diff_cons && smc_tx_prepared_sends(conn)) ||\n\t    conn->local_rx_ctrl.prod_flags.cons_curs_upd_req ||\n\t    conn->local_rx_ctrl.prod_flags.urg_data_pending) {\n\t\tif (!sock_owned_by_user(&smc->sk))\n\t\t\tsmc_tx_pending(conn);\n\t\telse\n\t\t\tconn->tx_in_release_sock = true;\n\t}\n\n\tif (diff_cons && conn->urg_tx_pend &&\n\t    atomic_read(&conn->peer_rmbe_space) == conn->peer_rmbe_size) {\n\t\t \n\t\tconn->urg_tx_pend = false;\n\t\tsmc->sk.sk_write_space(&smc->sk);\n\t}\n\n\tif (conn->local_rx_ctrl.conn_state_flags.peer_conn_abort) {\n\t\tsmc->sk.sk_err = ECONNRESET;\n\t\tconn->local_tx_ctrl.conn_state_flags.peer_conn_abort = 1;\n\t}\n\tif (smc_cdc_rxed_any_close_or_senddone(conn)) {\n\t\tsmc->sk.sk_shutdown |= RCV_SHUTDOWN;\n\t\tif (smc->clcsock && smc->clcsock->sk)\n\t\t\tsmc->clcsock->sk->sk_shutdown |= RCV_SHUTDOWN;\n\t\tsmc_sock_set_flag(&smc->sk, SOCK_DONE);\n\t\tsock_hold(&smc->sk);  \n\t\tif (!queue_work(smc_close_wq, &conn->close_work))\n\t\t\tsock_put(&smc->sk);\n\t}\n}\n\n \nstatic void smc_cdc_msg_recv(struct smc_sock *smc, struct smc_cdc_msg *cdc)\n{\n\tsock_hold(&smc->sk);\n\tbh_lock_sock(&smc->sk);\n\tsmc_cdc_msg_recv_action(smc, cdc);\n\tbh_unlock_sock(&smc->sk);\n\tsock_put(&smc->sk);  \n}\n\n \nstatic void smcd_cdc_rx_tsklet(struct tasklet_struct *t)\n{\n\tstruct smc_connection *conn = from_tasklet(conn, t, rx_tsklet);\n\tstruct smcd_cdc_msg *data_cdc;\n\tstruct smcd_cdc_msg cdc;\n\tstruct smc_sock *smc;\n\n\tif (!conn || conn->killed)\n\t\treturn;\n\n\tdata_cdc = (struct smcd_cdc_msg *)conn->rmb_desc->cpu_addr;\n\tsmcd_curs_copy(&cdc.prod, &data_cdc->prod, conn);\n\tsmcd_curs_copy(&cdc.cons, &data_cdc->cons, conn);\n\tsmc = container_of(conn, struct smc_sock, conn);\n\tsmc_cdc_msg_recv(smc, (struct smc_cdc_msg *)&cdc);\n}\n\n \nvoid smcd_cdc_rx_init(struct smc_connection *conn)\n{\n\ttasklet_setup(&conn->rx_tsklet, smcd_cdc_rx_tsklet);\n}\n\n \n\nstatic void smc_cdc_rx_handler(struct ib_wc *wc, void *buf)\n{\n\tstruct smc_link *link = (struct smc_link *)wc->qp->qp_context;\n\tstruct smc_cdc_msg *cdc = buf;\n\tstruct smc_connection *conn;\n\tstruct smc_link_group *lgr;\n\tstruct smc_sock *smc;\n\n\tif (wc->byte_len < offsetof(struct smc_cdc_msg, reserved))\n\t\treturn;  \n\tif (cdc->len != SMC_WR_TX_SIZE)\n\t\treturn;  \n\n\t \n\tlgr = smc_get_lgr(link);\n\tread_lock_bh(&lgr->conns_lock);\n\tconn = smc_lgr_find_conn(ntohl(cdc->token), lgr);\n\tread_unlock_bh(&lgr->conns_lock);\n\tif (!conn || conn->out_of_sync)\n\t\treturn;\n\tsmc = container_of(conn, struct smc_sock, conn);\n\n\tif (cdc->prod_flags.failover_validation) {\n\t\tsmc_cdc_msg_validate(smc, cdc, link);\n\t\treturn;\n\t}\n\tif (smc_cdc_before(ntohs(cdc->seqno),\n\t\t\t   conn->local_rx_ctrl.seqno))\n\t\t \n\t\treturn;\n\n\tsmc_cdc_msg_recv(smc, cdc);\n}\n\nstatic struct smc_wr_rx_handler smc_cdc_rx_handlers[] = {\n\t{\n\t\t.handler\t= smc_cdc_rx_handler,\n\t\t.type\t\t= SMC_CDC_MSG_TYPE\n\t},\n\t{\n\t\t.handler\t= NULL,\n\t}\n};\n\nint __init smc_cdc_init(void)\n{\n\tstruct smc_wr_rx_handler *handler;\n\tint rc = 0;\n\n\tfor (handler = smc_cdc_rx_handlers; handler->handler; handler++) {\n\t\tINIT_HLIST_NODE(&handler->list);\n\t\trc = smc_wr_rx_register_handler(handler);\n\t\tif (rc)\n\t\t\tbreak;\n\t}\n\treturn rc;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}