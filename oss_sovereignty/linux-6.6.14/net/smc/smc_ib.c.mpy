{
  "module_name": "smc_ib.c",
  "hash_id": "8239510ffbe93031085cee84a31527ab789d0c01ed02584d8b282a34759f6bcf",
  "original_prompt": "Ingested from linux-6.6.14/net/smc/smc_ib.c",
  "human_readable_source": "\n \n\n#include <linux/etherdevice.h>\n#include <linux/if_vlan.h>\n#include <linux/random.h>\n#include <linux/workqueue.h>\n#include <linux/scatterlist.h>\n#include <linux/wait.h>\n#include <linux/mutex.h>\n#include <linux/inetdevice.h>\n#include <rdma/ib_verbs.h>\n#include <rdma/ib_cache.h>\n\n#include \"smc_pnet.h\"\n#include \"smc_ib.h\"\n#include \"smc_core.h\"\n#include \"smc_wr.h\"\n#include \"smc.h\"\n#include \"smc_netlink.h\"\n\n#define SMC_MAX_CQE 32766\t \n\n#define SMC_QP_MIN_RNR_TIMER\t\t5\n#define SMC_QP_TIMEOUT\t\t\t15  \n#define SMC_QP_RETRY_CNT\t\t\t7  \n#define SMC_QP_RNR_RETRY\t\t\t7  \n\nstruct smc_ib_devices smc_ib_devices = {\t \n\t.mutex = __MUTEX_INITIALIZER(smc_ib_devices.mutex),\n\t.list = LIST_HEAD_INIT(smc_ib_devices.list),\n};\n\nu8 local_systemid[SMC_SYSTEMID_LEN];\t\t \n\nstatic int smc_ib_modify_qp_init(struct smc_link *lnk)\n{\n\tstruct ib_qp_attr qp_attr;\n\n\tmemset(&qp_attr, 0, sizeof(qp_attr));\n\tqp_attr.qp_state = IB_QPS_INIT;\n\tqp_attr.pkey_index = 0;\n\tqp_attr.port_num = lnk->ibport;\n\tqp_attr.qp_access_flags = IB_ACCESS_LOCAL_WRITE\n\t\t\t\t| IB_ACCESS_REMOTE_WRITE;\n\treturn ib_modify_qp(lnk->roce_qp, &qp_attr,\n\t\t\t    IB_QP_STATE | IB_QP_PKEY_INDEX |\n\t\t\t    IB_QP_ACCESS_FLAGS | IB_QP_PORT);\n}\n\nstatic int smc_ib_modify_qp_rtr(struct smc_link *lnk)\n{\n\tenum ib_qp_attr_mask qp_attr_mask =\n\t\tIB_QP_STATE | IB_QP_AV | IB_QP_PATH_MTU | IB_QP_DEST_QPN |\n\t\tIB_QP_RQ_PSN | IB_QP_MAX_DEST_RD_ATOMIC | IB_QP_MIN_RNR_TIMER;\n\tstruct ib_qp_attr qp_attr;\n\tu8 hop_lim = 1;\n\n\tmemset(&qp_attr, 0, sizeof(qp_attr));\n\tqp_attr.qp_state = IB_QPS_RTR;\n\tqp_attr.path_mtu = min(lnk->path_mtu, lnk->peer_mtu);\n\tqp_attr.ah_attr.type = RDMA_AH_ATTR_TYPE_ROCE;\n\trdma_ah_set_port_num(&qp_attr.ah_attr, lnk->ibport);\n\tif (lnk->lgr->smc_version == SMC_V2 && lnk->lgr->uses_gateway)\n\t\thop_lim = IPV6_DEFAULT_HOPLIMIT;\n\trdma_ah_set_grh(&qp_attr.ah_attr, NULL, 0, lnk->sgid_index, hop_lim, 0);\n\trdma_ah_set_dgid_raw(&qp_attr.ah_attr, lnk->peer_gid);\n\tif (lnk->lgr->smc_version == SMC_V2 && lnk->lgr->uses_gateway)\n\t\tmemcpy(&qp_attr.ah_attr.roce.dmac, lnk->lgr->nexthop_mac,\n\t\t       sizeof(lnk->lgr->nexthop_mac));\n\telse\n\t\tmemcpy(&qp_attr.ah_attr.roce.dmac, lnk->peer_mac,\n\t\t       sizeof(lnk->peer_mac));\n\tqp_attr.dest_qp_num = lnk->peer_qpn;\n\tqp_attr.rq_psn = lnk->peer_psn;  \n\tqp_attr.max_dest_rd_atomic = 1;  \n\tqp_attr.min_rnr_timer = SMC_QP_MIN_RNR_TIMER;\n\n\treturn ib_modify_qp(lnk->roce_qp, &qp_attr, qp_attr_mask);\n}\n\nint smc_ib_modify_qp_rts(struct smc_link *lnk)\n{\n\tstruct ib_qp_attr qp_attr;\n\n\tmemset(&qp_attr, 0, sizeof(qp_attr));\n\tqp_attr.qp_state = IB_QPS_RTS;\n\tqp_attr.timeout = SMC_QP_TIMEOUT;\t \n\tqp_attr.retry_cnt = SMC_QP_RETRY_CNT;\t \n\tqp_attr.rnr_retry = SMC_QP_RNR_RETRY;\t \n\tqp_attr.sq_psn = lnk->psn_initial;\t \n\tqp_attr.max_rd_atomic = 1;\t \n\treturn ib_modify_qp(lnk->roce_qp, &qp_attr,\n\t\t\t    IB_QP_STATE | IB_QP_TIMEOUT | IB_QP_RETRY_CNT |\n\t\t\t    IB_QP_SQ_PSN | IB_QP_RNR_RETRY |\n\t\t\t    IB_QP_MAX_QP_RD_ATOMIC);\n}\n\nint smc_ib_modify_qp_error(struct smc_link *lnk)\n{\n\tstruct ib_qp_attr qp_attr;\n\n\tmemset(&qp_attr, 0, sizeof(qp_attr));\n\tqp_attr.qp_state = IB_QPS_ERR;\n\treturn ib_modify_qp(lnk->roce_qp, &qp_attr, IB_QP_STATE);\n}\n\nint smc_ib_ready_link(struct smc_link *lnk)\n{\n\tstruct smc_link_group *lgr = smc_get_lgr(lnk);\n\tint rc = 0;\n\n\trc = smc_ib_modify_qp_init(lnk);\n\tif (rc)\n\t\tgoto out;\n\n\trc = smc_ib_modify_qp_rtr(lnk);\n\tif (rc)\n\t\tgoto out;\n\tsmc_wr_remember_qp_attr(lnk);\n\trc = ib_req_notify_cq(lnk->smcibdev->roce_cq_recv,\n\t\t\t      IB_CQ_SOLICITED_MASK);\n\tif (rc)\n\t\tgoto out;\n\trc = smc_wr_rx_post_init(lnk);\n\tif (rc)\n\t\tgoto out;\n\tsmc_wr_remember_qp_attr(lnk);\n\n\tif (lgr->role == SMC_SERV) {\n\t\trc = smc_ib_modify_qp_rts(lnk);\n\t\tif (rc)\n\t\t\tgoto out;\n\t\tsmc_wr_remember_qp_attr(lnk);\n\t}\nout:\n\treturn rc;\n}\n\nstatic int smc_ib_fill_mac(struct smc_ib_device *smcibdev, u8 ibport)\n{\n\tconst struct ib_gid_attr *attr;\n\tint rc;\n\n\tattr = rdma_get_gid_attr(smcibdev->ibdev, ibport, 0);\n\tif (IS_ERR(attr))\n\t\treturn -ENODEV;\n\n\trc = rdma_read_gid_l2_fields(attr, NULL, smcibdev->mac[ibport - 1]);\n\trdma_put_gid_attr(attr);\n\treturn rc;\n}\n\n \nstatic inline void smc_ib_define_local_systemid(struct smc_ib_device *smcibdev,\n\t\t\t\t\t\tu8 ibport)\n{\n\tmemcpy(&local_systemid[2], &smcibdev->mac[ibport - 1],\n\t       sizeof(smcibdev->mac[ibport - 1]));\n}\n\nbool smc_ib_is_valid_local_systemid(void)\n{\n\treturn !is_zero_ether_addr(&local_systemid[2]);\n}\n\nstatic void smc_ib_init_local_systemid(void)\n{\n\tget_random_bytes(&local_systemid[0], 2);\n}\n\nbool smc_ib_port_active(struct smc_ib_device *smcibdev, u8 ibport)\n{\n\treturn smcibdev->pattr[ibport - 1].state == IB_PORT_ACTIVE;\n}\n\nint smc_ib_find_route(struct net *net, __be32 saddr, __be32 daddr,\n\t\t      u8 nexthop_mac[], u8 *uses_gateway)\n{\n\tstruct neighbour *neigh = NULL;\n\tstruct rtable *rt = NULL;\n\tstruct flowi4 fl4 = {\n\t\t.saddr = saddr,\n\t\t.daddr = daddr\n\t};\n\n\tif (daddr == cpu_to_be32(INADDR_NONE))\n\t\tgoto out;\n\trt = ip_route_output_flow(net, &fl4, NULL);\n\tif (IS_ERR(rt))\n\t\tgoto out;\n\tif (rt->rt_uses_gateway && rt->rt_gw_family != AF_INET)\n\t\tgoto out;\n\tneigh = rt->dst.ops->neigh_lookup(&rt->dst, NULL, &fl4.daddr);\n\tif (neigh) {\n\t\tmemcpy(nexthop_mac, neigh->ha, ETH_ALEN);\n\t\t*uses_gateway = rt->rt_uses_gateway;\n\t\treturn 0;\n\t}\nout:\n\treturn -ENOENT;\n}\n\nstatic int smc_ib_determine_gid_rcu(const struct net_device *ndev,\n\t\t\t\t    const struct ib_gid_attr *attr,\n\t\t\t\t    u8 gid[], u8 *sgid_index,\n\t\t\t\t    struct smc_init_info_smcrv2 *smcrv2)\n{\n\tif (!smcrv2 && attr->gid_type == IB_GID_TYPE_ROCE) {\n\t\tif (gid)\n\t\t\tmemcpy(gid, &attr->gid, SMC_GID_SIZE);\n\t\tif (sgid_index)\n\t\t\t*sgid_index = attr->index;\n\t\treturn 0;\n\t}\n\tif (smcrv2 && attr->gid_type == IB_GID_TYPE_ROCE_UDP_ENCAP &&\n\t    smc_ib_gid_to_ipv4((u8 *)&attr->gid) != cpu_to_be32(INADDR_NONE)) {\n\t\tstruct in_device *in_dev = __in_dev_get_rcu(ndev);\n\t\tstruct net *net = dev_net(ndev);\n\t\tconst struct in_ifaddr *ifa;\n\t\tbool subnet_match = false;\n\n\t\tif (!in_dev)\n\t\t\tgoto out;\n\t\tin_dev_for_each_ifa_rcu(ifa, in_dev) {\n\t\t\tif (!inet_ifa_match(smcrv2->saddr, ifa))\n\t\t\t\tcontinue;\n\t\t\tsubnet_match = true;\n\t\t\tbreak;\n\t\t}\n\t\tif (!subnet_match)\n\t\t\tgoto out;\n\t\tif (smcrv2->daddr && smc_ib_find_route(net, smcrv2->saddr,\n\t\t\t\t\t\t       smcrv2->daddr,\n\t\t\t\t\t\t       smcrv2->nexthop_mac,\n\t\t\t\t\t\t       &smcrv2->uses_gateway))\n\t\t\tgoto out;\n\n\t\tif (gid)\n\t\t\tmemcpy(gid, &attr->gid, SMC_GID_SIZE);\n\t\tif (sgid_index)\n\t\t\t*sgid_index = attr->index;\n\t\treturn 0;\n\t}\nout:\n\treturn -ENODEV;\n}\n\n \nint smc_ib_determine_gid(struct smc_ib_device *smcibdev, u8 ibport,\n\t\t\t unsigned short vlan_id, u8 gid[], u8 *sgid_index,\n\t\t\t struct smc_init_info_smcrv2 *smcrv2)\n{\n\tconst struct ib_gid_attr *attr;\n\tconst struct net_device *ndev;\n\tint i;\n\n\tfor (i = 0; i < smcibdev->pattr[ibport - 1].gid_tbl_len; i++) {\n\t\tattr = rdma_get_gid_attr(smcibdev->ibdev, ibport, i);\n\t\tif (IS_ERR(attr))\n\t\t\tcontinue;\n\n\t\trcu_read_lock();\n\t\tndev = rdma_read_gid_attr_ndev_rcu(attr);\n\t\tif (!IS_ERR(ndev) &&\n\t\t    ((!vlan_id && !is_vlan_dev(ndev)) ||\n\t\t     (vlan_id && is_vlan_dev(ndev) &&\n\t\t      vlan_dev_vlan_id(ndev) == vlan_id))) {\n\t\t\tif (!smc_ib_determine_gid_rcu(ndev, attr, gid,\n\t\t\t\t\t\t      sgid_index, smcrv2)) {\n\t\t\t\trcu_read_unlock();\n\t\t\t\trdma_put_gid_attr(attr);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\t\trcu_read_unlock();\n\t\trdma_put_gid_attr(attr);\n\t}\n\treturn -ENODEV;\n}\n\n \nstatic bool smc_ib_check_link_gid(u8 gid[SMC_GID_SIZE], bool smcrv2,\n\t\t\t\t  struct smc_ib_device *smcibdev, u8 ibport)\n{\n\tconst struct ib_gid_attr *attr;\n\tbool rc = false;\n\tint i;\n\n\tfor (i = 0; !rc && i < smcibdev->pattr[ibport - 1].gid_tbl_len; i++) {\n\t\tattr = rdma_get_gid_attr(smcibdev->ibdev, ibport, i);\n\t\tif (IS_ERR(attr))\n\t\t\tcontinue;\n\n\t\trcu_read_lock();\n\t\tif ((!smcrv2 && attr->gid_type == IB_GID_TYPE_ROCE) ||\n\t\t    (smcrv2 && attr->gid_type == IB_GID_TYPE_ROCE_UDP_ENCAP &&\n\t\t     !(ipv6_addr_type((const struct in6_addr *)&attr->gid)\n\t\t\t\t     & IPV6_ADDR_LINKLOCAL)))\n\t\t\tif (!memcmp(gid, &attr->gid, SMC_GID_SIZE))\n\t\t\t\trc = true;\n\t\trcu_read_unlock();\n\t\trdma_put_gid_attr(attr);\n\t}\n\treturn rc;\n}\n\n \nstatic void smc_ib_gid_check(struct smc_ib_device *smcibdev, u8 ibport)\n{\n\tstruct smc_link_group *lgr;\n\tint i;\n\n\tspin_lock_bh(&smc_lgr_list.lock);\n\tlist_for_each_entry(lgr, &smc_lgr_list.list, list) {\n\t\tif (strncmp(smcibdev->pnetid[ibport - 1], lgr->pnet_id,\n\t\t\t    SMC_MAX_PNETID_LEN))\n\t\t\tcontinue;  \n\t\tif (list_empty(&lgr->list))\n\t\t\tcontinue;\n\t\tfor (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++) {\n\t\t\tif (lgr->lnk[i].state == SMC_LNK_UNUSED ||\n\t\t\t    lgr->lnk[i].smcibdev != smcibdev)\n\t\t\t\tcontinue;\n\t\t\tif (!smc_ib_check_link_gid(lgr->lnk[i].gid,\n\t\t\t\t\t\t   lgr->smc_version == SMC_V2,\n\t\t\t\t\t\t   smcibdev, ibport))\n\t\t\t\tsmcr_port_err(smcibdev, ibport);\n\t\t}\n\t}\n\tspin_unlock_bh(&smc_lgr_list.lock);\n}\n\nstatic int smc_ib_remember_port_attr(struct smc_ib_device *smcibdev, u8 ibport)\n{\n\tint rc;\n\n\tmemset(&smcibdev->pattr[ibport - 1], 0,\n\t       sizeof(smcibdev->pattr[ibport - 1]));\n\trc = ib_query_port(smcibdev->ibdev, ibport,\n\t\t\t   &smcibdev->pattr[ibport - 1]);\n\tif (rc)\n\t\tgoto out;\n\t \n\trc = smc_ib_fill_mac(smcibdev, ibport);\n\tif (rc)\n\t\tgoto out;\n\tif (!smc_ib_is_valid_local_systemid() &&\n\t    smc_ib_port_active(smcibdev, ibport))\n\t\t \n\t\tsmc_ib_define_local_systemid(smcibdev, ibport);\nout:\n\treturn rc;\n}\n\n \nstatic void smc_ib_port_event_work(struct work_struct *work)\n{\n\tstruct smc_ib_device *smcibdev = container_of(\n\t\twork, struct smc_ib_device, port_event_work);\n\tu8 port_idx;\n\n\tfor_each_set_bit(port_idx, &smcibdev->port_event_mask, SMC_MAX_PORTS) {\n\t\tsmc_ib_remember_port_attr(smcibdev, port_idx + 1);\n\t\tclear_bit(port_idx, &smcibdev->port_event_mask);\n\t\tif (!smc_ib_port_active(smcibdev, port_idx + 1)) {\n\t\t\tset_bit(port_idx, smcibdev->ports_going_away);\n\t\t\tsmcr_port_err(smcibdev, port_idx + 1);\n\t\t} else {\n\t\t\tclear_bit(port_idx, smcibdev->ports_going_away);\n\t\t\tsmcr_port_add(smcibdev, port_idx + 1);\n\t\t\tsmc_ib_gid_check(smcibdev, port_idx + 1);\n\t\t}\n\t}\n}\n\n \nstatic void smc_ib_global_event_handler(struct ib_event_handler *handler,\n\t\t\t\t\tstruct ib_event *ibevent)\n{\n\tstruct smc_ib_device *smcibdev;\n\tbool schedule = false;\n\tu8 port_idx;\n\n\tsmcibdev = container_of(handler, struct smc_ib_device, event_handler);\n\n\tswitch (ibevent->event) {\n\tcase IB_EVENT_DEVICE_FATAL:\n\t\t \n\t\tfor (port_idx = 0; port_idx < SMC_MAX_PORTS; port_idx++) {\n\t\t\tset_bit(port_idx, &smcibdev->port_event_mask);\n\t\t\tif (!test_and_set_bit(port_idx,\n\t\t\t\t\t      smcibdev->ports_going_away))\n\t\t\t\tschedule = true;\n\t\t}\n\t\tif (schedule)\n\t\t\tschedule_work(&smcibdev->port_event_work);\n\t\tbreak;\n\tcase IB_EVENT_PORT_ACTIVE:\n\t\tport_idx = ibevent->element.port_num - 1;\n\t\tif (port_idx >= SMC_MAX_PORTS)\n\t\t\tbreak;\n\t\tset_bit(port_idx, &smcibdev->port_event_mask);\n\t\tif (test_and_clear_bit(port_idx, smcibdev->ports_going_away))\n\t\t\tschedule_work(&smcibdev->port_event_work);\n\t\tbreak;\n\tcase IB_EVENT_PORT_ERR:\n\t\tport_idx = ibevent->element.port_num - 1;\n\t\tif (port_idx >= SMC_MAX_PORTS)\n\t\t\tbreak;\n\t\tset_bit(port_idx, &smcibdev->port_event_mask);\n\t\tif (!test_and_set_bit(port_idx, smcibdev->ports_going_away))\n\t\t\tschedule_work(&smcibdev->port_event_work);\n\t\tbreak;\n\tcase IB_EVENT_GID_CHANGE:\n\t\tport_idx = ibevent->element.port_num - 1;\n\t\tif (port_idx >= SMC_MAX_PORTS)\n\t\t\tbreak;\n\t\tset_bit(port_idx, &smcibdev->port_event_mask);\n\t\tschedule_work(&smcibdev->port_event_work);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nvoid smc_ib_dealloc_protection_domain(struct smc_link *lnk)\n{\n\tif (lnk->roce_pd)\n\t\tib_dealloc_pd(lnk->roce_pd);\n\tlnk->roce_pd = NULL;\n}\n\nint smc_ib_create_protection_domain(struct smc_link *lnk)\n{\n\tint rc;\n\n\tlnk->roce_pd = ib_alloc_pd(lnk->smcibdev->ibdev, 0);\n\trc = PTR_ERR_OR_ZERO(lnk->roce_pd);\n\tif (IS_ERR(lnk->roce_pd))\n\t\tlnk->roce_pd = NULL;\n\treturn rc;\n}\n\nstatic bool smcr_diag_is_dev_critical(struct smc_lgr_list *smc_lgr,\n\t\t\t\t      struct smc_ib_device *smcibdev)\n{\n\tstruct smc_link_group *lgr;\n\tbool rc = false;\n\tint i;\n\n\tspin_lock_bh(&smc_lgr->lock);\n\tlist_for_each_entry(lgr, &smc_lgr->list, list) {\n\t\tif (lgr->is_smcd)\n\t\t\tcontinue;\n\t\tfor (i = 0; i < SMC_LINKS_PER_LGR_MAX; i++) {\n\t\t\tif (lgr->lnk[i].state == SMC_LNK_UNUSED ||\n\t\t\t    lgr->lnk[i].smcibdev != smcibdev)\n\t\t\t\tcontinue;\n\t\t\tif (lgr->type == SMC_LGR_SINGLE ||\n\t\t\t    lgr->type == SMC_LGR_ASYMMETRIC_LOCAL) {\n\t\t\t\trc = true;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\nout:\n\tspin_unlock_bh(&smc_lgr->lock);\n\treturn rc;\n}\n\nstatic int smc_nl_handle_dev_port(struct sk_buff *skb,\n\t\t\t\t  struct ib_device *ibdev,\n\t\t\t\t  struct smc_ib_device *smcibdev,\n\t\t\t\t  int port)\n{\n\tchar smc_pnet[SMC_MAX_PNETID_LEN + 1];\n\tstruct nlattr *port_attrs;\n\tunsigned char port_state;\n\tint lnk_count = 0;\n\n\tport_attrs = nla_nest_start(skb, SMC_NLA_DEV_PORT + port);\n\tif (!port_attrs)\n\t\tgoto errout;\n\n\tif (nla_put_u8(skb, SMC_NLA_DEV_PORT_PNET_USR,\n\t\t       smcibdev->pnetid_by_user[port]))\n\t\tgoto errattr;\n\tmemcpy(smc_pnet, &smcibdev->pnetid[port], SMC_MAX_PNETID_LEN);\n\tsmc_pnet[SMC_MAX_PNETID_LEN] = 0;\n\tif (nla_put_string(skb, SMC_NLA_DEV_PORT_PNETID, smc_pnet))\n\t\tgoto errattr;\n\tif (nla_put_u32(skb, SMC_NLA_DEV_PORT_NETDEV,\n\t\t\tsmcibdev->ndev_ifidx[port]))\n\t\tgoto errattr;\n\tif (nla_put_u8(skb, SMC_NLA_DEV_PORT_VALID, 1))\n\t\tgoto errattr;\n\tport_state = smc_ib_port_active(smcibdev, port + 1);\n\tif (nla_put_u8(skb, SMC_NLA_DEV_PORT_STATE, port_state))\n\t\tgoto errattr;\n\tlnk_count = atomic_read(&smcibdev->lnk_cnt_by_port[port]);\n\tif (nla_put_u32(skb, SMC_NLA_DEV_PORT_LNK_CNT, lnk_count))\n\t\tgoto errattr;\n\tnla_nest_end(skb, port_attrs);\n\treturn 0;\nerrattr:\n\tnla_nest_cancel(skb, port_attrs);\nerrout:\n\treturn -EMSGSIZE;\n}\n\nstatic bool smc_nl_handle_pci_values(const struct smc_pci_dev *smc_pci_dev,\n\t\t\t\t     struct sk_buff *skb)\n{\n\tif (nla_put_u32(skb, SMC_NLA_DEV_PCI_FID, smc_pci_dev->pci_fid))\n\t\treturn false;\n\tif (nla_put_u16(skb, SMC_NLA_DEV_PCI_CHID, smc_pci_dev->pci_pchid))\n\t\treturn false;\n\tif (nla_put_u16(skb, SMC_NLA_DEV_PCI_VENDOR, smc_pci_dev->pci_vendor))\n\t\treturn false;\n\tif (nla_put_u16(skb, SMC_NLA_DEV_PCI_DEVICE, smc_pci_dev->pci_device))\n\t\treturn false;\n\tif (nla_put_string(skb, SMC_NLA_DEV_PCI_ID, smc_pci_dev->pci_id))\n\t\treturn false;\n\treturn true;\n}\n\nstatic int smc_nl_handle_smcr_dev(struct smc_ib_device *smcibdev,\n\t\t\t\t  struct sk_buff *skb,\n\t\t\t\t  struct netlink_callback *cb)\n{\n\tchar smc_ibname[IB_DEVICE_NAME_MAX];\n\tstruct smc_pci_dev smc_pci_dev;\n\tstruct pci_dev *pci_dev;\n\tunsigned char is_crit;\n\tstruct nlattr *attrs;\n\tvoid *nlh;\n\tint i;\n\n\tnlh = genlmsg_put(skb, NETLINK_CB(cb->skb).portid, cb->nlh->nlmsg_seq,\n\t\t\t  &smc_gen_nl_family, NLM_F_MULTI,\n\t\t\t  SMC_NETLINK_GET_DEV_SMCR);\n\tif (!nlh)\n\t\tgoto errmsg;\n\tattrs = nla_nest_start(skb, SMC_GEN_DEV_SMCR);\n\tif (!attrs)\n\t\tgoto errout;\n\tis_crit = smcr_diag_is_dev_critical(&smc_lgr_list, smcibdev);\n\tif (nla_put_u8(skb, SMC_NLA_DEV_IS_CRIT, is_crit))\n\t\tgoto errattr;\n\tif (smcibdev->ibdev->dev.parent) {\n\t\tmemset(&smc_pci_dev, 0, sizeof(smc_pci_dev));\n\t\tpci_dev = to_pci_dev(smcibdev->ibdev->dev.parent);\n\t\tsmc_set_pci_values(pci_dev, &smc_pci_dev);\n\t\tif (!smc_nl_handle_pci_values(&smc_pci_dev, skb))\n\t\t\tgoto errattr;\n\t}\n\tsnprintf(smc_ibname, sizeof(smc_ibname), \"%s\", smcibdev->ibdev->name);\n\tif (nla_put_string(skb, SMC_NLA_DEV_IB_NAME, smc_ibname))\n\t\tgoto errattr;\n\tfor (i = 1; i <= SMC_MAX_PORTS; i++) {\n\t\tif (!rdma_is_port_valid(smcibdev->ibdev, i))\n\t\t\tcontinue;\n\t\tif (smc_nl_handle_dev_port(skb, smcibdev->ibdev,\n\t\t\t\t\t   smcibdev, i - 1))\n\t\t\tgoto errattr;\n\t}\n\n\tnla_nest_end(skb, attrs);\n\tgenlmsg_end(skb, nlh);\n\treturn 0;\n\nerrattr:\n\tnla_nest_cancel(skb, attrs);\nerrout:\n\tgenlmsg_cancel(skb, nlh);\nerrmsg:\n\treturn -EMSGSIZE;\n}\n\nstatic void smc_nl_prep_smcr_dev(struct smc_ib_devices *dev_list,\n\t\t\t\t struct sk_buff *skb,\n\t\t\t\t struct netlink_callback *cb)\n{\n\tstruct smc_nl_dmp_ctx *cb_ctx = smc_nl_dmp_ctx(cb);\n\tstruct smc_ib_device *smcibdev;\n\tint snum = cb_ctx->pos[0];\n\tint num = 0;\n\n\tmutex_lock(&dev_list->mutex);\n\tlist_for_each_entry(smcibdev, &dev_list->list, list) {\n\t\tif (num < snum)\n\t\t\tgoto next;\n\t\tif (smc_nl_handle_smcr_dev(smcibdev, skb, cb))\n\t\t\tgoto errout;\nnext:\n\t\tnum++;\n\t}\nerrout:\n\tmutex_unlock(&dev_list->mutex);\n\tcb_ctx->pos[0] = num;\n}\n\nint smcr_nl_get_device(struct sk_buff *skb, struct netlink_callback *cb)\n{\n\tsmc_nl_prep_smcr_dev(&smc_ib_devices, skb, cb);\n\treturn skb->len;\n}\n\nstatic void smc_ib_qp_event_handler(struct ib_event *ibevent, void *priv)\n{\n\tstruct smc_link *lnk = (struct smc_link *)priv;\n\tstruct smc_ib_device *smcibdev = lnk->smcibdev;\n\tu8 port_idx;\n\n\tswitch (ibevent->event) {\n\tcase IB_EVENT_QP_FATAL:\n\tcase IB_EVENT_QP_ACCESS_ERR:\n\t\tport_idx = ibevent->element.qp->port - 1;\n\t\tif (port_idx >= SMC_MAX_PORTS)\n\t\t\tbreak;\n\t\tset_bit(port_idx, &smcibdev->port_event_mask);\n\t\tif (!test_and_set_bit(port_idx, smcibdev->ports_going_away))\n\t\t\tschedule_work(&smcibdev->port_event_work);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nvoid smc_ib_destroy_queue_pair(struct smc_link *lnk)\n{\n\tif (lnk->roce_qp)\n\t\tib_destroy_qp(lnk->roce_qp);\n\tlnk->roce_qp = NULL;\n}\n\n \nint smc_ib_create_queue_pair(struct smc_link *lnk)\n{\n\tint sges_per_buf = (lnk->lgr->smc_version == SMC_V2) ? 2 : 1;\n\tstruct ib_qp_init_attr qp_attr = {\n\t\t.event_handler = smc_ib_qp_event_handler,\n\t\t.qp_context = lnk,\n\t\t.send_cq = lnk->smcibdev->roce_cq_send,\n\t\t.recv_cq = lnk->smcibdev->roce_cq_recv,\n\t\t.srq = NULL,\n\t\t.cap = {\n\t\t\t\t \n\t\t\t.max_send_wr = SMC_WR_BUF_CNT * 3,\n\t\t\t.max_recv_wr = SMC_WR_BUF_CNT * 3,\n\t\t\t.max_send_sge = SMC_IB_MAX_SEND_SGE,\n\t\t\t.max_recv_sge = sges_per_buf,\n\t\t\t.max_inline_data = 0,\n\t\t},\n\t\t.sq_sig_type = IB_SIGNAL_REQ_WR,\n\t\t.qp_type = IB_QPT_RC,\n\t};\n\tint rc;\n\n\tlnk->roce_qp = ib_create_qp(lnk->roce_pd, &qp_attr);\n\trc = PTR_ERR_OR_ZERO(lnk->roce_qp);\n\tif (IS_ERR(lnk->roce_qp))\n\t\tlnk->roce_qp = NULL;\n\telse\n\t\tsmc_wr_remember_qp_attr(lnk);\n\treturn rc;\n}\n\nvoid smc_ib_put_memory_region(struct ib_mr *mr)\n{\n\tib_dereg_mr(mr);\n}\n\nstatic int smc_ib_map_mr_sg(struct smc_buf_desc *buf_slot, u8 link_idx)\n{\n\tunsigned int offset = 0;\n\tint sg_num;\n\n\t \n\tsg_num = ib_map_mr_sg(buf_slot->mr[link_idx],\n\t\t\t      buf_slot->sgt[link_idx].sgl,\n\t\t\t      buf_slot->sgt[link_idx].orig_nents,\n\t\t\t      &offset, PAGE_SIZE);\n\n\treturn sg_num;\n}\n\n \nint smc_ib_get_memory_region(struct ib_pd *pd, int access_flags,\n\t\t\t     struct smc_buf_desc *buf_slot, u8 link_idx)\n{\n\tif (buf_slot->mr[link_idx])\n\t\treturn 0;  \n\n\tbuf_slot->mr[link_idx] =\n\t\tib_alloc_mr(pd, IB_MR_TYPE_MEM_REG, 1 << buf_slot->order);\n\tif (IS_ERR(buf_slot->mr[link_idx])) {\n\t\tint rc;\n\n\t\trc = PTR_ERR(buf_slot->mr[link_idx]);\n\t\tbuf_slot->mr[link_idx] = NULL;\n\t\treturn rc;\n\t}\n\n\tif (smc_ib_map_mr_sg(buf_slot, link_idx) !=\n\t\t\t     buf_slot->sgt[link_idx].orig_nents)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nbool smc_ib_is_sg_need_sync(struct smc_link *lnk,\n\t\t\t    struct smc_buf_desc *buf_slot)\n{\n\tstruct scatterlist *sg;\n\tunsigned int i;\n\tbool ret = false;\n\n\t \n\tfor_each_sg(buf_slot->sgt[lnk->link_idx].sgl, sg,\n\t\t    buf_slot->sgt[lnk->link_idx].nents, i) {\n\t\tif (!sg_dma_len(sg))\n\t\t\tbreak;\n\t\tif (dma_need_sync(lnk->smcibdev->ibdev->dma_device,\n\t\t\t\t  sg_dma_address(sg))) {\n\t\t\tret = true;\n\t\t\tgoto out;\n\t\t}\n\t}\n\nout:\n\treturn ret;\n}\n\n \nvoid smc_ib_sync_sg_for_cpu(struct smc_link *lnk,\n\t\t\t    struct smc_buf_desc *buf_slot,\n\t\t\t    enum dma_data_direction data_direction)\n{\n\tstruct scatterlist *sg;\n\tunsigned int i;\n\n\tif (!(buf_slot->is_dma_need_sync & (1U << lnk->link_idx)))\n\t\treturn;\n\n\t \n\tfor_each_sg(buf_slot->sgt[lnk->link_idx].sgl, sg,\n\t\t    buf_slot->sgt[lnk->link_idx].nents, i) {\n\t\tif (!sg_dma_len(sg))\n\t\t\tbreak;\n\t\tib_dma_sync_single_for_cpu(lnk->smcibdev->ibdev,\n\t\t\t\t\t   sg_dma_address(sg),\n\t\t\t\t\t   sg_dma_len(sg),\n\t\t\t\t\t   data_direction);\n\t}\n}\n\n \nvoid smc_ib_sync_sg_for_device(struct smc_link *lnk,\n\t\t\t       struct smc_buf_desc *buf_slot,\n\t\t\t       enum dma_data_direction data_direction)\n{\n\tstruct scatterlist *sg;\n\tunsigned int i;\n\n\tif (!(buf_slot->is_dma_need_sync & (1U << lnk->link_idx)))\n\t\treturn;\n\n\t \n\tfor_each_sg(buf_slot->sgt[lnk->link_idx].sgl, sg,\n\t\t    buf_slot->sgt[lnk->link_idx].nents, i) {\n\t\tif (!sg_dma_len(sg))\n\t\t\tbreak;\n\t\tib_dma_sync_single_for_device(lnk->smcibdev->ibdev,\n\t\t\t\t\t      sg_dma_address(sg),\n\t\t\t\t\t      sg_dma_len(sg),\n\t\t\t\t\t      data_direction);\n\t}\n}\n\n \nint smc_ib_buf_map_sg(struct smc_link *lnk,\n\t\t      struct smc_buf_desc *buf_slot,\n\t\t      enum dma_data_direction data_direction)\n{\n\tint mapped_nents;\n\n\tmapped_nents = ib_dma_map_sg(lnk->smcibdev->ibdev,\n\t\t\t\t     buf_slot->sgt[lnk->link_idx].sgl,\n\t\t\t\t     buf_slot->sgt[lnk->link_idx].orig_nents,\n\t\t\t\t     data_direction);\n\tif (!mapped_nents)\n\t\treturn -ENOMEM;\n\n\treturn mapped_nents;\n}\n\nvoid smc_ib_buf_unmap_sg(struct smc_link *lnk,\n\t\t\t struct smc_buf_desc *buf_slot,\n\t\t\t enum dma_data_direction data_direction)\n{\n\tif (!buf_slot->sgt[lnk->link_idx].sgl->dma_address)\n\t\treturn;  \n\n\tib_dma_unmap_sg(lnk->smcibdev->ibdev,\n\t\t\tbuf_slot->sgt[lnk->link_idx].sgl,\n\t\t\tbuf_slot->sgt[lnk->link_idx].orig_nents,\n\t\t\tdata_direction);\n\tbuf_slot->sgt[lnk->link_idx].sgl->dma_address = 0;\n}\n\nlong smc_ib_setup_per_ibdev(struct smc_ib_device *smcibdev)\n{\n\tstruct ib_cq_init_attr cqattr =\t{\n\t\t.cqe = SMC_MAX_CQE, .comp_vector = 0 };\n\tint cqe_size_order, smc_order;\n\tlong rc;\n\n\tmutex_lock(&smcibdev->mutex);\n\trc = 0;\n\tif (smcibdev->initialized)\n\t\tgoto out;\n\t \n\tcqe_size_order = cache_line_size() == 128 ? 7 : 6;\n\tsmc_order = MAX_ORDER - cqe_size_order;\n\tif (SMC_MAX_CQE + 2 > (0x00000001 << smc_order) * PAGE_SIZE)\n\t\tcqattr.cqe = (0x00000001 << smc_order) * PAGE_SIZE - 2;\n\tsmcibdev->roce_cq_send = ib_create_cq(smcibdev->ibdev,\n\t\t\t\t\t      smc_wr_tx_cq_handler, NULL,\n\t\t\t\t\t      smcibdev, &cqattr);\n\trc = PTR_ERR_OR_ZERO(smcibdev->roce_cq_send);\n\tif (IS_ERR(smcibdev->roce_cq_send)) {\n\t\tsmcibdev->roce_cq_send = NULL;\n\t\tgoto out;\n\t}\n\tsmcibdev->roce_cq_recv = ib_create_cq(smcibdev->ibdev,\n\t\t\t\t\t      smc_wr_rx_cq_handler, NULL,\n\t\t\t\t\t      smcibdev, &cqattr);\n\trc = PTR_ERR_OR_ZERO(smcibdev->roce_cq_recv);\n\tif (IS_ERR(smcibdev->roce_cq_recv)) {\n\t\tsmcibdev->roce_cq_recv = NULL;\n\t\tgoto err;\n\t}\n\tsmc_wr_add_dev(smcibdev);\n\tsmcibdev->initialized = 1;\n\tgoto out;\n\nerr:\n\tib_destroy_cq(smcibdev->roce_cq_send);\nout:\n\tmutex_unlock(&smcibdev->mutex);\n\treturn rc;\n}\n\nstatic void smc_ib_cleanup_per_ibdev(struct smc_ib_device *smcibdev)\n{\n\tmutex_lock(&smcibdev->mutex);\n\tif (!smcibdev->initialized)\n\t\tgoto out;\n\tsmcibdev->initialized = 0;\n\tib_destroy_cq(smcibdev->roce_cq_recv);\n\tib_destroy_cq(smcibdev->roce_cq_send);\n\tsmc_wr_remove_dev(smcibdev);\nout:\n\tmutex_unlock(&smcibdev->mutex);\n}\n\nstatic struct ib_client smc_ib_client;\n\nstatic void smc_copy_netdev_ifindex(struct smc_ib_device *smcibdev, int port)\n{\n\tstruct ib_device *ibdev = smcibdev->ibdev;\n\tstruct net_device *ndev;\n\n\tif (!ibdev->ops.get_netdev)\n\t\treturn;\n\tndev = ibdev->ops.get_netdev(ibdev, port + 1);\n\tif (ndev) {\n\t\tsmcibdev->ndev_ifidx[port] = ndev->ifindex;\n\t\tdev_put(ndev);\n\t}\n}\n\nvoid smc_ib_ndev_change(struct net_device *ndev, unsigned long event)\n{\n\tstruct smc_ib_device *smcibdev;\n\tstruct ib_device *libdev;\n\tstruct net_device *lndev;\n\tu8 port_cnt;\n\tint i;\n\n\tmutex_lock(&smc_ib_devices.mutex);\n\tlist_for_each_entry(smcibdev, &smc_ib_devices.list, list) {\n\t\tport_cnt = smcibdev->ibdev->phys_port_cnt;\n\t\tfor (i = 0; i < min_t(size_t, port_cnt, SMC_MAX_PORTS); i++) {\n\t\t\tlibdev = smcibdev->ibdev;\n\t\t\tif (!libdev->ops.get_netdev)\n\t\t\t\tcontinue;\n\t\t\tlndev = libdev->ops.get_netdev(libdev, i + 1);\n\t\t\tdev_put(lndev);\n\t\t\tif (lndev != ndev)\n\t\t\t\tcontinue;\n\t\t\tif (event == NETDEV_REGISTER)\n\t\t\t\tsmcibdev->ndev_ifidx[i] = ndev->ifindex;\n\t\t\tif (event == NETDEV_UNREGISTER)\n\t\t\t\tsmcibdev->ndev_ifidx[i] = 0;\n\t\t}\n\t}\n\tmutex_unlock(&smc_ib_devices.mutex);\n}\n\n \nstatic int smc_ib_add_dev(struct ib_device *ibdev)\n{\n\tstruct smc_ib_device *smcibdev;\n\tu8 port_cnt;\n\tint i;\n\n\tif (ibdev->node_type != RDMA_NODE_IB_CA)\n\t\treturn -EOPNOTSUPP;\n\n\tsmcibdev = kzalloc(sizeof(*smcibdev), GFP_KERNEL);\n\tif (!smcibdev)\n\t\treturn -ENOMEM;\n\n\tsmcibdev->ibdev = ibdev;\n\tINIT_WORK(&smcibdev->port_event_work, smc_ib_port_event_work);\n\tatomic_set(&smcibdev->lnk_cnt, 0);\n\tinit_waitqueue_head(&smcibdev->lnks_deleted);\n\tmutex_init(&smcibdev->mutex);\n\tmutex_lock(&smc_ib_devices.mutex);\n\tlist_add_tail(&smcibdev->list, &smc_ib_devices.list);\n\tmutex_unlock(&smc_ib_devices.mutex);\n\tib_set_client_data(ibdev, &smc_ib_client, smcibdev);\n\tINIT_IB_EVENT_HANDLER(&smcibdev->event_handler, smcibdev->ibdev,\n\t\t\t      smc_ib_global_event_handler);\n\tib_register_event_handler(&smcibdev->event_handler);\n\n\t \n\tport_cnt = smcibdev->ibdev->phys_port_cnt;\n\tpr_warn_ratelimited(\"smc: adding ib device %s with port count %d\\n\",\n\t\t\t    smcibdev->ibdev->name, port_cnt);\n\tfor (i = 0;\n\t     i < min_t(size_t, port_cnt, SMC_MAX_PORTS);\n\t     i++) {\n\t\tset_bit(i, &smcibdev->port_event_mask);\n\t\t \n\t\tif (smc_pnetid_by_dev_port(ibdev->dev.parent, i,\n\t\t\t\t\t   smcibdev->pnetid[i]))\n\t\t\tsmc_pnetid_by_table_ib(smcibdev, i + 1);\n\t\tsmc_copy_netdev_ifindex(smcibdev, i);\n\t\tpr_warn_ratelimited(\"smc:    ib device %s port %d has pnetid \"\n\t\t\t\t    \"%.16s%s\\n\",\n\t\t\t\t    smcibdev->ibdev->name, i + 1,\n\t\t\t\t    smcibdev->pnetid[i],\n\t\t\t\t    smcibdev->pnetid_by_user[i] ?\n\t\t\t\t     \" (user defined)\" :\n\t\t\t\t     \"\");\n\t}\n\tschedule_work(&smcibdev->port_event_work);\n\treturn 0;\n}\n\n \nstatic void smc_ib_remove_dev(struct ib_device *ibdev, void *client_data)\n{\n\tstruct smc_ib_device *smcibdev = client_data;\n\n\tmutex_lock(&smc_ib_devices.mutex);\n\tlist_del_init(&smcibdev->list);  \n\tmutex_unlock(&smc_ib_devices.mutex);\n\tpr_warn_ratelimited(\"smc: removing ib device %s\\n\",\n\t\t\t    smcibdev->ibdev->name);\n\tsmc_smcr_terminate_all(smcibdev);\n\tsmc_ib_cleanup_per_ibdev(smcibdev);\n\tib_unregister_event_handler(&smcibdev->event_handler);\n\tcancel_work_sync(&smcibdev->port_event_work);\n\tkfree(smcibdev);\n}\n\nstatic struct ib_client smc_ib_client = {\n\t.name\t= \"smc_ib\",\n\t.add\t= smc_ib_add_dev,\n\t.remove = smc_ib_remove_dev,\n};\n\nint __init smc_ib_register_client(void)\n{\n\tsmc_ib_init_local_systemid();\n\treturn ib_register_client(&smc_ib_client);\n}\n\nvoid smc_ib_unregister_client(void)\n{\n\tib_unregister_client(&smc_ib_client);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}