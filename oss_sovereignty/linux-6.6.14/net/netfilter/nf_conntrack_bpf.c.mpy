{
  "module_name": "nf_conntrack_bpf.c",
  "hash_id": "38f22edfc5b67219f6c384263b2e988c9ca1b64cba4482b65004917d5b10cfdc",
  "original_prompt": "Ingested from linux-6.6.14/net/netfilter/nf_conntrack_bpf.c",
  "human_readable_source": "\n \n\n#include <linux/bpf_verifier.h>\n#include <linux/bpf.h>\n#include <linux/btf.h>\n#include <linux/filter.h>\n#include <linux/mutex.h>\n#include <linux/types.h>\n#include <linux/btf_ids.h>\n#include <linux/net_namespace.h>\n#include <net/xdp.h>\n#include <net/netfilter/nf_conntrack_bpf.h>\n#include <net/netfilter/nf_conntrack_core.h>\n\n \nstruct bpf_ct_opts {\n\ts32 netns_id;\n\ts32 error;\n\tu8 l4proto;\n\tu8 dir;\n\tu8 reserved[2];\n};\n\nenum {\n\tNF_BPF_CT_OPTS_SZ = 12,\n};\n\nstatic int bpf_nf_ct_tuple_parse(struct bpf_sock_tuple *bpf_tuple,\n\t\t\t\t u32 tuple_len, u8 protonum, u8 dir,\n\t\t\t\t struct nf_conntrack_tuple *tuple)\n{\n\tunion nf_inet_addr *src = dir ? &tuple->dst.u3 : &tuple->src.u3;\n\tunion nf_inet_addr *dst = dir ? &tuple->src.u3 : &tuple->dst.u3;\n\tunion nf_conntrack_man_proto *sport = dir ? (void *)&tuple->dst.u\n\t\t\t\t\t\t  : &tuple->src.u;\n\tunion nf_conntrack_man_proto *dport = dir ? &tuple->src.u\n\t\t\t\t\t\t  : (void *)&tuple->dst.u;\n\n\tif (unlikely(protonum != IPPROTO_TCP && protonum != IPPROTO_UDP))\n\t\treturn -EPROTO;\n\n\tmemset(tuple, 0, sizeof(*tuple));\n\n\tswitch (tuple_len) {\n\tcase sizeof(bpf_tuple->ipv4):\n\t\ttuple->src.l3num = AF_INET;\n\t\tsrc->ip = bpf_tuple->ipv4.saddr;\n\t\tsport->tcp.port = bpf_tuple->ipv4.sport;\n\t\tdst->ip = bpf_tuple->ipv4.daddr;\n\t\tdport->tcp.port = bpf_tuple->ipv4.dport;\n\t\tbreak;\n\tcase sizeof(bpf_tuple->ipv6):\n\t\ttuple->src.l3num = AF_INET6;\n\t\tmemcpy(src->ip6, bpf_tuple->ipv6.saddr, sizeof(bpf_tuple->ipv6.saddr));\n\t\tsport->tcp.port = bpf_tuple->ipv6.sport;\n\t\tmemcpy(dst->ip6, bpf_tuple->ipv6.daddr, sizeof(bpf_tuple->ipv6.daddr));\n\t\tdport->tcp.port = bpf_tuple->ipv6.dport;\n\t\tbreak;\n\tdefault:\n\t\treturn -EAFNOSUPPORT;\n\t}\n\ttuple->dst.protonum = protonum;\n\ttuple->dst.dir = dir;\n\n\treturn 0;\n}\n\nstatic struct nf_conn *\n__bpf_nf_ct_alloc_entry(struct net *net, struct bpf_sock_tuple *bpf_tuple,\n\t\t\tu32 tuple_len, struct bpf_ct_opts *opts, u32 opts_len,\n\t\t\tu32 timeout)\n{\n\tstruct nf_conntrack_tuple otuple, rtuple;\n\tstruct nf_conn *ct;\n\tint err;\n\n\tif (!opts || !bpf_tuple || opts->reserved[0] || opts->reserved[1] ||\n\t    opts_len != NF_BPF_CT_OPTS_SZ)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (unlikely(opts->netns_id < BPF_F_CURRENT_NETNS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\terr = bpf_nf_ct_tuple_parse(bpf_tuple, tuple_len, opts->l4proto,\n\t\t\t\t    IP_CT_DIR_ORIGINAL, &otuple);\n\tif (err < 0)\n\t\treturn ERR_PTR(err);\n\n\terr = bpf_nf_ct_tuple_parse(bpf_tuple, tuple_len, opts->l4proto,\n\t\t\t\t    IP_CT_DIR_REPLY, &rtuple);\n\tif (err < 0)\n\t\treturn ERR_PTR(err);\n\n\tif (opts->netns_id >= 0) {\n\t\tnet = get_net_ns_by_id(net, opts->netns_id);\n\t\tif (unlikely(!net))\n\t\t\treturn ERR_PTR(-ENONET);\n\t}\n\n\tct = nf_conntrack_alloc(net, &nf_ct_zone_dflt, &otuple, &rtuple,\n\t\t\t\tGFP_ATOMIC);\n\tif (IS_ERR(ct))\n\t\tgoto out;\n\n\tmemset(&ct->proto, 0, sizeof(ct->proto));\n\t__nf_ct_set_timeout(ct, timeout * HZ);\n\nout:\n\tif (opts->netns_id >= 0)\n\t\tput_net(net);\n\n\treturn ct;\n}\n\nstatic struct nf_conn *__bpf_nf_ct_lookup(struct net *net,\n\t\t\t\t\t  struct bpf_sock_tuple *bpf_tuple,\n\t\t\t\t\t  u32 tuple_len, struct bpf_ct_opts *opts,\n\t\t\t\t\t  u32 opts_len)\n{\n\tstruct nf_conntrack_tuple_hash *hash;\n\tstruct nf_conntrack_tuple tuple;\n\tstruct nf_conn *ct;\n\tint err;\n\n\tif (!opts || !bpf_tuple || opts->reserved[0] || opts->reserved[1] ||\n\t    opts_len != NF_BPF_CT_OPTS_SZ)\n\t\treturn ERR_PTR(-EINVAL);\n\tif (unlikely(opts->l4proto != IPPROTO_TCP && opts->l4proto != IPPROTO_UDP))\n\t\treturn ERR_PTR(-EPROTO);\n\tif (unlikely(opts->netns_id < BPF_F_CURRENT_NETNS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\terr = bpf_nf_ct_tuple_parse(bpf_tuple, tuple_len, opts->l4proto,\n\t\t\t\t    IP_CT_DIR_ORIGINAL, &tuple);\n\tif (err < 0)\n\t\treturn ERR_PTR(err);\n\n\tif (opts->netns_id >= 0) {\n\t\tnet = get_net_ns_by_id(net, opts->netns_id);\n\t\tif (unlikely(!net))\n\t\t\treturn ERR_PTR(-ENONET);\n\t}\n\n\thash = nf_conntrack_find_get(net, &nf_ct_zone_dflt, &tuple);\n\tif (opts->netns_id >= 0)\n\t\tput_net(net);\n\tif (!hash)\n\t\treturn ERR_PTR(-ENOENT);\n\n\tct = nf_ct_tuplehash_to_ctrack(hash);\n\topts->dir = NF_CT_DIRECTION(hash);\n\n\treturn ct;\n}\n\nBTF_ID_LIST(btf_nf_conn_ids)\nBTF_ID(struct, nf_conn)\nBTF_ID(struct, nf_conn___init)\n\n \nstatic int _nf_conntrack_btf_struct_access(struct bpf_verifier_log *log,\n\t\t\t\t\t   const struct bpf_reg_state *reg,\n\t\t\t\t\t   int off, int size)\n{\n\tconst struct btf_type *ncit, *nct, *t;\n\tsize_t end;\n\n\tncit = btf_type_by_id(reg->btf, btf_nf_conn_ids[1]);\n\tnct = btf_type_by_id(reg->btf, btf_nf_conn_ids[0]);\n\tt = btf_type_by_id(reg->btf, reg->btf_id);\n\tif (t != nct && t != ncit) {\n\t\tbpf_log(log, \"only read is supported\\n\");\n\t\treturn -EACCES;\n\t}\n\n\t \n\tswitch (off) {\n#if defined(CONFIG_NF_CONNTRACK_MARK)\n\tcase offsetof(struct nf_conn, mark):\n\t\tend = offsetofend(struct nf_conn, mark);\n\t\tbreak;\n#endif\n\tdefault:\n\t\tbpf_log(log, \"no write support to nf_conn at off %d\\n\", off);\n\t\treturn -EACCES;\n\t}\n\n\tif (off + size > end) {\n\t\tbpf_log(log,\n\t\t\t\"write access at off %d with size %d beyond the member of nf_conn ended at %zu\\n\",\n\t\t\toff, size, end);\n\t\treturn -EACCES;\n\t}\n\n\treturn 0;\n}\n\n__diag_push();\n__diag_ignore_all(\"-Wmissing-prototypes\",\n\t\t  \"Global functions as their definitions will be in nf_conntrack BTF\");\n\n \n__bpf_kfunc struct nf_conn___init *\nbpf_xdp_ct_alloc(struct xdp_md *xdp_ctx, struct bpf_sock_tuple *bpf_tuple,\n\t\t u32 tuple__sz, struct bpf_ct_opts *opts, u32 opts__sz)\n{\n\tstruct xdp_buff *ctx = (struct xdp_buff *)xdp_ctx;\n\tstruct nf_conn *nfct;\n\n\tnfct = __bpf_nf_ct_alloc_entry(dev_net(ctx->rxq->dev), bpf_tuple, tuple__sz,\n\t\t\t\t       opts, opts__sz, 10);\n\tif (IS_ERR(nfct)) {\n\t\tif (opts)\n\t\t\topts->error = PTR_ERR(nfct);\n\t\treturn NULL;\n\t}\n\n\treturn (struct nf_conn___init *)nfct;\n}\n\n \n__bpf_kfunc struct nf_conn *\nbpf_xdp_ct_lookup(struct xdp_md *xdp_ctx, struct bpf_sock_tuple *bpf_tuple,\n\t\t  u32 tuple__sz, struct bpf_ct_opts *opts, u32 opts__sz)\n{\n\tstruct xdp_buff *ctx = (struct xdp_buff *)xdp_ctx;\n\tstruct net *caller_net;\n\tstruct nf_conn *nfct;\n\n\tcaller_net = dev_net(ctx->rxq->dev);\n\tnfct = __bpf_nf_ct_lookup(caller_net, bpf_tuple, tuple__sz, opts, opts__sz);\n\tif (IS_ERR(nfct)) {\n\t\tif (opts)\n\t\t\topts->error = PTR_ERR(nfct);\n\t\treturn NULL;\n\t}\n\treturn nfct;\n}\n\n \n__bpf_kfunc struct nf_conn___init *\nbpf_skb_ct_alloc(struct __sk_buff *skb_ctx, struct bpf_sock_tuple *bpf_tuple,\n\t\t u32 tuple__sz, struct bpf_ct_opts *opts, u32 opts__sz)\n{\n\tstruct sk_buff *skb = (struct sk_buff *)skb_ctx;\n\tstruct nf_conn *nfct;\n\tstruct net *net;\n\n\tnet = skb->dev ? dev_net(skb->dev) : sock_net(skb->sk);\n\tnfct = __bpf_nf_ct_alloc_entry(net, bpf_tuple, tuple__sz, opts, opts__sz, 10);\n\tif (IS_ERR(nfct)) {\n\t\tif (opts)\n\t\t\topts->error = PTR_ERR(nfct);\n\t\treturn NULL;\n\t}\n\n\treturn (struct nf_conn___init *)nfct;\n}\n\n \n__bpf_kfunc struct nf_conn *\nbpf_skb_ct_lookup(struct __sk_buff *skb_ctx, struct bpf_sock_tuple *bpf_tuple,\n\t\t  u32 tuple__sz, struct bpf_ct_opts *opts, u32 opts__sz)\n{\n\tstruct sk_buff *skb = (struct sk_buff *)skb_ctx;\n\tstruct net *caller_net;\n\tstruct nf_conn *nfct;\n\n\tcaller_net = skb->dev ? dev_net(skb->dev) : sock_net(skb->sk);\n\tnfct = __bpf_nf_ct_lookup(caller_net, bpf_tuple, tuple__sz, opts, opts__sz);\n\tif (IS_ERR(nfct)) {\n\t\tif (opts)\n\t\t\topts->error = PTR_ERR(nfct);\n\t\treturn NULL;\n\t}\n\treturn nfct;\n}\n\n \n__bpf_kfunc struct nf_conn *bpf_ct_insert_entry(struct nf_conn___init *nfct_i)\n{\n\tstruct nf_conn *nfct = (struct nf_conn *)nfct_i;\n\tint err;\n\n\tif (!nf_ct_is_confirmed(nfct))\n\t\tnfct->timeout += nfct_time_stamp;\n\tnfct->status |= IPS_CONFIRMED;\n\terr = nf_conntrack_hash_check_insert(nfct);\n\tif (err < 0) {\n\t\tnf_conntrack_free(nfct);\n\t\treturn NULL;\n\t}\n\treturn nfct;\n}\n\n \n__bpf_kfunc void bpf_ct_release(struct nf_conn *nfct)\n{\n\tnf_ct_put(nfct);\n}\n\n \n__bpf_kfunc void bpf_ct_set_timeout(struct nf_conn___init *nfct, u32 timeout)\n{\n\t__nf_ct_set_timeout((struct nf_conn *)nfct, msecs_to_jiffies(timeout));\n}\n\n \n__bpf_kfunc int bpf_ct_change_timeout(struct nf_conn *nfct, u32 timeout)\n{\n\treturn __nf_ct_change_timeout(nfct, msecs_to_jiffies(timeout));\n}\n\n \n__bpf_kfunc int bpf_ct_set_status(const struct nf_conn___init *nfct, u32 status)\n{\n\treturn nf_ct_change_status_common((struct nf_conn *)nfct, status);\n}\n\n \n__bpf_kfunc int bpf_ct_change_status(struct nf_conn *nfct, u32 status)\n{\n\treturn nf_ct_change_status_common(nfct, status);\n}\n\n__diag_pop()\n\nBTF_SET8_START(nf_ct_kfunc_set)\nBTF_ID_FLAGS(func, bpf_xdp_ct_alloc, KF_ACQUIRE | KF_RET_NULL)\nBTF_ID_FLAGS(func, bpf_xdp_ct_lookup, KF_ACQUIRE | KF_RET_NULL)\nBTF_ID_FLAGS(func, bpf_skb_ct_alloc, KF_ACQUIRE | KF_RET_NULL)\nBTF_ID_FLAGS(func, bpf_skb_ct_lookup, KF_ACQUIRE | KF_RET_NULL)\nBTF_ID_FLAGS(func, bpf_ct_insert_entry, KF_ACQUIRE | KF_RET_NULL | KF_RELEASE)\nBTF_ID_FLAGS(func, bpf_ct_release, KF_RELEASE)\nBTF_ID_FLAGS(func, bpf_ct_set_timeout, KF_TRUSTED_ARGS)\nBTF_ID_FLAGS(func, bpf_ct_change_timeout, KF_TRUSTED_ARGS)\nBTF_ID_FLAGS(func, bpf_ct_set_status, KF_TRUSTED_ARGS)\nBTF_ID_FLAGS(func, bpf_ct_change_status, KF_TRUSTED_ARGS)\nBTF_SET8_END(nf_ct_kfunc_set)\n\nstatic const struct btf_kfunc_id_set nf_conntrack_kfunc_set = {\n\t.owner = THIS_MODULE,\n\t.set   = &nf_ct_kfunc_set,\n};\n\nint register_nf_conntrack_bpf(void)\n{\n\tint ret;\n\n\tret = register_btf_kfunc_id_set(BPF_PROG_TYPE_XDP, &nf_conntrack_kfunc_set);\n\tret = ret ?: register_btf_kfunc_id_set(BPF_PROG_TYPE_SCHED_CLS, &nf_conntrack_kfunc_set);\n\tif (!ret) {\n\t\tmutex_lock(&nf_conn_btf_access_lock);\n\t\tnfct_btf_struct_access = _nf_conntrack_btf_struct_access;\n\t\tmutex_unlock(&nf_conn_btf_access_lock);\n\t}\n\n\treturn ret;\n}\n\nvoid cleanup_nf_conntrack_bpf(void)\n{\n\tmutex_lock(&nf_conn_btf_access_lock);\n\tnfct_btf_struct_access = NULL;\n\tmutex_unlock(&nf_conn_btf_access_lock);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}