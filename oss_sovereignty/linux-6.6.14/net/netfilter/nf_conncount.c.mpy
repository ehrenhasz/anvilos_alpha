{
  "module_name": "nf_conncount.c",
  "hash_id": "a9db6284bc8d2e831d05de72f307e94cc557c45b146c45b41c2b6adf707bacf7",
  "original_prompt": "Ingested from linux-6.6.14/net/netfilter/nf_conncount.c",
  "human_readable_source": "\n \n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n#include <linux/in.h>\n#include <linux/in6.h>\n#include <linux/ip.h>\n#include <linux/ipv6.h>\n#include <linux/jhash.h>\n#include <linux/slab.h>\n#include <linux/list.h>\n#include <linux/rbtree.h>\n#include <linux/module.h>\n#include <linux/random.h>\n#include <linux/skbuff.h>\n#include <linux/spinlock.h>\n#include <linux/netfilter/nf_conntrack_tcp.h>\n#include <linux/netfilter/x_tables.h>\n#include <net/netfilter/nf_conntrack.h>\n#include <net/netfilter/nf_conntrack_count.h>\n#include <net/netfilter/nf_conntrack_core.h>\n#include <net/netfilter/nf_conntrack_tuple.h>\n#include <net/netfilter/nf_conntrack_zones.h>\n\n#define CONNCOUNT_SLOTS\t\t256U\n\n#define CONNCOUNT_GC_MAX_NODES\t8\n#define MAX_KEYLEN\t\t5\n\n \nstruct nf_conncount_tuple {\n\tstruct list_head\t\tnode;\n\tstruct nf_conntrack_tuple\ttuple;\n\tstruct nf_conntrack_zone\tzone;\n\tint\t\t\t\tcpu;\n\tu32\t\t\t\tjiffies32;\n};\n\nstruct nf_conncount_rb {\n\tstruct rb_node node;\n\tstruct nf_conncount_list list;\n\tu32 key[MAX_KEYLEN];\n\tstruct rcu_head rcu_head;\n};\n\nstatic spinlock_t nf_conncount_locks[CONNCOUNT_SLOTS] __cacheline_aligned_in_smp;\n\nstruct nf_conncount_data {\n\tunsigned int keylen;\n\tstruct rb_root root[CONNCOUNT_SLOTS];\n\tstruct net *net;\n\tstruct work_struct gc_work;\n\tunsigned long pending_trees[BITS_TO_LONGS(CONNCOUNT_SLOTS)];\n\tunsigned int gc_tree;\n};\n\nstatic u_int32_t conncount_rnd __read_mostly;\nstatic struct kmem_cache *conncount_rb_cachep __read_mostly;\nstatic struct kmem_cache *conncount_conn_cachep __read_mostly;\n\nstatic inline bool already_closed(const struct nf_conn *conn)\n{\n\tif (nf_ct_protonum(conn) == IPPROTO_TCP)\n\t\treturn conn->proto.tcp.state == TCP_CONNTRACK_TIME_WAIT ||\n\t\t       conn->proto.tcp.state == TCP_CONNTRACK_CLOSE;\n\telse\n\t\treturn false;\n}\n\nstatic int key_diff(const u32 *a, const u32 *b, unsigned int klen)\n{\n\treturn memcmp(a, b, klen * sizeof(u32));\n}\n\nstatic void conn_free(struct nf_conncount_list *list,\n\t\t      struct nf_conncount_tuple *conn)\n{\n\tlockdep_assert_held(&list->list_lock);\n\n\tlist->count--;\n\tlist_del(&conn->node);\n\n\tkmem_cache_free(conncount_conn_cachep, conn);\n}\n\nstatic const struct nf_conntrack_tuple_hash *\nfind_or_evict(struct net *net, struct nf_conncount_list *list,\n\t      struct nf_conncount_tuple *conn)\n{\n\tconst struct nf_conntrack_tuple_hash *found;\n\tunsigned long a, b;\n\tint cpu = raw_smp_processor_id();\n\tu32 age;\n\n\tfound = nf_conntrack_find_get(net, &conn->zone, &conn->tuple);\n\tif (found)\n\t\treturn found;\n\tb = conn->jiffies32;\n\ta = (u32)jiffies;\n\n\t \n\tage = a - b;\n\tif (conn->cpu == cpu || age >= 2) {\n\t\tconn_free(list, conn);\n\t\treturn ERR_PTR(-ENOENT);\n\t}\n\n\treturn ERR_PTR(-EAGAIN);\n}\n\nstatic int __nf_conncount_add(struct net *net,\n\t\t\t      struct nf_conncount_list *list,\n\t\t\t      const struct nf_conntrack_tuple *tuple,\n\t\t\t      const struct nf_conntrack_zone *zone)\n{\n\tconst struct nf_conntrack_tuple_hash *found;\n\tstruct nf_conncount_tuple *conn, *conn_n;\n\tstruct nf_conn *found_ct;\n\tunsigned int collect = 0;\n\n\tif (time_is_after_eq_jiffies((unsigned long)list->last_gc))\n\t\tgoto add_new_node;\n\n\t \n\tlist_for_each_entry_safe(conn, conn_n, &list->head, node) {\n\t\tif (collect > CONNCOUNT_GC_MAX_NODES)\n\t\t\tbreak;\n\n\t\tfound = find_or_evict(net, list, conn);\n\t\tif (IS_ERR(found)) {\n\t\t\t \n\t\t\tif (PTR_ERR(found) == -EAGAIN) {\n\t\t\t\tif (nf_ct_tuple_equal(&conn->tuple, tuple) &&\n\t\t\t\t    nf_ct_zone_id(&conn->zone, conn->zone.dir) ==\n\t\t\t\t    nf_ct_zone_id(zone, zone->dir))\n\t\t\t\t\treturn 0;  \n\t\t\t} else {\n\t\t\t\tcollect++;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\tfound_ct = nf_ct_tuplehash_to_ctrack(found);\n\n\t\tif (nf_ct_tuple_equal(&conn->tuple, tuple) &&\n\t\t    nf_ct_zone_equal(found_ct, zone, zone->dir)) {\n\t\t\t \n\t\t\tnf_ct_put(found_ct);\n\t\t\treturn 0;\n\t\t} else if (already_closed(found_ct)) {\n\t\t\t \n\t\t\tnf_ct_put(found_ct);\n\t\t\tconn_free(list, conn);\n\t\t\tcollect++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tnf_ct_put(found_ct);\n\t}\n\nadd_new_node:\n\tif (WARN_ON_ONCE(list->count > INT_MAX))\n\t\treturn -EOVERFLOW;\n\n\tconn = kmem_cache_alloc(conncount_conn_cachep, GFP_ATOMIC);\n\tif (conn == NULL)\n\t\treturn -ENOMEM;\n\n\tconn->tuple = *tuple;\n\tconn->zone = *zone;\n\tconn->cpu = raw_smp_processor_id();\n\tconn->jiffies32 = (u32)jiffies;\n\tlist_add_tail(&conn->node, &list->head);\n\tlist->count++;\n\tlist->last_gc = (u32)jiffies;\n\treturn 0;\n}\n\nint nf_conncount_add(struct net *net,\n\t\t     struct nf_conncount_list *list,\n\t\t     const struct nf_conntrack_tuple *tuple,\n\t\t     const struct nf_conntrack_zone *zone)\n{\n\tint ret;\n\n\t \n\tspin_lock_bh(&list->list_lock);\n\tret = __nf_conncount_add(net, list, tuple, zone);\n\tspin_unlock_bh(&list->list_lock);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(nf_conncount_add);\n\nvoid nf_conncount_list_init(struct nf_conncount_list *list)\n{\n\tspin_lock_init(&list->list_lock);\n\tINIT_LIST_HEAD(&list->head);\n\tlist->count = 0;\n\tlist->last_gc = (u32)jiffies;\n}\nEXPORT_SYMBOL_GPL(nf_conncount_list_init);\n\n \nbool nf_conncount_gc_list(struct net *net,\n\t\t\t  struct nf_conncount_list *list)\n{\n\tconst struct nf_conntrack_tuple_hash *found;\n\tstruct nf_conncount_tuple *conn, *conn_n;\n\tstruct nf_conn *found_ct;\n\tunsigned int collected = 0;\n\tbool ret = false;\n\n\t \n\tif (time_is_after_eq_jiffies((unsigned long)READ_ONCE(list->last_gc)))\n\t\treturn false;\n\n\t \n\tif (!spin_trylock(&list->list_lock))\n\t\treturn false;\n\n\tlist_for_each_entry_safe(conn, conn_n, &list->head, node) {\n\t\tfound = find_or_evict(net, list, conn);\n\t\tif (IS_ERR(found)) {\n\t\t\tif (PTR_ERR(found) == -ENOENT)\n\t\t\t\tcollected++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tfound_ct = nf_ct_tuplehash_to_ctrack(found);\n\t\tif (already_closed(found_ct)) {\n\t\t\t \n\t\t\tnf_ct_put(found_ct);\n\t\t\tconn_free(list, conn);\n\t\t\tcollected++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tnf_ct_put(found_ct);\n\t\tif (collected > CONNCOUNT_GC_MAX_NODES)\n\t\t\tbreak;\n\t}\n\n\tif (!list->count)\n\t\tret = true;\n\tlist->last_gc = (u32)jiffies;\n\tspin_unlock(&list->list_lock);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(nf_conncount_gc_list);\n\nstatic void __tree_nodes_free(struct rcu_head *h)\n{\n\tstruct nf_conncount_rb *rbconn;\n\n\trbconn = container_of(h, struct nf_conncount_rb, rcu_head);\n\tkmem_cache_free(conncount_rb_cachep, rbconn);\n}\n\n \nstatic void tree_nodes_free(struct rb_root *root,\n\t\t\t    struct nf_conncount_rb *gc_nodes[],\n\t\t\t    unsigned int gc_count)\n{\n\tstruct nf_conncount_rb *rbconn;\n\n\twhile (gc_count) {\n\t\trbconn = gc_nodes[--gc_count];\n\t\tspin_lock(&rbconn->list.list_lock);\n\t\tif (!rbconn->list.count) {\n\t\t\trb_erase(&rbconn->node, root);\n\t\t\tcall_rcu(&rbconn->rcu_head, __tree_nodes_free);\n\t\t}\n\t\tspin_unlock(&rbconn->list.list_lock);\n\t}\n}\n\nstatic void schedule_gc_worker(struct nf_conncount_data *data, int tree)\n{\n\tset_bit(tree, data->pending_trees);\n\tschedule_work(&data->gc_work);\n}\n\nstatic unsigned int\ninsert_tree(struct net *net,\n\t    struct nf_conncount_data *data,\n\t    struct rb_root *root,\n\t    unsigned int hash,\n\t    const u32 *key,\n\t    const struct nf_conntrack_tuple *tuple,\n\t    const struct nf_conntrack_zone *zone)\n{\n\tstruct nf_conncount_rb *gc_nodes[CONNCOUNT_GC_MAX_NODES];\n\tstruct rb_node **rbnode, *parent;\n\tstruct nf_conncount_rb *rbconn;\n\tstruct nf_conncount_tuple *conn;\n\tunsigned int count = 0, gc_count = 0;\n\tu8 keylen = data->keylen;\n\tbool do_gc = true;\n\n\tspin_lock_bh(&nf_conncount_locks[hash]);\nrestart:\n\tparent = NULL;\n\trbnode = &(root->rb_node);\n\twhile (*rbnode) {\n\t\tint diff;\n\t\trbconn = rb_entry(*rbnode, struct nf_conncount_rb, node);\n\n\t\tparent = *rbnode;\n\t\tdiff = key_diff(key, rbconn->key, keylen);\n\t\tif (diff < 0) {\n\t\t\trbnode = &((*rbnode)->rb_left);\n\t\t} else if (diff > 0) {\n\t\t\trbnode = &((*rbnode)->rb_right);\n\t\t} else {\n\t\t\tint ret;\n\n\t\t\tret = nf_conncount_add(net, &rbconn->list, tuple, zone);\n\t\t\tif (ret)\n\t\t\t\tcount = 0;  \n\t\t\telse\n\t\t\t\tcount = rbconn->list.count;\n\t\t\ttree_nodes_free(root, gc_nodes, gc_count);\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tif (gc_count >= ARRAY_SIZE(gc_nodes))\n\t\t\tcontinue;\n\n\t\tif (do_gc && nf_conncount_gc_list(net, &rbconn->list))\n\t\t\tgc_nodes[gc_count++] = rbconn;\n\t}\n\n\tif (gc_count) {\n\t\ttree_nodes_free(root, gc_nodes, gc_count);\n\t\tschedule_gc_worker(data, hash);\n\t\tgc_count = 0;\n\t\tdo_gc = false;\n\t\tgoto restart;\n\t}\n\n\t \n\trbconn = kmem_cache_alloc(conncount_rb_cachep, GFP_ATOMIC);\n\tif (rbconn == NULL)\n\t\tgoto out_unlock;\n\n\tconn = kmem_cache_alloc(conncount_conn_cachep, GFP_ATOMIC);\n\tif (conn == NULL) {\n\t\tkmem_cache_free(conncount_rb_cachep, rbconn);\n\t\tgoto out_unlock;\n\t}\n\n\tconn->tuple = *tuple;\n\tconn->zone = *zone;\n\tmemcpy(rbconn->key, key, sizeof(u32) * keylen);\n\n\tnf_conncount_list_init(&rbconn->list);\n\tlist_add(&conn->node, &rbconn->list.head);\n\tcount = 1;\n\trbconn->list.count = count;\n\n\trb_link_node_rcu(&rbconn->node, parent, rbnode);\n\trb_insert_color(&rbconn->node, root);\nout_unlock:\n\tspin_unlock_bh(&nf_conncount_locks[hash]);\n\treturn count;\n}\n\nstatic unsigned int\ncount_tree(struct net *net,\n\t   struct nf_conncount_data *data,\n\t   const u32 *key,\n\t   const struct nf_conntrack_tuple *tuple,\n\t   const struct nf_conntrack_zone *zone)\n{\n\tstruct rb_root *root;\n\tstruct rb_node *parent;\n\tstruct nf_conncount_rb *rbconn;\n\tunsigned int hash;\n\tu8 keylen = data->keylen;\n\n\thash = jhash2(key, data->keylen, conncount_rnd) % CONNCOUNT_SLOTS;\n\troot = &data->root[hash];\n\n\tparent = rcu_dereference_raw(root->rb_node);\n\twhile (parent) {\n\t\tint diff;\n\n\t\trbconn = rb_entry(parent, struct nf_conncount_rb, node);\n\n\t\tdiff = key_diff(key, rbconn->key, keylen);\n\t\tif (diff < 0) {\n\t\t\tparent = rcu_dereference_raw(parent->rb_left);\n\t\t} else if (diff > 0) {\n\t\t\tparent = rcu_dereference_raw(parent->rb_right);\n\t\t} else {\n\t\t\tint ret;\n\n\t\t\tif (!tuple) {\n\t\t\t\tnf_conncount_gc_list(net, &rbconn->list);\n\t\t\t\treturn rbconn->list.count;\n\t\t\t}\n\n\t\t\tspin_lock_bh(&rbconn->list.list_lock);\n\t\t\t \n\t\t\tif (rbconn->list.count == 0) {\n\t\t\t\tspin_unlock_bh(&rbconn->list.list_lock);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t \n\t\t\tret = __nf_conncount_add(net, &rbconn->list, tuple, zone);\n\t\t\tspin_unlock_bh(&rbconn->list.list_lock);\n\t\t\tif (ret)\n\t\t\t\treturn 0;  \n\t\t\telse\n\t\t\t\treturn rbconn->list.count;\n\t\t}\n\t}\n\n\tif (!tuple)\n\t\treturn 0;\n\n\treturn insert_tree(net, data, root, hash, key, tuple, zone);\n}\n\nstatic void tree_gc_worker(struct work_struct *work)\n{\n\tstruct nf_conncount_data *data = container_of(work, struct nf_conncount_data, gc_work);\n\tstruct nf_conncount_rb *gc_nodes[CONNCOUNT_GC_MAX_NODES], *rbconn;\n\tstruct rb_root *root;\n\tstruct rb_node *node;\n\tunsigned int tree, next_tree, gc_count = 0;\n\n\ttree = data->gc_tree % CONNCOUNT_SLOTS;\n\troot = &data->root[tree];\n\n\tlocal_bh_disable();\n\trcu_read_lock();\n\tfor (node = rb_first(root); node != NULL; node = rb_next(node)) {\n\t\trbconn = rb_entry(node, struct nf_conncount_rb, node);\n\t\tif (nf_conncount_gc_list(data->net, &rbconn->list))\n\t\t\tgc_count++;\n\t}\n\trcu_read_unlock();\n\tlocal_bh_enable();\n\n\tcond_resched();\n\n\tspin_lock_bh(&nf_conncount_locks[tree]);\n\tif (gc_count < ARRAY_SIZE(gc_nodes))\n\t\tgoto next;  \n\n\tgc_count = 0;\n\tnode = rb_first(root);\n\twhile (node != NULL) {\n\t\trbconn = rb_entry(node, struct nf_conncount_rb, node);\n\t\tnode = rb_next(node);\n\n\t\tif (rbconn->list.count > 0)\n\t\t\tcontinue;\n\n\t\tgc_nodes[gc_count++] = rbconn;\n\t\tif (gc_count >= ARRAY_SIZE(gc_nodes)) {\n\t\t\ttree_nodes_free(root, gc_nodes, gc_count);\n\t\t\tgc_count = 0;\n\t\t}\n\t}\n\n\ttree_nodes_free(root, gc_nodes, gc_count);\nnext:\n\tclear_bit(tree, data->pending_trees);\n\n\tnext_tree = (tree + 1) % CONNCOUNT_SLOTS;\n\tnext_tree = find_next_bit(data->pending_trees, CONNCOUNT_SLOTS, next_tree);\n\n\tif (next_tree < CONNCOUNT_SLOTS) {\n\t\tdata->gc_tree = next_tree;\n\t\tschedule_work(work);\n\t}\n\n\tspin_unlock_bh(&nf_conncount_locks[tree]);\n}\n\n \nunsigned int nf_conncount_count(struct net *net,\n\t\t\t\tstruct nf_conncount_data *data,\n\t\t\t\tconst u32 *key,\n\t\t\t\tconst struct nf_conntrack_tuple *tuple,\n\t\t\t\tconst struct nf_conntrack_zone *zone)\n{\n\treturn count_tree(net, data, key, tuple, zone);\n}\nEXPORT_SYMBOL_GPL(nf_conncount_count);\n\nstruct nf_conncount_data *nf_conncount_init(struct net *net, unsigned int family,\n\t\t\t\t\t    unsigned int keylen)\n{\n\tstruct nf_conncount_data *data;\n\tint ret, i;\n\n\tif (keylen % sizeof(u32) ||\n\t    keylen / sizeof(u32) > MAX_KEYLEN ||\n\t    keylen == 0)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tnet_get_random_once(&conncount_rnd, sizeof(conncount_rnd));\n\n\tdata = kmalloc(sizeof(*data), GFP_KERNEL);\n\tif (!data)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tret = nf_ct_netns_get(net, family);\n\tif (ret < 0) {\n\t\tkfree(data);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\tfor (i = 0; i < ARRAY_SIZE(data->root); ++i)\n\t\tdata->root[i] = RB_ROOT;\n\n\tdata->keylen = keylen / sizeof(u32);\n\tdata->net = net;\n\tINIT_WORK(&data->gc_work, tree_gc_worker);\n\n\treturn data;\n}\nEXPORT_SYMBOL_GPL(nf_conncount_init);\n\nvoid nf_conncount_cache_free(struct nf_conncount_list *list)\n{\n\tstruct nf_conncount_tuple *conn, *conn_n;\n\n\tlist_for_each_entry_safe(conn, conn_n, &list->head, node)\n\t\tkmem_cache_free(conncount_conn_cachep, conn);\n}\nEXPORT_SYMBOL_GPL(nf_conncount_cache_free);\n\nstatic void destroy_tree(struct rb_root *r)\n{\n\tstruct nf_conncount_rb *rbconn;\n\tstruct rb_node *node;\n\n\twhile ((node = rb_first(r)) != NULL) {\n\t\trbconn = rb_entry(node, struct nf_conncount_rb, node);\n\n\t\trb_erase(node, r);\n\n\t\tnf_conncount_cache_free(&rbconn->list);\n\n\t\tkmem_cache_free(conncount_rb_cachep, rbconn);\n\t}\n}\n\nvoid nf_conncount_destroy(struct net *net, unsigned int family,\n\t\t\t  struct nf_conncount_data *data)\n{\n\tunsigned int i;\n\n\tcancel_work_sync(&data->gc_work);\n\tnf_ct_netns_put(net, family);\n\n\tfor (i = 0; i < ARRAY_SIZE(data->root); ++i)\n\t\tdestroy_tree(&data->root[i]);\n\n\tkfree(data);\n}\nEXPORT_SYMBOL_GPL(nf_conncount_destroy);\n\nstatic int __init nf_conncount_modinit(void)\n{\n\tint i;\n\n\tfor (i = 0; i < CONNCOUNT_SLOTS; ++i)\n\t\tspin_lock_init(&nf_conncount_locks[i]);\n\n\tconncount_conn_cachep = kmem_cache_create(\"nf_conncount_tuple\",\n\t\t\t\t\t   sizeof(struct nf_conncount_tuple),\n\t\t\t\t\t   0, 0, NULL);\n\tif (!conncount_conn_cachep)\n\t\treturn -ENOMEM;\n\n\tconncount_rb_cachep = kmem_cache_create(\"nf_conncount_rb\",\n\t\t\t\t\t   sizeof(struct nf_conncount_rb),\n\t\t\t\t\t   0, 0, NULL);\n\tif (!conncount_rb_cachep) {\n\t\tkmem_cache_destroy(conncount_conn_cachep);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic void __exit nf_conncount_modexit(void)\n{\n\tkmem_cache_destroy(conncount_conn_cachep);\n\tkmem_cache_destroy(conncount_rb_cachep);\n}\n\nmodule_init(nf_conncount_modinit);\nmodule_exit(nf_conncount_modexit);\nMODULE_AUTHOR(\"Jan Engelhardt <jengelh@medozas.de>\");\nMODULE_AUTHOR(\"Florian Westphal <fw@strlen.de>\");\nMODULE_DESCRIPTION(\"netfilter: count number of connections matching a key\");\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}