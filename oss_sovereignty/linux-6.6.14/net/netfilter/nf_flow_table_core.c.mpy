{
  "module_name": "nf_flow_table_core.c",
  "hash_id": "fa63102817b1c8898fa326aaaf5ddfec2257d5ad04668dcc89038afada42a087",
  "original_prompt": "Ingested from linux-6.6.14/net/netfilter/nf_flow_table_core.c",
  "human_readable_source": "\n#include <linux/kernel.h>\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/netfilter.h>\n#include <linux/rhashtable.h>\n#include <linux/netdevice.h>\n#include <net/ip.h>\n#include <net/ip6_route.h>\n#include <net/netfilter/nf_tables.h>\n#include <net/netfilter/nf_flow_table.h>\n#include <net/netfilter/nf_conntrack.h>\n#include <net/netfilter/nf_conntrack_core.h>\n#include <net/netfilter/nf_conntrack_l4proto.h>\n#include <net/netfilter/nf_conntrack_tuple.h>\n\nstatic DEFINE_MUTEX(flowtable_lock);\nstatic LIST_HEAD(flowtables);\n\nstatic void\nflow_offload_fill_dir(struct flow_offload *flow,\n\t\t      enum flow_offload_tuple_dir dir)\n{\n\tstruct flow_offload_tuple *ft = &flow->tuplehash[dir].tuple;\n\tstruct nf_conntrack_tuple *ctt = &flow->ct->tuplehash[dir].tuple;\n\n\tft->dir = dir;\n\n\tswitch (ctt->src.l3num) {\n\tcase NFPROTO_IPV4:\n\t\tft->src_v4 = ctt->src.u3.in;\n\t\tft->dst_v4 = ctt->dst.u3.in;\n\t\tbreak;\n\tcase NFPROTO_IPV6:\n\t\tft->src_v6 = ctt->src.u3.in6;\n\t\tft->dst_v6 = ctt->dst.u3.in6;\n\t\tbreak;\n\t}\n\n\tft->l3proto = ctt->src.l3num;\n\tft->l4proto = ctt->dst.protonum;\n\n\tswitch (ctt->dst.protonum) {\n\tcase IPPROTO_TCP:\n\tcase IPPROTO_UDP:\n\t\tft->src_port = ctt->src.u.tcp.port;\n\t\tft->dst_port = ctt->dst.u.tcp.port;\n\t\tbreak;\n\t}\n}\n\nstruct flow_offload *flow_offload_alloc(struct nf_conn *ct)\n{\n\tstruct flow_offload *flow;\n\n\tif (unlikely(nf_ct_is_dying(ct)))\n\t\treturn NULL;\n\n\tflow = kzalloc(sizeof(*flow), GFP_ATOMIC);\n\tif (!flow)\n\t\treturn NULL;\n\n\trefcount_inc(&ct->ct_general.use);\n\tflow->ct = ct;\n\n\tflow_offload_fill_dir(flow, FLOW_OFFLOAD_DIR_ORIGINAL);\n\tflow_offload_fill_dir(flow, FLOW_OFFLOAD_DIR_REPLY);\n\n\tif (ct->status & IPS_SRC_NAT)\n\t\t__set_bit(NF_FLOW_SNAT, &flow->flags);\n\tif (ct->status & IPS_DST_NAT)\n\t\t__set_bit(NF_FLOW_DNAT, &flow->flags);\n\n\treturn flow;\n}\nEXPORT_SYMBOL_GPL(flow_offload_alloc);\n\nstatic u32 flow_offload_dst_cookie(struct flow_offload_tuple *flow_tuple)\n{\n\tconst struct rt6_info *rt;\n\n\tif (flow_tuple->l3proto == NFPROTO_IPV6) {\n\t\trt = (const struct rt6_info *)flow_tuple->dst_cache;\n\t\treturn rt6_get_cookie(rt);\n\t}\n\n\treturn 0;\n}\n\nstatic int flow_offload_fill_route(struct flow_offload *flow,\n\t\t\t\t   const struct nf_flow_route *route,\n\t\t\t\t   enum flow_offload_tuple_dir dir)\n{\n\tstruct flow_offload_tuple *flow_tuple = &flow->tuplehash[dir].tuple;\n\tstruct dst_entry *dst = route->tuple[dir].dst;\n\tint i, j = 0;\n\n\tswitch (flow_tuple->l3proto) {\n\tcase NFPROTO_IPV4:\n\t\tflow_tuple->mtu = ip_dst_mtu_maybe_forward(dst, true);\n\t\tbreak;\n\tcase NFPROTO_IPV6:\n\t\tflow_tuple->mtu = ip6_dst_mtu_maybe_forward(dst, true);\n\t\tbreak;\n\t}\n\n\tflow_tuple->iifidx = route->tuple[dir].in.ifindex;\n\tfor (i = route->tuple[dir].in.num_encaps - 1; i >= 0; i--) {\n\t\tflow_tuple->encap[j].id = route->tuple[dir].in.encap[i].id;\n\t\tflow_tuple->encap[j].proto = route->tuple[dir].in.encap[i].proto;\n\t\tif (route->tuple[dir].in.ingress_vlans & BIT(i))\n\t\t\tflow_tuple->in_vlan_ingress |= BIT(j);\n\t\tj++;\n\t}\n\tflow_tuple->encap_num = route->tuple[dir].in.num_encaps;\n\n\tswitch (route->tuple[dir].xmit_type) {\n\tcase FLOW_OFFLOAD_XMIT_DIRECT:\n\t\tmemcpy(flow_tuple->out.h_dest, route->tuple[dir].out.h_dest,\n\t\t       ETH_ALEN);\n\t\tmemcpy(flow_tuple->out.h_source, route->tuple[dir].out.h_source,\n\t\t       ETH_ALEN);\n\t\tflow_tuple->out.ifidx = route->tuple[dir].out.ifindex;\n\t\tflow_tuple->out.hw_ifidx = route->tuple[dir].out.hw_ifindex;\n\t\tbreak;\n\tcase FLOW_OFFLOAD_XMIT_XFRM:\n\tcase FLOW_OFFLOAD_XMIT_NEIGH:\n\t\tflow_tuple->dst_cache = dst;\n\t\tflow_tuple->dst_cookie = flow_offload_dst_cookie(flow_tuple);\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\tbreak;\n\t}\n\tflow_tuple->xmit_type = route->tuple[dir].xmit_type;\n\n\treturn 0;\n}\n\nstatic void nft_flow_dst_release(struct flow_offload *flow,\n\t\t\t\t enum flow_offload_tuple_dir dir)\n{\n\tif (flow->tuplehash[dir].tuple.xmit_type == FLOW_OFFLOAD_XMIT_NEIGH ||\n\t    flow->tuplehash[dir].tuple.xmit_type == FLOW_OFFLOAD_XMIT_XFRM)\n\t\tdst_release(flow->tuplehash[dir].tuple.dst_cache);\n}\n\nvoid flow_offload_route_init(struct flow_offload *flow,\n\t\t\t    const struct nf_flow_route *route)\n{\n\tflow_offload_fill_route(flow, route, FLOW_OFFLOAD_DIR_ORIGINAL);\n\tflow_offload_fill_route(flow, route, FLOW_OFFLOAD_DIR_REPLY);\n\tflow->type = NF_FLOW_OFFLOAD_ROUTE;\n}\nEXPORT_SYMBOL_GPL(flow_offload_route_init);\n\nstatic void flow_offload_fixup_tcp(struct ip_ct_tcp *tcp)\n{\n\ttcp->seen[0].td_maxwin = 0;\n\ttcp->seen[1].td_maxwin = 0;\n}\n\nstatic void flow_offload_fixup_ct(struct nf_conn *ct)\n{\n\tstruct net *net = nf_ct_net(ct);\n\tint l4num = nf_ct_protonum(ct);\n\ts32 timeout;\n\n\tif (l4num == IPPROTO_TCP) {\n\t\tstruct nf_tcp_net *tn = nf_tcp_pernet(net);\n\n\t\tflow_offload_fixup_tcp(&ct->proto.tcp);\n\n\t\ttimeout = tn->timeouts[ct->proto.tcp.state];\n\t\ttimeout -= tn->offload_timeout;\n\t} else if (l4num == IPPROTO_UDP) {\n\t\tstruct nf_udp_net *tn = nf_udp_pernet(net);\n\t\tenum udp_conntrack state =\n\t\t\ttest_bit(IPS_SEEN_REPLY_BIT, &ct->status) ?\n\t\t\tUDP_CT_REPLIED : UDP_CT_UNREPLIED;\n\n\t\ttimeout = tn->timeouts[state];\n\t\ttimeout -= tn->offload_timeout;\n\t} else {\n\t\treturn;\n\t}\n\n\tif (timeout < 0)\n\t\ttimeout = 0;\n\n\tif (nf_flow_timeout_delta(READ_ONCE(ct->timeout)) > (__s32)timeout)\n\t\tWRITE_ONCE(ct->timeout, nfct_time_stamp + timeout);\n}\n\nstatic void flow_offload_route_release(struct flow_offload *flow)\n{\n\tnft_flow_dst_release(flow, FLOW_OFFLOAD_DIR_ORIGINAL);\n\tnft_flow_dst_release(flow, FLOW_OFFLOAD_DIR_REPLY);\n}\n\nvoid flow_offload_free(struct flow_offload *flow)\n{\n\tswitch (flow->type) {\n\tcase NF_FLOW_OFFLOAD_ROUTE:\n\t\tflow_offload_route_release(flow);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\tnf_ct_put(flow->ct);\n\tkfree_rcu(flow, rcu_head);\n}\nEXPORT_SYMBOL_GPL(flow_offload_free);\n\nstatic u32 flow_offload_hash(const void *data, u32 len, u32 seed)\n{\n\tconst struct flow_offload_tuple *tuple = data;\n\n\treturn jhash(tuple, offsetof(struct flow_offload_tuple, __hash), seed);\n}\n\nstatic u32 flow_offload_hash_obj(const void *data, u32 len, u32 seed)\n{\n\tconst struct flow_offload_tuple_rhash *tuplehash = data;\n\n\treturn jhash(&tuplehash->tuple, offsetof(struct flow_offload_tuple, __hash), seed);\n}\n\nstatic int flow_offload_hash_cmp(struct rhashtable_compare_arg *arg,\n\t\t\t\t\tconst void *ptr)\n{\n\tconst struct flow_offload_tuple *tuple = arg->key;\n\tconst struct flow_offload_tuple_rhash *x = ptr;\n\n\tif (memcmp(&x->tuple, tuple, offsetof(struct flow_offload_tuple, __hash)))\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic const struct rhashtable_params nf_flow_offload_rhash_params = {\n\t.head_offset\t\t= offsetof(struct flow_offload_tuple_rhash, node),\n\t.hashfn\t\t\t= flow_offload_hash,\n\t.obj_hashfn\t\t= flow_offload_hash_obj,\n\t.obj_cmpfn\t\t= flow_offload_hash_cmp,\n\t.automatic_shrinking\t= true,\n};\n\nunsigned long flow_offload_get_timeout(struct flow_offload *flow)\n{\n\tunsigned long timeout = NF_FLOW_TIMEOUT;\n\tstruct net *net = nf_ct_net(flow->ct);\n\tint l4num = nf_ct_protonum(flow->ct);\n\n\tif (l4num == IPPROTO_TCP) {\n\t\tstruct nf_tcp_net *tn = nf_tcp_pernet(net);\n\n\t\ttimeout = tn->offload_timeout;\n\t} else if (l4num == IPPROTO_UDP) {\n\t\tstruct nf_udp_net *tn = nf_udp_pernet(net);\n\n\t\ttimeout = tn->offload_timeout;\n\t}\n\n\treturn timeout;\n}\n\nint flow_offload_add(struct nf_flowtable *flow_table, struct flow_offload *flow)\n{\n\tint err;\n\n\tflow->timeout = nf_flowtable_time_stamp + flow_offload_get_timeout(flow);\n\n\terr = rhashtable_insert_fast(&flow_table->rhashtable,\n\t\t\t\t     &flow->tuplehash[0].node,\n\t\t\t\t     nf_flow_offload_rhash_params);\n\tif (err < 0)\n\t\treturn err;\n\n\terr = rhashtable_insert_fast(&flow_table->rhashtable,\n\t\t\t\t     &flow->tuplehash[1].node,\n\t\t\t\t     nf_flow_offload_rhash_params);\n\tif (err < 0) {\n\t\trhashtable_remove_fast(&flow_table->rhashtable,\n\t\t\t\t       &flow->tuplehash[0].node,\n\t\t\t\t       nf_flow_offload_rhash_params);\n\t\treturn err;\n\t}\n\n\tnf_ct_offload_timeout(flow->ct);\n\n\tif (nf_flowtable_hw_offload(flow_table)) {\n\t\t__set_bit(NF_FLOW_HW, &flow->flags);\n\t\tnf_flow_offload_add(flow_table, flow);\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(flow_offload_add);\n\nvoid flow_offload_refresh(struct nf_flowtable *flow_table,\n\t\t\t  struct flow_offload *flow, bool force)\n{\n\tu32 timeout;\n\n\ttimeout = nf_flowtable_time_stamp + flow_offload_get_timeout(flow);\n\tif (force || timeout - READ_ONCE(flow->timeout) > HZ)\n\t\tWRITE_ONCE(flow->timeout, timeout);\n\telse\n\t\treturn;\n\n\tif (likely(!nf_flowtable_hw_offload(flow_table)))\n\t\treturn;\n\n\tnf_flow_offload_add(flow_table, flow);\n}\nEXPORT_SYMBOL_GPL(flow_offload_refresh);\n\nstatic inline bool nf_flow_has_expired(const struct flow_offload *flow)\n{\n\treturn nf_flow_timeout_delta(flow->timeout) <= 0;\n}\n\nstatic void flow_offload_del(struct nf_flowtable *flow_table,\n\t\t\t     struct flow_offload *flow)\n{\n\trhashtable_remove_fast(&flow_table->rhashtable,\n\t\t\t       &flow->tuplehash[FLOW_OFFLOAD_DIR_ORIGINAL].node,\n\t\t\t       nf_flow_offload_rhash_params);\n\trhashtable_remove_fast(&flow_table->rhashtable,\n\t\t\t       &flow->tuplehash[FLOW_OFFLOAD_DIR_REPLY].node,\n\t\t\t       nf_flow_offload_rhash_params);\n\tflow_offload_free(flow);\n}\n\nvoid flow_offload_teardown(struct flow_offload *flow)\n{\n\tclear_bit(IPS_OFFLOAD_BIT, &flow->ct->status);\n\tset_bit(NF_FLOW_TEARDOWN, &flow->flags);\n\tflow_offload_fixup_ct(flow->ct);\n}\nEXPORT_SYMBOL_GPL(flow_offload_teardown);\n\nstruct flow_offload_tuple_rhash *\nflow_offload_lookup(struct nf_flowtable *flow_table,\n\t\t    struct flow_offload_tuple *tuple)\n{\n\tstruct flow_offload_tuple_rhash *tuplehash;\n\tstruct flow_offload *flow;\n\tint dir;\n\n\ttuplehash = rhashtable_lookup(&flow_table->rhashtable, tuple,\n\t\t\t\t      nf_flow_offload_rhash_params);\n\tif (!tuplehash)\n\t\treturn NULL;\n\n\tdir = tuplehash->tuple.dir;\n\tflow = container_of(tuplehash, struct flow_offload, tuplehash[dir]);\n\tif (test_bit(NF_FLOW_TEARDOWN, &flow->flags))\n\t\treturn NULL;\n\n\tif (unlikely(nf_ct_is_dying(flow->ct)))\n\t\treturn NULL;\n\n\treturn tuplehash;\n}\nEXPORT_SYMBOL_GPL(flow_offload_lookup);\n\nstatic int\nnf_flow_table_iterate(struct nf_flowtable *flow_table,\n\t\t      void (*iter)(struct nf_flowtable *flowtable,\n\t\t\t\t   struct flow_offload *flow, void *data),\n\t\t      void *data)\n{\n\tstruct flow_offload_tuple_rhash *tuplehash;\n\tstruct rhashtable_iter hti;\n\tstruct flow_offload *flow;\n\tint err = 0;\n\n\trhashtable_walk_enter(&flow_table->rhashtable, &hti);\n\trhashtable_walk_start(&hti);\n\n\twhile ((tuplehash = rhashtable_walk_next(&hti))) {\n\t\tif (IS_ERR(tuplehash)) {\n\t\t\tif (PTR_ERR(tuplehash) != -EAGAIN) {\n\t\t\t\terr = PTR_ERR(tuplehash);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\t\tif (tuplehash->tuple.dir)\n\t\t\tcontinue;\n\n\t\tflow = container_of(tuplehash, struct flow_offload, tuplehash[0]);\n\n\t\titer(flow_table, flow, data);\n\t}\n\trhashtable_walk_stop(&hti);\n\trhashtable_walk_exit(&hti);\n\n\treturn err;\n}\n\nstatic bool nf_flow_custom_gc(struct nf_flowtable *flow_table,\n\t\t\t      const struct flow_offload *flow)\n{\n\treturn flow_table->type->gc && flow_table->type->gc(flow);\n}\n\nstatic void nf_flow_offload_gc_step(struct nf_flowtable *flow_table,\n\t\t\t\t    struct flow_offload *flow, void *data)\n{\n\tif (nf_flow_has_expired(flow) ||\n\t    nf_ct_is_dying(flow->ct) ||\n\t    nf_flow_custom_gc(flow_table, flow))\n\t\tflow_offload_teardown(flow);\n\n\tif (test_bit(NF_FLOW_TEARDOWN, &flow->flags)) {\n\t\tif (test_bit(NF_FLOW_HW, &flow->flags)) {\n\t\t\tif (!test_bit(NF_FLOW_HW_DYING, &flow->flags))\n\t\t\t\tnf_flow_offload_del(flow_table, flow);\n\t\t\telse if (test_bit(NF_FLOW_HW_DEAD, &flow->flags))\n\t\t\t\tflow_offload_del(flow_table, flow);\n\t\t} else {\n\t\t\tflow_offload_del(flow_table, flow);\n\t\t}\n\t} else if (test_bit(NF_FLOW_HW, &flow->flags)) {\n\t\tnf_flow_offload_stats(flow_table, flow);\n\t}\n}\n\nvoid nf_flow_table_gc_run(struct nf_flowtable *flow_table)\n{\n\tnf_flow_table_iterate(flow_table, nf_flow_offload_gc_step, NULL);\n}\n\nstatic void nf_flow_offload_work_gc(struct work_struct *work)\n{\n\tstruct nf_flowtable *flow_table;\n\n\tflow_table = container_of(work, struct nf_flowtable, gc_work.work);\n\tnf_flow_table_gc_run(flow_table);\n\tqueue_delayed_work(system_power_efficient_wq, &flow_table->gc_work, HZ);\n}\n\nstatic void nf_flow_nat_port_tcp(struct sk_buff *skb, unsigned int thoff,\n\t\t\t\t __be16 port, __be16 new_port)\n{\n\tstruct tcphdr *tcph;\n\n\ttcph = (void *)(skb_network_header(skb) + thoff);\n\tinet_proto_csum_replace2(&tcph->check, skb, port, new_port, false);\n}\n\nstatic void nf_flow_nat_port_udp(struct sk_buff *skb, unsigned int thoff,\n\t\t\t\t __be16 port, __be16 new_port)\n{\n\tstruct udphdr *udph;\n\n\tudph = (void *)(skb_network_header(skb) + thoff);\n\tif (udph->check || skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\tinet_proto_csum_replace2(&udph->check, skb, port,\n\t\t\t\t\t new_port, false);\n\t\tif (!udph->check)\n\t\t\tudph->check = CSUM_MANGLED_0;\n\t}\n}\n\nstatic void nf_flow_nat_port(struct sk_buff *skb, unsigned int thoff,\n\t\t\t     u8 protocol, __be16 port, __be16 new_port)\n{\n\tswitch (protocol) {\n\tcase IPPROTO_TCP:\n\t\tnf_flow_nat_port_tcp(skb, thoff, port, new_port);\n\t\tbreak;\n\tcase IPPROTO_UDP:\n\t\tnf_flow_nat_port_udp(skb, thoff, port, new_port);\n\t\tbreak;\n\t}\n}\n\nvoid nf_flow_snat_port(const struct flow_offload *flow,\n\t\t       struct sk_buff *skb, unsigned int thoff,\n\t\t       u8 protocol, enum flow_offload_tuple_dir dir)\n{\n\tstruct flow_ports *hdr;\n\t__be16 port, new_port;\n\n\thdr = (void *)(skb_network_header(skb) + thoff);\n\n\tswitch (dir) {\n\tcase FLOW_OFFLOAD_DIR_ORIGINAL:\n\t\tport = hdr->source;\n\t\tnew_port = flow->tuplehash[FLOW_OFFLOAD_DIR_REPLY].tuple.dst_port;\n\t\thdr->source = new_port;\n\t\tbreak;\n\tcase FLOW_OFFLOAD_DIR_REPLY:\n\t\tport = hdr->dest;\n\t\tnew_port = flow->tuplehash[FLOW_OFFLOAD_DIR_ORIGINAL].tuple.src_port;\n\t\thdr->dest = new_port;\n\t\tbreak;\n\t}\n\n\tnf_flow_nat_port(skb, thoff, protocol, port, new_port);\n}\nEXPORT_SYMBOL_GPL(nf_flow_snat_port);\n\nvoid nf_flow_dnat_port(const struct flow_offload *flow, struct sk_buff *skb,\n\t\t       unsigned int thoff, u8 protocol,\n\t\t       enum flow_offload_tuple_dir dir)\n{\n\tstruct flow_ports *hdr;\n\t__be16 port, new_port;\n\n\thdr = (void *)(skb_network_header(skb) + thoff);\n\n\tswitch (dir) {\n\tcase FLOW_OFFLOAD_DIR_ORIGINAL:\n\t\tport = hdr->dest;\n\t\tnew_port = flow->tuplehash[FLOW_OFFLOAD_DIR_REPLY].tuple.src_port;\n\t\thdr->dest = new_port;\n\t\tbreak;\n\tcase FLOW_OFFLOAD_DIR_REPLY:\n\t\tport = hdr->source;\n\t\tnew_port = flow->tuplehash[FLOW_OFFLOAD_DIR_ORIGINAL].tuple.dst_port;\n\t\thdr->source = new_port;\n\t\tbreak;\n\t}\n\n\tnf_flow_nat_port(skb, thoff, protocol, port, new_port);\n}\nEXPORT_SYMBOL_GPL(nf_flow_dnat_port);\n\nint nf_flow_table_init(struct nf_flowtable *flowtable)\n{\n\tint err;\n\n\tINIT_DELAYED_WORK(&flowtable->gc_work, nf_flow_offload_work_gc);\n\tflow_block_init(&flowtable->flow_block);\n\tinit_rwsem(&flowtable->flow_block_lock);\n\n\terr = rhashtable_init(&flowtable->rhashtable,\n\t\t\t      &nf_flow_offload_rhash_params);\n\tif (err < 0)\n\t\treturn err;\n\n\tqueue_delayed_work(system_power_efficient_wq,\n\t\t\t   &flowtable->gc_work, HZ);\n\n\tmutex_lock(&flowtable_lock);\n\tlist_add(&flowtable->list, &flowtables);\n\tmutex_unlock(&flowtable_lock);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(nf_flow_table_init);\n\nstatic void nf_flow_table_do_cleanup(struct nf_flowtable *flow_table,\n\t\t\t\t     struct flow_offload *flow, void *data)\n{\n\tstruct net_device *dev = data;\n\n\tif (!dev) {\n\t\tflow_offload_teardown(flow);\n\t\treturn;\n\t}\n\n\tif (net_eq(nf_ct_net(flow->ct), dev_net(dev)) &&\n\t    (flow->tuplehash[0].tuple.iifidx == dev->ifindex ||\n\t     flow->tuplehash[1].tuple.iifidx == dev->ifindex))\n\t\tflow_offload_teardown(flow);\n}\n\nvoid nf_flow_table_gc_cleanup(struct nf_flowtable *flowtable,\n\t\t\t      struct net_device *dev)\n{\n\tnf_flow_table_iterate(flowtable, nf_flow_table_do_cleanup, dev);\n\tflush_delayed_work(&flowtable->gc_work);\n\tnf_flow_table_offload_flush(flowtable);\n}\n\nvoid nf_flow_table_cleanup(struct net_device *dev)\n{\n\tstruct nf_flowtable *flowtable;\n\n\tmutex_lock(&flowtable_lock);\n\tlist_for_each_entry(flowtable, &flowtables, list)\n\t\tnf_flow_table_gc_cleanup(flowtable, dev);\n\tmutex_unlock(&flowtable_lock);\n}\nEXPORT_SYMBOL_GPL(nf_flow_table_cleanup);\n\nvoid nf_flow_table_free(struct nf_flowtable *flow_table)\n{\n\tmutex_lock(&flowtable_lock);\n\tlist_del(&flow_table->list);\n\tmutex_unlock(&flowtable_lock);\n\n\tcancel_delayed_work_sync(&flow_table->gc_work);\n\tnf_flow_table_offload_flush(flow_table);\n\t \n\tnf_flow_table_iterate(flow_table, nf_flow_table_do_cleanup, NULL);\n\tnf_flow_table_gc_run(flow_table);\n\tnf_flow_table_offload_flush_cleanup(flow_table);\n\trhashtable_destroy(&flow_table->rhashtable);\n}\nEXPORT_SYMBOL_GPL(nf_flow_table_free);\n\nstatic int nf_flow_table_init_net(struct net *net)\n{\n\tnet->ft.stat = alloc_percpu(struct nf_flow_table_stat);\n\treturn net->ft.stat ? 0 : -ENOMEM;\n}\n\nstatic void nf_flow_table_fini_net(struct net *net)\n{\n\tfree_percpu(net->ft.stat);\n}\n\nstatic int nf_flow_table_pernet_init(struct net *net)\n{\n\tint ret;\n\n\tret = nf_flow_table_init_net(net);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tret = nf_flow_table_init_proc(net);\n\tif (ret < 0)\n\t\tgoto out_proc;\n\n\treturn 0;\n\nout_proc:\n\tnf_flow_table_fini_net(net);\n\treturn ret;\n}\n\nstatic void nf_flow_table_pernet_exit(struct list_head *net_exit_list)\n{\n\tstruct net *net;\n\n\tlist_for_each_entry(net, net_exit_list, exit_list) {\n\t\tnf_flow_table_fini_proc(net);\n\t\tnf_flow_table_fini_net(net);\n\t}\n}\n\nstatic struct pernet_operations nf_flow_table_net_ops = {\n\t.init = nf_flow_table_pernet_init,\n\t.exit_batch = nf_flow_table_pernet_exit,\n};\n\nstatic int __init nf_flow_table_module_init(void)\n{\n\tint ret;\n\n\tret = register_pernet_subsys(&nf_flow_table_net_ops);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tret = nf_flow_table_offload_init();\n\tif (ret)\n\t\tgoto out_offload;\n\n\treturn 0;\n\nout_offload:\n\tunregister_pernet_subsys(&nf_flow_table_net_ops);\n\treturn ret;\n}\n\nstatic void __exit nf_flow_table_module_exit(void)\n{\n\tnf_flow_table_offload_exit();\n\tunregister_pernet_subsys(&nf_flow_table_net_ops);\n}\n\nmodule_init(nf_flow_table_module_init);\nmodule_exit(nf_flow_table_module_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_AUTHOR(\"Pablo Neira Ayuso <pablo@netfilter.org>\");\nMODULE_DESCRIPTION(\"Netfilter flow table module\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}