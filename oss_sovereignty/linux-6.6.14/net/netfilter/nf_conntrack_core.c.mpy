{
  "module_name": "nf_conntrack_core.c",
  "hash_id": "1194304c4efa9f7e7b1456426b67e825cbd553ce87f1454e72f4aa15ca26ebdc",
  "original_prompt": "Ingested from linux-6.6.14/net/netfilter/nf_conntrack_core.c",
  "human_readable_source": "\n \n\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/types.h>\n#include <linux/netfilter.h>\n#include <linux/module.h>\n#include <linux/sched.h>\n#include <linux/skbuff.h>\n#include <linux/proc_fs.h>\n#include <linux/vmalloc.h>\n#include <linux/stddef.h>\n#include <linux/slab.h>\n#include <linux/random.h>\n#include <linux/siphash.h>\n#include <linux/err.h>\n#include <linux/percpu.h>\n#include <linux/moduleparam.h>\n#include <linux/notifier.h>\n#include <linux/kernel.h>\n#include <linux/netdevice.h>\n#include <linux/socket.h>\n#include <linux/mm.h>\n#include <linux/nsproxy.h>\n#include <linux/rculist_nulls.h>\n\n#include <net/netfilter/nf_conntrack.h>\n#include <net/netfilter/nf_conntrack_bpf.h>\n#include <net/netfilter/nf_conntrack_l4proto.h>\n#include <net/netfilter/nf_conntrack_expect.h>\n#include <net/netfilter/nf_conntrack_helper.h>\n#include <net/netfilter/nf_conntrack_core.h>\n#include <net/netfilter/nf_conntrack_extend.h>\n#include <net/netfilter/nf_conntrack_acct.h>\n#include <net/netfilter/nf_conntrack_ecache.h>\n#include <net/netfilter/nf_conntrack_zones.h>\n#include <net/netfilter/nf_conntrack_timestamp.h>\n#include <net/netfilter/nf_conntrack_timeout.h>\n#include <net/netfilter/nf_conntrack_labels.h>\n#include <net/netfilter/nf_conntrack_synproxy.h>\n#include <net/netfilter/nf_nat.h>\n#include <net/netfilter/nf_nat_helper.h>\n#include <net/netns/hash.h>\n#include <net/ip.h>\n\n#include \"nf_internals.h\"\n\n__cacheline_aligned_in_smp spinlock_t nf_conntrack_locks[CONNTRACK_LOCKS];\nEXPORT_SYMBOL_GPL(nf_conntrack_locks);\n\n__cacheline_aligned_in_smp DEFINE_SPINLOCK(nf_conntrack_expect_lock);\nEXPORT_SYMBOL_GPL(nf_conntrack_expect_lock);\n\nstruct hlist_nulls_head *nf_conntrack_hash __read_mostly;\nEXPORT_SYMBOL_GPL(nf_conntrack_hash);\n\nstruct conntrack_gc_work {\n\tstruct delayed_work\tdwork;\n\tu32\t\t\tnext_bucket;\n\tu32\t\t\tavg_timeout;\n\tu32\t\t\tcount;\n\tu32\t\t\tstart_time;\n\tbool\t\t\texiting;\n\tbool\t\t\tearly_drop;\n};\n\nstatic __read_mostly struct kmem_cache *nf_conntrack_cachep;\nstatic DEFINE_SPINLOCK(nf_conntrack_locks_all_lock);\nstatic __read_mostly bool nf_conntrack_locks_all;\n\n \nstatic DEFINE_MUTEX(nf_conntrack_mutex);\n\n#define GC_SCAN_INTERVAL_MAX\t(60ul * HZ)\n#define GC_SCAN_INTERVAL_MIN\t(1ul * HZ)\n\n \n#define GC_SCAN_INTERVAL_CLAMP\t(300ul * HZ)\n\n \n#define GC_SCAN_INITIAL_COUNT\t100\n#define GC_SCAN_INTERVAL_INIT\tGC_SCAN_INTERVAL_MAX\n\n#define GC_SCAN_MAX_DURATION\tmsecs_to_jiffies(10)\n#define GC_SCAN_EXPIRED_MAX\t(64000u / HZ)\n\n#define MIN_CHAINLEN\t50u\n#define MAX_CHAINLEN\t(80u - MIN_CHAINLEN)\n\nstatic struct conntrack_gc_work conntrack_gc_work;\n\nvoid nf_conntrack_lock(spinlock_t *lock) __acquires(lock)\n{\n\t \n\tspin_lock(lock);\n\n\t \n\tif (likely(smp_load_acquire(&nf_conntrack_locks_all) == false))\n\t\treturn;\n\n\t \n\tspin_unlock(lock);\n\n\t \n\tspin_lock(&nf_conntrack_locks_all_lock);\n\n\t \n\tspin_lock(lock);\n\n\t \n\tspin_unlock(&nf_conntrack_locks_all_lock);\n}\nEXPORT_SYMBOL_GPL(nf_conntrack_lock);\n\nstatic void nf_conntrack_double_unlock(unsigned int h1, unsigned int h2)\n{\n\th1 %= CONNTRACK_LOCKS;\n\th2 %= CONNTRACK_LOCKS;\n\tspin_unlock(&nf_conntrack_locks[h1]);\n\tif (h1 != h2)\n\t\tspin_unlock(&nf_conntrack_locks[h2]);\n}\n\n \nstatic bool nf_conntrack_double_lock(struct net *net, unsigned int h1,\n\t\t\t\t     unsigned int h2, unsigned int sequence)\n{\n\th1 %= CONNTRACK_LOCKS;\n\th2 %= CONNTRACK_LOCKS;\n\tif (h1 <= h2) {\n\t\tnf_conntrack_lock(&nf_conntrack_locks[h1]);\n\t\tif (h1 != h2)\n\t\t\tspin_lock_nested(&nf_conntrack_locks[h2],\n\t\t\t\t\t SINGLE_DEPTH_NESTING);\n\t} else {\n\t\tnf_conntrack_lock(&nf_conntrack_locks[h2]);\n\t\tspin_lock_nested(&nf_conntrack_locks[h1],\n\t\t\t\t SINGLE_DEPTH_NESTING);\n\t}\n\tif (read_seqcount_retry(&nf_conntrack_generation, sequence)) {\n\t\tnf_conntrack_double_unlock(h1, h2);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic void nf_conntrack_all_lock(void)\n\t__acquires(&nf_conntrack_locks_all_lock)\n{\n\tint i;\n\n\tspin_lock(&nf_conntrack_locks_all_lock);\n\n\t \n\tWRITE_ONCE(nf_conntrack_locks_all, true);\n\n\tfor (i = 0; i < CONNTRACK_LOCKS; i++) {\n\t\tspin_lock(&nf_conntrack_locks[i]);\n\n\t\t \n\t\tspin_unlock(&nf_conntrack_locks[i]);\n\t}\n}\n\nstatic void nf_conntrack_all_unlock(void)\n\t__releases(&nf_conntrack_locks_all_lock)\n{\n\t \n\tsmp_store_release(&nf_conntrack_locks_all, false);\n\tspin_unlock(&nf_conntrack_locks_all_lock);\n}\n\nunsigned int nf_conntrack_htable_size __read_mostly;\nEXPORT_SYMBOL_GPL(nf_conntrack_htable_size);\n\nunsigned int nf_conntrack_max __read_mostly;\nEXPORT_SYMBOL_GPL(nf_conntrack_max);\nseqcount_spinlock_t nf_conntrack_generation __read_mostly;\nstatic siphash_aligned_key_t nf_conntrack_hash_rnd;\n\nstatic u32 hash_conntrack_raw(const struct nf_conntrack_tuple *tuple,\n\t\t\t      unsigned int zoneid,\n\t\t\t      const struct net *net)\n{\n\tsiphash_key_t key;\n\n\tget_random_once(&nf_conntrack_hash_rnd, sizeof(nf_conntrack_hash_rnd));\n\n\tkey = nf_conntrack_hash_rnd;\n\n\tkey.key[0] ^= zoneid;\n\tkey.key[1] ^= net_hash_mix(net);\n\n\treturn siphash((void *)tuple,\n\t\t\toffsetofend(struct nf_conntrack_tuple, dst.__nfct_hash_offsetend),\n\t\t\t&key);\n}\n\nstatic u32 scale_hash(u32 hash)\n{\n\treturn reciprocal_scale(hash, nf_conntrack_htable_size);\n}\n\nstatic u32 __hash_conntrack(const struct net *net,\n\t\t\t    const struct nf_conntrack_tuple *tuple,\n\t\t\t    unsigned int zoneid,\n\t\t\t    unsigned int size)\n{\n\treturn reciprocal_scale(hash_conntrack_raw(tuple, zoneid, net), size);\n}\n\nstatic u32 hash_conntrack(const struct net *net,\n\t\t\t  const struct nf_conntrack_tuple *tuple,\n\t\t\t  unsigned int zoneid)\n{\n\treturn scale_hash(hash_conntrack_raw(tuple, zoneid, net));\n}\n\nstatic bool nf_ct_get_tuple_ports(const struct sk_buff *skb,\n\t\t\t\t  unsigned int dataoff,\n\t\t\t\t  struct nf_conntrack_tuple *tuple)\n{\tstruct {\n\t\t__be16 sport;\n\t\t__be16 dport;\n\t} _inet_hdr, *inet_hdr;\n\n\t \n\tinet_hdr = skb_header_pointer(skb, dataoff, sizeof(_inet_hdr), &_inet_hdr);\n\tif (!inet_hdr)\n\t\treturn false;\n\n\ttuple->src.u.udp.port = inet_hdr->sport;\n\ttuple->dst.u.udp.port = inet_hdr->dport;\n\treturn true;\n}\n\nstatic bool\nnf_ct_get_tuple(const struct sk_buff *skb,\n\t\tunsigned int nhoff,\n\t\tunsigned int dataoff,\n\t\tu_int16_t l3num,\n\t\tu_int8_t protonum,\n\t\tstruct net *net,\n\t\tstruct nf_conntrack_tuple *tuple)\n{\n\tunsigned int size;\n\tconst __be32 *ap;\n\t__be32 _addrs[8];\n\n\tmemset(tuple, 0, sizeof(*tuple));\n\n\ttuple->src.l3num = l3num;\n\tswitch (l3num) {\n\tcase NFPROTO_IPV4:\n\t\tnhoff += offsetof(struct iphdr, saddr);\n\t\tsize = 2 * sizeof(__be32);\n\t\tbreak;\n\tcase NFPROTO_IPV6:\n\t\tnhoff += offsetof(struct ipv6hdr, saddr);\n\t\tsize = sizeof(_addrs);\n\t\tbreak;\n\tdefault:\n\t\treturn true;\n\t}\n\n\tap = skb_header_pointer(skb, nhoff, size, _addrs);\n\tif (!ap)\n\t\treturn false;\n\n\tswitch (l3num) {\n\tcase NFPROTO_IPV4:\n\t\ttuple->src.u3.ip = ap[0];\n\t\ttuple->dst.u3.ip = ap[1];\n\t\tbreak;\n\tcase NFPROTO_IPV6:\n\t\tmemcpy(tuple->src.u3.ip6, ap, sizeof(tuple->src.u3.ip6));\n\t\tmemcpy(tuple->dst.u3.ip6, ap + 4, sizeof(tuple->dst.u3.ip6));\n\t\tbreak;\n\t}\n\n\ttuple->dst.protonum = protonum;\n\ttuple->dst.dir = IP_CT_DIR_ORIGINAL;\n\n\tswitch (protonum) {\n#if IS_ENABLED(CONFIG_IPV6)\n\tcase IPPROTO_ICMPV6:\n\t\treturn icmpv6_pkt_to_tuple(skb, dataoff, net, tuple);\n#endif\n\tcase IPPROTO_ICMP:\n\t\treturn icmp_pkt_to_tuple(skb, dataoff, net, tuple);\n#ifdef CONFIG_NF_CT_PROTO_GRE\n\tcase IPPROTO_GRE:\n\t\treturn gre_pkt_to_tuple(skb, dataoff, net, tuple);\n#endif\n\tcase IPPROTO_TCP:\n\tcase IPPROTO_UDP:\n#ifdef CONFIG_NF_CT_PROTO_UDPLITE\n\tcase IPPROTO_UDPLITE:\n#endif\n#ifdef CONFIG_NF_CT_PROTO_SCTP\n\tcase IPPROTO_SCTP:\n#endif\n#ifdef CONFIG_NF_CT_PROTO_DCCP\n\tcase IPPROTO_DCCP:\n#endif\n\t\t \n\t\treturn nf_ct_get_tuple_ports(skb, dataoff, tuple);\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn true;\n}\n\nstatic int ipv4_get_l4proto(const struct sk_buff *skb, unsigned int nhoff,\n\t\t\t    u_int8_t *protonum)\n{\n\tint dataoff = -1;\n\tconst struct iphdr *iph;\n\tstruct iphdr _iph;\n\n\tiph = skb_header_pointer(skb, nhoff, sizeof(_iph), &_iph);\n\tif (!iph)\n\t\treturn -1;\n\n\t \n\tif (iph->frag_off & htons(IP_OFFSET))\n\t\treturn -1;\n\n\tdataoff = nhoff + (iph->ihl << 2);\n\t*protonum = iph->protocol;\n\n\t \n\tif (dataoff > skb->len) {\n\t\tpr_debug(\"bogus IPv4 packet: nhoff %u, ihl %u, skblen %u\\n\",\n\t\t\t nhoff, iph->ihl << 2, skb->len);\n\t\treturn -1;\n\t}\n\treturn dataoff;\n}\n\n#if IS_ENABLED(CONFIG_IPV6)\nstatic int ipv6_get_l4proto(const struct sk_buff *skb, unsigned int nhoff,\n\t\t\t    u8 *protonum)\n{\n\tint protoff = -1;\n\tunsigned int extoff = nhoff + sizeof(struct ipv6hdr);\n\t__be16 frag_off;\n\tu8 nexthdr;\n\n\tif (skb_copy_bits(skb, nhoff + offsetof(struct ipv6hdr, nexthdr),\n\t\t\t  &nexthdr, sizeof(nexthdr)) != 0) {\n\t\tpr_debug(\"can't get nexthdr\\n\");\n\t\treturn -1;\n\t}\n\tprotoff = ipv6_skip_exthdr(skb, extoff, &nexthdr, &frag_off);\n\t \n\tif (protoff < 0 || (frag_off & htons(~0x7)) != 0) {\n\t\tpr_debug(\"can't find proto in pkt\\n\");\n\t\treturn -1;\n\t}\n\n\t*protonum = nexthdr;\n\treturn protoff;\n}\n#endif\n\nstatic int get_l4proto(const struct sk_buff *skb,\n\t\t       unsigned int nhoff, u8 pf, u8 *l4num)\n{\n\tswitch (pf) {\n\tcase NFPROTO_IPV4:\n\t\treturn ipv4_get_l4proto(skb, nhoff, l4num);\n#if IS_ENABLED(CONFIG_IPV6)\n\tcase NFPROTO_IPV6:\n\t\treturn ipv6_get_l4proto(skb, nhoff, l4num);\n#endif\n\tdefault:\n\t\t*l4num = 0;\n\t\tbreak;\n\t}\n\treturn -1;\n}\n\nbool nf_ct_get_tuplepr(const struct sk_buff *skb, unsigned int nhoff,\n\t\t       u_int16_t l3num,\n\t\t       struct net *net, struct nf_conntrack_tuple *tuple)\n{\n\tu8 protonum;\n\tint protoff;\n\n\tprotoff = get_l4proto(skb, nhoff, l3num, &protonum);\n\tif (protoff <= 0)\n\t\treturn false;\n\n\treturn nf_ct_get_tuple(skb, nhoff, protoff, l3num, protonum, net, tuple);\n}\nEXPORT_SYMBOL_GPL(nf_ct_get_tuplepr);\n\nbool\nnf_ct_invert_tuple(struct nf_conntrack_tuple *inverse,\n\t\t   const struct nf_conntrack_tuple *orig)\n{\n\tmemset(inverse, 0, sizeof(*inverse));\n\n\tinverse->src.l3num = orig->src.l3num;\n\n\tswitch (orig->src.l3num) {\n\tcase NFPROTO_IPV4:\n\t\tinverse->src.u3.ip = orig->dst.u3.ip;\n\t\tinverse->dst.u3.ip = orig->src.u3.ip;\n\t\tbreak;\n\tcase NFPROTO_IPV6:\n\t\tinverse->src.u3.in6 = orig->dst.u3.in6;\n\t\tinverse->dst.u3.in6 = orig->src.u3.in6;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tinverse->dst.dir = !orig->dst.dir;\n\n\tinverse->dst.protonum = orig->dst.protonum;\n\n\tswitch (orig->dst.protonum) {\n\tcase IPPROTO_ICMP:\n\t\treturn nf_conntrack_invert_icmp_tuple(inverse, orig);\n#if IS_ENABLED(CONFIG_IPV6)\n\tcase IPPROTO_ICMPV6:\n\t\treturn nf_conntrack_invert_icmpv6_tuple(inverse, orig);\n#endif\n\t}\n\n\tinverse->src.u.all = orig->dst.u.all;\n\tinverse->dst.u.all = orig->src.u.all;\n\treturn true;\n}\nEXPORT_SYMBOL_GPL(nf_ct_invert_tuple);\n\n \nu32 nf_ct_get_id(const struct nf_conn *ct)\n{\n\tstatic siphash_aligned_key_t ct_id_seed;\n\tunsigned long a, b, c, d;\n\n\tnet_get_random_once(&ct_id_seed, sizeof(ct_id_seed));\n\n\ta = (unsigned long)ct;\n\tb = (unsigned long)ct->master;\n\tc = (unsigned long)nf_ct_net(ct);\n\td = (unsigned long)siphash(&ct->tuplehash[IP_CT_DIR_ORIGINAL].tuple,\n\t\t\t\t   sizeof(ct->tuplehash[IP_CT_DIR_ORIGINAL].tuple),\n\t\t\t\t   &ct_id_seed);\n#ifdef CONFIG_64BIT\n\treturn siphash_4u64((u64)a, (u64)b, (u64)c, (u64)d, &ct_id_seed);\n#else\n\treturn siphash_4u32((u32)a, (u32)b, (u32)c, (u32)d, &ct_id_seed);\n#endif\n}\nEXPORT_SYMBOL_GPL(nf_ct_get_id);\n\nstatic void\nclean_from_lists(struct nf_conn *ct)\n{\n\thlist_nulls_del_rcu(&ct->tuplehash[IP_CT_DIR_ORIGINAL].hnnode);\n\thlist_nulls_del_rcu(&ct->tuplehash[IP_CT_DIR_REPLY].hnnode);\n\n\t \n\tnf_ct_remove_expectations(ct);\n}\n\n#define NFCT_ALIGN(len)\t(((len) + NFCT_INFOMASK) & ~NFCT_INFOMASK)\n\n \nstruct nf_conn *nf_ct_tmpl_alloc(struct net *net,\n\t\t\t\t const struct nf_conntrack_zone *zone,\n\t\t\t\t gfp_t flags)\n{\n\tstruct nf_conn *tmpl, *p;\n\n\tif (ARCH_KMALLOC_MINALIGN <= NFCT_INFOMASK) {\n\t\ttmpl = kzalloc(sizeof(*tmpl) + NFCT_INFOMASK, flags);\n\t\tif (!tmpl)\n\t\t\treturn NULL;\n\n\t\tp = tmpl;\n\t\ttmpl = (struct nf_conn *)NFCT_ALIGN((unsigned long)p);\n\t\tif (tmpl != p) {\n\t\t\ttmpl = (struct nf_conn *)NFCT_ALIGN((unsigned long)p);\n\t\t\ttmpl->proto.tmpl_padto = (char *)tmpl - (char *)p;\n\t\t}\n\t} else {\n\t\ttmpl = kzalloc(sizeof(*tmpl), flags);\n\t\tif (!tmpl)\n\t\t\treturn NULL;\n\t}\n\n\ttmpl->status = IPS_TEMPLATE;\n\twrite_pnet(&tmpl->ct_net, net);\n\tnf_ct_zone_add(tmpl, zone);\n\trefcount_set(&tmpl->ct_general.use, 1);\n\n\treturn tmpl;\n}\nEXPORT_SYMBOL_GPL(nf_ct_tmpl_alloc);\n\nvoid nf_ct_tmpl_free(struct nf_conn *tmpl)\n{\n\tkfree(tmpl->ext);\n\n\tif (ARCH_KMALLOC_MINALIGN <= NFCT_INFOMASK)\n\t\tkfree((char *)tmpl - tmpl->proto.tmpl_padto);\n\telse\n\t\tkfree(tmpl);\n}\nEXPORT_SYMBOL_GPL(nf_ct_tmpl_free);\n\nstatic void destroy_gre_conntrack(struct nf_conn *ct)\n{\n#ifdef CONFIG_NF_CT_PROTO_GRE\n\tstruct nf_conn *master = ct->master;\n\n\tif (master)\n\t\tnf_ct_gre_keymap_destroy(master);\n#endif\n}\n\nvoid nf_ct_destroy(struct nf_conntrack *nfct)\n{\n\tstruct nf_conn *ct = (struct nf_conn *)nfct;\n\n\tWARN_ON(refcount_read(&nfct->use) != 0);\n\n\tif (unlikely(nf_ct_is_template(ct))) {\n\t\tnf_ct_tmpl_free(ct);\n\t\treturn;\n\t}\n\n\tif (unlikely(nf_ct_protonum(ct) == IPPROTO_GRE))\n\t\tdestroy_gre_conntrack(ct);\n\n\t \n\tnf_ct_remove_expectations(ct);\n\n\tif (ct->master)\n\t\tnf_ct_put(ct->master);\n\n\tnf_conntrack_free(ct);\n}\nEXPORT_SYMBOL(nf_ct_destroy);\n\nstatic void __nf_ct_delete_from_lists(struct nf_conn *ct)\n{\n\tstruct net *net = nf_ct_net(ct);\n\tunsigned int hash, reply_hash;\n\tunsigned int sequence;\n\n\tdo {\n\t\tsequence = read_seqcount_begin(&nf_conntrack_generation);\n\t\thash = hash_conntrack(net,\n\t\t\t\t      &ct->tuplehash[IP_CT_DIR_ORIGINAL].tuple,\n\t\t\t\t      nf_ct_zone_id(nf_ct_zone(ct), IP_CT_DIR_ORIGINAL));\n\t\treply_hash = hash_conntrack(net,\n\t\t\t\t\t   &ct->tuplehash[IP_CT_DIR_REPLY].tuple,\n\t\t\t\t\t   nf_ct_zone_id(nf_ct_zone(ct), IP_CT_DIR_REPLY));\n\t} while (nf_conntrack_double_lock(net, hash, reply_hash, sequence));\n\n\tclean_from_lists(ct);\n\tnf_conntrack_double_unlock(hash, reply_hash);\n}\n\nstatic void nf_ct_delete_from_lists(struct nf_conn *ct)\n{\n\tnf_ct_helper_destroy(ct);\n\tlocal_bh_disable();\n\n\t__nf_ct_delete_from_lists(ct);\n\n\tlocal_bh_enable();\n}\n\nstatic void nf_ct_add_to_ecache_list(struct nf_conn *ct)\n{\n#ifdef CONFIG_NF_CONNTRACK_EVENTS\n\tstruct nf_conntrack_net *cnet = nf_ct_pernet(nf_ct_net(ct));\n\n\tspin_lock(&cnet->ecache.dying_lock);\n\thlist_nulls_add_head_rcu(&ct->tuplehash[IP_CT_DIR_ORIGINAL].hnnode,\n\t\t\t\t &cnet->ecache.dying_list);\n\tspin_unlock(&cnet->ecache.dying_lock);\n#endif\n}\n\nbool nf_ct_delete(struct nf_conn *ct, u32 portid, int report)\n{\n\tstruct nf_conn_tstamp *tstamp;\n\tstruct net *net;\n\n\tif (test_and_set_bit(IPS_DYING_BIT, &ct->status))\n\t\treturn false;\n\n\ttstamp = nf_conn_tstamp_find(ct);\n\tif (tstamp) {\n\t\ts32 timeout = READ_ONCE(ct->timeout) - nfct_time_stamp;\n\n\t\ttstamp->stop = ktime_get_real_ns();\n\t\tif (timeout < 0)\n\t\t\ttstamp->stop -= jiffies_to_nsecs(-timeout);\n\t}\n\n\tif (nf_conntrack_event_report(IPCT_DESTROY, ct,\n\t\t\t\t    portid, report) < 0) {\n\t\t \n\t\tnf_ct_helper_destroy(ct);\n\t\tlocal_bh_disable();\n\t\t__nf_ct_delete_from_lists(ct);\n\t\tnf_ct_add_to_ecache_list(ct);\n\t\tlocal_bh_enable();\n\n\t\tnf_conntrack_ecache_work(nf_ct_net(ct), NFCT_ECACHE_DESTROY_FAIL);\n\t\treturn false;\n\t}\n\n\tnet = nf_ct_net(ct);\n\tif (nf_conntrack_ecache_dwork_pending(net))\n\t\tnf_conntrack_ecache_work(net, NFCT_ECACHE_DESTROY_SENT);\n\tnf_ct_delete_from_lists(ct);\n\tnf_ct_put(ct);\n\treturn true;\n}\nEXPORT_SYMBOL_GPL(nf_ct_delete);\n\nstatic inline bool\nnf_ct_key_equal(struct nf_conntrack_tuple_hash *h,\n\t\tconst struct nf_conntrack_tuple *tuple,\n\t\tconst struct nf_conntrack_zone *zone,\n\t\tconst struct net *net)\n{\n\tstruct nf_conn *ct = nf_ct_tuplehash_to_ctrack(h);\n\n\t \n\treturn nf_ct_tuple_equal(tuple, &h->tuple) &&\n\t       nf_ct_zone_equal(ct, zone, NF_CT_DIRECTION(h)) &&\n\t       nf_ct_is_confirmed(ct) &&\n\t       net_eq(net, nf_ct_net(ct));\n}\n\nstatic inline bool\nnf_ct_match(const struct nf_conn *ct1, const struct nf_conn *ct2)\n{\n\treturn nf_ct_tuple_equal(&ct1->tuplehash[IP_CT_DIR_ORIGINAL].tuple,\n\t\t\t\t &ct2->tuplehash[IP_CT_DIR_ORIGINAL].tuple) &&\n\t       nf_ct_tuple_equal(&ct1->tuplehash[IP_CT_DIR_REPLY].tuple,\n\t\t\t\t &ct2->tuplehash[IP_CT_DIR_REPLY].tuple) &&\n\t       nf_ct_zone_equal(ct1, nf_ct_zone(ct2), IP_CT_DIR_ORIGINAL) &&\n\t       nf_ct_zone_equal(ct1, nf_ct_zone(ct2), IP_CT_DIR_REPLY) &&\n\t       net_eq(nf_ct_net(ct1), nf_ct_net(ct2));\n}\n\n \nstatic void nf_ct_gc_expired(struct nf_conn *ct)\n{\n\tif (!refcount_inc_not_zero(&ct->ct_general.use))\n\t\treturn;\n\n\t \n\tsmp_acquire__after_ctrl_dep();\n\n\tif (nf_ct_should_gc(ct))\n\t\tnf_ct_kill(ct);\n\n\tnf_ct_put(ct);\n}\n\n \nstatic struct nf_conntrack_tuple_hash *\n____nf_conntrack_find(struct net *net, const struct nf_conntrack_zone *zone,\n\t\t      const struct nf_conntrack_tuple *tuple, u32 hash)\n{\n\tstruct nf_conntrack_tuple_hash *h;\n\tstruct hlist_nulls_head *ct_hash;\n\tstruct hlist_nulls_node *n;\n\tunsigned int bucket, hsize;\n\nbegin:\n\tnf_conntrack_get_ht(&ct_hash, &hsize);\n\tbucket = reciprocal_scale(hash, hsize);\n\n\thlist_nulls_for_each_entry_rcu(h, n, &ct_hash[bucket], hnnode) {\n\t\tstruct nf_conn *ct;\n\n\t\tct = nf_ct_tuplehash_to_ctrack(h);\n\t\tif (nf_ct_is_expired(ct)) {\n\t\t\tnf_ct_gc_expired(ct);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (nf_ct_key_equal(h, tuple, zone, net))\n\t\t\treturn h;\n\t}\n\t \n\tif (get_nulls_value(n) != bucket) {\n\t\tNF_CT_STAT_INC_ATOMIC(net, search_restart);\n\t\tgoto begin;\n\t}\n\n\treturn NULL;\n}\n\n \nstatic struct nf_conntrack_tuple_hash *\n__nf_conntrack_find_get(struct net *net, const struct nf_conntrack_zone *zone,\n\t\t\tconst struct nf_conntrack_tuple *tuple, u32 hash)\n{\n\tstruct nf_conntrack_tuple_hash *h;\n\tstruct nf_conn *ct;\n\n\th = ____nf_conntrack_find(net, zone, tuple, hash);\n\tif (h) {\n\t\t \n\t\tct = nf_ct_tuplehash_to_ctrack(h);\n\t\tif (likely(refcount_inc_not_zero(&ct->ct_general.use))) {\n\t\t\t \n\t\t\tsmp_acquire__after_ctrl_dep();\n\n\t\t\tif (likely(nf_ct_key_equal(h, tuple, zone, net)))\n\t\t\t\treturn h;\n\n\t\t\t \n\t\t\tnf_ct_put(ct);\n\t\t}\n\n\t\th = NULL;\n\t}\n\n\treturn h;\n}\n\nstruct nf_conntrack_tuple_hash *\nnf_conntrack_find_get(struct net *net, const struct nf_conntrack_zone *zone,\n\t\t      const struct nf_conntrack_tuple *tuple)\n{\n\tunsigned int rid, zone_id = nf_ct_zone_id(zone, IP_CT_DIR_ORIGINAL);\n\tstruct nf_conntrack_tuple_hash *thash;\n\n\trcu_read_lock();\n\n\tthash = __nf_conntrack_find_get(net, zone, tuple,\n\t\t\t\t\thash_conntrack_raw(tuple, zone_id, net));\n\n\tif (thash)\n\t\tgoto out_unlock;\n\n\trid = nf_ct_zone_id(zone, IP_CT_DIR_REPLY);\n\tif (rid != zone_id)\n\t\tthash = __nf_conntrack_find_get(net, zone, tuple,\n\t\t\t\t\t\thash_conntrack_raw(tuple, rid, net));\n\nout_unlock:\n\trcu_read_unlock();\n\treturn thash;\n}\nEXPORT_SYMBOL_GPL(nf_conntrack_find_get);\n\nstatic void __nf_conntrack_hash_insert(struct nf_conn *ct,\n\t\t\t\t       unsigned int hash,\n\t\t\t\t       unsigned int reply_hash)\n{\n\thlist_nulls_add_head_rcu(&ct->tuplehash[IP_CT_DIR_ORIGINAL].hnnode,\n\t\t\t   &nf_conntrack_hash[hash]);\n\thlist_nulls_add_head_rcu(&ct->tuplehash[IP_CT_DIR_REPLY].hnnode,\n\t\t\t   &nf_conntrack_hash[reply_hash]);\n}\n\nstatic bool nf_ct_ext_valid_pre(const struct nf_ct_ext *ext)\n{\n\t \n\treturn !ext || ext->gen_id == atomic_read(&nf_conntrack_ext_genid);\n}\n\nstatic bool nf_ct_ext_valid_post(struct nf_ct_ext *ext)\n{\n\tif (!ext)\n\t\treturn true;\n\n\tif (ext->gen_id != atomic_read(&nf_conntrack_ext_genid))\n\t\treturn false;\n\n\t \n\tWRITE_ONCE(ext->gen_id, 0);\n\treturn true;\n}\n\nint\nnf_conntrack_hash_check_insert(struct nf_conn *ct)\n{\n\tconst struct nf_conntrack_zone *zone;\n\tstruct net *net = nf_ct_net(ct);\n\tunsigned int hash, reply_hash;\n\tstruct nf_conntrack_tuple_hash *h;\n\tstruct hlist_nulls_node *n;\n\tunsigned int max_chainlen;\n\tunsigned int chainlen = 0;\n\tunsigned int sequence;\n\tint err = -EEXIST;\n\n\tzone = nf_ct_zone(ct);\n\n\tif (!nf_ct_ext_valid_pre(ct->ext))\n\t\treturn -EAGAIN;\n\n\tlocal_bh_disable();\n\tdo {\n\t\tsequence = read_seqcount_begin(&nf_conntrack_generation);\n\t\thash = hash_conntrack(net,\n\t\t\t\t      &ct->tuplehash[IP_CT_DIR_ORIGINAL].tuple,\n\t\t\t\t      nf_ct_zone_id(nf_ct_zone(ct), IP_CT_DIR_ORIGINAL));\n\t\treply_hash = hash_conntrack(net,\n\t\t\t\t\t   &ct->tuplehash[IP_CT_DIR_REPLY].tuple,\n\t\t\t\t\t   nf_ct_zone_id(nf_ct_zone(ct), IP_CT_DIR_REPLY));\n\t} while (nf_conntrack_double_lock(net, hash, reply_hash, sequence));\n\n\tmax_chainlen = MIN_CHAINLEN + get_random_u32_below(MAX_CHAINLEN);\n\n\t \n\thlist_nulls_for_each_entry(h, n, &nf_conntrack_hash[hash], hnnode) {\n\t\tif (nf_ct_key_equal(h, &ct->tuplehash[IP_CT_DIR_ORIGINAL].tuple,\n\t\t\t\t    zone, net))\n\t\t\tgoto out;\n\n\t\tif (chainlen++ > max_chainlen)\n\t\t\tgoto chaintoolong;\n\t}\n\n\tchainlen = 0;\n\n\thlist_nulls_for_each_entry(h, n, &nf_conntrack_hash[reply_hash], hnnode) {\n\t\tif (nf_ct_key_equal(h, &ct->tuplehash[IP_CT_DIR_REPLY].tuple,\n\t\t\t\t    zone, net))\n\t\t\tgoto out;\n\t\tif (chainlen++ > max_chainlen)\n\t\t\tgoto chaintoolong;\n\t}\n\n\t \n\tif (!nf_ct_ext_valid_post(ct->ext)) {\n\t\terr = -EAGAIN;\n\t\tgoto out;\n\t}\n\n\tsmp_wmb();\n\t \n\trefcount_set(&ct->ct_general.use, 2);\n\t__nf_conntrack_hash_insert(ct, hash, reply_hash);\n\tnf_conntrack_double_unlock(hash, reply_hash);\n\tNF_CT_STAT_INC(net, insert);\n\tlocal_bh_enable();\n\n\treturn 0;\nchaintoolong:\n\tNF_CT_STAT_INC(net, chaintoolong);\n\terr = -ENOSPC;\nout:\n\tnf_conntrack_double_unlock(hash, reply_hash);\n\tlocal_bh_enable();\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(nf_conntrack_hash_check_insert);\n\nvoid nf_ct_acct_add(struct nf_conn *ct, u32 dir, unsigned int packets,\n\t\t    unsigned int bytes)\n{\n\tstruct nf_conn_acct *acct;\n\n\tacct = nf_conn_acct_find(ct);\n\tif (acct) {\n\t\tstruct nf_conn_counter *counter = acct->counter;\n\n\t\tatomic64_add(packets, &counter[dir].packets);\n\t\tatomic64_add(bytes, &counter[dir].bytes);\n\t}\n}\nEXPORT_SYMBOL_GPL(nf_ct_acct_add);\n\nstatic void nf_ct_acct_merge(struct nf_conn *ct, enum ip_conntrack_info ctinfo,\n\t\t\t     const struct nf_conn *loser_ct)\n{\n\tstruct nf_conn_acct *acct;\n\n\tacct = nf_conn_acct_find(loser_ct);\n\tif (acct) {\n\t\tstruct nf_conn_counter *counter = acct->counter;\n\t\tunsigned int bytes;\n\n\t\t \n\t\tbytes = atomic64_read(&counter[CTINFO2DIR(ctinfo)].bytes);\n\t\tnf_ct_acct_update(ct, CTINFO2DIR(ctinfo), bytes);\n\t}\n}\n\nstatic void __nf_conntrack_insert_prepare(struct nf_conn *ct)\n{\n\tstruct nf_conn_tstamp *tstamp;\n\n\trefcount_inc(&ct->ct_general.use);\n\n\t \n\ttstamp = nf_conn_tstamp_find(ct);\n\tif (tstamp)\n\t\ttstamp->start = ktime_get_real_ns();\n}\n\n \nstatic int __nf_ct_resolve_clash(struct sk_buff *skb,\n\t\t\t\t struct nf_conntrack_tuple_hash *h)\n{\n\t \n\tstruct nf_conn *ct = nf_ct_tuplehash_to_ctrack(h);\n\tenum ip_conntrack_info ctinfo;\n\tstruct nf_conn *loser_ct;\n\n\tloser_ct = nf_ct_get(skb, &ctinfo);\n\n\tif (nf_ct_is_dying(ct))\n\t\treturn NF_DROP;\n\n\tif (((ct->status & IPS_NAT_DONE_MASK) == 0) ||\n\t    nf_ct_match(ct, loser_ct)) {\n\t\tstruct net *net = nf_ct_net(ct);\n\n\t\tnf_conntrack_get(&ct->ct_general);\n\n\t\tnf_ct_acct_merge(ct, ctinfo, loser_ct);\n\t\tnf_ct_put(loser_ct);\n\t\tnf_ct_set(skb, ct, ctinfo);\n\n\t\tNF_CT_STAT_INC(net, clash_resolve);\n\t\treturn NF_ACCEPT;\n\t}\n\n\treturn NF_DROP;\n}\n\n \nstatic int nf_ct_resolve_clash_harder(struct sk_buff *skb, u32 repl_idx)\n{\n\tstruct nf_conn *loser_ct = (struct nf_conn *)skb_nfct(skb);\n\tconst struct nf_conntrack_zone *zone;\n\tstruct nf_conntrack_tuple_hash *h;\n\tstruct hlist_nulls_node *n;\n\tstruct net *net;\n\n\tzone = nf_ct_zone(loser_ct);\n\tnet = nf_ct_net(loser_ct);\n\n\t \n\thlist_nulls_for_each_entry(h, n, &nf_conntrack_hash[repl_idx], hnnode) {\n\t\tif (nf_ct_key_equal(h,\n\t\t\t\t    &loser_ct->tuplehash[IP_CT_DIR_REPLY].tuple,\n\t\t\t\t    zone, net))\n\t\t\treturn __nf_ct_resolve_clash(skb, h);\n\t}\n\n\t \n\tWRITE_ONCE(loser_ct->timeout, nfct_time_stamp + HZ);\n\n\t \n\tloser_ct->status |= IPS_FIXED_TIMEOUT | IPS_NAT_CLASH;\n\n\t__nf_conntrack_insert_prepare(loser_ct);\n\n\t \n\thlist_nulls_add_fake(&loser_ct->tuplehash[IP_CT_DIR_ORIGINAL].hnnode);\n\n\thlist_nulls_add_head_rcu(&loser_ct->tuplehash[IP_CT_DIR_REPLY].hnnode,\n\t\t\t\t &nf_conntrack_hash[repl_idx]);\n\n\tNF_CT_STAT_INC(net, clash_resolve);\n\treturn NF_ACCEPT;\n}\n\n \nstatic __cold noinline int\nnf_ct_resolve_clash(struct sk_buff *skb, struct nf_conntrack_tuple_hash *h,\n\t\t    u32 reply_hash)\n{\n\t \n\tstruct nf_conn *ct = nf_ct_tuplehash_to_ctrack(h);\n\tconst struct nf_conntrack_l4proto *l4proto;\n\tenum ip_conntrack_info ctinfo;\n\tstruct nf_conn *loser_ct;\n\tstruct net *net;\n\tint ret;\n\n\tloser_ct = nf_ct_get(skb, &ctinfo);\n\tnet = nf_ct_net(loser_ct);\n\n\tl4proto = nf_ct_l4proto_find(nf_ct_protonum(ct));\n\tif (!l4proto->allow_clash)\n\t\tgoto drop;\n\n\tret = __nf_ct_resolve_clash(skb, h);\n\tif (ret == NF_ACCEPT)\n\t\treturn ret;\n\n\tret = nf_ct_resolve_clash_harder(skb, reply_hash);\n\tif (ret == NF_ACCEPT)\n\t\treturn ret;\n\ndrop:\n\tNF_CT_STAT_INC(net, drop);\n\tNF_CT_STAT_INC(net, insert_failed);\n\treturn NF_DROP;\n}\n\n \nint\n__nf_conntrack_confirm(struct sk_buff *skb)\n{\n\tunsigned int chainlen = 0, sequence, max_chainlen;\n\tconst struct nf_conntrack_zone *zone;\n\tunsigned int hash, reply_hash;\n\tstruct nf_conntrack_tuple_hash *h;\n\tstruct nf_conn *ct;\n\tstruct nf_conn_help *help;\n\tstruct hlist_nulls_node *n;\n\tenum ip_conntrack_info ctinfo;\n\tstruct net *net;\n\tint ret = NF_DROP;\n\n\tct = nf_ct_get(skb, &ctinfo);\n\tnet = nf_ct_net(ct);\n\n\t \n\tif (CTINFO2DIR(ctinfo) != IP_CT_DIR_ORIGINAL)\n\t\treturn NF_ACCEPT;\n\n\tzone = nf_ct_zone(ct);\n\tlocal_bh_disable();\n\n\tdo {\n\t\tsequence = read_seqcount_begin(&nf_conntrack_generation);\n\t\t \n\t\thash = *(unsigned long *)&ct->tuplehash[IP_CT_DIR_REPLY].hnnode.pprev;\n\t\thash = scale_hash(hash);\n\t\treply_hash = hash_conntrack(net,\n\t\t\t\t\t   &ct->tuplehash[IP_CT_DIR_REPLY].tuple,\n\t\t\t\t\t   nf_ct_zone_id(nf_ct_zone(ct), IP_CT_DIR_REPLY));\n\t} while (nf_conntrack_double_lock(net, hash, reply_hash, sequence));\n\n\t \n\n\t \n\tif (unlikely(nf_ct_is_confirmed(ct))) {\n\t\tWARN_ON_ONCE(1);\n\t\tnf_conntrack_double_unlock(hash, reply_hash);\n\t\tlocal_bh_enable();\n\t\treturn NF_DROP;\n\t}\n\n\tif (!nf_ct_ext_valid_pre(ct->ext)) {\n\t\tNF_CT_STAT_INC(net, insert_failed);\n\t\tgoto dying;\n\t}\n\n\t \n\tct->status |= IPS_CONFIRMED;\n\n\tif (unlikely(nf_ct_is_dying(ct))) {\n\t\tNF_CT_STAT_INC(net, insert_failed);\n\t\tgoto dying;\n\t}\n\n\tmax_chainlen = MIN_CHAINLEN + get_random_u32_below(MAX_CHAINLEN);\n\t \n\thlist_nulls_for_each_entry(h, n, &nf_conntrack_hash[hash], hnnode) {\n\t\tif (nf_ct_key_equal(h, &ct->tuplehash[IP_CT_DIR_ORIGINAL].tuple,\n\t\t\t\t    zone, net))\n\t\t\tgoto out;\n\t\tif (chainlen++ > max_chainlen)\n\t\t\tgoto chaintoolong;\n\t}\n\n\tchainlen = 0;\n\thlist_nulls_for_each_entry(h, n, &nf_conntrack_hash[reply_hash], hnnode) {\n\t\tif (nf_ct_key_equal(h, &ct->tuplehash[IP_CT_DIR_REPLY].tuple,\n\t\t\t\t    zone, net))\n\t\t\tgoto out;\n\t\tif (chainlen++ > max_chainlen) {\nchaintoolong:\n\t\t\tNF_CT_STAT_INC(net, chaintoolong);\n\t\t\tNF_CT_STAT_INC(net, insert_failed);\n\t\t\tret = NF_DROP;\n\t\t\tgoto dying;\n\t\t}\n\t}\n\n\t \n\tct->timeout += nfct_time_stamp;\n\n\t__nf_conntrack_insert_prepare(ct);\n\n\t \n\t__nf_conntrack_hash_insert(ct, hash, reply_hash);\n\tnf_conntrack_double_unlock(hash, reply_hash);\n\tlocal_bh_enable();\n\n\t \n\tif (!nf_ct_ext_valid_post(ct->ext)) {\n\t\tnf_ct_kill(ct);\n\t\tNF_CT_STAT_INC_ATOMIC(net, drop);\n\t\treturn NF_DROP;\n\t}\n\n\thelp = nfct_help(ct);\n\tif (help && help->helper)\n\t\tnf_conntrack_event_cache(IPCT_HELPER, ct);\n\n\tnf_conntrack_event_cache(master_ct(ct) ?\n\t\t\t\t IPCT_RELATED : IPCT_NEW, ct);\n\treturn NF_ACCEPT;\n\nout:\n\tret = nf_ct_resolve_clash(skb, h, reply_hash);\ndying:\n\tnf_conntrack_double_unlock(hash, reply_hash);\n\tlocal_bh_enable();\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(__nf_conntrack_confirm);\n\n \nint\nnf_conntrack_tuple_taken(const struct nf_conntrack_tuple *tuple,\n\t\t\t const struct nf_conn *ignored_conntrack)\n{\n\tstruct net *net = nf_ct_net(ignored_conntrack);\n\tconst struct nf_conntrack_zone *zone;\n\tstruct nf_conntrack_tuple_hash *h;\n\tstruct hlist_nulls_head *ct_hash;\n\tunsigned int hash, hsize;\n\tstruct hlist_nulls_node *n;\n\tstruct nf_conn *ct;\n\n\tzone = nf_ct_zone(ignored_conntrack);\n\n\trcu_read_lock();\n begin:\n\tnf_conntrack_get_ht(&ct_hash, &hsize);\n\thash = __hash_conntrack(net, tuple, nf_ct_zone_id(zone, IP_CT_DIR_REPLY), hsize);\n\n\thlist_nulls_for_each_entry_rcu(h, n, &ct_hash[hash], hnnode) {\n\t\tct = nf_ct_tuplehash_to_ctrack(h);\n\n\t\tif (ct == ignored_conntrack)\n\t\t\tcontinue;\n\n\t\tif (nf_ct_is_expired(ct)) {\n\t\t\tnf_ct_gc_expired(ct);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (nf_ct_key_equal(h, tuple, zone, net)) {\n\t\t\t \n\t\t\tif (nf_ct_tuple_equal(&ignored_conntrack->tuplehash[IP_CT_DIR_ORIGINAL].tuple,\n\t\t\t\t\t      &ct->tuplehash[IP_CT_DIR_ORIGINAL].tuple) &&\n\t\t\t\t\t      nf_ct_zone_equal(ct, zone, IP_CT_DIR_ORIGINAL))\n\t\t\t\tcontinue;\n\n\t\t\tNF_CT_STAT_INC_ATOMIC(net, found);\n\t\t\trcu_read_unlock();\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\tif (get_nulls_value(n) != hash) {\n\t\tNF_CT_STAT_INC_ATOMIC(net, search_restart);\n\t\tgoto begin;\n\t}\n\n\trcu_read_unlock();\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(nf_conntrack_tuple_taken);\n\n#define NF_CT_EVICTION_RANGE\t8\n\n \nstatic unsigned int early_drop_list(struct net *net,\n\t\t\t\t    struct hlist_nulls_head *head)\n{\n\tstruct nf_conntrack_tuple_hash *h;\n\tstruct hlist_nulls_node *n;\n\tunsigned int drops = 0;\n\tstruct nf_conn *tmp;\n\n\thlist_nulls_for_each_entry_rcu(h, n, head, hnnode) {\n\t\ttmp = nf_ct_tuplehash_to_ctrack(h);\n\n\t\tif (nf_ct_is_expired(tmp)) {\n\t\t\tnf_ct_gc_expired(tmp);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (test_bit(IPS_ASSURED_BIT, &tmp->status) ||\n\t\t    !net_eq(nf_ct_net(tmp), net) ||\n\t\t    nf_ct_is_dying(tmp))\n\t\t\tcontinue;\n\n\t\tif (!refcount_inc_not_zero(&tmp->ct_general.use))\n\t\t\tcontinue;\n\n\t\t \n\t\tsmp_acquire__after_ctrl_dep();\n\n\t\t \n\t\tif (net_eq(nf_ct_net(tmp), net) &&\n\t\t    nf_ct_is_confirmed(tmp) &&\n\t\t    nf_ct_delete(tmp, 0, 0))\n\t\t\tdrops++;\n\n\t\tnf_ct_put(tmp);\n\t}\n\n\treturn drops;\n}\n\nstatic noinline int early_drop(struct net *net, unsigned int hash)\n{\n\tunsigned int i, bucket;\n\n\tfor (i = 0; i < NF_CT_EVICTION_RANGE; i++) {\n\t\tstruct hlist_nulls_head *ct_hash;\n\t\tunsigned int hsize, drops;\n\n\t\trcu_read_lock();\n\t\tnf_conntrack_get_ht(&ct_hash, &hsize);\n\t\tif (!i)\n\t\t\tbucket = reciprocal_scale(hash, hsize);\n\t\telse\n\t\t\tbucket = (bucket + 1) % hsize;\n\n\t\tdrops = early_drop_list(net, &ct_hash[bucket]);\n\t\trcu_read_unlock();\n\n\t\tif (drops) {\n\t\t\tNF_CT_STAT_ADD_ATOMIC(net, early_drop, drops);\n\t\t\treturn true;\n\t\t}\n\t}\n\n\treturn false;\n}\n\nstatic bool gc_worker_skip_ct(const struct nf_conn *ct)\n{\n\treturn !nf_ct_is_confirmed(ct) || nf_ct_is_dying(ct);\n}\n\nstatic bool gc_worker_can_early_drop(const struct nf_conn *ct)\n{\n\tconst struct nf_conntrack_l4proto *l4proto;\n\tu8 protonum = nf_ct_protonum(ct);\n\n\tif (test_bit(IPS_OFFLOAD_BIT, &ct->status) && protonum != IPPROTO_UDP)\n\t\treturn false;\n\tif (!test_bit(IPS_ASSURED_BIT, &ct->status))\n\t\treturn true;\n\n\tl4proto = nf_ct_l4proto_find(protonum);\n\tif (l4proto->can_early_drop && l4proto->can_early_drop(ct))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic void gc_worker(struct work_struct *work)\n{\n\tunsigned int i, hashsz, nf_conntrack_max95 = 0;\n\tu32 end_time, start_time = nfct_time_stamp;\n\tstruct conntrack_gc_work *gc_work;\n\tunsigned int expired_count = 0;\n\tunsigned long next_run;\n\ts32 delta_time;\n\tlong count;\n\n\tgc_work = container_of(work, struct conntrack_gc_work, dwork.work);\n\n\ti = gc_work->next_bucket;\n\tif (gc_work->early_drop)\n\t\tnf_conntrack_max95 = nf_conntrack_max / 100u * 95u;\n\n\tif (i == 0) {\n\t\tgc_work->avg_timeout = GC_SCAN_INTERVAL_INIT;\n\t\tgc_work->count = GC_SCAN_INITIAL_COUNT;\n\t\tgc_work->start_time = start_time;\n\t}\n\n\tnext_run = gc_work->avg_timeout;\n\tcount = gc_work->count;\n\n\tend_time = start_time + GC_SCAN_MAX_DURATION;\n\n\tdo {\n\t\tstruct nf_conntrack_tuple_hash *h;\n\t\tstruct hlist_nulls_head *ct_hash;\n\t\tstruct hlist_nulls_node *n;\n\t\tstruct nf_conn *tmp;\n\n\t\trcu_read_lock();\n\n\t\tnf_conntrack_get_ht(&ct_hash, &hashsz);\n\t\tif (i >= hashsz) {\n\t\t\trcu_read_unlock();\n\t\t\tbreak;\n\t\t}\n\n\t\thlist_nulls_for_each_entry_rcu(h, n, &ct_hash[i], hnnode) {\n\t\t\tstruct nf_conntrack_net *cnet;\n\t\t\tstruct net *net;\n\t\t\tlong expires;\n\n\t\t\ttmp = nf_ct_tuplehash_to_ctrack(h);\n\n\t\t\tif (test_bit(IPS_OFFLOAD_BIT, &tmp->status)) {\n\t\t\t\tnf_ct_offload_timeout(tmp);\n\t\t\t\tif (!nf_conntrack_max95)\n\t\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (expired_count > GC_SCAN_EXPIRED_MAX) {\n\t\t\t\trcu_read_unlock();\n\n\t\t\t\tgc_work->next_bucket = i;\n\t\t\t\tgc_work->avg_timeout = next_run;\n\t\t\t\tgc_work->count = count;\n\n\t\t\t\tdelta_time = nfct_time_stamp - gc_work->start_time;\n\n\t\t\t\t \n\t\t\t\tnext_run = delta_time < (s32)GC_SCAN_INTERVAL_MAX;\n\t\t\t\tgoto early_exit;\n\t\t\t}\n\n\t\t\tif (nf_ct_is_expired(tmp)) {\n\t\t\t\tnf_ct_gc_expired(tmp);\n\t\t\t\texpired_count++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\texpires = clamp(nf_ct_expires(tmp), GC_SCAN_INTERVAL_MIN, GC_SCAN_INTERVAL_CLAMP);\n\t\t\texpires = (expires - (long)next_run) / ++count;\n\t\t\tnext_run += expires;\n\n\t\t\tif (nf_conntrack_max95 == 0 || gc_worker_skip_ct(tmp))\n\t\t\t\tcontinue;\n\n\t\t\tnet = nf_ct_net(tmp);\n\t\t\tcnet = nf_ct_pernet(net);\n\t\t\tif (atomic_read(&cnet->count) < nf_conntrack_max95)\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tif (!refcount_inc_not_zero(&tmp->ct_general.use))\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tsmp_acquire__after_ctrl_dep();\n\n\t\t\tif (gc_worker_skip_ct(tmp)) {\n\t\t\t\tnf_ct_put(tmp);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (gc_worker_can_early_drop(tmp)) {\n\t\t\t\tnf_ct_kill(tmp);\n\t\t\t\texpired_count++;\n\t\t\t}\n\n\t\t\tnf_ct_put(tmp);\n\t\t}\n\n\t\t \n\t\trcu_read_unlock();\n\t\tcond_resched();\n\t\ti++;\n\n\t\tdelta_time = nfct_time_stamp - end_time;\n\t\tif (delta_time > 0 && i < hashsz) {\n\t\t\tgc_work->avg_timeout = next_run;\n\t\t\tgc_work->count = count;\n\t\t\tgc_work->next_bucket = i;\n\t\t\tnext_run = 0;\n\t\t\tgoto early_exit;\n\t\t}\n\t} while (i < hashsz);\n\n\tgc_work->next_bucket = 0;\n\n\tnext_run = clamp(next_run, GC_SCAN_INTERVAL_MIN, GC_SCAN_INTERVAL_MAX);\n\n\tdelta_time = max_t(s32, nfct_time_stamp - gc_work->start_time, 1);\n\tif (next_run > (unsigned long)delta_time)\n\t\tnext_run -= delta_time;\n\telse\n\t\tnext_run = 1;\n\nearly_exit:\n\tif (gc_work->exiting)\n\t\treturn;\n\n\tif (next_run)\n\t\tgc_work->early_drop = false;\n\n\tqueue_delayed_work(system_power_efficient_wq, &gc_work->dwork, next_run);\n}\n\nstatic void conntrack_gc_work_init(struct conntrack_gc_work *gc_work)\n{\n\tINIT_DELAYED_WORK(&gc_work->dwork, gc_worker);\n\tgc_work->exiting = false;\n}\n\nstatic struct nf_conn *\n__nf_conntrack_alloc(struct net *net,\n\t\t     const struct nf_conntrack_zone *zone,\n\t\t     const struct nf_conntrack_tuple *orig,\n\t\t     const struct nf_conntrack_tuple *repl,\n\t\t     gfp_t gfp, u32 hash)\n{\n\tstruct nf_conntrack_net *cnet = nf_ct_pernet(net);\n\tunsigned int ct_count;\n\tstruct nf_conn *ct;\n\n\t \n\tct_count = atomic_inc_return(&cnet->count);\n\n\tif (nf_conntrack_max && unlikely(ct_count > nf_conntrack_max)) {\n\t\tif (!early_drop(net, hash)) {\n\t\t\tif (!conntrack_gc_work.early_drop)\n\t\t\t\tconntrack_gc_work.early_drop = true;\n\t\t\tatomic_dec(&cnet->count);\n\t\t\tnet_warn_ratelimited(\"nf_conntrack: table full, dropping packet\\n\");\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t}\n\n\t \n\tct = kmem_cache_alloc(nf_conntrack_cachep, gfp);\n\tif (ct == NULL)\n\t\tgoto out;\n\n\tspin_lock_init(&ct->lock);\n\tct->tuplehash[IP_CT_DIR_ORIGINAL].tuple = *orig;\n\tct->tuplehash[IP_CT_DIR_ORIGINAL].hnnode.pprev = NULL;\n\tct->tuplehash[IP_CT_DIR_REPLY].tuple = *repl;\n\t \n\t*(unsigned long *)(&ct->tuplehash[IP_CT_DIR_REPLY].hnnode.pprev) = hash;\n\tct->status = 0;\n\tWRITE_ONCE(ct->timeout, 0);\n\twrite_pnet(&ct->ct_net, net);\n\tmemset_after(ct, 0, __nfct_init_offset);\n\n\tnf_ct_zone_add(ct, zone);\n\n\t \n\trefcount_set(&ct->ct_general.use, 0);\n\treturn ct;\nout:\n\tatomic_dec(&cnet->count);\n\treturn ERR_PTR(-ENOMEM);\n}\n\nstruct nf_conn *nf_conntrack_alloc(struct net *net,\n\t\t\t\t   const struct nf_conntrack_zone *zone,\n\t\t\t\t   const struct nf_conntrack_tuple *orig,\n\t\t\t\t   const struct nf_conntrack_tuple *repl,\n\t\t\t\t   gfp_t gfp)\n{\n\treturn __nf_conntrack_alloc(net, zone, orig, repl, gfp, 0);\n}\nEXPORT_SYMBOL_GPL(nf_conntrack_alloc);\n\nvoid nf_conntrack_free(struct nf_conn *ct)\n{\n\tstruct net *net = nf_ct_net(ct);\n\tstruct nf_conntrack_net *cnet;\n\n\t \n\tWARN_ON(refcount_read(&ct->ct_general.use) != 0);\n\n\tif (ct->status & IPS_SRC_NAT_DONE) {\n\t\tconst struct nf_nat_hook *nat_hook;\n\n\t\trcu_read_lock();\n\t\tnat_hook = rcu_dereference(nf_nat_hook);\n\t\tif (nat_hook)\n\t\t\tnat_hook->remove_nat_bysrc(ct);\n\t\trcu_read_unlock();\n\t}\n\n\tkfree(ct->ext);\n\tkmem_cache_free(nf_conntrack_cachep, ct);\n\tcnet = nf_ct_pernet(net);\n\n\tsmp_mb__before_atomic();\n\tatomic_dec(&cnet->count);\n}\nEXPORT_SYMBOL_GPL(nf_conntrack_free);\n\n\n \nstatic noinline struct nf_conntrack_tuple_hash *\ninit_conntrack(struct net *net, struct nf_conn *tmpl,\n\t       const struct nf_conntrack_tuple *tuple,\n\t       struct sk_buff *skb,\n\t       unsigned int dataoff, u32 hash)\n{\n\tstruct nf_conn *ct;\n\tstruct nf_conn_help *help;\n\tstruct nf_conntrack_tuple repl_tuple;\n#ifdef CONFIG_NF_CONNTRACK_EVENTS\n\tstruct nf_conntrack_ecache *ecache;\n#endif\n\tstruct nf_conntrack_expect *exp = NULL;\n\tconst struct nf_conntrack_zone *zone;\n\tstruct nf_conn_timeout *timeout_ext;\n\tstruct nf_conntrack_zone tmp;\n\tstruct nf_conntrack_net *cnet;\n\n\tif (!nf_ct_invert_tuple(&repl_tuple, tuple))\n\t\treturn NULL;\n\n\tzone = nf_ct_zone_tmpl(tmpl, skb, &tmp);\n\tct = __nf_conntrack_alloc(net, zone, tuple, &repl_tuple, GFP_ATOMIC,\n\t\t\t\t  hash);\n\tif (IS_ERR(ct))\n\t\treturn (struct nf_conntrack_tuple_hash *)ct;\n\n\tif (!nf_ct_add_synproxy(ct, tmpl)) {\n\t\tnf_conntrack_free(ct);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\ttimeout_ext = tmpl ? nf_ct_timeout_find(tmpl) : NULL;\n\n\tif (timeout_ext)\n\t\tnf_ct_timeout_ext_add(ct, rcu_dereference(timeout_ext->timeout),\n\t\t\t\t      GFP_ATOMIC);\n\n\tnf_ct_acct_ext_add(ct, GFP_ATOMIC);\n\tnf_ct_tstamp_ext_add(ct, GFP_ATOMIC);\n\tnf_ct_labels_ext_add(ct);\n\n#ifdef CONFIG_NF_CONNTRACK_EVENTS\n\tecache = tmpl ? nf_ct_ecache_find(tmpl) : NULL;\n\n\tif ((ecache || net->ct.sysctl_events) &&\n\t    !nf_ct_ecache_ext_add(ct, ecache ? ecache->ctmask : 0,\n\t\t\t\t  ecache ? ecache->expmask : 0,\n\t\t\t\t  GFP_ATOMIC)) {\n\t\tnf_conntrack_free(ct);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n#endif\n\n\tcnet = nf_ct_pernet(net);\n\tif (cnet->expect_count) {\n\t\tspin_lock_bh(&nf_conntrack_expect_lock);\n\t\texp = nf_ct_find_expectation(net, zone, tuple, !tmpl || nf_ct_is_confirmed(tmpl));\n\t\tif (exp) {\n\t\t\t \n\t\t\t__set_bit(IPS_EXPECTED_BIT, &ct->status);\n\t\t\t \n\t\t\tct->master = exp->master;\n\t\t\tif (exp->helper) {\n\t\t\t\thelp = nf_ct_helper_ext_add(ct, GFP_ATOMIC);\n\t\t\t\tif (help)\n\t\t\t\t\trcu_assign_pointer(help->helper, exp->helper);\n\t\t\t}\n\n#ifdef CONFIG_NF_CONNTRACK_MARK\n\t\t\tct->mark = READ_ONCE(exp->master->mark);\n#endif\n#ifdef CONFIG_NF_CONNTRACK_SECMARK\n\t\t\tct->secmark = exp->master->secmark;\n#endif\n\t\t\tNF_CT_STAT_INC(net, expect_new);\n\t\t}\n\t\tspin_unlock_bh(&nf_conntrack_expect_lock);\n\t}\n\tif (!exp && tmpl)\n\t\t__nf_ct_try_assign_helper(ct, tmpl, GFP_ATOMIC);\n\n\t \n\tsmp_wmb();\n\n\t \n\trefcount_set(&ct->ct_general.use, 1);\n\n\tif (exp) {\n\t\tif (exp->expectfn)\n\t\t\texp->expectfn(ct, exp);\n\t\tnf_ct_expect_put(exp);\n\t}\n\n\treturn &ct->tuplehash[IP_CT_DIR_ORIGINAL];\n}\n\n \nstatic int\nresolve_normal_ct(struct nf_conn *tmpl,\n\t\t  struct sk_buff *skb,\n\t\t  unsigned int dataoff,\n\t\t  u_int8_t protonum,\n\t\t  const struct nf_hook_state *state)\n{\n\tconst struct nf_conntrack_zone *zone;\n\tstruct nf_conntrack_tuple tuple;\n\tstruct nf_conntrack_tuple_hash *h;\n\tenum ip_conntrack_info ctinfo;\n\tstruct nf_conntrack_zone tmp;\n\tu32 hash, zone_id, rid;\n\tstruct nf_conn *ct;\n\n\tif (!nf_ct_get_tuple(skb, skb_network_offset(skb),\n\t\t\t     dataoff, state->pf, protonum, state->net,\n\t\t\t     &tuple))\n\t\treturn 0;\n\n\t \n\tzone = nf_ct_zone_tmpl(tmpl, skb, &tmp);\n\n\tzone_id = nf_ct_zone_id(zone, IP_CT_DIR_ORIGINAL);\n\thash = hash_conntrack_raw(&tuple, zone_id, state->net);\n\th = __nf_conntrack_find_get(state->net, zone, &tuple, hash);\n\n\tif (!h) {\n\t\trid = nf_ct_zone_id(zone, IP_CT_DIR_REPLY);\n\t\tif (zone_id != rid) {\n\t\t\tu32 tmp = hash_conntrack_raw(&tuple, rid, state->net);\n\n\t\t\th = __nf_conntrack_find_get(state->net, zone, &tuple, tmp);\n\t\t}\n\t}\n\n\tif (!h) {\n\t\th = init_conntrack(state->net, tmpl, &tuple,\n\t\t\t\t   skb, dataoff, hash);\n\t\tif (!h)\n\t\t\treturn 0;\n\t\tif (IS_ERR(h))\n\t\t\treturn PTR_ERR(h);\n\t}\n\tct = nf_ct_tuplehash_to_ctrack(h);\n\n\t \n\tif (NF_CT_DIRECTION(h) == IP_CT_DIR_REPLY) {\n\t\tctinfo = IP_CT_ESTABLISHED_REPLY;\n\t} else {\n\t\tunsigned long status = READ_ONCE(ct->status);\n\n\t\t \n\t\tif (likely(status & IPS_SEEN_REPLY))\n\t\t\tctinfo = IP_CT_ESTABLISHED;\n\t\telse if (status & IPS_EXPECTED)\n\t\t\tctinfo = IP_CT_RELATED;\n\t\telse\n\t\t\tctinfo = IP_CT_NEW;\n\t}\n\tnf_ct_set(skb, ct, ctinfo);\n\treturn 0;\n}\n\n \nstatic unsigned int __cold\nnf_conntrack_handle_icmp(struct nf_conn *tmpl,\n\t\t\t struct sk_buff *skb,\n\t\t\t unsigned int dataoff,\n\t\t\t u8 protonum,\n\t\t\t const struct nf_hook_state *state)\n{\n\tint ret;\n\n\tif (state->pf == NFPROTO_IPV4 && protonum == IPPROTO_ICMP)\n\t\tret = nf_conntrack_icmpv4_error(tmpl, skb, dataoff, state);\n#if IS_ENABLED(CONFIG_IPV6)\n\telse if (state->pf == NFPROTO_IPV6 && protonum == IPPROTO_ICMPV6)\n\t\tret = nf_conntrack_icmpv6_error(tmpl, skb, dataoff, state);\n#endif\n\telse\n\t\treturn NF_ACCEPT;\n\n\tif (ret <= 0)\n\t\tNF_CT_STAT_INC_ATOMIC(state->net, error);\n\n\treturn ret;\n}\n\nstatic int generic_packet(struct nf_conn *ct, struct sk_buff *skb,\n\t\t\t  enum ip_conntrack_info ctinfo)\n{\n\tconst unsigned int *timeout = nf_ct_timeout_lookup(ct);\n\n\tif (!timeout)\n\t\ttimeout = &nf_generic_pernet(nf_ct_net(ct))->timeout;\n\n\tnf_ct_refresh_acct(ct, ctinfo, skb, *timeout);\n\treturn NF_ACCEPT;\n}\n\n \nstatic int nf_conntrack_handle_packet(struct nf_conn *ct,\n\t\t\t\t      struct sk_buff *skb,\n\t\t\t\t      unsigned int dataoff,\n\t\t\t\t      enum ip_conntrack_info ctinfo,\n\t\t\t\t      const struct nf_hook_state *state)\n{\n\tswitch (nf_ct_protonum(ct)) {\n\tcase IPPROTO_TCP:\n\t\treturn nf_conntrack_tcp_packet(ct, skb, dataoff,\n\t\t\t\t\t       ctinfo, state);\n\tcase IPPROTO_UDP:\n\t\treturn nf_conntrack_udp_packet(ct, skb, dataoff,\n\t\t\t\t\t       ctinfo, state);\n\tcase IPPROTO_ICMP:\n\t\treturn nf_conntrack_icmp_packet(ct, skb, ctinfo, state);\n#if IS_ENABLED(CONFIG_IPV6)\n\tcase IPPROTO_ICMPV6:\n\t\treturn nf_conntrack_icmpv6_packet(ct, skb, ctinfo, state);\n#endif\n#ifdef CONFIG_NF_CT_PROTO_UDPLITE\n\tcase IPPROTO_UDPLITE:\n\t\treturn nf_conntrack_udplite_packet(ct, skb, dataoff,\n\t\t\t\t\t\t   ctinfo, state);\n#endif\n#ifdef CONFIG_NF_CT_PROTO_SCTP\n\tcase IPPROTO_SCTP:\n\t\treturn nf_conntrack_sctp_packet(ct, skb, dataoff,\n\t\t\t\t\t\tctinfo, state);\n#endif\n#ifdef CONFIG_NF_CT_PROTO_DCCP\n\tcase IPPROTO_DCCP:\n\t\treturn nf_conntrack_dccp_packet(ct, skb, dataoff,\n\t\t\t\t\t\tctinfo, state);\n#endif\n#ifdef CONFIG_NF_CT_PROTO_GRE\n\tcase IPPROTO_GRE:\n\t\treturn nf_conntrack_gre_packet(ct, skb, dataoff,\n\t\t\t\t\t       ctinfo, state);\n#endif\n\t}\n\n\treturn generic_packet(ct, skb, ctinfo);\n}\n\nunsigned int\nnf_conntrack_in(struct sk_buff *skb, const struct nf_hook_state *state)\n{\n\tenum ip_conntrack_info ctinfo;\n\tstruct nf_conn *ct, *tmpl;\n\tu_int8_t protonum;\n\tint dataoff, ret;\n\n\ttmpl = nf_ct_get(skb, &ctinfo);\n\tif (tmpl || ctinfo == IP_CT_UNTRACKED) {\n\t\t \n\t\tif ((tmpl && !nf_ct_is_template(tmpl)) ||\n\t\t     ctinfo == IP_CT_UNTRACKED)\n\t\t\treturn NF_ACCEPT;\n\t\tskb->_nfct = 0;\n\t}\n\n\t \n\tdataoff = get_l4proto(skb, skb_network_offset(skb), state->pf, &protonum);\n\tif (dataoff <= 0) {\n\t\tNF_CT_STAT_INC_ATOMIC(state->net, invalid);\n\t\tret = NF_ACCEPT;\n\t\tgoto out;\n\t}\n\n\tif (protonum == IPPROTO_ICMP || protonum == IPPROTO_ICMPV6) {\n\t\tret = nf_conntrack_handle_icmp(tmpl, skb, dataoff,\n\t\t\t\t\t       protonum, state);\n\t\tif (ret <= 0) {\n\t\t\tret = -ret;\n\t\t\tgoto out;\n\t\t}\n\t\t \n\t\tif (skb->_nfct)\n\t\t\tgoto out;\n\t}\nrepeat:\n\tret = resolve_normal_ct(tmpl, skb, dataoff,\n\t\t\t\tprotonum, state);\n\tif (ret < 0) {\n\t\t \n\t\tNF_CT_STAT_INC_ATOMIC(state->net, drop);\n\t\tret = NF_DROP;\n\t\tgoto out;\n\t}\n\n\tct = nf_ct_get(skb, &ctinfo);\n\tif (!ct) {\n\t\t \n\t\tNF_CT_STAT_INC_ATOMIC(state->net, invalid);\n\t\tret = NF_ACCEPT;\n\t\tgoto out;\n\t}\n\n\tret = nf_conntrack_handle_packet(ct, skb, dataoff, ctinfo, state);\n\tif (ret <= 0) {\n\t\t \n\t\tnf_ct_put(ct);\n\t\tskb->_nfct = 0;\n\t\t \n\t\tif (ret == -NF_REPEAT)\n\t\t\tgoto repeat;\n\n\t\tNF_CT_STAT_INC_ATOMIC(state->net, invalid);\n\t\tif (ret == -NF_DROP)\n\t\t\tNF_CT_STAT_INC_ATOMIC(state->net, drop);\n\n\t\tret = -ret;\n\t\tgoto out;\n\t}\n\n\tif (ctinfo == IP_CT_ESTABLISHED_REPLY &&\n\t    !test_and_set_bit(IPS_SEEN_REPLY_BIT, &ct->status))\n\t\tnf_conntrack_event_cache(IPCT_REPLY, ct);\nout:\n\tif (tmpl)\n\t\tnf_ct_put(tmpl);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(nf_conntrack_in);\n\n \nvoid nf_conntrack_alter_reply(struct nf_conn *ct,\n\t\t\t      const struct nf_conntrack_tuple *newreply)\n{\n\tstruct nf_conn_help *help = nfct_help(ct);\n\n\t \n\tWARN_ON(nf_ct_is_confirmed(ct));\n\n\tnf_ct_dump_tuple(newreply);\n\n\tct->tuplehash[IP_CT_DIR_REPLY].tuple = *newreply;\n\tif (ct->master || (help && !hlist_empty(&help->expectations)))\n\t\treturn;\n}\nEXPORT_SYMBOL_GPL(nf_conntrack_alter_reply);\n\n \nvoid __nf_ct_refresh_acct(struct nf_conn *ct,\n\t\t\t  enum ip_conntrack_info ctinfo,\n\t\t\t  const struct sk_buff *skb,\n\t\t\t  u32 extra_jiffies,\n\t\t\t  bool do_acct)\n{\n\t \n\tif (test_bit(IPS_FIXED_TIMEOUT_BIT, &ct->status))\n\t\tgoto acct;\n\n\t \n\tif (nf_ct_is_confirmed(ct))\n\t\textra_jiffies += nfct_time_stamp;\n\n\tif (READ_ONCE(ct->timeout) != extra_jiffies)\n\t\tWRITE_ONCE(ct->timeout, extra_jiffies);\nacct:\n\tif (do_acct)\n\t\tnf_ct_acct_update(ct, CTINFO2DIR(ctinfo), skb->len);\n}\nEXPORT_SYMBOL_GPL(__nf_ct_refresh_acct);\n\nbool nf_ct_kill_acct(struct nf_conn *ct,\n\t\t     enum ip_conntrack_info ctinfo,\n\t\t     const struct sk_buff *skb)\n{\n\tnf_ct_acct_update(ct, CTINFO2DIR(ctinfo), skb->len);\n\n\treturn nf_ct_delete(ct, 0, 0);\n}\nEXPORT_SYMBOL_GPL(nf_ct_kill_acct);\n\n#if IS_ENABLED(CONFIG_NF_CT_NETLINK)\n\n#include <linux/netfilter/nfnetlink.h>\n#include <linux/netfilter/nfnetlink_conntrack.h>\n#include <linux/mutex.h>\n\n \nint nf_ct_port_tuple_to_nlattr(struct sk_buff *skb,\n\t\t\t       const struct nf_conntrack_tuple *tuple)\n{\n\tif (nla_put_be16(skb, CTA_PROTO_SRC_PORT, tuple->src.u.tcp.port) ||\n\t    nla_put_be16(skb, CTA_PROTO_DST_PORT, tuple->dst.u.tcp.port))\n\t\tgoto nla_put_failure;\n\treturn 0;\n\nnla_put_failure:\n\treturn -1;\n}\nEXPORT_SYMBOL_GPL(nf_ct_port_tuple_to_nlattr);\n\nconst struct nla_policy nf_ct_port_nla_policy[CTA_PROTO_MAX+1] = {\n\t[CTA_PROTO_SRC_PORT]  = { .type = NLA_U16 },\n\t[CTA_PROTO_DST_PORT]  = { .type = NLA_U16 },\n};\nEXPORT_SYMBOL_GPL(nf_ct_port_nla_policy);\n\nint nf_ct_port_nlattr_to_tuple(struct nlattr *tb[],\n\t\t\t       struct nf_conntrack_tuple *t,\n\t\t\t       u_int32_t flags)\n{\n\tif (flags & CTA_FILTER_FLAG(CTA_PROTO_SRC_PORT)) {\n\t\tif (!tb[CTA_PROTO_SRC_PORT])\n\t\t\treturn -EINVAL;\n\n\t\tt->src.u.tcp.port = nla_get_be16(tb[CTA_PROTO_SRC_PORT]);\n\t}\n\n\tif (flags & CTA_FILTER_FLAG(CTA_PROTO_DST_PORT)) {\n\t\tif (!tb[CTA_PROTO_DST_PORT])\n\t\t\treturn -EINVAL;\n\n\t\tt->dst.u.tcp.port = nla_get_be16(tb[CTA_PROTO_DST_PORT]);\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(nf_ct_port_nlattr_to_tuple);\n\nunsigned int nf_ct_port_nlattr_tuple_size(void)\n{\n\tstatic unsigned int size __read_mostly;\n\n\tif (!size)\n\t\tsize = nla_policy_len(nf_ct_port_nla_policy, CTA_PROTO_MAX + 1);\n\n\treturn size;\n}\nEXPORT_SYMBOL_GPL(nf_ct_port_nlattr_tuple_size);\n#endif\n\n \nstatic void nf_conntrack_attach(struct sk_buff *nskb, const struct sk_buff *skb)\n{\n\tstruct nf_conn *ct;\n\tenum ip_conntrack_info ctinfo;\n\n\t \n\tct = nf_ct_get(skb, &ctinfo);\n\tif (CTINFO2DIR(ctinfo) == IP_CT_DIR_ORIGINAL)\n\t\tctinfo = IP_CT_RELATED_REPLY;\n\telse\n\t\tctinfo = IP_CT_RELATED;\n\n\t \n\tnf_ct_set(nskb, ct, ctinfo);\n\tnf_conntrack_get(skb_nfct(nskb));\n}\n\nstatic int __nf_conntrack_update(struct net *net, struct sk_buff *skb,\n\t\t\t\t struct nf_conn *ct,\n\t\t\t\t enum ip_conntrack_info ctinfo)\n{\n\tconst struct nf_nat_hook *nat_hook;\n\tstruct nf_conntrack_tuple_hash *h;\n\tstruct nf_conntrack_tuple tuple;\n\tunsigned int status;\n\tint dataoff;\n\tu16 l3num;\n\tu8 l4num;\n\n\tl3num = nf_ct_l3num(ct);\n\n\tdataoff = get_l4proto(skb, skb_network_offset(skb), l3num, &l4num);\n\tif (dataoff <= 0)\n\t\treturn -1;\n\n\tif (!nf_ct_get_tuple(skb, skb_network_offset(skb), dataoff, l3num,\n\t\t\t     l4num, net, &tuple))\n\t\treturn -1;\n\n\tif (ct->status & IPS_SRC_NAT) {\n\t\tmemcpy(tuple.src.u3.all,\n\t\t       ct->tuplehash[IP_CT_DIR_ORIGINAL].tuple.src.u3.all,\n\t\t       sizeof(tuple.src.u3.all));\n\t\ttuple.src.u.all =\n\t\t\tct->tuplehash[IP_CT_DIR_ORIGINAL].tuple.src.u.all;\n\t}\n\n\tif (ct->status & IPS_DST_NAT) {\n\t\tmemcpy(tuple.dst.u3.all,\n\t\t       ct->tuplehash[IP_CT_DIR_ORIGINAL].tuple.dst.u3.all,\n\t\t       sizeof(tuple.dst.u3.all));\n\t\ttuple.dst.u.all =\n\t\t\tct->tuplehash[IP_CT_DIR_ORIGINAL].tuple.dst.u.all;\n\t}\n\n\th = nf_conntrack_find_get(net, nf_ct_zone(ct), &tuple);\n\tif (!h)\n\t\treturn 0;\n\n\t \n\tstatus = ct->status;\n\n\tnf_ct_put(ct);\n\tct = nf_ct_tuplehash_to_ctrack(h);\n\tnf_ct_set(skb, ct, ctinfo);\n\n\tnat_hook = rcu_dereference(nf_nat_hook);\n\tif (!nat_hook)\n\t\treturn 0;\n\n\tif (status & IPS_SRC_NAT &&\n\t    nat_hook->manip_pkt(skb, ct, NF_NAT_MANIP_SRC,\n\t\t\t\tIP_CT_DIR_ORIGINAL) == NF_DROP)\n\t\treturn -1;\n\n\tif (status & IPS_DST_NAT &&\n\t    nat_hook->manip_pkt(skb, ct, NF_NAT_MANIP_DST,\n\t\t\t\tIP_CT_DIR_ORIGINAL) == NF_DROP)\n\t\treturn -1;\n\n\treturn 0;\n}\n\n \nstatic int nf_confirm_cthelper(struct sk_buff *skb, struct nf_conn *ct,\n\t\t\t       enum ip_conntrack_info ctinfo)\n{\n\tconst struct nf_conntrack_helper *helper;\n\tconst struct nf_conn_help *help;\n\tint protoff;\n\n\thelp = nfct_help(ct);\n\tif (!help)\n\t\treturn 0;\n\n\thelper = rcu_dereference(help->helper);\n\tif (!helper)\n\t\treturn 0;\n\n\tif (!(helper->flags & NF_CT_HELPER_F_USERSPACE))\n\t\treturn 0;\n\n\tswitch (nf_ct_l3num(ct)) {\n\tcase NFPROTO_IPV4:\n\t\tprotoff = skb_network_offset(skb) + ip_hdrlen(skb);\n\t\tbreak;\n#if IS_ENABLED(CONFIG_IPV6)\n\tcase NFPROTO_IPV6: {\n\t\t__be16 frag_off;\n\t\tu8 pnum;\n\n\t\tpnum = ipv6_hdr(skb)->nexthdr;\n\t\tprotoff = ipv6_skip_exthdr(skb, sizeof(struct ipv6hdr), &pnum,\n\t\t\t\t\t   &frag_off);\n\t\tif (protoff < 0 || (frag_off & htons(~0x7)) != 0)\n\t\t\treturn 0;\n\t\tbreak;\n\t}\n#endif\n\tdefault:\n\t\treturn 0;\n\t}\n\n\tif (test_bit(IPS_SEQ_ADJUST_BIT, &ct->status) &&\n\t    !nf_is_loopback_packet(skb)) {\n\t\tif (!nf_ct_seq_adjust(skb, ct, ctinfo, protoff)) {\n\t\t\tNF_CT_STAT_INC_ATOMIC(nf_ct_net(ct), drop);\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\t \n\treturn nf_conntrack_confirm(skb) == NF_DROP ? - 1 : 0;\n}\n\nstatic int nf_conntrack_update(struct net *net, struct sk_buff *skb)\n{\n\tenum ip_conntrack_info ctinfo;\n\tstruct nf_conn *ct;\n\tint err;\n\n\tct = nf_ct_get(skb, &ctinfo);\n\tif (!ct)\n\t\treturn 0;\n\n\tif (!nf_ct_is_confirmed(ct)) {\n\t\terr = __nf_conntrack_update(net, skb, ct, ctinfo);\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\tct = nf_ct_get(skb, &ctinfo);\n\t}\n\n\treturn nf_confirm_cthelper(skb, ct, ctinfo);\n}\n\nstatic bool nf_conntrack_get_tuple_skb(struct nf_conntrack_tuple *dst_tuple,\n\t\t\t\t       const struct sk_buff *skb)\n{\n\tconst struct nf_conntrack_tuple *src_tuple;\n\tconst struct nf_conntrack_tuple_hash *hash;\n\tstruct nf_conntrack_tuple srctuple;\n\tenum ip_conntrack_info ctinfo;\n\tstruct nf_conn *ct;\n\n\tct = nf_ct_get(skb, &ctinfo);\n\tif (ct) {\n\t\tsrc_tuple = nf_ct_tuple(ct, CTINFO2DIR(ctinfo));\n\t\tmemcpy(dst_tuple, src_tuple, sizeof(*dst_tuple));\n\t\treturn true;\n\t}\n\n\tif (!nf_ct_get_tuplepr(skb, skb_network_offset(skb),\n\t\t\t       NFPROTO_IPV4, dev_net(skb->dev),\n\t\t\t       &srctuple))\n\t\treturn false;\n\n\thash = nf_conntrack_find_get(dev_net(skb->dev),\n\t\t\t\t     &nf_ct_zone_dflt,\n\t\t\t\t     &srctuple);\n\tif (!hash)\n\t\treturn false;\n\n\tct = nf_ct_tuplehash_to_ctrack(hash);\n\tsrc_tuple = nf_ct_tuple(ct, !hash->tuple.dst.dir);\n\tmemcpy(dst_tuple, src_tuple, sizeof(*dst_tuple));\n\tnf_ct_put(ct);\n\n\treturn true;\n}\n\n \nstatic struct nf_conn *\nget_next_corpse(int (*iter)(struct nf_conn *i, void *data),\n\t\tconst struct nf_ct_iter_data *iter_data, unsigned int *bucket)\n{\n\tstruct nf_conntrack_tuple_hash *h;\n\tstruct nf_conn *ct;\n\tstruct hlist_nulls_node *n;\n\tspinlock_t *lockp;\n\n\tfor (; *bucket < nf_conntrack_htable_size; (*bucket)++) {\n\t\tstruct hlist_nulls_head *hslot = &nf_conntrack_hash[*bucket];\n\n\t\tif (hlist_nulls_empty(hslot))\n\t\t\tcontinue;\n\n\t\tlockp = &nf_conntrack_locks[*bucket % CONNTRACK_LOCKS];\n\t\tlocal_bh_disable();\n\t\tnf_conntrack_lock(lockp);\n\t\thlist_nulls_for_each_entry(h, n, hslot, hnnode) {\n\t\t\tif (NF_CT_DIRECTION(h) != IP_CT_DIR_REPLY)\n\t\t\t\tcontinue;\n\t\t\t \n\t\t\tct = nf_ct_tuplehash_to_ctrack(h);\n\n\t\t\tif (iter_data->net &&\n\t\t\t    !net_eq(iter_data->net, nf_ct_net(ct)))\n\t\t\t\tcontinue;\n\n\t\t\tif (iter(ct, iter_data->data))\n\t\t\t\tgoto found;\n\t\t}\n\t\tspin_unlock(lockp);\n\t\tlocal_bh_enable();\n\t\tcond_resched();\n\t}\n\n\treturn NULL;\nfound:\n\trefcount_inc(&ct->ct_general.use);\n\tspin_unlock(lockp);\n\tlocal_bh_enable();\n\treturn ct;\n}\n\nstatic void nf_ct_iterate_cleanup(int (*iter)(struct nf_conn *i, void *data),\n\t\t\t\t  const struct nf_ct_iter_data *iter_data)\n{\n\tunsigned int bucket = 0;\n\tstruct nf_conn *ct;\n\n\tmight_sleep();\n\n\tmutex_lock(&nf_conntrack_mutex);\n\twhile ((ct = get_next_corpse(iter, iter_data, &bucket)) != NULL) {\n\t\t \n\n\t\tnf_ct_delete(ct, iter_data->portid, iter_data->report);\n\t\tnf_ct_put(ct);\n\t\tcond_resched();\n\t}\n\tmutex_unlock(&nf_conntrack_mutex);\n}\n\nvoid nf_ct_iterate_cleanup_net(int (*iter)(struct nf_conn *i, void *data),\n\t\t\t       const struct nf_ct_iter_data *iter_data)\n{\n\tstruct net *net = iter_data->net;\n\tstruct nf_conntrack_net *cnet = nf_ct_pernet(net);\n\n\tmight_sleep();\n\n\tif (atomic_read(&cnet->count) == 0)\n\t\treturn;\n\n\tnf_ct_iterate_cleanup(iter, iter_data);\n}\nEXPORT_SYMBOL_GPL(nf_ct_iterate_cleanup_net);\n\n \nvoid\nnf_ct_iterate_destroy(int (*iter)(struct nf_conn *i, void *data), void *data)\n{\n\tstruct nf_ct_iter_data iter_data = {};\n\tstruct net *net;\n\n\tdown_read(&net_rwsem);\n\tfor_each_net(net) {\n\t\tstruct nf_conntrack_net *cnet = nf_ct_pernet(net);\n\n\t\tif (atomic_read(&cnet->count) == 0)\n\t\t\tcontinue;\n\t\tnf_queue_nf_hook_drop(net);\n\t}\n\tup_read(&net_rwsem);\n\n\t \n\tnet_ns_barrier();\n\n\t \n\tsynchronize_net();\n\n\tnf_ct_ext_bump_genid();\n\titer_data.data = data;\n\tnf_ct_iterate_cleanup(iter, &iter_data);\n\n\t \n\tsynchronize_rcu();\n}\nEXPORT_SYMBOL_GPL(nf_ct_iterate_destroy);\n\nstatic int kill_all(struct nf_conn *i, void *data)\n{\n\treturn 1;\n}\n\nvoid nf_conntrack_cleanup_start(void)\n{\n\tcleanup_nf_conntrack_bpf();\n\tconntrack_gc_work.exiting = true;\n}\n\nvoid nf_conntrack_cleanup_end(void)\n{\n\tRCU_INIT_POINTER(nf_ct_hook, NULL);\n\tcancel_delayed_work_sync(&conntrack_gc_work.dwork);\n\tkvfree(nf_conntrack_hash);\n\n\tnf_conntrack_proto_fini();\n\tnf_conntrack_helper_fini();\n\tnf_conntrack_expect_fini();\n\n\tkmem_cache_destroy(nf_conntrack_cachep);\n}\n\n \nvoid nf_conntrack_cleanup_net(struct net *net)\n{\n\tLIST_HEAD(single);\n\n\tlist_add(&net->exit_list, &single);\n\tnf_conntrack_cleanup_net_list(&single);\n}\n\nvoid nf_conntrack_cleanup_net_list(struct list_head *net_exit_list)\n{\n\tstruct nf_ct_iter_data iter_data = {};\n\tstruct net *net;\n\tint busy;\n\n\t \n\tsynchronize_net();\ni_see_dead_people:\n\tbusy = 0;\n\tlist_for_each_entry(net, net_exit_list, exit_list) {\n\t\tstruct nf_conntrack_net *cnet = nf_ct_pernet(net);\n\n\t\titer_data.net = net;\n\t\tnf_ct_iterate_cleanup_net(kill_all, &iter_data);\n\t\tif (atomic_read(&cnet->count) != 0)\n\t\t\tbusy = 1;\n\t}\n\tif (busy) {\n\t\tschedule();\n\t\tgoto i_see_dead_people;\n\t}\n\n\tlist_for_each_entry(net, net_exit_list, exit_list) {\n\t\tnf_conntrack_ecache_pernet_fini(net);\n\t\tnf_conntrack_expect_pernet_fini(net);\n\t\tfree_percpu(net->ct.stat);\n\t}\n}\n\nvoid *nf_ct_alloc_hashtable(unsigned int *sizep, int nulls)\n{\n\tstruct hlist_nulls_head *hash;\n\tunsigned int nr_slots, i;\n\n\tif (*sizep > (UINT_MAX / sizeof(struct hlist_nulls_head)))\n\t\treturn NULL;\n\n\tBUILD_BUG_ON(sizeof(struct hlist_nulls_head) != sizeof(struct hlist_head));\n\tnr_slots = *sizep = roundup(*sizep, PAGE_SIZE / sizeof(struct hlist_nulls_head));\n\n\thash = kvcalloc(nr_slots, sizeof(struct hlist_nulls_head), GFP_KERNEL);\n\n\tif (hash && nulls)\n\t\tfor (i = 0; i < nr_slots; i++)\n\t\t\tINIT_HLIST_NULLS_HEAD(&hash[i], i);\n\n\treturn hash;\n}\nEXPORT_SYMBOL_GPL(nf_ct_alloc_hashtable);\n\nint nf_conntrack_hash_resize(unsigned int hashsize)\n{\n\tint i, bucket;\n\tunsigned int old_size;\n\tstruct hlist_nulls_head *hash, *old_hash;\n\tstruct nf_conntrack_tuple_hash *h;\n\tstruct nf_conn *ct;\n\n\tif (!hashsize)\n\t\treturn -EINVAL;\n\n\thash = nf_ct_alloc_hashtable(&hashsize, 1);\n\tif (!hash)\n\t\treturn -ENOMEM;\n\n\tmutex_lock(&nf_conntrack_mutex);\n\told_size = nf_conntrack_htable_size;\n\tif (old_size == hashsize) {\n\t\tmutex_unlock(&nf_conntrack_mutex);\n\t\tkvfree(hash);\n\t\treturn 0;\n\t}\n\n\tlocal_bh_disable();\n\tnf_conntrack_all_lock();\n\twrite_seqcount_begin(&nf_conntrack_generation);\n\n\t \n\n\tfor (i = 0; i < nf_conntrack_htable_size; i++) {\n\t\twhile (!hlist_nulls_empty(&nf_conntrack_hash[i])) {\n\t\t\tunsigned int zone_id;\n\n\t\t\th = hlist_nulls_entry(nf_conntrack_hash[i].first,\n\t\t\t\t\t      struct nf_conntrack_tuple_hash, hnnode);\n\t\t\tct = nf_ct_tuplehash_to_ctrack(h);\n\t\t\thlist_nulls_del_rcu(&h->hnnode);\n\n\t\t\tzone_id = nf_ct_zone_id(nf_ct_zone(ct), NF_CT_DIRECTION(h));\n\t\t\tbucket = __hash_conntrack(nf_ct_net(ct),\n\t\t\t\t\t\t  &h->tuple, zone_id, hashsize);\n\t\t\thlist_nulls_add_head_rcu(&h->hnnode, &hash[bucket]);\n\t\t}\n\t}\n\told_hash = nf_conntrack_hash;\n\n\tnf_conntrack_hash = hash;\n\tnf_conntrack_htable_size = hashsize;\n\n\twrite_seqcount_end(&nf_conntrack_generation);\n\tnf_conntrack_all_unlock();\n\tlocal_bh_enable();\n\n\tmutex_unlock(&nf_conntrack_mutex);\n\n\tsynchronize_net();\n\tkvfree(old_hash);\n\treturn 0;\n}\n\nint nf_conntrack_set_hashsize(const char *val, const struct kernel_param *kp)\n{\n\tunsigned int hashsize;\n\tint rc;\n\n\tif (current->nsproxy->net_ns != &init_net)\n\t\treturn -EOPNOTSUPP;\n\n\t \n\tif (!nf_conntrack_hash)\n\t\treturn param_set_uint(val, kp);\n\n\trc = kstrtouint(val, 0, &hashsize);\n\tif (rc)\n\t\treturn rc;\n\n\treturn nf_conntrack_hash_resize(hashsize);\n}\n\nint nf_conntrack_init_start(void)\n{\n\tunsigned long nr_pages = totalram_pages();\n\tint max_factor = 8;\n\tint ret = -ENOMEM;\n\tint i;\n\n\tseqcount_spinlock_init(&nf_conntrack_generation,\n\t\t\t       &nf_conntrack_locks_all_lock);\n\n\tfor (i = 0; i < CONNTRACK_LOCKS; i++)\n\t\tspin_lock_init(&nf_conntrack_locks[i]);\n\n\tif (!nf_conntrack_htable_size) {\n\t\tnf_conntrack_htable_size\n\t\t\t= (((nr_pages << PAGE_SHIFT) / 16384)\n\t\t\t   / sizeof(struct hlist_head));\n\t\tif (BITS_PER_LONG >= 64 &&\n\t\t    nr_pages > (4 * (1024 * 1024 * 1024 / PAGE_SIZE)))\n\t\t\tnf_conntrack_htable_size = 262144;\n\t\telse if (nr_pages > (1024 * 1024 * 1024 / PAGE_SIZE))\n\t\t\tnf_conntrack_htable_size = 65536;\n\n\t\tif (nf_conntrack_htable_size < 1024)\n\t\t\tnf_conntrack_htable_size = 1024;\n\t\t \n\t\tmax_factor = 1;\n\t}\n\n\tnf_conntrack_hash = nf_ct_alloc_hashtable(&nf_conntrack_htable_size, 1);\n\tif (!nf_conntrack_hash)\n\t\treturn -ENOMEM;\n\n\tnf_conntrack_max = max_factor * nf_conntrack_htable_size;\n\n\tnf_conntrack_cachep = kmem_cache_create(\"nf_conntrack\",\n\t\t\t\t\t\tsizeof(struct nf_conn),\n\t\t\t\t\t\tNFCT_INFOMASK + 1,\n\t\t\t\t\t\tSLAB_TYPESAFE_BY_RCU | SLAB_HWCACHE_ALIGN, NULL);\n\tif (!nf_conntrack_cachep)\n\t\tgoto err_cachep;\n\n\tret = nf_conntrack_expect_init();\n\tif (ret < 0)\n\t\tgoto err_expect;\n\n\tret = nf_conntrack_helper_init();\n\tif (ret < 0)\n\t\tgoto err_helper;\n\n\tret = nf_conntrack_proto_init();\n\tif (ret < 0)\n\t\tgoto err_proto;\n\n\tconntrack_gc_work_init(&conntrack_gc_work);\n\tqueue_delayed_work(system_power_efficient_wq, &conntrack_gc_work.dwork, HZ);\n\n\tret = register_nf_conntrack_bpf();\n\tif (ret < 0)\n\t\tgoto err_kfunc;\n\n\treturn 0;\n\nerr_kfunc:\n\tcancel_delayed_work_sync(&conntrack_gc_work.dwork);\n\tnf_conntrack_proto_fini();\nerr_proto:\n\tnf_conntrack_helper_fini();\nerr_helper:\n\tnf_conntrack_expect_fini();\nerr_expect:\n\tkmem_cache_destroy(nf_conntrack_cachep);\nerr_cachep:\n\tkvfree(nf_conntrack_hash);\n\treturn ret;\n}\n\nstatic void nf_conntrack_set_closing(struct nf_conntrack *nfct)\n{\n\tstruct nf_conn *ct = nf_ct_to_nf_conn(nfct);\n\n\tswitch (nf_ct_protonum(ct)) {\n\tcase IPPROTO_TCP:\n\t\tnf_conntrack_tcp_set_closing(ct);\n\t\tbreak;\n\t}\n}\n\nstatic const struct nf_ct_hook nf_conntrack_hook = {\n\t.update\t\t= nf_conntrack_update,\n\t.destroy\t= nf_ct_destroy,\n\t.get_tuple_skb  = nf_conntrack_get_tuple_skb,\n\t.attach\t\t= nf_conntrack_attach,\n\t.set_closing\t= nf_conntrack_set_closing,\n};\n\nvoid nf_conntrack_init_end(void)\n{\n\tRCU_INIT_POINTER(nf_ct_hook, &nf_conntrack_hook);\n}\n\n \n#define UNCONFIRMED_NULLS_VAL\t((1<<30)+0)\n\nint nf_conntrack_init_net(struct net *net)\n{\n\tstruct nf_conntrack_net *cnet = nf_ct_pernet(net);\n\tint ret = -ENOMEM;\n\n\tBUILD_BUG_ON(IP_CT_UNTRACKED == IP_CT_NUMBER);\n\tBUILD_BUG_ON_NOT_POWER_OF_2(CONNTRACK_LOCKS);\n\tatomic_set(&cnet->count, 0);\n\n\tnet->ct.stat = alloc_percpu(struct ip_conntrack_stat);\n\tif (!net->ct.stat)\n\t\treturn ret;\n\n\tret = nf_conntrack_expect_pernet_init(net);\n\tif (ret < 0)\n\t\tgoto err_expect;\n\n\tnf_conntrack_acct_pernet_init(net);\n\tnf_conntrack_tstamp_pernet_init(net);\n\tnf_conntrack_ecache_pernet_init(net);\n\tnf_conntrack_proto_pernet_init(net);\n\n\treturn 0;\n\nerr_expect:\n\tfree_percpu(net->ct.stat);\n\treturn ret;\n}\n\n \n\nint __nf_ct_change_timeout(struct nf_conn *ct, u64 timeout)\n{\n\tif (test_bit(IPS_FIXED_TIMEOUT_BIT, &ct->status))\n\t\treturn -EPERM;\n\n\t__nf_ct_set_timeout(ct, timeout);\n\n\tif (test_bit(IPS_DYING_BIT, &ct->status))\n\t\treturn -ETIME;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(__nf_ct_change_timeout);\n\nvoid __nf_ct_change_status(struct nf_conn *ct, unsigned long on, unsigned long off)\n{\n\tunsigned int bit;\n\n\t \n\ton &= ~IPS_UNCHANGEABLE_MASK;\n\toff &= ~IPS_UNCHANGEABLE_MASK;\n\n\tfor (bit = 0; bit < __IPS_MAX_BIT; bit++) {\n\t\tif (on & (1 << bit))\n\t\t\tset_bit(bit, &ct->status);\n\t\telse if (off & (1 << bit))\n\t\t\tclear_bit(bit, &ct->status);\n\t}\n}\nEXPORT_SYMBOL_GPL(__nf_ct_change_status);\n\nint nf_ct_change_status_common(struct nf_conn *ct, unsigned int status)\n{\n\tunsigned long d;\n\n\td = ct->status ^ status;\n\n\tif (d & (IPS_EXPECTED|IPS_CONFIRMED|IPS_DYING))\n\t\t \n\t\treturn -EBUSY;\n\n\tif (d & IPS_SEEN_REPLY && !(status & IPS_SEEN_REPLY))\n\t\t \n\t\treturn -EBUSY;\n\n\tif (d & IPS_ASSURED && !(status & IPS_ASSURED))\n\t\t \n\t\treturn -EBUSY;\n\n\t__nf_ct_change_status(ct, status, 0);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(nf_ct_change_status_common);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}