{
  "module_name": "nf_flow_table_ip.c",
  "hash_id": "317ce236a8c93ff098e8614b28026dc5b0079ecba432857a4bb7cbd61dc7efc3",
  "original_prompt": "Ingested from linux-6.6.14/net/netfilter/nf_flow_table_ip.c",
  "human_readable_source": "\n#include <linux/kernel.h>\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/netfilter.h>\n#include <linux/rhashtable.h>\n#include <linux/ip.h>\n#include <linux/ipv6.h>\n#include <linux/netdevice.h>\n#include <linux/if_ether.h>\n#include <net/gso.h>\n#include <net/ip.h>\n#include <net/ipv6.h>\n#include <net/ip6_route.h>\n#include <net/neighbour.h>\n#include <net/netfilter/nf_flow_table.h>\n#include <net/netfilter/nf_conntrack_acct.h>\n \n#include <linux/tcp.h>\n#include <linux/udp.h>\n\nstatic int nf_flow_state_check(struct flow_offload *flow, int proto,\n\t\t\t       struct sk_buff *skb, unsigned int thoff)\n{\n\tstruct tcphdr *tcph;\n\n\tif (proto != IPPROTO_TCP)\n\t\treturn 0;\n\n\ttcph = (void *)(skb_network_header(skb) + thoff);\n\tif (unlikely(tcph->fin || tcph->rst)) {\n\t\tflow_offload_teardown(flow);\n\t\treturn -1;\n\t}\n\n\treturn 0;\n}\n\nstatic void nf_flow_nat_ip_tcp(struct sk_buff *skb, unsigned int thoff,\n\t\t\t       __be32 addr, __be32 new_addr)\n{\n\tstruct tcphdr *tcph;\n\n\ttcph = (void *)(skb_network_header(skb) + thoff);\n\tinet_proto_csum_replace4(&tcph->check, skb, addr, new_addr, true);\n}\n\nstatic void nf_flow_nat_ip_udp(struct sk_buff *skb, unsigned int thoff,\n\t\t\t       __be32 addr, __be32 new_addr)\n{\n\tstruct udphdr *udph;\n\n\tudph = (void *)(skb_network_header(skb) + thoff);\n\tif (udph->check || skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\tinet_proto_csum_replace4(&udph->check, skb, addr,\n\t\t\t\t\t new_addr, true);\n\t\tif (!udph->check)\n\t\t\tudph->check = CSUM_MANGLED_0;\n\t}\n}\n\nstatic void nf_flow_nat_ip_l4proto(struct sk_buff *skb, struct iphdr *iph,\n\t\t\t\t   unsigned int thoff, __be32 addr,\n\t\t\t\t   __be32 new_addr)\n{\n\tswitch (iph->protocol) {\n\tcase IPPROTO_TCP:\n\t\tnf_flow_nat_ip_tcp(skb, thoff, addr, new_addr);\n\t\tbreak;\n\tcase IPPROTO_UDP:\n\t\tnf_flow_nat_ip_udp(skb, thoff, addr, new_addr);\n\t\tbreak;\n\t}\n}\n\nstatic void nf_flow_snat_ip(const struct flow_offload *flow,\n\t\t\t    struct sk_buff *skb, struct iphdr *iph,\n\t\t\t    unsigned int thoff, enum flow_offload_tuple_dir dir)\n{\n\t__be32 addr, new_addr;\n\n\tswitch (dir) {\n\tcase FLOW_OFFLOAD_DIR_ORIGINAL:\n\t\taddr = iph->saddr;\n\t\tnew_addr = flow->tuplehash[FLOW_OFFLOAD_DIR_REPLY].tuple.dst_v4.s_addr;\n\t\tiph->saddr = new_addr;\n\t\tbreak;\n\tcase FLOW_OFFLOAD_DIR_REPLY:\n\t\taddr = iph->daddr;\n\t\tnew_addr = flow->tuplehash[FLOW_OFFLOAD_DIR_ORIGINAL].tuple.src_v4.s_addr;\n\t\tiph->daddr = new_addr;\n\t\tbreak;\n\t}\n\tcsum_replace4(&iph->check, addr, new_addr);\n\n\tnf_flow_nat_ip_l4proto(skb, iph, thoff, addr, new_addr);\n}\n\nstatic void nf_flow_dnat_ip(const struct flow_offload *flow,\n\t\t\t    struct sk_buff *skb, struct iphdr *iph,\n\t\t\t    unsigned int thoff, enum flow_offload_tuple_dir dir)\n{\n\t__be32 addr, new_addr;\n\n\tswitch (dir) {\n\tcase FLOW_OFFLOAD_DIR_ORIGINAL:\n\t\taddr = iph->daddr;\n\t\tnew_addr = flow->tuplehash[FLOW_OFFLOAD_DIR_REPLY].tuple.src_v4.s_addr;\n\t\tiph->daddr = new_addr;\n\t\tbreak;\n\tcase FLOW_OFFLOAD_DIR_REPLY:\n\t\taddr = iph->saddr;\n\t\tnew_addr = flow->tuplehash[FLOW_OFFLOAD_DIR_ORIGINAL].tuple.dst_v4.s_addr;\n\t\tiph->saddr = new_addr;\n\t\tbreak;\n\t}\n\tcsum_replace4(&iph->check, addr, new_addr);\n\n\tnf_flow_nat_ip_l4proto(skb, iph, thoff, addr, new_addr);\n}\n\nstatic void nf_flow_nat_ip(const struct flow_offload *flow, struct sk_buff *skb,\n\t\t\t  unsigned int thoff, enum flow_offload_tuple_dir dir,\n\t\t\t  struct iphdr *iph)\n{\n\tif (test_bit(NF_FLOW_SNAT, &flow->flags)) {\n\t\tnf_flow_snat_port(flow, skb, thoff, iph->protocol, dir);\n\t\tnf_flow_snat_ip(flow, skb, iph, thoff, dir);\n\t}\n\tif (test_bit(NF_FLOW_DNAT, &flow->flags)) {\n\t\tnf_flow_dnat_port(flow, skb, thoff, iph->protocol, dir);\n\t\tnf_flow_dnat_ip(flow, skb, iph, thoff, dir);\n\t}\n}\n\nstatic bool ip_has_options(unsigned int thoff)\n{\n\treturn thoff != sizeof(struct iphdr);\n}\n\nstatic void nf_flow_tuple_encap(struct sk_buff *skb,\n\t\t\t\tstruct flow_offload_tuple *tuple)\n{\n\tstruct vlan_ethhdr *veth;\n\tstruct pppoe_hdr *phdr;\n\tint i = 0;\n\n\tif (skb_vlan_tag_present(skb)) {\n\t\ttuple->encap[i].id = skb_vlan_tag_get(skb);\n\t\ttuple->encap[i].proto = skb->vlan_proto;\n\t\ti++;\n\t}\n\tswitch (skb->protocol) {\n\tcase htons(ETH_P_8021Q):\n\t\tveth = (struct vlan_ethhdr *)skb_mac_header(skb);\n\t\ttuple->encap[i].id = ntohs(veth->h_vlan_TCI);\n\t\ttuple->encap[i].proto = skb->protocol;\n\t\tbreak;\n\tcase htons(ETH_P_PPP_SES):\n\t\tphdr = (struct pppoe_hdr *)skb_mac_header(skb);\n\t\ttuple->encap[i].id = ntohs(phdr->sid);\n\t\ttuple->encap[i].proto = skb->protocol;\n\t\tbreak;\n\t}\n}\n\nstruct nf_flowtable_ctx {\n\tconst struct net_device\t*in;\n\tu32\t\t\toffset;\n\tu32\t\t\thdrsize;\n};\n\nstatic int nf_flow_tuple_ip(struct nf_flowtable_ctx *ctx, struct sk_buff *skb,\n\t\t\t    struct flow_offload_tuple *tuple)\n{\n\tstruct flow_ports *ports;\n\tunsigned int thoff;\n\tstruct iphdr *iph;\n\tu8 ipproto;\n\n\tif (!pskb_may_pull(skb, sizeof(*iph) + ctx->offset))\n\t\treturn -1;\n\n\tiph = (struct iphdr *)(skb_network_header(skb) + ctx->offset);\n\tthoff = (iph->ihl * 4);\n\n\tif (ip_is_fragment(iph) ||\n\t    unlikely(ip_has_options(thoff)))\n\t\treturn -1;\n\n\tthoff += ctx->offset;\n\n\tipproto = iph->protocol;\n\tswitch (ipproto) {\n\tcase IPPROTO_TCP:\n\t\tctx->hdrsize = sizeof(struct tcphdr);\n\t\tbreak;\n\tcase IPPROTO_UDP:\n\t\tctx->hdrsize = sizeof(struct udphdr);\n\t\tbreak;\n#ifdef CONFIG_NF_CT_PROTO_GRE\n\tcase IPPROTO_GRE:\n\t\tctx->hdrsize = sizeof(struct gre_base_hdr);\n\t\tbreak;\n#endif\n\tdefault:\n\t\treturn -1;\n\t}\n\n\tif (iph->ttl <= 1)\n\t\treturn -1;\n\n\tif (!pskb_may_pull(skb, thoff + ctx->hdrsize))\n\t\treturn -1;\n\n\tswitch (ipproto) {\n\tcase IPPROTO_TCP:\n\tcase IPPROTO_UDP:\n\t\tports = (struct flow_ports *)(skb_network_header(skb) + thoff);\n\t\ttuple->src_port\t\t= ports->source;\n\t\ttuple->dst_port\t\t= ports->dest;\n\t\tbreak;\n\tcase IPPROTO_GRE: {\n\t\tstruct gre_base_hdr *greh;\n\n\t\tgreh = (struct gre_base_hdr *)(skb_network_header(skb) + thoff);\n\t\tif ((greh->flags & GRE_VERSION) != GRE_VERSION_0)\n\t\t\treturn -1;\n\t\tbreak;\n\t}\n\t}\n\n\tiph = (struct iphdr *)(skb_network_header(skb) + ctx->offset);\n\n\ttuple->src_v4.s_addr\t= iph->saddr;\n\ttuple->dst_v4.s_addr\t= iph->daddr;\n\ttuple->l3proto\t\t= AF_INET;\n\ttuple->l4proto\t\t= ipproto;\n\ttuple->iifidx\t\t= ctx->in->ifindex;\n\tnf_flow_tuple_encap(skb, tuple);\n\n\treturn 0;\n}\n\n \nstatic bool nf_flow_exceeds_mtu(const struct sk_buff *skb, unsigned int mtu)\n{\n\tif (skb->len <= mtu)\n\t\treturn false;\n\n\tif (skb_is_gso(skb) && skb_gso_validate_network_len(skb, mtu))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic inline bool nf_flow_dst_check(struct flow_offload_tuple *tuple)\n{\n\tif (tuple->xmit_type != FLOW_OFFLOAD_XMIT_NEIGH &&\n\t    tuple->xmit_type != FLOW_OFFLOAD_XMIT_XFRM)\n\t\treturn true;\n\n\treturn dst_check(tuple->dst_cache, tuple->dst_cookie);\n}\n\nstatic unsigned int nf_flow_xmit_xfrm(struct sk_buff *skb,\n\t\t\t\t      const struct nf_hook_state *state,\n\t\t\t\t      struct dst_entry *dst)\n{\n\tskb_orphan(skb);\n\tskb_dst_set_noref(skb, dst);\n\tdst_output(state->net, state->sk, skb);\n\treturn NF_STOLEN;\n}\n\nstatic bool nf_flow_skb_encap_protocol(const struct sk_buff *skb, __be16 proto,\n\t\t\t\t       u32 *offset)\n{\n\tstruct vlan_ethhdr *veth;\n\n\tswitch (skb->protocol) {\n\tcase htons(ETH_P_8021Q):\n\t\tveth = (struct vlan_ethhdr *)skb_mac_header(skb);\n\t\tif (veth->h_vlan_encapsulated_proto == proto) {\n\t\t\t*offset += VLAN_HLEN;\n\t\t\treturn true;\n\t\t}\n\t\tbreak;\n\tcase htons(ETH_P_PPP_SES):\n\t\tif (nf_flow_pppoe_proto(skb) == proto) {\n\t\t\t*offset += PPPOE_SES_HLEN;\n\t\t\treturn true;\n\t\t}\n\t\tbreak;\n\t}\n\n\treturn false;\n}\n\nstatic void nf_flow_encap_pop(struct sk_buff *skb,\n\t\t\t      struct flow_offload_tuple_rhash *tuplehash)\n{\n\tstruct vlan_hdr *vlan_hdr;\n\tint i;\n\n\tfor (i = 0; i < tuplehash->tuple.encap_num; i++) {\n\t\tif (skb_vlan_tag_present(skb)) {\n\t\t\t__vlan_hwaccel_clear_tag(skb);\n\t\t\tcontinue;\n\t\t}\n\t\tswitch (skb->protocol) {\n\t\tcase htons(ETH_P_8021Q):\n\t\t\tvlan_hdr = (struct vlan_hdr *)skb->data;\n\t\t\t__skb_pull(skb, VLAN_HLEN);\n\t\t\tvlan_set_encap_proto(skb, vlan_hdr);\n\t\t\tskb_reset_network_header(skb);\n\t\t\tbreak;\n\t\tcase htons(ETH_P_PPP_SES):\n\t\t\tskb->protocol = nf_flow_pppoe_proto(skb);\n\t\t\tskb_pull(skb, PPPOE_SES_HLEN);\n\t\t\tskb_reset_network_header(skb);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic unsigned int nf_flow_queue_xmit(struct net *net, struct sk_buff *skb,\n\t\t\t\t       const struct flow_offload_tuple_rhash *tuplehash,\n\t\t\t\t       unsigned short type)\n{\n\tstruct net_device *outdev;\n\n\toutdev = dev_get_by_index_rcu(net, tuplehash->tuple.out.ifidx);\n\tif (!outdev)\n\t\treturn NF_DROP;\n\n\tskb->dev = outdev;\n\tdev_hard_header(skb, skb->dev, type, tuplehash->tuple.out.h_dest,\n\t\t\ttuplehash->tuple.out.h_source, skb->len);\n\tdev_queue_xmit(skb);\n\n\treturn NF_STOLEN;\n}\n\nstatic struct flow_offload_tuple_rhash *\nnf_flow_offload_lookup(struct nf_flowtable_ctx *ctx,\n\t\t       struct nf_flowtable *flow_table, struct sk_buff *skb)\n{\n\tstruct flow_offload_tuple tuple = {};\n\n\tif (skb->protocol != htons(ETH_P_IP) &&\n\t    !nf_flow_skb_encap_protocol(skb, htons(ETH_P_IP), &ctx->offset))\n\t\treturn NULL;\n\n\tif (nf_flow_tuple_ip(ctx, skb, &tuple) < 0)\n\t\treturn NULL;\n\n\treturn flow_offload_lookup(flow_table, &tuple);\n}\n\nstatic int nf_flow_offload_forward(struct nf_flowtable_ctx *ctx,\n\t\t\t\t   struct nf_flowtable *flow_table,\n\t\t\t\t   struct flow_offload_tuple_rhash *tuplehash,\n\t\t\t\t   struct sk_buff *skb)\n{\n\tenum flow_offload_tuple_dir dir;\n\tstruct flow_offload *flow;\n\tunsigned int thoff, mtu;\n\tstruct iphdr *iph;\n\n\tdir = tuplehash->tuple.dir;\n\tflow = container_of(tuplehash, struct flow_offload, tuplehash[dir]);\n\n\tmtu = flow->tuplehash[dir].tuple.mtu + ctx->offset;\n\tif (unlikely(nf_flow_exceeds_mtu(skb, mtu)))\n\t\treturn 0;\n\n\tiph = (struct iphdr *)(skb_network_header(skb) + ctx->offset);\n\tthoff = (iph->ihl * 4) + ctx->offset;\n\tif (nf_flow_state_check(flow, iph->protocol, skb, thoff))\n\t\treturn 0;\n\n\tif (!nf_flow_dst_check(&tuplehash->tuple)) {\n\t\tflow_offload_teardown(flow);\n\t\treturn 0;\n\t}\n\n\tif (skb_try_make_writable(skb, thoff + ctx->hdrsize))\n\t\treturn -1;\n\n\tflow_offload_refresh(flow_table, flow, false);\n\n\tnf_flow_encap_pop(skb, tuplehash);\n\tthoff -= ctx->offset;\n\n\tiph = ip_hdr(skb);\n\tnf_flow_nat_ip(flow, skb, thoff, dir, iph);\n\n\tip_decrease_ttl(iph);\n\tskb_clear_tstamp(skb);\n\n\tif (flow_table->flags & NF_FLOWTABLE_COUNTER)\n\t\tnf_ct_acct_update(flow->ct, tuplehash->tuple.dir, skb->len);\n\n\treturn 1;\n}\n\nunsigned int\nnf_flow_offload_ip_hook(void *priv, struct sk_buff *skb,\n\t\t\tconst struct nf_hook_state *state)\n{\n\tstruct flow_offload_tuple_rhash *tuplehash;\n\tstruct nf_flowtable *flow_table = priv;\n\tenum flow_offload_tuple_dir dir;\n\tstruct nf_flowtable_ctx ctx = {\n\t\t.in\t= state->in,\n\t};\n\tstruct flow_offload *flow;\n\tstruct net_device *outdev;\n\tstruct rtable *rt;\n\t__be32 nexthop;\n\tint ret;\n\n\ttuplehash = nf_flow_offload_lookup(&ctx, flow_table, skb);\n\tif (!tuplehash)\n\t\treturn NF_ACCEPT;\n\n\tret = nf_flow_offload_forward(&ctx, flow_table, tuplehash, skb);\n\tif (ret < 0)\n\t\treturn NF_DROP;\n\telse if (ret == 0)\n\t\treturn NF_ACCEPT;\n\n\tif (unlikely(tuplehash->tuple.xmit_type == FLOW_OFFLOAD_XMIT_XFRM)) {\n\t\trt = (struct rtable *)tuplehash->tuple.dst_cache;\n\t\tmemset(skb->cb, 0, sizeof(struct inet_skb_parm));\n\t\tIPCB(skb)->iif = skb->dev->ifindex;\n\t\tIPCB(skb)->flags = IPSKB_FORWARDED;\n\t\treturn nf_flow_xmit_xfrm(skb, state, &rt->dst);\n\t}\n\n\tdir = tuplehash->tuple.dir;\n\tflow = container_of(tuplehash, struct flow_offload, tuplehash[dir]);\n\n\tswitch (tuplehash->tuple.xmit_type) {\n\tcase FLOW_OFFLOAD_XMIT_NEIGH:\n\t\trt = (struct rtable *)tuplehash->tuple.dst_cache;\n\t\toutdev = rt->dst.dev;\n\t\tskb->dev = outdev;\n\t\tnexthop = rt_nexthop(rt, flow->tuplehash[!dir].tuple.src_v4.s_addr);\n\t\tskb_dst_set_noref(skb, &rt->dst);\n\t\tneigh_xmit(NEIGH_ARP_TABLE, outdev, &nexthop, skb);\n\t\tret = NF_STOLEN;\n\t\tbreak;\n\tcase FLOW_OFFLOAD_XMIT_DIRECT:\n\t\tret = nf_flow_queue_xmit(state->net, skb, tuplehash, ETH_P_IP);\n\t\tif (ret == NF_DROP)\n\t\t\tflow_offload_teardown(flow);\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\tret = NF_DROP;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(nf_flow_offload_ip_hook);\n\nstatic void nf_flow_nat_ipv6_tcp(struct sk_buff *skb, unsigned int thoff,\n\t\t\t\t struct in6_addr *addr,\n\t\t\t\t struct in6_addr *new_addr,\n\t\t\t\t struct ipv6hdr *ip6h)\n{\n\tstruct tcphdr *tcph;\n\n\ttcph = (void *)(skb_network_header(skb) + thoff);\n\tinet_proto_csum_replace16(&tcph->check, skb, addr->s6_addr32,\n\t\t\t\t  new_addr->s6_addr32, true);\n}\n\nstatic void nf_flow_nat_ipv6_udp(struct sk_buff *skb, unsigned int thoff,\n\t\t\t\t struct in6_addr *addr,\n\t\t\t\t struct in6_addr *new_addr)\n{\n\tstruct udphdr *udph;\n\n\tudph = (void *)(skb_network_header(skb) + thoff);\n\tif (udph->check || skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\tinet_proto_csum_replace16(&udph->check, skb, addr->s6_addr32,\n\t\t\t\t\t  new_addr->s6_addr32, true);\n\t\tif (!udph->check)\n\t\t\tudph->check = CSUM_MANGLED_0;\n\t}\n}\n\nstatic void nf_flow_nat_ipv6_l4proto(struct sk_buff *skb, struct ipv6hdr *ip6h,\n\t\t\t\t     unsigned int thoff, struct in6_addr *addr,\n\t\t\t\t     struct in6_addr *new_addr)\n{\n\tswitch (ip6h->nexthdr) {\n\tcase IPPROTO_TCP:\n\t\tnf_flow_nat_ipv6_tcp(skb, thoff, addr, new_addr, ip6h);\n\t\tbreak;\n\tcase IPPROTO_UDP:\n\t\tnf_flow_nat_ipv6_udp(skb, thoff, addr, new_addr);\n\t\tbreak;\n\t}\n}\n\nstatic void nf_flow_snat_ipv6(const struct flow_offload *flow,\n\t\t\t      struct sk_buff *skb, struct ipv6hdr *ip6h,\n\t\t\t      unsigned int thoff,\n\t\t\t      enum flow_offload_tuple_dir dir)\n{\n\tstruct in6_addr addr, new_addr;\n\n\tswitch (dir) {\n\tcase FLOW_OFFLOAD_DIR_ORIGINAL:\n\t\taddr = ip6h->saddr;\n\t\tnew_addr = flow->tuplehash[FLOW_OFFLOAD_DIR_REPLY].tuple.dst_v6;\n\t\tip6h->saddr = new_addr;\n\t\tbreak;\n\tcase FLOW_OFFLOAD_DIR_REPLY:\n\t\taddr = ip6h->daddr;\n\t\tnew_addr = flow->tuplehash[FLOW_OFFLOAD_DIR_ORIGINAL].tuple.src_v6;\n\t\tip6h->daddr = new_addr;\n\t\tbreak;\n\t}\n\n\tnf_flow_nat_ipv6_l4proto(skb, ip6h, thoff, &addr, &new_addr);\n}\n\nstatic void nf_flow_dnat_ipv6(const struct flow_offload *flow,\n\t\t\t      struct sk_buff *skb, struct ipv6hdr *ip6h,\n\t\t\t      unsigned int thoff,\n\t\t\t      enum flow_offload_tuple_dir dir)\n{\n\tstruct in6_addr addr, new_addr;\n\n\tswitch (dir) {\n\tcase FLOW_OFFLOAD_DIR_ORIGINAL:\n\t\taddr = ip6h->daddr;\n\t\tnew_addr = flow->tuplehash[FLOW_OFFLOAD_DIR_REPLY].tuple.src_v6;\n\t\tip6h->daddr = new_addr;\n\t\tbreak;\n\tcase FLOW_OFFLOAD_DIR_REPLY:\n\t\taddr = ip6h->saddr;\n\t\tnew_addr = flow->tuplehash[FLOW_OFFLOAD_DIR_ORIGINAL].tuple.dst_v6;\n\t\tip6h->saddr = new_addr;\n\t\tbreak;\n\t}\n\n\tnf_flow_nat_ipv6_l4proto(skb, ip6h, thoff, &addr, &new_addr);\n}\n\nstatic void nf_flow_nat_ipv6(const struct flow_offload *flow,\n\t\t\t     struct sk_buff *skb,\n\t\t\t     enum flow_offload_tuple_dir dir,\n\t\t\t     struct ipv6hdr *ip6h)\n{\n\tunsigned int thoff = sizeof(*ip6h);\n\n\tif (test_bit(NF_FLOW_SNAT, &flow->flags)) {\n\t\tnf_flow_snat_port(flow, skb, thoff, ip6h->nexthdr, dir);\n\t\tnf_flow_snat_ipv6(flow, skb, ip6h, thoff, dir);\n\t}\n\tif (test_bit(NF_FLOW_DNAT, &flow->flags)) {\n\t\tnf_flow_dnat_port(flow, skb, thoff, ip6h->nexthdr, dir);\n\t\tnf_flow_dnat_ipv6(flow, skb, ip6h, thoff, dir);\n\t}\n}\n\nstatic int nf_flow_tuple_ipv6(struct nf_flowtable_ctx *ctx, struct sk_buff *skb,\n\t\t\t      struct flow_offload_tuple *tuple)\n{\n\tstruct flow_ports *ports;\n\tstruct ipv6hdr *ip6h;\n\tunsigned int thoff;\n\tu8 nexthdr;\n\n\tthoff = sizeof(*ip6h) + ctx->offset;\n\tif (!pskb_may_pull(skb, thoff))\n\t\treturn -1;\n\n\tip6h = (struct ipv6hdr *)(skb_network_header(skb) + ctx->offset);\n\n\tnexthdr = ip6h->nexthdr;\n\tswitch (nexthdr) {\n\tcase IPPROTO_TCP:\n\t\tctx->hdrsize = sizeof(struct tcphdr);\n\t\tbreak;\n\tcase IPPROTO_UDP:\n\t\tctx->hdrsize = sizeof(struct udphdr);\n\t\tbreak;\n#ifdef CONFIG_NF_CT_PROTO_GRE\n\tcase IPPROTO_GRE:\n\t\tctx->hdrsize = sizeof(struct gre_base_hdr);\n\t\tbreak;\n#endif\n\tdefault:\n\t\treturn -1;\n\t}\n\n\tif (ip6h->hop_limit <= 1)\n\t\treturn -1;\n\n\tif (!pskb_may_pull(skb, thoff + ctx->hdrsize))\n\t\treturn -1;\n\n\tswitch (nexthdr) {\n\tcase IPPROTO_TCP:\n\tcase IPPROTO_UDP:\n\t\tports = (struct flow_ports *)(skb_network_header(skb) + thoff);\n\t\ttuple->src_port\t\t= ports->source;\n\t\ttuple->dst_port\t\t= ports->dest;\n\t\tbreak;\n\tcase IPPROTO_GRE: {\n\t\tstruct gre_base_hdr *greh;\n\n\t\tgreh = (struct gre_base_hdr *)(skb_network_header(skb) + thoff);\n\t\tif ((greh->flags & GRE_VERSION) != GRE_VERSION_0)\n\t\t\treturn -1;\n\t\tbreak;\n\t}\n\t}\n\n\tip6h = (struct ipv6hdr *)(skb_network_header(skb) + ctx->offset);\n\n\ttuple->src_v6\t\t= ip6h->saddr;\n\ttuple->dst_v6\t\t= ip6h->daddr;\n\ttuple->l3proto\t\t= AF_INET6;\n\ttuple->l4proto\t\t= nexthdr;\n\ttuple->iifidx\t\t= ctx->in->ifindex;\n\tnf_flow_tuple_encap(skb, tuple);\n\n\treturn 0;\n}\n\nstatic int nf_flow_offload_ipv6_forward(struct nf_flowtable_ctx *ctx,\n\t\t\t\t\tstruct nf_flowtable *flow_table,\n\t\t\t\t\tstruct flow_offload_tuple_rhash *tuplehash,\n\t\t\t\t\tstruct sk_buff *skb)\n{\n\tenum flow_offload_tuple_dir dir;\n\tstruct flow_offload *flow;\n\tunsigned int thoff, mtu;\n\tstruct ipv6hdr *ip6h;\n\n\tdir = tuplehash->tuple.dir;\n\tflow = container_of(tuplehash, struct flow_offload, tuplehash[dir]);\n\n\tmtu = flow->tuplehash[dir].tuple.mtu + ctx->offset;\n\tif (unlikely(nf_flow_exceeds_mtu(skb, mtu)))\n\t\treturn 0;\n\n\tip6h = (struct ipv6hdr *)(skb_network_header(skb) + ctx->offset);\n\tthoff = sizeof(*ip6h) + ctx->offset;\n\tif (nf_flow_state_check(flow, ip6h->nexthdr, skb, thoff))\n\t\treturn 0;\n\n\tif (!nf_flow_dst_check(&tuplehash->tuple)) {\n\t\tflow_offload_teardown(flow);\n\t\treturn 0;\n\t}\n\n\tif (skb_try_make_writable(skb, thoff + ctx->hdrsize))\n\t\treturn -1;\n\n\tflow_offload_refresh(flow_table, flow, false);\n\n\tnf_flow_encap_pop(skb, tuplehash);\n\n\tip6h = ipv6_hdr(skb);\n\tnf_flow_nat_ipv6(flow, skb, dir, ip6h);\n\n\tip6h->hop_limit--;\n\tskb_clear_tstamp(skb);\n\n\tif (flow_table->flags & NF_FLOWTABLE_COUNTER)\n\t\tnf_ct_acct_update(flow->ct, tuplehash->tuple.dir, skb->len);\n\n\treturn 1;\n}\n\nstatic struct flow_offload_tuple_rhash *\nnf_flow_offload_ipv6_lookup(struct nf_flowtable_ctx *ctx,\n\t\t\t    struct nf_flowtable *flow_table,\n\t\t\t    struct sk_buff *skb)\n{\n\tstruct flow_offload_tuple tuple = {};\n\n\tif (skb->protocol != htons(ETH_P_IPV6) &&\n\t    !nf_flow_skb_encap_protocol(skb, htons(ETH_P_IPV6), &ctx->offset))\n\t\treturn NULL;\n\n\tif (nf_flow_tuple_ipv6(ctx, skb, &tuple) < 0)\n\t\treturn NULL;\n\n\treturn flow_offload_lookup(flow_table, &tuple);\n}\n\nunsigned int\nnf_flow_offload_ipv6_hook(void *priv, struct sk_buff *skb,\n\t\t\t  const struct nf_hook_state *state)\n{\n\tstruct flow_offload_tuple_rhash *tuplehash;\n\tstruct nf_flowtable *flow_table = priv;\n\tenum flow_offload_tuple_dir dir;\n\tstruct nf_flowtable_ctx ctx = {\n\t\t.in\t= state->in,\n\t};\n\tconst struct in6_addr *nexthop;\n\tstruct flow_offload *flow;\n\tstruct net_device *outdev;\n\tstruct rt6_info *rt;\n\tint ret;\n\n\ttuplehash = nf_flow_offload_ipv6_lookup(&ctx, flow_table, skb);\n\tif (tuplehash == NULL)\n\t\treturn NF_ACCEPT;\n\n\tret = nf_flow_offload_ipv6_forward(&ctx, flow_table, tuplehash, skb);\n\tif (ret < 0)\n\t\treturn NF_DROP;\n\telse if (ret == 0)\n\t\treturn NF_ACCEPT;\n\n\tif (unlikely(tuplehash->tuple.xmit_type == FLOW_OFFLOAD_XMIT_XFRM)) {\n\t\trt = (struct rt6_info *)tuplehash->tuple.dst_cache;\n\t\tmemset(skb->cb, 0, sizeof(struct inet6_skb_parm));\n\t\tIP6CB(skb)->iif = skb->dev->ifindex;\n\t\tIP6CB(skb)->flags = IP6SKB_FORWARDED;\n\t\treturn nf_flow_xmit_xfrm(skb, state, &rt->dst);\n\t}\n\n\tdir = tuplehash->tuple.dir;\n\tflow = container_of(tuplehash, struct flow_offload, tuplehash[dir]);\n\n\tswitch (tuplehash->tuple.xmit_type) {\n\tcase FLOW_OFFLOAD_XMIT_NEIGH:\n\t\trt = (struct rt6_info *)tuplehash->tuple.dst_cache;\n\t\toutdev = rt->dst.dev;\n\t\tskb->dev = outdev;\n\t\tnexthop = rt6_nexthop(rt, &flow->tuplehash[!dir].tuple.src_v6);\n\t\tskb_dst_set_noref(skb, &rt->dst);\n\t\tneigh_xmit(NEIGH_ND_TABLE, outdev, nexthop, skb);\n\t\tret = NF_STOLEN;\n\t\tbreak;\n\tcase FLOW_OFFLOAD_XMIT_DIRECT:\n\t\tret = nf_flow_queue_xmit(state->net, skb, tuplehash, ETH_P_IPV6);\n\t\tif (ret == NF_DROP)\n\t\t\tflow_offload_teardown(flow);\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\tret = NF_DROP;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(nf_flow_offload_ipv6_hook);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}