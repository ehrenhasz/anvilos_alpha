{
  "module_name": "nf_flow_table_offload.c",
  "hash_id": "0ceb2e6d630f442c0ce3f640cb4732ad584b1163cea9a5c2c11e090bcb2f4d2d",
  "original_prompt": "Ingested from linux-6.6.14/net/netfilter/nf_flow_table_offload.c",
  "human_readable_source": "#include <linux/kernel.h>\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/netfilter.h>\n#include <linux/rhashtable.h>\n#include <linux/netdevice.h>\n#include <linux/tc_act/tc_csum.h>\n#include <net/flow_offload.h>\n#include <net/netfilter/nf_flow_table.h>\n#include <net/netfilter/nf_tables.h>\n#include <net/netfilter/nf_conntrack.h>\n#include <net/netfilter/nf_conntrack_acct.h>\n#include <net/netfilter/nf_conntrack_core.h>\n#include <net/netfilter/nf_conntrack_tuple.h>\n\nstatic struct workqueue_struct *nf_flow_offload_add_wq;\nstatic struct workqueue_struct *nf_flow_offload_del_wq;\nstatic struct workqueue_struct *nf_flow_offload_stats_wq;\n\nstruct flow_offload_work {\n\tstruct list_head\tlist;\n\tenum flow_cls_command\tcmd;\n\tstruct nf_flowtable\t*flowtable;\n\tstruct flow_offload\t*flow;\n\tstruct work_struct\twork;\n};\n\n#define NF_FLOW_DISSECTOR(__match, __type, __field)\t\\\n\t(__match)->dissector.offset[__type] =\t\t\\\n\t\toffsetof(struct nf_flow_key, __field)\n\nstatic void nf_flow_rule_lwt_match(struct nf_flow_match *match,\n\t\t\t\t   struct ip_tunnel_info *tun_info)\n{\n\tstruct nf_flow_key *mask = &match->mask;\n\tstruct nf_flow_key *key = &match->key;\n\tunsigned long long enc_keys;\n\n\tif (!tun_info || !(tun_info->mode & IP_TUNNEL_INFO_TX))\n\t\treturn;\n\n\tNF_FLOW_DISSECTOR(match, FLOW_DISSECTOR_KEY_ENC_CONTROL, enc_control);\n\tNF_FLOW_DISSECTOR(match, FLOW_DISSECTOR_KEY_ENC_KEYID, enc_key_id);\n\tkey->enc_key_id.keyid = tunnel_id_to_key32(tun_info->key.tun_id);\n\tmask->enc_key_id.keyid = 0xffffffff;\n\tenc_keys = BIT_ULL(FLOW_DISSECTOR_KEY_ENC_KEYID) |\n\t\t   BIT_ULL(FLOW_DISSECTOR_KEY_ENC_CONTROL);\n\n\tif (ip_tunnel_info_af(tun_info) == AF_INET) {\n\t\tNF_FLOW_DISSECTOR(match, FLOW_DISSECTOR_KEY_ENC_IPV4_ADDRS,\n\t\t\t\t  enc_ipv4);\n\t\tkey->enc_ipv4.src = tun_info->key.u.ipv4.dst;\n\t\tkey->enc_ipv4.dst = tun_info->key.u.ipv4.src;\n\t\tif (key->enc_ipv4.src)\n\t\t\tmask->enc_ipv4.src = 0xffffffff;\n\t\tif (key->enc_ipv4.dst)\n\t\t\tmask->enc_ipv4.dst = 0xffffffff;\n\t\tenc_keys |= BIT_ULL(FLOW_DISSECTOR_KEY_ENC_IPV4_ADDRS);\n\t\tkey->enc_control.addr_type = FLOW_DISSECTOR_KEY_IPV4_ADDRS;\n\t} else {\n\t\tmemcpy(&key->enc_ipv6.src, &tun_info->key.u.ipv6.dst,\n\t\t       sizeof(struct in6_addr));\n\t\tmemcpy(&key->enc_ipv6.dst, &tun_info->key.u.ipv6.src,\n\t\t       sizeof(struct in6_addr));\n\t\tif (memcmp(&key->enc_ipv6.src, &in6addr_any,\n\t\t\t   sizeof(struct in6_addr)))\n\t\t\tmemset(&mask->enc_ipv6.src, 0xff,\n\t\t\t       sizeof(struct in6_addr));\n\t\tif (memcmp(&key->enc_ipv6.dst, &in6addr_any,\n\t\t\t   sizeof(struct in6_addr)))\n\t\t\tmemset(&mask->enc_ipv6.dst, 0xff,\n\t\t\t       sizeof(struct in6_addr));\n\t\tenc_keys |= BIT_ULL(FLOW_DISSECTOR_KEY_ENC_IPV6_ADDRS);\n\t\tkey->enc_control.addr_type = FLOW_DISSECTOR_KEY_IPV6_ADDRS;\n\t}\n\n\tmatch->dissector.used_keys |= enc_keys;\n}\n\nstatic void nf_flow_rule_vlan_match(struct flow_dissector_key_vlan *key,\n\t\t\t\t    struct flow_dissector_key_vlan *mask,\n\t\t\t\t    u16 vlan_id, __be16 proto)\n{\n\tkey->vlan_id = vlan_id;\n\tmask->vlan_id = VLAN_VID_MASK;\n\tkey->vlan_tpid = proto;\n\tmask->vlan_tpid = 0xffff;\n}\n\nstatic int nf_flow_rule_match(struct nf_flow_match *match,\n\t\t\t      const struct flow_offload_tuple *tuple,\n\t\t\t      struct dst_entry *other_dst)\n{\n\tstruct nf_flow_key *mask = &match->mask;\n\tstruct nf_flow_key *key = &match->key;\n\tstruct ip_tunnel_info *tun_info;\n\tbool vlan_encap = false;\n\n\tNF_FLOW_DISSECTOR(match, FLOW_DISSECTOR_KEY_META, meta);\n\tNF_FLOW_DISSECTOR(match, FLOW_DISSECTOR_KEY_CONTROL, control);\n\tNF_FLOW_DISSECTOR(match, FLOW_DISSECTOR_KEY_BASIC, basic);\n\tNF_FLOW_DISSECTOR(match, FLOW_DISSECTOR_KEY_IPV4_ADDRS, ipv4);\n\tNF_FLOW_DISSECTOR(match, FLOW_DISSECTOR_KEY_IPV6_ADDRS, ipv6);\n\tNF_FLOW_DISSECTOR(match, FLOW_DISSECTOR_KEY_TCP, tcp);\n\tNF_FLOW_DISSECTOR(match, FLOW_DISSECTOR_KEY_PORTS, tp);\n\n\tif (other_dst && other_dst->lwtstate) {\n\t\ttun_info = lwt_tun_info(other_dst->lwtstate);\n\t\tnf_flow_rule_lwt_match(match, tun_info);\n\t}\n\n\tif (tuple->xmit_type == FLOW_OFFLOAD_XMIT_TC)\n\t\tkey->meta.ingress_ifindex = tuple->tc.iifidx;\n\telse\n\t\tkey->meta.ingress_ifindex = tuple->iifidx;\n\n\tmask->meta.ingress_ifindex = 0xffffffff;\n\n\tif (tuple->encap_num > 0 && !(tuple->in_vlan_ingress & BIT(0)) &&\n\t    tuple->encap[0].proto == htons(ETH_P_8021Q)) {\n\t\tNF_FLOW_DISSECTOR(match, FLOW_DISSECTOR_KEY_VLAN, vlan);\n\t\tnf_flow_rule_vlan_match(&key->vlan, &mask->vlan,\n\t\t\t\t\ttuple->encap[0].id,\n\t\t\t\t\ttuple->encap[0].proto);\n\t\tvlan_encap = true;\n\t}\n\n\tif (tuple->encap_num > 1 && !(tuple->in_vlan_ingress & BIT(1)) &&\n\t    tuple->encap[1].proto == htons(ETH_P_8021Q)) {\n\t\tif (vlan_encap) {\n\t\t\tNF_FLOW_DISSECTOR(match, FLOW_DISSECTOR_KEY_CVLAN,\n\t\t\t\t\t  cvlan);\n\t\t\tnf_flow_rule_vlan_match(&key->cvlan, &mask->cvlan,\n\t\t\t\t\t\ttuple->encap[1].id,\n\t\t\t\t\t\ttuple->encap[1].proto);\n\t\t} else {\n\t\t\tNF_FLOW_DISSECTOR(match, FLOW_DISSECTOR_KEY_VLAN,\n\t\t\t\t\t  vlan);\n\t\t\tnf_flow_rule_vlan_match(&key->vlan, &mask->vlan,\n\t\t\t\t\t\ttuple->encap[1].id,\n\t\t\t\t\t\ttuple->encap[1].proto);\n\t\t}\n\t}\n\n\tswitch (tuple->l3proto) {\n\tcase AF_INET:\n\t\tkey->control.addr_type = FLOW_DISSECTOR_KEY_IPV4_ADDRS;\n\t\tkey->basic.n_proto = htons(ETH_P_IP);\n\t\tkey->ipv4.src = tuple->src_v4.s_addr;\n\t\tmask->ipv4.src = 0xffffffff;\n\t\tkey->ipv4.dst = tuple->dst_v4.s_addr;\n\t\tmask->ipv4.dst = 0xffffffff;\n\t\tbreak;\n       case AF_INET6:\n\t\tkey->control.addr_type = FLOW_DISSECTOR_KEY_IPV6_ADDRS;\n\t\tkey->basic.n_proto = htons(ETH_P_IPV6);\n\t\tkey->ipv6.src = tuple->src_v6;\n\t\tmemset(&mask->ipv6.src, 0xff, sizeof(mask->ipv6.src));\n\t\tkey->ipv6.dst = tuple->dst_v6;\n\t\tmemset(&mask->ipv6.dst, 0xff, sizeof(mask->ipv6.dst));\n\t\tbreak;\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n\tmask->control.addr_type = 0xffff;\n\tmatch->dissector.used_keys |= BIT_ULL(key->control.addr_type);\n\tmask->basic.n_proto = 0xffff;\n\n\tswitch (tuple->l4proto) {\n\tcase IPPROTO_TCP:\n\t\tkey->tcp.flags = 0;\n\t\tmask->tcp.flags = cpu_to_be16(be32_to_cpu(TCP_FLAG_RST | TCP_FLAG_FIN) >> 16);\n\t\tmatch->dissector.used_keys |= BIT_ULL(FLOW_DISSECTOR_KEY_TCP);\n\t\tbreak;\n\tcase IPPROTO_UDP:\n\tcase IPPROTO_GRE:\n\t\tbreak;\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tkey->basic.ip_proto = tuple->l4proto;\n\tmask->basic.ip_proto = 0xff;\n\n\tmatch->dissector.used_keys |= BIT_ULL(FLOW_DISSECTOR_KEY_META) |\n\t\t\t\t      BIT_ULL(FLOW_DISSECTOR_KEY_CONTROL) |\n\t\t\t\t      BIT_ULL(FLOW_DISSECTOR_KEY_BASIC);\n\n\tswitch (tuple->l4proto) {\n\tcase IPPROTO_TCP:\n\tcase IPPROTO_UDP:\n\t\tkey->tp.src = tuple->src_port;\n\t\tmask->tp.src = 0xffff;\n\t\tkey->tp.dst = tuple->dst_port;\n\t\tmask->tp.dst = 0xffff;\n\n\t\tmatch->dissector.used_keys |= BIT_ULL(FLOW_DISSECTOR_KEY_PORTS);\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic void flow_offload_mangle(struct flow_action_entry *entry,\n\t\t\t\tenum flow_action_mangle_base htype, u32 offset,\n\t\t\t\tconst __be32 *value, const __be32 *mask)\n{\n\tentry->id = FLOW_ACTION_MANGLE;\n\tentry->mangle.htype = htype;\n\tentry->mangle.offset = offset;\n\tmemcpy(&entry->mangle.mask, mask, sizeof(u32));\n\tmemcpy(&entry->mangle.val, value, sizeof(u32));\n}\n\nstatic inline struct flow_action_entry *\nflow_action_entry_next(struct nf_flow_rule *flow_rule)\n{\n\tint i = flow_rule->rule->action.num_entries++;\n\n\treturn &flow_rule->rule->action.entries[i];\n}\n\nstatic int flow_offload_eth_src(struct net *net,\n\t\t\t\tconst struct flow_offload *flow,\n\t\t\t\tenum flow_offload_tuple_dir dir,\n\t\t\t\tstruct nf_flow_rule *flow_rule)\n{\n\tstruct flow_action_entry *entry0 = flow_action_entry_next(flow_rule);\n\tstruct flow_action_entry *entry1 = flow_action_entry_next(flow_rule);\n\tconst struct flow_offload_tuple *other_tuple, *this_tuple;\n\tstruct net_device *dev = NULL;\n\tconst unsigned char *addr;\n\tu32 mask, val;\n\tu16 val16;\n\n\tthis_tuple = &flow->tuplehash[dir].tuple;\n\n\tswitch (this_tuple->xmit_type) {\n\tcase FLOW_OFFLOAD_XMIT_DIRECT:\n\t\taddr = this_tuple->out.h_source;\n\t\tbreak;\n\tcase FLOW_OFFLOAD_XMIT_NEIGH:\n\t\tother_tuple = &flow->tuplehash[!dir].tuple;\n\t\tdev = dev_get_by_index(net, other_tuple->iifidx);\n\t\tif (!dev)\n\t\t\treturn -ENOENT;\n\n\t\taddr = dev->dev_addr;\n\t\tbreak;\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tmask = ~0xffff0000;\n\tmemcpy(&val16, addr, 2);\n\tval = val16 << 16;\n\tflow_offload_mangle(entry0, FLOW_ACT_MANGLE_HDR_TYPE_ETH, 4,\n\t\t\t    &val, &mask);\n\n\tmask = ~0xffffffff;\n\tmemcpy(&val, addr + 2, 4);\n\tflow_offload_mangle(entry1, FLOW_ACT_MANGLE_HDR_TYPE_ETH, 8,\n\t\t\t    &val, &mask);\n\n\tdev_put(dev);\n\n\treturn 0;\n}\n\nstatic int flow_offload_eth_dst(struct net *net,\n\t\t\t\tconst struct flow_offload *flow,\n\t\t\t\tenum flow_offload_tuple_dir dir,\n\t\t\t\tstruct nf_flow_rule *flow_rule)\n{\n\tstruct flow_action_entry *entry0 = flow_action_entry_next(flow_rule);\n\tstruct flow_action_entry *entry1 = flow_action_entry_next(flow_rule);\n\tconst struct flow_offload_tuple *other_tuple, *this_tuple;\n\tconst struct dst_entry *dst_cache;\n\tunsigned char ha[ETH_ALEN];\n\tstruct neighbour *n;\n\tconst void *daddr;\n\tu32 mask, val;\n\tu8 nud_state;\n\tu16 val16;\n\n\tthis_tuple = &flow->tuplehash[dir].tuple;\n\n\tswitch (this_tuple->xmit_type) {\n\tcase FLOW_OFFLOAD_XMIT_DIRECT:\n\t\tether_addr_copy(ha, this_tuple->out.h_dest);\n\t\tbreak;\n\tcase FLOW_OFFLOAD_XMIT_NEIGH:\n\t\tother_tuple = &flow->tuplehash[!dir].tuple;\n\t\tdaddr = &other_tuple->src_v4;\n\t\tdst_cache = this_tuple->dst_cache;\n\t\tn = dst_neigh_lookup(dst_cache, daddr);\n\t\tif (!n)\n\t\t\treturn -ENOENT;\n\n\t\tread_lock_bh(&n->lock);\n\t\tnud_state = n->nud_state;\n\t\tether_addr_copy(ha, n->ha);\n\t\tread_unlock_bh(&n->lock);\n\t\tneigh_release(n);\n\n\t\tif (!(nud_state & NUD_VALID))\n\t\t\treturn -ENOENT;\n\t\tbreak;\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tmask = ~0xffffffff;\n\tmemcpy(&val, ha, 4);\n\tflow_offload_mangle(entry0, FLOW_ACT_MANGLE_HDR_TYPE_ETH, 0,\n\t\t\t    &val, &mask);\n\n\tmask = ~0x0000ffff;\n\tmemcpy(&val16, ha + 4, 2);\n\tval = val16;\n\tflow_offload_mangle(entry1, FLOW_ACT_MANGLE_HDR_TYPE_ETH, 4,\n\t\t\t    &val, &mask);\n\n\treturn 0;\n}\n\nstatic void flow_offload_ipv4_snat(struct net *net,\n\t\t\t\t   const struct flow_offload *flow,\n\t\t\t\t   enum flow_offload_tuple_dir dir,\n\t\t\t\t   struct nf_flow_rule *flow_rule)\n{\n\tstruct flow_action_entry *entry = flow_action_entry_next(flow_rule);\n\tu32 mask = ~htonl(0xffffffff);\n\t__be32 addr;\n\tu32 offset;\n\n\tswitch (dir) {\n\tcase FLOW_OFFLOAD_DIR_ORIGINAL:\n\t\taddr = flow->tuplehash[FLOW_OFFLOAD_DIR_REPLY].tuple.dst_v4.s_addr;\n\t\toffset = offsetof(struct iphdr, saddr);\n\t\tbreak;\n\tcase FLOW_OFFLOAD_DIR_REPLY:\n\t\taddr = flow->tuplehash[FLOW_OFFLOAD_DIR_ORIGINAL].tuple.src_v4.s_addr;\n\t\toffset = offsetof(struct iphdr, daddr);\n\t\tbreak;\n\tdefault:\n\t\treturn;\n\t}\n\n\tflow_offload_mangle(entry, FLOW_ACT_MANGLE_HDR_TYPE_IP4, offset,\n\t\t\t    &addr, &mask);\n}\n\nstatic void flow_offload_ipv4_dnat(struct net *net,\n\t\t\t\t   const struct flow_offload *flow,\n\t\t\t\t   enum flow_offload_tuple_dir dir,\n\t\t\t\t   struct nf_flow_rule *flow_rule)\n{\n\tstruct flow_action_entry *entry = flow_action_entry_next(flow_rule);\n\tu32 mask = ~htonl(0xffffffff);\n\t__be32 addr;\n\tu32 offset;\n\n\tswitch (dir) {\n\tcase FLOW_OFFLOAD_DIR_ORIGINAL:\n\t\taddr = flow->tuplehash[FLOW_OFFLOAD_DIR_REPLY].tuple.src_v4.s_addr;\n\t\toffset = offsetof(struct iphdr, daddr);\n\t\tbreak;\n\tcase FLOW_OFFLOAD_DIR_REPLY:\n\t\taddr = flow->tuplehash[FLOW_OFFLOAD_DIR_ORIGINAL].tuple.dst_v4.s_addr;\n\t\toffset = offsetof(struct iphdr, saddr);\n\t\tbreak;\n\tdefault:\n\t\treturn;\n\t}\n\n\tflow_offload_mangle(entry, FLOW_ACT_MANGLE_HDR_TYPE_IP4, offset,\n\t\t\t    &addr, &mask);\n}\n\nstatic void flow_offload_ipv6_mangle(struct nf_flow_rule *flow_rule,\n\t\t\t\t     unsigned int offset,\n\t\t\t\t     const __be32 *addr, const __be32 *mask)\n{\n\tstruct flow_action_entry *entry;\n\tint i;\n\n\tfor (i = 0; i < sizeof(struct in6_addr) / sizeof(u32); i++) {\n\t\tentry = flow_action_entry_next(flow_rule);\n\t\tflow_offload_mangle(entry, FLOW_ACT_MANGLE_HDR_TYPE_IP6,\n\t\t\t\t    offset + i * sizeof(u32), &addr[i], mask);\n\t}\n}\n\nstatic void flow_offload_ipv6_snat(struct net *net,\n\t\t\t\t   const struct flow_offload *flow,\n\t\t\t\t   enum flow_offload_tuple_dir dir,\n\t\t\t\t   struct nf_flow_rule *flow_rule)\n{\n\tu32 mask = ~htonl(0xffffffff);\n\tconst __be32 *addr;\n\tu32 offset;\n\n\tswitch (dir) {\n\tcase FLOW_OFFLOAD_DIR_ORIGINAL:\n\t\taddr = flow->tuplehash[FLOW_OFFLOAD_DIR_REPLY].tuple.dst_v6.s6_addr32;\n\t\toffset = offsetof(struct ipv6hdr, saddr);\n\t\tbreak;\n\tcase FLOW_OFFLOAD_DIR_REPLY:\n\t\taddr = flow->tuplehash[FLOW_OFFLOAD_DIR_ORIGINAL].tuple.src_v6.s6_addr32;\n\t\toffset = offsetof(struct ipv6hdr, daddr);\n\t\tbreak;\n\tdefault:\n\t\treturn;\n\t}\n\n\tflow_offload_ipv6_mangle(flow_rule, offset, addr, &mask);\n}\n\nstatic void flow_offload_ipv6_dnat(struct net *net,\n\t\t\t\t   const struct flow_offload *flow,\n\t\t\t\t   enum flow_offload_tuple_dir dir,\n\t\t\t\t   struct nf_flow_rule *flow_rule)\n{\n\tu32 mask = ~htonl(0xffffffff);\n\tconst __be32 *addr;\n\tu32 offset;\n\n\tswitch (dir) {\n\tcase FLOW_OFFLOAD_DIR_ORIGINAL:\n\t\taddr = flow->tuplehash[FLOW_OFFLOAD_DIR_REPLY].tuple.src_v6.s6_addr32;\n\t\toffset = offsetof(struct ipv6hdr, daddr);\n\t\tbreak;\n\tcase FLOW_OFFLOAD_DIR_REPLY:\n\t\taddr = flow->tuplehash[FLOW_OFFLOAD_DIR_ORIGINAL].tuple.dst_v6.s6_addr32;\n\t\toffset = offsetof(struct ipv6hdr, saddr);\n\t\tbreak;\n\tdefault:\n\t\treturn;\n\t}\n\n\tflow_offload_ipv6_mangle(flow_rule, offset, addr, &mask);\n}\n\nstatic int flow_offload_l4proto(const struct flow_offload *flow)\n{\n\tu8 protonum = flow->tuplehash[FLOW_OFFLOAD_DIR_ORIGINAL].tuple.l4proto;\n\tu8 type = 0;\n\n\tswitch (protonum) {\n\tcase IPPROTO_TCP:\n\t\ttype = FLOW_ACT_MANGLE_HDR_TYPE_TCP;\n\t\tbreak;\n\tcase IPPROTO_UDP:\n\t\ttype = FLOW_ACT_MANGLE_HDR_TYPE_UDP;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn type;\n}\n\nstatic void flow_offload_port_snat(struct net *net,\n\t\t\t\t   const struct flow_offload *flow,\n\t\t\t\t   enum flow_offload_tuple_dir dir,\n\t\t\t\t   struct nf_flow_rule *flow_rule)\n{\n\tstruct flow_action_entry *entry = flow_action_entry_next(flow_rule);\n\tu32 mask, port;\n\tu32 offset;\n\n\tswitch (dir) {\n\tcase FLOW_OFFLOAD_DIR_ORIGINAL:\n\t\tport = ntohs(flow->tuplehash[FLOW_OFFLOAD_DIR_REPLY].tuple.dst_port);\n\t\toffset = 0;  \n\t\tport = htonl(port << 16);\n\t\tmask = ~htonl(0xffff0000);\n\t\tbreak;\n\tcase FLOW_OFFLOAD_DIR_REPLY:\n\t\tport = ntohs(flow->tuplehash[FLOW_OFFLOAD_DIR_ORIGINAL].tuple.src_port);\n\t\toffset = 0;  \n\t\tport = htonl(port);\n\t\tmask = ~htonl(0xffff);\n\t\tbreak;\n\tdefault:\n\t\treturn;\n\t}\n\n\tflow_offload_mangle(entry, flow_offload_l4proto(flow), offset,\n\t\t\t    &port, &mask);\n}\n\nstatic void flow_offload_port_dnat(struct net *net,\n\t\t\t\t   const struct flow_offload *flow,\n\t\t\t\t   enum flow_offload_tuple_dir dir,\n\t\t\t\t   struct nf_flow_rule *flow_rule)\n{\n\tstruct flow_action_entry *entry = flow_action_entry_next(flow_rule);\n\tu32 mask, port;\n\tu32 offset;\n\n\tswitch (dir) {\n\tcase FLOW_OFFLOAD_DIR_ORIGINAL:\n\t\tport = ntohs(flow->tuplehash[FLOW_OFFLOAD_DIR_REPLY].tuple.src_port);\n\t\toffset = 0;  \n\t\tport = htonl(port);\n\t\tmask = ~htonl(0xffff);\n\t\tbreak;\n\tcase FLOW_OFFLOAD_DIR_REPLY:\n\t\tport = ntohs(flow->tuplehash[FLOW_OFFLOAD_DIR_ORIGINAL].tuple.dst_port);\n\t\toffset = 0;  \n\t\tport = htonl(port << 16);\n\t\tmask = ~htonl(0xffff0000);\n\t\tbreak;\n\tdefault:\n\t\treturn;\n\t}\n\n\tflow_offload_mangle(entry, flow_offload_l4proto(flow), offset,\n\t\t\t    &port, &mask);\n}\n\nstatic void flow_offload_ipv4_checksum(struct net *net,\n\t\t\t\t       const struct flow_offload *flow,\n\t\t\t\t       struct nf_flow_rule *flow_rule)\n{\n\tu8 protonum = flow->tuplehash[FLOW_OFFLOAD_DIR_ORIGINAL].tuple.l4proto;\n\tstruct flow_action_entry *entry = flow_action_entry_next(flow_rule);\n\n\tentry->id = FLOW_ACTION_CSUM;\n\tentry->csum_flags = TCA_CSUM_UPDATE_FLAG_IPV4HDR;\n\n\tswitch (protonum) {\n\tcase IPPROTO_TCP:\n\t\tentry->csum_flags |= TCA_CSUM_UPDATE_FLAG_TCP;\n\t\tbreak;\n\tcase IPPROTO_UDP:\n\t\tentry->csum_flags |= TCA_CSUM_UPDATE_FLAG_UDP;\n\t\tbreak;\n\t}\n}\n\nstatic void flow_offload_redirect(struct net *net,\n\t\t\t\t  const struct flow_offload *flow,\n\t\t\t\t  enum flow_offload_tuple_dir dir,\n\t\t\t\t  struct nf_flow_rule *flow_rule)\n{\n\tconst struct flow_offload_tuple *this_tuple, *other_tuple;\n\tstruct flow_action_entry *entry;\n\tstruct net_device *dev;\n\tint ifindex;\n\n\tthis_tuple = &flow->tuplehash[dir].tuple;\n\tswitch (this_tuple->xmit_type) {\n\tcase FLOW_OFFLOAD_XMIT_DIRECT:\n\t\tthis_tuple = &flow->tuplehash[dir].tuple;\n\t\tifindex = this_tuple->out.hw_ifidx;\n\t\tbreak;\n\tcase FLOW_OFFLOAD_XMIT_NEIGH:\n\t\tother_tuple = &flow->tuplehash[!dir].tuple;\n\t\tifindex = other_tuple->iifidx;\n\t\tbreak;\n\tdefault:\n\t\treturn;\n\t}\n\n\tdev = dev_get_by_index(net, ifindex);\n\tif (!dev)\n\t\treturn;\n\n\tentry = flow_action_entry_next(flow_rule);\n\tentry->id = FLOW_ACTION_REDIRECT;\n\tentry->dev = dev;\n}\n\nstatic void flow_offload_encap_tunnel(const struct flow_offload *flow,\n\t\t\t\t      enum flow_offload_tuple_dir dir,\n\t\t\t\t      struct nf_flow_rule *flow_rule)\n{\n\tconst struct flow_offload_tuple *this_tuple;\n\tstruct flow_action_entry *entry;\n\tstruct dst_entry *dst;\n\n\tthis_tuple = &flow->tuplehash[dir].tuple;\n\tif (this_tuple->xmit_type == FLOW_OFFLOAD_XMIT_DIRECT)\n\t\treturn;\n\n\tdst = this_tuple->dst_cache;\n\tif (dst && dst->lwtstate) {\n\t\tstruct ip_tunnel_info *tun_info;\n\n\t\ttun_info = lwt_tun_info(dst->lwtstate);\n\t\tif (tun_info && (tun_info->mode & IP_TUNNEL_INFO_TX)) {\n\t\t\tentry = flow_action_entry_next(flow_rule);\n\t\t\tentry->id = FLOW_ACTION_TUNNEL_ENCAP;\n\t\t\tentry->tunnel = tun_info;\n\t\t}\n\t}\n}\n\nstatic void flow_offload_decap_tunnel(const struct flow_offload *flow,\n\t\t\t\t      enum flow_offload_tuple_dir dir,\n\t\t\t\t      struct nf_flow_rule *flow_rule)\n{\n\tconst struct flow_offload_tuple *other_tuple;\n\tstruct flow_action_entry *entry;\n\tstruct dst_entry *dst;\n\n\tother_tuple = &flow->tuplehash[!dir].tuple;\n\tif (other_tuple->xmit_type == FLOW_OFFLOAD_XMIT_DIRECT)\n\t\treturn;\n\n\tdst = other_tuple->dst_cache;\n\tif (dst && dst->lwtstate) {\n\t\tstruct ip_tunnel_info *tun_info;\n\n\t\ttun_info = lwt_tun_info(dst->lwtstate);\n\t\tif (tun_info && (tun_info->mode & IP_TUNNEL_INFO_TX)) {\n\t\t\tentry = flow_action_entry_next(flow_rule);\n\t\t\tentry->id = FLOW_ACTION_TUNNEL_DECAP;\n\t\t}\n\t}\n}\n\nstatic int\nnf_flow_rule_route_common(struct net *net, const struct flow_offload *flow,\n\t\t\t  enum flow_offload_tuple_dir dir,\n\t\t\t  struct nf_flow_rule *flow_rule)\n{\n\tconst struct flow_offload_tuple *other_tuple;\n\tconst struct flow_offload_tuple *tuple;\n\tint i;\n\n\tflow_offload_decap_tunnel(flow, dir, flow_rule);\n\tflow_offload_encap_tunnel(flow, dir, flow_rule);\n\n\tif (flow_offload_eth_src(net, flow, dir, flow_rule) < 0 ||\n\t    flow_offload_eth_dst(net, flow, dir, flow_rule) < 0)\n\t\treturn -1;\n\n\ttuple = &flow->tuplehash[dir].tuple;\n\n\tfor (i = 0; i < tuple->encap_num; i++) {\n\t\tstruct flow_action_entry *entry;\n\n\t\tif (tuple->in_vlan_ingress & BIT(i))\n\t\t\tcontinue;\n\n\t\tif (tuple->encap[i].proto == htons(ETH_P_8021Q)) {\n\t\t\tentry = flow_action_entry_next(flow_rule);\n\t\t\tentry->id = FLOW_ACTION_VLAN_POP;\n\t\t}\n\t}\n\n\tother_tuple = &flow->tuplehash[!dir].tuple;\n\n\tfor (i = 0; i < other_tuple->encap_num; i++) {\n\t\tstruct flow_action_entry *entry;\n\n\t\tif (other_tuple->in_vlan_ingress & BIT(i))\n\t\t\tcontinue;\n\n\t\tentry = flow_action_entry_next(flow_rule);\n\n\t\tswitch (other_tuple->encap[i].proto) {\n\t\tcase htons(ETH_P_PPP_SES):\n\t\t\tentry->id = FLOW_ACTION_PPPOE_PUSH;\n\t\t\tentry->pppoe.sid = other_tuple->encap[i].id;\n\t\t\tbreak;\n\t\tcase htons(ETH_P_8021Q):\n\t\t\tentry->id = FLOW_ACTION_VLAN_PUSH;\n\t\t\tentry->vlan.vid = other_tuple->encap[i].id;\n\t\t\tentry->vlan.proto = other_tuple->encap[i].proto;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nint nf_flow_rule_route_ipv4(struct net *net, struct flow_offload *flow,\n\t\t\t    enum flow_offload_tuple_dir dir,\n\t\t\t    struct nf_flow_rule *flow_rule)\n{\n\tif (nf_flow_rule_route_common(net, flow, dir, flow_rule) < 0)\n\t\treturn -1;\n\n\tif (test_bit(NF_FLOW_SNAT, &flow->flags)) {\n\t\tflow_offload_ipv4_snat(net, flow, dir, flow_rule);\n\t\tflow_offload_port_snat(net, flow, dir, flow_rule);\n\t}\n\tif (test_bit(NF_FLOW_DNAT, &flow->flags)) {\n\t\tflow_offload_ipv4_dnat(net, flow, dir, flow_rule);\n\t\tflow_offload_port_dnat(net, flow, dir, flow_rule);\n\t}\n\tif (test_bit(NF_FLOW_SNAT, &flow->flags) ||\n\t    test_bit(NF_FLOW_DNAT, &flow->flags))\n\t\tflow_offload_ipv4_checksum(net, flow, flow_rule);\n\n\tflow_offload_redirect(net, flow, dir, flow_rule);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(nf_flow_rule_route_ipv4);\n\nint nf_flow_rule_route_ipv6(struct net *net, struct flow_offload *flow,\n\t\t\t    enum flow_offload_tuple_dir dir,\n\t\t\t    struct nf_flow_rule *flow_rule)\n{\n\tif (nf_flow_rule_route_common(net, flow, dir, flow_rule) < 0)\n\t\treturn -1;\n\n\tif (test_bit(NF_FLOW_SNAT, &flow->flags)) {\n\t\tflow_offload_ipv6_snat(net, flow, dir, flow_rule);\n\t\tflow_offload_port_snat(net, flow, dir, flow_rule);\n\t}\n\tif (test_bit(NF_FLOW_DNAT, &flow->flags)) {\n\t\tflow_offload_ipv6_dnat(net, flow, dir, flow_rule);\n\t\tflow_offload_port_dnat(net, flow, dir, flow_rule);\n\t}\n\n\tflow_offload_redirect(net, flow, dir, flow_rule);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(nf_flow_rule_route_ipv6);\n\n#define NF_FLOW_RULE_ACTION_MAX\t16\n\nstatic struct nf_flow_rule *\nnf_flow_offload_rule_alloc(struct net *net,\n\t\t\t   const struct flow_offload_work *offload,\n\t\t\t   enum flow_offload_tuple_dir dir)\n{\n\tconst struct nf_flowtable *flowtable = offload->flowtable;\n\tconst struct flow_offload_tuple *tuple, *other_tuple;\n\tstruct flow_offload *flow = offload->flow;\n\tstruct dst_entry *other_dst = NULL;\n\tstruct nf_flow_rule *flow_rule;\n\tint err = -ENOMEM;\n\n\tflow_rule = kzalloc(sizeof(*flow_rule), GFP_KERNEL);\n\tif (!flow_rule)\n\t\tgoto err_flow;\n\n\tflow_rule->rule = flow_rule_alloc(NF_FLOW_RULE_ACTION_MAX);\n\tif (!flow_rule->rule)\n\t\tgoto err_flow_rule;\n\n\tflow_rule->rule->match.dissector = &flow_rule->match.dissector;\n\tflow_rule->rule->match.mask = &flow_rule->match.mask;\n\tflow_rule->rule->match.key = &flow_rule->match.key;\n\n\ttuple = &flow->tuplehash[dir].tuple;\n\tother_tuple = &flow->tuplehash[!dir].tuple;\n\tif (other_tuple->xmit_type == FLOW_OFFLOAD_XMIT_NEIGH)\n\t\tother_dst = other_tuple->dst_cache;\n\n\terr = nf_flow_rule_match(&flow_rule->match, tuple, other_dst);\n\tif (err < 0)\n\t\tgoto err_flow_match;\n\n\tflow_rule->rule->action.num_entries = 0;\n\tif (flowtable->type->action(net, flow, dir, flow_rule) < 0)\n\t\tgoto err_flow_match;\n\n\treturn flow_rule;\n\nerr_flow_match:\n\tkfree(flow_rule->rule);\nerr_flow_rule:\n\tkfree(flow_rule);\nerr_flow:\n\treturn NULL;\n}\n\nstatic void __nf_flow_offload_destroy(struct nf_flow_rule *flow_rule)\n{\n\tstruct flow_action_entry *entry;\n\tint i;\n\n\tfor (i = 0; i < flow_rule->rule->action.num_entries; i++) {\n\t\tentry = &flow_rule->rule->action.entries[i];\n\t\tif (entry->id != FLOW_ACTION_REDIRECT)\n\t\t\tcontinue;\n\n\t\tdev_put(entry->dev);\n\t}\n\tkfree(flow_rule->rule);\n\tkfree(flow_rule);\n}\n\nstatic void nf_flow_offload_destroy(struct nf_flow_rule *flow_rule[])\n{\n\tint i;\n\n\tfor (i = 0; i < FLOW_OFFLOAD_DIR_MAX; i++)\n\t\t__nf_flow_offload_destroy(flow_rule[i]);\n}\n\nstatic int nf_flow_offload_alloc(const struct flow_offload_work *offload,\n\t\t\t\t struct nf_flow_rule *flow_rule[])\n{\n\tstruct net *net = read_pnet(&offload->flowtable->net);\n\n\tflow_rule[0] = nf_flow_offload_rule_alloc(net, offload,\n\t\t\t\t\t\t  FLOW_OFFLOAD_DIR_ORIGINAL);\n\tif (!flow_rule[0])\n\t\treturn -ENOMEM;\n\n\tflow_rule[1] = nf_flow_offload_rule_alloc(net, offload,\n\t\t\t\t\t\t  FLOW_OFFLOAD_DIR_REPLY);\n\tif (!flow_rule[1]) {\n\t\t__nf_flow_offload_destroy(flow_rule[0]);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic void nf_flow_offload_init(struct flow_cls_offload *cls_flow,\n\t\t\t\t __be16 proto, int priority,\n\t\t\t\t enum flow_cls_command cmd,\n\t\t\t\t const struct flow_offload_tuple *tuple,\n\t\t\t\t struct netlink_ext_ack *extack)\n{\n\tcls_flow->common.protocol = proto;\n\tcls_flow->common.prio = priority;\n\tcls_flow->common.extack = extack;\n\tcls_flow->command = cmd;\n\tcls_flow->cookie = (unsigned long)tuple;\n}\n\nstatic int nf_flow_offload_tuple(struct nf_flowtable *flowtable,\n\t\t\t\t struct flow_offload *flow,\n\t\t\t\t struct nf_flow_rule *flow_rule,\n\t\t\t\t enum flow_offload_tuple_dir dir,\n\t\t\t\t int priority, int cmd,\n\t\t\t\t struct flow_stats *stats,\n\t\t\t\t struct list_head *block_cb_list)\n{\n\tstruct flow_cls_offload cls_flow = {};\n\tstruct flow_block_cb *block_cb;\n\tstruct netlink_ext_ack extack;\n\t__be16 proto = ETH_P_ALL;\n\tint err, i = 0;\n\n\tnf_flow_offload_init(&cls_flow, proto, priority, cmd,\n\t\t\t     &flow->tuplehash[dir].tuple, &extack);\n\tif (cmd == FLOW_CLS_REPLACE)\n\t\tcls_flow.rule = flow_rule->rule;\n\n\tdown_read(&flowtable->flow_block_lock);\n\tlist_for_each_entry(block_cb, block_cb_list, list) {\n\t\terr = block_cb->cb(TC_SETUP_CLSFLOWER, &cls_flow,\n\t\t\t\t   block_cb->cb_priv);\n\t\tif (err < 0)\n\t\t\tcontinue;\n\n\t\ti++;\n\t}\n\tup_read(&flowtable->flow_block_lock);\n\n\tif (cmd == FLOW_CLS_STATS)\n\t\tmemcpy(stats, &cls_flow.stats, sizeof(*stats));\n\n\treturn i;\n}\n\nstatic int flow_offload_tuple_add(struct flow_offload_work *offload,\n\t\t\t\t  struct nf_flow_rule *flow_rule,\n\t\t\t\t  enum flow_offload_tuple_dir dir)\n{\n\treturn nf_flow_offload_tuple(offload->flowtable, offload->flow,\n\t\t\t\t     flow_rule, dir,\n\t\t\t\t     offload->flowtable->priority,\n\t\t\t\t     FLOW_CLS_REPLACE, NULL,\n\t\t\t\t     &offload->flowtable->flow_block.cb_list);\n}\n\nstatic void flow_offload_tuple_del(struct flow_offload_work *offload,\n\t\t\t\t   enum flow_offload_tuple_dir dir)\n{\n\tnf_flow_offload_tuple(offload->flowtable, offload->flow, NULL, dir,\n\t\t\t      offload->flowtable->priority,\n\t\t\t      FLOW_CLS_DESTROY, NULL,\n\t\t\t      &offload->flowtable->flow_block.cb_list);\n}\n\nstatic int flow_offload_rule_add(struct flow_offload_work *offload,\n\t\t\t\t struct nf_flow_rule *flow_rule[])\n{\n\tint ok_count = 0;\n\n\tok_count += flow_offload_tuple_add(offload, flow_rule[0],\n\t\t\t\t\t   FLOW_OFFLOAD_DIR_ORIGINAL);\n\tif (test_bit(NF_FLOW_HW_BIDIRECTIONAL, &offload->flow->flags))\n\t\tok_count += flow_offload_tuple_add(offload, flow_rule[1],\n\t\t\t\t\t\t   FLOW_OFFLOAD_DIR_REPLY);\n\tif (ok_count == 0)\n\t\treturn -ENOENT;\n\n\treturn 0;\n}\n\nstatic void flow_offload_work_add(struct flow_offload_work *offload)\n{\n\tstruct nf_flow_rule *flow_rule[FLOW_OFFLOAD_DIR_MAX];\n\tint err;\n\n\terr = nf_flow_offload_alloc(offload, flow_rule);\n\tif (err < 0)\n\t\treturn;\n\n\terr = flow_offload_rule_add(offload, flow_rule);\n\tif (err < 0)\n\t\tgoto out;\n\n\tset_bit(IPS_HW_OFFLOAD_BIT, &offload->flow->ct->status);\n\nout:\n\tnf_flow_offload_destroy(flow_rule);\n}\n\nstatic void flow_offload_work_del(struct flow_offload_work *offload)\n{\n\tclear_bit(IPS_HW_OFFLOAD_BIT, &offload->flow->ct->status);\n\tflow_offload_tuple_del(offload, FLOW_OFFLOAD_DIR_ORIGINAL);\n\tif (test_bit(NF_FLOW_HW_BIDIRECTIONAL, &offload->flow->flags))\n\t\tflow_offload_tuple_del(offload, FLOW_OFFLOAD_DIR_REPLY);\n\tset_bit(NF_FLOW_HW_DEAD, &offload->flow->flags);\n}\n\nstatic void flow_offload_tuple_stats(struct flow_offload_work *offload,\n\t\t\t\t     enum flow_offload_tuple_dir dir,\n\t\t\t\t     struct flow_stats *stats)\n{\n\tnf_flow_offload_tuple(offload->flowtable, offload->flow, NULL, dir,\n\t\t\t      offload->flowtable->priority,\n\t\t\t      FLOW_CLS_STATS, stats,\n\t\t\t      &offload->flowtable->flow_block.cb_list);\n}\n\nstatic void flow_offload_work_stats(struct flow_offload_work *offload)\n{\n\tstruct flow_stats stats[FLOW_OFFLOAD_DIR_MAX] = {};\n\tu64 lastused;\n\n\tflow_offload_tuple_stats(offload, FLOW_OFFLOAD_DIR_ORIGINAL, &stats[0]);\n\tif (test_bit(NF_FLOW_HW_BIDIRECTIONAL, &offload->flow->flags))\n\t\tflow_offload_tuple_stats(offload, FLOW_OFFLOAD_DIR_REPLY,\n\t\t\t\t\t &stats[1]);\n\n\tlastused = max_t(u64, stats[0].lastused, stats[1].lastused);\n\toffload->flow->timeout = max_t(u64, offload->flow->timeout,\n\t\t\t\t       lastused + flow_offload_get_timeout(offload->flow));\n\n\tif (offload->flowtable->flags & NF_FLOWTABLE_COUNTER) {\n\t\tif (stats[0].pkts)\n\t\t\tnf_ct_acct_add(offload->flow->ct,\n\t\t\t\t       FLOW_OFFLOAD_DIR_ORIGINAL,\n\t\t\t\t       stats[0].pkts, stats[0].bytes);\n\t\tif (stats[1].pkts)\n\t\t\tnf_ct_acct_add(offload->flow->ct,\n\t\t\t\t       FLOW_OFFLOAD_DIR_REPLY,\n\t\t\t\t       stats[1].pkts, stats[1].bytes);\n\t}\n}\n\nstatic void flow_offload_work_handler(struct work_struct *work)\n{\n\tstruct flow_offload_work *offload;\n\tstruct net *net;\n\n\toffload = container_of(work, struct flow_offload_work, work);\n\tnet = read_pnet(&offload->flowtable->net);\n\tswitch (offload->cmd) {\n\t\tcase FLOW_CLS_REPLACE:\n\t\t\tflow_offload_work_add(offload);\n\t\t\tNF_FLOW_TABLE_STAT_DEC_ATOMIC(net, count_wq_add);\n\t\t\tbreak;\n\t\tcase FLOW_CLS_DESTROY:\n\t\t\tflow_offload_work_del(offload);\n\t\t\tNF_FLOW_TABLE_STAT_DEC_ATOMIC(net, count_wq_del);\n\t\t\tbreak;\n\t\tcase FLOW_CLS_STATS:\n\t\t\tflow_offload_work_stats(offload);\n\t\t\tNF_FLOW_TABLE_STAT_DEC_ATOMIC(net, count_wq_stats);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tWARN_ON_ONCE(1);\n\t}\n\n\tclear_bit(NF_FLOW_HW_PENDING, &offload->flow->flags);\n\tkfree(offload);\n}\n\nstatic void flow_offload_queue_work(struct flow_offload_work *offload)\n{\n\tstruct net *net = read_pnet(&offload->flowtable->net);\n\n\tif (offload->cmd == FLOW_CLS_REPLACE) {\n\t\tNF_FLOW_TABLE_STAT_INC_ATOMIC(net, count_wq_add);\n\t\tqueue_work(nf_flow_offload_add_wq, &offload->work);\n\t} else if (offload->cmd == FLOW_CLS_DESTROY) {\n\t\tNF_FLOW_TABLE_STAT_INC_ATOMIC(net, count_wq_del);\n\t\tqueue_work(nf_flow_offload_del_wq, &offload->work);\n\t} else {\n\t\tNF_FLOW_TABLE_STAT_INC_ATOMIC(net, count_wq_stats);\n\t\tqueue_work(nf_flow_offload_stats_wq, &offload->work);\n\t}\n}\n\nstatic struct flow_offload_work *\nnf_flow_offload_work_alloc(struct nf_flowtable *flowtable,\n\t\t\t   struct flow_offload *flow, unsigned int cmd)\n{\n\tstruct flow_offload_work *offload;\n\n\tif (test_and_set_bit(NF_FLOW_HW_PENDING, &flow->flags))\n\t\treturn NULL;\n\n\toffload = kmalloc(sizeof(struct flow_offload_work), GFP_ATOMIC);\n\tif (!offload) {\n\t\tclear_bit(NF_FLOW_HW_PENDING, &flow->flags);\n\t\treturn NULL;\n\t}\n\n\toffload->cmd = cmd;\n\toffload->flow = flow;\n\toffload->flowtable = flowtable;\n\tINIT_WORK(&offload->work, flow_offload_work_handler);\n\n\treturn offload;\n}\n\n\nvoid nf_flow_offload_add(struct nf_flowtable *flowtable,\n\t\t\t struct flow_offload *flow)\n{\n\tstruct flow_offload_work *offload;\n\n\toffload = nf_flow_offload_work_alloc(flowtable, flow, FLOW_CLS_REPLACE);\n\tif (!offload)\n\t\treturn;\n\n\tflow_offload_queue_work(offload);\n}\n\nvoid nf_flow_offload_del(struct nf_flowtable *flowtable,\n\t\t\t struct flow_offload *flow)\n{\n\tstruct flow_offload_work *offload;\n\n\toffload = nf_flow_offload_work_alloc(flowtable, flow, FLOW_CLS_DESTROY);\n\tif (!offload)\n\t\treturn;\n\n\tset_bit(NF_FLOW_HW_DYING, &flow->flags);\n\tflow_offload_queue_work(offload);\n}\n\nvoid nf_flow_offload_stats(struct nf_flowtable *flowtable,\n\t\t\t   struct flow_offload *flow)\n{\n\tstruct flow_offload_work *offload;\n\t__s32 delta;\n\n\tdelta = nf_flow_timeout_delta(flow->timeout);\n\tif ((delta >= (9 * flow_offload_get_timeout(flow)) / 10))\n\t\treturn;\n\n\toffload = nf_flow_offload_work_alloc(flowtable, flow, FLOW_CLS_STATS);\n\tif (!offload)\n\t\treturn;\n\n\tflow_offload_queue_work(offload);\n}\n\nvoid nf_flow_table_offload_flush_cleanup(struct nf_flowtable *flowtable)\n{\n\tif (nf_flowtable_hw_offload(flowtable)) {\n\t\tflush_workqueue(nf_flow_offload_del_wq);\n\t\tnf_flow_table_gc_run(flowtable);\n\t}\n}\n\nvoid nf_flow_table_offload_flush(struct nf_flowtable *flowtable)\n{\n\tif (nf_flowtable_hw_offload(flowtable)) {\n\t\tflush_workqueue(nf_flow_offload_add_wq);\n\t\tflush_workqueue(nf_flow_offload_del_wq);\n\t\tflush_workqueue(nf_flow_offload_stats_wq);\n\t}\n}\n\nstatic int nf_flow_table_block_setup(struct nf_flowtable *flowtable,\n\t\t\t\t     struct flow_block_offload *bo,\n\t\t\t\t     enum flow_block_command cmd)\n{\n\tstruct flow_block_cb *block_cb, *next;\n\tint err = 0;\n\n\tdown_write(&flowtable->flow_block_lock);\n\tswitch (cmd) {\n\tcase FLOW_BLOCK_BIND:\n\t\tlist_splice(&bo->cb_list, &flowtable->flow_block.cb_list);\n\t\tbreak;\n\tcase FLOW_BLOCK_UNBIND:\n\t\tlist_for_each_entry_safe(block_cb, next, &bo->cb_list, list) {\n\t\t\tlist_del(&block_cb->list);\n\t\t\tflow_block_cb_free(block_cb);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\terr = -EOPNOTSUPP;\n\t}\n\tup_write(&flowtable->flow_block_lock);\n\n\treturn err;\n}\n\nstatic void nf_flow_table_block_offload_init(struct flow_block_offload *bo,\n\t\t\t\t\t     struct net *net,\n\t\t\t\t\t     enum flow_block_command cmd,\n\t\t\t\t\t     struct nf_flowtable *flowtable,\n\t\t\t\t\t     struct netlink_ext_ack *extack)\n{\n\tmemset(bo, 0, sizeof(*bo));\n\tbo->net\t\t= net;\n\tbo->block\t= &flowtable->flow_block;\n\tbo->command\t= cmd;\n\tbo->binder_type\t= FLOW_BLOCK_BINDER_TYPE_CLSACT_INGRESS;\n\tbo->extack\t= extack;\n\tbo->cb_list_head = &flowtable->flow_block.cb_list;\n\tINIT_LIST_HEAD(&bo->cb_list);\n}\n\nstatic void nf_flow_table_indr_cleanup(struct flow_block_cb *block_cb)\n{\n\tstruct nf_flowtable *flowtable = block_cb->indr.data;\n\tstruct net_device *dev = block_cb->indr.dev;\n\n\tnf_flow_table_gc_cleanup(flowtable, dev);\n\tdown_write(&flowtable->flow_block_lock);\n\tlist_del(&block_cb->list);\n\tlist_del(&block_cb->driver_list);\n\tflow_block_cb_free(block_cb);\n\tup_write(&flowtable->flow_block_lock);\n}\n\nstatic int nf_flow_table_indr_offload_cmd(struct flow_block_offload *bo,\n\t\t\t\t\t  struct nf_flowtable *flowtable,\n\t\t\t\t\t  struct net_device *dev,\n\t\t\t\t\t  enum flow_block_command cmd,\n\t\t\t\t\t  struct netlink_ext_ack *extack)\n{\n\tnf_flow_table_block_offload_init(bo, dev_net(dev), cmd, flowtable,\n\t\t\t\t\t extack);\n\n\treturn flow_indr_dev_setup_offload(dev, NULL, TC_SETUP_FT, flowtable, bo,\n\t\t\t\t\t   nf_flow_table_indr_cleanup);\n}\n\nstatic int nf_flow_table_offload_cmd(struct flow_block_offload *bo,\n\t\t\t\t     struct nf_flowtable *flowtable,\n\t\t\t\t     struct net_device *dev,\n\t\t\t\t     enum flow_block_command cmd,\n\t\t\t\t     struct netlink_ext_ack *extack)\n{\n\tint err;\n\n\tnf_flow_table_block_offload_init(bo, dev_net(dev), cmd, flowtable,\n\t\t\t\t\t extack);\n\tdown_write(&flowtable->flow_block_lock);\n\terr = dev->netdev_ops->ndo_setup_tc(dev, TC_SETUP_FT, bo);\n\tup_write(&flowtable->flow_block_lock);\n\tif (err < 0)\n\t\treturn err;\n\n\treturn 0;\n}\n\nint nf_flow_table_offload_setup(struct nf_flowtable *flowtable,\n\t\t\t\tstruct net_device *dev,\n\t\t\t\tenum flow_block_command cmd)\n{\n\tstruct netlink_ext_ack extack = {};\n\tstruct flow_block_offload bo;\n\tint err;\n\n\tif (!nf_flowtable_hw_offload(flowtable))\n\t\treturn 0;\n\n\tif (dev->netdev_ops->ndo_setup_tc)\n\t\terr = nf_flow_table_offload_cmd(&bo, flowtable, dev, cmd,\n\t\t\t\t\t\t&extack);\n\telse\n\t\terr = nf_flow_table_indr_offload_cmd(&bo, flowtable, dev, cmd,\n\t\t\t\t\t\t     &extack);\n\tif (err < 0)\n\t\treturn err;\n\n\treturn nf_flow_table_block_setup(flowtable, &bo, cmd);\n}\nEXPORT_SYMBOL_GPL(nf_flow_table_offload_setup);\n\nint nf_flow_table_offload_init(void)\n{\n\tnf_flow_offload_add_wq  = alloc_workqueue(\"nf_ft_offload_add\",\n\t\t\t\t\t\t  WQ_UNBOUND | WQ_SYSFS, 0);\n\tif (!nf_flow_offload_add_wq)\n\t\treturn -ENOMEM;\n\n\tnf_flow_offload_del_wq  = alloc_workqueue(\"nf_ft_offload_del\",\n\t\t\t\t\t\t  WQ_UNBOUND | WQ_SYSFS, 0);\n\tif (!nf_flow_offload_del_wq)\n\t\tgoto err_del_wq;\n\n\tnf_flow_offload_stats_wq  = alloc_workqueue(\"nf_ft_offload_stats\",\n\t\t\t\t\t\t    WQ_UNBOUND | WQ_SYSFS, 0);\n\tif (!nf_flow_offload_stats_wq)\n\t\tgoto err_stats_wq;\n\n\treturn 0;\n\nerr_stats_wq:\n\tdestroy_workqueue(nf_flow_offload_del_wq);\nerr_del_wq:\n\tdestroy_workqueue(nf_flow_offload_add_wq);\n\treturn -ENOMEM;\n}\n\nvoid nf_flow_table_offload_exit(void)\n{\n\tdestroy_workqueue(nf_flow_offload_add_wq);\n\tdestroy_workqueue(nf_flow_offload_del_wq);\n\tdestroy_workqueue(nf_flow_offload_stats_wq);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}