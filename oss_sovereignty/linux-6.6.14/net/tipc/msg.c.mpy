{
  "module_name": "msg.c",
  "hash_id": "8a6adeb7edef52b3e069b3cc4f0347ccc0c1e50521cb999c270057b9c32ad2cf",
  "original_prompt": "Ingested from linux-6.6.14/net/tipc/msg.c",
  "human_readable_source": " \n\n#include <net/sock.h>\n#include \"core.h\"\n#include \"msg.h\"\n#include \"addr.h\"\n#include \"name_table.h\"\n#include \"crypto.h\"\n\n#define BUF_ALIGN(x) ALIGN(x, 4)\n#define MAX_FORWARD_SIZE 1024\n#ifdef CONFIG_TIPC_CRYPTO\n#define BUF_HEADROOM ALIGN(((LL_MAX_HEADER + 48) + EHDR_MAX_SIZE), 16)\n#define BUF_OVERHEAD (BUF_HEADROOM + TIPC_AES_GCM_TAG_SIZE)\n#else\n#define BUF_HEADROOM (LL_MAX_HEADER + 48)\n#define BUF_OVERHEAD BUF_HEADROOM\n#endif\n\nconst int one_page_mtu = PAGE_SIZE - SKB_DATA_ALIGN(BUF_OVERHEAD) -\n\t\t\t SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\n \nstruct sk_buff *tipc_buf_acquire(u32 size, gfp_t gfp)\n{\n\tstruct sk_buff *skb;\n\n\tskb = alloc_skb_fclone(BUF_OVERHEAD + size, gfp);\n\tif (skb) {\n\t\tskb_reserve(skb, BUF_HEADROOM);\n\t\tskb_put(skb, size);\n\t\tskb->next = NULL;\n\t}\n\treturn skb;\n}\n\nvoid tipc_msg_init(u32 own_node, struct tipc_msg *m, u32 user, u32 type,\n\t\t   u32 hsize, u32 dnode)\n{\n\tmemset(m, 0, hsize);\n\tmsg_set_version(m);\n\tmsg_set_user(m, user);\n\tmsg_set_hdr_sz(m, hsize);\n\tmsg_set_size(m, hsize);\n\tmsg_set_prevnode(m, own_node);\n\tmsg_set_type(m, type);\n\tif (hsize > SHORT_H_SIZE) {\n\t\tmsg_set_orignode(m, own_node);\n\t\tmsg_set_destnode(m, dnode);\n\t}\n}\n\nstruct sk_buff *tipc_msg_create(uint user, uint type,\n\t\t\t\tuint hdr_sz, uint data_sz, u32 dnode,\n\t\t\t\tu32 onode, u32 dport, u32 oport, int errcode)\n{\n\tstruct tipc_msg *msg;\n\tstruct sk_buff *buf;\n\n\tbuf = tipc_buf_acquire(hdr_sz + data_sz, GFP_ATOMIC);\n\tif (unlikely(!buf))\n\t\treturn NULL;\n\n\tmsg = buf_msg(buf);\n\ttipc_msg_init(onode, msg, user, type, hdr_sz, dnode);\n\tmsg_set_size(msg, hdr_sz + data_sz);\n\tmsg_set_origport(msg, oport);\n\tmsg_set_destport(msg, dport);\n\tmsg_set_errcode(msg, errcode);\n\treturn buf;\n}\n\n \nint tipc_buf_append(struct sk_buff **headbuf, struct sk_buff **buf)\n{\n\tstruct sk_buff *head = *headbuf;\n\tstruct sk_buff *frag = *buf;\n\tstruct sk_buff *tail = NULL;\n\tstruct tipc_msg *msg;\n\tu32 fragid;\n\tint delta;\n\tbool headstolen;\n\n\tif (!frag)\n\t\tgoto err;\n\n\tmsg = buf_msg(frag);\n\tfragid = msg_type(msg);\n\tfrag->next = NULL;\n\tskb_pull(frag, msg_hdr_sz(msg));\n\n\tif (fragid == FIRST_FRAGMENT) {\n\t\tif (unlikely(head))\n\t\t\tgoto err;\n\t\t*buf = NULL;\n\t\tif (skb_has_frag_list(frag) && __skb_linearize(frag))\n\t\t\tgoto err;\n\t\tfrag = skb_unshare(frag, GFP_ATOMIC);\n\t\tif (unlikely(!frag))\n\t\t\tgoto err;\n\t\thead = *headbuf = frag;\n\t\tTIPC_SKB_CB(head)->tail = NULL;\n\t\treturn 0;\n\t}\n\n\tif (!head)\n\t\tgoto err;\n\n\tif (skb_try_coalesce(head, frag, &headstolen, &delta)) {\n\t\tkfree_skb_partial(frag, headstolen);\n\t} else {\n\t\ttail = TIPC_SKB_CB(head)->tail;\n\t\tif (!skb_has_frag_list(head))\n\t\t\tskb_shinfo(head)->frag_list = frag;\n\t\telse\n\t\t\ttail->next = frag;\n\t\thead->truesize += frag->truesize;\n\t\thead->data_len += frag->len;\n\t\thead->len += frag->len;\n\t\tTIPC_SKB_CB(head)->tail = frag;\n\t}\n\n\tif (fragid == LAST_FRAGMENT) {\n\t\tTIPC_SKB_CB(head)->validated = 0;\n\t\tif (unlikely(!tipc_msg_validate(&head)))\n\t\t\tgoto err;\n\t\t*buf = head;\n\t\tTIPC_SKB_CB(head)->tail = NULL;\n\t\t*headbuf = NULL;\n\t\treturn 1;\n\t}\n\t*buf = NULL;\n\treturn 0;\nerr:\n\tkfree_skb(*buf);\n\tkfree_skb(*headbuf);\n\t*buf = *headbuf = NULL;\n\treturn 0;\n}\n\n \nint tipc_msg_append(struct tipc_msg *_hdr, struct msghdr *m, int dlen,\n\t\t    int mss, struct sk_buff_head *txq)\n{\n\tstruct sk_buff *skb;\n\tint accounted, total, curr;\n\tint mlen, cpy, rem = dlen;\n\tstruct tipc_msg *hdr;\n\n\tskb = skb_peek_tail(txq);\n\taccounted = skb ? msg_blocks(buf_msg(skb)) : 0;\n\ttotal = accounted;\n\n\tdo {\n\t\tif (!skb || skb->len >= mss) {\n\t\t\tskb = tipc_buf_acquire(mss, GFP_KERNEL);\n\t\t\tif (unlikely(!skb))\n\t\t\t\treturn -ENOMEM;\n\t\t\tskb_orphan(skb);\n\t\t\tskb_trim(skb, MIN_H_SIZE);\n\t\t\thdr = buf_msg(skb);\n\t\t\tskb_copy_to_linear_data(skb, _hdr, MIN_H_SIZE);\n\t\t\tmsg_set_hdr_sz(hdr, MIN_H_SIZE);\n\t\t\tmsg_set_size(hdr, MIN_H_SIZE);\n\t\t\t__skb_queue_tail(txq, skb);\n\t\t\ttotal += 1;\n\t\t}\n\t\thdr = buf_msg(skb);\n\t\tcurr = msg_blocks(hdr);\n\t\tmlen = msg_size(hdr);\n\t\tcpy = min_t(size_t, rem, mss - mlen);\n\t\tif (cpy != copy_from_iter(skb->data + mlen, cpy, &m->msg_iter))\n\t\t\treturn -EFAULT;\n\t\tmsg_set_size(hdr, mlen + cpy);\n\t\tskb_put(skb, cpy);\n\t\trem -= cpy;\n\t\ttotal += msg_blocks(hdr) - curr;\n\t} while (rem > 0);\n\treturn total - accounted;\n}\n\n \nbool tipc_msg_validate(struct sk_buff **_skb)\n{\n\tstruct sk_buff *skb = *_skb;\n\tstruct tipc_msg *hdr;\n\tint msz, hsz;\n\n\t \n\tif (unlikely(skb->truesize / buf_roundup_len(skb) >= 4)) {\n\t\tskb = skb_copy_expand(skb, BUF_HEADROOM, 0, GFP_ATOMIC);\n\t\tif (!skb)\n\t\t\treturn false;\n\t\tkfree_skb(*_skb);\n\t\t*_skb = skb;\n\t}\n\n\tif (unlikely(TIPC_SKB_CB(skb)->validated))\n\t\treturn true;\n\n\tif (unlikely(!pskb_may_pull(skb, MIN_H_SIZE)))\n\t\treturn false;\n\n\thsz = msg_hdr_sz(buf_msg(skb));\n\tif (unlikely(hsz < MIN_H_SIZE) || (hsz > MAX_H_SIZE))\n\t\treturn false;\n\tif (unlikely(!pskb_may_pull(skb, hsz)))\n\t\treturn false;\n\n\thdr = buf_msg(skb);\n\tif (unlikely(msg_version(hdr) != TIPC_VERSION))\n\t\treturn false;\n\n\tmsz = msg_size(hdr);\n\tif (unlikely(msz < hsz))\n\t\treturn false;\n\tif (unlikely((msz - hsz) > TIPC_MAX_USER_MSG_SIZE))\n\t\treturn false;\n\tif (unlikely(skb->len < msz))\n\t\treturn false;\n\n\tTIPC_SKB_CB(skb)->validated = 1;\n\treturn true;\n}\n\n \nint tipc_msg_fragment(struct sk_buff *skb, const struct tipc_msg *hdr,\n\t\t      int pktmax, struct sk_buff_head *frags)\n{\n\tint pktno, nof_fragms, dsz, dmax, eat;\n\tstruct tipc_msg *_hdr;\n\tstruct sk_buff *_skb;\n\tu8 *data;\n\n\t \n\tif (skb_linearize(skb))\n\t\treturn -ENOMEM;\n\n\tdata = (u8 *)skb->data;\n\tdsz = msg_size(buf_msg(skb));\n\tdmax = pktmax - INT_H_SIZE;\n\tif (dsz <= dmax || !dmax)\n\t\treturn -EINVAL;\n\n\tnof_fragms = dsz / dmax + 1;\n\tfor (pktno = 1; pktno <= nof_fragms; pktno++) {\n\t\tif (pktno < nof_fragms)\n\t\t\teat = dmax;\n\t\telse\n\t\t\teat = dsz % dmax;\n\t\t \n\t\t_skb = tipc_buf_acquire(INT_H_SIZE + eat, GFP_ATOMIC);\n\t\tif (!_skb)\n\t\t\tgoto error;\n\t\tskb_orphan(_skb);\n\t\t__skb_queue_tail(frags, _skb);\n\t\t \n\t\tskb_copy_to_linear_data(_skb, hdr, INT_H_SIZE);\n\t\tskb_copy_to_linear_data_offset(_skb, INT_H_SIZE, data, eat);\n\t\tdata += eat;\n\t\t \n\t\t_hdr = buf_msg(_skb);\n\t\tmsg_set_fragm_no(_hdr, pktno);\n\t\tmsg_set_nof_fragms(_hdr, nof_fragms);\n\t\tmsg_set_size(_hdr, INT_H_SIZE + eat);\n\t}\n\treturn 0;\n\nerror:\n\t__skb_queue_purge(frags);\n\t__skb_queue_head_init(frags);\n\treturn -ENOMEM;\n}\n\n \nint tipc_msg_build(struct tipc_msg *mhdr, struct msghdr *m, int offset,\n\t\t   int dsz, int pktmax, struct sk_buff_head *list)\n{\n\tint mhsz = msg_hdr_sz(mhdr);\n\tstruct tipc_msg pkthdr;\n\tint msz = mhsz + dsz;\n\tint pktrem = pktmax;\n\tstruct sk_buff *skb;\n\tint drem = dsz;\n\tint pktno = 1;\n\tchar *pktpos;\n\tint pktsz;\n\tint rc;\n\n\tmsg_set_size(mhdr, msz);\n\n\t \n\tif (likely(msz <= pktmax)) {\n\t\tskb = tipc_buf_acquire(msz, GFP_KERNEL);\n\n\t\t \n\t\tif (unlikely(!skb)) {\n\t\t\tif (pktmax != MAX_MSG_SIZE)\n\t\t\t\treturn -ENOMEM;\n\t\t\trc = tipc_msg_build(mhdr, m, offset, dsz,\n\t\t\t\t\t    one_page_mtu, list);\n\t\t\tif (rc != dsz)\n\t\t\t\treturn rc;\n\t\t\tif (tipc_msg_assemble(list))\n\t\t\t\treturn dsz;\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tskb_orphan(skb);\n\t\t__skb_queue_tail(list, skb);\n\t\tskb_copy_to_linear_data(skb, mhdr, mhsz);\n\t\tpktpos = skb->data + mhsz;\n\t\tif (copy_from_iter_full(pktpos, dsz, &m->msg_iter))\n\t\t\treturn dsz;\n\t\trc = -EFAULT;\n\t\tgoto error;\n\t}\n\n\t \n\ttipc_msg_init(msg_prevnode(mhdr), &pkthdr, MSG_FRAGMENTER,\n\t\t      FIRST_FRAGMENT, INT_H_SIZE, msg_destnode(mhdr));\n\tmsg_set_size(&pkthdr, pktmax);\n\tmsg_set_fragm_no(&pkthdr, pktno);\n\tmsg_set_importance(&pkthdr, msg_importance(mhdr));\n\n\t \n\tskb = tipc_buf_acquire(pktmax, GFP_KERNEL);\n\tif (!skb)\n\t\treturn -ENOMEM;\n\tskb_orphan(skb);\n\t__skb_queue_tail(list, skb);\n\tpktpos = skb->data;\n\tskb_copy_to_linear_data(skb, &pkthdr, INT_H_SIZE);\n\tpktpos += INT_H_SIZE;\n\tpktrem -= INT_H_SIZE;\n\tskb_copy_to_linear_data_offset(skb, INT_H_SIZE, mhdr, mhsz);\n\tpktpos += mhsz;\n\tpktrem -= mhsz;\n\n\tdo {\n\t\tif (drem < pktrem)\n\t\t\tpktrem = drem;\n\n\t\tif (!copy_from_iter_full(pktpos, pktrem, &m->msg_iter)) {\n\t\t\trc = -EFAULT;\n\t\t\tgoto error;\n\t\t}\n\t\tdrem -= pktrem;\n\n\t\tif (!drem)\n\t\t\tbreak;\n\n\t\t \n\t\tif (drem < (pktmax - INT_H_SIZE))\n\t\t\tpktsz = drem + INT_H_SIZE;\n\t\telse\n\t\t\tpktsz = pktmax;\n\t\tskb = tipc_buf_acquire(pktsz, GFP_KERNEL);\n\t\tif (!skb) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto error;\n\t\t}\n\t\tskb_orphan(skb);\n\t\t__skb_queue_tail(list, skb);\n\t\tmsg_set_type(&pkthdr, FRAGMENT);\n\t\tmsg_set_size(&pkthdr, pktsz);\n\t\tmsg_set_fragm_no(&pkthdr, ++pktno);\n\t\tskb_copy_to_linear_data(skb, &pkthdr, INT_H_SIZE);\n\t\tpktpos = skb->data + INT_H_SIZE;\n\t\tpktrem = pktsz - INT_H_SIZE;\n\n\t} while (1);\n\tmsg_set_type(buf_msg(skb), LAST_FRAGMENT);\n\treturn dsz;\nerror:\n\t__skb_queue_purge(list);\n\t__skb_queue_head_init(list);\n\treturn rc;\n}\n\n \nstatic bool tipc_msg_bundle(struct sk_buff *bskb, struct tipc_msg *msg,\n\t\t\t    u32 max)\n{\n\tstruct tipc_msg *bmsg = buf_msg(bskb);\n\tu32 msz, bsz, offset, pad;\n\n\tmsz = msg_size(msg);\n\tbsz = msg_size(bmsg);\n\toffset = BUF_ALIGN(bsz);\n\tpad = offset - bsz;\n\n\tif (unlikely(skb_tailroom(bskb) < (pad + msz)))\n\t\treturn false;\n\tif (unlikely(max < (offset + msz)))\n\t\treturn false;\n\n\tskb_put(bskb, pad + msz);\n\tskb_copy_to_linear_data_offset(bskb, offset, msg, msz);\n\tmsg_set_size(bmsg, offset + msz);\n\tmsg_set_msgcnt(bmsg, msg_msgcnt(bmsg) + 1);\n\treturn true;\n}\n\n \nbool tipc_msg_try_bundle(struct sk_buff *tskb, struct sk_buff **skb, u32 mss,\n\t\t\t u32 dnode, bool *new_bundle)\n{\n\tstruct tipc_msg *msg, *inner, *outer;\n\tu32 tsz;\n\n\t \n\tmsg = buf_msg(*skb);\n\tif (msg_user(msg) == MSG_FRAGMENTER)\n\t\treturn false;\n\tif (msg_user(msg) == TUNNEL_PROTOCOL)\n\t\treturn false;\n\tif (msg_user(msg) == BCAST_PROTOCOL)\n\t\treturn false;\n\tif (mss <= INT_H_SIZE + msg_size(msg))\n\t\treturn false;\n\n\t \n\tif (unlikely(!tskb))\n\t\treturn true;\n\n\t \n\tif (msg_user(buf_msg(tskb)) == MSG_BUNDLER) {\n\t\t*new_bundle = false;\n\t\tgoto bundle;\n\t}\n\n\t \n\ttsz = msg_size(buf_msg(tskb));\n\tif (unlikely(mss < BUF_ALIGN(INT_H_SIZE + tsz) + msg_size(msg)))\n\t\treturn true;\n\tif (unlikely(pskb_expand_head(tskb, INT_H_SIZE, mss - tsz - INT_H_SIZE,\n\t\t\t\t      GFP_ATOMIC)))\n\t\treturn true;\n\tinner = buf_msg(tskb);\n\tskb_push(tskb, INT_H_SIZE);\n\touter = buf_msg(tskb);\n\ttipc_msg_init(msg_prevnode(inner), outer, MSG_BUNDLER, 0, INT_H_SIZE,\n\t\t      dnode);\n\tmsg_set_importance(outer, msg_importance(inner));\n\tmsg_set_size(outer, INT_H_SIZE + tsz);\n\tmsg_set_msgcnt(outer, 1);\n\t*new_bundle = true;\n\nbundle:\n\tif (likely(tipc_msg_bundle(tskb, msg, mss))) {\n\t\tconsume_skb(*skb);\n\t\t*skb = NULL;\n\t}\n\treturn true;\n}\n\n \nbool tipc_msg_extract(struct sk_buff *skb, struct sk_buff **iskb, int *pos)\n{\n\tstruct tipc_msg *hdr, *ihdr;\n\tint imsz;\n\n\t*iskb = NULL;\n\tif (unlikely(skb_linearize(skb)))\n\t\tgoto none;\n\n\thdr = buf_msg(skb);\n\tif (unlikely(*pos > (msg_data_sz(hdr) - MIN_H_SIZE)))\n\t\tgoto none;\n\n\tihdr = (struct tipc_msg *)(msg_data(hdr) + *pos);\n\timsz = msg_size(ihdr);\n\n\tif ((*pos + imsz) > msg_data_sz(hdr))\n\t\tgoto none;\n\n\t*iskb = tipc_buf_acquire(imsz, GFP_ATOMIC);\n\tif (!*iskb)\n\t\tgoto none;\n\n\tskb_copy_to_linear_data(*iskb, ihdr, imsz);\n\tif (unlikely(!tipc_msg_validate(iskb)))\n\t\tgoto none;\n\n\t*pos += BUF_ALIGN(imsz);\n\treturn true;\nnone:\n\tkfree_skb(skb);\n\tkfree_skb(*iskb);\n\t*iskb = NULL;\n\treturn false;\n}\n\n \nbool tipc_msg_reverse(u32 own_node,  struct sk_buff **skb, int err)\n{\n\tstruct sk_buff *_skb = *skb;\n\tstruct tipc_msg *_hdr, *hdr;\n\tint hlen, dlen;\n\n\tif (skb_linearize(_skb))\n\t\tgoto exit;\n\t_hdr = buf_msg(_skb);\n\tdlen = min_t(uint, msg_data_sz(_hdr), MAX_FORWARD_SIZE);\n\thlen = msg_hdr_sz(_hdr);\n\n\tif (msg_dest_droppable(_hdr))\n\t\tgoto exit;\n\tif (msg_errcode(_hdr))\n\t\tgoto exit;\n\n\t \n\tif (hlen == SHORT_H_SIZE)\n\t\thlen = BASIC_H_SIZE;\n\n\t \n\tif (msg_is_syn(_hdr) && err == TIPC_ERR_OVERLOAD)\n\t\tdlen = 0;\n\n\t \n\t*skb = tipc_buf_acquire(hlen + dlen, GFP_ATOMIC);\n\tif (!*skb)\n\t\tgoto exit;\n\tmemcpy((*skb)->data, _skb->data, msg_hdr_sz(_hdr));\n\tmemcpy((*skb)->data + hlen, msg_data(_hdr), dlen);\n\n\t \n\thdr = buf_msg(*skb);\n\tmsg_set_hdr_sz(hdr, hlen);\n\tmsg_set_errcode(hdr, err);\n\tmsg_set_non_seq(hdr, 0);\n\tmsg_set_origport(hdr, msg_destport(_hdr));\n\tmsg_set_destport(hdr, msg_origport(_hdr));\n\tmsg_set_destnode(hdr, msg_prevnode(_hdr));\n\tmsg_set_prevnode(hdr, own_node);\n\tmsg_set_orignode(hdr, own_node);\n\tmsg_set_size(hdr, hlen + dlen);\n\tskb_orphan(_skb);\n\tkfree_skb(_skb);\n\treturn true;\nexit:\n\tkfree_skb(_skb);\n\t*skb = NULL;\n\treturn false;\n}\n\nbool tipc_msg_skb_clone(struct sk_buff_head *msg, struct sk_buff_head *cpy)\n{\n\tstruct sk_buff *skb, *_skb;\n\n\tskb_queue_walk(msg, skb) {\n\t\t_skb = skb_clone(skb, GFP_ATOMIC);\n\t\tif (!_skb) {\n\t\t\t__skb_queue_purge(cpy);\n\t\t\tpr_err_ratelimited(\"Failed to clone buffer chain\\n\");\n\t\t\treturn false;\n\t\t}\n\t\t__skb_queue_tail(cpy, _skb);\n\t}\n\treturn true;\n}\n\n \nbool tipc_msg_lookup_dest(struct net *net, struct sk_buff *skb, int *err)\n{\n\tstruct tipc_msg *msg = buf_msg(skb);\n\tu32 scope = msg_lookup_scope(msg);\n\tu32 self = tipc_own_addr(net);\n\tu32 inst = msg_nameinst(msg);\n\tstruct tipc_socket_addr sk;\n\tstruct tipc_uaddr ua;\n\n\tif (!msg_isdata(msg))\n\t\treturn false;\n\tif (!msg_named(msg))\n\t\treturn false;\n\tif (msg_errcode(msg))\n\t\treturn false;\n\t*err = TIPC_ERR_NO_NAME;\n\tif (skb_linearize(skb))\n\t\treturn false;\n\tmsg = buf_msg(skb);\n\tif (msg_reroute_cnt(msg))\n\t\treturn false;\n\ttipc_uaddr(&ua, TIPC_SERVICE_RANGE, scope,\n\t\t   msg_nametype(msg), inst, inst);\n\tsk.node = tipc_scope2node(net, scope);\n\tif (!tipc_nametbl_lookup_anycast(net, &ua, &sk))\n\t\treturn false;\n\tmsg_incr_reroute_cnt(msg);\n\tif (sk.node != self)\n\t\tmsg_set_prevnode(msg, self);\n\tmsg_set_destnode(msg, sk.node);\n\tmsg_set_destport(msg, sk.ref);\n\t*err = TIPC_OK;\n\n\treturn true;\n}\n\n \nbool tipc_msg_assemble(struct sk_buff_head *list)\n{\n\tstruct sk_buff *skb, *tmp = NULL;\n\n\tif (skb_queue_len(list) == 1)\n\t\treturn true;\n\n\twhile ((skb = __skb_dequeue(list))) {\n\t\tskb->next = NULL;\n\t\tif (tipc_buf_append(&tmp, &skb)) {\n\t\t\t__skb_queue_tail(list, skb);\n\t\t\treturn true;\n\t\t}\n\t\tif (!tmp)\n\t\t\tbreak;\n\t}\n\t__skb_queue_purge(list);\n\t__skb_queue_head_init(list);\n\tpr_warn(\"Failed do assemble buffer\\n\");\n\treturn false;\n}\n\n \nbool tipc_msg_reassemble(struct sk_buff_head *list, struct sk_buff_head *rcvq)\n{\n\tstruct sk_buff *skb, *_skb;\n\tstruct sk_buff *frag = NULL;\n\tstruct sk_buff *head = NULL;\n\tint hdr_len;\n\n\t \n\tif (skb_queue_len(list) == 1) {\n\t\tskb = skb_peek(list);\n\t\thdr_len = skb_headroom(skb) + msg_hdr_sz(buf_msg(skb));\n\t\t_skb = __pskb_copy(skb, hdr_len, GFP_ATOMIC);\n\t\tif (!_skb)\n\t\t\treturn false;\n\t\t__skb_queue_tail(rcvq, _skb);\n\t\treturn true;\n\t}\n\n\t \n\tskb_queue_walk(list, skb) {\n\t\tfrag = skb_clone(skb, GFP_ATOMIC);\n\t\tif (!frag)\n\t\t\tgoto error;\n\t\tfrag->next = NULL;\n\t\tif (tipc_buf_append(&head, &frag))\n\t\t\tbreak;\n\t\tif (!head)\n\t\t\tgoto error;\n\t}\n\t__skb_queue_tail(rcvq, frag);\n\treturn true;\nerror:\n\tpr_warn(\"Failed do clone local mcast rcv buffer\\n\");\n\tkfree_skb(head);\n\treturn false;\n}\n\nbool tipc_msg_pskb_copy(u32 dst, struct sk_buff_head *msg,\n\t\t\tstruct sk_buff_head *cpy)\n{\n\tstruct sk_buff *skb, *_skb;\n\n\tskb_queue_walk(msg, skb) {\n\t\t_skb = pskb_copy(skb, GFP_ATOMIC);\n\t\tif (!_skb) {\n\t\t\t__skb_queue_purge(cpy);\n\t\t\treturn false;\n\t\t}\n\t\tmsg_set_destnode(buf_msg(_skb), dst);\n\t\t__skb_queue_tail(cpy, _skb);\n\t}\n\treturn true;\n}\n\n \nbool __tipc_skb_queue_sorted(struct sk_buff_head *list, u16 seqno,\n\t\t\t     struct sk_buff *skb)\n{\n\tstruct sk_buff *_skb, *tmp;\n\n\tif (skb_queue_empty(list) || less(seqno, buf_seqno(skb_peek(list)))) {\n\t\t__skb_queue_head(list, skb);\n\t\treturn true;\n\t}\n\n\tif (more(seqno, buf_seqno(skb_peek_tail(list)))) {\n\t\t__skb_queue_tail(list, skb);\n\t\treturn true;\n\t}\n\n\tskb_queue_walk_safe(list, _skb, tmp) {\n\t\tif (more(seqno, buf_seqno(_skb)))\n\t\t\tcontinue;\n\t\tif (seqno == buf_seqno(_skb))\n\t\t\tbreak;\n\t\t__skb_queue_before(list, _skb, skb);\n\t\treturn true;\n\t}\n\tkfree_skb(skb);\n\treturn false;\n}\n\nvoid tipc_skb_reject(struct net *net, int err, struct sk_buff *skb,\n\t\t     struct sk_buff_head *xmitq)\n{\n\tif (tipc_msg_reverse(tipc_own_addr(net), &skb, err))\n\t\t__skb_queue_tail(xmitq, skb);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}