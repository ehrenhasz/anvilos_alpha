{
  "module_name": "vmac.c",
  "hash_id": "110211879c3abe446fa43e1bfe30ae49fe7dae2d38d43245919c579481cc19fc",
  "original_prompt": "Ingested from linux-6.6.14/crypto/vmac.c",
  "human_readable_source": " \n\n \n\n#include <asm/unaligned.h>\n#include <linux/init.h>\n#include <linux/types.h>\n#include <linux/crypto.h>\n#include <linux/module.h>\n#include <linux/scatterlist.h>\n#include <asm/byteorder.h>\n#include <crypto/scatterwalk.h>\n#include <crypto/internal/cipher.h>\n#include <crypto/internal/hash.h>\n\n \n#define VMAC_TAG_LEN\t64\n#define VMAC_KEY_SIZE\t128 \n#define VMAC_KEY_LEN\t(VMAC_KEY_SIZE/8)\n#define VMAC_NHBYTES\t128 \n#define VMAC_NONCEBYTES\t16\n\n \nstruct vmac_tfm_ctx {\n\tstruct crypto_cipher *cipher;\n\tu64 nhkey[(VMAC_NHBYTES/8)+2*(VMAC_TAG_LEN/64-1)];\n\tu64 polykey[2*VMAC_TAG_LEN/64];\n\tu64 l3key[2*VMAC_TAG_LEN/64];\n};\n\n \nstruct vmac_desc_ctx {\n\tunion {\n\t\tu8 partial[VMAC_NHBYTES];\t \n\t\t__le64 partial_words[VMAC_NHBYTES / 8];\n\t};\n\tunsigned int partial_size;\t \n\tbool first_block_processed;\n\tu64 polytmp[2*VMAC_TAG_LEN/64];\t \n\tunion {\n\t\tu8 bytes[VMAC_NONCEBYTES];\n\t\t__be64 pads[VMAC_NONCEBYTES / 8];\n\t} nonce;\n\tunsigned int nonce_size;  \n};\n\n \n#define UINT64_C(x) x##ULL\nstatic const u64 p64   = UINT64_C(0xfffffffffffffeff);\t \nstatic const u64 m62   = UINT64_C(0x3fffffffffffffff);\t \nstatic const u64 m63   = UINT64_C(0x7fffffffffffffff);\t \nstatic const u64 m64   = UINT64_C(0xffffffffffffffff);\t \nstatic const u64 mpoly = UINT64_C(0x1fffffff1fffffff);\t \n\n#define pe64_to_cpup le64_to_cpup\t\t \n\n#ifdef __LITTLE_ENDIAN\n#define INDEX_HIGH 1\n#define INDEX_LOW 0\n#else\n#define INDEX_HIGH 0\n#define INDEX_LOW 1\n#endif\n\n \n\n#define ADD128(rh, rl, ih, il)\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tu64 _il = (il);\t\t\t\t\t\t\\\n\t\t(rl) += (_il);\t\t\t\t\t\t\\\n\t\tif ((rl) < (_il))\t\t\t\t\t\\\n\t\t\t(rh)++;\t\t\t\t\t\t\\\n\t\t(rh) += (ih);\t\t\t\t\t\t\\\n\t} while (0)\n\n#define MUL32(i1, i2)\t((u64)(u32)(i1)*(u32)(i2))\n\n#define PMUL64(rh, rl, i1, i2)\t \t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tu64 _i1 = (i1), _i2 = (i2);\t\t\t\t\\\n\t\tu64 m = MUL32(_i1, _i2>>32) + MUL32(_i1>>32, _i2);\t\\\n\t\trh = MUL32(_i1>>32, _i2>>32);\t\t\t\t\\\n\t\trl = MUL32(_i1, _i2);\t\t\t\t\t\\\n\t\tADD128(rh, rl, (m >> 32), (m << 32));\t\t\t\\\n\t} while (0)\n\n#define MUL64(rh, rl, i1, i2)\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tu64 _i1 = (i1), _i2 = (i2);\t\t\t\t\\\n\t\tu64 m1 = MUL32(_i1, _i2>>32);\t\t\t\t\\\n\t\tu64 m2 = MUL32(_i1>>32, _i2);\t\t\t\t\\\n\t\trh = MUL32(_i1>>32, _i2>>32);\t\t\t\t\\\n\t\trl = MUL32(_i1, _i2);\t\t\t\t\t\\\n\t\tADD128(rh, rl, (m1 >> 32), (m1 << 32));\t\t\t\\\n\t\tADD128(rh, rl, (m2 >> 32), (m2 << 32));\t\t\t\\\n\t} while (0)\n\n \n\n#ifdef CONFIG_64BIT\n\n#define nh_16(mp, kp, nw, rh, rl)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tint i; u64 th, tl;\t\t\t\t\t\\\n\t\trh = rl = 0;\t\t\t\t\t\t\\\n\t\tfor (i = 0; i < nw; i += 2) {\t\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i)+(kp)[i],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+1)+(kp)[i+1]);\t\\\n\t\t\tADD128(rh, rl, th, tl);\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t} while (0)\n\n#define nh_16_2(mp, kp, nw, rh, rl, rh1, rl1)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tint i; u64 th, tl;\t\t\t\t\t\\\n\t\trh1 = rl1 = rh = rl = 0;\t\t\t\t\\\n\t\tfor (i = 0; i < nw; i += 2) {\t\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i)+(kp)[i],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+1)+(kp)[i+1]);\t\\\n\t\t\tADD128(rh, rl, th, tl);\t\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i)+(kp)[i+2],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+1)+(kp)[i+3]);\t\\\n\t\t\tADD128(rh1, rl1, th, tl);\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t} while (0)\n\n#if (VMAC_NHBYTES >= 64)  \n#define nh_vmac_nhbytes(mp, kp, nw, rh, rl)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tint i; u64 th, tl;\t\t\t\t\t\\\n\t\trh = rl = 0;\t\t\t\t\t\t\\\n\t\tfor (i = 0; i < nw; i += 8) {\t\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i)+(kp)[i],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+1)+(kp)[i+1]);\t\\\n\t\t\tADD128(rh, rl, th, tl);\t\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i+2)+(kp)[i+2],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+3)+(kp)[i+3]);\t\\\n\t\t\tADD128(rh, rl, th, tl);\t\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i+4)+(kp)[i+4],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+5)+(kp)[i+5]);\t\\\n\t\t\tADD128(rh, rl, th, tl);\t\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i+6)+(kp)[i+6],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+7)+(kp)[i+7]);\t\\\n\t\t\tADD128(rh, rl, th, tl);\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t} while (0)\n\n#define nh_vmac_nhbytes_2(mp, kp, nw, rh, rl, rh1, rl1)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tint i; u64 th, tl;\t\t\t\t\t\\\n\t\trh1 = rl1 = rh = rl = 0;\t\t\t\t\\\n\t\tfor (i = 0; i < nw; i += 8) {\t\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i)+(kp)[i],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+1)+(kp)[i+1]);\t\\\n\t\t\tADD128(rh, rl, th, tl);\t\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i)+(kp)[i+2],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+1)+(kp)[i+3]);\t\\\n\t\t\tADD128(rh1, rl1, th, tl);\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i+2)+(kp)[i+2],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+3)+(kp)[i+3]);\t\\\n\t\t\tADD128(rh, rl, th, tl);\t\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i+2)+(kp)[i+4],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+3)+(kp)[i+5]);\t\\\n\t\t\tADD128(rh1, rl1, th, tl);\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i+4)+(kp)[i+4],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+5)+(kp)[i+5]);\t\\\n\t\t\tADD128(rh, rl, th, tl);\t\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i+4)+(kp)[i+6],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+5)+(kp)[i+7]);\t\\\n\t\t\tADD128(rh1, rl1, th, tl);\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i+6)+(kp)[i+6],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+7)+(kp)[i+7]);\t\\\n\t\t\tADD128(rh, rl, th, tl);\t\t\t\t\\\n\t\t\tMUL64(th, tl, pe64_to_cpup((mp)+i+6)+(kp)[i+8],\t\\\n\t\t\t\tpe64_to_cpup((mp)+i+7)+(kp)[i+9]);\t\\\n\t\t\tADD128(rh1, rl1, th, tl);\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t} while (0)\n#endif\n\n#define poly_step(ah, al, kh, kl, mh, ml)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tu64 t1h, t1l, t2h, t2l, t3h, t3l, z = 0;\t\t\\\n\t\t \t\\\n\t\tPMUL64(t3h, t3l, al, kh);\t\t\t\t\\\n\t\tPMUL64(t2h, t2l, ah, kl);\t\t\t\t\\\n\t\tPMUL64(t1h, t1l, ah, 2*kh);\t\t\t\t\\\n\t\tPMUL64(ah, al, al, kl);\t\t\t\t\t\\\n\t\t \t\t\t\t\\\n\t\tADD128(ah, al, t1h, t1l);\t\t\t\t\\\n\t\t \t\t\t\t\\\n\t\tADD128(t2h, t2l, t3h, t3l);\t\t\t\t\\\n\t\t \t\t\\\n\t\t \t\\\n\t\tADD128(t2h, ah, z, t2l);\t\t\t\t\\\n\t\t \t\t\t\\\n\t\tt2h = 2 * t2h + (ah >> 63);\t\t\t\t\\\n\t\tah &= m63;\t\t\t\t\t\t\\\n\t\t \t\t\t\t\\\n\t\tADD128(ah, al, mh, ml);\t\t\t\t\t\\\n\t\tADD128(ah, al, z, t2h);\t\t\t\t\t\\\n\t} while (0)\n\n#else  \n\n#ifndef nh_16\n#define nh_16(mp, kp, nw, rh, rl)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tu64 t1, t2, m1, m2, t;\t\t\t\t\t\\\n\t\tint i;\t\t\t\t\t\t\t\\\n\t\trh = rl = t = 0;\t\t\t\t\t\\\n\t\tfor (i = 0; i < nw; i += 2)  {\t\t\t\t\\\n\t\t\tt1 = pe64_to_cpup(mp+i) + kp[i];\t\t\\\n\t\t\tt2 = pe64_to_cpup(mp+i+1) + kp[i+1];\t\t\\\n\t\t\tm2 = MUL32(t1 >> 32, t2);\t\t\t\\\n\t\t\tm1 = MUL32(t1, t2 >> 32);\t\t\t\\\n\t\t\tADD128(rh, rl, MUL32(t1 >> 32, t2 >> 32),\t\\\n\t\t\t\tMUL32(t1, t2));\t\t\t\t\\\n\t\t\trh += (u64)(u32)(m1 >> 32)\t\t\t\\\n\t\t\t\t+ (u32)(m2 >> 32);\t\t\t\\\n\t\t\tt += (u64)(u32)m1 + (u32)m2;\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t\tADD128(rh, rl, (t >> 32), (t << 32));\t\t\t\\\n\t} while (0)\n#endif\n\nstatic void poly_step_func(u64 *ahi, u64 *alo,\n\t\t\tconst u64 *kh, const u64 *kl,\n\t\t\tconst u64 *mh, const u64 *ml)\n{\n#define a0 (*(((u32 *)alo)+INDEX_LOW))\n#define a1 (*(((u32 *)alo)+INDEX_HIGH))\n#define a2 (*(((u32 *)ahi)+INDEX_LOW))\n#define a3 (*(((u32 *)ahi)+INDEX_HIGH))\n#define k0 (*(((u32 *)kl)+INDEX_LOW))\n#define k1 (*(((u32 *)kl)+INDEX_HIGH))\n#define k2 (*(((u32 *)kh)+INDEX_LOW))\n#define k3 (*(((u32 *)kh)+INDEX_HIGH))\n\n\tu64 p, q, t;\n\tu32 t2;\n\n\tp = MUL32(a3, k3);\n\tp += p;\n\tp += *(u64 *)mh;\n\tp += MUL32(a0, k2);\n\tp += MUL32(a1, k1);\n\tp += MUL32(a2, k0);\n\tt = (u32)(p);\n\tp >>= 32;\n\tp += MUL32(a0, k3);\n\tp += MUL32(a1, k2);\n\tp += MUL32(a2, k1);\n\tp += MUL32(a3, k0);\n\tt |= ((u64)((u32)p & 0x7fffffff)) << 32;\n\tp >>= 31;\n\tp += (u64)(((u32 *)ml)[INDEX_LOW]);\n\tp += MUL32(a0, k0);\n\tq =  MUL32(a1, k3);\n\tq += MUL32(a2, k2);\n\tq += MUL32(a3, k1);\n\tq += q;\n\tp += q;\n\tt2 = (u32)(p);\n\tp >>= 32;\n\tp += (u64)(((u32 *)ml)[INDEX_HIGH]);\n\tp += MUL32(a0, k1);\n\tp += MUL32(a1, k0);\n\tq =  MUL32(a2, k3);\n\tq += MUL32(a3, k2);\n\tq += q;\n\tp += q;\n\t*(u64 *)(alo) = (p << 32) | t2;\n\tp >>= 32;\n\t*(u64 *)(ahi) = p + t;\n\n#undef a0\n#undef a1\n#undef a2\n#undef a3\n#undef k0\n#undef k1\n#undef k2\n#undef k3\n}\n\n#define poly_step(ah, al, kh, kl, mh, ml)\t\t\t\t\\\n\tpoly_step_func(&(ah), &(al), &(kh), &(kl), &(mh), &(ml))\n\n#endif   \n\n \n#ifndef nh_16_2\n#define nh_16_2(mp, kp, nw, rh, rl, rh2, rl2)\t\t\t\t\\\n\tdo { \t\t\t\t\t\t\t\t\\\n\t\tnh_16(mp, kp, nw, rh, rl);\t\t\t\t\\\n\t\tnh_16(mp, ((kp)+2), nw, rh2, rl2);\t\t\t\\\n\t} while (0)\n#endif\n#ifndef nh_vmac_nhbytes\n#define nh_vmac_nhbytes(mp, kp, nw, rh, rl)\t\t\t\t\\\n\tnh_16(mp, kp, nw, rh, rl)\n#endif\n#ifndef nh_vmac_nhbytes_2\n#define nh_vmac_nhbytes_2(mp, kp, nw, rh, rl, rh2, rl2)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tnh_vmac_nhbytes(mp, kp, nw, rh, rl);\t\t\t\\\n\t\tnh_vmac_nhbytes(mp, ((kp)+2), nw, rh2, rl2);\t\t\\\n\t} while (0)\n#endif\n\nstatic u64 l3hash(u64 p1, u64 p2, u64 k1, u64 k2, u64 len)\n{\n\tu64 rh, rl, t, z = 0;\n\n\t \n\tt = p1 >> 63;\n\tp1 &= m63;\n\tADD128(p1, p2, len, t);\n\t \n\tt = (p1 > m63) + ((p1 == m63) && (p2 == m64));\n\tADD128(p1, p2, z, t);\n\tp1 &= m63;\n\n\t \n\tt = p1 + (p2 >> 32);\n\tt += (t >> 32);\n\tt += (u32)t > 0xfffffffeu;\n\tp1 += (t >> 32);\n\tp2 += (p1 << 32);\n\n\t \n\tp1 += k1;\n\tp1 += (0 - (p1 < k1)) & 257;\n\tp2 += k2;\n\tp2 += (0 - (p2 < k2)) & 257;\n\n\t \n\tMUL64(rh, rl, p1, p2);\n\tt = rh >> 56;\n\tADD128(t, rl, z, rh);\n\trh <<= 8;\n\tADD128(t, rl, z, rh);\n\tt += t << 8;\n\trl += t;\n\trl += (0 - (rl < t)) & 257;\n\trl += (0 - (rl > p64-1)) & 257;\n\treturn rl;\n}\n\n \nstatic void vhash_blocks(const struct vmac_tfm_ctx *tctx,\n\t\t\t struct vmac_desc_ctx *dctx,\n\t\t\t const __le64 *mptr, unsigned int blocks)\n{\n\tconst u64 *kptr = tctx->nhkey;\n\tconst u64 pkh = tctx->polykey[0];\n\tconst u64 pkl = tctx->polykey[1];\n\tu64 ch = dctx->polytmp[0];\n\tu64 cl = dctx->polytmp[1];\n\tu64 rh, rl;\n\n\tif (!dctx->first_block_processed) {\n\t\tdctx->first_block_processed = true;\n\t\tnh_vmac_nhbytes(mptr, kptr, VMAC_NHBYTES/8, rh, rl);\n\t\trh &= m62;\n\t\tADD128(ch, cl, rh, rl);\n\t\tmptr += (VMAC_NHBYTES/sizeof(u64));\n\t\tblocks--;\n\t}\n\n\twhile (blocks--) {\n\t\tnh_vmac_nhbytes(mptr, kptr, VMAC_NHBYTES/8, rh, rl);\n\t\trh &= m62;\n\t\tpoly_step(ch, cl, pkh, pkl, rh, rl);\n\t\tmptr += (VMAC_NHBYTES/sizeof(u64));\n\t}\n\n\tdctx->polytmp[0] = ch;\n\tdctx->polytmp[1] = cl;\n}\n\nstatic int vmac_setkey(struct crypto_shash *tfm,\n\t\t       const u8 *key, unsigned int keylen)\n{\n\tstruct vmac_tfm_ctx *tctx = crypto_shash_ctx(tfm);\n\t__be64 out[2];\n\tu8 in[16] = { 0 };\n\tunsigned int i;\n\tint err;\n\n\tif (keylen != VMAC_KEY_LEN)\n\t\treturn -EINVAL;\n\n\terr = crypto_cipher_setkey(tctx->cipher, key, keylen);\n\tif (err)\n\t\treturn err;\n\n\t \n\tin[0] = 0x80;\n\tfor (i = 0; i < ARRAY_SIZE(tctx->nhkey); i += 2) {\n\t\tcrypto_cipher_encrypt_one(tctx->cipher, (u8 *)out, in);\n\t\ttctx->nhkey[i] = be64_to_cpu(out[0]);\n\t\ttctx->nhkey[i+1] = be64_to_cpu(out[1]);\n\t\tin[15]++;\n\t}\n\n\t \n\tin[0] = 0xC0;\n\tin[15] = 0;\n\tfor (i = 0; i < ARRAY_SIZE(tctx->polykey); i += 2) {\n\t\tcrypto_cipher_encrypt_one(tctx->cipher, (u8 *)out, in);\n\t\ttctx->polykey[i] = be64_to_cpu(out[0]) & mpoly;\n\t\ttctx->polykey[i+1] = be64_to_cpu(out[1]) & mpoly;\n\t\tin[15]++;\n\t}\n\n\t \n\tin[0] = 0xE0;\n\tin[15] = 0;\n\tfor (i = 0; i < ARRAY_SIZE(tctx->l3key); i += 2) {\n\t\tdo {\n\t\t\tcrypto_cipher_encrypt_one(tctx->cipher, (u8 *)out, in);\n\t\t\ttctx->l3key[i] = be64_to_cpu(out[0]);\n\t\t\ttctx->l3key[i+1] = be64_to_cpu(out[1]);\n\t\t\tin[15]++;\n\t\t} while (tctx->l3key[i] >= p64 || tctx->l3key[i+1] >= p64);\n\t}\n\n\treturn 0;\n}\n\nstatic int vmac_init(struct shash_desc *desc)\n{\n\tconst struct vmac_tfm_ctx *tctx = crypto_shash_ctx(desc->tfm);\n\tstruct vmac_desc_ctx *dctx = shash_desc_ctx(desc);\n\n\tdctx->partial_size = 0;\n\tdctx->first_block_processed = false;\n\tmemcpy(dctx->polytmp, tctx->polykey, sizeof(dctx->polytmp));\n\tdctx->nonce_size = 0;\n\treturn 0;\n}\n\nstatic int vmac_update(struct shash_desc *desc, const u8 *p, unsigned int len)\n{\n\tconst struct vmac_tfm_ctx *tctx = crypto_shash_ctx(desc->tfm);\n\tstruct vmac_desc_ctx *dctx = shash_desc_ctx(desc);\n\tunsigned int n;\n\n\t \n\tif (dctx->nonce_size < VMAC_NONCEBYTES) {\n\t\tn = min(len, VMAC_NONCEBYTES - dctx->nonce_size);\n\t\tmemcpy(&dctx->nonce.bytes[dctx->nonce_size], p, n);\n\t\tdctx->nonce_size += n;\n\t\tp += n;\n\t\tlen -= n;\n\t}\n\n\tif (dctx->partial_size) {\n\t\tn = min(len, VMAC_NHBYTES - dctx->partial_size);\n\t\tmemcpy(&dctx->partial[dctx->partial_size], p, n);\n\t\tdctx->partial_size += n;\n\t\tp += n;\n\t\tlen -= n;\n\t\tif (dctx->partial_size == VMAC_NHBYTES) {\n\t\t\tvhash_blocks(tctx, dctx, dctx->partial_words, 1);\n\t\t\tdctx->partial_size = 0;\n\t\t}\n\t}\n\n\tif (len >= VMAC_NHBYTES) {\n\t\tn = round_down(len, VMAC_NHBYTES);\n\t\t \n\t\tvhash_blocks(tctx, dctx, (const __le64 *)p, n / VMAC_NHBYTES);\n\t\tp += n;\n\t\tlen -= n;\n\t}\n\n\tif (len) {\n\t\tmemcpy(dctx->partial, p, len);\n\t\tdctx->partial_size = len;\n\t}\n\n\treturn 0;\n}\n\nstatic u64 vhash_final(const struct vmac_tfm_ctx *tctx,\n\t\t       struct vmac_desc_ctx *dctx)\n{\n\tunsigned int partial = dctx->partial_size;\n\tu64 ch = dctx->polytmp[0];\n\tu64 cl = dctx->polytmp[1];\n\n\t \n\tif (partial) {\n\t\t \n\t\tunsigned int n = round_up(partial, 16);\n\t\tu64 rh, rl;\n\n\t\tmemset(&dctx->partial[partial], 0, n - partial);\n\t\tnh_16(dctx->partial_words, tctx->nhkey, n / 8, rh, rl);\n\t\trh &= m62;\n\t\tif (dctx->first_block_processed)\n\t\t\tpoly_step(ch, cl, tctx->polykey[0], tctx->polykey[1],\n\t\t\t\t  rh, rl);\n\t\telse\n\t\t\tADD128(ch, cl, rh, rl);\n\t}\n\n\t \n\treturn l3hash(ch, cl, tctx->l3key[0], tctx->l3key[1], partial * 8);\n}\n\nstatic int vmac_final(struct shash_desc *desc, u8 *out)\n{\n\tconst struct vmac_tfm_ctx *tctx = crypto_shash_ctx(desc->tfm);\n\tstruct vmac_desc_ctx *dctx = shash_desc_ctx(desc);\n\tint index;\n\tu64 hash, pad;\n\n\tif (dctx->nonce_size != VMAC_NONCEBYTES)\n\t\treturn -EINVAL;\n\n\t \n\tif (dctx->nonce.bytes[0] & 0x80)\n\t\treturn -EINVAL;\n\n\t \n\thash = vhash_final(tctx, dctx);\n\n\t \n\tBUILD_BUG_ON(VMAC_NONCEBYTES != 2 * (VMAC_TAG_LEN / 8));\n\tindex = dctx->nonce.bytes[VMAC_NONCEBYTES - 1] & 1;\n\tdctx->nonce.bytes[VMAC_NONCEBYTES - 1] &= ~1;\n\tcrypto_cipher_encrypt_one(tctx->cipher, dctx->nonce.bytes,\n\t\t\t\t  dctx->nonce.bytes);\n\tpad = be64_to_cpu(dctx->nonce.pads[index]);\n\n\t \n\tput_unaligned_be64(hash + pad, out);\n\treturn 0;\n}\n\nstatic int vmac_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = crypto_tfm_alg_instance(tfm);\n\tstruct crypto_cipher_spawn *spawn = crypto_instance_ctx(inst);\n\tstruct vmac_tfm_ctx *tctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_cipher *cipher;\n\n\tcipher = crypto_spawn_cipher(spawn);\n\tif (IS_ERR(cipher))\n\t\treturn PTR_ERR(cipher);\n\n\ttctx->cipher = cipher;\n\treturn 0;\n}\n\nstatic void vmac_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct vmac_tfm_ctx *tctx = crypto_tfm_ctx(tfm);\n\n\tcrypto_free_cipher(tctx->cipher);\n}\n\nstatic int vmac_create(struct crypto_template *tmpl, struct rtattr **tb)\n{\n\tstruct shash_instance *inst;\n\tstruct crypto_cipher_spawn *spawn;\n\tstruct crypto_alg *alg;\n\tu32 mask;\n\tint err;\n\n\terr = crypto_check_attr_type(tb, CRYPTO_ALG_TYPE_SHASH, &mask);\n\tif (err)\n\t\treturn err;\n\n\tinst = kzalloc(sizeof(*inst) + sizeof(*spawn), GFP_KERNEL);\n\tif (!inst)\n\t\treturn -ENOMEM;\n\tspawn = shash_instance_ctx(inst);\n\n\terr = crypto_grab_cipher(spawn, shash_crypto_instance(inst),\n\t\t\t\t crypto_attr_alg_name(tb[1]), 0, mask);\n\tif (err)\n\t\tgoto err_free_inst;\n\talg = crypto_spawn_cipher_alg(spawn);\n\n\terr = -EINVAL;\n\tif (alg->cra_blocksize != VMAC_NONCEBYTES)\n\t\tgoto err_free_inst;\n\n\terr = crypto_inst_setname(shash_crypto_instance(inst), tmpl->name, alg);\n\tif (err)\n\t\tgoto err_free_inst;\n\n\tinst->alg.base.cra_priority = alg->cra_priority;\n\tinst->alg.base.cra_blocksize = alg->cra_blocksize;\n\tinst->alg.base.cra_alignmask = alg->cra_alignmask;\n\n\tinst->alg.base.cra_ctxsize = sizeof(struct vmac_tfm_ctx);\n\tinst->alg.base.cra_init = vmac_init_tfm;\n\tinst->alg.base.cra_exit = vmac_exit_tfm;\n\n\tinst->alg.descsize = sizeof(struct vmac_desc_ctx);\n\tinst->alg.digestsize = VMAC_TAG_LEN / 8;\n\tinst->alg.init = vmac_init;\n\tinst->alg.update = vmac_update;\n\tinst->alg.final = vmac_final;\n\tinst->alg.setkey = vmac_setkey;\n\n\tinst->free = shash_free_singlespawn_instance;\n\n\terr = shash_register_instance(tmpl, inst);\n\tif (err) {\nerr_free_inst:\n\t\tshash_free_singlespawn_instance(inst);\n\t}\n\treturn err;\n}\n\nstatic struct crypto_template vmac64_tmpl = {\n\t.name = \"vmac64\",\n\t.create = vmac_create,\n\t.module = THIS_MODULE,\n};\n\nstatic int __init vmac_module_init(void)\n{\n\treturn crypto_register_template(&vmac64_tmpl);\n}\n\nstatic void __exit vmac_module_exit(void)\n{\n\tcrypto_unregister_template(&vmac64_tmpl);\n}\n\nsubsys_initcall(vmac_module_init);\nmodule_exit(vmac_module_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"VMAC hash algorithm\");\nMODULE_ALIAS_CRYPTO(\"vmac64\");\nMODULE_IMPORT_NS(CRYPTO_INTERNAL);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}