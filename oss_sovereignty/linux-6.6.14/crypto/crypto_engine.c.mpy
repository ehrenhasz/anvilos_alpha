{
  "module_name": "crypto_engine.c",
  "hash_id": "722c78bcad6c362c6d326db1f53c7990184490477167ce82fe3d9383daf6fbea",
  "original_prompt": "Ingested from linux-6.6.14/crypto/crypto_engine.c",
  "human_readable_source": "\n \n\n#include <crypto/internal/aead.h>\n#include <crypto/internal/akcipher.h>\n#include <crypto/internal/engine.h>\n#include <crypto/internal/hash.h>\n#include <crypto/internal/kpp.h>\n#include <crypto/internal/skcipher.h>\n#include <linux/err.h>\n#include <linux/delay.h>\n#include <linux/device.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <uapi/linux/sched/types.h>\n#include \"internal.h\"\n\n#define CRYPTO_ENGINE_MAX_QLEN 10\n\n \n#define CRYPTO_ALG_ENGINE 0x200\n\nstruct crypto_engine_alg {\n\tstruct crypto_alg base;\n\tstruct crypto_engine_op op;\n};\n\n \nstatic void crypto_finalize_request(struct crypto_engine *engine,\n\t\t\t\t    struct crypto_async_request *req, int err)\n{\n\tunsigned long flags;\n\n\t \n\tif (!engine->retry_support) {\n\t\tspin_lock_irqsave(&engine->queue_lock, flags);\n\t\tif (engine->cur_req == req) {\n\t\t\tengine->cur_req = NULL;\n\t\t}\n\t\tspin_unlock_irqrestore(&engine->queue_lock, flags);\n\t}\n\n\tlockdep_assert_in_softirq();\n\tcrypto_request_complete(req, err);\n\n\tkthread_queue_work(engine->kworker, &engine->pump_requests);\n}\n\n \nstatic void crypto_pump_requests(struct crypto_engine *engine,\n\t\t\t\t bool in_kthread)\n{\n\tstruct crypto_async_request *async_req, *backlog;\n\tstruct crypto_engine_alg *alg;\n\tstruct crypto_engine_op *op;\n\tunsigned long flags;\n\tbool was_busy = false;\n\tint ret;\n\n\tspin_lock_irqsave(&engine->queue_lock, flags);\n\n\t \n\tif (!engine->retry_support && engine->cur_req)\n\t\tgoto out;\n\n\t \n\tif (engine->idling) {\n\t\tkthread_queue_work(engine->kworker, &engine->pump_requests);\n\t\tgoto out;\n\t}\n\n\t \n\tif (!crypto_queue_len(&engine->queue) || !engine->running) {\n\t\tif (!engine->busy)\n\t\t\tgoto out;\n\n\t\t \n\t\tif (!in_kthread) {\n\t\t\tkthread_queue_work(engine->kworker,\n\t\t\t\t\t   &engine->pump_requests);\n\t\t\tgoto out;\n\t\t}\n\n\t\tengine->busy = false;\n\t\tengine->idling = true;\n\t\tspin_unlock_irqrestore(&engine->queue_lock, flags);\n\n\t\tif (engine->unprepare_crypt_hardware &&\n\t\t    engine->unprepare_crypt_hardware(engine))\n\t\t\tdev_err(engine->dev, \"failed to unprepare crypt hardware\\n\");\n\n\t\tspin_lock_irqsave(&engine->queue_lock, flags);\n\t\tengine->idling = false;\n\t\tgoto out;\n\t}\n\nstart_request:\n\t \n\tbacklog = crypto_get_backlog(&engine->queue);\n\tasync_req = crypto_dequeue_request(&engine->queue);\n\tif (!async_req)\n\t\tgoto out;\n\n\t \n\tif (!engine->retry_support)\n\t\tengine->cur_req = async_req;\n\n\tif (engine->busy)\n\t\twas_busy = true;\n\telse\n\t\tengine->busy = true;\n\n\tspin_unlock_irqrestore(&engine->queue_lock, flags);\n\n\t \n\tif (!was_busy && engine->prepare_crypt_hardware) {\n\t\tret = engine->prepare_crypt_hardware(engine);\n\t\tif (ret) {\n\t\t\tdev_err(engine->dev, \"failed to prepare crypt hardware\\n\");\n\t\t\tgoto req_err_1;\n\t\t}\n\t}\n\n\tif (async_req->tfm->__crt_alg->cra_flags & CRYPTO_ALG_ENGINE) {\n\t\talg = container_of(async_req->tfm->__crt_alg,\n\t\t\t\t   struct crypto_engine_alg, base);\n\t\top = &alg->op;\n\t} else {\n\t\tdev_err(engine->dev, \"failed to do request\\n\");\n\t\tret = -EINVAL;\n\t\tgoto req_err_1;\n\t}\n\n\tret = op->do_one_request(engine, async_req);\n\n\t \n\tif (ret < 0) {\n\t\t \n\t\tif (!engine->retry_support ||\n\t\t    (ret != -ENOSPC)) {\n\t\t\tdev_err(engine->dev,\n\t\t\t\t\"Failed to do one request from queue: %d\\n\",\n\t\t\t\tret);\n\t\t\tgoto req_err_1;\n\t\t}\n\t\tspin_lock_irqsave(&engine->queue_lock, flags);\n\t\t \n\t\tcrypto_enqueue_request_head(&engine->queue, async_req);\n\n\t\tkthread_queue_work(engine->kworker, &engine->pump_requests);\n\t\tgoto out;\n\t}\n\n\tgoto retry;\n\nreq_err_1:\n\tcrypto_request_complete(async_req, ret);\n\nretry:\n\tif (backlog)\n\t\tcrypto_request_complete(backlog, -EINPROGRESS);\n\n\t \n\tif (engine->retry_support) {\n\t\tspin_lock_irqsave(&engine->queue_lock, flags);\n\t\tgoto start_request;\n\t}\n\treturn;\n\nout:\n\tspin_unlock_irqrestore(&engine->queue_lock, flags);\n\n\t \n\tif (engine->do_batch_requests) {\n\t\tret = engine->do_batch_requests(engine);\n\t\tif (ret)\n\t\t\tdev_err(engine->dev, \"failed to do batch requests: %d\\n\",\n\t\t\t\tret);\n\t}\n\n\treturn;\n}\n\nstatic void crypto_pump_work(struct kthread_work *work)\n{\n\tstruct crypto_engine *engine =\n\t\tcontainer_of(work, struct crypto_engine, pump_requests);\n\n\tcrypto_pump_requests(engine, true);\n}\n\n \nstatic int crypto_transfer_request(struct crypto_engine *engine,\n\t\t\t\t   struct crypto_async_request *req,\n\t\t\t\t   bool need_pump)\n{\n\tunsigned long flags;\n\tint ret;\n\n\tspin_lock_irqsave(&engine->queue_lock, flags);\n\n\tif (!engine->running) {\n\t\tspin_unlock_irqrestore(&engine->queue_lock, flags);\n\t\treturn -ESHUTDOWN;\n\t}\n\n\tret = crypto_enqueue_request(&engine->queue, req);\n\n\tif (!engine->busy && need_pump)\n\t\tkthread_queue_work(engine->kworker, &engine->pump_requests);\n\n\tspin_unlock_irqrestore(&engine->queue_lock, flags);\n\treturn ret;\n}\n\n \nstatic int crypto_transfer_request_to_engine(struct crypto_engine *engine,\n\t\t\t\t\t     struct crypto_async_request *req)\n{\n\treturn crypto_transfer_request(engine, req, true);\n}\n\n \nint crypto_transfer_aead_request_to_engine(struct crypto_engine *engine,\n\t\t\t\t\t   struct aead_request *req)\n{\n\treturn crypto_transfer_request_to_engine(engine, &req->base);\n}\nEXPORT_SYMBOL_GPL(crypto_transfer_aead_request_to_engine);\n\n \nint crypto_transfer_akcipher_request_to_engine(struct crypto_engine *engine,\n\t\t\t\t\t       struct akcipher_request *req)\n{\n\treturn crypto_transfer_request_to_engine(engine, &req->base);\n}\nEXPORT_SYMBOL_GPL(crypto_transfer_akcipher_request_to_engine);\n\n \nint crypto_transfer_hash_request_to_engine(struct crypto_engine *engine,\n\t\t\t\t\t   struct ahash_request *req)\n{\n\treturn crypto_transfer_request_to_engine(engine, &req->base);\n}\nEXPORT_SYMBOL_GPL(crypto_transfer_hash_request_to_engine);\n\n \nint crypto_transfer_kpp_request_to_engine(struct crypto_engine *engine,\n\t\t\t\t\t  struct kpp_request *req)\n{\n\treturn crypto_transfer_request_to_engine(engine, &req->base);\n}\nEXPORT_SYMBOL_GPL(crypto_transfer_kpp_request_to_engine);\n\n \nint crypto_transfer_skcipher_request_to_engine(struct crypto_engine *engine,\n\t\t\t\t\t       struct skcipher_request *req)\n{\n\treturn crypto_transfer_request_to_engine(engine, &req->base);\n}\nEXPORT_SYMBOL_GPL(crypto_transfer_skcipher_request_to_engine);\n\n \nvoid crypto_finalize_aead_request(struct crypto_engine *engine,\n\t\t\t\t  struct aead_request *req, int err)\n{\n\treturn crypto_finalize_request(engine, &req->base, err);\n}\nEXPORT_SYMBOL_GPL(crypto_finalize_aead_request);\n\n \nvoid crypto_finalize_akcipher_request(struct crypto_engine *engine,\n\t\t\t\t      struct akcipher_request *req, int err)\n{\n\treturn crypto_finalize_request(engine, &req->base, err);\n}\nEXPORT_SYMBOL_GPL(crypto_finalize_akcipher_request);\n\n \nvoid crypto_finalize_hash_request(struct crypto_engine *engine,\n\t\t\t\t  struct ahash_request *req, int err)\n{\n\treturn crypto_finalize_request(engine, &req->base, err);\n}\nEXPORT_SYMBOL_GPL(crypto_finalize_hash_request);\n\n \nvoid crypto_finalize_kpp_request(struct crypto_engine *engine,\n\t\t\t\t struct kpp_request *req, int err)\n{\n\treturn crypto_finalize_request(engine, &req->base, err);\n}\nEXPORT_SYMBOL_GPL(crypto_finalize_kpp_request);\n\n \nvoid crypto_finalize_skcipher_request(struct crypto_engine *engine,\n\t\t\t\t      struct skcipher_request *req, int err)\n{\n\treturn crypto_finalize_request(engine, &req->base, err);\n}\nEXPORT_SYMBOL_GPL(crypto_finalize_skcipher_request);\n\n \nint crypto_engine_start(struct crypto_engine *engine)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&engine->queue_lock, flags);\n\n\tif (engine->running || engine->busy) {\n\t\tspin_unlock_irqrestore(&engine->queue_lock, flags);\n\t\treturn -EBUSY;\n\t}\n\n\tengine->running = true;\n\tspin_unlock_irqrestore(&engine->queue_lock, flags);\n\n\tkthread_queue_work(engine->kworker, &engine->pump_requests);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(crypto_engine_start);\n\n \nint crypto_engine_stop(struct crypto_engine *engine)\n{\n\tunsigned long flags;\n\tunsigned int limit = 500;\n\tint ret = 0;\n\n\tspin_lock_irqsave(&engine->queue_lock, flags);\n\n\t \n\twhile ((crypto_queue_len(&engine->queue) || engine->busy) && limit--) {\n\t\tspin_unlock_irqrestore(&engine->queue_lock, flags);\n\t\tmsleep(20);\n\t\tspin_lock_irqsave(&engine->queue_lock, flags);\n\t}\n\n\tif (crypto_queue_len(&engine->queue) || engine->busy)\n\t\tret = -EBUSY;\n\telse\n\t\tengine->running = false;\n\n\tspin_unlock_irqrestore(&engine->queue_lock, flags);\n\n\tif (ret)\n\t\tdev_warn(engine->dev, \"could not stop engine\\n\");\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(crypto_engine_stop);\n\n \nstruct crypto_engine *crypto_engine_alloc_init_and_set(struct device *dev,\n\t\t\t\t\t\t       bool retry_support,\n\t\t\t\t\t\t       int (*cbk_do_batch)(struct crypto_engine *engine),\n\t\t\t\t\t\t       bool rt, int qlen)\n{\n\tstruct crypto_engine *engine;\n\n\tif (!dev)\n\t\treturn NULL;\n\n\tengine = devm_kzalloc(dev, sizeof(*engine), GFP_KERNEL);\n\tif (!engine)\n\t\treturn NULL;\n\n\tengine->dev = dev;\n\tengine->rt = rt;\n\tengine->running = false;\n\tengine->busy = false;\n\tengine->idling = false;\n\tengine->retry_support = retry_support;\n\tengine->priv_data = dev;\n\t \n\tengine->do_batch_requests = retry_support ? cbk_do_batch : NULL;\n\n\tsnprintf(engine->name, sizeof(engine->name),\n\t\t \"%s-engine\", dev_name(dev));\n\n\tcrypto_init_queue(&engine->queue, qlen);\n\tspin_lock_init(&engine->queue_lock);\n\n\tengine->kworker = kthread_create_worker(0, \"%s\", engine->name);\n\tif (IS_ERR(engine->kworker)) {\n\t\tdev_err(dev, \"failed to create crypto request pump task\\n\");\n\t\treturn NULL;\n\t}\n\tkthread_init_work(&engine->pump_requests, crypto_pump_work);\n\n\tif (engine->rt) {\n\t\tdev_info(dev, \"will run requests pump with realtime priority\\n\");\n\t\tsched_set_fifo(engine->kworker->task);\n\t}\n\n\treturn engine;\n}\nEXPORT_SYMBOL_GPL(crypto_engine_alloc_init_and_set);\n\n \nstruct crypto_engine *crypto_engine_alloc_init(struct device *dev, bool rt)\n{\n\treturn crypto_engine_alloc_init_and_set(dev, false, NULL, rt,\n\t\t\t\t\t\tCRYPTO_ENGINE_MAX_QLEN);\n}\nEXPORT_SYMBOL_GPL(crypto_engine_alloc_init);\n\n \nint crypto_engine_exit(struct crypto_engine *engine)\n{\n\tint ret;\n\n\tret = crypto_engine_stop(engine);\n\tif (ret)\n\t\treturn ret;\n\n\tkthread_destroy_worker(engine->kworker);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(crypto_engine_exit);\n\nint crypto_engine_register_aead(struct aead_engine_alg *alg)\n{\n\tif (!alg->op.do_one_request)\n\t\treturn -EINVAL;\n\n\talg->base.base.cra_flags |= CRYPTO_ALG_ENGINE;\n\n\treturn crypto_register_aead(&alg->base);\n}\nEXPORT_SYMBOL_GPL(crypto_engine_register_aead);\n\nvoid crypto_engine_unregister_aead(struct aead_engine_alg *alg)\n{\n\tcrypto_unregister_aead(&alg->base);\n}\nEXPORT_SYMBOL_GPL(crypto_engine_unregister_aead);\n\nint crypto_engine_register_aeads(struct aead_engine_alg *algs, int count)\n{\n\tint i, ret;\n\n\tfor (i = 0; i < count; i++) {\n\t\tret = crypto_engine_register_aead(&algs[i]);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\treturn 0;\n\nerr:\n\tcrypto_engine_unregister_aeads(algs, i);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(crypto_engine_register_aeads);\n\nvoid crypto_engine_unregister_aeads(struct aead_engine_alg *algs, int count)\n{\n\tint i;\n\n\tfor (i = count - 1; i >= 0; --i)\n\t\tcrypto_engine_unregister_aead(&algs[i]);\n}\nEXPORT_SYMBOL_GPL(crypto_engine_unregister_aeads);\n\nint crypto_engine_register_ahash(struct ahash_engine_alg *alg)\n{\n\tif (!alg->op.do_one_request)\n\t\treturn -EINVAL;\n\n\talg->base.halg.base.cra_flags |= CRYPTO_ALG_ENGINE;\n\n\treturn crypto_register_ahash(&alg->base);\n}\nEXPORT_SYMBOL_GPL(crypto_engine_register_ahash);\n\nvoid crypto_engine_unregister_ahash(struct ahash_engine_alg *alg)\n{\n\tcrypto_unregister_ahash(&alg->base);\n}\nEXPORT_SYMBOL_GPL(crypto_engine_unregister_ahash);\n\nint crypto_engine_register_ahashes(struct ahash_engine_alg *algs, int count)\n{\n\tint i, ret;\n\n\tfor (i = 0; i < count; i++) {\n\t\tret = crypto_engine_register_ahash(&algs[i]);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\treturn 0;\n\nerr:\n\tcrypto_engine_unregister_ahashes(algs, i);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(crypto_engine_register_ahashes);\n\nvoid crypto_engine_unregister_ahashes(struct ahash_engine_alg *algs,\n\t\t\t\t      int count)\n{\n\tint i;\n\n\tfor (i = count - 1; i >= 0; --i)\n\t\tcrypto_engine_unregister_ahash(&algs[i]);\n}\nEXPORT_SYMBOL_GPL(crypto_engine_unregister_ahashes);\n\nint crypto_engine_register_akcipher(struct akcipher_engine_alg *alg)\n{\n\tif (!alg->op.do_one_request)\n\t\treturn -EINVAL;\n\n\talg->base.base.cra_flags |= CRYPTO_ALG_ENGINE;\n\n\treturn crypto_register_akcipher(&alg->base);\n}\nEXPORT_SYMBOL_GPL(crypto_engine_register_akcipher);\n\nvoid crypto_engine_unregister_akcipher(struct akcipher_engine_alg *alg)\n{\n\tcrypto_unregister_akcipher(&alg->base);\n}\nEXPORT_SYMBOL_GPL(crypto_engine_unregister_akcipher);\n\nint crypto_engine_register_kpp(struct kpp_engine_alg *alg)\n{\n\tif (!alg->op.do_one_request)\n\t\treturn -EINVAL;\n\n\talg->base.base.cra_flags |= CRYPTO_ALG_ENGINE;\n\n\treturn crypto_register_kpp(&alg->base);\n}\nEXPORT_SYMBOL_GPL(crypto_engine_register_kpp);\n\nvoid crypto_engine_unregister_kpp(struct kpp_engine_alg *alg)\n{\n\tcrypto_unregister_kpp(&alg->base);\n}\nEXPORT_SYMBOL_GPL(crypto_engine_unregister_kpp);\n\nint crypto_engine_register_skcipher(struct skcipher_engine_alg *alg)\n{\n\tif (!alg->op.do_one_request)\n\t\treturn -EINVAL;\n\n\talg->base.base.cra_flags |= CRYPTO_ALG_ENGINE;\n\n\treturn crypto_register_skcipher(&alg->base);\n}\nEXPORT_SYMBOL_GPL(crypto_engine_register_skcipher);\n\nvoid crypto_engine_unregister_skcipher(struct skcipher_engine_alg *alg)\n{\n\treturn crypto_unregister_skcipher(&alg->base);\n}\nEXPORT_SYMBOL_GPL(crypto_engine_unregister_skcipher);\n\nint crypto_engine_register_skciphers(struct skcipher_engine_alg *algs,\n\t\t\t\t     int count)\n{\n\tint i, ret;\n\n\tfor (i = 0; i < count; i++) {\n\t\tret = crypto_engine_register_skcipher(&algs[i]);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\treturn 0;\n\nerr:\n\tcrypto_engine_unregister_skciphers(algs, i);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(crypto_engine_register_skciphers);\n\nvoid crypto_engine_unregister_skciphers(struct skcipher_engine_alg *algs,\n\t\t\t\t\tint count)\n{\n\tint i;\n\n\tfor (i = count - 1; i >= 0; --i)\n\t\tcrypto_engine_unregister_skcipher(&algs[i]);\n}\nEXPORT_SYMBOL_GPL(crypto_engine_unregister_skciphers);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"Crypto hardware engine framework\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}