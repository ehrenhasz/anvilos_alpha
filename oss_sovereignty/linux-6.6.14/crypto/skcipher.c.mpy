{
  "module_name": "skcipher.c",
  "hash_id": "1c6c26dd46602d2bcee6c585c030524cf95db07b7a1057f04f9a9a8c2dff6e92",
  "original_prompt": "Ingested from linux-6.6.14/crypto/skcipher.c",
  "human_readable_source": "\n \n\n#include <crypto/internal/aead.h>\n#include <crypto/internal/cipher.h>\n#include <crypto/internal/skcipher.h>\n#include <crypto/scatterwalk.h>\n#include <linux/bug.h>\n#include <linux/cryptouser.h>\n#include <linux/err.h>\n#include <linux/kernel.h>\n#include <linux/list.h>\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <linux/seq_file.h>\n#include <linux/slab.h>\n#include <linux/string.h>\n#include <net/netlink.h>\n\n#include \"internal.h\"\n\nenum {\n\tSKCIPHER_WALK_PHYS = 1 << 0,\n\tSKCIPHER_WALK_SLOW = 1 << 1,\n\tSKCIPHER_WALK_COPY = 1 << 2,\n\tSKCIPHER_WALK_DIFF = 1 << 3,\n\tSKCIPHER_WALK_SLEEP = 1 << 4,\n};\n\nstruct skcipher_walk_buffer {\n\tstruct list_head entry;\n\tstruct scatter_walk dst;\n\tunsigned int len;\n\tu8 *data;\n\tu8 buffer[];\n};\n\nstatic int skcipher_walk_next(struct skcipher_walk *walk);\n\nstatic inline void skcipher_map_src(struct skcipher_walk *walk)\n{\n\twalk->src.virt.addr = scatterwalk_map(&walk->in);\n}\n\nstatic inline void skcipher_map_dst(struct skcipher_walk *walk)\n{\n\twalk->dst.virt.addr = scatterwalk_map(&walk->out);\n}\n\nstatic inline void skcipher_unmap_src(struct skcipher_walk *walk)\n{\n\tscatterwalk_unmap(walk->src.virt.addr);\n}\n\nstatic inline void skcipher_unmap_dst(struct skcipher_walk *walk)\n{\n\tscatterwalk_unmap(walk->dst.virt.addr);\n}\n\nstatic inline gfp_t skcipher_walk_gfp(struct skcipher_walk *walk)\n{\n\treturn walk->flags & SKCIPHER_WALK_SLEEP ? GFP_KERNEL : GFP_ATOMIC;\n}\n\n \nstatic inline u8 *skcipher_get_spot(u8 *start, unsigned int len)\n{\n\tu8 *end_page = (u8 *)(((unsigned long)(start + len - 1)) & PAGE_MASK);\n\n\treturn max(start, end_page);\n}\n\nstatic inline struct skcipher_alg *__crypto_skcipher_alg(\n\tstruct crypto_alg *alg)\n{\n\treturn container_of(alg, struct skcipher_alg, base);\n}\n\nstatic inline struct crypto_istat_cipher *skcipher_get_stat(\n\tstruct skcipher_alg *alg)\n{\n#ifdef CONFIG_CRYPTO_STATS\n\treturn &alg->stat;\n#else\n\treturn NULL;\n#endif\n}\n\nstatic inline int crypto_skcipher_errstat(struct skcipher_alg *alg, int err)\n{\n\tstruct crypto_istat_cipher *istat = skcipher_get_stat(alg);\n\n\tif (!IS_ENABLED(CONFIG_CRYPTO_STATS))\n\t\treturn err;\n\n\tif (err && err != -EINPROGRESS && err != -EBUSY)\n\t\tatomic64_inc(&istat->err_cnt);\n\n\treturn err;\n}\n\nstatic int skcipher_done_slow(struct skcipher_walk *walk, unsigned int bsize)\n{\n\tu8 *addr;\n\n\taddr = (u8 *)ALIGN((unsigned long)walk->buffer, walk->alignmask + 1);\n\taddr = skcipher_get_spot(addr, bsize);\n\tscatterwalk_copychunks(addr, &walk->out, bsize,\n\t\t\t       (walk->flags & SKCIPHER_WALK_PHYS) ? 2 : 1);\n\treturn 0;\n}\n\nint skcipher_walk_done(struct skcipher_walk *walk, int err)\n{\n\tunsigned int n = walk->nbytes;\n\tunsigned int nbytes = 0;\n\n\tif (!n)\n\t\tgoto finish;\n\n\tif (likely(err >= 0)) {\n\t\tn -= err;\n\t\tnbytes = walk->total - n;\n\t}\n\n\tif (likely(!(walk->flags & (SKCIPHER_WALK_PHYS |\n\t\t\t\t    SKCIPHER_WALK_SLOW |\n\t\t\t\t    SKCIPHER_WALK_COPY |\n\t\t\t\t    SKCIPHER_WALK_DIFF)))) {\nunmap_src:\n\t\tskcipher_unmap_src(walk);\n\t} else if (walk->flags & SKCIPHER_WALK_DIFF) {\n\t\tskcipher_unmap_dst(walk);\n\t\tgoto unmap_src;\n\t} else if (walk->flags & SKCIPHER_WALK_COPY) {\n\t\tskcipher_map_dst(walk);\n\t\tmemcpy(walk->dst.virt.addr, walk->page, n);\n\t\tskcipher_unmap_dst(walk);\n\t} else if (unlikely(walk->flags & SKCIPHER_WALK_SLOW)) {\n\t\tif (err > 0) {\n\t\t\t \n\t\t\terr = -EINVAL;\n\t\t\tnbytes = 0;\n\t\t} else\n\t\t\tn = skcipher_done_slow(walk, n);\n\t}\n\n\tif (err > 0)\n\t\terr = 0;\n\n\twalk->total = nbytes;\n\twalk->nbytes = 0;\n\n\tscatterwalk_advance(&walk->in, n);\n\tscatterwalk_advance(&walk->out, n);\n\tscatterwalk_done(&walk->in, 0, nbytes);\n\tscatterwalk_done(&walk->out, 1, nbytes);\n\n\tif (nbytes) {\n\t\tcrypto_yield(walk->flags & SKCIPHER_WALK_SLEEP ?\n\t\t\t     CRYPTO_TFM_REQ_MAY_SLEEP : 0);\n\t\treturn skcipher_walk_next(walk);\n\t}\n\nfinish:\n\t \n\tif (!((unsigned long)walk->buffer | (unsigned long)walk->page))\n\t\tgoto out;\n\n\tif (walk->flags & SKCIPHER_WALK_PHYS)\n\t\tgoto out;\n\n\tif (walk->iv != walk->oiv)\n\t\tmemcpy(walk->oiv, walk->iv, walk->ivsize);\n\tif (walk->buffer != walk->page)\n\t\tkfree(walk->buffer);\n\tif (walk->page)\n\t\tfree_page((unsigned long)walk->page);\n\nout:\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(skcipher_walk_done);\n\nvoid skcipher_walk_complete(struct skcipher_walk *walk, int err)\n{\n\tstruct skcipher_walk_buffer *p, *tmp;\n\n\tlist_for_each_entry_safe(p, tmp, &walk->buffers, entry) {\n\t\tu8 *data;\n\n\t\tif (err)\n\t\t\tgoto done;\n\n\t\tdata = p->data;\n\t\tif (!data) {\n\t\t\tdata = PTR_ALIGN(&p->buffer[0], walk->alignmask + 1);\n\t\t\tdata = skcipher_get_spot(data, walk->stride);\n\t\t}\n\n\t\tscatterwalk_copychunks(data, &p->dst, p->len, 1);\n\n\t\tif (offset_in_page(p->data) + p->len + walk->stride >\n\t\t    PAGE_SIZE)\n\t\t\tfree_page((unsigned long)p->data);\n\ndone:\n\t\tlist_del(&p->entry);\n\t\tkfree(p);\n\t}\n\n\tif (!err && walk->iv != walk->oiv)\n\t\tmemcpy(walk->oiv, walk->iv, walk->ivsize);\n\tif (walk->buffer != walk->page)\n\t\tkfree(walk->buffer);\n\tif (walk->page)\n\t\tfree_page((unsigned long)walk->page);\n}\nEXPORT_SYMBOL_GPL(skcipher_walk_complete);\n\nstatic void skcipher_queue_write(struct skcipher_walk *walk,\n\t\t\t\t struct skcipher_walk_buffer *p)\n{\n\tp->dst = walk->out;\n\tlist_add_tail(&p->entry, &walk->buffers);\n}\n\nstatic int skcipher_next_slow(struct skcipher_walk *walk, unsigned int bsize)\n{\n\tbool phys = walk->flags & SKCIPHER_WALK_PHYS;\n\tunsigned alignmask = walk->alignmask;\n\tstruct skcipher_walk_buffer *p;\n\tunsigned a;\n\tunsigned n;\n\tu8 *buffer;\n\tvoid *v;\n\n\tif (!phys) {\n\t\tif (!walk->buffer)\n\t\t\twalk->buffer = walk->page;\n\t\tbuffer = walk->buffer;\n\t\tif (buffer)\n\t\t\tgoto ok;\n\t}\n\n\t \n\ta = crypto_tfm_ctx_alignment() - 1;\n\tn = bsize;\n\n\tif (phys) {\n\t\t \n\t\ta &= (sizeof(*p) ^ (sizeof(*p) - 1)) >> 1;\n\t\tn += sizeof(*p);\n\t}\n\n\t \n\tn += alignmask & ~a;\n\n\t \n\tn += (bsize - 1) & ~(alignmask | a);\n\n\tv = kzalloc(n, skcipher_walk_gfp(walk));\n\tif (!v)\n\t\treturn skcipher_walk_done(walk, -ENOMEM);\n\n\tif (phys) {\n\t\tp = v;\n\t\tp->len = bsize;\n\t\tskcipher_queue_write(walk, p);\n\t\tbuffer = p->buffer;\n\t} else {\n\t\twalk->buffer = v;\n\t\tbuffer = v;\n\t}\n\nok:\n\twalk->dst.virt.addr = PTR_ALIGN(buffer, alignmask + 1);\n\twalk->dst.virt.addr = skcipher_get_spot(walk->dst.virt.addr, bsize);\n\twalk->src.virt.addr = walk->dst.virt.addr;\n\n\tscatterwalk_copychunks(walk->src.virt.addr, &walk->in, bsize, 0);\n\n\twalk->nbytes = bsize;\n\twalk->flags |= SKCIPHER_WALK_SLOW;\n\n\treturn 0;\n}\n\nstatic int skcipher_next_copy(struct skcipher_walk *walk)\n{\n\tstruct skcipher_walk_buffer *p;\n\tu8 *tmp = walk->page;\n\n\tskcipher_map_src(walk);\n\tmemcpy(tmp, walk->src.virt.addr, walk->nbytes);\n\tskcipher_unmap_src(walk);\n\n\twalk->src.virt.addr = tmp;\n\twalk->dst.virt.addr = tmp;\n\n\tif (!(walk->flags & SKCIPHER_WALK_PHYS))\n\t\treturn 0;\n\n\tp = kmalloc(sizeof(*p), skcipher_walk_gfp(walk));\n\tif (!p)\n\t\treturn -ENOMEM;\n\n\tp->data = walk->page;\n\tp->len = walk->nbytes;\n\tskcipher_queue_write(walk, p);\n\n\tif (offset_in_page(walk->page) + walk->nbytes + walk->stride >\n\t    PAGE_SIZE)\n\t\twalk->page = NULL;\n\telse\n\t\twalk->page += walk->nbytes;\n\n\treturn 0;\n}\n\nstatic int skcipher_next_fast(struct skcipher_walk *walk)\n{\n\tunsigned long diff;\n\n\twalk->src.phys.page = scatterwalk_page(&walk->in);\n\twalk->src.phys.offset = offset_in_page(walk->in.offset);\n\twalk->dst.phys.page = scatterwalk_page(&walk->out);\n\twalk->dst.phys.offset = offset_in_page(walk->out.offset);\n\n\tif (walk->flags & SKCIPHER_WALK_PHYS)\n\t\treturn 0;\n\n\tdiff = walk->src.phys.offset - walk->dst.phys.offset;\n\tdiff |= walk->src.virt.page - walk->dst.virt.page;\n\n\tskcipher_map_src(walk);\n\twalk->dst.virt.addr = walk->src.virt.addr;\n\n\tif (diff) {\n\t\twalk->flags |= SKCIPHER_WALK_DIFF;\n\t\tskcipher_map_dst(walk);\n\t}\n\n\treturn 0;\n}\n\nstatic int skcipher_walk_next(struct skcipher_walk *walk)\n{\n\tunsigned int bsize;\n\tunsigned int n;\n\tint err;\n\n\twalk->flags &= ~(SKCIPHER_WALK_SLOW | SKCIPHER_WALK_COPY |\n\t\t\t SKCIPHER_WALK_DIFF);\n\n\tn = walk->total;\n\tbsize = min(walk->stride, max(n, walk->blocksize));\n\tn = scatterwalk_clamp(&walk->in, n);\n\tn = scatterwalk_clamp(&walk->out, n);\n\n\tif (unlikely(n < bsize)) {\n\t\tif (unlikely(walk->total < walk->blocksize))\n\t\t\treturn skcipher_walk_done(walk, -EINVAL);\n\nslow_path:\n\t\terr = skcipher_next_slow(walk, bsize);\n\t\tgoto set_phys_lowmem;\n\t}\n\n\tif (unlikely((walk->in.offset | walk->out.offset) & walk->alignmask)) {\n\t\tif (!walk->page) {\n\t\t\tgfp_t gfp = skcipher_walk_gfp(walk);\n\n\t\t\twalk->page = (void *)__get_free_page(gfp);\n\t\t\tif (!walk->page)\n\t\t\t\tgoto slow_path;\n\t\t}\n\n\t\twalk->nbytes = min_t(unsigned, n,\n\t\t\t\t     PAGE_SIZE - offset_in_page(walk->page));\n\t\twalk->flags |= SKCIPHER_WALK_COPY;\n\t\terr = skcipher_next_copy(walk);\n\t\tgoto set_phys_lowmem;\n\t}\n\n\twalk->nbytes = n;\n\n\treturn skcipher_next_fast(walk);\n\nset_phys_lowmem:\n\tif (!err && (walk->flags & SKCIPHER_WALK_PHYS)) {\n\t\twalk->src.phys.page = virt_to_page(walk->src.virt.addr);\n\t\twalk->dst.phys.page = virt_to_page(walk->dst.virt.addr);\n\t\twalk->src.phys.offset &= PAGE_SIZE - 1;\n\t\twalk->dst.phys.offset &= PAGE_SIZE - 1;\n\t}\n\treturn err;\n}\n\nstatic int skcipher_copy_iv(struct skcipher_walk *walk)\n{\n\tunsigned a = crypto_tfm_ctx_alignment() - 1;\n\tunsigned alignmask = walk->alignmask;\n\tunsigned ivsize = walk->ivsize;\n\tunsigned bs = walk->stride;\n\tunsigned aligned_bs;\n\tunsigned size;\n\tu8 *iv;\n\n\taligned_bs = ALIGN(bs, alignmask + 1);\n\n\t \n\tsize = alignmask & ~a;\n\n\tif (walk->flags & SKCIPHER_WALK_PHYS)\n\t\tsize += ivsize;\n\telse {\n\t\tsize += aligned_bs + ivsize;\n\n\t\t \n\t\tsize += (bs - 1) & ~(alignmask | a);\n\t}\n\n\twalk->buffer = kmalloc(size, skcipher_walk_gfp(walk));\n\tif (!walk->buffer)\n\t\treturn -ENOMEM;\n\n\tiv = PTR_ALIGN(walk->buffer, alignmask + 1);\n\tiv = skcipher_get_spot(iv, bs) + aligned_bs;\n\n\twalk->iv = memcpy(iv, walk->iv, walk->ivsize);\n\treturn 0;\n}\n\nstatic int skcipher_walk_first(struct skcipher_walk *walk)\n{\n\tif (WARN_ON_ONCE(in_hardirq()))\n\t\treturn -EDEADLK;\n\n\twalk->buffer = NULL;\n\tif (unlikely(((unsigned long)walk->iv & walk->alignmask))) {\n\t\tint err = skcipher_copy_iv(walk);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\twalk->page = NULL;\n\n\treturn skcipher_walk_next(walk);\n}\n\nstatic int skcipher_walk_skcipher(struct skcipher_walk *walk,\n\t\t\t\t  struct skcipher_request *req)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\n\twalk->total = req->cryptlen;\n\twalk->nbytes = 0;\n\twalk->iv = req->iv;\n\twalk->oiv = req->iv;\n\n\tif (unlikely(!walk->total))\n\t\treturn 0;\n\n\tscatterwalk_start(&walk->in, req->src);\n\tscatterwalk_start(&walk->out, req->dst);\n\n\twalk->flags &= ~SKCIPHER_WALK_SLEEP;\n\twalk->flags |= req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ?\n\t\t       SKCIPHER_WALK_SLEEP : 0;\n\n\twalk->blocksize = crypto_skcipher_blocksize(tfm);\n\twalk->stride = crypto_skcipher_walksize(tfm);\n\twalk->ivsize = crypto_skcipher_ivsize(tfm);\n\twalk->alignmask = crypto_skcipher_alignmask(tfm);\n\n\treturn skcipher_walk_first(walk);\n}\n\nint skcipher_walk_virt(struct skcipher_walk *walk,\n\t\t       struct skcipher_request *req, bool atomic)\n{\n\tint err;\n\n\tmight_sleep_if(req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP);\n\n\twalk->flags &= ~SKCIPHER_WALK_PHYS;\n\n\terr = skcipher_walk_skcipher(walk, req);\n\n\twalk->flags &= atomic ? ~SKCIPHER_WALK_SLEEP : ~0;\n\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(skcipher_walk_virt);\n\nint skcipher_walk_async(struct skcipher_walk *walk,\n\t\t\tstruct skcipher_request *req)\n{\n\twalk->flags |= SKCIPHER_WALK_PHYS;\n\n\tINIT_LIST_HEAD(&walk->buffers);\n\n\treturn skcipher_walk_skcipher(walk, req);\n}\nEXPORT_SYMBOL_GPL(skcipher_walk_async);\n\nstatic int skcipher_walk_aead_common(struct skcipher_walk *walk,\n\t\t\t\t     struct aead_request *req, bool atomic)\n{\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tint err;\n\n\twalk->nbytes = 0;\n\twalk->iv = req->iv;\n\twalk->oiv = req->iv;\n\n\tif (unlikely(!walk->total))\n\t\treturn 0;\n\n\twalk->flags &= ~SKCIPHER_WALK_PHYS;\n\n\tscatterwalk_start(&walk->in, req->src);\n\tscatterwalk_start(&walk->out, req->dst);\n\n\tscatterwalk_copychunks(NULL, &walk->in, req->assoclen, 2);\n\tscatterwalk_copychunks(NULL, &walk->out, req->assoclen, 2);\n\n\tscatterwalk_done(&walk->in, 0, walk->total);\n\tscatterwalk_done(&walk->out, 0, walk->total);\n\n\tif (req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP)\n\t\twalk->flags |= SKCIPHER_WALK_SLEEP;\n\telse\n\t\twalk->flags &= ~SKCIPHER_WALK_SLEEP;\n\n\twalk->blocksize = crypto_aead_blocksize(tfm);\n\twalk->stride = crypto_aead_chunksize(tfm);\n\twalk->ivsize = crypto_aead_ivsize(tfm);\n\twalk->alignmask = crypto_aead_alignmask(tfm);\n\n\terr = skcipher_walk_first(walk);\n\n\tif (atomic)\n\t\twalk->flags &= ~SKCIPHER_WALK_SLEEP;\n\n\treturn err;\n}\n\nint skcipher_walk_aead_encrypt(struct skcipher_walk *walk,\n\t\t\t       struct aead_request *req, bool atomic)\n{\n\twalk->total = req->cryptlen;\n\n\treturn skcipher_walk_aead_common(walk, req, atomic);\n}\nEXPORT_SYMBOL_GPL(skcipher_walk_aead_encrypt);\n\nint skcipher_walk_aead_decrypt(struct skcipher_walk *walk,\n\t\t\t       struct aead_request *req, bool atomic)\n{\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\n\twalk->total = req->cryptlen - crypto_aead_authsize(tfm);\n\n\treturn skcipher_walk_aead_common(walk, req, atomic);\n}\nEXPORT_SYMBOL_GPL(skcipher_walk_aead_decrypt);\n\nstatic void skcipher_set_needkey(struct crypto_skcipher *tfm)\n{\n\tif (crypto_skcipher_max_keysize(tfm) != 0)\n\t\tcrypto_skcipher_set_flags(tfm, CRYPTO_TFM_NEED_KEY);\n}\n\nstatic int skcipher_setkey_unaligned(struct crypto_skcipher *tfm,\n\t\t\t\t     const u8 *key, unsigned int keylen)\n{\n\tunsigned long alignmask = crypto_skcipher_alignmask(tfm);\n\tstruct skcipher_alg *cipher = crypto_skcipher_alg(tfm);\n\tu8 *buffer, *alignbuffer;\n\tunsigned long absize;\n\tint ret;\n\n\tabsize = keylen + alignmask;\n\tbuffer = kmalloc(absize, GFP_ATOMIC);\n\tif (!buffer)\n\t\treturn -ENOMEM;\n\n\talignbuffer = (u8 *)ALIGN((unsigned long)buffer, alignmask + 1);\n\tmemcpy(alignbuffer, key, keylen);\n\tret = cipher->setkey(tfm, alignbuffer, keylen);\n\tkfree_sensitive(buffer);\n\treturn ret;\n}\n\nint crypto_skcipher_setkey(struct crypto_skcipher *tfm, const u8 *key,\n\t\t\t   unsigned int keylen)\n{\n\tstruct skcipher_alg *cipher = crypto_skcipher_alg(tfm);\n\tunsigned long alignmask = crypto_skcipher_alignmask(tfm);\n\tint err;\n\n\tif (keylen < cipher->min_keysize || keylen > cipher->max_keysize)\n\t\treturn -EINVAL;\n\n\tif ((unsigned long)key & alignmask)\n\t\terr = skcipher_setkey_unaligned(tfm, key, keylen);\n\telse\n\t\terr = cipher->setkey(tfm, key, keylen);\n\n\tif (unlikely(err)) {\n\t\tskcipher_set_needkey(tfm);\n\t\treturn err;\n\t}\n\n\tcrypto_skcipher_clear_flags(tfm, CRYPTO_TFM_NEED_KEY);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(crypto_skcipher_setkey);\n\nint crypto_skcipher_encrypt(struct skcipher_request *req)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tstruct skcipher_alg *alg = crypto_skcipher_alg(tfm);\n\tint ret;\n\n\tif (IS_ENABLED(CONFIG_CRYPTO_STATS)) {\n\t\tstruct crypto_istat_cipher *istat = skcipher_get_stat(alg);\n\n\t\tatomic64_inc(&istat->encrypt_cnt);\n\t\tatomic64_add(req->cryptlen, &istat->encrypt_tlen);\n\t}\n\n\tif (crypto_skcipher_get_flags(tfm) & CRYPTO_TFM_NEED_KEY)\n\t\tret = -ENOKEY;\n\telse\n\t\tret = alg->encrypt(req);\n\n\treturn crypto_skcipher_errstat(alg, ret);\n}\nEXPORT_SYMBOL_GPL(crypto_skcipher_encrypt);\n\nint crypto_skcipher_decrypt(struct skcipher_request *req)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tstruct skcipher_alg *alg = crypto_skcipher_alg(tfm);\n\tint ret;\n\n\tif (IS_ENABLED(CONFIG_CRYPTO_STATS)) {\n\t\tstruct crypto_istat_cipher *istat = skcipher_get_stat(alg);\n\n\t\tatomic64_inc(&istat->decrypt_cnt);\n\t\tatomic64_add(req->cryptlen, &istat->decrypt_tlen);\n\t}\n\n\tif (crypto_skcipher_get_flags(tfm) & CRYPTO_TFM_NEED_KEY)\n\t\tret = -ENOKEY;\n\telse\n\t\tret = alg->decrypt(req);\n\n\treturn crypto_skcipher_errstat(alg, ret);\n}\nEXPORT_SYMBOL_GPL(crypto_skcipher_decrypt);\n\nstatic void crypto_skcipher_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_skcipher *skcipher = __crypto_skcipher_cast(tfm);\n\tstruct skcipher_alg *alg = crypto_skcipher_alg(skcipher);\n\n\talg->exit(skcipher);\n}\n\nstatic int crypto_skcipher_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_skcipher *skcipher = __crypto_skcipher_cast(tfm);\n\tstruct skcipher_alg *alg = crypto_skcipher_alg(skcipher);\n\n\tskcipher_set_needkey(skcipher);\n\n\tif (alg->exit)\n\t\tskcipher->base.exit = crypto_skcipher_exit_tfm;\n\n\tif (alg->init)\n\t\treturn alg->init(skcipher);\n\n\treturn 0;\n}\n\nstatic void crypto_skcipher_free_instance(struct crypto_instance *inst)\n{\n\tstruct skcipher_instance *skcipher =\n\t\tcontainer_of(inst, struct skcipher_instance, s.base);\n\n\tskcipher->free(skcipher);\n}\n\nstatic void crypto_skcipher_show(struct seq_file *m, struct crypto_alg *alg)\n\t__maybe_unused;\nstatic void crypto_skcipher_show(struct seq_file *m, struct crypto_alg *alg)\n{\n\tstruct skcipher_alg *skcipher = __crypto_skcipher_alg(alg);\n\n\tseq_printf(m, \"type         : skcipher\\n\");\n\tseq_printf(m, \"async        : %s\\n\",\n\t\t   alg->cra_flags & CRYPTO_ALG_ASYNC ?  \"yes\" : \"no\");\n\tseq_printf(m, \"blocksize    : %u\\n\", alg->cra_blocksize);\n\tseq_printf(m, \"min keysize  : %u\\n\", skcipher->min_keysize);\n\tseq_printf(m, \"max keysize  : %u\\n\", skcipher->max_keysize);\n\tseq_printf(m, \"ivsize       : %u\\n\", skcipher->ivsize);\n\tseq_printf(m, \"chunksize    : %u\\n\", skcipher->chunksize);\n\tseq_printf(m, \"walksize     : %u\\n\", skcipher->walksize);\n}\n\nstatic int __maybe_unused crypto_skcipher_report(\n\tstruct sk_buff *skb, struct crypto_alg *alg)\n{\n\tstruct skcipher_alg *skcipher = __crypto_skcipher_alg(alg);\n\tstruct crypto_report_blkcipher rblkcipher;\n\n\tmemset(&rblkcipher, 0, sizeof(rblkcipher));\n\n\tstrscpy(rblkcipher.type, \"skcipher\", sizeof(rblkcipher.type));\n\tstrscpy(rblkcipher.geniv, \"<none>\", sizeof(rblkcipher.geniv));\n\n\trblkcipher.blocksize = alg->cra_blocksize;\n\trblkcipher.min_keysize = skcipher->min_keysize;\n\trblkcipher.max_keysize = skcipher->max_keysize;\n\trblkcipher.ivsize = skcipher->ivsize;\n\n\treturn nla_put(skb, CRYPTOCFGA_REPORT_BLKCIPHER,\n\t\t       sizeof(rblkcipher), &rblkcipher);\n}\n\nstatic int __maybe_unused crypto_skcipher_report_stat(\n\tstruct sk_buff *skb, struct crypto_alg *alg)\n{\n\tstruct skcipher_alg *skcipher = __crypto_skcipher_alg(alg);\n\tstruct crypto_istat_cipher *istat;\n\tstruct crypto_stat_cipher rcipher;\n\n\tistat = skcipher_get_stat(skcipher);\n\n\tmemset(&rcipher, 0, sizeof(rcipher));\n\n\tstrscpy(rcipher.type, \"cipher\", sizeof(rcipher.type));\n\n\trcipher.stat_encrypt_cnt = atomic64_read(&istat->encrypt_cnt);\n\trcipher.stat_encrypt_tlen = atomic64_read(&istat->encrypt_tlen);\n\trcipher.stat_decrypt_cnt =  atomic64_read(&istat->decrypt_cnt);\n\trcipher.stat_decrypt_tlen = atomic64_read(&istat->decrypt_tlen);\n\trcipher.stat_err_cnt =  atomic64_read(&istat->err_cnt);\n\n\treturn nla_put(skb, CRYPTOCFGA_STAT_CIPHER, sizeof(rcipher), &rcipher);\n}\n\nstatic const struct crypto_type crypto_skcipher_type = {\n\t.extsize = crypto_alg_extsize,\n\t.init_tfm = crypto_skcipher_init_tfm,\n\t.free = crypto_skcipher_free_instance,\n#ifdef CONFIG_PROC_FS\n\t.show = crypto_skcipher_show,\n#endif\n#if IS_ENABLED(CONFIG_CRYPTO_USER)\n\t.report = crypto_skcipher_report,\n#endif\n#ifdef CONFIG_CRYPTO_STATS\n\t.report_stat = crypto_skcipher_report_stat,\n#endif\n\t.maskclear = ~CRYPTO_ALG_TYPE_MASK,\n\t.maskset = CRYPTO_ALG_TYPE_MASK,\n\t.type = CRYPTO_ALG_TYPE_SKCIPHER,\n\t.tfmsize = offsetof(struct crypto_skcipher, base),\n};\n\nint crypto_grab_skcipher(struct crypto_skcipher_spawn *spawn,\n\t\t\t struct crypto_instance *inst,\n\t\t\t const char *name, u32 type, u32 mask)\n{\n\tspawn->base.frontend = &crypto_skcipher_type;\n\treturn crypto_grab_spawn(&spawn->base, inst, name, type, mask);\n}\nEXPORT_SYMBOL_GPL(crypto_grab_skcipher);\n\nstruct crypto_skcipher *crypto_alloc_skcipher(const char *alg_name,\n\t\t\t\t\t      u32 type, u32 mask)\n{\n\treturn crypto_alloc_tfm(alg_name, &crypto_skcipher_type, type, mask);\n}\nEXPORT_SYMBOL_GPL(crypto_alloc_skcipher);\n\nstruct crypto_sync_skcipher *crypto_alloc_sync_skcipher(\n\t\t\t\tconst char *alg_name, u32 type, u32 mask)\n{\n\tstruct crypto_skcipher *tfm;\n\n\t \n\tmask |= CRYPTO_ALG_ASYNC | CRYPTO_ALG_SKCIPHER_REQSIZE_LARGE;\n\n\ttfm = crypto_alloc_tfm(alg_name, &crypto_skcipher_type, type, mask);\n\n\t \n\tif (!IS_ERR(tfm) && WARN_ON(crypto_skcipher_reqsize(tfm) >\n\t\t\t\t    MAX_SYNC_SKCIPHER_REQSIZE)) {\n\t\tcrypto_free_skcipher(tfm);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\treturn (struct crypto_sync_skcipher *)tfm;\n}\nEXPORT_SYMBOL_GPL(crypto_alloc_sync_skcipher);\n\nint crypto_has_skcipher(const char *alg_name, u32 type, u32 mask)\n{\n\treturn crypto_type_has_alg(alg_name, &crypto_skcipher_type, type, mask);\n}\nEXPORT_SYMBOL_GPL(crypto_has_skcipher);\n\nstatic int skcipher_prepare_alg(struct skcipher_alg *alg)\n{\n\tstruct crypto_istat_cipher *istat = skcipher_get_stat(alg);\n\tstruct crypto_alg *base = &alg->base;\n\n\tif (alg->ivsize > PAGE_SIZE / 8 || alg->chunksize > PAGE_SIZE / 8 ||\n\t    alg->walksize > PAGE_SIZE / 8)\n\t\treturn -EINVAL;\n\n\tif (!alg->chunksize)\n\t\talg->chunksize = base->cra_blocksize;\n\tif (!alg->walksize)\n\t\talg->walksize = alg->chunksize;\n\n\tbase->cra_type = &crypto_skcipher_type;\n\tbase->cra_flags &= ~CRYPTO_ALG_TYPE_MASK;\n\tbase->cra_flags |= CRYPTO_ALG_TYPE_SKCIPHER;\n\n\tif (IS_ENABLED(CONFIG_CRYPTO_STATS))\n\t\tmemset(istat, 0, sizeof(*istat));\n\n\treturn 0;\n}\n\nint crypto_register_skcipher(struct skcipher_alg *alg)\n{\n\tstruct crypto_alg *base = &alg->base;\n\tint err;\n\n\terr = skcipher_prepare_alg(alg);\n\tif (err)\n\t\treturn err;\n\n\treturn crypto_register_alg(base);\n}\nEXPORT_SYMBOL_GPL(crypto_register_skcipher);\n\nvoid crypto_unregister_skcipher(struct skcipher_alg *alg)\n{\n\tcrypto_unregister_alg(&alg->base);\n}\nEXPORT_SYMBOL_GPL(crypto_unregister_skcipher);\n\nint crypto_register_skciphers(struct skcipher_alg *algs, int count)\n{\n\tint i, ret;\n\n\tfor (i = 0; i < count; i++) {\n\t\tret = crypto_register_skcipher(&algs[i]);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\treturn 0;\n\nerr:\n\tfor (--i; i >= 0; --i)\n\t\tcrypto_unregister_skcipher(&algs[i]);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(crypto_register_skciphers);\n\nvoid crypto_unregister_skciphers(struct skcipher_alg *algs, int count)\n{\n\tint i;\n\n\tfor (i = count - 1; i >= 0; --i)\n\t\tcrypto_unregister_skcipher(&algs[i]);\n}\nEXPORT_SYMBOL_GPL(crypto_unregister_skciphers);\n\nint skcipher_register_instance(struct crypto_template *tmpl,\n\t\t\t   struct skcipher_instance *inst)\n{\n\tint err;\n\n\tif (WARN_ON(!inst->free))\n\t\treturn -EINVAL;\n\n\terr = skcipher_prepare_alg(&inst->alg);\n\tif (err)\n\t\treturn err;\n\n\treturn crypto_register_instance(tmpl, skcipher_crypto_instance(inst));\n}\nEXPORT_SYMBOL_GPL(skcipher_register_instance);\n\nstatic int skcipher_setkey_simple(struct crypto_skcipher *tfm, const u8 *key,\n\t\t\t\t  unsigned int keylen)\n{\n\tstruct crypto_cipher *cipher = skcipher_cipher_simple(tfm);\n\n\tcrypto_cipher_clear_flags(cipher, CRYPTO_TFM_REQ_MASK);\n\tcrypto_cipher_set_flags(cipher, crypto_skcipher_get_flags(tfm) &\n\t\t\t\tCRYPTO_TFM_REQ_MASK);\n\treturn crypto_cipher_setkey(cipher, key, keylen);\n}\n\nstatic int skcipher_init_tfm_simple(struct crypto_skcipher *tfm)\n{\n\tstruct skcipher_instance *inst = skcipher_alg_instance(tfm);\n\tstruct crypto_cipher_spawn *spawn = skcipher_instance_ctx(inst);\n\tstruct skcipher_ctx_simple *ctx = crypto_skcipher_ctx(tfm);\n\tstruct crypto_cipher *cipher;\n\n\tcipher = crypto_spawn_cipher(spawn);\n\tif (IS_ERR(cipher))\n\t\treturn PTR_ERR(cipher);\n\n\tctx->cipher = cipher;\n\treturn 0;\n}\n\nstatic void skcipher_exit_tfm_simple(struct crypto_skcipher *tfm)\n{\n\tstruct skcipher_ctx_simple *ctx = crypto_skcipher_ctx(tfm);\n\n\tcrypto_free_cipher(ctx->cipher);\n}\n\nstatic void skcipher_free_instance_simple(struct skcipher_instance *inst)\n{\n\tcrypto_drop_cipher(skcipher_instance_ctx(inst));\n\tkfree(inst);\n}\n\n \nstruct skcipher_instance *skcipher_alloc_instance_simple(\n\tstruct crypto_template *tmpl, struct rtattr **tb)\n{\n\tu32 mask;\n\tstruct skcipher_instance *inst;\n\tstruct crypto_cipher_spawn *spawn;\n\tstruct crypto_alg *cipher_alg;\n\tint err;\n\n\terr = crypto_check_attr_type(tb, CRYPTO_ALG_TYPE_SKCIPHER, &mask);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\tinst = kzalloc(sizeof(*inst) + sizeof(*spawn), GFP_KERNEL);\n\tif (!inst)\n\t\treturn ERR_PTR(-ENOMEM);\n\tspawn = skcipher_instance_ctx(inst);\n\n\terr = crypto_grab_cipher(spawn, skcipher_crypto_instance(inst),\n\t\t\t\t crypto_attr_alg_name(tb[1]), 0, mask);\n\tif (err)\n\t\tgoto err_free_inst;\n\tcipher_alg = crypto_spawn_cipher_alg(spawn);\n\n\terr = crypto_inst_setname(skcipher_crypto_instance(inst), tmpl->name,\n\t\t\t\t  cipher_alg);\n\tif (err)\n\t\tgoto err_free_inst;\n\n\tinst->free = skcipher_free_instance_simple;\n\n\t \n\tinst->alg.base.cra_blocksize = cipher_alg->cra_blocksize;\n\tinst->alg.base.cra_alignmask = cipher_alg->cra_alignmask;\n\tinst->alg.base.cra_priority = cipher_alg->cra_priority;\n\tinst->alg.min_keysize = cipher_alg->cra_cipher.cia_min_keysize;\n\tinst->alg.max_keysize = cipher_alg->cra_cipher.cia_max_keysize;\n\tinst->alg.ivsize = cipher_alg->cra_blocksize;\n\n\t \n\tinst->alg.base.cra_ctxsize = sizeof(struct skcipher_ctx_simple);\n\tinst->alg.setkey = skcipher_setkey_simple;\n\tinst->alg.init = skcipher_init_tfm_simple;\n\tinst->alg.exit = skcipher_exit_tfm_simple;\n\n\treturn inst;\n\nerr_free_inst:\n\tskcipher_free_instance_simple(inst);\n\treturn ERR_PTR(err);\n}\nEXPORT_SYMBOL_GPL(skcipher_alloc_instance_simple);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"Symmetric key cipher type\");\nMODULE_IMPORT_NS(CRYPTO_INTERNAL);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}