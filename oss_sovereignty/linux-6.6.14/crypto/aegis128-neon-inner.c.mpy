{
  "module_name": "aegis128-neon-inner.c",
  "hash_id": "8d1478cb5f57271dbb82e11f8563f11ca73f1321162cd900ad3fe496f289b2ab",
  "original_prompt": "Ingested from linux-6.6.14/crypto/aegis128-neon-inner.c",
  "human_readable_source": "\n \n\n#ifdef CONFIG_ARM64\n#include <asm/neon-intrinsics.h>\n\n#define AES_ROUND\t\"aese %0.16b, %1.16b \\n\\t aesmc %0.16b, %0.16b\"\n#else\n#include <arm_neon.h>\n\n#define AES_ROUND\t\"aese.8 %q0, %q1 \\n\\t aesmc.8 %q0, %q0\"\n#endif\n\n#define AEGIS_BLOCK_SIZE\t16\n\n#include <stddef.h>\n#include \"aegis-neon.h\"\n\nextern int aegis128_have_aes_insn;\n\nvoid *memcpy(void *dest, const void *src, size_t n);\n\nstruct aegis128_state {\n\tuint8x16_t v[5];\n};\n\nextern const uint8_t crypto_aes_sbox[];\n\nstatic struct aegis128_state aegis128_load_state_neon(const void *state)\n{\n\treturn (struct aegis128_state){ {\n\t\tvld1q_u8(state),\n\t\tvld1q_u8(state + 16),\n\t\tvld1q_u8(state + 32),\n\t\tvld1q_u8(state + 48),\n\t\tvld1q_u8(state + 64)\n\t} };\n}\n\nstatic void aegis128_save_state_neon(struct aegis128_state st, void *state)\n{\n\tvst1q_u8(state, st.v[0]);\n\tvst1q_u8(state + 16, st.v[1]);\n\tvst1q_u8(state + 32, st.v[2]);\n\tvst1q_u8(state + 48, st.v[3]);\n\tvst1q_u8(state + 64, st.v[4]);\n}\n\nstatic inline __attribute__((always_inline))\nuint8x16_t aegis_aes_round(uint8x16_t w)\n{\n\tuint8x16_t z = {};\n\n#ifdef CONFIG_ARM64\n\tif (!__builtin_expect(aegis128_have_aes_insn, 1)) {\n\t\tstatic const uint8_t shift_rows[] = {\n\t\t\t0x0, 0x5, 0xa, 0xf, 0x4, 0x9, 0xe, 0x3,\n\t\t\t0x8, 0xd, 0x2, 0x7, 0xc, 0x1, 0x6, 0xb,\n\t\t};\n\t\tstatic const uint8_t ror32by8[] = {\n\t\t\t0x1, 0x2, 0x3, 0x0, 0x5, 0x6, 0x7, 0x4,\n\t\t\t0x9, 0xa, 0xb, 0x8, 0xd, 0xe, 0xf, 0xc,\n\t\t};\n\t\tuint8x16_t v;\n\n\t\t\n\t\tw = vqtbl1q_u8(w, vld1q_u8(shift_rows));\n\n\t\t\n#ifndef CONFIG_CC_IS_GCC\n\t\tv = vqtbl4q_u8(vld1q_u8_x4(crypto_aes_sbox), w);\n\t\tv = vqtbx4q_u8(v, vld1q_u8_x4(crypto_aes_sbox + 0x40), w - 0x40);\n\t\tv = vqtbx4q_u8(v, vld1q_u8_x4(crypto_aes_sbox + 0x80), w - 0x80);\n\t\tv = vqtbx4q_u8(v, vld1q_u8_x4(crypto_aes_sbox + 0xc0), w - 0xc0);\n#else\n\t\tasm(\"tbl %0.16b, {v16.16b-v19.16b}, %1.16b\" : \"=w\"(v) : \"w\"(w));\n\t\tw -= 0x40;\n\t\tasm(\"tbx %0.16b, {v20.16b-v23.16b}, %1.16b\" : \"+w\"(v) : \"w\"(w));\n\t\tw -= 0x40;\n\t\tasm(\"tbx %0.16b, {v24.16b-v27.16b}, %1.16b\" : \"+w\"(v) : \"w\"(w));\n\t\tw -= 0x40;\n\t\tasm(\"tbx %0.16b, {v28.16b-v31.16b}, %1.16b\" : \"+w\"(v) : \"w\"(w));\n#endif\n\n\t\t\n\t\tw = (v << 1) ^ (uint8x16_t)(((int8x16_t)v >> 7) & 0x1b);\n\t\tw ^= (uint8x16_t)vrev32q_u16((uint16x8_t)v);\n\t\tw ^= vqtbl1q_u8(v ^ w, vld1q_u8(ror32by8));\n\n\t\treturn w;\n\t}\n#endif\n\n\t \n\tasm(AES_ROUND : \"+w\"(w) : \"w\"(z));\n\treturn w;\n}\n\nstatic inline __attribute__((always_inline))\nstruct aegis128_state aegis128_update_neon(struct aegis128_state st,\n\t\t\t\t\t   uint8x16_t m)\n{\n\tm       ^= aegis_aes_round(st.v[4]);\n\tst.v[4] ^= aegis_aes_round(st.v[3]);\n\tst.v[3] ^= aegis_aes_round(st.v[2]);\n\tst.v[2] ^= aegis_aes_round(st.v[1]);\n\tst.v[1] ^= aegis_aes_round(st.v[0]);\n\tst.v[0] ^= m;\n\n\treturn st;\n}\n\nstatic inline __attribute__((always_inline))\nvoid preload_sbox(void)\n{\n\tif (!IS_ENABLED(CONFIG_ARM64) ||\n\t    !IS_ENABLED(CONFIG_CC_IS_GCC) ||\n\t    __builtin_expect(aegis128_have_aes_insn, 1))\n\t\treturn;\n\n\tasm(\"ld1\t{v16.16b-v19.16b}, [%0], #64\t\\n\\t\"\n\t    \"ld1\t{v20.16b-v23.16b}, [%0], #64\t\\n\\t\"\n\t    \"ld1\t{v24.16b-v27.16b}, [%0], #64\t\\n\\t\"\n\t    \"ld1\t{v28.16b-v31.16b}, [%0]\t\t\\n\\t\"\n\t    :: \"r\"(crypto_aes_sbox));\n}\n\nvoid crypto_aegis128_init_neon(void *state, const void *key, const void *iv)\n{\n\tstatic const uint8_t const0[] = {\n\t\t0x00, 0x01, 0x01, 0x02, 0x03, 0x05, 0x08, 0x0d,\n\t\t0x15, 0x22, 0x37, 0x59, 0x90, 0xe9, 0x79, 0x62,\n\t};\n\tstatic const uint8_t const1[] = {\n\t\t0xdb, 0x3d, 0x18, 0x55, 0x6d, 0xc2, 0x2f, 0xf1,\n\t\t0x20, 0x11, 0x31, 0x42, 0x73, 0xb5, 0x28, 0xdd,\n\t};\n\tuint8x16_t k = vld1q_u8(key);\n\tuint8x16_t kiv = k ^ vld1q_u8(iv);\n\tstruct aegis128_state st = {{\n\t\tkiv,\n\t\tvld1q_u8(const1),\n\t\tvld1q_u8(const0),\n\t\tk ^ vld1q_u8(const0),\n\t\tk ^ vld1q_u8(const1),\n\t}};\n\tint i;\n\n\tpreload_sbox();\n\n\tfor (i = 0; i < 5; i++) {\n\t\tst = aegis128_update_neon(st, k);\n\t\tst = aegis128_update_neon(st, kiv);\n\t}\n\taegis128_save_state_neon(st, state);\n}\n\nvoid crypto_aegis128_update_neon(void *state, const void *msg)\n{\n\tstruct aegis128_state st = aegis128_load_state_neon(state);\n\n\tpreload_sbox();\n\n\tst = aegis128_update_neon(st, vld1q_u8(msg));\n\n\taegis128_save_state_neon(st, state);\n}\n\n#ifdef CONFIG_ARM\n \nstatic uint8x16_t vqtbl1q_u8(uint8x16_t a, uint8x16_t b)\n{\n\tunion {\n\t\tuint8x16_t\tval;\n\t\tuint8x8x2_t\tpair;\n\t} __a = { a };\n\n\treturn vcombine_u8(vtbl2_u8(__a.pair, vget_low_u8(b)),\n\t\t\t   vtbl2_u8(__a.pair, vget_high_u8(b)));\n}\n\nstatic uint8x16_t vqtbx1q_u8(uint8x16_t v, uint8x16_t a, uint8x16_t b)\n{\n\tunion {\n\t\tuint8x16_t\tval;\n\t\tuint8x8x2_t\tpair;\n\t} __a = { a };\n\n\treturn vcombine_u8(vtbx2_u8(vget_low_u8(v), __a.pair, vget_low_u8(b)),\n\t\t\t   vtbx2_u8(vget_high_u8(v), __a.pair, vget_high_u8(b)));\n}\n\nstatic int8_t vminvq_s8(int8x16_t v)\n{\n\tint8x8_t s = vpmin_s8(vget_low_s8(v), vget_high_s8(v));\n\n\ts = vpmin_s8(s, s);\n\ts = vpmin_s8(s, s);\n\ts = vpmin_s8(s, s);\n\n\treturn vget_lane_s8(s, 0);\n}\n#endif\n\nstatic const uint8_t permute[] __aligned(64) = {\n\t-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n\t 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n\t-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n};\n\nvoid crypto_aegis128_encrypt_chunk_neon(void *state, void *dst, const void *src,\n\t\t\t\t\tunsigned int size)\n{\n\tstruct aegis128_state st = aegis128_load_state_neon(state);\n\tconst int short_input = size < AEGIS_BLOCK_SIZE;\n\tuint8x16_t msg;\n\n\tpreload_sbox();\n\n\twhile (size >= AEGIS_BLOCK_SIZE) {\n\t\tuint8x16_t s = st.v[1] ^ (st.v[2] & st.v[3]) ^ st.v[4];\n\n\t\tmsg = vld1q_u8(src);\n\t\tst = aegis128_update_neon(st, msg);\n\t\tmsg ^= s;\n\t\tvst1q_u8(dst, msg);\n\n\t\tsize -= AEGIS_BLOCK_SIZE;\n\t\tsrc += AEGIS_BLOCK_SIZE;\n\t\tdst += AEGIS_BLOCK_SIZE;\n\t}\n\n\tif (size > 0) {\n\t\tuint8x16_t s = st.v[1] ^ (st.v[2] & st.v[3]) ^ st.v[4];\n\t\tuint8_t buf[AEGIS_BLOCK_SIZE];\n\t\tconst void *in = src;\n\t\tvoid *out = dst;\n\t\tuint8x16_t m;\n\n\t\tif (__builtin_expect(short_input, 0))\n\t\t\tin = out = memcpy(buf + AEGIS_BLOCK_SIZE - size, src, size);\n\n\t\tm = vqtbl1q_u8(vld1q_u8(in + size - AEGIS_BLOCK_SIZE),\n\t\t\t       vld1q_u8(permute + 32 - size));\n\n\t\tst = aegis128_update_neon(st, m);\n\n\t\tvst1q_u8(out + size - AEGIS_BLOCK_SIZE,\n\t\t\t vqtbl1q_u8(m ^ s, vld1q_u8(permute + size)));\n\n\t\tif (__builtin_expect(short_input, 0))\n\t\t\tmemcpy(dst, out, size);\n\t\telse\n\t\t\tvst1q_u8(out - AEGIS_BLOCK_SIZE, msg);\n\t}\n\n\taegis128_save_state_neon(st, state);\n}\n\nvoid crypto_aegis128_decrypt_chunk_neon(void *state, void *dst, const void *src,\n\t\t\t\t\tunsigned int size)\n{\n\tstruct aegis128_state st = aegis128_load_state_neon(state);\n\tconst int short_input = size < AEGIS_BLOCK_SIZE;\n\tuint8x16_t msg;\n\n\tpreload_sbox();\n\n\twhile (size >= AEGIS_BLOCK_SIZE) {\n\t\tmsg = vld1q_u8(src) ^ st.v[1] ^ (st.v[2] & st.v[3]) ^ st.v[4];\n\t\tst = aegis128_update_neon(st, msg);\n\t\tvst1q_u8(dst, msg);\n\n\t\tsize -= AEGIS_BLOCK_SIZE;\n\t\tsrc += AEGIS_BLOCK_SIZE;\n\t\tdst += AEGIS_BLOCK_SIZE;\n\t}\n\n\tif (size > 0) {\n\t\tuint8x16_t s = st.v[1] ^ (st.v[2] & st.v[3]) ^ st.v[4];\n\t\tuint8_t buf[AEGIS_BLOCK_SIZE];\n\t\tconst void *in = src;\n\t\tvoid *out = dst;\n\t\tuint8x16_t m;\n\n\t\tif (__builtin_expect(short_input, 0))\n\t\t\tin = out = memcpy(buf + AEGIS_BLOCK_SIZE - size, src, size);\n\n\t\tm = s ^ vqtbx1q_u8(s, vld1q_u8(in + size - AEGIS_BLOCK_SIZE),\n\t\t\t\t   vld1q_u8(permute + 32 - size));\n\n\t\tst = aegis128_update_neon(st, m);\n\n\t\tvst1q_u8(out + size - AEGIS_BLOCK_SIZE,\n\t\t\t vqtbl1q_u8(m, vld1q_u8(permute + size)));\n\n\t\tif (__builtin_expect(short_input, 0))\n\t\t\tmemcpy(dst, out, size);\n\t\telse\n\t\t\tvst1q_u8(out - AEGIS_BLOCK_SIZE, msg);\n\t}\n\n\taegis128_save_state_neon(st, state);\n}\n\nint crypto_aegis128_final_neon(void *state, void *tag_xor,\n\t\t\t       unsigned int assoclen,\n\t\t\t       unsigned int cryptlen,\n\t\t\t       unsigned int authsize)\n{\n\tstruct aegis128_state st = aegis128_load_state_neon(state);\n\tuint8x16_t v;\n\tint i;\n\n\tpreload_sbox();\n\n\tv = st.v[3] ^ (uint8x16_t)vcombine_u64(vmov_n_u64(8ULL * assoclen),\n\t\t\t\t\t       vmov_n_u64(8ULL * cryptlen));\n\n\tfor (i = 0; i < 7; i++)\n\t\tst = aegis128_update_neon(st, v);\n\n\tv = st.v[0] ^ st.v[1] ^ st.v[2] ^ st.v[3] ^ st.v[4];\n\n\tif (authsize > 0) {\n\t\tv = vqtbl1q_u8(~vceqq_u8(v, vld1q_u8(tag_xor)),\n\t\t\t       vld1q_u8(permute + authsize));\n\n\t\treturn vminvq_s8((int8x16_t)v);\n\t}\n\n\tvst1q_u8(tag_xor, v);\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}