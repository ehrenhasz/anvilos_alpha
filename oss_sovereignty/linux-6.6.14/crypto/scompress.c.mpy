{
  "module_name": "scompress.c",
  "hash_id": "d83cc0b55396e7eb796d16b2739ccfbc4b32fda6cc71b90700f46e62ce8dfb88",
  "original_prompt": "Ingested from linux-6.6.14/crypto/scompress.c",
  "human_readable_source": "\n \n\n#include <crypto/internal/acompress.h>\n#include <crypto/internal/scompress.h>\n#include <crypto/scatterwalk.h>\n#include <linux/cryptouser.h>\n#include <linux/err.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/scatterlist.h>\n#include <linux/seq_file.h>\n#include <linux/slab.h>\n#include <linux/string.h>\n#include <linux/vmalloc.h>\n#include <net/netlink.h>\n\n#include \"compress.h\"\n\nstruct scomp_scratch {\n\tspinlock_t\tlock;\n\tvoid\t\t*src;\n\tvoid\t\t*dst;\n};\n\nstatic DEFINE_PER_CPU(struct scomp_scratch, scomp_scratch) = {\n\t.lock = __SPIN_LOCK_UNLOCKED(scomp_scratch.lock),\n};\n\nstatic const struct crypto_type crypto_scomp_type;\nstatic int scomp_scratch_users;\nstatic DEFINE_MUTEX(scomp_lock);\n\nstatic int __maybe_unused crypto_scomp_report(\n\tstruct sk_buff *skb, struct crypto_alg *alg)\n{\n\tstruct crypto_report_comp rscomp;\n\n\tmemset(&rscomp, 0, sizeof(rscomp));\n\n\tstrscpy(rscomp.type, \"scomp\", sizeof(rscomp.type));\n\n\treturn nla_put(skb, CRYPTOCFGA_REPORT_COMPRESS,\n\t\t       sizeof(rscomp), &rscomp);\n}\n\nstatic void crypto_scomp_show(struct seq_file *m, struct crypto_alg *alg)\n\t__maybe_unused;\n\nstatic void crypto_scomp_show(struct seq_file *m, struct crypto_alg *alg)\n{\n\tseq_puts(m, \"type         : scomp\\n\");\n}\n\nstatic void crypto_scomp_free_scratches(void)\n{\n\tstruct scomp_scratch *scratch;\n\tint i;\n\n\tfor_each_possible_cpu(i) {\n\t\tscratch = per_cpu_ptr(&scomp_scratch, i);\n\n\t\tvfree(scratch->src);\n\t\tvfree(scratch->dst);\n\t\tscratch->src = NULL;\n\t\tscratch->dst = NULL;\n\t}\n}\n\nstatic int crypto_scomp_alloc_scratches(void)\n{\n\tstruct scomp_scratch *scratch;\n\tint i;\n\n\tfor_each_possible_cpu(i) {\n\t\tvoid *mem;\n\n\t\tscratch = per_cpu_ptr(&scomp_scratch, i);\n\n\t\tmem = vmalloc_node(SCOMP_SCRATCH_SIZE, cpu_to_node(i));\n\t\tif (!mem)\n\t\t\tgoto error;\n\t\tscratch->src = mem;\n\t\tmem = vmalloc_node(SCOMP_SCRATCH_SIZE, cpu_to_node(i));\n\t\tif (!mem)\n\t\t\tgoto error;\n\t\tscratch->dst = mem;\n\t}\n\treturn 0;\nerror:\n\tcrypto_scomp_free_scratches();\n\treturn -ENOMEM;\n}\n\nstatic int crypto_scomp_init_tfm(struct crypto_tfm *tfm)\n{\n\tint ret = 0;\n\n\tmutex_lock(&scomp_lock);\n\tif (!scomp_scratch_users++)\n\t\tret = crypto_scomp_alloc_scratches();\n\tmutex_unlock(&scomp_lock);\n\n\treturn ret;\n}\n\nstatic int scomp_acomp_comp_decomp(struct acomp_req *req, int dir)\n{\n\tstruct crypto_acomp *tfm = crypto_acomp_reqtfm(req);\n\tvoid **tfm_ctx = acomp_tfm_ctx(tfm);\n\tstruct crypto_scomp *scomp = *tfm_ctx;\n\tvoid **ctx = acomp_request_ctx(req);\n\tstruct scomp_scratch *scratch;\n\tunsigned int dlen;\n\tint ret;\n\n\tif (!req->src || !req->slen || req->slen > SCOMP_SCRATCH_SIZE)\n\t\treturn -EINVAL;\n\n\tif (req->dst && !req->dlen)\n\t\treturn -EINVAL;\n\n\tif (!req->dlen || req->dlen > SCOMP_SCRATCH_SIZE)\n\t\treq->dlen = SCOMP_SCRATCH_SIZE;\n\n\tdlen = req->dlen;\n\n\tscratch = raw_cpu_ptr(&scomp_scratch);\n\tspin_lock(&scratch->lock);\n\n\tscatterwalk_map_and_copy(scratch->src, req->src, 0, req->slen, 0);\n\tif (dir)\n\t\tret = crypto_scomp_compress(scomp, scratch->src, req->slen,\n\t\t\t\t\t    scratch->dst, &req->dlen, *ctx);\n\telse\n\t\tret = crypto_scomp_decompress(scomp, scratch->src, req->slen,\n\t\t\t\t\t      scratch->dst, &req->dlen, *ctx);\n\tif (!ret) {\n\t\tif (!req->dst) {\n\t\t\treq->dst = sgl_alloc(req->dlen, GFP_ATOMIC, NULL);\n\t\t\tif (!req->dst) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t} else if (req->dlen > dlen) {\n\t\t\tret = -ENOSPC;\n\t\t\tgoto out;\n\t\t}\n\t\tscatterwalk_map_and_copy(scratch->dst, req->dst, 0, req->dlen,\n\t\t\t\t\t 1);\n\t}\nout:\n\tspin_unlock(&scratch->lock);\n\treturn ret;\n}\n\nstatic int scomp_acomp_compress(struct acomp_req *req)\n{\n\treturn scomp_acomp_comp_decomp(req, 1);\n}\n\nstatic int scomp_acomp_decompress(struct acomp_req *req)\n{\n\treturn scomp_acomp_comp_decomp(req, 0);\n}\n\nstatic void crypto_exit_scomp_ops_async(struct crypto_tfm *tfm)\n{\n\tstruct crypto_scomp **ctx = crypto_tfm_ctx(tfm);\n\n\tcrypto_free_scomp(*ctx);\n\n\tmutex_lock(&scomp_lock);\n\tif (!--scomp_scratch_users)\n\t\tcrypto_scomp_free_scratches();\n\tmutex_unlock(&scomp_lock);\n}\n\nint crypto_init_scomp_ops_async(struct crypto_tfm *tfm)\n{\n\tstruct crypto_alg *calg = tfm->__crt_alg;\n\tstruct crypto_acomp *crt = __crypto_acomp_tfm(tfm);\n\tstruct crypto_scomp **ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_scomp *scomp;\n\n\tif (!crypto_mod_get(calg))\n\t\treturn -EAGAIN;\n\n\tscomp = crypto_create_tfm(calg, &crypto_scomp_type);\n\tif (IS_ERR(scomp)) {\n\t\tcrypto_mod_put(calg);\n\t\treturn PTR_ERR(scomp);\n\t}\n\n\t*ctx = scomp;\n\ttfm->exit = crypto_exit_scomp_ops_async;\n\n\tcrt->compress = scomp_acomp_compress;\n\tcrt->decompress = scomp_acomp_decompress;\n\tcrt->dst_free = sgl_free;\n\tcrt->reqsize = sizeof(void *);\n\n\treturn 0;\n}\n\nstruct acomp_req *crypto_acomp_scomp_alloc_ctx(struct acomp_req *req)\n{\n\tstruct crypto_acomp *acomp = crypto_acomp_reqtfm(req);\n\tstruct crypto_tfm *tfm = crypto_acomp_tfm(acomp);\n\tstruct crypto_scomp **tfm_ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_scomp *scomp = *tfm_ctx;\n\tvoid *ctx;\n\n\tctx = crypto_scomp_alloc_ctx(scomp);\n\tif (IS_ERR(ctx)) {\n\t\tkfree(req);\n\t\treturn NULL;\n\t}\n\n\t*req->__ctx = ctx;\n\n\treturn req;\n}\n\nvoid crypto_acomp_scomp_free_ctx(struct acomp_req *req)\n{\n\tstruct crypto_acomp *acomp = crypto_acomp_reqtfm(req);\n\tstruct crypto_tfm *tfm = crypto_acomp_tfm(acomp);\n\tstruct crypto_scomp **tfm_ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_scomp *scomp = *tfm_ctx;\n\tvoid *ctx = *req->__ctx;\n\n\tif (ctx)\n\t\tcrypto_scomp_free_ctx(scomp, ctx);\n}\n\nstatic const struct crypto_type crypto_scomp_type = {\n\t.extsize = crypto_alg_extsize,\n\t.init_tfm = crypto_scomp_init_tfm,\n#ifdef CONFIG_PROC_FS\n\t.show = crypto_scomp_show,\n#endif\n#if IS_ENABLED(CONFIG_CRYPTO_USER)\n\t.report = crypto_scomp_report,\n#endif\n#ifdef CONFIG_CRYPTO_STATS\n\t.report_stat = crypto_acomp_report_stat,\n#endif\n\t.maskclear = ~CRYPTO_ALG_TYPE_MASK,\n\t.maskset = CRYPTO_ALG_TYPE_MASK,\n\t.type = CRYPTO_ALG_TYPE_SCOMPRESS,\n\t.tfmsize = offsetof(struct crypto_scomp, base),\n};\n\nint crypto_register_scomp(struct scomp_alg *alg)\n{\n\tstruct crypto_alg *base = &alg->calg.base;\n\n\tcomp_prepare_alg(&alg->calg);\n\n\tbase->cra_type = &crypto_scomp_type;\n\tbase->cra_flags |= CRYPTO_ALG_TYPE_SCOMPRESS;\n\n\treturn crypto_register_alg(base);\n}\nEXPORT_SYMBOL_GPL(crypto_register_scomp);\n\nvoid crypto_unregister_scomp(struct scomp_alg *alg)\n{\n\tcrypto_unregister_alg(&alg->base);\n}\nEXPORT_SYMBOL_GPL(crypto_unregister_scomp);\n\nint crypto_register_scomps(struct scomp_alg *algs, int count)\n{\n\tint i, ret;\n\n\tfor (i = 0; i < count; i++) {\n\t\tret = crypto_register_scomp(&algs[i]);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\treturn 0;\n\nerr:\n\tfor (--i; i >= 0; --i)\n\t\tcrypto_unregister_scomp(&algs[i]);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(crypto_register_scomps);\n\nvoid crypto_unregister_scomps(struct scomp_alg *algs, int count)\n{\n\tint i;\n\n\tfor (i = count - 1; i >= 0; --i)\n\t\tcrypto_unregister_scomp(&algs[i]);\n}\nEXPORT_SYMBOL_GPL(crypto_unregister_scomps);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"Synchronous compression type\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}