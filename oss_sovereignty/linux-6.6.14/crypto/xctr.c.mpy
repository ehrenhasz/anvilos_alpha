{
  "module_name": "xctr.c",
  "hash_id": "0ac18e8aa90d01b75fcc3879ef27fb53285c6e3acab1df6090192f6a82d09ad6",
  "original_prompt": "Ingested from linux-6.6.14/crypto/xctr.c",
  "human_readable_source": "\n \n\n \n\n#include <crypto/algapi.h>\n#include <crypto/internal/cipher.h>\n#include <crypto/internal/skcipher.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/slab.h>\n\n \n#define XCTR_BLOCKSIZE 16\n\nstatic void crypto_xctr_crypt_final(struct skcipher_walk *walk,\n\t\t\t\t   struct crypto_cipher *tfm, u32 byte_ctr)\n{\n\tu8 keystream[XCTR_BLOCKSIZE];\n\tconst u8 *src = walk->src.virt.addr;\n\tu8 *dst = walk->dst.virt.addr;\n\tunsigned int nbytes = walk->nbytes;\n\t__le32 ctr32 = cpu_to_le32(byte_ctr / XCTR_BLOCKSIZE + 1);\n\n\tcrypto_xor(walk->iv, (u8 *)&ctr32, sizeof(ctr32));\n\tcrypto_cipher_encrypt_one(tfm, keystream, walk->iv);\n\tcrypto_xor_cpy(dst, keystream, src, nbytes);\n\tcrypto_xor(walk->iv, (u8 *)&ctr32, sizeof(ctr32));\n}\n\nstatic int crypto_xctr_crypt_segment(struct skcipher_walk *walk,\n\t\t\t\t    struct crypto_cipher *tfm, u32 byte_ctr)\n{\n\tvoid (*fn)(struct crypto_tfm *, u8 *, const u8 *) =\n\t\t   crypto_cipher_alg(tfm)->cia_encrypt;\n\tconst u8 *src = walk->src.virt.addr;\n\tu8 *dst = walk->dst.virt.addr;\n\tunsigned int nbytes = walk->nbytes;\n\t__le32 ctr32 = cpu_to_le32(byte_ctr / XCTR_BLOCKSIZE + 1);\n\n\tdo {\n\t\tcrypto_xor(walk->iv, (u8 *)&ctr32, sizeof(ctr32));\n\t\tfn(crypto_cipher_tfm(tfm), dst, walk->iv);\n\t\tcrypto_xor(dst, src, XCTR_BLOCKSIZE);\n\t\tcrypto_xor(walk->iv, (u8 *)&ctr32, sizeof(ctr32));\n\n\t\tle32_add_cpu(&ctr32, 1);\n\n\t\tsrc += XCTR_BLOCKSIZE;\n\t\tdst += XCTR_BLOCKSIZE;\n\t} while ((nbytes -= XCTR_BLOCKSIZE) >= XCTR_BLOCKSIZE);\n\n\treturn nbytes;\n}\n\nstatic int crypto_xctr_crypt_inplace(struct skcipher_walk *walk,\n\t\t\t\t    struct crypto_cipher *tfm, u32 byte_ctr)\n{\n\tvoid (*fn)(struct crypto_tfm *, u8 *, const u8 *) =\n\t\t   crypto_cipher_alg(tfm)->cia_encrypt;\n\tunsigned long alignmask = crypto_cipher_alignmask(tfm);\n\tunsigned int nbytes = walk->nbytes;\n\tu8 *data = walk->src.virt.addr;\n\tu8 tmp[XCTR_BLOCKSIZE + MAX_CIPHER_ALIGNMASK];\n\tu8 *keystream = PTR_ALIGN(tmp + 0, alignmask + 1);\n\t__le32 ctr32 = cpu_to_le32(byte_ctr / XCTR_BLOCKSIZE + 1);\n\n\tdo {\n\t\tcrypto_xor(walk->iv, (u8 *)&ctr32, sizeof(ctr32));\n\t\tfn(crypto_cipher_tfm(tfm), keystream, walk->iv);\n\t\tcrypto_xor(data, keystream, XCTR_BLOCKSIZE);\n\t\tcrypto_xor(walk->iv, (u8 *)&ctr32, sizeof(ctr32));\n\n\t\tle32_add_cpu(&ctr32, 1);\n\n\t\tdata += XCTR_BLOCKSIZE;\n\t} while ((nbytes -= XCTR_BLOCKSIZE) >= XCTR_BLOCKSIZE);\n\n\treturn nbytes;\n}\n\nstatic int crypto_xctr_crypt(struct skcipher_request *req)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tstruct crypto_cipher *cipher = skcipher_cipher_simple(tfm);\n\tstruct skcipher_walk walk;\n\tunsigned int nbytes;\n\tint err;\n\tu32 byte_ctr = 0;\n\n\terr = skcipher_walk_virt(&walk, req, false);\n\n\twhile (walk.nbytes >= XCTR_BLOCKSIZE) {\n\t\tif (walk.src.virt.addr == walk.dst.virt.addr)\n\t\t\tnbytes = crypto_xctr_crypt_inplace(&walk, cipher,\n\t\t\t\t\t\t\t   byte_ctr);\n\t\telse\n\t\t\tnbytes = crypto_xctr_crypt_segment(&walk, cipher,\n\t\t\t\t\t\t\t   byte_ctr);\n\n\t\tbyte_ctr += walk.nbytes - nbytes;\n\t\terr = skcipher_walk_done(&walk, nbytes);\n\t}\n\n\tif (walk.nbytes) {\n\t\tcrypto_xctr_crypt_final(&walk, cipher, byte_ctr);\n\t\terr = skcipher_walk_done(&walk, 0);\n\t}\n\n\treturn err;\n}\n\nstatic int crypto_xctr_create(struct crypto_template *tmpl, struct rtattr **tb)\n{\n\tstruct skcipher_instance *inst;\n\tstruct crypto_alg *alg;\n\tint err;\n\n\tinst = skcipher_alloc_instance_simple(tmpl, tb);\n\tif (IS_ERR(inst))\n\t\treturn PTR_ERR(inst);\n\n\talg = skcipher_ialg_simple(inst);\n\n\t \n\terr = -EINVAL;\n\tif (alg->cra_blocksize != XCTR_BLOCKSIZE)\n\t\tgoto out_free_inst;\n\n\t \n\tinst->alg.base.cra_blocksize = 1;\n\n\t \n\tinst->alg.chunksize = alg->cra_blocksize;\n\n\tinst->alg.encrypt = crypto_xctr_crypt;\n\tinst->alg.decrypt = crypto_xctr_crypt;\n\n\terr = skcipher_register_instance(tmpl, inst);\n\tif (err) {\nout_free_inst:\n\t\tinst->free(inst);\n\t}\n\n\treturn err;\n}\n\nstatic struct crypto_template crypto_xctr_tmpl = {\n\t.name = \"xctr\",\n\t.create = crypto_xctr_create,\n\t.module = THIS_MODULE,\n};\n\nstatic int __init crypto_xctr_module_init(void)\n{\n\treturn crypto_register_template(&crypto_xctr_tmpl);\n}\n\nstatic void __exit crypto_xctr_module_exit(void)\n{\n\tcrypto_unregister_template(&crypto_xctr_tmpl);\n}\n\nsubsys_initcall(crypto_xctr_module_init);\nmodule_exit(crypto_xctr_module_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"XCTR block cipher mode of operation\");\nMODULE_ALIAS_CRYPTO(\"xctr\");\nMODULE_IMPORT_NS(CRYPTO_INTERNAL);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}