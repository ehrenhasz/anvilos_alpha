{
  "module_name": "cryptd.c",
  "hash_id": "c776b45064abb472058122e0e5a596b8af5526b250911879d4cf235eada35ab6",
  "original_prompt": "Ingested from linux-6.6.14/crypto/cryptd.c",
  "human_readable_source": "\n \n\n#include <crypto/internal/hash.h>\n#include <crypto/internal/aead.h>\n#include <crypto/internal/skcipher.h>\n#include <crypto/cryptd.h>\n#include <linux/refcount.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/list.h>\n#include <linux/module.h>\n#include <linux/scatterlist.h>\n#include <linux/sched.h>\n#include <linux/slab.h>\n#include <linux/workqueue.h>\n\nstatic unsigned int cryptd_max_cpu_qlen = 1000;\nmodule_param(cryptd_max_cpu_qlen, uint, 0);\nMODULE_PARM_DESC(cryptd_max_cpu_qlen, \"Set cryptd Max queue depth\");\n\nstatic struct workqueue_struct *cryptd_wq;\n\nstruct cryptd_cpu_queue {\n\tstruct crypto_queue queue;\n\tstruct work_struct work;\n};\n\nstruct cryptd_queue {\n\t \n\tstruct cryptd_cpu_queue __percpu *cpu_queue;\n};\n\nstruct cryptd_instance_ctx {\n\tstruct crypto_spawn spawn;\n\tstruct cryptd_queue *queue;\n};\n\nstruct skcipherd_instance_ctx {\n\tstruct crypto_skcipher_spawn spawn;\n\tstruct cryptd_queue *queue;\n};\n\nstruct hashd_instance_ctx {\n\tstruct crypto_shash_spawn spawn;\n\tstruct cryptd_queue *queue;\n};\n\nstruct aead_instance_ctx {\n\tstruct crypto_aead_spawn aead_spawn;\n\tstruct cryptd_queue *queue;\n};\n\nstruct cryptd_skcipher_ctx {\n\trefcount_t refcnt;\n\tstruct crypto_skcipher *child;\n};\n\nstruct cryptd_skcipher_request_ctx {\n\tstruct skcipher_request req;\n};\n\nstruct cryptd_hash_ctx {\n\trefcount_t refcnt;\n\tstruct crypto_shash *child;\n};\n\nstruct cryptd_hash_request_ctx {\n\tcrypto_completion_t complete;\n\tvoid *data;\n\tstruct shash_desc desc;\n};\n\nstruct cryptd_aead_ctx {\n\trefcount_t refcnt;\n\tstruct crypto_aead *child;\n};\n\nstruct cryptd_aead_request_ctx {\n\tstruct aead_request req;\n};\n\nstatic void cryptd_queue_worker(struct work_struct *work);\n\nstatic int cryptd_init_queue(struct cryptd_queue *queue,\n\t\t\t     unsigned int max_cpu_qlen)\n{\n\tint cpu;\n\tstruct cryptd_cpu_queue *cpu_queue;\n\n\tqueue->cpu_queue = alloc_percpu(struct cryptd_cpu_queue);\n\tif (!queue->cpu_queue)\n\t\treturn -ENOMEM;\n\tfor_each_possible_cpu(cpu) {\n\t\tcpu_queue = per_cpu_ptr(queue->cpu_queue, cpu);\n\t\tcrypto_init_queue(&cpu_queue->queue, max_cpu_qlen);\n\t\tINIT_WORK(&cpu_queue->work, cryptd_queue_worker);\n\t}\n\tpr_info(\"cryptd: max_cpu_qlen set to %d\\n\", max_cpu_qlen);\n\treturn 0;\n}\n\nstatic void cryptd_fini_queue(struct cryptd_queue *queue)\n{\n\tint cpu;\n\tstruct cryptd_cpu_queue *cpu_queue;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tcpu_queue = per_cpu_ptr(queue->cpu_queue, cpu);\n\t\tBUG_ON(cpu_queue->queue.qlen);\n\t}\n\tfree_percpu(queue->cpu_queue);\n}\n\nstatic int cryptd_enqueue_request(struct cryptd_queue *queue,\n\t\t\t\t  struct crypto_async_request *request)\n{\n\tint err;\n\tstruct cryptd_cpu_queue *cpu_queue;\n\trefcount_t *refcnt;\n\n\tlocal_bh_disable();\n\tcpu_queue = this_cpu_ptr(queue->cpu_queue);\n\terr = crypto_enqueue_request(&cpu_queue->queue, request);\n\n\trefcnt = crypto_tfm_ctx(request->tfm);\n\n\tif (err == -ENOSPC)\n\t\tgoto out;\n\n\tqueue_work_on(smp_processor_id(), cryptd_wq, &cpu_queue->work);\n\n\tif (!refcount_read(refcnt))\n\t\tgoto out;\n\n\trefcount_inc(refcnt);\n\nout:\n\tlocal_bh_enable();\n\n\treturn err;\n}\n\n \nstatic void cryptd_queue_worker(struct work_struct *work)\n{\n\tstruct cryptd_cpu_queue *cpu_queue;\n\tstruct crypto_async_request *req, *backlog;\n\n\tcpu_queue = container_of(work, struct cryptd_cpu_queue, work);\n\t \n\tlocal_bh_disable();\n\tbacklog = crypto_get_backlog(&cpu_queue->queue);\n\treq = crypto_dequeue_request(&cpu_queue->queue);\n\tlocal_bh_enable();\n\n\tif (!req)\n\t\treturn;\n\n\tif (backlog)\n\t\tcrypto_request_complete(backlog, -EINPROGRESS);\n\tcrypto_request_complete(req, 0);\n\n\tif (cpu_queue->queue.qlen)\n\t\tqueue_work(cryptd_wq, &cpu_queue->work);\n}\n\nstatic inline struct cryptd_queue *cryptd_get_queue(struct crypto_tfm *tfm)\n{\n\tstruct crypto_instance *inst = crypto_tfm_alg_instance(tfm);\n\tstruct cryptd_instance_ctx *ictx = crypto_instance_ctx(inst);\n\treturn ictx->queue;\n}\n\nstatic void cryptd_type_and_mask(struct crypto_attr_type *algt,\n\t\t\t\t u32 *type, u32 *mask)\n{\n\t \n\t*type = algt->type & CRYPTO_ALG_INTERNAL;\n\t*mask = algt->mask & CRYPTO_ALG_INTERNAL;\n\n\t \n\t*mask |= CRYPTO_ALG_ASYNC;\n\n\t*mask |= crypto_algt_inherited_mask(algt);\n}\n\nstatic int cryptd_init_instance(struct crypto_instance *inst,\n\t\t\t\tstruct crypto_alg *alg)\n{\n\tif (snprintf(inst->alg.cra_driver_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"cryptd(%s)\",\n\t\t     alg->cra_driver_name) >= CRYPTO_MAX_ALG_NAME)\n\t\treturn -ENAMETOOLONG;\n\n\tmemcpy(inst->alg.cra_name, alg->cra_name, CRYPTO_MAX_ALG_NAME);\n\n\tinst->alg.cra_priority = alg->cra_priority + 50;\n\tinst->alg.cra_blocksize = alg->cra_blocksize;\n\tinst->alg.cra_alignmask = alg->cra_alignmask;\n\n\treturn 0;\n}\n\nstatic int cryptd_skcipher_setkey(struct crypto_skcipher *parent,\n\t\t\t\t  const u8 *key, unsigned int keylen)\n{\n\tstruct cryptd_skcipher_ctx *ctx = crypto_skcipher_ctx(parent);\n\tstruct crypto_skcipher *child = ctx->child;\n\n\tcrypto_skcipher_clear_flags(child, CRYPTO_TFM_REQ_MASK);\n\tcrypto_skcipher_set_flags(child,\n\t\t\t\t  crypto_skcipher_get_flags(parent) &\n\t\t\t\t  CRYPTO_TFM_REQ_MASK);\n\treturn crypto_skcipher_setkey(child, key, keylen);\n}\n\nstatic struct skcipher_request *cryptd_skcipher_prepare(\n\tstruct skcipher_request *req, int err)\n{\n\tstruct cryptd_skcipher_request_ctx *rctx = skcipher_request_ctx(req);\n\tstruct skcipher_request *subreq = &rctx->req;\n\tstruct cryptd_skcipher_ctx *ctx;\n\tstruct crypto_skcipher *child;\n\n\treq->base.complete = subreq->base.complete;\n\treq->base.data = subreq->base.data;\n\n\tif (unlikely(err == -EINPROGRESS))\n\t\treturn NULL;\n\n\tctx = crypto_skcipher_ctx(crypto_skcipher_reqtfm(req));\n\tchild = ctx->child;\n\n\tskcipher_request_set_tfm(subreq, child);\n\tskcipher_request_set_callback(subreq, CRYPTO_TFM_REQ_MAY_SLEEP,\n\t\t\t\t      NULL, NULL);\n\tskcipher_request_set_crypt(subreq, req->src, req->dst, req->cryptlen,\n\t\t\t\t   req->iv);\n\n\treturn subreq;\n}\n\nstatic void cryptd_skcipher_complete(struct skcipher_request *req, int err,\n\t\t\t\t     crypto_completion_t complete)\n{\n\tstruct cryptd_skcipher_request_ctx *rctx = skcipher_request_ctx(req);\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tstruct cryptd_skcipher_ctx *ctx = crypto_skcipher_ctx(tfm);\n\tstruct skcipher_request *subreq = &rctx->req;\n\tint refcnt = refcount_read(&ctx->refcnt);\n\n\tlocal_bh_disable();\n\tskcipher_request_complete(req, err);\n\tlocal_bh_enable();\n\n\tif (unlikely(err == -EINPROGRESS)) {\n\t\tsubreq->base.complete = req->base.complete;\n\t\tsubreq->base.data = req->base.data;\n\t\treq->base.complete = complete;\n\t\treq->base.data = req;\n\t} else if (refcnt && refcount_dec_and_test(&ctx->refcnt))\n\t\tcrypto_free_skcipher(tfm);\n}\n\nstatic void cryptd_skcipher_encrypt(void *data, int err)\n{\n\tstruct skcipher_request *req = data;\n\tstruct skcipher_request *subreq;\n\n\tsubreq = cryptd_skcipher_prepare(req, err);\n\tif (likely(subreq))\n\t\terr = crypto_skcipher_encrypt(subreq);\n\n\tcryptd_skcipher_complete(req, err, cryptd_skcipher_encrypt);\n}\n\nstatic void cryptd_skcipher_decrypt(void *data, int err)\n{\n\tstruct skcipher_request *req = data;\n\tstruct skcipher_request *subreq;\n\n\tsubreq = cryptd_skcipher_prepare(req, err);\n\tif (likely(subreq))\n\t\terr = crypto_skcipher_decrypt(subreq);\n\n\tcryptd_skcipher_complete(req, err, cryptd_skcipher_decrypt);\n}\n\nstatic int cryptd_skcipher_enqueue(struct skcipher_request *req,\n\t\t\t\t   crypto_completion_t compl)\n{\n\tstruct cryptd_skcipher_request_ctx *rctx = skcipher_request_ctx(req);\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tstruct skcipher_request *subreq = &rctx->req;\n\tstruct cryptd_queue *queue;\n\n\tqueue = cryptd_get_queue(crypto_skcipher_tfm(tfm));\n\tsubreq->base.complete = req->base.complete;\n\tsubreq->base.data = req->base.data;\n\treq->base.complete = compl;\n\treq->base.data = req;\n\n\treturn cryptd_enqueue_request(queue, &req->base);\n}\n\nstatic int cryptd_skcipher_encrypt_enqueue(struct skcipher_request *req)\n{\n\treturn cryptd_skcipher_enqueue(req, cryptd_skcipher_encrypt);\n}\n\nstatic int cryptd_skcipher_decrypt_enqueue(struct skcipher_request *req)\n{\n\treturn cryptd_skcipher_enqueue(req, cryptd_skcipher_decrypt);\n}\n\nstatic int cryptd_skcipher_init_tfm(struct crypto_skcipher *tfm)\n{\n\tstruct skcipher_instance *inst = skcipher_alg_instance(tfm);\n\tstruct skcipherd_instance_ctx *ictx = skcipher_instance_ctx(inst);\n\tstruct crypto_skcipher_spawn *spawn = &ictx->spawn;\n\tstruct cryptd_skcipher_ctx *ctx = crypto_skcipher_ctx(tfm);\n\tstruct crypto_skcipher *cipher;\n\n\tcipher = crypto_spawn_skcipher(spawn);\n\tif (IS_ERR(cipher))\n\t\treturn PTR_ERR(cipher);\n\n\tctx->child = cipher;\n\tcrypto_skcipher_set_reqsize(\n\t\ttfm, sizeof(struct cryptd_skcipher_request_ctx) +\n\t\t     crypto_skcipher_reqsize(cipher));\n\treturn 0;\n}\n\nstatic void cryptd_skcipher_exit_tfm(struct crypto_skcipher *tfm)\n{\n\tstruct cryptd_skcipher_ctx *ctx = crypto_skcipher_ctx(tfm);\n\n\tcrypto_free_skcipher(ctx->child);\n}\n\nstatic void cryptd_skcipher_free(struct skcipher_instance *inst)\n{\n\tstruct skcipherd_instance_ctx *ctx = skcipher_instance_ctx(inst);\n\n\tcrypto_drop_skcipher(&ctx->spawn);\n\tkfree(inst);\n}\n\nstatic int cryptd_create_skcipher(struct crypto_template *tmpl,\n\t\t\t\t  struct rtattr **tb,\n\t\t\t\t  struct crypto_attr_type *algt,\n\t\t\t\t  struct cryptd_queue *queue)\n{\n\tstruct skcipherd_instance_ctx *ctx;\n\tstruct skcipher_instance *inst;\n\tstruct skcipher_alg *alg;\n\tu32 type;\n\tu32 mask;\n\tint err;\n\n\tcryptd_type_and_mask(algt, &type, &mask);\n\n\tinst = kzalloc(sizeof(*inst) + sizeof(*ctx), GFP_KERNEL);\n\tif (!inst)\n\t\treturn -ENOMEM;\n\n\tctx = skcipher_instance_ctx(inst);\n\tctx->queue = queue;\n\n\terr = crypto_grab_skcipher(&ctx->spawn, skcipher_crypto_instance(inst),\n\t\t\t\t   crypto_attr_alg_name(tb[1]), type, mask);\n\tif (err)\n\t\tgoto err_free_inst;\n\n\talg = crypto_spawn_skcipher_alg(&ctx->spawn);\n\terr = cryptd_init_instance(skcipher_crypto_instance(inst), &alg->base);\n\tif (err)\n\t\tgoto err_free_inst;\n\n\tinst->alg.base.cra_flags |= CRYPTO_ALG_ASYNC |\n\t\t(alg->base.cra_flags & CRYPTO_ALG_INTERNAL);\n\tinst->alg.ivsize = crypto_skcipher_alg_ivsize(alg);\n\tinst->alg.chunksize = crypto_skcipher_alg_chunksize(alg);\n\tinst->alg.min_keysize = crypto_skcipher_alg_min_keysize(alg);\n\tinst->alg.max_keysize = crypto_skcipher_alg_max_keysize(alg);\n\n\tinst->alg.base.cra_ctxsize = sizeof(struct cryptd_skcipher_ctx);\n\n\tinst->alg.init = cryptd_skcipher_init_tfm;\n\tinst->alg.exit = cryptd_skcipher_exit_tfm;\n\n\tinst->alg.setkey = cryptd_skcipher_setkey;\n\tinst->alg.encrypt = cryptd_skcipher_encrypt_enqueue;\n\tinst->alg.decrypt = cryptd_skcipher_decrypt_enqueue;\n\n\tinst->free = cryptd_skcipher_free;\n\n\terr = skcipher_register_instance(tmpl, inst);\n\tif (err) {\nerr_free_inst:\n\t\tcryptd_skcipher_free(inst);\n\t}\n\treturn err;\n}\n\nstatic int cryptd_hash_init_tfm(struct crypto_ahash *tfm)\n{\n\tstruct ahash_instance *inst = ahash_alg_instance(tfm);\n\tstruct hashd_instance_ctx *ictx = ahash_instance_ctx(inst);\n\tstruct crypto_shash_spawn *spawn = &ictx->spawn;\n\tstruct cryptd_hash_ctx *ctx = crypto_ahash_ctx(tfm);\n\tstruct crypto_shash *hash;\n\n\thash = crypto_spawn_shash(spawn);\n\tif (IS_ERR(hash))\n\t\treturn PTR_ERR(hash);\n\n\tctx->child = hash;\n\tcrypto_ahash_set_reqsize(tfm,\n\t\t\t\t sizeof(struct cryptd_hash_request_ctx) +\n\t\t\t\t crypto_shash_descsize(hash));\n\treturn 0;\n}\n\nstatic int cryptd_hash_clone_tfm(struct crypto_ahash *ntfm,\n\t\t\t\t struct crypto_ahash *tfm)\n{\n\tstruct cryptd_hash_ctx *nctx = crypto_ahash_ctx(ntfm);\n\tstruct cryptd_hash_ctx *ctx = crypto_ahash_ctx(tfm);\n\tstruct crypto_shash *hash;\n\n\thash = crypto_clone_shash(ctx->child);\n\tif (IS_ERR(hash))\n\t\treturn PTR_ERR(hash);\n\n\tnctx->child = hash;\n\treturn 0;\n}\n\nstatic void cryptd_hash_exit_tfm(struct crypto_ahash *tfm)\n{\n\tstruct cryptd_hash_ctx *ctx = crypto_ahash_ctx(tfm);\n\n\tcrypto_free_shash(ctx->child);\n}\n\nstatic int cryptd_hash_setkey(struct crypto_ahash *parent,\n\t\t\t\t   const u8 *key, unsigned int keylen)\n{\n\tstruct cryptd_hash_ctx *ctx   = crypto_ahash_ctx(parent);\n\tstruct crypto_shash *child = ctx->child;\n\n\tcrypto_shash_clear_flags(child, CRYPTO_TFM_REQ_MASK);\n\tcrypto_shash_set_flags(child, crypto_ahash_get_flags(parent) &\n\t\t\t\t      CRYPTO_TFM_REQ_MASK);\n\treturn crypto_shash_setkey(child, key, keylen);\n}\n\nstatic int cryptd_hash_enqueue(struct ahash_request *req,\n\t\t\t\tcrypto_completion_t compl)\n{\n\tstruct cryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct cryptd_queue *queue =\n\t\tcryptd_get_queue(crypto_ahash_tfm(tfm));\n\n\trctx->complete = req->base.complete;\n\trctx->data = req->base.data;\n\treq->base.complete = compl;\n\treq->base.data = req;\n\n\treturn cryptd_enqueue_request(queue, &req->base);\n}\n\nstatic struct shash_desc *cryptd_hash_prepare(struct ahash_request *req,\n\t\t\t\t\t      int err)\n{\n\tstruct cryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\n\treq->base.complete = rctx->complete;\n\treq->base.data = rctx->data;\n\n\tif (unlikely(err == -EINPROGRESS))\n\t\treturn NULL;\n\n\treturn &rctx->desc;\n}\n\nstatic void cryptd_hash_complete(struct ahash_request *req, int err,\n\t\t\t\t crypto_completion_t complete)\n{\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct cryptd_hash_ctx *ctx = crypto_ahash_ctx(tfm);\n\tint refcnt = refcount_read(&ctx->refcnt);\n\n\tlocal_bh_disable();\n\tahash_request_complete(req, err);\n\tlocal_bh_enable();\n\n\tif (err == -EINPROGRESS) {\n\t\treq->base.complete = complete;\n\t\treq->base.data = req;\n\t} else if (refcnt && refcount_dec_and_test(&ctx->refcnt))\n\t\tcrypto_free_ahash(tfm);\n}\n\nstatic void cryptd_hash_init(void *data, int err)\n{\n\tstruct ahash_request *req = data;\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct cryptd_hash_ctx *ctx = crypto_ahash_ctx(tfm);\n\tstruct crypto_shash *child = ctx->child;\n\tstruct shash_desc *desc;\n\n\tdesc = cryptd_hash_prepare(req, err);\n\tif (unlikely(!desc))\n\t\tgoto out;\n\n\tdesc->tfm = child;\n\n\terr = crypto_shash_init(desc);\n\nout:\n\tcryptd_hash_complete(req, err, cryptd_hash_init);\n}\n\nstatic int cryptd_hash_init_enqueue(struct ahash_request *req)\n{\n\treturn cryptd_hash_enqueue(req, cryptd_hash_init);\n}\n\nstatic void cryptd_hash_update(void *data, int err)\n{\n\tstruct ahash_request *req = data;\n\tstruct shash_desc *desc;\n\n\tdesc = cryptd_hash_prepare(req, err);\n\tif (likely(desc))\n\t\terr = shash_ahash_update(req, desc);\n\n\tcryptd_hash_complete(req, err, cryptd_hash_update);\n}\n\nstatic int cryptd_hash_update_enqueue(struct ahash_request *req)\n{\n\treturn cryptd_hash_enqueue(req, cryptd_hash_update);\n}\n\nstatic void cryptd_hash_final(void *data, int err)\n{\n\tstruct ahash_request *req = data;\n\tstruct shash_desc *desc;\n\n\tdesc = cryptd_hash_prepare(req, err);\n\tif (likely(desc))\n\t\terr = crypto_shash_final(desc, req->result);\n\n\tcryptd_hash_complete(req, err, cryptd_hash_final);\n}\n\nstatic int cryptd_hash_final_enqueue(struct ahash_request *req)\n{\n\treturn cryptd_hash_enqueue(req, cryptd_hash_final);\n}\n\nstatic void cryptd_hash_finup(void *data, int err)\n{\n\tstruct ahash_request *req = data;\n\tstruct shash_desc *desc;\n\n\tdesc = cryptd_hash_prepare(req, err);\n\tif (likely(desc))\n\t\terr = shash_ahash_finup(req, desc);\n\n\tcryptd_hash_complete(req, err, cryptd_hash_finup);\n}\n\nstatic int cryptd_hash_finup_enqueue(struct ahash_request *req)\n{\n\treturn cryptd_hash_enqueue(req, cryptd_hash_finup);\n}\n\nstatic void cryptd_hash_digest(void *data, int err)\n{\n\tstruct ahash_request *req = data;\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct cryptd_hash_ctx *ctx = crypto_ahash_ctx(tfm);\n\tstruct crypto_shash *child = ctx->child;\n\tstruct shash_desc *desc;\n\n\tdesc = cryptd_hash_prepare(req, err);\n\tif (unlikely(!desc))\n\t\tgoto out;\n\n\tdesc->tfm = child;\n\n\terr = shash_ahash_digest(req, desc);\n\nout:\n\tcryptd_hash_complete(req, err, cryptd_hash_digest);\n}\n\nstatic int cryptd_hash_digest_enqueue(struct ahash_request *req)\n{\n\treturn cryptd_hash_enqueue(req, cryptd_hash_digest);\n}\n\nstatic int cryptd_hash_export(struct ahash_request *req, void *out)\n{\n\tstruct cryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\n\treturn crypto_shash_export(&rctx->desc, out);\n}\n\nstatic int cryptd_hash_import(struct ahash_request *req, const void *in)\n{\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\n\tstruct cryptd_hash_ctx *ctx = crypto_ahash_ctx(tfm);\n\tstruct shash_desc *desc = cryptd_shash_desc(req);\n\n\tdesc->tfm = ctx->child;\n\n\treturn crypto_shash_import(desc, in);\n}\n\nstatic void cryptd_hash_free(struct ahash_instance *inst)\n{\n\tstruct hashd_instance_ctx *ctx = ahash_instance_ctx(inst);\n\n\tcrypto_drop_shash(&ctx->spawn);\n\tkfree(inst);\n}\n\nstatic int cryptd_create_hash(struct crypto_template *tmpl, struct rtattr **tb,\n\t\t\t      struct crypto_attr_type *algt,\n\t\t\t      struct cryptd_queue *queue)\n{\n\tstruct hashd_instance_ctx *ctx;\n\tstruct ahash_instance *inst;\n\tstruct shash_alg *alg;\n\tu32 type;\n\tu32 mask;\n\tint err;\n\n\tcryptd_type_and_mask(algt, &type, &mask);\n\n\tinst = kzalloc(sizeof(*inst) + sizeof(*ctx), GFP_KERNEL);\n\tif (!inst)\n\t\treturn -ENOMEM;\n\n\tctx = ahash_instance_ctx(inst);\n\tctx->queue = queue;\n\n\terr = crypto_grab_shash(&ctx->spawn, ahash_crypto_instance(inst),\n\t\t\t\tcrypto_attr_alg_name(tb[1]), type, mask);\n\tif (err)\n\t\tgoto err_free_inst;\n\talg = crypto_spawn_shash_alg(&ctx->spawn);\n\n\terr = cryptd_init_instance(ahash_crypto_instance(inst), &alg->base);\n\tif (err)\n\t\tgoto err_free_inst;\n\n\tinst->alg.halg.base.cra_flags |= CRYPTO_ALG_ASYNC |\n\t\t(alg->base.cra_flags & (CRYPTO_ALG_INTERNAL|\n\t\t\t\t\tCRYPTO_ALG_OPTIONAL_KEY));\n\tinst->alg.halg.digestsize = alg->digestsize;\n\tinst->alg.halg.statesize = alg->statesize;\n\tinst->alg.halg.base.cra_ctxsize = sizeof(struct cryptd_hash_ctx);\n\n\tinst->alg.init_tfm = cryptd_hash_init_tfm;\n\tinst->alg.clone_tfm = cryptd_hash_clone_tfm;\n\tinst->alg.exit_tfm = cryptd_hash_exit_tfm;\n\n\tinst->alg.init   = cryptd_hash_init_enqueue;\n\tinst->alg.update = cryptd_hash_update_enqueue;\n\tinst->alg.final  = cryptd_hash_final_enqueue;\n\tinst->alg.finup  = cryptd_hash_finup_enqueue;\n\tinst->alg.export = cryptd_hash_export;\n\tinst->alg.import = cryptd_hash_import;\n\tif (crypto_shash_alg_has_setkey(alg))\n\t\tinst->alg.setkey = cryptd_hash_setkey;\n\tinst->alg.digest = cryptd_hash_digest_enqueue;\n\n\tinst->free = cryptd_hash_free;\n\n\terr = ahash_register_instance(tmpl, inst);\n\tif (err) {\nerr_free_inst:\n\t\tcryptd_hash_free(inst);\n\t}\n\treturn err;\n}\n\nstatic int cryptd_aead_setkey(struct crypto_aead *parent,\n\t\t\t      const u8 *key, unsigned int keylen)\n{\n\tstruct cryptd_aead_ctx *ctx = crypto_aead_ctx(parent);\n\tstruct crypto_aead *child = ctx->child;\n\n\treturn crypto_aead_setkey(child, key, keylen);\n}\n\nstatic int cryptd_aead_setauthsize(struct crypto_aead *parent,\n\t\t\t\t   unsigned int authsize)\n{\n\tstruct cryptd_aead_ctx *ctx = crypto_aead_ctx(parent);\n\tstruct crypto_aead *child = ctx->child;\n\n\treturn crypto_aead_setauthsize(child, authsize);\n}\n\nstatic void cryptd_aead_crypt(struct aead_request *req,\n\t\t\t      struct crypto_aead *child, int err,\n\t\t\t      int (*crypt)(struct aead_request *req),\n\t\t\t      crypto_completion_t compl)\n{\n\tstruct cryptd_aead_request_ctx *rctx;\n\tstruct aead_request *subreq;\n\tstruct cryptd_aead_ctx *ctx;\n\tstruct crypto_aead *tfm;\n\tint refcnt;\n\n\trctx = aead_request_ctx(req);\n\tsubreq = &rctx->req;\n\treq->base.complete = subreq->base.complete;\n\treq->base.data = subreq->base.data;\n\n\ttfm = crypto_aead_reqtfm(req);\n\n\tif (unlikely(err == -EINPROGRESS))\n\t\tgoto out;\n\n\taead_request_set_tfm(subreq, child);\n\taead_request_set_callback(subreq, CRYPTO_TFM_REQ_MAY_SLEEP,\n\t\t\t\t  NULL, NULL);\n\taead_request_set_crypt(subreq, req->src, req->dst, req->cryptlen,\n\t\t\t       req->iv);\n\taead_request_set_ad(subreq, req->assoclen);\n\n\terr = crypt(subreq);\n\nout:\n\tctx = crypto_aead_ctx(tfm);\n\trefcnt = refcount_read(&ctx->refcnt);\n\n\tlocal_bh_disable();\n\taead_request_complete(req, err);\n\tlocal_bh_enable();\n\n\tif (err == -EINPROGRESS) {\n\t\tsubreq->base.complete = req->base.complete;\n\t\tsubreq->base.data = req->base.data;\n\t\treq->base.complete = compl;\n\t\treq->base.data = req;\n\t} else if (refcnt && refcount_dec_and_test(&ctx->refcnt))\n\t\tcrypto_free_aead(tfm);\n}\n\nstatic void cryptd_aead_encrypt(void *data, int err)\n{\n\tstruct aead_request *req = data;\n\tstruct cryptd_aead_ctx *ctx;\n\tstruct crypto_aead *child;\n\n\tctx = crypto_aead_ctx(crypto_aead_reqtfm(req));\n\tchild = ctx->child;\n\tcryptd_aead_crypt(req, child, err, crypto_aead_alg(child)->encrypt,\n\t\t\t  cryptd_aead_encrypt);\n}\n\nstatic void cryptd_aead_decrypt(void *data, int err)\n{\n\tstruct aead_request *req = data;\n\tstruct cryptd_aead_ctx *ctx;\n\tstruct crypto_aead *child;\n\n\tctx = crypto_aead_ctx(crypto_aead_reqtfm(req));\n\tchild = ctx->child;\n\tcryptd_aead_crypt(req, child, err, crypto_aead_alg(child)->decrypt,\n\t\t\t  cryptd_aead_decrypt);\n}\n\nstatic int cryptd_aead_enqueue(struct aead_request *req,\n\t\t\t\t    crypto_completion_t compl)\n{\n\tstruct cryptd_aead_request_ctx *rctx = aead_request_ctx(req);\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tstruct cryptd_queue *queue = cryptd_get_queue(crypto_aead_tfm(tfm));\n\tstruct aead_request *subreq = &rctx->req;\n\n\tsubreq->base.complete = req->base.complete;\n\tsubreq->base.data = req->base.data;\n\treq->base.complete = compl;\n\treq->base.data = req;\n\treturn cryptd_enqueue_request(queue, &req->base);\n}\n\nstatic int cryptd_aead_encrypt_enqueue(struct aead_request *req)\n{\n\treturn cryptd_aead_enqueue(req, cryptd_aead_encrypt );\n}\n\nstatic int cryptd_aead_decrypt_enqueue(struct aead_request *req)\n{\n\treturn cryptd_aead_enqueue(req, cryptd_aead_decrypt );\n}\n\nstatic int cryptd_aead_init_tfm(struct crypto_aead *tfm)\n{\n\tstruct aead_instance *inst = aead_alg_instance(tfm);\n\tstruct aead_instance_ctx *ictx = aead_instance_ctx(inst);\n\tstruct crypto_aead_spawn *spawn = &ictx->aead_spawn;\n\tstruct cryptd_aead_ctx *ctx = crypto_aead_ctx(tfm);\n\tstruct crypto_aead *cipher;\n\n\tcipher = crypto_spawn_aead(spawn);\n\tif (IS_ERR(cipher))\n\t\treturn PTR_ERR(cipher);\n\n\tctx->child = cipher;\n\tcrypto_aead_set_reqsize(\n\t\ttfm, sizeof(struct cryptd_aead_request_ctx) +\n\t\t     crypto_aead_reqsize(cipher));\n\treturn 0;\n}\n\nstatic void cryptd_aead_exit_tfm(struct crypto_aead *tfm)\n{\n\tstruct cryptd_aead_ctx *ctx = crypto_aead_ctx(tfm);\n\tcrypto_free_aead(ctx->child);\n}\n\nstatic void cryptd_aead_free(struct aead_instance *inst)\n{\n\tstruct aead_instance_ctx *ctx = aead_instance_ctx(inst);\n\n\tcrypto_drop_aead(&ctx->aead_spawn);\n\tkfree(inst);\n}\n\nstatic int cryptd_create_aead(struct crypto_template *tmpl,\n\t\t              struct rtattr **tb,\n\t\t\t      struct crypto_attr_type *algt,\n\t\t\t      struct cryptd_queue *queue)\n{\n\tstruct aead_instance_ctx *ctx;\n\tstruct aead_instance *inst;\n\tstruct aead_alg *alg;\n\tu32 type;\n\tu32 mask;\n\tint err;\n\n\tcryptd_type_and_mask(algt, &type, &mask);\n\n\tinst = kzalloc(sizeof(*inst) + sizeof(*ctx), GFP_KERNEL);\n\tif (!inst)\n\t\treturn -ENOMEM;\n\n\tctx = aead_instance_ctx(inst);\n\tctx->queue = queue;\n\n\terr = crypto_grab_aead(&ctx->aead_spawn, aead_crypto_instance(inst),\n\t\t\t       crypto_attr_alg_name(tb[1]), type, mask);\n\tif (err)\n\t\tgoto err_free_inst;\n\n\talg = crypto_spawn_aead_alg(&ctx->aead_spawn);\n\terr = cryptd_init_instance(aead_crypto_instance(inst), &alg->base);\n\tif (err)\n\t\tgoto err_free_inst;\n\n\tinst->alg.base.cra_flags |= CRYPTO_ALG_ASYNC |\n\t\t(alg->base.cra_flags & CRYPTO_ALG_INTERNAL);\n\tinst->alg.base.cra_ctxsize = sizeof(struct cryptd_aead_ctx);\n\n\tinst->alg.ivsize = crypto_aead_alg_ivsize(alg);\n\tinst->alg.maxauthsize = crypto_aead_alg_maxauthsize(alg);\n\n\tinst->alg.init = cryptd_aead_init_tfm;\n\tinst->alg.exit = cryptd_aead_exit_tfm;\n\tinst->alg.setkey = cryptd_aead_setkey;\n\tinst->alg.setauthsize = cryptd_aead_setauthsize;\n\tinst->alg.encrypt = cryptd_aead_encrypt_enqueue;\n\tinst->alg.decrypt = cryptd_aead_decrypt_enqueue;\n\n\tinst->free = cryptd_aead_free;\n\n\terr = aead_register_instance(tmpl, inst);\n\tif (err) {\nerr_free_inst:\n\t\tcryptd_aead_free(inst);\n\t}\n\treturn err;\n}\n\nstatic struct cryptd_queue queue;\n\nstatic int cryptd_create(struct crypto_template *tmpl, struct rtattr **tb)\n{\n\tstruct crypto_attr_type *algt;\n\n\talgt = crypto_get_attr_type(tb);\n\tif (IS_ERR(algt))\n\t\treturn PTR_ERR(algt);\n\n\tswitch (algt->type & algt->mask & CRYPTO_ALG_TYPE_MASK) {\n\tcase CRYPTO_ALG_TYPE_SKCIPHER:\n\t\treturn cryptd_create_skcipher(tmpl, tb, algt, &queue);\n\tcase CRYPTO_ALG_TYPE_HASH:\n\t\treturn cryptd_create_hash(tmpl, tb, algt, &queue);\n\tcase CRYPTO_ALG_TYPE_AEAD:\n\t\treturn cryptd_create_aead(tmpl, tb, algt, &queue);\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic struct crypto_template cryptd_tmpl = {\n\t.name = \"cryptd\",\n\t.create = cryptd_create,\n\t.module = THIS_MODULE,\n};\n\nstruct cryptd_skcipher *cryptd_alloc_skcipher(const char *alg_name,\n\t\t\t\t\t      u32 type, u32 mask)\n{\n\tchar cryptd_alg_name[CRYPTO_MAX_ALG_NAME];\n\tstruct cryptd_skcipher_ctx *ctx;\n\tstruct crypto_skcipher *tfm;\n\n\tif (snprintf(cryptd_alg_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"cryptd(%s)\", alg_name) >= CRYPTO_MAX_ALG_NAME)\n\t\treturn ERR_PTR(-EINVAL);\n\n\ttfm = crypto_alloc_skcipher(cryptd_alg_name, type, mask);\n\tif (IS_ERR(tfm))\n\t\treturn ERR_CAST(tfm);\n\n\tif (tfm->base.__crt_alg->cra_module != THIS_MODULE) {\n\t\tcrypto_free_skcipher(tfm);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tctx = crypto_skcipher_ctx(tfm);\n\trefcount_set(&ctx->refcnt, 1);\n\n\treturn container_of(tfm, struct cryptd_skcipher, base);\n}\nEXPORT_SYMBOL_GPL(cryptd_alloc_skcipher);\n\nstruct crypto_skcipher *cryptd_skcipher_child(struct cryptd_skcipher *tfm)\n{\n\tstruct cryptd_skcipher_ctx *ctx = crypto_skcipher_ctx(&tfm->base);\n\n\treturn ctx->child;\n}\nEXPORT_SYMBOL_GPL(cryptd_skcipher_child);\n\nbool cryptd_skcipher_queued(struct cryptd_skcipher *tfm)\n{\n\tstruct cryptd_skcipher_ctx *ctx = crypto_skcipher_ctx(&tfm->base);\n\n\treturn refcount_read(&ctx->refcnt) - 1;\n}\nEXPORT_SYMBOL_GPL(cryptd_skcipher_queued);\n\nvoid cryptd_free_skcipher(struct cryptd_skcipher *tfm)\n{\n\tstruct cryptd_skcipher_ctx *ctx = crypto_skcipher_ctx(&tfm->base);\n\n\tif (refcount_dec_and_test(&ctx->refcnt))\n\t\tcrypto_free_skcipher(&tfm->base);\n}\nEXPORT_SYMBOL_GPL(cryptd_free_skcipher);\n\nstruct cryptd_ahash *cryptd_alloc_ahash(const char *alg_name,\n\t\t\t\t\tu32 type, u32 mask)\n{\n\tchar cryptd_alg_name[CRYPTO_MAX_ALG_NAME];\n\tstruct cryptd_hash_ctx *ctx;\n\tstruct crypto_ahash *tfm;\n\n\tif (snprintf(cryptd_alg_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"cryptd(%s)\", alg_name) >= CRYPTO_MAX_ALG_NAME)\n\t\treturn ERR_PTR(-EINVAL);\n\ttfm = crypto_alloc_ahash(cryptd_alg_name, type, mask);\n\tif (IS_ERR(tfm))\n\t\treturn ERR_CAST(tfm);\n\tif (tfm->base.__crt_alg->cra_module != THIS_MODULE) {\n\t\tcrypto_free_ahash(tfm);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tctx = crypto_ahash_ctx(tfm);\n\trefcount_set(&ctx->refcnt, 1);\n\n\treturn __cryptd_ahash_cast(tfm);\n}\nEXPORT_SYMBOL_GPL(cryptd_alloc_ahash);\n\nstruct crypto_shash *cryptd_ahash_child(struct cryptd_ahash *tfm)\n{\n\tstruct cryptd_hash_ctx *ctx = crypto_ahash_ctx(&tfm->base);\n\n\treturn ctx->child;\n}\nEXPORT_SYMBOL_GPL(cryptd_ahash_child);\n\nstruct shash_desc *cryptd_shash_desc(struct ahash_request *req)\n{\n\tstruct cryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\n\treturn &rctx->desc;\n}\nEXPORT_SYMBOL_GPL(cryptd_shash_desc);\n\nbool cryptd_ahash_queued(struct cryptd_ahash *tfm)\n{\n\tstruct cryptd_hash_ctx *ctx = crypto_ahash_ctx(&tfm->base);\n\n\treturn refcount_read(&ctx->refcnt) - 1;\n}\nEXPORT_SYMBOL_GPL(cryptd_ahash_queued);\n\nvoid cryptd_free_ahash(struct cryptd_ahash *tfm)\n{\n\tstruct cryptd_hash_ctx *ctx = crypto_ahash_ctx(&tfm->base);\n\n\tif (refcount_dec_and_test(&ctx->refcnt))\n\t\tcrypto_free_ahash(&tfm->base);\n}\nEXPORT_SYMBOL_GPL(cryptd_free_ahash);\n\nstruct cryptd_aead *cryptd_alloc_aead(const char *alg_name,\n\t\t\t\t\t\t  u32 type, u32 mask)\n{\n\tchar cryptd_alg_name[CRYPTO_MAX_ALG_NAME];\n\tstruct cryptd_aead_ctx *ctx;\n\tstruct crypto_aead *tfm;\n\n\tif (snprintf(cryptd_alg_name, CRYPTO_MAX_ALG_NAME,\n\t\t     \"cryptd(%s)\", alg_name) >= CRYPTO_MAX_ALG_NAME)\n\t\treturn ERR_PTR(-EINVAL);\n\ttfm = crypto_alloc_aead(cryptd_alg_name, type, mask);\n\tif (IS_ERR(tfm))\n\t\treturn ERR_CAST(tfm);\n\tif (tfm->base.__crt_alg->cra_module != THIS_MODULE) {\n\t\tcrypto_free_aead(tfm);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tctx = crypto_aead_ctx(tfm);\n\trefcount_set(&ctx->refcnt, 1);\n\n\treturn __cryptd_aead_cast(tfm);\n}\nEXPORT_SYMBOL_GPL(cryptd_alloc_aead);\n\nstruct crypto_aead *cryptd_aead_child(struct cryptd_aead *tfm)\n{\n\tstruct cryptd_aead_ctx *ctx;\n\tctx = crypto_aead_ctx(&tfm->base);\n\treturn ctx->child;\n}\nEXPORT_SYMBOL_GPL(cryptd_aead_child);\n\nbool cryptd_aead_queued(struct cryptd_aead *tfm)\n{\n\tstruct cryptd_aead_ctx *ctx = crypto_aead_ctx(&tfm->base);\n\n\treturn refcount_read(&ctx->refcnt) - 1;\n}\nEXPORT_SYMBOL_GPL(cryptd_aead_queued);\n\nvoid cryptd_free_aead(struct cryptd_aead *tfm)\n{\n\tstruct cryptd_aead_ctx *ctx = crypto_aead_ctx(&tfm->base);\n\n\tif (refcount_dec_and_test(&ctx->refcnt))\n\t\tcrypto_free_aead(&tfm->base);\n}\nEXPORT_SYMBOL_GPL(cryptd_free_aead);\n\nstatic int __init cryptd_init(void)\n{\n\tint err;\n\n\tcryptd_wq = alloc_workqueue(\"cryptd\", WQ_MEM_RECLAIM | WQ_CPU_INTENSIVE,\n\t\t\t\t    1);\n\tif (!cryptd_wq)\n\t\treturn -ENOMEM;\n\n\terr = cryptd_init_queue(&queue, cryptd_max_cpu_qlen);\n\tif (err)\n\t\tgoto err_destroy_wq;\n\n\terr = crypto_register_template(&cryptd_tmpl);\n\tif (err)\n\t\tgoto err_fini_queue;\n\n\treturn 0;\n\nerr_fini_queue:\n\tcryptd_fini_queue(&queue);\nerr_destroy_wq:\n\tdestroy_workqueue(cryptd_wq);\n\treturn err;\n}\n\nstatic void __exit cryptd_exit(void)\n{\n\tdestroy_workqueue(cryptd_wq);\n\tcryptd_fini_queue(&queue);\n\tcrypto_unregister_template(&cryptd_tmpl);\n}\n\nsubsys_initcall(cryptd_init);\nmodule_exit(cryptd_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"Software async crypto daemon\");\nMODULE_ALIAS_CRYPTO(\"cryptd\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}