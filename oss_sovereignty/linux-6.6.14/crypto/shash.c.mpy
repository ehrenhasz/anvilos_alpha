{
  "module_name": "shash.c",
  "hash_id": "271a8b146c8535378094436e67a2321b8b25b552bb02030a44113867ca118a5e",
  "original_prompt": "Ingested from linux-6.6.14/crypto/shash.c",
  "human_readable_source": "\n \n\n#include <crypto/scatterwalk.h>\n#include <linux/cryptouser.h>\n#include <linux/err.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/seq_file.h>\n#include <linux/string.h>\n#include <net/netlink.h>\n\n#include \"hash.h\"\n\n#define MAX_SHASH_ALIGNMASK 63\n\nstatic const struct crypto_type crypto_shash_type;\n\nstatic inline struct crypto_istat_hash *shash_get_stat(struct shash_alg *alg)\n{\n\treturn hash_get_stat(&alg->halg);\n}\n\nstatic inline int crypto_shash_errstat(struct shash_alg *alg, int err)\n{\n\treturn crypto_hash_errstat(&alg->halg, err);\n}\n\nint shash_no_setkey(struct crypto_shash *tfm, const u8 *key,\n\t\t    unsigned int keylen)\n{\n\treturn -ENOSYS;\n}\nEXPORT_SYMBOL_GPL(shash_no_setkey);\n\nstatic int shash_setkey_unaligned(struct crypto_shash *tfm, const u8 *key,\n\t\t\t\t  unsigned int keylen)\n{\n\tstruct shash_alg *shash = crypto_shash_alg(tfm);\n\tunsigned long alignmask = crypto_shash_alignmask(tfm);\n\tunsigned long absize;\n\tu8 *buffer, *alignbuffer;\n\tint err;\n\n\tabsize = keylen + (alignmask & ~(crypto_tfm_ctx_alignment() - 1));\n\tbuffer = kmalloc(absize, GFP_ATOMIC);\n\tif (!buffer)\n\t\treturn -ENOMEM;\n\n\talignbuffer = (u8 *)ALIGN((unsigned long)buffer, alignmask + 1);\n\tmemcpy(alignbuffer, key, keylen);\n\terr = shash->setkey(tfm, alignbuffer, keylen);\n\tkfree_sensitive(buffer);\n\treturn err;\n}\n\nstatic void shash_set_needkey(struct crypto_shash *tfm, struct shash_alg *alg)\n{\n\tif (crypto_shash_alg_needs_key(alg))\n\t\tcrypto_shash_set_flags(tfm, CRYPTO_TFM_NEED_KEY);\n}\n\nint crypto_shash_setkey(struct crypto_shash *tfm, const u8 *key,\n\t\t\tunsigned int keylen)\n{\n\tstruct shash_alg *shash = crypto_shash_alg(tfm);\n\tunsigned long alignmask = crypto_shash_alignmask(tfm);\n\tint err;\n\n\tif ((unsigned long)key & alignmask)\n\t\terr = shash_setkey_unaligned(tfm, key, keylen);\n\telse\n\t\terr = shash->setkey(tfm, key, keylen);\n\n\tif (unlikely(err)) {\n\t\tshash_set_needkey(tfm, shash);\n\t\treturn err;\n\t}\n\n\tcrypto_shash_clear_flags(tfm, CRYPTO_TFM_NEED_KEY);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(crypto_shash_setkey);\n\nstatic int shash_update_unaligned(struct shash_desc *desc, const u8 *data,\n\t\t\t\t  unsigned int len)\n{\n\tstruct crypto_shash *tfm = desc->tfm;\n\tstruct shash_alg *shash = crypto_shash_alg(tfm);\n\tunsigned long alignmask = crypto_shash_alignmask(tfm);\n\tunsigned int unaligned_len = alignmask + 1 -\n\t\t\t\t     ((unsigned long)data & alignmask);\n\t \n\tu8 ubuf[MAX_SHASH_ALIGNMASK * 2];\n\tu8 *buf = PTR_ALIGN(&ubuf[0], alignmask + 1);\n\tint err;\n\n\tif (WARN_ON(buf + unaligned_len > ubuf + sizeof(ubuf)))\n\t\treturn -EINVAL;\n\n\tif (unaligned_len > len)\n\t\tunaligned_len = len;\n\n\tmemcpy(buf, data, unaligned_len);\n\terr = shash->update(desc, buf, unaligned_len);\n\tmemset(buf, 0, unaligned_len);\n\n\treturn err ?:\n\t       shash->update(desc, data + unaligned_len, len - unaligned_len);\n}\n\nint crypto_shash_update(struct shash_desc *desc, const u8 *data,\n\t\t\tunsigned int len)\n{\n\tstruct crypto_shash *tfm = desc->tfm;\n\tstruct shash_alg *shash = crypto_shash_alg(tfm);\n\tunsigned long alignmask = crypto_shash_alignmask(tfm);\n\tint err;\n\n\tif (IS_ENABLED(CONFIG_CRYPTO_STATS))\n\t\tatomic64_add(len, &shash_get_stat(shash)->hash_tlen);\n\n\tif ((unsigned long)data & alignmask)\n\t\terr = shash_update_unaligned(desc, data, len);\n\telse\n\t\terr = shash->update(desc, data, len);\n\n\treturn crypto_shash_errstat(shash, err);\n}\nEXPORT_SYMBOL_GPL(crypto_shash_update);\n\nstatic int shash_final_unaligned(struct shash_desc *desc, u8 *out)\n{\n\tstruct crypto_shash *tfm = desc->tfm;\n\tunsigned long alignmask = crypto_shash_alignmask(tfm);\n\tstruct shash_alg *shash = crypto_shash_alg(tfm);\n\tunsigned int ds = crypto_shash_digestsize(tfm);\n\t \n\tu8 ubuf[MAX_SHASH_ALIGNMASK + HASH_MAX_DIGESTSIZE];\n\tu8 *buf = PTR_ALIGN(&ubuf[0], alignmask + 1);\n\tint err;\n\n\tif (WARN_ON(buf + ds > ubuf + sizeof(ubuf)))\n\t\treturn -EINVAL;\n\n\terr = shash->final(desc, buf);\n\tif (err)\n\t\tgoto out;\n\n\tmemcpy(out, buf, ds);\n\nout:\n\tmemset(buf, 0, ds);\n\treturn err;\n}\n\nint crypto_shash_final(struct shash_desc *desc, u8 *out)\n{\n\tstruct crypto_shash *tfm = desc->tfm;\n\tstruct shash_alg *shash = crypto_shash_alg(tfm);\n\tunsigned long alignmask = crypto_shash_alignmask(tfm);\n\tint err;\n\n\tif (IS_ENABLED(CONFIG_CRYPTO_STATS))\n\t\tatomic64_inc(&shash_get_stat(shash)->hash_cnt);\n\n\tif ((unsigned long)out & alignmask)\n\t\terr = shash_final_unaligned(desc, out);\n\telse\n\t\terr = shash->final(desc, out);\n\n\treturn crypto_shash_errstat(shash, err);\n}\nEXPORT_SYMBOL_GPL(crypto_shash_final);\n\nstatic int shash_finup_unaligned(struct shash_desc *desc, const u8 *data,\n\t\t\t\t unsigned int len, u8 *out)\n{\n\treturn shash_update_unaligned(desc, data, len) ?:\n\t       shash_final_unaligned(desc, out);\n}\n\nint crypto_shash_finup(struct shash_desc *desc, const u8 *data,\n\t\t       unsigned int len, u8 *out)\n{\n\tstruct crypto_shash *tfm = desc->tfm;\n\tstruct shash_alg *shash = crypto_shash_alg(tfm);\n\tunsigned long alignmask = crypto_shash_alignmask(tfm);\n\tint err;\n\n\tif (IS_ENABLED(CONFIG_CRYPTO_STATS)) {\n\t\tstruct crypto_istat_hash *istat = shash_get_stat(shash);\n\n\t\tatomic64_inc(&istat->hash_cnt);\n\t\tatomic64_add(len, &istat->hash_tlen);\n\t}\n\n\tif (((unsigned long)data | (unsigned long)out) & alignmask)\n\t\terr = shash_finup_unaligned(desc, data, len, out);\n\telse\n\t\terr = shash->finup(desc, data, len, out);\n\n\n\treturn crypto_shash_errstat(shash, err);\n}\nEXPORT_SYMBOL_GPL(crypto_shash_finup);\n\nstatic int shash_digest_unaligned(struct shash_desc *desc, const u8 *data,\n\t\t\t\t  unsigned int len, u8 *out)\n{\n\treturn crypto_shash_init(desc) ?:\n\t       shash_update_unaligned(desc, data, len) ?:\n\t       shash_final_unaligned(desc, out);\n}\n\nint crypto_shash_digest(struct shash_desc *desc, const u8 *data,\n\t\t\tunsigned int len, u8 *out)\n{\n\tstruct crypto_shash *tfm = desc->tfm;\n\tstruct shash_alg *shash = crypto_shash_alg(tfm);\n\tunsigned long alignmask = crypto_shash_alignmask(tfm);\n\tint err;\n\n\tif (IS_ENABLED(CONFIG_CRYPTO_STATS)) {\n\t\tstruct crypto_istat_hash *istat = shash_get_stat(shash);\n\n\t\tatomic64_inc(&istat->hash_cnt);\n\t\tatomic64_add(len, &istat->hash_tlen);\n\t}\n\n\tif (crypto_shash_get_flags(tfm) & CRYPTO_TFM_NEED_KEY)\n\t\terr = -ENOKEY;\n\telse if (((unsigned long)data | (unsigned long)out) & alignmask)\n\t\terr = shash_digest_unaligned(desc, data, len, out);\n\telse\n\t\terr = shash->digest(desc, data, len, out);\n\n\treturn crypto_shash_errstat(shash, err);\n}\nEXPORT_SYMBOL_GPL(crypto_shash_digest);\n\nint crypto_shash_tfm_digest(struct crypto_shash *tfm, const u8 *data,\n\t\t\t    unsigned int len, u8 *out)\n{\n\tSHASH_DESC_ON_STACK(desc, tfm);\n\tint err;\n\n\tdesc->tfm = tfm;\n\n\terr = crypto_shash_digest(desc, data, len, out);\n\n\tshash_desc_zero(desc);\n\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(crypto_shash_tfm_digest);\n\nstatic int shash_default_export(struct shash_desc *desc, void *out)\n{\n\tmemcpy(out, shash_desc_ctx(desc), crypto_shash_descsize(desc->tfm));\n\treturn 0;\n}\n\nstatic int shash_default_import(struct shash_desc *desc, const void *in)\n{\n\tmemcpy(shash_desc_ctx(desc), in, crypto_shash_descsize(desc->tfm));\n\treturn 0;\n}\n\nstatic int shash_async_setkey(struct crypto_ahash *tfm, const u8 *key,\n\t\t\t      unsigned int keylen)\n{\n\tstruct crypto_shash **ctx = crypto_ahash_ctx(tfm);\n\n\treturn crypto_shash_setkey(*ctx, key, keylen);\n}\n\nstatic int shash_async_init(struct ahash_request *req)\n{\n\tstruct crypto_shash **ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(req));\n\tstruct shash_desc *desc = ahash_request_ctx(req);\n\n\tdesc->tfm = *ctx;\n\n\treturn crypto_shash_init(desc);\n}\n\nint shash_ahash_update(struct ahash_request *req, struct shash_desc *desc)\n{\n\tstruct crypto_hash_walk walk;\n\tint nbytes;\n\n\tfor (nbytes = crypto_hash_walk_first(req, &walk); nbytes > 0;\n\t     nbytes = crypto_hash_walk_done(&walk, nbytes))\n\t\tnbytes = crypto_shash_update(desc, walk.data, nbytes);\n\n\treturn nbytes;\n}\nEXPORT_SYMBOL_GPL(shash_ahash_update);\n\nstatic int shash_async_update(struct ahash_request *req)\n{\n\treturn shash_ahash_update(req, ahash_request_ctx(req));\n}\n\nstatic int shash_async_final(struct ahash_request *req)\n{\n\treturn crypto_shash_final(ahash_request_ctx(req), req->result);\n}\n\nint shash_ahash_finup(struct ahash_request *req, struct shash_desc *desc)\n{\n\tstruct crypto_hash_walk walk;\n\tint nbytes;\n\n\tnbytes = crypto_hash_walk_first(req, &walk);\n\tif (!nbytes)\n\t\treturn crypto_shash_final(desc, req->result);\n\n\tdo {\n\t\tnbytes = crypto_hash_walk_last(&walk) ?\n\t\t\t crypto_shash_finup(desc, walk.data, nbytes,\n\t\t\t\t\t    req->result) :\n\t\t\t crypto_shash_update(desc, walk.data, nbytes);\n\t\tnbytes = crypto_hash_walk_done(&walk, nbytes);\n\t} while (nbytes > 0);\n\n\treturn nbytes;\n}\nEXPORT_SYMBOL_GPL(shash_ahash_finup);\n\nstatic int shash_async_finup(struct ahash_request *req)\n{\n\tstruct crypto_shash **ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(req));\n\tstruct shash_desc *desc = ahash_request_ctx(req);\n\n\tdesc->tfm = *ctx;\n\n\treturn shash_ahash_finup(req, desc);\n}\n\nint shash_ahash_digest(struct ahash_request *req, struct shash_desc *desc)\n{\n\tunsigned int nbytes = req->nbytes;\n\tstruct scatterlist *sg;\n\tunsigned int offset;\n\tint err;\n\n\tif (nbytes &&\n\t    (sg = req->src, offset = sg->offset,\n\t     nbytes <= min(sg->length, ((unsigned int)(PAGE_SIZE)) - offset))) {\n\t\tvoid *data;\n\n\t\tdata = kmap_local_page(sg_page(sg));\n\t\terr = crypto_shash_digest(desc, data + offset, nbytes,\n\t\t\t\t\t  req->result);\n\t\tkunmap_local(data);\n\t} else\n\t\terr = crypto_shash_init(desc) ?:\n\t\t      shash_ahash_finup(req, desc);\n\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(shash_ahash_digest);\n\nstatic int shash_async_digest(struct ahash_request *req)\n{\n\tstruct crypto_shash **ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(req));\n\tstruct shash_desc *desc = ahash_request_ctx(req);\n\n\tdesc->tfm = *ctx;\n\n\treturn shash_ahash_digest(req, desc);\n}\n\nstatic int shash_async_export(struct ahash_request *req, void *out)\n{\n\treturn crypto_shash_export(ahash_request_ctx(req), out);\n}\n\nstatic int shash_async_import(struct ahash_request *req, const void *in)\n{\n\tstruct crypto_shash **ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(req));\n\tstruct shash_desc *desc = ahash_request_ctx(req);\n\n\tdesc->tfm = *ctx;\n\n\treturn crypto_shash_import(desc, in);\n}\n\nstatic void crypto_exit_shash_ops_async(struct crypto_tfm *tfm)\n{\n\tstruct crypto_shash **ctx = crypto_tfm_ctx(tfm);\n\n\tcrypto_free_shash(*ctx);\n}\n\nint crypto_init_shash_ops_async(struct crypto_tfm *tfm)\n{\n\tstruct crypto_alg *calg = tfm->__crt_alg;\n\tstruct shash_alg *alg = __crypto_shash_alg(calg);\n\tstruct crypto_ahash *crt = __crypto_ahash_cast(tfm);\n\tstruct crypto_shash **ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_shash *shash;\n\n\tif (!crypto_mod_get(calg))\n\t\treturn -EAGAIN;\n\n\tshash = crypto_create_tfm(calg, &crypto_shash_type);\n\tif (IS_ERR(shash)) {\n\t\tcrypto_mod_put(calg);\n\t\treturn PTR_ERR(shash);\n\t}\n\n\t*ctx = shash;\n\ttfm->exit = crypto_exit_shash_ops_async;\n\n\tcrt->init = shash_async_init;\n\tcrt->update = shash_async_update;\n\tcrt->final = shash_async_final;\n\tcrt->finup = shash_async_finup;\n\tcrt->digest = shash_async_digest;\n\tif (crypto_shash_alg_has_setkey(alg))\n\t\tcrt->setkey = shash_async_setkey;\n\n\tcrypto_ahash_set_flags(crt, crypto_shash_get_flags(shash) &\n\t\t\t\t    CRYPTO_TFM_NEED_KEY);\n\n\tcrt->export = shash_async_export;\n\tcrt->import = shash_async_import;\n\n\tcrt->reqsize = sizeof(struct shash_desc) + crypto_shash_descsize(shash);\n\n\treturn 0;\n}\n\nstruct crypto_ahash *crypto_clone_shash_ops_async(struct crypto_ahash *nhash,\n\t\t\t\t\t\t  struct crypto_ahash *hash)\n{\n\tstruct crypto_shash **nctx = crypto_ahash_ctx(nhash);\n\tstruct crypto_shash **ctx = crypto_ahash_ctx(hash);\n\tstruct crypto_shash *shash;\n\n\tshash = crypto_clone_shash(*ctx);\n\tif (IS_ERR(shash)) {\n\t\tcrypto_free_ahash(nhash);\n\t\treturn ERR_CAST(shash);\n\t}\n\n\t*nctx = shash;\n\n\treturn nhash;\n}\n\nstatic void crypto_shash_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_shash *hash = __crypto_shash_cast(tfm);\n\tstruct shash_alg *alg = crypto_shash_alg(hash);\n\n\talg->exit_tfm(hash);\n}\n\nstatic int crypto_shash_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_shash *hash = __crypto_shash_cast(tfm);\n\tstruct shash_alg *alg = crypto_shash_alg(hash);\n\tint err;\n\n\thash->descsize = alg->descsize;\n\n\tshash_set_needkey(hash, alg);\n\n\tif (alg->exit_tfm)\n\t\ttfm->exit = crypto_shash_exit_tfm;\n\n\tif (!alg->init_tfm)\n\t\treturn 0;\n\n\terr = alg->init_tfm(hash);\n\tif (err)\n\t\treturn err;\n\n\t \n\tif (WARN_ON_ONCE(hash->descsize > HASH_MAX_DESCSIZE)) {\n\t\tif (alg->exit_tfm)\n\t\t\talg->exit_tfm(hash);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic void crypto_shash_free_instance(struct crypto_instance *inst)\n{\n\tstruct shash_instance *shash = shash_instance(inst);\n\n\tshash->free(shash);\n}\n\nstatic int __maybe_unused crypto_shash_report(\n\tstruct sk_buff *skb, struct crypto_alg *alg)\n{\n\tstruct crypto_report_hash rhash;\n\tstruct shash_alg *salg = __crypto_shash_alg(alg);\n\n\tmemset(&rhash, 0, sizeof(rhash));\n\n\tstrscpy(rhash.type, \"shash\", sizeof(rhash.type));\n\n\trhash.blocksize = alg->cra_blocksize;\n\trhash.digestsize = salg->digestsize;\n\n\treturn nla_put(skb, CRYPTOCFGA_REPORT_HASH, sizeof(rhash), &rhash);\n}\n\nstatic void crypto_shash_show(struct seq_file *m, struct crypto_alg *alg)\n\t__maybe_unused;\nstatic void crypto_shash_show(struct seq_file *m, struct crypto_alg *alg)\n{\n\tstruct shash_alg *salg = __crypto_shash_alg(alg);\n\n\tseq_printf(m, \"type         : shash\\n\");\n\tseq_printf(m, \"blocksize    : %u\\n\", alg->cra_blocksize);\n\tseq_printf(m, \"digestsize   : %u\\n\", salg->digestsize);\n}\n\nstatic int __maybe_unused crypto_shash_report_stat(\n\tstruct sk_buff *skb, struct crypto_alg *alg)\n{\n\treturn crypto_hash_report_stat(skb, alg, \"shash\");\n}\n\nstatic const struct crypto_type crypto_shash_type = {\n\t.extsize = crypto_alg_extsize,\n\t.init_tfm = crypto_shash_init_tfm,\n\t.free = crypto_shash_free_instance,\n#ifdef CONFIG_PROC_FS\n\t.show = crypto_shash_show,\n#endif\n#if IS_ENABLED(CONFIG_CRYPTO_USER)\n\t.report = crypto_shash_report,\n#endif\n#ifdef CONFIG_CRYPTO_STATS\n\t.report_stat = crypto_shash_report_stat,\n#endif\n\t.maskclear = ~CRYPTO_ALG_TYPE_MASK,\n\t.maskset = CRYPTO_ALG_TYPE_MASK,\n\t.type = CRYPTO_ALG_TYPE_SHASH,\n\t.tfmsize = offsetof(struct crypto_shash, base),\n};\n\nint crypto_grab_shash(struct crypto_shash_spawn *spawn,\n\t\t      struct crypto_instance *inst,\n\t\t      const char *name, u32 type, u32 mask)\n{\n\tspawn->base.frontend = &crypto_shash_type;\n\treturn crypto_grab_spawn(&spawn->base, inst, name, type, mask);\n}\nEXPORT_SYMBOL_GPL(crypto_grab_shash);\n\nstruct crypto_shash *crypto_alloc_shash(const char *alg_name, u32 type,\n\t\t\t\t\tu32 mask)\n{\n\treturn crypto_alloc_tfm(alg_name, &crypto_shash_type, type, mask);\n}\nEXPORT_SYMBOL_GPL(crypto_alloc_shash);\n\nint crypto_has_shash(const char *alg_name, u32 type, u32 mask)\n{\n\treturn crypto_type_has_alg(alg_name, &crypto_shash_type, type, mask);\n}\nEXPORT_SYMBOL_GPL(crypto_has_shash);\n\nstruct crypto_shash *crypto_clone_shash(struct crypto_shash *hash)\n{\n\tstruct crypto_tfm *tfm = crypto_shash_tfm(hash);\n\tstruct shash_alg *alg = crypto_shash_alg(hash);\n\tstruct crypto_shash *nhash;\n\tint err;\n\n\tif (!crypto_shash_alg_has_setkey(alg)) {\n\t\ttfm = crypto_tfm_get(tfm);\n\t\tif (IS_ERR(tfm))\n\t\t\treturn ERR_CAST(tfm);\n\n\t\treturn hash;\n\t}\n\n\tif (!alg->clone_tfm && (alg->init_tfm || alg->base.cra_init))\n\t\treturn ERR_PTR(-ENOSYS);\n\n\tnhash = crypto_clone_tfm(&crypto_shash_type, tfm);\n\tif (IS_ERR(nhash))\n\t\treturn nhash;\n\n\tnhash->descsize = hash->descsize;\n\n\tif (alg->clone_tfm) {\n\t\terr = alg->clone_tfm(nhash, hash);\n\t\tif (err) {\n\t\t\tcrypto_free_shash(nhash);\n\t\t\treturn ERR_PTR(err);\n\t\t}\n\t}\n\n\treturn nhash;\n}\nEXPORT_SYMBOL_GPL(crypto_clone_shash);\n\nint hash_prepare_alg(struct hash_alg_common *alg)\n{\n\tstruct crypto_istat_hash *istat = hash_get_stat(alg);\n\tstruct crypto_alg *base = &alg->base;\n\n\tif (alg->digestsize > HASH_MAX_DIGESTSIZE)\n\t\treturn -EINVAL;\n\n\tbase->cra_flags &= ~CRYPTO_ALG_TYPE_MASK;\n\n\tif (IS_ENABLED(CONFIG_CRYPTO_STATS))\n\t\tmemset(istat, 0, sizeof(*istat));\n\n\treturn 0;\n}\n\nstatic int shash_prepare_alg(struct shash_alg *alg)\n{\n\tstruct crypto_alg *base = &alg->halg.base;\n\tint err;\n\n\tif (alg->descsize > HASH_MAX_DESCSIZE)\n\t\treturn -EINVAL;\n\n\tif (base->cra_alignmask > MAX_SHASH_ALIGNMASK)\n\t\treturn -EINVAL;\n\n\tif ((alg->export && !alg->import) || (alg->import && !alg->export))\n\t\treturn -EINVAL;\n\n\terr = hash_prepare_alg(&alg->halg);\n\tif (err)\n\t\treturn err;\n\n\tbase->cra_type = &crypto_shash_type;\n\tbase->cra_flags |= CRYPTO_ALG_TYPE_SHASH;\n\n\tif (!alg->finup)\n\t\talg->finup = shash_finup_unaligned;\n\tif (!alg->digest)\n\t\talg->digest = shash_digest_unaligned;\n\tif (!alg->export) {\n\t\talg->export = shash_default_export;\n\t\talg->import = shash_default_import;\n\t\talg->halg.statesize = alg->descsize;\n\t}\n\tif (!alg->setkey)\n\t\talg->setkey = shash_no_setkey;\n\n\treturn 0;\n}\n\nint crypto_register_shash(struct shash_alg *alg)\n{\n\tstruct crypto_alg *base = &alg->base;\n\tint err;\n\n\terr = shash_prepare_alg(alg);\n\tif (err)\n\t\treturn err;\n\n\treturn crypto_register_alg(base);\n}\nEXPORT_SYMBOL_GPL(crypto_register_shash);\n\nvoid crypto_unregister_shash(struct shash_alg *alg)\n{\n\tcrypto_unregister_alg(&alg->base);\n}\nEXPORT_SYMBOL_GPL(crypto_unregister_shash);\n\nint crypto_register_shashes(struct shash_alg *algs, int count)\n{\n\tint i, ret;\n\n\tfor (i = 0; i < count; i++) {\n\t\tret = crypto_register_shash(&algs[i]);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\treturn 0;\n\nerr:\n\tfor (--i; i >= 0; --i)\n\t\tcrypto_unregister_shash(&algs[i]);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(crypto_register_shashes);\n\nvoid crypto_unregister_shashes(struct shash_alg *algs, int count)\n{\n\tint i;\n\n\tfor (i = count - 1; i >= 0; --i)\n\t\tcrypto_unregister_shash(&algs[i]);\n}\nEXPORT_SYMBOL_GPL(crypto_unregister_shashes);\n\nint shash_register_instance(struct crypto_template *tmpl,\n\t\t\t    struct shash_instance *inst)\n{\n\tint err;\n\n\tif (WARN_ON(!inst->free))\n\t\treturn -EINVAL;\n\n\terr = shash_prepare_alg(&inst->alg);\n\tif (err)\n\t\treturn err;\n\n\treturn crypto_register_instance(tmpl, shash_crypto_instance(inst));\n}\nEXPORT_SYMBOL_GPL(shash_register_instance);\n\nvoid shash_free_singlespawn_instance(struct shash_instance *inst)\n{\n\tcrypto_drop_spawn(shash_instance_ctx(inst));\n\tkfree(inst);\n}\nEXPORT_SYMBOL_GPL(shash_free_singlespawn_instance);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"Synchronous cryptographic hash type\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}