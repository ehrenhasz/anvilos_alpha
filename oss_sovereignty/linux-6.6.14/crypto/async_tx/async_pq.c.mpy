{
  "module_name": "async_pq.c",
  "hash_id": "8e305dea71222b061b587820aa01a413026fdcdae70ec423125ff45dcc85b5ad",
  "original_prompt": "Ingested from linux-6.6.14/crypto/async_tx/async_pq.c",
  "human_readable_source": "\n \n#include <linux/kernel.h>\n#include <linux/interrupt.h>\n#include <linux/module.h>\n#include <linux/dma-mapping.h>\n#include <linux/raid/pq.h>\n#include <linux/async_tx.h>\n#include <linux/gfp.h>\n\n \nstatic struct page *pq_scribble_page;\n\n \n#define P(b, d) (b[d-2])\n#define Q(b, d) (b[d-1])\n\n#define MAX_DISKS 255\n\n \nstatic __async_inline struct dma_async_tx_descriptor *\ndo_async_gen_syndrome(struct dma_chan *chan,\n\t\t      const unsigned char *scfs, int disks,\n\t\t      struct dmaengine_unmap_data *unmap,\n\t\t      enum dma_ctrl_flags dma_flags,\n\t\t      struct async_submit_ctl *submit)\n{\n\tstruct dma_async_tx_descriptor *tx = NULL;\n\tstruct dma_device *dma = chan->device;\n\tenum async_tx_flags flags_orig = submit->flags;\n\tdma_async_tx_callback cb_fn_orig = submit->cb_fn;\n\tdma_async_tx_callback cb_param_orig = submit->cb_param;\n\tint src_cnt = disks - 2;\n\tunsigned short pq_src_cnt;\n\tdma_addr_t dma_dest[2];\n\tint src_off = 0;\n\n\twhile (src_cnt > 0) {\n\t\tsubmit->flags = flags_orig;\n\t\tpq_src_cnt = min(src_cnt, dma_maxpq(dma, dma_flags));\n\t\t \n\t\tif (src_cnt > pq_src_cnt) {\n\t\t\tsubmit->flags &= ~ASYNC_TX_ACK;\n\t\t\tsubmit->flags |= ASYNC_TX_FENCE;\n\t\t\tsubmit->cb_fn = NULL;\n\t\t\tsubmit->cb_param = NULL;\n\t\t} else {\n\t\t\tsubmit->cb_fn = cb_fn_orig;\n\t\t\tsubmit->cb_param = cb_param_orig;\n\t\t\tif (cb_fn_orig)\n\t\t\t\tdma_flags |= DMA_PREP_INTERRUPT;\n\t\t}\n\t\tif (submit->flags & ASYNC_TX_FENCE)\n\t\t\tdma_flags |= DMA_PREP_FENCE;\n\n\t\t \n\t\tfor (;;) {\n\t\t\tdma_dest[0] = unmap->addr[disks - 2];\n\t\t\tdma_dest[1] = unmap->addr[disks - 1];\n\t\t\ttx = dma->device_prep_dma_pq(chan, dma_dest,\n\t\t\t\t\t\t     &unmap->addr[src_off],\n\t\t\t\t\t\t     pq_src_cnt,\n\t\t\t\t\t\t     &scfs[src_off], unmap->len,\n\t\t\t\t\t\t     dma_flags);\n\t\t\tif (likely(tx))\n\t\t\t\tbreak;\n\t\t\tasync_tx_quiesce(&submit->depend_tx);\n\t\t\tdma_async_issue_pending(chan);\n\t\t}\n\n\t\tdma_set_unmap(tx, unmap);\n\t\tasync_tx_submit(chan, tx, submit);\n\t\tsubmit->depend_tx = tx;\n\n\t\t \n\t\tsrc_cnt -= pq_src_cnt;\n\t\tsrc_off += pq_src_cnt;\n\n\t\tdma_flags |= DMA_PREP_CONTINUE;\n\t}\n\n\treturn tx;\n}\n\n \nstatic void\ndo_sync_gen_syndrome(struct page **blocks, unsigned int *offsets, int disks,\n\t\t     size_t len, struct async_submit_ctl *submit)\n{\n\tvoid **srcs;\n\tint i;\n\tint start = -1, stop = disks - 3;\n\n\tif (submit->scribble)\n\t\tsrcs = submit->scribble;\n\telse\n\t\tsrcs = (void **) blocks;\n\n\tfor (i = 0; i < disks; i++) {\n\t\tif (blocks[i] == NULL) {\n\t\t\tBUG_ON(i > disks - 3);  \n\t\t\tsrcs[i] = (void*)raid6_empty_zero_page;\n\t\t} else {\n\t\t\tsrcs[i] = page_address(blocks[i]) + offsets[i];\n\n\t\t\tif (i < disks - 2) {\n\t\t\t\tstop = i;\n\t\t\t\tif (start == -1)\n\t\t\t\t\tstart = i;\n\t\t\t}\n\t\t}\n\t}\n\tif (submit->flags & ASYNC_TX_PQ_XOR_DST) {\n\t\tBUG_ON(!raid6_call.xor_syndrome);\n\t\tif (start >= 0)\n\t\t\traid6_call.xor_syndrome(disks, start, stop, len, srcs);\n\t} else\n\t\traid6_call.gen_syndrome(disks, len, srcs);\n\tasync_tx_sync_epilog(submit);\n}\n\nstatic inline bool\nis_dma_pq_aligned_offs(struct dma_device *dev, unsigned int *offs,\n\t\t\t\t     int src_cnt, size_t len)\n{\n\tint i;\n\n\tfor (i = 0; i < src_cnt; i++) {\n\t\tif (!is_dma_pq_aligned(dev, offs[i], 0, len))\n\t\t\treturn false;\n\t}\n\treturn true;\n}\n\n \nstruct dma_async_tx_descriptor *\nasync_gen_syndrome(struct page **blocks, unsigned int *offsets, int disks,\n\t\t   size_t len, struct async_submit_ctl *submit)\n{\n\tint src_cnt = disks - 2;\n\tstruct dma_chan *chan = async_tx_find_channel(submit, DMA_PQ,\n\t\t\t\t\t\t      &P(blocks, disks), 2,\n\t\t\t\t\t\t      blocks, src_cnt, len);\n\tstruct dma_device *device = chan ? chan->device : NULL;\n\tstruct dmaengine_unmap_data *unmap = NULL;\n\n\tBUG_ON(disks > MAX_DISKS || !(P(blocks, disks) || Q(blocks, disks)));\n\n\tif (device)\n\t\tunmap = dmaengine_get_unmap_data(device->dev, disks, GFP_NOWAIT);\n\n\t \n\tif (unmap && !(submit->flags & ASYNC_TX_PQ_XOR_DST) &&\n\t    (src_cnt <= dma_maxpq(device, 0) ||\n\t     dma_maxpq(device, DMA_PREP_CONTINUE) > 0) &&\n\t    is_dma_pq_aligned_offs(device, offsets, disks, len)) {\n\t\tstruct dma_async_tx_descriptor *tx;\n\t\tenum dma_ctrl_flags dma_flags = 0;\n\t\tunsigned char coefs[MAX_DISKS];\n\t\tint i, j;\n\n\t\t \n\t\tpr_debug(\"%s: (async) disks: %d len: %zu\\n\",\n\t\t\t __func__, disks, len);\n\n\t\t \n\t\tunmap->len = len;\n\t\tfor (i = 0, j = 0; i < src_cnt; i++) {\n\t\t\tif (blocks[i] == NULL)\n\t\t\t\tcontinue;\n\t\t\tunmap->addr[j] = dma_map_page(device->dev, blocks[i],\n\t\t\t\t\t\toffsets[i], len, DMA_TO_DEVICE);\n\t\t\tcoefs[j] = raid6_gfexp[i];\n\t\t\tunmap->to_cnt++;\n\t\t\tj++;\n\t\t}\n\n\t\t \n\t\tunmap->bidi_cnt++;\n\t\tif (P(blocks, disks))\n\t\t\tunmap->addr[j++] = dma_map_page(device->dev, P(blocks, disks),\n\t\t\t\t\t\t\tP(offsets, disks),\n\t\t\t\t\t\t\tlen, DMA_BIDIRECTIONAL);\n\t\telse {\n\t\t\tunmap->addr[j++] = 0;\n\t\t\tdma_flags |= DMA_PREP_PQ_DISABLE_P;\n\t\t}\n\n\t\tunmap->bidi_cnt++;\n\t\tif (Q(blocks, disks))\n\t\t\tunmap->addr[j++] = dma_map_page(device->dev, Q(blocks, disks),\n\t\t\t\t\t\t\tQ(offsets, disks),\n\t\t\t\t\t\t\tlen, DMA_BIDIRECTIONAL);\n\t\telse {\n\t\t\tunmap->addr[j++] = 0;\n\t\t\tdma_flags |= DMA_PREP_PQ_DISABLE_Q;\n\t\t}\n\n\t\ttx = do_async_gen_syndrome(chan, coefs, j, unmap, dma_flags, submit);\n\t\tdmaengine_unmap_put(unmap);\n\t\treturn tx;\n\t}\n\n\tdmaengine_unmap_put(unmap);\n\n\t \n\tpr_debug(\"%s: (sync) disks: %d len: %zu\\n\", __func__, disks, len);\n\n\t \n\tasync_tx_quiesce(&submit->depend_tx);\n\n\tif (!P(blocks, disks)) {\n\t\tP(blocks, disks) = pq_scribble_page;\n\t\tP(offsets, disks) = 0;\n\t}\n\tif (!Q(blocks, disks)) {\n\t\tQ(blocks, disks) = pq_scribble_page;\n\t\tQ(offsets, disks) = 0;\n\t}\n\tdo_sync_gen_syndrome(blocks, offsets, disks, len, submit);\n\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(async_gen_syndrome);\n\nstatic inline struct dma_chan *\npq_val_chan(struct async_submit_ctl *submit, struct page **blocks, int disks, size_t len)\n{\n\t#ifdef CONFIG_ASYNC_TX_DISABLE_PQ_VAL_DMA\n\treturn NULL;\n\t#endif\n\treturn async_tx_find_channel(submit, DMA_PQ_VAL, NULL, 0,  blocks,\n\t\t\t\t     disks, len);\n}\n\n \nstruct dma_async_tx_descriptor *\nasync_syndrome_val(struct page **blocks, unsigned int *offsets, int disks,\n\t\t   size_t len, enum sum_check_flags *pqres, struct page *spare,\n\t\t   unsigned int s_off, struct async_submit_ctl *submit)\n{\n\tstruct dma_chan *chan = pq_val_chan(submit, blocks, disks, len);\n\tstruct dma_device *device = chan ? chan->device : NULL;\n\tstruct dma_async_tx_descriptor *tx;\n\tunsigned char coefs[MAX_DISKS];\n\tenum dma_ctrl_flags dma_flags = submit->cb_fn ? DMA_PREP_INTERRUPT : 0;\n\tstruct dmaengine_unmap_data *unmap = NULL;\n\n\tBUG_ON(disks < 4 || disks > MAX_DISKS);\n\n\tif (device)\n\t\tunmap = dmaengine_get_unmap_data(device->dev, disks, GFP_NOWAIT);\n\n\tif (unmap && disks <= dma_maxpq(device, 0) &&\n\t    is_dma_pq_aligned_offs(device, offsets, disks, len)) {\n\t\tstruct device *dev = device->dev;\n\t\tdma_addr_t pq[2];\n\t\tint i, j = 0, src_cnt = 0;\n\n\t\tpr_debug(\"%s: (async) disks: %d len: %zu\\n\",\n\t\t\t __func__, disks, len);\n\n\t\tunmap->len = len;\n\t\tfor (i = 0; i < disks-2; i++)\n\t\t\tif (likely(blocks[i])) {\n\t\t\t\tunmap->addr[j] = dma_map_page(dev, blocks[i],\n\t\t\t\t\t\t\t      offsets[i], len,\n\t\t\t\t\t\t\t      DMA_TO_DEVICE);\n\t\t\t\tcoefs[j] = raid6_gfexp[i];\n\t\t\t\tunmap->to_cnt++;\n\t\t\t\tsrc_cnt++;\n\t\t\t\tj++;\n\t\t\t}\n\n\t\tif (!P(blocks, disks)) {\n\t\t\tpq[0] = 0;\n\t\t\tdma_flags |= DMA_PREP_PQ_DISABLE_P;\n\t\t} else {\n\t\t\tpq[0] = dma_map_page(dev, P(blocks, disks),\n\t\t\t\t\t     P(offsets, disks), len,\n\t\t\t\t\t     DMA_TO_DEVICE);\n\t\t\tunmap->addr[j++] = pq[0];\n\t\t\tunmap->to_cnt++;\n\t\t}\n\t\tif (!Q(blocks, disks)) {\n\t\t\tpq[1] = 0;\n\t\t\tdma_flags |= DMA_PREP_PQ_DISABLE_Q;\n\t\t} else {\n\t\t\tpq[1] = dma_map_page(dev, Q(blocks, disks),\n\t\t\t\t\t     Q(offsets, disks), len,\n\t\t\t\t\t     DMA_TO_DEVICE);\n\t\t\tunmap->addr[j++] = pq[1];\n\t\t\tunmap->to_cnt++;\n\t\t}\n\n\t\tif (submit->flags & ASYNC_TX_FENCE)\n\t\t\tdma_flags |= DMA_PREP_FENCE;\n\t\tfor (;;) {\n\t\t\ttx = device->device_prep_dma_pq_val(chan, pq,\n\t\t\t\t\t\t\t    unmap->addr,\n\t\t\t\t\t\t\t    src_cnt,\n\t\t\t\t\t\t\t    coefs,\n\t\t\t\t\t\t\t    len, pqres,\n\t\t\t\t\t\t\t    dma_flags);\n\t\t\tif (likely(tx))\n\t\t\t\tbreak;\n\t\t\tasync_tx_quiesce(&submit->depend_tx);\n\t\t\tdma_async_issue_pending(chan);\n\t\t}\n\n\t\tdma_set_unmap(tx, unmap);\n\t\tasync_tx_submit(chan, tx, submit);\n\t} else {\n\t\tstruct page *p_src = P(blocks, disks);\n\t\tunsigned int p_off = P(offsets, disks);\n\t\tstruct page *q_src = Q(blocks, disks);\n\t\tunsigned int q_off = Q(offsets, disks);\n\t\tenum async_tx_flags flags_orig = submit->flags;\n\t\tdma_async_tx_callback cb_fn_orig = submit->cb_fn;\n\t\tvoid *scribble = submit->scribble;\n\t\tvoid *cb_param_orig = submit->cb_param;\n\t\tvoid *p, *q, *s;\n\n\t\tpr_debug(\"%s: (sync) disks: %d len: %zu\\n\",\n\t\t\t __func__, disks, len);\n\n\t\t \n\t\tBUG_ON(!spare || !scribble);\n\n\t\t \n\t\tasync_tx_quiesce(&submit->depend_tx);\n\n\t\t \n\t\ttx = NULL;\n\t\t*pqres = 0;\n\t\tif (p_src) {\n\t\t\tinit_async_submit(submit, ASYNC_TX_XOR_ZERO_DST, NULL,\n\t\t\t\t\t  NULL, NULL, scribble);\n\t\t\ttx = async_xor_offs(spare, s_off,\n\t\t\t\t\tblocks, offsets, disks-2, len, submit);\n\t\t\tasync_tx_quiesce(&tx);\n\t\t\tp = page_address(p_src) + p_off;\n\t\t\ts = page_address(spare) + s_off;\n\t\t\t*pqres |= !!memcmp(p, s, len) << SUM_CHECK_P;\n\t\t}\n\n\t\tif (q_src) {\n\t\t\tP(blocks, disks) = NULL;\n\t\t\tQ(blocks, disks) = spare;\n\t\t\tQ(offsets, disks) = s_off;\n\t\t\tinit_async_submit(submit, 0, NULL, NULL, NULL, scribble);\n\t\t\ttx = async_gen_syndrome(blocks, offsets, disks,\n\t\t\t\t\tlen, submit);\n\t\t\tasync_tx_quiesce(&tx);\n\t\t\tq = page_address(q_src) + q_off;\n\t\t\ts = page_address(spare) + s_off;\n\t\t\t*pqres |= !!memcmp(q, s, len) << SUM_CHECK_Q;\n\t\t}\n\n\t\t \n\t\tP(blocks, disks) = p_src;\n\t\tP(offsets, disks) = p_off;\n\t\tQ(blocks, disks) = q_src;\n\t\tQ(offsets, disks) = q_off;\n\n\t\tsubmit->cb_fn = cb_fn_orig;\n\t\tsubmit->cb_param = cb_param_orig;\n\t\tsubmit->flags = flags_orig;\n\t\tasync_tx_sync_epilog(submit);\n\t\ttx = NULL;\n\t}\n\tdmaengine_unmap_put(unmap);\n\n\treturn tx;\n}\nEXPORT_SYMBOL_GPL(async_syndrome_val);\n\nstatic int __init async_pq_init(void)\n{\n\tpq_scribble_page = alloc_page(GFP_KERNEL);\n\n\tif (pq_scribble_page)\n\t\treturn 0;\n\n\tpr_err(\"%s: failed to allocate required spare page\\n\", __func__);\n\n\treturn -ENOMEM;\n}\n\nstatic void __exit async_pq_exit(void)\n{\n\t__free_page(pq_scribble_page);\n}\n\nmodule_init(async_pq_init);\nmodule_exit(async_pq_exit);\n\nMODULE_DESCRIPTION(\"asynchronous raid6 syndrome generation/validation\");\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}