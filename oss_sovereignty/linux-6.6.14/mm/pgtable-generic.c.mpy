{
  "module_name": "pgtable-generic.c",
  "hash_id": "c79951aafedbb72ef7930e1cea72ccbabdc07f4535d958ba872f1c5d3c291c86",
  "original_prompt": "Ingested from linux-6.6.14/mm/pgtable-generic.c",
  "human_readable_source": "\n \n\n#include <linux/pagemap.h>\n#include <linux/hugetlb.h>\n#include <linux/pgtable.h>\n#include <linux/swap.h>\n#include <linux/swapops.h>\n#include <linux/mm_inline.h>\n#include <asm/pgalloc.h>\n#include <asm/tlb.h>\n\n \n\nvoid pgd_clear_bad(pgd_t *pgd)\n{\n\tpgd_ERROR(*pgd);\n\tpgd_clear(pgd);\n}\n\n#ifndef __PAGETABLE_P4D_FOLDED\nvoid p4d_clear_bad(p4d_t *p4d)\n{\n\tp4d_ERROR(*p4d);\n\tp4d_clear(p4d);\n}\n#endif\n\n#ifndef __PAGETABLE_PUD_FOLDED\nvoid pud_clear_bad(pud_t *pud)\n{\n\tpud_ERROR(*pud);\n\tpud_clear(pud);\n}\n#endif\n\n \nvoid pmd_clear_bad(pmd_t *pmd)\n{\n\tpmd_ERROR(*pmd);\n\tpmd_clear(pmd);\n}\n\n#ifndef __HAVE_ARCH_PTEP_SET_ACCESS_FLAGS\n \nint ptep_set_access_flags(struct vm_area_struct *vma,\n\t\t\t  unsigned long address, pte_t *ptep,\n\t\t\t  pte_t entry, int dirty)\n{\n\tint changed = !pte_same(ptep_get(ptep), entry);\n\tif (changed) {\n\t\tset_pte_at(vma->vm_mm, address, ptep, entry);\n\t\tflush_tlb_fix_spurious_fault(vma, address, ptep);\n\t}\n\treturn changed;\n}\n#endif\n\n#ifndef __HAVE_ARCH_PTEP_CLEAR_YOUNG_FLUSH\nint ptep_clear_flush_young(struct vm_area_struct *vma,\n\t\t\t   unsigned long address, pte_t *ptep)\n{\n\tint young;\n\tyoung = ptep_test_and_clear_young(vma, address, ptep);\n\tif (young)\n\t\tflush_tlb_page(vma, address);\n\treturn young;\n}\n#endif\n\n#ifndef __HAVE_ARCH_PTEP_CLEAR_FLUSH\npte_t ptep_clear_flush(struct vm_area_struct *vma, unsigned long address,\n\t\t       pte_t *ptep)\n{\n\tstruct mm_struct *mm = (vma)->vm_mm;\n\tpte_t pte;\n\tpte = ptep_get_and_clear(mm, address, ptep);\n\tif (pte_accessible(mm, pte))\n\t\tflush_tlb_page(vma, address);\n\treturn pte;\n}\n#endif\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\n#ifndef __HAVE_ARCH_PMDP_SET_ACCESS_FLAGS\nint pmdp_set_access_flags(struct vm_area_struct *vma,\n\t\t\t  unsigned long address, pmd_t *pmdp,\n\t\t\t  pmd_t entry, int dirty)\n{\n\tint changed = !pmd_same(*pmdp, entry);\n\tVM_BUG_ON(address & ~HPAGE_PMD_MASK);\n\tif (changed) {\n\t\tset_pmd_at(vma->vm_mm, address, pmdp, entry);\n\t\tflush_pmd_tlb_range(vma, address, address + HPAGE_PMD_SIZE);\n\t}\n\treturn changed;\n}\n#endif\n\n#ifndef __HAVE_ARCH_PMDP_CLEAR_YOUNG_FLUSH\nint pmdp_clear_flush_young(struct vm_area_struct *vma,\n\t\t\t   unsigned long address, pmd_t *pmdp)\n{\n\tint young;\n\tVM_BUG_ON(address & ~HPAGE_PMD_MASK);\n\tyoung = pmdp_test_and_clear_young(vma, address, pmdp);\n\tif (young)\n\t\tflush_pmd_tlb_range(vma, address, address + HPAGE_PMD_SIZE);\n\treturn young;\n}\n#endif\n\n#ifndef __HAVE_ARCH_PMDP_HUGE_CLEAR_FLUSH\npmd_t pmdp_huge_clear_flush(struct vm_area_struct *vma, unsigned long address,\n\t\t\t    pmd_t *pmdp)\n{\n\tpmd_t pmd;\n\tVM_BUG_ON(address & ~HPAGE_PMD_MASK);\n\tVM_BUG_ON(pmd_present(*pmdp) && !pmd_trans_huge(*pmdp) &&\n\t\t\t   !pmd_devmap(*pmdp));\n\tpmd = pmdp_huge_get_and_clear(vma->vm_mm, address, pmdp);\n\tflush_pmd_tlb_range(vma, address, address + HPAGE_PMD_SIZE);\n\treturn pmd;\n}\n\n#ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD\npud_t pudp_huge_clear_flush(struct vm_area_struct *vma, unsigned long address,\n\t\t\t    pud_t *pudp)\n{\n\tpud_t pud;\n\n\tVM_BUG_ON(address & ~HPAGE_PUD_MASK);\n\tVM_BUG_ON(!pud_trans_huge(*pudp) && !pud_devmap(*pudp));\n\tpud = pudp_huge_get_and_clear(vma->vm_mm, address, pudp);\n\tflush_pud_tlb_range(vma, address, address + HPAGE_PUD_SIZE);\n\treturn pud;\n}\n#endif\n#endif\n\n#ifndef __HAVE_ARCH_PGTABLE_DEPOSIT\nvoid pgtable_trans_huge_deposit(struct mm_struct *mm, pmd_t *pmdp,\n\t\t\t\tpgtable_t pgtable)\n{\n\tassert_spin_locked(pmd_lockptr(mm, pmdp));\n\n\t \n\tif (!pmd_huge_pte(mm, pmdp))\n\t\tINIT_LIST_HEAD(&pgtable->lru);\n\telse\n\t\tlist_add(&pgtable->lru, &pmd_huge_pte(mm, pmdp)->lru);\n\tpmd_huge_pte(mm, pmdp) = pgtable;\n}\n#endif\n\n#ifndef __HAVE_ARCH_PGTABLE_WITHDRAW\n \npgtable_t pgtable_trans_huge_withdraw(struct mm_struct *mm, pmd_t *pmdp)\n{\n\tpgtable_t pgtable;\n\n\tassert_spin_locked(pmd_lockptr(mm, pmdp));\n\n\t \n\tpgtable = pmd_huge_pte(mm, pmdp);\n\tpmd_huge_pte(mm, pmdp) = list_first_entry_or_null(&pgtable->lru,\n\t\t\t\t\t\t\t  struct page, lru);\n\tif (pmd_huge_pte(mm, pmdp))\n\t\tlist_del(&pgtable->lru);\n\treturn pgtable;\n}\n#endif\n\n#ifndef __HAVE_ARCH_PMDP_INVALIDATE\npmd_t pmdp_invalidate(struct vm_area_struct *vma, unsigned long address,\n\t\t     pmd_t *pmdp)\n{\n\tpmd_t old = pmdp_establish(vma, address, pmdp, pmd_mkinvalid(*pmdp));\n\tflush_pmd_tlb_range(vma, address, address + HPAGE_PMD_SIZE);\n\treturn old;\n}\n#endif\n\n#ifndef __HAVE_ARCH_PMDP_INVALIDATE_AD\npmd_t pmdp_invalidate_ad(struct vm_area_struct *vma, unsigned long address,\n\t\t\t pmd_t *pmdp)\n{\n\treturn pmdp_invalidate(vma, address, pmdp);\n}\n#endif\n\n#ifndef pmdp_collapse_flush\npmd_t pmdp_collapse_flush(struct vm_area_struct *vma, unsigned long address,\n\t\t\t  pmd_t *pmdp)\n{\n\t \n\tpmd_t pmd;\n\n\tVM_BUG_ON(address & ~HPAGE_PMD_MASK);\n\tVM_BUG_ON(pmd_trans_huge(*pmdp));\n\tpmd = pmdp_huge_get_and_clear(vma->vm_mm, address, pmdp);\n\n\t \n\tflush_tlb_range(vma, address, address + HPAGE_PMD_SIZE);\n\treturn pmd;\n}\n#endif\n\n \n#ifndef pte_free_defer\nstatic void pte_free_now(struct rcu_head *head)\n{\n\tstruct page *page;\n\n\tpage = container_of(head, struct page, rcu_head);\n\tpte_free(NULL  , (pgtable_t)page);\n}\n\nvoid pte_free_defer(struct mm_struct *mm, pgtable_t pgtable)\n{\n\tstruct page *page;\n\n\tpage = pgtable;\n\tcall_rcu(&page->rcu_head, pte_free_now);\n}\n#endif  \n#endif  \n\n#if defined(CONFIG_GUP_GET_PXX_LOW_HIGH) && \\\n\t(defined(CONFIG_SMP) || defined(CONFIG_PREEMPT_RCU))\n \nstatic unsigned long pmdp_get_lockless_start(void)\n{\n\tunsigned long irqflags;\n\n\tlocal_irq_save(irqflags);\n\treturn irqflags;\n}\nstatic void pmdp_get_lockless_end(unsigned long irqflags)\n{\n\tlocal_irq_restore(irqflags);\n}\n#else\nstatic unsigned long pmdp_get_lockless_start(void) { return 0; }\nstatic void pmdp_get_lockless_end(unsigned long irqflags) { }\n#endif\n\npte_t *__pte_offset_map(pmd_t *pmd, unsigned long addr, pmd_t *pmdvalp)\n{\n\tunsigned long irqflags;\n\tpmd_t pmdval;\n\n\trcu_read_lock();\n\tirqflags = pmdp_get_lockless_start();\n\tpmdval = pmdp_get_lockless(pmd);\n\tpmdp_get_lockless_end(irqflags);\n\n\tif (pmdvalp)\n\t\t*pmdvalp = pmdval;\n\tif (unlikely(pmd_none(pmdval) || is_pmd_migration_entry(pmdval)))\n\t\tgoto nomap;\n\tif (unlikely(pmd_trans_huge(pmdval) || pmd_devmap(pmdval)))\n\t\tgoto nomap;\n\tif (unlikely(pmd_bad(pmdval))) {\n\t\tpmd_clear_bad(pmd);\n\t\tgoto nomap;\n\t}\n\treturn __pte_map(&pmdval, addr);\nnomap:\n\trcu_read_unlock();\n\treturn NULL;\n}\n\npte_t *pte_offset_map_nolock(struct mm_struct *mm, pmd_t *pmd,\n\t\t\t     unsigned long addr, spinlock_t **ptlp)\n{\n\tpmd_t pmdval;\n\tpte_t *pte;\n\n\tpte = __pte_offset_map(pmd, addr, &pmdval);\n\tif (likely(pte))\n\t\t*ptlp = pte_lockptr(mm, &pmdval);\n\treturn pte;\n}\n\n \npte_t *__pte_offset_map_lock(struct mm_struct *mm, pmd_t *pmd,\n\t\t\t     unsigned long addr, spinlock_t **ptlp)\n{\n\tspinlock_t *ptl;\n\tpmd_t pmdval;\n\tpte_t *pte;\nagain:\n\tpte = __pte_offset_map(pmd, addr, &pmdval);\n\tif (unlikely(!pte))\n\t\treturn pte;\n\tptl = pte_lockptr(mm, &pmdval);\n\tspin_lock(ptl);\n\tif (likely(pmd_same(pmdval, pmdp_get_lockless(pmd)))) {\n\t\t*ptlp = ptl;\n\t\treturn pte;\n\t}\n\tpte_unmap_unlock(pte, ptl);\n\tgoto again;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}