{
  "module_name": "core.c",
  "hash_id": "933cb1d89ed137b466ed0abc73393eaeaebe528d5603440be32580bd4758488c",
  "original_prompt": "Ingested from linux-6.6.14/mm/kfence/core.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) \"kfence: \" fmt\n\n#include <linux/atomic.h>\n#include <linux/bug.h>\n#include <linux/debugfs.h>\n#include <linux/hash.h>\n#include <linux/irq_work.h>\n#include <linux/jhash.h>\n#include <linux/kcsan-checks.h>\n#include <linux/kfence.h>\n#include <linux/kmemleak.h>\n#include <linux/list.h>\n#include <linux/lockdep.h>\n#include <linux/log2.h>\n#include <linux/memblock.h>\n#include <linux/moduleparam.h>\n#include <linux/notifier.h>\n#include <linux/panic_notifier.h>\n#include <linux/random.h>\n#include <linux/rcupdate.h>\n#include <linux/sched/clock.h>\n#include <linux/seq_file.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n#include <linux/string.h>\n\n#include <asm/kfence.h>\n\n#include \"kfence.h\"\n\n \n#define KFENCE_WARN_ON(cond)                                                   \\\n\t({                                                                     \\\n\t\tconst bool __cond = WARN_ON(cond);                             \\\n\t\tif (unlikely(__cond)) {                                        \\\n\t\t\tWRITE_ONCE(kfence_enabled, false);                     \\\n\t\t\tdisabled_by_warn = true;                               \\\n\t\t}                                                              \\\n\t\t__cond;                                                        \\\n\t})\n\n \n\nstatic bool kfence_enabled __read_mostly;\nstatic bool disabled_by_warn __read_mostly;\n\nunsigned long kfence_sample_interval __read_mostly = CONFIG_KFENCE_SAMPLE_INTERVAL;\nEXPORT_SYMBOL_GPL(kfence_sample_interval);  \n\n#ifdef MODULE_PARAM_PREFIX\n#undef MODULE_PARAM_PREFIX\n#endif\n#define MODULE_PARAM_PREFIX \"kfence.\"\n\nstatic int kfence_enable_late(void);\nstatic int param_set_sample_interval(const char *val, const struct kernel_param *kp)\n{\n\tunsigned long num;\n\tint ret = kstrtoul(val, 0, &num);\n\n\tif (ret < 0)\n\t\treturn ret;\n\n\t \n\tif (!num && READ_ONCE(kfence_enabled)) {\n\t\tpr_info(\"disabled\\n\");\n\t\tWRITE_ONCE(kfence_enabled, false);\n\t}\n\n\t*((unsigned long *)kp->arg) = num;\n\n\tif (num && !READ_ONCE(kfence_enabled) && system_state != SYSTEM_BOOTING)\n\t\treturn disabled_by_warn ? -EINVAL : kfence_enable_late();\n\treturn 0;\n}\n\nstatic int param_get_sample_interval(char *buffer, const struct kernel_param *kp)\n{\n\tif (!READ_ONCE(kfence_enabled))\n\t\treturn sprintf(buffer, \"0\\n\");\n\n\treturn param_get_ulong(buffer, kp);\n}\n\nstatic const struct kernel_param_ops sample_interval_param_ops = {\n\t.set = param_set_sample_interval,\n\t.get = param_get_sample_interval,\n};\nmodule_param_cb(sample_interval, &sample_interval_param_ops, &kfence_sample_interval, 0600);\n\n \nstatic unsigned long kfence_skip_covered_thresh __read_mostly = 75;\nmodule_param_named(skip_covered_thresh, kfence_skip_covered_thresh, ulong, 0644);\n\n \nstatic bool kfence_deferrable __read_mostly = IS_ENABLED(CONFIG_KFENCE_DEFERRABLE);\nmodule_param_named(deferrable, kfence_deferrable, bool, 0444);\n\n \nstatic bool kfence_check_on_panic __read_mostly;\nmodule_param_named(check_on_panic, kfence_check_on_panic, bool, 0444);\n\n \nchar *__kfence_pool __read_mostly;\nEXPORT_SYMBOL(__kfence_pool);  \n\n \nstatic_assert(CONFIG_KFENCE_NUM_OBJECTS > 0);\nstruct kfence_metadata *kfence_metadata __read_mostly;\n\n \nstatic struct kfence_metadata *kfence_metadata_init __read_mostly;\n\n \nstatic struct list_head kfence_freelist = LIST_HEAD_INIT(kfence_freelist);\nstatic DEFINE_RAW_SPINLOCK(kfence_freelist_lock);  \n\n \nDEFINE_STATIC_KEY_FALSE(kfence_allocation_key);\n\n \natomic_t kfence_allocation_gate = ATOMIC_INIT(1);\n\n \n#define ALLOC_COVERED_HNUM\t2\n#define ALLOC_COVERED_ORDER\t(const_ilog2(CONFIG_KFENCE_NUM_OBJECTS) + 2)\n#define ALLOC_COVERED_SIZE\t(1 << ALLOC_COVERED_ORDER)\n#define ALLOC_COVERED_HNEXT(h)\thash_32(h, ALLOC_COVERED_ORDER)\n#define ALLOC_COVERED_MASK\t(ALLOC_COVERED_SIZE - 1)\nstatic atomic_t alloc_covered[ALLOC_COVERED_SIZE];\n\n \n#define UNIQUE_ALLOC_STACK_DEPTH ((size_t)8)\n\n \nstatic u32 stack_hash_seed __ro_after_init;\n\n \nenum kfence_counter_id {\n\tKFENCE_COUNTER_ALLOCATED,\n\tKFENCE_COUNTER_ALLOCS,\n\tKFENCE_COUNTER_FREES,\n\tKFENCE_COUNTER_ZOMBIES,\n\tKFENCE_COUNTER_BUGS,\n\tKFENCE_COUNTER_SKIP_INCOMPAT,\n\tKFENCE_COUNTER_SKIP_CAPACITY,\n\tKFENCE_COUNTER_SKIP_COVERED,\n\tKFENCE_COUNTER_COUNT,\n};\nstatic atomic_long_t counters[KFENCE_COUNTER_COUNT];\nstatic const char *const counter_names[] = {\n\t[KFENCE_COUNTER_ALLOCATED]\t= \"currently allocated\",\n\t[KFENCE_COUNTER_ALLOCS]\t\t= \"total allocations\",\n\t[KFENCE_COUNTER_FREES]\t\t= \"total frees\",\n\t[KFENCE_COUNTER_ZOMBIES]\t= \"zombie allocations\",\n\t[KFENCE_COUNTER_BUGS]\t\t= \"total bugs\",\n\t[KFENCE_COUNTER_SKIP_INCOMPAT]\t= \"skipped allocations (incompatible)\",\n\t[KFENCE_COUNTER_SKIP_CAPACITY]\t= \"skipped allocations (capacity)\",\n\t[KFENCE_COUNTER_SKIP_COVERED]\t= \"skipped allocations (covered)\",\n};\nstatic_assert(ARRAY_SIZE(counter_names) == KFENCE_COUNTER_COUNT);\n\n \n\nstatic inline bool should_skip_covered(void)\n{\n\tunsigned long thresh = (CONFIG_KFENCE_NUM_OBJECTS * kfence_skip_covered_thresh) / 100;\n\n\treturn atomic_long_read(&counters[KFENCE_COUNTER_ALLOCATED]) > thresh;\n}\n\nstatic u32 get_alloc_stack_hash(unsigned long *stack_entries, size_t num_entries)\n{\n\tnum_entries = min(num_entries, UNIQUE_ALLOC_STACK_DEPTH);\n\tnum_entries = filter_irq_stacks(stack_entries, num_entries);\n\treturn jhash(stack_entries, num_entries * sizeof(stack_entries[0]), stack_hash_seed);\n}\n\n \nstatic void alloc_covered_add(u32 alloc_stack_hash, int val)\n{\n\tint i;\n\n\tfor (i = 0; i < ALLOC_COVERED_HNUM; i++) {\n\t\tatomic_add(val, &alloc_covered[alloc_stack_hash & ALLOC_COVERED_MASK]);\n\t\talloc_stack_hash = ALLOC_COVERED_HNEXT(alloc_stack_hash);\n\t}\n}\n\n \nstatic bool alloc_covered_contains(u32 alloc_stack_hash)\n{\n\tint i;\n\n\tfor (i = 0; i < ALLOC_COVERED_HNUM; i++) {\n\t\tif (!atomic_read(&alloc_covered[alloc_stack_hash & ALLOC_COVERED_MASK]))\n\t\t\treturn false;\n\t\talloc_stack_hash = ALLOC_COVERED_HNEXT(alloc_stack_hash);\n\t}\n\n\treturn true;\n}\n\nstatic bool kfence_protect(unsigned long addr)\n{\n\treturn !KFENCE_WARN_ON(!kfence_protect_page(ALIGN_DOWN(addr, PAGE_SIZE), true));\n}\n\nstatic bool kfence_unprotect(unsigned long addr)\n{\n\treturn !KFENCE_WARN_ON(!kfence_protect_page(ALIGN_DOWN(addr, PAGE_SIZE), false));\n}\n\nstatic inline unsigned long metadata_to_pageaddr(const struct kfence_metadata *meta)\n{\n\tunsigned long offset = (meta - kfence_metadata + 1) * PAGE_SIZE * 2;\n\tunsigned long pageaddr = (unsigned long)&__kfence_pool[offset];\n\n\t \n\n\t \n\tif (KFENCE_WARN_ON(meta < kfence_metadata ||\n\t\t\t   meta >= kfence_metadata + CONFIG_KFENCE_NUM_OBJECTS))\n\t\treturn 0;\n\n\t \n\tif (KFENCE_WARN_ON(ALIGN_DOWN(meta->addr, PAGE_SIZE) != pageaddr))\n\t\treturn 0;\n\n\treturn pageaddr;\n}\n\n \nstatic noinline void\nmetadata_update_state(struct kfence_metadata *meta, enum kfence_object_state next,\n\t\t      unsigned long *stack_entries, size_t num_stack_entries)\n{\n\tstruct kfence_track *track =\n\t\tnext == KFENCE_OBJECT_FREED ? &meta->free_track : &meta->alloc_track;\n\n\tlockdep_assert_held(&meta->lock);\n\n\tif (stack_entries) {\n\t\tmemcpy(track->stack_entries, stack_entries,\n\t\t       num_stack_entries * sizeof(stack_entries[0]));\n\t} else {\n\t\t \n\t\tnum_stack_entries = stack_trace_save(track->stack_entries, KFENCE_STACK_DEPTH, 1);\n\t}\n\ttrack->num_stack_entries = num_stack_entries;\n\ttrack->pid = task_pid_nr(current);\n\ttrack->cpu = raw_smp_processor_id();\n\ttrack->ts_nsec = local_clock();  \n\n\t \n\tWRITE_ONCE(meta->state, next);\n}\n\n \nstatic inline bool check_canary_byte(u8 *addr)\n{\n\tstruct kfence_metadata *meta;\n\tunsigned long flags;\n\n\tif (likely(*addr == KFENCE_CANARY_PATTERN_U8(addr)))\n\t\treturn true;\n\n\tatomic_long_inc(&counters[KFENCE_COUNTER_BUGS]);\n\n\tmeta = addr_to_metadata((unsigned long)addr);\n\traw_spin_lock_irqsave(&meta->lock, flags);\n\tkfence_report_error((unsigned long)addr, false, NULL, meta, KFENCE_ERROR_CORRUPTION);\n\traw_spin_unlock_irqrestore(&meta->lock, flags);\n\n\treturn false;\n}\n\nstatic inline void set_canary(const struct kfence_metadata *meta)\n{\n\tconst unsigned long pageaddr = ALIGN_DOWN(meta->addr, PAGE_SIZE);\n\tunsigned long addr = pageaddr;\n\n\t \n\tfor (; addr < meta->addr; addr += sizeof(u64))\n\t\t*((u64 *)addr) = KFENCE_CANARY_PATTERN_U64;\n\n\taddr = ALIGN_DOWN(meta->addr + meta->size, sizeof(u64));\n\tfor (; addr - pageaddr < PAGE_SIZE; addr += sizeof(u64))\n\t\t*((u64 *)addr) = KFENCE_CANARY_PATTERN_U64;\n}\n\nstatic inline void check_canary(const struct kfence_metadata *meta)\n{\n\tconst unsigned long pageaddr = ALIGN_DOWN(meta->addr, PAGE_SIZE);\n\tunsigned long addr = pageaddr;\n\n\t \n\n\t \n\tfor (; meta->addr - addr >= sizeof(u64); addr += sizeof(u64)) {\n\t\tif (unlikely(*((u64 *)addr) != KFENCE_CANARY_PATTERN_U64))\n\t\t\tbreak;\n\t}\n\n\t \n\tfor (; addr < meta->addr; addr++) {\n\t\tif (unlikely(!check_canary_byte((u8 *)addr)))\n\t\t\tbreak;\n\t}\n\n\t \n\tfor (addr = meta->addr + meta->size; addr % sizeof(u64) != 0; addr++) {\n\t\tif (unlikely(!check_canary_byte((u8 *)addr)))\n\t\t\treturn;\n\t}\n\tfor (; addr - pageaddr < PAGE_SIZE; addr += sizeof(u64)) {\n\t\tif (unlikely(*((u64 *)addr) != KFENCE_CANARY_PATTERN_U64)) {\n\n\t\t\tfor (; addr - pageaddr < PAGE_SIZE; addr++) {\n\t\t\t\tif (!check_canary_byte((u8 *)addr))\n\t\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic void *kfence_guarded_alloc(struct kmem_cache *cache, size_t size, gfp_t gfp,\n\t\t\t\t  unsigned long *stack_entries, size_t num_stack_entries,\n\t\t\t\t  u32 alloc_stack_hash)\n{\n\tstruct kfence_metadata *meta = NULL;\n\tunsigned long flags;\n\tstruct slab *slab;\n\tvoid *addr;\n\tconst bool random_right_allocate = get_random_u32_below(2);\n\tconst bool random_fault = CONFIG_KFENCE_STRESS_TEST_FAULTS &&\n\t\t\t\t  !get_random_u32_below(CONFIG_KFENCE_STRESS_TEST_FAULTS);\n\n\t \n\traw_spin_lock_irqsave(&kfence_freelist_lock, flags);\n\tif (!list_empty(&kfence_freelist)) {\n\t\tmeta = list_entry(kfence_freelist.next, struct kfence_metadata, list);\n\t\tlist_del_init(&meta->list);\n\t}\n\traw_spin_unlock_irqrestore(&kfence_freelist_lock, flags);\n\tif (!meta) {\n\t\tatomic_long_inc(&counters[KFENCE_COUNTER_SKIP_CAPACITY]);\n\t\treturn NULL;\n\t}\n\n\tif (unlikely(!raw_spin_trylock_irqsave(&meta->lock, flags))) {\n\t\t \n\t\traw_spin_lock_irqsave(&kfence_freelist_lock, flags);\n\t\t \n\t\tlist_add_tail(&meta->list, &kfence_freelist);\n\t\traw_spin_unlock_irqrestore(&kfence_freelist_lock, flags);\n\n\t\treturn NULL;\n\t}\n\n\tmeta->addr = metadata_to_pageaddr(meta);\n\t \n\tif (meta->state == KFENCE_OBJECT_FREED)\n\t\tkfence_unprotect(meta->addr);\n\n\t \n\tif (random_right_allocate) {\n\t\t \n\t\tmeta->addr += PAGE_SIZE - size;\n\t\tmeta->addr = ALIGN_DOWN(meta->addr, cache->align);\n\t}\n\n\taddr = (void *)meta->addr;\n\n\t \n\tmetadata_update_state(meta, KFENCE_OBJECT_ALLOCATED, stack_entries, num_stack_entries);\n\t \n\tWRITE_ONCE(meta->cache, cache);\n\tmeta->size = size;\n\tmeta->alloc_stack_hash = alloc_stack_hash;\n\traw_spin_unlock_irqrestore(&meta->lock, flags);\n\n\talloc_covered_add(alloc_stack_hash, 1);\n\n\t \n\tslab = virt_to_slab((void *)meta->addr);\n\tslab->slab_cache = cache;\n#if defined(CONFIG_SLUB)\n\tslab->objects = 1;\n#elif defined(CONFIG_SLAB)\n\tslab->s_mem = addr;\n#endif\n\n\t \n\tset_canary(meta);\n\n\t \n\tif (unlikely(slab_want_init_on_alloc(gfp, cache)))\n\t\tmemzero_explicit(addr, size);\n\tif (cache->ctor)\n\t\tcache->ctor(addr);\n\n\tif (random_fault)\n\t\tkfence_protect(meta->addr);  \n\n\tatomic_long_inc(&counters[KFENCE_COUNTER_ALLOCATED]);\n\tatomic_long_inc(&counters[KFENCE_COUNTER_ALLOCS]);\n\n\treturn addr;\n}\n\nstatic void kfence_guarded_free(void *addr, struct kfence_metadata *meta, bool zombie)\n{\n\tstruct kcsan_scoped_access assert_page_exclusive;\n\tunsigned long flags;\n\tbool init;\n\n\traw_spin_lock_irqsave(&meta->lock, flags);\n\n\tif (meta->state != KFENCE_OBJECT_ALLOCATED || meta->addr != (unsigned long)addr) {\n\t\t \n\t\tatomic_long_inc(&counters[KFENCE_COUNTER_BUGS]);\n\t\tkfence_report_error((unsigned long)addr, false, NULL, meta,\n\t\t\t\t    KFENCE_ERROR_INVALID_FREE);\n\t\traw_spin_unlock_irqrestore(&meta->lock, flags);\n\t\treturn;\n\t}\n\n\t \n\tkcsan_begin_scoped_access((void *)ALIGN_DOWN((unsigned long)addr, PAGE_SIZE), PAGE_SIZE,\n\t\t\t\t  KCSAN_ACCESS_SCOPED | KCSAN_ACCESS_WRITE | KCSAN_ACCESS_ASSERT,\n\t\t\t\t  &assert_page_exclusive);\n\n\tif (CONFIG_KFENCE_STRESS_TEST_FAULTS)\n\t\tkfence_unprotect((unsigned long)addr);  \n\n\t \n\tif (meta->unprotected_page) {\n\t\tmemzero_explicit((void *)ALIGN_DOWN(meta->unprotected_page, PAGE_SIZE), PAGE_SIZE);\n\t\tkfence_protect(meta->unprotected_page);\n\t\tmeta->unprotected_page = 0;\n\t}\n\n\t \n\tmetadata_update_state(meta, KFENCE_OBJECT_FREED, NULL, 0);\n\tinit = slab_want_init_on_free(meta->cache);\n\traw_spin_unlock_irqrestore(&meta->lock, flags);\n\n\talloc_covered_add(meta->alloc_stack_hash, -1);\n\n\t \n\tcheck_canary(meta);\n\n\t \n\tif (!zombie && unlikely(init))\n\t\tmemzero_explicit(addr, meta->size);\n\n\t \n\tkfence_protect((unsigned long)addr);\n\n\tkcsan_end_scoped_access(&assert_page_exclusive);\n\tif (!zombie) {\n\t\t \n\t\traw_spin_lock_irqsave(&kfence_freelist_lock, flags);\n\t\tKFENCE_WARN_ON(!list_empty(&meta->list));\n\t\tlist_add_tail(&meta->list, &kfence_freelist);\n\t\traw_spin_unlock_irqrestore(&kfence_freelist_lock, flags);\n\n\t\tatomic_long_dec(&counters[KFENCE_COUNTER_ALLOCATED]);\n\t\tatomic_long_inc(&counters[KFENCE_COUNTER_FREES]);\n\t} else {\n\t\t \n\t\tatomic_long_inc(&counters[KFENCE_COUNTER_ZOMBIES]);\n\t}\n}\n\nstatic void rcu_guarded_free(struct rcu_head *h)\n{\n\tstruct kfence_metadata *meta = container_of(h, struct kfence_metadata, rcu_head);\n\n\tkfence_guarded_free((void *)meta->addr, meta, false);\n}\n\n \nstatic unsigned long kfence_init_pool(void)\n{\n\tunsigned long addr;\n\tstruct page *pages;\n\tint i;\n\n\tif (!arch_kfence_init_pool())\n\t\treturn (unsigned long)__kfence_pool;\n\n\taddr = (unsigned long)__kfence_pool;\n\tpages = virt_to_page(__kfence_pool);\n\n\t \n\tfor (i = 0; i < KFENCE_POOL_SIZE / PAGE_SIZE; i++) {\n\t\tstruct slab *slab = page_slab(nth_page(pages, i));\n\n\t\tif (!i || (i % 2))\n\t\t\tcontinue;\n\n\t\t__folio_set_slab(slab_folio(slab));\n#ifdef CONFIG_MEMCG\n\t\tslab->memcg_data = (unsigned long)&kfence_metadata_init[i / 2 - 1].objcg |\n\t\t\t\t   MEMCG_DATA_OBJCGS;\n#endif\n\t}\n\n\t \n\tfor (i = 0; i < 2; i++) {\n\t\tif (unlikely(!kfence_protect(addr)))\n\t\t\treturn addr;\n\n\t\taddr += PAGE_SIZE;\n\t}\n\n\tfor (i = 0; i < CONFIG_KFENCE_NUM_OBJECTS; i++) {\n\t\tstruct kfence_metadata *meta = &kfence_metadata_init[i];\n\n\t\t \n\t\tINIT_LIST_HEAD(&meta->list);\n\t\traw_spin_lock_init(&meta->lock);\n\t\tmeta->state = KFENCE_OBJECT_UNUSED;\n\t\tmeta->addr = addr;  \n\t\tlist_add_tail(&meta->list, &kfence_freelist);\n\n\t\t \n\t\tif (unlikely(!kfence_protect(addr + PAGE_SIZE)))\n\t\t\tgoto reset_slab;\n\n\t\taddr += 2 * PAGE_SIZE;\n\t}\n\n\t \n\tsmp_store_release(&kfence_metadata, kfence_metadata_init);\n\treturn 0;\n\nreset_slab:\n\tfor (i = 0; i < KFENCE_POOL_SIZE / PAGE_SIZE; i++) {\n\t\tstruct slab *slab = page_slab(nth_page(pages, i));\n\n\t\tif (!i || (i % 2))\n\t\t\tcontinue;\n#ifdef CONFIG_MEMCG\n\t\tslab->memcg_data = 0;\n#endif\n\t\t__folio_clear_slab(slab_folio(slab));\n\t}\n\n\treturn addr;\n}\n\nstatic bool __init kfence_init_pool_early(void)\n{\n\tunsigned long addr;\n\n\tif (!__kfence_pool)\n\t\treturn false;\n\n\taddr = kfence_init_pool();\n\n\tif (!addr) {\n\t\t \n\t\tkmemleak_ignore_phys(__pa(__kfence_pool));\n\t\treturn true;\n\t}\n\n\t \n\tmemblock_free_late(__pa(addr), KFENCE_POOL_SIZE - (addr - (unsigned long)__kfence_pool));\n\t__kfence_pool = NULL;\n\n\tmemblock_free_late(__pa(kfence_metadata_init), KFENCE_METADATA_SIZE);\n\tkfence_metadata_init = NULL;\n\n\treturn false;\n}\n\n \n\nstatic int stats_show(struct seq_file *seq, void *v)\n{\n\tint i;\n\n\tseq_printf(seq, \"enabled: %i\\n\", READ_ONCE(kfence_enabled));\n\tfor (i = 0; i < KFENCE_COUNTER_COUNT; i++)\n\t\tseq_printf(seq, \"%s: %ld\\n\", counter_names[i], atomic_long_read(&counters[i]));\n\n\treturn 0;\n}\nDEFINE_SHOW_ATTRIBUTE(stats);\n\n \nstatic void *start_object(struct seq_file *seq, loff_t *pos)\n{\n\tif (*pos < CONFIG_KFENCE_NUM_OBJECTS)\n\t\treturn (void *)((long)*pos + 1);\n\treturn NULL;\n}\n\nstatic void stop_object(struct seq_file *seq, void *v)\n{\n}\n\nstatic void *next_object(struct seq_file *seq, void *v, loff_t *pos)\n{\n\t++*pos;\n\tif (*pos < CONFIG_KFENCE_NUM_OBJECTS)\n\t\treturn (void *)((long)*pos + 1);\n\treturn NULL;\n}\n\nstatic int show_object(struct seq_file *seq, void *v)\n{\n\tstruct kfence_metadata *meta = &kfence_metadata[(long)v - 1];\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&meta->lock, flags);\n\tkfence_print_object(seq, meta);\n\traw_spin_unlock_irqrestore(&meta->lock, flags);\n\tseq_puts(seq, \"---------------------------------\\n\");\n\n\treturn 0;\n}\n\nstatic const struct seq_operations objects_sops = {\n\t.start = start_object,\n\t.next = next_object,\n\t.stop = stop_object,\n\t.show = show_object,\n};\nDEFINE_SEQ_ATTRIBUTE(objects);\n\nstatic int kfence_debugfs_init(void)\n{\n\tstruct dentry *kfence_dir;\n\n\tif (!READ_ONCE(kfence_enabled))\n\t\treturn 0;\n\n\tkfence_dir = debugfs_create_dir(\"kfence\", NULL);\n\tdebugfs_create_file(\"stats\", 0444, kfence_dir, NULL, &stats_fops);\n\tdebugfs_create_file(\"objects\", 0400, kfence_dir, NULL, &objects_fops);\n\treturn 0;\n}\n\nlate_initcall(kfence_debugfs_init);\n\n \n\nstatic void kfence_check_all_canary(void)\n{\n\tint i;\n\n\tfor (i = 0; i < CONFIG_KFENCE_NUM_OBJECTS; i++) {\n\t\tstruct kfence_metadata *meta = &kfence_metadata[i];\n\n\t\tif (meta->state == KFENCE_OBJECT_ALLOCATED)\n\t\t\tcheck_canary(meta);\n\t}\n}\n\nstatic int kfence_check_canary_callback(struct notifier_block *nb,\n\t\t\t\t\tunsigned long reason, void *arg)\n{\n\tkfence_check_all_canary();\n\treturn NOTIFY_OK;\n}\n\nstatic struct notifier_block kfence_check_canary_notifier = {\n\t.notifier_call = kfence_check_canary_callback,\n};\n\n \n\nstatic struct delayed_work kfence_timer;\n\n#ifdef CONFIG_KFENCE_STATIC_KEYS\n \nstatic DECLARE_WAIT_QUEUE_HEAD(allocation_wait);\n\nstatic void wake_up_kfence_timer(struct irq_work *work)\n{\n\twake_up(&allocation_wait);\n}\nstatic DEFINE_IRQ_WORK(wake_up_kfence_timer_work, wake_up_kfence_timer);\n#endif\n\n \nstatic void toggle_allocation_gate(struct work_struct *work)\n{\n\tif (!READ_ONCE(kfence_enabled))\n\t\treturn;\n\n\tatomic_set(&kfence_allocation_gate, 0);\n#ifdef CONFIG_KFENCE_STATIC_KEYS\n\t \n\tstatic_branch_enable(&kfence_allocation_key);\n\n\twait_event_idle(allocation_wait, atomic_read(&kfence_allocation_gate));\n\n\t \n\tstatic_branch_disable(&kfence_allocation_key);\n#endif\n\tqueue_delayed_work(system_unbound_wq, &kfence_timer,\n\t\t\t   msecs_to_jiffies(kfence_sample_interval));\n}\n\n \n\nvoid __init kfence_alloc_pool_and_metadata(void)\n{\n\tif (!kfence_sample_interval)\n\t\treturn;\n\n\t \n\tif (!__kfence_pool)\n\t\t__kfence_pool = memblock_alloc(KFENCE_POOL_SIZE, PAGE_SIZE);\n\n\tif (!__kfence_pool) {\n\t\tpr_err(\"failed to allocate pool\\n\");\n\t\treturn;\n\t}\n\n\t \n\tkfence_metadata_init = memblock_alloc(KFENCE_METADATA_SIZE, PAGE_SIZE);\n\tif (!kfence_metadata_init) {\n\t\tpr_err(\"failed to allocate metadata\\n\");\n\t\tmemblock_free(__kfence_pool, KFENCE_POOL_SIZE);\n\t\t__kfence_pool = NULL;\n\t}\n}\n\nstatic void kfence_init_enable(void)\n{\n\tif (!IS_ENABLED(CONFIG_KFENCE_STATIC_KEYS))\n\t\tstatic_branch_enable(&kfence_allocation_key);\n\n\tif (kfence_deferrable)\n\t\tINIT_DEFERRABLE_WORK(&kfence_timer, toggle_allocation_gate);\n\telse\n\t\tINIT_DELAYED_WORK(&kfence_timer, toggle_allocation_gate);\n\n\tif (kfence_check_on_panic)\n\t\tatomic_notifier_chain_register(&panic_notifier_list, &kfence_check_canary_notifier);\n\n\tWRITE_ONCE(kfence_enabled, true);\n\tqueue_delayed_work(system_unbound_wq, &kfence_timer, 0);\n\n\tpr_info(\"initialized - using %lu bytes for %d objects at 0x%p-0x%p\\n\", KFENCE_POOL_SIZE,\n\t\tCONFIG_KFENCE_NUM_OBJECTS, (void *)__kfence_pool,\n\t\t(void *)(__kfence_pool + KFENCE_POOL_SIZE));\n}\n\nvoid __init kfence_init(void)\n{\n\tstack_hash_seed = get_random_u32();\n\n\t \n\tif (!kfence_sample_interval)\n\t\treturn;\n\n\tif (!kfence_init_pool_early()) {\n\t\tpr_err(\"%s failed\\n\", __func__);\n\t\treturn;\n\t}\n\n\tkfence_init_enable();\n}\n\nstatic int kfence_init_late(void)\n{\n\tconst unsigned long nr_pages_pool = KFENCE_POOL_SIZE / PAGE_SIZE;\n\tconst unsigned long nr_pages_meta = KFENCE_METADATA_SIZE / PAGE_SIZE;\n\tunsigned long addr = (unsigned long)__kfence_pool;\n\tunsigned long free_size = KFENCE_POOL_SIZE;\n\tint err = -ENOMEM;\n\n#ifdef CONFIG_CONTIG_ALLOC\n\tstruct page *pages;\n\n\tpages = alloc_contig_pages(nr_pages_pool, GFP_KERNEL, first_online_node,\n\t\t\t\t   NULL);\n\tif (!pages)\n\t\treturn -ENOMEM;\n\n\t__kfence_pool = page_to_virt(pages);\n\tpages = alloc_contig_pages(nr_pages_meta, GFP_KERNEL, first_online_node,\n\t\t\t\t   NULL);\n\tif (pages)\n\t\tkfence_metadata_init = page_to_virt(pages);\n#else\n\tif (nr_pages_pool > MAX_ORDER_NR_PAGES ||\n\t    nr_pages_meta > MAX_ORDER_NR_PAGES) {\n\t\tpr_warn(\"KFENCE_NUM_OBJECTS too large for buddy allocator\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t__kfence_pool = alloc_pages_exact(KFENCE_POOL_SIZE, GFP_KERNEL);\n\tif (!__kfence_pool)\n\t\treturn -ENOMEM;\n\n\tkfence_metadata_init = alloc_pages_exact(KFENCE_METADATA_SIZE, GFP_KERNEL);\n#endif\n\n\tif (!kfence_metadata_init)\n\t\tgoto free_pool;\n\n\tmemzero_explicit(kfence_metadata_init, KFENCE_METADATA_SIZE);\n\taddr = kfence_init_pool();\n\tif (!addr) {\n\t\tkfence_init_enable();\n\t\tkfence_debugfs_init();\n\t\treturn 0;\n\t}\n\n\tpr_err(\"%s failed\\n\", __func__);\n\tfree_size = KFENCE_POOL_SIZE - (addr - (unsigned long)__kfence_pool);\n\terr = -EBUSY;\n\n#ifdef CONFIG_CONTIG_ALLOC\n\tfree_contig_range(page_to_pfn(virt_to_page((void *)kfence_metadata_init)),\n\t\t\t  nr_pages_meta);\nfree_pool:\n\tfree_contig_range(page_to_pfn(virt_to_page((void *)addr)),\n\t\t\t  free_size / PAGE_SIZE);\n#else\n\tfree_pages_exact((void *)kfence_metadata_init, KFENCE_METADATA_SIZE);\nfree_pool:\n\tfree_pages_exact((void *)addr, free_size);\n#endif\n\n\tkfence_metadata_init = NULL;\n\t__kfence_pool = NULL;\n\treturn err;\n}\n\nstatic int kfence_enable_late(void)\n{\n\tif (!__kfence_pool)\n\t\treturn kfence_init_late();\n\n\tWRITE_ONCE(kfence_enabled, true);\n\tqueue_delayed_work(system_unbound_wq, &kfence_timer, 0);\n\tpr_info(\"re-enabled\\n\");\n\treturn 0;\n}\n\nvoid kfence_shutdown_cache(struct kmem_cache *s)\n{\n\tunsigned long flags;\n\tstruct kfence_metadata *meta;\n\tint i;\n\n\t \n\tif (!smp_load_acquire(&kfence_metadata))\n\t\treturn;\n\n\tfor (i = 0; i < CONFIG_KFENCE_NUM_OBJECTS; i++) {\n\t\tbool in_use;\n\n\t\tmeta = &kfence_metadata[i];\n\n\t\t \n\t\tif (READ_ONCE(meta->cache) != s ||\n\t\t    READ_ONCE(meta->state) != KFENCE_OBJECT_ALLOCATED)\n\t\t\tcontinue;\n\n\t\traw_spin_lock_irqsave(&meta->lock, flags);\n\t\tin_use = meta->cache == s && meta->state == KFENCE_OBJECT_ALLOCATED;\n\t\traw_spin_unlock_irqrestore(&meta->lock, flags);\n\n\t\tif (in_use) {\n\t\t\t \n\t\t\tkfence_guarded_free((void *)meta->addr, meta,  true);\n\t\t}\n\t}\n\n\tfor (i = 0; i < CONFIG_KFENCE_NUM_OBJECTS; i++) {\n\t\tmeta = &kfence_metadata[i];\n\n\t\t \n\t\tif (READ_ONCE(meta->cache) != s || READ_ONCE(meta->state) != KFENCE_OBJECT_FREED)\n\t\t\tcontinue;\n\n\t\traw_spin_lock_irqsave(&meta->lock, flags);\n\t\tif (meta->cache == s && meta->state == KFENCE_OBJECT_FREED)\n\t\t\tmeta->cache = NULL;\n\t\traw_spin_unlock_irqrestore(&meta->lock, flags);\n\t}\n}\n\nvoid *__kfence_alloc(struct kmem_cache *s, size_t size, gfp_t flags)\n{\n\tunsigned long stack_entries[KFENCE_STACK_DEPTH];\n\tsize_t num_stack_entries;\n\tu32 alloc_stack_hash;\n\n\t \n\tif (size > PAGE_SIZE) {\n\t\tatomic_long_inc(&counters[KFENCE_COUNTER_SKIP_INCOMPAT]);\n\t\treturn NULL;\n\t}\n\n\t \n\tif ((flags & GFP_ZONEMASK) ||\n\t    (s->flags & (SLAB_CACHE_DMA | SLAB_CACHE_DMA32))) {\n\t\tatomic_long_inc(&counters[KFENCE_COUNTER_SKIP_INCOMPAT]);\n\t\treturn NULL;\n\t}\n\n\t \n\tif (s->flags & SLAB_SKIP_KFENCE)\n\t\treturn NULL;\n\n\tif (atomic_inc_return(&kfence_allocation_gate) > 1)\n\t\treturn NULL;\n#ifdef CONFIG_KFENCE_STATIC_KEYS\n\t \n\tif (waitqueue_active(&allocation_wait)) {\n\t\t \n\t\tirq_work_queue(&wake_up_kfence_timer_work);\n\t}\n#endif\n\n\tif (!READ_ONCE(kfence_enabled))\n\t\treturn NULL;\n\n\tnum_stack_entries = stack_trace_save(stack_entries, KFENCE_STACK_DEPTH, 0);\n\n\t \n\talloc_stack_hash = get_alloc_stack_hash(stack_entries, num_stack_entries);\n\tif (should_skip_covered() && alloc_covered_contains(alloc_stack_hash)) {\n\t\tatomic_long_inc(&counters[KFENCE_COUNTER_SKIP_COVERED]);\n\t\treturn NULL;\n\t}\n\n\treturn kfence_guarded_alloc(s, size, flags, stack_entries, num_stack_entries,\n\t\t\t\t    alloc_stack_hash);\n}\n\nsize_t kfence_ksize(const void *addr)\n{\n\tconst struct kfence_metadata *meta = addr_to_metadata((unsigned long)addr);\n\n\t \n\treturn meta ? meta->size : 0;\n}\n\nvoid *kfence_object_start(const void *addr)\n{\n\tconst struct kfence_metadata *meta = addr_to_metadata((unsigned long)addr);\n\n\t \n\treturn meta ? (void *)meta->addr : NULL;\n}\n\nvoid __kfence_free(void *addr)\n{\n\tstruct kfence_metadata *meta = addr_to_metadata((unsigned long)addr);\n\n#ifdef CONFIG_MEMCG\n\tKFENCE_WARN_ON(meta->objcg);\n#endif\n\t \n\tif (unlikely(meta->cache && (meta->cache->flags & SLAB_TYPESAFE_BY_RCU)))\n\t\tcall_rcu(&meta->rcu_head, rcu_guarded_free);\n\telse\n\t\tkfence_guarded_free(addr, meta, false);\n}\n\nbool kfence_handle_page_fault(unsigned long addr, bool is_write, struct pt_regs *regs)\n{\n\tconst int page_index = (addr - (unsigned long)__kfence_pool) / PAGE_SIZE;\n\tstruct kfence_metadata *to_report = NULL;\n\tenum kfence_error_type error_type;\n\tunsigned long flags;\n\n\tif (!is_kfence_address((void *)addr))\n\t\treturn false;\n\n\tif (!READ_ONCE(kfence_enabled))  \n\t\treturn kfence_unprotect(addr);  \n\n\tatomic_long_inc(&counters[KFENCE_COUNTER_BUGS]);\n\n\tif (page_index % 2) {\n\t\t \n\t\tstruct kfence_metadata *meta;\n\t\tint distance = 0;\n\n\t\tmeta = addr_to_metadata(addr - PAGE_SIZE);\n\t\tif (meta && READ_ONCE(meta->state) == KFENCE_OBJECT_ALLOCATED) {\n\t\t\tto_report = meta;\n\t\t\t \n\t\t\tdistance = addr - data_race(meta->addr + meta->size);\n\t\t}\n\n\t\tmeta = addr_to_metadata(addr + PAGE_SIZE);\n\t\tif (meta && READ_ONCE(meta->state) == KFENCE_OBJECT_ALLOCATED) {\n\t\t\t \n\t\t\tif (!to_report || distance > data_race(meta->addr) - addr)\n\t\t\t\tto_report = meta;\n\t\t}\n\n\t\tif (!to_report)\n\t\t\tgoto out;\n\n\t\traw_spin_lock_irqsave(&to_report->lock, flags);\n\t\tto_report->unprotected_page = addr;\n\t\terror_type = KFENCE_ERROR_OOB;\n\n\t\t \n\t} else {\n\t\tto_report = addr_to_metadata(addr);\n\t\tif (!to_report)\n\t\t\tgoto out;\n\n\t\traw_spin_lock_irqsave(&to_report->lock, flags);\n\t\terror_type = KFENCE_ERROR_UAF;\n\t\t \n\t}\n\nout:\n\tif (to_report) {\n\t\tkfence_report_error(addr, is_write, regs, to_report, error_type);\n\t\traw_spin_unlock_irqrestore(&to_report->lock, flags);\n\t} else {\n\t\t \n\t\tkfence_report_error(addr, is_write, regs, NULL, KFENCE_ERROR_INVALID);\n\t}\n\n\treturn kfence_unprotect(addr);  \n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}