{
  "module_name": "mmu_notifier.c",
  "hash_id": "bdc4965003eccc0c6fd30f5bb86502ff7f53cfc542bf1a22af45b94f9aaafc61",
  "original_prompt": "Ingested from linux-6.6.14/mm/mmu_notifier.c",
  "human_readable_source": "\n \n\n#include <linux/rculist.h>\n#include <linux/mmu_notifier.h>\n#include <linux/export.h>\n#include <linux/mm.h>\n#include <linux/err.h>\n#include <linux/interval_tree.h>\n#include <linux/srcu.h>\n#include <linux/rcupdate.h>\n#include <linux/sched.h>\n#include <linux/sched/mm.h>\n#include <linux/slab.h>\n\n \nDEFINE_STATIC_SRCU(srcu);\n\n#ifdef CONFIG_LOCKDEP\nstruct lockdep_map __mmu_notifier_invalidate_range_start_map = {\n\t.name = \"mmu_notifier_invalidate_range_start\"\n};\n#endif\n\n \nstruct mmu_notifier_subscriptions {\n\t \n\tstruct hlist_head list;\n\tbool has_itree;\n\t \n\tspinlock_t lock;\n\tunsigned long invalidate_seq;\n\tunsigned long active_invalidate_ranges;\n\tstruct rb_root_cached itree;\n\twait_queue_head_t wq;\n\tstruct hlist_head deferred_list;\n};\n\n \nstatic bool\nmn_itree_is_invalidating(struct mmu_notifier_subscriptions *subscriptions)\n{\n\tlockdep_assert_held(&subscriptions->lock);\n\treturn subscriptions->invalidate_seq & 1;\n}\n\nstatic struct mmu_interval_notifier *\nmn_itree_inv_start_range(struct mmu_notifier_subscriptions *subscriptions,\n\t\t\t const struct mmu_notifier_range *range,\n\t\t\t unsigned long *seq)\n{\n\tstruct interval_tree_node *node;\n\tstruct mmu_interval_notifier *res = NULL;\n\n\tspin_lock(&subscriptions->lock);\n\tsubscriptions->active_invalidate_ranges++;\n\tnode = interval_tree_iter_first(&subscriptions->itree, range->start,\n\t\t\t\t\trange->end - 1);\n\tif (node) {\n\t\tsubscriptions->invalidate_seq |= 1;\n\t\tres = container_of(node, struct mmu_interval_notifier,\n\t\t\t\t   interval_tree);\n\t}\n\n\t*seq = subscriptions->invalidate_seq;\n\tspin_unlock(&subscriptions->lock);\n\treturn res;\n}\n\nstatic struct mmu_interval_notifier *\nmn_itree_inv_next(struct mmu_interval_notifier *interval_sub,\n\t\t  const struct mmu_notifier_range *range)\n{\n\tstruct interval_tree_node *node;\n\n\tnode = interval_tree_iter_next(&interval_sub->interval_tree,\n\t\t\t\t       range->start, range->end - 1);\n\tif (!node)\n\t\treturn NULL;\n\treturn container_of(node, struct mmu_interval_notifier, interval_tree);\n}\n\nstatic void mn_itree_inv_end(struct mmu_notifier_subscriptions *subscriptions)\n{\n\tstruct mmu_interval_notifier *interval_sub;\n\tstruct hlist_node *next;\n\n\tspin_lock(&subscriptions->lock);\n\tif (--subscriptions->active_invalidate_ranges ||\n\t    !mn_itree_is_invalidating(subscriptions)) {\n\t\tspin_unlock(&subscriptions->lock);\n\t\treturn;\n\t}\n\n\t \n\tsubscriptions->invalidate_seq++;\n\n\t \n\thlist_for_each_entry_safe(interval_sub, next,\n\t\t\t\t  &subscriptions->deferred_list,\n\t\t\t\t  deferred_item) {\n\t\tif (RB_EMPTY_NODE(&interval_sub->interval_tree.rb))\n\t\t\tinterval_tree_insert(&interval_sub->interval_tree,\n\t\t\t\t\t     &subscriptions->itree);\n\t\telse\n\t\t\tinterval_tree_remove(&interval_sub->interval_tree,\n\t\t\t\t\t     &subscriptions->itree);\n\t\thlist_del(&interval_sub->deferred_item);\n\t}\n\tspin_unlock(&subscriptions->lock);\n\n\twake_up_all(&subscriptions->wq);\n}\n\n \nunsigned long\nmmu_interval_read_begin(struct mmu_interval_notifier *interval_sub)\n{\n\tstruct mmu_notifier_subscriptions *subscriptions =\n\t\tinterval_sub->mm->notifier_subscriptions;\n\tunsigned long seq;\n\tbool is_invalidating;\n\n\t \n\tspin_lock(&subscriptions->lock);\n\t \n\tseq = READ_ONCE(interval_sub->invalidate_seq);\n\tis_invalidating = seq == subscriptions->invalidate_seq;\n\tspin_unlock(&subscriptions->lock);\n\n\t \n\tlock_map_acquire(&__mmu_notifier_invalidate_range_start_map);\n\tlock_map_release(&__mmu_notifier_invalidate_range_start_map);\n\tif (is_invalidating)\n\t\twait_event(subscriptions->wq,\n\t\t\t   READ_ONCE(subscriptions->invalidate_seq) != seq);\n\n\t \n\n\treturn seq;\n}\nEXPORT_SYMBOL_GPL(mmu_interval_read_begin);\n\nstatic void mn_itree_release(struct mmu_notifier_subscriptions *subscriptions,\n\t\t\t     struct mm_struct *mm)\n{\n\tstruct mmu_notifier_range range = {\n\t\t.flags = MMU_NOTIFIER_RANGE_BLOCKABLE,\n\t\t.event = MMU_NOTIFY_RELEASE,\n\t\t.mm = mm,\n\t\t.start = 0,\n\t\t.end = ULONG_MAX,\n\t};\n\tstruct mmu_interval_notifier *interval_sub;\n\tunsigned long cur_seq;\n\tbool ret;\n\n\tfor (interval_sub =\n\t\t     mn_itree_inv_start_range(subscriptions, &range, &cur_seq);\n\t     interval_sub;\n\t     interval_sub = mn_itree_inv_next(interval_sub, &range)) {\n\t\tret = interval_sub->ops->invalidate(interval_sub, &range,\n\t\t\t\t\t\t    cur_seq);\n\t\tWARN_ON(!ret);\n\t}\n\n\tmn_itree_inv_end(subscriptions);\n}\n\n \nstatic void mn_hlist_release(struct mmu_notifier_subscriptions *subscriptions,\n\t\t\t     struct mm_struct *mm)\n{\n\tstruct mmu_notifier *subscription;\n\tint id;\n\n\t \n\tid = srcu_read_lock(&srcu);\n\thlist_for_each_entry_rcu(subscription, &subscriptions->list, hlist,\n\t\t\t\t srcu_read_lock_held(&srcu))\n\t\t \n\t\tif (subscription->ops->release)\n\t\t\tsubscription->ops->release(subscription, mm);\n\n\tspin_lock(&subscriptions->lock);\n\twhile (unlikely(!hlist_empty(&subscriptions->list))) {\n\t\tsubscription = hlist_entry(subscriptions->list.first,\n\t\t\t\t\t   struct mmu_notifier, hlist);\n\t\t \n\t\thlist_del_init_rcu(&subscription->hlist);\n\t}\n\tspin_unlock(&subscriptions->lock);\n\tsrcu_read_unlock(&srcu, id);\n\n\t \n\tsynchronize_srcu(&srcu);\n}\n\nvoid __mmu_notifier_release(struct mm_struct *mm)\n{\n\tstruct mmu_notifier_subscriptions *subscriptions =\n\t\tmm->notifier_subscriptions;\n\n\tif (subscriptions->has_itree)\n\t\tmn_itree_release(subscriptions, mm);\n\n\tif (!hlist_empty(&subscriptions->list))\n\t\tmn_hlist_release(subscriptions, mm);\n}\n\n \nint __mmu_notifier_clear_flush_young(struct mm_struct *mm,\n\t\t\t\t\tunsigned long start,\n\t\t\t\t\tunsigned long end)\n{\n\tstruct mmu_notifier *subscription;\n\tint young = 0, id;\n\n\tid = srcu_read_lock(&srcu);\n\thlist_for_each_entry_rcu(subscription,\n\t\t\t\t &mm->notifier_subscriptions->list, hlist,\n\t\t\t\t srcu_read_lock_held(&srcu)) {\n\t\tif (subscription->ops->clear_flush_young)\n\t\t\tyoung |= subscription->ops->clear_flush_young(\n\t\t\t\tsubscription, mm, start, end);\n\t}\n\tsrcu_read_unlock(&srcu, id);\n\n\treturn young;\n}\n\nint __mmu_notifier_clear_young(struct mm_struct *mm,\n\t\t\t       unsigned long start,\n\t\t\t       unsigned long end)\n{\n\tstruct mmu_notifier *subscription;\n\tint young = 0, id;\n\n\tid = srcu_read_lock(&srcu);\n\thlist_for_each_entry_rcu(subscription,\n\t\t\t\t &mm->notifier_subscriptions->list, hlist,\n\t\t\t\t srcu_read_lock_held(&srcu)) {\n\t\tif (subscription->ops->clear_young)\n\t\t\tyoung |= subscription->ops->clear_young(subscription,\n\t\t\t\t\t\t\t\tmm, start, end);\n\t}\n\tsrcu_read_unlock(&srcu, id);\n\n\treturn young;\n}\n\nint __mmu_notifier_test_young(struct mm_struct *mm,\n\t\t\t      unsigned long address)\n{\n\tstruct mmu_notifier *subscription;\n\tint young = 0, id;\n\n\tid = srcu_read_lock(&srcu);\n\thlist_for_each_entry_rcu(subscription,\n\t\t\t\t &mm->notifier_subscriptions->list, hlist,\n\t\t\t\t srcu_read_lock_held(&srcu)) {\n\t\tif (subscription->ops->test_young) {\n\t\t\tyoung = subscription->ops->test_young(subscription, mm,\n\t\t\t\t\t\t\t      address);\n\t\t\tif (young)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tsrcu_read_unlock(&srcu, id);\n\n\treturn young;\n}\n\nvoid __mmu_notifier_change_pte(struct mm_struct *mm, unsigned long address,\n\t\t\t       pte_t pte)\n{\n\tstruct mmu_notifier *subscription;\n\tint id;\n\n\tid = srcu_read_lock(&srcu);\n\thlist_for_each_entry_rcu(subscription,\n\t\t\t\t &mm->notifier_subscriptions->list, hlist,\n\t\t\t\t srcu_read_lock_held(&srcu)) {\n\t\tif (subscription->ops->change_pte)\n\t\t\tsubscription->ops->change_pte(subscription, mm, address,\n\t\t\t\t\t\t      pte);\n\t}\n\tsrcu_read_unlock(&srcu, id);\n}\n\nstatic int mn_itree_invalidate(struct mmu_notifier_subscriptions *subscriptions,\n\t\t\t       const struct mmu_notifier_range *range)\n{\n\tstruct mmu_interval_notifier *interval_sub;\n\tunsigned long cur_seq;\n\n\tfor (interval_sub =\n\t\t     mn_itree_inv_start_range(subscriptions, range, &cur_seq);\n\t     interval_sub;\n\t     interval_sub = mn_itree_inv_next(interval_sub, range)) {\n\t\tbool ret;\n\n\t\tret = interval_sub->ops->invalidate(interval_sub, range,\n\t\t\t\t\t\t    cur_seq);\n\t\tif (!ret) {\n\t\t\tif (WARN_ON(mmu_notifier_range_blockable(range)))\n\t\t\t\tcontinue;\n\t\t\tgoto out_would_block;\n\t\t}\n\t}\n\treturn 0;\n\nout_would_block:\n\t \n\tmn_itree_inv_end(subscriptions);\n\treturn -EAGAIN;\n}\n\nstatic int mn_hlist_invalidate_range_start(\n\tstruct mmu_notifier_subscriptions *subscriptions,\n\tstruct mmu_notifier_range *range)\n{\n\tstruct mmu_notifier *subscription;\n\tint ret = 0;\n\tint id;\n\n\tid = srcu_read_lock(&srcu);\n\thlist_for_each_entry_rcu(subscription, &subscriptions->list, hlist,\n\t\t\t\t srcu_read_lock_held(&srcu)) {\n\t\tconst struct mmu_notifier_ops *ops = subscription->ops;\n\n\t\tif (ops->invalidate_range_start) {\n\t\t\tint _ret;\n\n\t\t\tif (!mmu_notifier_range_blockable(range))\n\t\t\t\tnon_block_start();\n\t\t\t_ret = ops->invalidate_range_start(subscription, range);\n\t\t\tif (!mmu_notifier_range_blockable(range))\n\t\t\t\tnon_block_end();\n\t\t\tif (_ret) {\n\t\t\t\tpr_info(\"%pS callback failed with %d in %sblockable context.\\n\",\n\t\t\t\t\tops->invalidate_range_start, _ret,\n\t\t\t\t\t!mmu_notifier_range_blockable(range) ?\n\t\t\t\t\t\t\"non-\" :\n\t\t\t\t\t\t\"\");\n\t\t\t\tWARN_ON(mmu_notifier_range_blockable(range) ||\n\t\t\t\t\t_ret != -EAGAIN);\n\t\t\t\t \n\t\t\t\tWARN_ON(ops->invalidate_range_end);\n\t\t\t\tret = _ret;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (ret) {\n\t\t \n\t\thlist_for_each_entry_rcu(subscription, &subscriptions->list,\n\t\t\t\t\t hlist, srcu_read_lock_held(&srcu)) {\n\t\t\tif (!subscription->ops->invalidate_range_end)\n\t\t\t\tcontinue;\n\n\t\t\tsubscription->ops->invalidate_range_end(subscription,\n\t\t\t\t\t\t\t\trange);\n\t\t}\n\t}\n\tsrcu_read_unlock(&srcu, id);\n\n\treturn ret;\n}\n\nint __mmu_notifier_invalidate_range_start(struct mmu_notifier_range *range)\n{\n\tstruct mmu_notifier_subscriptions *subscriptions =\n\t\trange->mm->notifier_subscriptions;\n\tint ret;\n\n\tif (subscriptions->has_itree) {\n\t\tret = mn_itree_invalidate(subscriptions, range);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\tif (!hlist_empty(&subscriptions->list))\n\t\treturn mn_hlist_invalidate_range_start(subscriptions, range);\n\treturn 0;\n}\n\nstatic void\nmn_hlist_invalidate_end(struct mmu_notifier_subscriptions *subscriptions,\n\t\t\tstruct mmu_notifier_range *range)\n{\n\tstruct mmu_notifier *subscription;\n\tint id;\n\n\tid = srcu_read_lock(&srcu);\n\thlist_for_each_entry_rcu(subscription, &subscriptions->list, hlist,\n\t\t\t\t srcu_read_lock_held(&srcu)) {\n\t\tif (subscription->ops->invalidate_range_end) {\n\t\t\tif (!mmu_notifier_range_blockable(range))\n\t\t\t\tnon_block_start();\n\t\t\tsubscription->ops->invalidate_range_end(subscription,\n\t\t\t\t\t\t\t\trange);\n\t\t\tif (!mmu_notifier_range_blockable(range))\n\t\t\t\tnon_block_end();\n\t\t}\n\t}\n\tsrcu_read_unlock(&srcu, id);\n}\n\nvoid __mmu_notifier_invalidate_range_end(struct mmu_notifier_range *range)\n{\n\tstruct mmu_notifier_subscriptions *subscriptions =\n\t\trange->mm->notifier_subscriptions;\n\n\tlock_map_acquire(&__mmu_notifier_invalidate_range_start_map);\n\tif (subscriptions->has_itree)\n\t\tmn_itree_inv_end(subscriptions);\n\n\tif (!hlist_empty(&subscriptions->list))\n\t\tmn_hlist_invalidate_end(subscriptions, range);\n\tlock_map_release(&__mmu_notifier_invalidate_range_start_map);\n}\n\nvoid __mmu_notifier_arch_invalidate_secondary_tlbs(struct mm_struct *mm,\n\t\t\t\t\tunsigned long start, unsigned long end)\n{\n\tstruct mmu_notifier *subscription;\n\tint id;\n\n\tid = srcu_read_lock(&srcu);\n\thlist_for_each_entry_rcu(subscription,\n\t\t\t\t &mm->notifier_subscriptions->list, hlist,\n\t\t\t\t srcu_read_lock_held(&srcu)) {\n\t\tif (subscription->ops->arch_invalidate_secondary_tlbs)\n\t\t\tsubscription->ops->arch_invalidate_secondary_tlbs(\n\t\t\t\tsubscription, mm,\n\t\t\t\tstart, end);\n\t}\n\tsrcu_read_unlock(&srcu, id);\n}\n\n \nint __mmu_notifier_register(struct mmu_notifier *subscription,\n\t\t\t    struct mm_struct *mm)\n{\n\tstruct mmu_notifier_subscriptions *subscriptions = NULL;\n\tint ret;\n\n\tmmap_assert_write_locked(mm);\n\tBUG_ON(atomic_read(&mm->mm_users) <= 0);\n\n\t \n\tif (WARN_ON_ONCE(subscription &&\n\t\t\t (subscription->ops->arch_invalidate_secondary_tlbs &&\n\t\t\t (subscription->ops->invalidate_range_start ||\n\t\t\t  subscription->ops->invalidate_range_end))))\n\t\treturn -EINVAL;\n\n\tif (!mm->notifier_subscriptions) {\n\t\t \n\t\tsubscriptions = kzalloc(\n\t\t\tsizeof(struct mmu_notifier_subscriptions), GFP_KERNEL);\n\t\tif (!subscriptions)\n\t\t\treturn -ENOMEM;\n\n\t\tINIT_HLIST_HEAD(&subscriptions->list);\n\t\tspin_lock_init(&subscriptions->lock);\n\t\tsubscriptions->invalidate_seq = 2;\n\t\tsubscriptions->itree = RB_ROOT_CACHED;\n\t\tinit_waitqueue_head(&subscriptions->wq);\n\t\tINIT_HLIST_HEAD(&subscriptions->deferred_list);\n\t}\n\n\tret = mm_take_all_locks(mm);\n\tif (unlikely(ret))\n\t\tgoto out_clean;\n\n\t \n\tif (subscriptions)\n\t\tsmp_store_release(&mm->notifier_subscriptions, subscriptions);\n\n\tif (subscription) {\n\t\t \n\t\tmmgrab(mm);\n\t\tsubscription->mm = mm;\n\t\tsubscription->users = 1;\n\n\t\tspin_lock(&mm->notifier_subscriptions->lock);\n\t\thlist_add_head_rcu(&subscription->hlist,\n\t\t\t\t   &mm->notifier_subscriptions->list);\n\t\tspin_unlock(&mm->notifier_subscriptions->lock);\n\t} else\n\t\tmm->notifier_subscriptions->has_itree = true;\n\n\tmm_drop_all_locks(mm);\n\tBUG_ON(atomic_read(&mm->mm_users) <= 0);\n\treturn 0;\n\nout_clean:\n\tkfree(subscriptions);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(__mmu_notifier_register);\n\n \nint mmu_notifier_register(struct mmu_notifier *subscription,\n\t\t\t  struct mm_struct *mm)\n{\n\tint ret;\n\n\tmmap_write_lock(mm);\n\tret = __mmu_notifier_register(subscription, mm);\n\tmmap_write_unlock(mm);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(mmu_notifier_register);\n\nstatic struct mmu_notifier *\nfind_get_mmu_notifier(struct mm_struct *mm, const struct mmu_notifier_ops *ops)\n{\n\tstruct mmu_notifier *subscription;\n\n\tspin_lock(&mm->notifier_subscriptions->lock);\n\thlist_for_each_entry_rcu(subscription,\n\t\t\t\t &mm->notifier_subscriptions->list, hlist,\n\t\t\t\t lockdep_is_held(&mm->notifier_subscriptions->lock)) {\n\t\tif (subscription->ops != ops)\n\t\t\tcontinue;\n\n\t\tif (likely(subscription->users != UINT_MAX))\n\t\t\tsubscription->users++;\n\t\telse\n\t\t\tsubscription = ERR_PTR(-EOVERFLOW);\n\t\tspin_unlock(&mm->notifier_subscriptions->lock);\n\t\treturn subscription;\n\t}\n\tspin_unlock(&mm->notifier_subscriptions->lock);\n\treturn NULL;\n}\n\n \nstruct mmu_notifier *mmu_notifier_get_locked(const struct mmu_notifier_ops *ops,\n\t\t\t\t\t     struct mm_struct *mm)\n{\n\tstruct mmu_notifier *subscription;\n\tint ret;\n\n\tmmap_assert_write_locked(mm);\n\n\tif (mm->notifier_subscriptions) {\n\t\tsubscription = find_get_mmu_notifier(mm, ops);\n\t\tif (subscription)\n\t\t\treturn subscription;\n\t}\n\n\tsubscription = ops->alloc_notifier(mm);\n\tif (IS_ERR(subscription))\n\t\treturn subscription;\n\tsubscription->ops = ops;\n\tret = __mmu_notifier_register(subscription, mm);\n\tif (ret)\n\t\tgoto out_free;\n\treturn subscription;\nout_free:\n\tsubscription->ops->free_notifier(subscription);\n\treturn ERR_PTR(ret);\n}\nEXPORT_SYMBOL_GPL(mmu_notifier_get_locked);\n\n \nvoid __mmu_notifier_subscriptions_destroy(struct mm_struct *mm)\n{\n\tBUG_ON(!hlist_empty(&mm->notifier_subscriptions->list));\n\tkfree(mm->notifier_subscriptions);\n\tmm->notifier_subscriptions = LIST_POISON1;  \n}\n\n \nvoid mmu_notifier_unregister(struct mmu_notifier *subscription,\n\t\t\t     struct mm_struct *mm)\n{\n\tBUG_ON(atomic_read(&mm->mm_count) <= 0);\n\n\tif (!hlist_unhashed(&subscription->hlist)) {\n\t\t \n\t\tint id;\n\n\t\tid = srcu_read_lock(&srcu);\n\t\t \n\t\tif (subscription->ops->release)\n\t\t\tsubscription->ops->release(subscription, mm);\n\t\tsrcu_read_unlock(&srcu, id);\n\n\t\tspin_lock(&mm->notifier_subscriptions->lock);\n\t\t \n\t\thlist_del_init_rcu(&subscription->hlist);\n\t\tspin_unlock(&mm->notifier_subscriptions->lock);\n\t}\n\n\t \n\tsynchronize_srcu(&srcu);\n\n\tBUG_ON(atomic_read(&mm->mm_count) <= 0);\n\n\tmmdrop(mm);\n}\nEXPORT_SYMBOL_GPL(mmu_notifier_unregister);\n\nstatic void mmu_notifier_free_rcu(struct rcu_head *rcu)\n{\n\tstruct mmu_notifier *subscription =\n\t\tcontainer_of(rcu, struct mmu_notifier, rcu);\n\tstruct mm_struct *mm = subscription->mm;\n\n\tsubscription->ops->free_notifier(subscription);\n\t \n\tmmdrop(mm);\n}\n\n \nvoid mmu_notifier_put(struct mmu_notifier *subscription)\n{\n\tstruct mm_struct *mm = subscription->mm;\n\n\tspin_lock(&mm->notifier_subscriptions->lock);\n\tif (WARN_ON(!subscription->users) || --subscription->users)\n\t\tgoto out_unlock;\n\thlist_del_init_rcu(&subscription->hlist);\n\tspin_unlock(&mm->notifier_subscriptions->lock);\n\n\tcall_srcu(&srcu, &subscription->rcu, mmu_notifier_free_rcu);\n\treturn;\n\nout_unlock:\n\tspin_unlock(&mm->notifier_subscriptions->lock);\n}\nEXPORT_SYMBOL_GPL(mmu_notifier_put);\n\nstatic int __mmu_interval_notifier_insert(\n\tstruct mmu_interval_notifier *interval_sub, struct mm_struct *mm,\n\tstruct mmu_notifier_subscriptions *subscriptions, unsigned long start,\n\tunsigned long length, const struct mmu_interval_notifier_ops *ops)\n{\n\tinterval_sub->mm = mm;\n\tinterval_sub->ops = ops;\n\tRB_CLEAR_NODE(&interval_sub->interval_tree.rb);\n\tinterval_sub->interval_tree.start = start;\n\t \n\tif (length == 0 ||\n\t    check_add_overflow(start, length - 1,\n\t\t\t       &interval_sub->interval_tree.last))\n\t\treturn -EOVERFLOW;\n\n\t \n\tif (WARN_ON(atomic_read(&mm->mm_users) <= 0))\n\t\treturn -EINVAL;\n\n\t \n\tmmgrab(mm);\n\n\t \n\tspin_lock(&subscriptions->lock);\n\tif (subscriptions->active_invalidate_ranges) {\n\t\tif (mn_itree_is_invalidating(subscriptions))\n\t\t\thlist_add_head(&interval_sub->deferred_item,\n\t\t\t\t       &subscriptions->deferred_list);\n\t\telse {\n\t\t\tsubscriptions->invalidate_seq |= 1;\n\t\t\tinterval_tree_insert(&interval_sub->interval_tree,\n\t\t\t\t\t     &subscriptions->itree);\n\t\t}\n\t\tinterval_sub->invalidate_seq = subscriptions->invalidate_seq;\n\t} else {\n\t\tWARN_ON(mn_itree_is_invalidating(subscriptions));\n\t\t \n\t\tinterval_sub->invalidate_seq =\n\t\t\tsubscriptions->invalidate_seq - 1;\n\t\tinterval_tree_insert(&interval_sub->interval_tree,\n\t\t\t\t     &subscriptions->itree);\n\t}\n\tspin_unlock(&subscriptions->lock);\n\treturn 0;\n}\n\n \nint mmu_interval_notifier_insert(struct mmu_interval_notifier *interval_sub,\n\t\t\t\t struct mm_struct *mm, unsigned long start,\n\t\t\t\t unsigned long length,\n\t\t\t\t const struct mmu_interval_notifier_ops *ops)\n{\n\tstruct mmu_notifier_subscriptions *subscriptions;\n\tint ret;\n\n\tmight_lock(&mm->mmap_lock);\n\n\tsubscriptions = smp_load_acquire(&mm->notifier_subscriptions);\n\tif (!subscriptions || !subscriptions->has_itree) {\n\t\tret = mmu_notifier_register(NULL, mm);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tsubscriptions = mm->notifier_subscriptions;\n\t}\n\treturn __mmu_interval_notifier_insert(interval_sub, mm, subscriptions,\n\t\t\t\t\t      start, length, ops);\n}\nEXPORT_SYMBOL_GPL(mmu_interval_notifier_insert);\n\nint mmu_interval_notifier_insert_locked(\n\tstruct mmu_interval_notifier *interval_sub, struct mm_struct *mm,\n\tunsigned long start, unsigned long length,\n\tconst struct mmu_interval_notifier_ops *ops)\n{\n\tstruct mmu_notifier_subscriptions *subscriptions =\n\t\tmm->notifier_subscriptions;\n\tint ret;\n\n\tmmap_assert_write_locked(mm);\n\n\tif (!subscriptions || !subscriptions->has_itree) {\n\t\tret = __mmu_notifier_register(NULL, mm);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tsubscriptions = mm->notifier_subscriptions;\n\t}\n\treturn __mmu_interval_notifier_insert(interval_sub, mm, subscriptions,\n\t\t\t\t\t      start, length, ops);\n}\nEXPORT_SYMBOL_GPL(mmu_interval_notifier_insert_locked);\n\nstatic bool\nmmu_interval_seq_released(struct mmu_notifier_subscriptions *subscriptions,\n\t\t\t  unsigned long seq)\n{\n\tbool ret;\n\n\tspin_lock(&subscriptions->lock);\n\tret = subscriptions->invalidate_seq != seq;\n\tspin_unlock(&subscriptions->lock);\n\treturn ret;\n}\n\n \nvoid mmu_interval_notifier_remove(struct mmu_interval_notifier *interval_sub)\n{\n\tstruct mm_struct *mm = interval_sub->mm;\n\tstruct mmu_notifier_subscriptions *subscriptions =\n\t\tmm->notifier_subscriptions;\n\tunsigned long seq = 0;\n\n\tmight_sleep();\n\n\tspin_lock(&subscriptions->lock);\n\tif (mn_itree_is_invalidating(subscriptions)) {\n\t\t \n\t\tif (RB_EMPTY_NODE(&interval_sub->interval_tree.rb)) {\n\t\t\thlist_del(&interval_sub->deferred_item);\n\t\t} else {\n\t\t\thlist_add_head(&interval_sub->deferred_item,\n\t\t\t\t       &subscriptions->deferred_list);\n\t\t\tseq = subscriptions->invalidate_seq;\n\t\t}\n\t} else {\n\t\tWARN_ON(RB_EMPTY_NODE(&interval_sub->interval_tree.rb));\n\t\tinterval_tree_remove(&interval_sub->interval_tree,\n\t\t\t\t     &subscriptions->itree);\n\t}\n\tspin_unlock(&subscriptions->lock);\n\n\t \n\tlock_map_acquire(&__mmu_notifier_invalidate_range_start_map);\n\tlock_map_release(&__mmu_notifier_invalidate_range_start_map);\n\tif (seq)\n\t\twait_event(subscriptions->wq,\n\t\t\t   mmu_interval_seq_released(subscriptions, seq));\n\n\t \n\tmmdrop(mm);\n}\nEXPORT_SYMBOL_GPL(mmu_interval_notifier_remove);\n\n \nvoid mmu_notifier_synchronize(void)\n{\n\tsynchronize_srcu(&srcu);\n}\nEXPORT_SYMBOL_GPL(mmu_notifier_synchronize);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}