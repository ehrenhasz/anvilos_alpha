{
  "module_name": "hugetlb_vmemmap.c",
  "hash_id": "171e421f2537e2cbf5876596fdd3175502929e088d53cac23477a411acbd767d",
  "original_prompt": "Ingested from linux-6.6.14/mm/hugetlb_vmemmap.c",
  "human_readable_source": "\n \n#define pr_fmt(fmt)\t\"HugeTLB: \" fmt\n\n#include <linux/pgtable.h>\n#include <linux/moduleparam.h>\n#include <linux/bootmem_info.h>\n#include <asm/pgalloc.h>\n#include <asm/tlbflush.h>\n#include \"hugetlb_vmemmap.h\"\n\n \nstruct vmemmap_remap_walk {\n\tvoid\t\t\t(*remap_pte)(pte_t *pte, unsigned long addr,\n\t\t\t\t\t     struct vmemmap_remap_walk *walk);\n\tunsigned long\t\tnr_walked;\n\tstruct page\t\t*reuse_page;\n\tunsigned long\t\treuse_addr;\n\tstruct list_head\t*vmemmap_pages;\n};\n\nstatic int split_vmemmap_huge_pmd(pmd_t *pmd, unsigned long start)\n{\n\tpmd_t __pmd;\n\tint i;\n\tunsigned long addr = start;\n\tstruct page *head;\n\tpte_t *pgtable;\n\n\tspin_lock(&init_mm.page_table_lock);\n\thead = pmd_leaf(*pmd) ? pmd_page(*pmd) : NULL;\n\tspin_unlock(&init_mm.page_table_lock);\n\n\tif (!head)\n\t\treturn 0;\n\n\tpgtable = pte_alloc_one_kernel(&init_mm);\n\tif (!pgtable)\n\t\treturn -ENOMEM;\n\n\tpmd_populate_kernel(&init_mm, &__pmd, pgtable);\n\n\tfor (i = 0; i < PTRS_PER_PTE; i++, addr += PAGE_SIZE) {\n\t\tpte_t entry, *pte;\n\t\tpgprot_t pgprot = PAGE_KERNEL;\n\n\t\tentry = mk_pte(head + i, pgprot);\n\t\tpte = pte_offset_kernel(&__pmd, addr);\n\t\tset_pte_at(&init_mm, addr, pte, entry);\n\t}\n\n\tspin_lock(&init_mm.page_table_lock);\n\tif (likely(pmd_leaf(*pmd))) {\n\t\t \n\t\tif (!PageReserved(head))\n\t\t\tsplit_page(head, get_order(PMD_SIZE));\n\n\t\t \n\t\tsmp_wmb();\n\t\tpmd_populate_kernel(&init_mm, pmd, pgtable);\n\t\tflush_tlb_kernel_range(start, start + PMD_SIZE);\n\t} else {\n\t\tpte_free_kernel(&init_mm, pgtable);\n\t}\n\tspin_unlock(&init_mm.page_table_lock);\n\n\treturn 0;\n}\n\nstatic void vmemmap_pte_range(pmd_t *pmd, unsigned long addr,\n\t\t\t      unsigned long end,\n\t\t\t      struct vmemmap_remap_walk *walk)\n{\n\tpte_t *pte = pte_offset_kernel(pmd, addr);\n\n\t \n\tif (!walk->reuse_page) {\n\t\twalk->reuse_page = pte_page(ptep_get(pte));\n\t\t \n\t\taddr += PAGE_SIZE;\n\t\tpte++;\n\t\twalk->nr_walked++;\n\t}\n\n\tfor (; addr != end; addr += PAGE_SIZE, pte++) {\n\t\twalk->remap_pte(pte, addr, walk);\n\t\twalk->nr_walked++;\n\t}\n}\n\nstatic int vmemmap_pmd_range(pud_t *pud, unsigned long addr,\n\t\t\t     unsigned long end,\n\t\t\t     struct vmemmap_remap_walk *walk)\n{\n\tpmd_t *pmd;\n\tunsigned long next;\n\n\tpmd = pmd_offset(pud, addr);\n\tdo {\n\t\tint ret;\n\n\t\tret = split_vmemmap_huge_pmd(pmd, addr & PMD_MASK);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tnext = pmd_addr_end(addr, end);\n\t\tvmemmap_pte_range(pmd, addr, next, walk);\n\t} while (pmd++, addr = next, addr != end);\n\n\treturn 0;\n}\n\nstatic int vmemmap_pud_range(p4d_t *p4d, unsigned long addr,\n\t\t\t     unsigned long end,\n\t\t\t     struct vmemmap_remap_walk *walk)\n{\n\tpud_t *pud;\n\tunsigned long next;\n\n\tpud = pud_offset(p4d, addr);\n\tdo {\n\t\tint ret;\n\n\t\tnext = pud_addr_end(addr, end);\n\t\tret = vmemmap_pmd_range(pud, addr, next, walk);\n\t\tif (ret)\n\t\t\treturn ret;\n\t} while (pud++, addr = next, addr != end);\n\n\treturn 0;\n}\n\nstatic int vmemmap_p4d_range(pgd_t *pgd, unsigned long addr,\n\t\t\t     unsigned long end,\n\t\t\t     struct vmemmap_remap_walk *walk)\n{\n\tp4d_t *p4d;\n\tunsigned long next;\n\n\tp4d = p4d_offset(pgd, addr);\n\tdo {\n\t\tint ret;\n\n\t\tnext = p4d_addr_end(addr, end);\n\t\tret = vmemmap_pud_range(p4d, addr, next, walk);\n\t\tif (ret)\n\t\t\treturn ret;\n\t} while (p4d++, addr = next, addr != end);\n\n\treturn 0;\n}\n\nstatic int vmemmap_remap_range(unsigned long start, unsigned long end,\n\t\t\t       struct vmemmap_remap_walk *walk)\n{\n\tunsigned long addr = start;\n\tunsigned long next;\n\tpgd_t *pgd;\n\n\tVM_BUG_ON(!PAGE_ALIGNED(start));\n\tVM_BUG_ON(!PAGE_ALIGNED(end));\n\n\tpgd = pgd_offset_k(addr);\n\tdo {\n\t\tint ret;\n\n\t\tnext = pgd_addr_end(addr, end);\n\t\tret = vmemmap_p4d_range(pgd, addr, next, walk);\n\t\tif (ret)\n\t\t\treturn ret;\n\t} while (pgd++, addr = next, addr != end);\n\n\tflush_tlb_kernel_range(start, end);\n\n\treturn 0;\n}\n\n \nstatic inline void free_vmemmap_page(struct page *page)\n{\n\tif (PageReserved(page))\n\t\tfree_bootmem_page(page);\n\telse\n\t\t__free_page(page);\n}\n\n \nstatic void free_vmemmap_page_list(struct list_head *list)\n{\n\tstruct page *page, *next;\n\n\tlist_for_each_entry_safe(page, next, list, lru)\n\t\tfree_vmemmap_page(page);\n}\n\nstatic void vmemmap_remap_pte(pte_t *pte, unsigned long addr,\n\t\t\t      struct vmemmap_remap_walk *walk)\n{\n\t \n\tpgprot_t pgprot = PAGE_KERNEL_RO;\n\tstruct page *page = pte_page(ptep_get(pte));\n\tpte_t entry;\n\n\t \n\tif (unlikely(addr == walk->reuse_addr)) {\n\t\tpgprot = PAGE_KERNEL;\n\t\tlist_del(&walk->reuse_page->lru);\n\n\t\t \n\t\tsmp_wmb();\n\t}\n\n\tentry = mk_pte(walk->reuse_page, pgprot);\n\tlist_add_tail(&page->lru, walk->vmemmap_pages);\n\tset_pte_at(&init_mm, addr, pte, entry);\n}\n\n \n#define NR_RESET_STRUCT_PAGE\t\t3\n\nstatic inline void reset_struct_pages(struct page *start)\n{\n\tstruct page *from = start + NR_RESET_STRUCT_PAGE;\n\n\tBUILD_BUG_ON(NR_RESET_STRUCT_PAGE * 2 > PAGE_SIZE / sizeof(struct page));\n\tmemcpy(start, from, sizeof(*from) * NR_RESET_STRUCT_PAGE);\n}\n\nstatic void vmemmap_restore_pte(pte_t *pte, unsigned long addr,\n\t\t\t\tstruct vmemmap_remap_walk *walk)\n{\n\tpgprot_t pgprot = PAGE_KERNEL;\n\tstruct page *page;\n\tvoid *to;\n\n\tBUG_ON(pte_page(ptep_get(pte)) != walk->reuse_page);\n\n\tpage = list_first_entry(walk->vmemmap_pages, struct page, lru);\n\tlist_del(&page->lru);\n\tto = page_to_virt(page);\n\tcopy_page(to, (void *)walk->reuse_addr);\n\treset_struct_pages(to);\n\n\t \n\tsmp_wmb();\n\tset_pte_at(&init_mm, addr, pte, mk_pte(page, pgprot));\n}\n\n \nstatic int vmemmap_remap_free(unsigned long start, unsigned long end,\n\t\t\t      unsigned long reuse)\n{\n\tint ret;\n\tLIST_HEAD(vmemmap_pages);\n\tstruct vmemmap_remap_walk walk = {\n\t\t.remap_pte\t= vmemmap_remap_pte,\n\t\t.reuse_addr\t= reuse,\n\t\t.vmemmap_pages\t= &vmemmap_pages,\n\t};\n\tint nid = page_to_nid((struct page *)start);\n\tgfp_t gfp_mask = GFP_KERNEL | __GFP_THISNODE | __GFP_NORETRY |\n\t\t\t__GFP_NOWARN;\n\n\t \n\twalk.reuse_page = alloc_pages_node(nid, gfp_mask, 0);\n\tif (walk.reuse_page) {\n\t\tcopy_page(page_to_virt(walk.reuse_page),\n\t\t\t  (void *)walk.reuse_addr);\n\t\tlist_add(&walk.reuse_page->lru, &vmemmap_pages);\n\t}\n\n\t \n\tBUG_ON(start - reuse != PAGE_SIZE);\n\n\tmmap_read_lock(&init_mm);\n\tret = vmemmap_remap_range(reuse, end, &walk);\n\tif (ret && walk.nr_walked) {\n\t\tend = reuse + walk.nr_walked * PAGE_SIZE;\n\t\t \n\t\twalk = (struct vmemmap_remap_walk) {\n\t\t\t.remap_pte\t= vmemmap_restore_pte,\n\t\t\t.reuse_addr\t= reuse,\n\t\t\t.vmemmap_pages\t= &vmemmap_pages,\n\t\t};\n\n\t\tvmemmap_remap_range(reuse, end, &walk);\n\t}\n\tmmap_read_unlock(&init_mm);\n\n\tfree_vmemmap_page_list(&vmemmap_pages);\n\n\treturn ret;\n}\n\nstatic int alloc_vmemmap_page_list(unsigned long start, unsigned long end,\n\t\t\t\t   struct list_head *list)\n{\n\tgfp_t gfp_mask = GFP_KERNEL | __GFP_RETRY_MAYFAIL | __GFP_THISNODE;\n\tunsigned long nr_pages = (end - start) >> PAGE_SHIFT;\n\tint nid = page_to_nid((struct page *)start);\n\tstruct page *page, *next;\n\n\twhile (nr_pages--) {\n\t\tpage = alloc_pages_node(nid, gfp_mask, 0);\n\t\tif (!page)\n\t\t\tgoto out;\n\t\tlist_add_tail(&page->lru, list);\n\t}\n\n\treturn 0;\nout:\n\tlist_for_each_entry_safe(page, next, list, lru)\n\t\t__free_page(page);\n\treturn -ENOMEM;\n}\n\n \nstatic int vmemmap_remap_alloc(unsigned long start, unsigned long end,\n\t\t\t       unsigned long reuse)\n{\n\tLIST_HEAD(vmemmap_pages);\n\tstruct vmemmap_remap_walk walk = {\n\t\t.remap_pte\t= vmemmap_restore_pte,\n\t\t.reuse_addr\t= reuse,\n\t\t.vmemmap_pages\t= &vmemmap_pages,\n\t};\n\n\t \n\tBUG_ON(start - reuse != PAGE_SIZE);\n\n\tif (alloc_vmemmap_page_list(start, end, &vmemmap_pages))\n\t\treturn -ENOMEM;\n\n\tmmap_read_lock(&init_mm);\n\tvmemmap_remap_range(reuse, end, &walk);\n\tmmap_read_unlock(&init_mm);\n\n\treturn 0;\n}\n\nDEFINE_STATIC_KEY_FALSE(hugetlb_optimize_vmemmap_key);\nEXPORT_SYMBOL(hugetlb_optimize_vmemmap_key);\n\nstatic bool vmemmap_optimize_enabled = IS_ENABLED(CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP_DEFAULT_ON);\ncore_param(hugetlb_free_vmemmap, vmemmap_optimize_enabled, bool, 0);\n\n \nint hugetlb_vmemmap_restore(const struct hstate *h, struct page *head)\n{\n\tint ret;\n\tunsigned long vmemmap_start = (unsigned long)head, vmemmap_end;\n\tunsigned long vmemmap_reuse;\n\n\tif (!HPageVmemmapOptimized(head))\n\t\treturn 0;\n\n\tvmemmap_end\t= vmemmap_start + hugetlb_vmemmap_size(h);\n\tvmemmap_reuse\t= vmemmap_start;\n\tvmemmap_start\t+= HUGETLB_VMEMMAP_RESERVE_SIZE;\n\n\t \n\tret = vmemmap_remap_alloc(vmemmap_start, vmemmap_end, vmemmap_reuse);\n\tif (!ret) {\n\t\tClearHPageVmemmapOptimized(head);\n\t\tstatic_branch_dec(&hugetlb_optimize_vmemmap_key);\n\t}\n\n\treturn ret;\n}\n\n \nstatic bool vmemmap_should_optimize(const struct hstate *h, const struct page *head)\n{\n\tif (!READ_ONCE(vmemmap_optimize_enabled))\n\t\treturn false;\n\n\tif (!hugetlb_vmemmap_optimizable(h))\n\t\treturn false;\n\n\tif (IS_ENABLED(CONFIG_MEMORY_HOTPLUG)) {\n\t\tpmd_t *pmdp, pmd;\n\t\tstruct page *vmemmap_page;\n\t\tunsigned long vaddr = (unsigned long)head;\n\n\t\t \n\t\tpmdp = pmd_off_k(vaddr);\n\t\t \n\t\tpmd = READ_ONCE(*pmdp);\n\t\tif (pmd_leaf(pmd))\n\t\t\tvmemmap_page = pmd_page(pmd) + pte_index(vaddr);\n\t\telse\n\t\t\tvmemmap_page = pte_page(*pte_offset_kernel(pmdp, vaddr));\n\t\t \n\t\tif (PageVmemmapSelfHosted(vmemmap_page))\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\n \nvoid hugetlb_vmemmap_optimize(const struct hstate *h, struct page *head)\n{\n\tunsigned long vmemmap_start = (unsigned long)head, vmemmap_end;\n\tunsigned long vmemmap_reuse;\n\n\tif (!vmemmap_should_optimize(h, head))\n\t\treturn;\n\n\tstatic_branch_inc(&hugetlb_optimize_vmemmap_key);\n\n\tvmemmap_end\t= vmemmap_start + hugetlb_vmemmap_size(h);\n\tvmemmap_reuse\t= vmemmap_start;\n\tvmemmap_start\t+= HUGETLB_VMEMMAP_RESERVE_SIZE;\n\n\t \n\tif (vmemmap_remap_free(vmemmap_start, vmemmap_end, vmemmap_reuse))\n\t\tstatic_branch_dec(&hugetlb_optimize_vmemmap_key);\n\telse\n\t\tSetHPageVmemmapOptimized(head);\n}\n\nstatic struct ctl_table hugetlb_vmemmap_sysctls[] = {\n\t{\n\t\t.procname\t= \"hugetlb_optimize_vmemmap\",\n\t\t.data\t\t= &vmemmap_optimize_enabled,\n\t\t.maxlen\t\t= sizeof(vmemmap_optimize_enabled),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dobool,\n\t},\n\t{ }\n};\n\nstatic int __init hugetlb_vmemmap_init(void)\n{\n\tconst struct hstate *h;\n\n\t \n\tBUILD_BUG_ON(__NR_USED_SUBPAGE * sizeof(struct page) > HUGETLB_VMEMMAP_RESERVE_SIZE);\n\n\tfor_each_hstate(h) {\n\t\tif (hugetlb_vmemmap_optimizable(h)) {\n\t\t\tregister_sysctl_init(\"vm\", hugetlb_vmemmap_sysctls);\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn 0;\n}\nlate_initcall(hugetlb_vmemmap_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}