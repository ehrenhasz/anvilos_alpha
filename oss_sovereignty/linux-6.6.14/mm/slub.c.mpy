{
  "module_name": "slub.c",
  "hash_id": "94a3b1a720965555f308bb68fe6500ccb98546f8752d43ffdf8771f1251a6422",
  "original_prompt": "Ingested from linux-6.6.14/mm/slub.c",
  "human_readable_source": "\n \n\n#include <linux/mm.h>\n#include <linux/swap.h>  \n#include <linux/module.h>\n#include <linux/bit_spinlock.h>\n#include <linux/interrupt.h>\n#include <linux/swab.h>\n#include <linux/bitops.h>\n#include <linux/slab.h>\n#include \"slab.h\"\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <linux/kasan.h>\n#include <linux/kmsan.h>\n#include <linux/cpu.h>\n#include <linux/cpuset.h>\n#include <linux/mempolicy.h>\n#include <linux/ctype.h>\n#include <linux/stackdepot.h>\n#include <linux/debugobjects.h>\n#include <linux/kallsyms.h>\n#include <linux/kfence.h>\n#include <linux/memory.h>\n#include <linux/math64.h>\n#include <linux/fault-inject.h>\n#include <linux/stacktrace.h>\n#include <linux/prefetch.h>\n#include <linux/memcontrol.h>\n#include <linux/random.h>\n#include <kunit/test.h>\n#include <kunit/test-bug.h>\n#include <linux/sort.h>\n\n#include <linux/debugfs.h>\n#include <trace/events/kmem.h>\n\n#include \"internal.h\"\n\n \n\n \n#ifndef CONFIG_PREEMPT_RT\n#define slub_get_cpu_ptr(var)\t\tget_cpu_ptr(var)\n#define slub_put_cpu_ptr(var)\t\tput_cpu_ptr(var)\n#define USE_LOCKLESS_FAST_PATH()\t(true)\n#else\n#define slub_get_cpu_ptr(var)\t\t\\\n({\t\t\t\t\t\\\n\tmigrate_disable();\t\t\\\n\tthis_cpu_ptr(var);\t\t\\\n})\n#define slub_put_cpu_ptr(var)\t\t\\\ndo {\t\t\t\t\t\\\n\t(void)(var);\t\t\t\\\n\tmigrate_enable();\t\t\\\n} while (0)\n#define USE_LOCKLESS_FAST_PATH()\t(false)\n#endif\n\n#ifndef CONFIG_SLUB_TINY\n#define __fastpath_inline __always_inline\n#else\n#define __fastpath_inline\n#endif\n\n#ifdef CONFIG_SLUB_DEBUG\n#ifdef CONFIG_SLUB_DEBUG_ON\nDEFINE_STATIC_KEY_TRUE(slub_debug_enabled);\n#else\nDEFINE_STATIC_KEY_FALSE(slub_debug_enabled);\n#endif\n#endif\t\t \n\n \nstruct partial_context {\n\tstruct slab **slab;\n\tgfp_t flags;\n\tunsigned int orig_size;\n};\n\nstatic inline bool kmem_cache_debug(struct kmem_cache *s)\n{\n\treturn kmem_cache_debug_flags(s, SLAB_DEBUG_FLAGS);\n}\n\nstatic inline bool slub_debug_orig_size(struct kmem_cache *s)\n{\n\treturn (kmem_cache_debug_flags(s, SLAB_STORE_USER) &&\n\t\t\t(s->flags & SLAB_KMALLOC));\n}\n\nvoid *fixup_red_left(struct kmem_cache *s, void *p)\n{\n\tif (kmem_cache_debug_flags(s, SLAB_RED_ZONE))\n\t\tp += s->red_left_pad;\n\n\treturn p;\n}\n\nstatic inline bool kmem_cache_has_cpu_partial(struct kmem_cache *s)\n{\n#ifdef CONFIG_SLUB_CPU_PARTIAL\n\treturn !kmem_cache_debug(s);\n#else\n\treturn false;\n#endif\n}\n\n \n\n \n#undef SLUB_DEBUG_CMPXCHG\n\n#ifndef CONFIG_SLUB_TINY\n \n#define MIN_PARTIAL 5\n\n \n#define MAX_PARTIAL 10\n#else\n#define MIN_PARTIAL 0\n#define MAX_PARTIAL 0\n#endif\n\n#define DEBUG_DEFAULT_FLAGS (SLAB_CONSISTENCY_CHECKS | SLAB_RED_ZONE | \\\n\t\t\t\tSLAB_POISON | SLAB_STORE_USER)\n\n \n#define SLAB_NO_CMPXCHG (SLAB_CONSISTENCY_CHECKS | SLAB_STORE_USER | \\\n\t\t\t\tSLAB_TRACE)\n\n\n \n#define DEBUG_METADATA_FLAGS (SLAB_RED_ZONE | SLAB_POISON | SLAB_STORE_USER)\n\n#define OO_SHIFT\t16\n#define OO_MASK\t\t((1 << OO_SHIFT) - 1)\n#define MAX_OBJS_PER_PAGE\t32767  \n\n \n \n#define __OBJECT_POISON\t\t((slab_flags_t __force)0x80000000U)\n \n\n#ifdef system_has_freelist_aba\n#define __CMPXCHG_DOUBLE\t((slab_flags_t __force)0x40000000U)\n#else\n#define __CMPXCHG_DOUBLE\t((slab_flags_t __force)0U)\n#endif\n\n \n#define TRACK_ADDRS_COUNT 16\nstruct track {\n\tunsigned long addr;\t \n#ifdef CONFIG_STACKDEPOT\n\tdepot_stack_handle_t handle;\n#endif\n\tint cpu;\t\t \n\tint pid;\t\t \n\tunsigned long when;\t \n};\n\nenum track_item { TRACK_ALLOC, TRACK_FREE };\n\n#ifdef SLAB_SUPPORTS_SYSFS\nstatic int sysfs_slab_add(struct kmem_cache *);\nstatic int sysfs_slab_alias(struct kmem_cache *, const char *);\n#else\nstatic inline int sysfs_slab_add(struct kmem_cache *s) { return 0; }\nstatic inline int sysfs_slab_alias(struct kmem_cache *s, const char *p)\n\t\t\t\t\t\t\t{ return 0; }\n#endif\n\n#if defined(CONFIG_DEBUG_FS) && defined(CONFIG_SLUB_DEBUG)\nstatic void debugfs_slab_add(struct kmem_cache *);\n#else\nstatic inline void debugfs_slab_add(struct kmem_cache *s) { }\n#endif\n\nstatic inline void stat(const struct kmem_cache *s, enum stat_item si)\n{\n#ifdef CONFIG_SLUB_STATS\n\t \n\traw_cpu_inc(s->cpu_slab->stat[si]);\n#endif\n}\n\n \nstatic nodemask_t slab_nodes;\n\n#ifndef CONFIG_SLUB_TINY\n \nstatic struct workqueue_struct *flushwq;\n#endif\n\n \n\n \ntypedef struct { unsigned long v; } freeptr_t;\n\n \nstatic inline freeptr_t freelist_ptr_encode(const struct kmem_cache *s,\n\t\t\t\t\t    void *ptr, unsigned long ptr_addr)\n{\n\tunsigned long encoded;\n\n#ifdef CONFIG_SLAB_FREELIST_HARDENED\n\tencoded = (unsigned long)ptr ^ s->random ^ swab(ptr_addr);\n#else\n\tencoded = (unsigned long)ptr;\n#endif\n\treturn (freeptr_t){.v = encoded};\n}\n\nstatic inline void *freelist_ptr_decode(const struct kmem_cache *s,\n\t\t\t\t\tfreeptr_t ptr, unsigned long ptr_addr)\n{\n\tvoid *decoded;\n\n#ifdef CONFIG_SLAB_FREELIST_HARDENED\n\tdecoded = (void *)(ptr.v ^ s->random ^ swab(ptr_addr));\n#else\n\tdecoded = (void *)ptr.v;\n#endif\n\treturn decoded;\n}\n\nstatic inline void *get_freepointer(struct kmem_cache *s, void *object)\n{\n\tunsigned long ptr_addr;\n\tfreeptr_t p;\n\n\tobject = kasan_reset_tag(object);\n\tptr_addr = (unsigned long)object + s->offset;\n\tp = *(freeptr_t *)(ptr_addr);\n\treturn freelist_ptr_decode(s, p, ptr_addr);\n}\n\n#ifndef CONFIG_SLUB_TINY\nstatic void prefetch_freepointer(const struct kmem_cache *s, void *object)\n{\n\tprefetchw(object + s->offset);\n}\n#endif\n\n \n__no_kmsan_checks\nstatic inline void *get_freepointer_safe(struct kmem_cache *s, void *object)\n{\n\tunsigned long freepointer_addr;\n\tfreeptr_t p;\n\n\tif (!debug_pagealloc_enabled_static())\n\t\treturn get_freepointer(s, object);\n\n\tobject = kasan_reset_tag(object);\n\tfreepointer_addr = (unsigned long)object + s->offset;\n\tcopy_from_kernel_nofault(&p, (freeptr_t *)freepointer_addr, sizeof(p));\n\treturn freelist_ptr_decode(s, p, freepointer_addr);\n}\n\nstatic inline void set_freepointer(struct kmem_cache *s, void *object, void *fp)\n{\n\tunsigned long freeptr_addr = (unsigned long)object + s->offset;\n\n#ifdef CONFIG_SLAB_FREELIST_HARDENED\n\tBUG_ON(object == fp);  \n#endif\n\n\tfreeptr_addr = (unsigned long)kasan_reset_tag((void *)freeptr_addr);\n\t*(freeptr_t *)freeptr_addr = freelist_ptr_encode(s, fp, freeptr_addr);\n}\n\n \n#define for_each_object(__p, __s, __addr, __objects) \\\n\tfor (__p = fixup_red_left(__s, __addr); \\\n\t\t__p < (__addr) + (__objects) * (__s)->size; \\\n\t\t__p += (__s)->size)\n\nstatic inline unsigned int order_objects(unsigned int order, unsigned int size)\n{\n\treturn ((unsigned int)PAGE_SIZE << order) / size;\n}\n\nstatic inline struct kmem_cache_order_objects oo_make(unsigned int order,\n\t\tunsigned int size)\n{\n\tstruct kmem_cache_order_objects x = {\n\t\t(order << OO_SHIFT) + order_objects(order, size)\n\t};\n\n\treturn x;\n}\n\nstatic inline unsigned int oo_order(struct kmem_cache_order_objects x)\n{\n\treturn x.x >> OO_SHIFT;\n}\n\nstatic inline unsigned int oo_objects(struct kmem_cache_order_objects x)\n{\n\treturn x.x & OO_MASK;\n}\n\n#ifdef CONFIG_SLUB_CPU_PARTIAL\nstatic void slub_set_cpu_partial(struct kmem_cache *s, unsigned int nr_objects)\n{\n\tunsigned int nr_slabs;\n\n\ts->cpu_partial = nr_objects;\n\n\t \n\tnr_slabs = DIV_ROUND_UP(nr_objects * 2, oo_objects(s->oo));\n\ts->cpu_partial_slabs = nr_slabs;\n}\n#else\nstatic inline void\nslub_set_cpu_partial(struct kmem_cache *s, unsigned int nr_objects)\n{\n}\n#endif  \n\n \nstatic __always_inline void slab_lock(struct slab *slab)\n{\n\tstruct page *page = slab_page(slab);\n\n\tVM_BUG_ON_PAGE(PageTail(page), page);\n\tbit_spin_lock(PG_locked, &page->flags);\n}\n\nstatic __always_inline void slab_unlock(struct slab *slab)\n{\n\tstruct page *page = slab_page(slab);\n\n\tVM_BUG_ON_PAGE(PageTail(page), page);\n\t__bit_spin_unlock(PG_locked, &page->flags);\n}\n\nstatic inline bool\n__update_freelist_fast(struct slab *slab,\n\t\t      void *freelist_old, unsigned long counters_old,\n\t\t      void *freelist_new, unsigned long counters_new)\n{\n#ifdef system_has_freelist_aba\n\tfreelist_aba_t old = { .freelist = freelist_old, .counter = counters_old };\n\tfreelist_aba_t new = { .freelist = freelist_new, .counter = counters_new };\n\n\treturn try_cmpxchg_freelist(&slab->freelist_counter.full, &old.full, new.full);\n#else\n\treturn false;\n#endif\n}\n\nstatic inline bool\n__update_freelist_slow(struct slab *slab,\n\t\t      void *freelist_old, unsigned long counters_old,\n\t\t      void *freelist_new, unsigned long counters_new)\n{\n\tbool ret = false;\n\n\tslab_lock(slab);\n\tif (slab->freelist == freelist_old &&\n\t    slab->counters == counters_old) {\n\t\tslab->freelist = freelist_new;\n\t\tslab->counters = counters_new;\n\t\tret = true;\n\t}\n\tslab_unlock(slab);\n\n\treturn ret;\n}\n\n \nstatic inline bool __slab_update_freelist(struct kmem_cache *s, struct slab *slab,\n\t\tvoid *freelist_old, unsigned long counters_old,\n\t\tvoid *freelist_new, unsigned long counters_new,\n\t\tconst char *n)\n{\n\tbool ret;\n\n\tif (USE_LOCKLESS_FAST_PATH())\n\t\tlockdep_assert_irqs_disabled();\n\n\tif (s->flags & __CMPXCHG_DOUBLE) {\n\t\tret = __update_freelist_fast(slab, freelist_old, counters_old,\n\t\t\t\t            freelist_new, counters_new);\n\t} else {\n\t\tret = __update_freelist_slow(slab, freelist_old, counters_old,\n\t\t\t\t            freelist_new, counters_new);\n\t}\n\tif (likely(ret))\n\t\treturn true;\n\n\tcpu_relax();\n\tstat(s, CMPXCHG_DOUBLE_FAIL);\n\n#ifdef SLUB_DEBUG_CMPXCHG\n\tpr_info(\"%s %s: cmpxchg double redo \", n, s->name);\n#endif\n\n\treturn false;\n}\n\nstatic inline bool slab_update_freelist(struct kmem_cache *s, struct slab *slab,\n\t\tvoid *freelist_old, unsigned long counters_old,\n\t\tvoid *freelist_new, unsigned long counters_new,\n\t\tconst char *n)\n{\n\tbool ret;\n\n\tif (s->flags & __CMPXCHG_DOUBLE) {\n\t\tret = __update_freelist_fast(slab, freelist_old, counters_old,\n\t\t\t\t            freelist_new, counters_new);\n\t} else {\n\t\tunsigned long flags;\n\n\t\tlocal_irq_save(flags);\n\t\tret = __update_freelist_slow(slab, freelist_old, counters_old,\n\t\t\t\t            freelist_new, counters_new);\n\t\tlocal_irq_restore(flags);\n\t}\n\tif (likely(ret))\n\t\treturn true;\n\n\tcpu_relax();\n\tstat(s, CMPXCHG_DOUBLE_FAIL);\n\n#ifdef SLUB_DEBUG_CMPXCHG\n\tpr_info(\"%s %s: cmpxchg double redo \", n, s->name);\n#endif\n\n\treturn false;\n}\n\n#ifdef CONFIG_SLUB_DEBUG\nstatic unsigned long object_map[BITS_TO_LONGS(MAX_OBJS_PER_PAGE)];\nstatic DEFINE_SPINLOCK(object_map_lock);\n\nstatic void __fill_map(unsigned long *obj_map, struct kmem_cache *s,\n\t\t       struct slab *slab)\n{\n\tvoid *addr = slab_address(slab);\n\tvoid *p;\n\n\tbitmap_zero(obj_map, slab->objects);\n\n\tfor (p = slab->freelist; p; p = get_freepointer(s, p))\n\t\tset_bit(__obj_to_index(s, addr, p), obj_map);\n}\n\n#if IS_ENABLED(CONFIG_KUNIT)\nstatic bool slab_add_kunit_errors(void)\n{\n\tstruct kunit_resource *resource;\n\n\tif (!kunit_get_current_test())\n\t\treturn false;\n\n\tresource = kunit_find_named_resource(current->kunit_test, \"slab_errors\");\n\tif (!resource)\n\t\treturn false;\n\n\t(*(int *)resource->data)++;\n\tkunit_put_resource(resource);\n\treturn true;\n}\n#else\nstatic inline bool slab_add_kunit_errors(void) { return false; }\n#endif\n\nstatic inline unsigned int size_from_object(struct kmem_cache *s)\n{\n\tif (s->flags & SLAB_RED_ZONE)\n\t\treturn s->size - s->red_left_pad;\n\n\treturn s->size;\n}\n\nstatic inline void *restore_red_left(struct kmem_cache *s, void *p)\n{\n\tif (s->flags & SLAB_RED_ZONE)\n\t\tp -= s->red_left_pad;\n\n\treturn p;\n}\n\n \n#if defined(CONFIG_SLUB_DEBUG_ON)\nstatic slab_flags_t slub_debug = DEBUG_DEFAULT_FLAGS;\n#else\nstatic slab_flags_t slub_debug;\n#endif\n\nstatic char *slub_debug_string;\nstatic int disable_higher_order_debug;\n\n \nstatic inline void metadata_access_enable(void)\n{\n\tkasan_disable_current();\n}\n\nstatic inline void metadata_access_disable(void)\n{\n\tkasan_enable_current();\n}\n\n \n\n \nstatic inline int check_valid_pointer(struct kmem_cache *s,\n\t\t\t\tstruct slab *slab, void *object)\n{\n\tvoid *base;\n\n\tif (!object)\n\t\treturn 1;\n\n\tbase = slab_address(slab);\n\tobject = kasan_reset_tag(object);\n\tobject = restore_red_left(s, object);\n\tif (object < base || object >= base + slab->objects * s->size ||\n\t\t(object - base) % s->size) {\n\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\nstatic void print_section(char *level, char *text, u8 *addr,\n\t\t\t  unsigned int length)\n{\n\tmetadata_access_enable();\n\tprint_hex_dump(level, text, DUMP_PREFIX_ADDRESS,\n\t\t\t16, 1, kasan_reset_tag((void *)addr), length, 1);\n\tmetadata_access_disable();\n}\n\n \nstatic inline bool freeptr_outside_object(struct kmem_cache *s)\n{\n\treturn s->offset >= s->inuse;\n}\n\n \nstatic inline unsigned int get_info_end(struct kmem_cache *s)\n{\n\tif (freeptr_outside_object(s))\n\t\treturn s->inuse + sizeof(void *);\n\telse\n\t\treturn s->inuse;\n}\n\nstatic struct track *get_track(struct kmem_cache *s, void *object,\n\tenum track_item alloc)\n{\n\tstruct track *p;\n\n\tp = object + get_info_end(s);\n\n\treturn kasan_reset_tag(p + alloc);\n}\n\n#ifdef CONFIG_STACKDEPOT\nstatic noinline depot_stack_handle_t set_track_prepare(void)\n{\n\tdepot_stack_handle_t handle;\n\tunsigned long entries[TRACK_ADDRS_COUNT];\n\tunsigned int nr_entries;\n\n\tnr_entries = stack_trace_save(entries, ARRAY_SIZE(entries), 3);\n\thandle = stack_depot_save(entries, nr_entries, GFP_NOWAIT);\n\n\treturn handle;\n}\n#else\nstatic inline depot_stack_handle_t set_track_prepare(void)\n{\n\treturn 0;\n}\n#endif\n\nstatic void set_track_update(struct kmem_cache *s, void *object,\n\t\t\t     enum track_item alloc, unsigned long addr,\n\t\t\t     depot_stack_handle_t handle)\n{\n\tstruct track *p = get_track(s, object, alloc);\n\n#ifdef CONFIG_STACKDEPOT\n\tp->handle = handle;\n#endif\n\tp->addr = addr;\n\tp->cpu = smp_processor_id();\n\tp->pid = current->pid;\n\tp->when = jiffies;\n}\n\nstatic __always_inline void set_track(struct kmem_cache *s, void *object,\n\t\t\t\t      enum track_item alloc, unsigned long addr)\n{\n\tdepot_stack_handle_t handle = set_track_prepare();\n\n\tset_track_update(s, object, alloc, addr, handle);\n}\n\nstatic void init_tracking(struct kmem_cache *s, void *object)\n{\n\tstruct track *p;\n\n\tif (!(s->flags & SLAB_STORE_USER))\n\t\treturn;\n\n\tp = get_track(s, object, TRACK_ALLOC);\n\tmemset(p, 0, 2*sizeof(struct track));\n}\n\nstatic void print_track(const char *s, struct track *t, unsigned long pr_time)\n{\n\tdepot_stack_handle_t handle __maybe_unused;\n\n\tif (!t->addr)\n\t\treturn;\n\n\tpr_err(\"%s in %pS age=%lu cpu=%u pid=%d\\n\",\n\t       s, (void *)t->addr, pr_time - t->when, t->cpu, t->pid);\n#ifdef CONFIG_STACKDEPOT\n\thandle = READ_ONCE(t->handle);\n\tif (handle)\n\t\tstack_depot_print(handle);\n\telse\n\t\tpr_err(\"object allocation/free stack trace missing\\n\");\n#endif\n}\n\nvoid print_tracking(struct kmem_cache *s, void *object)\n{\n\tunsigned long pr_time = jiffies;\n\tif (!(s->flags & SLAB_STORE_USER))\n\t\treturn;\n\n\tprint_track(\"Allocated\", get_track(s, object, TRACK_ALLOC), pr_time);\n\tprint_track(\"Freed\", get_track(s, object, TRACK_FREE), pr_time);\n}\n\nstatic void print_slab_info(const struct slab *slab)\n{\n\tstruct folio *folio = (struct folio *)slab_folio(slab);\n\n\tpr_err(\"Slab 0x%p objects=%u used=%u fp=0x%p flags=%pGp\\n\",\n\t       slab, slab->objects, slab->inuse, slab->freelist,\n\t       folio_flags(folio, 0));\n}\n\n \nstatic inline void set_orig_size(struct kmem_cache *s,\n\t\t\t\tvoid *object, unsigned int orig_size)\n{\n\tvoid *p = kasan_reset_tag(object);\n\n\tif (!slub_debug_orig_size(s))\n\t\treturn;\n\n#ifdef CONFIG_KASAN_GENERIC\n\t \n\tif (kasan_metadata_size(s, true) > orig_size)\n\t\torig_size = s->object_size;\n#endif\n\n\tp += get_info_end(s);\n\tp += sizeof(struct track) * 2;\n\n\t*(unsigned int *)p = orig_size;\n}\n\nstatic inline unsigned int get_orig_size(struct kmem_cache *s, void *object)\n{\n\tvoid *p = kasan_reset_tag(object);\n\n\tif (!slub_debug_orig_size(s))\n\t\treturn s->object_size;\n\n\tp += get_info_end(s);\n\tp += sizeof(struct track) * 2;\n\n\treturn *(unsigned int *)p;\n}\n\nvoid skip_orig_size_check(struct kmem_cache *s, const void *object)\n{\n\tset_orig_size(s, (void *)object, s->object_size);\n}\n\nstatic void slab_bug(struct kmem_cache *s, char *fmt, ...)\n{\n\tstruct va_format vaf;\n\tva_list args;\n\n\tva_start(args, fmt);\n\tvaf.fmt = fmt;\n\tvaf.va = &args;\n\tpr_err(\"=============================================================================\\n\");\n\tpr_err(\"BUG %s (%s): %pV\\n\", s->name, print_tainted(), &vaf);\n\tpr_err(\"-----------------------------------------------------------------------------\\n\\n\");\n\tva_end(args);\n}\n\n__printf(2, 3)\nstatic void slab_fix(struct kmem_cache *s, char *fmt, ...)\n{\n\tstruct va_format vaf;\n\tva_list args;\n\n\tif (slab_add_kunit_errors())\n\t\treturn;\n\n\tva_start(args, fmt);\n\tvaf.fmt = fmt;\n\tvaf.va = &args;\n\tpr_err(\"FIX %s: %pV\\n\", s->name, &vaf);\n\tva_end(args);\n}\n\nstatic void print_trailer(struct kmem_cache *s, struct slab *slab, u8 *p)\n{\n\tunsigned int off;\t \n\tu8 *addr = slab_address(slab);\n\n\tprint_tracking(s, p);\n\n\tprint_slab_info(slab);\n\n\tpr_err(\"Object 0x%p @offset=%tu fp=0x%p\\n\\n\",\n\t       p, p - addr, get_freepointer(s, p));\n\n\tif (s->flags & SLAB_RED_ZONE)\n\t\tprint_section(KERN_ERR, \"Redzone  \", p - s->red_left_pad,\n\t\t\t      s->red_left_pad);\n\telse if (p > addr + 16)\n\t\tprint_section(KERN_ERR, \"Bytes b4 \", p - 16, 16);\n\n\tprint_section(KERN_ERR,         \"Object   \", p,\n\t\t      min_t(unsigned int, s->object_size, PAGE_SIZE));\n\tif (s->flags & SLAB_RED_ZONE)\n\t\tprint_section(KERN_ERR, \"Redzone  \", p + s->object_size,\n\t\t\ts->inuse - s->object_size);\n\n\toff = get_info_end(s);\n\n\tif (s->flags & SLAB_STORE_USER)\n\t\toff += 2 * sizeof(struct track);\n\n\tif (slub_debug_orig_size(s))\n\t\toff += sizeof(unsigned int);\n\n\toff += kasan_metadata_size(s, false);\n\n\tif (off != size_from_object(s))\n\t\t \n\t\tprint_section(KERN_ERR, \"Padding  \", p + off,\n\t\t\t      size_from_object(s) - off);\n\n\tdump_stack();\n}\n\nstatic void object_err(struct kmem_cache *s, struct slab *slab,\n\t\t\tu8 *object, char *reason)\n{\n\tif (slab_add_kunit_errors())\n\t\treturn;\n\n\tslab_bug(s, \"%s\", reason);\n\tprint_trailer(s, slab, object);\n\tadd_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);\n}\n\nstatic bool freelist_corrupted(struct kmem_cache *s, struct slab *slab,\n\t\t\t       void **freelist, void *nextfree)\n{\n\tif ((s->flags & SLAB_CONSISTENCY_CHECKS) &&\n\t    !check_valid_pointer(s, slab, nextfree) && freelist) {\n\t\tobject_err(s, slab, *freelist, \"Freechain corrupt\");\n\t\t*freelist = NULL;\n\t\tslab_fix(s, \"Isolate corrupted freechain\");\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic __printf(3, 4) void slab_err(struct kmem_cache *s, struct slab *slab,\n\t\t\tconst char *fmt, ...)\n{\n\tva_list args;\n\tchar buf[100];\n\n\tif (slab_add_kunit_errors())\n\t\treturn;\n\n\tva_start(args, fmt);\n\tvsnprintf(buf, sizeof(buf), fmt, args);\n\tva_end(args);\n\tslab_bug(s, \"%s\", buf);\n\tprint_slab_info(slab);\n\tdump_stack();\n\tadd_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);\n}\n\nstatic void init_object(struct kmem_cache *s, void *object, u8 val)\n{\n\tu8 *p = kasan_reset_tag(object);\n\tunsigned int poison_size = s->object_size;\n\n\tif (s->flags & SLAB_RED_ZONE) {\n\t\tmemset(p - s->red_left_pad, val, s->red_left_pad);\n\n\t\tif (slub_debug_orig_size(s) && val == SLUB_RED_ACTIVE) {\n\t\t\t \n\t\t\tpoison_size = get_orig_size(s, object);\n\t\t}\n\t}\n\n\tif (s->flags & __OBJECT_POISON) {\n\t\tmemset(p, POISON_FREE, poison_size - 1);\n\t\tp[poison_size - 1] = POISON_END;\n\t}\n\n\tif (s->flags & SLAB_RED_ZONE)\n\t\tmemset(p + poison_size, val, s->inuse - poison_size);\n}\n\nstatic void restore_bytes(struct kmem_cache *s, char *message, u8 data,\n\t\t\t\t\t\tvoid *from, void *to)\n{\n\tslab_fix(s, \"Restoring %s 0x%p-0x%p=0x%x\", message, from, to - 1, data);\n\tmemset(from, data, to - from);\n}\n\nstatic int check_bytes_and_report(struct kmem_cache *s, struct slab *slab,\n\t\t\tu8 *object, char *what,\n\t\t\tu8 *start, unsigned int value, unsigned int bytes)\n{\n\tu8 *fault;\n\tu8 *end;\n\tu8 *addr = slab_address(slab);\n\n\tmetadata_access_enable();\n\tfault = memchr_inv(kasan_reset_tag(start), value, bytes);\n\tmetadata_access_disable();\n\tif (!fault)\n\t\treturn 1;\n\n\tend = start + bytes;\n\twhile (end > fault && end[-1] == value)\n\t\tend--;\n\n\tif (slab_add_kunit_errors())\n\t\tgoto skip_bug_print;\n\n\tslab_bug(s, \"%s overwritten\", what);\n\tpr_err(\"0x%p-0x%p @offset=%tu. First byte 0x%x instead of 0x%x\\n\",\n\t\t\t\t\tfault, end - 1, fault - addr,\n\t\t\t\t\tfault[0], value);\n\tprint_trailer(s, slab, object);\n\tadd_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);\n\nskip_bug_print:\n\trestore_bytes(s, what, value, fault, end);\n\treturn 0;\n}\n\n \n\nstatic int check_pad_bytes(struct kmem_cache *s, struct slab *slab, u8 *p)\n{\n\tunsigned long off = get_info_end(s);\t \n\n\tif (s->flags & SLAB_STORE_USER) {\n\t\t \n\t\toff += 2 * sizeof(struct track);\n\n\t\tif (s->flags & SLAB_KMALLOC)\n\t\t\toff += sizeof(unsigned int);\n\t}\n\n\toff += kasan_metadata_size(s, false);\n\n\tif (size_from_object(s) == off)\n\t\treturn 1;\n\n\treturn check_bytes_and_report(s, slab, p, \"Object padding\",\n\t\t\tp + off, POISON_INUSE, size_from_object(s) - off);\n}\n\n \nstatic void slab_pad_check(struct kmem_cache *s, struct slab *slab)\n{\n\tu8 *start;\n\tu8 *fault;\n\tu8 *end;\n\tu8 *pad;\n\tint length;\n\tint remainder;\n\n\tif (!(s->flags & SLAB_POISON))\n\t\treturn;\n\n\tstart = slab_address(slab);\n\tlength = slab_size(slab);\n\tend = start + length;\n\tremainder = length % s->size;\n\tif (!remainder)\n\t\treturn;\n\n\tpad = end - remainder;\n\tmetadata_access_enable();\n\tfault = memchr_inv(kasan_reset_tag(pad), POISON_INUSE, remainder);\n\tmetadata_access_disable();\n\tif (!fault)\n\t\treturn;\n\twhile (end > fault && end[-1] == POISON_INUSE)\n\t\tend--;\n\n\tslab_err(s, slab, \"Padding overwritten. 0x%p-0x%p @offset=%tu\",\n\t\t\tfault, end - 1, fault - start);\n\tprint_section(KERN_ERR, \"Padding \", pad, remainder);\n\n\trestore_bytes(s, \"slab padding\", POISON_INUSE, fault, end);\n}\n\nstatic int check_object(struct kmem_cache *s, struct slab *slab,\n\t\t\t\t\tvoid *object, u8 val)\n{\n\tu8 *p = object;\n\tu8 *endobject = object + s->object_size;\n\tunsigned int orig_size;\n\n\tif (s->flags & SLAB_RED_ZONE) {\n\t\tif (!check_bytes_and_report(s, slab, object, \"Left Redzone\",\n\t\t\tobject - s->red_left_pad, val, s->red_left_pad))\n\t\t\treturn 0;\n\n\t\tif (!check_bytes_and_report(s, slab, object, \"Right Redzone\",\n\t\t\tendobject, val, s->inuse - s->object_size))\n\t\t\treturn 0;\n\n\t\tif (slub_debug_orig_size(s) && val == SLUB_RED_ACTIVE) {\n\t\t\torig_size = get_orig_size(s, object);\n\n\t\t\tif (s->object_size > orig_size  &&\n\t\t\t\t!check_bytes_and_report(s, slab, object,\n\t\t\t\t\t\"kmalloc Redzone\", p + orig_size,\n\t\t\t\t\tval, s->object_size - orig_size)) {\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tif ((s->flags & SLAB_POISON) && s->object_size < s->inuse) {\n\t\t\tcheck_bytes_and_report(s, slab, p, \"Alignment padding\",\n\t\t\t\tendobject, POISON_INUSE,\n\t\t\t\ts->inuse - s->object_size);\n\t\t}\n\t}\n\n\tif (s->flags & SLAB_POISON) {\n\t\tif (val != SLUB_RED_ACTIVE && (s->flags & __OBJECT_POISON) &&\n\t\t\t(!check_bytes_and_report(s, slab, p, \"Poison\", p,\n\t\t\t\t\tPOISON_FREE, s->object_size - 1) ||\n\t\t\t !check_bytes_and_report(s, slab, p, \"End Poison\",\n\t\t\t\tp + s->object_size - 1, POISON_END, 1)))\n\t\t\treturn 0;\n\t\t \n\t\tcheck_pad_bytes(s, slab, p);\n\t}\n\n\tif (!freeptr_outside_object(s) && val == SLUB_RED_ACTIVE)\n\t\t \n\t\treturn 1;\n\n\t \n\tif (!check_valid_pointer(s, slab, get_freepointer(s, p))) {\n\t\tobject_err(s, slab, p, \"Freepointer corrupt\");\n\t\t \n\t\tset_freepointer(s, p, NULL);\n\t\treturn 0;\n\t}\n\treturn 1;\n}\n\nstatic int check_slab(struct kmem_cache *s, struct slab *slab)\n{\n\tint maxobj;\n\n\tif (!folio_test_slab(slab_folio(slab))) {\n\t\tslab_err(s, slab, \"Not a valid slab page\");\n\t\treturn 0;\n\t}\n\n\tmaxobj = order_objects(slab_order(slab), s->size);\n\tif (slab->objects > maxobj) {\n\t\tslab_err(s, slab, \"objects %u > max %u\",\n\t\t\tslab->objects, maxobj);\n\t\treturn 0;\n\t}\n\tif (slab->inuse > slab->objects) {\n\t\tslab_err(s, slab, \"inuse %u > max %u\",\n\t\t\tslab->inuse, slab->objects);\n\t\treturn 0;\n\t}\n\t \n\tslab_pad_check(s, slab);\n\treturn 1;\n}\n\n \nstatic int on_freelist(struct kmem_cache *s, struct slab *slab, void *search)\n{\n\tint nr = 0;\n\tvoid *fp;\n\tvoid *object = NULL;\n\tint max_objects;\n\n\tfp = slab->freelist;\n\twhile (fp && nr <= slab->objects) {\n\t\tif (fp == search)\n\t\t\treturn 1;\n\t\tif (!check_valid_pointer(s, slab, fp)) {\n\t\t\tif (object) {\n\t\t\t\tobject_err(s, slab, object,\n\t\t\t\t\t\"Freechain corrupt\");\n\t\t\t\tset_freepointer(s, object, NULL);\n\t\t\t} else {\n\t\t\t\tslab_err(s, slab, \"Freepointer corrupt\");\n\t\t\t\tslab->freelist = NULL;\n\t\t\t\tslab->inuse = slab->objects;\n\t\t\t\tslab_fix(s, \"Freelist cleared\");\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\tobject = fp;\n\t\tfp = get_freepointer(s, object);\n\t\tnr++;\n\t}\n\n\tmax_objects = order_objects(slab_order(slab), s->size);\n\tif (max_objects > MAX_OBJS_PER_PAGE)\n\t\tmax_objects = MAX_OBJS_PER_PAGE;\n\n\tif (slab->objects != max_objects) {\n\t\tslab_err(s, slab, \"Wrong number of objects. Found %d but should be %d\",\n\t\t\t slab->objects, max_objects);\n\t\tslab->objects = max_objects;\n\t\tslab_fix(s, \"Number of objects adjusted\");\n\t}\n\tif (slab->inuse != slab->objects - nr) {\n\t\tslab_err(s, slab, \"Wrong object count. Counter is %d but counted were %d\",\n\t\t\t slab->inuse, slab->objects - nr);\n\t\tslab->inuse = slab->objects - nr;\n\t\tslab_fix(s, \"Object count adjusted\");\n\t}\n\treturn search == NULL;\n}\n\nstatic void trace(struct kmem_cache *s, struct slab *slab, void *object,\n\t\t\t\t\t\t\t\tint alloc)\n{\n\tif (s->flags & SLAB_TRACE) {\n\t\tpr_info(\"TRACE %s %s 0x%p inuse=%d fp=0x%p\\n\",\n\t\t\ts->name,\n\t\t\talloc ? \"alloc\" : \"free\",\n\t\t\tobject, slab->inuse,\n\t\t\tslab->freelist);\n\n\t\tif (!alloc)\n\t\t\tprint_section(KERN_INFO, \"Object \", (void *)object,\n\t\t\t\t\ts->object_size);\n\n\t\tdump_stack();\n\t}\n}\n\n \nstatic void add_full(struct kmem_cache *s,\n\tstruct kmem_cache_node *n, struct slab *slab)\n{\n\tif (!(s->flags & SLAB_STORE_USER))\n\t\treturn;\n\n\tlockdep_assert_held(&n->list_lock);\n\tlist_add(&slab->slab_list, &n->full);\n}\n\nstatic void remove_full(struct kmem_cache *s, struct kmem_cache_node *n, struct slab *slab)\n{\n\tif (!(s->flags & SLAB_STORE_USER))\n\t\treturn;\n\n\tlockdep_assert_held(&n->list_lock);\n\tlist_del(&slab->slab_list);\n}\n\nstatic inline unsigned long node_nr_slabs(struct kmem_cache_node *n)\n{\n\treturn atomic_long_read(&n->nr_slabs);\n}\n\nstatic inline void inc_slabs_node(struct kmem_cache *s, int node, int objects)\n{\n\tstruct kmem_cache_node *n = get_node(s, node);\n\n\t \n\tif (likely(n)) {\n\t\tatomic_long_inc(&n->nr_slabs);\n\t\tatomic_long_add(objects, &n->total_objects);\n\t}\n}\nstatic inline void dec_slabs_node(struct kmem_cache *s, int node, int objects)\n{\n\tstruct kmem_cache_node *n = get_node(s, node);\n\n\tatomic_long_dec(&n->nr_slabs);\n\tatomic_long_sub(objects, &n->total_objects);\n}\n\n \nstatic void setup_object_debug(struct kmem_cache *s, void *object)\n{\n\tif (!kmem_cache_debug_flags(s, SLAB_STORE_USER|SLAB_RED_ZONE|__OBJECT_POISON))\n\t\treturn;\n\n\tinit_object(s, object, SLUB_RED_INACTIVE);\n\tinit_tracking(s, object);\n}\n\nstatic\nvoid setup_slab_debug(struct kmem_cache *s, struct slab *slab, void *addr)\n{\n\tif (!kmem_cache_debug_flags(s, SLAB_POISON))\n\t\treturn;\n\n\tmetadata_access_enable();\n\tmemset(kasan_reset_tag(addr), POISON_INUSE, slab_size(slab));\n\tmetadata_access_disable();\n}\n\nstatic inline int alloc_consistency_checks(struct kmem_cache *s,\n\t\t\t\t\tstruct slab *slab, void *object)\n{\n\tif (!check_slab(s, slab))\n\t\treturn 0;\n\n\tif (!check_valid_pointer(s, slab, object)) {\n\t\tobject_err(s, slab, object, \"Freelist Pointer check fails\");\n\t\treturn 0;\n\t}\n\n\tif (!check_object(s, slab, object, SLUB_RED_INACTIVE))\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic noinline bool alloc_debug_processing(struct kmem_cache *s,\n\t\t\tstruct slab *slab, void *object, int orig_size)\n{\n\tif (s->flags & SLAB_CONSISTENCY_CHECKS) {\n\t\tif (!alloc_consistency_checks(s, slab, object))\n\t\t\tgoto bad;\n\t}\n\n\t \n\ttrace(s, slab, object, 1);\n\tset_orig_size(s, object, orig_size);\n\tinit_object(s, object, SLUB_RED_ACTIVE);\n\treturn true;\n\nbad:\n\tif (folio_test_slab(slab_folio(slab))) {\n\t\t \n\t\tslab_fix(s, \"Marking all objects used\");\n\t\tslab->inuse = slab->objects;\n\t\tslab->freelist = NULL;\n\t}\n\treturn false;\n}\n\nstatic inline int free_consistency_checks(struct kmem_cache *s,\n\t\tstruct slab *slab, void *object, unsigned long addr)\n{\n\tif (!check_valid_pointer(s, slab, object)) {\n\t\tslab_err(s, slab, \"Invalid object pointer 0x%p\", object);\n\t\treturn 0;\n\t}\n\n\tif (on_freelist(s, slab, object)) {\n\t\tobject_err(s, slab, object, \"Object already free\");\n\t\treturn 0;\n\t}\n\n\tif (!check_object(s, slab, object, SLUB_RED_ACTIVE))\n\t\treturn 0;\n\n\tif (unlikely(s != slab->slab_cache)) {\n\t\tif (!folio_test_slab(slab_folio(slab))) {\n\t\t\tslab_err(s, slab, \"Attempt to free object(0x%p) outside of slab\",\n\t\t\t\t object);\n\t\t} else if (!slab->slab_cache) {\n\t\t\tpr_err(\"SLUB <none>: no slab for object 0x%p.\\n\",\n\t\t\t       object);\n\t\t\tdump_stack();\n\t\t} else\n\t\t\tobject_err(s, slab, object,\n\t\t\t\t\t\"page slab pointer corrupt.\");\n\t\treturn 0;\n\t}\n\treturn 1;\n}\n\n \nstatic char *\nparse_slub_debug_flags(char *str, slab_flags_t *flags, char **slabs, bool init)\n{\n\tbool higher_order_disable = false;\n\n\t \n\twhile (*str && *str == ';')\n\t\tstr++;\n\n\tif (*str == ',') {\n\t\t \n\t\t*flags = DEBUG_DEFAULT_FLAGS;\n\t\tgoto check_slabs;\n\t}\n\t*flags = 0;\n\n\t \n\tfor (; *str && *str != ',' && *str != ';'; str++) {\n\t\tswitch (tolower(*str)) {\n\t\tcase '-':\n\t\t\t*flags = 0;\n\t\t\tbreak;\n\t\tcase 'f':\n\t\t\t*flags |= SLAB_CONSISTENCY_CHECKS;\n\t\t\tbreak;\n\t\tcase 'z':\n\t\t\t*flags |= SLAB_RED_ZONE;\n\t\t\tbreak;\n\t\tcase 'p':\n\t\t\t*flags |= SLAB_POISON;\n\t\t\tbreak;\n\t\tcase 'u':\n\t\t\t*flags |= SLAB_STORE_USER;\n\t\t\tbreak;\n\t\tcase 't':\n\t\t\t*flags |= SLAB_TRACE;\n\t\t\tbreak;\n\t\tcase 'a':\n\t\t\t*flags |= SLAB_FAILSLAB;\n\t\t\tbreak;\n\t\tcase 'o':\n\t\t\t \n\t\t\thigher_order_disable = true;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tif (init)\n\t\t\t\tpr_err(\"slub_debug option '%c' unknown. skipped\\n\", *str);\n\t\t}\n\t}\ncheck_slabs:\n\tif (*str == ',')\n\t\t*slabs = ++str;\n\telse\n\t\t*slabs = NULL;\n\n\t \n\twhile (*str && *str != ';')\n\t\tstr++;\n\n\t \n\twhile (*str && *str == ';')\n\t\tstr++;\n\n\tif (init && higher_order_disable)\n\t\tdisable_higher_order_debug = 1;\n\n\tif (*str)\n\t\treturn str;\n\telse\n\t\treturn NULL;\n}\n\nstatic int __init setup_slub_debug(char *str)\n{\n\tslab_flags_t flags;\n\tslab_flags_t global_flags;\n\tchar *saved_str;\n\tchar *slab_list;\n\tbool global_slub_debug_changed = false;\n\tbool slab_list_specified = false;\n\n\tglobal_flags = DEBUG_DEFAULT_FLAGS;\n\tif (*str++ != '=' || !*str)\n\t\t \n\t\tgoto out;\n\n\tsaved_str = str;\n\twhile (str) {\n\t\tstr = parse_slub_debug_flags(str, &flags, &slab_list, true);\n\n\t\tif (!slab_list) {\n\t\t\tglobal_flags = flags;\n\t\t\tglobal_slub_debug_changed = true;\n\t\t} else {\n\t\t\tslab_list_specified = true;\n\t\t\tif (flags & SLAB_STORE_USER)\n\t\t\t\tstack_depot_request_early_init();\n\t\t}\n\t}\n\n\t \n\tif (slab_list_specified) {\n\t\tif (!global_slub_debug_changed)\n\t\t\tglobal_flags = slub_debug;\n\t\tslub_debug_string = saved_str;\n\t}\nout:\n\tslub_debug = global_flags;\n\tif (slub_debug & SLAB_STORE_USER)\n\t\tstack_depot_request_early_init();\n\tif (slub_debug != 0 || slub_debug_string)\n\t\tstatic_branch_enable(&slub_debug_enabled);\n\telse\n\t\tstatic_branch_disable(&slub_debug_enabled);\n\tif ((static_branch_unlikely(&init_on_alloc) ||\n\t     static_branch_unlikely(&init_on_free)) &&\n\t    (slub_debug & SLAB_POISON))\n\t\tpr_info(\"mem auto-init: SLAB_POISON will take precedence over init_on_alloc/init_on_free\\n\");\n\treturn 1;\n}\n\n__setup(\"slub_debug\", setup_slub_debug);\n\n \nslab_flags_t kmem_cache_flags(unsigned int object_size,\n\tslab_flags_t flags, const char *name)\n{\n\tchar *iter;\n\tsize_t len;\n\tchar *next_block;\n\tslab_flags_t block_flags;\n\tslab_flags_t slub_debug_local = slub_debug;\n\n\tif (flags & SLAB_NO_USER_FLAGS)\n\t\treturn flags;\n\n\t \n\tif (flags & SLAB_NOLEAKTRACE)\n\t\tslub_debug_local &= ~SLAB_STORE_USER;\n\n\tlen = strlen(name);\n\tnext_block = slub_debug_string;\n\t \n\twhile (next_block) {\n\t\tnext_block = parse_slub_debug_flags(next_block, &block_flags, &iter, false);\n\t\tif (!iter)\n\t\t\tcontinue;\n\t\t \n\t\twhile (*iter) {\n\t\t\tchar *end, *glob;\n\t\t\tsize_t cmplen;\n\n\t\t\tend = strchrnul(iter, ',');\n\t\t\tif (next_block && next_block < end)\n\t\t\t\tend = next_block - 1;\n\n\t\t\tglob = strnchr(iter, end - iter, '*');\n\t\t\tif (glob)\n\t\t\t\tcmplen = glob - iter;\n\t\t\telse\n\t\t\t\tcmplen = max_t(size_t, len, (end - iter));\n\n\t\t\tif (!strncmp(name, iter, cmplen)) {\n\t\t\t\tflags |= block_flags;\n\t\t\t\treturn flags;\n\t\t\t}\n\n\t\t\tif (!*end || *end == ';')\n\t\t\t\tbreak;\n\t\t\titer = end + 1;\n\t\t}\n\t}\n\n\treturn flags | slub_debug_local;\n}\n#else  \nstatic inline void setup_object_debug(struct kmem_cache *s, void *object) {}\nstatic inline\nvoid setup_slab_debug(struct kmem_cache *s, struct slab *slab, void *addr) {}\n\nstatic inline bool alloc_debug_processing(struct kmem_cache *s,\n\tstruct slab *slab, void *object, int orig_size) { return true; }\n\nstatic inline bool free_debug_processing(struct kmem_cache *s,\n\tstruct slab *slab, void *head, void *tail, int *bulk_cnt,\n\tunsigned long addr, depot_stack_handle_t handle) { return true; }\n\nstatic inline void slab_pad_check(struct kmem_cache *s, struct slab *slab) {}\nstatic inline int check_object(struct kmem_cache *s, struct slab *slab,\n\t\t\tvoid *object, u8 val) { return 1; }\nstatic inline depot_stack_handle_t set_track_prepare(void) { return 0; }\nstatic inline void set_track(struct kmem_cache *s, void *object,\n\t\t\t     enum track_item alloc, unsigned long addr) {}\nstatic inline void add_full(struct kmem_cache *s, struct kmem_cache_node *n,\n\t\t\t\t\tstruct slab *slab) {}\nstatic inline void remove_full(struct kmem_cache *s, struct kmem_cache_node *n,\n\t\t\t\t\tstruct slab *slab) {}\nslab_flags_t kmem_cache_flags(unsigned int object_size,\n\tslab_flags_t flags, const char *name)\n{\n\treturn flags;\n}\n#define slub_debug 0\n\n#define disable_higher_order_debug 0\n\nstatic inline unsigned long node_nr_slabs(struct kmem_cache_node *n)\n\t\t\t\t\t\t\t{ return 0; }\nstatic inline void inc_slabs_node(struct kmem_cache *s, int node,\n\t\t\t\t\t\t\tint objects) {}\nstatic inline void dec_slabs_node(struct kmem_cache *s, int node,\n\t\t\t\t\t\t\tint objects) {}\n\n#ifndef CONFIG_SLUB_TINY\nstatic bool freelist_corrupted(struct kmem_cache *s, struct slab *slab,\n\t\t\t       void **freelist, void *nextfree)\n{\n\treturn false;\n}\n#endif\n#endif  \n\n \nstatic __always_inline bool slab_free_hook(struct kmem_cache *s,\n\t\t\t\t\t\tvoid *x, bool init)\n{\n\tkmemleak_free_recursive(x, s->flags);\n\tkmsan_slab_free(s, x);\n\n\tdebug_check_no_locks_freed(x, s->object_size);\n\n\tif (!(s->flags & SLAB_DEBUG_OBJECTS))\n\t\tdebug_check_no_obj_freed(x, s->object_size);\n\n\t \n\tif (!(s->flags & SLAB_TYPESAFE_BY_RCU))\n\t\t__kcsan_check_access(x, s->object_size,\n\t\t\t\t     KCSAN_ACCESS_WRITE | KCSAN_ACCESS_ASSERT);\n\n\t \n\tif (init) {\n\t\tint rsize;\n\n\t\tif (!kasan_has_integrated_init())\n\t\t\tmemset(kasan_reset_tag(x), 0, s->object_size);\n\t\trsize = (s->flags & SLAB_RED_ZONE) ? s->red_left_pad : 0;\n\t\tmemset((char *)kasan_reset_tag(x) + s->inuse, 0,\n\t\t       s->size - s->inuse - rsize);\n\t}\n\t \n\treturn kasan_slab_free(s, x, init);\n}\n\nstatic inline bool slab_free_freelist_hook(struct kmem_cache *s,\n\t\t\t\t\t   void **head, void **tail,\n\t\t\t\t\t   int *cnt)\n{\n\n\tvoid *object;\n\tvoid *next = *head;\n\tvoid *old_tail = *tail ? *tail : *head;\n\n\tif (is_kfence_address(next)) {\n\t\tslab_free_hook(s, next, false);\n\t\treturn true;\n\t}\n\n\t \n\t*head = NULL;\n\t*tail = NULL;\n\n\tdo {\n\t\tobject = next;\n\t\tnext = get_freepointer(s, object);\n\n\t\t \n\t\tif (!slab_free_hook(s, object, slab_want_init_on_free(s))) {\n\t\t\t \n\t\t\tset_freepointer(s, object, *head);\n\t\t\t*head = object;\n\t\t\tif (!*tail)\n\t\t\t\t*tail = object;\n\t\t} else {\n\t\t\t \n\t\t\t--(*cnt);\n\t\t}\n\t} while (object != old_tail);\n\n\tif (*head == *tail)\n\t\t*tail = NULL;\n\n\treturn *head != NULL;\n}\n\nstatic void *setup_object(struct kmem_cache *s, void *object)\n{\n\tsetup_object_debug(s, object);\n\tobject = kasan_init_slab_obj(s, object);\n\tif (unlikely(s->ctor)) {\n\t\tkasan_unpoison_object_data(s, object);\n\t\ts->ctor(object);\n\t\tkasan_poison_object_data(s, object);\n\t}\n\treturn object;\n}\n\n \nstatic inline struct slab *alloc_slab_page(gfp_t flags, int node,\n\t\tstruct kmem_cache_order_objects oo)\n{\n\tstruct folio *folio;\n\tstruct slab *slab;\n\tunsigned int order = oo_order(oo);\n\n\tif (node == NUMA_NO_NODE)\n\t\tfolio = (struct folio *)alloc_pages(flags, order);\n\telse\n\t\tfolio = (struct folio *)__alloc_pages_node(node, flags, order);\n\n\tif (!folio)\n\t\treturn NULL;\n\n\tslab = folio_slab(folio);\n\t__folio_set_slab(folio);\n\t \n\tsmp_wmb();\n\tif (folio_is_pfmemalloc(folio))\n\t\tslab_set_pfmemalloc(slab);\n\n\treturn slab;\n}\n\n#ifdef CONFIG_SLAB_FREELIST_RANDOM\n \nstatic int init_cache_random_seq(struct kmem_cache *s)\n{\n\tunsigned int count = oo_objects(s->oo);\n\tint err;\n\n\t \n\tif (s->random_seq)\n\t\treturn 0;\n\n\terr = cache_random_seq_create(s, count, GFP_KERNEL);\n\tif (err) {\n\t\tpr_err(\"SLUB: Unable to initialize free list for %s\\n\",\n\t\t\ts->name);\n\t\treturn err;\n\t}\n\n\t \n\tif (s->random_seq) {\n\t\tunsigned int i;\n\n\t\tfor (i = 0; i < count; i++)\n\t\t\ts->random_seq[i] *= s->size;\n\t}\n\treturn 0;\n}\n\n \nstatic void __init init_freelist_randomization(void)\n{\n\tstruct kmem_cache *s;\n\n\tmutex_lock(&slab_mutex);\n\n\tlist_for_each_entry(s, &slab_caches, list)\n\t\tinit_cache_random_seq(s);\n\n\tmutex_unlock(&slab_mutex);\n}\n\n \nstatic void *next_freelist_entry(struct kmem_cache *s, struct slab *slab,\n\t\t\t\tunsigned long *pos, void *start,\n\t\t\t\tunsigned long page_limit,\n\t\t\t\tunsigned long freelist_count)\n{\n\tunsigned int idx;\n\n\t \n\tdo {\n\t\tidx = s->random_seq[*pos];\n\t\t*pos += 1;\n\t\tif (*pos >= freelist_count)\n\t\t\t*pos = 0;\n\t} while (unlikely(idx >= page_limit));\n\n\treturn (char *)start + idx;\n}\n\n \nstatic bool shuffle_freelist(struct kmem_cache *s, struct slab *slab)\n{\n\tvoid *start;\n\tvoid *cur;\n\tvoid *next;\n\tunsigned long idx, pos, page_limit, freelist_count;\n\n\tif (slab->objects < 2 || !s->random_seq)\n\t\treturn false;\n\n\tfreelist_count = oo_objects(s->oo);\n\tpos = get_random_u32_below(freelist_count);\n\n\tpage_limit = slab->objects * s->size;\n\tstart = fixup_red_left(s, slab_address(slab));\n\n\t \n\tcur = next_freelist_entry(s, slab, &pos, start, page_limit,\n\t\t\t\tfreelist_count);\n\tcur = setup_object(s, cur);\n\tslab->freelist = cur;\n\n\tfor (idx = 1; idx < slab->objects; idx++) {\n\t\tnext = next_freelist_entry(s, slab, &pos, start, page_limit,\n\t\t\tfreelist_count);\n\t\tnext = setup_object(s, next);\n\t\tset_freepointer(s, cur, next);\n\t\tcur = next;\n\t}\n\tset_freepointer(s, cur, NULL);\n\n\treturn true;\n}\n#else\nstatic inline int init_cache_random_seq(struct kmem_cache *s)\n{\n\treturn 0;\n}\nstatic inline void init_freelist_randomization(void) { }\nstatic inline bool shuffle_freelist(struct kmem_cache *s, struct slab *slab)\n{\n\treturn false;\n}\n#endif  \n\nstatic struct slab *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)\n{\n\tstruct slab *slab;\n\tstruct kmem_cache_order_objects oo = s->oo;\n\tgfp_t alloc_gfp;\n\tvoid *start, *p, *next;\n\tint idx;\n\tbool shuffle;\n\n\tflags &= gfp_allowed_mask;\n\n\tflags |= s->allocflags;\n\n\t \n\talloc_gfp = (flags | __GFP_NOWARN | __GFP_NORETRY) & ~__GFP_NOFAIL;\n\tif ((alloc_gfp & __GFP_DIRECT_RECLAIM) && oo_order(oo) > oo_order(s->min))\n\t\talloc_gfp = (alloc_gfp | __GFP_NOMEMALLOC) & ~__GFP_RECLAIM;\n\n\tslab = alloc_slab_page(alloc_gfp, node, oo);\n\tif (unlikely(!slab)) {\n\t\too = s->min;\n\t\talloc_gfp = flags;\n\t\t \n\t\tslab = alloc_slab_page(alloc_gfp, node, oo);\n\t\tif (unlikely(!slab))\n\t\t\treturn NULL;\n\t\tstat(s, ORDER_FALLBACK);\n\t}\n\n\tslab->objects = oo_objects(oo);\n\tslab->inuse = 0;\n\tslab->frozen = 0;\n\n\taccount_slab(slab, oo_order(oo), s, flags);\n\n\tslab->slab_cache = s;\n\n\tkasan_poison_slab(slab);\n\n\tstart = slab_address(slab);\n\n\tsetup_slab_debug(s, slab, start);\n\n\tshuffle = shuffle_freelist(s, slab);\n\n\tif (!shuffle) {\n\t\tstart = fixup_red_left(s, start);\n\t\tstart = setup_object(s, start);\n\t\tslab->freelist = start;\n\t\tfor (idx = 0, p = start; idx < slab->objects - 1; idx++) {\n\t\t\tnext = p + s->size;\n\t\t\tnext = setup_object(s, next);\n\t\t\tset_freepointer(s, p, next);\n\t\t\tp = next;\n\t\t}\n\t\tset_freepointer(s, p, NULL);\n\t}\n\n\treturn slab;\n}\n\nstatic struct slab *new_slab(struct kmem_cache *s, gfp_t flags, int node)\n{\n\tif (unlikely(flags & GFP_SLAB_BUG_MASK))\n\t\tflags = kmalloc_fix_flags(flags);\n\n\tWARN_ON_ONCE(s->ctor && (flags & __GFP_ZERO));\n\n\treturn allocate_slab(s,\n\t\tflags & (GFP_RECLAIM_MASK | GFP_CONSTRAINT_MASK), node);\n}\n\nstatic void __free_slab(struct kmem_cache *s, struct slab *slab)\n{\n\tstruct folio *folio = slab_folio(slab);\n\tint order = folio_order(folio);\n\tint pages = 1 << order;\n\n\t__slab_clear_pfmemalloc(slab);\n\tfolio->mapping = NULL;\n\t \n\tsmp_wmb();\n\t__folio_clear_slab(folio);\n\tmm_account_reclaimed_pages(pages);\n\tunaccount_slab(slab, order, s);\n\t__free_pages(&folio->page, order);\n}\n\nstatic void rcu_free_slab(struct rcu_head *h)\n{\n\tstruct slab *slab = container_of(h, struct slab, rcu_head);\n\n\t__free_slab(slab->slab_cache, slab);\n}\n\nstatic void free_slab(struct kmem_cache *s, struct slab *slab)\n{\n\tif (kmem_cache_debug_flags(s, SLAB_CONSISTENCY_CHECKS)) {\n\t\tvoid *p;\n\n\t\tslab_pad_check(s, slab);\n\t\tfor_each_object(p, s, slab_address(slab), slab->objects)\n\t\t\tcheck_object(s, slab, p, SLUB_RED_INACTIVE);\n\t}\n\n\tif (unlikely(s->flags & SLAB_TYPESAFE_BY_RCU))\n\t\tcall_rcu(&slab->rcu_head, rcu_free_slab);\n\telse\n\t\t__free_slab(s, slab);\n}\n\nstatic void discard_slab(struct kmem_cache *s, struct slab *slab)\n{\n\tdec_slabs_node(s, slab_nid(slab), slab->objects);\n\tfree_slab(s, slab);\n}\n\n \nstatic inline void\n__add_partial(struct kmem_cache_node *n, struct slab *slab, int tail)\n{\n\tn->nr_partial++;\n\tif (tail == DEACTIVATE_TO_TAIL)\n\t\tlist_add_tail(&slab->slab_list, &n->partial);\n\telse\n\t\tlist_add(&slab->slab_list, &n->partial);\n}\n\nstatic inline void add_partial(struct kmem_cache_node *n,\n\t\t\t\tstruct slab *slab, int tail)\n{\n\tlockdep_assert_held(&n->list_lock);\n\t__add_partial(n, slab, tail);\n}\n\nstatic inline void remove_partial(struct kmem_cache_node *n,\n\t\t\t\t\tstruct slab *slab)\n{\n\tlockdep_assert_held(&n->list_lock);\n\tlist_del(&slab->slab_list);\n\tn->nr_partial--;\n}\n\n \nstatic void *alloc_single_from_partial(struct kmem_cache *s,\n\t\tstruct kmem_cache_node *n, struct slab *slab, int orig_size)\n{\n\tvoid *object;\n\n\tlockdep_assert_held(&n->list_lock);\n\n\tobject = slab->freelist;\n\tslab->freelist = get_freepointer(s, object);\n\tslab->inuse++;\n\n\tif (!alloc_debug_processing(s, slab, object, orig_size)) {\n\t\tremove_partial(n, slab);\n\t\treturn NULL;\n\t}\n\n\tif (slab->inuse == slab->objects) {\n\t\tremove_partial(n, slab);\n\t\tadd_full(s, n, slab);\n\t}\n\n\treturn object;\n}\n\n \nstatic void *alloc_single_from_new_slab(struct kmem_cache *s,\n\t\t\t\t\tstruct slab *slab, int orig_size)\n{\n\tint nid = slab_nid(slab);\n\tstruct kmem_cache_node *n = get_node(s, nid);\n\tunsigned long flags;\n\tvoid *object;\n\n\n\tobject = slab->freelist;\n\tslab->freelist = get_freepointer(s, object);\n\tslab->inuse = 1;\n\n\tif (!alloc_debug_processing(s, slab, object, orig_size))\n\t\t \n\t\treturn NULL;\n\n\tspin_lock_irqsave(&n->list_lock, flags);\n\n\tif (slab->inuse == slab->objects)\n\t\tadd_full(s, n, slab);\n\telse\n\t\tadd_partial(n, slab, DEACTIVATE_TO_HEAD);\n\n\tinc_slabs_node(s, nid, slab->objects);\n\tspin_unlock_irqrestore(&n->list_lock, flags);\n\n\treturn object;\n}\n\n \nstatic inline void *acquire_slab(struct kmem_cache *s,\n\t\tstruct kmem_cache_node *n, struct slab *slab,\n\t\tint mode)\n{\n\tvoid *freelist;\n\tunsigned long counters;\n\tstruct slab new;\n\n\tlockdep_assert_held(&n->list_lock);\n\n\t \n\tfreelist = slab->freelist;\n\tcounters = slab->counters;\n\tnew.counters = counters;\n\tif (mode) {\n\t\tnew.inuse = slab->objects;\n\t\tnew.freelist = NULL;\n\t} else {\n\t\tnew.freelist = freelist;\n\t}\n\n\tVM_BUG_ON(new.frozen);\n\tnew.frozen = 1;\n\n\tif (!__slab_update_freelist(s, slab,\n\t\t\tfreelist, counters,\n\t\t\tnew.freelist, new.counters,\n\t\t\t\"acquire_slab\"))\n\t\treturn NULL;\n\n\tremove_partial(n, slab);\n\tWARN_ON(!freelist);\n\treturn freelist;\n}\n\n#ifdef CONFIG_SLUB_CPU_PARTIAL\nstatic void put_cpu_partial(struct kmem_cache *s, struct slab *slab, int drain);\n#else\nstatic inline void put_cpu_partial(struct kmem_cache *s, struct slab *slab,\n\t\t\t\t   int drain) { }\n#endif\nstatic inline bool pfmemalloc_match(struct slab *slab, gfp_t gfpflags);\n\n \nstatic void *get_partial_node(struct kmem_cache *s, struct kmem_cache_node *n,\n\t\t\t      struct partial_context *pc)\n{\n\tstruct slab *slab, *slab2;\n\tvoid *object = NULL;\n\tunsigned long flags;\n\tunsigned int partial_slabs = 0;\n\n\t \n\tif (!n || !n->nr_partial)\n\t\treturn NULL;\n\n\tspin_lock_irqsave(&n->list_lock, flags);\n\tlist_for_each_entry_safe(slab, slab2, &n->partial, slab_list) {\n\t\tvoid *t;\n\n\t\tif (!pfmemalloc_match(slab, pc->flags))\n\t\t\tcontinue;\n\n\t\tif (IS_ENABLED(CONFIG_SLUB_TINY) || kmem_cache_debug(s)) {\n\t\t\tobject = alloc_single_from_partial(s, n, slab,\n\t\t\t\t\t\t\tpc->orig_size);\n\t\t\tif (object)\n\t\t\t\tbreak;\n\t\t\tcontinue;\n\t\t}\n\n\t\tt = acquire_slab(s, n, slab, object == NULL);\n\t\tif (!t)\n\t\t\tbreak;\n\n\t\tif (!object) {\n\t\t\t*pc->slab = slab;\n\t\t\tstat(s, ALLOC_FROM_PARTIAL);\n\t\t\tobject = t;\n\t\t} else {\n\t\t\tput_cpu_partial(s, slab, 0);\n\t\t\tstat(s, CPU_PARTIAL_NODE);\n\t\t\tpartial_slabs++;\n\t\t}\n#ifdef CONFIG_SLUB_CPU_PARTIAL\n\t\tif (!kmem_cache_has_cpu_partial(s)\n\t\t\t|| partial_slabs > s->cpu_partial_slabs / 2)\n\t\t\tbreak;\n#else\n\t\tbreak;\n#endif\n\n\t}\n\tspin_unlock_irqrestore(&n->list_lock, flags);\n\treturn object;\n}\n\n \nstatic void *get_any_partial(struct kmem_cache *s, struct partial_context *pc)\n{\n#ifdef CONFIG_NUMA\n\tstruct zonelist *zonelist;\n\tstruct zoneref *z;\n\tstruct zone *zone;\n\tenum zone_type highest_zoneidx = gfp_zone(pc->flags);\n\tvoid *object;\n\tunsigned int cpuset_mems_cookie;\n\n\t \n\tif (!s->remote_node_defrag_ratio ||\n\t\t\tget_cycles() % 1024 > s->remote_node_defrag_ratio)\n\t\treturn NULL;\n\n\tdo {\n\t\tcpuset_mems_cookie = read_mems_allowed_begin();\n\t\tzonelist = node_zonelist(mempolicy_slab_node(), pc->flags);\n\t\tfor_each_zone_zonelist(zone, z, zonelist, highest_zoneidx) {\n\t\t\tstruct kmem_cache_node *n;\n\n\t\t\tn = get_node(s, zone_to_nid(zone));\n\n\t\t\tif (n && cpuset_zone_allowed(zone, pc->flags) &&\n\t\t\t\t\tn->nr_partial > s->min_partial) {\n\t\t\t\tobject = get_partial_node(s, n, pc);\n\t\t\t\tif (object) {\n\t\t\t\t\t \n\t\t\t\t\treturn object;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t} while (read_mems_allowed_retry(cpuset_mems_cookie));\n#endif\t \n\treturn NULL;\n}\n\n \nstatic void *get_partial(struct kmem_cache *s, int node, struct partial_context *pc)\n{\n\tvoid *object;\n\tint searchnode = node;\n\n\tif (node == NUMA_NO_NODE)\n\t\tsearchnode = numa_mem_id();\n\n\tobject = get_partial_node(s, get_node(s, searchnode), pc);\n\tif (object || node != NUMA_NO_NODE)\n\t\treturn object;\n\n\treturn get_any_partial(s, pc);\n}\n\n#ifndef CONFIG_SLUB_TINY\n\n#ifdef CONFIG_PREEMPTION\n \n#define TID_STEP  roundup_pow_of_two(CONFIG_NR_CPUS)\n#else\n \n#define TID_STEP 1\n#endif  \n\nstatic inline unsigned long next_tid(unsigned long tid)\n{\n\treturn tid + TID_STEP;\n}\n\n#ifdef SLUB_DEBUG_CMPXCHG\nstatic inline unsigned int tid_to_cpu(unsigned long tid)\n{\n\treturn tid % TID_STEP;\n}\n\nstatic inline unsigned long tid_to_event(unsigned long tid)\n{\n\treturn tid / TID_STEP;\n}\n#endif\n\nstatic inline unsigned int init_tid(int cpu)\n{\n\treturn cpu;\n}\n\nstatic inline void note_cmpxchg_failure(const char *n,\n\t\tconst struct kmem_cache *s, unsigned long tid)\n{\n#ifdef SLUB_DEBUG_CMPXCHG\n\tunsigned long actual_tid = __this_cpu_read(s->cpu_slab->tid);\n\n\tpr_info(\"%s %s: cmpxchg redo \", n, s->name);\n\n#ifdef CONFIG_PREEMPTION\n\tif (tid_to_cpu(tid) != tid_to_cpu(actual_tid))\n\t\tpr_warn(\"due to cpu change %d -> %d\\n\",\n\t\t\ttid_to_cpu(tid), tid_to_cpu(actual_tid));\n\telse\n#endif\n\tif (tid_to_event(tid) != tid_to_event(actual_tid))\n\t\tpr_warn(\"due to cpu running other code. Event %ld->%ld\\n\",\n\t\t\ttid_to_event(tid), tid_to_event(actual_tid));\n\telse\n\t\tpr_warn(\"for unknown reason: actual=%lx was=%lx target=%lx\\n\",\n\t\t\tactual_tid, tid, next_tid(tid));\n#endif\n\tstat(s, CMPXCHG_DOUBLE_CPU_FAIL);\n}\n\nstatic void init_kmem_cache_cpus(struct kmem_cache *s)\n{\n\tint cpu;\n\tstruct kmem_cache_cpu *c;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tc = per_cpu_ptr(s->cpu_slab, cpu);\n\t\tlocal_lock_init(&c->lock);\n\t\tc->tid = init_tid(cpu);\n\t}\n}\n\n \nstatic void deactivate_slab(struct kmem_cache *s, struct slab *slab,\n\t\t\t    void *freelist)\n{\n\tenum slab_modes { M_NONE, M_PARTIAL, M_FREE, M_FULL_NOLIST };\n\tstruct kmem_cache_node *n = get_node(s, slab_nid(slab));\n\tint free_delta = 0;\n\tenum slab_modes mode = M_NONE;\n\tvoid *nextfree, *freelist_iter, *freelist_tail;\n\tint tail = DEACTIVATE_TO_HEAD;\n\tunsigned long flags = 0;\n\tstruct slab new;\n\tstruct slab old;\n\n\tif (slab->freelist) {\n\t\tstat(s, DEACTIVATE_REMOTE_FREES);\n\t\ttail = DEACTIVATE_TO_TAIL;\n\t}\n\n\t \n\tfreelist_tail = NULL;\n\tfreelist_iter = freelist;\n\twhile (freelist_iter) {\n\t\tnextfree = get_freepointer(s, freelist_iter);\n\n\t\t \n\t\tif (freelist_corrupted(s, slab, &freelist_iter, nextfree))\n\t\t\tbreak;\n\n\t\tfreelist_tail = freelist_iter;\n\t\tfree_delta++;\n\n\t\tfreelist_iter = nextfree;\n\t}\n\n\t \nredo:\n\n\told.freelist = READ_ONCE(slab->freelist);\n\told.counters = READ_ONCE(slab->counters);\n\tVM_BUG_ON(!old.frozen);\n\n\t \n\tnew.counters = old.counters;\n\tif (freelist_tail) {\n\t\tnew.inuse -= free_delta;\n\t\tset_freepointer(s, freelist_tail, old.freelist);\n\t\tnew.freelist = freelist;\n\t} else\n\t\tnew.freelist = old.freelist;\n\n\tnew.frozen = 0;\n\n\tif (!new.inuse && n->nr_partial >= s->min_partial) {\n\t\tmode = M_FREE;\n\t} else if (new.freelist) {\n\t\tmode = M_PARTIAL;\n\t\t \n\t\tspin_lock_irqsave(&n->list_lock, flags);\n\t} else {\n\t\tmode = M_FULL_NOLIST;\n\t}\n\n\n\tif (!slab_update_freelist(s, slab,\n\t\t\t\told.freelist, old.counters,\n\t\t\t\tnew.freelist, new.counters,\n\t\t\t\t\"unfreezing slab\")) {\n\t\tif (mode == M_PARTIAL)\n\t\t\tspin_unlock_irqrestore(&n->list_lock, flags);\n\t\tgoto redo;\n\t}\n\n\n\tif (mode == M_PARTIAL) {\n\t\tadd_partial(n, slab, tail);\n\t\tspin_unlock_irqrestore(&n->list_lock, flags);\n\t\tstat(s, tail);\n\t} else if (mode == M_FREE) {\n\t\tstat(s, DEACTIVATE_EMPTY);\n\t\tdiscard_slab(s, slab);\n\t\tstat(s, FREE_SLAB);\n\t} else if (mode == M_FULL_NOLIST) {\n\t\tstat(s, DEACTIVATE_FULL);\n\t}\n}\n\n#ifdef CONFIG_SLUB_CPU_PARTIAL\nstatic void __unfreeze_partials(struct kmem_cache *s, struct slab *partial_slab)\n{\n\tstruct kmem_cache_node *n = NULL, *n2 = NULL;\n\tstruct slab *slab, *slab_to_discard = NULL;\n\tunsigned long flags = 0;\n\n\twhile (partial_slab) {\n\t\tstruct slab new;\n\t\tstruct slab old;\n\n\t\tslab = partial_slab;\n\t\tpartial_slab = slab->next;\n\n\t\tn2 = get_node(s, slab_nid(slab));\n\t\tif (n != n2) {\n\t\t\tif (n)\n\t\t\t\tspin_unlock_irqrestore(&n->list_lock, flags);\n\n\t\t\tn = n2;\n\t\t\tspin_lock_irqsave(&n->list_lock, flags);\n\t\t}\n\n\t\tdo {\n\n\t\t\told.freelist = slab->freelist;\n\t\t\told.counters = slab->counters;\n\t\t\tVM_BUG_ON(!old.frozen);\n\n\t\t\tnew.counters = old.counters;\n\t\t\tnew.freelist = old.freelist;\n\n\t\t\tnew.frozen = 0;\n\n\t\t} while (!__slab_update_freelist(s, slab,\n\t\t\t\told.freelist, old.counters,\n\t\t\t\tnew.freelist, new.counters,\n\t\t\t\t\"unfreezing slab\"));\n\n\t\tif (unlikely(!new.inuse && n->nr_partial >= s->min_partial)) {\n\t\t\tslab->next = slab_to_discard;\n\t\t\tslab_to_discard = slab;\n\t\t} else {\n\t\t\tadd_partial(n, slab, DEACTIVATE_TO_TAIL);\n\t\t\tstat(s, FREE_ADD_PARTIAL);\n\t\t}\n\t}\n\n\tif (n)\n\t\tspin_unlock_irqrestore(&n->list_lock, flags);\n\n\twhile (slab_to_discard) {\n\t\tslab = slab_to_discard;\n\t\tslab_to_discard = slab_to_discard->next;\n\n\t\tstat(s, DEACTIVATE_EMPTY);\n\t\tdiscard_slab(s, slab);\n\t\tstat(s, FREE_SLAB);\n\t}\n}\n\n \nstatic void unfreeze_partials(struct kmem_cache *s)\n{\n\tstruct slab *partial_slab;\n\tunsigned long flags;\n\n\tlocal_lock_irqsave(&s->cpu_slab->lock, flags);\n\tpartial_slab = this_cpu_read(s->cpu_slab->partial);\n\tthis_cpu_write(s->cpu_slab->partial, NULL);\n\tlocal_unlock_irqrestore(&s->cpu_slab->lock, flags);\n\n\tif (partial_slab)\n\t\t__unfreeze_partials(s, partial_slab);\n}\n\nstatic void unfreeze_partials_cpu(struct kmem_cache *s,\n\t\t\t\t  struct kmem_cache_cpu *c)\n{\n\tstruct slab *partial_slab;\n\n\tpartial_slab = slub_percpu_partial(c);\n\tc->partial = NULL;\n\n\tif (partial_slab)\n\t\t__unfreeze_partials(s, partial_slab);\n}\n\n \nstatic void put_cpu_partial(struct kmem_cache *s, struct slab *slab, int drain)\n{\n\tstruct slab *oldslab;\n\tstruct slab *slab_to_unfreeze = NULL;\n\tunsigned long flags;\n\tint slabs = 0;\n\n\tlocal_lock_irqsave(&s->cpu_slab->lock, flags);\n\n\toldslab = this_cpu_read(s->cpu_slab->partial);\n\n\tif (oldslab) {\n\t\tif (drain && oldslab->slabs >= s->cpu_partial_slabs) {\n\t\t\t \n\t\t\tslab_to_unfreeze = oldslab;\n\t\t\toldslab = NULL;\n\t\t} else {\n\t\t\tslabs = oldslab->slabs;\n\t\t}\n\t}\n\n\tslabs++;\n\n\tslab->slabs = slabs;\n\tslab->next = oldslab;\n\n\tthis_cpu_write(s->cpu_slab->partial, slab);\n\n\tlocal_unlock_irqrestore(&s->cpu_slab->lock, flags);\n\n\tif (slab_to_unfreeze) {\n\t\t__unfreeze_partials(s, slab_to_unfreeze);\n\t\tstat(s, CPU_PARTIAL_DRAIN);\n\t}\n}\n\n#else\t \n\nstatic inline void unfreeze_partials(struct kmem_cache *s) { }\nstatic inline void unfreeze_partials_cpu(struct kmem_cache *s,\n\t\t\t\t  struct kmem_cache_cpu *c) { }\n\n#endif\t \n\nstatic inline void flush_slab(struct kmem_cache *s, struct kmem_cache_cpu *c)\n{\n\tunsigned long flags;\n\tstruct slab *slab;\n\tvoid *freelist;\n\n\tlocal_lock_irqsave(&s->cpu_slab->lock, flags);\n\n\tslab = c->slab;\n\tfreelist = c->freelist;\n\n\tc->slab = NULL;\n\tc->freelist = NULL;\n\tc->tid = next_tid(c->tid);\n\n\tlocal_unlock_irqrestore(&s->cpu_slab->lock, flags);\n\n\tif (slab) {\n\t\tdeactivate_slab(s, slab, freelist);\n\t\tstat(s, CPUSLAB_FLUSH);\n\t}\n}\n\nstatic inline void __flush_cpu_slab(struct kmem_cache *s, int cpu)\n{\n\tstruct kmem_cache_cpu *c = per_cpu_ptr(s->cpu_slab, cpu);\n\tvoid *freelist = c->freelist;\n\tstruct slab *slab = c->slab;\n\n\tc->slab = NULL;\n\tc->freelist = NULL;\n\tc->tid = next_tid(c->tid);\n\n\tif (slab) {\n\t\tdeactivate_slab(s, slab, freelist);\n\t\tstat(s, CPUSLAB_FLUSH);\n\t}\n\n\tunfreeze_partials_cpu(s, c);\n}\n\nstruct slub_flush_work {\n\tstruct work_struct work;\n\tstruct kmem_cache *s;\n\tbool skip;\n};\n\n \nstatic void flush_cpu_slab(struct work_struct *w)\n{\n\tstruct kmem_cache *s;\n\tstruct kmem_cache_cpu *c;\n\tstruct slub_flush_work *sfw;\n\n\tsfw = container_of(w, struct slub_flush_work, work);\n\n\ts = sfw->s;\n\tc = this_cpu_ptr(s->cpu_slab);\n\n\tif (c->slab)\n\t\tflush_slab(s, c);\n\n\tunfreeze_partials(s);\n}\n\nstatic bool has_cpu_slab(int cpu, struct kmem_cache *s)\n{\n\tstruct kmem_cache_cpu *c = per_cpu_ptr(s->cpu_slab, cpu);\n\n\treturn c->slab || slub_percpu_partial(c);\n}\n\nstatic DEFINE_MUTEX(flush_lock);\nstatic DEFINE_PER_CPU(struct slub_flush_work, slub_flush);\n\nstatic void flush_all_cpus_locked(struct kmem_cache *s)\n{\n\tstruct slub_flush_work *sfw;\n\tunsigned int cpu;\n\n\tlockdep_assert_cpus_held();\n\tmutex_lock(&flush_lock);\n\n\tfor_each_online_cpu(cpu) {\n\t\tsfw = &per_cpu(slub_flush, cpu);\n\t\tif (!has_cpu_slab(cpu, s)) {\n\t\t\tsfw->skip = true;\n\t\t\tcontinue;\n\t\t}\n\t\tINIT_WORK(&sfw->work, flush_cpu_slab);\n\t\tsfw->skip = false;\n\t\tsfw->s = s;\n\t\tqueue_work_on(cpu, flushwq, &sfw->work);\n\t}\n\n\tfor_each_online_cpu(cpu) {\n\t\tsfw = &per_cpu(slub_flush, cpu);\n\t\tif (sfw->skip)\n\t\t\tcontinue;\n\t\tflush_work(&sfw->work);\n\t}\n\n\tmutex_unlock(&flush_lock);\n}\n\nstatic void flush_all(struct kmem_cache *s)\n{\n\tcpus_read_lock();\n\tflush_all_cpus_locked(s);\n\tcpus_read_unlock();\n}\n\n \nstatic int slub_cpu_dead(unsigned int cpu)\n{\n\tstruct kmem_cache *s;\n\n\tmutex_lock(&slab_mutex);\n\tlist_for_each_entry(s, &slab_caches, list)\n\t\t__flush_cpu_slab(s, cpu);\n\tmutex_unlock(&slab_mutex);\n\treturn 0;\n}\n\n#else  \nstatic inline void flush_all_cpus_locked(struct kmem_cache *s) { }\nstatic inline void flush_all(struct kmem_cache *s) { }\nstatic inline void __flush_cpu_slab(struct kmem_cache *s, int cpu) { }\nstatic inline int slub_cpu_dead(unsigned int cpu) { return 0; }\n#endif  \n\n \nstatic inline int node_match(struct slab *slab, int node)\n{\n#ifdef CONFIG_NUMA\n\tif (node != NUMA_NO_NODE && slab_nid(slab) != node)\n\t\treturn 0;\n#endif\n\treturn 1;\n}\n\n#ifdef CONFIG_SLUB_DEBUG\nstatic int count_free(struct slab *slab)\n{\n\treturn slab->objects - slab->inuse;\n}\n\nstatic inline unsigned long node_nr_objs(struct kmem_cache_node *n)\n{\n\treturn atomic_long_read(&n->total_objects);\n}\n\n \nstatic inline bool free_debug_processing(struct kmem_cache *s,\n\tstruct slab *slab, void *head, void *tail, int *bulk_cnt,\n\tunsigned long addr, depot_stack_handle_t handle)\n{\n\tbool checks_ok = false;\n\tvoid *object = head;\n\tint cnt = 0;\n\n\tif (s->flags & SLAB_CONSISTENCY_CHECKS) {\n\t\tif (!check_slab(s, slab))\n\t\t\tgoto out;\n\t}\n\n\tif (slab->inuse < *bulk_cnt) {\n\t\tslab_err(s, slab, \"Slab has %d allocated objects but %d are to be freed\\n\",\n\t\t\t slab->inuse, *bulk_cnt);\n\t\tgoto out;\n\t}\n\nnext_object:\n\n\tif (++cnt > *bulk_cnt)\n\t\tgoto out_cnt;\n\n\tif (s->flags & SLAB_CONSISTENCY_CHECKS) {\n\t\tif (!free_consistency_checks(s, slab, object, addr))\n\t\t\tgoto out;\n\t}\n\n\tif (s->flags & SLAB_STORE_USER)\n\t\tset_track_update(s, object, TRACK_FREE, addr, handle);\n\ttrace(s, slab, object, 0);\n\t \n\tinit_object(s, object, SLUB_RED_INACTIVE);\n\n\t \n\tif (object != tail) {\n\t\tobject = get_freepointer(s, object);\n\t\tgoto next_object;\n\t}\n\tchecks_ok = true;\n\nout_cnt:\n\tif (cnt != *bulk_cnt) {\n\t\tslab_err(s, slab, \"Bulk free expected %d objects but found %d\\n\",\n\t\t\t *bulk_cnt, cnt);\n\t\t*bulk_cnt = cnt;\n\t}\n\nout:\n\n\tif (!checks_ok)\n\t\tslab_fix(s, \"Object at 0x%p not freed\", object);\n\n\treturn checks_ok;\n}\n#endif  \n\n#if defined(CONFIG_SLUB_DEBUG) || defined(SLAB_SUPPORTS_SYSFS)\nstatic unsigned long count_partial(struct kmem_cache_node *n,\n\t\t\t\t\tint (*get_count)(struct slab *))\n{\n\tunsigned long flags;\n\tunsigned long x = 0;\n\tstruct slab *slab;\n\n\tspin_lock_irqsave(&n->list_lock, flags);\n\tlist_for_each_entry(slab, &n->partial, slab_list)\n\t\tx += get_count(slab);\n\tspin_unlock_irqrestore(&n->list_lock, flags);\n\treturn x;\n}\n#endif  \n\n#ifdef CONFIG_SLUB_DEBUG\nstatic noinline void\nslab_out_of_memory(struct kmem_cache *s, gfp_t gfpflags, int nid)\n{\n\tstatic DEFINE_RATELIMIT_STATE(slub_oom_rs, DEFAULT_RATELIMIT_INTERVAL,\n\t\t\t\t      DEFAULT_RATELIMIT_BURST);\n\tint node;\n\tstruct kmem_cache_node *n;\n\n\tif ((gfpflags & __GFP_NOWARN) || !__ratelimit(&slub_oom_rs))\n\t\treturn;\n\n\tpr_warn(\"SLUB: Unable to allocate memory on node %d, gfp=%#x(%pGg)\\n\",\n\t\tnid, gfpflags, &gfpflags);\n\tpr_warn(\"  cache: %s, object size: %u, buffer size: %u, default order: %u, min order: %u\\n\",\n\t\ts->name, s->object_size, s->size, oo_order(s->oo),\n\t\too_order(s->min));\n\n\tif (oo_order(s->min) > get_order(s->object_size))\n\t\tpr_warn(\"  %s debugging increased min order, use slub_debug=O to disable.\\n\",\n\t\t\ts->name);\n\n\tfor_each_kmem_cache_node(s, node, n) {\n\t\tunsigned long nr_slabs;\n\t\tunsigned long nr_objs;\n\t\tunsigned long nr_free;\n\n\t\tnr_free  = count_partial(n, count_free);\n\t\tnr_slabs = node_nr_slabs(n);\n\t\tnr_objs  = node_nr_objs(n);\n\n\t\tpr_warn(\"  node %d: slabs: %ld, objs: %ld, free: %ld\\n\",\n\t\t\tnode, nr_slabs, nr_objs, nr_free);\n\t}\n}\n#else  \nstatic inline void\nslab_out_of_memory(struct kmem_cache *s, gfp_t gfpflags, int nid) { }\n#endif\n\nstatic inline bool pfmemalloc_match(struct slab *slab, gfp_t gfpflags)\n{\n\tif (unlikely(slab_test_pfmemalloc(slab)))\n\t\treturn gfp_pfmemalloc_allowed(gfpflags);\n\n\treturn true;\n}\n\n#ifndef CONFIG_SLUB_TINY\nstatic inline bool\n__update_cpu_freelist_fast(struct kmem_cache *s,\n\t\t\t   void *freelist_old, void *freelist_new,\n\t\t\t   unsigned long tid)\n{\n\tfreelist_aba_t old = { .freelist = freelist_old, .counter = tid };\n\tfreelist_aba_t new = { .freelist = freelist_new, .counter = next_tid(tid) };\n\n\treturn this_cpu_try_cmpxchg_freelist(s->cpu_slab->freelist_tid.full,\n\t\t\t\t\t     &old.full, new.full);\n}\n\n \nstatic inline void *get_freelist(struct kmem_cache *s, struct slab *slab)\n{\n\tstruct slab new;\n\tunsigned long counters;\n\tvoid *freelist;\n\n\tlockdep_assert_held(this_cpu_ptr(&s->cpu_slab->lock));\n\n\tdo {\n\t\tfreelist = slab->freelist;\n\t\tcounters = slab->counters;\n\n\t\tnew.counters = counters;\n\t\tVM_BUG_ON(!new.frozen);\n\n\t\tnew.inuse = slab->objects;\n\t\tnew.frozen = freelist != NULL;\n\n\t} while (!__slab_update_freelist(s, slab,\n\t\tfreelist, counters,\n\t\tNULL, new.counters,\n\t\t\"get_freelist\"));\n\n\treturn freelist;\n}\n\n \nstatic void *___slab_alloc(struct kmem_cache *s, gfp_t gfpflags, int node,\n\t\t\t  unsigned long addr, struct kmem_cache_cpu *c, unsigned int orig_size)\n{\n\tvoid *freelist;\n\tstruct slab *slab;\n\tunsigned long flags;\n\tstruct partial_context pc;\n\n\tstat(s, ALLOC_SLOWPATH);\n\nreread_slab:\n\n\tslab = READ_ONCE(c->slab);\n\tif (!slab) {\n\t\t \n\t\tif (unlikely(node != NUMA_NO_NODE &&\n\t\t\t     !node_isset(node, slab_nodes)))\n\t\t\tnode = NUMA_NO_NODE;\n\t\tgoto new_slab;\n\t}\nredo:\n\n\tif (unlikely(!node_match(slab, node))) {\n\t\t \n\t\tif (!node_isset(node, slab_nodes)) {\n\t\t\tnode = NUMA_NO_NODE;\n\t\t} else {\n\t\t\tstat(s, ALLOC_NODE_MISMATCH);\n\t\t\tgoto deactivate_slab;\n\t\t}\n\t}\n\n\t \n\tif (unlikely(!pfmemalloc_match(slab, gfpflags)))\n\t\tgoto deactivate_slab;\n\n\t \n\tlocal_lock_irqsave(&s->cpu_slab->lock, flags);\n\tif (unlikely(slab != c->slab)) {\n\t\tlocal_unlock_irqrestore(&s->cpu_slab->lock, flags);\n\t\tgoto reread_slab;\n\t}\n\tfreelist = c->freelist;\n\tif (freelist)\n\t\tgoto load_freelist;\n\n\tfreelist = get_freelist(s, slab);\n\n\tif (!freelist) {\n\t\tc->slab = NULL;\n\t\tc->tid = next_tid(c->tid);\n\t\tlocal_unlock_irqrestore(&s->cpu_slab->lock, flags);\n\t\tstat(s, DEACTIVATE_BYPASS);\n\t\tgoto new_slab;\n\t}\n\n\tstat(s, ALLOC_REFILL);\n\nload_freelist:\n\n\tlockdep_assert_held(this_cpu_ptr(&s->cpu_slab->lock));\n\n\t \n\tVM_BUG_ON(!c->slab->frozen);\n\tc->freelist = get_freepointer(s, freelist);\n\tc->tid = next_tid(c->tid);\n\tlocal_unlock_irqrestore(&s->cpu_slab->lock, flags);\n\treturn freelist;\n\ndeactivate_slab:\n\n\tlocal_lock_irqsave(&s->cpu_slab->lock, flags);\n\tif (slab != c->slab) {\n\t\tlocal_unlock_irqrestore(&s->cpu_slab->lock, flags);\n\t\tgoto reread_slab;\n\t}\n\tfreelist = c->freelist;\n\tc->slab = NULL;\n\tc->freelist = NULL;\n\tc->tid = next_tid(c->tid);\n\tlocal_unlock_irqrestore(&s->cpu_slab->lock, flags);\n\tdeactivate_slab(s, slab, freelist);\n\nnew_slab:\n\n\tif (slub_percpu_partial(c)) {\n\t\tlocal_lock_irqsave(&s->cpu_slab->lock, flags);\n\t\tif (unlikely(c->slab)) {\n\t\t\tlocal_unlock_irqrestore(&s->cpu_slab->lock, flags);\n\t\t\tgoto reread_slab;\n\t\t}\n\t\tif (unlikely(!slub_percpu_partial(c))) {\n\t\t\tlocal_unlock_irqrestore(&s->cpu_slab->lock, flags);\n\t\t\t \n\t\t\tgoto new_objects;\n\t\t}\n\n\t\tslab = c->slab = slub_percpu_partial(c);\n\t\tslub_set_percpu_partial(c, slab);\n\t\tlocal_unlock_irqrestore(&s->cpu_slab->lock, flags);\n\t\tstat(s, CPU_PARTIAL_ALLOC);\n\t\tgoto redo;\n\t}\n\nnew_objects:\n\n\tpc.flags = gfpflags;\n\tpc.slab = &slab;\n\tpc.orig_size = orig_size;\n\tfreelist = get_partial(s, node, &pc);\n\tif (freelist)\n\t\tgoto check_new_slab;\n\n\tslub_put_cpu_ptr(s->cpu_slab);\n\tslab = new_slab(s, gfpflags, node);\n\tc = slub_get_cpu_ptr(s->cpu_slab);\n\n\tif (unlikely(!slab)) {\n\t\tslab_out_of_memory(s, gfpflags, node);\n\t\treturn NULL;\n\t}\n\n\tstat(s, ALLOC_SLAB);\n\n\tif (kmem_cache_debug(s)) {\n\t\tfreelist = alloc_single_from_new_slab(s, slab, orig_size);\n\n\t\tif (unlikely(!freelist))\n\t\t\tgoto new_objects;\n\n\t\tif (s->flags & SLAB_STORE_USER)\n\t\t\tset_track(s, freelist, TRACK_ALLOC, addr);\n\n\t\treturn freelist;\n\t}\n\n\t \n\tfreelist = slab->freelist;\n\tslab->freelist = NULL;\n\tslab->inuse = slab->objects;\n\tslab->frozen = 1;\n\n\tinc_slabs_node(s, slab_nid(slab), slab->objects);\n\ncheck_new_slab:\n\n\tif (kmem_cache_debug(s)) {\n\t\t \n\t\tif (s->flags & SLAB_STORE_USER)\n\t\t\tset_track(s, freelist, TRACK_ALLOC, addr);\n\n\t\treturn freelist;\n\t}\n\n\tif (unlikely(!pfmemalloc_match(slab, gfpflags))) {\n\t\t \n\t\tdeactivate_slab(s, slab, get_freepointer(s, freelist));\n\t\treturn freelist;\n\t}\n\nretry_load_slab:\n\n\tlocal_lock_irqsave(&s->cpu_slab->lock, flags);\n\tif (unlikely(c->slab)) {\n\t\tvoid *flush_freelist = c->freelist;\n\t\tstruct slab *flush_slab = c->slab;\n\n\t\tc->slab = NULL;\n\t\tc->freelist = NULL;\n\t\tc->tid = next_tid(c->tid);\n\n\t\tlocal_unlock_irqrestore(&s->cpu_slab->lock, flags);\n\n\t\tdeactivate_slab(s, flush_slab, flush_freelist);\n\n\t\tstat(s, CPUSLAB_FLUSH);\n\n\t\tgoto retry_load_slab;\n\t}\n\tc->slab = slab;\n\n\tgoto load_freelist;\n}\n\n \nstatic void *__slab_alloc(struct kmem_cache *s, gfp_t gfpflags, int node,\n\t\t\t  unsigned long addr, struct kmem_cache_cpu *c, unsigned int orig_size)\n{\n\tvoid *p;\n\n#ifdef CONFIG_PREEMPT_COUNT\n\t \n\tc = slub_get_cpu_ptr(s->cpu_slab);\n#endif\n\n\tp = ___slab_alloc(s, gfpflags, node, addr, c, orig_size);\n#ifdef CONFIG_PREEMPT_COUNT\n\tslub_put_cpu_ptr(s->cpu_slab);\n#endif\n\treturn p;\n}\n\nstatic __always_inline void *__slab_alloc_node(struct kmem_cache *s,\n\t\tgfp_t gfpflags, int node, unsigned long addr, size_t orig_size)\n{\n\tstruct kmem_cache_cpu *c;\n\tstruct slab *slab;\n\tunsigned long tid;\n\tvoid *object;\n\nredo:\n\t \n\tc = raw_cpu_ptr(s->cpu_slab);\n\ttid = READ_ONCE(c->tid);\n\n\t \n\tbarrier();\n\n\t \n\n\tobject = c->freelist;\n\tslab = c->slab;\n\n\tif (!USE_LOCKLESS_FAST_PATH() ||\n\t    unlikely(!object || !slab || !node_match(slab, node))) {\n\t\tobject = __slab_alloc(s, gfpflags, node, addr, c, orig_size);\n\t} else {\n\t\tvoid *next_object = get_freepointer_safe(s, object);\n\n\t\t \n\t\tif (unlikely(!__update_cpu_freelist_fast(s, object, next_object, tid))) {\n\t\t\tnote_cmpxchg_failure(\"slab_alloc\", s, tid);\n\t\t\tgoto redo;\n\t\t}\n\t\tprefetch_freepointer(s, next_object);\n\t\tstat(s, ALLOC_FASTPATH);\n\t}\n\n\treturn object;\n}\n#else  \nstatic void *__slab_alloc_node(struct kmem_cache *s,\n\t\tgfp_t gfpflags, int node, unsigned long addr, size_t orig_size)\n{\n\tstruct partial_context pc;\n\tstruct slab *slab;\n\tvoid *object;\n\n\tpc.flags = gfpflags;\n\tpc.slab = &slab;\n\tpc.orig_size = orig_size;\n\tobject = get_partial(s, node, &pc);\n\n\tif (object)\n\t\treturn object;\n\n\tslab = new_slab(s, gfpflags, node);\n\tif (unlikely(!slab)) {\n\t\tslab_out_of_memory(s, gfpflags, node);\n\t\treturn NULL;\n\t}\n\n\tobject = alloc_single_from_new_slab(s, slab, orig_size);\n\n\treturn object;\n}\n#endif  \n\n \nstatic __always_inline void maybe_wipe_obj_freeptr(struct kmem_cache *s,\n\t\t\t\t\t\t   void *obj)\n{\n\tif (unlikely(slab_want_init_on_free(s)) && obj)\n\t\tmemset((void *)((char *)kasan_reset_tag(obj) + s->offset),\n\t\t\t0, sizeof(void *));\n}\n\n \nstatic __fastpath_inline void *slab_alloc_node(struct kmem_cache *s, struct list_lru *lru,\n\t\tgfp_t gfpflags, int node, unsigned long addr, size_t orig_size)\n{\n\tvoid *object;\n\tstruct obj_cgroup *objcg = NULL;\n\tbool init = false;\n\n\ts = slab_pre_alloc_hook(s, lru, &objcg, 1, gfpflags);\n\tif (!s)\n\t\treturn NULL;\n\n\tobject = kfence_alloc(s, orig_size, gfpflags);\n\tif (unlikely(object))\n\t\tgoto out;\n\n\tobject = __slab_alloc_node(s, gfpflags, node, addr, orig_size);\n\n\tmaybe_wipe_obj_freeptr(s, object);\n\tinit = slab_want_init_on_alloc(gfpflags, s);\n\nout:\n\t \n\tslab_post_alloc_hook(s, objcg, gfpflags, 1, &object, init, orig_size);\n\n\treturn object;\n}\n\nstatic __fastpath_inline void *slab_alloc(struct kmem_cache *s, struct list_lru *lru,\n\t\tgfp_t gfpflags, unsigned long addr, size_t orig_size)\n{\n\treturn slab_alloc_node(s, lru, gfpflags, NUMA_NO_NODE, addr, orig_size);\n}\n\nstatic __fastpath_inline\nvoid *__kmem_cache_alloc_lru(struct kmem_cache *s, struct list_lru *lru,\n\t\t\t     gfp_t gfpflags)\n{\n\tvoid *ret = slab_alloc(s, lru, gfpflags, _RET_IP_, s->object_size);\n\n\ttrace_kmem_cache_alloc(_RET_IP_, ret, s, gfpflags, NUMA_NO_NODE);\n\n\treturn ret;\n}\n\nvoid *kmem_cache_alloc(struct kmem_cache *s, gfp_t gfpflags)\n{\n\treturn __kmem_cache_alloc_lru(s, NULL, gfpflags);\n}\nEXPORT_SYMBOL(kmem_cache_alloc);\n\nvoid *kmem_cache_alloc_lru(struct kmem_cache *s, struct list_lru *lru,\n\t\t\t   gfp_t gfpflags)\n{\n\treturn __kmem_cache_alloc_lru(s, lru, gfpflags);\n}\nEXPORT_SYMBOL(kmem_cache_alloc_lru);\n\nvoid *__kmem_cache_alloc_node(struct kmem_cache *s, gfp_t gfpflags,\n\t\t\t      int node, size_t orig_size,\n\t\t\t      unsigned long caller)\n{\n\treturn slab_alloc_node(s, NULL, gfpflags, node,\n\t\t\t       caller, orig_size);\n}\n\nvoid *kmem_cache_alloc_node(struct kmem_cache *s, gfp_t gfpflags, int node)\n{\n\tvoid *ret = slab_alloc_node(s, NULL, gfpflags, node, _RET_IP_, s->object_size);\n\n\ttrace_kmem_cache_alloc(_RET_IP_, ret, s, gfpflags, node);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(kmem_cache_alloc_node);\n\nstatic noinline void free_to_partial_list(\n\tstruct kmem_cache *s, struct slab *slab,\n\tvoid *head, void *tail, int bulk_cnt,\n\tunsigned long addr)\n{\n\tstruct kmem_cache_node *n = get_node(s, slab_nid(slab));\n\tstruct slab *slab_free = NULL;\n\tint cnt = bulk_cnt;\n\tunsigned long flags;\n\tdepot_stack_handle_t handle = 0;\n\n\tif (s->flags & SLAB_STORE_USER)\n\t\thandle = set_track_prepare();\n\n\tspin_lock_irqsave(&n->list_lock, flags);\n\n\tif (free_debug_processing(s, slab, head, tail, &cnt, addr, handle)) {\n\t\tvoid *prior = slab->freelist;\n\n\t\t \n\t\tslab->inuse -= cnt;\n\t\tset_freepointer(s, tail, prior);\n\t\tslab->freelist = head;\n\n\t\t \n\t\tif (slab->inuse == 0 && n->nr_partial >= s->min_partial)\n\t\t\tslab_free = slab;\n\n\t\tif (!prior) {\n\t\t\t \n\t\t\tremove_full(s, n, slab);\n\t\t\tif (!slab_free) {\n\t\t\t\tadd_partial(n, slab, DEACTIVATE_TO_TAIL);\n\t\t\t\tstat(s, FREE_ADD_PARTIAL);\n\t\t\t}\n\t\t} else if (slab_free) {\n\t\t\tremove_partial(n, slab);\n\t\t\tstat(s, FREE_REMOVE_PARTIAL);\n\t\t}\n\t}\n\n\tif (slab_free) {\n\t\t \n\t\tdec_slabs_node(s, slab_nid(slab_free), slab_free->objects);\n\t}\n\n\tspin_unlock_irqrestore(&n->list_lock, flags);\n\n\tif (slab_free) {\n\t\tstat(s, FREE_SLAB);\n\t\tfree_slab(s, slab_free);\n\t}\n}\n\n \nstatic void __slab_free(struct kmem_cache *s, struct slab *slab,\n\t\t\tvoid *head, void *tail, int cnt,\n\t\t\tunsigned long addr)\n\n{\n\tvoid *prior;\n\tint was_frozen;\n\tstruct slab new;\n\tunsigned long counters;\n\tstruct kmem_cache_node *n = NULL;\n\tunsigned long flags;\n\n\tstat(s, FREE_SLOWPATH);\n\n\tif (kfence_free(head))\n\t\treturn;\n\n\tif (IS_ENABLED(CONFIG_SLUB_TINY) || kmem_cache_debug(s)) {\n\t\tfree_to_partial_list(s, slab, head, tail, cnt, addr);\n\t\treturn;\n\t}\n\n\tdo {\n\t\tif (unlikely(n)) {\n\t\t\tspin_unlock_irqrestore(&n->list_lock, flags);\n\t\t\tn = NULL;\n\t\t}\n\t\tprior = slab->freelist;\n\t\tcounters = slab->counters;\n\t\tset_freepointer(s, tail, prior);\n\t\tnew.counters = counters;\n\t\twas_frozen = new.frozen;\n\t\tnew.inuse -= cnt;\n\t\tif ((!new.inuse || !prior) && !was_frozen) {\n\n\t\t\tif (kmem_cache_has_cpu_partial(s) && !prior) {\n\n\t\t\t\t \n\t\t\t\tnew.frozen = 1;\n\n\t\t\t} else {  \n\n\t\t\t\tn = get_node(s, slab_nid(slab));\n\t\t\t\t \n\t\t\t\tspin_lock_irqsave(&n->list_lock, flags);\n\n\t\t\t}\n\t\t}\n\n\t} while (!slab_update_freelist(s, slab,\n\t\tprior, counters,\n\t\thead, new.counters,\n\t\t\"__slab_free\"));\n\n\tif (likely(!n)) {\n\n\t\tif (likely(was_frozen)) {\n\t\t\t \n\t\t\tstat(s, FREE_FROZEN);\n\t\t} else if (new.frozen) {\n\t\t\t \n\t\t\tput_cpu_partial(s, slab, 1);\n\t\t\tstat(s, CPU_PARTIAL_FREE);\n\t\t}\n\n\t\treturn;\n\t}\n\n\tif (unlikely(!new.inuse && n->nr_partial >= s->min_partial))\n\t\tgoto slab_empty;\n\n\t \n\tif (!kmem_cache_has_cpu_partial(s) && unlikely(!prior)) {\n\t\tremove_full(s, n, slab);\n\t\tadd_partial(n, slab, DEACTIVATE_TO_TAIL);\n\t\tstat(s, FREE_ADD_PARTIAL);\n\t}\n\tspin_unlock_irqrestore(&n->list_lock, flags);\n\treturn;\n\nslab_empty:\n\tif (prior) {\n\t\t \n\t\tremove_partial(n, slab);\n\t\tstat(s, FREE_REMOVE_PARTIAL);\n\t} else {\n\t\t \n\t\tremove_full(s, n, slab);\n\t}\n\n\tspin_unlock_irqrestore(&n->list_lock, flags);\n\tstat(s, FREE_SLAB);\n\tdiscard_slab(s, slab);\n}\n\n#ifndef CONFIG_SLUB_TINY\n \nstatic __always_inline void do_slab_free(struct kmem_cache *s,\n\t\t\t\tstruct slab *slab, void *head, void *tail,\n\t\t\t\tint cnt, unsigned long addr)\n{\n\tvoid *tail_obj = tail ? : head;\n\tstruct kmem_cache_cpu *c;\n\tunsigned long tid;\n\tvoid **freelist;\n\nredo:\n\t \n\tc = raw_cpu_ptr(s->cpu_slab);\n\ttid = READ_ONCE(c->tid);\n\n\t \n\tbarrier();\n\n\tif (unlikely(slab != c->slab)) {\n\t\t__slab_free(s, slab, head, tail_obj, cnt, addr);\n\t\treturn;\n\t}\n\n\tif (USE_LOCKLESS_FAST_PATH()) {\n\t\tfreelist = READ_ONCE(c->freelist);\n\n\t\tset_freepointer(s, tail_obj, freelist);\n\n\t\tif (unlikely(!__update_cpu_freelist_fast(s, freelist, head, tid))) {\n\t\t\tnote_cmpxchg_failure(\"slab_free\", s, tid);\n\t\t\tgoto redo;\n\t\t}\n\t} else {\n\t\t \n\t\tlocal_lock(&s->cpu_slab->lock);\n\t\tc = this_cpu_ptr(s->cpu_slab);\n\t\tif (unlikely(slab != c->slab)) {\n\t\t\tlocal_unlock(&s->cpu_slab->lock);\n\t\t\tgoto redo;\n\t\t}\n\t\ttid = c->tid;\n\t\tfreelist = c->freelist;\n\n\t\tset_freepointer(s, tail_obj, freelist);\n\t\tc->freelist = head;\n\t\tc->tid = next_tid(tid);\n\n\t\tlocal_unlock(&s->cpu_slab->lock);\n\t}\n\tstat(s, FREE_FASTPATH);\n}\n#else  \nstatic void do_slab_free(struct kmem_cache *s,\n\t\t\t\tstruct slab *slab, void *head, void *tail,\n\t\t\t\tint cnt, unsigned long addr)\n{\n\tvoid *tail_obj = tail ? : head;\n\n\t__slab_free(s, slab, head, tail_obj, cnt, addr);\n}\n#endif  \n\nstatic __fastpath_inline void slab_free(struct kmem_cache *s, struct slab *slab,\n\t\t\t\t      void *head, void *tail, void **p, int cnt,\n\t\t\t\t      unsigned long addr)\n{\n\tmemcg_slab_free_hook(s, slab, p, cnt);\n\t \n\tif (slab_free_freelist_hook(s, &head, &tail, &cnt))\n\t\tdo_slab_free(s, slab, head, tail, cnt, addr);\n}\n\n#ifdef CONFIG_KASAN_GENERIC\nvoid ___cache_free(struct kmem_cache *cache, void *x, unsigned long addr)\n{\n\tdo_slab_free(cache, virt_to_slab(x), x, NULL, 1, addr);\n}\n#endif\n\nvoid __kmem_cache_free(struct kmem_cache *s, void *x, unsigned long caller)\n{\n\tslab_free(s, virt_to_slab(x), x, NULL, &x, 1, caller);\n}\n\nvoid kmem_cache_free(struct kmem_cache *s, void *x)\n{\n\ts = cache_from_obj(s, x);\n\tif (!s)\n\t\treturn;\n\ttrace_kmem_cache_free(_RET_IP_, x, s);\n\tslab_free(s, virt_to_slab(x), x, NULL, &x, 1, _RET_IP_);\n}\nEXPORT_SYMBOL(kmem_cache_free);\n\nstruct detached_freelist {\n\tstruct slab *slab;\n\tvoid *tail;\n\tvoid *freelist;\n\tint cnt;\n\tstruct kmem_cache *s;\n};\n\n \nstatic inline\nint build_detached_freelist(struct kmem_cache *s, size_t size,\n\t\t\t    void **p, struct detached_freelist *df)\n{\n\tint lookahead = 3;\n\tvoid *object;\n\tstruct folio *folio;\n\tsize_t same;\n\n\tobject = p[--size];\n\tfolio = virt_to_folio(object);\n\tif (!s) {\n\t\t \n\t\tif (unlikely(!folio_test_slab(folio))) {\n\t\t\tfree_large_kmalloc(folio, object);\n\t\t\tdf->slab = NULL;\n\t\t\treturn size;\n\t\t}\n\t\t \n\t\tdf->slab = folio_slab(folio);\n\t\tdf->s = df->slab->slab_cache;\n\t} else {\n\t\tdf->slab = folio_slab(folio);\n\t\tdf->s = cache_from_obj(s, object);  \n\t}\n\n\t \n\tdf->tail = object;\n\tdf->freelist = object;\n\tdf->cnt = 1;\n\n\tif (is_kfence_address(object))\n\t\treturn size;\n\n\tset_freepointer(df->s, object, NULL);\n\n\tsame = size;\n\twhile (size) {\n\t\tobject = p[--size];\n\t\t \n\t\tif (df->slab == virt_to_slab(object)) {\n\t\t\t \n\t\t\tset_freepointer(df->s, object, df->freelist);\n\t\t\tdf->freelist = object;\n\t\t\tdf->cnt++;\n\t\t\tsame--;\n\t\t\tif (size != same)\n\t\t\t\tswap(p[size], p[same]);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (!--lookahead)\n\t\t\tbreak;\n\t}\n\n\treturn same;\n}\n\n \nvoid kmem_cache_free_bulk(struct kmem_cache *s, size_t size, void **p)\n{\n\tif (!size)\n\t\treturn;\n\n\tdo {\n\t\tstruct detached_freelist df;\n\n\t\tsize = build_detached_freelist(s, size, p, &df);\n\t\tif (!df.slab)\n\t\t\tcontinue;\n\n\t\tslab_free(df.s, df.slab, df.freelist, df.tail, &p[size], df.cnt,\n\t\t\t  _RET_IP_);\n\t} while (likely(size));\n}\nEXPORT_SYMBOL(kmem_cache_free_bulk);\n\n#ifndef CONFIG_SLUB_TINY\nstatic inline int __kmem_cache_alloc_bulk(struct kmem_cache *s, gfp_t flags,\n\t\t\tsize_t size, void **p, struct obj_cgroup *objcg)\n{\n\tstruct kmem_cache_cpu *c;\n\tunsigned long irqflags;\n\tint i;\n\n\t \n\tc = slub_get_cpu_ptr(s->cpu_slab);\n\tlocal_lock_irqsave(&s->cpu_slab->lock, irqflags);\n\n\tfor (i = 0; i < size; i++) {\n\t\tvoid *object = kfence_alloc(s, s->object_size, flags);\n\n\t\tif (unlikely(object)) {\n\t\t\tp[i] = object;\n\t\t\tcontinue;\n\t\t}\n\n\t\tobject = c->freelist;\n\t\tif (unlikely(!object)) {\n\t\t\t \n\t\t\tc->tid = next_tid(c->tid);\n\n\t\t\tlocal_unlock_irqrestore(&s->cpu_slab->lock, irqflags);\n\n\t\t\t \n\t\t\tp[i] = ___slab_alloc(s, flags, NUMA_NO_NODE,\n\t\t\t\t\t    _RET_IP_, c, s->object_size);\n\t\t\tif (unlikely(!p[i]))\n\t\t\t\tgoto error;\n\n\t\t\tc = this_cpu_ptr(s->cpu_slab);\n\t\t\tmaybe_wipe_obj_freeptr(s, p[i]);\n\n\t\t\tlocal_lock_irqsave(&s->cpu_slab->lock, irqflags);\n\n\t\t\tcontinue;  \n\t\t}\n\t\tc->freelist = get_freepointer(s, object);\n\t\tp[i] = object;\n\t\tmaybe_wipe_obj_freeptr(s, p[i]);\n\t}\n\tc->tid = next_tid(c->tid);\n\tlocal_unlock_irqrestore(&s->cpu_slab->lock, irqflags);\n\tslub_put_cpu_ptr(s->cpu_slab);\n\n\treturn i;\n\nerror:\n\tslub_put_cpu_ptr(s->cpu_slab);\n\tslab_post_alloc_hook(s, objcg, flags, i, p, false, s->object_size);\n\tkmem_cache_free_bulk(s, i, p);\n\treturn 0;\n\n}\n#else  \nstatic int __kmem_cache_alloc_bulk(struct kmem_cache *s, gfp_t flags,\n\t\t\tsize_t size, void **p, struct obj_cgroup *objcg)\n{\n\tint i;\n\n\tfor (i = 0; i < size; i++) {\n\t\tvoid *object = kfence_alloc(s, s->object_size, flags);\n\n\t\tif (unlikely(object)) {\n\t\t\tp[i] = object;\n\t\t\tcontinue;\n\t\t}\n\n\t\tp[i] = __slab_alloc_node(s, flags, NUMA_NO_NODE,\n\t\t\t\t\t _RET_IP_, s->object_size);\n\t\tif (unlikely(!p[i]))\n\t\t\tgoto error;\n\n\t\tmaybe_wipe_obj_freeptr(s, p[i]);\n\t}\n\n\treturn i;\n\nerror:\n\tslab_post_alloc_hook(s, objcg, flags, i, p, false, s->object_size);\n\tkmem_cache_free_bulk(s, i, p);\n\treturn 0;\n}\n#endif  \n\n \nint kmem_cache_alloc_bulk(struct kmem_cache *s, gfp_t flags, size_t size,\n\t\t\t  void **p)\n{\n\tint i;\n\tstruct obj_cgroup *objcg = NULL;\n\n\tif (!size)\n\t\treturn 0;\n\n\t \n\ts = slab_pre_alloc_hook(s, NULL, &objcg, size, flags);\n\tif (unlikely(!s))\n\t\treturn 0;\n\n\ti = __kmem_cache_alloc_bulk(s, flags, size, p, objcg);\n\n\t \n\tif (i != 0)\n\t\tslab_post_alloc_hook(s, objcg, flags, size, p,\n\t\t\tslab_want_init_on_alloc(flags, s), s->object_size);\n\treturn i;\n}\nEXPORT_SYMBOL(kmem_cache_alloc_bulk);\n\n\n \n\n \nstatic unsigned int slub_min_order;\nstatic unsigned int slub_max_order =\n\tIS_ENABLED(CONFIG_SLUB_TINY) ? 1 : PAGE_ALLOC_COSTLY_ORDER;\nstatic unsigned int slub_min_objects;\n\n \nstatic inline unsigned int calc_slab_order(unsigned int size,\n\t\tunsigned int min_objects, unsigned int max_order,\n\t\tunsigned int fract_leftover)\n{\n\tunsigned int min_order = slub_min_order;\n\tunsigned int order;\n\n\tif (order_objects(min_order, size) > MAX_OBJS_PER_PAGE)\n\t\treturn get_order(size * MAX_OBJS_PER_PAGE) - 1;\n\n\tfor (order = max(min_order, (unsigned int)get_order(min_objects * size));\n\t\t\torder <= max_order; order++) {\n\n\t\tunsigned int slab_size = (unsigned int)PAGE_SIZE << order;\n\t\tunsigned int rem;\n\n\t\trem = slab_size % size;\n\n\t\tif (rem <= slab_size / fract_leftover)\n\t\t\tbreak;\n\t}\n\n\treturn order;\n}\n\nstatic inline int calculate_order(unsigned int size)\n{\n\tunsigned int order;\n\tunsigned int min_objects;\n\tunsigned int max_objects;\n\tunsigned int nr_cpus;\n\n\t \n\tmin_objects = slub_min_objects;\n\tif (!min_objects) {\n\t\t \n\t\tnr_cpus = num_present_cpus();\n\t\tif (nr_cpus <= 1)\n\t\t\tnr_cpus = nr_cpu_ids;\n\t\tmin_objects = 4 * (fls(nr_cpus) + 1);\n\t}\n\tmax_objects = order_objects(slub_max_order, size);\n\tmin_objects = min(min_objects, max_objects);\n\n\twhile (min_objects > 1) {\n\t\tunsigned int fraction;\n\n\t\tfraction = 16;\n\t\twhile (fraction >= 4) {\n\t\t\torder = calc_slab_order(size, min_objects,\n\t\t\t\t\tslub_max_order, fraction);\n\t\t\tif (order <= slub_max_order)\n\t\t\t\treturn order;\n\t\t\tfraction /= 2;\n\t\t}\n\t\tmin_objects--;\n\t}\n\n\t \n\torder = calc_slab_order(size, 1, slub_max_order, 1);\n\tif (order <= slub_max_order)\n\t\treturn order;\n\n\t \n\torder = calc_slab_order(size, 1, MAX_ORDER, 1);\n\tif (order <= MAX_ORDER)\n\t\treturn order;\n\treturn -ENOSYS;\n}\n\nstatic void\ninit_kmem_cache_node(struct kmem_cache_node *n)\n{\n\tn->nr_partial = 0;\n\tspin_lock_init(&n->list_lock);\n\tINIT_LIST_HEAD(&n->partial);\n#ifdef CONFIG_SLUB_DEBUG\n\tatomic_long_set(&n->nr_slabs, 0);\n\tatomic_long_set(&n->total_objects, 0);\n\tINIT_LIST_HEAD(&n->full);\n#endif\n}\n\n#ifndef CONFIG_SLUB_TINY\nstatic inline int alloc_kmem_cache_cpus(struct kmem_cache *s)\n{\n\tBUILD_BUG_ON(PERCPU_DYNAMIC_EARLY_SIZE <\n\t\t\tNR_KMALLOC_TYPES * KMALLOC_SHIFT_HIGH *\n\t\t\tsizeof(struct kmem_cache_cpu));\n\n\t \n\ts->cpu_slab = __alloc_percpu(sizeof(struct kmem_cache_cpu),\n\t\t\t\t     2 * sizeof(void *));\n\n\tif (!s->cpu_slab)\n\t\treturn 0;\n\n\tinit_kmem_cache_cpus(s);\n\n\treturn 1;\n}\n#else\nstatic inline int alloc_kmem_cache_cpus(struct kmem_cache *s)\n{\n\treturn 1;\n}\n#endif  \n\nstatic struct kmem_cache *kmem_cache_node;\n\n \nstatic void early_kmem_cache_node_alloc(int node)\n{\n\tstruct slab *slab;\n\tstruct kmem_cache_node *n;\n\n\tBUG_ON(kmem_cache_node->size < sizeof(struct kmem_cache_node));\n\n\tslab = new_slab(kmem_cache_node, GFP_NOWAIT, node);\n\n\tBUG_ON(!slab);\n\tinc_slabs_node(kmem_cache_node, slab_nid(slab), slab->objects);\n\tif (slab_nid(slab) != node) {\n\t\tpr_err(\"SLUB: Unable to allocate memory from node %d\\n\", node);\n\t\tpr_err(\"SLUB: Allocating a useless per node structure in order to be able to continue\\n\");\n\t}\n\n\tn = slab->freelist;\n\tBUG_ON(!n);\n#ifdef CONFIG_SLUB_DEBUG\n\tinit_object(kmem_cache_node, n, SLUB_RED_ACTIVE);\n\tinit_tracking(kmem_cache_node, n);\n#endif\n\tn = kasan_slab_alloc(kmem_cache_node, n, GFP_KERNEL, false);\n\tslab->freelist = get_freepointer(kmem_cache_node, n);\n\tslab->inuse = 1;\n\tkmem_cache_node->node[node] = n;\n\tinit_kmem_cache_node(n);\n\tinc_slabs_node(kmem_cache_node, node, slab->objects);\n\n\t \n\t__add_partial(n, slab, DEACTIVATE_TO_HEAD);\n}\n\nstatic void free_kmem_cache_nodes(struct kmem_cache *s)\n{\n\tint node;\n\tstruct kmem_cache_node *n;\n\n\tfor_each_kmem_cache_node(s, node, n) {\n\t\ts->node[node] = NULL;\n\t\tkmem_cache_free(kmem_cache_node, n);\n\t}\n}\n\nvoid __kmem_cache_release(struct kmem_cache *s)\n{\n\tcache_random_seq_destroy(s);\n#ifndef CONFIG_SLUB_TINY\n\tfree_percpu(s->cpu_slab);\n#endif\n\tfree_kmem_cache_nodes(s);\n}\n\nstatic int init_kmem_cache_nodes(struct kmem_cache *s)\n{\n\tint node;\n\n\tfor_each_node_mask(node, slab_nodes) {\n\t\tstruct kmem_cache_node *n;\n\n\t\tif (slab_state == DOWN) {\n\t\t\tearly_kmem_cache_node_alloc(node);\n\t\t\tcontinue;\n\t\t}\n\t\tn = kmem_cache_alloc_node(kmem_cache_node,\n\t\t\t\t\t\tGFP_KERNEL, node);\n\n\t\tif (!n) {\n\t\t\tfree_kmem_cache_nodes(s);\n\t\t\treturn 0;\n\t\t}\n\n\t\tinit_kmem_cache_node(n);\n\t\ts->node[node] = n;\n\t}\n\treturn 1;\n}\n\nstatic void set_cpu_partial(struct kmem_cache *s)\n{\n#ifdef CONFIG_SLUB_CPU_PARTIAL\n\tunsigned int nr_objects;\n\n\t \n\tif (!kmem_cache_has_cpu_partial(s))\n\t\tnr_objects = 0;\n\telse if (s->size >= PAGE_SIZE)\n\t\tnr_objects = 6;\n\telse if (s->size >= 1024)\n\t\tnr_objects = 24;\n\telse if (s->size >= 256)\n\t\tnr_objects = 52;\n\telse\n\t\tnr_objects = 120;\n\n\tslub_set_cpu_partial(s, nr_objects);\n#endif\n}\n\n \nstatic int calculate_sizes(struct kmem_cache *s)\n{\n\tslab_flags_t flags = s->flags;\n\tunsigned int size = s->object_size;\n\tunsigned int order;\n\n\t \n\tsize = ALIGN(size, sizeof(void *));\n\n#ifdef CONFIG_SLUB_DEBUG\n\t \n\tif ((flags & SLAB_POISON) && !(flags & SLAB_TYPESAFE_BY_RCU) &&\n\t\t\t!s->ctor)\n\t\ts->flags |= __OBJECT_POISON;\n\telse\n\t\ts->flags &= ~__OBJECT_POISON;\n\n\n\t \n\tif ((flags & SLAB_RED_ZONE) && size == s->object_size)\n\t\tsize += sizeof(void *);\n#endif\n\n\t \n\ts->inuse = size;\n\n\tif (slub_debug_orig_size(s) ||\n\t    (flags & (SLAB_TYPESAFE_BY_RCU | SLAB_POISON)) ||\n\t    ((flags & SLAB_RED_ZONE) && s->object_size < sizeof(void *)) ||\n\t    s->ctor) {\n\t\t \n\t\ts->offset = size;\n\t\tsize += sizeof(void *);\n\t} else {\n\t\t \n\t\ts->offset = ALIGN_DOWN(s->object_size / 2, sizeof(void *));\n\t}\n\n#ifdef CONFIG_SLUB_DEBUG\n\tif (flags & SLAB_STORE_USER) {\n\t\t \n\t\tsize += 2 * sizeof(struct track);\n\n\t\t \n\t\tif (flags & SLAB_KMALLOC)\n\t\t\tsize += sizeof(unsigned int);\n\t}\n#endif\n\n\tkasan_cache_create(s, &size, &s->flags);\n#ifdef CONFIG_SLUB_DEBUG\n\tif (flags & SLAB_RED_ZONE) {\n\t\t \n\t\tsize += sizeof(void *);\n\n\t\ts->red_left_pad = sizeof(void *);\n\t\ts->red_left_pad = ALIGN(s->red_left_pad, s->align);\n\t\tsize += s->red_left_pad;\n\t}\n#endif\n\n\t \n\tsize = ALIGN(size, s->align);\n\ts->size = size;\n\ts->reciprocal_size = reciprocal_value(size);\n\torder = calculate_order(size);\n\n\tif ((int)order < 0)\n\t\treturn 0;\n\n\ts->allocflags = 0;\n\tif (order)\n\t\ts->allocflags |= __GFP_COMP;\n\n\tif (s->flags & SLAB_CACHE_DMA)\n\t\ts->allocflags |= GFP_DMA;\n\n\tif (s->flags & SLAB_CACHE_DMA32)\n\t\ts->allocflags |= GFP_DMA32;\n\n\tif (s->flags & SLAB_RECLAIM_ACCOUNT)\n\t\ts->allocflags |= __GFP_RECLAIMABLE;\n\n\t \n\ts->oo = oo_make(order, size);\n\ts->min = oo_make(get_order(size), size);\n\n\treturn !!oo_objects(s->oo);\n}\n\nstatic int kmem_cache_open(struct kmem_cache *s, slab_flags_t flags)\n{\n\ts->flags = kmem_cache_flags(s->size, flags, s->name);\n#ifdef CONFIG_SLAB_FREELIST_HARDENED\n\ts->random = get_random_long();\n#endif\n\n\tif (!calculate_sizes(s))\n\t\tgoto error;\n\tif (disable_higher_order_debug) {\n\t\t \n\t\tif (get_order(s->size) > get_order(s->object_size)) {\n\t\t\ts->flags &= ~DEBUG_METADATA_FLAGS;\n\t\t\ts->offset = 0;\n\t\t\tif (!calculate_sizes(s))\n\t\t\t\tgoto error;\n\t\t}\n\t}\n\n#ifdef system_has_freelist_aba\n\tif (system_has_freelist_aba() && !(s->flags & SLAB_NO_CMPXCHG)) {\n\t\t \n\t\ts->flags |= __CMPXCHG_DOUBLE;\n\t}\n#endif\n\n\t \n\ts->min_partial = min_t(unsigned long, MAX_PARTIAL, ilog2(s->size) / 2);\n\ts->min_partial = max_t(unsigned long, MIN_PARTIAL, s->min_partial);\n\n\tset_cpu_partial(s);\n\n#ifdef CONFIG_NUMA\n\ts->remote_node_defrag_ratio = 1000;\n#endif\n\n\t \n\tif (slab_state >= UP) {\n\t\tif (init_cache_random_seq(s))\n\t\t\tgoto error;\n\t}\n\n\tif (!init_kmem_cache_nodes(s))\n\t\tgoto error;\n\n\tif (alloc_kmem_cache_cpus(s))\n\t\treturn 0;\n\nerror:\n\t__kmem_cache_release(s);\n\treturn -EINVAL;\n}\n\nstatic void list_slab_objects(struct kmem_cache *s, struct slab *slab,\n\t\t\t      const char *text)\n{\n#ifdef CONFIG_SLUB_DEBUG\n\tvoid *addr = slab_address(slab);\n\tvoid *p;\n\n\tslab_err(s, slab, text, s->name);\n\n\tspin_lock(&object_map_lock);\n\t__fill_map(object_map, s, slab);\n\n\tfor_each_object(p, s, addr, slab->objects) {\n\n\t\tif (!test_bit(__obj_to_index(s, addr, p), object_map)) {\n\t\t\tpr_err(\"Object 0x%p @offset=%tu\\n\", p, p - addr);\n\t\t\tprint_tracking(s, p);\n\t\t}\n\t}\n\tspin_unlock(&object_map_lock);\n#endif\n}\n\n \nstatic void free_partial(struct kmem_cache *s, struct kmem_cache_node *n)\n{\n\tLIST_HEAD(discard);\n\tstruct slab *slab, *h;\n\n\tBUG_ON(irqs_disabled());\n\tspin_lock_irq(&n->list_lock);\n\tlist_for_each_entry_safe(slab, h, &n->partial, slab_list) {\n\t\tif (!slab->inuse) {\n\t\t\tremove_partial(n, slab);\n\t\t\tlist_add(&slab->slab_list, &discard);\n\t\t} else {\n\t\t\tlist_slab_objects(s, slab,\n\t\t\t  \"Objects remaining in %s on __kmem_cache_shutdown()\");\n\t\t}\n\t}\n\tspin_unlock_irq(&n->list_lock);\n\n\tlist_for_each_entry_safe(slab, h, &discard, slab_list)\n\t\tdiscard_slab(s, slab);\n}\n\nbool __kmem_cache_empty(struct kmem_cache *s)\n{\n\tint node;\n\tstruct kmem_cache_node *n;\n\n\tfor_each_kmem_cache_node(s, node, n)\n\t\tif (n->nr_partial || node_nr_slabs(n))\n\t\t\treturn false;\n\treturn true;\n}\n\n \nint __kmem_cache_shutdown(struct kmem_cache *s)\n{\n\tint node;\n\tstruct kmem_cache_node *n;\n\n\tflush_all_cpus_locked(s);\n\t \n\tfor_each_kmem_cache_node(s, node, n) {\n\t\tfree_partial(s, n);\n\t\tif (n->nr_partial || node_nr_slabs(n))\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}\n\n#ifdef CONFIG_PRINTK\nvoid __kmem_obj_info(struct kmem_obj_info *kpp, void *object, struct slab *slab)\n{\n\tvoid *base;\n\tint __maybe_unused i;\n\tunsigned int objnr;\n\tvoid *objp;\n\tvoid *objp0;\n\tstruct kmem_cache *s = slab->slab_cache;\n\tstruct track __maybe_unused *trackp;\n\n\tkpp->kp_ptr = object;\n\tkpp->kp_slab = slab;\n\tkpp->kp_slab_cache = s;\n\tbase = slab_address(slab);\n\tobjp0 = kasan_reset_tag(object);\n#ifdef CONFIG_SLUB_DEBUG\n\tobjp = restore_red_left(s, objp0);\n#else\n\tobjp = objp0;\n#endif\n\tobjnr = obj_to_index(s, slab, objp);\n\tkpp->kp_data_offset = (unsigned long)((char *)objp0 - (char *)objp);\n\tobjp = base + s->size * objnr;\n\tkpp->kp_objp = objp;\n\tif (WARN_ON_ONCE(objp < base || objp >= base + slab->objects * s->size\n\t\t\t || (objp - base) % s->size) ||\n\t    !(s->flags & SLAB_STORE_USER))\n\t\treturn;\n#ifdef CONFIG_SLUB_DEBUG\n\tobjp = fixup_red_left(s, objp);\n\ttrackp = get_track(s, objp, TRACK_ALLOC);\n\tkpp->kp_ret = (void *)trackp->addr;\n#ifdef CONFIG_STACKDEPOT\n\t{\n\t\tdepot_stack_handle_t handle;\n\t\tunsigned long *entries;\n\t\tunsigned int nr_entries;\n\n\t\thandle = READ_ONCE(trackp->handle);\n\t\tif (handle) {\n\t\t\tnr_entries = stack_depot_fetch(handle, &entries);\n\t\t\tfor (i = 0; i < KS_ADDRS_COUNT && i < nr_entries; i++)\n\t\t\t\tkpp->kp_stack[i] = (void *)entries[i];\n\t\t}\n\n\t\ttrackp = get_track(s, objp, TRACK_FREE);\n\t\thandle = READ_ONCE(trackp->handle);\n\t\tif (handle) {\n\t\t\tnr_entries = stack_depot_fetch(handle, &entries);\n\t\t\tfor (i = 0; i < KS_ADDRS_COUNT && i < nr_entries; i++)\n\t\t\t\tkpp->kp_free_stack[i] = (void *)entries[i];\n\t\t}\n\t}\n#endif\n#endif\n}\n#endif\n\n \n\nstatic int __init setup_slub_min_order(char *str)\n{\n\tget_option(&str, (int *)&slub_min_order);\n\n\treturn 1;\n}\n\n__setup(\"slub_min_order=\", setup_slub_min_order);\n\nstatic int __init setup_slub_max_order(char *str)\n{\n\tget_option(&str, (int *)&slub_max_order);\n\tslub_max_order = min_t(unsigned int, slub_max_order, MAX_ORDER);\n\n\treturn 1;\n}\n\n__setup(\"slub_max_order=\", setup_slub_max_order);\n\nstatic int __init setup_slub_min_objects(char *str)\n{\n\tget_option(&str, (int *)&slub_min_objects);\n\n\treturn 1;\n}\n\n__setup(\"slub_min_objects=\", setup_slub_min_objects);\n\n#ifdef CONFIG_HARDENED_USERCOPY\n \nvoid __check_heap_object(const void *ptr, unsigned long n,\n\t\t\t const struct slab *slab, bool to_user)\n{\n\tstruct kmem_cache *s;\n\tunsigned int offset;\n\tbool is_kfence = is_kfence_address(ptr);\n\n\tptr = kasan_reset_tag(ptr);\n\n\t \n\ts = slab->slab_cache;\n\n\t \n\tif (ptr < slab_address(slab))\n\t\tusercopy_abort(\"SLUB object not in SLUB page?!\", NULL,\n\t\t\t       to_user, 0, n);\n\n\t \n\tif (is_kfence)\n\t\toffset = ptr - kfence_object_start(ptr);\n\telse\n\t\toffset = (ptr - slab_address(slab)) % s->size;\n\n\t \n\tif (!is_kfence && kmem_cache_debug_flags(s, SLAB_RED_ZONE)) {\n\t\tif (offset < s->red_left_pad)\n\t\t\tusercopy_abort(\"SLUB object in left red zone\",\n\t\t\t\t       s->name, to_user, offset, n);\n\t\toffset -= s->red_left_pad;\n\t}\n\n\t \n\tif (offset >= s->useroffset &&\n\t    offset - s->useroffset <= s->usersize &&\n\t    n <= s->useroffset - offset + s->usersize)\n\t\treturn;\n\n\tusercopy_abort(\"SLUB object\", s->name, to_user, offset, n);\n}\n#endif  \n\n#define SHRINK_PROMOTE_MAX 32\n\n \nstatic int __kmem_cache_do_shrink(struct kmem_cache *s)\n{\n\tint node;\n\tint i;\n\tstruct kmem_cache_node *n;\n\tstruct slab *slab;\n\tstruct slab *t;\n\tstruct list_head discard;\n\tstruct list_head promote[SHRINK_PROMOTE_MAX];\n\tunsigned long flags;\n\tint ret = 0;\n\n\tfor_each_kmem_cache_node(s, node, n) {\n\t\tINIT_LIST_HEAD(&discard);\n\t\tfor (i = 0; i < SHRINK_PROMOTE_MAX; i++)\n\t\t\tINIT_LIST_HEAD(promote + i);\n\n\t\tspin_lock_irqsave(&n->list_lock, flags);\n\n\t\t \n\t\tlist_for_each_entry_safe(slab, t, &n->partial, slab_list) {\n\t\t\tint free = slab->objects - slab->inuse;\n\n\t\t\t \n\t\t\tbarrier();\n\n\t\t\t \n\t\t\tBUG_ON(free <= 0);\n\n\t\t\tif (free == slab->objects) {\n\t\t\t\tlist_move(&slab->slab_list, &discard);\n\t\t\t\tn->nr_partial--;\n\t\t\t\tdec_slabs_node(s, node, slab->objects);\n\t\t\t} else if (free <= SHRINK_PROMOTE_MAX)\n\t\t\t\tlist_move(&slab->slab_list, promote + free - 1);\n\t\t}\n\n\t\t \n\t\tfor (i = SHRINK_PROMOTE_MAX - 1; i >= 0; i--)\n\t\t\tlist_splice(promote + i, &n->partial);\n\n\t\tspin_unlock_irqrestore(&n->list_lock, flags);\n\n\t\t \n\t\tlist_for_each_entry_safe(slab, t, &discard, slab_list)\n\t\t\tfree_slab(s, slab);\n\n\t\tif (node_nr_slabs(n))\n\t\t\tret = 1;\n\t}\n\n\treturn ret;\n}\n\nint __kmem_cache_shrink(struct kmem_cache *s)\n{\n\tflush_all(s);\n\treturn __kmem_cache_do_shrink(s);\n}\n\nstatic int slab_mem_going_offline_callback(void *arg)\n{\n\tstruct kmem_cache *s;\n\n\tmutex_lock(&slab_mutex);\n\tlist_for_each_entry(s, &slab_caches, list) {\n\t\tflush_all_cpus_locked(s);\n\t\t__kmem_cache_do_shrink(s);\n\t}\n\tmutex_unlock(&slab_mutex);\n\n\treturn 0;\n}\n\nstatic void slab_mem_offline_callback(void *arg)\n{\n\tstruct memory_notify *marg = arg;\n\tint offline_node;\n\n\toffline_node = marg->status_change_nid_normal;\n\n\t \n\tif (offline_node < 0)\n\t\treturn;\n\n\tmutex_lock(&slab_mutex);\n\tnode_clear(offline_node, slab_nodes);\n\t \n\tmutex_unlock(&slab_mutex);\n}\n\nstatic int slab_mem_going_online_callback(void *arg)\n{\n\tstruct kmem_cache_node *n;\n\tstruct kmem_cache *s;\n\tstruct memory_notify *marg = arg;\n\tint nid = marg->status_change_nid_normal;\n\tint ret = 0;\n\n\t \n\tif (nid < 0)\n\t\treturn 0;\n\n\t \n\tmutex_lock(&slab_mutex);\n\tlist_for_each_entry(s, &slab_caches, list) {\n\t\t \n\t\tif (get_node(s, nid))\n\t\t\tcontinue;\n\t\t \n\t\tn = kmem_cache_alloc(kmem_cache_node, GFP_KERNEL);\n\t\tif (!n) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tinit_kmem_cache_node(n);\n\t\ts->node[nid] = n;\n\t}\n\t \n\tnode_set(nid, slab_nodes);\nout:\n\tmutex_unlock(&slab_mutex);\n\treturn ret;\n}\n\nstatic int slab_memory_callback(struct notifier_block *self,\n\t\t\t\tunsigned long action, void *arg)\n{\n\tint ret = 0;\n\n\tswitch (action) {\n\tcase MEM_GOING_ONLINE:\n\t\tret = slab_mem_going_online_callback(arg);\n\t\tbreak;\n\tcase MEM_GOING_OFFLINE:\n\t\tret = slab_mem_going_offline_callback(arg);\n\t\tbreak;\n\tcase MEM_OFFLINE:\n\tcase MEM_CANCEL_ONLINE:\n\t\tslab_mem_offline_callback(arg);\n\t\tbreak;\n\tcase MEM_ONLINE:\n\tcase MEM_CANCEL_OFFLINE:\n\t\tbreak;\n\t}\n\tif (ret)\n\t\tret = notifier_from_errno(ret);\n\telse\n\t\tret = NOTIFY_OK;\n\treturn ret;\n}\n\n \n\n \n\nstatic struct kmem_cache * __init bootstrap(struct kmem_cache *static_cache)\n{\n\tint node;\n\tstruct kmem_cache *s = kmem_cache_zalloc(kmem_cache, GFP_NOWAIT);\n\tstruct kmem_cache_node *n;\n\n\tmemcpy(s, static_cache, kmem_cache->object_size);\n\n\t \n\t__flush_cpu_slab(s, smp_processor_id());\n\tfor_each_kmem_cache_node(s, node, n) {\n\t\tstruct slab *p;\n\n\t\tlist_for_each_entry(p, &n->partial, slab_list)\n\t\t\tp->slab_cache = s;\n\n#ifdef CONFIG_SLUB_DEBUG\n\t\tlist_for_each_entry(p, &n->full, slab_list)\n\t\t\tp->slab_cache = s;\n#endif\n\t}\n\tlist_add(&s->list, &slab_caches);\n\treturn s;\n}\n\nvoid __init kmem_cache_init(void)\n{\n\tstatic __initdata struct kmem_cache boot_kmem_cache,\n\t\tboot_kmem_cache_node;\n\tint node;\n\n\tif (debug_guardpage_minorder())\n\t\tslub_max_order = 0;\n\n\t \n\tif (__slub_debug_enabled())\n\t\tno_hash_pointers_enable(NULL);\n\n\tkmem_cache_node = &boot_kmem_cache_node;\n\tkmem_cache = &boot_kmem_cache;\n\n\t \n\tfor_each_node_state(node, N_NORMAL_MEMORY)\n\t\tnode_set(node, slab_nodes);\n\n\tcreate_boot_cache(kmem_cache_node, \"kmem_cache_node\",\n\t\tsizeof(struct kmem_cache_node), SLAB_HWCACHE_ALIGN, 0, 0);\n\n\thotplug_memory_notifier(slab_memory_callback, SLAB_CALLBACK_PRI);\n\n\t \n\tslab_state = PARTIAL;\n\n\tcreate_boot_cache(kmem_cache, \"kmem_cache\",\n\t\t\toffsetof(struct kmem_cache, node) +\n\t\t\t\tnr_node_ids * sizeof(struct kmem_cache_node *),\n\t\t       SLAB_HWCACHE_ALIGN, 0, 0);\n\n\tkmem_cache = bootstrap(&boot_kmem_cache);\n\tkmem_cache_node = bootstrap(&boot_kmem_cache_node);\n\n\t \n\tsetup_kmalloc_cache_index_table();\n\tcreate_kmalloc_caches(0);\n\n\t \n\tinit_freelist_randomization();\n\n\tcpuhp_setup_state_nocalls(CPUHP_SLUB_DEAD, \"slub:dead\", NULL,\n\t\t\t\t  slub_cpu_dead);\n\n\tpr_info(\"SLUB: HWalign=%d, Order=%u-%u, MinObjects=%u, CPUs=%u, Nodes=%u\\n\",\n\t\tcache_line_size(),\n\t\tslub_min_order, slub_max_order, slub_min_objects,\n\t\tnr_cpu_ids, nr_node_ids);\n}\n\nvoid __init kmem_cache_init_late(void)\n{\n#ifndef CONFIG_SLUB_TINY\n\tflushwq = alloc_workqueue(\"slub_flushwq\", WQ_MEM_RECLAIM, 0);\n\tWARN_ON(!flushwq);\n#endif\n}\n\nstruct kmem_cache *\n__kmem_cache_alias(const char *name, unsigned int size, unsigned int align,\n\t\t   slab_flags_t flags, void (*ctor)(void *))\n{\n\tstruct kmem_cache *s;\n\n\ts = find_mergeable(size, align, flags, name, ctor);\n\tif (s) {\n\t\tif (sysfs_slab_alias(s, name))\n\t\t\treturn NULL;\n\n\t\ts->refcount++;\n\n\t\t \n\t\ts->object_size = max(s->object_size, size);\n\t\ts->inuse = max(s->inuse, ALIGN(size, sizeof(void *)));\n\t}\n\n\treturn s;\n}\n\nint __kmem_cache_create(struct kmem_cache *s, slab_flags_t flags)\n{\n\tint err;\n\n\terr = kmem_cache_open(s, flags);\n\tif (err)\n\t\treturn err;\n\n\t \n\tif (slab_state <= UP)\n\t\treturn 0;\n\n\terr = sysfs_slab_add(s);\n\tif (err) {\n\t\t__kmem_cache_release(s);\n\t\treturn err;\n\t}\n\n\tif (s->flags & SLAB_STORE_USER)\n\t\tdebugfs_slab_add(s);\n\n\treturn 0;\n}\n\n#ifdef SLAB_SUPPORTS_SYSFS\nstatic int count_inuse(struct slab *slab)\n{\n\treturn slab->inuse;\n}\n\nstatic int count_total(struct slab *slab)\n{\n\treturn slab->objects;\n}\n#endif\n\n#ifdef CONFIG_SLUB_DEBUG\nstatic void validate_slab(struct kmem_cache *s, struct slab *slab,\n\t\t\t  unsigned long *obj_map)\n{\n\tvoid *p;\n\tvoid *addr = slab_address(slab);\n\n\tif (!check_slab(s, slab) || !on_freelist(s, slab, NULL))\n\t\treturn;\n\n\t \n\t__fill_map(obj_map, s, slab);\n\tfor_each_object(p, s, addr, slab->objects) {\n\t\tu8 val = test_bit(__obj_to_index(s, addr, p), obj_map) ?\n\t\t\t SLUB_RED_INACTIVE : SLUB_RED_ACTIVE;\n\n\t\tif (!check_object(s, slab, p, val))\n\t\t\tbreak;\n\t}\n}\n\nstatic int validate_slab_node(struct kmem_cache *s,\n\t\tstruct kmem_cache_node *n, unsigned long *obj_map)\n{\n\tunsigned long count = 0;\n\tstruct slab *slab;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&n->list_lock, flags);\n\n\tlist_for_each_entry(slab, &n->partial, slab_list) {\n\t\tvalidate_slab(s, slab, obj_map);\n\t\tcount++;\n\t}\n\tif (count != n->nr_partial) {\n\t\tpr_err(\"SLUB %s: %ld partial slabs counted but counter=%ld\\n\",\n\t\t       s->name, count, n->nr_partial);\n\t\tslab_add_kunit_errors();\n\t}\n\n\tif (!(s->flags & SLAB_STORE_USER))\n\t\tgoto out;\n\n\tlist_for_each_entry(slab, &n->full, slab_list) {\n\t\tvalidate_slab(s, slab, obj_map);\n\t\tcount++;\n\t}\n\tif (count != node_nr_slabs(n)) {\n\t\tpr_err(\"SLUB: %s %ld slabs counted but counter=%ld\\n\",\n\t\t       s->name, count, node_nr_slabs(n));\n\t\tslab_add_kunit_errors();\n\t}\n\nout:\n\tspin_unlock_irqrestore(&n->list_lock, flags);\n\treturn count;\n}\n\nlong validate_slab_cache(struct kmem_cache *s)\n{\n\tint node;\n\tunsigned long count = 0;\n\tstruct kmem_cache_node *n;\n\tunsigned long *obj_map;\n\n\tobj_map = bitmap_alloc(oo_objects(s->oo), GFP_KERNEL);\n\tif (!obj_map)\n\t\treturn -ENOMEM;\n\n\tflush_all(s);\n\tfor_each_kmem_cache_node(s, node, n)\n\t\tcount += validate_slab_node(s, n, obj_map);\n\n\tbitmap_free(obj_map);\n\n\treturn count;\n}\nEXPORT_SYMBOL(validate_slab_cache);\n\n#ifdef CONFIG_DEBUG_FS\n \n\nstruct location {\n\tdepot_stack_handle_t handle;\n\tunsigned long count;\n\tunsigned long addr;\n\tunsigned long waste;\n\tlong long sum_time;\n\tlong min_time;\n\tlong max_time;\n\tlong min_pid;\n\tlong max_pid;\n\tDECLARE_BITMAP(cpus, NR_CPUS);\n\tnodemask_t nodes;\n};\n\nstruct loc_track {\n\tunsigned long max;\n\tunsigned long count;\n\tstruct location *loc;\n\tloff_t idx;\n};\n\nstatic struct dentry *slab_debugfs_root;\n\nstatic void free_loc_track(struct loc_track *t)\n{\n\tif (t->max)\n\t\tfree_pages((unsigned long)t->loc,\n\t\t\tget_order(sizeof(struct location) * t->max));\n}\n\nstatic int alloc_loc_track(struct loc_track *t, unsigned long max, gfp_t flags)\n{\n\tstruct location *l;\n\tint order;\n\n\torder = get_order(sizeof(struct location) * max);\n\n\tl = (void *)__get_free_pages(flags, order);\n\tif (!l)\n\t\treturn 0;\n\n\tif (t->count) {\n\t\tmemcpy(l, t->loc, sizeof(struct location) * t->count);\n\t\tfree_loc_track(t);\n\t}\n\tt->max = max;\n\tt->loc = l;\n\treturn 1;\n}\n\nstatic int add_location(struct loc_track *t, struct kmem_cache *s,\n\t\t\t\tconst struct track *track,\n\t\t\t\tunsigned int orig_size)\n{\n\tlong start, end, pos;\n\tstruct location *l;\n\tunsigned long caddr, chandle, cwaste;\n\tunsigned long age = jiffies - track->when;\n\tdepot_stack_handle_t handle = 0;\n\tunsigned int waste = s->object_size - orig_size;\n\n#ifdef CONFIG_STACKDEPOT\n\thandle = READ_ONCE(track->handle);\n#endif\n\tstart = -1;\n\tend = t->count;\n\n\tfor ( ; ; ) {\n\t\tpos = start + (end - start + 1) / 2;\n\n\t\t \n\t\tif (pos == end)\n\t\t\tbreak;\n\n\t\tl = &t->loc[pos];\n\t\tcaddr = l->addr;\n\t\tchandle = l->handle;\n\t\tcwaste = l->waste;\n\t\tif ((track->addr == caddr) && (handle == chandle) &&\n\t\t\t(waste == cwaste)) {\n\n\t\t\tl->count++;\n\t\t\tif (track->when) {\n\t\t\t\tl->sum_time += age;\n\t\t\t\tif (age < l->min_time)\n\t\t\t\t\tl->min_time = age;\n\t\t\t\tif (age > l->max_time)\n\t\t\t\t\tl->max_time = age;\n\n\t\t\t\tif (track->pid < l->min_pid)\n\t\t\t\t\tl->min_pid = track->pid;\n\t\t\t\tif (track->pid > l->max_pid)\n\t\t\t\t\tl->max_pid = track->pid;\n\n\t\t\t\tcpumask_set_cpu(track->cpu,\n\t\t\t\t\t\tto_cpumask(l->cpus));\n\t\t\t}\n\t\t\tnode_set(page_to_nid(virt_to_page(track)), l->nodes);\n\t\t\treturn 1;\n\t\t}\n\n\t\tif (track->addr < caddr)\n\t\t\tend = pos;\n\t\telse if (track->addr == caddr && handle < chandle)\n\t\t\tend = pos;\n\t\telse if (track->addr == caddr && handle == chandle &&\n\t\t\t\twaste < cwaste)\n\t\t\tend = pos;\n\t\telse\n\t\t\tstart = pos;\n\t}\n\n\t \n\tif (t->count >= t->max && !alloc_loc_track(t, 2 * t->max, GFP_ATOMIC))\n\t\treturn 0;\n\n\tl = t->loc + pos;\n\tif (pos < t->count)\n\t\tmemmove(l + 1, l,\n\t\t\t(t->count - pos) * sizeof(struct location));\n\tt->count++;\n\tl->count = 1;\n\tl->addr = track->addr;\n\tl->sum_time = age;\n\tl->min_time = age;\n\tl->max_time = age;\n\tl->min_pid = track->pid;\n\tl->max_pid = track->pid;\n\tl->handle = handle;\n\tl->waste = waste;\n\tcpumask_clear(to_cpumask(l->cpus));\n\tcpumask_set_cpu(track->cpu, to_cpumask(l->cpus));\n\tnodes_clear(l->nodes);\n\tnode_set(page_to_nid(virt_to_page(track)), l->nodes);\n\treturn 1;\n}\n\nstatic void process_slab(struct loc_track *t, struct kmem_cache *s,\n\t\tstruct slab *slab, enum track_item alloc,\n\t\tunsigned long *obj_map)\n{\n\tvoid *addr = slab_address(slab);\n\tbool is_alloc = (alloc == TRACK_ALLOC);\n\tvoid *p;\n\n\t__fill_map(obj_map, s, slab);\n\n\tfor_each_object(p, s, addr, slab->objects)\n\t\tif (!test_bit(__obj_to_index(s, addr, p), obj_map))\n\t\t\tadd_location(t, s, get_track(s, p, alloc),\n\t\t\t\t     is_alloc ? get_orig_size(s, p) :\n\t\t\t\t\t\ts->object_size);\n}\n#endif   \n#endif\t \n\n#ifdef SLAB_SUPPORTS_SYSFS\nenum slab_stat_type {\n\tSL_ALL,\t\t\t \n\tSL_PARTIAL,\t\t \n\tSL_CPU,\t\t\t \n\tSL_OBJECTS,\t\t \n\tSL_TOTAL\t\t \n};\n\n#define SO_ALL\t\t(1 << SL_ALL)\n#define SO_PARTIAL\t(1 << SL_PARTIAL)\n#define SO_CPU\t\t(1 << SL_CPU)\n#define SO_OBJECTS\t(1 << SL_OBJECTS)\n#define SO_TOTAL\t(1 << SL_TOTAL)\n\nstatic ssize_t show_slab_objects(struct kmem_cache *s,\n\t\t\t\t char *buf, unsigned long flags)\n{\n\tunsigned long total = 0;\n\tint node;\n\tint x;\n\tunsigned long *nodes;\n\tint len = 0;\n\n\tnodes = kcalloc(nr_node_ids, sizeof(unsigned long), GFP_KERNEL);\n\tif (!nodes)\n\t\treturn -ENOMEM;\n\n\tif (flags & SO_CPU) {\n\t\tint cpu;\n\n\t\tfor_each_possible_cpu(cpu) {\n\t\t\tstruct kmem_cache_cpu *c = per_cpu_ptr(s->cpu_slab,\n\t\t\t\t\t\t\t       cpu);\n\t\t\tint node;\n\t\t\tstruct slab *slab;\n\n\t\t\tslab = READ_ONCE(c->slab);\n\t\t\tif (!slab)\n\t\t\t\tcontinue;\n\n\t\t\tnode = slab_nid(slab);\n\t\t\tif (flags & SO_TOTAL)\n\t\t\t\tx = slab->objects;\n\t\t\telse if (flags & SO_OBJECTS)\n\t\t\t\tx = slab->inuse;\n\t\t\telse\n\t\t\t\tx = 1;\n\n\t\t\ttotal += x;\n\t\t\tnodes[node] += x;\n\n#ifdef CONFIG_SLUB_CPU_PARTIAL\n\t\t\tslab = slub_percpu_partial_read_once(c);\n\t\t\tif (slab) {\n\t\t\t\tnode = slab_nid(slab);\n\t\t\t\tif (flags & SO_TOTAL)\n\t\t\t\t\tWARN_ON_ONCE(1);\n\t\t\t\telse if (flags & SO_OBJECTS)\n\t\t\t\t\tWARN_ON_ONCE(1);\n\t\t\t\telse\n\t\t\t\t\tx = slab->slabs;\n\t\t\t\ttotal += x;\n\t\t\t\tnodes[node] += x;\n\t\t\t}\n#endif\n\t\t}\n\t}\n\n\t \n\n#ifdef CONFIG_SLUB_DEBUG\n\tif (flags & SO_ALL) {\n\t\tstruct kmem_cache_node *n;\n\n\t\tfor_each_kmem_cache_node(s, node, n) {\n\n\t\t\tif (flags & SO_TOTAL)\n\t\t\t\tx = node_nr_objs(n);\n\t\t\telse if (flags & SO_OBJECTS)\n\t\t\t\tx = node_nr_objs(n) - count_partial(n, count_free);\n\t\t\telse\n\t\t\t\tx = node_nr_slabs(n);\n\t\t\ttotal += x;\n\t\t\tnodes[node] += x;\n\t\t}\n\n\t} else\n#endif\n\tif (flags & SO_PARTIAL) {\n\t\tstruct kmem_cache_node *n;\n\n\t\tfor_each_kmem_cache_node(s, node, n) {\n\t\t\tif (flags & SO_TOTAL)\n\t\t\t\tx = count_partial(n, count_total);\n\t\t\telse if (flags & SO_OBJECTS)\n\t\t\t\tx = count_partial(n, count_inuse);\n\t\t\telse\n\t\t\t\tx = n->nr_partial;\n\t\t\ttotal += x;\n\t\t\tnodes[node] += x;\n\t\t}\n\t}\n\n\tlen += sysfs_emit_at(buf, len, \"%lu\", total);\n#ifdef CONFIG_NUMA\n\tfor (node = 0; node < nr_node_ids; node++) {\n\t\tif (nodes[node])\n\t\t\tlen += sysfs_emit_at(buf, len, \" N%d=%lu\",\n\t\t\t\t\t     node, nodes[node]);\n\t}\n#endif\n\tlen += sysfs_emit_at(buf, len, \"\\n\");\n\tkfree(nodes);\n\n\treturn len;\n}\n\n#define to_slab_attr(n) container_of(n, struct slab_attribute, attr)\n#define to_slab(n) container_of(n, struct kmem_cache, kobj)\n\nstruct slab_attribute {\n\tstruct attribute attr;\n\tssize_t (*show)(struct kmem_cache *s, char *buf);\n\tssize_t (*store)(struct kmem_cache *s, const char *x, size_t count);\n};\n\n#define SLAB_ATTR_RO(_name) \\\n\tstatic struct slab_attribute _name##_attr = __ATTR_RO_MODE(_name, 0400)\n\n#define SLAB_ATTR(_name) \\\n\tstatic struct slab_attribute _name##_attr = __ATTR_RW_MODE(_name, 0600)\n\nstatic ssize_t slab_size_show(struct kmem_cache *s, char *buf)\n{\n\treturn sysfs_emit(buf, \"%u\\n\", s->size);\n}\nSLAB_ATTR_RO(slab_size);\n\nstatic ssize_t align_show(struct kmem_cache *s, char *buf)\n{\n\treturn sysfs_emit(buf, \"%u\\n\", s->align);\n}\nSLAB_ATTR_RO(align);\n\nstatic ssize_t object_size_show(struct kmem_cache *s, char *buf)\n{\n\treturn sysfs_emit(buf, \"%u\\n\", s->object_size);\n}\nSLAB_ATTR_RO(object_size);\n\nstatic ssize_t objs_per_slab_show(struct kmem_cache *s, char *buf)\n{\n\treturn sysfs_emit(buf, \"%u\\n\", oo_objects(s->oo));\n}\nSLAB_ATTR_RO(objs_per_slab);\n\nstatic ssize_t order_show(struct kmem_cache *s, char *buf)\n{\n\treturn sysfs_emit(buf, \"%u\\n\", oo_order(s->oo));\n}\nSLAB_ATTR_RO(order);\n\nstatic ssize_t min_partial_show(struct kmem_cache *s, char *buf)\n{\n\treturn sysfs_emit(buf, \"%lu\\n\", s->min_partial);\n}\n\nstatic ssize_t min_partial_store(struct kmem_cache *s, const char *buf,\n\t\t\t\t size_t length)\n{\n\tunsigned long min;\n\tint err;\n\n\terr = kstrtoul(buf, 10, &min);\n\tif (err)\n\t\treturn err;\n\n\ts->min_partial = min;\n\treturn length;\n}\nSLAB_ATTR(min_partial);\n\nstatic ssize_t cpu_partial_show(struct kmem_cache *s, char *buf)\n{\n\tunsigned int nr_partial = 0;\n#ifdef CONFIG_SLUB_CPU_PARTIAL\n\tnr_partial = s->cpu_partial;\n#endif\n\n\treturn sysfs_emit(buf, \"%u\\n\", nr_partial);\n}\n\nstatic ssize_t cpu_partial_store(struct kmem_cache *s, const char *buf,\n\t\t\t\t size_t length)\n{\n\tunsigned int objects;\n\tint err;\n\n\terr = kstrtouint(buf, 10, &objects);\n\tif (err)\n\t\treturn err;\n\tif (objects && !kmem_cache_has_cpu_partial(s))\n\t\treturn -EINVAL;\n\n\tslub_set_cpu_partial(s, objects);\n\tflush_all(s);\n\treturn length;\n}\nSLAB_ATTR(cpu_partial);\n\nstatic ssize_t ctor_show(struct kmem_cache *s, char *buf)\n{\n\tif (!s->ctor)\n\t\treturn 0;\n\treturn sysfs_emit(buf, \"%pS\\n\", s->ctor);\n}\nSLAB_ATTR_RO(ctor);\n\nstatic ssize_t aliases_show(struct kmem_cache *s, char *buf)\n{\n\treturn sysfs_emit(buf, \"%d\\n\", s->refcount < 0 ? 0 : s->refcount - 1);\n}\nSLAB_ATTR_RO(aliases);\n\nstatic ssize_t partial_show(struct kmem_cache *s, char *buf)\n{\n\treturn show_slab_objects(s, buf, SO_PARTIAL);\n}\nSLAB_ATTR_RO(partial);\n\nstatic ssize_t cpu_slabs_show(struct kmem_cache *s, char *buf)\n{\n\treturn show_slab_objects(s, buf, SO_CPU);\n}\nSLAB_ATTR_RO(cpu_slabs);\n\nstatic ssize_t objects_partial_show(struct kmem_cache *s, char *buf)\n{\n\treturn show_slab_objects(s, buf, SO_PARTIAL|SO_OBJECTS);\n}\nSLAB_ATTR_RO(objects_partial);\n\nstatic ssize_t slabs_cpu_partial_show(struct kmem_cache *s, char *buf)\n{\n\tint objects = 0;\n\tint slabs = 0;\n\tint cpu __maybe_unused;\n\tint len = 0;\n\n#ifdef CONFIG_SLUB_CPU_PARTIAL\n\tfor_each_online_cpu(cpu) {\n\t\tstruct slab *slab;\n\n\t\tslab = slub_percpu_partial(per_cpu_ptr(s->cpu_slab, cpu));\n\n\t\tif (slab)\n\t\t\tslabs += slab->slabs;\n\t}\n#endif\n\n\t \n\tobjects = (slabs * oo_objects(s->oo)) / 2;\n\tlen += sysfs_emit_at(buf, len, \"%d(%d)\", objects, slabs);\n\n#ifdef CONFIG_SLUB_CPU_PARTIAL\n\tfor_each_online_cpu(cpu) {\n\t\tstruct slab *slab;\n\n\t\tslab = slub_percpu_partial(per_cpu_ptr(s->cpu_slab, cpu));\n\t\tif (slab) {\n\t\t\tslabs = READ_ONCE(slab->slabs);\n\t\t\tobjects = (slabs * oo_objects(s->oo)) / 2;\n\t\t\tlen += sysfs_emit_at(buf, len, \" C%d=%d(%d)\",\n\t\t\t\t\t     cpu, objects, slabs);\n\t\t}\n\t}\n#endif\n\tlen += sysfs_emit_at(buf, len, \"\\n\");\n\n\treturn len;\n}\nSLAB_ATTR_RO(slabs_cpu_partial);\n\nstatic ssize_t reclaim_account_show(struct kmem_cache *s, char *buf)\n{\n\treturn sysfs_emit(buf, \"%d\\n\", !!(s->flags & SLAB_RECLAIM_ACCOUNT));\n}\nSLAB_ATTR_RO(reclaim_account);\n\nstatic ssize_t hwcache_align_show(struct kmem_cache *s, char *buf)\n{\n\treturn sysfs_emit(buf, \"%d\\n\", !!(s->flags & SLAB_HWCACHE_ALIGN));\n}\nSLAB_ATTR_RO(hwcache_align);\n\n#ifdef CONFIG_ZONE_DMA\nstatic ssize_t cache_dma_show(struct kmem_cache *s, char *buf)\n{\n\treturn sysfs_emit(buf, \"%d\\n\", !!(s->flags & SLAB_CACHE_DMA));\n}\nSLAB_ATTR_RO(cache_dma);\n#endif\n\n#ifdef CONFIG_HARDENED_USERCOPY\nstatic ssize_t usersize_show(struct kmem_cache *s, char *buf)\n{\n\treturn sysfs_emit(buf, \"%u\\n\", s->usersize);\n}\nSLAB_ATTR_RO(usersize);\n#endif\n\nstatic ssize_t destroy_by_rcu_show(struct kmem_cache *s, char *buf)\n{\n\treturn sysfs_emit(buf, \"%d\\n\", !!(s->flags & SLAB_TYPESAFE_BY_RCU));\n}\nSLAB_ATTR_RO(destroy_by_rcu);\n\n#ifdef CONFIG_SLUB_DEBUG\nstatic ssize_t slabs_show(struct kmem_cache *s, char *buf)\n{\n\treturn show_slab_objects(s, buf, SO_ALL);\n}\nSLAB_ATTR_RO(slabs);\n\nstatic ssize_t total_objects_show(struct kmem_cache *s, char *buf)\n{\n\treturn show_slab_objects(s, buf, SO_ALL|SO_TOTAL);\n}\nSLAB_ATTR_RO(total_objects);\n\nstatic ssize_t objects_show(struct kmem_cache *s, char *buf)\n{\n\treturn show_slab_objects(s, buf, SO_ALL|SO_OBJECTS);\n}\nSLAB_ATTR_RO(objects);\n\nstatic ssize_t sanity_checks_show(struct kmem_cache *s, char *buf)\n{\n\treturn sysfs_emit(buf, \"%d\\n\", !!(s->flags & SLAB_CONSISTENCY_CHECKS));\n}\nSLAB_ATTR_RO(sanity_checks);\n\nstatic ssize_t trace_show(struct kmem_cache *s, char *buf)\n{\n\treturn sysfs_emit(buf, \"%d\\n\", !!(s->flags & SLAB_TRACE));\n}\nSLAB_ATTR_RO(trace);\n\nstatic ssize_t red_zone_show(struct kmem_cache *s, char *buf)\n{\n\treturn sysfs_emit(buf, \"%d\\n\", !!(s->flags & SLAB_RED_ZONE));\n}\n\nSLAB_ATTR_RO(red_zone);\n\nstatic ssize_t poison_show(struct kmem_cache *s, char *buf)\n{\n\treturn sysfs_emit(buf, \"%d\\n\", !!(s->flags & SLAB_POISON));\n}\n\nSLAB_ATTR_RO(poison);\n\nstatic ssize_t store_user_show(struct kmem_cache *s, char *buf)\n{\n\treturn sysfs_emit(buf, \"%d\\n\", !!(s->flags & SLAB_STORE_USER));\n}\n\nSLAB_ATTR_RO(store_user);\n\nstatic ssize_t validate_show(struct kmem_cache *s, char *buf)\n{\n\treturn 0;\n}\n\nstatic ssize_t validate_store(struct kmem_cache *s,\n\t\t\tconst char *buf, size_t length)\n{\n\tint ret = -EINVAL;\n\n\tif (buf[0] == '1' && kmem_cache_debug(s)) {\n\t\tret = validate_slab_cache(s);\n\t\tif (ret >= 0)\n\t\t\tret = length;\n\t}\n\treturn ret;\n}\nSLAB_ATTR(validate);\n\n#endif  \n\n#ifdef CONFIG_FAILSLAB\nstatic ssize_t failslab_show(struct kmem_cache *s, char *buf)\n{\n\treturn sysfs_emit(buf, \"%d\\n\", !!(s->flags & SLAB_FAILSLAB));\n}\n\nstatic ssize_t failslab_store(struct kmem_cache *s, const char *buf,\n\t\t\t\tsize_t length)\n{\n\tif (s->refcount > 1)\n\t\treturn -EINVAL;\n\n\tif (buf[0] == '1')\n\t\tWRITE_ONCE(s->flags, s->flags | SLAB_FAILSLAB);\n\telse\n\t\tWRITE_ONCE(s->flags, s->flags & ~SLAB_FAILSLAB);\n\n\treturn length;\n}\nSLAB_ATTR(failslab);\n#endif\n\nstatic ssize_t shrink_show(struct kmem_cache *s, char *buf)\n{\n\treturn 0;\n}\n\nstatic ssize_t shrink_store(struct kmem_cache *s,\n\t\t\tconst char *buf, size_t length)\n{\n\tif (buf[0] == '1')\n\t\tkmem_cache_shrink(s);\n\telse\n\t\treturn -EINVAL;\n\treturn length;\n}\nSLAB_ATTR(shrink);\n\n#ifdef CONFIG_NUMA\nstatic ssize_t remote_node_defrag_ratio_show(struct kmem_cache *s, char *buf)\n{\n\treturn sysfs_emit(buf, \"%u\\n\", s->remote_node_defrag_ratio / 10);\n}\n\nstatic ssize_t remote_node_defrag_ratio_store(struct kmem_cache *s,\n\t\t\t\tconst char *buf, size_t length)\n{\n\tunsigned int ratio;\n\tint err;\n\n\terr = kstrtouint(buf, 10, &ratio);\n\tif (err)\n\t\treturn err;\n\tif (ratio > 100)\n\t\treturn -ERANGE;\n\n\ts->remote_node_defrag_ratio = ratio * 10;\n\n\treturn length;\n}\nSLAB_ATTR(remote_node_defrag_ratio);\n#endif\n\n#ifdef CONFIG_SLUB_STATS\nstatic int show_stat(struct kmem_cache *s, char *buf, enum stat_item si)\n{\n\tunsigned long sum  = 0;\n\tint cpu;\n\tint len = 0;\n\tint *data = kmalloc_array(nr_cpu_ids, sizeof(int), GFP_KERNEL);\n\n\tif (!data)\n\t\treturn -ENOMEM;\n\n\tfor_each_online_cpu(cpu) {\n\t\tunsigned x = per_cpu_ptr(s->cpu_slab, cpu)->stat[si];\n\n\t\tdata[cpu] = x;\n\t\tsum += x;\n\t}\n\n\tlen += sysfs_emit_at(buf, len, \"%lu\", sum);\n\n#ifdef CONFIG_SMP\n\tfor_each_online_cpu(cpu) {\n\t\tif (data[cpu])\n\t\t\tlen += sysfs_emit_at(buf, len, \" C%d=%u\",\n\t\t\t\t\t     cpu, data[cpu]);\n\t}\n#endif\n\tkfree(data);\n\tlen += sysfs_emit_at(buf, len, \"\\n\");\n\n\treturn len;\n}\n\nstatic void clear_stat(struct kmem_cache *s, enum stat_item si)\n{\n\tint cpu;\n\n\tfor_each_online_cpu(cpu)\n\t\tper_cpu_ptr(s->cpu_slab, cpu)->stat[si] = 0;\n}\n\n#define STAT_ATTR(si, text) \t\t\t\t\t\\\nstatic ssize_t text##_show(struct kmem_cache *s, char *buf)\t\\\n{\t\t\t\t\t\t\t\t\\\n\treturn show_stat(s, buf, si);\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\\\nstatic ssize_t text##_store(struct kmem_cache *s,\t\t\\\n\t\t\t\tconst char *buf, size_t length)\t\\\n{\t\t\t\t\t\t\t\t\\\n\tif (buf[0] != '0')\t\t\t\t\t\\\n\t\treturn -EINVAL;\t\t\t\t\t\\\n\tclear_stat(s, si);\t\t\t\t\t\\\n\treturn length;\t\t\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\\\nSLAB_ATTR(text);\t\t\t\t\t\t\\\n\nSTAT_ATTR(ALLOC_FASTPATH, alloc_fastpath);\nSTAT_ATTR(ALLOC_SLOWPATH, alloc_slowpath);\nSTAT_ATTR(FREE_FASTPATH, free_fastpath);\nSTAT_ATTR(FREE_SLOWPATH, free_slowpath);\nSTAT_ATTR(FREE_FROZEN, free_frozen);\nSTAT_ATTR(FREE_ADD_PARTIAL, free_add_partial);\nSTAT_ATTR(FREE_REMOVE_PARTIAL, free_remove_partial);\nSTAT_ATTR(ALLOC_FROM_PARTIAL, alloc_from_partial);\nSTAT_ATTR(ALLOC_SLAB, alloc_slab);\nSTAT_ATTR(ALLOC_REFILL, alloc_refill);\nSTAT_ATTR(ALLOC_NODE_MISMATCH, alloc_node_mismatch);\nSTAT_ATTR(FREE_SLAB, free_slab);\nSTAT_ATTR(CPUSLAB_FLUSH, cpuslab_flush);\nSTAT_ATTR(DEACTIVATE_FULL, deactivate_full);\nSTAT_ATTR(DEACTIVATE_EMPTY, deactivate_empty);\nSTAT_ATTR(DEACTIVATE_TO_HEAD, deactivate_to_head);\nSTAT_ATTR(DEACTIVATE_TO_TAIL, deactivate_to_tail);\nSTAT_ATTR(DEACTIVATE_REMOTE_FREES, deactivate_remote_frees);\nSTAT_ATTR(DEACTIVATE_BYPASS, deactivate_bypass);\nSTAT_ATTR(ORDER_FALLBACK, order_fallback);\nSTAT_ATTR(CMPXCHG_DOUBLE_CPU_FAIL, cmpxchg_double_cpu_fail);\nSTAT_ATTR(CMPXCHG_DOUBLE_FAIL, cmpxchg_double_fail);\nSTAT_ATTR(CPU_PARTIAL_ALLOC, cpu_partial_alloc);\nSTAT_ATTR(CPU_PARTIAL_FREE, cpu_partial_free);\nSTAT_ATTR(CPU_PARTIAL_NODE, cpu_partial_node);\nSTAT_ATTR(CPU_PARTIAL_DRAIN, cpu_partial_drain);\n#endif\t \n\n#ifdef CONFIG_KFENCE\nstatic ssize_t skip_kfence_show(struct kmem_cache *s, char *buf)\n{\n\treturn sysfs_emit(buf, \"%d\\n\", !!(s->flags & SLAB_SKIP_KFENCE));\n}\n\nstatic ssize_t skip_kfence_store(struct kmem_cache *s,\n\t\t\tconst char *buf, size_t length)\n{\n\tint ret = length;\n\n\tif (buf[0] == '0')\n\t\ts->flags &= ~SLAB_SKIP_KFENCE;\n\telse if (buf[0] == '1')\n\t\ts->flags |= SLAB_SKIP_KFENCE;\n\telse\n\t\tret = -EINVAL;\n\n\treturn ret;\n}\nSLAB_ATTR(skip_kfence);\n#endif\n\nstatic struct attribute *slab_attrs[] = {\n\t&slab_size_attr.attr,\n\t&object_size_attr.attr,\n\t&objs_per_slab_attr.attr,\n\t&order_attr.attr,\n\t&min_partial_attr.attr,\n\t&cpu_partial_attr.attr,\n\t&objects_partial_attr.attr,\n\t&partial_attr.attr,\n\t&cpu_slabs_attr.attr,\n\t&ctor_attr.attr,\n\t&aliases_attr.attr,\n\t&align_attr.attr,\n\t&hwcache_align_attr.attr,\n\t&reclaim_account_attr.attr,\n\t&destroy_by_rcu_attr.attr,\n\t&shrink_attr.attr,\n\t&slabs_cpu_partial_attr.attr,\n#ifdef CONFIG_SLUB_DEBUG\n\t&total_objects_attr.attr,\n\t&objects_attr.attr,\n\t&slabs_attr.attr,\n\t&sanity_checks_attr.attr,\n\t&trace_attr.attr,\n\t&red_zone_attr.attr,\n\t&poison_attr.attr,\n\t&store_user_attr.attr,\n\t&validate_attr.attr,\n#endif\n#ifdef CONFIG_ZONE_DMA\n\t&cache_dma_attr.attr,\n#endif\n#ifdef CONFIG_NUMA\n\t&remote_node_defrag_ratio_attr.attr,\n#endif\n#ifdef CONFIG_SLUB_STATS\n\t&alloc_fastpath_attr.attr,\n\t&alloc_slowpath_attr.attr,\n\t&free_fastpath_attr.attr,\n\t&free_slowpath_attr.attr,\n\t&free_frozen_attr.attr,\n\t&free_add_partial_attr.attr,\n\t&free_remove_partial_attr.attr,\n\t&alloc_from_partial_attr.attr,\n\t&alloc_slab_attr.attr,\n\t&alloc_refill_attr.attr,\n\t&alloc_node_mismatch_attr.attr,\n\t&free_slab_attr.attr,\n\t&cpuslab_flush_attr.attr,\n\t&deactivate_full_attr.attr,\n\t&deactivate_empty_attr.attr,\n\t&deactivate_to_head_attr.attr,\n\t&deactivate_to_tail_attr.attr,\n\t&deactivate_remote_frees_attr.attr,\n\t&deactivate_bypass_attr.attr,\n\t&order_fallback_attr.attr,\n\t&cmpxchg_double_fail_attr.attr,\n\t&cmpxchg_double_cpu_fail_attr.attr,\n\t&cpu_partial_alloc_attr.attr,\n\t&cpu_partial_free_attr.attr,\n\t&cpu_partial_node_attr.attr,\n\t&cpu_partial_drain_attr.attr,\n#endif\n#ifdef CONFIG_FAILSLAB\n\t&failslab_attr.attr,\n#endif\n#ifdef CONFIG_HARDENED_USERCOPY\n\t&usersize_attr.attr,\n#endif\n#ifdef CONFIG_KFENCE\n\t&skip_kfence_attr.attr,\n#endif\n\n\tNULL\n};\n\nstatic const struct attribute_group slab_attr_group = {\n\t.attrs = slab_attrs,\n};\n\nstatic ssize_t slab_attr_show(struct kobject *kobj,\n\t\t\t\tstruct attribute *attr,\n\t\t\t\tchar *buf)\n{\n\tstruct slab_attribute *attribute;\n\tstruct kmem_cache *s;\n\n\tattribute = to_slab_attr(attr);\n\ts = to_slab(kobj);\n\n\tif (!attribute->show)\n\t\treturn -EIO;\n\n\treturn attribute->show(s, buf);\n}\n\nstatic ssize_t slab_attr_store(struct kobject *kobj,\n\t\t\t\tstruct attribute *attr,\n\t\t\t\tconst char *buf, size_t len)\n{\n\tstruct slab_attribute *attribute;\n\tstruct kmem_cache *s;\n\n\tattribute = to_slab_attr(attr);\n\ts = to_slab(kobj);\n\n\tif (!attribute->store)\n\t\treturn -EIO;\n\n\treturn attribute->store(s, buf, len);\n}\n\nstatic void kmem_cache_release(struct kobject *k)\n{\n\tslab_kmem_cache_release(to_slab(k));\n}\n\nstatic const struct sysfs_ops slab_sysfs_ops = {\n\t.show = slab_attr_show,\n\t.store = slab_attr_store,\n};\n\nstatic const struct kobj_type slab_ktype = {\n\t.sysfs_ops = &slab_sysfs_ops,\n\t.release = kmem_cache_release,\n};\n\nstatic struct kset *slab_kset;\n\nstatic inline struct kset *cache_kset(struct kmem_cache *s)\n{\n\treturn slab_kset;\n}\n\n#define ID_STR_LENGTH 32\n\n \nstatic char *create_unique_id(struct kmem_cache *s)\n{\n\tchar *name = kmalloc(ID_STR_LENGTH, GFP_KERNEL);\n\tchar *p = name;\n\n\tif (!name)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\t*p++ = ':';\n\t \n\tif (s->flags & SLAB_CACHE_DMA)\n\t\t*p++ = 'd';\n\tif (s->flags & SLAB_CACHE_DMA32)\n\t\t*p++ = 'D';\n\tif (s->flags & SLAB_RECLAIM_ACCOUNT)\n\t\t*p++ = 'a';\n\tif (s->flags & SLAB_CONSISTENCY_CHECKS)\n\t\t*p++ = 'F';\n\tif (s->flags & SLAB_ACCOUNT)\n\t\t*p++ = 'A';\n\tif (p != name + 1)\n\t\t*p++ = '-';\n\tp += snprintf(p, ID_STR_LENGTH - (p - name), \"%07u\", s->size);\n\n\tif (WARN_ON(p > name + ID_STR_LENGTH - 1)) {\n\t\tkfree(name);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\tkmsan_unpoison_memory(name, p - name);\n\treturn name;\n}\n\nstatic int sysfs_slab_add(struct kmem_cache *s)\n{\n\tint err;\n\tconst char *name;\n\tstruct kset *kset = cache_kset(s);\n\tint unmergeable = slab_unmergeable(s);\n\n\tif (!unmergeable && disable_higher_order_debug &&\n\t\t\t(slub_debug & DEBUG_METADATA_FLAGS))\n\t\tunmergeable = 1;\n\n\tif (unmergeable) {\n\t\t \n\t\tsysfs_remove_link(&slab_kset->kobj, s->name);\n\t\tname = s->name;\n\t} else {\n\t\t \n\t\tname = create_unique_id(s);\n\t\tif (IS_ERR(name))\n\t\t\treturn PTR_ERR(name);\n\t}\n\n\ts->kobj.kset = kset;\n\terr = kobject_init_and_add(&s->kobj, &slab_ktype, NULL, \"%s\", name);\n\tif (err)\n\t\tgoto out;\n\n\terr = sysfs_create_group(&s->kobj, &slab_attr_group);\n\tif (err)\n\t\tgoto out_del_kobj;\n\n\tif (!unmergeable) {\n\t\t \n\t\tsysfs_slab_alias(s, s->name);\n\t}\nout:\n\tif (!unmergeable)\n\t\tkfree(name);\n\treturn err;\nout_del_kobj:\n\tkobject_del(&s->kobj);\n\tgoto out;\n}\n\nvoid sysfs_slab_unlink(struct kmem_cache *s)\n{\n\tif (slab_state >= FULL)\n\t\tkobject_del(&s->kobj);\n}\n\nvoid sysfs_slab_release(struct kmem_cache *s)\n{\n\tif (slab_state >= FULL)\n\t\tkobject_put(&s->kobj);\n}\n\n \nstruct saved_alias {\n\tstruct kmem_cache *s;\n\tconst char *name;\n\tstruct saved_alias *next;\n};\n\nstatic struct saved_alias *alias_list;\n\nstatic int sysfs_slab_alias(struct kmem_cache *s, const char *name)\n{\n\tstruct saved_alias *al;\n\n\tif (slab_state == FULL) {\n\t\t \n\t\tsysfs_remove_link(&slab_kset->kobj, name);\n\t\treturn sysfs_create_link(&slab_kset->kobj, &s->kobj, name);\n\t}\n\n\tal = kmalloc(sizeof(struct saved_alias), GFP_KERNEL);\n\tif (!al)\n\t\treturn -ENOMEM;\n\n\tal->s = s;\n\tal->name = name;\n\tal->next = alias_list;\n\talias_list = al;\n\tkmsan_unpoison_memory(al, sizeof(*al));\n\treturn 0;\n}\n\nstatic int __init slab_sysfs_init(void)\n{\n\tstruct kmem_cache *s;\n\tint err;\n\n\tmutex_lock(&slab_mutex);\n\n\tslab_kset = kset_create_and_add(\"slab\", NULL, kernel_kobj);\n\tif (!slab_kset) {\n\t\tmutex_unlock(&slab_mutex);\n\t\tpr_err(\"Cannot register slab subsystem.\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tslab_state = FULL;\n\n\tlist_for_each_entry(s, &slab_caches, list) {\n\t\terr = sysfs_slab_add(s);\n\t\tif (err)\n\t\t\tpr_err(\"SLUB: Unable to add boot slab %s to sysfs\\n\",\n\t\t\t       s->name);\n\t}\n\n\twhile (alias_list) {\n\t\tstruct saved_alias *al = alias_list;\n\n\t\talias_list = alias_list->next;\n\t\terr = sysfs_slab_alias(al->s, al->name);\n\t\tif (err)\n\t\t\tpr_err(\"SLUB: Unable to add boot slab alias %s to sysfs\\n\",\n\t\t\t       al->name);\n\t\tkfree(al);\n\t}\n\n\tmutex_unlock(&slab_mutex);\n\treturn 0;\n}\nlate_initcall(slab_sysfs_init);\n#endif  \n\n#if defined(CONFIG_SLUB_DEBUG) && defined(CONFIG_DEBUG_FS)\nstatic int slab_debugfs_show(struct seq_file *seq, void *v)\n{\n\tstruct loc_track *t = seq->private;\n\tstruct location *l;\n\tunsigned long idx;\n\n\tidx = (unsigned long) t->idx;\n\tif (idx < t->count) {\n\t\tl = &t->loc[idx];\n\n\t\tseq_printf(seq, \"%7ld \", l->count);\n\n\t\tif (l->addr)\n\t\t\tseq_printf(seq, \"%pS\", (void *)l->addr);\n\t\telse\n\t\t\tseq_puts(seq, \"<not-available>\");\n\n\t\tif (l->waste)\n\t\t\tseq_printf(seq, \" waste=%lu/%lu\",\n\t\t\t\tl->count * l->waste, l->waste);\n\n\t\tif (l->sum_time != l->min_time) {\n\t\t\tseq_printf(seq, \" age=%ld/%llu/%ld\",\n\t\t\t\tl->min_time, div_u64(l->sum_time, l->count),\n\t\t\t\tl->max_time);\n\t\t} else\n\t\t\tseq_printf(seq, \" age=%ld\", l->min_time);\n\n\t\tif (l->min_pid != l->max_pid)\n\t\t\tseq_printf(seq, \" pid=%ld-%ld\", l->min_pid, l->max_pid);\n\t\telse\n\t\t\tseq_printf(seq, \" pid=%ld\",\n\t\t\t\tl->min_pid);\n\n\t\tif (num_online_cpus() > 1 && !cpumask_empty(to_cpumask(l->cpus)))\n\t\t\tseq_printf(seq, \" cpus=%*pbl\",\n\t\t\t\t cpumask_pr_args(to_cpumask(l->cpus)));\n\n\t\tif (nr_online_nodes > 1 && !nodes_empty(l->nodes))\n\t\t\tseq_printf(seq, \" nodes=%*pbl\",\n\t\t\t\t nodemask_pr_args(&l->nodes));\n\n#ifdef CONFIG_STACKDEPOT\n\t\t{\n\t\t\tdepot_stack_handle_t handle;\n\t\t\tunsigned long *entries;\n\t\t\tunsigned int nr_entries, j;\n\n\t\t\thandle = READ_ONCE(l->handle);\n\t\t\tif (handle) {\n\t\t\t\tnr_entries = stack_depot_fetch(handle, &entries);\n\t\t\t\tseq_puts(seq, \"\\n\");\n\t\t\t\tfor (j = 0; j < nr_entries; j++)\n\t\t\t\t\tseq_printf(seq, \"        %pS\\n\", (void *)entries[j]);\n\t\t\t}\n\t\t}\n#endif\n\t\tseq_puts(seq, \"\\n\");\n\t}\n\n\tif (!idx && !t->count)\n\t\tseq_puts(seq, \"No data\\n\");\n\n\treturn 0;\n}\n\nstatic void slab_debugfs_stop(struct seq_file *seq, void *v)\n{\n}\n\nstatic void *slab_debugfs_next(struct seq_file *seq, void *v, loff_t *ppos)\n{\n\tstruct loc_track *t = seq->private;\n\n\tt->idx = ++(*ppos);\n\tif (*ppos <= t->count)\n\t\treturn ppos;\n\n\treturn NULL;\n}\n\nstatic int cmp_loc_by_count(const void *a, const void *b, const void *data)\n{\n\tstruct location *loc1 = (struct location *)a;\n\tstruct location *loc2 = (struct location *)b;\n\n\tif (loc1->count > loc2->count)\n\t\treturn -1;\n\telse\n\t\treturn 1;\n}\n\nstatic void *slab_debugfs_start(struct seq_file *seq, loff_t *ppos)\n{\n\tstruct loc_track *t = seq->private;\n\n\tt->idx = *ppos;\n\treturn ppos;\n}\n\nstatic const struct seq_operations slab_debugfs_sops = {\n\t.start  = slab_debugfs_start,\n\t.next   = slab_debugfs_next,\n\t.stop   = slab_debugfs_stop,\n\t.show   = slab_debugfs_show,\n};\n\nstatic int slab_debug_trace_open(struct inode *inode, struct file *filep)\n{\n\n\tstruct kmem_cache_node *n;\n\tenum track_item alloc;\n\tint node;\n\tstruct loc_track *t = __seq_open_private(filep, &slab_debugfs_sops,\n\t\t\t\t\t\tsizeof(struct loc_track));\n\tstruct kmem_cache *s = file_inode(filep)->i_private;\n\tunsigned long *obj_map;\n\n\tif (!t)\n\t\treturn -ENOMEM;\n\n\tobj_map = bitmap_alloc(oo_objects(s->oo), GFP_KERNEL);\n\tif (!obj_map) {\n\t\tseq_release_private(inode, filep);\n\t\treturn -ENOMEM;\n\t}\n\n\tif (strcmp(filep->f_path.dentry->d_name.name, \"alloc_traces\") == 0)\n\t\talloc = TRACK_ALLOC;\n\telse\n\t\talloc = TRACK_FREE;\n\n\tif (!alloc_loc_track(t, PAGE_SIZE / sizeof(struct location), GFP_KERNEL)) {\n\t\tbitmap_free(obj_map);\n\t\tseq_release_private(inode, filep);\n\t\treturn -ENOMEM;\n\t}\n\n\tfor_each_kmem_cache_node(s, node, n) {\n\t\tunsigned long flags;\n\t\tstruct slab *slab;\n\n\t\tif (!node_nr_slabs(n))\n\t\t\tcontinue;\n\n\t\tspin_lock_irqsave(&n->list_lock, flags);\n\t\tlist_for_each_entry(slab, &n->partial, slab_list)\n\t\t\tprocess_slab(t, s, slab, alloc, obj_map);\n\t\tlist_for_each_entry(slab, &n->full, slab_list)\n\t\t\tprocess_slab(t, s, slab, alloc, obj_map);\n\t\tspin_unlock_irqrestore(&n->list_lock, flags);\n\t}\n\n\t \n\tsort_r(t->loc, t->count, sizeof(struct location),\n\t\tcmp_loc_by_count, NULL, NULL);\n\n\tbitmap_free(obj_map);\n\treturn 0;\n}\n\nstatic int slab_debug_trace_release(struct inode *inode, struct file *file)\n{\n\tstruct seq_file *seq = file->private_data;\n\tstruct loc_track *t = seq->private;\n\n\tfree_loc_track(t);\n\treturn seq_release_private(inode, file);\n}\n\nstatic const struct file_operations slab_debugfs_fops = {\n\t.open    = slab_debug_trace_open,\n\t.read    = seq_read,\n\t.llseek  = seq_lseek,\n\t.release = slab_debug_trace_release,\n};\n\nstatic void debugfs_slab_add(struct kmem_cache *s)\n{\n\tstruct dentry *slab_cache_dir;\n\n\tif (unlikely(!slab_debugfs_root))\n\t\treturn;\n\n\tslab_cache_dir = debugfs_create_dir(s->name, slab_debugfs_root);\n\n\tdebugfs_create_file(\"alloc_traces\", 0400,\n\t\tslab_cache_dir, s, &slab_debugfs_fops);\n\n\tdebugfs_create_file(\"free_traces\", 0400,\n\t\tslab_cache_dir, s, &slab_debugfs_fops);\n}\n\nvoid debugfs_slab_release(struct kmem_cache *s)\n{\n\tdebugfs_lookup_and_remove(s->name, slab_debugfs_root);\n}\n\nstatic int __init slab_debugfs_init(void)\n{\n\tstruct kmem_cache *s;\n\n\tslab_debugfs_root = debugfs_create_dir(\"slab\", NULL);\n\n\tlist_for_each_entry(s, &slab_caches, list)\n\t\tif (s->flags & SLAB_STORE_USER)\n\t\t\tdebugfs_slab_add(s);\n\n\treturn 0;\n\n}\n__initcall(slab_debugfs_init);\n#endif\n \n#ifdef CONFIG_SLUB_DEBUG\nvoid get_slabinfo(struct kmem_cache *s, struct slabinfo *sinfo)\n{\n\tunsigned long nr_slabs = 0;\n\tunsigned long nr_objs = 0;\n\tunsigned long nr_free = 0;\n\tint node;\n\tstruct kmem_cache_node *n;\n\n\tfor_each_kmem_cache_node(s, node, n) {\n\t\tnr_slabs += node_nr_slabs(n);\n\t\tnr_objs += node_nr_objs(n);\n\t\tnr_free += count_partial(n, count_free);\n\t}\n\n\tsinfo->active_objs = nr_objs - nr_free;\n\tsinfo->num_objs = nr_objs;\n\tsinfo->active_slabs = nr_slabs;\n\tsinfo->num_slabs = nr_slabs;\n\tsinfo->objects_per_slab = oo_objects(s->oo);\n\tsinfo->cache_order = oo_order(s->oo);\n}\n\nvoid slabinfo_show_stats(struct seq_file *m, struct kmem_cache *s)\n{\n}\n\nssize_t slabinfo_write(struct file *file, const char __user *buffer,\n\t\t       size_t count, loff_t *ppos)\n{\n\treturn -EIO;\n}\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}