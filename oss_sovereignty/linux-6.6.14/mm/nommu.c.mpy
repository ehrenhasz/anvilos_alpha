{
  "module_name": "nommu.c",
  "hash_id": "d534efd9a79e82d5c06d595589f3c3d8708eedb6c2befdde29d867f5327a09ee",
  "original_prompt": "Ingested from linux-6.6.14/mm/nommu.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/export.h>\n#include <linux/mm.h>\n#include <linux/sched/mm.h>\n#include <linux/mman.h>\n#include <linux/swap.h>\n#include <linux/file.h>\n#include <linux/highmem.h>\n#include <linux/pagemap.h>\n#include <linux/slab.h>\n#include <linux/vmalloc.h>\n#include <linux/backing-dev.h>\n#include <linux/compiler.h>\n#include <linux/mount.h>\n#include <linux/personality.h>\n#include <linux/security.h>\n#include <linux/syscalls.h>\n#include <linux/audit.h>\n#include <linux/printk.h>\n\n#include <linux/uaccess.h>\n#include <linux/uio.h>\n#include <asm/tlb.h>\n#include <asm/tlbflush.h>\n#include <asm/mmu_context.h>\n#include \"internal.h\"\n\nvoid *high_memory;\nEXPORT_SYMBOL(high_memory);\nstruct page *mem_map;\nunsigned long max_mapnr;\nEXPORT_SYMBOL(max_mapnr);\nunsigned long highest_memmap_pfn;\nint sysctl_nr_trim_pages = CONFIG_NOMMU_INITIAL_TRIM_EXCESS;\nint heap_stack_gap = 0;\n\natomic_long_t mmap_pages_allocated;\n\nEXPORT_SYMBOL(mem_map);\n\n \nstatic struct kmem_cache *vm_region_jar;\nstruct rb_root nommu_region_tree = RB_ROOT;\nDECLARE_RWSEM(nommu_region_sem);\n\nconst struct vm_operations_struct generic_file_vm_ops = {\n};\n\n \nunsigned int kobjsize(const void *objp)\n{\n\tstruct page *page;\n\n\t \n\tif (!objp || !virt_addr_valid(objp))\n\t\treturn 0;\n\n\tpage = virt_to_head_page(objp);\n\n\t \n\tif (PageSlab(page))\n\t\treturn ksize(objp);\n\n\t \n\tif (!PageCompound(page)) {\n\t\tstruct vm_area_struct *vma;\n\n\t\tvma = find_vma(current->mm, (unsigned long)objp);\n\t\tif (vma)\n\t\t\treturn vma->vm_end - vma->vm_start;\n\t}\n\n\t \n\treturn page_size(page);\n}\n\n \nint follow_pfn(struct vm_area_struct *vma, unsigned long address,\n\tunsigned long *pfn)\n{\n\tif (!(vma->vm_flags & (VM_IO | VM_PFNMAP)))\n\t\treturn -EINVAL;\n\n\t*pfn = address >> PAGE_SHIFT;\n\treturn 0;\n}\nEXPORT_SYMBOL(follow_pfn);\n\nLIST_HEAD(vmap_area_list);\n\nvoid vfree(const void *addr)\n{\n\tkfree(addr);\n}\nEXPORT_SYMBOL(vfree);\n\nvoid *__vmalloc(unsigned long size, gfp_t gfp_mask)\n{\n\t \n\treturn kmalloc(size, (gfp_mask | __GFP_COMP) & ~__GFP_HIGHMEM);\n}\nEXPORT_SYMBOL(__vmalloc);\n\nvoid *__vmalloc_node_range(unsigned long size, unsigned long align,\n\t\tunsigned long start, unsigned long end, gfp_t gfp_mask,\n\t\tpgprot_t prot, unsigned long vm_flags, int node,\n\t\tconst void *caller)\n{\n\treturn __vmalloc(size, gfp_mask);\n}\n\nvoid *__vmalloc_node(unsigned long size, unsigned long align, gfp_t gfp_mask,\n\t\tint node, const void *caller)\n{\n\treturn __vmalloc(size, gfp_mask);\n}\n\nstatic void *__vmalloc_user_flags(unsigned long size, gfp_t flags)\n{\n\tvoid *ret;\n\n\tret = __vmalloc(size, flags);\n\tif (ret) {\n\t\tstruct vm_area_struct *vma;\n\n\t\tmmap_write_lock(current->mm);\n\t\tvma = find_vma(current->mm, (unsigned long)ret);\n\t\tif (vma)\n\t\t\tvm_flags_set(vma, VM_USERMAP);\n\t\tmmap_write_unlock(current->mm);\n\t}\n\n\treturn ret;\n}\n\nvoid *vmalloc_user(unsigned long size)\n{\n\treturn __vmalloc_user_flags(size, GFP_KERNEL | __GFP_ZERO);\n}\nEXPORT_SYMBOL(vmalloc_user);\n\nstruct page *vmalloc_to_page(const void *addr)\n{\n\treturn virt_to_page(addr);\n}\nEXPORT_SYMBOL(vmalloc_to_page);\n\nunsigned long vmalloc_to_pfn(const void *addr)\n{\n\treturn page_to_pfn(virt_to_page(addr));\n}\nEXPORT_SYMBOL(vmalloc_to_pfn);\n\nlong vread_iter(struct iov_iter *iter, const char *addr, size_t count)\n{\n\t \n\tif ((unsigned long) addr + count < count)\n\t\tcount = -(unsigned long) addr;\n\n\treturn copy_to_iter(addr, count, iter);\n}\n\n \nvoid *vmalloc(unsigned long size)\n{\n\treturn __vmalloc(size, GFP_KERNEL);\n}\nEXPORT_SYMBOL(vmalloc);\n\nvoid *vmalloc_huge(unsigned long size, gfp_t gfp_mask) __weak __alias(__vmalloc);\n\n \nvoid *vzalloc(unsigned long size)\n{\n\treturn __vmalloc(size, GFP_KERNEL | __GFP_ZERO);\n}\nEXPORT_SYMBOL(vzalloc);\n\n \nvoid *vmalloc_node(unsigned long size, int node)\n{\n\treturn vmalloc(size);\n}\nEXPORT_SYMBOL(vmalloc_node);\n\n \nvoid *vzalloc_node(unsigned long size, int node)\n{\n\treturn vzalloc(size);\n}\nEXPORT_SYMBOL(vzalloc_node);\n\n \nvoid *vmalloc_32(unsigned long size)\n{\n\treturn __vmalloc(size, GFP_KERNEL);\n}\nEXPORT_SYMBOL(vmalloc_32);\n\n \nvoid *vmalloc_32_user(unsigned long size)\n{\n\t \n\treturn vmalloc_user(size);\n}\nEXPORT_SYMBOL(vmalloc_32_user);\n\nvoid *vmap(struct page **pages, unsigned int count, unsigned long flags, pgprot_t prot)\n{\n\tBUG();\n\treturn NULL;\n}\nEXPORT_SYMBOL(vmap);\n\nvoid vunmap(const void *addr)\n{\n\tBUG();\n}\nEXPORT_SYMBOL(vunmap);\n\nvoid *vm_map_ram(struct page **pages, unsigned int count, int node)\n{\n\tBUG();\n\treturn NULL;\n}\nEXPORT_SYMBOL(vm_map_ram);\n\nvoid vm_unmap_ram(const void *mem, unsigned int count)\n{\n\tBUG();\n}\nEXPORT_SYMBOL(vm_unmap_ram);\n\nvoid vm_unmap_aliases(void)\n{\n}\nEXPORT_SYMBOL_GPL(vm_unmap_aliases);\n\nvoid free_vm_area(struct vm_struct *area)\n{\n\tBUG();\n}\nEXPORT_SYMBOL_GPL(free_vm_area);\n\nint vm_insert_page(struct vm_area_struct *vma, unsigned long addr,\n\t\t   struct page *page)\n{\n\treturn -EINVAL;\n}\nEXPORT_SYMBOL(vm_insert_page);\n\nint vm_map_pages(struct vm_area_struct *vma, struct page **pages,\n\t\t\tunsigned long num)\n{\n\treturn -EINVAL;\n}\nEXPORT_SYMBOL(vm_map_pages);\n\nint vm_map_pages_zero(struct vm_area_struct *vma, struct page **pages,\n\t\t\t\tunsigned long num)\n{\n\treturn -EINVAL;\n}\nEXPORT_SYMBOL(vm_map_pages_zero);\n\n \nSYSCALL_DEFINE1(brk, unsigned long, brk)\n{\n\tstruct mm_struct *mm = current->mm;\n\n\tif (brk < mm->start_brk || brk > mm->context.end_brk)\n\t\treturn mm->brk;\n\n\tif (mm->brk == brk)\n\t\treturn mm->brk;\n\n\t \n\tif (brk <= mm->brk) {\n\t\tmm->brk = brk;\n\t\treturn brk;\n\t}\n\n\t \n\tflush_icache_user_range(mm->brk, brk);\n\treturn mm->brk = brk;\n}\n\n \nvoid __init mmap_init(void)\n{\n\tint ret;\n\n\tret = percpu_counter_init(&vm_committed_as, 0, GFP_KERNEL);\n\tVM_BUG_ON(ret);\n\tvm_region_jar = KMEM_CACHE(vm_region, SLAB_PANIC|SLAB_ACCOUNT);\n}\n\n \n#ifdef CONFIG_DEBUG_NOMMU_REGIONS\nstatic noinline void validate_nommu_regions(void)\n{\n\tstruct vm_region *region, *last;\n\tstruct rb_node *p, *lastp;\n\n\tlastp = rb_first(&nommu_region_tree);\n\tif (!lastp)\n\t\treturn;\n\n\tlast = rb_entry(lastp, struct vm_region, vm_rb);\n\tBUG_ON(last->vm_end <= last->vm_start);\n\tBUG_ON(last->vm_top < last->vm_end);\n\n\twhile ((p = rb_next(lastp))) {\n\t\tregion = rb_entry(p, struct vm_region, vm_rb);\n\t\tlast = rb_entry(lastp, struct vm_region, vm_rb);\n\n\t\tBUG_ON(region->vm_end <= region->vm_start);\n\t\tBUG_ON(region->vm_top < region->vm_end);\n\t\tBUG_ON(region->vm_start < last->vm_top);\n\n\t\tlastp = p;\n\t}\n}\n#else\nstatic void validate_nommu_regions(void)\n{\n}\n#endif\n\n \nstatic void add_nommu_region(struct vm_region *region)\n{\n\tstruct vm_region *pregion;\n\tstruct rb_node **p, *parent;\n\n\tvalidate_nommu_regions();\n\n\tparent = NULL;\n\tp = &nommu_region_tree.rb_node;\n\twhile (*p) {\n\t\tparent = *p;\n\t\tpregion = rb_entry(parent, struct vm_region, vm_rb);\n\t\tif (region->vm_start < pregion->vm_start)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (region->vm_start > pregion->vm_start)\n\t\t\tp = &(*p)->rb_right;\n\t\telse if (pregion == region)\n\t\t\treturn;\n\t\telse\n\t\t\tBUG();\n\t}\n\n\trb_link_node(&region->vm_rb, parent, p);\n\trb_insert_color(&region->vm_rb, &nommu_region_tree);\n\n\tvalidate_nommu_regions();\n}\n\n \nstatic void delete_nommu_region(struct vm_region *region)\n{\n\tBUG_ON(!nommu_region_tree.rb_node);\n\n\tvalidate_nommu_regions();\n\trb_erase(&region->vm_rb, &nommu_region_tree);\n\tvalidate_nommu_regions();\n}\n\n \nstatic void free_page_series(unsigned long from, unsigned long to)\n{\n\tfor (; from < to; from += PAGE_SIZE) {\n\t\tstruct page *page = virt_to_page((void *)from);\n\n\t\tatomic_long_dec(&mmap_pages_allocated);\n\t\tput_page(page);\n\t}\n}\n\n \nstatic void __put_nommu_region(struct vm_region *region)\n\t__releases(nommu_region_sem)\n{\n\tBUG_ON(!nommu_region_tree.rb_node);\n\n\tif (--region->vm_usage == 0) {\n\t\tif (region->vm_top > region->vm_start)\n\t\t\tdelete_nommu_region(region);\n\t\tup_write(&nommu_region_sem);\n\n\t\tif (region->vm_file)\n\t\t\tfput(region->vm_file);\n\n\t\t \n\t\tif (region->vm_flags & VM_MAPPED_COPY)\n\t\t\tfree_page_series(region->vm_start, region->vm_top);\n\t\tkmem_cache_free(vm_region_jar, region);\n\t} else {\n\t\tup_write(&nommu_region_sem);\n\t}\n}\n\n \nstatic void put_nommu_region(struct vm_region *region)\n{\n\tdown_write(&nommu_region_sem);\n\t__put_nommu_region(region);\n}\n\nstatic void setup_vma_to_mm(struct vm_area_struct *vma, struct mm_struct *mm)\n{\n\tvma->vm_mm = mm;\n\n\t \n\tif (vma->vm_file) {\n\t\tstruct address_space *mapping = vma->vm_file->f_mapping;\n\n\t\ti_mmap_lock_write(mapping);\n\t\tflush_dcache_mmap_lock(mapping);\n\t\tvma_interval_tree_insert(vma, &mapping->i_mmap);\n\t\tflush_dcache_mmap_unlock(mapping);\n\t\ti_mmap_unlock_write(mapping);\n\t}\n}\n\nstatic void cleanup_vma_from_mm(struct vm_area_struct *vma)\n{\n\tvma->vm_mm->map_count--;\n\t \n\tif (vma->vm_file) {\n\t\tstruct address_space *mapping;\n\t\tmapping = vma->vm_file->f_mapping;\n\n\t\ti_mmap_lock_write(mapping);\n\t\tflush_dcache_mmap_lock(mapping);\n\t\tvma_interval_tree_remove(vma, &mapping->i_mmap);\n\t\tflush_dcache_mmap_unlock(mapping);\n\t\ti_mmap_unlock_write(mapping);\n\t}\n}\n\n \nstatic int delete_vma_from_mm(struct vm_area_struct *vma)\n{\n\tVMA_ITERATOR(vmi, vma->vm_mm, vma->vm_start);\n\n\tvma_iter_config(&vmi, vma->vm_start, vma->vm_end);\n\tif (vma_iter_prealloc(&vmi, vma)) {\n\t\tpr_warn(\"Allocation of vma tree for process %d failed\\n\",\n\t\t       current->pid);\n\t\treturn -ENOMEM;\n\t}\n\tcleanup_vma_from_mm(vma);\n\n\t \n\tvma_iter_clear(&vmi);\n\treturn 0;\n}\n \nstatic void delete_vma(struct mm_struct *mm, struct vm_area_struct *vma)\n{\n\tif (vma->vm_ops && vma->vm_ops->close)\n\t\tvma->vm_ops->close(vma);\n\tif (vma->vm_file)\n\t\tfput(vma->vm_file);\n\tput_nommu_region(vma->vm_region);\n\tvm_area_free(vma);\n}\n\nstruct vm_area_struct *find_vma_intersection(struct mm_struct *mm,\n\t\t\t\t\t     unsigned long start_addr,\n\t\t\t\t\t     unsigned long end_addr)\n{\n\tunsigned long index = start_addr;\n\n\tmmap_assert_locked(mm);\n\treturn mt_find(&mm->mm_mt, &index, end_addr - 1);\n}\nEXPORT_SYMBOL(find_vma_intersection);\n\n \nstruct vm_area_struct *find_vma(struct mm_struct *mm, unsigned long addr)\n{\n\tVMA_ITERATOR(vmi, mm, addr);\n\n\treturn vma_iter_load(&vmi);\n}\nEXPORT_SYMBOL(find_vma);\n\n \nstruct vm_area_struct *lock_mm_and_find_vma(struct mm_struct *mm,\n\t\t\tunsigned long addr, struct pt_regs *regs)\n{\n\tstruct vm_area_struct *vma;\n\n\tmmap_read_lock(mm);\n\tvma = vma_lookup(mm, addr);\n\tif (!vma)\n\t\tmmap_read_unlock(mm);\n\treturn vma;\n}\n\n \nint expand_stack_locked(struct vm_area_struct *vma, unsigned long addr)\n{\n\treturn -ENOMEM;\n}\n\nstruct vm_area_struct *expand_stack(struct mm_struct *mm, unsigned long addr)\n{\n\tmmap_read_unlock(mm);\n\treturn NULL;\n}\n\n \nstatic struct vm_area_struct *find_vma_exact(struct mm_struct *mm,\n\t\t\t\t\t     unsigned long addr,\n\t\t\t\t\t     unsigned long len)\n{\n\tstruct vm_area_struct *vma;\n\tunsigned long end = addr + len;\n\tVMA_ITERATOR(vmi, mm, addr);\n\n\tvma = vma_iter_load(&vmi);\n\tif (!vma)\n\t\treturn NULL;\n\tif (vma->vm_start != addr)\n\t\treturn NULL;\n\tif (vma->vm_end != end)\n\t\treturn NULL;\n\n\treturn vma;\n}\n\n \nstatic int validate_mmap_request(struct file *file,\n\t\t\t\t unsigned long addr,\n\t\t\t\t unsigned long len,\n\t\t\t\t unsigned long prot,\n\t\t\t\t unsigned long flags,\n\t\t\t\t unsigned long pgoff,\n\t\t\t\t unsigned long *_capabilities)\n{\n\tunsigned long capabilities, rlen;\n\tint ret;\n\n\t \n\tif (flags & MAP_FIXED)\n\t\treturn -EINVAL;\n\n\tif ((flags & MAP_TYPE) != MAP_PRIVATE &&\n\t    (flags & MAP_TYPE) != MAP_SHARED)\n\t\treturn -EINVAL;\n\n\tif (!len)\n\t\treturn -EINVAL;\n\n\t \n\trlen = PAGE_ALIGN(len);\n\tif (!rlen || rlen > TASK_SIZE)\n\t\treturn -ENOMEM;\n\n\t \n\tif ((pgoff + (rlen >> PAGE_SHIFT)) < pgoff)\n\t\treturn -EOVERFLOW;\n\n\tif (file) {\n\t\t \n\t\tif (!file->f_op->mmap)\n\t\t\treturn -ENODEV;\n\n\t\t \n\t\tif (file->f_op->mmap_capabilities) {\n\t\t\tcapabilities = file->f_op->mmap_capabilities(file);\n\t\t} else {\n\t\t\t \n\t\t\tswitch (file_inode(file)->i_mode & S_IFMT) {\n\t\t\tcase S_IFREG:\n\t\t\tcase S_IFBLK:\n\t\t\t\tcapabilities = NOMMU_MAP_COPY;\n\t\t\t\tbreak;\n\n\t\t\tcase S_IFCHR:\n\t\t\t\tcapabilities =\n\t\t\t\t\tNOMMU_MAP_DIRECT |\n\t\t\t\t\tNOMMU_MAP_READ |\n\t\t\t\t\tNOMMU_MAP_WRITE;\n\t\t\t\tbreak;\n\n\t\t\tdefault:\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (!file->f_op->get_unmapped_area)\n\t\t\tcapabilities &= ~NOMMU_MAP_DIRECT;\n\t\tif (!(file->f_mode & FMODE_CAN_READ))\n\t\t\tcapabilities &= ~NOMMU_MAP_COPY;\n\n\t\t \n\t\tif (!(file->f_mode & FMODE_READ))\n\t\t\treturn -EACCES;\n\n\t\tif (flags & MAP_SHARED) {\n\t\t\t \n\t\t\tif ((prot & PROT_WRITE) &&\n\t\t\t    !(file->f_mode & FMODE_WRITE))\n\t\t\t\treturn -EACCES;\n\n\t\t\tif (IS_APPEND(file_inode(file)) &&\n\t\t\t    (file->f_mode & FMODE_WRITE))\n\t\t\t\treturn -EACCES;\n\n\t\t\tif (!(capabilities & NOMMU_MAP_DIRECT))\n\t\t\t\treturn -ENODEV;\n\n\t\t\t \n\t\t\tcapabilities &= ~NOMMU_MAP_COPY;\n\t\t} else {\n\t\t\t \n\t\t\tif (!(capabilities & NOMMU_MAP_COPY))\n\t\t\t\treturn -ENODEV;\n\n\t\t\t \n\t\t\tif (prot & PROT_WRITE)\n\t\t\t\tcapabilities &= ~NOMMU_MAP_DIRECT;\n\t\t}\n\n\t\tif (capabilities & NOMMU_MAP_DIRECT) {\n\t\t\tif (((prot & PROT_READ)  && !(capabilities & NOMMU_MAP_READ))  ||\n\t\t\t    ((prot & PROT_WRITE) && !(capabilities & NOMMU_MAP_WRITE)) ||\n\t\t\t    ((prot & PROT_EXEC)  && !(capabilities & NOMMU_MAP_EXEC))\n\t\t\t    ) {\n\t\t\t\tcapabilities &= ~NOMMU_MAP_DIRECT;\n\t\t\t\tif (flags & MAP_SHARED) {\n\t\t\t\t\tpr_warn(\"MAP_SHARED not completely supported on !MMU\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (path_noexec(&file->f_path)) {\n\t\t\tif (prot & PROT_EXEC)\n\t\t\t\treturn -EPERM;\n\t\t} else if ((prot & PROT_READ) && !(prot & PROT_EXEC)) {\n\t\t\t \n\t\t\tif (current->personality & READ_IMPLIES_EXEC) {\n\t\t\t\tif (capabilities & NOMMU_MAP_EXEC)\n\t\t\t\t\tprot |= PROT_EXEC;\n\t\t\t}\n\t\t} else if ((prot & PROT_READ) &&\n\t\t\t (prot & PROT_EXEC) &&\n\t\t\t !(capabilities & NOMMU_MAP_EXEC)\n\t\t\t ) {\n\t\t\t \n\t\t\tcapabilities &= ~NOMMU_MAP_DIRECT;\n\t\t}\n\t} else {\n\t\t \n\t\tcapabilities = NOMMU_MAP_COPY;\n\n\t\t \n\t\tif ((prot & PROT_READ) &&\n\t\t    (current->personality & READ_IMPLIES_EXEC))\n\t\t\tprot |= PROT_EXEC;\n\t}\n\n\t \n\tret = security_mmap_addr(addr);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t \n\t*_capabilities = capabilities;\n\treturn 0;\n}\n\n \nstatic unsigned long determine_vm_flags(struct file *file,\n\t\t\t\t\tunsigned long prot,\n\t\t\t\t\tunsigned long flags,\n\t\t\t\t\tunsigned long capabilities)\n{\n\tunsigned long vm_flags;\n\n\tvm_flags = calc_vm_prot_bits(prot, 0) | calc_vm_flag_bits(flags);\n\n\tif (!file) {\n\t\t \n\t\tvm_flags |= VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC;\n\t} else if (flags & MAP_PRIVATE) {\n\t\t \n\t\tif (capabilities & NOMMU_MAP_DIRECT)\n\t\t\tvm_flags |= (capabilities & NOMMU_VMFLAGS);\n\t\telse\n\t\t\tvm_flags |= VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC;\n\n\t\tif (!(prot & PROT_WRITE) && !current->ptrace)\n\t\t\t \n\t\t\tvm_flags |= VM_MAYOVERLAY;\n\t} else {\n\t\t \n\t\tvm_flags |= VM_SHARED | VM_MAYSHARE |\n\t\t\t    (capabilities & NOMMU_VMFLAGS);\n\t}\n\n\treturn vm_flags;\n}\n\n \nstatic int do_mmap_shared_file(struct vm_area_struct *vma)\n{\n\tint ret;\n\n\tret = call_mmap(vma->vm_file, vma);\n\tif (ret == 0) {\n\t\tvma->vm_region->vm_top = vma->vm_region->vm_end;\n\t\treturn 0;\n\t}\n\tif (ret != -ENOSYS)\n\t\treturn ret;\n\n\t \n\treturn -ENODEV;\n}\n\n \nstatic int do_mmap_private(struct vm_area_struct *vma,\n\t\t\t   struct vm_region *region,\n\t\t\t   unsigned long len,\n\t\t\t   unsigned long capabilities)\n{\n\tunsigned long total, point;\n\tvoid *base;\n\tint ret, order;\n\n\t \n\tif (capabilities & NOMMU_MAP_DIRECT) {\n\t\tret = call_mmap(vma->vm_file, vma);\n\t\t \n\t\tif (WARN_ON_ONCE(!is_nommu_shared_mapping(vma->vm_flags)))\n\t\t\tret = -ENOSYS;\n\t\tif (ret == 0) {\n\t\t\tvma->vm_region->vm_top = vma->vm_region->vm_end;\n\t\t\treturn 0;\n\t\t}\n\t\tif (ret != -ENOSYS)\n\t\t\treturn ret;\n\n\t\t \n\t}\n\n\n\t \n\torder = get_order(len);\n\ttotal = 1 << order;\n\tpoint = len >> PAGE_SHIFT;\n\n\t \n\tif (sysctl_nr_trim_pages && total - point >= sysctl_nr_trim_pages)\n\t\ttotal = point;\n\n\tbase = alloc_pages_exact(total << PAGE_SHIFT, GFP_KERNEL);\n\tif (!base)\n\t\tgoto enomem;\n\n\tatomic_long_add(total, &mmap_pages_allocated);\n\n\tvm_flags_set(vma, VM_MAPPED_COPY);\n\tregion->vm_flags = vma->vm_flags;\n\tregion->vm_start = (unsigned long) base;\n\tregion->vm_end   = region->vm_start + len;\n\tregion->vm_top   = region->vm_start + (total << PAGE_SHIFT);\n\n\tvma->vm_start = region->vm_start;\n\tvma->vm_end   = region->vm_start + len;\n\n\tif (vma->vm_file) {\n\t\t \n\t\tloff_t fpos;\n\n\t\tfpos = vma->vm_pgoff;\n\t\tfpos <<= PAGE_SHIFT;\n\n\t\tret = kernel_read(vma->vm_file, base, len, &fpos);\n\t\tif (ret < 0)\n\t\t\tgoto error_free;\n\n\t\t \n\t\tif (ret < len)\n\t\t\tmemset(base + ret, 0, len - ret);\n\n\t} else {\n\t\tvma_set_anonymous(vma);\n\t}\n\n\treturn 0;\n\nerror_free:\n\tfree_page_series(region->vm_start, region->vm_top);\n\tregion->vm_start = vma->vm_start = 0;\n\tregion->vm_end   = vma->vm_end = 0;\n\tregion->vm_top   = 0;\n\treturn ret;\n\nenomem:\n\tpr_err(\"Allocation of length %lu from process %d (%s) failed\\n\",\n\t       len, current->pid, current->comm);\n\tshow_mem();\n\treturn -ENOMEM;\n}\n\n \nunsigned long do_mmap(struct file *file,\n\t\t\tunsigned long addr,\n\t\t\tunsigned long len,\n\t\t\tunsigned long prot,\n\t\t\tunsigned long flags,\n\t\t\tvm_flags_t vm_flags,\n\t\t\tunsigned long pgoff,\n\t\t\tunsigned long *populate,\n\t\t\tstruct list_head *uf)\n{\n\tstruct vm_area_struct *vma;\n\tstruct vm_region *region;\n\tstruct rb_node *rb;\n\tunsigned long capabilities, result;\n\tint ret;\n\tVMA_ITERATOR(vmi, current->mm, 0);\n\n\t*populate = 0;\n\n\t \n\tret = validate_mmap_request(file, addr, len, prot, flags, pgoff,\n\t\t\t\t    &capabilities);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t \n\taddr = 0;\n\tlen = PAGE_ALIGN(len);\n\n\t \n\tvm_flags |= determine_vm_flags(file, prot, flags, capabilities);\n\n\n\t \n\tregion = kmem_cache_zalloc(vm_region_jar, GFP_KERNEL);\n\tif (!region)\n\t\tgoto error_getting_region;\n\n\tvma = vm_area_alloc(current->mm);\n\tif (!vma)\n\t\tgoto error_getting_vma;\n\n\tregion->vm_usage = 1;\n\tregion->vm_flags = vm_flags;\n\tregion->vm_pgoff = pgoff;\n\n\tvm_flags_init(vma, vm_flags);\n\tvma->vm_pgoff = pgoff;\n\n\tif (file) {\n\t\tregion->vm_file = get_file(file);\n\t\tvma->vm_file = get_file(file);\n\t}\n\n\tdown_write(&nommu_region_sem);\n\n\t \n\tif (is_nommu_shared_mapping(vm_flags)) {\n\t\tstruct vm_region *pregion;\n\t\tunsigned long pglen, rpglen, pgend, rpgend, start;\n\n\t\tpglen = (len + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t\tpgend = pgoff + pglen;\n\n\t\tfor (rb = rb_first(&nommu_region_tree); rb; rb = rb_next(rb)) {\n\t\t\tpregion = rb_entry(rb, struct vm_region, vm_rb);\n\n\t\t\tif (!is_nommu_shared_mapping(pregion->vm_flags))\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tif (file_inode(pregion->vm_file) !=\n\t\t\t    file_inode(file))\n\t\t\t\tcontinue;\n\n\t\t\tif (pregion->vm_pgoff >= pgend)\n\t\t\t\tcontinue;\n\n\t\t\trpglen = pregion->vm_end - pregion->vm_start;\n\t\t\trpglen = (rpglen + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t\t\trpgend = pregion->vm_pgoff + rpglen;\n\t\t\tif (pgoff >= rpgend)\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tif ((pregion->vm_pgoff != pgoff || rpglen != pglen) &&\n\t\t\t    !(pgoff >= pregion->vm_pgoff && pgend <= rpgend)) {\n\t\t\t\t \n\t\t\t\tif (!(capabilities & NOMMU_MAP_DIRECT))\n\t\t\t\t\tgoto sharing_violation;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t \n\t\t\tpregion->vm_usage++;\n\t\t\tvma->vm_region = pregion;\n\t\t\tstart = pregion->vm_start;\n\t\t\tstart += (pgoff - pregion->vm_pgoff) << PAGE_SHIFT;\n\t\t\tvma->vm_start = start;\n\t\t\tvma->vm_end = start + len;\n\n\t\t\tif (pregion->vm_flags & VM_MAPPED_COPY)\n\t\t\t\tvm_flags_set(vma, VM_MAPPED_COPY);\n\t\t\telse {\n\t\t\t\tret = do_mmap_shared_file(vma);\n\t\t\t\tif (ret < 0) {\n\t\t\t\t\tvma->vm_region = NULL;\n\t\t\t\t\tvma->vm_start = 0;\n\t\t\t\t\tvma->vm_end = 0;\n\t\t\t\t\tpregion->vm_usage--;\n\t\t\t\t\tpregion = NULL;\n\t\t\t\t\tgoto error_just_free;\n\t\t\t\t}\n\t\t\t}\n\t\t\tfput(region->vm_file);\n\t\t\tkmem_cache_free(vm_region_jar, region);\n\t\t\tregion = pregion;\n\t\t\tresult = start;\n\t\t\tgoto share;\n\t\t}\n\n\t\t \n\t\tif (capabilities & NOMMU_MAP_DIRECT) {\n\t\t\taddr = file->f_op->get_unmapped_area(file, addr, len,\n\t\t\t\t\t\t\t     pgoff, flags);\n\t\t\tif (IS_ERR_VALUE(addr)) {\n\t\t\t\tret = addr;\n\t\t\t\tif (ret != -ENOSYS)\n\t\t\t\t\tgoto error_just_free;\n\n\t\t\t\t \n\t\t\t\tret = -ENODEV;\n\t\t\t\tif (!(capabilities & NOMMU_MAP_COPY))\n\t\t\t\t\tgoto error_just_free;\n\n\t\t\t\tcapabilities &= ~NOMMU_MAP_DIRECT;\n\t\t\t} else {\n\t\t\t\tvma->vm_start = region->vm_start = addr;\n\t\t\t\tvma->vm_end = region->vm_end = addr + len;\n\t\t\t}\n\t\t}\n\t}\n\n\tvma->vm_region = region;\n\n\t \n\tif (file && vma->vm_flags & VM_SHARED)\n\t\tret = do_mmap_shared_file(vma);\n\telse\n\t\tret = do_mmap_private(vma, region, len, capabilities);\n\tif (ret < 0)\n\t\tgoto error_just_free;\n\tadd_nommu_region(region);\n\n\t \n\tif (!vma->vm_file &&\n\t    (!IS_ENABLED(CONFIG_MMAP_ALLOW_UNINITIALIZED) ||\n\t     !(flags & MAP_UNINITIALIZED)))\n\t\tmemset((void *)region->vm_start, 0,\n\t\t       region->vm_end - region->vm_start);\n\n\t \n\tresult = vma->vm_start;\n\n\tcurrent->mm->total_vm += len >> PAGE_SHIFT;\n\nshare:\n\tBUG_ON(!vma->vm_region);\n\tvma_iter_config(&vmi, vma->vm_start, vma->vm_end);\n\tif (vma_iter_prealloc(&vmi, vma))\n\t\tgoto error_just_free;\n\n\tsetup_vma_to_mm(vma, current->mm);\n\tcurrent->mm->map_count++;\n\t \n\tvma_iter_store(&vmi, vma);\n\n\t \n\tif (vma->vm_flags & VM_EXEC && !region->vm_icache_flushed) {\n\t\tflush_icache_user_range(region->vm_start, region->vm_end);\n\t\tregion->vm_icache_flushed = true;\n\t}\n\n\tup_write(&nommu_region_sem);\n\n\treturn result;\n\nerror_just_free:\n\tup_write(&nommu_region_sem);\nerror:\n\tvma_iter_free(&vmi);\n\tif (region->vm_file)\n\t\tfput(region->vm_file);\n\tkmem_cache_free(vm_region_jar, region);\n\tif (vma->vm_file)\n\t\tfput(vma->vm_file);\n\tvm_area_free(vma);\n\treturn ret;\n\nsharing_violation:\n\tup_write(&nommu_region_sem);\n\tpr_warn(\"Attempt to share mismatched mappings\\n\");\n\tret = -EINVAL;\n\tgoto error;\n\nerror_getting_vma:\n\tkmem_cache_free(vm_region_jar, region);\n\tpr_warn(\"Allocation of vma for %lu byte allocation from process %d failed\\n\",\n\t\t\tlen, current->pid);\n\tshow_mem();\n\treturn -ENOMEM;\n\nerror_getting_region:\n\tpr_warn(\"Allocation of vm region for %lu byte allocation from process %d failed\\n\",\n\t\t\tlen, current->pid);\n\tshow_mem();\n\treturn -ENOMEM;\n}\n\nunsigned long ksys_mmap_pgoff(unsigned long addr, unsigned long len,\n\t\t\t      unsigned long prot, unsigned long flags,\n\t\t\t      unsigned long fd, unsigned long pgoff)\n{\n\tstruct file *file = NULL;\n\tunsigned long retval = -EBADF;\n\n\taudit_mmap_fd(fd, flags);\n\tif (!(flags & MAP_ANONYMOUS)) {\n\t\tfile = fget(fd);\n\t\tif (!file)\n\t\t\tgoto out;\n\t}\n\n\tretval = vm_mmap_pgoff(file, addr, len, prot, flags, pgoff);\n\n\tif (file)\n\t\tfput(file);\nout:\n\treturn retval;\n}\n\nSYSCALL_DEFINE6(mmap_pgoff, unsigned long, addr, unsigned long, len,\n\t\tunsigned long, prot, unsigned long, flags,\n\t\tunsigned long, fd, unsigned long, pgoff)\n{\n\treturn ksys_mmap_pgoff(addr, len, prot, flags, fd, pgoff);\n}\n\n#ifdef __ARCH_WANT_SYS_OLD_MMAP\nstruct mmap_arg_struct {\n\tunsigned long addr;\n\tunsigned long len;\n\tunsigned long prot;\n\tunsigned long flags;\n\tunsigned long fd;\n\tunsigned long offset;\n};\n\nSYSCALL_DEFINE1(old_mmap, struct mmap_arg_struct __user *, arg)\n{\n\tstruct mmap_arg_struct a;\n\n\tif (copy_from_user(&a, arg, sizeof(a)))\n\t\treturn -EFAULT;\n\tif (offset_in_page(a.offset))\n\t\treturn -EINVAL;\n\n\treturn ksys_mmap_pgoff(a.addr, a.len, a.prot, a.flags, a.fd,\n\t\t\t       a.offset >> PAGE_SHIFT);\n}\n#endif  \n\n \nint split_vma(struct vma_iterator *vmi, struct vm_area_struct *vma,\n\t      unsigned long addr, int new_below)\n{\n\tstruct vm_area_struct *new;\n\tstruct vm_region *region;\n\tunsigned long npages;\n\tstruct mm_struct *mm;\n\n\t \n\tif (vma->vm_file)\n\t\treturn -ENOMEM;\n\n\tmm = vma->vm_mm;\n\tif (mm->map_count >= sysctl_max_map_count)\n\t\treturn -ENOMEM;\n\n\tregion = kmem_cache_alloc(vm_region_jar, GFP_KERNEL);\n\tif (!region)\n\t\treturn -ENOMEM;\n\n\tnew = vm_area_dup(vma);\n\tif (!new)\n\t\tgoto err_vma_dup;\n\n\t \n\t*region = *vma->vm_region;\n\tnew->vm_region = region;\n\n\tnpages = (addr - vma->vm_start) >> PAGE_SHIFT;\n\n\tif (new_below) {\n\t\tregion->vm_top = region->vm_end = new->vm_end = addr;\n\t} else {\n\t\tregion->vm_start = new->vm_start = addr;\n\t\tregion->vm_pgoff = new->vm_pgoff += npages;\n\t}\n\n\tvma_iter_config(vmi, new->vm_start, new->vm_end);\n\tif (vma_iter_prealloc(vmi, vma)) {\n\t\tpr_warn(\"Allocation of vma tree for process %d failed\\n\",\n\t\t\tcurrent->pid);\n\t\tgoto err_vmi_preallocate;\n\t}\n\n\tif (new->vm_ops && new->vm_ops->open)\n\t\tnew->vm_ops->open(new);\n\n\tdown_write(&nommu_region_sem);\n\tdelete_nommu_region(vma->vm_region);\n\tif (new_below) {\n\t\tvma->vm_region->vm_start = vma->vm_start = addr;\n\t\tvma->vm_region->vm_pgoff = vma->vm_pgoff += npages;\n\t} else {\n\t\tvma->vm_region->vm_end = vma->vm_end = addr;\n\t\tvma->vm_region->vm_top = addr;\n\t}\n\tadd_nommu_region(vma->vm_region);\n\tadd_nommu_region(new->vm_region);\n\tup_write(&nommu_region_sem);\n\n\tsetup_vma_to_mm(vma, mm);\n\tsetup_vma_to_mm(new, mm);\n\tvma_iter_store(vmi, new);\n\tmm->map_count++;\n\treturn 0;\n\nerr_vmi_preallocate:\n\tvm_area_free(new);\nerr_vma_dup:\n\tkmem_cache_free(vm_region_jar, region);\n\treturn -ENOMEM;\n}\n\n \nstatic int vmi_shrink_vma(struct vma_iterator *vmi,\n\t\t      struct vm_area_struct *vma,\n\t\t      unsigned long from, unsigned long to)\n{\n\tstruct vm_region *region;\n\n\t \n\tif (from > vma->vm_start) {\n\t\tif (vma_iter_clear_gfp(vmi, from, vma->vm_end, GFP_KERNEL))\n\t\t\treturn -ENOMEM;\n\t\tvma->vm_end = from;\n\t} else {\n\t\tif (vma_iter_clear_gfp(vmi, vma->vm_start, to, GFP_KERNEL))\n\t\t\treturn -ENOMEM;\n\t\tvma->vm_start = to;\n\t}\n\n\t \n\tregion = vma->vm_region;\n\tBUG_ON(region->vm_usage != 1);\n\n\tdown_write(&nommu_region_sem);\n\tdelete_nommu_region(region);\n\tif (from > region->vm_start) {\n\t\tto = region->vm_top;\n\t\tregion->vm_top = region->vm_end = from;\n\t} else {\n\t\tregion->vm_start = to;\n\t}\n\tadd_nommu_region(region);\n\tup_write(&nommu_region_sem);\n\n\tfree_page_series(from, to);\n\treturn 0;\n}\n\n \nint do_munmap(struct mm_struct *mm, unsigned long start, size_t len, struct list_head *uf)\n{\n\tVMA_ITERATOR(vmi, mm, start);\n\tstruct vm_area_struct *vma;\n\tunsigned long end;\n\tint ret = 0;\n\n\tlen = PAGE_ALIGN(len);\n\tif (len == 0)\n\t\treturn -EINVAL;\n\n\tend = start + len;\n\n\t \n\tvma = vma_find(&vmi, end);\n\tif (!vma) {\n\t\tstatic int limit;\n\t\tif (limit < 5) {\n\t\t\tpr_warn(\"munmap of memory not mmapped by process %d (%s): 0x%lx-0x%lx\\n\",\n\t\t\t\t\tcurrent->pid, current->comm,\n\t\t\t\t\tstart, start + len - 1);\n\t\t\tlimit++;\n\t\t}\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (vma->vm_file) {\n\t\tdo {\n\t\t\tif (start > vma->vm_start)\n\t\t\t\treturn -EINVAL;\n\t\t\tif (end == vma->vm_end)\n\t\t\t\tgoto erase_whole_vma;\n\t\t\tvma = vma_find(&vmi, end);\n\t\t} while (vma);\n\t\treturn -EINVAL;\n\t} else {\n\t\t \n\t\tif (start == vma->vm_start && end == vma->vm_end)\n\t\t\tgoto erase_whole_vma;\n\t\tif (start < vma->vm_start || end > vma->vm_end)\n\t\t\treturn -EINVAL;\n\t\tif (offset_in_page(start))\n\t\t\treturn -EINVAL;\n\t\tif (end != vma->vm_end && offset_in_page(end))\n\t\t\treturn -EINVAL;\n\t\tif (start != vma->vm_start && end != vma->vm_end) {\n\t\t\tret = split_vma(&vmi, vma, start, 1);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\t\t}\n\t\treturn vmi_shrink_vma(&vmi, vma, start, end);\n\t}\n\nerase_whole_vma:\n\tif (delete_vma_from_mm(vma))\n\t\tret = -ENOMEM;\n\telse\n\t\tdelete_vma(mm, vma);\n\treturn ret;\n}\n\nint vm_munmap(unsigned long addr, size_t len)\n{\n\tstruct mm_struct *mm = current->mm;\n\tint ret;\n\n\tmmap_write_lock(mm);\n\tret = do_munmap(mm, addr, len, NULL);\n\tmmap_write_unlock(mm);\n\treturn ret;\n}\nEXPORT_SYMBOL(vm_munmap);\n\nSYSCALL_DEFINE2(munmap, unsigned long, addr, size_t, len)\n{\n\treturn vm_munmap(addr, len);\n}\n\n \nvoid exit_mmap(struct mm_struct *mm)\n{\n\tVMA_ITERATOR(vmi, mm, 0);\n\tstruct vm_area_struct *vma;\n\n\tif (!mm)\n\t\treturn;\n\n\tmm->total_vm = 0;\n\n\t \n\tmmap_write_lock(mm);\n\tfor_each_vma(vmi, vma) {\n\t\tcleanup_vma_from_mm(vma);\n\t\tdelete_vma(mm, vma);\n\t\tcond_resched();\n\t}\n\t__mt_destroy(&mm->mm_mt);\n\tmmap_write_unlock(mm);\n}\n\nint vm_brk(unsigned long addr, unsigned long len)\n{\n\treturn -ENOMEM;\n}\n\n \nstatic unsigned long do_mremap(unsigned long addr,\n\t\t\tunsigned long old_len, unsigned long new_len,\n\t\t\tunsigned long flags, unsigned long new_addr)\n{\n\tstruct vm_area_struct *vma;\n\n\t \n\told_len = PAGE_ALIGN(old_len);\n\tnew_len = PAGE_ALIGN(new_len);\n\tif (old_len == 0 || new_len == 0)\n\t\treturn (unsigned long) -EINVAL;\n\n\tif (offset_in_page(addr))\n\t\treturn -EINVAL;\n\n\tif (flags & MREMAP_FIXED && new_addr != addr)\n\t\treturn (unsigned long) -EINVAL;\n\n\tvma = find_vma_exact(current->mm, addr, old_len);\n\tif (!vma)\n\t\treturn (unsigned long) -EINVAL;\n\n\tif (vma->vm_end != vma->vm_start + old_len)\n\t\treturn (unsigned long) -EFAULT;\n\n\tif (is_nommu_shared_mapping(vma->vm_flags))\n\t\treturn (unsigned long) -EPERM;\n\n\tif (new_len > vma->vm_region->vm_end - vma->vm_region->vm_start)\n\t\treturn (unsigned long) -ENOMEM;\n\n\t \n\tvma->vm_end = vma->vm_start + new_len;\n\treturn vma->vm_start;\n}\n\nSYSCALL_DEFINE5(mremap, unsigned long, addr, unsigned long, old_len,\n\t\tunsigned long, new_len, unsigned long, flags,\n\t\tunsigned long, new_addr)\n{\n\tunsigned long ret;\n\n\tmmap_write_lock(current->mm);\n\tret = do_mremap(addr, old_len, new_len, flags, new_addr);\n\tmmap_write_unlock(current->mm);\n\treturn ret;\n}\n\nstruct page *follow_page(struct vm_area_struct *vma, unsigned long address,\n\t\t\t unsigned int foll_flags)\n{\n\treturn NULL;\n}\n\nint remap_pfn_range(struct vm_area_struct *vma, unsigned long addr,\n\t\tunsigned long pfn, unsigned long size, pgprot_t prot)\n{\n\tif (addr != (pfn << PAGE_SHIFT))\n\t\treturn -EINVAL;\n\n\tvm_flags_set(vma, VM_IO | VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP);\n\treturn 0;\n}\nEXPORT_SYMBOL(remap_pfn_range);\n\nint vm_iomap_memory(struct vm_area_struct *vma, phys_addr_t start, unsigned long len)\n{\n\tunsigned long pfn = start >> PAGE_SHIFT;\n\tunsigned long vm_len = vma->vm_end - vma->vm_start;\n\n\tpfn += vma->vm_pgoff;\n\treturn io_remap_pfn_range(vma, vma->vm_start, pfn, vm_len, vma->vm_page_prot);\n}\nEXPORT_SYMBOL(vm_iomap_memory);\n\nint remap_vmalloc_range(struct vm_area_struct *vma, void *addr,\n\t\t\tunsigned long pgoff)\n{\n\tunsigned int size = vma->vm_end - vma->vm_start;\n\n\tif (!(vma->vm_flags & VM_USERMAP))\n\t\treturn -EINVAL;\n\n\tvma->vm_start = (unsigned long)(addr + (pgoff << PAGE_SHIFT));\n\tvma->vm_end = vma->vm_start + size;\n\n\treturn 0;\n}\nEXPORT_SYMBOL(remap_vmalloc_range);\n\nvm_fault_t filemap_fault(struct vm_fault *vmf)\n{\n\tBUG();\n\treturn 0;\n}\nEXPORT_SYMBOL(filemap_fault);\n\nvm_fault_t filemap_map_pages(struct vm_fault *vmf,\n\t\tpgoff_t start_pgoff, pgoff_t end_pgoff)\n{\n\tBUG();\n\treturn 0;\n}\nEXPORT_SYMBOL(filemap_map_pages);\n\nint __access_remote_vm(struct mm_struct *mm, unsigned long addr, void *buf,\n\t\t       int len, unsigned int gup_flags)\n{\n\tstruct vm_area_struct *vma;\n\tint write = gup_flags & FOLL_WRITE;\n\n\tif (mmap_read_lock_killable(mm))\n\t\treturn 0;\n\n\t \n\tvma = find_vma(mm, addr);\n\tif (vma) {\n\t\t \n\t\tif (addr + len >= vma->vm_end)\n\t\t\tlen = vma->vm_end - addr;\n\n\t\t \n\t\tif (write && vma->vm_flags & VM_MAYWRITE)\n\t\t\tcopy_to_user_page(vma, NULL, addr,\n\t\t\t\t\t (void *) addr, buf, len);\n\t\telse if (!write && vma->vm_flags & VM_MAYREAD)\n\t\t\tcopy_from_user_page(vma, NULL, addr,\n\t\t\t\t\t    buf, (void *) addr, len);\n\t\telse\n\t\t\tlen = 0;\n\t} else {\n\t\tlen = 0;\n\t}\n\n\tmmap_read_unlock(mm);\n\n\treturn len;\n}\n\n \nint access_remote_vm(struct mm_struct *mm, unsigned long addr,\n\t\tvoid *buf, int len, unsigned int gup_flags)\n{\n\treturn __access_remote_vm(mm, addr, buf, len, gup_flags);\n}\n\n \nint access_process_vm(struct task_struct *tsk, unsigned long addr, void *buf, int len,\n\t\tunsigned int gup_flags)\n{\n\tstruct mm_struct *mm;\n\n\tif (addr + len < addr)\n\t\treturn 0;\n\n\tmm = get_task_mm(tsk);\n\tif (!mm)\n\t\treturn 0;\n\n\tlen = __access_remote_vm(mm, addr, buf, len, gup_flags);\n\n\tmmput(mm);\n\treturn len;\n}\nEXPORT_SYMBOL_GPL(access_process_vm);\n\n \nint nommu_shrink_inode_mappings(struct inode *inode, size_t size,\n\t\t\t\tsize_t newsize)\n{\n\tstruct vm_area_struct *vma;\n\tstruct vm_region *region;\n\tpgoff_t low, high;\n\tsize_t r_size, r_top;\n\n\tlow = newsize >> PAGE_SHIFT;\n\thigh = (size + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\n\tdown_write(&nommu_region_sem);\n\ti_mmap_lock_read(inode->i_mapping);\n\n\t \n\tvma_interval_tree_foreach(vma, &inode->i_mapping->i_mmap, low, high) {\n\t\t \n\t\tif (vma->vm_flags & VM_SHARED) {\n\t\t\ti_mmap_unlock_read(inode->i_mapping);\n\t\t\tup_write(&nommu_region_sem);\n\t\t\treturn -ETXTBSY;  \n\t\t}\n\t}\n\n\t \n\tvma_interval_tree_foreach(vma, &inode->i_mapping->i_mmap, 0, ULONG_MAX) {\n\t\tif (!(vma->vm_flags & VM_SHARED))\n\t\t\tcontinue;\n\n\t\tregion = vma->vm_region;\n\t\tr_size = region->vm_top - region->vm_start;\n\t\tr_top = (region->vm_pgoff << PAGE_SHIFT) + r_size;\n\n\t\tif (r_top > newsize) {\n\t\t\tregion->vm_top -= r_top - newsize;\n\t\t\tif (region->vm_end > region->vm_top)\n\t\t\t\tregion->vm_end = region->vm_top;\n\t\t}\n\t}\n\n\ti_mmap_unlock_read(inode->i_mapping);\n\tup_write(&nommu_region_sem);\n\treturn 0;\n}\n\n \nstatic int __meminit init_user_reserve(void)\n{\n\tunsigned long free_kbytes;\n\n\tfree_kbytes = K(global_zone_page_state(NR_FREE_PAGES));\n\n\tsysctl_user_reserve_kbytes = min(free_kbytes / 32, 1UL << 17);\n\treturn 0;\n}\nsubsys_initcall(init_user_reserve);\n\n \nstatic int __meminit init_admin_reserve(void)\n{\n\tunsigned long free_kbytes;\n\n\tfree_kbytes = K(global_zone_page_state(NR_FREE_PAGES));\n\n\tsysctl_admin_reserve_kbytes = min(free_kbytes / 32, 1UL << 13);\n\treturn 0;\n}\nsubsys_initcall(init_admin_reserve);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}