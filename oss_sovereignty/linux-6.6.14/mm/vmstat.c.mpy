{
  "module_name": "vmstat.c",
  "hash_id": "73c64c6309fdca3ae9e66877d046f380ea0c61d40dd01d46a5742a8d496768bd",
  "original_prompt": "Ingested from linux-6.6.14/mm/vmstat.c",
  "human_readable_source": "\n \n#include <linux/fs.h>\n#include <linux/mm.h>\n#include <linux/err.h>\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/cpu.h>\n#include <linux/cpumask.h>\n#include <linux/vmstat.h>\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <linux/debugfs.h>\n#include <linux/sched.h>\n#include <linux/math64.h>\n#include <linux/writeback.h>\n#include <linux/compaction.h>\n#include <linux/mm_inline.h>\n#include <linux/page_owner.h>\n#include <linux/sched/isolation.h>\n\n#include \"internal.h\"\n\n#ifdef CONFIG_NUMA\nint sysctl_vm_numa_stat = ENABLE_NUMA_STAT;\n\n \nstatic void zero_zone_numa_counters(struct zone *zone)\n{\n\tint item, cpu;\n\n\tfor (item = 0; item < NR_VM_NUMA_EVENT_ITEMS; item++) {\n\t\tatomic_long_set(&zone->vm_numa_event[item], 0);\n\t\tfor_each_online_cpu(cpu) {\n\t\t\tper_cpu_ptr(zone->per_cpu_zonestats, cpu)->vm_numa_event[item]\n\t\t\t\t\t\t= 0;\n\t\t}\n\t}\n}\n\n \nstatic void zero_zones_numa_counters(void)\n{\n\tstruct zone *zone;\n\n\tfor_each_populated_zone(zone)\n\t\tzero_zone_numa_counters(zone);\n}\n\n \nstatic void zero_global_numa_counters(void)\n{\n\tint item;\n\n\tfor (item = 0; item < NR_VM_NUMA_EVENT_ITEMS; item++)\n\t\tatomic_long_set(&vm_numa_event[item], 0);\n}\n\nstatic void invalid_numa_statistics(void)\n{\n\tzero_zones_numa_counters();\n\tzero_global_numa_counters();\n}\n\nstatic DEFINE_MUTEX(vm_numa_stat_lock);\n\nint sysctl_vm_numa_stat_handler(struct ctl_table *table, int write,\n\t\tvoid *buffer, size_t *length, loff_t *ppos)\n{\n\tint ret, oldval;\n\n\tmutex_lock(&vm_numa_stat_lock);\n\tif (write)\n\t\toldval = sysctl_vm_numa_stat;\n\tret = proc_dointvec_minmax(table, write, buffer, length, ppos);\n\tif (ret || !write)\n\t\tgoto out;\n\n\tif (oldval == sysctl_vm_numa_stat)\n\t\tgoto out;\n\telse if (sysctl_vm_numa_stat == ENABLE_NUMA_STAT) {\n\t\tstatic_branch_enable(&vm_numa_stat_key);\n\t\tpr_info(\"enable numa statistics\\n\");\n\t} else {\n\t\tstatic_branch_disable(&vm_numa_stat_key);\n\t\tinvalid_numa_statistics();\n\t\tpr_info(\"disable numa statistics, and clear numa counters\\n\");\n\t}\n\nout:\n\tmutex_unlock(&vm_numa_stat_lock);\n\treturn ret;\n}\n#endif\n\n#ifdef CONFIG_VM_EVENT_COUNTERS\nDEFINE_PER_CPU(struct vm_event_state, vm_event_states) = {{0}};\nEXPORT_PER_CPU_SYMBOL(vm_event_states);\n\nstatic void sum_vm_events(unsigned long *ret)\n{\n\tint cpu;\n\tint i;\n\n\tmemset(ret, 0, NR_VM_EVENT_ITEMS * sizeof(unsigned long));\n\n\tfor_each_online_cpu(cpu) {\n\t\tstruct vm_event_state *this = &per_cpu(vm_event_states, cpu);\n\n\t\tfor (i = 0; i < NR_VM_EVENT_ITEMS; i++)\n\t\t\tret[i] += this->event[i];\n\t}\n}\n\n \nvoid all_vm_events(unsigned long *ret)\n{\n\tcpus_read_lock();\n\tsum_vm_events(ret);\n\tcpus_read_unlock();\n}\nEXPORT_SYMBOL_GPL(all_vm_events);\n\n \nvoid vm_events_fold_cpu(int cpu)\n{\n\tstruct vm_event_state *fold_state = &per_cpu(vm_event_states, cpu);\n\tint i;\n\n\tfor (i = 0; i < NR_VM_EVENT_ITEMS; i++) {\n\t\tcount_vm_events(i, fold_state->event[i]);\n\t\tfold_state->event[i] = 0;\n\t}\n}\n\n#endif  \n\n \natomic_long_t vm_zone_stat[NR_VM_ZONE_STAT_ITEMS] __cacheline_aligned_in_smp;\natomic_long_t vm_node_stat[NR_VM_NODE_STAT_ITEMS] __cacheline_aligned_in_smp;\natomic_long_t vm_numa_event[NR_VM_NUMA_EVENT_ITEMS] __cacheline_aligned_in_smp;\nEXPORT_SYMBOL(vm_zone_stat);\nEXPORT_SYMBOL(vm_node_stat);\n\n#ifdef CONFIG_NUMA\nstatic void fold_vm_zone_numa_events(struct zone *zone)\n{\n\tunsigned long zone_numa_events[NR_VM_NUMA_EVENT_ITEMS] = { 0, };\n\tint cpu;\n\tenum numa_stat_item item;\n\n\tfor_each_online_cpu(cpu) {\n\t\tstruct per_cpu_zonestat *pzstats;\n\n\t\tpzstats = per_cpu_ptr(zone->per_cpu_zonestats, cpu);\n\t\tfor (item = 0; item < NR_VM_NUMA_EVENT_ITEMS; item++)\n\t\t\tzone_numa_events[item] += xchg(&pzstats->vm_numa_event[item], 0);\n\t}\n\n\tfor (item = 0; item < NR_VM_NUMA_EVENT_ITEMS; item++)\n\t\tzone_numa_event_add(zone_numa_events[item], zone, item);\n}\n\nvoid fold_vm_numa_events(void)\n{\n\tstruct zone *zone;\n\n\tfor_each_populated_zone(zone)\n\t\tfold_vm_zone_numa_events(zone);\n}\n#endif\n\n#ifdef CONFIG_SMP\n\nint calculate_pressure_threshold(struct zone *zone)\n{\n\tint threshold;\n\tint watermark_distance;\n\n\t \n\twatermark_distance = low_wmark_pages(zone) - min_wmark_pages(zone);\n\tthreshold = max(1, (int)(watermark_distance / num_online_cpus()));\n\n\t \n\tthreshold = min(125, threshold);\n\n\treturn threshold;\n}\n\nint calculate_normal_threshold(struct zone *zone)\n{\n\tint threshold;\n\tint mem;\t \n\n\t \n\n\tmem = zone_managed_pages(zone) >> (27 - PAGE_SHIFT);\n\n\tthreshold = 2 * fls(num_online_cpus()) * (1 + fls(mem));\n\n\t \n\tthreshold = min(125, threshold);\n\n\treturn threshold;\n}\n\n \nvoid refresh_zone_stat_thresholds(void)\n{\n\tstruct pglist_data *pgdat;\n\tstruct zone *zone;\n\tint cpu;\n\tint threshold;\n\n\t \n\tfor_each_online_pgdat(pgdat) {\n\t\tfor_each_online_cpu(cpu) {\n\t\t\tper_cpu_ptr(pgdat->per_cpu_nodestats, cpu)->stat_threshold = 0;\n\t\t}\n\t}\n\n\tfor_each_populated_zone(zone) {\n\t\tstruct pglist_data *pgdat = zone->zone_pgdat;\n\t\tunsigned long max_drift, tolerate_drift;\n\n\t\tthreshold = calculate_normal_threshold(zone);\n\n\t\tfor_each_online_cpu(cpu) {\n\t\t\tint pgdat_threshold;\n\n\t\t\tper_cpu_ptr(zone->per_cpu_zonestats, cpu)->stat_threshold\n\t\t\t\t\t\t\t= threshold;\n\n\t\t\t \n\t\t\tpgdat_threshold = per_cpu_ptr(pgdat->per_cpu_nodestats, cpu)->stat_threshold;\n\t\t\tper_cpu_ptr(pgdat->per_cpu_nodestats, cpu)->stat_threshold\n\t\t\t\t= max(threshold, pgdat_threshold);\n\t\t}\n\n\t\t \n\t\ttolerate_drift = low_wmark_pages(zone) - min_wmark_pages(zone);\n\t\tmax_drift = num_online_cpus() * threshold;\n\t\tif (max_drift > tolerate_drift)\n\t\t\tzone->percpu_drift_mark = high_wmark_pages(zone) +\n\t\t\t\t\tmax_drift;\n\t}\n}\n\nvoid set_pgdat_percpu_threshold(pg_data_t *pgdat,\n\t\t\t\tint (*calculate_pressure)(struct zone *))\n{\n\tstruct zone *zone;\n\tint cpu;\n\tint threshold;\n\tint i;\n\n\tfor (i = 0; i < pgdat->nr_zones; i++) {\n\t\tzone = &pgdat->node_zones[i];\n\t\tif (!zone->percpu_drift_mark)\n\t\t\tcontinue;\n\n\t\tthreshold = (*calculate_pressure)(zone);\n\t\tfor_each_online_cpu(cpu)\n\t\t\tper_cpu_ptr(zone->per_cpu_zonestats, cpu)->stat_threshold\n\t\t\t\t\t\t\t= threshold;\n\t}\n}\n\n \nvoid __mod_zone_page_state(struct zone *zone, enum zone_stat_item item,\n\t\t\t   long delta)\n{\n\tstruct per_cpu_zonestat __percpu *pcp = zone->per_cpu_zonestats;\n\ts8 __percpu *p = pcp->vm_stat_diff + item;\n\tlong x;\n\tlong t;\n\n\t \n\tpreempt_disable_nested();\n\n\tx = delta + __this_cpu_read(*p);\n\n\tt = __this_cpu_read(pcp->stat_threshold);\n\n\tif (unlikely(abs(x) > t)) {\n\t\tzone_page_state_add(x, zone, item);\n\t\tx = 0;\n\t}\n\t__this_cpu_write(*p, x);\n\n\tpreempt_enable_nested();\n}\nEXPORT_SYMBOL(__mod_zone_page_state);\n\nvoid __mod_node_page_state(struct pglist_data *pgdat, enum node_stat_item item,\n\t\t\t\tlong delta)\n{\n\tstruct per_cpu_nodestat __percpu *pcp = pgdat->per_cpu_nodestats;\n\ts8 __percpu *p = pcp->vm_node_stat_diff + item;\n\tlong x;\n\tlong t;\n\n\tif (vmstat_item_in_bytes(item)) {\n\t\t \n\t\tVM_WARN_ON_ONCE(delta & (PAGE_SIZE - 1));\n\t\tdelta >>= PAGE_SHIFT;\n\t}\n\n\t \n\tpreempt_disable_nested();\n\n\tx = delta + __this_cpu_read(*p);\n\n\tt = __this_cpu_read(pcp->stat_threshold);\n\n\tif (unlikely(abs(x) > t)) {\n\t\tnode_page_state_add(x, pgdat, item);\n\t\tx = 0;\n\t}\n\t__this_cpu_write(*p, x);\n\n\tpreempt_enable_nested();\n}\nEXPORT_SYMBOL(__mod_node_page_state);\n\n \nvoid __inc_zone_state(struct zone *zone, enum zone_stat_item item)\n{\n\tstruct per_cpu_zonestat __percpu *pcp = zone->per_cpu_zonestats;\n\ts8 __percpu *p = pcp->vm_stat_diff + item;\n\ts8 v, t;\n\n\t \n\tpreempt_disable_nested();\n\n\tv = __this_cpu_inc_return(*p);\n\tt = __this_cpu_read(pcp->stat_threshold);\n\tif (unlikely(v > t)) {\n\t\ts8 overstep = t >> 1;\n\n\t\tzone_page_state_add(v + overstep, zone, item);\n\t\t__this_cpu_write(*p, -overstep);\n\t}\n\n\tpreempt_enable_nested();\n}\n\nvoid __inc_node_state(struct pglist_data *pgdat, enum node_stat_item item)\n{\n\tstruct per_cpu_nodestat __percpu *pcp = pgdat->per_cpu_nodestats;\n\ts8 __percpu *p = pcp->vm_node_stat_diff + item;\n\ts8 v, t;\n\n\tVM_WARN_ON_ONCE(vmstat_item_in_bytes(item));\n\n\t \n\tpreempt_disable_nested();\n\n\tv = __this_cpu_inc_return(*p);\n\tt = __this_cpu_read(pcp->stat_threshold);\n\tif (unlikely(v > t)) {\n\t\ts8 overstep = t >> 1;\n\n\t\tnode_page_state_add(v + overstep, pgdat, item);\n\t\t__this_cpu_write(*p, -overstep);\n\t}\n\n\tpreempt_enable_nested();\n}\n\nvoid __inc_zone_page_state(struct page *page, enum zone_stat_item item)\n{\n\t__inc_zone_state(page_zone(page), item);\n}\nEXPORT_SYMBOL(__inc_zone_page_state);\n\nvoid __inc_node_page_state(struct page *page, enum node_stat_item item)\n{\n\t__inc_node_state(page_pgdat(page), item);\n}\nEXPORT_SYMBOL(__inc_node_page_state);\n\nvoid __dec_zone_state(struct zone *zone, enum zone_stat_item item)\n{\n\tstruct per_cpu_zonestat __percpu *pcp = zone->per_cpu_zonestats;\n\ts8 __percpu *p = pcp->vm_stat_diff + item;\n\ts8 v, t;\n\n\t \n\tpreempt_disable_nested();\n\n\tv = __this_cpu_dec_return(*p);\n\tt = __this_cpu_read(pcp->stat_threshold);\n\tif (unlikely(v < - t)) {\n\t\ts8 overstep = t >> 1;\n\n\t\tzone_page_state_add(v - overstep, zone, item);\n\t\t__this_cpu_write(*p, overstep);\n\t}\n\n\tpreempt_enable_nested();\n}\n\nvoid __dec_node_state(struct pglist_data *pgdat, enum node_stat_item item)\n{\n\tstruct per_cpu_nodestat __percpu *pcp = pgdat->per_cpu_nodestats;\n\ts8 __percpu *p = pcp->vm_node_stat_diff + item;\n\ts8 v, t;\n\n\tVM_WARN_ON_ONCE(vmstat_item_in_bytes(item));\n\n\t \n\tpreempt_disable_nested();\n\n\tv = __this_cpu_dec_return(*p);\n\tt = __this_cpu_read(pcp->stat_threshold);\n\tif (unlikely(v < - t)) {\n\t\ts8 overstep = t >> 1;\n\n\t\tnode_page_state_add(v - overstep, pgdat, item);\n\t\t__this_cpu_write(*p, overstep);\n\t}\n\n\tpreempt_enable_nested();\n}\n\nvoid __dec_zone_page_state(struct page *page, enum zone_stat_item item)\n{\n\t__dec_zone_state(page_zone(page), item);\n}\nEXPORT_SYMBOL(__dec_zone_page_state);\n\nvoid __dec_node_page_state(struct page *page, enum node_stat_item item)\n{\n\t__dec_node_state(page_pgdat(page), item);\n}\nEXPORT_SYMBOL(__dec_node_page_state);\n\n#ifdef CONFIG_HAVE_CMPXCHG_LOCAL\n \nstatic inline void mod_zone_state(struct zone *zone,\n       enum zone_stat_item item, long delta, int overstep_mode)\n{\n\tstruct per_cpu_zonestat __percpu *pcp = zone->per_cpu_zonestats;\n\ts8 __percpu *p = pcp->vm_stat_diff + item;\n\tlong o, n, t, z;\n\n\tdo {\n\t\tz = 0;   \n\n\t\t \n\t\tt = this_cpu_read(pcp->stat_threshold);\n\n\t\to = this_cpu_read(*p);\n\t\tn = delta + o;\n\n\t\tif (abs(n) > t) {\n\t\t\tint os = overstep_mode * (t >> 1) ;\n\n\t\t\t \n\t\t\tz = n + os;\n\t\t\tn = -os;\n\t\t}\n\t} while (this_cpu_cmpxchg(*p, o, n) != o);\n\n\tif (z)\n\t\tzone_page_state_add(z, zone, item);\n}\n\nvoid mod_zone_page_state(struct zone *zone, enum zone_stat_item item,\n\t\t\t long delta)\n{\n\tmod_zone_state(zone, item, delta, 0);\n}\nEXPORT_SYMBOL(mod_zone_page_state);\n\nvoid inc_zone_page_state(struct page *page, enum zone_stat_item item)\n{\n\tmod_zone_state(page_zone(page), item, 1, 1);\n}\nEXPORT_SYMBOL(inc_zone_page_state);\n\nvoid dec_zone_page_state(struct page *page, enum zone_stat_item item)\n{\n\tmod_zone_state(page_zone(page), item, -1, -1);\n}\nEXPORT_SYMBOL(dec_zone_page_state);\n\nstatic inline void mod_node_state(struct pglist_data *pgdat,\n       enum node_stat_item item, int delta, int overstep_mode)\n{\n\tstruct per_cpu_nodestat __percpu *pcp = pgdat->per_cpu_nodestats;\n\ts8 __percpu *p = pcp->vm_node_stat_diff + item;\n\tlong o, n, t, z;\n\n\tif (vmstat_item_in_bytes(item)) {\n\t\t \n\t\tVM_WARN_ON_ONCE(delta & (PAGE_SIZE - 1));\n\t\tdelta >>= PAGE_SHIFT;\n\t}\n\n\tdo {\n\t\tz = 0;   \n\n\t\t \n\t\tt = this_cpu_read(pcp->stat_threshold);\n\n\t\to = this_cpu_read(*p);\n\t\tn = delta + o;\n\n\t\tif (abs(n) > t) {\n\t\t\tint os = overstep_mode * (t >> 1) ;\n\n\t\t\t \n\t\t\tz = n + os;\n\t\t\tn = -os;\n\t\t}\n\t} while (this_cpu_cmpxchg(*p, o, n) != o);\n\n\tif (z)\n\t\tnode_page_state_add(z, pgdat, item);\n}\n\nvoid mod_node_page_state(struct pglist_data *pgdat, enum node_stat_item item,\n\t\t\t\t\tlong delta)\n{\n\tmod_node_state(pgdat, item, delta, 0);\n}\nEXPORT_SYMBOL(mod_node_page_state);\n\nvoid inc_node_state(struct pglist_data *pgdat, enum node_stat_item item)\n{\n\tmod_node_state(pgdat, item, 1, 1);\n}\n\nvoid inc_node_page_state(struct page *page, enum node_stat_item item)\n{\n\tmod_node_state(page_pgdat(page), item, 1, 1);\n}\nEXPORT_SYMBOL(inc_node_page_state);\n\nvoid dec_node_page_state(struct page *page, enum node_stat_item item)\n{\n\tmod_node_state(page_pgdat(page), item, -1, -1);\n}\nEXPORT_SYMBOL(dec_node_page_state);\n#else\n \nvoid mod_zone_page_state(struct zone *zone, enum zone_stat_item item,\n\t\t\t long delta)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\t__mod_zone_page_state(zone, item, delta);\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL(mod_zone_page_state);\n\nvoid inc_zone_page_state(struct page *page, enum zone_stat_item item)\n{\n\tunsigned long flags;\n\tstruct zone *zone;\n\n\tzone = page_zone(page);\n\tlocal_irq_save(flags);\n\t__inc_zone_state(zone, item);\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL(inc_zone_page_state);\n\nvoid dec_zone_page_state(struct page *page, enum zone_stat_item item)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\t__dec_zone_page_state(page, item);\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL(dec_zone_page_state);\n\nvoid inc_node_state(struct pglist_data *pgdat, enum node_stat_item item)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\t__inc_node_state(pgdat, item);\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL(inc_node_state);\n\nvoid mod_node_page_state(struct pglist_data *pgdat, enum node_stat_item item,\n\t\t\t\t\tlong delta)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\t__mod_node_page_state(pgdat, item, delta);\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL(mod_node_page_state);\n\nvoid inc_node_page_state(struct page *page, enum node_stat_item item)\n{\n\tunsigned long flags;\n\tstruct pglist_data *pgdat;\n\n\tpgdat = page_pgdat(page);\n\tlocal_irq_save(flags);\n\t__inc_node_state(pgdat, item);\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL(inc_node_page_state);\n\nvoid dec_node_page_state(struct page *page, enum node_stat_item item)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\t__dec_node_page_state(page, item);\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL(dec_node_page_state);\n#endif\n\n \nstatic int fold_diff(int *zone_diff, int *node_diff)\n{\n\tint i;\n\tint changes = 0;\n\n\tfor (i = 0; i < NR_VM_ZONE_STAT_ITEMS; i++)\n\t\tif (zone_diff[i]) {\n\t\t\tatomic_long_add(zone_diff[i], &vm_zone_stat[i]);\n\t\t\tchanges++;\n\t}\n\n\tfor (i = 0; i < NR_VM_NODE_STAT_ITEMS; i++)\n\t\tif (node_diff[i]) {\n\t\t\tatomic_long_add(node_diff[i], &vm_node_stat[i]);\n\t\t\tchanges++;\n\t}\n\treturn changes;\n}\n\n \nstatic int refresh_cpu_vm_stats(bool do_pagesets)\n{\n\tstruct pglist_data *pgdat;\n\tstruct zone *zone;\n\tint i;\n\tint global_zone_diff[NR_VM_ZONE_STAT_ITEMS] = { 0, };\n\tint global_node_diff[NR_VM_NODE_STAT_ITEMS] = { 0, };\n\tint changes = 0;\n\n\tfor_each_populated_zone(zone) {\n\t\tstruct per_cpu_zonestat __percpu *pzstats = zone->per_cpu_zonestats;\n#ifdef CONFIG_NUMA\n\t\tstruct per_cpu_pages __percpu *pcp = zone->per_cpu_pageset;\n#endif\n\n\t\tfor (i = 0; i < NR_VM_ZONE_STAT_ITEMS; i++) {\n\t\t\tint v;\n\n\t\t\tv = this_cpu_xchg(pzstats->vm_stat_diff[i], 0);\n\t\t\tif (v) {\n\n\t\t\t\tatomic_long_add(v, &zone->vm_stat[i]);\n\t\t\t\tglobal_zone_diff[i] += v;\n#ifdef CONFIG_NUMA\n\t\t\t\t \n\t\t\t\t__this_cpu_write(pcp->expire, 3);\n#endif\n\t\t\t}\n\t\t}\n#ifdef CONFIG_NUMA\n\n\t\tif (do_pagesets) {\n\t\t\tcond_resched();\n\t\t\t \n\t\t\tif (!__this_cpu_read(pcp->expire) ||\n\t\t\t       !__this_cpu_read(pcp->count))\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tif (zone_to_nid(zone) == numa_node_id()) {\n\t\t\t\t__this_cpu_write(pcp->expire, 0);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (__this_cpu_dec_return(pcp->expire))\n\t\t\t\tcontinue;\n\n\t\t\tif (__this_cpu_read(pcp->count)) {\n\t\t\t\tdrain_zone_pages(zone, this_cpu_ptr(pcp));\n\t\t\t\tchanges++;\n\t\t\t}\n\t\t}\n#endif\n\t}\n\n\tfor_each_online_pgdat(pgdat) {\n\t\tstruct per_cpu_nodestat __percpu *p = pgdat->per_cpu_nodestats;\n\n\t\tfor (i = 0; i < NR_VM_NODE_STAT_ITEMS; i++) {\n\t\t\tint v;\n\n\t\t\tv = this_cpu_xchg(p->vm_node_stat_diff[i], 0);\n\t\t\tif (v) {\n\t\t\t\tatomic_long_add(v, &pgdat->vm_stat[i]);\n\t\t\t\tglobal_node_diff[i] += v;\n\t\t\t}\n\t\t}\n\t}\n\n\tchanges += fold_diff(global_zone_diff, global_node_diff);\n\treturn changes;\n}\n\n \nvoid cpu_vm_stats_fold(int cpu)\n{\n\tstruct pglist_data *pgdat;\n\tstruct zone *zone;\n\tint i;\n\tint global_zone_diff[NR_VM_ZONE_STAT_ITEMS] = { 0, };\n\tint global_node_diff[NR_VM_NODE_STAT_ITEMS] = { 0, };\n\n\tfor_each_populated_zone(zone) {\n\t\tstruct per_cpu_zonestat *pzstats;\n\n\t\tpzstats = per_cpu_ptr(zone->per_cpu_zonestats, cpu);\n\n\t\tfor (i = 0; i < NR_VM_ZONE_STAT_ITEMS; i++) {\n\t\t\tif (pzstats->vm_stat_diff[i]) {\n\t\t\t\tint v;\n\n\t\t\t\tv = pzstats->vm_stat_diff[i];\n\t\t\t\tpzstats->vm_stat_diff[i] = 0;\n\t\t\t\tatomic_long_add(v, &zone->vm_stat[i]);\n\t\t\t\tglobal_zone_diff[i] += v;\n\t\t\t}\n\t\t}\n#ifdef CONFIG_NUMA\n\t\tfor (i = 0; i < NR_VM_NUMA_EVENT_ITEMS; i++) {\n\t\t\tif (pzstats->vm_numa_event[i]) {\n\t\t\t\tunsigned long v;\n\n\t\t\t\tv = pzstats->vm_numa_event[i];\n\t\t\t\tpzstats->vm_numa_event[i] = 0;\n\t\t\t\tzone_numa_event_add(v, zone, i);\n\t\t\t}\n\t\t}\n#endif\n\t}\n\n\tfor_each_online_pgdat(pgdat) {\n\t\tstruct per_cpu_nodestat *p;\n\n\t\tp = per_cpu_ptr(pgdat->per_cpu_nodestats, cpu);\n\n\t\tfor (i = 0; i < NR_VM_NODE_STAT_ITEMS; i++)\n\t\t\tif (p->vm_node_stat_diff[i]) {\n\t\t\t\tint v;\n\n\t\t\t\tv = p->vm_node_stat_diff[i];\n\t\t\t\tp->vm_node_stat_diff[i] = 0;\n\t\t\t\tatomic_long_add(v, &pgdat->vm_stat[i]);\n\t\t\t\tglobal_node_diff[i] += v;\n\t\t\t}\n\t}\n\n\tfold_diff(global_zone_diff, global_node_diff);\n}\n\n \nvoid drain_zonestat(struct zone *zone, struct per_cpu_zonestat *pzstats)\n{\n\tunsigned long v;\n\tint i;\n\n\tfor (i = 0; i < NR_VM_ZONE_STAT_ITEMS; i++) {\n\t\tif (pzstats->vm_stat_diff[i]) {\n\t\t\tv = pzstats->vm_stat_diff[i];\n\t\t\tpzstats->vm_stat_diff[i] = 0;\n\t\t\tzone_page_state_add(v, zone, i);\n\t\t}\n\t}\n\n#ifdef CONFIG_NUMA\n\tfor (i = 0; i < NR_VM_NUMA_EVENT_ITEMS; i++) {\n\t\tif (pzstats->vm_numa_event[i]) {\n\t\t\tv = pzstats->vm_numa_event[i];\n\t\t\tpzstats->vm_numa_event[i] = 0;\n\t\t\tzone_numa_event_add(v, zone, i);\n\t\t}\n\t}\n#endif\n}\n#endif\n\n#ifdef CONFIG_NUMA\n \nunsigned long sum_zone_node_page_state(int node,\n\t\t\t\t enum zone_stat_item item)\n{\n\tstruct zone *zones = NODE_DATA(node)->node_zones;\n\tint i;\n\tunsigned long count = 0;\n\n\tfor (i = 0; i < MAX_NR_ZONES; i++)\n\t\tcount += zone_page_state(zones + i, item);\n\n\treturn count;\n}\n\n \nunsigned long sum_zone_numa_event_state(int node,\n\t\t\t\t enum numa_stat_item item)\n{\n\tstruct zone *zones = NODE_DATA(node)->node_zones;\n\tunsigned long count = 0;\n\tint i;\n\n\tfor (i = 0; i < MAX_NR_ZONES; i++)\n\t\tcount += zone_numa_event_state(zones + i, item);\n\n\treturn count;\n}\n\n \nunsigned long node_page_state_pages(struct pglist_data *pgdat,\n\t\t\t\t    enum node_stat_item item)\n{\n\tlong x = atomic_long_read(&pgdat->vm_stat[item]);\n#ifdef CONFIG_SMP\n\tif (x < 0)\n\t\tx = 0;\n#endif\n\treturn x;\n}\n\nunsigned long node_page_state(struct pglist_data *pgdat,\n\t\t\t      enum node_stat_item item)\n{\n\tVM_WARN_ON_ONCE(vmstat_item_in_bytes(item));\n\n\treturn node_page_state_pages(pgdat, item);\n}\n#endif\n\n#ifdef CONFIG_COMPACTION\n\nstruct contig_page_info {\n\tunsigned long free_pages;\n\tunsigned long free_blocks_total;\n\tunsigned long free_blocks_suitable;\n};\n\n \nstatic void fill_contig_page_info(struct zone *zone,\n\t\t\t\tunsigned int suitable_order,\n\t\t\t\tstruct contig_page_info *info)\n{\n\tunsigned int order;\n\n\tinfo->free_pages = 0;\n\tinfo->free_blocks_total = 0;\n\tinfo->free_blocks_suitable = 0;\n\n\tfor (order = 0; order <= MAX_ORDER; order++) {\n\t\tunsigned long blocks;\n\n\t\t \n\t\tblocks = data_race(zone->free_area[order].nr_free);\n\t\tinfo->free_blocks_total += blocks;\n\n\t\t \n\t\tinfo->free_pages += blocks << order;\n\n\t\t \n\t\tif (order >= suitable_order)\n\t\t\tinfo->free_blocks_suitable += blocks <<\n\t\t\t\t\t\t(order - suitable_order);\n\t}\n}\n\n \nstatic int __fragmentation_index(unsigned int order, struct contig_page_info *info)\n{\n\tunsigned long requested = 1UL << order;\n\n\tif (WARN_ON_ONCE(order > MAX_ORDER))\n\t\treturn 0;\n\n\tif (!info->free_blocks_total)\n\t\treturn 0;\n\n\t \n\tif (info->free_blocks_suitable)\n\t\treturn -1000;\n\n\t \n\treturn 1000 - div_u64( (1000+(div_u64(info->free_pages * 1000ULL, requested))), info->free_blocks_total);\n}\n\n \nunsigned int extfrag_for_order(struct zone *zone, unsigned int order)\n{\n\tstruct contig_page_info info;\n\n\tfill_contig_page_info(zone, order, &info);\n\tif (info.free_pages == 0)\n\t\treturn 0;\n\n\treturn div_u64((info.free_pages -\n\t\t\t(info.free_blocks_suitable << order)) * 100,\n\t\t\tinfo.free_pages);\n}\n\n \nint fragmentation_index(struct zone *zone, unsigned int order)\n{\n\tstruct contig_page_info info;\n\n\tfill_contig_page_info(zone, order, &info);\n\treturn __fragmentation_index(order, &info);\n}\n#endif\n\n#if defined(CONFIG_PROC_FS) || defined(CONFIG_SYSFS) || \\\n    defined(CONFIG_NUMA) || defined(CONFIG_MEMCG)\n#ifdef CONFIG_ZONE_DMA\n#define TEXT_FOR_DMA(xx) xx \"_dma\",\n#else\n#define TEXT_FOR_DMA(xx)\n#endif\n\n#ifdef CONFIG_ZONE_DMA32\n#define TEXT_FOR_DMA32(xx) xx \"_dma32\",\n#else\n#define TEXT_FOR_DMA32(xx)\n#endif\n\n#ifdef CONFIG_HIGHMEM\n#define TEXT_FOR_HIGHMEM(xx) xx \"_high\",\n#else\n#define TEXT_FOR_HIGHMEM(xx)\n#endif\n\n#ifdef CONFIG_ZONE_DEVICE\n#define TEXT_FOR_DEVICE(xx) xx \"_device\",\n#else\n#define TEXT_FOR_DEVICE(xx)\n#endif\n\n#define TEXTS_FOR_ZONES(xx) TEXT_FOR_DMA(xx) TEXT_FOR_DMA32(xx) xx \"_normal\", \\\n\t\t\t\t\tTEXT_FOR_HIGHMEM(xx) xx \"_movable\", \\\n\t\t\t\t\tTEXT_FOR_DEVICE(xx)\n\nconst char * const vmstat_text[] = {\n\t \n\t\"nr_free_pages\",\n\t\"nr_zone_inactive_anon\",\n\t\"nr_zone_active_anon\",\n\t\"nr_zone_inactive_file\",\n\t\"nr_zone_active_file\",\n\t\"nr_zone_unevictable\",\n\t\"nr_zone_write_pending\",\n\t\"nr_mlock\",\n\t\"nr_bounce\",\n#if IS_ENABLED(CONFIG_ZSMALLOC)\n\t\"nr_zspages\",\n#endif\n\t\"nr_free_cma\",\n#ifdef CONFIG_UNACCEPTED_MEMORY\n\t\"nr_unaccepted\",\n#endif\n\n\t \n#ifdef CONFIG_NUMA\n\t\"numa_hit\",\n\t\"numa_miss\",\n\t\"numa_foreign\",\n\t\"numa_interleave\",\n\t\"numa_local\",\n\t\"numa_other\",\n#endif\n\n\t \n\t\"nr_inactive_anon\",\n\t\"nr_active_anon\",\n\t\"nr_inactive_file\",\n\t\"nr_active_file\",\n\t\"nr_unevictable\",\n\t\"nr_slab_reclaimable\",\n\t\"nr_slab_unreclaimable\",\n\t\"nr_isolated_anon\",\n\t\"nr_isolated_file\",\n\t\"workingset_nodes\",\n\t\"workingset_refault_anon\",\n\t\"workingset_refault_file\",\n\t\"workingset_activate_anon\",\n\t\"workingset_activate_file\",\n\t\"workingset_restore_anon\",\n\t\"workingset_restore_file\",\n\t\"workingset_nodereclaim\",\n\t\"nr_anon_pages\",\n\t\"nr_mapped\",\n\t\"nr_file_pages\",\n\t\"nr_dirty\",\n\t\"nr_writeback\",\n\t\"nr_writeback_temp\",\n\t\"nr_shmem\",\n\t\"nr_shmem_hugepages\",\n\t\"nr_shmem_pmdmapped\",\n\t\"nr_file_hugepages\",\n\t\"nr_file_pmdmapped\",\n\t\"nr_anon_transparent_hugepages\",\n\t\"nr_vmscan_write\",\n\t\"nr_vmscan_immediate_reclaim\",\n\t\"nr_dirtied\",\n\t\"nr_written\",\n\t\"nr_throttled_written\",\n\t\"nr_kernel_misc_reclaimable\",\n\t\"nr_foll_pin_acquired\",\n\t\"nr_foll_pin_released\",\n\t\"nr_kernel_stack\",\n#if IS_ENABLED(CONFIG_SHADOW_CALL_STACK)\n\t\"nr_shadow_call_stack\",\n#endif\n\t\"nr_page_table_pages\",\n\t\"nr_sec_page_table_pages\",\n#ifdef CONFIG_SWAP\n\t\"nr_swapcached\",\n#endif\n#ifdef CONFIG_NUMA_BALANCING\n\t\"pgpromote_success\",\n\t\"pgpromote_candidate\",\n#endif\n\n\t \n\t\"nr_dirty_threshold\",\n\t\"nr_dirty_background_threshold\",\n\n#if defined(CONFIG_VM_EVENT_COUNTERS) || defined(CONFIG_MEMCG)\n\t \n\t\"pgpgin\",\n\t\"pgpgout\",\n\t\"pswpin\",\n\t\"pswpout\",\n\n\tTEXTS_FOR_ZONES(\"pgalloc\")\n\tTEXTS_FOR_ZONES(\"allocstall\")\n\tTEXTS_FOR_ZONES(\"pgskip\")\n\n\t\"pgfree\",\n\t\"pgactivate\",\n\t\"pgdeactivate\",\n\t\"pglazyfree\",\n\n\t\"pgfault\",\n\t\"pgmajfault\",\n\t\"pglazyfreed\",\n\n\t\"pgrefill\",\n\t\"pgreuse\",\n\t\"pgsteal_kswapd\",\n\t\"pgsteal_direct\",\n\t\"pgsteal_khugepaged\",\n\t\"pgdemote_kswapd\",\n\t\"pgdemote_direct\",\n\t\"pgdemote_khugepaged\",\n\t\"pgscan_kswapd\",\n\t\"pgscan_direct\",\n\t\"pgscan_khugepaged\",\n\t\"pgscan_direct_throttle\",\n\t\"pgscan_anon\",\n\t\"pgscan_file\",\n\t\"pgsteal_anon\",\n\t\"pgsteal_file\",\n\n#ifdef CONFIG_NUMA\n\t\"zone_reclaim_failed\",\n#endif\n\t\"pginodesteal\",\n\t\"slabs_scanned\",\n\t\"kswapd_inodesteal\",\n\t\"kswapd_low_wmark_hit_quickly\",\n\t\"kswapd_high_wmark_hit_quickly\",\n\t\"pageoutrun\",\n\n\t\"pgrotated\",\n\n\t\"drop_pagecache\",\n\t\"drop_slab\",\n\t\"oom_kill\",\n\n#ifdef CONFIG_NUMA_BALANCING\n\t\"numa_pte_updates\",\n\t\"numa_huge_pte_updates\",\n\t\"numa_hint_faults\",\n\t\"numa_hint_faults_local\",\n\t\"numa_pages_migrated\",\n#endif\n#ifdef CONFIG_MIGRATION\n\t\"pgmigrate_success\",\n\t\"pgmigrate_fail\",\n\t\"thp_migration_success\",\n\t\"thp_migration_fail\",\n\t\"thp_migration_split\",\n#endif\n#ifdef CONFIG_COMPACTION\n\t\"compact_migrate_scanned\",\n\t\"compact_free_scanned\",\n\t\"compact_isolated\",\n\t\"compact_stall\",\n\t\"compact_fail\",\n\t\"compact_success\",\n\t\"compact_daemon_wake\",\n\t\"compact_daemon_migrate_scanned\",\n\t\"compact_daemon_free_scanned\",\n#endif\n\n#ifdef CONFIG_HUGETLB_PAGE\n\t\"htlb_buddy_alloc_success\",\n\t\"htlb_buddy_alloc_fail\",\n#endif\n#ifdef CONFIG_CMA\n\t\"cma_alloc_success\",\n\t\"cma_alloc_fail\",\n#endif\n\t\"unevictable_pgs_culled\",\n\t\"unevictable_pgs_scanned\",\n\t\"unevictable_pgs_rescued\",\n\t\"unevictable_pgs_mlocked\",\n\t\"unevictable_pgs_munlocked\",\n\t\"unevictable_pgs_cleared\",\n\t\"unevictable_pgs_stranded\",\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\t\"thp_fault_alloc\",\n\t\"thp_fault_fallback\",\n\t\"thp_fault_fallback_charge\",\n\t\"thp_collapse_alloc\",\n\t\"thp_collapse_alloc_failed\",\n\t\"thp_file_alloc\",\n\t\"thp_file_fallback\",\n\t\"thp_file_fallback_charge\",\n\t\"thp_file_mapped\",\n\t\"thp_split_page\",\n\t\"thp_split_page_failed\",\n\t\"thp_deferred_split_page\",\n\t\"thp_split_pmd\",\n\t\"thp_scan_exceed_none_pte\",\n\t\"thp_scan_exceed_swap_pte\",\n\t\"thp_scan_exceed_share_pte\",\n#ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD\n\t\"thp_split_pud\",\n#endif\n\t\"thp_zero_page_alloc\",\n\t\"thp_zero_page_alloc_failed\",\n\t\"thp_swpout\",\n\t\"thp_swpout_fallback\",\n#endif\n#ifdef CONFIG_MEMORY_BALLOON\n\t\"balloon_inflate\",\n\t\"balloon_deflate\",\n#ifdef CONFIG_BALLOON_COMPACTION\n\t\"balloon_migrate\",\n#endif\n#endif  \n#ifdef CONFIG_DEBUG_TLBFLUSH\n\t\"nr_tlb_remote_flush\",\n\t\"nr_tlb_remote_flush_received\",\n\t\"nr_tlb_local_flush_all\",\n\t\"nr_tlb_local_flush_one\",\n#endif  \n\n#ifdef CONFIG_SWAP\n\t\"swap_ra\",\n\t\"swap_ra_hit\",\n#ifdef CONFIG_KSM\n\t\"ksm_swpin_copy\",\n#endif\n#endif\n#ifdef CONFIG_KSM\n\t\"cow_ksm\",\n#endif\n#ifdef CONFIG_ZSWAP\n\t\"zswpin\",\n\t\"zswpout\",\n#endif\n#ifdef CONFIG_X86\n\t\"direct_map_level2_splits\",\n\t\"direct_map_level3_splits\",\n#endif\n#ifdef CONFIG_PER_VMA_LOCK_STATS\n\t\"vma_lock_success\",\n\t\"vma_lock_abort\",\n\t\"vma_lock_retry\",\n\t\"vma_lock_miss\",\n#endif\n#endif  \n};\n#endif  \n\n#if (defined(CONFIG_DEBUG_FS) && defined(CONFIG_COMPACTION)) || \\\n     defined(CONFIG_PROC_FS)\nstatic void *frag_start(struct seq_file *m, loff_t *pos)\n{\n\tpg_data_t *pgdat;\n\tloff_t node = *pos;\n\n\tfor (pgdat = first_online_pgdat();\n\t     pgdat && node;\n\t     pgdat = next_online_pgdat(pgdat))\n\t\t--node;\n\n\treturn pgdat;\n}\n\nstatic void *frag_next(struct seq_file *m, void *arg, loff_t *pos)\n{\n\tpg_data_t *pgdat = (pg_data_t *)arg;\n\n\t(*pos)++;\n\treturn next_online_pgdat(pgdat);\n}\n\nstatic void frag_stop(struct seq_file *m, void *arg)\n{\n}\n\n \nstatic void walk_zones_in_node(struct seq_file *m, pg_data_t *pgdat,\n\t\tbool assert_populated, bool nolock,\n\t\tvoid (*print)(struct seq_file *m, pg_data_t *, struct zone *))\n{\n\tstruct zone *zone;\n\tstruct zone *node_zones = pgdat->node_zones;\n\tunsigned long flags;\n\n\tfor (zone = node_zones; zone - node_zones < MAX_NR_ZONES; ++zone) {\n\t\tif (assert_populated && !populated_zone(zone))\n\t\t\tcontinue;\n\n\t\tif (!nolock)\n\t\t\tspin_lock_irqsave(&zone->lock, flags);\n\t\tprint(m, pgdat, zone);\n\t\tif (!nolock)\n\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n\t}\n}\n#endif\n\n#ifdef CONFIG_PROC_FS\nstatic void frag_show_print(struct seq_file *m, pg_data_t *pgdat,\n\t\t\t\t\t\tstruct zone *zone)\n{\n\tint order;\n\n\tseq_printf(m, \"Node %d, zone %8s \", pgdat->node_id, zone->name);\n\tfor (order = 0; order <= MAX_ORDER; ++order)\n\t\t \n\t\tseq_printf(m, \"%6lu \", data_race(zone->free_area[order].nr_free));\n\tseq_putc(m, '\\n');\n}\n\n \nstatic int frag_show(struct seq_file *m, void *arg)\n{\n\tpg_data_t *pgdat = (pg_data_t *)arg;\n\twalk_zones_in_node(m, pgdat, true, false, frag_show_print);\n\treturn 0;\n}\n\nstatic void pagetypeinfo_showfree_print(struct seq_file *m,\n\t\t\t\t\tpg_data_t *pgdat, struct zone *zone)\n{\n\tint order, mtype;\n\n\tfor (mtype = 0; mtype < MIGRATE_TYPES; mtype++) {\n\t\tseq_printf(m, \"Node %4d, zone %8s, type %12s \",\n\t\t\t\t\tpgdat->node_id,\n\t\t\t\t\tzone->name,\n\t\t\t\t\tmigratetype_names[mtype]);\n\t\tfor (order = 0; order <= MAX_ORDER; ++order) {\n\t\t\tunsigned long freecount = 0;\n\t\t\tstruct free_area *area;\n\t\t\tstruct list_head *curr;\n\t\t\tbool overflow = false;\n\n\t\t\tarea = &(zone->free_area[order]);\n\n\t\t\tlist_for_each(curr, &area->free_list[mtype]) {\n\t\t\t\t \n\t\t\t\tif (++freecount >= 100000) {\n\t\t\t\t\toverflow = true;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tseq_printf(m, \"%s%6lu \", overflow ? \">\" : \"\", freecount);\n\t\t\tspin_unlock_irq(&zone->lock);\n\t\t\tcond_resched();\n\t\t\tspin_lock_irq(&zone->lock);\n\t\t}\n\t\tseq_putc(m, '\\n');\n\t}\n}\n\n \nstatic void pagetypeinfo_showfree(struct seq_file *m, void *arg)\n{\n\tint order;\n\tpg_data_t *pgdat = (pg_data_t *)arg;\n\n\t \n\tseq_printf(m, \"%-43s \", \"Free pages count per migrate type at order\");\n\tfor (order = 0; order <= MAX_ORDER; ++order)\n\t\tseq_printf(m, \"%6d \", order);\n\tseq_putc(m, '\\n');\n\n\twalk_zones_in_node(m, pgdat, true, false, pagetypeinfo_showfree_print);\n}\n\nstatic void pagetypeinfo_showblockcount_print(struct seq_file *m,\n\t\t\t\t\tpg_data_t *pgdat, struct zone *zone)\n{\n\tint mtype;\n\tunsigned long pfn;\n\tunsigned long start_pfn = zone->zone_start_pfn;\n\tunsigned long end_pfn = zone_end_pfn(zone);\n\tunsigned long count[MIGRATE_TYPES] = { 0, };\n\n\tfor (pfn = start_pfn; pfn < end_pfn; pfn += pageblock_nr_pages) {\n\t\tstruct page *page;\n\n\t\tpage = pfn_to_online_page(pfn);\n\t\tif (!page)\n\t\t\tcontinue;\n\n\t\tif (page_zone(page) != zone)\n\t\t\tcontinue;\n\n\t\tmtype = get_pageblock_migratetype(page);\n\n\t\tif (mtype < MIGRATE_TYPES)\n\t\t\tcount[mtype]++;\n\t}\n\n\t \n\tseq_printf(m, \"Node %d, zone %8s \", pgdat->node_id, zone->name);\n\tfor (mtype = 0; mtype < MIGRATE_TYPES; mtype++)\n\t\tseq_printf(m, \"%12lu \", count[mtype]);\n\tseq_putc(m, '\\n');\n}\n\n \nstatic void pagetypeinfo_showblockcount(struct seq_file *m, void *arg)\n{\n\tint mtype;\n\tpg_data_t *pgdat = (pg_data_t *)arg;\n\n\tseq_printf(m, \"\\n%-23s\", \"Number of blocks type \");\n\tfor (mtype = 0; mtype < MIGRATE_TYPES; mtype++)\n\t\tseq_printf(m, \"%12s \", migratetype_names[mtype]);\n\tseq_putc(m, '\\n');\n\twalk_zones_in_node(m, pgdat, true, false,\n\t\tpagetypeinfo_showblockcount_print);\n}\n\n \nstatic void pagetypeinfo_showmixedcount(struct seq_file *m, pg_data_t *pgdat)\n{\n#ifdef CONFIG_PAGE_OWNER\n\tint mtype;\n\n\tif (!static_branch_unlikely(&page_owner_inited))\n\t\treturn;\n\n\tdrain_all_pages(NULL);\n\n\tseq_printf(m, \"\\n%-23s\", \"Number of mixed blocks \");\n\tfor (mtype = 0; mtype < MIGRATE_TYPES; mtype++)\n\t\tseq_printf(m, \"%12s \", migratetype_names[mtype]);\n\tseq_putc(m, '\\n');\n\n\twalk_zones_in_node(m, pgdat, true, true,\n\t\tpagetypeinfo_showmixedcount_print);\n#endif  \n}\n\n \nstatic int pagetypeinfo_show(struct seq_file *m, void *arg)\n{\n\tpg_data_t *pgdat = (pg_data_t *)arg;\n\n\t \n\tif (!node_state(pgdat->node_id, N_MEMORY))\n\t\treturn 0;\n\n\tseq_printf(m, \"Page block order: %d\\n\", pageblock_order);\n\tseq_printf(m, \"Pages per block:  %lu\\n\", pageblock_nr_pages);\n\tseq_putc(m, '\\n');\n\tpagetypeinfo_showfree(m, pgdat);\n\tpagetypeinfo_showblockcount(m, pgdat);\n\tpagetypeinfo_showmixedcount(m, pgdat);\n\n\treturn 0;\n}\n\nstatic const struct seq_operations fragmentation_op = {\n\t.start\t= frag_start,\n\t.next\t= frag_next,\n\t.stop\t= frag_stop,\n\t.show\t= frag_show,\n};\n\nstatic const struct seq_operations pagetypeinfo_op = {\n\t.start\t= frag_start,\n\t.next\t= frag_next,\n\t.stop\t= frag_stop,\n\t.show\t= pagetypeinfo_show,\n};\n\nstatic bool is_zone_first_populated(pg_data_t *pgdat, struct zone *zone)\n{\n\tint zid;\n\n\tfor (zid = 0; zid < MAX_NR_ZONES; zid++) {\n\t\tstruct zone *compare = &pgdat->node_zones[zid];\n\n\t\tif (populated_zone(compare))\n\t\t\treturn zone == compare;\n\t}\n\n\treturn false;\n}\n\nstatic void zoneinfo_show_print(struct seq_file *m, pg_data_t *pgdat,\n\t\t\t\t\t\t\tstruct zone *zone)\n{\n\tint i;\n\tseq_printf(m, \"Node %d, zone %8s\", pgdat->node_id, zone->name);\n\tif (is_zone_first_populated(pgdat, zone)) {\n\t\tseq_printf(m, \"\\n  per-node stats\");\n\t\tfor (i = 0; i < NR_VM_NODE_STAT_ITEMS; i++) {\n\t\t\tunsigned long pages = node_page_state_pages(pgdat, i);\n\n\t\t\tif (vmstat_item_print_in_thp(i))\n\t\t\t\tpages /= HPAGE_PMD_NR;\n\t\t\tseq_printf(m, \"\\n      %-12s %lu\", node_stat_name(i),\n\t\t\t\t   pages);\n\t\t}\n\t}\n\tseq_printf(m,\n\t\t   \"\\n  pages free     %lu\"\n\t\t   \"\\n        boost    %lu\"\n\t\t   \"\\n        min      %lu\"\n\t\t   \"\\n        low      %lu\"\n\t\t   \"\\n        high     %lu\"\n\t\t   \"\\n        spanned  %lu\"\n\t\t   \"\\n        present  %lu\"\n\t\t   \"\\n        managed  %lu\"\n\t\t   \"\\n        cma      %lu\",\n\t\t   zone_page_state(zone, NR_FREE_PAGES),\n\t\t   zone->watermark_boost,\n\t\t   min_wmark_pages(zone),\n\t\t   low_wmark_pages(zone),\n\t\t   high_wmark_pages(zone),\n\t\t   zone->spanned_pages,\n\t\t   zone->present_pages,\n\t\t   zone_managed_pages(zone),\n\t\t   zone_cma_pages(zone));\n\n\tseq_printf(m,\n\t\t   \"\\n        protection: (%ld\",\n\t\t   zone->lowmem_reserve[0]);\n\tfor (i = 1; i < ARRAY_SIZE(zone->lowmem_reserve); i++)\n\t\tseq_printf(m, \", %ld\", zone->lowmem_reserve[i]);\n\tseq_putc(m, ')');\n\n\t \n\tif (!populated_zone(zone)) {\n\t\tseq_putc(m, '\\n');\n\t\treturn;\n\t}\n\n\tfor (i = 0; i < NR_VM_ZONE_STAT_ITEMS; i++)\n\t\tseq_printf(m, \"\\n      %-12s %lu\", zone_stat_name(i),\n\t\t\t   zone_page_state(zone, i));\n\n#ifdef CONFIG_NUMA\n\tfor (i = 0; i < NR_VM_NUMA_EVENT_ITEMS; i++)\n\t\tseq_printf(m, \"\\n      %-12s %lu\", numa_stat_name(i),\n\t\t\t   zone_numa_event_state(zone, i));\n#endif\n\n\tseq_printf(m, \"\\n  pagesets\");\n\tfor_each_online_cpu(i) {\n\t\tstruct per_cpu_pages *pcp;\n\t\tstruct per_cpu_zonestat __maybe_unused *pzstats;\n\n\t\tpcp = per_cpu_ptr(zone->per_cpu_pageset, i);\n\t\tseq_printf(m,\n\t\t\t   \"\\n    cpu: %i\"\n\t\t\t   \"\\n              count: %i\"\n\t\t\t   \"\\n              high:  %i\"\n\t\t\t   \"\\n              batch: %i\",\n\t\t\t   i,\n\t\t\t   pcp->count,\n\t\t\t   pcp->high,\n\t\t\t   pcp->batch);\n#ifdef CONFIG_SMP\n\t\tpzstats = per_cpu_ptr(zone->per_cpu_zonestats, i);\n\t\tseq_printf(m, \"\\n  vm stats threshold: %d\",\n\t\t\t\tpzstats->stat_threshold);\n#endif\n\t}\n\tseq_printf(m,\n\t\t   \"\\n  node_unreclaimable:  %u\"\n\t\t   \"\\n  start_pfn:           %lu\",\n\t\t   pgdat->kswapd_failures >= MAX_RECLAIM_RETRIES,\n\t\t   zone->zone_start_pfn);\n\tseq_putc(m, '\\n');\n}\n\n \nstatic int zoneinfo_show(struct seq_file *m, void *arg)\n{\n\tpg_data_t *pgdat = (pg_data_t *)arg;\n\twalk_zones_in_node(m, pgdat, false, false, zoneinfo_show_print);\n\treturn 0;\n}\n\nstatic const struct seq_operations zoneinfo_op = {\n\t.start\t= frag_start,  \n\t.next\t= frag_next,\n\t.stop\t= frag_stop,\n\t.show\t= zoneinfo_show,\n};\n\n#define NR_VMSTAT_ITEMS (NR_VM_ZONE_STAT_ITEMS + \\\n\t\t\t NR_VM_NUMA_EVENT_ITEMS + \\\n\t\t\t NR_VM_NODE_STAT_ITEMS + \\\n\t\t\t NR_VM_WRITEBACK_STAT_ITEMS + \\\n\t\t\t (IS_ENABLED(CONFIG_VM_EVENT_COUNTERS) ? \\\n\t\t\t  NR_VM_EVENT_ITEMS : 0))\n\nstatic void *vmstat_start(struct seq_file *m, loff_t *pos)\n{\n\tunsigned long *v;\n\tint i;\n\n\tif (*pos >= NR_VMSTAT_ITEMS)\n\t\treturn NULL;\n\n\tBUILD_BUG_ON(ARRAY_SIZE(vmstat_text) < NR_VMSTAT_ITEMS);\n\tfold_vm_numa_events();\n\tv = kmalloc_array(NR_VMSTAT_ITEMS, sizeof(unsigned long), GFP_KERNEL);\n\tm->private = v;\n\tif (!v)\n\t\treturn ERR_PTR(-ENOMEM);\n\tfor (i = 0; i < NR_VM_ZONE_STAT_ITEMS; i++)\n\t\tv[i] = global_zone_page_state(i);\n\tv += NR_VM_ZONE_STAT_ITEMS;\n\n#ifdef CONFIG_NUMA\n\tfor (i = 0; i < NR_VM_NUMA_EVENT_ITEMS; i++)\n\t\tv[i] = global_numa_event_state(i);\n\tv += NR_VM_NUMA_EVENT_ITEMS;\n#endif\n\n\tfor (i = 0; i < NR_VM_NODE_STAT_ITEMS; i++) {\n\t\tv[i] = global_node_page_state_pages(i);\n\t\tif (vmstat_item_print_in_thp(i))\n\t\t\tv[i] /= HPAGE_PMD_NR;\n\t}\n\tv += NR_VM_NODE_STAT_ITEMS;\n\n\tglobal_dirty_limits(v + NR_DIRTY_BG_THRESHOLD,\n\t\t\t    v + NR_DIRTY_THRESHOLD);\n\tv += NR_VM_WRITEBACK_STAT_ITEMS;\n\n#ifdef CONFIG_VM_EVENT_COUNTERS\n\tall_vm_events(v);\n\tv[PGPGIN] /= 2;\t\t \n\tv[PGPGOUT] /= 2;\n#endif\n\treturn (unsigned long *)m->private + *pos;\n}\n\nstatic void *vmstat_next(struct seq_file *m, void *arg, loff_t *pos)\n{\n\t(*pos)++;\n\tif (*pos >= NR_VMSTAT_ITEMS)\n\t\treturn NULL;\n\treturn (unsigned long *)m->private + *pos;\n}\n\nstatic int vmstat_show(struct seq_file *m, void *arg)\n{\n\tunsigned long *l = arg;\n\tunsigned long off = l - (unsigned long *)m->private;\n\n\tseq_puts(m, vmstat_text[off]);\n\tseq_put_decimal_ull(m, \" \", *l);\n\tseq_putc(m, '\\n');\n\n\tif (off == NR_VMSTAT_ITEMS - 1) {\n\t\t \n\t\tseq_puts(m, \"nr_unstable 0\\n\");\n\t}\n\treturn 0;\n}\n\nstatic void vmstat_stop(struct seq_file *m, void *arg)\n{\n\tkfree(m->private);\n\tm->private = NULL;\n}\n\nstatic const struct seq_operations vmstat_op = {\n\t.start\t= vmstat_start,\n\t.next\t= vmstat_next,\n\t.stop\t= vmstat_stop,\n\t.show\t= vmstat_show,\n};\n#endif  \n\n#ifdef CONFIG_SMP\nstatic DEFINE_PER_CPU(struct delayed_work, vmstat_work);\nint sysctl_stat_interval __read_mostly = HZ;\n\n#ifdef CONFIG_PROC_FS\nstatic void refresh_vm_stats(struct work_struct *work)\n{\n\trefresh_cpu_vm_stats(true);\n}\n\nint vmstat_refresh(struct ctl_table *table, int write,\n\t\t   void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tlong val;\n\tint err;\n\tint i;\n\n\t \n\terr = schedule_on_each_cpu(refresh_vm_stats);\n\tif (err)\n\t\treturn err;\n\tfor (i = 0; i < NR_VM_ZONE_STAT_ITEMS; i++) {\n\t\t \n\t\tswitch (i) {\n\t\tcase NR_ZONE_WRITE_PENDING:\n\t\tcase NR_FREE_CMA_PAGES:\n\t\t\tcontinue;\n\t\t}\n\t\tval = atomic_long_read(&vm_zone_stat[i]);\n\t\tif (val < 0) {\n\t\t\tpr_warn(\"%s: %s %ld\\n\",\n\t\t\t\t__func__, zone_stat_name(i), val);\n\t\t}\n\t}\n\tfor (i = 0; i < NR_VM_NODE_STAT_ITEMS; i++) {\n\t\t \n\t\tswitch (i) {\n\t\tcase NR_WRITEBACK:\n\t\t\tcontinue;\n\t\t}\n\t\tval = atomic_long_read(&vm_node_stat[i]);\n\t\tif (val < 0) {\n\t\t\tpr_warn(\"%s: %s %ld\\n\",\n\t\t\t\t__func__, node_stat_name(i), val);\n\t\t}\n\t}\n\tif (write)\n\t\t*ppos += *lenp;\n\telse\n\t\t*lenp = 0;\n\treturn 0;\n}\n#endif  \n\nstatic void vmstat_update(struct work_struct *w)\n{\n\tif (refresh_cpu_vm_stats(true)) {\n\t\t \n\t\tqueue_delayed_work_on(smp_processor_id(), mm_percpu_wq,\n\t\t\t\tthis_cpu_ptr(&vmstat_work),\n\t\t\t\tround_jiffies_relative(sysctl_stat_interval));\n\t}\n}\n\n \nstatic bool need_update(int cpu)\n{\n\tpg_data_t *last_pgdat = NULL;\n\tstruct zone *zone;\n\n\tfor_each_populated_zone(zone) {\n\t\tstruct per_cpu_zonestat *pzstats = per_cpu_ptr(zone->per_cpu_zonestats, cpu);\n\t\tstruct per_cpu_nodestat *n;\n\n\t\t \n\t\tif (memchr_inv(pzstats->vm_stat_diff, 0, sizeof(pzstats->vm_stat_diff)))\n\t\t\treturn true;\n\n\t\tif (last_pgdat == zone->zone_pgdat)\n\t\t\tcontinue;\n\t\tlast_pgdat = zone->zone_pgdat;\n\t\tn = per_cpu_ptr(zone->zone_pgdat->per_cpu_nodestats, cpu);\n\t\tif (memchr_inv(n->vm_node_stat_diff, 0, sizeof(n->vm_node_stat_diff)))\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\n \nvoid quiet_vmstat(void)\n{\n\tif (system_state != SYSTEM_RUNNING)\n\t\treturn;\n\n\tif (!delayed_work_pending(this_cpu_ptr(&vmstat_work)))\n\t\treturn;\n\n\tif (!need_update(smp_processor_id()))\n\t\treturn;\n\n\t \n\trefresh_cpu_vm_stats(false);\n}\n\n \nstatic void vmstat_shepherd(struct work_struct *w);\n\nstatic DECLARE_DEFERRABLE_WORK(shepherd, vmstat_shepherd);\n\nstatic void vmstat_shepherd(struct work_struct *w)\n{\n\tint cpu;\n\n\tcpus_read_lock();\n\t \n\tfor_each_online_cpu(cpu) {\n\t\tstruct delayed_work *dw = &per_cpu(vmstat_work, cpu);\n\n\t\t \n\t\tif (cpu_is_isolated(cpu))\n\t\t\tcontinue;\n\n\t\tif (!delayed_work_pending(dw) && need_update(cpu))\n\t\t\tqueue_delayed_work_on(cpu, mm_percpu_wq, dw, 0);\n\n\t\tcond_resched();\n\t}\n\tcpus_read_unlock();\n\n\tschedule_delayed_work(&shepherd,\n\t\tround_jiffies_relative(sysctl_stat_interval));\n}\n\nstatic void __init start_shepherd_timer(void)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu)\n\t\tINIT_DEFERRABLE_WORK(per_cpu_ptr(&vmstat_work, cpu),\n\t\t\tvmstat_update);\n\n\tschedule_delayed_work(&shepherd,\n\t\tround_jiffies_relative(sysctl_stat_interval));\n}\n\nstatic void __init init_cpu_node_state(void)\n{\n\tint node;\n\n\tfor_each_online_node(node) {\n\t\tif (!cpumask_empty(cpumask_of_node(node)))\n\t\t\tnode_set_state(node, N_CPU);\n\t}\n}\n\nstatic int vmstat_cpu_online(unsigned int cpu)\n{\n\trefresh_zone_stat_thresholds();\n\n\tif (!node_state(cpu_to_node(cpu), N_CPU)) {\n\t\tnode_set_state(cpu_to_node(cpu), N_CPU);\n\t}\n\n\treturn 0;\n}\n\nstatic int vmstat_cpu_down_prep(unsigned int cpu)\n{\n\tcancel_delayed_work_sync(&per_cpu(vmstat_work, cpu));\n\treturn 0;\n}\n\nstatic int vmstat_cpu_dead(unsigned int cpu)\n{\n\tconst struct cpumask *node_cpus;\n\tint node;\n\n\tnode = cpu_to_node(cpu);\n\n\trefresh_zone_stat_thresholds();\n\tnode_cpus = cpumask_of_node(node);\n\tif (!cpumask_empty(node_cpus))\n\t\treturn 0;\n\n\tnode_clear_state(node, N_CPU);\n\n\treturn 0;\n}\n\n#endif\n\nstruct workqueue_struct *mm_percpu_wq;\n\nvoid __init init_mm_internals(void)\n{\n\tint ret __maybe_unused;\n\n\tmm_percpu_wq = alloc_workqueue(\"mm_percpu_wq\", WQ_MEM_RECLAIM, 0);\n\n#ifdef CONFIG_SMP\n\tret = cpuhp_setup_state_nocalls(CPUHP_MM_VMSTAT_DEAD, \"mm/vmstat:dead\",\n\t\t\t\t\tNULL, vmstat_cpu_dead);\n\tif (ret < 0)\n\t\tpr_err(\"vmstat: failed to register 'dead' hotplug state\\n\");\n\n\tret = cpuhp_setup_state_nocalls(CPUHP_AP_ONLINE_DYN, \"mm/vmstat:online\",\n\t\t\t\t\tvmstat_cpu_online,\n\t\t\t\t\tvmstat_cpu_down_prep);\n\tif (ret < 0)\n\t\tpr_err(\"vmstat: failed to register 'online' hotplug state\\n\");\n\n\tcpus_read_lock();\n\tinit_cpu_node_state();\n\tcpus_read_unlock();\n\n\tstart_shepherd_timer();\n#endif\n#ifdef CONFIG_PROC_FS\n\tproc_create_seq(\"buddyinfo\", 0444, NULL, &fragmentation_op);\n\tproc_create_seq(\"pagetypeinfo\", 0400, NULL, &pagetypeinfo_op);\n\tproc_create_seq(\"vmstat\", 0444, NULL, &vmstat_op);\n\tproc_create_seq(\"zoneinfo\", 0444, NULL, &zoneinfo_op);\n#endif\n}\n\n#if defined(CONFIG_DEBUG_FS) && defined(CONFIG_COMPACTION)\n\n \nstatic int unusable_free_index(unsigned int order,\n\t\t\t\tstruct contig_page_info *info)\n{\n\t \n\tif (info->free_pages == 0)\n\t\treturn 1000;\n\n\t \n\treturn div_u64((info->free_pages - (info->free_blocks_suitable << order)) * 1000ULL, info->free_pages);\n\n}\n\nstatic void unusable_show_print(struct seq_file *m,\n\t\t\t\t\tpg_data_t *pgdat, struct zone *zone)\n{\n\tunsigned int order;\n\tint index;\n\tstruct contig_page_info info;\n\n\tseq_printf(m, \"Node %d, zone %8s \",\n\t\t\t\tpgdat->node_id,\n\t\t\t\tzone->name);\n\tfor (order = 0; order <= MAX_ORDER; ++order) {\n\t\tfill_contig_page_info(zone, order, &info);\n\t\tindex = unusable_free_index(order, &info);\n\t\tseq_printf(m, \"%d.%03d \", index / 1000, index % 1000);\n\t}\n\n\tseq_putc(m, '\\n');\n}\n\n \nstatic int unusable_show(struct seq_file *m, void *arg)\n{\n\tpg_data_t *pgdat = (pg_data_t *)arg;\n\n\t \n\tif (!node_state(pgdat->node_id, N_MEMORY))\n\t\treturn 0;\n\n\twalk_zones_in_node(m, pgdat, true, false, unusable_show_print);\n\n\treturn 0;\n}\n\nstatic const struct seq_operations unusable_sops = {\n\t.start\t= frag_start,\n\t.next\t= frag_next,\n\t.stop\t= frag_stop,\n\t.show\t= unusable_show,\n};\n\nDEFINE_SEQ_ATTRIBUTE(unusable);\n\nstatic void extfrag_show_print(struct seq_file *m,\n\t\t\t\t\tpg_data_t *pgdat, struct zone *zone)\n{\n\tunsigned int order;\n\tint index;\n\n\t \n\tstruct contig_page_info info;\n\n\tseq_printf(m, \"Node %d, zone %8s \",\n\t\t\t\tpgdat->node_id,\n\t\t\t\tzone->name);\n\tfor (order = 0; order <= MAX_ORDER; ++order) {\n\t\tfill_contig_page_info(zone, order, &info);\n\t\tindex = __fragmentation_index(order, &info);\n\t\tseq_printf(m, \"%2d.%03d \", index / 1000, index % 1000);\n\t}\n\n\tseq_putc(m, '\\n');\n}\n\n \nstatic int extfrag_show(struct seq_file *m, void *arg)\n{\n\tpg_data_t *pgdat = (pg_data_t *)arg;\n\n\twalk_zones_in_node(m, pgdat, true, false, extfrag_show_print);\n\n\treturn 0;\n}\n\nstatic const struct seq_operations extfrag_sops = {\n\t.start\t= frag_start,\n\t.next\t= frag_next,\n\t.stop\t= frag_stop,\n\t.show\t= extfrag_show,\n};\n\nDEFINE_SEQ_ATTRIBUTE(extfrag);\n\nstatic int __init extfrag_debug_init(void)\n{\n\tstruct dentry *extfrag_debug_root;\n\n\textfrag_debug_root = debugfs_create_dir(\"extfrag\", NULL);\n\n\tdebugfs_create_file(\"unusable_index\", 0444, extfrag_debug_root, NULL,\n\t\t\t    &unusable_fops);\n\n\tdebugfs_create_file(\"extfrag_index\", 0444, extfrag_debug_root, NULL,\n\t\t\t    &extfrag_fops);\n\n\treturn 0;\n}\n\nmodule_init(extfrag_debug_init);\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}