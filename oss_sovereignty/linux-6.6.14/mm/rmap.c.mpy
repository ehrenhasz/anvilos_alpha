{
  "module_name": "rmap.c",
  "hash_id": "ec9ff08b191becddbe080b73a0469cc8721859fe8890c7c7086cbaf3b7c72355",
  "original_prompt": "Ingested from linux-6.6.14/mm/rmap.c",
  "human_readable_source": " \n\n \n\n#include <linux/mm.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/task.h>\n#include <linux/pagemap.h>\n#include <linux/swap.h>\n#include <linux/swapops.h>\n#include <linux/slab.h>\n#include <linux/init.h>\n#include <linux/ksm.h>\n#include <linux/rmap.h>\n#include <linux/rcupdate.h>\n#include <linux/export.h>\n#include <linux/memcontrol.h>\n#include <linux/mmu_notifier.h>\n#include <linux/migrate.h>\n#include <linux/hugetlb.h>\n#include <linux/huge_mm.h>\n#include <linux/backing-dev.h>\n#include <linux/page_idle.h>\n#include <linux/memremap.h>\n#include <linux/userfaultfd_k.h>\n#include <linux/mm_inline.h>\n\n#include <asm/tlbflush.h>\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/tlb.h>\n#include <trace/events/migrate.h>\n\n#include \"internal.h\"\n\nstatic struct kmem_cache *anon_vma_cachep;\nstatic struct kmem_cache *anon_vma_chain_cachep;\n\nstatic inline struct anon_vma *anon_vma_alloc(void)\n{\n\tstruct anon_vma *anon_vma;\n\n\tanon_vma = kmem_cache_alloc(anon_vma_cachep, GFP_KERNEL);\n\tif (anon_vma) {\n\t\tatomic_set(&anon_vma->refcount, 1);\n\t\tanon_vma->num_children = 0;\n\t\tanon_vma->num_active_vmas = 0;\n\t\tanon_vma->parent = anon_vma;\n\t\t \n\t\tanon_vma->root = anon_vma;\n\t}\n\n\treturn anon_vma;\n}\n\nstatic inline void anon_vma_free(struct anon_vma *anon_vma)\n{\n\tVM_BUG_ON(atomic_read(&anon_vma->refcount));\n\n\t \n\tmight_sleep();\n\tif (rwsem_is_locked(&anon_vma->root->rwsem)) {\n\t\tanon_vma_lock_write(anon_vma);\n\t\tanon_vma_unlock_write(anon_vma);\n\t}\n\n\tkmem_cache_free(anon_vma_cachep, anon_vma);\n}\n\nstatic inline struct anon_vma_chain *anon_vma_chain_alloc(gfp_t gfp)\n{\n\treturn kmem_cache_alloc(anon_vma_chain_cachep, gfp);\n}\n\nstatic void anon_vma_chain_free(struct anon_vma_chain *anon_vma_chain)\n{\n\tkmem_cache_free(anon_vma_chain_cachep, anon_vma_chain);\n}\n\nstatic void anon_vma_chain_link(struct vm_area_struct *vma,\n\t\t\t\tstruct anon_vma_chain *avc,\n\t\t\t\tstruct anon_vma *anon_vma)\n{\n\tavc->vma = vma;\n\tavc->anon_vma = anon_vma;\n\tlist_add(&avc->same_vma, &vma->anon_vma_chain);\n\tanon_vma_interval_tree_insert(avc, &anon_vma->rb_root);\n}\n\n \nint __anon_vma_prepare(struct vm_area_struct *vma)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct anon_vma *anon_vma, *allocated;\n\tstruct anon_vma_chain *avc;\n\n\tmight_sleep();\n\n\tavc = anon_vma_chain_alloc(GFP_KERNEL);\n\tif (!avc)\n\t\tgoto out_enomem;\n\n\tanon_vma = find_mergeable_anon_vma(vma);\n\tallocated = NULL;\n\tif (!anon_vma) {\n\t\tanon_vma = anon_vma_alloc();\n\t\tif (unlikely(!anon_vma))\n\t\t\tgoto out_enomem_free_avc;\n\t\tanon_vma->num_children++;  \n\t\tallocated = anon_vma;\n\t}\n\n\tanon_vma_lock_write(anon_vma);\n\t \n\tspin_lock(&mm->page_table_lock);\n\tif (likely(!vma->anon_vma)) {\n\t\tvma->anon_vma = anon_vma;\n\t\tanon_vma_chain_link(vma, avc, anon_vma);\n\t\tanon_vma->num_active_vmas++;\n\t\tallocated = NULL;\n\t\tavc = NULL;\n\t}\n\tspin_unlock(&mm->page_table_lock);\n\tanon_vma_unlock_write(anon_vma);\n\n\tif (unlikely(allocated))\n\t\tput_anon_vma(allocated);\n\tif (unlikely(avc))\n\t\tanon_vma_chain_free(avc);\n\n\treturn 0;\n\n out_enomem_free_avc:\n\tanon_vma_chain_free(avc);\n out_enomem:\n\treturn -ENOMEM;\n}\n\n \nstatic inline struct anon_vma *lock_anon_vma_root(struct anon_vma *root, struct anon_vma *anon_vma)\n{\n\tstruct anon_vma *new_root = anon_vma->root;\n\tif (new_root != root) {\n\t\tif (WARN_ON_ONCE(root))\n\t\t\tup_write(&root->rwsem);\n\t\troot = new_root;\n\t\tdown_write(&root->rwsem);\n\t}\n\treturn root;\n}\n\nstatic inline void unlock_anon_vma_root(struct anon_vma *root)\n{\n\tif (root)\n\t\tup_write(&root->rwsem);\n}\n\n \nint anon_vma_clone(struct vm_area_struct *dst, struct vm_area_struct *src)\n{\n\tstruct anon_vma_chain *avc, *pavc;\n\tstruct anon_vma *root = NULL;\n\n\tlist_for_each_entry_reverse(pavc, &src->anon_vma_chain, same_vma) {\n\t\tstruct anon_vma *anon_vma;\n\n\t\tavc = anon_vma_chain_alloc(GFP_NOWAIT | __GFP_NOWARN);\n\t\tif (unlikely(!avc)) {\n\t\t\tunlock_anon_vma_root(root);\n\t\t\troot = NULL;\n\t\t\tavc = anon_vma_chain_alloc(GFP_KERNEL);\n\t\t\tif (!avc)\n\t\t\t\tgoto enomem_failure;\n\t\t}\n\t\tanon_vma = pavc->anon_vma;\n\t\troot = lock_anon_vma_root(root, anon_vma);\n\t\tanon_vma_chain_link(dst, avc, anon_vma);\n\n\t\t \n\t\tif (!dst->anon_vma && src->anon_vma &&\n\t\t    anon_vma->num_children < 2 &&\n\t\t    anon_vma->num_active_vmas == 0)\n\t\t\tdst->anon_vma = anon_vma;\n\t}\n\tif (dst->anon_vma)\n\t\tdst->anon_vma->num_active_vmas++;\n\tunlock_anon_vma_root(root);\n\treturn 0;\n\n enomem_failure:\n\t \n\tdst->anon_vma = NULL;\n\tunlink_anon_vmas(dst);\n\treturn -ENOMEM;\n}\n\n \nint anon_vma_fork(struct vm_area_struct *vma, struct vm_area_struct *pvma)\n{\n\tstruct anon_vma_chain *avc;\n\tstruct anon_vma *anon_vma;\n\tint error;\n\n\t \n\tif (!pvma->anon_vma)\n\t\treturn 0;\n\n\t \n\tvma->anon_vma = NULL;\n\n\t \n\terror = anon_vma_clone(vma, pvma);\n\tif (error)\n\t\treturn error;\n\n\t \n\tif (vma->anon_vma)\n\t\treturn 0;\n\n\t \n\tanon_vma = anon_vma_alloc();\n\tif (!anon_vma)\n\t\tgoto out_error;\n\tanon_vma->num_active_vmas++;\n\tavc = anon_vma_chain_alloc(GFP_KERNEL);\n\tif (!avc)\n\t\tgoto out_error_free_anon_vma;\n\n\t \n\tanon_vma->root = pvma->anon_vma->root;\n\tanon_vma->parent = pvma->anon_vma;\n\t \n\tget_anon_vma(anon_vma->root);\n\t \n\tvma->anon_vma = anon_vma;\n\tanon_vma_lock_write(anon_vma);\n\tanon_vma_chain_link(vma, avc, anon_vma);\n\tanon_vma->parent->num_children++;\n\tanon_vma_unlock_write(anon_vma);\n\n\treturn 0;\n\n out_error_free_anon_vma:\n\tput_anon_vma(anon_vma);\n out_error:\n\tunlink_anon_vmas(vma);\n\treturn -ENOMEM;\n}\n\nvoid unlink_anon_vmas(struct vm_area_struct *vma)\n{\n\tstruct anon_vma_chain *avc, *next;\n\tstruct anon_vma *root = NULL;\n\n\t \n\tlist_for_each_entry_safe(avc, next, &vma->anon_vma_chain, same_vma) {\n\t\tstruct anon_vma *anon_vma = avc->anon_vma;\n\n\t\troot = lock_anon_vma_root(root, anon_vma);\n\t\tanon_vma_interval_tree_remove(avc, &anon_vma->rb_root);\n\n\t\t \n\t\tif (RB_EMPTY_ROOT(&anon_vma->rb_root.rb_root)) {\n\t\t\tanon_vma->parent->num_children--;\n\t\t\tcontinue;\n\t\t}\n\n\t\tlist_del(&avc->same_vma);\n\t\tanon_vma_chain_free(avc);\n\t}\n\tif (vma->anon_vma) {\n\t\tvma->anon_vma->num_active_vmas--;\n\n\t\t \n\t\tvma->anon_vma = NULL;\n\t}\n\tunlock_anon_vma_root(root);\n\n\t \n\tlist_for_each_entry_safe(avc, next, &vma->anon_vma_chain, same_vma) {\n\t\tstruct anon_vma *anon_vma = avc->anon_vma;\n\n\t\tVM_WARN_ON(anon_vma->num_children);\n\t\tVM_WARN_ON(anon_vma->num_active_vmas);\n\t\tput_anon_vma(anon_vma);\n\n\t\tlist_del(&avc->same_vma);\n\t\tanon_vma_chain_free(avc);\n\t}\n}\n\nstatic void anon_vma_ctor(void *data)\n{\n\tstruct anon_vma *anon_vma = data;\n\n\tinit_rwsem(&anon_vma->rwsem);\n\tatomic_set(&anon_vma->refcount, 0);\n\tanon_vma->rb_root = RB_ROOT_CACHED;\n}\n\nvoid __init anon_vma_init(void)\n{\n\tanon_vma_cachep = kmem_cache_create(\"anon_vma\", sizeof(struct anon_vma),\n\t\t\t0, SLAB_TYPESAFE_BY_RCU|SLAB_PANIC|SLAB_ACCOUNT,\n\t\t\tanon_vma_ctor);\n\tanon_vma_chain_cachep = KMEM_CACHE(anon_vma_chain,\n\t\t\tSLAB_PANIC|SLAB_ACCOUNT);\n}\n\n \nstruct anon_vma *folio_get_anon_vma(struct folio *folio)\n{\n\tstruct anon_vma *anon_vma = NULL;\n\tunsigned long anon_mapping;\n\n\trcu_read_lock();\n\tanon_mapping = (unsigned long)READ_ONCE(folio->mapping);\n\tif ((anon_mapping & PAGE_MAPPING_FLAGS) != PAGE_MAPPING_ANON)\n\t\tgoto out;\n\tif (!folio_mapped(folio))\n\t\tgoto out;\n\n\tanon_vma = (struct anon_vma *) (anon_mapping - PAGE_MAPPING_ANON);\n\tif (!atomic_inc_not_zero(&anon_vma->refcount)) {\n\t\tanon_vma = NULL;\n\t\tgoto out;\n\t}\n\n\t \n\tif (!folio_mapped(folio)) {\n\t\trcu_read_unlock();\n\t\tput_anon_vma(anon_vma);\n\t\treturn NULL;\n\t}\nout:\n\trcu_read_unlock();\n\n\treturn anon_vma;\n}\n\n \nstruct anon_vma *folio_lock_anon_vma_read(struct folio *folio,\n\t\t\t\t\t  struct rmap_walk_control *rwc)\n{\n\tstruct anon_vma *anon_vma = NULL;\n\tstruct anon_vma *root_anon_vma;\n\tunsigned long anon_mapping;\n\n\trcu_read_lock();\n\tanon_mapping = (unsigned long)READ_ONCE(folio->mapping);\n\tif ((anon_mapping & PAGE_MAPPING_FLAGS) != PAGE_MAPPING_ANON)\n\t\tgoto out;\n\tif (!folio_mapped(folio))\n\t\tgoto out;\n\n\tanon_vma = (struct anon_vma *) (anon_mapping - PAGE_MAPPING_ANON);\n\troot_anon_vma = READ_ONCE(anon_vma->root);\n\tif (down_read_trylock(&root_anon_vma->rwsem)) {\n\t\t \n\t\tif (!folio_mapped(folio)) {\n\t\t\tup_read(&root_anon_vma->rwsem);\n\t\t\tanon_vma = NULL;\n\t\t}\n\t\tgoto out;\n\t}\n\n\tif (rwc && rwc->try_lock) {\n\t\tanon_vma = NULL;\n\t\trwc->contended = true;\n\t\tgoto out;\n\t}\n\n\t \n\tif (!atomic_inc_not_zero(&anon_vma->refcount)) {\n\t\tanon_vma = NULL;\n\t\tgoto out;\n\t}\n\n\tif (!folio_mapped(folio)) {\n\t\trcu_read_unlock();\n\t\tput_anon_vma(anon_vma);\n\t\treturn NULL;\n\t}\n\n\t \n\trcu_read_unlock();\n\tanon_vma_lock_read(anon_vma);\n\n\tif (atomic_dec_and_test(&anon_vma->refcount)) {\n\t\t \n\t\tanon_vma_unlock_read(anon_vma);\n\t\t__put_anon_vma(anon_vma);\n\t\tanon_vma = NULL;\n\t}\n\n\treturn anon_vma;\n\nout:\n\trcu_read_unlock();\n\treturn anon_vma;\n}\n\n#ifdef CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH\n \nvoid try_to_unmap_flush(void)\n{\n\tstruct tlbflush_unmap_batch *tlb_ubc = &current->tlb_ubc;\n\n\tif (!tlb_ubc->flush_required)\n\t\treturn;\n\n\tarch_tlbbatch_flush(&tlb_ubc->arch);\n\ttlb_ubc->flush_required = false;\n\ttlb_ubc->writable = false;\n}\n\n \nvoid try_to_unmap_flush_dirty(void)\n{\n\tstruct tlbflush_unmap_batch *tlb_ubc = &current->tlb_ubc;\n\n\tif (tlb_ubc->writable)\n\t\ttry_to_unmap_flush();\n}\n\n \n#define TLB_FLUSH_BATCH_FLUSHED_SHIFT\t16\n#define TLB_FLUSH_BATCH_PENDING_MASK\t\t\t\\\n\t((1 << (TLB_FLUSH_BATCH_FLUSHED_SHIFT - 1)) - 1)\n#define TLB_FLUSH_BATCH_PENDING_LARGE\t\t\t\\\n\t(TLB_FLUSH_BATCH_PENDING_MASK / 2)\n\nstatic void set_tlb_ubc_flush_pending(struct mm_struct *mm, pte_t pteval,\n\t\t\t\t      unsigned long uaddr)\n{\n\tstruct tlbflush_unmap_batch *tlb_ubc = &current->tlb_ubc;\n\tint batch;\n\tbool writable = pte_dirty(pteval);\n\n\tif (!pte_accessible(mm, pteval))\n\t\treturn;\n\n\tarch_tlbbatch_add_pending(&tlb_ubc->arch, mm, uaddr);\n\ttlb_ubc->flush_required = true;\n\n\t \n\tbarrier();\n\tbatch = atomic_read(&mm->tlb_flush_batched);\nretry:\n\tif ((batch & TLB_FLUSH_BATCH_PENDING_MASK) > TLB_FLUSH_BATCH_PENDING_LARGE) {\n\t\t \n\t\tif (!atomic_try_cmpxchg(&mm->tlb_flush_batched, &batch, 1))\n\t\t\tgoto retry;\n\t} else {\n\t\tatomic_inc(&mm->tlb_flush_batched);\n\t}\n\n\t \n\tif (writable)\n\t\ttlb_ubc->writable = true;\n}\n\n \nstatic bool should_defer_flush(struct mm_struct *mm, enum ttu_flags flags)\n{\n\tif (!(flags & TTU_BATCH_FLUSH))\n\t\treturn false;\n\n\treturn arch_tlbbatch_should_defer(mm);\n}\n\n \nvoid flush_tlb_batched_pending(struct mm_struct *mm)\n{\n\tint batch = atomic_read(&mm->tlb_flush_batched);\n\tint pending = batch & TLB_FLUSH_BATCH_PENDING_MASK;\n\tint flushed = batch >> TLB_FLUSH_BATCH_FLUSHED_SHIFT;\n\n\tif (pending != flushed) {\n\t\tarch_flush_tlb_batched_pending(mm);\n\t\t \n\t\tatomic_cmpxchg(&mm->tlb_flush_batched, batch,\n\t\t\t       pending | (pending << TLB_FLUSH_BATCH_FLUSHED_SHIFT));\n\t}\n}\n#else\nstatic void set_tlb_ubc_flush_pending(struct mm_struct *mm, pte_t pteval,\n\t\t\t\t      unsigned long uaddr)\n{\n}\n\nstatic bool should_defer_flush(struct mm_struct *mm, enum ttu_flags flags)\n{\n\treturn false;\n}\n#endif  \n\n \nunsigned long page_address_in_vma(struct page *page, struct vm_area_struct *vma)\n{\n\tstruct folio *folio = page_folio(page);\n\tif (folio_test_anon(folio)) {\n\t\tstruct anon_vma *page__anon_vma = folio_anon_vma(folio);\n\t\t \n\t\tif (!vma->anon_vma || !page__anon_vma ||\n\t\t    vma->anon_vma->root != page__anon_vma->root)\n\t\t\treturn -EFAULT;\n\t} else if (!vma->vm_file) {\n\t\treturn -EFAULT;\n\t} else if (vma->vm_file->f_mapping != folio->mapping) {\n\t\treturn -EFAULT;\n\t}\n\n\treturn vma_address(page, vma);\n}\n\n \npmd_t *mm_find_pmd(struct mm_struct *mm, unsigned long address)\n{\n\tpgd_t *pgd;\n\tp4d_t *p4d;\n\tpud_t *pud;\n\tpmd_t *pmd = NULL;\n\n\tpgd = pgd_offset(mm, address);\n\tif (!pgd_present(*pgd))\n\t\tgoto out;\n\n\tp4d = p4d_offset(pgd, address);\n\tif (!p4d_present(*p4d))\n\t\tgoto out;\n\n\tpud = pud_offset(p4d, address);\n\tif (!pud_present(*pud))\n\t\tgoto out;\n\n\tpmd = pmd_offset(pud, address);\nout:\n\treturn pmd;\n}\n\nstruct folio_referenced_arg {\n\tint mapcount;\n\tint referenced;\n\tunsigned long vm_flags;\n\tstruct mem_cgroup *memcg;\n};\n \nstatic bool folio_referenced_one(struct folio *folio,\n\t\tstruct vm_area_struct *vma, unsigned long address, void *arg)\n{\n\tstruct folio_referenced_arg *pra = arg;\n\tDEFINE_FOLIO_VMA_WALK(pvmw, folio, vma, address, 0);\n\tint referenced = 0;\n\n\twhile (page_vma_mapped_walk(&pvmw)) {\n\t\taddress = pvmw.address;\n\n\t\tif ((vma->vm_flags & VM_LOCKED) &&\n\t\t    (!folio_test_large(folio) || !pvmw.pte)) {\n\t\t\t \n\t\t\tmlock_vma_folio(folio, vma, !pvmw.pte);\n\t\t\tpage_vma_mapped_walk_done(&pvmw);\n\t\t\tpra->vm_flags |= VM_LOCKED;\n\t\t\treturn false;  \n\t\t}\n\n\t\tif (pvmw.pte) {\n\t\t\tif (lru_gen_enabled() &&\n\t\t\t    pte_young(ptep_get(pvmw.pte))) {\n\t\t\t\tlru_gen_look_around(&pvmw);\n\t\t\t\treferenced++;\n\t\t\t}\n\n\t\t\tif (ptep_clear_flush_young_notify(vma, address,\n\t\t\t\t\t\tpvmw.pte))\n\t\t\t\treferenced++;\n\t\t} else if (IS_ENABLED(CONFIG_TRANSPARENT_HUGEPAGE)) {\n\t\t\tif (pmdp_clear_flush_young_notify(vma, address,\n\t\t\t\t\t\tpvmw.pmd))\n\t\t\t\treferenced++;\n\t\t} else {\n\t\t\t \n\t\t\tWARN_ON_ONCE(1);\n\t\t}\n\n\t\tpra->mapcount--;\n\t}\n\n\tif (referenced)\n\t\tfolio_clear_idle(folio);\n\tif (folio_test_clear_young(folio))\n\t\treferenced++;\n\n\tif (referenced) {\n\t\tpra->referenced++;\n\t\tpra->vm_flags |= vma->vm_flags & ~VM_LOCKED;\n\t}\n\n\tif (!pra->mapcount)\n\t\treturn false;  \n\n\treturn true;\n}\n\nstatic bool invalid_folio_referenced_vma(struct vm_area_struct *vma, void *arg)\n{\n\tstruct folio_referenced_arg *pra = arg;\n\tstruct mem_cgroup *memcg = pra->memcg;\n\n\t \n\tif (!vma_has_recency(vma))\n\t\treturn true;\n\n\t \n\tif (memcg && !mm_match_cgroup(vma->vm_mm, memcg))\n\t\treturn true;\n\n\treturn false;\n}\n\n \nint folio_referenced(struct folio *folio, int is_locked,\n\t\t     struct mem_cgroup *memcg, unsigned long *vm_flags)\n{\n\tint we_locked = 0;\n\tstruct folio_referenced_arg pra = {\n\t\t.mapcount = folio_mapcount(folio),\n\t\t.memcg = memcg,\n\t};\n\tstruct rmap_walk_control rwc = {\n\t\t.rmap_one = folio_referenced_one,\n\t\t.arg = (void *)&pra,\n\t\t.anon_lock = folio_lock_anon_vma_read,\n\t\t.try_lock = true,\n\t\t.invalid_vma = invalid_folio_referenced_vma,\n\t};\n\n\t*vm_flags = 0;\n\tif (!pra.mapcount)\n\t\treturn 0;\n\n\tif (!folio_raw_mapping(folio))\n\t\treturn 0;\n\n\tif (!is_locked && (!folio_test_anon(folio) || folio_test_ksm(folio))) {\n\t\twe_locked = folio_trylock(folio);\n\t\tif (!we_locked)\n\t\t\treturn 1;\n\t}\n\n\trmap_walk(folio, &rwc);\n\t*vm_flags = pra.vm_flags;\n\n\tif (we_locked)\n\t\tfolio_unlock(folio);\n\n\treturn rwc.contended ? -1 : pra.referenced;\n}\n\nstatic int page_vma_mkclean_one(struct page_vma_mapped_walk *pvmw)\n{\n\tint cleaned = 0;\n\tstruct vm_area_struct *vma = pvmw->vma;\n\tstruct mmu_notifier_range range;\n\tunsigned long address = pvmw->address;\n\n\t \n\tmmu_notifier_range_init(&range, MMU_NOTIFY_PROTECTION_PAGE, 0,\n\t\t\t\tvma->vm_mm, address, vma_address_end(pvmw));\n\tmmu_notifier_invalidate_range_start(&range);\n\n\twhile (page_vma_mapped_walk(pvmw)) {\n\t\tint ret = 0;\n\n\t\taddress = pvmw->address;\n\t\tif (pvmw->pte) {\n\t\t\tpte_t *pte = pvmw->pte;\n\t\t\tpte_t entry = ptep_get(pte);\n\n\t\t\tif (!pte_dirty(entry) && !pte_write(entry))\n\t\t\t\tcontinue;\n\n\t\t\tflush_cache_page(vma, address, pte_pfn(entry));\n\t\t\tentry = ptep_clear_flush(vma, address, pte);\n\t\t\tentry = pte_wrprotect(entry);\n\t\t\tentry = pte_mkclean(entry);\n\t\t\tset_pte_at(vma->vm_mm, address, pte, entry);\n\t\t\tret = 1;\n\t\t} else {\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\t\t\tpmd_t *pmd = pvmw->pmd;\n\t\t\tpmd_t entry;\n\n\t\t\tif (!pmd_dirty(*pmd) && !pmd_write(*pmd))\n\t\t\t\tcontinue;\n\n\t\t\tflush_cache_range(vma, address,\n\t\t\t\t\t  address + HPAGE_PMD_SIZE);\n\t\t\tentry = pmdp_invalidate(vma, address, pmd);\n\t\t\tentry = pmd_wrprotect(entry);\n\t\t\tentry = pmd_mkclean(entry);\n\t\t\tset_pmd_at(vma->vm_mm, address, pmd, entry);\n\t\t\tret = 1;\n#else\n\t\t\t \n\t\t\tWARN_ON_ONCE(1);\n#endif\n\t\t}\n\n\t\tif (ret)\n\t\t\tcleaned++;\n\t}\n\n\tmmu_notifier_invalidate_range_end(&range);\n\n\treturn cleaned;\n}\n\nstatic bool page_mkclean_one(struct folio *folio, struct vm_area_struct *vma,\n\t\t\t     unsigned long address, void *arg)\n{\n\tDEFINE_FOLIO_VMA_WALK(pvmw, folio, vma, address, PVMW_SYNC);\n\tint *cleaned = arg;\n\n\t*cleaned += page_vma_mkclean_one(&pvmw);\n\n\treturn true;\n}\n\nstatic bool invalid_mkclean_vma(struct vm_area_struct *vma, void *arg)\n{\n\tif (vma->vm_flags & VM_SHARED)\n\t\treturn false;\n\n\treturn true;\n}\n\nint folio_mkclean(struct folio *folio)\n{\n\tint cleaned = 0;\n\tstruct address_space *mapping;\n\tstruct rmap_walk_control rwc = {\n\t\t.arg = (void *)&cleaned,\n\t\t.rmap_one = page_mkclean_one,\n\t\t.invalid_vma = invalid_mkclean_vma,\n\t};\n\n\tBUG_ON(!folio_test_locked(folio));\n\n\tif (!folio_mapped(folio))\n\t\treturn 0;\n\n\tmapping = folio_mapping(folio);\n\tif (!mapping)\n\t\treturn 0;\n\n\trmap_walk(folio, &rwc);\n\n\treturn cleaned;\n}\nEXPORT_SYMBOL_GPL(folio_mkclean);\n\n \nint pfn_mkclean_range(unsigned long pfn, unsigned long nr_pages, pgoff_t pgoff,\n\t\t      struct vm_area_struct *vma)\n{\n\tstruct page_vma_mapped_walk pvmw = {\n\t\t.pfn\t\t= pfn,\n\t\t.nr_pages\t= nr_pages,\n\t\t.pgoff\t\t= pgoff,\n\t\t.vma\t\t= vma,\n\t\t.flags\t\t= PVMW_SYNC,\n\t};\n\n\tif (invalid_mkclean_vma(vma, NULL))\n\t\treturn 0;\n\n\tpvmw.address = vma_pgoff_address(pgoff, nr_pages, vma);\n\tVM_BUG_ON_VMA(pvmw.address == -EFAULT, vma);\n\n\treturn page_vma_mkclean_one(&pvmw);\n}\n\nint folio_total_mapcount(struct folio *folio)\n{\n\tint mapcount = folio_entire_mapcount(folio);\n\tint nr_pages;\n\tint i;\n\n\t \n\tif (folio_nr_pages_mapped(folio) == 0)\n\t\treturn mapcount;\n\t \n\tnr_pages = folio_nr_pages(folio);\n\tfor (i = 0; i < nr_pages; i++)\n\t\tmapcount += atomic_read(&folio_page(folio, i)->_mapcount);\n\n\t \n\tmapcount += nr_pages;\n\treturn mapcount;\n}\n\n \nvoid page_move_anon_rmap(struct page *page, struct vm_area_struct *vma)\n{\n\tvoid *anon_vma = vma->anon_vma;\n\tstruct folio *folio = page_folio(page);\n\n\tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n\tVM_BUG_ON_VMA(!anon_vma, vma);\n\n\tanon_vma += PAGE_MAPPING_ANON;\n\t \n\tWRITE_ONCE(folio->mapping, anon_vma);\n\tSetPageAnonExclusive(page);\n}\n\n \nstatic void __page_set_anon_rmap(struct folio *folio, struct page *page,\n\tstruct vm_area_struct *vma, unsigned long address, int exclusive)\n{\n\tstruct anon_vma *anon_vma = vma->anon_vma;\n\n\tBUG_ON(!anon_vma);\n\n\tif (folio_test_anon(folio))\n\t\tgoto out;\n\n\t \n\tif (!exclusive)\n\t\tanon_vma = anon_vma->root;\n\n\t \n\tanon_vma = (void *) anon_vma + PAGE_MAPPING_ANON;\n\tWRITE_ONCE(folio->mapping, (struct address_space *) anon_vma);\n\tfolio->index = linear_page_index(vma, address);\nout:\n\tif (exclusive)\n\t\tSetPageAnonExclusive(page);\n}\n\n \nstatic void __page_check_anon_rmap(struct folio *folio, struct page *page,\n\tstruct vm_area_struct *vma, unsigned long address)\n{\n\t \n\tVM_BUG_ON_FOLIO(folio_anon_vma(folio)->root != vma->anon_vma->root,\n\t\t\tfolio);\n\tVM_BUG_ON_PAGE(page_to_pgoff(page) != linear_page_index(vma, address),\n\t\t       page);\n}\n\n \nvoid page_add_anon_rmap(struct page *page, struct vm_area_struct *vma,\n\t\tunsigned long address, rmap_t flags)\n{\n\tstruct folio *folio = page_folio(page);\n\tatomic_t *mapped = &folio->_nr_pages_mapped;\n\tint nr = 0, nr_pmdmapped = 0;\n\tbool compound = flags & RMAP_COMPOUND;\n\tbool first = true;\n\n\t \n\tif (likely(!compound)) {\n\t\tfirst = atomic_inc_and_test(&page->_mapcount);\n\t\tnr = first;\n\t\tif (first && folio_test_large(folio)) {\n\t\t\tnr = atomic_inc_return_relaxed(mapped);\n\t\t\tnr = (nr < COMPOUND_MAPPED);\n\t\t}\n\t} else if (folio_test_pmd_mappable(folio)) {\n\t\t \n\n\t\tfirst = atomic_inc_and_test(&folio->_entire_mapcount);\n\t\tif (first) {\n\t\t\tnr = atomic_add_return_relaxed(COMPOUND_MAPPED, mapped);\n\t\t\tif (likely(nr < COMPOUND_MAPPED + COMPOUND_MAPPED)) {\n\t\t\t\tnr_pmdmapped = folio_nr_pages(folio);\n\t\t\t\tnr = nr_pmdmapped - (nr & FOLIO_PAGES_MAPPED);\n\t\t\t\t \n\t\t\t\tif (unlikely(nr < 0))\n\t\t\t\t\tnr = 0;\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tnr = 0;\n\t\t\t}\n\t\t}\n\t}\n\n\tVM_BUG_ON_PAGE(!first && (flags & RMAP_EXCLUSIVE), page);\n\tVM_BUG_ON_PAGE(!first && PageAnonExclusive(page), page);\n\n\tif (nr_pmdmapped)\n\t\t__lruvec_stat_mod_folio(folio, NR_ANON_THPS, nr_pmdmapped);\n\tif (nr)\n\t\t__lruvec_stat_mod_folio(folio, NR_ANON_MAPPED, nr);\n\n\tif (likely(!folio_test_ksm(folio))) {\n\t\t \n\t\tif (first)\n\t\t\t__page_set_anon_rmap(folio, page, vma, address,\n\t\t\t\t\t     !!(flags & RMAP_EXCLUSIVE));\n\t\telse\n\t\t\t__page_check_anon_rmap(folio, page, vma, address);\n\t}\n\n\tmlock_vma_folio(folio, vma, compound);\n}\n\n \nvoid folio_add_new_anon_rmap(struct folio *folio, struct vm_area_struct *vma,\n\t\tunsigned long address)\n{\n\tint nr;\n\n\tVM_BUG_ON_VMA(address < vma->vm_start || address >= vma->vm_end, vma);\n\t__folio_set_swapbacked(folio);\n\n\tif (likely(!folio_test_pmd_mappable(folio))) {\n\t\t \n\t\tatomic_set(&folio->_mapcount, 0);\n\t\tnr = 1;\n\t} else {\n\t\t \n\t\tatomic_set(&folio->_entire_mapcount, 0);\n\t\tatomic_set(&folio->_nr_pages_mapped, COMPOUND_MAPPED);\n\t\tnr = folio_nr_pages(folio);\n\t\t__lruvec_stat_mod_folio(folio, NR_ANON_THPS, nr);\n\t}\n\n\t__lruvec_stat_mod_folio(folio, NR_ANON_MAPPED, nr);\n\t__page_set_anon_rmap(folio, &folio->page, vma, address, 1);\n}\n\n \nvoid folio_add_file_rmap_range(struct folio *folio, struct page *page,\n\t\t\tunsigned int nr_pages, struct vm_area_struct *vma,\n\t\t\tbool compound)\n{\n\tatomic_t *mapped = &folio->_nr_pages_mapped;\n\tunsigned int nr_pmdmapped = 0, first;\n\tint nr = 0;\n\n\tVM_WARN_ON_FOLIO(compound && !folio_test_pmd_mappable(folio), folio);\n\n\t \n\tif (likely(!compound)) {\n\t\tdo {\n\t\t\tfirst = atomic_inc_and_test(&page->_mapcount);\n\t\t\tif (first && folio_test_large(folio)) {\n\t\t\t\tfirst = atomic_inc_return_relaxed(mapped);\n\t\t\t\tfirst = (first < COMPOUND_MAPPED);\n\t\t\t}\n\n\t\t\tif (first)\n\t\t\t\tnr++;\n\t\t} while (page++, --nr_pages > 0);\n\t} else if (folio_test_pmd_mappable(folio)) {\n\t\t \n\n\t\tfirst = atomic_inc_and_test(&folio->_entire_mapcount);\n\t\tif (first) {\n\t\t\tnr = atomic_add_return_relaxed(COMPOUND_MAPPED, mapped);\n\t\t\tif (likely(nr < COMPOUND_MAPPED + COMPOUND_MAPPED)) {\n\t\t\t\tnr_pmdmapped = folio_nr_pages(folio);\n\t\t\t\tnr = nr_pmdmapped - (nr & FOLIO_PAGES_MAPPED);\n\t\t\t\t \n\t\t\t\tif (unlikely(nr < 0))\n\t\t\t\t\tnr = 0;\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tnr = 0;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (nr_pmdmapped)\n\t\t__lruvec_stat_mod_folio(folio, folio_test_swapbacked(folio) ?\n\t\t\tNR_SHMEM_PMDMAPPED : NR_FILE_PMDMAPPED, nr_pmdmapped);\n\tif (nr)\n\t\t__lruvec_stat_mod_folio(folio, NR_FILE_MAPPED, nr);\n\n\tmlock_vma_folio(folio, vma, compound);\n}\n\n \nvoid page_add_file_rmap(struct page *page, struct vm_area_struct *vma,\n\t\tbool compound)\n{\n\tstruct folio *folio = page_folio(page);\n\tunsigned int nr_pages;\n\n\tVM_WARN_ON_ONCE_PAGE(compound && !PageTransHuge(page), page);\n\n\tif (likely(!compound))\n\t\tnr_pages = 1;\n\telse\n\t\tnr_pages = folio_nr_pages(folio);\n\n\tfolio_add_file_rmap_range(folio, page, nr_pages, vma, compound);\n}\n\n \nvoid page_remove_rmap(struct page *page, struct vm_area_struct *vma,\n\t\tbool compound)\n{\n\tstruct folio *folio = page_folio(page);\n\tatomic_t *mapped = &folio->_nr_pages_mapped;\n\tint nr = 0, nr_pmdmapped = 0;\n\tbool last;\n\tenum node_stat_item idx;\n\n\tVM_BUG_ON_PAGE(compound && !PageHead(page), page);\n\n\t \n\tif (unlikely(folio_test_hugetlb(folio))) {\n\t\t \n\t\tatomic_dec(&folio->_entire_mapcount);\n\t\treturn;\n\t}\n\n\t \n\tif (likely(!compound)) {\n\t\tlast = atomic_add_negative(-1, &page->_mapcount);\n\t\tnr = last;\n\t\tif (last && folio_test_large(folio)) {\n\t\t\tnr = atomic_dec_return_relaxed(mapped);\n\t\t\tnr = (nr < COMPOUND_MAPPED);\n\t\t}\n\t} else if (folio_test_pmd_mappable(folio)) {\n\t\t \n\n\t\tlast = atomic_add_negative(-1, &folio->_entire_mapcount);\n\t\tif (last) {\n\t\t\tnr = atomic_sub_return_relaxed(COMPOUND_MAPPED, mapped);\n\t\t\tif (likely(nr < COMPOUND_MAPPED)) {\n\t\t\t\tnr_pmdmapped = folio_nr_pages(folio);\n\t\t\t\tnr = nr_pmdmapped - (nr & FOLIO_PAGES_MAPPED);\n\t\t\t\t \n\t\t\t\tif (unlikely(nr < 0))\n\t\t\t\t\tnr = 0;\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tnr = 0;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (nr_pmdmapped) {\n\t\tif (folio_test_anon(folio))\n\t\t\tidx = NR_ANON_THPS;\n\t\telse if (folio_test_swapbacked(folio))\n\t\t\tidx = NR_SHMEM_PMDMAPPED;\n\t\telse\n\t\t\tidx = NR_FILE_PMDMAPPED;\n\t\t__lruvec_stat_mod_folio(folio, idx, -nr_pmdmapped);\n\t}\n\tif (nr) {\n\t\tidx = folio_test_anon(folio) ? NR_ANON_MAPPED : NR_FILE_MAPPED;\n\t\t__lruvec_stat_mod_folio(folio, idx, -nr);\n\n\t\t \n\t\tif (folio_test_pmd_mappable(folio) && folio_test_anon(folio))\n\t\t\tif (!compound || nr < nr_pmdmapped)\n\t\t\t\tdeferred_split_folio(folio);\n\t}\n\n\t \n\n\tmunlock_vma_folio(folio, vma, compound);\n}\n\n \nstatic bool try_to_unmap_one(struct folio *folio, struct vm_area_struct *vma,\n\t\t     unsigned long address, void *arg)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tDEFINE_FOLIO_VMA_WALK(pvmw, folio, vma, address, 0);\n\tpte_t pteval;\n\tstruct page *subpage;\n\tbool anon_exclusive, ret = true;\n\tstruct mmu_notifier_range range;\n\tenum ttu_flags flags = (enum ttu_flags)(long)arg;\n\tunsigned long pfn;\n\tunsigned long hsz = 0;\n\n\t \n\tif (flags & TTU_SYNC)\n\t\tpvmw.flags = PVMW_SYNC;\n\n\tif (flags & TTU_SPLIT_HUGE_PMD)\n\t\tsplit_huge_pmd_address(vma, address, false, folio);\n\n\t \n\trange.end = vma_address_end(&pvmw);\n\tmmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, vma->vm_mm,\n\t\t\t\taddress, range.end);\n\tif (folio_test_hugetlb(folio)) {\n\t\t \n\t\tadjust_range_if_pmd_sharing_possible(vma, &range.start,\n\t\t\t\t\t\t     &range.end);\n\n\t\t \n\t\thsz = huge_page_size(hstate_vma(vma));\n\t}\n\tmmu_notifier_invalidate_range_start(&range);\n\n\twhile (page_vma_mapped_walk(&pvmw)) {\n\t\t \n\t\tVM_BUG_ON_FOLIO(!pvmw.pte, folio);\n\n\t\t \n\t\tif (!(flags & TTU_IGNORE_MLOCK) &&\n\t\t    (vma->vm_flags & VM_LOCKED)) {\n\t\t\t \n\t\t\tmlock_vma_folio(folio, vma, false);\n\t\t\tpage_vma_mapped_walk_done(&pvmw);\n\t\t\tret = false;\n\t\t\tbreak;\n\t\t}\n\n\t\tpfn = pte_pfn(ptep_get(pvmw.pte));\n\t\tsubpage = folio_page(folio, pfn - folio_pfn(folio));\n\t\taddress = pvmw.address;\n\t\tanon_exclusive = folio_test_anon(folio) &&\n\t\t\t\t PageAnonExclusive(subpage);\n\n\t\tif (folio_test_hugetlb(folio)) {\n\t\t\tbool anon = folio_test_anon(folio);\n\n\t\t\t \n\t\t\tVM_BUG_ON_PAGE(!PageHWPoison(subpage), subpage);\n\t\t\t \n\t\t\tflush_cache_range(vma, range.start, range.end);\n\n\t\t\t \n\t\t\tif (!anon) {\n\t\t\t\tVM_BUG_ON(!(flags & TTU_RMAP_LOCKED));\n\t\t\t\tif (!hugetlb_vma_trylock_write(vma)) {\n\t\t\t\t\tpage_vma_mapped_walk_done(&pvmw);\n\t\t\t\t\tret = false;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (huge_pmd_unshare(mm, vma, address, pvmw.pte)) {\n\t\t\t\t\thugetlb_vma_unlock_write(vma);\n\t\t\t\t\tflush_tlb_range(vma,\n\t\t\t\t\t\trange.start, range.end);\n\t\t\t\t\t \n\t\t\t\t\tpage_vma_mapped_walk_done(&pvmw);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\thugetlb_vma_unlock_write(vma);\n\t\t\t}\n\t\t\tpteval = huge_ptep_clear_flush(vma, address, pvmw.pte);\n\t\t} else {\n\t\t\tflush_cache_page(vma, address, pfn);\n\t\t\t \n\t\t\tif (should_defer_flush(mm, flags)) {\n\t\t\t\t \n\t\t\t\tpteval = ptep_get_and_clear(mm, address, pvmw.pte);\n\n\t\t\t\tset_tlb_ubc_flush_pending(mm, pteval, address);\n\t\t\t} else {\n\t\t\t\tpteval = ptep_clear_flush(vma, address, pvmw.pte);\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tpte_install_uffd_wp_if_needed(vma, address, pvmw.pte, pteval);\n\n\t\t \n\t\tif (pte_dirty(pteval))\n\t\t\tfolio_mark_dirty(folio);\n\n\t\t \n\t\tupdate_hiwater_rss(mm);\n\n\t\tif (PageHWPoison(subpage) && (flags & TTU_HWPOISON)) {\n\t\t\tpteval = swp_entry_to_pte(make_hwpoison_entry(subpage));\n\t\t\tif (folio_test_hugetlb(folio)) {\n\t\t\t\thugetlb_count_sub(folio_nr_pages(folio), mm);\n\t\t\t\tset_huge_pte_at(mm, address, pvmw.pte, pteval,\n\t\t\t\t\t\thsz);\n\t\t\t} else {\n\t\t\t\tdec_mm_counter(mm, mm_counter(&folio->page));\n\t\t\t\tset_pte_at(mm, address, pvmw.pte, pteval);\n\t\t\t}\n\n\t\t} else if (pte_unused(pteval) && !userfaultfd_armed(vma)) {\n\t\t\t \n\t\t\tdec_mm_counter(mm, mm_counter(&folio->page));\n\t\t} else if (folio_test_anon(folio)) {\n\t\t\tswp_entry_t entry = page_swap_entry(subpage);\n\t\t\tpte_t swp_pte;\n\t\t\t \n\t\t\tif (unlikely(folio_test_swapbacked(folio) !=\n\t\t\t\t\tfolio_test_swapcache(folio))) {\n\t\t\t\tWARN_ON_ONCE(1);\n\t\t\t\tret = false;\n\t\t\t\tpage_vma_mapped_walk_done(&pvmw);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (!folio_test_swapbacked(folio)) {\n\t\t\t\tint ref_count, map_count;\n\n\t\t\t\t \n\t\t\t\tsmp_mb();\n\n\t\t\t\tref_count = folio_ref_count(folio);\n\t\t\t\tmap_count = folio_mapcount(folio);\n\n\t\t\t\t \n\t\t\t\tsmp_rmb();\n\n\t\t\t\t \n\t\t\t\tif (ref_count == 1 + map_count &&\n\t\t\t\t    !folio_test_dirty(folio)) {\n\t\t\t\t\tdec_mm_counter(mm, MM_ANONPAGES);\n\t\t\t\t\tgoto discard;\n\t\t\t\t}\n\n\t\t\t\t \n\t\t\t\tset_pte_at(mm, address, pvmw.pte, pteval);\n\t\t\t\tfolio_set_swapbacked(folio);\n\t\t\t\tret = false;\n\t\t\t\tpage_vma_mapped_walk_done(&pvmw);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (swap_duplicate(entry) < 0) {\n\t\t\t\tset_pte_at(mm, address, pvmw.pte, pteval);\n\t\t\t\tret = false;\n\t\t\t\tpage_vma_mapped_walk_done(&pvmw);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (arch_unmap_one(mm, vma, address, pteval) < 0) {\n\t\t\t\tswap_free(entry);\n\t\t\t\tset_pte_at(mm, address, pvmw.pte, pteval);\n\t\t\t\tret = false;\n\t\t\t\tpage_vma_mapped_walk_done(&pvmw);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (anon_exclusive &&\n\t\t\t    page_try_share_anon_rmap(subpage)) {\n\t\t\t\tswap_free(entry);\n\t\t\t\tset_pte_at(mm, address, pvmw.pte, pteval);\n\t\t\t\tret = false;\n\t\t\t\tpage_vma_mapped_walk_done(&pvmw);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (list_empty(&mm->mmlist)) {\n\t\t\t\tspin_lock(&mmlist_lock);\n\t\t\t\tif (list_empty(&mm->mmlist))\n\t\t\t\t\tlist_add(&mm->mmlist, &init_mm.mmlist);\n\t\t\t\tspin_unlock(&mmlist_lock);\n\t\t\t}\n\t\t\tdec_mm_counter(mm, MM_ANONPAGES);\n\t\t\tinc_mm_counter(mm, MM_SWAPENTS);\n\t\t\tswp_pte = swp_entry_to_pte(entry);\n\t\t\tif (anon_exclusive)\n\t\t\t\tswp_pte = pte_swp_mkexclusive(swp_pte);\n\t\t\tif (pte_soft_dirty(pteval))\n\t\t\t\tswp_pte = pte_swp_mksoft_dirty(swp_pte);\n\t\t\tif (pte_uffd_wp(pteval))\n\t\t\t\tswp_pte = pte_swp_mkuffd_wp(swp_pte);\n\t\t\tset_pte_at(mm, address, pvmw.pte, swp_pte);\n\t\t} else {\n\t\t\t \n\t\t\tdec_mm_counter(mm, mm_counter_file(&folio->page));\n\t\t}\ndiscard:\n\t\tpage_remove_rmap(subpage, vma, folio_test_hugetlb(folio));\n\t\tif (vma->vm_flags & VM_LOCKED)\n\t\t\tmlock_drain_local();\n\t\tfolio_put(folio);\n\t}\n\n\tmmu_notifier_invalidate_range_end(&range);\n\n\treturn ret;\n}\n\nstatic bool invalid_migration_vma(struct vm_area_struct *vma, void *arg)\n{\n\treturn vma_is_temporary_stack(vma);\n}\n\nstatic int folio_not_mapped(struct folio *folio)\n{\n\treturn !folio_mapped(folio);\n}\n\n \nvoid try_to_unmap(struct folio *folio, enum ttu_flags flags)\n{\n\tstruct rmap_walk_control rwc = {\n\t\t.rmap_one = try_to_unmap_one,\n\t\t.arg = (void *)flags,\n\t\t.done = folio_not_mapped,\n\t\t.anon_lock = folio_lock_anon_vma_read,\n\t};\n\n\tif (flags & TTU_RMAP_LOCKED)\n\t\trmap_walk_locked(folio, &rwc);\n\telse\n\t\trmap_walk(folio, &rwc);\n}\n\n \nstatic bool try_to_migrate_one(struct folio *folio, struct vm_area_struct *vma,\n\t\t     unsigned long address, void *arg)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tDEFINE_FOLIO_VMA_WALK(pvmw, folio, vma, address, 0);\n\tpte_t pteval;\n\tstruct page *subpage;\n\tbool anon_exclusive, ret = true;\n\tstruct mmu_notifier_range range;\n\tenum ttu_flags flags = (enum ttu_flags)(long)arg;\n\tunsigned long pfn;\n\tunsigned long hsz = 0;\n\n\t \n\tif (flags & TTU_SYNC)\n\t\tpvmw.flags = PVMW_SYNC;\n\n\t \n\tif (flags & TTU_SPLIT_HUGE_PMD)\n\t\tsplit_huge_pmd_address(vma, address, true, folio);\n\n\t \n\trange.end = vma_address_end(&pvmw);\n\tmmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, vma->vm_mm,\n\t\t\t\taddress, range.end);\n\tif (folio_test_hugetlb(folio)) {\n\t\t \n\t\tadjust_range_if_pmd_sharing_possible(vma, &range.start,\n\t\t\t\t\t\t     &range.end);\n\n\t\t \n\t\thsz = huge_page_size(hstate_vma(vma));\n\t}\n\tmmu_notifier_invalidate_range_start(&range);\n\n\twhile (page_vma_mapped_walk(&pvmw)) {\n#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION\n\t\t \n\t\tif (!pvmw.pte) {\n\t\t\tsubpage = folio_page(folio,\n\t\t\t\tpmd_pfn(*pvmw.pmd) - folio_pfn(folio));\n\t\t\tVM_BUG_ON_FOLIO(folio_test_hugetlb(folio) ||\n\t\t\t\t\t!folio_test_pmd_mappable(folio), folio);\n\n\t\t\tif (set_pmd_migration_entry(&pvmw, subpage)) {\n\t\t\t\tret = false;\n\t\t\t\tpage_vma_mapped_walk_done(&pvmw);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n#endif\n\n\t\t \n\t\tVM_BUG_ON_FOLIO(!pvmw.pte, folio);\n\n\t\tpfn = pte_pfn(ptep_get(pvmw.pte));\n\n\t\tif (folio_is_zone_device(folio)) {\n\t\t\t \n\t\t\tVM_BUG_ON_FOLIO(folio_nr_pages(folio) > 1, folio);\n\t\t\tsubpage = &folio->page;\n\t\t} else {\n\t\t\tsubpage = folio_page(folio, pfn - folio_pfn(folio));\n\t\t}\n\t\taddress = pvmw.address;\n\t\tanon_exclusive = folio_test_anon(folio) &&\n\t\t\t\t PageAnonExclusive(subpage);\n\n\t\tif (folio_test_hugetlb(folio)) {\n\t\t\tbool anon = folio_test_anon(folio);\n\n\t\t\t \n\t\t\tflush_cache_range(vma, range.start, range.end);\n\n\t\t\t \n\t\t\tif (!anon) {\n\t\t\t\tVM_BUG_ON(!(flags & TTU_RMAP_LOCKED));\n\t\t\t\tif (!hugetlb_vma_trylock_write(vma)) {\n\t\t\t\t\tpage_vma_mapped_walk_done(&pvmw);\n\t\t\t\t\tret = false;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (huge_pmd_unshare(mm, vma, address, pvmw.pte)) {\n\t\t\t\t\thugetlb_vma_unlock_write(vma);\n\t\t\t\t\tflush_tlb_range(vma,\n\t\t\t\t\t\trange.start, range.end);\n\n\t\t\t\t\t \n\t\t\t\t\tpage_vma_mapped_walk_done(&pvmw);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\thugetlb_vma_unlock_write(vma);\n\t\t\t}\n\t\t\t \n\t\t\tpteval = huge_ptep_clear_flush(vma, address, pvmw.pte);\n\t\t} else {\n\t\t\tflush_cache_page(vma, address, pfn);\n\t\t\t \n\t\t\tif (should_defer_flush(mm, flags)) {\n\t\t\t\t \n\t\t\t\tpteval = ptep_get_and_clear(mm, address, pvmw.pte);\n\n\t\t\t\tset_tlb_ubc_flush_pending(mm, pteval, address);\n\t\t\t} else {\n\t\t\t\tpteval = ptep_clear_flush(vma, address, pvmw.pte);\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (pte_dirty(pteval))\n\t\t\tfolio_mark_dirty(folio);\n\n\t\t \n\t\tupdate_hiwater_rss(mm);\n\n\t\tif (folio_is_device_private(folio)) {\n\t\t\tunsigned long pfn = folio_pfn(folio);\n\t\t\tswp_entry_t entry;\n\t\t\tpte_t swp_pte;\n\n\t\t\tif (anon_exclusive)\n\t\t\t\tBUG_ON(page_try_share_anon_rmap(subpage));\n\n\t\t\t \n\t\t\tentry = pte_to_swp_entry(pteval);\n\t\t\tif (is_writable_device_private_entry(entry))\n\t\t\t\tentry = make_writable_migration_entry(pfn);\n\t\t\telse if (anon_exclusive)\n\t\t\t\tentry = make_readable_exclusive_migration_entry(pfn);\n\t\t\telse\n\t\t\t\tentry = make_readable_migration_entry(pfn);\n\t\t\tswp_pte = swp_entry_to_pte(entry);\n\n\t\t\t \n\t\t\tif (pte_swp_soft_dirty(pteval))\n\t\t\t\tswp_pte = pte_swp_mksoft_dirty(swp_pte);\n\t\t\tif (pte_swp_uffd_wp(pteval))\n\t\t\t\tswp_pte = pte_swp_mkuffd_wp(swp_pte);\n\t\t\tset_pte_at(mm, pvmw.address, pvmw.pte, swp_pte);\n\t\t\ttrace_set_migration_pte(pvmw.address, pte_val(swp_pte),\n\t\t\t\t\t\tcompound_order(&folio->page));\n\t\t\t \n\t\t} else if (PageHWPoison(subpage)) {\n\t\t\tpteval = swp_entry_to_pte(make_hwpoison_entry(subpage));\n\t\t\tif (folio_test_hugetlb(folio)) {\n\t\t\t\thugetlb_count_sub(folio_nr_pages(folio), mm);\n\t\t\t\tset_huge_pte_at(mm, address, pvmw.pte, pteval,\n\t\t\t\t\t\thsz);\n\t\t\t} else {\n\t\t\t\tdec_mm_counter(mm, mm_counter(&folio->page));\n\t\t\t\tset_pte_at(mm, address, pvmw.pte, pteval);\n\t\t\t}\n\n\t\t} else if (pte_unused(pteval) && !userfaultfd_armed(vma)) {\n\t\t\t \n\t\t\tdec_mm_counter(mm, mm_counter(&folio->page));\n\t\t} else {\n\t\t\tswp_entry_t entry;\n\t\t\tpte_t swp_pte;\n\n\t\t\tif (arch_unmap_one(mm, vma, address, pteval) < 0) {\n\t\t\t\tif (folio_test_hugetlb(folio))\n\t\t\t\t\tset_huge_pte_at(mm, address, pvmw.pte,\n\t\t\t\t\t\t\tpteval, hsz);\n\t\t\t\telse\n\t\t\t\t\tset_pte_at(mm, address, pvmw.pte, pteval);\n\t\t\t\tret = false;\n\t\t\t\tpage_vma_mapped_walk_done(&pvmw);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tVM_BUG_ON_PAGE(pte_write(pteval) && folio_test_anon(folio) &&\n\t\t\t\t       !anon_exclusive, subpage);\n\n\t\t\t \n\t\t\tif (anon_exclusive &&\n\t\t\t    page_try_share_anon_rmap(subpage)) {\n\t\t\t\tif (folio_test_hugetlb(folio))\n\t\t\t\t\tset_huge_pte_at(mm, address, pvmw.pte,\n\t\t\t\t\t\t\tpteval, hsz);\n\t\t\t\telse\n\t\t\t\t\tset_pte_at(mm, address, pvmw.pte, pteval);\n\t\t\t\tret = false;\n\t\t\t\tpage_vma_mapped_walk_done(&pvmw);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (pte_write(pteval))\n\t\t\t\tentry = make_writable_migration_entry(\n\t\t\t\t\t\t\tpage_to_pfn(subpage));\n\t\t\telse if (anon_exclusive)\n\t\t\t\tentry = make_readable_exclusive_migration_entry(\n\t\t\t\t\t\t\tpage_to_pfn(subpage));\n\t\t\telse\n\t\t\t\tentry = make_readable_migration_entry(\n\t\t\t\t\t\t\tpage_to_pfn(subpage));\n\t\t\tif (pte_young(pteval))\n\t\t\t\tentry = make_migration_entry_young(entry);\n\t\t\tif (pte_dirty(pteval))\n\t\t\t\tentry = make_migration_entry_dirty(entry);\n\t\t\tswp_pte = swp_entry_to_pte(entry);\n\t\t\tif (pte_soft_dirty(pteval))\n\t\t\t\tswp_pte = pte_swp_mksoft_dirty(swp_pte);\n\t\t\tif (pte_uffd_wp(pteval))\n\t\t\t\tswp_pte = pte_swp_mkuffd_wp(swp_pte);\n\t\t\tif (folio_test_hugetlb(folio))\n\t\t\t\tset_huge_pte_at(mm, address, pvmw.pte, swp_pte,\n\t\t\t\t\t\thsz);\n\t\t\telse\n\t\t\t\tset_pte_at(mm, address, pvmw.pte, swp_pte);\n\t\t\ttrace_set_migration_pte(address, pte_val(swp_pte),\n\t\t\t\t\t\tcompound_order(&folio->page));\n\t\t\t \n\t\t}\n\n\t\tpage_remove_rmap(subpage, vma, folio_test_hugetlb(folio));\n\t\tif (vma->vm_flags & VM_LOCKED)\n\t\t\tmlock_drain_local();\n\t\tfolio_put(folio);\n\t}\n\n\tmmu_notifier_invalidate_range_end(&range);\n\n\treturn ret;\n}\n\n \nvoid try_to_migrate(struct folio *folio, enum ttu_flags flags)\n{\n\tstruct rmap_walk_control rwc = {\n\t\t.rmap_one = try_to_migrate_one,\n\t\t.arg = (void *)flags,\n\t\t.done = folio_not_mapped,\n\t\t.anon_lock = folio_lock_anon_vma_read,\n\t};\n\n\t \n\tif (WARN_ON_ONCE(flags & ~(TTU_RMAP_LOCKED | TTU_SPLIT_HUGE_PMD |\n\t\t\t\t\tTTU_SYNC | TTU_BATCH_FLUSH)))\n\t\treturn;\n\n\tif (folio_is_zone_device(folio) &&\n\t    (!folio_is_device_private(folio) && !folio_is_device_coherent(folio)))\n\t\treturn;\n\n\t \n\tif (!folio_test_ksm(folio) && folio_test_anon(folio))\n\t\trwc.invalid_vma = invalid_migration_vma;\n\n\tif (flags & TTU_RMAP_LOCKED)\n\t\trmap_walk_locked(folio, &rwc);\n\telse\n\t\trmap_walk(folio, &rwc);\n}\n\n#ifdef CONFIG_DEVICE_PRIVATE\nstruct make_exclusive_args {\n\tstruct mm_struct *mm;\n\tunsigned long address;\n\tvoid *owner;\n\tbool valid;\n};\n\nstatic bool page_make_device_exclusive_one(struct folio *folio,\n\t\tstruct vm_area_struct *vma, unsigned long address, void *priv)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tDEFINE_FOLIO_VMA_WALK(pvmw, folio, vma, address, 0);\n\tstruct make_exclusive_args *args = priv;\n\tpte_t pteval;\n\tstruct page *subpage;\n\tbool ret = true;\n\tstruct mmu_notifier_range range;\n\tswp_entry_t entry;\n\tpte_t swp_pte;\n\tpte_t ptent;\n\n\tmmu_notifier_range_init_owner(&range, MMU_NOTIFY_EXCLUSIVE, 0,\n\t\t\t\t      vma->vm_mm, address, min(vma->vm_end,\n\t\t\t\t      address + folio_size(folio)),\n\t\t\t\t      args->owner);\n\tmmu_notifier_invalidate_range_start(&range);\n\n\twhile (page_vma_mapped_walk(&pvmw)) {\n\t\t \n\t\tVM_BUG_ON_FOLIO(!pvmw.pte, folio);\n\n\t\tptent = ptep_get(pvmw.pte);\n\t\tif (!pte_present(ptent)) {\n\t\t\tret = false;\n\t\t\tpage_vma_mapped_walk_done(&pvmw);\n\t\t\tbreak;\n\t\t}\n\n\t\tsubpage = folio_page(folio,\n\t\t\t\tpte_pfn(ptent) - folio_pfn(folio));\n\t\taddress = pvmw.address;\n\n\t\t \n\t\tflush_cache_page(vma, address, pte_pfn(ptent));\n\t\tpteval = ptep_clear_flush(vma, address, pvmw.pte);\n\n\t\t \n\t\tif (pte_dirty(pteval))\n\t\t\tfolio_mark_dirty(folio);\n\n\t\t \n\t\tif (args->mm == mm && args->address == address &&\n\t\t    pte_write(pteval))\n\t\t\targs->valid = true;\n\n\t\t \n\t\tif (pte_write(pteval))\n\t\t\tentry = make_writable_device_exclusive_entry(\n\t\t\t\t\t\t\tpage_to_pfn(subpage));\n\t\telse\n\t\t\tentry = make_readable_device_exclusive_entry(\n\t\t\t\t\t\t\tpage_to_pfn(subpage));\n\t\tswp_pte = swp_entry_to_pte(entry);\n\t\tif (pte_soft_dirty(pteval))\n\t\t\tswp_pte = pte_swp_mksoft_dirty(swp_pte);\n\t\tif (pte_uffd_wp(pteval))\n\t\t\tswp_pte = pte_swp_mkuffd_wp(swp_pte);\n\n\t\tset_pte_at(mm, address, pvmw.pte, swp_pte);\n\n\t\t \n\t\tpage_remove_rmap(subpage, vma, false);\n\t}\n\n\tmmu_notifier_invalidate_range_end(&range);\n\n\treturn ret;\n}\n\n \nstatic bool folio_make_device_exclusive(struct folio *folio,\n\t\tstruct mm_struct *mm, unsigned long address, void *owner)\n{\n\tstruct make_exclusive_args args = {\n\t\t.mm = mm,\n\t\t.address = address,\n\t\t.owner = owner,\n\t\t.valid = false,\n\t};\n\tstruct rmap_walk_control rwc = {\n\t\t.rmap_one = page_make_device_exclusive_one,\n\t\t.done = folio_not_mapped,\n\t\t.anon_lock = folio_lock_anon_vma_read,\n\t\t.arg = &args,\n\t};\n\n\t \n\tif (!folio_test_anon(folio))\n\t\treturn false;\n\n\trmap_walk(folio, &rwc);\n\n\treturn args.valid && !folio_mapcount(folio);\n}\n\n \nint make_device_exclusive_range(struct mm_struct *mm, unsigned long start,\n\t\t\t\tunsigned long end, struct page **pages,\n\t\t\t\tvoid *owner)\n{\n\tlong npages = (end - start) >> PAGE_SHIFT;\n\tlong i;\n\n\tnpages = get_user_pages_remote(mm, start, npages,\n\t\t\t\t       FOLL_GET | FOLL_WRITE | FOLL_SPLIT_PMD,\n\t\t\t\t       pages, NULL);\n\tif (npages < 0)\n\t\treturn npages;\n\n\tfor (i = 0; i < npages; i++, start += PAGE_SIZE) {\n\t\tstruct folio *folio = page_folio(pages[i]);\n\t\tif (PageTail(pages[i]) || !folio_trylock(folio)) {\n\t\t\tfolio_put(folio);\n\t\t\tpages[i] = NULL;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!folio_make_device_exclusive(folio, mm, start, owner)) {\n\t\t\tfolio_unlock(folio);\n\t\t\tfolio_put(folio);\n\t\t\tpages[i] = NULL;\n\t\t}\n\t}\n\n\treturn npages;\n}\nEXPORT_SYMBOL_GPL(make_device_exclusive_range);\n#endif\n\nvoid __put_anon_vma(struct anon_vma *anon_vma)\n{\n\tstruct anon_vma *root = anon_vma->root;\n\n\tanon_vma_free(anon_vma);\n\tif (root != anon_vma && atomic_dec_and_test(&root->refcount))\n\t\tanon_vma_free(root);\n}\n\nstatic struct anon_vma *rmap_walk_anon_lock(struct folio *folio,\n\t\t\t\t\t    struct rmap_walk_control *rwc)\n{\n\tstruct anon_vma *anon_vma;\n\n\tif (rwc->anon_lock)\n\t\treturn rwc->anon_lock(folio, rwc);\n\n\t \n\tanon_vma = folio_anon_vma(folio);\n\tif (!anon_vma)\n\t\treturn NULL;\n\n\tif (anon_vma_trylock_read(anon_vma))\n\t\tgoto out;\n\n\tif (rwc->try_lock) {\n\t\tanon_vma = NULL;\n\t\trwc->contended = true;\n\t\tgoto out;\n\t}\n\n\tanon_vma_lock_read(anon_vma);\nout:\n\treturn anon_vma;\n}\n\n \nstatic void rmap_walk_anon(struct folio *folio,\n\t\tstruct rmap_walk_control *rwc, bool locked)\n{\n\tstruct anon_vma *anon_vma;\n\tpgoff_t pgoff_start, pgoff_end;\n\tstruct anon_vma_chain *avc;\n\n\tif (locked) {\n\t\tanon_vma = folio_anon_vma(folio);\n\t\t \n\t\tVM_BUG_ON_FOLIO(!anon_vma, folio);\n\t} else {\n\t\tanon_vma = rmap_walk_anon_lock(folio, rwc);\n\t}\n\tif (!anon_vma)\n\t\treturn;\n\n\tpgoff_start = folio_pgoff(folio);\n\tpgoff_end = pgoff_start + folio_nr_pages(folio) - 1;\n\tanon_vma_interval_tree_foreach(avc, &anon_vma->rb_root,\n\t\t\tpgoff_start, pgoff_end) {\n\t\tstruct vm_area_struct *vma = avc->vma;\n\t\tunsigned long address = vma_address(&folio->page, vma);\n\n\t\tVM_BUG_ON_VMA(address == -EFAULT, vma);\n\t\tcond_resched();\n\n\t\tif (rwc->invalid_vma && rwc->invalid_vma(vma, rwc->arg))\n\t\t\tcontinue;\n\n\t\tif (!rwc->rmap_one(folio, vma, address, rwc->arg))\n\t\t\tbreak;\n\t\tif (rwc->done && rwc->done(folio))\n\t\t\tbreak;\n\t}\n\n\tif (!locked)\n\t\tanon_vma_unlock_read(anon_vma);\n}\n\n \nstatic void rmap_walk_file(struct folio *folio,\n\t\tstruct rmap_walk_control *rwc, bool locked)\n{\n\tstruct address_space *mapping = folio_mapping(folio);\n\tpgoff_t pgoff_start, pgoff_end;\n\tstruct vm_area_struct *vma;\n\n\t \n\tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n\n\tif (!mapping)\n\t\treturn;\n\n\tpgoff_start = folio_pgoff(folio);\n\tpgoff_end = pgoff_start + folio_nr_pages(folio) - 1;\n\tif (!locked) {\n\t\tif (i_mmap_trylock_read(mapping))\n\t\t\tgoto lookup;\n\n\t\tif (rwc->try_lock) {\n\t\t\trwc->contended = true;\n\t\t\treturn;\n\t\t}\n\n\t\ti_mmap_lock_read(mapping);\n\t}\nlookup:\n\tvma_interval_tree_foreach(vma, &mapping->i_mmap,\n\t\t\tpgoff_start, pgoff_end) {\n\t\tunsigned long address = vma_address(&folio->page, vma);\n\n\t\tVM_BUG_ON_VMA(address == -EFAULT, vma);\n\t\tcond_resched();\n\n\t\tif (rwc->invalid_vma && rwc->invalid_vma(vma, rwc->arg))\n\t\t\tcontinue;\n\n\t\tif (!rwc->rmap_one(folio, vma, address, rwc->arg))\n\t\t\tgoto done;\n\t\tif (rwc->done && rwc->done(folio))\n\t\t\tgoto done;\n\t}\n\ndone:\n\tif (!locked)\n\t\ti_mmap_unlock_read(mapping);\n}\n\nvoid rmap_walk(struct folio *folio, struct rmap_walk_control *rwc)\n{\n\tif (unlikely(folio_test_ksm(folio)))\n\t\trmap_walk_ksm(folio, rwc);\n\telse if (folio_test_anon(folio))\n\t\trmap_walk_anon(folio, rwc, false);\n\telse\n\t\trmap_walk_file(folio, rwc, false);\n}\n\n \nvoid rmap_walk_locked(struct folio *folio, struct rmap_walk_control *rwc)\n{\n\t \n\tVM_BUG_ON_FOLIO(folio_test_ksm(folio), folio);\n\tif (folio_test_anon(folio))\n\t\trmap_walk_anon(folio, rwc, true);\n\telse\n\t\trmap_walk_file(folio, rwc, true);\n}\n\n#ifdef CONFIG_HUGETLB_PAGE\n \nvoid hugepage_add_anon_rmap(struct page *page, struct vm_area_struct *vma,\n\t\t\t    unsigned long address, rmap_t flags)\n{\n\tstruct folio *folio = page_folio(page);\n\tstruct anon_vma *anon_vma = vma->anon_vma;\n\tint first;\n\n\tBUG_ON(!folio_test_locked(folio));\n\tBUG_ON(!anon_vma);\n\t \n\tfirst = atomic_inc_and_test(&folio->_entire_mapcount);\n\tVM_BUG_ON_PAGE(!first && (flags & RMAP_EXCLUSIVE), page);\n\tVM_BUG_ON_PAGE(!first && PageAnonExclusive(page), page);\n\tif (first)\n\t\t__page_set_anon_rmap(folio, page, vma, address,\n\t\t\t\t     !!(flags & RMAP_EXCLUSIVE));\n}\n\nvoid hugepage_add_new_anon_rmap(struct folio *folio,\n\t\t\tstruct vm_area_struct *vma, unsigned long address)\n{\n\tBUG_ON(address < vma->vm_start || address >= vma->vm_end);\n\t \n\tatomic_set(&folio->_entire_mapcount, 0);\n\tfolio_clear_hugetlb_restore_reserve(folio);\n\t__page_set_anon_rmap(folio, &folio->page, vma, address, 1);\n}\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}