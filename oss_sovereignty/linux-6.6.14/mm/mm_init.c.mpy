{
  "module_name": "mm_init.c",
  "hash_id": "4c46e7588efa7c217bfe986e4fe1a28d04b4844c69f33b388856b0d997050951",
  "original_prompt": "Ingested from linux-6.6.14/mm/mm_init.c",
  "human_readable_source": "\n \n#include <linux/kernel.h>\n#include <linux/init.h>\n#include <linux/kobject.h>\n#include <linux/export.h>\n#include <linux/memory.h>\n#include <linux/notifier.h>\n#include <linux/sched.h>\n#include <linux/mman.h>\n#include <linux/memblock.h>\n#include <linux/page-isolation.h>\n#include <linux/padata.h>\n#include <linux/nmi.h>\n#include <linux/buffer_head.h>\n#include <linux/kmemleak.h>\n#include <linux/kfence.h>\n#include <linux/page_ext.h>\n#include <linux/pti.h>\n#include <linux/pgtable.h>\n#include <linux/swap.h>\n#include <linux/cma.h>\n#include \"internal.h\"\n#include \"slab.h\"\n#include \"shuffle.h\"\n\n#include <asm/setup.h>\n\n#ifdef CONFIG_DEBUG_MEMORY_INIT\nint __meminitdata mminit_loglevel;\n\n \nvoid __init mminit_verify_zonelist(void)\n{\n\tint nid;\n\n\tif (mminit_loglevel < MMINIT_VERIFY)\n\t\treturn;\n\n\tfor_each_online_node(nid) {\n\t\tpg_data_t *pgdat = NODE_DATA(nid);\n\t\tstruct zone *zone;\n\t\tstruct zoneref *z;\n\t\tstruct zonelist *zonelist;\n\t\tint i, listid, zoneid;\n\n\t\tBUILD_BUG_ON(MAX_ZONELISTS > 2);\n\t\tfor (i = 0; i < MAX_ZONELISTS * MAX_NR_ZONES; i++) {\n\n\t\t\t \n\t\t\tzoneid = i % MAX_NR_ZONES;\n\t\t\tlistid = i / MAX_NR_ZONES;\n\t\t\tzonelist = &pgdat->node_zonelists[listid];\n\t\t\tzone = &pgdat->node_zones[zoneid];\n\t\t\tif (!populated_zone(zone))\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tprintk(KERN_DEBUG \"mminit::zonelist %s %d:%s = \",\n\t\t\t\tlistid > 0 ? \"thisnode\" : \"general\", nid,\n\t\t\t\tzone->name);\n\n\t\t\t \n\t\t\tfor_each_zone_zonelist(zone, z, zonelist, zoneid)\n\t\t\t\tpr_cont(\"%d:%s \", zone_to_nid(zone), zone->name);\n\t\t\tpr_cont(\"\\n\");\n\t\t}\n\t}\n}\n\nvoid __init mminit_verify_pageflags_layout(void)\n{\n\tint shift, width;\n\tunsigned long or_mask, add_mask;\n\n\tshift = BITS_PER_LONG;\n\twidth = shift - SECTIONS_WIDTH - NODES_WIDTH - ZONES_WIDTH\n\t\t- LAST_CPUPID_SHIFT - KASAN_TAG_WIDTH - LRU_GEN_WIDTH - LRU_REFS_WIDTH;\n\tmminit_dprintk(MMINIT_TRACE, \"pageflags_layout_widths\",\n\t\t\"Section %d Node %d Zone %d Lastcpupid %d Kasantag %d Gen %d Tier %d Flags %d\\n\",\n\t\tSECTIONS_WIDTH,\n\t\tNODES_WIDTH,\n\t\tZONES_WIDTH,\n\t\tLAST_CPUPID_WIDTH,\n\t\tKASAN_TAG_WIDTH,\n\t\tLRU_GEN_WIDTH,\n\t\tLRU_REFS_WIDTH,\n\t\tNR_PAGEFLAGS);\n\tmminit_dprintk(MMINIT_TRACE, \"pageflags_layout_shifts\",\n\t\t\"Section %d Node %d Zone %d Lastcpupid %d Kasantag %d\\n\",\n\t\tSECTIONS_SHIFT,\n\t\tNODES_SHIFT,\n\t\tZONES_SHIFT,\n\t\tLAST_CPUPID_SHIFT,\n\t\tKASAN_TAG_WIDTH);\n\tmminit_dprintk(MMINIT_TRACE, \"pageflags_layout_pgshifts\",\n\t\t\"Section %lu Node %lu Zone %lu Lastcpupid %lu Kasantag %lu\\n\",\n\t\t(unsigned long)SECTIONS_PGSHIFT,\n\t\t(unsigned long)NODES_PGSHIFT,\n\t\t(unsigned long)ZONES_PGSHIFT,\n\t\t(unsigned long)LAST_CPUPID_PGSHIFT,\n\t\t(unsigned long)KASAN_TAG_PGSHIFT);\n\tmminit_dprintk(MMINIT_TRACE, \"pageflags_layout_nodezoneid\",\n\t\t\"Node/Zone ID: %lu -> %lu\\n\",\n\t\t(unsigned long)(ZONEID_PGOFF + ZONEID_SHIFT),\n\t\t(unsigned long)ZONEID_PGOFF);\n\tmminit_dprintk(MMINIT_TRACE, \"pageflags_layout_usage\",\n\t\t\"location: %d -> %d layout %d -> %d unused %d -> %d page-flags\\n\",\n\t\tshift, width, width, NR_PAGEFLAGS, NR_PAGEFLAGS, 0);\n#ifdef NODE_NOT_IN_PAGE_FLAGS\n\tmminit_dprintk(MMINIT_TRACE, \"pageflags_layout_nodeflags\",\n\t\t\"Node not in page flags\");\n#endif\n#ifdef LAST_CPUPID_NOT_IN_PAGE_FLAGS\n\tmminit_dprintk(MMINIT_TRACE, \"pageflags_layout_nodeflags\",\n\t\t\"Last cpupid not in page flags\");\n#endif\n\n\tif (SECTIONS_WIDTH) {\n\t\tshift -= SECTIONS_WIDTH;\n\t\tBUG_ON(shift != SECTIONS_PGSHIFT);\n\t}\n\tif (NODES_WIDTH) {\n\t\tshift -= NODES_WIDTH;\n\t\tBUG_ON(shift != NODES_PGSHIFT);\n\t}\n\tif (ZONES_WIDTH) {\n\t\tshift -= ZONES_WIDTH;\n\t\tBUG_ON(shift != ZONES_PGSHIFT);\n\t}\n\n\t \n\tor_mask = (ZONES_MASK << ZONES_PGSHIFT) |\n\t\t\t(NODES_MASK << NODES_PGSHIFT) |\n\t\t\t(SECTIONS_MASK << SECTIONS_PGSHIFT);\n\tadd_mask = (ZONES_MASK << ZONES_PGSHIFT) +\n\t\t\t(NODES_MASK << NODES_PGSHIFT) +\n\t\t\t(SECTIONS_MASK << SECTIONS_PGSHIFT);\n\tBUG_ON(or_mask != add_mask);\n}\n\nstatic __init int set_mminit_loglevel(char *str)\n{\n\tget_option(&str, &mminit_loglevel);\n\treturn 0;\n}\nearly_param(\"mminit_loglevel\", set_mminit_loglevel);\n#endif  \n\nstruct kobject *mm_kobj;\n\n#ifdef CONFIG_SMP\ns32 vm_committed_as_batch = 32;\n\nvoid mm_compute_batch(int overcommit_policy)\n{\n\tu64 memsized_batch;\n\ts32 nr = num_present_cpus();\n\ts32 batch = max_t(s32, nr*2, 32);\n\tunsigned long ram_pages = totalram_pages();\n\n\t \n\tif (overcommit_policy == OVERCOMMIT_NEVER)\n\t\tmemsized_batch = min_t(u64, ram_pages/nr/256, INT_MAX);\n\telse\n\t\tmemsized_batch = min_t(u64, ram_pages/nr/4, INT_MAX);\n\n\tvm_committed_as_batch = max_t(s32, memsized_batch, batch);\n}\n\nstatic int __meminit mm_compute_batch_notifier(struct notifier_block *self,\n\t\t\t\t\tunsigned long action, void *arg)\n{\n\tswitch (action) {\n\tcase MEM_ONLINE:\n\tcase MEM_OFFLINE:\n\t\tmm_compute_batch(sysctl_overcommit_memory);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn NOTIFY_OK;\n}\n\nstatic int __init mm_compute_batch_init(void)\n{\n\tmm_compute_batch(sysctl_overcommit_memory);\n\thotplug_memory_notifier(mm_compute_batch_notifier, MM_COMPUTE_BATCH_PRI);\n\treturn 0;\n}\n\n__initcall(mm_compute_batch_init);\n\n#endif\n\nstatic int __init mm_sysfs_init(void)\n{\n\tmm_kobj = kobject_create_and_add(\"mm\", kernel_kobj);\n\tif (!mm_kobj)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\npostcore_initcall(mm_sysfs_init);\n\nstatic unsigned long arch_zone_lowest_possible_pfn[MAX_NR_ZONES] __initdata;\nstatic unsigned long arch_zone_highest_possible_pfn[MAX_NR_ZONES] __initdata;\nstatic unsigned long zone_movable_pfn[MAX_NUMNODES] __initdata;\n\nstatic unsigned long required_kernelcore __initdata;\nstatic unsigned long required_kernelcore_percent __initdata;\nstatic unsigned long required_movablecore __initdata;\nstatic unsigned long required_movablecore_percent __initdata;\n\nstatic unsigned long nr_kernel_pages __initdata;\nstatic unsigned long nr_all_pages __initdata;\nstatic unsigned long dma_reserve __initdata;\n\nstatic bool deferred_struct_pages __meminitdata;\n\nstatic DEFINE_PER_CPU(struct per_cpu_nodestat, boot_nodestats);\n\nstatic int __init cmdline_parse_core(char *p, unsigned long *core,\n\t\t\t\t     unsigned long *percent)\n{\n\tunsigned long long coremem;\n\tchar *endptr;\n\n\tif (!p)\n\t\treturn -EINVAL;\n\n\t \n\tcoremem = simple_strtoull(p, &endptr, 0);\n\tif (*endptr == '%') {\n\t\t \n\t\tWARN_ON(coremem > 100);\n\n\t\t*percent = coremem;\n\t} else {\n\t\tcoremem = memparse(p, &p);\n\t\t \n\t\tWARN_ON((coremem >> PAGE_SHIFT) > ULONG_MAX);\n\n\t\t*core = coremem >> PAGE_SHIFT;\n\t\t*percent = 0UL;\n\t}\n\treturn 0;\n}\n\nbool mirrored_kernelcore __initdata_memblock;\n\n \nstatic int __init cmdline_parse_kernelcore(char *p)\n{\n\t \n\tif (parse_option_str(p, \"mirror\")) {\n\t\tmirrored_kernelcore = true;\n\t\treturn 0;\n\t}\n\n\treturn cmdline_parse_core(p, &required_kernelcore,\n\t\t\t\t  &required_kernelcore_percent);\n}\nearly_param(\"kernelcore\", cmdline_parse_kernelcore);\n\n \nstatic int __init cmdline_parse_movablecore(char *p)\n{\n\treturn cmdline_parse_core(p, &required_movablecore,\n\t\t\t\t  &required_movablecore_percent);\n}\nearly_param(\"movablecore\", cmdline_parse_movablecore);\n\n \nstatic unsigned long __init early_calculate_totalpages(void)\n{\n\tunsigned long totalpages = 0;\n\tunsigned long start_pfn, end_pfn;\n\tint i, nid;\n\n\tfor_each_mem_pfn_range(i, MAX_NUMNODES, &start_pfn, &end_pfn, &nid) {\n\t\tunsigned long pages = end_pfn - start_pfn;\n\n\t\ttotalpages += pages;\n\t\tif (pages)\n\t\t\tnode_set_state(nid, N_MEMORY);\n\t}\n\treturn totalpages;\n}\n\n \nstatic void __init find_usable_zone_for_movable(void)\n{\n\tint zone_index;\n\tfor (zone_index = MAX_NR_ZONES - 1; zone_index >= 0; zone_index--) {\n\t\tif (zone_index == ZONE_MOVABLE)\n\t\t\tcontinue;\n\n\t\tif (arch_zone_highest_possible_pfn[zone_index] >\n\t\t\t\tarch_zone_lowest_possible_pfn[zone_index])\n\t\t\tbreak;\n\t}\n\n\tVM_BUG_ON(zone_index == -1);\n\tmovable_zone = zone_index;\n}\n\n \nstatic void __init find_zone_movable_pfns_for_nodes(void)\n{\n\tint i, nid;\n\tunsigned long usable_startpfn;\n\tunsigned long kernelcore_node, kernelcore_remaining;\n\t \n\tnodemask_t saved_node_state = node_states[N_MEMORY];\n\tunsigned long totalpages = early_calculate_totalpages();\n\tint usable_nodes = nodes_weight(node_states[N_MEMORY]);\n\tstruct memblock_region *r;\n\n\t \n\tfind_usable_zone_for_movable();\n\n\t \n\tif (movable_node_is_enabled()) {\n\t\tfor_each_mem_region(r) {\n\t\t\tif (!memblock_is_hotpluggable(r))\n\t\t\t\tcontinue;\n\n\t\t\tnid = memblock_get_region_node(r);\n\n\t\t\tusable_startpfn = PFN_DOWN(r->base);\n\t\t\tzone_movable_pfn[nid] = zone_movable_pfn[nid] ?\n\t\t\t\tmin(usable_startpfn, zone_movable_pfn[nid]) :\n\t\t\t\tusable_startpfn;\n\t\t}\n\n\t\tgoto out2;\n\t}\n\n\t \n\tif (mirrored_kernelcore) {\n\t\tbool mem_below_4gb_not_mirrored = false;\n\n\t\tif (!memblock_has_mirror()) {\n\t\t\tpr_warn(\"The system has no mirror memory, ignore kernelcore=mirror.\\n\");\n\t\t\tgoto out;\n\t\t}\n\n\t\tfor_each_mem_region(r) {\n\t\t\tif (memblock_is_mirror(r))\n\t\t\t\tcontinue;\n\n\t\t\tnid = memblock_get_region_node(r);\n\n\t\t\tusable_startpfn = memblock_region_memory_base_pfn(r);\n\n\t\t\tif (usable_startpfn < PHYS_PFN(SZ_4G)) {\n\t\t\t\tmem_below_4gb_not_mirrored = true;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tzone_movable_pfn[nid] = zone_movable_pfn[nid] ?\n\t\t\t\tmin(usable_startpfn, zone_movable_pfn[nid]) :\n\t\t\t\tusable_startpfn;\n\t\t}\n\n\t\tif (mem_below_4gb_not_mirrored)\n\t\t\tpr_warn(\"This configuration results in unmirrored kernel memory.\\n\");\n\n\t\tgoto out2;\n\t}\n\n\t \n\tif (required_kernelcore_percent)\n\t\trequired_kernelcore = (totalpages * 100 * required_kernelcore_percent) /\n\t\t\t\t       10000UL;\n\tif (required_movablecore_percent)\n\t\trequired_movablecore = (totalpages * 100 * required_movablecore_percent) /\n\t\t\t\t\t10000UL;\n\n\t \n\tif (required_movablecore) {\n\t\tunsigned long corepages;\n\n\t\t \n\t\trequired_movablecore =\n\t\t\troundup(required_movablecore, MAX_ORDER_NR_PAGES);\n\t\trequired_movablecore = min(totalpages, required_movablecore);\n\t\tcorepages = totalpages - required_movablecore;\n\n\t\trequired_kernelcore = max(required_kernelcore, corepages);\n\t}\n\n\t \n\tif (!required_kernelcore || required_kernelcore >= totalpages)\n\t\tgoto out;\n\n\t \n\tusable_startpfn = arch_zone_lowest_possible_pfn[movable_zone];\n\nrestart:\n\t \n\tkernelcore_node = required_kernelcore / usable_nodes;\n\tfor_each_node_state(nid, N_MEMORY) {\n\t\tunsigned long start_pfn, end_pfn;\n\n\t\t \n\t\tif (required_kernelcore < kernelcore_node)\n\t\t\tkernelcore_node = required_kernelcore / usable_nodes;\n\n\t\t \n\t\tkernelcore_remaining = kernelcore_node;\n\n\t\t \n\t\tfor_each_mem_pfn_range(i, nid, &start_pfn, &end_pfn, NULL) {\n\t\t\tunsigned long size_pages;\n\n\t\t\tstart_pfn = max(start_pfn, zone_movable_pfn[nid]);\n\t\t\tif (start_pfn >= end_pfn)\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tif (start_pfn < usable_startpfn) {\n\t\t\t\tunsigned long kernel_pages;\n\t\t\t\tkernel_pages = min(end_pfn, usable_startpfn)\n\t\t\t\t\t\t\t\t- start_pfn;\n\n\t\t\t\tkernelcore_remaining -= min(kernel_pages,\n\t\t\t\t\t\t\tkernelcore_remaining);\n\t\t\t\trequired_kernelcore -= min(kernel_pages,\n\t\t\t\t\t\t\trequired_kernelcore);\n\n\t\t\t\t \n\t\t\t\tif (end_pfn <= usable_startpfn) {\n\n\t\t\t\t\t \n\t\t\t\t\tzone_movable_pfn[nid] = end_pfn;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tstart_pfn = usable_startpfn;\n\t\t\t}\n\n\t\t\t \n\t\t\tsize_pages = end_pfn - start_pfn;\n\t\t\tif (size_pages > kernelcore_remaining)\n\t\t\t\tsize_pages = kernelcore_remaining;\n\t\t\tzone_movable_pfn[nid] = start_pfn + size_pages;\n\n\t\t\t \n\t\t\trequired_kernelcore -= min(required_kernelcore,\n\t\t\t\t\t\t\t\tsize_pages);\n\t\t\tkernelcore_remaining -= size_pages;\n\t\t\tif (!kernelcore_remaining)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\tusable_nodes--;\n\tif (usable_nodes && required_kernelcore > usable_nodes)\n\t\tgoto restart;\n\nout2:\n\t \n\tfor (nid = 0; nid < MAX_NUMNODES; nid++) {\n\t\tunsigned long start_pfn, end_pfn;\n\n\t\tzone_movable_pfn[nid] =\n\t\t\troundup(zone_movable_pfn[nid], MAX_ORDER_NR_PAGES);\n\n\t\tget_pfn_range_for_nid(nid, &start_pfn, &end_pfn);\n\t\tif (zone_movable_pfn[nid] >= end_pfn)\n\t\t\tzone_movable_pfn[nid] = 0;\n\t}\n\nout:\n\t \n\tnode_states[N_MEMORY] = saved_node_state;\n}\n\nstatic void __meminit __init_single_page(struct page *page, unsigned long pfn,\n\t\t\t\tunsigned long zone, int nid)\n{\n\tmm_zero_struct_page(page);\n\tset_page_links(page, zone, nid, pfn);\n\tinit_page_count(page);\n\tpage_mapcount_reset(page);\n\tpage_cpupid_reset_last(page);\n\tpage_kasan_tag_reset(page);\n\n\tINIT_LIST_HEAD(&page->lru);\n#ifdef WANT_PAGE_VIRTUAL\n\t \n\tif (!is_highmem_idx(zone))\n\t\tset_page_address(page, __va(pfn << PAGE_SHIFT));\n#endif\n}\n\n#ifdef CONFIG_NUMA\n \nstruct mminit_pfnnid_cache {\n\tunsigned long last_start;\n\tunsigned long last_end;\n\tint last_nid;\n};\n\nstatic struct mminit_pfnnid_cache early_pfnnid_cache __meminitdata;\n\n \nstatic int __meminit __early_pfn_to_nid(unsigned long pfn,\n\t\t\t\t\tstruct mminit_pfnnid_cache *state)\n{\n\tunsigned long start_pfn, end_pfn;\n\tint nid;\n\n\tif (state->last_start <= pfn && pfn < state->last_end)\n\t\treturn state->last_nid;\n\n\tnid = memblock_search_pfn_nid(pfn, &start_pfn, &end_pfn);\n\tif (nid != NUMA_NO_NODE) {\n\t\tstate->last_start = start_pfn;\n\t\tstate->last_end = end_pfn;\n\t\tstate->last_nid = nid;\n\t}\n\n\treturn nid;\n}\n\nint __meminit early_pfn_to_nid(unsigned long pfn)\n{\n\tstatic DEFINE_SPINLOCK(early_pfn_lock);\n\tint nid;\n\n\tspin_lock(&early_pfn_lock);\n\tnid = __early_pfn_to_nid(pfn, &early_pfnnid_cache);\n\tif (nid < 0)\n\t\tnid = first_online_node;\n\tspin_unlock(&early_pfn_lock);\n\n\treturn nid;\n}\n\nint hashdist = HASHDIST_DEFAULT;\n\nstatic int __init set_hashdist(char *str)\n{\n\tif (!str)\n\t\treturn 0;\n\thashdist = simple_strtoul(str, &str, 0);\n\treturn 1;\n}\n__setup(\"hashdist=\", set_hashdist);\n\nstatic inline void fixup_hashdist(void)\n{\n\tif (num_node_state(N_MEMORY) == 1)\n\t\thashdist = 0;\n}\n#else\nstatic inline void fixup_hashdist(void) {}\n#endif  \n\n#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT\nstatic inline void pgdat_set_deferred_range(pg_data_t *pgdat)\n{\n\tpgdat->first_deferred_pfn = ULONG_MAX;\n}\n\n \nstatic inline bool __meminit early_page_initialised(unsigned long pfn, int nid)\n{\n\tif (node_online(nid) && pfn >= NODE_DATA(nid)->first_deferred_pfn)\n\t\treturn false;\n\n\treturn true;\n}\n\n \nstatic bool __meminit\ndefer_init(int nid, unsigned long pfn, unsigned long end_pfn)\n{\n\tstatic unsigned long prev_end_pfn, nr_initialised;\n\n\tif (early_page_ext_enabled())\n\t\treturn false;\n\t \n\tif (prev_end_pfn != end_pfn) {\n\t\tprev_end_pfn = end_pfn;\n\t\tnr_initialised = 0;\n\t}\n\n\t \n\tif (end_pfn < pgdat_end_pfn(NODE_DATA(nid)))\n\t\treturn false;\n\n\tif (NODE_DATA(nid)->first_deferred_pfn != ULONG_MAX)\n\t\treturn true;\n\t \n\tnr_initialised++;\n\tif ((nr_initialised > PAGES_PER_SECTION) &&\n\t    (pfn & (PAGES_PER_SECTION - 1)) == 0) {\n\t\tNODE_DATA(nid)->first_deferred_pfn = pfn;\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic void __meminit init_reserved_page(unsigned long pfn, int nid)\n{\n\tpg_data_t *pgdat;\n\tint zid;\n\n\tif (early_page_initialised(pfn, nid))\n\t\treturn;\n\n\tpgdat = NODE_DATA(nid);\n\n\tfor (zid = 0; zid < MAX_NR_ZONES; zid++) {\n\t\tstruct zone *zone = &pgdat->node_zones[zid];\n\n\t\tif (zone_spans_pfn(zone, pfn))\n\t\t\tbreak;\n\t}\n\t__init_single_page(pfn_to_page(pfn), pfn, zid, nid);\n}\n#else\nstatic inline void pgdat_set_deferred_range(pg_data_t *pgdat) {}\n\nstatic inline bool early_page_initialised(unsigned long pfn, int nid)\n{\n\treturn true;\n}\n\nstatic inline bool defer_init(int nid, unsigned long pfn, unsigned long end_pfn)\n{\n\treturn false;\n}\n\nstatic inline void init_reserved_page(unsigned long pfn, int nid)\n{\n}\n#endif  \n\n \nvoid __meminit reserve_bootmem_region(phys_addr_t start,\n\t\t\t\t      phys_addr_t end, int nid)\n{\n\tunsigned long start_pfn = PFN_DOWN(start);\n\tunsigned long end_pfn = PFN_UP(end);\n\n\tfor (; start_pfn < end_pfn; start_pfn++) {\n\t\tif (pfn_valid(start_pfn)) {\n\t\t\tstruct page *page = pfn_to_page(start_pfn);\n\n\t\t\tinit_reserved_page(start_pfn, nid);\n\n\t\t\t \n\t\t\tINIT_LIST_HEAD(&page->lru);\n\n\t\t\t \n\t\t\t__SetPageReserved(page);\n\t\t}\n\t}\n}\n\n \nstatic bool __meminit\noverlap_memmap_init(unsigned long zone, unsigned long *pfn)\n{\n\tstatic struct memblock_region *r;\n\n\tif (mirrored_kernelcore && zone == ZONE_MOVABLE) {\n\t\tif (!r || *pfn >= memblock_region_memory_end_pfn(r)) {\n\t\t\tfor_each_mem_region(r) {\n\t\t\t\tif (*pfn < memblock_region_memory_end_pfn(r))\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (*pfn >= memblock_region_memory_base_pfn(r) &&\n\t\t    memblock_is_mirror(r)) {\n\t\t\t*pfn = memblock_region_memory_end_pfn(r);\n\t\t\treturn true;\n\t\t}\n\t}\n\treturn false;\n}\n\n \nstatic void __init init_unavailable_range(unsigned long spfn,\n\t\t\t\t\t  unsigned long epfn,\n\t\t\t\t\t  int zone, int node)\n{\n\tunsigned long pfn;\n\tu64 pgcnt = 0;\n\n\tfor (pfn = spfn; pfn < epfn; pfn++) {\n\t\tif (!pfn_valid(pageblock_start_pfn(pfn))) {\n\t\t\tpfn = pageblock_end_pfn(pfn) - 1;\n\t\t\tcontinue;\n\t\t}\n\t\t__init_single_page(pfn_to_page(pfn), pfn, zone, node);\n\t\t__SetPageReserved(pfn_to_page(pfn));\n\t\tpgcnt++;\n\t}\n\n\tif (pgcnt)\n\t\tpr_info(\"On node %d, zone %s: %lld pages in unavailable ranges\",\n\t\t\tnode, zone_names[zone], pgcnt);\n}\n\n \nvoid __meminit memmap_init_range(unsigned long size, int nid, unsigned long zone,\n\t\tunsigned long start_pfn, unsigned long zone_end_pfn,\n\t\tenum meminit_context context,\n\t\tstruct vmem_altmap *altmap, int migratetype)\n{\n\tunsigned long pfn, end_pfn = start_pfn + size;\n\tstruct page *page;\n\n\tif (highest_memmap_pfn < end_pfn - 1)\n\t\thighest_memmap_pfn = end_pfn - 1;\n\n#ifdef CONFIG_ZONE_DEVICE\n\t \n\tif (zone == ZONE_DEVICE) {\n\t\tif (!altmap)\n\t\t\treturn;\n\n\t\tif (start_pfn == altmap->base_pfn)\n\t\t\tstart_pfn += altmap->reserve;\n\t\tend_pfn = altmap->base_pfn + vmem_altmap_offset(altmap);\n\t}\n#endif\n\n\tfor (pfn = start_pfn; pfn < end_pfn; ) {\n\t\t \n\t\tif (context == MEMINIT_EARLY) {\n\t\t\tif (overlap_memmap_init(zone, &pfn))\n\t\t\t\tcontinue;\n\t\t\tif (defer_init(nid, pfn, zone_end_pfn)) {\n\t\t\t\tdeferred_struct_pages = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tpage = pfn_to_page(pfn);\n\t\t__init_single_page(page, pfn, zone, nid);\n\t\tif (context == MEMINIT_HOTPLUG)\n\t\t\t__SetPageReserved(page);\n\n\t\t \n\t\tif (pageblock_aligned(pfn)) {\n\t\t\tset_pageblock_migratetype(page, migratetype);\n\t\t\tcond_resched();\n\t\t}\n\t\tpfn++;\n\t}\n}\n\nstatic void __init memmap_init_zone_range(struct zone *zone,\n\t\t\t\t\t  unsigned long start_pfn,\n\t\t\t\t\t  unsigned long end_pfn,\n\t\t\t\t\t  unsigned long *hole_pfn)\n{\n\tunsigned long zone_start_pfn = zone->zone_start_pfn;\n\tunsigned long zone_end_pfn = zone_start_pfn + zone->spanned_pages;\n\tint nid = zone_to_nid(zone), zone_id = zone_idx(zone);\n\n\tstart_pfn = clamp(start_pfn, zone_start_pfn, zone_end_pfn);\n\tend_pfn = clamp(end_pfn, zone_start_pfn, zone_end_pfn);\n\n\tif (start_pfn >= end_pfn)\n\t\treturn;\n\n\tmemmap_init_range(end_pfn - start_pfn, nid, zone_id, start_pfn,\n\t\t\t  zone_end_pfn, MEMINIT_EARLY, NULL, MIGRATE_MOVABLE);\n\n\tif (*hole_pfn < start_pfn)\n\t\tinit_unavailable_range(*hole_pfn, start_pfn, zone_id, nid);\n\n\t*hole_pfn = end_pfn;\n}\n\nstatic void __init memmap_init(void)\n{\n\tunsigned long start_pfn, end_pfn;\n\tunsigned long hole_pfn = 0;\n\tint i, j, zone_id = 0, nid;\n\n\tfor_each_mem_pfn_range(i, MAX_NUMNODES, &start_pfn, &end_pfn, &nid) {\n\t\tstruct pglist_data *node = NODE_DATA(nid);\n\n\t\tfor (j = 0; j < MAX_NR_ZONES; j++) {\n\t\t\tstruct zone *zone = node->node_zones + j;\n\n\t\t\tif (!populated_zone(zone))\n\t\t\t\tcontinue;\n\n\t\t\tmemmap_init_zone_range(zone, start_pfn, end_pfn,\n\t\t\t\t\t       &hole_pfn);\n\t\t\tzone_id = j;\n\t\t}\n\t}\n\n#ifdef CONFIG_SPARSEMEM\n\t \n\tend_pfn = round_up(end_pfn, PAGES_PER_SECTION);\n\tif (hole_pfn < end_pfn)\n#endif\n\t\tinit_unavailable_range(hole_pfn, end_pfn, zone_id, nid);\n}\n\n#ifdef CONFIG_ZONE_DEVICE\nstatic void __ref __init_zone_device_page(struct page *page, unsigned long pfn,\n\t\t\t\t\t  unsigned long zone_idx, int nid,\n\t\t\t\t\t  struct dev_pagemap *pgmap)\n{\n\n\t__init_single_page(page, pfn, zone_idx, nid);\n\n\t \n\t__SetPageReserved(page);\n\n\t \n\tpage->pgmap = pgmap;\n\tpage->zone_device_data = NULL;\n\n\t \n\tif (pageblock_aligned(pfn)) {\n\t\tset_pageblock_migratetype(page, MIGRATE_MOVABLE);\n\t\tcond_resched();\n\t}\n\n\t \n\tif (pgmap->type == MEMORY_DEVICE_PRIVATE ||\n\t    pgmap->type == MEMORY_DEVICE_COHERENT)\n\t\tset_page_count(page, 0);\n}\n\n \nstatic inline unsigned long compound_nr_pages(struct vmem_altmap *altmap,\n\t\t\t\t\t      struct dev_pagemap *pgmap)\n{\n\tif (!vmemmap_can_optimize(altmap, pgmap))\n\t\treturn pgmap_vmemmap_nr(pgmap);\n\n\treturn VMEMMAP_RESERVE_NR * (PAGE_SIZE / sizeof(struct page));\n}\n\nstatic void __ref memmap_init_compound(struct page *head,\n\t\t\t\t       unsigned long head_pfn,\n\t\t\t\t       unsigned long zone_idx, int nid,\n\t\t\t\t       struct dev_pagemap *pgmap,\n\t\t\t\t       unsigned long nr_pages)\n{\n\tunsigned long pfn, end_pfn = head_pfn + nr_pages;\n\tunsigned int order = pgmap->vmemmap_shift;\n\n\t__SetPageHead(head);\n\tfor (pfn = head_pfn + 1; pfn < end_pfn; pfn++) {\n\t\tstruct page *page = pfn_to_page(pfn);\n\n\t\t__init_zone_device_page(page, pfn, zone_idx, nid, pgmap);\n\t\tprep_compound_tail(head, pfn - head_pfn);\n\t\tset_page_count(page, 0);\n\n\t\t \n\t\tif (pfn == head_pfn + 1)\n\t\t\tprep_compound_head(head, order);\n\t}\n}\n\nvoid __ref memmap_init_zone_device(struct zone *zone,\n\t\t\t\t   unsigned long start_pfn,\n\t\t\t\t   unsigned long nr_pages,\n\t\t\t\t   struct dev_pagemap *pgmap)\n{\n\tunsigned long pfn, end_pfn = start_pfn + nr_pages;\n\tstruct pglist_data *pgdat = zone->zone_pgdat;\n\tstruct vmem_altmap *altmap = pgmap_altmap(pgmap);\n\tunsigned int pfns_per_compound = pgmap_vmemmap_nr(pgmap);\n\tunsigned long zone_idx = zone_idx(zone);\n\tunsigned long start = jiffies;\n\tint nid = pgdat->node_id;\n\n\tif (WARN_ON_ONCE(!pgmap || zone_idx != ZONE_DEVICE))\n\t\treturn;\n\n\t \n\tif (altmap) {\n\t\tstart_pfn = altmap->base_pfn + vmem_altmap_offset(altmap);\n\t\tnr_pages = end_pfn - start_pfn;\n\t}\n\n\tfor (pfn = start_pfn; pfn < end_pfn; pfn += pfns_per_compound) {\n\t\tstruct page *page = pfn_to_page(pfn);\n\n\t\t__init_zone_device_page(page, pfn, zone_idx, nid, pgmap);\n\n\t\tif (pfns_per_compound == 1)\n\t\t\tcontinue;\n\n\t\tmemmap_init_compound(page, pfn, zone_idx, nid, pgmap,\n\t\t\t\t     compound_nr_pages(altmap, pgmap));\n\t}\n\n\tpr_debug(\"%s initialised %lu pages in %ums\\n\", __func__,\n\t\tnr_pages, jiffies_to_msecs(jiffies - start));\n}\n#endif\n\n \nstatic void __init adjust_zone_range_for_zone_movable(int nid,\n\t\t\t\t\tunsigned long zone_type,\n\t\t\t\t\tunsigned long node_end_pfn,\n\t\t\t\t\tunsigned long *zone_start_pfn,\n\t\t\t\t\tunsigned long *zone_end_pfn)\n{\n\t \n\tif (zone_movable_pfn[nid]) {\n\t\t \n\t\tif (zone_type == ZONE_MOVABLE) {\n\t\t\t*zone_start_pfn = zone_movable_pfn[nid];\n\t\t\t*zone_end_pfn = min(node_end_pfn,\n\t\t\t\tarch_zone_highest_possible_pfn[movable_zone]);\n\n\t\t \n\t\t} else if (!mirrored_kernelcore &&\n\t\t\t*zone_start_pfn < zone_movable_pfn[nid] &&\n\t\t\t*zone_end_pfn > zone_movable_pfn[nid]) {\n\t\t\t*zone_end_pfn = zone_movable_pfn[nid];\n\n\t\t \n\t\t} else if (*zone_start_pfn >= zone_movable_pfn[nid])\n\t\t\t*zone_start_pfn = *zone_end_pfn;\n\t}\n}\n\n \nunsigned long __init __absent_pages_in_range(int nid,\n\t\t\t\tunsigned long range_start_pfn,\n\t\t\t\tunsigned long range_end_pfn)\n{\n\tunsigned long nr_absent = range_end_pfn - range_start_pfn;\n\tunsigned long start_pfn, end_pfn;\n\tint i;\n\n\tfor_each_mem_pfn_range(i, nid, &start_pfn, &end_pfn, NULL) {\n\t\tstart_pfn = clamp(start_pfn, range_start_pfn, range_end_pfn);\n\t\tend_pfn = clamp(end_pfn, range_start_pfn, range_end_pfn);\n\t\tnr_absent -= end_pfn - start_pfn;\n\t}\n\treturn nr_absent;\n}\n\n \nunsigned long __init absent_pages_in_range(unsigned long start_pfn,\n\t\t\t\t\t\t\tunsigned long end_pfn)\n{\n\treturn __absent_pages_in_range(MAX_NUMNODES, start_pfn, end_pfn);\n}\n\n \nstatic unsigned long __init zone_absent_pages_in_node(int nid,\n\t\t\t\t\tunsigned long zone_type,\n\t\t\t\t\tunsigned long zone_start_pfn,\n\t\t\t\t\tunsigned long zone_end_pfn)\n{\n\tunsigned long nr_absent;\n\n\t \n\tif (zone_start_pfn == zone_end_pfn)\n\t\treturn 0;\n\n\tnr_absent = __absent_pages_in_range(nid, zone_start_pfn, zone_end_pfn);\n\n\t \n\tif (mirrored_kernelcore && zone_movable_pfn[nid]) {\n\t\tunsigned long start_pfn, end_pfn;\n\t\tstruct memblock_region *r;\n\n\t\tfor_each_mem_region(r) {\n\t\t\tstart_pfn = clamp(memblock_region_memory_base_pfn(r),\n\t\t\t\t\t  zone_start_pfn, zone_end_pfn);\n\t\t\tend_pfn = clamp(memblock_region_memory_end_pfn(r),\n\t\t\t\t\tzone_start_pfn, zone_end_pfn);\n\n\t\t\tif (zone_type == ZONE_MOVABLE &&\n\t\t\t    memblock_is_mirror(r))\n\t\t\t\tnr_absent += end_pfn - start_pfn;\n\n\t\t\tif (zone_type == ZONE_NORMAL &&\n\t\t\t    !memblock_is_mirror(r))\n\t\t\t\tnr_absent += end_pfn - start_pfn;\n\t\t}\n\t}\n\n\treturn nr_absent;\n}\n\n \nstatic unsigned long __init zone_spanned_pages_in_node(int nid,\n\t\t\t\t\tunsigned long zone_type,\n\t\t\t\t\tunsigned long node_start_pfn,\n\t\t\t\t\tunsigned long node_end_pfn,\n\t\t\t\t\tunsigned long *zone_start_pfn,\n\t\t\t\t\tunsigned long *zone_end_pfn)\n{\n\tunsigned long zone_low = arch_zone_lowest_possible_pfn[zone_type];\n\tunsigned long zone_high = arch_zone_highest_possible_pfn[zone_type];\n\n\t \n\t*zone_start_pfn = clamp(node_start_pfn, zone_low, zone_high);\n\t*zone_end_pfn = clamp(node_end_pfn, zone_low, zone_high);\n\tadjust_zone_range_for_zone_movable(nid, zone_type, node_end_pfn,\n\t\t\t\t\t   zone_start_pfn, zone_end_pfn);\n\n\t \n\tif (*zone_end_pfn < node_start_pfn || *zone_start_pfn > node_end_pfn)\n\t\treturn 0;\n\n\t \n\t*zone_end_pfn = min(*zone_end_pfn, node_end_pfn);\n\t*zone_start_pfn = max(*zone_start_pfn, node_start_pfn);\n\n\t \n\treturn *zone_end_pfn - *zone_start_pfn;\n}\n\nstatic void __init reset_memoryless_node_totalpages(struct pglist_data *pgdat)\n{\n\tstruct zone *z;\n\n\tfor (z = pgdat->node_zones; z < pgdat->node_zones + MAX_NR_ZONES; z++) {\n\t\tz->zone_start_pfn = 0;\n\t\tz->spanned_pages = 0;\n\t\tz->present_pages = 0;\n#if defined(CONFIG_MEMORY_HOTPLUG)\n\t\tz->present_early_pages = 0;\n#endif\n\t}\n\n\tpgdat->node_spanned_pages = 0;\n\tpgdat->node_present_pages = 0;\n\tpr_debug(\"On node %d totalpages: 0\\n\", pgdat->node_id);\n}\n\nstatic void __init calculate_node_totalpages(struct pglist_data *pgdat,\n\t\t\t\t\t\tunsigned long node_start_pfn,\n\t\t\t\t\t\tunsigned long node_end_pfn)\n{\n\tunsigned long realtotalpages = 0, totalpages = 0;\n\tenum zone_type i;\n\n\tfor (i = 0; i < MAX_NR_ZONES; i++) {\n\t\tstruct zone *zone = pgdat->node_zones + i;\n\t\tunsigned long zone_start_pfn, zone_end_pfn;\n\t\tunsigned long spanned, absent;\n\t\tunsigned long real_size;\n\n\t\tspanned = zone_spanned_pages_in_node(pgdat->node_id, i,\n\t\t\t\t\t\t     node_start_pfn,\n\t\t\t\t\t\t     node_end_pfn,\n\t\t\t\t\t\t     &zone_start_pfn,\n\t\t\t\t\t\t     &zone_end_pfn);\n\t\tabsent = zone_absent_pages_in_node(pgdat->node_id, i,\n\t\t\t\t\t\t   zone_start_pfn,\n\t\t\t\t\t\t   zone_end_pfn);\n\n\t\treal_size = spanned - absent;\n\n\t\tif (spanned)\n\t\t\tzone->zone_start_pfn = zone_start_pfn;\n\t\telse\n\t\t\tzone->zone_start_pfn = 0;\n\t\tzone->spanned_pages = spanned;\n\t\tzone->present_pages = real_size;\n#if defined(CONFIG_MEMORY_HOTPLUG)\n\t\tzone->present_early_pages = real_size;\n#endif\n\n\t\ttotalpages += spanned;\n\t\trealtotalpages += real_size;\n\t}\n\n\tpgdat->node_spanned_pages = totalpages;\n\tpgdat->node_present_pages = realtotalpages;\n\tpr_debug(\"On node %d totalpages: %lu\\n\", pgdat->node_id, realtotalpages);\n}\n\nstatic unsigned long __init calc_memmap_size(unsigned long spanned_pages,\n\t\t\t\t\t\tunsigned long present_pages)\n{\n\tunsigned long pages = spanned_pages;\n\n\t \n\tif (spanned_pages > present_pages + (present_pages >> 4) &&\n\t    IS_ENABLED(CONFIG_SPARSEMEM))\n\t\tpages = present_pages;\n\n\treturn PAGE_ALIGN(pages * sizeof(struct page)) >> PAGE_SHIFT;\n}\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\nstatic void pgdat_init_split_queue(struct pglist_data *pgdat)\n{\n\tstruct deferred_split *ds_queue = &pgdat->deferred_split_queue;\n\n\tspin_lock_init(&ds_queue->split_queue_lock);\n\tINIT_LIST_HEAD(&ds_queue->split_queue);\n\tds_queue->split_queue_len = 0;\n}\n#else\nstatic void pgdat_init_split_queue(struct pglist_data *pgdat) {}\n#endif\n\n#ifdef CONFIG_COMPACTION\nstatic void pgdat_init_kcompactd(struct pglist_data *pgdat)\n{\n\tinit_waitqueue_head(&pgdat->kcompactd_wait);\n}\n#else\nstatic void pgdat_init_kcompactd(struct pglist_data *pgdat) {}\n#endif\n\nstatic void __meminit pgdat_init_internals(struct pglist_data *pgdat)\n{\n\tint i;\n\n\tpgdat_resize_init(pgdat);\n\tpgdat_kswapd_lock_init(pgdat);\n\n\tpgdat_init_split_queue(pgdat);\n\tpgdat_init_kcompactd(pgdat);\n\n\tinit_waitqueue_head(&pgdat->kswapd_wait);\n\tinit_waitqueue_head(&pgdat->pfmemalloc_wait);\n\n\tfor (i = 0; i < NR_VMSCAN_THROTTLE; i++)\n\t\tinit_waitqueue_head(&pgdat->reclaim_wait[i]);\n\n\tpgdat_page_ext_init(pgdat);\n\tlruvec_init(&pgdat->__lruvec);\n}\n\nstatic void __meminit zone_init_internals(struct zone *zone, enum zone_type idx, int nid,\n\t\t\t\t\t\t\tunsigned long remaining_pages)\n{\n\tatomic_long_set(&zone->managed_pages, remaining_pages);\n\tzone_set_nid(zone, nid);\n\tzone->name = zone_names[idx];\n\tzone->zone_pgdat = NODE_DATA(nid);\n\tspin_lock_init(&zone->lock);\n\tzone_seqlock_init(zone);\n\tzone_pcp_init(zone);\n}\n\nstatic void __meminit zone_init_free_lists(struct zone *zone)\n{\n\tunsigned int order, t;\n\tfor_each_migratetype_order(order, t) {\n\t\tINIT_LIST_HEAD(&zone->free_area[order].free_list[t]);\n\t\tzone->free_area[order].nr_free = 0;\n\t}\n\n#ifdef CONFIG_UNACCEPTED_MEMORY\n\tINIT_LIST_HEAD(&zone->unaccepted_pages);\n#endif\n}\n\nvoid __meminit init_currently_empty_zone(struct zone *zone,\n\t\t\t\t\tunsigned long zone_start_pfn,\n\t\t\t\t\tunsigned long size)\n{\n\tstruct pglist_data *pgdat = zone->zone_pgdat;\n\tint zone_idx = zone_idx(zone) + 1;\n\n\tif (zone_idx > pgdat->nr_zones)\n\t\tpgdat->nr_zones = zone_idx;\n\n\tzone->zone_start_pfn = zone_start_pfn;\n\n\tmminit_dprintk(MMINIT_TRACE, \"memmap_init\",\n\t\t\t\"Initialising map node %d zone %lu pfns %lu -> %lu\\n\",\n\t\t\tpgdat->node_id,\n\t\t\t(unsigned long)zone_idx(zone),\n\t\t\tzone_start_pfn, (zone_start_pfn + size));\n\n\tzone_init_free_lists(zone);\n\tzone->initialized = 1;\n}\n\n#ifndef CONFIG_SPARSEMEM\n \nstatic unsigned long __init usemap_size(unsigned long zone_start_pfn, unsigned long zonesize)\n{\n\tunsigned long usemapsize;\n\n\tzonesize += zone_start_pfn & (pageblock_nr_pages-1);\n\tusemapsize = roundup(zonesize, pageblock_nr_pages);\n\tusemapsize = usemapsize >> pageblock_order;\n\tusemapsize *= NR_PAGEBLOCK_BITS;\n\tusemapsize = roundup(usemapsize, BITS_PER_LONG);\n\n\treturn usemapsize / BITS_PER_BYTE;\n}\n\nstatic void __ref setup_usemap(struct zone *zone)\n{\n\tunsigned long usemapsize = usemap_size(zone->zone_start_pfn,\n\t\t\t\t\t       zone->spanned_pages);\n\tzone->pageblock_flags = NULL;\n\tif (usemapsize) {\n\t\tzone->pageblock_flags =\n\t\t\tmemblock_alloc_node(usemapsize, SMP_CACHE_BYTES,\n\t\t\t\t\t    zone_to_nid(zone));\n\t\tif (!zone->pageblock_flags)\n\t\t\tpanic(\"Failed to allocate %ld bytes for zone %s pageblock flags on node %d\\n\",\n\t\t\t      usemapsize, zone->name, zone_to_nid(zone));\n\t}\n}\n#else\nstatic inline void setup_usemap(struct zone *zone) {}\n#endif  \n\n#ifdef CONFIG_HUGETLB_PAGE_SIZE_VARIABLE\n\n \nvoid __init set_pageblock_order(void)\n{\n\tunsigned int order = MAX_ORDER;\n\n\t \n\tif (pageblock_order)\n\t\treturn;\n\n\t \n\tif (HPAGE_SHIFT > PAGE_SHIFT && HUGETLB_PAGE_ORDER < order)\n\t\torder = HUGETLB_PAGE_ORDER;\n\n\t \n\tpageblock_order = order;\n}\n#else  \n\n \nvoid __init set_pageblock_order(void)\n{\n}\n\n#endif  \n\n \n#ifdef CONFIG_MEMORY_HOTPLUG\nvoid __ref free_area_init_core_hotplug(struct pglist_data *pgdat)\n{\n\tint nid = pgdat->node_id;\n\tenum zone_type z;\n\tint cpu;\n\n\tpgdat_init_internals(pgdat);\n\n\tif (pgdat->per_cpu_nodestats == &boot_nodestats)\n\t\tpgdat->per_cpu_nodestats = alloc_percpu(struct per_cpu_nodestat);\n\n\t \n\tpgdat->nr_zones = 0;\n\tpgdat->kswapd_order = 0;\n\tpgdat->kswapd_highest_zoneidx = 0;\n\tpgdat->node_start_pfn = 0;\n\tpgdat->node_present_pages = 0;\n\n\tfor_each_online_cpu(cpu) {\n\t\tstruct per_cpu_nodestat *p;\n\n\t\tp = per_cpu_ptr(pgdat->per_cpu_nodestats, cpu);\n\t\tmemset(p, 0, sizeof(*p));\n\t}\n\n\t \n\tfor (z = 0; z < MAX_NR_ZONES; z++) {\n\t\tstruct zone *zone = pgdat->node_zones + z;\n\n\t\tzone->present_pages = 0;\n\t\tzone_init_internals(zone, z, nid, 0);\n\t}\n}\n#endif\n\n \nstatic void __init free_area_init_core(struct pglist_data *pgdat)\n{\n\tenum zone_type j;\n\tint nid = pgdat->node_id;\n\n\tpgdat_init_internals(pgdat);\n\tpgdat->per_cpu_nodestats = &boot_nodestats;\n\n\tfor (j = 0; j < MAX_NR_ZONES; j++) {\n\t\tstruct zone *zone = pgdat->node_zones + j;\n\t\tunsigned long size, freesize, memmap_pages;\n\n\t\tsize = zone->spanned_pages;\n\t\tfreesize = zone->present_pages;\n\n\t\t \n\t\tmemmap_pages = calc_memmap_size(size, freesize);\n\t\tif (!is_highmem_idx(j)) {\n\t\t\tif (freesize >= memmap_pages) {\n\t\t\t\tfreesize -= memmap_pages;\n\t\t\t\tif (memmap_pages)\n\t\t\t\t\tpr_debug(\"  %s zone: %lu pages used for memmap\\n\",\n\t\t\t\t\t\t zone_names[j], memmap_pages);\n\t\t\t} else\n\t\t\t\tpr_warn(\"  %s zone: %lu memmap pages exceeds freesize %lu\\n\",\n\t\t\t\t\tzone_names[j], memmap_pages, freesize);\n\t\t}\n\n\t\t \n\t\tif (j == 0 && freesize > dma_reserve) {\n\t\t\tfreesize -= dma_reserve;\n\t\t\tpr_debug(\"  %s zone: %lu pages reserved\\n\", zone_names[0], dma_reserve);\n\t\t}\n\n\t\tif (!is_highmem_idx(j))\n\t\t\tnr_kernel_pages += freesize;\n\t\t \n\t\telse if (nr_kernel_pages > memmap_pages * 2)\n\t\t\tnr_kernel_pages -= memmap_pages;\n\t\tnr_all_pages += freesize;\n\n\t\t \n\t\tzone_init_internals(zone, j, nid, freesize);\n\n\t\tif (!size)\n\t\t\tcontinue;\n\n\t\tsetup_usemap(zone);\n\t\tinit_currently_empty_zone(zone, zone->zone_start_pfn, size);\n\t}\n}\n\nvoid __init *memmap_alloc(phys_addr_t size, phys_addr_t align,\n\t\t\t  phys_addr_t min_addr, int nid, bool exact_nid)\n{\n\tvoid *ptr;\n\n\tif (exact_nid)\n\t\tptr = memblock_alloc_exact_nid_raw(size, align, min_addr,\n\t\t\t\t\t\t   MEMBLOCK_ALLOC_ACCESSIBLE,\n\t\t\t\t\t\t   nid);\n\telse\n\t\tptr = memblock_alloc_try_nid_raw(size, align, min_addr,\n\t\t\t\t\t\t MEMBLOCK_ALLOC_ACCESSIBLE,\n\t\t\t\t\t\t nid);\n\n\tif (ptr && size > 0)\n\t\tpage_init_poison(ptr, size);\n\n\treturn ptr;\n}\n\n#ifdef CONFIG_FLATMEM\nstatic void __init alloc_node_mem_map(struct pglist_data *pgdat)\n{\n\tunsigned long __maybe_unused start = 0;\n\tunsigned long __maybe_unused offset = 0;\n\n\t \n\tif (!pgdat->node_spanned_pages)\n\t\treturn;\n\n\tstart = pgdat->node_start_pfn & ~(MAX_ORDER_NR_PAGES - 1);\n\toffset = pgdat->node_start_pfn - start;\n\t \n\tif (!pgdat->node_mem_map) {\n\t\tunsigned long size, end;\n\t\tstruct page *map;\n\n\t\t \n\t\tend = pgdat_end_pfn(pgdat);\n\t\tend = ALIGN(end, MAX_ORDER_NR_PAGES);\n\t\tsize =  (end - start) * sizeof(struct page);\n\t\tmap = memmap_alloc(size, SMP_CACHE_BYTES, MEMBLOCK_LOW_LIMIT,\n\t\t\t\t   pgdat->node_id, false);\n\t\tif (!map)\n\t\t\tpanic(\"Failed to allocate %ld bytes for node %d memory map\\n\",\n\t\t\t      size, pgdat->node_id);\n\t\tpgdat->node_mem_map = map + offset;\n\t}\n\tpr_debug(\"%s: node %d, pgdat %08lx, node_mem_map %08lx\\n\",\n\t\t\t\t__func__, pgdat->node_id, (unsigned long)pgdat,\n\t\t\t\t(unsigned long)pgdat->node_mem_map);\n#ifndef CONFIG_NUMA\n\t \n\tif (pgdat == NODE_DATA(0)) {\n\t\tmem_map = NODE_DATA(0)->node_mem_map;\n\t\tif (page_to_pfn(mem_map) != pgdat->node_start_pfn)\n\t\t\tmem_map -= offset;\n\t}\n#endif\n}\n#else\nstatic inline void alloc_node_mem_map(struct pglist_data *pgdat) { }\n#endif  \n\n \nvoid __init get_pfn_range_for_nid(unsigned int nid,\n\t\t\tunsigned long *start_pfn, unsigned long *end_pfn)\n{\n\tunsigned long this_start_pfn, this_end_pfn;\n\tint i;\n\n\t*start_pfn = -1UL;\n\t*end_pfn = 0;\n\n\tfor_each_mem_pfn_range(i, nid, &this_start_pfn, &this_end_pfn, NULL) {\n\t\t*start_pfn = min(*start_pfn, this_start_pfn);\n\t\t*end_pfn = max(*end_pfn, this_end_pfn);\n\t}\n\n\tif (*start_pfn == -1UL)\n\t\t*start_pfn = 0;\n}\n\nstatic void __init free_area_init_node(int nid)\n{\n\tpg_data_t *pgdat = NODE_DATA(nid);\n\tunsigned long start_pfn = 0;\n\tunsigned long end_pfn = 0;\n\n\t \n\tWARN_ON(pgdat->nr_zones || pgdat->kswapd_highest_zoneidx);\n\n\tget_pfn_range_for_nid(nid, &start_pfn, &end_pfn);\n\n\tpgdat->node_id = nid;\n\tpgdat->node_start_pfn = start_pfn;\n\tpgdat->per_cpu_nodestats = NULL;\n\n\tif (start_pfn != end_pfn) {\n\t\tpr_info(\"Initmem setup node %d [mem %#018Lx-%#018Lx]\\n\", nid,\n\t\t\t(u64)start_pfn << PAGE_SHIFT,\n\t\t\tend_pfn ? ((u64)end_pfn << PAGE_SHIFT) - 1 : 0);\n\n\t\tcalculate_node_totalpages(pgdat, start_pfn, end_pfn);\n\t} else {\n\t\tpr_info(\"Initmem setup node %d as memoryless\\n\", nid);\n\n\t\treset_memoryless_node_totalpages(pgdat);\n\t}\n\n\talloc_node_mem_map(pgdat);\n\tpgdat_set_deferred_range(pgdat);\n\n\tfree_area_init_core(pgdat);\n\tlru_gen_init_pgdat(pgdat);\n}\n\n \nstatic void __init check_for_memory(pg_data_t *pgdat)\n{\n\tenum zone_type zone_type;\n\n\tfor (zone_type = 0; zone_type <= ZONE_MOVABLE - 1; zone_type++) {\n\t\tstruct zone *zone = &pgdat->node_zones[zone_type];\n\t\tif (populated_zone(zone)) {\n\t\t\tif (IS_ENABLED(CONFIG_HIGHMEM))\n\t\t\t\tnode_set_state(pgdat->node_id, N_HIGH_MEMORY);\n\t\t\tif (zone_type <= ZONE_NORMAL)\n\t\t\t\tnode_set_state(pgdat->node_id, N_NORMAL_MEMORY);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\n#if MAX_NUMNODES > 1\n \nvoid __init setup_nr_node_ids(void)\n{\n\tunsigned int highest;\n\n\thighest = find_last_bit(node_possible_map.bits, MAX_NUMNODES);\n\tnr_node_ids = highest + 1;\n}\n#endif\n\n \nstatic bool arch_has_descending_max_zone_pfns(void)\n{\n\treturn IS_ENABLED(CONFIG_ARC) && !IS_ENABLED(CONFIG_ARC_HAS_PAE40);\n}\n\n \nvoid __init free_area_init(unsigned long *max_zone_pfn)\n{\n\tunsigned long start_pfn, end_pfn;\n\tint i, nid, zone;\n\tbool descending;\n\n\t \n\tmemset(arch_zone_lowest_possible_pfn, 0,\n\t\t\t\tsizeof(arch_zone_lowest_possible_pfn));\n\tmemset(arch_zone_highest_possible_pfn, 0,\n\t\t\t\tsizeof(arch_zone_highest_possible_pfn));\n\n\tstart_pfn = PHYS_PFN(memblock_start_of_DRAM());\n\tdescending = arch_has_descending_max_zone_pfns();\n\n\tfor (i = 0; i < MAX_NR_ZONES; i++) {\n\t\tif (descending)\n\t\t\tzone = MAX_NR_ZONES - i - 1;\n\t\telse\n\t\t\tzone = i;\n\n\t\tif (zone == ZONE_MOVABLE)\n\t\t\tcontinue;\n\n\t\tend_pfn = max(max_zone_pfn[zone], start_pfn);\n\t\tarch_zone_lowest_possible_pfn[zone] = start_pfn;\n\t\tarch_zone_highest_possible_pfn[zone] = end_pfn;\n\n\t\tstart_pfn = end_pfn;\n\t}\n\n\t \n\tmemset(zone_movable_pfn, 0, sizeof(zone_movable_pfn));\n\tfind_zone_movable_pfns_for_nodes();\n\n\t \n\tpr_info(\"Zone ranges:\\n\");\n\tfor (i = 0; i < MAX_NR_ZONES; i++) {\n\t\tif (i == ZONE_MOVABLE)\n\t\t\tcontinue;\n\t\tpr_info(\"  %-8s \", zone_names[i]);\n\t\tif (arch_zone_lowest_possible_pfn[i] ==\n\t\t\t\tarch_zone_highest_possible_pfn[i])\n\t\t\tpr_cont(\"empty\\n\");\n\t\telse\n\t\t\tpr_cont(\"[mem %#018Lx-%#018Lx]\\n\",\n\t\t\t\t(u64)arch_zone_lowest_possible_pfn[i]\n\t\t\t\t\t<< PAGE_SHIFT,\n\t\t\t\t((u64)arch_zone_highest_possible_pfn[i]\n\t\t\t\t\t<< PAGE_SHIFT) - 1);\n\t}\n\n\t \n\tpr_info(\"Movable zone start for each node\\n\");\n\tfor (i = 0; i < MAX_NUMNODES; i++) {\n\t\tif (zone_movable_pfn[i])\n\t\t\tpr_info(\"  Node %d: %#018Lx\\n\", i,\n\t\t\t       (u64)zone_movable_pfn[i] << PAGE_SHIFT);\n\t}\n\n\t \n\tpr_info(\"Early memory node ranges\\n\");\n\tfor_each_mem_pfn_range(i, MAX_NUMNODES, &start_pfn, &end_pfn, &nid) {\n\t\tpr_info(\"  node %3d: [mem %#018Lx-%#018Lx]\\n\", nid,\n\t\t\t(u64)start_pfn << PAGE_SHIFT,\n\t\t\t((u64)end_pfn << PAGE_SHIFT) - 1);\n\t\tsubsection_map_init(start_pfn, end_pfn - start_pfn);\n\t}\n\n\t \n\tmminit_verify_pageflags_layout();\n\tsetup_nr_node_ids();\n\tset_pageblock_order();\n\n\tfor_each_node(nid) {\n\t\tpg_data_t *pgdat;\n\n\t\tif (!node_online(nid)) {\n\t\t\tpr_info(\"Initializing node %d as memoryless\\n\", nid);\n\n\t\t\t \n\t\t\tpgdat = arch_alloc_nodedata(nid);\n\t\t\tif (!pgdat)\n\t\t\t\tpanic(\"Cannot allocate %zuB for node %d.\\n\",\n\t\t\t\t       sizeof(*pgdat), nid);\n\t\t\tarch_refresh_nodedata(nid, pgdat);\n\t\t\tfree_area_init_node(nid);\n\n\t\t\t \n\t\t\tcontinue;\n\t\t}\n\n\t\tpgdat = NODE_DATA(nid);\n\t\tfree_area_init_node(nid);\n\n\t\t \n\t\tif (pgdat->node_present_pages)\n\t\t\tnode_set_state(nid, N_MEMORY);\n\t\tcheck_for_memory(pgdat);\n\t}\n\n\tmemmap_init();\n\n\t \n\tfixup_hashdist();\n}\n\n \nunsigned long __init node_map_pfn_alignment(void)\n{\n\tunsigned long accl_mask = 0, last_end = 0;\n\tunsigned long start, end, mask;\n\tint last_nid = NUMA_NO_NODE;\n\tint i, nid;\n\n\tfor_each_mem_pfn_range(i, MAX_NUMNODES, &start, &end, &nid) {\n\t\tif (!start || last_nid < 0 || last_nid == nid) {\n\t\t\tlast_nid = nid;\n\t\t\tlast_end = end;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tmask = ~((1 << __ffs(start)) - 1);\n\t\twhile (mask && last_end <= (start & (mask << 1)))\n\t\t\tmask <<= 1;\n\n\t\t \n\t\taccl_mask |= mask;\n\t}\n\n\t \n\treturn ~accl_mask + 1;\n}\n\n#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT\nstatic void __init deferred_free_range(unsigned long pfn,\n\t\t\t\t       unsigned long nr_pages)\n{\n\tstruct page *page;\n\tunsigned long i;\n\n\tif (!nr_pages)\n\t\treturn;\n\n\tpage = pfn_to_page(pfn);\n\n\t \n\tif (nr_pages == MAX_ORDER_NR_PAGES && IS_MAX_ORDER_ALIGNED(pfn)) {\n\t\tfor (i = 0; i < nr_pages; i += pageblock_nr_pages)\n\t\t\tset_pageblock_migratetype(page + i, MIGRATE_MOVABLE);\n\t\t__free_pages_core(page, MAX_ORDER);\n\t\treturn;\n\t}\n\n\t \n\taccept_memory(PFN_PHYS(pfn), PFN_PHYS(pfn + nr_pages));\n\n\tfor (i = 0; i < nr_pages; i++, page++, pfn++) {\n\t\tif (pageblock_aligned(pfn))\n\t\t\tset_pageblock_migratetype(page, MIGRATE_MOVABLE);\n\t\t__free_pages_core(page, 0);\n\t}\n}\n\n \nstatic atomic_t pgdat_init_n_undone __initdata;\nstatic __initdata DECLARE_COMPLETION(pgdat_init_all_done_comp);\n\nstatic inline void __init pgdat_init_report_one_done(void)\n{\n\tif (atomic_dec_and_test(&pgdat_init_n_undone))\n\t\tcomplete(&pgdat_init_all_done_comp);\n}\n\n \nstatic inline bool __init deferred_pfn_valid(unsigned long pfn)\n{\n\tif (IS_MAX_ORDER_ALIGNED(pfn) && !pfn_valid(pfn))\n\t\treturn false;\n\treturn true;\n}\n\n \nstatic void __init deferred_free_pages(unsigned long pfn,\n\t\t\t\t       unsigned long end_pfn)\n{\n\tunsigned long nr_free = 0;\n\n\tfor (; pfn < end_pfn; pfn++) {\n\t\tif (!deferred_pfn_valid(pfn)) {\n\t\t\tdeferred_free_range(pfn - nr_free, nr_free);\n\t\t\tnr_free = 0;\n\t\t} else if (IS_MAX_ORDER_ALIGNED(pfn)) {\n\t\t\tdeferred_free_range(pfn - nr_free, nr_free);\n\t\t\tnr_free = 1;\n\t\t} else {\n\t\t\tnr_free++;\n\t\t}\n\t}\n\t \n\tdeferred_free_range(pfn - nr_free, nr_free);\n}\n\n \nstatic unsigned long  __init deferred_init_pages(struct zone *zone,\n\t\t\t\t\t\t unsigned long pfn,\n\t\t\t\t\t\t unsigned long end_pfn)\n{\n\tint nid = zone_to_nid(zone);\n\tunsigned long nr_pages = 0;\n\tint zid = zone_idx(zone);\n\tstruct page *page = NULL;\n\n\tfor (; pfn < end_pfn; pfn++) {\n\t\tif (!deferred_pfn_valid(pfn)) {\n\t\t\tpage = NULL;\n\t\t\tcontinue;\n\t\t} else if (!page || IS_MAX_ORDER_ALIGNED(pfn)) {\n\t\t\tpage = pfn_to_page(pfn);\n\t\t} else {\n\t\t\tpage++;\n\t\t}\n\t\t__init_single_page(page, pfn, zid, nid);\n\t\tnr_pages++;\n\t}\n\treturn (nr_pages);\n}\n\n \nstatic bool __init\ndeferred_init_mem_pfn_range_in_zone(u64 *i, struct zone *zone,\n\t\t\t\t    unsigned long *spfn, unsigned long *epfn,\n\t\t\t\t    unsigned long first_init_pfn)\n{\n\tu64 j;\n\n\t \n\tfor_each_free_mem_pfn_range_in_zone(j, zone, spfn, epfn) {\n\t\tif (*epfn <= first_init_pfn)\n\t\t\tcontinue;\n\t\tif (*spfn < first_init_pfn)\n\t\t\t*spfn = first_init_pfn;\n\t\t*i = j;\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nstatic unsigned long __init\ndeferred_init_maxorder(u64 *i, struct zone *zone, unsigned long *start_pfn,\n\t\t       unsigned long *end_pfn)\n{\n\tunsigned long mo_pfn = ALIGN(*start_pfn + 1, MAX_ORDER_NR_PAGES);\n\tunsigned long spfn = *start_pfn, epfn = *end_pfn;\n\tunsigned long nr_pages = 0;\n\tu64 j = *i;\n\n\t \n\tfor_each_free_mem_pfn_range_in_zone_from(j, zone, start_pfn, end_pfn) {\n\t\tunsigned long t;\n\n\t\tif (mo_pfn <= *start_pfn)\n\t\t\tbreak;\n\n\t\tt = min(mo_pfn, *end_pfn);\n\t\tnr_pages += deferred_init_pages(zone, *start_pfn, t);\n\n\t\tif (mo_pfn < *end_pfn) {\n\t\t\t*start_pfn = mo_pfn;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\tswap(j, *i);\n\n\tfor_each_free_mem_pfn_range_in_zone_from(j, zone, &spfn, &epfn) {\n\t\tunsigned long t;\n\n\t\tif (mo_pfn <= spfn)\n\t\t\tbreak;\n\n\t\tt = min(mo_pfn, epfn);\n\t\tdeferred_free_pages(spfn, t);\n\n\t\tif (mo_pfn <= epfn)\n\t\t\tbreak;\n\t}\n\n\treturn nr_pages;\n}\n\nstatic void __init\ndeferred_init_memmap_chunk(unsigned long start_pfn, unsigned long end_pfn,\n\t\t\t   void *arg)\n{\n\tunsigned long spfn, epfn;\n\tstruct zone *zone = arg;\n\tu64 i;\n\n\tdeferred_init_mem_pfn_range_in_zone(&i, zone, &spfn, &epfn, start_pfn);\n\n\t \n\twhile (spfn < end_pfn) {\n\t\tdeferred_init_maxorder(&i, zone, &spfn, &epfn);\n\t\tcond_resched();\n\t}\n}\n\n \n__weak int __init\ndeferred_page_init_max_threads(const struct cpumask *node_cpumask)\n{\n\treturn 1;\n}\n\n \nstatic int __init deferred_init_memmap(void *data)\n{\n\tpg_data_t *pgdat = data;\n\tconst struct cpumask *cpumask = cpumask_of_node(pgdat->node_id);\n\tunsigned long spfn = 0, epfn = 0;\n\tunsigned long first_init_pfn, flags;\n\tunsigned long start = jiffies;\n\tstruct zone *zone;\n\tint zid, max_threads;\n\tu64 i;\n\n\t \n\tif (!cpumask_empty(cpumask))\n\t\tset_cpus_allowed_ptr(current, cpumask);\n\n\tpgdat_resize_lock(pgdat, &flags);\n\tfirst_init_pfn = pgdat->first_deferred_pfn;\n\tif (first_init_pfn == ULONG_MAX) {\n\t\tpgdat_resize_unlock(pgdat, &flags);\n\t\tpgdat_init_report_one_done();\n\t\treturn 0;\n\t}\n\n\t \n\tBUG_ON(pgdat->first_deferred_pfn < pgdat->node_start_pfn);\n\tBUG_ON(pgdat->first_deferred_pfn > pgdat_end_pfn(pgdat));\n\tpgdat->first_deferred_pfn = ULONG_MAX;\n\n\t \n\tpgdat_resize_unlock(pgdat, &flags);\n\n\t \n\tfor (zid = 0; zid < MAX_NR_ZONES; zid++) {\n\t\tzone = pgdat->node_zones + zid;\n\t\tif (first_init_pfn < zone_end_pfn(zone))\n\t\t\tbreak;\n\t}\n\n\t \n\tif (!deferred_init_mem_pfn_range_in_zone(&i, zone, &spfn, &epfn,\n\t\t\t\t\t\t first_init_pfn))\n\t\tgoto zone_empty;\n\n\tmax_threads = deferred_page_init_max_threads(cpumask);\n\n\twhile (spfn < epfn) {\n\t\tunsigned long epfn_align = ALIGN(epfn, PAGES_PER_SECTION);\n\t\tstruct padata_mt_job job = {\n\t\t\t.thread_fn   = deferred_init_memmap_chunk,\n\t\t\t.fn_arg      = zone,\n\t\t\t.start       = spfn,\n\t\t\t.size        = epfn_align - spfn,\n\t\t\t.align       = PAGES_PER_SECTION,\n\t\t\t.min_chunk   = PAGES_PER_SECTION,\n\t\t\t.max_threads = max_threads,\n\t\t};\n\n\t\tpadata_do_multithreaded(&job);\n\t\tdeferred_init_mem_pfn_range_in_zone(&i, zone, &spfn, &epfn,\n\t\t\t\t\t\t    epfn_align);\n\t}\nzone_empty:\n\t \n\tWARN_ON(++zid < MAX_NR_ZONES && populated_zone(++zone));\n\n\tpr_info(\"node %d deferred pages initialised in %ums\\n\",\n\t\tpgdat->node_id, jiffies_to_msecs(jiffies - start));\n\n\tpgdat_init_report_one_done();\n\treturn 0;\n}\n\n \nbool __init deferred_grow_zone(struct zone *zone, unsigned int order)\n{\n\tunsigned long nr_pages_needed = ALIGN(1 << order, PAGES_PER_SECTION);\n\tpg_data_t *pgdat = zone->zone_pgdat;\n\tunsigned long first_deferred_pfn = pgdat->first_deferred_pfn;\n\tunsigned long spfn, epfn, flags;\n\tunsigned long nr_pages = 0;\n\tu64 i;\n\n\t \n\tif (zone_end_pfn(zone) != pgdat_end_pfn(pgdat))\n\t\treturn false;\n\n\tpgdat_resize_lock(pgdat, &flags);\n\n\t \n\tif (first_deferred_pfn != pgdat->first_deferred_pfn) {\n\t\tpgdat_resize_unlock(pgdat, &flags);\n\t\treturn true;\n\t}\n\n\t \n\tif (!deferred_init_mem_pfn_range_in_zone(&i, zone, &spfn, &epfn,\n\t\t\t\t\t\t first_deferred_pfn)) {\n\t\tpgdat->first_deferred_pfn = ULONG_MAX;\n\t\tpgdat_resize_unlock(pgdat, &flags);\n\t\t \n\t\treturn first_deferred_pfn != ULONG_MAX;\n\t}\n\n\t \n\twhile (spfn < epfn) {\n\t\t \n\t\tfirst_deferred_pfn = spfn;\n\n\t\tnr_pages += deferred_init_maxorder(&i, zone, &spfn, &epfn);\n\t\ttouch_nmi_watchdog();\n\n\t\t \n\t\tif ((first_deferred_pfn ^ spfn) < PAGES_PER_SECTION)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (nr_pages >= nr_pages_needed)\n\t\t\tbreak;\n\t}\n\n\tpgdat->first_deferred_pfn = spfn;\n\tpgdat_resize_unlock(pgdat, &flags);\n\n\treturn nr_pages > 0;\n}\n\n#endif  \n\n#ifdef CONFIG_CMA\nvoid __init init_cma_reserved_pageblock(struct page *page)\n{\n\tunsigned i = pageblock_nr_pages;\n\tstruct page *p = page;\n\n\tdo {\n\t\t__ClearPageReserved(p);\n\t\tset_page_count(p, 0);\n\t} while (++p, --i);\n\n\tset_pageblock_migratetype(page, MIGRATE_CMA);\n\tset_page_refcounted(page);\n\t__free_pages(page, pageblock_order);\n\n\tadjust_managed_page_count(page, pageblock_nr_pages);\n\tpage_zone(page)->cma_pages += pageblock_nr_pages;\n}\n#endif\n\nvoid set_zone_contiguous(struct zone *zone)\n{\n\tunsigned long block_start_pfn = zone->zone_start_pfn;\n\tunsigned long block_end_pfn;\n\n\tblock_end_pfn = pageblock_end_pfn(block_start_pfn);\n\tfor (; block_start_pfn < zone_end_pfn(zone);\n\t\t\tblock_start_pfn = block_end_pfn,\n\t\t\t block_end_pfn += pageblock_nr_pages) {\n\n\t\tblock_end_pfn = min(block_end_pfn, zone_end_pfn(zone));\n\n\t\tif (!__pageblock_pfn_to_page(block_start_pfn,\n\t\t\t\t\t     block_end_pfn, zone))\n\t\t\treturn;\n\t\tcond_resched();\n\t}\n\n\t \n\tzone->contiguous = true;\n}\n\nvoid __init page_alloc_init_late(void)\n{\n\tstruct zone *zone;\n\tint nid;\n\n#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT\n\n\t \n\tatomic_set(&pgdat_init_n_undone, num_node_state(N_MEMORY));\n\tfor_each_node_state(nid, N_MEMORY) {\n\t\tkthread_run(deferred_init_memmap, NODE_DATA(nid), \"pgdatinit%d\", nid);\n\t}\n\n\t \n\twait_for_completion(&pgdat_init_all_done_comp);\n\n\t \n\tstatic_branch_disable(&deferred_pages);\n\n\t \n\tfiles_maxfiles_init();\n#endif\n\n\tbuffer_init();\n\n\t \n\tmemblock_discard();\n\n\tfor_each_node_state(nid, N_MEMORY)\n\t\tshuffle_free_memory(NODE_DATA(nid));\n\n\tfor_each_populated_zone(zone)\n\t\tset_zone_contiguous(zone);\n\n\t \n\tif (deferred_struct_pages)\n\t\tpage_ext_init();\n\n\tpage_alloc_sysctl_init();\n}\n\n#ifndef __HAVE_ARCH_RESERVED_KERNEL_PAGES\n \nstatic unsigned long __init arch_reserved_kernel_pages(void)\n{\n\treturn 0;\n}\n#endif\n\n \n#if __BITS_PER_LONG > 32\n#define ADAPT_SCALE_BASE\t(64ul << 30)\n#define ADAPT_SCALE_SHIFT\t2\n#define ADAPT_SCALE_NPAGES\t(ADAPT_SCALE_BASE >> PAGE_SHIFT)\n#endif\n\n \nvoid *__init alloc_large_system_hash(const char *tablename,\n\t\t\t\t     unsigned long bucketsize,\n\t\t\t\t     unsigned long numentries,\n\t\t\t\t     int scale,\n\t\t\t\t     int flags,\n\t\t\t\t     unsigned int *_hash_shift,\n\t\t\t\t     unsigned int *_hash_mask,\n\t\t\t\t     unsigned long low_limit,\n\t\t\t\t     unsigned long high_limit)\n{\n\tunsigned long long max = high_limit;\n\tunsigned long log2qty, size;\n\tvoid *table;\n\tgfp_t gfp_flags;\n\tbool virt;\n\tbool huge;\n\n\t \n\tif (!numentries) {\n\t\t \n\t\tnumentries = nr_kernel_pages;\n\t\tnumentries -= arch_reserved_kernel_pages();\n\n\t\t \n\t\tif (PAGE_SIZE < SZ_1M)\n\t\t\tnumentries = round_up(numentries, SZ_1M / PAGE_SIZE);\n\n#if __BITS_PER_LONG > 32\n\t\tif (!high_limit) {\n\t\t\tunsigned long adapt;\n\n\t\t\tfor (adapt = ADAPT_SCALE_NPAGES; adapt < numentries;\n\t\t\t     adapt <<= ADAPT_SCALE_SHIFT)\n\t\t\t\tscale++;\n\t\t}\n#endif\n\n\t\t \n\t\tif (scale > PAGE_SHIFT)\n\t\t\tnumentries >>= (scale - PAGE_SHIFT);\n\t\telse\n\t\t\tnumentries <<= (PAGE_SHIFT - scale);\n\n\t\tif (unlikely((numentries * bucketsize) < PAGE_SIZE))\n\t\t\tnumentries = PAGE_SIZE / bucketsize;\n\t}\n\tnumentries = roundup_pow_of_two(numentries);\n\n\t \n\tif (max == 0) {\n\t\tmax = ((unsigned long long)nr_all_pages << PAGE_SHIFT) >> 4;\n\t\tdo_div(max, bucketsize);\n\t}\n\tmax = min(max, 0x80000000ULL);\n\n\tif (numentries < low_limit)\n\t\tnumentries = low_limit;\n\tif (numentries > max)\n\t\tnumentries = max;\n\n\tlog2qty = ilog2(numentries);\n\n\tgfp_flags = (flags & HASH_ZERO) ? GFP_ATOMIC | __GFP_ZERO : GFP_ATOMIC;\n\tdo {\n\t\tvirt = false;\n\t\tsize = bucketsize << log2qty;\n\t\tif (flags & HASH_EARLY) {\n\t\t\tif (flags & HASH_ZERO)\n\t\t\t\ttable = memblock_alloc(size, SMP_CACHE_BYTES);\n\t\t\telse\n\t\t\t\ttable = memblock_alloc_raw(size,\n\t\t\t\t\t\t\t   SMP_CACHE_BYTES);\n\t\t} else if (get_order(size) > MAX_ORDER || hashdist) {\n\t\t\ttable = vmalloc_huge(size, gfp_flags);\n\t\t\tvirt = true;\n\t\t\tif (table)\n\t\t\t\thuge = is_vm_area_hugepages(table);\n\t\t} else {\n\t\t\t \n\t\t\ttable = alloc_pages_exact(size, gfp_flags);\n\t\t\tkmemleak_alloc(table, size, 1, gfp_flags);\n\t\t}\n\t} while (!table && size > PAGE_SIZE && --log2qty);\n\n\tif (!table)\n\t\tpanic(\"Failed to allocate %s hash table\\n\", tablename);\n\n\tpr_info(\"%s hash table entries: %ld (order: %d, %lu bytes, %s)\\n\",\n\t\ttablename, 1UL << log2qty, ilog2(size) - PAGE_SHIFT, size,\n\t\tvirt ? (huge ? \"vmalloc hugepage\" : \"vmalloc\") : \"linear\");\n\n\tif (_hash_shift)\n\t\t*_hash_shift = log2qty;\n\tif (_hash_mask)\n\t\t*_hash_mask = (1 << log2qty) - 1;\n\n\treturn table;\n}\n\n \nvoid __init set_dma_reserve(unsigned long new_dma_reserve)\n{\n\tdma_reserve = new_dma_reserve;\n}\n\nvoid __init memblock_free_pages(struct page *page, unsigned long pfn,\n\t\t\t\t\t\t\tunsigned int order)\n{\n\n\tif (IS_ENABLED(CONFIG_DEFERRED_STRUCT_PAGE_INIT)) {\n\t\tint nid = early_pfn_to_nid(pfn);\n\n\t\tif (!early_page_initialised(pfn, nid))\n\t\t\treturn;\n\t}\n\n\tif (!kmsan_memblock_free_pages(page, order)) {\n\t\t \n\t\treturn;\n\t}\n\t__free_pages_core(page, order);\n}\n\nDEFINE_STATIC_KEY_MAYBE(CONFIG_INIT_ON_ALLOC_DEFAULT_ON, init_on_alloc);\nEXPORT_SYMBOL(init_on_alloc);\n\nDEFINE_STATIC_KEY_MAYBE(CONFIG_INIT_ON_FREE_DEFAULT_ON, init_on_free);\nEXPORT_SYMBOL(init_on_free);\n\nstatic bool _init_on_alloc_enabled_early __read_mostly\n\t\t\t\t= IS_ENABLED(CONFIG_INIT_ON_ALLOC_DEFAULT_ON);\nstatic int __init early_init_on_alloc(char *buf)\n{\n\n\treturn kstrtobool(buf, &_init_on_alloc_enabled_early);\n}\nearly_param(\"init_on_alloc\", early_init_on_alloc);\n\nstatic bool _init_on_free_enabled_early __read_mostly\n\t\t\t\t= IS_ENABLED(CONFIG_INIT_ON_FREE_DEFAULT_ON);\nstatic int __init early_init_on_free(char *buf)\n{\n\treturn kstrtobool(buf, &_init_on_free_enabled_early);\n}\nearly_param(\"init_on_free\", early_init_on_free);\n\nDEFINE_STATIC_KEY_MAYBE(CONFIG_DEBUG_VM, check_pages_enabled);\n\n \nstatic void __init mem_debugging_and_hardening_init(void)\n{\n\tbool page_poisoning_requested = false;\n\tbool want_check_pages = false;\n\n#ifdef CONFIG_PAGE_POISONING\n\t \n\tif (page_poisoning_enabled() ||\n\t     (!IS_ENABLED(CONFIG_ARCH_SUPPORTS_DEBUG_PAGEALLOC) &&\n\t      debug_pagealloc_enabled())) {\n\t\tstatic_branch_enable(&_page_poisoning_enabled);\n\t\tpage_poisoning_requested = true;\n\t\twant_check_pages = true;\n\t}\n#endif\n\n\tif ((_init_on_alloc_enabled_early || _init_on_free_enabled_early) &&\n\t    page_poisoning_requested) {\n\t\tpr_info(\"mem auto-init: CONFIG_PAGE_POISONING is on, \"\n\t\t\t\"will take precedence over init_on_alloc and init_on_free\\n\");\n\t\t_init_on_alloc_enabled_early = false;\n\t\t_init_on_free_enabled_early = false;\n\t}\n\n\tif (_init_on_alloc_enabled_early) {\n\t\twant_check_pages = true;\n\t\tstatic_branch_enable(&init_on_alloc);\n\t} else {\n\t\tstatic_branch_disable(&init_on_alloc);\n\t}\n\n\tif (_init_on_free_enabled_early) {\n\t\twant_check_pages = true;\n\t\tstatic_branch_enable(&init_on_free);\n\t} else {\n\t\tstatic_branch_disable(&init_on_free);\n\t}\n\n\tif (IS_ENABLED(CONFIG_KMSAN) &&\n\t    (_init_on_alloc_enabled_early || _init_on_free_enabled_early))\n\t\tpr_info(\"mem auto-init: please make sure init_on_alloc and init_on_free are disabled when running KMSAN\\n\");\n\n#ifdef CONFIG_DEBUG_PAGEALLOC\n\tif (debug_pagealloc_enabled()) {\n\t\twant_check_pages = true;\n\t\tstatic_branch_enable(&_debug_pagealloc_enabled);\n\n\t\tif (debug_guardpage_minorder())\n\t\t\tstatic_branch_enable(&_debug_guardpage_enabled);\n\t}\n#endif\n\n\t \n\tif (!IS_ENABLED(CONFIG_DEBUG_VM) && want_check_pages)\n\t\tstatic_branch_enable(&check_pages_enabled);\n}\n\n \nstatic void __init report_meminit(void)\n{\n\tconst char *stack;\n\n\tif (IS_ENABLED(CONFIG_INIT_STACK_ALL_PATTERN))\n\t\tstack = \"all(pattern)\";\n\telse if (IS_ENABLED(CONFIG_INIT_STACK_ALL_ZERO))\n\t\tstack = \"all(zero)\";\n\telse if (IS_ENABLED(CONFIG_GCC_PLUGIN_STRUCTLEAK_BYREF_ALL))\n\t\tstack = \"byref_all(zero)\";\n\telse if (IS_ENABLED(CONFIG_GCC_PLUGIN_STRUCTLEAK_BYREF))\n\t\tstack = \"byref(zero)\";\n\telse if (IS_ENABLED(CONFIG_GCC_PLUGIN_STRUCTLEAK_USER))\n\t\tstack = \"__user(zero)\";\n\telse\n\t\tstack = \"off\";\n\n\tpr_info(\"mem auto-init: stack:%s, heap alloc:%s, heap free:%s\\n\",\n\t\tstack, want_init_on_alloc(GFP_KERNEL) ? \"on\" : \"off\",\n\t\twant_init_on_free() ? \"on\" : \"off\");\n\tif (want_init_on_free())\n\t\tpr_info(\"mem auto-init: clearing system memory may take some time...\\n\");\n}\n\nstatic void __init mem_init_print_info(void)\n{\n\tunsigned long physpages, codesize, datasize, rosize, bss_size;\n\tunsigned long init_code_size, init_data_size;\n\n\tphyspages = get_num_physpages();\n\tcodesize = _etext - _stext;\n\tdatasize = _edata - _sdata;\n\trosize = __end_rodata - __start_rodata;\n\tbss_size = __bss_stop - __bss_start;\n\tinit_data_size = __init_end - __init_begin;\n\tinit_code_size = _einittext - _sinittext;\n\n\t \n#define adj_init_size(start, end, size, pos, adj) \\\n\tdo { \\\n\t\tif (&start[0] <= &pos[0] && &pos[0] < &end[0] && size > adj) \\\n\t\t\tsize -= adj; \\\n\t} while (0)\n\n\tadj_init_size(__init_begin, __init_end, init_data_size,\n\t\t     _sinittext, init_code_size);\n\tadj_init_size(_stext, _etext, codesize, _sinittext, init_code_size);\n\tadj_init_size(_sdata, _edata, datasize, __init_begin, init_data_size);\n\tadj_init_size(_stext, _etext, codesize, __start_rodata, rosize);\n\tadj_init_size(_sdata, _edata, datasize, __start_rodata, rosize);\n\n#undef\tadj_init_size\n\n\tpr_info(\"Memory: %luK/%luK available (%luK kernel code, %luK rwdata, %luK rodata, %luK init, %luK bss, %luK reserved, %luK cma-reserved\"\n#ifdef\tCONFIG_HIGHMEM\n\t\t\", %luK highmem\"\n#endif\n\t\t\")\\n\",\n\t\tK(nr_free_pages()), K(physpages),\n\t\tcodesize / SZ_1K, datasize / SZ_1K, rosize / SZ_1K,\n\t\t(init_data_size + init_code_size) / SZ_1K, bss_size / SZ_1K,\n\t\tK(physpages - totalram_pages() - totalcma_pages),\n\t\tK(totalcma_pages)\n#ifdef\tCONFIG_HIGHMEM\n\t\t, K(totalhigh_pages())\n#endif\n\t\t);\n}\n\n \nvoid __init mm_core_init(void)\n{\n\t \n\tbuild_all_zonelists(NULL);\n\tpage_alloc_init_cpuhp();\n\n\t \n\tpage_ext_init_flatmem();\n\tmem_debugging_and_hardening_init();\n\tkfence_alloc_pool_and_metadata();\n\treport_meminit();\n\tkmsan_init_shadow();\n\tstack_depot_early_init();\n\tmem_init();\n\tmem_init_print_info();\n\tkmem_cache_init();\n\t \n\tpage_ext_init_flatmem_late();\n\tkmemleak_init();\n\tptlock_cache_init();\n\tpgtable_cache_init();\n\tdebug_objects_mem_init();\n\tvmalloc_init();\n\t \n\tif (!deferred_struct_pages)\n\t\tpage_ext_init();\n\t \n\tinit_espfix_bsp();\n\t \n\tpti_init();\n\tkmsan_init_runtime();\n\tmm_cache_init();\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}