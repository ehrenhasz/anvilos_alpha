{
  "module_name": "swap.c",
  "hash_id": "cb7af0c4174344f2e6d5012f55df33531a31ec2b24a2ad1afe5b8fe905f8510f",
  "original_prompt": "Ingested from linux-6.6.14/mm/swap.c",
  "human_readable_source": "\n \n\n \n\n#include <linux/mm.h>\n#include <linux/sched.h>\n#include <linux/kernel_stat.h>\n#include <linux/swap.h>\n#include <linux/mman.h>\n#include <linux/pagemap.h>\n#include <linux/pagevec.h>\n#include <linux/init.h>\n#include <linux/export.h>\n#include <linux/mm_inline.h>\n#include <linux/percpu_counter.h>\n#include <linux/memremap.h>\n#include <linux/percpu.h>\n#include <linux/cpu.h>\n#include <linux/notifier.h>\n#include <linux/backing-dev.h>\n#include <linux/memcontrol.h>\n#include <linux/gfp.h>\n#include <linux/uio.h>\n#include <linux/hugetlb.h>\n#include <linux/page_idle.h>\n#include <linux/local_lock.h>\n#include <linux/buffer_head.h>\n\n#include \"internal.h\"\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/pagemap.h>\n\n \nint page_cluster;\nconst int page_cluster_max = 31;\n\n \nstruct lru_rotate {\n\tlocal_lock_t lock;\n\tstruct folio_batch fbatch;\n};\nstatic DEFINE_PER_CPU(struct lru_rotate, lru_rotate) = {\n\t.lock = INIT_LOCAL_LOCK(lock),\n};\n\n \nstruct cpu_fbatches {\n\tlocal_lock_t lock;\n\tstruct folio_batch lru_add;\n\tstruct folio_batch lru_deactivate_file;\n\tstruct folio_batch lru_deactivate;\n\tstruct folio_batch lru_lazyfree;\n#ifdef CONFIG_SMP\n\tstruct folio_batch activate;\n#endif\n};\nstatic DEFINE_PER_CPU(struct cpu_fbatches, cpu_fbatches) = {\n\t.lock = INIT_LOCAL_LOCK(lock),\n};\n\n \nstatic void __page_cache_release(struct folio *folio)\n{\n\tif (folio_test_lru(folio)) {\n\t\tstruct lruvec *lruvec;\n\t\tunsigned long flags;\n\n\t\tlruvec = folio_lruvec_lock_irqsave(folio, &flags);\n\t\tlruvec_del_folio(lruvec, folio);\n\t\t__folio_clear_lru_flags(folio);\n\t\tunlock_page_lruvec_irqrestore(lruvec, flags);\n\t}\n\t \n\tif (unlikely(folio_test_mlocked(folio))) {\n\t\tlong nr_pages = folio_nr_pages(folio);\n\n\t\t__folio_clear_mlocked(folio);\n\t\tzone_stat_mod_folio(folio, NR_MLOCK, -nr_pages);\n\t\tcount_vm_events(UNEVICTABLE_PGCLEARED, nr_pages);\n\t}\n}\n\nstatic void __folio_put_small(struct folio *folio)\n{\n\t__page_cache_release(folio);\n\tmem_cgroup_uncharge(folio);\n\tfree_unref_page(&folio->page, 0);\n}\n\nstatic void __folio_put_large(struct folio *folio)\n{\n\t \n\tif (!folio_test_hugetlb(folio))\n\t\t__page_cache_release(folio);\n\tdestroy_large_folio(folio);\n}\n\nvoid __folio_put(struct folio *folio)\n{\n\tif (unlikely(folio_is_zone_device(folio)))\n\t\tfree_zone_device_page(&folio->page);\n\telse if (unlikely(folio_test_large(folio)))\n\t\t__folio_put_large(folio);\n\telse\n\t\t__folio_put_small(folio);\n}\nEXPORT_SYMBOL(__folio_put);\n\n \nvoid put_pages_list(struct list_head *pages)\n{\n\tstruct folio *folio, *next;\n\n\tlist_for_each_entry_safe(folio, next, pages, lru) {\n\t\tif (!folio_put_testzero(folio)) {\n\t\t\tlist_del(&folio->lru);\n\t\t\tcontinue;\n\t\t}\n\t\tif (folio_test_large(folio)) {\n\t\t\tlist_del(&folio->lru);\n\t\t\t__folio_put_large(folio);\n\t\t\tcontinue;\n\t\t}\n\t\t \n\t}\n\n\tfree_unref_page_list(pages);\n\tINIT_LIST_HEAD(pages);\n}\nEXPORT_SYMBOL(put_pages_list);\n\ntypedef void (*move_fn_t)(struct lruvec *lruvec, struct folio *folio);\n\nstatic void lru_add_fn(struct lruvec *lruvec, struct folio *folio)\n{\n\tint was_unevictable = folio_test_clear_unevictable(folio);\n\tlong nr_pages = folio_nr_pages(folio);\n\n\tVM_BUG_ON_FOLIO(folio_test_lru(folio), folio);\n\n\t \n\tif (folio_evictable(folio)) {\n\t\tif (was_unevictable)\n\t\t\t__count_vm_events(UNEVICTABLE_PGRESCUED, nr_pages);\n\t} else {\n\t\tfolio_clear_active(folio);\n\t\tfolio_set_unevictable(folio);\n\t\t \n\t\tfolio->mlock_count = 0;\n\t\tif (!was_unevictable)\n\t\t\t__count_vm_events(UNEVICTABLE_PGCULLED, nr_pages);\n\t}\n\n\tlruvec_add_folio(lruvec, folio);\n\ttrace_mm_lru_insertion(folio);\n}\n\nstatic void folio_batch_move_lru(struct folio_batch *fbatch, move_fn_t move_fn)\n{\n\tint i;\n\tstruct lruvec *lruvec = NULL;\n\tunsigned long flags = 0;\n\n\tfor (i = 0; i < folio_batch_count(fbatch); i++) {\n\t\tstruct folio *folio = fbatch->folios[i];\n\n\t\t \n\t\tif (move_fn != lru_add_fn && !folio_test_clear_lru(folio))\n\t\t\tcontinue;\n\n\t\tlruvec = folio_lruvec_relock_irqsave(folio, lruvec, &flags);\n\t\tmove_fn(lruvec, folio);\n\n\t\tfolio_set_lru(folio);\n\t}\n\n\tif (lruvec)\n\t\tunlock_page_lruvec_irqrestore(lruvec, flags);\n\tfolios_put(fbatch->folios, folio_batch_count(fbatch));\n\tfolio_batch_reinit(fbatch);\n}\n\nstatic void folio_batch_add_and_move(struct folio_batch *fbatch,\n\t\tstruct folio *folio, move_fn_t move_fn)\n{\n\tif (folio_batch_add(fbatch, folio) && !folio_test_large(folio) &&\n\t    !lru_cache_disabled())\n\t\treturn;\n\tfolio_batch_move_lru(fbatch, move_fn);\n}\n\nstatic void lru_move_tail_fn(struct lruvec *lruvec, struct folio *folio)\n{\n\tif (!folio_test_unevictable(folio)) {\n\t\tlruvec_del_folio(lruvec, folio);\n\t\tfolio_clear_active(folio);\n\t\tlruvec_add_folio_tail(lruvec, folio);\n\t\t__count_vm_events(PGROTATED, folio_nr_pages(folio));\n\t}\n}\n\n \nvoid folio_rotate_reclaimable(struct folio *folio)\n{\n\tif (!folio_test_locked(folio) && !folio_test_dirty(folio) &&\n\t    !folio_test_unevictable(folio) && folio_test_lru(folio)) {\n\t\tstruct folio_batch *fbatch;\n\t\tunsigned long flags;\n\n\t\tfolio_get(folio);\n\t\tlocal_lock_irqsave(&lru_rotate.lock, flags);\n\t\tfbatch = this_cpu_ptr(&lru_rotate.fbatch);\n\t\tfolio_batch_add_and_move(fbatch, folio, lru_move_tail_fn);\n\t\tlocal_unlock_irqrestore(&lru_rotate.lock, flags);\n\t}\n}\n\nvoid lru_note_cost(struct lruvec *lruvec, bool file,\n\t\t   unsigned int nr_io, unsigned int nr_rotated)\n{\n\tunsigned long cost;\n\n\t \n\tcost = nr_io * SWAP_CLUSTER_MAX + nr_rotated;\n\n\tdo {\n\t\tunsigned long lrusize;\n\n\t\t \n\t\tspin_lock_irq(&lruvec->lru_lock);\n\t\t \n\t\tif (file)\n\t\t\tlruvec->file_cost += cost;\n\t\telse\n\t\t\tlruvec->anon_cost += cost;\n\n\t\t \n\t\tlrusize = lruvec_page_state(lruvec, NR_INACTIVE_ANON) +\n\t\t\t  lruvec_page_state(lruvec, NR_ACTIVE_ANON) +\n\t\t\t  lruvec_page_state(lruvec, NR_INACTIVE_FILE) +\n\t\t\t  lruvec_page_state(lruvec, NR_ACTIVE_FILE);\n\n\t\tif (lruvec->file_cost + lruvec->anon_cost > lrusize / 4) {\n\t\t\tlruvec->file_cost /= 2;\n\t\t\tlruvec->anon_cost /= 2;\n\t\t}\n\t\tspin_unlock_irq(&lruvec->lru_lock);\n\t} while ((lruvec = parent_lruvec(lruvec)));\n}\n\nvoid lru_note_cost_refault(struct folio *folio)\n{\n\tlru_note_cost(folio_lruvec(folio), folio_is_file_lru(folio),\n\t\t      folio_nr_pages(folio), 0);\n}\n\nstatic void folio_activate_fn(struct lruvec *lruvec, struct folio *folio)\n{\n\tif (!folio_test_active(folio) && !folio_test_unevictable(folio)) {\n\t\tlong nr_pages = folio_nr_pages(folio);\n\n\t\tlruvec_del_folio(lruvec, folio);\n\t\tfolio_set_active(folio);\n\t\tlruvec_add_folio(lruvec, folio);\n\t\ttrace_mm_lru_activate(folio);\n\n\t\t__count_vm_events(PGACTIVATE, nr_pages);\n\t\t__count_memcg_events(lruvec_memcg(lruvec), PGACTIVATE,\n\t\t\t\t     nr_pages);\n\t}\n}\n\n#ifdef CONFIG_SMP\nstatic void folio_activate_drain(int cpu)\n{\n\tstruct folio_batch *fbatch = &per_cpu(cpu_fbatches.activate, cpu);\n\n\tif (folio_batch_count(fbatch))\n\t\tfolio_batch_move_lru(fbatch, folio_activate_fn);\n}\n\nvoid folio_activate(struct folio *folio)\n{\n\tif (folio_test_lru(folio) && !folio_test_active(folio) &&\n\t    !folio_test_unevictable(folio)) {\n\t\tstruct folio_batch *fbatch;\n\n\t\tfolio_get(folio);\n\t\tlocal_lock(&cpu_fbatches.lock);\n\t\tfbatch = this_cpu_ptr(&cpu_fbatches.activate);\n\t\tfolio_batch_add_and_move(fbatch, folio, folio_activate_fn);\n\t\tlocal_unlock(&cpu_fbatches.lock);\n\t}\n}\n\n#else\nstatic inline void folio_activate_drain(int cpu)\n{\n}\n\nvoid folio_activate(struct folio *folio)\n{\n\tstruct lruvec *lruvec;\n\n\tif (folio_test_clear_lru(folio)) {\n\t\tlruvec = folio_lruvec_lock_irq(folio);\n\t\tfolio_activate_fn(lruvec, folio);\n\t\tunlock_page_lruvec_irq(lruvec);\n\t\tfolio_set_lru(folio);\n\t}\n}\n#endif\n\nstatic void __lru_cache_activate_folio(struct folio *folio)\n{\n\tstruct folio_batch *fbatch;\n\tint i;\n\n\tlocal_lock(&cpu_fbatches.lock);\n\tfbatch = this_cpu_ptr(&cpu_fbatches.lru_add);\n\n\t \n\tfor (i = folio_batch_count(fbatch) - 1; i >= 0; i--) {\n\t\tstruct folio *batch_folio = fbatch->folios[i];\n\n\t\tif (batch_folio == folio) {\n\t\t\tfolio_set_active(folio);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tlocal_unlock(&cpu_fbatches.lock);\n}\n\n#ifdef CONFIG_LRU_GEN\nstatic void folio_inc_refs(struct folio *folio)\n{\n\tunsigned long new_flags, old_flags = READ_ONCE(folio->flags);\n\n\tif (folio_test_unevictable(folio))\n\t\treturn;\n\n\tif (!folio_test_referenced(folio)) {\n\t\tfolio_set_referenced(folio);\n\t\treturn;\n\t}\n\n\tif (!folio_test_workingset(folio)) {\n\t\tfolio_set_workingset(folio);\n\t\treturn;\n\t}\n\n\t \n\tdo {\n\t\tnew_flags = old_flags & LRU_REFS_MASK;\n\t\tif (new_flags == LRU_REFS_MASK)\n\t\t\tbreak;\n\n\t\tnew_flags += BIT(LRU_REFS_PGOFF);\n\t\tnew_flags |= old_flags & ~LRU_REFS_MASK;\n\t} while (!try_cmpxchg(&folio->flags, &old_flags, new_flags));\n}\n#else\nstatic void folio_inc_refs(struct folio *folio)\n{\n}\n#endif  \n\n \nvoid folio_mark_accessed(struct folio *folio)\n{\n\tif (lru_gen_enabled()) {\n\t\tfolio_inc_refs(folio);\n\t\treturn;\n\t}\n\n\tif (!folio_test_referenced(folio)) {\n\t\tfolio_set_referenced(folio);\n\t} else if (folio_test_unevictable(folio)) {\n\t\t \n\t} else if (!folio_test_active(folio)) {\n\t\t \n\t\tif (folio_test_lru(folio))\n\t\t\tfolio_activate(folio);\n\t\telse\n\t\t\t__lru_cache_activate_folio(folio);\n\t\tfolio_clear_referenced(folio);\n\t\tworkingset_activation(folio);\n\t}\n\tif (folio_test_idle(folio))\n\t\tfolio_clear_idle(folio);\n}\nEXPORT_SYMBOL(folio_mark_accessed);\n\n \nvoid folio_add_lru(struct folio *folio)\n{\n\tstruct folio_batch *fbatch;\n\n\tVM_BUG_ON_FOLIO(folio_test_active(folio) &&\n\t\t\tfolio_test_unevictable(folio), folio);\n\tVM_BUG_ON_FOLIO(folio_test_lru(folio), folio);\n\n\t \n\tif (lru_gen_enabled() && !folio_test_unevictable(folio) &&\n\t    lru_gen_in_fault() && !(current->flags & PF_MEMALLOC))\n\t\tfolio_set_active(folio);\n\n\tfolio_get(folio);\n\tlocal_lock(&cpu_fbatches.lock);\n\tfbatch = this_cpu_ptr(&cpu_fbatches.lru_add);\n\tfolio_batch_add_and_move(fbatch, folio, lru_add_fn);\n\tlocal_unlock(&cpu_fbatches.lock);\n}\nEXPORT_SYMBOL(folio_add_lru);\n\n \nvoid folio_add_lru_vma(struct folio *folio, struct vm_area_struct *vma)\n{\n\tVM_BUG_ON_FOLIO(folio_test_lru(folio), folio);\n\n\tif (unlikely((vma->vm_flags & (VM_LOCKED | VM_SPECIAL)) == VM_LOCKED))\n\t\tmlock_new_folio(folio);\n\telse\n\t\tfolio_add_lru(folio);\n}\n\n \nstatic void lru_deactivate_file_fn(struct lruvec *lruvec, struct folio *folio)\n{\n\tbool active = folio_test_active(folio);\n\tlong nr_pages = folio_nr_pages(folio);\n\n\tif (folio_test_unevictable(folio))\n\t\treturn;\n\n\t \n\tif (folio_mapped(folio))\n\t\treturn;\n\n\tlruvec_del_folio(lruvec, folio);\n\tfolio_clear_active(folio);\n\tfolio_clear_referenced(folio);\n\n\tif (folio_test_writeback(folio) || folio_test_dirty(folio)) {\n\t\t \n\t\tlruvec_add_folio(lruvec, folio);\n\t\tfolio_set_reclaim(folio);\n\t} else {\n\t\t \n\t\tlruvec_add_folio_tail(lruvec, folio);\n\t\t__count_vm_events(PGROTATED, nr_pages);\n\t}\n\n\tif (active) {\n\t\t__count_vm_events(PGDEACTIVATE, nr_pages);\n\t\t__count_memcg_events(lruvec_memcg(lruvec), PGDEACTIVATE,\n\t\t\t\t     nr_pages);\n\t}\n}\n\nstatic void lru_deactivate_fn(struct lruvec *lruvec, struct folio *folio)\n{\n\tif (!folio_test_unevictable(folio) && (folio_test_active(folio) || lru_gen_enabled())) {\n\t\tlong nr_pages = folio_nr_pages(folio);\n\n\t\tlruvec_del_folio(lruvec, folio);\n\t\tfolio_clear_active(folio);\n\t\tfolio_clear_referenced(folio);\n\t\tlruvec_add_folio(lruvec, folio);\n\n\t\t__count_vm_events(PGDEACTIVATE, nr_pages);\n\t\t__count_memcg_events(lruvec_memcg(lruvec), PGDEACTIVATE,\n\t\t\t\t     nr_pages);\n\t}\n}\n\nstatic void lru_lazyfree_fn(struct lruvec *lruvec, struct folio *folio)\n{\n\tif (folio_test_anon(folio) && folio_test_swapbacked(folio) &&\n\t    !folio_test_swapcache(folio) && !folio_test_unevictable(folio)) {\n\t\tlong nr_pages = folio_nr_pages(folio);\n\n\t\tlruvec_del_folio(lruvec, folio);\n\t\tfolio_clear_active(folio);\n\t\tfolio_clear_referenced(folio);\n\t\t \n\t\tfolio_clear_swapbacked(folio);\n\t\tlruvec_add_folio(lruvec, folio);\n\n\t\t__count_vm_events(PGLAZYFREE, nr_pages);\n\t\t__count_memcg_events(lruvec_memcg(lruvec), PGLAZYFREE,\n\t\t\t\t     nr_pages);\n\t}\n}\n\n \nvoid lru_add_drain_cpu(int cpu)\n{\n\tstruct cpu_fbatches *fbatches = &per_cpu(cpu_fbatches, cpu);\n\tstruct folio_batch *fbatch = &fbatches->lru_add;\n\n\tif (folio_batch_count(fbatch))\n\t\tfolio_batch_move_lru(fbatch, lru_add_fn);\n\n\tfbatch = &per_cpu(lru_rotate.fbatch, cpu);\n\t \n\tif (data_race(folio_batch_count(fbatch))) {\n\t\tunsigned long flags;\n\n\t\t \n\t\tlocal_lock_irqsave(&lru_rotate.lock, flags);\n\t\tfolio_batch_move_lru(fbatch, lru_move_tail_fn);\n\t\tlocal_unlock_irqrestore(&lru_rotate.lock, flags);\n\t}\n\n\tfbatch = &fbatches->lru_deactivate_file;\n\tif (folio_batch_count(fbatch))\n\t\tfolio_batch_move_lru(fbatch, lru_deactivate_file_fn);\n\n\tfbatch = &fbatches->lru_deactivate;\n\tif (folio_batch_count(fbatch))\n\t\tfolio_batch_move_lru(fbatch, lru_deactivate_fn);\n\n\tfbatch = &fbatches->lru_lazyfree;\n\tif (folio_batch_count(fbatch))\n\t\tfolio_batch_move_lru(fbatch, lru_lazyfree_fn);\n\n\tfolio_activate_drain(cpu);\n}\n\n \nvoid deactivate_file_folio(struct folio *folio)\n{\n\tstruct folio_batch *fbatch;\n\n\t \n\tif (folio_test_unevictable(folio))\n\t\treturn;\n\n\tfolio_get(folio);\n\tlocal_lock(&cpu_fbatches.lock);\n\tfbatch = this_cpu_ptr(&cpu_fbatches.lru_deactivate_file);\n\tfolio_batch_add_and_move(fbatch, folio, lru_deactivate_file_fn);\n\tlocal_unlock(&cpu_fbatches.lock);\n}\n\n \nvoid folio_deactivate(struct folio *folio)\n{\n\tif (folio_test_lru(folio) && !folio_test_unevictable(folio) &&\n\t    (folio_test_active(folio) || lru_gen_enabled())) {\n\t\tstruct folio_batch *fbatch;\n\n\t\tfolio_get(folio);\n\t\tlocal_lock(&cpu_fbatches.lock);\n\t\tfbatch = this_cpu_ptr(&cpu_fbatches.lru_deactivate);\n\t\tfolio_batch_add_and_move(fbatch, folio, lru_deactivate_fn);\n\t\tlocal_unlock(&cpu_fbatches.lock);\n\t}\n}\n\n \nvoid folio_mark_lazyfree(struct folio *folio)\n{\n\tif (folio_test_lru(folio) && folio_test_anon(folio) &&\n\t    folio_test_swapbacked(folio) && !folio_test_swapcache(folio) &&\n\t    !folio_test_unevictable(folio)) {\n\t\tstruct folio_batch *fbatch;\n\n\t\tfolio_get(folio);\n\t\tlocal_lock(&cpu_fbatches.lock);\n\t\tfbatch = this_cpu_ptr(&cpu_fbatches.lru_lazyfree);\n\t\tfolio_batch_add_and_move(fbatch, folio, lru_lazyfree_fn);\n\t\tlocal_unlock(&cpu_fbatches.lock);\n\t}\n}\n\nvoid lru_add_drain(void)\n{\n\tlocal_lock(&cpu_fbatches.lock);\n\tlru_add_drain_cpu(smp_processor_id());\n\tlocal_unlock(&cpu_fbatches.lock);\n\tmlock_drain_local();\n}\n\n \nstatic void lru_add_and_bh_lrus_drain(void)\n{\n\tlocal_lock(&cpu_fbatches.lock);\n\tlru_add_drain_cpu(smp_processor_id());\n\tlocal_unlock(&cpu_fbatches.lock);\n\tinvalidate_bh_lrus_cpu();\n\tmlock_drain_local();\n}\n\nvoid lru_add_drain_cpu_zone(struct zone *zone)\n{\n\tlocal_lock(&cpu_fbatches.lock);\n\tlru_add_drain_cpu(smp_processor_id());\n\tdrain_local_pages(zone);\n\tlocal_unlock(&cpu_fbatches.lock);\n\tmlock_drain_local();\n}\n\n#ifdef CONFIG_SMP\n\nstatic DEFINE_PER_CPU(struct work_struct, lru_add_drain_work);\n\nstatic void lru_add_drain_per_cpu(struct work_struct *dummy)\n{\n\tlru_add_and_bh_lrus_drain();\n}\n\nstatic bool cpu_needs_drain(unsigned int cpu)\n{\n\tstruct cpu_fbatches *fbatches = &per_cpu(cpu_fbatches, cpu);\n\n\t \n\treturn folio_batch_count(&fbatches->lru_add) ||\n\t\tdata_race(folio_batch_count(&per_cpu(lru_rotate.fbatch, cpu))) ||\n\t\tfolio_batch_count(&fbatches->lru_deactivate_file) ||\n\t\tfolio_batch_count(&fbatches->lru_deactivate) ||\n\t\tfolio_batch_count(&fbatches->lru_lazyfree) ||\n\t\tfolio_batch_count(&fbatches->activate) ||\n\t\tneed_mlock_drain(cpu) ||\n\t\thas_bh_in_lru(cpu, NULL);\n}\n\n \nstatic inline void __lru_add_drain_all(bool force_all_cpus)\n{\n\t \n\tstatic unsigned int lru_drain_gen;\n\tstatic struct cpumask has_work;\n\tstatic DEFINE_MUTEX(lock);\n\tunsigned cpu, this_gen;\n\n\t \n\tif (WARN_ON(!mm_percpu_wq))\n\t\treturn;\n\n\t \n\tsmp_mb();\n\n\t \n\tthis_gen = smp_load_acquire(&lru_drain_gen);\n\n\tmutex_lock(&lock);\n\n\t \n\tif (unlikely(this_gen != lru_drain_gen && !force_all_cpus))\n\t\tgoto done;\n\n\t \n\tWRITE_ONCE(lru_drain_gen, lru_drain_gen + 1);\n\tsmp_mb();\n\n\tcpumask_clear(&has_work);\n\tfor_each_online_cpu(cpu) {\n\t\tstruct work_struct *work = &per_cpu(lru_add_drain_work, cpu);\n\n\t\tif (cpu_needs_drain(cpu)) {\n\t\t\tINIT_WORK(work, lru_add_drain_per_cpu);\n\t\t\tqueue_work_on(cpu, mm_percpu_wq, work);\n\t\t\t__cpumask_set_cpu(cpu, &has_work);\n\t\t}\n\t}\n\n\tfor_each_cpu(cpu, &has_work)\n\t\tflush_work(&per_cpu(lru_add_drain_work, cpu));\n\ndone:\n\tmutex_unlock(&lock);\n}\n\nvoid lru_add_drain_all(void)\n{\n\t__lru_add_drain_all(false);\n}\n#else\nvoid lru_add_drain_all(void)\n{\n\tlru_add_drain();\n}\n#endif  \n\natomic_t lru_disable_count = ATOMIC_INIT(0);\n\n \nvoid lru_cache_disable(void)\n{\n\tatomic_inc(&lru_disable_count);\n\t \n\tsynchronize_rcu_expedited();\n#ifdef CONFIG_SMP\n\t__lru_add_drain_all(true);\n#else\n\tlru_add_and_bh_lrus_drain();\n#endif\n}\n\n \nvoid release_pages(release_pages_arg arg, int nr)\n{\n\tint i;\n\tstruct encoded_page **encoded = arg.encoded_pages;\n\tLIST_HEAD(pages_to_free);\n\tstruct lruvec *lruvec = NULL;\n\tunsigned long flags = 0;\n\tunsigned int lock_batch;\n\n\tfor (i = 0; i < nr; i++) {\n\t\tstruct folio *folio;\n\n\t\t \n\t\tfolio = page_folio(encoded_page_ptr(encoded[i]));\n\n\t\t \n\t\tif (lruvec && ++lock_batch == SWAP_CLUSTER_MAX) {\n\t\t\tunlock_page_lruvec_irqrestore(lruvec, flags);\n\t\t\tlruvec = NULL;\n\t\t}\n\n\t\tif (is_huge_zero_page(&folio->page))\n\t\t\tcontinue;\n\n\t\tif (folio_is_zone_device(folio)) {\n\t\t\tif (lruvec) {\n\t\t\t\tunlock_page_lruvec_irqrestore(lruvec, flags);\n\t\t\t\tlruvec = NULL;\n\t\t\t}\n\t\t\tif (put_devmap_managed_page(&folio->page))\n\t\t\t\tcontinue;\n\t\t\tif (folio_put_testzero(folio))\n\t\t\t\tfree_zone_device_page(&folio->page);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!folio_put_testzero(folio))\n\t\t\tcontinue;\n\n\t\tif (folio_test_large(folio)) {\n\t\t\tif (lruvec) {\n\t\t\t\tunlock_page_lruvec_irqrestore(lruvec, flags);\n\t\t\t\tlruvec = NULL;\n\t\t\t}\n\t\t\t__folio_put_large(folio);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (folio_test_lru(folio)) {\n\t\t\tstruct lruvec *prev_lruvec = lruvec;\n\n\t\t\tlruvec = folio_lruvec_relock_irqsave(folio, lruvec,\n\t\t\t\t\t\t\t\t\t&flags);\n\t\t\tif (prev_lruvec != lruvec)\n\t\t\t\tlock_batch = 0;\n\n\t\t\tlruvec_del_folio(lruvec, folio);\n\t\t\t__folio_clear_lru_flags(folio);\n\t\t}\n\n\t\t \n\t\tif (unlikely(folio_test_mlocked(folio))) {\n\t\t\t__folio_clear_mlocked(folio);\n\t\t\tzone_stat_sub_folio(folio, NR_MLOCK);\n\t\t\tcount_vm_event(UNEVICTABLE_PGCLEARED);\n\t\t}\n\n\t\tlist_add(&folio->lru, &pages_to_free);\n\t}\n\tif (lruvec)\n\t\tunlock_page_lruvec_irqrestore(lruvec, flags);\n\n\tmem_cgroup_uncharge_list(&pages_to_free);\n\tfree_unref_page_list(&pages_to_free);\n}\nEXPORT_SYMBOL(release_pages);\n\n \nvoid __folio_batch_release(struct folio_batch *fbatch)\n{\n\tif (!fbatch->percpu_pvec_drained) {\n\t\tlru_add_drain();\n\t\tfbatch->percpu_pvec_drained = true;\n\t}\n\trelease_pages(fbatch->folios, folio_batch_count(fbatch));\n\tfolio_batch_reinit(fbatch);\n}\nEXPORT_SYMBOL(__folio_batch_release);\n\n \nvoid folio_batch_remove_exceptionals(struct folio_batch *fbatch)\n{\n\tunsigned int i, j;\n\n\tfor (i = 0, j = 0; i < folio_batch_count(fbatch); i++) {\n\t\tstruct folio *folio = fbatch->folios[i];\n\t\tif (!xa_is_value(folio))\n\t\t\tfbatch->folios[j++] = folio;\n\t}\n\tfbatch->nr = j;\n}\n\n \nvoid __init swap_setup(void)\n{\n\tunsigned long megs = totalram_pages() >> (20 - PAGE_SHIFT);\n\n\t \n\tif (megs < 16)\n\t\tpage_cluster = 2;\n\telse\n\t\tpage_cluster = 3;\n\t \n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}