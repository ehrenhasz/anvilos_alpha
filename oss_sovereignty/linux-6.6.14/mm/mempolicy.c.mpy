{
  "module_name": "mempolicy.c",
  "hash_id": "db49f3c4f7b9464ed0eea53287ff81d69363cea9d0619ca7ae5e369d84cca71c",
  "original_prompt": "Ingested from linux-6.6.14/mm/mempolicy.c",
  "human_readable_source": "\n \n\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/mempolicy.h>\n#include <linux/pagewalk.h>\n#include <linux/highmem.h>\n#include <linux/hugetlb.h>\n#include <linux/kernel.h>\n#include <linux/sched.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/task.h>\n#include <linux/nodemask.h>\n#include <linux/cpuset.h>\n#include <linux/slab.h>\n#include <linux/string.h>\n#include <linux/export.h>\n#include <linux/nsproxy.h>\n#include <linux/interrupt.h>\n#include <linux/init.h>\n#include <linux/compat.h>\n#include <linux/ptrace.h>\n#include <linux/swap.h>\n#include <linux/seq_file.h>\n#include <linux/proc_fs.h>\n#include <linux/migrate.h>\n#include <linux/ksm.h>\n#include <linux/rmap.h>\n#include <linux/security.h>\n#include <linux/syscalls.h>\n#include <linux/ctype.h>\n#include <linux/mm_inline.h>\n#include <linux/mmu_notifier.h>\n#include <linux/printk.h>\n#include <linux/swapops.h>\n\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <linux/uaccess.h>\n\n#include \"internal.h\"\n\n \n#define MPOL_MF_DISCONTIG_OK (MPOL_MF_INTERNAL << 0)\t \n#define MPOL_MF_INVERT (MPOL_MF_INTERNAL << 1)\t\t \n\nstatic struct kmem_cache *policy_cache;\nstatic struct kmem_cache *sn_cache;\n\n \nenum zone_type policy_zone = 0;\n\n \nstatic struct mempolicy default_policy = {\n\t.refcnt = ATOMIC_INIT(1),  \n\t.mode = MPOL_LOCAL,\n};\n\nstatic struct mempolicy preferred_node_policy[MAX_NUMNODES];\n\n \nint numa_nearest_node(int node, unsigned int state)\n{\n\tint min_dist = INT_MAX, dist, n, min_node;\n\n\tif (state >= NR_NODE_STATES)\n\t\treturn -EINVAL;\n\n\tif (node == NUMA_NO_NODE || node_state(node, state))\n\t\treturn node;\n\n\tmin_node = node;\n\tfor_each_node_state(n, state) {\n\t\tdist = node_distance(node, n);\n\t\tif (dist < min_dist) {\n\t\t\tmin_dist = dist;\n\t\t\tmin_node = n;\n\t\t}\n\t}\n\n\treturn min_node;\n}\nEXPORT_SYMBOL_GPL(numa_nearest_node);\n\nstruct mempolicy *get_task_policy(struct task_struct *p)\n{\n\tstruct mempolicy *pol = p->mempolicy;\n\tint node;\n\n\tif (pol)\n\t\treturn pol;\n\n\tnode = numa_node_id();\n\tif (node != NUMA_NO_NODE) {\n\t\tpol = &preferred_node_policy[node];\n\t\t \n\t\tif (pol->mode)\n\t\t\treturn pol;\n\t}\n\n\treturn &default_policy;\n}\n\nstatic const struct mempolicy_operations {\n\tint (*create)(struct mempolicy *pol, const nodemask_t *nodes);\n\tvoid (*rebind)(struct mempolicy *pol, const nodemask_t *nodes);\n} mpol_ops[MPOL_MAX];\n\nstatic inline int mpol_store_user_nodemask(const struct mempolicy *pol)\n{\n\treturn pol->flags & MPOL_MODE_FLAGS;\n}\n\nstatic void mpol_relative_nodemask(nodemask_t *ret, const nodemask_t *orig,\n\t\t\t\t   const nodemask_t *rel)\n{\n\tnodemask_t tmp;\n\tnodes_fold(tmp, *orig, nodes_weight(*rel));\n\tnodes_onto(*ret, tmp, *rel);\n}\n\nstatic int mpol_new_nodemask(struct mempolicy *pol, const nodemask_t *nodes)\n{\n\tif (nodes_empty(*nodes))\n\t\treturn -EINVAL;\n\tpol->nodes = *nodes;\n\treturn 0;\n}\n\nstatic int mpol_new_preferred(struct mempolicy *pol, const nodemask_t *nodes)\n{\n\tif (nodes_empty(*nodes))\n\t\treturn -EINVAL;\n\n\tnodes_clear(pol->nodes);\n\tnode_set(first_node(*nodes), pol->nodes);\n\treturn 0;\n}\n\n \nstatic int mpol_set_nodemask(struct mempolicy *pol,\n\t\t     const nodemask_t *nodes, struct nodemask_scratch *nsc)\n{\n\tint ret;\n\n\t \n\tif (!pol || pol->mode == MPOL_LOCAL)\n\t\treturn 0;\n\n\t \n\tnodes_and(nsc->mask1,\n\t\t  cpuset_current_mems_allowed, node_states[N_MEMORY]);\n\n\tVM_BUG_ON(!nodes);\n\n\tif (pol->flags & MPOL_F_RELATIVE_NODES)\n\t\tmpol_relative_nodemask(&nsc->mask2, nodes, &nsc->mask1);\n\telse\n\t\tnodes_and(nsc->mask2, *nodes, nsc->mask1);\n\n\tif (mpol_store_user_nodemask(pol))\n\t\tpol->w.user_nodemask = *nodes;\n\telse\n\t\tpol->w.cpuset_mems_allowed = cpuset_current_mems_allowed;\n\n\tret = mpol_ops[pol->mode].create(pol, &nsc->mask2);\n\treturn ret;\n}\n\n \nstatic struct mempolicy *mpol_new(unsigned short mode, unsigned short flags,\n\t\t\t\t  nodemask_t *nodes)\n{\n\tstruct mempolicy *policy;\n\n\tpr_debug(\"setting mode %d flags %d nodes[0] %lx\\n\",\n\t\t mode, flags, nodes ? nodes_addr(*nodes)[0] : NUMA_NO_NODE);\n\n\tif (mode == MPOL_DEFAULT) {\n\t\tif (nodes && !nodes_empty(*nodes))\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\treturn NULL;\n\t}\n\tVM_BUG_ON(!nodes);\n\n\t \n\tif (mode == MPOL_PREFERRED) {\n\t\tif (nodes_empty(*nodes)) {\n\t\t\tif (((flags & MPOL_F_STATIC_NODES) ||\n\t\t\t     (flags & MPOL_F_RELATIVE_NODES)))\n\t\t\t\treturn ERR_PTR(-EINVAL);\n\n\t\t\tmode = MPOL_LOCAL;\n\t\t}\n\t} else if (mode == MPOL_LOCAL) {\n\t\tif (!nodes_empty(*nodes) ||\n\t\t    (flags & MPOL_F_STATIC_NODES) ||\n\t\t    (flags & MPOL_F_RELATIVE_NODES))\n\t\t\treturn ERR_PTR(-EINVAL);\n\t} else if (nodes_empty(*nodes))\n\t\treturn ERR_PTR(-EINVAL);\n\tpolicy = kmem_cache_alloc(policy_cache, GFP_KERNEL);\n\tif (!policy)\n\t\treturn ERR_PTR(-ENOMEM);\n\tatomic_set(&policy->refcnt, 1);\n\tpolicy->mode = mode;\n\tpolicy->flags = flags;\n\tpolicy->home_node = NUMA_NO_NODE;\n\n\treturn policy;\n}\n\n \nvoid __mpol_put(struct mempolicy *p)\n{\n\tif (!atomic_dec_and_test(&p->refcnt))\n\t\treturn;\n\tkmem_cache_free(policy_cache, p);\n}\n\nstatic void mpol_rebind_default(struct mempolicy *pol, const nodemask_t *nodes)\n{\n}\n\nstatic void mpol_rebind_nodemask(struct mempolicy *pol, const nodemask_t *nodes)\n{\n\tnodemask_t tmp;\n\n\tif (pol->flags & MPOL_F_STATIC_NODES)\n\t\tnodes_and(tmp, pol->w.user_nodemask, *nodes);\n\telse if (pol->flags & MPOL_F_RELATIVE_NODES)\n\t\tmpol_relative_nodemask(&tmp, &pol->w.user_nodemask, nodes);\n\telse {\n\t\tnodes_remap(tmp, pol->nodes, pol->w.cpuset_mems_allowed,\n\t\t\t\t\t\t\t\t*nodes);\n\t\tpol->w.cpuset_mems_allowed = *nodes;\n\t}\n\n\tif (nodes_empty(tmp))\n\t\ttmp = *nodes;\n\n\tpol->nodes = tmp;\n}\n\nstatic void mpol_rebind_preferred(struct mempolicy *pol,\n\t\t\t\t\t\tconst nodemask_t *nodes)\n{\n\tpol->w.cpuset_mems_allowed = *nodes;\n}\n\n \nstatic void mpol_rebind_policy(struct mempolicy *pol, const nodemask_t *newmask)\n{\n\tif (!pol || pol->mode == MPOL_LOCAL)\n\t\treturn;\n\tif (!mpol_store_user_nodemask(pol) &&\n\t    nodes_equal(pol->w.cpuset_mems_allowed, *newmask))\n\t\treturn;\n\n\tmpol_ops[pol->mode].rebind(pol, newmask);\n}\n\n \n\nvoid mpol_rebind_task(struct task_struct *tsk, const nodemask_t *new)\n{\n\tmpol_rebind_policy(tsk->mempolicy, new);\n}\n\n \n\nvoid mpol_rebind_mm(struct mm_struct *mm, nodemask_t *new)\n{\n\tstruct vm_area_struct *vma;\n\tVMA_ITERATOR(vmi, mm, 0);\n\n\tmmap_write_lock(mm);\n\tfor_each_vma(vmi, vma) {\n\t\tvma_start_write(vma);\n\t\tmpol_rebind_policy(vma->vm_policy, new);\n\t}\n\tmmap_write_unlock(mm);\n}\n\nstatic const struct mempolicy_operations mpol_ops[MPOL_MAX] = {\n\t[MPOL_DEFAULT] = {\n\t\t.rebind = mpol_rebind_default,\n\t},\n\t[MPOL_INTERLEAVE] = {\n\t\t.create = mpol_new_nodemask,\n\t\t.rebind = mpol_rebind_nodemask,\n\t},\n\t[MPOL_PREFERRED] = {\n\t\t.create = mpol_new_preferred,\n\t\t.rebind = mpol_rebind_preferred,\n\t},\n\t[MPOL_BIND] = {\n\t\t.create = mpol_new_nodemask,\n\t\t.rebind = mpol_rebind_nodemask,\n\t},\n\t[MPOL_LOCAL] = {\n\t\t.rebind = mpol_rebind_default,\n\t},\n\t[MPOL_PREFERRED_MANY] = {\n\t\t.create = mpol_new_nodemask,\n\t\t.rebind = mpol_rebind_preferred,\n\t},\n};\n\nstatic int migrate_folio_add(struct folio *folio, struct list_head *foliolist,\n\t\t\t\tunsigned long flags);\n\nstruct queue_pages {\n\tstruct list_head *pagelist;\n\tunsigned long flags;\n\tnodemask_t *nmask;\n\tunsigned long start;\n\tunsigned long end;\n\tstruct vm_area_struct *first;\n\tbool has_unmovable;\n};\n\n \nstatic inline bool queue_folio_required(struct folio *folio,\n\t\t\t\t\tstruct queue_pages *qp)\n{\n\tint nid = folio_nid(folio);\n\tunsigned long flags = qp->flags;\n\n\treturn node_isset(nid, *qp->nmask) == !(flags & MPOL_MF_INVERT);\n}\n\n \nstatic int queue_folios_pmd(pmd_t *pmd, spinlock_t *ptl, unsigned long addr,\n\t\t\t\tunsigned long end, struct mm_walk *walk)\n\t__releases(ptl)\n{\n\tint ret = 0;\n\tstruct folio *folio;\n\tstruct queue_pages *qp = walk->private;\n\tunsigned long flags;\n\n\tif (unlikely(is_pmd_migration_entry(*pmd))) {\n\t\tret = -EIO;\n\t\tgoto unlock;\n\t}\n\tfolio = pfn_folio(pmd_pfn(*pmd));\n\tif (is_huge_zero_page(&folio->page)) {\n\t\twalk->action = ACTION_CONTINUE;\n\t\tgoto unlock;\n\t}\n\tif (!queue_folio_required(folio, qp))\n\t\tgoto unlock;\n\n\tflags = qp->flags;\n\t \n\tif (flags & (MPOL_MF_MOVE | MPOL_MF_MOVE_ALL)) {\n\t\tif (!vma_migratable(walk->vma) ||\n\t\t    migrate_folio_add(folio, qp->pagelist, flags)) {\n\t\t\tqp->has_unmovable = true;\n\t\t\tgoto unlock;\n\t\t}\n\t} else\n\t\tret = -EIO;\nunlock:\n\tspin_unlock(ptl);\n\treturn ret;\n}\n\n \nstatic int queue_folios_pte_range(pmd_t *pmd, unsigned long addr,\n\t\t\tunsigned long end, struct mm_walk *walk)\n{\n\tstruct vm_area_struct *vma = walk->vma;\n\tstruct folio *folio;\n\tstruct queue_pages *qp = walk->private;\n\tunsigned long flags = qp->flags;\n\tpte_t *pte, *mapped_pte;\n\tpte_t ptent;\n\tspinlock_t *ptl;\n\n\tptl = pmd_trans_huge_lock(pmd, vma);\n\tif (ptl)\n\t\treturn queue_folios_pmd(pmd, ptl, addr, end, walk);\n\n\tmapped_pte = pte = pte_offset_map_lock(walk->mm, pmd, addr, &ptl);\n\tif (!pte) {\n\t\twalk->action = ACTION_AGAIN;\n\t\treturn 0;\n\t}\n\tfor (; addr != end; pte++, addr += PAGE_SIZE) {\n\t\tptent = ptep_get(pte);\n\t\tif (!pte_present(ptent))\n\t\t\tcontinue;\n\t\tfolio = vm_normal_folio(vma, addr, ptent);\n\t\tif (!folio || folio_is_zone_device(folio))\n\t\t\tcontinue;\n\t\t \n\t\tif (folio_test_reserved(folio))\n\t\t\tcontinue;\n\t\tif (!queue_folio_required(folio, qp))\n\t\t\tcontinue;\n\t\tif (flags & (MPOL_MF_MOVE | MPOL_MF_MOVE_ALL)) {\n\t\t\t \n\t\t\tif (!vma_migratable(vma))\n\t\t\t\tqp->has_unmovable = true;\n\n\t\t\t \n\t\t\tif (migrate_folio_add(folio, qp->pagelist, flags))\n\t\t\t\tqp->has_unmovable = true;\n\t\t} else\n\t\t\tbreak;\n\t}\n\tpte_unmap_unlock(mapped_pte, ptl);\n\tcond_resched();\n\n\treturn addr != end ? -EIO : 0;\n}\n\nstatic int queue_folios_hugetlb(pte_t *pte, unsigned long hmask,\n\t\t\t       unsigned long addr, unsigned long end,\n\t\t\t       struct mm_walk *walk)\n{\n\tint ret = 0;\n#ifdef CONFIG_HUGETLB_PAGE\n\tstruct queue_pages *qp = walk->private;\n\tunsigned long flags = (qp->flags & MPOL_MF_VALID);\n\tstruct folio *folio;\n\tspinlock_t *ptl;\n\tpte_t entry;\n\n\tptl = huge_pte_lock(hstate_vma(walk->vma), walk->mm, pte);\n\tentry = huge_ptep_get(pte);\n\tif (!pte_present(entry))\n\t\tgoto unlock;\n\tfolio = pfn_folio(pte_pfn(entry));\n\tif (!queue_folio_required(folio, qp))\n\t\tgoto unlock;\n\n\tif (flags == MPOL_MF_STRICT) {\n\t\t \n\t\tret = -EIO;\n\t\tgoto unlock;\n\t}\n\n\tif (!vma_migratable(walk->vma)) {\n\t\t \n\t\tqp->has_unmovable = true;\n\t\tgoto unlock;\n\t}\n\n\t \n\tif (flags & (MPOL_MF_MOVE_ALL) ||\n\t    (flags & MPOL_MF_MOVE && folio_estimated_sharers(folio) == 1 &&\n\t     !hugetlb_pmd_shared(pte))) {\n\t\tif (!isolate_hugetlb(folio, qp->pagelist) &&\n\t\t\t(flags & MPOL_MF_STRICT))\n\t\t\t \n\t\t\tqp->has_unmovable = true;\n\t}\nunlock:\n\tspin_unlock(ptl);\n#else\n\tBUG();\n#endif\n\treturn ret;\n}\n\n#ifdef CONFIG_NUMA_BALANCING\n \nunsigned long change_prot_numa(struct vm_area_struct *vma,\n\t\t\tunsigned long addr, unsigned long end)\n{\n\tstruct mmu_gather tlb;\n\tlong nr_updated;\n\n\ttlb_gather_mmu(&tlb, vma->vm_mm);\n\n\tnr_updated = change_protection(&tlb, vma, addr, end, MM_CP_PROT_NUMA);\n\tif (nr_updated > 0)\n\t\tcount_vm_numa_events(NUMA_PTE_UPDATES, nr_updated);\n\n\ttlb_finish_mmu(&tlb);\n\n\treturn nr_updated;\n}\n#else\nstatic unsigned long change_prot_numa(struct vm_area_struct *vma,\n\t\t\tunsigned long addr, unsigned long end)\n{\n\treturn 0;\n}\n#endif  \n\nstatic int queue_pages_test_walk(unsigned long start, unsigned long end,\n\t\t\t\tstruct mm_walk *walk)\n{\n\tstruct vm_area_struct *next, *vma = walk->vma;\n\tstruct queue_pages *qp = walk->private;\n\tunsigned long endvma = vma->vm_end;\n\tunsigned long flags = qp->flags;\n\n\t \n\tVM_BUG_ON_VMA(!range_in_vma(vma, start, end), vma);\n\n\tif (!qp->first) {\n\t\tqp->first = vma;\n\t\tif (!(flags & MPOL_MF_DISCONTIG_OK) &&\n\t\t\t(qp->start < vma->vm_start))\n\t\t\t \n\t\t\treturn -EFAULT;\n\t}\n\tnext = find_vma(vma->vm_mm, vma->vm_end);\n\tif (!(flags & MPOL_MF_DISCONTIG_OK) &&\n\t\t((vma->vm_end < qp->end) &&\n\t\t(!next || vma->vm_end < next->vm_start)))\n\t\t \n\t\treturn -EFAULT;\n\n\t \n\tif (!vma_migratable(vma) &&\n\t    !(flags & MPOL_MF_STRICT))\n\t\treturn 1;\n\n\tif (endvma > end)\n\t\tendvma = end;\n\n\tif (flags & MPOL_MF_LAZY) {\n\t\t \n\t\tif (!is_vm_hugetlb_page(vma) && vma_is_accessible(vma) &&\n\t\t\t!(vma->vm_flags & VM_MIXEDMAP))\n\t\t\tchange_prot_numa(vma, start, endvma);\n\t\treturn 1;\n\t}\n\n\t \n\tif (flags & MPOL_MF_VALID)\n\t\treturn 0;\n\treturn 1;\n}\n\nstatic const struct mm_walk_ops queue_pages_walk_ops = {\n\t.hugetlb_entry\t\t= queue_folios_hugetlb,\n\t.pmd_entry\t\t= queue_folios_pte_range,\n\t.test_walk\t\t= queue_pages_test_walk,\n\t.walk_lock\t\t= PGWALK_RDLOCK,\n};\n\nstatic const struct mm_walk_ops queue_pages_lock_vma_walk_ops = {\n\t.hugetlb_entry\t\t= queue_folios_hugetlb,\n\t.pmd_entry\t\t= queue_folios_pte_range,\n\t.test_walk\t\t= queue_pages_test_walk,\n\t.walk_lock\t\t= PGWALK_WRLOCK,\n};\n\n \nstatic int\nqueue_pages_range(struct mm_struct *mm, unsigned long start, unsigned long end,\n\t\tnodemask_t *nodes, unsigned long flags,\n\t\tstruct list_head *pagelist, bool lock_vma)\n{\n\tint err;\n\tstruct queue_pages qp = {\n\t\t.pagelist = pagelist,\n\t\t.flags = flags,\n\t\t.nmask = nodes,\n\t\t.start = start,\n\t\t.end = end,\n\t\t.first = NULL,\n\t\t.has_unmovable = false,\n\t};\n\tconst struct mm_walk_ops *ops = lock_vma ?\n\t\t\t&queue_pages_lock_vma_walk_ops : &queue_pages_walk_ops;\n\n\terr = walk_page_range(mm, start, end, ops, &qp);\n\n\tif (qp.has_unmovable)\n\t\terr = 1;\n\tif (!qp.first)\n\t\t \n\t\terr = -EFAULT;\n\n\treturn err;\n}\n\n \nstatic int vma_replace_policy(struct vm_area_struct *vma,\n\t\t\t\t\t\tstruct mempolicy *pol)\n{\n\tint err;\n\tstruct mempolicy *old;\n\tstruct mempolicy *new;\n\n\tvma_assert_write_locked(vma);\n\n\tpr_debug(\"vma %lx-%lx/%lx vm_ops %p vm_file %p set_policy %p\\n\",\n\t\t vma->vm_start, vma->vm_end, vma->vm_pgoff,\n\t\t vma->vm_ops, vma->vm_file,\n\t\t vma->vm_ops ? vma->vm_ops->set_policy : NULL);\n\n\tnew = mpol_dup(pol);\n\tif (IS_ERR(new))\n\t\treturn PTR_ERR(new);\n\n\tif (vma->vm_ops && vma->vm_ops->set_policy) {\n\t\terr = vma->vm_ops->set_policy(vma, new);\n\t\tif (err)\n\t\t\tgoto err_out;\n\t}\n\n\told = vma->vm_policy;\n\tvma->vm_policy = new;  \n\tmpol_put(old);\n\n\treturn 0;\n err_out:\n\tmpol_put(new);\n\treturn err;\n}\n\n \nstatic int mbind_range(struct vma_iterator *vmi, struct vm_area_struct *vma,\n\t\tstruct vm_area_struct **prev, unsigned long start,\n\t\tunsigned long end, struct mempolicy *new_pol)\n{\n\tstruct vm_area_struct *merged;\n\tunsigned long vmstart, vmend;\n\tpgoff_t pgoff;\n\tint err;\n\n\tvmend = min(end, vma->vm_end);\n\tif (start > vma->vm_start) {\n\t\t*prev = vma;\n\t\tvmstart = start;\n\t} else {\n\t\tvmstart = vma->vm_start;\n\t}\n\n\tif (mpol_equal(vma_policy(vma), new_pol)) {\n\t\t*prev = vma;\n\t\treturn 0;\n\t}\n\n\tpgoff = vma->vm_pgoff + ((vmstart - vma->vm_start) >> PAGE_SHIFT);\n\tmerged = vma_merge(vmi, vma->vm_mm, *prev, vmstart, vmend, vma->vm_flags,\n\t\t\t vma->anon_vma, vma->vm_file, pgoff, new_pol,\n\t\t\t vma->vm_userfaultfd_ctx, anon_vma_name(vma));\n\tif (merged) {\n\t\t*prev = merged;\n\t\treturn vma_replace_policy(merged, new_pol);\n\t}\n\n\tif (vma->vm_start != vmstart) {\n\t\terr = split_vma(vmi, vma, vmstart, 1);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (vma->vm_end != vmend) {\n\t\terr = split_vma(vmi, vma, vmend, 0);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\t*prev = vma;\n\treturn vma_replace_policy(vma, new_pol);\n}\n\n \nstatic long do_set_mempolicy(unsigned short mode, unsigned short flags,\n\t\t\t     nodemask_t *nodes)\n{\n\tstruct mempolicy *new, *old;\n\tNODEMASK_SCRATCH(scratch);\n\tint ret;\n\n\tif (!scratch)\n\t\treturn -ENOMEM;\n\n\tnew = mpol_new(mode, flags, nodes);\n\tif (IS_ERR(new)) {\n\t\tret = PTR_ERR(new);\n\t\tgoto out;\n\t}\n\n\ttask_lock(current);\n\tret = mpol_set_nodemask(new, nodes, scratch);\n\tif (ret) {\n\t\ttask_unlock(current);\n\t\tmpol_put(new);\n\t\tgoto out;\n\t}\n\n\told = current->mempolicy;\n\tcurrent->mempolicy = new;\n\tif (new && new->mode == MPOL_INTERLEAVE)\n\t\tcurrent->il_prev = MAX_NUMNODES-1;\n\ttask_unlock(current);\n\tmpol_put(old);\n\tret = 0;\nout:\n\tNODEMASK_SCRATCH_FREE(scratch);\n\treturn ret;\n}\n\n \nstatic void get_policy_nodemask(struct mempolicy *p, nodemask_t *nodes)\n{\n\tnodes_clear(*nodes);\n\tif (p == &default_policy)\n\t\treturn;\n\n\tswitch (p->mode) {\n\tcase MPOL_BIND:\n\tcase MPOL_INTERLEAVE:\n\tcase MPOL_PREFERRED:\n\tcase MPOL_PREFERRED_MANY:\n\t\t*nodes = p->nodes;\n\t\tbreak;\n\tcase MPOL_LOCAL:\n\t\t \n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n}\n\nstatic int lookup_node(struct mm_struct *mm, unsigned long addr)\n{\n\tstruct page *p = NULL;\n\tint ret;\n\n\tret = get_user_pages_fast(addr & PAGE_MASK, 1, 0, &p);\n\tif (ret > 0) {\n\t\tret = page_to_nid(p);\n\t\tput_page(p);\n\t}\n\treturn ret;\n}\n\n \nstatic long do_get_mempolicy(int *policy, nodemask_t *nmask,\n\t\t\t     unsigned long addr, unsigned long flags)\n{\n\tint err;\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma = NULL;\n\tstruct mempolicy *pol = current->mempolicy, *pol_refcount = NULL;\n\n\tif (flags &\n\t\t~(unsigned long)(MPOL_F_NODE|MPOL_F_ADDR|MPOL_F_MEMS_ALLOWED))\n\t\treturn -EINVAL;\n\n\tif (flags & MPOL_F_MEMS_ALLOWED) {\n\t\tif (flags & (MPOL_F_NODE|MPOL_F_ADDR))\n\t\t\treturn -EINVAL;\n\t\t*policy = 0;\t \n\t\ttask_lock(current);\n\t\t*nmask  = cpuset_current_mems_allowed;\n\t\ttask_unlock(current);\n\t\treturn 0;\n\t}\n\n\tif (flags & MPOL_F_ADDR) {\n\t\t \n\t\tmmap_read_lock(mm);\n\t\tvma = vma_lookup(mm, addr);\n\t\tif (!vma) {\n\t\t\tmmap_read_unlock(mm);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tif (vma->vm_ops && vma->vm_ops->get_policy)\n\t\t\tpol = vma->vm_ops->get_policy(vma, addr);\n\t\telse\n\t\t\tpol = vma->vm_policy;\n\t} else if (addr)\n\t\treturn -EINVAL;\n\n\tif (!pol)\n\t\tpol = &default_policy;\t \n\n\tif (flags & MPOL_F_NODE) {\n\t\tif (flags & MPOL_F_ADDR) {\n\t\t\t \n\t\t\tpol_refcount = pol;\n\t\t\tvma = NULL;\n\t\t\tmpol_get(pol);\n\t\t\tmmap_read_unlock(mm);\n\t\t\terr = lookup_node(mm, addr);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t\t*policy = err;\n\t\t} else if (pol == current->mempolicy &&\n\t\t\t\tpol->mode == MPOL_INTERLEAVE) {\n\t\t\t*policy = next_node_in(current->il_prev, pol->nodes);\n\t\t} else {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\t*policy = pol == &default_policy ? MPOL_DEFAULT :\n\t\t\t\t\t\tpol->mode;\n\t\t \n\t\t*policy |= (pol->flags & MPOL_MODE_FLAGS);\n\t}\n\n\terr = 0;\n\tif (nmask) {\n\t\tif (mpol_store_user_nodemask(pol)) {\n\t\t\t*nmask = pol->w.user_nodemask;\n\t\t} else {\n\t\t\ttask_lock(current);\n\t\t\tget_policy_nodemask(pol, nmask);\n\t\t\ttask_unlock(current);\n\t\t}\n\t}\n\n out:\n\tmpol_cond_put(pol);\n\tif (vma)\n\t\tmmap_read_unlock(mm);\n\tif (pol_refcount)\n\t\tmpol_put(pol_refcount);\n\treturn err;\n}\n\n#ifdef CONFIG_MIGRATION\nstatic int migrate_folio_add(struct folio *folio, struct list_head *foliolist,\n\t\t\t\tunsigned long flags)\n{\n\t \n\tif ((flags & MPOL_MF_MOVE_ALL) || folio_estimated_sharers(folio) == 1) {\n\t\tif (folio_isolate_lru(folio)) {\n\t\t\tlist_add_tail(&folio->lru, foliolist);\n\t\t\tnode_stat_mod_folio(folio,\n\t\t\t\tNR_ISOLATED_ANON + folio_is_file_lru(folio),\n\t\t\t\tfolio_nr_pages(folio));\n\t\t} else if (flags & MPOL_MF_STRICT) {\n\t\t\t \n\t\t\treturn -EIO;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic int migrate_to_node(struct mm_struct *mm, int source, int dest,\n\t\t\t   int flags)\n{\n\tnodemask_t nmask;\n\tstruct vm_area_struct *vma;\n\tLIST_HEAD(pagelist);\n\tint err = 0;\n\tstruct migration_target_control mtc = {\n\t\t.nid = dest,\n\t\t.gfp_mask = GFP_HIGHUSER_MOVABLE | __GFP_THISNODE,\n\t};\n\n\tnodes_clear(nmask);\n\tnode_set(source, nmask);\n\n\t \n\tvma = find_vma(mm, 0);\n\tVM_BUG_ON(!(flags & (MPOL_MF_MOVE | MPOL_MF_MOVE_ALL)));\n\tqueue_pages_range(mm, vma->vm_start, mm->task_size, &nmask,\n\t\t\tflags | MPOL_MF_DISCONTIG_OK, &pagelist, false);\n\n\tif (!list_empty(&pagelist)) {\n\t\terr = migrate_pages(&pagelist, alloc_migration_target, NULL,\n\t\t\t\t(unsigned long)&mtc, MIGRATE_SYNC, MR_SYSCALL, NULL);\n\t\tif (err)\n\t\t\tputback_movable_pages(&pagelist);\n\t}\n\n\treturn err;\n}\n\n \nint do_migrate_pages(struct mm_struct *mm, const nodemask_t *from,\n\t\t     const nodemask_t *to, int flags)\n{\n\tint busy = 0;\n\tint err = 0;\n\tnodemask_t tmp;\n\n\tlru_cache_disable();\n\n\tmmap_read_lock(mm);\n\n\t \n\n\ttmp = *from;\n\twhile (!nodes_empty(tmp)) {\n\t\tint s, d;\n\t\tint source = NUMA_NO_NODE;\n\t\tint dest = 0;\n\n\t\tfor_each_node_mask(s, tmp) {\n\n\t\t\t \n\n\t\t\tif ((nodes_weight(*from) != nodes_weight(*to)) &&\n\t\t\t\t\t\t(node_isset(s, *to)))\n\t\t\t\tcontinue;\n\n\t\t\td = node_remap(s, *from, *to);\n\t\t\tif (s == d)\n\t\t\t\tcontinue;\n\n\t\t\tsource = s;\t \n\t\t\tdest = d;\n\n\t\t\t \n\t\t\tif (!node_isset(dest, tmp))\n\t\t\t\tbreak;\n\t\t}\n\t\tif (source == NUMA_NO_NODE)\n\t\t\tbreak;\n\n\t\tnode_clear(source, tmp);\n\t\terr = migrate_to_node(mm, source, dest, flags);\n\t\tif (err > 0)\n\t\t\tbusy += err;\n\t\tif (err < 0)\n\t\t\tbreak;\n\t}\n\tmmap_read_unlock(mm);\n\n\tlru_cache_enable();\n\tif (err < 0)\n\t\treturn err;\n\treturn busy;\n\n}\n\n \nstatic struct folio *new_folio(struct folio *src, unsigned long start)\n{\n\tstruct vm_area_struct *vma;\n\tunsigned long address;\n\tVMA_ITERATOR(vmi, current->mm, start);\n\tgfp_t gfp = GFP_HIGHUSER_MOVABLE | __GFP_RETRY_MAYFAIL;\n\n\tfor_each_vma(vmi, vma) {\n\t\taddress = page_address_in_vma(&src->page, vma);\n\t\tif (address != -EFAULT)\n\t\t\tbreak;\n\t}\n\n\tif (folio_test_hugetlb(src)) {\n\t\treturn alloc_hugetlb_folio_vma(folio_hstate(src),\n\t\t\t\tvma, address);\n\t}\n\n\tif (folio_test_large(src))\n\t\tgfp = GFP_TRANSHUGE;\n\n\t \n\treturn vma_alloc_folio(gfp, folio_order(src), vma, address,\n\t\t\tfolio_test_large(src));\n}\n#else\n\nstatic int migrate_folio_add(struct folio *folio, struct list_head *foliolist,\n\t\t\t\tunsigned long flags)\n{\n\treturn -EIO;\n}\n\nint do_migrate_pages(struct mm_struct *mm, const nodemask_t *from,\n\t\t     const nodemask_t *to, int flags)\n{\n\treturn -ENOSYS;\n}\n\nstatic struct folio *new_folio(struct folio *src, unsigned long start)\n{\n\treturn NULL;\n}\n#endif\n\nstatic long do_mbind(unsigned long start, unsigned long len,\n\t\t     unsigned short mode, unsigned short mode_flags,\n\t\t     nodemask_t *nmask, unsigned long flags)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma, *prev;\n\tstruct vma_iterator vmi;\n\tstruct mempolicy *new;\n\tunsigned long end;\n\tint err;\n\tint ret;\n\tLIST_HEAD(pagelist);\n\n\tif (flags & ~(unsigned long)MPOL_MF_VALID)\n\t\treturn -EINVAL;\n\tif ((flags & MPOL_MF_MOVE_ALL) && !capable(CAP_SYS_NICE))\n\t\treturn -EPERM;\n\n\tif (start & ~PAGE_MASK)\n\t\treturn -EINVAL;\n\n\tif (mode == MPOL_DEFAULT)\n\t\tflags &= ~MPOL_MF_STRICT;\n\n\tlen = PAGE_ALIGN(len);\n\tend = start + len;\n\n\tif (end < start)\n\t\treturn -EINVAL;\n\tif (end == start)\n\t\treturn 0;\n\n\tnew = mpol_new(mode, mode_flags, nmask);\n\tif (IS_ERR(new))\n\t\treturn PTR_ERR(new);\n\n\tif (flags & MPOL_MF_LAZY)\n\t\tnew->flags |= MPOL_F_MOF;\n\n\t \n\tif (!new)\n\t\tflags |= MPOL_MF_DISCONTIG_OK;\n\n\tpr_debug(\"mbind %lx-%lx mode:%d flags:%d nodes:%lx\\n\",\n\t\t start, start + len, mode, mode_flags,\n\t\t nmask ? nodes_addr(*nmask)[0] : NUMA_NO_NODE);\n\n\tif (flags & (MPOL_MF_MOVE | MPOL_MF_MOVE_ALL)) {\n\n\t\tlru_cache_disable();\n\t}\n\t{\n\t\tNODEMASK_SCRATCH(scratch);\n\t\tif (scratch) {\n\t\t\tmmap_write_lock(mm);\n\t\t\terr = mpol_set_nodemask(new, nmask, scratch);\n\t\t\tif (err)\n\t\t\t\tmmap_write_unlock(mm);\n\t\t} else\n\t\t\terr = -ENOMEM;\n\t\tNODEMASK_SCRATCH_FREE(scratch);\n\t}\n\tif (err)\n\t\tgoto mpol_out;\n\n\t \n\tret = queue_pages_range(mm, start, end, nmask,\n\t\t\t  flags | MPOL_MF_INVERT, &pagelist, true);\n\n\tif (ret < 0) {\n\t\terr = ret;\n\t\tgoto up_out;\n\t}\n\n\tvma_iter_init(&vmi, mm, start);\n\tprev = vma_prev(&vmi);\n\tfor_each_vma_range(vmi, vma, end) {\n\t\terr = mbind_range(&vmi, vma, &prev, start, end, new);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\n\tif (!err) {\n\t\tint nr_failed = 0;\n\n\t\tif (!list_empty(&pagelist)) {\n\t\t\tWARN_ON_ONCE(flags & MPOL_MF_LAZY);\n\t\t\tnr_failed = migrate_pages(&pagelist, new_folio, NULL,\n\t\t\t\tstart, MIGRATE_SYNC, MR_MEMPOLICY_MBIND, NULL);\n\t\t\tif (nr_failed)\n\t\t\t\tputback_movable_pages(&pagelist);\n\t\t}\n\n\t\tif (((ret > 0) || nr_failed) && (flags & MPOL_MF_STRICT))\n\t\t\terr = -EIO;\n\t} else {\nup_out:\n\t\tif (!list_empty(&pagelist))\n\t\t\tputback_movable_pages(&pagelist);\n\t}\n\n\tmmap_write_unlock(mm);\nmpol_out:\n\tmpol_put(new);\n\tif (flags & (MPOL_MF_MOVE | MPOL_MF_MOVE_ALL))\n\t\tlru_cache_enable();\n\treturn err;\n}\n\n \nstatic int get_bitmap(unsigned long *mask, const unsigned long __user *nmask,\n\t\t      unsigned long maxnode)\n{\n\tunsigned long nlongs = BITS_TO_LONGS(maxnode);\n\tint ret;\n\n\tif (in_compat_syscall())\n\t\tret = compat_get_bitmap(mask,\n\t\t\t\t\t(const compat_ulong_t __user *)nmask,\n\t\t\t\t\tmaxnode);\n\telse\n\t\tret = copy_from_user(mask, nmask,\n\t\t\t\t     nlongs * sizeof(unsigned long));\n\n\tif (ret)\n\t\treturn -EFAULT;\n\n\tif (maxnode % BITS_PER_LONG)\n\t\tmask[nlongs - 1] &= (1UL << (maxnode % BITS_PER_LONG)) - 1;\n\n\treturn 0;\n}\n\n \nstatic int get_nodes(nodemask_t *nodes, const unsigned long __user *nmask,\n\t\t     unsigned long maxnode)\n{\n\t--maxnode;\n\tnodes_clear(*nodes);\n\tif (maxnode == 0 || !nmask)\n\t\treturn 0;\n\tif (maxnode > PAGE_SIZE*BITS_PER_BYTE)\n\t\treturn -EINVAL;\n\n\t \n\twhile (maxnode > MAX_NUMNODES) {\n\t\tunsigned long bits = min_t(unsigned long, maxnode, BITS_PER_LONG);\n\t\tunsigned long t;\n\n\t\tif (get_bitmap(&t, &nmask[(maxnode - 1) / BITS_PER_LONG], bits))\n\t\t\treturn -EFAULT;\n\n\t\tif (maxnode - bits >= MAX_NUMNODES) {\n\t\t\tmaxnode -= bits;\n\t\t} else {\n\t\t\tmaxnode = MAX_NUMNODES;\n\t\t\tt &= ~((1UL << (MAX_NUMNODES % BITS_PER_LONG)) - 1);\n\t\t}\n\t\tif (t)\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn get_bitmap(nodes_addr(*nodes), nmask, maxnode);\n}\n\n \nstatic int copy_nodes_to_user(unsigned long __user *mask, unsigned long maxnode,\n\t\t\t      nodemask_t *nodes)\n{\n\tunsigned long copy = ALIGN(maxnode-1, 64) / 8;\n\tunsigned int nbytes = BITS_TO_LONGS(nr_node_ids) * sizeof(long);\n\tbool compat = in_compat_syscall();\n\n\tif (compat)\n\t\tnbytes = BITS_TO_COMPAT_LONGS(nr_node_ids) * sizeof(compat_long_t);\n\n\tif (copy > nbytes) {\n\t\tif (copy > PAGE_SIZE)\n\t\t\treturn -EINVAL;\n\t\tif (clear_user((char __user *)mask + nbytes, copy - nbytes))\n\t\t\treturn -EFAULT;\n\t\tcopy = nbytes;\n\t\tmaxnode = nr_node_ids;\n\t}\n\n\tif (compat)\n\t\treturn compat_put_bitmap((compat_ulong_t __user *)mask,\n\t\t\t\t\t nodes_addr(*nodes), maxnode);\n\n\treturn copy_to_user(mask, nodes_addr(*nodes), copy) ? -EFAULT : 0;\n}\n\n \nstatic inline int sanitize_mpol_flags(int *mode, unsigned short *flags)\n{\n\t*flags = *mode & MPOL_MODE_FLAGS;\n\t*mode &= ~MPOL_MODE_FLAGS;\n\n\tif ((unsigned int)(*mode) >=  MPOL_MAX)\n\t\treturn -EINVAL;\n\tif ((*flags & MPOL_F_STATIC_NODES) && (*flags & MPOL_F_RELATIVE_NODES))\n\t\treturn -EINVAL;\n\tif (*flags & MPOL_F_NUMA_BALANCING) {\n\t\tif (*mode != MPOL_BIND)\n\t\t\treturn -EINVAL;\n\t\t*flags |= (MPOL_F_MOF | MPOL_F_MORON);\n\t}\n\treturn 0;\n}\n\nstatic long kernel_mbind(unsigned long start, unsigned long len,\n\t\t\t unsigned long mode, const unsigned long __user *nmask,\n\t\t\t unsigned long maxnode, unsigned int flags)\n{\n\tunsigned short mode_flags;\n\tnodemask_t nodes;\n\tint lmode = mode;\n\tint err;\n\n\tstart = untagged_addr(start);\n\terr = sanitize_mpol_flags(&lmode, &mode_flags);\n\tif (err)\n\t\treturn err;\n\n\terr = get_nodes(&nodes, nmask, maxnode);\n\tif (err)\n\t\treturn err;\n\n\treturn do_mbind(start, len, lmode, mode_flags, &nodes, flags);\n}\n\nSYSCALL_DEFINE4(set_mempolicy_home_node, unsigned long, start, unsigned long, len,\n\t\tunsigned long, home_node, unsigned long, flags)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma, *prev;\n\tstruct mempolicy *new, *old;\n\tunsigned long end;\n\tint err = -ENOENT;\n\tVMA_ITERATOR(vmi, mm, start);\n\n\tstart = untagged_addr(start);\n\tif (start & ~PAGE_MASK)\n\t\treturn -EINVAL;\n\t \n\tif (flags != 0)\n\t\treturn -EINVAL;\n\n\t \n\tif (home_node >= MAX_NUMNODES || !node_online(home_node))\n\t\treturn -EINVAL;\n\n\tlen = PAGE_ALIGN(len);\n\tend = start + len;\n\n\tif (end < start)\n\t\treturn -EINVAL;\n\tif (end == start)\n\t\treturn 0;\n\tmmap_write_lock(mm);\n\tprev = vma_prev(&vmi);\n\tfor_each_vma_range(vmi, vma, end) {\n\t\t \n\t\told = vma_policy(vma);\n\t\tif (!old) {\n\t\t\tprev = vma;\n\t\t\tcontinue;\n\t\t}\n\t\tif (old->mode != MPOL_BIND && old->mode != MPOL_PREFERRED_MANY) {\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tbreak;\n\t\t}\n\t\tnew = mpol_dup(old);\n\t\tif (IS_ERR(new)) {\n\t\t\terr = PTR_ERR(new);\n\t\t\tbreak;\n\t\t}\n\n\t\tvma_start_write(vma);\n\t\tnew->home_node = home_node;\n\t\terr = mbind_range(&vmi, vma, &prev, start, end, new);\n\t\tmpol_put(new);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\tmmap_write_unlock(mm);\n\treturn err;\n}\n\nSYSCALL_DEFINE6(mbind, unsigned long, start, unsigned long, len,\n\t\tunsigned long, mode, const unsigned long __user *, nmask,\n\t\tunsigned long, maxnode, unsigned int, flags)\n{\n\treturn kernel_mbind(start, len, mode, nmask, maxnode, flags);\n}\n\n \nstatic long kernel_set_mempolicy(int mode, const unsigned long __user *nmask,\n\t\t\t\t unsigned long maxnode)\n{\n\tunsigned short mode_flags;\n\tnodemask_t nodes;\n\tint lmode = mode;\n\tint err;\n\n\terr = sanitize_mpol_flags(&lmode, &mode_flags);\n\tif (err)\n\t\treturn err;\n\n\terr = get_nodes(&nodes, nmask, maxnode);\n\tif (err)\n\t\treturn err;\n\n\treturn do_set_mempolicy(lmode, mode_flags, &nodes);\n}\n\nSYSCALL_DEFINE3(set_mempolicy, int, mode, const unsigned long __user *, nmask,\n\t\tunsigned long, maxnode)\n{\n\treturn kernel_set_mempolicy(mode, nmask, maxnode);\n}\n\nstatic int kernel_migrate_pages(pid_t pid, unsigned long maxnode,\n\t\t\t\tconst unsigned long __user *old_nodes,\n\t\t\t\tconst unsigned long __user *new_nodes)\n{\n\tstruct mm_struct *mm = NULL;\n\tstruct task_struct *task;\n\tnodemask_t task_nodes;\n\tint err;\n\tnodemask_t *old;\n\tnodemask_t *new;\n\tNODEMASK_SCRATCH(scratch);\n\n\tif (!scratch)\n\t\treturn -ENOMEM;\n\n\told = &scratch->mask1;\n\tnew = &scratch->mask2;\n\n\terr = get_nodes(old, old_nodes, maxnode);\n\tif (err)\n\t\tgoto out;\n\n\terr = get_nodes(new, new_nodes, maxnode);\n\tif (err)\n\t\tgoto out;\n\n\t \n\trcu_read_lock();\n\ttask = pid ? find_task_by_vpid(pid) : current;\n\tif (!task) {\n\t\trcu_read_unlock();\n\t\terr = -ESRCH;\n\t\tgoto out;\n\t}\n\tget_task_struct(task);\n\n\terr = -EINVAL;\n\n\t \n\tif (!ptrace_may_access(task, PTRACE_MODE_READ_REALCREDS)) {\n\t\trcu_read_unlock();\n\t\terr = -EPERM;\n\t\tgoto out_put;\n\t}\n\trcu_read_unlock();\n\n\ttask_nodes = cpuset_mems_allowed(task);\n\t \n\tif (!nodes_subset(*new, task_nodes) && !capable(CAP_SYS_NICE)) {\n\t\terr = -EPERM;\n\t\tgoto out_put;\n\t}\n\n\ttask_nodes = cpuset_mems_allowed(current);\n\tnodes_and(*new, *new, task_nodes);\n\tif (nodes_empty(*new))\n\t\tgoto out_put;\n\n\terr = security_task_movememory(task);\n\tif (err)\n\t\tgoto out_put;\n\n\tmm = get_task_mm(task);\n\tput_task_struct(task);\n\n\tif (!mm) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\terr = do_migrate_pages(mm, old, new,\n\t\tcapable(CAP_SYS_NICE) ? MPOL_MF_MOVE_ALL : MPOL_MF_MOVE);\n\n\tmmput(mm);\nout:\n\tNODEMASK_SCRATCH_FREE(scratch);\n\n\treturn err;\n\nout_put:\n\tput_task_struct(task);\n\tgoto out;\n\n}\n\nSYSCALL_DEFINE4(migrate_pages, pid_t, pid, unsigned long, maxnode,\n\t\tconst unsigned long __user *, old_nodes,\n\t\tconst unsigned long __user *, new_nodes)\n{\n\treturn kernel_migrate_pages(pid, maxnode, old_nodes, new_nodes);\n}\n\n\n \nstatic int kernel_get_mempolicy(int __user *policy,\n\t\t\t\tunsigned long __user *nmask,\n\t\t\t\tunsigned long maxnode,\n\t\t\t\tunsigned long addr,\n\t\t\t\tunsigned long flags)\n{\n\tint err;\n\tint pval;\n\tnodemask_t nodes;\n\n\tif (nmask != NULL && maxnode < nr_node_ids)\n\t\treturn -EINVAL;\n\n\taddr = untagged_addr(addr);\n\n\terr = do_get_mempolicy(&pval, &nodes, addr, flags);\n\n\tif (err)\n\t\treturn err;\n\n\tif (policy && put_user(pval, policy))\n\t\treturn -EFAULT;\n\n\tif (nmask)\n\t\terr = copy_nodes_to_user(nmask, maxnode, &nodes);\n\n\treturn err;\n}\n\nSYSCALL_DEFINE5(get_mempolicy, int __user *, policy,\n\t\tunsigned long __user *, nmask, unsigned long, maxnode,\n\t\tunsigned long, addr, unsigned long, flags)\n{\n\treturn kernel_get_mempolicy(policy, nmask, maxnode, addr, flags);\n}\n\nbool vma_migratable(struct vm_area_struct *vma)\n{\n\tif (vma->vm_flags & (VM_IO | VM_PFNMAP))\n\t\treturn false;\n\n\t \n\tif (vma_is_dax(vma))\n\t\treturn false;\n\n\tif (is_vm_hugetlb_page(vma) &&\n\t\t!hugepage_migration_supported(hstate_vma(vma)))\n\t\treturn false;\n\n\t \n\tif (vma->vm_file &&\n\t\tgfp_zone(mapping_gfp_mask(vma->vm_file->f_mapping))\n\t\t\t< policy_zone)\n\t\treturn false;\n\treturn true;\n}\n\nstruct mempolicy *__get_vma_policy(struct vm_area_struct *vma,\n\t\t\t\t\t\tunsigned long addr)\n{\n\tstruct mempolicy *pol = NULL;\n\n\tif (vma) {\n\t\tif (vma->vm_ops && vma->vm_ops->get_policy) {\n\t\t\tpol = vma->vm_ops->get_policy(vma, addr);\n\t\t} else if (vma->vm_policy) {\n\t\t\tpol = vma->vm_policy;\n\n\t\t\t \n\t\t\tif (mpol_needs_cond_ref(pol))\n\t\t\t\tmpol_get(pol);\n\t\t}\n\t}\n\n\treturn pol;\n}\n\n \nstatic struct mempolicy *get_vma_policy(struct vm_area_struct *vma,\n\t\t\t\t\t\tunsigned long addr)\n{\n\tstruct mempolicy *pol = __get_vma_policy(vma, addr);\n\n\tif (!pol)\n\t\tpol = get_task_policy(current);\n\n\treturn pol;\n}\n\nbool vma_policy_mof(struct vm_area_struct *vma)\n{\n\tstruct mempolicy *pol;\n\n\tif (vma->vm_ops && vma->vm_ops->get_policy) {\n\t\tbool ret = false;\n\n\t\tpol = vma->vm_ops->get_policy(vma, vma->vm_start);\n\t\tif (pol && (pol->flags & MPOL_F_MOF))\n\t\t\tret = true;\n\t\tmpol_cond_put(pol);\n\n\t\treturn ret;\n\t}\n\n\tpol = vma->vm_policy;\n\tif (!pol)\n\t\tpol = get_task_policy(current);\n\n\treturn pol->flags & MPOL_F_MOF;\n}\n\nbool apply_policy_zone(struct mempolicy *policy, enum zone_type zone)\n{\n\tenum zone_type dynamic_policy_zone = policy_zone;\n\n\tBUG_ON(dynamic_policy_zone == ZONE_MOVABLE);\n\n\t \n\tif (!nodes_intersects(policy->nodes, node_states[N_HIGH_MEMORY]))\n\t\tdynamic_policy_zone = ZONE_MOVABLE;\n\n\treturn zone >= dynamic_policy_zone;\n}\n\n \nnodemask_t *policy_nodemask(gfp_t gfp, struct mempolicy *policy)\n{\n\tint mode = policy->mode;\n\n\t \n\tif (unlikely(mode == MPOL_BIND) &&\n\t\tapply_policy_zone(policy, gfp_zone(gfp)) &&\n\t\tcpuset_nodemask_valid_mems_allowed(&policy->nodes))\n\t\treturn &policy->nodes;\n\n\tif (mode == MPOL_PREFERRED_MANY)\n\t\treturn &policy->nodes;\n\n\treturn NULL;\n}\n\n \nstatic int policy_node(gfp_t gfp, struct mempolicy *policy, int nd)\n{\n\tif (policy->mode == MPOL_PREFERRED) {\n\t\tnd = first_node(policy->nodes);\n\t} else {\n\t\t \n\t\tWARN_ON_ONCE(policy->mode == MPOL_BIND && (gfp & __GFP_THISNODE));\n\t}\n\n\tif ((policy->mode == MPOL_BIND ||\n\t     policy->mode == MPOL_PREFERRED_MANY) &&\n\t    policy->home_node != NUMA_NO_NODE)\n\t\treturn policy->home_node;\n\n\treturn nd;\n}\n\n \nstatic unsigned interleave_nodes(struct mempolicy *policy)\n{\n\tunsigned next;\n\tstruct task_struct *me = current;\n\n\tnext = next_node_in(me->il_prev, policy->nodes);\n\tif (next < MAX_NUMNODES)\n\t\tme->il_prev = next;\n\treturn next;\n}\n\n \nunsigned int mempolicy_slab_node(void)\n{\n\tstruct mempolicy *policy;\n\tint node = numa_mem_id();\n\n\tif (!in_task())\n\t\treturn node;\n\n\tpolicy = current->mempolicy;\n\tif (!policy)\n\t\treturn node;\n\n\tswitch (policy->mode) {\n\tcase MPOL_PREFERRED:\n\t\treturn first_node(policy->nodes);\n\n\tcase MPOL_INTERLEAVE:\n\t\treturn interleave_nodes(policy);\n\n\tcase MPOL_BIND:\n\tcase MPOL_PREFERRED_MANY:\n\t{\n\t\tstruct zoneref *z;\n\n\t\t \n\t\tstruct zonelist *zonelist;\n\t\tenum zone_type highest_zoneidx = gfp_zone(GFP_KERNEL);\n\t\tzonelist = &NODE_DATA(node)->node_zonelists[ZONELIST_FALLBACK];\n\t\tz = first_zones_zonelist(zonelist, highest_zoneidx,\n\t\t\t\t\t\t\t&policy->nodes);\n\t\treturn z->zone ? zone_to_nid(z->zone) : node;\n\t}\n\tcase MPOL_LOCAL:\n\t\treturn node;\n\n\tdefault:\n\t\tBUG();\n\t}\n}\n\n \nstatic unsigned offset_il_node(struct mempolicy *pol, unsigned long n)\n{\n\tnodemask_t nodemask = pol->nodes;\n\tunsigned int target, nnodes;\n\tint i;\n\tint nid;\n\t \n\tbarrier();\n\n\tnnodes = nodes_weight(nodemask);\n\tif (!nnodes)\n\t\treturn numa_node_id();\n\ttarget = (unsigned int)n % nnodes;\n\tnid = first_node(nodemask);\n\tfor (i = 0; i < target; i++)\n\t\tnid = next_node(nid, nodemask);\n\treturn nid;\n}\n\n \nstatic inline unsigned interleave_nid(struct mempolicy *pol,\n\t\t struct vm_area_struct *vma, unsigned long addr, int shift)\n{\n\tif (vma) {\n\t\tunsigned long off;\n\n\t\t \n\t\tBUG_ON(shift < PAGE_SHIFT);\n\t\toff = vma->vm_pgoff >> (shift - PAGE_SHIFT);\n\t\toff += (addr - vma->vm_start) >> shift;\n\t\treturn offset_il_node(pol, off);\n\t} else\n\t\treturn interleave_nodes(pol);\n}\n\n#ifdef CONFIG_HUGETLBFS\n \nint huge_node(struct vm_area_struct *vma, unsigned long addr, gfp_t gfp_flags,\n\t\t\t\tstruct mempolicy **mpol, nodemask_t **nodemask)\n{\n\tint nid;\n\tint mode;\n\n\t*mpol = get_vma_policy(vma, addr);\n\t*nodemask = NULL;\n\tmode = (*mpol)->mode;\n\n\tif (unlikely(mode == MPOL_INTERLEAVE)) {\n\t\tnid = interleave_nid(*mpol, vma, addr,\n\t\t\t\t\thuge_page_shift(hstate_vma(vma)));\n\t} else {\n\t\tnid = policy_node(gfp_flags, *mpol, numa_node_id());\n\t\tif (mode == MPOL_BIND || mode == MPOL_PREFERRED_MANY)\n\t\t\t*nodemask = &(*mpol)->nodes;\n\t}\n\treturn nid;\n}\n\n \nbool init_nodemask_of_mempolicy(nodemask_t *mask)\n{\n\tstruct mempolicy *mempolicy;\n\n\tif (!(mask && current->mempolicy))\n\t\treturn false;\n\n\ttask_lock(current);\n\tmempolicy = current->mempolicy;\n\tswitch (mempolicy->mode) {\n\tcase MPOL_PREFERRED:\n\tcase MPOL_PREFERRED_MANY:\n\tcase MPOL_BIND:\n\tcase MPOL_INTERLEAVE:\n\t\t*mask = mempolicy->nodes;\n\t\tbreak;\n\n\tcase MPOL_LOCAL:\n\t\tinit_nodemask_of_node(mask, numa_node_id());\n\t\tbreak;\n\n\tdefault:\n\t\tBUG();\n\t}\n\ttask_unlock(current);\n\n\treturn true;\n}\n#endif\n\n \nbool mempolicy_in_oom_domain(struct task_struct *tsk,\n\t\t\t\t\tconst nodemask_t *mask)\n{\n\tstruct mempolicy *mempolicy;\n\tbool ret = true;\n\n\tif (!mask)\n\t\treturn ret;\n\n\ttask_lock(tsk);\n\tmempolicy = tsk->mempolicy;\n\tif (mempolicy && mempolicy->mode == MPOL_BIND)\n\t\tret = nodes_intersects(mempolicy->nodes, *mask);\n\ttask_unlock(tsk);\n\n\treturn ret;\n}\n\n \nstatic struct page *alloc_page_interleave(gfp_t gfp, unsigned order,\n\t\t\t\t\tunsigned nid)\n{\n\tstruct page *page;\n\n\tpage = __alloc_pages(gfp, order, nid, NULL);\n\t \n\tif (!static_branch_likely(&vm_numa_stat_key))\n\t\treturn page;\n\tif (page && page_to_nid(page) == nid) {\n\t\tpreempt_disable();\n\t\t__count_numa_event(page_zone(page), NUMA_INTERLEAVE_HIT);\n\t\tpreempt_enable();\n\t}\n\treturn page;\n}\n\nstatic struct page *alloc_pages_preferred_many(gfp_t gfp, unsigned int order,\n\t\t\t\t\t\tint nid, struct mempolicy *pol)\n{\n\tstruct page *page;\n\tgfp_t preferred_gfp;\n\n\t \n\tpreferred_gfp = gfp | __GFP_NOWARN;\n\tpreferred_gfp &= ~(__GFP_DIRECT_RECLAIM | __GFP_NOFAIL);\n\tpage = __alloc_pages(preferred_gfp, order, nid, &pol->nodes);\n\tif (!page)\n\t\tpage = __alloc_pages(gfp, order, nid, NULL);\n\n\treturn page;\n}\n\n \nstruct folio *vma_alloc_folio(gfp_t gfp, int order, struct vm_area_struct *vma,\n\t\tunsigned long addr, bool hugepage)\n{\n\tstruct mempolicy *pol;\n\tint node = numa_node_id();\n\tstruct folio *folio;\n\tint preferred_nid;\n\tnodemask_t *nmask;\n\n\tpol = get_vma_policy(vma, addr);\n\n\tif (pol->mode == MPOL_INTERLEAVE) {\n\t\tstruct page *page;\n\t\tunsigned nid;\n\n\t\tnid = interleave_nid(pol, vma, addr, PAGE_SHIFT + order);\n\t\tmpol_cond_put(pol);\n\t\tgfp |= __GFP_COMP;\n\t\tpage = alloc_page_interleave(gfp, order, nid);\n\t\tfolio = (struct folio *)page;\n\t\tif (folio && order > 1)\n\t\t\tfolio_prep_large_rmappable(folio);\n\t\tgoto out;\n\t}\n\n\tif (pol->mode == MPOL_PREFERRED_MANY) {\n\t\tstruct page *page;\n\n\t\tnode = policy_node(gfp, pol, node);\n\t\tgfp |= __GFP_COMP;\n\t\tpage = alloc_pages_preferred_many(gfp, order, node, pol);\n\t\tmpol_cond_put(pol);\n\t\tfolio = (struct folio *)page;\n\t\tif (folio && order > 1)\n\t\t\tfolio_prep_large_rmappable(folio);\n\t\tgoto out;\n\t}\n\n\tif (unlikely(IS_ENABLED(CONFIG_TRANSPARENT_HUGEPAGE) && hugepage)) {\n\t\tint hpage_node = node;\n\n\t\t \n\t\tif (pol->mode == MPOL_PREFERRED)\n\t\t\thpage_node = first_node(pol->nodes);\n\n\t\tnmask = policy_nodemask(gfp, pol);\n\t\tif (!nmask || node_isset(hpage_node, *nmask)) {\n\t\t\tmpol_cond_put(pol);\n\t\t\t \n\t\t\tfolio = __folio_alloc_node(gfp | __GFP_THISNODE |\n\t\t\t\t\t__GFP_NORETRY, order, hpage_node);\n\n\t\t\t \n\t\t\tif (!folio && (gfp & __GFP_DIRECT_RECLAIM))\n\t\t\t\tfolio = __folio_alloc(gfp, order, hpage_node,\n\t\t\t\t\t\t      nmask);\n\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tnmask = policy_nodemask(gfp, pol);\n\tpreferred_nid = policy_node(gfp, pol, node);\n\tfolio = __folio_alloc(gfp, order, preferred_nid, nmask);\n\tmpol_cond_put(pol);\nout:\n\treturn folio;\n}\nEXPORT_SYMBOL(vma_alloc_folio);\n\n \nstruct page *alloc_pages(gfp_t gfp, unsigned order)\n{\n\tstruct mempolicy *pol = &default_policy;\n\tstruct page *page;\n\n\tif (!in_interrupt() && !(gfp & __GFP_THISNODE))\n\t\tpol = get_task_policy(current);\n\n\t \n\tif (pol->mode == MPOL_INTERLEAVE)\n\t\tpage = alloc_page_interleave(gfp, order, interleave_nodes(pol));\n\telse if (pol->mode == MPOL_PREFERRED_MANY)\n\t\tpage = alloc_pages_preferred_many(gfp, order,\n\t\t\t\t  policy_node(gfp, pol, numa_node_id()), pol);\n\telse\n\t\tpage = __alloc_pages(gfp, order,\n\t\t\t\tpolicy_node(gfp, pol, numa_node_id()),\n\t\t\t\tpolicy_nodemask(gfp, pol));\n\n\treturn page;\n}\nEXPORT_SYMBOL(alloc_pages);\n\nstruct folio *folio_alloc(gfp_t gfp, unsigned order)\n{\n\tstruct page *page = alloc_pages(gfp | __GFP_COMP, order);\n\tstruct folio *folio = (struct folio *)page;\n\n\tif (folio && order > 1)\n\t\tfolio_prep_large_rmappable(folio);\n\treturn folio;\n}\nEXPORT_SYMBOL(folio_alloc);\n\nstatic unsigned long alloc_pages_bulk_array_interleave(gfp_t gfp,\n\t\tstruct mempolicy *pol, unsigned long nr_pages,\n\t\tstruct page **page_array)\n{\n\tint nodes;\n\tunsigned long nr_pages_per_node;\n\tint delta;\n\tint i;\n\tunsigned long nr_allocated;\n\tunsigned long total_allocated = 0;\n\n\tnodes = nodes_weight(pol->nodes);\n\tnr_pages_per_node = nr_pages / nodes;\n\tdelta = nr_pages - nodes * nr_pages_per_node;\n\n\tfor (i = 0; i < nodes; i++) {\n\t\tif (delta) {\n\t\t\tnr_allocated = __alloc_pages_bulk(gfp,\n\t\t\t\t\tinterleave_nodes(pol), NULL,\n\t\t\t\t\tnr_pages_per_node + 1, NULL,\n\t\t\t\t\tpage_array);\n\t\t\tdelta--;\n\t\t} else {\n\t\t\tnr_allocated = __alloc_pages_bulk(gfp,\n\t\t\t\t\tinterleave_nodes(pol), NULL,\n\t\t\t\t\tnr_pages_per_node, NULL, page_array);\n\t\t}\n\n\t\tpage_array += nr_allocated;\n\t\ttotal_allocated += nr_allocated;\n\t}\n\n\treturn total_allocated;\n}\n\nstatic unsigned long alloc_pages_bulk_array_preferred_many(gfp_t gfp, int nid,\n\t\tstruct mempolicy *pol, unsigned long nr_pages,\n\t\tstruct page **page_array)\n{\n\tgfp_t preferred_gfp;\n\tunsigned long nr_allocated = 0;\n\n\tpreferred_gfp = gfp | __GFP_NOWARN;\n\tpreferred_gfp &= ~(__GFP_DIRECT_RECLAIM | __GFP_NOFAIL);\n\n\tnr_allocated  = __alloc_pages_bulk(preferred_gfp, nid, &pol->nodes,\n\t\t\t\t\t   nr_pages, NULL, page_array);\n\n\tif (nr_allocated < nr_pages)\n\t\tnr_allocated += __alloc_pages_bulk(gfp, numa_node_id(), NULL,\n\t\t\t\tnr_pages - nr_allocated, NULL,\n\t\t\t\tpage_array + nr_allocated);\n\treturn nr_allocated;\n}\n\n \nunsigned long alloc_pages_bulk_array_mempolicy(gfp_t gfp,\n\t\tunsigned long nr_pages, struct page **page_array)\n{\n\tstruct mempolicy *pol = &default_policy;\n\n\tif (!in_interrupt() && !(gfp & __GFP_THISNODE))\n\t\tpol = get_task_policy(current);\n\n\tif (pol->mode == MPOL_INTERLEAVE)\n\t\treturn alloc_pages_bulk_array_interleave(gfp, pol,\n\t\t\t\t\t\t\t nr_pages, page_array);\n\n\tif (pol->mode == MPOL_PREFERRED_MANY)\n\t\treturn alloc_pages_bulk_array_preferred_many(gfp,\n\t\t\t\tnuma_node_id(), pol, nr_pages, page_array);\n\n\treturn __alloc_pages_bulk(gfp, policy_node(gfp, pol, numa_node_id()),\n\t\t\t\t  policy_nodemask(gfp, pol), nr_pages, NULL,\n\t\t\t\t  page_array);\n}\n\nint vma_dup_policy(struct vm_area_struct *src, struct vm_area_struct *dst)\n{\n\tstruct mempolicy *pol = mpol_dup(vma_policy(src));\n\n\tif (IS_ERR(pol))\n\t\treturn PTR_ERR(pol);\n\tdst->vm_policy = pol;\n\treturn 0;\n}\n\n \n\n \nstruct mempolicy *__mpol_dup(struct mempolicy *old)\n{\n\tstruct mempolicy *new = kmem_cache_alloc(policy_cache, GFP_KERNEL);\n\n\tif (!new)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\t \n\tif (old == current->mempolicy) {\n\t\ttask_lock(current);\n\t\t*new = *old;\n\t\ttask_unlock(current);\n\t} else\n\t\t*new = *old;\n\n\tif (current_cpuset_is_being_rebound()) {\n\t\tnodemask_t mems = cpuset_mems_allowed(current);\n\t\tmpol_rebind_policy(new, &mems);\n\t}\n\tatomic_set(&new->refcnt, 1);\n\treturn new;\n}\n\n \nbool __mpol_equal(struct mempolicy *a, struct mempolicy *b)\n{\n\tif (!a || !b)\n\t\treturn false;\n\tif (a->mode != b->mode)\n\t\treturn false;\n\tif (a->flags != b->flags)\n\t\treturn false;\n\tif (a->home_node != b->home_node)\n\t\treturn false;\n\tif (mpol_store_user_nodemask(a))\n\t\tif (!nodes_equal(a->w.user_nodemask, b->w.user_nodemask))\n\t\t\treturn false;\n\n\tswitch (a->mode) {\n\tcase MPOL_BIND:\n\tcase MPOL_INTERLEAVE:\n\tcase MPOL_PREFERRED:\n\tcase MPOL_PREFERRED_MANY:\n\t\treturn !!nodes_equal(a->nodes, b->nodes);\n\tcase MPOL_LOCAL:\n\t\treturn true;\n\tdefault:\n\t\tBUG();\n\t\treturn false;\n\t}\n}\n\n \n\n \nstatic struct sp_node *\nsp_lookup(struct shared_policy *sp, unsigned long start, unsigned long end)\n{\n\tstruct rb_node *n = sp->root.rb_node;\n\n\twhile (n) {\n\t\tstruct sp_node *p = rb_entry(n, struct sp_node, nd);\n\n\t\tif (start >= p->end)\n\t\t\tn = n->rb_right;\n\t\telse if (end <= p->start)\n\t\t\tn = n->rb_left;\n\t\telse\n\t\t\tbreak;\n\t}\n\tif (!n)\n\t\treturn NULL;\n\tfor (;;) {\n\t\tstruct sp_node *w = NULL;\n\t\tstruct rb_node *prev = rb_prev(n);\n\t\tif (!prev)\n\t\t\tbreak;\n\t\tw = rb_entry(prev, struct sp_node, nd);\n\t\tif (w->end <= start)\n\t\t\tbreak;\n\t\tn = prev;\n\t}\n\treturn rb_entry(n, struct sp_node, nd);\n}\n\n \nstatic void sp_insert(struct shared_policy *sp, struct sp_node *new)\n{\n\tstruct rb_node **p = &sp->root.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct sp_node *nd;\n\n\twhile (*p) {\n\t\tparent = *p;\n\t\tnd = rb_entry(parent, struct sp_node, nd);\n\t\tif (new->start < nd->start)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (new->end > nd->end)\n\t\t\tp = &(*p)->rb_right;\n\t\telse\n\t\t\tBUG();\n\t}\n\trb_link_node(&new->nd, parent, p);\n\trb_insert_color(&new->nd, &sp->root);\n\tpr_debug(\"inserting %lx-%lx: %d\\n\", new->start, new->end,\n\t\t new->policy ? new->policy->mode : 0);\n}\n\n \nstruct mempolicy *\nmpol_shared_policy_lookup(struct shared_policy *sp, unsigned long idx)\n{\n\tstruct mempolicy *pol = NULL;\n\tstruct sp_node *sn;\n\n\tif (!sp->root.rb_node)\n\t\treturn NULL;\n\tread_lock(&sp->lock);\n\tsn = sp_lookup(sp, idx, idx+1);\n\tif (sn) {\n\t\tmpol_get(sn->policy);\n\t\tpol = sn->policy;\n\t}\n\tread_unlock(&sp->lock);\n\treturn pol;\n}\n\nstatic void sp_free(struct sp_node *n)\n{\n\tmpol_put(n->policy);\n\tkmem_cache_free(sn_cache, n);\n}\n\n \nint mpol_misplaced(struct page *page, struct vm_area_struct *vma, unsigned long addr)\n{\n\tstruct mempolicy *pol;\n\tstruct zoneref *z;\n\tint curnid = page_to_nid(page);\n\tunsigned long pgoff;\n\tint thiscpu = raw_smp_processor_id();\n\tint thisnid = cpu_to_node(thiscpu);\n\tint polnid = NUMA_NO_NODE;\n\tint ret = NUMA_NO_NODE;\n\n\tpol = get_vma_policy(vma, addr);\n\tif (!(pol->flags & MPOL_F_MOF))\n\t\tgoto out;\n\n\tswitch (pol->mode) {\n\tcase MPOL_INTERLEAVE:\n\t\tpgoff = vma->vm_pgoff;\n\t\tpgoff += (addr - vma->vm_start) >> PAGE_SHIFT;\n\t\tpolnid = offset_il_node(pol, pgoff);\n\t\tbreak;\n\n\tcase MPOL_PREFERRED:\n\t\tif (node_isset(curnid, pol->nodes))\n\t\t\tgoto out;\n\t\tpolnid = first_node(pol->nodes);\n\t\tbreak;\n\n\tcase MPOL_LOCAL:\n\t\tpolnid = numa_node_id();\n\t\tbreak;\n\n\tcase MPOL_BIND:\n\t\t \n\t\tif (pol->flags & MPOL_F_MORON) {\n\t\t\tif (node_isset(thisnid, pol->nodes))\n\t\t\t\tbreak;\n\t\t\tgoto out;\n\t\t}\n\t\tfallthrough;\n\n\tcase MPOL_PREFERRED_MANY:\n\t\t \n\t\tif (node_isset(curnid, pol->nodes))\n\t\t\tgoto out;\n\t\tz = first_zones_zonelist(\n\t\t\t\tnode_zonelist(numa_node_id(), GFP_HIGHUSER),\n\t\t\t\tgfp_zone(GFP_HIGHUSER),\n\t\t\t\t&pol->nodes);\n\t\tpolnid = zone_to_nid(z->zone);\n\t\tbreak;\n\n\tdefault:\n\t\tBUG();\n\t}\n\n\t \n\tif (pol->flags & MPOL_F_MORON) {\n\t\tpolnid = thisnid;\n\n\t\tif (!should_numa_migrate_memory(current, page, curnid, thiscpu))\n\t\t\tgoto out;\n\t}\n\n\tif (curnid != polnid)\n\t\tret = polnid;\nout:\n\tmpol_cond_put(pol);\n\n\treturn ret;\n}\n\n \nvoid mpol_put_task_policy(struct task_struct *task)\n{\n\tstruct mempolicy *pol;\n\n\ttask_lock(task);\n\tpol = task->mempolicy;\n\ttask->mempolicy = NULL;\n\ttask_unlock(task);\n\tmpol_put(pol);\n}\n\nstatic void sp_delete(struct shared_policy *sp, struct sp_node *n)\n{\n\tpr_debug(\"deleting %lx-l%lx\\n\", n->start, n->end);\n\trb_erase(&n->nd, &sp->root);\n\tsp_free(n);\n}\n\nstatic void sp_node_init(struct sp_node *node, unsigned long start,\n\t\t\tunsigned long end, struct mempolicy *pol)\n{\n\tnode->start = start;\n\tnode->end = end;\n\tnode->policy = pol;\n}\n\nstatic struct sp_node *sp_alloc(unsigned long start, unsigned long end,\n\t\t\t\tstruct mempolicy *pol)\n{\n\tstruct sp_node *n;\n\tstruct mempolicy *newpol;\n\n\tn = kmem_cache_alloc(sn_cache, GFP_KERNEL);\n\tif (!n)\n\t\treturn NULL;\n\n\tnewpol = mpol_dup(pol);\n\tif (IS_ERR(newpol)) {\n\t\tkmem_cache_free(sn_cache, n);\n\t\treturn NULL;\n\t}\n\tnewpol->flags |= MPOL_F_SHARED;\n\tsp_node_init(n, start, end, newpol);\n\n\treturn n;\n}\n\n \nstatic int shared_policy_replace(struct shared_policy *sp, unsigned long start,\n\t\t\t\t unsigned long end, struct sp_node *new)\n{\n\tstruct sp_node *n;\n\tstruct sp_node *n_new = NULL;\n\tstruct mempolicy *mpol_new = NULL;\n\tint ret = 0;\n\nrestart:\n\twrite_lock(&sp->lock);\n\tn = sp_lookup(sp, start, end);\n\t \n\twhile (n && n->start < end) {\n\t\tstruct rb_node *next = rb_next(&n->nd);\n\t\tif (n->start >= start) {\n\t\t\tif (n->end <= end)\n\t\t\t\tsp_delete(sp, n);\n\t\t\telse\n\t\t\t\tn->start = end;\n\t\t} else {\n\t\t\t \n\t\t\tif (n->end > end) {\n\t\t\t\tif (!n_new)\n\t\t\t\t\tgoto alloc_new;\n\n\t\t\t\t*mpol_new = *n->policy;\n\t\t\t\tatomic_set(&mpol_new->refcnt, 1);\n\t\t\t\tsp_node_init(n_new, end, n->end, mpol_new);\n\t\t\t\tn->end = start;\n\t\t\t\tsp_insert(sp, n_new);\n\t\t\t\tn_new = NULL;\n\t\t\t\tmpol_new = NULL;\n\t\t\t\tbreak;\n\t\t\t} else\n\t\t\t\tn->end = start;\n\t\t}\n\t\tif (!next)\n\t\t\tbreak;\n\t\tn = rb_entry(next, struct sp_node, nd);\n\t}\n\tif (new)\n\t\tsp_insert(sp, new);\n\twrite_unlock(&sp->lock);\n\tret = 0;\n\nerr_out:\n\tif (mpol_new)\n\t\tmpol_put(mpol_new);\n\tif (n_new)\n\t\tkmem_cache_free(sn_cache, n_new);\n\n\treturn ret;\n\nalloc_new:\n\twrite_unlock(&sp->lock);\n\tret = -ENOMEM;\n\tn_new = kmem_cache_alloc(sn_cache, GFP_KERNEL);\n\tif (!n_new)\n\t\tgoto err_out;\n\tmpol_new = kmem_cache_alloc(policy_cache, GFP_KERNEL);\n\tif (!mpol_new)\n\t\tgoto err_out;\n\tatomic_set(&mpol_new->refcnt, 1);\n\tgoto restart;\n}\n\n \nvoid mpol_shared_policy_init(struct shared_policy *sp, struct mempolicy *mpol)\n{\n\tint ret;\n\n\tsp->root = RB_ROOT;\t\t \n\trwlock_init(&sp->lock);\n\n\tif (mpol) {\n\t\tstruct vm_area_struct pvma;\n\t\tstruct mempolicy *new;\n\t\tNODEMASK_SCRATCH(scratch);\n\n\t\tif (!scratch)\n\t\t\tgoto put_mpol;\n\t\t \n\t\tnew = mpol_new(mpol->mode, mpol->flags, &mpol->w.user_nodemask);\n\t\tif (IS_ERR(new))\n\t\t\tgoto free_scratch;  \n\n\t\ttask_lock(current);\n\t\tret = mpol_set_nodemask(new, &mpol->w.user_nodemask, scratch);\n\t\ttask_unlock(current);\n\t\tif (ret)\n\t\t\tgoto put_new;\n\n\t\t \n\t\tvma_init(&pvma, NULL);\n\t\tpvma.vm_end = TASK_SIZE;\t \n\t\tmpol_set_shared_policy(sp, &pvma, new);  \n\nput_new:\n\t\tmpol_put(new);\t\t\t \nfree_scratch:\n\t\tNODEMASK_SCRATCH_FREE(scratch);\nput_mpol:\n\t\tmpol_put(mpol);\t \n\t}\n}\n\nint mpol_set_shared_policy(struct shared_policy *info,\n\t\t\tstruct vm_area_struct *vma, struct mempolicy *npol)\n{\n\tint err;\n\tstruct sp_node *new = NULL;\n\tunsigned long sz = vma_pages(vma);\n\n\tpr_debug(\"set_shared_policy %lx sz %lu %d %d %lx\\n\",\n\t\t vma->vm_pgoff,\n\t\t sz, npol ? npol->mode : -1,\n\t\t npol ? npol->flags : -1,\n\t\t npol ? nodes_addr(npol->nodes)[0] : NUMA_NO_NODE);\n\n\tif (npol) {\n\t\tnew = sp_alloc(vma->vm_pgoff, vma->vm_pgoff + sz, npol);\n\t\tif (!new)\n\t\t\treturn -ENOMEM;\n\t}\n\terr = shared_policy_replace(info, vma->vm_pgoff, vma->vm_pgoff+sz, new);\n\tif (err && new)\n\t\tsp_free(new);\n\treturn err;\n}\n\n \nvoid mpol_free_shared_policy(struct shared_policy *p)\n{\n\tstruct sp_node *n;\n\tstruct rb_node *next;\n\n\tif (!p->root.rb_node)\n\t\treturn;\n\twrite_lock(&p->lock);\n\tnext = rb_first(&p->root);\n\twhile (next) {\n\t\tn = rb_entry(next, struct sp_node, nd);\n\t\tnext = rb_next(&n->nd);\n\t\tsp_delete(p, n);\n\t}\n\twrite_unlock(&p->lock);\n}\n\n#ifdef CONFIG_NUMA_BALANCING\nstatic int __initdata numabalancing_override;\n\nstatic void __init check_numabalancing_enable(void)\n{\n\tbool numabalancing_default = false;\n\n\tif (IS_ENABLED(CONFIG_NUMA_BALANCING_DEFAULT_ENABLED))\n\t\tnumabalancing_default = true;\n\n\t \n\tif (numabalancing_override)\n\t\tset_numabalancing_state(numabalancing_override == 1);\n\n\tif (num_online_nodes() > 1 && !numabalancing_override) {\n\t\tpr_info(\"%s automatic NUMA balancing. Configure with numa_balancing= or the kernel.numa_balancing sysctl\\n\",\n\t\t\tnumabalancing_default ? \"Enabling\" : \"Disabling\");\n\t\tset_numabalancing_state(numabalancing_default);\n\t}\n}\n\nstatic int __init setup_numabalancing(char *str)\n{\n\tint ret = 0;\n\tif (!str)\n\t\tgoto out;\n\n\tif (!strcmp(str, \"enable\")) {\n\t\tnumabalancing_override = 1;\n\t\tret = 1;\n\t} else if (!strcmp(str, \"disable\")) {\n\t\tnumabalancing_override = -1;\n\t\tret = 1;\n\t}\nout:\n\tif (!ret)\n\t\tpr_warn(\"Unable to parse numa_balancing=\\n\");\n\n\treturn ret;\n}\n__setup(\"numa_balancing=\", setup_numabalancing);\n#else\nstatic inline void __init check_numabalancing_enable(void)\n{\n}\n#endif  \n\n \nvoid __init numa_policy_init(void)\n{\n\tnodemask_t interleave_nodes;\n\tunsigned long largest = 0;\n\tint nid, prefer = 0;\n\n\tpolicy_cache = kmem_cache_create(\"numa_policy\",\n\t\t\t\t\t sizeof(struct mempolicy),\n\t\t\t\t\t 0, SLAB_PANIC, NULL);\n\n\tsn_cache = kmem_cache_create(\"shared_policy_node\",\n\t\t\t\t     sizeof(struct sp_node),\n\t\t\t\t     0, SLAB_PANIC, NULL);\n\n\tfor_each_node(nid) {\n\t\tpreferred_node_policy[nid] = (struct mempolicy) {\n\t\t\t.refcnt = ATOMIC_INIT(1),\n\t\t\t.mode = MPOL_PREFERRED,\n\t\t\t.flags = MPOL_F_MOF | MPOL_F_MORON,\n\t\t\t.nodes = nodemask_of_node(nid),\n\t\t};\n\t}\n\n\t \n\tnodes_clear(interleave_nodes);\n\tfor_each_node_state(nid, N_MEMORY) {\n\t\tunsigned long total_pages = node_present_pages(nid);\n\n\t\t \n\t\tif (largest < total_pages) {\n\t\t\tlargest = total_pages;\n\t\t\tprefer = nid;\n\t\t}\n\n\t\t \n\t\tif ((total_pages << PAGE_SHIFT) >= (16 << 20))\n\t\t\tnode_set(nid, interleave_nodes);\n\t}\n\n\t \n\tif (unlikely(nodes_empty(interleave_nodes)))\n\t\tnode_set(prefer, interleave_nodes);\n\n\tif (do_set_mempolicy(MPOL_INTERLEAVE, 0, &interleave_nodes))\n\t\tpr_err(\"%s: interleaving failed\\n\", __func__);\n\n\tcheck_numabalancing_enable();\n}\n\n \nvoid numa_default_policy(void)\n{\n\tdo_set_mempolicy(MPOL_DEFAULT, 0, NULL);\n}\n\n \n\nstatic const char * const policy_modes[] =\n{\n\t[MPOL_DEFAULT]    = \"default\",\n\t[MPOL_PREFERRED]  = \"prefer\",\n\t[MPOL_BIND]       = \"bind\",\n\t[MPOL_INTERLEAVE] = \"interleave\",\n\t[MPOL_LOCAL]      = \"local\",\n\t[MPOL_PREFERRED_MANY]  = \"prefer (many)\",\n};\n\n\n#ifdef CONFIG_TMPFS\n \nint mpol_parse_str(char *str, struct mempolicy **mpol)\n{\n\tstruct mempolicy *new = NULL;\n\tunsigned short mode_flags;\n\tnodemask_t nodes;\n\tchar *nodelist = strchr(str, ':');\n\tchar *flags = strchr(str, '=');\n\tint err = 1, mode;\n\n\tif (flags)\n\t\t*flags++ = '\\0';\t \n\n\tif (nodelist) {\n\t\t \n\t\t*nodelist++ = '\\0';\n\t\tif (nodelist_parse(nodelist, nodes))\n\t\t\tgoto out;\n\t\tif (!nodes_subset(nodes, node_states[N_MEMORY]))\n\t\t\tgoto out;\n\t} else\n\t\tnodes_clear(nodes);\n\n\tmode = match_string(policy_modes, MPOL_MAX, str);\n\tif (mode < 0)\n\t\tgoto out;\n\n\tswitch (mode) {\n\tcase MPOL_PREFERRED:\n\t\t \n\t\tif (nodelist) {\n\t\t\tchar *rest = nodelist;\n\t\t\twhile (isdigit(*rest))\n\t\t\t\trest++;\n\t\t\tif (*rest)\n\t\t\t\tgoto out;\n\t\t\tif (nodes_empty(nodes))\n\t\t\t\tgoto out;\n\t\t}\n\t\tbreak;\n\tcase MPOL_INTERLEAVE:\n\t\t \n\t\tif (!nodelist)\n\t\t\tnodes = node_states[N_MEMORY];\n\t\tbreak;\n\tcase MPOL_LOCAL:\n\t\t \n\t\tif (nodelist)\n\t\t\tgoto out;\n\t\tbreak;\n\tcase MPOL_DEFAULT:\n\t\t \n\t\tif (!nodelist)\n\t\t\terr = 0;\n\t\tgoto out;\n\tcase MPOL_PREFERRED_MANY:\n\tcase MPOL_BIND:\n\t\t \n\t\tif (!nodelist)\n\t\t\tgoto out;\n\t}\n\n\tmode_flags = 0;\n\tif (flags) {\n\t\t \n\t\tif (!strcmp(flags, \"static\"))\n\t\t\tmode_flags |= MPOL_F_STATIC_NODES;\n\t\telse if (!strcmp(flags, \"relative\"))\n\t\t\tmode_flags |= MPOL_F_RELATIVE_NODES;\n\t\telse\n\t\t\tgoto out;\n\t}\n\n\tnew = mpol_new(mode, mode_flags, &nodes);\n\tif (IS_ERR(new))\n\t\tgoto out;\n\n\t \n\tif (mode != MPOL_PREFERRED) {\n\t\tnew->nodes = nodes;\n\t} else if (nodelist) {\n\t\tnodes_clear(new->nodes);\n\t\tnode_set(first_node(nodes), new->nodes);\n\t} else {\n\t\tnew->mode = MPOL_LOCAL;\n\t}\n\n\t \n\tnew->w.user_nodemask = nodes;\n\n\terr = 0;\n\nout:\n\t \n\tif (nodelist)\n\t\t*--nodelist = ':';\n\tif (flags)\n\t\t*--flags = '=';\n\tif (!err)\n\t\t*mpol = new;\n\treturn err;\n}\n#endif  \n\n \nvoid mpol_to_str(char *buffer, int maxlen, struct mempolicy *pol)\n{\n\tchar *p = buffer;\n\tnodemask_t nodes = NODE_MASK_NONE;\n\tunsigned short mode = MPOL_DEFAULT;\n\tunsigned short flags = 0;\n\n\tif (pol && pol != &default_policy && !(pol->flags & MPOL_F_MORON)) {\n\t\tmode = pol->mode;\n\t\tflags = pol->flags;\n\t}\n\n\tswitch (mode) {\n\tcase MPOL_DEFAULT:\n\tcase MPOL_LOCAL:\n\t\tbreak;\n\tcase MPOL_PREFERRED:\n\tcase MPOL_PREFERRED_MANY:\n\tcase MPOL_BIND:\n\tcase MPOL_INTERLEAVE:\n\t\tnodes = pol->nodes;\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\tsnprintf(p, maxlen, \"unknown\");\n\t\treturn;\n\t}\n\n\tp += snprintf(p, maxlen, \"%s\", policy_modes[mode]);\n\n\tif (flags & MPOL_MODE_FLAGS) {\n\t\tp += snprintf(p, buffer + maxlen - p, \"=\");\n\n\t\t \n\t\tif (flags & MPOL_F_STATIC_NODES)\n\t\t\tp += snprintf(p, buffer + maxlen - p, \"static\");\n\t\telse if (flags & MPOL_F_RELATIVE_NODES)\n\t\t\tp += snprintf(p, buffer + maxlen - p, \"relative\");\n\t}\n\n\tif (!nodes_empty(nodes))\n\t\tp += scnprintf(p, buffer + maxlen - p, \":%*pbl\",\n\t\t\t       nodemask_pr_args(&nodes));\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}