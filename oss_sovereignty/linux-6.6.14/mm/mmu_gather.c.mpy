{
  "module_name": "mmu_gather.c",
  "hash_id": "25879063dee8942db1638b6854199faa0d995a2345419104d4d2fd4ac3ecb346",
  "original_prompt": "Ingested from linux-6.6.14/mm/mmu_gather.c",
  "human_readable_source": "#include <linux/gfp.h>\n#include <linux/highmem.h>\n#include <linux/kernel.h>\n#include <linux/mmdebug.h>\n#include <linux/mm_types.h>\n#include <linux/mm_inline.h>\n#include <linux/pagemap.h>\n#include <linux/rcupdate.h>\n#include <linux/smp.h>\n#include <linux/swap.h>\n#include <linux/rmap.h>\n\n#include <asm/pgalloc.h>\n#include <asm/tlb.h>\n\n#ifndef CONFIG_MMU_GATHER_NO_GATHER\n\nstatic bool tlb_next_batch(struct mmu_gather *tlb)\n{\n\tstruct mmu_gather_batch *batch;\n\n\t \n\tif (tlb->delayed_rmap && tlb->active != &tlb->local)\n\t\treturn false;\n\n\tbatch = tlb->active;\n\tif (batch->next) {\n\t\ttlb->active = batch->next;\n\t\treturn true;\n\t}\n\n\tif (tlb->batch_count == MAX_GATHER_BATCH_COUNT)\n\t\treturn false;\n\n\tbatch = (void *)__get_free_page(GFP_NOWAIT | __GFP_NOWARN);\n\tif (!batch)\n\t\treturn false;\n\n\ttlb->batch_count++;\n\tbatch->next = NULL;\n\tbatch->nr   = 0;\n\tbatch->max  = MAX_GATHER_BATCH;\n\n\ttlb->active->next = batch;\n\ttlb->active = batch;\n\n\treturn true;\n}\n\n#ifdef CONFIG_SMP\nstatic void tlb_flush_rmap_batch(struct mmu_gather_batch *batch, struct vm_area_struct *vma)\n{\n\tfor (int i = 0; i < batch->nr; i++) {\n\t\tstruct encoded_page *enc = batch->encoded_pages[i];\n\n\t\tif (encoded_page_flags(enc)) {\n\t\t\tstruct page *page = encoded_page_ptr(enc);\n\t\t\tpage_remove_rmap(page, vma, false);\n\t\t}\n\t}\n}\n\n \nvoid tlb_flush_rmaps(struct mmu_gather *tlb, struct vm_area_struct *vma)\n{\n\tif (!tlb->delayed_rmap)\n\t\treturn;\n\n\ttlb_flush_rmap_batch(&tlb->local, vma);\n\tif (tlb->active != &tlb->local)\n\t\ttlb_flush_rmap_batch(tlb->active, vma);\n\ttlb->delayed_rmap = 0;\n}\n#endif\n\nstatic void tlb_batch_pages_flush(struct mmu_gather *tlb)\n{\n\tstruct mmu_gather_batch *batch;\n\n\tfor (batch = &tlb->local; batch && batch->nr; batch = batch->next) {\n\t\tstruct encoded_page **pages = batch->encoded_pages;\n\n\t\tdo {\n\t\t\t \n\t\t\tunsigned int nr = min(512U, batch->nr);\n\n\t\t\tfree_pages_and_swap_cache(pages, nr);\n\t\t\tpages += nr;\n\t\t\tbatch->nr -= nr;\n\n\t\t\tcond_resched();\n\t\t} while (batch->nr);\n\t}\n\ttlb->active = &tlb->local;\n}\n\nstatic void tlb_batch_list_free(struct mmu_gather *tlb)\n{\n\tstruct mmu_gather_batch *batch, *next;\n\n\tfor (batch = tlb->local.next; batch; batch = next) {\n\t\tnext = batch->next;\n\t\tfree_pages((unsigned long)batch, 0);\n\t}\n\ttlb->local.next = NULL;\n}\n\nbool __tlb_remove_page_size(struct mmu_gather *tlb, struct encoded_page *page, int page_size)\n{\n\tstruct mmu_gather_batch *batch;\n\n\tVM_BUG_ON(!tlb->end);\n\n#ifdef CONFIG_MMU_GATHER_PAGE_SIZE\n\tVM_WARN_ON(tlb->page_size != page_size);\n#endif\n\n\tbatch = tlb->active;\n\t \n\tbatch->encoded_pages[batch->nr++] = page;\n\tif (batch->nr == batch->max) {\n\t\tif (!tlb_next_batch(tlb))\n\t\t\treturn true;\n\t\tbatch = tlb->active;\n\t}\n\tVM_BUG_ON_PAGE(batch->nr > batch->max, encoded_page_ptr(page));\n\n\treturn false;\n}\n\n#endif  \n\n#ifdef CONFIG_MMU_GATHER_TABLE_FREE\n\nstatic void __tlb_remove_table_free(struct mmu_table_batch *batch)\n{\n\tint i;\n\n\tfor (i = 0; i < batch->nr; i++)\n\t\t__tlb_remove_table(batch->tables[i]);\n\n\tfree_page((unsigned long)batch);\n}\n\n#ifdef CONFIG_MMU_GATHER_RCU_TABLE_FREE\n\n \n\nstatic void tlb_remove_table_smp_sync(void *arg)\n{\n\t \n}\n\nvoid tlb_remove_table_sync_one(void)\n{\n\t \n\tsmp_call_function(tlb_remove_table_smp_sync, NULL, 1);\n}\n\nstatic void tlb_remove_table_rcu(struct rcu_head *head)\n{\n\t__tlb_remove_table_free(container_of(head, struct mmu_table_batch, rcu));\n}\n\nstatic void tlb_remove_table_free(struct mmu_table_batch *batch)\n{\n\tcall_rcu(&batch->rcu, tlb_remove_table_rcu);\n}\n\n#else  \n\nstatic void tlb_remove_table_free(struct mmu_table_batch *batch)\n{\n\t__tlb_remove_table_free(batch);\n}\n\n#endif  \n\n \nstatic inline void tlb_table_invalidate(struct mmu_gather *tlb)\n{\n\tif (tlb_needs_table_invalidate()) {\n\t\t \n\t\ttlb_flush_mmu_tlbonly(tlb);\n\t}\n}\n\nstatic void tlb_remove_table_one(void *table)\n{\n\ttlb_remove_table_sync_one();\n\t__tlb_remove_table(table);\n}\n\nstatic void tlb_table_flush(struct mmu_gather *tlb)\n{\n\tstruct mmu_table_batch **batch = &tlb->batch;\n\n\tif (*batch) {\n\t\ttlb_table_invalidate(tlb);\n\t\ttlb_remove_table_free(*batch);\n\t\t*batch = NULL;\n\t}\n}\n\nvoid tlb_remove_table(struct mmu_gather *tlb, void *table)\n{\n\tstruct mmu_table_batch **batch = &tlb->batch;\n\n\tif (*batch == NULL) {\n\t\t*batch = (struct mmu_table_batch *)__get_free_page(GFP_NOWAIT | __GFP_NOWARN);\n\t\tif (*batch == NULL) {\n\t\t\ttlb_table_invalidate(tlb);\n\t\t\ttlb_remove_table_one(table);\n\t\t\treturn;\n\t\t}\n\t\t(*batch)->nr = 0;\n\t}\n\n\t(*batch)->tables[(*batch)->nr++] = table;\n\tif ((*batch)->nr == MAX_TABLE_BATCH)\n\t\ttlb_table_flush(tlb);\n}\n\nstatic inline void tlb_table_init(struct mmu_gather *tlb)\n{\n\ttlb->batch = NULL;\n}\n\n#else  \n\nstatic inline void tlb_table_flush(struct mmu_gather *tlb) { }\nstatic inline void tlb_table_init(struct mmu_gather *tlb) { }\n\n#endif  \n\nstatic void tlb_flush_mmu_free(struct mmu_gather *tlb)\n{\n\ttlb_table_flush(tlb);\n#ifndef CONFIG_MMU_GATHER_NO_GATHER\n\ttlb_batch_pages_flush(tlb);\n#endif\n}\n\nvoid tlb_flush_mmu(struct mmu_gather *tlb)\n{\n\ttlb_flush_mmu_tlbonly(tlb);\n\ttlb_flush_mmu_free(tlb);\n}\n\nstatic void __tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm,\n\t\t\t     bool fullmm)\n{\n\ttlb->mm = mm;\n\ttlb->fullmm = fullmm;\n\n#ifndef CONFIG_MMU_GATHER_NO_GATHER\n\ttlb->need_flush_all = 0;\n\ttlb->local.next = NULL;\n\ttlb->local.nr   = 0;\n\ttlb->local.max  = ARRAY_SIZE(tlb->__pages);\n\ttlb->active     = &tlb->local;\n\ttlb->batch_count = 0;\n#endif\n\ttlb->delayed_rmap = 0;\n\n\ttlb_table_init(tlb);\n#ifdef CONFIG_MMU_GATHER_PAGE_SIZE\n\ttlb->page_size = 0;\n#endif\n\n\t__tlb_reset_range(tlb);\n\tinc_tlb_flush_pending(tlb->mm);\n}\n\n \nvoid tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm)\n{\n\t__tlb_gather_mmu(tlb, mm, false);\n}\n\n \nvoid tlb_gather_mmu_fullmm(struct mmu_gather *tlb, struct mm_struct *mm)\n{\n\t__tlb_gather_mmu(tlb, mm, true);\n}\n\n \nvoid tlb_finish_mmu(struct mmu_gather *tlb)\n{\n\t \n\tif (mm_tlb_flush_nested(tlb->mm)) {\n\t\t \n\t\ttlb->fullmm = 1;\n\t\t__tlb_reset_range(tlb);\n\t\ttlb->freed_tables = 1;\n\t}\n\n\ttlb_flush_mmu(tlb);\n\n#ifndef CONFIG_MMU_GATHER_NO_GATHER\n\ttlb_batch_list_free(tlb);\n#endif\n\tdec_tlb_flush_pending(tlb->mm);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}