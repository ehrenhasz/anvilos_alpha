{
  "module_name": "shadow.c",
  "hash_id": "a3ecb8c57acd854b76c76ad636e55f16512df3d13ac8f57f6bb65e86f2938c36",
  "original_prompt": "Ingested from linux-6.6.14/mm/kmsan/shadow.c",
  "human_readable_source": "\n \n\n#include <asm/kmsan.h>\n#include <asm/tlbflush.h>\n#include <linux/cacheflush.h>\n#include <linux/memblock.h>\n#include <linux/mm_types.h>\n#include <linux/slab.h>\n#include <linux/smp.h>\n#include <linux/stddef.h>\n\n#include \"../internal.h\"\n#include \"kmsan.h\"\n\n#define shadow_page_for(page) ((page)->kmsan_shadow)\n\n#define origin_page_for(page) ((page)->kmsan_origin)\n\nstatic void *shadow_ptr_for(struct page *page)\n{\n\treturn page_address(shadow_page_for(page));\n}\n\nstatic void *origin_ptr_for(struct page *page)\n{\n\treturn page_address(origin_page_for(page));\n}\n\nstatic bool page_has_metadata(struct page *page)\n{\n\treturn shadow_page_for(page) && origin_page_for(page);\n}\n\nstatic void set_no_shadow_origin_page(struct page *page)\n{\n\tshadow_page_for(page) = NULL;\n\torigin_page_for(page) = NULL;\n}\n\n \nstatic char dummy_load_page[PAGE_SIZE] __aligned(PAGE_SIZE);\nstatic char dummy_store_page[PAGE_SIZE] __aligned(PAGE_SIZE);\n\nstatic unsigned long vmalloc_meta(void *addr, bool is_origin)\n{\n\tunsigned long addr64 = (unsigned long)addr, off;\n\n\tKMSAN_WARN_ON(is_origin && !IS_ALIGNED(addr64, KMSAN_ORIGIN_SIZE));\n\tif (kmsan_internal_is_vmalloc_addr(addr)) {\n\t\toff = addr64 - VMALLOC_START;\n\t\treturn off + (is_origin ? KMSAN_VMALLOC_ORIGIN_START :\n\t\t\t\t\t  KMSAN_VMALLOC_SHADOW_START);\n\t}\n\tif (kmsan_internal_is_module_addr(addr)) {\n\t\toff = addr64 - MODULES_VADDR;\n\t\treturn off + (is_origin ? KMSAN_MODULES_ORIGIN_START :\n\t\t\t\t\t  KMSAN_MODULES_SHADOW_START);\n\t}\n\treturn 0;\n}\n\nstatic struct page *virt_to_page_or_null(void *vaddr)\n{\n\tif (kmsan_virt_addr_valid(vaddr))\n\t\treturn virt_to_page(vaddr);\n\telse\n\t\treturn NULL;\n}\n\nstruct shadow_origin_ptr kmsan_get_shadow_origin_ptr(void *address, u64 size,\n\t\t\t\t\t\t     bool store)\n{\n\tstruct shadow_origin_ptr ret;\n\tvoid *shadow;\n\n\t \n\tKMSAN_WARN_ON(size > PAGE_SIZE);\n\n\tif (!kmsan_enabled)\n\t\tgoto return_dummy;\n\n\tKMSAN_WARN_ON(!kmsan_metadata_is_contiguous(address, size));\n\tshadow = kmsan_get_metadata(address, KMSAN_META_SHADOW);\n\tif (!shadow)\n\t\tgoto return_dummy;\n\n\tret.shadow = shadow;\n\tret.origin = kmsan_get_metadata(address, KMSAN_META_ORIGIN);\n\treturn ret;\n\nreturn_dummy:\n\tif (store) {\n\t\t \n\t\tret.shadow = dummy_store_page;\n\t\tret.origin = dummy_store_page;\n\t} else {\n\t\t \n\t\tret.shadow = dummy_load_page;\n\t\tret.origin = dummy_load_page;\n\t}\n\treturn ret;\n}\n\n \nvoid *kmsan_get_metadata(void *address, bool is_origin)\n{\n\tu64 addr = (u64)address, pad, off;\n\tstruct page *page;\n\tvoid *ret;\n\n\tif (is_origin && !IS_ALIGNED(addr, KMSAN_ORIGIN_SIZE)) {\n\t\tpad = addr % KMSAN_ORIGIN_SIZE;\n\t\taddr -= pad;\n\t}\n\taddress = (void *)addr;\n\tif (kmsan_internal_is_vmalloc_addr(address) ||\n\t    kmsan_internal_is_module_addr(address))\n\t\treturn (void *)vmalloc_meta(address, is_origin);\n\n\tret = arch_kmsan_get_meta_or_null(address, is_origin);\n\tif (ret)\n\t\treturn ret;\n\n\tpage = virt_to_page_or_null(address);\n\tif (!page)\n\t\treturn NULL;\n\tif (!page_has_metadata(page))\n\t\treturn NULL;\n\toff = offset_in_page(addr);\n\n\treturn (is_origin ? origin_ptr_for(page) : shadow_ptr_for(page)) + off;\n}\n\nvoid kmsan_copy_page_meta(struct page *dst, struct page *src)\n{\n\tif (!kmsan_enabled || kmsan_in_runtime())\n\t\treturn;\n\tif (!dst || !page_has_metadata(dst))\n\t\treturn;\n\tif (!src || !page_has_metadata(src)) {\n\t\tkmsan_internal_unpoison_memory(page_address(dst), PAGE_SIZE,\n\t\t\t\t\t         false);\n\t\treturn;\n\t}\n\n\tkmsan_enter_runtime();\n\t__memcpy(shadow_ptr_for(dst), shadow_ptr_for(src), PAGE_SIZE);\n\t__memcpy(origin_ptr_for(dst), origin_ptr_for(src), PAGE_SIZE);\n\tkmsan_leave_runtime();\n}\nEXPORT_SYMBOL(kmsan_copy_page_meta);\n\nvoid kmsan_alloc_page(struct page *page, unsigned int order, gfp_t flags)\n{\n\tbool initialized = (flags & __GFP_ZERO) || !kmsan_enabled;\n\tstruct page *shadow, *origin;\n\tdepot_stack_handle_t handle;\n\tint pages = 1 << order;\n\n\tif (!page)\n\t\treturn;\n\n\tshadow = shadow_page_for(page);\n\torigin = origin_page_for(page);\n\n\tif (initialized) {\n\t\t__memset(page_address(shadow), 0, PAGE_SIZE * pages);\n\t\t__memset(page_address(origin), 0, PAGE_SIZE * pages);\n\t\treturn;\n\t}\n\n\t \n\tif (kmsan_in_runtime())\n\t\treturn;\n\n\t__memset(page_address(shadow), -1, PAGE_SIZE * pages);\n\tkmsan_enter_runtime();\n\thandle = kmsan_save_stack_with_flags(flags,   0);\n\tkmsan_leave_runtime();\n\t \n\tfor (int i = 0; i < PAGE_SIZE * pages / sizeof(handle); i++)\n\t\t((depot_stack_handle_t *)page_address(origin))[i] = handle;\n}\n\nvoid kmsan_free_page(struct page *page, unsigned int order)\n{\n\tif (!kmsan_enabled || kmsan_in_runtime())\n\t\treturn;\n\tkmsan_enter_runtime();\n\tkmsan_internal_poison_memory(page_address(page),\n\t\t\t\t     page_size(page),\n\t\t\t\t     GFP_KERNEL,\n\t\t\t\t     KMSAN_POISON_CHECK | KMSAN_POISON_FREE);\n\tkmsan_leave_runtime();\n}\n\nint kmsan_vmap_pages_range_noflush(unsigned long start, unsigned long end,\n\t\t\t\t   pgprot_t prot, struct page **pages,\n\t\t\t\t   unsigned int page_shift)\n{\n\tunsigned long shadow_start, origin_start, shadow_end, origin_end;\n\tstruct page **s_pages, **o_pages;\n\tint nr, mapped, err = 0;\n\n\tif (!kmsan_enabled)\n\t\treturn 0;\n\n\tshadow_start = vmalloc_meta((void *)start, KMSAN_META_SHADOW);\n\tshadow_end = vmalloc_meta((void *)end, KMSAN_META_SHADOW);\n\tif (!shadow_start)\n\t\treturn 0;\n\n\tnr = (end - start) / PAGE_SIZE;\n\ts_pages = kcalloc(nr, sizeof(*s_pages), GFP_KERNEL);\n\to_pages = kcalloc(nr, sizeof(*o_pages), GFP_KERNEL);\n\tif (!s_pages || !o_pages) {\n\t\terr = -ENOMEM;\n\t\tgoto ret;\n\t}\n\tfor (int i = 0; i < nr; i++) {\n\t\ts_pages[i] = shadow_page_for(pages[i]);\n\t\to_pages[i] = origin_page_for(pages[i]);\n\t}\n\tprot = __pgprot(pgprot_val(prot) | _PAGE_NX);\n\tprot = PAGE_KERNEL;\n\n\torigin_start = vmalloc_meta((void *)start, KMSAN_META_ORIGIN);\n\torigin_end = vmalloc_meta((void *)end, KMSAN_META_ORIGIN);\n\tkmsan_enter_runtime();\n\tmapped = __vmap_pages_range_noflush(shadow_start, shadow_end, prot,\n\t\t\t\t\t    s_pages, page_shift);\n\tif (mapped) {\n\t\terr = mapped;\n\t\tgoto ret;\n\t}\n\tmapped = __vmap_pages_range_noflush(origin_start, origin_end, prot,\n\t\t\t\t\t    o_pages, page_shift);\n\tif (mapped) {\n\t\terr = mapped;\n\t\tgoto ret;\n\t}\n\tkmsan_leave_runtime();\n\tflush_tlb_kernel_range(shadow_start, shadow_end);\n\tflush_tlb_kernel_range(origin_start, origin_end);\n\tflush_cache_vmap(shadow_start, shadow_end);\n\tflush_cache_vmap(origin_start, origin_end);\n\nret:\n\tkfree(s_pages);\n\tkfree(o_pages);\n\treturn err;\n}\n\n \nvoid __init kmsan_init_alloc_meta_for_range(void *start, void *end)\n{\n\tstruct page *shadow_p, *origin_p;\n\tvoid *shadow, *origin;\n\tstruct page *page;\n\tu64 size;\n\n\tstart = (void *)PAGE_ALIGN_DOWN((u64)start);\n\tsize = PAGE_ALIGN((u64)end - (u64)start);\n\tshadow = memblock_alloc(size, PAGE_SIZE);\n\torigin = memblock_alloc(size, PAGE_SIZE);\n\tfor (u64 addr = 0; addr < size; addr += PAGE_SIZE) {\n\t\tpage = virt_to_page_or_null((char *)start + addr);\n\t\tshadow_p = virt_to_page_or_null((char *)shadow + addr);\n\t\tset_no_shadow_origin_page(shadow_p);\n\t\tshadow_page_for(page) = shadow_p;\n\t\torigin_p = virt_to_page_or_null((char *)origin + addr);\n\t\tset_no_shadow_origin_page(origin_p);\n\t\torigin_page_for(page) = origin_p;\n\t}\n}\n\nvoid kmsan_setup_meta(struct page *page, struct page *shadow,\n\t\t      struct page *origin, int order)\n{\n\tfor (int i = 0; i < (1 << order); i++) {\n\t\tset_no_shadow_origin_page(&shadow[i]);\n\t\tset_no_shadow_origin_page(&origin[i]);\n\t\tshadow_page_for(&page[i]) = &shadow[i];\n\t\torigin_page_for(&page[i]) = &origin[i];\n\t}\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}