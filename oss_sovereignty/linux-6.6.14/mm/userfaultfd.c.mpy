{
  "module_name": "userfaultfd.c",
  "hash_id": "523a1dcabb3e0707fff4a0fa21de19ee70f814a3397b6e6168643987b33561b7",
  "original_prompt": "Ingested from linux-6.6.14/mm/userfaultfd.c",
  "human_readable_source": "\n \n\n#include <linux/mm.h>\n#include <linux/sched/signal.h>\n#include <linux/pagemap.h>\n#include <linux/rmap.h>\n#include <linux/swap.h>\n#include <linux/swapops.h>\n#include <linux/userfaultfd_k.h>\n#include <linux/mmu_notifier.h>\n#include <linux/hugetlb.h>\n#include <linux/shmem_fs.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include \"internal.h\"\n\nstatic __always_inline\nstruct vm_area_struct *find_dst_vma(struct mm_struct *dst_mm,\n\t\t\t\t    unsigned long dst_start,\n\t\t\t\t    unsigned long len)\n{\n\t \n\tstruct vm_area_struct *dst_vma;\n\n\tdst_vma = find_vma(dst_mm, dst_start);\n\tif (!range_in_vma(dst_vma, dst_start, dst_start + len))\n\t\treturn NULL;\n\n\t \n\tif (!dst_vma->vm_userfaultfd_ctx.ctx)\n\t\treturn NULL;\n\n\treturn dst_vma;\n}\n\n \nstatic bool mfill_file_over_size(struct vm_area_struct *dst_vma,\n\t\t\t\t unsigned long dst_addr)\n{\n\tstruct inode *inode;\n\tpgoff_t offset, max_off;\n\n\tif (!dst_vma->vm_file)\n\t\treturn false;\n\n\tinode = dst_vma->vm_file->f_inode;\n\toffset = linear_page_index(dst_vma, dst_addr);\n\tmax_off = DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE);\n\treturn offset >= max_off;\n}\n\n \nint mfill_atomic_install_pte(pmd_t *dst_pmd,\n\t\t\t     struct vm_area_struct *dst_vma,\n\t\t\t     unsigned long dst_addr, struct page *page,\n\t\t\t     bool newly_allocated, uffd_flags_t flags)\n{\n\tint ret;\n\tstruct mm_struct *dst_mm = dst_vma->vm_mm;\n\tpte_t _dst_pte, *dst_pte;\n\tbool writable = dst_vma->vm_flags & VM_WRITE;\n\tbool vm_shared = dst_vma->vm_flags & VM_SHARED;\n\tbool page_in_cache = page_mapping(page);\n\tspinlock_t *ptl;\n\tstruct folio *folio;\n\n\t_dst_pte = mk_pte(page, dst_vma->vm_page_prot);\n\t_dst_pte = pte_mkdirty(_dst_pte);\n\tif (page_in_cache && !vm_shared)\n\t\twritable = false;\n\tif (writable)\n\t\t_dst_pte = pte_mkwrite(_dst_pte, dst_vma);\n\tif (flags & MFILL_ATOMIC_WP)\n\t\t_dst_pte = pte_mkuffd_wp(_dst_pte);\n\n\tret = -EAGAIN;\n\tdst_pte = pte_offset_map_lock(dst_mm, dst_pmd, dst_addr, &ptl);\n\tif (!dst_pte)\n\t\tgoto out;\n\n\tif (mfill_file_over_size(dst_vma, dst_addr)) {\n\t\tret = -EFAULT;\n\t\tgoto out_unlock;\n\t}\n\n\tret = -EEXIST;\n\t \n\tif (!pte_none_mostly(ptep_get(dst_pte)))\n\t\tgoto out_unlock;\n\n\tfolio = page_folio(page);\n\tif (page_in_cache) {\n\t\t \n\t\tif (newly_allocated)\n\t\t\tfolio_add_lru(folio);\n\t\tpage_add_file_rmap(page, dst_vma, false);\n\t} else {\n\t\tpage_add_new_anon_rmap(page, dst_vma, dst_addr);\n\t\tfolio_add_lru_vma(folio, dst_vma);\n\t}\n\n\t \n\tinc_mm_counter(dst_mm, mm_counter(page));\n\n\tset_pte_at(dst_mm, dst_addr, dst_pte, _dst_pte);\n\n\t \n\tupdate_mmu_cache(dst_vma, dst_addr, dst_pte);\n\tret = 0;\nout_unlock:\n\tpte_unmap_unlock(dst_pte, ptl);\nout:\n\treturn ret;\n}\n\nstatic int mfill_atomic_pte_copy(pmd_t *dst_pmd,\n\t\t\t\t struct vm_area_struct *dst_vma,\n\t\t\t\t unsigned long dst_addr,\n\t\t\t\t unsigned long src_addr,\n\t\t\t\t uffd_flags_t flags,\n\t\t\t\t struct folio **foliop)\n{\n\tvoid *kaddr;\n\tint ret;\n\tstruct folio *folio;\n\n\tif (!*foliop) {\n\t\tret = -ENOMEM;\n\t\tfolio = vma_alloc_folio(GFP_HIGHUSER_MOVABLE, 0, dst_vma,\n\t\t\t\t\tdst_addr, false);\n\t\tif (!folio)\n\t\t\tgoto out;\n\n\t\tkaddr = kmap_local_folio(folio, 0);\n\t\t \n\t\tpagefault_disable();\n\t\tret = copy_from_user(kaddr, (const void __user *) src_addr,\n\t\t\t\t     PAGE_SIZE);\n\t\tpagefault_enable();\n\t\tkunmap_local(kaddr);\n\n\t\t \n\t\tif (unlikely(ret)) {\n\t\t\tret = -ENOENT;\n\t\t\t*foliop = folio;\n\t\t\t \n\t\t\tgoto out;\n\t\t}\n\n\t\tflush_dcache_folio(folio);\n\t} else {\n\t\tfolio = *foliop;\n\t\t*foliop = NULL;\n\t}\n\n\t \n\t__folio_mark_uptodate(folio);\n\n\tret = -ENOMEM;\n\tif (mem_cgroup_charge(folio, dst_vma->vm_mm, GFP_KERNEL))\n\t\tgoto out_release;\n\n\tret = mfill_atomic_install_pte(dst_pmd, dst_vma, dst_addr,\n\t\t\t\t       &folio->page, true, flags);\n\tif (ret)\n\t\tgoto out_release;\nout:\n\treturn ret;\nout_release:\n\tfolio_put(folio);\n\tgoto out;\n}\n\nstatic int mfill_atomic_pte_zeropage(pmd_t *dst_pmd,\n\t\t\t\t     struct vm_area_struct *dst_vma,\n\t\t\t\t     unsigned long dst_addr)\n{\n\tpte_t _dst_pte, *dst_pte;\n\tspinlock_t *ptl;\n\tint ret;\n\n\t_dst_pte = pte_mkspecial(pfn_pte(my_zero_pfn(dst_addr),\n\t\t\t\t\t dst_vma->vm_page_prot));\n\tret = -EAGAIN;\n\tdst_pte = pte_offset_map_lock(dst_vma->vm_mm, dst_pmd, dst_addr, &ptl);\n\tif (!dst_pte)\n\t\tgoto out;\n\tif (mfill_file_over_size(dst_vma, dst_addr)) {\n\t\tret = -EFAULT;\n\t\tgoto out_unlock;\n\t}\n\tret = -EEXIST;\n\tif (!pte_none(ptep_get(dst_pte)))\n\t\tgoto out_unlock;\n\tset_pte_at(dst_vma->vm_mm, dst_addr, dst_pte, _dst_pte);\n\t \n\tupdate_mmu_cache(dst_vma, dst_addr, dst_pte);\n\tret = 0;\nout_unlock:\n\tpte_unmap_unlock(dst_pte, ptl);\nout:\n\treturn ret;\n}\n\n \nstatic int mfill_atomic_pte_continue(pmd_t *dst_pmd,\n\t\t\t\t     struct vm_area_struct *dst_vma,\n\t\t\t\t     unsigned long dst_addr,\n\t\t\t\t     uffd_flags_t flags)\n{\n\tstruct inode *inode = file_inode(dst_vma->vm_file);\n\tpgoff_t pgoff = linear_page_index(dst_vma, dst_addr);\n\tstruct folio *folio;\n\tstruct page *page;\n\tint ret;\n\n\tret = shmem_get_folio(inode, pgoff, &folio, SGP_NOALLOC);\n\t \n\tif (ret == -ENOENT)\n\t\tret = -EFAULT;\n\tif (ret)\n\t\tgoto out;\n\tif (!folio) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tpage = folio_file_page(folio, pgoff);\n\tif (PageHWPoison(page)) {\n\t\tret = -EIO;\n\t\tgoto out_release;\n\t}\n\n\tret = mfill_atomic_install_pte(dst_pmd, dst_vma, dst_addr,\n\t\t\t\t       page, false, flags);\n\tif (ret)\n\t\tgoto out_release;\n\n\tfolio_unlock(folio);\n\tret = 0;\nout:\n\treturn ret;\nout_release:\n\tfolio_unlock(folio);\n\tfolio_put(folio);\n\tgoto out;\n}\n\n \nstatic int mfill_atomic_pte_poison(pmd_t *dst_pmd,\n\t\t\t\t   struct vm_area_struct *dst_vma,\n\t\t\t\t   unsigned long dst_addr,\n\t\t\t\t   uffd_flags_t flags)\n{\n\tint ret;\n\tstruct mm_struct *dst_mm = dst_vma->vm_mm;\n\tpte_t _dst_pte, *dst_pte;\n\tspinlock_t *ptl;\n\n\t_dst_pte = make_pte_marker(PTE_MARKER_POISONED);\n\tret = -EAGAIN;\n\tdst_pte = pte_offset_map_lock(dst_mm, dst_pmd, dst_addr, &ptl);\n\tif (!dst_pte)\n\t\tgoto out;\n\n\tif (mfill_file_over_size(dst_vma, dst_addr)) {\n\t\tret = -EFAULT;\n\t\tgoto out_unlock;\n\t}\n\n\tret = -EEXIST;\n\t \n\tif (!pte_none(*dst_pte))\n\t\tgoto out_unlock;\n\n\tset_pte_at(dst_mm, dst_addr, dst_pte, _dst_pte);\n\n\t \n\tupdate_mmu_cache(dst_vma, dst_addr, dst_pte);\n\tret = 0;\nout_unlock:\n\tpte_unmap_unlock(dst_pte, ptl);\nout:\n\treturn ret;\n}\n\nstatic pmd_t *mm_alloc_pmd(struct mm_struct *mm, unsigned long address)\n{\n\tpgd_t *pgd;\n\tp4d_t *p4d;\n\tpud_t *pud;\n\n\tpgd = pgd_offset(mm, address);\n\tp4d = p4d_alloc(mm, pgd, address);\n\tif (!p4d)\n\t\treturn NULL;\n\tpud = pud_alloc(mm, p4d, address);\n\tif (!pud)\n\t\treturn NULL;\n\t \n\treturn pmd_alloc(mm, pud, address);\n}\n\n#ifdef CONFIG_HUGETLB_PAGE\n \nstatic __always_inline ssize_t mfill_atomic_hugetlb(\n\t\t\t\t\t      struct vm_area_struct *dst_vma,\n\t\t\t\t\t      unsigned long dst_start,\n\t\t\t\t\t      unsigned long src_start,\n\t\t\t\t\t      unsigned long len,\n\t\t\t\t\t      uffd_flags_t flags)\n{\n\tstruct mm_struct *dst_mm = dst_vma->vm_mm;\n\tint vm_shared = dst_vma->vm_flags & VM_SHARED;\n\tssize_t err;\n\tpte_t *dst_pte;\n\tunsigned long src_addr, dst_addr;\n\tlong copied;\n\tstruct folio *folio;\n\tunsigned long vma_hpagesize;\n\tpgoff_t idx;\n\tu32 hash;\n\tstruct address_space *mapping;\n\n\t \n\tif (uffd_flags_mode_is(flags, MFILL_ATOMIC_ZEROPAGE)) {\n\t\tmmap_read_unlock(dst_mm);\n\t\treturn -EINVAL;\n\t}\n\n\tsrc_addr = src_start;\n\tdst_addr = dst_start;\n\tcopied = 0;\n\tfolio = NULL;\n\tvma_hpagesize = vma_kernel_pagesize(dst_vma);\n\n\t \n\terr = -EINVAL;\n\tif (dst_start & (vma_hpagesize - 1) || len & (vma_hpagesize - 1))\n\t\tgoto out_unlock;\n\nretry:\n\t \n\tif (!dst_vma) {\n\t\terr = -ENOENT;\n\t\tdst_vma = find_dst_vma(dst_mm, dst_start, len);\n\t\tif (!dst_vma || !is_vm_hugetlb_page(dst_vma))\n\t\t\tgoto out_unlock;\n\n\t\terr = -EINVAL;\n\t\tif (vma_hpagesize != vma_kernel_pagesize(dst_vma))\n\t\t\tgoto out_unlock;\n\n\t\tvm_shared = dst_vma->vm_flags & VM_SHARED;\n\t}\n\n\t \n\terr = -ENOMEM;\n\tif (!vm_shared) {\n\t\tif (unlikely(anon_vma_prepare(dst_vma)))\n\t\t\tgoto out_unlock;\n\t}\n\n\twhile (src_addr < src_start + len) {\n\t\tBUG_ON(dst_addr >= dst_start + len);\n\n\t\t \n\t\tidx = linear_page_index(dst_vma, dst_addr);\n\t\tmapping = dst_vma->vm_file->f_mapping;\n\t\thash = hugetlb_fault_mutex_hash(mapping, idx);\n\t\tmutex_lock(&hugetlb_fault_mutex_table[hash]);\n\t\thugetlb_vma_lock_read(dst_vma);\n\n\t\terr = -ENOMEM;\n\t\tdst_pte = huge_pte_alloc(dst_mm, dst_vma, dst_addr, vma_hpagesize);\n\t\tif (!dst_pte) {\n\t\t\thugetlb_vma_unlock_read(dst_vma);\n\t\t\tmutex_unlock(&hugetlb_fault_mutex_table[hash]);\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tif (!uffd_flags_mode_is(flags, MFILL_ATOMIC_CONTINUE) &&\n\t\t    !huge_pte_none_mostly(huge_ptep_get(dst_pte))) {\n\t\t\terr = -EEXIST;\n\t\t\thugetlb_vma_unlock_read(dst_vma);\n\t\t\tmutex_unlock(&hugetlb_fault_mutex_table[hash]);\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\terr = hugetlb_mfill_atomic_pte(dst_pte, dst_vma, dst_addr,\n\t\t\t\t\t       src_addr, flags, &folio);\n\n\t\thugetlb_vma_unlock_read(dst_vma);\n\t\tmutex_unlock(&hugetlb_fault_mutex_table[hash]);\n\n\t\tcond_resched();\n\n\t\tif (unlikely(err == -ENOENT)) {\n\t\t\tmmap_read_unlock(dst_mm);\n\t\t\tBUG_ON(!folio);\n\n\t\t\terr = copy_folio_from_user(folio,\n\t\t\t\t\t\t   (const void __user *)src_addr, true);\n\t\t\tif (unlikely(err)) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tmmap_read_lock(dst_mm);\n\n\t\t\tdst_vma = NULL;\n\t\t\tgoto retry;\n\t\t} else\n\t\t\tBUG_ON(folio);\n\n\t\tif (!err) {\n\t\t\tdst_addr += vma_hpagesize;\n\t\t\tsrc_addr += vma_hpagesize;\n\t\t\tcopied += vma_hpagesize;\n\n\t\t\tif (fatal_signal_pending(current))\n\t\t\t\terr = -EINTR;\n\t\t}\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\nout_unlock:\n\tmmap_read_unlock(dst_mm);\nout:\n\tif (folio)\n\t\tfolio_put(folio);\n\tBUG_ON(copied < 0);\n\tBUG_ON(err > 0);\n\tBUG_ON(!copied && !err);\n\treturn copied ? copied : err;\n}\n#else  \n \nextern ssize_t mfill_atomic_hugetlb(struct vm_area_struct *dst_vma,\n\t\t\t\t    unsigned long dst_start,\n\t\t\t\t    unsigned long src_start,\n\t\t\t\t    unsigned long len,\n\t\t\t\t    uffd_flags_t flags);\n#endif  \n\nstatic __always_inline ssize_t mfill_atomic_pte(pmd_t *dst_pmd,\n\t\t\t\t\t\tstruct vm_area_struct *dst_vma,\n\t\t\t\t\t\tunsigned long dst_addr,\n\t\t\t\t\t\tunsigned long src_addr,\n\t\t\t\t\t\tuffd_flags_t flags,\n\t\t\t\t\t\tstruct folio **foliop)\n{\n\tssize_t err;\n\n\tif (uffd_flags_mode_is(flags, MFILL_ATOMIC_CONTINUE)) {\n\t\treturn mfill_atomic_pte_continue(dst_pmd, dst_vma,\n\t\t\t\t\t\t dst_addr, flags);\n\t} else if (uffd_flags_mode_is(flags, MFILL_ATOMIC_POISON)) {\n\t\treturn mfill_atomic_pte_poison(dst_pmd, dst_vma,\n\t\t\t\t\t       dst_addr, flags);\n\t}\n\n\t \n\tif (!(dst_vma->vm_flags & VM_SHARED)) {\n\t\tif (uffd_flags_mode_is(flags, MFILL_ATOMIC_COPY))\n\t\t\terr = mfill_atomic_pte_copy(dst_pmd, dst_vma,\n\t\t\t\t\t\t    dst_addr, src_addr,\n\t\t\t\t\t\t    flags, foliop);\n\t\telse\n\t\t\terr = mfill_atomic_pte_zeropage(dst_pmd,\n\t\t\t\t\t\t dst_vma, dst_addr);\n\t} else {\n\t\terr = shmem_mfill_atomic_pte(dst_pmd, dst_vma,\n\t\t\t\t\t     dst_addr, src_addr,\n\t\t\t\t\t     flags, foliop);\n\t}\n\n\treturn err;\n}\n\nstatic __always_inline ssize_t mfill_atomic(struct mm_struct *dst_mm,\n\t\t\t\t\t    unsigned long dst_start,\n\t\t\t\t\t    unsigned long src_start,\n\t\t\t\t\t    unsigned long len,\n\t\t\t\t\t    atomic_t *mmap_changing,\n\t\t\t\t\t    uffd_flags_t flags)\n{\n\tstruct vm_area_struct *dst_vma;\n\tssize_t err;\n\tpmd_t *dst_pmd;\n\tunsigned long src_addr, dst_addr;\n\tlong copied;\n\tstruct folio *folio;\n\n\t \n\tBUG_ON(dst_start & ~PAGE_MASK);\n\tBUG_ON(len & ~PAGE_MASK);\n\n\t \n\tBUG_ON(src_start + len <= src_start);\n\tBUG_ON(dst_start + len <= dst_start);\n\n\tsrc_addr = src_start;\n\tdst_addr = dst_start;\n\tcopied = 0;\n\tfolio = NULL;\nretry:\n\tmmap_read_lock(dst_mm);\n\n\t \n\terr = -EAGAIN;\n\tif (mmap_changing && atomic_read(mmap_changing))\n\t\tgoto out_unlock;\n\n\t \n\terr = -ENOENT;\n\tdst_vma = find_dst_vma(dst_mm, dst_start, len);\n\tif (!dst_vma)\n\t\tgoto out_unlock;\n\n\terr = -EINVAL;\n\t \n\tif (WARN_ON_ONCE(vma_is_anonymous(dst_vma) &&\n\t    dst_vma->vm_flags & VM_SHARED))\n\t\tgoto out_unlock;\n\n\t \n\tif ((flags & MFILL_ATOMIC_WP) && !(dst_vma->vm_flags & VM_UFFD_WP))\n\t\tgoto out_unlock;\n\n\t \n\tif (is_vm_hugetlb_page(dst_vma))\n\t\treturn  mfill_atomic_hugetlb(dst_vma, dst_start,\n\t\t\t\t\t     src_start, len, flags);\n\n\tif (!vma_is_anonymous(dst_vma) && !vma_is_shmem(dst_vma))\n\t\tgoto out_unlock;\n\tif (!vma_is_shmem(dst_vma) &&\n\t    uffd_flags_mode_is(flags, MFILL_ATOMIC_CONTINUE))\n\t\tgoto out_unlock;\n\n\t \n\terr = -ENOMEM;\n\tif (!(dst_vma->vm_flags & VM_SHARED) &&\n\t    unlikely(anon_vma_prepare(dst_vma)))\n\t\tgoto out_unlock;\n\n\twhile (src_addr < src_start + len) {\n\t\tpmd_t dst_pmdval;\n\n\t\tBUG_ON(dst_addr >= dst_start + len);\n\n\t\tdst_pmd = mm_alloc_pmd(dst_mm, dst_addr);\n\t\tif (unlikely(!dst_pmd)) {\n\t\t\terr = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\n\t\tdst_pmdval = pmdp_get_lockless(dst_pmd);\n\t\t \n\t\tif (unlikely(pmd_trans_huge(dst_pmdval))) {\n\t\t\terr = -EEXIST;\n\t\t\tbreak;\n\t\t}\n\t\tif (unlikely(pmd_none(dst_pmdval)) &&\n\t\t    unlikely(__pte_alloc(dst_mm, dst_pmd))) {\n\t\t\terr = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\t\t \n\t\tif (unlikely(pmd_trans_huge(*dst_pmd))) {\n\t\t\terr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tBUG_ON(pmd_none(*dst_pmd));\n\t\tBUG_ON(pmd_trans_huge(*dst_pmd));\n\n\t\terr = mfill_atomic_pte(dst_pmd, dst_vma, dst_addr,\n\t\t\t\t       src_addr, flags, &folio);\n\t\tcond_resched();\n\n\t\tif (unlikely(err == -ENOENT)) {\n\t\t\tvoid *kaddr;\n\n\t\t\tmmap_read_unlock(dst_mm);\n\t\t\tBUG_ON(!folio);\n\n\t\t\tkaddr = kmap_local_folio(folio, 0);\n\t\t\terr = copy_from_user(kaddr,\n\t\t\t\t\t     (const void __user *) src_addr,\n\t\t\t\t\t     PAGE_SIZE);\n\t\t\tkunmap_local(kaddr);\n\t\t\tif (unlikely(err)) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tflush_dcache_folio(folio);\n\t\t\tgoto retry;\n\t\t} else\n\t\t\tBUG_ON(folio);\n\n\t\tif (!err) {\n\t\t\tdst_addr += PAGE_SIZE;\n\t\t\tsrc_addr += PAGE_SIZE;\n\t\t\tcopied += PAGE_SIZE;\n\n\t\t\tif (fatal_signal_pending(current))\n\t\t\t\terr = -EINTR;\n\t\t}\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\nout_unlock:\n\tmmap_read_unlock(dst_mm);\nout:\n\tif (folio)\n\t\tfolio_put(folio);\n\tBUG_ON(copied < 0);\n\tBUG_ON(err > 0);\n\tBUG_ON(!copied && !err);\n\treturn copied ? copied : err;\n}\n\nssize_t mfill_atomic_copy(struct mm_struct *dst_mm, unsigned long dst_start,\n\t\t\t  unsigned long src_start, unsigned long len,\n\t\t\t  atomic_t *mmap_changing, uffd_flags_t flags)\n{\n\treturn mfill_atomic(dst_mm, dst_start, src_start, len, mmap_changing,\n\t\t\t    uffd_flags_set_mode(flags, MFILL_ATOMIC_COPY));\n}\n\nssize_t mfill_atomic_zeropage(struct mm_struct *dst_mm, unsigned long start,\n\t\t\t      unsigned long len, atomic_t *mmap_changing)\n{\n\treturn mfill_atomic(dst_mm, start, 0, len, mmap_changing,\n\t\t\t    uffd_flags_set_mode(0, MFILL_ATOMIC_ZEROPAGE));\n}\n\nssize_t mfill_atomic_continue(struct mm_struct *dst_mm, unsigned long start,\n\t\t\t      unsigned long len, atomic_t *mmap_changing,\n\t\t\t      uffd_flags_t flags)\n{\n\treturn mfill_atomic(dst_mm, start, 0, len, mmap_changing,\n\t\t\t    uffd_flags_set_mode(flags, MFILL_ATOMIC_CONTINUE));\n}\n\nssize_t mfill_atomic_poison(struct mm_struct *dst_mm, unsigned long start,\n\t\t\t    unsigned long len, atomic_t *mmap_changing,\n\t\t\t    uffd_flags_t flags)\n{\n\treturn mfill_atomic(dst_mm, start, 0, len, mmap_changing,\n\t\t\t    uffd_flags_set_mode(flags, MFILL_ATOMIC_POISON));\n}\n\nlong uffd_wp_range(struct vm_area_struct *dst_vma,\n\t\t   unsigned long start, unsigned long len, bool enable_wp)\n{\n\tunsigned int mm_cp_flags;\n\tstruct mmu_gather tlb;\n\tlong ret;\n\n\tVM_WARN_ONCE(start < dst_vma->vm_start || start + len > dst_vma->vm_end,\n\t\t\t\"The address range exceeds VMA boundary.\\n\");\n\tif (enable_wp)\n\t\tmm_cp_flags = MM_CP_UFFD_WP;\n\telse\n\t\tmm_cp_flags = MM_CP_UFFD_WP_RESOLVE;\n\n\t \n\tif (!enable_wp && vma_wants_manual_pte_write_upgrade(dst_vma))\n\t\tmm_cp_flags |= MM_CP_TRY_CHANGE_WRITABLE;\n\ttlb_gather_mmu(&tlb, dst_vma->vm_mm);\n\tret = change_protection(&tlb, dst_vma, start, start + len, mm_cp_flags);\n\ttlb_finish_mmu(&tlb);\n\n\treturn ret;\n}\n\nint mwriteprotect_range(struct mm_struct *dst_mm, unsigned long start,\n\t\t\tunsigned long len, bool enable_wp,\n\t\t\tatomic_t *mmap_changing)\n{\n\tunsigned long end = start + len;\n\tunsigned long _start, _end;\n\tstruct vm_area_struct *dst_vma;\n\tunsigned long page_mask;\n\tlong err;\n\tVMA_ITERATOR(vmi, dst_mm, start);\n\n\t \n\tBUG_ON(start & ~PAGE_MASK);\n\tBUG_ON(len & ~PAGE_MASK);\n\n\t \n\tBUG_ON(start + len <= start);\n\n\tmmap_read_lock(dst_mm);\n\n\t \n\terr = -EAGAIN;\n\tif (mmap_changing && atomic_read(mmap_changing))\n\t\tgoto out_unlock;\n\n\terr = -ENOENT;\n\tfor_each_vma_range(vmi, dst_vma, end) {\n\n\t\tif (!userfaultfd_wp(dst_vma)) {\n\t\t\terr = -ENOENT;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (is_vm_hugetlb_page(dst_vma)) {\n\t\t\terr = -EINVAL;\n\t\t\tpage_mask = vma_kernel_pagesize(dst_vma) - 1;\n\t\t\tif ((start & page_mask) || (len & page_mask))\n\t\t\t\tbreak;\n\t\t}\n\n\t\t_start = max(dst_vma->vm_start, start);\n\t\t_end = min(dst_vma->vm_end, end);\n\n\t\terr = uffd_wp_range(dst_vma, _start, _end - _start, enable_wp);\n\n\t\t \n\t\tif (err < 0)\n\t\t\tbreak;\n\t\terr = 0;\n\t}\nout_unlock:\n\tmmap_read_unlock(dst_mm);\n\treturn err;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}