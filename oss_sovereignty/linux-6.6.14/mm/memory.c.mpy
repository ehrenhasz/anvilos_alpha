{
  "module_name": "memory.c",
  "hash_id": "f75a0716a66b64a60a6391703062707fe38b4af329561626491477af6484074a",
  "original_prompt": "Ingested from linux-6.6.14/mm/memory.c",
  "human_readable_source": "\n \n\n \n\n \n\n \n\n \n\n#include <linux/kernel_stat.h>\n#include <linux/mm.h>\n#include <linux/mm_inline.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/task.h>\n#include <linux/hugetlb.h>\n#include <linux/mman.h>\n#include <linux/swap.h>\n#include <linux/highmem.h>\n#include <linux/pagemap.h>\n#include <linux/memremap.h>\n#include <linux/kmsan.h>\n#include <linux/ksm.h>\n#include <linux/rmap.h>\n#include <linux/export.h>\n#include <linux/delayacct.h>\n#include <linux/init.h>\n#include <linux/pfn_t.h>\n#include <linux/writeback.h>\n#include <linux/memcontrol.h>\n#include <linux/mmu_notifier.h>\n#include <linux/swapops.h>\n#include <linux/elf.h>\n#include <linux/gfp.h>\n#include <linux/migrate.h>\n#include <linux/string.h>\n#include <linux/memory-tiers.h>\n#include <linux/debugfs.h>\n#include <linux/userfaultfd_k.h>\n#include <linux/dax.h>\n#include <linux/oom.h>\n#include <linux/numa.h>\n#include <linux/perf_event.h>\n#include <linux/ptrace.h>\n#include <linux/vmalloc.h>\n#include <linux/sched/sysctl.h>\n\n#include <trace/events/kmem.h>\n\n#include <asm/io.h>\n#include <asm/mmu_context.h>\n#include <asm/pgalloc.h>\n#include <linux/uaccess.h>\n#include <asm/tlb.h>\n#include <asm/tlbflush.h>\n\n#include \"pgalloc-track.h\"\n#include \"internal.h\"\n#include \"swap.h\"\n\n#if defined(LAST_CPUPID_NOT_IN_PAGE_FLAGS) && !defined(CONFIG_COMPILE_TEST)\n#warning Unfortunate NUMA and NUMA Balancing config, growing page-frame for last_cpupid.\n#endif\n\n#ifndef CONFIG_NUMA\nunsigned long max_mapnr;\nEXPORT_SYMBOL(max_mapnr);\n\nstruct page *mem_map;\nEXPORT_SYMBOL(mem_map);\n#endif\n\nstatic vm_fault_t do_fault(struct vm_fault *vmf);\nstatic vm_fault_t do_anonymous_page(struct vm_fault *vmf);\nstatic bool vmf_pte_changed(struct vm_fault *vmf);\n\n \nstatic bool vmf_orig_pte_uffd_wp(struct vm_fault *vmf)\n{\n\tif (!(vmf->flags & FAULT_FLAG_ORIG_PTE_VALID))\n\t\treturn false;\n\n\treturn pte_marker_uffd_wp(vmf->orig_pte);\n}\n\n \nvoid *high_memory;\nEXPORT_SYMBOL(high_memory);\n\n \nint randomize_va_space __read_mostly =\n#ifdef CONFIG_COMPAT_BRK\n\t\t\t\t\t1;\n#else\n\t\t\t\t\t2;\n#endif\n\n#ifndef arch_wants_old_prefaulted_pte\nstatic inline bool arch_wants_old_prefaulted_pte(void)\n{\n\t \n\treturn false;\n}\n#endif\n\nstatic int __init disable_randmaps(char *s)\n{\n\trandomize_va_space = 0;\n\treturn 1;\n}\n__setup(\"norandmaps\", disable_randmaps);\n\nunsigned long zero_pfn __read_mostly;\nEXPORT_SYMBOL(zero_pfn);\n\nunsigned long highest_memmap_pfn __read_mostly;\n\n \nstatic int __init init_zero_pfn(void)\n{\n\tzero_pfn = page_to_pfn(ZERO_PAGE(0));\n\treturn 0;\n}\nearly_initcall(init_zero_pfn);\n\nvoid mm_trace_rss_stat(struct mm_struct *mm, int member)\n{\n\ttrace_rss_stat(mm, member);\n}\n\n \nstatic void free_pte_range(struct mmu_gather *tlb, pmd_t *pmd,\n\t\t\t   unsigned long addr)\n{\n\tpgtable_t token = pmd_pgtable(*pmd);\n\tpmd_clear(pmd);\n\tpte_free_tlb(tlb, token, addr);\n\tmm_dec_nr_ptes(tlb->mm);\n}\n\nstatic inline void free_pmd_range(struct mmu_gather *tlb, pud_t *pud,\n\t\t\t\tunsigned long addr, unsigned long end,\n\t\t\t\tunsigned long floor, unsigned long ceiling)\n{\n\tpmd_t *pmd;\n\tunsigned long next;\n\tunsigned long start;\n\n\tstart = addr;\n\tpmd = pmd_offset(pud, addr);\n\tdo {\n\t\tnext = pmd_addr_end(addr, end);\n\t\tif (pmd_none_or_clear_bad(pmd))\n\t\t\tcontinue;\n\t\tfree_pte_range(tlb, pmd, addr);\n\t} while (pmd++, addr = next, addr != end);\n\n\tstart &= PUD_MASK;\n\tif (start < floor)\n\t\treturn;\n\tif (ceiling) {\n\t\tceiling &= PUD_MASK;\n\t\tif (!ceiling)\n\t\t\treturn;\n\t}\n\tif (end - 1 > ceiling - 1)\n\t\treturn;\n\n\tpmd = pmd_offset(pud, start);\n\tpud_clear(pud);\n\tpmd_free_tlb(tlb, pmd, start);\n\tmm_dec_nr_pmds(tlb->mm);\n}\n\nstatic inline void free_pud_range(struct mmu_gather *tlb, p4d_t *p4d,\n\t\t\t\tunsigned long addr, unsigned long end,\n\t\t\t\tunsigned long floor, unsigned long ceiling)\n{\n\tpud_t *pud;\n\tunsigned long next;\n\tunsigned long start;\n\n\tstart = addr;\n\tpud = pud_offset(p4d, addr);\n\tdo {\n\t\tnext = pud_addr_end(addr, end);\n\t\tif (pud_none_or_clear_bad(pud))\n\t\t\tcontinue;\n\t\tfree_pmd_range(tlb, pud, addr, next, floor, ceiling);\n\t} while (pud++, addr = next, addr != end);\n\n\tstart &= P4D_MASK;\n\tif (start < floor)\n\t\treturn;\n\tif (ceiling) {\n\t\tceiling &= P4D_MASK;\n\t\tif (!ceiling)\n\t\t\treturn;\n\t}\n\tif (end - 1 > ceiling - 1)\n\t\treturn;\n\n\tpud = pud_offset(p4d, start);\n\tp4d_clear(p4d);\n\tpud_free_tlb(tlb, pud, start);\n\tmm_dec_nr_puds(tlb->mm);\n}\n\nstatic inline void free_p4d_range(struct mmu_gather *tlb, pgd_t *pgd,\n\t\t\t\tunsigned long addr, unsigned long end,\n\t\t\t\tunsigned long floor, unsigned long ceiling)\n{\n\tp4d_t *p4d;\n\tunsigned long next;\n\tunsigned long start;\n\n\tstart = addr;\n\tp4d = p4d_offset(pgd, addr);\n\tdo {\n\t\tnext = p4d_addr_end(addr, end);\n\t\tif (p4d_none_or_clear_bad(p4d))\n\t\t\tcontinue;\n\t\tfree_pud_range(tlb, p4d, addr, next, floor, ceiling);\n\t} while (p4d++, addr = next, addr != end);\n\n\tstart &= PGDIR_MASK;\n\tif (start < floor)\n\t\treturn;\n\tif (ceiling) {\n\t\tceiling &= PGDIR_MASK;\n\t\tif (!ceiling)\n\t\t\treturn;\n\t}\n\tif (end - 1 > ceiling - 1)\n\t\treturn;\n\n\tp4d = p4d_offset(pgd, start);\n\tpgd_clear(pgd);\n\tp4d_free_tlb(tlb, p4d, start);\n}\n\n \nvoid free_pgd_range(struct mmu_gather *tlb,\n\t\t\tunsigned long addr, unsigned long end,\n\t\t\tunsigned long floor, unsigned long ceiling)\n{\n\tpgd_t *pgd;\n\tunsigned long next;\n\n\t \n\n\taddr &= PMD_MASK;\n\tif (addr < floor) {\n\t\taddr += PMD_SIZE;\n\t\tif (!addr)\n\t\t\treturn;\n\t}\n\tif (ceiling) {\n\t\tceiling &= PMD_MASK;\n\t\tif (!ceiling)\n\t\t\treturn;\n\t}\n\tif (end - 1 > ceiling - 1)\n\t\tend -= PMD_SIZE;\n\tif (addr > end - 1)\n\t\treturn;\n\t \n\ttlb_change_page_size(tlb, PAGE_SIZE);\n\tpgd = pgd_offset(tlb->mm, addr);\n\tdo {\n\t\tnext = pgd_addr_end(addr, end);\n\t\tif (pgd_none_or_clear_bad(pgd))\n\t\t\tcontinue;\n\t\tfree_p4d_range(tlb, pgd, addr, next, floor, ceiling);\n\t} while (pgd++, addr = next, addr != end);\n}\n\nvoid free_pgtables(struct mmu_gather *tlb, struct ma_state *mas,\n\t\t   struct vm_area_struct *vma, unsigned long floor,\n\t\t   unsigned long ceiling, bool mm_wr_locked)\n{\n\tdo {\n\t\tunsigned long addr = vma->vm_start;\n\t\tstruct vm_area_struct *next;\n\n\t\t \n\t\tnext = mas_find(mas, ceiling - 1);\n\n\t\t \n\t\tif (mm_wr_locked)\n\t\t\tvma_start_write(vma);\n\t\tunlink_anon_vmas(vma);\n\t\tunlink_file_vma(vma);\n\n\t\tif (is_vm_hugetlb_page(vma)) {\n\t\t\thugetlb_free_pgd_range(tlb, addr, vma->vm_end,\n\t\t\t\tfloor, next ? next->vm_start : ceiling);\n\t\t} else {\n\t\t\t \n\t\t\twhile (next && next->vm_start <= vma->vm_end + PMD_SIZE\n\t\t\t       && !is_vm_hugetlb_page(next)) {\n\t\t\t\tvma = next;\n\t\t\t\tnext = mas_find(mas, ceiling - 1);\n\t\t\t\tif (mm_wr_locked)\n\t\t\t\t\tvma_start_write(vma);\n\t\t\t\tunlink_anon_vmas(vma);\n\t\t\t\tunlink_file_vma(vma);\n\t\t\t}\n\t\t\tfree_pgd_range(tlb, addr, vma->vm_end,\n\t\t\t\tfloor, next ? next->vm_start : ceiling);\n\t\t}\n\t\tvma = next;\n\t} while (vma);\n}\n\nvoid pmd_install(struct mm_struct *mm, pmd_t *pmd, pgtable_t *pte)\n{\n\tspinlock_t *ptl = pmd_lock(mm, pmd);\n\n\tif (likely(pmd_none(*pmd))) {\t \n\t\tmm_inc_nr_ptes(mm);\n\t\t \n\t\tsmp_wmb();  \n\t\tpmd_populate(mm, pmd, *pte);\n\t\t*pte = NULL;\n\t}\n\tspin_unlock(ptl);\n}\n\nint __pte_alloc(struct mm_struct *mm, pmd_t *pmd)\n{\n\tpgtable_t new = pte_alloc_one(mm);\n\tif (!new)\n\t\treturn -ENOMEM;\n\n\tpmd_install(mm, pmd, &new);\n\tif (new)\n\t\tpte_free(mm, new);\n\treturn 0;\n}\n\nint __pte_alloc_kernel(pmd_t *pmd)\n{\n\tpte_t *new = pte_alloc_one_kernel(&init_mm);\n\tif (!new)\n\t\treturn -ENOMEM;\n\n\tspin_lock(&init_mm.page_table_lock);\n\tif (likely(pmd_none(*pmd))) {\t \n\t\tsmp_wmb();  \n\t\tpmd_populate_kernel(&init_mm, pmd, new);\n\t\tnew = NULL;\n\t}\n\tspin_unlock(&init_mm.page_table_lock);\n\tif (new)\n\t\tpte_free_kernel(&init_mm, new);\n\treturn 0;\n}\n\nstatic inline void init_rss_vec(int *rss)\n{\n\tmemset(rss, 0, sizeof(int) * NR_MM_COUNTERS);\n}\n\nstatic inline void add_mm_rss_vec(struct mm_struct *mm, int *rss)\n{\n\tint i;\n\n\tif (current->mm == mm)\n\t\tsync_mm_rss(mm);\n\tfor (i = 0; i < NR_MM_COUNTERS; i++)\n\t\tif (rss[i])\n\t\t\tadd_mm_counter(mm, i, rss[i]);\n}\n\n \nstatic void print_bad_pte(struct vm_area_struct *vma, unsigned long addr,\n\t\t\t  pte_t pte, struct page *page)\n{\n\tpgd_t *pgd = pgd_offset(vma->vm_mm, addr);\n\tp4d_t *p4d = p4d_offset(pgd, addr);\n\tpud_t *pud = pud_offset(p4d, addr);\n\tpmd_t *pmd = pmd_offset(pud, addr);\n\tstruct address_space *mapping;\n\tpgoff_t index;\n\tstatic unsigned long resume;\n\tstatic unsigned long nr_shown;\n\tstatic unsigned long nr_unshown;\n\n\t \n\tif (nr_shown == 60) {\n\t\tif (time_before(jiffies, resume)) {\n\t\t\tnr_unshown++;\n\t\t\treturn;\n\t\t}\n\t\tif (nr_unshown) {\n\t\t\tpr_alert(\"BUG: Bad page map: %lu messages suppressed\\n\",\n\t\t\t\t nr_unshown);\n\t\t\tnr_unshown = 0;\n\t\t}\n\t\tnr_shown = 0;\n\t}\n\tif (nr_shown++ == 0)\n\t\tresume = jiffies + 60 * HZ;\n\n\tmapping = vma->vm_file ? vma->vm_file->f_mapping : NULL;\n\tindex = linear_page_index(vma, addr);\n\n\tpr_alert(\"BUG: Bad page map in process %s  pte:%08llx pmd:%08llx\\n\",\n\t\t current->comm,\n\t\t (long long)pte_val(pte), (long long)pmd_val(*pmd));\n\tif (page)\n\t\tdump_page(page, \"bad pte\");\n\tpr_alert(\"addr:%px vm_flags:%08lx anon_vma:%px mapping:%px index:%lx\\n\",\n\t\t (void *)addr, vma->vm_flags, vma->anon_vma, mapping, index);\n\tpr_alert(\"file:%pD fault:%ps mmap:%ps read_folio:%ps\\n\",\n\t\t vma->vm_file,\n\t\t vma->vm_ops ? vma->vm_ops->fault : NULL,\n\t\t vma->vm_file ? vma->vm_file->f_op->mmap : NULL,\n\t\t mapping ? mapping->a_ops->read_folio : NULL);\n\tdump_stack();\n\tadd_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);\n}\n\n \nstruct page *vm_normal_page(struct vm_area_struct *vma, unsigned long addr,\n\t\t\t    pte_t pte)\n{\n\tunsigned long pfn = pte_pfn(pte);\n\n\tif (IS_ENABLED(CONFIG_ARCH_HAS_PTE_SPECIAL)) {\n\t\tif (likely(!pte_special(pte)))\n\t\t\tgoto check_pfn;\n\t\tif (vma->vm_ops && vma->vm_ops->find_special_page)\n\t\t\treturn vma->vm_ops->find_special_page(vma, addr);\n\t\tif (vma->vm_flags & (VM_PFNMAP | VM_MIXEDMAP))\n\t\t\treturn NULL;\n\t\tif (is_zero_pfn(pfn))\n\t\t\treturn NULL;\n\t\tif (pte_devmap(pte))\n\t\t \n\t\t\treturn NULL;\n\n\t\tprint_bad_pte(vma, addr, pte, NULL);\n\t\treturn NULL;\n\t}\n\n\t \n\n\tif (unlikely(vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP))) {\n\t\tif (vma->vm_flags & VM_MIXEDMAP) {\n\t\t\tif (!pfn_valid(pfn))\n\t\t\t\treturn NULL;\n\t\t\tgoto out;\n\t\t} else {\n\t\t\tunsigned long off;\n\t\t\toff = (addr - vma->vm_start) >> PAGE_SHIFT;\n\t\t\tif (pfn == vma->vm_pgoff + off)\n\t\t\t\treturn NULL;\n\t\t\tif (!is_cow_mapping(vma->vm_flags))\n\t\t\t\treturn NULL;\n\t\t}\n\t}\n\n\tif (is_zero_pfn(pfn))\n\t\treturn NULL;\n\ncheck_pfn:\n\tif (unlikely(pfn > highest_memmap_pfn)) {\n\t\tprint_bad_pte(vma, addr, pte, NULL);\n\t\treturn NULL;\n\t}\n\n\t \nout:\n\treturn pfn_to_page(pfn);\n}\n\nstruct folio *vm_normal_folio(struct vm_area_struct *vma, unsigned long addr,\n\t\t\t    pte_t pte)\n{\n\tstruct page *page = vm_normal_page(vma, addr, pte);\n\n\tif (page)\n\t\treturn page_folio(page);\n\treturn NULL;\n}\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\nstruct page *vm_normal_page_pmd(struct vm_area_struct *vma, unsigned long addr,\n\t\t\t\tpmd_t pmd)\n{\n\tunsigned long pfn = pmd_pfn(pmd);\n\n\t \n\tif (unlikely(vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP))) {\n\t\tif (vma->vm_flags & VM_MIXEDMAP) {\n\t\t\tif (!pfn_valid(pfn))\n\t\t\t\treturn NULL;\n\t\t\tgoto out;\n\t\t} else {\n\t\t\tunsigned long off;\n\t\t\toff = (addr - vma->vm_start) >> PAGE_SHIFT;\n\t\t\tif (pfn == vma->vm_pgoff + off)\n\t\t\t\treturn NULL;\n\t\t\tif (!is_cow_mapping(vma->vm_flags))\n\t\t\t\treturn NULL;\n\t\t}\n\t}\n\n\tif (pmd_devmap(pmd))\n\t\treturn NULL;\n\tif (is_huge_zero_pmd(pmd))\n\t\treturn NULL;\n\tif (unlikely(pfn > highest_memmap_pfn))\n\t\treturn NULL;\n\n\t \nout:\n\treturn pfn_to_page(pfn);\n}\n#endif\n\nstatic void restore_exclusive_pte(struct vm_area_struct *vma,\n\t\t\t\t  struct page *page, unsigned long address,\n\t\t\t\t  pte_t *ptep)\n{\n\tpte_t orig_pte;\n\tpte_t pte;\n\tswp_entry_t entry;\n\n\torig_pte = ptep_get(ptep);\n\tpte = pte_mkold(mk_pte(page, READ_ONCE(vma->vm_page_prot)));\n\tif (pte_swp_soft_dirty(orig_pte))\n\t\tpte = pte_mksoft_dirty(pte);\n\n\tentry = pte_to_swp_entry(orig_pte);\n\tif (pte_swp_uffd_wp(orig_pte))\n\t\tpte = pte_mkuffd_wp(pte);\n\telse if (is_writable_device_exclusive_entry(entry))\n\t\tpte = maybe_mkwrite(pte_mkdirty(pte), vma);\n\n\tVM_BUG_ON(pte_write(pte) && !(PageAnon(page) && PageAnonExclusive(page)));\n\n\t \n\tif (PageAnon(page))\n\t\tpage_add_anon_rmap(page, vma, address, RMAP_NONE);\n\telse\n\t\t \n\t\tWARN_ON_ONCE(1);\n\n\tset_pte_at(vma->vm_mm, address, ptep, pte);\n\n\t \n\tupdate_mmu_cache(vma, address, ptep);\n}\n\n \nstatic int\ntry_restore_exclusive_pte(pte_t *src_pte, struct vm_area_struct *vma,\n\t\t\tunsigned long addr)\n{\n\tswp_entry_t entry = pte_to_swp_entry(ptep_get(src_pte));\n\tstruct page *page = pfn_swap_entry_to_page(entry);\n\n\tif (trylock_page(page)) {\n\t\trestore_exclusive_pte(vma, page, addr, src_pte);\n\t\tunlock_page(page);\n\t\treturn 0;\n\t}\n\n\treturn -EBUSY;\n}\n\n \n\nstatic unsigned long\ncopy_nonpresent_pte(struct mm_struct *dst_mm, struct mm_struct *src_mm,\n\t\tpte_t *dst_pte, pte_t *src_pte, struct vm_area_struct *dst_vma,\n\t\tstruct vm_area_struct *src_vma, unsigned long addr, int *rss)\n{\n\tunsigned long vm_flags = dst_vma->vm_flags;\n\tpte_t orig_pte = ptep_get(src_pte);\n\tpte_t pte = orig_pte;\n\tstruct page *page;\n\tswp_entry_t entry = pte_to_swp_entry(orig_pte);\n\n\tif (likely(!non_swap_entry(entry))) {\n\t\tif (swap_duplicate(entry) < 0)\n\t\t\treturn -EIO;\n\n\t\t \n\t\tif (unlikely(list_empty(&dst_mm->mmlist))) {\n\t\t\tspin_lock(&mmlist_lock);\n\t\t\tif (list_empty(&dst_mm->mmlist))\n\t\t\t\tlist_add(&dst_mm->mmlist,\n\t\t\t\t\t\t&src_mm->mmlist);\n\t\t\tspin_unlock(&mmlist_lock);\n\t\t}\n\t\t \n\t\tif (pte_swp_exclusive(orig_pte)) {\n\t\t\tpte = pte_swp_clear_exclusive(orig_pte);\n\t\t\tset_pte_at(src_mm, addr, src_pte, pte);\n\t\t}\n\t\trss[MM_SWAPENTS]++;\n\t} else if (is_migration_entry(entry)) {\n\t\tpage = pfn_swap_entry_to_page(entry);\n\n\t\trss[mm_counter(page)]++;\n\n\t\tif (!is_readable_migration_entry(entry) &&\n\t\t\t\tis_cow_mapping(vm_flags)) {\n\t\t\t \n\t\t\tentry = make_readable_migration_entry(\n\t\t\t\t\t\t\tswp_offset(entry));\n\t\t\tpte = swp_entry_to_pte(entry);\n\t\t\tif (pte_swp_soft_dirty(orig_pte))\n\t\t\t\tpte = pte_swp_mksoft_dirty(pte);\n\t\t\tif (pte_swp_uffd_wp(orig_pte))\n\t\t\t\tpte = pte_swp_mkuffd_wp(pte);\n\t\t\tset_pte_at(src_mm, addr, src_pte, pte);\n\t\t}\n\t} else if (is_device_private_entry(entry)) {\n\t\tpage = pfn_swap_entry_to_page(entry);\n\n\t\t \n\t\tget_page(page);\n\t\trss[mm_counter(page)]++;\n\t\t \n\t\tBUG_ON(page_try_dup_anon_rmap(page, false, src_vma));\n\n\t\t \n\t\tif (is_writable_device_private_entry(entry) &&\n\t\t    is_cow_mapping(vm_flags)) {\n\t\t\tentry = make_readable_device_private_entry(\n\t\t\t\t\t\t\tswp_offset(entry));\n\t\t\tpte = swp_entry_to_pte(entry);\n\t\t\tif (pte_swp_uffd_wp(orig_pte))\n\t\t\t\tpte = pte_swp_mkuffd_wp(pte);\n\t\t\tset_pte_at(src_mm, addr, src_pte, pte);\n\t\t}\n\t} else if (is_device_exclusive_entry(entry)) {\n\t\t \n\t\tVM_BUG_ON(!is_cow_mapping(src_vma->vm_flags));\n\t\tif (try_restore_exclusive_pte(src_pte, src_vma, addr))\n\t\t\treturn -EBUSY;\n\t\treturn -ENOENT;\n\t} else if (is_pte_marker_entry(entry)) {\n\t\tpte_marker marker = copy_pte_marker(entry, dst_vma);\n\n\t\tif (marker)\n\t\t\tset_pte_at(dst_mm, addr, dst_pte,\n\t\t\t\t   make_pte_marker(marker));\n\t\treturn 0;\n\t}\n\tif (!userfaultfd_wp(dst_vma))\n\t\tpte = pte_swp_clear_uffd_wp(pte);\n\tset_pte_at(dst_mm, addr, dst_pte, pte);\n\treturn 0;\n}\n\n \nstatic inline int\ncopy_present_page(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,\n\t\t  pte_t *dst_pte, pte_t *src_pte, unsigned long addr, int *rss,\n\t\t  struct folio **prealloc, struct page *page)\n{\n\tstruct folio *new_folio;\n\tpte_t pte;\n\n\tnew_folio = *prealloc;\n\tif (!new_folio)\n\t\treturn -EAGAIN;\n\n\t \n\t*prealloc = NULL;\n\tcopy_user_highpage(&new_folio->page, page, addr, src_vma);\n\t__folio_mark_uptodate(new_folio);\n\tfolio_add_new_anon_rmap(new_folio, dst_vma, addr);\n\tfolio_add_lru_vma(new_folio, dst_vma);\n\trss[MM_ANONPAGES]++;\n\n\t \n\tpte = mk_pte(&new_folio->page, dst_vma->vm_page_prot);\n\tpte = maybe_mkwrite(pte_mkdirty(pte), dst_vma);\n\tif (userfaultfd_pte_wp(dst_vma, ptep_get(src_pte)))\n\t\t \n\t\tpte = pte_mkuffd_wp(pte);\n\tset_pte_at(dst_vma->vm_mm, addr, dst_pte, pte);\n\treturn 0;\n}\n\n \nstatic inline int\ncopy_present_pte(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,\n\t\t pte_t *dst_pte, pte_t *src_pte, unsigned long addr, int *rss,\n\t\t struct folio **prealloc)\n{\n\tstruct mm_struct *src_mm = src_vma->vm_mm;\n\tunsigned long vm_flags = src_vma->vm_flags;\n\tpte_t pte = ptep_get(src_pte);\n\tstruct page *page;\n\tstruct folio *folio;\n\n\tpage = vm_normal_page(src_vma, addr, pte);\n\tif (page)\n\t\tfolio = page_folio(page);\n\tif (page && folio_test_anon(folio)) {\n\t\t \n\t\tfolio_get(folio);\n\t\tif (unlikely(page_try_dup_anon_rmap(page, false, src_vma))) {\n\t\t\t \n\t\t\tfolio_put(folio);\n\t\t\treturn copy_present_page(dst_vma, src_vma, dst_pte, src_pte,\n\t\t\t\t\t\t addr, rss, prealloc, page);\n\t\t}\n\t\trss[MM_ANONPAGES]++;\n\t} else if (page) {\n\t\tfolio_get(folio);\n\t\tpage_dup_file_rmap(page, false);\n\t\trss[mm_counter_file(page)]++;\n\t}\n\n\t \n\tif (is_cow_mapping(vm_flags) && pte_write(pte)) {\n\t\tptep_set_wrprotect(src_mm, addr, src_pte);\n\t\tpte = pte_wrprotect(pte);\n\t}\n\tVM_BUG_ON(page && folio_test_anon(folio) && PageAnonExclusive(page));\n\n\t \n\tif (vm_flags & VM_SHARED)\n\t\tpte = pte_mkclean(pte);\n\tpte = pte_mkold(pte);\n\n\tif (!userfaultfd_wp(dst_vma))\n\t\tpte = pte_clear_uffd_wp(pte);\n\n\tset_pte_at(dst_vma->vm_mm, addr, dst_pte, pte);\n\treturn 0;\n}\n\nstatic inline struct folio *page_copy_prealloc(struct mm_struct *src_mm,\n\t\tstruct vm_area_struct *vma, unsigned long addr)\n{\n\tstruct folio *new_folio;\n\n\tnew_folio = vma_alloc_folio(GFP_HIGHUSER_MOVABLE, 0, vma, addr, false);\n\tif (!new_folio)\n\t\treturn NULL;\n\n\tif (mem_cgroup_charge(new_folio, src_mm, GFP_KERNEL)) {\n\t\tfolio_put(new_folio);\n\t\treturn NULL;\n\t}\n\tfolio_throttle_swaprate(new_folio, GFP_KERNEL);\n\n\treturn new_folio;\n}\n\nstatic int\ncopy_pte_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,\n\t       pmd_t *dst_pmd, pmd_t *src_pmd, unsigned long addr,\n\t       unsigned long end)\n{\n\tstruct mm_struct *dst_mm = dst_vma->vm_mm;\n\tstruct mm_struct *src_mm = src_vma->vm_mm;\n\tpte_t *orig_src_pte, *orig_dst_pte;\n\tpte_t *src_pte, *dst_pte;\n\tpte_t ptent;\n\tspinlock_t *src_ptl, *dst_ptl;\n\tint progress, ret = 0;\n\tint rss[NR_MM_COUNTERS];\n\tswp_entry_t entry = (swp_entry_t){0};\n\tstruct folio *prealloc = NULL;\n\nagain:\n\tprogress = 0;\n\tinit_rss_vec(rss);\n\n\t \n\tdst_pte = pte_alloc_map_lock(dst_mm, dst_pmd, addr, &dst_ptl);\n\tif (!dst_pte) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tsrc_pte = pte_offset_map_nolock(src_mm, src_pmd, addr, &src_ptl);\n\tif (!src_pte) {\n\t\tpte_unmap_unlock(dst_pte, dst_ptl);\n\t\t \n\t\tgoto out;\n\t}\n\tspin_lock_nested(src_ptl, SINGLE_DEPTH_NESTING);\n\torig_src_pte = src_pte;\n\torig_dst_pte = dst_pte;\n\tarch_enter_lazy_mmu_mode();\n\n\tdo {\n\t\t \n\t\tif (progress >= 32) {\n\t\t\tprogress = 0;\n\t\t\tif (need_resched() ||\n\t\t\t    spin_needbreak(src_ptl) || spin_needbreak(dst_ptl))\n\t\t\t\tbreak;\n\t\t}\n\t\tptent = ptep_get(src_pte);\n\t\tif (pte_none(ptent)) {\n\t\t\tprogress++;\n\t\t\tcontinue;\n\t\t}\n\t\tif (unlikely(!pte_present(ptent))) {\n\t\t\tret = copy_nonpresent_pte(dst_mm, src_mm,\n\t\t\t\t\t\t  dst_pte, src_pte,\n\t\t\t\t\t\t  dst_vma, src_vma,\n\t\t\t\t\t\t  addr, rss);\n\t\t\tif (ret == -EIO) {\n\t\t\t\tentry = pte_to_swp_entry(ptep_get(src_pte));\n\t\t\t\tbreak;\n\t\t\t} else if (ret == -EBUSY) {\n\t\t\t\tbreak;\n\t\t\t} else if (!ret) {\n\t\t\t\tprogress += 8;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t \n\t\t\tWARN_ON_ONCE(ret != -ENOENT);\n\t\t}\n\t\t \n\t\tret = copy_present_pte(dst_vma, src_vma, dst_pte, src_pte,\n\t\t\t\t       addr, rss, &prealloc);\n\t\t \n\t\tif (unlikely(ret == -EAGAIN))\n\t\t\tbreak;\n\t\tif (unlikely(prealloc)) {\n\t\t\t \n\t\t\tfolio_put(prealloc);\n\t\t\tprealloc = NULL;\n\t\t}\n\t\tprogress += 8;\n\t} while (dst_pte++, src_pte++, addr += PAGE_SIZE, addr != end);\n\n\tarch_leave_lazy_mmu_mode();\n\tpte_unmap_unlock(orig_src_pte, src_ptl);\n\tadd_mm_rss_vec(dst_mm, rss);\n\tpte_unmap_unlock(orig_dst_pte, dst_ptl);\n\tcond_resched();\n\n\tif (ret == -EIO) {\n\t\tVM_WARN_ON_ONCE(!entry.val);\n\t\tif (add_swap_count_continuation(entry, GFP_KERNEL) < 0) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tentry.val = 0;\n\t} else if (ret == -EBUSY) {\n\t\tgoto out;\n\t} else if (ret ==  -EAGAIN) {\n\t\tprealloc = page_copy_prealloc(src_mm, src_vma, addr);\n\t\tif (!prealloc)\n\t\t\treturn -ENOMEM;\n\t} else if (ret) {\n\t\tVM_WARN_ON_ONCE(1);\n\t}\n\n\t \n\tret = 0;\n\n\tif (addr != end)\n\t\tgoto again;\nout:\n\tif (unlikely(prealloc))\n\t\tfolio_put(prealloc);\n\treturn ret;\n}\n\nstatic inline int\ncopy_pmd_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,\n\t       pud_t *dst_pud, pud_t *src_pud, unsigned long addr,\n\t       unsigned long end)\n{\n\tstruct mm_struct *dst_mm = dst_vma->vm_mm;\n\tstruct mm_struct *src_mm = src_vma->vm_mm;\n\tpmd_t *src_pmd, *dst_pmd;\n\tunsigned long next;\n\n\tdst_pmd = pmd_alloc(dst_mm, dst_pud, addr);\n\tif (!dst_pmd)\n\t\treturn -ENOMEM;\n\tsrc_pmd = pmd_offset(src_pud, addr);\n\tdo {\n\t\tnext = pmd_addr_end(addr, end);\n\t\tif (is_swap_pmd(*src_pmd) || pmd_trans_huge(*src_pmd)\n\t\t\t|| pmd_devmap(*src_pmd)) {\n\t\t\tint err;\n\t\t\tVM_BUG_ON_VMA(next-addr != HPAGE_PMD_SIZE, src_vma);\n\t\t\terr = copy_huge_pmd(dst_mm, src_mm, dst_pmd, src_pmd,\n\t\t\t\t\t    addr, dst_vma, src_vma);\n\t\t\tif (err == -ENOMEM)\n\t\t\t\treturn -ENOMEM;\n\t\t\tif (!err)\n\t\t\t\tcontinue;\n\t\t\t \n\t\t}\n\t\tif (pmd_none_or_clear_bad(src_pmd))\n\t\t\tcontinue;\n\t\tif (copy_pte_range(dst_vma, src_vma, dst_pmd, src_pmd,\n\t\t\t\t   addr, next))\n\t\t\treturn -ENOMEM;\n\t} while (dst_pmd++, src_pmd++, addr = next, addr != end);\n\treturn 0;\n}\n\nstatic inline int\ncopy_pud_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,\n\t       p4d_t *dst_p4d, p4d_t *src_p4d, unsigned long addr,\n\t       unsigned long end)\n{\n\tstruct mm_struct *dst_mm = dst_vma->vm_mm;\n\tstruct mm_struct *src_mm = src_vma->vm_mm;\n\tpud_t *src_pud, *dst_pud;\n\tunsigned long next;\n\n\tdst_pud = pud_alloc(dst_mm, dst_p4d, addr);\n\tif (!dst_pud)\n\t\treturn -ENOMEM;\n\tsrc_pud = pud_offset(src_p4d, addr);\n\tdo {\n\t\tnext = pud_addr_end(addr, end);\n\t\tif (pud_trans_huge(*src_pud) || pud_devmap(*src_pud)) {\n\t\t\tint err;\n\n\t\t\tVM_BUG_ON_VMA(next-addr != HPAGE_PUD_SIZE, src_vma);\n\t\t\terr = copy_huge_pud(dst_mm, src_mm,\n\t\t\t\t\t    dst_pud, src_pud, addr, src_vma);\n\t\t\tif (err == -ENOMEM)\n\t\t\t\treturn -ENOMEM;\n\t\t\tif (!err)\n\t\t\t\tcontinue;\n\t\t\t \n\t\t}\n\t\tif (pud_none_or_clear_bad(src_pud))\n\t\t\tcontinue;\n\t\tif (copy_pmd_range(dst_vma, src_vma, dst_pud, src_pud,\n\t\t\t\t   addr, next))\n\t\t\treturn -ENOMEM;\n\t} while (dst_pud++, src_pud++, addr = next, addr != end);\n\treturn 0;\n}\n\nstatic inline int\ncopy_p4d_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,\n\t       pgd_t *dst_pgd, pgd_t *src_pgd, unsigned long addr,\n\t       unsigned long end)\n{\n\tstruct mm_struct *dst_mm = dst_vma->vm_mm;\n\tp4d_t *src_p4d, *dst_p4d;\n\tunsigned long next;\n\n\tdst_p4d = p4d_alloc(dst_mm, dst_pgd, addr);\n\tif (!dst_p4d)\n\t\treturn -ENOMEM;\n\tsrc_p4d = p4d_offset(src_pgd, addr);\n\tdo {\n\t\tnext = p4d_addr_end(addr, end);\n\t\tif (p4d_none_or_clear_bad(src_p4d))\n\t\t\tcontinue;\n\t\tif (copy_pud_range(dst_vma, src_vma, dst_p4d, src_p4d,\n\t\t\t\t   addr, next))\n\t\t\treturn -ENOMEM;\n\t} while (dst_p4d++, src_p4d++, addr = next, addr != end);\n\treturn 0;\n}\n\n \nstatic bool\nvma_needs_copy(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma)\n{\n\t \n\tif (userfaultfd_wp(dst_vma))\n\t\treturn true;\n\n\tif (src_vma->vm_flags & (VM_PFNMAP | VM_MIXEDMAP))\n\t\treturn true;\n\n\tif (src_vma->anon_vma)\n\t\treturn true;\n\n\t \n\treturn false;\n}\n\nint\ncopy_page_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma)\n{\n\tpgd_t *src_pgd, *dst_pgd;\n\tunsigned long next;\n\tunsigned long addr = src_vma->vm_start;\n\tunsigned long end = src_vma->vm_end;\n\tstruct mm_struct *dst_mm = dst_vma->vm_mm;\n\tstruct mm_struct *src_mm = src_vma->vm_mm;\n\tstruct mmu_notifier_range range;\n\tbool is_cow;\n\tint ret;\n\n\tif (!vma_needs_copy(dst_vma, src_vma))\n\t\treturn 0;\n\n\tif (is_vm_hugetlb_page(src_vma))\n\t\treturn copy_hugetlb_page_range(dst_mm, src_mm, dst_vma, src_vma);\n\n\tif (unlikely(src_vma->vm_flags & VM_PFNMAP)) {\n\t\t \n\t\tret = track_pfn_copy(src_vma);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t \n\tis_cow = is_cow_mapping(src_vma->vm_flags);\n\n\tif (is_cow) {\n\t\tmmu_notifier_range_init(&range, MMU_NOTIFY_PROTECTION_PAGE,\n\t\t\t\t\t0, src_mm, addr, end);\n\t\tmmu_notifier_invalidate_range_start(&range);\n\t\t \n\t\tvma_assert_write_locked(src_vma);\n\t\traw_write_seqcount_begin(&src_mm->write_protect_seq);\n\t}\n\n\tret = 0;\n\tdst_pgd = pgd_offset(dst_mm, addr);\n\tsrc_pgd = pgd_offset(src_mm, addr);\n\tdo {\n\t\tnext = pgd_addr_end(addr, end);\n\t\tif (pgd_none_or_clear_bad(src_pgd))\n\t\t\tcontinue;\n\t\tif (unlikely(copy_p4d_range(dst_vma, src_vma, dst_pgd, src_pgd,\n\t\t\t\t\t    addr, next))) {\n\t\t\tuntrack_pfn_clear(dst_vma);\n\t\t\tret = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\t} while (dst_pgd++, src_pgd++, addr = next, addr != end);\n\n\tif (is_cow) {\n\t\traw_write_seqcount_end(&src_mm->write_protect_seq);\n\t\tmmu_notifier_invalidate_range_end(&range);\n\t}\n\treturn ret;\n}\n\n \nstatic inline bool should_zap_cows(struct zap_details *details)\n{\n\t \n\tif (!details)\n\t\treturn true;\n\n\t \n\treturn details->even_cows;\n}\n\n \nstatic inline bool should_zap_page(struct zap_details *details, struct page *page)\n{\n\t \n\tif (should_zap_cows(details))\n\t\treturn true;\n\n\t \n\tif (!page)\n\t\treturn true;\n\n\t \n\treturn !PageAnon(page);\n}\n\nstatic inline bool zap_drop_file_uffd_wp(struct zap_details *details)\n{\n\tif (!details)\n\t\treturn false;\n\n\treturn details->zap_flags & ZAP_FLAG_DROP_MARKER;\n}\n\n \nstatic inline void\nzap_install_uffd_wp_if_needed(struct vm_area_struct *vma,\n\t\t\t      unsigned long addr, pte_t *pte,\n\t\t\t      struct zap_details *details, pte_t pteval)\n{\n\t \n\tif (vma_is_anonymous(vma))\n\t\treturn;\n\n\tif (zap_drop_file_uffd_wp(details))\n\t\treturn;\n\n\tpte_install_uffd_wp_if_needed(vma, addr, pte, pteval);\n}\n\nstatic unsigned long zap_pte_range(struct mmu_gather *tlb,\n\t\t\t\tstruct vm_area_struct *vma, pmd_t *pmd,\n\t\t\t\tunsigned long addr, unsigned long end,\n\t\t\t\tstruct zap_details *details)\n{\n\tstruct mm_struct *mm = tlb->mm;\n\tint force_flush = 0;\n\tint rss[NR_MM_COUNTERS];\n\tspinlock_t *ptl;\n\tpte_t *start_pte;\n\tpte_t *pte;\n\tswp_entry_t entry;\n\n\ttlb_change_page_size(tlb, PAGE_SIZE);\n\tinit_rss_vec(rss);\n\tstart_pte = pte = pte_offset_map_lock(mm, pmd, addr, &ptl);\n\tif (!pte)\n\t\treturn addr;\n\n\tflush_tlb_batched_pending(mm);\n\tarch_enter_lazy_mmu_mode();\n\tdo {\n\t\tpte_t ptent = ptep_get(pte);\n\t\tstruct page *page;\n\n\t\tif (pte_none(ptent))\n\t\t\tcontinue;\n\n\t\tif (need_resched())\n\t\t\tbreak;\n\n\t\tif (pte_present(ptent)) {\n\t\t\tunsigned int delay_rmap;\n\n\t\t\tpage = vm_normal_page(vma, addr, ptent);\n\t\t\tif (unlikely(!should_zap_page(details, page)))\n\t\t\t\tcontinue;\n\t\t\tptent = ptep_get_and_clear_full(mm, addr, pte,\n\t\t\t\t\t\t\ttlb->fullmm);\n\t\t\tarch_check_zapped_pte(vma, ptent);\n\t\t\ttlb_remove_tlb_entry(tlb, pte, addr);\n\t\t\tzap_install_uffd_wp_if_needed(vma, addr, pte, details,\n\t\t\t\t\t\t      ptent);\n\t\t\tif (unlikely(!page)) {\n\t\t\t\tksm_might_unmap_zero_page(mm, ptent);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tdelay_rmap = 0;\n\t\t\tif (!PageAnon(page)) {\n\t\t\t\tif (pte_dirty(ptent)) {\n\t\t\t\t\tset_page_dirty(page);\n\t\t\t\t\tif (tlb_delay_rmap(tlb)) {\n\t\t\t\t\t\tdelay_rmap = 1;\n\t\t\t\t\t\tforce_flush = 1;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (pte_young(ptent) && likely(vma_has_recency(vma)))\n\t\t\t\t\tmark_page_accessed(page);\n\t\t\t}\n\t\t\trss[mm_counter(page)]--;\n\t\t\tif (!delay_rmap) {\n\t\t\t\tpage_remove_rmap(page, vma, false);\n\t\t\t\tif (unlikely(page_mapcount(page) < 0))\n\t\t\t\t\tprint_bad_pte(vma, addr, ptent, page);\n\t\t\t}\n\t\t\tif (unlikely(__tlb_remove_page(tlb, page, delay_rmap))) {\n\t\t\t\tforce_flush = 1;\n\t\t\t\taddr += PAGE_SIZE;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\tentry = pte_to_swp_entry(ptent);\n\t\tif (is_device_private_entry(entry) ||\n\t\t    is_device_exclusive_entry(entry)) {\n\t\t\tpage = pfn_swap_entry_to_page(entry);\n\t\t\tif (unlikely(!should_zap_page(details, page)))\n\t\t\t\tcontinue;\n\t\t\t \n\t\t\tWARN_ON_ONCE(!vma_is_anonymous(vma));\n\t\t\trss[mm_counter(page)]--;\n\t\t\tif (is_device_private_entry(entry))\n\t\t\t\tpage_remove_rmap(page, vma, false);\n\t\t\tput_page(page);\n\t\t} else if (!non_swap_entry(entry)) {\n\t\t\t \n\t\t\tif (!should_zap_cows(details))\n\t\t\t\tcontinue;\n\t\t\trss[MM_SWAPENTS]--;\n\t\t\tif (unlikely(!free_swap_and_cache(entry)))\n\t\t\t\tprint_bad_pte(vma, addr, ptent, NULL);\n\t\t} else if (is_migration_entry(entry)) {\n\t\t\tpage = pfn_swap_entry_to_page(entry);\n\t\t\tif (!should_zap_page(details, page))\n\t\t\t\tcontinue;\n\t\t\trss[mm_counter(page)]--;\n\t\t} else if (pte_marker_entry_uffd_wp(entry)) {\n\t\t\t \n\t\t\tif (!vma_is_anonymous(vma) &&\n\t\t\t    !zap_drop_file_uffd_wp(details))\n\t\t\t\tcontinue;\n\t\t} else if (is_hwpoison_entry(entry) ||\n\t\t\t   is_poisoned_swp_entry(entry)) {\n\t\t\tif (!should_zap_cows(details))\n\t\t\t\tcontinue;\n\t\t} else {\n\t\t\t \n\t\t\tWARN_ON_ONCE(1);\n\t\t}\n\t\tpte_clear_not_present_full(mm, addr, pte, tlb->fullmm);\n\t\tzap_install_uffd_wp_if_needed(vma, addr, pte, details, ptent);\n\t} while (pte++, addr += PAGE_SIZE, addr != end);\n\n\tadd_mm_rss_vec(mm, rss);\n\tarch_leave_lazy_mmu_mode();\n\n\t \n\tif (force_flush) {\n\t\ttlb_flush_mmu_tlbonly(tlb);\n\t\ttlb_flush_rmaps(tlb, vma);\n\t}\n\tpte_unmap_unlock(start_pte, ptl);\n\n\t \n\tif (force_flush)\n\t\ttlb_flush_mmu(tlb);\n\n\treturn addr;\n}\n\nstatic inline unsigned long zap_pmd_range(struct mmu_gather *tlb,\n\t\t\t\tstruct vm_area_struct *vma, pud_t *pud,\n\t\t\t\tunsigned long addr, unsigned long end,\n\t\t\t\tstruct zap_details *details)\n{\n\tpmd_t *pmd;\n\tunsigned long next;\n\n\tpmd = pmd_offset(pud, addr);\n\tdo {\n\t\tnext = pmd_addr_end(addr, end);\n\t\tif (is_swap_pmd(*pmd) || pmd_trans_huge(*pmd) || pmd_devmap(*pmd)) {\n\t\t\tif (next - addr != HPAGE_PMD_SIZE)\n\t\t\t\t__split_huge_pmd(vma, pmd, addr, false, NULL);\n\t\t\telse if (zap_huge_pmd(tlb, vma, pmd, addr)) {\n\t\t\t\taddr = next;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\t \n\t\t} else if (details && details->single_folio &&\n\t\t\t   folio_test_pmd_mappable(details->single_folio) &&\n\t\t\t   next - addr == HPAGE_PMD_SIZE && pmd_none(*pmd)) {\n\t\t\tspinlock_t *ptl = pmd_lock(tlb->mm, pmd);\n\t\t\t \n\t\t\tspin_unlock(ptl);\n\t\t}\n\t\tif (pmd_none(*pmd)) {\n\t\t\taddr = next;\n\t\t\tcontinue;\n\t\t}\n\t\taddr = zap_pte_range(tlb, vma, pmd, addr, next, details);\n\t\tif (addr != next)\n\t\t\tpmd--;\n\t} while (pmd++, cond_resched(), addr != end);\n\n\treturn addr;\n}\n\nstatic inline unsigned long zap_pud_range(struct mmu_gather *tlb,\n\t\t\t\tstruct vm_area_struct *vma, p4d_t *p4d,\n\t\t\t\tunsigned long addr, unsigned long end,\n\t\t\t\tstruct zap_details *details)\n{\n\tpud_t *pud;\n\tunsigned long next;\n\n\tpud = pud_offset(p4d, addr);\n\tdo {\n\t\tnext = pud_addr_end(addr, end);\n\t\tif (pud_trans_huge(*pud) || pud_devmap(*pud)) {\n\t\t\tif (next - addr != HPAGE_PUD_SIZE) {\n\t\t\t\tmmap_assert_locked(tlb->mm);\n\t\t\t\tsplit_huge_pud(vma, pud, addr);\n\t\t\t} else if (zap_huge_pud(tlb, vma, pud, addr))\n\t\t\t\tgoto next;\n\t\t\t \n\t\t}\n\t\tif (pud_none_or_clear_bad(pud))\n\t\t\tcontinue;\n\t\tnext = zap_pmd_range(tlb, vma, pud, addr, next, details);\nnext:\n\t\tcond_resched();\n\t} while (pud++, addr = next, addr != end);\n\n\treturn addr;\n}\n\nstatic inline unsigned long zap_p4d_range(struct mmu_gather *tlb,\n\t\t\t\tstruct vm_area_struct *vma, pgd_t *pgd,\n\t\t\t\tunsigned long addr, unsigned long end,\n\t\t\t\tstruct zap_details *details)\n{\n\tp4d_t *p4d;\n\tunsigned long next;\n\n\tp4d = p4d_offset(pgd, addr);\n\tdo {\n\t\tnext = p4d_addr_end(addr, end);\n\t\tif (p4d_none_or_clear_bad(p4d))\n\t\t\tcontinue;\n\t\tnext = zap_pud_range(tlb, vma, p4d, addr, next, details);\n\t} while (p4d++, addr = next, addr != end);\n\n\treturn addr;\n}\n\nvoid unmap_page_range(struct mmu_gather *tlb,\n\t\t\t     struct vm_area_struct *vma,\n\t\t\t     unsigned long addr, unsigned long end,\n\t\t\t     struct zap_details *details)\n{\n\tpgd_t *pgd;\n\tunsigned long next;\n\n\tBUG_ON(addr >= end);\n\ttlb_start_vma(tlb, vma);\n\tpgd = pgd_offset(vma->vm_mm, addr);\n\tdo {\n\t\tnext = pgd_addr_end(addr, end);\n\t\tif (pgd_none_or_clear_bad(pgd))\n\t\t\tcontinue;\n\t\tnext = zap_p4d_range(tlb, vma, pgd, addr, next, details);\n\t} while (pgd++, addr = next, addr != end);\n\ttlb_end_vma(tlb, vma);\n}\n\n\nstatic void unmap_single_vma(struct mmu_gather *tlb,\n\t\tstruct vm_area_struct *vma, unsigned long start_addr,\n\t\tunsigned long end_addr,\n\t\tstruct zap_details *details, bool mm_wr_locked)\n{\n\tunsigned long start = max(vma->vm_start, start_addr);\n\tunsigned long end;\n\n\tif (start >= vma->vm_end)\n\t\treturn;\n\tend = min(vma->vm_end, end_addr);\n\tif (end <= vma->vm_start)\n\t\treturn;\n\n\tif (vma->vm_file)\n\t\tuprobe_munmap(vma, start, end);\n\n\tif (unlikely(vma->vm_flags & VM_PFNMAP))\n\t\tuntrack_pfn(vma, 0, 0, mm_wr_locked);\n\n\tif (start != end) {\n\t\tif (unlikely(is_vm_hugetlb_page(vma))) {\n\t\t\t \n\t\t\tif (vma->vm_file) {\n\t\t\t\tzap_flags_t zap_flags = details ?\n\t\t\t\t    details->zap_flags : 0;\n\t\t\t\t__unmap_hugepage_range(tlb, vma, start, end,\n\t\t\t\t\t\t\t     NULL, zap_flags);\n\t\t\t}\n\t\t} else\n\t\t\tunmap_page_range(tlb, vma, start, end, details);\n\t}\n}\n\n \nvoid unmap_vmas(struct mmu_gather *tlb, struct ma_state *mas,\n\t\tstruct vm_area_struct *vma, unsigned long start_addr,\n\t\tunsigned long end_addr, unsigned long tree_end,\n\t\tbool mm_wr_locked)\n{\n\tstruct mmu_notifier_range range;\n\tstruct zap_details details = {\n\t\t.zap_flags = ZAP_FLAG_DROP_MARKER | ZAP_FLAG_UNMAP,\n\t\t \n\t\t.even_cows = true,\n\t};\n\n\tmmu_notifier_range_init(&range, MMU_NOTIFY_UNMAP, 0, vma->vm_mm,\n\t\t\t\tstart_addr, end_addr);\n\tmmu_notifier_invalidate_range_start(&range);\n\tdo {\n\t\tunsigned long start = start_addr;\n\t\tunsigned long end = end_addr;\n\t\thugetlb_zap_begin(vma, &start, &end);\n\t\tunmap_single_vma(tlb, vma, start, end, &details,\n\t\t\t\t mm_wr_locked);\n\t\thugetlb_zap_end(vma, &details);\n\t} while ((vma = mas_find(mas, tree_end - 1)) != NULL);\n\tmmu_notifier_invalidate_range_end(&range);\n}\n\n \nvoid zap_page_range_single(struct vm_area_struct *vma, unsigned long address,\n\t\tunsigned long size, struct zap_details *details)\n{\n\tconst unsigned long end = address + size;\n\tstruct mmu_notifier_range range;\n\tstruct mmu_gather tlb;\n\n\tlru_add_drain();\n\tmmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, vma->vm_mm,\n\t\t\t\taddress, end);\n\thugetlb_zap_begin(vma, &range.start, &range.end);\n\ttlb_gather_mmu(&tlb, vma->vm_mm);\n\tupdate_hiwater_rss(vma->vm_mm);\n\tmmu_notifier_invalidate_range_start(&range);\n\t \n\tunmap_single_vma(&tlb, vma, address, end, details, false);\n\tmmu_notifier_invalidate_range_end(&range);\n\ttlb_finish_mmu(&tlb);\n\thugetlb_zap_end(vma, details);\n}\n\n \nvoid zap_vma_ptes(struct vm_area_struct *vma, unsigned long address,\n\t\tunsigned long size)\n{\n\tif (!range_in_vma(vma, address, address + size) ||\n\t    \t\t!(vma->vm_flags & VM_PFNMAP))\n\t\treturn;\n\n\tzap_page_range_single(vma, address, size, NULL);\n}\nEXPORT_SYMBOL_GPL(zap_vma_ptes);\n\nstatic pmd_t *walk_to_pmd(struct mm_struct *mm, unsigned long addr)\n{\n\tpgd_t *pgd;\n\tp4d_t *p4d;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\n\tpgd = pgd_offset(mm, addr);\n\tp4d = p4d_alloc(mm, pgd, addr);\n\tif (!p4d)\n\t\treturn NULL;\n\tpud = pud_alloc(mm, p4d, addr);\n\tif (!pud)\n\t\treturn NULL;\n\tpmd = pmd_alloc(mm, pud, addr);\n\tif (!pmd)\n\t\treturn NULL;\n\n\tVM_BUG_ON(pmd_trans_huge(*pmd));\n\treturn pmd;\n}\n\npte_t *__get_locked_pte(struct mm_struct *mm, unsigned long addr,\n\t\t\tspinlock_t **ptl)\n{\n\tpmd_t *pmd = walk_to_pmd(mm, addr);\n\n\tif (!pmd)\n\t\treturn NULL;\n\treturn pte_alloc_map_lock(mm, pmd, addr, ptl);\n}\n\nstatic int validate_page_before_insert(struct page *page)\n{\n\tif (PageAnon(page) || PageSlab(page) || page_has_type(page))\n\t\treturn -EINVAL;\n\tflush_dcache_page(page);\n\treturn 0;\n}\n\nstatic int insert_page_into_pte_locked(struct vm_area_struct *vma, pte_t *pte,\n\t\t\tunsigned long addr, struct page *page, pgprot_t prot)\n{\n\tif (!pte_none(ptep_get(pte)))\n\t\treturn -EBUSY;\n\t \n\tget_page(page);\n\tinc_mm_counter(vma->vm_mm, mm_counter_file(page));\n\tpage_add_file_rmap(page, vma, false);\n\tset_pte_at(vma->vm_mm, addr, pte, mk_pte(page, prot));\n\treturn 0;\n}\n\n \nstatic int insert_page(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tstruct page *page, pgprot_t prot)\n{\n\tint retval;\n\tpte_t *pte;\n\tspinlock_t *ptl;\n\n\tretval = validate_page_before_insert(page);\n\tif (retval)\n\t\tgoto out;\n\tretval = -ENOMEM;\n\tpte = get_locked_pte(vma->vm_mm, addr, &ptl);\n\tif (!pte)\n\t\tgoto out;\n\tretval = insert_page_into_pte_locked(vma, pte, addr, page, prot);\n\tpte_unmap_unlock(pte, ptl);\nout:\n\treturn retval;\n}\n\nstatic int insert_page_in_batch_locked(struct vm_area_struct *vma, pte_t *pte,\n\t\t\tunsigned long addr, struct page *page, pgprot_t prot)\n{\n\tint err;\n\n\tif (!page_count(page))\n\t\treturn -EINVAL;\n\terr = validate_page_before_insert(page);\n\tif (err)\n\t\treturn err;\n\treturn insert_page_into_pte_locked(vma, pte, addr, page, prot);\n}\n\n \nstatic int insert_pages(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tstruct page **pages, unsigned long *num, pgprot_t prot)\n{\n\tpmd_t *pmd = NULL;\n\tpte_t *start_pte, *pte;\n\tspinlock_t *pte_lock;\n\tstruct mm_struct *const mm = vma->vm_mm;\n\tunsigned long curr_page_idx = 0;\n\tunsigned long remaining_pages_total = *num;\n\tunsigned long pages_to_write_in_pmd;\n\tint ret;\nmore:\n\tret = -EFAULT;\n\tpmd = walk_to_pmd(mm, addr);\n\tif (!pmd)\n\t\tgoto out;\n\n\tpages_to_write_in_pmd = min_t(unsigned long,\n\t\tremaining_pages_total, PTRS_PER_PTE - pte_index(addr));\n\n\t \n\tret = -ENOMEM;\n\tif (pte_alloc(mm, pmd))\n\t\tgoto out;\n\n\twhile (pages_to_write_in_pmd) {\n\t\tint pte_idx = 0;\n\t\tconst int batch_size = min_t(int, pages_to_write_in_pmd, 8);\n\n\t\tstart_pte = pte_offset_map_lock(mm, pmd, addr, &pte_lock);\n\t\tif (!start_pte) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t\tfor (pte = start_pte; pte_idx < batch_size; ++pte, ++pte_idx) {\n\t\t\tint err = insert_page_in_batch_locked(vma, pte,\n\t\t\t\taddr, pages[curr_page_idx], prot);\n\t\t\tif (unlikely(err)) {\n\t\t\t\tpte_unmap_unlock(start_pte, pte_lock);\n\t\t\t\tret = err;\n\t\t\t\tremaining_pages_total -= pte_idx;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\taddr += PAGE_SIZE;\n\t\t\t++curr_page_idx;\n\t\t}\n\t\tpte_unmap_unlock(start_pte, pte_lock);\n\t\tpages_to_write_in_pmd -= batch_size;\n\t\tremaining_pages_total -= batch_size;\n\t}\n\tif (remaining_pages_total)\n\t\tgoto more;\n\tret = 0;\nout:\n\t*num = remaining_pages_total;\n\treturn ret;\n}\n\n \nint vm_insert_pages(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tstruct page **pages, unsigned long *num)\n{\n\tconst unsigned long end_addr = addr + (*num * PAGE_SIZE) - 1;\n\n\tif (addr < vma->vm_start || end_addr >= vma->vm_end)\n\t\treturn -EFAULT;\n\tif (!(vma->vm_flags & VM_MIXEDMAP)) {\n\t\tBUG_ON(mmap_read_trylock(vma->vm_mm));\n\t\tBUG_ON(vma->vm_flags & VM_PFNMAP);\n\t\tvm_flags_set(vma, VM_MIXEDMAP);\n\t}\n\t \n\treturn insert_pages(vma, addr, pages, num, vma->vm_page_prot);\n}\nEXPORT_SYMBOL(vm_insert_pages);\n\n \nint vm_insert_page(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tstruct page *page)\n{\n\tif (addr < vma->vm_start || addr >= vma->vm_end)\n\t\treturn -EFAULT;\n\tif (!page_count(page))\n\t\treturn -EINVAL;\n\tif (!(vma->vm_flags & VM_MIXEDMAP)) {\n\t\tBUG_ON(mmap_read_trylock(vma->vm_mm));\n\t\tBUG_ON(vma->vm_flags & VM_PFNMAP);\n\t\tvm_flags_set(vma, VM_MIXEDMAP);\n\t}\n\treturn insert_page(vma, addr, page, vma->vm_page_prot);\n}\nEXPORT_SYMBOL(vm_insert_page);\n\n \nstatic int __vm_map_pages(struct vm_area_struct *vma, struct page **pages,\n\t\t\t\tunsigned long num, unsigned long offset)\n{\n\tunsigned long count = vma_pages(vma);\n\tunsigned long uaddr = vma->vm_start;\n\tint ret, i;\n\n\t \n\tif (offset >= num)\n\t\treturn -ENXIO;\n\n\t \n\tif (count > num - offset)\n\t\treturn -ENXIO;\n\n\tfor (i = 0; i < count; i++) {\n\t\tret = vm_insert_page(vma, uaddr, pages[offset + i]);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\tuaddr += PAGE_SIZE;\n\t}\n\n\treturn 0;\n}\n\n \nint vm_map_pages(struct vm_area_struct *vma, struct page **pages,\n\t\t\t\tunsigned long num)\n{\n\treturn __vm_map_pages(vma, pages, num, vma->vm_pgoff);\n}\nEXPORT_SYMBOL(vm_map_pages);\n\n \nint vm_map_pages_zero(struct vm_area_struct *vma, struct page **pages,\n\t\t\t\tunsigned long num)\n{\n\treturn __vm_map_pages(vma, pages, num, 0);\n}\nEXPORT_SYMBOL(vm_map_pages_zero);\n\nstatic vm_fault_t insert_pfn(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tpfn_t pfn, pgprot_t prot, bool mkwrite)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tpte_t *pte, entry;\n\tspinlock_t *ptl;\n\n\tpte = get_locked_pte(mm, addr, &ptl);\n\tif (!pte)\n\t\treturn VM_FAULT_OOM;\n\tentry = ptep_get(pte);\n\tif (!pte_none(entry)) {\n\t\tif (mkwrite) {\n\t\t\t \n\t\t\tif (pte_pfn(entry) != pfn_t_to_pfn(pfn)) {\n\t\t\t\tWARN_ON_ONCE(!is_zero_pfn(pte_pfn(entry)));\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t\tentry = pte_mkyoung(entry);\n\t\t\tentry = maybe_mkwrite(pte_mkdirty(entry), vma);\n\t\t\tif (ptep_set_access_flags(vma, addr, pte, entry, 1))\n\t\t\t\tupdate_mmu_cache(vma, addr, pte);\n\t\t}\n\t\tgoto out_unlock;\n\t}\n\n\t \n\tif (pfn_t_devmap(pfn))\n\t\tentry = pte_mkdevmap(pfn_t_pte(pfn, prot));\n\telse\n\t\tentry = pte_mkspecial(pfn_t_pte(pfn, prot));\n\n\tif (mkwrite) {\n\t\tentry = pte_mkyoung(entry);\n\t\tentry = maybe_mkwrite(pte_mkdirty(entry), vma);\n\t}\n\n\tset_pte_at(mm, addr, pte, entry);\n\tupdate_mmu_cache(vma, addr, pte);  \n\nout_unlock:\n\tpte_unmap_unlock(pte, ptl);\n\treturn VM_FAULT_NOPAGE;\n}\n\n \nvm_fault_t vmf_insert_pfn_prot(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tunsigned long pfn, pgprot_t pgprot)\n{\n\t \n\tBUG_ON(!(vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP)));\n\tBUG_ON((vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP)) ==\n\t\t\t\t\t\t(VM_PFNMAP|VM_MIXEDMAP));\n\tBUG_ON((vma->vm_flags & VM_PFNMAP) && is_cow_mapping(vma->vm_flags));\n\tBUG_ON((vma->vm_flags & VM_MIXEDMAP) && pfn_valid(pfn));\n\n\tif (addr < vma->vm_start || addr >= vma->vm_end)\n\t\treturn VM_FAULT_SIGBUS;\n\n\tif (!pfn_modify_allowed(pfn, pgprot))\n\t\treturn VM_FAULT_SIGBUS;\n\n\ttrack_pfn_insert(vma, &pgprot, __pfn_to_pfn_t(pfn, PFN_DEV));\n\n\treturn insert_pfn(vma, addr, __pfn_to_pfn_t(pfn, PFN_DEV), pgprot,\n\t\t\tfalse);\n}\nEXPORT_SYMBOL(vmf_insert_pfn_prot);\n\n \nvm_fault_t vmf_insert_pfn(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tunsigned long pfn)\n{\n\treturn vmf_insert_pfn_prot(vma, addr, pfn, vma->vm_page_prot);\n}\nEXPORT_SYMBOL(vmf_insert_pfn);\n\nstatic bool vm_mixed_ok(struct vm_area_struct *vma, pfn_t pfn)\n{\n\t \n\tif (vma->vm_flags & VM_MIXEDMAP)\n\t\treturn true;\n\tif (pfn_t_devmap(pfn))\n\t\treturn true;\n\tif (pfn_t_special(pfn))\n\t\treturn true;\n\tif (is_zero_pfn(pfn_t_to_pfn(pfn)))\n\t\treturn true;\n\treturn false;\n}\n\nstatic vm_fault_t __vm_insert_mixed(struct vm_area_struct *vma,\n\t\tunsigned long addr, pfn_t pfn, bool mkwrite)\n{\n\tpgprot_t pgprot = vma->vm_page_prot;\n\tint err;\n\n\tBUG_ON(!vm_mixed_ok(vma, pfn));\n\n\tif (addr < vma->vm_start || addr >= vma->vm_end)\n\t\treturn VM_FAULT_SIGBUS;\n\n\ttrack_pfn_insert(vma, &pgprot, pfn);\n\n\tif (!pfn_modify_allowed(pfn_t_to_pfn(pfn), pgprot))\n\t\treturn VM_FAULT_SIGBUS;\n\n\t \n\tif (!IS_ENABLED(CONFIG_ARCH_HAS_PTE_SPECIAL) &&\n\t    !pfn_t_devmap(pfn) && pfn_t_valid(pfn)) {\n\t\tstruct page *page;\n\n\t\t \n\t\tpage = pfn_to_page(pfn_t_to_pfn(pfn));\n\t\terr = insert_page(vma, addr, page, pgprot);\n\t} else {\n\t\treturn insert_pfn(vma, addr, pfn, pgprot, mkwrite);\n\t}\n\n\tif (err == -ENOMEM)\n\t\treturn VM_FAULT_OOM;\n\tif (err < 0 && err != -EBUSY)\n\t\treturn VM_FAULT_SIGBUS;\n\n\treturn VM_FAULT_NOPAGE;\n}\n\nvm_fault_t vmf_insert_mixed(struct vm_area_struct *vma, unsigned long addr,\n\t\tpfn_t pfn)\n{\n\treturn __vm_insert_mixed(vma, addr, pfn, false);\n}\nEXPORT_SYMBOL(vmf_insert_mixed);\n\n \nvm_fault_t vmf_insert_mixed_mkwrite(struct vm_area_struct *vma,\n\t\tunsigned long addr, pfn_t pfn)\n{\n\treturn __vm_insert_mixed(vma, addr, pfn, true);\n}\nEXPORT_SYMBOL(vmf_insert_mixed_mkwrite);\n\n \nstatic int remap_pte_range(struct mm_struct *mm, pmd_t *pmd,\n\t\t\tunsigned long addr, unsigned long end,\n\t\t\tunsigned long pfn, pgprot_t prot)\n{\n\tpte_t *pte, *mapped_pte;\n\tspinlock_t *ptl;\n\tint err = 0;\n\n\tmapped_pte = pte = pte_alloc_map_lock(mm, pmd, addr, &ptl);\n\tif (!pte)\n\t\treturn -ENOMEM;\n\tarch_enter_lazy_mmu_mode();\n\tdo {\n\t\tBUG_ON(!pte_none(ptep_get(pte)));\n\t\tif (!pfn_modify_allowed(pfn, prot)) {\n\t\t\terr = -EACCES;\n\t\t\tbreak;\n\t\t}\n\t\tset_pte_at(mm, addr, pte, pte_mkspecial(pfn_pte(pfn, prot)));\n\t\tpfn++;\n\t} while (pte++, addr += PAGE_SIZE, addr != end);\n\tarch_leave_lazy_mmu_mode();\n\tpte_unmap_unlock(mapped_pte, ptl);\n\treturn err;\n}\n\nstatic inline int remap_pmd_range(struct mm_struct *mm, pud_t *pud,\n\t\t\tunsigned long addr, unsigned long end,\n\t\t\tunsigned long pfn, pgprot_t prot)\n{\n\tpmd_t *pmd;\n\tunsigned long next;\n\tint err;\n\n\tpfn -= addr >> PAGE_SHIFT;\n\tpmd = pmd_alloc(mm, pud, addr);\n\tif (!pmd)\n\t\treturn -ENOMEM;\n\tVM_BUG_ON(pmd_trans_huge(*pmd));\n\tdo {\n\t\tnext = pmd_addr_end(addr, end);\n\t\terr = remap_pte_range(mm, pmd, addr, next,\n\t\t\t\tpfn + (addr >> PAGE_SHIFT), prot);\n\t\tif (err)\n\t\t\treturn err;\n\t} while (pmd++, addr = next, addr != end);\n\treturn 0;\n}\n\nstatic inline int remap_pud_range(struct mm_struct *mm, p4d_t *p4d,\n\t\t\tunsigned long addr, unsigned long end,\n\t\t\tunsigned long pfn, pgprot_t prot)\n{\n\tpud_t *pud;\n\tunsigned long next;\n\tint err;\n\n\tpfn -= addr >> PAGE_SHIFT;\n\tpud = pud_alloc(mm, p4d, addr);\n\tif (!pud)\n\t\treturn -ENOMEM;\n\tdo {\n\t\tnext = pud_addr_end(addr, end);\n\t\terr = remap_pmd_range(mm, pud, addr, next,\n\t\t\t\tpfn + (addr >> PAGE_SHIFT), prot);\n\t\tif (err)\n\t\t\treturn err;\n\t} while (pud++, addr = next, addr != end);\n\treturn 0;\n}\n\nstatic inline int remap_p4d_range(struct mm_struct *mm, pgd_t *pgd,\n\t\t\tunsigned long addr, unsigned long end,\n\t\t\tunsigned long pfn, pgprot_t prot)\n{\n\tp4d_t *p4d;\n\tunsigned long next;\n\tint err;\n\n\tpfn -= addr >> PAGE_SHIFT;\n\tp4d = p4d_alloc(mm, pgd, addr);\n\tif (!p4d)\n\t\treturn -ENOMEM;\n\tdo {\n\t\tnext = p4d_addr_end(addr, end);\n\t\terr = remap_pud_range(mm, p4d, addr, next,\n\t\t\t\tpfn + (addr >> PAGE_SHIFT), prot);\n\t\tif (err)\n\t\t\treturn err;\n\t} while (p4d++, addr = next, addr != end);\n\treturn 0;\n}\n\n \nint remap_pfn_range_notrack(struct vm_area_struct *vma, unsigned long addr,\n\t\tunsigned long pfn, unsigned long size, pgprot_t prot)\n{\n\tpgd_t *pgd;\n\tunsigned long next;\n\tunsigned long end = addr + PAGE_ALIGN(size);\n\tstruct mm_struct *mm = vma->vm_mm;\n\tint err;\n\n\tif (WARN_ON_ONCE(!PAGE_ALIGNED(addr)))\n\t\treturn -EINVAL;\n\n\t \n\tif (is_cow_mapping(vma->vm_flags)) {\n\t\tif (addr != vma->vm_start || end != vma->vm_end)\n\t\t\treturn -EINVAL;\n\t\tvma->vm_pgoff = pfn;\n\t}\n\n\tvm_flags_set(vma, VM_IO | VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP);\n\n\tBUG_ON(addr >= end);\n\tpfn -= addr >> PAGE_SHIFT;\n\tpgd = pgd_offset(mm, addr);\n\tflush_cache_range(vma, addr, end);\n\tdo {\n\t\tnext = pgd_addr_end(addr, end);\n\t\terr = remap_p4d_range(mm, pgd, addr, next,\n\t\t\t\tpfn + (addr >> PAGE_SHIFT), prot);\n\t\tif (err)\n\t\t\treturn err;\n\t} while (pgd++, addr = next, addr != end);\n\n\treturn 0;\n}\n\n \nint remap_pfn_range(struct vm_area_struct *vma, unsigned long addr,\n\t\t    unsigned long pfn, unsigned long size, pgprot_t prot)\n{\n\tint err;\n\n\terr = track_pfn_remap(vma, &prot, pfn, addr, PAGE_ALIGN(size));\n\tif (err)\n\t\treturn -EINVAL;\n\n\terr = remap_pfn_range_notrack(vma, addr, pfn, size, prot);\n\tif (err)\n\t\tuntrack_pfn(vma, pfn, PAGE_ALIGN(size), true);\n\treturn err;\n}\nEXPORT_SYMBOL(remap_pfn_range);\n\n \nint vm_iomap_memory(struct vm_area_struct *vma, phys_addr_t start, unsigned long len)\n{\n\tunsigned long vm_len, pfn, pages;\n\n\t \n\tif (start + len < start)\n\t\treturn -EINVAL;\n\t \n\tlen += start & ~PAGE_MASK;\n\tpfn = start >> PAGE_SHIFT;\n\tpages = (len + ~PAGE_MASK) >> PAGE_SHIFT;\n\tif (pfn + pages < pfn)\n\t\treturn -EINVAL;\n\n\t \n\tif (vma->vm_pgoff > pages)\n\t\treturn -EINVAL;\n\tpfn += vma->vm_pgoff;\n\tpages -= vma->vm_pgoff;\n\n\t \n\tvm_len = vma->vm_end - vma->vm_start;\n\tif (vm_len >> PAGE_SHIFT > pages)\n\t\treturn -EINVAL;\n\n\t \n\treturn io_remap_pfn_range(vma, vma->vm_start, pfn, vm_len, vma->vm_page_prot);\n}\nEXPORT_SYMBOL(vm_iomap_memory);\n\nstatic int apply_to_pte_range(struct mm_struct *mm, pmd_t *pmd,\n\t\t\t\t     unsigned long addr, unsigned long end,\n\t\t\t\t     pte_fn_t fn, void *data, bool create,\n\t\t\t\t     pgtbl_mod_mask *mask)\n{\n\tpte_t *pte, *mapped_pte;\n\tint err = 0;\n\tspinlock_t *ptl;\n\n\tif (create) {\n\t\tmapped_pte = pte = (mm == &init_mm) ?\n\t\t\tpte_alloc_kernel_track(pmd, addr, mask) :\n\t\t\tpte_alloc_map_lock(mm, pmd, addr, &ptl);\n\t\tif (!pte)\n\t\t\treturn -ENOMEM;\n\t} else {\n\t\tmapped_pte = pte = (mm == &init_mm) ?\n\t\t\tpte_offset_kernel(pmd, addr) :\n\t\t\tpte_offset_map_lock(mm, pmd, addr, &ptl);\n\t\tif (!pte)\n\t\t\treturn -EINVAL;\n\t}\n\n\tarch_enter_lazy_mmu_mode();\n\n\tif (fn) {\n\t\tdo {\n\t\t\tif (create || !pte_none(ptep_get(pte))) {\n\t\t\t\terr = fn(pte++, addr, data);\n\t\t\t\tif (err)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t} while (addr += PAGE_SIZE, addr != end);\n\t}\n\t*mask |= PGTBL_PTE_MODIFIED;\n\n\tarch_leave_lazy_mmu_mode();\n\n\tif (mm != &init_mm)\n\t\tpte_unmap_unlock(mapped_pte, ptl);\n\treturn err;\n}\n\nstatic int apply_to_pmd_range(struct mm_struct *mm, pud_t *pud,\n\t\t\t\t     unsigned long addr, unsigned long end,\n\t\t\t\t     pte_fn_t fn, void *data, bool create,\n\t\t\t\t     pgtbl_mod_mask *mask)\n{\n\tpmd_t *pmd;\n\tunsigned long next;\n\tint err = 0;\n\n\tBUG_ON(pud_huge(*pud));\n\n\tif (create) {\n\t\tpmd = pmd_alloc_track(mm, pud, addr, mask);\n\t\tif (!pmd)\n\t\t\treturn -ENOMEM;\n\t} else {\n\t\tpmd = pmd_offset(pud, addr);\n\t}\n\tdo {\n\t\tnext = pmd_addr_end(addr, end);\n\t\tif (pmd_none(*pmd) && !create)\n\t\t\tcontinue;\n\t\tif (WARN_ON_ONCE(pmd_leaf(*pmd)))\n\t\t\treturn -EINVAL;\n\t\tif (!pmd_none(*pmd) && WARN_ON_ONCE(pmd_bad(*pmd))) {\n\t\t\tif (!create)\n\t\t\t\tcontinue;\n\t\t\tpmd_clear_bad(pmd);\n\t\t}\n\t\terr = apply_to_pte_range(mm, pmd, addr, next,\n\t\t\t\t\t fn, data, create, mask);\n\t\tif (err)\n\t\t\tbreak;\n\t} while (pmd++, addr = next, addr != end);\n\n\treturn err;\n}\n\nstatic int apply_to_pud_range(struct mm_struct *mm, p4d_t *p4d,\n\t\t\t\t     unsigned long addr, unsigned long end,\n\t\t\t\t     pte_fn_t fn, void *data, bool create,\n\t\t\t\t     pgtbl_mod_mask *mask)\n{\n\tpud_t *pud;\n\tunsigned long next;\n\tint err = 0;\n\n\tif (create) {\n\t\tpud = pud_alloc_track(mm, p4d, addr, mask);\n\t\tif (!pud)\n\t\t\treturn -ENOMEM;\n\t} else {\n\t\tpud = pud_offset(p4d, addr);\n\t}\n\tdo {\n\t\tnext = pud_addr_end(addr, end);\n\t\tif (pud_none(*pud) && !create)\n\t\t\tcontinue;\n\t\tif (WARN_ON_ONCE(pud_leaf(*pud)))\n\t\t\treturn -EINVAL;\n\t\tif (!pud_none(*pud) && WARN_ON_ONCE(pud_bad(*pud))) {\n\t\t\tif (!create)\n\t\t\t\tcontinue;\n\t\t\tpud_clear_bad(pud);\n\t\t}\n\t\terr = apply_to_pmd_range(mm, pud, addr, next,\n\t\t\t\t\t fn, data, create, mask);\n\t\tif (err)\n\t\t\tbreak;\n\t} while (pud++, addr = next, addr != end);\n\n\treturn err;\n}\n\nstatic int apply_to_p4d_range(struct mm_struct *mm, pgd_t *pgd,\n\t\t\t\t     unsigned long addr, unsigned long end,\n\t\t\t\t     pte_fn_t fn, void *data, bool create,\n\t\t\t\t     pgtbl_mod_mask *mask)\n{\n\tp4d_t *p4d;\n\tunsigned long next;\n\tint err = 0;\n\n\tif (create) {\n\t\tp4d = p4d_alloc_track(mm, pgd, addr, mask);\n\t\tif (!p4d)\n\t\t\treturn -ENOMEM;\n\t} else {\n\t\tp4d = p4d_offset(pgd, addr);\n\t}\n\tdo {\n\t\tnext = p4d_addr_end(addr, end);\n\t\tif (p4d_none(*p4d) && !create)\n\t\t\tcontinue;\n\t\tif (WARN_ON_ONCE(p4d_leaf(*p4d)))\n\t\t\treturn -EINVAL;\n\t\tif (!p4d_none(*p4d) && WARN_ON_ONCE(p4d_bad(*p4d))) {\n\t\t\tif (!create)\n\t\t\t\tcontinue;\n\t\t\tp4d_clear_bad(p4d);\n\t\t}\n\t\terr = apply_to_pud_range(mm, p4d, addr, next,\n\t\t\t\t\t fn, data, create, mask);\n\t\tif (err)\n\t\t\tbreak;\n\t} while (p4d++, addr = next, addr != end);\n\n\treturn err;\n}\n\nstatic int __apply_to_page_range(struct mm_struct *mm, unsigned long addr,\n\t\t\t\t unsigned long size, pte_fn_t fn,\n\t\t\t\t void *data, bool create)\n{\n\tpgd_t *pgd;\n\tunsigned long start = addr, next;\n\tunsigned long end = addr + size;\n\tpgtbl_mod_mask mask = 0;\n\tint err = 0;\n\n\tif (WARN_ON(addr >= end))\n\t\treturn -EINVAL;\n\n\tpgd = pgd_offset(mm, addr);\n\tdo {\n\t\tnext = pgd_addr_end(addr, end);\n\t\tif (pgd_none(*pgd) && !create)\n\t\t\tcontinue;\n\t\tif (WARN_ON_ONCE(pgd_leaf(*pgd)))\n\t\t\treturn -EINVAL;\n\t\tif (!pgd_none(*pgd) && WARN_ON_ONCE(pgd_bad(*pgd))) {\n\t\t\tif (!create)\n\t\t\t\tcontinue;\n\t\t\tpgd_clear_bad(pgd);\n\t\t}\n\t\terr = apply_to_p4d_range(mm, pgd, addr, next,\n\t\t\t\t\t fn, data, create, &mask);\n\t\tif (err)\n\t\t\tbreak;\n\t} while (pgd++, addr = next, addr != end);\n\n\tif (mask & ARCH_PAGE_TABLE_SYNC_MASK)\n\t\tarch_sync_kernel_mappings(start, start + size);\n\n\treturn err;\n}\n\n \nint apply_to_page_range(struct mm_struct *mm, unsigned long addr,\n\t\t\tunsigned long size, pte_fn_t fn, void *data)\n{\n\treturn __apply_to_page_range(mm, addr, size, fn, data, true);\n}\nEXPORT_SYMBOL_GPL(apply_to_page_range);\n\n \nint apply_to_existing_page_range(struct mm_struct *mm, unsigned long addr,\n\t\t\t\t unsigned long size, pte_fn_t fn, void *data)\n{\n\treturn __apply_to_page_range(mm, addr, size, fn, data, false);\n}\nEXPORT_SYMBOL_GPL(apply_to_existing_page_range);\n\n \nstatic inline int pte_unmap_same(struct vm_fault *vmf)\n{\n\tint same = 1;\n#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPTION)\n\tif (sizeof(pte_t) > sizeof(unsigned long)) {\n\t\tspin_lock(vmf->ptl);\n\t\tsame = pte_same(ptep_get(vmf->pte), vmf->orig_pte);\n\t\tspin_unlock(vmf->ptl);\n\t}\n#endif\n\tpte_unmap(vmf->pte);\n\tvmf->pte = NULL;\n\treturn same;\n}\n\n \nstatic inline int __wp_page_copy_user(struct page *dst, struct page *src,\n\t\t\t\t      struct vm_fault *vmf)\n{\n\tint ret;\n\tvoid *kaddr;\n\tvoid __user *uaddr;\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long addr = vmf->address;\n\n\tif (likely(src)) {\n\t\tif (copy_mc_user_highpage(dst, src, addr, vma)) {\n\t\t\tmemory_failure_queue(page_to_pfn(src), 0);\n\t\t\treturn -EHWPOISON;\n\t\t}\n\t\treturn 0;\n\t}\n\n\t \n\tkaddr = kmap_atomic(dst);\n\tuaddr = (void __user *)(addr & PAGE_MASK);\n\n\t \n\tvmf->pte = NULL;\n\tif (!arch_has_hw_pte_young() && !pte_young(vmf->orig_pte)) {\n\t\tpte_t entry;\n\n\t\tvmf->pte = pte_offset_map_lock(mm, vmf->pmd, addr, &vmf->ptl);\n\t\tif (unlikely(!vmf->pte || !pte_same(ptep_get(vmf->pte), vmf->orig_pte))) {\n\t\t\t \n\t\t\tif (vmf->pte)\n\t\t\t\tupdate_mmu_tlb(vma, addr, vmf->pte);\n\t\t\tret = -EAGAIN;\n\t\t\tgoto pte_unlock;\n\t\t}\n\n\t\tentry = pte_mkyoung(vmf->orig_pte);\n\t\tif (ptep_set_access_flags(vma, addr, vmf->pte, entry, 0))\n\t\t\tupdate_mmu_cache_range(vmf, vma, addr, vmf->pte, 1);\n\t}\n\n\t \n\tif (__copy_from_user_inatomic(kaddr, uaddr, PAGE_SIZE)) {\n\t\tif (vmf->pte)\n\t\t\tgoto warn;\n\n\t\t \n\t\tvmf->pte = pte_offset_map_lock(mm, vmf->pmd, addr, &vmf->ptl);\n\t\tif (unlikely(!vmf->pte || !pte_same(ptep_get(vmf->pte), vmf->orig_pte))) {\n\t\t\t \n\t\t\tif (vmf->pte)\n\t\t\t\tupdate_mmu_tlb(vma, addr, vmf->pte);\n\t\t\tret = -EAGAIN;\n\t\t\tgoto pte_unlock;\n\t\t}\n\n\t\t \n\t\tif (__copy_from_user_inatomic(kaddr, uaddr, PAGE_SIZE)) {\n\t\t\t \nwarn:\n\t\t\tWARN_ON_ONCE(1);\n\t\t\tclear_page(kaddr);\n\t\t}\n\t}\n\n\tret = 0;\n\npte_unlock:\n\tif (vmf->pte)\n\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\tkunmap_atomic(kaddr);\n\tflush_dcache_page(dst);\n\n\treturn ret;\n}\n\nstatic gfp_t __get_fault_gfp_mask(struct vm_area_struct *vma)\n{\n\tstruct file *vm_file = vma->vm_file;\n\n\tif (vm_file)\n\t\treturn mapping_gfp_mask(vm_file->f_mapping) | __GFP_FS | __GFP_IO;\n\n\t \n\treturn GFP_KERNEL;\n}\n\n \nstatic vm_fault_t do_page_mkwrite(struct vm_fault *vmf, struct folio *folio)\n{\n\tvm_fault_t ret;\n\tunsigned int old_flags = vmf->flags;\n\n\tvmf->flags = FAULT_FLAG_WRITE|FAULT_FLAG_MKWRITE;\n\n\tif (vmf->vma->vm_file &&\n\t    IS_SWAPFILE(vmf->vma->vm_file->f_mapping->host))\n\t\treturn VM_FAULT_SIGBUS;\n\n\tret = vmf->vma->vm_ops->page_mkwrite(vmf);\n\t \n\tvmf->flags = old_flags;\n\tif (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE)))\n\t\treturn ret;\n\tif (unlikely(!(ret & VM_FAULT_LOCKED))) {\n\t\tfolio_lock(folio);\n\t\tif (!folio->mapping) {\n\t\t\tfolio_unlock(folio);\n\t\t\treturn 0;  \n\t\t}\n\t\tret |= VM_FAULT_LOCKED;\n\t} else\n\t\tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n\treturn ret;\n}\n\n \nstatic vm_fault_t fault_dirty_shared_page(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct address_space *mapping;\n\tstruct folio *folio = page_folio(vmf->page);\n\tbool dirtied;\n\tbool page_mkwrite = vma->vm_ops && vma->vm_ops->page_mkwrite;\n\n\tdirtied = folio_mark_dirty(folio);\n\tVM_BUG_ON_FOLIO(folio_test_anon(folio), folio);\n\t \n\tmapping = folio_raw_mapping(folio);\n\tfolio_unlock(folio);\n\n\tif (!page_mkwrite)\n\t\tfile_update_time(vma->vm_file);\n\n\t \n\tif ((dirtied || page_mkwrite) && mapping) {\n\t\tstruct file *fpin;\n\n\t\tfpin = maybe_unlock_mmap_for_io(vmf, NULL);\n\t\tbalance_dirty_pages_ratelimited(mapping);\n\t\tif (fpin) {\n\t\t\tfput(fpin);\n\t\t\treturn VM_FAULT_COMPLETED;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic inline void wp_page_reuse(struct vm_fault *vmf)\n\t__releases(vmf->ptl)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct page *page = vmf->page;\n\tpte_t entry;\n\n\tVM_BUG_ON(!(vmf->flags & FAULT_FLAG_WRITE));\n\tVM_BUG_ON(page && PageAnon(page) && !PageAnonExclusive(page));\n\n\t \n\tif (page)\n\t\tpage_cpupid_xchg_last(page, (1 << LAST_CPUPID_SHIFT) - 1);\n\n\tflush_cache_page(vma, vmf->address, pte_pfn(vmf->orig_pte));\n\tentry = pte_mkyoung(vmf->orig_pte);\n\tentry = maybe_mkwrite(pte_mkdirty(entry), vma);\n\tif (ptep_set_access_flags(vma, vmf->address, vmf->pte, entry, 1))\n\t\tupdate_mmu_cache_range(vmf, vma, vmf->address, vmf->pte, 1);\n\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\tcount_vm_event(PGREUSE);\n}\n\n \nstatic vm_fault_t wp_page_copy(struct vm_fault *vmf)\n{\n\tconst bool unshare = vmf->flags & FAULT_FLAG_UNSHARE;\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct folio *old_folio = NULL;\n\tstruct folio *new_folio = NULL;\n\tpte_t entry;\n\tint page_copied = 0;\n\tstruct mmu_notifier_range range;\n\tint ret;\n\n\tdelayacct_wpcopy_start();\n\n\tif (vmf->page)\n\t\told_folio = page_folio(vmf->page);\n\tif (unlikely(anon_vma_prepare(vma)))\n\t\tgoto oom;\n\n\tif (is_zero_pfn(pte_pfn(vmf->orig_pte))) {\n\t\tnew_folio = vma_alloc_zeroed_movable_folio(vma, vmf->address);\n\t\tif (!new_folio)\n\t\t\tgoto oom;\n\t} else {\n\t\tnew_folio = vma_alloc_folio(GFP_HIGHUSER_MOVABLE, 0, vma,\n\t\t\t\tvmf->address, false);\n\t\tif (!new_folio)\n\t\t\tgoto oom;\n\n\t\tret = __wp_page_copy_user(&new_folio->page, vmf->page, vmf);\n\t\tif (ret) {\n\t\t\t \n\t\t\tfolio_put(new_folio);\n\t\t\tif (old_folio)\n\t\t\t\tfolio_put(old_folio);\n\n\t\t\tdelayacct_wpcopy_end();\n\t\t\treturn ret == -EHWPOISON ? VM_FAULT_HWPOISON : 0;\n\t\t}\n\t\tkmsan_copy_page_meta(&new_folio->page, vmf->page);\n\t}\n\n\tif (mem_cgroup_charge(new_folio, mm, GFP_KERNEL))\n\t\tgoto oom_free_new;\n\tfolio_throttle_swaprate(new_folio, GFP_KERNEL);\n\n\t__folio_mark_uptodate(new_folio);\n\n\tmmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, mm,\n\t\t\t\tvmf->address & PAGE_MASK,\n\t\t\t\t(vmf->address & PAGE_MASK) + PAGE_SIZE);\n\tmmu_notifier_invalidate_range_start(&range);\n\n\t \n\tvmf->pte = pte_offset_map_lock(mm, vmf->pmd, vmf->address, &vmf->ptl);\n\tif (likely(vmf->pte && pte_same(ptep_get(vmf->pte), vmf->orig_pte))) {\n\t\tif (old_folio) {\n\t\t\tif (!folio_test_anon(old_folio)) {\n\t\t\t\tdec_mm_counter(mm, mm_counter_file(&old_folio->page));\n\t\t\t\tinc_mm_counter(mm, MM_ANONPAGES);\n\t\t\t}\n\t\t} else {\n\t\t\tksm_might_unmap_zero_page(mm, vmf->orig_pte);\n\t\t\tinc_mm_counter(mm, MM_ANONPAGES);\n\t\t}\n\t\tflush_cache_page(vma, vmf->address, pte_pfn(vmf->orig_pte));\n\t\tentry = mk_pte(&new_folio->page, vma->vm_page_prot);\n\t\tentry = pte_sw_mkyoung(entry);\n\t\tif (unlikely(unshare)) {\n\t\t\tif (pte_soft_dirty(vmf->orig_pte))\n\t\t\t\tentry = pte_mksoft_dirty(entry);\n\t\t\tif (pte_uffd_wp(vmf->orig_pte))\n\t\t\t\tentry = pte_mkuffd_wp(entry);\n\t\t} else {\n\t\t\tentry = maybe_mkwrite(pte_mkdirty(entry), vma);\n\t\t}\n\n\t\t \n\t\tptep_clear_flush(vma, vmf->address, vmf->pte);\n\t\tfolio_add_new_anon_rmap(new_folio, vma, vmf->address);\n\t\tfolio_add_lru_vma(new_folio, vma);\n\t\t \n\t\tBUG_ON(unshare && pte_write(entry));\n\t\tset_pte_at_notify(mm, vmf->address, vmf->pte, entry);\n\t\tupdate_mmu_cache_range(vmf, vma, vmf->address, vmf->pte, 1);\n\t\tif (old_folio) {\n\t\t\t \n\t\t\tpage_remove_rmap(vmf->page, vma, false);\n\t\t}\n\n\t\t \n\t\tnew_folio = old_folio;\n\t\tpage_copied = 1;\n\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\t} else if (vmf->pte) {\n\t\tupdate_mmu_tlb(vma, vmf->address, vmf->pte);\n\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\t}\n\n\tmmu_notifier_invalidate_range_end(&range);\n\n\tif (new_folio)\n\t\tfolio_put(new_folio);\n\tif (old_folio) {\n\t\tif (page_copied)\n\t\t\tfree_swap_cache(&old_folio->page);\n\t\tfolio_put(old_folio);\n\t}\n\n\tdelayacct_wpcopy_end();\n\treturn 0;\noom_free_new:\n\tfolio_put(new_folio);\noom:\n\tif (old_folio)\n\t\tfolio_put(old_folio);\n\n\tdelayacct_wpcopy_end();\n\treturn VM_FAULT_OOM;\n}\n\n \nvm_fault_t finish_mkwrite_fault(struct vm_fault *vmf)\n{\n\tWARN_ON_ONCE(!(vmf->vma->vm_flags & VM_SHARED));\n\tvmf->pte = pte_offset_map_lock(vmf->vma->vm_mm, vmf->pmd, vmf->address,\n\t\t\t\t       &vmf->ptl);\n\tif (!vmf->pte)\n\t\treturn VM_FAULT_NOPAGE;\n\t \n\tif (!pte_same(ptep_get(vmf->pte), vmf->orig_pte)) {\n\t\tupdate_mmu_tlb(vmf->vma, vmf->address, vmf->pte);\n\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\t\treturn VM_FAULT_NOPAGE;\n\t}\n\twp_page_reuse(vmf);\n\treturn 0;\n}\n\n \nstatic vm_fault_t wp_pfn_shared(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\n\tif (vma->vm_ops && vma->vm_ops->pfn_mkwrite) {\n\t\tvm_fault_t ret;\n\n\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\t\tif (vmf->flags & FAULT_FLAG_VMA_LOCK) {\n\t\t\tvma_end_read(vmf->vma);\n\t\t\treturn VM_FAULT_RETRY;\n\t\t}\n\n\t\tvmf->flags |= FAULT_FLAG_MKWRITE;\n\t\tret = vma->vm_ops->pfn_mkwrite(vmf);\n\t\tif (ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE))\n\t\t\treturn ret;\n\t\treturn finish_mkwrite_fault(vmf);\n\t}\n\twp_page_reuse(vmf);\n\treturn 0;\n}\n\nstatic vm_fault_t wp_page_shared(struct vm_fault *vmf, struct folio *folio)\n\t__releases(vmf->ptl)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tvm_fault_t ret = 0;\n\n\tfolio_get(folio);\n\n\tif (vma->vm_ops && vma->vm_ops->page_mkwrite) {\n\t\tvm_fault_t tmp;\n\n\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\t\tif (vmf->flags & FAULT_FLAG_VMA_LOCK) {\n\t\t\tfolio_put(folio);\n\t\t\tvma_end_read(vmf->vma);\n\t\t\treturn VM_FAULT_RETRY;\n\t\t}\n\n\t\ttmp = do_page_mkwrite(vmf, folio);\n\t\tif (unlikely(!tmp || (tmp &\n\t\t\t\t      (VM_FAULT_ERROR | VM_FAULT_NOPAGE)))) {\n\t\t\tfolio_put(folio);\n\t\t\treturn tmp;\n\t\t}\n\t\ttmp = finish_mkwrite_fault(vmf);\n\t\tif (unlikely(tmp & (VM_FAULT_ERROR | VM_FAULT_NOPAGE))) {\n\t\t\tfolio_unlock(folio);\n\t\t\tfolio_put(folio);\n\t\t\treturn tmp;\n\t\t}\n\t} else {\n\t\twp_page_reuse(vmf);\n\t\tfolio_lock(folio);\n\t}\n\tret |= fault_dirty_shared_page(vmf);\n\tfolio_put(folio);\n\n\treturn ret;\n}\n\n \nstatic vm_fault_t do_wp_page(struct vm_fault *vmf)\n\t__releases(vmf->ptl)\n{\n\tconst bool unshare = vmf->flags & FAULT_FLAG_UNSHARE;\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct folio *folio = NULL;\n\n\tif (likely(!unshare)) {\n\t\tif (userfaultfd_pte_wp(vma, ptep_get(vmf->pte))) {\n\t\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\t\t\treturn handle_userfault(vmf, VM_UFFD_WP);\n\t\t}\n\n\t\t \n\t\tif (unlikely(userfaultfd_wp(vmf->vma) &&\n\t\t\t     mm_tlb_flush_pending(vmf->vma->vm_mm)))\n\t\t\tflush_tlb_page(vmf->vma, vmf->address);\n\t}\n\n\tvmf->page = vm_normal_page(vma, vmf->address, vmf->orig_pte);\n\n\tif (vmf->page)\n\t\tfolio = page_folio(vmf->page);\n\n\t \n\tif (vma->vm_flags & (VM_SHARED | VM_MAYSHARE)) {\n\t\t \n\t\tif (!vmf->page)\n\t\t\treturn wp_pfn_shared(vmf);\n\t\treturn wp_page_shared(vmf, folio);\n\t}\n\n\t \n\tif (folio && folio_test_anon(folio)) {\n\t\t \n\t\tif (PageAnonExclusive(vmf->page))\n\t\t\tgoto reuse;\n\n\t\t \n\t\tif (folio_test_ksm(folio) || folio_ref_count(folio) > 3)\n\t\t\tgoto copy;\n\t\tif (!folio_test_lru(folio))\n\t\t\t \n\t\t\tlru_add_drain();\n\t\tif (folio_ref_count(folio) > 1 + folio_test_swapcache(folio))\n\t\t\tgoto copy;\n\t\tif (!folio_trylock(folio))\n\t\t\tgoto copy;\n\t\tif (folio_test_swapcache(folio))\n\t\t\tfolio_free_swap(folio);\n\t\tif (folio_test_ksm(folio) || folio_ref_count(folio) != 1) {\n\t\t\tfolio_unlock(folio);\n\t\t\tgoto copy;\n\t\t}\n\t\t \n\t\tpage_move_anon_rmap(vmf->page, vma);\n\t\tfolio_unlock(folio);\nreuse:\n\t\tif (unlikely(unshare)) {\n\t\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\t\t\treturn 0;\n\t\t}\n\t\twp_page_reuse(vmf);\n\t\treturn 0;\n\t}\ncopy:\n\tif ((vmf->flags & FAULT_FLAG_VMA_LOCK) && !vma->anon_vma) {\n\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\t\tvma_end_read(vmf->vma);\n\t\treturn VM_FAULT_RETRY;\n\t}\n\n\t \n\tif (folio)\n\t\tfolio_get(folio);\n\n\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n#ifdef CONFIG_KSM\n\tif (folio && folio_test_ksm(folio))\n\t\tcount_vm_event(COW_KSM);\n#endif\n\treturn wp_page_copy(vmf);\n}\n\nstatic void unmap_mapping_range_vma(struct vm_area_struct *vma,\n\t\tunsigned long start_addr, unsigned long end_addr,\n\t\tstruct zap_details *details)\n{\n\tzap_page_range_single(vma, start_addr, end_addr - start_addr, details);\n}\n\nstatic inline void unmap_mapping_range_tree(struct rb_root_cached *root,\n\t\t\t\t\t    pgoff_t first_index,\n\t\t\t\t\t    pgoff_t last_index,\n\t\t\t\t\t    struct zap_details *details)\n{\n\tstruct vm_area_struct *vma;\n\tpgoff_t vba, vea, zba, zea;\n\n\tvma_interval_tree_foreach(vma, root, first_index, last_index) {\n\t\tvba = vma->vm_pgoff;\n\t\tvea = vba + vma_pages(vma) - 1;\n\t\tzba = max(first_index, vba);\n\t\tzea = min(last_index, vea);\n\n\t\tunmap_mapping_range_vma(vma,\n\t\t\t((zba - vba) << PAGE_SHIFT) + vma->vm_start,\n\t\t\t((zea - vba + 1) << PAGE_SHIFT) + vma->vm_start,\n\t\t\t\tdetails);\n\t}\n}\n\n \nvoid unmap_mapping_folio(struct folio *folio)\n{\n\tstruct address_space *mapping = folio->mapping;\n\tstruct zap_details details = { };\n\tpgoff_t\tfirst_index;\n\tpgoff_t\tlast_index;\n\n\tVM_BUG_ON(!folio_test_locked(folio));\n\n\tfirst_index = folio->index;\n\tlast_index = folio_next_index(folio) - 1;\n\n\tdetails.even_cows = false;\n\tdetails.single_folio = folio;\n\tdetails.zap_flags = ZAP_FLAG_DROP_MARKER;\n\n\ti_mmap_lock_read(mapping);\n\tif (unlikely(!RB_EMPTY_ROOT(&mapping->i_mmap.rb_root)))\n\t\tunmap_mapping_range_tree(&mapping->i_mmap, first_index,\n\t\t\t\t\t last_index, &details);\n\ti_mmap_unlock_read(mapping);\n}\n\n \nvoid unmap_mapping_pages(struct address_space *mapping, pgoff_t start,\n\t\tpgoff_t nr, bool even_cows)\n{\n\tstruct zap_details details = { };\n\tpgoff_t\tfirst_index = start;\n\tpgoff_t\tlast_index = start + nr - 1;\n\n\tdetails.even_cows = even_cows;\n\tif (last_index < first_index)\n\t\tlast_index = ULONG_MAX;\n\n\ti_mmap_lock_read(mapping);\n\tif (unlikely(!RB_EMPTY_ROOT(&mapping->i_mmap.rb_root)))\n\t\tunmap_mapping_range_tree(&mapping->i_mmap, first_index,\n\t\t\t\t\t last_index, &details);\n\ti_mmap_unlock_read(mapping);\n}\nEXPORT_SYMBOL_GPL(unmap_mapping_pages);\n\n \nvoid unmap_mapping_range(struct address_space *mapping,\n\t\tloff_t const holebegin, loff_t const holelen, int even_cows)\n{\n\tpgoff_t hba = (pgoff_t)(holebegin) >> PAGE_SHIFT;\n\tpgoff_t hlen = ((pgoff_t)(holelen) + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\n\t \n\tif (sizeof(holelen) > sizeof(hlen)) {\n\t\tlong long holeend =\n\t\t\t(holebegin + holelen + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t\tif (holeend & ~(long long)ULONG_MAX)\n\t\t\thlen = ULONG_MAX - hba + 1;\n\t}\n\n\tunmap_mapping_pages(mapping, hba, hlen, even_cows);\n}\nEXPORT_SYMBOL(unmap_mapping_range);\n\n \nstatic vm_fault_t remove_device_exclusive_entry(struct vm_fault *vmf)\n{\n\tstruct folio *folio = page_folio(vmf->page);\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct mmu_notifier_range range;\n\tvm_fault_t ret;\n\n\t \n\tif (!folio_try_get(folio))\n\t\treturn 0;\n\n\tret = folio_lock_or_retry(folio, vmf);\n\tif (ret) {\n\t\tfolio_put(folio);\n\t\treturn ret;\n\t}\n\tmmu_notifier_range_init_owner(&range, MMU_NOTIFY_EXCLUSIVE, 0,\n\t\t\t\tvma->vm_mm, vmf->address & PAGE_MASK,\n\t\t\t\t(vmf->address & PAGE_MASK) + PAGE_SIZE, NULL);\n\tmmu_notifier_invalidate_range_start(&range);\n\n\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd, vmf->address,\n\t\t\t\t&vmf->ptl);\n\tif (likely(vmf->pte && pte_same(ptep_get(vmf->pte), vmf->orig_pte)))\n\t\trestore_exclusive_pte(vma, vmf->page, vmf->address, vmf->pte);\n\n\tif (vmf->pte)\n\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\tfolio_unlock(folio);\n\tfolio_put(folio);\n\n\tmmu_notifier_invalidate_range_end(&range);\n\treturn 0;\n}\n\nstatic inline bool should_try_to_free_swap(struct folio *folio,\n\t\t\t\t\t   struct vm_area_struct *vma,\n\t\t\t\t\t   unsigned int fault_flags)\n{\n\tif (!folio_test_swapcache(folio))\n\t\treturn false;\n\tif (mem_cgroup_swap_full(folio) || (vma->vm_flags & VM_LOCKED) ||\n\t    folio_test_mlocked(folio))\n\t\treturn true;\n\t \n\treturn (fault_flags & FAULT_FLAG_WRITE) && !folio_test_ksm(folio) &&\n\t\tfolio_ref_count(folio) == 2;\n}\n\nstatic vm_fault_t pte_marker_clear(struct vm_fault *vmf)\n{\n\tvmf->pte = pte_offset_map_lock(vmf->vma->vm_mm, vmf->pmd,\n\t\t\t\t       vmf->address, &vmf->ptl);\n\tif (!vmf->pte)\n\t\treturn 0;\n\t \n\tif (pte_same(vmf->orig_pte, ptep_get(vmf->pte)))\n\t\tpte_clear(vmf->vma->vm_mm, vmf->address, vmf->pte);\n\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\treturn 0;\n}\n\nstatic vm_fault_t do_pte_missing(struct vm_fault *vmf)\n{\n\tif (vma_is_anonymous(vmf->vma))\n\t\treturn do_anonymous_page(vmf);\n\telse\n\t\treturn do_fault(vmf);\n}\n\n \nstatic vm_fault_t pte_marker_handle_uffd_wp(struct vm_fault *vmf)\n{\n\t \n\tif (unlikely(!userfaultfd_wp(vmf->vma)))\n\t\treturn pte_marker_clear(vmf);\n\n\treturn do_pte_missing(vmf);\n}\n\nstatic vm_fault_t handle_pte_marker(struct vm_fault *vmf)\n{\n\tswp_entry_t entry = pte_to_swp_entry(vmf->orig_pte);\n\tunsigned long marker = pte_marker_get(entry);\n\n\t \n\tif (WARN_ON_ONCE(!marker))\n\t\treturn VM_FAULT_SIGBUS;\n\n\t \n\tif (marker & PTE_MARKER_POISONED)\n\t\treturn VM_FAULT_HWPOISON;\n\n\tif (pte_marker_entry_uffd_wp(entry))\n\t\treturn pte_marker_handle_uffd_wp(vmf);\n\n\t \n\treturn VM_FAULT_SIGBUS;\n}\n\n \nvm_fault_t do_swap_page(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct folio *swapcache, *folio = NULL;\n\tstruct page *page;\n\tstruct swap_info_struct *si = NULL;\n\trmap_t rmap_flags = RMAP_NONE;\n\tbool exclusive = false;\n\tswp_entry_t entry;\n\tpte_t pte;\n\tvm_fault_t ret = 0;\n\tvoid *shadow = NULL;\n\n\tif (!pte_unmap_same(vmf))\n\t\tgoto out;\n\n\tentry = pte_to_swp_entry(vmf->orig_pte);\n\tif (unlikely(non_swap_entry(entry))) {\n\t\tif (is_migration_entry(entry)) {\n\t\t\tmigration_entry_wait(vma->vm_mm, vmf->pmd,\n\t\t\t\t\t     vmf->address);\n\t\t} else if (is_device_exclusive_entry(entry)) {\n\t\t\tvmf->page = pfn_swap_entry_to_page(entry);\n\t\t\tret = remove_device_exclusive_entry(vmf);\n\t\t} else if (is_device_private_entry(entry)) {\n\t\t\tif (vmf->flags & FAULT_FLAG_VMA_LOCK) {\n\t\t\t\t \n\t\t\t\tvma_end_read(vma);\n\t\t\t\tret = VM_FAULT_RETRY;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tvmf->page = pfn_swap_entry_to_page(entry);\n\t\t\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,\n\t\t\t\t\tvmf->address, &vmf->ptl);\n\t\t\tif (unlikely(!vmf->pte ||\n\t\t\t\t     !pte_same(ptep_get(vmf->pte),\n\t\t\t\t\t\t\tvmf->orig_pte)))\n\t\t\t\tgoto unlock;\n\n\t\t\t \n\t\t\tget_page(vmf->page);\n\t\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\t\t\tret = vmf->page->pgmap->ops->migrate_to_ram(vmf);\n\t\t\tput_page(vmf->page);\n\t\t} else if (is_hwpoison_entry(entry)) {\n\t\t\tret = VM_FAULT_HWPOISON;\n\t\t} else if (is_pte_marker_entry(entry)) {\n\t\t\tret = handle_pte_marker(vmf);\n\t\t} else {\n\t\t\tprint_bad_pte(vma, vmf->address, vmf->orig_pte, NULL);\n\t\t\tret = VM_FAULT_SIGBUS;\n\t\t}\n\t\tgoto out;\n\t}\n\n\t \n\tsi = get_swap_device(entry);\n\tif (unlikely(!si))\n\t\tgoto out;\n\n\tfolio = swap_cache_get_folio(entry, vma, vmf->address);\n\tif (folio)\n\t\tpage = folio_file_page(folio, swp_offset(entry));\n\tswapcache = folio;\n\n\tif (!folio) {\n\t\tif (data_race(si->flags & SWP_SYNCHRONOUS_IO) &&\n\t\t    __swap_count(entry) == 1) {\n\t\t\t \n\t\t\tfolio = vma_alloc_folio(GFP_HIGHUSER_MOVABLE, 0,\n\t\t\t\t\t\tvma, vmf->address, false);\n\t\t\tpage = &folio->page;\n\t\t\tif (folio) {\n\t\t\t\t__folio_set_locked(folio);\n\t\t\t\t__folio_set_swapbacked(folio);\n\n\t\t\t\tif (mem_cgroup_swapin_charge_folio(folio,\n\t\t\t\t\t\t\tvma->vm_mm, GFP_KERNEL,\n\t\t\t\t\t\t\tentry)) {\n\t\t\t\t\tret = VM_FAULT_OOM;\n\t\t\t\t\tgoto out_page;\n\t\t\t\t}\n\t\t\t\tmem_cgroup_swapin_uncharge_swap(entry);\n\n\t\t\t\tshadow = get_shadow_from_swap_cache(entry);\n\t\t\t\tif (shadow)\n\t\t\t\t\tworkingset_refault(folio, shadow);\n\n\t\t\t\tfolio_add_lru(folio);\n\n\t\t\t\t \n\t\t\t\tfolio->swap = entry;\n\t\t\t\tswap_readpage(page, true, NULL);\n\t\t\t\tfolio->private = NULL;\n\t\t\t}\n\t\t} else {\n\t\t\tpage = swapin_readahead(entry, GFP_HIGHUSER_MOVABLE,\n\t\t\t\t\t\tvmf);\n\t\t\tif (page)\n\t\t\t\tfolio = page_folio(page);\n\t\t\tswapcache = folio;\n\t\t}\n\n\t\tif (!folio) {\n\t\t\t \n\t\t\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,\n\t\t\t\t\tvmf->address, &vmf->ptl);\n\t\t\tif (likely(vmf->pte &&\n\t\t\t\t   pte_same(ptep_get(vmf->pte), vmf->orig_pte)))\n\t\t\t\tret = VM_FAULT_OOM;\n\t\t\tgoto unlock;\n\t\t}\n\n\t\t \n\t\tret = VM_FAULT_MAJOR;\n\t\tcount_vm_event(PGMAJFAULT);\n\t\tcount_memcg_event_mm(vma->vm_mm, PGMAJFAULT);\n\t} else if (PageHWPoison(page)) {\n\t\t \n\t\tret = VM_FAULT_HWPOISON;\n\t\tgoto out_release;\n\t}\n\n\tret |= folio_lock_or_retry(folio, vmf);\n\tif (ret & VM_FAULT_RETRY)\n\t\tgoto out_release;\n\n\tif (swapcache) {\n\t\t \n\t\tif (unlikely(!folio_test_swapcache(folio) ||\n\t\t\t     page_swap_entry(page).val != entry.val))\n\t\t\tgoto out_page;\n\n\t\t \n\t\tpage = ksm_might_need_to_copy(page, vma, vmf->address);\n\t\tif (unlikely(!page)) {\n\t\t\tret = VM_FAULT_OOM;\n\t\t\tgoto out_page;\n\t\t} else if (unlikely(PTR_ERR(page) == -EHWPOISON)) {\n\t\t\tret = VM_FAULT_HWPOISON;\n\t\t\tgoto out_page;\n\t\t}\n\t\tfolio = page_folio(page);\n\n\t\t \n\t\tif ((vmf->flags & FAULT_FLAG_WRITE) && folio == swapcache &&\n\t\t    !folio_test_ksm(folio) && !folio_test_lru(folio))\n\t\t\tlru_add_drain();\n\t}\n\n\tfolio_throttle_swaprate(folio, GFP_KERNEL);\n\n\t \n\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd, vmf->address,\n\t\t\t&vmf->ptl);\n\tif (unlikely(!vmf->pte || !pte_same(ptep_get(vmf->pte), vmf->orig_pte)))\n\t\tgoto out_nomap;\n\n\tif (unlikely(!folio_test_uptodate(folio))) {\n\t\tret = VM_FAULT_SIGBUS;\n\t\tgoto out_nomap;\n\t}\n\n\t \n\tBUG_ON(!folio_test_anon(folio) && folio_test_mappedtodisk(folio));\n\tBUG_ON(folio_test_anon(folio) && PageAnonExclusive(page));\n\n\t \n\tif (!folio_test_ksm(folio)) {\n\t\texclusive = pte_swp_exclusive(vmf->orig_pte);\n\t\tif (folio != swapcache) {\n\t\t\t \n\t\t\texclusive = true;\n\t\t} else if (exclusive && folio_test_writeback(folio) &&\n\t\t\t  data_race(si->flags & SWP_STABLE_WRITES)) {\n\t\t\t \n\t\t\texclusive = false;\n\t\t}\n\t}\n\n\t \n\tarch_swap_restore(entry, folio);\n\n\t \n\tswap_free(entry);\n\tif (should_try_to_free_swap(folio, vma, vmf->flags))\n\t\tfolio_free_swap(folio);\n\n\tinc_mm_counter(vma->vm_mm, MM_ANONPAGES);\n\tdec_mm_counter(vma->vm_mm, MM_SWAPENTS);\n\tpte = mk_pte(page, vma->vm_page_prot);\n\n\t \n\tif (!folio_test_ksm(folio) &&\n\t    (exclusive || folio_ref_count(folio) == 1)) {\n\t\tif (vmf->flags & FAULT_FLAG_WRITE) {\n\t\t\tpte = maybe_mkwrite(pte_mkdirty(pte), vma);\n\t\t\tvmf->flags &= ~FAULT_FLAG_WRITE;\n\t\t}\n\t\trmap_flags |= RMAP_EXCLUSIVE;\n\t}\n\tflush_icache_page(vma, page);\n\tif (pte_swp_soft_dirty(vmf->orig_pte))\n\t\tpte = pte_mksoft_dirty(pte);\n\tif (pte_swp_uffd_wp(vmf->orig_pte))\n\t\tpte = pte_mkuffd_wp(pte);\n\tvmf->orig_pte = pte;\n\n\t \n\tif (unlikely(folio != swapcache && swapcache)) {\n\t\tpage_add_new_anon_rmap(page, vma, vmf->address);\n\t\tfolio_add_lru_vma(folio, vma);\n\t} else {\n\t\tpage_add_anon_rmap(page, vma, vmf->address, rmap_flags);\n\t}\n\n\tVM_BUG_ON(!folio_test_anon(folio) ||\n\t\t\t(pte_write(pte) && !PageAnonExclusive(page)));\n\tset_pte_at(vma->vm_mm, vmf->address, vmf->pte, pte);\n\tarch_do_swap_page(vma->vm_mm, vma, vmf->address, pte, vmf->orig_pte);\n\n\tfolio_unlock(folio);\n\tif (folio != swapcache && swapcache) {\n\t\t \n\t\tfolio_unlock(swapcache);\n\t\tfolio_put(swapcache);\n\t}\n\n\tif (vmf->flags & FAULT_FLAG_WRITE) {\n\t\tret |= do_wp_page(vmf);\n\t\tif (ret & VM_FAULT_ERROR)\n\t\t\tret &= VM_FAULT_ERROR;\n\t\tgoto out;\n\t}\n\n\t \n\tupdate_mmu_cache_range(vmf, vma, vmf->address, vmf->pte, 1);\nunlock:\n\tif (vmf->pte)\n\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\nout:\n\tif (si)\n\t\tput_swap_device(si);\n\treturn ret;\nout_nomap:\n\tif (vmf->pte)\n\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\nout_page:\n\tfolio_unlock(folio);\nout_release:\n\tfolio_put(folio);\n\tif (folio != swapcache && swapcache) {\n\t\tfolio_unlock(swapcache);\n\t\tfolio_put(swapcache);\n\t}\n\tif (si)\n\t\tput_swap_device(si);\n\treturn ret;\n}\n\n \nstatic vm_fault_t do_anonymous_page(struct vm_fault *vmf)\n{\n\tbool uffd_wp = vmf_orig_pte_uffd_wp(vmf);\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct folio *folio;\n\tvm_fault_t ret = 0;\n\tpte_t entry;\n\n\t \n\tif (vma->vm_flags & VM_SHARED)\n\t\treturn VM_FAULT_SIGBUS;\n\n\t \n\tif (pte_alloc(vma->vm_mm, vmf->pmd))\n\t\treturn VM_FAULT_OOM;\n\n\t \n\tif (!(vmf->flags & FAULT_FLAG_WRITE) &&\n\t\t\t!mm_forbids_zeropage(vma->vm_mm)) {\n\t\tentry = pte_mkspecial(pfn_pte(my_zero_pfn(vmf->address),\n\t\t\t\t\t\tvma->vm_page_prot));\n\t\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,\n\t\t\t\tvmf->address, &vmf->ptl);\n\t\tif (!vmf->pte)\n\t\t\tgoto unlock;\n\t\tif (vmf_pte_changed(vmf)) {\n\t\t\tupdate_mmu_tlb(vma, vmf->address, vmf->pte);\n\t\t\tgoto unlock;\n\t\t}\n\t\tret = check_stable_address_space(vma->vm_mm);\n\t\tif (ret)\n\t\t\tgoto unlock;\n\t\t \n\t\tif (userfaultfd_missing(vma)) {\n\t\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\t\t\treturn handle_userfault(vmf, VM_UFFD_MISSING);\n\t\t}\n\t\tgoto setpte;\n\t}\n\n\t \n\tif (unlikely(anon_vma_prepare(vma)))\n\t\tgoto oom;\n\tfolio = vma_alloc_zeroed_movable_folio(vma, vmf->address);\n\tif (!folio)\n\t\tgoto oom;\n\n\tif (mem_cgroup_charge(folio, vma->vm_mm, GFP_KERNEL))\n\t\tgoto oom_free_page;\n\tfolio_throttle_swaprate(folio, GFP_KERNEL);\n\n\t \n\t__folio_mark_uptodate(folio);\n\n\tentry = mk_pte(&folio->page, vma->vm_page_prot);\n\tentry = pte_sw_mkyoung(entry);\n\tif (vma->vm_flags & VM_WRITE)\n\t\tentry = pte_mkwrite(pte_mkdirty(entry), vma);\n\n\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd, vmf->address,\n\t\t\t&vmf->ptl);\n\tif (!vmf->pte)\n\t\tgoto release;\n\tif (vmf_pte_changed(vmf)) {\n\t\tupdate_mmu_tlb(vma, vmf->address, vmf->pte);\n\t\tgoto release;\n\t}\n\n\tret = check_stable_address_space(vma->vm_mm);\n\tif (ret)\n\t\tgoto release;\n\n\t \n\tif (userfaultfd_missing(vma)) {\n\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\t\tfolio_put(folio);\n\t\treturn handle_userfault(vmf, VM_UFFD_MISSING);\n\t}\n\n\tinc_mm_counter(vma->vm_mm, MM_ANONPAGES);\n\tfolio_add_new_anon_rmap(folio, vma, vmf->address);\n\tfolio_add_lru_vma(folio, vma);\nsetpte:\n\tif (uffd_wp)\n\t\tentry = pte_mkuffd_wp(entry);\n\tset_pte_at(vma->vm_mm, vmf->address, vmf->pte, entry);\n\n\t \n\tupdate_mmu_cache_range(vmf, vma, vmf->address, vmf->pte, 1);\nunlock:\n\tif (vmf->pte)\n\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\treturn ret;\nrelease:\n\tfolio_put(folio);\n\tgoto unlock;\noom_free_page:\n\tfolio_put(folio);\noom:\n\treturn VM_FAULT_OOM;\n}\n\n \nstatic vm_fault_t __do_fault(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tvm_fault_t ret;\n\n\t \n\tif (pmd_none(*vmf->pmd) && !vmf->prealloc_pte) {\n\t\tvmf->prealloc_pte = pte_alloc_one(vma->vm_mm);\n\t\tif (!vmf->prealloc_pte)\n\t\t\treturn VM_FAULT_OOM;\n\t}\n\n\tret = vma->vm_ops->fault(vmf);\n\tif (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY |\n\t\t\t    VM_FAULT_DONE_COW)))\n\t\treturn ret;\n\n\tif (unlikely(PageHWPoison(vmf->page))) {\n\t\tstruct page *page = vmf->page;\n\t\tvm_fault_t poisonret = VM_FAULT_HWPOISON;\n\t\tif (ret & VM_FAULT_LOCKED) {\n\t\t\tif (page_mapped(page))\n\t\t\t\tunmap_mapping_pages(page_mapping(page),\n\t\t\t\t\t\t    page->index, 1, false);\n\t\t\t \n\t\t\tif (invalidate_inode_page(page))\n\t\t\t\tpoisonret = VM_FAULT_NOPAGE;\n\t\t\tunlock_page(page);\n\t\t}\n\t\tput_page(page);\n\t\tvmf->page = NULL;\n\t\treturn poisonret;\n\t}\n\n\tif (unlikely(!(ret & VM_FAULT_LOCKED)))\n\t\tlock_page(vmf->page);\n\telse\n\t\tVM_BUG_ON_PAGE(!PageLocked(vmf->page), vmf->page);\n\n\treturn ret;\n}\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\nstatic void deposit_prealloc_pte(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\n\tpgtable_trans_huge_deposit(vma->vm_mm, vmf->pmd, vmf->prealloc_pte);\n\t \n\tmm_inc_nr_ptes(vma->vm_mm);\n\tvmf->prealloc_pte = NULL;\n}\n\nvm_fault_t do_set_pmd(struct vm_fault *vmf, struct page *page)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tbool write = vmf->flags & FAULT_FLAG_WRITE;\n\tunsigned long haddr = vmf->address & HPAGE_PMD_MASK;\n\tpmd_t entry;\n\tvm_fault_t ret = VM_FAULT_FALLBACK;\n\n\tif (!transhuge_vma_suitable(vma, haddr))\n\t\treturn ret;\n\n\tpage = compound_head(page);\n\tif (compound_order(page) != HPAGE_PMD_ORDER)\n\t\treturn ret;\n\n\t \n\tif (unlikely(PageHasHWPoisoned(page)))\n\t\treturn ret;\n\n\t \n\tif (arch_needs_pgtable_deposit() && !vmf->prealloc_pte) {\n\t\tvmf->prealloc_pte = pte_alloc_one(vma->vm_mm);\n\t\tif (!vmf->prealloc_pte)\n\t\t\treturn VM_FAULT_OOM;\n\t}\n\n\tvmf->ptl = pmd_lock(vma->vm_mm, vmf->pmd);\n\tif (unlikely(!pmd_none(*vmf->pmd)))\n\t\tgoto out;\n\n\tflush_icache_pages(vma, page, HPAGE_PMD_NR);\n\n\tentry = mk_huge_pmd(page, vma->vm_page_prot);\n\tif (write)\n\t\tentry = maybe_pmd_mkwrite(pmd_mkdirty(entry), vma);\n\n\tadd_mm_counter(vma->vm_mm, mm_counter_file(page), HPAGE_PMD_NR);\n\tpage_add_file_rmap(page, vma, true);\n\n\t \n\tif (arch_needs_pgtable_deposit())\n\t\tdeposit_prealloc_pte(vmf);\n\n\tset_pmd_at(vma->vm_mm, haddr, vmf->pmd, entry);\n\n\tupdate_mmu_cache_pmd(vma, haddr, vmf->pmd);\n\n\t \n\tret = 0;\n\tcount_vm_event(THP_FILE_MAPPED);\nout:\n\tspin_unlock(vmf->ptl);\n\treturn ret;\n}\n#else\nvm_fault_t do_set_pmd(struct vm_fault *vmf, struct page *page)\n{\n\treturn VM_FAULT_FALLBACK;\n}\n#endif\n\n \nvoid set_pte_range(struct vm_fault *vmf, struct folio *folio,\n\t\tstruct page *page, unsigned int nr, unsigned long addr)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tbool uffd_wp = vmf_orig_pte_uffd_wp(vmf);\n\tbool write = vmf->flags & FAULT_FLAG_WRITE;\n\tbool prefault = in_range(vmf->address, addr, nr * PAGE_SIZE);\n\tpte_t entry;\n\n\tflush_icache_pages(vma, page, nr);\n\tentry = mk_pte(page, vma->vm_page_prot);\n\n\tif (prefault && arch_wants_old_prefaulted_pte())\n\t\tentry = pte_mkold(entry);\n\telse\n\t\tentry = pte_sw_mkyoung(entry);\n\n\tif (write)\n\t\tentry = maybe_mkwrite(pte_mkdirty(entry), vma);\n\tif (unlikely(uffd_wp))\n\t\tentry = pte_mkuffd_wp(entry);\n\t \n\tif (write && !(vma->vm_flags & VM_SHARED)) {\n\t\tadd_mm_counter(vma->vm_mm, MM_ANONPAGES, nr);\n\t\tVM_BUG_ON_FOLIO(nr != 1, folio);\n\t\tfolio_add_new_anon_rmap(folio, vma, addr);\n\t\tfolio_add_lru_vma(folio, vma);\n\t} else {\n\t\tadd_mm_counter(vma->vm_mm, mm_counter_file(page), nr);\n\t\tfolio_add_file_rmap_range(folio, page, nr, vma, false);\n\t}\n\tset_ptes(vma->vm_mm, addr, vmf->pte, entry, nr);\n\n\t \n\tupdate_mmu_cache_range(vmf, vma, addr, vmf->pte, nr);\n}\n\nstatic bool vmf_pte_changed(struct vm_fault *vmf)\n{\n\tif (vmf->flags & FAULT_FLAG_ORIG_PTE_VALID)\n\t\treturn !pte_same(ptep_get(vmf->pte), vmf->orig_pte);\n\n\treturn !pte_none(ptep_get(vmf->pte));\n}\n\n \nvm_fault_t finish_fault(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct page *page;\n\tvm_fault_t ret;\n\n\t \n\tif ((vmf->flags & FAULT_FLAG_WRITE) && !(vma->vm_flags & VM_SHARED))\n\t\tpage = vmf->cow_page;\n\telse\n\t\tpage = vmf->page;\n\n\t \n\tif (!(vma->vm_flags & VM_SHARED)) {\n\t\tret = check_stable_address_space(vma->vm_mm);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (pmd_none(*vmf->pmd)) {\n\t\tif (PageTransCompound(page)) {\n\t\t\tret = do_set_pmd(vmf, page);\n\t\t\tif (ret != VM_FAULT_FALLBACK)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tif (vmf->prealloc_pte)\n\t\t\tpmd_install(vma->vm_mm, vmf->pmd, &vmf->prealloc_pte);\n\t\telse if (unlikely(pte_alloc(vma->vm_mm, vmf->pmd)))\n\t\t\treturn VM_FAULT_OOM;\n\t}\n\n\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,\n\t\t\t\t      vmf->address, &vmf->ptl);\n\tif (!vmf->pte)\n\t\treturn VM_FAULT_NOPAGE;\n\n\t \n\tif (likely(!vmf_pte_changed(vmf))) {\n\t\tstruct folio *folio = page_folio(page);\n\n\t\tset_pte_range(vmf, folio, page, 1, vmf->address);\n\t\tret = 0;\n\t} else {\n\t\tupdate_mmu_tlb(vma, vmf->address, vmf->pte);\n\t\tret = VM_FAULT_NOPAGE;\n\t}\n\n\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\treturn ret;\n}\n\nstatic unsigned long fault_around_pages __read_mostly =\n\t65536 >> PAGE_SHIFT;\n\n#ifdef CONFIG_DEBUG_FS\nstatic int fault_around_bytes_get(void *data, u64 *val)\n{\n\t*val = fault_around_pages << PAGE_SHIFT;\n\treturn 0;\n}\n\n \nstatic int fault_around_bytes_set(void *data, u64 val)\n{\n\tif (val / PAGE_SIZE > PTRS_PER_PTE)\n\t\treturn -EINVAL;\n\n\t \n\tfault_around_pages = max(rounddown_pow_of_two(val) >> PAGE_SHIFT, 1UL);\n\n\treturn 0;\n}\nDEFINE_DEBUGFS_ATTRIBUTE(fault_around_bytes_fops,\n\t\tfault_around_bytes_get, fault_around_bytes_set, \"%llu\\n\");\n\nstatic int __init fault_around_debugfs(void)\n{\n\tdebugfs_create_file_unsafe(\"fault_around_bytes\", 0644, NULL, NULL,\n\t\t\t\t   &fault_around_bytes_fops);\n\treturn 0;\n}\nlate_initcall(fault_around_debugfs);\n#endif\n\n \nstatic vm_fault_t do_fault_around(struct vm_fault *vmf)\n{\n\tpgoff_t nr_pages = READ_ONCE(fault_around_pages);\n\tpgoff_t pte_off = pte_index(vmf->address);\n\t \n\tpgoff_t vma_off = vmf->pgoff - vmf->vma->vm_pgoff;\n\tpgoff_t from_pte, to_pte;\n\tvm_fault_t ret;\n\n\t \n\tfrom_pte = max(ALIGN_DOWN(pte_off, nr_pages),\n\t\t       pte_off - min(pte_off, vma_off));\n\n\t \n\tto_pte = min3(from_pte + nr_pages, (pgoff_t)PTRS_PER_PTE,\n\t\t      pte_off + vma_pages(vmf->vma) - vma_off) - 1;\n\n\tif (pmd_none(*vmf->pmd)) {\n\t\tvmf->prealloc_pte = pte_alloc_one(vmf->vma->vm_mm);\n\t\tif (!vmf->prealloc_pte)\n\t\t\treturn VM_FAULT_OOM;\n\t}\n\n\trcu_read_lock();\n\tret = vmf->vma->vm_ops->map_pages(vmf,\n\t\t\tvmf->pgoff + from_pte - pte_off,\n\t\t\tvmf->pgoff + to_pte - pte_off);\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\n \nstatic inline bool should_fault_around(struct vm_fault *vmf)\n{\n\t \n\tif (!vmf->vma->vm_ops->map_pages)\n\t\treturn false;\n\n\tif (uffd_disable_fault_around(vmf->vma))\n\t\treturn false;\n\n\t \n\treturn fault_around_pages > 1;\n}\n\nstatic vm_fault_t do_read_fault(struct vm_fault *vmf)\n{\n\tvm_fault_t ret = 0;\n\tstruct folio *folio;\n\n\t \n\tif (should_fault_around(vmf)) {\n\t\tret = do_fault_around(vmf);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (vmf->flags & FAULT_FLAG_VMA_LOCK) {\n\t\tvma_end_read(vmf->vma);\n\t\treturn VM_FAULT_RETRY;\n\t}\n\n\tret = __do_fault(vmf);\n\tif (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))\n\t\treturn ret;\n\n\tret |= finish_fault(vmf);\n\tfolio = page_folio(vmf->page);\n\tfolio_unlock(folio);\n\tif (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))\n\t\tfolio_put(folio);\n\treturn ret;\n}\n\nstatic vm_fault_t do_cow_fault(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tvm_fault_t ret;\n\n\tif (vmf->flags & FAULT_FLAG_VMA_LOCK) {\n\t\tvma_end_read(vma);\n\t\treturn VM_FAULT_RETRY;\n\t}\n\n\tif (unlikely(anon_vma_prepare(vma)))\n\t\treturn VM_FAULT_OOM;\n\n\tvmf->cow_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma, vmf->address);\n\tif (!vmf->cow_page)\n\t\treturn VM_FAULT_OOM;\n\n\tif (mem_cgroup_charge(page_folio(vmf->cow_page), vma->vm_mm,\n\t\t\t\tGFP_KERNEL)) {\n\t\tput_page(vmf->cow_page);\n\t\treturn VM_FAULT_OOM;\n\t}\n\tfolio_throttle_swaprate(page_folio(vmf->cow_page), GFP_KERNEL);\n\n\tret = __do_fault(vmf);\n\tif (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))\n\t\tgoto uncharge_out;\n\tif (ret & VM_FAULT_DONE_COW)\n\t\treturn ret;\n\n\tcopy_user_highpage(vmf->cow_page, vmf->page, vmf->address, vma);\n\t__SetPageUptodate(vmf->cow_page);\n\n\tret |= finish_fault(vmf);\n\tunlock_page(vmf->page);\n\tput_page(vmf->page);\n\tif (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))\n\t\tgoto uncharge_out;\n\treturn ret;\nuncharge_out:\n\tput_page(vmf->cow_page);\n\treturn ret;\n}\n\nstatic vm_fault_t do_shared_fault(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tvm_fault_t ret, tmp;\n\tstruct folio *folio;\n\n\tif (vmf->flags & FAULT_FLAG_VMA_LOCK) {\n\t\tvma_end_read(vma);\n\t\treturn VM_FAULT_RETRY;\n\t}\n\n\tret = __do_fault(vmf);\n\tif (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))\n\t\treturn ret;\n\n\tfolio = page_folio(vmf->page);\n\n\t \n\tif (vma->vm_ops->page_mkwrite) {\n\t\tfolio_unlock(folio);\n\t\ttmp = do_page_mkwrite(vmf, folio);\n\t\tif (unlikely(!tmp ||\n\t\t\t\t(tmp & (VM_FAULT_ERROR | VM_FAULT_NOPAGE)))) {\n\t\t\tfolio_put(folio);\n\t\t\treturn tmp;\n\t\t}\n\t}\n\n\tret |= finish_fault(vmf);\n\tif (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE |\n\t\t\t\t\tVM_FAULT_RETRY))) {\n\t\tfolio_unlock(folio);\n\t\tfolio_put(folio);\n\t\treturn ret;\n\t}\n\n\tret |= fault_dirty_shared_page(vmf);\n\treturn ret;\n}\n\n \nstatic vm_fault_t do_fault(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct mm_struct *vm_mm = vma->vm_mm;\n\tvm_fault_t ret;\n\n\t \n\tif (!vma->vm_ops->fault) {\n\t\tvmf->pte = pte_offset_map_lock(vmf->vma->vm_mm, vmf->pmd,\n\t\t\t\t\t       vmf->address, &vmf->ptl);\n\t\tif (unlikely(!vmf->pte))\n\t\t\tret = VM_FAULT_SIGBUS;\n\t\telse {\n\t\t\t \n\t\t\tif (unlikely(pte_none(ptep_get(vmf->pte))))\n\t\t\t\tret = VM_FAULT_SIGBUS;\n\t\t\telse\n\t\t\t\tret = VM_FAULT_NOPAGE;\n\n\t\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\t\t}\n\t} else if (!(vmf->flags & FAULT_FLAG_WRITE))\n\t\tret = do_read_fault(vmf);\n\telse if (!(vma->vm_flags & VM_SHARED))\n\t\tret = do_cow_fault(vmf);\n\telse\n\t\tret = do_shared_fault(vmf);\n\n\t \n\tif (vmf->prealloc_pte) {\n\t\tpte_free(vm_mm, vmf->prealloc_pte);\n\t\tvmf->prealloc_pte = NULL;\n\t}\n\treturn ret;\n}\n\nint numa_migrate_prep(struct page *page, struct vm_area_struct *vma,\n\t\t      unsigned long addr, int page_nid, int *flags)\n{\n\tget_page(page);\n\n\t \n\tvma_set_access_pid_bit(vma);\n\n\tcount_vm_numa_event(NUMA_HINT_FAULTS);\n\tif (page_nid == numa_node_id()) {\n\t\tcount_vm_numa_event(NUMA_HINT_FAULTS_LOCAL);\n\t\t*flags |= TNF_FAULT_LOCAL;\n\t}\n\n\treturn mpol_misplaced(page, vma, addr);\n}\n\nstatic vm_fault_t do_numa_page(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct page *page = NULL;\n\tint page_nid = NUMA_NO_NODE;\n\tbool writable = false;\n\tint last_cpupid;\n\tint target_nid;\n\tpte_t pte, old_pte;\n\tint flags = 0;\n\n\t \n\tspin_lock(vmf->ptl);\n\tif (unlikely(!pte_same(ptep_get(vmf->pte), vmf->orig_pte))) {\n\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\t\tgoto out;\n\t}\n\n\t \n\told_pte = ptep_get(vmf->pte);\n\tpte = pte_modify(old_pte, vma->vm_page_prot);\n\n\t \n\twritable = pte_write(pte);\n\tif (!writable && vma_wants_manual_pte_write_upgrade(vma) &&\n\t    can_change_pte_writable(vma, vmf->address, pte))\n\t\twritable = true;\n\n\tpage = vm_normal_page(vma, vmf->address, pte);\n\tif (!page || is_zone_device_page(page))\n\t\tgoto out_map;\n\n\t \n\tif (PageCompound(page))\n\t\tgoto out_map;\n\n\t \n\tif (!writable)\n\t\tflags |= TNF_NO_GROUP;\n\n\t \n\tif (page_mapcount(page) > 1 && (vma->vm_flags & VM_SHARED))\n\t\tflags |= TNF_SHARED;\n\n\tpage_nid = page_to_nid(page);\n\t \n\tif ((sysctl_numa_balancing_mode & NUMA_BALANCING_MEMORY_TIERING) &&\n\t    !node_is_toptier(page_nid))\n\t\tlast_cpupid = (-1 & LAST_CPUPID_MASK);\n\telse\n\t\tlast_cpupid = page_cpupid_last(page);\n\ttarget_nid = numa_migrate_prep(page, vma, vmf->address, page_nid,\n\t\t\t&flags);\n\tif (target_nid == NUMA_NO_NODE) {\n\t\tput_page(page);\n\t\tgoto out_map;\n\t}\n\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\twritable = false;\n\n\t \n\tif (migrate_misplaced_page(page, vma, target_nid)) {\n\t\tpage_nid = target_nid;\n\t\tflags |= TNF_MIGRATED;\n\t} else {\n\t\tflags |= TNF_MIGRATE_FAIL;\n\t\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,\n\t\t\t\t\t       vmf->address, &vmf->ptl);\n\t\tif (unlikely(!vmf->pte))\n\t\t\tgoto out;\n\t\tif (unlikely(!pte_same(ptep_get(vmf->pte), vmf->orig_pte))) {\n\t\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\t\t\tgoto out;\n\t\t}\n\t\tgoto out_map;\n\t}\n\nout:\n\tif (page_nid != NUMA_NO_NODE)\n\t\ttask_numa_fault(last_cpupid, page_nid, 1, flags);\n\treturn 0;\nout_map:\n\t \n\told_pte = ptep_modify_prot_start(vma, vmf->address, vmf->pte);\n\tpte = pte_modify(old_pte, vma->vm_page_prot);\n\tpte = pte_mkyoung(pte);\n\tif (writable)\n\t\tpte = pte_mkwrite(pte, vma);\n\tptep_modify_prot_commit(vma, vmf->address, vmf->pte, old_pte, pte);\n\tupdate_mmu_cache_range(vmf, vma, vmf->address, vmf->pte, 1);\n\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\tgoto out;\n}\n\nstatic inline vm_fault_t create_huge_pmd(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tif (vma_is_anonymous(vma))\n\t\treturn do_huge_pmd_anonymous_page(vmf);\n\tif (vma->vm_ops->huge_fault)\n\t\treturn vma->vm_ops->huge_fault(vmf, PMD_ORDER);\n\treturn VM_FAULT_FALLBACK;\n}\n\n \nstatic inline vm_fault_t wp_huge_pmd(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tconst bool unshare = vmf->flags & FAULT_FLAG_UNSHARE;\n\tvm_fault_t ret;\n\n\tif (vma_is_anonymous(vma)) {\n\t\tif (likely(!unshare) &&\n\t\t    userfaultfd_huge_pmd_wp(vma, vmf->orig_pmd))\n\t\t\treturn handle_userfault(vmf, VM_UFFD_WP);\n\t\treturn do_huge_pmd_wp_page(vmf);\n\t}\n\n\tif (vma->vm_flags & (VM_SHARED | VM_MAYSHARE)) {\n\t\tif (vma->vm_ops->huge_fault) {\n\t\t\tret = vma->vm_ops->huge_fault(vmf, PMD_ORDER);\n\t\t\tif (!(ret & VM_FAULT_FALLBACK))\n\t\t\t\treturn ret;\n\t\t}\n\t}\n\n\t \n\t__split_huge_pmd(vma, vmf->pmd, vmf->address, false, NULL);\n\n\treturn VM_FAULT_FALLBACK;\n}\n\nstatic vm_fault_t create_huge_pud(struct vm_fault *vmf)\n{\n#if defined(CONFIG_TRANSPARENT_HUGEPAGE) &&\t\t\t\\\n\tdefined(CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD)\n\tstruct vm_area_struct *vma = vmf->vma;\n\t \n\tif (vma_is_anonymous(vma))\n\t\treturn VM_FAULT_FALLBACK;\n\tif (vma->vm_ops->huge_fault)\n\t\treturn vma->vm_ops->huge_fault(vmf, PUD_ORDER);\n#endif  \n\treturn VM_FAULT_FALLBACK;\n}\n\nstatic vm_fault_t wp_huge_pud(struct vm_fault *vmf, pud_t orig_pud)\n{\n#if defined(CONFIG_TRANSPARENT_HUGEPAGE) &&\t\t\t\\\n\tdefined(CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD)\n\tstruct vm_area_struct *vma = vmf->vma;\n\tvm_fault_t ret;\n\n\t \n\tif (vma_is_anonymous(vma))\n\t\tgoto split;\n\tif (vma->vm_flags & (VM_SHARED | VM_MAYSHARE)) {\n\t\tif (vma->vm_ops->huge_fault) {\n\t\t\tret = vma->vm_ops->huge_fault(vmf, PUD_ORDER);\n\t\t\tif (!(ret & VM_FAULT_FALLBACK))\n\t\t\t\treturn ret;\n\t\t}\n\t}\nsplit:\n\t \n\t__split_huge_pud(vma, vmf->pud, vmf->address);\n#endif  \n\treturn VM_FAULT_FALLBACK;\n}\n\n \nstatic vm_fault_t handle_pte_fault(struct vm_fault *vmf)\n{\n\tpte_t entry;\n\n\tif (unlikely(pmd_none(*vmf->pmd))) {\n\t\t \n\t\tvmf->pte = NULL;\n\t\tvmf->flags &= ~FAULT_FLAG_ORIG_PTE_VALID;\n\t} else {\n\t\t \n\t\tvmf->pte = pte_offset_map_nolock(vmf->vma->vm_mm, vmf->pmd,\n\t\t\t\t\t\t vmf->address, &vmf->ptl);\n\t\tif (unlikely(!vmf->pte))\n\t\t\treturn 0;\n\t\tvmf->orig_pte = ptep_get_lockless(vmf->pte);\n\t\tvmf->flags |= FAULT_FLAG_ORIG_PTE_VALID;\n\n\t\tif (pte_none(vmf->orig_pte)) {\n\t\t\tpte_unmap(vmf->pte);\n\t\t\tvmf->pte = NULL;\n\t\t}\n\t}\n\n\tif (!vmf->pte)\n\t\treturn do_pte_missing(vmf);\n\n\tif (!pte_present(vmf->orig_pte))\n\t\treturn do_swap_page(vmf);\n\n\tif (pte_protnone(vmf->orig_pte) && vma_is_accessible(vmf->vma))\n\t\treturn do_numa_page(vmf);\n\n\tspin_lock(vmf->ptl);\n\tentry = vmf->orig_pte;\n\tif (unlikely(!pte_same(ptep_get(vmf->pte), entry))) {\n\t\tupdate_mmu_tlb(vmf->vma, vmf->address, vmf->pte);\n\t\tgoto unlock;\n\t}\n\tif (vmf->flags & (FAULT_FLAG_WRITE|FAULT_FLAG_UNSHARE)) {\n\t\tif (!pte_write(entry))\n\t\t\treturn do_wp_page(vmf);\n\t\telse if (likely(vmf->flags & FAULT_FLAG_WRITE))\n\t\t\tentry = pte_mkdirty(entry);\n\t}\n\tentry = pte_mkyoung(entry);\n\tif (ptep_set_access_flags(vmf->vma, vmf->address, vmf->pte, entry,\n\t\t\t\tvmf->flags & FAULT_FLAG_WRITE)) {\n\t\tupdate_mmu_cache_range(vmf, vmf->vma, vmf->address,\n\t\t\t\tvmf->pte, 1);\n\t} else {\n\t\t \n\t\tif (vmf->flags & FAULT_FLAG_TRIED)\n\t\t\tgoto unlock;\n\t\t \n\t\tif (vmf->flags & FAULT_FLAG_WRITE)\n\t\t\tflush_tlb_fix_spurious_fault(vmf->vma, vmf->address,\n\t\t\t\t\t\t     vmf->pte);\n\t}\nunlock:\n\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\treturn 0;\n}\n\n \nstatic vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,\n\t\tunsigned long address, unsigned int flags)\n{\n\tstruct vm_fault vmf = {\n\t\t.vma = vma,\n\t\t.address = address & PAGE_MASK,\n\t\t.real_address = address,\n\t\t.flags = flags,\n\t\t.pgoff = linear_page_index(vma, address),\n\t\t.gfp_mask = __get_fault_gfp_mask(vma),\n\t};\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long vm_flags = vma->vm_flags;\n\tpgd_t *pgd;\n\tp4d_t *p4d;\n\tvm_fault_t ret;\n\n\tpgd = pgd_offset(mm, address);\n\tp4d = p4d_alloc(mm, pgd, address);\n\tif (!p4d)\n\t\treturn VM_FAULT_OOM;\n\n\tvmf.pud = pud_alloc(mm, p4d, address);\n\tif (!vmf.pud)\n\t\treturn VM_FAULT_OOM;\nretry_pud:\n\tif (pud_none(*vmf.pud) &&\n\t    hugepage_vma_check(vma, vm_flags, false, true, true)) {\n\t\tret = create_huge_pud(&vmf);\n\t\tif (!(ret & VM_FAULT_FALLBACK))\n\t\t\treturn ret;\n\t} else {\n\t\tpud_t orig_pud = *vmf.pud;\n\n\t\tbarrier();\n\t\tif (pud_trans_huge(orig_pud) || pud_devmap(orig_pud)) {\n\n\t\t\t \n\t\t\tif ((flags & FAULT_FLAG_WRITE) && !pud_write(orig_pud)) {\n\t\t\t\tret = wp_huge_pud(&vmf, orig_pud);\n\t\t\t\tif (!(ret & VM_FAULT_FALLBACK))\n\t\t\t\t\treturn ret;\n\t\t\t} else {\n\t\t\t\thuge_pud_set_accessed(&vmf, orig_pud);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\t}\n\n\tvmf.pmd = pmd_alloc(mm, vmf.pud, address);\n\tif (!vmf.pmd)\n\t\treturn VM_FAULT_OOM;\n\n\t \n\tif (pud_trans_unstable(vmf.pud))\n\t\tgoto retry_pud;\n\n\tif (pmd_none(*vmf.pmd) &&\n\t    hugepage_vma_check(vma, vm_flags, false, true, true)) {\n\t\tret = create_huge_pmd(&vmf);\n\t\tif (!(ret & VM_FAULT_FALLBACK))\n\t\t\treturn ret;\n\t} else {\n\t\tvmf.orig_pmd = pmdp_get_lockless(vmf.pmd);\n\n\t\tif (unlikely(is_swap_pmd(vmf.orig_pmd))) {\n\t\t\tVM_BUG_ON(thp_migration_supported() &&\n\t\t\t\t\t  !is_pmd_migration_entry(vmf.orig_pmd));\n\t\t\tif (is_pmd_migration_entry(vmf.orig_pmd))\n\t\t\t\tpmd_migration_entry_wait(mm, vmf.pmd);\n\t\t\treturn 0;\n\t\t}\n\t\tif (pmd_trans_huge(vmf.orig_pmd) || pmd_devmap(vmf.orig_pmd)) {\n\t\t\tif (pmd_protnone(vmf.orig_pmd) && vma_is_accessible(vma))\n\t\t\t\treturn do_huge_pmd_numa_page(&vmf);\n\n\t\t\tif ((flags & (FAULT_FLAG_WRITE|FAULT_FLAG_UNSHARE)) &&\n\t\t\t    !pmd_write(vmf.orig_pmd)) {\n\t\t\t\tret = wp_huge_pmd(&vmf);\n\t\t\t\tif (!(ret & VM_FAULT_FALLBACK))\n\t\t\t\t\treturn ret;\n\t\t\t} else {\n\t\t\t\thuge_pmd_set_accessed(&vmf);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn handle_pte_fault(&vmf);\n}\n\n \nstatic inline void mm_account_fault(struct mm_struct *mm, struct pt_regs *regs,\n\t\t\t\t    unsigned long address, unsigned int flags,\n\t\t\t\t    vm_fault_t ret)\n{\n\tbool major;\n\n\t \n\tif (ret & VM_FAULT_RETRY)\n\t\treturn;\n\n\t \n\tcount_vm_event(PGFAULT);\n\tcount_memcg_event_mm(mm, PGFAULT);\n\n\t \n\tif (ret & VM_FAULT_ERROR)\n\t\treturn;\n\n\t \n\tmajor = (ret & VM_FAULT_MAJOR) || (flags & FAULT_FLAG_TRIED);\n\n\tif (major)\n\t\tcurrent->maj_flt++;\n\telse\n\t\tcurrent->min_flt++;\n\n\t \n\tif (!regs)\n\t\treturn;\n\n\tif (major)\n\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, regs, address);\n\telse\n\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, regs, address);\n}\n\n#ifdef CONFIG_LRU_GEN\nstatic void lru_gen_enter_fault(struct vm_area_struct *vma)\n{\n\t \n\tcurrent->in_lru_fault = vma_has_recency(vma);\n}\n\nstatic void lru_gen_exit_fault(void)\n{\n\tcurrent->in_lru_fault = false;\n}\n#else\nstatic void lru_gen_enter_fault(struct vm_area_struct *vma)\n{\n}\n\nstatic void lru_gen_exit_fault(void)\n{\n}\n#endif  \n\nstatic vm_fault_t sanitize_fault_flags(struct vm_area_struct *vma,\n\t\t\t\t       unsigned int *flags)\n{\n\tif (unlikely(*flags & FAULT_FLAG_UNSHARE)) {\n\t\tif (WARN_ON_ONCE(*flags & FAULT_FLAG_WRITE))\n\t\t\treturn VM_FAULT_SIGSEGV;\n\t\t \n\t\tif (!is_cow_mapping(vma->vm_flags))\n\t\t\t*flags &= ~FAULT_FLAG_UNSHARE;\n\t} else if (*flags & FAULT_FLAG_WRITE) {\n\t\t \n\t\tif (WARN_ON_ONCE(!(vma->vm_flags & VM_MAYWRITE)))\n\t\t\treturn VM_FAULT_SIGSEGV;\n\t\t \n\t\tif (WARN_ON_ONCE(!(vma->vm_flags & VM_WRITE) &&\n\t\t\t\t !is_cow_mapping(vma->vm_flags)))\n\t\t\treturn VM_FAULT_SIGSEGV;\n\t}\n#ifdef CONFIG_PER_VMA_LOCK\n\t \n\tif (WARN_ON_ONCE((*flags &\n\t\t\t(FAULT_FLAG_VMA_LOCK | FAULT_FLAG_RETRY_NOWAIT)) ==\n\t\t\t(FAULT_FLAG_VMA_LOCK | FAULT_FLAG_RETRY_NOWAIT)))\n\t\treturn VM_FAULT_SIGSEGV;\n#endif\n\n\treturn 0;\n}\n\n \nvm_fault_t handle_mm_fault(struct vm_area_struct *vma, unsigned long address,\n\t\t\t   unsigned int flags, struct pt_regs *regs)\n{\n\t \n\tstruct mm_struct *mm = vma->vm_mm;\n\tvm_fault_t ret;\n\n\t__set_current_state(TASK_RUNNING);\n\n\tret = sanitize_fault_flags(vma, &flags);\n\tif (ret)\n\t\tgoto out;\n\n\tif (!arch_vma_access_permitted(vma, flags & FAULT_FLAG_WRITE,\n\t\t\t\t\t    flags & FAULT_FLAG_INSTRUCTION,\n\t\t\t\t\t    flags & FAULT_FLAG_REMOTE)) {\n\t\tret = VM_FAULT_SIGSEGV;\n\t\tgoto out;\n\t}\n\n\t \n\tif (flags & FAULT_FLAG_USER)\n\t\tmem_cgroup_enter_user_fault();\n\n\tlru_gen_enter_fault(vma);\n\n\tif (unlikely(is_vm_hugetlb_page(vma)))\n\t\tret = hugetlb_fault(vma->vm_mm, vma, address, flags);\n\telse\n\t\tret = __handle_mm_fault(vma, address, flags);\n\n\tlru_gen_exit_fault();\n\n\tif (flags & FAULT_FLAG_USER) {\n\t\tmem_cgroup_exit_user_fault();\n\t\t \n\t\tif (task_in_memcg_oom(current) && !(ret & VM_FAULT_OOM))\n\t\t\tmem_cgroup_oom_synchronize(false);\n\t}\nout:\n\tmm_account_fault(mm, regs, address, flags, ret);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(handle_mm_fault);\n\n#ifdef CONFIG_LOCK_MM_AND_FIND_VMA\n#include <linux/extable.h>\n\nstatic inline bool get_mmap_lock_carefully(struct mm_struct *mm, struct pt_regs *regs)\n{\n\tif (likely(mmap_read_trylock(mm)))\n\t\treturn true;\n\n\tif (regs && !user_mode(regs)) {\n\t\tunsigned long ip = instruction_pointer(regs);\n\t\tif (!search_exception_tables(ip))\n\t\t\treturn false;\n\t}\n\n\treturn !mmap_read_lock_killable(mm);\n}\n\nstatic inline bool mmap_upgrade_trylock(struct mm_struct *mm)\n{\n\t \n\treturn false;\n}\n\nstatic inline bool upgrade_mmap_lock_carefully(struct mm_struct *mm, struct pt_regs *regs)\n{\n\tmmap_read_unlock(mm);\n\tif (regs && !user_mode(regs)) {\n\t\tunsigned long ip = instruction_pointer(regs);\n\t\tif (!search_exception_tables(ip))\n\t\t\treturn false;\n\t}\n\treturn !mmap_write_lock_killable(mm);\n}\n\n \nstruct vm_area_struct *lock_mm_and_find_vma(struct mm_struct *mm,\n\t\t\tunsigned long addr, struct pt_regs *regs)\n{\n\tstruct vm_area_struct *vma;\n\n\tif (!get_mmap_lock_carefully(mm, regs))\n\t\treturn NULL;\n\n\tvma = find_vma(mm, addr);\n\tif (likely(vma && (vma->vm_start <= addr)))\n\t\treturn vma;\n\n\t \n\tif (!vma || !(vma->vm_flags & VM_GROWSDOWN)) {\n\t\tmmap_read_unlock(mm);\n\t\treturn NULL;\n\t}\n\n\t \n\tif (!mmap_upgrade_trylock(mm)) {\n\t\tif (!upgrade_mmap_lock_carefully(mm, regs))\n\t\t\treturn NULL;\n\n\t\tvma = find_vma(mm, addr);\n\t\tif (!vma)\n\t\t\tgoto fail;\n\t\tif (vma->vm_start <= addr)\n\t\t\tgoto success;\n\t\tif (!(vma->vm_flags & VM_GROWSDOWN))\n\t\t\tgoto fail;\n\t}\n\n\tif (expand_stack_locked(vma, addr))\n\t\tgoto fail;\n\nsuccess:\n\tmmap_write_downgrade(mm);\n\treturn vma;\n\nfail:\n\tmmap_write_unlock(mm);\n\treturn NULL;\n}\n#endif\n\n#ifdef CONFIG_PER_VMA_LOCK\n \nstruct vm_area_struct *lock_vma_under_rcu(struct mm_struct *mm,\n\t\t\t\t\t  unsigned long address)\n{\n\tMA_STATE(mas, &mm->mm_mt, address, address);\n\tstruct vm_area_struct *vma;\n\n\trcu_read_lock();\nretry:\n\tvma = mas_walk(&mas);\n\tif (!vma)\n\t\tgoto inval;\n\n\tif (!vma_start_read(vma))\n\t\tgoto inval;\n\n\t \n\tif (unlikely(vma_is_anonymous(vma) && !vma->anon_vma))\n\t\tgoto inval_end_read;\n\n\t \n\tif (unlikely(address < vma->vm_start || address >= vma->vm_end))\n\t\tgoto inval_end_read;\n\n\t \n\tif (vma->detached) {\n\t\tvma_end_read(vma);\n\t\tcount_vm_vma_lock_event(VMA_LOCK_MISS);\n\t\t \n\t\tgoto retry;\n\t}\n\n\trcu_read_unlock();\n\treturn vma;\n\ninval_end_read:\n\tvma_end_read(vma);\ninval:\n\trcu_read_unlock();\n\tcount_vm_vma_lock_event(VMA_LOCK_ABORT);\n\treturn NULL;\n}\n#endif  \n\n#ifndef __PAGETABLE_P4D_FOLDED\n \nint __p4d_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address)\n{\n\tp4d_t *new = p4d_alloc_one(mm, address);\n\tif (!new)\n\t\treturn -ENOMEM;\n\n\tspin_lock(&mm->page_table_lock);\n\tif (pgd_present(*pgd)) {\t \n\t\tp4d_free(mm, new);\n\t} else {\n\t\tsmp_wmb();  \n\t\tpgd_populate(mm, pgd, new);\n\t}\n\tspin_unlock(&mm->page_table_lock);\n\treturn 0;\n}\n#endif  \n\n#ifndef __PAGETABLE_PUD_FOLDED\n \nint __pud_alloc(struct mm_struct *mm, p4d_t *p4d, unsigned long address)\n{\n\tpud_t *new = pud_alloc_one(mm, address);\n\tif (!new)\n\t\treturn -ENOMEM;\n\n\tspin_lock(&mm->page_table_lock);\n\tif (!p4d_present(*p4d)) {\n\t\tmm_inc_nr_puds(mm);\n\t\tsmp_wmb();  \n\t\tp4d_populate(mm, p4d, new);\n\t} else\t \n\t\tpud_free(mm, new);\n\tspin_unlock(&mm->page_table_lock);\n\treturn 0;\n}\n#endif  \n\n#ifndef __PAGETABLE_PMD_FOLDED\n \nint __pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)\n{\n\tspinlock_t *ptl;\n\tpmd_t *new = pmd_alloc_one(mm, address);\n\tif (!new)\n\t\treturn -ENOMEM;\n\n\tptl = pud_lock(mm, pud);\n\tif (!pud_present(*pud)) {\n\t\tmm_inc_nr_pmds(mm);\n\t\tsmp_wmb();  \n\t\tpud_populate(mm, pud, new);\n\t} else {\t \n\t\tpmd_free(mm, new);\n\t}\n\tspin_unlock(ptl);\n\treturn 0;\n}\n#endif  \n\n \nint follow_pte(struct mm_struct *mm, unsigned long address,\n\t       pte_t **ptepp, spinlock_t **ptlp)\n{\n\tpgd_t *pgd;\n\tp4d_t *p4d;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\tpte_t *ptep;\n\n\tpgd = pgd_offset(mm, address);\n\tif (pgd_none(*pgd) || unlikely(pgd_bad(*pgd)))\n\t\tgoto out;\n\n\tp4d = p4d_offset(pgd, address);\n\tif (p4d_none(*p4d) || unlikely(p4d_bad(*p4d)))\n\t\tgoto out;\n\n\tpud = pud_offset(p4d, address);\n\tif (pud_none(*pud) || unlikely(pud_bad(*pud)))\n\t\tgoto out;\n\n\tpmd = pmd_offset(pud, address);\n\tVM_BUG_ON(pmd_trans_huge(*pmd));\n\n\tptep = pte_offset_map_lock(mm, pmd, address, ptlp);\n\tif (!ptep)\n\t\tgoto out;\n\tif (!pte_present(ptep_get(ptep)))\n\t\tgoto unlock;\n\t*ptepp = ptep;\n\treturn 0;\nunlock:\n\tpte_unmap_unlock(ptep, *ptlp);\nout:\n\treturn -EINVAL;\n}\nEXPORT_SYMBOL_GPL(follow_pte);\n\n \nint follow_pfn(struct vm_area_struct *vma, unsigned long address,\n\tunsigned long *pfn)\n{\n\tint ret = -EINVAL;\n\tspinlock_t *ptl;\n\tpte_t *ptep;\n\n\tif (!(vma->vm_flags & (VM_IO | VM_PFNMAP)))\n\t\treturn ret;\n\n\tret = follow_pte(vma->vm_mm, address, &ptep, &ptl);\n\tif (ret)\n\t\treturn ret;\n\t*pfn = pte_pfn(ptep_get(ptep));\n\tpte_unmap_unlock(ptep, ptl);\n\treturn 0;\n}\nEXPORT_SYMBOL(follow_pfn);\n\n#ifdef CONFIG_HAVE_IOREMAP_PROT\nint follow_phys(struct vm_area_struct *vma,\n\t\tunsigned long address, unsigned int flags,\n\t\tunsigned long *prot, resource_size_t *phys)\n{\n\tint ret = -EINVAL;\n\tpte_t *ptep, pte;\n\tspinlock_t *ptl;\n\n\tif (!(vma->vm_flags & (VM_IO | VM_PFNMAP)))\n\t\tgoto out;\n\n\tif (follow_pte(vma->vm_mm, address, &ptep, &ptl))\n\t\tgoto out;\n\tpte = ptep_get(ptep);\n\n\tif ((flags & FOLL_WRITE) && !pte_write(pte))\n\t\tgoto unlock;\n\n\t*prot = pgprot_val(pte_pgprot(pte));\n\t*phys = (resource_size_t)pte_pfn(pte) << PAGE_SHIFT;\n\n\tret = 0;\nunlock:\n\tpte_unmap_unlock(ptep, ptl);\nout:\n\treturn ret;\n}\n\n \nint generic_access_phys(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tvoid *buf, int len, int write)\n{\n\tresource_size_t phys_addr;\n\tunsigned long prot = 0;\n\tvoid __iomem *maddr;\n\tpte_t *ptep, pte;\n\tspinlock_t *ptl;\n\tint offset = offset_in_page(addr);\n\tint ret = -EINVAL;\n\n\tif (!(vma->vm_flags & (VM_IO | VM_PFNMAP)))\n\t\treturn -EINVAL;\n\nretry:\n\tif (follow_pte(vma->vm_mm, addr, &ptep, &ptl))\n\t\treturn -EINVAL;\n\tpte = ptep_get(ptep);\n\tpte_unmap_unlock(ptep, ptl);\n\n\tprot = pgprot_val(pte_pgprot(pte));\n\tphys_addr = (resource_size_t)pte_pfn(pte) << PAGE_SHIFT;\n\n\tif ((write & FOLL_WRITE) && !pte_write(pte))\n\t\treturn -EINVAL;\n\n\tmaddr = ioremap_prot(phys_addr, PAGE_ALIGN(len + offset), prot);\n\tif (!maddr)\n\t\treturn -ENOMEM;\n\n\tif (follow_pte(vma->vm_mm, addr, &ptep, &ptl))\n\t\tgoto out_unmap;\n\n\tif (!pte_same(pte, ptep_get(ptep))) {\n\t\tpte_unmap_unlock(ptep, ptl);\n\t\tiounmap(maddr);\n\n\t\tgoto retry;\n\t}\n\n\tif (write)\n\t\tmemcpy_toio(maddr + offset, buf, len);\n\telse\n\t\tmemcpy_fromio(buf, maddr + offset, len);\n\tret = len;\n\tpte_unmap_unlock(ptep, ptl);\nout_unmap:\n\tiounmap(maddr);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(generic_access_phys);\n#endif\n\n \nint __access_remote_vm(struct mm_struct *mm, unsigned long addr, void *buf,\n\t\t       int len, unsigned int gup_flags)\n{\n\tvoid *old_buf = buf;\n\tint write = gup_flags & FOLL_WRITE;\n\n\tif (mmap_read_lock_killable(mm))\n\t\treturn 0;\n\n\t \n\taddr = untagged_addr_remote(mm, addr);\n\n\t \n\tif (!vma_lookup(mm, addr) && !expand_stack(mm, addr))\n\t\treturn 0;\n\n\t \n\twhile (len) {\n\t\tint bytes, offset;\n\t\tvoid *maddr;\n\t\tstruct vm_area_struct *vma = NULL;\n\t\tstruct page *page = get_user_page_vma_remote(mm, addr,\n\t\t\t\t\t\t\t     gup_flags, &vma);\n\n\t\tif (IS_ERR_OR_NULL(page)) {\n\t\t\t \n\t\t\tvma = vma_lookup(mm, addr);\n\t\t\tif (!vma) {\n\t\t\t\tvma = expand_stack(mm, addr);\n\n\t\t\t\t \n\t\t\t\tif (!vma)\n\t\t\t\t\treturn buf - old_buf;\n\n\t\t\t\t \n\t\t\t\tcontinue;\n\t\t\t}\n\n\n\t\t\t \n\t\t\tbytes = 0;\n#ifdef CONFIG_HAVE_IOREMAP_PROT\n\t\t\tif (vma->vm_ops && vma->vm_ops->access)\n\t\t\t\tbytes = vma->vm_ops->access(vma, addr, buf,\n\t\t\t\t\t\t\t    len, write);\n#endif\n\t\t\tif (bytes <= 0)\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tbytes = len;\n\t\t\toffset = addr & (PAGE_SIZE-1);\n\t\t\tif (bytes > PAGE_SIZE-offset)\n\t\t\t\tbytes = PAGE_SIZE-offset;\n\n\t\t\tmaddr = kmap(page);\n\t\t\tif (write) {\n\t\t\t\tcopy_to_user_page(vma, page, addr,\n\t\t\t\t\t\t  maddr + offset, buf, bytes);\n\t\t\t\tset_page_dirty_lock(page);\n\t\t\t} else {\n\t\t\t\tcopy_from_user_page(vma, page, addr,\n\t\t\t\t\t\t    buf, maddr + offset, bytes);\n\t\t\t}\n\t\t\tkunmap(page);\n\t\t\tput_page(page);\n\t\t}\n\t\tlen -= bytes;\n\t\tbuf += bytes;\n\t\taddr += bytes;\n\t}\n\tmmap_read_unlock(mm);\n\n\treturn buf - old_buf;\n}\n\n \nint access_remote_vm(struct mm_struct *mm, unsigned long addr,\n\t\tvoid *buf, int len, unsigned int gup_flags)\n{\n\treturn __access_remote_vm(mm, addr, buf, len, gup_flags);\n}\n\n \nint access_process_vm(struct task_struct *tsk, unsigned long addr,\n\t\tvoid *buf, int len, unsigned int gup_flags)\n{\n\tstruct mm_struct *mm;\n\tint ret;\n\n\tmm = get_task_mm(tsk);\n\tif (!mm)\n\t\treturn 0;\n\n\tret = __access_remote_vm(mm, addr, buf, len, gup_flags);\n\n\tmmput(mm);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(access_process_vm);\n\n \nvoid print_vma_addr(char *prefix, unsigned long ip)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\n\t \n\tif (!mmap_read_trylock(mm))\n\t\treturn;\n\n\tvma = find_vma(mm, ip);\n\tif (vma && vma->vm_file) {\n\t\tstruct file *f = vma->vm_file;\n\t\tchar *buf = (char *)__get_free_page(GFP_NOWAIT);\n\t\tif (buf) {\n\t\t\tchar *p;\n\n\t\t\tp = file_path(f, buf, PAGE_SIZE);\n\t\t\tif (IS_ERR(p))\n\t\t\t\tp = \"?\";\n\t\t\tprintk(\"%s%s[%lx+%lx]\", prefix, kbasename(p),\n\t\t\t\t\tvma->vm_start,\n\t\t\t\t\tvma->vm_end - vma->vm_start);\n\t\t\tfree_page((unsigned long)buf);\n\t\t}\n\t}\n\tmmap_read_unlock(mm);\n}\n\n#if defined(CONFIG_PROVE_LOCKING) || defined(CONFIG_DEBUG_ATOMIC_SLEEP)\nvoid __might_fault(const char *file, int line)\n{\n\tif (pagefault_disabled())\n\t\treturn;\n\t__might_sleep(file, line);\n#if defined(CONFIG_DEBUG_ATOMIC_SLEEP)\n\tif (current->mm)\n\t\tmight_lock_read(&current->mm->mmap_lock);\n#endif\n}\nEXPORT_SYMBOL(__might_fault);\n#endif\n\n#if defined(CONFIG_TRANSPARENT_HUGEPAGE) || defined(CONFIG_HUGETLBFS)\n \nstatic inline int process_huge_page(\n\tunsigned long addr_hint, unsigned int pages_per_huge_page,\n\tint (*process_subpage)(unsigned long addr, int idx, void *arg),\n\tvoid *arg)\n{\n\tint i, n, base, l, ret;\n\tunsigned long addr = addr_hint &\n\t\t~(((unsigned long)pages_per_huge_page << PAGE_SHIFT) - 1);\n\n\t \n\tmight_sleep();\n\tn = (addr_hint - addr) / PAGE_SIZE;\n\tif (2 * n <= pages_per_huge_page) {\n\t\t \n\t\tbase = 0;\n\t\tl = n;\n\t\t \n\t\tfor (i = pages_per_huge_page - 1; i >= 2 * n; i--) {\n\t\t\tcond_resched();\n\t\t\tret = process_subpage(addr + i * PAGE_SIZE, i, arg);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t} else {\n\t\t \n\t\tbase = pages_per_huge_page - 2 * (pages_per_huge_page - n);\n\t\tl = pages_per_huge_page - n;\n\t\t \n\t\tfor (i = 0; i < base; i++) {\n\t\t\tcond_resched();\n\t\t\tret = process_subpage(addr + i * PAGE_SIZE, i, arg);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t}\n\t \n\tfor (i = 0; i < l; i++) {\n\t\tint left_idx = base + i;\n\t\tint right_idx = base + 2 * l - 1 - i;\n\n\t\tcond_resched();\n\t\tret = process_subpage(addr + left_idx * PAGE_SIZE, left_idx, arg);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tcond_resched();\n\t\tret = process_subpage(addr + right_idx * PAGE_SIZE, right_idx, arg);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\treturn 0;\n}\n\nstatic void clear_gigantic_page(struct page *page,\n\t\t\t\tunsigned long addr,\n\t\t\t\tunsigned int pages_per_huge_page)\n{\n\tint i;\n\tstruct page *p;\n\n\tmight_sleep();\n\tfor (i = 0; i < pages_per_huge_page; i++) {\n\t\tp = nth_page(page, i);\n\t\tcond_resched();\n\t\tclear_user_highpage(p, addr + i * PAGE_SIZE);\n\t}\n}\n\nstatic int clear_subpage(unsigned long addr, int idx, void *arg)\n{\n\tstruct page *page = arg;\n\n\tclear_user_highpage(page + idx, addr);\n\treturn 0;\n}\n\nvoid clear_huge_page(struct page *page,\n\t\t     unsigned long addr_hint, unsigned int pages_per_huge_page)\n{\n\tunsigned long addr = addr_hint &\n\t\t~(((unsigned long)pages_per_huge_page << PAGE_SHIFT) - 1);\n\n\tif (unlikely(pages_per_huge_page > MAX_ORDER_NR_PAGES)) {\n\t\tclear_gigantic_page(page, addr, pages_per_huge_page);\n\t\treturn;\n\t}\n\n\tprocess_huge_page(addr_hint, pages_per_huge_page, clear_subpage, page);\n}\n\nstatic int copy_user_gigantic_page(struct folio *dst, struct folio *src,\n\t\t\t\t     unsigned long addr,\n\t\t\t\t     struct vm_area_struct *vma,\n\t\t\t\t     unsigned int pages_per_huge_page)\n{\n\tint i;\n\tstruct page *dst_page;\n\tstruct page *src_page;\n\n\tfor (i = 0; i < pages_per_huge_page; i++) {\n\t\tdst_page = folio_page(dst, i);\n\t\tsrc_page = folio_page(src, i);\n\n\t\tcond_resched();\n\t\tif (copy_mc_user_highpage(dst_page, src_page,\n\t\t\t\t\t  addr + i*PAGE_SIZE, vma)) {\n\t\t\tmemory_failure_queue(page_to_pfn(src_page), 0);\n\t\t\treturn -EHWPOISON;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstruct copy_subpage_arg {\n\tstruct page *dst;\n\tstruct page *src;\n\tstruct vm_area_struct *vma;\n};\n\nstatic int copy_subpage(unsigned long addr, int idx, void *arg)\n{\n\tstruct copy_subpage_arg *copy_arg = arg;\n\n\tif (copy_mc_user_highpage(copy_arg->dst + idx, copy_arg->src + idx,\n\t\t\t\t  addr, copy_arg->vma)) {\n\t\tmemory_failure_queue(page_to_pfn(copy_arg->src + idx), 0);\n\t\treturn -EHWPOISON;\n\t}\n\treturn 0;\n}\n\nint copy_user_large_folio(struct folio *dst, struct folio *src,\n\t\t\t  unsigned long addr_hint, struct vm_area_struct *vma)\n{\n\tunsigned int pages_per_huge_page = folio_nr_pages(dst);\n\tunsigned long addr = addr_hint &\n\t\t~(((unsigned long)pages_per_huge_page << PAGE_SHIFT) - 1);\n\tstruct copy_subpage_arg arg = {\n\t\t.dst = &dst->page,\n\t\t.src = &src->page,\n\t\t.vma = vma,\n\t};\n\n\tif (unlikely(pages_per_huge_page > MAX_ORDER_NR_PAGES))\n\t\treturn copy_user_gigantic_page(dst, src, addr, vma,\n\t\t\t\t\t       pages_per_huge_page);\n\n\treturn process_huge_page(addr_hint, pages_per_huge_page, copy_subpage, &arg);\n}\n\nlong copy_folio_from_user(struct folio *dst_folio,\n\t\t\t   const void __user *usr_src,\n\t\t\t   bool allow_pagefault)\n{\n\tvoid *kaddr;\n\tunsigned long i, rc = 0;\n\tunsigned int nr_pages = folio_nr_pages(dst_folio);\n\tunsigned long ret_val = nr_pages * PAGE_SIZE;\n\tstruct page *subpage;\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tsubpage = folio_page(dst_folio, i);\n\t\tkaddr = kmap_local_page(subpage);\n\t\tif (!allow_pagefault)\n\t\t\tpagefault_disable();\n\t\trc = copy_from_user(kaddr, usr_src + i * PAGE_SIZE, PAGE_SIZE);\n\t\tif (!allow_pagefault)\n\t\t\tpagefault_enable();\n\t\tkunmap_local(kaddr);\n\n\t\tret_val -= (PAGE_SIZE - rc);\n\t\tif (rc)\n\t\t\tbreak;\n\n\t\tflush_dcache_page(subpage);\n\n\t\tcond_resched();\n\t}\n\treturn ret_val;\n}\n#endif  \n\n#if USE_SPLIT_PTE_PTLOCKS && ALLOC_SPLIT_PTLOCKS\n\nstatic struct kmem_cache *page_ptl_cachep;\n\nvoid __init ptlock_cache_init(void)\n{\n\tpage_ptl_cachep = kmem_cache_create(\"page->ptl\", sizeof(spinlock_t), 0,\n\t\t\tSLAB_PANIC, NULL);\n}\n\nbool ptlock_alloc(struct ptdesc *ptdesc)\n{\n\tspinlock_t *ptl;\n\n\tptl = kmem_cache_alloc(page_ptl_cachep, GFP_KERNEL);\n\tif (!ptl)\n\t\treturn false;\n\tptdesc->ptl = ptl;\n\treturn true;\n}\n\nvoid ptlock_free(struct ptdesc *ptdesc)\n{\n\tkmem_cache_free(page_ptl_cachep, ptdesc->ptl);\n}\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}