{
  "module_name": "memblock.c",
  "hash_id": "b9d4392b9ee8def056af7d317b0e7b776996b1af953670775a0ee2cd0ccc0be2",
  "original_prompt": "Ingested from linux-6.6.14/mm/memblock.c",
  "human_readable_source": "\n \n\n#include <linux/kernel.h>\n#include <linux/slab.h>\n#include <linux/init.h>\n#include <linux/bitops.h>\n#include <linux/poison.h>\n#include <linux/pfn.h>\n#include <linux/debugfs.h>\n#include <linux/kmemleak.h>\n#include <linux/seq_file.h>\n#include <linux/memblock.h>\n\n#include <asm/sections.h>\n#include <linux/io.h>\n\n#include \"internal.h\"\n\n#define INIT_MEMBLOCK_REGIONS\t\t\t128\n#define INIT_PHYSMEM_REGIONS\t\t\t4\n\n#ifndef INIT_MEMBLOCK_RESERVED_REGIONS\n# define INIT_MEMBLOCK_RESERVED_REGIONS\t\tINIT_MEMBLOCK_REGIONS\n#endif\n\n#ifndef INIT_MEMBLOCK_MEMORY_REGIONS\n#define INIT_MEMBLOCK_MEMORY_REGIONS\t\tINIT_MEMBLOCK_REGIONS\n#endif\n\n \n\n#ifndef CONFIG_NUMA\nstruct pglist_data __refdata contig_page_data;\nEXPORT_SYMBOL(contig_page_data);\n#endif\n\nunsigned long max_low_pfn;\nunsigned long min_low_pfn;\nunsigned long max_pfn;\nunsigned long long max_possible_pfn;\n\nstatic struct memblock_region memblock_memory_init_regions[INIT_MEMBLOCK_MEMORY_REGIONS] __initdata_memblock;\nstatic struct memblock_region memblock_reserved_init_regions[INIT_MEMBLOCK_RESERVED_REGIONS] __initdata_memblock;\n#ifdef CONFIG_HAVE_MEMBLOCK_PHYS_MAP\nstatic struct memblock_region memblock_physmem_init_regions[INIT_PHYSMEM_REGIONS];\n#endif\n\nstruct memblock memblock __initdata_memblock = {\n\t.memory.regions\t\t= memblock_memory_init_regions,\n\t.memory.cnt\t\t= 1,\t \n\t.memory.max\t\t= INIT_MEMBLOCK_MEMORY_REGIONS,\n\t.memory.name\t\t= \"memory\",\n\n\t.reserved.regions\t= memblock_reserved_init_regions,\n\t.reserved.cnt\t\t= 1,\t \n\t.reserved.max\t\t= INIT_MEMBLOCK_RESERVED_REGIONS,\n\t.reserved.name\t\t= \"reserved\",\n\n\t.bottom_up\t\t= false,\n\t.current_limit\t\t= MEMBLOCK_ALLOC_ANYWHERE,\n};\n\n#ifdef CONFIG_HAVE_MEMBLOCK_PHYS_MAP\nstruct memblock_type physmem = {\n\t.regions\t\t= memblock_physmem_init_regions,\n\t.cnt\t\t\t= 1,\t \n\t.max\t\t\t= INIT_PHYSMEM_REGIONS,\n\t.name\t\t\t= \"physmem\",\n};\n#endif\n\n \nstatic __refdata struct memblock_type *memblock_memory = &memblock.memory;\n\n#define for_each_memblock_type(i, memblock_type, rgn)\t\t\t\\\n\tfor (i = 0, rgn = &memblock_type->regions[0];\t\t\t\\\n\t     i < memblock_type->cnt;\t\t\t\t\t\\\n\t     i++, rgn = &memblock_type->regions[i])\n\n#define memblock_dbg(fmt, ...)\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (memblock_debug)\t\t\t\t\t\\\n\t\t\tpr_info(fmt, ##__VA_ARGS__);\t\t\t\\\n\t} while (0)\n\nstatic int memblock_debug __initdata_memblock;\nstatic bool system_has_some_mirror __initdata_memblock;\nstatic int memblock_can_resize __initdata_memblock;\nstatic int memblock_memory_in_slab __initdata_memblock;\nstatic int memblock_reserved_in_slab __initdata_memblock;\n\nbool __init_memblock memblock_has_mirror(void)\n{\n\treturn system_has_some_mirror;\n}\n\nstatic enum memblock_flags __init_memblock choose_memblock_flags(void)\n{\n\treturn system_has_some_mirror ? MEMBLOCK_MIRROR : MEMBLOCK_NONE;\n}\n\n \nstatic inline phys_addr_t memblock_cap_size(phys_addr_t base, phys_addr_t *size)\n{\n\treturn *size = min(*size, PHYS_ADDR_MAX - base);\n}\n\n \nstatic unsigned long __init_memblock memblock_addrs_overlap(phys_addr_t base1, phys_addr_t size1,\n\t\t\t\t       phys_addr_t base2, phys_addr_t size2)\n{\n\treturn ((base1 < (base2 + size2)) && (base2 < (base1 + size1)));\n}\n\nbool __init_memblock memblock_overlaps_region(struct memblock_type *type,\n\t\t\t\t\tphys_addr_t base, phys_addr_t size)\n{\n\tunsigned long i;\n\n\tmemblock_cap_size(base, &size);\n\n\tfor (i = 0; i < type->cnt; i++)\n\t\tif (memblock_addrs_overlap(base, size, type->regions[i].base,\n\t\t\t\t\t   type->regions[i].size))\n\t\t\tbreak;\n\treturn i < type->cnt;\n}\n\n \nstatic phys_addr_t __init_memblock\n__memblock_find_range_bottom_up(phys_addr_t start, phys_addr_t end,\n\t\t\t\tphys_addr_t size, phys_addr_t align, int nid,\n\t\t\t\tenum memblock_flags flags)\n{\n\tphys_addr_t this_start, this_end, cand;\n\tu64 i;\n\n\tfor_each_free_mem_range(i, nid, flags, &this_start, &this_end, NULL) {\n\t\tthis_start = clamp(this_start, start, end);\n\t\tthis_end = clamp(this_end, start, end);\n\n\t\tcand = round_up(this_start, align);\n\t\tif (cand < this_end && this_end - cand >= size)\n\t\t\treturn cand;\n\t}\n\n\treturn 0;\n}\n\n \nstatic phys_addr_t __init_memblock\n__memblock_find_range_top_down(phys_addr_t start, phys_addr_t end,\n\t\t\t       phys_addr_t size, phys_addr_t align, int nid,\n\t\t\t       enum memblock_flags flags)\n{\n\tphys_addr_t this_start, this_end, cand;\n\tu64 i;\n\n\tfor_each_free_mem_range_reverse(i, nid, flags, &this_start, &this_end,\n\t\t\t\t\tNULL) {\n\t\tthis_start = clamp(this_start, start, end);\n\t\tthis_end = clamp(this_end, start, end);\n\n\t\tif (this_end < size)\n\t\t\tcontinue;\n\n\t\tcand = round_down(this_end - size, align);\n\t\tif (cand >= this_start)\n\t\t\treturn cand;\n\t}\n\n\treturn 0;\n}\n\n \nstatic phys_addr_t __init_memblock memblock_find_in_range_node(phys_addr_t size,\n\t\t\t\t\tphys_addr_t align, phys_addr_t start,\n\t\t\t\t\tphys_addr_t end, int nid,\n\t\t\t\t\tenum memblock_flags flags)\n{\n\t \n\tif (end == MEMBLOCK_ALLOC_ACCESSIBLE ||\n\t    end == MEMBLOCK_ALLOC_NOLEAKTRACE)\n\t\tend = memblock.current_limit;\n\n\t \n\tstart = max_t(phys_addr_t, start, PAGE_SIZE);\n\tend = max(start, end);\n\n\tif (memblock_bottom_up())\n\t\treturn __memblock_find_range_bottom_up(start, end, size, align,\n\t\t\t\t\t\t       nid, flags);\n\telse\n\t\treturn __memblock_find_range_top_down(start, end, size, align,\n\t\t\t\t\t\t      nid, flags);\n}\n\n \nstatic phys_addr_t __init_memblock memblock_find_in_range(phys_addr_t start,\n\t\t\t\t\tphys_addr_t end, phys_addr_t size,\n\t\t\t\t\tphys_addr_t align)\n{\n\tphys_addr_t ret;\n\tenum memblock_flags flags = choose_memblock_flags();\n\nagain:\n\tret = memblock_find_in_range_node(size, align, start, end,\n\t\t\t\t\t    NUMA_NO_NODE, flags);\n\n\tif (!ret && (flags & MEMBLOCK_MIRROR)) {\n\t\tpr_warn_ratelimited(\"Could not allocate %pap bytes of mirrored memory\\n\",\n\t\t\t&size);\n\t\tflags &= ~MEMBLOCK_MIRROR;\n\t\tgoto again;\n\t}\n\n\treturn ret;\n}\n\nstatic void __init_memblock memblock_remove_region(struct memblock_type *type, unsigned long r)\n{\n\ttype->total_size -= type->regions[r].size;\n\tmemmove(&type->regions[r], &type->regions[r + 1],\n\t\t(type->cnt - (r + 1)) * sizeof(type->regions[r]));\n\ttype->cnt--;\n\n\t \n\tif (type->cnt == 0) {\n\t\tWARN_ON(type->total_size != 0);\n\t\ttype->cnt = 1;\n\t\ttype->regions[0].base = 0;\n\t\ttype->regions[0].size = 0;\n\t\ttype->regions[0].flags = 0;\n\t\tmemblock_set_region_node(&type->regions[0], MAX_NUMNODES);\n\t}\n}\n\n#ifndef CONFIG_ARCH_KEEP_MEMBLOCK\n \nvoid __init memblock_discard(void)\n{\n\tphys_addr_t addr, size;\n\n\tif (memblock.reserved.regions != memblock_reserved_init_regions) {\n\t\taddr = __pa(memblock.reserved.regions);\n\t\tsize = PAGE_ALIGN(sizeof(struct memblock_region) *\n\t\t\t\t  memblock.reserved.max);\n\t\tif (memblock_reserved_in_slab)\n\t\t\tkfree(memblock.reserved.regions);\n\t\telse\n\t\t\tmemblock_free_late(addr, size);\n\t}\n\n\tif (memblock.memory.regions != memblock_memory_init_regions) {\n\t\taddr = __pa(memblock.memory.regions);\n\t\tsize = PAGE_ALIGN(sizeof(struct memblock_region) *\n\t\t\t\t  memblock.memory.max);\n\t\tif (memblock_memory_in_slab)\n\t\t\tkfree(memblock.memory.regions);\n\t\telse\n\t\t\tmemblock_free_late(addr, size);\n\t}\n\n\tmemblock_memory = NULL;\n}\n#endif\n\n \nstatic int __init_memblock memblock_double_array(struct memblock_type *type,\n\t\t\t\t\t\tphys_addr_t new_area_start,\n\t\t\t\t\t\tphys_addr_t new_area_size)\n{\n\tstruct memblock_region *new_array, *old_array;\n\tphys_addr_t old_alloc_size, new_alloc_size;\n\tphys_addr_t old_size, new_size, addr, new_end;\n\tint use_slab = slab_is_available();\n\tint *in_slab;\n\n\t \n\tif (!memblock_can_resize)\n\t\treturn -1;\n\n\t \n\told_size = type->max * sizeof(struct memblock_region);\n\tnew_size = old_size << 1;\n\t \n\told_alloc_size = PAGE_ALIGN(old_size);\n\tnew_alloc_size = PAGE_ALIGN(new_size);\n\n\t \n\tif (type == &memblock.memory)\n\t\tin_slab = &memblock_memory_in_slab;\n\telse\n\t\tin_slab = &memblock_reserved_in_slab;\n\n\t \n\tif (use_slab) {\n\t\tnew_array = kmalloc(new_size, GFP_KERNEL);\n\t\taddr = new_array ? __pa(new_array) : 0;\n\t} else {\n\t\t \n\t\tif (type != &memblock.reserved)\n\t\t\tnew_area_start = new_area_size = 0;\n\n\t\taddr = memblock_find_in_range(new_area_start + new_area_size,\n\t\t\t\t\t\tmemblock.current_limit,\n\t\t\t\t\t\tnew_alloc_size, PAGE_SIZE);\n\t\tif (!addr && new_area_size)\n\t\t\taddr = memblock_find_in_range(0,\n\t\t\t\tmin(new_area_start, memblock.current_limit),\n\t\t\t\tnew_alloc_size, PAGE_SIZE);\n\n\t\tnew_array = addr ? __va(addr) : NULL;\n\t}\n\tif (!addr) {\n\t\tpr_err(\"memblock: Failed to double %s array from %ld to %ld entries !\\n\",\n\t\t       type->name, type->max, type->max * 2);\n\t\treturn -1;\n\t}\n\n\tnew_end = addr + new_size - 1;\n\tmemblock_dbg(\"memblock: %s is doubled to %ld at [%pa-%pa]\",\n\t\t\ttype->name, type->max * 2, &addr, &new_end);\n\n\t \n\tmemcpy(new_array, type->regions, old_size);\n\tmemset(new_array + type->max, 0, old_size);\n\told_array = type->regions;\n\ttype->regions = new_array;\n\ttype->max <<= 1;\n\n\t \n\tif (*in_slab)\n\t\tkfree(old_array);\n\telse if (old_array != memblock_memory_init_regions &&\n\t\t old_array != memblock_reserved_init_regions)\n\t\tmemblock_free(old_array, old_alloc_size);\n\n\t \n\tif (!use_slab)\n\t\tBUG_ON(memblock_reserve(addr, new_alloc_size));\n\n\t \n\t*in_slab = use_slab;\n\n\treturn 0;\n}\n\n \nstatic void __init_memblock memblock_merge_regions(struct memblock_type *type,\n\t\t\t\t\t\t   unsigned long start_rgn,\n\t\t\t\t\t\t   unsigned long end_rgn)\n{\n\tint i = 0;\n\tif (start_rgn)\n\t\ti = start_rgn - 1;\n\tend_rgn = min(end_rgn, type->cnt - 1);\n\twhile (i < end_rgn) {\n\t\tstruct memblock_region *this = &type->regions[i];\n\t\tstruct memblock_region *next = &type->regions[i + 1];\n\n\t\tif (this->base + this->size != next->base ||\n\t\t    memblock_get_region_node(this) !=\n\t\t    memblock_get_region_node(next) ||\n\t\t    this->flags != next->flags) {\n\t\t\tBUG_ON(this->base + this->size > next->base);\n\t\t\ti++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tthis->size += next->size;\n\t\t \n\t\tmemmove(next, next + 1, (type->cnt - (i + 2)) * sizeof(*next));\n\t\ttype->cnt--;\n\t\tend_rgn--;\n\t}\n}\n\n \nstatic void __init_memblock memblock_insert_region(struct memblock_type *type,\n\t\t\t\t\t\t   int idx, phys_addr_t base,\n\t\t\t\t\t\t   phys_addr_t size,\n\t\t\t\t\t\t   int nid,\n\t\t\t\t\t\t   enum memblock_flags flags)\n{\n\tstruct memblock_region *rgn = &type->regions[idx];\n\n\tBUG_ON(type->cnt >= type->max);\n\tmemmove(rgn + 1, rgn, (type->cnt - idx) * sizeof(*rgn));\n\trgn->base = base;\n\trgn->size = size;\n\trgn->flags = flags;\n\tmemblock_set_region_node(rgn, nid);\n\ttype->cnt++;\n\ttype->total_size += size;\n}\n\n \nstatic int __init_memblock memblock_add_range(struct memblock_type *type,\n\t\t\t\tphys_addr_t base, phys_addr_t size,\n\t\t\t\tint nid, enum memblock_flags flags)\n{\n\tbool insert = false;\n\tphys_addr_t obase = base;\n\tphys_addr_t end = base + memblock_cap_size(base, &size);\n\tint idx, nr_new, start_rgn = -1, end_rgn;\n\tstruct memblock_region *rgn;\n\n\tif (!size)\n\t\treturn 0;\n\n\t \n\tif (type->regions[0].size == 0) {\n\t\tWARN_ON(type->cnt != 1 || type->total_size);\n\t\ttype->regions[0].base = base;\n\t\ttype->regions[0].size = size;\n\t\ttype->regions[0].flags = flags;\n\t\tmemblock_set_region_node(&type->regions[0], nid);\n\t\ttype->total_size = size;\n\t\treturn 0;\n\t}\n\n\t \n\tif (type->cnt * 2 + 1 <= type->max)\n\t\tinsert = true;\n\nrepeat:\n\t \n\tbase = obase;\n\tnr_new = 0;\n\n\tfor_each_memblock_type(idx, type, rgn) {\n\t\tphys_addr_t rbase = rgn->base;\n\t\tphys_addr_t rend = rbase + rgn->size;\n\n\t\tif (rbase >= end)\n\t\t\tbreak;\n\t\tif (rend <= base)\n\t\t\tcontinue;\n\t\t \n\t\tif (rbase > base) {\n#ifdef CONFIG_NUMA\n\t\t\tWARN_ON(nid != memblock_get_region_node(rgn));\n#endif\n\t\t\tWARN_ON(flags != rgn->flags);\n\t\t\tnr_new++;\n\t\t\tif (insert) {\n\t\t\t\tif (start_rgn == -1)\n\t\t\t\t\tstart_rgn = idx;\n\t\t\t\tend_rgn = idx + 1;\n\t\t\t\tmemblock_insert_region(type, idx++, base,\n\t\t\t\t\t\t       rbase - base, nid,\n\t\t\t\t\t\t       flags);\n\t\t\t}\n\t\t}\n\t\t \n\t\tbase = min(rend, end);\n\t}\n\n\t \n\tif (base < end) {\n\t\tnr_new++;\n\t\tif (insert) {\n\t\t\tif (start_rgn == -1)\n\t\t\t\tstart_rgn = idx;\n\t\t\tend_rgn = idx + 1;\n\t\t\tmemblock_insert_region(type, idx, base, end - base,\n\t\t\t\t\t       nid, flags);\n\t\t}\n\t}\n\n\tif (!nr_new)\n\t\treturn 0;\n\n\t \n\tif (!insert) {\n\t\twhile (type->cnt + nr_new > type->max)\n\t\t\tif (memblock_double_array(type, obase, size) < 0)\n\t\t\t\treturn -ENOMEM;\n\t\tinsert = true;\n\t\tgoto repeat;\n\t} else {\n\t\tmemblock_merge_regions(type, start_rgn, end_rgn);\n\t\treturn 0;\n\t}\n}\n\n \nint __init_memblock memblock_add_node(phys_addr_t base, phys_addr_t size,\n\t\t\t\t      int nid, enum memblock_flags flags)\n{\n\tphys_addr_t end = base + size - 1;\n\n\tmemblock_dbg(\"%s: [%pa-%pa] nid=%d flags=%x %pS\\n\", __func__,\n\t\t     &base, &end, nid, flags, (void *)_RET_IP_);\n\n\treturn memblock_add_range(&memblock.memory, base, size, nid, flags);\n}\n\n \nint __init_memblock memblock_add(phys_addr_t base, phys_addr_t size)\n{\n\tphys_addr_t end = base + size - 1;\n\n\tmemblock_dbg(\"%s: [%pa-%pa] %pS\\n\", __func__,\n\t\t     &base, &end, (void *)_RET_IP_);\n\n\treturn memblock_add_range(&memblock.memory, base, size, MAX_NUMNODES, 0);\n}\n\n \nstatic int __init_memblock memblock_isolate_range(struct memblock_type *type,\n\t\t\t\t\tphys_addr_t base, phys_addr_t size,\n\t\t\t\t\tint *start_rgn, int *end_rgn)\n{\n\tphys_addr_t end = base + memblock_cap_size(base, &size);\n\tint idx;\n\tstruct memblock_region *rgn;\n\n\t*start_rgn = *end_rgn = 0;\n\n\tif (!size)\n\t\treturn 0;\n\n\t \n\twhile (type->cnt + 2 > type->max)\n\t\tif (memblock_double_array(type, base, size) < 0)\n\t\t\treturn -ENOMEM;\n\n\tfor_each_memblock_type(idx, type, rgn) {\n\t\tphys_addr_t rbase = rgn->base;\n\t\tphys_addr_t rend = rbase + rgn->size;\n\n\t\tif (rbase >= end)\n\t\t\tbreak;\n\t\tif (rend <= base)\n\t\t\tcontinue;\n\n\t\tif (rbase < base) {\n\t\t\t \n\t\t\trgn->base = base;\n\t\t\trgn->size -= base - rbase;\n\t\t\ttype->total_size -= base - rbase;\n\t\t\tmemblock_insert_region(type, idx, rbase, base - rbase,\n\t\t\t\t\t       memblock_get_region_node(rgn),\n\t\t\t\t\t       rgn->flags);\n\t\t} else if (rend > end) {\n\t\t\t \n\t\t\trgn->base = end;\n\t\t\trgn->size -= end - rbase;\n\t\t\ttype->total_size -= end - rbase;\n\t\t\tmemblock_insert_region(type, idx--, rbase, end - rbase,\n\t\t\t\t\t       memblock_get_region_node(rgn),\n\t\t\t\t\t       rgn->flags);\n\t\t} else {\n\t\t\t \n\t\t\tif (!*end_rgn)\n\t\t\t\t*start_rgn = idx;\n\t\t\t*end_rgn = idx + 1;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int __init_memblock memblock_remove_range(struct memblock_type *type,\n\t\t\t\t\t  phys_addr_t base, phys_addr_t size)\n{\n\tint start_rgn, end_rgn;\n\tint i, ret;\n\n\tret = memblock_isolate_range(type, base, size, &start_rgn, &end_rgn);\n\tif (ret)\n\t\treturn ret;\n\n\tfor (i = end_rgn - 1; i >= start_rgn; i--)\n\t\tmemblock_remove_region(type, i);\n\treturn 0;\n}\n\nint __init_memblock memblock_remove(phys_addr_t base, phys_addr_t size)\n{\n\tphys_addr_t end = base + size - 1;\n\n\tmemblock_dbg(\"%s: [%pa-%pa] %pS\\n\", __func__,\n\t\t     &base, &end, (void *)_RET_IP_);\n\n\treturn memblock_remove_range(&memblock.memory, base, size);\n}\n\n \nvoid __init_memblock memblock_free(void *ptr, size_t size)\n{\n\tif (ptr)\n\t\tmemblock_phys_free(__pa(ptr), size);\n}\n\n \nint __init_memblock memblock_phys_free(phys_addr_t base, phys_addr_t size)\n{\n\tphys_addr_t end = base + size - 1;\n\n\tmemblock_dbg(\"%s: [%pa-%pa] %pS\\n\", __func__,\n\t\t     &base, &end, (void *)_RET_IP_);\n\n\tkmemleak_free_part_phys(base, size);\n\treturn memblock_remove_range(&memblock.reserved, base, size);\n}\n\nint __init_memblock memblock_reserve(phys_addr_t base, phys_addr_t size)\n{\n\tphys_addr_t end = base + size - 1;\n\n\tmemblock_dbg(\"%s: [%pa-%pa] %pS\\n\", __func__,\n\t\t     &base, &end, (void *)_RET_IP_);\n\n\treturn memblock_add_range(&memblock.reserved, base, size, MAX_NUMNODES, 0);\n}\n\n#ifdef CONFIG_HAVE_MEMBLOCK_PHYS_MAP\nint __init_memblock memblock_physmem_add(phys_addr_t base, phys_addr_t size)\n{\n\tphys_addr_t end = base + size - 1;\n\n\tmemblock_dbg(\"%s: [%pa-%pa] %pS\\n\", __func__,\n\t\t     &base, &end, (void *)_RET_IP_);\n\n\treturn memblock_add_range(&physmem, base, size, MAX_NUMNODES, 0);\n}\n#endif\n\n \nstatic int __init_memblock memblock_setclr_flag(phys_addr_t base,\n\t\t\t\tphys_addr_t size, int set, int flag)\n{\n\tstruct memblock_type *type = &memblock.memory;\n\tint i, ret, start_rgn, end_rgn;\n\n\tret = memblock_isolate_range(type, base, size, &start_rgn, &end_rgn);\n\tif (ret)\n\t\treturn ret;\n\n\tfor (i = start_rgn; i < end_rgn; i++) {\n\t\tstruct memblock_region *r = &type->regions[i];\n\n\t\tif (set)\n\t\t\tr->flags |= flag;\n\t\telse\n\t\t\tr->flags &= ~flag;\n\t}\n\n\tmemblock_merge_regions(type, start_rgn, end_rgn);\n\treturn 0;\n}\n\n \nint __init_memblock memblock_mark_hotplug(phys_addr_t base, phys_addr_t size)\n{\n\treturn memblock_setclr_flag(base, size, 1, MEMBLOCK_HOTPLUG);\n}\n\n \nint __init_memblock memblock_clear_hotplug(phys_addr_t base, phys_addr_t size)\n{\n\treturn memblock_setclr_flag(base, size, 0, MEMBLOCK_HOTPLUG);\n}\n\n \nint __init_memblock memblock_mark_mirror(phys_addr_t base, phys_addr_t size)\n{\n\tif (!mirrored_kernelcore)\n\t\treturn 0;\n\n\tsystem_has_some_mirror = true;\n\n\treturn memblock_setclr_flag(base, size, 1, MEMBLOCK_MIRROR);\n}\n\n \nint __init_memblock memblock_mark_nomap(phys_addr_t base, phys_addr_t size)\n{\n\treturn memblock_setclr_flag(base, size, 1, MEMBLOCK_NOMAP);\n}\n\n \nint __init_memblock memblock_clear_nomap(phys_addr_t base, phys_addr_t size)\n{\n\treturn memblock_setclr_flag(base, size, 0, MEMBLOCK_NOMAP);\n}\n\nstatic bool should_skip_region(struct memblock_type *type,\n\t\t\t       struct memblock_region *m,\n\t\t\t       int nid, int flags)\n{\n\tint m_nid = memblock_get_region_node(m);\n\n\t \n\tif (type != memblock_memory)\n\t\treturn false;\n\n\t \n\tif (nid != NUMA_NO_NODE && nid != m_nid)\n\t\treturn true;\n\n\t \n\tif (movable_node_is_enabled() && memblock_is_hotpluggable(m) &&\n\t    !(flags & MEMBLOCK_HOTPLUG))\n\t\treturn true;\n\n\t \n\tif ((flags & MEMBLOCK_MIRROR) && !memblock_is_mirror(m))\n\t\treturn true;\n\n\t \n\tif (!(flags & MEMBLOCK_NOMAP) && memblock_is_nomap(m))\n\t\treturn true;\n\n\t \n\tif (!(flags & MEMBLOCK_DRIVER_MANAGED) && memblock_is_driver_managed(m))\n\t\treturn true;\n\n\treturn false;\n}\n\n \nvoid __next_mem_range(u64 *idx, int nid, enum memblock_flags flags,\n\t\t      struct memblock_type *type_a,\n\t\t      struct memblock_type *type_b, phys_addr_t *out_start,\n\t\t      phys_addr_t *out_end, int *out_nid)\n{\n\tint idx_a = *idx & 0xffffffff;\n\tint idx_b = *idx >> 32;\n\n\tif (WARN_ONCE(nid == MAX_NUMNODES,\n\t\"Usage of MAX_NUMNODES is deprecated. Use NUMA_NO_NODE instead\\n\"))\n\t\tnid = NUMA_NO_NODE;\n\n\tfor (; idx_a < type_a->cnt; idx_a++) {\n\t\tstruct memblock_region *m = &type_a->regions[idx_a];\n\n\t\tphys_addr_t m_start = m->base;\n\t\tphys_addr_t m_end = m->base + m->size;\n\t\tint\t    m_nid = memblock_get_region_node(m);\n\n\t\tif (should_skip_region(type_a, m, nid, flags))\n\t\t\tcontinue;\n\n\t\tif (!type_b) {\n\t\t\tif (out_start)\n\t\t\t\t*out_start = m_start;\n\t\t\tif (out_end)\n\t\t\t\t*out_end = m_end;\n\t\t\tif (out_nid)\n\t\t\t\t*out_nid = m_nid;\n\t\t\tidx_a++;\n\t\t\t*idx = (u32)idx_a | (u64)idx_b << 32;\n\t\t\treturn;\n\t\t}\n\n\t\t \n\t\tfor (; idx_b < type_b->cnt + 1; idx_b++) {\n\t\t\tstruct memblock_region *r;\n\t\t\tphys_addr_t r_start;\n\t\t\tphys_addr_t r_end;\n\n\t\t\tr = &type_b->regions[idx_b];\n\t\t\tr_start = idx_b ? r[-1].base + r[-1].size : 0;\n\t\t\tr_end = idx_b < type_b->cnt ?\n\t\t\t\tr->base : PHYS_ADDR_MAX;\n\n\t\t\t \n\t\t\tif (r_start >= m_end)\n\t\t\t\tbreak;\n\t\t\t \n\t\t\tif (m_start < r_end) {\n\t\t\t\tif (out_start)\n\t\t\t\t\t*out_start =\n\t\t\t\t\t\tmax(m_start, r_start);\n\t\t\t\tif (out_end)\n\t\t\t\t\t*out_end = min(m_end, r_end);\n\t\t\t\tif (out_nid)\n\t\t\t\t\t*out_nid = m_nid;\n\t\t\t\t \n\t\t\t\tif (m_end <= r_end)\n\t\t\t\t\tidx_a++;\n\t\t\t\telse\n\t\t\t\t\tidx_b++;\n\t\t\t\t*idx = (u32)idx_a | (u64)idx_b << 32;\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\t*idx = ULLONG_MAX;\n}\n\n \nvoid __init_memblock __next_mem_range_rev(u64 *idx, int nid,\n\t\t\t\t\t  enum memblock_flags flags,\n\t\t\t\t\t  struct memblock_type *type_a,\n\t\t\t\t\t  struct memblock_type *type_b,\n\t\t\t\t\t  phys_addr_t *out_start,\n\t\t\t\t\t  phys_addr_t *out_end, int *out_nid)\n{\n\tint idx_a = *idx & 0xffffffff;\n\tint idx_b = *idx >> 32;\n\n\tif (WARN_ONCE(nid == MAX_NUMNODES, \"Usage of MAX_NUMNODES is deprecated. Use NUMA_NO_NODE instead\\n\"))\n\t\tnid = NUMA_NO_NODE;\n\n\tif (*idx == (u64)ULLONG_MAX) {\n\t\tidx_a = type_a->cnt - 1;\n\t\tif (type_b != NULL)\n\t\t\tidx_b = type_b->cnt;\n\t\telse\n\t\t\tidx_b = 0;\n\t}\n\n\tfor (; idx_a >= 0; idx_a--) {\n\t\tstruct memblock_region *m = &type_a->regions[idx_a];\n\n\t\tphys_addr_t m_start = m->base;\n\t\tphys_addr_t m_end = m->base + m->size;\n\t\tint m_nid = memblock_get_region_node(m);\n\n\t\tif (should_skip_region(type_a, m, nid, flags))\n\t\t\tcontinue;\n\n\t\tif (!type_b) {\n\t\t\tif (out_start)\n\t\t\t\t*out_start = m_start;\n\t\t\tif (out_end)\n\t\t\t\t*out_end = m_end;\n\t\t\tif (out_nid)\n\t\t\t\t*out_nid = m_nid;\n\t\t\tidx_a--;\n\t\t\t*idx = (u32)idx_a | (u64)idx_b << 32;\n\t\t\treturn;\n\t\t}\n\n\t\t \n\t\tfor (; idx_b >= 0; idx_b--) {\n\t\t\tstruct memblock_region *r;\n\t\t\tphys_addr_t r_start;\n\t\t\tphys_addr_t r_end;\n\n\t\t\tr = &type_b->regions[idx_b];\n\t\t\tr_start = idx_b ? r[-1].base + r[-1].size : 0;\n\t\t\tr_end = idx_b < type_b->cnt ?\n\t\t\t\tr->base : PHYS_ADDR_MAX;\n\t\t\t \n\n\t\t\tif (r_end <= m_start)\n\t\t\t\tbreak;\n\t\t\t \n\t\t\tif (m_end > r_start) {\n\t\t\t\tif (out_start)\n\t\t\t\t\t*out_start = max(m_start, r_start);\n\t\t\t\tif (out_end)\n\t\t\t\t\t*out_end = min(m_end, r_end);\n\t\t\t\tif (out_nid)\n\t\t\t\t\t*out_nid = m_nid;\n\t\t\t\tif (m_start >= r_start)\n\t\t\t\t\tidx_a--;\n\t\t\t\telse\n\t\t\t\t\tidx_b--;\n\t\t\t\t*idx = (u32)idx_a | (u64)idx_b << 32;\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t}\n\t \n\t*idx = ULLONG_MAX;\n}\n\n \nvoid __init_memblock __next_mem_pfn_range(int *idx, int nid,\n\t\t\t\tunsigned long *out_start_pfn,\n\t\t\t\tunsigned long *out_end_pfn, int *out_nid)\n{\n\tstruct memblock_type *type = &memblock.memory;\n\tstruct memblock_region *r;\n\tint r_nid;\n\n\twhile (++*idx < type->cnt) {\n\t\tr = &type->regions[*idx];\n\t\tr_nid = memblock_get_region_node(r);\n\n\t\tif (PFN_UP(r->base) >= PFN_DOWN(r->base + r->size))\n\t\t\tcontinue;\n\t\tif (nid == MAX_NUMNODES || nid == r_nid)\n\t\t\tbreak;\n\t}\n\tif (*idx >= type->cnt) {\n\t\t*idx = -1;\n\t\treturn;\n\t}\n\n\tif (out_start_pfn)\n\t\t*out_start_pfn = PFN_UP(r->base);\n\tif (out_end_pfn)\n\t\t*out_end_pfn = PFN_DOWN(r->base + r->size);\n\tif (out_nid)\n\t\t*out_nid = r_nid;\n}\n\n \nint __init_memblock memblock_set_node(phys_addr_t base, phys_addr_t size,\n\t\t\t\t      struct memblock_type *type, int nid)\n{\n#ifdef CONFIG_NUMA\n\tint start_rgn, end_rgn;\n\tint i, ret;\n\n\tret = memblock_isolate_range(type, base, size, &start_rgn, &end_rgn);\n\tif (ret)\n\t\treturn ret;\n\n\tfor (i = start_rgn; i < end_rgn; i++)\n\t\tmemblock_set_region_node(&type->regions[i], nid);\n\n\tmemblock_merge_regions(type, start_rgn, end_rgn);\n#endif\n\treturn 0;\n}\n\n#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT\n \nvoid __init_memblock\n__next_mem_pfn_range_in_zone(u64 *idx, struct zone *zone,\n\t\t\t     unsigned long *out_spfn, unsigned long *out_epfn)\n{\n\tint zone_nid = zone_to_nid(zone);\n\tphys_addr_t spa, epa;\n\n\t__next_mem_range(idx, zone_nid, MEMBLOCK_NONE,\n\t\t\t &memblock.memory, &memblock.reserved,\n\t\t\t &spa, &epa, NULL);\n\n\twhile (*idx != U64_MAX) {\n\t\tunsigned long epfn = PFN_DOWN(epa);\n\t\tunsigned long spfn = PFN_UP(spa);\n\n\t\t \n\t\tif (zone->zone_start_pfn < epfn && spfn < epfn) {\n\t\t\t \n\t\t\tif (zone_end_pfn(zone) <= spfn) {\n\t\t\t\t*idx = U64_MAX;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (out_spfn)\n\t\t\t\t*out_spfn = max(zone->zone_start_pfn, spfn);\n\t\t\tif (out_epfn)\n\t\t\t\t*out_epfn = min(zone_end_pfn(zone), epfn);\n\n\t\t\treturn;\n\t\t}\n\n\t\t__next_mem_range(idx, zone_nid, MEMBLOCK_NONE,\n\t\t\t\t &memblock.memory, &memblock.reserved,\n\t\t\t\t &spa, &epa, NULL);\n\t}\n\n\t \n\tif (out_spfn)\n\t\t*out_spfn = ULONG_MAX;\n\tif (out_epfn)\n\t\t*out_epfn = 0;\n}\n\n#endif  \n\n \nphys_addr_t __init memblock_alloc_range_nid(phys_addr_t size,\n\t\t\t\t\tphys_addr_t align, phys_addr_t start,\n\t\t\t\t\tphys_addr_t end, int nid,\n\t\t\t\t\tbool exact_nid)\n{\n\tenum memblock_flags flags = choose_memblock_flags();\n\tphys_addr_t found;\n\n\tif (WARN_ONCE(nid == MAX_NUMNODES, \"Usage of MAX_NUMNODES is deprecated. Use NUMA_NO_NODE instead\\n\"))\n\t\tnid = NUMA_NO_NODE;\n\n\tif (!align) {\n\t\t \n\t\tdump_stack();\n\t\talign = SMP_CACHE_BYTES;\n\t}\n\nagain:\n\tfound = memblock_find_in_range_node(size, align, start, end, nid,\n\t\t\t\t\t    flags);\n\tif (found && !memblock_reserve(found, size))\n\t\tgoto done;\n\n\tif (nid != NUMA_NO_NODE && !exact_nid) {\n\t\tfound = memblock_find_in_range_node(size, align, start,\n\t\t\t\t\t\t    end, NUMA_NO_NODE,\n\t\t\t\t\t\t    flags);\n\t\tif (found && !memblock_reserve(found, size))\n\t\t\tgoto done;\n\t}\n\n\tif (flags & MEMBLOCK_MIRROR) {\n\t\tflags &= ~MEMBLOCK_MIRROR;\n\t\tpr_warn_ratelimited(\"Could not allocate %pap bytes of mirrored memory\\n\",\n\t\t\t&size);\n\t\tgoto again;\n\t}\n\n\treturn 0;\n\ndone:\n\t \n\tif (end != MEMBLOCK_ALLOC_NOLEAKTRACE)\n\t\t \n\t\tkmemleak_alloc_phys(found, size, 0);\n\n\t \n\taccept_memory(found, found + size);\n\n\treturn found;\n}\n\n \nphys_addr_t __init memblock_phys_alloc_range(phys_addr_t size,\n\t\t\t\t\t     phys_addr_t align,\n\t\t\t\t\t     phys_addr_t start,\n\t\t\t\t\t     phys_addr_t end)\n{\n\tmemblock_dbg(\"%s: %llu bytes align=0x%llx from=%pa max_addr=%pa %pS\\n\",\n\t\t     __func__, (u64)size, (u64)align, &start, &end,\n\t\t     (void *)_RET_IP_);\n\treturn memblock_alloc_range_nid(size, align, start, end, NUMA_NO_NODE,\n\t\t\t\t\tfalse);\n}\n\n \nphys_addr_t __init memblock_phys_alloc_try_nid(phys_addr_t size, phys_addr_t align, int nid)\n{\n\treturn memblock_alloc_range_nid(size, align, 0,\n\t\t\t\t\tMEMBLOCK_ALLOC_ACCESSIBLE, nid, false);\n}\n\n \nstatic void * __init memblock_alloc_internal(\n\t\t\t\tphys_addr_t size, phys_addr_t align,\n\t\t\t\tphys_addr_t min_addr, phys_addr_t max_addr,\n\t\t\t\tint nid, bool exact_nid)\n{\n\tphys_addr_t alloc;\n\n\t \n\tif (WARN_ON_ONCE(slab_is_available()))\n\t\treturn kzalloc_node(size, GFP_NOWAIT, nid);\n\n\tif (max_addr > memblock.current_limit)\n\t\tmax_addr = memblock.current_limit;\n\n\talloc = memblock_alloc_range_nid(size, align, min_addr, max_addr, nid,\n\t\t\t\t\texact_nid);\n\n\t \n\tif (!alloc && min_addr)\n\t\talloc = memblock_alloc_range_nid(size, align, 0, max_addr, nid,\n\t\t\t\t\t\texact_nid);\n\n\tif (!alloc)\n\t\treturn NULL;\n\n\treturn phys_to_virt(alloc);\n}\n\n \nvoid * __init memblock_alloc_exact_nid_raw(\n\t\t\tphys_addr_t size, phys_addr_t align,\n\t\t\tphys_addr_t min_addr, phys_addr_t max_addr,\n\t\t\tint nid)\n{\n\tmemblock_dbg(\"%s: %llu bytes align=0x%llx nid=%d from=%pa max_addr=%pa %pS\\n\",\n\t\t     __func__, (u64)size, (u64)align, nid, &min_addr,\n\t\t     &max_addr, (void *)_RET_IP_);\n\n\treturn memblock_alloc_internal(size, align, min_addr, max_addr, nid,\n\t\t\t\t       true);\n}\n\n \nvoid * __init memblock_alloc_try_nid_raw(\n\t\t\tphys_addr_t size, phys_addr_t align,\n\t\t\tphys_addr_t min_addr, phys_addr_t max_addr,\n\t\t\tint nid)\n{\n\tmemblock_dbg(\"%s: %llu bytes align=0x%llx nid=%d from=%pa max_addr=%pa %pS\\n\",\n\t\t     __func__, (u64)size, (u64)align, nid, &min_addr,\n\t\t     &max_addr, (void *)_RET_IP_);\n\n\treturn memblock_alloc_internal(size, align, min_addr, max_addr, nid,\n\t\t\t\t       false);\n}\n\n \nvoid * __init memblock_alloc_try_nid(\n\t\t\tphys_addr_t size, phys_addr_t align,\n\t\t\tphys_addr_t min_addr, phys_addr_t max_addr,\n\t\t\tint nid)\n{\n\tvoid *ptr;\n\n\tmemblock_dbg(\"%s: %llu bytes align=0x%llx nid=%d from=%pa max_addr=%pa %pS\\n\",\n\t\t     __func__, (u64)size, (u64)align, nid, &min_addr,\n\t\t     &max_addr, (void *)_RET_IP_);\n\tptr = memblock_alloc_internal(size, align,\n\t\t\t\t\t   min_addr, max_addr, nid, false);\n\tif (ptr)\n\t\tmemset(ptr, 0, size);\n\n\treturn ptr;\n}\n\n \nvoid __init memblock_free_late(phys_addr_t base, phys_addr_t size)\n{\n\tphys_addr_t cursor, end;\n\n\tend = base + size - 1;\n\tmemblock_dbg(\"%s: [%pa-%pa] %pS\\n\",\n\t\t     __func__, &base, &end, (void *)_RET_IP_);\n\tkmemleak_free_part_phys(base, size);\n\tcursor = PFN_UP(base);\n\tend = PFN_DOWN(base + size);\n\n\tfor (; cursor < end; cursor++) {\n\t\tmemblock_free_pages(pfn_to_page(cursor), cursor, 0);\n\t\ttotalram_pages_inc();\n\t}\n}\n\n \n\nphys_addr_t __init_memblock memblock_phys_mem_size(void)\n{\n\treturn memblock.memory.total_size;\n}\n\nphys_addr_t __init_memblock memblock_reserved_size(void)\n{\n\treturn memblock.reserved.total_size;\n}\n\n \nphys_addr_t __init_memblock memblock_start_of_DRAM(void)\n{\n\treturn memblock.memory.regions[0].base;\n}\n\nphys_addr_t __init_memblock memblock_end_of_DRAM(void)\n{\n\tint idx = memblock.memory.cnt - 1;\n\n\treturn (memblock.memory.regions[idx].base + memblock.memory.regions[idx].size);\n}\n\nstatic phys_addr_t __init_memblock __find_max_addr(phys_addr_t limit)\n{\n\tphys_addr_t max_addr = PHYS_ADDR_MAX;\n\tstruct memblock_region *r;\n\n\t \n\tfor_each_mem_region(r) {\n\t\tif (limit <= r->size) {\n\t\t\tmax_addr = r->base + limit;\n\t\t\tbreak;\n\t\t}\n\t\tlimit -= r->size;\n\t}\n\n\treturn max_addr;\n}\n\nvoid __init memblock_enforce_memory_limit(phys_addr_t limit)\n{\n\tphys_addr_t max_addr;\n\n\tif (!limit)\n\t\treturn;\n\n\tmax_addr = __find_max_addr(limit);\n\n\t \n\tif (max_addr == PHYS_ADDR_MAX)\n\t\treturn;\n\n\t \n\tmemblock_remove_range(&memblock.memory, max_addr,\n\t\t\t      PHYS_ADDR_MAX);\n\tmemblock_remove_range(&memblock.reserved, max_addr,\n\t\t\t      PHYS_ADDR_MAX);\n}\n\nvoid __init memblock_cap_memory_range(phys_addr_t base, phys_addr_t size)\n{\n\tint start_rgn, end_rgn;\n\tint i, ret;\n\n\tif (!size)\n\t\treturn;\n\n\tif (!memblock_memory->total_size) {\n\t\tpr_warn(\"%s: No memory registered yet\\n\", __func__);\n\t\treturn;\n\t}\n\n\tret = memblock_isolate_range(&memblock.memory, base, size,\n\t\t\t\t\t\t&start_rgn, &end_rgn);\n\tif (ret)\n\t\treturn;\n\n\t \n\tfor (i = memblock.memory.cnt - 1; i >= end_rgn; i--)\n\t\tif (!memblock_is_nomap(&memblock.memory.regions[i]))\n\t\t\tmemblock_remove_region(&memblock.memory, i);\n\n\tfor (i = start_rgn - 1; i >= 0; i--)\n\t\tif (!memblock_is_nomap(&memblock.memory.regions[i]))\n\t\t\tmemblock_remove_region(&memblock.memory, i);\n\n\t \n\tmemblock_remove_range(&memblock.reserved, 0, base);\n\tmemblock_remove_range(&memblock.reserved,\n\t\t\tbase + size, PHYS_ADDR_MAX);\n}\n\nvoid __init memblock_mem_limit_remove_map(phys_addr_t limit)\n{\n\tphys_addr_t max_addr;\n\n\tif (!limit)\n\t\treturn;\n\n\tmax_addr = __find_max_addr(limit);\n\n\t \n\tif (max_addr == PHYS_ADDR_MAX)\n\t\treturn;\n\n\tmemblock_cap_memory_range(0, max_addr);\n}\n\nstatic int __init_memblock memblock_search(struct memblock_type *type, phys_addr_t addr)\n{\n\tunsigned int left = 0, right = type->cnt;\n\n\tdo {\n\t\tunsigned int mid = (right + left) / 2;\n\n\t\tif (addr < type->regions[mid].base)\n\t\t\tright = mid;\n\t\telse if (addr >= (type->regions[mid].base +\n\t\t\t\t  type->regions[mid].size))\n\t\t\tleft = mid + 1;\n\t\telse\n\t\t\treturn mid;\n\t} while (left < right);\n\treturn -1;\n}\n\nbool __init_memblock memblock_is_reserved(phys_addr_t addr)\n{\n\treturn memblock_search(&memblock.reserved, addr) != -1;\n}\n\nbool __init_memblock memblock_is_memory(phys_addr_t addr)\n{\n\treturn memblock_search(&memblock.memory, addr) != -1;\n}\n\nbool __init_memblock memblock_is_map_memory(phys_addr_t addr)\n{\n\tint i = memblock_search(&memblock.memory, addr);\n\n\tif (i == -1)\n\t\treturn false;\n\treturn !memblock_is_nomap(&memblock.memory.regions[i]);\n}\n\nint __init_memblock memblock_search_pfn_nid(unsigned long pfn,\n\t\t\t unsigned long *start_pfn, unsigned long *end_pfn)\n{\n\tstruct memblock_type *type = &memblock.memory;\n\tint mid = memblock_search(type, PFN_PHYS(pfn));\n\n\tif (mid == -1)\n\t\treturn -1;\n\n\t*start_pfn = PFN_DOWN(type->regions[mid].base);\n\t*end_pfn = PFN_DOWN(type->regions[mid].base + type->regions[mid].size);\n\n\treturn memblock_get_region_node(&type->regions[mid]);\n}\n\n \nbool __init_memblock memblock_is_region_memory(phys_addr_t base, phys_addr_t size)\n{\n\tint idx = memblock_search(&memblock.memory, base);\n\tphys_addr_t end = base + memblock_cap_size(base, &size);\n\n\tif (idx == -1)\n\t\treturn false;\n\treturn (memblock.memory.regions[idx].base +\n\t\t memblock.memory.regions[idx].size) >= end;\n}\n\n \nbool __init_memblock memblock_is_region_reserved(phys_addr_t base, phys_addr_t size)\n{\n\treturn memblock_overlaps_region(&memblock.reserved, base, size);\n}\n\nvoid __init_memblock memblock_trim_memory(phys_addr_t align)\n{\n\tphys_addr_t start, end, orig_start, orig_end;\n\tstruct memblock_region *r;\n\n\tfor_each_mem_region(r) {\n\t\torig_start = r->base;\n\t\torig_end = r->base + r->size;\n\t\tstart = round_up(orig_start, align);\n\t\tend = round_down(orig_end, align);\n\n\t\tif (start == orig_start && end == orig_end)\n\t\t\tcontinue;\n\n\t\tif (start < end) {\n\t\t\tr->base = start;\n\t\t\tr->size = end - start;\n\t\t} else {\n\t\t\tmemblock_remove_region(&memblock.memory,\n\t\t\t\t\t       r - memblock.memory.regions);\n\t\t\tr--;\n\t\t}\n\t}\n}\n\nvoid __init_memblock memblock_set_current_limit(phys_addr_t limit)\n{\n\tmemblock.current_limit = limit;\n}\n\nphys_addr_t __init_memblock memblock_get_current_limit(void)\n{\n\treturn memblock.current_limit;\n}\n\nstatic void __init_memblock memblock_dump(struct memblock_type *type)\n{\n\tphys_addr_t base, end, size;\n\tenum memblock_flags flags;\n\tint idx;\n\tstruct memblock_region *rgn;\n\n\tpr_info(\" %s.cnt  = 0x%lx\\n\", type->name, type->cnt);\n\n\tfor_each_memblock_type(idx, type, rgn) {\n\t\tchar nid_buf[32] = \"\";\n\n\t\tbase = rgn->base;\n\t\tsize = rgn->size;\n\t\tend = base + size - 1;\n\t\tflags = rgn->flags;\n#ifdef CONFIG_NUMA\n\t\tif (memblock_get_region_node(rgn) != MAX_NUMNODES)\n\t\t\tsnprintf(nid_buf, sizeof(nid_buf), \" on node %d\",\n\t\t\t\t memblock_get_region_node(rgn));\n#endif\n\t\tpr_info(\" %s[%#x]\\t[%pa-%pa], %pa bytes%s flags: %#x\\n\",\n\t\t\ttype->name, idx, &base, &end, &size, nid_buf, flags);\n\t}\n}\n\nstatic void __init_memblock __memblock_dump_all(void)\n{\n\tpr_info(\"MEMBLOCK configuration:\\n\");\n\tpr_info(\" memory size = %pa reserved size = %pa\\n\",\n\t\t&memblock.memory.total_size,\n\t\t&memblock.reserved.total_size);\n\n\tmemblock_dump(&memblock.memory);\n\tmemblock_dump(&memblock.reserved);\n#ifdef CONFIG_HAVE_MEMBLOCK_PHYS_MAP\n\tmemblock_dump(&physmem);\n#endif\n}\n\nvoid __init_memblock memblock_dump_all(void)\n{\n\tif (memblock_debug)\n\t\t__memblock_dump_all();\n}\n\nvoid __init memblock_allow_resize(void)\n{\n\tmemblock_can_resize = 1;\n}\n\nstatic int __init early_memblock(char *p)\n{\n\tif (p && strstr(p, \"debug\"))\n\t\tmemblock_debug = 1;\n\treturn 0;\n}\nearly_param(\"memblock\", early_memblock);\n\nstatic void __init free_memmap(unsigned long start_pfn, unsigned long end_pfn)\n{\n\tstruct page *start_pg, *end_pg;\n\tphys_addr_t pg, pgend;\n\n\t \n\tstart_pg = pfn_to_page(start_pfn - 1) + 1;\n\tend_pg = pfn_to_page(end_pfn - 1) + 1;\n\n\t \n\tpg = PAGE_ALIGN(__pa(start_pg));\n\tpgend = __pa(end_pg) & PAGE_MASK;\n\n\t \n\tif (pg < pgend)\n\t\tmemblock_phys_free(pg, pgend - pg);\n}\n\n \nstatic void __init free_unused_memmap(void)\n{\n\tunsigned long start, end, prev_end = 0;\n\tint i;\n\n\tif (!IS_ENABLED(CONFIG_HAVE_ARCH_PFN_VALID) ||\n\t    IS_ENABLED(CONFIG_SPARSEMEM_VMEMMAP))\n\t\treturn;\n\n\t \n\tfor_each_mem_pfn_range(i, MAX_NUMNODES, &start, &end, NULL) {\n#ifdef CONFIG_SPARSEMEM\n\t\t \n\t\tstart = min(start, ALIGN(prev_end, PAGES_PER_SECTION));\n#endif\n\t\t \n\t\tstart = pageblock_start_pfn(start);\n\n\t\t \n\t\tif (prev_end && prev_end < start)\n\t\t\tfree_memmap(prev_end, start);\n\n\t\t \n\t\tprev_end = pageblock_align(end);\n\t}\n\n#ifdef CONFIG_SPARSEMEM\n\tif (!IS_ALIGNED(prev_end, PAGES_PER_SECTION)) {\n\t\tprev_end = pageblock_align(end);\n\t\tfree_memmap(prev_end, ALIGN(prev_end, PAGES_PER_SECTION));\n\t}\n#endif\n}\n\nstatic void __init __free_pages_memory(unsigned long start, unsigned long end)\n{\n\tint order;\n\n\twhile (start < end) {\n\t\t \n\t\tif (start)\n\t\t\torder = min_t(int, MAX_ORDER, __ffs(start));\n\t\telse\n\t\t\torder = MAX_ORDER;\n\n\t\twhile (start + (1UL << order) > end)\n\t\t\torder--;\n\n\t\tmemblock_free_pages(pfn_to_page(start), start, order);\n\n\t\tstart += (1UL << order);\n\t}\n}\n\nstatic unsigned long __init __free_memory_core(phys_addr_t start,\n\t\t\t\t phys_addr_t end)\n{\n\tunsigned long start_pfn = PFN_UP(start);\n\tunsigned long end_pfn = min_t(unsigned long,\n\t\t\t\t      PFN_DOWN(end), max_low_pfn);\n\n\tif (start_pfn >= end_pfn)\n\t\treturn 0;\n\n\t__free_pages_memory(start_pfn, end_pfn);\n\n\treturn end_pfn - start_pfn;\n}\n\nstatic void __init memmap_init_reserved_pages(void)\n{\n\tstruct memblock_region *region;\n\tphys_addr_t start, end;\n\tint nid;\n\n\t \n\tfor_each_mem_region(region) {\n\t\tnid = memblock_get_region_node(region);\n\t\tstart = region->base;\n\t\tend = start + region->size;\n\n\t\tif (memblock_is_nomap(region))\n\t\t\treserve_bootmem_region(start, end, nid);\n\n\t\tmemblock_set_node(start, end, &memblock.reserved, nid);\n\t}\n\n\t \n\tfor_each_reserved_mem_region(region) {\n\t\tnid = memblock_get_region_node(region);\n\t\tstart = region->base;\n\t\tend = start + region->size;\n\n\t\treserve_bootmem_region(start, end, nid);\n\t}\n}\n\nstatic unsigned long __init free_low_memory_core_early(void)\n{\n\tunsigned long count = 0;\n\tphys_addr_t start, end;\n\tu64 i;\n\n\tmemblock_clear_hotplug(0, -1);\n\n\tmemmap_init_reserved_pages();\n\n\t \n\tfor_each_free_mem_range(i, NUMA_NO_NODE, MEMBLOCK_NONE, &start, &end,\n\t\t\t\tNULL)\n\t\tcount += __free_memory_core(start, end);\n\n\treturn count;\n}\n\nstatic int reset_managed_pages_done __initdata;\n\nstatic void __init reset_node_managed_pages(pg_data_t *pgdat)\n{\n\tstruct zone *z;\n\n\tfor (z = pgdat->node_zones; z < pgdat->node_zones + MAX_NR_ZONES; z++)\n\t\tatomic_long_set(&z->managed_pages, 0);\n}\n\nvoid __init reset_all_zones_managed_pages(void)\n{\n\tstruct pglist_data *pgdat;\n\n\tif (reset_managed_pages_done)\n\t\treturn;\n\n\tfor_each_online_pgdat(pgdat)\n\t\treset_node_managed_pages(pgdat);\n\n\treset_managed_pages_done = 1;\n}\n\n \nvoid __init memblock_free_all(void)\n{\n\tunsigned long pages;\n\n\tfree_unused_memmap();\n\treset_all_zones_managed_pages();\n\n\tpages = free_low_memory_core_early();\n\ttotalram_pages_add(pages);\n}\n\n#if defined(CONFIG_DEBUG_FS) && defined(CONFIG_ARCH_KEEP_MEMBLOCK)\nstatic const char * const flagname[] = {\n\t[ilog2(MEMBLOCK_HOTPLUG)] = \"HOTPLUG\",\n\t[ilog2(MEMBLOCK_MIRROR)] = \"MIRROR\",\n\t[ilog2(MEMBLOCK_NOMAP)] = \"NOMAP\",\n\t[ilog2(MEMBLOCK_DRIVER_MANAGED)] = \"DRV_MNG\",\n};\n\nstatic int memblock_debug_show(struct seq_file *m, void *private)\n{\n\tstruct memblock_type *type = m->private;\n\tstruct memblock_region *reg;\n\tint i, j, nid;\n\tunsigned int count = ARRAY_SIZE(flagname);\n\tphys_addr_t end;\n\n\tfor (i = 0; i < type->cnt; i++) {\n\t\treg = &type->regions[i];\n\t\tend = reg->base + reg->size - 1;\n\t\tnid = memblock_get_region_node(reg);\n\n\t\tseq_printf(m, \"%4d: \", i);\n\t\tseq_printf(m, \"%pa..%pa \", &reg->base, &end);\n\t\tif (nid != MAX_NUMNODES)\n\t\t\tseq_printf(m, \"%4d \", nid);\n\t\telse\n\t\t\tseq_printf(m, \"%4c \", 'x');\n\t\tif (reg->flags) {\n\t\t\tfor (j = 0; j < count; j++) {\n\t\t\t\tif (reg->flags & (1U << j)) {\n\t\t\t\t\tseq_printf(m, \"%s\\n\", flagname[j]);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (j == count)\n\t\t\t\tseq_printf(m, \"%s\\n\", \"UNKNOWN\");\n\t\t} else {\n\t\t\tseq_printf(m, \"%s\\n\", \"NONE\");\n\t\t}\n\t}\n\treturn 0;\n}\nDEFINE_SHOW_ATTRIBUTE(memblock_debug);\n\nstatic int __init memblock_init_debugfs(void)\n{\n\tstruct dentry *root = debugfs_create_dir(\"memblock\", NULL);\n\n\tdebugfs_create_file(\"memory\", 0444, root,\n\t\t\t    &memblock.memory, &memblock_debug_fops);\n\tdebugfs_create_file(\"reserved\", 0444, root,\n\t\t\t    &memblock.reserved, &memblock_debug_fops);\n#ifdef CONFIG_HAVE_MEMBLOCK_PHYS_MAP\n\tdebugfs_create_file(\"physmem\", 0444, root, &physmem,\n\t\t\t    &memblock_debug_fops);\n#endif\n\n\treturn 0;\n}\n__initcall(memblock_init_debugfs);\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}