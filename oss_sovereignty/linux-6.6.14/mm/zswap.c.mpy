{
  "module_name": "zswap.c",
  "hash_id": "070b0867e863a3d2bc71c8b3a95537e5c22600ec28a55c996e61714a81fab7a3",
  "original_prompt": "Ingested from linux-6.6.14/mm/zswap.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/module.h>\n#include <linux/cpu.h>\n#include <linux/highmem.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n#include <linux/types.h>\n#include <linux/atomic.h>\n#include <linux/rbtree.h>\n#include <linux/swap.h>\n#include <linux/crypto.h>\n#include <linux/scatterlist.h>\n#include <linux/mempool.h>\n#include <linux/zpool.h>\n#include <crypto/acompress.h>\n#include <linux/zswap.h>\n#include <linux/mm_types.h>\n#include <linux/page-flags.h>\n#include <linux/swapops.h>\n#include <linux/writeback.h>\n#include <linux/pagemap.h>\n#include <linux/workqueue.h>\n\n#include \"swap.h\"\n#include \"internal.h\"\n\n \n \nu64 zswap_pool_total_size;\n \natomic_t zswap_stored_pages = ATOMIC_INIT(0);\n \nstatic atomic_t zswap_same_filled_pages = ATOMIC_INIT(0);\n\n \n\n \nstatic u64 zswap_pool_limit_hit;\n \nstatic u64 zswap_written_back_pages;\n \nstatic u64 zswap_reject_reclaim_fail;\n \nstatic u64 zswap_reject_compress_poor;\n \nstatic u64 zswap_reject_alloc_fail;\n \nstatic u64 zswap_reject_kmemcache_fail;\n \nstatic u64 zswap_duplicate_entry;\n\n \nstatic struct workqueue_struct *shrink_wq;\n \nstatic bool zswap_pool_reached_full;\n\n \n\n#define ZSWAP_PARAM_UNSET \"\"\n\nstatic int zswap_setup(void);\n\n \nstatic bool zswap_enabled = IS_ENABLED(CONFIG_ZSWAP_DEFAULT_ON);\nstatic int zswap_enabled_param_set(const char *,\n\t\t\t\t   const struct kernel_param *);\nstatic const struct kernel_param_ops zswap_enabled_param_ops = {\n\t.set =\t\tzswap_enabled_param_set,\n\t.get =\t\tparam_get_bool,\n};\nmodule_param_cb(enabled, &zswap_enabled_param_ops, &zswap_enabled, 0644);\n\n \nstatic char *zswap_compressor = CONFIG_ZSWAP_COMPRESSOR_DEFAULT;\nstatic int zswap_compressor_param_set(const char *,\n\t\t\t\t      const struct kernel_param *);\nstatic const struct kernel_param_ops zswap_compressor_param_ops = {\n\t.set =\t\tzswap_compressor_param_set,\n\t.get =\t\tparam_get_charp,\n\t.free =\t\tparam_free_charp,\n};\nmodule_param_cb(compressor, &zswap_compressor_param_ops,\n\t\t&zswap_compressor, 0644);\n\n \nstatic char *zswap_zpool_type = CONFIG_ZSWAP_ZPOOL_DEFAULT;\nstatic int zswap_zpool_param_set(const char *, const struct kernel_param *);\nstatic const struct kernel_param_ops zswap_zpool_param_ops = {\n\t.set =\t\tzswap_zpool_param_set,\n\t.get =\t\tparam_get_charp,\n\t.free =\t\tparam_free_charp,\n};\nmodule_param_cb(zpool, &zswap_zpool_param_ops, &zswap_zpool_type, 0644);\n\n \nstatic unsigned int zswap_max_pool_percent = 20;\nmodule_param_named(max_pool_percent, zswap_max_pool_percent, uint, 0644);\n\n \nstatic unsigned int zswap_accept_thr_percent = 90;  \nmodule_param_named(accept_threshold_percent, zswap_accept_thr_percent,\n\t\t   uint, 0644);\n\n \nstatic bool zswap_same_filled_pages_enabled = true;\nmodule_param_named(same_filled_pages_enabled, zswap_same_filled_pages_enabled,\n\t\t   bool, 0644);\n\n \nstatic bool zswap_non_same_filled_pages_enabled = true;\nmodule_param_named(non_same_filled_pages_enabled, zswap_non_same_filled_pages_enabled,\n\t\t   bool, 0644);\n\nstatic bool zswap_exclusive_loads_enabled = IS_ENABLED(\n\t\tCONFIG_ZSWAP_EXCLUSIVE_LOADS_DEFAULT_ON);\nmodule_param_named(exclusive_loads, zswap_exclusive_loads_enabled, bool, 0644);\n\n \n#define ZSWAP_NR_ZPOOLS 32\n\n \n\nstruct crypto_acomp_ctx {\n\tstruct crypto_acomp *acomp;\n\tstruct acomp_req *req;\n\tstruct crypto_wait wait;\n\tu8 *dstmem;\n\tstruct mutex *mutex;\n};\n\n \nstruct zswap_pool {\n\tstruct zpool *zpools[ZSWAP_NR_ZPOOLS];\n\tstruct crypto_acomp_ctx __percpu *acomp_ctx;\n\tstruct kref kref;\n\tstruct list_head list;\n\tstruct work_struct release_work;\n\tstruct work_struct shrink_work;\n\tstruct hlist_node node;\n\tchar tfm_name[CRYPTO_MAX_ALG_NAME];\n\tstruct list_head lru;\n\tspinlock_t lru_lock;\n};\n\n \nstruct zswap_entry {\n\tstruct rb_node rbnode;\n\tswp_entry_t swpentry;\n\tint refcount;\n\tunsigned int length;\n\tstruct zswap_pool *pool;\n\tunion {\n\t\tunsigned long handle;\n\t\tunsigned long value;\n\t};\n\tstruct obj_cgroup *objcg;\n\tstruct list_head lru;\n};\n\n \nstruct zswap_tree {\n\tstruct rb_root rbroot;\n\tspinlock_t lock;\n};\n\nstatic struct zswap_tree *zswap_trees[MAX_SWAPFILES];\n\n \nstatic LIST_HEAD(zswap_pools);\n \nstatic DEFINE_SPINLOCK(zswap_pools_lock);\n \nstatic atomic_t zswap_pools_count = ATOMIC_INIT(0);\n\nenum zswap_init_type {\n\tZSWAP_UNINIT,\n\tZSWAP_INIT_SUCCEED,\n\tZSWAP_INIT_FAILED\n};\n\nstatic enum zswap_init_type zswap_init_state;\n\n \nstatic DEFINE_MUTEX(zswap_init_lock);\n\n \nstatic bool zswap_has_pool;\n\n \n\n#define zswap_pool_debug(msg, p)\t\t\t\t\\\n\tpr_debug(\"%s pool %s/%s\\n\", msg, (p)->tfm_name,\t\t\\\n\t\t zpool_get_type((p)->zpools[0]))\n\nstatic int zswap_writeback_entry(struct zswap_entry *entry,\n\t\t\t\t struct zswap_tree *tree);\nstatic int zswap_pool_get(struct zswap_pool *pool);\nstatic void zswap_pool_put(struct zswap_pool *pool);\n\nstatic bool zswap_is_full(void)\n{\n\treturn totalram_pages() * zswap_max_pool_percent / 100 <\n\t\t\tDIV_ROUND_UP(zswap_pool_total_size, PAGE_SIZE);\n}\n\nstatic bool zswap_can_accept(void)\n{\n\treturn totalram_pages() * zswap_accept_thr_percent / 100 *\n\t\t\t\tzswap_max_pool_percent / 100 >\n\t\t\tDIV_ROUND_UP(zswap_pool_total_size, PAGE_SIZE);\n}\n\nstatic void zswap_update_total_size(void)\n{\n\tstruct zswap_pool *pool;\n\tu64 total = 0;\n\tint i;\n\n\trcu_read_lock();\n\n\tlist_for_each_entry_rcu(pool, &zswap_pools, list)\n\t\tfor (i = 0; i < ZSWAP_NR_ZPOOLS; i++)\n\t\t\ttotal += zpool_get_total_size(pool->zpools[i]);\n\n\trcu_read_unlock();\n\n\tzswap_pool_total_size = total;\n}\n\n \nstatic struct kmem_cache *zswap_entry_cache;\n\nstatic struct zswap_entry *zswap_entry_cache_alloc(gfp_t gfp)\n{\n\tstruct zswap_entry *entry;\n\tentry = kmem_cache_alloc(zswap_entry_cache, gfp);\n\tif (!entry)\n\t\treturn NULL;\n\tentry->refcount = 1;\n\tRB_CLEAR_NODE(&entry->rbnode);\n\treturn entry;\n}\n\nstatic void zswap_entry_cache_free(struct zswap_entry *entry)\n{\n\tkmem_cache_free(zswap_entry_cache, entry);\n}\n\n \nstatic struct zswap_entry *zswap_rb_search(struct rb_root *root, pgoff_t offset)\n{\n\tstruct rb_node *node = root->rb_node;\n\tstruct zswap_entry *entry;\n\tpgoff_t entry_offset;\n\n\twhile (node) {\n\t\tentry = rb_entry(node, struct zswap_entry, rbnode);\n\t\tentry_offset = swp_offset(entry->swpentry);\n\t\tif (entry_offset > offset)\n\t\t\tnode = node->rb_left;\n\t\telse if (entry_offset < offset)\n\t\t\tnode = node->rb_right;\n\t\telse\n\t\t\treturn entry;\n\t}\n\treturn NULL;\n}\n\n \nstatic int zswap_rb_insert(struct rb_root *root, struct zswap_entry *entry,\n\t\t\tstruct zswap_entry **dupentry)\n{\n\tstruct rb_node **link = &root->rb_node, *parent = NULL;\n\tstruct zswap_entry *myentry;\n\tpgoff_t myentry_offset, entry_offset = swp_offset(entry->swpentry);\n\n\twhile (*link) {\n\t\tparent = *link;\n\t\tmyentry = rb_entry(parent, struct zswap_entry, rbnode);\n\t\tmyentry_offset = swp_offset(myentry->swpentry);\n\t\tif (myentry_offset > entry_offset)\n\t\t\tlink = &(*link)->rb_left;\n\t\telse if (myentry_offset < entry_offset)\n\t\t\tlink = &(*link)->rb_right;\n\t\telse {\n\t\t\t*dupentry = myentry;\n\t\t\treturn -EEXIST;\n\t\t}\n\t}\n\trb_link_node(&entry->rbnode, parent, link);\n\trb_insert_color(&entry->rbnode, root);\n\treturn 0;\n}\n\nstatic bool zswap_rb_erase(struct rb_root *root, struct zswap_entry *entry)\n{\n\tif (!RB_EMPTY_NODE(&entry->rbnode)) {\n\t\trb_erase(&entry->rbnode, root);\n\t\tRB_CLEAR_NODE(&entry->rbnode);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic struct zpool *zswap_find_zpool(struct zswap_entry *entry)\n{\n\tint i = 0;\n\n\tif (ZSWAP_NR_ZPOOLS > 1)\n\t\ti = hash_ptr(entry, ilog2(ZSWAP_NR_ZPOOLS));\n\n\treturn entry->pool->zpools[i];\n}\n\n \nstatic void zswap_free_entry(struct zswap_entry *entry)\n{\n\tif (entry->objcg) {\n\t\tobj_cgroup_uncharge_zswap(entry->objcg, entry->length);\n\t\tobj_cgroup_put(entry->objcg);\n\t}\n\tif (!entry->length)\n\t\tatomic_dec(&zswap_same_filled_pages);\n\telse {\n\t\tspin_lock(&entry->pool->lru_lock);\n\t\tlist_del(&entry->lru);\n\t\tspin_unlock(&entry->pool->lru_lock);\n\t\tzpool_free(zswap_find_zpool(entry), entry->handle);\n\t\tzswap_pool_put(entry->pool);\n\t}\n\tzswap_entry_cache_free(entry);\n\tatomic_dec(&zswap_stored_pages);\n\tzswap_update_total_size();\n}\n\n \nstatic void zswap_entry_get(struct zswap_entry *entry)\n{\n\tentry->refcount++;\n}\n\n \nstatic void zswap_entry_put(struct zswap_tree *tree,\n\t\t\tstruct zswap_entry *entry)\n{\n\tint refcount = --entry->refcount;\n\n\tWARN_ON_ONCE(refcount < 0);\n\tif (refcount == 0) {\n\t\tWARN_ON_ONCE(!RB_EMPTY_NODE(&entry->rbnode));\n\t\tzswap_free_entry(entry);\n\t}\n}\n\n \nstatic struct zswap_entry *zswap_entry_find_get(struct rb_root *root,\n\t\t\t\tpgoff_t offset)\n{\n\tstruct zswap_entry *entry;\n\n\tentry = zswap_rb_search(root, offset);\n\tif (entry)\n\t\tzswap_entry_get(entry);\n\n\treturn entry;\n}\n\n \nstatic DEFINE_PER_CPU(u8 *, zswap_dstmem);\n \nstatic DEFINE_PER_CPU(struct mutex *, zswap_mutex);\n\nstatic int zswap_dstmem_prepare(unsigned int cpu)\n{\n\tstruct mutex *mutex;\n\tu8 *dst;\n\n\tdst = kmalloc_node(PAGE_SIZE * 2, GFP_KERNEL, cpu_to_node(cpu));\n\tif (!dst)\n\t\treturn -ENOMEM;\n\n\tmutex = kmalloc_node(sizeof(*mutex), GFP_KERNEL, cpu_to_node(cpu));\n\tif (!mutex) {\n\t\tkfree(dst);\n\t\treturn -ENOMEM;\n\t}\n\n\tmutex_init(mutex);\n\tper_cpu(zswap_dstmem, cpu) = dst;\n\tper_cpu(zswap_mutex, cpu) = mutex;\n\treturn 0;\n}\n\nstatic int zswap_dstmem_dead(unsigned int cpu)\n{\n\tstruct mutex *mutex;\n\tu8 *dst;\n\n\tmutex = per_cpu(zswap_mutex, cpu);\n\tkfree(mutex);\n\tper_cpu(zswap_mutex, cpu) = NULL;\n\n\tdst = per_cpu(zswap_dstmem, cpu);\n\tkfree(dst);\n\tper_cpu(zswap_dstmem, cpu) = NULL;\n\n\treturn 0;\n}\n\nstatic int zswap_cpu_comp_prepare(unsigned int cpu, struct hlist_node *node)\n{\n\tstruct zswap_pool *pool = hlist_entry(node, struct zswap_pool, node);\n\tstruct crypto_acomp_ctx *acomp_ctx = per_cpu_ptr(pool->acomp_ctx, cpu);\n\tstruct crypto_acomp *acomp;\n\tstruct acomp_req *req;\n\n\tacomp = crypto_alloc_acomp_node(pool->tfm_name, 0, 0, cpu_to_node(cpu));\n\tif (IS_ERR(acomp)) {\n\t\tpr_err(\"could not alloc crypto acomp %s : %ld\\n\",\n\t\t\t\tpool->tfm_name, PTR_ERR(acomp));\n\t\treturn PTR_ERR(acomp);\n\t}\n\tacomp_ctx->acomp = acomp;\n\n\treq = acomp_request_alloc(acomp_ctx->acomp);\n\tif (!req) {\n\t\tpr_err(\"could not alloc crypto acomp_request %s\\n\",\n\t\t       pool->tfm_name);\n\t\tcrypto_free_acomp(acomp_ctx->acomp);\n\t\treturn -ENOMEM;\n\t}\n\tacomp_ctx->req = req;\n\n\tcrypto_init_wait(&acomp_ctx->wait);\n\t \n\tacomp_request_set_callback(req, CRYPTO_TFM_REQ_MAY_BACKLOG,\n\t\t\t\t   crypto_req_done, &acomp_ctx->wait);\n\n\tacomp_ctx->mutex = per_cpu(zswap_mutex, cpu);\n\tacomp_ctx->dstmem = per_cpu(zswap_dstmem, cpu);\n\n\treturn 0;\n}\n\nstatic int zswap_cpu_comp_dead(unsigned int cpu, struct hlist_node *node)\n{\n\tstruct zswap_pool *pool = hlist_entry(node, struct zswap_pool, node);\n\tstruct crypto_acomp_ctx *acomp_ctx = per_cpu_ptr(pool->acomp_ctx, cpu);\n\n\tif (!IS_ERR_OR_NULL(acomp_ctx)) {\n\t\tif (!IS_ERR_OR_NULL(acomp_ctx->req))\n\t\t\tacomp_request_free(acomp_ctx->req);\n\t\tif (!IS_ERR_OR_NULL(acomp_ctx->acomp))\n\t\t\tcrypto_free_acomp(acomp_ctx->acomp);\n\t}\n\n\treturn 0;\n}\n\n \n\nstatic struct zswap_pool *__zswap_pool_current(void)\n{\n\tstruct zswap_pool *pool;\n\n\tpool = list_first_or_null_rcu(&zswap_pools, typeof(*pool), list);\n\tWARN_ONCE(!pool && zswap_has_pool,\n\t\t  \"%s: no page storage pool!\\n\", __func__);\n\n\treturn pool;\n}\n\nstatic struct zswap_pool *zswap_pool_current(void)\n{\n\tassert_spin_locked(&zswap_pools_lock);\n\n\treturn __zswap_pool_current();\n}\n\nstatic struct zswap_pool *zswap_pool_current_get(void)\n{\n\tstruct zswap_pool *pool;\n\n\trcu_read_lock();\n\n\tpool = __zswap_pool_current();\n\tif (!zswap_pool_get(pool))\n\t\tpool = NULL;\n\n\trcu_read_unlock();\n\n\treturn pool;\n}\n\nstatic struct zswap_pool *zswap_pool_last_get(void)\n{\n\tstruct zswap_pool *pool, *last = NULL;\n\n\trcu_read_lock();\n\n\tlist_for_each_entry_rcu(pool, &zswap_pools, list)\n\t\tlast = pool;\n\tWARN_ONCE(!last && zswap_has_pool,\n\t\t  \"%s: no page storage pool!\\n\", __func__);\n\tif (!zswap_pool_get(last))\n\t\tlast = NULL;\n\n\trcu_read_unlock();\n\n\treturn last;\n}\n\n \nstatic struct zswap_pool *zswap_pool_find_get(char *type, char *compressor)\n{\n\tstruct zswap_pool *pool;\n\n\tassert_spin_locked(&zswap_pools_lock);\n\n\tlist_for_each_entry_rcu(pool, &zswap_pools, list) {\n\t\tif (strcmp(pool->tfm_name, compressor))\n\t\t\tcontinue;\n\t\t \n\t\tif (strcmp(zpool_get_type(pool->zpools[0]), type))\n\t\t\tcontinue;\n\t\t \n\t\tif (!zswap_pool_get(pool))\n\t\t\tcontinue;\n\t\treturn pool;\n\t}\n\n\treturn NULL;\n}\n\n \nstatic void zswap_invalidate_entry(struct zswap_tree *tree,\n\t\t\t\t   struct zswap_entry *entry)\n{\n\tif (zswap_rb_erase(&tree->rbroot, entry))\n\t\tzswap_entry_put(tree, entry);\n}\n\nstatic int zswap_reclaim_entry(struct zswap_pool *pool)\n{\n\tstruct zswap_entry *entry;\n\tstruct zswap_tree *tree;\n\tpgoff_t swpoffset;\n\tint ret;\n\n\t \n\tspin_lock(&pool->lru_lock);\n\tif (list_empty(&pool->lru)) {\n\t\tspin_unlock(&pool->lru_lock);\n\t\treturn -EINVAL;\n\t}\n\tentry = list_last_entry(&pool->lru, struct zswap_entry, lru);\n\tlist_del_init(&entry->lru);\n\t \n\tswpoffset = swp_offset(entry->swpentry);\n\ttree = zswap_trees[swp_type(entry->swpentry)];\n\tspin_unlock(&pool->lru_lock);\n\n\t \n\tspin_lock(&tree->lock);\n\tif (entry != zswap_rb_search(&tree->rbroot, swpoffset)) {\n\t\tret = -EAGAIN;\n\t\tgoto unlock;\n\t}\n\t \n\tzswap_entry_get(entry);\n\tspin_unlock(&tree->lock);\n\n\tret = zswap_writeback_entry(entry, tree);\n\n\tspin_lock(&tree->lock);\n\tif (ret) {\n\t\t \n\t\tspin_lock(&pool->lru_lock);\n\t\tlist_move(&entry->lru, &pool->lru);\n\t\tspin_unlock(&pool->lru_lock);\n\t\tgoto put_unlock;\n\t}\n\n\t \n\tzswap_invalidate_entry(tree, entry);\n\nput_unlock:\n\t \n\tzswap_entry_put(tree, entry);\nunlock:\n\tspin_unlock(&tree->lock);\n\treturn ret ? -EAGAIN : 0;\n}\n\nstatic void shrink_worker(struct work_struct *w)\n{\n\tstruct zswap_pool *pool = container_of(w, typeof(*pool),\n\t\t\t\t\t\tshrink_work);\n\tint ret, failures = 0;\n\n\tdo {\n\t\tret = zswap_reclaim_entry(pool);\n\t\tif (ret) {\n\t\t\tzswap_reject_reclaim_fail++;\n\t\t\tif (ret != -EAGAIN)\n\t\t\t\tbreak;\n\t\t\tif (++failures == MAX_RECLAIM_RETRIES)\n\t\t\t\tbreak;\n\t\t}\n\t\tcond_resched();\n\t} while (!zswap_can_accept());\n\tzswap_pool_put(pool);\n}\n\nstatic struct zswap_pool *zswap_pool_create(char *type, char *compressor)\n{\n\tint i;\n\tstruct zswap_pool *pool;\n\tchar name[38];  \n\tgfp_t gfp = __GFP_NORETRY | __GFP_NOWARN | __GFP_KSWAPD_RECLAIM;\n\tint ret;\n\n\tif (!zswap_has_pool) {\n\t\t \n\t\tif (!strcmp(type, ZSWAP_PARAM_UNSET))\n\t\t\treturn NULL;\n\t\tif (!strcmp(compressor, ZSWAP_PARAM_UNSET))\n\t\t\treturn NULL;\n\t}\n\n\tpool = kzalloc(sizeof(*pool), GFP_KERNEL);\n\tif (!pool)\n\t\treturn NULL;\n\n\tfor (i = 0; i < ZSWAP_NR_ZPOOLS; i++) {\n\t\t \n\t\tsnprintf(name, 38, \"zswap%x\",\n\t\t\t atomic_inc_return(&zswap_pools_count));\n\n\t\tpool->zpools[i] = zpool_create_pool(type, name, gfp);\n\t\tif (!pool->zpools[i]) {\n\t\t\tpr_err(\"%s zpool not available\\n\", type);\n\t\t\tgoto error;\n\t\t}\n\t}\n\tpr_debug(\"using %s zpool\\n\", zpool_get_type(pool->zpools[0]));\n\n\tstrscpy(pool->tfm_name, compressor, sizeof(pool->tfm_name));\n\n\tpool->acomp_ctx = alloc_percpu(*pool->acomp_ctx);\n\tif (!pool->acomp_ctx) {\n\t\tpr_err(\"percpu alloc failed\\n\");\n\t\tgoto error;\n\t}\n\n\tret = cpuhp_state_add_instance(CPUHP_MM_ZSWP_POOL_PREPARE,\n\t\t\t\t       &pool->node);\n\tif (ret)\n\t\tgoto error;\n\tpr_debug(\"using %s compressor\\n\", pool->tfm_name);\n\n\t \n\tkref_init(&pool->kref);\n\tINIT_LIST_HEAD(&pool->list);\n\tINIT_LIST_HEAD(&pool->lru);\n\tspin_lock_init(&pool->lru_lock);\n\tINIT_WORK(&pool->shrink_work, shrink_worker);\n\n\tzswap_pool_debug(\"created\", pool);\n\n\treturn pool;\n\nerror:\n\tif (pool->acomp_ctx)\n\t\tfree_percpu(pool->acomp_ctx);\n\twhile (i--)\n\t\tzpool_destroy_pool(pool->zpools[i]);\n\tkfree(pool);\n\treturn NULL;\n}\n\nstatic struct zswap_pool *__zswap_pool_create_fallback(void)\n{\n\tbool has_comp, has_zpool;\n\n\thas_comp = crypto_has_acomp(zswap_compressor, 0, 0);\n\tif (!has_comp && strcmp(zswap_compressor,\n\t\t\t\tCONFIG_ZSWAP_COMPRESSOR_DEFAULT)) {\n\t\tpr_err(\"compressor %s not available, using default %s\\n\",\n\t\t       zswap_compressor, CONFIG_ZSWAP_COMPRESSOR_DEFAULT);\n\t\tparam_free_charp(&zswap_compressor);\n\t\tzswap_compressor = CONFIG_ZSWAP_COMPRESSOR_DEFAULT;\n\t\thas_comp = crypto_has_acomp(zswap_compressor, 0, 0);\n\t}\n\tif (!has_comp) {\n\t\tpr_err(\"default compressor %s not available\\n\",\n\t\t       zswap_compressor);\n\t\tparam_free_charp(&zswap_compressor);\n\t\tzswap_compressor = ZSWAP_PARAM_UNSET;\n\t}\n\n\thas_zpool = zpool_has_pool(zswap_zpool_type);\n\tif (!has_zpool && strcmp(zswap_zpool_type,\n\t\t\t\t CONFIG_ZSWAP_ZPOOL_DEFAULT)) {\n\t\tpr_err(\"zpool %s not available, using default %s\\n\",\n\t\t       zswap_zpool_type, CONFIG_ZSWAP_ZPOOL_DEFAULT);\n\t\tparam_free_charp(&zswap_zpool_type);\n\t\tzswap_zpool_type = CONFIG_ZSWAP_ZPOOL_DEFAULT;\n\t\thas_zpool = zpool_has_pool(zswap_zpool_type);\n\t}\n\tif (!has_zpool) {\n\t\tpr_err(\"default zpool %s not available\\n\",\n\t\t       zswap_zpool_type);\n\t\tparam_free_charp(&zswap_zpool_type);\n\t\tzswap_zpool_type = ZSWAP_PARAM_UNSET;\n\t}\n\n\tif (!has_comp || !has_zpool)\n\t\treturn NULL;\n\n\treturn zswap_pool_create(zswap_zpool_type, zswap_compressor);\n}\n\nstatic void zswap_pool_destroy(struct zswap_pool *pool)\n{\n\tint i;\n\n\tzswap_pool_debug(\"destroying\", pool);\n\n\tcpuhp_state_remove_instance(CPUHP_MM_ZSWP_POOL_PREPARE, &pool->node);\n\tfree_percpu(pool->acomp_ctx);\n\tfor (i = 0; i < ZSWAP_NR_ZPOOLS; i++)\n\t\tzpool_destroy_pool(pool->zpools[i]);\n\tkfree(pool);\n}\n\nstatic int __must_check zswap_pool_get(struct zswap_pool *pool)\n{\n\tif (!pool)\n\t\treturn 0;\n\n\treturn kref_get_unless_zero(&pool->kref);\n}\n\nstatic void __zswap_pool_release(struct work_struct *work)\n{\n\tstruct zswap_pool *pool = container_of(work, typeof(*pool),\n\t\t\t\t\t\trelease_work);\n\n\tsynchronize_rcu();\n\n\t \n\tWARN_ON(kref_get_unless_zero(&pool->kref));\n\n\t \n\tzswap_pool_destroy(pool);\n}\n\nstatic void __zswap_pool_empty(struct kref *kref)\n{\n\tstruct zswap_pool *pool;\n\n\tpool = container_of(kref, typeof(*pool), kref);\n\n\tspin_lock(&zswap_pools_lock);\n\n\tWARN_ON(pool == zswap_pool_current());\n\n\tlist_del_rcu(&pool->list);\n\n\tINIT_WORK(&pool->release_work, __zswap_pool_release);\n\tschedule_work(&pool->release_work);\n\n\tspin_unlock(&zswap_pools_lock);\n}\n\nstatic void zswap_pool_put(struct zswap_pool *pool)\n{\n\tkref_put(&pool->kref, __zswap_pool_empty);\n}\n\n \n\nstatic bool zswap_pool_changed(const char *s, const struct kernel_param *kp)\n{\n\t \n\tif (!strcmp(s, *(char **)kp->arg) && zswap_has_pool)\n\t\treturn false;\n\treturn true;\n}\n\n \nstatic int __zswap_param_set(const char *val, const struct kernel_param *kp,\n\t\t\t     char *type, char *compressor)\n{\n\tstruct zswap_pool *pool, *put_pool = NULL;\n\tchar *s = strstrip((char *)val);\n\tint ret = 0;\n\tbool new_pool = false;\n\n\tmutex_lock(&zswap_init_lock);\n\tswitch (zswap_init_state) {\n\tcase ZSWAP_UNINIT:\n\t\t \n\t\tret = param_set_charp(s, kp);\n\t\tbreak;\n\tcase ZSWAP_INIT_SUCCEED:\n\t\tnew_pool = zswap_pool_changed(s, kp);\n\t\tbreak;\n\tcase ZSWAP_INIT_FAILED:\n\t\tpr_err(\"can't set param, initialization failed\\n\");\n\t\tret = -ENODEV;\n\t}\n\tmutex_unlock(&zswap_init_lock);\n\n\t \n\tif (!new_pool)\n\t\treturn ret;\n\n\tif (!type) {\n\t\tif (!zpool_has_pool(s)) {\n\t\t\tpr_err(\"zpool %s not available\\n\", s);\n\t\t\treturn -ENOENT;\n\t\t}\n\t\ttype = s;\n\t} else if (!compressor) {\n\t\tif (!crypto_has_acomp(s, 0, 0)) {\n\t\t\tpr_err(\"compressor %s not available\\n\", s);\n\t\t\treturn -ENOENT;\n\t\t}\n\t\tcompressor = s;\n\t} else {\n\t\tWARN_ON(1);\n\t\treturn -EINVAL;\n\t}\n\n\tspin_lock(&zswap_pools_lock);\n\n\tpool = zswap_pool_find_get(type, compressor);\n\tif (pool) {\n\t\tzswap_pool_debug(\"using existing\", pool);\n\t\tWARN_ON(pool == zswap_pool_current());\n\t\tlist_del_rcu(&pool->list);\n\t}\n\n\tspin_unlock(&zswap_pools_lock);\n\n\tif (!pool)\n\t\tpool = zswap_pool_create(type, compressor);\n\n\tif (pool)\n\t\tret = param_set_charp(s, kp);\n\telse\n\t\tret = -EINVAL;\n\n\tspin_lock(&zswap_pools_lock);\n\n\tif (!ret) {\n\t\tput_pool = zswap_pool_current();\n\t\tlist_add_rcu(&pool->list, &zswap_pools);\n\t\tzswap_has_pool = true;\n\t} else if (pool) {\n\t\t \n\t\tlist_add_tail_rcu(&pool->list, &zswap_pools);\n\t\tput_pool = pool;\n\t}\n\n\tspin_unlock(&zswap_pools_lock);\n\n\tif (!zswap_has_pool && !pool) {\n\t\t \n\t\tret = param_set_charp(s, kp);\n\t}\n\n\t \n\tif (put_pool)\n\t\tzswap_pool_put(put_pool);\n\n\treturn ret;\n}\n\nstatic int zswap_compressor_param_set(const char *val,\n\t\t\t\t      const struct kernel_param *kp)\n{\n\treturn __zswap_param_set(val, kp, zswap_zpool_type, NULL);\n}\n\nstatic int zswap_zpool_param_set(const char *val,\n\t\t\t\t const struct kernel_param *kp)\n{\n\treturn __zswap_param_set(val, kp, NULL, zswap_compressor);\n}\n\nstatic int zswap_enabled_param_set(const char *val,\n\t\t\t\t   const struct kernel_param *kp)\n{\n\tint ret = -ENODEV;\n\n\t \n\tif (system_state != SYSTEM_RUNNING)\n\t\treturn param_set_bool(val, kp);\n\n\tmutex_lock(&zswap_init_lock);\n\tswitch (zswap_init_state) {\n\tcase ZSWAP_UNINIT:\n\t\tif (zswap_setup())\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase ZSWAP_INIT_SUCCEED:\n\t\tif (!zswap_has_pool)\n\t\t\tpr_err(\"can't enable, no pool configured\\n\");\n\t\telse\n\t\t\tret = param_set_bool(val, kp);\n\t\tbreak;\n\tcase ZSWAP_INIT_FAILED:\n\t\tpr_err(\"can't enable, initialization failed\\n\");\n\t}\n\tmutex_unlock(&zswap_init_lock);\n\n\treturn ret;\n}\n\n \n \nstatic int zswap_writeback_entry(struct zswap_entry *entry,\n\t\t\t\t struct zswap_tree *tree)\n{\n\tswp_entry_t swpentry = entry->swpentry;\n\tstruct page *page;\n\tstruct scatterlist input, output;\n\tstruct crypto_acomp_ctx *acomp_ctx;\n\tstruct zpool *pool = zswap_find_zpool(entry);\n\tbool page_was_allocated;\n\tu8 *src, *tmp = NULL;\n\tunsigned int dlen;\n\tint ret;\n\tstruct writeback_control wbc = {\n\t\t.sync_mode = WB_SYNC_NONE,\n\t};\n\n\tif (!zpool_can_sleep_mapped(pool)) {\n\t\ttmp = kmalloc(PAGE_SIZE, GFP_KERNEL);\n\t\tif (!tmp)\n\t\t\treturn -ENOMEM;\n\t}\n\n\t \n\tpage = __read_swap_cache_async(swpentry, GFP_KERNEL, NULL, 0,\n\t\t\t\t       &page_was_allocated);\n\tif (!page) {\n\t\tret = -ENOMEM;\n\t\tgoto fail;\n\t}\n\n\t \n\tif (!page_was_allocated) {\n\t\tput_page(page);\n\t\tret = -EEXIST;\n\t\tgoto fail;\n\t}\n\n\t \n\tspin_lock(&tree->lock);\n\tif (zswap_rb_search(&tree->rbroot, swp_offset(entry->swpentry)) != entry) {\n\t\tspin_unlock(&tree->lock);\n\t\tdelete_from_swap_cache(page_folio(page));\n\t\tret = -ENOMEM;\n\t\tgoto fail;\n\t}\n\tspin_unlock(&tree->lock);\n\n\t \n\tacomp_ctx = raw_cpu_ptr(entry->pool->acomp_ctx);\n\tdlen = PAGE_SIZE;\n\n\tsrc = zpool_map_handle(pool, entry->handle, ZPOOL_MM_RO);\n\tif (!zpool_can_sleep_mapped(pool)) {\n\t\tmemcpy(tmp, src, entry->length);\n\t\tsrc = tmp;\n\t\tzpool_unmap_handle(pool, entry->handle);\n\t}\n\n\tmutex_lock(acomp_ctx->mutex);\n\tsg_init_one(&input, src, entry->length);\n\tsg_init_table(&output, 1);\n\tsg_set_page(&output, page, PAGE_SIZE, 0);\n\tacomp_request_set_params(acomp_ctx->req, &input, &output, entry->length, dlen);\n\tret = crypto_wait_req(crypto_acomp_decompress(acomp_ctx->req), &acomp_ctx->wait);\n\tdlen = acomp_ctx->req->dlen;\n\tmutex_unlock(acomp_ctx->mutex);\n\n\tif (!zpool_can_sleep_mapped(pool))\n\t\tkfree(tmp);\n\telse\n\t\tzpool_unmap_handle(pool, entry->handle);\n\n\tBUG_ON(ret);\n\tBUG_ON(dlen != PAGE_SIZE);\n\n\t \n\tSetPageUptodate(page);\n\n\t \n\tSetPageReclaim(page);\n\n\t \n\t__swap_writepage(page, &wbc);\n\tput_page(page);\n\tzswap_written_back_pages++;\n\n\treturn ret;\n\nfail:\n\tif (!zpool_can_sleep_mapped(pool))\n\t\tkfree(tmp);\n\n\t \n\treturn ret;\n}\n\nstatic int zswap_is_page_same_filled(void *ptr, unsigned long *value)\n{\n\tunsigned long *page;\n\tunsigned long val;\n\tunsigned int pos, last_pos = PAGE_SIZE / sizeof(*page) - 1;\n\n\tpage = (unsigned long *)ptr;\n\tval = page[0];\n\n\tif (val != page[last_pos])\n\t\treturn 0;\n\n\tfor (pos = 1; pos < last_pos; pos++) {\n\t\tif (val != page[pos])\n\t\t\treturn 0;\n\t}\n\n\t*value = val;\n\n\treturn 1;\n}\n\nstatic void zswap_fill_page(void *ptr, unsigned long value)\n{\n\tunsigned long *page;\n\n\tpage = (unsigned long *)ptr;\n\tmemset_l(page, value, PAGE_SIZE / sizeof(unsigned long));\n}\n\nbool zswap_store(struct folio *folio)\n{\n\tswp_entry_t swp = folio->swap;\n\tint type = swp_type(swp);\n\tpgoff_t offset = swp_offset(swp);\n\tstruct page *page = &folio->page;\n\tstruct zswap_tree *tree = zswap_trees[type];\n\tstruct zswap_entry *entry, *dupentry;\n\tstruct scatterlist input, output;\n\tstruct crypto_acomp_ctx *acomp_ctx;\n\tstruct obj_cgroup *objcg = NULL;\n\tstruct zswap_pool *pool;\n\tstruct zpool *zpool;\n\tunsigned int dlen = PAGE_SIZE;\n\tunsigned long handle, value;\n\tchar *buf;\n\tu8 *src, *dst;\n\tgfp_t gfp;\n\tint ret;\n\n\tVM_WARN_ON_ONCE(!folio_test_locked(folio));\n\tVM_WARN_ON_ONCE(!folio_test_swapcache(folio));\n\n\t \n\tif (folio_test_large(folio))\n\t\treturn false;\n\n\tif (!zswap_enabled || !tree)\n\t\treturn false;\n\n\t \n\tspin_lock(&tree->lock);\n\tdupentry = zswap_rb_search(&tree->rbroot, offset);\n\tif (dupentry) {\n\t\tzswap_duplicate_entry++;\n\t\tzswap_invalidate_entry(tree, dupentry);\n\t}\n\tspin_unlock(&tree->lock);\n\n\t \n\tobjcg = get_obj_cgroup_from_folio(folio);\n\tif (objcg && !obj_cgroup_may_zswap(objcg))\n\t\tgoto reject;\n\n\t \n\tif (zswap_is_full()) {\n\t\tzswap_pool_limit_hit++;\n\t\tzswap_pool_reached_full = true;\n\t\tgoto shrink;\n\t}\n\n\tif (zswap_pool_reached_full) {\n\t       if (!zswap_can_accept())\n\t\t\tgoto shrink;\n\t\telse\n\t\t\tzswap_pool_reached_full = false;\n\t}\n\n\t \n\tentry = zswap_entry_cache_alloc(GFP_KERNEL);\n\tif (!entry) {\n\t\tzswap_reject_kmemcache_fail++;\n\t\tgoto reject;\n\t}\n\n\tif (zswap_same_filled_pages_enabled) {\n\t\tsrc = kmap_atomic(page);\n\t\tif (zswap_is_page_same_filled(src, &value)) {\n\t\t\tkunmap_atomic(src);\n\t\t\tentry->swpentry = swp_entry(type, offset);\n\t\t\tentry->length = 0;\n\t\t\tentry->value = value;\n\t\t\tatomic_inc(&zswap_same_filled_pages);\n\t\t\tgoto insert_entry;\n\t\t}\n\t\tkunmap_atomic(src);\n\t}\n\n\tif (!zswap_non_same_filled_pages_enabled)\n\t\tgoto freepage;\n\n\t \n\tentry->pool = zswap_pool_current_get();\n\tif (!entry->pool)\n\t\tgoto freepage;\n\n\t \n\tacomp_ctx = raw_cpu_ptr(entry->pool->acomp_ctx);\n\n\tmutex_lock(acomp_ctx->mutex);\n\n\tdst = acomp_ctx->dstmem;\n\tsg_init_table(&input, 1);\n\tsg_set_page(&input, page, PAGE_SIZE, 0);\n\n\t \n\tsg_init_one(&output, dst, PAGE_SIZE * 2);\n\tacomp_request_set_params(acomp_ctx->req, &input, &output, PAGE_SIZE, dlen);\n\t \n\tret = crypto_wait_req(crypto_acomp_compress(acomp_ctx->req), &acomp_ctx->wait);\n\tdlen = acomp_ctx->req->dlen;\n\n\tif (ret)\n\t\tgoto put_dstmem;\n\n\t \n\tzpool = zswap_find_zpool(entry);\n\tgfp = __GFP_NORETRY | __GFP_NOWARN | __GFP_KSWAPD_RECLAIM;\n\tif (zpool_malloc_support_movable(zpool))\n\t\tgfp |= __GFP_HIGHMEM | __GFP_MOVABLE;\n\tret = zpool_malloc(zpool, dlen, gfp, &handle);\n\tif (ret == -ENOSPC) {\n\t\tzswap_reject_compress_poor++;\n\t\tgoto put_dstmem;\n\t}\n\tif (ret) {\n\t\tzswap_reject_alloc_fail++;\n\t\tgoto put_dstmem;\n\t}\n\tbuf = zpool_map_handle(zpool, handle, ZPOOL_MM_WO);\n\tmemcpy(buf, dst, dlen);\n\tzpool_unmap_handle(zpool, handle);\n\tmutex_unlock(acomp_ctx->mutex);\n\n\t \n\tentry->swpentry = swp_entry(type, offset);\n\tentry->handle = handle;\n\tentry->length = dlen;\n\ninsert_entry:\n\tentry->objcg = objcg;\n\tif (objcg) {\n\t\tobj_cgroup_charge_zswap(objcg, entry->length);\n\t\t \n\t\tcount_objcg_event(objcg, ZSWPOUT);\n\t}\n\n\t \n\tspin_lock(&tree->lock);\n\t \n\twhile (zswap_rb_insert(&tree->rbroot, entry, &dupentry) == -EEXIST) {\n\t\tWARN_ON(1);\n\t\tzswap_duplicate_entry++;\n\t\tzswap_invalidate_entry(tree, dupentry);\n\t}\n\tif (entry->length) {\n\t\tspin_lock(&entry->pool->lru_lock);\n\t\tlist_add(&entry->lru, &entry->pool->lru);\n\t\tspin_unlock(&entry->pool->lru_lock);\n\t}\n\tspin_unlock(&tree->lock);\n\n\t \n\tatomic_inc(&zswap_stored_pages);\n\tzswap_update_total_size();\n\tcount_vm_event(ZSWPOUT);\n\n\treturn true;\n\nput_dstmem:\n\tmutex_unlock(acomp_ctx->mutex);\n\tzswap_pool_put(entry->pool);\nfreepage:\n\tzswap_entry_cache_free(entry);\nreject:\n\tif (objcg)\n\t\tobj_cgroup_put(objcg);\n\treturn false;\n\nshrink:\n\tpool = zswap_pool_last_get();\n\tif (pool && !queue_work(shrink_wq, &pool->shrink_work))\n\t\tzswap_pool_put(pool);\n\tgoto reject;\n}\n\nbool zswap_load(struct folio *folio)\n{\n\tswp_entry_t swp = folio->swap;\n\tint type = swp_type(swp);\n\tpgoff_t offset = swp_offset(swp);\n\tstruct page *page = &folio->page;\n\tstruct zswap_tree *tree = zswap_trees[type];\n\tstruct zswap_entry *entry;\n\tstruct scatterlist input, output;\n\tstruct crypto_acomp_ctx *acomp_ctx;\n\tu8 *src, *dst, *tmp;\n\tstruct zpool *zpool;\n\tunsigned int dlen;\n\tbool ret;\n\n\tVM_WARN_ON_ONCE(!folio_test_locked(folio));\n\n\t \n\tspin_lock(&tree->lock);\n\tentry = zswap_entry_find_get(&tree->rbroot, offset);\n\tif (!entry) {\n\t\tspin_unlock(&tree->lock);\n\t\treturn false;\n\t}\n\tspin_unlock(&tree->lock);\n\n\tif (!entry->length) {\n\t\tdst = kmap_atomic(page);\n\t\tzswap_fill_page(dst, entry->value);\n\t\tkunmap_atomic(dst);\n\t\tret = true;\n\t\tgoto stats;\n\t}\n\n\tzpool = zswap_find_zpool(entry);\n\tif (!zpool_can_sleep_mapped(zpool)) {\n\t\ttmp = kmalloc(entry->length, GFP_KERNEL);\n\t\tif (!tmp) {\n\t\t\tret = false;\n\t\t\tgoto freeentry;\n\t\t}\n\t}\n\n\t \n\tdlen = PAGE_SIZE;\n\tsrc = zpool_map_handle(zpool, entry->handle, ZPOOL_MM_RO);\n\n\tif (!zpool_can_sleep_mapped(zpool)) {\n\t\tmemcpy(tmp, src, entry->length);\n\t\tsrc = tmp;\n\t\tzpool_unmap_handle(zpool, entry->handle);\n\t}\n\n\tacomp_ctx = raw_cpu_ptr(entry->pool->acomp_ctx);\n\tmutex_lock(acomp_ctx->mutex);\n\tsg_init_one(&input, src, entry->length);\n\tsg_init_table(&output, 1);\n\tsg_set_page(&output, page, PAGE_SIZE, 0);\n\tacomp_request_set_params(acomp_ctx->req, &input, &output, entry->length, dlen);\n\tif (crypto_wait_req(crypto_acomp_decompress(acomp_ctx->req), &acomp_ctx->wait))\n\t\tWARN_ON(1);\n\tmutex_unlock(acomp_ctx->mutex);\n\n\tif (zpool_can_sleep_mapped(zpool))\n\t\tzpool_unmap_handle(zpool, entry->handle);\n\telse\n\t\tkfree(tmp);\n\n\tret = true;\nstats:\n\tcount_vm_event(ZSWPIN);\n\tif (entry->objcg)\n\t\tcount_objcg_event(entry->objcg, ZSWPIN);\nfreeentry:\n\tspin_lock(&tree->lock);\n\tif (ret && zswap_exclusive_loads_enabled) {\n\t\tzswap_invalidate_entry(tree, entry);\n\t\tfolio_mark_dirty(folio);\n\t} else if (entry->length) {\n\t\tspin_lock(&entry->pool->lru_lock);\n\t\tlist_move(&entry->lru, &entry->pool->lru);\n\t\tspin_unlock(&entry->pool->lru_lock);\n\t}\n\tzswap_entry_put(tree, entry);\n\tspin_unlock(&tree->lock);\n\n\treturn ret;\n}\n\nvoid zswap_invalidate(int type, pgoff_t offset)\n{\n\tstruct zswap_tree *tree = zswap_trees[type];\n\tstruct zswap_entry *entry;\n\n\t \n\tspin_lock(&tree->lock);\n\tentry = zswap_rb_search(&tree->rbroot, offset);\n\tif (!entry) {\n\t\t \n\t\tspin_unlock(&tree->lock);\n\t\treturn;\n\t}\n\tzswap_invalidate_entry(tree, entry);\n\tspin_unlock(&tree->lock);\n}\n\nvoid zswap_swapon(int type)\n{\n\tstruct zswap_tree *tree;\n\n\ttree = kzalloc(sizeof(*tree), GFP_KERNEL);\n\tif (!tree) {\n\t\tpr_err(\"alloc failed, zswap disabled for swap type %d\\n\", type);\n\t\treturn;\n\t}\n\n\ttree->rbroot = RB_ROOT;\n\tspin_lock_init(&tree->lock);\n\tzswap_trees[type] = tree;\n}\n\nvoid zswap_swapoff(int type)\n{\n\tstruct zswap_tree *tree = zswap_trees[type];\n\tstruct zswap_entry *entry, *n;\n\n\tif (!tree)\n\t\treturn;\n\n\t \n\tspin_lock(&tree->lock);\n\trbtree_postorder_for_each_entry_safe(entry, n, &tree->rbroot, rbnode)\n\t\tzswap_free_entry(entry);\n\ttree->rbroot = RB_ROOT;\n\tspin_unlock(&tree->lock);\n\tkfree(tree);\n\tzswap_trees[type] = NULL;\n}\n\n \n#ifdef CONFIG_DEBUG_FS\n#include <linux/debugfs.h>\n\nstatic struct dentry *zswap_debugfs_root;\n\nstatic int zswap_debugfs_init(void)\n{\n\tif (!debugfs_initialized())\n\t\treturn -ENODEV;\n\n\tzswap_debugfs_root = debugfs_create_dir(\"zswap\", NULL);\n\n\tdebugfs_create_u64(\"pool_limit_hit\", 0444,\n\t\t\t   zswap_debugfs_root, &zswap_pool_limit_hit);\n\tdebugfs_create_u64(\"reject_reclaim_fail\", 0444,\n\t\t\t   zswap_debugfs_root, &zswap_reject_reclaim_fail);\n\tdebugfs_create_u64(\"reject_alloc_fail\", 0444,\n\t\t\t   zswap_debugfs_root, &zswap_reject_alloc_fail);\n\tdebugfs_create_u64(\"reject_kmemcache_fail\", 0444,\n\t\t\t   zswap_debugfs_root, &zswap_reject_kmemcache_fail);\n\tdebugfs_create_u64(\"reject_compress_poor\", 0444,\n\t\t\t   zswap_debugfs_root, &zswap_reject_compress_poor);\n\tdebugfs_create_u64(\"written_back_pages\", 0444,\n\t\t\t   zswap_debugfs_root, &zswap_written_back_pages);\n\tdebugfs_create_u64(\"duplicate_entry\", 0444,\n\t\t\t   zswap_debugfs_root, &zswap_duplicate_entry);\n\tdebugfs_create_u64(\"pool_total_size\", 0444,\n\t\t\t   zswap_debugfs_root, &zswap_pool_total_size);\n\tdebugfs_create_atomic_t(\"stored_pages\", 0444,\n\t\t\t\tzswap_debugfs_root, &zswap_stored_pages);\n\tdebugfs_create_atomic_t(\"same_filled_pages\", 0444,\n\t\t\t\tzswap_debugfs_root, &zswap_same_filled_pages);\n\n\treturn 0;\n}\n#else\nstatic int zswap_debugfs_init(void)\n{\n\treturn 0;\n}\n#endif\n\n \nstatic int zswap_setup(void)\n{\n\tstruct zswap_pool *pool;\n\tint ret;\n\n\tzswap_entry_cache = KMEM_CACHE(zswap_entry, 0);\n\tif (!zswap_entry_cache) {\n\t\tpr_err(\"entry cache creation failed\\n\");\n\t\tgoto cache_fail;\n\t}\n\n\tret = cpuhp_setup_state(CPUHP_MM_ZSWP_MEM_PREPARE, \"mm/zswap:prepare\",\n\t\t\t\tzswap_dstmem_prepare, zswap_dstmem_dead);\n\tif (ret) {\n\t\tpr_err(\"dstmem alloc failed\\n\");\n\t\tgoto dstmem_fail;\n\t}\n\n\tret = cpuhp_setup_state_multi(CPUHP_MM_ZSWP_POOL_PREPARE,\n\t\t\t\t      \"mm/zswap_pool:prepare\",\n\t\t\t\t      zswap_cpu_comp_prepare,\n\t\t\t\t      zswap_cpu_comp_dead);\n\tif (ret)\n\t\tgoto hp_fail;\n\n\tpool = __zswap_pool_create_fallback();\n\tif (pool) {\n\t\tpr_info(\"loaded using pool %s/%s\\n\", pool->tfm_name,\n\t\t\tzpool_get_type(pool->zpools[0]));\n\t\tlist_add(&pool->list, &zswap_pools);\n\t\tzswap_has_pool = true;\n\t} else {\n\t\tpr_err(\"pool creation failed\\n\");\n\t\tzswap_enabled = false;\n\t}\n\n\tshrink_wq = create_workqueue(\"zswap-shrink\");\n\tif (!shrink_wq)\n\t\tgoto fallback_fail;\n\n\tif (zswap_debugfs_init())\n\t\tpr_warn(\"debugfs initialization failed\\n\");\n\tzswap_init_state = ZSWAP_INIT_SUCCEED;\n\treturn 0;\n\nfallback_fail:\n\tif (pool)\n\t\tzswap_pool_destroy(pool);\nhp_fail:\n\tcpuhp_remove_state(CPUHP_MM_ZSWP_MEM_PREPARE);\ndstmem_fail:\n\tkmem_cache_destroy(zswap_entry_cache);\ncache_fail:\n\t \n\tzswap_init_state = ZSWAP_INIT_FAILED;\n\tzswap_enabled = false;\n\treturn -ENOMEM;\n}\n\nstatic int __init zswap_init(void)\n{\n\tif (!zswap_enabled)\n\t\treturn 0;\n\treturn zswap_setup();\n}\n \nlate_initcall(zswap_init);\n\nMODULE_AUTHOR(\"Seth Jennings <sjennings@variantweb.net>\");\nMODULE_DESCRIPTION(\"Compressed cache for swap pages\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}