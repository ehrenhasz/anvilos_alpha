{
  "module_name": "memremap.c",
  "hash_id": "40574c3cb30345d1cf533a4f48b09ee23b4e4bc7d94c156af98bd5953a4bbf25",
  "original_prompt": "Ingested from linux-6.6.14/mm/memremap.c",
  "human_readable_source": "\n \n#include <linux/device.h>\n#include <linux/io.h>\n#include <linux/kasan.h>\n#include <linux/memory_hotplug.h>\n#include <linux/memremap.h>\n#include <linux/pfn_t.h>\n#include <linux/swap.h>\n#include <linux/mmzone.h>\n#include <linux/swapops.h>\n#include <linux/types.h>\n#include <linux/wait_bit.h>\n#include <linux/xarray.h>\n#include \"internal.h\"\n\nstatic DEFINE_XARRAY(pgmap_array);\n\n \n#ifndef CONFIG_ARCH_HAS_MEMREMAP_COMPAT_ALIGN\nunsigned long memremap_compat_align(void)\n{\n\treturn SUBSECTION_SIZE;\n}\nEXPORT_SYMBOL_GPL(memremap_compat_align);\n#endif\n\n#ifdef CONFIG_FS_DAX\nDEFINE_STATIC_KEY_FALSE(devmap_managed_key);\nEXPORT_SYMBOL(devmap_managed_key);\n\nstatic void devmap_managed_enable_put(struct dev_pagemap *pgmap)\n{\n\tif (pgmap->type == MEMORY_DEVICE_FS_DAX)\n\t\tstatic_branch_dec(&devmap_managed_key);\n}\n\nstatic void devmap_managed_enable_get(struct dev_pagemap *pgmap)\n{\n\tif (pgmap->type == MEMORY_DEVICE_FS_DAX)\n\t\tstatic_branch_inc(&devmap_managed_key);\n}\n#else\nstatic void devmap_managed_enable_get(struct dev_pagemap *pgmap)\n{\n}\nstatic void devmap_managed_enable_put(struct dev_pagemap *pgmap)\n{\n}\n#endif  \n\nstatic void pgmap_array_delete(struct range *range)\n{\n\txa_store_range(&pgmap_array, PHYS_PFN(range->start), PHYS_PFN(range->end),\n\t\t\tNULL, GFP_KERNEL);\n\tsynchronize_rcu();\n}\n\nstatic unsigned long pfn_first(struct dev_pagemap *pgmap, int range_id)\n{\n\tstruct range *range = &pgmap->ranges[range_id];\n\tunsigned long pfn = PHYS_PFN(range->start);\n\n\tif (range_id)\n\t\treturn pfn;\n\treturn pfn + vmem_altmap_offset(pgmap_altmap(pgmap));\n}\n\nbool pgmap_pfn_valid(struct dev_pagemap *pgmap, unsigned long pfn)\n{\n\tint i;\n\n\tfor (i = 0; i < pgmap->nr_range; i++) {\n\t\tstruct range *range = &pgmap->ranges[i];\n\n\t\tif (pfn >= PHYS_PFN(range->start) &&\n\t\t    pfn <= PHYS_PFN(range->end))\n\t\t\treturn pfn >= pfn_first(pgmap, i);\n\t}\n\n\treturn false;\n}\n\nstatic unsigned long pfn_end(struct dev_pagemap *pgmap, int range_id)\n{\n\tconst struct range *range = &pgmap->ranges[range_id];\n\n\treturn (range->start + range_len(range)) >> PAGE_SHIFT;\n}\n\nstatic unsigned long pfn_len(struct dev_pagemap *pgmap, unsigned long range_id)\n{\n\treturn (pfn_end(pgmap, range_id) -\n\t\tpfn_first(pgmap, range_id)) >> pgmap->vmemmap_shift;\n}\n\nstatic void pageunmap_range(struct dev_pagemap *pgmap, int range_id)\n{\n\tstruct range *range = &pgmap->ranges[range_id];\n\tstruct page *first_page;\n\n\t \n\tfirst_page = pfn_to_page(pfn_first(pgmap, range_id));\n\n\t \n\tmem_hotplug_begin();\n\tremove_pfn_range_from_zone(page_zone(first_page), PHYS_PFN(range->start),\n\t\t\t\t   PHYS_PFN(range_len(range)));\n\tif (pgmap->type == MEMORY_DEVICE_PRIVATE) {\n\t\t__remove_pages(PHYS_PFN(range->start),\n\t\t\t       PHYS_PFN(range_len(range)), NULL);\n\t} else {\n\t\tarch_remove_memory(range->start, range_len(range),\n\t\t\t\tpgmap_altmap(pgmap));\n\t\tkasan_remove_zero_shadow(__va(range->start), range_len(range));\n\t}\n\tmem_hotplug_done();\n\n\tuntrack_pfn(NULL, PHYS_PFN(range->start), range_len(range), true);\n\tpgmap_array_delete(range);\n}\n\nvoid memunmap_pages(struct dev_pagemap *pgmap)\n{\n\tint i;\n\n\tpercpu_ref_kill(&pgmap->ref);\n\tif (pgmap->type != MEMORY_DEVICE_PRIVATE &&\n\t    pgmap->type != MEMORY_DEVICE_COHERENT)\n\t\tfor (i = 0; i < pgmap->nr_range; i++)\n\t\t\tpercpu_ref_put_many(&pgmap->ref, pfn_len(pgmap, i));\n\n\twait_for_completion(&pgmap->done);\n\n\tfor (i = 0; i < pgmap->nr_range; i++)\n\t\tpageunmap_range(pgmap, i);\n\tpercpu_ref_exit(&pgmap->ref);\n\n\tWARN_ONCE(pgmap->altmap.alloc, \"failed to free all reserved pages\\n\");\n\tdevmap_managed_enable_put(pgmap);\n}\nEXPORT_SYMBOL_GPL(memunmap_pages);\n\nstatic void devm_memremap_pages_release(void *data)\n{\n\tmemunmap_pages(data);\n}\n\nstatic void dev_pagemap_percpu_release(struct percpu_ref *ref)\n{\n\tstruct dev_pagemap *pgmap = container_of(ref, struct dev_pagemap, ref);\n\n\tcomplete(&pgmap->done);\n}\n\nstatic int pagemap_range(struct dev_pagemap *pgmap, struct mhp_params *params,\n\t\tint range_id, int nid)\n{\n\tconst bool is_private = pgmap->type == MEMORY_DEVICE_PRIVATE;\n\tstruct range *range = &pgmap->ranges[range_id];\n\tstruct dev_pagemap *conflict_pgmap;\n\tint error, is_ram;\n\n\tif (WARN_ONCE(pgmap_altmap(pgmap) && range_id > 0,\n\t\t\t\t\"altmap not supported for multiple ranges\\n\"))\n\t\treturn -EINVAL;\n\n\tconflict_pgmap = get_dev_pagemap(PHYS_PFN(range->start), NULL);\n\tif (conflict_pgmap) {\n\t\tWARN(1, \"Conflicting mapping in same section\\n\");\n\t\tput_dev_pagemap(conflict_pgmap);\n\t\treturn -ENOMEM;\n\t}\n\n\tconflict_pgmap = get_dev_pagemap(PHYS_PFN(range->end), NULL);\n\tif (conflict_pgmap) {\n\t\tWARN(1, \"Conflicting mapping in same section\\n\");\n\t\tput_dev_pagemap(conflict_pgmap);\n\t\treturn -ENOMEM;\n\t}\n\n\tis_ram = region_intersects(range->start, range_len(range),\n\t\tIORESOURCE_SYSTEM_RAM, IORES_DESC_NONE);\n\n\tif (is_ram != REGION_DISJOINT) {\n\t\tWARN_ONCE(1, \"attempted on %s region %#llx-%#llx\\n\",\n\t\t\t\tis_ram == REGION_MIXED ? \"mixed\" : \"ram\",\n\t\t\t\trange->start, range->end);\n\t\treturn -ENXIO;\n\t}\n\n\terror = xa_err(xa_store_range(&pgmap_array, PHYS_PFN(range->start),\n\t\t\t\tPHYS_PFN(range->end), pgmap, GFP_KERNEL));\n\tif (error)\n\t\treturn error;\n\n\tif (nid < 0)\n\t\tnid = numa_mem_id();\n\n\terror = track_pfn_remap(NULL, &params->pgprot, PHYS_PFN(range->start), 0,\n\t\t\trange_len(range));\n\tif (error)\n\t\tgoto err_pfn_remap;\n\n\tif (!mhp_range_allowed(range->start, range_len(range), !is_private)) {\n\t\terror = -EINVAL;\n\t\tgoto err_kasan;\n\t}\n\n\tmem_hotplug_begin();\n\n\t \n\tif (is_private) {\n\t\terror = add_pages(nid, PHYS_PFN(range->start),\n\t\t\t\tPHYS_PFN(range_len(range)), params);\n\t} else {\n\t\terror = kasan_add_zero_shadow(__va(range->start), range_len(range));\n\t\tif (error) {\n\t\t\tmem_hotplug_done();\n\t\t\tgoto err_kasan;\n\t\t}\n\n\t\terror = arch_add_memory(nid, range->start, range_len(range),\n\t\t\t\t\tparams);\n\t}\n\n\tif (!error) {\n\t\tstruct zone *zone;\n\n\t\tzone = &NODE_DATA(nid)->node_zones[ZONE_DEVICE];\n\t\tmove_pfn_range_to_zone(zone, PHYS_PFN(range->start),\n\t\t\t\tPHYS_PFN(range_len(range)), params->altmap,\n\t\t\t\tMIGRATE_MOVABLE);\n\t}\n\n\tmem_hotplug_done();\n\tif (error)\n\t\tgoto err_add_memory;\n\n\t \n\tmemmap_init_zone_device(&NODE_DATA(nid)->node_zones[ZONE_DEVICE],\n\t\t\t\tPHYS_PFN(range->start),\n\t\t\t\tPHYS_PFN(range_len(range)), pgmap);\n\tif (pgmap->type != MEMORY_DEVICE_PRIVATE &&\n\t    pgmap->type != MEMORY_DEVICE_COHERENT)\n\t\tpercpu_ref_get_many(&pgmap->ref, pfn_len(pgmap, range_id));\n\treturn 0;\n\nerr_add_memory:\n\tif (!is_private)\n\t\tkasan_remove_zero_shadow(__va(range->start), range_len(range));\nerr_kasan:\n\tuntrack_pfn(NULL, PHYS_PFN(range->start), range_len(range), true);\nerr_pfn_remap:\n\tpgmap_array_delete(range);\n\treturn error;\n}\n\n\n \nvoid *memremap_pages(struct dev_pagemap *pgmap, int nid)\n{\n\tstruct mhp_params params = {\n\t\t.altmap = pgmap_altmap(pgmap),\n\t\t.pgmap = pgmap,\n\t\t.pgprot = PAGE_KERNEL,\n\t};\n\tconst int nr_range = pgmap->nr_range;\n\tint error, i;\n\n\tif (WARN_ONCE(!nr_range, \"nr_range must be specified\\n\"))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tswitch (pgmap->type) {\n\tcase MEMORY_DEVICE_PRIVATE:\n\t\tif (!IS_ENABLED(CONFIG_DEVICE_PRIVATE)) {\n\t\t\tWARN(1, \"Device private memory not supported\\n\");\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\t}\n\t\tif (!pgmap->ops || !pgmap->ops->migrate_to_ram) {\n\t\t\tWARN(1, \"Missing migrate_to_ram method\\n\");\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\t}\n\t\tif (!pgmap->ops->page_free) {\n\t\t\tWARN(1, \"Missing page_free method\\n\");\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\t}\n\t\tif (!pgmap->owner) {\n\t\t\tWARN(1, \"Missing owner\\n\");\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\t}\n\t\tbreak;\n\tcase MEMORY_DEVICE_COHERENT:\n\t\tif (!pgmap->ops->page_free) {\n\t\t\tWARN(1, \"Missing page_free method\\n\");\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\t}\n\t\tif (!pgmap->owner) {\n\t\t\tWARN(1, \"Missing owner\\n\");\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\t}\n\t\tbreak;\n\tcase MEMORY_DEVICE_FS_DAX:\n\t\tif (IS_ENABLED(CONFIG_FS_DAX_LIMITED)) {\n\t\t\tWARN(1, \"File system DAX not supported\\n\");\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\t}\n\t\tparams.pgprot = pgprot_decrypted(params.pgprot);\n\t\tbreak;\n\tcase MEMORY_DEVICE_GENERIC:\n\t\tbreak;\n\tcase MEMORY_DEVICE_PCI_P2PDMA:\n\t\tparams.pgprot = pgprot_noncached(params.pgprot);\n\t\tbreak;\n\tdefault:\n\t\tWARN(1, \"Invalid pgmap type %d\\n\", pgmap->type);\n\t\tbreak;\n\t}\n\n\tinit_completion(&pgmap->done);\n\terror = percpu_ref_init(&pgmap->ref, dev_pagemap_percpu_release, 0,\n\t\t\t\tGFP_KERNEL);\n\tif (error)\n\t\treturn ERR_PTR(error);\n\n\tdevmap_managed_enable_get(pgmap);\n\n\t \n\tpgmap->nr_range = 0;\n\terror = 0;\n\tfor (i = 0; i < nr_range; i++) {\n\t\terror = pagemap_range(pgmap, &params, i, nid);\n\t\tif (error)\n\t\t\tbreak;\n\t\tpgmap->nr_range++;\n\t}\n\n\tif (i < nr_range) {\n\t\tmemunmap_pages(pgmap);\n\t\tpgmap->nr_range = nr_range;\n\t\treturn ERR_PTR(error);\n\t}\n\n\treturn __va(pgmap->ranges[0].start);\n}\nEXPORT_SYMBOL_GPL(memremap_pages);\n\n \nvoid *devm_memremap_pages(struct device *dev, struct dev_pagemap *pgmap)\n{\n\tint error;\n\tvoid *ret;\n\n\tret = memremap_pages(pgmap, dev_to_node(dev));\n\tif (IS_ERR(ret))\n\t\treturn ret;\n\n\terror = devm_add_action_or_reset(dev, devm_memremap_pages_release,\n\t\t\tpgmap);\n\tif (error)\n\t\treturn ERR_PTR(error);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(devm_memremap_pages);\n\nvoid devm_memunmap_pages(struct device *dev, struct dev_pagemap *pgmap)\n{\n\tdevm_release_action(dev, devm_memremap_pages_release, pgmap);\n}\nEXPORT_SYMBOL_GPL(devm_memunmap_pages);\n\nunsigned long vmem_altmap_offset(struct vmem_altmap *altmap)\n{\n\t \n\tif (altmap)\n\t\treturn altmap->reserve + altmap->free;\n\treturn 0;\n}\n\nvoid vmem_altmap_free(struct vmem_altmap *altmap, unsigned long nr_pfns)\n{\n\taltmap->alloc -= nr_pfns;\n}\n\n \nstruct dev_pagemap *get_dev_pagemap(unsigned long pfn,\n\t\tstruct dev_pagemap *pgmap)\n{\n\tresource_size_t phys = PFN_PHYS(pfn);\n\n\t \n\tif (pgmap) {\n\t\tif (phys >= pgmap->range.start && phys <= pgmap->range.end)\n\t\t\treturn pgmap;\n\t\tput_dev_pagemap(pgmap);\n\t}\n\n\t \n\trcu_read_lock();\n\tpgmap = xa_load(&pgmap_array, PHYS_PFN(phys));\n\tif (pgmap && !percpu_ref_tryget_live_rcu(&pgmap->ref))\n\t\tpgmap = NULL;\n\trcu_read_unlock();\n\n\treturn pgmap;\n}\nEXPORT_SYMBOL_GPL(get_dev_pagemap);\n\nvoid free_zone_device_page(struct page *page)\n{\n\tif (WARN_ON_ONCE(!page->pgmap->ops || !page->pgmap->ops->page_free))\n\t\treturn;\n\n\tmem_cgroup_uncharge(page_folio(page));\n\n\t \n\tVM_BUG_ON_PAGE(PageAnon(page) && PageCompound(page), page);\n\tif (PageAnon(page))\n\t\t__ClearPageAnonExclusive(page);\n\n\t \n\tpage->mapping = NULL;\n\tpage->pgmap->ops->page_free(page);\n\n\tif (page->pgmap->type != MEMORY_DEVICE_PRIVATE &&\n\t    page->pgmap->type != MEMORY_DEVICE_COHERENT)\n\t\t \n\t\tset_page_count(page, 1);\n\telse\n\t\tput_dev_pagemap(page->pgmap);\n}\n\nvoid zone_device_page_init(struct page *page)\n{\n\t \n\tWARN_ON_ONCE(!percpu_ref_tryget_live(&page->pgmap->ref));\n\tset_page_count(page, 1);\n\tlock_page(page);\n}\nEXPORT_SYMBOL_GPL(zone_device_page_init);\n\n#ifdef CONFIG_FS_DAX\nbool __put_devmap_managed_page_refs(struct page *page, int refs)\n{\n\tif (page->pgmap->type != MEMORY_DEVICE_FS_DAX)\n\t\treturn false;\n\n\t \n\tif (page_ref_sub_return(page, refs) == 1)\n\t\twake_up_var(&page->_refcount);\n\treturn true;\n}\nEXPORT_SYMBOL(__put_devmap_managed_page_refs);\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}