{
  "module_name": "swapfile.c",
  "hash_id": "df47d86f6dea19c3b593fa3051f874ad5c498ff488ed5a239eba64dfd594ae8e",
  "original_prompt": "Ingested from linux-6.6.14/mm/swapfile.c",
  "human_readable_source": "\n \n\n#include <linux/blkdev.h>\n#include <linux/mm.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/task.h>\n#include <linux/hugetlb.h>\n#include <linux/mman.h>\n#include <linux/slab.h>\n#include <linux/kernel_stat.h>\n#include <linux/swap.h>\n#include <linux/vmalloc.h>\n#include <linux/pagemap.h>\n#include <linux/namei.h>\n#include <linux/shmem_fs.h>\n#include <linux/blk-cgroup.h>\n#include <linux/random.h>\n#include <linux/writeback.h>\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <linux/init.h>\n#include <linux/ksm.h>\n#include <linux/rmap.h>\n#include <linux/security.h>\n#include <linux/backing-dev.h>\n#include <linux/mutex.h>\n#include <linux/capability.h>\n#include <linux/syscalls.h>\n#include <linux/memcontrol.h>\n#include <linux/poll.h>\n#include <linux/oom.h>\n#include <linux/swapfile.h>\n#include <linux/export.h>\n#include <linux/swap_slots.h>\n#include <linux/sort.h>\n#include <linux/completion.h>\n#include <linux/suspend.h>\n#include <linux/zswap.h>\n\n#include <asm/tlbflush.h>\n#include <linux/swapops.h>\n#include <linux/swap_cgroup.h>\n#include \"internal.h\"\n#include \"swap.h\"\n\nstatic bool swap_count_continued(struct swap_info_struct *, pgoff_t,\n\t\t\t\t unsigned char);\nstatic void free_swap_count_continuations(struct swap_info_struct *);\n\nstatic DEFINE_SPINLOCK(swap_lock);\nstatic unsigned int nr_swapfiles;\natomic_long_t nr_swap_pages;\n \nEXPORT_SYMBOL_GPL(nr_swap_pages);\n \nlong total_swap_pages;\nstatic int least_priority = -1;\nunsigned long swapfile_maximum_size;\n#ifdef CONFIG_MIGRATION\nbool swap_migration_ad_supported;\n#endif\t \n\nstatic const char Bad_file[] = \"Bad swap file entry \";\nstatic const char Unused_file[] = \"Unused swap file entry \";\nstatic const char Bad_offset[] = \"Bad swap offset entry \";\nstatic const char Unused_offset[] = \"Unused swap offset entry \";\n\n \nstatic PLIST_HEAD(swap_active_head);\n\n \nstatic struct plist_head *swap_avail_heads;\nstatic DEFINE_SPINLOCK(swap_avail_lock);\n\nstatic struct swap_info_struct *swap_info[MAX_SWAPFILES];\n\nstatic DEFINE_MUTEX(swapon_mutex);\n\nstatic DECLARE_WAIT_QUEUE_HEAD(proc_poll_wait);\n \nstatic atomic_t proc_poll_event = ATOMIC_INIT(0);\n\natomic_t nr_rotate_swap = ATOMIC_INIT(0);\n\nstatic struct swap_info_struct *swap_type_to_swap_info(int type)\n{\n\tif (type >= MAX_SWAPFILES)\n\t\treturn NULL;\n\n\treturn READ_ONCE(swap_info[type]);  \n}\n\nstatic inline unsigned char swap_count(unsigned char ent)\n{\n\treturn ent & ~SWAP_HAS_CACHE;\t \n}\n\n \n#define TTRS_ANYWAY\t\t0x1\n \n#define TTRS_UNMAPPED\t\t0x2\n \n#define TTRS_FULL\t\t0x4\n\n \nstatic int __try_to_reclaim_swap(struct swap_info_struct *si,\n\t\t\t\t unsigned long offset, unsigned long flags)\n{\n\tswp_entry_t entry = swp_entry(si->type, offset);\n\tstruct folio *folio;\n\tint ret = 0;\n\n\tfolio = filemap_get_folio(swap_address_space(entry), offset);\n\tif (IS_ERR(folio))\n\t\treturn 0;\n\t \n\tif (folio_trylock(folio)) {\n\t\tif ((flags & TTRS_ANYWAY) ||\n\t\t    ((flags & TTRS_UNMAPPED) && !folio_mapped(folio)) ||\n\t\t    ((flags & TTRS_FULL) && mem_cgroup_swap_full(folio)))\n\t\t\tret = folio_free_swap(folio);\n\t\tfolio_unlock(folio);\n\t}\n\tfolio_put(folio);\n\treturn ret;\n}\n\nstatic inline struct swap_extent *first_se(struct swap_info_struct *sis)\n{\n\tstruct rb_node *rb = rb_first(&sis->swap_extent_root);\n\treturn rb_entry(rb, struct swap_extent, rb_node);\n}\n\nstatic inline struct swap_extent *next_se(struct swap_extent *se)\n{\n\tstruct rb_node *rb = rb_next(&se->rb_node);\n\treturn rb ? rb_entry(rb, struct swap_extent, rb_node) : NULL;\n}\n\n \nstatic int discard_swap(struct swap_info_struct *si)\n{\n\tstruct swap_extent *se;\n\tsector_t start_block;\n\tsector_t nr_blocks;\n\tint err = 0;\n\n\t \n\tse = first_se(si);\n\tstart_block = (se->start_block + 1) << (PAGE_SHIFT - 9);\n\tnr_blocks = ((sector_t)se->nr_pages - 1) << (PAGE_SHIFT - 9);\n\tif (nr_blocks) {\n\t\terr = blkdev_issue_discard(si->bdev, start_block,\n\t\t\t\tnr_blocks, GFP_KERNEL);\n\t\tif (err)\n\t\t\treturn err;\n\t\tcond_resched();\n\t}\n\n\tfor (se = next_se(se); se; se = next_se(se)) {\n\t\tstart_block = se->start_block << (PAGE_SHIFT - 9);\n\t\tnr_blocks = (sector_t)se->nr_pages << (PAGE_SHIFT - 9);\n\n\t\terr = blkdev_issue_discard(si->bdev, start_block,\n\t\t\t\tnr_blocks, GFP_KERNEL);\n\t\tif (err)\n\t\t\tbreak;\n\n\t\tcond_resched();\n\t}\n\treturn err;\t\t \n}\n\nstatic struct swap_extent *\noffset_to_swap_extent(struct swap_info_struct *sis, unsigned long offset)\n{\n\tstruct swap_extent *se;\n\tstruct rb_node *rb;\n\n\trb = sis->swap_extent_root.rb_node;\n\twhile (rb) {\n\t\tse = rb_entry(rb, struct swap_extent, rb_node);\n\t\tif (offset < se->start_page)\n\t\t\trb = rb->rb_left;\n\t\telse if (offset >= se->start_page + se->nr_pages)\n\t\t\trb = rb->rb_right;\n\t\telse\n\t\t\treturn se;\n\t}\n\t \n\tBUG();\n}\n\nsector_t swap_page_sector(struct page *page)\n{\n\tstruct swap_info_struct *sis = page_swap_info(page);\n\tstruct swap_extent *se;\n\tsector_t sector;\n\tpgoff_t offset;\n\n\toffset = __page_file_index(page);\n\tse = offset_to_swap_extent(sis, offset);\n\tsector = se->start_block + (offset - se->start_page);\n\treturn sector << (PAGE_SHIFT - 9);\n}\n\n \nstatic void discard_swap_cluster(struct swap_info_struct *si,\n\t\t\t\t pgoff_t start_page, pgoff_t nr_pages)\n{\n\tstruct swap_extent *se = offset_to_swap_extent(si, start_page);\n\n\twhile (nr_pages) {\n\t\tpgoff_t offset = start_page - se->start_page;\n\t\tsector_t start_block = se->start_block + offset;\n\t\tsector_t nr_blocks = se->nr_pages - offset;\n\n\t\tif (nr_blocks > nr_pages)\n\t\t\tnr_blocks = nr_pages;\n\t\tstart_page += nr_blocks;\n\t\tnr_pages -= nr_blocks;\n\n\t\tstart_block <<= PAGE_SHIFT - 9;\n\t\tnr_blocks <<= PAGE_SHIFT - 9;\n\t\tif (blkdev_issue_discard(si->bdev, start_block,\n\t\t\t\t\tnr_blocks, GFP_NOIO))\n\t\t\tbreak;\n\n\t\tse = next_se(se);\n\t}\n}\n\n#ifdef CONFIG_THP_SWAP\n#define SWAPFILE_CLUSTER\tHPAGE_PMD_NR\n\n#define swap_entry_size(size)\t(size)\n#else\n#define SWAPFILE_CLUSTER\t256\n\n \n#define swap_entry_size(size)\t1\n#endif\n#define LATENCY_LIMIT\t\t256\n\nstatic inline void cluster_set_flag(struct swap_cluster_info *info,\n\tunsigned int flag)\n{\n\tinfo->flags = flag;\n}\n\nstatic inline unsigned int cluster_count(struct swap_cluster_info *info)\n{\n\treturn info->data;\n}\n\nstatic inline void cluster_set_count(struct swap_cluster_info *info,\n\t\t\t\t     unsigned int c)\n{\n\tinfo->data = c;\n}\n\nstatic inline void cluster_set_count_flag(struct swap_cluster_info *info,\n\t\t\t\t\t unsigned int c, unsigned int f)\n{\n\tinfo->flags = f;\n\tinfo->data = c;\n}\n\nstatic inline unsigned int cluster_next(struct swap_cluster_info *info)\n{\n\treturn info->data;\n}\n\nstatic inline void cluster_set_next(struct swap_cluster_info *info,\n\t\t\t\t    unsigned int n)\n{\n\tinfo->data = n;\n}\n\nstatic inline void cluster_set_next_flag(struct swap_cluster_info *info,\n\t\t\t\t\t unsigned int n, unsigned int f)\n{\n\tinfo->flags = f;\n\tinfo->data = n;\n}\n\nstatic inline bool cluster_is_free(struct swap_cluster_info *info)\n{\n\treturn info->flags & CLUSTER_FLAG_FREE;\n}\n\nstatic inline bool cluster_is_null(struct swap_cluster_info *info)\n{\n\treturn info->flags & CLUSTER_FLAG_NEXT_NULL;\n}\n\nstatic inline void cluster_set_null(struct swap_cluster_info *info)\n{\n\tinfo->flags = CLUSTER_FLAG_NEXT_NULL;\n\tinfo->data = 0;\n}\n\nstatic inline bool cluster_is_huge(struct swap_cluster_info *info)\n{\n\tif (IS_ENABLED(CONFIG_THP_SWAP))\n\t\treturn info->flags & CLUSTER_FLAG_HUGE;\n\treturn false;\n}\n\nstatic inline void cluster_clear_huge(struct swap_cluster_info *info)\n{\n\tinfo->flags &= ~CLUSTER_FLAG_HUGE;\n}\n\nstatic inline struct swap_cluster_info *lock_cluster(struct swap_info_struct *si,\n\t\t\t\t\t\t     unsigned long offset)\n{\n\tstruct swap_cluster_info *ci;\n\n\tci = si->cluster_info;\n\tif (ci) {\n\t\tci += offset / SWAPFILE_CLUSTER;\n\t\tspin_lock(&ci->lock);\n\t}\n\treturn ci;\n}\n\nstatic inline void unlock_cluster(struct swap_cluster_info *ci)\n{\n\tif (ci)\n\t\tspin_unlock(&ci->lock);\n}\n\n \nstatic inline struct swap_cluster_info *lock_cluster_or_swap_info(\n\t\tstruct swap_info_struct *si, unsigned long offset)\n{\n\tstruct swap_cluster_info *ci;\n\n\t \n\tci = lock_cluster(si, offset);\n\t \n\tif (!ci)\n\t\tspin_lock(&si->lock);\n\n\treturn ci;\n}\n\nstatic inline void unlock_cluster_or_swap_info(struct swap_info_struct *si,\n\t\t\t\t\t       struct swap_cluster_info *ci)\n{\n\tif (ci)\n\t\tunlock_cluster(ci);\n\telse\n\t\tspin_unlock(&si->lock);\n}\n\nstatic inline bool cluster_list_empty(struct swap_cluster_list *list)\n{\n\treturn cluster_is_null(&list->head);\n}\n\nstatic inline unsigned int cluster_list_first(struct swap_cluster_list *list)\n{\n\treturn cluster_next(&list->head);\n}\n\nstatic void cluster_list_init(struct swap_cluster_list *list)\n{\n\tcluster_set_null(&list->head);\n\tcluster_set_null(&list->tail);\n}\n\nstatic void cluster_list_add_tail(struct swap_cluster_list *list,\n\t\t\t\t  struct swap_cluster_info *ci,\n\t\t\t\t  unsigned int idx)\n{\n\tif (cluster_list_empty(list)) {\n\t\tcluster_set_next_flag(&list->head, idx, 0);\n\t\tcluster_set_next_flag(&list->tail, idx, 0);\n\t} else {\n\t\tstruct swap_cluster_info *ci_tail;\n\t\tunsigned int tail = cluster_next(&list->tail);\n\n\t\t \n\t\tci_tail = ci + tail;\n\t\tspin_lock_nested(&ci_tail->lock, SINGLE_DEPTH_NESTING);\n\t\tcluster_set_next(ci_tail, idx);\n\t\tspin_unlock(&ci_tail->lock);\n\t\tcluster_set_next_flag(&list->tail, idx, 0);\n\t}\n}\n\nstatic unsigned int cluster_list_del_first(struct swap_cluster_list *list,\n\t\t\t\t\t   struct swap_cluster_info *ci)\n{\n\tunsigned int idx;\n\n\tidx = cluster_next(&list->head);\n\tif (cluster_next(&list->tail) == idx) {\n\t\tcluster_set_null(&list->head);\n\t\tcluster_set_null(&list->tail);\n\t} else\n\t\tcluster_set_next_flag(&list->head,\n\t\t\t\t      cluster_next(&ci[idx]), 0);\n\n\treturn idx;\n}\n\n \nstatic void swap_cluster_schedule_discard(struct swap_info_struct *si,\n\t\tunsigned int idx)\n{\n\t \n\tmemset(si->swap_map + idx * SWAPFILE_CLUSTER,\n\t\t\tSWAP_MAP_BAD, SWAPFILE_CLUSTER);\n\n\tcluster_list_add_tail(&si->discard_clusters, si->cluster_info, idx);\n\n\tschedule_work(&si->discard_work);\n}\n\nstatic void __free_cluster(struct swap_info_struct *si, unsigned long idx)\n{\n\tstruct swap_cluster_info *ci = si->cluster_info;\n\n\tcluster_set_flag(ci + idx, CLUSTER_FLAG_FREE);\n\tcluster_list_add_tail(&si->free_clusters, ci, idx);\n}\n\n \nstatic void swap_do_scheduled_discard(struct swap_info_struct *si)\n{\n\tstruct swap_cluster_info *info, *ci;\n\tunsigned int idx;\n\n\tinfo = si->cluster_info;\n\n\twhile (!cluster_list_empty(&si->discard_clusters)) {\n\t\tidx = cluster_list_del_first(&si->discard_clusters, info);\n\t\tspin_unlock(&si->lock);\n\n\t\tdiscard_swap_cluster(si, idx * SWAPFILE_CLUSTER,\n\t\t\t\tSWAPFILE_CLUSTER);\n\n\t\tspin_lock(&si->lock);\n\t\tci = lock_cluster(si, idx * SWAPFILE_CLUSTER);\n\t\t__free_cluster(si, idx);\n\t\tmemset(si->swap_map + idx * SWAPFILE_CLUSTER,\n\t\t\t\t0, SWAPFILE_CLUSTER);\n\t\tunlock_cluster(ci);\n\t}\n}\n\nstatic void swap_discard_work(struct work_struct *work)\n{\n\tstruct swap_info_struct *si;\n\n\tsi = container_of(work, struct swap_info_struct, discard_work);\n\n\tspin_lock(&si->lock);\n\tswap_do_scheduled_discard(si);\n\tspin_unlock(&si->lock);\n}\n\nstatic void swap_users_ref_free(struct percpu_ref *ref)\n{\n\tstruct swap_info_struct *si;\n\n\tsi = container_of(ref, struct swap_info_struct, users);\n\tcomplete(&si->comp);\n}\n\nstatic void alloc_cluster(struct swap_info_struct *si, unsigned long idx)\n{\n\tstruct swap_cluster_info *ci = si->cluster_info;\n\n\tVM_BUG_ON(cluster_list_first(&si->free_clusters) != idx);\n\tcluster_list_del_first(&si->free_clusters, ci);\n\tcluster_set_count_flag(ci + idx, 0, 0);\n}\n\nstatic void free_cluster(struct swap_info_struct *si, unsigned long idx)\n{\n\tstruct swap_cluster_info *ci = si->cluster_info + idx;\n\n\tVM_BUG_ON(cluster_count(ci) != 0);\n\t \n\tif ((si->flags & (SWP_WRITEOK | SWP_PAGE_DISCARD)) ==\n\t    (SWP_WRITEOK | SWP_PAGE_DISCARD)) {\n\t\tswap_cluster_schedule_discard(si, idx);\n\t\treturn;\n\t}\n\n\t__free_cluster(si, idx);\n}\n\n \nstatic void inc_cluster_info_page(struct swap_info_struct *p,\n\tstruct swap_cluster_info *cluster_info, unsigned long page_nr)\n{\n\tunsigned long idx = page_nr / SWAPFILE_CLUSTER;\n\n\tif (!cluster_info)\n\t\treturn;\n\tif (cluster_is_free(&cluster_info[idx]))\n\t\talloc_cluster(p, idx);\n\n\tVM_BUG_ON(cluster_count(&cluster_info[idx]) >= SWAPFILE_CLUSTER);\n\tcluster_set_count(&cluster_info[idx],\n\t\tcluster_count(&cluster_info[idx]) + 1);\n}\n\n \nstatic void dec_cluster_info_page(struct swap_info_struct *p,\n\tstruct swap_cluster_info *cluster_info, unsigned long page_nr)\n{\n\tunsigned long idx = page_nr / SWAPFILE_CLUSTER;\n\n\tif (!cluster_info)\n\t\treturn;\n\n\tVM_BUG_ON(cluster_count(&cluster_info[idx]) == 0);\n\tcluster_set_count(&cluster_info[idx],\n\t\tcluster_count(&cluster_info[idx]) - 1);\n\n\tif (cluster_count(&cluster_info[idx]) == 0)\n\t\tfree_cluster(p, idx);\n}\n\n \nstatic bool\nscan_swap_map_ssd_cluster_conflict(struct swap_info_struct *si,\n\tunsigned long offset)\n{\n\tstruct percpu_cluster *percpu_cluster;\n\tbool conflict;\n\n\toffset /= SWAPFILE_CLUSTER;\n\tconflict = !cluster_list_empty(&si->free_clusters) &&\n\t\toffset != cluster_list_first(&si->free_clusters) &&\n\t\tcluster_is_free(&si->cluster_info[offset]);\n\n\tif (!conflict)\n\t\treturn false;\n\n\tpercpu_cluster = this_cpu_ptr(si->percpu_cluster);\n\tcluster_set_null(&percpu_cluster->index);\n\treturn true;\n}\n\n \nstatic bool scan_swap_map_try_ssd_cluster(struct swap_info_struct *si,\n\tunsigned long *offset, unsigned long *scan_base)\n{\n\tstruct percpu_cluster *cluster;\n\tstruct swap_cluster_info *ci;\n\tunsigned long tmp, max;\n\nnew_cluster:\n\tcluster = this_cpu_ptr(si->percpu_cluster);\n\tif (cluster_is_null(&cluster->index)) {\n\t\tif (!cluster_list_empty(&si->free_clusters)) {\n\t\t\tcluster->index = si->free_clusters.head;\n\t\t\tcluster->next = cluster_next(&cluster->index) *\n\t\t\t\t\tSWAPFILE_CLUSTER;\n\t\t} else if (!cluster_list_empty(&si->discard_clusters)) {\n\t\t\t \n\t\t\tswap_do_scheduled_discard(si);\n\t\t\t*scan_base = this_cpu_read(*si->cluster_next_cpu);\n\t\t\t*offset = *scan_base;\n\t\t\tgoto new_cluster;\n\t\t} else\n\t\t\treturn false;\n\t}\n\n\t \n\ttmp = cluster->next;\n\tmax = min_t(unsigned long, si->max,\n\t\t    (cluster_next(&cluster->index) + 1) * SWAPFILE_CLUSTER);\n\tif (tmp < max) {\n\t\tci = lock_cluster(si, tmp);\n\t\twhile (tmp < max) {\n\t\t\tif (!si->swap_map[tmp])\n\t\t\t\tbreak;\n\t\t\ttmp++;\n\t\t}\n\t\tunlock_cluster(ci);\n\t}\n\tif (tmp >= max) {\n\t\tcluster_set_null(&cluster->index);\n\t\tgoto new_cluster;\n\t}\n\tcluster->next = tmp + 1;\n\t*offset = tmp;\n\t*scan_base = tmp;\n\treturn true;\n}\n\nstatic void __del_from_avail_list(struct swap_info_struct *p)\n{\n\tint nid;\n\n\tassert_spin_locked(&p->lock);\n\tfor_each_node(nid)\n\t\tplist_del(&p->avail_lists[nid], &swap_avail_heads[nid]);\n}\n\nstatic void del_from_avail_list(struct swap_info_struct *p)\n{\n\tspin_lock(&swap_avail_lock);\n\t__del_from_avail_list(p);\n\tspin_unlock(&swap_avail_lock);\n}\n\nstatic void swap_range_alloc(struct swap_info_struct *si, unsigned long offset,\n\t\t\t     unsigned int nr_entries)\n{\n\tunsigned int end = offset + nr_entries - 1;\n\n\tif (offset == si->lowest_bit)\n\t\tsi->lowest_bit += nr_entries;\n\tif (end == si->highest_bit)\n\t\tWRITE_ONCE(si->highest_bit, si->highest_bit - nr_entries);\n\tWRITE_ONCE(si->inuse_pages, si->inuse_pages + nr_entries);\n\tif (si->inuse_pages == si->pages) {\n\t\tsi->lowest_bit = si->max;\n\t\tsi->highest_bit = 0;\n\t\tdel_from_avail_list(si);\n\t}\n}\n\nstatic void add_to_avail_list(struct swap_info_struct *p)\n{\n\tint nid;\n\n\tspin_lock(&swap_avail_lock);\n\tfor_each_node(nid)\n\t\tplist_add(&p->avail_lists[nid], &swap_avail_heads[nid]);\n\tspin_unlock(&swap_avail_lock);\n}\n\nstatic void swap_range_free(struct swap_info_struct *si, unsigned long offset,\n\t\t\t    unsigned int nr_entries)\n{\n\tunsigned long begin = offset;\n\tunsigned long end = offset + nr_entries - 1;\n\tvoid (*swap_slot_free_notify)(struct block_device *, unsigned long);\n\n\tif (offset < si->lowest_bit)\n\t\tsi->lowest_bit = offset;\n\tif (end > si->highest_bit) {\n\t\tbool was_full = !si->highest_bit;\n\n\t\tWRITE_ONCE(si->highest_bit, end);\n\t\tif (was_full && (si->flags & SWP_WRITEOK))\n\t\t\tadd_to_avail_list(si);\n\t}\n\tatomic_long_add(nr_entries, &nr_swap_pages);\n\tWRITE_ONCE(si->inuse_pages, si->inuse_pages - nr_entries);\n\tif (si->flags & SWP_BLKDEV)\n\t\tswap_slot_free_notify =\n\t\t\tsi->bdev->bd_disk->fops->swap_slot_free_notify;\n\telse\n\t\tswap_slot_free_notify = NULL;\n\twhile (offset <= end) {\n\t\tarch_swap_invalidate_page(si->type, offset);\n\t\tzswap_invalidate(si->type, offset);\n\t\tif (swap_slot_free_notify)\n\t\t\tswap_slot_free_notify(si->bdev, offset);\n\t\toffset++;\n\t}\n\tclear_shadow_from_swap_cache(si->type, begin, end);\n}\n\nstatic void set_cluster_next(struct swap_info_struct *si, unsigned long next)\n{\n\tunsigned long prev;\n\n\tif (!(si->flags & SWP_SOLIDSTATE)) {\n\t\tsi->cluster_next = next;\n\t\treturn;\n\t}\n\n\tprev = this_cpu_read(*si->cluster_next_cpu);\n\t \n\tif ((prev >> SWAP_ADDRESS_SPACE_SHIFT) !=\n\t    (next >> SWAP_ADDRESS_SPACE_SHIFT)) {\n\t\t \n\t\tif (si->highest_bit <= si->lowest_bit)\n\t\t\treturn;\n\t\tnext = get_random_u32_inclusive(si->lowest_bit, si->highest_bit);\n\t\tnext = ALIGN_DOWN(next, SWAP_ADDRESS_SPACE_PAGES);\n\t\tnext = max_t(unsigned int, next, si->lowest_bit);\n\t}\n\tthis_cpu_write(*si->cluster_next_cpu, next);\n}\n\nstatic bool swap_offset_available_and_locked(struct swap_info_struct *si,\n\t\t\t\t\t     unsigned long offset)\n{\n\tif (data_race(!si->swap_map[offset])) {\n\t\tspin_lock(&si->lock);\n\t\treturn true;\n\t}\n\n\tif (vm_swap_full() && READ_ONCE(si->swap_map[offset]) == SWAP_HAS_CACHE) {\n\t\tspin_lock(&si->lock);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic int scan_swap_map_slots(struct swap_info_struct *si,\n\t\t\t       unsigned char usage, int nr,\n\t\t\t       swp_entry_t slots[])\n{\n\tstruct swap_cluster_info *ci;\n\tunsigned long offset;\n\tunsigned long scan_base;\n\tunsigned long last_in_cluster = 0;\n\tint latency_ration = LATENCY_LIMIT;\n\tint n_ret = 0;\n\tbool scanned_many = false;\n\n\t \n\n\tsi->flags += SWP_SCANNING;\n\t \n\tif (si->flags & SWP_SOLIDSTATE)\n\t\tscan_base = this_cpu_read(*si->cluster_next_cpu);\n\telse\n\t\tscan_base = si->cluster_next;\n\toffset = scan_base;\n\n\t \n\tif (si->cluster_info) {\n\t\tif (!scan_swap_map_try_ssd_cluster(si, &offset, &scan_base))\n\t\t\tgoto scan;\n\t} else if (unlikely(!si->cluster_nr--)) {\n\t\tif (si->pages - si->inuse_pages < SWAPFILE_CLUSTER) {\n\t\t\tsi->cluster_nr = SWAPFILE_CLUSTER - 1;\n\t\t\tgoto checks;\n\t\t}\n\n\t\tspin_unlock(&si->lock);\n\n\t\t \n\t\tscan_base = offset = si->lowest_bit;\n\t\tlast_in_cluster = offset + SWAPFILE_CLUSTER - 1;\n\n\t\t \n\t\tfor (; last_in_cluster <= si->highest_bit; offset++) {\n\t\t\tif (si->swap_map[offset])\n\t\t\t\tlast_in_cluster = offset + SWAPFILE_CLUSTER;\n\t\t\telse if (offset == last_in_cluster) {\n\t\t\t\tspin_lock(&si->lock);\n\t\t\t\toffset -= SWAPFILE_CLUSTER - 1;\n\t\t\t\tsi->cluster_next = offset;\n\t\t\t\tsi->cluster_nr = SWAPFILE_CLUSTER - 1;\n\t\t\t\tgoto checks;\n\t\t\t}\n\t\t\tif (unlikely(--latency_ration < 0)) {\n\t\t\t\tcond_resched();\n\t\t\t\tlatency_ration = LATENCY_LIMIT;\n\t\t\t}\n\t\t}\n\n\t\toffset = scan_base;\n\t\tspin_lock(&si->lock);\n\t\tsi->cluster_nr = SWAPFILE_CLUSTER - 1;\n\t}\n\nchecks:\n\tif (si->cluster_info) {\n\t\twhile (scan_swap_map_ssd_cluster_conflict(si, offset)) {\n\t\t \n\t\t\tif (n_ret)\n\t\t\t\tgoto done;\n\t\t\tif (!scan_swap_map_try_ssd_cluster(si, &offset,\n\t\t\t\t\t\t\t&scan_base))\n\t\t\t\tgoto scan;\n\t\t}\n\t}\n\tif (!(si->flags & SWP_WRITEOK))\n\t\tgoto no_page;\n\tif (!si->highest_bit)\n\t\tgoto no_page;\n\tif (offset > si->highest_bit)\n\t\tscan_base = offset = si->lowest_bit;\n\n\tci = lock_cluster(si, offset);\n\t \n\tif (vm_swap_full() && si->swap_map[offset] == SWAP_HAS_CACHE) {\n\t\tint swap_was_freed;\n\t\tunlock_cluster(ci);\n\t\tspin_unlock(&si->lock);\n\t\tswap_was_freed = __try_to_reclaim_swap(si, offset, TTRS_ANYWAY);\n\t\tspin_lock(&si->lock);\n\t\t \n\t\tif (swap_was_freed)\n\t\t\tgoto checks;\n\t\tgoto scan;  \n\t}\n\n\tif (si->swap_map[offset]) {\n\t\tunlock_cluster(ci);\n\t\tif (!n_ret)\n\t\t\tgoto scan;\n\t\telse\n\t\t\tgoto done;\n\t}\n\tWRITE_ONCE(si->swap_map[offset], usage);\n\tinc_cluster_info_page(si, si->cluster_info, offset);\n\tunlock_cluster(ci);\n\n\tswap_range_alloc(si, offset, 1);\n\tslots[n_ret++] = swp_entry(si->type, offset);\n\n\t \n\tif ((n_ret == nr) || (offset >= si->highest_bit))\n\t\tgoto done;\n\n\t \n\n\t \n\tif (unlikely(--latency_ration < 0)) {\n\t\tif (n_ret)\n\t\t\tgoto done;\n\t\tspin_unlock(&si->lock);\n\t\tcond_resched();\n\t\tspin_lock(&si->lock);\n\t\tlatency_ration = LATENCY_LIMIT;\n\t}\n\n\t \n\tif (si->cluster_info) {\n\t\tif (scan_swap_map_try_ssd_cluster(si, &offset, &scan_base))\n\t\t\tgoto checks;\n\t} else if (si->cluster_nr && !si->swap_map[++offset]) {\n\t\t \n\t\t--si->cluster_nr;\n\t\tgoto checks;\n\t}\n\n\t \n\tif (!scanned_many) {\n\t\tunsigned long scan_limit;\n\n\t\tif (offset < scan_base)\n\t\t\tscan_limit = scan_base;\n\t\telse\n\t\t\tscan_limit = si->highest_bit;\n\t\tfor (; offset <= scan_limit && --latency_ration > 0;\n\t\t     offset++) {\n\t\t\tif (!si->swap_map[offset])\n\t\t\t\tgoto checks;\n\t\t}\n\t}\n\ndone:\n\tset_cluster_next(si, offset + 1);\n\tsi->flags -= SWP_SCANNING;\n\treturn n_ret;\n\nscan:\n\tspin_unlock(&si->lock);\n\twhile (++offset <= READ_ONCE(si->highest_bit)) {\n\t\tif (unlikely(--latency_ration < 0)) {\n\t\t\tcond_resched();\n\t\t\tlatency_ration = LATENCY_LIMIT;\n\t\t\tscanned_many = true;\n\t\t}\n\t\tif (swap_offset_available_and_locked(si, offset))\n\t\t\tgoto checks;\n\t}\n\toffset = si->lowest_bit;\n\twhile (offset < scan_base) {\n\t\tif (unlikely(--latency_ration < 0)) {\n\t\t\tcond_resched();\n\t\t\tlatency_ration = LATENCY_LIMIT;\n\t\t\tscanned_many = true;\n\t\t}\n\t\tif (swap_offset_available_and_locked(si, offset))\n\t\t\tgoto checks;\n\t\toffset++;\n\t}\n\tspin_lock(&si->lock);\n\nno_page:\n\tsi->flags -= SWP_SCANNING;\n\treturn n_ret;\n}\n\nstatic int swap_alloc_cluster(struct swap_info_struct *si, swp_entry_t *slot)\n{\n\tunsigned long idx;\n\tstruct swap_cluster_info *ci;\n\tunsigned long offset;\n\n\t \n\tif (!IS_ENABLED(CONFIG_THP_SWAP)) {\n\t\tVM_WARN_ON_ONCE(1);\n\t\treturn 0;\n\t}\n\n\tif (cluster_list_empty(&si->free_clusters))\n\t\treturn 0;\n\n\tidx = cluster_list_first(&si->free_clusters);\n\toffset = idx * SWAPFILE_CLUSTER;\n\tci = lock_cluster(si, offset);\n\talloc_cluster(si, idx);\n\tcluster_set_count_flag(ci, SWAPFILE_CLUSTER, CLUSTER_FLAG_HUGE);\n\n\tmemset(si->swap_map + offset, SWAP_HAS_CACHE, SWAPFILE_CLUSTER);\n\tunlock_cluster(ci);\n\tswap_range_alloc(si, offset, SWAPFILE_CLUSTER);\n\t*slot = swp_entry(si->type, offset);\n\n\treturn 1;\n}\n\nstatic void swap_free_cluster(struct swap_info_struct *si, unsigned long idx)\n{\n\tunsigned long offset = idx * SWAPFILE_CLUSTER;\n\tstruct swap_cluster_info *ci;\n\n\tci = lock_cluster(si, offset);\n\tmemset(si->swap_map + offset, 0, SWAPFILE_CLUSTER);\n\tcluster_set_count_flag(ci, 0, 0);\n\tfree_cluster(si, idx);\n\tunlock_cluster(ci);\n\tswap_range_free(si, offset, SWAPFILE_CLUSTER);\n}\n\nint get_swap_pages(int n_goal, swp_entry_t swp_entries[], int entry_size)\n{\n\tunsigned long size = swap_entry_size(entry_size);\n\tstruct swap_info_struct *si, *next;\n\tlong avail_pgs;\n\tint n_ret = 0;\n\tint node;\n\n\t \n\tWARN_ON_ONCE(n_goal > 1 && size == SWAPFILE_CLUSTER);\n\n\tspin_lock(&swap_avail_lock);\n\n\tavail_pgs = atomic_long_read(&nr_swap_pages) / size;\n\tif (avail_pgs <= 0) {\n\t\tspin_unlock(&swap_avail_lock);\n\t\tgoto noswap;\n\t}\n\n\tn_goal = min3((long)n_goal, (long)SWAP_BATCH, avail_pgs);\n\n\tatomic_long_sub(n_goal * size, &nr_swap_pages);\n\nstart_over:\n\tnode = numa_node_id();\n\tplist_for_each_entry_safe(si, next, &swap_avail_heads[node], avail_lists[node]) {\n\t\t \n\t\tplist_requeue(&si->avail_lists[node], &swap_avail_heads[node]);\n\t\tspin_unlock(&swap_avail_lock);\n\t\tspin_lock(&si->lock);\n\t\tif (!si->highest_bit || !(si->flags & SWP_WRITEOK)) {\n\t\t\tspin_lock(&swap_avail_lock);\n\t\t\tif (plist_node_empty(&si->avail_lists[node])) {\n\t\t\t\tspin_unlock(&si->lock);\n\t\t\t\tgoto nextsi;\n\t\t\t}\n\t\t\tWARN(!si->highest_bit,\n\t\t\t     \"swap_info %d in list but !highest_bit\\n\",\n\t\t\t     si->type);\n\t\t\tWARN(!(si->flags & SWP_WRITEOK),\n\t\t\t     \"swap_info %d in list but !SWP_WRITEOK\\n\",\n\t\t\t     si->type);\n\t\t\t__del_from_avail_list(si);\n\t\t\tspin_unlock(&si->lock);\n\t\t\tgoto nextsi;\n\t\t}\n\t\tif (size == SWAPFILE_CLUSTER) {\n\t\t\tif (si->flags & SWP_BLKDEV)\n\t\t\t\tn_ret = swap_alloc_cluster(si, swp_entries);\n\t\t} else\n\t\t\tn_ret = scan_swap_map_slots(si, SWAP_HAS_CACHE,\n\t\t\t\t\t\t    n_goal, swp_entries);\n\t\tspin_unlock(&si->lock);\n\t\tif (n_ret || size == SWAPFILE_CLUSTER)\n\t\t\tgoto check_out;\n\t\tcond_resched();\n\n\t\tspin_lock(&swap_avail_lock);\nnextsi:\n\t\t \n\t\tif (plist_node_empty(&next->avail_lists[node]))\n\t\t\tgoto start_over;\n\t}\n\n\tspin_unlock(&swap_avail_lock);\n\ncheck_out:\n\tif (n_ret < n_goal)\n\t\tatomic_long_add((long)(n_goal - n_ret) * size,\n\t\t\t\t&nr_swap_pages);\nnoswap:\n\treturn n_ret;\n}\n\nstatic struct swap_info_struct *_swap_info_get(swp_entry_t entry)\n{\n\tstruct swap_info_struct *p;\n\tunsigned long offset;\n\n\tif (!entry.val)\n\t\tgoto out;\n\tp = swp_swap_info(entry);\n\tif (!p)\n\t\tgoto bad_nofile;\n\tif (data_race(!(p->flags & SWP_USED)))\n\t\tgoto bad_device;\n\toffset = swp_offset(entry);\n\tif (offset >= p->max)\n\t\tgoto bad_offset;\n\tif (data_race(!p->swap_map[swp_offset(entry)]))\n\t\tgoto bad_free;\n\treturn p;\n\nbad_free:\n\tpr_err(\"%s: %s%08lx\\n\", __func__, Unused_offset, entry.val);\n\tgoto out;\nbad_offset:\n\tpr_err(\"%s: %s%08lx\\n\", __func__, Bad_offset, entry.val);\n\tgoto out;\nbad_device:\n\tpr_err(\"%s: %s%08lx\\n\", __func__, Unused_file, entry.val);\n\tgoto out;\nbad_nofile:\n\tpr_err(\"%s: %s%08lx\\n\", __func__, Bad_file, entry.val);\nout:\n\treturn NULL;\n}\n\nstatic struct swap_info_struct *swap_info_get_cont(swp_entry_t entry,\n\t\t\t\t\tstruct swap_info_struct *q)\n{\n\tstruct swap_info_struct *p;\n\n\tp = _swap_info_get(entry);\n\n\tif (p != q) {\n\t\tif (q != NULL)\n\t\t\tspin_unlock(&q->lock);\n\t\tif (p != NULL)\n\t\t\tspin_lock(&p->lock);\n\t}\n\treturn p;\n}\n\nstatic unsigned char __swap_entry_free_locked(struct swap_info_struct *p,\n\t\t\t\t\t      unsigned long offset,\n\t\t\t\t\t      unsigned char usage)\n{\n\tunsigned char count;\n\tunsigned char has_cache;\n\n\tcount = p->swap_map[offset];\n\n\thas_cache = count & SWAP_HAS_CACHE;\n\tcount &= ~SWAP_HAS_CACHE;\n\n\tif (usage == SWAP_HAS_CACHE) {\n\t\tVM_BUG_ON(!has_cache);\n\t\thas_cache = 0;\n\t} else if (count == SWAP_MAP_SHMEM) {\n\t\t \n\t\tcount = 0;\n\t} else if ((count & ~COUNT_CONTINUED) <= SWAP_MAP_MAX) {\n\t\tif (count == COUNT_CONTINUED) {\n\t\t\tif (swap_count_continued(p, offset, count))\n\t\t\t\tcount = SWAP_MAP_MAX | COUNT_CONTINUED;\n\t\t\telse\n\t\t\t\tcount = SWAP_MAP_MAX;\n\t\t} else\n\t\t\tcount--;\n\t}\n\n\tusage = count | has_cache;\n\tif (usage)\n\t\tWRITE_ONCE(p->swap_map[offset], usage);\n\telse\n\t\tWRITE_ONCE(p->swap_map[offset], SWAP_HAS_CACHE);\n\n\treturn usage;\n}\n\n \nstruct swap_info_struct *get_swap_device(swp_entry_t entry)\n{\n\tstruct swap_info_struct *si;\n\tunsigned long offset;\n\n\tif (!entry.val)\n\t\tgoto out;\n\tsi = swp_swap_info(entry);\n\tif (!si)\n\t\tgoto bad_nofile;\n\tif (!percpu_ref_tryget_live(&si->users))\n\t\tgoto out;\n\t \n\tsmp_rmb();\n\toffset = swp_offset(entry);\n\tif (offset >= si->max)\n\t\tgoto put_out;\n\n\treturn si;\nbad_nofile:\n\tpr_err(\"%s: %s%08lx\\n\", __func__, Bad_file, entry.val);\nout:\n\treturn NULL;\nput_out:\n\tpr_err(\"%s: %s%08lx\\n\", __func__, Bad_offset, entry.val);\n\tpercpu_ref_put(&si->users);\n\treturn NULL;\n}\n\nstatic unsigned char __swap_entry_free(struct swap_info_struct *p,\n\t\t\t\t       swp_entry_t entry)\n{\n\tstruct swap_cluster_info *ci;\n\tunsigned long offset = swp_offset(entry);\n\tunsigned char usage;\n\n\tci = lock_cluster_or_swap_info(p, offset);\n\tusage = __swap_entry_free_locked(p, offset, 1);\n\tunlock_cluster_or_swap_info(p, ci);\n\tif (!usage)\n\t\tfree_swap_slot(entry);\n\n\treturn usage;\n}\n\nstatic void swap_entry_free(struct swap_info_struct *p, swp_entry_t entry)\n{\n\tstruct swap_cluster_info *ci;\n\tunsigned long offset = swp_offset(entry);\n\tunsigned char count;\n\n\tci = lock_cluster(p, offset);\n\tcount = p->swap_map[offset];\n\tVM_BUG_ON(count != SWAP_HAS_CACHE);\n\tp->swap_map[offset] = 0;\n\tdec_cluster_info_page(p, p->cluster_info, offset);\n\tunlock_cluster(ci);\n\n\tmem_cgroup_uncharge_swap(entry, 1);\n\tswap_range_free(p, offset, 1);\n}\n\n \nvoid swap_free(swp_entry_t entry)\n{\n\tstruct swap_info_struct *p;\n\n\tp = _swap_info_get(entry);\n\tif (p)\n\t\t__swap_entry_free(p, entry);\n}\n\n \nvoid put_swap_folio(struct folio *folio, swp_entry_t entry)\n{\n\tunsigned long offset = swp_offset(entry);\n\tunsigned long idx = offset / SWAPFILE_CLUSTER;\n\tstruct swap_cluster_info *ci;\n\tstruct swap_info_struct *si;\n\tunsigned char *map;\n\tunsigned int i, free_entries = 0;\n\tunsigned char val;\n\tint size = swap_entry_size(folio_nr_pages(folio));\n\n\tsi = _swap_info_get(entry);\n\tif (!si)\n\t\treturn;\n\n\tci = lock_cluster_or_swap_info(si, offset);\n\tif (size == SWAPFILE_CLUSTER) {\n\t\tVM_BUG_ON(!cluster_is_huge(ci));\n\t\tmap = si->swap_map + offset;\n\t\tfor (i = 0; i < SWAPFILE_CLUSTER; i++) {\n\t\t\tval = map[i];\n\t\t\tVM_BUG_ON(!(val & SWAP_HAS_CACHE));\n\t\t\tif (val == SWAP_HAS_CACHE)\n\t\t\t\tfree_entries++;\n\t\t}\n\t\tcluster_clear_huge(ci);\n\t\tif (free_entries == SWAPFILE_CLUSTER) {\n\t\t\tunlock_cluster_or_swap_info(si, ci);\n\t\t\tspin_lock(&si->lock);\n\t\t\tmem_cgroup_uncharge_swap(entry, SWAPFILE_CLUSTER);\n\t\t\tswap_free_cluster(si, idx);\n\t\t\tspin_unlock(&si->lock);\n\t\t\treturn;\n\t\t}\n\t}\n\tfor (i = 0; i < size; i++, entry.val++) {\n\t\tif (!__swap_entry_free_locked(si, offset + i, SWAP_HAS_CACHE)) {\n\t\t\tunlock_cluster_or_swap_info(si, ci);\n\t\t\tfree_swap_slot(entry);\n\t\t\tif (i == size - 1)\n\t\t\t\treturn;\n\t\t\tlock_cluster_or_swap_info(si, offset);\n\t\t}\n\t}\n\tunlock_cluster_or_swap_info(si, ci);\n}\n\n#ifdef CONFIG_THP_SWAP\nint split_swap_cluster(swp_entry_t entry)\n{\n\tstruct swap_info_struct *si;\n\tstruct swap_cluster_info *ci;\n\tunsigned long offset = swp_offset(entry);\n\n\tsi = _swap_info_get(entry);\n\tif (!si)\n\t\treturn -EBUSY;\n\tci = lock_cluster(si, offset);\n\tcluster_clear_huge(ci);\n\tunlock_cluster(ci);\n\treturn 0;\n}\n#endif\n\nstatic int swp_entry_cmp(const void *ent1, const void *ent2)\n{\n\tconst swp_entry_t *e1 = ent1, *e2 = ent2;\n\n\treturn (int)swp_type(*e1) - (int)swp_type(*e2);\n}\n\nvoid swapcache_free_entries(swp_entry_t *entries, int n)\n{\n\tstruct swap_info_struct *p, *prev;\n\tint i;\n\n\tif (n <= 0)\n\t\treturn;\n\n\tprev = NULL;\n\tp = NULL;\n\n\t \n\tif (nr_swapfiles > 1)\n\t\tsort(entries, n, sizeof(entries[0]), swp_entry_cmp, NULL);\n\tfor (i = 0; i < n; ++i) {\n\t\tp = swap_info_get_cont(entries[i], prev);\n\t\tif (p)\n\t\t\tswap_entry_free(p, entries[i]);\n\t\tprev = p;\n\t}\n\tif (p)\n\t\tspin_unlock(&p->lock);\n}\n\nint __swap_count(swp_entry_t entry)\n{\n\tstruct swap_info_struct *si = swp_swap_info(entry);\n\tpgoff_t offset = swp_offset(entry);\n\n\treturn swap_count(si->swap_map[offset]);\n}\n\n \nint swap_swapcount(struct swap_info_struct *si, swp_entry_t entry)\n{\n\tpgoff_t offset = swp_offset(entry);\n\tstruct swap_cluster_info *ci;\n\tint count;\n\n\tci = lock_cluster_or_swap_info(si, offset);\n\tcount = swap_count(si->swap_map[offset]);\n\tunlock_cluster_or_swap_info(si, ci);\n\treturn count;\n}\n\n \nint swp_swapcount(swp_entry_t entry)\n{\n\tint count, tmp_count, n;\n\tstruct swap_info_struct *p;\n\tstruct swap_cluster_info *ci;\n\tstruct page *page;\n\tpgoff_t offset;\n\tunsigned char *map;\n\n\tp = _swap_info_get(entry);\n\tif (!p)\n\t\treturn 0;\n\n\toffset = swp_offset(entry);\n\n\tci = lock_cluster_or_swap_info(p, offset);\n\n\tcount = swap_count(p->swap_map[offset]);\n\tif (!(count & COUNT_CONTINUED))\n\t\tgoto out;\n\n\tcount &= ~COUNT_CONTINUED;\n\tn = SWAP_MAP_MAX + 1;\n\n\tpage = vmalloc_to_page(p->swap_map + offset);\n\toffset &= ~PAGE_MASK;\n\tVM_BUG_ON(page_private(page) != SWP_CONTINUED);\n\n\tdo {\n\t\tpage = list_next_entry(page, lru);\n\t\tmap = kmap_atomic(page);\n\t\ttmp_count = map[offset];\n\t\tkunmap_atomic(map);\n\n\t\tcount += (tmp_count & ~COUNT_CONTINUED) * n;\n\t\tn *= (SWAP_CONT_MAX + 1);\n\t} while (tmp_count & COUNT_CONTINUED);\nout:\n\tunlock_cluster_or_swap_info(p, ci);\n\treturn count;\n}\n\nstatic bool swap_page_trans_huge_swapped(struct swap_info_struct *si,\n\t\t\t\t\t swp_entry_t entry)\n{\n\tstruct swap_cluster_info *ci;\n\tunsigned char *map = si->swap_map;\n\tunsigned long roffset = swp_offset(entry);\n\tunsigned long offset = round_down(roffset, SWAPFILE_CLUSTER);\n\tint i;\n\tbool ret = false;\n\n\tci = lock_cluster_or_swap_info(si, offset);\n\tif (!ci || !cluster_is_huge(ci)) {\n\t\tif (swap_count(map[roffset]))\n\t\t\tret = true;\n\t\tgoto unlock_out;\n\t}\n\tfor (i = 0; i < SWAPFILE_CLUSTER; i++) {\n\t\tif (swap_count(map[offset + i])) {\n\t\t\tret = true;\n\t\t\tbreak;\n\t\t}\n\t}\nunlock_out:\n\tunlock_cluster_or_swap_info(si, ci);\n\treturn ret;\n}\n\nstatic bool folio_swapped(struct folio *folio)\n{\n\tswp_entry_t entry = folio->swap;\n\tstruct swap_info_struct *si = _swap_info_get(entry);\n\n\tif (!si)\n\t\treturn false;\n\n\tif (!IS_ENABLED(CONFIG_THP_SWAP) || likely(!folio_test_large(folio)))\n\t\treturn swap_swapcount(si, entry) != 0;\n\n\treturn swap_page_trans_huge_swapped(si, entry);\n}\n\n \nbool folio_free_swap(struct folio *folio)\n{\n\tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n\n\tif (!folio_test_swapcache(folio))\n\t\treturn false;\n\tif (folio_test_writeback(folio))\n\t\treturn false;\n\tif (folio_swapped(folio))\n\t\treturn false;\n\n\t \n\tif (pm_suspended_storage())\n\t\treturn false;\n\n\tdelete_from_swap_cache(folio);\n\tfolio_set_dirty(folio);\n\treturn true;\n}\n\n \nint free_swap_and_cache(swp_entry_t entry)\n{\n\tstruct swap_info_struct *p;\n\tunsigned char count;\n\n\tif (non_swap_entry(entry))\n\t\treturn 1;\n\n\tp = _swap_info_get(entry);\n\tif (p) {\n\t\tcount = __swap_entry_free(p, entry);\n\t\tif (count == SWAP_HAS_CACHE &&\n\t\t    !swap_page_trans_huge_swapped(p, entry))\n\t\t\t__try_to_reclaim_swap(p, swp_offset(entry),\n\t\t\t\t\t      TTRS_UNMAPPED | TTRS_FULL);\n\t}\n\treturn p != NULL;\n}\n\n#ifdef CONFIG_HIBERNATION\n\nswp_entry_t get_swap_page_of_type(int type)\n{\n\tstruct swap_info_struct *si = swap_type_to_swap_info(type);\n\tswp_entry_t entry = {0};\n\n\tif (!si)\n\t\tgoto fail;\n\n\t \n\tspin_lock(&si->lock);\n\tif ((si->flags & SWP_WRITEOK) && scan_swap_map_slots(si, 1, 1, &entry))\n\t\tatomic_long_dec(&nr_swap_pages);\n\tspin_unlock(&si->lock);\nfail:\n\treturn entry;\n}\n\n \nint swap_type_of(dev_t device, sector_t offset)\n{\n\tint type;\n\n\tif (!device)\n\t\treturn -1;\n\n\tspin_lock(&swap_lock);\n\tfor (type = 0; type < nr_swapfiles; type++) {\n\t\tstruct swap_info_struct *sis = swap_info[type];\n\n\t\tif (!(sis->flags & SWP_WRITEOK))\n\t\t\tcontinue;\n\n\t\tif (device == sis->bdev->bd_dev) {\n\t\t\tstruct swap_extent *se = first_se(sis);\n\n\t\t\tif (se->start_block == offset) {\n\t\t\t\tspin_unlock(&swap_lock);\n\t\t\t\treturn type;\n\t\t\t}\n\t\t}\n\t}\n\tspin_unlock(&swap_lock);\n\treturn -ENODEV;\n}\n\nint find_first_swap(dev_t *device)\n{\n\tint type;\n\n\tspin_lock(&swap_lock);\n\tfor (type = 0; type < nr_swapfiles; type++) {\n\t\tstruct swap_info_struct *sis = swap_info[type];\n\n\t\tif (!(sis->flags & SWP_WRITEOK))\n\t\t\tcontinue;\n\t\t*device = sis->bdev->bd_dev;\n\t\tspin_unlock(&swap_lock);\n\t\treturn type;\n\t}\n\tspin_unlock(&swap_lock);\n\treturn -ENODEV;\n}\n\n \nsector_t swapdev_block(int type, pgoff_t offset)\n{\n\tstruct swap_info_struct *si = swap_type_to_swap_info(type);\n\tstruct swap_extent *se;\n\n\tif (!si || !(si->flags & SWP_WRITEOK))\n\t\treturn 0;\n\tse = offset_to_swap_extent(si, offset);\n\treturn se->start_block + (offset - se->start_page);\n}\n\n \nunsigned int count_swap_pages(int type, int free)\n{\n\tunsigned int n = 0;\n\n\tspin_lock(&swap_lock);\n\tif ((unsigned int)type < nr_swapfiles) {\n\t\tstruct swap_info_struct *sis = swap_info[type];\n\n\t\tspin_lock(&sis->lock);\n\t\tif (sis->flags & SWP_WRITEOK) {\n\t\t\tn = sis->pages;\n\t\t\tif (free)\n\t\t\t\tn -= sis->inuse_pages;\n\t\t}\n\t\tspin_unlock(&sis->lock);\n\t}\n\tspin_unlock(&swap_lock);\n\treturn n;\n}\n#endif  \n\nstatic inline int pte_same_as_swp(pte_t pte, pte_t swp_pte)\n{\n\treturn pte_same(pte_swp_clear_flags(pte), swp_pte);\n}\n\n \nstatic int unuse_pte(struct vm_area_struct *vma, pmd_t *pmd,\n\t\tunsigned long addr, swp_entry_t entry, struct folio *folio)\n{\n\tstruct page *page = folio_file_page(folio, swp_offset(entry));\n\tstruct page *swapcache;\n\tspinlock_t *ptl;\n\tpte_t *pte, new_pte, old_pte;\n\tbool hwpoisoned = PageHWPoison(page);\n\tint ret = 1;\n\n\tswapcache = page;\n\tpage = ksm_might_need_to_copy(page, vma, addr);\n\tif (unlikely(!page))\n\t\treturn -ENOMEM;\n\telse if (unlikely(PTR_ERR(page) == -EHWPOISON))\n\t\thwpoisoned = true;\n\n\tpte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);\n\tif (unlikely(!pte || !pte_same_as_swp(ptep_get(pte),\n\t\t\t\t\t\tswp_entry_to_pte(entry)))) {\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\told_pte = ptep_get(pte);\n\n\tif (unlikely(hwpoisoned || !PageUptodate(page))) {\n\t\tswp_entry_t swp_entry;\n\n\t\tdec_mm_counter(vma->vm_mm, MM_SWAPENTS);\n\t\tif (hwpoisoned) {\n\t\t\tswp_entry = make_hwpoison_entry(swapcache);\n\t\t\tpage = swapcache;\n\t\t} else {\n\t\t\tswp_entry = make_poisoned_swp_entry();\n\t\t}\n\t\tnew_pte = swp_entry_to_pte(swp_entry);\n\t\tret = 0;\n\t\tgoto setpte;\n\t}\n\n\t \n\tarch_swap_restore(entry, page_folio(page));\n\n\t \n\tBUG_ON(!PageAnon(page) && PageMappedToDisk(page));\n\tBUG_ON(PageAnon(page) && PageAnonExclusive(page));\n\n\tdec_mm_counter(vma->vm_mm, MM_SWAPENTS);\n\tinc_mm_counter(vma->vm_mm, MM_ANONPAGES);\n\tget_page(page);\n\tif (page == swapcache) {\n\t\trmap_t rmap_flags = RMAP_NONE;\n\n\t\t \n\t\tVM_BUG_ON_PAGE(PageWriteback(page), page);\n\t\tif (pte_swp_exclusive(old_pte))\n\t\t\trmap_flags |= RMAP_EXCLUSIVE;\n\n\t\tpage_add_anon_rmap(page, vma, addr, rmap_flags);\n\t} else {  \n\t\tpage_add_new_anon_rmap(page, vma, addr);\n\t\tlru_cache_add_inactive_or_unevictable(page, vma);\n\t}\n\tnew_pte = pte_mkold(mk_pte(page, vma->vm_page_prot));\n\tif (pte_swp_soft_dirty(old_pte))\n\t\tnew_pte = pte_mksoft_dirty(new_pte);\n\tif (pte_swp_uffd_wp(old_pte))\n\t\tnew_pte = pte_mkuffd_wp(new_pte);\nsetpte:\n\tset_pte_at(vma->vm_mm, addr, pte, new_pte);\n\tswap_free(entry);\nout:\n\tif (pte)\n\t\tpte_unmap_unlock(pte, ptl);\n\tif (page != swapcache) {\n\t\tunlock_page(page);\n\t\tput_page(page);\n\t}\n\treturn ret;\n}\n\nstatic int unuse_pte_range(struct vm_area_struct *vma, pmd_t *pmd,\n\t\t\tunsigned long addr, unsigned long end,\n\t\t\tunsigned int type)\n{\n\tpte_t *pte = NULL;\n\tstruct swap_info_struct *si;\n\n\tsi = swap_info[type];\n\tdo {\n\t\tstruct folio *folio;\n\t\tunsigned long offset;\n\t\tunsigned char swp_count;\n\t\tswp_entry_t entry;\n\t\tint ret;\n\t\tpte_t ptent;\n\n\t\tif (!pte++) {\n\t\t\tpte = pte_offset_map(pmd, addr);\n\t\t\tif (!pte)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tptent = ptep_get_lockless(pte);\n\n\t\tif (!is_swap_pte(ptent))\n\t\t\tcontinue;\n\n\t\tentry = pte_to_swp_entry(ptent);\n\t\tif (swp_type(entry) != type)\n\t\t\tcontinue;\n\n\t\toffset = swp_offset(entry);\n\t\tpte_unmap(pte);\n\t\tpte = NULL;\n\n\t\tfolio = swap_cache_get_folio(entry, vma, addr);\n\t\tif (!folio) {\n\t\t\tstruct page *page;\n\t\t\tstruct vm_fault vmf = {\n\t\t\t\t.vma = vma,\n\t\t\t\t.address = addr,\n\t\t\t\t.real_address = addr,\n\t\t\t\t.pmd = pmd,\n\t\t\t};\n\n\t\t\tpage = swapin_readahead(entry, GFP_HIGHUSER_MOVABLE,\n\t\t\t\t\t\t&vmf);\n\t\t\tif (page)\n\t\t\t\tfolio = page_folio(page);\n\t\t}\n\t\tif (!folio) {\n\t\t\tswp_count = READ_ONCE(si->swap_map[offset]);\n\t\t\tif (swp_count == 0 || swp_count == SWAP_MAP_BAD)\n\t\t\t\tcontinue;\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tfolio_lock(folio);\n\t\tfolio_wait_writeback(folio);\n\t\tret = unuse_pte(vma, pmd, addr, entry, folio);\n\t\tif (ret < 0) {\n\t\t\tfolio_unlock(folio);\n\t\t\tfolio_put(folio);\n\t\t\treturn ret;\n\t\t}\n\n\t\tfolio_free_swap(folio);\n\t\tfolio_unlock(folio);\n\t\tfolio_put(folio);\n\t} while (addr += PAGE_SIZE, addr != end);\n\n\tif (pte)\n\t\tpte_unmap(pte);\n\treturn 0;\n}\n\nstatic inline int unuse_pmd_range(struct vm_area_struct *vma, pud_t *pud,\n\t\t\t\tunsigned long addr, unsigned long end,\n\t\t\t\tunsigned int type)\n{\n\tpmd_t *pmd;\n\tunsigned long next;\n\tint ret;\n\n\tpmd = pmd_offset(pud, addr);\n\tdo {\n\t\tcond_resched();\n\t\tnext = pmd_addr_end(addr, end);\n\t\tret = unuse_pte_range(vma, pmd, addr, next, type);\n\t\tif (ret)\n\t\t\treturn ret;\n\t} while (pmd++, addr = next, addr != end);\n\treturn 0;\n}\n\nstatic inline int unuse_pud_range(struct vm_area_struct *vma, p4d_t *p4d,\n\t\t\t\tunsigned long addr, unsigned long end,\n\t\t\t\tunsigned int type)\n{\n\tpud_t *pud;\n\tunsigned long next;\n\tint ret;\n\n\tpud = pud_offset(p4d, addr);\n\tdo {\n\t\tnext = pud_addr_end(addr, end);\n\t\tif (pud_none_or_clear_bad(pud))\n\t\t\tcontinue;\n\t\tret = unuse_pmd_range(vma, pud, addr, next, type);\n\t\tif (ret)\n\t\t\treturn ret;\n\t} while (pud++, addr = next, addr != end);\n\treturn 0;\n}\n\nstatic inline int unuse_p4d_range(struct vm_area_struct *vma, pgd_t *pgd,\n\t\t\t\tunsigned long addr, unsigned long end,\n\t\t\t\tunsigned int type)\n{\n\tp4d_t *p4d;\n\tunsigned long next;\n\tint ret;\n\n\tp4d = p4d_offset(pgd, addr);\n\tdo {\n\t\tnext = p4d_addr_end(addr, end);\n\t\tif (p4d_none_or_clear_bad(p4d))\n\t\t\tcontinue;\n\t\tret = unuse_pud_range(vma, p4d, addr, next, type);\n\t\tif (ret)\n\t\t\treturn ret;\n\t} while (p4d++, addr = next, addr != end);\n\treturn 0;\n}\n\nstatic int unuse_vma(struct vm_area_struct *vma, unsigned int type)\n{\n\tpgd_t *pgd;\n\tunsigned long addr, end, next;\n\tint ret;\n\n\taddr = vma->vm_start;\n\tend = vma->vm_end;\n\n\tpgd = pgd_offset(vma->vm_mm, addr);\n\tdo {\n\t\tnext = pgd_addr_end(addr, end);\n\t\tif (pgd_none_or_clear_bad(pgd))\n\t\t\tcontinue;\n\t\tret = unuse_p4d_range(vma, pgd, addr, next, type);\n\t\tif (ret)\n\t\t\treturn ret;\n\t} while (pgd++, addr = next, addr != end);\n\treturn 0;\n}\n\nstatic int unuse_mm(struct mm_struct *mm, unsigned int type)\n{\n\tstruct vm_area_struct *vma;\n\tint ret = 0;\n\tVMA_ITERATOR(vmi, mm, 0);\n\n\tmmap_read_lock(mm);\n\tfor_each_vma(vmi, vma) {\n\t\tif (vma->anon_vma) {\n\t\t\tret = unuse_vma(vma, type);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tcond_resched();\n\t}\n\tmmap_read_unlock(mm);\n\treturn ret;\n}\n\n \nstatic unsigned int find_next_to_unuse(struct swap_info_struct *si,\n\t\t\t\t\tunsigned int prev)\n{\n\tunsigned int i;\n\tunsigned char count;\n\n\t \n\tfor (i = prev + 1; i < si->max; i++) {\n\t\tcount = READ_ONCE(si->swap_map[i]);\n\t\tif (count && swap_count(count) != SWAP_MAP_BAD)\n\t\t\tbreak;\n\t\tif ((i % LATENCY_LIMIT) == 0)\n\t\t\tcond_resched();\n\t}\n\n\tif (i == si->max)\n\t\ti = 0;\n\n\treturn i;\n}\n\nstatic int try_to_unuse(unsigned int type)\n{\n\tstruct mm_struct *prev_mm;\n\tstruct mm_struct *mm;\n\tstruct list_head *p;\n\tint retval = 0;\n\tstruct swap_info_struct *si = swap_info[type];\n\tstruct folio *folio;\n\tswp_entry_t entry;\n\tunsigned int i;\n\n\tif (!READ_ONCE(si->inuse_pages))\n\t\treturn 0;\n\nretry:\n\tretval = shmem_unuse(type);\n\tif (retval)\n\t\treturn retval;\n\n\tprev_mm = &init_mm;\n\tmmget(prev_mm);\n\n\tspin_lock(&mmlist_lock);\n\tp = &init_mm.mmlist;\n\twhile (READ_ONCE(si->inuse_pages) &&\n\t       !signal_pending(current) &&\n\t       (p = p->next) != &init_mm.mmlist) {\n\n\t\tmm = list_entry(p, struct mm_struct, mmlist);\n\t\tif (!mmget_not_zero(mm))\n\t\t\tcontinue;\n\t\tspin_unlock(&mmlist_lock);\n\t\tmmput(prev_mm);\n\t\tprev_mm = mm;\n\t\tretval = unuse_mm(mm, type);\n\t\tif (retval) {\n\t\t\tmmput(prev_mm);\n\t\t\treturn retval;\n\t\t}\n\n\t\t \n\t\tcond_resched();\n\t\tspin_lock(&mmlist_lock);\n\t}\n\tspin_unlock(&mmlist_lock);\n\n\tmmput(prev_mm);\n\n\ti = 0;\n\twhile (READ_ONCE(si->inuse_pages) &&\n\t       !signal_pending(current) &&\n\t       (i = find_next_to_unuse(si, i)) != 0) {\n\n\t\tentry = swp_entry(type, i);\n\t\tfolio = filemap_get_folio(swap_address_space(entry), i);\n\t\tif (IS_ERR(folio))\n\t\t\tcontinue;\n\n\t\t \n\t\tfolio_lock(folio);\n\t\tfolio_wait_writeback(folio);\n\t\tfolio_free_swap(folio);\n\t\tfolio_unlock(folio);\n\t\tfolio_put(folio);\n\t}\n\n\t \n\tif (READ_ONCE(si->inuse_pages)) {\n\t\tif (!signal_pending(current))\n\t\t\tgoto retry;\n\t\treturn -EINTR;\n\t}\n\n\treturn 0;\n}\n\n \nstatic void drain_mmlist(void)\n{\n\tstruct list_head *p, *next;\n\tunsigned int type;\n\n\tfor (type = 0; type < nr_swapfiles; type++)\n\t\tif (swap_info[type]->inuse_pages)\n\t\t\treturn;\n\tspin_lock(&mmlist_lock);\n\tlist_for_each_safe(p, next, &init_mm.mmlist)\n\t\tlist_del_init(p);\n\tspin_unlock(&mmlist_lock);\n}\n\n \nstatic void destroy_swap_extents(struct swap_info_struct *sis)\n{\n\twhile (!RB_EMPTY_ROOT(&sis->swap_extent_root)) {\n\t\tstruct rb_node *rb = sis->swap_extent_root.rb_node;\n\t\tstruct swap_extent *se = rb_entry(rb, struct swap_extent, rb_node);\n\n\t\trb_erase(rb, &sis->swap_extent_root);\n\t\tkfree(se);\n\t}\n\n\tif (sis->flags & SWP_ACTIVATED) {\n\t\tstruct file *swap_file = sis->swap_file;\n\t\tstruct address_space *mapping = swap_file->f_mapping;\n\n\t\tsis->flags &= ~SWP_ACTIVATED;\n\t\tif (mapping->a_ops->swap_deactivate)\n\t\t\tmapping->a_ops->swap_deactivate(swap_file);\n\t}\n}\n\n \nint\nadd_swap_extent(struct swap_info_struct *sis, unsigned long start_page,\n\t\tunsigned long nr_pages, sector_t start_block)\n{\n\tstruct rb_node **link = &sis->swap_extent_root.rb_node, *parent = NULL;\n\tstruct swap_extent *se;\n\tstruct swap_extent *new_se;\n\n\t \n\twhile (*link) {\n\t\tparent = *link;\n\t\tlink = &parent->rb_right;\n\t}\n\n\tif (parent) {\n\t\tse = rb_entry(parent, struct swap_extent, rb_node);\n\t\tBUG_ON(se->start_page + se->nr_pages != start_page);\n\t\tif (se->start_block + se->nr_pages == start_block) {\n\t\t\t \n\t\t\tse->nr_pages += nr_pages;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t \n\tnew_se = kmalloc(sizeof(*se), GFP_KERNEL);\n\tif (new_se == NULL)\n\t\treturn -ENOMEM;\n\tnew_se->start_page = start_page;\n\tnew_se->nr_pages = nr_pages;\n\tnew_se->start_block = start_block;\n\n\trb_link_node(&new_se->rb_node, parent, link);\n\trb_insert_color(&new_se->rb_node, &sis->swap_extent_root);\n\treturn 1;\n}\nEXPORT_SYMBOL_GPL(add_swap_extent);\n\n \nstatic int setup_swap_extents(struct swap_info_struct *sis, sector_t *span)\n{\n\tstruct file *swap_file = sis->swap_file;\n\tstruct address_space *mapping = swap_file->f_mapping;\n\tstruct inode *inode = mapping->host;\n\tint ret;\n\n\tif (S_ISBLK(inode->i_mode)) {\n\t\tret = add_swap_extent(sis, 0, sis->max, 0);\n\t\t*span = sis->pages;\n\t\treturn ret;\n\t}\n\n\tif (mapping->a_ops->swap_activate) {\n\t\tret = mapping->a_ops->swap_activate(sis, swap_file, span);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\tsis->flags |= SWP_ACTIVATED;\n\t\tif ((sis->flags & SWP_FS_OPS) &&\n\t\t    sio_pool_init() != 0) {\n\t\t\tdestroy_swap_extents(sis);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\treturn ret;\n\t}\n\n\treturn generic_swapfile_activate(sis, swap_file, span);\n}\n\nstatic int swap_node(struct swap_info_struct *p)\n{\n\tstruct block_device *bdev;\n\n\tif (p->bdev)\n\t\tbdev = p->bdev;\n\telse\n\t\tbdev = p->swap_file->f_inode->i_sb->s_bdev;\n\n\treturn bdev ? bdev->bd_disk->node_id : NUMA_NO_NODE;\n}\n\nstatic void setup_swap_info(struct swap_info_struct *p, int prio,\n\t\t\t    unsigned char *swap_map,\n\t\t\t    struct swap_cluster_info *cluster_info)\n{\n\tint i;\n\n\tif (prio >= 0)\n\t\tp->prio = prio;\n\telse\n\t\tp->prio = --least_priority;\n\t \n\tp->list.prio = -p->prio;\n\tfor_each_node(i) {\n\t\tif (p->prio >= 0)\n\t\t\tp->avail_lists[i].prio = -p->prio;\n\t\telse {\n\t\t\tif (swap_node(p) == i)\n\t\t\t\tp->avail_lists[i].prio = 1;\n\t\t\telse\n\t\t\t\tp->avail_lists[i].prio = -p->prio;\n\t\t}\n\t}\n\tp->swap_map = swap_map;\n\tp->cluster_info = cluster_info;\n}\n\nstatic void _enable_swap_info(struct swap_info_struct *p)\n{\n\tp->flags |= SWP_WRITEOK;\n\tatomic_long_add(p->pages, &nr_swap_pages);\n\ttotal_swap_pages += p->pages;\n\n\tassert_spin_locked(&swap_lock);\n\t \n\tplist_add(&p->list, &swap_active_head);\n\n\t \n\tif (p->highest_bit)\n\t\tadd_to_avail_list(p);\n}\n\nstatic void enable_swap_info(struct swap_info_struct *p, int prio,\n\t\t\t\tunsigned char *swap_map,\n\t\t\t\tstruct swap_cluster_info *cluster_info)\n{\n\tzswap_swapon(p->type);\n\n\tspin_lock(&swap_lock);\n\tspin_lock(&p->lock);\n\tsetup_swap_info(p, prio, swap_map, cluster_info);\n\tspin_unlock(&p->lock);\n\tspin_unlock(&swap_lock);\n\t \n\tpercpu_ref_resurrect(&p->users);\n\tspin_lock(&swap_lock);\n\tspin_lock(&p->lock);\n\t_enable_swap_info(p);\n\tspin_unlock(&p->lock);\n\tspin_unlock(&swap_lock);\n}\n\nstatic void reinsert_swap_info(struct swap_info_struct *p)\n{\n\tspin_lock(&swap_lock);\n\tspin_lock(&p->lock);\n\tsetup_swap_info(p, p->prio, p->swap_map, p->cluster_info);\n\t_enable_swap_info(p);\n\tspin_unlock(&p->lock);\n\tspin_unlock(&swap_lock);\n}\n\nbool has_usable_swap(void)\n{\n\tbool ret = true;\n\n\tspin_lock(&swap_lock);\n\tif (plist_head_empty(&swap_active_head))\n\t\tret = false;\n\tspin_unlock(&swap_lock);\n\treturn ret;\n}\n\nSYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n{\n\tstruct swap_info_struct *p = NULL;\n\tunsigned char *swap_map;\n\tstruct swap_cluster_info *cluster_info;\n\tstruct file *swap_file, *victim;\n\tstruct address_space *mapping;\n\tstruct inode *inode;\n\tstruct filename *pathname;\n\tint err, found = 0;\n\tunsigned int old_block_size;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tBUG_ON(!current->mm);\n\n\tpathname = getname(specialfile);\n\tif (IS_ERR(pathname))\n\t\treturn PTR_ERR(pathname);\n\n\tvictim = file_open_name(pathname, O_RDWR|O_LARGEFILE, 0);\n\terr = PTR_ERR(victim);\n\tif (IS_ERR(victim))\n\t\tgoto out;\n\n\tmapping = victim->f_mapping;\n\tspin_lock(&swap_lock);\n\tplist_for_each_entry(p, &swap_active_head, list) {\n\t\tif (p->flags & SWP_WRITEOK) {\n\t\t\tif (p->swap_file->f_mapping == mapping) {\n\t\t\t\tfound = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\tif (!found) {\n\t\terr = -EINVAL;\n\t\tspin_unlock(&swap_lock);\n\t\tgoto out_dput;\n\t}\n\tif (!security_vm_enough_memory_mm(current->mm, p->pages))\n\t\tvm_unacct_memory(p->pages);\n\telse {\n\t\terr = -ENOMEM;\n\t\tspin_unlock(&swap_lock);\n\t\tgoto out_dput;\n\t}\n\tspin_lock(&p->lock);\n\tdel_from_avail_list(p);\n\tif (p->prio < 0) {\n\t\tstruct swap_info_struct *si = p;\n\t\tint nid;\n\n\t\tplist_for_each_entry_continue(si, &swap_active_head, list) {\n\t\t\tsi->prio++;\n\t\t\tsi->list.prio--;\n\t\t\tfor_each_node(nid) {\n\t\t\t\tif (si->avail_lists[nid].prio != 1)\n\t\t\t\t\tsi->avail_lists[nid].prio--;\n\t\t\t}\n\t\t}\n\t\tleast_priority++;\n\t}\n\tplist_del(&p->list, &swap_active_head);\n\tatomic_long_sub(p->pages, &nr_swap_pages);\n\ttotal_swap_pages -= p->pages;\n\tp->flags &= ~SWP_WRITEOK;\n\tspin_unlock(&p->lock);\n\tspin_unlock(&swap_lock);\n\n\tdisable_swap_slots_cache_lock();\n\n\tset_current_oom_origin();\n\terr = try_to_unuse(p->type);\n\tclear_current_oom_origin();\n\n\tif (err) {\n\t\t \n\t\treinsert_swap_info(p);\n\t\treenable_swap_slots_cache_unlock();\n\t\tgoto out_dput;\n\t}\n\n\treenable_swap_slots_cache_unlock();\n\n\t \n\tpercpu_ref_kill(&p->users);\n\tsynchronize_rcu();\n\twait_for_completion(&p->comp);\n\n\tflush_work(&p->discard_work);\n\n\tdestroy_swap_extents(p);\n\tif (p->flags & SWP_CONTINUED)\n\t\tfree_swap_count_continuations(p);\n\n\tif (!p->bdev || !bdev_nonrot(p->bdev))\n\t\tatomic_dec(&nr_rotate_swap);\n\n\tmutex_lock(&swapon_mutex);\n\tspin_lock(&swap_lock);\n\tspin_lock(&p->lock);\n\tdrain_mmlist();\n\n\t \n\tp->highest_bit = 0;\t\t \n\twhile (p->flags >= SWP_SCANNING) {\n\t\tspin_unlock(&p->lock);\n\t\tspin_unlock(&swap_lock);\n\t\tschedule_timeout_uninterruptible(1);\n\t\tspin_lock(&swap_lock);\n\t\tspin_lock(&p->lock);\n\t}\n\n\tswap_file = p->swap_file;\n\told_block_size = p->old_block_size;\n\tp->swap_file = NULL;\n\tp->max = 0;\n\tswap_map = p->swap_map;\n\tp->swap_map = NULL;\n\tcluster_info = p->cluster_info;\n\tp->cluster_info = NULL;\n\tspin_unlock(&p->lock);\n\tspin_unlock(&swap_lock);\n\tarch_swap_invalidate_area(p->type);\n\tzswap_swapoff(p->type);\n\tmutex_unlock(&swapon_mutex);\n\tfree_percpu(p->percpu_cluster);\n\tp->percpu_cluster = NULL;\n\tfree_percpu(p->cluster_next_cpu);\n\tp->cluster_next_cpu = NULL;\n\tvfree(swap_map);\n\tkvfree(cluster_info);\n\t \n\tswap_cgroup_swapoff(p->type);\n\texit_swap_address_space(p->type);\n\n\tinode = mapping->host;\n\tif (S_ISBLK(inode->i_mode)) {\n\t\tstruct block_device *bdev = I_BDEV(inode);\n\n\t\tset_blocksize(bdev, old_block_size);\n\t\tblkdev_put(bdev, p);\n\t}\n\n\tinode_lock(inode);\n\tinode->i_flags &= ~S_SWAPFILE;\n\tinode_unlock(inode);\n\tfilp_close(swap_file, NULL);\n\n\t \n\tspin_lock(&swap_lock);\n\tp->flags = 0;\n\tspin_unlock(&swap_lock);\n\n\terr = 0;\n\tatomic_inc(&proc_poll_event);\n\twake_up_interruptible(&proc_poll_wait);\n\nout_dput:\n\tfilp_close(victim, NULL);\nout:\n\tputname(pathname);\n\treturn err;\n}\n\n#ifdef CONFIG_PROC_FS\nstatic __poll_t swaps_poll(struct file *file, poll_table *wait)\n{\n\tstruct seq_file *seq = file->private_data;\n\n\tpoll_wait(file, &proc_poll_wait, wait);\n\n\tif (seq->poll_event != atomic_read(&proc_poll_event)) {\n\t\tseq->poll_event = atomic_read(&proc_poll_event);\n\t\treturn EPOLLIN | EPOLLRDNORM | EPOLLERR | EPOLLPRI;\n\t}\n\n\treturn EPOLLIN | EPOLLRDNORM;\n}\n\n \nstatic void *swap_start(struct seq_file *swap, loff_t *pos)\n{\n\tstruct swap_info_struct *si;\n\tint type;\n\tloff_t l = *pos;\n\n\tmutex_lock(&swapon_mutex);\n\n\tif (!l)\n\t\treturn SEQ_START_TOKEN;\n\n\tfor (type = 0; (si = swap_type_to_swap_info(type)); type++) {\n\t\tif (!(si->flags & SWP_USED) || !si->swap_map)\n\t\t\tcontinue;\n\t\tif (!--l)\n\t\t\treturn si;\n\t}\n\n\treturn NULL;\n}\n\nstatic void *swap_next(struct seq_file *swap, void *v, loff_t *pos)\n{\n\tstruct swap_info_struct *si = v;\n\tint type;\n\n\tif (v == SEQ_START_TOKEN)\n\t\ttype = 0;\n\telse\n\t\ttype = si->type + 1;\n\n\t++(*pos);\n\tfor (; (si = swap_type_to_swap_info(type)); type++) {\n\t\tif (!(si->flags & SWP_USED) || !si->swap_map)\n\t\t\tcontinue;\n\t\treturn si;\n\t}\n\n\treturn NULL;\n}\n\nstatic void swap_stop(struct seq_file *swap, void *v)\n{\n\tmutex_unlock(&swapon_mutex);\n}\n\nstatic int swap_show(struct seq_file *swap, void *v)\n{\n\tstruct swap_info_struct *si = v;\n\tstruct file *file;\n\tint len;\n\tunsigned long bytes, inuse;\n\n\tif (si == SEQ_START_TOKEN) {\n\t\tseq_puts(swap, \"Filename\\t\\t\\t\\tType\\t\\tSize\\t\\tUsed\\t\\tPriority\\n\");\n\t\treturn 0;\n\t}\n\n\tbytes = K(si->pages);\n\tinuse = K(READ_ONCE(si->inuse_pages));\n\n\tfile = si->swap_file;\n\tlen = seq_file_path(swap, file, \" \\t\\n\\\\\");\n\tseq_printf(swap, \"%*s%s\\t%lu\\t%s%lu\\t%s%d\\n\",\n\t\t\tlen < 40 ? 40 - len : 1, \" \",\n\t\t\tS_ISBLK(file_inode(file)->i_mode) ?\n\t\t\t\t\"partition\" : \"file\\t\",\n\t\t\tbytes, bytes < 10000000 ? \"\\t\" : \"\",\n\t\t\tinuse, inuse < 10000000 ? \"\\t\" : \"\",\n\t\t\tsi->prio);\n\treturn 0;\n}\n\nstatic const struct seq_operations swaps_op = {\n\t.start =\tswap_start,\n\t.next =\t\tswap_next,\n\t.stop =\t\tswap_stop,\n\t.show =\t\tswap_show\n};\n\nstatic int swaps_open(struct inode *inode, struct file *file)\n{\n\tstruct seq_file *seq;\n\tint ret;\n\n\tret = seq_open(file, &swaps_op);\n\tif (ret)\n\t\treturn ret;\n\n\tseq = file->private_data;\n\tseq->poll_event = atomic_read(&proc_poll_event);\n\treturn 0;\n}\n\nstatic const struct proc_ops swaps_proc_ops = {\n\t.proc_flags\t= PROC_ENTRY_PERMANENT,\n\t.proc_open\t= swaps_open,\n\t.proc_read\t= seq_read,\n\t.proc_lseek\t= seq_lseek,\n\t.proc_release\t= seq_release,\n\t.proc_poll\t= swaps_poll,\n};\n\nstatic int __init procswaps_init(void)\n{\n\tproc_create(\"swaps\", 0, NULL, &swaps_proc_ops);\n\treturn 0;\n}\n__initcall(procswaps_init);\n#endif  \n\n#ifdef MAX_SWAPFILES_CHECK\nstatic int __init max_swapfiles_check(void)\n{\n\tMAX_SWAPFILES_CHECK();\n\treturn 0;\n}\nlate_initcall(max_swapfiles_check);\n#endif\n\nstatic struct swap_info_struct *alloc_swap_info(void)\n{\n\tstruct swap_info_struct *p;\n\tstruct swap_info_struct *defer = NULL;\n\tunsigned int type;\n\tint i;\n\n\tp = kvzalloc(struct_size(p, avail_lists, nr_node_ids), GFP_KERNEL);\n\tif (!p)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (percpu_ref_init(&p->users, swap_users_ref_free,\n\t\t\t    PERCPU_REF_INIT_DEAD, GFP_KERNEL)) {\n\t\tkvfree(p);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tspin_lock(&swap_lock);\n\tfor (type = 0; type < nr_swapfiles; type++) {\n\t\tif (!(swap_info[type]->flags & SWP_USED))\n\t\t\tbreak;\n\t}\n\tif (type >= MAX_SWAPFILES) {\n\t\tspin_unlock(&swap_lock);\n\t\tpercpu_ref_exit(&p->users);\n\t\tkvfree(p);\n\t\treturn ERR_PTR(-EPERM);\n\t}\n\tif (type >= nr_swapfiles) {\n\t\tp->type = type;\n\t\t \n\t\tsmp_store_release(&swap_info[type], p);  \n\t\tnr_swapfiles++;\n\t} else {\n\t\tdefer = p;\n\t\tp = swap_info[type];\n\t\t \n\t}\n\tp->swap_extent_root = RB_ROOT;\n\tplist_node_init(&p->list, 0);\n\tfor_each_node(i)\n\t\tplist_node_init(&p->avail_lists[i], 0);\n\tp->flags = SWP_USED;\n\tspin_unlock(&swap_lock);\n\tif (defer) {\n\t\tpercpu_ref_exit(&defer->users);\n\t\tkvfree(defer);\n\t}\n\tspin_lock_init(&p->lock);\n\tspin_lock_init(&p->cont_lock);\n\tinit_completion(&p->comp);\n\n\treturn p;\n}\n\nstatic int claim_swapfile(struct swap_info_struct *p, struct inode *inode)\n{\n\tint error;\n\n\tif (S_ISBLK(inode->i_mode)) {\n\t\tp->bdev = blkdev_get_by_dev(inode->i_rdev,\n\t\t\t\tBLK_OPEN_READ | BLK_OPEN_WRITE, p, NULL);\n\t\tif (IS_ERR(p->bdev)) {\n\t\t\terror = PTR_ERR(p->bdev);\n\t\t\tp->bdev = NULL;\n\t\t\treturn error;\n\t\t}\n\t\tp->old_block_size = block_size(p->bdev);\n\t\terror = set_blocksize(p->bdev, PAGE_SIZE);\n\t\tif (error < 0)\n\t\t\treturn error;\n\t\t \n\t\tif (bdev_is_zoned(p->bdev))\n\t\t\treturn -EINVAL;\n\t\tp->flags |= SWP_BLKDEV;\n\t} else if (S_ISREG(inode->i_mode)) {\n\t\tp->bdev = inode->i_sb->s_bdev;\n\t}\n\n\treturn 0;\n}\n\n\n \nunsigned long generic_max_swapfile_size(void)\n{\n\treturn swp_offset(pte_to_swp_entry(\n\t\t\tswp_entry_to_pte(swp_entry(0, ~0UL)))) + 1;\n}\n\n \n__weak unsigned long arch_max_swapfile_size(void)\n{\n\treturn generic_max_swapfile_size();\n}\n\nstatic unsigned long read_swap_header(struct swap_info_struct *p,\n\t\t\t\t\tunion swap_header *swap_header,\n\t\t\t\t\tstruct inode *inode)\n{\n\tint i;\n\tunsigned long maxpages;\n\tunsigned long swapfilepages;\n\tunsigned long last_page;\n\n\tif (memcmp(\"SWAPSPACE2\", swap_header->magic.magic, 10)) {\n\t\tpr_err(\"Unable to find swap-space signature\\n\");\n\t\treturn 0;\n\t}\n\n\t \n\tif (swab32(swap_header->info.version) == 1) {\n\t\tswab32s(&swap_header->info.version);\n\t\tswab32s(&swap_header->info.last_page);\n\t\tswab32s(&swap_header->info.nr_badpages);\n\t\tif (swap_header->info.nr_badpages > MAX_SWAP_BADPAGES)\n\t\t\treturn 0;\n\t\tfor (i = 0; i < swap_header->info.nr_badpages; i++)\n\t\t\tswab32s(&swap_header->info.badpages[i]);\n\t}\n\t \n\tif (swap_header->info.version != 1) {\n\t\tpr_warn(\"Unable to handle swap header version %d\\n\",\n\t\t\tswap_header->info.version);\n\t\treturn 0;\n\t}\n\n\tp->lowest_bit  = 1;\n\tp->cluster_next = 1;\n\tp->cluster_nr = 0;\n\n\tmaxpages = swapfile_maximum_size;\n\tlast_page = swap_header->info.last_page;\n\tif (!last_page) {\n\t\tpr_warn(\"Empty swap-file\\n\");\n\t\treturn 0;\n\t}\n\tif (last_page > maxpages) {\n\t\tpr_warn(\"Truncating oversized swap area, only using %luk out of %luk\\n\",\n\t\t\tK(maxpages), K(last_page));\n\t}\n\tif (maxpages > last_page) {\n\t\tmaxpages = last_page + 1;\n\t\t \n\t\tif ((unsigned int)maxpages == 0)\n\t\t\tmaxpages = UINT_MAX;\n\t}\n\tp->highest_bit = maxpages - 1;\n\n\tif (!maxpages)\n\t\treturn 0;\n\tswapfilepages = i_size_read(inode) >> PAGE_SHIFT;\n\tif (swapfilepages && maxpages > swapfilepages) {\n\t\tpr_warn(\"Swap area shorter than signature indicates\\n\");\n\t\treturn 0;\n\t}\n\tif (swap_header->info.nr_badpages && S_ISREG(inode->i_mode))\n\t\treturn 0;\n\tif (swap_header->info.nr_badpages > MAX_SWAP_BADPAGES)\n\t\treturn 0;\n\n\treturn maxpages;\n}\n\n#define SWAP_CLUSTER_INFO_COLS\t\t\t\t\t\t\\\n\tDIV_ROUND_UP(L1_CACHE_BYTES, sizeof(struct swap_cluster_info))\n#define SWAP_CLUSTER_SPACE_COLS\t\t\t\t\t\t\\\n\tDIV_ROUND_UP(SWAP_ADDRESS_SPACE_PAGES, SWAPFILE_CLUSTER)\n#define SWAP_CLUSTER_COLS\t\t\t\t\t\t\\\n\tmax_t(unsigned int, SWAP_CLUSTER_INFO_COLS, SWAP_CLUSTER_SPACE_COLS)\n\nstatic int setup_swap_map_and_extents(struct swap_info_struct *p,\n\t\t\t\t\tunion swap_header *swap_header,\n\t\t\t\t\tunsigned char *swap_map,\n\t\t\t\t\tstruct swap_cluster_info *cluster_info,\n\t\t\t\t\tunsigned long maxpages,\n\t\t\t\t\tsector_t *span)\n{\n\tunsigned int j, k;\n\tunsigned int nr_good_pages;\n\tint nr_extents;\n\tunsigned long nr_clusters = DIV_ROUND_UP(maxpages, SWAPFILE_CLUSTER);\n\tunsigned long col = p->cluster_next / SWAPFILE_CLUSTER % SWAP_CLUSTER_COLS;\n\tunsigned long i, idx;\n\n\tnr_good_pages = maxpages - 1;\t \n\n\tcluster_list_init(&p->free_clusters);\n\tcluster_list_init(&p->discard_clusters);\n\n\tfor (i = 0; i < swap_header->info.nr_badpages; i++) {\n\t\tunsigned int page_nr = swap_header->info.badpages[i];\n\t\tif (page_nr == 0 || page_nr > swap_header->info.last_page)\n\t\t\treturn -EINVAL;\n\t\tif (page_nr < maxpages) {\n\t\t\tswap_map[page_nr] = SWAP_MAP_BAD;\n\t\t\tnr_good_pages--;\n\t\t\t \n\t\t\tinc_cluster_info_page(p, cluster_info, page_nr);\n\t\t}\n\t}\n\n\t \n\tfor (i = maxpages; i < round_up(maxpages, SWAPFILE_CLUSTER); i++)\n\t\tinc_cluster_info_page(p, cluster_info, i);\n\n\tif (nr_good_pages) {\n\t\tswap_map[0] = SWAP_MAP_BAD;\n\t\t \n\t\tinc_cluster_info_page(p, cluster_info, 0);\n\t\tp->max = maxpages;\n\t\tp->pages = nr_good_pages;\n\t\tnr_extents = setup_swap_extents(p, span);\n\t\tif (nr_extents < 0)\n\t\t\treturn nr_extents;\n\t\tnr_good_pages = p->pages;\n\t}\n\tif (!nr_good_pages) {\n\t\tpr_warn(\"Empty swap-file\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!cluster_info)\n\t\treturn nr_extents;\n\n\n\t \n\tfor (k = 0; k < SWAP_CLUSTER_COLS; k++) {\n\t\tj = (k + col) % SWAP_CLUSTER_COLS;\n\t\tfor (i = 0; i < DIV_ROUND_UP(nr_clusters, SWAP_CLUSTER_COLS); i++) {\n\t\t\tidx = i * SWAP_CLUSTER_COLS + j;\n\t\t\tif (idx >= nr_clusters)\n\t\t\t\tcontinue;\n\t\t\tif (cluster_count(&cluster_info[idx]))\n\t\t\t\tcontinue;\n\t\t\tcluster_set_flag(&cluster_info[idx], CLUSTER_FLAG_FREE);\n\t\t\tcluster_list_add_tail(&p->free_clusters, cluster_info,\n\t\t\t\t\t      idx);\n\t\t}\n\t}\n\treturn nr_extents;\n}\n\nSYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n{\n\tstruct swap_info_struct *p;\n\tstruct filename *name;\n\tstruct file *swap_file = NULL;\n\tstruct address_space *mapping;\n\tstruct dentry *dentry;\n\tint prio;\n\tint error;\n\tunion swap_header *swap_header;\n\tint nr_extents;\n\tsector_t span;\n\tunsigned long maxpages;\n\tunsigned char *swap_map = NULL;\n\tstruct swap_cluster_info *cluster_info = NULL;\n\tstruct page *page = NULL;\n\tstruct inode *inode = NULL;\n\tbool inced_nr_rotate_swap = false;\n\n\tif (swap_flags & ~SWAP_FLAGS_VALID)\n\t\treturn -EINVAL;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!swap_avail_heads)\n\t\treturn -ENOMEM;\n\n\tp = alloc_swap_info();\n\tif (IS_ERR(p))\n\t\treturn PTR_ERR(p);\n\n\tINIT_WORK(&p->discard_work, swap_discard_work);\n\n\tname = getname(specialfile);\n\tif (IS_ERR(name)) {\n\t\terror = PTR_ERR(name);\n\t\tname = NULL;\n\t\tgoto bad_swap;\n\t}\n\tswap_file = file_open_name(name, O_RDWR|O_LARGEFILE, 0);\n\tif (IS_ERR(swap_file)) {\n\t\terror = PTR_ERR(swap_file);\n\t\tswap_file = NULL;\n\t\tgoto bad_swap;\n\t}\n\n\tp->swap_file = swap_file;\n\tmapping = swap_file->f_mapping;\n\tdentry = swap_file->f_path.dentry;\n\tinode = mapping->host;\n\n\terror = claim_swapfile(p, inode);\n\tif (unlikely(error))\n\t\tgoto bad_swap;\n\n\tinode_lock(inode);\n\tif (d_unlinked(dentry) || cant_mount(dentry)) {\n\t\terror = -ENOENT;\n\t\tgoto bad_swap_unlock_inode;\n\t}\n\tif (IS_SWAPFILE(inode)) {\n\t\terror = -EBUSY;\n\t\tgoto bad_swap_unlock_inode;\n\t}\n\n\t \n\tif (!mapping->a_ops->read_folio) {\n\t\terror = -EINVAL;\n\t\tgoto bad_swap_unlock_inode;\n\t}\n\tpage = read_mapping_page(mapping, 0, swap_file);\n\tif (IS_ERR(page)) {\n\t\terror = PTR_ERR(page);\n\t\tgoto bad_swap_unlock_inode;\n\t}\n\tswap_header = kmap(page);\n\n\tmaxpages = read_swap_header(p, swap_header, inode);\n\tif (unlikely(!maxpages)) {\n\t\terror = -EINVAL;\n\t\tgoto bad_swap_unlock_inode;\n\t}\n\n\t \n\tswap_map = vzalloc(maxpages);\n\tif (!swap_map) {\n\t\terror = -ENOMEM;\n\t\tgoto bad_swap_unlock_inode;\n\t}\n\n\tif (p->bdev && bdev_stable_writes(p->bdev))\n\t\tp->flags |= SWP_STABLE_WRITES;\n\n\tif (p->bdev && bdev_synchronous(p->bdev))\n\t\tp->flags |= SWP_SYNCHRONOUS_IO;\n\n\tif (p->bdev && bdev_nonrot(p->bdev)) {\n\t\tint cpu;\n\t\tunsigned long ci, nr_cluster;\n\n\t\tp->flags |= SWP_SOLIDSTATE;\n\t\tp->cluster_next_cpu = alloc_percpu(unsigned int);\n\t\tif (!p->cluster_next_cpu) {\n\t\t\terror = -ENOMEM;\n\t\t\tgoto bad_swap_unlock_inode;\n\t\t}\n\t\t \n\t\tfor_each_possible_cpu(cpu) {\n\t\t\tper_cpu(*p->cluster_next_cpu, cpu) =\n\t\t\t\tget_random_u32_inclusive(1, p->highest_bit);\n\t\t}\n\t\tnr_cluster = DIV_ROUND_UP(maxpages, SWAPFILE_CLUSTER);\n\n\t\tcluster_info = kvcalloc(nr_cluster, sizeof(*cluster_info),\n\t\t\t\t\tGFP_KERNEL);\n\t\tif (!cluster_info) {\n\t\t\terror = -ENOMEM;\n\t\t\tgoto bad_swap_unlock_inode;\n\t\t}\n\n\t\tfor (ci = 0; ci < nr_cluster; ci++)\n\t\t\tspin_lock_init(&((cluster_info + ci)->lock));\n\n\t\tp->percpu_cluster = alloc_percpu(struct percpu_cluster);\n\t\tif (!p->percpu_cluster) {\n\t\t\terror = -ENOMEM;\n\t\t\tgoto bad_swap_unlock_inode;\n\t\t}\n\t\tfor_each_possible_cpu(cpu) {\n\t\t\tstruct percpu_cluster *cluster;\n\t\t\tcluster = per_cpu_ptr(p->percpu_cluster, cpu);\n\t\t\tcluster_set_null(&cluster->index);\n\t\t}\n\t} else {\n\t\tatomic_inc(&nr_rotate_swap);\n\t\tinced_nr_rotate_swap = true;\n\t}\n\n\terror = swap_cgroup_swapon(p->type, maxpages);\n\tif (error)\n\t\tgoto bad_swap_unlock_inode;\n\n\tnr_extents = setup_swap_map_and_extents(p, swap_header, swap_map,\n\t\tcluster_info, maxpages, &span);\n\tif (unlikely(nr_extents < 0)) {\n\t\terror = nr_extents;\n\t\tgoto bad_swap_unlock_inode;\n\t}\n\n\tif ((swap_flags & SWAP_FLAG_DISCARD) &&\n\t    p->bdev && bdev_max_discard_sectors(p->bdev)) {\n\t\t \n\t\tp->flags |= (SWP_DISCARDABLE | SWP_AREA_DISCARD |\n\t\t\t     SWP_PAGE_DISCARD);\n\n\t\t \n\t\tif (swap_flags & SWAP_FLAG_DISCARD_ONCE)\n\t\t\tp->flags &= ~SWP_PAGE_DISCARD;\n\t\telse if (swap_flags & SWAP_FLAG_DISCARD_PAGES)\n\t\t\tp->flags &= ~SWP_AREA_DISCARD;\n\n\t\t \n\t\tif (p->flags & SWP_AREA_DISCARD) {\n\t\t\tint err = discard_swap(p);\n\t\t\tif (unlikely(err))\n\t\t\t\tpr_err(\"swapon: discard_swap(%p): %d\\n\",\n\t\t\t\t\tp, err);\n\t\t}\n\t}\n\n\terror = init_swap_address_space(p->type, maxpages);\n\tif (error)\n\t\tgoto bad_swap_unlock_inode;\n\n\t \n\tinode->i_flags |= S_SWAPFILE;\n\terror = inode_drain_writes(inode);\n\tif (error) {\n\t\tinode->i_flags &= ~S_SWAPFILE;\n\t\tgoto free_swap_address_space;\n\t}\n\n\tmutex_lock(&swapon_mutex);\n\tprio = -1;\n\tif (swap_flags & SWAP_FLAG_PREFER)\n\t\tprio =\n\t\t  (swap_flags & SWAP_FLAG_PRIO_MASK) >> SWAP_FLAG_PRIO_SHIFT;\n\tenable_swap_info(p, prio, swap_map, cluster_info);\n\n\tpr_info(\"Adding %uk swap on %s.  Priority:%d extents:%d across:%lluk %s%s%s%s\\n\",\n\t\tK(p->pages), name->name, p->prio, nr_extents,\n\t\tK((unsigned long long)span),\n\t\t(p->flags & SWP_SOLIDSTATE) ? \"SS\" : \"\",\n\t\t(p->flags & SWP_DISCARDABLE) ? \"D\" : \"\",\n\t\t(p->flags & SWP_AREA_DISCARD) ? \"s\" : \"\",\n\t\t(p->flags & SWP_PAGE_DISCARD) ? \"c\" : \"\");\n\n\tmutex_unlock(&swapon_mutex);\n\tatomic_inc(&proc_poll_event);\n\twake_up_interruptible(&proc_poll_wait);\n\n\terror = 0;\n\tgoto out;\nfree_swap_address_space:\n\texit_swap_address_space(p->type);\nbad_swap_unlock_inode:\n\tinode_unlock(inode);\nbad_swap:\n\tfree_percpu(p->percpu_cluster);\n\tp->percpu_cluster = NULL;\n\tfree_percpu(p->cluster_next_cpu);\n\tp->cluster_next_cpu = NULL;\n\tif (inode && S_ISBLK(inode->i_mode) && p->bdev) {\n\t\tset_blocksize(p->bdev, p->old_block_size);\n\t\tblkdev_put(p->bdev, p);\n\t}\n\tinode = NULL;\n\tdestroy_swap_extents(p);\n\tswap_cgroup_swapoff(p->type);\n\tspin_lock(&swap_lock);\n\tp->swap_file = NULL;\n\tp->flags = 0;\n\tspin_unlock(&swap_lock);\n\tvfree(swap_map);\n\tkvfree(cluster_info);\n\tif (inced_nr_rotate_swap)\n\t\tatomic_dec(&nr_rotate_swap);\n\tif (swap_file)\n\t\tfilp_close(swap_file, NULL);\nout:\n\tif (page && !IS_ERR(page)) {\n\t\tkunmap(page);\n\t\tput_page(page);\n\t}\n\tif (name)\n\t\tputname(name);\n\tif (inode)\n\t\tinode_unlock(inode);\n\tif (!error)\n\t\tenable_swap_slots_cache();\n\treturn error;\n}\n\nvoid si_swapinfo(struct sysinfo *val)\n{\n\tunsigned int type;\n\tunsigned long nr_to_be_unused = 0;\n\n\tspin_lock(&swap_lock);\n\tfor (type = 0; type < nr_swapfiles; type++) {\n\t\tstruct swap_info_struct *si = swap_info[type];\n\n\t\tif ((si->flags & SWP_USED) && !(si->flags & SWP_WRITEOK))\n\t\t\tnr_to_be_unused += READ_ONCE(si->inuse_pages);\n\t}\n\tval->freeswap = atomic_long_read(&nr_swap_pages) + nr_to_be_unused;\n\tval->totalswap = total_swap_pages + nr_to_be_unused;\n\tspin_unlock(&swap_lock);\n}\n\n \nstatic int __swap_duplicate(swp_entry_t entry, unsigned char usage)\n{\n\tstruct swap_info_struct *p;\n\tstruct swap_cluster_info *ci;\n\tunsigned long offset;\n\tunsigned char count;\n\tunsigned char has_cache;\n\tint err;\n\n\tp = swp_swap_info(entry);\n\n\toffset = swp_offset(entry);\n\tci = lock_cluster_or_swap_info(p, offset);\n\n\tcount = p->swap_map[offset];\n\n\t \n\tif (unlikely(swap_count(count) == SWAP_MAP_BAD)) {\n\t\terr = -ENOENT;\n\t\tgoto unlock_out;\n\t}\n\n\thas_cache = count & SWAP_HAS_CACHE;\n\tcount &= ~SWAP_HAS_CACHE;\n\terr = 0;\n\n\tif (usage == SWAP_HAS_CACHE) {\n\n\t\t \n\t\tif (!has_cache && count)\n\t\t\thas_cache = SWAP_HAS_CACHE;\n\t\telse if (has_cache)\t\t \n\t\t\terr = -EEXIST;\n\t\telse\t\t\t\t \n\t\t\terr = -ENOENT;\n\n\t} else if (count || has_cache) {\n\n\t\tif ((count & ~COUNT_CONTINUED) < SWAP_MAP_MAX)\n\t\t\tcount += usage;\n\t\telse if ((count & ~COUNT_CONTINUED) > SWAP_MAP_MAX)\n\t\t\terr = -EINVAL;\n\t\telse if (swap_count_continued(p, offset, count))\n\t\t\tcount = COUNT_CONTINUED;\n\t\telse\n\t\t\terr = -ENOMEM;\n\t} else\n\t\terr = -ENOENT;\t\t\t \n\n\tWRITE_ONCE(p->swap_map[offset], count | has_cache);\n\nunlock_out:\n\tunlock_cluster_or_swap_info(p, ci);\n\treturn err;\n}\n\n \nvoid swap_shmem_alloc(swp_entry_t entry)\n{\n\t__swap_duplicate(entry, SWAP_MAP_SHMEM);\n}\n\n \nint swap_duplicate(swp_entry_t entry)\n{\n\tint err = 0;\n\n\twhile (!err && __swap_duplicate(entry, 1) == -ENOMEM)\n\t\terr = add_swap_count_continuation(entry, GFP_ATOMIC);\n\treturn err;\n}\n\n \nint swapcache_prepare(swp_entry_t entry)\n{\n\treturn __swap_duplicate(entry, SWAP_HAS_CACHE);\n}\n\nstruct swap_info_struct *swp_swap_info(swp_entry_t entry)\n{\n\treturn swap_type_to_swap_info(swp_type(entry));\n}\n\nstruct swap_info_struct *page_swap_info(struct page *page)\n{\n\tswp_entry_t entry = page_swap_entry(page);\n\treturn swp_swap_info(entry);\n}\n\n \nstruct address_space *swapcache_mapping(struct folio *folio)\n{\n\treturn page_swap_info(&folio->page)->swap_file->f_mapping;\n}\nEXPORT_SYMBOL_GPL(swapcache_mapping);\n\npgoff_t __page_file_index(struct page *page)\n{\n\tswp_entry_t swap = page_swap_entry(page);\n\treturn swp_offset(swap);\n}\nEXPORT_SYMBOL_GPL(__page_file_index);\n\n \nint add_swap_count_continuation(swp_entry_t entry, gfp_t gfp_mask)\n{\n\tstruct swap_info_struct *si;\n\tstruct swap_cluster_info *ci;\n\tstruct page *head;\n\tstruct page *page;\n\tstruct page *list_page;\n\tpgoff_t offset;\n\tunsigned char count;\n\tint ret = 0;\n\n\t \n\tpage = alloc_page(gfp_mask | __GFP_HIGHMEM);\n\n\tsi = get_swap_device(entry);\n\tif (!si) {\n\t\t \n\t\tgoto outer;\n\t}\n\tspin_lock(&si->lock);\n\n\toffset = swp_offset(entry);\n\n\tci = lock_cluster(si, offset);\n\n\tcount = swap_count(si->swap_map[offset]);\n\n\tif ((count & ~COUNT_CONTINUED) != SWAP_MAP_MAX) {\n\t\t \n\t\tgoto out;\n\t}\n\n\tif (!page) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\thead = vmalloc_to_page(si->swap_map + offset);\n\toffset &= ~PAGE_MASK;\n\n\tspin_lock(&si->cont_lock);\n\t \n\tif (!page_private(head)) {\n\t\tBUG_ON(count & COUNT_CONTINUED);\n\t\tINIT_LIST_HEAD(&head->lru);\n\t\tset_page_private(head, SWP_CONTINUED);\n\t\tsi->flags |= SWP_CONTINUED;\n\t}\n\n\tlist_for_each_entry(list_page, &head->lru, lru) {\n\t\tunsigned char *map;\n\n\t\t \n\t\tif (!(count & COUNT_CONTINUED))\n\t\t\tgoto out_unlock_cont;\n\n\t\tmap = kmap_atomic(list_page) + offset;\n\t\tcount = *map;\n\t\tkunmap_atomic(map);\n\n\t\t \n\t\tif ((count & ~COUNT_CONTINUED) != SWAP_CONT_MAX)\n\t\t\tgoto out_unlock_cont;\n\t}\n\n\tlist_add_tail(&page->lru, &head->lru);\n\tpage = NULL;\t\t\t \nout_unlock_cont:\n\tspin_unlock(&si->cont_lock);\nout:\n\tunlock_cluster(ci);\n\tspin_unlock(&si->lock);\n\tput_swap_device(si);\nouter:\n\tif (page)\n\t\t__free_page(page);\n\treturn ret;\n}\n\n \nstatic bool swap_count_continued(struct swap_info_struct *si,\n\t\t\t\t pgoff_t offset, unsigned char count)\n{\n\tstruct page *head;\n\tstruct page *page;\n\tunsigned char *map;\n\tbool ret;\n\n\thead = vmalloc_to_page(si->swap_map + offset);\n\tif (page_private(head) != SWP_CONTINUED) {\n\t\tBUG_ON(count & COUNT_CONTINUED);\n\t\treturn false;\t\t \n\t}\n\n\tspin_lock(&si->cont_lock);\n\toffset &= ~PAGE_MASK;\n\tpage = list_next_entry(head, lru);\n\tmap = kmap_atomic(page) + offset;\n\n\tif (count == SWAP_MAP_MAX)\t \n\t\tgoto init_map;\t\t \n\n\tif (count == (SWAP_MAP_MAX | COUNT_CONTINUED)) {  \n\t\t \n\t\twhile (*map == (SWAP_CONT_MAX | COUNT_CONTINUED)) {\n\t\t\tkunmap_atomic(map);\n\t\t\tpage = list_next_entry(page, lru);\n\t\t\tBUG_ON(page == head);\n\t\t\tmap = kmap_atomic(page) + offset;\n\t\t}\n\t\tif (*map == SWAP_CONT_MAX) {\n\t\t\tkunmap_atomic(map);\n\t\t\tpage = list_next_entry(page, lru);\n\t\t\tif (page == head) {\n\t\t\t\tret = false;\t \n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tmap = kmap_atomic(page) + offset;\ninit_map:\t\t*map = 0;\t\t \n\t\t}\n\t\t*map += 1;\n\t\tkunmap_atomic(map);\n\t\twhile ((page = list_prev_entry(page, lru)) != head) {\n\t\t\tmap = kmap_atomic(page) + offset;\n\t\t\t*map = COUNT_CONTINUED;\n\t\t\tkunmap_atomic(map);\n\t\t}\n\t\tret = true;\t\t\t \n\n\t} else {\t\t\t\t \n\t\t \n\t\tBUG_ON(count != COUNT_CONTINUED);\n\t\twhile (*map == COUNT_CONTINUED) {\n\t\t\tkunmap_atomic(map);\n\t\t\tpage = list_next_entry(page, lru);\n\t\t\tBUG_ON(page == head);\n\t\t\tmap = kmap_atomic(page) + offset;\n\t\t}\n\t\tBUG_ON(*map == 0);\n\t\t*map -= 1;\n\t\tif (*map == 0)\n\t\t\tcount = 0;\n\t\tkunmap_atomic(map);\n\t\twhile ((page = list_prev_entry(page, lru)) != head) {\n\t\t\tmap = kmap_atomic(page) + offset;\n\t\t\t*map = SWAP_CONT_MAX | count;\n\t\t\tcount = COUNT_CONTINUED;\n\t\t\tkunmap_atomic(map);\n\t\t}\n\t\tret = count == COUNT_CONTINUED;\n\t}\nout:\n\tspin_unlock(&si->cont_lock);\n\treturn ret;\n}\n\n \nstatic void free_swap_count_continuations(struct swap_info_struct *si)\n{\n\tpgoff_t offset;\n\n\tfor (offset = 0; offset < si->max; offset += PAGE_SIZE) {\n\t\tstruct page *head;\n\t\thead = vmalloc_to_page(si->swap_map + offset);\n\t\tif (page_private(head)) {\n\t\t\tstruct page *page, *next;\n\n\t\t\tlist_for_each_entry_safe(page, next, &head->lru, lru) {\n\t\t\t\tlist_del(&page->lru);\n\t\t\t\t__free_page(page);\n\t\t\t}\n\t\t}\n\t}\n}\n\n#if defined(CONFIG_MEMCG) && defined(CONFIG_BLK_CGROUP)\nvoid __folio_throttle_swaprate(struct folio *folio, gfp_t gfp)\n{\n\tstruct swap_info_struct *si, *next;\n\tint nid = folio_nid(folio);\n\n\tif (!(gfp & __GFP_IO))\n\t\treturn;\n\n\tif (!blk_cgroup_congested())\n\t\treturn;\n\n\t \n\tif (current->throttle_disk)\n\t\treturn;\n\n\tspin_lock(&swap_avail_lock);\n\tplist_for_each_entry_safe(si, next, &swap_avail_heads[nid],\n\t\t\t\t  avail_lists[nid]) {\n\t\tif (si->bdev) {\n\t\t\tblkcg_schedule_throttle(si->bdev->bd_disk, true);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&swap_avail_lock);\n}\n#endif\n\nstatic int __init swapfile_init(void)\n{\n\tint nid;\n\n\tswap_avail_heads = kmalloc_array(nr_node_ids, sizeof(struct plist_head),\n\t\t\t\t\t GFP_KERNEL);\n\tif (!swap_avail_heads) {\n\t\tpr_emerg(\"Not enough memory for swap heads, swap is disabled\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tfor_each_node(nid)\n\t\tplist_head_init(&swap_avail_heads[nid]);\n\n\tswapfile_maximum_size = arch_max_swapfile_size();\n\n#ifdef CONFIG_MIGRATION\n\tif (swapfile_maximum_size >= (1UL << SWP_MIG_TOTAL_BITS))\n\t\tswap_migration_ad_supported = true;\n#endif\t \n\n\treturn 0;\n}\nsubsys_initcall(swapfile_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}