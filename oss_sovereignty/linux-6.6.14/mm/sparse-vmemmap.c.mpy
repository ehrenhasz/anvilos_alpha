{
  "module_name": "sparse-vmemmap.c",
  "hash_id": "a25b7c16c7de6ddc5a1a0950a78349c7c4572659931b7ce808d37481c3f82a16",
  "original_prompt": "Ingested from linux-6.6.14/mm/sparse-vmemmap.c",
  "human_readable_source": "\n \n#include <linux/mm.h>\n#include <linux/mmzone.h>\n#include <linux/memblock.h>\n#include <linux/memremap.h>\n#include <linux/highmem.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n#include <linux/vmalloc.h>\n#include <linux/sched.h>\n\n#include <asm/dma.h>\n#include <asm/pgalloc.h>\n\n \n\nstatic void * __ref __earlyonly_bootmem_alloc(int node,\n\t\t\t\tunsigned long size,\n\t\t\t\tunsigned long align,\n\t\t\t\tunsigned long goal)\n{\n\treturn memblock_alloc_try_nid_raw(size, align, goal,\n\t\t\t\t\t       MEMBLOCK_ALLOC_ACCESSIBLE, node);\n}\n\nvoid * __meminit vmemmap_alloc_block(unsigned long size, int node)\n{\n\t \n\tif (slab_is_available()) {\n\t\tgfp_t gfp_mask = GFP_KERNEL|__GFP_RETRY_MAYFAIL|__GFP_NOWARN;\n\t\tint order = get_order(size);\n\t\tstatic bool warned;\n\t\tstruct page *page;\n\n\t\tpage = alloc_pages_node(node, gfp_mask, order);\n\t\tif (page)\n\t\t\treturn page_address(page);\n\n\t\tif (!warned) {\n\t\t\twarn_alloc(gfp_mask & ~__GFP_NOWARN, NULL,\n\t\t\t\t   \"vmemmap alloc failure: order:%u\", order);\n\t\t\twarned = true;\n\t\t}\n\t\treturn NULL;\n\t} else\n\t\treturn __earlyonly_bootmem_alloc(node, size, size,\n\t\t\t\t__pa(MAX_DMA_ADDRESS));\n}\n\nstatic void * __meminit altmap_alloc_block_buf(unsigned long size,\n\t\t\t\t\t       struct vmem_altmap *altmap);\n\n \nvoid * __meminit vmemmap_alloc_block_buf(unsigned long size, int node,\n\t\t\t\t\t struct vmem_altmap *altmap)\n{\n\tvoid *ptr;\n\n\tif (altmap)\n\t\treturn altmap_alloc_block_buf(size, altmap);\n\n\tptr = sparse_buffer_alloc(size);\n\tif (!ptr)\n\t\tptr = vmemmap_alloc_block(size, node);\n\treturn ptr;\n}\n\nstatic unsigned long __meminit vmem_altmap_next_pfn(struct vmem_altmap *altmap)\n{\n\treturn altmap->base_pfn + altmap->reserve + altmap->alloc\n\t\t+ altmap->align;\n}\n\nstatic unsigned long __meminit vmem_altmap_nr_free(struct vmem_altmap *altmap)\n{\n\tunsigned long allocated = altmap->alloc + altmap->align;\n\n\tif (altmap->free > allocated)\n\t\treturn altmap->free - allocated;\n\treturn 0;\n}\n\nstatic void * __meminit altmap_alloc_block_buf(unsigned long size,\n\t\t\t\t\t       struct vmem_altmap *altmap)\n{\n\tunsigned long pfn, nr_pfns, nr_align;\n\n\tif (size & ~PAGE_MASK) {\n\t\tpr_warn_once(\"%s: allocations must be multiple of PAGE_SIZE (%ld)\\n\",\n\t\t\t\t__func__, size);\n\t\treturn NULL;\n\t}\n\n\tpfn = vmem_altmap_next_pfn(altmap);\n\tnr_pfns = size >> PAGE_SHIFT;\n\tnr_align = 1UL << find_first_bit(&nr_pfns, BITS_PER_LONG);\n\tnr_align = ALIGN(pfn, nr_align) - pfn;\n\tif (nr_pfns + nr_align > vmem_altmap_nr_free(altmap))\n\t\treturn NULL;\n\n\taltmap->alloc += nr_pfns;\n\taltmap->align += nr_align;\n\tpfn += nr_align;\n\n\tpr_debug(\"%s: pfn: %#lx alloc: %ld align: %ld nr: %#lx\\n\",\n\t\t\t__func__, pfn, altmap->alloc, altmap->align, nr_pfns);\n\treturn __va(__pfn_to_phys(pfn));\n}\n\nvoid __meminit vmemmap_verify(pte_t *pte, int node,\n\t\t\t\tunsigned long start, unsigned long end)\n{\n\tunsigned long pfn = pte_pfn(ptep_get(pte));\n\tint actual_node = early_pfn_to_nid(pfn);\n\n\tif (node_distance(actual_node, node) > LOCAL_DISTANCE)\n\t\tpr_warn_once(\"[%lx-%lx] potential offnode page_structs\\n\",\n\t\t\tstart, end - 1);\n}\n\npte_t * __meminit vmemmap_pte_populate(pmd_t *pmd, unsigned long addr, int node,\n\t\t\t\t       struct vmem_altmap *altmap,\n\t\t\t\t       struct page *reuse)\n{\n\tpte_t *pte = pte_offset_kernel(pmd, addr);\n\tif (pte_none(ptep_get(pte))) {\n\t\tpte_t entry;\n\t\tvoid *p;\n\n\t\tif (!reuse) {\n\t\t\tp = vmemmap_alloc_block_buf(PAGE_SIZE, node, altmap);\n\t\t\tif (!p)\n\t\t\t\treturn NULL;\n\t\t} else {\n\t\t\t \n\t\t\tget_page(reuse);\n\t\t\tp = page_to_virt(reuse);\n\t\t}\n\t\tentry = pfn_pte(__pa(p) >> PAGE_SHIFT, PAGE_KERNEL);\n\t\tset_pte_at(&init_mm, addr, pte, entry);\n\t}\n\treturn pte;\n}\n\nstatic void * __meminit vmemmap_alloc_block_zero(unsigned long size, int node)\n{\n\tvoid *p = vmemmap_alloc_block(size, node);\n\n\tif (!p)\n\t\treturn NULL;\n\tmemset(p, 0, size);\n\n\treturn p;\n}\n\npmd_t * __meminit vmemmap_pmd_populate(pud_t *pud, unsigned long addr, int node)\n{\n\tpmd_t *pmd = pmd_offset(pud, addr);\n\tif (pmd_none(*pmd)) {\n\t\tvoid *p = vmemmap_alloc_block_zero(PAGE_SIZE, node);\n\t\tif (!p)\n\t\t\treturn NULL;\n\t\tpmd_populate_kernel(&init_mm, pmd, p);\n\t}\n\treturn pmd;\n}\n\nvoid __weak __meminit pmd_init(void *addr)\n{\n}\n\npud_t * __meminit vmemmap_pud_populate(p4d_t *p4d, unsigned long addr, int node)\n{\n\tpud_t *pud = pud_offset(p4d, addr);\n\tif (pud_none(*pud)) {\n\t\tvoid *p = vmemmap_alloc_block_zero(PAGE_SIZE, node);\n\t\tif (!p)\n\t\t\treturn NULL;\n\t\tpmd_init(p);\n\t\tpud_populate(&init_mm, pud, p);\n\t}\n\treturn pud;\n}\n\nvoid __weak __meminit pud_init(void *addr)\n{\n}\n\np4d_t * __meminit vmemmap_p4d_populate(pgd_t *pgd, unsigned long addr, int node)\n{\n\tp4d_t *p4d = p4d_offset(pgd, addr);\n\tif (p4d_none(*p4d)) {\n\t\tvoid *p = vmemmap_alloc_block_zero(PAGE_SIZE, node);\n\t\tif (!p)\n\t\t\treturn NULL;\n\t\tpud_init(p);\n\t\tp4d_populate(&init_mm, p4d, p);\n\t}\n\treturn p4d;\n}\n\npgd_t * __meminit vmemmap_pgd_populate(unsigned long addr, int node)\n{\n\tpgd_t *pgd = pgd_offset_k(addr);\n\tif (pgd_none(*pgd)) {\n\t\tvoid *p = vmemmap_alloc_block_zero(PAGE_SIZE, node);\n\t\tif (!p)\n\t\t\treturn NULL;\n\t\tpgd_populate(&init_mm, pgd, p);\n\t}\n\treturn pgd;\n}\n\nstatic pte_t * __meminit vmemmap_populate_address(unsigned long addr, int node,\n\t\t\t\t\t      struct vmem_altmap *altmap,\n\t\t\t\t\t      struct page *reuse)\n{\n\tpgd_t *pgd;\n\tp4d_t *p4d;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\tpte_t *pte;\n\n\tpgd = vmemmap_pgd_populate(addr, node);\n\tif (!pgd)\n\t\treturn NULL;\n\tp4d = vmemmap_p4d_populate(pgd, addr, node);\n\tif (!p4d)\n\t\treturn NULL;\n\tpud = vmemmap_pud_populate(p4d, addr, node);\n\tif (!pud)\n\t\treturn NULL;\n\tpmd = vmemmap_pmd_populate(pud, addr, node);\n\tif (!pmd)\n\t\treturn NULL;\n\tpte = vmemmap_pte_populate(pmd, addr, node, altmap, reuse);\n\tif (!pte)\n\t\treturn NULL;\n\tvmemmap_verify(pte, node, addr, addr + PAGE_SIZE);\n\n\treturn pte;\n}\n\nstatic int __meminit vmemmap_populate_range(unsigned long start,\n\t\t\t\t\t    unsigned long end, int node,\n\t\t\t\t\t    struct vmem_altmap *altmap,\n\t\t\t\t\t    struct page *reuse)\n{\n\tunsigned long addr = start;\n\tpte_t *pte;\n\n\tfor (; addr < end; addr += PAGE_SIZE) {\n\t\tpte = vmemmap_populate_address(addr, node, altmap, reuse);\n\t\tif (!pte)\n\t\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nint __meminit vmemmap_populate_basepages(unsigned long start, unsigned long end,\n\t\t\t\t\t int node, struct vmem_altmap *altmap)\n{\n\treturn vmemmap_populate_range(start, end, node, altmap, NULL);\n}\n\nvoid __weak __meminit vmemmap_set_pmd(pmd_t *pmd, void *p, int node,\n\t\t\t\t      unsigned long addr, unsigned long next)\n{\n}\n\nint __weak __meminit vmemmap_check_pmd(pmd_t *pmd, int node,\n\t\t\t\t       unsigned long addr, unsigned long next)\n{\n\treturn 0;\n}\n\nint __meminit vmemmap_populate_hugepages(unsigned long start, unsigned long end,\n\t\t\t\t\t int node, struct vmem_altmap *altmap)\n{\n\tunsigned long addr;\n\tunsigned long next;\n\tpgd_t *pgd;\n\tp4d_t *p4d;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\n\tfor (addr = start; addr < end; addr = next) {\n\t\tnext = pmd_addr_end(addr, end);\n\n\t\tpgd = vmemmap_pgd_populate(addr, node);\n\t\tif (!pgd)\n\t\t\treturn -ENOMEM;\n\n\t\tp4d = vmemmap_p4d_populate(pgd, addr, node);\n\t\tif (!p4d)\n\t\t\treturn -ENOMEM;\n\n\t\tpud = vmemmap_pud_populate(p4d, addr, node);\n\t\tif (!pud)\n\t\t\treturn -ENOMEM;\n\n\t\tpmd = pmd_offset(pud, addr);\n\t\tif (pmd_none(READ_ONCE(*pmd))) {\n\t\t\tvoid *p;\n\n\t\t\tp = vmemmap_alloc_block_buf(PMD_SIZE, node, altmap);\n\t\t\tif (p) {\n\t\t\t\tvmemmap_set_pmd(pmd, p, node, addr, next);\n\t\t\t\tcontinue;\n\t\t\t} else if (altmap) {\n\t\t\t\t \n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t} else if (vmemmap_check_pmd(pmd, node, addr, next))\n\t\t\tcontinue;\n\t\tif (vmemmap_populate_basepages(addr, next, node, altmap))\n\t\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\n#ifndef vmemmap_populate_compound_pages\n \nstatic bool __meminit reuse_compound_section(unsigned long start_pfn,\n\t\t\t\t\t     struct dev_pagemap *pgmap)\n{\n\tunsigned long nr_pages = pgmap_vmemmap_nr(pgmap);\n\tunsigned long offset = start_pfn -\n\t\tPHYS_PFN(pgmap->ranges[pgmap->nr_range].start);\n\n\treturn !IS_ALIGNED(offset, nr_pages) && nr_pages > PAGES_PER_SUBSECTION;\n}\n\nstatic pte_t * __meminit compound_section_tail_page(unsigned long addr)\n{\n\tpte_t *pte;\n\n\taddr -= PAGE_SIZE;\n\n\t \n\tpte = pte_offset_kernel(pmd_off_k(addr), addr);\n\tif (!pte)\n\t\treturn NULL;\n\n\treturn pte;\n}\n\nstatic int __meminit vmemmap_populate_compound_pages(unsigned long start_pfn,\n\t\t\t\t\t\t     unsigned long start,\n\t\t\t\t\t\t     unsigned long end, int node,\n\t\t\t\t\t\t     struct dev_pagemap *pgmap)\n{\n\tunsigned long size, addr;\n\tpte_t *pte;\n\tint rc;\n\n\tif (reuse_compound_section(start_pfn, pgmap)) {\n\t\tpte = compound_section_tail_page(start);\n\t\tif (!pte)\n\t\t\treturn -ENOMEM;\n\n\t\t \n\t\treturn vmemmap_populate_range(start, end, node, NULL,\n\t\t\t\t\t      pte_page(ptep_get(pte)));\n\t}\n\n\tsize = min(end - start, pgmap_vmemmap_nr(pgmap) * sizeof(struct page));\n\tfor (addr = start; addr < end; addr += size) {\n\t\tunsigned long next, last = addr + size;\n\n\t\t \n\t\tpte = vmemmap_populate_address(addr, node, NULL, NULL);\n\t\tif (!pte)\n\t\t\treturn -ENOMEM;\n\n\t\t \n\t\tnext = addr + PAGE_SIZE;\n\t\tpte = vmemmap_populate_address(next, node, NULL, NULL);\n\t\tif (!pte)\n\t\t\treturn -ENOMEM;\n\n\t\t \n\t\tnext += PAGE_SIZE;\n\t\trc = vmemmap_populate_range(next, last, node, NULL,\n\t\t\t\t\t    pte_page(ptep_get(pte)));\n\t\tif (rc)\n\t\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\n#endif\n\nstruct page * __meminit __populate_section_memmap(unsigned long pfn,\n\t\tunsigned long nr_pages, int nid, struct vmem_altmap *altmap,\n\t\tstruct dev_pagemap *pgmap)\n{\n\tunsigned long start = (unsigned long) pfn_to_page(pfn);\n\tunsigned long end = start + nr_pages * sizeof(struct page);\n\tint r;\n\n\tif (WARN_ON_ONCE(!IS_ALIGNED(pfn, PAGES_PER_SUBSECTION) ||\n\t\t!IS_ALIGNED(nr_pages, PAGES_PER_SUBSECTION)))\n\t\treturn NULL;\n\n\tif (vmemmap_can_optimize(altmap, pgmap))\n\t\tr = vmemmap_populate_compound_pages(pfn, start, end, nid, pgmap);\n\telse\n\t\tr = vmemmap_populate(start, end, nid, altmap);\n\n\tif (r < 0)\n\t\treturn NULL;\n\n\treturn pfn_to_page(pfn);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}