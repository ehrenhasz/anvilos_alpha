{
  "module_name": "highmem.c",
  "hash_id": "b89577c1f3acfe2dfdc4cfb6641ff1d48110eee50be561c025e09cc2eee4d90d",
  "original_prompt": "Ingested from linux-6.6.14/mm/highmem.c",
  "human_readable_source": "\n \n\n#include <linux/mm.h>\n#include <linux/export.h>\n#include <linux/swap.h>\n#include <linux/bio.h>\n#include <linux/pagemap.h>\n#include <linux/mempool.h>\n#include <linux/init.h>\n#include <linux/hash.h>\n#include <linux/highmem.h>\n#include <linux/kgdb.h>\n#include <asm/tlbflush.h>\n#include <linux/vmalloc.h>\n\n#ifdef CONFIG_KMAP_LOCAL\nstatic inline int kmap_local_calc_idx(int idx)\n{\n\treturn idx + KM_MAX_IDX * smp_processor_id();\n}\n\n#ifndef arch_kmap_local_map_idx\n#define arch_kmap_local_map_idx(idx, pfn)\tkmap_local_calc_idx(idx)\n#endif\n#endif  \n\n \n#ifdef CONFIG_HIGHMEM\n\n \n#ifndef get_pkmap_color\n\n \nstatic inline unsigned int get_pkmap_color(struct page *page)\n{\n\treturn 0;\n}\n#define get_pkmap_color get_pkmap_color\n\n \nstatic inline unsigned int get_next_pkmap_nr(unsigned int color)\n{\n\tstatic unsigned int last_pkmap_nr;\n\n\tlast_pkmap_nr = (last_pkmap_nr + 1) & LAST_PKMAP_MASK;\n\treturn last_pkmap_nr;\n}\n\n \nstatic inline int no_more_pkmaps(unsigned int pkmap_nr, unsigned int color)\n{\n\treturn pkmap_nr == 0;\n}\n\n \nstatic inline int get_pkmap_entries_count(unsigned int color)\n{\n\treturn LAST_PKMAP;\n}\n\n \nstatic inline wait_queue_head_t *get_pkmap_wait_queue_head(unsigned int color)\n{\n\tstatic DECLARE_WAIT_QUEUE_HEAD(pkmap_map_wait);\n\n\treturn &pkmap_map_wait;\n}\n#endif\n\natomic_long_t _totalhigh_pages __read_mostly;\nEXPORT_SYMBOL(_totalhigh_pages);\n\nunsigned int __nr_free_highpages(void)\n{\n\tstruct zone *zone;\n\tunsigned int pages = 0;\n\n\tfor_each_populated_zone(zone) {\n\t\tif (is_highmem(zone))\n\t\t\tpages += zone_page_state(zone, NR_FREE_PAGES);\n\t}\n\n\treturn pages;\n}\n\nstatic int pkmap_count[LAST_PKMAP];\nstatic  __cacheline_aligned_in_smp DEFINE_SPINLOCK(kmap_lock);\n\npte_t *pkmap_page_table;\n\n \n#ifdef ARCH_NEEDS_KMAP_HIGH_GET\n#define lock_kmap()             spin_lock_irq(&kmap_lock)\n#define unlock_kmap()           spin_unlock_irq(&kmap_lock)\n#define lock_kmap_any(flags)    spin_lock_irqsave(&kmap_lock, flags)\n#define unlock_kmap_any(flags)  spin_unlock_irqrestore(&kmap_lock, flags)\n#else\n#define lock_kmap()             spin_lock(&kmap_lock)\n#define unlock_kmap()           spin_unlock(&kmap_lock)\n#define lock_kmap_any(flags)    \\\n\t\tdo { spin_lock(&kmap_lock); (void)(flags); } while (0)\n#define unlock_kmap_any(flags)  \\\n\t\tdo { spin_unlock(&kmap_lock); (void)(flags); } while (0)\n#endif\n\nstruct page *__kmap_to_page(void *vaddr)\n{\n\tunsigned long base = (unsigned long) vaddr & PAGE_MASK;\n\tstruct kmap_ctrl *kctrl = &current->kmap_ctrl;\n\tunsigned long addr = (unsigned long)vaddr;\n\tint i;\n\n\t \n\tif (WARN_ON_ONCE(addr >= PKMAP_ADDR(0) &&\n\t\t\t addr < PKMAP_ADDR(LAST_PKMAP)))\n\t\treturn pte_page(ptep_get(&pkmap_page_table[PKMAP_NR(addr)]));\n\n\t \n\tif (WARN_ON_ONCE(base >= __fix_to_virt(FIX_KMAP_END) &&\n\t\t\t base < __fix_to_virt(FIX_KMAP_BEGIN))) {\n\t\tfor (i = 0; i < kctrl->idx; i++) {\n\t\t\tunsigned long base_addr;\n\t\t\tint idx;\n\n\t\t\tidx = arch_kmap_local_map_idx(i, pte_pfn(pteval));\n\t\t\tbase_addr = __fix_to_virt(FIX_KMAP_BEGIN + idx);\n\n\t\t\tif (base_addr == base)\n\t\t\t\treturn pte_page(kctrl->pteval[i]);\n\t\t}\n\t}\n\n\treturn virt_to_page(vaddr);\n}\nEXPORT_SYMBOL(__kmap_to_page);\n\nstatic void flush_all_zero_pkmaps(void)\n{\n\tint i;\n\tint need_flush = 0;\n\n\tflush_cache_kmaps();\n\n\tfor (i = 0; i < LAST_PKMAP; i++) {\n\t\tstruct page *page;\n\t\tpte_t ptent;\n\n\t\t \n\t\tif (pkmap_count[i] != 1)\n\t\t\tcontinue;\n\t\tpkmap_count[i] = 0;\n\n\t\t \n\t\tptent = ptep_get(&pkmap_page_table[i]);\n\t\tBUG_ON(pte_none(ptent));\n\n\t\t \n\t\tpage = pte_page(ptent);\n\t\tpte_clear(&init_mm, PKMAP_ADDR(i), &pkmap_page_table[i]);\n\n\t\tset_page_address(page, NULL);\n\t\tneed_flush = 1;\n\t}\n\tif (need_flush)\n\t\tflush_tlb_kernel_range(PKMAP_ADDR(0), PKMAP_ADDR(LAST_PKMAP));\n}\n\nvoid __kmap_flush_unused(void)\n{\n\tlock_kmap();\n\tflush_all_zero_pkmaps();\n\tunlock_kmap();\n}\n\nstatic inline unsigned long map_new_virtual(struct page *page)\n{\n\tunsigned long vaddr;\n\tint count;\n\tunsigned int last_pkmap_nr;\n\tunsigned int color = get_pkmap_color(page);\n\nstart:\n\tcount = get_pkmap_entries_count(color);\n\t \n\tfor (;;) {\n\t\tlast_pkmap_nr = get_next_pkmap_nr(color);\n\t\tif (no_more_pkmaps(last_pkmap_nr, color)) {\n\t\t\tflush_all_zero_pkmaps();\n\t\t\tcount = get_pkmap_entries_count(color);\n\t\t}\n\t\tif (!pkmap_count[last_pkmap_nr])\n\t\t\tbreak;\t \n\t\tif (--count)\n\t\t\tcontinue;\n\n\t\t \n\t\t{\n\t\t\tDECLARE_WAITQUEUE(wait, current);\n\t\t\twait_queue_head_t *pkmap_map_wait =\n\t\t\t\tget_pkmap_wait_queue_head(color);\n\n\t\t\t__set_current_state(TASK_UNINTERRUPTIBLE);\n\t\t\tadd_wait_queue(pkmap_map_wait, &wait);\n\t\t\tunlock_kmap();\n\t\t\tschedule();\n\t\t\tremove_wait_queue(pkmap_map_wait, &wait);\n\t\t\tlock_kmap();\n\n\t\t\t \n\t\t\tif (page_address(page))\n\t\t\t\treturn (unsigned long)page_address(page);\n\n\t\t\t \n\t\t\tgoto start;\n\t\t}\n\t}\n\tvaddr = PKMAP_ADDR(last_pkmap_nr);\n\tset_pte_at(&init_mm, vaddr,\n\t\t   &(pkmap_page_table[last_pkmap_nr]), mk_pte(page, kmap_prot));\n\n\tpkmap_count[last_pkmap_nr] = 1;\n\tset_page_address(page, (void *)vaddr);\n\n\treturn vaddr;\n}\n\n \nvoid *kmap_high(struct page *page)\n{\n\tunsigned long vaddr;\n\n\t \n\tlock_kmap();\n\tvaddr = (unsigned long)page_address(page);\n\tif (!vaddr)\n\t\tvaddr = map_new_virtual(page);\n\tpkmap_count[PKMAP_NR(vaddr)]++;\n\tBUG_ON(pkmap_count[PKMAP_NR(vaddr)] < 2);\n\tunlock_kmap();\n\treturn (void *) vaddr;\n}\nEXPORT_SYMBOL(kmap_high);\n\n#ifdef ARCH_NEEDS_KMAP_HIGH_GET\n \nvoid *kmap_high_get(struct page *page)\n{\n\tunsigned long vaddr, flags;\n\n\tlock_kmap_any(flags);\n\tvaddr = (unsigned long)page_address(page);\n\tif (vaddr) {\n\t\tBUG_ON(pkmap_count[PKMAP_NR(vaddr)] < 1);\n\t\tpkmap_count[PKMAP_NR(vaddr)]++;\n\t}\n\tunlock_kmap_any(flags);\n\treturn (void *) vaddr;\n}\n#endif\n\n \nvoid kunmap_high(struct page *page)\n{\n\tunsigned long vaddr;\n\tunsigned long nr;\n\tunsigned long flags;\n\tint need_wakeup;\n\tunsigned int color = get_pkmap_color(page);\n\twait_queue_head_t *pkmap_map_wait;\n\n\tlock_kmap_any(flags);\n\tvaddr = (unsigned long)page_address(page);\n\tBUG_ON(!vaddr);\n\tnr = PKMAP_NR(vaddr);\n\n\t \n\tneed_wakeup = 0;\n\tswitch (--pkmap_count[nr]) {\n\tcase 0:\n\t\tBUG();\n\tcase 1:\n\t\t \n\t\tpkmap_map_wait = get_pkmap_wait_queue_head(color);\n\t\tneed_wakeup = waitqueue_active(pkmap_map_wait);\n\t}\n\tunlock_kmap_any(flags);\n\n\t \n\tif (need_wakeup)\n\t\twake_up(pkmap_map_wait);\n}\nEXPORT_SYMBOL(kunmap_high);\n\nvoid zero_user_segments(struct page *page, unsigned start1, unsigned end1,\n\t\tunsigned start2, unsigned end2)\n{\n\tunsigned int i;\n\n\tBUG_ON(end1 > page_size(page) || end2 > page_size(page));\n\n\tif (start1 >= end1)\n\t\tstart1 = end1 = 0;\n\tif (start2 >= end2)\n\t\tstart2 = end2 = 0;\n\n\tfor (i = 0; i < compound_nr(page); i++) {\n\t\tvoid *kaddr = NULL;\n\n\t\tif (start1 >= PAGE_SIZE) {\n\t\t\tstart1 -= PAGE_SIZE;\n\t\t\tend1 -= PAGE_SIZE;\n\t\t} else {\n\t\t\tunsigned this_end = min_t(unsigned, end1, PAGE_SIZE);\n\n\t\t\tif (end1 > start1) {\n\t\t\t\tkaddr = kmap_local_page(page + i);\n\t\t\t\tmemset(kaddr + start1, 0, this_end - start1);\n\t\t\t}\n\t\t\tend1 -= this_end;\n\t\t\tstart1 = 0;\n\t\t}\n\n\t\tif (start2 >= PAGE_SIZE) {\n\t\t\tstart2 -= PAGE_SIZE;\n\t\t\tend2 -= PAGE_SIZE;\n\t\t} else {\n\t\t\tunsigned this_end = min_t(unsigned, end2, PAGE_SIZE);\n\n\t\t\tif (end2 > start2) {\n\t\t\t\tif (!kaddr)\n\t\t\t\t\tkaddr = kmap_local_page(page + i);\n\t\t\t\tmemset(kaddr + start2, 0, this_end - start2);\n\t\t\t}\n\t\t\tend2 -= this_end;\n\t\t\tstart2 = 0;\n\t\t}\n\n\t\tif (kaddr) {\n\t\t\tkunmap_local(kaddr);\n\t\t\tflush_dcache_page(page + i);\n\t\t}\n\n\t\tif (!end1 && !end2)\n\t\t\tbreak;\n\t}\n\n\tBUG_ON((start1 | start2 | end1 | end2) != 0);\n}\nEXPORT_SYMBOL(zero_user_segments);\n#endif  \n\n#ifdef CONFIG_KMAP_LOCAL\n\n#include <asm/kmap_size.h>\n\n \n#ifdef CONFIG_DEBUG_KMAP_LOCAL\n# define KM_INCR\t2\n#else\n# define KM_INCR\t1\n#endif\n\nstatic inline int kmap_local_idx_push(void)\n{\n\tWARN_ON_ONCE(in_hardirq() && !irqs_disabled());\n\tcurrent->kmap_ctrl.idx += KM_INCR;\n\tBUG_ON(current->kmap_ctrl.idx >= KM_MAX_IDX);\n\treturn current->kmap_ctrl.idx - 1;\n}\n\nstatic inline int kmap_local_idx(void)\n{\n\treturn current->kmap_ctrl.idx - 1;\n}\n\nstatic inline void kmap_local_idx_pop(void)\n{\n\tcurrent->kmap_ctrl.idx -= KM_INCR;\n\tBUG_ON(current->kmap_ctrl.idx < 0);\n}\n\n#ifndef arch_kmap_local_post_map\n# define arch_kmap_local_post_map(vaddr, pteval)\tdo { } while (0)\n#endif\n\n#ifndef arch_kmap_local_pre_unmap\n# define arch_kmap_local_pre_unmap(vaddr)\t\tdo { } while (0)\n#endif\n\n#ifndef arch_kmap_local_post_unmap\n# define arch_kmap_local_post_unmap(vaddr)\t\tdo { } while (0)\n#endif\n\n#ifndef arch_kmap_local_unmap_idx\n#define arch_kmap_local_unmap_idx(idx, vaddr)\tkmap_local_calc_idx(idx)\n#endif\n\n#ifndef arch_kmap_local_high_get\nstatic inline void *arch_kmap_local_high_get(struct page *page)\n{\n\treturn NULL;\n}\n#endif\n\n#ifndef arch_kmap_local_set_pte\n#define arch_kmap_local_set_pte(mm, vaddr, ptep, ptev)\t\\\n\tset_pte_at(mm, vaddr, ptep, ptev)\n#endif\n\n \nstatic inline bool kmap_high_unmap_local(unsigned long vaddr)\n{\n#ifdef ARCH_NEEDS_KMAP_HIGH_GET\n\tif (vaddr >= PKMAP_ADDR(0) && vaddr < PKMAP_ADDR(LAST_PKMAP)) {\n\t\tkunmap_high(pte_page(ptep_get(&pkmap_page_table[PKMAP_NR(vaddr)])));\n\t\treturn true;\n\t}\n#endif\n\treturn false;\n}\n\nstatic pte_t *__kmap_pte;\n\nstatic pte_t *kmap_get_pte(unsigned long vaddr, int idx)\n{\n\tif (IS_ENABLED(CONFIG_KMAP_LOCAL_NON_LINEAR_PTE_ARRAY))\n\t\t \n\t\treturn virt_to_kpte(vaddr);\n\tif (!__kmap_pte)\n\t\t__kmap_pte = virt_to_kpte(__fix_to_virt(FIX_KMAP_BEGIN));\n\treturn &__kmap_pte[-idx];\n}\n\nvoid *__kmap_local_pfn_prot(unsigned long pfn, pgprot_t prot)\n{\n\tpte_t pteval, *kmap_pte;\n\tunsigned long vaddr;\n\tint idx;\n\n\t \n\tmigrate_disable();\n\tpreempt_disable();\n\tidx = arch_kmap_local_map_idx(kmap_local_idx_push(), pfn);\n\tvaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);\n\tkmap_pte = kmap_get_pte(vaddr, idx);\n\tBUG_ON(!pte_none(ptep_get(kmap_pte)));\n\tpteval = pfn_pte(pfn, prot);\n\tarch_kmap_local_set_pte(&init_mm, vaddr, kmap_pte, pteval);\n\tarch_kmap_local_post_map(vaddr, pteval);\n\tcurrent->kmap_ctrl.pteval[kmap_local_idx()] = pteval;\n\tpreempt_enable();\n\n\treturn (void *)vaddr;\n}\nEXPORT_SYMBOL_GPL(__kmap_local_pfn_prot);\n\nvoid *__kmap_local_page_prot(struct page *page, pgprot_t prot)\n{\n\tvoid *kmap;\n\n\t \n\tif (!IS_ENABLED(CONFIG_DEBUG_KMAP_LOCAL_FORCE_MAP) && !PageHighMem(page))\n\t\treturn page_address(page);\n\n\t \n\tkmap = arch_kmap_local_high_get(page);\n\tif (kmap)\n\t\treturn kmap;\n\n\treturn __kmap_local_pfn_prot(page_to_pfn(page), prot);\n}\nEXPORT_SYMBOL(__kmap_local_page_prot);\n\nvoid kunmap_local_indexed(const void *vaddr)\n{\n\tunsigned long addr = (unsigned long) vaddr & PAGE_MASK;\n\tpte_t *kmap_pte;\n\tint idx;\n\n\tif (addr < __fix_to_virt(FIX_KMAP_END) ||\n\t    addr > __fix_to_virt(FIX_KMAP_BEGIN)) {\n\t\tif (IS_ENABLED(CONFIG_DEBUG_KMAP_LOCAL_FORCE_MAP)) {\n\t\t\t \n\t\t\tWARN_ON_ONCE(1);\n\t\t\treturn;\n\t\t}\n\t\t \n\t\tif (!kmap_high_unmap_local(addr))\n\t\t\tWARN_ON_ONCE(addr < PAGE_OFFSET);\n\t\treturn;\n\t}\n\n\tpreempt_disable();\n\tidx = arch_kmap_local_unmap_idx(kmap_local_idx(), addr);\n\tWARN_ON_ONCE(addr != __fix_to_virt(FIX_KMAP_BEGIN + idx));\n\n\tkmap_pte = kmap_get_pte(addr, idx);\n\tarch_kmap_local_pre_unmap(addr);\n\tpte_clear(&init_mm, addr, kmap_pte);\n\tarch_kmap_local_post_unmap(addr);\n\tcurrent->kmap_ctrl.pteval[kmap_local_idx()] = __pte(0);\n\tkmap_local_idx_pop();\n\tpreempt_enable();\n\tmigrate_enable();\n}\nEXPORT_SYMBOL(kunmap_local_indexed);\n\n \nvoid __kmap_local_sched_out(void)\n{\n\tstruct task_struct *tsk = current;\n\tpte_t *kmap_pte;\n\tint i;\n\n\t \n\tfor (i = 0; i < tsk->kmap_ctrl.idx; i++) {\n\t\tpte_t pteval = tsk->kmap_ctrl.pteval[i];\n\t\tunsigned long addr;\n\t\tint idx;\n\n\t\t \n\t\tif (IS_ENABLED(CONFIG_DEBUG_KMAP_LOCAL) && !(i & 0x01)) {\n\t\t\tWARN_ON_ONCE(pte_val(pteval) != 0);\n\t\t\tcontinue;\n\t\t}\n\t\tif (WARN_ON_ONCE(pte_none(pteval)))\n\t\t\tcontinue;\n\n\t\t \n\t\tidx = arch_kmap_local_map_idx(i, pte_pfn(pteval));\n\n\t\taddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);\n\t\tkmap_pte = kmap_get_pte(addr, idx);\n\t\tarch_kmap_local_pre_unmap(addr);\n\t\tpte_clear(&init_mm, addr, kmap_pte);\n\t\tarch_kmap_local_post_unmap(addr);\n\t}\n}\n\nvoid __kmap_local_sched_in(void)\n{\n\tstruct task_struct *tsk = current;\n\tpte_t *kmap_pte;\n\tint i;\n\n\t \n\tfor (i = 0; i < tsk->kmap_ctrl.idx; i++) {\n\t\tpte_t pteval = tsk->kmap_ctrl.pteval[i];\n\t\tunsigned long addr;\n\t\tint idx;\n\n\t\t \n\t\tif (IS_ENABLED(CONFIG_DEBUG_KMAP_LOCAL) && !(i & 0x01)) {\n\t\t\tWARN_ON_ONCE(pte_val(pteval) != 0);\n\t\t\tcontinue;\n\t\t}\n\t\tif (WARN_ON_ONCE(pte_none(pteval)))\n\t\t\tcontinue;\n\n\t\t \n\t\tidx = arch_kmap_local_map_idx(i, pte_pfn(pteval));\n\t\taddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);\n\t\tkmap_pte = kmap_get_pte(addr, idx);\n\t\tset_pte_at(&init_mm, addr, kmap_pte, pteval);\n\t\tarch_kmap_local_post_map(addr, pteval);\n\t}\n}\n\nvoid kmap_local_fork(struct task_struct *tsk)\n{\n\tif (WARN_ON_ONCE(tsk->kmap_ctrl.idx))\n\t\tmemset(&tsk->kmap_ctrl, 0, sizeof(tsk->kmap_ctrl));\n}\n\n#endif\n\n#if defined(HASHED_PAGE_VIRTUAL)\n\n#define PA_HASH_ORDER\t7\n\n \nstruct page_address_map {\n\tstruct page *page;\n\tvoid *virtual;\n\tstruct list_head list;\n};\n\nstatic struct page_address_map page_address_maps[LAST_PKMAP];\n\n \nstatic struct page_address_slot {\n\tstruct list_head lh;\t\t\t \n\tspinlock_t lock;\t\t\t \n} ____cacheline_aligned_in_smp page_address_htable[1<<PA_HASH_ORDER];\n\nstatic struct page_address_slot *page_slot(const struct page *page)\n{\n\treturn &page_address_htable[hash_ptr(page, PA_HASH_ORDER)];\n}\n\n \nvoid *page_address(const struct page *page)\n{\n\tunsigned long flags;\n\tvoid *ret;\n\tstruct page_address_slot *pas;\n\n\tif (!PageHighMem(page))\n\t\treturn lowmem_page_address(page);\n\n\tpas = page_slot(page);\n\tret = NULL;\n\tspin_lock_irqsave(&pas->lock, flags);\n\tif (!list_empty(&pas->lh)) {\n\t\tstruct page_address_map *pam;\n\n\t\tlist_for_each_entry(pam, &pas->lh, list) {\n\t\t\tif (pam->page == page) {\n\t\t\t\tret = pam->virtual;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tspin_unlock_irqrestore(&pas->lock, flags);\n\treturn ret;\n}\nEXPORT_SYMBOL(page_address);\n\n \nvoid set_page_address(struct page *page, void *virtual)\n{\n\tunsigned long flags;\n\tstruct page_address_slot *pas;\n\tstruct page_address_map *pam;\n\n\tBUG_ON(!PageHighMem(page));\n\n\tpas = page_slot(page);\n\tif (virtual) {\t\t \n\t\tpam = &page_address_maps[PKMAP_NR((unsigned long)virtual)];\n\t\tpam->page = page;\n\t\tpam->virtual = virtual;\n\n\t\tspin_lock_irqsave(&pas->lock, flags);\n\t\tlist_add_tail(&pam->list, &pas->lh);\n\t\tspin_unlock_irqrestore(&pas->lock, flags);\n\t} else {\t\t \n\t\tspin_lock_irqsave(&pas->lock, flags);\n\t\tlist_for_each_entry(pam, &pas->lh, list) {\n\t\t\tif (pam->page == page) {\n\t\t\t\tlist_del(&pam->list);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tspin_unlock_irqrestore(&pas->lock, flags);\n\t}\n\n\treturn;\n}\n\nvoid __init page_address_init(void)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(page_address_htable); i++) {\n\t\tINIT_LIST_HEAD(&page_address_htable[i].lh);\n\t\tspin_lock_init(&page_address_htable[i].lock);\n\t}\n}\n\n#endif\t \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}