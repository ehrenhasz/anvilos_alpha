{
  "module_name": "mremap.c",
  "hash_id": "30b2085bf6a8d989074db5abe6f1fd6dc466c005a0de06bf6ffd9720275c0bd8",
  "original_prompt": "Ingested from linux-6.6.14/mm/mremap.c",
  "human_readable_source": "\n \n\n#include <linux/mm.h>\n#include <linux/mm_inline.h>\n#include <linux/hugetlb.h>\n#include <linux/shm.h>\n#include <linux/ksm.h>\n#include <linux/mman.h>\n#include <linux/swap.h>\n#include <linux/capability.h>\n#include <linux/fs.h>\n#include <linux/swapops.h>\n#include <linux/highmem.h>\n#include <linux/security.h>\n#include <linux/syscalls.h>\n#include <linux/mmu_notifier.h>\n#include <linux/uaccess.h>\n#include <linux/userfaultfd_k.h>\n#include <linux/mempolicy.h>\n\n#include <asm/cacheflush.h>\n#include <asm/tlb.h>\n#include <asm/pgalloc.h>\n\n#include \"internal.h\"\n\nstatic pud_t *get_old_pud(struct mm_struct *mm, unsigned long addr)\n{\n\tpgd_t *pgd;\n\tp4d_t *p4d;\n\tpud_t *pud;\n\n\tpgd = pgd_offset(mm, addr);\n\tif (pgd_none_or_clear_bad(pgd))\n\t\treturn NULL;\n\n\tp4d = p4d_offset(pgd, addr);\n\tif (p4d_none_or_clear_bad(p4d))\n\t\treturn NULL;\n\n\tpud = pud_offset(p4d, addr);\n\tif (pud_none_or_clear_bad(pud))\n\t\treturn NULL;\n\n\treturn pud;\n}\n\nstatic pmd_t *get_old_pmd(struct mm_struct *mm, unsigned long addr)\n{\n\tpud_t *pud;\n\tpmd_t *pmd;\n\n\tpud = get_old_pud(mm, addr);\n\tif (!pud)\n\t\treturn NULL;\n\n\tpmd = pmd_offset(pud, addr);\n\tif (pmd_none(*pmd))\n\t\treturn NULL;\n\n\treturn pmd;\n}\n\nstatic pud_t *alloc_new_pud(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\t    unsigned long addr)\n{\n\tpgd_t *pgd;\n\tp4d_t *p4d;\n\n\tpgd = pgd_offset(mm, addr);\n\tp4d = p4d_alloc(mm, pgd, addr);\n\tif (!p4d)\n\t\treturn NULL;\n\n\treturn pud_alloc(mm, p4d, addr);\n}\n\nstatic pmd_t *alloc_new_pmd(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\t    unsigned long addr)\n{\n\tpud_t *pud;\n\tpmd_t *pmd;\n\n\tpud = alloc_new_pud(mm, vma, addr);\n\tif (!pud)\n\t\treturn NULL;\n\n\tpmd = pmd_alloc(mm, pud, addr);\n\tif (!pmd)\n\t\treturn NULL;\n\n\tVM_BUG_ON(pmd_trans_huge(*pmd));\n\n\treturn pmd;\n}\n\nstatic void take_rmap_locks(struct vm_area_struct *vma)\n{\n\tif (vma->vm_file)\n\t\ti_mmap_lock_write(vma->vm_file->f_mapping);\n\tif (vma->anon_vma)\n\t\tanon_vma_lock_write(vma->anon_vma);\n}\n\nstatic void drop_rmap_locks(struct vm_area_struct *vma)\n{\n\tif (vma->anon_vma)\n\t\tanon_vma_unlock_write(vma->anon_vma);\n\tif (vma->vm_file)\n\t\ti_mmap_unlock_write(vma->vm_file->f_mapping);\n}\n\nstatic pte_t move_soft_dirty_pte(pte_t pte)\n{\n\t \n#ifdef CONFIG_MEM_SOFT_DIRTY\n\tif (pte_present(pte))\n\t\tpte = pte_mksoft_dirty(pte);\n\telse if (is_swap_pte(pte))\n\t\tpte = pte_swp_mksoft_dirty(pte);\n#endif\n\treturn pte;\n}\n\nstatic int move_ptes(struct vm_area_struct *vma, pmd_t *old_pmd,\n\t\tunsigned long old_addr, unsigned long old_end,\n\t\tstruct vm_area_struct *new_vma, pmd_t *new_pmd,\n\t\tunsigned long new_addr, bool need_rmap_locks)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tpte_t *old_pte, *new_pte, pte;\n\tspinlock_t *old_ptl, *new_ptl;\n\tbool force_flush = false;\n\tunsigned long len = old_end - old_addr;\n\tint err = 0;\n\n\t \n\tif (need_rmap_locks)\n\t\ttake_rmap_locks(vma);\n\n\t \n\told_pte = pte_offset_map_lock(mm, old_pmd, old_addr, &old_ptl);\n\tif (!old_pte) {\n\t\terr = -EAGAIN;\n\t\tgoto out;\n\t}\n\tnew_pte = pte_offset_map_nolock(mm, new_pmd, new_addr, &new_ptl);\n\tif (!new_pte) {\n\t\tpte_unmap_unlock(old_pte, old_ptl);\n\t\terr = -EAGAIN;\n\t\tgoto out;\n\t}\n\tif (new_ptl != old_ptl)\n\t\tspin_lock_nested(new_ptl, SINGLE_DEPTH_NESTING);\n\tflush_tlb_batched_pending(vma->vm_mm);\n\tarch_enter_lazy_mmu_mode();\n\n\tfor (; old_addr < old_end; old_pte++, old_addr += PAGE_SIZE,\n\t\t\t\t   new_pte++, new_addr += PAGE_SIZE) {\n\t\tif (pte_none(ptep_get(old_pte)))\n\t\t\tcontinue;\n\n\t\tpte = ptep_get_and_clear(mm, old_addr, old_pte);\n\t\t \n\t\tif (pte_present(pte))\n\t\t\tforce_flush = true;\n\t\tpte = move_pte(pte, new_vma->vm_page_prot, old_addr, new_addr);\n\t\tpte = move_soft_dirty_pte(pte);\n\t\tset_pte_at(mm, new_addr, new_pte, pte);\n\t}\n\n\tarch_leave_lazy_mmu_mode();\n\tif (force_flush)\n\t\tflush_tlb_range(vma, old_end - len, old_end);\n\tif (new_ptl != old_ptl)\n\t\tspin_unlock(new_ptl);\n\tpte_unmap(new_pte - 1);\n\tpte_unmap_unlock(old_pte - 1, old_ptl);\nout:\n\tif (need_rmap_locks)\n\t\tdrop_rmap_locks(vma);\n\treturn err;\n}\n\n#ifndef arch_supports_page_table_move\n#define arch_supports_page_table_move arch_supports_page_table_move\nstatic inline bool arch_supports_page_table_move(void)\n{\n\treturn IS_ENABLED(CONFIG_HAVE_MOVE_PMD) ||\n\t\tIS_ENABLED(CONFIG_HAVE_MOVE_PUD);\n}\n#endif\n\n#ifdef CONFIG_HAVE_MOVE_PMD\nstatic bool move_normal_pmd(struct vm_area_struct *vma, unsigned long old_addr,\n\t\t  unsigned long new_addr, pmd_t *old_pmd, pmd_t *new_pmd)\n{\n\tspinlock_t *old_ptl, *new_ptl;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tpmd_t pmd;\n\n\tif (!arch_supports_page_table_move())\n\t\treturn false;\n\t \n\tif (WARN_ON_ONCE(!pmd_none(*new_pmd)))\n\t\treturn false;\n\n\t \n\told_ptl = pmd_lock(vma->vm_mm, old_pmd);\n\tnew_ptl = pmd_lockptr(mm, new_pmd);\n\tif (new_ptl != old_ptl)\n\t\tspin_lock_nested(new_ptl, SINGLE_DEPTH_NESTING);\n\n\t \n\tpmd = *old_pmd;\n\tpmd_clear(old_pmd);\n\n\tVM_BUG_ON(!pmd_none(*new_pmd));\n\n\tpmd_populate(mm, new_pmd, pmd_pgtable(pmd));\n\tflush_tlb_range(vma, old_addr, old_addr + PMD_SIZE);\n\tif (new_ptl != old_ptl)\n\t\tspin_unlock(new_ptl);\n\tspin_unlock(old_ptl);\n\n\treturn true;\n}\n#else\nstatic inline bool move_normal_pmd(struct vm_area_struct *vma,\n\t\tunsigned long old_addr, unsigned long new_addr, pmd_t *old_pmd,\n\t\tpmd_t *new_pmd)\n{\n\treturn false;\n}\n#endif\n\n#if CONFIG_PGTABLE_LEVELS > 2 && defined(CONFIG_HAVE_MOVE_PUD)\nstatic bool move_normal_pud(struct vm_area_struct *vma, unsigned long old_addr,\n\t\t  unsigned long new_addr, pud_t *old_pud, pud_t *new_pud)\n{\n\tspinlock_t *old_ptl, *new_ptl;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tpud_t pud;\n\n\tif (!arch_supports_page_table_move())\n\t\treturn false;\n\t \n\tif (WARN_ON_ONCE(!pud_none(*new_pud)))\n\t\treturn false;\n\n\t \n\told_ptl = pud_lock(vma->vm_mm, old_pud);\n\tnew_ptl = pud_lockptr(mm, new_pud);\n\tif (new_ptl != old_ptl)\n\t\tspin_lock_nested(new_ptl, SINGLE_DEPTH_NESTING);\n\n\t \n\tpud = *old_pud;\n\tpud_clear(old_pud);\n\n\tVM_BUG_ON(!pud_none(*new_pud));\n\n\tpud_populate(mm, new_pud, pud_pgtable(pud));\n\tflush_tlb_range(vma, old_addr, old_addr + PUD_SIZE);\n\tif (new_ptl != old_ptl)\n\t\tspin_unlock(new_ptl);\n\tspin_unlock(old_ptl);\n\n\treturn true;\n}\n#else\nstatic inline bool move_normal_pud(struct vm_area_struct *vma,\n\t\tunsigned long old_addr, unsigned long new_addr, pud_t *old_pud,\n\t\tpud_t *new_pud)\n{\n\treturn false;\n}\n#endif\n\n#if defined(CONFIG_TRANSPARENT_HUGEPAGE) && defined(CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD)\nstatic bool move_huge_pud(struct vm_area_struct *vma, unsigned long old_addr,\n\t\t\t  unsigned long new_addr, pud_t *old_pud, pud_t *new_pud)\n{\n\tspinlock_t *old_ptl, *new_ptl;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tpud_t pud;\n\n\t \n\tif (WARN_ON_ONCE(!pud_none(*new_pud)))\n\t\treturn false;\n\n\t \n\told_ptl = pud_lock(vma->vm_mm, old_pud);\n\tnew_ptl = pud_lockptr(mm, new_pud);\n\tif (new_ptl != old_ptl)\n\t\tspin_lock_nested(new_ptl, SINGLE_DEPTH_NESTING);\n\n\t \n\tpud = *old_pud;\n\tpud_clear(old_pud);\n\n\tVM_BUG_ON(!pud_none(*new_pud));\n\n\t \n\t \n\tset_pud_at(mm, new_addr, new_pud, pud);\n\tflush_pud_tlb_range(vma, old_addr, old_addr + HPAGE_PUD_SIZE);\n\tif (new_ptl != old_ptl)\n\t\tspin_unlock(new_ptl);\n\tspin_unlock(old_ptl);\n\n\treturn true;\n}\n#else\nstatic bool move_huge_pud(struct vm_area_struct *vma, unsigned long old_addr,\n\t\t\t  unsigned long new_addr, pud_t *old_pud, pud_t *new_pud)\n{\n\tWARN_ON_ONCE(1);\n\treturn false;\n\n}\n#endif\n\nenum pgt_entry {\n\tNORMAL_PMD,\n\tHPAGE_PMD,\n\tNORMAL_PUD,\n\tHPAGE_PUD,\n};\n\n \nstatic __always_inline unsigned long get_extent(enum pgt_entry entry,\n\t\t\tunsigned long old_addr, unsigned long old_end,\n\t\t\tunsigned long new_addr)\n{\n\tunsigned long next, extent, mask, size;\n\n\tswitch (entry) {\n\tcase HPAGE_PMD:\n\tcase NORMAL_PMD:\n\t\tmask = PMD_MASK;\n\t\tsize = PMD_SIZE;\n\t\tbreak;\n\tcase HPAGE_PUD:\n\tcase NORMAL_PUD:\n\t\tmask = PUD_MASK;\n\t\tsize = PUD_SIZE;\n\t\tbreak;\n\tdefault:\n\t\tBUILD_BUG();\n\t\tbreak;\n\t}\n\n\tnext = (old_addr + size) & mask;\n\t \n\textent = next - old_addr;\n\tif (extent > old_end - old_addr)\n\t\textent = old_end - old_addr;\n\tnext = (new_addr + size) & mask;\n\tif (extent > next - new_addr)\n\t\textent = next - new_addr;\n\treturn extent;\n}\n\n \nstatic bool move_pgt_entry(enum pgt_entry entry, struct vm_area_struct *vma,\n\t\t\tunsigned long old_addr, unsigned long new_addr,\n\t\t\tvoid *old_entry, void *new_entry, bool need_rmap_locks)\n{\n\tbool moved = false;\n\n\t \n\tif (need_rmap_locks)\n\t\ttake_rmap_locks(vma);\n\n\tswitch (entry) {\n\tcase NORMAL_PMD:\n\t\tmoved = move_normal_pmd(vma, old_addr, new_addr, old_entry,\n\t\t\t\t\tnew_entry);\n\t\tbreak;\n\tcase NORMAL_PUD:\n\t\tmoved = move_normal_pud(vma, old_addr, new_addr, old_entry,\n\t\t\t\t\tnew_entry);\n\t\tbreak;\n\tcase HPAGE_PMD:\n\t\tmoved = IS_ENABLED(CONFIG_TRANSPARENT_HUGEPAGE) &&\n\t\t\tmove_huge_pmd(vma, old_addr, new_addr, old_entry,\n\t\t\t\t      new_entry);\n\t\tbreak;\n\tcase HPAGE_PUD:\n\t\tmoved = IS_ENABLED(CONFIG_TRANSPARENT_HUGEPAGE) &&\n\t\t\tmove_huge_pud(vma, old_addr, new_addr, old_entry,\n\t\t\t\t      new_entry);\n\t\tbreak;\n\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\tbreak;\n\t}\n\n\tif (need_rmap_locks)\n\t\tdrop_rmap_locks(vma);\n\n\treturn moved;\n}\n\nunsigned long move_page_tables(struct vm_area_struct *vma,\n\t\tunsigned long old_addr, struct vm_area_struct *new_vma,\n\t\tunsigned long new_addr, unsigned long len,\n\t\tbool need_rmap_locks)\n{\n\tunsigned long extent, old_end;\n\tstruct mmu_notifier_range range;\n\tpmd_t *old_pmd, *new_pmd;\n\tpud_t *old_pud, *new_pud;\n\n\tif (!len)\n\t\treturn 0;\n\n\told_end = old_addr + len;\n\n\tif (is_vm_hugetlb_page(vma))\n\t\treturn move_hugetlb_page_tables(vma, new_vma, old_addr,\n\t\t\t\t\t\tnew_addr, len);\n\n\tflush_cache_range(vma, old_addr, old_end);\n\tmmu_notifier_range_init(&range, MMU_NOTIFY_UNMAP, 0, vma->vm_mm,\n\t\t\t\told_addr, old_end);\n\tmmu_notifier_invalidate_range_start(&range);\n\n\tfor (; old_addr < old_end; old_addr += extent, new_addr += extent) {\n\t\tcond_resched();\n\t\t \n\t\textent = get_extent(NORMAL_PUD, old_addr, old_end, new_addr);\n\n\t\told_pud = get_old_pud(vma->vm_mm, old_addr);\n\t\tif (!old_pud)\n\t\t\tcontinue;\n\t\tnew_pud = alloc_new_pud(vma->vm_mm, vma, new_addr);\n\t\tif (!new_pud)\n\t\t\tbreak;\n\t\tif (pud_trans_huge(*old_pud) || pud_devmap(*old_pud)) {\n\t\t\tif (extent == HPAGE_PUD_SIZE) {\n\t\t\t\tmove_pgt_entry(HPAGE_PUD, vma, old_addr, new_addr,\n\t\t\t\t\t       old_pud, new_pud, need_rmap_locks);\n\t\t\t\t \n\t\t\t\tcontinue;\n\t\t\t}\n\t\t} else if (IS_ENABLED(CONFIG_HAVE_MOVE_PUD) && extent == PUD_SIZE) {\n\n\t\t\tif (move_pgt_entry(NORMAL_PUD, vma, old_addr, new_addr,\n\t\t\t\t\t   old_pud, new_pud, true))\n\t\t\t\tcontinue;\n\t\t}\n\n\t\textent = get_extent(NORMAL_PMD, old_addr, old_end, new_addr);\n\t\told_pmd = get_old_pmd(vma->vm_mm, old_addr);\n\t\tif (!old_pmd)\n\t\t\tcontinue;\n\t\tnew_pmd = alloc_new_pmd(vma->vm_mm, vma, new_addr);\n\t\tif (!new_pmd)\n\t\t\tbreak;\nagain:\n\t\tif (is_swap_pmd(*old_pmd) || pmd_trans_huge(*old_pmd) ||\n\t\t    pmd_devmap(*old_pmd)) {\n\t\t\tif (extent == HPAGE_PMD_SIZE &&\n\t\t\t    move_pgt_entry(HPAGE_PMD, vma, old_addr, new_addr,\n\t\t\t\t\t   old_pmd, new_pmd, need_rmap_locks))\n\t\t\t\tcontinue;\n\t\t\tsplit_huge_pmd(vma, old_pmd, old_addr);\n\t\t} else if (IS_ENABLED(CONFIG_HAVE_MOVE_PMD) &&\n\t\t\t   extent == PMD_SIZE) {\n\t\t\t \n\t\t\tif (move_pgt_entry(NORMAL_PMD, vma, old_addr, new_addr,\n\t\t\t\t\t   old_pmd, new_pmd, true))\n\t\t\t\tcontinue;\n\t\t}\n\t\tif (pmd_none(*old_pmd))\n\t\t\tcontinue;\n\t\tif (pte_alloc(new_vma->vm_mm, new_pmd))\n\t\t\tbreak;\n\t\tif (move_ptes(vma, old_pmd, old_addr, old_addr + extent,\n\t\t\t      new_vma, new_pmd, new_addr, need_rmap_locks) < 0)\n\t\t\tgoto again;\n\t}\n\n\tmmu_notifier_invalidate_range_end(&range);\n\n\treturn len + old_addr - old_end;\t \n}\n\nstatic unsigned long move_vma(struct vm_area_struct *vma,\n\t\tunsigned long old_addr, unsigned long old_len,\n\t\tunsigned long new_len, unsigned long new_addr,\n\t\tbool *locked, unsigned long flags,\n\t\tstruct vm_userfaultfd_ctx *uf, struct list_head *uf_unmap)\n{\n\tlong to_account = new_len - old_len;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct vm_area_struct *new_vma;\n\tunsigned long vm_flags = vma->vm_flags;\n\tunsigned long new_pgoff;\n\tunsigned long moved_len;\n\tunsigned long account_start = 0;\n\tunsigned long account_end = 0;\n\tunsigned long hiwater_vm;\n\tint err = 0;\n\tbool need_rmap_locks;\n\tstruct vma_iterator vmi;\n\n\t \n\tif (mm->map_count >= sysctl_max_map_count - 3)\n\t\treturn -ENOMEM;\n\n\tif (unlikely(flags & MREMAP_DONTUNMAP))\n\t\tto_account = new_len;\n\n\tif (vma->vm_ops && vma->vm_ops->may_split) {\n\t\tif (vma->vm_start != old_addr)\n\t\t\terr = vma->vm_ops->may_split(vma, old_addr);\n\t\tif (!err && vma->vm_end != old_addr + old_len)\n\t\t\terr = vma->vm_ops->may_split(vma, old_addr + old_len);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\t \n\terr = ksm_madvise(vma, old_addr, old_addr + old_len,\n\t\t\t\t\t\tMADV_UNMERGEABLE, &vm_flags);\n\tif (err)\n\t\treturn err;\n\n\tif (vm_flags & VM_ACCOUNT) {\n\t\tif (security_vm_enough_memory_mm(mm, to_account >> PAGE_SHIFT))\n\t\t\treturn -ENOMEM;\n\t}\n\n\tvma_start_write(vma);\n\tnew_pgoff = vma->vm_pgoff + ((old_addr - vma->vm_start) >> PAGE_SHIFT);\n\tnew_vma = copy_vma(&vma, new_addr, new_len, new_pgoff,\n\t\t\t   &need_rmap_locks);\n\tif (!new_vma) {\n\t\tif (vm_flags & VM_ACCOUNT)\n\t\t\tvm_unacct_memory(to_account >> PAGE_SHIFT);\n\t\treturn -ENOMEM;\n\t}\n\n\tmoved_len = move_page_tables(vma, old_addr, new_vma, new_addr, old_len,\n\t\t\t\t     need_rmap_locks);\n\tif (moved_len < old_len) {\n\t\terr = -ENOMEM;\n\t} else if (vma->vm_ops && vma->vm_ops->mremap) {\n\t\terr = vma->vm_ops->mremap(new_vma);\n\t}\n\n\tif (unlikely(err)) {\n\t\t \n\t\tmove_page_tables(new_vma, new_addr, vma, old_addr, moved_len,\n\t\t\t\t true);\n\t\tvma = new_vma;\n\t\told_len = new_len;\n\t\told_addr = new_addr;\n\t\tnew_addr = err;\n\t} else {\n\t\tmremap_userfaultfd_prep(new_vma, uf);\n\t}\n\n\tif (is_vm_hugetlb_page(vma)) {\n\t\tclear_vma_resv_huge_pages(vma);\n\t}\n\n\t \n\tif (vm_flags & VM_ACCOUNT && !(flags & MREMAP_DONTUNMAP)) {\n\t\tvm_flags_clear(vma, VM_ACCOUNT);\n\t\tif (vma->vm_start < old_addr)\n\t\t\taccount_start = vma->vm_start;\n\t\tif (vma->vm_end > old_addr + old_len)\n\t\t\taccount_end = vma->vm_end;\n\t}\n\n\t \n\thiwater_vm = mm->hiwater_vm;\n\tvm_stat_account(mm, vma->vm_flags, new_len >> PAGE_SHIFT);\n\n\t \n\tif (unlikely(vma->vm_flags & VM_PFNMAP))\n\t\tuntrack_pfn_clear(vma);\n\n\tif (unlikely(!err && (flags & MREMAP_DONTUNMAP))) {\n\t\t \n\t\tvm_flags_clear(vma, VM_LOCKED_MASK);\n\n\t\t \n\t\tif (new_vma != vma && vma->vm_start == old_addr &&\n\t\t\tvma->vm_end == (old_addr + old_len))\n\t\t\tunlink_anon_vmas(vma);\n\n\t\t \n\t\treturn new_addr;\n\t}\n\n\tvma_iter_init(&vmi, mm, old_addr);\n\tif (do_vmi_munmap(&vmi, mm, old_addr, old_len, uf_unmap, false) < 0) {\n\t\t \n\t\tif (vm_flags & VM_ACCOUNT && !(flags & MREMAP_DONTUNMAP))\n\t\t\tvm_acct_memory(old_len >> PAGE_SHIFT);\n\t\taccount_start = account_end = 0;\n\t}\n\n\tif (vm_flags & VM_LOCKED) {\n\t\tmm->locked_vm += new_len >> PAGE_SHIFT;\n\t\t*locked = true;\n\t}\n\n\tmm->hiwater_vm = hiwater_vm;\n\n\t \n\tif (account_start) {\n\t\tvma = vma_prev(&vmi);\n\t\tvm_flags_set(vma, VM_ACCOUNT);\n\t}\n\n\tif (account_end) {\n\t\tvma = vma_next(&vmi);\n\t\tvm_flags_set(vma, VM_ACCOUNT);\n\t}\n\n\treturn new_addr;\n}\n\nstatic struct vm_area_struct *vma_to_resize(unsigned long addr,\n\tunsigned long old_len, unsigned long new_len, unsigned long flags)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long pgoff;\n\n\tvma = vma_lookup(mm, addr);\n\tif (!vma)\n\t\treturn ERR_PTR(-EFAULT);\n\n\t \n\tif (!old_len && !(vma->vm_flags & (VM_SHARED | VM_MAYSHARE))) {\n\t\tpr_warn_once(\"%s (%d): attempted to duplicate a private mapping with mremap.  This is not supported.\\n\", current->comm, current->pid);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif ((flags & MREMAP_DONTUNMAP) &&\n\t\t\t(vma->vm_flags & (VM_DONTEXPAND | VM_PFNMAP)))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t \n\tif (old_len > vma->vm_end - addr)\n\t\treturn ERR_PTR(-EFAULT);\n\n\tif (new_len == old_len)\n\t\treturn vma;\n\n\t \n\tpgoff = (addr - vma->vm_start) >> PAGE_SHIFT;\n\tpgoff += vma->vm_pgoff;\n\tif (pgoff + (new_len >> PAGE_SHIFT) < pgoff)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (vma->vm_flags & (VM_DONTEXPAND | VM_PFNMAP))\n\t\treturn ERR_PTR(-EFAULT);\n\n\tif (!mlock_future_ok(mm, vma->vm_flags, new_len - old_len))\n\t\treturn ERR_PTR(-EAGAIN);\n\n\tif (!may_expand_vm(mm, vma->vm_flags,\n\t\t\t\t(new_len - old_len) >> PAGE_SHIFT))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\treturn vma;\n}\n\nstatic unsigned long mremap_to(unsigned long addr, unsigned long old_len,\n\t\tunsigned long new_addr, unsigned long new_len, bool *locked,\n\t\tunsigned long flags, struct vm_userfaultfd_ctx *uf,\n\t\tstruct list_head *uf_unmap_early,\n\t\tstruct list_head *uf_unmap)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long ret = -EINVAL;\n\tunsigned long map_flags = 0;\n\n\tif (offset_in_page(new_addr))\n\t\tgoto out;\n\n\tif (new_len > TASK_SIZE || new_addr > TASK_SIZE - new_len)\n\t\tgoto out;\n\n\t \n\tif (addr + old_len > new_addr && new_addr + new_len > addr)\n\t\tgoto out;\n\n\t \n\tif ((mm->map_count + 2) >= sysctl_max_map_count - 3)\n\t\treturn -ENOMEM;\n\n\tif (flags & MREMAP_FIXED) {\n\t\tret = do_munmap(mm, new_addr, new_len, uf_unmap_early);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tif (old_len > new_len) {\n\t\tret = do_munmap(mm, addr+new_len, old_len - new_len, uf_unmap);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\told_len = new_len;\n\t}\n\n\tvma = vma_to_resize(addr, old_len, new_len, flags);\n\tif (IS_ERR(vma)) {\n\t\tret = PTR_ERR(vma);\n\t\tgoto out;\n\t}\n\n\t \n\tif (flags & MREMAP_DONTUNMAP &&\n\t\t!may_expand_vm(mm, vma->vm_flags, old_len >> PAGE_SHIFT)) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tif (flags & MREMAP_FIXED)\n\t\tmap_flags |= MAP_FIXED;\n\n\tif (vma->vm_flags & VM_MAYSHARE)\n\t\tmap_flags |= MAP_SHARED;\n\n\tret = get_unmapped_area(vma->vm_file, new_addr, new_len, vma->vm_pgoff +\n\t\t\t\t((addr - vma->vm_start) >> PAGE_SHIFT),\n\t\t\t\tmap_flags);\n\tif (IS_ERR_VALUE(ret))\n\t\tgoto out;\n\n\t \n\tif (!(flags & MREMAP_FIXED))\n\t\tnew_addr = ret;\n\n\tret = move_vma(vma, addr, old_len, new_len, new_addr, locked, flags, uf,\n\t\t       uf_unmap);\n\nout:\n\treturn ret;\n}\n\nstatic int vma_expandable(struct vm_area_struct *vma, unsigned long delta)\n{\n\tunsigned long end = vma->vm_end + delta;\n\n\tif (end < vma->vm_end)  \n\t\treturn 0;\n\tif (find_vma_intersection(vma->vm_mm, vma->vm_end, end))\n\t\treturn 0;\n\tif (get_unmapped_area(NULL, vma->vm_start, end - vma->vm_start,\n\t\t\t      0, MAP_FIXED) & ~PAGE_MASK)\n\t\treturn 0;\n\treturn 1;\n}\n\n \nSYSCALL_DEFINE5(mremap, unsigned long, addr, unsigned long, old_len,\n\t\tunsigned long, new_len, unsigned long, flags,\n\t\tunsigned long, new_addr)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long ret = -EINVAL;\n\tbool locked = false;\n\tstruct vm_userfaultfd_ctx uf = NULL_VM_UFFD_CTX;\n\tLIST_HEAD(uf_unmap_early);\n\tLIST_HEAD(uf_unmap);\n\n\t \n\taddr = untagged_addr(addr);\n\n\tif (flags & ~(MREMAP_FIXED | MREMAP_MAYMOVE | MREMAP_DONTUNMAP))\n\t\treturn ret;\n\n\tif (flags & MREMAP_FIXED && !(flags & MREMAP_MAYMOVE))\n\t\treturn ret;\n\n\t \n\tif (flags & MREMAP_DONTUNMAP &&\n\t\t\t(!(flags & MREMAP_MAYMOVE) || old_len != new_len))\n\t\treturn ret;\n\n\n\tif (offset_in_page(addr))\n\t\treturn ret;\n\n\told_len = PAGE_ALIGN(old_len);\n\tnew_len = PAGE_ALIGN(new_len);\n\n\t \n\tif (!new_len)\n\t\treturn ret;\n\n\tif (mmap_write_lock_killable(current->mm))\n\t\treturn -EINTR;\n\tvma = vma_lookup(mm, addr);\n\tif (!vma) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tif (is_vm_hugetlb_page(vma)) {\n\t\tstruct hstate *h __maybe_unused = hstate_vma(vma);\n\n\t\told_len = ALIGN(old_len, huge_page_size(h));\n\t\tnew_len = ALIGN(new_len, huge_page_size(h));\n\n\t\t \n\t\tif (addr & ~huge_page_mask(h))\n\t\t\tgoto out;\n\t\tif (new_addr & ~huge_page_mask(h))\n\t\t\tgoto out;\n\n\t\t \n\t\tif (new_len > old_len)\n\t\t\tgoto out;\n\t}\n\n\tif (flags & (MREMAP_FIXED | MREMAP_DONTUNMAP)) {\n\t\tret = mremap_to(addr, old_len, new_addr, new_len,\n\t\t\t\t&locked, flags, &uf, &uf_unmap_early,\n\t\t\t\t&uf_unmap);\n\t\tgoto out;\n\t}\n\n\t \n\tif (old_len >= new_len) {\n\t\tVMA_ITERATOR(vmi, mm, addr + new_len);\n\n\t\tif (old_len == new_len) {\n\t\t\tret = addr;\n\t\t\tgoto out;\n\t\t}\n\n\t\tret = do_vmi_munmap(&vmi, mm, addr + new_len, old_len - new_len,\n\t\t\t\t    &uf_unmap, true);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tret = addr;\n\t\tgoto out_unlocked;\n\t}\n\n\t \n\tvma = vma_to_resize(addr, old_len, new_len, flags);\n\tif (IS_ERR(vma)) {\n\t\tret = PTR_ERR(vma);\n\t\tgoto out;\n\t}\n\n\t \n\tif (old_len == vma->vm_end - addr) {\n\t\t \n\t\tif (vma_expandable(vma, new_len - old_len)) {\n\t\t\tlong pages = (new_len - old_len) >> PAGE_SHIFT;\n\t\t\tunsigned long extension_start = addr + old_len;\n\t\t\tunsigned long extension_end = addr + new_len;\n\t\t\tpgoff_t extension_pgoff = vma->vm_pgoff +\n\t\t\t\t((extension_start - vma->vm_start) >> PAGE_SHIFT);\n\t\t\tVMA_ITERATOR(vmi, mm, extension_start);\n\n\t\t\tif (vma->vm_flags & VM_ACCOUNT) {\n\t\t\t\tif (security_vm_enough_memory_mm(mm, pages)) {\n\t\t\t\t\tret = -ENOMEM;\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t \n\t\t\tvma = vma_merge(&vmi, mm, vma, extension_start,\n\t\t\t\textension_end, vma->vm_flags, vma->anon_vma,\n\t\t\t\tvma->vm_file, extension_pgoff, vma_policy(vma),\n\t\t\t\tvma->vm_userfaultfd_ctx, anon_vma_name(vma));\n\t\t\tif (!vma) {\n\t\t\t\tvm_unacct_memory(pages);\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tvm_stat_account(mm, vma->vm_flags, pages);\n\t\t\tif (vma->vm_flags & VM_LOCKED) {\n\t\t\t\tmm->locked_vm += pages;\n\t\t\t\tlocked = true;\n\t\t\t\tnew_addr = addr;\n\t\t\t}\n\t\t\tret = addr;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t \n\tret = -ENOMEM;\n\tif (flags & MREMAP_MAYMOVE) {\n\t\tunsigned long map_flags = 0;\n\t\tif (vma->vm_flags & VM_MAYSHARE)\n\t\t\tmap_flags |= MAP_SHARED;\n\n\t\tnew_addr = get_unmapped_area(vma->vm_file, 0, new_len,\n\t\t\t\t\tvma->vm_pgoff +\n\t\t\t\t\t((addr - vma->vm_start) >> PAGE_SHIFT),\n\t\t\t\t\tmap_flags);\n\t\tif (IS_ERR_VALUE(new_addr)) {\n\t\t\tret = new_addr;\n\t\t\tgoto out;\n\t\t}\n\n\t\tret = move_vma(vma, addr, old_len, new_len, new_addr,\n\t\t\t       &locked, flags, &uf, &uf_unmap);\n\t}\nout:\n\tif (offset_in_page(ret))\n\t\tlocked = false;\n\tmmap_write_unlock(current->mm);\n\tif (locked && new_len > old_len)\n\t\tmm_populate(new_addr + old_len, new_len - old_len);\nout_unlocked:\n\tuserfaultfd_unmap_complete(mm, &uf_unmap_early);\n\tmremap_userfaultfd_complete(&uf, addr, ret, old_len);\n\tuserfaultfd_unmap_complete(mm, &uf_unmap);\n\treturn ret;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}