{
  "module_name": "pagewalk.c",
  "hash_id": "b3f66446fec5bcb69ecffe7f6916665e938b8fd31bae9b7ea9356b71469ff290",
  "original_prompt": "Ingested from linux-6.6.14/mm/pagewalk.c",
  "human_readable_source": "\n#include <linux/pagewalk.h>\n#include <linux/highmem.h>\n#include <linux/sched.h>\n#include <linux/hugetlb.h>\n\n \nstatic int real_depth(int depth)\n{\n\tif (depth == 3 && PTRS_PER_PMD == 1)\n\t\tdepth = 2;\n\tif (depth == 2 && PTRS_PER_PUD == 1)\n\t\tdepth = 1;\n\tif (depth == 1 && PTRS_PER_P4D == 1)\n\t\tdepth = 0;\n\treturn depth;\n}\n\nstatic int walk_pte_range_inner(pte_t *pte, unsigned long addr,\n\t\t\t\tunsigned long end, struct mm_walk *walk)\n{\n\tconst struct mm_walk_ops *ops = walk->ops;\n\tint err = 0;\n\n\tfor (;;) {\n\t\terr = ops->pte_entry(pte, addr, addr + PAGE_SIZE, walk);\n\t\tif (err)\n\t\t       break;\n\t\tif (addr >= end - PAGE_SIZE)\n\t\t\tbreak;\n\t\taddr += PAGE_SIZE;\n\t\tpte++;\n\t}\n\treturn err;\n}\n\nstatic int walk_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,\n\t\t\t  struct mm_walk *walk)\n{\n\tpte_t *pte;\n\tint err = 0;\n\tspinlock_t *ptl;\n\n\tif (walk->no_vma) {\n\t\t \n\t\tif (walk->mm == &init_mm || addr >= TASK_SIZE)\n\t\t\tpte = pte_offset_kernel(pmd, addr);\n\t\telse\n\t\t\tpte = pte_offset_map(pmd, addr);\n\t\tif (pte) {\n\t\t\terr = walk_pte_range_inner(pte, addr, end, walk);\n\t\t\tif (walk->mm != &init_mm && addr < TASK_SIZE)\n\t\t\t\tpte_unmap(pte);\n\t\t}\n\t} else {\n\t\tpte = pte_offset_map_lock(walk->mm, pmd, addr, &ptl);\n\t\tif (pte) {\n\t\t\terr = walk_pte_range_inner(pte, addr, end, walk);\n\t\t\tpte_unmap_unlock(pte, ptl);\n\t\t}\n\t}\n\tif (!pte)\n\t\twalk->action = ACTION_AGAIN;\n\treturn err;\n}\n\n#ifdef CONFIG_ARCH_HAS_HUGEPD\nstatic int walk_hugepd_range(hugepd_t *phpd, unsigned long addr,\n\t\t\t     unsigned long end, struct mm_walk *walk, int pdshift)\n{\n\tint err = 0;\n\tconst struct mm_walk_ops *ops = walk->ops;\n\tint shift = hugepd_shift(*phpd);\n\tint page_size = 1 << shift;\n\n\tif (!ops->pte_entry)\n\t\treturn 0;\n\n\tif (addr & (page_size - 1))\n\t\treturn 0;\n\n\tfor (;;) {\n\t\tpte_t *pte;\n\n\t\tspin_lock(&walk->mm->page_table_lock);\n\t\tpte = hugepte_offset(*phpd, addr, pdshift);\n\t\terr = ops->pte_entry(pte, addr, addr + page_size, walk);\n\t\tspin_unlock(&walk->mm->page_table_lock);\n\n\t\tif (err)\n\t\t\tbreak;\n\t\tif (addr >= end - page_size)\n\t\t\tbreak;\n\t\taddr += page_size;\n\t}\n\treturn err;\n}\n#else\nstatic int walk_hugepd_range(hugepd_t *phpd, unsigned long addr,\n\t\t\t     unsigned long end, struct mm_walk *walk, int pdshift)\n{\n\treturn 0;\n}\n#endif\n\nstatic int walk_pmd_range(pud_t *pud, unsigned long addr, unsigned long end,\n\t\t\t  struct mm_walk *walk)\n{\n\tpmd_t *pmd;\n\tunsigned long next;\n\tconst struct mm_walk_ops *ops = walk->ops;\n\tint err = 0;\n\tint depth = real_depth(3);\n\n\tpmd = pmd_offset(pud, addr);\n\tdo {\nagain:\n\t\tnext = pmd_addr_end(addr, end);\n\t\tif (pmd_none(*pmd)) {\n\t\t\tif (ops->pte_hole)\n\t\t\t\terr = ops->pte_hole(addr, next, depth, walk);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tcontinue;\n\t\t}\n\n\t\twalk->action = ACTION_SUBTREE;\n\n\t\t \n\t\tif (ops->pmd_entry)\n\t\t\terr = ops->pmd_entry(pmd, addr, next, walk);\n\t\tif (err)\n\t\t\tbreak;\n\n\t\tif (walk->action == ACTION_AGAIN)\n\t\t\tgoto again;\n\n\t\t \n\t\tif ((!walk->vma && (pmd_leaf(*pmd) || !pmd_present(*pmd))) ||\n\t\t    walk->action == ACTION_CONTINUE ||\n\t\t    !(ops->pte_entry))\n\t\t\tcontinue;\n\n\t\tif (walk->vma)\n\t\t\tsplit_huge_pmd(walk->vma, pmd, addr);\n\n\t\tif (is_hugepd(__hugepd(pmd_val(*pmd))))\n\t\t\terr = walk_hugepd_range((hugepd_t *)pmd, addr, next, walk, PMD_SHIFT);\n\t\telse\n\t\t\terr = walk_pte_range(pmd, addr, next, walk);\n\t\tif (err)\n\t\t\tbreak;\n\n\t\tif (walk->action == ACTION_AGAIN)\n\t\t\tgoto again;\n\n\t} while (pmd++, addr = next, addr != end);\n\n\treturn err;\n}\n\nstatic int walk_pud_range(p4d_t *p4d, unsigned long addr, unsigned long end,\n\t\t\t  struct mm_walk *walk)\n{\n\tpud_t *pud;\n\tunsigned long next;\n\tconst struct mm_walk_ops *ops = walk->ops;\n\tint err = 0;\n\tint depth = real_depth(2);\n\n\tpud = pud_offset(p4d, addr);\n\tdo {\n again:\n\t\tnext = pud_addr_end(addr, end);\n\t\tif (pud_none(*pud)) {\n\t\t\tif (ops->pte_hole)\n\t\t\t\terr = ops->pte_hole(addr, next, depth, walk);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tcontinue;\n\t\t}\n\n\t\twalk->action = ACTION_SUBTREE;\n\n\t\tif (ops->pud_entry)\n\t\t\terr = ops->pud_entry(pud, addr, next, walk);\n\t\tif (err)\n\t\t\tbreak;\n\n\t\tif (walk->action == ACTION_AGAIN)\n\t\t\tgoto again;\n\n\t\tif ((!walk->vma && (pud_leaf(*pud) || !pud_present(*pud))) ||\n\t\t    walk->action == ACTION_CONTINUE ||\n\t\t    !(ops->pmd_entry || ops->pte_entry))\n\t\t\tcontinue;\n\n\t\tif (walk->vma)\n\t\t\tsplit_huge_pud(walk->vma, pud, addr);\n\t\tif (pud_none(*pud))\n\t\t\tgoto again;\n\n\t\tif (is_hugepd(__hugepd(pud_val(*pud))))\n\t\t\terr = walk_hugepd_range((hugepd_t *)pud, addr, next, walk, PUD_SHIFT);\n\t\telse\n\t\t\terr = walk_pmd_range(pud, addr, next, walk);\n\t\tif (err)\n\t\t\tbreak;\n\t} while (pud++, addr = next, addr != end);\n\n\treturn err;\n}\n\nstatic int walk_p4d_range(pgd_t *pgd, unsigned long addr, unsigned long end,\n\t\t\t  struct mm_walk *walk)\n{\n\tp4d_t *p4d;\n\tunsigned long next;\n\tconst struct mm_walk_ops *ops = walk->ops;\n\tint err = 0;\n\tint depth = real_depth(1);\n\n\tp4d = p4d_offset(pgd, addr);\n\tdo {\n\t\tnext = p4d_addr_end(addr, end);\n\t\tif (p4d_none_or_clear_bad(p4d)) {\n\t\t\tif (ops->pte_hole)\n\t\t\t\terr = ops->pte_hole(addr, next, depth, walk);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tcontinue;\n\t\t}\n\t\tif (ops->p4d_entry) {\n\t\t\terr = ops->p4d_entry(p4d, addr, next, walk);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (is_hugepd(__hugepd(p4d_val(*p4d))))\n\t\t\terr = walk_hugepd_range((hugepd_t *)p4d, addr, next, walk, P4D_SHIFT);\n\t\telse if (ops->pud_entry || ops->pmd_entry || ops->pte_entry)\n\t\t\terr = walk_pud_range(p4d, addr, next, walk);\n\t\tif (err)\n\t\t\tbreak;\n\t} while (p4d++, addr = next, addr != end);\n\n\treturn err;\n}\n\nstatic int walk_pgd_range(unsigned long addr, unsigned long end,\n\t\t\t  struct mm_walk *walk)\n{\n\tpgd_t *pgd;\n\tunsigned long next;\n\tconst struct mm_walk_ops *ops = walk->ops;\n\tint err = 0;\n\n\tif (walk->pgd)\n\t\tpgd = walk->pgd + pgd_index(addr);\n\telse\n\t\tpgd = pgd_offset(walk->mm, addr);\n\tdo {\n\t\tnext = pgd_addr_end(addr, end);\n\t\tif (pgd_none_or_clear_bad(pgd)) {\n\t\t\tif (ops->pte_hole)\n\t\t\t\terr = ops->pte_hole(addr, next, 0, walk);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tcontinue;\n\t\t}\n\t\tif (ops->pgd_entry) {\n\t\t\terr = ops->pgd_entry(pgd, addr, next, walk);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (is_hugepd(__hugepd(pgd_val(*pgd))))\n\t\t\terr = walk_hugepd_range((hugepd_t *)pgd, addr, next, walk, PGDIR_SHIFT);\n\t\telse if (ops->p4d_entry || ops->pud_entry || ops->pmd_entry || ops->pte_entry)\n\t\t\terr = walk_p4d_range(pgd, addr, next, walk);\n\t\tif (err)\n\t\t\tbreak;\n\t} while (pgd++, addr = next, addr != end);\n\n\treturn err;\n}\n\n#ifdef CONFIG_HUGETLB_PAGE\nstatic unsigned long hugetlb_entry_end(struct hstate *h, unsigned long addr,\n\t\t\t\t       unsigned long end)\n{\n\tunsigned long boundary = (addr & huge_page_mask(h)) + huge_page_size(h);\n\treturn boundary < end ? boundary : end;\n}\n\nstatic int walk_hugetlb_range(unsigned long addr, unsigned long end,\n\t\t\t      struct mm_walk *walk)\n{\n\tstruct vm_area_struct *vma = walk->vma;\n\tstruct hstate *h = hstate_vma(vma);\n\tunsigned long next;\n\tunsigned long hmask = huge_page_mask(h);\n\tunsigned long sz = huge_page_size(h);\n\tpte_t *pte;\n\tconst struct mm_walk_ops *ops = walk->ops;\n\tint err = 0;\n\n\thugetlb_vma_lock_read(vma);\n\tdo {\n\t\tnext = hugetlb_entry_end(h, addr, end);\n\t\tpte = hugetlb_walk(vma, addr & hmask, sz);\n\t\tif (pte)\n\t\t\terr = ops->hugetlb_entry(pte, hmask, addr, next, walk);\n\t\telse if (ops->pte_hole)\n\t\t\terr = ops->pte_hole(addr, next, -1, walk);\n\t\tif (err)\n\t\t\tbreak;\n\t} while (addr = next, addr != end);\n\thugetlb_vma_unlock_read(vma);\n\n\treturn err;\n}\n\n#else  \nstatic int walk_hugetlb_range(unsigned long addr, unsigned long end,\n\t\t\t      struct mm_walk *walk)\n{\n\treturn 0;\n}\n\n#endif  \n\n \nstatic int walk_page_test(unsigned long start, unsigned long end,\n\t\t\tstruct mm_walk *walk)\n{\n\tstruct vm_area_struct *vma = walk->vma;\n\tconst struct mm_walk_ops *ops = walk->ops;\n\n\tif (ops->test_walk)\n\t\treturn ops->test_walk(start, end, walk);\n\n\t \n\tif (vma->vm_flags & VM_PFNMAP) {\n\t\tint err = 1;\n\t\tif (ops->pte_hole)\n\t\t\terr = ops->pte_hole(start, end, -1, walk);\n\t\treturn err ? err : 1;\n\t}\n\treturn 0;\n}\n\nstatic int __walk_page_range(unsigned long start, unsigned long end,\n\t\t\tstruct mm_walk *walk)\n{\n\tint err = 0;\n\tstruct vm_area_struct *vma = walk->vma;\n\tconst struct mm_walk_ops *ops = walk->ops;\n\n\tif (ops->pre_vma) {\n\t\terr = ops->pre_vma(start, end, walk);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (is_vm_hugetlb_page(vma)) {\n\t\tif (ops->hugetlb_entry)\n\t\t\terr = walk_hugetlb_range(start, end, walk);\n\t} else\n\t\terr = walk_pgd_range(start, end, walk);\n\n\tif (ops->post_vma)\n\t\tops->post_vma(walk);\n\n\treturn err;\n}\n\nstatic inline void process_mm_walk_lock(struct mm_struct *mm,\n\t\t\t\t\tenum page_walk_lock walk_lock)\n{\n\tif (walk_lock == PGWALK_RDLOCK)\n\t\tmmap_assert_locked(mm);\n\telse\n\t\tmmap_assert_write_locked(mm);\n}\n\nstatic inline void process_vma_walk_lock(struct vm_area_struct *vma,\n\t\t\t\t\t enum page_walk_lock walk_lock)\n{\n#ifdef CONFIG_PER_VMA_LOCK\n\tswitch (walk_lock) {\n\tcase PGWALK_WRLOCK:\n\t\tvma_start_write(vma);\n\t\tbreak;\n\tcase PGWALK_WRLOCK_VERIFY:\n\t\tvma_assert_write_locked(vma);\n\t\tbreak;\n\tcase PGWALK_RDLOCK:\n\t\t \n\t\tbreak;\n\t}\n#endif\n}\n\n \nint walk_page_range(struct mm_struct *mm, unsigned long start,\n\t\tunsigned long end, const struct mm_walk_ops *ops,\n\t\tvoid *private)\n{\n\tint err = 0;\n\tunsigned long next;\n\tstruct vm_area_struct *vma;\n\tstruct mm_walk walk = {\n\t\t.ops\t\t= ops,\n\t\t.mm\t\t= mm,\n\t\t.private\t= private,\n\t};\n\n\tif (start >= end)\n\t\treturn -EINVAL;\n\n\tif (!walk.mm)\n\t\treturn -EINVAL;\n\n\tprocess_mm_walk_lock(walk.mm, ops->walk_lock);\n\n\tvma = find_vma(walk.mm, start);\n\tdo {\n\t\tif (!vma) {  \n\t\t\twalk.vma = NULL;\n\t\t\tnext = end;\n\t\t\tif (ops->pte_hole)\n\t\t\t\terr = ops->pte_hole(start, next, -1, &walk);\n\t\t} else if (start < vma->vm_start) {  \n\t\t\twalk.vma = NULL;\n\t\t\tnext = min(end, vma->vm_start);\n\t\t\tif (ops->pte_hole)\n\t\t\t\terr = ops->pte_hole(start, next, -1, &walk);\n\t\t} else {  \n\t\t\tprocess_vma_walk_lock(vma, ops->walk_lock);\n\t\t\twalk.vma = vma;\n\t\t\tnext = min(end, vma->vm_end);\n\t\t\tvma = find_vma(mm, vma->vm_end);\n\n\t\t\terr = walk_page_test(start, next, &walk);\n\t\t\tif (err > 0) {\n\t\t\t\t \n\t\t\t\terr = 0;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (err < 0)\n\t\t\t\tbreak;\n\t\t\terr = __walk_page_range(start, next, &walk);\n\t\t}\n\t\tif (err)\n\t\t\tbreak;\n\t} while (start = next, start < end);\n\treturn err;\n}\n\n \nint walk_page_range_novma(struct mm_struct *mm, unsigned long start,\n\t\t\t  unsigned long end, const struct mm_walk_ops *ops,\n\t\t\t  pgd_t *pgd,\n\t\t\t  void *private)\n{\n\tstruct mm_walk walk = {\n\t\t.ops\t\t= ops,\n\t\t.mm\t\t= mm,\n\t\t.pgd\t\t= pgd,\n\t\t.private\t= private,\n\t\t.no_vma\t\t= true\n\t};\n\n\tif (start >= end || !walk.mm)\n\t\treturn -EINVAL;\n\n\tmmap_assert_write_locked(walk.mm);\n\n\treturn walk_pgd_range(start, end, &walk);\n}\n\nint walk_page_range_vma(struct vm_area_struct *vma, unsigned long start,\n\t\t\tunsigned long end, const struct mm_walk_ops *ops,\n\t\t\tvoid *private)\n{\n\tstruct mm_walk walk = {\n\t\t.ops\t\t= ops,\n\t\t.mm\t\t= vma->vm_mm,\n\t\t.vma\t\t= vma,\n\t\t.private\t= private,\n\t};\n\n\tif (start >= end || !walk.mm)\n\t\treturn -EINVAL;\n\tif (start < vma->vm_start || end > vma->vm_end)\n\t\treturn -EINVAL;\n\n\tprocess_mm_walk_lock(walk.mm, ops->walk_lock);\n\tprocess_vma_walk_lock(vma, ops->walk_lock);\n\treturn __walk_page_range(start, end, &walk);\n}\n\nint walk_page_vma(struct vm_area_struct *vma, const struct mm_walk_ops *ops,\n\t\tvoid *private)\n{\n\tstruct mm_walk walk = {\n\t\t.ops\t\t= ops,\n\t\t.mm\t\t= vma->vm_mm,\n\t\t.vma\t\t= vma,\n\t\t.private\t= private,\n\t};\n\n\tif (!walk.mm)\n\t\treturn -EINVAL;\n\n\tprocess_mm_walk_lock(walk.mm, ops->walk_lock);\n\tprocess_vma_walk_lock(vma, ops->walk_lock);\n\treturn __walk_page_range(vma->vm_start, vma->vm_end, &walk);\n}\n\n \nint walk_page_mapping(struct address_space *mapping, pgoff_t first_index,\n\t\t      pgoff_t nr, const struct mm_walk_ops *ops,\n\t\t      void *private)\n{\n\tstruct mm_walk walk = {\n\t\t.ops\t\t= ops,\n\t\t.private\t= private,\n\t};\n\tstruct vm_area_struct *vma;\n\tpgoff_t vba, vea, cba, cea;\n\tunsigned long start_addr, end_addr;\n\tint err = 0;\n\n\tlockdep_assert_held(&mapping->i_mmap_rwsem);\n\tvma_interval_tree_foreach(vma, &mapping->i_mmap, first_index,\n\t\t\t\t  first_index + nr - 1) {\n\t\t \n\t\tvba = vma->vm_pgoff;\n\t\tvea = vba + vma_pages(vma);\n\t\tcba = first_index;\n\t\tcba = max(cba, vba);\n\t\tcea = first_index + nr;\n\t\tcea = min(cea, vea);\n\n\t\tstart_addr = ((cba - vba) << PAGE_SHIFT) + vma->vm_start;\n\t\tend_addr = ((cea - vba) << PAGE_SHIFT) + vma->vm_start;\n\t\tif (start_addr >= end_addr)\n\t\t\tcontinue;\n\n\t\twalk.vma = vma;\n\t\twalk.mm = vma->vm_mm;\n\n\t\terr = walk_page_test(vma->vm_start, vma->vm_end, &walk);\n\t\tif (err > 0) {\n\t\t\terr = 0;\n\t\t\tbreak;\n\t\t} else if (err < 0)\n\t\t\tbreak;\n\n\t\terr = __walk_page_range(start_addr, end_addr, &walk);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\n\treturn err;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}