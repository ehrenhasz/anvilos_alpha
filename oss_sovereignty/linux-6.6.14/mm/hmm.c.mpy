{
  "module_name": "hmm.c",
  "hash_id": "f1b6e5f3acfcdf63c0e2aff9073cd41e19d148f201f96849c962e90ca7a4bf0d",
  "original_prompt": "Ingested from linux-6.6.14/mm/hmm.c",
  "human_readable_source": "\n \n \n#include <linux/pagewalk.h>\n#include <linux/hmm.h>\n#include <linux/init.h>\n#include <linux/rmap.h>\n#include <linux/swap.h>\n#include <linux/slab.h>\n#include <linux/sched.h>\n#include <linux/mmzone.h>\n#include <linux/pagemap.h>\n#include <linux/swapops.h>\n#include <linux/hugetlb.h>\n#include <linux/memremap.h>\n#include <linux/sched/mm.h>\n#include <linux/jump_label.h>\n#include <linux/dma-mapping.h>\n#include <linux/mmu_notifier.h>\n#include <linux/memory_hotplug.h>\n\n#include \"internal.h\"\n\nstruct hmm_vma_walk {\n\tstruct hmm_range\t*range;\n\tunsigned long\t\tlast;\n};\n\nenum {\n\tHMM_NEED_FAULT = 1 << 0,\n\tHMM_NEED_WRITE_FAULT = 1 << 1,\n\tHMM_NEED_ALL_BITS = HMM_NEED_FAULT | HMM_NEED_WRITE_FAULT,\n};\n\nstatic int hmm_pfns_fill(unsigned long addr, unsigned long end,\n\t\t\t struct hmm_range *range, unsigned long cpu_flags)\n{\n\tunsigned long i = (addr - range->start) >> PAGE_SHIFT;\n\n\tfor (; addr < end; addr += PAGE_SIZE, i++)\n\t\trange->hmm_pfns[i] = cpu_flags;\n\treturn 0;\n}\n\n \nstatic int hmm_vma_fault(unsigned long addr, unsigned long end,\n\t\t\t unsigned int required_fault, struct mm_walk *walk)\n{\n\tstruct hmm_vma_walk *hmm_vma_walk = walk->private;\n\tstruct vm_area_struct *vma = walk->vma;\n\tunsigned int fault_flags = FAULT_FLAG_REMOTE;\n\n\tWARN_ON_ONCE(!required_fault);\n\thmm_vma_walk->last = addr;\n\n\tif (required_fault & HMM_NEED_WRITE_FAULT) {\n\t\tif (!(vma->vm_flags & VM_WRITE))\n\t\t\treturn -EPERM;\n\t\tfault_flags |= FAULT_FLAG_WRITE;\n\t}\n\n\tfor (; addr < end; addr += PAGE_SIZE)\n\t\tif (handle_mm_fault(vma, addr, fault_flags, NULL) &\n\t\t    VM_FAULT_ERROR)\n\t\t\treturn -EFAULT;\n\treturn -EBUSY;\n}\n\nstatic unsigned int hmm_pte_need_fault(const struct hmm_vma_walk *hmm_vma_walk,\n\t\t\t\t       unsigned long pfn_req_flags,\n\t\t\t\t       unsigned long cpu_flags)\n{\n\tstruct hmm_range *range = hmm_vma_walk->range;\n\n\t \n\tpfn_req_flags &= range->pfn_flags_mask;\n\tpfn_req_flags |= range->default_flags;\n\n\t \n\tif (!(pfn_req_flags & HMM_PFN_REQ_FAULT))\n\t\treturn 0;\n\n\t \n\tif ((pfn_req_flags & HMM_PFN_REQ_WRITE) &&\n\t    !(cpu_flags & HMM_PFN_WRITE))\n\t\treturn HMM_NEED_FAULT | HMM_NEED_WRITE_FAULT;\n\n\t \n\tif (!(cpu_flags & HMM_PFN_VALID))\n\t\treturn HMM_NEED_FAULT;\n\treturn 0;\n}\n\nstatic unsigned int\nhmm_range_need_fault(const struct hmm_vma_walk *hmm_vma_walk,\n\t\t     const unsigned long hmm_pfns[], unsigned long npages,\n\t\t     unsigned long cpu_flags)\n{\n\tstruct hmm_range *range = hmm_vma_walk->range;\n\tunsigned int required_fault = 0;\n\tunsigned long i;\n\n\t \n\tif (!((range->default_flags | range->pfn_flags_mask) &\n\t      HMM_PFN_REQ_FAULT))\n\t\treturn 0;\n\n\tfor (i = 0; i < npages; ++i) {\n\t\trequired_fault |= hmm_pte_need_fault(hmm_vma_walk, hmm_pfns[i],\n\t\t\t\t\t\t     cpu_flags);\n\t\tif (required_fault == HMM_NEED_ALL_BITS)\n\t\t\treturn required_fault;\n\t}\n\treturn required_fault;\n}\n\nstatic int hmm_vma_walk_hole(unsigned long addr, unsigned long end,\n\t\t\t     __always_unused int depth, struct mm_walk *walk)\n{\n\tstruct hmm_vma_walk *hmm_vma_walk = walk->private;\n\tstruct hmm_range *range = hmm_vma_walk->range;\n\tunsigned int required_fault;\n\tunsigned long i, npages;\n\tunsigned long *hmm_pfns;\n\n\ti = (addr - range->start) >> PAGE_SHIFT;\n\tnpages = (end - addr) >> PAGE_SHIFT;\n\thmm_pfns = &range->hmm_pfns[i];\n\trequired_fault =\n\t\thmm_range_need_fault(hmm_vma_walk, hmm_pfns, npages, 0);\n\tif (!walk->vma) {\n\t\tif (required_fault)\n\t\t\treturn -EFAULT;\n\t\treturn hmm_pfns_fill(addr, end, range, HMM_PFN_ERROR);\n\t}\n\tif (required_fault)\n\t\treturn hmm_vma_fault(addr, end, required_fault, walk);\n\treturn hmm_pfns_fill(addr, end, range, 0);\n}\n\nstatic inline unsigned long hmm_pfn_flags_order(unsigned long order)\n{\n\treturn order << HMM_PFN_ORDER_SHIFT;\n}\n\nstatic inline unsigned long pmd_to_hmm_pfn_flags(struct hmm_range *range,\n\t\t\t\t\t\t pmd_t pmd)\n{\n\tif (pmd_protnone(pmd))\n\t\treturn 0;\n\treturn (pmd_write(pmd) ? (HMM_PFN_VALID | HMM_PFN_WRITE) :\n\t\t\t\t HMM_PFN_VALID) |\n\t       hmm_pfn_flags_order(PMD_SHIFT - PAGE_SHIFT);\n}\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\nstatic int hmm_vma_handle_pmd(struct mm_walk *walk, unsigned long addr,\n\t\t\t      unsigned long end, unsigned long hmm_pfns[],\n\t\t\t      pmd_t pmd)\n{\n\tstruct hmm_vma_walk *hmm_vma_walk = walk->private;\n\tstruct hmm_range *range = hmm_vma_walk->range;\n\tunsigned long pfn, npages, i;\n\tunsigned int required_fault;\n\tunsigned long cpu_flags;\n\n\tnpages = (end - addr) >> PAGE_SHIFT;\n\tcpu_flags = pmd_to_hmm_pfn_flags(range, pmd);\n\trequired_fault =\n\t\thmm_range_need_fault(hmm_vma_walk, hmm_pfns, npages, cpu_flags);\n\tif (required_fault)\n\t\treturn hmm_vma_fault(addr, end, required_fault, walk);\n\n\tpfn = pmd_pfn(pmd) + ((addr & ~PMD_MASK) >> PAGE_SHIFT);\n\tfor (i = 0; addr < end; addr += PAGE_SIZE, i++, pfn++)\n\t\thmm_pfns[i] = pfn | cpu_flags;\n\treturn 0;\n}\n#else  \n \nint hmm_vma_handle_pmd(struct mm_walk *walk, unsigned long addr,\n\t\tunsigned long end, unsigned long hmm_pfns[], pmd_t pmd);\n#endif  \n\nstatic inline unsigned long pte_to_hmm_pfn_flags(struct hmm_range *range,\n\t\t\t\t\t\t pte_t pte)\n{\n\tif (pte_none(pte) || !pte_present(pte) || pte_protnone(pte))\n\t\treturn 0;\n\treturn pte_write(pte) ? (HMM_PFN_VALID | HMM_PFN_WRITE) : HMM_PFN_VALID;\n}\n\nstatic int hmm_vma_handle_pte(struct mm_walk *walk, unsigned long addr,\n\t\t\t      unsigned long end, pmd_t *pmdp, pte_t *ptep,\n\t\t\t      unsigned long *hmm_pfn)\n{\n\tstruct hmm_vma_walk *hmm_vma_walk = walk->private;\n\tstruct hmm_range *range = hmm_vma_walk->range;\n\tunsigned int required_fault;\n\tunsigned long cpu_flags;\n\tpte_t pte = ptep_get(ptep);\n\tuint64_t pfn_req_flags = *hmm_pfn;\n\n\tif (pte_none_mostly(pte)) {\n\t\trequired_fault =\n\t\t\thmm_pte_need_fault(hmm_vma_walk, pfn_req_flags, 0);\n\t\tif (required_fault)\n\t\t\tgoto fault;\n\t\t*hmm_pfn = 0;\n\t\treturn 0;\n\t}\n\n\tif (!pte_present(pte)) {\n\t\tswp_entry_t entry = pte_to_swp_entry(pte);\n\n\t\t \n\t\tif (is_device_private_entry(entry) &&\n\t\t    pfn_swap_entry_to_page(entry)->pgmap->owner ==\n\t\t    range->dev_private_owner) {\n\t\t\tcpu_flags = HMM_PFN_VALID;\n\t\t\tif (is_writable_device_private_entry(entry))\n\t\t\t\tcpu_flags |= HMM_PFN_WRITE;\n\t\t\t*hmm_pfn = swp_offset_pfn(entry) | cpu_flags;\n\t\t\treturn 0;\n\t\t}\n\n\t\trequired_fault =\n\t\t\thmm_pte_need_fault(hmm_vma_walk, pfn_req_flags, 0);\n\t\tif (!required_fault) {\n\t\t\t*hmm_pfn = 0;\n\t\t\treturn 0;\n\t\t}\n\n\t\tif (!non_swap_entry(entry))\n\t\t\tgoto fault;\n\n\t\tif (is_device_private_entry(entry))\n\t\t\tgoto fault;\n\n\t\tif (is_device_exclusive_entry(entry))\n\t\t\tgoto fault;\n\n\t\tif (is_migration_entry(entry)) {\n\t\t\tpte_unmap(ptep);\n\t\t\thmm_vma_walk->last = addr;\n\t\t\tmigration_entry_wait(walk->mm, pmdp, addr);\n\t\t\treturn -EBUSY;\n\t\t}\n\n\t\t \n\t\tpte_unmap(ptep);\n\t\treturn -EFAULT;\n\t}\n\n\tcpu_flags = pte_to_hmm_pfn_flags(range, pte);\n\trequired_fault =\n\t\thmm_pte_need_fault(hmm_vma_walk, pfn_req_flags, cpu_flags);\n\tif (required_fault)\n\t\tgoto fault;\n\n\t \n\tif (!vm_normal_page(walk->vma, addr, pte) &&\n\t    !pte_devmap(pte) &&\n\t    !is_zero_pfn(pte_pfn(pte))) {\n\t\tif (hmm_pte_need_fault(hmm_vma_walk, pfn_req_flags, 0)) {\n\t\t\tpte_unmap(ptep);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\t*hmm_pfn = HMM_PFN_ERROR;\n\t\treturn 0;\n\t}\n\n\t*hmm_pfn = pte_pfn(pte) | cpu_flags;\n\treturn 0;\n\nfault:\n\tpte_unmap(ptep);\n\t \n\treturn hmm_vma_fault(addr, end, required_fault, walk);\n}\n\nstatic int hmm_vma_walk_pmd(pmd_t *pmdp,\n\t\t\t    unsigned long start,\n\t\t\t    unsigned long end,\n\t\t\t    struct mm_walk *walk)\n{\n\tstruct hmm_vma_walk *hmm_vma_walk = walk->private;\n\tstruct hmm_range *range = hmm_vma_walk->range;\n\tunsigned long *hmm_pfns =\n\t\t&range->hmm_pfns[(start - range->start) >> PAGE_SHIFT];\n\tunsigned long npages = (end - start) >> PAGE_SHIFT;\n\tunsigned long addr = start;\n\tpte_t *ptep;\n\tpmd_t pmd;\n\nagain:\n\tpmd = pmdp_get_lockless(pmdp);\n\tif (pmd_none(pmd))\n\t\treturn hmm_vma_walk_hole(start, end, -1, walk);\n\n\tif (thp_migration_supported() && is_pmd_migration_entry(pmd)) {\n\t\tif (hmm_range_need_fault(hmm_vma_walk, hmm_pfns, npages, 0)) {\n\t\t\thmm_vma_walk->last = addr;\n\t\t\tpmd_migration_entry_wait(walk->mm, pmdp);\n\t\t\treturn -EBUSY;\n\t\t}\n\t\treturn hmm_pfns_fill(start, end, range, 0);\n\t}\n\n\tif (!pmd_present(pmd)) {\n\t\tif (hmm_range_need_fault(hmm_vma_walk, hmm_pfns, npages, 0))\n\t\t\treturn -EFAULT;\n\t\treturn hmm_pfns_fill(start, end, range, HMM_PFN_ERROR);\n\t}\n\n\tif (pmd_devmap(pmd) || pmd_trans_huge(pmd)) {\n\t\t \n\t\tpmd = pmdp_get_lockless(pmdp);\n\t\tif (!pmd_devmap(pmd) && !pmd_trans_huge(pmd))\n\t\t\tgoto again;\n\n\t\treturn hmm_vma_handle_pmd(walk, addr, end, hmm_pfns, pmd);\n\t}\n\n\t \n\tif (pmd_bad(pmd)) {\n\t\tif (hmm_range_need_fault(hmm_vma_walk, hmm_pfns, npages, 0))\n\t\t\treturn -EFAULT;\n\t\treturn hmm_pfns_fill(start, end, range, HMM_PFN_ERROR);\n\t}\n\n\tptep = pte_offset_map(pmdp, addr);\n\tif (!ptep)\n\t\tgoto again;\n\tfor (; addr < end; addr += PAGE_SIZE, ptep++, hmm_pfns++) {\n\t\tint r;\n\n\t\tr = hmm_vma_handle_pte(walk, addr, end, pmdp, ptep, hmm_pfns);\n\t\tif (r) {\n\t\t\t \n\t\t\treturn r;\n\t\t}\n\t}\n\tpte_unmap(ptep - 1);\n\treturn 0;\n}\n\n#if defined(CONFIG_ARCH_HAS_PTE_DEVMAP) && \\\n    defined(CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD)\nstatic inline unsigned long pud_to_hmm_pfn_flags(struct hmm_range *range,\n\t\t\t\t\t\t pud_t pud)\n{\n\tif (!pud_present(pud))\n\t\treturn 0;\n\treturn (pud_write(pud) ? (HMM_PFN_VALID | HMM_PFN_WRITE) :\n\t\t\t\t HMM_PFN_VALID) |\n\t       hmm_pfn_flags_order(PUD_SHIFT - PAGE_SHIFT);\n}\n\nstatic int hmm_vma_walk_pud(pud_t *pudp, unsigned long start, unsigned long end,\n\t\tstruct mm_walk *walk)\n{\n\tstruct hmm_vma_walk *hmm_vma_walk = walk->private;\n\tstruct hmm_range *range = hmm_vma_walk->range;\n\tunsigned long addr = start;\n\tpud_t pud;\n\tspinlock_t *ptl = pud_trans_huge_lock(pudp, walk->vma);\n\n\tif (!ptl)\n\t\treturn 0;\n\n\t \n\twalk->action = ACTION_CONTINUE;\n\n\tpud = READ_ONCE(*pudp);\n\tif (pud_none(pud)) {\n\t\tspin_unlock(ptl);\n\t\treturn hmm_vma_walk_hole(start, end, -1, walk);\n\t}\n\n\tif (pud_huge(pud) && pud_devmap(pud)) {\n\t\tunsigned long i, npages, pfn;\n\t\tunsigned int required_fault;\n\t\tunsigned long *hmm_pfns;\n\t\tunsigned long cpu_flags;\n\n\t\tif (!pud_present(pud)) {\n\t\t\tspin_unlock(ptl);\n\t\t\treturn hmm_vma_walk_hole(start, end, -1, walk);\n\t\t}\n\n\t\ti = (addr - range->start) >> PAGE_SHIFT;\n\t\tnpages = (end - addr) >> PAGE_SHIFT;\n\t\thmm_pfns = &range->hmm_pfns[i];\n\n\t\tcpu_flags = pud_to_hmm_pfn_flags(range, pud);\n\t\trequired_fault = hmm_range_need_fault(hmm_vma_walk, hmm_pfns,\n\t\t\t\t\t\t      npages, cpu_flags);\n\t\tif (required_fault) {\n\t\t\tspin_unlock(ptl);\n\t\t\treturn hmm_vma_fault(addr, end, required_fault, walk);\n\t\t}\n\n\t\tpfn = pud_pfn(pud) + ((addr & ~PUD_MASK) >> PAGE_SHIFT);\n\t\tfor (i = 0; i < npages; ++i, ++pfn)\n\t\t\thmm_pfns[i] = pfn | cpu_flags;\n\t\tgoto out_unlock;\n\t}\n\n\t \n\twalk->action = ACTION_SUBTREE;\n\nout_unlock:\n\tspin_unlock(ptl);\n\treturn 0;\n}\n#else\n#define hmm_vma_walk_pud\tNULL\n#endif\n\n#ifdef CONFIG_HUGETLB_PAGE\nstatic int hmm_vma_walk_hugetlb_entry(pte_t *pte, unsigned long hmask,\n\t\t\t\t      unsigned long start, unsigned long end,\n\t\t\t\t      struct mm_walk *walk)\n{\n\tunsigned long addr = start, i, pfn;\n\tstruct hmm_vma_walk *hmm_vma_walk = walk->private;\n\tstruct hmm_range *range = hmm_vma_walk->range;\n\tstruct vm_area_struct *vma = walk->vma;\n\tunsigned int required_fault;\n\tunsigned long pfn_req_flags;\n\tunsigned long cpu_flags;\n\tspinlock_t *ptl;\n\tpte_t entry;\n\n\tptl = huge_pte_lock(hstate_vma(vma), walk->mm, pte);\n\tentry = huge_ptep_get(pte);\n\n\ti = (start - range->start) >> PAGE_SHIFT;\n\tpfn_req_flags = range->hmm_pfns[i];\n\tcpu_flags = pte_to_hmm_pfn_flags(range, entry) |\n\t\t    hmm_pfn_flags_order(huge_page_order(hstate_vma(vma)));\n\trequired_fault =\n\t\thmm_pte_need_fault(hmm_vma_walk, pfn_req_flags, cpu_flags);\n\tif (required_fault) {\n\t\tint ret;\n\n\t\tspin_unlock(ptl);\n\t\thugetlb_vma_unlock_read(vma);\n\t\t \n\t\tret = hmm_vma_fault(addr, end, required_fault, walk);\n\t\thugetlb_vma_lock_read(vma);\n\t\treturn ret;\n\t}\n\n\tpfn = pte_pfn(entry) + ((start & ~hmask) >> PAGE_SHIFT);\n\tfor (; addr < end; addr += PAGE_SIZE, i++, pfn++)\n\t\trange->hmm_pfns[i] = pfn | cpu_flags;\n\n\tspin_unlock(ptl);\n\treturn 0;\n}\n#else\n#define hmm_vma_walk_hugetlb_entry NULL\n#endif  \n\nstatic int hmm_vma_walk_test(unsigned long start, unsigned long end,\n\t\t\t     struct mm_walk *walk)\n{\n\tstruct hmm_vma_walk *hmm_vma_walk = walk->private;\n\tstruct hmm_range *range = hmm_vma_walk->range;\n\tstruct vm_area_struct *vma = walk->vma;\n\n\tif (!(vma->vm_flags & (VM_IO | VM_PFNMAP)) &&\n\t    vma->vm_flags & VM_READ)\n\t\treturn 0;\n\n\t \n\tif (hmm_range_need_fault(hmm_vma_walk,\n\t\t\t\t range->hmm_pfns +\n\t\t\t\t\t ((start - range->start) >> PAGE_SHIFT),\n\t\t\t\t (end - start) >> PAGE_SHIFT, 0))\n\t\treturn -EFAULT;\n\n\thmm_pfns_fill(start, end, range, HMM_PFN_ERROR);\n\n\t \n\treturn 1;\n}\n\nstatic const struct mm_walk_ops hmm_walk_ops = {\n\t.pud_entry\t= hmm_vma_walk_pud,\n\t.pmd_entry\t= hmm_vma_walk_pmd,\n\t.pte_hole\t= hmm_vma_walk_hole,\n\t.hugetlb_entry\t= hmm_vma_walk_hugetlb_entry,\n\t.test_walk\t= hmm_vma_walk_test,\n\t.walk_lock\t= PGWALK_RDLOCK,\n};\n\n \nint hmm_range_fault(struct hmm_range *range)\n{\n\tstruct hmm_vma_walk hmm_vma_walk = {\n\t\t.range = range,\n\t\t.last = range->start,\n\t};\n\tstruct mm_struct *mm = range->notifier->mm;\n\tint ret;\n\n\tmmap_assert_locked(mm);\n\n\tdo {\n\t\t \n\t\tif (mmu_interval_check_retry(range->notifier,\n\t\t\t\t\t     range->notifier_seq))\n\t\t\treturn -EBUSY;\n\t\tret = walk_page_range(mm, hmm_vma_walk.last, range->end,\n\t\t\t\t      &hmm_walk_ops, &hmm_vma_walk);\n\t\t \n\t} while (ret == -EBUSY);\n\treturn ret;\n}\nEXPORT_SYMBOL(hmm_range_fault);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}