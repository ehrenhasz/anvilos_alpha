{
  "module_name": "z3fold.c",
  "hash_id": "1fd91774802c8280d5c115097cca50e2032bdf4caa0ff243f9ce153a8e1295a7",
  "original_prompt": "Ingested from linux-6.6.14/mm/z3fold.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/atomic.h>\n#include <linux/sched.h>\n#include <linux/cpumask.h>\n#include <linux/list.h>\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <linux/page-flags.h>\n#include <linux/migrate.h>\n#include <linux/node.h>\n#include <linux/compaction.h>\n#include <linux/percpu.h>\n#include <linux/preempt.h>\n#include <linux/workqueue.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n#include <linux/zpool.h>\n#include <linux/kmemleak.h>\n\n \n#define NCHUNKS_ORDER\t6\n\n#define CHUNK_SHIFT\t(PAGE_SHIFT - NCHUNKS_ORDER)\n#define CHUNK_SIZE\t(1 << CHUNK_SHIFT)\n#define ZHDR_SIZE_ALIGNED round_up(sizeof(struct z3fold_header), CHUNK_SIZE)\n#define ZHDR_CHUNKS\t(ZHDR_SIZE_ALIGNED >> CHUNK_SHIFT)\n#define TOTAL_CHUNKS\t(PAGE_SIZE >> CHUNK_SHIFT)\n#define NCHUNKS\t\t(TOTAL_CHUNKS - ZHDR_CHUNKS)\n\n#define BUDDY_MASK\t(0x3)\n#define BUDDY_SHIFT\t2\n#define SLOTS_ALIGN\t(0x40)\n\n \nstruct z3fold_pool;\n\nenum buddy {\n\tHEADLESS = 0,\n\tFIRST,\n\tMIDDLE,\n\tLAST,\n\tBUDDIES_MAX = LAST\n};\n\nstruct z3fold_buddy_slots {\n\t \n\tunsigned long slot[BUDDY_MASK + 1];\n\tunsigned long pool;  \n\trwlock_t lock;\n};\n#define HANDLE_FLAG_MASK\t(0x03)\n\n \nstruct z3fold_header {\n\tstruct list_head buddy;\n\tspinlock_t page_lock;\n\tstruct kref refcount;\n\tstruct work_struct work;\n\tstruct z3fold_buddy_slots *slots;\n\tstruct z3fold_pool *pool;\n\tshort cpu;\n\tunsigned short first_chunks;\n\tunsigned short middle_chunks;\n\tunsigned short last_chunks;\n\tunsigned short start_middle;\n\tunsigned short first_num:2;\n\tunsigned short mapped_count:2;\n\tunsigned short foreign_handles:2;\n};\n\n \nstruct z3fold_pool {\n\tconst char *name;\n\tspinlock_t lock;\n\tspinlock_t stale_lock;\n\tstruct list_head *unbuddied;\n\tstruct list_head stale;\n\tatomic64_t pages_nr;\n\tstruct kmem_cache *c_handle;\n\tstruct workqueue_struct *compact_wq;\n\tstruct workqueue_struct *release_wq;\n\tstruct work_struct work;\n};\n\n \nenum z3fold_page_flags {\n\tPAGE_HEADLESS = 0,\n\tMIDDLE_CHUNK_MAPPED,\n\tNEEDS_COMPACTING,\n\tPAGE_STALE,\n\tPAGE_CLAIMED,  \n\tPAGE_MIGRATED,  \n};\n\n \nenum z3fold_handle_flags {\n\tHANDLES_NOFREE = 0,\n};\n\n \nstatic struct z3fold_header *__z3fold_alloc(struct z3fold_pool *, size_t, bool);\nstatic void compact_page_work(struct work_struct *w);\n\n \n\n \nstatic int size_to_chunks(size_t size)\n{\n\treturn (size + CHUNK_SIZE - 1) >> CHUNK_SHIFT;\n}\n\n#define for_each_unbuddied_list(_iter, _begin) \\\n\tfor ((_iter) = (_begin); (_iter) < NCHUNKS; (_iter)++)\n\nstatic inline struct z3fold_buddy_slots *alloc_slots(struct z3fold_pool *pool,\n\t\t\t\t\t\t\tgfp_t gfp)\n{\n\tstruct z3fold_buddy_slots *slots = kmem_cache_zalloc(pool->c_handle,\n\t\t\t\t\t\t\t     gfp);\n\n\tif (slots) {\n\t\t \n\t\tkmemleak_not_leak(slots);\n\t\tslots->pool = (unsigned long)pool;\n\t\trwlock_init(&slots->lock);\n\t}\n\n\treturn slots;\n}\n\nstatic inline struct z3fold_pool *slots_to_pool(struct z3fold_buddy_slots *s)\n{\n\treturn (struct z3fold_pool *)(s->pool & ~HANDLE_FLAG_MASK);\n}\n\nstatic inline struct z3fold_buddy_slots *handle_to_slots(unsigned long handle)\n{\n\treturn (struct z3fold_buddy_slots *)(handle & ~(SLOTS_ALIGN - 1));\n}\n\n \nstatic inline void z3fold_page_lock(struct z3fold_header *zhdr)\n{\n\tspin_lock(&zhdr->page_lock);\n}\n\n \nstatic inline int z3fold_page_trylock(struct z3fold_header *zhdr)\n{\n\treturn spin_trylock(&zhdr->page_lock);\n}\n\n \nstatic inline void z3fold_page_unlock(struct z3fold_header *zhdr)\n{\n\tspin_unlock(&zhdr->page_lock);\n}\n\n \nstatic inline struct z3fold_header *get_z3fold_header(unsigned long handle)\n{\n\tstruct z3fold_buddy_slots *slots;\n\tstruct z3fold_header *zhdr;\n\tint locked = 0;\n\n\tif (!(handle & (1 << PAGE_HEADLESS))) {\n\t\tslots = handle_to_slots(handle);\n\t\tdo {\n\t\t\tunsigned long addr;\n\n\t\t\tread_lock(&slots->lock);\n\t\t\taddr = *(unsigned long *)handle;\n\t\t\tzhdr = (struct z3fold_header *)(addr & PAGE_MASK);\n\t\t\tlocked = z3fold_page_trylock(zhdr);\n\t\t\tread_unlock(&slots->lock);\n\t\t\tif (locked) {\n\t\t\t\tstruct page *page = virt_to_page(zhdr);\n\n\t\t\t\tif (!test_bit(PAGE_MIGRATED, &page->private))\n\t\t\t\t\tbreak;\n\t\t\t\tz3fold_page_unlock(zhdr);\n\t\t\t}\n\t\t\tcpu_relax();\n\t\t} while (true);\n\t} else {\n\t\tzhdr = (struct z3fold_header *)(handle & PAGE_MASK);\n\t}\n\n\treturn zhdr;\n}\n\nstatic inline void put_z3fold_header(struct z3fold_header *zhdr)\n{\n\tstruct page *page = virt_to_page(zhdr);\n\n\tif (!test_bit(PAGE_HEADLESS, &page->private))\n\t\tz3fold_page_unlock(zhdr);\n}\n\nstatic inline void free_handle(unsigned long handle, struct z3fold_header *zhdr)\n{\n\tstruct z3fold_buddy_slots *slots;\n\tint i;\n\tbool is_free;\n\n\tif (WARN_ON(*(unsigned long *)handle == 0))\n\t\treturn;\n\n\tslots = handle_to_slots(handle);\n\twrite_lock(&slots->lock);\n\t*(unsigned long *)handle = 0;\n\n\tif (test_bit(HANDLES_NOFREE, &slots->pool)) {\n\t\twrite_unlock(&slots->lock);\n\t\treturn;  \n\t}\n\n\tif (zhdr->slots != slots)\n\t\tzhdr->foreign_handles--;\n\n\tis_free = true;\n\tfor (i = 0; i <= BUDDY_MASK; i++) {\n\t\tif (slots->slot[i]) {\n\t\t\tis_free = false;\n\t\t\tbreak;\n\t\t}\n\t}\n\twrite_unlock(&slots->lock);\n\n\tif (is_free) {\n\t\tstruct z3fold_pool *pool = slots_to_pool(slots);\n\n\t\tif (zhdr->slots == slots)\n\t\t\tzhdr->slots = NULL;\n\t\tkmem_cache_free(pool->c_handle, slots);\n\t}\n}\n\n \nstatic struct z3fold_header *init_z3fold_page(struct page *page, bool headless,\n\t\t\t\t\tstruct z3fold_pool *pool, gfp_t gfp)\n{\n\tstruct z3fold_header *zhdr = page_address(page);\n\tstruct z3fold_buddy_slots *slots;\n\n\tclear_bit(PAGE_HEADLESS, &page->private);\n\tclear_bit(MIDDLE_CHUNK_MAPPED, &page->private);\n\tclear_bit(NEEDS_COMPACTING, &page->private);\n\tclear_bit(PAGE_STALE, &page->private);\n\tclear_bit(PAGE_CLAIMED, &page->private);\n\tclear_bit(PAGE_MIGRATED, &page->private);\n\tif (headless)\n\t\treturn zhdr;\n\n\tslots = alloc_slots(pool, gfp);\n\tif (!slots)\n\t\treturn NULL;\n\n\tmemset(zhdr, 0, sizeof(*zhdr));\n\tspin_lock_init(&zhdr->page_lock);\n\tkref_init(&zhdr->refcount);\n\tzhdr->cpu = -1;\n\tzhdr->slots = slots;\n\tzhdr->pool = pool;\n\tINIT_LIST_HEAD(&zhdr->buddy);\n\tINIT_WORK(&zhdr->work, compact_page_work);\n\treturn zhdr;\n}\n\n \nstatic void free_z3fold_page(struct page *page, bool headless)\n{\n\tif (!headless) {\n\t\tlock_page(page);\n\t\t__ClearPageMovable(page);\n\t\tunlock_page(page);\n\t}\n\t__free_page(page);\n}\n\n \nstatic inline int __idx(struct z3fold_header *zhdr, enum buddy bud)\n{\n\treturn (bud + zhdr->first_num) & BUDDY_MASK;\n}\n\n \nstatic unsigned long __encode_handle(struct z3fold_header *zhdr,\n\t\t\t\tstruct z3fold_buddy_slots *slots,\n\t\t\t\tenum buddy bud)\n{\n\tunsigned long h = (unsigned long)zhdr;\n\tint idx = 0;\n\n\t \n\tif (bud == HEADLESS)\n\t\treturn h | (1 << PAGE_HEADLESS);\n\n\t \n\tidx = __idx(zhdr, bud);\n\th += idx;\n\tif (bud == LAST)\n\t\th |= (zhdr->last_chunks << BUDDY_SHIFT);\n\n\twrite_lock(&slots->lock);\n\tslots->slot[idx] = h;\n\twrite_unlock(&slots->lock);\n\treturn (unsigned long)&slots->slot[idx];\n}\n\nstatic unsigned long encode_handle(struct z3fold_header *zhdr, enum buddy bud)\n{\n\treturn __encode_handle(zhdr, zhdr->slots, bud);\n}\n\n \nstatic unsigned short handle_to_chunks(unsigned long handle)\n{\n\tstruct z3fold_buddy_slots *slots = handle_to_slots(handle);\n\tunsigned long addr;\n\n\tread_lock(&slots->lock);\n\taddr = *(unsigned long *)handle;\n\tread_unlock(&slots->lock);\n\treturn (addr & ~PAGE_MASK) >> BUDDY_SHIFT;\n}\n\n \nstatic enum buddy handle_to_buddy(unsigned long handle)\n{\n\tstruct z3fold_header *zhdr;\n\tstruct z3fold_buddy_slots *slots = handle_to_slots(handle);\n\tunsigned long addr;\n\n\tread_lock(&slots->lock);\n\tWARN_ON(handle & (1 << PAGE_HEADLESS));\n\taddr = *(unsigned long *)handle;\n\tread_unlock(&slots->lock);\n\tzhdr = (struct z3fold_header *)(addr & PAGE_MASK);\n\treturn (addr - zhdr->first_num) & BUDDY_MASK;\n}\n\nstatic inline struct z3fold_pool *zhdr_to_pool(struct z3fold_header *zhdr)\n{\n\treturn zhdr->pool;\n}\n\nstatic void __release_z3fold_page(struct z3fold_header *zhdr, bool locked)\n{\n\tstruct page *page = virt_to_page(zhdr);\n\tstruct z3fold_pool *pool = zhdr_to_pool(zhdr);\n\n\tWARN_ON(!list_empty(&zhdr->buddy));\n\tset_bit(PAGE_STALE, &page->private);\n\tclear_bit(NEEDS_COMPACTING, &page->private);\n\tspin_lock(&pool->lock);\n\tspin_unlock(&pool->lock);\n\n\tif (locked)\n\t\tz3fold_page_unlock(zhdr);\n\n\tspin_lock(&pool->stale_lock);\n\tlist_add(&zhdr->buddy, &pool->stale);\n\tqueue_work(pool->release_wq, &pool->work);\n\tspin_unlock(&pool->stale_lock);\n\n\tatomic64_dec(&pool->pages_nr);\n}\n\nstatic void release_z3fold_page_locked(struct kref *ref)\n{\n\tstruct z3fold_header *zhdr = container_of(ref, struct z3fold_header,\n\t\t\t\t\t\trefcount);\n\tWARN_ON(z3fold_page_trylock(zhdr));\n\t__release_z3fold_page(zhdr, true);\n}\n\nstatic void release_z3fold_page_locked_list(struct kref *ref)\n{\n\tstruct z3fold_header *zhdr = container_of(ref, struct z3fold_header,\n\t\t\t\t\t       refcount);\n\tstruct z3fold_pool *pool = zhdr_to_pool(zhdr);\n\n\tspin_lock(&pool->lock);\n\tlist_del_init(&zhdr->buddy);\n\tspin_unlock(&pool->lock);\n\n\tWARN_ON(z3fold_page_trylock(zhdr));\n\t__release_z3fold_page(zhdr, true);\n}\n\nstatic inline int put_z3fold_locked(struct z3fold_header *zhdr)\n{\n\treturn kref_put(&zhdr->refcount, release_z3fold_page_locked);\n}\n\nstatic inline int put_z3fold_locked_list(struct z3fold_header *zhdr)\n{\n\treturn kref_put(&zhdr->refcount, release_z3fold_page_locked_list);\n}\n\nstatic void free_pages_work(struct work_struct *w)\n{\n\tstruct z3fold_pool *pool = container_of(w, struct z3fold_pool, work);\n\n\tspin_lock(&pool->stale_lock);\n\twhile (!list_empty(&pool->stale)) {\n\t\tstruct z3fold_header *zhdr = list_first_entry(&pool->stale,\n\t\t\t\t\t\tstruct z3fold_header, buddy);\n\t\tstruct page *page = virt_to_page(zhdr);\n\n\t\tlist_del(&zhdr->buddy);\n\t\tif (WARN_ON(!test_bit(PAGE_STALE, &page->private)))\n\t\t\tcontinue;\n\t\tspin_unlock(&pool->stale_lock);\n\t\tcancel_work_sync(&zhdr->work);\n\t\tfree_z3fold_page(page, false);\n\t\tcond_resched();\n\t\tspin_lock(&pool->stale_lock);\n\t}\n\tspin_unlock(&pool->stale_lock);\n}\n\n \nstatic int num_free_chunks(struct z3fold_header *zhdr)\n{\n\tint nfree;\n\t \n\tif (zhdr->middle_chunks != 0) {\n\t\tint nfree_before = zhdr->first_chunks ?\n\t\t\t0 : zhdr->start_middle - ZHDR_CHUNKS;\n\t\tint nfree_after = zhdr->last_chunks ?\n\t\t\t0 : TOTAL_CHUNKS -\n\t\t\t\t(zhdr->start_middle + zhdr->middle_chunks);\n\t\tnfree = max(nfree_before, nfree_after);\n\t} else\n\t\tnfree = NCHUNKS - zhdr->first_chunks - zhdr->last_chunks;\n\treturn nfree;\n}\n\n \nstatic inline void add_to_unbuddied(struct z3fold_pool *pool,\n\t\t\t\tstruct z3fold_header *zhdr)\n{\n\tif (zhdr->first_chunks == 0 || zhdr->last_chunks == 0 ||\n\t\t\tzhdr->middle_chunks == 0) {\n\t\tstruct list_head *unbuddied;\n\t\tint freechunks = num_free_chunks(zhdr);\n\n\t\tmigrate_disable();\n\t\tunbuddied = this_cpu_ptr(pool->unbuddied);\n\t\tspin_lock(&pool->lock);\n\t\tlist_add(&zhdr->buddy, &unbuddied[freechunks]);\n\t\tspin_unlock(&pool->lock);\n\t\tzhdr->cpu = smp_processor_id();\n\t\tmigrate_enable();\n\t}\n}\n\nstatic inline enum buddy get_free_buddy(struct z3fold_header *zhdr, int chunks)\n{\n\tenum buddy bud = HEADLESS;\n\n\tif (zhdr->middle_chunks) {\n\t\tif (!zhdr->first_chunks &&\n\t\t    chunks <= zhdr->start_middle - ZHDR_CHUNKS)\n\t\t\tbud = FIRST;\n\t\telse if (!zhdr->last_chunks)\n\t\t\tbud = LAST;\n\t} else {\n\t\tif (!zhdr->first_chunks)\n\t\t\tbud = FIRST;\n\t\telse if (!zhdr->last_chunks)\n\t\t\tbud = LAST;\n\t\telse\n\t\t\tbud = MIDDLE;\n\t}\n\n\treturn bud;\n}\n\nstatic inline void *mchunk_memmove(struct z3fold_header *zhdr,\n\t\t\t\tunsigned short dst_chunk)\n{\n\tvoid *beg = zhdr;\n\treturn memmove(beg + (dst_chunk << CHUNK_SHIFT),\n\t\t       beg + (zhdr->start_middle << CHUNK_SHIFT),\n\t\t       zhdr->middle_chunks << CHUNK_SHIFT);\n}\n\nstatic inline bool buddy_single(struct z3fold_header *zhdr)\n{\n\treturn !((zhdr->first_chunks && zhdr->middle_chunks) ||\n\t\t\t(zhdr->first_chunks && zhdr->last_chunks) ||\n\t\t\t(zhdr->middle_chunks && zhdr->last_chunks));\n}\n\nstatic struct z3fold_header *compact_single_buddy(struct z3fold_header *zhdr)\n{\n\tstruct z3fold_pool *pool = zhdr_to_pool(zhdr);\n\tvoid *p = zhdr;\n\tunsigned long old_handle = 0;\n\tsize_t sz = 0;\n\tstruct z3fold_header *new_zhdr = NULL;\n\tint first_idx = __idx(zhdr, FIRST);\n\tint middle_idx = __idx(zhdr, MIDDLE);\n\tint last_idx = __idx(zhdr, LAST);\n\tunsigned short *moved_chunks = NULL;\n\n\t \n\tif (zhdr->first_chunks && zhdr->slots->slot[first_idx]) {\n\t\tp += ZHDR_SIZE_ALIGNED;\n\t\tsz = zhdr->first_chunks << CHUNK_SHIFT;\n\t\told_handle = (unsigned long)&zhdr->slots->slot[first_idx];\n\t\tmoved_chunks = &zhdr->first_chunks;\n\t} else if (zhdr->middle_chunks && zhdr->slots->slot[middle_idx]) {\n\t\tp += zhdr->start_middle << CHUNK_SHIFT;\n\t\tsz = zhdr->middle_chunks << CHUNK_SHIFT;\n\t\told_handle = (unsigned long)&zhdr->slots->slot[middle_idx];\n\t\tmoved_chunks = &zhdr->middle_chunks;\n\t} else if (zhdr->last_chunks && zhdr->slots->slot[last_idx]) {\n\t\tp += PAGE_SIZE - (zhdr->last_chunks << CHUNK_SHIFT);\n\t\tsz = zhdr->last_chunks << CHUNK_SHIFT;\n\t\told_handle = (unsigned long)&zhdr->slots->slot[last_idx];\n\t\tmoved_chunks = &zhdr->last_chunks;\n\t}\n\n\tif (sz > 0) {\n\t\tenum buddy new_bud = HEADLESS;\n\t\tshort chunks = size_to_chunks(sz);\n\t\tvoid *q;\n\n\t\tnew_zhdr = __z3fold_alloc(pool, sz, false);\n\t\tif (!new_zhdr)\n\t\t\treturn NULL;\n\n\t\tif (WARN_ON(new_zhdr == zhdr))\n\t\t\tgoto out_fail;\n\n\t\tnew_bud = get_free_buddy(new_zhdr, chunks);\n\t\tq = new_zhdr;\n\t\tswitch (new_bud) {\n\t\tcase FIRST:\n\t\t\tnew_zhdr->first_chunks = chunks;\n\t\t\tq += ZHDR_SIZE_ALIGNED;\n\t\t\tbreak;\n\t\tcase MIDDLE:\n\t\t\tnew_zhdr->middle_chunks = chunks;\n\t\t\tnew_zhdr->start_middle =\n\t\t\t\tnew_zhdr->first_chunks + ZHDR_CHUNKS;\n\t\t\tq += new_zhdr->start_middle << CHUNK_SHIFT;\n\t\t\tbreak;\n\t\tcase LAST:\n\t\t\tnew_zhdr->last_chunks = chunks;\n\t\t\tq += PAGE_SIZE - (new_zhdr->last_chunks << CHUNK_SHIFT);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto out_fail;\n\t\t}\n\t\tnew_zhdr->foreign_handles++;\n\t\tmemcpy(q, p, sz);\n\t\twrite_lock(&zhdr->slots->lock);\n\t\t*(unsigned long *)old_handle = (unsigned long)new_zhdr +\n\t\t\t__idx(new_zhdr, new_bud);\n\t\tif (new_bud == LAST)\n\t\t\t*(unsigned long *)old_handle |=\n\t\t\t\t\t(new_zhdr->last_chunks << BUDDY_SHIFT);\n\t\twrite_unlock(&zhdr->slots->lock);\n\t\tadd_to_unbuddied(pool, new_zhdr);\n\t\tz3fold_page_unlock(new_zhdr);\n\n\t\t*moved_chunks = 0;\n\t}\n\n\treturn new_zhdr;\n\nout_fail:\n\tif (new_zhdr && !put_z3fold_locked(new_zhdr)) {\n\t\tadd_to_unbuddied(pool, new_zhdr);\n\t\tz3fold_page_unlock(new_zhdr);\n\t}\n\treturn NULL;\n\n}\n\n#define BIG_CHUNK_GAP\t3\n \nstatic int z3fold_compact_page(struct z3fold_header *zhdr)\n{\n\tstruct page *page = virt_to_page(zhdr);\n\n\tif (test_bit(MIDDLE_CHUNK_MAPPED, &page->private))\n\t\treturn 0;  \n\n\tif (unlikely(PageIsolated(page)))\n\t\treturn 0;\n\n\tif (zhdr->middle_chunks == 0)\n\t\treturn 0;  \n\n\tif (zhdr->first_chunks == 0 && zhdr->last_chunks == 0) {\n\t\t \n\t\tmchunk_memmove(zhdr, ZHDR_CHUNKS);\n\t\tzhdr->first_chunks = zhdr->middle_chunks;\n\t\tzhdr->middle_chunks = 0;\n\t\tzhdr->start_middle = 0;\n\t\tzhdr->first_num++;\n\t\treturn 1;\n\t}\n\n\t \n\tif (zhdr->first_chunks != 0 && zhdr->last_chunks == 0 &&\n\t    zhdr->start_middle - (zhdr->first_chunks + ZHDR_CHUNKS) >=\n\t\t\tBIG_CHUNK_GAP) {\n\t\tmchunk_memmove(zhdr, zhdr->first_chunks + ZHDR_CHUNKS);\n\t\tzhdr->start_middle = zhdr->first_chunks + ZHDR_CHUNKS;\n\t\treturn 1;\n\t} else if (zhdr->last_chunks != 0 && zhdr->first_chunks == 0 &&\n\t\t   TOTAL_CHUNKS - (zhdr->last_chunks + zhdr->start_middle\n\t\t\t\t\t+ zhdr->middle_chunks) >=\n\t\t\tBIG_CHUNK_GAP) {\n\t\tunsigned short new_start = TOTAL_CHUNKS - zhdr->last_chunks -\n\t\t\tzhdr->middle_chunks;\n\t\tmchunk_memmove(zhdr, new_start);\n\t\tzhdr->start_middle = new_start;\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic void do_compact_page(struct z3fold_header *zhdr, bool locked)\n{\n\tstruct z3fold_pool *pool = zhdr_to_pool(zhdr);\n\tstruct page *page;\n\n\tpage = virt_to_page(zhdr);\n\tif (locked)\n\t\tWARN_ON(z3fold_page_trylock(zhdr));\n\telse\n\t\tz3fold_page_lock(zhdr);\n\tif (WARN_ON(!test_and_clear_bit(NEEDS_COMPACTING, &page->private))) {\n\t\tz3fold_page_unlock(zhdr);\n\t\treturn;\n\t}\n\tspin_lock(&pool->lock);\n\tlist_del_init(&zhdr->buddy);\n\tspin_unlock(&pool->lock);\n\n\tif (put_z3fold_locked(zhdr))\n\t\treturn;\n\n\tif (test_bit(PAGE_STALE, &page->private) ||\n\t    test_and_set_bit(PAGE_CLAIMED, &page->private)) {\n\t\tz3fold_page_unlock(zhdr);\n\t\treturn;\n\t}\n\n\tif (!zhdr->foreign_handles && buddy_single(zhdr) &&\n\t    zhdr->mapped_count == 0 && compact_single_buddy(zhdr)) {\n\t\tif (!put_z3fold_locked(zhdr)) {\n\t\t\tclear_bit(PAGE_CLAIMED, &page->private);\n\t\t\tz3fold_page_unlock(zhdr);\n\t\t}\n\t\treturn;\n\t}\n\n\tz3fold_compact_page(zhdr);\n\tadd_to_unbuddied(pool, zhdr);\n\tclear_bit(PAGE_CLAIMED, &page->private);\n\tz3fold_page_unlock(zhdr);\n}\n\nstatic void compact_page_work(struct work_struct *w)\n{\n\tstruct z3fold_header *zhdr = container_of(w, struct z3fold_header,\n\t\t\t\t\t\twork);\n\n\tdo_compact_page(zhdr, false);\n}\n\n \nstatic inline struct z3fold_header *__z3fold_alloc(struct z3fold_pool *pool,\n\t\t\t\t\t\tsize_t size, bool can_sleep)\n{\n\tstruct z3fold_header *zhdr = NULL;\n\tstruct page *page;\n\tstruct list_head *unbuddied;\n\tint chunks = size_to_chunks(size), i;\n\nlookup:\n\tmigrate_disable();\n\t \n\tunbuddied = this_cpu_ptr(pool->unbuddied);\n\tfor_each_unbuddied_list(i, chunks) {\n\t\tstruct list_head *l = &unbuddied[i];\n\n\t\tzhdr = list_first_entry_or_null(READ_ONCE(l),\n\t\t\t\t\tstruct z3fold_header, buddy);\n\n\t\tif (!zhdr)\n\t\t\tcontinue;\n\n\t\t \n\t\tspin_lock(&pool->lock);\n\t\tif (unlikely(zhdr != list_first_entry(READ_ONCE(l),\n\t\t\t\t\t\tstruct z3fold_header, buddy)) ||\n\t\t    !z3fold_page_trylock(zhdr)) {\n\t\t\tspin_unlock(&pool->lock);\n\t\t\tzhdr = NULL;\n\t\t\tmigrate_enable();\n\t\t\tif (can_sleep)\n\t\t\t\tcond_resched();\n\t\t\tgoto lookup;\n\t\t}\n\t\tlist_del_init(&zhdr->buddy);\n\t\tzhdr->cpu = -1;\n\t\tspin_unlock(&pool->lock);\n\n\t\tpage = virt_to_page(zhdr);\n\t\tif (test_bit(NEEDS_COMPACTING, &page->private) ||\n\t\t    test_bit(PAGE_CLAIMED, &page->private)) {\n\t\t\tz3fold_page_unlock(zhdr);\n\t\t\tzhdr = NULL;\n\t\t\tmigrate_enable();\n\t\t\tif (can_sleep)\n\t\t\t\tcond_resched();\n\t\t\tgoto lookup;\n\t\t}\n\n\t\t \n\t\tkref_get(&zhdr->refcount);\n\t\tbreak;\n\t}\n\tmigrate_enable();\n\n\tif (!zhdr) {\n\t\tint cpu;\n\n\t\t \n\t\tfor_each_online_cpu(cpu) {\n\t\t\tstruct list_head *l;\n\n\t\t\tunbuddied = per_cpu_ptr(pool->unbuddied, cpu);\n\t\t\tspin_lock(&pool->lock);\n\t\t\tl = &unbuddied[chunks];\n\n\t\t\tzhdr = list_first_entry_or_null(READ_ONCE(l),\n\t\t\t\t\t\tstruct z3fold_header, buddy);\n\n\t\t\tif (!zhdr || !z3fold_page_trylock(zhdr)) {\n\t\t\t\tspin_unlock(&pool->lock);\n\t\t\t\tzhdr = NULL;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tlist_del_init(&zhdr->buddy);\n\t\t\tzhdr->cpu = -1;\n\t\t\tspin_unlock(&pool->lock);\n\n\t\t\tpage = virt_to_page(zhdr);\n\t\t\tif (test_bit(NEEDS_COMPACTING, &page->private) ||\n\t\t\t    test_bit(PAGE_CLAIMED, &page->private)) {\n\t\t\t\tz3fold_page_unlock(zhdr);\n\t\t\t\tzhdr = NULL;\n\t\t\t\tif (can_sleep)\n\t\t\t\t\tcond_resched();\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tkref_get(&zhdr->refcount);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (zhdr && !zhdr->slots) {\n\t\tzhdr->slots = alloc_slots(pool, GFP_ATOMIC);\n\t\tif (!zhdr->slots)\n\t\t\tgoto out_fail;\n\t}\n\treturn zhdr;\n\nout_fail:\n\tif (!put_z3fold_locked(zhdr)) {\n\t\tadd_to_unbuddied(pool, zhdr);\n\t\tz3fold_page_unlock(zhdr);\n\t}\n\treturn NULL;\n}\n\n \n\n \nstatic struct z3fold_pool *z3fold_create_pool(const char *name, gfp_t gfp)\n{\n\tstruct z3fold_pool *pool = NULL;\n\tint i, cpu;\n\n\tpool = kzalloc(sizeof(struct z3fold_pool), gfp);\n\tif (!pool)\n\t\tgoto out;\n\tpool->c_handle = kmem_cache_create(\"z3fold_handle\",\n\t\t\t\tsizeof(struct z3fold_buddy_slots),\n\t\t\t\tSLOTS_ALIGN, 0, NULL);\n\tif (!pool->c_handle)\n\t\tgoto out_c;\n\tspin_lock_init(&pool->lock);\n\tspin_lock_init(&pool->stale_lock);\n\tpool->unbuddied = __alloc_percpu(sizeof(struct list_head) * NCHUNKS,\n\t\t\t\t\t __alignof__(struct list_head));\n\tif (!pool->unbuddied)\n\t\tgoto out_pool;\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct list_head *unbuddied =\n\t\t\t\tper_cpu_ptr(pool->unbuddied, cpu);\n\t\tfor_each_unbuddied_list(i, 0)\n\t\t\tINIT_LIST_HEAD(&unbuddied[i]);\n\t}\n\tINIT_LIST_HEAD(&pool->stale);\n\tatomic64_set(&pool->pages_nr, 0);\n\tpool->name = name;\n\tpool->compact_wq = create_singlethread_workqueue(pool->name);\n\tif (!pool->compact_wq)\n\t\tgoto out_unbuddied;\n\tpool->release_wq = create_singlethread_workqueue(pool->name);\n\tif (!pool->release_wq)\n\t\tgoto out_wq;\n\tINIT_WORK(&pool->work, free_pages_work);\n\treturn pool;\n\nout_wq:\n\tdestroy_workqueue(pool->compact_wq);\nout_unbuddied:\n\tfree_percpu(pool->unbuddied);\nout_pool:\n\tkmem_cache_destroy(pool->c_handle);\nout_c:\n\tkfree(pool);\nout:\n\treturn NULL;\n}\n\n \nstatic void z3fold_destroy_pool(struct z3fold_pool *pool)\n{\n\tkmem_cache_destroy(pool->c_handle);\n\n\t \n\n\tdestroy_workqueue(pool->compact_wq);\n\tdestroy_workqueue(pool->release_wq);\n\tfree_percpu(pool->unbuddied);\n\tkfree(pool);\n}\n\nstatic const struct movable_operations z3fold_mops;\n\n \nstatic int z3fold_alloc(struct z3fold_pool *pool, size_t size, gfp_t gfp,\n\t\t\tunsigned long *handle)\n{\n\tint chunks = size_to_chunks(size);\n\tstruct z3fold_header *zhdr = NULL;\n\tstruct page *page = NULL;\n\tenum buddy bud;\n\tbool can_sleep = gfpflags_allow_blocking(gfp);\n\n\tif (!size || (gfp & __GFP_HIGHMEM))\n\t\treturn -EINVAL;\n\n\tif (size > PAGE_SIZE)\n\t\treturn -ENOSPC;\n\n\tif (size > PAGE_SIZE - ZHDR_SIZE_ALIGNED - CHUNK_SIZE)\n\t\tbud = HEADLESS;\n\telse {\nretry:\n\t\tzhdr = __z3fold_alloc(pool, size, can_sleep);\n\t\tif (zhdr) {\n\t\t\tbud = get_free_buddy(zhdr, chunks);\n\t\t\tif (bud == HEADLESS) {\n\t\t\t\tif (!put_z3fold_locked(zhdr))\n\t\t\t\t\tz3fold_page_unlock(zhdr);\n\t\t\t\tpr_err(\"No free chunks in unbuddied\\n\");\n\t\t\t\tWARN_ON(1);\n\t\t\t\tgoto retry;\n\t\t\t}\n\t\t\tpage = virt_to_page(zhdr);\n\t\t\tgoto found;\n\t\t}\n\t\tbud = FIRST;\n\t}\n\n\tpage = alloc_page(gfp);\n\tif (!page)\n\t\treturn -ENOMEM;\n\n\tzhdr = init_z3fold_page(page, bud == HEADLESS, pool, gfp);\n\tif (!zhdr) {\n\t\t__free_page(page);\n\t\treturn -ENOMEM;\n\t}\n\tatomic64_inc(&pool->pages_nr);\n\n\tif (bud == HEADLESS) {\n\t\tset_bit(PAGE_HEADLESS, &page->private);\n\t\tgoto headless;\n\t}\n\tif (can_sleep) {\n\t\tlock_page(page);\n\t\t__SetPageMovable(page, &z3fold_mops);\n\t\tunlock_page(page);\n\t} else {\n\t\tWARN_ON(!trylock_page(page));\n\t\t__SetPageMovable(page, &z3fold_mops);\n\t\tunlock_page(page);\n\t}\n\tz3fold_page_lock(zhdr);\n\nfound:\n\tif (bud == FIRST)\n\t\tzhdr->first_chunks = chunks;\n\telse if (bud == LAST)\n\t\tzhdr->last_chunks = chunks;\n\telse {\n\t\tzhdr->middle_chunks = chunks;\n\t\tzhdr->start_middle = zhdr->first_chunks + ZHDR_CHUNKS;\n\t}\n\tadd_to_unbuddied(pool, zhdr);\n\nheadless:\n\tspin_lock(&pool->lock);\n\t*handle = encode_handle(zhdr, bud);\n\tspin_unlock(&pool->lock);\n\tif (bud != HEADLESS)\n\t\tz3fold_page_unlock(zhdr);\n\n\treturn 0;\n}\n\n \nstatic void z3fold_free(struct z3fold_pool *pool, unsigned long handle)\n{\n\tstruct z3fold_header *zhdr;\n\tstruct page *page;\n\tenum buddy bud;\n\tbool page_claimed;\n\n\tzhdr = get_z3fold_header(handle);\n\tpage = virt_to_page(zhdr);\n\tpage_claimed = test_and_set_bit(PAGE_CLAIMED, &page->private);\n\n\tif (test_bit(PAGE_HEADLESS, &page->private)) {\n\t\t \n\t\tif (!page_claimed) {\n\t\t\tput_z3fold_header(zhdr);\n\t\t\tfree_z3fold_page(page, true);\n\t\t\tatomic64_dec(&pool->pages_nr);\n\t\t}\n\t\treturn;\n\t}\n\n\t \n\tbud = handle_to_buddy(handle);\n\n\tswitch (bud) {\n\tcase FIRST:\n\t\tzhdr->first_chunks = 0;\n\t\tbreak;\n\tcase MIDDLE:\n\t\tzhdr->middle_chunks = 0;\n\t\tbreak;\n\tcase LAST:\n\t\tzhdr->last_chunks = 0;\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"%s: unknown bud %d\\n\", __func__, bud);\n\t\tWARN_ON(1);\n\t\tput_z3fold_header(zhdr);\n\t\treturn;\n\t}\n\n\tif (!page_claimed)\n\t\tfree_handle(handle, zhdr);\n\tif (put_z3fold_locked_list(zhdr))\n\t\treturn;\n\tif (page_claimed) {\n\t\t \n\t\tput_z3fold_header(zhdr);\n\t\treturn;\n\t}\n\tif (test_and_set_bit(NEEDS_COMPACTING, &page->private)) {\n\t\tclear_bit(PAGE_CLAIMED, &page->private);\n\t\tput_z3fold_header(zhdr);\n\t\treturn;\n\t}\n\tif (zhdr->cpu < 0 || !cpu_online(zhdr->cpu)) {\n\t\tzhdr->cpu = -1;\n\t\tkref_get(&zhdr->refcount);\n\t\tclear_bit(PAGE_CLAIMED, &page->private);\n\t\tdo_compact_page(zhdr, true);\n\t\treturn;\n\t}\n\tkref_get(&zhdr->refcount);\n\tclear_bit(PAGE_CLAIMED, &page->private);\n\tqueue_work_on(zhdr->cpu, pool->compact_wq, &zhdr->work);\n\tput_z3fold_header(zhdr);\n}\n\n \nstatic void *z3fold_map(struct z3fold_pool *pool, unsigned long handle)\n{\n\tstruct z3fold_header *zhdr;\n\tstruct page *page;\n\tvoid *addr;\n\tenum buddy buddy;\n\n\tzhdr = get_z3fold_header(handle);\n\taddr = zhdr;\n\tpage = virt_to_page(zhdr);\n\n\tif (test_bit(PAGE_HEADLESS, &page->private))\n\t\tgoto out;\n\n\tbuddy = handle_to_buddy(handle);\n\tswitch (buddy) {\n\tcase FIRST:\n\t\taddr += ZHDR_SIZE_ALIGNED;\n\t\tbreak;\n\tcase MIDDLE:\n\t\taddr += zhdr->start_middle << CHUNK_SHIFT;\n\t\tset_bit(MIDDLE_CHUNK_MAPPED, &page->private);\n\t\tbreak;\n\tcase LAST:\n\t\taddr += PAGE_SIZE - (handle_to_chunks(handle) << CHUNK_SHIFT);\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"unknown buddy id %d\\n\", buddy);\n\t\tWARN_ON(1);\n\t\taddr = NULL;\n\t\tbreak;\n\t}\n\n\tif (addr)\n\t\tzhdr->mapped_count++;\nout:\n\tput_z3fold_header(zhdr);\n\treturn addr;\n}\n\n \nstatic void z3fold_unmap(struct z3fold_pool *pool, unsigned long handle)\n{\n\tstruct z3fold_header *zhdr;\n\tstruct page *page;\n\tenum buddy buddy;\n\n\tzhdr = get_z3fold_header(handle);\n\tpage = virt_to_page(zhdr);\n\n\tif (test_bit(PAGE_HEADLESS, &page->private))\n\t\treturn;\n\n\tbuddy = handle_to_buddy(handle);\n\tif (buddy == MIDDLE)\n\t\tclear_bit(MIDDLE_CHUNK_MAPPED, &page->private);\n\tzhdr->mapped_count--;\n\tput_z3fold_header(zhdr);\n}\n\n \nstatic u64 z3fold_get_pool_size(struct z3fold_pool *pool)\n{\n\treturn atomic64_read(&pool->pages_nr);\n}\n\nstatic bool z3fold_page_isolate(struct page *page, isolate_mode_t mode)\n{\n\tstruct z3fold_header *zhdr;\n\tstruct z3fold_pool *pool;\n\n\tVM_BUG_ON_PAGE(PageIsolated(page), page);\n\n\tif (test_bit(PAGE_HEADLESS, &page->private))\n\t\treturn false;\n\n\tzhdr = page_address(page);\n\tz3fold_page_lock(zhdr);\n\tif (test_bit(NEEDS_COMPACTING, &page->private) ||\n\t    test_bit(PAGE_STALE, &page->private))\n\t\tgoto out;\n\n\tif (zhdr->mapped_count != 0 || zhdr->foreign_handles != 0)\n\t\tgoto out;\n\n\tif (test_and_set_bit(PAGE_CLAIMED, &page->private))\n\t\tgoto out;\n\tpool = zhdr_to_pool(zhdr);\n\tspin_lock(&pool->lock);\n\tif (!list_empty(&zhdr->buddy))\n\t\tlist_del_init(&zhdr->buddy);\n\tspin_unlock(&pool->lock);\n\n\tkref_get(&zhdr->refcount);\n\tz3fold_page_unlock(zhdr);\n\treturn true;\n\nout:\n\tz3fold_page_unlock(zhdr);\n\treturn false;\n}\n\nstatic int z3fold_page_migrate(struct page *newpage, struct page *page,\n\t\tenum migrate_mode mode)\n{\n\tstruct z3fold_header *zhdr, *new_zhdr;\n\tstruct z3fold_pool *pool;\n\n\tVM_BUG_ON_PAGE(!PageIsolated(page), page);\n\tVM_BUG_ON_PAGE(!test_bit(PAGE_CLAIMED, &page->private), page);\n\tVM_BUG_ON_PAGE(!PageLocked(newpage), newpage);\n\n\tzhdr = page_address(page);\n\tpool = zhdr_to_pool(zhdr);\n\n\tif (!z3fold_page_trylock(zhdr))\n\t\treturn -EAGAIN;\n\tif (zhdr->mapped_count != 0 || zhdr->foreign_handles != 0) {\n\t\tclear_bit(PAGE_CLAIMED, &page->private);\n\t\tz3fold_page_unlock(zhdr);\n\t\treturn -EBUSY;\n\t}\n\tif (work_pending(&zhdr->work)) {\n\t\tz3fold_page_unlock(zhdr);\n\t\treturn -EAGAIN;\n\t}\n\tnew_zhdr = page_address(newpage);\n\tmemcpy(new_zhdr, zhdr, PAGE_SIZE);\n\tnewpage->private = page->private;\n\tset_bit(PAGE_MIGRATED, &page->private);\n\tz3fold_page_unlock(zhdr);\n\tspin_lock_init(&new_zhdr->page_lock);\n\tINIT_WORK(&new_zhdr->work, compact_page_work);\n\t \n\tINIT_LIST_HEAD(&new_zhdr->buddy);\n\t__ClearPageMovable(page);\n\n\tget_page(newpage);\n\tz3fold_page_lock(new_zhdr);\n\tif (new_zhdr->first_chunks)\n\t\tencode_handle(new_zhdr, FIRST);\n\tif (new_zhdr->last_chunks)\n\t\tencode_handle(new_zhdr, LAST);\n\tif (new_zhdr->middle_chunks)\n\t\tencode_handle(new_zhdr, MIDDLE);\n\tset_bit(NEEDS_COMPACTING, &newpage->private);\n\tnew_zhdr->cpu = smp_processor_id();\n\t__SetPageMovable(newpage, &z3fold_mops);\n\tz3fold_page_unlock(new_zhdr);\n\n\tqueue_work_on(new_zhdr->cpu, pool->compact_wq, &new_zhdr->work);\n\n\t \n\tpage->private = 0;\n\tput_page(page);\n\treturn 0;\n}\n\nstatic void z3fold_page_putback(struct page *page)\n{\n\tstruct z3fold_header *zhdr;\n\tstruct z3fold_pool *pool;\n\n\tzhdr = page_address(page);\n\tpool = zhdr_to_pool(zhdr);\n\n\tz3fold_page_lock(zhdr);\n\tif (!list_empty(&zhdr->buddy))\n\t\tlist_del_init(&zhdr->buddy);\n\tINIT_LIST_HEAD(&page->lru);\n\tif (put_z3fold_locked(zhdr))\n\t\treturn;\n\tif (list_empty(&zhdr->buddy))\n\t\tadd_to_unbuddied(pool, zhdr);\n\tclear_bit(PAGE_CLAIMED, &page->private);\n\tz3fold_page_unlock(zhdr);\n}\n\nstatic const struct movable_operations z3fold_mops = {\n\t.isolate_page = z3fold_page_isolate,\n\t.migrate_page = z3fold_page_migrate,\n\t.putback_page = z3fold_page_putback,\n};\n\n \n\nstatic void *z3fold_zpool_create(const char *name, gfp_t gfp)\n{\n\treturn z3fold_create_pool(name, gfp);\n}\n\nstatic void z3fold_zpool_destroy(void *pool)\n{\n\tz3fold_destroy_pool(pool);\n}\n\nstatic int z3fold_zpool_malloc(void *pool, size_t size, gfp_t gfp,\n\t\t\tunsigned long *handle)\n{\n\treturn z3fold_alloc(pool, size, gfp, handle);\n}\nstatic void z3fold_zpool_free(void *pool, unsigned long handle)\n{\n\tz3fold_free(pool, handle);\n}\n\nstatic void *z3fold_zpool_map(void *pool, unsigned long handle,\n\t\t\tenum zpool_mapmode mm)\n{\n\treturn z3fold_map(pool, handle);\n}\nstatic void z3fold_zpool_unmap(void *pool, unsigned long handle)\n{\n\tz3fold_unmap(pool, handle);\n}\n\nstatic u64 z3fold_zpool_total_size(void *pool)\n{\n\treturn z3fold_get_pool_size(pool) * PAGE_SIZE;\n}\n\nstatic struct zpool_driver z3fold_zpool_driver = {\n\t.type =\t\t\"z3fold\",\n\t.sleep_mapped = true,\n\t.owner =\tTHIS_MODULE,\n\t.create =\tz3fold_zpool_create,\n\t.destroy =\tz3fold_zpool_destroy,\n\t.malloc =\tz3fold_zpool_malloc,\n\t.free =\t\tz3fold_zpool_free,\n\t.map =\t\tz3fold_zpool_map,\n\t.unmap =\tz3fold_zpool_unmap,\n\t.total_size =\tz3fold_zpool_total_size,\n};\n\nMODULE_ALIAS(\"zpool-z3fold\");\n\nstatic int __init init_z3fold(void)\n{\n\t \n\tBUILD_BUG_ON(ZHDR_SIZE_ALIGNED > PAGE_SIZE - CHUNK_SIZE);\n\tzpool_register_driver(&z3fold_zpool_driver);\n\n\treturn 0;\n}\n\nstatic void __exit exit_z3fold(void)\n{\n\tzpool_unregister_driver(&z3fold_zpool_driver);\n}\n\nmodule_init(init_z3fold);\nmodule_exit(exit_z3fold);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_AUTHOR(\"Vitaly Wool <vitalywool@gmail.com>\");\nMODULE_DESCRIPTION(\"3-Fold Allocator for Compressed Pages\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}