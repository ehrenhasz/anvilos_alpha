{
  "module_name": "workingset.c",
  "hash_id": "10f1f1e119e4b6b33f614201790dde89e33f82d811695bb61925c94146b1331e",
  "original_prompt": "Ingested from linux-6.6.14/mm/workingset.c",
  "human_readable_source": "\n \n\n#include <linux/memcontrol.h>\n#include <linux/mm_inline.h>\n#include <linux/writeback.h>\n#include <linux/shmem_fs.h>\n#include <linux/pagemap.h>\n#include <linux/atomic.h>\n#include <linux/module.h>\n#include <linux/swap.h>\n#include <linux/dax.h>\n#include <linux/fs.h>\n#include <linux/mm.h>\n\n \n\n#define WORKINGSET_SHIFT 1\n#define EVICTION_SHIFT\t((BITS_PER_LONG - BITS_PER_XA_VALUE) +\t\\\n\t\t\t WORKINGSET_SHIFT + NODES_SHIFT + \\\n\t\t\t MEM_CGROUP_ID_SHIFT)\n#define EVICTION_MASK\t(~0UL >> EVICTION_SHIFT)\n\n \nstatic unsigned int bucket_order __read_mostly;\n\nstatic void *pack_shadow(int memcgid, pg_data_t *pgdat, unsigned long eviction,\n\t\t\t bool workingset)\n{\n\teviction &= EVICTION_MASK;\n\teviction = (eviction << MEM_CGROUP_ID_SHIFT) | memcgid;\n\teviction = (eviction << NODES_SHIFT) | pgdat->node_id;\n\teviction = (eviction << WORKINGSET_SHIFT) | workingset;\n\n\treturn xa_mk_value(eviction);\n}\n\nstatic void unpack_shadow(void *shadow, int *memcgidp, pg_data_t **pgdat,\n\t\t\t  unsigned long *evictionp, bool *workingsetp)\n{\n\tunsigned long entry = xa_to_value(shadow);\n\tint memcgid, nid;\n\tbool workingset;\n\n\tworkingset = entry & ((1UL << WORKINGSET_SHIFT) - 1);\n\tentry >>= WORKINGSET_SHIFT;\n\tnid = entry & ((1UL << NODES_SHIFT) - 1);\n\tentry >>= NODES_SHIFT;\n\tmemcgid = entry & ((1UL << MEM_CGROUP_ID_SHIFT) - 1);\n\tentry >>= MEM_CGROUP_ID_SHIFT;\n\n\t*memcgidp = memcgid;\n\t*pgdat = NODE_DATA(nid);\n\t*evictionp = entry;\n\t*workingsetp = workingset;\n}\n\n#ifdef CONFIG_LRU_GEN\n\nstatic void *lru_gen_eviction(struct folio *folio)\n{\n\tint hist;\n\tunsigned long token;\n\tunsigned long min_seq;\n\tstruct lruvec *lruvec;\n\tstruct lru_gen_folio *lrugen;\n\tint type = folio_is_file_lru(folio);\n\tint delta = folio_nr_pages(folio);\n\tint refs = folio_lru_refs(folio);\n\tint tier = lru_tier_from_refs(refs);\n\tstruct mem_cgroup *memcg = folio_memcg(folio);\n\tstruct pglist_data *pgdat = folio_pgdat(folio);\n\n\tBUILD_BUG_ON(LRU_GEN_WIDTH + LRU_REFS_WIDTH > BITS_PER_LONG - EVICTION_SHIFT);\n\n\tlruvec = mem_cgroup_lruvec(memcg, pgdat);\n\tlrugen = &lruvec->lrugen;\n\tmin_seq = READ_ONCE(lrugen->min_seq[type]);\n\ttoken = (min_seq << LRU_REFS_WIDTH) | max(refs - 1, 0);\n\n\thist = lru_hist_from_seq(min_seq);\n\tatomic_long_add(delta, &lrugen->evicted[hist][type][tier]);\n\n\treturn pack_shadow(mem_cgroup_id(memcg), pgdat, token, refs);\n}\n\n \nstatic bool lru_gen_test_recent(void *shadow, bool file, struct lruvec **lruvec,\n\t\t\t\tunsigned long *token, bool *workingset)\n{\n\tint memcg_id;\n\tunsigned long min_seq;\n\tstruct mem_cgroup *memcg;\n\tstruct pglist_data *pgdat;\n\n\tunpack_shadow(shadow, &memcg_id, &pgdat, token, workingset);\n\n\tmemcg = mem_cgroup_from_id(memcg_id);\n\t*lruvec = mem_cgroup_lruvec(memcg, pgdat);\n\n\tmin_seq = READ_ONCE((*lruvec)->lrugen.min_seq[file]);\n\treturn (*token >> LRU_REFS_WIDTH) == (min_seq & (EVICTION_MASK >> LRU_REFS_WIDTH));\n}\n\nstatic void lru_gen_refault(struct folio *folio, void *shadow)\n{\n\tbool recent;\n\tint hist, tier, refs;\n\tbool workingset;\n\tunsigned long token;\n\tstruct lruvec *lruvec;\n\tstruct lru_gen_folio *lrugen;\n\tint type = folio_is_file_lru(folio);\n\tint delta = folio_nr_pages(folio);\n\n\trcu_read_lock();\n\n\trecent = lru_gen_test_recent(shadow, type, &lruvec, &token, &workingset);\n\tif (lruvec != folio_lruvec(folio))\n\t\tgoto unlock;\n\n\tmod_lruvec_state(lruvec, WORKINGSET_REFAULT_BASE + type, delta);\n\n\tif (!recent)\n\t\tgoto unlock;\n\n\tlrugen = &lruvec->lrugen;\n\n\thist = lru_hist_from_seq(READ_ONCE(lrugen->min_seq[type]));\n\t \n\trefs = (token & (BIT(LRU_REFS_WIDTH) - 1)) + workingset;\n\ttier = lru_tier_from_refs(refs);\n\n\tatomic_long_add(delta, &lrugen->refaulted[hist][type][tier]);\n\tmod_lruvec_state(lruvec, WORKINGSET_ACTIVATE_BASE + type, delta);\n\n\t \n\tif (lru_gen_in_fault() || refs >= BIT(LRU_REFS_WIDTH) - 1) {\n\t\tset_mask_bits(&folio->flags, 0, LRU_REFS_MASK | BIT(PG_workingset));\n\t\tmod_lruvec_state(lruvec, WORKINGSET_RESTORE_BASE + type, delta);\n\t}\nunlock:\n\trcu_read_unlock();\n}\n\n#else  \n\nstatic void *lru_gen_eviction(struct folio *folio)\n{\n\treturn NULL;\n}\n\nstatic bool lru_gen_test_recent(void *shadow, bool file, struct lruvec **lruvec,\n\t\t\t\tunsigned long *token, bool *workingset)\n{\n\treturn false;\n}\n\nstatic void lru_gen_refault(struct folio *folio, void *shadow)\n{\n}\n\n#endif  \n\n \nvoid workingset_age_nonresident(struct lruvec *lruvec, unsigned long nr_pages)\n{\n\t \n\tdo {\n\t\tatomic_long_add(nr_pages, &lruvec->nonresident_age);\n\t} while ((lruvec = parent_lruvec(lruvec)));\n}\n\n \nvoid *workingset_eviction(struct folio *folio, struct mem_cgroup *target_memcg)\n{\n\tstruct pglist_data *pgdat = folio_pgdat(folio);\n\tunsigned long eviction;\n\tstruct lruvec *lruvec;\n\tint memcgid;\n\n\t \n\tVM_BUG_ON_FOLIO(folio_test_lru(folio), folio);\n\tVM_BUG_ON_FOLIO(folio_ref_count(folio), folio);\n\tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n\n\tif (lru_gen_enabled())\n\t\treturn lru_gen_eviction(folio);\n\n\tlruvec = mem_cgroup_lruvec(target_memcg, pgdat);\n\t \n\tmemcgid = mem_cgroup_id(lruvec_memcg(lruvec));\n\teviction = atomic_long_read(&lruvec->nonresident_age);\n\teviction >>= bucket_order;\n\tworkingset_age_nonresident(lruvec, folio_nr_pages(folio));\n\treturn pack_shadow(memcgid, pgdat, eviction,\n\t\t\t\tfolio_test_workingset(folio));\n}\n\n \nbool workingset_test_recent(void *shadow, bool file, bool *workingset)\n{\n\tstruct mem_cgroup *eviction_memcg;\n\tstruct lruvec *eviction_lruvec;\n\tunsigned long refault_distance;\n\tunsigned long workingset_size;\n\tunsigned long refault;\n\tint memcgid;\n\tstruct pglist_data *pgdat;\n\tunsigned long eviction;\n\n\tif (lru_gen_enabled())\n\t\treturn lru_gen_test_recent(shadow, file, &eviction_lruvec, &eviction, workingset);\n\n\tunpack_shadow(shadow, &memcgid, &pgdat, &eviction, workingset);\n\teviction <<= bucket_order;\n\n\t \n\teviction_memcg = mem_cgroup_from_id(memcgid);\n\tif (!mem_cgroup_disabled() && !eviction_memcg)\n\t\treturn false;\n\n\teviction_lruvec = mem_cgroup_lruvec(eviction_memcg, pgdat);\n\trefault = atomic_long_read(&eviction_lruvec->nonresident_age);\n\n\t \n\trefault_distance = (refault - eviction) & EVICTION_MASK;\n\n\t \n\tworkingset_size = lruvec_page_state(eviction_lruvec, NR_ACTIVE_FILE);\n\tif (!file) {\n\t\tworkingset_size += lruvec_page_state(eviction_lruvec,\n\t\t\t\t\t\t     NR_INACTIVE_FILE);\n\t}\n\tif (mem_cgroup_get_nr_swap_pages(eviction_memcg) > 0) {\n\t\tworkingset_size += lruvec_page_state(eviction_lruvec,\n\t\t\t\t\t\t     NR_ACTIVE_ANON);\n\t\tif (file) {\n\t\t\tworkingset_size += lruvec_page_state(eviction_lruvec,\n\t\t\t\t\t\t     NR_INACTIVE_ANON);\n\t\t}\n\t}\n\n\treturn refault_distance <= workingset_size;\n}\n\n \nvoid workingset_refault(struct folio *folio, void *shadow)\n{\n\tbool file = folio_is_file_lru(folio);\n\tstruct pglist_data *pgdat;\n\tstruct mem_cgroup *memcg;\n\tstruct lruvec *lruvec;\n\tbool workingset;\n\tlong nr;\n\n\tif (lru_gen_enabled()) {\n\t\tlru_gen_refault(folio, shadow);\n\t\treturn;\n\t}\n\n\t \n\tmem_cgroup_flush_stats_ratelimited();\n\n\trcu_read_lock();\n\n\t \n\tnr = folio_nr_pages(folio);\n\tmemcg = folio_memcg(folio);\n\tpgdat = folio_pgdat(folio);\n\tlruvec = mem_cgroup_lruvec(memcg, pgdat);\n\n\tmod_lruvec_state(lruvec, WORKINGSET_REFAULT_BASE + file, nr);\n\n\tif (!workingset_test_recent(shadow, file, &workingset))\n\t\tgoto out;\n\n\tfolio_set_active(folio);\n\tworkingset_age_nonresident(lruvec, nr);\n\tmod_lruvec_state(lruvec, WORKINGSET_ACTIVATE_BASE + file, nr);\n\n\t \n\tif (workingset) {\n\t\tfolio_set_workingset(folio);\n\t\t \n\t\tlru_note_cost_refault(folio);\n\t\tmod_lruvec_state(lruvec, WORKINGSET_RESTORE_BASE + file, nr);\n\t}\nout:\n\trcu_read_unlock();\n}\n\n \nvoid workingset_activation(struct folio *folio)\n{\n\tstruct mem_cgroup *memcg;\n\n\trcu_read_lock();\n\t \n\tmemcg = folio_memcg_rcu(folio);\n\tif (!mem_cgroup_disabled() && !memcg)\n\t\tgoto out;\n\tworkingset_age_nonresident(folio_lruvec(folio), folio_nr_pages(folio));\nout:\n\trcu_read_unlock();\n}\n\n \n\nstruct list_lru shadow_nodes;\n\nvoid workingset_update_node(struct xa_node *node)\n{\n\tstruct address_space *mapping;\n\n\t \n\tmapping = container_of(node->array, struct address_space, i_pages);\n\tlockdep_assert_held(&mapping->i_pages.xa_lock);\n\n\tif (node->count && node->count == node->nr_values) {\n\t\tif (list_empty(&node->private_list)) {\n\t\t\tlist_lru_add(&shadow_nodes, &node->private_list);\n\t\t\t__inc_lruvec_kmem_state(node, WORKINGSET_NODES);\n\t\t}\n\t} else {\n\t\tif (!list_empty(&node->private_list)) {\n\t\t\tlist_lru_del(&shadow_nodes, &node->private_list);\n\t\t\t__dec_lruvec_kmem_state(node, WORKINGSET_NODES);\n\t\t}\n\t}\n}\n\nstatic unsigned long count_shadow_nodes(struct shrinker *shrinker,\n\t\t\t\t\tstruct shrink_control *sc)\n{\n\tunsigned long max_nodes;\n\tunsigned long nodes;\n\tunsigned long pages;\n\n\tnodes = list_lru_shrink_count(&shadow_nodes, sc);\n\tif (!nodes)\n\t\treturn SHRINK_EMPTY;\n\n\t \n#ifdef CONFIG_MEMCG\n\tif (sc->memcg) {\n\t\tstruct lruvec *lruvec;\n\t\tint i;\n\n\t\tmem_cgroup_flush_stats();\n\t\tlruvec = mem_cgroup_lruvec(sc->memcg, NODE_DATA(sc->nid));\n\t\tfor (pages = 0, i = 0; i < NR_LRU_LISTS; i++)\n\t\t\tpages += lruvec_page_state_local(lruvec,\n\t\t\t\t\t\t\t NR_LRU_BASE + i);\n\t\tpages += lruvec_page_state_local(\n\t\t\tlruvec, NR_SLAB_RECLAIMABLE_B) >> PAGE_SHIFT;\n\t\tpages += lruvec_page_state_local(\n\t\t\tlruvec, NR_SLAB_UNRECLAIMABLE_B) >> PAGE_SHIFT;\n\t} else\n#endif\n\t\tpages = node_present_pages(sc->nid);\n\n\tmax_nodes = pages >> (XA_CHUNK_SHIFT - 3);\n\n\tif (nodes <= max_nodes)\n\t\treturn 0;\n\treturn nodes - max_nodes;\n}\n\nstatic enum lru_status shadow_lru_isolate(struct list_head *item,\n\t\t\t\t\t  struct list_lru_one *lru,\n\t\t\t\t\t  spinlock_t *lru_lock,\n\t\t\t\t\t  void *arg) __must_hold(lru_lock)\n{\n\tstruct xa_node *node = container_of(item, struct xa_node, private_list);\n\tstruct address_space *mapping;\n\tint ret;\n\n\t \n\n\tmapping = container_of(node->array, struct address_space, i_pages);\n\n\t \n\tif (!xa_trylock(&mapping->i_pages)) {\n\t\tspin_unlock_irq(lru_lock);\n\t\tret = LRU_RETRY;\n\t\tgoto out;\n\t}\n\n\t \n\tif (mapping->host != NULL) {\n\t\tif (!spin_trylock(&mapping->host->i_lock)) {\n\t\t\txa_unlock(&mapping->i_pages);\n\t\t\tspin_unlock_irq(lru_lock);\n\t\t\tret = LRU_RETRY;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tlist_lru_isolate(lru, item);\n\t__dec_lruvec_kmem_state(node, WORKINGSET_NODES);\n\n\tspin_unlock(lru_lock);\n\n\t \n\tif (WARN_ON_ONCE(!node->nr_values))\n\t\tgoto out_invalid;\n\tif (WARN_ON_ONCE(node->count != node->nr_values))\n\t\tgoto out_invalid;\n\txa_delete_node(node, workingset_update_node);\n\t__inc_lruvec_kmem_state(node, WORKINGSET_NODERECLAIM);\n\nout_invalid:\n\txa_unlock_irq(&mapping->i_pages);\n\tif (mapping->host != NULL) {\n\t\tif (mapping_shrinkable(mapping))\n\t\t\tinode_add_lru(mapping->host);\n\t\tspin_unlock(&mapping->host->i_lock);\n\t}\n\tret = LRU_REMOVED_RETRY;\nout:\n\tcond_resched();\n\tspin_lock_irq(lru_lock);\n\treturn ret;\n}\n\nstatic unsigned long scan_shadow_nodes(struct shrinker *shrinker,\n\t\t\t\t       struct shrink_control *sc)\n{\n\t \n\treturn list_lru_shrink_walk_irq(&shadow_nodes, sc, shadow_lru_isolate,\n\t\t\t\t\tNULL);\n}\n\nstatic struct shrinker workingset_shadow_shrinker = {\n\t.count_objects = count_shadow_nodes,\n\t.scan_objects = scan_shadow_nodes,\n\t.seeks = 0,  \n\t.flags = SHRINKER_NUMA_AWARE | SHRINKER_MEMCG_AWARE,\n};\n\n \nstatic struct lock_class_key shadow_nodes_key;\n\nstatic int __init workingset_init(void)\n{\n\tunsigned int timestamp_bits;\n\tunsigned int max_order;\n\tint ret;\n\n\tBUILD_BUG_ON(BITS_PER_LONG < EVICTION_SHIFT);\n\t \n\ttimestamp_bits = BITS_PER_LONG - EVICTION_SHIFT;\n\tmax_order = fls_long(totalram_pages() - 1);\n\tif (max_order > timestamp_bits)\n\t\tbucket_order = max_order - timestamp_bits;\n\tpr_info(\"workingset: timestamp_bits=%d max_order=%d bucket_order=%u\\n\",\n\t       timestamp_bits, max_order, bucket_order);\n\n\tret = prealloc_shrinker(&workingset_shadow_shrinker, \"mm-shadow\");\n\tif (ret)\n\t\tgoto err;\n\tret = __list_lru_init(&shadow_nodes, true, &shadow_nodes_key,\n\t\t\t      &workingset_shadow_shrinker);\n\tif (ret)\n\t\tgoto err_list_lru;\n\tregister_shrinker_prepared(&workingset_shadow_shrinker);\n\treturn 0;\nerr_list_lru:\n\tfree_prealloced_shrinker(&workingset_shadow_shrinker);\nerr:\n\treturn ret;\n}\nmodule_init(workingset_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}