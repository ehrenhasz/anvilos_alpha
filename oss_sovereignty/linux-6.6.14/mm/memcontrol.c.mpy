{
  "module_name": "memcontrol.c",
  "hash_id": "22bbf1e65223d75b04b8b5aa3df251dbe116ddd0ca7443d1fca3a393e53cfd69",
  "original_prompt": "Ingested from linux-6.6.14/mm/memcontrol.c",
  "human_readable_source": "\n \n\n#include <linux/page_counter.h>\n#include <linux/memcontrol.h>\n#include <linux/cgroup.h>\n#include <linux/pagewalk.h>\n#include <linux/sched/mm.h>\n#include <linux/shmem_fs.h>\n#include <linux/hugetlb.h>\n#include <linux/pagemap.h>\n#include <linux/vm_event_item.h>\n#include <linux/smp.h>\n#include <linux/page-flags.h>\n#include <linux/backing-dev.h>\n#include <linux/bit_spinlock.h>\n#include <linux/rcupdate.h>\n#include <linux/limits.h>\n#include <linux/export.h>\n#include <linux/mutex.h>\n#include <linux/rbtree.h>\n#include <linux/slab.h>\n#include <linux/swap.h>\n#include <linux/swapops.h>\n#include <linux/spinlock.h>\n#include <linux/eventfd.h>\n#include <linux/poll.h>\n#include <linux/sort.h>\n#include <linux/fs.h>\n#include <linux/seq_file.h>\n#include <linux/vmpressure.h>\n#include <linux/memremap.h>\n#include <linux/mm_inline.h>\n#include <linux/swap_cgroup.h>\n#include <linux/cpu.h>\n#include <linux/oom.h>\n#include <linux/lockdep.h>\n#include <linux/file.h>\n#include <linux/resume_user_mode.h>\n#include <linux/psi.h>\n#include <linux/seq_buf.h>\n#include <linux/sched/isolation.h>\n#include \"internal.h\"\n#include <net/sock.h>\n#include <net/ip.h>\n#include \"slab.h\"\n#include \"swap.h\"\n\n#include <linux/uaccess.h>\n\n#include <trace/events/vmscan.h>\n\nstruct cgroup_subsys memory_cgrp_subsys __read_mostly;\nEXPORT_SYMBOL(memory_cgrp_subsys);\n\nstruct mem_cgroup *root_mem_cgroup __read_mostly;\n\n \nDEFINE_PER_CPU(struct mem_cgroup *, int_active_memcg);\nEXPORT_PER_CPU_SYMBOL_GPL(int_active_memcg);\n\n \nstatic bool cgroup_memory_nosocket __ro_after_init;\n\n \nstatic bool cgroup_memory_nokmem __ro_after_init;\n\n \nstatic bool cgroup_memory_nobpf __ro_after_init;\n\n#ifdef CONFIG_CGROUP_WRITEBACK\nstatic DECLARE_WAIT_QUEUE_HEAD(memcg_cgwb_frn_waitq);\n#endif\n\n \nstatic bool do_memsw_account(void)\n{\n\treturn !cgroup_subsys_on_dfl(memory_cgrp_subsys);\n}\n\n#define THRESHOLDS_EVENTS_TARGET 128\n#define SOFTLIMIT_EVENTS_TARGET 1024\n\n \n\nstruct mem_cgroup_tree_per_node {\n\tstruct rb_root rb_root;\n\tstruct rb_node *rb_rightmost;\n\tspinlock_t lock;\n};\n\nstruct mem_cgroup_tree {\n\tstruct mem_cgroup_tree_per_node *rb_tree_per_node[MAX_NUMNODES];\n};\n\nstatic struct mem_cgroup_tree soft_limit_tree __read_mostly;\n\n \nstruct mem_cgroup_eventfd_list {\n\tstruct list_head list;\n\tstruct eventfd_ctx *eventfd;\n};\n\n \nstruct mem_cgroup_event {\n\t \n\tstruct mem_cgroup *memcg;\n\t \n\tstruct eventfd_ctx *eventfd;\n\t \n\tstruct list_head list;\n\t \n\tint (*register_event)(struct mem_cgroup *memcg,\n\t\t\t      struct eventfd_ctx *eventfd, const char *args);\n\t \n\tvoid (*unregister_event)(struct mem_cgroup *memcg,\n\t\t\t\t struct eventfd_ctx *eventfd);\n\t \n\tpoll_table pt;\n\twait_queue_head_t *wqh;\n\twait_queue_entry_t wait;\n\tstruct work_struct remove;\n};\n\nstatic void mem_cgroup_threshold(struct mem_cgroup *memcg);\nstatic void mem_cgroup_oom_notify(struct mem_cgroup *memcg);\n\n \n \n#define MOVE_ANON\t0x1U\n#define MOVE_FILE\t0x2U\n#define MOVE_MASK\t(MOVE_ANON | MOVE_FILE)\n\n \nstatic struct move_charge_struct {\n\tspinlock_t\t  lock;  \n\tstruct mm_struct  *mm;\n\tstruct mem_cgroup *from;\n\tstruct mem_cgroup *to;\n\tunsigned long flags;\n\tunsigned long precharge;\n\tunsigned long moved_charge;\n\tunsigned long moved_swap;\n\tstruct task_struct *moving_task;\t \n\twait_queue_head_t waitq;\t\t \n} mc = {\n\t.lock = __SPIN_LOCK_UNLOCKED(mc.lock),\n\t.waitq = __WAIT_QUEUE_HEAD_INITIALIZER(mc.waitq),\n};\n\n \n#define\tMEM_CGROUP_MAX_RECLAIM_LOOPS\t\t100\n#define\tMEM_CGROUP_MAX_SOFT_LIMIT_RECLAIM_LOOPS\t2\n\n \nenum res_type {\n\t_MEM,\n\t_MEMSWAP,\n\t_KMEM,\n\t_TCP,\n};\n\n#define MEMFILE_PRIVATE(x, val)\t((x) << 16 | (val))\n#define MEMFILE_TYPE(val)\t((val) >> 16 & 0xffff)\n#define MEMFILE_ATTR(val)\t((val) & 0xffff)\n\n \n#define for_each_mem_cgroup_tree(iter, root)\t\t\\\n\tfor (iter = mem_cgroup_iter(root, NULL, NULL);\t\\\n\t     iter != NULL;\t\t\t\t\\\n\t     iter = mem_cgroup_iter(root, iter, NULL))\n\n#define for_each_mem_cgroup(iter)\t\t\t\\\n\tfor (iter = mem_cgroup_iter(NULL, NULL, NULL);\t\\\n\t     iter != NULL;\t\t\t\t\\\n\t     iter = mem_cgroup_iter(NULL, iter, NULL))\n\nstatic inline bool task_is_dying(void)\n{\n\treturn tsk_is_oom_victim(current) || fatal_signal_pending(current) ||\n\t\t(current->flags & PF_EXITING);\n}\n\n \nstruct vmpressure *memcg_to_vmpressure(struct mem_cgroup *memcg)\n{\n\tif (!memcg)\n\t\tmemcg = root_mem_cgroup;\n\treturn &memcg->vmpressure;\n}\n\nstruct mem_cgroup *vmpressure_to_memcg(struct vmpressure *vmpr)\n{\n\treturn container_of(vmpr, struct mem_cgroup, vmpressure);\n}\n\n#ifdef CONFIG_MEMCG_KMEM\nstatic DEFINE_SPINLOCK(objcg_lock);\n\nbool mem_cgroup_kmem_disabled(void)\n{\n\treturn cgroup_memory_nokmem;\n}\n\nstatic void obj_cgroup_uncharge_pages(struct obj_cgroup *objcg,\n\t\t\t\t      unsigned int nr_pages);\n\nstatic void obj_cgroup_release(struct percpu_ref *ref)\n{\n\tstruct obj_cgroup *objcg = container_of(ref, struct obj_cgroup, refcnt);\n\tunsigned int nr_bytes;\n\tunsigned int nr_pages;\n\tunsigned long flags;\n\n\t \n\tnr_bytes = atomic_read(&objcg->nr_charged_bytes);\n\tWARN_ON_ONCE(nr_bytes & (PAGE_SIZE - 1));\n\tnr_pages = nr_bytes >> PAGE_SHIFT;\n\n\tif (nr_pages)\n\t\tobj_cgroup_uncharge_pages(objcg, nr_pages);\n\n\tspin_lock_irqsave(&objcg_lock, flags);\n\tlist_del(&objcg->list);\n\tspin_unlock_irqrestore(&objcg_lock, flags);\n\n\tpercpu_ref_exit(ref);\n\tkfree_rcu(objcg, rcu);\n}\n\nstatic struct obj_cgroup *obj_cgroup_alloc(void)\n{\n\tstruct obj_cgroup *objcg;\n\tint ret;\n\n\tobjcg = kzalloc(sizeof(struct obj_cgroup), GFP_KERNEL);\n\tif (!objcg)\n\t\treturn NULL;\n\n\tret = percpu_ref_init(&objcg->refcnt, obj_cgroup_release, 0,\n\t\t\t      GFP_KERNEL);\n\tif (ret) {\n\t\tkfree(objcg);\n\t\treturn NULL;\n\t}\n\tINIT_LIST_HEAD(&objcg->list);\n\treturn objcg;\n}\n\nstatic void memcg_reparent_objcgs(struct mem_cgroup *memcg,\n\t\t\t\t  struct mem_cgroup *parent)\n{\n\tstruct obj_cgroup *objcg, *iter;\n\n\tobjcg = rcu_replace_pointer(memcg->objcg, NULL, true);\n\n\tspin_lock_irq(&objcg_lock);\n\n\t \n\tlist_add(&objcg->list, &memcg->objcg_list);\n\t \n\tlist_for_each_entry(iter, &memcg->objcg_list, list)\n\t\tWRITE_ONCE(iter->memcg, parent);\n\t \n\tlist_splice(&memcg->objcg_list, &parent->objcg_list);\n\n\tspin_unlock_irq(&objcg_lock);\n\n\tpercpu_ref_kill(&objcg->refcnt);\n}\n\n \nDEFINE_STATIC_KEY_FALSE(memcg_kmem_online_key);\nEXPORT_SYMBOL(memcg_kmem_online_key);\n\nDEFINE_STATIC_KEY_FALSE(memcg_bpf_enabled_key);\nEXPORT_SYMBOL(memcg_bpf_enabled_key);\n#endif\n\n \nstruct cgroup_subsys_state *mem_cgroup_css_from_folio(struct folio *folio)\n{\n\tstruct mem_cgroup *memcg = folio_memcg(folio);\n\n\tif (!memcg || !cgroup_subsys_on_dfl(memory_cgrp_subsys))\n\t\tmemcg = root_mem_cgroup;\n\n\treturn &memcg->css;\n}\n\n \nino_t page_cgroup_ino(struct page *page)\n{\n\tstruct mem_cgroup *memcg;\n\tunsigned long ino = 0;\n\n\trcu_read_lock();\n\t \n\tmemcg = folio_memcg_check(page_folio(page));\n\n\twhile (memcg && !(memcg->css.flags & CSS_ONLINE))\n\t\tmemcg = parent_mem_cgroup(memcg);\n\tif (memcg)\n\t\tino = cgroup_ino(memcg->css.cgroup);\n\trcu_read_unlock();\n\treturn ino;\n}\n\nstatic void __mem_cgroup_insert_exceeded(struct mem_cgroup_per_node *mz,\n\t\t\t\t\t struct mem_cgroup_tree_per_node *mctz,\n\t\t\t\t\t unsigned long new_usage_in_excess)\n{\n\tstruct rb_node **p = &mctz->rb_root.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct mem_cgroup_per_node *mz_node;\n\tbool rightmost = true;\n\n\tif (mz->on_tree)\n\t\treturn;\n\n\tmz->usage_in_excess = new_usage_in_excess;\n\tif (!mz->usage_in_excess)\n\t\treturn;\n\twhile (*p) {\n\t\tparent = *p;\n\t\tmz_node = rb_entry(parent, struct mem_cgroup_per_node,\n\t\t\t\t\ttree_node);\n\t\tif (mz->usage_in_excess < mz_node->usage_in_excess) {\n\t\t\tp = &(*p)->rb_left;\n\t\t\trightmost = false;\n\t\t} else {\n\t\t\tp = &(*p)->rb_right;\n\t\t}\n\t}\n\n\tif (rightmost)\n\t\tmctz->rb_rightmost = &mz->tree_node;\n\n\trb_link_node(&mz->tree_node, parent, p);\n\trb_insert_color(&mz->tree_node, &mctz->rb_root);\n\tmz->on_tree = true;\n}\n\nstatic void __mem_cgroup_remove_exceeded(struct mem_cgroup_per_node *mz,\n\t\t\t\t\t struct mem_cgroup_tree_per_node *mctz)\n{\n\tif (!mz->on_tree)\n\t\treturn;\n\n\tif (&mz->tree_node == mctz->rb_rightmost)\n\t\tmctz->rb_rightmost = rb_prev(&mz->tree_node);\n\n\trb_erase(&mz->tree_node, &mctz->rb_root);\n\tmz->on_tree = false;\n}\n\nstatic void mem_cgroup_remove_exceeded(struct mem_cgroup_per_node *mz,\n\t\t\t\t       struct mem_cgroup_tree_per_node *mctz)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&mctz->lock, flags);\n\t__mem_cgroup_remove_exceeded(mz, mctz);\n\tspin_unlock_irqrestore(&mctz->lock, flags);\n}\n\nstatic unsigned long soft_limit_excess(struct mem_cgroup *memcg)\n{\n\tunsigned long nr_pages = page_counter_read(&memcg->memory);\n\tunsigned long soft_limit = READ_ONCE(memcg->soft_limit);\n\tunsigned long excess = 0;\n\n\tif (nr_pages > soft_limit)\n\t\texcess = nr_pages - soft_limit;\n\n\treturn excess;\n}\n\nstatic void mem_cgroup_update_tree(struct mem_cgroup *memcg, int nid)\n{\n\tunsigned long excess;\n\tstruct mem_cgroup_per_node *mz;\n\tstruct mem_cgroup_tree_per_node *mctz;\n\n\tif (lru_gen_enabled()) {\n\t\tif (soft_limit_excess(memcg))\n\t\t\tlru_gen_soft_reclaim(memcg, nid);\n\t\treturn;\n\t}\n\n\tmctz = soft_limit_tree.rb_tree_per_node[nid];\n\tif (!mctz)\n\t\treturn;\n\t \n\tfor (; memcg; memcg = parent_mem_cgroup(memcg)) {\n\t\tmz = memcg->nodeinfo[nid];\n\t\texcess = soft_limit_excess(memcg);\n\t\t \n\t\tif (excess || mz->on_tree) {\n\t\t\tunsigned long flags;\n\n\t\t\tspin_lock_irqsave(&mctz->lock, flags);\n\t\t\t \n\t\t\tif (mz->on_tree)\n\t\t\t\t__mem_cgroup_remove_exceeded(mz, mctz);\n\t\t\t \n\t\t\t__mem_cgroup_insert_exceeded(mz, mctz, excess);\n\t\t\tspin_unlock_irqrestore(&mctz->lock, flags);\n\t\t}\n\t}\n}\n\nstatic void mem_cgroup_remove_from_trees(struct mem_cgroup *memcg)\n{\n\tstruct mem_cgroup_tree_per_node *mctz;\n\tstruct mem_cgroup_per_node *mz;\n\tint nid;\n\n\tfor_each_node(nid) {\n\t\tmz = memcg->nodeinfo[nid];\n\t\tmctz = soft_limit_tree.rb_tree_per_node[nid];\n\t\tif (mctz)\n\t\t\tmem_cgroup_remove_exceeded(mz, mctz);\n\t}\n}\n\nstatic struct mem_cgroup_per_node *\n__mem_cgroup_largest_soft_limit_node(struct mem_cgroup_tree_per_node *mctz)\n{\n\tstruct mem_cgroup_per_node *mz;\n\nretry:\n\tmz = NULL;\n\tif (!mctz->rb_rightmost)\n\t\tgoto done;\t\t \n\n\tmz = rb_entry(mctz->rb_rightmost,\n\t\t      struct mem_cgroup_per_node, tree_node);\n\t \n\t__mem_cgroup_remove_exceeded(mz, mctz);\n\tif (!soft_limit_excess(mz->memcg) ||\n\t    !css_tryget(&mz->memcg->css))\n\t\tgoto retry;\ndone:\n\treturn mz;\n}\n\nstatic struct mem_cgroup_per_node *\nmem_cgroup_largest_soft_limit_node(struct mem_cgroup_tree_per_node *mctz)\n{\n\tstruct mem_cgroup_per_node *mz;\n\n\tspin_lock_irq(&mctz->lock);\n\tmz = __mem_cgroup_largest_soft_limit_node(mctz);\n\tspin_unlock_irq(&mctz->lock);\n\treturn mz;\n}\n\n \nstatic void flush_memcg_stats_dwork(struct work_struct *w);\nstatic DECLARE_DEFERRABLE_WORK(stats_flush_dwork, flush_memcg_stats_dwork);\nstatic DEFINE_PER_CPU(unsigned int, stats_updates);\nstatic atomic_t stats_flush_ongoing = ATOMIC_INIT(0);\nstatic atomic_t stats_flush_threshold = ATOMIC_INIT(0);\nstatic u64 flush_next_time;\n\n#define FLUSH_TIME (2UL*HZ)\n\n \nstatic void memcg_stats_lock(void)\n{\n\tpreempt_disable_nested();\n\tVM_WARN_ON_IRQS_ENABLED();\n}\n\nstatic void __memcg_stats_lock(void)\n{\n\tpreempt_disable_nested();\n}\n\nstatic void memcg_stats_unlock(void)\n{\n\tpreempt_enable_nested();\n}\n\nstatic inline void memcg_rstat_updated(struct mem_cgroup *memcg, int val)\n{\n\tunsigned int x;\n\n\tif (!val)\n\t\treturn;\n\n\tcgroup_rstat_updated(memcg->css.cgroup, smp_processor_id());\n\n\tx = __this_cpu_add_return(stats_updates, abs(val));\n\tif (x > MEMCG_CHARGE_BATCH) {\n\t\t \n\t\tif (atomic_read(&stats_flush_threshold) <= num_online_cpus())\n\t\t\tatomic_add(x / MEMCG_CHARGE_BATCH, &stats_flush_threshold);\n\t\t__this_cpu_write(stats_updates, 0);\n\t}\n}\n\nstatic void do_flush_stats(void)\n{\n\t \n\tif (atomic_read(&stats_flush_ongoing) ||\n\t    atomic_xchg(&stats_flush_ongoing, 1))\n\t\treturn;\n\n\tWRITE_ONCE(flush_next_time, jiffies_64 + 2*FLUSH_TIME);\n\n\tcgroup_rstat_flush(root_mem_cgroup->css.cgroup);\n\n\tatomic_set(&stats_flush_threshold, 0);\n\tatomic_set(&stats_flush_ongoing, 0);\n}\n\nvoid mem_cgroup_flush_stats(void)\n{\n\tif (atomic_read(&stats_flush_threshold) > num_online_cpus())\n\t\tdo_flush_stats();\n}\n\nvoid mem_cgroup_flush_stats_ratelimited(void)\n{\n\tif (time_after64(jiffies_64, READ_ONCE(flush_next_time)))\n\t\tmem_cgroup_flush_stats();\n}\n\nstatic void flush_memcg_stats_dwork(struct work_struct *w)\n{\n\t \n\tdo_flush_stats();\n\tqueue_delayed_work(system_unbound_wq, &stats_flush_dwork, FLUSH_TIME);\n}\n\n \nstatic const unsigned int memcg_vm_event_stat[] = {\n\tPGPGIN,\n\tPGPGOUT,\n\tPGSCAN_KSWAPD,\n\tPGSCAN_DIRECT,\n\tPGSCAN_KHUGEPAGED,\n\tPGSTEAL_KSWAPD,\n\tPGSTEAL_DIRECT,\n\tPGSTEAL_KHUGEPAGED,\n\tPGFAULT,\n\tPGMAJFAULT,\n\tPGREFILL,\n\tPGACTIVATE,\n\tPGDEACTIVATE,\n\tPGLAZYFREE,\n\tPGLAZYFREED,\n#if defined(CONFIG_MEMCG_KMEM) && defined(CONFIG_ZSWAP)\n\tZSWPIN,\n\tZSWPOUT,\n#endif\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tTHP_FAULT_ALLOC,\n\tTHP_COLLAPSE_ALLOC,\n#endif\n};\n\n#define NR_MEMCG_EVENTS ARRAY_SIZE(memcg_vm_event_stat)\nstatic int mem_cgroup_events_index[NR_VM_EVENT_ITEMS] __read_mostly;\n\nstatic void init_memcg_events(void)\n{\n\tint i;\n\n\tfor (i = 0; i < NR_MEMCG_EVENTS; ++i)\n\t\tmem_cgroup_events_index[memcg_vm_event_stat[i]] = i + 1;\n}\n\nstatic inline int memcg_events_index(enum vm_event_item idx)\n{\n\treturn mem_cgroup_events_index[idx] - 1;\n}\n\nstruct memcg_vmstats_percpu {\n\t \n\tlong\t\t\tstate[MEMCG_NR_STAT];\n\tunsigned long\t\tevents[NR_MEMCG_EVENTS];\n\n\t \n\tlong\t\t\tstate_prev[MEMCG_NR_STAT];\n\tunsigned long\t\tevents_prev[NR_MEMCG_EVENTS];\n\n\t \n\tunsigned long\t\tnr_page_events;\n\tunsigned long\t\ttargets[MEM_CGROUP_NTARGETS];\n};\n\nstruct memcg_vmstats {\n\t \n\tlong\t\t\tstate[MEMCG_NR_STAT];\n\tunsigned long\t\tevents[NR_MEMCG_EVENTS];\n\n\t \n\tlong\t\t\tstate_local[MEMCG_NR_STAT];\n\tunsigned long\t\tevents_local[NR_MEMCG_EVENTS];\n\n\t \n\tlong\t\t\tstate_pending[MEMCG_NR_STAT];\n\tunsigned long\t\tevents_pending[NR_MEMCG_EVENTS];\n};\n\nunsigned long memcg_page_state(struct mem_cgroup *memcg, int idx)\n{\n\tlong x = READ_ONCE(memcg->vmstats->state[idx]);\n#ifdef CONFIG_SMP\n\tif (x < 0)\n\t\tx = 0;\n#endif\n\treturn x;\n}\n\n \nvoid __mod_memcg_state(struct mem_cgroup *memcg, int idx, int val)\n{\n\tif (mem_cgroup_disabled())\n\t\treturn;\n\n\t__this_cpu_add(memcg->vmstats_percpu->state[idx], val);\n\tmemcg_rstat_updated(memcg, val);\n}\n\n \nstatic unsigned long memcg_page_state_local(struct mem_cgroup *memcg, int idx)\n{\n\tlong x = READ_ONCE(memcg->vmstats->state_local[idx]);\n\n#ifdef CONFIG_SMP\n\tif (x < 0)\n\t\tx = 0;\n#endif\n\treturn x;\n}\n\nvoid __mod_memcg_lruvec_state(struct lruvec *lruvec, enum node_stat_item idx,\n\t\t\t      int val)\n{\n\tstruct mem_cgroup_per_node *pn;\n\tstruct mem_cgroup *memcg;\n\n\tpn = container_of(lruvec, struct mem_cgroup_per_node, lruvec);\n\tmemcg = pn->memcg;\n\n\t \n\t__memcg_stats_lock();\n\tif (IS_ENABLED(CONFIG_DEBUG_VM)) {\n\t\tswitch (idx) {\n\t\tcase NR_ANON_MAPPED:\n\t\tcase NR_FILE_MAPPED:\n\t\tcase NR_ANON_THPS:\n\t\tcase NR_SHMEM_PMDMAPPED:\n\t\tcase NR_FILE_PMDMAPPED:\n\t\t\tWARN_ON_ONCE(!in_task());\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tVM_WARN_ON_IRQS_ENABLED();\n\t\t}\n\t}\n\n\t \n\t__this_cpu_add(memcg->vmstats_percpu->state[idx], val);\n\n\t \n\t__this_cpu_add(pn->lruvec_stats_percpu->state[idx], val);\n\n\tmemcg_rstat_updated(memcg, val);\n\tmemcg_stats_unlock();\n}\n\n \nvoid __mod_lruvec_state(struct lruvec *lruvec, enum node_stat_item idx,\n\t\t\tint val)\n{\n\t \n\t__mod_node_page_state(lruvec_pgdat(lruvec), idx, val);\n\n\t \n\tif (!mem_cgroup_disabled())\n\t\t__mod_memcg_lruvec_state(lruvec, idx, val);\n}\n\nvoid __mod_lruvec_page_state(struct page *page, enum node_stat_item idx,\n\t\t\t     int val)\n{\n\tstruct page *head = compound_head(page);  \n\tstruct mem_cgroup *memcg;\n\tpg_data_t *pgdat = page_pgdat(page);\n\tstruct lruvec *lruvec;\n\n\trcu_read_lock();\n\tmemcg = page_memcg(head);\n\t \n\tif (!memcg) {\n\t\trcu_read_unlock();\n\t\t__mod_node_page_state(pgdat, idx, val);\n\t\treturn;\n\t}\n\n\tlruvec = mem_cgroup_lruvec(memcg, pgdat);\n\t__mod_lruvec_state(lruvec, idx, val);\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL(__mod_lruvec_page_state);\n\nvoid __mod_lruvec_kmem_state(void *p, enum node_stat_item idx, int val)\n{\n\tpg_data_t *pgdat = page_pgdat(virt_to_page(p));\n\tstruct mem_cgroup *memcg;\n\tstruct lruvec *lruvec;\n\n\trcu_read_lock();\n\tmemcg = mem_cgroup_from_slab_obj(p);\n\n\t \n\tif (!memcg) {\n\t\t__mod_node_page_state(pgdat, idx, val);\n\t} else {\n\t\tlruvec = mem_cgroup_lruvec(memcg, pgdat);\n\t\t__mod_lruvec_state(lruvec, idx, val);\n\t}\n\trcu_read_unlock();\n}\n\n \nvoid __count_memcg_events(struct mem_cgroup *memcg, enum vm_event_item idx,\n\t\t\t  unsigned long count)\n{\n\tint index = memcg_events_index(idx);\n\n\tif (mem_cgroup_disabled() || index < 0)\n\t\treturn;\n\n\tmemcg_stats_lock();\n\t__this_cpu_add(memcg->vmstats_percpu->events[index], count);\n\tmemcg_rstat_updated(memcg, count);\n\tmemcg_stats_unlock();\n}\n\nstatic unsigned long memcg_events(struct mem_cgroup *memcg, int event)\n{\n\tint index = memcg_events_index(event);\n\n\tif (index < 0)\n\t\treturn 0;\n\treturn READ_ONCE(memcg->vmstats->events[index]);\n}\n\nstatic unsigned long memcg_events_local(struct mem_cgroup *memcg, int event)\n{\n\tint index = memcg_events_index(event);\n\n\tif (index < 0)\n\t\treturn 0;\n\n\treturn READ_ONCE(memcg->vmstats->events_local[index]);\n}\n\nstatic void mem_cgroup_charge_statistics(struct mem_cgroup *memcg,\n\t\t\t\t\t int nr_pages)\n{\n\t \n\tif (nr_pages > 0)\n\t\t__count_memcg_events(memcg, PGPGIN, 1);\n\telse {\n\t\t__count_memcg_events(memcg, PGPGOUT, 1);\n\t\tnr_pages = -nr_pages;  \n\t}\n\n\t__this_cpu_add(memcg->vmstats_percpu->nr_page_events, nr_pages);\n}\n\nstatic bool mem_cgroup_event_ratelimit(struct mem_cgroup *memcg,\n\t\t\t\t       enum mem_cgroup_events_target target)\n{\n\tunsigned long val, next;\n\n\tval = __this_cpu_read(memcg->vmstats_percpu->nr_page_events);\n\tnext = __this_cpu_read(memcg->vmstats_percpu->targets[target]);\n\t \n\tif ((long)(next - val) < 0) {\n\t\tswitch (target) {\n\t\tcase MEM_CGROUP_TARGET_THRESH:\n\t\t\tnext = val + THRESHOLDS_EVENTS_TARGET;\n\t\t\tbreak;\n\t\tcase MEM_CGROUP_TARGET_SOFTLIMIT:\n\t\t\tnext = val + SOFTLIMIT_EVENTS_TARGET;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\t__this_cpu_write(memcg->vmstats_percpu->targets[target], next);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n \nstatic void memcg_check_events(struct mem_cgroup *memcg, int nid)\n{\n\tif (IS_ENABLED(CONFIG_PREEMPT_RT))\n\t\treturn;\n\n\t \n\tif (unlikely(mem_cgroup_event_ratelimit(memcg,\n\t\t\t\t\t\tMEM_CGROUP_TARGET_THRESH))) {\n\t\tbool do_softlimit;\n\n\t\tdo_softlimit = mem_cgroup_event_ratelimit(memcg,\n\t\t\t\t\t\tMEM_CGROUP_TARGET_SOFTLIMIT);\n\t\tmem_cgroup_threshold(memcg);\n\t\tif (unlikely(do_softlimit))\n\t\t\tmem_cgroup_update_tree(memcg, nid);\n\t}\n}\n\nstruct mem_cgroup *mem_cgroup_from_task(struct task_struct *p)\n{\n\t \n\tif (unlikely(!p))\n\t\treturn NULL;\n\n\treturn mem_cgroup_from_css(task_css(p, memory_cgrp_id));\n}\nEXPORT_SYMBOL(mem_cgroup_from_task);\n\nstatic __always_inline struct mem_cgroup *active_memcg(void)\n{\n\tif (!in_task())\n\t\treturn this_cpu_read(int_active_memcg);\n\telse\n\t\treturn current->active_memcg;\n}\n\n \nstruct mem_cgroup *get_mem_cgroup_from_mm(struct mm_struct *mm)\n{\n\tstruct mem_cgroup *memcg;\n\n\tif (mem_cgroup_disabled())\n\t\treturn NULL;\n\n\t \n\tif (unlikely(!mm)) {\n\t\tmemcg = active_memcg();\n\t\tif (unlikely(memcg)) {\n\t\t\t \n\t\t\tcss_get(&memcg->css);\n\t\t\treturn memcg;\n\t\t}\n\t\tmm = current->mm;\n\t\tif (unlikely(!mm))\n\t\t\treturn root_mem_cgroup;\n\t}\n\n\trcu_read_lock();\n\tdo {\n\t\tmemcg = mem_cgroup_from_task(rcu_dereference(mm->owner));\n\t\tif (unlikely(!memcg))\n\t\t\tmemcg = root_mem_cgroup;\n\t} while (!css_tryget(&memcg->css));\n\trcu_read_unlock();\n\treturn memcg;\n}\nEXPORT_SYMBOL(get_mem_cgroup_from_mm);\n\nstatic __always_inline bool memcg_kmem_bypass(void)\n{\n\t \n\tif (unlikely(active_memcg()))\n\t\treturn false;\n\n\t \n\tif (!in_task() || !current->mm || (current->flags & PF_KTHREAD))\n\t\treturn true;\n\n\treturn false;\n}\n\n \nstruct mem_cgroup *mem_cgroup_iter(struct mem_cgroup *root,\n\t\t\t\t   struct mem_cgroup *prev,\n\t\t\t\t   struct mem_cgroup_reclaim_cookie *reclaim)\n{\n\tstruct mem_cgroup_reclaim_iter *iter;\n\tstruct cgroup_subsys_state *css = NULL;\n\tstruct mem_cgroup *memcg = NULL;\n\tstruct mem_cgroup *pos = NULL;\n\n\tif (mem_cgroup_disabled())\n\t\treturn NULL;\n\n\tif (!root)\n\t\troot = root_mem_cgroup;\n\n\trcu_read_lock();\n\n\tif (reclaim) {\n\t\tstruct mem_cgroup_per_node *mz;\n\n\t\tmz = root->nodeinfo[reclaim->pgdat->node_id];\n\t\titer = &mz->iter;\n\n\t\t \n\t\tif (!prev)\n\t\t\treclaim->generation = iter->generation;\n\t\telse if (reclaim->generation != iter->generation)\n\t\t\tgoto out_unlock;\n\n\t\twhile (1) {\n\t\t\tpos = READ_ONCE(iter->position);\n\t\t\tif (!pos || css_tryget(&pos->css))\n\t\t\t\tbreak;\n\t\t\t \n\t\t\t(void)cmpxchg(&iter->position, pos, NULL);\n\t\t}\n\t} else if (prev) {\n\t\tpos = prev;\n\t}\n\n\tif (pos)\n\t\tcss = &pos->css;\n\n\tfor (;;) {\n\t\tcss = css_next_descendant_pre(css, &root->css);\n\t\tif (!css) {\n\t\t\t \n\t\t\tif (!prev)\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (css == &root->css || css_tryget(css)) {\n\t\t\tmemcg = mem_cgroup_from_css(css);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (reclaim) {\n\t\t \n\t\t(void)cmpxchg(&iter->position, pos, memcg);\n\n\t\tif (pos)\n\t\t\tcss_put(&pos->css);\n\n\t\tif (!memcg)\n\t\t\titer->generation++;\n\t}\n\nout_unlock:\n\trcu_read_unlock();\n\tif (prev && prev != root)\n\t\tcss_put(&prev->css);\n\n\treturn memcg;\n}\n\n \nvoid mem_cgroup_iter_break(struct mem_cgroup *root,\n\t\t\t   struct mem_cgroup *prev)\n{\n\tif (!root)\n\t\troot = root_mem_cgroup;\n\tif (prev && prev != root)\n\t\tcss_put(&prev->css);\n}\n\nstatic void __invalidate_reclaim_iterators(struct mem_cgroup *from,\n\t\t\t\t\tstruct mem_cgroup *dead_memcg)\n{\n\tstruct mem_cgroup_reclaim_iter *iter;\n\tstruct mem_cgroup_per_node *mz;\n\tint nid;\n\n\tfor_each_node(nid) {\n\t\tmz = from->nodeinfo[nid];\n\t\titer = &mz->iter;\n\t\tcmpxchg(&iter->position, dead_memcg, NULL);\n\t}\n}\n\nstatic void invalidate_reclaim_iterators(struct mem_cgroup *dead_memcg)\n{\n\tstruct mem_cgroup *memcg = dead_memcg;\n\tstruct mem_cgroup *last;\n\n\tdo {\n\t\t__invalidate_reclaim_iterators(memcg, dead_memcg);\n\t\tlast = memcg;\n\t} while ((memcg = parent_mem_cgroup(memcg)));\n\n\t \n\tif (!mem_cgroup_is_root(last))\n\t\t__invalidate_reclaim_iterators(root_mem_cgroup,\n\t\t\t\t\t\tdead_memcg);\n}\n\n \nvoid mem_cgroup_scan_tasks(struct mem_cgroup *memcg,\n\t\t\t   int (*fn)(struct task_struct *, void *), void *arg)\n{\n\tstruct mem_cgroup *iter;\n\tint ret = 0;\n\n\tBUG_ON(mem_cgroup_is_root(memcg));\n\n\tfor_each_mem_cgroup_tree(iter, memcg) {\n\t\tstruct css_task_iter it;\n\t\tstruct task_struct *task;\n\n\t\tcss_task_iter_start(&iter->css, CSS_TASK_ITER_PROCS, &it);\n\t\twhile (!ret && (task = css_task_iter_next(&it)))\n\t\t\tret = fn(task, arg);\n\t\tcss_task_iter_end(&it);\n\t\tif (ret) {\n\t\t\tmem_cgroup_iter_break(memcg, iter);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\n#ifdef CONFIG_DEBUG_VM\nvoid lruvec_memcg_debug(struct lruvec *lruvec, struct folio *folio)\n{\n\tstruct mem_cgroup *memcg;\n\n\tif (mem_cgroup_disabled())\n\t\treturn;\n\n\tmemcg = folio_memcg(folio);\n\n\tif (!memcg)\n\t\tVM_BUG_ON_FOLIO(!mem_cgroup_is_root(lruvec_memcg(lruvec)), folio);\n\telse\n\t\tVM_BUG_ON_FOLIO(lruvec_memcg(lruvec) != memcg, folio);\n}\n#endif\n\n \nstruct lruvec *folio_lruvec_lock(struct folio *folio)\n{\n\tstruct lruvec *lruvec = folio_lruvec(folio);\n\n\tspin_lock(&lruvec->lru_lock);\n\tlruvec_memcg_debug(lruvec, folio);\n\n\treturn lruvec;\n}\n\n \nstruct lruvec *folio_lruvec_lock_irq(struct folio *folio)\n{\n\tstruct lruvec *lruvec = folio_lruvec(folio);\n\n\tspin_lock_irq(&lruvec->lru_lock);\n\tlruvec_memcg_debug(lruvec, folio);\n\n\treturn lruvec;\n}\n\n \nstruct lruvec *folio_lruvec_lock_irqsave(struct folio *folio,\n\t\tunsigned long *flags)\n{\n\tstruct lruvec *lruvec = folio_lruvec(folio);\n\n\tspin_lock_irqsave(&lruvec->lru_lock, *flags);\n\tlruvec_memcg_debug(lruvec, folio);\n\n\treturn lruvec;\n}\n\n \nvoid mem_cgroup_update_lru_size(struct lruvec *lruvec, enum lru_list lru,\n\t\t\t\tint zid, int nr_pages)\n{\n\tstruct mem_cgroup_per_node *mz;\n\tunsigned long *lru_size;\n\tlong size;\n\n\tif (mem_cgroup_disabled())\n\t\treturn;\n\n\tmz = container_of(lruvec, struct mem_cgroup_per_node, lruvec);\n\tlru_size = &mz->lru_zone_size[zid][lru];\n\n\tif (nr_pages < 0)\n\t\t*lru_size += nr_pages;\n\n\tsize = *lru_size;\n\tif (WARN_ONCE(size < 0,\n\t\t\"%s(%p, %d, %d): lru_size %ld\\n\",\n\t\t__func__, lruvec, lru, nr_pages, size)) {\n\t\tVM_BUG_ON(1);\n\t\t*lru_size = 0;\n\t}\n\n\tif (nr_pages > 0)\n\t\t*lru_size += nr_pages;\n}\n\n \nstatic unsigned long mem_cgroup_margin(struct mem_cgroup *memcg)\n{\n\tunsigned long margin = 0;\n\tunsigned long count;\n\tunsigned long limit;\n\n\tcount = page_counter_read(&memcg->memory);\n\tlimit = READ_ONCE(memcg->memory.max);\n\tif (count < limit)\n\t\tmargin = limit - count;\n\n\tif (do_memsw_account()) {\n\t\tcount = page_counter_read(&memcg->memsw);\n\t\tlimit = READ_ONCE(memcg->memsw.max);\n\t\tif (count < limit)\n\t\t\tmargin = min(margin, limit - count);\n\t\telse\n\t\t\tmargin = 0;\n\t}\n\n\treturn margin;\n}\n\n \nstatic bool mem_cgroup_under_move(struct mem_cgroup *memcg)\n{\n\tstruct mem_cgroup *from;\n\tstruct mem_cgroup *to;\n\tbool ret = false;\n\t \n\tspin_lock(&mc.lock);\n\tfrom = mc.from;\n\tto = mc.to;\n\tif (!from)\n\t\tgoto unlock;\n\n\tret = mem_cgroup_is_descendant(from, memcg) ||\n\t\tmem_cgroup_is_descendant(to, memcg);\nunlock:\n\tspin_unlock(&mc.lock);\n\treturn ret;\n}\n\nstatic bool mem_cgroup_wait_acct_move(struct mem_cgroup *memcg)\n{\n\tif (mc.moving_task && current != mc.moving_task) {\n\t\tif (mem_cgroup_under_move(memcg)) {\n\t\t\tDEFINE_WAIT(wait);\n\t\t\tprepare_to_wait(&mc.waitq, &wait, TASK_INTERRUPTIBLE);\n\t\t\t \n\t\t\tif (mc.moving_task)\n\t\t\t\tschedule();\n\t\t\tfinish_wait(&mc.waitq, &wait);\n\t\t\treturn true;\n\t\t}\n\t}\n\treturn false;\n}\n\nstruct memory_stat {\n\tconst char *name;\n\tunsigned int idx;\n};\n\nstatic const struct memory_stat memory_stats[] = {\n\t{ \"anon\",\t\t\tNR_ANON_MAPPED\t\t\t},\n\t{ \"file\",\t\t\tNR_FILE_PAGES\t\t\t},\n\t{ \"kernel\",\t\t\tMEMCG_KMEM\t\t\t},\n\t{ \"kernel_stack\",\t\tNR_KERNEL_STACK_KB\t\t},\n\t{ \"pagetables\",\t\t\tNR_PAGETABLE\t\t\t},\n\t{ \"sec_pagetables\",\t\tNR_SECONDARY_PAGETABLE\t\t},\n\t{ \"percpu\",\t\t\tMEMCG_PERCPU_B\t\t\t},\n\t{ \"sock\",\t\t\tMEMCG_SOCK\t\t\t},\n\t{ \"vmalloc\",\t\t\tMEMCG_VMALLOC\t\t\t},\n\t{ \"shmem\",\t\t\tNR_SHMEM\t\t\t},\n#if defined(CONFIG_MEMCG_KMEM) && defined(CONFIG_ZSWAP)\n\t{ \"zswap\",\t\t\tMEMCG_ZSWAP_B\t\t\t},\n\t{ \"zswapped\",\t\t\tMEMCG_ZSWAPPED\t\t\t},\n#endif\n\t{ \"file_mapped\",\t\tNR_FILE_MAPPED\t\t\t},\n\t{ \"file_dirty\",\t\t\tNR_FILE_DIRTY\t\t\t},\n\t{ \"file_writeback\",\t\tNR_WRITEBACK\t\t\t},\n#ifdef CONFIG_SWAP\n\t{ \"swapcached\",\t\t\tNR_SWAPCACHE\t\t\t},\n#endif\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\t{ \"anon_thp\",\t\t\tNR_ANON_THPS\t\t\t},\n\t{ \"file_thp\",\t\t\tNR_FILE_THPS\t\t\t},\n\t{ \"shmem_thp\",\t\t\tNR_SHMEM_THPS\t\t\t},\n#endif\n\t{ \"inactive_anon\",\t\tNR_INACTIVE_ANON\t\t},\n\t{ \"active_anon\",\t\tNR_ACTIVE_ANON\t\t\t},\n\t{ \"inactive_file\",\t\tNR_INACTIVE_FILE\t\t},\n\t{ \"active_file\",\t\tNR_ACTIVE_FILE\t\t\t},\n\t{ \"unevictable\",\t\tNR_UNEVICTABLE\t\t\t},\n\t{ \"slab_reclaimable\",\t\tNR_SLAB_RECLAIMABLE_B\t\t},\n\t{ \"slab_unreclaimable\",\t\tNR_SLAB_UNRECLAIMABLE_B\t\t},\n\n\t \n\t{ \"workingset_refault_anon\",\tWORKINGSET_REFAULT_ANON\t\t},\n\t{ \"workingset_refault_file\",\tWORKINGSET_REFAULT_FILE\t\t},\n\t{ \"workingset_activate_anon\",\tWORKINGSET_ACTIVATE_ANON\t},\n\t{ \"workingset_activate_file\",\tWORKINGSET_ACTIVATE_FILE\t},\n\t{ \"workingset_restore_anon\",\tWORKINGSET_RESTORE_ANON\t\t},\n\t{ \"workingset_restore_file\",\tWORKINGSET_RESTORE_FILE\t\t},\n\t{ \"workingset_nodereclaim\",\tWORKINGSET_NODERECLAIM\t\t},\n};\n\n \nstatic int memcg_page_state_unit(int item)\n{\n\tswitch (item) {\n\tcase MEMCG_PERCPU_B:\n\tcase MEMCG_ZSWAP_B:\n\tcase NR_SLAB_RECLAIMABLE_B:\n\tcase NR_SLAB_UNRECLAIMABLE_B:\n\tcase WORKINGSET_REFAULT_ANON:\n\tcase WORKINGSET_REFAULT_FILE:\n\tcase WORKINGSET_ACTIVATE_ANON:\n\tcase WORKINGSET_ACTIVATE_FILE:\n\tcase WORKINGSET_RESTORE_ANON:\n\tcase WORKINGSET_RESTORE_FILE:\n\tcase WORKINGSET_NODERECLAIM:\n\t\treturn 1;\n\tcase NR_KERNEL_STACK_KB:\n\t\treturn SZ_1K;\n\tdefault:\n\t\treturn PAGE_SIZE;\n\t}\n}\n\nstatic inline unsigned long memcg_page_state_output(struct mem_cgroup *memcg,\n\t\t\t\t\t\t    int item)\n{\n\treturn memcg_page_state(memcg, item) * memcg_page_state_unit(item);\n}\n\nstatic void memcg_stat_format(struct mem_cgroup *memcg, struct seq_buf *s)\n{\n\tint i;\n\n\t \n\tmem_cgroup_flush_stats();\n\n\tfor (i = 0; i < ARRAY_SIZE(memory_stats); i++) {\n\t\tu64 size;\n\n\t\tsize = memcg_page_state_output(memcg, memory_stats[i].idx);\n\t\tseq_buf_printf(s, \"%s %llu\\n\", memory_stats[i].name, size);\n\n\t\tif (unlikely(memory_stats[i].idx == NR_SLAB_UNRECLAIMABLE_B)) {\n\t\t\tsize += memcg_page_state_output(memcg,\n\t\t\t\t\t\t\tNR_SLAB_RECLAIMABLE_B);\n\t\t\tseq_buf_printf(s, \"slab %llu\\n\", size);\n\t\t}\n\t}\n\n\t \n\tseq_buf_printf(s, \"pgscan %lu\\n\",\n\t\t       memcg_events(memcg, PGSCAN_KSWAPD) +\n\t\t       memcg_events(memcg, PGSCAN_DIRECT) +\n\t\t       memcg_events(memcg, PGSCAN_KHUGEPAGED));\n\tseq_buf_printf(s, \"pgsteal %lu\\n\",\n\t\t       memcg_events(memcg, PGSTEAL_KSWAPD) +\n\t\t       memcg_events(memcg, PGSTEAL_DIRECT) +\n\t\t       memcg_events(memcg, PGSTEAL_KHUGEPAGED));\n\n\tfor (i = 0; i < ARRAY_SIZE(memcg_vm_event_stat); i++) {\n\t\tif (memcg_vm_event_stat[i] == PGPGIN ||\n\t\t    memcg_vm_event_stat[i] == PGPGOUT)\n\t\t\tcontinue;\n\n\t\tseq_buf_printf(s, \"%s %lu\\n\",\n\t\t\t       vm_event_name(memcg_vm_event_stat[i]),\n\t\t\t       memcg_events(memcg, memcg_vm_event_stat[i]));\n\t}\n\n\t \n\tWARN_ON_ONCE(seq_buf_has_overflowed(s));\n}\n\nstatic void memcg1_stat_format(struct mem_cgroup *memcg, struct seq_buf *s);\n\nstatic void memory_stat_format(struct mem_cgroup *memcg, struct seq_buf *s)\n{\n\tif (cgroup_subsys_on_dfl(memory_cgrp_subsys))\n\t\tmemcg_stat_format(memcg, s);\n\telse\n\t\tmemcg1_stat_format(memcg, s);\n\tWARN_ON_ONCE(seq_buf_has_overflowed(s));\n}\n\n \nvoid mem_cgroup_print_oom_context(struct mem_cgroup *memcg, struct task_struct *p)\n{\n\trcu_read_lock();\n\n\tif (memcg) {\n\t\tpr_cont(\",oom_memcg=\");\n\t\tpr_cont_cgroup_path(memcg->css.cgroup);\n\t} else\n\t\tpr_cont(\",global_oom\");\n\tif (p) {\n\t\tpr_cont(\",task_memcg=\");\n\t\tpr_cont_cgroup_path(task_cgroup(p, memory_cgrp_id));\n\t}\n\trcu_read_unlock();\n}\n\n \nvoid mem_cgroup_print_oom_meminfo(struct mem_cgroup *memcg)\n{\n\t \n\tstatic char buf[PAGE_SIZE];\n\tstruct seq_buf s;\n\n\tlockdep_assert_held(&oom_lock);\n\n\tpr_info(\"memory: usage %llukB, limit %llukB, failcnt %lu\\n\",\n\t\tK((u64)page_counter_read(&memcg->memory)),\n\t\tK((u64)READ_ONCE(memcg->memory.max)), memcg->memory.failcnt);\n\tif (cgroup_subsys_on_dfl(memory_cgrp_subsys))\n\t\tpr_info(\"swap: usage %llukB, limit %llukB, failcnt %lu\\n\",\n\t\t\tK((u64)page_counter_read(&memcg->swap)),\n\t\t\tK((u64)READ_ONCE(memcg->swap.max)), memcg->swap.failcnt);\n\telse {\n\t\tpr_info(\"memory+swap: usage %llukB, limit %llukB, failcnt %lu\\n\",\n\t\t\tK((u64)page_counter_read(&memcg->memsw)),\n\t\t\tK((u64)memcg->memsw.max), memcg->memsw.failcnt);\n\t\tpr_info(\"kmem: usage %llukB, limit %llukB, failcnt %lu\\n\",\n\t\t\tK((u64)page_counter_read(&memcg->kmem)),\n\t\t\tK((u64)memcg->kmem.max), memcg->kmem.failcnt);\n\t}\n\n\tpr_info(\"Memory cgroup stats for \");\n\tpr_cont_cgroup_path(memcg->css.cgroup);\n\tpr_cont(\":\");\n\tseq_buf_init(&s, buf, sizeof(buf));\n\tmemory_stat_format(memcg, &s);\n\tseq_buf_do_printk(&s, KERN_INFO);\n}\n\n \nunsigned long mem_cgroup_get_max(struct mem_cgroup *memcg)\n{\n\tunsigned long max = READ_ONCE(memcg->memory.max);\n\n\tif (do_memsw_account()) {\n\t\tif (mem_cgroup_swappiness(memcg)) {\n\t\t\t \n\t\t\tunsigned long swap = READ_ONCE(memcg->memsw.max) - max;\n\n\t\t\tmax += min(swap, (unsigned long)total_swap_pages);\n\t\t}\n\t} else {\n\t\tif (mem_cgroup_swappiness(memcg))\n\t\t\tmax += min(READ_ONCE(memcg->swap.max),\n\t\t\t\t   (unsigned long)total_swap_pages);\n\t}\n\treturn max;\n}\n\nunsigned long mem_cgroup_size(struct mem_cgroup *memcg)\n{\n\treturn page_counter_read(&memcg->memory);\n}\n\nstatic bool mem_cgroup_out_of_memory(struct mem_cgroup *memcg, gfp_t gfp_mask,\n\t\t\t\t     int order)\n{\n\tstruct oom_control oc = {\n\t\t.zonelist = NULL,\n\t\t.nodemask = NULL,\n\t\t.memcg = memcg,\n\t\t.gfp_mask = gfp_mask,\n\t\t.order = order,\n\t};\n\tbool ret = true;\n\n\tif (mutex_lock_killable(&oom_lock))\n\t\treturn true;\n\n\tif (mem_cgroup_margin(memcg) >= (1 << order))\n\t\tgoto unlock;\n\n\t \n\tret = task_is_dying() || out_of_memory(&oc);\n\nunlock:\n\tmutex_unlock(&oom_lock);\n\treturn ret;\n}\n\nstatic int mem_cgroup_soft_reclaim(struct mem_cgroup *root_memcg,\n\t\t\t\t   pg_data_t *pgdat,\n\t\t\t\t   gfp_t gfp_mask,\n\t\t\t\t   unsigned long *total_scanned)\n{\n\tstruct mem_cgroup *victim = NULL;\n\tint total = 0;\n\tint loop = 0;\n\tunsigned long excess;\n\tunsigned long nr_scanned;\n\tstruct mem_cgroup_reclaim_cookie reclaim = {\n\t\t.pgdat = pgdat,\n\t};\n\n\texcess = soft_limit_excess(root_memcg);\n\n\twhile (1) {\n\t\tvictim = mem_cgroup_iter(root_memcg, victim, &reclaim);\n\t\tif (!victim) {\n\t\t\tloop++;\n\t\t\tif (loop >= 2) {\n\t\t\t\t \n\t\t\t\tif (!total)\n\t\t\t\t\tbreak;\n\t\t\t\t \n\t\t\t\tif (total >= (excess >> 2) ||\n\t\t\t\t\t(loop > MEM_CGROUP_MAX_RECLAIM_LOOPS))\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\t\ttotal += mem_cgroup_shrink_node(victim, gfp_mask, false,\n\t\t\t\t\tpgdat, &nr_scanned);\n\t\t*total_scanned += nr_scanned;\n\t\tif (!soft_limit_excess(root_memcg))\n\t\t\tbreak;\n\t}\n\tmem_cgroup_iter_break(root_memcg, victim);\n\treturn total;\n}\n\n#ifdef CONFIG_LOCKDEP\nstatic struct lockdep_map memcg_oom_lock_dep_map = {\n\t.name = \"memcg_oom_lock\",\n};\n#endif\n\nstatic DEFINE_SPINLOCK(memcg_oom_lock);\n\n \nstatic bool mem_cgroup_oom_trylock(struct mem_cgroup *memcg)\n{\n\tstruct mem_cgroup *iter, *failed = NULL;\n\n\tspin_lock(&memcg_oom_lock);\n\n\tfor_each_mem_cgroup_tree(iter, memcg) {\n\t\tif (iter->oom_lock) {\n\t\t\t \n\t\t\tfailed = iter;\n\t\t\tmem_cgroup_iter_break(memcg, iter);\n\t\t\tbreak;\n\t\t} else\n\t\t\titer->oom_lock = true;\n\t}\n\n\tif (failed) {\n\t\t \n\t\tfor_each_mem_cgroup_tree(iter, memcg) {\n\t\t\tif (iter == failed) {\n\t\t\t\tmem_cgroup_iter_break(memcg, iter);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\titer->oom_lock = false;\n\t\t}\n\t} else\n\t\tmutex_acquire(&memcg_oom_lock_dep_map, 0, 1, _RET_IP_);\n\n\tspin_unlock(&memcg_oom_lock);\n\n\treturn !failed;\n}\n\nstatic void mem_cgroup_oom_unlock(struct mem_cgroup *memcg)\n{\n\tstruct mem_cgroup *iter;\n\n\tspin_lock(&memcg_oom_lock);\n\tmutex_release(&memcg_oom_lock_dep_map, _RET_IP_);\n\tfor_each_mem_cgroup_tree(iter, memcg)\n\t\titer->oom_lock = false;\n\tspin_unlock(&memcg_oom_lock);\n}\n\nstatic void mem_cgroup_mark_under_oom(struct mem_cgroup *memcg)\n{\n\tstruct mem_cgroup *iter;\n\n\tspin_lock(&memcg_oom_lock);\n\tfor_each_mem_cgroup_tree(iter, memcg)\n\t\titer->under_oom++;\n\tspin_unlock(&memcg_oom_lock);\n}\n\nstatic void mem_cgroup_unmark_under_oom(struct mem_cgroup *memcg)\n{\n\tstruct mem_cgroup *iter;\n\n\t \n\tspin_lock(&memcg_oom_lock);\n\tfor_each_mem_cgroup_tree(iter, memcg)\n\t\tif (iter->under_oom > 0)\n\t\t\titer->under_oom--;\n\tspin_unlock(&memcg_oom_lock);\n}\n\nstatic DECLARE_WAIT_QUEUE_HEAD(memcg_oom_waitq);\n\nstruct oom_wait_info {\n\tstruct mem_cgroup *memcg;\n\twait_queue_entry_t\twait;\n};\n\nstatic int memcg_oom_wake_function(wait_queue_entry_t *wait,\n\tunsigned mode, int sync, void *arg)\n{\n\tstruct mem_cgroup *wake_memcg = (struct mem_cgroup *)arg;\n\tstruct mem_cgroup *oom_wait_memcg;\n\tstruct oom_wait_info *oom_wait_info;\n\n\toom_wait_info = container_of(wait, struct oom_wait_info, wait);\n\toom_wait_memcg = oom_wait_info->memcg;\n\n\tif (!mem_cgroup_is_descendant(wake_memcg, oom_wait_memcg) &&\n\t    !mem_cgroup_is_descendant(oom_wait_memcg, wake_memcg))\n\t\treturn 0;\n\treturn autoremove_wake_function(wait, mode, sync, arg);\n}\n\nstatic void memcg_oom_recover(struct mem_cgroup *memcg)\n{\n\t \n\tif (memcg && memcg->under_oom)\n\t\t__wake_up(&memcg_oom_waitq, TASK_NORMAL, 0, memcg);\n}\n\n \nstatic bool mem_cgroup_oom(struct mem_cgroup *memcg, gfp_t mask, int order)\n{\n\tbool locked, ret;\n\n\tif (order > PAGE_ALLOC_COSTLY_ORDER)\n\t\treturn false;\n\n\tmemcg_memory_event(memcg, MEMCG_OOM);\n\n\t \n\tif (READ_ONCE(memcg->oom_kill_disable)) {\n\t\tif (current->in_user_fault) {\n\t\t\tcss_get(&memcg->css);\n\t\t\tcurrent->memcg_in_oom = memcg;\n\t\t\tcurrent->memcg_oom_gfp_mask = mask;\n\t\t\tcurrent->memcg_oom_order = order;\n\t\t}\n\t\treturn false;\n\t}\n\n\tmem_cgroup_mark_under_oom(memcg);\n\n\tlocked = mem_cgroup_oom_trylock(memcg);\n\n\tif (locked)\n\t\tmem_cgroup_oom_notify(memcg);\n\n\tmem_cgroup_unmark_under_oom(memcg);\n\tret = mem_cgroup_out_of_memory(memcg, mask, order);\n\n\tif (locked)\n\t\tmem_cgroup_oom_unlock(memcg);\n\n\treturn ret;\n}\n\n \nbool mem_cgroup_oom_synchronize(bool handle)\n{\n\tstruct mem_cgroup *memcg = current->memcg_in_oom;\n\tstruct oom_wait_info owait;\n\tbool locked;\n\n\t \n\tif (!memcg)\n\t\treturn false;\n\n\tif (!handle)\n\t\tgoto cleanup;\n\n\towait.memcg = memcg;\n\towait.wait.flags = 0;\n\towait.wait.func = memcg_oom_wake_function;\n\towait.wait.private = current;\n\tINIT_LIST_HEAD(&owait.wait.entry);\n\n\tprepare_to_wait(&memcg_oom_waitq, &owait.wait, TASK_KILLABLE);\n\tmem_cgroup_mark_under_oom(memcg);\n\n\tlocked = mem_cgroup_oom_trylock(memcg);\n\n\tif (locked)\n\t\tmem_cgroup_oom_notify(memcg);\n\n\tschedule();\n\tmem_cgroup_unmark_under_oom(memcg);\n\tfinish_wait(&memcg_oom_waitq, &owait.wait);\n\n\tif (locked)\n\t\tmem_cgroup_oom_unlock(memcg);\ncleanup:\n\tcurrent->memcg_in_oom = NULL;\n\tcss_put(&memcg->css);\n\treturn true;\n}\n\n \nstruct mem_cgroup *mem_cgroup_get_oom_group(struct task_struct *victim,\n\t\t\t\t\t    struct mem_cgroup *oom_domain)\n{\n\tstruct mem_cgroup *oom_group = NULL;\n\tstruct mem_cgroup *memcg;\n\n\tif (!cgroup_subsys_on_dfl(memory_cgrp_subsys))\n\t\treturn NULL;\n\n\tif (!oom_domain)\n\t\toom_domain = root_mem_cgroup;\n\n\trcu_read_lock();\n\n\tmemcg = mem_cgroup_from_task(victim);\n\tif (mem_cgroup_is_root(memcg))\n\t\tgoto out;\n\n\t \n\tif (unlikely(!mem_cgroup_is_descendant(memcg, oom_domain)))\n\t\tgoto out;\n\n\t \n\tfor (; memcg; memcg = parent_mem_cgroup(memcg)) {\n\t\tif (READ_ONCE(memcg->oom_group))\n\t\t\toom_group = memcg;\n\n\t\tif (memcg == oom_domain)\n\t\t\tbreak;\n\t}\n\n\tif (oom_group)\n\t\tcss_get(&oom_group->css);\nout:\n\trcu_read_unlock();\n\n\treturn oom_group;\n}\n\nvoid mem_cgroup_print_oom_group(struct mem_cgroup *memcg)\n{\n\tpr_info(\"Tasks in \");\n\tpr_cont_cgroup_path(memcg->css.cgroup);\n\tpr_cont(\" are going to be killed due to memory.oom.group set\\n\");\n}\n\n \nvoid folio_memcg_lock(struct folio *folio)\n{\n\tstruct mem_cgroup *memcg;\n\tunsigned long flags;\n\n\t \n\trcu_read_lock();\n\n\tif (mem_cgroup_disabled())\n\t\treturn;\nagain:\n\tmemcg = folio_memcg(folio);\n\tif (unlikely(!memcg))\n\t\treturn;\n\n#ifdef CONFIG_PROVE_LOCKING\n\tlocal_irq_save(flags);\n\tmight_lock(&memcg->move_lock);\n\tlocal_irq_restore(flags);\n#endif\n\n\tif (atomic_read(&memcg->moving_account) <= 0)\n\t\treturn;\n\n\tspin_lock_irqsave(&memcg->move_lock, flags);\n\tif (memcg != folio_memcg(folio)) {\n\t\tspin_unlock_irqrestore(&memcg->move_lock, flags);\n\t\tgoto again;\n\t}\n\n\t \n\tmemcg->move_lock_task = current;\n\tmemcg->move_lock_flags = flags;\n}\n\nstatic void __folio_memcg_unlock(struct mem_cgroup *memcg)\n{\n\tif (memcg && memcg->move_lock_task == current) {\n\t\tunsigned long flags = memcg->move_lock_flags;\n\n\t\tmemcg->move_lock_task = NULL;\n\t\tmemcg->move_lock_flags = 0;\n\n\t\tspin_unlock_irqrestore(&memcg->move_lock, flags);\n\t}\n\n\trcu_read_unlock();\n}\n\n \nvoid folio_memcg_unlock(struct folio *folio)\n{\n\t__folio_memcg_unlock(folio_memcg(folio));\n}\n\nstruct memcg_stock_pcp {\n\tlocal_lock_t stock_lock;\n\tstruct mem_cgroup *cached;  \n\tunsigned int nr_pages;\n\n#ifdef CONFIG_MEMCG_KMEM\n\tstruct obj_cgroup *cached_objcg;\n\tstruct pglist_data *cached_pgdat;\n\tunsigned int nr_bytes;\n\tint nr_slab_reclaimable_b;\n\tint nr_slab_unreclaimable_b;\n#endif\n\n\tstruct work_struct work;\n\tunsigned long flags;\n#define FLUSHING_CACHED_CHARGE\t0\n};\nstatic DEFINE_PER_CPU(struct memcg_stock_pcp, memcg_stock) = {\n\t.stock_lock = INIT_LOCAL_LOCK(stock_lock),\n};\nstatic DEFINE_MUTEX(percpu_charge_mutex);\n\n#ifdef CONFIG_MEMCG_KMEM\nstatic struct obj_cgroup *drain_obj_stock(struct memcg_stock_pcp *stock);\nstatic bool obj_stock_flush_required(struct memcg_stock_pcp *stock,\n\t\t\t\t     struct mem_cgroup *root_memcg);\nstatic void memcg_account_kmem(struct mem_cgroup *memcg, int nr_pages);\n\n#else\nstatic inline struct obj_cgroup *drain_obj_stock(struct memcg_stock_pcp *stock)\n{\n\treturn NULL;\n}\nstatic bool obj_stock_flush_required(struct memcg_stock_pcp *stock,\n\t\t\t\t     struct mem_cgroup *root_memcg)\n{\n\treturn false;\n}\nstatic void memcg_account_kmem(struct mem_cgroup *memcg, int nr_pages)\n{\n}\n#endif\n\n \nstatic bool consume_stock(struct mem_cgroup *memcg, unsigned int nr_pages)\n{\n\tstruct memcg_stock_pcp *stock;\n\tunsigned long flags;\n\tbool ret = false;\n\n\tif (nr_pages > MEMCG_CHARGE_BATCH)\n\t\treturn ret;\n\n\tlocal_lock_irqsave(&memcg_stock.stock_lock, flags);\n\n\tstock = this_cpu_ptr(&memcg_stock);\n\tif (memcg == READ_ONCE(stock->cached) && stock->nr_pages >= nr_pages) {\n\t\tstock->nr_pages -= nr_pages;\n\t\tret = true;\n\t}\n\n\tlocal_unlock_irqrestore(&memcg_stock.stock_lock, flags);\n\n\treturn ret;\n}\n\n \nstatic void drain_stock(struct memcg_stock_pcp *stock)\n{\n\tstruct mem_cgroup *old = READ_ONCE(stock->cached);\n\n\tif (!old)\n\t\treturn;\n\n\tif (stock->nr_pages) {\n\t\tpage_counter_uncharge(&old->memory, stock->nr_pages);\n\t\tif (do_memsw_account())\n\t\t\tpage_counter_uncharge(&old->memsw, stock->nr_pages);\n\t\tstock->nr_pages = 0;\n\t}\n\n\tcss_put(&old->css);\n\tWRITE_ONCE(stock->cached, NULL);\n}\n\nstatic void drain_local_stock(struct work_struct *dummy)\n{\n\tstruct memcg_stock_pcp *stock;\n\tstruct obj_cgroup *old = NULL;\n\tunsigned long flags;\n\n\t \n\tlocal_lock_irqsave(&memcg_stock.stock_lock, flags);\n\n\tstock = this_cpu_ptr(&memcg_stock);\n\told = drain_obj_stock(stock);\n\tdrain_stock(stock);\n\tclear_bit(FLUSHING_CACHED_CHARGE, &stock->flags);\n\n\tlocal_unlock_irqrestore(&memcg_stock.stock_lock, flags);\n\tif (old)\n\t\tobj_cgroup_put(old);\n}\n\n \nstatic void __refill_stock(struct mem_cgroup *memcg, unsigned int nr_pages)\n{\n\tstruct memcg_stock_pcp *stock;\n\n\tstock = this_cpu_ptr(&memcg_stock);\n\tif (READ_ONCE(stock->cached) != memcg) {  \n\t\tdrain_stock(stock);\n\t\tcss_get(&memcg->css);\n\t\tWRITE_ONCE(stock->cached, memcg);\n\t}\n\tstock->nr_pages += nr_pages;\n\n\tif (stock->nr_pages > MEMCG_CHARGE_BATCH)\n\t\tdrain_stock(stock);\n}\n\nstatic void refill_stock(struct mem_cgroup *memcg, unsigned int nr_pages)\n{\n\tunsigned long flags;\n\n\tlocal_lock_irqsave(&memcg_stock.stock_lock, flags);\n\t__refill_stock(memcg, nr_pages);\n\tlocal_unlock_irqrestore(&memcg_stock.stock_lock, flags);\n}\n\n \nstatic void drain_all_stock(struct mem_cgroup *root_memcg)\n{\n\tint cpu, curcpu;\n\n\t \n\tif (!mutex_trylock(&percpu_charge_mutex))\n\t\treturn;\n\t \n\tmigrate_disable();\n\tcurcpu = smp_processor_id();\n\tfor_each_online_cpu(cpu) {\n\t\tstruct memcg_stock_pcp *stock = &per_cpu(memcg_stock, cpu);\n\t\tstruct mem_cgroup *memcg;\n\t\tbool flush = false;\n\n\t\trcu_read_lock();\n\t\tmemcg = READ_ONCE(stock->cached);\n\t\tif (memcg && stock->nr_pages &&\n\t\t    mem_cgroup_is_descendant(memcg, root_memcg))\n\t\t\tflush = true;\n\t\telse if (obj_stock_flush_required(stock, root_memcg))\n\t\t\tflush = true;\n\t\trcu_read_unlock();\n\n\t\tif (flush &&\n\t\t    !test_and_set_bit(FLUSHING_CACHED_CHARGE, &stock->flags)) {\n\t\t\tif (cpu == curcpu)\n\t\t\t\tdrain_local_stock(&stock->work);\n\t\t\telse if (!cpu_is_isolated(cpu))\n\t\t\t\tschedule_work_on(cpu, &stock->work);\n\t\t}\n\t}\n\tmigrate_enable();\n\tmutex_unlock(&percpu_charge_mutex);\n}\n\nstatic int memcg_hotplug_cpu_dead(unsigned int cpu)\n{\n\tstruct memcg_stock_pcp *stock;\n\n\tstock = &per_cpu(memcg_stock, cpu);\n\tdrain_stock(stock);\n\n\treturn 0;\n}\n\nstatic unsigned long reclaim_high(struct mem_cgroup *memcg,\n\t\t\t\t  unsigned int nr_pages,\n\t\t\t\t  gfp_t gfp_mask)\n{\n\tunsigned long nr_reclaimed = 0;\n\n\tdo {\n\t\tunsigned long pflags;\n\n\t\tif (page_counter_read(&memcg->memory) <=\n\t\t    READ_ONCE(memcg->memory.high))\n\t\t\tcontinue;\n\n\t\tmemcg_memory_event(memcg, MEMCG_HIGH);\n\n\t\tpsi_memstall_enter(&pflags);\n\t\tnr_reclaimed += try_to_free_mem_cgroup_pages(memcg, nr_pages,\n\t\t\t\t\t\t\tgfp_mask,\n\t\t\t\t\t\t\tMEMCG_RECLAIM_MAY_SWAP);\n\t\tpsi_memstall_leave(&pflags);\n\t} while ((memcg = parent_mem_cgroup(memcg)) &&\n\t\t !mem_cgroup_is_root(memcg));\n\n\treturn nr_reclaimed;\n}\n\nstatic void high_work_func(struct work_struct *work)\n{\n\tstruct mem_cgroup *memcg;\n\n\tmemcg = container_of(work, struct mem_cgroup, high_work);\n\treclaim_high(memcg, MEMCG_CHARGE_BATCH, GFP_KERNEL);\n}\n\n \n#define MEMCG_MAX_HIGH_DELAY_JIFFIES (2UL*HZ)\n\n \n #define MEMCG_DELAY_PRECISION_SHIFT 20\n #define MEMCG_DELAY_SCALING_SHIFT 14\n\nstatic u64 calculate_overage(unsigned long usage, unsigned long high)\n{\n\tu64 overage;\n\n\tif (usage <= high)\n\t\treturn 0;\n\n\t \n\thigh = max(high, 1UL);\n\n\toverage = usage - high;\n\toverage <<= MEMCG_DELAY_PRECISION_SHIFT;\n\treturn div64_u64(overage, high);\n}\n\nstatic u64 mem_find_max_overage(struct mem_cgroup *memcg)\n{\n\tu64 overage, max_overage = 0;\n\n\tdo {\n\t\toverage = calculate_overage(page_counter_read(&memcg->memory),\n\t\t\t\t\t    READ_ONCE(memcg->memory.high));\n\t\tmax_overage = max(overage, max_overage);\n\t} while ((memcg = parent_mem_cgroup(memcg)) &&\n\t\t !mem_cgroup_is_root(memcg));\n\n\treturn max_overage;\n}\n\nstatic u64 swap_find_max_overage(struct mem_cgroup *memcg)\n{\n\tu64 overage, max_overage = 0;\n\n\tdo {\n\t\toverage = calculate_overage(page_counter_read(&memcg->swap),\n\t\t\t\t\t    READ_ONCE(memcg->swap.high));\n\t\tif (overage)\n\t\t\tmemcg_memory_event(memcg, MEMCG_SWAP_HIGH);\n\t\tmax_overage = max(overage, max_overage);\n\t} while ((memcg = parent_mem_cgroup(memcg)) &&\n\t\t !mem_cgroup_is_root(memcg));\n\n\treturn max_overage;\n}\n\n \nstatic unsigned long calculate_high_delay(struct mem_cgroup *memcg,\n\t\t\t\t\t  unsigned int nr_pages,\n\t\t\t\t\t  u64 max_overage)\n{\n\tunsigned long penalty_jiffies;\n\n\tif (!max_overage)\n\t\treturn 0;\n\n\t \n\tpenalty_jiffies = max_overage * max_overage * HZ;\n\tpenalty_jiffies >>= MEMCG_DELAY_PRECISION_SHIFT;\n\tpenalty_jiffies >>= MEMCG_DELAY_SCALING_SHIFT;\n\n\t \n\treturn penalty_jiffies * nr_pages / MEMCG_CHARGE_BATCH;\n}\n\n \nvoid mem_cgroup_handle_over_high(gfp_t gfp_mask)\n{\n\tunsigned long penalty_jiffies;\n\tunsigned long pflags;\n\tunsigned long nr_reclaimed;\n\tunsigned int nr_pages = current->memcg_nr_pages_over_high;\n\tint nr_retries = MAX_RECLAIM_RETRIES;\n\tstruct mem_cgroup *memcg;\n\tbool in_retry = false;\n\n\tif (likely(!nr_pages))\n\t\treturn;\n\n\tmemcg = get_mem_cgroup_from_mm(current->mm);\n\tcurrent->memcg_nr_pages_over_high = 0;\n\nretry_reclaim:\n\t \n\tnr_reclaimed = reclaim_high(memcg,\n\t\t\t\t    in_retry ? SWAP_CLUSTER_MAX : nr_pages,\n\t\t\t\t    gfp_mask);\n\n\t \n\tpenalty_jiffies = calculate_high_delay(memcg, nr_pages,\n\t\t\t\t\t       mem_find_max_overage(memcg));\n\n\tpenalty_jiffies += calculate_high_delay(memcg, nr_pages,\n\t\t\t\t\t\tswap_find_max_overage(memcg));\n\n\t \n\tpenalty_jiffies = min(penalty_jiffies, MEMCG_MAX_HIGH_DELAY_JIFFIES);\n\n\t \n\tif (penalty_jiffies <= HZ / 100)\n\t\tgoto out;\n\n\t \n\tif (nr_reclaimed || nr_retries--) {\n\t\tin_retry = true;\n\t\tgoto retry_reclaim;\n\t}\n\n\t \n\tpsi_memstall_enter(&pflags);\n\tschedule_timeout_killable(penalty_jiffies);\n\tpsi_memstall_leave(&pflags);\n\nout:\n\tcss_put(&memcg->css);\n}\n\nstatic int try_charge_memcg(struct mem_cgroup *memcg, gfp_t gfp_mask,\n\t\t\tunsigned int nr_pages)\n{\n\tunsigned int batch = max(MEMCG_CHARGE_BATCH, nr_pages);\n\tint nr_retries = MAX_RECLAIM_RETRIES;\n\tstruct mem_cgroup *mem_over_limit;\n\tstruct page_counter *counter;\n\tunsigned long nr_reclaimed;\n\tbool passed_oom = false;\n\tunsigned int reclaim_options = MEMCG_RECLAIM_MAY_SWAP;\n\tbool drained = false;\n\tbool raised_max_event = false;\n\tunsigned long pflags;\n\nretry:\n\tif (consume_stock(memcg, nr_pages))\n\t\treturn 0;\n\n\tif (!do_memsw_account() ||\n\t    page_counter_try_charge(&memcg->memsw, batch, &counter)) {\n\t\tif (page_counter_try_charge(&memcg->memory, batch, &counter))\n\t\t\tgoto done_restock;\n\t\tif (do_memsw_account())\n\t\t\tpage_counter_uncharge(&memcg->memsw, batch);\n\t\tmem_over_limit = mem_cgroup_from_counter(counter, memory);\n\t} else {\n\t\tmem_over_limit = mem_cgroup_from_counter(counter, memsw);\n\t\treclaim_options &= ~MEMCG_RECLAIM_MAY_SWAP;\n\t}\n\n\tif (batch > nr_pages) {\n\t\tbatch = nr_pages;\n\t\tgoto retry;\n\t}\n\n\t \n\tif (unlikely(current->flags & PF_MEMALLOC))\n\t\tgoto force;\n\n\tif (unlikely(task_in_memcg_oom(current)))\n\t\tgoto nomem;\n\n\tif (!gfpflags_allow_blocking(gfp_mask))\n\t\tgoto nomem;\n\n\tmemcg_memory_event(mem_over_limit, MEMCG_MAX);\n\traised_max_event = true;\n\n\tpsi_memstall_enter(&pflags);\n\tnr_reclaimed = try_to_free_mem_cgroup_pages(mem_over_limit, nr_pages,\n\t\t\t\t\t\t    gfp_mask, reclaim_options);\n\tpsi_memstall_leave(&pflags);\n\n\tif (mem_cgroup_margin(mem_over_limit) >= nr_pages)\n\t\tgoto retry;\n\n\tif (!drained) {\n\t\tdrain_all_stock(mem_over_limit);\n\t\tdrained = true;\n\t\tgoto retry;\n\t}\n\n\tif (gfp_mask & __GFP_NORETRY)\n\t\tgoto nomem;\n\t \n\tif (nr_reclaimed && nr_pages <= (1 << PAGE_ALLOC_COSTLY_ORDER))\n\t\tgoto retry;\n\t \n\tif (mem_cgroup_wait_acct_move(mem_over_limit))\n\t\tgoto retry;\n\n\tif (nr_retries--)\n\t\tgoto retry;\n\n\tif (gfp_mask & __GFP_RETRY_MAYFAIL)\n\t\tgoto nomem;\n\n\t \n\tif (passed_oom && task_is_dying())\n\t\tgoto nomem;\n\n\t \n\tif (mem_cgroup_oom(mem_over_limit, gfp_mask,\n\t\t\t   get_order(nr_pages * PAGE_SIZE))) {\n\t\tpassed_oom = true;\n\t\tnr_retries = MAX_RECLAIM_RETRIES;\n\t\tgoto retry;\n\t}\nnomem:\n\t \n\tif (!(gfp_mask & (__GFP_NOFAIL | __GFP_HIGH)))\n\t\treturn -ENOMEM;\nforce:\n\t \n\tif (!raised_max_event)\n\t\tmemcg_memory_event(mem_over_limit, MEMCG_MAX);\n\n\t \n\tpage_counter_charge(&memcg->memory, nr_pages);\n\tif (do_memsw_account())\n\t\tpage_counter_charge(&memcg->memsw, nr_pages);\n\n\treturn 0;\n\ndone_restock:\n\tif (batch > nr_pages)\n\t\trefill_stock(memcg, batch - nr_pages);\n\n\t \n\tdo {\n\t\tbool mem_high, swap_high;\n\n\t\tmem_high = page_counter_read(&memcg->memory) >\n\t\t\tREAD_ONCE(memcg->memory.high);\n\t\tswap_high = page_counter_read(&memcg->swap) >\n\t\t\tREAD_ONCE(memcg->swap.high);\n\n\t\t \n\t\tif (!in_task()) {\n\t\t\tif (mem_high) {\n\t\t\t\tschedule_work(&memcg->high_work);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (mem_high || swap_high) {\n\t\t\t \n\t\t\tcurrent->memcg_nr_pages_over_high += batch;\n\t\t\tset_notify_resume(current);\n\t\t\tbreak;\n\t\t}\n\t} while ((memcg = parent_mem_cgroup(memcg)));\n\n\tif (current->memcg_nr_pages_over_high > MEMCG_CHARGE_BATCH &&\n\t    !(current->flags & PF_MEMALLOC) &&\n\t    gfpflags_allow_blocking(gfp_mask)) {\n\t\tmem_cgroup_handle_over_high(gfp_mask);\n\t}\n\treturn 0;\n}\n\nstatic inline int try_charge(struct mem_cgroup *memcg, gfp_t gfp_mask,\n\t\t\t     unsigned int nr_pages)\n{\n\tif (mem_cgroup_is_root(memcg))\n\t\treturn 0;\n\n\treturn try_charge_memcg(memcg, gfp_mask, nr_pages);\n}\n\nstatic inline void cancel_charge(struct mem_cgroup *memcg, unsigned int nr_pages)\n{\n\tif (mem_cgroup_is_root(memcg))\n\t\treturn;\n\n\tpage_counter_uncharge(&memcg->memory, nr_pages);\n\tif (do_memsw_account())\n\t\tpage_counter_uncharge(&memcg->memsw, nr_pages);\n}\n\nstatic void commit_charge(struct folio *folio, struct mem_cgroup *memcg)\n{\n\tVM_BUG_ON_FOLIO(folio_memcg(folio), folio);\n\t \n\tfolio->memcg_data = (unsigned long)memcg;\n}\n\n#ifdef CONFIG_MEMCG_KMEM\n \n#define OBJCGS_CLEAR_MASK\t(__GFP_DMA | __GFP_RECLAIMABLE | \\\n\t\t\t\t __GFP_ACCOUNT | __GFP_NOFAIL)\n\n \nstatic inline void mod_objcg_mlstate(struct obj_cgroup *objcg,\n\t\t\t\t     struct pglist_data *pgdat,\n\t\t\t\t     enum node_stat_item idx, int nr)\n{\n\tstruct mem_cgroup *memcg;\n\tstruct lruvec *lruvec;\n\n\trcu_read_lock();\n\tmemcg = obj_cgroup_memcg(objcg);\n\tlruvec = mem_cgroup_lruvec(memcg, pgdat);\n\tmod_memcg_lruvec_state(lruvec, idx, nr);\n\trcu_read_unlock();\n}\n\nint memcg_alloc_slab_cgroups(struct slab *slab, struct kmem_cache *s,\n\t\t\t\t gfp_t gfp, bool new_slab)\n{\n\tunsigned int objects = objs_per_slab(s, slab);\n\tunsigned long memcg_data;\n\tvoid *vec;\n\n\tgfp &= ~OBJCGS_CLEAR_MASK;\n\tvec = kcalloc_node(objects, sizeof(struct obj_cgroup *), gfp,\n\t\t\t   slab_nid(slab));\n\tif (!vec)\n\t\treturn -ENOMEM;\n\n\tmemcg_data = (unsigned long) vec | MEMCG_DATA_OBJCGS;\n\tif (new_slab) {\n\t\t \n\t\tslab->memcg_data = memcg_data;\n\t} else if (cmpxchg(&slab->memcg_data, 0, memcg_data)) {\n\t\t \n\t\tkfree(vec);\n\t\treturn 0;\n\t}\n\n\tkmemleak_not_leak(vec);\n\treturn 0;\n}\n\nstatic __always_inline\nstruct mem_cgroup *mem_cgroup_from_obj_folio(struct folio *folio, void *p)\n{\n\t \n\tif (folio_test_slab(folio)) {\n\t\tstruct obj_cgroup **objcgs;\n\t\tstruct slab *slab;\n\t\tunsigned int off;\n\n\t\tslab = folio_slab(folio);\n\t\tobjcgs = slab_objcgs(slab);\n\t\tif (!objcgs)\n\t\t\treturn NULL;\n\n\t\toff = obj_to_index(slab->slab_cache, slab, p);\n\t\tif (objcgs[off])\n\t\t\treturn obj_cgroup_memcg(objcgs[off]);\n\n\t\treturn NULL;\n\t}\n\n\t \n\treturn folio_memcg_check(folio);\n}\n\n \nstruct mem_cgroup *mem_cgroup_from_obj(void *p)\n{\n\tstruct folio *folio;\n\n\tif (mem_cgroup_disabled())\n\t\treturn NULL;\n\n\tif (unlikely(is_vmalloc_addr(p)))\n\t\tfolio = page_folio(vmalloc_to_page(p));\n\telse\n\t\tfolio = virt_to_folio(p);\n\n\treturn mem_cgroup_from_obj_folio(folio, p);\n}\n\n \nstruct mem_cgroup *mem_cgroup_from_slab_obj(void *p)\n{\n\tif (mem_cgroup_disabled())\n\t\treturn NULL;\n\n\treturn mem_cgroup_from_obj_folio(virt_to_folio(p), p);\n}\n\nstatic struct obj_cgroup *__get_obj_cgroup_from_memcg(struct mem_cgroup *memcg)\n{\n\tstruct obj_cgroup *objcg = NULL;\n\n\tfor (; !mem_cgroup_is_root(memcg); memcg = parent_mem_cgroup(memcg)) {\n\t\tobjcg = rcu_dereference(memcg->objcg);\n\t\tif (objcg && obj_cgroup_tryget(objcg))\n\t\t\tbreak;\n\t\tobjcg = NULL;\n\t}\n\treturn objcg;\n}\n\n__always_inline struct obj_cgroup *get_obj_cgroup_from_current(void)\n{\n\tstruct obj_cgroup *objcg = NULL;\n\tstruct mem_cgroup *memcg;\n\n\tif (memcg_kmem_bypass())\n\t\treturn NULL;\n\n\trcu_read_lock();\n\tif (unlikely(active_memcg()))\n\t\tmemcg = active_memcg();\n\telse\n\t\tmemcg = mem_cgroup_from_task(current);\n\tobjcg = __get_obj_cgroup_from_memcg(memcg);\n\trcu_read_unlock();\n\treturn objcg;\n}\n\nstruct obj_cgroup *get_obj_cgroup_from_folio(struct folio *folio)\n{\n\tstruct obj_cgroup *objcg;\n\n\tif (!memcg_kmem_online())\n\t\treturn NULL;\n\n\tif (folio_memcg_kmem(folio)) {\n\t\tobjcg = __folio_objcg(folio);\n\t\tobj_cgroup_get(objcg);\n\t} else {\n\t\tstruct mem_cgroup *memcg;\n\n\t\trcu_read_lock();\n\t\tmemcg = __folio_memcg(folio);\n\t\tif (memcg)\n\t\t\tobjcg = __get_obj_cgroup_from_memcg(memcg);\n\t\telse\n\t\t\tobjcg = NULL;\n\t\trcu_read_unlock();\n\t}\n\treturn objcg;\n}\n\nstatic void memcg_account_kmem(struct mem_cgroup *memcg, int nr_pages)\n{\n\tmod_memcg_state(memcg, MEMCG_KMEM, nr_pages);\n\tif (!cgroup_subsys_on_dfl(memory_cgrp_subsys)) {\n\t\tif (nr_pages > 0)\n\t\t\tpage_counter_charge(&memcg->kmem, nr_pages);\n\t\telse\n\t\t\tpage_counter_uncharge(&memcg->kmem, -nr_pages);\n\t}\n}\n\n\n \nstatic void obj_cgroup_uncharge_pages(struct obj_cgroup *objcg,\n\t\t\t\t      unsigned int nr_pages)\n{\n\tstruct mem_cgroup *memcg;\n\n\tmemcg = get_mem_cgroup_from_objcg(objcg);\n\n\tmemcg_account_kmem(memcg, -nr_pages);\n\trefill_stock(memcg, nr_pages);\n\n\tcss_put(&memcg->css);\n}\n\n \nstatic int obj_cgroup_charge_pages(struct obj_cgroup *objcg, gfp_t gfp,\n\t\t\t\t   unsigned int nr_pages)\n{\n\tstruct mem_cgroup *memcg;\n\tint ret;\n\n\tmemcg = get_mem_cgroup_from_objcg(objcg);\n\n\tret = try_charge_memcg(memcg, gfp, nr_pages);\n\tif (ret)\n\t\tgoto out;\n\n\tmemcg_account_kmem(memcg, nr_pages);\nout:\n\tcss_put(&memcg->css);\n\n\treturn ret;\n}\n\n \nint __memcg_kmem_charge_page(struct page *page, gfp_t gfp, int order)\n{\n\tstruct obj_cgroup *objcg;\n\tint ret = 0;\n\n\tobjcg = get_obj_cgroup_from_current();\n\tif (objcg) {\n\t\tret = obj_cgroup_charge_pages(objcg, gfp, 1 << order);\n\t\tif (!ret) {\n\t\t\tpage->memcg_data = (unsigned long)objcg |\n\t\t\t\tMEMCG_DATA_KMEM;\n\t\t\treturn 0;\n\t\t}\n\t\tobj_cgroup_put(objcg);\n\t}\n\treturn ret;\n}\n\n \nvoid __memcg_kmem_uncharge_page(struct page *page, int order)\n{\n\tstruct folio *folio = page_folio(page);\n\tstruct obj_cgroup *objcg;\n\tunsigned int nr_pages = 1 << order;\n\n\tif (!folio_memcg_kmem(folio))\n\t\treturn;\n\n\tobjcg = __folio_objcg(folio);\n\tobj_cgroup_uncharge_pages(objcg, nr_pages);\n\tfolio->memcg_data = 0;\n\tobj_cgroup_put(objcg);\n}\n\nvoid mod_objcg_state(struct obj_cgroup *objcg, struct pglist_data *pgdat,\n\t\t     enum node_stat_item idx, int nr)\n{\n\tstruct memcg_stock_pcp *stock;\n\tstruct obj_cgroup *old = NULL;\n\tunsigned long flags;\n\tint *bytes;\n\n\tlocal_lock_irqsave(&memcg_stock.stock_lock, flags);\n\tstock = this_cpu_ptr(&memcg_stock);\n\n\t \n\tif (READ_ONCE(stock->cached_objcg) != objcg) {\n\t\told = drain_obj_stock(stock);\n\t\tobj_cgroup_get(objcg);\n\t\tstock->nr_bytes = atomic_read(&objcg->nr_charged_bytes)\n\t\t\t\t? atomic_xchg(&objcg->nr_charged_bytes, 0) : 0;\n\t\tWRITE_ONCE(stock->cached_objcg, objcg);\n\t\tstock->cached_pgdat = pgdat;\n\t} else if (stock->cached_pgdat != pgdat) {\n\t\t \n\t\tstruct pglist_data *oldpg = stock->cached_pgdat;\n\n\t\tif (stock->nr_slab_reclaimable_b) {\n\t\t\tmod_objcg_mlstate(objcg, oldpg, NR_SLAB_RECLAIMABLE_B,\n\t\t\t\t\t  stock->nr_slab_reclaimable_b);\n\t\t\tstock->nr_slab_reclaimable_b = 0;\n\t\t}\n\t\tif (stock->nr_slab_unreclaimable_b) {\n\t\t\tmod_objcg_mlstate(objcg, oldpg, NR_SLAB_UNRECLAIMABLE_B,\n\t\t\t\t\t  stock->nr_slab_unreclaimable_b);\n\t\t\tstock->nr_slab_unreclaimable_b = 0;\n\t\t}\n\t\tstock->cached_pgdat = pgdat;\n\t}\n\n\tbytes = (idx == NR_SLAB_RECLAIMABLE_B) ? &stock->nr_slab_reclaimable_b\n\t\t\t\t\t       : &stock->nr_slab_unreclaimable_b;\n\t \n\tif (!*bytes) {\n\t\t*bytes = nr;\n\t\tnr = 0;\n\t} else {\n\t\t*bytes += nr;\n\t\tif (abs(*bytes) > PAGE_SIZE) {\n\t\t\tnr = *bytes;\n\t\t\t*bytes = 0;\n\t\t} else {\n\t\t\tnr = 0;\n\t\t}\n\t}\n\tif (nr)\n\t\tmod_objcg_mlstate(objcg, pgdat, idx, nr);\n\n\tlocal_unlock_irqrestore(&memcg_stock.stock_lock, flags);\n\tif (old)\n\t\tobj_cgroup_put(old);\n}\n\nstatic bool consume_obj_stock(struct obj_cgroup *objcg, unsigned int nr_bytes)\n{\n\tstruct memcg_stock_pcp *stock;\n\tunsigned long flags;\n\tbool ret = false;\n\n\tlocal_lock_irqsave(&memcg_stock.stock_lock, flags);\n\n\tstock = this_cpu_ptr(&memcg_stock);\n\tif (objcg == READ_ONCE(stock->cached_objcg) && stock->nr_bytes >= nr_bytes) {\n\t\tstock->nr_bytes -= nr_bytes;\n\t\tret = true;\n\t}\n\n\tlocal_unlock_irqrestore(&memcg_stock.stock_lock, flags);\n\n\treturn ret;\n}\n\nstatic struct obj_cgroup *drain_obj_stock(struct memcg_stock_pcp *stock)\n{\n\tstruct obj_cgroup *old = READ_ONCE(stock->cached_objcg);\n\n\tif (!old)\n\t\treturn NULL;\n\n\tif (stock->nr_bytes) {\n\t\tunsigned int nr_pages = stock->nr_bytes >> PAGE_SHIFT;\n\t\tunsigned int nr_bytes = stock->nr_bytes & (PAGE_SIZE - 1);\n\n\t\tif (nr_pages) {\n\t\t\tstruct mem_cgroup *memcg;\n\n\t\t\tmemcg = get_mem_cgroup_from_objcg(old);\n\n\t\t\tmemcg_account_kmem(memcg, -nr_pages);\n\t\t\t__refill_stock(memcg, nr_pages);\n\n\t\t\tcss_put(&memcg->css);\n\t\t}\n\n\t\t \n\t\tatomic_add(nr_bytes, &old->nr_charged_bytes);\n\t\tstock->nr_bytes = 0;\n\t}\n\n\t \n\tif (stock->nr_slab_reclaimable_b || stock->nr_slab_unreclaimable_b) {\n\t\tif (stock->nr_slab_reclaimable_b) {\n\t\t\tmod_objcg_mlstate(old, stock->cached_pgdat,\n\t\t\t\t\t  NR_SLAB_RECLAIMABLE_B,\n\t\t\t\t\t  stock->nr_slab_reclaimable_b);\n\t\t\tstock->nr_slab_reclaimable_b = 0;\n\t\t}\n\t\tif (stock->nr_slab_unreclaimable_b) {\n\t\t\tmod_objcg_mlstate(old, stock->cached_pgdat,\n\t\t\t\t\t  NR_SLAB_UNRECLAIMABLE_B,\n\t\t\t\t\t  stock->nr_slab_unreclaimable_b);\n\t\t\tstock->nr_slab_unreclaimable_b = 0;\n\t\t}\n\t\tstock->cached_pgdat = NULL;\n\t}\n\n\tWRITE_ONCE(stock->cached_objcg, NULL);\n\t \n\treturn old;\n}\n\nstatic bool obj_stock_flush_required(struct memcg_stock_pcp *stock,\n\t\t\t\t     struct mem_cgroup *root_memcg)\n{\n\tstruct obj_cgroup *objcg = READ_ONCE(stock->cached_objcg);\n\tstruct mem_cgroup *memcg;\n\n\tif (objcg) {\n\t\tmemcg = obj_cgroup_memcg(objcg);\n\t\tif (memcg && mem_cgroup_is_descendant(memcg, root_memcg))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic void refill_obj_stock(struct obj_cgroup *objcg, unsigned int nr_bytes,\n\t\t\t     bool allow_uncharge)\n{\n\tstruct memcg_stock_pcp *stock;\n\tstruct obj_cgroup *old = NULL;\n\tunsigned long flags;\n\tunsigned int nr_pages = 0;\n\n\tlocal_lock_irqsave(&memcg_stock.stock_lock, flags);\n\n\tstock = this_cpu_ptr(&memcg_stock);\n\tif (READ_ONCE(stock->cached_objcg) != objcg) {  \n\t\told = drain_obj_stock(stock);\n\t\tobj_cgroup_get(objcg);\n\t\tWRITE_ONCE(stock->cached_objcg, objcg);\n\t\tstock->nr_bytes = atomic_read(&objcg->nr_charged_bytes)\n\t\t\t\t? atomic_xchg(&objcg->nr_charged_bytes, 0) : 0;\n\t\tallow_uncharge = true;\t \n\t}\n\tstock->nr_bytes += nr_bytes;\n\n\tif (allow_uncharge && (stock->nr_bytes > PAGE_SIZE)) {\n\t\tnr_pages = stock->nr_bytes >> PAGE_SHIFT;\n\t\tstock->nr_bytes &= (PAGE_SIZE - 1);\n\t}\n\n\tlocal_unlock_irqrestore(&memcg_stock.stock_lock, flags);\n\tif (old)\n\t\tobj_cgroup_put(old);\n\n\tif (nr_pages)\n\t\tobj_cgroup_uncharge_pages(objcg, nr_pages);\n}\n\nint obj_cgroup_charge(struct obj_cgroup *objcg, gfp_t gfp, size_t size)\n{\n\tunsigned int nr_pages, nr_bytes;\n\tint ret;\n\n\tif (consume_obj_stock(objcg, size))\n\t\treturn 0;\n\n\t \n\tnr_pages = size >> PAGE_SHIFT;\n\tnr_bytes = size & (PAGE_SIZE - 1);\n\n\tif (nr_bytes)\n\t\tnr_pages += 1;\n\n\tret = obj_cgroup_charge_pages(objcg, gfp, nr_pages);\n\tif (!ret && nr_bytes)\n\t\trefill_obj_stock(objcg, PAGE_SIZE - nr_bytes, false);\n\n\treturn ret;\n}\n\nvoid obj_cgroup_uncharge(struct obj_cgroup *objcg, size_t size)\n{\n\trefill_obj_stock(objcg, size, true);\n}\n\n#endif  \n\n \nvoid split_page_memcg(struct page *head, unsigned int nr)\n{\n\tstruct folio *folio = page_folio(head);\n\tstruct mem_cgroup *memcg = folio_memcg(folio);\n\tint i;\n\n\tif (mem_cgroup_disabled() || !memcg)\n\t\treturn;\n\n\tfor (i = 1; i < nr; i++)\n\t\tfolio_page(folio, i)->memcg_data = folio->memcg_data;\n\n\tif (folio_memcg_kmem(folio))\n\t\tobj_cgroup_get_many(__folio_objcg(folio), nr - 1);\n\telse\n\t\tcss_get_many(&memcg->css, nr - 1);\n}\n\n#ifdef CONFIG_SWAP\n \nstatic int mem_cgroup_move_swap_account(swp_entry_t entry,\n\t\t\t\tstruct mem_cgroup *from, struct mem_cgroup *to)\n{\n\tunsigned short old_id, new_id;\n\n\told_id = mem_cgroup_id(from);\n\tnew_id = mem_cgroup_id(to);\n\n\tif (swap_cgroup_cmpxchg(entry, old_id, new_id) == old_id) {\n\t\tmod_memcg_state(from, MEMCG_SWAP, -1);\n\t\tmod_memcg_state(to, MEMCG_SWAP, 1);\n\t\treturn 0;\n\t}\n\treturn -EINVAL;\n}\n#else\nstatic inline int mem_cgroup_move_swap_account(swp_entry_t entry,\n\t\t\t\tstruct mem_cgroup *from, struct mem_cgroup *to)\n{\n\treturn -EINVAL;\n}\n#endif\n\nstatic DEFINE_MUTEX(memcg_max_mutex);\n\nstatic int mem_cgroup_resize_max(struct mem_cgroup *memcg,\n\t\t\t\t unsigned long max, bool memsw)\n{\n\tbool enlarge = false;\n\tbool drained = false;\n\tint ret;\n\tbool limits_invariant;\n\tstruct page_counter *counter = memsw ? &memcg->memsw : &memcg->memory;\n\n\tdo {\n\t\tif (signal_pending(current)) {\n\t\t\tret = -EINTR;\n\t\t\tbreak;\n\t\t}\n\n\t\tmutex_lock(&memcg_max_mutex);\n\t\t \n\t\tlimits_invariant = memsw ? max >= READ_ONCE(memcg->memory.max) :\n\t\t\t\t\t   max <= memcg->memsw.max;\n\t\tif (!limits_invariant) {\n\t\t\tmutex_unlock(&memcg_max_mutex);\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tif (max > counter->max)\n\t\t\tenlarge = true;\n\t\tret = page_counter_set_max(counter, max);\n\t\tmutex_unlock(&memcg_max_mutex);\n\n\t\tif (!ret)\n\t\t\tbreak;\n\n\t\tif (!drained) {\n\t\t\tdrain_all_stock(memcg);\n\t\t\tdrained = true;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!try_to_free_mem_cgroup_pages(memcg, 1, GFP_KERNEL,\n\t\t\t\t\tmemsw ? 0 : MEMCG_RECLAIM_MAY_SWAP)) {\n\t\t\tret = -EBUSY;\n\t\t\tbreak;\n\t\t}\n\t} while (true);\n\n\tif (!ret && enlarge)\n\t\tmemcg_oom_recover(memcg);\n\n\treturn ret;\n}\n\nunsigned long mem_cgroup_soft_limit_reclaim(pg_data_t *pgdat, int order,\n\t\t\t\t\t    gfp_t gfp_mask,\n\t\t\t\t\t    unsigned long *total_scanned)\n{\n\tunsigned long nr_reclaimed = 0;\n\tstruct mem_cgroup_per_node *mz, *next_mz = NULL;\n\tunsigned long reclaimed;\n\tint loop = 0;\n\tstruct mem_cgroup_tree_per_node *mctz;\n\tunsigned long excess;\n\n\tif (lru_gen_enabled())\n\t\treturn 0;\n\n\tif (order > 0)\n\t\treturn 0;\n\n\tmctz = soft_limit_tree.rb_tree_per_node[pgdat->node_id];\n\n\t \n\tif (!mctz || RB_EMPTY_ROOT(&mctz->rb_root))\n\t\treturn 0;\n\n\t \n\tdo {\n\t\tif (next_mz)\n\t\t\tmz = next_mz;\n\t\telse\n\t\t\tmz = mem_cgroup_largest_soft_limit_node(mctz);\n\t\tif (!mz)\n\t\t\tbreak;\n\n\t\treclaimed = mem_cgroup_soft_reclaim(mz->memcg, pgdat,\n\t\t\t\t\t\t    gfp_mask, total_scanned);\n\t\tnr_reclaimed += reclaimed;\n\t\tspin_lock_irq(&mctz->lock);\n\n\t\t \n\t\tnext_mz = NULL;\n\t\tif (!reclaimed)\n\t\t\tnext_mz = __mem_cgroup_largest_soft_limit_node(mctz);\n\n\t\texcess = soft_limit_excess(mz->memcg);\n\t\t \n\t\t \n\t\t__mem_cgroup_insert_exceeded(mz, mctz, excess);\n\t\tspin_unlock_irq(&mctz->lock);\n\t\tcss_put(&mz->memcg->css);\n\t\tloop++;\n\t\t \n\t\tif (!nr_reclaimed &&\n\t\t\t(next_mz == NULL ||\n\t\t\tloop > MEM_CGROUP_MAX_SOFT_LIMIT_RECLAIM_LOOPS))\n\t\t\tbreak;\n\t} while (!nr_reclaimed);\n\tif (next_mz)\n\t\tcss_put(&next_mz->memcg->css);\n\treturn nr_reclaimed;\n}\n\n \nstatic int mem_cgroup_force_empty(struct mem_cgroup *memcg)\n{\n\tint nr_retries = MAX_RECLAIM_RETRIES;\n\n\t \n\tlru_add_drain_all();\n\n\tdrain_all_stock(memcg);\n\n\t \n\twhile (nr_retries && page_counter_read(&memcg->memory)) {\n\t\tif (signal_pending(current))\n\t\t\treturn -EINTR;\n\n\t\tif (!try_to_free_mem_cgroup_pages(memcg, 1, GFP_KERNEL,\n\t\t\t\t\t\t  MEMCG_RECLAIM_MAY_SWAP))\n\t\t\tnr_retries--;\n\t}\n\n\treturn 0;\n}\n\nstatic ssize_t mem_cgroup_force_empty_write(struct kernfs_open_file *of,\n\t\t\t\t\t    char *buf, size_t nbytes,\n\t\t\t\t\t    loff_t off)\n{\n\tstruct mem_cgroup *memcg = mem_cgroup_from_css(of_css(of));\n\n\tif (mem_cgroup_is_root(memcg))\n\t\treturn -EINVAL;\n\treturn mem_cgroup_force_empty(memcg) ?: nbytes;\n}\n\nstatic u64 mem_cgroup_hierarchy_read(struct cgroup_subsys_state *css,\n\t\t\t\t     struct cftype *cft)\n{\n\treturn 1;\n}\n\nstatic int mem_cgroup_hierarchy_write(struct cgroup_subsys_state *css,\n\t\t\t\t      struct cftype *cft, u64 val)\n{\n\tif (val == 1)\n\t\treturn 0;\n\n\tpr_warn_once(\"Non-hierarchical mode is deprecated. \"\n\t\t     \"Please report your usecase to linux-mm@kvack.org if you \"\n\t\t     \"depend on this functionality.\\n\");\n\n\treturn -EINVAL;\n}\n\nstatic unsigned long mem_cgroup_usage(struct mem_cgroup *memcg, bool swap)\n{\n\tunsigned long val;\n\n\tif (mem_cgroup_is_root(memcg)) {\n\t\t \n\t\tval = global_node_page_state(NR_FILE_PAGES) +\n\t\t\tglobal_node_page_state(NR_ANON_MAPPED);\n\t\tif (swap)\n\t\t\tval += total_swap_pages - get_nr_swap_pages();\n\t} else {\n\t\tif (!swap)\n\t\t\tval = page_counter_read(&memcg->memory);\n\t\telse\n\t\t\tval = page_counter_read(&memcg->memsw);\n\t}\n\treturn val;\n}\n\nenum {\n\tRES_USAGE,\n\tRES_LIMIT,\n\tRES_MAX_USAGE,\n\tRES_FAILCNT,\n\tRES_SOFT_LIMIT,\n};\n\nstatic u64 mem_cgroup_read_u64(struct cgroup_subsys_state *css,\n\t\t\t       struct cftype *cft)\n{\n\tstruct mem_cgroup *memcg = mem_cgroup_from_css(css);\n\tstruct page_counter *counter;\n\n\tswitch (MEMFILE_TYPE(cft->private)) {\n\tcase _MEM:\n\t\tcounter = &memcg->memory;\n\t\tbreak;\n\tcase _MEMSWAP:\n\t\tcounter = &memcg->memsw;\n\t\tbreak;\n\tcase _KMEM:\n\t\tcounter = &memcg->kmem;\n\t\tbreak;\n\tcase _TCP:\n\t\tcounter = &memcg->tcpmem;\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\tswitch (MEMFILE_ATTR(cft->private)) {\n\tcase RES_USAGE:\n\t\tif (counter == &memcg->memory)\n\t\t\treturn (u64)mem_cgroup_usage(memcg, false) * PAGE_SIZE;\n\t\tif (counter == &memcg->memsw)\n\t\t\treturn (u64)mem_cgroup_usage(memcg, true) * PAGE_SIZE;\n\t\treturn (u64)page_counter_read(counter) * PAGE_SIZE;\n\tcase RES_LIMIT:\n\t\treturn (u64)counter->max * PAGE_SIZE;\n\tcase RES_MAX_USAGE:\n\t\treturn (u64)counter->watermark * PAGE_SIZE;\n\tcase RES_FAILCNT:\n\t\treturn counter->failcnt;\n\tcase RES_SOFT_LIMIT:\n\t\treturn (u64)READ_ONCE(memcg->soft_limit) * PAGE_SIZE;\n\tdefault:\n\t\tBUG();\n\t}\n}\n\n \nstatic int mem_cgroup_dummy_seq_show(__always_unused struct seq_file *m,\n\t\t\t\t     __always_unused void *v)\n{\n\treturn -EINVAL;\n}\n\n#ifdef CONFIG_MEMCG_KMEM\nstatic int memcg_online_kmem(struct mem_cgroup *memcg)\n{\n\tstruct obj_cgroup *objcg;\n\n\tif (mem_cgroup_kmem_disabled())\n\t\treturn 0;\n\n\tif (unlikely(mem_cgroup_is_root(memcg)))\n\t\treturn 0;\n\n\tobjcg = obj_cgroup_alloc();\n\tif (!objcg)\n\t\treturn -ENOMEM;\n\n\tobjcg->memcg = memcg;\n\trcu_assign_pointer(memcg->objcg, objcg);\n\n\tstatic_branch_enable(&memcg_kmem_online_key);\n\n\tmemcg->kmemcg_id = memcg->id.id;\n\n\treturn 0;\n}\n\nstatic void memcg_offline_kmem(struct mem_cgroup *memcg)\n{\n\tstruct mem_cgroup *parent;\n\n\tif (mem_cgroup_kmem_disabled())\n\t\treturn;\n\n\tif (unlikely(mem_cgroup_is_root(memcg)))\n\t\treturn;\n\n\tparent = parent_mem_cgroup(memcg);\n\tif (!parent)\n\t\tparent = root_mem_cgroup;\n\n\tmemcg_reparent_objcgs(memcg, parent);\n\n\t \n\tmemcg_reparent_list_lrus(memcg, parent);\n}\n#else\nstatic int memcg_online_kmem(struct mem_cgroup *memcg)\n{\n\treturn 0;\n}\nstatic void memcg_offline_kmem(struct mem_cgroup *memcg)\n{\n}\n#endif  \n\nstatic int memcg_update_tcp_max(struct mem_cgroup *memcg, unsigned long max)\n{\n\tint ret;\n\n\tmutex_lock(&memcg_max_mutex);\n\n\tret = page_counter_set_max(&memcg->tcpmem, max);\n\tif (ret)\n\t\tgoto out;\n\n\tif (!memcg->tcpmem_active) {\n\t\t \n\t\tstatic_branch_inc(&memcg_sockets_enabled_key);\n\t\tmemcg->tcpmem_active = true;\n\t}\nout:\n\tmutex_unlock(&memcg_max_mutex);\n\treturn ret;\n}\n\n \nstatic ssize_t mem_cgroup_write(struct kernfs_open_file *of,\n\t\t\t\tchar *buf, size_t nbytes, loff_t off)\n{\n\tstruct mem_cgroup *memcg = mem_cgroup_from_css(of_css(of));\n\tunsigned long nr_pages;\n\tint ret;\n\n\tbuf = strstrip(buf);\n\tret = page_counter_memparse(buf, \"-1\", &nr_pages);\n\tif (ret)\n\t\treturn ret;\n\n\tswitch (MEMFILE_ATTR(of_cft(of)->private)) {\n\tcase RES_LIMIT:\n\t\tif (mem_cgroup_is_root(memcg)) {  \n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tswitch (MEMFILE_TYPE(of_cft(of)->private)) {\n\t\tcase _MEM:\n\t\t\tret = mem_cgroup_resize_max(memcg, nr_pages, false);\n\t\t\tbreak;\n\t\tcase _MEMSWAP:\n\t\t\tret = mem_cgroup_resize_max(memcg, nr_pages, true);\n\t\t\tbreak;\n\t\tcase _KMEM:\n\t\t\tpr_warn_once(\"kmem.limit_in_bytes is deprecated and will be removed. \"\n\t\t\t\t     \"Writing any value to this file has no effect. \"\n\t\t\t\t     \"Please report your usecase to linux-mm@kvack.org if you \"\n\t\t\t\t     \"depend on this functionality.\\n\");\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\tcase _TCP:\n\t\t\tret = memcg_update_tcp_max(memcg, nr_pages);\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase RES_SOFT_LIMIT:\n\t\tif (IS_ENABLED(CONFIG_PREEMPT_RT)) {\n\t\t\tret = -EOPNOTSUPP;\n\t\t} else {\n\t\t\tWRITE_ONCE(memcg->soft_limit, nr_pages);\n\t\t\tret = 0;\n\t\t}\n\t\tbreak;\n\t}\n\treturn ret ?: nbytes;\n}\n\nstatic ssize_t mem_cgroup_reset(struct kernfs_open_file *of, char *buf,\n\t\t\t\tsize_t nbytes, loff_t off)\n{\n\tstruct mem_cgroup *memcg = mem_cgroup_from_css(of_css(of));\n\tstruct page_counter *counter;\n\n\tswitch (MEMFILE_TYPE(of_cft(of)->private)) {\n\tcase _MEM:\n\t\tcounter = &memcg->memory;\n\t\tbreak;\n\tcase _MEMSWAP:\n\t\tcounter = &memcg->memsw;\n\t\tbreak;\n\tcase _KMEM:\n\t\tcounter = &memcg->kmem;\n\t\tbreak;\n\tcase _TCP:\n\t\tcounter = &memcg->tcpmem;\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\tswitch (MEMFILE_ATTR(of_cft(of)->private)) {\n\tcase RES_MAX_USAGE:\n\t\tpage_counter_reset_watermark(counter);\n\t\tbreak;\n\tcase RES_FAILCNT:\n\t\tcounter->failcnt = 0;\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\treturn nbytes;\n}\n\nstatic u64 mem_cgroup_move_charge_read(struct cgroup_subsys_state *css,\n\t\t\t\t\tstruct cftype *cft)\n{\n\treturn mem_cgroup_from_css(css)->move_charge_at_immigrate;\n}\n\n#ifdef CONFIG_MMU\nstatic int mem_cgroup_move_charge_write(struct cgroup_subsys_state *css,\n\t\t\t\t\tstruct cftype *cft, u64 val)\n{\n\tstruct mem_cgroup *memcg = mem_cgroup_from_css(css);\n\n\tpr_warn_once(\"Cgroup memory moving (move_charge_at_immigrate) is deprecated. \"\n\t\t     \"Please report your usecase to linux-mm@kvack.org if you \"\n\t\t     \"depend on this functionality.\\n\");\n\n\tif (val & ~MOVE_MASK)\n\t\treturn -EINVAL;\n\n\t \n\tmemcg->move_charge_at_immigrate = val;\n\treturn 0;\n}\n#else\nstatic int mem_cgroup_move_charge_write(struct cgroup_subsys_state *css,\n\t\t\t\t\tstruct cftype *cft, u64 val)\n{\n\treturn -ENOSYS;\n}\n#endif\n\n#ifdef CONFIG_NUMA\n\n#define LRU_ALL_FILE (BIT(LRU_INACTIVE_FILE) | BIT(LRU_ACTIVE_FILE))\n#define LRU_ALL_ANON (BIT(LRU_INACTIVE_ANON) | BIT(LRU_ACTIVE_ANON))\n#define LRU_ALL\t     ((1 << NR_LRU_LISTS) - 1)\n\nstatic unsigned long mem_cgroup_node_nr_lru_pages(struct mem_cgroup *memcg,\n\t\t\t\tint nid, unsigned int lru_mask, bool tree)\n{\n\tstruct lruvec *lruvec = mem_cgroup_lruvec(memcg, NODE_DATA(nid));\n\tunsigned long nr = 0;\n\tenum lru_list lru;\n\n\tVM_BUG_ON((unsigned)nid >= nr_node_ids);\n\n\tfor_each_lru(lru) {\n\t\tif (!(BIT(lru) & lru_mask))\n\t\t\tcontinue;\n\t\tif (tree)\n\t\t\tnr += lruvec_page_state(lruvec, NR_LRU_BASE + lru);\n\t\telse\n\t\t\tnr += lruvec_page_state_local(lruvec, NR_LRU_BASE + lru);\n\t}\n\treturn nr;\n}\n\nstatic unsigned long mem_cgroup_nr_lru_pages(struct mem_cgroup *memcg,\n\t\t\t\t\t     unsigned int lru_mask,\n\t\t\t\t\t     bool tree)\n{\n\tunsigned long nr = 0;\n\tenum lru_list lru;\n\n\tfor_each_lru(lru) {\n\t\tif (!(BIT(lru) & lru_mask))\n\t\t\tcontinue;\n\t\tif (tree)\n\t\t\tnr += memcg_page_state(memcg, NR_LRU_BASE + lru);\n\t\telse\n\t\t\tnr += memcg_page_state_local(memcg, NR_LRU_BASE + lru);\n\t}\n\treturn nr;\n}\n\nstatic int memcg_numa_stat_show(struct seq_file *m, void *v)\n{\n\tstruct numa_stat {\n\t\tconst char *name;\n\t\tunsigned int lru_mask;\n\t};\n\n\tstatic const struct numa_stat stats[] = {\n\t\t{ \"total\", LRU_ALL },\n\t\t{ \"file\", LRU_ALL_FILE },\n\t\t{ \"anon\", LRU_ALL_ANON },\n\t\t{ \"unevictable\", BIT(LRU_UNEVICTABLE) },\n\t};\n\tconst struct numa_stat *stat;\n\tint nid;\n\tstruct mem_cgroup *memcg = mem_cgroup_from_seq(m);\n\n\tmem_cgroup_flush_stats();\n\n\tfor (stat = stats; stat < stats + ARRAY_SIZE(stats); stat++) {\n\t\tseq_printf(m, \"%s=%lu\", stat->name,\n\t\t\t   mem_cgroup_nr_lru_pages(memcg, stat->lru_mask,\n\t\t\t\t\t\t   false));\n\t\tfor_each_node_state(nid, N_MEMORY)\n\t\t\tseq_printf(m, \" N%d=%lu\", nid,\n\t\t\t\t   mem_cgroup_node_nr_lru_pages(memcg, nid,\n\t\t\t\t\t\t\tstat->lru_mask, false));\n\t\tseq_putc(m, '\\n');\n\t}\n\n\tfor (stat = stats; stat < stats + ARRAY_SIZE(stats); stat++) {\n\n\t\tseq_printf(m, \"hierarchical_%s=%lu\", stat->name,\n\t\t\t   mem_cgroup_nr_lru_pages(memcg, stat->lru_mask,\n\t\t\t\t\t\t   true));\n\t\tfor_each_node_state(nid, N_MEMORY)\n\t\t\tseq_printf(m, \" N%d=%lu\", nid,\n\t\t\t\t   mem_cgroup_node_nr_lru_pages(memcg, nid,\n\t\t\t\t\t\t\tstat->lru_mask, true));\n\t\tseq_putc(m, '\\n');\n\t}\n\n\treturn 0;\n}\n#endif  \n\nstatic const unsigned int memcg1_stats[] = {\n\tNR_FILE_PAGES,\n\tNR_ANON_MAPPED,\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tNR_ANON_THPS,\n#endif\n\tNR_SHMEM,\n\tNR_FILE_MAPPED,\n\tNR_FILE_DIRTY,\n\tNR_WRITEBACK,\n\tWORKINGSET_REFAULT_ANON,\n\tWORKINGSET_REFAULT_FILE,\n\tMEMCG_SWAP,\n};\n\nstatic const char *const memcg1_stat_names[] = {\n\t\"cache\",\n\t\"rss\",\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\t\"rss_huge\",\n#endif\n\t\"shmem\",\n\t\"mapped_file\",\n\t\"dirty\",\n\t\"writeback\",\n\t\"workingset_refault_anon\",\n\t\"workingset_refault_file\",\n\t\"swap\",\n};\n\n \nstatic const unsigned int memcg1_events[] = {\n\tPGPGIN,\n\tPGPGOUT,\n\tPGFAULT,\n\tPGMAJFAULT,\n};\n\nstatic void memcg1_stat_format(struct mem_cgroup *memcg, struct seq_buf *s)\n{\n\tunsigned long memory, memsw;\n\tstruct mem_cgroup *mi;\n\tunsigned int i;\n\n\tBUILD_BUG_ON(ARRAY_SIZE(memcg1_stat_names) != ARRAY_SIZE(memcg1_stats));\n\n\tmem_cgroup_flush_stats();\n\n\tfor (i = 0; i < ARRAY_SIZE(memcg1_stats); i++) {\n\t\tunsigned long nr;\n\n\t\tif (memcg1_stats[i] == MEMCG_SWAP && !do_memsw_account())\n\t\t\tcontinue;\n\t\tnr = memcg_page_state_local(memcg, memcg1_stats[i]);\n\t\tseq_buf_printf(s, \"%s %lu\\n\", memcg1_stat_names[i],\n\t\t\t   nr * memcg_page_state_unit(memcg1_stats[i]));\n\t}\n\n\tfor (i = 0; i < ARRAY_SIZE(memcg1_events); i++)\n\t\tseq_buf_printf(s, \"%s %lu\\n\", vm_event_name(memcg1_events[i]),\n\t\t\t       memcg_events_local(memcg, memcg1_events[i]));\n\n\tfor (i = 0; i < NR_LRU_LISTS; i++)\n\t\tseq_buf_printf(s, \"%s %lu\\n\", lru_list_name(i),\n\t\t\t       memcg_page_state_local(memcg, NR_LRU_BASE + i) *\n\t\t\t       PAGE_SIZE);\n\n\t \n\tmemory = memsw = PAGE_COUNTER_MAX;\n\tfor (mi = memcg; mi; mi = parent_mem_cgroup(mi)) {\n\t\tmemory = min(memory, READ_ONCE(mi->memory.max));\n\t\tmemsw = min(memsw, READ_ONCE(mi->memsw.max));\n\t}\n\tseq_buf_printf(s, \"hierarchical_memory_limit %llu\\n\",\n\t\t       (u64)memory * PAGE_SIZE);\n\tif (do_memsw_account())\n\t\tseq_buf_printf(s, \"hierarchical_memsw_limit %llu\\n\",\n\t\t\t       (u64)memsw * PAGE_SIZE);\n\n\tfor (i = 0; i < ARRAY_SIZE(memcg1_stats); i++) {\n\t\tunsigned long nr;\n\n\t\tif (memcg1_stats[i] == MEMCG_SWAP && !do_memsw_account())\n\t\t\tcontinue;\n\t\tnr = memcg_page_state(memcg, memcg1_stats[i]);\n\t\tseq_buf_printf(s, \"total_%s %llu\\n\", memcg1_stat_names[i],\n\t\t\t   (u64)nr * memcg_page_state_unit(memcg1_stats[i]));\n\t}\n\n\tfor (i = 0; i < ARRAY_SIZE(memcg1_events); i++)\n\t\tseq_buf_printf(s, \"total_%s %llu\\n\",\n\t\t\t       vm_event_name(memcg1_events[i]),\n\t\t\t       (u64)memcg_events(memcg, memcg1_events[i]));\n\n\tfor (i = 0; i < NR_LRU_LISTS; i++)\n\t\tseq_buf_printf(s, \"total_%s %llu\\n\", lru_list_name(i),\n\t\t\t       (u64)memcg_page_state(memcg, NR_LRU_BASE + i) *\n\t\t\t       PAGE_SIZE);\n\n#ifdef CONFIG_DEBUG_VM\n\t{\n\t\tpg_data_t *pgdat;\n\t\tstruct mem_cgroup_per_node *mz;\n\t\tunsigned long anon_cost = 0;\n\t\tunsigned long file_cost = 0;\n\n\t\tfor_each_online_pgdat(pgdat) {\n\t\t\tmz = memcg->nodeinfo[pgdat->node_id];\n\n\t\t\tanon_cost += mz->lruvec.anon_cost;\n\t\t\tfile_cost += mz->lruvec.file_cost;\n\t\t}\n\t\tseq_buf_printf(s, \"anon_cost %lu\\n\", anon_cost);\n\t\tseq_buf_printf(s, \"file_cost %lu\\n\", file_cost);\n\t}\n#endif\n}\n\nstatic u64 mem_cgroup_swappiness_read(struct cgroup_subsys_state *css,\n\t\t\t\t      struct cftype *cft)\n{\n\tstruct mem_cgroup *memcg = mem_cgroup_from_css(css);\n\n\treturn mem_cgroup_swappiness(memcg);\n}\n\nstatic int mem_cgroup_swappiness_write(struct cgroup_subsys_state *css,\n\t\t\t\t       struct cftype *cft, u64 val)\n{\n\tstruct mem_cgroup *memcg = mem_cgroup_from_css(css);\n\n\tif (val > 200)\n\t\treturn -EINVAL;\n\n\tif (!mem_cgroup_is_root(memcg))\n\t\tWRITE_ONCE(memcg->swappiness, val);\n\telse\n\t\tWRITE_ONCE(vm_swappiness, val);\n\n\treturn 0;\n}\n\nstatic void __mem_cgroup_threshold(struct mem_cgroup *memcg, bool swap)\n{\n\tstruct mem_cgroup_threshold_ary *t;\n\tunsigned long usage;\n\tint i;\n\n\trcu_read_lock();\n\tif (!swap)\n\t\tt = rcu_dereference(memcg->thresholds.primary);\n\telse\n\t\tt = rcu_dereference(memcg->memsw_thresholds.primary);\n\n\tif (!t)\n\t\tgoto unlock;\n\n\tusage = mem_cgroup_usage(memcg, swap);\n\n\t \n\ti = t->current_threshold;\n\n\t \n\tfor (; i >= 0 && unlikely(t->entries[i].threshold > usage); i--)\n\t\teventfd_signal(t->entries[i].eventfd, 1);\n\n\t \n\ti++;\n\n\t \n\tfor (; i < t->size && unlikely(t->entries[i].threshold <= usage); i++)\n\t\teventfd_signal(t->entries[i].eventfd, 1);\n\n\t \n\tt->current_threshold = i - 1;\nunlock:\n\trcu_read_unlock();\n}\n\nstatic void mem_cgroup_threshold(struct mem_cgroup *memcg)\n{\n\twhile (memcg) {\n\t\t__mem_cgroup_threshold(memcg, false);\n\t\tif (do_memsw_account())\n\t\t\t__mem_cgroup_threshold(memcg, true);\n\n\t\tmemcg = parent_mem_cgroup(memcg);\n\t}\n}\n\nstatic int compare_thresholds(const void *a, const void *b)\n{\n\tconst struct mem_cgroup_threshold *_a = a;\n\tconst struct mem_cgroup_threshold *_b = b;\n\n\tif (_a->threshold > _b->threshold)\n\t\treturn 1;\n\n\tif (_a->threshold < _b->threshold)\n\t\treturn -1;\n\n\treturn 0;\n}\n\nstatic int mem_cgroup_oom_notify_cb(struct mem_cgroup *memcg)\n{\n\tstruct mem_cgroup_eventfd_list *ev;\n\n\tspin_lock(&memcg_oom_lock);\n\n\tlist_for_each_entry(ev, &memcg->oom_notify, list)\n\t\teventfd_signal(ev->eventfd, 1);\n\n\tspin_unlock(&memcg_oom_lock);\n\treturn 0;\n}\n\nstatic void mem_cgroup_oom_notify(struct mem_cgroup *memcg)\n{\n\tstruct mem_cgroup *iter;\n\n\tfor_each_mem_cgroup_tree(iter, memcg)\n\t\tmem_cgroup_oom_notify_cb(iter);\n}\n\nstatic int __mem_cgroup_usage_register_event(struct mem_cgroup *memcg,\n\tstruct eventfd_ctx *eventfd, const char *args, enum res_type type)\n{\n\tstruct mem_cgroup_thresholds *thresholds;\n\tstruct mem_cgroup_threshold_ary *new;\n\tunsigned long threshold;\n\tunsigned long usage;\n\tint i, size, ret;\n\n\tret = page_counter_memparse(args, \"-1\", &threshold);\n\tif (ret)\n\t\treturn ret;\n\n\tmutex_lock(&memcg->thresholds_lock);\n\n\tif (type == _MEM) {\n\t\tthresholds = &memcg->thresholds;\n\t\tusage = mem_cgroup_usage(memcg, false);\n\t} else if (type == _MEMSWAP) {\n\t\tthresholds = &memcg->memsw_thresholds;\n\t\tusage = mem_cgroup_usage(memcg, true);\n\t} else\n\t\tBUG();\n\n\t \n\tif (thresholds->primary)\n\t\t__mem_cgroup_threshold(memcg, type == _MEMSWAP);\n\n\tsize = thresholds->primary ? thresholds->primary->size + 1 : 1;\n\n\t \n\tnew = kmalloc(struct_size(new, entries, size), GFP_KERNEL);\n\tif (!new) {\n\t\tret = -ENOMEM;\n\t\tgoto unlock;\n\t}\n\tnew->size = size;\n\n\t \n\tif (thresholds->primary)\n\t\tmemcpy(new->entries, thresholds->primary->entries,\n\t\t       flex_array_size(new, entries, size - 1));\n\n\t \n\tnew->entries[size - 1].eventfd = eventfd;\n\tnew->entries[size - 1].threshold = threshold;\n\n\t \n\tsort(new->entries, size, sizeof(*new->entries),\n\t\t\tcompare_thresholds, NULL);\n\n\t \n\tnew->current_threshold = -1;\n\tfor (i = 0; i < size; i++) {\n\t\tif (new->entries[i].threshold <= usage) {\n\t\t\t \n\t\t\t++new->current_threshold;\n\t\t} else\n\t\t\tbreak;\n\t}\n\n\t \n\tkfree(thresholds->spare);\n\tthresholds->spare = thresholds->primary;\n\n\trcu_assign_pointer(thresholds->primary, new);\n\n\t \n\tsynchronize_rcu();\n\nunlock:\n\tmutex_unlock(&memcg->thresholds_lock);\n\n\treturn ret;\n}\n\nstatic int mem_cgroup_usage_register_event(struct mem_cgroup *memcg,\n\tstruct eventfd_ctx *eventfd, const char *args)\n{\n\treturn __mem_cgroup_usage_register_event(memcg, eventfd, args, _MEM);\n}\n\nstatic int memsw_cgroup_usage_register_event(struct mem_cgroup *memcg,\n\tstruct eventfd_ctx *eventfd, const char *args)\n{\n\treturn __mem_cgroup_usage_register_event(memcg, eventfd, args, _MEMSWAP);\n}\n\nstatic void __mem_cgroup_usage_unregister_event(struct mem_cgroup *memcg,\n\tstruct eventfd_ctx *eventfd, enum res_type type)\n{\n\tstruct mem_cgroup_thresholds *thresholds;\n\tstruct mem_cgroup_threshold_ary *new;\n\tunsigned long usage;\n\tint i, j, size, entries;\n\n\tmutex_lock(&memcg->thresholds_lock);\n\n\tif (type == _MEM) {\n\t\tthresholds = &memcg->thresholds;\n\t\tusage = mem_cgroup_usage(memcg, false);\n\t} else if (type == _MEMSWAP) {\n\t\tthresholds = &memcg->memsw_thresholds;\n\t\tusage = mem_cgroup_usage(memcg, true);\n\t} else\n\t\tBUG();\n\n\tif (!thresholds->primary)\n\t\tgoto unlock;\n\n\t \n\t__mem_cgroup_threshold(memcg, type == _MEMSWAP);\n\n\t \n\tsize = entries = 0;\n\tfor (i = 0; i < thresholds->primary->size; i++) {\n\t\tif (thresholds->primary->entries[i].eventfd != eventfd)\n\t\t\tsize++;\n\t\telse\n\t\t\tentries++;\n\t}\n\n\tnew = thresholds->spare;\n\n\t \n\tif (!entries)\n\t\tgoto unlock;\n\n\t \n\tif (!size) {\n\t\tkfree(new);\n\t\tnew = NULL;\n\t\tgoto swap_buffers;\n\t}\n\n\tnew->size = size;\n\n\t \n\tnew->current_threshold = -1;\n\tfor (i = 0, j = 0; i < thresholds->primary->size; i++) {\n\t\tif (thresholds->primary->entries[i].eventfd == eventfd)\n\t\t\tcontinue;\n\n\t\tnew->entries[j] = thresholds->primary->entries[i];\n\t\tif (new->entries[j].threshold <= usage) {\n\t\t\t \n\t\t\t++new->current_threshold;\n\t\t}\n\t\tj++;\n\t}\n\nswap_buffers:\n\t \n\tthresholds->spare = thresholds->primary;\n\n\trcu_assign_pointer(thresholds->primary, new);\n\n\t \n\tsynchronize_rcu();\n\n\t \n\tif (!new) {\n\t\tkfree(thresholds->spare);\n\t\tthresholds->spare = NULL;\n\t}\nunlock:\n\tmutex_unlock(&memcg->thresholds_lock);\n}\n\nstatic void mem_cgroup_usage_unregister_event(struct mem_cgroup *memcg,\n\tstruct eventfd_ctx *eventfd)\n{\n\treturn __mem_cgroup_usage_unregister_event(memcg, eventfd, _MEM);\n}\n\nstatic void memsw_cgroup_usage_unregister_event(struct mem_cgroup *memcg,\n\tstruct eventfd_ctx *eventfd)\n{\n\treturn __mem_cgroup_usage_unregister_event(memcg, eventfd, _MEMSWAP);\n}\n\nstatic int mem_cgroup_oom_register_event(struct mem_cgroup *memcg,\n\tstruct eventfd_ctx *eventfd, const char *args)\n{\n\tstruct mem_cgroup_eventfd_list *event;\n\n\tevent = kmalloc(sizeof(*event),\tGFP_KERNEL);\n\tif (!event)\n\t\treturn -ENOMEM;\n\n\tspin_lock(&memcg_oom_lock);\n\n\tevent->eventfd = eventfd;\n\tlist_add(&event->list, &memcg->oom_notify);\n\n\t \n\tif (memcg->under_oom)\n\t\teventfd_signal(eventfd, 1);\n\tspin_unlock(&memcg_oom_lock);\n\n\treturn 0;\n}\n\nstatic void mem_cgroup_oom_unregister_event(struct mem_cgroup *memcg,\n\tstruct eventfd_ctx *eventfd)\n{\n\tstruct mem_cgroup_eventfd_list *ev, *tmp;\n\n\tspin_lock(&memcg_oom_lock);\n\n\tlist_for_each_entry_safe(ev, tmp, &memcg->oom_notify, list) {\n\t\tif (ev->eventfd == eventfd) {\n\t\t\tlist_del(&ev->list);\n\t\t\tkfree(ev);\n\t\t}\n\t}\n\n\tspin_unlock(&memcg_oom_lock);\n}\n\nstatic int mem_cgroup_oom_control_read(struct seq_file *sf, void *v)\n{\n\tstruct mem_cgroup *memcg = mem_cgroup_from_seq(sf);\n\n\tseq_printf(sf, \"oom_kill_disable %d\\n\", READ_ONCE(memcg->oom_kill_disable));\n\tseq_printf(sf, \"under_oom %d\\n\", (bool)memcg->under_oom);\n\tseq_printf(sf, \"oom_kill %lu\\n\",\n\t\t   atomic_long_read(&memcg->memory_events[MEMCG_OOM_KILL]));\n\treturn 0;\n}\n\nstatic int mem_cgroup_oom_control_write(struct cgroup_subsys_state *css,\n\tstruct cftype *cft, u64 val)\n{\n\tstruct mem_cgroup *memcg = mem_cgroup_from_css(css);\n\n\t \n\tif (mem_cgroup_is_root(memcg) || !((val == 0) || (val == 1)))\n\t\treturn -EINVAL;\n\n\tWRITE_ONCE(memcg->oom_kill_disable, val);\n\tif (!val)\n\t\tmemcg_oom_recover(memcg);\n\n\treturn 0;\n}\n\n#ifdef CONFIG_CGROUP_WRITEBACK\n\n#include <trace/events/writeback.h>\n\nstatic int memcg_wb_domain_init(struct mem_cgroup *memcg, gfp_t gfp)\n{\n\treturn wb_domain_init(&memcg->cgwb_domain, gfp);\n}\n\nstatic void memcg_wb_domain_exit(struct mem_cgroup *memcg)\n{\n\twb_domain_exit(&memcg->cgwb_domain);\n}\n\nstatic void memcg_wb_domain_size_changed(struct mem_cgroup *memcg)\n{\n\twb_domain_size_changed(&memcg->cgwb_domain);\n}\n\nstruct wb_domain *mem_cgroup_wb_domain(struct bdi_writeback *wb)\n{\n\tstruct mem_cgroup *memcg = mem_cgroup_from_css(wb->memcg_css);\n\n\tif (!memcg->css.parent)\n\t\treturn NULL;\n\n\treturn &memcg->cgwb_domain;\n}\n\n \nvoid mem_cgroup_wb_stats(struct bdi_writeback *wb, unsigned long *pfilepages,\n\t\t\t unsigned long *pheadroom, unsigned long *pdirty,\n\t\t\t unsigned long *pwriteback)\n{\n\tstruct mem_cgroup *memcg = mem_cgroup_from_css(wb->memcg_css);\n\tstruct mem_cgroup *parent;\n\n\tmem_cgroup_flush_stats();\n\n\t*pdirty = memcg_page_state(memcg, NR_FILE_DIRTY);\n\t*pwriteback = memcg_page_state(memcg, NR_WRITEBACK);\n\t*pfilepages = memcg_page_state(memcg, NR_INACTIVE_FILE) +\n\t\t\tmemcg_page_state(memcg, NR_ACTIVE_FILE);\n\n\t*pheadroom = PAGE_COUNTER_MAX;\n\twhile ((parent = parent_mem_cgroup(memcg))) {\n\t\tunsigned long ceiling = min(READ_ONCE(memcg->memory.max),\n\t\t\t\t\t    READ_ONCE(memcg->memory.high));\n\t\tunsigned long used = page_counter_read(&memcg->memory);\n\n\t\t*pheadroom = min(*pheadroom, ceiling - min(ceiling, used));\n\t\tmemcg = parent;\n\t}\n}\n\n \nvoid mem_cgroup_track_foreign_dirty_slowpath(struct folio *folio,\n\t\t\t\t\t     struct bdi_writeback *wb)\n{\n\tstruct mem_cgroup *memcg = folio_memcg(folio);\n\tstruct memcg_cgwb_frn *frn;\n\tu64 now = get_jiffies_64();\n\tu64 oldest_at = now;\n\tint oldest = -1;\n\tint i;\n\n\ttrace_track_foreign_dirty(folio, wb);\n\n\t \n\tfor (i = 0; i < MEMCG_CGWB_FRN_CNT; i++) {\n\t\tfrn = &memcg->cgwb_frn[i];\n\t\tif (frn->bdi_id == wb->bdi->id &&\n\t\t    frn->memcg_id == wb->memcg_css->id)\n\t\t\tbreak;\n\t\tif (time_before64(frn->at, oldest_at) &&\n\t\t    atomic_read(&frn->done.cnt) == 1) {\n\t\t\toldest = i;\n\t\t\toldest_at = frn->at;\n\t\t}\n\t}\n\n\tif (i < MEMCG_CGWB_FRN_CNT) {\n\t\t \n\t\tunsigned long update_intv =\n\t\t\tmin_t(unsigned long, HZ,\n\t\t\t      msecs_to_jiffies(dirty_expire_interval * 10) / 8);\n\n\t\tif (time_before64(frn->at, now - update_intv))\n\t\t\tfrn->at = now;\n\t} else if (oldest >= 0) {\n\t\t \n\t\tfrn = &memcg->cgwb_frn[oldest];\n\t\tfrn->bdi_id = wb->bdi->id;\n\t\tfrn->memcg_id = wb->memcg_css->id;\n\t\tfrn->at = now;\n\t}\n}\n\n \nvoid mem_cgroup_flush_foreign(struct bdi_writeback *wb)\n{\n\tstruct mem_cgroup *memcg = mem_cgroup_from_css(wb->memcg_css);\n\tunsigned long intv = msecs_to_jiffies(dirty_expire_interval * 10);\n\tu64 now = jiffies_64;\n\tint i;\n\n\tfor (i = 0; i < MEMCG_CGWB_FRN_CNT; i++) {\n\t\tstruct memcg_cgwb_frn *frn = &memcg->cgwb_frn[i];\n\n\t\t \n\t\tif (time_after64(frn->at, now - intv) &&\n\t\t    atomic_read(&frn->done.cnt) == 1) {\n\t\t\tfrn->at = 0;\n\t\t\ttrace_flush_foreign(wb, frn->bdi_id, frn->memcg_id);\n\t\t\tcgroup_writeback_by_id(frn->bdi_id, frn->memcg_id,\n\t\t\t\t\t       WB_REASON_FOREIGN_FLUSH,\n\t\t\t\t\t       &frn->done);\n\t\t}\n\t}\n}\n\n#else\t \n\nstatic int memcg_wb_domain_init(struct mem_cgroup *memcg, gfp_t gfp)\n{\n\treturn 0;\n}\n\nstatic void memcg_wb_domain_exit(struct mem_cgroup *memcg)\n{\n}\n\nstatic void memcg_wb_domain_size_changed(struct mem_cgroup *memcg)\n{\n}\n\n#endif\t \n\n \n\n \nstatic void memcg_event_remove(struct work_struct *work)\n{\n\tstruct mem_cgroup_event *event =\n\t\tcontainer_of(work, struct mem_cgroup_event, remove);\n\tstruct mem_cgroup *memcg = event->memcg;\n\n\tremove_wait_queue(event->wqh, &event->wait);\n\n\tevent->unregister_event(memcg, event->eventfd);\n\n\t \n\teventfd_signal(event->eventfd, 1);\n\n\teventfd_ctx_put(event->eventfd);\n\tkfree(event);\n\tcss_put(&memcg->css);\n}\n\n \nstatic int memcg_event_wake(wait_queue_entry_t *wait, unsigned mode,\n\t\t\t    int sync, void *key)\n{\n\tstruct mem_cgroup_event *event =\n\t\tcontainer_of(wait, struct mem_cgroup_event, wait);\n\tstruct mem_cgroup *memcg = event->memcg;\n\t__poll_t flags = key_to_poll(key);\n\n\tif (flags & EPOLLHUP) {\n\t\t \n\t\tspin_lock(&memcg->event_list_lock);\n\t\tif (!list_empty(&event->list)) {\n\t\t\tlist_del_init(&event->list);\n\t\t\t \n\t\t\tschedule_work(&event->remove);\n\t\t}\n\t\tspin_unlock(&memcg->event_list_lock);\n\t}\n\n\treturn 0;\n}\n\nstatic void memcg_event_ptable_queue_proc(struct file *file,\n\t\twait_queue_head_t *wqh, poll_table *pt)\n{\n\tstruct mem_cgroup_event *event =\n\t\tcontainer_of(pt, struct mem_cgroup_event, pt);\n\n\tevent->wqh = wqh;\n\tadd_wait_queue(wqh, &event->wait);\n}\n\n \nstatic ssize_t memcg_write_event_control(struct kernfs_open_file *of,\n\t\t\t\t\t char *buf, size_t nbytes, loff_t off)\n{\n\tstruct cgroup_subsys_state *css = of_css(of);\n\tstruct mem_cgroup *memcg = mem_cgroup_from_css(css);\n\tstruct mem_cgroup_event *event;\n\tstruct cgroup_subsys_state *cfile_css;\n\tunsigned int efd, cfd;\n\tstruct fd efile;\n\tstruct fd cfile;\n\tstruct dentry *cdentry;\n\tconst char *name;\n\tchar *endp;\n\tint ret;\n\n\tif (IS_ENABLED(CONFIG_PREEMPT_RT))\n\t\treturn -EOPNOTSUPP;\n\n\tbuf = strstrip(buf);\n\n\tefd = simple_strtoul(buf, &endp, 10);\n\tif (*endp != ' ')\n\t\treturn -EINVAL;\n\tbuf = endp + 1;\n\n\tcfd = simple_strtoul(buf, &endp, 10);\n\tif ((*endp != ' ') && (*endp != '\\0'))\n\t\treturn -EINVAL;\n\tbuf = endp + 1;\n\n\tevent = kzalloc(sizeof(*event), GFP_KERNEL);\n\tif (!event)\n\t\treturn -ENOMEM;\n\n\tevent->memcg = memcg;\n\tINIT_LIST_HEAD(&event->list);\n\tinit_poll_funcptr(&event->pt, memcg_event_ptable_queue_proc);\n\tinit_waitqueue_func_entry(&event->wait, memcg_event_wake);\n\tINIT_WORK(&event->remove, memcg_event_remove);\n\n\tefile = fdget(efd);\n\tif (!efile.file) {\n\t\tret = -EBADF;\n\t\tgoto out_kfree;\n\t}\n\n\tevent->eventfd = eventfd_ctx_fileget(efile.file);\n\tif (IS_ERR(event->eventfd)) {\n\t\tret = PTR_ERR(event->eventfd);\n\t\tgoto out_put_efile;\n\t}\n\n\tcfile = fdget(cfd);\n\tif (!cfile.file) {\n\t\tret = -EBADF;\n\t\tgoto out_put_eventfd;\n\t}\n\n\t \n\t \n\tret = file_permission(cfile.file, MAY_READ);\n\tif (ret < 0)\n\t\tgoto out_put_cfile;\n\n\t \n\tcdentry = cfile.file->f_path.dentry;\n\tif (cdentry->d_sb->s_type != &cgroup_fs_type || !d_is_reg(cdentry)) {\n\t\tret = -EINVAL;\n\t\tgoto out_put_cfile;\n\t}\n\n\t \n\tname = cdentry->d_name.name;\n\n\tif (!strcmp(name, \"memory.usage_in_bytes\")) {\n\t\tevent->register_event = mem_cgroup_usage_register_event;\n\t\tevent->unregister_event = mem_cgroup_usage_unregister_event;\n\t} else if (!strcmp(name, \"memory.oom_control\")) {\n\t\tevent->register_event = mem_cgroup_oom_register_event;\n\t\tevent->unregister_event = mem_cgroup_oom_unregister_event;\n\t} else if (!strcmp(name, \"memory.pressure_level\")) {\n\t\tevent->register_event = vmpressure_register_event;\n\t\tevent->unregister_event = vmpressure_unregister_event;\n\t} else if (!strcmp(name, \"memory.memsw.usage_in_bytes\")) {\n\t\tevent->register_event = memsw_cgroup_usage_register_event;\n\t\tevent->unregister_event = memsw_cgroup_usage_unregister_event;\n\t} else {\n\t\tret = -EINVAL;\n\t\tgoto out_put_cfile;\n\t}\n\n\t \n\tcfile_css = css_tryget_online_from_dir(cdentry->d_parent,\n\t\t\t\t\t       &memory_cgrp_subsys);\n\tret = -EINVAL;\n\tif (IS_ERR(cfile_css))\n\t\tgoto out_put_cfile;\n\tif (cfile_css != css) {\n\t\tcss_put(cfile_css);\n\t\tgoto out_put_cfile;\n\t}\n\n\tret = event->register_event(memcg, event->eventfd, buf);\n\tif (ret)\n\t\tgoto out_put_css;\n\n\tvfs_poll(efile.file, &event->pt);\n\n\tspin_lock_irq(&memcg->event_list_lock);\n\tlist_add(&event->list, &memcg->event_list);\n\tspin_unlock_irq(&memcg->event_list_lock);\n\n\tfdput(cfile);\n\tfdput(efile);\n\n\treturn nbytes;\n\nout_put_css:\n\tcss_put(css);\nout_put_cfile:\n\tfdput(cfile);\nout_put_eventfd:\n\teventfd_ctx_put(event->eventfd);\nout_put_efile:\n\tfdput(efile);\nout_kfree:\n\tkfree(event);\n\n\treturn ret;\n}\n\n#if defined(CONFIG_MEMCG_KMEM) && (defined(CONFIG_SLAB) || defined(CONFIG_SLUB_DEBUG))\nstatic int mem_cgroup_slab_show(struct seq_file *m, void *p)\n{\n\t \n\treturn 0;\n}\n#endif\n\nstatic int memory_stat_show(struct seq_file *m, void *v);\n\nstatic struct cftype mem_cgroup_legacy_files[] = {\n\t{\n\t\t.name = \"usage_in_bytes\",\n\t\t.private = MEMFILE_PRIVATE(_MEM, RES_USAGE),\n\t\t.read_u64 = mem_cgroup_read_u64,\n\t},\n\t{\n\t\t.name = \"max_usage_in_bytes\",\n\t\t.private = MEMFILE_PRIVATE(_MEM, RES_MAX_USAGE),\n\t\t.write = mem_cgroup_reset,\n\t\t.read_u64 = mem_cgroup_read_u64,\n\t},\n\t{\n\t\t.name = \"limit_in_bytes\",\n\t\t.private = MEMFILE_PRIVATE(_MEM, RES_LIMIT),\n\t\t.write = mem_cgroup_write,\n\t\t.read_u64 = mem_cgroup_read_u64,\n\t},\n\t{\n\t\t.name = \"soft_limit_in_bytes\",\n\t\t.private = MEMFILE_PRIVATE(_MEM, RES_SOFT_LIMIT),\n\t\t.write = mem_cgroup_write,\n\t\t.read_u64 = mem_cgroup_read_u64,\n\t},\n\t{\n\t\t.name = \"failcnt\",\n\t\t.private = MEMFILE_PRIVATE(_MEM, RES_FAILCNT),\n\t\t.write = mem_cgroup_reset,\n\t\t.read_u64 = mem_cgroup_read_u64,\n\t},\n\t{\n\t\t.name = \"stat\",\n\t\t.seq_show = memory_stat_show,\n\t},\n\t{\n\t\t.name = \"force_empty\",\n\t\t.write = mem_cgroup_force_empty_write,\n\t},\n\t{\n\t\t.name = \"use_hierarchy\",\n\t\t.write_u64 = mem_cgroup_hierarchy_write,\n\t\t.read_u64 = mem_cgroup_hierarchy_read,\n\t},\n\t{\n\t\t.name = \"cgroup.event_control\",\t\t \n\t\t.write = memcg_write_event_control,\n\t\t.flags = CFTYPE_NO_PREFIX | CFTYPE_WORLD_WRITABLE,\n\t},\n\t{\n\t\t.name = \"swappiness\",\n\t\t.read_u64 = mem_cgroup_swappiness_read,\n\t\t.write_u64 = mem_cgroup_swappiness_write,\n\t},\n\t{\n\t\t.name = \"move_charge_at_immigrate\",\n\t\t.read_u64 = mem_cgroup_move_charge_read,\n\t\t.write_u64 = mem_cgroup_move_charge_write,\n\t},\n\t{\n\t\t.name = \"oom_control\",\n\t\t.seq_show = mem_cgroup_oom_control_read,\n\t\t.write_u64 = mem_cgroup_oom_control_write,\n\t},\n\t{\n\t\t.name = \"pressure_level\",\n\t\t.seq_show = mem_cgroup_dummy_seq_show,\n\t},\n#ifdef CONFIG_NUMA\n\t{\n\t\t.name = \"numa_stat\",\n\t\t.seq_show = memcg_numa_stat_show,\n\t},\n#endif\n\t{\n\t\t.name = \"kmem.limit_in_bytes\",\n\t\t.private = MEMFILE_PRIVATE(_KMEM, RES_LIMIT),\n\t\t.write = mem_cgroup_write,\n\t\t.read_u64 = mem_cgroup_read_u64,\n\t},\n\t{\n\t\t.name = \"kmem.usage_in_bytes\",\n\t\t.private = MEMFILE_PRIVATE(_KMEM, RES_USAGE),\n\t\t.read_u64 = mem_cgroup_read_u64,\n\t},\n\t{\n\t\t.name = \"kmem.failcnt\",\n\t\t.private = MEMFILE_PRIVATE(_KMEM, RES_FAILCNT),\n\t\t.write = mem_cgroup_reset,\n\t\t.read_u64 = mem_cgroup_read_u64,\n\t},\n\t{\n\t\t.name = \"kmem.max_usage_in_bytes\",\n\t\t.private = MEMFILE_PRIVATE(_KMEM, RES_MAX_USAGE),\n\t\t.write = mem_cgroup_reset,\n\t\t.read_u64 = mem_cgroup_read_u64,\n\t},\n#if defined(CONFIG_MEMCG_KMEM) && \\\n\t(defined(CONFIG_SLAB) || defined(CONFIG_SLUB_DEBUG))\n\t{\n\t\t.name = \"kmem.slabinfo\",\n\t\t.seq_show = mem_cgroup_slab_show,\n\t},\n#endif\n\t{\n\t\t.name = \"kmem.tcp.limit_in_bytes\",\n\t\t.private = MEMFILE_PRIVATE(_TCP, RES_LIMIT),\n\t\t.write = mem_cgroup_write,\n\t\t.read_u64 = mem_cgroup_read_u64,\n\t},\n\t{\n\t\t.name = \"kmem.tcp.usage_in_bytes\",\n\t\t.private = MEMFILE_PRIVATE(_TCP, RES_USAGE),\n\t\t.read_u64 = mem_cgroup_read_u64,\n\t},\n\t{\n\t\t.name = \"kmem.tcp.failcnt\",\n\t\t.private = MEMFILE_PRIVATE(_TCP, RES_FAILCNT),\n\t\t.write = mem_cgroup_reset,\n\t\t.read_u64 = mem_cgroup_read_u64,\n\t},\n\t{\n\t\t.name = \"kmem.tcp.max_usage_in_bytes\",\n\t\t.private = MEMFILE_PRIVATE(_TCP, RES_MAX_USAGE),\n\t\t.write = mem_cgroup_reset,\n\t\t.read_u64 = mem_cgroup_read_u64,\n\t},\n\t{ },\t \n};\n\n \n\n#define MEM_CGROUP_ID_MAX\t((1UL << MEM_CGROUP_ID_SHIFT) - 1)\nstatic DEFINE_IDR(mem_cgroup_idr);\n\nstatic void mem_cgroup_id_remove(struct mem_cgroup *memcg)\n{\n\tif (memcg->id.id > 0) {\n\t\tidr_remove(&mem_cgroup_idr, memcg->id.id);\n\t\tmemcg->id.id = 0;\n\t}\n}\n\nstatic void __maybe_unused mem_cgroup_id_get_many(struct mem_cgroup *memcg,\n\t\t\t\t\t\t  unsigned int n)\n{\n\trefcount_add(n, &memcg->id.ref);\n}\n\nstatic void mem_cgroup_id_put_many(struct mem_cgroup *memcg, unsigned int n)\n{\n\tif (refcount_sub_and_test(n, &memcg->id.ref)) {\n\t\tmem_cgroup_id_remove(memcg);\n\n\t\t \n\t\tcss_put(&memcg->css);\n\t}\n}\n\nstatic inline void mem_cgroup_id_put(struct mem_cgroup *memcg)\n{\n\tmem_cgroup_id_put_many(memcg, 1);\n}\n\n \nstruct mem_cgroup *mem_cgroup_from_id(unsigned short id)\n{\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\treturn idr_find(&mem_cgroup_idr, id);\n}\n\n#ifdef CONFIG_SHRINKER_DEBUG\nstruct mem_cgroup *mem_cgroup_get_from_ino(unsigned long ino)\n{\n\tstruct cgroup *cgrp;\n\tstruct cgroup_subsys_state *css;\n\tstruct mem_cgroup *memcg;\n\n\tcgrp = cgroup_get_from_id(ino);\n\tif (IS_ERR(cgrp))\n\t\treturn ERR_CAST(cgrp);\n\n\tcss = cgroup_get_e_css(cgrp, &memory_cgrp_subsys);\n\tif (css)\n\t\tmemcg = container_of(css, struct mem_cgroup, css);\n\telse\n\t\tmemcg = ERR_PTR(-ENOENT);\n\n\tcgroup_put(cgrp);\n\n\treturn memcg;\n}\n#endif\n\nstatic int alloc_mem_cgroup_per_node_info(struct mem_cgroup *memcg, int node)\n{\n\tstruct mem_cgroup_per_node *pn;\n\n\tpn = kzalloc_node(sizeof(*pn), GFP_KERNEL, node);\n\tif (!pn)\n\t\treturn 1;\n\n\tpn->lruvec_stats_percpu = alloc_percpu_gfp(struct lruvec_stats_percpu,\n\t\t\t\t\t\t   GFP_KERNEL_ACCOUNT);\n\tif (!pn->lruvec_stats_percpu) {\n\t\tkfree(pn);\n\t\treturn 1;\n\t}\n\n\tlruvec_init(&pn->lruvec);\n\tpn->memcg = memcg;\n\n\tmemcg->nodeinfo[node] = pn;\n\treturn 0;\n}\n\nstatic void free_mem_cgroup_per_node_info(struct mem_cgroup *memcg, int node)\n{\n\tstruct mem_cgroup_per_node *pn = memcg->nodeinfo[node];\n\n\tif (!pn)\n\t\treturn;\n\n\tfree_percpu(pn->lruvec_stats_percpu);\n\tkfree(pn);\n}\n\nstatic void __mem_cgroup_free(struct mem_cgroup *memcg)\n{\n\tint node;\n\n\tfor_each_node(node)\n\t\tfree_mem_cgroup_per_node_info(memcg, node);\n\tkfree(memcg->vmstats);\n\tfree_percpu(memcg->vmstats_percpu);\n\tkfree(memcg);\n}\n\nstatic void mem_cgroup_free(struct mem_cgroup *memcg)\n{\n\tlru_gen_exit_memcg(memcg);\n\tmemcg_wb_domain_exit(memcg);\n\t__mem_cgroup_free(memcg);\n}\n\nstatic struct mem_cgroup *mem_cgroup_alloc(void)\n{\n\tstruct mem_cgroup *memcg;\n\tint node;\n\tint __maybe_unused i;\n\tlong error = -ENOMEM;\n\n\tmemcg = kzalloc(struct_size(memcg, nodeinfo, nr_node_ids), GFP_KERNEL);\n\tif (!memcg)\n\t\treturn ERR_PTR(error);\n\n\tmemcg->id.id = idr_alloc(&mem_cgroup_idr, NULL,\n\t\t\t\t 1, MEM_CGROUP_ID_MAX + 1, GFP_KERNEL);\n\tif (memcg->id.id < 0) {\n\t\terror = memcg->id.id;\n\t\tgoto fail;\n\t}\n\n\tmemcg->vmstats = kzalloc(sizeof(struct memcg_vmstats), GFP_KERNEL);\n\tif (!memcg->vmstats)\n\t\tgoto fail;\n\n\tmemcg->vmstats_percpu = alloc_percpu_gfp(struct memcg_vmstats_percpu,\n\t\t\t\t\t\t GFP_KERNEL_ACCOUNT);\n\tif (!memcg->vmstats_percpu)\n\t\tgoto fail;\n\n\tfor_each_node(node)\n\t\tif (alloc_mem_cgroup_per_node_info(memcg, node))\n\t\t\tgoto fail;\n\n\tif (memcg_wb_domain_init(memcg, GFP_KERNEL))\n\t\tgoto fail;\n\n\tINIT_WORK(&memcg->high_work, high_work_func);\n\tINIT_LIST_HEAD(&memcg->oom_notify);\n\tmutex_init(&memcg->thresholds_lock);\n\tspin_lock_init(&memcg->move_lock);\n\tvmpressure_init(&memcg->vmpressure);\n\tINIT_LIST_HEAD(&memcg->event_list);\n\tspin_lock_init(&memcg->event_list_lock);\n\tmemcg->socket_pressure = jiffies;\n#ifdef CONFIG_MEMCG_KMEM\n\tmemcg->kmemcg_id = -1;\n\tINIT_LIST_HEAD(&memcg->objcg_list);\n#endif\n#ifdef CONFIG_CGROUP_WRITEBACK\n\tINIT_LIST_HEAD(&memcg->cgwb_list);\n\tfor (i = 0; i < MEMCG_CGWB_FRN_CNT; i++)\n\t\tmemcg->cgwb_frn[i].done =\n\t\t\t__WB_COMPLETION_INIT(&memcg_cgwb_frn_waitq);\n#endif\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tspin_lock_init(&memcg->deferred_split_queue.split_queue_lock);\n\tINIT_LIST_HEAD(&memcg->deferred_split_queue.split_queue);\n\tmemcg->deferred_split_queue.split_queue_len = 0;\n#endif\n\tlru_gen_init_memcg(memcg);\n\treturn memcg;\nfail:\n\tmem_cgroup_id_remove(memcg);\n\t__mem_cgroup_free(memcg);\n\treturn ERR_PTR(error);\n}\n\nstatic struct cgroup_subsys_state * __ref\nmem_cgroup_css_alloc(struct cgroup_subsys_state *parent_css)\n{\n\tstruct mem_cgroup *parent = mem_cgroup_from_css(parent_css);\n\tstruct mem_cgroup *memcg, *old_memcg;\n\n\told_memcg = set_active_memcg(parent);\n\tmemcg = mem_cgroup_alloc();\n\tset_active_memcg(old_memcg);\n\tif (IS_ERR(memcg))\n\t\treturn ERR_CAST(memcg);\n\n\tpage_counter_set_high(&memcg->memory, PAGE_COUNTER_MAX);\n\tWRITE_ONCE(memcg->soft_limit, PAGE_COUNTER_MAX);\n#if defined(CONFIG_MEMCG_KMEM) && defined(CONFIG_ZSWAP)\n\tmemcg->zswap_max = PAGE_COUNTER_MAX;\n#endif\n\tpage_counter_set_high(&memcg->swap, PAGE_COUNTER_MAX);\n\tif (parent) {\n\t\tWRITE_ONCE(memcg->swappiness, mem_cgroup_swappiness(parent));\n\t\tWRITE_ONCE(memcg->oom_kill_disable, READ_ONCE(parent->oom_kill_disable));\n\n\t\tpage_counter_init(&memcg->memory, &parent->memory);\n\t\tpage_counter_init(&memcg->swap, &parent->swap);\n\t\tpage_counter_init(&memcg->kmem, &parent->kmem);\n\t\tpage_counter_init(&memcg->tcpmem, &parent->tcpmem);\n\t} else {\n\t\tinit_memcg_events();\n\t\tpage_counter_init(&memcg->memory, NULL);\n\t\tpage_counter_init(&memcg->swap, NULL);\n\t\tpage_counter_init(&memcg->kmem, NULL);\n\t\tpage_counter_init(&memcg->tcpmem, NULL);\n\n\t\troot_mem_cgroup = memcg;\n\t\treturn &memcg->css;\n\t}\n\n\tif (cgroup_subsys_on_dfl(memory_cgrp_subsys) && !cgroup_memory_nosocket)\n\t\tstatic_branch_inc(&memcg_sockets_enabled_key);\n\n#if defined(CONFIG_MEMCG_KMEM)\n\tif (!cgroup_memory_nobpf)\n\t\tstatic_branch_inc(&memcg_bpf_enabled_key);\n#endif\n\n\treturn &memcg->css;\n}\n\nstatic int mem_cgroup_css_online(struct cgroup_subsys_state *css)\n{\n\tstruct mem_cgroup *memcg = mem_cgroup_from_css(css);\n\n\tif (memcg_online_kmem(memcg))\n\t\tgoto remove_id;\n\n\t \n\tif (alloc_shrinker_info(memcg))\n\t\tgoto offline_kmem;\n\n\tif (unlikely(mem_cgroup_is_root(memcg)))\n\t\tqueue_delayed_work(system_unbound_wq, &stats_flush_dwork,\n\t\t\t\t   FLUSH_TIME);\n\tlru_gen_online_memcg(memcg);\n\n\t \n\trefcount_set(&memcg->id.ref, 1);\n\tcss_get(css);\n\n\t \n\tidr_replace(&mem_cgroup_idr, memcg, memcg->id.id);\n\n\treturn 0;\noffline_kmem:\n\tmemcg_offline_kmem(memcg);\nremove_id:\n\tmem_cgroup_id_remove(memcg);\n\treturn -ENOMEM;\n}\n\nstatic void mem_cgroup_css_offline(struct cgroup_subsys_state *css)\n{\n\tstruct mem_cgroup *memcg = mem_cgroup_from_css(css);\n\tstruct mem_cgroup_event *event, *tmp;\n\n\t \n\tspin_lock_irq(&memcg->event_list_lock);\n\tlist_for_each_entry_safe(event, tmp, &memcg->event_list, list) {\n\t\tlist_del_init(&event->list);\n\t\tschedule_work(&event->remove);\n\t}\n\tspin_unlock_irq(&memcg->event_list_lock);\n\n\tpage_counter_set_min(&memcg->memory, 0);\n\tpage_counter_set_low(&memcg->memory, 0);\n\n\tmemcg_offline_kmem(memcg);\n\treparent_shrinker_deferred(memcg);\n\twb_memcg_offline(memcg);\n\tlru_gen_offline_memcg(memcg);\n\n\tdrain_all_stock(memcg);\n\n\tmem_cgroup_id_put(memcg);\n}\n\nstatic void mem_cgroup_css_released(struct cgroup_subsys_state *css)\n{\n\tstruct mem_cgroup *memcg = mem_cgroup_from_css(css);\n\n\tinvalidate_reclaim_iterators(memcg);\n\tlru_gen_release_memcg(memcg);\n}\n\nstatic void mem_cgroup_css_free(struct cgroup_subsys_state *css)\n{\n\tstruct mem_cgroup *memcg = mem_cgroup_from_css(css);\n\tint __maybe_unused i;\n\n#ifdef CONFIG_CGROUP_WRITEBACK\n\tfor (i = 0; i < MEMCG_CGWB_FRN_CNT; i++)\n\t\twb_wait_for_completion(&memcg->cgwb_frn[i].done);\n#endif\n\tif (cgroup_subsys_on_dfl(memory_cgrp_subsys) && !cgroup_memory_nosocket)\n\t\tstatic_branch_dec(&memcg_sockets_enabled_key);\n\n\tif (!cgroup_subsys_on_dfl(memory_cgrp_subsys) && memcg->tcpmem_active)\n\t\tstatic_branch_dec(&memcg_sockets_enabled_key);\n\n#if defined(CONFIG_MEMCG_KMEM)\n\tif (!cgroup_memory_nobpf)\n\t\tstatic_branch_dec(&memcg_bpf_enabled_key);\n#endif\n\n\tvmpressure_cleanup(&memcg->vmpressure);\n\tcancel_work_sync(&memcg->high_work);\n\tmem_cgroup_remove_from_trees(memcg);\n\tfree_shrinker_info(memcg);\n\tmem_cgroup_free(memcg);\n}\n\n \nstatic void mem_cgroup_css_reset(struct cgroup_subsys_state *css)\n{\n\tstruct mem_cgroup *memcg = mem_cgroup_from_css(css);\n\n\tpage_counter_set_max(&memcg->memory, PAGE_COUNTER_MAX);\n\tpage_counter_set_max(&memcg->swap, PAGE_COUNTER_MAX);\n\tpage_counter_set_max(&memcg->kmem, PAGE_COUNTER_MAX);\n\tpage_counter_set_max(&memcg->tcpmem, PAGE_COUNTER_MAX);\n\tpage_counter_set_min(&memcg->memory, 0);\n\tpage_counter_set_low(&memcg->memory, 0);\n\tpage_counter_set_high(&memcg->memory, PAGE_COUNTER_MAX);\n\tWRITE_ONCE(memcg->soft_limit, PAGE_COUNTER_MAX);\n\tpage_counter_set_high(&memcg->swap, PAGE_COUNTER_MAX);\n\tmemcg_wb_domain_size_changed(memcg);\n}\n\nstatic void mem_cgroup_css_rstat_flush(struct cgroup_subsys_state *css, int cpu)\n{\n\tstruct mem_cgroup *memcg = mem_cgroup_from_css(css);\n\tstruct mem_cgroup *parent = parent_mem_cgroup(memcg);\n\tstruct memcg_vmstats_percpu *statc;\n\tlong delta, delta_cpu, v;\n\tint i, nid;\n\n\tstatc = per_cpu_ptr(memcg->vmstats_percpu, cpu);\n\n\tfor (i = 0; i < MEMCG_NR_STAT; i++) {\n\t\t \n\t\tdelta = memcg->vmstats->state_pending[i];\n\t\tif (delta)\n\t\t\tmemcg->vmstats->state_pending[i] = 0;\n\n\t\t \n\t\tdelta_cpu = 0;\n\t\tv = READ_ONCE(statc->state[i]);\n\t\tif (v != statc->state_prev[i]) {\n\t\t\tdelta_cpu = v - statc->state_prev[i];\n\t\t\tdelta += delta_cpu;\n\t\t\tstatc->state_prev[i] = v;\n\t\t}\n\n\t\t \n\t\tif (delta_cpu)\n\t\t\tmemcg->vmstats->state_local[i] += delta_cpu;\n\n\t\tif (delta) {\n\t\t\tmemcg->vmstats->state[i] += delta;\n\t\t\tif (parent)\n\t\t\t\tparent->vmstats->state_pending[i] += delta;\n\t\t}\n\t}\n\n\tfor (i = 0; i < NR_MEMCG_EVENTS; i++) {\n\t\tdelta = memcg->vmstats->events_pending[i];\n\t\tif (delta)\n\t\t\tmemcg->vmstats->events_pending[i] = 0;\n\n\t\tdelta_cpu = 0;\n\t\tv = READ_ONCE(statc->events[i]);\n\t\tif (v != statc->events_prev[i]) {\n\t\t\tdelta_cpu = v - statc->events_prev[i];\n\t\t\tdelta += delta_cpu;\n\t\t\tstatc->events_prev[i] = v;\n\t\t}\n\n\t\tif (delta_cpu)\n\t\t\tmemcg->vmstats->events_local[i] += delta_cpu;\n\n\t\tif (delta) {\n\t\t\tmemcg->vmstats->events[i] += delta;\n\t\t\tif (parent)\n\t\t\t\tparent->vmstats->events_pending[i] += delta;\n\t\t}\n\t}\n\n\tfor_each_node_state(nid, N_MEMORY) {\n\t\tstruct mem_cgroup_per_node *pn = memcg->nodeinfo[nid];\n\t\tstruct mem_cgroup_per_node *ppn = NULL;\n\t\tstruct lruvec_stats_percpu *lstatc;\n\n\t\tif (parent)\n\t\t\tppn = parent->nodeinfo[nid];\n\n\t\tlstatc = per_cpu_ptr(pn->lruvec_stats_percpu, cpu);\n\n\t\tfor (i = 0; i < NR_VM_NODE_STAT_ITEMS; i++) {\n\t\t\tdelta = pn->lruvec_stats.state_pending[i];\n\t\t\tif (delta)\n\t\t\t\tpn->lruvec_stats.state_pending[i] = 0;\n\n\t\t\tdelta_cpu = 0;\n\t\t\tv = READ_ONCE(lstatc->state[i]);\n\t\t\tif (v != lstatc->state_prev[i]) {\n\t\t\t\tdelta_cpu = v - lstatc->state_prev[i];\n\t\t\t\tdelta += delta_cpu;\n\t\t\t\tlstatc->state_prev[i] = v;\n\t\t\t}\n\n\t\t\tif (delta_cpu)\n\t\t\t\tpn->lruvec_stats.state_local[i] += delta_cpu;\n\n\t\t\tif (delta) {\n\t\t\t\tpn->lruvec_stats.state[i] += delta;\n\t\t\t\tif (ppn)\n\t\t\t\t\tppn->lruvec_stats.state_pending[i] += delta;\n\t\t\t}\n\t\t}\n\t}\n}\n\n#ifdef CONFIG_MMU\n \nstatic int mem_cgroup_do_precharge(unsigned long count)\n{\n\tint ret;\n\n\t \n\tret = try_charge(mc.to, GFP_KERNEL & ~__GFP_DIRECT_RECLAIM, count);\n\tif (!ret) {\n\t\tmc.precharge += count;\n\t\treturn ret;\n\t}\n\n\t \n\twhile (count--) {\n\t\tret = try_charge(mc.to, GFP_KERNEL | __GFP_NORETRY, 1);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tmc.precharge++;\n\t\tcond_resched();\n\t}\n\treturn 0;\n}\n\nunion mc_target {\n\tstruct page\t*page;\n\tswp_entry_t\tent;\n};\n\nenum mc_target_type {\n\tMC_TARGET_NONE = 0,\n\tMC_TARGET_PAGE,\n\tMC_TARGET_SWAP,\n\tMC_TARGET_DEVICE,\n};\n\nstatic struct page *mc_handle_present_pte(struct vm_area_struct *vma,\n\t\t\t\t\t\tunsigned long addr, pte_t ptent)\n{\n\tstruct page *page = vm_normal_page(vma, addr, ptent);\n\n\tif (!page)\n\t\treturn NULL;\n\tif (PageAnon(page)) {\n\t\tif (!(mc.flags & MOVE_ANON))\n\t\t\treturn NULL;\n\t} else {\n\t\tif (!(mc.flags & MOVE_FILE))\n\t\t\treturn NULL;\n\t}\n\tget_page(page);\n\n\treturn page;\n}\n\n#if defined(CONFIG_SWAP) || defined(CONFIG_DEVICE_PRIVATE)\nstatic struct page *mc_handle_swap_pte(struct vm_area_struct *vma,\n\t\t\tpte_t ptent, swp_entry_t *entry)\n{\n\tstruct page *page = NULL;\n\tswp_entry_t ent = pte_to_swp_entry(ptent);\n\n\tif (!(mc.flags & MOVE_ANON))\n\t\treturn NULL;\n\n\t \n\tif (is_device_private_entry(ent)) {\n\t\tpage = pfn_swap_entry_to_page(ent);\n\t\tif (!get_page_unless_zero(page))\n\t\t\treturn NULL;\n\t\treturn page;\n\t}\n\n\tif (non_swap_entry(ent))\n\t\treturn NULL;\n\n\t \n\tpage = find_get_page(swap_address_space(ent), swp_offset(ent));\n\tentry->val = ent.val;\n\n\treturn page;\n}\n#else\nstatic struct page *mc_handle_swap_pte(struct vm_area_struct *vma,\n\t\t\tpte_t ptent, swp_entry_t *entry)\n{\n\treturn NULL;\n}\n#endif\n\nstatic struct page *mc_handle_file_pte(struct vm_area_struct *vma,\n\t\t\tunsigned long addr, pte_t ptent)\n{\n\tunsigned long index;\n\tstruct folio *folio;\n\n\tif (!vma->vm_file)  \n\t\treturn NULL;\n\tif (!(mc.flags & MOVE_FILE))\n\t\treturn NULL;\n\n\t \n\t \n\tindex = linear_page_index(vma, addr);\n\tfolio = filemap_get_incore_folio(vma->vm_file->f_mapping, index);\n\tif (IS_ERR(folio))\n\t\treturn NULL;\n\treturn folio_file_page(folio, index);\n}\n\n \nstatic int mem_cgroup_move_account(struct page *page,\n\t\t\t\t   bool compound,\n\t\t\t\t   struct mem_cgroup *from,\n\t\t\t\t   struct mem_cgroup *to)\n{\n\tstruct folio *folio = page_folio(page);\n\tstruct lruvec *from_vec, *to_vec;\n\tstruct pglist_data *pgdat;\n\tunsigned int nr_pages = compound ? folio_nr_pages(folio) : 1;\n\tint nid, ret;\n\n\tVM_BUG_ON(from == to);\n\tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n\tVM_BUG_ON_FOLIO(folio_test_lru(folio), folio);\n\tVM_BUG_ON(compound && !folio_test_large(folio));\n\n\tret = -EINVAL;\n\tif (folio_memcg(folio) != from)\n\t\tgoto out;\n\n\tpgdat = folio_pgdat(folio);\n\tfrom_vec = mem_cgroup_lruvec(from, pgdat);\n\tto_vec = mem_cgroup_lruvec(to, pgdat);\n\n\tfolio_memcg_lock(folio);\n\n\tif (folio_test_anon(folio)) {\n\t\tif (folio_mapped(folio)) {\n\t\t\t__mod_lruvec_state(from_vec, NR_ANON_MAPPED, -nr_pages);\n\t\t\t__mod_lruvec_state(to_vec, NR_ANON_MAPPED, nr_pages);\n\t\t\tif (folio_test_pmd_mappable(folio)) {\n\t\t\t\t__mod_lruvec_state(from_vec, NR_ANON_THPS,\n\t\t\t\t\t\t   -nr_pages);\n\t\t\t\t__mod_lruvec_state(to_vec, NR_ANON_THPS,\n\t\t\t\t\t\t   nr_pages);\n\t\t\t}\n\t\t}\n\t} else {\n\t\t__mod_lruvec_state(from_vec, NR_FILE_PAGES, -nr_pages);\n\t\t__mod_lruvec_state(to_vec, NR_FILE_PAGES, nr_pages);\n\n\t\tif (folio_test_swapbacked(folio)) {\n\t\t\t__mod_lruvec_state(from_vec, NR_SHMEM, -nr_pages);\n\t\t\t__mod_lruvec_state(to_vec, NR_SHMEM, nr_pages);\n\t\t}\n\n\t\tif (folio_mapped(folio)) {\n\t\t\t__mod_lruvec_state(from_vec, NR_FILE_MAPPED, -nr_pages);\n\t\t\t__mod_lruvec_state(to_vec, NR_FILE_MAPPED, nr_pages);\n\t\t}\n\n\t\tif (folio_test_dirty(folio)) {\n\t\t\tstruct address_space *mapping = folio_mapping(folio);\n\n\t\t\tif (mapping_can_writeback(mapping)) {\n\t\t\t\t__mod_lruvec_state(from_vec, NR_FILE_DIRTY,\n\t\t\t\t\t\t   -nr_pages);\n\t\t\t\t__mod_lruvec_state(to_vec, NR_FILE_DIRTY,\n\t\t\t\t\t\t   nr_pages);\n\t\t\t}\n\t\t}\n\t}\n\n#ifdef CONFIG_SWAP\n\tif (folio_test_swapcache(folio)) {\n\t\t__mod_lruvec_state(from_vec, NR_SWAPCACHE, -nr_pages);\n\t\t__mod_lruvec_state(to_vec, NR_SWAPCACHE, nr_pages);\n\t}\n#endif\n\tif (folio_test_writeback(folio)) {\n\t\t__mod_lruvec_state(from_vec, NR_WRITEBACK, -nr_pages);\n\t\t__mod_lruvec_state(to_vec, NR_WRITEBACK, nr_pages);\n\t}\n\n\t \n\tsmp_mb();\n\n\tcss_get(&to->css);\n\tcss_put(&from->css);\n\n\tfolio->memcg_data = (unsigned long)to;\n\n\t__folio_memcg_unlock(from);\n\n\tret = 0;\n\tnid = folio_nid(folio);\n\n\tlocal_irq_disable();\n\tmem_cgroup_charge_statistics(to, nr_pages);\n\tmemcg_check_events(to, nid);\n\tmem_cgroup_charge_statistics(from, -nr_pages);\n\tmemcg_check_events(from, nid);\n\tlocal_irq_enable();\nout:\n\treturn ret;\n}\n\n \nstatic enum mc_target_type get_mctgt_type(struct vm_area_struct *vma,\n\t\tunsigned long addr, pte_t ptent, union mc_target *target)\n{\n\tstruct page *page = NULL;\n\tenum mc_target_type ret = MC_TARGET_NONE;\n\tswp_entry_t ent = { .val = 0 };\n\n\tif (pte_present(ptent))\n\t\tpage = mc_handle_present_pte(vma, addr, ptent);\n\telse if (pte_none_mostly(ptent))\n\t\t \n\t\tpage = mc_handle_file_pte(vma, addr, ptent);\n\telse if (is_swap_pte(ptent))\n\t\tpage = mc_handle_swap_pte(vma, ptent, &ent);\n\n\tif (target && page) {\n\t\tif (!trylock_page(page)) {\n\t\t\tput_page(page);\n\t\t\treturn ret;\n\t\t}\n\t\t \n\t\tif (!pte_present(ptent) && page_mapped(page)) {\n\t\t\tunlock_page(page);\n\t\t\tput_page(page);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tif (!page && !ent.val)\n\t\treturn ret;\n\tif (page) {\n\t\t \n\t\tif (page_memcg(page) == mc.from) {\n\t\t\tret = MC_TARGET_PAGE;\n\t\t\tif (is_device_private_page(page) ||\n\t\t\t    is_device_coherent_page(page))\n\t\t\t\tret = MC_TARGET_DEVICE;\n\t\t\tif (target)\n\t\t\t\ttarget->page = page;\n\t\t}\n\t\tif (!ret || !target) {\n\t\t\tif (target)\n\t\t\t\tunlock_page(page);\n\t\t\tput_page(page);\n\t\t}\n\t}\n\t \n\tif (ent.val && !ret && (!page || !PageTransCompound(page)) &&\n\t    mem_cgroup_id(mc.from) == lookup_swap_cgroup_id(ent)) {\n\t\tret = MC_TARGET_SWAP;\n\t\tif (target)\n\t\t\ttarget->ent = ent;\n\t}\n\treturn ret;\n}\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n \nstatic enum mc_target_type get_mctgt_type_thp(struct vm_area_struct *vma,\n\t\tunsigned long addr, pmd_t pmd, union mc_target *target)\n{\n\tstruct page *page = NULL;\n\tenum mc_target_type ret = MC_TARGET_NONE;\n\n\tif (unlikely(is_swap_pmd(pmd))) {\n\t\tVM_BUG_ON(thp_migration_supported() &&\n\t\t\t\t  !is_pmd_migration_entry(pmd));\n\t\treturn ret;\n\t}\n\tpage = pmd_page(pmd);\n\tVM_BUG_ON_PAGE(!page || !PageHead(page), page);\n\tif (!(mc.flags & MOVE_ANON))\n\t\treturn ret;\n\tif (page_memcg(page) == mc.from) {\n\t\tret = MC_TARGET_PAGE;\n\t\tif (target) {\n\t\t\tget_page(page);\n\t\t\tif (!trylock_page(page)) {\n\t\t\t\tput_page(page);\n\t\t\t\treturn MC_TARGET_NONE;\n\t\t\t}\n\t\t\ttarget->page = page;\n\t\t}\n\t}\n\treturn ret;\n}\n#else\nstatic inline enum mc_target_type get_mctgt_type_thp(struct vm_area_struct *vma,\n\t\tunsigned long addr, pmd_t pmd, union mc_target *target)\n{\n\treturn MC_TARGET_NONE;\n}\n#endif\n\nstatic int mem_cgroup_count_precharge_pte_range(pmd_t *pmd,\n\t\t\t\t\tunsigned long addr, unsigned long end,\n\t\t\t\t\tstruct mm_walk *walk)\n{\n\tstruct vm_area_struct *vma = walk->vma;\n\tpte_t *pte;\n\tspinlock_t *ptl;\n\n\tptl = pmd_trans_huge_lock(pmd, vma);\n\tif (ptl) {\n\t\t \n\t\tif (get_mctgt_type_thp(vma, addr, *pmd, NULL) == MC_TARGET_PAGE)\n\t\t\tmc.precharge += HPAGE_PMD_NR;\n\t\tspin_unlock(ptl);\n\t\treturn 0;\n\t}\n\n\tpte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);\n\tif (!pte)\n\t\treturn 0;\n\tfor (; addr != end; pte++, addr += PAGE_SIZE)\n\t\tif (get_mctgt_type(vma, addr, ptep_get(pte), NULL))\n\t\t\tmc.precharge++;\t \n\tpte_unmap_unlock(pte - 1, ptl);\n\tcond_resched();\n\n\treturn 0;\n}\n\nstatic const struct mm_walk_ops precharge_walk_ops = {\n\t.pmd_entry\t= mem_cgroup_count_precharge_pte_range,\n\t.walk_lock\t= PGWALK_RDLOCK,\n};\n\nstatic unsigned long mem_cgroup_count_precharge(struct mm_struct *mm)\n{\n\tunsigned long precharge;\n\n\tmmap_read_lock(mm);\n\twalk_page_range(mm, 0, ULONG_MAX, &precharge_walk_ops, NULL);\n\tmmap_read_unlock(mm);\n\n\tprecharge = mc.precharge;\n\tmc.precharge = 0;\n\n\treturn precharge;\n}\n\nstatic int mem_cgroup_precharge_mc(struct mm_struct *mm)\n{\n\tunsigned long precharge = mem_cgroup_count_precharge(mm);\n\n\tVM_BUG_ON(mc.moving_task);\n\tmc.moving_task = current;\n\treturn mem_cgroup_do_precharge(precharge);\n}\n\n \nstatic void __mem_cgroup_clear_mc(void)\n{\n\tstruct mem_cgroup *from = mc.from;\n\tstruct mem_cgroup *to = mc.to;\n\n\t \n\tif (mc.precharge) {\n\t\tcancel_charge(mc.to, mc.precharge);\n\t\tmc.precharge = 0;\n\t}\n\t \n\tif (mc.moved_charge) {\n\t\tcancel_charge(mc.from, mc.moved_charge);\n\t\tmc.moved_charge = 0;\n\t}\n\t \n\tif (mc.moved_swap) {\n\t\t \n\t\tif (!mem_cgroup_is_root(mc.from))\n\t\t\tpage_counter_uncharge(&mc.from->memsw, mc.moved_swap);\n\n\t\tmem_cgroup_id_put_many(mc.from, mc.moved_swap);\n\n\t\t \n\t\tif (!mem_cgroup_is_root(mc.to))\n\t\t\tpage_counter_uncharge(&mc.to->memory, mc.moved_swap);\n\n\t\tmc.moved_swap = 0;\n\t}\n\tmemcg_oom_recover(from);\n\tmemcg_oom_recover(to);\n\twake_up_all(&mc.waitq);\n}\n\nstatic void mem_cgroup_clear_mc(void)\n{\n\tstruct mm_struct *mm = mc.mm;\n\n\t \n\tmc.moving_task = NULL;\n\t__mem_cgroup_clear_mc();\n\tspin_lock(&mc.lock);\n\tmc.from = NULL;\n\tmc.to = NULL;\n\tmc.mm = NULL;\n\tspin_unlock(&mc.lock);\n\n\tmmput(mm);\n}\n\nstatic int mem_cgroup_can_attach(struct cgroup_taskset *tset)\n{\n\tstruct cgroup_subsys_state *css;\n\tstruct mem_cgroup *memcg = NULL;  \n\tstruct mem_cgroup *from;\n\tstruct task_struct *leader, *p;\n\tstruct mm_struct *mm;\n\tunsigned long move_flags;\n\tint ret = 0;\n\n\t \n\tif (cgroup_subsys_on_dfl(memory_cgrp_subsys))\n\t\treturn 0;\n\n\t \n\tp = NULL;\n\tcgroup_taskset_for_each_leader(leader, css, tset) {\n\t\tWARN_ON_ONCE(p);\n\t\tp = leader;\n\t\tmemcg = mem_cgroup_from_css(css);\n\t}\n\tif (!p)\n\t\treturn 0;\n\n\t \n\tmove_flags = READ_ONCE(memcg->move_charge_at_immigrate);\n\tif (!move_flags)\n\t\treturn 0;\n\n\tfrom = mem_cgroup_from_task(p);\n\n\tVM_BUG_ON(from == memcg);\n\n\tmm = get_task_mm(p);\n\tif (!mm)\n\t\treturn 0;\n\t \n\tif (mm->owner == p) {\n\t\tVM_BUG_ON(mc.from);\n\t\tVM_BUG_ON(mc.to);\n\t\tVM_BUG_ON(mc.precharge);\n\t\tVM_BUG_ON(mc.moved_charge);\n\t\tVM_BUG_ON(mc.moved_swap);\n\n\t\tspin_lock(&mc.lock);\n\t\tmc.mm = mm;\n\t\tmc.from = from;\n\t\tmc.to = memcg;\n\t\tmc.flags = move_flags;\n\t\tspin_unlock(&mc.lock);\n\t\t \n\n\t\tret = mem_cgroup_precharge_mc(mm);\n\t\tif (ret)\n\t\t\tmem_cgroup_clear_mc();\n\t} else {\n\t\tmmput(mm);\n\t}\n\treturn ret;\n}\n\nstatic void mem_cgroup_cancel_attach(struct cgroup_taskset *tset)\n{\n\tif (mc.to)\n\t\tmem_cgroup_clear_mc();\n}\n\nstatic int mem_cgroup_move_charge_pte_range(pmd_t *pmd,\n\t\t\t\tunsigned long addr, unsigned long end,\n\t\t\t\tstruct mm_walk *walk)\n{\n\tint ret = 0;\n\tstruct vm_area_struct *vma = walk->vma;\n\tpte_t *pte;\n\tspinlock_t *ptl;\n\tenum mc_target_type target_type;\n\tunion mc_target target;\n\tstruct page *page;\n\n\tptl = pmd_trans_huge_lock(pmd, vma);\n\tif (ptl) {\n\t\tif (mc.precharge < HPAGE_PMD_NR) {\n\t\t\tspin_unlock(ptl);\n\t\t\treturn 0;\n\t\t}\n\t\ttarget_type = get_mctgt_type_thp(vma, addr, *pmd, &target);\n\t\tif (target_type == MC_TARGET_PAGE) {\n\t\t\tpage = target.page;\n\t\t\tif (isolate_lru_page(page)) {\n\t\t\t\tif (!mem_cgroup_move_account(page, true,\n\t\t\t\t\t\t\t     mc.from, mc.to)) {\n\t\t\t\t\tmc.precharge -= HPAGE_PMD_NR;\n\t\t\t\t\tmc.moved_charge += HPAGE_PMD_NR;\n\t\t\t\t}\n\t\t\t\tputback_lru_page(page);\n\t\t\t}\n\t\t\tunlock_page(page);\n\t\t\tput_page(page);\n\t\t} else if (target_type == MC_TARGET_DEVICE) {\n\t\t\tpage = target.page;\n\t\t\tif (!mem_cgroup_move_account(page, true,\n\t\t\t\t\t\t     mc.from, mc.to)) {\n\t\t\t\tmc.precharge -= HPAGE_PMD_NR;\n\t\t\t\tmc.moved_charge += HPAGE_PMD_NR;\n\t\t\t}\n\t\t\tunlock_page(page);\n\t\t\tput_page(page);\n\t\t}\n\t\tspin_unlock(ptl);\n\t\treturn 0;\n\t}\n\nretry:\n\tpte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);\n\tif (!pte)\n\t\treturn 0;\n\tfor (; addr != end; addr += PAGE_SIZE) {\n\t\tpte_t ptent = ptep_get(pte++);\n\t\tbool device = false;\n\t\tswp_entry_t ent;\n\n\t\tif (!mc.precharge)\n\t\t\tbreak;\n\n\t\tswitch (get_mctgt_type(vma, addr, ptent, &target)) {\n\t\tcase MC_TARGET_DEVICE:\n\t\t\tdevice = true;\n\t\t\tfallthrough;\n\t\tcase MC_TARGET_PAGE:\n\t\t\tpage = target.page;\n\t\t\t \n\t\t\tif (PageTransCompound(page))\n\t\t\t\tgoto put;\n\t\t\tif (!device && !isolate_lru_page(page))\n\t\t\t\tgoto put;\n\t\t\tif (!mem_cgroup_move_account(page, false,\n\t\t\t\t\t\tmc.from, mc.to)) {\n\t\t\t\tmc.precharge--;\n\t\t\t\t \n\t\t\t\tmc.moved_charge++;\n\t\t\t}\n\t\t\tif (!device)\n\t\t\t\tputback_lru_page(page);\nput:\t\t\t \n\t\t\tunlock_page(page);\n\t\t\tput_page(page);\n\t\t\tbreak;\n\t\tcase MC_TARGET_SWAP:\n\t\t\tent = target.ent;\n\t\t\tif (!mem_cgroup_move_swap_account(ent, mc.from, mc.to)) {\n\t\t\t\tmc.precharge--;\n\t\t\t\tmem_cgroup_id_get_many(mc.to, 1);\n\t\t\t\t \n\t\t\t\tmc.moved_swap++;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\tpte_unmap_unlock(pte - 1, ptl);\n\tcond_resched();\n\n\tif (addr != end) {\n\t\t \n\t\tret = mem_cgroup_do_precharge(1);\n\t\tif (!ret)\n\t\t\tgoto retry;\n\t}\n\n\treturn ret;\n}\n\nstatic const struct mm_walk_ops charge_walk_ops = {\n\t.pmd_entry\t= mem_cgroup_move_charge_pte_range,\n\t.walk_lock\t= PGWALK_RDLOCK,\n};\n\nstatic void mem_cgroup_move_charge(void)\n{\n\tlru_add_drain_all();\n\t \n\tatomic_inc(&mc.from->moving_account);\n\tsynchronize_rcu();\nretry:\n\tif (unlikely(!mmap_read_trylock(mc.mm))) {\n\t\t \n\t\t__mem_cgroup_clear_mc();\n\t\tcond_resched();\n\t\tgoto retry;\n\t}\n\t \n\twalk_page_range(mc.mm, 0, ULONG_MAX, &charge_walk_ops, NULL);\n\tmmap_read_unlock(mc.mm);\n\tatomic_dec(&mc.from->moving_account);\n}\n\nstatic void mem_cgroup_move_task(void)\n{\n\tif (mc.to) {\n\t\tmem_cgroup_move_charge();\n\t\tmem_cgroup_clear_mc();\n\t}\n}\n#else\t \nstatic int mem_cgroup_can_attach(struct cgroup_taskset *tset)\n{\n\treturn 0;\n}\nstatic void mem_cgroup_cancel_attach(struct cgroup_taskset *tset)\n{\n}\nstatic void mem_cgroup_move_task(void)\n{\n}\n#endif\n\n#ifdef CONFIG_LRU_GEN\nstatic void mem_cgroup_attach(struct cgroup_taskset *tset)\n{\n\tstruct task_struct *task;\n\tstruct cgroup_subsys_state *css;\n\n\t \n\tcgroup_taskset_for_each_leader(task, css, tset)\n\t\tbreak;\n\n\tif (!task)\n\t\treturn;\n\n\ttask_lock(task);\n\tif (task->mm && READ_ONCE(task->mm->owner) == task)\n\t\tlru_gen_migrate_mm(task->mm);\n\ttask_unlock(task);\n}\n#else\nstatic void mem_cgroup_attach(struct cgroup_taskset *tset)\n{\n}\n#endif  \n\nstatic int seq_puts_memcg_tunable(struct seq_file *m, unsigned long value)\n{\n\tif (value == PAGE_COUNTER_MAX)\n\t\tseq_puts(m, \"max\\n\");\n\telse\n\t\tseq_printf(m, \"%llu\\n\", (u64)value * PAGE_SIZE);\n\n\treturn 0;\n}\n\nstatic u64 memory_current_read(struct cgroup_subsys_state *css,\n\t\t\t       struct cftype *cft)\n{\n\tstruct mem_cgroup *memcg = mem_cgroup_from_css(css);\n\n\treturn (u64)page_counter_read(&memcg->memory) * PAGE_SIZE;\n}\n\nstatic u64 memory_peak_read(struct cgroup_subsys_state *css,\n\t\t\t    struct cftype *cft)\n{\n\tstruct mem_cgroup *memcg = mem_cgroup_from_css(css);\n\n\treturn (u64)memcg->memory.watermark * PAGE_SIZE;\n}\n\nstatic int memory_min_show(struct seq_file *m, void *v)\n{\n\treturn seq_puts_memcg_tunable(m,\n\t\tREAD_ONCE(mem_cgroup_from_seq(m)->memory.min));\n}\n\nstatic ssize_t memory_min_write(struct kernfs_open_file *of,\n\t\t\t\tchar *buf, size_t nbytes, loff_t off)\n{\n\tstruct mem_cgroup *memcg = mem_cgroup_from_css(of_css(of));\n\tunsigned long min;\n\tint err;\n\n\tbuf = strstrip(buf);\n\terr = page_counter_memparse(buf, \"max\", &min);\n\tif (err)\n\t\treturn err;\n\n\tpage_counter_set_min(&memcg->memory, min);\n\n\treturn nbytes;\n}\n\nstatic int memory_low_show(struct seq_file *m, void *v)\n{\n\treturn seq_puts_memcg_tunable(m,\n\t\tREAD_ONCE(mem_cgroup_from_seq(m)->memory.low));\n}\n\nstatic ssize_t memory_low_write(struct kernfs_open_file *of,\n\t\t\t\tchar *buf, size_t nbytes, loff_t off)\n{\n\tstruct mem_cgroup *memcg = mem_cgroup_from_css(of_css(of));\n\tunsigned long low;\n\tint err;\n\n\tbuf = strstrip(buf);\n\terr = page_counter_memparse(buf, \"max\", &low);\n\tif (err)\n\t\treturn err;\n\n\tpage_counter_set_low(&memcg->memory, low);\n\n\treturn nbytes;\n}\n\nstatic int memory_high_show(struct seq_file *m, void *v)\n{\n\treturn seq_puts_memcg_tunable(m,\n\t\tREAD_ONCE(mem_cgroup_from_seq(m)->memory.high));\n}\n\nstatic ssize_t memory_high_write(struct kernfs_open_file *of,\n\t\t\t\t char *buf, size_t nbytes, loff_t off)\n{\n\tstruct mem_cgroup *memcg = mem_cgroup_from_css(of_css(of));\n\tunsigned int nr_retries = MAX_RECLAIM_RETRIES;\n\tbool drained = false;\n\tunsigned long high;\n\tint err;\n\n\tbuf = strstrip(buf);\n\terr = page_counter_memparse(buf, \"max\", &high);\n\tif (err)\n\t\treturn err;\n\n\tpage_counter_set_high(&memcg->memory, high);\n\n\tfor (;;) {\n\t\tunsigned long nr_pages = page_counter_read(&memcg->memory);\n\t\tunsigned long reclaimed;\n\n\t\tif (nr_pages <= high)\n\t\t\tbreak;\n\n\t\tif (signal_pending(current))\n\t\t\tbreak;\n\n\t\tif (!drained) {\n\t\t\tdrain_all_stock(memcg);\n\t\t\tdrained = true;\n\t\t\tcontinue;\n\t\t}\n\n\t\treclaimed = try_to_free_mem_cgroup_pages(memcg, nr_pages - high,\n\t\t\t\t\tGFP_KERNEL, MEMCG_RECLAIM_MAY_SWAP);\n\n\t\tif (!reclaimed && !nr_retries--)\n\t\t\tbreak;\n\t}\n\n\tmemcg_wb_domain_size_changed(memcg);\n\treturn nbytes;\n}\n\nstatic int memory_max_show(struct seq_file *m, void *v)\n{\n\treturn seq_puts_memcg_tunable(m,\n\t\tREAD_ONCE(mem_cgroup_from_seq(m)->memory.max));\n}\n\nstatic ssize_t memory_max_write(struct kernfs_open_file *of,\n\t\t\t\tchar *buf, size_t nbytes, loff_t off)\n{\n\tstruct mem_cgroup *memcg = mem_cgroup_from_css(of_css(of));\n\tunsigned int nr_reclaims = MAX_RECLAIM_RETRIES;\n\tbool drained = false;\n\tunsigned long max;\n\tint err;\n\n\tbuf = strstrip(buf);\n\terr = page_counter_memparse(buf, \"max\", &max);\n\tif (err)\n\t\treturn err;\n\n\txchg(&memcg->memory.max, max);\n\n\tfor (;;) {\n\t\tunsigned long nr_pages = page_counter_read(&memcg->memory);\n\n\t\tif (nr_pages <= max)\n\t\t\tbreak;\n\n\t\tif (signal_pending(current))\n\t\t\tbreak;\n\n\t\tif (!drained) {\n\t\t\tdrain_all_stock(memcg);\n\t\t\tdrained = true;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (nr_reclaims) {\n\t\t\tif (!try_to_free_mem_cgroup_pages(memcg, nr_pages - max,\n\t\t\t\t\tGFP_KERNEL, MEMCG_RECLAIM_MAY_SWAP))\n\t\t\t\tnr_reclaims--;\n\t\t\tcontinue;\n\t\t}\n\n\t\tmemcg_memory_event(memcg, MEMCG_OOM);\n\t\tif (!mem_cgroup_out_of_memory(memcg, GFP_KERNEL, 0))\n\t\t\tbreak;\n\t}\n\n\tmemcg_wb_domain_size_changed(memcg);\n\treturn nbytes;\n}\n\nstatic void __memory_events_show(struct seq_file *m, atomic_long_t *events)\n{\n\tseq_printf(m, \"low %lu\\n\", atomic_long_read(&events[MEMCG_LOW]));\n\tseq_printf(m, \"high %lu\\n\", atomic_long_read(&events[MEMCG_HIGH]));\n\tseq_printf(m, \"max %lu\\n\", atomic_long_read(&events[MEMCG_MAX]));\n\tseq_printf(m, \"oom %lu\\n\", atomic_long_read(&events[MEMCG_OOM]));\n\tseq_printf(m, \"oom_kill %lu\\n\",\n\t\t   atomic_long_read(&events[MEMCG_OOM_KILL]));\n\tseq_printf(m, \"oom_group_kill %lu\\n\",\n\t\t   atomic_long_read(&events[MEMCG_OOM_GROUP_KILL]));\n}\n\nstatic int memory_events_show(struct seq_file *m, void *v)\n{\n\tstruct mem_cgroup *memcg = mem_cgroup_from_seq(m);\n\n\t__memory_events_show(m, memcg->memory_events);\n\treturn 0;\n}\n\nstatic int memory_events_local_show(struct seq_file *m, void *v)\n{\n\tstruct mem_cgroup *memcg = mem_cgroup_from_seq(m);\n\n\t__memory_events_show(m, memcg->memory_events_local);\n\treturn 0;\n}\n\nstatic int memory_stat_show(struct seq_file *m, void *v)\n{\n\tstruct mem_cgroup *memcg = mem_cgroup_from_seq(m);\n\tchar *buf = kmalloc(PAGE_SIZE, GFP_KERNEL);\n\tstruct seq_buf s;\n\n\tif (!buf)\n\t\treturn -ENOMEM;\n\tseq_buf_init(&s, buf, PAGE_SIZE);\n\tmemory_stat_format(memcg, &s);\n\tseq_puts(m, buf);\n\tkfree(buf);\n\treturn 0;\n}\n\n#ifdef CONFIG_NUMA\nstatic inline unsigned long lruvec_page_state_output(struct lruvec *lruvec,\n\t\t\t\t\t\t     int item)\n{\n\treturn lruvec_page_state(lruvec, item) * memcg_page_state_unit(item);\n}\n\nstatic int memory_numa_stat_show(struct seq_file *m, void *v)\n{\n\tint i;\n\tstruct mem_cgroup *memcg = mem_cgroup_from_seq(m);\n\n\tmem_cgroup_flush_stats();\n\n\tfor (i = 0; i < ARRAY_SIZE(memory_stats); i++) {\n\t\tint nid;\n\n\t\tif (memory_stats[i].idx >= NR_VM_NODE_STAT_ITEMS)\n\t\t\tcontinue;\n\n\t\tseq_printf(m, \"%s\", memory_stats[i].name);\n\t\tfor_each_node_state(nid, N_MEMORY) {\n\t\t\tu64 size;\n\t\t\tstruct lruvec *lruvec;\n\n\t\t\tlruvec = mem_cgroup_lruvec(memcg, NODE_DATA(nid));\n\t\t\tsize = lruvec_page_state_output(lruvec,\n\t\t\t\t\t\t\tmemory_stats[i].idx);\n\t\t\tseq_printf(m, \" N%d=%llu\", nid, size);\n\t\t}\n\t\tseq_putc(m, '\\n');\n\t}\n\n\treturn 0;\n}\n#endif\n\nstatic int memory_oom_group_show(struct seq_file *m, void *v)\n{\n\tstruct mem_cgroup *memcg = mem_cgroup_from_seq(m);\n\n\tseq_printf(m, \"%d\\n\", READ_ONCE(memcg->oom_group));\n\n\treturn 0;\n}\n\nstatic ssize_t memory_oom_group_write(struct kernfs_open_file *of,\n\t\t\t\t      char *buf, size_t nbytes, loff_t off)\n{\n\tstruct mem_cgroup *memcg = mem_cgroup_from_css(of_css(of));\n\tint ret, oom_group;\n\n\tbuf = strstrip(buf);\n\tif (!buf)\n\t\treturn -EINVAL;\n\n\tret = kstrtoint(buf, 0, &oom_group);\n\tif (ret)\n\t\treturn ret;\n\n\tif (oom_group != 0 && oom_group != 1)\n\t\treturn -EINVAL;\n\n\tWRITE_ONCE(memcg->oom_group, oom_group);\n\n\treturn nbytes;\n}\n\nstatic ssize_t memory_reclaim(struct kernfs_open_file *of, char *buf,\n\t\t\t      size_t nbytes, loff_t off)\n{\n\tstruct mem_cgroup *memcg = mem_cgroup_from_css(of_css(of));\n\tunsigned int nr_retries = MAX_RECLAIM_RETRIES;\n\tunsigned long nr_to_reclaim, nr_reclaimed = 0;\n\tunsigned int reclaim_options;\n\tint err;\n\n\tbuf = strstrip(buf);\n\terr = page_counter_memparse(buf, \"\", &nr_to_reclaim);\n\tif (err)\n\t\treturn err;\n\n\treclaim_options\t= MEMCG_RECLAIM_MAY_SWAP | MEMCG_RECLAIM_PROACTIVE;\n\twhile (nr_reclaimed < nr_to_reclaim) {\n\t\tunsigned long reclaimed;\n\n\t\tif (signal_pending(current))\n\t\t\treturn -EINTR;\n\n\t\t \n\t\tif (!nr_retries)\n\t\t\tlru_add_drain_all();\n\n\t\treclaimed = try_to_free_mem_cgroup_pages(memcg,\n\t\t\t\t\tmin(nr_to_reclaim - nr_reclaimed, SWAP_CLUSTER_MAX),\n\t\t\t\t\tGFP_KERNEL, reclaim_options);\n\n\t\tif (!reclaimed && !nr_retries--)\n\t\t\treturn -EAGAIN;\n\n\t\tnr_reclaimed += reclaimed;\n\t}\n\n\treturn nbytes;\n}\n\nstatic struct cftype memory_files[] = {\n\t{\n\t\t.name = \"current\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.read_u64 = memory_current_read,\n\t},\n\t{\n\t\t.name = \"peak\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.read_u64 = memory_peak_read,\n\t},\n\t{\n\t\t.name = \"min\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.seq_show = memory_min_show,\n\t\t.write = memory_min_write,\n\t},\n\t{\n\t\t.name = \"low\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.seq_show = memory_low_show,\n\t\t.write = memory_low_write,\n\t},\n\t{\n\t\t.name = \"high\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.seq_show = memory_high_show,\n\t\t.write = memory_high_write,\n\t},\n\t{\n\t\t.name = \"max\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.seq_show = memory_max_show,\n\t\t.write = memory_max_write,\n\t},\n\t{\n\t\t.name = \"events\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.file_offset = offsetof(struct mem_cgroup, events_file),\n\t\t.seq_show = memory_events_show,\n\t},\n\t{\n\t\t.name = \"events.local\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.file_offset = offsetof(struct mem_cgroup, events_local_file),\n\t\t.seq_show = memory_events_local_show,\n\t},\n\t{\n\t\t.name = \"stat\",\n\t\t.seq_show = memory_stat_show,\n\t},\n#ifdef CONFIG_NUMA\n\t{\n\t\t.name = \"numa_stat\",\n\t\t.seq_show = memory_numa_stat_show,\n\t},\n#endif\n\t{\n\t\t.name = \"oom.group\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT | CFTYPE_NS_DELEGATABLE,\n\t\t.seq_show = memory_oom_group_show,\n\t\t.write = memory_oom_group_write,\n\t},\n\t{\n\t\t.name = \"reclaim\",\n\t\t.flags = CFTYPE_NS_DELEGATABLE,\n\t\t.write = memory_reclaim,\n\t},\n\t{ }\t \n};\n\nstruct cgroup_subsys memory_cgrp_subsys = {\n\t.css_alloc = mem_cgroup_css_alloc,\n\t.css_online = mem_cgroup_css_online,\n\t.css_offline = mem_cgroup_css_offline,\n\t.css_released = mem_cgroup_css_released,\n\t.css_free = mem_cgroup_css_free,\n\t.css_reset = mem_cgroup_css_reset,\n\t.css_rstat_flush = mem_cgroup_css_rstat_flush,\n\t.can_attach = mem_cgroup_can_attach,\n\t.attach = mem_cgroup_attach,\n\t.cancel_attach = mem_cgroup_cancel_attach,\n\t.post_attach = mem_cgroup_move_task,\n\t.dfl_cftypes = memory_files,\n\t.legacy_cftypes = mem_cgroup_legacy_files,\n\t.early_init = 0,\n};\n\n \nstatic unsigned long effective_protection(unsigned long usage,\n\t\t\t\t\t  unsigned long parent_usage,\n\t\t\t\t\t  unsigned long setting,\n\t\t\t\t\t  unsigned long parent_effective,\n\t\t\t\t\t  unsigned long siblings_protected)\n{\n\tunsigned long protected;\n\tunsigned long ep;\n\n\tprotected = min(usage, setting);\n\t \n\tif (siblings_protected > parent_effective)\n\t\treturn protected * parent_effective / siblings_protected;\n\n\t \n\tep = protected;\n\n\t \n\tif (!(cgrp_dfl_root.flags & CGRP_ROOT_MEMORY_RECURSIVE_PROT))\n\t\treturn ep;\n\tif (parent_effective > siblings_protected &&\n\t    parent_usage > siblings_protected &&\n\t    usage > protected) {\n\t\tunsigned long unclaimed;\n\n\t\tunclaimed = parent_effective - siblings_protected;\n\t\tunclaimed *= usage - protected;\n\t\tunclaimed /= parent_usage - siblings_protected;\n\n\t\tep += unclaimed;\n\t}\n\n\treturn ep;\n}\n\n \nvoid mem_cgroup_calculate_protection(struct mem_cgroup *root,\n\t\t\t\t     struct mem_cgroup *memcg)\n{\n\tunsigned long usage, parent_usage;\n\tstruct mem_cgroup *parent;\n\n\tif (mem_cgroup_disabled())\n\t\treturn;\n\n\tif (!root)\n\t\troot = root_mem_cgroup;\n\n\t \n\tif (memcg == root)\n\t\treturn;\n\n\tusage = page_counter_read(&memcg->memory);\n\tif (!usage)\n\t\treturn;\n\n\tparent = parent_mem_cgroup(memcg);\n\n\tif (parent == root) {\n\t\tmemcg->memory.emin = READ_ONCE(memcg->memory.min);\n\t\tmemcg->memory.elow = READ_ONCE(memcg->memory.low);\n\t\treturn;\n\t}\n\n\tparent_usage = page_counter_read(&parent->memory);\n\n\tWRITE_ONCE(memcg->memory.emin, effective_protection(usage, parent_usage,\n\t\t\tREAD_ONCE(memcg->memory.min),\n\t\t\tREAD_ONCE(parent->memory.emin),\n\t\t\tatomic_long_read(&parent->memory.children_min_usage)));\n\n\tWRITE_ONCE(memcg->memory.elow, effective_protection(usage, parent_usage,\n\t\t\tREAD_ONCE(memcg->memory.low),\n\t\t\tREAD_ONCE(parent->memory.elow),\n\t\t\tatomic_long_read(&parent->memory.children_low_usage)));\n}\n\nstatic int charge_memcg(struct folio *folio, struct mem_cgroup *memcg,\n\t\t\tgfp_t gfp)\n{\n\tlong nr_pages = folio_nr_pages(folio);\n\tint ret;\n\n\tret = try_charge(memcg, gfp, nr_pages);\n\tif (ret)\n\t\tgoto out;\n\n\tcss_get(&memcg->css);\n\tcommit_charge(folio, memcg);\n\n\tlocal_irq_disable();\n\tmem_cgroup_charge_statistics(memcg, nr_pages);\n\tmemcg_check_events(memcg, folio_nid(folio));\n\tlocal_irq_enable();\nout:\n\treturn ret;\n}\n\nint __mem_cgroup_charge(struct folio *folio, struct mm_struct *mm, gfp_t gfp)\n{\n\tstruct mem_cgroup *memcg;\n\tint ret;\n\n\tmemcg = get_mem_cgroup_from_mm(mm);\n\tret = charge_memcg(folio, memcg, gfp);\n\tcss_put(&memcg->css);\n\n\treturn ret;\n}\n\n \nint mem_cgroup_swapin_charge_folio(struct folio *folio, struct mm_struct *mm,\n\t\t\t\t  gfp_t gfp, swp_entry_t entry)\n{\n\tstruct mem_cgroup *memcg;\n\tunsigned short id;\n\tint ret;\n\n\tif (mem_cgroup_disabled())\n\t\treturn 0;\n\n\tid = lookup_swap_cgroup_id(entry);\n\trcu_read_lock();\n\tmemcg = mem_cgroup_from_id(id);\n\tif (!memcg || !css_tryget_online(&memcg->css))\n\t\tmemcg = get_mem_cgroup_from_mm(mm);\n\trcu_read_unlock();\n\n\tret = charge_memcg(folio, memcg, gfp);\n\n\tcss_put(&memcg->css);\n\treturn ret;\n}\n\n \nvoid mem_cgroup_swapin_uncharge_swap(swp_entry_t entry)\n{\n\t \n\tif (!mem_cgroup_disabled() && do_memsw_account()) {\n\t\t \n\t\tmem_cgroup_uncharge_swap(entry, 1);\n\t}\n}\n\nstruct uncharge_gather {\n\tstruct mem_cgroup *memcg;\n\tunsigned long nr_memory;\n\tunsigned long pgpgout;\n\tunsigned long nr_kmem;\n\tint nid;\n};\n\nstatic inline void uncharge_gather_clear(struct uncharge_gather *ug)\n{\n\tmemset(ug, 0, sizeof(*ug));\n}\n\nstatic void uncharge_batch(const struct uncharge_gather *ug)\n{\n\tunsigned long flags;\n\n\tif (ug->nr_memory) {\n\t\tpage_counter_uncharge(&ug->memcg->memory, ug->nr_memory);\n\t\tif (do_memsw_account())\n\t\t\tpage_counter_uncharge(&ug->memcg->memsw, ug->nr_memory);\n\t\tif (ug->nr_kmem)\n\t\t\tmemcg_account_kmem(ug->memcg, -ug->nr_kmem);\n\t\tmemcg_oom_recover(ug->memcg);\n\t}\n\n\tlocal_irq_save(flags);\n\t__count_memcg_events(ug->memcg, PGPGOUT, ug->pgpgout);\n\t__this_cpu_add(ug->memcg->vmstats_percpu->nr_page_events, ug->nr_memory);\n\tmemcg_check_events(ug->memcg, ug->nid);\n\tlocal_irq_restore(flags);\n\n\t \n\tcss_put(&ug->memcg->css);\n}\n\nstatic void uncharge_folio(struct folio *folio, struct uncharge_gather *ug)\n{\n\tlong nr_pages;\n\tstruct mem_cgroup *memcg;\n\tstruct obj_cgroup *objcg;\n\n\tVM_BUG_ON_FOLIO(folio_test_lru(folio), folio);\n\n\t \n\tif (folio_memcg_kmem(folio)) {\n\t\tobjcg = __folio_objcg(folio);\n\t\t \n\t\tmemcg = get_mem_cgroup_from_objcg(objcg);\n\t} else {\n\t\tmemcg = __folio_memcg(folio);\n\t}\n\n\tif (!memcg)\n\t\treturn;\n\n\tif (ug->memcg != memcg) {\n\t\tif (ug->memcg) {\n\t\t\tuncharge_batch(ug);\n\t\t\tuncharge_gather_clear(ug);\n\t\t}\n\t\tug->memcg = memcg;\n\t\tug->nid = folio_nid(folio);\n\n\t\t \n\t\tcss_get(&memcg->css);\n\t}\n\n\tnr_pages = folio_nr_pages(folio);\n\n\tif (folio_memcg_kmem(folio)) {\n\t\tug->nr_memory += nr_pages;\n\t\tug->nr_kmem += nr_pages;\n\n\t\tfolio->memcg_data = 0;\n\t\tobj_cgroup_put(objcg);\n\t} else {\n\t\t \n\t\tif (!mem_cgroup_is_root(memcg))\n\t\t\tug->nr_memory += nr_pages;\n\t\tug->pgpgout++;\n\n\t\tfolio->memcg_data = 0;\n\t}\n\n\tcss_put(&memcg->css);\n}\n\nvoid __mem_cgroup_uncharge(struct folio *folio)\n{\n\tstruct uncharge_gather ug;\n\n\t \n\tif (!folio_memcg(folio))\n\t\treturn;\n\n\tuncharge_gather_clear(&ug);\n\tuncharge_folio(folio, &ug);\n\tuncharge_batch(&ug);\n}\n\n \nvoid __mem_cgroup_uncharge_list(struct list_head *page_list)\n{\n\tstruct uncharge_gather ug;\n\tstruct folio *folio;\n\n\tuncharge_gather_clear(&ug);\n\tlist_for_each_entry(folio, page_list, lru)\n\t\tuncharge_folio(folio, &ug);\n\tif (ug.memcg)\n\t\tuncharge_batch(&ug);\n}\n\n \nvoid mem_cgroup_migrate(struct folio *old, struct folio *new)\n{\n\tstruct mem_cgroup *memcg;\n\tlong nr_pages = folio_nr_pages(new);\n\tunsigned long flags;\n\n\tVM_BUG_ON_FOLIO(!folio_test_locked(old), old);\n\tVM_BUG_ON_FOLIO(!folio_test_locked(new), new);\n\tVM_BUG_ON_FOLIO(folio_test_anon(old) != folio_test_anon(new), new);\n\tVM_BUG_ON_FOLIO(folio_nr_pages(old) != nr_pages, new);\n\n\tif (mem_cgroup_disabled())\n\t\treturn;\n\n\t \n\tif (folio_memcg(new))\n\t\treturn;\n\n\tmemcg = folio_memcg(old);\n\tVM_WARN_ON_ONCE_FOLIO(!memcg, old);\n\tif (!memcg)\n\t\treturn;\n\n\t \n\tif (!mem_cgroup_is_root(memcg)) {\n\t\tpage_counter_charge(&memcg->memory, nr_pages);\n\t\tif (do_memsw_account())\n\t\t\tpage_counter_charge(&memcg->memsw, nr_pages);\n\t}\n\n\tcss_get(&memcg->css);\n\tcommit_charge(new, memcg);\n\n\tlocal_irq_save(flags);\n\tmem_cgroup_charge_statistics(memcg, nr_pages);\n\tmemcg_check_events(memcg, folio_nid(new));\n\tlocal_irq_restore(flags);\n}\n\nDEFINE_STATIC_KEY_FALSE(memcg_sockets_enabled_key);\nEXPORT_SYMBOL(memcg_sockets_enabled_key);\n\nvoid mem_cgroup_sk_alloc(struct sock *sk)\n{\n\tstruct mem_cgroup *memcg;\n\n\tif (!mem_cgroup_sockets_enabled)\n\t\treturn;\n\n\t \n\tif (!in_task())\n\t\treturn;\n\n\trcu_read_lock();\n\tmemcg = mem_cgroup_from_task(current);\n\tif (mem_cgroup_is_root(memcg))\n\t\tgoto out;\n\tif (!cgroup_subsys_on_dfl(memory_cgrp_subsys) && !memcg->tcpmem_active)\n\t\tgoto out;\n\tif (css_tryget(&memcg->css))\n\t\tsk->sk_memcg = memcg;\nout:\n\trcu_read_unlock();\n}\n\nvoid mem_cgroup_sk_free(struct sock *sk)\n{\n\tif (sk->sk_memcg)\n\t\tcss_put(&sk->sk_memcg->css);\n}\n\n \nbool mem_cgroup_charge_skmem(struct mem_cgroup *memcg, unsigned int nr_pages,\n\t\t\t     gfp_t gfp_mask)\n{\n\tif (!cgroup_subsys_on_dfl(memory_cgrp_subsys)) {\n\t\tstruct page_counter *fail;\n\n\t\tif (page_counter_try_charge(&memcg->tcpmem, nr_pages, &fail)) {\n\t\t\tmemcg->tcpmem_pressure = 0;\n\t\t\treturn true;\n\t\t}\n\t\tmemcg->tcpmem_pressure = 1;\n\t\tif (gfp_mask & __GFP_NOFAIL) {\n\t\t\tpage_counter_charge(&memcg->tcpmem, nr_pages);\n\t\t\treturn true;\n\t\t}\n\t\treturn false;\n\t}\n\n\tif (try_charge(memcg, gfp_mask, nr_pages) == 0) {\n\t\tmod_memcg_state(memcg, MEMCG_SOCK, nr_pages);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nvoid mem_cgroup_uncharge_skmem(struct mem_cgroup *memcg, unsigned int nr_pages)\n{\n\tif (!cgroup_subsys_on_dfl(memory_cgrp_subsys)) {\n\t\tpage_counter_uncharge(&memcg->tcpmem, nr_pages);\n\t\treturn;\n\t}\n\n\tmod_memcg_state(memcg, MEMCG_SOCK, -nr_pages);\n\n\trefill_stock(memcg, nr_pages);\n}\n\nstatic int __init cgroup_memory(char *s)\n{\n\tchar *token;\n\n\twhile ((token = strsep(&s, \",\")) != NULL) {\n\t\tif (!*token)\n\t\t\tcontinue;\n\t\tif (!strcmp(token, \"nosocket\"))\n\t\t\tcgroup_memory_nosocket = true;\n\t\tif (!strcmp(token, \"nokmem\"))\n\t\t\tcgroup_memory_nokmem = true;\n\t\tif (!strcmp(token, \"nobpf\"))\n\t\t\tcgroup_memory_nobpf = true;\n\t}\n\treturn 1;\n}\n__setup(\"cgroup.memory=\", cgroup_memory);\n\n \nstatic int __init mem_cgroup_init(void)\n{\n\tint cpu, node;\n\n\t \n\tBUILD_BUG_ON(MEMCG_CHARGE_BATCH > S32_MAX / PAGE_SIZE);\n\n\tcpuhp_setup_state_nocalls(CPUHP_MM_MEMCQ_DEAD, \"mm/memctrl:dead\", NULL,\n\t\t\t\t  memcg_hotplug_cpu_dead);\n\n\tfor_each_possible_cpu(cpu)\n\t\tINIT_WORK(&per_cpu_ptr(&memcg_stock, cpu)->work,\n\t\t\t  drain_local_stock);\n\n\tfor_each_node(node) {\n\t\tstruct mem_cgroup_tree_per_node *rtpn;\n\n\t\trtpn = kzalloc_node(sizeof(*rtpn), GFP_KERNEL, node);\n\n\t\trtpn->rb_root = RB_ROOT;\n\t\trtpn->rb_rightmost = NULL;\n\t\tspin_lock_init(&rtpn->lock);\n\t\tsoft_limit_tree.rb_tree_per_node[node] = rtpn;\n\t}\n\n\treturn 0;\n}\nsubsys_initcall(mem_cgroup_init);\n\n#ifdef CONFIG_SWAP\nstatic struct mem_cgroup *mem_cgroup_id_get_online(struct mem_cgroup *memcg)\n{\n\twhile (!refcount_inc_not_zero(&memcg->id.ref)) {\n\t\t \n\t\tif (WARN_ON_ONCE(mem_cgroup_is_root(memcg))) {\n\t\t\tVM_BUG_ON(1);\n\t\t\tbreak;\n\t\t}\n\t\tmemcg = parent_mem_cgroup(memcg);\n\t\tif (!memcg)\n\t\t\tmemcg = root_mem_cgroup;\n\t}\n\treturn memcg;\n}\n\n \nvoid mem_cgroup_swapout(struct folio *folio, swp_entry_t entry)\n{\n\tstruct mem_cgroup *memcg, *swap_memcg;\n\tunsigned int nr_entries;\n\tunsigned short oldid;\n\n\tVM_BUG_ON_FOLIO(folio_test_lru(folio), folio);\n\tVM_BUG_ON_FOLIO(folio_ref_count(folio), folio);\n\n\tif (mem_cgroup_disabled())\n\t\treturn;\n\n\tif (!do_memsw_account())\n\t\treturn;\n\n\tmemcg = folio_memcg(folio);\n\n\tVM_WARN_ON_ONCE_FOLIO(!memcg, folio);\n\tif (!memcg)\n\t\treturn;\n\n\t \n\tswap_memcg = mem_cgroup_id_get_online(memcg);\n\tnr_entries = folio_nr_pages(folio);\n\t \n\tif (nr_entries > 1)\n\t\tmem_cgroup_id_get_many(swap_memcg, nr_entries - 1);\n\toldid = swap_cgroup_record(entry, mem_cgroup_id(swap_memcg),\n\t\t\t\t   nr_entries);\n\tVM_BUG_ON_FOLIO(oldid, folio);\n\tmod_memcg_state(swap_memcg, MEMCG_SWAP, nr_entries);\n\n\tfolio->memcg_data = 0;\n\n\tif (!mem_cgroup_is_root(memcg))\n\t\tpage_counter_uncharge(&memcg->memory, nr_entries);\n\n\tif (memcg != swap_memcg) {\n\t\tif (!mem_cgroup_is_root(swap_memcg))\n\t\t\tpage_counter_charge(&swap_memcg->memsw, nr_entries);\n\t\tpage_counter_uncharge(&memcg->memsw, nr_entries);\n\t}\n\n\t \n\tmemcg_stats_lock();\n\tmem_cgroup_charge_statistics(memcg, -nr_entries);\n\tmemcg_stats_unlock();\n\tmemcg_check_events(memcg, folio_nid(folio));\n\n\tcss_put(&memcg->css);\n}\n\n \nint __mem_cgroup_try_charge_swap(struct folio *folio, swp_entry_t entry)\n{\n\tunsigned int nr_pages = folio_nr_pages(folio);\n\tstruct page_counter *counter;\n\tstruct mem_cgroup *memcg;\n\tunsigned short oldid;\n\n\tif (do_memsw_account())\n\t\treturn 0;\n\n\tmemcg = folio_memcg(folio);\n\n\tVM_WARN_ON_ONCE_FOLIO(!memcg, folio);\n\tif (!memcg)\n\t\treturn 0;\n\n\tif (!entry.val) {\n\t\tmemcg_memory_event(memcg, MEMCG_SWAP_FAIL);\n\t\treturn 0;\n\t}\n\n\tmemcg = mem_cgroup_id_get_online(memcg);\n\n\tif (!mem_cgroup_is_root(memcg) &&\n\t    !page_counter_try_charge(&memcg->swap, nr_pages, &counter)) {\n\t\tmemcg_memory_event(memcg, MEMCG_SWAP_MAX);\n\t\tmemcg_memory_event(memcg, MEMCG_SWAP_FAIL);\n\t\tmem_cgroup_id_put(memcg);\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tif (nr_pages > 1)\n\t\tmem_cgroup_id_get_many(memcg, nr_pages - 1);\n\toldid = swap_cgroup_record(entry, mem_cgroup_id(memcg), nr_pages);\n\tVM_BUG_ON_FOLIO(oldid, folio);\n\tmod_memcg_state(memcg, MEMCG_SWAP, nr_pages);\n\n\treturn 0;\n}\n\n \nvoid __mem_cgroup_uncharge_swap(swp_entry_t entry, unsigned int nr_pages)\n{\n\tstruct mem_cgroup *memcg;\n\tunsigned short id;\n\n\tid = swap_cgroup_record(entry, 0, nr_pages);\n\trcu_read_lock();\n\tmemcg = mem_cgroup_from_id(id);\n\tif (memcg) {\n\t\tif (!mem_cgroup_is_root(memcg)) {\n\t\t\tif (do_memsw_account())\n\t\t\t\tpage_counter_uncharge(&memcg->memsw, nr_pages);\n\t\t\telse\n\t\t\t\tpage_counter_uncharge(&memcg->swap, nr_pages);\n\t\t}\n\t\tmod_memcg_state(memcg, MEMCG_SWAP, -nr_pages);\n\t\tmem_cgroup_id_put_many(memcg, nr_pages);\n\t}\n\trcu_read_unlock();\n}\n\nlong mem_cgroup_get_nr_swap_pages(struct mem_cgroup *memcg)\n{\n\tlong nr_swap_pages = get_nr_swap_pages();\n\n\tif (mem_cgroup_disabled() || do_memsw_account())\n\t\treturn nr_swap_pages;\n\tfor (; !mem_cgroup_is_root(memcg); memcg = parent_mem_cgroup(memcg))\n\t\tnr_swap_pages = min_t(long, nr_swap_pages,\n\t\t\t\t      READ_ONCE(memcg->swap.max) -\n\t\t\t\t      page_counter_read(&memcg->swap));\n\treturn nr_swap_pages;\n}\n\nbool mem_cgroup_swap_full(struct folio *folio)\n{\n\tstruct mem_cgroup *memcg;\n\n\tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n\n\tif (vm_swap_full())\n\t\treturn true;\n\tif (do_memsw_account())\n\t\treturn false;\n\n\tmemcg = folio_memcg(folio);\n\tif (!memcg)\n\t\treturn false;\n\n\tfor (; !mem_cgroup_is_root(memcg); memcg = parent_mem_cgroup(memcg)) {\n\t\tunsigned long usage = page_counter_read(&memcg->swap);\n\n\t\tif (usage * 2 >= READ_ONCE(memcg->swap.high) ||\n\t\t    usage * 2 >= READ_ONCE(memcg->swap.max))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic int __init setup_swap_account(char *s)\n{\n\tpr_warn_once(\"The swapaccount= commandline option is deprecated. \"\n\t\t     \"Please report your usecase to linux-mm@kvack.org if you \"\n\t\t     \"depend on this functionality.\\n\");\n\treturn 1;\n}\n__setup(\"swapaccount=\", setup_swap_account);\n\nstatic u64 swap_current_read(struct cgroup_subsys_state *css,\n\t\t\t     struct cftype *cft)\n{\n\tstruct mem_cgroup *memcg = mem_cgroup_from_css(css);\n\n\treturn (u64)page_counter_read(&memcg->swap) * PAGE_SIZE;\n}\n\nstatic u64 swap_peak_read(struct cgroup_subsys_state *css,\n\t\t\t  struct cftype *cft)\n{\n\tstruct mem_cgroup *memcg = mem_cgroup_from_css(css);\n\n\treturn (u64)memcg->swap.watermark * PAGE_SIZE;\n}\n\nstatic int swap_high_show(struct seq_file *m, void *v)\n{\n\treturn seq_puts_memcg_tunable(m,\n\t\tREAD_ONCE(mem_cgroup_from_seq(m)->swap.high));\n}\n\nstatic ssize_t swap_high_write(struct kernfs_open_file *of,\n\t\t\t       char *buf, size_t nbytes, loff_t off)\n{\n\tstruct mem_cgroup *memcg = mem_cgroup_from_css(of_css(of));\n\tunsigned long high;\n\tint err;\n\n\tbuf = strstrip(buf);\n\terr = page_counter_memparse(buf, \"max\", &high);\n\tif (err)\n\t\treturn err;\n\n\tpage_counter_set_high(&memcg->swap, high);\n\n\treturn nbytes;\n}\n\nstatic int swap_max_show(struct seq_file *m, void *v)\n{\n\treturn seq_puts_memcg_tunable(m,\n\t\tREAD_ONCE(mem_cgroup_from_seq(m)->swap.max));\n}\n\nstatic ssize_t swap_max_write(struct kernfs_open_file *of,\n\t\t\t      char *buf, size_t nbytes, loff_t off)\n{\n\tstruct mem_cgroup *memcg = mem_cgroup_from_css(of_css(of));\n\tunsigned long max;\n\tint err;\n\n\tbuf = strstrip(buf);\n\terr = page_counter_memparse(buf, \"max\", &max);\n\tif (err)\n\t\treturn err;\n\n\txchg(&memcg->swap.max, max);\n\n\treturn nbytes;\n}\n\nstatic int swap_events_show(struct seq_file *m, void *v)\n{\n\tstruct mem_cgroup *memcg = mem_cgroup_from_seq(m);\n\n\tseq_printf(m, \"high %lu\\n\",\n\t\t   atomic_long_read(&memcg->memory_events[MEMCG_SWAP_HIGH]));\n\tseq_printf(m, \"max %lu\\n\",\n\t\t   atomic_long_read(&memcg->memory_events[MEMCG_SWAP_MAX]));\n\tseq_printf(m, \"fail %lu\\n\",\n\t\t   atomic_long_read(&memcg->memory_events[MEMCG_SWAP_FAIL]));\n\n\treturn 0;\n}\n\nstatic struct cftype swap_files[] = {\n\t{\n\t\t.name = \"swap.current\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.read_u64 = swap_current_read,\n\t},\n\t{\n\t\t.name = \"swap.high\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.seq_show = swap_high_show,\n\t\t.write = swap_high_write,\n\t},\n\t{\n\t\t.name = \"swap.max\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.seq_show = swap_max_show,\n\t\t.write = swap_max_write,\n\t},\n\t{\n\t\t.name = \"swap.peak\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.read_u64 = swap_peak_read,\n\t},\n\t{\n\t\t.name = \"swap.events\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.file_offset = offsetof(struct mem_cgroup, swap_events_file),\n\t\t.seq_show = swap_events_show,\n\t},\n\t{ }\t \n};\n\nstatic struct cftype memsw_files[] = {\n\t{\n\t\t.name = \"memsw.usage_in_bytes\",\n\t\t.private = MEMFILE_PRIVATE(_MEMSWAP, RES_USAGE),\n\t\t.read_u64 = mem_cgroup_read_u64,\n\t},\n\t{\n\t\t.name = \"memsw.max_usage_in_bytes\",\n\t\t.private = MEMFILE_PRIVATE(_MEMSWAP, RES_MAX_USAGE),\n\t\t.write = mem_cgroup_reset,\n\t\t.read_u64 = mem_cgroup_read_u64,\n\t},\n\t{\n\t\t.name = \"memsw.limit_in_bytes\",\n\t\t.private = MEMFILE_PRIVATE(_MEMSWAP, RES_LIMIT),\n\t\t.write = mem_cgroup_write,\n\t\t.read_u64 = mem_cgroup_read_u64,\n\t},\n\t{\n\t\t.name = \"memsw.failcnt\",\n\t\t.private = MEMFILE_PRIVATE(_MEMSWAP, RES_FAILCNT),\n\t\t.write = mem_cgroup_reset,\n\t\t.read_u64 = mem_cgroup_read_u64,\n\t},\n\t{ },\t \n};\n\n#if defined(CONFIG_MEMCG_KMEM) && defined(CONFIG_ZSWAP)\n \nbool obj_cgroup_may_zswap(struct obj_cgroup *objcg)\n{\n\tstruct mem_cgroup *memcg, *original_memcg;\n\tbool ret = true;\n\n\tif (!cgroup_subsys_on_dfl(memory_cgrp_subsys))\n\t\treturn true;\n\n\toriginal_memcg = get_mem_cgroup_from_objcg(objcg);\n\tfor (memcg = original_memcg; !mem_cgroup_is_root(memcg);\n\t     memcg = parent_mem_cgroup(memcg)) {\n\t\tunsigned long max = READ_ONCE(memcg->zswap_max);\n\t\tunsigned long pages;\n\n\t\tif (max == PAGE_COUNTER_MAX)\n\t\t\tcontinue;\n\t\tif (max == 0) {\n\t\t\tret = false;\n\t\t\tbreak;\n\t\t}\n\n\t\tcgroup_rstat_flush(memcg->css.cgroup);\n\t\tpages = memcg_page_state(memcg, MEMCG_ZSWAP_B) / PAGE_SIZE;\n\t\tif (pages < max)\n\t\t\tcontinue;\n\t\tret = false;\n\t\tbreak;\n\t}\n\tmem_cgroup_put(original_memcg);\n\treturn ret;\n}\n\n \nvoid obj_cgroup_charge_zswap(struct obj_cgroup *objcg, size_t size)\n{\n\tstruct mem_cgroup *memcg;\n\n\tif (!cgroup_subsys_on_dfl(memory_cgrp_subsys))\n\t\treturn;\n\n\tVM_WARN_ON_ONCE(!(current->flags & PF_MEMALLOC));\n\n\t \n\tif (obj_cgroup_charge(objcg, GFP_KERNEL, size))\n\t\tVM_WARN_ON_ONCE(1);\n\n\trcu_read_lock();\n\tmemcg = obj_cgroup_memcg(objcg);\n\tmod_memcg_state(memcg, MEMCG_ZSWAP_B, size);\n\tmod_memcg_state(memcg, MEMCG_ZSWAPPED, 1);\n\trcu_read_unlock();\n}\n\n \nvoid obj_cgroup_uncharge_zswap(struct obj_cgroup *objcg, size_t size)\n{\n\tstruct mem_cgroup *memcg;\n\n\tif (!cgroup_subsys_on_dfl(memory_cgrp_subsys))\n\t\treturn;\n\n\tobj_cgroup_uncharge(objcg, size);\n\n\trcu_read_lock();\n\tmemcg = obj_cgroup_memcg(objcg);\n\tmod_memcg_state(memcg, MEMCG_ZSWAP_B, -size);\n\tmod_memcg_state(memcg, MEMCG_ZSWAPPED, -1);\n\trcu_read_unlock();\n}\n\nstatic u64 zswap_current_read(struct cgroup_subsys_state *css,\n\t\t\t      struct cftype *cft)\n{\n\tcgroup_rstat_flush(css->cgroup);\n\treturn memcg_page_state(mem_cgroup_from_css(css), MEMCG_ZSWAP_B);\n}\n\nstatic int zswap_max_show(struct seq_file *m, void *v)\n{\n\treturn seq_puts_memcg_tunable(m,\n\t\tREAD_ONCE(mem_cgroup_from_seq(m)->zswap_max));\n}\n\nstatic ssize_t zswap_max_write(struct kernfs_open_file *of,\n\t\t\t       char *buf, size_t nbytes, loff_t off)\n{\n\tstruct mem_cgroup *memcg = mem_cgroup_from_css(of_css(of));\n\tunsigned long max;\n\tint err;\n\n\tbuf = strstrip(buf);\n\terr = page_counter_memparse(buf, \"max\", &max);\n\tif (err)\n\t\treturn err;\n\n\txchg(&memcg->zswap_max, max);\n\n\treturn nbytes;\n}\n\nstatic struct cftype zswap_files[] = {\n\t{\n\t\t.name = \"zswap.current\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.read_u64 = zswap_current_read,\n\t},\n\t{\n\t\t.name = \"zswap.max\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.seq_show = zswap_max_show,\n\t\t.write = zswap_max_write,\n\t},\n\t{ }\t \n};\n#endif  \n\nstatic int __init mem_cgroup_swap_init(void)\n{\n\tif (mem_cgroup_disabled())\n\t\treturn 0;\n\n\tWARN_ON(cgroup_add_dfl_cftypes(&memory_cgrp_subsys, swap_files));\n\tWARN_ON(cgroup_add_legacy_cftypes(&memory_cgrp_subsys, memsw_files));\n#if defined(CONFIG_MEMCG_KMEM) && defined(CONFIG_ZSWAP)\n\tWARN_ON(cgroup_add_dfl_cftypes(&memory_cgrp_subsys, zswap_files));\n#endif\n\treturn 0;\n}\nsubsys_initcall(mem_cgroup_swap_init);\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}