{
  "module_name": "gup.c",
  "hash_id": "35557fe70a185bba0500ad6ed89797bdc0d31f86efc4415c31f26d8a048faf30",
  "original_prompt": "Ingested from linux-6.6.14/mm/gup.c",
  "human_readable_source": "\n#include <linux/kernel.h>\n#include <linux/errno.h>\n#include <linux/err.h>\n#include <linux/spinlock.h>\n\n#include <linux/mm.h>\n#include <linux/memremap.h>\n#include <linux/pagemap.h>\n#include <linux/rmap.h>\n#include <linux/swap.h>\n#include <linux/swapops.h>\n#include <linux/secretmem.h>\n\n#include <linux/sched/signal.h>\n#include <linux/rwsem.h>\n#include <linux/hugetlb.h>\n#include <linux/migrate.h>\n#include <linux/mm_inline.h>\n#include <linux/sched/mm.h>\n#include <linux/shmem_fs.h>\n\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n\n#include \"internal.h\"\n\nstruct follow_page_context {\n\tstruct dev_pagemap *pgmap;\n\tunsigned int page_mask;\n};\n\nstatic inline void sanity_check_pinned_pages(struct page **pages,\n\t\t\t\t\t     unsigned long npages)\n{\n\tif (!IS_ENABLED(CONFIG_DEBUG_VM))\n\t\treturn;\n\n\t \n\tfor (; npages; npages--, pages++) {\n\t\tstruct page *page = *pages;\n\t\tstruct folio *folio = page_folio(page);\n\n\t\tif (is_zero_page(page) ||\n\t\t    !folio_test_anon(folio))\n\t\t\tcontinue;\n\t\tif (!folio_test_large(folio) || folio_test_hugetlb(folio))\n\t\t\tVM_BUG_ON_PAGE(!PageAnonExclusive(&folio->page), page);\n\t\telse\n\t\t\t \n\t\t\tVM_BUG_ON_PAGE(!PageAnonExclusive(&folio->page) &&\n\t\t\t\t       !PageAnonExclusive(page), page);\n\t}\n}\n\n \nstatic inline struct folio *try_get_folio(struct page *page, int refs)\n{\n\tstruct folio *folio;\n\nretry:\n\tfolio = page_folio(page);\n\tif (WARN_ON_ONCE(folio_ref_count(folio) < 0))\n\t\treturn NULL;\n\tif (unlikely(!folio_ref_try_add_rcu(folio, refs)))\n\t\treturn NULL;\n\n\t \n\tif (unlikely(page_folio(page) != folio)) {\n\t\tif (!put_devmap_managed_page_refs(&folio->page, refs))\n\t\t\tfolio_put_refs(folio, refs);\n\t\tgoto retry;\n\t}\n\n\treturn folio;\n}\n\n \nstruct folio *try_grab_folio(struct page *page, int refs, unsigned int flags)\n{\n\tstruct folio *folio;\n\n\tif (WARN_ON_ONCE((flags & (FOLL_GET | FOLL_PIN)) == 0))\n\t\treturn NULL;\n\n\tif (unlikely(!(flags & FOLL_PCI_P2PDMA) && is_pci_p2pdma_page(page)))\n\t\treturn NULL;\n\n\tif (flags & FOLL_GET)\n\t\treturn try_get_folio(page, refs);\n\n\t \n\n\t \n\tif (is_zero_page(page))\n\t\treturn page_folio(page);\n\n\tfolio = try_get_folio(page, refs);\n\tif (!folio)\n\t\treturn NULL;\n\n\t \n\tif (unlikely((flags & FOLL_LONGTERM) &&\n\t\t     !folio_is_longterm_pinnable(folio))) {\n\t\tif (!put_devmap_managed_page_refs(&folio->page, refs))\n\t\t\tfolio_put_refs(folio, refs);\n\t\treturn NULL;\n\t}\n\n\t \n\tif (folio_test_large(folio))\n\t\tatomic_add(refs, &folio->_pincount);\n\telse\n\t\tfolio_ref_add(folio,\n\t\t\t\trefs * (GUP_PIN_COUNTING_BIAS - 1));\n\t \n\tsmp_mb__after_atomic();\n\n\tnode_stat_mod_folio(folio, NR_FOLL_PIN_ACQUIRED, refs);\n\n\treturn folio;\n}\n\nstatic void gup_put_folio(struct folio *folio, int refs, unsigned int flags)\n{\n\tif (flags & FOLL_PIN) {\n\t\tif (is_zero_folio(folio))\n\t\t\treturn;\n\t\tnode_stat_mod_folio(folio, NR_FOLL_PIN_RELEASED, refs);\n\t\tif (folio_test_large(folio))\n\t\t\tatomic_sub(refs, &folio->_pincount);\n\t\telse\n\t\t\trefs *= GUP_PIN_COUNTING_BIAS;\n\t}\n\n\tif (!put_devmap_managed_page_refs(&folio->page, refs))\n\t\tfolio_put_refs(folio, refs);\n}\n\n \nint __must_check try_grab_page(struct page *page, unsigned int flags)\n{\n\tstruct folio *folio = page_folio(page);\n\n\tif (WARN_ON_ONCE(folio_ref_count(folio) <= 0))\n\t\treturn -ENOMEM;\n\n\tif (unlikely(!(flags & FOLL_PCI_P2PDMA) && is_pci_p2pdma_page(page)))\n\t\treturn -EREMOTEIO;\n\n\tif (flags & FOLL_GET)\n\t\tfolio_ref_inc(folio);\n\telse if (flags & FOLL_PIN) {\n\t\t \n\t\tif (is_zero_page(page))\n\t\t\treturn 0;\n\n\t\t \n\t\tif (folio_test_large(folio)) {\n\t\t\tfolio_ref_add(folio, 1);\n\t\t\tatomic_add(1, &folio->_pincount);\n\t\t} else {\n\t\t\tfolio_ref_add(folio, GUP_PIN_COUNTING_BIAS);\n\t\t}\n\n\t\tnode_stat_mod_folio(folio, NR_FOLL_PIN_ACQUIRED, 1);\n\t}\n\n\treturn 0;\n}\n\n \nvoid unpin_user_page(struct page *page)\n{\n\tsanity_check_pinned_pages(&page, 1);\n\tgup_put_folio(page_folio(page), 1, FOLL_PIN);\n}\nEXPORT_SYMBOL(unpin_user_page);\n\n \nvoid folio_add_pin(struct folio *folio)\n{\n\tif (is_zero_folio(folio))\n\t\treturn;\n\n\t \n\tif (folio_test_large(folio)) {\n\t\tWARN_ON_ONCE(atomic_read(&folio->_pincount) < 1);\n\t\tfolio_ref_inc(folio);\n\t\tatomic_inc(&folio->_pincount);\n\t} else {\n\t\tWARN_ON_ONCE(folio_ref_count(folio) < GUP_PIN_COUNTING_BIAS);\n\t\tfolio_ref_add(folio, GUP_PIN_COUNTING_BIAS);\n\t}\n}\n\nstatic inline struct folio *gup_folio_range_next(struct page *start,\n\t\tunsigned long npages, unsigned long i, unsigned int *ntails)\n{\n\tstruct page *next = nth_page(start, i);\n\tstruct folio *folio = page_folio(next);\n\tunsigned int nr = 1;\n\n\tif (folio_test_large(folio))\n\t\tnr = min_t(unsigned int, npages - i,\n\t\t\t   folio_nr_pages(folio) - folio_page_idx(folio, next));\n\n\t*ntails = nr;\n\treturn folio;\n}\n\nstatic inline struct folio *gup_folio_next(struct page **list,\n\t\tunsigned long npages, unsigned long i, unsigned int *ntails)\n{\n\tstruct folio *folio = page_folio(list[i]);\n\tunsigned int nr;\n\n\tfor (nr = i + 1; nr < npages; nr++) {\n\t\tif (page_folio(list[nr]) != folio)\n\t\t\tbreak;\n\t}\n\n\t*ntails = nr - i;\n\treturn folio;\n}\n\n \nvoid unpin_user_pages_dirty_lock(struct page **pages, unsigned long npages,\n\t\t\t\t bool make_dirty)\n{\n\tunsigned long i;\n\tstruct folio *folio;\n\tunsigned int nr;\n\n\tif (!make_dirty) {\n\t\tunpin_user_pages(pages, npages);\n\t\treturn;\n\t}\n\n\tsanity_check_pinned_pages(pages, npages);\n\tfor (i = 0; i < npages; i += nr) {\n\t\tfolio = gup_folio_next(pages, npages, i, &nr);\n\t\t \n\t\tif (!folio_test_dirty(folio)) {\n\t\t\tfolio_lock(folio);\n\t\t\tfolio_mark_dirty(folio);\n\t\t\tfolio_unlock(folio);\n\t\t}\n\t\tgup_put_folio(folio, nr, FOLL_PIN);\n\t}\n}\nEXPORT_SYMBOL(unpin_user_pages_dirty_lock);\n\n \nvoid unpin_user_page_range_dirty_lock(struct page *page, unsigned long npages,\n\t\t\t\t      bool make_dirty)\n{\n\tunsigned long i;\n\tstruct folio *folio;\n\tunsigned int nr;\n\n\tfor (i = 0; i < npages; i += nr) {\n\t\tfolio = gup_folio_range_next(page, npages, i, &nr);\n\t\tif (make_dirty && !folio_test_dirty(folio)) {\n\t\t\tfolio_lock(folio);\n\t\t\tfolio_mark_dirty(folio);\n\t\t\tfolio_unlock(folio);\n\t\t}\n\t\tgup_put_folio(folio, nr, FOLL_PIN);\n\t}\n}\nEXPORT_SYMBOL(unpin_user_page_range_dirty_lock);\n\nstatic void unpin_user_pages_lockless(struct page **pages, unsigned long npages)\n{\n\tunsigned long i;\n\tstruct folio *folio;\n\tunsigned int nr;\n\n\t \n\tfor (i = 0; i < npages; i += nr) {\n\t\tfolio = gup_folio_next(pages, npages, i, &nr);\n\t\tgup_put_folio(folio, nr, FOLL_PIN);\n\t}\n}\n\n \nvoid unpin_user_pages(struct page **pages, unsigned long npages)\n{\n\tunsigned long i;\n\tstruct folio *folio;\n\tunsigned int nr;\n\n\t \n\tif (WARN_ON(IS_ERR_VALUE(npages)))\n\t\treturn;\n\n\tsanity_check_pinned_pages(pages, npages);\n\tfor (i = 0; i < npages; i += nr) {\n\t\tfolio = gup_folio_next(pages, npages, i, &nr);\n\t\tgup_put_folio(folio, nr, FOLL_PIN);\n\t}\n}\nEXPORT_SYMBOL(unpin_user_pages);\n\n \nstatic inline void mm_set_has_pinned_flag(unsigned long *mm_flags)\n{\n\tif (!test_bit(MMF_HAS_PINNED, mm_flags))\n\t\tset_bit(MMF_HAS_PINNED, mm_flags);\n}\n\n#ifdef CONFIG_MMU\nstatic struct page *no_page_table(struct vm_area_struct *vma,\n\t\tunsigned int flags)\n{\n\t \n\tif ((flags & FOLL_DUMP) &&\n\t\t\t(vma_is_anonymous(vma) || !vma->vm_ops->fault))\n\t\treturn ERR_PTR(-EFAULT);\n\treturn NULL;\n}\n\nstatic int follow_pfn_pte(struct vm_area_struct *vma, unsigned long address,\n\t\tpte_t *pte, unsigned int flags)\n{\n\tif (flags & FOLL_TOUCH) {\n\t\tpte_t orig_entry = ptep_get(pte);\n\t\tpte_t entry = orig_entry;\n\n\t\tif (flags & FOLL_WRITE)\n\t\t\tentry = pte_mkdirty(entry);\n\t\tentry = pte_mkyoung(entry);\n\n\t\tif (!pte_same(orig_entry, entry)) {\n\t\t\tset_pte_at(vma->vm_mm, address, pte, entry);\n\t\t\tupdate_mmu_cache(vma, address, pte);\n\t\t}\n\t}\n\n\t \n\treturn -EEXIST;\n}\n\n \nstatic inline bool can_follow_write_pte(pte_t pte, struct page *page,\n\t\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\t\tunsigned int flags)\n{\n\t \n\tif (pte_write(pte))\n\t\treturn true;\n\n\t \n\tif (!(flags & FOLL_FORCE))\n\t\treturn false;\n\n\t \n\tif (vma->vm_flags & (VM_MAYSHARE | VM_SHARED))\n\t\treturn false;\n\n\t \n\tif (!(vma->vm_flags & VM_MAYWRITE))\n\t\treturn false;\n\n\t \n\tif (vma->vm_flags & VM_WRITE)\n\t\treturn false;\n\n\t \n\tif (!page || !PageAnon(page) || !PageAnonExclusive(page))\n\t\treturn false;\n\n\t \n\tif (vma_soft_dirty_enabled(vma) && !pte_soft_dirty(pte))\n\t\treturn false;\n\treturn !userfaultfd_pte_wp(vma, pte);\n}\n\nstatic struct page *follow_page_pte(struct vm_area_struct *vma,\n\t\tunsigned long address, pmd_t *pmd, unsigned int flags,\n\t\tstruct dev_pagemap **pgmap)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct page *page;\n\tspinlock_t *ptl;\n\tpte_t *ptep, pte;\n\tint ret;\n\n\t \n\tif (WARN_ON_ONCE((flags & (FOLL_PIN | FOLL_GET)) ==\n\t\t\t (FOLL_PIN | FOLL_GET)))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tptep = pte_offset_map_lock(mm, pmd, address, &ptl);\n\tif (!ptep)\n\t\treturn no_page_table(vma, flags);\n\tpte = ptep_get(ptep);\n\tif (!pte_present(pte))\n\t\tgoto no_page;\n\tif (pte_protnone(pte) && !gup_can_follow_protnone(vma, flags))\n\t\tgoto no_page;\n\n\tpage = vm_normal_page(vma, address, pte);\n\n\t \n\tif ((flags & FOLL_WRITE) &&\n\t    !can_follow_write_pte(pte, page, vma, flags)) {\n\t\tpage = NULL;\n\t\tgoto out;\n\t}\n\n\tif (!page && pte_devmap(pte) && (flags & (FOLL_GET | FOLL_PIN))) {\n\t\t \n\t\t*pgmap = get_dev_pagemap(pte_pfn(pte), *pgmap);\n\t\tif (*pgmap)\n\t\t\tpage = pte_page(pte);\n\t\telse\n\t\t\tgoto no_page;\n\t} else if (unlikely(!page)) {\n\t\tif (flags & FOLL_DUMP) {\n\t\t\t \n\t\t\tpage = ERR_PTR(-EFAULT);\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (is_zero_pfn(pte_pfn(pte))) {\n\t\t\tpage = pte_page(pte);\n\t\t} else {\n\t\t\tret = follow_pfn_pte(vma, address, ptep, flags);\n\t\t\tpage = ERR_PTR(ret);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (!pte_write(pte) && gup_must_unshare(vma, flags, page)) {\n\t\tpage = ERR_PTR(-EMLINK);\n\t\tgoto out;\n\t}\n\n\tVM_BUG_ON_PAGE((flags & FOLL_PIN) && PageAnon(page) &&\n\t\t       !PageAnonExclusive(page), page);\n\n\t \n\tret = try_grab_page(page, flags);\n\tif (unlikely(ret)) {\n\t\tpage = ERR_PTR(ret);\n\t\tgoto out;\n\t}\n\n\t \n\tif (flags & FOLL_PIN) {\n\t\tret = arch_make_page_accessible(page);\n\t\tif (ret) {\n\t\t\tunpin_user_page(page);\n\t\t\tpage = ERR_PTR(ret);\n\t\t\tgoto out;\n\t\t}\n\t}\n\tif (flags & FOLL_TOUCH) {\n\t\tif ((flags & FOLL_WRITE) &&\n\t\t    !pte_dirty(pte) && !PageDirty(page))\n\t\t\tset_page_dirty(page);\n\t\t \n\t\tmark_page_accessed(page);\n\t}\nout:\n\tpte_unmap_unlock(ptep, ptl);\n\treturn page;\nno_page:\n\tpte_unmap_unlock(ptep, ptl);\n\tif (!pte_none(pte))\n\t\treturn NULL;\n\treturn no_page_table(vma, flags);\n}\n\nstatic struct page *follow_pmd_mask(struct vm_area_struct *vma,\n\t\t\t\t    unsigned long address, pud_t *pudp,\n\t\t\t\t    unsigned int flags,\n\t\t\t\t    struct follow_page_context *ctx)\n{\n\tpmd_t *pmd, pmdval;\n\tspinlock_t *ptl;\n\tstruct page *page;\n\tstruct mm_struct *mm = vma->vm_mm;\n\n\tpmd = pmd_offset(pudp, address);\n\tpmdval = pmdp_get_lockless(pmd);\n\tif (pmd_none(pmdval))\n\t\treturn no_page_table(vma, flags);\n\tif (!pmd_present(pmdval))\n\t\treturn no_page_table(vma, flags);\n\tif (pmd_devmap(pmdval)) {\n\t\tptl = pmd_lock(mm, pmd);\n\t\tpage = follow_devmap_pmd(vma, address, pmd, flags, &ctx->pgmap);\n\t\tspin_unlock(ptl);\n\t\tif (page)\n\t\t\treturn page;\n\t}\n\tif (likely(!pmd_trans_huge(pmdval)))\n\t\treturn follow_page_pte(vma, address, pmd, flags, &ctx->pgmap);\n\n\tif (pmd_protnone(pmdval) && !gup_can_follow_protnone(vma, flags))\n\t\treturn no_page_table(vma, flags);\n\n\tptl = pmd_lock(mm, pmd);\n\tif (unlikely(!pmd_present(*pmd))) {\n\t\tspin_unlock(ptl);\n\t\treturn no_page_table(vma, flags);\n\t}\n\tif (unlikely(!pmd_trans_huge(*pmd))) {\n\t\tspin_unlock(ptl);\n\t\treturn follow_page_pte(vma, address, pmd, flags, &ctx->pgmap);\n\t}\n\tif (flags & FOLL_SPLIT_PMD) {\n\t\tspin_unlock(ptl);\n\t\tsplit_huge_pmd(vma, pmd, address);\n\t\t \n\t\treturn pte_alloc(mm, pmd) ? ERR_PTR(-ENOMEM) :\n\t\t\tfollow_page_pte(vma, address, pmd, flags, &ctx->pgmap);\n\t}\n\tpage = follow_trans_huge_pmd(vma, address, pmd, flags);\n\tspin_unlock(ptl);\n\tctx->page_mask = HPAGE_PMD_NR - 1;\n\treturn page;\n}\n\nstatic struct page *follow_pud_mask(struct vm_area_struct *vma,\n\t\t\t\t    unsigned long address, p4d_t *p4dp,\n\t\t\t\t    unsigned int flags,\n\t\t\t\t    struct follow_page_context *ctx)\n{\n\tpud_t *pud;\n\tspinlock_t *ptl;\n\tstruct page *page;\n\tstruct mm_struct *mm = vma->vm_mm;\n\n\tpud = pud_offset(p4dp, address);\n\tif (pud_none(*pud))\n\t\treturn no_page_table(vma, flags);\n\tif (pud_devmap(*pud)) {\n\t\tptl = pud_lock(mm, pud);\n\t\tpage = follow_devmap_pud(vma, address, pud, flags, &ctx->pgmap);\n\t\tspin_unlock(ptl);\n\t\tif (page)\n\t\t\treturn page;\n\t}\n\tif (unlikely(pud_bad(*pud)))\n\t\treturn no_page_table(vma, flags);\n\n\treturn follow_pmd_mask(vma, address, pud, flags, ctx);\n}\n\nstatic struct page *follow_p4d_mask(struct vm_area_struct *vma,\n\t\t\t\t    unsigned long address, pgd_t *pgdp,\n\t\t\t\t    unsigned int flags,\n\t\t\t\t    struct follow_page_context *ctx)\n{\n\tp4d_t *p4d;\n\n\tp4d = p4d_offset(pgdp, address);\n\tif (p4d_none(*p4d))\n\t\treturn no_page_table(vma, flags);\n\tBUILD_BUG_ON(p4d_huge(*p4d));\n\tif (unlikely(p4d_bad(*p4d)))\n\t\treturn no_page_table(vma, flags);\n\n\treturn follow_pud_mask(vma, address, p4d, flags, ctx);\n}\n\n \nstatic struct page *follow_page_mask(struct vm_area_struct *vma,\n\t\t\t      unsigned long address, unsigned int flags,\n\t\t\t      struct follow_page_context *ctx)\n{\n\tpgd_t *pgd;\n\tstruct mm_struct *mm = vma->vm_mm;\n\n\tctx->page_mask = 0;\n\n\t \n\tif (is_vm_hugetlb_page(vma))\n\t\treturn hugetlb_follow_page_mask(vma, address, flags,\n\t\t\t\t\t\t&ctx->page_mask);\n\n\tpgd = pgd_offset(mm, address);\n\n\tif (pgd_none(*pgd) || unlikely(pgd_bad(*pgd)))\n\t\treturn no_page_table(vma, flags);\n\n\treturn follow_p4d_mask(vma, address, pgd, flags, ctx);\n}\n\nstruct page *follow_page(struct vm_area_struct *vma, unsigned long address,\n\t\t\t unsigned int foll_flags)\n{\n\tstruct follow_page_context ctx = { NULL };\n\tstruct page *page;\n\n\tif (vma_is_secretmem(vma))\n\t\treturn NULL;\n\n\tif (WARN_ON_ONCE(foll_flags & FOLL_PIN))\n\t\treturn NULL;\n\n\t \n\tpage = follow_page_mask(vma, address, foll_flags, &ctx);\n\tif (ctx.pgmap)\n\t\tput_dev_pagemap(ctx.pgmap);\n\treturn page;\n}\n\nstatic int get_gate_page(struct mm_struct *mm, unsigned long address,\n\t\tunsigned int gup_flags, struct vm_area_struct **vma,\n\t\tstruct page **page)\n{\n\tpgd_t *pgd;\n\tp4d_t *p4d;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\tpte_t *pte;\n\tpte_t entry;\n\tint ret = -EFAULT;\n\n\t \n\tif (gup_flags & FOLL_WRITE)\n\t\treturn -EFAULT;\n\tif (address > TASK_SIZE)\n\t\tpgd = pgd_offset_k(address);\n\telse\n\t\tpgd = pgd_offset_gate(mm, address);\n\tif (pgd_none(*pgd))\n\t\treturn -EFAULT;\n\tp4d = p4d_offset(pgd, address);\n\tif (p4d_none(*p4d))\n\t\treturn -EFAULT;\n\tpud = pud_offset(p4d, address);\n\tif (pud_none(*pud))\n\t\treturn -EFAULT;\n\tpmd = pmd_offset(pud, address);\n\tif (!pmd_present(*pmd))\n\t\treturn -EFAULT;\n\tpte = pte_offset_map(pmd, address);\n\tif (!pte)\n\t\treturn -EFAULT;\n\tentry = ptep_get(pte);\n\tif (pte_none(entry))\n\t\tgoto unmap;\n\t*vma = get_gate_vma(mm);\n\tif (!page)\n\t\tgoto out;\n\t*page = vm_normal_page(*vma, address, entry);\n\tif (!*page) {\n\t\tif ((gup_flags & FOLL_DUMP) || !is_zero_pfn(pte_pfn(entry)))\n\t\t\tgoto unmap;\n\t\t*page = pte_page(entry);\n\t}\n\tret = try_grab_page(*page, gup_flags);\n\tif (unlikely(ret))\n\t\tgoto unmap;\nout:\n\tret = 0;\nunmap:\n\tpte_unmap(pte);\n\treturn ret;\n}\n\n \nstatic int faultin_page(struct vm_area_struct *vma,\n\t\tunsigned long address, unsigned int *flags, bool unshare,\n\t\tint *locked)\n{\n\tunsigned int fault_flags = 0;\n\tvm_fault_t ret;\n\n\tif (*flags & FOLL_NOFAULT)\n\t\treturn -EFAULT;\n\tif (*flags & FOLL_WRITE)\n\t\tfault_flags |= FAULT_FLAG_WRITE;\n\tif (*flags & FOLL_REMOTE)\n\t\tfault_flags |= FAULT_FLAG_REMOTE;\n\tif (*flags & FOLL_UNLOCKABLE) {\n\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;\n\t\t \n\t\tif (*flags & FOLL_INTERRUPTIBLE)\n\t\t\tfault_flags |= FAULT_FLAG_INTERRUPTIBLE;\n\t}\n\tif (*flags & FOLL_NOWAIT)\n\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_RETRY_NOWAIT;\n\tif (*flags & FOLL_TRIED) {\n\t\t \n\t\tfault_flags |= FAULT_FLAG_TRIED;\n\t}\n\tif (unshare) {\n\t\tfault_flags |= FAULT_FLAG_UNSHARE;\n\t\t \n\t\tVM_BUG_ON(fault_flags & FAULT_FLAG_WRITE);\n\t}\n\n\tret = handle_mm_fault(vma, address, fault_flags, NULL);\n\n\tif (ret & VM_FAULT_COMPLETED) {\n\t\t \n\t\tWARN_ON_ONCE(fault_flags & FAULT_FLAG_RETRY_NOWAIT);\n\t\t*locked = 0;\n\n\t\t \n\t\treturn -EAGAIN;\n\t}\n\n\tif (ret & VM_FAULT_ERROR) {\n\t\tint err = vm_fault_to_errno(ret, *flags);\n\n\t\tif (err)\n\t\t\treturn err;\n\t\tBUG();\n\t}\n\n\tif (ret & VM_FAULT_RETRY) {\n\t\tif (!(fault_flags & FAULT_FLAG_RETRY_NOWAIT))\n\t\t\t*locked = 0;\n\t\treturn -EBUSY;\n\t}\n\n\treturn 0;\n}\n\n \nstatic bool writable_file_mapping_allowed(struct vm_area_struct *vma,\n\t\t\t\t\t  unsigned long gup_flags)\n{\n\t \n\tif ((gup_flags & (FOLL_PIN | FOLL_LONGTERM)) !=\n\t    (FOLL_PIN | FOLL_LONGTERM))\n\t\treturn true;\n\n\t \n\treturn !vma_needs_dirty_tracking(vma);\n}\n\nstatic int check_vma_flags(struct vm_area_struct *vma, unsigned long gup_flags)\n{\n\tvm_flags_t vm_flags = vma->vm_flags;\n\tint write = (gup_flags & FOLL_WRITE);\n\tint foreign = (gup_flags & FOLL_REMOTE);\n\tbool vma_anon = vma_is_anonymous(vma);\n\n\tif (vm_flags & (VM_IO | VM_PFNMAP))\n\t\treturn -EFAULT;\n\n\tif ((gup_flags & FOLL_ANON) && !vma_anon)\n\t\treturn -EFAULT;\n\n\tif ((gup_flags & FOLL_LONGTERM) && vma_is_fsdax(vma))\n\t\treturn -EOPNOTSUPP;\n\n\tif (vma_is_secretmem(vma))\n\t\treturn -EFAULT;\n\n\tif (write) {\n\t\tif (!vma_anon &&\n\t\t    !writable_file_mapping_allowed(vma, gup_flags))\n\t\t\treturn -EFAULT;\n\n\t\tif (!(vm_flags & VM_WRITE) || (vm_flags & VM_SHADOW_STACK)) {\n\t\t\tif (!(gup_flags & FOLL_FORCE))\n\t\t\t\treturn -EFAULT;\n\t\t\t \n\t\t\tif (is_vm_hugetlb_page(vma))\n\t\t\t\treturn -EFAULT;\n\t\t\t \n\t\t\tif (!is_cow_mapping(vm_flags))\n\t\t\t\treturn -EFAULT;\n\t\t}\n\t} else if (!(vm_flags & VM_READ)) {\n\t\tif (!(gup_flags & FOLL_FORCE))\n\t\t\treturn -EFAULT;\n\t\t \n\t\tif (!(vm_flags & VM_MAYREAD))\n\t\t\treturn -EFAULT;\n\t}\n\t \n\tif (!arch_vma_access_permitted(vma, write, false, foreign))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\n \nstatic struct vm_area_struct *gup_vma_lookup(struct mm_struct *mm,\n\t unsigned long addr)\n{\n#ifdef CONFIG_STACK_GROWSUP\n\treturn vma_lookup(mm, addr);\n#else\n\tstatic volatile unsigned long next_warn;\n\tstruct vm_area_struct *vma;\n\tunsigned long now, next;\n\n\tvma = find_vma(mm, addr);\n\tif (!vma || (addr >= vma->vm_start))\n\t\treturn vma;\n\n\t \n\tif (!(vma->vm_flags & VM_GROWSDOWN))\n\t\treturn NULL;\n\tif (vma->vm_start - addr > 65536)\n\t\treturn NULL;\n\n\t \n\tnow = jiffies; next = next_warn;\n\tif (next && time_before(now, next))\n\t\treturn NULL;\n\tnext_warn = now + 60*60*HZ;\n\n\t \n\tpr_warn(\"GUP no longer grows the stack in %s (%d): %lx-%lx (%lx)\\n\",\n\t\tcurrent->comm, task_pid_nr(current),\n\t\tvma->vm_start, vma->vm_end, addr);\n\tdump_stack();\n\treturn NULL;\n#endif\n}\n\n \nstatic long __get_user_pages(struct mm_struct *mm,\n\t\tunsigned long start, unsigned long nr_pages,\n\t\tunsigned int gup_flags, struct page **pages,\n\t\tint *locked)\n{\n\tlong ret = 0, i = 0;\n\tstruct vm_area_struct *vma = NULL;\n\tstruct follow_page_context ctx = { NULL };\n\n\tif (!nr_pages)\n\t\treturn 0;\n\n\tstart = untagged_addr_remote(mm, start);\n\n\tVM_BUG_ON(!!pages != !!(gup_flags & (FOLL_GET | FOLL_PIN)));\n\n\tdo {\n\t\tstruct page *page;\n\t\tunsigned int foll_flags = gup_flags;\n\t\tunsigned int page_increm;\n\n\t\t \n\t\tif (!vma || start >= vma->vm_end) {\n\t\t\tvma = gup_vma_lookup(mm, start);\n\t\t\tif (!vma && in_gate_area(mm, start)) {\n\t\t\t\tret = get_gate_page(mm, start & PAGE_MASK,\n\t\t\t\t\t\tgup_flags, &vma,\n\t\t\t\t\t\tpages ? &page : NULL);\n\t\t\t\tif (ret)\n\t\t\t\t\tgoto out;\n\t\t\t\tctx.page_mask = 0;\n\t\t\t\tgoto next_page;\n\t\t\t}\n\n\t\t\tif (!vma) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tret = check_vma_flags(vma, gup_flags);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t}\nretry:\n\t\t \n\t\tif (fatal_signal_pending(current)) {\n\t\t\tret = -EINTR;\n\t\t\tgoto out;\n\t\t}\n\t\tcond_resched();\n\n\t\tpage = follow_page_mask(vma, start, foll_flags, &ctx);\n\t\tif (!page || PTR_ERR(page) == -EMLINK) {\n\t\t\tret = faultin_page(vma, start, &foll_flags,\n\t\t\t\t\t   PTR_ERR(page) == -EMLINK, locked);\n\t\t\tswitch (ret) {\n\t\t\tcase 0:\n\t\t\t\tgoto retry;\n\t\t\tcase -EBUSY:\n\t\t\tcase -EAGAIN:\n\t\t\t\tret = 0;\n\t\t\t\tfallthrough;\n\t\t\tcase -EFAULT:\n\t\t\tcase -ENOMEM:\n\t\t\tcase -EHWPOISON:\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tBUG();\n\t\t} else if (PTR_ERR(page) == -EEXIST) {\n\t\t\t \n\t\t\tif (pages) {\n\t\t\t\tret = PTR_ERR(page);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t} else if (IS_ERR(page)) {\n\t\t\tret = PTR_ERR(page);\n\t\t\tgoto out;\n\t\t}\nnext_page:\n\t\tpage_increm = 1 + (~(start >> PAGE_SHIFT) & ctx.page_mask);\n\t\tif (page_increm > nr_pages)\n\t\t\tpage_increm = nr_pages;\n\n\t\tif (pages) {\n\t\t\tstruct page *subpage;\n\t\t\tunsigned int j;\n\n\t\t\t \n\t\t\tif (page_increm > 1) {\n\t\t\t\tstruct folio *folio;\n\n\t\t\t\t \n\t\t\t\tfolio = try_grab_folio(page, page_increm - 1,\n\t\t\t\t\t\t       foll_flags);\n\t\t\t\tif (WARN_ON_ONCE(!folio)) {\n\t\t\t\t\t \n\t\t\t\t\tgup_put_folio(page_folio(page), 1,\n\t\t\t\t\t\t      foll_flags);\n\t\t\t\t\tret = -EFAULT;\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tfor (j = 0; j < page_increm; j++) {\n\t\t\t\tsubpage = nth_page(page, j);\n\t\t\t\tpages[i + j] = subpage;\n\t\t\t\tflush_anon_page(vma, subpage, start + j * PAGE_SIZE);\n\t\t\t\tflush_dcache_page(subpage);\n\t\t\t}\n\t\t}\n\n\t\ti += page_increm;\n\t\tstart += page_increm * PAGE_SIZE;\n\t\tnr_pages -= page_increm;\n\t} while (nr_pages);\nout:\n\tif (ctx.pgmap)\n\t\tput_dev_pagemap(ctx.pgmap);\n\treturn i ? i : ret;\n}\n\nstatic bool vma_permits_fault(struct vm_area_struct *vma,\n\t\t\t      unsigned int fault_flags)\n{\n\tbool write   = !!(fault_flags & FAULT_FLAG_WRITE);\n\tbool foreign = !!(fault_flags & FAULT_FLAG_REMOTE);\n\tvm_flags_t vm_flags = write ? VM_WRITE : VM_READ;\n\n\tif (!(vm_flags & vma->vm_flags))\n\t\treturn false;\n\n\t \n\tif (!arch_vma_access_permitted(vma, write, false, foreign))\n\t\treturn false;\n\n\treturn true;\n}\n\n \nint fixup_user_fault(struct mm_struct *mm,\n\t\t     unsigned long address, unsigned int fault_flags,\n\t\t     bool *unlocked)\n{\n\tstruct vm_area_struct *vma;\n\tvm_fault_t ret;\n\n\taddress = untagged_addr_remote(mm, address);\n\n\tif (unlocked)\n\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;\n\nretry:\n\tvma = gup_vma_lookup(mm, address);\n\tif (!vma)\n\t\treturn -EFAULT;\n\n\tif (!vma_permits_fault(vma, fault_flags))\n\t\treturn -EFAULT;\n\n\tif ((fault_flags & FAULT_FLAG_KILLABLE) &&\n\t    fatal_signal_pending(current))\n\t\treturn -EINTR;\n\n\tret = handle_mm_fault(vma, address, fault_flags, NULL);\n\n\tif (ret & VM_FAULT_COMPLETED) {\n\t\t \n\t\tmmap_read_lock(mm);\n\t\t*unlocked = true;\n\t\treturn 0;\n\t}\n\n\tif (ret & VM_FAULT_ERROR) {\n\t\tint err = vm_fault_to_errno(ret, 0);\n\n\t\tif (err)\n\t\t\treturn err;\n\t\tBUG();\n\t}\n\n\tif (ret & VM_FAULT_RETRY) {\n\t\tmmap_read_lock(mm);\n\t\t*unlocked = true;\n\t\tfault_flags |= FAULT_FLAG_TRIED;\n\t\tgoto retry;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(fixup_user_fault);\n\n \nstatic bool gup_signal_pending(unsigned int flags)\n{\n\tif (fatal_signal_pending(current))\n\t\treturn true;\n\n\tif (!(flags & FOLL_INTERRUPTIBLE))\n\t\treturn false;\n\n\treturn signal_pending(current);\n}\n\n \nstatic __always_inline long __get_user_pages_locked(struct mm_struct *mm,\n\t\t\t\t\t\tunsigned long start,\n\t\t\t\t\t\tunsigned long nr_pages,\n\t\t\t\t\t\tstruct page **pages,\n\t\t\t\t\t\tint *locked,\n\t\t\t\t\t\tunsigned int flags)\n{\n\tlong ret, pages_done;\n\tbool must_unlock = false;\n\n\t \n\tif (!*locked) {\n\t\tif (mmap_read_lock_killable(mm))\n\t\t\treturn -EAGAIN;\n\t\tmust_unlock = true;\n\t\t*locked = 1;\n\t}\n\telse\n\t\tmmap_assert_locked(mm);\n\n\tif (flags & FOLL_PIN)\n\t\tmm_set_has_pinned_flag(&mm->flags);\n\n\t \n\tif (pages && !(flags & FOLL_PIN))\n\t\tflags |= FOLL_GET;\n\n\tpages_done = 0;\n\tfor (;;) {\n\t\tret = __get_user_pages(mm, start, nr_pages, flags, pages,\n\t\t\t\t       locked);\n\t\tif (!(flags & FOLL_UNLOCKABLE)) {\n\t\t\t \n\t\t\tpages_done = ret;\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (!*locked) {\n\t\t\tBUG_ON(ret < 0);\n\t\t\tBUG_ON(ret >= nr_pages);\n\t\t}\n\n\t\tif (ret > 0) {\n\t\t\tnr_pages -= ret;\n\t\t\tpages_done += ret;\n\t\t\tif (!nr_pages)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (*locked) {\n\t\t\t \n\t\t\tif (!pages_done)\n\t\t\t\tpages_done = ret;\n\t\t\tbreak;\n\t\t}\n\t\t \n\t\tif (likely(pages))\n\t\t\tpages += ret;\n\t\tstart += ret << PAGE_SHIFT;\n\n\t\t \n\t\tmust_unlock = true;\n\nretry:\n\t\t \n\t\tif (gup_signal_pending(flags)) {\n\t\t\tif (!pages_done)\n\t\t\t\tpages_done = -EINTR;\n\t\t\tbreak;\n\t\t}\n\n\t\tret = mmap_read_lock_killable(mm);\n\t\tif (ret) {\n\t\t\tBUG_ON(ret > 0);\n\t\t\tif (!pages_done)\n\t\t\t\tpages_done = ret;\n\t\t\tbreak;\n\t\t}\n\n\t\t*locked = 1;\n\t\tret = __get_user_pages(mm, start, 1, flags | FOLL_TRIED,\n\t\t\t\t       pages, locked);\n\t\tif (!*locked) {\n\t\t\t \n\t\t\tBUG_ON(ret != 0);\n\t\t\tgoto retry;\n\t\t}\n\t\tif (ret != 1) {\n\t\t\tBUG_ON(ret > 1);\n\t\t\tif (!pages_done)\n\t\t\t\tpages_done = ret;\n\t\t\tbreak;\n\t\t}\n\t\tnr_pages--;\n\t\tpages_done++;\n\t\tif (!nr_pages)\n\t\t\tbreak;\n\t\tif (likely(pages))\n\t\t\tpages++;\n\t\tstart += PAGE_SIZE;\n\t}\n\tif (must_unlock && *locked) {\n\t\t \n\t\tmmap_read_unlock(mm);\n\t\t*locked = 0;\n\t}\n\treturn pages_done;\n}\n\n \nlong populate_vma_page_range(struct vm_area_struct *vma,\n\t\tunsigned long start, unsigned long end, int *locked)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long nr_pages = (end - start) / PAGE_SIZE;\n\tint local_locked = 1;\n\tint gup_flags;\n\tlong ret;\n\n\tVM_BUG_ON(!PAGE_ALIGNED(start));\n\tVM_BUG_ON(!PAGE_ALIGNED(end));\n\tVM_BUG_ON_VMA(start < vma->vm_start, vma);\n\tVM_BUG_ON_VMA(end   > vma->vm_end, vma);\n\tmmap_assert_locked(mm);\n\n\t \n\tif (vma->vm_flags & VM_LOCKONFAULT)\n\t\treturn nr_pages;\n\n\tgup_flags = FOLL_TOUCH;\n\t \n\tif ((vma->vm_flags & (VM_WRITE | VM_SHARED)) == VM_WRITE)\n\t\tgup_flags |= FOLL_WRITE;\n\n\t \n\tif (vma_is_accessible(vma))\n\t\tgup_flags |= FOLL_FORCE;\n\n\tif (locked)\n\t\tgup_flags |= FOLL_UNLOCKABLE;\n\n\t \n\tret = __get_user_pages(mm, start, nr_pages, gup_flags,\n\t\t\t       NULL, locked ? locked : &local_locked);\n\tlru_add_drain();\n\treturn ret;\n}\n\n \nlong faultin_vma_page_range(struct vm_area_struct *vma, unsigned long start,\n\t\t\t    unsigned long end, bool write, int *locked)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long nr_pages = (end - start) / PAGE_SIZE;\n\tint gup_flags;\n\tlong ret;\n\n\tVM_BUG_ON(!PAGE_ALIGNED(start));\n\tVM_BUG_ON(!PAGE_ALIGNED(end));\n\tVM_BUG_ON_VMA(start < vma->vm_start, vma);\n\tVM_BUG_ON_VMA(end > vma->vm_end, vma);\n\tmmap_assert_locked(mm);\n\n\t \n\tgup_flags = FOLL_TOUCH | FOLL_HWPOISON | FOLL_UNLOCKABLE;\n\tif (write)\n\t\tgup_flags |= FOLL_WRITE;\n\n\t \n\tif (check_vma_flags(vma, gup_flags))\n\t\treturn -EINVAL;\n\n\tret = __get_user_pages(mm, start, nr_pages, gup_flags,\n\t\t\t       NULL, locked);\n\tlru_add_drain();\n\treturn ret;\n}\n\n \nint __mm_populate(unsigned long start, unsigned long len, int ignore_errors)\n{\n\tstruct mm_struct *mm = current->mm;\n\tunsigned long end, nstart, nend;\n\tstruct vm_area_struct *vma = NULL;\n\tint locked = 0;\n\tlong ret = 0;\n\n\tend = start + len;\n\n\tfor (nstart = start; nstart < end; nstart = nend) {\n\t\t \n\t\tif (!locked) {\n\t\t\tlocked = 1;\n\t\t\tmmap_read_lock(mm);\n\t\t\tvma = find_vma_intersection(mm, nstart, end);\n\t\t} else if (nstart >= vma->vm_end)\n\t\t\tvma = find_vma_intersection(mm, vma->vm_end, end);\n\n\t\tif (!vma)\n\t\t\tbreak;\n\t\t \n\t\tnend = min(end, vma->vm_end);\n\t\tif (vma->vm_flags & (VM_IO | VM_PFNMAP))\n\t\t\tcontinue;\n\t\tif (nstart < vma->vm_start)\n\t\t\tnstart = vma->vm_start;\n\t\t \n\t\tret = populate_vma_page_range(vma, nstart, nend, &locked);\n\t\tif (ret < 0) {\n\t\t\tif (ignore_errors) {\n\t\t\t\tret = 0;\n\t\t\t\tcontinue;\t \n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\tnend = nstart + ret * PAGE_SIZE;\n\t\tret = 0;\n\t}\n\tif (locked)\n\t\tmmap_read_unlock(mm);\n\treturn ret;\t \n}\n#else  \nstatic long __get_user_pages_locked(struct mm_struct *mm, unsigned long start,\n\t\tunsigned long nr_pages, struct page **pages,\n\t\tint *locked, unsigned int foll_flags)\n{\n\tstruct vm_area_struct *vma;\n\tbool must_unlock = false;\n\tunsigned long vm_flags;\n\tlong i;\n\n\tif (!nr_pages)\n\t\treturn 0;\n\n\t \n\tif (!*locked) {\n\t\tif (mmap_read_lock_killable(mm))\n\t\t\treturn -EAGAIN;\n\t\tmust_unlock = true;\n\t\t*locked = 1;\n\t}\n\n\t \n\tvm_flags  = (foll_flags & FOLL_WRITE) ?\n\t\t\t(VM_WRITE | VM_MAYWRITE) : (VM_READ | VM_MAYREAD);\n\tvm_flags &= (foll_flags & FOLL_FORCE) ?\n\t\t\t(VM_MAYREAD | VM_MAYWRITE) : (VM_READ | VM_WRITE);\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tvma = find_vma(mm, start);\n\t\tif (!vma)\n\t\t\tbreak;\n\n\t\t \n\t\tif ((vma->vm_flags & (VM_IO | VM_PFNMAP)) ||\n\t\t    !(vm_flags & vma->vm_flags))\n\t\t\tbreak;\n\n\t\tif (pages) {\n\t\t\tpages[i] = virt_to_page((void *)start);\n\t\t\tif (pages[i])\n\t\t\t\tget_page(pages[i]);\n\t\t}\n\n\t\tstart = (start + PAGE_SIZE) & PAGE_MASK;\n\t}\n\n\tif (must_unlock && *locked) {\n\t\tmmap_read_unlock(mm);\n\t\t*locked = 0;\n\t}\n\n\treturn i ? : -EFAULT;\n}\n#endif  \n\n \nsize_t fault_in_writeable(char __user *uaddr, size_t size)\n{\n\tchar __user *start = uaddr, *end;\n\n\tif (unlikely(size == 0))\n\t\treturn 0;\n\tif (!user_write_access_begin(uaddr, size))\n\t\treturn size;\n\tif (!PAGE_ALIGNED(uaddr)) {\n\t\tunsafe_put_user(0, uaddr, out);\n\t\tuaddr = (char __user *)PAGE_ALIGN((unsigned long)uaddr);\n\t}\n\tend = (char __user *)PAGE_ALIGN((unsigned long)start + size);\n\tif (unlikely(end < start))\n\t\tend = NULL;\n\twhile (uaddr != end) {\n\t\tunsafe_put_user(0, uaddr, out);\n\t\tuaddr += PAGE_SIZE;\n\t}\n\nout:\n\tuser_write_access_end();\n\tif (size > uaddr - start)\n\t\treturn size - (uaddr - start);\n\treturn 0;\n}\nEXPORT_SYMBOL(fault_in_writeable);\n\n \nsize_t fault_in_subpage_writeable(char __user *uaddr, size_t size)\n{\n\tsize_t faulted_in;\n\n\t \n\tfaulted_in = size - fault_in_writeable(uaddr, size);\n\tif (faulted_in)\n\t\tfaulted_in -= probe_subpage_writeable(uaddr, faulted_in);\n\n\treturn size - faulted_in;\n}\nEXPORT_SYMBOL(fault_in_subpage_writeable);\n\n \nsize_t fault_in_safe_writeable(const char __user *uaddr, size_t size)\n{\n\tunsigned long start = (unsigned long)uaddr, end;\n\tstruct mm_struct *mm = current->mm;\n\tbool unlocked = false;\n\n\tif (unlikely(size == 0))\n\t\treturn 0;\n\tend = PAGE_ALIGN(start + size);\n\tif (end < start)\n\t\tend = 0;\n\n\tmmap_read_lock(mm);\n\tdo {\n\t\tif (fixup_user_fault(mm, start, FAULT_FLAG_WRITE, &unlocked))\n\t\t\tbreak;\n\t\tstart = (start + PAGE_SIZE) & PAGE_MASK;\n\t} while (start != end);\n\tmmap_read_unlock(mm);\n\n\tif (size > (unsigned long)uaddr - start)\n\t\treturn size - ((unsigned long)uaddr - start);\n\treturn 0;\n}\nEXPORT_SYMBOL(fault_in_safe_writeable);\n\n \nsize_t fault_in_readable(const char __user *uaddr, size_t size)\n{\n\tconst char __user *start = uaddr, *end;\n\tvolatile char c;\n\n\tif (unlikely(size == 0))\n\t\treturn 0;\n\tif (!user_read_access_begin(uaddr, size))\n\t\treturn size;\n\tif (!PAGE_ALIGNED(uaddr)) {\n\t\tunsafe_get_user(c, uaddr, out);\n\t\tuaddr = (const char __user *)PAGE_ALIGN((unsigned long)uaddr);\n\t}\n\tend = (const char __user *)PAGE_ALIGN((unsigned long)start + size);\n\tif (unlikely(end < start))\n\t\tend = NULL;\n\twhile (uaddr != end) {\n\t\tunsafe_get_user(c, uaddr, out);\n\t\tuaddr += PAGE_SIZE;\n\t}\n\nout:\n\tuser_read_access_end();\n\t(void)c;\n\tif (size > uaddr - start)\n\t\treturn size - (uaddr - start);\n\treturn 0;\n}\nEXPORT_SYMBOL(fault_in_readable);\n\n \n#ifdef CONFIG_ELF_CORE\nstruct page *get_dump_page(unsigned long addr)\n{\n\tstruct page *page;\n\tint locked = 0;\n\tint ret;\n\n\tret = __get_user_pages_locked(current->mm, addr, 1, &page, &locked,\n\t\t\t\t      FOLL_FORCE | FOLL_DUMP | FOLL_GET);\n\treturn (ret == 1) ? page : NULL;\n}\n#endif  \n\n#ifdef CONFIG_MIGRATION\n \nstatic unsigned long collect_longterm_unpinnable_pages(\n\t\t\t\t\tstruct list_head *movable_page_list,\n\t\t\t\t\tunsigned long nr_pages,\n\t\t\t\t\tstruct page **pages)\n{\n\tunsigned long i, collected = 0;\n\tstruct folio *prev_folio = NULL;\n\tbool drain_allow = true;\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tstruct folio *folio = page_folio(pages[i]);\n\n\t\tif (folio == prev_folio)\n\t\t\tcontinue;\n\t\tprev_folio = folio;\n\n\t\tif (folio_is_longterm_pinnable(folio))\n\t\t\tcontinue;\n\n\t\tcollected++;\n\n\t\tif (folio_is_device_coherent(folio))\n\t\t\tcontinue;\n\n\t\tif (folio_test_hugetlb(folio)) {\n\t\t\tisolate_hugetlb(folio, movable_page_list);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!folio_test_lru(folio) && drain_allow) {\n\t\t\tlru_add_drain_all();\n\t\t\tdrain_allow = false;\n\t\t}\n\n\t\tif (!folio_isolate_lru(folio))\n\t\t\tcontinue;\n\n\t\tlist_add_tail(&folio->lru, movable_page_list);\n\t\tnode_stat_mod_folio(folio,\n\t\t\t\t    NR_ISOLATED_ANON + folio_is_file_lru(folio),\n\t\t\t\t    folio_nr_pages(folio));\n\t}\n\n\treturn collected;\n}\n\n \nstatic int migrate_longterm_unpinnable_pages(\n\t\t\t\t\tstruct list_head *movable_page_list,\n\t\t\t\t\tunsigned long nr_pages,\n\t\t\t\t\tstruct page **pages)\n{\n\tint ret;\n\tunsigned long i;\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tstruct folio *folio = page_folio(pages[i]);\n\n\t\tif (folio_is_device_coherent(folio)) {\n\t\t\t \n\t\t\tpages[i] = NULL;\n\t\t\tfolio_get(folio);\n\t\t\tgup_put_folio(folio, 1, FOLL_PIN);\n\n\t\t\tif (migrate_device_coherent_page(&folio->page)) {\n\t\t\t\tret = -EBUSY;\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tunpin_user_page(pages[i]);\n\t\tpages[i] = NULL;\n\t}\n\n\tif (!list_empty(movable_page_list)) {\n\t\tstruct migration_target_control mtc = {\n\t\t\t.nid = NUMA_NO_NODE,\n\t\t\t.gfp_mask = GFP_USER | __GFP_NOWARN,\n\t\t};\n\n\t\tif (migrate_pages(movable_page_list, alloc_migration_target,\n\t\t\t\t  NULL, (unsigned long)&mtc, MIGRATE_SYNC,\n\t\t\t\t  MR_LONGTERM_PIN, NULL)) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err;\n\t\t}\n\t}\n\n\tputback_movable_pages(movable_page_list);\n\n\treturn -EAGAIN;\n\nerr:\n\tfor (i = 0; i < nr_pages; i++)\n\t\tif (pages[i])\n\t\t\tunpin_user_page(pages[i]);\n\tputback_movable_pages(movable_page_list);\n\n\treturn ret;\n}\n\n \nstatic long check_and_migrate_movable_pages(unsigned long nr_pages,\n\t\t\t\t\t    struct page **pages)\n{\n\tunsigned long collected;\n\tLIST_HEAD(movable_page_list);\n\n\tcollected = collect_longterm_unpinnable_pages(&movable_page_list,\n\t\t\t\t\t\tnr_pages, pages);\n\tif (!collected)\n\t\treturn 0;\n\n\treturn migrate_longterm_unpinnable_pages(&movable_page_list, nr_pages,\n\t\t\t\t\t\tpages);\n}\n#else\nstatic long check_and_migrate_movable_pages(unsigned long nr_pages,\n\t\t\t\t\t    struct page **pages)\n{\n\treturn 0;\n}\n#endif  \n\n \nstatic long __gup_longterm_locked(struct mm_struct *mm,\n\t\t\t\t  unsigned long start,\n\t\t\t\t  unsigned long nr_pages,\n\t\t\t\t  struct page **pages,\n\t\t\t\t  int *locked,\n\t\t\t\t  unsigned int gup_flags)\n{\n\tunsigned int flags;\n\tlong rc, nr_pinned_pages;\n\n\tif (!(gup_flags & FOLL_LONGTERM))\n\t\treturn __get_user_pages_locked(mm, start, nr_pages, pages,\n\t\t\t\t\t       locked, gup_flags);\n\n\tflags = memalloc_pin_save();\n\tdo {\n\t\tnr_pinned_pages = __get_user_pages_locked(mm, start, nr_pages,\n\t\t\t\t\t\t\t  pages, locked,\n\t\t\t\t\t\t\t  gup_flags);\n\t\tif (nr_pinned_pages <= 0) {\n\t\t\trc = nr_pinned_pages;\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\trc = check_and_migrate_movable_pages(nr_pinned_pages, pages);\n\t} while (rc == -EAGAIN);\n\tmemalloc_pin_restore(flags);\n\treturn rc ? rc : nr_pinned_pages;\n}\n\n \nstatic bool is_valid_gup_args(struct page **pages, int *locked,\n\t\t\t      unsigned int *gup_flags_p, unsigned int to_set)\n{\n\tunsigned int gup_flags = *gup_flags_p;\n\n\t \n\tif (WARN_ON_ONCE(gup_flags & (FOLL_PIN | FOLL_TRIED | FOLL_UNLOCKABLE |\n\t\t\t\t      FOLL_REMOTE | FOLL_FAST_ONLY)))\n\t\treturn false;\n\n\tgup_flags |= to_set;\n\tif (locked) {\n\t\t \n\t\tif (WARN_ON_ONCE(*locked != 1))\n\t\t\treturn false;\n\n\t\tgup_flags |= FOLL_UNLOCKABLE;\n\t}\n\n\t \n\tif (WARN_ON_ONCE((gup_flags & (FOLL_PIN | FOLL_GET)) ==\n\t\t\t (FOLL_PIN | FOLL_GET)))\n\t\treturn false;\n\n\t \n\tif (WARN_ON_ONCE(!(gup_flags & FOLL_PIN) && (gup_flags & FOLL_LONGTERM)))\n\t\treturn false;\n\n\t \n\tif (WARN_ON_ONCE((gup_flags & (FOLL_GET | FOLL_PIN)) && !pages))\n\t\treturn false;\n\n\t \n\tif (WARN_ON_ONCE((gup_flags & FOLL_LONGTERM) &&\n\t\t\t (gup_flags & FOLL_PCI_P2PDMA)))\n\t\treturn false;\n\n\t*gup_flags_p = gup_flags;\n\treturn true;\n}\n\n#ifdef CONFIG_MMU\n \nlong get_user_pages_remote(struct mm_struct *mm,\n\t\tunsigned long start, unsigned long nr_pages,\n\t\tunsigned int gup_flags, struct page **pages,\n\t\tint *locked)\n{\n\tint local_locked = 1;\n\n\tif (!is_valid_gup_args(pages, locked, &gup_flags,\n\t\t\t       FOLL_TOUCH | FOLL_REMOTE))\n\t\treturn -EINVAL;\n\n\treturn __get_user_pages_locked(mm, start, nr_pages, pages,\n\t\t\t\t       locked ? locked : &local_locked,\n\t\t\t\t       gup_flags);\n}\nEXPORT_SYMBOL(get_user_pages_remote);\n\n#else  \nlong get_user_pages_remote(struct mm_struct *mm,\n\t\t\t   unsigned long start, unsigned long nr_pages,\n\t\t\t   unsigned int gup_flags, struct page **pages,\n\t\t\t   int *locked)\n{\n\treturn 0;\n}\n#endif  \n\n \nlong get_user_pages(unsigned long start, unsigned long nr_pages,\n\t\t    unsigned int gup_flags, struct page **pages)\n{\n\tint locked = 1;\n\n\tif (!is_valid_gup_args(pages, NULL, &gup_flags, FOLL_TOUCH))\n\t\treturn -EINVAL;\n\n\treturn __get_user_pages_locked(current->mm, start, nr_pages, pages,\n\t\t\t\t       &locked, gup_flags);\n}\nEXPORT_SYMBOL(get_user_pages);\n\n \nlong get_user_pages_unlocked(unsigned long start, unsigned long nr_pages,\n\t\t\t     struct page **pages, unsigned int gup_flags)\n{\n\tint locked = 0;\n\n\tif (!is_valid_gup_args(pages, NULL, &gup_flags,\n\t\t\t       FOLL_TOUCH | FOLL_UNLOCKABLE))\n\t\treturn -EINVAL;\n\n\treturn __get_user_pages_locked(current->mm, start, nr_pages, pages,\n\t\t\t\t       &locked, gup_flags);\n}\nEXPORT_SYMBOL(get_user_pages_unlocked);\n\n \n#ifdef CONFIG_HAVE_FAST_GUP\n\n \nstatic bool folio_fast_pin_allowed(struct folio *folio, unsigned int flags)\n{\n\tstruct address_space *mapping;\n\tunsigned long mapping_flags;\n\n\t \n\tif ((flags & (FOLL_PIN | FOLL_LONGTERM | FOLL_WRITE)) !=\n\t    (FOLL_PIN | FOLL_LONGTERM | FOLL_WRITE))\n\t\treturn true;\n\n\t \n\n\tif (WARN_ON_ONCE(folio_test_slab(folio)))\n\t\treturn false;\n\n\t \n\tif (folio_test_hugetlb(folio))\n\t\treturn true;\n\n\t \n\tlockdep_assert_irqs_disabled();\n\n\t \n\tmapping = READ_ONCE(folio->mapping);\n\n\t \n\tif (!mapping)\n\t\treturn false;\n\n\t \n\tmapping_flags = (unsigned long)mapping & PAGE_MAPPING_FLAGS;\n\tif (mapping_flags)\n\t\treturn mapping_flags & PAGE_MAPPING_ANON;\n\n\t \n\treturn shmem_mapping(mapping);\n}\n\nstatic void __maybe_unused undo_dev_pagemap(int *nr, int nr_start,\n\t\t\t\t\t    unsigned int flags,\n\t\t\t\t\t    struct page **pages)\n{\n\twhile ((*nr) - nr_start) {\n\t\tstruct page *page = pages[--(*nr)];\n\n\t\tClearPageReferenced(page);\n\t\tif (flags & FOLL_PIN)\n\t\t\tunpin_user_page(page);\n\t\telse\n\t\t\tput_page(page);\n\t}\n}\n\n#ifdef CONFIG_ARCH_HAS_PTE_SPECIAL\n \nstatic int gup_pte_range(pmd_t pmd, pmd_t *pmdp, unsigned long addr,\n\t\t\t unsigned long end, unsigned int flags,\n\t\t\t struct page **pages, int *nr)\n{\n\tstruct dev_pagemap *pgmap = NULL;\n\tint nr_start = *nr, ret = 0;\n\tpte_t *ptep, *ptem;\n\n\tptem = ptep = pte_offset_map(&pmd, addr);\n\tif (!ptep)\n\t\treturn 0;\n\tdo {\n\t\tpte_t pte = ptep_get_lockless(ptep);\n\t\tstruct page *page;\n\t\tstruct folio *folio;\n\n\t\t \n\t\tif (pte_protnone(pte))\n\t\t\tgoto pte_unmap;\n\n\t\tif (!pte_access_permitted(pte, flags & FOLL_WRITE))\n\t\t\tgoto pte_unmap;\n\n\t\tif (pte_devmap(pte)) {\n\t\t\tif (unlikely(flags & FOLL_LONGTERM))\n\t\t\t\tgoto pte_unmap;\n\n\t\t\tpgmap = get_dev_pagemap(pte_pfn(pte), pgmap);\n\t\t\tif (unlikely(!pgmap)) {\n\t\t\t\tundo_dev_pagemap(nr, nr_start, flags, pages);\n\t\t\t\tgoto pte_unmap;\n\t\t\t}\n\t\t} else if (pte_special(pte))\n\t\t\tgoto pte_unmap;\n\n\t\tVM_BUG_ON(!pfn_valid(pte_pfn(pte)));\n\t\tpage = pte_page(pte);\n\n\t\tfolio = try_grab_folio(page, 1, flags);\n\t\tif (!folio)\n\t\t\tgoto pte_unmap;\n\n\t\tif (unlikely(folio_is_secretmem(folio))) {\n\t\t\tgup_put_folio(folio, 1, flags);\n\t\t\tgoto pte_unmap;\n\t\t}\n\n\t\tif (unlikely(pmd_val(pmd) != pmd_val(*pmdp)) ||\n\t\t    unlikely(pte_val(pte) != pte_val(ptep_get(ptep)))) {\n\t\t\tgup_put_folio(folio, 1, flags);\n\t\t\tgoto pte_unmap;\n\t\t}\n\n\t\tif (!folio_fast_pin_allowed(folio, flags)) {\n\t\t\tgup_put_folio(folio, 1, flags);\n\t\t\tgoto pte_unmap;\n\t\t}\n\n\t\tif (!pte_write(pte) && gup_must_unshare(NULL, flags, page)) {\n\t\t\tgup_put_folio(folio, 1, flags);\n\t\t\tgoto pte_unmap;\n\t\t}\n\n\t\t \n\t\tif (flags & FOLL_PIN) {\n\t\t\tret = arch_make_page_accessible(page);\n\t\t\tif (ret) {\n\t\t\t\tgup_put_folio(folio, 1, flags);\n\t\t\t\tgoto pte_unmap;\n\t\t\t}\n\t\t}\n\t\tfolio_set_referenced(folio);\n\t\tpages[*nr] = page;\n\t\t(*nr)++;\n\t} while (ptep++, addr += PAGE_SIZE, addr != end);\n\n\tret = 1;\n\npte_unmap:\n\tif (pgmap)\n\t\tput_dev_pagemap(pgmap);\n\tpte_unmap(ptem);\n\treturn ret;\n}\n#else\n\n \nstatic int gup_pte_range(pmd_t pmd, pmd_t *pmdp, unsigned long addr,\n\t\t\t unsigned long end, unsigned int flags,\n\t\t\t struct page **pages, int *nr)\n{\n\treturn 0;\n}\n#endif  \n\n#if defined(CONFIG_ARCH_HAS_PTE_DEVMAP) && defined(CONFIG_TRANSPARENT_HUGEPAGE)\nstatic int __gup_device_huge(unsigned long pfn, unsigned long addr,\n\t\t\t     unsigned long end, unsigned int flags,\n\t\t\t     struct page **pages, int *nr)\n{\n\tint nr_start = *nr;\n\tstruct dev_pagemap *pgmap = NULL;\n\n\tdo {\n\t\tstruct page *page = pfn_to_page(pfn);\n\n\t\tpgmap = get_dev_pagemap(pfn, pgmap);\n\t\tif (unlikely(!pgmap)) {\n\t\t\tundo_dev_pagemap(nr, nr_start, flags, pages);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!(flags & FOLL_PCI_P2PDMA) && is_pci_p2pdma_page(page)) {\n\t\t\tundo_dev_pagemap(nr, nr_start, flags, pages);\n\t\t\tbreak;\n\t\t}\n\n\t\tSetPageReferenced(page);\n\t\tpages[*nr] = page;\n\t\tif (unlikely(try_grab_page(page, flags))) {\n\t\t\tundo_dev_pagemap(nr, nr_start, flags, pages);\n\t\t\tbreak;\n\t\t}\n\t\t(*nr)++;\n\t\tpfn++;\n\t} while (addr += PAGE_SIZE, addr != end);\n\n\tput_dev_pagemap(pgmap);\n\treturn addr == end;\n}\n\nstatic int __gup_device_huge_pmd(pmd_t orig, pmd_t *pmdp, unsigned long addr,\n\t\t\t\t unsigned long end, unsigned int flags,\n\t\t\t\t struct page **pages, int *nr)\n{\n\tunsigned long fault_pfn;\n\tint nr_start = *nr;\n\n\tfault_pfn = pmd_pfn(orig) + ((addr & ~PMD_MASK) >> PAGE_SHIFT);\n\tif (!__gup_device_huge(fault_pfn, addr, end, flags, pages, nr))\n\t\treturn 0;\n\n\tif (unlikely(pmd_val(orig) != pmd_val(*pmdp))) {\n\t\tundo_dev_pagemap(nr, nr_start, flags, pages);\n\t\treturn 0;\n\t}\n\treturn 1;\n}\n\nstatic int __gup_device_huge_pud(pud_t orig, pud_t *pudp, unsigned long addr,\n\t\t\t\t unsigned long end, unsigned int flags,\n\t\t\t\t struct page **pages, int *nr)\n{\n\tunsigned long fault_pfn;\n\tint nr_start = *nr;\n\n\tfault_pfn = pud_pfn(orig) + ((addr & ~PUD_MASK) >> PAGE_SHIFT);\n\tif (!__gup_device_huge(fault_pfn, addr, end, flags, pages, nr))\n\t\treturn 0;\n\n\tif (unlikely(pud_val(orig) != pud_val(*pudp))) {\n\t\tundo_dev_pagemap(nr, nr_start, flags, pages);\n\t\treturn 0;\n\t}\n\treturn 1;\n}\n#else\nstatic int __gup_device_huge_pmd(pmd_t orig, pmd_t *pmdp, unsigned long addr,\n\t\t\t\t unsigned long end, unsigned int flags,\n\t\t\t\t struct page **pages, int *nr)\n{\n\tBUILD_BUG();\n\treturn 0;\n}\n\nstatic int __gup_device_huge_pud(pud_t pud, pud_t *pudp, unsigned long addr,\n\t\t\t\t unsigned long end, unsigned int flags,\n\t\t\t\t struct page **pages, int *nr)\n{\n\tBUILD_BUG();\n\treturn 0;\n}\n#endif\n\nstatic int record_subpages(struct page *page, unsigned long addr,\n\t\t\t   unsigned long end, struct page **pages)\n{\n\tint nr;\n\n\tfor (nr = 0; addr != end; nr++, addr += PAGE_SIZE)\n\t\tpages[nr] = nth_page(page, nr);\n\n\treturn nr;\n}\n\n#ifdef CONFIG_ARCH_HAS_HUGEPD\nstatic unsigned long hugepte_addr_end(unsigned long addr, unsigned long end,\n\t\t\t\t      unsigned long sz)\n{\n\tunsigned long __boundary = (addr + sz) & ~(sz-1);\n\treturn (__boundary - 1 < end - 1) ? __boundary : end;\n}\n\nstatic int gup_hugepte(pte_t *ptep, unsigned long sz, unsigned long addr,\n\t\t       unsigned long end, unsigned int flags,\n\t\t       struct page **pages, int *nr)\n{\n\tunsigned long pte_end;\n\tstruct page *page;\n\tstruct folio *folio;\n\tpte_t pte;\n\tint refs;\n\n\tpte_end = (addr + sz) & ~(sz-1);\n\tif (pte_end < end)\n\t\tend = pte_end;\n\n\tpte = huge_ptep_get(ptep);\n\n\tif (!pte_access_permitted(pte, flags & FOLL_WRITE))\n\t\treturn 0;\n\n\t \n\tVM_BUG_ON(!pfn_valid(pte_pfn(pte)));\n\n\tpage = nth_page(pte_page(pte), (addr & (sz - 1)) >> PAGE_SHIFT);\n\trefs = record_subpages(page, addr, end, pages + *nr);\n\n\tfolio = try_grab_folio(page, refs, flags);\n\tif (!folio)\n\t\treturn 0;\n\n\tif (unlikely(pte_val(pte) != pte_val(ptep_get(ptep)))) {\n\t\tgup_put_folio(folio, refs, flags);\n\t\treturn 0;\n\t}\n\n\tif (!folio_fast_pin_allowed(folio, flags)) {\n\t\tgup_put_folio(folio, refs, flags);\n\t\treturn 0;\n\t}\n\n\tif (!pte_write(pte) && gup_must_unshare(NULL, flags, &folio->page)) {\n\t\tgup_put_folio(folio, refs, flags);\n\t\treturn 0;\n\t}\n\n\t*nr += refs;\n\tfolio_set_referenced(folio);\n\treturn 1;\n}\n\nstatic int gup_huge_pd(hugepd_t hugepd, unsigned long addr,\n\t\tunsigned int pdshift, unsigned long end, unsigned int flags,\n\t\tstruct page **pages, int *nr)\n{\n\tpte_t *ptep;\n\tunsigned long sz = 1UL << hugepd_shift(hugepd);\n\tunsigned long next;\n\n\tptep = hugepte_offset(hugepd, addr, pdshift);\n\tdo {\n\t\tnext = hugepte_addr_end(addr, end, sz);\n\t\tif (!gup_hugepte(ptep, sz, addr, end, flags, pages, nr))\n\t\t\treturn 0;\n\t} while (ptep++, addr = next, addr != end);\n\n\treturn 1;\n}\n#else\nstatic inline int gup_huge_pd(hugepd_t hugepd, unsigned long addr,\n\t\tunsigned int pdshift, unsigned long end, unsigned int flags,\n\t\tstruct page **pages, int *nr)\n{\n\treturn 0;\n}\n#endif  \n\nstatic int gup_huge_pmd(pmd_t orig, pmd_t *pmdp, unsigned long addr,\n\t\t\tunsigned long end, unsigned int flags,\n\t\t\tstruct page **pages, int *nr)\n{\n\tstruct page *page;\n\tstruct folio *folio;\n\tint refs;\n\n\tif (!pmd_access_permitted(orig, flags & FOLL_WRITE))\n\t\treturn 0;\n\n\tif (pmd_devmap(orig)) {\n\t\tif (unlikely(flags & FOLL_LONGTERM))\n\t\t\treturn 0;\n\t\treturn __gup_device_huge_pmd(orig, pmdp, addr, end, flags,\n\t\t\t\t\t     pages, nr);\n\t}\n\n\tpage = nth_page(pmd_page(orig), (addr & ~PMD_MASK) >> PAGE_SHIFT);\n\trefs = record_subpages(page, addr, end, pages + *nr);\n\n\tfolio = try_grab_folio(page, refs, flags);\n\tif (!folio)\n\t\treturn 0;\n\n\tif (unlikely(pmd_val(orig) != pmd_val(*pmdp))) {\n\t\tgup_put_folio(folio, refs, flags);\n\t\treturn 0;\n\t}\n\n\tif (!folio_fast_pin_allowed(folio, flags)) {\n\t\tgup_put_folio(folio, refs, flags);\n\t\treturn 0;\n\t}\n\tif (!pmd_write(orig) && gup_must_unshare(NULL, flags, &folio->page)) {\n\t\tgup_put_folio(folio, refs, flags);\n\t\treturn 0;\n\t}\n\n\t*nr += refs;\n\tfolio_set_referenced(folio);\n\treturn 1;\n}\n\nstatic int gup_huge_pud(pud_t orig, pud_t *pudp, unsigned long addr,\n\t\t\tunsigned long end, unsigned int flags,\n\t\t\tstruct page **pages, int *nr)\n{\n\tstruct page *page;\n\tstruct folio *folio;\n\tint refs;\n\n\tif (!pud_access_permitted(orig, flags & FOLL_WRITE))\n\t\treturn 0;\n\n\tif (pud_devmap(orig)) {\n\t\tif (unlikely(flags & FOLL_LONGTERM))\n\t\t\treturn 0;\n\t\treturn __gup_device_huge_pud(orig, pudp, addr, end, flags,\n\t\t\t\t\t     pages, nr);\n\t}\n\n\tpage = nth_page(pud_page(orig), (addr & ~PUD_MASK) >> PAGE_SHIFT);\n\trefs = record_subpages(page, addr, end, pages + *nr);\n\n\tfolio = try_grab_folio(page, refs, flags);\n\tif (!folio)\n\t\treturn 0;\n\n\tif (unlikely(pud_val(orig) != pud_val(*pudp))) {\n\t\tgup_put_folio(folio, refs, flags);\n\t\treturn 0;\n\t}\n\n\tif (!folio_fast_pin_allowed(folio, flags)) {\n\t\tgup_put_folio(folio, refs, flags);\n\t\treturn 0;\n\t}\n\n\tif (!pud_write(orig) && gup_must_unshare(NULL, flags, &folio->page)) {\n\t\tgup_put_folio(folio, refs, flags);\n\t\treturn 0;\n\t}\n\n\t*nr += refs;\n\tfolio_set_referenced(folio);\n\treturn 1;\n}\n\nstatic int gup_huge_pgd(pgd_t orig, pgd_t *pgdp, unsigned long addr,\n\t\t\tunsigned long end, unsigned int flags,\n\t\t\tstruct page **pages, int *nr)\n{\n\tint refs;\n\tstruct page *page;\n\tstruct folio *folio;\n\n\tif (!pgd_access_permitted(orig, flags & FOLL_WRITE))\n\t\treturn 0;\n\n\tBUILD_BUG_ON(pgd_devmap(orig));\n\n\tpage = nth_page(pgd_page(orig), (addr & ~PGDIR_MASK) >> PAGE_SHIFT);\n\trefs = record_subpages(page, addr, end, pages + *nr);\n\n\tfolio = try_grab_folio(page, refs, flags);\n\tif (!folio)\n\t\treturn 0;\n\n\tif (unlikely(pgd_val(orig) != pgd_val(*pgdp))) {\n\t\tgup_put_folio(folio, refs, flags);\n\t\treturn 0;\n\t}\n\n\tif (!pgd_write(orig) && gup_must_unshare(NULL, flags, &folio->page)) {\n\t\tgup_put_folio(folio, refs, flags);\n\t\treturn 0;\n\t}\n\n\tif (!folio_fast_pin_allowed(folio, flags)) {\n\t\tgup_put_folio(folio, refs, flags);\n\t\treturn 0;\n\t}\n\n\t*nr += refs;\n\tfolio_set_referenced(folio);\n\treturn 1;\n}\n\nstatic int gup_pmd_range(pud_t *pudp, pud_t pud, unsigned long addr, unsigned long end,\n\t\tunsigned int flags, struct page **pages, int *nr)\n{\n\tunsigned long next;\n\tpmd_t *pmdp;\n\n\tpmdp = pmd_offset_lockless(pudp, pud, addr);\n\tdo {\n\t\tpmd_t pmd = pmdp_get_lockless(pmdp);\n\n\t\tnext = pmd_addr_end(addr, end);\n\t\tif (!pmd_present(pmd))\n\t\t\treturn 0;\n\n\t\tif (unlikely(pmd_trans_huge(pmd) || pmd_huge(pmd) ||\n\t\t\t     pmd_devmap(pmd))) {\n\t\t\t \n\t\t\tif (pmd_protnone(pmd))\n\t\t\t\treturn 0;\n\n\t\t\tif (!gup_huge_pmd(pmd, pmdp, addr, next, flags,\n\t\t\t\tpages, nr))\n\t\t\t\treturn 0;\n\n\t\t} else if (unlikely(is_hugepd(__hugepd(pmd_val(pmd))))) {\n\t\t\t \n\t\t\tif (!gup_huge_pd(__hugepd(pmd_val(pmd)), addr,\n\t\t\t\t\t PMD_SHIFT, next, flags, pages, nr))\n\t\t\t\treturn 0;\n\t\t} else if (!gup_pte_range(pmd, pmdp, addr, next, flags, pages, nr))\n\t\t\treturn 0;\n\t} while (pmdp++, addr = next, addr != end);\n\n\treturn 1;\n}\n\nstatic int gup_pud_range(p4d_t *p4dp, p4d_t p4d, unsigned long addr, unsigned long end,\n\t\t\t unsigned int flags, struct page **pages, int *nr)\n{\n\tunsigned long next;\n\tpud_t *pudp;\n\n\tpudp = pud_offset_lockless(p4dp, p4d, addr);\n\tdo {\n\t\tpud_t pud = READ_ONCE(*pudp);\n\n\t\tnext = pud_addr_end(addr, end);\n\t\tif (unlikely(!pud_present(pud)))\n\t\t\treturn 0;\n\t\tif (unlikely(pud_huge(pud) || pud_devmap(pud))) {\n\t\t\tif (!gup_huge_pud(pud, pudp, addr, next, flags,\n\t\t\t\t\t  pages, nr))\n\t\t\t\treturn 0;\n\t\t} else if (unlikely(is_hugepd(__hugepd(pud_val(pud))))) {\n\t\t\tif (!gup_huge_pd(__hugepd(pud_val(pud)), addr,\n\t\t\t\t\t PUD_SHIFT, next, flags, pages, nr))\n\t\t\t\treturn 0;\n\t\t} else if (!gup_pmd_range(pudp, pud, addr, next, flags, pages, nr))\n\t\t\treturn 0;\n\t} while (pudp++, addr = next, addr != end);\n\n\treturn 1;\n}\n\nstatic int gup_p4d_range(pgd_t *pgdp, pgd_t pgd, unsigned long addr, unsigned long end,\n\t\t\t unsigned int flags, struct page **pages, int *nr)\n{\n\tunsigned long next;\n\tp4d_t *p4dp;\n\n\tp4dp = p4d_offset_lockless(pgdp, pgd, addr);\n\tdo {\n\t\tp4d_t p4d = READ_ONCE(*p4dp);\n\n\t\tnext = p4d_addr_end(addr, end);\n\t\tif (p4d_none(p4d))\n\t\t\treturn 0;\n\t\tBUILD_BUG_ON(p4d_huge(p4d));\n\t\tif (unlikely(is_hugepd(__hugepd(p4d_val(p4d))))) {\n\t\t\tif (!gup_huge_pd(__hugepd(p4d_val(p4d)), addr,\n\t\t\t\t\t P4D_SHIFT, next, flags, pages, nr))\n\t\t\t\treturn 0;\n\t\t} else if (!gup_pud_range(p4dp, p4d, addr, next, flags, pages, nr))\n\t\t\treturn 0;\n\t} while (p4dp++, addr = next, addr != end);\n\n\treturn 1;\n}\n\nstatic void gup_pgd_range(unsigned long addr, unsigned long end,\n\t\tunsigned int flags, struct page **pages, int *nr)\n{\n\tunsigned long next;\n\tpgd_t *pgdp;\n\n\tpgdp = pgd_offset(current->mm, addr);\n\tdo {\n\t\tpgd_t pgd = READ_ONCE(*pgdp);\n\n\t\tnext = pgd_addr_end(addr, end);\n\t\tif (pgd_none(pgd))\n\t\t\treturn;\n\t\tif (unlikely(pgd_huge(pgd))) {\n\t\t\tif (!gup_huge_pgd(pgd, pgdp, addr, next, flags,\n\t\t\t\t\t  pages, nr))\n\t\t\t\treturn;\n\t\t} else if (unlikely(is_hugepd(__hugepd(pgd_val(pgd))))) {\n\t\t\tif (!gup_huge_pd(__hugepd(pgd_val(pgd)), addr,\n\t\t\t\t\t PGDIR_SHIFT, next, flags, pages, nr))\n\t\t\t\treturn;\n\t\t} else if (!gup_p4d_range(pgdp, pgd, addr, next, flags, pages, nr))\n\t\t\treturn;\n\t} while (pgdp++, addr = next, addr != end);\n}\n#else\nstatic inline void gup_pgd_range(unsigned long addr, unsigned long end,\n\t\tunsigned int flags, struct page **pages, int *nr)\n{\n}\n#endif  \n\n#ifndef gup_fast_permitted\n \nstatic bool gup_fast_permitted(unsigned long start, unsigned long end)\n{\n\treturn true;\n}\n#endif\n\nstatic unsigned long lockless_pages_from_mm(unsigned long start,\n\t\t\t\t\t    unsigned long end,\n\t\t\t\t\t    unsigned int gup_flags,\n\t\t\t\t\t    struct page **pages)\n{\n\tunsigned long flags;\n\tint nr_pinned = 0;\n\tunsigned seq;\n\n\tif (!IS_ENABLED(CONFIG_HAVE_FAST_GUP) ||\n\t    !gup_fast_permitted(start, end))\n\t\treturn 0;\n\n\tif (gup_flags & FOLL_PIN) {\n\t\tseq = raw_read_seqcount(&current->mm->write_protect_seq);\n\t\tif (seq & 1)\n\t\t\treturn 0;\n\t}\n\n\t \n\tlocal_irq_save(flags);\n\tgup_pgd_range(start, end, gup_flags, pages, &nr_pinned);\n\tlocal_irq_restore(flags);\n\n\t \n\tif (gup_flags & FOLL_PIN) {\n\t\tif (read_seqcount_retry(&current->mm->write_protect_seq, seq)) {\n\t\t\tunpin_user_pages_lockless(pages, nr_pinned);\n\t\t\treturn 0;\n\t\t} else {\n\t\t\tsanity_check_pinned_pages(pages, nr_pinned);\n\t\t}\n\t}\n\treturn nr_pinned;\n}\n\nstatic int internal_get_user_pages_fast(unsigned long start,\n\t\t\t\t\tunsigned long nr_pages,\n\t\t\t\t\tunsigned int gup_flags,\n\t\t\t\t\tstruct page **pages)\n{\n\tunsigned long len, end;\n\tunsigned long nr_pinned;\n\tint locked = 0;\n\tint ret;\n\n\tif (WARN_ON_ONCE(gup_flags & ~(FOLL_WRITE | FOLL_LONGTERM |\n\t\t\t\t       FOLL_FORCE | FOLL_PIN | FOLL_GET |\n\t\t\t\t       FOLL_FAST_ONLY | FOLL_NOFAULT |\n\t\t\t\t       FOLL_PCI_P2PDMA | FOLL_HONOR_NUMA_FAULT)))\n\t\treturn -EINVAL;\n\n\tif (gup_flags & FOLL_PIN)\n\t\tmm_set_has_pinned_flag(&current->mm->flags);\n\n\tif (!(gup_flags & FOLL_FAST_ONLY))\n\t\tmight_lock_read(&current->mm->mmap_lock);\n\n\tstart = untagged_addr(start) & PAGE_MASK;\n\tlen = nr_pages << PAGE_SHIFT;\n\tif (check_add_overflow(start, len, &end))\n\t\treturn -EOVERFLOW;\n\tif (end > TASK_SIZE_MAX)\n\t\treturn -EFAULT;\n\tif (unlikely(!access_ok((void __user *)start, len)))\n\t\treturn -EFAULT;\n\n\tnr_pinned = lockless_pages_from_mm(start, end, gup_flags, pages);\n\tif (nr_pinned == nr_pages || gup_flags & FOLL_FAST_ONLY)\n\t\treturn nr_pinned;\n\n\t \n\tstart += nr_pinned << PAGE_SHIFT;\n\tpages += nr_pinned;\n\tret = __gup_longterm_locked(current->mm, start, nr_pages - nr_pinned,\n\t\t\t\t    pages, &locked,\n\t\t\t\t    gup_flags | FOLL_TOUCH | FOLL_UNLOCKABLE);\n\tif (ret < 0) {\n\t\t \n\t\tif (nr_pinned)\n\t\t\treturn nr_pinned;\n\t\treturn ret;\n\t}\n\treturn ret + nr_pinned;\n}\n\n \nint get_user_pages_fast_only(unsigned long start, int nr_pages,\n\t\t\t     unsigned int gup_flags, struct page **pages)\n{\n\t \n\tif (!is_valid_gup_args(pages, NULL, &gup_flags,\n\t\t\t       FOLL_GET | FOLL_FAST_ONLY))\n\t\treturn -EINVAL;\n\n\treturn internal_get_user_pages_fast(start, nr_pages, gup_flags, pages);\n}\nEXPORT_SYMBOL_GPL(get_user_pages_fast_only);\n\n \nint get_user_pages_fast(unsigned long start, int nr_pages,\n\t\t\tunsigned int gup_flags, struct page **pages)\n{\n\t \n\tif (!is_valid_gup_args(pages, NULL, &gup_flags, FOLL_GET))\n\t\treturn -EINVAL;\n\treturn internal_get_user_pages_fast(start, nr_pages, gup_flags, pages);\n}\nEXPORT_SYMBOL_GPL(get_user_pages_fast);\n\n \nint pin_user_pages_fast(unsigned long start, int nr_pages,\n\t\t\tunsigned int gup_flags, struct page **pages)\n{\n\tif (!is_valid_gup_args(pages, NULL, &gup_flags, FOLL_PIN))\n\t\treturn -EINVAL;\n\treturn internal_get_user_pages_fast(start, nr_pages, gup_flags, pages);\n}\nEXPORT_SYMBOL_GPL(pin_user_pages_fast);\n\n \nlong pin_user_pages_remote(struct mm_struct *mm,\n\t\t\t   unsigned long start, unsigned long nr_pages,\n\t\t\t   unsigned int gup_flags, struct page **pages,\n\t\t\t   int *locked)\n{\n\tint local_locked = 1;\n\n\tif (!is_valid_gup_args(pages, locked, &gup_flags,\n\t\t\t       FOLL_PIN | FOLL_TOUCH | FOLL_REMOTE))\n\t\treturn 0;\n\treturn __gup_longterm_locked(mm, start, nr_pages, pages,\n\t\t\t\t     locked ? locked : &local_locked,\n\t\t\t\t     gup_flags);\n}\nEXPORT_SYMBOL(pin_user_pages_remote);\n\n \nlong pin_user_pages(unsigned long start, unsigned long nr_pages,\n\t\t    unsigned int gup_flags, struct page **pages)\n{\n\tint locked = 1;\n\n\tif (!is_valid_gup_args(pages, NULL, &gup_flags, FOLL_PIN))\n\t\treturn 0;\n\treturn __gup_longterm_locked(current->mm, start, nr_pages,\n\t\t\t\t     pages, &locked, gup_flags);\n}\nEXPORT_SYMBOL(pin_user_pages);\n\n \nlong pin_user_pages_unlocked(unsigned long start, unsigned long nr_pages,\n\t\t\t     struct page **pages, unsigned int gup_flags)\n{\n\tint locked = 0;\n\n\tif (!is_valid_gup_args(pages, NULL, &gup_flags,\n\t\t\t       FOLL_PIN | FOLL_TOUCH | FOLL_UNLOCKABLE))\n\t\treturn 0;\n\n\treturn __gup_longterm_locked(current->mm, start, nr_pages, pages,\n\t\t\t\t     &locked, gup_flags);\n}\nEXPORT_SYMBOL(pin_user_pages_unlocked);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}