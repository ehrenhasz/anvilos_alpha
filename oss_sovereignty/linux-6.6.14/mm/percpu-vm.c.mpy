{
  "module_name": "percpu-vm.c",
  "hash_id": "045310726bea57f571dd0227fcb793b698fb124b85000594dcfde2755c451b18",
  "original_prompt": "Ingested from linux-6.6.14/mm/percpu-vm.c",
  "human_readable_source": "\n \n#include \"internal.h\"\n\nstatic struct page *pcpu_chunk_page(struct pcpu_chunk *chunk,\n\t\t\t\t    unsigned int cpu, int page_idx)\n{\n\t \n\tWARN_ON(chunk->immutable);\n\n\treturn vmalloc_to_page((void *)pcpu_chunk_addr(chunk, cpu, page_idx));\n}\n\n \nstatic struct page **pcpu_get_pages(void)\n{\n\tstatic struct page **pages;\n\tsize_t pages_size = pcpu_nr_units * pcpu_unit_pages * sizeof(pages[0]);\n\n\tlockdep_assert_held(&pcpu_alloc_mutex);\n\n\tif (!pages)\n\t\tpages = pcpu_mem_zalloc(pages_size, GFP_KERNEL);\n\treturn pages;\n}\n\n \nstatic void pcpu_free_pages(struct pcpu_chunk *chunk,\n\t\t\t    struct page **pages, int page_start, int page_end)\n{\n\tunsigned int cpu;\n\tint i;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tfor (i = page_start; i < page_end; i++) {\n\t\t\tstruct page *page = pages[pcpu_page_idx(cpu, i)];\n\n\t\t\tif (page)\n\t\t\t\t__free_page(page);\n\t\t}\n\t}\n}\n\n \nstatic int pcpu_alloc_pages(struct pcpu_chunk *chunk,\n\t\t\t    struct page **pages, int page_start, int page_end,\n\t\t\t    gfp_t gfp)\n{\n\tunsigned int cpu, tcpu;\n\tint i;\n\n\tgfp |= __GFP_HIGHMEM;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tfor (i = page_start; i < page_end; i++) {\n\t\t\tstruct page **pagep = &pages[pcpu_page_idx(cpu, i)];\n\n\t\t\t*pagep = alloc_pages_node(cpu_to_node(cpu), gfp, 0);\n\t\t\tif (!*pagep)\n\t\t\t\tgoto err;\n\t\t}\n\t}\n\treturn 0;\n\nerr:\n\twhile (--i >= page_start)\n\t\t__free_page(pages[pcpu_page_idx(cpu, i)]);\n\n\tfor_each_possible_cpu(tcpu) {\n\t\tif (tcpu == cpu)\n\t\t\tbreak;\n\t\tfor (i = page_start; i < page_end; i++)\n\t\t\t__free_page(pages[pcpu_page_idx(tcpu, i)]);\n\t}\n\treturn -ENOMEM;\n}\n\n \nstatic void pcpu_pre_unmap_flush(struct pcpu_chunk *chunk,\n\t\t\t\t int page_start, int page_end)\n{\n\tflush_cache_vunmap(\n\t\tpcpu_chunk_addr(chunk, pcpu_low_unit_cpu, page_start),\n\t\tpcpu_chunk_addr(chunk, pcpu_high_unit_cpu, page_end));\n}\n\nstatic void __pcpu_unmap_pages(unsigned long addr, int nr_pages)\n{\n\tvunmap_range_noflush(addr, addr + (nr_pages << PAGE_SHIFT));\n}\n\n \nstatic void pcpu_unmap_pages(struct pcpu_chunk *chunk,\n\t\t\t     struct page **pages, int page_start, int page_end)\n{\n\tunsigned int cpu;\n\tint i;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tfor (i = page_start; i < page_end; i++) {\n\t\t\tstruct page *page;\n\n\t\t\tpage = pcpu_chunk_page(chunk, cpu, i);\n\t\t\tWARN_ON(!page);\n\t\t\tpages[pcpu_page_idx(cpu, i)] = page;\n\t\t}\n\t\t__pcpu_unmap_pages(pcpu_chunk_addr(chunk, cpu, page_start),\n\t\t\t\t   page_end - page_start);\n\t}\n}\n\n \nstatic void pcpu_post_unmap_tlb_flush(struct pcpu_chunk *chunk,\n\t\t\t\t      int page_start, int page_end)\n{\n\tflush_tlb_kernel_range(\n\t\tpcpu_chunk_addr(chunk, pcpu_low_unit_cpu, page_start),\n\t\tpcpu_chunk_addr(chunk, pcpu_high_unit_cpu, page_end));\n}\n\nstatic int __pcpu_map_pages(unsigned long addr, struct page **pages,\n\t\t\t    int nr_pages)\n{\n\treturn vmap_pages_range_noflush(addr, addr + (nr_pages << PAGE_SHIFT),\n\t\t\t\t\tPAGE_KERNEL, pages, PAGE_SHIFT);\n}\n\n \nstatic int pcpu_map_pages(struct pcpu_chunk *chunk,\n\t\t\t  struct page **pages, int page_start, int page_end)\n{\n\tunsigned int cpu, tcpu;\n\tint i, err;\n\n\tfor_each_possible_cpu(cpu) {\n\t\terr = __pcpu_map_pages(pcpu_chunk_addr(chunk, cpu, page_start),\n\t\t\t\t       &pages[pcpu_page_idx(cpu, page_start)],\n\t\t\t\t       page_end - page_start);\n\t\tif (err < 0)\n\t\t\tgoto err;\n\n\t\tfor (i = page_start; i < page_end; i++)\n\t\t\tpcpu_set_page_chunk(pages[pcpu_page_idx(cpu, i)],\n\t\t\t\t\t    chunk);\n\t}\n\treturn 0;\nerr:\n\tfor_each_possible_cpu(tcpu) {\n\t\tif (tcpu == cpu)\n\t\t\tbreak;\n\t\t__pcpu_unmap_pages(pcpu_chunk_addr(chunk, tcpu, page_start),\n\t\t\t\t   page_end - page_start);\n\t}\n\tpcpu_post_unmap_tlb_flush(chunk, page_start, page_end);\n\treturn err;\n}\n\n \nstatic void pcpu_post_map_flush(struct pcpu_chunk *chunk,\n\t\t\t\tint page_start, int page_end)\n{\n\tflush_cache_vmap(\n\t\tpcpu_chunk_addr(chunk, pcpu_low_unit_cpu, page_start),\n\t\tpcpu_chunk_addr(chunk, pcpu_high_unit_cpu, page_end));\n}\n\n \nstatic int pcpu_populate_chunk(struct pcpu_chunk *chunk,\n\t\t\t       int page_start, int page_end, gfp_t gfp)\n{\n\tstruct page **pages;\n\n\tpages = pcpu_get_pages();\n\tif (!pages)\n\t\treturn -ENOMEM;\n\n\tif (pcpu_alloc_pages(chunk, pages, page_start, page_end, gfp))\n\t\treturn -ENOMEM;\n\n\tif (pcpu_map_pages(chunk, pages, page_start, page_end)) {\n\t\tpcpu_free_pages(chunk, pages, page_start, page_end);\n\t\treturn -ENOMEM;\n\t}\n\tpcpu_post_map_flush(chunk, page_start, page_end);\n\n\treturn 0;\n}\n\n \nstatic void pcpu_depopulate_chunk(struct pcpu_chunk *chunk,\n\t\t\t\t  int page_start, int page_end)\n{\n\tstruct page **pages;\n\n\t \n\tpages = pcpu_get_pages();\n\tBUG_ON(!pages);\n\n\t \n\tpcpu_pre_unmap_flush(chunk, page_start, page_end);\n\n\tpcpu_unmap_pages(chunk, pages, page_start, page_end);\n\n\tpcpu_free_pages(chunk, pages, page_start, page_end);\n}\n\nstatic struct pcpu_chunk *pcpu_create_chunk(gfp_t gfp)\n{\n\tstruct pcpu_chunk *chunk;\n\tstruct vm_struct **vms;\n\n\tchunk = pcpu_alloc_chunk(gfp);\n\tif (!chunk)\n\t\treturn NULL;\n\n\tvms = pcpu_get_vm_areas(pcpu_group_offsets, pcpu_group_sizes,\n\t\t\t\tpcpu_nr_groups, pcpu_atom_size);\n\tif (!vms) {\n\t\tpcpu_free_chunk(chunk);\n\t\treturn NULL;\n\t}\n\n\tchunk->data = vms;\n\tchunk->base_addr = vms[0]->addr - pcpu_group_offsets[0];\n\n\tpcpu_stats_chunk_alloc();\n\ttrace_percpu_create_chunk(chunk->base_addr);\n\n\treturn chunk;\n}\n\nstatic void pcpu_destroy_chunk(struct pcpu_chunk *chunk)\n{\n\tif (!chunk)\n\t\treturn;\n\n\tpcpu_stats_chunk_dealloc();\n\ttrace_percpu_destroy_chunk(chunk->base_addr);\n\n\tif (chunk->data)\n\t\tpcpu_free_vm_areas(chunk->data, pcpu_nr_groups);\n\tpcpu_free_chunk(chunk);\n}\n\nstatic struct page *pcpu_addr_to_page(void *addr)\n{\n\treturn vmalloc_to_page(addr);\n}\n\nstatic int __init pcpu_verify_alloc_info(const struct pcpu_alloc_info *ai)\n{\n\t \n\treturn 0;\n}\n\n \nstatic bool pcpu_should_reclaim_chunk(struct pcpu_chunk *chunk)\n{\n\t \n\tif (chunk == pcpu_first_chunk || chunk == pcpu_reserved_chunk)\n\t\treturn false;\n\n\t \n\treturn ((chunk->isolated && chunk->nr_empty_pop_pages) ||\n\t\t(pcpu_nr_empty_pop_pages >\n\t\t (PCPU_EMPTY_POP_PAGES_HIGH + chunk->nr_empty_pop_pages) &&\n\t\t chunk->nr_empty_pop_pages >= chunk->nr_pages / 4));\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}