{
  "module_name": "ksm.c",
  "hash_id": "99b228fca16cad72900e08b01a4e43cb6a91ebce49b4d2968a17a500cb1110e3",
  "original_prompt": "Ingested from linux-6.6.14/mm/ksm.c",
  "human_readable_source": "\n \n\n#include <linux/errno.h>\n#include <linux/mm.h>\n#include <linux/mm_inline.h>\n#include <linux/fs.h>\n#include <linux/mman.h>\n#include <linux/sched.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/coredump.h>\n#include <linux/rwsem.h>\n#include <linux/pagemap.h>\n#include <linux/rmap.h>\n#include <linux/spinlock.h>\n#include <linux/xxhash.h>\n#include <linux/delay.h>\n#include <linux/kthread.h>\n#include <linux/wait.h>\n#include <linux/slab.h>\n#include <linux/rbtree.h>\n#include <linux/memory.h>\n#include <linux/mmu_notifier.h>\n#include <linux/swap.h>\n#include <linux/ksm.h>\n#include <linux/hashtable.h>\n#include <linux/freezer.h>\n#include <linux/oom.h>\n#include <linux/numa.h>\n#include <linux/pagewalk.h>\n\n#include <asm/tlbflush.h>\n#include \"internal.h\"\n#include \"mm_slot.h\"\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/ksm.h>\n\n#ifdef CONFIG_NUMA\n#define NUMA(x)\t\t(x)\n#define DO_NUMA(x)\tdo { (x); } while (0)\n#else\n#define NUMA(x)\t\t(0)\n#define DO_NUMA(x)\tdo { } while (0)\n#endif\n\n \n\n \nstruct ksm_mm_slot {\n\tstruct mm_slot slot;\n\tstruct ksm_rmap_item *rmap_list;\n};\n\n \nstruct ksm_scan {\n\tstruct ksm_mm_slot *mm_slot;\n\tunsigned long address;\n\tstruct ksm_rmap_item **rmap_list;\n\tunsigned long seqnr;\n};\n\n \nstruct ksm_stable_node {\n\tunion {\n\t\tstruct rb_node node;\t \n\t\tstruct {\t\t \n\t\t\tstruct list_head *head;\n\t\t\tstruct {\n\t\t\t\tstruct hlist_node hlist_dup;\n\t\t\t\tstruct list_head list;\n\t\t\t};\n\t\t};\n\t};\n\tstruct hlist_head hlist;\n\tunion {\n\t\tunsigned long kpfn;\n\t\tunsigned long chain_prune_time;\n\t};\n\t \n#define STABLE_NODE_CHAIN -1024\n\tint rmap_hlist_len;\n#ifdef CONFIG_NUMA\n\tint nid;\n#endif\n};\n\n \nstruct ksm_rmap_item {\n\tstruct ksm_rmap_item *rmap_list;\n\tunion {\n\t\tstruct anon_vma *anon_vma;\t \n#ifdef CONFIG_NUMA\n\t\tint nid;\t\t \n#endif\n\t};\n\tstruct mm_struct *mm;\n\tunsigned long address;\t\t \n\tunsigned int oldchecksum;\t \n\tunion {\n\t\tstruct rb_node node;\t \n\t\tstruct {\t\t \n\t\t\tstruct ksm_stable_node *head;\n\t\t\tstruct hlist_node hlist;\n\t\t};\n\t};\n};\n\n#define SEQNR_MASK\t0x0ff\t \n#define UNSTABLE_FLAG\t0x100\t \n#define STABLE_FLAG\t0x200\t \n\n \nstatic struct rb_root one_stable_tree[1] = { RB_ROOT };\nstatic struct rb_root one_unstable_tree[1] = { RB_ROOT };\nstatic struct rb_root *root_stable_tree = one_stable_tree;\nstatic struct rb_root *root_unstable_tree = one_unstable_tree;\n\n \nstatic LIST_HEAD(migrate_nodes);\n#define STABLE_NODE_DUP_HEAD ((struct list_head *)&migrate_nodes.prev)\n\n#define MM_SLOTS_HASH_BITS 10\nstatic DEFINE_HASHTABLE(mm_slots_hash, MM_SLOTS_HASH_BITS);\n\nstatic struct ksm_mm_slot ksm_mm_head = {\n\t.slot.mm_node = LIST_HEAD_INIT(ksm_mm_head.slot.mm_node),\n};\nstatic struct ksm_scan ksm_scan = {\n\t.mm_slot = &ksm_mm_head,\n};\n\nstatic struct kmem_cache *rmap_item_cache;\nstatic struct kmem_cache *stable_node_cache;\nstatic struct kmem_cache *mm_slot_cache;\n\n \nstatic unsigned long ksm_pages_scanned;\n\n \nstatic unsigned long ksm_pages_shared;\n\n \nstatic unsigned long ksm_pages_sharing;\n\n \nstatic unsigned long ksm_pages_unshared;\n\n \nstatic unsigned long ksm_rmap_items;\n\n \nstatic unsigned long ksm_stable_node_chains;\n\n \nstatic unsigned long ksm_stable_node_dups;\n\n \nstatic unsigned int ksm_stable_node_chains_prune_millisecs = 2000;\n\n \nstatic int ksm_max_page_sharing = 256;\n\n \nstatic unsigned int ksm_thread_pages_to_scan = 100;\n\n \nstatic unsigned int ksm_thread_sleep_millisecs = 20;\n\n \nstatic unsigned int zero_checksum __read_mostly;\n\n \nstatic bool ksm_use_zero_pages __read_mostly;\n\n \nunsigned long ksm_zero_pages;\n\n#ifdef CONFIG_NUMA\n \nstatic unsigned int ksm_merge_across_nodes = 1;\nstatic int ksm_nr_node_ids = 1;\n#else\n#define ksm_merge_across_nodes\t1U\n#define ksm_nr_node_ids\t\t1\n#endif\n\n#define KSM_RUN_STOP\t0\n#define KSM_RUN_MERGE\t1\n#define KSM_RUN_UNMERGE\t2\n#define KSM_RUN_OFFLINE\t4\nstatic unsigned long ksm_run = KSM_RUN_STOP;\nstatic void wait_while_offlining(void);\n\nstatic DECLARE_WAIT_QUEUE_HEAD(ksm_thread_wait);\nstatic DECLARE_WAIT_QUEUE_HEAD(ksm_iter_wait);\nstatic DEFINE_MUTEX(ksm_thread_mutex);\nstatic DEFINE_SPINLOCK(ksm_mmlist_lock);\n\n#define KSM_KMEM_CACHE(__struct, __flags) kmem_cache_create(#__struct,\\\n\t\tsizeof(struct __struct), __alignof__(struct __struct),\\\n\t\t(__flags), NULL)\n\nstatic int __init ksm_slab_init(void)\n{\n\trmap_item_cache = KSM_KMEM_CACHE(ksm_rmap_item, 0);\n\tif (!rmap_item_cache)\n\t\tgoto out;\n\n\tstable_node_cache = KSM_KMEM_CACHE(ksm_stable_node, 0);\n\tif (!stable_node_cache)\n\t\tgoto out_free1;\n\n\tmm_slot_cache = KSM_KMEM_CACHE(ksm_mm_slot, 0);\n\tif (!mm_slot_cache)\n\t\tgoto out_free2;\n\n\treturn 0;\n\nout_free2:\n\tkmem_cache_destroy(stable_node_cache);\nout_free1:\n\tkmem_cache_destroy(rmap_item_cache);\nout:\n\treturn -ENOMEM;\n}\n\nstatic void __init ksm_slab_free(void)\n{\n\tkmem_cache_destroy(mm_slot_cache);\n\tkmem_cache_destroy(stable_node_cache);\n\tkmem_cache_destroy(rmap_item_cache);\n\tmm_slot_cache = NULL;\n}\n\nstatic __always_inline bool is_stable_node_chain(struct ksm_stable_node *chain)\n{\n\treturn chain->rmap_hlist_len == STABLE_NODE_CHAIN;\n}\n\nstatic __always_inline bool is_stable_node_dup(struct ksm_stable_node *dup)\n{\n\treturn dup->head == STABLE_NODE_DUP_HEAD;\n}\n\nstatic inline void stable_node_chain_add_dup(struct ksm_stable_node *dup,\n\t\t\t\t\t     struct ksm_stable_node *chain)\n{\n\tVM_BUG_ON(is_stable_node_dup(dup));\n\tdup->head = STABLE_NODE_DUP_HEAD;\n\tVM_BUG_ON(!is_stable_node_chain(chain));\n\thlist_add_head(&dup->hlist_dup, &chain->hlist);\n\tksm_stable_node_dups++;\n}\n\nstatic inline void __stable_node_dup_del(struct ksm_stable_node *dup)\n{\n\tVM_BUG_ON(!is_stable_node_dup(dup));\n\thlist_del(&dup->hlist_dup);\n\tksm_stable_node_dups--;\n}\n\nstatic inline void stable_node_dup_del(struct ksm_stable_node *dup)\n{\n\tVM_BUG_ON(is_stable_node_chain(dup));\n\tif (is_stable_node_dup(dup))\n\t\t__stable_node_dup_del(dup);\n\telse\n\t\trb_erase(&dup->node, root_stable_tree + NUMA(dup->nid));\n#ifdef CONFIG_DEBUG_VM\n\tdup->head = NULL;\n#endif\n}\n\nstatic inline struct ksm_rmap_item *alloc_rmap_item(void)\n{\n\tstruct ksm_rmap_item *rmap_item;\n\n\trmap_item = kmem_cache_zalloc(rmap_item_cache, GFP_KERNEL |\n\t\t\t\t\t\t__GFP_NORETRY | __GFP_NOWARN);\n\tif (rmap_item)\n\t\tksm_rmap_items++;\n\treturn rmap_item;\n}\n\nstatic inline void free_rmap_item(struct ksm_rmap_item *rmap_item)\n{\n\tksm_rmap_items--;\n\trmap_item->mm->ksm_rmap_items--;\n\trmap_item->mm = NULL;\t \n\tkmem_cache_free(rmap_item_cache, rmap_item);\n}\n\nstatic inline struct ksm_stable_node *alloc_stable_node(void)\n{\n\t \n\treturn kmem_cache_alloc(stable_node_cache, GFP_KERNEL | __GFP_HIGH);\n}\n\nstatic inline void free_stable_node(struct ksm_stable_node *stable_node)\n{\n\tVM_BUG_ON(stable_node->rmap_hlist_len &&\n\t\t  !is_stable_node_chain(stable_node));\n\tkmem_cache_free(stable_node_cache, stable_node);\n}\n\n \nstatic inline bool ksm_test_exit(struct mm_struct *mm)\n{\n\treturn atomic_read(&mm->mm_users) == 0;\n}\n\nstatic int break_ksm_pmd_entry(pmd_t *pmd, unsigned long addr, unsigned long next,\n\t\t\tstruct mm_walk *walk)\n{\n\tstruct page *page = NULL;\n\tspinlock_t *ptl;\n\tpte_t *pte;\n\tpte_t ptent;\n\tint ret;\n\n\tpte = pte_offset_map_lock(walk->mm, pmd, addr, &ptl);\n\tif (!pte)\n\t\treturn 0;\n\tptent = ptep_get(pte);\n\tif (pte_present(ptent)) {\n\t\tpage = vm_normal_page(walk->vma, addr, ptent);\n\t} else if (!pte_none(ptent)) {\n\t\tswp_entry_t entry = pte_to_swp_entry(ptent);\n\n\t\t \n\t\tif (is_migration_entry(entry))\n\t\t\tpage = pfn_swap_entry_to_page(entry);\n\t}\n\t \n\tret = (page && PageKsm(page)) || is_ksm_zero_pte(*pte);\n\tpte_unmap_unlock(pte, ptl);\n\treturn ret;\n}\n\nstatic const struct mm_walk_ops break_ksm_ops = {\n\t.pmd_entry = break_ksm_pmd_entry,\n\t.walk_lock = PGWALK_RDLOCK,\n};\n\nstatic const struct mm_walk_ops break_ksm_lock_vma_ops = {\n\t.pmd_entry = break_ksm_pmd_entry,\n\t.walk_lock = PGWALK_WRLOCK,\n};\n\n \nstatic int break_ksm(struct vm_area_struct *vma, unsigned long addr, bool lock_vma)\n{\n\tvm_fault_t ret = 0;\n\tconst struct mm_walk_ops *ops = lock_vma ?\n\t\t\t\t&break_ksm_lock_vma_ops : &break_ksm_ops;\n\n\tdo {\n\t\tint ksm_page;\n\n\t\tcond_resched();\n\t\tksm_page = walk_page_range_vma(vma, addr, addr + 1, ops, NULL);\n\t\tif (WARN_ON_ONCE(ksm_page < 0))\n\t\t\treturn ksm_page;\n\t\tif (!ksm_page)\n\t\t\treturn 0;\n\t\tret = handle_mm_fault(vma, addr,\n\t\t\t\t      FAULT_FLAG_UNSHARE | FAULT_FLAG_REMOTE,\n\t\t\t\t      NULL);\n\t} while (!(ret & (VM_FAULT_SIGBUS | VM_FAULT_SIGSEGV | VM_FAULT_OOM)));\n\t \n\treturn (ret & VM_FAULT_OOM) ? -ENOMEM : 0;\n}\n\nstatic bool vma_ksm_compatible(struct vm_area_struct *vma)\n{\n\tif (vma->vm_flags & (VM_SHARED  | VM_MAYSHARE   | VM_PFNMAP  |\n\t\t\t     VM_IO      | VM_DONTEXPAND | VM_HUGETLB |\n\t\t\t     VM_MIXEDMAP))\n\t\treturn false;\t\t \n\n\tif (vma_is_dax(vma))\n\t\treturn false;\n\n#ifdef VM_SAO\n\tif (vma->vm_flags & VM_SAO)\n\t\treturn false;\n#endif\n#ifdef VM_SPARC_ADI\n\tif (vma->vm_flags & VM_SPARC_ADI)\n\t\treturn false;\n#endif\n\n\treturn true;\n}\n\nstatic struct vm_area_struct *find_mergeable_vma(struct mm_struct *mm,\n\t\tunsigned long addr)\n{\n\tstruct vm_area_struct *vma;\n\tif (ksm_test_exit(mm))\n\t\treturn NULL;\n\tvma = vma_lookup(mm, addr);\n\tif (!vma || !(vma->vm_flags & VM_MERGEABLE) || !vma->anon_vma)\n\t\treturn NULL;\n\treturn vma;\n}\n\nstatic void break_cow(struct ksm_rmap_item *rmap_item)\n{\n\tstruct mm_struct *mm = rmap_item->mm;\n\tunsigned long addr = rmap_item->address;\n\tstruct vm_area_struct *vma;\n\n\t \n\tput_anon_vma(rmap_item->anon_vma);\n\n\tmmap_read_lock(mm);\n\tvma = find_mergeable_vma(mm, addr);\n\tif (vma)\n\t\tbreak_ksm(vma, addr, false);\n\tmmap_read_unlock(mm);\n}\n\nstatic struct page *get_mergeable_page(struct ksm_rmap_item *rmap_item)\n{\n\tstruct mm_struct *mm = rmap_item->mm;\n\tunsigned long addr = rmap_item->address;\n\tstruct vm_area_struct *vma;\n\tstruct page *page;\n\n\tmmap_read_lock(mm);\n\tvma = find_mergeable_vma(mm, addr);\n\tif (!vma)\n\t\tgoto out;\n\n\tpage = follow_page(vma, addr, FOLL_GET);\n\tif (IS_ERR_OR_NULL(page))\n\t\tgoto out;\n\tif (is_zone_device_page(page))\n\t\tgoto out_putpage;\n\tif (PageAnon(page)) {\n\t\tflush_anon_page(vma, page, addr);\n\t\tflush_dcache_page(page);\n\t} else {\nout_putpage:\n\t\tput_page(page);\nout:\n\t\tpage = NULL;\n\t}\n\tmmap_read_unlock(mm);\n\treturn page;\n}\n\n \nstatic inline int get_kpfn_nid(unsigned long kpfn)\n{\n\treturn ksm_merge_across_nodes ? 0 : NUMA(pfn_to_nid(kpfn));\n}\n\nstatic struct ksm_stable_node *alloc_stable_node_chain(struct ksm_stable_node *dup,\n\t\t\t\t\t\t   struct rb_root *root)\n{\n\tstruct ksm_stable_node *chain = alloc_stable_node();\n\tVM_BUG_ON(is_stable_node_chain(dup));\n\tif (likely(chain)) {\n\t\tINIT_HLIST_HEAD(&chain->hlist);\n\t\tchain->chain_prune_time = jiffies;\n\t\tchain->rmap_hlist_len = STABLE_NODE_CHAIN;\n#if defined (CONFIG_DEBUG_VM) && defined(CONFIG_NUMA)\n\t\tchain->nid = NUMA_NO_NODE;  \n#endif\n\t\tksm_stable_node_chains++;\n\n\t\t \n\t\trb_replace_node(&dup->node, &chain->node, root);\n\n\t\t \n\t\tstable_node_chain_add_dup(dup, chain);\n\t}\n\treturn chain;\n}\n\nstatic inline void free_stable_node_chain(struct ksm_stable_node *chain,\n\t\t\t\t\t  struct rb_root *root)\n{\n\trb_erase(&chain->node, root);\n\tfree_stable_node(chain);\n\tksm_stable_node_chains--;\n}\n\nstatic void remove_node_from_stable_tree(struct ksm_stable_node *stable_node)\n{\n\tstruct ksm_rmap_item *rmap_item;\n\n\t \n\tBUG_ON(stable_node->rmap_hlist_len < 0);\n\n\thlist_for_each_entry(rmap_item, &stable_node->hlist, hlist) {\n\t\tif (rmap_item->hlist.next) {\n\t\t\tksm_pages_sharing--;\n\t\t\ttrace_ksm_remove_rmap_item(stable_node->kpfn, rmap_item, rmap_item->mm);\n\t\t} else {\n\t\t\tksm_pages_shared--;\n\t\t}\n\n\t\trmap_item->mm->ksm_merging_pages--;\n\n\t\tVM_BUG_ON(stable_node->rmap_hlist_len <= 0);\n\t\tstable_node->rmap_hlist_len--;\n\t\tput_anon_vma(rmap_item->anon_vma);\n\t\trmap_item->address &= PAGE_MASK;\n\t\tcond_resched();\n\t}\n\n\t \n\tBUILD_BUG_ON(STABLE_NODE_DUP_HEAD <= &migrate_nodes);\n\tBUILD_BUG_ON(STABLE_NODE_DUP_HEAD >= &migrate_nodes + 1);\n\n\ttrace_ksm_remove_ksm_page(stable_node->kpfn);\n\tif (stable_node->head == &migrate_nodes)\n\t\tlist_del(&stable_node->list);\n\telse\n\t\tstable_node_dup_del(stable_node);\n\tfree_stable_node(stable_node);\n}\n\nenum get_ksm_page_flags {\n\tGET_KSM_PAGE_NOLOCK,\n\tGET_KSM_PAGE_LOCK,\n\tGET_KSM_PAGE_TRYLOCK\n};\n\n \nstatic struct page *get_ksm_page(struct ksm_stable_node *stable_node,\n\t\t\t\t enum get_ksm_page_flags flags)\n{\n\tstruct page *page;\n\tvoid *expected_mapping;\n\tunsigned long kpfn;\n\n\texpected_mapping = (void *)((unsigned long)stable_node |\n\t\t\t\t\tPAGE_MAPPING_KSM);\nagain:\n\tkpfn = READ_ONCE(stable_node->kpfn);  \n\tpage = pfn_to_page(kpfn);\n\tif (READ_ONCE(page->mapping) != expected_mapping)\n\t\tgoto stale;\n\n\t \n\twhile (!get_page_unless_zero(page)) {\n\t\t \n\t\tif (!PageSwapCache(page))\n\t\t\tgoto stale;\n\t\tcpu_relax();\n\t}\n\n\tif (READ_ONCE(page->mapping) != expected_mapping) {\n\t\tput_page(page);\n\t\tgoto stale;\n\t}\n\n\tif (flags == GET_KSM_PAGE_TRYLOCK) {\n\t\tif (!trylock_page(page)) {\n\t\t\tput_page(page);\n\t\t\treturn ERR_PTR(-EBUSY);\n\t\t}\n\t} else if (flags == GET_KSM_PAGE_LOCK)\n\t\tlock_page(page);\n\n\tif (flags != GET_KSM_PAGE_NOLOCK) {\n\t\tif (READ_ONCE(page->mapping) != expected_mapping) {\n\t\t\tunlock_page(page);\n\t\t\tput_page(page);\n\t\t\tgoto stale;\n\t\t}\n\t}\n\treturn page;\n\nstale:\n\t \n\tsmp_rmb();\n\tif (READ_ONCE(stable_node->kpfn) != kpfn)\n\t\tgoto again;\n\tremove_node_from_stable_tree(stable_node);\n\treturn NULL;\n}\n\n \nstatic void remove_rmap_item_from_tree(struct ksm_rmap_item *rmap_item)\n{\n\tif (rmap_item->address & STABLE_FLAG) {\n\t\tstruct ksm_stable_node *stable_node;\n\t\tstruct page *page;\n\n\t\tstable_node = rmap_item->head;\n\t\tpage = get_ksm_page(stable_node, GET_KSM_PAGE_LOCK);\n\t\tif (!page)\n\t\t\tgoto out;\n\n\t\thlist_del(&rmap_item->hlist);\n\t\tunlock_page(page);\n\t\tput_page(page);\n\n\t\tif (!hlist_empty(&stable_node->hlist))\n\t\t\tksm_pages_sharing--;\n\t\telse\n\t\t\tksm_pages_shared--;\n\n\t\trmap_item->mm->ksm_merging_pages--;\n\n\t\tVM_BUG_ON(stable_node->rmap_hlist_len <= 0);\n\t\tstable_node->rmap_hlist_len--;\n\n\t\tput_anon_vma(rmap_item->anon_vma);\n\t\trmap_item->head = NULL;\n\t\trmap_item->address &= PAGE_MASK;\n\n\t} else if (rmap_item->address & UNSTABLE_FLAG) {\n\t\tunsigned char age;\n\t\t \n\t\tage = (unsigned char)(ksm_scan.seqnr - rmap_item->address);\n\t\tBUG_ON(age > 1);\n\t\tif (!age)\n\t\t\trb_erase(&rmap_item->node,\n\t\t\t\t root_unstable_tree + NUMA(rmap_item->nid));\n\t\tksm_pages_unshared--;\n\t\trmap_item->address &= PAGE_MASK;\n\t}\nout:\n\tcond_resched();\t\t \n}\n\nstatic void remove_trailing_rmap_items(struct ksm_rmap_item **rmap_list)\n{\n\twhile (*rmap_list) {\n\t\tstruct ksm_rmap_item *rmap_item = *rmap_list;\n\t\t*rmap_list = rmap_item->rmap_list;\n\t\tremove_rmap_item_from_tree(rmap_item);\n\t\tfree_rmap_item(rmap_item);\n\t}\n}\n\n \nstatic int unmerge_ksm_pages(struct vm_area_struct *vma,\n\t\t\t     unsigned long start, unsigned long end, bool lock_vma)\n{\n\tunsigned long addr;\n\tint err = 0;\n\n\tfor (addr = start; addr < end && !err; addr += PAGE_SIZE) {\n\t\tif (ksm_test_exit(vma->vm_mm))\n\t\t\tbreak;\n\t\tif (signal_pending(current))\n\t\t\terr = -ERESTARTSYS;\n\t\telse\n\t\t\terr = break_ksm(vma, addr, lock_vma);\n\t}\n\treturn err;\n}\n\nstatic inline struct ksm_stable_node *folio_stable_node(struct folio *folio)\n{\n\treturn folio_test_ksm(folio) ? folio_raw_mapping(folio) : NULL;\n}\n\nstatic inline struct ksm_stable_node *page_stable_node(struct page *page)\n{\n\treturn folio_stable_node(page_folio(page));\n}\n\nstatic inline void set_page_stable_node(struct page *page,\n\t\t\t\t\tstruct ksm_stable_node *stable_node)\n{\n\tVM_BUG_ON_PAGE(PageAnon(page) && PageAnonExclusive(page), page);\n\tpage->mapping = (void *)((unsigned long)stable_node | PAGE_MAPPING_KSM);\n}\n\n#ifdef CONFIG_SYSFS\n \nstatic int remove_stable_node(struct ksm_stable_node *stable_node)\n{\n\tstruct page *page;\n\tint err;\n\n\tpage = get_ksm_page(stable_node, GET_KSM_PAGE_LOCK);\n\tif (!page) {\n\t\t \n\t\treturn 0;\n\t}\n\n\t \n\terr = -EBUSY;\n\tif (!page_mapped(page)) {\n\t\t \n\t\tset_page_stable_node(page, NULL);\n\t\tremove_node_from_stable_tree(stable_node);\n\t\terr = 0;\n\t}\n\n\tunlock_page(page);\n\tput_page(page);\n\treturn err;\n}\n\nstatic int remove_stable_node_chain(struct ksm_stable_node *stable_node,\n\t\t\t\t    struct rb_root *root)\n{\n\tstruct ksm_stable_node *dup;\n\tstruct hlist_node *hlist_safe;\n\n\tif (!is_stable_node_chain(stable_node)) {\n\t\tVM_BUG_ON(is_stable_node_dup(stable_node));\n\t\tif (remove_stable_node(stable_node))\n\t\t\treturn true;\n\t\telse\n\t\t\treturn false;\n\t}\n\n\thlist_for_each_entry_safe(dup, hlist_safe,\n\t\t\t\t  &stable_node->hlist, hlist_dup) {\n\t\tVM_BUG_ON(!is_stable_node_dup(dup));\n\t\tif (remove_stable_node(dup))\n\t\t\treturn true;\n\t}\n\tBUG_ON(!hlist_empty(&stable_node->hlist));\n\tfree_stable_node_chain(stable_node, root);\n\treturn false;\n}\n\nstatic int remove_all_stable_nodes(void)\n{\n\tstruct ksm_stable_node *stable_node, *next;\n\tint nid;\n\tint err = 0;\n\n\tfor (nid = 0; nid < ksm_nr_node_ids; nid++) {\n\t\twhile (root_stable_tree[nid].rb_node) {\n\t\t\tstable_node = rb_entry(root_stable_tree[nid].rb_node,\n\t\t\t\t\t\tstruct ksm_stable_node, node);\n\t\t\tif (remove_stable_node_chain(stable_node,\n\t\t\t\t\t\t     root_stable_tree + nid)) {\n\t\t\t\terr = -EBUSY;\n\t\t\t\tbreak;\t \n\t\t\t}\n\t\t\tcond_resched();\n\t\t}\n\t}\n\tlist_for_each_entry_safe(stable_node, next, &migrate_nodes, list) {\n\t\tif (remove_stable_node(stable_node))\n\t\t\terr = -EBUSY;\n\t\tcond_resched();\n\t}\n\treturn err;\n}\n\nstatic int unmerge_and_remove_all_rmap_items(void)\n{\n\tstruct ksm_mm_slot *mm_slot;\n\tstruct mm_slot *slot;\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct *vma;\n\tint err = 0;\n\n\tspin_lock(&ksm_mmlist_lock);\n\tslot = list_entry(ksm_mm_head.slot.mm_node.next,\n\t\t\t  struct mm_slot, mm_node);\n\tksm_scan.mm_slot = mm_slot_entry(slot, struct ksm_mm_slot, slot);\n\tspin_unlock(&ksm_mmlist_lock);\n\n\tfor (mm_slot = ksm_scan.mm_slot; mm_slot != &ksm_mm_head;\n\t     mm_slot = ksm_scan.mm_slot) {\n\t\tVMA_ITERATOR(vmi, mm_slot->slot.mm, 0);\n\n\t\tmm = mm_slot->slot.mm;\n\t\tmmap_read_lock(mm);\n\n\t\t \n\t\tif (ksm_test_exit(mm))\n\t\t\tgoto mm_exiting;\n\n\t\tfor_each_vma(vmi, vma) {\n\t\t\tif (!(vma->vm_flags & VM_MERGEABLE) || !vma->anon_vma)\n\t\t\t\tcontinue;\n\t\t\terr = unmerge_ksm_pages(vma,\n\t\t\t\t\t\tvma->vm_start, vma->vm_end, false);\n\t\t\tif (err)\n\t\t\t\tgoto error;\n\t\t}\n\nmm_exiting:\n\t\tremove_trailing_rmap_items(&mm_slot->rmap_list);\n\t\tmmap_read_unlock(mm);\n\n\t\tspin_lock(&ksm_mmlist_lock);\n\t\tslot = list_entry(mm_slot->slot.mm_node.next,\n\t\t\t\t  struct mm_slot, mm_node);\n\t\tksm_scan.mm_slot = mm_slot_entry(slot, struct ksm_mm_slot, slot);\n\t\tif (ksm_test_exit(mm)) {\n\t\t\thash_del(&mm_slot->slot.hash);\n\t\t\tlist_del(&mm_slot->slot.mm_node);\n\t\t\tspin_unlock(&ksm_mmlist_lock);\n\n\t\t\tmm_slot_free(mm_slot_cache, mm_slot);\n\t\t\tclear_bit(MMF_VM_MERGEABLE, &mm->flags);\n\t\t\tclear_bit(MMF_VM_MERGE_ANY, &mm->flags);\n\t\t\tmmdrop(mm);\n\t\t} else\n\t\t\tspin_unlock(&ksm_mmlist_lock);\n\t}\n\n\t \n\tremove_all_stable_nodes();\n\tksm_scan.seqnr = 0;\n\treturn 0;\n\nerror:\n\tmmap_read_unlock(mm);\n\tspin_lock(&ksm_mmlist_lock);\n\tksm_scan.mm_slot = &ksm_mm_head;\n\tspin_unlock(&ksm_mmlist_lock);\n\treturn err;\n}\n#endif  \n\nstatic u32 calc_checksum(struct page *page)\n{\n\tu32 checksum;\n\tvoid *addr = kmap_atomic(page);\n\tchecksum = xxhash(addr, PAGE_SIZE, 0);\n\tkunmap_atomic(addr);\n\treturn checksum;\n}\n\nstatic int write_protect_page(struct vm_area_struct *vma, struct page *page,\n\t\t\t      pte_t *orig_pte)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tDEFINE_PAGE_VMA_WALK(pvmw, page, vma, 0, 0);\n\tint swapped;\n\tint err = -EFAULT;\n\tstruct mmu_notifier_range range;\n\tbool anon_exclusive;\n\tpte_t entry;\n\n\tpvmw.address = page_address_in_vma(page, vma);\n\tif (pvmw.address == -EFAULT)\n\t\tgoto out;\n\n\tBUG_ON(PageTransCompound(page));\n\n\tmmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, mm, pvmw.address,\n\t\t\t\tpvmw.address + PAGE_SIZE);\n\tmmu_notifier_invalidate_range_start(&range);\n\n\tif (!page_vma_mapped_walk(&pvmw))\n\t\tgoto out_mn;\n\tif (WARN_ONCE(!pvmw.pte, \"Unexpected PMD mapping?\"))\n\t\tgoto out_unlock;\n\n\tanon_exclusive = PageAnonExclusive(page);\n\tentry = ptep_get(pvmw.pte);\n\tif (pte_write(entry) || pte_dirty(entry) ||\n\t    anon_exclusive || mm_tlb_flush_pending(mm)) {\n\t\tswapped = PageSwapCache(page);\n\t\tflush_cache_page(vma, pvmw.address, page_to_pfn(page));\n\t\t \n\t\tentry = ptep_clear_flush(vma, pvmw.address, pvmw.pte);\n\t\t \n\t\tif (page_mapcount(page) + 1 + swapped != page_count(page)) {\n\t\t\tset_pte_at(mm, pvmw.address, pvmw.pte, entry);\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\t \n\t\tif (anon_exclusive && page_try_share_anon_rmap(page)) {\n\t\t\tset_pte_at(mm, pvmw.address, pvmw.pte, entry);\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tif (pte_dirty(entry))\n\t\t\tset_page_dirty(page);\n\t\tentry = pte_mkclean(entry);\n\n\t\tif (pte_write(entry))\n\t\t\tentry = pte_wrprotect(entry);\n\n\t\tset_pte_at_notify(mm, pvmw.address, pvmw.pte, entry);\n\t}\n\t*orig_pte = entry;\n\terr = 0;\n\nout_unlock:\n\tpage_vma_mapped_walk_done(&pvmw);\nout_mn:\n\tmmu_notifier_invalidate_range_end(&range);\nout:\n\treturn err;\n}\n\n \nstatic int replace_page(struct vm_area_struct *vma, struct page *page,\n\t\t\tstruct page *kpage, pte_t orig_pte)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct folio *folio;\n\tpmd_t *pmd;\n\tpmd_t pmde;\n\tpte_t *ptep;\n\tpte_t newpte;\n\tspinlock_t *ptl;\n\tunsigned long addr;\n\tint err = -EFAULT;\n\tstruct mmu_notifier_range range;\n\n\taddr = page_address_in_vma(page, vma);\n\tif (addr == -EFAULT)\n\t\tgoto out;\n\n\tpmd = mm_find_pmd(mm, addr);\n\tif (!pmd)\n\t\tgoto out;\n\t \n\tpmde = pmdp_get_lockless(pmd);\n\tif (!pmd_present(pmde) || pmd_trans_huge(pmde))\n\t\tgoto out;\n\n\tmmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, mm, addr,\n\t\t\t\taddr + PAGE_SIZE);\n\tmmu_notifier_invalidate_range_start(&range);\n\n\tptep = pte_offset_map_lock(mm, pmd, addr, &ptl);\n\tif (!ptep)\n\t\tgoto out_mn;\n\tif (!pte_same(ptep_get(ptep), orig_pte)) {\n\t\tpte_unmap_unlock(ptep, ptl);\n\t\tgoto out_mn;\n\t}\n\tVM_BUG_ON_PAGE(PageAnonExclusive(page), page);\n\tVM_BUG_ON_PAGE(PageAnon(kpage) && PageAnonExclusive(kpage), kpage);\n\n\t \n\tif (!is_zero_pfn(page_to_pfn(kpage))) {\n\t\tget_page(kpage);\n\t\tpage_add_anon_rmap(kpage, vma, addr, RMAP_NONE);\n\t\tnewpte = mk_pte(kpage, vma->vm_page_prot);\n\t} else {\n\t\t \n\t\tnewpte = pte_mkdirty(pte_mkspecial(pfn_pte(page_to_pfn(kpage), vma->vm_page_prot)));\n\t\tksm_zero_pages++;\n\t\tmm->ksm_zero_pages++;\n\t\t \n\t\tdec_mm_counter(mm, MM_ANONPAGES);\n\t}\n\n\tflush_cache_page(vma, addr, pte_pfn(ptep_get(ptep)));\n\t \n\tptep_clear_flush(vma, addr, ptep);\n\tset_pte_at_notify(mm, addr, ptep, newpte);\n\n\tfolio = page_folio(page);\n\tpage_remove_rmap(page, vma, false);\n\tif (!folio_mapped(folio))\n\t\tfolio_free_swap(folio);\n\tfolio_put(folio);\n\n\tpte_unmap_unlock(ptep, ptl);\n\terr = 0;\nout_mn:\n\tmmu_notifier_invalidate_range_end(&range);\nout:\n\treturn err;\n}\n\n \nstatic int try_to_merge_one_page(struct vm_area_struct *vma,\n\t\t\t\t struct page *page, struct page *kpage)\n{\n\tpte_t orig_pte = __pte(0);\n\tint err = -EFAULT;\n\n\tif (page == kpage)\t\t\t \n\t\treturn 0;\n\n\tif (!PageAnon(page))\n\t\tgoto out;\n\n\t \n\tif (!trylock_page(page))\n\t\tgoto out;\n\n\tif (PageTransCompound(page)) {\n\t\tif (split_huge_page(page))\n\t\t\tgoto out_unlock;\n\t}\n\n\t \n\tif (write_protect_page(vma, page, &orig_pte) == 0) {\n\t\tif (!kpage) {\n\t\t\t \n\t\t\tset_page_stable_node(page, NULL);\n\t\t\tmark_page_accessed(page);\n\t\t\t \n\t\t\tif (!PageDirty(page))\n\t\t\t\tSetPageDirty(page);\n\t\t\terr = 0;\n\t\t} else if (pages_identical(page, kpage))\n\t\t\terr = replace_page(vma, page, kpage, orig_pte);\n\t}\n\nout_unlock:\n\tunlock_page(page);\nout:\n\treturn err;\n}\n\n \nstatic int try_to_merge_with_ksm_page(struct ksm_rmap_item *rmap_item,\n\t\t\t\t      struct page *page, struct page *kpage)\n{\n\tstruct mm_struct *mm = rmap_item->mm;\n\tstruct vm_area_struct *vma;\n\tint err = -EFAULT;\n\n\tmmap_read_lock(mm);\n\tvma = find_mergeable_vma(mm, rmap_item->address);\n\tif (!vma)\n\t\tgoto out;\n\n\terr = try_to_merge_one_page(vma, page, kpage);\n\tif (err)\n\t\tgoto out;\n\n\t \n\tremove_rmap_item_from_tree(rmap_item);\n\n\t \n\trmap_item->anon_vma = vma->anon_vma;\n\tget_anon_vma(vma->anon_vma);\nout:\n\tmmap_read_unlock(mm);\n\ttrace_ksm_merge_with_ksm_page(kpage, page_to_pfn(kpage ? kpage : page),\n\t\t\t\trmap_item, mm, err);\n\treturn err;\n}\n\n \nstatic struct page *try_to_merge_two_pages(struct ksm_rmap_item *rmap_item,\n\t\t\t\t\t   struct page *page,\n\t\t\t\t\t   struct ksm_rmap_item *tree_rmap_item,\n\t\t\t\t\t   struct page *tree_page)\n{\n\tint err;\n\n\terr = try_to_merge_with_ksm_page(rmap_item, page, NULL);\n\tif (!err) {\n\t\terr = try_to_merge_with_ksm_page(tree_rmap_item,\n\t\t\t\t\t\t\ttree_page, page);\n\t\t \n\t\tif (err)\n\t\t\tbreak_cow(rmap_item);\n\t}\n\treturn err ? NULL : page;\n}\n\nstatic __always_inline\nbool __is_page_sharing_candidate(struct ksm_stable_node *stable_node, int offset)\n{\n\tVM_BUG_ON(stable_node->rmap_hlist_len < 0);\n\t \n\treturn stable_node->rmap_hlist_len &&\n\t\tstable_node->rmap_hlist_len + offset < ksm_max_page_sharing;\n}\n\nstatic __always_inline\nbool is_page_sharing_candidate(struct ksm_stable_node *stable_node)\n{\n\treturn __is_page_sharing_candidate(stable_node, 0);\n}\n\nstatic struct page *stable_node_dup(struct ksm_stable_node **_stable_node_dup,\n\t\t\t\t    struct ksm_stable_node **_stable_node,\n\t\t\t\t    struct rb_root *root,\n\t\t\t\t    bool prune_stale_stable_nodes)\n{\n\tstruct ksm_stable_node *dup, *found = NULL, *stable_node = *_stable_node;\n\tstruct hlist_node *hlist_safe;\n\tstruct page *_tree_page, *tree_page = NULL;\n\tint nr = 0;\n\tint found_rmap_hlist_len;\n\n\tif (!prune_stale_stable_nodes ||\n\t    time_before(jiffies, stable_node->chain_prune_time +\n\t\t\tmsecs_to_jiffies(\n\t\t\t\tksm_stable_node_chains_prune_millisecs)))\n\t\tprune_stale_stable_nodes = false;\n\telse\n\t\tstable_node->chain_prune_time = jiffies;\n\n\thlist_for_each_entry_safe(dup, hlist_safe,\n\t\t\t\t  &stable_node->hlist, hlist_dup) {\n\t\tcond_resched();\n\t\t \n\t\t_tree_page = get_ksm_page(dup, GET_KSM_PAGE_NOLOCK);\n\t\tif (!_tree_page)\n\t\t\tcontinue;\n\t\tnr += 1;\n\t\tif (is_page_sharing_candidate(dup)) {\n\t\t\tif (!found ||\n\t\t\t    dup->rmap_hlist_len > found_rmap_hlist_len) {\n\t\t\t\tif (found)\n\t\t\t\t\tput_page(tree_page);\n\t\t\t\tfound = dup;\n\t\t\t\tfound_rmap_hlist_len = found->rmap_hlist_len;\n\t\t\t\ttree_page = _tree_page;\n\n\t\t\t\t \n\t\t\t\tif (!prune_stale_stable_nodes)\n\t\t\t\t\tbreak;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\t\tput_page(_tree_page);\n\t}\n\n\tif (found) {\n\t\t \n\t\tif (prune_stale_stable_nodes && nr == 1) {\n\t\t\t \n\t\t\tBUG_ON(stable_node->hlist.first->next);\n\n\t\t\t \n\t\t\trb_replace_node(&stable_node->node, &found->node,\n\t\t\t\t\troot);\n\t\t\tfree_stable_node(stable_node);\n\t\t\tksm_stable_node_chains--;\n\t\t\tksm_stable_node_dups--;\n\t\t\t \n\t\t\t*_stable_node = found;\n\t\t\t \n\t\t\tstable_node = NULL;\n\t\t} else if (stable_node->hlist.first != &found->hlist_dup &&\n\t\t\t   __is_page_sharing_candidate(found, 1)) {\n\t\t\t \n\t\t\thlist_del(&found->hlist_dup);\n\t\t\thlist_add_head(&found->hlist_dup,\n\t\t\t\t       &stable_node->hlist);\n\t\t}\n\t}\n\n\t*_stable_node_dup = found;\n\treturn tree_page;\n}\n\nstatic struct ksm_stable_node *stable_node_dup_any(struct ksm_stable_node *stable_node,\n\t\t\t\t\t       struct rb_root *root)\n{\n\tif (!is_stable_node_chain(stable_node))\n\t\treturn stable_node;\n\tif (hlist_empty(&stable_node->hlist)) {\n\t\tfree_stable_node_chain(stable_node, root);\n\t\treturn NULL;\n\t}\n\treturn hlist_entry(stable_node->hlist.first,\n\t\t\t   typeof(*stable_node), hlist_dup);\n}\n\n \nstatic struct page *__stable_node_chain(struct ksm_stable_node **_stable_node_dup,\n\t\t\t\t\tstruct ksm_stable_node **_stable_node,\n\t\t\t\t\tstruct rb_root *root,\n\t\t\t\t\tbool prune_stale_stable_nodes)\n{\n\tstruct ksm_stable_node *stable_node = *_stable_node;\n\tif (!is_stable_node_chain(stable_node)) {\n\t\tif (is_page_sharing_candidate(stable_node)) {\n\t\t\t*_stable_node_dup = stable_node;\n\t\t\treturn get_ksm_page(stable_node, GET_KSM_PAGE_NOLOCK);\n\t\t}\n\t\t \n\t\t*_stable_node_dup = NULL;\n\t\treturn NULL;\n\t}\n\treturn stable_node_dup(_stable_node_dup, _stable_node, root,\n\t\t\t       prune_stale_stable_nodes);\n}\n\nstatic __always_inline struct page *chain_prune(struct ksm_stable_node **s_n_d,\n\t\t\t\t\t\tstruct ksm_stable_node **s_n,\n\t\t\t\t\t\tstruct rb_root *root)\n{\n\treturn __stable_node_chain(s_n_d, s_n, root, true);\n}\n\nstatic __always_inline struct page *chain(struct ksm_stable_node **s_n_d,\n\t\t\t\t\t  struct ksm_stable_node *s_n,\n\t\t\t\t\t  struct rb_root *root)\n{\n\tstruct ksm_stable_node *old_stable_node = s_n;\n\tstruct page *tree_page;\n\n\ttree_page = __stable_node_chain(s_n_d, &s_n, root, false);\n\t \n\tVM_BUG_ON(s_n != old_stable_node);\n\treturn tree_page;\n}\n\n \nstatic struct page *stable_tree_search(struct page *page)\n{\n\tint nid;\n\tstruct rb_root *root;\n\tstruct rb_node **new;\n\tstruct rb_node *parent;\n\tstruct ksm_stable_node *stable_node, *stable_node_dup, *stable_node_any;\n\tstruct ksm_stable_node *page_node;\n\n\tpage_node = page_stable_node(page);\n\tif (page_node && page_node->head != &migrate_nodes) {\n\t\t \n\t\tget_page(page);\n\t\treturn page;\n\t}\n\n\tnid = get_kpfn_nid(page_to_pfn(page));\n\troot = root_stable_tree + nid;\nagain:\n\tnew = &root->rb_node;\n\tparent = NULL;\n\n\twhile (*new) {\n\t\tstruct page *tree_page;\n\t\tint ret;\n\n\t\tcond_resched();\n\t\tstable_node = rb_entry(*new, struct ksm_stable_node, node);\n\t\tstable_node_any = NULL;\n\t\ttree_page = chain_prune(&stable_node_dup, &stable_node,\troot);\n\t\t \n\t\tif (!stable_node_dup) {\n\t\t\t \n\t\t\tstable_node_any = stable_node_dup_any(stable_node,\n\t\t\t\t\t\t\t      root);\n\t\t\tif (!stable_node_any) {\n\t\t\t\t \n\t\t\t\tgoto again;\n\t\t\t}\n\t\t\t \n\t\t\ttree_page = get_ksm_page(stable_node_any,\n\t\t\t\t\t\t GET_KSM_PAGE_NOLOCK);\n\t\t}\n\t\tVM_BUG_ON(!stable_node_dup ^ !!stable_node_any);\n\t\tif (!tree_page) {\n\t\t\t \n\t\t\tgoto again;\n\t\t}\n\n\t\tret = memcmp_pages(page, tree_page);\n\t\tput_page(tree_page);\n\n\t\tparent = *new;\n\t\tif (ret < 0)\n\t\t\tnew = &parent->rb_left;\n\t\telse if (ret > 0)\n\t\t\tnew = &parent->rb_right;\n\t\telse {\n\t\t\tif (page_node) {\n\t\t\t\tVM_BUG_ON(page_node->head != &migrate_nodes);\n\t\t\t\t \n\t\t\t\tif (page_mapcount(page) > 1)\n\t\t\t\t\tgoto chain_append;\n\t\t\t}\n\n\t\t\tif (!stable_node_dup) {\n\t\t\t\t \n\t\t\t\treturn NULL;\n\t\t\t}\n\n\t\t\t \n\t\t\ttree_page = get_ksm_page(stable_node_dup,\n\t\t\t\t\t\t GET_KSM_PAGE_TRYLOCK);\n\n\t\t\tif (PTR_ERR(tree_page) == -EBUSY)\n\t\t\t\treturn ERR_PTR(-EBUSY);\n\n\t\t\tif (unlikely(!tree_page))\n\t\t\t\t \n\t\t\t\tgoto again;\n\t\t\tunlock_page(tree_page);\n\n\t\t\tif (get_kpfn_nid(stable_node_dup->kpfn) !=\n\t\t\t    NUMA(stable_node_dup->nid)) {\n\t\t\t\tput_page(tree_page);\n\t\t\t\tgoto replace;\n\t\t\t}\n\t\t\treturn tree_page;\n\t\t}\n\t}\n\n\tif (!page_node)\n\t\treturn NULL;\n\n\tlist_del(&page_node->list);\n\tDO_NUMA(page_node->nid = nid);\n\trb_link_node(&page_node->node, parent, new);\n\trb_insert_color(&page_node->node, root);\nout:\n\tif (is_page_sharing_candidate(page_node)) {\n\t\tget_page(page);\n\t\treturn page;\n\t} else\n\t\treturn NULL;\n\nreplace:\n\t \n\tif (stable_node_dup == stable_node) {\n\t\tVM_BUG_ON(is_stable_node_chain(stable_node_dup));\n\t\tVM_BUG_ON(is_stable_node_dup(stable_node_dup));\n\t\t \n\t\tif (page_node) {\n\t\t\tVM_BUG_ON(page_node->head != &migrate_nodes);\n\t\t\tlist_del(&page_node->list);\n\t\t\tDO_NUMA(page_node->nid = nid);\n\t\t\trb_replace_node(&stable_node_dup->node,\n\t\t\t\t\t&page_node->node,\n\t\t\t\t\troot);\n\t\t\tif (is_page_sharing_candidate(page_node))\n\t\t\t\tget_page(page);\n\t\t\telse\n\t\t\t\tpage = NULL;\n\t\t} else {\n\t\t\trb_erase(&stable_node_dup->node, root);\n\t\t\tpage = NULL;\n\t\t}\n\t} else {\n\t\tVM_BUG_ON(!is_stable_node_chain(stable_node));\n\t\t__stable_node_dup_del(stable_node_dup);\n\t\tif (page_node) {\n\t\t\tVM_BUG_ON(page_node->head != &migrate_nodes);\n\t\t\tlist_del(&page_node->list);\n\t\t\tDO_NUMA(page_node->nid = nid);\n\t\t\tstable_node_chain_add_dup(page_node, stable_node);\n\t\t\tif (is_page_sharing_candidate(page_node))\n\t\t\t\tget_page(page);\n\t\t\telse\n\t\t\t\tpage = NULL;\n\t\t} else {\n\t\t\tpage = NULL;\n\t\t}\n\t}\n\tstable_node_dup->head = &migrate_nodes;\n\tlist_add(&stable_node_dup->list, stable_node_dup->head);\n\treturn page;\n\nchain_append:\n\t \n\tif (!stable_node_dup)\n\t\tstable_node_dup = stable_node_any;\n\t \n\tif (stable_node_dup == stable_node) {\n\t\tVM_BUG_ON(is_stable_node_dup(stable_node_dup));\n\t\t \n\t\tstable_node = alloc_stable_node_chain(stable_node_dup,\n\t\t\t\t\t\t      root);\n\t\tif (!stable_node)\n\t\t\treturn NULL;\n\t}\n\t \n\tVM_BUG_ON(!is_stable_node_dup(stable_node_dup));\n\tVM_BUG_ON(page_node->head != &migrate_nodes);\n\tlist_del(&page_node->list);\n\tDO_NUMA(page_node->nid = nid);\n\tstable_node_chain_add_dup(page_node, stable_node);\n\tgoto out;\n}\n\n \nstatic struct ksm_stable_node *stable_tree_insert(struct page *kpage)\n{\n\tint nid;\n\tunsigned long kpfn;\n\tstruct rb_root *root;\n\tstruct rb_node **new;\n\tstruct rb_node *parent;\n\tstruct ksm_stable_node *stable_node, *stable_node_dup, *stable_node_any;\n\tbool need_chain = false;\n\n\tkpfn = page_to_pfn(kpage);\n\tnid = get_kpfn_nid(kpfn);\n\troot = root_stable_tree + nid;\nagain:\n\tparent = NULL;\n\tnew = &root->rb_node;\n\n\twhile (*new) {\n\t\tstruct page *tree_page;\n\t\tint ret;\n\n\t\tcond_resched();\n\t\tstable_node = rb_entry(*new, struct ksm_stable_node, node);\n\t\tstable_node_any = NULL;\n\t\ttree_page = chain(&stable_node_dup, stable_node, root);\n\t\tif (!stable_node_dup) {\n\t\t\t \n\t\t\tstable_node_any = stable_node_dup_any(stable_node,\n\t\t\t\t\t\t\t      root);\n\t\t\tif (!stable_node_any) {\n\t\t\t\t \n\t\t\t\tgoto again;\n\t\t\t}\n\t\t\t \n\t\t\ttree_page = get_ksm_page(stable_node_any,\n\t\t\t\t\t\t GET_KSM_PAGE_NOLOCK);\n\t\t}\n\t\tVM_BUG_ON(!stable_node_dup ^ !!stable_node_any);\n\t\tif (!tree_page) {\n\t\t\t \n\t\t\tgoto again;\n\t\t}\n\n\t\tret = memcmp_pages(kpage, tree_page);\n\t\tput_page(tree_page);\n\n\t\tparent = *new;\n\t\tif (ret < 0)\n\t\t\tnew = &parent->rb_left;\n\t\telse if (ret > 0)\n\t\t\tnew = &parent->rb_right;\n\t\telse {\n\t\t\tneed_chain = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tstable_node_dup = alloc_stable_node();\n\tif (!stable_node_dup)\n\t\treturn NULL;\n\n\tINIT_HLIST_HEAD(&stable_node_dup->hlist);\n\tstable_node_dup->kpfn = kpfn;\n\tset_page_stable_node(kpage, stable_node_dup);\n\tstable_node_dup->rmap_hlist_len = 0;\n\tDO_NUMA(stable_node_dup->nid = nid);\n\tif (!need_chain) {\n\t\trb_link_node(&stable_node_dup->node, parent, new);\n\t\trb_insert_color(&stable_node_dup->node, root);\n\t} else {\n\t\tif (!is_stable_node_chain(stable_node)) {\n\t\t\tstruct ksm_stable_node *orig = stable_node;\n\t\t\t \n\t\t\tstable_node = alloc_stable_node_chain(orig, root);\n\t\t\tif (!stable_node) {\n\t\t\t\tfree_stable_node(stable_node_dup);\n\t\t\t\treturn NULL;\n\t\t\t}\n\t\t}\n\t\tstable_node_chain_add_dup(stable_node_dup, stable_node);\n\t}\n\n\treturn stable_node_dup;\n}\n\n \nstatic\nstruct ksm_rmap_item *unstable_tree_search_insert(struct ksm_rmap_item *rmap_item,\n\t\t\t\t\t      struct page *page,\n\t\t\t\t\t      struct page **tree_pagep)\n{\n\tstruct rb_node **new;\n\tstruct rb_root *root;\n\tstruct rb_node *parent = NULL;\n\tint nid;\n\n\tnid = get_kpfn_nid(page_to_pfn(page));\n\troot = root_unstable_tree + nid;\n\tnew = &root->rb_node;\n\n\twhile (*new) {\n\t\tstruct ksm_rmap_item *tree_rmap_item;\n\t\tstruct page *tree_page;\n\t\tint ret;\n\n\t\tcond_resched();\n\t\ttree_rmap_item = rb_entry(*new, struct ksm_rmap_item, node);\n\t\ttree_page = get_mergeable_page(tree_rmap_item);\n\t\tif (!tree_page)\n\t\t\treturn NULL;\n\n\t\t \n\t\tif (page == tree_page) {\n\t\t\tput_page(tree_page);\n\t\t\treturn NULL;\n\t\t}\n\n\t\tret = memcmp_pages(page, tree_page);\n\n\t\tparent = *new;\n\t\tif (ret < 0) {\n\t\t\tput_page(tree_page);\n\t\t\tnew = &parent->rb_left;\n\t\t} else if (ret > 0) {\n\t\t\tput_page(tree_page);\n\t\t\tnew = &parent->rb_right;\n\t\t} else if (!ksm_merge_across_nodes &&\n\t\t\t   page_to_nid(tree_page) != nid) {\n\t\t\t \n\t\t\tput_page(tree_page);\n\t\t\treturn NULL;\n\t\t} else {\n\t\t\t*tree_pagep = tree_page;\n\t\t\treturn tree_rmap_item;\n\t\t}\n\t}\n\n\trmap_item->address |= UNSTABLE_FLAG;\n\trmap_item->address |= (ksm_scan.seqnr & SEQNR_MASK);\n\tDO_NUMA(rmap_item->nid = nid);\n\trb_link_node(&rmap_item->node, parent, new);\n\trb_insert_color(&rmap_item->node, root);\n\n\tksm_pages_unshared++;\n\treturn NULL;\n}\n\n \nstatic void stable_tree_append(struct ksm_rmap_item *rmap_item,\n\t\t\t       struct ksm_stable_node *stable_node,\n\t\t\t       bool max_page_sharing_bypass)\n{\n\t \n\tBUG_ON(stable_node->rmap_hlist_len < 0);\n\n\tstable_node->rmap_hlist_len++;\n\tif (!max_page_sharing_bypass)\n\t\t \n\t\tWARN_ON_ONCE(stable_node->rmap_hlist_len >\n\t\t\t     ksm_max_page_sharing);\n\n\trmap_item->head = stable_node;\n\trmap_item->address |= STABLE_FLAG;\n\thlist_add_head(&rmap_item->hlist, &stable_node->hlist);\n\n\tif (rmap_item->hlist.next)\n\t\tksm_pages_sharing++;\n\telse\n\t\tksm_pages_shared++;\n\n\trmap_item->mm->ksm_merging_pages++;\n}\n\n \nstatic void cmp_and_merge_page(struct page *page, struct ksm_rmap_item *rmap_item)\n{\n\tstruct mm_struct *mm = rmap_item->mm;\n\tstruct ksm_rmap_item *tree_rmap_item;\n\tstruct page *tree_page = NULL;\n\tstruct ksm_stable_node *stable_node;\n\tstruct page *kpage;\n\tunsigned int checksum;\n\tint err;\n\tbool max_page_sharing_bypass = false;\n\n\tstable_node = page_stable_node(page);\n\tif (stable_node) {\n\t\tif (stable_node->head != &migrate_nodes &&\n\t\t    get_kpfn_nid(READ_ONCE(stable_node->kpfn)) !=\n\t\t    NUMA(stable_node->nid)) {\n\t\t\tstable_node_dup_del(stable_node);\n\t\t\tstable_node->head = &migrate_nodes;\n\t\t\tlist_add(&stable_node->list, stable_node->head);\n\t\t}\n\t\tif (stable_node->head != &migrate_nodes &&\n\t\t    rmap_item->head == stable_node)\n\t\t\treturn;\n\t\t \n\t\tif (!is_page_sharing_candidate(stable_node))\n\t\t\tmax_page_sharing_bypass = true;\n\t}\n\n\t \n\tkpage = stable_tree_search(page);\n\tif (kpage == page && rmap_item->head == stable_node) {\n\t\tput_page(kpage);\n\t\treturn;\n\t}\n\n\tremove_rmap_item_from_tree(rmap_item);\n\n\tif (kpage) {\n\t\tif (PTR_ERR(kpage) == -EBUSY)\n\t\t\treturn;\n\n\t\terr = try_to_merge_with_ksm_page(rmap_item, page, kpage);\n\t\tif (!err) {\n\t\t\t \n\t\t\tlock_page(kpage);\n\t\t\tstable_tree_append(rmap_item, page_stable_node(kpage),\n\t\t\t\t\t   max_page_sharing_bypass);\n\t\t\tunlock_page(kpage);\n\t\t}\n\t\tput_page(kpage);\n\t\treturn;\n\t}\n\n\t \n\tchecksum = calc_checksum(page);\n\tif (rmap_item->oldchecksum != checksum) {\n\t\trmap_item->oldchecksum = checksum;\n\t\treturn;\n\t}\n\n\t \n\tif (ksm_use_zero_pages && (checksum == zero_checksum)) {\n\t\tstruct vm_area_struct *vma;\n\n\t\tmmap_read_lock(mm);\n\t\tvma = find_mergeable_vma(mm, rmap_item->address);\n\t\tif (vma) {\n\t\t\terr = try_to_merge_one_page(vma, page,\n\t\t\t\t\tZERO_PAGE(rmap_item->address));\n\t\t\ttrace_ksm_merge_one_page(\n\t\t\t\tpage_to_pfn(ZERO_PAGE(rmap_item->address)),\n\t\t\t\trmap_item, mm, err);\n\t\t} else {\n\t\t\t \n\t\t\terr = 0;\n\t\t}\n\t\tmmap_read_unlock(mm);\n\t\t \n\t\tif (!err)\n\t\t\treturn;\n\t}\n\ttree_rmap_item =\n\t\tunstable_tree_search_insert(rmap_item, page, &tree_page);\n\tif (tree_rmap_item) {\n\t\tbool split;\n\n\t\tkpage = try_to_merge_two_pages(rmap_item, page,\n\t\t\t\t\t\ttree_rmap_item, tree_page);\n\t\t \n\t\tsplit = PageTransCompound(page)\n\t\t\t&& compound_head(page) == compound_head(tree_page);\n\t\tput_page(tree_page);\n\t\tif (kpage) {\n\t\t\t \n\t\t\tlock_page(kpage);\n\t\t\tstable_node = stable_tree_insert(kpage);\n\t\t\tif (stable_node) {\n\t\t\t\tstable_tree_append(tree_rmap_item, stable_node,\n\t\t\t\t\t\t   false);\n\t\t\t\tstable_tree_append(rmap_item, stable_node,\n\t\t\t\t\t\t   false);\n\t\t\t}\n\t\t\tunlock_page(kpage);\n\n\t\t\t \n\t\t\tif (!stable_node) {\n\t\t\t\tbreak_cow(tree_rmap_item);\n\t\t\t\tbreak_cow(rmap_item);\n\t\t\t}\n\t\t} else if (split) {\n\t\t\t \n\t\t\tif (!trylock_page(page))\n\t\t\t\treturn;\n\t\t\tsplit_huge_page(page);\n\t\t\tunlock_page(page);\n\t\t}\n\t}\n}\n\nstatic struct ksm_rmap_item *get_next_rmap_item(struct ksm_mm_slot *mm_slot,\n\t\t\t\t\t    struct ksm_rmap_item **rmap_list,\n\t\t\t\t\t    unsigned long addr)\n{\n\tstruct ksm_rmap_item *rmap_item;\n\n\twhile (*rmap_list) {\n\t\trmap_item = *rmap_list;\n\t\tif ((rmap_item->address & PAGE_MASK) == addr)\n\t\t\treturn rmap_item;\n\t\tif (rmap_item->address > addr)\n\t\t\tbreak;\n\t\t*rmap_list = rmap_item->rmap_list;\n\t\tremove_rmap_item_from_tree(rmap_item);\n\t\tfree_rmap_item(rmap_item);\n\t}\n\n\trmap_item = alloc_rmap_item();\n\tif (rmap_item) {\n\t\t \n\t\trmap_item->mm = mm_slot->slot.mm;\n\t\trmap_item->mm->ksm_rmap_items++;\n\t\trmap_item->address = addr;\n\t\trmap_item->rmap_list = *rmap_list;\n\t\t*rmap_list = rmap_item;\n\t}\n\treturn rmap_item;\n}\n\nstatic struct ksm_rmap_item *scan_get_next_rmap_item(struct page **page)\n{\n\tstruct mm_struct *mm;\n\tstruct ksm_mm_slot *mm_slot;\n\tstruct mm_slot *slot;\n\tstruct vm_area_struct *vma;\n\tstruct ksm_rmap_item *rmap_item;\n\tstruct vma_iterator vmi;\n\tint nid;\n\n\tif (list_empty(&ksm_mm_head.slot.mm_node))\n\t\treturn NULL;\n\n\tmm_slot = ksm_scan.mm_slot;\n\tif (mm_slot == &ksm_mm_head) {\n\t\ttrace_ksm_start_scan(ksm_scan.seqnr, ksm_rmap_items);\n\n\t\t \n\t\tlru_add_drain_all();\n\n\t\t \n\t\tif (!ksm_merge_across_nodes) {\n\t\t\tstruct ksm_stable_node *stable_node, *next;\n\t\t\tstruct page *page;\n\n\t\t\tlist_for_each_entry_safe(stable_node, next,\n\t\t\t\t\t\t &migrate_nodes, list) {\n\t\t\t\tpage = get_ksm_page(stable_node,\n\t\t\t\t\t\t    GET_KSM_PAGE_NOLOCK);\n\t\t\t\tif (page)\n\t\t\t\t\tput_page(page);\n\t\t\t\tcond_resched();\n\t\t\t}\n\t\t}\n\n\t\tfor (nid = 0; nid < ksm_nr_node_ids; nid++)\n\t\t\troot_unstable_tree[nid] = RB_ROOT;\n\n\t\tspin_lock(&ksm_mmlist_lock);\n\t\tslot = list_entry(mm_slot->slot.mm_node.next,\n\t\t\t\t  struct mm_slot, mm_node);\n\t\tmm_slot = mm_slot_entry(slot, struct ksm_mm_slot, slot);\n\t\tksm_scan.mm_slot = mm_slot;\n\t\tspin_unlock(&ksm_mmlist_lock);\n\t\t \n\t\tif (mm_slot == &ksm_mm_head)\n\t\t\treturn NULL;\nnext_mm:\n\t\tksm_scan.address = 0;\n\t\tksm_scan.rmap_list = &mm_slot->rmap_list;\n\t}\n\n\tslot = &mm_slot->slot;\n\tmm = slot->mm;\n\tvma_iter_init(&vmi, mm, ksm_scan.address);\n\n\tmmap_read_lock(mm);\n\tif (ksm_test_exit(mm))\n\t\tgoto no_vmas;\n\n\tfor_each_vma(vmi, vma) {\n\t\tif (!(vma->vm_flags & VM_MERGEABLE))\n\t\t\tcontinue;\n\t\tif (ksm_scan.address < vma->vm_start)\n\t\t\tksm_scan.address = vma->vm_start;\n\t\tif (!vma->anon_vma)\n\t\t\tksm_scan.address = vma->vm_end;\n\n\t\twhile (ksm_scan.address < vma->vm_end) {\n\t\t\tif (ksm_test_exit(mm))\n\t\t\t\tbreak;\n\t\t\t*page = follow_page(vma, ksm_scan.address, FOLL_GET);\n\t\t\tif (IS_ERR_OR_NULL(*page)) {\n\t\t\t\tksm_scan.address += PAGE_SIZE;\n\t\t\t\tcond_resched();\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (is_zone_device_page(*page))\n\t\t\t\tgoto next_page;\n\t\t\tif (PageAnon(*page)) {\n\t\t\t\tflush_anon_page(vma, *page, ksm_scan.address);\n\t\t\t\tflush_dcache_page(*page);\n\t\t\t\trmap_item = get_next_rmap_item(mm_slot,\n\t\t\t\t\tksm_scan.rmap_list, ksm_scan.address);\n\t\t\t\tif (rmap_item) {\n\t\t\t\t\tksm_scan.rmap_list =\n\t\t\t\t\t\t\t&rmap_item->rmap_list;\n\t\t\t\t\tksm_scan.address += PAGE_SIZE;\n\t\t\t\t} else\n\t\t\t\t\tput_page(*page);\n\t\t\t\tmmap_read_unlock(mm);\n\t\t\t\treturn rmap_item;\n\t\t\t}\nnext_page:\n\t\t\tput_page(*page);\n\t\t\tksm_scan.address += PAGE_SIZE;\n\t\t\tcond_resched();\n\t\t}\n\t}\n\n\tif (ksm_test_exit(mm)) {\nno_vmas:\n\t\tksm_scan.address = 0;\n\t\tksm_scan.rmap_list = &mm_slot->rmap_list;\n\t}\n\t \n\tremove_trailing_rmap_items(ksm_scan.rmap_list);\n\n\tspin_lock(&ksm_mmlist_lock);\n\tslot = list_entry(mm_slot->slot.mm_node.next,\n\t\t\t  struct mm_slot, mm_node);\n\tksm_scan.mm_slot = mm_slot_entry(slot, struct ksm_mm_slot, slot);\n\tif (ksm_scan.address == 0) {\n\t\t \n\t\thash_del(&mm_slot->slot.hash);\n\t\tlist_del(&mm_slot->slot.mm_node);\n\t\tspin_unlock(&ksm_mmlist_lock);\n\n\t\tmm_slot_free(mm_slot_cache, mm_slot);\n\t\tclear_bit(MMF_VM_MERGEABLE, &mm->flags);\n\t\tclear_bit(MMF_VM_MERGE_ANY, &mm->flags);\n\t\tmmap_read_unlock(mm);\n\t\tmmdrop(mm);\n\t} else {\n\t\tmmap_read_unlock(mm);\n\t\t \n\t\tspin_unlock(&ksm_mmlist_lock);\n\t}\n\n\t \n\tmm_slot = ksm_scan.mm_slot;\n\tif (mm_slot != &ksm_mm_head)\n\t\tgoto next_mm;\n\n\ttrace_ksm_stop_scan(ksm_scan.seqnr, ksm_rmap_items);\n\tksm_scan.seqnr++;\n\treturn NULL;\n}\n\n \nstatic void ksm_do_scan(unsigned int scan_npages)\n{\n\tstruct ksm_rmap_item *rmap_item;\n\tstruct page *page;\n\tunsigned int npages = scan_npages;\n\n\twhile (npages-- && likely(!freezing(current))) {\n\t\tcond_resched();\n\t\trmap_item = scan_get_next_rmap_item(&page);\n\t\tif (!rmap_item)\n\t\t\treturn;\n\t\tcmp_and_merge_page(page, rmap_item);\n\t\tput_page(page);\n\t}\n\n\tksm_pages_scanned += scan_npages - npages;\n}\n\nstatic int ksmd_should_run(void)\n{\n\treturn (ksm_run & KSM_RUN_MERGE) && !list_empty(&ksm_mm_head.slot.mm_node);\n}\n\nstatic int ksm_scan_thread(void *nothing)\n{\n\tunsigned int sleep_ms;\n\n\tset_freezable();\n\tset_user_nice(current, 5);\n\n\twhile (!kthread_should_stop()) {\n\t\tmutex_lock(&ksm_thread_mutex);\n\t\twait_while_offlining();\n\t\tif (ksmd_should_run())\n\t\t\tksm_do_scan(ksm_thread_pages_to_scan);\n\t\tmutex_unlock(&ksm_thread_mutex);\n\n\t\ttry_to_freeze();\n\n\t\tif (ksmd_should_run()) {\n\t\t\tsleep_ms = READ_ONCE(ksm_thread_sleep_millisecs);\n\t\t\twait_event_interruptible_timeout(ksm_iter_wait,\n\t\t\t\tsleep_ms != READ_ONCE(ksm_thread_sleep_millisecs),\n\t\t\t\tmsecs_to_jiffies(sleep_ms));\n\t\t} else {\n\t\t\twait_event_freezable(ksm_thread_wait,\n\t\t\t\tksmd_should_run() || kthread_should_stop());\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic void __ksm_add_vma(struct vm_area_struct *vma)\n{\n\tunsigned long vm_flags = vma->vm_flags;\n\n\tif (vm_flags & VM_MERGEABLE)\n\t\treturn;\n\n\tif (vma_ksm_compatible(vma))\n\t\tvm_flags_set(vma, VM_MERGEABLE);\n}\n\nstatic int __ksm_del_vma(struct vm_area_struct *vma)\n{\n\tint err;\n\n\tif (!(vma->vm_flags & VM_MERGEABLE))\n\t\treturn 0;\n\n\tif (vma->anon_vma) {\n\t\terr = unmerge_ksm_pages(vma, vma->vm_start, vma->vm_end, true);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tvm_flags_clear(vma, VM_MERGEABLE);\n\treturn 0;\n}\n \nvoid ksm_add_vma(struct vm_area_struct *vma)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\n\tif (test_bit(MMF_VM_MERGE_ANY, &mm->flags))\n\t\t__ksm_add_vma(vma);\n}\n\nstatic void ksm_add_vmas(struct mm_struct *mm)\n{\n\tstruct vm_area_struct *vma;\n\n\tVMA_ITERATOR(vmi, mm, 0);\n\tfor_each_vma(vmi, vma)\n\t\t__ksm_add_vma(vma);\n}\n\nstatic int ksm_del_vmas(struct mm_struct *mm)\n{\n\tstruct vm_area_struct *vma;\n\tint err;\n\n\tVMA_ITERATOR(vmi, mm, 0);\n\tfor_each_vma(vmi, vma) {\n\t\terr = __ksm_del_vma(vma);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\treturn 0;\n}\n\n \nint ksm_enable_merge_any(struct mm_struct *mm)\n{\n\tint err;\n\n\tif (test_bit(MMF_VM_MERGE_ANY, &mm->flags))\n\t\treturn 0;\n\n\tif (!test_bit(MMF_VM_MERGEABLE, &mm->flags)) {\n\t\terr = __ksm_enter(mm);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tset_bit(MMF_VM_MERGE_ANY, &mm->flags);\n\tksm_add_vmas(mm);\n\n\treturn 0;\n}\n\n \nint ksm_disable_merge_any(struct mm_struct *mm)\n{\n\tint err;\n\n\tif (!test_bit(MMF_VM_MERGE_ANY, &mm->flags))\n\t\treturn 0;\n\n\terr = ksm_del_vmas(mm);\n\tif (err) {\n\t\tksm_add_vmas(mm);\n\t\treturn err;\n\t}\n\n\tclear_bit(MMF_VM_MERGE_ANY, &mm->flags);\n\treturn 0;\n}\n\nint ksm_disable(struct mm_struct *mm)\n{\n\tmmap_assert_write_locked(mm);\n\n\tif (!test_bit(MMF_VM_MERGEABLE, &mm->flags))\n\t\treturn 0;\n\tif (test_bit(MMF_VM_MERGE_ANY, &mm->flags))\n\t\treturn ksm_disable_merge_any(mm);\n\treturn ksm_del_vmas(mm);\n}\n\nint ksm_madvise(struct vm_area_struct *vma, unsigned long start,\n\t\tunsigned long end, int advice, unsigned long *vm_flags)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tint err;\n\n\tswitch (advice) {\n\tcase MADV_MERGEABLE:\n\t\tif (vma->vm_flags & VM_MERGEABLE)\n\t\t\treturn 0;\n\t\tif (!vma_ksm_compatible(vma))\n\t\t\treturn 0;\n\n\t\tif (!test_bit(MMF_VM_MERGEABLE, &mm->flags)) {\n\t\t\terr = __ksm_enter(mm);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\t*vm_flags |= VM_MERGEABLE;\n\t\tbreak;\n\n\tcase MADV_UNMERGEABLE:\n\t\tif (!(*vm_flags & VM_MERGEABLE))\n\t\t\treturn 0;\t\t \n\n\t\tif (vma->anon_vma) {\n\t\t\terr = unmerge_ksm_pages(vma, start, end, true);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\t*vm_flags &= ~VM_MERGEABLE;\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(ksm_madvise);\n\nint __ksm_enter(struct mm_struct *mm)\n{\n\tstruct ksm_mm_slot *mm_slot;\n\tstruct mm_slot *slot;\n\tint needs_wakeup;\n\n\tmm_slot = mm_slot_alloc(mm_slot_cache);\n\tif (!mm_slot)\n\t\treturn -ENOMEM;\n\n\tslot = &mm_slot->slot;\n\n\t \n\tneeds_wakeup = list_empty(&ksm_mm_head.slot.mm_node);\n\n\tspin_lock(&ksm_mmlist_lock);\n\tmm_slot_insert(mm_slots_hash, mm, slot);\n\t \n\tif (ksm_run & KSM_RUN_UNMERGE)\n\t\tlist_add_tail(&slot->mm_node, &ksm_mm_head.slot.mm_node);\n\telse\n\t\tlist_add_tail(&slot->mm_node, &ksm_scan.mm_slot->slot.mm_node);\n\tspin_unlock(&ksm_mmlist_lock);\n\n\tset_bit(MMF_VM_MERGEABLE, &mm->flags);\n\tmmgrab(mm);\n\n\tif (needs_wakeup)\n\t\twake_up_interruptible(&ksm_thread_wait);\n\n\ttrace_ksm_enter(mm);\n\treturn 0;\n}\n\nvoid __ksm_exit(struct mm_struct *mm)\n{\n\tstruct ksm_mm_slot *mm_slot;\n\tstruct mm_slot *slot;\n\tint easy_to_free = 0;\n\n\t \n\n\tspin_lock(&ksm_mmlist_lock);\n\tslot = mm_slot_lookup(mm_slots_hash, mm);\n\tmm_slot = mm_slot_entry(slot, struct ksm_mm_slot, slot);\n\tif (mm_slot && ksm_scan.mm_slot != mm_slot) {\n\t\tif (!mm_slot->rmap_list) {\n\t\t\thash_del(&slot->hash);\n\t\t\tlist_del(&slot->mm_node);\n\t\t\teasy_to_free = 1;\n\t\t} else {\n\t\t\tlist_move(&slot->mm_node,\n\t\t\t\t  &ksm_scan.mm_slot->slot.mm_node);\n\t\t}\n\t}\n\tspin_unlock(&ksm_mmlist_lock);\n\n\tif (easy_to_free) {\n\t\tmm_slot_free(mm_slot_cache, mm_slot);\n\t\tclear_bit(MMF_VM_MERGE_ANY, &mm->flags);\n\t\tclear_bit(MMF_VM_MERGEABLE, &mm->flags);\n\t\tmmdrop(mm);\n\t} else if (mm_slot) {\n\t\tmmap_write_lock(mm);\n\t\tmmap_write_unlock(mm);\n\t}\n\n\ttrace_ksm_exit(mm);\n}\n\nstruct page *ksm_might_need_to_copy(struct page *page,\n\t\t\tstruct vm_area_struct *vma, unsigned long address)\n{\n\tstruct folio *folio = page_folio(page);\n\tstruct anon_vma *anon_vma = folio_anon_vma(folio);\n\tstruct page *new_page;\n\n\tif (PageKsm(page)) {\n\t\tif (page_stable_node(page) &&\n\t\t    !(ksm_run & KSM_RUN_UNMERGE))\n\t\t\treturn page;\t \n\t} else if (!anon_vma) {\n\t\treturn page;\t\t \n\t} else if (page->index == linear_page_index(vma, address) &&\n\t\t\tanon_vma->root == vma->anon_vma->root) {\n\t\treturn page;\t\t \n\t}\n\tif (PageHWPoison(page))\n\t\treturn ERR_PTR(-EHWPOISON);\n\tif (!PageUptodate(page))\n\t\treturn page;\t\t \n\n\tnew_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma, address);\n\tif (new_page &&\n\t    mem_cgroup_charge(page_folio(new_page), vma->vm_mm, GFP_KERNEL)) {\n\t\tput_page(new_page);\n\t\tnew_page = NULL;\n\t}\n\tif (new_page) {\n\t\tif (copy_mc_user_highpage(new_page, page, address, vma)) {\n\t\t\tput_page(new_page);\n\t\t\tmemory_failure_queue(page_to_pfn(page), 0);\n\t\t\treturn ERR_PTR(-EHWPOISON);\n\t\t}\n\t\tSetPageDirty(new_page);\n\t\t__SetPageUptodate(new_page);\n\t\t__SetPageLocked(new_page);\n#ifdef CONFIG_SWAP\n\t\tcount_vm_event(KSM_SWPIN_COPY);\n#endif\n\t}\n\n\treturn new_page;\n}\n\nvoid rmap_walk_ksm(struct folio *folio, struct rmap_walk_control *rwc)\n{\n\tstruct ksm_stable_node *stable_node;\n\tstruct ksm_rmap_item *rmap_item;\n\tint search_new_forks = 0;\n\n\tVM_BUG_ON_FOLIO(!folio_test_ksm(folio), folio);\n\n\t \n\tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n\n\tstable_node = folio_stable_node(folio);\n\tif (!stable_node)\n\t\treturn;\nagain:\n\thlist_for_each_entry(rmap_item, &stable_node->hlist, hlist) {\n\t\tstruct anon_vma *anon_vma = rmap_item->anon_vma;\n\t\tstruct anon_vma_chain *vmac;\n\t\tstruct vm_area_struct *vma;\n\n\t\tcond_resched();\n\t\tif (!anon_vma_trylock_read(anon_vma)) {\n\t\t\tif (rwc->try_lock) {\n\t\t\t\trwc->contended = true;\n\t\t\t\treturn;\n\t\t\t}\n\t\t\tanon_vma_lock_read(anon_vma);\n\t\t}\n\t\tanon_vma_interval_tree_foreach(vmac, &anon_vma->rb_root,\n\t\t\t\t\t       0, ULONG_MAX) {\n\t\t\tunsigned long addr;\n\n\t\t\tcond_resched();\n\t\t\tvma = vmac->vma;\n\n\t\t\t \n\t\t\taddr = rmap_item->address & PAGE_MASK;\n\n\t\t\tif (addr < vma->vm_start || addr >= vma->vm_end)\n\t\t\t\tcontinue;\n\t\t\t \n\t\t\tif ((rmap_item->mm == vma->vm_mm) == search_new_forks)\n\t\t\t\tcontinue;\n\n\t\t\tif (rwc->invalid_vma && rwc->invalid_vma(vma, rwc->arg))\n\t\t\t\tcontinue;\n\n\t\t\tif (!rwc->rmap_one(folio, vma, addr, rwc->arg)) {\n\t\t\t\tanon_vma_unlock_read(anon_vma);\n\t\t\t\treturn;\n\t\t\t}\n\t\t\tif (rwc->done && rwc->done(folio)) {\n\t\t\t\tanon_vma_unlock_read(anon_vma);\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t\tanon_vma_unlock_read(anon_vma);\n\t}\n\tif (!search_new_forks++)\n\t\tgoto again;\n}\n\n#ifdef CONFIG_MEMORY_FAILURE\n \nvoid collect_procs_ksm(struct page *page, struct list_head *to_kill,\n\t\t       int force_early)\n{\n\tstruct ksm_stable_node *stable_node;\n\tstruct ksm_rmap_item *rmap_item;\n\tstruct folio *folio = page_folio(page);\n\tstruct vm_area_struct *vma;\n\tstruct task_struct *tsk;\n\n\tstable_node = folio_stable_node(folio);\n\tif (!stable_node)\n\t\treturn;\n\thlist_for_each_entry(rmap_item, &stable_node->hlist, hlist) {\n\t\tstruct anon_vma *av = rmap_item->anon_vma;\n\n\t\tanon_vma_lock_read(av);\n\t\trcu_read_lock();\n\t\tfor_each_process(tsk) {\n\t\t\tstruct anon_vma_chain *vmac;\n\t\t\tunsigned long addr;\n\t\t\tstruct task_struct *t =\n\t\t\t\ttask_early_kill(tsk, force_early);\n\t\t\tif (!t)\n\t\t\t\tcontinue;\n\t\t\tanon_vma_interval_tree_foreach(vmac, &av->rb_root, 0,\n\t\t\t\t\t\t       ULONG_MAX)\n\t\t\t{\n\t\t\t\tvma = vmac->vma;\n\t\t\t\tif (vma->vm_mm == t->mm) {\n\t\t\t\t\taddr = rmap_item->address & PAGE_MASK;\n\t\t\t\t\tadd_to_kill_ksm(t, page, vma, to_kill,\n\t\t\t\t\t\t\taddr);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\trcu_read_unlock();\n\t\tanon_vma_unlock_read(av);\n\t}\n}\n#endif\n\n#ifdef CONFIG_MIGRATION\nvoid folio_migrate_ksm(struct folio *newfolio, struct folio *folio)\n{\n\tstruct ksm_stable_node *stable_node;\n\n\tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n\tVM_BUG_ON_FOLIO(!folio_test_locked(newfolio), newfolio);\n\tVM_BUG_ON_FOLIO(newfolio->mapping != folio->mapping, newfolio);\n\n\tstable_node = folio_stable_node(folio);\n\tif (stable_node) {\n\t\tVM_BUG_ON_FOLIO(stable_node->kpfn != folio_pfn(folio), folio);\n\t\tstable_node->kpfn = folio_pfn(newfolio);\n\t\t \n\t\tsmp_wmb();\n\t\tset_page_stable_node(&folio->page, NULL);\n\t}\n}\n#endif  \n\n#ifdef CONFIG_MEMORY_HOTREMOVE\nstatic void wait_while_offlining(void)\n{\n\twhile (ksm_run & KSM_RUN_OFFLINE) {\n\t\tmutex_unlock(&ksm_thread_mutex);\n\t\twait_on_bit(&ksm_run, ilog2(KSM_RUN_OFFLINE),\n\t\t\t    TASK_UNINTERRUPTIBLE);\n\t\tmutex_lock(&ksm_thread_mutex);\n\t}\n}\n\nstatic bool stable_node_dup_remove_range(struct ksm_stable_node *stable_node,\n\t\t\t\t\t unsigned long start_pfn,\n\t\t\t\t\t unsigned long end_pfn)\n{\n\tif (stable_node->kpfn >= start_pfn &&\n\t    stable_node->kpfn < end_pfn) {\n\t\t \n\t\tremove_node_from_stable_tree(stable_node);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic bool stable_node_chain_remove_range(struct ksm_stable_node *stable_node,\n\t\t\t\t\t   unsigned long start_pfn,\n\t\t\t\t\t   unsigned long end_pfn,\n\t\t\t\t\t   struct rb_root *root)\n{\n\tstruct ksm_stable_node *dup;\n\tstruct hlist_node *hlist_safe;\n\n\tif (!is_stable_node_chain(stable_node)) {\n\t\tVM_BUG_ON(is_stable_node_dup(stable_node));\n\t\treturn stable_node_dup_remove_range(stable_node, start_pfn,\n\t\t\t\t\t\t    end_pfn);\n\t}\n\n\thlist_for_each_entry_safe(dup, hlist_safe,\n\t\t\t\t  &stable_node->hlist, hlist_dup) {\n\t\tVM_BUG_ON(!is_stable_node_dup(dup));\n\t\tstable_node_dup_remove_range(dup, start_pfn, end_pfn);\n\t}\n\tif (hlist_empty(&stable_node->hlist)) {\n\t\tfree_stable_node_chain(stable_node, root);\n\t\treturn true;  \n\t} else\n\t\treturn false;\n}\n\nstatic void ksm_check_stable_tree(unsigned long start_pfn,\n\t\t\t\t  unsigned long end_pfn)\n{\n\tstruct ksm_stable_node *stable_node, *next;\n\tstruct rb_node *node;\n\tint nid;\n\n\tfor (nid = 0; nid < ksm_nr_node_ids; nid++) {\n\t\tnode = rb_first(root_stable_tree + nid);\n\t\twhile (node) {\n\t\t\tstable_node = rb_entry(node, struct ksm_stable_node, node);\n\t\t\tif (stable_node_chain_remove_range(stable_node,\n\t\t\t\t\t\t\t   start_pfn, end_pfn,\n\t\t\t\t\t\t\t   root_stable_tree +\n\t\t\t\t\t\t\t   nid))\n\t\t\t\tnode = rb_first(root_stable_tree + nid);\n\t\t\telse\n\t\t\t\tnode = rb_next(node);\n\t\t\tcond_resched();\n\t\t}\n\t}\n\tlist_for_each_entry_safe(stable_node, next, &migrate_nodes, list) {\n\t\tif (stable_node->kpfn >= start_pfn &&\n\t\t    stable_node->kpfn < end_pfn)\n\t\t\tremove_node_from_stable_tree(stable_node);\n\t\tcond_resched();\n\t}\n}\n\nstatic int ksm_memory_callback(struct notifier_block *self,\n\t\t\t       unsigned long action, void *arg)\n{\n\tstruct memory_notify *mn = arg;\n\n\tswitch (action) {\n\tcase MEM_GOING_OFFLINE:\n\t\t \n\t\tmutex_lock(&ksm_thread_mutex);\n\t\tksm_run |= KSM_RUN_OFFLINE;\n\t\tmutex_unlock(&ksm_thread_mutex);\n\t\tbreak;\n\n\tcase MEM_OFFLINE:\n\t\t \n\t\tksm_check_stable_tree(mn->start_pfn,\n\t\t\t\t      mn->start_pfn + mn->nr_pages);\n\t\tfallthrough;\n\tcase MEM_CANCEL_OFFLINE:\n\t\tmutex_lock(&ksm_thread_mutex);\n\t\tksm_run &= ~KSM_RUN_OFFLINE;\n\t\tmutex_unlock(&ksm_thread_mutex);\n\n\t\tsmp_mb();\t \n\t\twake_up_bit(&ksm_run, ilog2(KSM_RUN_OFFLINE));\n\t\tbreak;\n\t}\n\treturn NOTIFY_OK;\n}\n#else\nstatic void wait_while_offlining(void)\n{\n}\n#endif  \n\n#ifdef CONFIG_PROC_FS\nlong ksm_process_profit(struct mm_struct *mm)\n{\n\treturn (long)(mm->ksm_merging_pages + mm->ksm_zero_pages) * PAGE_SIZE -\n\t\tmm->ksm_rmap_items * sizeof(struct ksm_rmap_item);\n}\n#endif  \n\n#ifdef CONFIG_SYSFS\n \n\n#define KSM_ATTR_RO(_name) \\\n\tstatic struct kobj_attribute _name##_attr = __ATTR_RO(_name)\n#define KSM_ATTR(_name) \\\n\tstatic struct kobj_attribute _name##_attr = __ATTR_RW(_name)\n\nstatic ssize_t sleep_millisecs_show(struct kobject *kobj,\n\t\t\t\t    struct kobj_attribute *attr, char *buf)\n{\n\treturn sysfs_emit(buf, \"%u\\n\", ksm_thread_sleep_millisecs);\n}\n\nstatic ssize_t sleep_millisecs_store(struct kobject *kobj,\n\t\t\t\t     struct kobj_attribute *attr,\n\t\t\t\t     const char *buf, size_t count)\n{\n\tunsigned int msecs;\n\tint err;\n\n\terr = kstrtouint(buf, 10, &msecs);\n\tif (err)\n\t\treturn -EINVAL;\n\n\tksm_thread_sleep_millisecs = msecs;\n\twake_up_interruptible(&ksm_iter_wait);\n\n\treturn count;\n}\nKSM_ATTR(sleep_millisecs);\n\nstatic ssize_t pages_to_scan_show(struct kobject *kobj,\n\t\t\t\t  struct kobj_attribute *attr, char *buf)\n{\n\treturn sysfs_emit(buf, \"%u\\n\", ksm_thread_pages_to_scan);\n}\n\nstatic ssize_t pages_to_scan_store(struct kobject *kobj,\n\t\t\t\t   struct kobj_attribute *attr,\n\t\t\t\t   const char *buf, size_t count)\n{\n\tunsigned int nr_pages;\n\tint err;\n\n\terr = kstrtouint(buf, 10, &nr_pages);\n\tif (err)\n\t\treturn -EINVAL;\n\n\tksm_thread_pages_to_scan = nr_pages;\n\n\treturn count;\n}\nKSM_ATTR(pages_to_scan);\n\nstatic ssize_t run_show(struct kobject *kobj, struct kobj_attribute *attr,\n\t\t\tchar *buf)\n{\n\treturn sysfs_emit(buf, \"%lu\\n\", ksm_run);\n}\n\nstatic ssize_t run_store(struct kobject *kobj, struct kobj_attribute *attr,\n\t\t\t const char *buf, size_t count)\n{\n\tunsigned int flags;\n\tint err;\n\n\terr = kstrtouint(buf, 10, &flags);\n\tif (err)\n\t\treturn -EINVAL;\n\tif (flags > KSM_RUN_UNMERGE)\n\t\treturn -EINVAL;\n\n\t \n\n\tmutex_lock(&ksm_thread_mutex);\n\twait_while_offlining();\n\tif (ksm_run != flags) {\n\t\tksm_run = flags;\n\t\tif (flags & KSM_RUN_UNMERGE) {\n\t\t\tset_current_oom_origin();\n\t\t\terr = unmerge_and_remove_all_rmap_items();\n\t\t\tclear_current_oom_origin();\n\t\t\tif (err) {\n\t\t\t\tksm_run = KSM_RUN_STOP;\n\t\t\t\tcount = err;\n\t\t\t}\n\t\t}\n\t}\n\tmutex_unlock(&ksm_thread_mutex);\n\n\tif (flags & KSM_RUN_MERGE)\n\t\twake_up_interruptible(&ksm_thread_wait);\n\n\treturn count;\n}\nKSM_ATTR(run);\n\n#ifdef CONFIG_NUMA\nstatic ssize_t merge_across_nodes_show(struct kobject *kobj,\n\t\t\t\t       struct kobj_attribute *attr, char *buf)\n{\n\treturn sysfs_emit(buf, \"%u\\n\", ksm_merge_across_nodes);\n}\n\nstatic ssize_t merge_across_nodes_store(struct kobject *kobj,\n\t\t\t\t   struct kobj_attribute *attr,\n\t\t\t\t   const char *buf, size_t count)\n{\n\tint err;\n\tunsigned long knob;\n\n\terr = kstrtoul(buf, 10, &knob);\n\tif (err)\n\t\treturn err;\n\tif (knob > 1)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&ksm_thread_mutex);\n\twait_while_offlining();\n\tif (ksm_merge_across_nodes != knob) {\n\t\tif (ksm_pages_shared || remove_all_stable_nodes())\n\t\t\terr = -EBUSY;\n\t\telse if (root_stable_tree == one_stable_tree) {\n\t\t\tstruct rb_root *buf;\n\t\t\t \n\t\t\tbuf = kcalloc(nr_node_ids + nr_node_ids, sizeof(*buf),\n\t\t\t\t      GFP_KERNEL);\n\t\t\t \n\t\t\tif (!buf)\n\t\t\t\terr = -ENOMEM;\n\t\t\telse {\n\t\t\t\troot_stable_tree = buf;\n\t\t\t\troot_unstable_tree = buf + nr_node_ids;\n\t\t\t\t \n\t\t\t\troot_unstable_tree[0] = one_unstable_tree[0];\n\t\t\t}\n\t\t}\n\t\tif (!err) {\n\t\t\tksm_merge_across_nodes = knob;\n\t\t\tksm_nr_node_ids = knob ? 1 : nr_node_ids;\n\t\t}\n\t}\n\tmutex_unlock(&ksm_thread_mutex);\n\n\treturn err ? err : count;\n}\nKSM_ATTR(merge_across_nodes);\n#endif\n\nstatic ssize_t use_zero_pages_show(struct kobject *kobj,\n\t\t\t\t   struct kobj_attribute *attr, char *buf)\n{\n\treturn sysfs_emit(buf, \"%u\\n\", ksm_use_zero_pages);\n}\nstatic ssize_t use_zero_pages_store(struct kobject *kobj,\n\t\t\t\t   struct kobj_attribute *attr,\n\t\t\t\t   const char *buf, size_t count)\n{\n\tint err;\n\tbool value;\n\n\terr = kstrtobool(buf, &value);\n\tif (err)\n\t\treturn -EINVAL;\n\n\tksm_use_zero_pages = value;\n\n\treturn count;\n}\nKSM_ATTR(use_zero_pages);\n\nstatic ssize_t max_page_sharing_show(struct kobject *kobj,\n\t\t\t\t     struct kobj_attribute *attr, char *buf)\n{\n\treturn sysfs_emit(buf, \"%u\\n\", ksm_max_page_sharing);\n}\n\nstatic ssize_t max_page_sharing_store(struct kobject *kobj,\n\t\t\t\t      struct kobj_attribute *attr,\n\t\t\t\t      const char *buf, size_t count)\n{\n\tint err;\n\tint knob;\n\n\terr = kstrtoint(buf, 10, &knob);\n\tif (err)\n\t\treturn err;\n\t \n\tif (knob < 2)\n\t\treturn -EINVAL;\n\n\tif (READ_ONCE(ksm_max_page_sharing) == knob)\n\t\treturn count;\n\n\tmutex_lock(&ksm_thread_mutex);\n\twait_while_offlining();\n\tif (ksm_max_page_sharing != knob) {\n\t\tif (ksm_pages_shared || remove_all_stable_nodes())\n\t\t\terr = -EBUSY;\n\t\telse\n\t\t\tksm_max_page_sharing = knob;\n\t}\n\tmutex_unlock(&ksm_thread_mutex);\n\n\treturn err ? err : count;\n}\nKSM_ATTR(max_page_sharing);\n\nstatic ssize_t pages_scanned_show(struct kobject *kobj,\n\t\t\t\t  struct kobj_attribute *attr, char *buf)\n{\n\treturn sysfs_emit(buf, \"%lu\\n\", ksm_pages_scanned);\n}\nKSM_ATTR_RO(pages_scanned);\n\nstatic ssize_t pages_shared_show(struct kobject *kobj,\n\t\t\t\t struct kobj_attribute *attr, char *buf)\n{\n\treturn sysfs_emit(buf, \"%lu\\n\", ksm_pages_shared);\n}\nKSM_ATTR_RO(pages_shared);\n\nstatic ssize_t pages_sharing_show(struct kobject *kobj,\n\t\t\t\t  struct kobj_attribute *attr, char *buf)\n{\n\treturn sysfs_emit(buf, \"%lu\\n\", ksm_pages_sharing);\n}\nKSM_ATTR_RO(pages_sharing);\n\nstatic ssize_t pages_unshared_show(struct kobject *kobj,\n\t\t\t\t   struct kobj_attribute *attr, char *buf)\n{\n\treturn sysfs_emit(buf, \"%lu\\n\", ksm_pages_unshared);\n}\nKSM_ATTR_RO(pages_unshared);\n\nstatic ssize_t pages_volatile_show(struct kobject *kobj,\n\t\t\t\t   struct kobj_attribute *attr, char *buf)\n{\n\tlong ksm_pages_volatile;\n\n\tksm_pages_volatile = ksm_rmap_items - ksm_pages_shared\n\t\t\t\t- ksm_pages_sharing - ksm_pages_unshared;\n\t \n\tif (ksm_pages_volatile < 0)\n\t\tksm_pages_volatile = 0;\n\treturn sysfs_emit(buf, \"%ld\\n\", ksm_pages_volatile);\n}\nKSM_ATTR_RO(pages_volatile);\n\nstatic ssize_t ksm_zero_pages_show(struct kobject *kobj,\n\t\t\t\tstruct kobj_attribute *attr, char *buf)\n{\n\treturn sysfs_emit(buf, \"%ld\\n\", ksm_zero_pages);\n}\nKSM_ATTR_RO(ksm_zero_pages);\n\nstatic ssize_t general_profit_show(struct kobject *kobj,\n\t\t\t\t   struct kobj_attribute *attr, char *buf)\n{\n\tlong general_profit;\n\n\tgeneral_profit = (ksm_pages_sharing + ksm_zero_pages) * PAGE_SIZE -\n\t\t\t\tksm_rmap_items * sizeof(struct ksm_rmap_item);\n\n\treturn sysfs_emit(buf, \"%ld\\n\", general_profit);\n}\nKSM_ATTR_RO(general_profit);\n\nstatic ssize_t stable_node_dups_show(struct kobject *kobj,\n\t\t\t\t     struct kobj_attribute *attr, char *buf)\n{\n\treturn sysfs_emit(buf, \"%lu\\n\", ksm_stable_node_dups);\n}\nKSM_ATTR_RO(stable_node_dups);\n\nstatic ssize_t stable_node_chains_show(struct kobject *kobj,\n\t\t\t\t       struct kobj_attribute *attr, char *buf)\n{\n\treturn sysfs_emit(buf, \"%lu\\n\", ksm_stable_node_chains);\n}\nKSM_ATTR_RO(stable_node_chains);\n\nstatic ssize_t\nstable_node_chains_prune_millisecs_show(struct kobject *kobj,\n\t\t\t\t\tstruct kobj_attribute *attr,\n\t\t\t\t\tchar *buf)\n{\n\treturn sysfs_emit(buf, \"%u\\n\", ksm_stable_node_chains_prune_millisecs);\n}\n\nstatic ssize_t\nstable_node_chains_prune_millisecs_store(struct kobject *kobj,\n\t\t\t\t\t struct kobj_attribute *attr,\n\t\t\t\t\t const char *buf, size_t count)\n{\n\tunsigned int msecs;\n\tint err;\n\n\terr = kstrtouint(buf, 10, &msecs);\n\tif (err)\n\t\treturn -EINVAL;\n\n\tksm_stable_node_chains_prune_millisecs = msecs;\n\n\treturn count;\n}\nKSM_ATTR(stable_node_chains_prune_millisecs);\n\nstatic ssize_t full_scans_show(struct kobject *kobj,\n\t\t\t       struct kobj_attribute *attr, char *buf)\n{\n\treturn sysfs_emit(buf, \"%lu\\n\", ksm_scan.seqnr);\n}\nKSM_ATTR_RO(full_scans);\n\nstatic struct attribute *ksm_attrs[] = {\n\t&sleep_millisecs_attr.attr,\n\t&pages_to_scan_attr.attr,\n\t&run_attr.attr,\n\t&pages_scanned_attr.attr,\n\t&pages_shared_attr.attr,\n\t&pages_sharing_attr.attr,\n\t&pages_unshared_attr.attr,\n\t&pages_volatile_attr.attr,\n\t&ksm_zero_pages_attr.attr,\n\t&full_scans_attr.attr,\n#ifdef CONFIG_NUMA\n\t&merge_across_nodes_attr.attr,\n#endif\n\t&max_page_sharing_attr.attr,\n\t&stable_node_chains_attr.attr,\n\t&stable_node_dups_attr.attr,\n\t&stable_node_chains_prune_millisecs_attr.attr,\n\t&use_zero_pages_attr.attr,\n\t&general_profit_attr.attr,\n\tNULL,\n};\n\nstatic const struct attribute_group ksm_attr_group = {\n\t.attrs = ksm_attrs,\n\t.name = \"ksm\",\n};\n#endif  \n\nstatic int __init ksm_init(void)\n{\n\tstruct task_struct *ksm_thread;\n\tint err;\n\n\t \n\tzero_checksum = calc_checksum(ZERO_PAGE(0));\n\t \n\tksm_use_zero_pages = false;\n\n\terr = ksm_slab_init();\n\tif (err)\n\t\tgoto out;\n\n\tksm_thread = kthread_run(ksm_scan_thread, NULL, \"ksmd\");\n\tif (IS_ERR(ksm_thread)) {\n\t\tpr_err(\"ksm: creating kthread failed\\n\");\n\t\terr = PTR_ERR(ksm_thread);\n\t\tgoto out_free;\n\t}\n\n#ifdef CONFIG_SYSFS\n\terr = sysfs_create_group(mm_kobj, &ksm_attr_group);\n\tif (err) {\n\t\tpr_err(\"ksm: register sysfs failed\\n\");\n\t\tkthread_stop(ksm_thread);\n\t\tgoto out_free;\n\t}\n#else\n\tksm_run = KSM_RUN_MERGE;\t \n\n#endif  \n\n#ifdef CONFIG_MEMORY_HOTREMOVE\n\t \n\thotplug_memory_notifier(ksm_memory_callback, KSM_CALLBACK_PRI);\n#endif\n\treturn 0;\n\nout_free:\n\tksm_slab_free();\nout:\n\treturn err;\n}\nsubsys_initcall(ksm_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}