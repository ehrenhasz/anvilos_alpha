{
  "module_name": "internal.h",
  "hash_id": "daeded4c90cd8efe505405e50ebfb460db8c327c38f05fde4109947c6405dc4b",
  "original_prompt": "Ingested from linux-6.6.14/mm/internal.h",
  "human_readable_source": " \n \n#ifndef __MM_INTERNAL_H\n#define __MM_INTERNAL_H\n\n#include <linux/fs.h>\n#include <linux/mm.h>\n#include <linux/pagemap.h>\n#include <linux/rmap.h>\n#include <linux/tracepoint-defs.h>\n\nstruct folio_batch;\n\n \n#define GFP_RECLAIM_MASK (__GFP_RECLAIM|__GFP_HIGH|__GFP_IO|__GFP_FS|\\\n\t\t\t__GFP_NOWARN|__GFP_RETRY_MAYFAIL|__GFP_NOFAIL|\\\n\t\t\t__GFP_NORETRY|__GFP_MEMALLOC|__GFP_NOMEMALLOC|\\\n\t\t\t__GFP_NOLOCKDEP)\n\n \n#define GFP_BOOT_MASK (__GFP_BITS_MASK & ~(__GFP_RECLAIM|__GFP_IO|__GFP_FS))\n\n \n#define GFP_CONSTRAINT_MASK (__GFP_HARDWALL|__GFP_THISNODE)\n\n \n#define GFP_SLAB_BUG_MASK (__GFP_DMA32|__GFP_HIGHMEM|~__GFP_BITS_MASK)\n\n \n#define WARN_ON_ONCE_GFP(cond, gfp)\t({\t\t\t\t\\\n\tstatic bool __section(\".data.once\") __warned;\t\t\t\\\n\tint __ret_warn_once = !!(cond);\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tif (unlikely(!(gfp & __GFP_NOWARN) && __ret_warn_once && !__warned)) { \\\n\t\t__warned = true;\t\t\t\t\t\\\n\t\tWARN_ON(1);\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tunlikely(__ret_warn_once);\t\t\t\t\t\\\n})\n\nvoid page_writeback_init(void);\n\n \n#define COMPOUND_MAPPED\t\t0x800000\n#define FOLIO_PAGES_MAPPED\t(COMPOUND_MAPPED - 1)\n\n \n#define SHOW_MEM_FILTER_NODES\t\t(0x0001u)\t \n\n \nstatic inline int folio_nr_pages_mapped(struct folio *folio)\n{\n\treturn atomic_read(&folio->_nr_pages_mapped) & FOLIO_PAGES_MAPPED;\n}\n\nstatic inline void *folio_raw_mapping(struct folio *folio)\n{\n\tunsigned long mapping = (unsigned long)folio->mapping;\n\n\treturn (void *)(mapping & ~PAGE_MAPPING_FLAGS);\n}\n\nvoid __acct_reclaim_writeback(pg_data_t *pgdat, struct folio *folio,\n\t\t\t\t\t\tint nr_throttled);\nstatic inline void acct_reclaim_writeback(struct folio *folio)\n{\n\tpg_data_t *pgdat = folio_pgdat(folio);\n\tint nr_throttled = atomic_read(&pgdat->nr_writeback_throttled);\n\n\tif (nr_throttled)\n\t\t__acct_reclaim_writeback(pgdat, folio, nr_throttled);\n}\n\nstatic inline void wake_throttle_isolated(pg_data_t *pgdat)\n{\n\twait_queue_head_t *wqh;\n\n\twqh = &pgdat->reclaim_wait[VMSCAN_THROTTLE_ISOLATED];\n\tif (waitqueue_active(wqh))\n\t\twake_up(wqh);\n}\n\nvm_fault_t do_swap_page(struct vm_fault *vmf);\nvoid folio_rotate_reclaimable(struct folio *folio);\nbool __folio_end_writeback(struct folio *folio);\nvoid deactivate_file_folio(struct folio *folio);\nvoid folio_activate(struct folio *folio);\n\nvoid free_pgtables(struct mmu_gather *tlb, struct ma_state *mas,\n\t\t   struct vm_area_struct *start_vma, unsigned long floor,\n\t\t   unsigned long ceiling, bool mm_wr_locked);\nvoid pmd_install(struct mm_struct *mm, pmd_t *pmd, pgtable_t *pte);\n\nstruct zap_details;\nvoid unmap_page_range(struct mmu_gather *tlb,\n\t\t\t     struct vm_area_struct *vma,\n\t\t\t     unsigned long addr, unsigned long end,\n\t\t\t     struct zap_details *details);\n\nvoid page_cache_ra_order(struct readahead_control *, struct file_ra_state *,\n\t\tunsigned int order);\nvoid force_page_cache_ra(struct readahead_control *, unsigned long nr);\nstatic inline void force_page_cache_readahead(struct address_space *mapping,\n\t\tstruct file *file, pgoff_t index, unsigned long nr_to_read)\n{\n\tDEFINE_READAHEAD(ractl, file, &file->f_ra, mapping, index);\n\tforce_page_cache_ra(&ractl, nr_to_read);\n}\n\nunsigned find_lock_entries(struct address_space *mapping, pgoff_t *start,\n\t\tpgoff_t end, struct folio_batch *fbatch, pgoff_t *indices);\nunsigned find_get_entries(struct address_space *mapping, pgoff_t *start,\n\t\tpgoff_t end, struct folio_batch *fbatch, pgoff_t *indices);\nvoid filemap_free_folio(struct address_space *mapping, struct folio *folio);\nint truncate_inode_folio(struct address_space *mapping, struct folio *folio);\nbool truncate_inode_partial_folio(struct folio *folio, loff_t start,\n\t\tloff_t end);\nlong invalidate_inode_page(struct page *page);\nunsigned long mapping_try_invalidate(struct address_space *mapping,\n\t\tpgoff_t start, pgoff_t end, unsigned long *nr_failed);\n\n \nstatic inline bool folio_evictable(struct folio *folio)\n{\n\tbool ret;\n\n\t \n\trcu_read_lock();\n\tret = !mapping_unevictable(folio_mapping(folio)) &&\n\t\t\t!folio_test_mlocked(folio);\n\trcu_read_unlock();\n\treturn ret;\n}\n\n \nstatic inline void set_page_refcounted(struct page *page)\n{\n\tVM_BUG_ON_PAGE(PageTail(page), page);\n\tVM_BUG_ON_PAGE(page_ref_count(page), page);\n\tset_page_count(page, 1);\n}\n\n \nstatic inline bool folio_needs_release(struct folio *folio)\n{\n\tstruct address_space *mapping = folio_mapping(folio);\n\n\treturn folio_has_private(folio) ||\n\t\t(mapping && mapping_release_always(mapping));\n}\n\nextern unsigned long highest_memmap_pfn;\n\n \n#define MAX_RECLAIM_RETRIES 16\n\n \nbool isolate_lru_page(struct page *page);\nbool folio_isolate_lru(struct folio *folio);\nvoid putback_lru_page(struct page *page);\nvoid folio_putback_lru(struct folio *folio);\nextern void reclaim_throttle(pg_data_t *pgdat, enum vmscan_throttle_state reason);\n\n \npmd_t *mm_find_pmd(struct mm_struct *mm, unsigned long address);\n\n \n#define K(x) ((x) << (PAGE_SHIFT-10))\n\nextern char * const zone_names[MAX_NR_ZONES];\n\n \nDECLARE_STATIC_KEY_MAYBE(CONFIG_DEBUG_VM, check_pages_enabled);\n\nextern int min_free_kbytes;\n\nvoid setup_per_zone_wmarks(void);\nvoid calculate_min_free_kbytes(void);\nint __meminit init_per_zone_wmark_min(void);\nvoid page_alloc_sysctl_init(void);\n\n \nstruct alloc_context {\n\tstruct zonelist *zonelist;\n\tnodemask_t *nodemask;\n\tstruct zoneref *preferred_zoneref;\n\tint migratetype;\n\n\t \n\tenum zone_type highest_zoneidx;\n\tbool spread_dirty_pages;\n};\n\n \nstatic inline unsigned int buddy_order(struct page *page)\n{\n\t \n\treturn page_private(page);\n}\n\n \n#define buddy_order_unsafe(page)\tREAD_ONCE(page_private(page))\n\n \nstatic inline bool page_is_buddy(struct page *page, struct page *buddy,\n\t\t\t\t unsigned int order)\n{\n\tif (!page_is_guard(buddy) && !PageBuddy(buddy))\n\t\treturn false;\n\n\tif (buddy_order(buddy) != order)\n\t\treturn false;\n\n\t \n\tif (page_zone_id(page) != page_zone_id(buddy))\n\t\treturn false;\n\n\tVM_BUG_ON_PAGE(page_count(buddy) != 0, buddy);\n\n\treturn true;\n}\n\n \nstatic inline unsigned long\n__find_buddy_pfn(unsigned long page_pfn, unsigned int order)\n{\n\treturn page_pfn ^ (1 << order);\n}\n\n \nstatic inline struct page *find_buddy_page_pfn(struct page *page,\n\t\t\tunsigned long pfn, unsigned int order, unsigned long *buddy_pfn)\n{\n\tunsigned long __buddy_pfn = __find_buddy_pfn(pfn, order);\n\tstruct page *buddy;\n\n\tbuddy = page + (__buddy_pfn - pfn);\n\tif (buddy_pfn)\n\t\t*buddy_pfn = __buddy_pfn;\n\n\tif (page_is_buddy(page, buddy, order))\n\t\treturn buddy;\n\treturn NULL;\n}\n\nextern struct page *__pageblock_pfn_to_page(unsigned long start_pfn,\n\t\t\t\tunsigned long end_pfn, struct zone *zone);\n\nstatic inline struct page *pageblock_pfn_to_page(unsigned long start_pfn,\n\t\t\t\tunsigned long end_pfn, struct zone *zone)\n{\n\tif (zone->contiguous)\n\t\treturn pfn_to_page(start_pfn);\n\n\treturn __pageblock_pfn_to_page(start_pfn, end_pfn, zone);\n}\n\nvoid set_zone_contiguous(struct zone *zone);\n\nstatic inline void clear_zone_contiguous(struct zone *zone)\n{\n\tzone->contiguous = false;\n}\n\nextern int __isolate_free_page(struct page *page, unsigned int order);\nextern void __putback_isolated_page(struct page *page, unsigned int order,\n\t\t\t\t    int mt);\nextern void memblock_free_pages(struct page *page, unsigned long pfn,\n\t\t\t\t\tunsigned int order);\nextern void __free_pages_core(struct page *page, unsigned int order);\n\n \nstatic inline void folio_set_order(struct folio *folio, unsigned int order)\n{\n\tif (WARN_ON_ONCE(!order || !folio_test_large(folio)))\n\t\treturn;\n\n\tfolio->_flags_1 = (folio->_flags_1 & ~0xffUL) | order;\n#ifdef CONFIG_64BIT\n\tfolio->_folio_nr_pages = 1U << order;\n#endif\n}\n\nvoid folio_undo_large_rmappable(struct folio *folio);\n\nstatic inline void prep_compound_head(struct page *page, unsigned int order)\n{\n\tstruct folio *folio = (struct folio *)page;\n\n\tfolio_set_order(folio, order);\n\tatomic_set(&folio->_entire_mapcount, -1);\n\tatomic_set(&folio->_nr_pages_mapped, 0);\n\tatomic_set(&folio->_pincount, 0);\n}\n\nstatic inline void prep_compound_tail(struct page *head, int tail_idx)\n{\n\tstruct page *p = head + tail_idx;\n\n\tp->mapping = TAIL_MAPPING;\n\tset_compound_head(p, head);\n\tset_page_private(p, 0);\n}\n\nextern void prep_compound_page(struct page *page, unsigned int order);\n\nextern void post_alloc_hook(struct page *page, unsigned int order,\n\t\t\t\t\tgfp_t gfp_flags);\nextern int user_min_free_kbytes;\n\nextern void free_unref_page(struct page *page, unsigned int order);\nextern void free_unref_page_list(struct list_head *list);\n\nextern void zone_pcp_reset(struct zone *zone);\nextern void zone_pcp_disable(struct zone *zone);\nextern void zone_pcp_enable(struct zone *zone);\nextern void zone_pcp_init(struct zone *zone);\n\nextern void *memmap_alloc(phys_addr_t size, phys_addr_t align,\n\t\t\t  phys_addr_t min_addr,\n\t\t\t  int nid, bool exact_nid);\n\nvoid memmap_init_range(unsigned long, int, unsigned long, unsigned long,\n\t\tunsigned long, enum meminit_context, struct vmem_altmap *, int);\n\n\nint split_free_page(struct page *free_page,\n\t\t\tunsigned int order, unsigned long split_pfn_offset);\n\n#if defined CONFIG_COMPACTION || defined CONFIG_CMA\n\n \n \nstruct compact_control {\n\tstruct list_head freepages;\t \n\tstruct list_head migratepages;\t \n\tunsigned int nr_freepages;\t \n\tunsigned int nr_migratepages;\t \n\tunsigned long free_pfn;\t\t \n\t \n\tunsigned long migrate_pfn;\n\tunsigned long fast_start_pfn;\t \n\tstruct zone *zone;\n\tunsigned long total_migrate_scanned;\n\tunsigned long total_free_scanned;\n\tunsigned short fast_search_fail; \n\tshort search_order;\t\t \n\tconst gfp_t gfp_mask;\t\t \n\tint order;\t\t\t \n\tint migratetype;\t\t \n\tconst unsigned int alloc_flags;\t \n\tconst int highest_zoneidx;\t \n\tenum migrate_mode mode;\t\t \n\tbool ignore_skip_hint;\t\t \n\tbool no_set_skip_hint;\t\t \n\tbool ignore_block_suitable;\t \n\tbool direct_compaction;\t\t \n\tbool proactive_compaction;\t \n\tbool whole_zone;\t\t \n\tbool contended;\t\t\t \n\tbool finish_pageblock;\t\t \n\tbool alloc_contig;\t\t \n};\n\n \nstruct capture_control {\n\tstruct compact_control *cc;\n\tstruct page *page;\n};\n\nunsigned long\nisolate_freepages_range(struct compact_control *cc,\n\t\t\tunsigned long start_pfn, unsigned long end_pfn);\nint\nisolate_migratepages_range(struct compact_control *cc,\n\t\t\t   unsigned long low_pfn, unsigned long end_pfn);\n\nint __alloc_contig_migrate_range(struct compact_control *cc,\n\t\t\t\t\tunsigned long start, unsigned long end);\n\n \nvoid init_cma_reserved_pageblock(struct page *page);\n\n#endif  \n\nint find_suitable_fallback(struct free_area *area, unsigned int order,\n\t\t\tint migratetype, bool only_stealable, bool *can_steal);\n\nstatic inline bool free_area_empty(struct free_area *area, int migratetype)\n{\n\treturn list_empty(&area->free_list[migratetype]);\n}\n\n \n\n \nstatic inline bool is_exec_mapping(vm_flags_t flags)\n{\n\treturn (flags & (VM_EXEC | VM_WRITE | VM_STACK)) == VM_EXEC;\n}\n\n \nstatic inline bool is_stack_mapping(vm_flags_t flags)\n{\n\treturn ((flags & VM_STACK) == VM_STACK) || (flags & VM_SHADOW_STACK);\n}\n\n \nstatic inline bool is_data_mapping(vm_flags_t flags)\n{\n\treturn (flags & (VM_WRITE | VM_SHARED | VM_STACK)) == VM_WRITE;\n}\n\n \nstruct anon_vma *folio_anon_vma(struct folio *folio);\n\n#ifdef CONFIG_MMU\nvoid unmap_mapping_folio(struct folio *folio);\nextern long populate_vma_page_range(struct vm_area_struct *vma,\n\t\tunsigned long start, unsigned long end, int *locked);\nextern long faultin_vma_page_range(struct vm_area_struct *vma,\n\t\t\t\t   unsigned long start, unsigned long end,\n\t\t\t\t   bool write, int *locked);\nextern bool mlock_future_ok(struct mm_struct *mm, unsigned long flags,\n\t\t\t       unsigned long bytes);\n \nvoid mlock_folio(struct folio *folio);\nstatic inline void mlock_vma_folio(struct folio *folio,\n\t\t\tstruct vm_area_struct *vma, bool compound)\n{\n\t \n\tif (unlikely((vma->vm_flags & (VM_LOCKED|VM_SPECIAL)) == VM_LOCKED) &&\n\t    (compound || !folio_test_large(folio)))\n\t\tmlock_folio(folio);\n}\n\nvoid munlock_folio(struct folio *folio);\nstatic inline void munlock_vma_folio(struct folio *folio,\n\t\t\tstruct vm_area_struct *vma, bool compound)\n{\n\tif (unlikely(vma->vm_flags & VM_LOCKED) &&\n\t    (compound || !folio_test_large(folio)))\n\t\tmunlock_folio(folio);\n}\n\nvoid mlock_new_folio(struct folio *folio);\nbool need_mlock_drain(int cpu);\nvoid mlock_drain_local(void);\nvoid mlock_drain_remote(int cpu);\n\nextern pmd_t maybe_pmd_mkwrite(pmd_t pmd, struct vm_area_struct *vma);\n\n \nstatic inline unsigned long\nvma_pgoff_address(pgoff_t pgoff, unsigned long nr_pages,\n\t\t  struct vm_area_struct *vma)\n{\n\tunsigned long address;\n\n\tif (pgoff >= vma->vm_pgoff) {\n\t\taddress = vma->vm_start +\n\t\t\t((pgoff - vma->vm_pgoff) << PAGE_SHIFT);\n\t\t \n\t\tif (address < vma->vm_start || address >= vma->vm_end)\n\t\t\taddress = -EFAULT;\n\t} else if (pgoff + nr_pages - 1 >= vma->vm_pgoff) {\n\t\t \n\t\taddress = vma->vm_start;\n\t} else {\n\t\taddress = -EFAULT;\n\t}\n\treturn address;\n}\n\n \nstatic inline unsigned long\nvma_address(struct page *page, struct vm_area_struct *vma)\n{\n\tVM_BUG_ON_PAGE(PageKsm(page), page);\t \n\treturn vma_pgoff_address(page_to_pgoff(page), compound_nr(page), vma);\n}\n\n \nstatic inline unsigned long vma_address_end(struct page_vma_mapped_walk *pvmw)\n{\n\tstruct vm_area_struct *vma = pvmw->vma;\n\tpgoff_t pgoff;\n\tunsigned long address;\n\n\t \n\tif (pvmw->nr_pages == 1)\n\t\treturn pvmw->address + PAGE_SIZE;\n\n\tpgoff = pvmw->pgoff + pvmw->nr_pages;\n\taddress = vma->vm_start + ((pgoff - vma->vm_pgoff) << PAGE_SHIFT);\n\t \n\tif (address < vma->vm_start || address > vma->vm_end)\n\t\taddress = vma->vm_end;\n\treturn address;\n}\n\nstatic inline struct file *maybe_unlock_mmap_for_io(struct vm_fault *vmf,\n\t\t\t\t\t\t    struct file *fpin)\n{\n\tint flags = vmf->flags;\n\n\tif (fpin)\n\t\treturn fpin;\n\n\t \n\tif (fault_flag_allow_retry_first(flags) &&\n\t    !(flags & FAULT_FLAG_RETRY_NOWAIT)) {\n\t\tfpin = get_file(vmf->vma->vm_file);\n\t\trelease_fault_lock(vmf);\n\t}\n\treturn fpin;\n}\n#else  \nstatic inline void unmap_mapping_folio(struct folio *folio) { }\nstatic inline void mlock_new_folio(struct folio *folio) { }\nstatic inline bool need_mlock_drain(int cpu) { return false; }\nstatic inline void mlock_drain_local(void) { }\nstatic inline void mlock_drain_remote(int cpu) { }\nstatic inline void vunmap_range_noflush(unsigned long start, unsigned long end)\n{\n}\n#endif  \n\n \n#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT\nDECLARE_STATIC_KEY_TRUE(deferred_pages);\n\nbool __init deferred_grow_zone(struct zone *zone, unsigned int order);\n#endif  \n\nenum mminit_level {\n\tMMINIT_WARNING,\n\tMMINIT_VERIFY,\n\tMMINIT_TRACE\n};\n\n#ifdef CONFIG_DEBUG_MEMORY_INIT\n\nextern int mminit_loglevel;\n\n#define mminit_dprintk(level, prefix, fmt, arg...) \\\ndo { \\\n\tif (level < mminit_loglevel) { \\\n\t\tif (level <= MMINIT_WARNING) \\\n\t\t\tpr_warn(\"mminit::\" prefix \" \" fmt, ##arg);\t\\\n\t\telse \\\n\t\t\tprintk(KERN_DEBUG \"mminit::\" prefix \" \" fmt, ##arg); \\\n\t} \\\n} while (0)\n\nextern void mminit_verify_pageflags_layout(void);\nextern void mminit_verify_zonelist(void);\n#else\n\nstatic inline void mminit_dprintk(enum mminit_level level,\n\t\t\t\tconst char *prefix, const char *fmt, ...)\n{\n}\n\nstatic inline void mminit_verify_pageflags_layout(void)\n{\n}\n\nstatic inline void mminit_verify_zonelist(void)\n{\n}\n#endif  \n\n#define NODE_RECLAIM_NOSCAN\t-2\n#define NODE_RECLAIM_FULL\t-1\n#define NODE_RECLAIM_SOME\t0\n#define NODE_RECLAIM_SUCCESS\t1\n\n#ifdef CONFIG_NUMA\nextern int node_reclaim(struct pglist_data *, gfp_t, unsigned int);\nextern int find_next_best_node(int node, nodemask_t *used_node_mask);\n#else\nstatic inline int node_reclaim(struct pglist_data *pgdat, gfp_t mask,\n\t\t\t\tunsigned int order)\n{\n\treturn NODE_RECLAIM_NOSCAN;\n}\nstatic inline int find_next_best_node(int node, nodemask_t *used_node_mask)\n{\n\treturn NUMA_NO_NODE;\n}\n#endif\n\n \nextern int hwpoison_filter(struct page *p);\n\nextern u32 hwpoison_filter_dev_major;\nextern u32 hwpoison_filter_dev_minor;\nextern u64 hwpoison_filter_flags_mask;\nextern u64 hwpoison_filter_flags_value;\nextern u64 hwpoison_filter_memcg;\nextern u32 hwpoison_filter_enable;\n\nextern unsigned long  __must_check vm_mmap_pgoff(struct file *, unsigned long,\n        unsigned long, unsigned long,\n        unsigned long, unsigned long);\n\nextern void set_pageblock_order(void);\nunsigned long reclaim_pages(struct list_head *folio_list);\nunsigned int reclaim_clean_pages_from_list(struct zone *zone,\n\t\t\t\t\t    struct list_head *folio_list);\n \n#define ALLOC_WMARK_MIN\t\tWMARK_MIN\n#define ALLOC_WMARK_LOW\t\tWMARK_LOW\n#define ALLOC_WMARK_HIGH\tWMARK_HIGH\n#define ALLOC_NO_WATERMARKS\t0x04  \n\n \n#define ALLOC_WMARK_MASK\t(ALLOC_NO_WATERMARKS-1)\n\n \n#ifdef CONFIG_MMU\n#define ALLOC_OOM\t\t0x08\n#else\n#define ALLOC_OOM\t\tALLOC_NO_WATERMARKS\n#endif\n\n#define ALLOC_NON_BLOCK\t\t 0x10  \n#define ALLOC_MIN_RESERVE\t 0x20  \n#define ALLOC_CPUSET\t\t 0x40  \n#define ALLOC_CMA\t\t 0x80  \n#ifdef CONFIG_ZONE_DMA32\n#define ALLOC_NOFRAGMENT\t0x100  \n#else\n#define ALLOC_NOFRAGMENT\t  0x0\n#endif\n#define ALLOC_HIGHATOMIC\t0x200  \n#define ALLOC_KSWAPD\t\t0x800  \n\n \n#define ALLOC_RESERVES (ALLOC_NON_BLOCK|ALLOC_MIN_RESERVE|ALLOC_HIGHATOMIC|ALLOC_OOM)\n\nenum ttu_flags;\nstruct tlbflush_unmap_batch;\n\n\n \nextern struct workqueue_struct *mm_percpu_wq;\n\n#ifdef CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH\nvoid try_to_unmap_flush(void);\nvoid try_to_unmap_flush_dirty(void);\nvoid flush_tlb_batched_pending(struct mm_struct *mm);\n#else\nstatic inline void try_to_unmap_flush(void)\n{\n}\nstatic inline void try_to_unmap_flush_dirty(void)\n{\n}\nstatic inline void flush_tlb_batched_pending(struct mm_struct *mm)\n{\n}\n#endif  \n\nextern const struct trace_print_flags pageflag_names[];\nextern const struct trace_print_flags pagetype_names[];\nextern const struct trace_print_flags vmaflag_names[];\nextern const struct trace_print_flags gfpflag_names[];\n\nstatic inline bool is_migrate_highatomic(enum migratetype migratetype)\n{\n\treturn migratetype == MIGRATE_HIGHATOMIC;\n}\n\nstatic inline bool is_migrate_highatomic_page(struct page *page)\n{\n\treturn get_pageblock_migratetype(page) == MIGRATE_HIGHATOMIC;\n}\n\nvoid setup_zone_pageset(struct zone *zone);\n\nstruct migration_target_control {\n\tint nid;\t\t \n\tnodemask_t *nmask;\n\tgfp_t gfp_mask;\n};\n\n \nsize_t splice_folio_into_pipe(struct pipe_inode_info *pipe,\n\t\t\t      struct folio *folio, loff_t fpos, size_t size);\n\n \n#ifdef CONFIG_MMU\nvoid __init vmalloc_init(void);\nint __must_check vmap_pages_range_noflush(unsigned long addr, unsigned long end,\n                pgprot_t prot, struct page **pages, unsigned int page_shift);\n#else\nstatic inline void vmalloc_init(void)\n{\n}\n\nstatic inline\nint __must_check vmap_pages_range_noflush(unsigned long addr, unsigned long end,\n                pgprot_t prot, struct page **pages, unsigned int page_shift)\n{\n\treturn -EINVAL;\n}\n#endif\n\nint __must_check __vmap_pages_range_noflush(unsigned long addr,\n\t\t\t       unsigned long end, pgprot_t prot,\n\t\t\t       struct page **pages, unsigned int page_shift);\n\nvoid vunmap_range_noflush(unsigned long start, unsigned long end);\n\nvoid __vunmap_range_noflush(unsigned long start, unsigned long end);\n\nint numa_migrate_prep(struct page *page, struct vm_area_struct *vma,\n\t\t      unsigned long addr, int page_nid, int *flags);\n\nvoid free_zone_device_page(struct page *page);\nint migrate_device_coherent_page(struct page *page);\n\n \nstruct folio *try_grab_folio(struct page *page, int refs, unsigned int flags);\nint __must_check try_grab_page(struct page *page, unsigned int flags);\n\n \nstruct page *follow_trans_huge_pmd(struct vm_area_struct *vma,\n\t\t\t\t   unsigned long addr, pmd_t *pmd,\n\t\t\t\t   unsigned int flags);\n\nenum {\n\t \n\tFOLL_TOUCH = 1 << 16,\n\t \n\tFOLL_TRIED = 1 << 17,\n\t \n\tFOLL_REMOTE = 1 << 18,\n\t \n\tFOLL_PIN = 1 << 19,\n\t \n\tFOLL_FAST_ONLY = 1 << 20,\n\t \n\tFOLL_UNLOCKABLE = 1 << 21,\n};\n\n \nstatic inline bool gup_must_unshare(struct vm_area_struct *vma,\n\t\t\t\t    unsigned int flags, struct page *page)\n{\n\t \n\tif ((flags & (FOLL_WRITE | FOLL_PIN)) != FOLL_PIN)\n\t\treturn false;\n\t \n\tif (!PageAnon(page)) {\n\t\t \n\t\tif (!(flags & FOLL_LONGTERM))\n\t\t\treturn false;\n\n\t\t \n\t\tif (!vma)\n\t\t\treturn true;\n\n\t\t \n\t\treturn is_cow_mapping(vma->vm_flags);\n\t}\n\n\t \n\tif (IS_ENABLED(CONFIG_HAVE_FAST_GUP))\n\t\tsmp_rmb();\n\n\t \n\tif (unlikely(!PageHead(page) && PageHuge(page)))\n\t\tpage = compound_head(page);\n\n\t \n\treturn !PageAnonExclusive(page);\n}\n\nextern bool mirrored_kernelcore;\nextern bool memblock_has_mirror(void);\n\nstatic inline bool vma_soft_dirty_enabled(struct vm_area_struct *vma)\n{\n\t \n\tif (!IS_ENABLED(CONFIG_MEM_SOFT_DIRTY))\n\t\treturn false;\n\n\t \n\treturn !(vma->vm_flags & VM_SOFTDIRTY);\n}\n\nstatic inline void vma_iter_config(struct vma_iterator *vmi,\n\t\tunsigned long index, unsigned long last)\n{\n\tMAS_BUG_ON(&vmi->mas, vmi->mas.node != MAS_START &&\n\t\t   (vmi->mas.index > index || vmi->mas.last < index));\n\t__mas_set_range(&vmi->mas, index, last - 1);\n}\n\n \nstatic inline int vma_iter_prealloc(struct vma_iterator *vmi,\n\t\tstruct vm_area_struct *vma)\n{\n\treturn mas_preallocate(&vmi->mas, vma, GFP_KERNEL);\n}\n\nstatic inline void vma_iter_clear(struct vma_iterator *vmi)\n{\n\tmas_store_prealloc(&vmi->mas, NULL);\n}\n\nstatic inline int vma_iter_clear_gfp(struct vma_iterator *vmi,\n\t\t\tunsigned long start, unsigned long end, gfp_t gfp)\n{\n\t__mas_set_range(&vmi->mas, start, end - 1);\n\tmas_store_gfp(&vmi->mas, NULL, gfp);\n\tif (unlikely(mas_is_err(&vmi->mas)))\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic inline struct vm_area_struct *vma_iter_load(struct vma_iterator *vmi)\n{\n\treturn mas_walk(&vmi->mas);\n}\n\n \nstatic inline void vma_iter_store(struct vma_iterator *vmi,\n\t\t\t\t  struct vm_area_struct *vma)\n{\n\n#if defined(CONFIG_DEBUG_VM_MAPLE_TREE)\n\tif (MAS_WARN_ON(&vmi->mas, vmi->mas.node != MAS_START &&\n\t\t\tvmi->mas.index > vma->vm_start)) {\n\t\tpr_warn(\"%lx > %lx\\n store vma %lx-%lx\\n into slot %lx-%lx\\n\",\n\t\t\tvmi->mas.index, vma->vm_start, vma->vm_start,\n\t\t\tvma->vm_end, vmi->mas.index, vmi->mas.last);\n\t}\n\tif (MAS_WARN_ON(&vmi->mas, vmi->mas.node != MAS_START &&\n\t\t\tvmi->mas.last <  vma->vm_start)) {\n\t\tpr_warn(\"%lx < %lx\\nstore vma %lx-%lx\\ninto slot %lx-%lx\\n\",\n\t\t       vmi->mas.last, vma->vm_start, vma->vm_start, vma->vm_end,\n\t\t       vmi->mas.index, vmi->mas.last);\n\t}\n#endif\n\n\tif (vmi->mas.node != MAS_START &&\n\t    ((vmi->mas.index > vma->vm_start) || (vmi->mas.last < vma->vm_start)))\n\t\tvma_iter_invalidate(vmi);\n\n\t__mas_set_range(&vmi->mas, vma->vm_start, vma->vm_end - 1);\n\tmas_store_prealloc(&vmi->mas, vma);\n}\n\nstatic inline int vma_iter_store_gfp(struct vma_iterator *vmi,\n\t\t\tstruct vm_area_struct *vma, gfp_t gfp)\n{\n\tif (vmi->mas.node != MAS_START &&\n\t    ((vmi->mas.index > vma->vm_start) || (vmi->mas.last < vma->vm_start)))\n\t\tvma_iter_invalidate(vmi);\n\n\t__mas_set_range(&vmi->mas, vma->vm_start, vma->vm_end - 1);\n\tmas_store_gfp(&vmi->mas, vma, gfp);\n\tif (unlikely(mas_is_err(&vmi->mas)))\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\n \nstruct vma_prepare {\n\tstruct vm_area_struct *vma;\n\tstruct vm_area_struct *adj_next;\n\tstruct file *file;\n\tstruct address_space *mapping;\n\tstruct anon_vma *anon_vma;\n\tstruct vm_area_struct *insert;\n\tstruct vm_area_struct *remove;\n\tstruct vm_area_struct *remove2;\n};\n#endif\t \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}