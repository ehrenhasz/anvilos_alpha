{
  "module_name": "mmap_lock.c",
  "hash_id": "babfccba43e606b1bb33f612e64840f1ea1286383da7bf8774a15ab46280015c",
  "original_prompt": "Ingested from linux-6.6.14/mm/mmap_lock.c",
  "human_readable_source": "\n#define CREATE_TRACE_POINTS\n#include <trace/events/mmap_lock.h>\n\n#include <linux/mm.h>\n#include <linux/cgroup.h>\n#include <linux/memcontrol.h>\n#include <linux/mmap_lock.h>\n#include <linux/mutex.h>\n#include <linux/percpu.h>\n#include <linux/rcupdate.h>\n#include <linux/smp.h>\n#include <linux/trace_events.h>\n#include <linux/local_lock.h>\n\nEXPORT_TRACEPOINT_SYMBOL(mmap_lock_start_locking);\nEXPORT_TRACEPOINT_SYMBOL(mmap_lock_acquire_returned);\nEXPORT_TRACEPOINT_SYMBOL(mmap_lock_released);\n\n#ifdef CONFIG_MEMCG\n\n \nstatic DEFINE_MUTEX(reg_lock);\nstatic int reg_refcount;  \n\n \n#define MEMCG_PATH_BUF_SIZE MAX_FILTER_STR_VAL\n\n \n#define CONTEXT_COUNT 4\n\nstruct memcg_path {\n\tlocal_lock_t lock;\n\tchar __rcu *buf;\n\tlocal_t buf_idx;\n};\nstatic DEFINE_PER_CPU(struct memcg_path, memcg_paths) = {\n\t.lock = INIT_LOCAL_LOCK(lock),\n\t.buf_idx = LOCAL_INIT(0),\n};\n\nstatic char **tmp_bufs;\n\n \nstatic void free_memcg_path_bufs(void)\n{\n\tstruct memcg_path *memcg_path;\n\tint cpu;\n\tchar **old = tmp_bufs;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tmemcg_path = per_cpu_ptr(&memcg_paths, cpu);\n\t\t*(old++) = rcu_dereference_protected(memcg_path->buf,\n\t\t\tlockdep_is_held(&reg_lock));\n\t\trcu_assign_pointer(memcg_path->buf, NULL);\n\t}\n\n\t \n\tsynchronize_rcu();\n\n\told = tmp_bufs;\n\tfor_each_possible_cpu(cpu) {\n\t\tkfree(*(old++));\n\t}\n\n\tkfree(tmp_bufs);\n\ttmp_bufs = NULL;\n}\n\nint trace_mmap_lock_reg(void)\n{\n\tint cpu;\n\tchar *new;\n\n\tmutex_lock(&reg_lock);\n\n\t \n\tif (reg_refcount++)\n\t\tgoto out;\n\n\ttmp_bufs = kmalloc_array(num_possible_cpus(), sizeof(*tmp_bufs),\n\t\t\t\t GFP_KERNEL);\n\tif (tmp_bufs == NULL)\n\t\tgoto out_fail;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tnew = kmalloc(MEMCG_PATH_BUF_SIZE * CONTEXT_COUNT, GFP_KERNEL);\n\t\tif (new == NULL)\n\t\t\tgoto out_fail_free;\n\t\trcu_assign_pointer(per_cpu_ptr(&memcg_paths, cpu)->buf, new);\n\t\t \n\t}\n\nout:\n\tmutex_unlock(&reg_lock);\n\treturn 0;\n\nout_fail_free:\n\tfree_memcg_path_bufs();\nout_fail:\n\t \n\t--reg_refcount;\n\n\tmutex_unlock(&reg_lock);\n\treturn -ENOMEM;\n}\n\nvoid trace_mmap_lock_unreg(void)\n{\n\tmutex_lock(&reg_lock);\n\n\t \n\tif (--reg_refcount)\n\t\tgoto out;\n\n\tfree_memcg_path_bufs();\n\nout:\n\tmutex_unlock(&reg_lock);\n}\n\nstatic inline char *get_memcg_path_buf(void)\n{\n\tstruct memcg_path *memcg_path = this_cpu_ptr(&memcg_paths);\n\tchar *buf;\n\tint idx;\n\n\trcu_read_lock();\n\tbuf = rcu_dereference(memcg_path->buf);\n\tif (buf == NULL) {\n\t\trcu_read_unlock();\n\t\treturn NULL;\n\t}\n\tidx = local_add_return(MEMCG_PATH_BUF_SIZE, &memcg_path->buf_idx) -\n\t      MEMCG_PATH_BUF_SIZE;\n\treturn &buf[idx];\n}\n\nstatic inline void put_memcg_path_buf(void)\n{\n\tlocal_sub(MEMCG_PATH_BUF_SIZE, &this_cpu_ptr(&memcg_paths)->buf_idx);\n\trcu_read_unlock();\n}\n\n#define TRACE_MMAP_LOCK_EVENT(type, mm, ...)                                   \\\n\tdo {                                                                   \\\n\t\tconst char *memcg_path;                                        \\\n\t\tlocal_lock(&memcg_paths.lock);                                 \\\n\t\tmemcg_path = get_mm_memcg_path(mm);                            \\\n\t\ttrace_mmap_lock_##type(mm,                                     \\\n\t\t\t\t       memcg_path != NULL ? memcg_path : \"\",   \\\n\t\t\t\t       ##__VA_ARGS__);                         \\\n\t\tif (likely(memcg_path != NULL))                                \\\n\t\t\tput_memcg_path_buf();                                  \\\n\t\tlocal_unlock(&memcg_paths.lock);                               \\\n\t} while (0)\n\n#else  \n\nint trace_mmap_lock_reg(void)\n{\n\treturn 0;\n}\n\nvoid trace_mmap_lock_unreg(void)\n{\n}\n\n#define TRACE_MMAP_LOCK_EVENT(type, mm, ...)                                   \\\n\ttrace_mmap_lock_##type(mm, \"\", ##__VA_ARGS__)\n\n#endif  \n\n#ifdef CONFIG_TRACING\n#ifdef CONFIG_MEMCG\n \nstatic const char *get_mm_memcg_path(struct mm_struct *mm)\n{\n\tchar *buf = NULL;\n\tstruct mem_cgroup *memcg = get_mem_cgroup_from_mm(mm);\n\n\tif (memcg == NULL)\n\t\tgoto out;\n\tif (unlikely(memcg->css.cgroup == NULL))\n\t\tgoto out_put;\n\n\tbuf = get_memcg_path_buf();\n\tif (buf == NULL)\n\t\tgoto out_put;\n\n\tcgroup_path(memcg->css.cgroup, buf, MEMCG_PATH_BUF_SIZE);\n\nout_put:\n\tcss_put(&memcg->css);\nout:\n\treturn buf;\n}\n\n#endif  \n\n \n\nvoid __mmap_lock_do_trace_start_locking(struct mm_struct *mm, bool write)\n{\n\tTRACE_MMAP_LOCK_EVENT(start_locking, mm, write);\n}\nEXPORT_SYMBOL(__mmap_lock_do_trace_start_locking);\n\nvoid __mmap_lock_do_trace_acquire_returned(struct mm_struct *mm, bool write,\n\t\t\t\t\t   bool success)\n{\n\tTRACE_MMAP_LOCK_EVENT(acquire_returned, mm, write, success);\n}\nEXPORT_SYMBOL(__mmap_lock_do_trace_acquire_returned);\n\nvoid __mmap_lock_do_trace_released(struct mm_struct *mm, bool write)\n{\n\tTRACE_MMAP_LOCK_EVENT(released, mm, write);\n}\nEXPORT_SYMBOL(__mmap_lock_do_trace_released);\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}