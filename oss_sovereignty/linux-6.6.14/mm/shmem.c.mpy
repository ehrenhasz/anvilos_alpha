{
  "module_name": "shmem.c",
  "hash_id": "795092722613ebef37ee56c282ee479c9cd21bf88bfba4621c1af6c723697683",
  "original_prompt": "Ingested from linux-6.6.14/mm/shmem.c",
  "human_readable_source": " \n\n#include <linux/fs.h>\n#include <linux/init.h>\n#include <linux/vfs.h>\n#include <linux/mount.h>\n#include <linux/ramfs.h>\n#include <linux/pagemap.h>\n#include <linux/file.h>\n#include <linux/fileattr.h>\n#include <linux/mm.h>\n#include <linux/random.h>\n#include <linux/sched/signal.h>\n#include <linux/export.h>\n#include <linux/shmem_fs.h>\n#include <linux/swap.h>\n#include <linux/uio.h>\n#include <linux/hugetlb.h>\n#include <linux/fs_parser.h>\n#include <linux/swapfile.h>\n#include <linux/iversion.h>\n#include \"swap.h\"\n\nstatic struct vfsmount *shm_mnt;\n\n#ifdef CONFIG_SHMEM\n \n\n#include <linux/xattr.h>\n#include <linux/exportfs.h>\n#include <linux/posix_acl.h>\n#include <linux/posix_acl_xattr.h>\n#include <linux/mman.h>\n#include <linux/string.h>\n#include <linux/slab.h>\n#include <linux/backing-dev.h>\n#include <linux/writeback.h>\n#include <linux/pagevec.h>\n#include <linux/percpu_counter.h>\n#include <linux/falloc.h>\n#include <linux/splice.h>\n#include <linux/security.h>\n#include <linux/swapops.h>\n#include <linux/mempolicy.h>\n#include <linux/namei.h>\n#include <linux/ctype.h>\n#include <linux/migrate.h>\n#include <linux/highmem.h>\n#include <linux/seq_file.h>\n#include <linux/magic.h>\n#include <linux/syscalls.h>\n#include <linux/fcntl.h>\n#include <uapi/linux/memfd.h>\n#include <linux/rmap.h>\n#include <linux/uuid.h>\n#include <linux/quotaops.h>\n\n#include <linux/uaccess.h>\n\n#include \"internal.h\"\n\n#define BLOCKS_PER_PAGE  (PAGE_SIZE/512)\n#define VM_ACCT(size)    (PAGE_ALIGN(size) >> PAGE_SHIFT)\n\n \n#define BOGO_DIRENT_SIZE 20\n\n \n#define BOGO_INODE_SIZE 1024\n\n \n#define SHORT_SYMLINK_LEN 128\n\n \nstruct shmem_falloc {\n\twait_queue_head_t *waitq;  \n\tpgoff_t start;\t\t \n\tpgoff_t next;\t\t \n\tpgoff_t nr_falloced;\t \n\tpgoff_t nr_unswapped;\t \n};\n\nstruct shmem_options {\n\tunsigned long long blocks;\n\tunsigned long long inodes;\n\tstruct mempolicy *mpol;\n\tkuid_t uid;\n\tkgid_t gid;\n\tumode_t mode;\n\tbool full_inums;\n\tint huge;\n\tint seen;\n\tbool noswap;\n\tunsigned short quota_types;\n\tstruct shmem_quota_limits qlimits;\n#define SHMEM_SEEN_BLOCKS 1\n#define SHMEM_SEEN_INODES 2\n#define SHMEM_SEEN_HUGE 4\n#define SHMEM_SEEN_INUMS 8\n#define SHMEM_SEEN_NOSWAP 16\n#define SHMEM_SEEN_QUOTA 32\n};\n\n#ifdef CONFIG_TMPFS\nstatic unsigned long shmem_default_max_blocks(void)\n{\n\treturn totalram_pages() / 2;\n}\n\nstatic unsigned long shmem_default_max_inodes(void)\n{\n\tunsigned long nr_pages = totalram_pages();\n\n\treturn min3(nr_pages - totalhigh_pages(), nr_pages / 2,\n\t\t\tULONG_MAX / BOGO_INODE_SIZE);\n}\n#endif\n\nstatic int shmem_swapin_folio(struct inode *inode, pgoff_t index,\n\t\t\t     struct folio **foliop, enum sgp_type sgp,\n\t\t\t     gfp_t gfp, struct vm_area_struct *vma,\n\t\t\t     vm_fault_t *fault_type);\n\nstatic inline struct shmem_sb_info *SHMEM_SB(struct super_block *sb)\n{\n\treturn sb->s_fs_info;\n}\n\n \nstatic inline int shmem_acct_size(unsigned long flags, loff_t size)\n{\n\treturn (flags & VM_NORESERVE) ?\n\t\t0 : security_vm_enough_memory_mm(current->mm, VM_ACCT(size));\n}\n\nstatic inline void shmem_unacct_size(unsigned long flags, loff_t size)\n{\n\tif (!(flags & VM_NORESERVE))\n\t\tvm_unacct_memory(VM_ACCT(size));\n}\n\nstatic inline int shmem_reacct_size(unsigned long flags,\n\t\tloff_t oldsize, loff_t newsize)\n{\n\tif (!(flags & VM_NORESERVE)) {\n\t\tif (VM_ACCT(newsize) > VM_ACCT(oldsize))\n\t\t\treturn security_vm_enough_memory_mm(current->mm,\n\t\t\t\t\tVM_ACCT(newsize) - VM_ACCT(oldsize));\n\t\telse if (VM_ACCT(newsize) < VM_ACCT(oldsize))\n\t\t\tvm_unacct_memory(VM_ACCT(oldsize) - VM_ACCT(newsize));\n\t}\n\treturn 0;\n}\n\n \nstatic inline int shmem_acct_block(unsigned long flags, long pages)\n{\n\tif (!(flags & VM_NORESERVE))\n\t\treturn 0;\n\n\treturn security_vm_enough_memory_mm(current->mm,\n\t\t\tpages * VM_ACCT(PAGE_SIZE));\n}\n\nstatic inline void shmem_unacct_blocks(unsigned long flags, long pages)\n{\n\tif (flags & VM_NORESERVE)\n\t\tvm_unacct_memory(pages * VM_ACCT(PAGE_SIZE));\n}\n\nstatic int shmem_inode_acct_block(struct inode *inode, long pages)\n{\n\tstruct shmem_inode_info *info = SHMEM_I(inode);\n\tstruct shmem_sb_info *sbinfo = SHMEM_SB(inode->i_sb);\n\tint err = -ENOSPC;\n\n\tif (shmem_acct_block(info->flags, pages))\n\t\treturn err;\n\n\tmight_sleep();\t \n\tif (sbinfo->max_blocks) {\n\t\tif (percpu_counter_compare(&sbinfo->used_blocks,\n\t\t\t\t\t   sbinfo->max_blocks - pages) > 0)\n\t\t\tgoto unacct;\n\n\t\terr = dquot_alloc_block_nodirty(inode, pages);\n\t\tif (err)\n\t\t\tgoto unacct;\n\n\t\tpercpu_counter_add(&sbinfo->used_blocks, pages);\n\t} else {\n\t\terr = dquot_alloc_block_nodirty(inode, pages);\n\t\tif (err)\n\t\t\tgoto unacct;\n\t}\n\n\treturn 0;\n\nunacct:\n\tshmem_unacct_blocks(info->flags, pages);\n\treturn err;\n}\n\nstatic void shmem_inode_unacct_blocks(struct inode *inode, long pages)\n{\n\tstruct shmem_inode_info *info = SHMEM_I(inode);\n\tstruct shmem_sb_info *sbinfo = SHMEM_SB(inode->i_sb);\n\n\tmight_sleep();\t \n\tdquot_free_block_nodirty(inode, pages);\n\n\tif (sbinfo->max_blocks)\n\t\tpercpu_counter_sub(&sbinfo->used_blocks, pages);\n\tshmem_unacct_blocks(info->flags, pages);\n}\n\nstatic const struct super_operations shmem_ops;\nconst struct address_space_operations shmem_aops;\nstatic const struct file_operations shmem_file_operations;\nstatic const struct inode_operations shmem_inode_operations;\nstatic const struct inode_operations shmem_dir_inode_operations;\nstatic const struct inode_operations shmem_special_inode_operations;\nstatic const struct vm_operations_struct shmem_vm_ops;\nstatic const struct vm_operations_struct shmem_anon_vm_ops;\nstatic struct file_system_type shmem_fs_type;\n\nbool vma_is_anon_shmem(struct vm_area_struct *vma)\n{\n\treturn vma->vm_ops == &shmem_anon_vm_ops;\n}\n\nbool vma_is_shmem(struct vm_area_struct *vma)\n{\n\treturn vma_is_anon_shmem(vma) || vma->vm_ops == &shmem_vm_ops;\n}\n\nstatic LIST_HEAD(shmem_swaplist);\nstatic DEFINE_MUTEX(shmem_swaplist_mutex);\n\n#ifdef CONFIG_TMPFS_QUOTA\n\nstatic int shmem_enable_quotas(struct super_block *sb,\n\t\t\t       unsigned short quota_types)\n{\n\tint type, err = 0;\n\n\tsb_dqopt(sb)->flags |= DQUOT_QUOTA_SYS_FILE | DQUOT_NOLIST_DIRTY;\n\tfor (type = 0; type < SHMEM_MAXQUOTAS; type++) {\n\t\tif (!(quota_types & (1 << type)))\n\t\t\tcontinue;\n\t\terr = dquot_load_quota_sb(sb, type, QFMT_SHMEM,\n\t\t\t\t\t  DQUOT_USAGE_ENABLED |\n\t\t\t\t\t  DQUOT_LIMITS_ENABLED);\n\t\tif (err)\n\t\t\tgoto out_err;\n\t}\n\treturn 0;\n\nout_err:\n\tpr_warn(\"tmpfs: failed to enable quota tracking (type=%d, err=%d)\\n\",\n\t\ttype, err);\n\tfor (type--; type >= 0; type--)\n\t\tdquot_quota_off(sb, type);\n\treturn err;\n}\n\nstatic void shmem_disable_quotas(struct super_block *sb)\n{\n\tint type;\n\n\tfor (type = 0; type < SHMEM_MAXQUOTAS; type++)\n\t\tdquot_quota_off(sb, type);\n}\n\nstatic struct dquot **shmem_get_dquots(struct inode *inode)\n{\n\treturn SHMEM_I(inode)->i_dquot;\n}\n#endif  \n\n \n#define SHMEM_INO_BATCH 1024\nstatic int shmem_reserve_inode(struct super_block *sb, ino_t *inop)\n{\n\tstruct shmem_sb_info *sbinfo = SHMEM_SB(sb);\n\tino_t ino;\n\n\tif (!(sb->s_flags & SB_KERNMOUNT)) {\n\t\traw_spin_lock(&sbinfo->stat_lock);\n\t\tif (sbinfo->max_inodes) {\n\t\t\tif (sbinfo->free_ispace < BOGO_INODE_SIZE) {\n\t\t\t\traw_spin_unlock(&sbinfo->stat_lock);\n\t\t\t\treturn -ENOSPC;\n\t\t\t}\n\t\t\tsbinfo->free_ispace -= BOGO_INODE_SIZE;\n\t\t}\n\t\tif (inop) {\n\t\t\tino = sbinfo->next_ino++;\n\t\t\tif (unlikely(is_zero_ino(ino)))\n\t\t\t\tino = sbinfo->next_ino++;\n\t\t\tif (unlikely(!sbinfo->full_inums &&\n\t\t\t\t     ino > UINT_MAX)) {\n\t\t\t\t \n\t\t\t\tif (IS_ENABLED(CONFIG_64BIT))\n\t\t\t\t\tpr_warn(\"%s: inode number overflow on device %d, consider using inode64 mount option\\n\",\n\t\t\t\t\t\t__func__, MINOR(sb->s_dev));\n\t\t\t\tsbinfo->next_ino = 1;\n\t\t\t\tino = sbinfo->next_ino++;\n\t\t\t}\n\t\t\t*inop = ino;\n\t\t}\n\t\traw_spin_unlock(&sbinfo->stat_lock);\n\t} else if (inop) {\n\t\t \n\t\tino_t *next_ino;\n\n\t\tnext_ino = per_cpu_ptr(sbinfo->ino_batch, get_cpu());\n\t\tino = *next_ino;\n\t\tif (unlikely(ino % SHMEM_INO_BATCH == 0)) {\n\t\t\traw_spin_lock(&sbinfo->stat_lock);\n\t\t\tino = sbinfo->next_ino;\n\t\t\tsbinfo->next_ino += SHMEM_INO_BATCH;\n\t\t\traw_spin_unlock(&sbinfo->stat_lock);\n\t\t\tif (unlikely(is_zero_ino(ino)))\n\t\t\t\tino++;\n\t\t}\n\t\t*inop = ino;\n\t\t*next_ino = ++ino;\n\t\tput_cpu();\n\t}\n\n\treturn 0;\n}\n\nstatic void shmem_free_inode(struct super_block *sb, size_t freed_ispace)\n{\n\tstruct shmem_sb_info *sbinfo = SHMEM_SB(sb);\n\tif (sbinfo->max_inodes) {\n\t\traw_spin_lock(&sbinfo->stat_lock);\n\t\tsbinfo->free_ispace += BOGO_INODE_SIZE + freed_ispace;\n\t\traw_spin_unlock(&sbinfo->stat_lock);\n\t}\n}\n\n \nstatic void shmem_recalc_inode(struct inode *inode, long alloced, long swapped)\n{\n\tstruct shmem_inode_info *info = SHMEM_I(inode);\n\tlong freed;\n\n\tspin_lock(&info->lock);\n\tinfo->alloced += alloced;\n\tinfo->swapped += swapped;\n\tfreed = info->alloced - info->swapped -\n\t\tREAD_ONCE(inode->i_mapping->nrpages);\n\t \n\tif (swapped > 0)\n\t\tfreed += swapped;\n\tif (freed > 0)\n\t\tinfo->alloced -= freed;\n\tspin_unlock(&info->lock);\n\n\t \n\tif (freed > 0)\n\t\tshmem_inode_unacct_blocks(inode, freed);\n}\n\nbool shmem_charge(struct inode *inode, long pages)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\n\tif (shmem_inode_acct_block(inode, pages))\n\t\treturn false;\n\n\t \n\txa_lock_irq(&mapping->i_pages);\n\tmapping->nrpages += pages;\n\txa_unlock_irq(&mapping->i_pages);\n\n\tshmem_recalc_inode(inode, pages, 0);\n\treturn true;\n}\n\nvoid shmem_uncharge(struct inode *inode, long pages)\n{\n\t \n\t \n\n\tshmem_recalc_inode(inode, 0, 0);\n}\n\n \nstatic int shmem_replace_entry(struct address_space *mapping,\n\t\t\tpgoff_t index, void *expected, void *replacement)\n{\n\tXA_STATE(xas, &mapping->i_pages, index);\n\tvoid *item;\n\n\tVM_BUG_ON(!expected);\n\tVM_BUG_ON(!replacement);\n\titem = xas_load(&xas);\n\tif (item != expected)\n\t\treturn -ENOENT;\n\txas_store(&xas, replacement);\n\treturn 0;\n}\n\n \nstatic bool shmem_confirm_swap(struct address_space *mapping,\n\t\t\t       pgoff_t index, swp_entry_t swap)\n{\n\treturn xa_load(&mapping->i_pages, index) == swp_to_radix_entry(swap);\n}\n\n \n\n#define SHMEM_HUGE_NEVER\t0\n#define SHMEM_HUGE_ALWAYS\t1\n#define SHMEM_HUGE_WITHIN_SIZE\t2\n#define SHMEM_HUGE_ADVISE\t3\n\n \n#define SHMEM_HUGE_DENY\t\t(-1)\n#define SHMEM_HUGE_FORCE\t(-2)\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n \n\nstatic int shmem_huge __read_mostly = SHMEM_HUGE_NEVER;\n\nbool shmem_is_huge(struct inode *inode, pgoff_t index, bool shmem_huge_force,\n\t\t   struct mm_struct *mm, unsigned long vm_flags)\n{\n\tloff_t i_size;\n\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn false;\n\tif (mm && ((vm_flags & VM_NOHUGEPAGE) || test_bit(MMF_DISABLE_THP, &mm->flags)))\n\t\treturn false;\n\tif (shmem_huge == SHMEM_HUGE_DENY)\n\t\treturn false;\n\tif (shmem_huge_force || shmem_huge == SHMEM_HUGE_FORCE)\n\t\treturn true;\n\n\tswitch (SHMEM_SB(inode->i_sb)->huge) {\n\tcase SHMEM_HUGE_ALWAYS:\n\t\treturn true;\n\tcase SHMEM_HUGE_WITHIN_SIZE:\n\t\tindex = round_up(index + 1, HPAGE_PMD_NR);\n\t\ti_size = round_up(i_size_read(inode), PAGE_SIZE);\n\t\tif (i_size >> PAGE_SHIFT >= index)\n\t\t\treturn true;\n\t\tfallthrough;\n\tcase SHMEM_HUGE_ADVISE:\n\t\tif (mm && (vm_flags & VM_HUGEPAGE))\n\t\t\treturn true;\n\t\tfallthrough;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n#if defined(CONFIG_SYSFS)\nstatic int shmem_parse_huge(const char *str)\n{\n\tif (!strcmp(str, \"never\"))\n\t\treturn SHMEM_HUGE_NEVER;\n\tif (!strcmp(str, \"always\"))\n\t\treturn SHMEM_HUGE_ALWAYS;\n\tif (!strcmp(str, \"within_size\"))\n\t\treturn SHMEM_HUGE_WITHIN_SIZE;\n\tif (!strcmp(str, \"advise\"))\n\t\treturn SHMEM_HUGE_ADVISE;\n\tif (!strcmp(str, \"deny\"))\n\t\treturn SHMEM_HUGE_DENY;\n\tif (!strcmp(str, \"force\"))\n\t\treturn SHMEM_HUGE_FORCE;\n\treturn -EINVAL;\n}\n#endif\n\n#if defined(CONFIG_SYSFS) || defined(CONFIG_TMPFS)\nstatic const char *shmem_format_huge(int huge)\n{\n\tswitch (huge) {\n\tcase SHMEM_HUGE_NEVER:\n\t\treturn \"never\";\n\tcase SHMEM_HUGE_ALWAYS:\n\t\treturn \"always\";\n\tcase SHMEM_HUGE_WITHIN_SIZE:\n\t\treturn \"within_size\";\n\tcase SHMEM_HUGE_ADVISE:\n\t\treturn \"advise\";\n\tcase SHMEM_HUGE_DENY:\n\t\treturn \"deny\";\n\tcase SHMEM_HUGE_FORCE:\n\t\treturn \"force\";\n\tdefault:\n\t\tVM_BUG_ON(1);\n\t\treturn \"bad_val\";\n\t}\n}\n#endif\n\nstatic unsigned long shmem_unused_huge_shrink(struct shmem_sb_info *sbinfo,\n\t\tstruct shrink_control *sc, unsigned long nr_to_split)\n{\n\tLIST_HEAD(list), *pos, *next;\n\tLIST_HEAD(to_remove);\n\tstruct inode *inode;\n\tstruct shmem_inode_info *info;\n\tstruct folio *folio;\n\tunsigned long batch = sc ? sc->nr_to_scan : 128;\n\tint split = 0;\n\n\tif (list_empty(&sbinfo->shrinklist))\n\t\treturn SHRINK_STOP;\n\n\tspin_lock(&sbinfo->shrinklist_lock);\n\tlist_for_each_safe(pos, next, &sbinfo->shrinklist) {\n\t\tinfo = list_entry(pos, struct shmem_inode_info, shrinklist);\n\n\t\t \n\t\tinode = igrab(&info->vfs_inode);\n\n\t\t \n\t\tif (!inode) {\n\t\t\tlist_del_init(&info->shrinklist);\n\t\t\tgoto next;\n\t\t}\n\n\t\t \n\t\tif (round_up(inode->i_size, PAGE_SIZE) ==\n\t\t\t\tround_up(inode->i_size, HPAGE_PMD_SIZE)) {\n\t\t\tlist_move(&info->shrinklist, &to_remove);\n\t\t\tgoto next;\n\t\t}\n\n\t\tlist_move(&info->shrinklist, &list);\nnext:\n\t\tsbinfo->shrinklist_len--;\n\t\tif (!--batch)\n\t\t\tbreak;\n\t}\n\tspin_unlock(&sbinfo->shrinklist_lock);\n\n\tlist_for_each_safe(pos, next, &to_remove) {\n\t\tinfo = list_entry(pos, struct shmem_inode_info, shrinklist);\n\t\tinode = &info->vfs_inode;\n\t\tlist_del_init(&info->shrinklist);\n\t\tiput(inode);\n\t}\n\n\tlist_for_each_safe(pos, next, &list) {\n\t\tint ret;\n\t\tpgoff_t index;\n\n\t\tinfo = list_entry(pos, struct shmem_inode_info, shrinklist);\n\t\tinode = &info->vfs_inode;\n\n\t\tif (nr_to_split && split >= nr_to_split)\n\t\t\tgoto move_back;\n\n\t\tindex = (inode->i_size & HPAGE_PMD_MASK) >> PAGE_SHIFT;\n\t\tfolio = filemap_get_folio(inode->i_mapping, index);\n\t\tif (IS_ERR(folio))\n\t\t\tgoto drop;\n\n\t\t \n\t\tif (!folio_test_large(folio)) {\n\t\t\tfolio_put(folio);\n\t\t\tgoto drop;\n\t\t}\n\n\t\t \n\t\tif (!folio_trylock(folio)) {\n\t\t\tfolio_put(folio);\n\t\t\tgoto move_back;\n\t\t}\n\n\t\tret = split_folio(folio);\n\t\tfolio_unlock(folio);\n\t\tfolio_put(folio);\n\n\t\t \n\t\tif (ret)\n\t\t\tgoto move_back;\n\n\t\tsplit++;\ndrop:\n\t\tlist_del_init(&info->shrinklist);\n\t\tgoto put;\nmove_back:\n\t\t \n\t\tspin_lock(&sbinfo->shrinklist_lock);\n\t\tlist_move(&info->shrinklist, &sbinfo->shrinklist);\n\t\tsbinfo->shrinklist_len++;\n\t\tspin_unlock(&sbinfo->shrinklist_lock);\nput:\n\t\tiput(inode);\n\t}\n\n\treturn split;\n}\n\nstatic long shmem_unused_huge_scan(struct super_block *sb,\n\t\tstruct shrink_control *sc)\n{\n\tstruct shmem_sb_info *sbinfo = SHMEM_SB(sb);\n\n\tif (!READ_ONCE(sbinfo->shrinklist_len))\n\t\treturn SHRINK_STOP;\n\n\treturn shmem_unused_huge_shrink(sbinfo, sc, 0);\n}\n\nstatic long shmem_unused_huge_count(struct super_block *sb,\n\t\tstruct shrink_control *sc)\n{\n\tstruct shmem_sb_info *sbinfo = SHMEM_SB(sb);\n\treturn READ_ONCE(sbinfo->shrinklist_len);\n}\n#else  \n\n#define shmem_huge SHMEM_HUGE_DENY\n\nbool shmem_is_huge(struct inode *inode, pgoff_t index, bool shmem_huge_force,\n\t\t   struct mm_struct *mm, unsigned long vm_flags)\n{\n\treturn false;\n}\n\nstatic unsigned long shmem_unused_huge_shrink(struct shmem_sb_info *sbinfo,\n\t\tstruct shrink_control *sc, unsigned long nr_to_split)\n{\n\treturn 0;\n}\n#endif  \n\n \nstatic int shmem_add_to_page_cache(struct folio *folio,\n\t\t\t\t   struct address_space *mapping,\n\t\t\t\t   pgoff_t index, void *expected, gfp_t gfp,\n\t\t\t\t   struct mm_struct *charge_mm)\n{\n\tXA_STATE_ORDER(xas, &mapping->i_pages, index, folio_order(folio));\n\tlong nr = folio_nr_pages(folio);\n\tint error;\n\n\tVM_BUG_ON_FOLIO(index != round_down(index, nr), folio);\n\tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n\tVM_BUG_ON_FOLIO(!folio_test_swapbacked(folio), folio);\n\tVM_BUG_ON(expected && folio_test_large(folio));\n\n\tfolio_ref_add(folio, nr);\n\tfolio->mapping = mapping;\n\tfolio->index = index;\n\n\tif (!folio_test_swapcache(folio)) {\n\t\terror = mem_cgroup_charge(folio, charge_mm, gfp);\n\t\tif (error) {\n\t\t\tif (folio_test_pmd_mappable(folio)) {\n\t\t\t\tcount_vm_event(THP_FILE_FALLBACK);\n\t\t\t\tcount_vm_event(THP_FILE_FALLBACK_CHARGE);\n\t\t\t}\n\t\t\tgoto error;\n\t\t}\n\t}\n\tfolio_throttle_swaprate(folio, gfp);\n\n\tdo {\n\t\txas_lock_irq(&xas);\n\t\tif (expected != xas_find_conflict(&xas)) {\n\t\t\txas_set_err(&xas, -EEXIST);\n\t\t\tgoto unlock;\n\t\t}\n\t\tif (expected && xas_find_conflict(&xas)) {\n\t\t\txas_set_err(&xas, -EEXIST);\n\t\t\tgoto unlock;\n\t\t}\n\t\txas_store(&xas, folio);\n\t\tif (xas_error(&xas))\n\t\t\tgoto unlock;\n\t\tif (folio_test_pmd_mappable(folio)) {\n\t\t\tcount_vm_event(THP_FILE_ALLOC);\n\t\t\t__lruvec_stat_mod_folio(folio, NR_SHMEM_THPS, nr);\n\t\t}\n\t\tmapping->nrpages += nr;\n\t\t__lruvec_stat_mod_folio(folio, NR_FILE_PAGES, nr);\n\t\t__lruvec_stat_mod_folio(folio, NR_SHMEM, nr);\nunlock:\n\t\txas_unlock_irq(&xas);\n\t} while (xas_nomem(&xas, gfp));\n\n\tif (xas_error(&xas)) {\n\t\terror = xas_error(&xas);\n\t\tgoto error;\n\t}\n\n\treturn 0;\nerror:\n\tfolio->mapping = NULL;\n\tfolio_ref_sub(folio, nr);\n\treturn error;\n}\n\n \nstatic void shmem_delete_from_page_cache(struct folio *folio, void *radswap)\n{\n\tstruct address_space *mapping = folio->mapping;\n\tlong nr = folio_nr_pages(folio);\n\tint error;\n\n\txa_lock_irq(&mapping->i_pages);\n\terror = shmem_replace_entry(mapping, folio->index, folio, radswap);\n\tfolio->mapping = NULL;\n\tmapping->nrpages -= nr;\n\t__lruvec_stat_mod_folio(folio, NR_FILE_PAGES, -nr);\n\t__lruvec_stat_mod_folio(folio, NR_SHMEM, -nr);\n\txa_unlock_irq(&mapping->i_pages);\n\tfolio_put(folio);\n\tBUG_ON(error);\n}\n\n \nstatic int shmem_free_swap(struct address_space *mapping,\n\t\t\t   pgoff_t index, void *radswap)\n{\n\tvoid *old;\n\n\told = xa_cmpxchg_irq(&mapping->i_pages, index, radswap, NULL, 0);\n\tif (old != radswap)\n\t\treturn -ENOENT;\n\tfree_swap_and_cache(radix_to_swp_entry(radswap));\n\treturn 0;\n}\n\n \nunsigned long shmem_partial_swap_usage(struct address_space *mapping,\n\t\t\t\t\t\tpgoff_t start, pgoff_t end)\n{\n\tXA_STATE(xas, &mapping->i_pages, start);\n\tstruct page *page;\n\tunsigned long swapped = 0;\n\tunsigned long max = end - 1;\n\n\trcu_read_lock();\n\txas_for_each(&xas, page, max) {\n\t\tif (xas_retry(&xas, page))\n\t\t\tcontinue;\n\t\tif (xa_is_value(page))\n\t\t\tswapped++;\n\t\tif (xas.xa_index == max)\n\t\t\tbreak;\n\t\tif (need_resched()) {\n\t\t\txas_pause(&xas);\n\t\t\tcond_resched_rcu();\n\t\t}\n\t}\n\n\trcu_read_unlock();\n\n\treturn swapped << PAGE_SHIFT;\n}\n\n \nunsigned long shmem_swap_usage(struct vm_area_struct *vma)\n{\n\tstruct inode *inode = file_inode(vma->vm_file);\n\tstruct shmem_inode_info *info = SHMEM_I(inode);\n\tstruct address_space *mapping = inode->i_mapping;\n\tunsigned long swapped;\n\n\t \n\tswapped = READ_ONCE(info->swapped);\n\n\t \n\tif (!swapped)\n\t\treturn 0;\n\n\tif (!vma->vm_pgoff && vma->vm_end - vma->vm_start >= inode->i_size)\n\t\treturn swapped << PAGE_SHIFT;\n\n\t \n\treturn shmem_partial_swap_usage(mapping, vma->vm_pgoff,\n\t\t\t\t\tvma->vm_pgoff + vma_pages(vma));\n}\n\n \nvoid shmem_unlock_mapping(struct address_space *mapping)\n{\n\tstruct folio_batch fbatch;\n\tpgoff_t index = 0;\n\n\tfolio_batch_init(&fbatch);\n\t \n\twhile (!mapping_unevictable(mapping) &&\n\t       filemap_get_folios(mapping, &index, ~0UL, &fbatch)) {\n\t\tcheck_move_unevictable_folios(&fbatch);\n\t\tfolio_batch_release(&fbatch);\n\t\tcond_resched();\n\t}\n}\n\nstatic struct folio *shmem_get_partial_folio(struct inode *inode, pgoff_t index)\n{\n\tstruct folio *folio;\n\n\t \n\tfolio = filemap_get_entry(inode->i_mapping, index);\n\tif (!folio)\n\t\treturn folio;\n\tif (!xa_is_value(folio)) {\n\t\tfolio_lock(folio);\n\t\tif (folio->mapping == inode->i_mapping)\n\t\t\treturn folio;\n\t\t \n\t\tfolio_unlock(folio);\n\t\tfolio_put(folio);\n\t}\n\t \n\tfolio = NULL;\n\tshmem_get_folio(inode, index, &folio, SGP_READ);\n\treturn folio;\n}\n\n \nstatic void shmem_undo_range(struct inode *inode, loff_t lstart, loff_t lend,\n\t\t\t\t\t\t\t\t bool unfalloc)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct shmem_inode_info *info = SHMEM_I(inode);\n\tpgoff_t start = (lstart + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\tpgoff_t end = (lend + 1) >> PAGE_SHIFT;\n\tstruct folio_batch fbatch;\n\tpgoff_t indices[PAGEVEC_SIZE];\n\tstruct folio *folio;\n\tbool same_folio;\n\tlong nr_swaps_freed = 0;\n\tpgoff_t index;\n\tint i;\n\n\tif (lend == -1)\n\t\tend = -1;\t \n\n\tif (info->fallocend > start && info->fallocend <= end && !unfalloc)\n\t\tinfo->fallocend = start;\n\n\tfolio_batch_init(&fbatch);\n\tindex = start;\n\twhile (index < end && find_lock_entries(mapping, &index, end - 1,\n\t\t\t&fbatch, indices)) {\n\t\tfor (i = 0; i < folio_batch_count(&fbatch); i++) {\n\t\t\tfolio = fbatch.folios[i];\n\n\t\t\tif (xa_is_value(folio)) {\n\t\t\t\tif (unfalloc)\n\t\t\t\t\tcontinue;\n\t\t\t\tnr_swaps_freed += !shmem_free_swap(mapping,\n\t\t\t\t\t\t\tindices[i], folio);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (!unfalloc || !folio_test_uptodate(folio))\n\t\t\t\ttruncate_inode_folio(mapping, folio);\n\t\t\tfolio_unlock(folio);\n\t\t}\n\t\tfolio_batch_remove_exceptionals(&fbatch);\n\t\tfolio_batch_release(&fbatch);\n\t\tcond_resched();\n\t}\n\n\t \n\tif (unfalloc)\n\t\tgoto whole_folios;\n\n\tsame_folio = (lstart >> PAGE_SHIFT) == (lend >> PAGE_SHIFT);\n\tfolio = shmem_get_partial_folio(inode, lstart >> PAGE_SHIFT);\n\tif (folio) {\n\t\tsame_folio = lend < folio_pos(folio) + folio_size(folio);\n\t\tfolio_mark_dirty(folio);\n\t\tif (!truncate_inode_partial_folio(folio, lstart, lend)) {\n\t\t\tstart = folio_next_index(folio);\n\t\t\tif (same_folio)\n\t\t\t\tend = folio->index;\n\t\t}\n\t\tfolio_unlock(folio);\n\t\tfolio_put(folio);\n\t\tfolio = NULL;\n\t}\n\n\tif (!same_folio)\n\t\tfolio = shmem_get_partial_folio(inode, lend >> PAGE_SHIFT);\n\tif (folio) {\n\t\tfolio_mark_dirty(folio);\n\t\tif (!truncate_inode_partial_folio(folio, lstart, lend))\n\t\t\tend = folio->index;\n\t\tfolio_unlock(folio);\n\t\tfolio_put(folio);\n\t}\n\nwhole_folios:\n\n\tindex = start;\n\twhile (index < end) {\n\t\tcond_resched();\n\n\t\tif (!find_get_entries(mapping, &index, end - 1, &fbatch,\n\t\t\t\tindices)) {\n\t\t\t \n\t\t\tif (index == start || end != -1)\n\t\t\t\tbreak;\n\t\t\t \n\t\t\tindex = start;\n\t\t\tcontinue;\n\t\t}\n\t\tfor (i = 0; i < folio_batch_count(&fbatch); i++) {\n\t\t\tfolio = fbatch.folios[i];\n\n\t\t\tif (xa_is_value(folio)) {\n\t\t\t\tif (unfalloc)\n\t\t\t\t\tcontinue;\n\t\t\t\tif (shmem_free_swap(mapping, indices[i], folio)) {\n\t\t\t\t\t \n\t\t\t\t\tindex = indices[i];\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tnr_swaps_freed++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tfolio_lock(folio);\n\n\t\t\tif (!unfalloc || !folio_test_uptodate(folio)) {\n\t\t\t\tif (folio_mapping(folio) != mapping) {\n\t\t\t\t\t \n\t\t\t\t\tfolio_unlock(folio);\n\t\t\t\t\tindex = indices[i];\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tVM_BUG_ON_FOLIO(folio_test_writeback(folio),\n\t\t\t\t\t\tfolio);\n\n\t\t\t\tif (!folio_test_large(folio)) {\n\t\t\t\t\ttruncate_inode_folio(mapping, folio);\n\t\t\t\t} else if (truncate_inode_partial_folio(folio, lstart, lend)) {\n\t\t\t\t\t \n\t\t\t\t\tif (!folio_test_large(folio)) {\n\t\t\t\t\t\tfolio_unlock(folio);\n\t\t\t\t\t\tindex = start;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tfolio_unlock(folio);\n\t\t}\n\t\tfolio_batch_remove_exceptionals(&fbatch);\n\t\tfolio_batch_release(&fbatch);\n\t}\n\n\tshmem_recalc_inode(inode, 0, -nr_swaps_freed);\n}\n\nvoid shmem_truncate_range(struct inode *inode, loff_t lstart, loff_t lend)\n{\n\tshmem_undo_range(inode, lstart, lend, false);\n\tinode->i_mtime = inode_set_ctime_current(inode);\n\tinode_inc_iversion(inode);\n}\nEXPORT_SYMBOL_GPL(shmem_truncate_range);\n\nstatic int shmem_getattr(struct mnt_idmap *idmap,\n\t\t\t const struct path *path, struct kstat *stat,\n\t\t\t u32 request_mask, unsigned int query_flags)\n{\n\tstruct inode *inode = path->dentry->d_inode;\n\tstruct shmem_inode_info *info = SHMEM_I(inode);\n\n\tif (info->alloced - info->swapped != inode->i_mapping->nrpages)\n\t\tshmem_recalc_inode(inode, 0, 0);\n\n\tif (info->fsflags & FS_APPEND_FL)\n\t\tstat->attributes |= STATX_ATTR_APPEND;\n\tif (info->fsflags & FS_IMMUTABLE_FL)\n\t\tstat->attributes |= STATX_ATTR_IMMUTABLE;\n\tif (info->fsflags & FS_NODUMP_FL)\n\t\tstat->attributes |= STATX_ATTR_NODUMP;\n\tstat->attributes_mask |= (STATX_ATTR_APPEND |\n\t\t\tSTATX_ATTR_IMMUTABLE |\n\t\t\tSTATX_ATTR_NODUMP);\n\tgeneric_fillattr(idmap, request_mask, inode, stat);\n\n\tif (shmem_is_huge(inode, 0, false, NULL, 0))\n\t\tstat->blksize = HPAGE_PMD_SIZE;\n\n\tif (request_mask & STATX_BTIME) {\n\t\tstat->result_mask |= STATX_BTIME;\n\t\tstat->btime.tv_sec = info->i_crtime.tv_sec;\n\t\tstat->btime.tv_nsec = info->i_crtime.tv_nsec;\n\t}\n\n\treturn 0;\n}\n\nstatic int shmem_setattr(struct mnt_idmap *idmap,\n\t\t\t struct dentry *dentry, struct iattr *attr)\n{\n\tstruct inode *inode = d_inode(dentry);\n\tstruct shmem_inode_info *info = SHMEM_I(inode);\n\tint error;\n\tbool update_mtime = false;\n\tbool update_ctime = true;\n\n\terror = setattr_prepare(idmap, dentry, attr);\n\tif (error)\n\t\treturn error;\n\n\tif ((info->seals & F_SEAL_EXEC) && (attr->ia_valid & ATTR_MODE)) {\n\t\tif ((inode->i_mode ^ attr->ia_mode) & 0111) {\n\t\t\treturn -EPERM;\n\t\t}\n\t}\n\n\tif (S_ISREG(inode->i_mode) && (attr->ia_valid & ATTR_SIZE)) {\n\t\tloff_t oldsize = inode->i_size;\n\t\tloff_t newsize = attr->ia_size;\n\n\t\t \n\t\tif ((newsize < oldsize && (info->seals & F_SEAL_SHRINK)) ||\n\t\t    (newsize > oldsize && (info->seals & F_SEAL_GROW)))\n\t\t\treturn -EPERM;\n\n\t\tif (newsize != oldsize) {\n\t\t\terror = shmem_reacct_size(SHMEM_I(inode)->flags,\n\t\t\t\t\toldsize, newsize);\n\t\t\tif (error)\n\t\t\t\treturn error;\n\t\t\ti_size_write(inode, newsize);\n\t\t\tupdate_mtime = true;\n\t\t} else {\n\t\t\tupdate_ctime = false;\n\t\t}\n\t\tif (newsize <= oldsize) {\n\t\t\tloff_t holebegin = round_up(newsize, PAGE_SIZE);\n\t\t\tif (oldsize > holebegin)\n\t\t\t\tunmap_mapping_range(inode->i_mapping,\n\t\t\t\t\t\t\tholebegin, 0, 1);\n\t\t\tif (info->alloced)\n\t\t\t\tshmem_truncate_range(inode,\n\t\t\t\t\t\t\tnewsize, (loff_t)-1);\n\t\t\t \n\t\t\tif (oldsize > holebegin)\n\t\t\t\tunmap_mapping_range(inode->i_mapping,\n\t\t\t\t\t\t\tholebegin, 0, 1);\n\t\t}\n\t}\n\n\tif (is_quota_modification(idmap, inode, attr)) {\n\t\terror = dquot_initialize(inode);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\n\t \n\tif (i_uid_needs_update(idmap, attr, inode) ||\n\t    i_gid_needs_update(idmap, attr, inode)) {\n\t\terror = dquot_transfer(idmap, inode, attr);\n\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\n\tsetattr_copy(idmap, inode, attr);\n\tif (attr->ia_valid & ATTR_MODE)\n\t\terror = posix_acl_chmod(idmap, dentry, inode->i_mode);\n\tif (!error && update_ctime) {\n\t\tinode_set_ctime_current(inode);\n\t\tif (update_mtime)\n\t\t\tinode->i_mtime = inode_get_ctime(inode);\n\t\tinode_inc_iversion(inode);\n\t}\n\treturn error;\n}\n\nstatic void shmem_evict_inode(struct inode *inode)\n{\n\tstruct shmem_inode_info *info = SHMEM_I(inode);\n\tstruct shmem_sb_info *sbinfo = SHMEM_SB(inode->i_sb);\n\tsize_t freed = 0;\n\n\tif (shmem_mapping(inode->i_mapping)) {\n\t\tshmem_unacct_size(info->flags, inode->i_size);\n\t\tinode->i_size = 0;\n\t\tmapping_set_exiting(inode->i_mapping);\n\t\tshmem_truncate_range(inode, 0, (loff_t)-1);\n\t\tif (!list_empty(&info->shrinklist)) {\n\t\t\tspin_lock(&sbinfo->shrinklist_lock);\n\t\t\tif (!list_empty(&info->shrinklist)) {\n\t\t\t\tlist_del_init(&info->shrinklist);\n\t\t\t\tsbinfo->shrinklist_len--;\n\t\t\t}\n\t\t\tspin_unlock(&sbinfo->shrinklist_lock);\n\t\t}\n\t\twhile (!list_empty(&info->swaplist)) {\n\t\t\t \n\t\t\twait_var_event(&info->stop_eviction,\n\t\t\t\t       !atomic_read(&info->stop_eviction));\n\t\t\tmutex_lock(&shmem_swaplist_mutex);\n\t\t\t \n\t\t\tif (!atomic_read(&info->stop_eviction))\n\t\t\t\tlist_del_init(&info->swaplist);\n\t\t\tmutex_unlock(&shmem_swaplist_mutex);\n\t\t}\n\t}\n\n\tsimple_xattrs_free(&info->xattrs, sbinfo->max_inodes ? &freed : NULL);\n\tshmem_free_inode(inode->i_sb, freed);\n\tWARN_ON(inode->i_blocks);\n\tclear_inode(inode);\n#ifdef CONFIG_TMPFS_QUOTA\n\tdquot_free_inode(inode);\n\tdquot_drop(inode);\n#endif\n}\n\nstatic int shmem_find_swap_entries(struct address_space *mapping,\n\t\t\t\t   pgoff_t start, struct folio_batch *fbatch,\n\t\t\t\t   pgoff_t *indices, unsigned int type)\n{\n\tXA_STATE(xas, &mapping->i_pages, start);\n\tstruct folio *folio;\n\tswp_entry_t entry;\n\n\trcu_read_lock();\n\txas_for_each(&xas, folio, ULONG_MAX) {\n\t\tif (xas_retry(&xas, folio))\n\t\t\tcontinue;\n\n\t\tif (!xa_is_value(folio))\n\t\t\tcontinue;\n\n\t\tentry = radix_to_swp_entry(folio);\n\t\t \n\t\tif (swp_type(entry) != type)\n\t\t\tcontinue;\n\n\t\tindices[folio_batch_count(fbatch)] = xas.xa_index;\n\t\tif (!folio_batch_add(fbatch, folio))\n\t\t\tbreak;\n\n\t\tif (need_resched()) {\n\t\t\txas_pause(&xas);\n\t\t\tcond_resched_rcu();\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\treturn xas.xa_index;\n}\n\n \nstatic int shmem_unuse_swap_entries(struct inode *inode,\n\t\tstruct folio_batch *fbatch, pgoff_t *indices)\n{\n\tint i = 0;\n\tint ret = 0;\n\tint error = 0;\n\tstruct address_space *mapping = inode->i_mapping;\n\n\tfor (i = 0; i < folio_batch_count(fbatch); i++) {\n\t\tstruct folio *folio = fbatch->folios[i];\n\n\t\tif (!xa_is_value(folio))\n\t\t\tcontinue;\n\t\terror = shmem_swapin_folio(inode, indices[i],\n\t\t\t\t\t  &folio, SGP_CACHE,\n\t\t\t\t\t  mapping_gfp_mask(mapping),\n\t\t\t\t\t  NULL, NULL);\n\t\tif (error == 0) {\n\t\t\tfolio_unlock(folio);\n\t\t\tfolio_put(folio);\n\t\t\tret++;\n\t\t}\n\t\tif (error == -ENOMEM)\n\t\t\tbreak;\n\t\terror = 0;\n\t}\n\treturn error ? error : ret;\n}\n\n \nstatic int shmem_unuse_inode(struct inode *inode, unsigned int type)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tpgoff_t start = 0;\n\tstruct folio_batch fbatch;\n\tpgoff_t indices[PAGEVEC_SIZE];\n\tint ret = 0;\n\n\tdo {\n\t\tfolio_batch_init(&fbatch);\n\t\tshmem_find_swap_entries(mapping, start, &fbatch, indices, type);\n\t\tif (folio_batch_count(&fbatch) == 0) {\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tret = shmem_unuse_swap_entries(inode, &fbatch, indices);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\n\t\tstart = indices[folio_batch_count(&fbatch) - 1];\n\t} while (true);\n\n\treturn ret;\n}\n\n \nint shmem_unuse(unsigned int type)\n{\n\tstruct shmem_inode_info *info, *next;\n\tint error = 0;\n\n\tif (list_empty(&shmem_swaplist))\n\t\treturn 0;\n\n\tmutex_lock(&shmem_swaplist_mutex);\n\tlist_for_each_entry_safe(info, next, &shmem_swaplist, swaplist) {\n\t\tif (!info->swapped) {\n\t\t\tlist_del_init(&info->swaplist);\n\t\t\tcontinue;\n\t\t}\n\t\t \n\t\tatomic_inc(&info->stop_eviction);\n\t\tmutex_unlock(&shmem_swaplist_mutex);\n\n\t\terror = shmem_unuse_inode(&info->vfs_inode, type);\n\t\tcond_resched();\n\n\t\tmutex_lock(&shmem_swaplist_mutex);\n\t\tnext = list_next_entry(info, swaplist);\n\t\tif (!info->swapped)\n\t\t\tlist_del_init(&info->swaplist);\n\t\tif (atomic_dec_and_test(&info->stop_eviction))\n\t\t\twake_up_var(&info->stop_eviction);\n\t\tif (error)\n\t\t\tbreak;\n\t}\n\tmutex_unlock(&shmem_swaplist_mutex);\n\n\treturn error;\n}\n\n \nstatic int shmem_writepage(struct page *page, struct writeback_control *wbc)\n{\n\tstruct folio *folio = page_folio(page);\n\tstruct address_space *mapping = folio->mapping;\n\tstruct inode *inode = mapping->host;\n\tstruct shmem_inode_info *info = SHMEM_I(inode);\n\tstruct shmem_sb_info *sbinfo = SHMEM_SB(inode->i_sb);\n\tswp_entry_t swap;\n\tpgoff_t index;\n\n\t \n\tif (WARN_ON_ONCE(!wbc->for_reclaim))\n\t\tgoto redirty;\n\n\tif (WARN_ON_ONCE((info->flags & VM_LOCKED) || sbinfo->noswap))\n\t\tgoto redirty;\n\n\tif (!total_swap_pages)\n\t\tgoto redirty;\n\n\t \n\tif (folio_test_large(folio)) {\n\t\t \n\t\tfolio_test_set_dirty(folio);\n\t\tif (split_huge_page(page) < 0)\n\t\t\tgoto redirty;\n\t\tfolio = page_folio(page);\n\t\tfolio_clear_dirty(folio);\n\t}\n\n\tindex = folio->index;\n\n\t \n\tif (!folio_test_uptodate(folio)) {\n\t\tif (inode->i_private) {\n\t\t\tstruct shmem_falloc *shmem_falloc;\n\t\t\tspin_lock(&inode->i_lock);\n\t\t\tshmem_falloc = inode->i_private;\n\t\t\tif (shmem_falloc &&\n\t\t\t    !shmem_falloc->waitq &&\n\t\t\t    index >= shmem_falloc->start &&\n\t\t\t    index < shmem_falloc->next)\n\t\t\t\tshmem_falloc->nr_unswapped++;\n\t\t\telse\n\t\t\t\tshmem_falloc = NULL;\n\t\t\tspin_unlock(&inode->i_lock);\n\t\t\tif (shmem_falloc)\n\t\t\t\tgoto redirty;\n\t\t}\n\t\tfolio_zero_range(folio, 0, folio_size(folio));\n\t\tflush_dcache_folio(folio);\n\t\tfolio_mark_uptodate(folio);\n\t}\n\n\tswap = folio_alloc_swap(folio);\n\tif (!swap.val)\n\t\tgoto redirty;\n\n\t \n\tmutex_lock(&shmem_swaplist_mutex);\n\tif (list_empty(&info->swaplist))\n\t\tlist_add(&info->swaplist, &shmem_swaplist);\n\n\tif (add_to_swap_cache(folio, swap,\n\t\t\t__GFP_HIGH | __GFP_NOMEMALLOC | __GFP_NOWARN,\n\t\t\tNULL) == 0) {\n\t\tshmem_recalc_inode(inode, 0, 1);\n\t\tswap_shmem_alloc(swap);\n\t\tshmem_delete_from_page_cache(folio, swp_to_radix_entry(swap));\n\n\t\tmutex_unlock(&shmem_swaplist_mutex);\n\t\tBUG_ON(folio_mapped(folio));\n\t\tswap_writepage(&folio->page, wbc);\n\t\treturn 0;\n\t}\n\n\tmutex_unlock(&shmem_swaplist_mutex);\n\tput_swap_folio(folio, swap);\nredirty:\n\tfolio_mark_dirty(folio);\n\tif (wbc->for_reclaim)\n\t\treturn AOP_WRITEPAGE_ACTIVATE;\t \n\tfolio_unlock(folio);\n\treturn 0;\n}\n\n#if defined(CONFIG_NUMA) && defined(CONFIG_TMPFS)\nstatic void shmem_show_mpol(struct seq_file *seq, struct mempolicy *mpol)\n{\n\tchar buffer[64];\n\n\tif (!mpol || mpol->mode == MPOL_DEFAULT)\n\t\treturn;\t\t \n\n\tmpol_to_str(buffer, sizeof(buffer), mpol);\n\n\tseq_printf(seq, \",mpol=%s\", buffer);\n}\n\nstatic struct mempolicy *shmem_get_sbmpol(struct shmem_sb_info *sbinfo)\n{\n\tstruct mempolicy *mpol = NULL;\n\tif (sbinfo->mpol) {\n\t\traw_spin_lock(&sbinfo->stat_lock);\t \n\t\tmpol = sbinfo->mpol;\n\t\tmpol_get(mpol);\n\t\traw_spin_unlock(&sbinfo->stat_lock);\n\t}\n\treturn mpol;\n}\n#else  \nstatic inline void shmem_show_mpol(struct seq_file *seq, struct mempolicy *mpol)\n{\n}\nstatic inline struct mempolicy *shmem_get_sbmpol(struct shmem_sb_info *sbinfo)\n{\n\treturn NULL;\n}\n#endif  \n#ifndef CONFIG_NUMA\n#define vm_policy vm_private_data\n#endif\n\nstatic void shmem_pseudo_vma_init(struct vm_area_struct *vma,\n\t\tstruct shmem_inode_info *info, pgoff_t index)\n{\n\t \n\tvma_init(vma, NULL);\n\t \n\tvma->vm_pgoff = index + info->vfs_inode.i_ino;\n\tvma->vm_policy = mpol_shared_policy_lookup(&info->policy, index);\n}\n\nstatic void shmem_pseudo_vma_destroy(struct vm_area_struct *vma)\n{\n\t \n\tmpol_cond_put(vma->vm_policy);\n}\n\nstatic struct folio *shmem_swapin(swp_entry_t swap, gfp_t gfp,\n\t\t\tstruct shmem_inode_info *info, pgoff_t index)\n{\n\tstruct vm_area_struct pvma;\n\tstruct page *page;\n\tstruct vm_fault vmf = {\n\t\t.vma = &pvma,\n\t};\n\n\tshmem_pseudo_vma_init(&pvma, info, index);\n\tpage = swap_cluster_readahead(swap, gfp, &vmf);\n\tshmem_pseudo_vma_destroy(&pvma);\n\n\tif (!page)\n\t\treturn NULL;\n\treturn page_folio(page);\n}\n\n \nstatic gfp_t limit_gfp_mask(gfp_t huge_gfp, gfp_t limit_gfp)\n{\n\tgfp_t allowflags = __GFP_IO | __GFP_FS | __GFP_RECLAIM;\n\tgfp_t denyflags = __GFP_NOWARN | __GFP_NORETRY;\n\tgfp_t zoneflags = limit_gfp & GFP_ZONEMASK;\n\tgfp_t result = huge_gfp & ~(allowflags | GFP_ZONEMASK);\n\n\t \n\tresult |= zoneflags;\n\n\t \n\tresult |= (limit_gfp & denyflags);\n\tresult |= (huge_gfp & limit_gfp) & allowflags;\n\n\treturn result;\n}\n\nstatic struct folio *shmem_alloc_hugefolio(gfp_t gfp,\n\t\tstruct shmem_inode_info *info, pgoff_t index)\n{\n\tstruct vm_area_struct pvma;\n\tstruct address_space *mapping = info->vfs_inode.i_mapping;\n\tpgoff_t hindex;\n\tstruct folio *folio;\n\n\thindex = round_down(index, HPAGE_PMD_NR);\n\tif (xa_find(&mapping->i_pages, &hindex, hindex + HPAGE_PMD_NR - 1,\n\t\t\t\t\t\t\t\tXA_PRESENT))\n\t\treturn NULL;\n\n\tshmem_pseudo_vma_init(&pvma, info, hindex);\n\tfolio = vma_alloc_folio(gfp, HPAGE_PMD_ORDER, &pvma, 0, true);\n\tshmem_pseudo_vma_destroy(&pvma);\n\tif (!folio)\n\t\tcount_vm_event(THP_FILE_FALLBACK);\n\treturn folio;\n}\n\nstatic struct folio *shmem_alloc_folio(gfp_t gfp,\n\t\t\tstruct shmem_inode_info *info, pgoff_t index)\n{\n\tstruct vm_area_struct pvma;\n\tstruct folio *folio;\n\n\tshmem_pseudo_vma_init(&pvma, info, index);\n\tfolio = vma_alloc_folio(gfp, 0, &pvma, 0, false);\n\tshmem_pseudo_vma_destroy(&pvma);\n\n\treturn folio;\n}\n\nstatic struct folio *shmem_alloc_and_acct_folio(gfp_t gfp, struct inode *inode,\n\t\tpgoff_t index, bool huge)\n{\n\tstruct shmem_inode_info *info = SHMEM_I(inode);\n\tstruct folio *folio;\n\tint nr;\n\tint err;\n\n\tif (!IS_ENABLED(CONFIG_TRANSPARENT_HUGEPAGE))\n\t\thuge = false;\n\tnr = huge ? HPAGE_PMD_NR : 1;\n\n\terr = shmem_inode_acct_block(inode, nr);\n\tif (err)\n\t\tgoto failed;\n\n\tif (huge)\n\t\tfolio = shmem_alloc_hugefolio(gfp, info, index);\n\telse\n\t\tfolio = shmem_alloc_folio(gfp, info, index);\n\tif (folio) {\n\t\t__folio_set_locked(folio);\n\t\t__folio_set_swapbacked(folio);\n\t\treturn folio;\n\t}\n\n\terr = -ENOMEM;\n\tshmem_inode_unacct_blocks(inode, nr);\nfailed:\n\treturn ERR_PTR(err);\n}\n\n \nstatic bool shmem_should_replace_folio(struct folio *folio, gfp_t gfp)\n{\n\treturn folio_zonenum(folio) > gfp_zone(gfp);\n}\n\nstatic int shmem_replace_folio(struct folio **foliop, gfp_t gfp,\n\t\t\t\tstruct shmem_inode_info *info, pgoff_t index)\n{\n\tstruct folio *old, *new;\n\tstruct address_space *swap_mapping;\n\tswp_entry_t entry;\n\tpgoff_t swap_index;\n\tint error;\n\n\told = *foliop;\n\tentry = old->swap;\n\tswap_index = swp_offset(entry);\n\tswap_mapping = swap_address_space(entry);\n\n\t \n\tgfp &= ~GFP_CONSTRAINT_MASK;\n\tVM_BUG_ON_FOLIO(folio_test_large(old), old);\n\tnew = shmem_alloc_folio(gfp, info, index);\n\tif (!new)\n\t\treturn -ENOMEM;\n\n\tfolio_get(new);\n\tfolio_copy(new, old);\n\tflush_dcache_folio(new);\n\n\t__folio_set_locked(new);\n\t__folio_set_swapbacked(new);\n\tfolio_mark_uptodate(new);\n\tnew->swap = entry;\n\tfolio_set_swapcache(new);\n\n\t \n\txa_lock_irq(&swap_mapping->i_pages);\n\terror = shmem_replace_entry(swap_mapping, swap_index, old, new);\n\tif (!error) {\n\t\tmem_cgroup_migrate(old, new);\n\t\t__lruvec_stat_mod_folio(new, NR_FILE_PAGES, 1);\n\t\t__lruvec_stat_mod_folio(new, NR_SHMEM, 1);\n\t\t__lruvec_stat_mod_folio(old, NR_FILE_PAGES, -1);\n\t\t__lruvec_stat_mod_folio(old, NR_SHMEM, -1);\n\t}\n\txa_unlock_irq(&swap_mapping->i_pages);\n\n\tif (unlikely(error)) {\n\t\t \n\t\told = new;\n\t} else {\n\t\tfolio_add_lru(new);\n\t\t*foliop = new;\n\t}\n\n\tfolio_clear_swapcache(old);\n\told->private = NULL;\n\n\tfolio_unlock(old);\n\tfolio_put_refs(old, 2);\n\treturn error;\n}\n\nstatic void shmem_set_folio_swapin_error(struct inode *inode, pgoff_t index,\n\t\t\t\t\t struct folio *folio, swp_entry_t swap)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tswp_entry_t swapin_error;\n\tvoid *old;\n\n\tswapin_error = make_poisoned_swp_entry();\n\told = xa_cmpxchg_irq(&mapping->i_pages, index,\n\t\t\t     swp_to_radix_entry(swap),\n\t\t\t     swp_to_radix_entry(swapin_error), 0);\n\tif (old != swp_to_radix_entry(swap))\n\t\treturn;\n\n\tfolio_wait_writeback(folio);\n\tdelete_from_swap_cache(folio);\n\t \n\tshmem_recalc_inode(inode, -1, -1);\n\tswap_free(swap);\n}\n\n \nstatic int shmem_swapin_folio(struct inode *inode, pgoff_t index,\n\t\t\t     struct folio **foliop, enum sgp_type sgp,\n\t\t\t     gfp_t gfp, struct vm_area_struct *vma,\n\t\t\t     vm_fault_t *fault_type)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct shmem_inode_info *info = SHMEM_I(inode);\n\tstruct mm_struct *charge_mm = vma ? vma->vm_mm : NULL;\n\tstruct swap_info_struct *si;\n\tstruct folio *folio = NULL;\n\tswp_entry_t swap;\n\tint error;\n\n\tVM_BUG_ON(!*foliop || !xa_is_value(*foliop));\n\tswap = radix_to_swp_entry(*foliop);\n\t*foliop = NULL;\n\n\tif (is_poisoned_swp_entry(swap))\n\t\treturn -EIO;\n\n\tsi = get_swap_device(swap);\n\tif (!si) {\n\t\tif (!shmem_confirm_swap(mapping, index, swap))\n\t\t\treturn -EEXIST;\n\t\telse\n\t\t\treturn -EINVAL;\n\t}\n\n\t \n\tfolio = swap_cache_get_folio(swap, NULL, 0);\n\tif (!folio) {\n\t\t \n\t\tif (fault_type) {\n\t\t\t*fault_type |= VM_FAULT_MAJOR;\n\t\t\tcount_vm_event(PGMAJFAULT);\n\t\t\tcount_memcg_event_mm(charge_mm, PGMAJFAULT);\n\t\t}\n\t\t \n\t\tfolio = shmem_swapin(swap, gfp, info, index);\n\t\tif (!folio) {\n\t\t\terror = -ENOMEM;\n\t\t\tgoto failed;\n\t\t}\n\t}\n\n\t \n\tfolio_lock(folio);\n\tif (!folio_test_swapcache(folio) ||\n\t    folio->swap.val != swap.val ||\n\t    !shmem_confirm_swap(mapping, index, swap)) {\n\t\terror = -EEXIST;\n\t\tgoto unlock;\n\t}\n\tif (!folio_test_uptodate(folio)) {\n\t\terror = -EIO;\n\t\tgoto failed;\n\t}\n\tfolio_wait_writeback(folio);\n\n\t \n\tarch_swap_restore(swap, folio);\n\n\tif (shmem_should_replace_folio(folio, gfp)) {\n\t\terror = shmem_replace_folio(&folio, gfp, info, index);\n\t\tif (error)\n\t\t\tgoto failed;\n\t}\n\n\terror = shmem_add_to_page_cache(folio, mapping, index,\n\t\t\t\t\tswp_to_radix_entry(swap), gfp,\n\t\t\t\t\tcharge_mm);\n\tif (error)\n\t\tgoto failed;\n\n\tshmem_recalc_inode(inode, 0, -1);\n\n\tif (sgp == SGP_WRITE)\n\t\tfolio_mark_accessed(folio);\n\n\tdelete_from_swap_cache(folio);\n\tfolio_mark_dirty(folio);\n\tswap_free(swap);\n\tput_swap_device(si);\n\n\t*foliop = folio;\n\treturn 0;\nfailed:\n\tif (!shmem_confirm_swap(mapping, index, swap))\n\t\terror = -EEXIST;\n\tif (error == -EIO)\n\t\tshmem_set_folio_swapin_error(inode, index, folio, swap);\nunlock:\n\tif (folio) {\n\t\tfolio_unlock(folio);\n\t\tfolio_put(folio);\n\t}\n\tput_swap_device(si);\n\n\treturn error;\n}\n\n \nstatic int shmem_get_folio_gfp(struct inode *inode, pgoff_t index,\n\t\tstruct folio **foliop, enum sgp_type sgp, gfp_t gfp,\n\t\tstruct vm_area_struct *vma, struct vm_fault *vmf,\n\t\tvm_fault_t *fault_type)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct shmem_inode_info *info = SHMEM_I(inode);\n\tstruct shmem_sb_info *sbinfo;\n\tstruct mm_struct *charge_mm;\n\tstruct folio *folio;\n\tpgoff_t hindex;\n\tgfp_t huge_gfp;\n\tint error;\n\tint once = 0;\n\tint alloced = 0;\n\n\tif (index > (MAX_LFS_FILESIZE >> PAGE_SHIFT))\n\t\treturn -EFBIG;\nrepeat:\n\tif (sgp <= SGP_CACHE &&\n\t    ((loff_t)index << PAGE_SHIFT) >= i_size_read(inode)) {\n\t\treturn -EINVAL;\n\t}\n\n\tsbinfo = SHMEM_SB(inode->i_sb);\n\tcharge_mm = vma ? vma->vm_mm : NULL;\n\n\tfolio = filemap_get_entry(mapping, index);\n\tif (folio && vma && userfaultfd_minor(vma)) {\n\t\tif (!xa_is_value(folio))\n\t\t\tfolio_put(folio);\n\t\t*fault_type = handle_userfault(vmf, VM_UFFD_MINOR);\n\t\treturn 0;\n\t}\n\n\tif (xa_is_value(folio)) {\n\t\terror = shmem_swapin_folio(inode, index, &folio,\n\t\t\t\t\t  sgp, gfp, vma, fault_type);\n\t\tif (error == -EEXIST)\n\t\t\tgoto repeat;\n\n\t\t*foliop = folio;\n\t\treturn error;\n\t}\n\n\tif (folio) {\n\t\tfolio_lock(folio);\n\n\t\t \n\t\tif (unlikely(folio->mapping != mapping)) {\n\t\t\tfolio_unlock(folio);\n\t\t\tfolio_put(folio);\n\t\t\tgoto repeat;\n\t\t}\n\t\tif (sgp == SGP_WRITE)\n\t\t\tfolio_mark_accessed(folio);\n\t\tif (folio_test_uptodate(folio))\n\t\t\tgoto out;\n\t\t \n\t\tif (sgp != SGP_READ)\n\t\t\tgoto clear;\n\t\tfolio_unlock(folio);\n\t\tfolio_put(folio);\n\t}\n\n\t \n\t*foliop = NULL;\n\tif (sgp == SGP_READ)\n\t\treturn 0;\n\tif (sgp == SGP_NOALLOC)\n\t\treturn -ENOENT;\n\n\t \n\n\tif (vma && userfaultfd_missing(vma)) {\n\t\t*fault_type = handle_userfault(vmf, VM_UFFD_MISSING);\n\t\treturn 0;\n\t}\n\n\tif (!shmem_is_huge(inode, index, false,\n\t\t\t   vma ? vma->vm_mm : NULL, vma ? vma->vm_flags : 0))\n\t\tgoto alloc_nohuge;\n\n\thuge_gfp = vma_thp_gfp_mask(vma);\n\thuge_gfp = limit_gfp_mask(huge_gfp, gfp);\n\tfolio = shmem_alloc_and_acct_folio(huge_gfp, inode, index, true);\n\tif (IS_ERR(folio)) {\nalloc_nohuge:\n\t\tfolio = shmem_alloc_and_acct_folio(gfp, inode, index, false);\n\t}\n\tif (IS_ERR(folio)) {\n\t\tint retry = 5;\n\n\t\terror = PTR_ERR(folio);\n\t\tfolio = NULL;\n\t\tif (error != -ENOSPC)\n\t\t\tgoto unlock;\n\t\t \n\t\twhile (retry--) {\n\t\t\tint ret;\n\n\t\t\tret = shmem_unused_huge_shrink(sbinfo, NULL, 1);\n\t\t\tif (ret == SHRINK_STOP)\n\t\t\t\tbreak;\n\t\t\tif (ret)\n\t\t\t\tgoto alloc_nohuge;\n\t\t}\n\t\tgoto unlock;\n\t}\n\n\thindex = round_down(index, folio_nr_pages(folio));\n\n\tif (sgp == SGP_WRITE)\n\t\t__folio_set_referenced(folio);\n\n\terror = shmem_add_to_page_cache(folio, mapping, hindex,\n\t\t\t\t\tNULL, gfp & GFP_RECLAIM_MASK,\n\t\t\t\t\tcharge_mm);\n\tif (error)\n\t\tgoto unacct;\n\n\tfolio_add_lru(folio);\n\tshmem_recalc_inode(inode, folio_nr_pages(folio), 0);\n\talloced = true;\n\n\tif (folio_test_pmd_mappable(folio) &&\n\t    DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE) <\n\t\t\t\t\tfolio_next_index(folio) - 1) {\n\t\t \n\t\tspin_lock(&sbinfo->shrinklist_lock);\n\t\t \n\t\tif (list_empty_careful(&info->shrinklist)) {\n\t\t\tlist_add_tail(&info->shrinklist,\n\t\t\t\t      &sbinfo->shrinklist);\n\t\t\tsbinfo->shrinklist_len++;\n\t\t}\n\t\tspin_unlock(&sbinfo->shrinklist_lock);\n\t}\n\n\t \n\tif (sgp == SGP_FALLOC)\n\t\tsgp = SGP_WRITE;\nclear:\n\t \n\tif (sgp != SGP_WRITE && !folio_test_uptodate(folio)) {\n\t\tlong i, n = folio_nr_pages(folio);\n\n\t\tfor (i = 0; i < n; i++)\n\t\t\tclear_highpage(folio_page(folio, i));\n\t\tflush_dcache_folio(folio);\n\t\tfolio_mark_uptodate(folio);\n\t}\n\n\t \n\tif (sgp <= SGP_CACHE &&\n\t    ((loff_t)index << PAGE_SHIFT) >= i_size_read(inode)) {\n\t\tif (alloced) {\n\t\t\tfolio_clear_dirty(folio);\n\t\t\tfilemap_remove_folio(folio);\n\t\t\tshmem_recalc_inode(inode, 0, 0);\n\t\t}\n\t\terror = -EINVAL;\n\t\tgoto unlock;\n\t}\nout:\n\t*foliop = folio;\n\treturn 0;\n\n\t \nunacct:\n\tshmem_inode_unacct_blocks(inode, folio_nr_pages(folio));\n\n\tif (folio_test_large(folio)) {\n\t\tfolio_unlock(folio);\n\t\tfolio_put(folio);\n\t\tgoto alloc_nohuge;\n\t}\nunlock:\n\tif (folio) {\n\t\tfolio_unlock(folio);\n\t\tfolio_put(folio);\n\t}\n\tif (error == -ENOSPC && !once++) {\n\t\tshmem_recalc_inode(inode, 0, 0);\n\t\tgoto repeat;\n\t}\n\tif (error == -EEXIST)\n\t\tgoto repeat;\n\treturn error;\n}\n\nint shmem_get_folio(struct inode *inode, pgoff_t index, struct folio **foliop,\n\t\tenum sgp_type sgp)\n{\n\treturn shmem_get_folio_gfp(inode, index, foliop, sgp,\n\t\t\tmapping_gfp_mask(inode->i_mapping), NULL, NULL, NULL);\n}\n\n \nstatic int synchronous_wake_function(wait_queue_entry_t *wait, unsigned mode, int sync, void *key)\n{\n\tint ret = default_wake_function(wait, mode, sync, key);\n\tlist_del_init(&wait->entry);\n\treturn ret;\n}\n\nstatic vm_fault_t shmem_fault(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct inode *inode = file_inode(vma->vm_file);\n\tgfp_t gfp = mapping_gfp_mask(inode->i_mapping);\n\tstruct folio *folio = NULL;\n\tint err;\n\tvm_fault_t ret = VM_FAULT_LOCKED;\n\n\t \n\tif (unlikely(inode->i_private)) {\n\t\tstruct shmem_falloc *shmem_falloc;\n\n\t\tspin_lock(&inode->i_lock);\n\t\tshmem_falloc = inode->i_private;\n\t\tif (shmem_falloc &&\n\t\t    shmem_falloc->waitq &&\n\t\t    vmf->pgoff >= shmem_falloc->start &&\n\t\t    vmf->pgoff < shmem_falloc->next) {\n\t\t\tstruct file *fpin;\n\t\t\twait_queue_head_t *shmem_falloc_waitq;\n\t\t\tDEFINE_WAIT_FUNC(shmem_fault_wait, synchronous_wake_function);\n\n\t\t\tret = VM_FAULT_NOPAGE;\n\t\t\tfpin = maybe_unlock_mmap_for_io(vmf, NULL);\n\t\t\tif (fpin)\n\t\t\t\tret = VM_FAULT_RETRY;\n\n\t\t\tshmem_falloc_waitq = shmem_falloc->waitq;\n\t\t\tprepare_to_wait(shmem_falloc_waitq, &shmem_fault_wait,\n\t\t\t\t\tTASK_UNINTERRUPTIBLE);\n\t\t\tspin_unlock(&inode->i_lock);\n\t\t\tschedule();\n\n\t\t\t \n\t\t\tspin_lock(&inode->i_lock);\n\t\t\tfinish_wait(shmem_falloc_waitq, &shmem_fault_wait);\n\t\t\tspin_unlock(&inode->i_lock);\n\n\t\t\tif (fpin)\n\t\t\t\tfput(fpin);\n\t\t\treturn ret;\n\t\t}\n\t\tspin_unlock(&inode->i_lock);\n\t}\n\n\terr = shmem_get_folio_gfp(inode, vmf->pgoff, &folio, SGP_CACHE,\n\t\t\t\t  gfp, vma, vmf, &ret);\n\tif (err)\n\t\treturn vmf_error(err);\n\tif (folio)\n\t\tvmf->page = folio_file_page(folio, vmf->pgoff);\n\treturn ret;\n}\n\nunsigned long shmem_get_unmapped_area(struct file *file,\n\t\t\t\t      unsigned long uaddr, unsigned long len,\n\t\t\t\t      unsigned long pgoff, unsigned long flags)\n{\n\tunsigned long (*get_area)(struct file *,\n\t\tunsigned long, unsigned long, unsigned long, unsigned long);\n\tunsigned long addr;\n\tunsigned long offset;\n\tunsigned long inflated_len;\n\tunsigned long inflated_addr;\n\tunsigned long inflated_offset;\n\n\tif (len > TASK_SIZE)\n\t\treturn -ENOMEM;\n\n\tget_area = current->mm->get_unmapped_area;\n\taddr = get_area(file, uaddr, len, pgoff, flags);\n\n\tif (!IS_ENABLED(CONFIG_TRANSPARENT_HUGEPAGE))\n\t\treturn addr;\n\tif (IS_ERR_VALUE(addr))\n\t\treturn addr;\n\tif (addr & ~PAGE_MASK)\n\t\treturn addr;\n\tif (addr > TASK_SIZE - len)\n\t\treturn addr;\n\n\tif (shmem_huge == SHMEM_HUGE_DENY)\n\t\treturn addr;\n\tif (len < HPAGE_PMD_SIZE)\n\t\treturn addr;\n\tif (flags & MAP_FIXED)\n\t\treturn addr;\n\t \n\tif (uaddr == addr)\n\t\treturn addr;\n\n\tif (shmem_huge != SHMEM_HUGE_FORCE) {\n\t\tstruct super_block *sb;\n\n\t\tif (file) {\n\t\t\tVM_BUG_ON(file->f_op != &shmem_file_operations);\n\t\t\tsb = file_inode(file)->i_sb;\n\t\t} else {\n\t\t\t \n\t\t\tif (IS_ERR(shm_mnt))\n\t\t\t\treturn addr;\n\t\t\tsb = shm_mnt->mnt_sb;\n\t\t}\n\t\tif (SHMEM_SB(sb)->huge == SHMEM_HUGE_NEVER)\n\t\t\treturn addr;\n\t}\n\n\toffset = (pgoff << PAGE_SHIFT) & (HPAGE_PMD_SIZE-1);\n\tif (offset && offset + len < 2 * HPAGE_PMD_SIZE)\n\t\treturn addr;\n\tif ((addr & (HPAGE_PMD_SIZE-1)) == offset)\n\t\treturn addr;\n\n\tinflated_len = len + HPAGE_PMD_SIZE - PAGE_SIZE;\n\tif (inflated_len > TASK_SIZE)\n\t\treturn addr;\n\tif (inflated_len < len)\n\t\treturn addr;\n\n\tinflated_addr = get_area(NULL, uaddr, inflated_len, 0, flags);\n\tif (IS_ERR_VALUE(inflated_addr))\n\t\treturn addr;\n\tif (inflated_addr & ~PAGE_MASK)\n\t\treturn addr;\n\n\tinflated_offset = inflated_addr & (HPAGE_PMD_SIZE-1);\n\tinflated_addr += offset - inflated_offset;\n\tif (inflated_offset > offset)\n\t\tinflated_addr += HPAGE_PMD_SIZE;\n\n\tif (inflated_addr > TASK_SIZE - len)\n\t\treturn addr;\n\treturn inflated_addr;\n}\n\n#ifdef CONFIG_NUMA\nstatic int shmem_set_policy(struct vm_area_struct *vma, struct mempolicy *mpol)\n{\n\tstruct inode *inode = file_inode(vma->vm_file);\n\treturn mpol_set_shared_policy(&SHMEM_I(inode)->policy, vma, mpol);\n}\n\nstatic struct mempolicy *shmem_get_policy(struct vm_area_struct *vma,\n\t\t\t\t\t  unsigned long addr)\n{\n\tstruct inode *inode = file_inode(vma->vm_file);\n\tpgoff_t index;\n\n\tindex = ((addr - vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;\n\treturn mpol_shared_policy_lookup(&SHMEM_I(inode)->policy, index);\n}\n#endif\n\nint shmem_lock(struct file *file, int lock, struct ucounts *ucounts)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct shmem_inode_info *info = SHMEM_I(inode);\n\tint retval = -ENOMEM;\n\n\t \n\tif (lock && !(info->flags & VM_LOCKED)) {\n\t\tif (!user_shm_lock(inode->i_size, ucounts))\n\t\t\tgoto out_nomem;\n\t\tinfo->flags |= VM_LOCKED;\n\t\tmapping_set_unevictable(file->f_mapping);\n\t}\n\tif (!lock && (info->flags & VM_LOCKED) && ucounts) {\n\t\tuser_shm_unlock(inode->i_size, ucounts);\n\t\tinfo->flags &= ~VM_LOCKED;\n\t\tmapping_clear_unevictable(file->f_mapping);\n\t}\n\tretval = 0;\n\nout_nomem:\n\treturn retval;\n}\n\nstatic int shmem_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct shmem_inode_info *info = SHMEM_I(inode);\n\tint ret;\n\n\tret = seal_check_future_write(info->seals, vma);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tvm_flags_set(vma, VM_MTE_ALLOWED);\n\n\tfile_accessed(file);\n\t \n\tif (inode->i_nlink)\n\t\tvma->vm_ops = &shmem_vm_ops;\n\telse\n\t\tvma->vm_ops = &shmem_anon_vm_ops;\n\treturn 0;\n}\n\nstatic int shmem_file_open(struct inode *inode, struct file *file)\n{\n\tfile->f_mode |= FMODE_CAN_ODIRECT;\n\treturn generic_file_open(inode, file);\n}\n\n#ifdef CONFIG_TMPFS_XATTR\nstatic int shmem_initxattrs(struct inode *, const struct xattr *, void *);\n\n \nstatic void shmem_set_inode_flags(struct inode *inode, unsigned int fsflags)\n{\n\tunsigned int i_flags = 0;\n\n\tif (fsflags & FS_NOATIME_FL)\n\t\ti_flags |= S_NOATIME;\n\tif (fsflags & FS_APPEND_FL)\n\t\ti_flags |= S_APPEND;\n\tif (fsflags & FS_IMMUTABLE_FL)\n\t\ti_flags |= S_IMMUTABLE;\n\t \n\tinode_set_flags(inode, i_flags, S_NOATIME | S_APPEND | S_IMMUTABLE);\n}\n#else\nstatic void shmem_set_inode_flags(struct inode *inode, unsigned int fsflags)\n{\n}\n#define shmem_initxattrs NULL\n#endif\n\nstatic struct offset_ctx *shmem_get_offset_ctx(struct inode *inode)\n{\n\treturn &SHMEM_I(inode)->dir_offsets;\n}\n\nstatic struct inode *__shmem_get_inode(struct mnt_idmap *idmap,\n\t\t\t\t\t     struct super_block *sb,\n\t\t\t\t\t     struct inode *dir, umode_t mode,\n\t\t\t\t\t     dev_t dev, unsigned long flags)\n{\n\tstruct inode *inode;\n\tstruct shmem_inode_info *info;\n\tstruct shmem_sb_info *sbinfo = SHMEM_SB(sb);\n\tino_t ino;\n\tint err;\n\n\terr = shmem_reserve_inode(sb, &ino);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\n\tinode = new_inode(sb);\n\tif (!inode) {\n\t\tshmem_free_inode(sb, 0);\n\t\treturn ERR_PTR(-ENOSPC);\n\t}\n\n\tinode->i_ino = ino;\n\tinode_init_owner(idmap, inode, dir, mode);\n\tinode->i_blocks = 0;\n\tinode->i_atime = inode->i_mtime = inode_set_ctime_current(inode);\n\tinode->i_generation = get_random_u32();\n\tinfo = SHMEM_I(inode);\n\tmemset(info, 0, (char *)inode - (char *)info);\n\tspin_lock_init(&info->lock);\n\tatomic_set(&info->stop_eviction, 0);\n\tinfo->seals = F_SEAL_SEAL;\n\tinfo->flags = flags & VM_NORESERVE;\n\tinfo->i_crtime = inode->i_mtime;\n\tinfo->fsflags = (dir == NULL) ? 0 :\n\t\tSHMEM_I(dir)->fsflags & SHMEM_FL_INHERITED;\n\tif (info->fsflags)\n\t\tshmem_set_inode_flags(inode, info->fsflags);\n\tINIT_LIST_HEAD(&info->shrinklist);\n\tINIT_LIST_HEAD(&info->swaplist);\n\tINIT_LIST_HEAD(&info->swaplist);\n\tif (sbinfo->noswap)\n\t\tmapping_set_unevictable(inode->i_mapping);\n\tsimple_xattrs_init(&info->xattrs);\n\tcache_no_acl(inode);\n\tmapping_set_large_folios(inode->i_mapping);\n\n\tswitch (mode & S_IFMT) {\n\tdefault:\n\t\tinode->i_op = &shmem_special_inode_operations;\n\t\tinit_special_inode(inode, mode, dev);\n\t\tbreak;\n\tcase S_IFREG:\n\t\tinode->i_mapping->a_ops = &shmem_aops;\n\t\tinode->i_op = &shmem_inode_operations;\n\t\tinode->i_fop = &shmem_file_operations;\n\t\tmpol_shared_policy_init(&info->policy,\n\t\t\t\t\t shmem_get_sbmpol(sbinfo));\n\t\tbreak;\n\tcase S_IFDIR:\n\t\tinc_nlink(inode);\n\t\t \n\t\tinode->i_size = 2 * BOGO_DIRENT_SIZE;\n\t\tinode->i_op = &shmem_dir_inode_operations;\n\t\tinode->i_fop = &simple_offset_dir_operations;\n\t\tsimple_offset_init(shmem_get_offset_ctx(inode));\n\t\tbreak;\n\tcase S_IFLNK:\n\t\t \n\t\tmpol_shared_policy_init(&info->policy, NULL);\n\t\tbreak;\n\t}\n\n\tlockdep_annotate_inode_mutex_key(inode);\n\treturn inode;\n}\n\n#ifdef CONFIG_TMPFS_QUOTA\nstatic struct inode *shmem_get_inode(struct mnt_idmap *idmap,\n\t\t\t\t     struct super_block *sb, struct inode *dir,\n\t\t\t\t     umode_t mode, dev_t dev, unsigned long flags)\n{\n\tint err;\n\tstruct inode *inode;\n\n\tinode = __shmem_get_inode(idmap, sb, dir, mode, dev, flags);\n\tif (IS_ERR(inode))\n\t\treturn inode;\n\n\terr = dquot_initialize(inode);\n\tif (err)\n\t\tgoto errout;\n\n\terr = dquot_alloc_inode(inode);\n\tif (err) {\n\t\tdquot_drop(inode);\n\t\tgoto errout;\n\t}\n\treturn inode;\n\nerrout:\n\tinode->i_flags |= S_NOQUOTA;\n\tiput(inode);\n\treturn ERR_PTR(err);\n}\n#else\nstatic inline struct inode *shmem_get_inode(struct mnt_idmap *idmap,\n\t\t\t\t     struct super_block *sb, struct inode *dir,\n\t\t\t\t     umode_t mode, dev_t dev, unsigned long flags)\n{\n\treturn __shmem_get_inode(idmap, sb, dir, mode, dev, flags);\n}\n#endif  \n\n#ifdef CONFIG_USERFAULTFD\nint shmem_mfill_atomic_pte(pmd_t *dst_pmd,\n\t\t\t   struct vm_area_struct *dst_vma,\n\t\t\t   unsigned long dst_addr,\n\t\t\t   unsigned long src_addr,\n\t\t\t   uffd_flags_t flags,\n\t\t\t   struct folio **foliop)\n{\n\tstruct inode *inode = file_inode(dst_vma->vm_file);\n\tstruct shmem_inode_info *info = SHMEM_I(inode);\n\tstruct address_space *mapping = inode->i_mapping;\n\tgfp_t gfp = mapping_gfp_mask(mapping);\n\tpgoff_t pgoff = linear_page_index(dst_vma, dst_addr);\n\tvoid *page_kaddr;\n\tstruct folio *folio;\n\tint ret;\n\tpgoff_t max_off;\n\n\tif (shmem_inode_acct_block(inode, 1)) {\n\t\t \n\t\tif (unlikely(*foliop)) {\n\t\t\tfolio_put(*foliop);\n\t\t\t*foliop = NULL;\n\t\t}\n\t\treturn -ENOMEM;\n\t}\n\n\tif (!*foliop) {\n\t\tret = -ENOMEM;\n\t\tfolio = shmem_alloc_folio(gfp, info, pgoff);\n\t\tif (!folio)\n\t\t\tgoto out_unacct_blocks;\n\n\t\tif (uffd_flags_mode_is(flags, MFILL_ATOMIC_COPY)) {\n\t\t\tpage_kaddr = kmap_local_folio(folio, 0);\n\t\t\t \n\t\t\tpagefault_disable();\n\t\t\tret = copy_from_user(page_kaddr,\n\t\t\t\t\t     (const void __user *)src_addr,\n\t\t\t\t\t     PAGE_SIZE);\n\t\t\tpagefault_enable();\n\t\t\tkunmap_local(page_kaddr);\n\n\t\t\t \n\t\t\tif (unlikely(ret)) {\n\t\t\t\t*foliop = folio;\n\t\t\t\tret = -ENOENT;\n\t\t\t\t \n\t\t\t\tgoto out_unacct_blocks;\n\t\t\t}\n\n\t\t\tflush_dcache_folio(folio);\n\t\t} else {\t\t \n\t\t\tclear_user_highpage(&folio->page, dst_addr);\n\t\t}\n\t} else {\n\t\tfolio = *foliop;\n\t\tVM_BUG_ON_FOLIO(folio_test_large(folio), folio);\n\t\t*foliop = NULL;\n\t}\n\n\tVM_BUG_ON(folio_test_locked(folio));\n\tVM_BUG_ON(folio_test_swapbacked(folio));\n\t__folio_set_locked(folio);\n\t__folio_set_swapbacked(folio);\n\t__folio_mark_uptodate(folio);\n\n\tret = -EFAULT;\n\tmax_off = DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE);\n\tif (unlikely(pgoff >= max_off))\n\t\tgoto out_release;\n\n\tret = shmem_add_to_page_cache(folio, mapping, pgoff, NULL,\n\t\t\t\t      gfp & GFP_RECLAIM_MASK, dst_vma->vm_mm);\n\tif (ret)\n\t\tgoto out_release;\n\n\tret = mfill_atomic_install_pte(dst_pmd, dst_vma, dst_addr,\n\t\t\t\t       &folio->page, true, flags);\n\tif (ret)\n\t\tgoto out_delete_from_cache;\n\n\tshmem_recalc_inode(inode, 1, 0);\n\tfolio_unlock(folio);\n\treturn 0;\nout_delete_from_cache:\n\tfilemap_remove_folio(folio);\nout_release:\n\tfolio_unlock(folio);\n\tfolio_put(folio);\nout_unacct_blocks:\n\tshmem_inode_unacct_blocks(inode, 1);\n\treturn ret;\n}\n#endif  \n\n#ifdef CONFIG_TMPFS\nstatic const struct inode_operations shmem_symlink_inode_operations;\nstatic const struct inode_operations shmem_short_symlink_operations;\n\nstatic int\nshmem_write_begin(struct file *file, struct address_space *mapping,\n\t\t\tloff_t pos, unsigned len,\n\t\t\tstruct page **pagep, void **fsdata)\n{\n\tstruct inode *inode = mapping->host;\n\tstruct shmem_inode_info *info = SHMEM_I(inode);\n\tpgoff_t index = pos >> PAGE_SHIFT;\n\tstruct folio *folio;\n\tint ret = 0;\n\n\t \n\tif (unlikely(info->seals & (F_SEAL_GROW |\n\t\t\t\t   F_SEAL_WRITE | F_SEAL_FUTURE_WRITE))) {\n\t\tif (info->seals & (F_SEAL_WRITE | F_SEAL_FUTURE_WRITE))\n\t\t\treturn -EPERM;\n\t\tif ((info->seals & F_SEAL_GROW) && pos + len > inode->i_size)\n\t\t\treturn -EPERM;\n\t}\n\n\tret = shmem_get_folio(inode, index, &folio, SGP_WRITE);\n\n\tif (ret)\n\t\treturn ret;\n\n\t*pagep = folio_file_page(folio, index);\n\tif (PageHWPoison(*pagep)) {\n\t\tfolio_unlock(folio);\n\t\tfolio_put(folio);\n\t\t*pagep = NULL;\n\t\treturn -EIO;\n\t}\n\n\treturn 0;\n}\n\nstatic int\nshmem_write_end(struct file *file, struct address_space *mapping,\n\t\t\tloff_t pos, unsigned len, unsigned copied,\n\t\t\tstruct page *page, void *fsdata)\n{\n\tstruct folio *folio = page_folio(page);\n\tstruct inode *inode = mapping->host;\n\n\tif (pos + copied > inode->i_size)\n\t\ti_size_write(inode, pos + copied);\n\n\tif (!folio_test_uptodate(folio)) {\n\t\tif (copied < folio_size(folio)) {\n\t\t\tsize_t from = offset_in_folio(folio, pos);\n\t\t\tfolio_zero_segments(folio, 0, from,\n\t\t\t\t\tfrom + copied, folio_size(folio));\n\t\t}\n\t\tfolio_mark_uptodate(folio);\n\t}\n\tfolio_mark_dirty(folio);\n\tfolio_unlock(folio);\n\tfolio_put(folio);\n\n\treturn copied;\n}\n\nstatic ssize_t shmem_file_read_iter(struct kiocb *iocb, struct iov_iter *to)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct inode *inode = file_inode(file);\n\tstruct address_space *mapping = inode->i_mapping;\n\tpgoff_t index;\n\tunsigned long offset;\n\tint error = 0;\n\tssize_t retval = 0;\n\tloff_t *ppos = &iocb->ki_pos;\n\n\tindex = *ppos >> PAGE_SHIFT;\n\toffset = *ppos & ~PAGE_MASK;\n\n\tfor (;;) {\n\t\tstruct folio *folio = NULL;\n\t\tstruct page *page = NULL;\n\t\tpgoff_t end_index;\n\t\tunsigned long nr, ret;\n\t\tloff_t i_size = i_size_read(inode);\n\n\t\tend_index = i_size >> PAGE_SHIFT;\n\t\tif (index > end_index)\n\t\t\tbreak;\n\t\tif (index == end_index) {\n\t\t\tnr = i_size & ~PAGE_MASK;\n\t\t\tif (nr <= offset)\n\t\t\t\tbreak;\n\t\t}\n\n\t\terror = shmem_get_folio(inode, index, &folio, SGP_READ);\n\t\tif (error) {\n\t\t\tif (error == -EINVAL)\n\t\t\t\terror = 0;\n\t\t\tbreak;\n\t\t}\n\t\tif (folio) {\n\t\t\tfolio_unlock(folio);\n\n\t\t\tpage = folio_file_page(folio, index);\n\t\t\tif (PageHWPoison(page)) {\n\t\t\t\tfolio_put(folio);\n\t\t\t\terror = -EIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tnr = PAGE_SIZE;\n\t\ti_size = i_size_read(inode);\n\t\tend_index = i_size >> PAGE_SHIFT;\n\t\tif (index == end_index) {\n\t\t\tnr = i_size & ~PAGE_MASK;\n\t\t\tif (nr <= offset) {\n\t\t\t\tif (folio)\n\t\t\t\t\tfolio_put(folio);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tnr -= offset;\n\n\t\tif (folio) {\n\t\t\t \n\t\t\tif (mapping_writably_mapped(mapping))\n\t\t\t\tflush_dcache_page(page);\n\t\t\t \n\t\t\tif (!offset)\n\t\t\t\tfolio_mark_accessed(folio);\n\t\t\t \n\t\t\tret = copy_page_to_iter(page, offset, nr, to);\n\t\t\tfolio_put(folio);\n\n\t\t} else if (user_backed_iter(to)) {\n\t\t\t \n\t\t\tret = copy_page_to_iter(ZERO_PAGE(0), offset, nr, to);\n\t\t} else {\n\t\t\t \n\t\t\tret = iov_iter_zero(nr, to);\n\t\t}\n\n\t\tretval += ret;\n\t\toffset += ret;\n\t\tindex += offset >> PAGE_SHIFT;\n\t\toffset &= ~PAGE_MASK;\n\n\t\tif (!iov_iter_count(to))\n\t\t\tbreak;\n\t\tif (ret < nr) {\n\t\t\terror = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tcond_resched();\n\t}\n\n\t*ppos = ((loff_t) index << PAGE_SHIFT) + offset;\n\tfile_accessed(file);\n\treturn retval ? retval : error;\n}\n\nstatic ssize_t shmem_file_write_iter(struct kiocb *iocb, struct iov_iter *from)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct inode *inode = file->f_mapping->host;\n\tssize_t ret;\n\n\tinode_lock(inode);\n\tret = generic_write_checks(iocb, from);\n\tif (ret <= 0)\n\t\tgoto unlock;\n\tret = file_remove_privs(file);\n\tif (ret)\n\t\tgoto unlock;\n\tret = file_update_time(file);\n\tif (ret)\n\t\tgoto unlock;\n\tret = generic_perform_write(iocb, from);\nunlock:\n\tinode_unlock(inode);\n\treturn ret;\n}\n\nstatic bool zero_pipe_buf_get(struct pipe_inode_info *pipe,\n\t\t\t      struct pipe_buffer *buf)\n{\n\treturn true;\n}\n\nstatic void zero_pipe_buf_release(struct pipe_inode_info *pipe,\n\t\t\t\t  struct pipe_buffer *buf)\n{\n}\n\nstatic bool zero_pipe_buf_try_steal(struct pipe_inode_info *pipe,\n\t\t\t\t    struct pipe_buffer *buf)\n{\n\treturn false;\n}\n\nstatic const struct pipe_buf_operations zero_pipe_buf_ops = {\n\t.release\t= zero_pipe_buf_release,\n\t.try_steal\t= zero_pipe_buf_try_steal,\n\t.get\t\t= zero_pipe_buf_get,\n};\n\nstatic size_t splice_zeropage_into_pipe(struct pipe_inode_info *pipe,\n\t\t\t\t\tloff_t fpos, size_t size)\n{\n\tsize_t offset = fpos & ~PAGE_MASK;\n\n\tsize = min_t(size_t, size, PAGE_SIZE - offset);\n\n\tif (!pipe_full(pipe->head, pipe->tail, pipe->max_usage)) {\n\t\tstruct pipe_buffer *buf = pipe_head_buf(pipe);\n\n\t\t*buf = (struct pipe_buffer) {\n\t\t\t.ops\t= &zero_pipe_buf_ops,\n\t\t\t.page\t= ZERO_PAGE(0),\n\t\t\t.offset\t= offset,\n\t\t\t.len\t= size,\n\t\t};\n\t\tpipe->head++;\n\t}\n\n\treturn size;\n}\n\nstatic ssize_t shmem_file_splice_read(struct file *in, loff_t *ppos,\n\t\t\t\t      struct pipe_inode_info *pipe,\n\t\t\t\t      size_t len, unsigned int flags)\n{\n\tstruct inode *inode = file_inode(in);\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct folio *folio = NULL;\n\tsize_t total_spliced = 0, used, npages, n, part;\n\tloff_t isize;\n\tint error = 0;\n\n\t \n\tused = pipe_occupancy(pipe->head, pipe->tail);\n\tnpages = max_t(ssize_t, pipe->max_usage - used, 0);\n\tlen = min_t(size_t, len, npages * PAGE_SIZE);\n\n\tdo {\n\t\tif (*ppos >= i_size_read(inode))\n\t\t\tbreak;\n\n\t\terror = shmem_get_folio(inode, *ppos / PAGE_SIZE, &folio,\n\t\t\t\t\tSGP_READ);\n\t\tif (error) {\n\t\t\tif (error == -EINVAL)\n\t\t\t\terror = 0;\n\t\t\tbreak;\n\t\t}\n\t\tif (folio) {\n\t\t\tfolio_unlock(folio);\n\n\t\t\tif (folio_test_hwpoison(folio) ||\n\t\t\t    (folio_test_large(folio) &&\n\t\t\t     folio_test_has_hwpoisoned(folio))) {\n\t\t\t\terror = -EIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tisize = i_size_read(inode);\n\t\tif (unlikely(*ppos >= isize))\n\t\t\tbreak;\n\t\tpart = min_t(loff_t, isize - *ppos, len);\n\n\t\tif (folio) {\n\t\t\t \n\t\t\tif (mapping_writably_mapped(mapping))\n\t\t\t\tflush_dcache_folio(folio);\n\t\t\tfolio_mark_accessed(folio);\n\t\t\t \n\t\t\tn = splice_folio_into_pipe(pipe, folio, *ppos, part);\n\t\t\tfolio_put(folio);\n\t\t\tfolio = NULL;\n\t\t} else {\n\t\t\tn = splice_zeropage_into_pipe(pipe, *ppos, part);\n\t\t}\n\n\t\tif (!n)\n\t\t\tbreak;\n\t\tlen -= n;\n\t\ttotal_spliced += n;\n\t\t*ppos += n;\n\t\tin->f_ra.prev_pos = *ppos;\n\t\tif (pipe_full(pipe->head, pipe->tail, pipe->max_usage))\n\t\t\tbreak;\n\n\t\tcond_resched();\n\t} while (len);\n\n\tif (folio)\n\t\tfolio_put(folio);\n\n\tfile_accessed(in);\n\treturn total_spliced ? total_spliced : error;\n}\n\nstatic loff_t shmem_file_llseek(struct file *file, loff_t offset, int whence)\n{\n\tstruct address_space *mapping = file->f_mapping;\n\tstruct inode *inode = mapping->host;\n\n\tif (whence != SEEK_DATA && whence != SEEK_HOLE)\n\t\treturn generic_file_llseek_size(file, offset, whence,\n\t\t\t\t\tMAX_LFS_FILESIZE, i_size_read(inode));\n\tif (offset < 0)\n\t\treturn -ENXIO;\n\n\tinode_lock(inode);\n\t \n\toffset = mapping_seek_hole_data(mapping, offset, inode->i_size, whence);\n\tif (offset >= 0)\n\t\toffset = vfs_setpos(file, offset, MAX_LFS_FILESIZE);\n\tinode_unlock(inode);\n\treturn offset;\n}\n\nstatic long shmem_fallocate(struct file *file, int mode, loff_t offset,\n\t\t\t\t\t\t\t loff_t len)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct shmem_sb_info *sbinfo = SHMEM_SB(inode->i_sb);\n\tstruct shmem_inode_info *info = SHMEM_I(inode);\n\tstruct shmem_falloc shmem_falloc;\n\tpgoff_t start, index, end, undo_fallocend;\n\tint error;\n\n\tif (mode & ~(FALLOC_FL_KEEP_SIZE | FALLOC_FL_PUNCH_HOLE))\n\t\treturn -EOPNOTSUPP;\n\n\tinode_lock(inode);\n\n\tif (mode & FALLOC_FL_PUNCH_HOLE) {\n\t\tstruct address_space *mapping = file->f_mapping;\n\t\tloff_t unmap_start = round_up(offset, PAGE_SIZE);\n\t\tloff_t unmap_end = round_down(offset + len, PAGE_SIZE) - 1;\n\t\tDECLARE_WAIT_QUEUE_HEAD_ONSTACK(shmem_falloc_waitq);\n\n\t\t \n\t\tif (info->seals & (F_SEAL_WRITE | F_SEAL_FUTURE_WRITE)) {\n\t\t\terror = -EPERM;\n\t\t\tgoto out;\n\t\t}\n\n\t\tshmem_falloc.waitq = &shmem_falloc_waitq;\n\t\tshmem_falloc.start = (u64)unmap_start >> PAGE_SHIFT;\n\t\tshmem_falloc.next = (unmap_end + 1) >> PAGE_SHIFT;\n\t\tspin_lock(&inode->i_lock);\n\t\tinode->i_private = &shmem_falloc;\n\t\tspin_unlock(&inode->i_lock);\n\n\t\tif ((u64)unmap_end > (u64)unmap_start)\n\t\t\tunmap_mapping_range(mapping, unmap_start,\n\t\t\t\t\t    1 + unmap_end - unmap_start, 0);\n\t\tshmem_truncate_range(inode, offset, offset + len - 1);\n\t\t \n\n\t\tspin_lock(&inode->i_lock);\n\t\tinode->i_private = NULL;\n\t\twake_up_all(&shmem_falloc_waitq);\n\t\tWARN_ON_ONCE(!list_empty(&shmem_falloc_waitq.head));\n\t\tspin_unlock(&inode->i_lock);\n\t\terror = 0;\n\t\tgoto out;\n\t}\n\n\t \n\terror = inode_newsize_ok(inode, offset + len);\n\tif (error)\n\t\tgoto out;\n\n\tif ((info->seals & F_SEAL_GROW) && offset + len > inode->i_size) {\n\t\terror = -EPERM;\n\t\tgoto out;\n\t}\n\n\tstart = offset >> PAGE_SHIFT;\n\tend = (offset + len + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t \n\tif (sbinfo->max_blocks && end - start > sbinfo->max_blocks) {\n\t\terror = -ENOSPC;\n\t\tgoto out;\n\t}\n\n\tshmem_falloc.waitq = NULL;\n\tshmem_falloc.start = start;\n\tshmem_falloc.next  = start;\n\tshmem_falloc.nr_falloced = 0;\n\tshmem_falloc.nr_unswapped = 0;\n\tspin_lock(&inode->i_lock);\n\tinode->i_private = &shmem_falloc;\n\tspin_unlock(&inode->i_lock);\n\n\t \n\tundo_fallocend = info->fallocend;\n\tif (info->fallocend < end)\n\t\tinfo->fallocend = end;\n\n\tfor (index = start; index < end; ) {\n\t\tstruct folio *folio;\n\n\t\t \n\t\tif (signal_pending(current))\n\t\t\terror = -EINTR;\n\t\telse if (shmem_falloc.nr_unswapped > shmem_falloc.nr_falloced)\n\t\t\terror = -ENOMEM;\n\t\telse\n\t\t\terror = shmem_get_folio(inode, index, &folio,\n\t\t\t\t\t\tSGP_FALLOC);\n\t\tif (error) {\n\t\t\tinfo->fallocend = undo_fallocend;\n\t\t\t \n\t\t\tif (index > start) {\n\t\t\t\tshmem_undo_range(inode,\n\t\t\t\t    (loff_t)start << PAGE_SHIFT,\n\t\t\t\t    ((loff_t)index << PAGE_SHIFT) - 1, true);\n\t\t\t}\n\t\t\tgoto undone;\n\t\t}\n\n\t\t \n\t\tindex = folio_next_index(folio);\n\t\t \n\t\tif (!index)\n\t\t\tindex--;\n\n\t\t \n\t\tif (!folio_test_uptodate(folio))\n\t\t\tshmem_falloc.nr_falloced += index - shmem_falloc.next;\n\t\tshmem_falloc.next = index;\n\n\t\t \n\t\tfolio_mark_dirty(folio);\n\t\tfolio_unlock(folio);\n\t\tfolio_put(folio);\n\t\tcond_resched();\n\t}\n\n\tif (!(mode & FALLOC_FL_KEEP_SIZE) && offset + len > inode->i_size)\n\t\ti_size_write(inode, offset + len);\nundone:\n\tspin_lock(&inode->i_lock);\n\tinode->i_private = NULL;\n\tspin_unlock(&inode->i_lock);\nout:\n\tif (!error)\n\t\tfile_modified(file);\n\tinode_unlock(inode);\n\treturn error;\n}\n\nstatic int shmem_statfs(struct dentry *dentry, struct kstatfs *buf)\n{\n\tstruct shmem_sb_info *sbinfo = SHMEM_SB(dentry->d_sb);\n\n\tbuf->f_type = TMPFS_MAGIC;\n\tbuf->f_bsize = PAGE_SIZE;\n\tbuf->f_namelen = NAME_MAX;\n\tif (sbinfo->max_blocks) {\n\t\tbuf->f_blocks = sbinfo->max_blocks;\n\t\tbuf->f_bavail =\n\t\tbuf->f_bfree  = sbinfo->max_blocks -\n\t\t\t\tpercpu_counter_sum(&sbinfo->used_blocks);\n\t}\n\tif (sbinfo->max_inodes) {\n\t\tbuf->f_files = sbinfo->max_inodes;\n\t\tbuf->f_ffree = sbinfo->free_ispace / BOGO_INODE_SIZE;\n\t}\n\t \n\n\tbuf->f_fsid = uuid_to_fsid(dentry->d_sb->s_uuid.b);\n\n\treturn 0;\n}\n\n \nstatic int\nshmem_mknod(struct mnt_idmap *idmap, struct inode *dir,\n\t    struct dentry *dentry, umode_t mode, dev_t dev)\n{\n\tstruct inode *inode;\n\tint error;\n\n\tinode = shmem_get_inode(idmap, dir->i_sb, dir, mode, dev, VM_NORESERVE);\n\tif (IS_ERR(inode))\n\t\treturn PTR_ERR(inode);\n\n\terror = simple_acl_create(dir, inode);\n\tif (error)\n\t\tgoto out_iput;\n\terror = security_inode_init_security(inode, dir,\n\t\t\t\t\t     &dentry->d_name,\n\t\t\t\t\t     shmem_initxattrs, NULL);\n\tif (error && error != -EOPNOTSUPP)\n\t\tgoto out_iput;\n\n\terror = simple_offset_add(shmem_get_offset_ctx(dir), dentry);\n\tif (error)\n\t\tgoto out_iput;\n\n\tdir->i_size += BOGO_DIRENT_SIZE;\n\tdir->i_mtime = inode_set_ctime_current(dir);\n\tinode_inc_iversion(dir);\n\td_instantiate(dentry, inode);\n\tdget(dentry);  \n\treturn error;\n\nout_iput:\n\tiput(inode);\n\treturn error;\n}\n\nstatic int\nshmem_tmpfile(struct mnt_idmap *idmap, struct inode *dir,\n\t      struct file *file, umode_t mode)\n{\n\tstruct inode *inode;\n\tint error;\n\n\tinode = shmem_get_inode(idmap, dir->i_sb, dir, mode, 0, VM_NORESERVE);\n\n\tif (IS_ERR(inode)) {\n\t\terror = PTR_ERR(inode);\n\t\tgoto err_out;\n\t}\n\n\terror = security_inode_init_security(inode, dir,\n\t\t\t\t\t     NULL,\n\t\t\t\t\t     shmem_initxattrs, NULL);\n\tif (error && error != -EOPNOTSUPP)\n\t\tgoto out_iput;\n\terror = simple_acl_create(dir, inode);\n\tif (error)\n\t\tgoto out_iput;\n\td_tmpfile(file, inode);\n\nerr_out:\n\treturn finish_open_simple(file, error);\nout_iput:\n\tiput(inode);\n\treturn error;\n}\n\nstatic int shmem_mkdir(struct mnt_idmap *idmap, struct inode *dir,\n\t\t       struct dentry *dentry, umode_t mode)\n{\n\tint error;\n\n\terror = shmem_mknod(idmap, dir, dentry, mode | S_IFDIR, 0);\n\tif (error)\n\t\treturn error;\n\tinc_nlink(dir);\n\treturn 0;\n}\n\nstatic int shmem_create(struct mnt_idmap *idmap, struct inode *dir,\n\t\t\tstruct dentry *dentry, umode_t mode, bool excl)\n{\n\treturn shmem_mknod(idmap, dir, dentry, mode | S_IFREG, 0);\n}\n\n \nstatic int shmem_link(struct dentry *old_dentry, struct inode *dir, struct dentry *dentry)\n{\n\tstruct inode *inode = d_inode(old_dentry);\n\tint ret = 0;\n\n\t \n\tif (inode->i_nlink) {\n\t\tret = shmem_reserve_inode(inode->i_sb, NULL);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tret = simple_offset_add(shmem_get_offset_ctx(dir), dentry);\n\tif (ret) {\n\t\tif (inode->i_nlink)\n\t\t\tshmem_free_inode(inode->i_sb, 0);\n\t\tgoto out;\n\t}\n\n\tdir->i_size += BOGO_DIRENT_SIZE;\n\tdir->i_mtime = inode_set_ctime_to_ts(dir,\n\t\t\t\t\t     inode_set_ctime_current(inode));\n\tinode_inc_iversion(dir);\n\tinc_nlink(inode);\n\tihold(inode);\t \n\tdget(dentry);\t\t \n\td_instantiate(dentry, inode);\nout:\n\treturn ret;\n}\n\nstatic int shmem_unlink(struct inode *dir, struct dentry *dentry)\n{\n\tstruct inode *inode = d_inode(dentry);\n\n\tif (inode->i_nlink > 1 && !S_ISDIR(inode->i_mode))\n\t\tshmem_free_inode(inode->i_sb, 0);\n\n\tsimple_offset_remove(shmem_get_offset_ctx(dir), dentry);\n\n\tdir->i_size -= BOGO_DIRENT_SIZE;\n\tdir->i_mtime = inode_set_ctime_to_ts(dir,\n\t\t\t\t\t     inode_set_ctime_current(inode));\n\tinode_inc_iversion(dir);\n\tdrop_nlink(inode);\n\tdput(dentry);\t \n\treturn 0;\n}\n\nstatic int shmem_rmdir(struct inode *dir, struct dentry *dentry)\n{\n\tif (!simple_empty(dentry))\n\t\treturn -ENOTEMPTY;\n\n\tdrop_nlink(d_inode(dentry));\n\tdrop_nlink(dir);\n\treturn shmem_unlink(dir, dentry);\n}\n\nstatic int shmem_whiteout(struct mnt_idmap *idmap,\n\t\t\t  struct inode *old_dir, struct dentry *old_dentry)\n{\n\tstruct dentry *whiteout;\n\tint error;\n\n\twhiteout = d_alloc(old_dentry->d_parent, &old_dentry->d_name);\n\tif (!whiteout)\n\t\treturn -ENOMEM;\n\n\terror = shmem_mknod(idmap, old_dir, whiteout,\n\t\t\t    S_IFCHR | WHITEOUT_MODE, WHITEOUT_DEV);\n\tdput(whiteout);\n\tif (error)\n\t\treturn error;\n\n\t \n\td_rehash(whiteout);\n\treturn 0;\n}\n\n \nstatic int shmem_rename2(struct mnt_idmap *idmap,\n\t\t\t struct inode *old_dir, struct dentry *old_dentry,\n\t\t\t struct inode *new_dir, struct dentry *new_dentry,\n\t\t\t unsigned int flags)\n{\n\tstruct inode *inode = d_inode(old_dentry);\n\tint they_are_dirs = S_ISDIR(inode->i_mode);\n\tint error;\n\n\tif (flags & ~(RENAME_NOREPLACE | RENAME_EXCHANGE | RENAME_WHITEOUT))\n\t\treturn -EINVAL;\n\n\tif (flags & RENAME_EXCHANGE)\n\t\treturn simple_offset_rename_exchange(old_dir, old_dentry,\n\t\t\t\t\t\t     new_dir, new_dentry);\n\n\tif (!simple_empty(new_dentry))\n\t\treturn -ENOTEMPTY;\n\n\tif (flags & RENAME_WHITEOUT) {\n\t\terror = shmem_whiteout(idmap, old_dir, old_dentry);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\n\tsimple_offset_remove(shmem_get_offset_ctx(old_dir), old_dentry);\n\terror = simple_offset_add(shmem_get_offset_ctx(new_dir), old_dentry);\n\tif (error)\n\t\treturn error;\n\n\tif (d_really_is_positive(new_dentry)) {\n\t\t(void) shmem_unlink(new_dir, new_dentry);\n\t\tif (they_are_dirs) {\n\t\t\tdrop_nlink(d_inode(new_dentry));\n\t\t\tdrop_nlink(old_dir);\n\t\t}\n\t} else if (they_are_dirs) {\n\t\tdrop_nlink(old_dir);\n\t\tinc_nlink(new_dir);\n\t}\n\n\told_dir->i_size -= BOGO_DIRENT_SIZE;\n\tnew_dir->i_size += BOGO_DIRENT_SIZE;\n\tsimple_rename_timestamp(old_dir, old_dentry, new_dir, new_dentry);\n\tinode_inc_iversion(old_dir);\n\tinode_inc_iversion(new_dir);\n\treturn 0;\n}\n\nstatic int shmem_symlink(struct mnt_idmap *idmap, struct inode *dir,\n\t\t\t struct dentry *dentry, const char *symname)\n{\n\tint error;\n\tint len;\n\tstruct inode *inode;\n\tstruct folio *folio;\n\n\tlen = strlen(symname) + 1;\n\tif (len > PAGE_SIZE)\n\t\treturn -ENAMETOOLONG;\n\n\tinode = shmem_get_inode(idmap, dir->i_sb, dir, S_IFLNK | 0777, 0,\n\t\t\t\tVM_NORESERVE);\n\n\tif (IS_ERR(inode))\n\t\treturn PTR_ERR(inode);\n\n\terror = security_inode_init_security(inode, dir, &dentry->d_name,\n\t\t\t\t\t     shmem_initxattrs, NULL);\n\tif (error && error != -EOPNOTSUPP)\n\t\tgoto out_iput;\n\n\terror = simple_offset_add(shmem_get_offset_ctx(dir), dentry);\n\tif (error)\n\t\tgoto out_iput;\n\n\tinode->i_size = len-1;\n\tif (len <= SHORT_SYMLINK_LEN) {\n\t\tinode->i_link = kmemdup(symname, len, GFP_KERNEL);\n\t\tif (!inode->i_link) {\n\t\t\terror = -ENOMEM;\n\t\t\tgoto out_remove_offset;\n\t\t}\n\t\tinode->i_op = &shmem_short_symlink_operations;\n\t} else {\n\t\tinode_nohighmem(inode);\n\t\terror = shmem_get_folio(inode, 0, &folio, SGP_WRITE);\n\t\tif (error)\n\t\t\tgoto out_remove_offset;\n\t\tinode->i_mapping->a_ops = &shmem_aops;\n\t\tinode->i_op = &shmem_symlink_inode_operations;\n\t\tmemcpy(folio_address(folio), symname, len);\n\t\tfolio_mark_uptodate(folio);\n\t\tfolio_mark_dirty(folio);\n\t\tfolio_unlock(folio);\n\t\tfolio_put(folio);\n\t}\n\tdir->i_size += BOGO_DIRENT_SIZE;\n\tdir->i_mtime = inode_set_ctime_current(dir);\n\tinode_inc_iversion(dir);\n\td_instantiate(dentry, inode);\n\tdget(dentry);\n\treturn 0;\n\nout_remove_offset:\n\tsimple_offset_remove(shmem_get_offset_ctx(dir), dentry);\nout_iput:\n\tiput(inode);\n\treturn error;\n}\n\nstatic void shmem_put_link(void *arg)\n{\n\tfolio_mark_accessed(arg);\n\tfolio_put(arg);\n}\n\nstatic const char *shmem_get_link(struct dentry *dentry,\n\t\t\t\t  struct inode *inode,\n\t\t\t\t  struct delayed_call *done)\n{\n\tstruct folio *folio = NULL;\n\tint error;\n\n\tif (!dentry) {\n\t\tfolio = filemap_get_folio(inode->i_mapping, 0);\n\t\tif (IS_ERR(folio))\n\t\t\treturn ERR_PTR(-ECHILD);\n\t\tif (PageHWPoison(folio_page(folio, 0)) ||\n\t\t    !folio_test_uptodate(folio)) {\n\t\t\tfolio_put(folio);\n\t\t\treturn ERR_PTR(-ECHILD);\n\t\t}\n\t} else {\n\t\terror = shmem_get_folio(inode, 0, &folio, SGP_READ);\n\t\tif (error)\n\t\t\treturn ERR_PTR(error);\n\t\tif (!folio)\n\t\t\treturn ERR_PTR(-ECHILD);\n\t\tif (PageHWPoison(folio_page(folio, 0))) {\n\t\t\tfolio_unlock(folio);\n\t\t\tfolio_put(folio);\n\t\t\treturn ERR_PTR(-ECHILD);\n\t\t}\n\t\tfolio_unlock(folio);\n\t}\n\tset_delayed_call(done, shmem_put_link, folio);\n\treturn folio_address(folio);\n}\n\n#ifdef CONFIG_TMPFS_XATTR\n\nstatic int shmem_fileattr_get(struct dentry *dentry, struct fileattr *fa)\n{\n\tstruct shmem_inode_info *info = SHMEM_I(d_inode(dentry));\n\n\tfileattr_fill_flags(fa, info->fsflags & SHMEM_FL_USER_VISIBLE);\n\n\treturn 0;\n}\n\nstatic int shmem_fileattr_set(struct mnt_idmap *idmap,\n\t\t\t      struct dentry *dentry, struct fileattr *fa)\n{\n\tstruct inode *inode = d_inode(dentry);\n\tstruct shmem_inode_info *info = SHMEM_I(inode);\n\n\tif (fileattr_has_fsx(fa))\n\t\treturn -EOPNOTSUPP;\n\tif (fa->flags & ~SHMEM_FL_USER_MODIFIABLE)\n\t\treturn -EOPNOTSUPP;\n\n\tinfo->fsflags = (info->fsflags & ~SHMEM_FL_USER_MODIFIABLE) |\n\t\t(fa->flags & SHMEM_FL_USER_MODIFIABLE);\n\n\tshmem_set_inode_flags(inode, info->fsflags);\n\tinode_set_ctime_current(inode);\n\tinode_inc_iversion(inode);\n\treturn 0;\n}\n\n \n\n \nstatic int shmem_initxattrs(struct inode *inode,\n\t\t\t    const struct xattr *xattr_array,\n\t\t\t    void *fs_info)\n{\n\tstruct shmem_inode_info *info = SHMEM_I(inode);\n\tstruct shmem_sb_info *sbinfo = SHMEM_SB(inode->i_sb);\n\tconst struct xattr *xattr;\n\tstruct simple_xattr *new_xattr;\n\tsize_t ispace = 0;\n\tsize_t len;\n\n\tif (sbinfo->max_inodes) {\n\t\tfor (xattr = xattr_array; xattr->name != NULL; xattr++) {\n\t\t\tispace += simple_xattr_space(xattr->name,\n\t\t\t\txattr->value_len + XATTR_SECURITY_PREFIX_LEN);\n\t\t}\n\t\tif (ispace) {\n\t\t\traw_spin_lock(&sbinfo->stat_lock);\n\t\t\tif (sbinfo->free_ispace < ispace)\n\t\t\t\tispace = 0;\n\t\t\telse\n\t\t\t\tsbinfo->free_ispace -= ispace;\n\t\t\traw_spin_unlock(&sbinfo->stat_lock);\n\t\t\tif (!ispace)\n\t\t\t\treturn -ENOSPC;\n\t\t}\n\t}\n\n\tfor (xattr = xattr_array; xattr->name != NULL; xattr++) {\n\t\tnew_xattr = simple_xattr_alloc(xattr->value, xattr->value_len);\n\t\tif (!new_xattr)\n\t\t\tbreak;\n\n\t\tlen = strlen(xattr->name) + 1;\n\t\tnew_xattr->name = kmalloc(XATTR_SECURITY_PREFIX_LEN + len,\n\t\t\t\t\t  GFP_KERNEL_ACCOUNT);\n\t\tif (!new_xattr->name) {\n\t\t\tkvfree(new_xattr);\n\t\t\tbreak;\n\t\t}\n\n\t\tmemcpy(new_xattr->name, XATTR_SECURITY_PREFIX,\n\t\t       XATTR_SECURITY_PREFIX_LEN);\n\t\tmemcpy(new_xattr->name + XATTR_SECURITY_PREFIX_LEN,\n\t\t       xattr->name, len);\n\n\t\tsimple_xattr_add(&info->xattrs, new_xattr);\n\t}\n\n\tif (xattr->name != NULL) {\n\t\tif (ispace) {\n\t\t\traw_spin_lock(&sbinfo->stat_lock);\n\t\t\tsbinfo->free_ispace += ispace;\n\t\t\traw_spin_unlock(&sbinfo->stat_lock);\n\t\t}\n\t\tsimple_xattrs_free(&info->xattrs, NULL);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic int shmem_xattr_handler_get(const struct xattr_handler *handler,\n\t\t\t\t   struct dentry *unused, struct inode *inode,\n\t\t\t\t   const char *name, void *buffer, size_t size)\n{\n\tstruct shmem_inode_info *info = SHMEM_I(inode);\n\n\tname = xattr_full_name(handler, name);\n\treturn simple_xattr_get(&info->xattrs, name, buffer, size);\n}\n\nstatic int shmem_xattr_handler_set(const struct xattr_handler *handler,\n\t\t\t\t   struct mnt_idmap *idmap,\n\t\t\t\t   struct dentry *unused, struct inode *inode,\n\t\t\t\t   const char *name, const void *value,\n\t\t\t\t   size_t size, int flags)\n{\n\tstruct shmem_inode_info *info = SHMEM_I(inode);\n\tstruct shmem_sb_info *sbinfo = SHMEM_SB(inode->i_sb);\n\tstruct simple_xattr *old_xattr;\n\tsize_t ispace = 0;\n\n\tname = xattr_full_name(handler, name);\n\tif (value && sbinfo->max_inodes) {\n\t\tispace = simple_xattr_space(name, size);\n\t\traw_spin_lock(&sbinfo->stat_lock);\n\t\tif (sbinfo->free_ispace < ispace)\n\t\t\tispace = 0;\n\t\telse\n\t\t\tsbinfo->free_ispace -= ispace;\n\t\traw_spin_unlock(&sbinfo->stat_lock);\n\t\tif (!ispace)\n\t\t\treturn -ENOSPC;\n\t}\n\n\told_xattr = simple_xattr_set(&info->xattrs, name, value, size, flags);\n\tif (!IS_ERR(old_xattr)) {\n\t\tispace = 0;\n\t\tif (old_xattr && sbinfo->max_inodes)\n\t\t\tispace = simple_xattr_space(old_xattr->name,\n\t\t\t\t\t\t    old_xattr->size);\n\t\tsimple_xattr_free(old_xattr);\n\t\told_xattr = NULL;\n\t\tinode_set_ctime_current(inode);\n\t\tinode_inc_iversion(inode);\n\t}\n\tif (ispace) {\n\t\traw_spin_lock(&sbinfo->stat_lock);\n\t\tsbinfo->free_ispace += ispace;\n\t\traw_spin_unlock(&sbinfo->stat_lock);\n\t}\n\treturn PTR_ERR(old_xattr);\n}\n\nstatic const struct xattr_handler shmem_security_xattr_handler = {\n\t.prefix = XATTR_SECURITY_PREFIX,\n\t.get = shmem_xattr_handler_get,\n\t.set = shmem_xattr_handler_set,\n};\n\nstatic const struct xattr_handler shmem_trusted_xattr_handler = {\n\t.prefix = XATTR_TRUSTED_PREFIX,\n\t.get = shmem_xattr_handler_get,\n\t.set = shmem_xattr_handler_set,\n};\n\nstatic const struct xattr_handler shmem_user_xattr_handler = {\n\t.prefix = XATTR_USER_PREFIX,\n\t.get = shmem_xattr_handler_get,\n\t.set = shmem_xattr_handler_set,\n};\n\nstatic const struct xattr_handler *shmem_xattr_handlers[] = {\n\t&shmem_security_xattr_handler,\n\t&shmem_trusted_xattr_handler,\n\t&shmem_user_xattr_handler,\n\tNULL\n};\n\nstatic ssize_t shmem_listxattr(struct dentry *dentry, char *buffer, size_t size)\n{\n\tstruct shmem_inode_info *info = SHMEM_I(d_inode(dentry));\n\treturn simple_xattr_list(d_inode(dentry), &info->xattrs, buffer, size);\n}\n#endif  \n\nstatic const struct inode_operations shmem_short_symlink_operations = {\n\t.getattr\t= shmem_getattr,\n\t.setattr\t= shmem_setattr,\n\t.get_link\t= simple_get_link,\n#ifdef CONFIG_TMPFS_XATTR\n\t.listxattr\t= shmem_listxattr,\n#endif\n};\n\nstatic const struct inode_operations shmem_symlink_inode_operations = {\n\t.getattr\t= shmem_getattr,\n\t.setattr\t= shmem_setattr,\n\t.get_link\t= shmem_get_link,\n#ifdef CONFIG_TMPFS_XATTR\n\t.listxattr\t= shmem_listxattr,\n#endif\n};\n\nstatic struct dentry *shmem_get_parent(struct dentry *child)\n{\n\treturn ERR_PTR(-ESTALE);\n}\n\nstatic int shmem_match(struct inode *ino, void *vfh)\n{\n\t__u32 *fh = vfh;\n\t__u64 inum = fh[2];\n\tinum = (inum << 32) | fh[1];\n\treturn ino->i_ino == inum && fh[0] == ino->i_generation;\n}\n\n \nstatic struct dentry *shmem_find_alias(struct inode *inode)\n{\n\tstruct dentry *alias = d_find_alias(inode);\n\n\treturn alias ?: d_find_any_alias(inode);\n}\n\n\nstatic struct dentry *shmem_fh_to_dentry(struct super_block *sb,\n\t\tstruct fid *fid, int fh_len, int fh_type)\n{\n\tstruct inode *inode;\n\tstruct dentry *dentry = NULL;\n\tu64 inum;\n\n\tif (fh_len < 3)\n\t\treturn NULL;\n\n\tinum = fid->raw[2];\n\tinum = (inum << 32) | fid->raw[1];\n\n\tinode = ilookup5(sb, (unsigned long)(inum + fid->raw[0]),\n\t\t\tshmem_match, fid->raw);\n\tif (inode) {\n\t\tdentry = shmem_find_alias(inode);\n\t\tiput(inode);\n\t}\n\n\treturn dentry;\n}\n\nstatic int shmem_encode_fh(struct inode *inode, __u32 *fh, int *len,\n\t\t\t\tstruct inode *parent)\n{\n\tif (*len < 3) {\n\t\t*len = 3;\n\t\treturn FILEID_INVALID;\n\t}\n\n\tif (inode_unhashed(inode)) {\n\t\t \n\t\tstatic DEFINE_SPINLOCK(lock);\n\t\tspin_lock(&lock);\n\t\tif (inode_unhashed(inode))\n\t\t\t__insert_inode_hash(inode,\n\t\t\t\t\t    inode->i_ino + inode->i_generation);\n\t\tspin_unlock(&lock);\n\t}\n\n\tfh[0] = inode->i_generation;\n\tfh[1] = inode->i_ino;\n\tfh[2] = ((__u64)inode->i_ino) >> 32;\n\n\t*len = 3;\n\treturn 1;\n}\n\nstatic const struct export_operations shmem_export_ops = {\n\t.get_parent     = shmem_get_parent,\n\t.encode_fh      = shmem_encode_fh,\n\t.fh_to_dentry\t= shmem_fh_to_dentry,\n};\n\nenum shmem_param {\n\tOpt_gid,\n\tOpt_huge,\n\tOpt_mode,\n\tOpt_mpol,\n\tOpt_nr_blocks,\n\tOpt_nr_inodes,\n\tOpt_size,\n\tOpt_uid,\n\tOpt_inode32,\n\tOpt_inode64,\n\tOpt_noswap,\n\tOpt_quota,\n\tOpt_usrquota,\n\tOpt_grpquota,\n\tOpt_usrquota_block_hardlimit,\n\tOpt_usrquota_inode_hardlimit,\n\tOpt_grpquota_block_hardlimit,\n\tOpt_grpquota_inode_hardlimit,\n};\n\nstatic const struct constant_table shmem_param_enums_huge[] = {\n\t{\"never\",\tSHMEM_HUGE_NEVER },\n\t{\"always\",\tSHMEM_HUGE_ALWAYS },\n\t{\"within_size\",\tSHMEM_HUGE_WITHIN_SIZE },\n\t{\"advise\",\tSHMEM_HUGE_ADVISE },\n\t{}\n};\n\nconst struct fs_parameter_spec shmem_fs_parameters[] = {\n\tfsparam_u32   (\"gid\",\t\tOpt_gid),\n\tfsparam_enum  (\"huge\",\t\tOpt_huge,  shmem_param_enums_huge),\n\tfsparam_u32oct(\"mode\",\t\tOpt_mode),\n\tfsparam_string(\"mpol\",\t\tOpt_mpol),\n\tfsparam_string(\"nr_blocks\",\tOpt_nr_blocks),\n\tfsparam_string(\"nr_inodes\",\tOpt_nr_inodes),\n\tfsparam_string(\"size\",\t\tOpt_size),\n\tfsparam_u32   (\"uid\",\t\tOpt_uid),\n\tfsparam_flag  (\"inode32\",\tOpt_inode32),\n\tfsparam_flag  (\"inode64\",\tOpt_inode64),\n\tfsparam_flag  (\"noswap\",\tOpt_noswap),\n#ifdef CONFIG_TMPFS_QUOTA\n\tfsparam_flag  (\"quota\",\t\tOpt_quota),\n\tfsparam_flag  (\"usrquota\",\tOpt_usrquota),\n\tfsparam_flag  (\"grpquota\",\tOpt_grpquota),\n\tfsparam_string(\"usrquota_block_hardlimit\", Opt_usrquota_block_hardlimit),\n\tfsparam_string(\"usrquota_inode_hardlimit\", Opt_usrquota_inode_hardlimit),\n\tfsparam_string(\"grpquota_block_hardlimit\", Opt_grpquota_block_hardlimit),\n\tfsparam_string(\"grpquota_inode_hardlimit\", Opt_grpquota_inode_hardlimit),\n#endif\n\t{}\n};\n\nstatic int shmem_parse_one(struct fs_context *fc, struct fs_parameter *param)\n{\n\tstruct shmem_options *ctx = fc->fs_private;\n\tstruct fs_parse_result result;\n\tunsigned long long size;\n\tchar *rest;\n\tint opt;\n\tkuid_t kuid;\n\tkgid_t kgid;\n\n\topt = fs_parse(fc, shmem_fs_parameters, param, &result);\n\tif (opt < 0)\n\t\treturn opt;\n\n\tswitch (opt) {\n\tcase Opt_size:\n\t\tsize = memparse(param->string, &rest);\n\t\tif (*rest == '%') {\n\t\t\tsize <<= PAGE_SHIFT;\n\t\t\tsize *= totalram_pages();\n\t\t\tdo_div(size, 100);\n\t\t\trest++;\n\t\t}\n\t\tif (*rest)\n\t\t\tgoto bad_value;\n\t\tctx->blocks = DIV_ROUND_UP(size, PAGE_SIZE);\n\t\tctx->seen |= SHMEM_SEEN_BLOCKS;\n\t\tbreak;\n\tcase Opt_nr_blocks:\n\t\tctx->blocks = memparse(param->string, &rest);\n\t\tif (*rest || ctx->blocks > LONG_MAX)\n\t\t\tgoto bad_value;\n\t\tctx->seen |= SHMEM_SEEN_BLOCKS;\n\t\tbreak;\n\tcase Opt_nr_inodes:\n\t\tctx->inodes = memparse(param->string, &rest);\n\t\tif (*rest || ctx->inodes > ULONG_MAX / BOGO_INODE_SIZE)\n\t\t\tgoto bad_value;\n\t\tctx->seen |= SHMEM_SEEN_INODES;\n\t\tbreak;\n\tcase Opt_mode:\n\t\tctx->mode = result.uint_32 & 07777;\n\t\tbreak;\n\tcase Opt_uid:\n\t\tkuid = make_kuid(current_user_ns(), result.uint_32);\n\t\tif (!uid_valid(kuid))\n\t\t\tgoto bad_value;\n\n\t\t \n\t\tif (!kuid_has_mapping(fc->user_ns, kuid))\n\t\t\tgoto bad_value;\n\n\t\tctx->uid = kuid;\n\t\tbreak;\n\tcase Opt_gid:\n\t\tkgid = make_kgid(current_user_ns(), result.uint_32);\n\t\tif (!gid_valid(kgid))\n\t\t\tgoto bad_value;\n\n\t\t \n\t\tif (!kgid_has_mapping(fc->user_ns, kgid))\n\t\t\tgoto bad_value;\n\n\t\tctx->gid = kgid;\n\t\tbreak;\n\tcase Opt_huge:\n\t\tctx->huge = result.uint_32;\n\t\tif (ctx->huge != SHMEM_HUGE_NEVER &&\n\t\t    !(IS_ENABLED(CONFIG_TRANSPARENT_HUGEPAGE) &&\n\t\t      has_transparent_hugepage()))\n\t\t\tgoto unsupported_parameter;\n\t\tctx->seen |= SHMEM_SEEN_HUGE;\n\t\tbreak;\n\tcase Opt_mpol:\n\t\tif (IS_ENABLED(CONFIG_NUMA)) {\n\t\t\tmpol_put(ctx->mpol);\n\t\t\tctx->mpol = NULL;\n\t\t\tif (mpol_parse_str(param->string, &ctx->mpol))\n\t\t\t\tgoto bad_value;\n\t\t\tbreak;\n\t\t}\n\t\tgoto unsupported_parameter;\n\tcase Opt_inode32:\n\t\tctx->full_inums = false;\n\t\tctx->seen |= SHMEM_SEEN_INUMS;\n\t\tbreak;\n\tcase Opt_inode64:\n\t\tif (sizeof(ino_t) < 8) {\n\t\t\treturn invalfc(fc,\n\t\t\t\t       \"Cannot use inode64 with <64bit inums in kernel\\n\");\n\t\t}\n\t\tctx->full_inums = true;\n\t\tctx->seen |= SHMEM_SEEN_INUMS;\n\t\tbreak;\n\tcase Opt_noswap:\n\t\tif ((fc->user_ns != &init_user_ns) || !capable(CAP_SYS_ADMIN)) {\n\t\t\treturn invalfc(fc,\n\t\t\t\t       \"Turning off swap in unprivileged tmpfs mounts unsupported\");\n\t\t}\n\t\tctx->noswap = true;\n\t\tctx->seen |= SHMEM_SEEN_NOSWAP;\n\t\tbreak;\n\tcase Opt_quota:\n\t\tif (fc->user_ns != &init_user_ns)\n\t\t\treturn invalfc(fc, \"Quotas in unprivileged tmpfs mounts are unsupported\");\n\t\tctx->seen |= SHMEM_SEEN_QUOTA;\n\t\tctx->quota_types |= (QTYPE_MASK_USR | QTYPE_MASK_GRP);\n\t\tbreak;\n\tcase Opt_usrquota:\n\t\tif (fc->user_ns != &init_user_ns)\n\t\t\treturn invalfc(fc, \"Quotas in unprivileged tmpfs mounts are unsupported\");\n\t\tctx->seen |= SHMEM_SEEN_QUOTA;\n\t\tctx->quota_types |= QTYPE_MASK_USR;\n\t\tbreak;\n\tcase Opt_grpquota:\n\t\tif (fc->user_ns != &init_user_ns)\n\t\t\treturn invalfc(fc, \"Quotas in unprivileged tmpfs mounts are unsupported\");\n\t\tctx->seen |= SHMEM_SEEN_QUOTA;\n\t\tctx->quota_types |= QTYPE_MASK_GRP;\n\t\tbreak;\n\tcase Opt_usrquota_block_hardlimit:\n\t\tsize = memparse(param->string, &rest);\n\t\tif (*rest || !size)\n\t\t\tgoto bad_value;\n\t\tif (size > SHMEM_QUOTA_MAX_SPC_LIMIT)\n\t\t\treturn invalfc(fc,\n\t\t\t\t       \"User quota block hardlimit too large.\");\n\t\tctx->qlimits.usrquota_bhardlimit = size;\n\t\tbreak;\n\tcase Opt_grpquota_block_hardlimit:\n\t\tsize = memparse(param->string, &rest);\n\t\tif (*rest || !size)\n\t\t\tgoto bad_value;\n\t\tif (size > SHMEM_QUOTA_MAX_SPC_LIMIT)\n\t\t\treturn invalfc(fc,\n\t\t\t\t       \"Group quota block hardlimit too large.\");\n\t\tctx->qlimits.grpquota_bhardlimit = size;\n\t\tbreak;\n\tcase Opt_usrquota_inode_hardlimit:\n\t\tsize = memparse(param->string, &rest);\n\t\tif (*rest || !size)\n\t\t\tgoto bad_value;\n\t\tif (size > SHMEM_QUOTA_MAX_INO_LIMIT)\n\t\t\treturn invalfc(fc,\n\t\t\t\t       \"User quota inode hardlimit too large.\");\n\t\tctx->qlimits.usrquota_ihardlimit = size;\n\t\tbreak;\n\tcase Opt_grpquota_inode_hardlimit:\n\t\tsize = memparse(param->string, &rest);\n\t\tif (*rest || !size)\n\t\t\tgoto bad_value;\n\t\tif (size > SHMEM_QUOTA_MAX_INO_LIMIT)\n\t\t\treturn invalfc(fc,\n\t\t\t\t       \"Group quota inode hardlimit too large.\");\n\t\tctx->qlimits.grpquota_ihardlimit = size;\n\t\tbreak;\n\t}\n\treturn 0;\n\nunsupported_parameter:\n\treturn invalfc(fc, \"Unsupported parameter '%s'\", param->key);\nbad_value:\n\treturn invalfc(fc, \"Bad value for '%s'\", param->key);\n}\n\nstatic int shmem_parse_options(struct fs_context *fc, void *data)\n{\n\tchar *options = data;\n\n\tif (options) {\n\t\tint err = security_sb_eat_lsm_opts(options, &fc->security);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\twhile (options != NULL) {\n\t\tchar *this_char = options;\n\t\tfor (;;) {\n\t\t\t \n\t\t\toptions = strchr(options, ',');\n\t\t\tif (options == NULL)\n\t\t\t\tbreak;\n\t\t\toptions++;\n\t\t\tif (!isdigit(*options)) {\n\t\t\t\toptions[-1] = '\\0';\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (*this_char) {\n\t\t\tchar *value = strchr(this_char, '=');\n\t\t\tsize_t len = 0;\n\t\t\tint err;\n\n\t\t\tif (value) {\n\t\t\t\t*value++ = '\\0';\n\t\t\t\tlen = strlen(value);\n\t\t\t}\n\t\t\terr = vfs_parse_fs_string(fc, this_char, value, len);\n\t\t\tif (err < 0)\n\t\t\t\treturn err;\n\t\t}\n\t}\n\treturn 0;\n}\n\n \nstatic int shmem_reconfigure(struct fs_context *fc)\n{\n\tstruct shmem_options *ctx = fc->fs_private;\n\tstruct shmem_sb_info *sbinfo = SHMEM_SB(fc->root->d_sb);\n\tunsigned long used_isp;\n\tstruct mempolicy *mpol = NULL;\n\tconst char *err;\n\n\traw_spin_lock(&sbinfo->stat_lock);\n\tused_isp = sbinfo->max_inodes * BOGO_INODE_SIZE - sbinfo->free_ispace;\n\n\tif ((ctx->seen & SHMEM_SEEN_BLOCKS) && ctx->blocks) {\n\t\tif (!sbinfo->max_blocks) {\n\t\t\terr = \"Cannot retroactively limit size\";\n\t\t\tgoto out;\n\t\t}\n\t\tif (percpu_counter_compare(&sbinfo->used_blocks,\n\t\t\t\t\t   ctx->blocks) > 0) {\n\t\t\terr = \"Too small a size for current use\";\n\t\t\tgoto out;\n\t\t}\n\t}\n\tif ((ctx->seen & SHMEM_SEEN_INODES) && ctx->inodes) {\n\t\tif (!sbinfo->max_inodes) {\n\t\t\terr = \"Cannot retroactively limit inodes\";\n\t\t\tgoto out;\n\t\t}\n\t\tif (ctx->inodes * BOGO_INODE_SIZE < used_isp) {\n\t\t\terr = \"Too few inodes for current use\";\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif ((ctx->seen & SHMEM_SEEN_INUMS) && !ctx->full_inums &&\n\t    sbinfo->next_ino > UINT_MAX) {\n\t\terr = \"Current inum too high to switch to 32-bit inums\";\n\t\tgoto out;\n\t}\n\tif ((ctx->seen & SHMEM_SEEN_NOSWAP) && ctx->noswap && !sbinfo->noswap) {\n\t\terr = \"Cannot disable swap on remount\";\n\t\tgoto out;\n\t}\n\tif (!(ctx->seen & SHMEM_SEEN_NOSWAP) && !ctx->noswap && sbinfo->noswap) {\n\t\terr = \"Cannot enable swap on remount if it was disabled on first mount\";\n\t\tgoto out;\n\t}\n\n\tif (ctx->seen & SHMEM_SEEN_QUOTA &&\n\t    !sb_any_quota_loaded(fc->root->d_sb)) {\n\t\terr = \"Cannot enable quota on remount\";\n\t\tgoto out;\n\t}\n\n#ifdef CONFIG_TMPFS_QUOTA\n#define CHANGED_LIMIT(name)\t\t\t\t\t\t\\\n\t(ctx->qlimits.name## hardlimit &&\t\t\t\t\\\n\t(ctx->qlimits.name## hardlimit != sbinfo->qlimits.name## hardlimit))\n\n\tif (CHANGED_LIMIT(usrquota_b) || CHANGED_LIMIT(usrquota_i) ||\n\t    CHANGED_LIMIT(grpquota_b) || CHANGED_LIMIT(grpquota_i)) {\n\t\terr = \"Cannot change global quota limit on remount\";\n\t\tgoto out;\n\t}\n#endif  \n\n\tif (ctx->seen & SHMEM_SEEN_HUGE)\n\t\tsbinfo->huge = ctx->huge;\n\tif (ctx->seen & SHMEM_SEEN_INUMS)\n\t\tsbinfo->full_inums = ctx->full_inums;\n\tif (ctx->seen & SHMEM_SEEN_BLOCKS)\n\t\tsbinfo->max_blocks  = ctx->blocks;\n\tif (ctx->seen & SHMEM_SEEN_INODES) {\n\t\tsbinfo->max_inodes  = ctx->inodes;\n\t\tsbinfo->free_ispace = ctx->inodes * BOGO_INODE_SIZE - used_isp;\n\t}\n\n\t \n\tif (ctx->mpol) {\n\t\tmpol = sbinfo->mpol;\n\t\tsbinfo->mpol = ctx->mpol;\t \n\t\tctx->mpol = NULL;\n\t}\n\n\tif (ctx->noswap)\n\t\tsbinfo->noswap = true;\n\n\traw_spin_unlock(&sbinfo->stat_lock);\n\tmpol_put(mpol);\n\treturn 0;\nout:\n\traw_spin_unlock(&sbinfo->stat_lock);\n\treturn invalfc(fc, \"%s\", err);\n}\n\nstatic int shmem_show_options(struct seq_file *seq, struct dentry *root)\n{\n\tstruct shmem_sb_info *sbinfo = SHMEM_SB(root->d_sb);\n\tstruct mempolicy *mpol;\n\n\tif (sbinfo->max_blocks != shmem_default_max_blocks())\n\t\tseq_printf(seq, \",size=%luk\", K(sbinfo->max_blocks));\n\tif (sbinfo->max_inodes != shmem_default_max_inodes())\n\t\tseq_printf(seq, \",nr_inodes=%lu\", sbinfo->max_inodes);\n\tif (sbinfo->mode != (0777 | S_ISVTX))\n\t\tseq_printf(seq, \",mode=%03ho\", sbinfo->mode);\n\tif (!uid_eq(sbinfo->uid, GLOBAL_ROOT_UID))\n\t\tseq_printf(seq, \",uid=%u\",\n\t\t\t\tfrom_kuid_munged(&init_user_ns, sbinfo->uid));\n\tif (!gid_eq(sbinfo->gid, GLOBAL_ROOT_GID))\n\t\tseq_printf(seq, \",gid=%u\",\n\t\t\t\tfrom_kgid_munged(&init_user_ns, sbinfo->gid));\n\n\t \n\tif (IS_ENABLED(CONFIG_TMPFS_INODE64) || sbinfo->full_inums)\n\t\tseq_printf(seq, \",inode%d\", (sbinfo->full_inums ? 64 : 32));\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\t \n\tif (sbinfo->huge)\n\t\tseq_printf(seq, \",huge=%s\", shmem_format_huge(sbinfo->huge));\n#endif\n\tmpol = shmem_get_sbmpol(sbinfo);\n\tshmem_show_mpol(seq, mpol);\n\tmpol_put(mpol);\n\tif (sbinfo->noswap)\n\t\tseq_printf(seq, \",noswap\");\n\treturn 0;\n}\n\n#endif  \n\nstatic void shmem_put_super(struct super_block *sb)\n{\n\tstruct shmem_sb_info *sbinfo = SHMEM_SB(sb);\n\n#ifdef CONFIG_TMPFS_QUOTA\n\tshmem_disable_quotas(sb);\n#endif\n\tfree_percpu(sbinfo->ino_batch);\n\tpercpu_counter_destroy(&sbinfo->used_blocks);\n\tmpol_put(sbinfo->mpol);\n\tkfree(sbinfo);\n\tsb->s_fs_info = NULL;\n}\n\nstatic int shmem_fill_super(struct super_block *sb, struct fs_context *fc)\n{\n\tstruct shmem_options *ctx = fc->fs_private;\n\tstruct inode *inode;\n\tstruct shmem_sb_info *sbinfo;\n\tint error = -ENOMEM;\n\n\t \n\tsbinfo = kzalloc(max((int)sizeof(struct shmem_sb_info),\n\t\t\t\tL1_CACHE_BYTES), GFP_KERNEL);\n\tif (!sbinfo)\n\t\treturn error;\n\n\tsb->s_fs_info = sbinfo;\n\n#ifdef CONFIG_TMPFS\n\t \n\tif (!(sb->s_flags & SB_KERNMOUNT)) {\n\t\tif (!(ctx->seen & SHMEM_SEEN_BLOCKS))\n\t\t\tctx->blocks = shmem_default_max_blocks();\n\t\tif (!(ctx->seen & SHMEM_SEEN_INODES))\n\t\t\tctx->inodes = shmem_default_max_inodes();\n\t\tif (!(ctx->seen & SHMEM_SEEN_INUMS))\n\t\t\tctx->full_inums = IS_ENABLED(CONFIG_TMPFS_INODE64);\n\t\tsbinfo->noswap = ctx->noswap;\n\t} else {\n\t\tsb->s_flags |= SB_NOUSER;\n\t}\n\tsb->s_export_op = &shmem_export_ops;\n\tsb->s_flags |= SB_NOSEC | SB_I_VERSION;\n#else\n\tsb->s_flags |= SB_NOUSER;\n#endif\n\tsbinfo->max_blocks = ctx->blocks;\n\tsbinfo->max_inodes = ctx->inodes;\n\tsbinfo->free_ispace = sbinfo->max_inodes * BOGO_INODE_SIZE;\n\tif (sb->s_flags & SB_KERNMOUNT) {\n\t\tsbinfo->ino_batch = alloc_percpu(ino_t);\n\t\tif (!sbinfo->ino_batch)\n\t\t\tgoto failed;\n\t}\n\tsbinfo->uid = ctx->uid;\n\tsbinfo->gid = ctx->gid;\n\tsbinfo->full_inums = ctx->full_inums;\n\tsbinfo->mode = ctx->mode;\n\tsbinfo->huge = ctx->huge;\n\tsbinfo->mpol = ctx->mpol;\n\tctx->mpol = NULL;\n\n\traw_spin_lock_init(&sbinfo->stat_lock);\n\tif (percpu_counter_init(&sbinfo->used_blocks, 0, GFP_KERNEL))\n\t\tgoto failed;\n\tspin_lock_init(&sbinfo->shrinklist_lock);\n\tINIT_LIST_HEAD(&sbinfo->shrinklist);\n\n\tsb->s_maxbytes = MAX_LFS_FILESIZE;\n\tsb->s_blocksize = PAGE_SIZE;\n\tsb->s_blocksize_bits = PAGE_SHIFT;\n\tsb->s_magic = TMPFS_MAGIC;\n\tsb->s_op = &shmem_ops;\n\tsb->s_time_gran = 1;\n#ifdef CONFIG_TMPFS_XATTR\n\tsb->s_xattr = shmem_xattr_handlers;\n#endif\n#ifdef CONFIG_TMPFS_POSIX_ACL\n\tsb->s_flags |= SB_POSIXACL;\n#endif\n\tuuid_gen(&sb->s_uuid);\n\n#ifdef CONFIG_TMPFS_QUOTA\n\tif (ctx->seen & SHMEM_SEEN_QUOTA) {\n\t\tsb->dq_op = &shmem_quota_operations;\n\t\tsb->s_qcop = &dquot_quotactl_sysfile_ops;\n\t\tsb->s_quota_types = QTYPE_MASK_USR | QTYPE_MASK_GRP;\n\n\t\t \n\t\tmemcpy(&sbinfo->qlimits, &ctx->qlimits,\n\t\t       sizeof(struct shmem_quota_limits));\n\n\t\tif (shmem_enable_quotas(sb, ctx->quota_types))\n\t\t\tgoto failed;\n\t}\n#endif  \n\n\tinode = shmem_get_inode(&nop_mnt_idmap, sb, NULL, S_IFDIR | sbinfo->mode, 0,\n\t\t\t\tVM_NORESERVE);\n\tif (IS_ERR(inode)) {\n\t\terror = PTR_ERR(inode);\n\t\tgoto failed;\n\t}\n\tinode->i_uid = sbinfo->uid;\n\tinode->i_gid = sbinfo->gid;\n\tsb->s_root = d_make_root(inode);\n\tif (!sb->s_root)\n\t\tgoto failed;\n\treturn 0;\n\nfailed:\n\tshmem_put_super(sb);\n\treturn error;\n}\n\nstatic int shmem_get_tree(struct fs_context *fc)\n{\n\treturn get_tree_nodev(fc, shmem_fill_super);\n}\n\nstatic void shmem_free_fc(struct fs_context *fc)\n{\n\tstruct shmem_options *ctx = fc->fs_private;\n\n\tif (ctx) {\n\t\tmpol_put(ctx->mpol);\n\t\tkfree(ctx);\n\t}\n}\n\nstatic const struct fs_context_operations shmem_fs_context_ops = {\n\t.free\t\t\t= shmem_free_fc,\n\t.get_tree\t\t= shmem_get_tree,\n#ifdef CONFIG_TMPFS\n\t.parse_monolithic\t= shmem_parse_options,\n\t.parse_param\t\t= shmem_parse_one,\n\t.reconfigure\t\t= shmem_reconfigure,\n#endif\n};\n\nstatic struct kmem_cache *shmem_inode_cachep;\n\nstatic struct inode *shmem_alloc_inode(struct super_block *sb)\n{\n\tstruct shmem_inode_info *info;\n\tinfo = alloc_inode_sb(sb, shmem_inode_cachep, GFP_KERNEL);\n\tif (!info)\n\t\treturn NULL;\n\treturn &info->vfs_inode;\n}\n\nstatic void shmem_free_in_core_inode(struct inode *inode)\n{\n\tif (S_ISLNK(inode->i_mode))\n\t\tkfree(inode->i_link);\n\tkmem_cache_free(shmem_inode_cachep, SHMEM_I(inode));\n}\n\nstatic void shmem_destroy_inode(struct inode *inode)\n{\n\tif (S_ISREG(inode->i_mode))\n\t\tmpol_free_shared_policy(&SHMEM_I(inode)->policy);\n\tif (S_ISDIR(inode->i_mode))\n\t\tsimple_offset_destroy(shmem_get_offset_ctx(inode));\n}\n\nstatic void shmem_init_inode(void *foo)\n{\n\tstruct shmem_inode_info *info = foo;\n\tinode_init_once(&info->vfs_inode);\n}\n\nstatic void shmem_init_inodecache(void)\n{\n\tshmem_inode_cachep = kmem_cache_create(\"shmem_inode_cache\",\n\t\t\t\tsizeof(struct shmem_inode_info),\n\t\t\t\t0, SLAB_PANIC|SLAB_ACCOUNT, shmem_init_inode);\n}\n\nstatic void shmem_destroy_inodecache(void)\n{\n\tkmem_cache_destroy(shmem_inode_cachep);\n}\n\n \nstatic int shmem_error_remove_page(struct address_space *mapping,\n\t\t\t\t   struct page *page)\n{\n\treturn 0;\n}\n\nconst struct address_space_operations shmem_aops = {\n\t.writepage\t= shmem_writepage,\n\t.dirty_folio\t= noop_dirty_folio,\n#ifdef CONFIG_TMPFS\n\t.write_begin\t= shmem_write_begin,\n\t.write_end\t= shmem_write_end,\n#endif\n#ifdef CONFIG_MIGRATION\n\t.migrate_folio\t= migrate_folio,\n#endif\n\t.error_remove_page = shmem_error_remove_page,\n};\nEXPORT_SYMBOL(shmem_aops);\n\nstatic const struct file_operations shmem_file_operations = {\n\t.mmap\t\t= shmem_mmap,\n\t.open\t\t= shmem_file_open,\n\t.get_unmapped_area = shmem_get_unmapped_area,\n#ifdef CONFIG_TMPFS\n\t.llseek\t\t= shmem_file_llseek,\n\t.read_iter\t= shmem_file_read_iter,\n\t.write_iter\t= shmem_file_write_iter,\n\t.fsync\t\t= noop_fsync,\n\t.splice_read\t= shmem_file_splice_read,\n\t.splice_write\t= iter_file_splice_write,\n\t.fallocate\t= shmem_fallocate,\n#endif\n};\n\nstatic const struct inode_operations shmem_inode_operations = {\n\t.getattr\t= shmem_getattr,\n\t.setattr\t= shmem_setattr,\n#ifdef CONFIG_TMPFS_XATTR\n\t.listxattr\t= shmem_listxattr,\n\t.set_acl\t= simple_set_acl,\n\t.fileattr_get\t= shmem_fileattr_get,\n\t.fileattr_set\t= shmem_fileattr_set,\n#endif\n};\n\nstatic const struct inode_operations shmem_dir_inode_operations = {\n#ifdef CONFIG_TMPFS\n\t.getattr\t= shmem_getattr,\n\t.create\t\t= shmem_create,\n\t.lookup\t\t= simple_lookup,\n\t.link\t\t= shmem_link,\n\t.unlink\t\t= shmem_unlink,\n\t.symlink\t= shmem_symlink,\n\t.mkdir\t\t= shmem_mkdir,\n\t.rmdir\t\t= shmem_rmdir,\n\t.mknod\t\t= shmem_mknod,\n\t.rename\t\t= shmem_rename2,\n\t.tmpfile\t= shmem_tmpfile,\n\t.get_offset_ctx\t= shmem_get_offset_ctx,\n#endif\n#ifdef CONFIG_TMPFS_XATTR\n\t.listxattr\t= shmem_listxattr,\n\t.fileattr_get\t= shmem_fileattr_get,\n\t.fileattr_set\t= shmem_fileattr_set,\n#endif\n#ifdef CONFIG_TMPFS_POSIX_ACL\n\t.setattr\t= shmem_setattr,\n\t.set_acl\t= simple_set_acl,\n#endif\n};\n\nstatic const struct inode_operations shmem_special_inode_operations = {\n\t.getattr\t= shmem_getattr,\n#ifdef CONFIG_TMPFS_XATTR\n\t.listxattr\t= shmem_listxattr,\n#endif\n#ifdef CONFIG_TMPFS_POSIX_ACL\n\t.setattr\t= shmem_setattr,\n\t.set_acl\t= simple_set_acl,\n#endif\n};\n\nstatic const struct super_operations shmem_ops = {\n\t.alloc_inode\t= shmem_alloc_inode,\n\t.free_inode\t= shmem_free_in_core_inode,\n\t.destroy_inode\t= shmem_destroy_inode,\n#ifdef CONFIG_TMPFS\n\t.statfs\t\t= shmem_statfs,\n\t.show_options\t= shmem_show_options,\n#endif\n#ifdef CONFIG_TMPFS_QUOTA\n\t.get_dquots\t= shmem_get_dquots,\n#endif\n\t.evict_inode\t= shmem_evict_inode,\n\t.drop_inode\t= generic_delete_inode,\n\t.put_super\t= shmem_put_super,\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\t.nr_cached_objects\t= shmem_unused_huge_count,\n\t.free_cached_objects\t= shmem_unused_huge_scan,\n#endif\n};\n\nstatic const struct vm_operations_struct shmem_vm_ops = {\n\t.fault\t\t= shmem_fault,\n\t.map_pages\t= filemap_map_pages,\n#ifdef CONFIG_NUMA\n\t.set_policy     = shmem_set_policy,\n\t.get_policy     = shmem_get_policy,\n#endif\n};\n\nstatic const struct vm_operations_struct shmem_anon_vm_ops = {\n\t.fault\t\t= shmem_fault,\n\t.map_pages\t= filemap_map_pages,\n#ifdef CONFIG_NUMA\n\t.set_policy     = shmem_set_policy,\n\t.get_policy     = shmem_get_policy,\n#endif\n};\n\nint shmem_init_fs_context(struct fs_context *fc)\n{\n\tstruct shmem_options *ctx;\n\n\tctx = kzalloc(sizeof(struct shmem_options), GFP_KERNEL);\n\tif (!ctx)\n\t\treturn -ENOMEM;\n\n\tctx->mode = 0777 | S_ISVTX;\n\tctx->uid = current_fsuid();\n\tctx->gid = current_fsgid();\n\n\tfc->fs_private = ctx;\n\tfc->ops = &shmem_fs_context_ops;\n\treturn 0;\n}\n\nstatic struct file_system_type shmem_fs_type = {\n\t.owner\t\t= THIS_MODULE,\n\t.name\t\t= \"tmpfs\",\n\t.init_fs_context = shmem_init_fs_context,\n#ifdef CONFIG_TMPFS\n\t.parameters\t= shmem_fs_parameters,\n#endif\n\t.kill_sb\t= kill_litter_super,\n#ifdef CONFIG_SHMEM\n\t.fs_flags\t= FS_USERNS_MOUNT | FS_ALLOW_IDMAP,\n#else\n\t.fs_flags\t= FS_USERNS_MOUNT,\n#endif\n};\n\nvoid __init shmem_init(void)\n{\n\tint error;\n\n\tshmem_init_inodecache();\n\n#ifdef CONFIG_TMPFS_QUOTA\n\terror = register_quota_format(&shmem_quota_format);\n\tif (error < 0) {\n\t\tpr_err(\"Could not register quota format\\n\");\n\t\tgoto out3;\n\t}\n#endif\n\n\terror = register_filesystem(&shmem_fs_type);\n\tif (error) {\n\t\tpr_err(\"Could not register tmpfs\\n\");\n\t\tgoto out2;\n\t}\n\n\tshm_mnt = kern_mount(&shmem_fs_type);\n\tif (IS_ERR(shm_mnt)) {\n\t\terror = PTR_ERR(shm_mnt);\n\t\tpr_err(\"Could not kern_mount tmpfs\\n\");\n\t\tgoto out1;\n\t}\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tif (has_transparent_hugepage() && shmem_huge > SHMEM_HUGE_DENY)\n\t\tSHMEM_SB(shm_mnt->mnt_sb)->huge = shmem_huge;\n\telse\n\t\tshmem_huge = SHMEM_HUGE_NEVER;  \n#endif\n\treturn;\n\nout1:\n\tunregister_filesystem(&shmem_fs_type);\nout2:\n#ifdef CONFIG_TMPFS_QUOTA\n\tunregister_quota_format(&shmem_quota_format);\nout3:\n#endif\n\tshmem_destroy_inodecache();\n\tshm_mnt = ERR_PTR(error);\n}\n\n#if defined(CONFIG_TRANSPARENT_HUGEPAGE) && defined(CONFIG_SYSFS)\nstatic ssize_t shmem_enabled_show(struct kobject *kobj,\n\t\t\t\t  struct kobj_attribute *attr, char *buf)\n{\n\tstatic const int values[] = {\n\t\tSHMEM_HUGE_ALWAYS,\n\t\tSHMEM_HUGE_WITHIN_SIZE,\n\t\tSHMEM_HUGE_ADVISE,\n\t\tSHMEM_HUGE_NEVER,\n\t\tSHMEM_HUGE_DENY,\n\t\tSHMEM_HUGE_FORCE,\n\t};\n\tint len = 0;\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(values); i++) {\n\t\tlen += sysfs_emit_at(buf, len,\n\t\t\t\t     shmem_huge == values[i] ? \"%s[%s]\" : \"%s%s\",\n\t\t\t\t     i ? \" \" : \"\",\n\t\t\t\t     shmem_format_huge(values[i]));\n\t}\n\n\tlen += sysfs_emit_at(buf, len, \"\\n\");\n\n\treturn len;\n}\n\nstatic ssize_t shmem_enabled_store(struct kobject *kobj,\n\t\tstruct kobj_attribute *attr, const char *buf, size_t count)\n{\n\tchar tmp[16];\n\tint huge;\n\n\tif (count + 1 > sizeof(tmp))\n\t\treturn -EINVAL;\n\tmemcpy(tmp, buf, count);\n\ttmp[count] = '\\0';\n\tif (count && tmp[count - 1] == '\\n')\n\t\ttmp[count - 1] = '\\0';\n\n\thuge = shmem_parse_huge(tmp);\n\tif (huge == -EINVAL)\n\t\treturn -EINVAL;\n\tif (!has_transparent_hugepage() &&\n\t\t\thuge != SHMEM_HUGE_NEVER && huge != SHMEM_HUGE_DENY)\n\t\treturn -EINVAL;\n\n\tshmem_huge = huge;\n\tif (shmem_huge > SHMEM_HUGE_DENY)\n\t\tSHMEM_SB(shm_mnt->mnt_sb)->huge = shmem_huge;\n\treturn count;\n}\n\nstruct kobj_attribute shmem_enabled_attr = __ATTR_RW(shmem_enabled);\n#endif  \n\n#else  \n\n \n\nstatic struct file_system_type shmem_fs_type = {\n\t.name\t\t= \"tmpfs\",\n\t.init_fs_context = ramfs_init_fs_context,\n\t.parameters\t= ramfs_fs_parameters,\n\t.kill_sb\t= ramfs_kill_sb,\n\t.fs_flags\t= FS_USERNS_MOUNT,\n};\n\nvoid __init shmem_init(void)\n{\n\tBUG_ON(register_filesystem(&shmem_fs_type) != 0);\n\n\tshm_mnt = kern_mount(&shmem_fs_type);\n\tBUG_ON(IS_ERR(shm_mnt));\n}\n\nint shmem_unuse(unsigned int type)\n{\n\treturn 0;\n}\n\nint shmem_lock(struct file *file, int lock, struct ucounts *ucounts)\n{\n\treturn 0;\n}\n\nvoid shmem_unlock_mapping(struct address_space *mapping)\n{\n}\n\n#ifdef CONFIG_MMU\nunsigned long shmem_get_unmapped_area(struct file *file,\n\t\t\t\t      unsigned long addr, unsigned long len,\n\t\t\t\t      unsigned long pgoff, unsigned long flags)\n{\n\treturn current->mm->get_unmapped_area(file, addr, len, pgoff, flags);\n}\n#endif\n\nvoid shmem_truncate_range(struct inode *inode, loff_t lstart, loff_t lend)\n{\n\ttruncate_inode_pages_range(inode->i_mapping, lstart, lend);\n}\nEXPORT_SYMBOL_GPL(shmem_truncate_range);\n\n#define shmem_vm_ops\t\t\t\tgeneric_file_vm_ops\n#define shmem_anon_vm_ops\t\t\tgeneric_file_vm_ops\n#define shmem_file_operations\t\t\tramfs_file_operations\n#define shmem_acct_size(flags, size)\t\t0\n#define shmem_unacct_size(flags, size)\t\tdo {} while (0)\n\nstatic inline struct inode *shmem_get_inode(struct mnt_idmap *idmap, struct super_block *sb, struct inode *dir,\n\t\t\t\t\t    umode_t mode, dev_t dev, unsigned long flags)\n{\n\tstruct inode *inode = ramfs_get_inode(sb, dir, mode, dev);\n\treturn inode ? inode : ERR_PTR(-ENOSPC);\n}\n\n#endif  \n\n \n\nstatic struct file *__shmem_file_setup(struct vfsmount *mnt, const char *name, loff_t size,\n\t\t\t\t       unsigned long flags, unsigned int i_flags)\n{\n\tstruct inode *inode;\n\tstruct file *res;\n\n\tif (IS_ERR(mnt))\n\t\treturn ERR_CAST(mnt);\n\n\tif (size < 0 || size > MAX_LFS_FILESIZE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (shmem_acct_size(flags, size))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (is_idmapped_mnt(mnt))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tinode = shmem_get_inode(&nop_mnt_idmap, mnt->mnt_sb, NULL,\n\t\t\t\tS_IFREG | S_IRWXUGO, 0, flags);\n\n\tif (IS_ERR(inode)) {\n\t\tshmem_unacct_size(flags, size);\n\t\treturn ERR_CAST(inode);\n\t}\n\tinode->i_flags |= i_flags;\n\tinode->i_size = size;\n\tclear_nlink(inode);\t \n\tres = ERR_PTR(ramfs_nommu_expand_for_mapping(inode, size));\n\tif (!IS_ERR(res))\n\t\tres = alloc_file_pseudo(inode, mnt, name, O_RDWR,\n\t\t\t\t&shmem_file_operations);\n\tif (IS_ERR(res))\n\t\tiput(inode);\n\treturn res;\n}\n\n \nstruct file *shmem_kernel_file_setup(const char *name, loff_t size, unsigned long flags)\n{\n\treturn __shmem_file_setup(shm_mnt, name, size, flags, S_PRIVATE);\n}\n\n \nstruct file *shmem_file_setup(const char *name, loff_t size, unsigned long flags)\n{\n\treturn __shmem_file_setup(shm_mnt, name, size, flags, 0);\n}\nEXPORT_SYMBOL_GPL(shmem_file_setup);\n\n \nstruct file *shmem_file_setup_with_mnt(struct vfsmount *mnt, const char *name,\n\t\t\t\t       loff_t size, unsigned long flags)\n{\n\treturn __shmem_file_setup(mnt, name, size, flags, 0);\n}\nEXPORT_SYMBOL_GPL(shmem_file_setup_with_mnt);\n\n \nint shmem_zero_setup(struct vm_area_struct *vma)\n{\n\tstruct file *file;\n\tloff_t size = vma->vm_end - vma->vm_start;\n\n\t \n\tfile = shmem_kernel_file_setup(\"dev/zero\", size, vma->vm_flags);\n\tif (IS_ERR(file))\n\t\treturn PTR_ERR(file);\n\n\tif (vma->vm_file)\n\t\tfput(vma->vm_file);\n\tvma->vm_file = file;\n\tvma->vm_ops = &shmem_anon_vm_ops;\n\n\treturn 0;\n}\n\n \nstruct folio *shmem_read_folio_gfp(struct address_space *mapping,\n\t\tpgoff_t index, gfp_t gfp)\n{\n#ifdef CONFIG_SHMEM\n\tstruct inode *inode = mapping->host;\n\tstruct folio *folio;\n\tint error;\n\n\tBUG_ON(!shmem_mapping(mapping));\n\terror = shmem_get_folio_gfp(inode, index, &folio, SGP_CACHE,\n\t\t\t\t  gfp, NULL, NULL, NULL);\n\tif (error)\n\t\treturn ERR_PTR(error);\n\n\tfolio_unlock(folio);\n\treturn folio;\n#else\n\t \n\treturn mapping_read_folio_gfp(mapping, index, gfp);\n#endif\n}\nEXPORT_SYMBOL_GPL(shmem_read_folio_gfp);\n\nstruct page *shmem_read_mapping_page_gfp(struct address_space *mapping,\n\t\t\t\t\t pgoff_t index, gfp_t gfp)\n{\n\tstruct folio *folio = shmem_read_folio_gfp(mapping, index, gfp);\n\tstruct page *page;\n\n\tif (IS_ERR(folio))\n\t\treturn &folio->page;\n\n\tpage = folio_file_page(folio, index);\n\tif (PageHWPoison(page)) {\n\t\tfolio_put(folio);\n\t\treturn ERR_PTR(-EIO);\n\t}\n\n\treturn page;\n}\nEXPORT_SYMBOL_GPL(shmem_read_mapping_page_gfp);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}