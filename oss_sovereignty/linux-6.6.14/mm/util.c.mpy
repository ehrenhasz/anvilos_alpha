{
  "module_name": "util.c",
  "hash_id": "3daa91b0075ab857d9d21752dd3cd4d3ac27a065ddfca4b10f26b1eeba1d4b1d",
  "original_prompt": "Ingested from linux-6.6.14/mm/util.c",
  "human_readable_source": "\n#include <linux/mm.h>\n#include <linux/slab.h>\n#include <linux/string.h>\n#include <linux/compiler.h>\n#include <linux/export.h>\n#include <linux/err.h>\n#include <linux/sched.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/task_stack.h>\n#include <linux/security.h>\n#include <linux/swap.h>\n#include <linux/swapops.h>\n#include <linux/mman.h>\n#include <linux/hugetlb.h>\n#include <linux/vmalloc.h>\n#include <linux/userfaultfd_k.h>\n#include <linux/elf.h>\n#include <linux/elf-randomize.h>\n#include <linux/personality.h>\n#include <linux/random.h>\n#include <linux/processor.h>\n#include <linux/sizes.h>\n#include <linux/compat.h>\n\n#include <linux/uaccess.h>\n\n#include \"internal.h\"\n#include \"swap.h\"\n\n \nvoid kfree_const(const void *x)\n{\n\tif (!is_kernel_rodata((unsigned long)x))\n\t\tkfree(x);\n}\nEXPORT_SYMBOL(kfree_const);\n\n \nnoinline\nchar *kstrdup(const char *s, gfp_t gfp)\n{\n\tsize_t len;\n\tchar *buf;\n\n\tif (!s)\n\t\treturn NULL;\n\n\tlen = strlen(s) + 1;\n\tbuf = kmalloc_track_caller(len, gfp);\n\tif (buf)\n\t\tmemcpy(buf, s, len);\n\treturn buf;\n}\nEXPORT_SYMBOL(kstrdup);\n\n \nconst char *kstrdup_const(const char *s, gfp_t gfp)\n{\n\tif (is_kernel_rodata((unsigned long)s))\n\t\treturn s;\n\n\treturn kstrdup(s, gfp);\n}\nEXPORT_SYMBOL(kstrdup_const);\n\n \nchar *kstrndup(const char *s, size_t max, gfp_t gfp)\n{\n\tsize_t len;\n\tchar *buf;\n\n\tif (!s)\n\t\treturn NULL;\n\n\tlen = strnlen(s, max);\n\tbuf = kmalloc_track_caller(len+1, gfp);\n\tif (buf) {\n\t\tmemcpy(buf, s, len);\n\t\tbuf[len] = '\\0';\n\t}\n\treturn buf;\n}\nEXPORT_SYMBOL(kstrndup);\n\n \nvoid *kmemdup(const void *src, size_t len, gfp_t gfp)\n{\n\tvoid *p;\n\n\tp = kmalloc_track_caller(len, gfp);\n\tif (p)\n\t\tmemcpy(p, src, len);\n\treturn p;\n}\nEXPORT_SYMBOL(kmemdup);\n\n \nvoid *kvmemdup(const void *src, size_t len, gfp_t gfp)\n{\n\tvoid *p;\n\n\tp = kvmalloc(len, gfp);\n\tif (p)\n\t\tmemcpy(p, src, len);\n\treturn p;\n}\nEXPORT_SYMBOL(kvmemdup);\n\n \nchar *kmemdup_nul(const char *s, size_t len, gfp_t gfp)\n{\n\tchar *buf;\n\n\tif (!s)\n\t\treturn NULL;\n\n\tbuf = kmalloc_track_caller(len + 1, gfp);\n\tif (buf) {\n\t\tmemcpy(buf, s, len);\n\t\tbuf[len] = '\\0';\n\t}\n\treturn buf;\n}\nEXPORT_SYMBOL(kmemdup_nul);\n\n \nvoid *memdup_user(const void __user *src, size_t len)\n{\n\tvoid *p;\n\n\tp = kmalloc_track_caller(len, GFP_USER | __GFP_NOWARN);\n\tif (!p)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (copy_from_user(p, src, len)) {\n\t\tkfree(p);\n\t\treturn ERR_PTR(-EFAULT);\n\t}\n\n\treturn p;\n}\nEXPORT_SYMBOL(memdup_user);\n\n \nvoid *vmemdup_user(const void __user *src, size_t len)\n{\n\tvoid *p;\n\n\tp = kvmalloc(len, GFP_USER);\n\tif (!p)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (copy_from_user(p, src, len)) {\n\t\tkvfree(p);\n\t\treturn ERR_PTR(-EFAULT);\n\t}\n\n\treturn p;\n}\nEXPORT_SYMBOL(vmemdup_user);\n\n \nchar *strndup_user(const char __user *s, long n)\n{\n\tchar *p;\n\tlong length;\n\n\tlength = strnlen_user(s, n);\n\n\tif (!length)\n\t\treturn ERR_PTR(-EFAULT);\n\n\tif (length > n)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tp = memdup_user(s, length);\n\n\tif (IS_ERR(p))\n\t\treturn p;\n\n\tp[length - 1] = '\\0';\n\n\treturn p;\n}\nEXPORT_SYMBOL(strndup_user);\n\n \nvoid *memdup_user_nul(const void __user *src, size_t len)\n{\n\tchar *p;\n\n\t \n\tp = kmalloc_track_caller(len + 1, GFP_KERNEL);\n\tif (!p)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (copy_from_user(p, src, len)) {\n\t\tkfree(p);\n\t\treturn ERR_PTR(-EFAULT);\n\t}\n\tp[len] = '\\0';\n\n\treturn p;\n}\nEXPORT_SYMBOL(memdup_user_nul);\n\n \nint vma_is_stack_for_current(struct vm_area_struct *vma)\n{\n\tstruct task_struct * __maybe_unused t = current;\n\n\treturn (vma->vm_start <= KSTK_ESP(t) && vma->vm_end >= KSTK_ESP(t));\n}\n\n \nvoid vma_set_file(struct vm_area_struct *vma, struct file *file)\n{\n\t \n\tget_file(file);\n\tswap(vma->vm_file, file);\n\tfput(file);\n}\nEXPORT_SYMBOL(vma_set_file);\n\n#ifndef STACK_RND_MASK\n#define STACK_RND_MASK (0x7ff >> (PAGE_SHIFT - 12))      \n#endif\n\nunsigned long randomize_stack_top(unsigned long stack_top)\n{\n\tunsigned long random_variable = 0;\n\n\tif (current->flags & PF_RANDOMIZE) {\n\t\trandom_variable = get_random_long();\n\t\trandom_variable &= STACK_RND_MASK;\n\t\trandom_variable <<= PAGE_SHIFT;\n\t}\n#ifdef CONFIG_STACK_GROWSUP\n\treturn PAGE_ALIGN(stack_top) + random_variable;\n#else\n\treturn PAGE_ALIGN(stack_top) - random_variable;\n#endif\n}\n\n \nunsigned long randomize_page(unsigned long start, unsigned long range)\n{\n\tif (!PAGE_ALIGNED(start)) {\n\t\trange -= PAGE_ALIGN(start) - start;\n\t\tstart = PAGE_ALIGN(start);\n\t}\n\n\tif (start > ULONG_MAX - range)\n\t\trange = ULONG_MAX - start;\n\n\trange >>= PAGE_SHIFT;\n\n\tif (range == 0)\n\t\treturn start;\n\n\treturn start + (get_random_long() % range << PAGE_SHIFT);\n}\n\n#ifdef CONFIG_ARCH_WANT_DEFAULT_TOPDOWN_MMAP_LAYOUT\nunsigned long __weak arch_randomize_brk(struct mm_struct *mm)\n{\n\t \n\tif (!IS_ENABLED(CONFIG_64BIT) || is_compat_task())\n\t\treturn randomize_page(mm->brk, SZ_32M);\n\n\treturn randomize_page(mm->brk, SZ_1G);\n}\n\nunsigned long arch_mmap_rnd(void)\n{\n\tunsigned long rnd;\n\n#ifdef CONFIG_HAVE_ARCH_MMAP_RND_COMPAT_BITS\n\tif (is_compat_task())\n\t\trnd = get_random_long() & ((1UL << mmap_rnd_compat_bits) - 1);\n\telse\n#endif  \n\t\trnd = get_random_long() & ((1UL << mmap_rnd_bits) - 1);\n\n\treturn rnd << PAGE_SHIFT;\n}\n\nstatic int mmap_is_legacy(struct rlimit *rlim_stack)\n{\n\tif (current->personality & ADDR_COMPAT_LAYOUT)\n\t\treturn 1;\n\n\t \n\tif (rlim_stack->rlim_cur == RLIM_INFINITY &&\n\t\t!IS_ENABLED(CONFIG_STACK_GROWSUP))\n\t\treturn 1;\n\n\treturn sysctl_legacy_va_layout;\n}\n\n \n#define MIN_GAP\t\t(SZ_128M)\n#define MAX_GAP\t\t(STACK_TOP / 6 * 5)\n\nstatic unsigned long mmap_base(unsigned long rnd, struct rlimit *rlim_stack)\n{\n#ifdef CONFIG_STACK_GROWSUP\n\t \n\treturn PAGE_ALIGN_DOWN(mmap_upper_limit(rlim_stack) - rnd);\n#else\n\tunsigned long gap = rlim_stack->rlim_cur;\n\tunsigned long pad = stack_guard_gap;\n\n\t \n\tif (current->flags & PF_RANDOMIZE)\n\t\tpad += (STACK_RND_MASK << PAGE_SHIFT);\n\n\t \n\tif (gap + pad > gap)\n\t\tgap += pad;\n\n\tif (gap < MIN_GAP)\n\t\tgap = MIN_GAP;\n\telse if (gap > MAX_GAP)\n\t\tgap = MAX_GAP;\n\n\treturn PAGE_ALIGN(STACK_TOP - gap - rnd);\n#endif\n}\n\nvoid arch_pick_mmap_layout(struct mm_struct *mm, struct rlimit *rlim_stack)\n{\n\tunsigned long random_factor = 0UL;\n\n\tif (current->flags & PF_RANDOMIZE)\n\t\trandom_factor = arch_mmap_rnd();\n\n\tif (mmap_is_legacy(rlim_stack)) {\n\t\tmm->mmap_base = TASK_UNMAPPED_BASE + random_factor;\n\t\tmm->get_unmapped_area = arch_get_unmapped_area;\n\t} else {\n\t\tmm->mmap_base = mmap_base(random_factor, rlim_stack);\n\t\tmm->get_unmapped_area = arch_get_unmapped_area_topdown;\n\t}\n}\n#elif defined(CONFIG_MMU) && !defined(HAVE_ARCH_PICK_MMAP_LAYOUT)\nvoid arch_pick_mmap_layout(struct mm_struct *mm, struct rlimit *rlim_stack)\n{\n\tmm->mmap_base = TASK_UNMAPPED_BASE;\n\tmm->get_unmapped_area = arch_get_unmapped_area;\n}\n#endif\n\n \nint __account_locked_vm(struct mm_struct *mm, unsigned long pages, bool inc,\n\t\t\tstruct task_struct *task, bool bypass_rlim)\n{\n\tunsigned long locked_vm, limit;\n\tint ret = 0;\n\n\tmmap_assert_write_locked(mm);\n\n\tlocked_vm = mm->locked_vm;\n\tif (inc) {\n\t\tif (!bypass_rlim) {\n\t\t\tlimit = task_rlimit(task, RLIMIT_MEMLOCK) >> PAGE_SHIFT;\n\t\t\tif (locked_vm + pages > limit)\n\t\t\t\tret = -ENOMEM;\n\t\t}\n\t\tif (!ret)\n\t\t\tmm->locked_vm = locked_vm + pages;\n\t} else {\n\t\tWARN_ON_ONCE(pages > locked_vm);\n\t\tmm->locked_vm = locked_vm - pages;\n\t}\n\n\tpr_debug(\"%s: [%d] caller %ps %c%lu %lu/%lu%s\\n\", __func__, task->pid,\n\t\t (void *)_RET_IP_, (inc) ? '+' : '-', pages << PAGE_SHIFT,\n\t\t locked_vm << PAGE_SHIFT, task_rlimit(task, RLIMIT_MEMLOCK),\n\t\t ret ? \" - exceeded\" : \"\");\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(__account_locked_vm);\n\n \nint account_locked_vm(struct mm_struct *mm, unsigned long pages, bool inc)\n{\n\tint ret;\n\n\tif (pages == 0 || !mm)\n\t\treturn 0;\n\n\tmmap_write_lock(mm);\n\tret = __account_locked_vm(mm, pages, inc, current,\n\t\t\t\t  capable(CAP_IPC_LOCK));\n\tmmap_write_unlock(mm);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(account_locked_vm);\n\nunsigned long vm_mmap_pgoff(struct file *file, unsigned long addr,\n\tunsigned long len, unsigned long prot,\n\tunsigned long flag, unsigned long pgoff)\n{\n\tunsigned long ret;\n\tstruct mm_struct *mm = current->mm;\n\tunsigned long populate;\n\tLIST_HEAD(uf);\n\n\tret = security_mmap_file(file, prot, flag);\n\tif (!ret) {\n\t\tif (mmap_write_lock_killable(mm))\n\t\t\treturn -EINTR;\n\t\tret = do_mmap(file, addr, len, prot, flag, 0, pgoff, &populate,\n\t\t\t      &uf);\n\t\tmmap_write_unlock(mm);\n\t\tuserfaultfd_unmap_complete(mm, &uf);\n\t\tif (populate)\n\t\t\tmm_populate(ret, populate);\n\t}\n\treturn ret;\n}\n\nunsigned long vm_mmap(struct file *file, unsigned long addr,\n\tunsigned long len, unsigned long prot,\n\tunsigned long flag, unsigned long offset)\n{\n\tif (unlikely(offset + PAGE_ALIGN(len) < offset))\n\t\treturn -EINVAL;\n\tif (unlikely(offset_in_page(offset)))\n\t\treturn -EINVAL;\n\n\treturn vm_mmap_pgoff(file, addr, len, prot, flag, offset >> PAGE_SHIFT);\n}\nEXPORT_SYMBOL(vm_mmap);\n\n \nvoid *kvmalloc_node(size_t size, gfp_t flags, int node)\n{\n\tgfp_t kmalloc_flags = flags;\n\tvoid *ret;\n\n\t \n\tif (size > PAGE_SIZE) {\n\t\tkmalloc_flags |= __GFP_NOWARN;\n\n\t\tif (!(kmalloc_flags & __GFP_RETRY_MAYFAIL))\n\t\t\tkmalloc_flags |= __GFP_NORETRY;\n\n\t\t \n\t\tkmalloc_flags &= ~__GFP_NOFAIL;\n\t}\n\n\tret = kmalloc_node(size, kmalloc_flags, node);\n\n\t \n\tif (ret || size <= PAGE_SIZE)\n\t\treturn ret;\n\n\t \n\tif (!gfpflags_allow_blocking(flags))\n\t\treturn NULL;\n\n\t \n\tif (unlikely(size > INT_MAX)) {\n\t\tWARN_ON_ONCE(!(flags & __GFP_NOWARN));\n\t\treturn NULL;\n\t}\n\n\t \n\treturn __vmalloc_node_range(size, 1, VMALLOC_START, VMALLOC_END,\n\t\t\tflags, PAGE_KERNEL, VM_ALLOW_HUGE_VMAP,\n\t\t\tnode, __builtin_return_address(0));\n}\nEXPORT_SYMBOL(kvmalloc_node);\n\n \nvoid kvfree(const void *addr)\n{\n\tif (is_vmalloc_addr(addr))\n\t\tvfree(addr);\n\telse\n\t\tkfree(addr);\n}\nEXPORT_SYMBOL(kvfree);\n\n \nvoid kvfree_sensitive(const void *addr, size_t len)\n{\n\tif (likely(!ZERO_OR_NULL_PTR(addr))) {\n\t\tmemzero_explicit((void *)addr, len);\n\t\tkvfree(addr);\n\t}\n}\nEXPORT_SYMBOL(kvfree_sensitive);\n\nvoid *kvrealloc(const void *p, size_t oldsize, size_t newsize, gfp_t flags)\n{\n\tvoid *newp;\n\n\tif (oldsize >= newsize)\n\t\treturn (void *)p;\n\tnewp = kvmalloc(newsize, flags);\n\tif (!newp)\n\t\treturn NULL;\n\tmemcpy(newp, p, oldsize);\n\tkvfree(p);\n\treturn newp;\n}\nEXPORT_SYMBOL(kvrealloc);\n\n \nvoid *__vmalloc_array(size_t n, size_t size, gfp_t flags)\n{\n\tsize_t bytes;\n\n\tif (unlikely(check_mul_overflow(n, size, &bytes)))\n\t\treturn NULL;\n\treturn __vmalloc(bytes, flags);\n}\nEXPORT_SYMBOL(__vmalloc_array);\n\n \nvoid *vmalloc_array(size_t n, size_t size)\n{\n\treturn __vmalloc_array(n, size, GFP_KERNEL);\n}\nEXPORT_SYMBOL(vmalloc_array);\n\n \nvoid *__vcalloc(size_t n, size_t size, gfp_t flags)\n{\n\treturn __vmalloc_array(n, size, flags | __GFP_ZERO);\n}\nEXPORT_SYMBOL(__vcalloc);\n\n \nvoid *vcalloc(size_t n, size_t size)\n{\n\treturn __vmalloc_array(n, size, GFP_KERNEL | __GFP_ZERO);\n}\nEXPORT_SYMBOL(vcalloc);\n\nstruct anon_vma *folio_anon_vma(struct folio *folio)\n{\n\tunsigned long mapping = (unsigned long)folio->mapping;\n\n\tif ((mapping & PAGE_MAPPING_FLAGS) != PAGE_MAPPING_ANON)\n\t\treturn NULL;\n\treturn (void *)(mapping - PAGE_MAPPING_ANON);\n}\n\n \nstruct address_space *folio_mapping(struct folio *folio)\n{\n\tstruct address_space *mapping;\n\n\t \n\tif (unlikely(folio_test_slab(folio)))\n\t\treturn NULL;\n\n\tif (unlikely(folio_test_swapcache(folio)))\n\t\treturn swap_address_space(folio->swap);\n\n\tmapping = folio->mapping;\n\tif ((unsigned long)mapping & PAGE_MAPPING_FLAGS)\n\t\treturn NULL;\n\n\treturn mapping;\n}\nEXPORT_SYMBOL(folio_mapping);\n\n \nvoid folio_copy(struct folio *dst, struct folio *src)\n{\n\tlong i = 0;\n\tlong nr = folio_nr_pages(src);\n\n\tfor (;;) {\n\t\tcopy_highpage(folio_page(dst, i), folio_page(src, i));\n\t\tif (++i == nr)\n\t\t\tbreak;\n\t\tcond_resched();\n\t}\n}\n\nint sysctl_overcommit_memory __read_mostly = OVERCOMMIT_GUESS;\nint sysctl_overcommit_ratio __read_mostly = 50;\nunsigned long sysctl_overcommit_kbytes __read_mostly;\nint sysctl_max_map_count __read_mostly = DEFAULT_MAX_MAP_COUNT;\nunsigned long sysctl_user_reserve_kbytes __read_mostly = 1UL << 17;  \nunsigned long sysctl_admin_reserve_kbytes __read_mostly = 1UL << 13;  \n\nint overcommit_ratio_handler(struct ctl_table *table, int write, void *buffer,\n\t\tsize_t *lenp, loff_t *ppos)\n{\n\tint ret;\n\n\tret = proc_dointvec(table, write, buffer, lenp, ppos);\n\tif (ret == 0 && write)\n\t\tsysctl_overcommit_kbytes = 0;\n\treturn ret;\n}\n\nstatic void sync_overcommit_as(struct work_struct *dummy)\n{\n\tpercpu_counter_sync(&vm_committed_as);\n}\n\nint overcommit_policy_handler(struct ctl_table *table, int write, void *buffer,\n\t\tsize_t *lenp, loff_t *ppos)\n{\n\tstruct ctl_table t;\n\tint new_policy = -1;\n\tint ret;\n\n\t \n\tif (write) {\n\t\tt = *table;\n\t\tt.data = &new_policy;\n\t\tret = proc_dointvec_minmax(&t, write, buffer, lenp, ppos);\n\t\tif (ret || new_policy == -1)\n\t\t\treturn ret;\n\n\t\tmm_compute_batch(new_policy);\n\t\tif (new_policy == OVERCOMMIT_NEVER)\n\t\t\tschedule_on_each_cpu(sync_overcommit_as);\n\t\tsysctl_overcommit_memory = new_policy;\n\t} else {\n\t\tret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);\n\t}\n\n\treturn ret;\n}\n\nint overcommit_kbytes_handler(struct ctl_table *table, int write, void *buffer,\n\t\tsize_t *lenp, loff_t *ppos)\n{\n\tint ret;\n\n\tret = proc_doulongvec_minmax(table, write, buffer, lenp, ppos);\n\tif (ret == 0 && write)\n\t\tsysctl_overcommit_ratio = 0;\n\treturn ret;\n}\n\n \nunsigned long vm_commit_limit(void)\n{\n\tunsigned long allowed;\n\n\tif (sysctl_overcommit_kbytes)\n\t\tallowed = sysctl_overcommit_kbytes >> (PAGE_SHIFT - 10);\n\telse\n\t\tallowed = ((totalram_pages() - hugetlb_total_pages())\n\t\t\t   * sysctl_overcommit_ratio / 100);\n\tallowed += total_swap_pages;\n\n\treturn allowed;\n}\n\n \nstruct percpu_counter vm_committed_as ____cacheline_aligned_in_smp;\n\n \nunsigned long vm_memory_committed(void)\n{\n\treturn percpu_counter_sum_positive(&vm_committed_as);\n}\nEXPORT_SYMBOL_GPL(vm_memory_committed);\n\n \nint __vm_enough_memory(struct mm_struct *mm, long pages, int cap_sys_admin)\n{\n\tlong allowed;\n\n\tvm_acct_memory(pages);\n\n\t \n\tif (sysctl_overcommit_memory == OVERCOMMIT_ALWAYS)\n\t\treturn 0;\n\n\tif (sysctl_overcommit_memory == OVERCOMMIT_GUESS) {\n\t\tif (pages > totalram_pages() + total_swap_pages)\n\t\t\tgoto error;\n\t\treturn 0;\n\t}\n\n\tallowed = vm_commit_limit();\n\t \n\tif (!cap_sys_admin)\n\t\tallowed -= sysctl_admin_reserve_kbytes >> (PAGE_SHIFT - 10);\n\n\t \n\tif (mm) {\n\t\tlong reserve = sysctl_user_reserve_kbytes >> (PAGE_SHIFT - 10);\n\n\t\tallowed -= min_t(long, mm->total_vm / 32, reserve);\n\t}\n\n\tif (percpu_counter_read_positive(&vm_committed_as) < allowed)\n\t\treturn 0;\nerror:\n\tpr_warn_ratelimited(\"%s: pid: %d, comm: %s, not enough memory for the allocation\\n\",\n\t\t\t    __func__, current->pid, current->comm);\n\tvm_unacct_memory(pages);\n\n\treturn -ENOMEM;\n}\n\n \nint get_cmdline(struct task_struct *task, char *buffer, int buflen)\n{\n\tint res = 0;\n\tunsigned int len;\n\tstruct mm_struct *mm = get_task_mm(task);\n\tunsigned long arg_start, arg_end, env_start, env_end;\n\tif (!mm)\n\t\tgoto out;\n\tif (!mm->arg_end)\n\t\tgoto out_mm;\t \n\n\tspin_lock(&mm->arg_lock);\n\targ_start = mm->arg_start;\n\targ_end = mm->arg_end;\n\tenv_start = mm->env_start;\n\tenv_end = mm->env_end;\n\tspin_unlock(&mm->arg_lock);\n\n\tlen = arg_end - arg_start;\n\n\tif (len > buflen)\n\t\tlen = buflen;\n\n\tres = access_process_vm(task, arg_start, buffer, len, FOLL_FORCE);\n\n\t \n\tif (res > 0 && buffer[res-1] != '\\0' && len < buflen) {\n\t\tlen = strnlen(buffer, res);\n\t\tif (len < res) {\n\t\t\tres = len;\n\t\t} else {\n\t\t\tlen = env_end - env_start;\n\t\t\tif (len > buflen - res)\n\t\t\t\tlen = buflen - res;\n\t\t\tres += access_process_vm(task, env_start,\n\t\t\t\t\t\t buffer+res, len,\n\t\t\t\t\t\t FOLL_FORCE);\n\t\t\tres = strnlen(buffer, res);\n\t\t}\n\t}\nout_mm:\n\tmmput(mm);\nout:\n\treturn res;\n}\n\nint __weak memcmp_pages(struct page *page1, struct page *page2)\n{\n\tchar *addr1, *addr2;\n\tint ret;\n\n\taddr1 = kmap_atomic(page1);\n\taddr2 = kmap_atomic(page2);\n\tret = memcmp(addr1, addr2, PAGE_SIZE);\n\tkunmap_atomic(addr2);\n\tkunmap_atomic(addr1);\n\treturn ret;\n}\n\n#ifdef CONFIG_PRINTK\n \nvoid mem_dump_obj(void *object)\n{\n\tconst char *type;\n\n\tif (kmem_valid_obj(object)) {\n\t\tkmem_dump_obj(object);\n\t\treturn;\n\t}\n\n\tif (vmalloc_dump_obj(object))\n\t\treturn;\n\n\tif (is_vmalloc_addr(object))\n\t\ttype = \"vmalloc memory\";\n\telse if (virt_addr_valid(object))\n\t\ttype = \"non-slab/vmalloc memory\";\n\telse if (object == NULL)\n\t\ttype = \"NULL pointer\";\n\telse if (object == ZERO_SIZE_PTR)\n\t\ttype = \"zero-size pointer\";\n\telse\n\t\ttype = \"non-paged memory\";\n\n\tpr_cont(\" %s\\n\", type);\n}\nEXPORT_SYMBOL_GPL(mem_dump_obj);\n#endif\n\n \nstatic DECLARE_RWSEM(page_offline_rwsem);\n\nvoid page_offline_freeze(void)\n{\n\tdown_read(&page_offline_rwsem);\n}\n\nvoid page_offline_thaw(void)\n{\n\tup_read(&page_offline_rwsem);\n}\n\nvoid page_offline_begin(void)\n{\n\tdown_write(&page_offline_rwsem);\n}\nEXPORT_SYMBOL(page_offline_begin);\n\nvoid page_offline_end(void)\n{\n\tup_write(&page_offline_rwsem);\n}\nEXPORT_SYMBOL(page_offline_end);\n\n#ifndef flush_dcache_folio\nvoid flush_dcache_folio(struct folio *folio)\n{\n\tlong i, nr = folio_nr_pages(folio);\n\n\tfor (i = 0; i < nr; i++)\n\t\tflush_dcache_page(folio_page(folio, i));\n}\nEXPORT_SYMBOL(flush_dcache_folio);\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}