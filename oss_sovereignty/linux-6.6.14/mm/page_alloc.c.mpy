{
  "module_name": "page_alloc.c",
  "hash_id": "daaf4384d50f059d3f740c8c94230fc0cd92b8190d0912726e29ff42d0d8f0d6",
  "original_prompt": "Ingested from linux-6.6.14/mm/page_alloc.c",
  "human_readable_source": "\n \n\n#include <linux/stddef.h>\n#include <linux/mm.h>\n#include <linux/highmem.h>\n#include <linux/interrupt.h>\n#include <linux/jiffies.h>\n#include <linux/compiler.h>\n#include <linux/kernel.h>\n#include <linux/kasan.h>\n#include <linux/kmsan.h>\n#include <linux/module.h>\n#include <linux/suspend.h>\n#include <linux/ratelimit.h>\n#include <linux/oom.h>\n#include <linux/topology.h>\n#include <linux/sysctl.h>\n#include <linux/cpu.h>\n#include <linux/cpuset.h>\n#include <linux/memory_hotplug.h>\n#include <linux/nodemask.h>\n#include <linux/vmstat.h>\n#include <linux/fault-inject.h>\n#include <linux/compaction.h>\n#include <trace/events/kmem.h>\n#include <trace/events/oom.h>\n#include <linux/prefetch.h>\n#include <linux/mm_inline.h>\n#include <linux/mmu_notifier.h>\n#include <linux/migrate.h>\n#include <linux/sched/mm.h>\n#include <linux/page_owner.h>\n#include <linux/page_table_check.h>\n#include <linux/memcontrol.h>\n#include <linux/ftrace.h>\n#include <linux/lockdep.h>\n#include <linux/psi.h>\n#include <linux/khugepaged.h>\n#include <linux/delayacct.h>\n#include <asm/div64.h>\n#include \"internal.h\"\n#include \"shuffle.h\"\n#include \"page_reporting.h\"\n\n \ntypedef int __bitwise fpi_t;\n\n \n#define FPI_NONE\t\t((__force fpi_t)0)\n\n \n#define FPI_SKIP_REPORT_NOTIFY\t((__force fpi_t)BIT(0))\n\n \n#define FPI_TO_TAIL\t\t((__force fpi_t)BIT(1))\n\n \nstatic DEFINE_MUTEX(pcp_batch_high_lock);\n#define MIN_PERCPU_PAGELIST_HIGH_FRACTION (8)\n\n#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT_RT)\n \n#define pcp_trylock_prepare(flags)\tdo { } while (0)\n#define pcp_trylock_finish(flag)\tdo { } while (0)\n#else\n\n \n#define pcp_trylock_prepare(flags)\tlocal_irq_save(flags)\n#define pcp_trylock_finish(flags)\tlocal_irq_restore(flags)\n#endif\n\n \n#ifndef CONFIG_PREEMPT_RT\n#define pcpu_task_pin()\t\tpreempt_disable()\n#define pcpu_task_unpin()\tpreempt_enable()\n#else\n#define pcpu_task_pin()\t\tmigrate_disable()\n#define pcpu_task_unpin()\tmigrate_enable()\n#endif\n\n \n#define pcpu_spin_lock(type, member, ptr)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\ttype *_ret;\t\t\t\t\t\t\t\\\n\tpcpu_task_pin();\t\t\t\t\t\t\\\n\t_ret = this_cpu_ptr(ptr);\t\t\t\t\t\\\n\tspin_lock(&_ret->member);\t\t\t\t\t\\\n\t_ret;\t\t\t\t\t\t\t\t\\\n})\n\n#define pcpu_spin_trylock(type, member, ptr)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\ttype *_ret;\t\t\t\t\t\t\t\\\n\tpcpu_task_pin();\t\t\t\t\t\t\\\n\t_ret = this_cpu_ptr(ptr);\t\t\t\t\t\\\n\tif (!spin_trylock(&_ret->member)) {\t\t\t\t\\\n\t\tpcpu_task_unpin();\t\t\t\t\t\\\n\t\t_ret = NULL;\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\t_ret;\t\t\t\t\t\t\t\t\\\n})\n\n#define pcpu_spin_unlock(member, ptr)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tspin_unlock(&ptr->member);\t\t\t\t\t\\\n\tpcpu_task_unpin();\t\t\t\t\t\t\\\n})\n\n \n#define pcp_spin_lock(ptr)\t\t\t\t\t\t\\\n\tpcpu_spin_lock(struct per_cpu_pages, lock, ptr)\n\n#define pcp_spin_trylock(ptr)\t\t\t\t\t\t\\\n\tpcpu_spin_trylock(struct per_cpu_pages, lock, ptr)\n\n#define pcp_spin_unlock(ptr)\t\t\t\t\t\t\\\n\tpcpu_spin_unlock(lock, ptr)\n\n#ifdef CONFIG_USE_PERCPU_NUMA_NODE_ID\nDEFINE_PER_CPU(int, numa_node);\nEXPORT_PER_CPU_SYMBOL(numa_node);\n#endif\n\nDEFINE_STATIC_KEY_TRUE(vm_numa_stat_key);\n\n#ifdef CONFIG_HAVE_MEMORYLESS_NODES\n \nDEFINE_PER_CPU(int, _numa_mem_);\t\t \nEXPORT_PER_CPU_SYMBOL(_numa_mem_);\n#endif\n\nstatic DEFINE_MUTEX(pcpu_drain_mutex);\n\n#ifdef CONFIG_GCC_PLUGIN_LATENT_ENTROPY\nvolatile unsigned long latent_entropy __latent_entropy;\nEXPORT_SYMBOL(latent_entropy);\n#endif\n\n \nnodemask_t node_states[NR_NODE_STATES] __read_mostly = {\n\t[N_POSSIBLE] = NODE_MASK_ALL,\n\t[N_ONLINE] = { { [0] = 1UL } },\n#ifndef CONFIG_NUMA\n\t[N_NORMAL_MEMORY] = { { [0] = 1UL } },\n#ifdef CONFIG_HIGHMEM\n\t[N_HIGH_MEMORY] = { { [0] = 1UL } },\n#endif\n\t[N_MEMORY] = { { [0] = 1UL } },\n\t[N_CPU] = { { [0] = 1UL } },\n#endif\t \n};\nEXPORT_SYMBOL(node_states);\n\ngfp_t gfp_allowed_mask __read_mostly = GFP_BOOT_MASK;\n\n \nstatic inline int get_pcppage_migratetype(struct page *page)\n{\n\treturn page->index;\n}\n\nstatic inline void set_pcppage_migratetype(struct page *page, int migratetype)\n{\n\tpage->index = migratetype;\n}\n\n#ifdef CONFIG_HUGETLB_PAGE_SIZE_VARIABLE\nunsigned int pageblock_order __read_mostly;\n#endif\n\nstatic void __free_pages_ok(struct page *page, unsigned int order,\n\t\t\t    fpi_t fpi_flags);\n\n \nstatic int sysctl_lowmem_reserve_ratio[MAX_NR_ZONES] = {\n#ifdef CONFIG_ZONE_DMA\n\t[ZONE_DMA] = 256,\n#endif\n#ifdef CONFIG_ZONE_DMA32\n\t[ZONE_DMA32] = 256,\n#endif\n\t[ZONE_NORMAL] = 32,\n#ifdef CONFIG_HIGHMEM\n\t[ZONE_HIGHMEM] = 0,\n#endif\n\t[ZONE_MOVABLE] = 0,\n};\n\nchar * const zone_names[MAX_NR_ZONES] = {\n#ifdef CONFIG_ZONE_DMA\n\t \"DMA\",\n#endif\n#ifdef CONFIG_ZONE_DMA32\n\t \"DMA32\",\n#endif\n\t \"Normal\",\n#ifdef CONFIG_HIGHMEM\n\t \"HighMem\",\n#endif\n\t \"Movable\",\n#ifdef CONFIG_ZONE_DEVICE\n\t \"Device\",\n#endif\n};\n\nconst char * const migratetype_names[MIGRATE_TYPES] = {\n\t\"Unmovable\",\n\t\"Movable\",\n\t\"Reclaimable\",\n\t\"HighAtomic\",\n#ifdef CONFIG_CMA\n\t\"CMA\",\n#endif\n#ifdef CONFIG_MEMORY_ISOLATION\n\t\"Isolate\",\n#endif\n};\n\nint min_free_kbytes = 1024;\nint user_min_free_kbytes = -1;\nstatic int watermark_boost_factor __read_mostly = 15000;\nstatic int watermark_scale_factor = 10;\n\n \nint movable_zone;\nEXPORT_SYMBOL(movable_zone);\n\n#if MAX_NUMNODES > 1\nunsigned int nr_node_ids __read_mostly = MAX_NUMNODES;\nunsigned int nr_online_nodes __read_mostly = 1;\nEXPORT_SYMBOL(nr_node_ids);\nEXPORT_SYMBOL(nr_online_nodes);\n#endif\n\nstatic bool page_contains_unaccepted(struct page *page, unsigned int order);\nstatic void accept_page(struct page *page, unsigned int order);\nstatic bool try_to_accept_memory(struct zone *zone, unsigned int order);\nstatic inline bool has_unaccepted_memory(void);\nstatic bool __free_unaccepted(struct page *page);\n\nint page_group_by_mobility_disabled __read_mostly;\n\n#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT\n \nDEFINE_STATIC_KEY_TRUE(deferred_pages);\n\nstatic inline bool deferred_pages_enabled(void)\n{\n\treturn static_branch_unlikely(&deferred_pages);\n}\n\n \nstatic bool __ref\n_deferred_grow_zone(struct zone *zone, unsigned int order)\n{\n       return deferred_grow_zone(zone, order);\n}\n#else\nstatic inline bool deferred_pages_enabled(void)\n{\n\treturn false;\n}\n#endif  \n\n \nstatic inline unsigned long *get_pageblock_bitmap(const struct page *page,\n\t\t\t\t\t\t\tunsigned long pfn)\n{\n#ifdef CONFIG_SPARSEMEM\n\treturn section_to_usemap(__pfn_to_section(pfn));\n#else\n\treturn page_zone(page)->pageblock_flags;\n#endif  \n}\n\nstatic inline int pfn_to_bitidx(const struct page *page, unsigned long pfn)\n{\n#ifdef CONFIG_SPARSEMEM\n\tpfn &= (PAGES_PER_SECTION-1);\n#else\n\tpfn = pfn - pageblock_start_pfn(page_zone(page)->zone_start_pfn);\n#endif  \n\treturn (pfn >> pageblock_order) * NR_PAGEBLOCK_BITS;\n}\n\n \nunsigned long get_pfnblock_flags_mask(const struct page *page,\n\t\t\t\t\tunsigned long pfn, unsigned long mask)\n{\n\tunsigned long *bitmap;\n\tunsigned long bitidx, word_bitidx;\n\tunsigned long word;\n\n\tbitmap = get_pageblock_bitmap(page, pfn);\n\tbitidx = pfn_to_bitidx(page, pfn);\n\tword_bitidx = bitidx / BITS_PER_LONG;\n\tbitidx &= (BITS_PER_LONG-1);\n\t \n\tword = READ_ONCE(bitmap[word_bitidx]);\n\treturn (word >> bitidx) & mask;\n}\n\nstatic __always_inline int get_pfnblock_migratetype(const struct page *page,\n\t\t\t\t\tunsigned long pfn)\n{\n\treturn get_pfnblock_flags_mask(page, pfn, MIGRATETYPE_MASK);\n}\n\n \nvoid set_pfnblock_flags_mask(struct page *page, unsigned long flags,\n\t\t\t\t\tunsigned long pfn,\n\t\t\t\t\tunsigned long mask)\n{\n\tunsigned long *bitmap;\n\tunsigned long bitidx, word_bitidx;\n\tunsigned long word;\n\n\tBUILD_BUG_ON(NR_PAGEBLOCK_BITS != 4);\n\tBUILD_BUG_ON(MIGRATE_TYPES > (1 << PB_migratetype_bits));\n\n\tbitmap = get_pageblock_bitmap(page, pfn);\n\tbitidx = pfn_to_bitidx(page, pfn);\n\tword_bitidx = bitidx / BITS_PER_LONG;\n\tbitidx &= (BITS_PER_LONG-1);\n\n\tVM_BUG_ON_PAGE(!zone_spans_pfn(page_zone(page), pfn), page);\n\n\tmask <<= bitidx;\n\tflags <<= bitidx;\n\n\tword = READ_ONCE(bitmap[word_bitidx]);\n\tdo {\n\t} while (!try_cmpxchg(&bitmap[word_bitidx], &word, (word & ~mask) | flags));\n}\n\nvoid set_pageblock_migratetype(struct page *page, int migratetype)\n{\n\tif (unlikely(page_group_by_mobility_disabled &&\n\t\t     migratetype < MIGRATE_PCPTYPES))\n\t\tmigratetype = MIGRATE_UNMOVABLE;\n\n\tset_pfnblock_flags_mask(page, (unsigned long)migratetype,\n\t\t\t\tpage_to_pfn(page), MIGRATETYPE_MASK);\n}\n\n#ifdef CONFIG_DEBUG_VM\nstatic int page_outside_zone_boundaries(struct zone *zone, struct page *page)\n{\n\tint ret;\n\tunsigned seq;\n\tunsigned long pfn = page_to_pfn(page);\n\tunsigned long sp, start_pfn;\n\n\tdo {\n\t\tseq = zone_span_seqbegin(zone);\n\t\tstart_pfn = zone->zone_start_pfn;\n\t\tsp = zone->spanned_pages;\n\t\tret = !zone_spans_pfn(zone, pfn);\n\t} while (zone_span_seqretry(zone, seq));\n\n\tif (ret)\n\t\tpr_err(\"page 0x%lx outside node %d zone %s [ 0x%lx - 0x%lx ]\\n\",\n\t\t\tpfn, zone_to_nid(zone), zone->name,\n\t\t\tstart_pfn, start_pfn + sp);\n\n\treturn ret;\n}\n\n \nstatic int __maybe_unused bad_range(struct zone *zone, struct page *page)\n{\n\tif (page_outside_zone_boundaries(zone, page))\n\t\treturn 1;\n\tif (zone != page_zone(page))\n\t\treturn 1;\n\n\treturn 0;\n}\n#else\nstatic inline int __maybe_unused bad_range(struct zone *zone, struct page *page)\n{\n\treturn 0;\n}\n#endif\n\nstatic void bad_page(struct page *page, const char *reason)\n{\n\tstatic unsigned long resume;\n\tstatic unsigned long nr_shown;\n\tstatic unsigned long nr_unshown;\n\n\t \n\tif (nr_shown == 60) {\n\t\tif (time_before(jiffies, resume)) {\n\t\t\tnr_unshown++;\n\t\t\tgoto out;\n\t\t}\n\t\tif (nr_unshown) {\n\t\t\tpr_alert(\n\t\t\t      \"BUG: Bad page state: %lu messages suppressed\\n\",\n\t\t\t\tnr_unshown);\n\t\t\tnr_unshown = 0;\n\t\t}\n\t\tnr_shown = 0;\n\t}\n\tif (nr_shown++ == 0)\n\t\tresume = jiffies + 60 * HZ;\n\n\tpr_alert(\"BUG: Bad page state in process %s  pfn:%05lx\\n\",\n\t\tcurrent->comm, page_to_pfn(page));\n\tdump_page(page, reason);\n\n\tprint_modules();\n\tdump_stack();\nout:\n\t \n\tpage_mapcount_reset(page);  \n\tadd_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);\n}\n\nstatic inline unsigned int order_to_pindex(int migratetype, int order)\n{\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tif (order > PAGE_ALLOC_COSTLY_ORDER) {\n\t\tVM_BUG_ON(order != pageblock_order);\n\t\treturn NR_LOWORDER_PCP_LISTS;\n\t}\n#else\n\tVM_BUG_ON(order > PAGE_ALLOC_COSTLY_ORDER);\n#endif\n\n\treturn (MIGRATE_PCPTYPES * order) + migratetype;\n}\n\nstatic inline int pindex_to_order(unsigned int pindex)\n{\n\tint order = pindex / MIGRATE_PCPTYPES;\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tif (pindex == NR_LOWORDER_PCP_LISTS)\n\t\torder = pageblock_order;\n#else\n\tVM_BUG_ON(order > PAGE_ALLOC_COSTLY_ORDER);\n#endif\n\n\treturn order;\n}\n\nstatic inline bool pcp_allowed_order(unsigned int order)\n{\n\tif (order <= PAGE_ALLOC_COSTLY_ORDER)\n\t\treturn true;\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tif (order == pageblock_order)\n\t\treturn true;\n#endif\n\treturn false;\n}\n\nstatic inline void free_the_page(struct page *page, unsigned int order)\n{\n\tif (pcp_allowed_order(order))\t\t \n\t\tfree_unref_page(page, order);\n\telse\n\t\t__free_pages_ok(page, order, FPI_NONE);\n}\n\n \n\nvoid prep_compound_page(struct page *page, unsigned int order)\n{\n\tint i;\n\tint nr_pages = 1 << order;\n\n\t__SetPageHead(page);\n\tfor (i = 1; i < nr_pages; i++)\n\t\tprep_compound_tail(page, i);\n\n\tprep_compound_head(page, order);\n}\n\nvoid destroy_large_folio(struct folio *folio)\n{\n\tif (folio_test_hugetlb(folio)) {\n\t\tfree_huge_folio(folio);\n\t\treturn;\n\t}\n\n\tif (folio_test_large_rmappable(folio))\n\t\tfolio_undo_large_rmappable(folio);\n\n\tmem_cgroup_uncharge(folio);\n\tfree_the_page(&folio->page, folio_order(folio));\n}\n\nstatic inline void set_buddy_order(struct page *page, unsigned int order)\n{\n\tset_page_private(page, order);\n\t__SetPageBuddy(page);\n}\n\n#ifdef CONFIG_COMPACTION\nstatic inline struct capture_control *task_capc(struct zone *zone)\n{\n\tstruct capture_control *capc = current->capture_control;\n\n\treturn unlikely(capc) &&\n\t\t!(current->flags & PF_KTHREAD) &&\n\t\t!capc->page &&\n\t\tcapc->cc->zone == zone ? capc : NULL;\n}\n\nstatic inline bool\ncompaction_capture(struct capture_control *capc, struct page *page,\n\t\t   int order, int migratetype)\n{\n\tif (!capc || order != capc->cc->order)\n\t\treturn false;\n\n\t \n\tif (is_migrate_cma(migratetype) ||\n\t    is_migrate_isolate(migratetype))\n\t\treturn false;\n\n\t \n\tif (order < pageblock_order && migratetype == MIGRATE_MOVABLE)\n\t\treturn false;\n\n\tcapc->page = page;\n\treturn true;\n}\n\n#else\nstatic inline struct capture_control *task_capc(struct zone *zone)\n{\n\treturn NULL;\n}\n\nstatic inline bool\ncompaction_capture(struct capture_control *capc, struct page *page,\n\t\t   int order, int migratetype)\n{\n\treturn false;\n}\n#endif  \n\n \nstatic inline void add_to_free_list(struct page *page, struct zone *zone,\n\t\t\t\t    unsigned int order, int migratetype)\n{\n\tstruct free_area *area = &zone->free_area[order];\n\n\tlist_add(&page->buddy_list, &area->free_list[migratetype]);\n\tarea->nr_free++;\n}\n\n \nstatic inline void add_to_free_list_tail(struct page *page, struct zone *zone,\n\t\t\t\t\t unsigned int order, int migratetype)\n{\n\tstruct free_area *area = &zone->free_area[order];\n\n\tlist_add_tail(&page->buddy_list, &area->free_list[migratetype]);\n\tarea->nr_free++;\n}\n\n \nstatic inline void move_to_free_list(struct page *page, struct zone *zone,\n\t\t\t\t     unsigned int order, int migratetype)\n{\n\tstruct free_area *area = &zone->free_area[order];\n\n\tlist_move_tail(&page->buddy_list, &area->free_list[migratetype]);\n}\n\nstatic inline void del_page_from_free_list(struct page *page, struct zone *zone,\n\t\t\t\t\t   unsigned int order)\n{\n\t \n\tif (page_reported(page))\n\t\t__ClearPageReported(page);\n\n\tlist_del(&page->buddy_list);\n\t__ClearPageBuddy(page);\n\tset_page_private(page, 0);\n\tzone->free_area[order].nr_free--;\n}\n\nstatic inline struct page *get_page_from_free_area(struct free_area *area,\n\t\t\t\t\t    int migratetype)\n{\n\treturn list_first_entry_or_null(&area->free_list[migratetype],\n\t\t\t\t\tstruct page, buddy_list);\n}\n\n \nstatic inline bool\nbuddy_merge_likely(unsigned long pfn, unsigned long buddy_pfn,\n\t\t   struct page *page, unsigned int order)\n{\n\tunsigned long higher_page_pfn;\n\tstruct page *higher_page;\n\n\tif (order >= MAX_ORDER - 1)\n\t\treturn false;\n\n\thigher_page_pfn = buddy_pfn & pfn;\n\thigher_page = page + (higher_page_pfn - pfn);\n\n\treturn find_buddy_page_pfn(higher_page, higher_page_pfn, order + 1,\n\t\t\tNULL) != NULL;\n}\n\n \n\nstatic inline void __free_one_page(struct page *page,\n\t\tunsigned long pfn,\n\t\tstruct zone *zone, unsigned int order,\n\t\tint migratetype, fpi_t fpi_flags)\n{\n\tstruct capture_control *capc = task_capc(zone);\n\tunsigned long buddy_pfn = 0;\n\tunsigned long combined_pfn;\n\tstruct page *buddy;\n\tbool to_tail;\n\n\tVM_BUG_ON(!zone_is_initialized(zone));\n\tVM_BUG_ON_PAGE(page->flags & PAGE_FLAGS_CHECK_AT_PREP, page);\n\n\tVM_BUG_ON(migratetype == -1);\n\tif (likely(!is_migrate_isolate(migratetype)))\n\t\t__mod_zone_freepage_state(zone, 1 << order, migratetype);\n\n\tVM_BUG_ON_PAGE(pfn & ((1 << order) - 1), page);\n\tVM_BUG_ON_PAGE(bad_range(zone, page), page);\n\n\twhile (order < MAX_ORDER) {\n\t\tif (compaction_capture(capc, page, order, migratetype)) {\n\t\t\t__mod_zone_freepage_state(zone, -(1 << order),\n\t\t\t\t\t\t\t\tmigratetype);\n\t\t\treturn;\n\t\t}\n\n\t\tbuddy = find_buddy_page_pfn(page, pfn, order, &buddy_pfn);\n\t\tif (!buddy)\n\t\t\tgoto done_merging;\n\n\t\tif (unlikely(order >= pageblock_order)) {\n\t\t\t \n\t\t\tint buddy_mt = get_pfnblock_migratetype(buddy, buddy_pfn);\n\n\t\t\tif (migratetype != buddy_mt\n\t\t\t\t\t&& (!migratetype_is_mergeable(migratetype) ||\n\t\t\t\t\t\t!migratetype_is_mergeable(buddy_mt)))\n\t\t\t\tgoto done_merging;\n\t\t}\n\n\t\t \n\t\tif (page_is_guard(buddy))\n\t\t\tclear_page_guard(zone, buddy, order, migratetype);\n\t\telse\n\t\t\tdel_page_from_free_list(buddy, zone, order);\n\t\tcombined_pfn = buddy_pfn & pfn;\n\t\tpage = page + (combined_pfn - pfn);\n\t\tpfn = combined_pfn;\n\t\torder++;\n\t}\n\ndone_merging:\n\tset_buddy_order(page, order);\n\n\tif (fpi_flags & FPI_TO_TAIL)\n\t\tto_tail = true;\n\telse if (is_shuffle_order(order))\n\t\tto_tail = shuffle_pick_tail();\n\telse\n\t\tto_tail = buddy_merge_likely(pfn, buddy_pfn, page, order);\n\n\tif (to_tail)\n\t\tadd_to_free_list_tail(page, zone, order, migratetype);\n\telse\n\t\tadd_to_free_list(page, zone, order, migratetype);\n\n\t \n\tif (!(fpi_flags & FPI_SKIP_REPORT_NOTIFY))\n\t\tpage_reporting_notify_free(order);\n}\n\n \nint split_free_page(struct page *free_page,\n\t\t\tunsigned int order, unsigned long split_pfn_offset)\n{\n\tstruct zone *zone = page_zone(free_page);\n\tunsigned long free_page_pfn = page_to_pfn(free_page);\n\tunsigned long pfn;\n\tunsigned long flags;\n\tint free_page_order;\n\tint mt;\n\tint ret = 0;\n\n\tif (split_pfn_offset == 0)\n\t\treturn ret;\n\n\tspin_lock_irqsave(&zone->lock, flags);\n\n\tif (!PageBuddy(free_page) || buddy_order(free_page) != order) {\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\tmt = get_pfnblock_migratetype(free_page, free_page_pfn);\n\tif (likely(!is_migrate_isolate(mt)))\n\t\t__mod_zone_freepage_state(zone, -(1UL << order), mt);\n\n\tdel_page_from_free_list(free_page, zone, order);\n\tfor (pfn = free_page_pfn;\n\t     pfn < free_page_pfn + (1UL << order);) {\n\t\tint mt = get_pfnblock_migratetype(pfn_to_page(pfn), pfn);\n\n\t\tfree_page_order = min_t(unsigned int,\n\t\t\t\t\tpfn ? __ffs(pfn) : order,\n\t\t\t\t\t__fls(split_pfn_offset));\n\t\t__free_one_page(pfn_to_page(pfn), pfn, zone, free_page_order,\n\t\t\t\tmt, FPI_NONE);\n\t\tpfn += 1UL << free_page_order;\n\t\tsplit_pfn_offset -= (1UL << free_page_order);\n\t\t \n\t\tif (split_pfn_offset == 0)\n\t\t\tsplit_pfn_offset = (1UL << order) - (pfn - free_page_pfn);\n\t}\nout:\n\tspin_unlock_irqrestore(&zone->lock, flags);\n\treturn ret;\n}\n \nstatic inline bool page_expected_state(struct page *page,\n\t\t\t\t\tunsigned long check_flags)\n{\n\tif (unlikely(atomic_read(&page->_mapcount) != -1))\n\t\treturn false;\n\n\tif (unlikely((unsigned long)page->mapping |\n\t\t\tpage_ref_count(page) |\n#ifdef CONFIG_MEMCG\n\t\t\tpage->memcg_data |\n#endif\n\t\t\t(page->flags & check_flags)))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic const char *page_bad_reason(struct page *page, unsigned long flags)\n{\n\tconst char *bad_reason = NULL;\n\n\tif (unlikely(atomic_read(&page->_mapcount) != -1))\n\t\tbad_reason = \"nonzero mapcount\";\n\tif (unlikely(page->mapping != NULL))\n\t\tbad_reason = \"non-NULL mapping\";\n\tif (unlikely(page_ref_count(page) != 0))\n\t\tbad_reason = \"nonzero _refcount\";\n\tif (unlikely(page->flags & flags)) {\n\t\tif (flags == PAGE_FLAGS_CHECK_AT_PREP)\n\t\t\tbad_reason = \"PAGE_FLAGS_CHECK_AT_PREP flag(s) set\";\n\t\telse\n\t\t\tbad_reason = \"PAGE_FLAGS_CHECK_AT_FREE flag(s) set\";\n\t}\n#ifdef CONFIG_MEMCG\n\tif (unlikely(page->memcg_data))\n\t\tbad_reason = \"page still charged to cgroup\";\n#endif\n\treturn bad_reason;\n}\n\nstatic void free_page_is_bad_report(struct page *page)\n{\n\tbad_page(page,\n\t\t page_bad_reason(page, PAGE_FLAGS_CHECK_AT_FREE));\n}\n\nstatic inline bool free_page_is_bad(struct page *page)\n{\n\tif (likely(page_expected_state(page, PAGE_FLAGS_CHECK_AT_FREE)))\n\t\treturn false;\n\n\t \n\tfree_page_is_bad_report(page);\n\treturn true;\n}\n\nstatic inline bool is_check_pages_enabled(void)\n{\n\treturn static_branch_unlikely(&check_pages_enabled);\n}\n\nstatic int free_tail_page_prepare(struct page *head_page, struct page *page)\n{\n\tstruct folio *folio = (struct folio *)head_page;\n\tint ret = 1;\n\n\t \n\tBUILD_BUG_ON((unsigned long)LIST_POISON1 & 1);\n\n\tif (!is_check_pages_enabled()) {\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\tswitch (page - head_page) {\n\tcase 1:\n\t\t \n\t\tif (unlikely(folio_entire_mapcount(folio))) {\n\t\t\tbad_page(page, \"nonzero entire_mapcount\");\n\t\t\tgoto out;\n\t\t}\n\t\tif (unlikely(atomic_read(&folio->_nr_pages_mapped))) {\n\t\t\tbad_page(page, \"nonzero nr_pages_mapped\");\n\t\t\tgoto out;\n\t\t}\n\t\tif (unlikely(atomic_read(&folio->_pincount))) {\n\t\t\tbad_page(page, \"nonzero pincount\");\n\t\t\tgoto out;\n\t\t}\n\t\tbreak;\n\tcase 2:\n\t\t \n\t\tbreak;\n\tdefault:\n\t\tif (page->mapping != TAIL_MAPPING) {\n\t\t\tbad_page(page, \"corrupted mapping in tail page\");\n\t\t\tgoto out;\n\t\t}\n\t\tbreak;\n\t}\n\tif (unlikely(!PageTail(page))) {\n\t\tbad_page(page, \"PageTail not set\");\n\t\tgoto out;\n\t}\n\tif (unlikely(compound_head(page) != head_page)) {\n\t\tbad_page(page, \"compound_head not consistent\");\n\t\tgoto out;\n\t}\n\tret = 0;\nout:\n\tpage->mapping = NULL;\n\tclear_compound_head(page);\n\treturn ret;\n}\n\n \nstatic inline bool should_skip_kasan_poison(struct page *page, fpi_t fpi_flags)\n{\n\tif (IS_ENABLED(CONFIG_KASAN_GENERIC))\n\t\treturn deferred_pages_enabled();\n\n\treturn page_kasan_tag(page) == 0xff;\n}\n\nstatic void kernel_init_pages(struct page *page, int numpages)\n{\n\tint i;\n\n\t \n\tkasan_disable_current();\n\tfor (i = 0; i < numpages; i++)\n\t\tclear_highpage_kasan_tagged(page + i);\n\tkasan_enable_current();\n}\n\nstatic __always_inline bool free_pages_prepare(struct page *page,\n\t\t\tunsigned int order, fpi_t fpi_flags)\n{\n\tint bad = 0;\n\tbool skip_kasan_poison = should_skip_kasan_poison(page, fpi_flags);\n\tbool init = want_init_on_free();\n\n\tVM_BUG_ON_PAGE(PageTail(page), page);\n\n\ttrace_mm_page_free(page, order);\n\tkmsan_free_page(page, order);\n\n\tif (unlikely(PageHWPoison(page)) && !order) {\n\t\t \n\t\tif (memcg_kmem_online() && PageMemcgKmem(page))\n\t\t\t__memcg_kmem_uncharge_page(page, order);\n\t\treset_page_owner(page, order);\n\t\tpage_table_check_free(page, order);\n\t\treturn false;\n\t}\n\n\t \n\tif (unlikely(order)) {\n\t\tbool compound = PageCompound(page);\n\t\tint i;\n\n\t\tVM_BUG_ON_PAGE(compound && compound_order(page) != order, page);\n\n\t\tif (compound)\n\t\t\tpage[1].flags &= ~PAGE_FLAGS_SECOND;\n\t\tfor (i = 1; i < (1 << order); i++) {\n\t\t\tif (compound)\n\t\t\t\tbad += free_tail_page_prepare(page, page + i);\n\t\t\tif (is_check_pages_enabled()) {\n\t\t\t\tif (free_page_is_bad(page + i)) {\n\t\t\t\t\tbad++;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t}\n\t\t\t(page + i)->flags &= ~PAGE_FLAGS_CHECK_AT_PREP;\n\t\t}\n\t}\n\tif (PageMappingFlags(page))\n\t\tpage->mapping = NULL;\n\tif (memcg_kmem_online() && PageMemcgKmem(page))\n\t\t__memcg_kmem_uncharge_page(page, order);\n\tif (is_check_pages_enabled()) {\n\t\tif (free_page_is_bad(page))\n\t\t\tbad++;\n\t\tif (bad)\n\t\t\treturn false;\n\t}\n\n\tpage_cpupid_reset_last(page);\n\tpage->flags &= ~PAGE_FLAGS_CHECK_AT_PREP;\n\treset_page_owner(page, order);\n\tpage_table_check_free(page, order);\n\n\tif (!PageHighMem(page)) {\n\t\tdebug_check_no_locks_freed(page_address(page),\n\t\t\t\t\t   PAGE_SIZE << order);\n\t\tdebug_check_no_obj_freed(page_address(page),\n\t\t\t\t\t   PAGE_SIZE << order);\n\t}\n\n\tkernel_poison_pages(page, 1 << order);\n\n\t \n\tif (!skip_kasan_poison) {\n\t\tkasan_poison_pages(page, order, init);\n\n\t\t \n\t\tif (kasan_has_integrated_init())\n\t\t\tinit = false;\n\t}\n\tif (init)\n\t\tkernel_init_pages(page, 1 << order);\n\n\t \n\tarch_free_page(page, order);\n\n\tdebug_pagealloc_unmap_pages(page, 1 << order);\n\n\treturn true;\n}\n\n \nstatic void free_pcppages_bulk(struct zone *zone, int count,\n\t\t\t\t\tstruct per_cpu_pages *pcp,\n\t\t\t\t\tint pindex)\n{\n\tunsigned long flags;\n\tunsigned int order;\n\tbool isolated_pageblocks;\n\tstruct page *page;\n\n\t \n\tcount = min(pcp->count, count);\n\n\t \n\tpindex = pindex - 1;\n\n\tspin_lock_irqsave(&zone->lock, flags);\n\tisolated_pageblocks = has_isolate_pageblock(zone);\n\n\twhile (count > 0) {\n\t\tstruct list_head *list;\n\t\tint nr_pages;\n\n\t\t \n\t\tdo {\n\t\t\tif (++pindex > NR_PCP_LISTS - 1)\n\t\t\t\tpindex = 0;\n\t\t\tlist = &pcp->lists[pindex];\n\t\t} while (list_empty(list));\n\n\t\torder = pindex_to_order(pindex);\n\t\tnr_pages = 1 << order;\n\t\tdo {\n\t\t\tint mt;\n\n\t\t\tpage = list_last_entry(list, struct page, pcp_list);\n\t\t\tmt = get_pcppage_migratetype(page);\n\n\t\t\t \n\t\t\tlist_del(&page->pcp_list);\n\t\t\tcount -= nr_pages;\n\t\t\tpcp->count -= nr_pages;\n\n\t\t\t \n\t\t\tVM_BUG_ON_PAGE(is_migrate_isolate(mt), page);\n\t\t\t \n\t\t\tif (unlikely(isolated_pageblocks))\n\t\t\t\tmt = get_pageblock_migratetype(page);\n\n\t\t\t__free_one_page(page, page_to_pfn(page), zone, order, mt, FPI_NONE);\n\t\t\ttrace_mm_page_pcpu_drain(page, order, mt);\n\t\t} while (count > 0 && !list_empty(list));\n\t}\n\n\tspin_unlock_irqrestore(&zone->lock, flags);\n}\n\nstatic void free_one_page(struct zone *zone,\n\t\t\t\tstruct page *page, unsigned long pfn,\n\t\t\t\tunsigned int order,\n\t\t\t\tint migratetype, fpi_t fpi_flags)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&zone->lock, flags);\n\tif (unlikely(has_isolate_pageblock(zone) ||\n\t\tis_migrate_isolate(migratetype))) {\n\t\tmigratetype = get_pfnblock_migratetype(page, pfn);\n\t}\n\t__free_one_page(page, pfn, zone, order, migratetype, fpi_flags);\n\tspin_unlock_irqrestore(&zone->lock, flags);\n}\n\nstatic void __free_pages_ok(struct page *page, unsigned int order,\n\t\t\t    fpi_t fpi_flags)\n{\n\tunsigned long flags;\n\tint migratetype;\n\tunsigned long pfn = page_to_pfn(page);\n\tstruct zone *zone = page_zone(page);\n\n\tif (!free_pages_prepare(page, order, fpi_flags))\n\t\treturn;\n\n\t \n\tmigratetype = get_pfnblock_migratetype(page, pfn);\n\n\tspin_lock_irqsave(&zone->lock, flags);\n\tif (unlikely(has_isolate_pageblock(zone) ||\n\t\tis_migrate_isolate(migratetype))) {\n\t\tmigratetype = get_pfnblock_migratetype(page, pfn);\n\t}\n\t__free_one_page(page, pfn, zone, order, migratetype, fpi_flags);\n\tspin_unlock_irqrestore(&zone->lock, flags);\n\n\t__count_vm_events(PGFREE, 1 << order);\n}\n\nvoid __free_pages_core(struct page *page, unsigned int order)\n{\n\tunsigned int nr_pages = 1 << order;\n\tstruct page *p = page;\n\tunsigned int loop;\n\n\t \n\tprefetchw(p);\n\tfor (loop = 0; loop < (nr_pages - 1); loop++, p++) {\n\t\tprefetchw(p + 1);\n\t\t__ClearPageReserved(p);\n\t\tset_page_count(p, 0);\n\t}\n\t__ClearPageReserved(p);\n\tset_page_count(p, 0);\n\n\tatomic_long_add(nr_pages, &page_zone(page)->managed_pages);\n\n\tif (page_contains_unaccepted(page, order)) {\n\t\tif (order == MAX_ORDER && __free_unaccepted(page))\n\t\t\treturn;\n\n\t\taccept_page(page, order);\n\t}\n\n\t \n\t__free_pages_ok(page, order, FPI_TO_TAIL);\n}\n\n \nstruct page *__pageblock_pfn_to_page(unsigned long start_pfn,\n\t\t\t\t     unsigned long end_pfn, struct zone *zone)\n{\n\tstruct page *start_page;\n\tstruct page *end_page;\n\n\t \n\tend_pfn--;\n\n\tif (!pfn_valid(end_pfn))\n\t\treturn NULL;\n\n\tstart_page = pfn_to_online_page(start_pfn);\n\tif (!start_page)\n\t\treturn NULL;\n\n\tif (page_zone(start_page) != zone)\n\t\treturn NULL;\n\n\tend_page = pfn_to_page(end_pfn);\n\n\t \n\tif (page_zone_id(start_page) != page_zone_id(end_page))\n\t\treturn NULL;\n\n\treturn start_page;\n}\n\n \nstatic inline void expand(struct zone *zone, struct page *page,\n\tint low, int high, int migratetype)\n{\n\tunsigned long size = 1 << high;\n\n\twhile (high > low) {\n\t\thigh--;\n\t\tsize >>= 1;\n\t\tVM_BUG_ON_PAGE(bad_range(zone, &page[size]), &page[size]);\n\n\t\t \n\t\tif (set_page_guard(zone, &page[size], high, migratetype))\n\t\t\tcontinue;\n\n\t\tadd_to_free_list(&page[size], zone, high, migratetype);\n\t\tset_buddy_order(&page[size], high);\n\t}\n}\n\nstatic void check_new_page_bad(struct page *page)\n{\n\tif (unlikely(page->flags & __PG_HWPOISON)) {\n\t\t \n\t\tpage_mapcount_reset(page);  \n\t\treturn;\n\t}\n\n\tbad_page(page,\n\t\t page_bad_reason(page, PAGE_FLAGS_CHECK_AT_PREP));\n}\n\n \nstatic int check_new_page(struct page *page)\n{\n\tif (likely(page_expected_state(page,\n\t\t\t\tPAGE_FLAGS_CHECK_AT_PREP|__PG_HWPOISON)))\n\t\treturn 0;\n\n\tcheck_new_page_bad(page);\n\treturn 1;\n}\n\nstatic inline bool check_new_pages(struct page *page, unsigned int order)\n{\n\tif (is_check_pages_enabled()) {\n\t\tfor (int i = 0; i < (1 << order); i++) {\n\t\t\tstruct page *p = page + i;\n\n\t\t\tif (check_new_page(p))\n\t\t\t\treturn true;\n\t\t}\n\t}\n\n\treturn false;\n}\n\nstatic inline bool should_skip_kasan_unpoison(gfp_t flags)\n{\n\t \n\tif (IS_ENABLED(CONFIG_KASAN_GENERIC) ||\n\t    IS_ENABLED(CONFIG_KASAN_SW_TAGS))\n\t\treturn false;\n\n\t \n\tif (!kasan_hw_tags_enabled())\n\t\treturn true;\n\n\t \n\treturn flags & __GFP_SKIP_KASAN;\n}\n\nstatic inline bool should_skip_init(gfp_t flags)\n{\n\t \n\tif (!kasan_hw_tags_enabled())\n\t\treturn false;\n\n\t \n\treturn (flags & __GFP_SKIP_ZERO);\n}\n\ninline void post_alloc_hook(struct page *page, unsigned int order,\n\t\t\t\tgfp_t gfp_flags)\n{\n\tbool init = !want_init_on_free() && want_init_on_alloc(gfp_flags) &&\n\t\t\t!should_skip_init(gfp_flags);\n\tbool zero_tags = init && (gfp_flags & __GFP_ZEROTAGS);\n\tint i;\n\n\tset_page_private(page, 0);\n\tset_page_refcounted(page);\n\n\tarch_alloc_page(page, order);\n\tdebug_pagealloc_map_pages(page, 1 << order);\n\n\t \n\tkernel_unpoison_pages(page, 1 << order);\n\n\t \n\n\t \n\tif (zero_tags) {\n\t\t \n\t\tfor (i = 0; i != 1 << order; ++i)\n\t\t\ttag_clear_highpage(page + i);\n\n\t\t \n\t\tinit = false;\n\t}\n\tif (!should_skip_kasan_unpoison(gfp_flags) &&\n\t    kasan_unpoison_pages(page, order, init)) {\n\t\t \n\t\tif (kasan_has_integrated_init())\n\t\t\tinit = false;\n\t} else {\n\t\t \n\t\tfor (i = 0; i != 1 << order; ++i)\n\t\t\tpage_kasan_tag_reset(page + i);\n\t}\n\t \n\tif (init)\n\t\tkernel_init_pages(page, 1 << order);\n\n\tset_page_owner(page, order, gfp_flags);\n\tpage_table_check_alloc(page, order);\n}\n\nstatic void prep_new_page(struct page *page, unsigned int order, gfp_t gfp_flags,\n\t\t\t\t\t\t\tunsigned int alloc_flags)\n{\n\tpost_alloc_hook(page, order, gfp_flags);\n\n\tif (order && (gfp_flags & __GFP_COMP))\n\t\tprep_compound_page(page, order);\n\n\t \n\tif (alloc_flags & ALLOC_NO_WATERMARKS)\n\t\tset_page_pfmemalloc(page);\n\telse\n\t\tclear_page_pfmemalloc(page);\n}\n\n \nstatic __always_inline\nstruct page *__rmqueue_smallest(struct zone *zone, unsigned int order,\n\t\t\t\t\t\tint migratetype)\n{\n\tunsigned int current_order;\n\tstruct free_area *area;\n\tstruct page *page;\n\n\t \n\tfor (current_order = order; current_order <= MAX_ORDER; ++current_order) {\n\t\tarea = &(zone->free_area[current_order]);\n\t\tpage = get_page_from_free_area(area, migratetype);\n\t\tif (!page)\n\t\t\tcontinue;\n\t\tdel_page_from_free_list(page, zone, current_order);\n\t\texpand(zone, page, order, current_order, migratetype);\n\t\tset_pcppage_migratetype(page, migratetype);\n\t\ttrace_mm_page_alloc_zone_locked(page, order, migratetype,\n\t\t\t\tpcp_allowed_order(order) &&\n\t\t\t\tmigratetype < MIGRATE_PCPTYPES);\n\t\treturn page;\n\t}\n\n\treturn NULL;\n}\n\n\n \nstatic int fallbacks[MIGRATE_TYPES][MIGRATE_PCPTYPES - 1] = {\n\t[MIGRATE_UNMOVABLE]   = { MIGRATE_RECLAIMABLE, MIGRATE_MOVABLE   },\n\t[MIGRATE_MOVABLE]     = { MIGRATE_RECLAIMABLE, MIGRATE_UNMOVABLE },\n\t[MIGRATE_RECLAIMABLE] = { MIGRATE_UNMOVABLE,   MIGRATE_MOVABLE   },\n};\n\n#ifdef CONFIG_CMA\nstatic __always_inline struct page *__rmqueue_cma_fallback(struct zone *zone,\n\t\t\t\t\tunsigned int order)\n{\n\treturn __rmqueue_smallest(zone, order, MIGRATE_CMA);\n}\n#else\nstatic inline struct page *__rmqueue_cma_fallback(struct zone *zone,\n\t\t\t\t\tunsigned int order) { return NULL; }\n#endif\n\n \nstatic int move_freepages(struct zone *zone,\n\t\t\t  unsigned long start_pfn, unsigned long end_pfn,\n\t\t\t  int migratetype, int *num_movable)\n{\n\tstruct page *page;\n\tunsigned long pfn;\n\tunsigned int order;\n\tint pages_moved = 0;\n\n\tfor (pfn = start_pfn; pfn <= end_pfn;) {\n\t\tpage = pfn_to_page(pfn);\n\t\tif (!PageBuddy(page)) {\n\t\t\t \n\t\t\tif (num_movable &&\n\t\t\t\t\t(PageLRU(page) || __PageMovable(page)))\n\t\t\t\t(*num_movable)++;\n\t\t\tpfn++;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tVM_BUG_ON_PAGE(page_to_nid(page) != zone_to_nid(zone), page);\n\t\tVM_BUG_ON_PAGE(page_zone(page) != zone, page);\n\n\t\torder = buddy_order(page);\n\t\tmove_to_free_list(page, zone, order, migratetype);\n\t\tpfn += 1 << order;\n\t\tpages_moved += 1 << order;\n\t}\n\n\treturn pages_moved;\n}\n\nint move_freepages_block(struct zone *zone, struct page *page,\n\t\t\t\tint migratetype, int *num_movable)\n{\n\tunsigned long start_pfn, end_pfn, pfn;\n\n\tif (num_movable)\n\t\t*num_movable = 0;\n\n\tpfn = page_to_pfn(page);\n\tstart_pfn = pageblock_start_pfn(pfn);\n\tend_pfn = pageblock_end_pfn(pfn) - 1;\n\n\t \n\tif (!zone_spans_pfn(zone, start_pfn))\n\t\tstart_pfn = pfn;\n\tif (!zone_spans_pfn(zone, end_pfn))\n\t\treturn 0;\n\n\treturn move_freepages(zone, start_pfn, end_pfn, migratetype,\n\t\t\t\t\t\t\t\tnum_movable);\n}\n\nstatic void change_pageblock_range(struct page *pageblock_page,\n\t\t\t\t\tint start_order, int migratetype)\n{\n\tint nr_pageblocks = 1 << (start_order - pageblock_order);\n\n\twhile (nr_pageblocks--) {\n\t\tset_pageblock_migratetype(pageblock_page, migratetype);\n\t\tpageblock_page += pageblock_nr_pages;\n\t}\n}\n\n \nstatic bool can_steal_fallback(unsigned int order, int start_mt)\n{\n\t \n\tif (order >= pageblock_order)\n\t\treturn true;\n\n\tif (order >= pageblock_order / 2 ||\n\t\tstart_mt == MIGRATE_RECLAIMABLE ||\n\t\tstart_mt == MIGRATE_UNMOVABLE ||\n\t\tpage_group_by_mobility_disabled)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic inline bool boost_watermark(struct zone *zone)\n{\n\tunsigned long max_boost;\n\n\tif (!watermark_boost_factor)\n\t\treturn false;\n\t \n\tif ((pageblock_nr_pages * 4) > zone_managed_pages(zone))\n\t\treturn false;\n\n\tmax_boost = mult_frac(zone->_watermark[WMARK_HIGH],\n\t\t\twatermark_boost_factor, 10000);\n\n\t \n\tif (!max_boost)\n\t\treturn false;\n\n\tmax_boost = max(pageblock_nr_pages, max_boost);\n\n\tzone->watermark_boost = min(zone->watermark_boost + pageblock_nr_pages,\n\t\tmax_boost);\n\n\treturn true;\n}\n\n \nstatic void steal_suitable_fallback(struct zone *zone, struct page *page,\n\t\tunsigned int alloc_flags, int start_type, bool whole_block)\n{\n\tunsigned int current_order = buddy_order(page);\n\tint free_pages, movable_pages, alike_pages;\n\tint old_block_type;\n\n\told_block_type = get_pageblock_migratetype(page);\n\n\t \n\tif (is_migrate_highatomic(old_block_type))\n\t\tgoto single_page;\n\n\t \n\tif (current_order >= pageblock_order) {\n\t\tchange_pageblock_range(page, current_order, start_type);\n\t\tgoto single_page;\n\t}\n\n\t \n\tif (boost_watermark(zone) && (alloc_flags & ALLOC_KSWAPD))\n\t\tset_bit(ZONE_BOOSTED_WATERMARK, &zone->flags);\n\n\t \n\tif (!whole_block)\n\t\tgoto single_page;\n\n\tfree_pages = move_freepages_block(zone, page, start_type,\n\t\t\t\t\t\t&movable_pages);\n\t \n\tif (!free_pages)\n\t\tgoto single_page;\n\n\t \n\tif (start_type == MIGRATE_MOVABLE) {\n\t\talike_pages = movable_pages;\n\t} else {\n\t\t \n\t\tif (old_block_type == MIGRATE_MOVABLE)\n\t\t\talike_pages = pageblock_nr_pages\n\t\t\t\t\t\t- (free_pages + movable_pages);\n\t\telse\n\t\t\talike_pages = 0;\n\t}\n\t \n\tif (free_pages + alike_pages >= (1 << (pageblock_order-1)) ||\n\t\t\tpage_group_by_mobility_disabled)\n\t\tset_pageblock_migratetype(page, start_type);\n\n\treturn;\n\nsingle_page:\n\tmove_to_free_list(page, zone, current_order, start_type);\n}\n\n \nint find_suitable_fallback(struct free_area *area, unsigned int order,\n\t\t\tint migratetype, bool only_stealable, bool *can_steal)\n{\n\tint i;\n\tint fallback_mt;\n\n\tif (area->nr_free == 0)\n\t\treturn -1;\n\n\t*can_steal = false;\n\tfor (i = 0; i < MIGRATE_PCPTYPES - 1 ; i++) {\n\t\tfallback_mt = fallbacks[migratetype][i];\n\t\tif (free_area_empty(area, fallback_mt))\n\t\t\tcontinue;\n\n\t\tif (can_steal_fallback(order, migratetype))\n\t\t\t*can_steal = true;\n\n\t\tif (!only_stealable)\n\t\t\treturn fallback_mt;\n\n\t\tif (*can_steal)\n\t\t\treturn fallback_mt;\n\t}\n\n\treturn -1;\n}\n\n \nstatic void reserve_highatomic_pageblock(struct page *page, struct zone *zone)\n{\n\tint mt;\n\tunsigned long max_managed, flags;\n\n\t \n\tmax_managed = (zone_managed_pages(zone) / 100) + pageblock_nr_pages;\n\tif (zone->nr_reserved_highatomic >= max_managed)\n\t\treturn;\n\n\tspin_lock_irqsave(&zone->lock, flags);\n\n\t \n\tif (zone->nr_reserved_highatomic >= max_managed)\n\t\tgoto out_unlock;\n\n\t \n\tmt = get_pageblock_migratetype(page);\n\t \n\tif (migratetype_is_mergeable(mt)) {\n\t\tzone->nr_reserved_highatomic += pageblock_nr_pages;\n\t\tset_pageblock_migratetype(page, MIGRATE_HIGHATOMIC);\n\t\tmove_freepages_block(zone, page, MIGRATE_HIGHATOMIC, NULL);\n\t}\n\nout_unlock:\n\tspin_unlock_irqrestore(&zone->lock, flags);\n}\n\n \nstatic bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n\t\t\t\t\t\tbool force)\n{\n\tstruct zonelist *zonelist = ac->zonelist;\n\tunsigned long flags;\n\tstruct zoneref *z;\n\tstruct zone *zone;\n\tstruct page *page;\n\tint order;\n\tbool ret;\n\n\tfor_each_zone_zonelist_nodemask(zone, z, zonelist, ac->highest_zoneidx,\n\t\t\t\t\t\t\t\tac->nodemask) {\n\t\t \n\t\tif (!force && zone->nr_reserved_highatomic <=\n\t\t\t\t\tpageblock_nr_pages)\n\t\t\tcontinue;\n\n\t\tspin_lock_irqsave(&zone->lock, flags);\n\t\tfor (order = 0; order <= MAX_ORDER; order++) {\n\t\t\tstruct free_area *area = &(zone->free_area[order]);\n\n\t\t\tpage = get_page_from_free_area(area, MIGRATE_HIGHATOMIC);\n\t\t\tif (!page)\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tif (is_migrate_highatomic_page(page)) {\n\t\t\t\t \n\t\t\t\tzone->nr_reserved_highatomic -= min(\n\t\t\t\t\t\tpageblock_nr_pages,\n\t\t\t\t\t\tzone->nr_reserved_highatomic);\n\t\t\t}\n\n\t\t\t \n\t\t\tset_pageblock_migratetype(page, ac->migratetype);\n\t\t\tret = move_freepages_block(zone, page, ac->migratetype,\n\t\t\t\t\t\t\t\t\tNULL);\n\t\t\tif (ret) {\n\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\t\tspin_unlock_irqrestore(&zone->lock, flags);\n\t}\n\n\treturn false;\n}\n\n \nstatic __always_inline bool\n__rmqueue_fallback(struct zone *zone, int order, int start_migratetype,\n\t\t\t\t\t\tunsigned int alloc_flags)\n{\n\tstruct free_area *area;\n\tint current_order;\n\tint min_order = order;\n\tstruct page *page;\n\tint fallback_mt;\n\tbool can_steal;\n\n\t \n\tif (order < pageblock_order && alloc_flags & ALLOC_NOFRAGMENT)\n\t\tmin_order = pageblock_order;\n\n\t \n\tfor (current_order = MAX_ORDER; current_order >= min_order;\n\t\t\t\t--current_order) {\n\t\tarea = &(zone->free_area[current_order]);\n\t\tfallback_mt = find_suitable_fallback(area, current_order,\n\t\t\t\tstart_migratetype, false, &can_steal);\n\t\tif (fallback_mt == -1)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (!can_steal && start_migratetype == MIGRATE_MOVABLE\n\t\t\t\t\t&& current_order > order)\n\t\t\tgoto find_smallest;\n\n\t\tgoto do_steal;\n\t}\n\n\treturn false;\n\nfind_smallest:\n\tfor (current_order = order; current_order <= MAX_ORDER;\n\t\t\t\t\t\t\tcurrent_order++) {\n\t\tarea = &(zone->free_area[current_order]);\n\t\tfallback_mt = find_suitable_fallback(area, current_order,\n\t\t\t\tstart_migratetype, false, &can_steal);\n\t\tif (fallback_mt != -1)\n\t\t\tbreak;\n\t}\n\n\t \n\tVM_BUG_ON(current_order > MAX_ORDER);\n\ndo_steal:\n\tpage = get_page_from_free_area(area, fallback_mt);\n\n\tsteal_suitable_fallback(zone, page, alloc_flags, start_migratetype,\n\t\t\t\t\t\t\t\tcan_steal);\n\n\ttrace_mm_page_alloc_extfrag(page, order, current_order,\n\t\tstart_migratetype, fallback_mt);\n\n\treturn true;\n\n}\n\n \nstatic __always_inline struct page *\n__rmqueue(struct zone *zone, unsigned int order, int migratetype,\n\t\t\t\t\t\tunsigned int alloc_flags)\n{\n\tstruct page *page;\n\n\tif (IS_ENABLED(CONFIG_CMA)) {\n\t\t \n\t\tif (alloc_flags & ALLOC_CMA &&\n\t\t    zone_page_state(zone, NR_FREE_CMA_PAGES) >\n\t\t    zone_page_state(zone, NR_FREE_PAGES) / 2) {\n\t\t\tpage = __rmqueue_cma_fallback(zone, order);\n\t\t\tif (page)\n\t\t\t\treturn page;\n\t\t}\n\t}\nretry:\n\tpage = __rmqueue_smallest(zone, order, migratetype);\n\tif (unlikely(!page)) {\n\t\tif (alloc_flags & ALLOC_CMA)\n\t\t\tpage = __rmqueue_cma_fallback(zone, order);\n\n\t\tif (!page && __rmqueue_fallback(zone, order, migratetype,\n\t\t\t\t\t\t\t\talloc_flags))\n\t\t\tgoto retry;\n\t}\n\treturn page;\n}\n\n \nstatic int rmqueue_bulk(struct zone *zone, unsigned int order,\n\t\t\tunsigned long count, struct list_head *list,\n\t\t\tint migratetype, unsigned int alloc_flags)\n{\n\tunsigned long flags;\n\tint i;\n\n\tspin_lock_irqsave(&zone->lock, flags);\n\tfor (i = 0; i < count; ++i) {\n\t\tstruct page *page = __rmqueue(zone, order, migratetype,\n\t\t\t\t\t\t\t\talloc_flags);\n\t\tif (unlikely(page == NULL))\n\t\t\tbreak;\n\n\t\t \n\t\tlist_add_tail(&page->pcp_list, list);\n\t\tif (is_migrate_cma(get_pcppage_migratetype(page)))\n\t\t\t__mod_zone_page_state(zone, NR_FREE_CMA_PAGES,\n\t\t\t\t\t      -(1 << order));\n\t}\n\n\t__mod_zone_page_state(zone, NR_FREE_PAGES, -(i << order));\n\tspin_unlock_irqrestore(&zone->lock, flags);\n\n\treturn i;\n}\n\n#ifdef CONFIG_NUMA\n \nvoid drain_zone_pages(struct zone *zone, struct per_cpu_pages *pcp)\n{\n\tint to_drain, batch;\n\n\tbatch = READ_ONCE(pcp->batch);\n\tto_drain = min(pcp->count, batch);\n\tif (to_drain > 0) {\n\t\tspin_lock(&pcp->lock);\n\t\tfree_pcppages_bulk(zone, to_drain, pcp, 0);\n\t\tspin_unlock(&pcp->lock);\n\t}\n}\n#endif\n\n \nstatic void drain_pages_zone(unsigned int cpu, struct zone *zone)\n{\n\tstruct per_cpu_pages *pcp;\n\n\tpcp = per_cpu_ptr(zone->per_cpu_pageset, cpu);\n\tif (pcp->count) {\n\t\tspin_lock(&pcp->lock);\n\t\tfree_pcppages_bulk(zone, pcp->count, pcp, 0);\n\t\tspin_unlock(&pcp->lock);\n\t}\n}\n\n \nstatic void drain_pages(unsigned int cpu)\n{\n\tstruct zone *zone;\n\n\tfor_each_populated_zone(zone) {\n\t\tdrain_pages_zone(cpu, zone);\n\t}\n}\n\n \nvoid drain_local_pages(struct zone *zone)\n{\n\tint cpu = smp_processor_id();\n\n\tif (zone)\n\t\tdrain_pages_zone(cpu, zone);\n\telse\n\t\tdrain_pages(cpu);\n}\n\n \nstatic void __drain_all_pages(struct zone *zone, bool force_all_cpus)\n{\n\tint cpu;\n\n\t \n\tstatic cpumask_t cpus_with_pcps;\n\n\t \n\tif (unlikely(!mutex_trylock(&pcpu_drain_mutex))) {\n\t\tif (!zone)\n\t\t\treturn;\n\t\tmutex_lock(&pcpu_drain_mutex);\n\t}\n\n\t \n\tfor_each_online_cpu(cpu) {\n\t\tstruct per_cpu_pages *pcp;\n\t\tstruct zone *z;\n\t\tbool has_pcps = false;\n\n\t\tif (force_all_cpus) {\n\t\t\t \n\t\t\thas_pcps = true;\n\t\t} else if (zone) {\n\t\t\tpcp = per_cpu_ptr(zone->per_cpu_pageset, cpu);\n\t\t\tif (pcp->count)\n\t\t\t\thas_pcps = true;\n\t\t} else {\n\t\t\tfor_each_populated_zone(z) {\n\t\t\t\tpcp = per_cpu_ptr(z->per_cpu_pageset, cpu);\n\t\t\t\tif (pcp->count) {\n\t\t\t\t\thas_pcps = true;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif (has_pcps)\n\t\t\tcpumask_set_cpu(cpu, &cpus_with_pcps);\n\t\telse\n\t\t\tcpumask_clear_cpu(cpu, &cpus_with_pcps);\n\t}\n\n\tfor_each_cpu(cpu, &cpus_with_pcps) {\n\t\tif (zone)\n\t\t\tdrain_pages_zone(cpu, zone);\n\t\telse\n\t\t\tdrain_pages(cpu);\n\t}\n\n\tmutex_unlock(&pcpu_drain_mutex);\n}\n\n \nvoid drain_all_pages(struct zone *zone)\n{\n\t__drain_all_pages(zone, false);\n}\n\nstatic bool free_unref_page_prepare(struct page *page, unsigned long pfn,\n\t\t\t\t\t\t\tunsigned int order)\n{\n\tint migratetype;\n\n\tif (!free_pages_prepare(page, order, FPI_NONE))\n\t\treturn false;\n\n\tmigratetype = get_pfnblock_migratetype(page, pfn);\n\tset_pcppage_migratetype(page, migratetype);\n\treturn true;\n}\n\nstatic int nr_pcp_free(struct per_cpu_pages *pcp, int high, bool free_high)\n{\n\tint min_nr_free, max_nr_free;\n\tint batch = READ_ONCE(pcp->batch);\n\n\t \n\tif (unlikely(free_high))\n\t\treturn pcp->count;\n\n\t \n\tif (unlikely(high < batch))\n\t\treturn 1;\n\n\t \n\tmin_nr_free = batch;\n\tmax_nr_free = high - batch;\n\n\t \n\tbatch <<= pcp->free_factor;\n\tif (batch < max_nr_free)\n\t\tpcp->free_factor++;\n\tbatch = clamp(batch, min_nr_free, max_nr_free);\n\n\treturn batch;\n}\n\nstatic int nr_pcp_high(struct per_cpu_pages *pcp, struct zone *zone,\n\t\t       bool free_high)\n{\n\tint high = READ_ONCE(pcp->high);\n\n\tif (unlikely(!high || free_high))\n\t\treturn 0;\n\n\tif (!test_bit(ZONE_RECLAIM_ACTIVE, &zone->flags))\n\t\treturn high;\n\n\t \n\treturn min(READ_ONCE(pcp->batch) << 2, high);\n}\n\nstatic void free_unref_page_commit(struct zone *zone, struct per_cpu_pages *pcp,\n\t\t\t\t   struct page *page, int migratetype,\n\t\t\t\t   unsigned int order)\n{\n\tint high;\n\tint pindex;\n\tbool free_high;\n\n\t__count_vm_events(PGFREE, 1 << order);\n\tpindex = order_to_pindex(migratetype, order);\n\tlist_add(&page->pcp_list, &pcp->lists[pindex]);\n\tpcp->count += 1 << order;\n\n\t \n\tfree_high = (pcp->free_factor && order && order <= PAGE_ALLOC_COSTLY_ORDER);\n\n\thigh = nr_pcp_high(pcp, zone, free_high);\n\tif (pcp->count >= high) {\n\t\tfree_pcppages_bulk(zone, nr_pcp_free(pcp, high, free_high), pcp, pindex);\n\t}\n}\n\n \nvoid free_unref_page(struct page *page, unsigned int order)\n{\n\tunsigned long __maybe_unused UP_flags;\n\tstruct per_cpu_pages *pcp;\n\tstruct zone *zone;\n\tunsigned long pfn = page_to_pfn(page);\n\tint migratetype, pcpmigratetype;\n\n\tif (!free_unref_page_prepare(page, pfn, order))\n\t\treturn;\n\n\t \n\tmigratetype = pcpmigratetype = get_pcppage_migratetype(page);\n\tif (unlikely(migratetype >= MIGRATE_PCPTYPES)) {\n\t\tif (unlikely(is_migrate_isolate(migratetype))) {\n\t\t\tfree_one_page(page_zone(page), page, pfn, order, migratetype, FPI_NONE);\n\t\t\treturn;\n\t\t}\n\t\tpcpmigratetype = MIGRATE_MOVABLE;\n\t}\n\n\tzone = page_zone(page);\n\tpcp_trylock_prepare(UP_flags);\n\tpcp = pcp_spin_trylock(zone->per_cpu_pageset);\n\tif (pcp) {\n\t\tfree_unref_page_commit(zone, pcp, page, pcpmigratetype, order);\n\t\tpcp_spin_unlock(pcp);\n\t} else {\n\t\tfree_one_page(zone, page, pfn, order, migratetype, FPI_NONE);\n\t}\n\tpcp_trylock_finish(UP_flags);\n}\n\n \nvoid free_unref_page_list(struct list_head *list)\n{\n\tunsigned long __maybe_unused UP_flags;\n\tstruct page *page, *next;\n\tstruct per_cpu_pages *pcp = NULL;\n\tstruct zone *locked_zone = NULL;\n\tint batch_count = 0;\n\tint migratetype;\n\n\t \n\tlist_for_each_entry_safe(page, next, list, lru) {\n\t\tunsigned long pfn = page_to_pfn(page);\n\t\tif (!free_unref_page_prepare(page, pfn, 0)) {\n\t\t\tlist_del(&page->lru);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tmigratetype = get_pcppage_migratetype(page);\n\t\tif (unlikely(is_migrate_isolate(migratetype))) {\n\t\t\tlist_del(&page->lru);\n\t\t\tfree_one_page(page_zone(page), page, pfn, 0, migratetype, FPI_NONE);\n\t\t\tcontinue;\n\t\t}\n\t}\n\n\tlist_for_each_entry_safe(page, next, list, lru) {\n\t\tstruct zone *zone = page_zone(page);\n\n\t\tlist_del(&page->lru);\n\t\tmigratetype = get_pcppage_migratetype(page);\n\n\t\t \n\t\tif (zone != locked_zone || batch_count == SWAP_CLUSTER_MAX) {\n\t\t\tif (pcp) {\n\t\t\t\tpcp_spin_unlock(pcp);\n\t\t\t\tpcp_trylock_finish(UP_flags);\n\t\t\t}\n\n\t\t\tbatch_count = 0;\n\n\t\t\t \n\t\t\tpcp_trylock_prepare(UP_flags);\n\t\t\tpcp = pcp_spin_trylock(zone->per_cpu_pageset);\n\t\t\tif (unlikely(!pcp)) {\n\t\t\t\tpcp_trylock_finish(UP_flags);\n\t\t\t\tfree_one_page(zone, page, page_to_pfn(page),\n\t\t\t\t\t      0, migratetype, FPI_NONE);\n\t\t\t\tlocked_zone = NULL;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tlocked_zone = zone;\n\t\t}\n\n\t\t \n\t\tif (unlikely(migratetype >= MIGRATE_PCPTYPES))\n\t\t\tmigratetype = MIGRATE_MOVABLE;\n\n\t\ttrace_mm_page_free_batched(page);\n\t\tfree_unref_page_commit(zone, pcp, page, migratetype, 0);\n\t\tbatch_count++;\n\t}\n\n\tif (pcp) {\n\t\tpcp_spin_unlock(pcp);\n\t\tpcp_trylock_finish(UP_flags);\n\t}\n}\n\n \nvoid split_page(struct page *page, unsigned int order)\n{\n\tint i;\n\n\tVM_BUG_ON_PAGE(PageCompound(page), page);\n\tVM_BUG_ON_PAGE(!page_count(page), page);\n\n\tfor (i = 1; i < (1 << order); i++)\n\t\tset_page_refcounted(page + i);\n\tsplit_page_owner(page, 1 << order);\n\tsplit_page_memcg(page, 1 << order);\n}\nEXPORT_SYMBOL_GPL(split_page);\n\nint __isolate_free_page(struct page *page, unsigned int order)\n{\n\tstruct zone *zone = page_zone(page);\n\tint mt = get_pageblock_migratetype(page);\n\n\tif (!is_migrate_isolate(mt)) {\n\t\tunsigned long watermark;\n\t\t \n\t\twatermark = zone->_watermark[WMARK_MIN] + (1UL << order);\n\t\tif (!zone_watermark_ok(zone, 0, watermark, 0, ALLOC_CMA))\n\t\t\treturn 0;\n\n\t\t__mod_zone_freepage_state(zone, -(1UL << order), mt);\n\t}\n\n\tdel_page_from_free_list(page, zone, order);\n\n\t \n\tif (order >= pageblock_order - 1) {\n\t\tstruct page *endpage = page + (1 << order) - 1;\n\t\tfor (; page < endpage; page += pageblock_nr_pages) {\n\t\t\tint mt = get_pageblock_migratetype(page);\n\t\t\t \n\t\t\tif (migratetype_is_mergeable(mt))\n\t\t\t\tset_pageblock_migratetype(page,\n\t\t\t\t\t\t\t  MIGRATE_MOVABLE);\n\t\t}\n\t}\n\n\treturn 1UL << order;\n}\n\n \nvoid __putback_isolated_page(struct page *page, unsigned int order, int mt)\n{\n\tstruct zone *zone = page_zone(page);\n\n\t \n\tlockdep_assert_held(&zone->lock);\n\n\t \n\t__free_one_page(page, page_to_pfn(page), zone, order, mt,\n\t\t\tFPI_SKIP_REPORT_NOTIFY | FPI_TO_TAIL);\n}\n\n \nstatic inline void zone_statistics(struct zone *preferred_zone, struct zone *z,\n\t\t\t\t   long nr_account)\n{\n#ifdef CONFIG_NUMA\n\tenum numa_stat_item local_stat = NUMA_LOCAL;\n\n\t \n\tif (!static_branch_likely(&vm_numa_stat_key))\n\t\treturn;\n\n\tif (zone_to_nid(z) != numa_node_id())\n\t\tlocal_stat = NUMA_OTHER;\n\n\tif (zone_to_nid(z) == zone_to_nid(preferred_zone))\n\t\t__count_numa_events(z, NUMA_HIT, nr_account);\n\telse {\n\t\t__count_numa_events(z, NUMA_MISS, nr_account);\n\t\t__count_numa_events(preferred_zone, NUMA_FOREIGN, nr_account);\n\t}\n\t__count_numa_events(z, local_stat, nr_account);\n#endif\n}\n\nstatic __always_inline\nstruct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n\t\t\t   unsigned int order, unsigned int alloc_flags,\n\t\t\t   int migratetype)\n{\n\tstruct page *page;\n\tunsigned long flags;\n\n\tdo {\n\t\tpage = NULL;\n\t\tspin_lock_irqsave(&zone->lock, flags);\n\t\tif (alloc_flags & ALLOC_HIGHATOMIC)\n\t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n\t\tif (!page) {\n\t\t\tpage = __rmqueue(zone, order, migratetype, alloc_flags);\n\n\t\t\t \n\t\t\tif (!page && (alloc_flags & ALLOC_OOM))\n\t\t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n\n\t\t\tif (!page) {\n\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n\t\t\t\treturn NULL;\n\t\t\t}\n\t\t}\n\t\t__mod_zone_freepage_state(zone, -(1 << order),\n\t\t\t\t\t  get_pcppage_migratetype(page));\n\t\tspin_unlock_irqrestore(&zone->lock, flags);\n\t} while (check_new_pages(page, order));\n\n\t__count_zid_vm_events(PGALLOC, page_zonenum(page), 1 << order);\n\tzone_statistics(preferred_zone, zone, 1);\n\n\treturn page;\n}\n\n \nstatic inline\nstruct page *__rmqueue_pcplist(struct zone *zone, unsigned int order,\n\t\t\tint migratetype,\n\t\t\tunsigned int alloc_flags,\n\t\t\tstruct per_cpu_pages *pcp,\n\t\t\tstruct list_head *list)\n{\n\tstruct page *page;\n\n\tdo {\n\t\tif (list_empty(list)) {\n\t\t\tint batch = READ_ONCE(pcp->batch);\n\t\t\tint alloced;\n\n\t\t\t \n\t\t\tif (batch > 1)\n\t\t\t\tbatch = max(batch >> order, 2);\n\t\t\talloced = rmqueue_bulk(zone, order,\n\t\t\t\t\tbatch, list,\n\t\t\t\t\tmigratetype, alloc_flags);\n\n\t\t\tpcp->count += alloced << order;\n\t\t\tif (unlikely(list_empty(list)))\n\t\t\t\treturn NULL;\n\t\t}\n\n\t\tpage = list_first_entry(list, struct page, pcp_list);\n\t\tlist_del(&page->pcp_list);\n\t\tpcp->count -= 1 << order;\n\t} while (check_new_pages(page, order));\n\n\treturn page;\n}\n\n \nstatic struct page *rmqueue_pcplist(struct zone *preferred_zone,\n\t\t\tstruct zone *zone, unsigned int order,\n\t\t\tint migratetype, unsigned int alloc_flags)\n{\n\tstruct per_cpu_pages *pcp;\n\tstruct list_head *list;\n\tstruct page *page;\n\tunsigned long __maybe_unused UP_flags;\n\n\t \n\tpcp_trylock_prepare(UP_flags);\n\tpcp = pcp_spin_trylock(zone->per_cpu_pageset);\n\tif (!pcp) {\n\t\tpcp_trylock_finish(UP_flags);\n\t\treturn NULL;\n\t}\n\n\t \n\tpcp->free_factor >>= 1;\n\tlist = &pcp->lists[order_to_pindex(migratetype, order)];\n\tpage = __rmqueue_pcplist(zone, order, migratetype, alloc_flags, pcp, list);\n\tpcp_spin_unlock(pcp);\n\tpcp_trylock_finish(UP_flags);\n\tif (page) {\n\t\t__count_zid_vm_events(PGALLOC, page_zonenum(page), 1 << order);\n\t\tzone_statistics(preferred_zone, zone, 1);\n\t}\n\treturn page;\n}\n\n \n\n \n__no_sanitize_memory\nstatic inline\nstruct page *rmqueue(struct zone *preferred_zone,\n\t\t\tstruct zone *zone, unsigned int order,\n\t\t\tgfp_t gfp_flags, unsigned int alloc_flags,\n\t\t\tint migratetype)\n{\n\tstruct page *page;\n\n\t \n\tWARN_ON_ONCE((gfp_flags & __GFP_NOFAIL) && (order > 1));\n\n\tif (likely(pcp_allowed_order(order))) {\n\t\tpage = rmqueue_pcplist(preferred_zone, zone, order,\n\t\t\t\t       migratetype, alloc_flags);\n\t\tif (likely(page))\n\t\t\tgoto out;\n\t}\n\n\tpage = rmqueue_buddy(preferred_zone, zone, order, alloc_flags,\n\t\t\t\t\t\t\tmigratetype);\n\nout:\n\t \n\tif ((alloc_flags & ALLOC_KSWAPD) &&\n\t    unlikely(test_bit(ZONE_BOOSTED_WATERMARK, &zone->flags))) {\n\t\tclear_bit(ZONE_BOOSTED_WATERMARK, &zone->flags);\n\t\twakeup_kswapd(zone, 0, 0, zone_idx(zone));\n\t}\n\n\tVM_BUG_ON_PAGE(page && bad_range(zone, page), page);\n\treturn page;\n}\n\nnoinline bool should_fail_alloc_page(gfp_t gfp_mask, unsigned int order)\n{\n\treturn __should_fail_alloc_page(gfp_mask, order);\n}\nALLOW_ERROR_INJECTION(should_fail_alloc_page, TRUE);\n\nstatic inline long __zone_watermark_unusable_free(struct zone *z,\n\t\t\t\tunsigned int order, unsigned int alloc_flags)\n{\n\tlong unusable_free = (1 << order) - 1;\n\n\t \n\tif (likely(!(alloc_flags & ALLOC_RESERVES)))\n\t\tunusable_free += z->nr_reserved_highatomic;\n\n#ifdef CONFIG_CMA\n\t \n\tif (!(alloc_flags & ALLOC_CMA))\n\t\tunusable_free += zone_page_state(z, NR_FREE_CMA_PAGES);\n#endif\n#ifdef CONFIG_UNACCEPTED_MEMORY\n\tunusable_free += zone_page_state(z, NR_UNACCEPTED);\n#endif\n\n\treturn unusable_free;\n}\n\n \nbool __zone_watermark_ok(struct zone *z, unsigned int order, unsigned long mark,\n\t\t\t int highest_zoneidx, unsigned int alloc_flags,\n\t\t\t long free_pages)\n{\n\tlong min = mark;\n\tint o;\n\n\t \n\tfree_pages -= __zone_watermark_unusable_free(z, order, alloc_flags);\n\n\tif (unlikely(alloc_flags & ALLOC_RESERVES)) {\n\t\t \n\t\tif (alloc_flags & ALLOC_MIN_RESERVE) {\n\t\t\tmin -= min / 2;\n\n\t\t\t \n\t\t\tif (alloc_flags & ALLOC_NON_BLOCK)\n\t\t\t\tmin -= min / 4;\n\t\t}\n\n\t\t \n\t\tif (alloc_flags & ALLOC_OOM)\n\t\t\tmin -= min / 2;\n\t}\n\n\t \n\tif (free_pages <= min + z->lowmem_reserve[highest_zoneidx])\n\t\treturn false;\n\n\t \n\tif (!order)\n\t\treturn true;\n\n\t \n\tfor (o = order; o <= MAX_ORDER; o++) {\n\t\tstruct free_area *area = &z->free_area[o];\n\t\tint mt;\n\n\t\tif (!area->nr_free)\n\t\t\tcontinue;\n\n\t\tfor (mt = 0; mt < MIGRATE_PCPTYPES; mt++) {\n\t\t\tif (!free_area_empty(area, mt))\n\t\t\t\treturn true;\n\t\t}\n\n#ifdef CONFIG_CMA\n\t\tif ((alloc_flags & ALLOC_CMA) &&\n\t\t    !free_area_empty(area, MIGRATE_CMA)) {\n\t\t\treturn true;\n\t\t}\n#endif\n\t\tif ((alloc_flags & (ALLOC_HIGHATOMIC|ALLOC_OOM)) &&\n\t\t    !free_area_empty(area, MIGRATE_HIGHATOMIC)) {\n\t\t\treturn true;\n\t\t}\n\t}\n\treturn false;\n}\n\nbool zone_watermark_ok(struct zone *z, unsigned int order, unsigned long mark,\n\t\t      int highest_zoneidx, unsigned int alloc_flags)\n{\n\treturn __zone_watermark_ok(z, order, mark, highest_zoneidx, alloc_flags,\n\t\t\t\t\tzone_page_state(z, NR_FREE_PAGES));\n}\n\nstatic inline bool zone_watermark_fast(struct zone *z, unsigned int order,\n\t\t\t\tunsigned long mark, int highest_zoneidx,\n\t\t\t\tunsigned int alloc_flags, gfp_t gfp_mask)\n{\n\tlong free_pages;\n\n\tfree_pages = zone_page_state(z, NR_FREE_PAGES);\n\n\t \n\tif (!order) {\n\t\tlong usable_free;\n\t\tlong reserved;\n\n\t\tusable_free = free_pages;\n\t\treserved = __zone_watermark_unusable_free(z, 0, alloc_flags);\n\n\t\t \n\t\tusable_free -= min(usable_free, reserved);\n\t\tif (usable_free > mark + z->lowmem_reserve[highest_zoneidx])\n\t\t\treturn true;\n\t}\n\n\tif (__zone_watermark_ok(z, order, mark, highest_zoneidx, alloc_flags,\n\t\t\t\t\tfree_pages))\n\t\treturn true;\n\n\t \n\tif (unlikely(!order && (alloc_flags & ALLOC_MIN_RESERVE) && z->watermark_boost\n\t\t&& ((alloc_flags & ALLOC_WMARK_MASK) == WMARK_MIN))) {\n\t\tmark = z->_watermark[WMARK_MIN];\n\t\treturn __zone_watermark_ok(z, order, mark, highest_zoneidx,\n\t\t\t\t\talloc_flags, free_pages);\n\t}\n\n\treturn false;\n}\n\nbool zone_watermark_ok_safe(struct zone *z, unsigned int order,\n\t\t\tunsigned long mark, int highest_zoneidx)\n{\n\tlong free_pages = zone_page_state(z, NR_FREE_PAGES);\n\n\tif (z->percpu_drift_mark && free_pages < z->percpu_drift_mark)\n\t\tfree_pages = zone_page_state_snapshot(z, NR_FREE_PAGES);\n\n\treturn __zone_watermark_ok(z, order, mark, highest_zoneidx, 0,\n\t\t\t\t\t\t\t\tfree_pages);\n}\n\n#ifdef CONFIG_NUMA\nint __read_mostly node_reclaim_distance = RECLAIM_DISTANCE;\n\nstatic bool zone_allows_reclaim(struct zone *local_zone, struct zone *zone)\n{\n\treturn node_distance(zone_to_nid(local_zone), zone_to_nid(zone)) <=\n\t\t\t\tnode_reclaim_distance;\n}\n#else\t \nstatic bool zone_allows_reclaim(struct zone *local_zone, struct zone *zone)\n{\n\treturn true;\n}\n#endif\t \n\n \nstatic inline unsigned int\nalloc_flags_nofragment(struct zone *zone, gfp_t gfp_mask)\n{\n\tunsigned int alloc_flags;\n\n\t \n\talloc_flags = (__force int) (gfp_mask & __GFP_KSWAPD_RECLAIM);\n\n#ifdef CONFIG_ZONE_DMA32\n\tif (!zone)\n\t\treturn alloc_flags;\n\n\tif (zone_idx(zone) != ZONE_NORMAL)\n\t\treturn alloc_flags;\n\n\t \n\tBUILD_BUG_ON(ZONE_NORMAL - ZONE_DMA32 != 1);\n\tif (nr_online_nodes > 1 && !populated_zone(--zone))\n\t\treturn alloc_flags;\n\n\talloc_flags |= ALLOC_NOFRAGMENT;\n#endif  \n\treturn alloc_flags;\n}\n\n \nstatic inline unsigned int gfp_to_alloc_flags_cma(gfp_t gfp_mask,\n\t\t\t\t\t\t  unsigned int alloc_flags)\n{\n#ifdef CONFIG_CMA\n\tif (gfp_migratetype(gfp_mask) == MIGRATE_MOVABLE)\n\t\talloc_flags |= ALLOC_CMA;\n#endif\n\treturn alloc_flags;\n}\n\n \nstatic struct page *\nget_page_from_freelist(gfp_t gfp_mask, unsigned int order, int alloc_flags,\n\t\t\t\t\t\tconst struct alloc_context *ac)\n{\n\tstruct zoneref *z;\n\tstruct zone *zone;\n\tstruct pglist_data *last_pgdat = NULL;\n\tbool last_pgdat_dirty_ok = false;\n\tbool no_fallback;\n\nretry:\n\t \n\tno_fallback = alloc_flags & ALLOC_NOFRAGMENT;\n\tz = ac->preferred_zoneref;\n\tfor_next_zone_zonelist_nodemask(zone, z, ac->highest_zoneidx,\n\t\t\t\t\tac->nodemask) {\n\t\tstruct page *page;\n\t\tunsigned long mark;\n\n\t\tif (cpusets_enabled() &&\n\t\t\t(alloc_flags & ALLOC_CPUSET) &&\n\t\t\t!__cpuset_zone_allowed(zone, gfp_mask))\n\t\t\t\tcontinue;\n\t\t \n\t\tif (ac->spread_dirty_pages) {\n\t\t\tif (last_pgdat != zone->zone_pgdat) {\n\t\t\t\tlast_pgdat = zone->zone_pgdat;\n\t\t\t\tlast_pgdat_dirty_ok = node_dirty_ok(zone->zone_pgdat);\n\t\t\t}\n\n\t\t\tif (!last_pgdat_dirty_ok)\n\t\t\t\tcontinue;\n\t\t}\n\n\t\tif (no_fallback && nr_online_nodes > 1 &&\n\t\t    zone != ac->preferred_zoneref->zone) {\n\t\t\tint local_nid;\n\n\t\t\t \n\t\t\tlocal_nid = zone_to_nid(ac->preferred_zoneref->zone);\n\t\t\tif (zone_to_nid(zone) != local_nid) {\n\t\t\t\talloc_flags &= ~ALLOC_NOFRAGMENT;\n\t\t\t\tgoto retry;\n\t\t\t}\n\t\t}\n\n\t\tmark = wmark_pages(zone, alloc_flags & ALLOC_WMARK_MASK);\n\t\tif (!zone_watermark_fast(zone, order, mark,\n\t\t\t\t       ac->highest_zoneidx, alloc_flags,\n\t\t\t\t       gfp_mask)) {\n\t\t\tint ret;\n\n\t\t\tif (has_unaccepted_memory()) {\n\t\t\t\tif (try_to_accept_memory(zone, order))\n\t\t\t\t\tgoto try_this_zone;\n\t\t\t}\n\n#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT\n\t\t\t \n\t\t\tif (deferred_pages_enabled()) {\n\t\t\t\tif (_deferred_grow_zone(zone, order))\n\t\t\t\t\tgoto try_this_zone;\n\t\t\t}\n#endif\n\t\t\t \n\t\t\tBUILD_BUG_ON(ALLOC_NO_WATERMARKS < NR_WMARK);\n\t\t\tif (alloc_flags & ALLOC_NO_WATERMARKS)\n\t\t\t\tgoto try_this_zone;\n\n\t\t\tif (!node_reclaim_enabled() ||\n\t\t\t    !zone_allows_reclaim(ac->preferred_zoneref->zone, zone))\n\t\t\t\tcontinue;\n\n\t\t\tret = node_reclaim(zone->zone_pgdat, gfp_mask, order);\n\t\t\tswitch (ret) {\n\t\t\tcase NODE_RECLAIM_NOSCAN:\n\t\t\t\t \n\t\t\t\tcontinue;\n\t\t\tcase NODE_RECLAIM_FULL:\n\t\t\t\t \n\t\t\t\tcontinue;\n\t\t\tdefault:\n\t\t\t\t \n\t\t\t\tif (zone_watermark_ok(zone, order, mark,\n\t\t\t\t\tac->highest_zoneidx, alloc_flags))\n\t\t\t\t\tgoto try_this_zone;\n\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\ntry_this_zone:\n\t\tpage = rmqueue(ac->preferred_zoneref->zone, zone, order,\n\t\t\t\tgfp_mask, alloc_flags, ac->migratetype);\n\t\tif (page) {\n\t\t\tprep_new_page(page, order, gfp_mask, alloc_flags);\n\n\t\t\t \n\t\t\tif (unlikely(alloc_flags & ALLOC_HIGHATOMIC))\n\t\t\t\treserve_highatomic_pageblock(page, zone);\n\n\t\t\treturn page;\n\t\t} else {\n\t\t\tif (has_unaccepted_memory()) {\n\t\t\t\tif (try_to_accept_memory(zone, order))\n\t\t\t\t\tgoto try_this_zone;\n\t\t\t}\n\n#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT\n\t\t\t \n\t\t\tif (deferred_pages_enabled()) {\n\t\t\t\tif (_deferred_grow_zone(zone, order))\n\t\t\t\t\tgoto try_this_zone;\n\t\t\t}\n#endif\n\t\t}\n\t}\n\n\t \n\tif (no_fallback) {\n\t\talloc_flags &= ~ALLOC_NOFRAGMENT;\n\t\tgoto retry;\n\t}\n\n\treturn NULL;\n}\n\nstatic void warn_alloc_show_mem(gfp_t gfp_mask, nodemask_t *nodemask)\n{\n\tunsigned int filter = SHOW_MEM_FILTER_NODES;\n\n\t \n\tif (!(gfp_mask & __GFP_NOMEMALLOC))\n\t\tif (tsk_is_oom_victim(current) ||\n\t\t    (current->flags & (PF_MEMALLOC | PF_EXITING)))\n\t\t\tfilter &= ~SHOW_MEM_FILTER_NODES;\n\tif (!in_task() || !(gfp_mask & __GFP_DIRECT_RECLAIM))\n\t\tfilter &= ~SHOW_MEM_FILTER_NODES;\n\n\t__show_mem(filter, nodemask, gfp_zone(gfp_mask));\n}\n\nvoid warn_alloc(gfp_t gfp_mask, nodemask_t *nodemask, const char *fmt, ...)\n{\n\tstruct va_format vaf;\n\tva_list args;\n\tstatic DEFINE_RATELIMIT_STATE(nopage_rs, 10*HZ, 1);\n\n\tif ((gfp_mask & __GFP_NOWARN) ||\n\t     !__ratelimit(&nopage_rs) ||\n\t     ((gfp_mask & __GFP_DMA) && !has_managed_dma()))\n\t\treturn;\n\n\tva_start(args, fmt);\n\tvaf.fmt = fmt;\n\tvaf.va = &args;\n\tpr_warn(\"%s: %pV, mode:%#x(%pGg), nodemask=%*pbl\",\n\t\t\tcurrent->comm, &vaf, gfp_mask, &gfp_mask,\n\t\t\tnodemask_pr_args(nodemask));\n\tva_end(args);\n\n\tcpuset_print_current_mems_allowed();\n\tpr_cont(\"\\n\");\n\tdump_stack();\n\twarn_alloc_show_mem(gfp_mask, nodemask);\n}\n\nstatic inline struct page *\n__alloc_pages_cpuset_fallback(gfp_t gfp_mask, unsigned int order,\n\t\t\t      unsigned int alloc_flags,\n\t\t\t      const struct alloc_context *ac)\n{\n\tstruct page *page;\n\n\tpage = get_page_from_freelist(gfp_mask, order,\n\t\t\talloc_flags|ALLOC_CPUSET, ac);\n\t \n\tif (!page)\n\t\tpage = get_page_from_freelist(gfp_mask, order,\n\t\t\t\talloc_flags, ac);\n\n\treturn page;\n}\n\nstatic inline struct page *\n__alloc_pages_may_oom(gfp_t gfp_mask, unsigned int order,\n\tconst struct alloc_context *ac, unsigned long *did_some_progress)\n{\n\tstruct oom_control oc = {\n\t\t.zonelist = ac->zonelist,\n\t\t.nodemask = ac->nodemask,\n\t\t.memcg = NULL,\n\t\t.gfp_mask = gfp_mask,\n\t\t.order = order,\n\t};\n\tstruct page *page;\n\n\t*did_some_progress = 0;\n\n\t \n\tif (!mutex_trylock(&oom_lock)) {\n\t\t*did_some_progress = 1;\n\t\tschedule_timeout_uninterruptible(1);\n\t\treturn NULL;\n\t}\n\n\t \n\tpage = get_page_from_freelist((gfp_mask | __GFP_HARDWALL) &\n\t\t\t\t      ~__GFP_DIRECT_RECLAIM, order,\n\t\t\t\t      ALLOC_WMARK_HIGH|ALLOC_CPUSET, ac);\n\tif (page)\n\t\tgoto out;\n\n\t \n\tif (current->flags & PF_DUMPCORE)\n\t\tgoto out;\n\t \n\tif (order > PAGE_ALLOC_COSTLY_ORDER)\n\t\tgoto out;\n\t \n\tif (gfp_mask & (__GFP_RETRY_MAYFAIL | __GFP_THISNODE))\n\t\tgoto out;\n\t \n\tif (ac->highest_zoneidx < ZONE_NORMAL)\n\t\tgoto out;\n\tif (pm_suspended_storage())\n\t\tgoto out;\n\t \n\n\t \n\tif (out_of_memory(&oc) ||\n\t    WARN_ON_ONCE_GFP(gfp_mask & __GFP_NOFAIL, gfp_mask)) {\n\t\t*did_some_progress = 1;\n\n\t\t \n\t\tif (gfp_mask & __GFP_NOFAIL)\n\t\t\tpage = __alloc_pages_cpuset_fallback(gfp_mask, order,\n\t\t\t\t\tALLOC_NO_WATERMARKS, ac);\n\t}\nout:\n\tmutex_unlock(&oom_lock);\n\treturn page;\n}\n\n \n#define MAX_COMPACT_RETRIES 16\n\n#ifdef CONFIG_COMPACTION\n \nstatic struct page *\n__alloc_pages_direct_compact(gfp_t gfp_mask, unsigned int order,\n\t\tunsigned int alloc_flags, const struct alloc_context *ac,\n\t\tenum compact_priority prio, enum compact_result *compact_result)\n{\n\tstruct page *page = NULL;\n\tunsigned long pflags;\n\tunsigned int noreclaim_flag;\n\n\tif (!order)\n\t\treturn NULL;\n\n\tpsi_memstall_enter(&pflags);\n\tdelayacct_compact_start();\n\tnoreclaim_flag = memalloc_noreclaim_save();\n\n\t*compact_result = try_to_compact_pages(gfp_mask, order, alloc_flags, ac,\n\t\t\t\t\t\t\t\tprio, &page);\n\n\tmemalloc_noreclaim_restore(noreclaim_flag);\n\tpsi_memstall_leave(&pflags);\n\tdelayacct_compact_end();\n\n\tif (*compact_result == COMPACT_SKIPPED)\n\t\treturn NULL;\n\t \n\tcount_vm_event(COMPACTSTALL);\n\n\t \n\tif (page)\n\t\tprep_new_page(page, order, gfp_mask, alloc_flags);\n\n\t \n\tif (!page)\n\t\tpage = get_page_from_freelist(gfp_mask, order, alloc_flags, ac);\n\n\tif (page) {\n\t\tstruct zone *zone = page_zone(page);\n\n\t\tzone->compact_blockskip_flush = false;\n\t\tcompaction_defer_reset(zone, order, true);\n\t\tcount_vm_event(COMPACTSUCCESS);\n\t\treturn page;\n\t}\n\n\t \n\tcount_vm_event(COMPACTFAIL);\n\n\tcond_resched();\n\n\treturn NULL;\n}\n\nstatic inline bool\nshould_compact_retry(struct alloc_context *ac, int order, int alloc_flags,\n\t\t     enum compact_result compact_result,\n\t\t     enum compact_priority *compact_priority,\n\t\t     int *compaction_retries)\n{\n\tint max_retries = MAX_COMPACT_RETRIES;\n\tint min_priority;\n\tbool ret = false;\n\tint retries = *compaction_retries;\n\tenum compact_priority priority = *compact_priority;\n\n\tif (!order)\n\t\treturn false;\n\n\tif (fatal_signal_pending(current))\n\t\treturn false;\n\n\t \n\tif (compact_result == COMPACT_SKIPPED) {\n\t\tret = compaction_zonelist_suitable(ac, order, alloc_flags);\n\t\tgoto out;\n\t}\n\n\t \n\tif (compact_result == COMPACT_SUCCESS) {\n\t\t \n\t\tif (order > PAGE_ALLOC_COSTLY_ORDER)\n\t\t\tmax_retries /= 4;\n\n\t\tif (++(*compaction_retries) <= max_retries) {\n\t\t\tret = true;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t \n\tmin_priority = (order > PAGE_ALLOC_COSTLY_ORDER) ?\n\t\t\tMIN_COMPACT_COSTLY_PRIORITY : MIN_COMPACT_PRIORITY;\n\n\tif (*compact_priority > min_priority) {\n\t\t(*compact_priority)--;\n\t\t*compaction_retries = 0;\n\t\tret = true;\n\t}\nout:\n\ttrace_compact_retry(order, priority, compact_result, retries, max_retries, ret);\n\treturn ret;\n}\n#else\nstatic inline struct page *\n__alloc_pages_direct_compact(gfp_t gfp_mask, unsigned int order,\n\t\tunsigned int alloc_flags, const struct alloc_context *ac,\n\t\tenum compact_priority prio, enum compact_result *compact_result)\n{\n\t*compact_result = COMPACT_SKIPPED;\n\treturn NULL;\n}\n\nstatic inline bool\nshould_compact_retry(struct alloc_context *ac, unsigned int order, int alloc_flags,\n\t\t     enum compact_result compact_result,\n\t\t     enum compact_priority *compact_priority,\n\t\t     int *compaction_retries)\n{\n\tstruct zone *zone;\n\tstruct zoneref *z;\n\n\tif (!order || order > PAGE_ALLOC_COSTLY_ORDER)\n\t\treturn false;\n\n\t \n\tfor_each_zone_zonelist_nodemask(zone, z, ac->zonelist,\n\t\t\t\tac->highest_zoneidx, ac->nodemask) {\n\t\tif (zone_watermark_ok(zone, 0, min_wmark_pages(zone),\n\t\t\t\t\tac->highest_zoneidx, alloc_flags))\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n#endif  \n\n#ifdef CONFIG_LOCKDEP\nstatic struct lockdep_map __fs_reclaim_map =\n\tSTATIC_LOCKDEP_MAP_INIT(\"fs_reclaim\", &__fs_reclaim_map);\n\nstatic bool __need_reclaim(gfp_t gfp_mask)\n{\n\t \n\tif (!(gfp_mask & __GFP_DIRECT_RECLAIM))\n\t\treturn false;\n\n\t \n\tif (current->flags & PF_MEMALLOC)\n\t\treturn false;\n\n\tif (gfp_mask & __GFP_NOLOCKDEP)\n\t\treturn false;\n\n\treturn true;\n}\n\nvoid __fs_reclaim_acquire(unsigned long ip)\n{\n\tlock_acquire_exclusive(&__fs_reclaim_map, 0, 0, NULL, ip);\n}\n\nvoid __fs_reclaim_release(unsigned long ip)\n{\n\tlock_release(&__fs_reclaim_map, ip);\n}\n\nvoid fs_reclaim_acquire(gfp_t gfp_mask)\n{\n\tgfp_mask = current_gfp_context(gfp_mask);\n\n\tif (__need_reclaim(gfp_mask)) {\n\t\tif (gfp_mask & __GFP_FS)\n\t\t\t__fs_reclaim_acquire(_RET_IP_);\n\n#ifdef CONFIG_MMU_NOTIFIER\n\t\tlock_map_acquire(&__mmu_notifier_invalidate_range_start_map);\n\t\tlock_map_release(&__mmu_notifier_invalidate_range_start_map);\n#endif\n\n\t}\n}\nEXPORT_SYMBOL_GPL(fs_reclaim_acquire);\n\nvoid fs_reclaim_release(gfp_t gfp_mask)\n{\n\tgfp_mask = current_gfp_context(gfp_mask);\n\n\tif (__need_reclaim(gfp_mask)) {\n\t\tif (gfp_mask & __GFP_FS)\n\t\t\t__fs_reclaim_release(_RET_IP_);\n\t}\n}\nEXPORT_SYMBOL_GPL(fs_reclaim_release);\n#endif\n\n \nstatic DEFINE_SEQLOCK(zonelist_update_seq);\n\nstatic unsigned int zonelist_iter_begin(void)\n{\n\tif (IS_ENABLED(CONFIG_MEMORY_HOTREMOVE))\n\t\treturn read_seqbegin(&zonelist_update_seq);\n\n\treturn 0;\n}\n\nstatic unsigned int check_retry_zonelist(unsigned int seq)\n{\n\tif (IS_ENABLED(CONFIG_MEMORY_HOTREMOVE))\n\t\treturn read_seqretry(&zonelist_update_seq, seq);\n\n\treturn seq;\n}\n\n \nstatic unsigned long\n__perform_reclaim(gfp_t gfp_mask, unsigned int order,\n\t\t\t\t\tconst struct alloc_context *ac)\n{\n\tunsigned int noreclaim_flag;\n\tunsigned long progress;\n\n\tcond_resched();\n\n\t \n\tcpuset_memory_pressure_bump();\n\tfs_reclaim_acquire(gfp_mask);\n\tnoreclaim_flag = memalloc_noreclaim_save();\n\n\tprogress = try_to_free_pages(ac->zonelist, order, gfp_mask,\n\t\t\t\t\t\t\t\tac->nodemask);\n\n\tmemalloc_noreclaim_restore(noreclaim_flag);\n\tfs_reclaim_release(gfp_mask);\n\n\tcond_resched();\n\n\treturn progress;\n}\n\n \nstatic inline struct page *\n__alloc_pages_direct_reclaim(gfp_t gfp_mask, unsigned int order,\n\t\tunsigned int alloc_flags, const struct alloc_context *ac,\n\t\tunsigned long *did_some_progress)\n{\n\tstruct page *page = NULL;\n\tunsigned long pflags;\n\tbool drained = false;\n\n\tpsi_memstall_enter(&pflags);\n\t*did_some_progress = __perform_reclaim(gfp_mask, order, ac);\n\tif (unlikely(!(*did_some_progress)))\n\t\tgoto out;\n\nretry:\n\tpage = get_page_from_freelist(gfp_mask, order, alloc_flags, ac);\n\n\t \n\tif (!page && !drained) {\n\t\tunreserve_highatomic_pageblock(ac, false);\n\t\tdrain_all_pages(NULL);\n\t\tdrained = true;\n\t\tgoto retry;\n\t}\nout:\n\tpsi_memstall_leave(&pflags);\n\n\treturn page;\n}\n\nstatic void wake_all_kswapds(unsigned int order, gfp_t gfp_mask,\n\t\t\t     const struct alloc_context *ac)\n{\n\tstruct zoneref *z;\n\tstruct zone *zone;\n\tpg_data_t *last_pgdat = NULL;\n\tenum zone_type highest_zoneidx = ac->highest_zoneidx;\n\n\tfor_each_zone_zonelist_nodemask(zone, z, ac->zonelist, highest_zoneidx,\n\t\t\t\t\tac->nodemask) {\n\t\tif (!managed_zone(zone))\n\t\t\tcontinue;\n\t\tif (last_pgdat != zone->zone_pgdat) {\n\t\t\twakeup_kswapd(zone, gfp_mask, order, highest_zoneidx);\n\t\t\tlast_pgdat = zone->zone_pgdat;\n\t\t}\n\t}\n}\n\nstatic inline unsigned int\ngfp_to_alloc_flags(gfp_t gfp_mask, unsigned int order)\n{\n\tunsigned int alloc_flags = ALLOC_WMARK_MIN | ALLOC_CPUSET;\n\n\t \n\tBUILD_BUG_ON(__GFP_HIGH != (__force gfp_t) ALLOC_MIN_RESERVE);\n\tBUILD_BUG_ON(__GFP_KSWAPD_RECLAIM != (__force gfp_t) ALLOC_KSWAPD);\n\n\t \n\talloc_flags |= (__force int)\n\t\t(gfp_mask & (__GFP_HIGH | __GFP_KSWAPD_RECLAIM));\n\n\tif (!(gfp_mask & __GFP_DIRECT_RECLAIM)) {\n\t\t \n\t\tif (!(gfp_mask & __GFP_NOMEMALLOC)) {\n\t\t\talloc_flags |= ALLOC_NON_BLOCK;\n\n\t\t\tif (order > 0)\n\t\t\t\talloc_flags |= ALLOC_HIGHATOMIC;\n\t\t}\n\n\t\t \n\t\tif (alloc_flags & ALLOC_MIN_RESERVE)\n\t\t\talloc_flags &= ~ALLOC_CPUSET;\n\t} else if (unlikely(rt_task(current)) && in_task())\n\t\talloc_flags |= ALLOC_MIN_RESERVE;\n\n\talloc_flags = gfp_to_alloc_flags_cma(gfp_mask, alloc_flags);\n\n\treturn alloc_flags;\n}\n\nstatic bool oom_reserves_allowed(struct task_struct *tsk)\n{\n\tif (!tsk_is_oom_victim(tsk))\n\t\treturn false;\n\n\t \n\tif (!IS_ENABLED(CONFIG_MMU) && !test_thread_flag(TIF_MEMDIE))\n\t\treturn false;\n\n\treturn true;\n}\n\n \nstatic inline int __gfp_pfmemalloc_flags(gfp_t gfp_mask)\n{\n\tif (unlikely(gfp_mask & __GFP_NOMEMALLOC))\n\t\treturn 0;\n\tif (gfp_mask & __GFP_MEMALLOC)\n\t\treturn ALLOC_NO_WATERMARKS;\n\tif (in_serving_softirq() && (current->flags & PF_MEMALLOC))\n\t\treturn ALLOC_NO_WATERMARKS;\n\tif (!in_interrupt()) {\n\t\tif (current->flags & PF_MEMALLOC)\n\t\t\treturn ALLOC_NO_WATERMARKS;\n\t\telse if (oom_reserves_allowed(current))\n\t\t\treturn ALLOC_OOM;\n\t}\n\n\treturn 0;\n}\n\nbool gfp_pfmemalloc_allowed(gfp_t gfp_mask)\n{\n\treturn !!__gfp_pfmemalloc_flags(gfp_mask);\n}\n\n \nstatic inline bool\nshould_reclaim_retry(gfp_t gfp_mask, unsigned order,\n\t\t     struct alloc_context *ac, int alloc_flags,\n\t\t     bool did_some_progress, int *no_progress_loops)\n{\n\tstruct zone *zone;\n\tstruct zoneref *z;\n\tbool ret = false;\n\n\t \n\tif (did_some_progress && order <= PAGE_ALLOC_COSTLY_ORDER)\n\t\t*no_progress_loops = 0;\n\telse\n\t\t(*no_progress_loops)++;\n\n\t \n\tif (*no_progress_loops > MAX_RECLAIM_RETRIES) {\n\t\t \n\t\treturn unreserve_highatomic_pageblock(ac, true);\n\t}\n\n\t \n\tfor_each_zone_zonelist_nodemask(zone, z, ac->zonelist,\n\t\t\t\tac->highest_zoneidx, ac->nodemask) {\n\t\tunsigned long available;\n\t\tunsigned long reclaimable;\n\t\tunsigned long min_wmark = min_wmark_pages(zone);\n\t\tbool wmark;\n\n\t\tavailable = reclaimable = zone_reclaimable_pages(zone);\n\t\tavailable += zone_page_state_snapshot(zone, NR_FREE_PAGES);\n\n\t\t \n\t\twmark = __zone_watermark_ok(zone, order, min_wmark,\n\t\t\t\tac->highest_zoneidx, alloc_flags, available);\n\t\ttrace_reclaim_retry_zone(z, order, reclaimable,\n\t\t\t\tavailable, min_wmark, *no_progress_loops, wmark);\n\t\tif (wmark) {\n\t\t\tret = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\tif (current->flags & PF_WQ_WORKER)\n\t\tschedule_timeout_uninterruptible(1);\n\telse\n\t\tcond_resched();\n\treturn ret;\n}\n\nstatic inline bool\ncheck_retry_cpuset(int cpuset_mems_cookie, struct alloc_context *ac)\n{\n\t \n\tif (cpusets_enabled() && ac->nodemask &&\n\t\t\t!cpuset_nodemask_valid_mems_allowed(ac->nodemask)) {\n\t\tac->nodemask = NULL;\n\t\treturn true;\n\t}\n\n\t \n\tif (read_mems_allowed_retry(cpuset_mems_cookie))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic inline struct page *\n__alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,\n\t\t\t\t\t\tstruct alloc_context *ac)\n{\n\tbool can_direct_reclaim = gfp_mask & __GFP_DIRECT_RECLAIM;\n\tconst bool costly_order = order > PAGE_ALLOC_COSTLY_ORDER;\n\tstruct page *page = NULL;\n\tunsigned int alloc_flags;\n\tunsigned long did_some_progress;\n\tenum compact_priority compact_priority;\n\tenum compact_result compact_result;\n\tint compaction_retries;\n\tint no_progress_loops;\n\tunsigned int cpuset_mems_cookie;\n\tunsigned int zonelist_iter_cookie;\n\tint reserve_flags;\n\nrestart:\n\tcompaction_retries = 0;\n\tno_progress_loops = 0;\n\tcompact_priority = DEF_COMPACT_PRIORITY;\n\tcpuset_mems_cookie = read_mems_allowed_begin();\n\tzonelist_iter_cookie = zonelist_iter_begin();\n\n\t \n\talloc_flags = gfp_to_alloc_flags(gfp_mask, order);\n\n\t \n\tac->preferred_zoneref = first_zones_zonelist(ac->zonelist,\n\t\t\t\t\tac->highest_zoneidx, ac->nodemask);\n\tif (!ac->preferred_zoneref->zone)\n\t\tgoto nopage;\n\n\t \n\tif (cpusets_insane_config() && (gfp_mask & __GFP_HARDWALL)) {\n\t\tstruct zoneref *z = first_zones_zonelist(ac->zonelist,\n\t\t\t\t\tac->highest_zoneidx,\n\t\t\t\t\t&cpuset_current_mems_allowed);\n\t\tif (!z->zone)\n\t\t\tgoto nopage;\n\t}\n\n\tif (alloc_flags & ALLOC_KSWAPD)\n\t\twake_all_kswapds(order, gfp_mask, ac);\n\n\t \n\tpage = get_page_from_freelist(gfp_mask, order, alloc_flags, ac);\n\tif (page)\n\t\tgoto got_pg;\n\n\t \n\tif (can_direct_reclaim &&\n\t\t\t(costly_order ||\n\t\t\t   (order > 0 && ac->migratetype != MIGRATE_MOVABLE))\n\t\t\t&& !gfp_pfmemalloc_allowed(gfp_mask)) {\n\t\tpage = __alloc_pages_direct_compact(gfp_mask, order,\n\t\t\t\t\t\talloc_flags, ac,\n\t\t\t\t\t\tINIT_COMPACT_PRIORITY,\n\t\t\t\t\t\t&compact_result);\n\t\tif (page)\n\t\t\tgoto got_pg;\n\n\t\t \n\t\tif (costly_order && (gfp_mask & __GFP_NORETRY)) {\n\t\t\t \n\t\t\tif (compact_result == COMPACT_SKIPPED ||\n\t\t\t    compact_result == COMPACT_DEFERRED)\n\t\t\t\tgoto nopage;\n\n\t\t\t \n\t\t\tcompact_priority = INIT_COMPACT_PRIORITY;\n\t\t}\n\t}\n\nretry:\n\t \n\tif (alloc_flags & ALLOC_KSWAPD)\n\t\twake_all_kswapds(order, gfp_mask, ac);\n\n\treserve_flags = __gfp_pfmemalloc_flags(gfp_mask);\n\tif (reserve_flags)\n\t\talloc_flags = gfp_to_alloc_flags_cma(gfp_mask, reserve_flags) |\n\t\t\t\t\t  (alloc_flags & ALLOC_KSWAPD);\n\n\t \n\tif (!(alloc_flags & ALLOC_CPUSET) || reserve_flags) {\n\t\tac->nodemask = NULL;\n\t\tac->preferred_zoneref = first_zones_zonelist(ac->zonelist,\n\t\t\t\t\tac->highest_zoneidx, ac->nodemask);\n\t}\n\n\t \n\tpage = get_page_from_freelist(gfp_mask, order, alloc_flags, ac);\n\tif (page)\n\t\tgoto got_pg;\n\n\t \n\tif (!can_direct_reclaim)\n\t\tgoto nopage;\n\n\t \n\tif (current->flags & PF_MEMALLOC)\n\t\tgoto nopage;\n\n\t \n\tpage = __alloc_pages_direct_reclaim(gfp_mask, order, alloc_flags, ac,\n\t\t\t\t\t\t\t&did_some_progress);\n\tif (page)\n\t\tgoto got_pg;\n\n\t \n\tpage = __alloc_pages_direct_compact(gfp_mask, order, alloc_flags, ac,\n\t\t\t\t\tcompact_priority, &compact_result);\n\tif (page)\n\t\tgoto got_pg;\n\n\t \n\tif (gfp_mask & __GFP_NORETRY)\n\t\tgoto nopage;\n\n\t \n\tif (costly_order && !(gfp_mask & __GFP_RETRY_MAYFAIL))\n\t\tgoto nopage;\n\n\tif (should_reclaim_retry(gfp_mask, order, ac, alloc_flags,\n\t\t\t\t did_some_progress > 0, &no_progress_loops))\n\t\tgoto retry;\n\n\t \n\tif (did_some_progress > 0 &&\n\t\t\tshould_compact_retry(ac, order, alloc_flags,\n\t\t\t\tcompact_result, &compact_priority,\n\t\t\t\t&compaction_retries))\n\t\tgoto retry;\n\n\n\t \n\tif (check_retry_cpuset(cpuset_mems_cookie, ac) ||\n\t    check_retry_zonelist(zonelist_iter_cookie))\n\t\tgoto restart;\n\n\t \n\tpage = __alloc_pages_may_oom(gfp_mask, order, ac, &did_some_progress);\n\tif (page)\n\t\tgoto got_pg;\n\n\t \n\tif (tsk_is_oom_victim(current) &&\n\t    (alloc_flags & ALLOC_OOM ||\n\t     (gfp_mask & __GFP_NOMEMALLOC)))\n\t\tgoto nopage;\n\n\t \n\tif (did_some_progress) {\n\t\tno_progress_loops = 0;\n\t\tgoto retry;\n\t}\n\nnopage:\n\t \n\tif (check_retry_cpuset(cpuset_mems_cookie, ac) ||\n\t    check_retry_zonelist(zonelist_iter_cookie))\n\t\tgoto restart;\n\n\t \n\tif (gfp_mask & __GFP_NOFAIL) {\n\t\t \n\t\tif (WARN_ON_ONCE_GFP(!can_direct_reclaim, gfp_mask))\n\t\t\tgoto fail;\n\n\t\t \n\t\tWARN_ON_ONCE_GFP(current->flags & PF_MEMALLOC, gfp_mask);\n\n\t\t \n\t\tWARN_ON_ONCE_GFP(costly_order, gfp_mask);\n\n\t\t \n\t\tpage = __alloc_pages_cpuset_fallback(gfp_mask, order, ALLOC_MIN_RESERVE, ac);\n\t\tif (page)\n\t\t\tgoto got_pg;\n\n\t\tcond_resched();\n\t\tgoto retry;\n\t}\nfail:\n\twarn_alloc(gfp_mask, ac->nodemask,\n\t\t\t\"page allocation failure: order:%u\", order);\ngot_pg:\n\treturn page;\n}\n\nstatic inline bool prepare_alloc_pages(gfp_t gfp_mask, unsigned int order,\n\t\tint preferred_nid, nodemask_t *nodemask,\n\t\tstruct alloc_context *ac, gfp_t *alloc_gfp,\n\t\tunsigned int *alloc_flags)\n{\n\tac->highest_zoneidx = gfp_zone(gfp_mask);\n\tac->zonelist = node_zonelist(preferred_nid, gfp_mask);\n\tac->nodemask = nodemask;\n\tac->migratetype = gfp_migratetype(gfp_mask);\n\n\tif (cpusets_enabled()) {\n\t\t*alloc_gfp |= __GFP_HARDWALL;\n\t\t \n\t\tif (in_task() && !ac->nodemask)\n\t\t\tac->nodemask = &cpuset_current_mems_allowed;\n\t\telse\n\t\t\t*alloc_flags |= ALLOC_CPUSET;\n\t}\n\n\tmight_alloc(gfp_mask);\n\n\tif (should_fail_alloc_page(gfp_mask, order))\n\t\treturn false;\n\n\t*alloc_flags = gfp_to_alloc_flags_cma(gfp_mask, *alloc_flags);\n\n\t \n\tac->spread_dirty_pages = (gfp_mask & __GFP_WRITE);\n\n\t \n\tac->preferred_zoneref = first_zones_zonelist(ac->zonelist,\n\t\t\t\t\tac->highest_zoneidx, ac->nodemask);\n\n\treturn true;\n}\n\n \nunsigned long __alloc_pages_bulk(gfp_t gfp, int preferred_nid,\n\t\t\tnodemask_t *nodemask, int nr_pages,\n\t\t\tstruct list_head *page_list,\n\t\t\tstruct page **page_array)\n{\n\tstruct page *page;\n\tunsigned long __maybe_unused UP_flags;\n\tstruct zone *zone;\n\tstruct zoneref *z;\n\tstruct per_cpu_pages *pcp;\n\tstruct list_head *pcp_list;\n\tstruct alloc_context ac;\n\tgfp_t alloc_gfp;\n\tunsigned int alloc_flags = ALLOC_WMARK_LOW;\n\tint nr_populated = 0, nr_account = 0;\n\n\t \n\twhile (page_array && nr_populated < nr_pages && page_array[nr_populated])\n\t\tnr_populated++;\n\n\t \n\tif (unlikely(nr_pages <= 0))\n\t\tgoto out;\n\n\t \n\tif (unlikely(page_array && nr_pages - nr_populated == 0))\n\t\tgoto out;\n\n\t \n\tif (memcg_kmem_online() && (gfp & __GFP_ACCOUNT))\n\t\tgoto failed;\n\n\t \n\tif (nr_pages - nr_populated == 1)\n\t\tgoto failed;\n\n#ifdef CONFIG_PAGE_OWNER\n\t \n\tif (static_branch_unlikely(&page_owner_inited))\n\t\tgoto failed;\n#endif\n\n\t \n\tgfp &= gfp_allowed_mask;\n\talloc_gfp = gfp;\n\tif (!prepare_alloc_pages(gfp, 0, preferred_nid, nodemask, &ac, &alloc_gfp, &alloc_flags))\n\t\tgoto out;\n\tgfp = alloc_gfp;\n\n\t \n\tfor_each_zone_zonelist_nodemask(zone, z, ac.zonelist, ac.highest_zoneidx, ac.nodemask) {\n\t\tunsigned long mark;\n\n\t\tif (cpusets_enabled() && (alloc_flags & ALLOC_CPUSET) &&\n\t\t    !__cpuset_zone_allowed(zone, gfp)) {\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (nr_online_nodes > 1 && zone != ac.preferred_zoneref->zone &&\n\t\t    zone_to_nid(zone) != zone_to_nid(ac.preferred_zoneref->zone)) {\n\t\t\tgoto failed;\n\t\t}\n\n\t\tmark = wmark_pages(zone, alloc_flags & ALLOC_WMARK_MASK) + nr_pages;\n\t\tif (zone_watermark_fast(zone, 0,  mark,\n\t\t\t\tzonelist_zone_idx(ac.preferred_zoneref),\n\t\t\t\talloc_flags, gfp)) {\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\tif (unlikely(!zone))\n\t\tgoto failed;\n\n\t \n\tpcp_trylock_prepare(UP_flags);\n\tpcp = pcp_spin_trylock(zone->per_cpu_pageset);\n\tif (!pcp)\n\t\tgoto failed_irq;\n\n\t \n\tpcp_list = &pcp->lists[order_to_pindex(ac.migratetype, 0)];\n\twhile (nr_populated < nr_pages) {\n\n\t\t \n\t\tif (page_array && page_array[nr_populated]) {\n\t\t\tnr_populated++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tpage = __rmqueue_pcplist(zone, 0, ac.migratetype, alloc_flags,\n\t\t\t\t\t\t\t\tpcp, pcp_list);\n\t\tif (unlikely(!page)) {\n\t\t\t \n\t\t\tif (!nr_account) {\n\t\t\t\tpcp_spin_unlock(pcp);\n\t\t\t\tgoto failed_irq;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\tnr_account++;\n\n\t\tprep_new_page(page, 0, gfp, 0);\n\t\tif (page_list)\n\t\t\tlist_add(&page->lru, page_list);\n\t\telse\n\t\t\tpage_array[nr_populated] = page;\n\t\tnr_populated++;\n\t}\n\n\tpcp_spin_unlock(pcp);\n\tpcp_trylock_finish(UP_flags);\n\n\t__count_zid_vm_events(PGALLOC, zone_idx(zone), nr_account);\n\tzone_statistics(ac.preferred_zoneref->zone, zone, nr_account);\n\nout:\n\treturn nr_populated;\n\nfailed_irq:\n\tpcp_trylock_finish(UP_flags);\n\nfailed:\n\tpage = __alloc_pages(gfp, 0, preferred_nid, nodemask);\n\tif (page) {\n\t\tif (page_list)\n\t\t\tlist_add(&page->lru, page_list);\n\t\telse\n\t\t\tpage_array[nr_populated] = page;\n\t\tnr_populated++;\n\t}\n\n\tgoto out;\n}\nEXPORT_SYMBOL_GPL(__alloc_pages_bulk);\n\n \nstruct page *__alloc_pages(gfp_t gfp, unsigned int order, int preferred_nid,\n\t\t\t\t\t\t\tnodemask_t *nodemask)\n{\n\tstruct page *page;\n\tunsigned int alloc_flags = ALLOC_WMARK_LOW;\n\tgfp_t alloc_gfp;  \n\tstruct alloc_context ac = { };\n\n\t \n\tif (WARN_ON_ONCE_GFP(order > MAX_ORDER, gfp))\n\t\treturn NULL;\n\n\tgfp &= gfp_allowed_mask;\n\t \n\tgfp = current_gfp_context(gfp);\n\talloc_gfp = gfp;\n\tif (!prepare_alloc_pages(gfp, order, preferred_nid, nodemask, &ac,\n\t\t\t&alloc_gfp, &alloc_flags))\n\t\treturn NULL;\n\n\t \n\talloc_flags |= alloc_flags_nofragment(ac.preferred_zoneref->zone, gfp);\n\n\t \n\tpage = get_page_from_freelist(alloc_gfp, order, alloc_flags, &ac);\n\tif (likely(page))\n\t\tgoto out;\n\n\talloc_gfp = gfp;\n\tac.spread_dirty_pages = false;\n\n\t \n\tac.nodemask = nodemask;\n\n\tpage = __alloc_pages_slowpath(alloc_gfp, order, &ac);\n\nout:\n\tif (memcg_kmem_online() && (gfp & __GFP_ACCOUNT) && page &&\n\t    unlikely(__memcg_kmem_charge_page(page, gfp, order) != 0)) {\n\t\t__free_pages(page, order);\n\t\tpage = NULL;\n\t}\n\n\ttrace_mm_page_alloc(page, order, alloc_gfp, ac.migratetype);\n\tkmsan_alloc_page(page, order, alloc_gfp);\n\n\treturn page;\n}\nEXPORT_SYMBOL(__alloc_pages);\n\nstruct folio *__folio_alloc(gfp_t gfp, unsigned int order, int preferred_nid,\n\t\tnodemask_t *nodemask)\n{\n\tstruct page *page = __alloc_pages(gfp | __GFP_COMP, order,\n\t\t\tpreferred_nid, nodemask);\n\tstruct folio *folio = (struct folio *)page;\n\n\tif (folio && order > 1)\n\t\tfolio_prep_large_rmappable(folio);\n\treturn folio;\n}\nEXPORT_SYMBOL(__folio_alloc);\n\n \nunsigned long __get_free_pages(gfp_t gfp_mask, unsigned int order)\n{\n\tstruct page *page;\n\n\tpage = alloc_pages(gfp_mask & ~__GFP_HIGHMEM, order);\n\tif (!page)\n\t\treturn 0;\n\treturn (unsigned long) page_address(page);\n}\nEXPORT_SYMBOL(__get_free_pages);\n\nunsigned long get_zeroed_page(gfp_t gfp_mask)\n{\n\treturn __get_free_page(gfp_mask | __GFP_ZERO);\n}\nEXPORT_SYMBOL(get_zeroed_page);\n\n \nvoid __free_pages(struct page *page, unsigned int order)\n{\n\t \n\tint head = PageHead(page);\n\n\tif (put_page_testzero(page))\n\t\tfree_the_page(page, order);\n\telse if (!head)\n\t\twhile (order-- > 0)\n\t\t\tfree_the_page(page + (1 << order), order);\n}\nEXPORT_SYMBOL(__free_pages);\n\nvoid free_pages(unsigned long addr, unsigned int order)\n{\n\tif (addr != 0) {\n\t\tVM_BUG_ON(!virt_addr_valid((void *)addr));\n\t\t__free_pages(virt_to_page((void *)addr), order);\n\t}\n}\n\nEXPORT_SYMBOL(free_pages);\n\n \nstatic struct page *__page_frag_cache_refill(struct page_frag_cache *nc,\n\t\t\t\t\t     gfp_t gfp_mask)\n{\n\tstruct page *page = NULL;\n\tgfp_t gfp = gfp_mask;\n\n#if (PAGE_SIZE < PAGE_FRAG_CACHE_MAX_SIZE)\n\tgfp_mask |= __GFP_COMP | __GFP_NOWARN | __GFP_NORETRY |\n\t\t    __GFP_NOMEMALLOC;\n\tpage = alloc_pages_node(NUMA_NO_NODE, gfp_mask,\n\t\t\t\tPAGE_FRAG_CACHE_MAX_ORDER);\n\tnc->size = page ? PAGE_FRAG_CACHE_MAX_SIZE : PAGE_SIZE;\n#endif\n\tif (unlikely(!page))\n\t\tpage = alloc_pages_node(NUMA_NO_NODE, gfp, 0);\n\n\tnc->va = page ? page_address(page) : NULL;\n\n\treturn page;\n}\n\nvoid __page_frag_cache_drain(struct page *page, unsigned int count)\n{\n\tVM_BUG_ON_PAGE(page_ref_count(page) == 0, page);\n\n\tif (page_ref_sub_and_test(page, count))\n\t\tfree_the_page(page, compound_order(page));\n}\nEXPORT_SYMBOL(__page_frag_cache_drain);\n\nvoid *page_frag_alloc_align(struct page_frag_cache *nc,\n\t\t      unsigned int fragsz, gfp_t gfp_mask,\n\t\t      unsigned int align_mask)\n{\n\tunsigned int size = PAGE_SIZE;\n\tstruct page *page;\n\tint offset;\n\n\tif (unlikely(!nc->va)) {\nrefill:\n\t\tpage = __page_frag_cache_refill(nc, gfp_mask);\n\t\tif (!page)\n\t\t\treturn NULL;\n\n#if (PAGE_SIZE < PAGE_FRAG_CACHE_MAX_SIZE)\n\t\t \n\t\tsize = nc->size;\n#endif\n\t\t \n\t\tpage_ref_add(page, PAGE_FRAG_CACHE_MAX_SIZE);\n\n\t\t \n\t\tnc->pfmemalloc = page_is_pfmemalloc(page);\n\t\tnc->pagecnt_bias = PAGE_FRAG_CACHE_MAX_SIZE + 1;\n\t\tnc->offset = size;\n\t}\n\n\toffset = nc->offset - fragsz;\n\tif (unlikely(offset < 0)) {\n\t\tpage = virt_to_page(nc->va);\n\n\t\tif (!page_ref_sub_and_test(page, nc->pagecnt_bias))\n\t\t\tgoto refill;\n\n\t\tif (unlikely(nc->pfmemalloc)) {\n\t\t\tfree_the_page(page, compound_order(page));\n\t\t\tgoto refill;\n\t\t}\n\n#if (PAGE_SIZE < PAGE_FRAG_CACHE_MAX_SIZE)\n\t\t \n\t\tsize = nc->size;\n#endif\n\t\t \n\t\tset_page_count(page, PAGE_FRAG_CACHE_MAX_SIZE + 1);\n\n\t\t \n\t\tnc->pagecnt_bias = PAGE_FRAG_CACHE_MAX_SIZE + 1;\n\t\toffset = size - fragsz;\n\t\tif (unlikely(offset < 0)) {\n\t\t\t \n\t\t\treturn NULL;\n\t\t}\n\t}\n\n\tnc->pagecnt_bias--;\n\toffset &= align_mask;\n\tnc->offset = offset;\n\n\treturn nc->va + offset;\n}\nEXPORT_SYMBOL(page_frag_alloc_align);\n\n \nvoid page_frag_free(void *addr)\n{\n\tstruct page *page = virt_to_head_page(addr);\n\n\tif (unlikely(put_page_testzero(page)))\n\t\tfree_the_page(page, compound_order(page));\n}\nEXPORT_SYMBOL(page_frag_free);\n\nstatic void *make_alloc_exact(unsigned long addr, unsigned int order,\n\t\tsize_t size)\n{\n\tif (addr) {\n\t\tunsigned long nr = DIV_ROUND_UP(size, PAGE_SIZE);\n\t\tstruct page *page = virt_to_page((void *)addr);\n\t\tstruct page *last = page + nr;\n\n\t\tsplit_page_owner(page, 1 << order);\n\t\tsplit_page_memcg(page, 1 << order);\n\t\twhile (page < --last)\n\t\t\tset_page_refcounted(last);\n\n\t\tlast = page + (1UL << order);\n\t\tfor (page += nr; page < last; page++)\n\t\t\t__free_pages_ok(page, 0, FPI_TO_TAIL);\n\t}\n\treturn (void *)addr;\n}\n\n \nvoid *alloc_pages_exact(size_t size, gfp_t gfp_mask)\n{\n\tunsigned int order = get_order(size);\n\tunsigned long addr;\n\n\tif (WARN_ON_ONCE(gfp_mask & (__GFP_COMP | __GFP_HIGHMEM)))\n\t\tgfp_mask &= ~(__GFP_COMP | __GFP_HIGHMEM);\n\n\taddr = __get_free_pages(gfp_mask, order);\n\treturn make_alloc_exact(addr, order, size);\n}\nEXPORT_SYMBOL(alloc_pages_exact);\n\n \nvoid * __meminit alloc_pages_exact_nid(int nid, size_t size, gfp_t gfp_mask)\n{\n\tunsigned int order = get_order(size);\n\tstruct page *p;\n\n\tif (WARN_ON_ONCE(gfp_mask & (__GFP_COMP | __GFP_HIGHMEM)))\n\t\tgfp_mask &= ~(__GFP_COMP | __GFP_HIGHMEM);\n\n\tp = alloc_pages_node(nid, gfp_mask, order);\n\tif (!p)\n\t\treturn NULL;\n\treturn make_alloc_exact((unsigned long)page_address(p), order, size);\n}\n\n \nvoid free_pages_exact(void *virt, size_t size)\n{\n\tunsigned long addr = (unsigned long)virt;\n\tunsigned long end = addr + PAGE_ALIGN(size);\n\n\twhile (addr < end) {\n\t\tfree_page(addr);\n\t\taddr += PAGE_SIZE;\n\t}\n}\nEXPORT_SYMBOL(free_pages_exact);\n\n \nstatic unsigned long nr_free_zone_pages(int offset)\n{\n\tstruct zoneref *z;\n\tstruct zone *zone;\n\n\t \n\tunsigned long sum = 0;\n\n\tstruct zonelist *zonelist = node_zonelist(numa_node_id(), GFP_KERNEL);\n\n\tfor_each_zone_zonelist(zone, z, zonelist, offset) {\n\t\tunsigned long size = zone_managed_pages(zone);\n\t\tunsigned long high = high_wmark_pages(zone);\n\t\tif (size > high)\n\t\t\tsum += size - high;\n\t}\n\n\treturn sum;\n}\n\n \nunsigned long nr_free_buffer_pages(void)\n{\n\treturn nr_free_zone_pages(gfp_zone(GFP_USER));\n}\nEXPORT_SYMBOL_GPL(nr_free_buffer_pages);\n\nstatic void zoneref_set_zone(struct zone *zone, struct zoneref *zoneref)\n{\n\tzoneref->zone = zone;\n\tzoneref->zone_idx = zone_idx(zone);\n}\n\n \nstatic int build_zonerefs_node(pg_data_t *pgdat, struct zoneref *zonerefs)\n{\n\tstruct zone *zone;\n\tenum zone_type zone_type = MAX_NR_ZONES;\n\tint nr_zones = 0;\n\n\tdo {\n\t\tzone_type--;\n\t\tzone = pgdat->node_zones + zone_type;\n\t\tif (populated_zone(zone)) {\n\t\t\tzoneref_set_zone(zone, &zonerefs[nr_zones++]);\n\t\t\tcheck_highest_zone(zone_type);\n\t\t}\n\t} while (zone_type);\n\n\treturn nr_zones;\n}\n\n#ifdef CONFIG_NUMA\n\nstatic int __parse_numa_zonelist_order(char *s)\n{\n\t \n\tif (!(*s == 'd' || *s == 'D' || *s == 'n' || *s == 'N')) {\n\t\tpr_warn(\"Ignoring unsupported numa_zonelist_order value:  %s\\n\", s);\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nstatic char numa_zonelist_order[] = \"Node\";\n#define NUMA_ZONELIST_ORDER_LEN\t16\n \nstatic int numa_zonelist_order_handler(struct ctl_table *table, int write,\n\t\tvoid *buffer, size_t *length, loff_t *ppos)\n{\n\tif (write)\n\t\treturn __parse_numa_zonelist_order(buffer);\n\treturn proc_dostring(table, write, buffer, length, ppos);\n}\n\nstatic int node_load[MAX_NUMNODES];\n\n \nint find_next_best_node(int node, nodemask_t *used_node_mask)\n{\n\tint n, val;\n\tint min_val = INT_MAX;\n\tint best_node = NUMA_NO_NODE;\n\n\t \n\tif (!node_isset(node, *used_node_mask)) {\n\t\tnode_set(node, *used_node_mask);\n\t\treturn node;\n\t}\n\n\tfor_each_node_state(n, N_MEMORY) {\n\n\t\t \n\t\tif (node_isset(n, *used_node_mask))\n\t\t\tcontinue;\n\n\t\t \n\t\tval = node_distance(node, n);\n\n\t\t \n\t\tval += (n < node);\n\n\t\t \n\t\tif (!cpumask_empty(cpumask_of_node(n)))\n\t\t\tval += PENALTY_FOR_NODE_WITH_CPUS;\n\n\t\t \n\t\tval *= MAX_NUMNODES;\n\t\tval += node_load[n];\n\n\t\tif (val < min_val) {\n\t\t\tmin_val = val;\n\t\t\tbest_node = n;\n\t\t}\n\t}\n\n\tif (best_node >= 0)\n\t\tnode_set(best_node, *used_node_mask);\n\n\treturn best_node;\n}\n\n\n \nstatic void build_zonelists_in_node_order(pg_data_t *pgdat, int *node_order,\n\t\tunsigned nr_nodes)\n{\n\tstruct zoneref *zonerefs;\n\tint i;\n\n\tzonerefs = pgdat->node_zonelists[ZONELIST_FALLBACK]._zonerefs;\n\n\tfor (i = 0; i < nr_nodes; i++) {\n\t\tint nr_zones;\n\n\t\tpg_data_t *node = NODE_DATA(node_order[i]);\n\n\t\tnr_zones = build_zonerefs_node(node, zonerefs);\n\t\tzonerefs += nr_zones;\n\t}\n\tzonerefs->zone = NULL;\n\tzonerefs->zone_idx = 0;\n}\n\n \nstatic void build_thisnode_zonelists(pg_data_t *pgdat)\n{\n\tstruct zoneref *zonerefs;\n\tint nr_zones;\n\n\tzonerefs = pgdat->node_zonelists[ZONELIST_NOFALLBACK]._zonerefs;\n\tnr_zones = build_zonerefs_node(pgdat, zonerefs);\n\tzonerefs += nr_zones;\n\tzonerefs->zone = NULL;\n\tzonerefs->zone_idx = 0;\n}\n\n \n\nstatic void build_zonelists(pg_data_t *pgdat)\n{\n\tstatic int node_order[MAX_NUMNODES];\n\tint node, nr_nodes = 0;\n\tnodemask_t used_mask = NODE_MASK_NONE;\n\tint local_node, prev_node;\n\n\t \n\tlocal_node = pgdat->node_id;\n\tprev_node = local_node;\n\n\tmemset(node_order, 0, sizeof(node_order));\n\twhile ((node = find_next_best_node(local_node, &used_mask)) >= 0) {\n\t\t \n\t\tif (node_distance(local_node, node) !=\n\t\t    node_distance(local_node, prev_node))\n\t\t\tnode_load[node] += 1;\n\n\t\tnode_order[nr_nodes++] = node;\n\t\tprev_node = node;\n\t}\n\n\tbuild_zonelists_in_node_order(pgdat, node_order, nr_nodes);\n\tbuild_thisnode_zonelists(pgdat);\n\tpr_info(\"Fallback order for Node %d: \", local_node);\n\tfor (node = 0; node < nr_nodes; node++)\n\t\tpr_cont(\"%d \", node_order[node]);\n\tpr_cont(\"\\n\");\n}\n\n#ifdef CONFIG_HAVE_MEMORYLESS_NODES\n \nint local_memory_node(int node)\n{\n\tstruct zoneref *z;\n\n\tz = first_zones_zonelist(node_zonelist(node, GFP_KERNEL),\n\t\t\t\t   gfp_zone(GFP_KERNEL),\n\t\t\t\t   NULL);\n\treturn zone_to_nid(z->zone);\n}\n#endif\n\nstatic void setup_min_unmapped_ratio(void);\nstatic void setup_min_slab_ratio(void);\n#else\t \n\nstatic void build_zonelists(pg_data_t *pgdat)\n{\n\tint node, local_node;\n\tstruct zoneref *zonerefs;\n\tint nr_zones;\n\n\tlocal_node = pgdat->node_id;\n\n\tzonerefs = pgdat->node_zonelists[ZONELIST_FALLBACK]._zonerefs;\n\tnr_zones = build_zonerefs_node(pgdat, zonerefs);\n\tzonerefs += nr_zones;\n\n\t \n\tfor (node = local_node + 1; node < MAX_NUMNODES; node++) {\n\t\tif (!node_online(node))\n\t\t\tcontinue;\n\t\tnr_zones = build_zonerefs_node(NODE_DATA(node), zonerefs);\n\t\tzonerefs += nr_zones;\n\t}\n\tfor (node = 0; node < local_node; node++) {\n\t\tif (!node_online(node))\n\t\t\tcontinue;\n\t\tnr_zones = build_zonerefs_node(NODE_DATA(node), zonerefs);\n\t\tzonerefs += nr_zones;\n\t}\n\n\tzonerefs->zone = NULL;\n\tzonerefs->zone_idx = 0;\n}\n\n#endif\t \n\n \nstatic void per_cpu_pages_init(struct per_cpu_pages *pcp, struct per_cpu_zonestat *pzstats);\n \n#define BOOT_PAGESET_HIGH\t0\n#define BOOT_PAGESET_BATCH\t1\nstatic DEFINE_PER_CPU(struct per_cpu_pages, boot_pageset);\nstatic DEFINE_PER_CPU(struct per_cpu_zonestat, boot_zonestats);\n\nstatic void __build_all_zonelists(void *data)\n{\n\tint nid;\n\tint __maybe_unused cpu;\n\tpg_data_t *self = data;\n\tunsigned long flags;\n\n\t \n\twrite_seqlock_irqsave(&zonelist_update_seq, flags);\n\t \n\tprintk_deferred_enter();\n\n#ifdef CONFIG_NUMA\n\tmemset(node_load, 0, sizeof(node_load));\n#endif\n\n\t \n\tif (self && !node_online(self->node_id)) {\n\t\tbuild_zonelists(self);\n\t} else {\n\t\t \n\t\tfor_each_node(nid) {\n\t\t\tpg_data_t *pgdat = NODE_DATA(nid);\n\n\t\t\tbuild_zonelists(pgdat);\n\t\t}\n\n#ifdef CONFIG_HAVE_MEMORYLESS_NODES\n\t\t \n\t\tfor_each_online_cpu(cpu)\n\t\t\tset_cpu_numa_mem(cpu, local_memory_node(cpu_to_node(cpu)));\n#endif\n\t}\n\n\tprintk_deferred_exit();\n\twrite_sequnlock_irqrestore(&zonelist_update_seq, flags);\n}\n\nstatic noinline void __init\nbuild_all_zonelists_init(void)\n{\n\tint cpu;\n\n\t__build_all_zonelists(NULL);\n\n\t \n\tfor_each_possible_cpu(cpu)\n\t\tper_cpu_pages_init(&per_cpu(boot_pageset, cpu), &per_cpu(boot_zonestats, cpu));\n\n\tmminit_verify_zonelist();\n\tcpuset_init_current_mems_allowed();\n}\n\n \nvoid __ref build_all_zonelists(pg_data_t *pgdat)\n{\n\tunsigned long vm_total_pages;\n\n\tif (system_state == SYSTEM_BOOTING) {\n\t\tbuild_all_zonelists_init();\n\t} else {\n\t\t__build_all_zonelists(pgdat);\n\t\t \n\t}\n\t \n\tvm_total_pages = nr_free_zone_pages(gfp_zone(GFP_HIGHUSER_MOVABLE));\n\t \n\tif (vm_total_pages < (pageblock_nr_pages * MIGRATE_TYPES))\n\t\tpage_group_by_mobility_disabled = 1;\n\telse\n\t\tpage_group_by_mobility_disabled = 0;\n\n\tpr_info(\"Built %u zonelists, mobility grouping %s.  Total pages: %ld\\n\",\n\t\tnr_online_nodes,\n\t\tpage_group_by_mobility_disabled ? \"off\" : \"on\",\n\t\tvm_total_pages);\n#ifdef CONFIG_NUMA\n\tpr_info(\"Policy zone: %s\\n\", zone_names[policy_zone]);\n#endif\n}\n\nstatic int zone_batchsize(struct zone *zone)\n{\n#ifdef CONFIG_MMU\n\tint batch;\n\n\t \n\tbatch = min(zone_managed_pages(zone) >> 10, SZ_1M / PAGE_SIZE);\n\tbatch /= 4;\t\t \n\tif (batch < 1)\n\t\tbatch = 1;\n\n\t \n\tbatch = rounddown_pow_of_two(batch + batch/2) - 1;\n\n\treturn batch;\n\n#else\n\t \n\treturn 0;\n#endif\n}\n\nstatic int percpu_pagelist_high_fraction;\nstatic int zone_highsize(struct zone *zone, int batch, int cpu_online)\n{\n#ifdef CONFIG_MMU\n\tint high;\n\tint nr_split_cpus;\n\tunsigned long total_pages;\n\n\tif (!percpu_pagelist_high_fraction) {\n\t\t \n\t\ttotal_pages = low_wmark_pages(zone);\n\t} else {\n\t\t \n\t\ttotal_pages = zone_managed_pages(zone) / percpu_pagelist_high_fraction;\n\t}\n\n\t \n\tnr_split_cpus = cpumask_weight(cpumask_of_node(zone_to_nid(zone))) + cpu_online;\n\tif (!nr_split_cpus)\n\t\tnr_split_cpus = num_online_cpus();\n\thigh = total_pages / nr_split_cpus;\n\n\t \n\thigh = max(high, batch << 2);\n\n\treturn high;\n#else\n\treturn 0;\n#endif\n}\n\n \nstatic void pageset_update(struct per_cpu_pages *pcp, unsigned long high,\n\t\tunsigned long batch)\n{\n\tWRITE_ONCE(pcp->batch, batch);\n\tWRITE_ONCE(pcp->high, high);\n}\n\nstatic void per_cpu_pages_init(struct per_cpu_pages *pcp, struct per_cpu_zonestat *pzstats)\n{\n\tint pindex;\n\n\tmemset(pcp, 0, sizeof(*pcp));\n\tmemset(pzstats, 0, sizeof(*pzstats));\n\n\tspin_lock_init(&pcp->lock);\n\tfor (pindex = 0; pindex < NR_PCP_LISTS; pindex++)\n\t\tINIT_LIST_HEAD(&pcp->lists[pindex]);\n\n\t \n\tpcp->high = BOOT_PAGESET_HIGH;\n\tpcp->batch = BOOT_PAGESET_BATCH;\n\tpcp->free_factor = 0;\n}\n\nstatic void __zone_set_pageset_high_and_batch(struct zone *zone, unsigned long high,\n\t\tunsigned long batch)\n{\n\tstruct per_cpu_pages *pcp;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tpcp = per_cpu_ptr(zone->per_cpu_pageset, cpu);\n\t\tpageset_update(pcp, high, batch);\n\t}\n}\n\n \nstatic void zone_set_pageset_high_and_batch(struct zone *zone, int cpu_online)\n{\n\tint new_high, new_batch;\n\n\tnew_batch = max(1, zone_batchsize(zone));\n\tnew_high = zone_highsize(zone, new_batch, cpu_online);\n\n\tif (zone->pageset_high == new_high &&\n\t    zone->pageset_batch == new_batch)\n\t\treturn;\n\n\tzone->pageset_high = new_high;\n\tzone->pageset_batch = new_batch;\n\n\t__zone_set_pageset_high_and_batch(zone, new_high, new_batch);\n}\n\nvoid __meminit setup_zone_pageset(struct zone *zone)\n{\n\tint cpu;\n\n\t \n\tif (sizeof(struct per_cpu_zonestat) > 0)\n\t\tzone->per_cpu_zonestats = alloc_percpu(struct per_cpu_zonestat);\n\n\tzone->per_cpu_pageset = alloc_percpu(struct per_cpu_pages);\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct per_cpu_pages *pcp;\n\t\tstruct per_cpu_zonestat *pzstats;\n\n\t\tpcp = per_cpu_ptr(zone->per_cpu_pageset, cpu);\n\t\tpzstats = per_cpu_ptr(zone->per_cpu_zonestats, cpu);\n\t\tper_cpu_pages_init(pcp, pzstats);\n\t}\n\n\tzone_set_pageset_high_and_batch(zone, 0);\n}\n\n \nstatic void zone_pcp_update(struct zone *zone, int cpu_online)\n{\n\tmutex_lock(&pcp_batch_high_lock);\n\tzone_set_pageset_high_and_batch(zone, cpu_online);\n\tmutex_unlock(&pcp_batch_high_lock);\n}\n\n \nvoid __init setup_per_cpu_pageset(void)\n{\n\tstruct pglist_data *pgdat;\n\tstruct zone *zone;\n\tint __maybe_unused cpu;\n\n\tfor_each_populated_zone(zone)\n\t\tsetup_zone_pageset(zone);\n\n#ifdef CONFIG_NUMA\n\t \n\tfor_each_possible_cpu(cpu) {\n\t\tstruct per_cpu_zonestat *pzstats = &per_cpu(boot_zonestats, cpu);\n\t\tmemset(pzstats->vm_numa_event, 0,\n\t\t       sizeof(pzstats->vm_numa_event));\n\t}\n#endif\n\n\tfor_each_online_pgdat(pgdat)\n\t\tpgdat->per_cpu_nodestats =\n\t\t\talloc_percpu(struct per_cpu_nodestat);\n}\n\n__meminit void zone_pcp_init(struct zone *zone)\n{\n\t \n\tzone->per_cpu_pageset = &boot_pageset;\n\tzone->per_cpu_zonestats = &boot_zonestats;\n\tzone->pageset_high = BOOT_PAGESET_HIGH;\n\tzone->pageset_batch = BOOT_PAGESET_BATCH;\n\n\tif (populated_zone(zone))\n\t\tpr_debug(\"  %s zone: %lu pages, LIFO batch:%u\\n\", zone->name,\n\t\t\t zone->present_pages, zone_batchsize(zone));\n}\n\nvoid adjust_managed_page_count(struct page *page, long count)\n{\n\tatomic_long_add(count, &page_zone(page)->managed_pages);\n\ttotalram_pages_add(count);\n#ifdef CONFIG_HIGHMEM\n\tif (PageHighMem(page))\n\t\ttotalhigh_pages_add(count);\n#endif\n}\nEXPORT_SYMBOL(adjust_managed_page_count);\n\nunsigned long free_reserved_area(void *start, void *end, int poison, const char *s)\n{\n\tvoid *pos;\n\tunsigned long pages = 0;\n\n\tstart = (void *)PAGE_ALIGN((unsigned long)start);\n\tend = (void *)((unsigned long)end & PAGE_MASK);\n\tfor (pos = start; pos < end; pos += PAGE_SIZE, pages++) {\n\t\tstruct page *page = virt_to_page(pos);\n\t\tvoid *direct_map_addr;\n\n\t\t \n\t\tdirect_map_addr = page_address(page);\n\t\t \n\t\tdirect_map_addr = kasan_reset_tag(direct_map_addr);\n\t\tif ((unsigned int)poison <= 0xFF)\n\t\t\tmemset(direct_map_addr, poison, PAGE_SIZE);\n\n\t\tfree_reserved_page(page);\n\t}\n\n\tif (pages && s)\n\t\tpr_info(\"Freeing %s memory: %ldK\\n\", s, K(pages));\n\n\treturn pages;\n}\n\nstatic int page_alloc_cpu_dead(unsigned int cpu)\n{\n\tstruct zone *zone;\n\n\tlru_add_drain_cpu(cpu);\n\tmlock_drain_remote(cpu);\n\tdrain_pages(cpu);\n\n\t \n\tvm_events_fold_cpu(cpu);\n\n\t \n\tcpu_vm_stats_fold(cpu);\n\n\tfor_each_populated_zone(zone)\n\t\tzone_pcp_update(zone, 0);\n\n\treturn 0;\n}\n\nstatic int page_alloc_cpu_online(unsigned int cpu)\n{\n\tstruct zone *zone;\n\n\tfor_each_populated_zone(zone)\n\t\tzone_pcp_update(zone, 1);\n\treturn 0;\n}\n\nvoid __init page_alloc_init_cpuhp(void)\n{\n\tint ret;\n\n\tret = cpuhp_setup_state_nocalls(CPUHP_PAGE_ALLOC,\n\t\t\t\t\t\"mm/page_alloc:pcp\",\n\t\t\t\t\tpage_alloc_cpu_online,\n\t\t\t\t\tpage_alloc_cpu_dead);\n\tWARN_ON(ret < 0);\n}\n\n \nstatic void calculate_totalreserve_pages(void)\n{\n\tstruct pglist_data *pgdat;\n\tunsigned long reserve_pages = 0;\n\tenum zone_type i, j;\n\n\tfor_each_online_pgdat(pgdat) {\n\n\t\tpgdat->totalreserve_pages = 0;\n\n\t\tfor (i = 0; i < MAX_NR_ZONES; i++) {\n\t\t\tstruct zone *zone = pgdat->node_zones + i;\n\t\t\tlong max = 0;\n\t\t\tunsigned long managed_pages = zone_managed_pages(zone);\n\n\t\t\t \n\t\t\tfor (j = i; j < MAX_NR_ZONES; j++) {\n\t\t\t\tif (zone->lowmem_reserve[j] > max)\n\t\t\t\t\tmax = zone->lowmem_reserve[j];\n\t\t\t}\n\n\t\t\t \n\t\t\tmax += high_wmark_pages(zone);\n\n\t\t\tif (max > managed_pages)\n\t\t\t\tmax = managed_pages;\n\n\t\t\tpgdat->totalreserve_pages += max;\n\n\t\t\treserve_pages += max;\n\t\t}\n\t}\n\ttotalreserve_pages = reserve_pages;\n}\n\n \nstatic void setup_per_zone_lowmem_reserve(void)\n{\n\tstruct pglist_data *pgdat;\n\tenum zone_type i, j;\n\n\tfor_each_online_pgdat(pgdat) {\n\t\tfor (i = 0; i < MAX_NR_ZONES - 1; i++) {\n\t\t\tstruct zone *zone = &pgdat->node_zones[i];\n\t\t\tint ratio = sysctl_lowmem_reserve_ratio[i];\n\t\t\tbool clear = !ratio || !zone_managed_pages(zone);\n\t\t\tunsigned long managed_pages = 0;\n\n\t\t\tfor (j = i + 1; j < MAX_NR_ZONES; j++) {\n\t\t\t\tstruct zone *upper_zone = &pgdat->node_zones[j];\n\n\t\t\t\tmanaged_pages += zone_managed_pages(upper_zone);\n\n\t\t\t\tif (clear)\n\t\t\t\t\tzone->lowmem_reserve[j] = 0;\n\t\t\t\telse\n\t\t\t\t\tzone->lowmem_reserve[j] = managed_pages / ratio;\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tcalculate_totalreserve_pages();\n}\n\nstatic void __setup_per_zone_wmarks(void)\n{\n\tunsigned long pages_min = min_free_kbytes >> (PAGE_SHIFT - 10);\n\tunsigned long lowmem_pages = 0;\n\tstruct zone *zone;\n\tunsigned long flags;\n\n\t \n\tfor_each_zone(zone) {\n\t\tif (!is_highmem(zone) && zone_idx(zone) != ZONE_MOVABLE)\n\t\t\tlowmem_pages += zone_managed_pages(zone);\n\t}\n\n\tfor_each_zone(zone) {\n\t\tu64 tmp;\n\n\t\tspin_lock_irqsave(&zone->lock, flags);\n\t\ttmp = (u64)pages_min * zone_managed_pages(zone);\n\t\tdo_div(tmp, lowmem_pages);\n\t\tif (is_highmem(zone) || zone_idx(zone) == ZONE_MOVABLE) {\n\t\t\t \n\t\t\tunsigned long min_pages;\n\n\t\t\tmin_pages = zone_managed_pages(zone) / 1024;\n\t\t\tmin_pages = clamp(min_pages, SWAP_CLUSTER_MAX, 128UL);\n\t\t\tzone->_watermark[WMARK_MIN] = min_pages;\n\t\t} else {\n\t\t\t \n\t\t\tzone->_watermark[WMARK_MIN] = tmp;\n\t\t}\n\n\t\t \n\t\ttmp = max_t(u64, tmp >> 2,\n\t\t\t    mult_frac(zone_managed_pages(zone),\n\t\t\t\t      watermark_scale_factor, 10000));\n\n\t\tzone->watermark_boost = 0;\n\t\tzone->_watermark[WMARK_LOW]  = min_wmark_pages(zone) + tmp;\n\t\tzone->_watermark[WMARK_HIGH] = low_wmark_pages(zone) + tmp;\n\t\tzone->_watermark[WMARK_PROMO] = high_wmark_pages(zone) + tmp;\n\n\t\tspin_unlock_irqrestore(&zone->lock, flags);\n\t}\n\n\t \n\tcalculate_totalreserve_pages();\n}\n\n \nvoid setup_per_zone_wmarks(void)\n{\n\tstruct zone *zone;\n\tstatic DEFINE_SPINLOCK(lock);\n\n\tspin_lock(&lock);\n\t__setup_per_zone_wmarks();\n\tspin_unlock(&lock);\n\n\t \n\tfor_each_zone(zone)\n\t\tzone_pcp_update(zone, 0);\n}\n\n \nvoid calculate_min_free_kbytes(void)\n{\n\tunsigned long lowmem_kbytes;\n\tint new_min_free_kbytes;\n\n\tlowmem_kbytes = nr_free_buffer_pages() * (PAGE_SIZE >> 10);\n\tnew_min_free_kbytes = int_sqrt(lowmem_kbytes * 16);\n\n\tif (new_min_free_kbytes > user_min_free_kbytes)\n\t\tmin_free_kbytes = clamp(new_min_free_kbytes, 128, 262144);\n\telse\n\t\tpr_warn(\"min_free_kbytes is not updated to %d because user defined value %d is preferred\\n\",\n\t\t\t\tnew_min_free_kbytes, user_min_free_kbytes);\n\n}\n\nint __meminit init_per_zone_wmark_min(void)\n{\n\tcalculate_min_free_kbytes();\n\tsetup_per_zone_wmarks();\n\trefresh_zone_stat_thresholds();\n\tsetup_per_zone_lowmem_reserve();\n\n#ifdef CONFIG_NUMA\n\tsetup_min_unmapped_ratio();\n\tsetup_min_slab_ratio();\n#endif\n\n\tkhugepaged_min_free_kbytes_update();\n\n\treturn 0;\n}\npostcore_initcall(init_per_zone_wmark_min)\n\n \nstatic int min_free_kbytes_sysctl_handler(struct ctl_table *table, int write,\n\t\tvoid *buffer, size_t *length, loff_t *ppos)\n{\n\tint rc;\n\n\trc = proc_dointvec_minmax(table, write, buffer, length, ppos);\n\tif (rc)\n\t\treturn rc;\n\n\tif (write) {\n\t\tuser_min_free_kbytes = min_free_kbytes;\n\t\tsetup_per_zone_wmarks();\n\t}\n\treturn 0;\n}\n\nstatic int watermark_scale_factor_sysctl_handler(struct ctl_table *table, int write,\n\t\tvoid *buffer, size_t *length, loff_t *ppos)\n{\n\tint rc;\n\n\trc = proc_dointvec_minmax(table, write, buffer, length, ppos);\n\tif (rc)\n\t\treturn rc;\n\n\tif (write)\n\t\tsetup_per_zone_wmarks();\n\n\treturn 0;\n}\n\n#ifdef CONFIG_NUMA\nstatic void setup_min_unmapped_ratio(void)\n{\n\tpg_data_t *pgdat;\n\tstruct zone *zone;\n\n\tfor_each_online_pgdat(pgdat)\n\t\tpgdat->min_unmapped_pages = 0;\n\n\tfor_each_zone(zone)\n\t\tzone->zone_pgdat->min_unmapped_pages += (zone_managed_pages(zone) *\n\t\t\t\t\t\t         sysctl_min_unmapped_ratio) / 100;\n}\n\n\nstatic int sysctl_min_unmapped_ratio_sysctl_handler(struct ctl_table *table, int write,\n\t\tvoid *buffer, size_t *length, loff_t *ppos)\n{\n\tint rc;\n\n\trc = proc_dointvec_minmax(table, write, buffer, length, ppos);\n\tif (rc)\n\t\treturn rc;\n\n\tsetup_min_unmapped_ratio();\n\n\treturn 0;\n}\n\nstatic void setup_min_slab_ratio(void)\n{\n\tpg_data_t *pgdat;\n\tstruct zone *zone;\n\n\tfor_each_online_pgdat(pgdat)\n\t\tpgdat->min_slab_pages = 0;\n\n\tfor_each_zone(zone)\n\t\tzone->zone_pgdat->min_slab_pages += (zone_managed_pages(zone) *\n\t\t\t\t\t\t     sysctl_min_slab_ratio) / 100;\n}\n\nstatic int sysctl_min_slab_ratio_sysctl_handler(struct ctl_table *table, int write,\n\t\tvoid *buffer, size_t *length, loff_t *ppos)\n{\n\tint rc;\n\n\trc = proc_dointvec_minmax(table, write, buffer, length, ppos);\n\tif (rc)\n\t\treturn rc;\n\n\tsetup_min_slab_ratio();\n\n\treturn 0;\n}\n#endif\n\n \nstatic int lowmem_reserve_ratio_sysctl_handler(struct ctl_table *table,\n\t\tint write, void *buffer, size_t *length, loff_t *ppos)\n{\n\tint i;\n\n\tproc_dointvec_minmax(table, write, buffer, length, ppos);\n\n\tfor (i = 0; i < MAX_NR_ZONES; i++) {\n\t\tif (sysctl_lowmem_reserve_ratio[i] < 1)\n\t\t\tsysctl_lowmem_reserve_ratio[i] = 0;\n\t}\n\n\tsetup_per_zone_lowmem_reserve();\n\treturn 0;\n}\n\n \nstatic int percpu_pagelist_high_fraction_sysctl_handler(struct ctl_table *table,\n\t\tint write, void *buffer, size_t *length, loff_t *ppos)\n{\n\tstruct zone *zone;\n\tint old_percpu_pagelist_high_fraction;\n\tint ret;\n\n\tmutex_lock(&pcp_batch_high_lock);\n\told_percpu_pagelist_high_fraction = percpu_pagelist_high_fraction;\n\n\tret = proc_dointvec_minmax(table, write, buffer, length, ppos);\n\tif (!write || ret < 0)\n\t\tgoto out;\n\n\t \n\tif (percpu_pagelist_high_fraction &&\n\t    percpu_pagelist_high_fraction < MIN_PERCPU_PAGELIST_HIGH_FRACTION) {\n\t\tpercpu_pagelist_high_fraction = old_percpu_pagelist_high_fraction;\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t \n\tif (percpu_pagelist_high_fraction == old_percpu_pagelist_high_fraction)\n\t\tgoto out;\n\n\tfor_each_populated_zone(zone)\n\t\tzone_set_pageset_high_and_batch(zone, 0);\nout:\n\tmutex_unlock(&pcp_batch_high_lock);\n\treturn ret;\n}\n\nstatic struct ctl_table page_alloc_sysctl_table[] = {\n\t{\n\t\t.procname\t= \"min_free_kbytes\",\n\t\t.data\t\t= &min_free_kbytes,\n\t\t.maxlen\t\t= sizeof(min_free_kbytes),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= min_free_kbytes_sysctl_handler,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t},\n\t{\n\t\t.procname\t= \"watermark_boost_factor\",\n\t\t.data\t\t= &watermark_boost_factor,\n\t\t.maxlen\t\t= sizeof(watermark_boost_factor),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t},\n\t{\n\t\t.procname\t= \"watermark_scale_factor\",\n\t\t.data\t\t= &watermark_scale_factor,\n\t\t.maxlen\t\t= sizeof(watermark_scale_factor),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= watermark_scale_factor_sysctl_handler,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_THREE_THOUSAND,\n\t},\n\t{\n\t\t.procname\t= \"percpu_pagelist_high_fraction\",\n\t\t.data\t\t= &percpu_pagelist_high_fraction,\n\t\t.maxlen\t\t= sizeof(percpu_pagelist_high_fraction),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= percpu_pagelist_high_fraction_sysctl_handler,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t},\n\t{\n\t\t.procname\t= \"lowmem_reserve_ratio\",\n\t\t.data\t\t= &sysctl_lowmem_reserve_ratio,\n\t\t.maxlen\t\t= sizeof(sysctl_lowmem_reserve_ratio),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= lowmem_reserve_ratio_sysctl_handler,\n\t},\n#ifdef CONFIG_NUMA\n\t{\n\t\t.procname\t= \"numa_zonelist_order\",\n\t\t.data\t\t= &numa_zonelist_order,\n\t\t.maxlen\t\t= NUMA_ZONELIST_ORDER_LEN,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= numa_zonelist_order_handler,\n\t},\n\t{\n\t\t.procname\t= \"min_unmapped_ratio\",\n\t\t.data\t\t= &sysctl_min_unmapped_ratio,\n\t\t.maxlen\t\t= sizeof(sysctl_min_unmapped_ratio),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= sysctl_min_unmapped_ratio_sysctl_handler,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_ONE_HUNDRED,\n\t},\n\t{\n\t\t.procname\t= \"min_slab_ratio\",\n\t\t.data\t\t= &sysctl_min_slab_ratio,\n\t\t.maxlen\t\t= sizeof(sysctl_min_slab_ratio),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= sysctl_min_slab_ratio_sysctl_handler,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_ONE_HUNDRED,\n\t},\n#endif\n\t{}\n};\n\nvoid __init page_alloc_sysctl_init(void)\n{\n\tregister_sysctl_init(\"vm\", page_alloc_sysctl_table);\n}\n\n#ifdef CONFIG_CONTIG_ALLOC\n \nstatic void alloc_contig_dump_pages(struct list_head *page_list)\n{\n\tDEFINE_DYNAMIC_DEBUG_METADATA(descriptor, \"migrate failure\");\n\n\tif (DYNAMIC_DEBUG_BRANCH(descriptor)) {\n\t\tstruct page *page;\n\n\t\tdump_stack();\n\t\tlist_for_each_entry(page, page_list, lru)\n\t\t\tdump_page(page, \"migration failure\");\n\t}\n}\n\n \nint __alloc_contig_migrate_range(struct compact_control *cc,\n\t\t\t\t\tunsigned long start, unsigned long end)\n{\n\t \n\tunsigned int nr_reclaimed;\n\tunsigned long pfn = start;\n\tunsigned int tries = 0;\n\tint ret = 0;\n\tstruct migration_target_control mtc = {\n\t\t.nid = zone_to_nid(cc->zone),\n\t\t.gfp_mask = GFP_USER | __GFP_MOVABLE | __GFP_RETRY_MAYFAIL,\n\t};\n\n\tlru_cache_disable();\n\n\twhile (pfn < end || !list_empty(&cc->migratepages)) {\n\t\tif (fatal_signal_pending(current)) {\n\t\t\tret = -EINTR;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (list_empty(&cc->migratepages)) {\n\t\t\tcc->nr_migratepages = 0;\n\t\t\tret = isolate_migratepages_range(cc, pfn, end);\n\t\t\tif (ret && ret != -EAGAIN)\n\t\t\t\tbreak;\n\t\t\tpfn = cc->migrate_pfn;\n\t\t\ttries = 0;\n\t\t} else if (++tries == 5) {\n\t\t\tret = -EBUSY;\n\t\t\tbreak;\n\t\t}\n\n\t\tnr_reclaimed = reclaim_clean_pages_from_list(cc->zone,\n\t\t\t\t\t\t\t&cc->migratepages);\n\t\tcc->nr_migratepages -= nr_reclaimed;\n\n\t\tret = migrate_pages(&cc->migratepages, alloc_migration_target,\n\t\t\tNULL, (unsigned long)&mtc, cc->mode, MR_CONTIG_RANGE, NULL);\n\n\t\t \n\t\tif (ret == -ENOMEM)\n\t\t\tbreak;\n\t}\n\n\tlru_cache_enable();\n\tif (ret < 0) {\n\t\tif (!(cc->gfp_mask & __GFP_NOWARN) && ret == -EBUSY)\n\t\t\talloc_contig_dump_pages(&cc->migratepages);\n\t\tputback_movable_pages(&cc->migratepages);\n\t\treturn ret;\n\t}\n\treturn 0;\n}\n\n \nint alloc_contig_range(unsigned long start, unsigned long end,\n\t\t       unsigned migratetype, gfp_t gfp_mask)\n{\n\tunsigned long outer_start, outer_end;\n\tint order;\n\tint ret = 0;\n\n\tstruct compact_control cc = {\n\t\t.nr_migratepages = 0,\n\t\t.order = -1,\n\t\t.zone = page_zone(pfn_to_page(start)),\n\t\t.mode = MIGRATE_SYNC,\n\t\t.ignore_skip_hint = true,\n\t\t.no_set_skip_hint = true,\n\t\t.gfp_mask = current_gfp_context(gfp_mask),\n\t\t.alloc_contig = true,\n\t};\n\tINIT_LIST_HEAD(&cc.migratepages);\n\n\t \n\n\tret = start_isolate_page_range(start, end, migratetype, 0, gfp_mask);\n\tif (ret)\n\t\tgoto done;\n\n\tdrain_all_pages(cc.zone);\n\n\t \n\tret = __alloc_contig_migrate_range(&cc, start, end);\n\tif (ret && ret != -EBUSY)\n\t\tgoto done;\n\tret = 0;\n\n\t \n\n\torder = 0;\n\touter_start = start;\n\twhile (!PageBuddy(pfn_to_page(outer_start))) {\n\t\tif (++order > MAX_ORDER) {\n\t\t\touter_start = start;\n\t\t\tbreak;\n\t\t}\n\t\touter_start &= ~0UL << order;\n\t}\n\n\tif (outer_start != start) {\n\t\torder = buddy_order(pfn_to_page(outer_start));\n\n\t\t \n\t\tif (outer_start + (1UL << order) <= start)\n\t\t\touter_start = start;\n\t}\n\n\t \n\tif (test_pages_isolated(outer_start, end, 0)) {\n\t\tret = -EBUSY;\n\t\tgoto done;\n\t}\n\n\t \n\touter_end = isolate_freepages_range(&cc, outer_start, end);\n\tif (!outer_end) {\n\t\tret = -EBUSY;\n\t\tgoto done;\n\t}\n\n\t \n\tif (start != outer_start)\n\t\tfree_contig_range(outer_start, start - outer_start);\n\tif (end != outer_end)\n\t\tfree_contig_range(end, outer_end - end);\n\ndone:\n\tundo_isolate_page_range(start, end, migratetype);\n\treturn ret;\n}\nEXPORT_SYMBOL(alloc_contig_range);\n\nstatic int __alloc_contig_pages(unsigned long start_pfn,\n\t\t\t\tunsigned long nr_pages, gfp_t gfp_mask)\n{\n\tunsigned long end_pfn = start_pfn + nr_pages;\n\n\treturn alloc_contig_range(start_pfn, end_pfn, MIGRATE_MOVABLE,\n\t\t\t\t  gfp_mask);\n}\n\nstatic bool pfn_range_valid_contig(struct zone *z, unsigned long start_pfn,\n\t\t\t\t   unsigned long nr_pages)\n{\n\tunsigned long i, end_pfn = start_pfn + nr_pages;\n\tstruct page *page;\n\n\tfor (i = start_pfn; i < end_pfn; i++) {\n\t\tpage = pfn_to_online_page(i);\n\t\tif (!page)\n\t\t\treturn false;\n\n\t\tif (page_zone(page) != z)\n\t\t\treturn false;\n\n\t\tif (PageReserved(page))\n\t\t\treturn false;\n\n\t\tif (PageHuge(page))\n\t\t\treturn false;\n\t}\n\treturn true;\n}\n\nstatic bool zone_spans_last_pfn(const struct zone *zone,\n\t\t\t\tunsigned long start_pfn, unsigned long nr_pages)\n{\n\tunsigned long last_pfn = start_pfn + nr_pages - 1;\n\n\treturn zone_spans_pfn(zone, last_pfn);\n}\n\n \nstruct page *alloc_contig_pages(unsigned long nr_pages, gfp_t gfp_mask,\n\t\t\t\tint nid, nodemask_t *nodemask)\n{\n\tunsigned long ret, pfn, flags;\n\tstruct zonelist *zonelist;\n\tstruct zone *zone;\n\tstruct zoneref *z;\n\n\tzonelist = node_zonelist(nid, gfp_mask);\n\tfor_each_zone_zonelist_nodemask(zone, z, zonelist,\n\t\t\t\t\tgfp_zone(gfp_mask), nodemask) {\n\t\tspin_lock_irqsave(&zone->lock, flags);\n\n\t\tpfn = ALIGN(zone->zone_start_pfn, nr_pages);\n\t\twhile (zone_spans_last_pfn(zone, pfn, nr_pages)) {\n\t\t\tif (pfn_range_valid_contig(zone, pfn, nr_pages)) {\n\t\t\t\t \n\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n\t\t\t\tret = __alloc_contig_pages(pfn, nr_pages,\n\t\t\t\t\t\t\tgfp_mask);\n\t\t\t\tif (!ret)\n\t\t\t\t\treturn pfn_to_page(pfn);\n\t\t\t\tspin_lock_irqsave(&zone->lock, flags);\n\t\t\t}\n\t\t\tpfn += nr_pages;\n\t\t}\n\t\tspin_unlock_irqrestore(&zone->lock, flags);\n\t}\n\treturn NULL;\n}\n#endif  \n\nvoid free_contig_range(unsigned long pfn, unsigned long nr_pages)\n{\n\tunsigned long count = 0;\n\n\tfor (; nr_pages--; pfn++) {\n\t\tstruct page *page = pfn_to_page(pfn);\n\n\t\tcount += page_count(page) != 1;\n\t\t__free_page(page);\n\t}\n\tWARN(count != 0, \"%lu pages are still in use!\\n\", count);\n}\nEXPORT_SYMBOL(free_contig_range);\n\n \nvoid zone_pcp_disable(struct zone *zone)\n{\n\tmutex_lock(&pcp_batch_high_lock);\n\t__zone_set_pageset_high_and_batch(zone, 0, 1);\n\t__drain_all_pages(zone, true);\n}\n\nvoid zone_pcp_enable(struct zone *zone)\n{\n\t__zone_set_pageset_high_and_batch(zone, zone->pageset_high, zone->pageset_batch);\n\tmutex_unlock(&pcp_batch_high_lock);\n}\n\nvoid zone_pcp_reset(struct zone *zone)\n{\n\tint cpu;\n\tstruct per_cpu_zonestat *pzstats;\n\n\tif (zone->per_cpu_pageset != &boot_pageset) {\n\t\tfor_each_online_cpu(cpu) {\n\t\t\tpzstats = per_cpu_ptr(zone->per_cpu_zonestats, cpu);\n\t\t\tdrain_zonestat(zone, pzstats);\n\t\t}\n\t\tfree_percpu(zone->per_cpu_pageset);\n\t\tzone->per_cpu_pageset = &boot_pageset;\n\t\tif (zone->per_cpu_zonestats != &boot_zonestats) {\n\t\t\tfree_percpu(zone->per_cpu_zonestats);\n\t\t\tzone->per_cpu_zonestats = &boot_zonestats;\n\t\t}\n\t}\n}\n\n#ifdef CONFIG_MEMORY_HOTREMOVE\n \nvoid __offline_isolated_pages(unsigned long start_pfn, unsigned long end_pfn)\n{\n\tunsigned long pfn = start_pfn;\n\tstruct page *page;\n\tstruct zone *zone;\n\tunsigned int order;\n\tunsigned long flags;\n\n\toffline_mem_sections(pfn, end_pfn);\n\tzone = page_zone(pfn_to_page(pfn));\n\tspin_lock_irqsave(&zone->lock, flags);\n\twhile (pfn < end_pfn) {\n\t\tpage = pfn_to_page(pfn);\n\t\t \n\t\tif (unlikely(!PageBuddy(page) && PageHWPoison(page))) {\n\t\t\tpfn++;\n\t\t\tcontinue;\n\t\t}\n\t\t \n\t\tif (PageOffline(page)) {\n\t\t\tBUG_ON(page_count(page));\n\t\t\tBUG_ON(PageBuddy(page));\n\t\t\tpfn++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tBUG_ON(page_count(page));\n\t\tBUG_ON(!PageBuddy(page));\n\t\torder = buddy_order(page);\n\t\tdel_page_from_free_list(page, zone, order);\n\t\tpfn += (1 << order);\n\t}\n\tspin_unlock_irqrestore(&zone->lock, flags);\n}\n#endif\n\n \nbool is_free_buddy_page(struct page *page)\n{\n\tunsigned long pfn = page_to_pfn(page);\n\tunsigned int order;\n\n\tfor (order = 0; order <= MAX_ORDER; order++) {\n\t\tstruct page *page_head = page - (pfn & ((1 << order) - 1));\n\n\t\tif (PageBuddy(page_head) &&\n\t\t    buddy_order_unsafe(page_head) >= order)\n\t\t\tbreak;\n\t}\n\n\treturn order <= MAX_ORDER;\n}\nEXPORT_SYMBOL(is_free_buddy_page);\n\n#ifdef CONFIG_MEMORY_FAILURE\n \nstatic void break_down_buddy_pages(struct zone *zone, struct page *page,\n\t\t\t\t   struct page *target, int low, int high,\n\t\t\t\t   int migratetype)\n{\n\tunsigned long size = 1 << high;\n\tstruct page *current_buddy, *next_page;\n\n\twhile (high > low) {\n\t\thigh--;\n\t\tsize >>= 1;\n\n\t\tif (target >= &page[size]) {\n\t\t\tnext_page = page + size;\n\t\t\tcurrent_buddy = page;\n\t\t} else {\n\t\t\tnext_page = page;\n\t\t\tcurrent_buddy = page + size;\n\t\t}\n\t\tpage = next_page;\n\n\t\tif (set_page_guard(zone, current_buddy, high, migratetype))\n\t\t\tcontinue;\n\n\t\tif (current_buddy != target) {\n\t\t\tadd_to_free_list(current_buddy, zone, high, migratetype);\n\t\t\tset_buddy_order(current_buddy, high);\n\t\t}\n\t}\n}\n\n \nbool take_page_off_buddy(struct page *page)\n{\n\tstruct zone *zone = page_zone(page);\n\tunsigned long pfn = page_to_pfn(page);\n\tunsigned long flags;\n\tunsigned int order;\n\tbool ret = false;\n\n\tspin_lock_irqsave(&zone->lock, flags);\n\tfor (order = 0; order <= MAX_ORDER; order++) {\n\t\tstruct page *page_head = page - (pfn & ((1 << order) - 1));\n\t\tint page_order = buddy_order(page_head);\n\n\t\tif (PageBuddy(page_head) && page_order >= order) {\n\t\t\tunsigned long pfn_head = page_to_pfn(page_head);\n\t\t\tint migratetype = get_pfnblock_migratetype(page_head,\n\t\t\t\t\t\t\t\t   pfn_head);\n\n\t\t\tdel_page_from_free_list(page_head, zone, page_order);\n\t\t\tbreak_down_buddy_pages(zone, page_head, page, 0,\n\t\t\t\t\t\tpage_order, migratetype);\n\t\t\tSetPageHWPoisonTakenOff(page);\n\t\t\tif (!is_migrate_isolate(migratetype))\n\t\t\t\t__mod_zone_freepage_state(zone, -1, migratetype);\n\t\t\tret = true;\n\t\t\tbreak;\n\t\t}\n\t\tif (page_count(page_head) > 0)\n\t\t\tbreak;\n\t}\n\tspin_unlock_irqrestore(&zone->lock, flags);\n\treturn ret;\n}\n\n \nbool put_page_back_buddy(struct page *page)\n{\n\tstruct zone *zone = page_zone(page);\n\tunsigned long pfn = page_to_pfn(page);\n\tunsigned long flags;\n\tint migratetype = get_pfnblock_migratetype(page, pfn);\n\tbool ret = false;\n\n\tspin_lock_irqsave(&zone->lock, flags);\n\tif (put_page_testzero(page)) {\n\t\tClearPageHWPoisonTakenOff(page);\n\t\t__free_one_page(page, pfn, zone, 0, migratetype, FPI_NONE);\n\t\tif (TestClearPageHWPoison(page)) {\n\t\t\tret = true;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&zone->lock, flags);\n\n\treturn ret;\n}\n#endif\n\n#ifdef CONFIG_ZONE_DMA\nbool has_managed_dma(void)\n{\n\tstruct pglist_data *pgdat;\n\n\tfor_each_online_pgdat(pgdat) {\n\t\tstruct zone *zone = &pgdat->node_zones[ZONE_DMA];\n\n\t\tif (managed_zone(zone))\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n#endif  \n\n#ifdef CONFIG_UNACCEPTED_MEMORY\n\n \nstatic DEFINE_STATIC_KEY_FALSE(zones_with_unaccepted_pages);\n\nstatic bool lazy_accept = true;\n\nstatic int __init accept_memory_parse(char *p)\n{\n\tif (!strcmp(p, \"lazy\")) {\n\t\tlazy_accept = true;\n\t\treturn 0;\n\t} else if (!strcmp(p, \"eager\")) {\n\t\tlazy_accept = false;\n\t\treturn 0;\n\t} else {\n\t\treturn -EINVAL;\n\t}\n}\nearly_param(\"accept_memory\", accept_memory_parse);\n\nstatic bool page_contains_unaccepted(struct page *page, unsigned int order)\n{\n\tphys_addr_t start = page_to_phys(page);\n\tphys_addr_t end = start + (PAGE_SIZE << order);\n\n\treturn range_contains_unaccepted_memory(start, end);\n}\n\nstatic void accept_page(struct page *page, unsigned int order)\n{\n\tphys_addr_t start = page_to_phys(page);\n\n\taccept_memory(start, start + (PAGE_SIZE << order));\n}\n\nstatic bool try_to_accept_memory_one(struct zone *zone)\n{\n\tunsigned long flags;\n\tstruct page *page;\n\tbool last;\n\n\tif (list_empty(&zone->unaccepted_pages))\n\t\treturn false;\n\n\tspin_lock_irqsave(&zone->lock, flags);\n\tpage = list_first_entry_or_null(&zone->unaccepted_pages,\n\t\t\t\t\tstruct page, lru);\n\tif (!page) {\n\t\tspin_unlock_irqrestore(&zone->lock, flags);\n\t\treturn false;\n\t}\n\n\tlist_del(&page->lru);\n\tlast = list_empty(&zone->unaccepted_pages);\n\n\t__mod_zone_freepage_state(zone, -MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n\t__mod_zone_page_state(zone, NR_UNACCEPTED, -MAX_ORDER_NR_PAGES);\n\tspin_unlock_irqrestore(&zone->lock, flags);\n\n\taccept_page(page, MAX_ORDER);\n\n\t__free_pages_ok(page, MAX_ORDER, FPI_TO_TAIL);\n\n\tif (last)\n\t\tstatic_branch_dec(&zones_with_unaccepted_pages);\n\n\treturn true;\n}\n\nstatic bool try_to_accept_memory(struct zone *zone, unsigned int order)\n{\n\tlong to_accept;\n\tint ret = false;\n\n\t \n\tto_accept = high_wmark_pages(zone) -\n\t\t    (zone_page_state(zone, NR_FREE_PAGES) -\n\t\t    __zone_watermark_unusable_free(zone, order, 0));\n\n\t \n\tdo {\n\t\tif (!try_to_accept_memory_one(zone))\n\t\t\tbreak;\n\t\tret = true;\n\t\tto_accept -= MAX_ORDER_NR_PAGES;\n\t} while (to_accept > 0);\n\n\treturn ret;\n}\n\nstatic inline bool has_unaccepted_memory(void)\n{\n\treturn static_branch_unlikely(&zones_with_unaccepted_pages);\n}\n\nstatic bool __free_unaccepted(struct page *page)\n{\n\tstruct zone *zone = page_zone(page);\n\tunsigned long flags;\n\tbool first = false;\n\n\tif (!lazy_accept)\n\t\treturn false;\n\n\tspin_lock_irqsave(&zone->lock, flags);\n\tfirst = list_empty(&zone->unaccepted_pages);\n\tlist_add_tail(&page->lru, &zone->unaccepted_pages);\n\t__mod_zone_freepage_state(zone, MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n\t__mod_zone_page_state(zone, NR_UNACCEPTED, MAX_ORDER_NR_PAGES);\n\tspin_unlock_irqrestore(&zone->lock, flags);\n\n\tif (first)\n\t\tstatic_branch_inc(&zones_with_unaccepted_pages);\n\n\treturn true;\n}\n\n#else\n\nstatic bool page_contains_unaccepted(struct page *page, unsigned int order)\n{\n\treturn false;\n}\n\nstatic void accept_page(struct page *page, unsigned int order)\n{\n}\n\nstatic bool try_to_accept_memory(struct zone *zone, unsigned int order)\n{\n\treturn false;\n}\n\nstatic inline bool has_unaccepted_memory(void)\n{\n\treturn false;\n}\n\nstatic bool __free_unaccepted(struct page *page)\n{\n\tBUILD_BUG();\n\treturn false;\n}\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}