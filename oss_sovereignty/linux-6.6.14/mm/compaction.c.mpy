{
  "module_name": "compaction.c",
  "hash_id": "51cd55691b66ef3fa506656030018f466440c8e54c9c0c271c7e5dab594a0d5b",
  "original_prompt": "Ingested from linux-6.6.14/mm/compaction.c",
  "human_readable_source": "\n \n#include <linux/cpu.h>\n#include <linux/swap.h>\n#include <linux/migrate.h>\n#include <linux/compaction.h>\n#include <linux/mm_inline.h>\n#include <linux/sched/signal.h>\n#include <linux/backing-dev.h>\n#include <linux/sysctl.h>\n#include <linux/sysfs.h>\n#include <linux/page-isolation.h>\n#include <linux/kasan.h>\n#include <linux/kthread.h>\n#include <linux/freezer.h>\n#include <linux/page_owner.h>\n#include <linux/psi.h>\n#include \"internal.h\"\n\n#ifdef CONFIG_COMPACTION\n \n#define HPAGE_FRAG_CHECK_INTERVAL_MSEC\t(500)\n\nstatic inline void count_compact_event(enum vm_event_item item)\n{\n\tcount_vm_event(item);\n}\n\nstatic inline void count_compact_events(enum vm_event_item item, long delta)\n{\n\tcount_vm_events(item, delta);\n}\n#else\n#define count_compact_event(item) do { } while (0)\n#define count_compact_events(item, delta) do { } while (0)\n#endif\n\n#if defined CONFIG_COMPACTION || defined CONFIG_CMA\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/compaction.h>\n\n#define block_start_pfn(pfn, order)\tround_down(pfn, 1UL << (order))\n#define block_end_pfn(pfn, order)\tALIGN((pfn) + 1, 1UL << (order))\n\n \n#if defined CONFIG_TRANSPARENT_HUGEPAGE\n#define COMPACTION_HPAGE_ORDER\tHPAGE_PMD_ORDER\n#elif defined CONFIG_HUGETLBFS\n#define COMPACTION_HPAGE_ORDER\tHUGETLB_PAGE_ORDER\n#else\n#define COMPACTION_HPAGE_ORDER\t(PMD_SHIFT - PAGE_SHIFT)\n#endif\n\nstatic unsigned long release_freepages(struct list_head *freelist)\n{\n\tstruct page *page, *next;\n\tunsigned long high_pfn = 0;\n\n\tlist_for_each_entry_safe(page, next, freelist, lru) {\n\t\tunsigned long pfn = page_to_pfn(page);\n\t\tlist_del(&page->lru);\n\t\t__free_page(page);\n\t\tif (pfn > high_pfn)\n\t\t\thigh_pfn = pfn;\n\t}\n\n\treturn high_pfn;\n}\n\nstatic void split_map_pages(struct list_head *list)\n{\n\tunsigned int i, order, nr_pages;\n\tstruct page *page, *next;\n\tLIST_HEAD(tmp_list);\n\n\tlist_for_each_entry_safe(page, next, list, lru) {\n\t\tlist_del(&page->lru);\n\n\t\torder = page_private(page);\n\t\tnr_pages = 1 << order;\n\n\t\tpost_alloc_hook(page, order, __GFP_MOVABLE);\n\t\tif (order)\n\t\t\tsplit_page(page, order);\n\n\t\tfor (i = 0; i < nr_pages; i++) {\n\t\t\tlist_add(&page->lru, &tmp_list);\n\t\t\tpage++;\n\t\t}\n\t}\n\n\tlist_splice(&tmp_list, list);\n}\n\n#ifdef CONFIG_COMPACTION\nbool PageMovable(struct page *page)\n{\n\tconst struct movable_operations *mops;\n\n\tVM_BUG_ON_PAGE(!PageLocked(page), page);\n\tif (!__PageMovable(page))\n\t\treturn false;\n\n\tmops = page_movable_ops(page);\n\tif (mops)\n\t\treturn true;\n\n\treturn false;\n}\n\nvoid __SetPageMovable(struct page *page, const struct movable_operations *mops)\n{\n\tVM_BUG_ON_PAGE(!PageLocked(page), page);\n\tVM_BUG_ON_PAGE((unsigned long)mops & PAGE_MAPPING_MOVABLE, page);\n\tpage->mapping = (void *)((unsigned long)mops | PAGE_MAPPING_MOVABLE);\n}\nEXPORT_SYMBOL(__SetPageMovable);\n\nvoid __ClearPageMovable(struct page *page)\n{\n\tVM_BUG_ON_PAGE(!PageMovable(page), page);\n\t \n\tpage->mapping = (void *)PAGE_MAPPING_MOVABLE;\n}\nEXPORT_SYMBOL(__ClearPageMovable);\n\n \n#define COMPACT_MAX_DEFER_SHIFT 6\n\n \nstatic void defer_compaction(struct zone *zone, int order)\n{\n\tzone->compact_considered = 0;\n\tzone->compact_defer_shift++;\n\n\tif (order < zone->compact_order_failed)\n\t\tzone->compact_order_failed = order;\n\n\tif (zone->compact_defer_shift > COMPACT_MAX_DEFER_SHIFT)\n\t\tzone->compact_defer_shift = COMPACT_MAX_DEFER_SHIFT;\n\n\ttrace_mm_compaction_defer_compaction(zone, order);\n}\n\n \nstatic bool compaction_deferred(struct zone *zone, int order)\n{\n\tunsigned long defer_limit = 1UL << zone->compact_defer_shift;\n\n\tif (order < zone->compact_order_failed)\n\t\treturn false;\n\n\t \n\tif (++zone->compact_considered >= defer_limit) {\n\t\tzone->compact_considered = defer_limit;\n\t\treturn false;\n\t}\n\n\ttrace_mm_compaction_deferred(zone, order);\n\n\treturn true;\n}\n\n \nvoid compaction_defer_reset(struct zone *zone, int order,\n\t\tbool alloc_success)\n{\n\tif (alloc_success) {\n\t\tzone->compact_considered = 0;\n\t\tzone->compact_defer_shift = 0;\n\t}\n\tif (order >= zone->compact_order_failed)\n\t\tzone->compact_order_failed = order + 1;\n\n\ttrace_mm_compaction_defer_reset(zone, order);\n}\n\n \nstatic bool compaction_restarting(struct zone *zone, int order)\n{\n\tif (order < zone->compact_order_failed)\n\t\treturn false;\n\n\treturn zone->compact_defer_shift == COMPACT_MAX_DEFER_SHIFT &&\n\t\tzone->compact_considered >= 1UL << zone->compact_defer_shift;\n}\n\n \nstatic inline bool isolation_suitable(struct compact_control *cc,\n\t\t\t\t\tstruct page *page)\n{\n\tif (cc->ignore_skip_hint)\n\t\treturn true;\n\n\treturn !get_pageblock_skip(page);\n}\n\nstatic void reset_cached_positions(struct zone *zone)\n{\n\tzone->compact_cached_migrate_pfn[0] = zone->zone_start_pfn;\n\tzone->compact_cached_migrate_pfn[1] = zone->zone_start_pfn;\n\tzone->compact_cached_free_pfn =\n\t\t\t\tpageblock_start_pfn(zone_end_pfn(zone) - 1);\n}\n\n#ifdef CONFIG_SPARSEMEM\n \nstatic unsigned long skip_offline_sections(unsigned long start_pfn)\n{\n\tunsigned long start_nr = pfn_to_section_nr(start_pfn);\n\n\tif (online_section_nr(start_nr))\n\t\treturn 0;\n\n\twhile (++start_nr <= __highest_present_section_nr) {\n\t\tif (online_section_nr(start_nr))\n\t\t\treturn section_nr_to_pfn(start_nr);\n\t}\n\n\treturn 0;\n}\n\n \nstatic unsigned long skip_offline_sections_reverse(unsigned long start_pfn)\n{\n\tunsigned long start_nr = pfn_to_section_nr(start_pfn);\n\n\tif (!start_nr || online_section_nr(start_nr))\n\t\treturn 0;\n\n\twhile (start_nr-- > 0) {\n\t\tif (online_section_nr(start_nr))\n\t\t\treturn section_nr_to_pfn(start_nr) + PAGES_PER_SECTION;\n\t}\n\n\treturn 0;\n}\n#else\nstatic unsigned long skip_offline_sections(unsigned long start_pfn)\n{\n\treturn 0;\n}\n\nstatic unsigned long skip_offline_sections_reverse(unsigned long start_pfn)\n{\n\treturn 0;\n}\n#endif\n\n \nstatic bool pageblock_skip_persistent(struct page *page)\n{\n\tif (!PageCompound(page))\n\t\treturn false;\n\n\tpage = compound_head(page);\n\n\tif (compound_order(page) >= pageblock_order)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic bool\n__reset_isolation_pfn(struct zone *zone, unsigned long pfn, bool check_source,\n\t\t\t\t\t\t\tbool check_target)\n{\n\tstruct page *page = pfn_to_online_page(pfn);\n\tstruct page *block_page;\n\tstruct page *end_page;\n\tunsigned long block_pfn;\n\n\tif (!page)\n\t\treturn false;\n\tif (zone != page_zone(page))\n\t\treturn false;\n\tif (pageblock_skip_persistent(page))\n\t\treturn false;\n\n\t \n\tif (check_source && check_target && !get_pageblock_skip(page))\n\t\treturn true;\n\n\t \n\tif (!check_source && check_target &&\n\t    get_pageblock_migratetype(page) != MIGRATE_MOVABLE)\n\t\treturn false;\n\n\t \n\tblock_pfn = pageblock_start_pfn(pfn);\n\tblock_pfn = max(block_pfn, zone->zone_start_pfn);\n\tblock_page = pfn_to_online_page(block_pfn);\n\tif (block_page) {\n\t\tpage = block_page;\n\t\tpfn = block_pfn;\n\t}\n\n\t \n\tblock_pfn = pageblock_end_pfn(pfn) - 1;\n\tblock_pfn = min(block_pfn, zone_end_pfn(zone) - 1);\n\tend_page = pfn_to_online_page(block_pfn);\n\tif (!end_page)\n\t\treturn false;\n\n\t \n\tdo {\n\t\tif (check_source && PageLRU(page)) {\n\t\t\tclear_pageblock_skip(page);\n\t\t\treturn true;\n\t\t}\n\n\t\tif (check_target && PageBuddy(page)) {\n\t\t\tclear_pageblock_skip(page);\n\t\t\treturn true;\n\t\t}\n\n\t\tpage += (1 << PAGE_ALLOC_COSTLY_ORDER);\n\t} while (page <= end_page);\n\n\treturn false;\n}\n\n \nstatic void __reset_isolation_suitable(struct zone *zone)\n{\n\tunsigned long migrate_pfn = zone->zone_start_pfn;\n\tunsigned long free_pfn = zone_end_pfn(zone) - 1;\n\tunsigned long reset_migrate = free_pfn;\n\tunsigned long reset_free = migrate_pfn;\n\tbool source_set = false;\n\tbool free_set = false;\n\n\tif (!zone->compact_blockskip_flush)\n\t\treturn;\n\n\tzone->compact_blockskip_flush = false;\n\n\t \n\tfor (; migrate_pfn < free_pfn; migrate_pfn += pageblock_nr_pages,\n\t\t\t\t\tfree_pfn -= pageblock_nr_pages) {\n\t\tcond_resched();\n\n\t\t \n\t\tif (__reset_isolation_pfn(zone, migrate_pfn, true, source_set) &&\n\t\t    migrate_pfn < reset_migrate) {\n\t\t\tsource_set = true;\n\t\t\treset_migrate = migrate_pfn;\n\t\t\tzone->compact_init_migrate_pfn = reset_migrate;\n\t\t\tzone->compact_cached_migrate_pfn[0] = reset_migrate;\n\t\t\tzone->compact_cached_migrate_pfn[1] = reset_migrate;\n\t\t}\n\n\t\t \n\t\tif (__reset_isolation_pfn(zone, free_pfn, free_set, true) &&\n\t\t    free_pfn > reset_free) {\n\t\t\tfree_set = true;\n\t\t\treset_free = free_pfn;\n\t\t\tzone->compact_init_free_pfn = reset_free;\n\t\t\tzone->compact_cached_free_pfn = reset_free;\n\t\t}\n\t}\n\n\t \n\tif (reset_migrate >= reset_free) {\n\t\tzone->compact_cached_migrate_pfn[0] = migrate_pfn;\n\t\tzone->compact_cached_migrate_pfn[1] = migrate_pfn;\n\t\tzone->compact_cached_free_pfn = free_pfn;\n\t}\n}\n\nvoid reset_isolation_suitable(pg_data_t *pgdat)\n{\n\tint zoneid;\n\n\tfor (zoneid = 0; zoneid < MAX_NR_ZONES; zoneid++) {\n\t\tstruct zone *zone = &pgdat->node_zones[zoneid];\n\t\tif (!populated_zone(zone))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (zone->compact_blockskip_flush)\n\t\t\t__reset_isolation_suitable(zone);\n\t}\n}\n\n \nstatic bool test_and_set_skip(struct compact_control *cc, struct page *page)\n{\n\tbool skip;\n\n\t \n\tif (cc->ignore_skip_hint)\n\t\treturn false;\n\n\tskip = get_pageblock_skip(page);\n\tif (!skip && !cc->no_set_skip_hint)\n\t\tset_pageblock_skip(page);\n\n\treturn skip;\n}\n\nstatic void update_cached_migrate(struct compact_control *cc, unsigned long pfn)\n{\n\tstruct zone *zone = cc->zone;\n\n\t \n\tif (cc->no_set_skip_hint)\n\t\treturn;\n\n\tpfn = pageblock_end_pfn(pfn);\n\n\t \n\tif (pfn > zone->compact_cached_migrate_pfn[0])\n\t\tzone->compact_cached_migrate_pfn[0] = pfn;\n\tif (cc->mode != MIGRATE_ASYNC &&\n\t    pfn > zone->compact_cached_migrate_pfn[1])\n\t\tzone->compact_cached_migrate_pfn[1] = pfn;\n}\n\n \nstatic void update_pageblock_skip(struct compact_control *cc,\n\t\t\tstruct page *page, unsigned long pfn)\n{\n\tstruct zone *zone = cc->zone;\n\n\tif (cc->no_set_skip_hint)\n\t\treturn;\n\n\tset_pageblock_skip(page);\n\n\tif (pfn < zone->compact_cached_free_pfn)\n\t\tzone->compact_cached_free_pfn = pfn;\n}\n#else\nstatic inline bool isolation_suitable(struct compact_control *cc,\n\t\t\t\t\tstruct page *page)\n{\n\treturn true;\n}\n\nstatic inline bool pageblock_skip_persistent(struct page *page)\n{\n\treturn false;\n}\n\nstatic inline void update_pageblock_skip(struct compact_control *cc,\n\t\t\tstruct page *page, unsigned long pfn)\n{\n}\n\nstatic void update_cached_migrate(struct compact_control *cc, unsigned long pfn)\n{\n}\n\nstatic bool test_and_set_skip(struct compact_control *cc, struct page *page)\n{\n\treturn false;\n}\n#endif  \n\n \nstatic bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n\t\t\t\t\t\tstruct compact_control *cc)\n\t__acquires(lock)\n{\n\t \n\tif (cc->mode == MIGRATE_ASYNC && !cc->contended) {\n\t\tif (spin_trylock_irqsave(lock, *flags))\n\t\t\treturn true;\n\n\t\tcc->contended = true;\n\t}\n\n\tspin_lock_irqsave(lock, *flags);\n\treturn true;\n}\n\n \nstatic bool compact_unlock_should_abort(spinlock_t *lock,\n\t\tunsigned long flags, bool *locked, struct compact_control *cc)\n{\n\tif (*locked) {\n\t\tspin_unlock_irqrestore(lock, flags);\n\t\t*locked = false;\n\t}\n\n\tif (fatal_signal_pending(current)) {\n\t\tcc->contended = true;\n\t\treturn true;\n\t}\n\n\tcond_resched();\n\n\treturn false;\n}\n\n \nstatic unsigned long isolate_freepages_block(struct compact_control *cc,\n\t\t\t\tunsigned long *start_pfn,\n\t\t\t\tunsigned long end_pfn,\n\t\t\t\tstruct list_head *freelist,\n\t\t\t\tunsigned int stride,\n\t\t\t\tbool strict)\n{\n\tint nr_scanned = 0, total_isolated = 0;\n\tstruct page *page;\n\tunsigned long flags = 0;\n\tbool locked = false;\n\tunsigned long blockpfn = *start_pfn;\n\tunsigned int order;\n\n\t \n\tif (strict)\n\t\tstride = 1;\n\n\tpage = pfn_to_page(blockpfn);\n\n\t \n\tfor (; blockpfn < end_pfn; blockpfn += stride, page += stride) {\n\t\tint isolated;\n\n\t\t \n\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX)\n\t\t    && compact_unlock_should_abort(&cc->zone->lock, flags,\n\t\t\t\t\t\t\t\t&locked, cc))\n\t\t\tbreak;\n\n\t\tnr_scanned++;\n\n\t\t \n\t\tif (PageCompound(page)) {\n\t\t\tconst unsigned int order = compound_order(page);\n\n\t\t\tif (likely(order <= MAX_ORDER)) {\n\t\t\t\tblockpfn += (1UL << order) - 1;\n\t\t\t\tpage += (1UL << order) - 1;\n\t\t\t\tnr_scanned += (1UL << order) - 1;\n\t\t\t}\n\t\t\tgoto isolate_fail;\n\t\t}\n\n\t\tif (!PageBuddy(page))\n\t\t\tgoto isolate_fail;\n\n\t\t \n\t\tif (!locked) {\n\t\t\tlocked = compact_lock_irqsave(&cc->zone->lock,\n\t\t\t\t\t\t\t\t&flags, cc);\n\n\t\t\t \n\t\t\tif (!PageBuddy(page))\n\t\t\t\tgoto isolate_fail;\n\t\t}\n\n\t\t \n\t\torder = buddy_order(page);\n\t\tisolated = __isolate_free_page(page, order);\n\t\tif (!isolated)\n\t\t\tbreak;\n\t\tset_page_private(page, order);\n\n\t\tnr_scanned += isolated - 1;\n\t\ttotal_isolated += isolated;\n\t\tcc->nr_freepages += isolated;\n\t\tlist_add_tail(&page->lru, freelist);\n\n\t\tif (!strict && cc->nr_migratepages <= cc->nr_freepages) {\n\t\t\tblockpfn += isolated;\n\t\t\tbreak;\n\t\t}\n\t\t \n\t\tblockpfn += isolated - 1;\n\t\tpage += isolated - 1;\n\t\tcontinue;\n\nisolate_fail:\n\t\tif (strict)\n\t\t\tbreak;\n\n\t}\n\n\tif (locked)\n\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n\n\t \n\tif (unlikely(blockpfn > end_pfn))\n\t\tblockpfn = end_pfn;\n\n\ttrace_mm_compaction_isolate_freepages(*start_pfn, blockpfn,\n\t\t\t\t\tnr_scanned, total_isolated);\n\n\t \n\t*start_pfn = blockpfn;\n\n\t \n\tif (strict && blockpfn < end_pfn)\n\t\ttotal_isolated = 0;\n\n\tcc->total_free_scanned += nr_scanned;\n\tif (total_isolated)\n\t\tcount_compact_events(COMPACTISOLATED, total_isolated);\n\treturn total_isolated;\n}\n\n \nunsigned long\nisolate_freepages_range(struct compact_control *cc,\n\t\t\tunsigned long start_pfn, unsigned long end_pfn)\n{\n\tunsigned long isolated, pfn, block_start_pfn, block_end_pfn;\n\tLIST_HEAD(freelist);\n\n\tpfn = start_pfn;\n\tblock_start_pfn = pageblock_start_pfn(pfn);\n\tif (block_start_pfn < cc->zone->zone_start_pfn)\n\t\tblock_start_pfn = cc->zone->zone_start_pfn;\n\tblock_end_pfn = pageblock_end_pfn(pfn);\n\n\tfor (; pfn < end_pfn; pfn += isolated,\n\t\t\t\tblock_start_pfn = block_end_pfn,\n\t\t\t\tblock_end_pfn += pageblock_nr_pages) {\n\t\t \n\t\tunsigned long isolate_start_pfn = pfn;\n\n\t\t \n\t\tif (pfn >= block_end_pfn) {\n\t\t\tblock_start_pfn = pageblock_start_pfn(pfn);\n\t\t\tblock_end_pfn = pageblock_end_pfn(pfn);\n\t\t}\n\n\t\tblock_end_pfn = min(block_end_pfn, end_pfn);\n\n\t\tif (!pageblock_pfn_to_page(block_start_pfn,\n\t\t\t\t\tblock_end_pfn, cc->zone))\n\t\t\tbreak;\n\n\t\tisolated = isolate_freepages_block(cc, &isolate_start_pfn,\n\t\t\t\t\tblock_end_pfn, &freelist, 0, true);\n\n\t\t \n\t\tif (!isolated)\n\t\t\tbreak;\n\n\t\t \n\t}\n\n\t \n\tsplit_map_pages(&freelist);\n\n\tif (pfn < end_pfn) {\n\t\t \n\t\trelease_freepages(&freelist);\n\t\treturn 0;\n\t}\n\n\t \n\treturn pfn;\n}\n\n \nstatic bool too_many_isolated(struct compact_control *cc)\n{\n\tpg_data_t *pgdat = cc->zone->zone_pgdat;\n\tbool too_many;\n\n\tunsigned long active, inactive, isolated;\n\n\tinactive = node_page_state(pgdat, NR_INACTIVE_FILE) +\n\t\t\tnode_page_state(pgdat, NR_INACTIVE_ANON);\n\tactive = node_page_state(pgdat, NR_ACTIVE_FILE) +\n\t\t\tnode_page_state(pgdat, NR_ACTIVE_ANON);\n\tisolated = node_page_state(pgdat, NR_ISOLATED_FILE) +\n\t\t\tnode_page_state(pgdat, NR_ISOLATED_ANON);\n\n\t \n\tif (cc->gfp_mask & __GFP_FS) {\n\t\tinactive >>= 3;\n\t\tactive >>= 3;\n\t}\n\n\ttoo_many = isolated > (inactive + active) / 2;\n\tif (!too_many)\n\t\twake_throttle_isolated(pgdat);\n\n\treturn too_many;\n}\n\n \nstatic int\nisolate_migratepages_block(struct compact_control *cc, unsigned long low_pfn,\n\t\t\tunsigned long end_pfn, isolate_mode_t mode)\n{\n\tpg_data_t *pgdat = cc->zone->zone_pgdat;\n\tunsigned long nr_scanned = 0, nr_isolated = 0;\n\tstruct lruvec *lruvec;\n\tunsigned long flags = 0;\n\tstruct lruvec *locked = NULL;\n\tstruct folio *folio = NULL;\n\tstruct page *page = NULL, *valid_page = NULL;\n\tstruct address_space *mapping;\n\tunsigned long start_pfn = low_pfn;\n\tbool skip_on_failure = false;\n\tunsigned long next_skip_pfn = 0;\n\tbool skip_updated = false;\n\tint ret = 0;\n\n\tcc->migrate_pfn = low_pfn;\n\n\t \n\twhile (unlikely(too_many_isolated(cc))) {\n\t\t \n\t\tif (cc->nr_migratepages)\n\t\t\treturn -EAGAIN;\n\n\t\t \n\t\tif (cc->mode == MIGRATE_ASYNC)\n\t\t\treturn -EAGAIN;\n\n\t\treclaim_throttle(pgdat, VMSCAN_THROTTLE_ISOLATED);\n\n\t\tif (fatal_signal_pending(current))\n\t\t\treturn -EINTR;\n\t}\n\n\tcond_resched();\n\n\tif (cc->direct_compaction && (cc->mode == MIGRATE_ASYNC)) {\n\t\tskip_on_failure = true;\n\t\tnext_skip_pfn = block_end_pfn(low_pfn, cc->order);\n\t}\n\n\t \n\tfor (; low_pfn < end_pfn; low_pfn++) {\n\n\t\tif (skip_on_failure && low_pfn >= next_skip_pfn) {\n\t\t\t \n\t\t\tif (nr_isolated)\n\t\t\t\tbreak;\n\n\t\t\t \n\t\t\tnext_skip_pfn = block_end_pfn(low_pfn, cc->order);\n\t\t}\n\n\t\t \n\t\tif (!(low_pfn % COMPACT_CLUSTER_MAX)) {\n\t\t\tif (locked) {\n\t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);\n\t\t\t\tlocked = NULL;\n\t\t\t}\n\n\t\t\tif (fatal_signal_pending(current)) {\n\t\t\t\tcc->contended = true;\n\t\t\t\tret = -EINTR;\n\n\t\t\t\tgoto fatal_pending;\n\t\t\t}\n\n\t\t\tcond_resched();\n\t\t}\n\n\t\tnr_scanned++;\n\n\t\tpage = pfn_to_page(low_pfn);\n\n\t\t \n\t\tif (!valid_page && (pageblock_aligned(low_pfn) ||\n\t\t\t\t    low_pfn == cc->zone->zone_start_pfn)) {\n\t\t\tif (!isolation_suitable(cc, page)) {\n\t\t\t\tlow_pfn = end_pfn;\n\t\t\t\tfolio = NULL;\n\t\t\t\tgoto isolate_abort;\n\t\t\t}\n\t\t\tvalid_page = page;\n\t\t}\n\n\t\tif (PageHuge(page) && cc->alloc_contig) {\n\t\t\tif (locked) {\n\t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);\n\t\t\t\tlocked = NULL;\n\t\t\t}\n\n\t\t\tret = isolate_or_dissolve_huge_page(page, &cc->migratepages);\n\n\t\t\t \n\t\t\tif (ret < 0) {\n\t\t\t\t  \n\t\t\t\tif (ret == -EBUSY)\n\t\t\t\t\tret = 0;\n\t\t\t\tlow_pfn += compound_nr(page) - 1;\n\t\t\t\tnr_scanned += compound_nr(page) - 1;\n\t\t\t\tgoto isolate_fail;\n\t\t\t}\n\n\t\t\tif (PageHuge(page)) {\n\t\t\t\t \n\t\t\t\tfolio = page_folio(page);\n\t\t\t\tlow_pfn += folio_nr_pages(folio) - 1;\n\t\t\t\tgoto isolate_success_no_list;\n\t\t\t}\n\n\t\t\t \n\t\t}\n\n\t\t \n\t\tif (PageBuddy(page)) {\n\t\t\tunsigned long freepage_order = buddy_order_unsafe(page);\n\n\t\t\t \n\t\t\tif (freepage_order > 0 && freepage_order <= MAX_ORDER) {\n\t\t\t\tlow_pfn += (1UL << freepage_order) - 1;\n\t\t\t\tnr_scanned += (1UL << freepage_order) - 1;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (PageCompound(page) && !cc->alloc_contig) {\n\t\t\tconst unsigned int order = compound_order(page);\n\n\t\t\tif (likely(order <= MAX_ORDER)) {\n\t\t\t\tlow_pfn += (1UL << order) - 1;\n\t\t\t\tnr_scanned += (1UL << order) - 1;\n\t\t\t}\n\t\t\tgoto isolate_fail;\n\t\t}\n\n\t\t \n\t\tif (!PageLRU(page)) {\n\t\t\t \n\t\t\tif (unlikely(__PageMovable(page)) &&\n\t\t\t\t\t!PageIsolated(page)) {\n\t\t\t\tif (locked) {\n\t\t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);\n\t\t\t\t\tlocked = NULL;\n\t\t\t\t}\n\n\t\t\t\tif (isolate_movable_page(page, mode)) {\n\t\t\t\t\tfolio = page_folio(page);\n\t\t\t\t\tgoto isolate_success;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tgoto isolate_fail;\n\t\t}\n\n\t\t \n\t\tfolio = folio_get_nontail_page(page);\n\t\tif (unlikely(!folio))\n\t\t\tgoto isolate_fail;\n\n\t\t \n\t\tmapping = folio_mapping(folio);\n\t\tif (!mapping && (folio_ref_count(folio) - 1) > folio_mapcount(folio))\n\t\t\tgoto isolate_fail_put;\n\n\t\t \n\t\tif (!(cc->gfp_mask & __GFP_FS) && mapping)\n\t\t\tgoto isolate_fail_put;\n\n\t\t \n\t\tif (!folio_test_lru(folio))\n\t\t\tgoto isolate_fail_put;\n\n\t\t \n\t\tif (!(mode & ISOLATE_UNEVICTABLE) && folio_test_unevictable(folio))\n\t\t\tgoto isolate_fail_put;\n\n\t\t \n\t\tif ((mode & ISOLATE_ASYNC_MIGRATE) && folio_test_writeback(folio))\n\t\t\tgoto isolate_fail_put;\n\n\t\tif ((mode & ISOLATE_ASYNC_MIGRATE) && folio_test_dirty(folio)) {\n\t\t\tbool migrate_dirty;\n\n\t\t\t \n\t\t\tif (!folio_trylock(folio))\n\t\t\t\tgoto isolate_fail_put;\n\n\t\t\tmapping = folio_mapping(folio);\n\t\t\tmigrate_dirty = !mapping ||\n\t\t\t\t\tmapping->a_ops->migrate_folio;\n\t\t\tfolio_unlock(folio);\n\t\t\tif (!migrate_dirty)\n\t\t\t\tgoto isolate_fail_put;\n\t\t}\n\n\t\t \n\t\tif (!folio_test_clear_lru(folio))\n\t\t\tgoto isolate_fail_put;\n\n\t\tlruvec = folio_lruvec(folio);\n\n\t\t \n\t\tif (lruvec != locked) {\n\t\t\tif (locked)\n\t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);\n\n\t\t\tcompact_lock_irqsave(&lruvec->lru_lock, &flags, cc);\n\t\t\tlocked = lruvec;\n\n\t\t\tlruvec_memcg_debug(lruvec, folio);\n\n\t\t\t \n\t\t\tif (!skip_updated && valid_page) {\n\t\t\t\tskip_updated = true;\n\t\t\t\tif (test_and_set_skip(cc, valid_page) &&\n\t\t\t\t    !cc->finish_pageblock) {\n\t\t\t\t\tlow_pfn = end_pfn;\n\t\t\t\t\tgoto isolate_abort;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t \n\t\t\tif (unlikely(folio_test_large(folio) && !cc->alloc_contig)) {\n\t\t\t\tlow_pfn += folio_nr_pages(folio) - 1;\n\t\t\t\tnr_scanned += folio_nr_pages(folio) - 1;\n\t\t\t\tfolio_set_lru(folio);\n\t\t\t\tgoto isolate_fail_put;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (folio_test_large(folio))\n\t\t\tlow_pfn += folio_nr_pages(folio) - 1;\n\n\t\t \n\t\tlruvec_del_folio(lruvec, folio);\n\t\tnode_stat_mod_folio(folio,\n\t\t\t\tNR_ISOLATED_ANON + folio_is_file_lru(folio),\n\t\t\t\tfolio_nr_pages(folio));\n\nisolate_success:\n\t\tlist_add(&folio->lru, &cc->migratepages);\nisolate_success_no_list:\n\t\tcc->nr_migratepages += folio_nr_pages(folio);\n\t\tnr_isolated += folio_nr_pages(folio);\n\t\tnr_scanned += folio_nr_pages(folio) - 1;\n\n\t\t \n\t\tif (cc->nr_migratepages >= COMPACT_CLUSTER_MAX &&\n\t\t    !cc->finish_pageblock && !cc->contended) {\n\t\t\t++low_pfn;\n\t\t\tbreak;\n\t\t}\n\n\t\tcontinue;\n\nisolate_fail_put:\n\t\t \n\t\tif (locked) {\n\t\t\tunlock_page_lruvec_irqrestore(locked, flags);\n\t\t\tlocked = NULL;\n\t\t}\n\t\tfolio_put(folio);\n\nisolate_fail:\n\t\tif (!skip_on_failure && ret != -ENOMEM)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (nr_isolated) {\n\t\t\tif (locked) {\n\t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);\n\t\t\t\tlocked = NULL;\n\t\t\t}\n\t\t\tputback_movable_pages(&cc->migratepages);\n\t\t\tcc->nr_migratepages = 0;\n\t\t\tnr_isolated = 0;\n\t\t}\n\n\t\tif (low_pfn < next_skip_pfn) {\n\t\t\tlow_pfn = next_skip_pfn - 1;\n\t\t\t \n\t\t\tnext_skip_pfn += 1UL << cc->order;\n\t\t}\n\n\t\tif (ret == -ENOMEM)\n\t\t\tbreak;\n\t}\n\n\t \n\tif (unlikely(low_pfn > end_pfn))\n\t\tlow_pfn = end_pfn;\n\n\tfolio = NULL;\n\nisolate_abort:\n\tif (locked)\n\t\tunlock_page_lruvec_irqrestore(locked, flags);\n\tif (folio) {\n\t\tfolio_set_lru(folio);\n\t\tfolio_put(folio);\n\t}\n\n\t \n\tif (low_pfn == end_pfn && (!nr_isolated || cc->finish_pageblock)) {\n\t\tif (!cc->no_set_skip_hint && valid_page && !skip_updated)\n\t\t\tset_pageblock_skip(valid_page);\n\t\tupdate_cached_migrate(cc, low_pfn);\n\t}\n\n\ttrace_mm_compaction_isolate_migratepages(start_pfn, low_pfn,\n\t\t\t\t\t\tnr_scanned, nr_isolated);\n\nfatal_pending:\n\tcc->total_migrate_scanned += nr_scanned;\n\tif (nr_isolated)\n\t\tcount_compact_events(COMPACTISOLATED, nr_isolated);\n\n\tcc->migrate_pfn = low_pfn;\n\n\treturn ret;\n}\n\n \nint\nisolate_migratepages_range(struct compact_control *cc, unsigned long start_pfn,\n\t\t\t\t\t\t\tunsigned long end_pfn)\n{\n\tunsigned long pfn, block_start_pfn, block_end_pfn;\n\tint ret = 0;\n\n\t \n\tpfn = start_pfn;\n\tblock_start_pfn = pageblock_start_pfn(pfn);\n\tif (block_start_pfn < cc->zone->zone_start_pfn)\n\t\tblock_start_pfn = cc->zone->zone_start_pfn;\n\tblock_end_pfn = pageblock_end_pfn(pfn);\n\n\tfor (; pfn < end_pfn; pfn = block_end_pfn,\n\t\t\t\tblock_start_pfn = block_end_pfn,\n\t\t\t\tblock_end_pfn += pageblock_nr_pages) {\n\n\t\tblock_end_pfn = min(block_end_pfn, end_pfn);\n\n\t\tif (!pageblock_pfn_to_page(block_start_pfn,\n\t\t\t\t\tblock_end_pfn, cc->zone))\n\t\t\tcontinue;\n\n\t\tret = isolate_migratepages_block(cc, pfn, block_end_pfn,\n\t\t\t\t\t\t ISOLATE_UNEVICTABLE);\n\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tif (cc->nr_migratepages >= COMPACT_CLUSTER_MAX)\n\t\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\n#endif  \n#ifdef CONFIG_COMPACTION\n\nstatic bool suitable_migration_source(struct compact_control *cc,\n\t\t\t\t\t\t\tstruct page *page)\n{\n\tint block_mt;\n\n\tif (pageblock_skip_persistent(page))\n\t\treturn false;\n\n\tif ((cc->mode != MIGRATE_ASYNC) || !cc->direct_compaction)\n\t\treturn true;\n\n\tblock_mt = get_pageblock_migratetype(page);\n\n\tif (cc->migratetype == MIGRATE_MOVABLE)\n\t\treturn is_migrate_movable(block_mt);\n\telse\n\t\treturn block_mt == cc->migratetype;\n}\n\n \nstatic bool suitable_migration_target(struct compact_control *cc,\n\t\t\t\t\t\t\tstruct page *page)\n{\n\t \n\tif (PageBuddy(page)) {\n\t\t \n\t\tif (buddy_order_unsafe(page) >= pageblock_order)\n\t\t\treturn false;\n\t}\n\n\tif (cc->ignore_block_suitable)\n\t\treturn true;\n\n\t \n\tif (is_migrate_movable(get_pageblock_migratetype(page)))\n\t\treturn true;\n\n\t \n\treturn false;\n}\n\nstatic inline unsigned int\nfreelist_scan_limit(struct compact_control *cc)\n{\n\tunsigned short shift = BITS_PER_LONG - 1;\n\n\treturn (COMPACT_CLUSTER_MAX >> min(shift, cc->fast_search_fail)) + 1;\n}\n\n \nstatic inline bool compact_scanners_met(struct compact_control *cc)\n{\n\treturn (cc->free_pfn >> pageblock_order)\n\t\t<= (cc->migrate_pfn >> pageblock_order);\n}\n\n \nstatic void\nmove_freelist_head(struct list_head *freelist, struct page *freepage)\n{\n\tLIST_HEAD(sublist);\n\n\tif (!list_is_last(freelist, &freepage->lru)) {\n\t\tlist_cut_before(&sublist, freelist, &freepage->lru);\n\t\tlist_splice_tail(&sublist, freelist);\n\t}\n}\n\n \nstatic void\nmove_freelist_tail(struct list_head *freelist, struct page *freepage)\n{\n\tLIST_HEAD(sublist);\n\n\tif (!list_is_first(freelist, &freepage->lru)) {\n\t\tlist_cut_position(&sublist, freelist, &freepage->lru);\n\t\tlist_splice_tail(&sublist, freelist);\n\t}\n}\n\nstatic void\nfast_isolate_around(struct compact_control *cc, unsigned long pfn)\n{\n\tunsigned long start_pfn, end_pfn;\n\tstruct page *page;\n\n\t \n\tif (cc->nr_freepages >= cc->nr_migratepages)\n\t\treturn;\n\n\t \n\tif (cc->direct_compaction && cc->mode == MIGRATE_ASYNC)\n\t\treturn;\n\n\t \n\tstart_pfn = max(pageblock_start_pfn(pfn), cc->zone->zone_start_pfn);\n\tend_pfn = min(pageblock_end_pfn(pfn), zone_end_pfn(cc->zone));\n\n\tpage = pageblock_pfn_to_page(start_pfn, end_pfn, cc->zone);\n\tif (!page)\n\t\treturn;\n\n\tisolate_freepages_block(cc, &start_pfn, end_pfn, &cc->freepages, 1, false);\n\n\t \n\tif (start_pfn == end_pfn && !cc->no_set_skip_hint)\n\t\tset_pageblock_skip(page);\n}\n\n \nstatic int next_search_order(struct compact_control *cc, int order)\n{\n\torder--;\n\tif (order < 0)\n\t\torder = cc->order - 1;\n\n\t \n\tif (order == cc->search_order) {\n\t\tcc->search_order--;\n\t\tif (cc->search_order < 0)\n\t\t\tcc->search_order = cc->order - 1;\n\t\treturn -1;\n\t}\n\n\treturn order;\n}\n\nstatic void fast_isolate_freepages(struct compact_control *cc)\n{\n\tunsigned int limit = max(1U, freelist_scan_limit(cc) >> 1);\n\tunsigned int nr_scanned = 0, total_isolated = 0;\n\tunsigned long low_pfn, min_pfn, highest = 0;\n\tunsigned long nr_isolated = 0;\n\tunsigned long distance;\n\tstruct page *page = NULL;\n\tbool scan_start = false;\n\tint order;\n\n\t \n\tif (cc->order <= 0)\n\t\treturn;\n\n\t \n\tif (cc->free_pfn >= cc->zone->compact_init_free_pfn) {\n\t\tlimit = pageblock_nr_pages >> 1;\n\t\tscan_start = true;\n\t}\n\n\t \n\tdistance = (cc->free_pfn - cc->migrate_pfn);\n\tlow_pfn = pageblock_start_pfn(cc->free_pfn - (distance >> 2));\n\tmin_pfn = pageblock_start_pfn(cc->free_pfn - (distance >> 1));\n\n\tif (WARN_ON_ONCE(min_pfn > low_pfn))\n\t\tlow_pfn = min_pfn;\n\n\t \n\tcc->search_order = min_t(unsigned int, cc->order - 1, cc->search_order);\n\n\tfor (order = cc->search_order;\n\t     !page && order >= 0;\n\t     order = next_search_order(cc, order)) {\n\t\tstruct free_area *area = &cc->zone->free_area[order];\n\t\tstruct list_head *freelist;\n\t\tstruct page *freepage;\n\t\tunsigned long flags;\n\t\tunsigned int order_scanned = 0;\n\t\tunsigned long high_pfn = 0;\n\n\t\tif (!area->nr_free)\n\t\t\tcontinue;\n\n\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n\t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n\t\tlist_for_each_entry_reverse(freepage, freelist, buddy_list) {\n\t\t\tunsigned long pfn;\n\n\t\t\torder_scanned++;\n\t\t\tnr_scanned++;\n\t\t\tpfn = page_to_pfn(freepage);\n\n\t\t\tif (pfn >= highest)\n\t\t\t\thighest = max(pageblock_start_pfn(pfn),\n\t\t\t\t\t      cc->zone->zone_start_pfn);\n\n\t\t\tif (pfn >= low_pfn) {\n\t\t\t\tcc->fast_search_fail = 0;\n\t\t\t\tcc->search_order = order;\n\t\t\t\tpage = freepage;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (pfn >= min_pfn && pfn > high_pfn) {\n\t\t\t\thigh_pfn = pfn;\n\n\t\t\t\t \n\t\t\t\tlimit >>= 1;\n\t\t\t}\n\n\t\t\tif (order_scanned >= limit)\n\t\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (!page && high_pfn) {\n\t\t\tpage = pfn_to_page(high_pfn);\n\n\t\t\t \n\t\t\tfreepage = page;\n\t\t}\n\n\t\t \n\t\tmove_freelist_head(freelist, freepage);\n\n\t\t \n\t\tif (page) {\n\t\t\tif (__isolate_free_page(page, order)) {\n\t\t\t\tset_page_private(page, order);\n\t\t\t\tnr_isolated = 1 << order;\n\t\t\t\tnr_scanned += nr_isolated - 1;\n\t\t\t\ttotal_isolated += nr_isolated;\n\t\t\t\tcc->nr_freepages += nr_isolated;\n\t\t\t\tlist_add_tail(&page->lru, &cc->freepages);\n\t\t\t\tcount_compact_events(COMPACTISOLATED, nr_isolated);\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\torder = cc->search_order + 1;\n\t\t\t\tpage = NULL;\n\t\t\t}\n\t\t}\n\n\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n\n\t\t \n\t\tif (cc->nr_freepages >= cc->nr_migratepages)\n\t\t\tbreak;\n\n\t\t \n\t\tif (order_scanned >= limit)\n\t\t\tlimit = max(1U, limit >> 1);\n\t}\n\n\ttrace_mm_compaction_fast_isolate_freepages(min_pfn, cc->free_pfn,\n\t\t\t\t\t\t   nr_scanned, total_isolated);\n\n\tif (!page) {\n\t\tcc->fast_search_fail++;\n\t\tif (scan_start) {\n\t\t\t \n\t\t\tif (highest >= min_pfn) {\n\t\t\t\tpage = pfn_to_page(highest);\n\t\t\t\tcc->free_pfn = highest;\n\t\t\t} else {\n\t\t\t\tif (cc->direct_compaction && pfn_valid(min_pfn)) {\n\t\t\t\t\tpage = pageblock_pfn_to_page(min_pfn,\n\t\t\t\t\t\tmin(pageblock_end_pfn(min_pfn),\n\t\t\t\t\t\t    zone_end_pfn(cc->zone)),\n\t\t\t\t\t\tcc->zone);\n\t\t\t\t\tcc->free_pfn = min_pfn;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif (highest && highest >= cc->zone->compact_cached_free_pfn) {\n\t\thighest -= pageblock_nr_pages;\n\t\tcc->zone->compact_cached_free_pfn = highest;\n\t}\n\n\tcc->total_free_scanned += nr_scanned;\n\tif (!page)\n\t\treturn;\n\n\tlow_pfn = page_to_pfn(page);\n\tfast_isolate_around(cc, low_pfn);\n}\n\n \nstatic void isolate_freepages(struct compact_control *cc)\n{\n\tstruct zone *zone = cc->zone;\n\tstruct page *page;\n\tunsigned long block_start_pfn;\t \n\tunsigned long isolate_start_pfn;  \n\tunsigned long block_end_pfn;\t \n\tunsigned long low_pfn;\t      \n\tstruct list_head *freelist = &cc->freepages;\n\tunsigned int stride;\n\n\t \n\tfast_isolate_freepages(cc);\n\tif (cc->nr_freepages)\n\t\tgoto splitmap;\n\n\t \n\tisolate_start_pfn = cc->free_pfn;\n\tblock_start_pfn = pageblock_start_pfn(isolate_start_pfn);\n\tblock_end_pfn = min(block_start_pfn + pageblock_nr_pages,\n\t\t\t\t\t\tzone_end_pfn(zone));\n\tlow_pfn = pageblock_end_pfn(cc->migrate_pfn);\n\tstride = cc->mode == MIGRATE_ASYNC ? COMPACT_CLUSTER_MAX : 1;\n\n\t \n\tfor (; block_start_pfn >= low_pfn;\n\t\t\t\tblock_end_pfn = block_start_pfn,\n\t\t\t\tblock_start_pfn -= pageblock_nr_pages,\n\t\t\t\tisolate_start_pfn = block_start_pfn) {\n\t\tunsigned long nr_isolated;\n\n\t\t \n\t\tif (!(block_start_pfn % (COMPACT_CLUSTER_MAX * pageblock_nr_pages)))\n\t\t\tcond_resched();\n\n\t\tpage = pageblock_pfn_to_page(block_start_pfn, block_end_pfn,\n\t\t\t\t\t\t\t\t\tzone);\n\t\tif (!page) {\n\t\t\tunsigned long next_pfn;\n\n\t\t\tnext_pfn = skip_offline_sections_reverse(block_start_pfn);\n\t\t\tif (next_pfn)\n\t\t\t\tblock_start_pfn = max(next_pfn, low_pfn);\n\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (!suitable_migration_target(cc, page))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (!isolation_suitable(cc, page))\n\t\t\tcontinue;\n\n\t\t \n\t\tnr_isolated = isolate_freepages_block(cc, &isolate_start_pfn,\n\t\t\t\t\tblock_end_pfn, freelist, stride, false);\n\n\t\t \n\t\tif (isolate_start_pfn == block_end_pfn)\n\t\t\tupdate_pageblock_skip(cc, page, block_start_pfn -\n\t\t\t\t\t      pageblock_nr_pages);\n\n\t\t \n\t\tif (cc->nr_freepages >= cc->nr_migratepages) {\n\t\t\tif (isolate_start_pfn >= block_end_pfn) {\n\t\t\t\t \n\t\t\t\tisolate_start_pfn =\n\t\t\t\t\tblock_start_pfn - pageblock_nr_pages;\n\t\t\t}\n\t\t\tbreak;\n\t\t} else if (isolate_start_pfn < block_end_pfn) {\n\t\t\t \n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (nr_isolated) {\n\t\t\tstride = 1;\n\t\t\tcontinue;\n\t\t}\n\t\tstride = min_t(unsigned int, COMPACT_CLUSTER_MAX, stride << 1);\n\t}\n\n\t \n\tcc->free_pfn = isolate_start_pfn;\n\nsplitmap:\n\t \n\tsplit_map_pages(freelist);\n}\n\n \nstatic struct folio *compaction_alloc(struct folio *src, unsigned long data)\n{\n\tstruct compact_control *cc = (struct compact_control *)data;\n\tstruct folio *dst;\n\n\tif (list_empty(&cc->freepages)) {\n\t\tisolate_freepages(cc);\n\n\t\tif (list_empty(&cc->freepages))\n\t\t\treturn NULL;\n\t}\n\n\tdst = list_entry(cc->freepages.next, struct folio, lru);\n\tlist_del(&dst->lru);\n\tcc->nr_freepages--;\n\n\treturn dst;\n}\n\n \nstatic void compaction_free(struct folio *dst, unsigned long data)\n{\n\tstruct compact_control *cc = (struct compact_control *)data;\n\n\tlist_add(&dst->lru, &cc->freepages);\n\tcc->nr_freepages++;\n}\n\n \ntypedef enum {\n\tISOLATE_ABORT,\t\t \n\tISOLATE_NONE,\t\t \n\tISOLATE_SUCCESS,\t \n} isolate_migrate_t;\n\n \nstatic int sysctl_compact_unevictable_allowed __read_mostly = CONFIG_COMPACT_UNEVICTABLE_DEFAULT;\n \nstatic unsigned int __read_mostly sysctl_compaction_proactiveness = 20;\nstatic int sysctl_extfrag_threshold = 500;\nstatic int __read_mostly sysctl_compact_memory;\n\nstatic inline void\nupdate_fast_start_pfn(struct compact_control *cc, unsigned long pfn)\n{\n\tif (cc->fast_start_pfn == ULONG_MAX)\n\t\treturn;\n\n\tif (!cc->fast_start_pfn)\n\t\tcc->fast_start_pfn = pfn;\n\n\tcc->fast_start_pfn = min(cc->fast_start_pfn, pfn);\n}\n\nstatic inline unsigned long\nreinit_migrate_pfn(struct compact_control *cc)\n{\n\tif (!cc->fast_start_pfn || cc->fast_start_pfn == ULONG_MAX)\n\t\treturn cc->migrate_pfn;\n\n\tcc->migrate_pfn = cc->fast_start_pfn;\n\tcc->fast_start_pfn = ULONG_MAX;\n\n\treturn cc->migrate_pfn;\n}\n\n \nstatic unsigned long fast_find_migrateblock(struct compact_control *cc)\n{\n\tunsigned int limit = freelist_scan_limit(cc);\n\tunsigned int nr_scanned = 0;\n\tunsigned long distance;\n\tunsigned long pfn = cc->migrate_pfn;\n\tunsigned long high_pfn;\n\tint order;\n\tbool found_block = false;\n\n\t \n\tif (cc->ignore_skip_hint)\n\t\treturn pfn;\n\n\t \n\tif (cc->finish_pageblock)\n\t\treturn pfn;\n\n\t \n\tif (pfn != cc->zone->zone_start_pfn && pfn != pageblock_start_pfn(pfn))\n\t\treturn pfn;\n\n\t \n\tif (cc->order <= PAGE_ALLOC_COSTLY_ORDER)\n\t\treturn pfn;\n\n\t \n\tif (cc->direct_compaction && cc->migratetype != MIGRATE_MOVABLE)\n\t\treturn pfn;\n\n\t \n\tdistance = (cc->free_pfn - cc->migrate_pfn) >> 1;\n\tif (cc->migrate_pfn != cc->zone->zone_start_pfn)\n\t\tdistance >>= 2;\n\thigh_pfn = pageblock_start_pfn(cc->migrate_pfn + distance);\n\n\tfor (order = cc->order - 1;\n\t     order >= PAGE_ALLOC_COSTLY_ORDER && !found_block && nr_scanned < limit;\n\t     order--) {\n\t\tstruct free_area *area = &cc->zone->free_area[order];\n\t\tstruct list_head *freelist;\n\t\tunsigned long flags;\n\t\tstruct page *freepage;\n\n\t\tif (!area->nr_free)\n\t\t\tcontinue;\n\n\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n\t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n\t\tlist_for_each_entry(freepage, freelist, buddy_list) {\n\t\t\tunsigned long free_pfn;\n\n\t\t\tif (nr_scanned++ >= limit) {\n\t\t\t\tmove_freelist_tail(freelist, freepage);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tfree_pfn = page_to_pfn(freepage);\n\t\t\tif (free_pfn < high_pfn) {\n\t\t\t\t \n\t\t\t\tif (get_pageblock_skip(freepage))\n\t\t\t\t\tcontinue;\n\n\t\t\t\t \n\t\t\t\tmove_freelist_tail(freelist, freepage);\n\n\t\t\t\tupdate_fast_start_pfn(cc, free_pfn);\n\t\t\t\tpfn = pageblock_start_pfn(free_pfn);\n\t\t\t\tif (pfn < cc->zone->zone_start_pfn)\n\t\t\t\t\tpfn = cc->zone->zone_start_pfn;\n\t\t\t\tcc->fast_search_fail = 0;\n\t\t\t\tfound_block = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n\t}\n\n\tcc->total_migrate_scanned += nr_scanned;\n\n\t \n\tif (!found_block) {\n\t\tcc->fast_search_fail++;\n\t\tpfn = reinit_migrate_pfn(cc);\n\t}\n\treturn pfn;\n}\n\n \nstatic isolate_migrate_t isolate_migratepages(struct compact_control *cc)\n{\n\tunsigned long block_start_pfn;\n\tunsigned long block_end_pfn;\n\tunsigned long low_pfn;\n\tstruct page *page;\n\tconst isolate_mode_t isolate_mode =\n\t\t(sysctl_compact_unevictable_allowed ? ISOLATE_UNEVICTABLE : 0) |\n\t\t(cc->mode != MIGRATE_SYNC ? ISOLATE_ASYNC_MIGRATE : 0);\n\tbool fast_find_block;\n\n\t \n\tlow_pfn = fast_find_migrateblock(cc);\n\tblock_start_pfn = pageblock_start_pfn(low_pfn);\n\tif (block_start_pfn < cc->zone->zone_start_pfn)\n\t\tblock_start_pfn = cc->zone->zone_start_pfn;\n\n\t \n\tfast_find_block = low_pfn != cc->migrate_pfn && !cc->fast_search_fail;\n\n\t \n\tblock_end_pfn = pageblock_end_pfn(low_pfn);\n\n\t \n\tfor (; block_end_pfn <= cc->free_pfn;\n\t\t\tfast_find_block = false,\n\t\t\tcc->migrate_pfn = low_pfn = block_end_pfn,\n\t\t\tblock_start_pfn = block_end_pfn,\n\t\t\tblock_end_pfn += pageblock_nr_pages) {\n\n\t\t \n\t\tif (!(low_pfn % (COMPACT_CLUSTER_MAX * pageblock_nr_pages)))\n\t\t\tcond_resched();\n\n\t\tpage = pageblock_pfn_to_page(block_start_pfn,\n\t\t\t\t\t\tblock_end_pfn, cc->zone);\n\t\tif (!page) {\n\t\t\tunsigned long next_pfn;\n\n\t\t\tnext_pfn = skip_offline_sections(block_start_pfn);\n\t\t\tif (next_pfn)\n\t\t\t\tblock_end_pfn = min(next_pfn, cc->free_pfn);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif ((pageblock_aligned(low_pfn) ||\n\t\t     low_pfn == cc->zone->zone_start_pfn) &&\n\t\t    !fast_find_block && !isolation_suitable(cc, page))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (!suitable_migration_source(cc, page)) {\n\t\t\tupdate_cached_migrate(cc, block_end_pfn);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (isolate_migratepages_block(cc, low_pfn, block_end_pfn,\n\t\t\t\t\t\tisolate_mode))\n\t\t\treturn ISOLATE_ABORT;\n\n\t\t \n\t\tbreak;\n\t}\n\n\treturn cc->nr_migratepages ? ISOLATE_SUCCESS : ISOLATE_NONE;\n}\n\n \nstatic inline bool is_via_compact_memory(int order)\n{\n\treturn order == -1;\n}\n\n \nstatic bool kswapd_is_running(pg_data_t *pgdat)\n{\n\tbool running;\n\n\tpgdat_kswapd_lock(pgdat);\n\trunning = pgdat->kswapd && task_is_running(pgdat->kswapd);\n\tpgdat_kswapd_unlock(pgdat);\n\n\treturn running;\n}\n\n \nstatic unsigned int fragmentation_score_zone(struct zone *zone)\n{\n\treturn extfrag_for_order(zone, COMPACTION_HPAGE_ORDER);\n}\n\n \nstatic unsigned int fragmentation_score_zone_weighted(struct zone *zone)\n{\n\tunsigned long score;\n\n\tscore = zone->present_pages * fragmentation_score_zone(zone);\n\treturn div64_ul(score, zone->zone_pgdat->node_present_pages + 1);\n}\n\n \nstatic unsigned int fragmentation_score_node(pg_data_t *pgdat)\n{\n\tunsigned int score = 0;\n\tint zoneid;\n\n\tfor (zoneid = 0; zoneid < MAX_NR_ZONES; zoneid++) {\n\t\tstruct zone *zone;\n\n\t\tzone = &pgdat->node_zones[zoneid];\n\t\tif (!populated_zone(zone))\n\t\t\tcontinue;\n\t\tscore += fragmentation_score_zone_weighted(zone);\n\t}\n\n\treturn score;\n}\n\nstatic unsigned int fragmentation_score_wmark(bool low)\n{\n\tunsigned int wmark_low;\n\n\t \n\twmark_low = max(100U - sysctl_compaction_proactiveness, 5U);\n\treturn low ? wmark_low : min(wmark_low + 10, 100U);\n}\n\nstatic bool should_proactive_compact_node(pg_data_t *pgdat)\n{\n\tint wmark_high;\n\n\tif (!sysctl_compaction_proactiveness || kswapd_is_running(pgdat))\n\t\treturn false;\n\n\twmark_high = fragmentation_score_wmark(false);\n\treturn fragmentation_score_node(pgdat) > wmark_high;\n}\n\nstatic enum compact_result __compact_finished(struct compact_control *cc)\n{\n\tunsigned int order;\n\tconst int migratetype = cc->migratetype;\n\tint ret;\n\n\t \n\tif (compact_scanners_met(cc)) {\n\t\t \n\t\treset_cached_positions(cc->zone);\n\n\t\t \n\t\tif (cc->direct_compaction)\n\t\t\tcc->zone->compact_blockskip_flush = true;\n\n\t\tif (cc->whole_zone)\n\t\t\treturn COMPACT_COMPLETE;\n\t\telse\n\t\t\treturn COMPACT_PARTIAL_SKIPPED;\n\t}\n\n\tif (cc->proactive_compaction) {\n\t\tint score, wmark_low;\n\t\tpg_data_t *pgdat;\n\n\t\tpgdat = cc->zone->zone_pgdat;\n\t\tif (kswapd_is_running(pgdat))\n\t\t\treturn COMPACT_PARTIAL_SKIPPED;\n\n\t\tscore = fragmentation_score_zone(cc->zone);\n\t\twmark_low = fragmentation_score_wmark(true);\n\n\t\tif (score > wmark_low)\n\t\t\tret = COMPACT_CONTINUE;\n\t\telse\n\t\t\tret = COMPACT_SUCCESS;\n\n\t\tgoto out;\n\t}\n\n\tif (is_via_compact_memory(cc->order))\n\t\treturn COMPACT_CONTINUE;\n\n\t \n\tif (!pageblock_aligned(cc->migrate_pfn))\n\t\treturn COMPACT_CONTINUE;\n\n\t \n\tret = COMPACT_NO_SUITABLE_PAGE;\n\tfor (order = cc->order; order <= MAX_ORDER; order++) {\n\t\tstruct free_area *area = &cc->zone->free_area[order];\n\t\tbool can_steal;\n\n\t\t \n\t\tif (!free_area_empty(area, migratetype))\n\t\t\treturn COMPACT_SUCCESS;\n\n#ifdef CONFIG_CMA\n\t\t \n\t\tif (migratetype == MIGRATE_MOVABLE &&\n\t\t\t!free_area_empty(area, MIGRATE_CMA))\n\t\t\treturn COMPACT_SUCCESS;\n#endif\n\t\t \n\t\tif (find_suitable_fallback(area, order, migratetype,\n\t\t\t\t\t\ttrue, &can_steal) != -1)\n\t\t\t \n\t\t\treturn COMPACT_SUCCESS;\n\t}\n\nout:\n\tif (cc->contended || fatal_signal_pending(current))\n\t\tret = COMPACT_CONTENDED;\n\n\treturn ret;\n}\n\nstatic enum compact_result compact_finished(struct compact_control *cc)\n{\n\tint ret;\n\n\tret = __compact_finished(cc);\n\ttrace_mm_compaction_finished(cc->zone, cc->order, ret);\n\tif (ret == COMPACT_NO_SUITABLE_PAGE)\n\t\tret = COMPACT_CONTINUE;\n\n\treturn ret;\n}\n\nstatic bool __compaction_suitable(struct zone *zone, int order,\n\t\t\t\t  int highest_zoneidx,\n\t\t\t\t  unsigned long wmark_target)\n{\n\tunsigned long watermark;\n\t \n\twatermark = (order > PAGE_ALLOC_COSTLY_ORDER) ?\n\t\t\t\tlow_wmark_pages(zone) : min_wmark_pages(zone);\n\twatermark += compact_gap(order);\n\treturn __zone_watermark_ok(zone, 0, watermark, highest_zoneidx,\n\t\t\t\t   ALLOC_CMA, wmark_target);\n}\n\n \nbool compaction_suitable(struct zone *zone, int order, int highest_zoneidx)\n{\n\tenum compact_result compact_result;\n\tbool suitable;\n\n\tsuitable = __compaction_suitable(zone, order, highest_zoneidx,\n\t\t\t\t\t zone_page_state(zone, NR_FREE_PAGES));\n\t \n\tif (suitable) {\n\t\tcompact_result = COMPACT_CONTINUE;\n\t\tif (order > PAGE_ALLOC_COSTLY_ORDER) {\n\t\t\tint fragindex = fragmentation_index(zone, order);\n\n\t\t\tif (fragindex >= 0 &&\n\t\t\t    fragindex <= sysctl_extfrag_threshold) {\n\t\t\t\tsuitable = false;\n\t\t\t\tcompact_result = COMPACT_NOT_SUITABLE_ZONE;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tcompact_result = COMPACT_SKIPPED;\n\t}\n\n\ttrace_mm_compaction_suitable(zone, order, compact_result);\n\n\treturn suitable;\n}\n\nbool compaction_zonelist_suitable(struct alloc_context *ac, int order,\n\t\tint alloc_flags)\n{\n\tstruct zone *zone;\n\tstruct zoneref *z;\n\n\t \n\tfor_each_zone_zonelist_nodemask(zone, z, ac->zonelist,\n\t\t\t\tac->highest_zoneidx, ac->nodemask) {\n\t\tunsigned long available;\n\n\t\t \n\t\tavailable = zone_reclaimable_pages(zone) / order;\n\t\tavailable += zone_page_state_snapshot(zone, NR_FREE_PAGES);\n\t\tif (__compaction_suitable(zone, order, ac->highest_zoneidx,\n\t\t\t\t\t  available))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic enum compact_result\ncompact_zone(struct compact_control *cc, struct capture_control *capc)\n{\n\tenum compact_result ret;\n\tunsigned long start_pfn = cc->zone->zone_start_pfn;\n\tunsigned long end_pfn = zone_end_pfn(cc->zone);\n\tunsigned long last_migrated_pfn;\n\tconst bool sync = cc->mode != MIGRATE_ASYNC;\n\tbool update_cached;\n\tunsigned int nr_succeeded = 0;\n\n\t \n\tcc->total_migrate_scanned = 0;\n\tcc->total_free_scanned = 0;\n\tcc->nr_migratepages = 0;\n\tcc->nr_freepages = 0;\n\tINIT_LIST_HEAD(&cc->freepages);\n\tINIT_LIST_HEAD(&cc->migratepages);\n\n\tcc->migratetype = gfp_migratetype(cc->gfp_mask);\n\n\tif (!is_via_compact_memory(cc->order)) {\n\t\tunsigned long watermark;\n\n\t\t \n\t\twatermark = wmark_pages(cc->zone,\n\t\t\t\t\tcc->alloc_flags & ALLOC_WMARK_MASK);\n\t\tif (zone_watermark_ok(cc->zone, cc->order, watermark,\n\t\t\t\t      cc->highest_zoneidx, cc->alloc_flags))\n\t\t\treturn COMPACT_SUCCESS;\n\n\t\t \n\t\tif (!compaction_suitable(cc->zone, cc->order,\n\t\t\t\t\t cc->highest_zoneidx))\n\t\t\treturn COMPACT_SKIPPED;\n\t}\n\n\t \n\tif (compaction_restarting(cc->zone, cc->order))\n\t\t__reset_isolation_suitable(cc->zone);\n\n\t \n\tcc->fast_start_pfn = 0;\n\tif (cc->whole_zone) {\n\t\tcc->migrate_pfn = start_pfn;\n\t\tcc->free_pfn = pageblock_start_pfn(end_pfn - 1);\n\t} else {\n\t\tcc->migrate_pfn = cc->zone->compact_cached_migrate_pfn[sync];\n\t\tcc->free_pfn = cc->zone->compact_cached_free_pfn;\n\t\tif (cc->free_pfn < start_pfn || cc->free_pfn >= end_pfn) {\n\t\t\tcc->free_pfn = pageblock_start_pfn(end_pfn - 1);\n\t\t\tcc->zone->compact_cached_free_pfn = cc->free_pfn;\n\t\t}\n\t\tif (cc->migrate_pfn < start_pfn || cc->migrate_pfn >= end_pfn) {\n\t\t\tcc->migrate_pfn = start_pfn;\n\t\t\tcc->zone->compact_cached_migrate_pfn[0] = cc->migrate_pfn;\n\t\t\tcc->zone->compact_cached_migrate_pfn[1] = cc->migrate_pfn;\n\t\t}\n\n\t\tif (cc->migrate_pfn <= cc->zone->compact_init_migrate_pfn)\n\t\t\tcc->whole_zone = true;\n\t}\n\n\tlast_migrated_pfn = 0;\n\n\t \n\tupdate_cached = !sync &&\n\t\tcc->zone->compact_cached_migrate_pfn[0] == cc->zone->compact_cached_migrate_pfn[1];\n\n\ttrace_mm_compaction_begin(cc, start_pfn, end_pfn, sync);\n\n\t \n\tlru_add_drain();\n\n\twhile ((ret = compact_finished(cc)) == COMPACT_CONTINUE) {\n\t\tint err;\n\t\tunsigned long iteration_start_pfn = cc->migrate_pfn;\n\n\t\t \n\t\tcc->finish_pageblock = false;\n\t\tif (pageblock_start_pfn(last_migrated_pfn) ==\n\t\t    pageblock_start_pfn(iteration_start_pfn)) {\n\t\t\tcc->finish_pageblock = true;\n\t\t}\n\nrescan:\n\t\tswitch (isolate_migratepages(cc)) {\n\t\tcase ISOLATE_ABORT:\n\t\t\tret = COMPACT_CONTENDED;\n\t\t\tputback_movable_pages(&cc->migratepages);\n\t\t\tcc->nr_migratepages = 0;\n\t\t\tgoto out;\n\t\tcase ISOLATE_NONE:\n\t\t\tif (update_cached) {\n\t\t\t\tcc->zone->compact_cached_migrate_pfn[1] =\n\t\t\t\t\tcc->zone->compact_cached_migrate_pfn[0];\n\t\t\t}\n\n\t\t\t \n\t\t\tgoto check_drain;\n\t\tcase ISOLATE_SUCCESS:\n\t\t\tupdate_cached = false;\n\t\t\tlast_migrated_pfn = max(cc->zone->zone_start_pfn,\n\t\t\t\tpageblock_start_pfn(cc->migrate_pfn - 1));\n\t\t}\n\n\t\terr = migrate_pages(&cc->migratepages, compaction_alloc,\n\t\t\t\tcompaction_free, (unsigned long)cc, cc->mode,\n\t\t\t\tMR_COMPACTION, &nr_succeeded);\n\n\t\ttrace_mm_compaction_migratepages(cc, nr_succeeded);\n\n\t\t \n\t\tcc->nr_migratepages = 0;\n\t\tif (err) {\n\t\t\tputback_movable_pages(&cc->migratepages);\n\t\t\t \n\t\t\tif (err == -ENOMEM && !compact_scanners_met(cc)) {\n\t\t\t\tret = COMPACT_CONTENDED;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\t \n\t\t\tif (!pageblock_aligned(cc->migrate_pfn) &&\n\t\t\t    !cc->ignore_skip_hint && !cc->finish_pageblock &&\n\t\t\t    (cc->mode < MIGRATE_SYNC)) {\n\t\t\t\tcc->finish_pageblock = true;\n\n\t\t\t\t \n\t\t\t\tif (cc->order == COMPACTION_HPAGE_ORDER)\n\t\t\t\t\tlast_migrated_pfn = 0;\n\n\t\t\t\tgoto rescan;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (capc && capc->page) {\n\t\t\tret = COMPACT_SUCCESS;\n\t\t\tbreak;\n\t\t}\n\ncheck_drain:\n\t\t \n\t\tif (cc->order > 0 && last_migrated_pfn) {\n\t\t\tunsigned long current_block_start =\n\t\t\t\tblock_start_pfn(cc->migrate_pfn, cc->order);\n\n\t\t\tif (last_migrated_pfn < current_block_start) {\n\t\t\t\tlru_add_drain_cpu_zone(cc->zone);\n\t\t\t\t \n\t\t\t\tlast_migrated_pfn = 0;\n\t\t\t}\n\t\t}\n\t}\n\nout:\n\t \n\tif (cc->nr_freepages > 0) {\n\t\tunsigned long free_pfn = release_freepages(&cc->freepages);\n\n\t\tcc->nr_freepages = 0;\n\t\tVM_BUG_ON(free_pfn == 0);\n\t\t \n\t\tfree_pfn = pageblock_start_pfn(free_pfn);\n\t\t \n\t\tif (free_pfn > cc->zone->compact_cached_free_pfn)\n\t\t\tcc->zone->compact_cached_free_pfn = free_pfn;\n\t}\n\n\tcount_compact_events(COMPACTMIGRATE_SCANNED, cc->total_migrate_scanned);\n\tcount_compact_events(COMPACTFREE_SCANNED, cc->total_free_scanned);\n\n\ttrace_mm_compaction_end(cc, start_pfn, end_pfn, sync, ret);\n\n\tVM_BUG_ON(!list_empty(&cc->freepages));\n\tVM_BUG_ON(!list_empty(&cc->migratepages));\n\n\treturn ret;\n}\n\nstatic enum compact_result compact_zone_order(struct zone *zone, int order,\n\t\tgfp_t gfp_mask, enum compact_priority prio,\n\t\tunsigned int alloc_flags, int highest_zoneidx,\n\t\tstruct page **capture)\n{\n\tenum compact_result ret;\n\tstruct compact_control cc = {\n\t\t.order = order,\n\t\t.search_order = order,\n\t\t.gfp_mask = gfp_mask,\n\t\t.zone = zone,\n\t\t.mode = (prio == COMPACT_PRIO_ASYNC) ?\n\t\t\t\t\tMIGRATE_ASYNC :\tMIGRATE_SYNC_LIGHT,\n\t\t.alloc_flags = alloc_flags,\n\t\t.highest_zoneidx = highest_zoneidx,\n\t\t.direct_compaction = true,\n\t\t.whole_zone = (prio == MIN_COMPACT_PRIORITY),\n\t\t.ignore_skip_hint = (prio == MIN_COMPACT_PRIORITY),\n\t\t.ignore_block_suitable = (prio == MIN_COMPACT_PRIORITY)\n\t};\n\tstruct capture_control capc = {\n\t\t.cc = &cc,\n\t\t.page = NULL,\n\t};\n\n\t \n\tbarrier();\n\tWRITE_ONCE(current->capture_control, &capc);\n\n\tret = compact_zone(&cc, &capc);\n\n\t \n\tWRITE_ONCE(current->capture_control, NULL);\n\t*capture = READ_ONCE(capc.page);\n\t \n\tif (*capture)\n\t\tret = COMPACT_SUCCESS;\n\n\treturn ret;\n}\n\n \nenum compact_result try_to_compact_pages(gfp_t gfp_mask, unsigned int order,\n\t\tunsigned int alloc_flags, const struct alloc_context *ac,\n\t\tenum compact_priority prio, struct page **capture)\n{\n\tint may_perform_io = (__force int)(gfp_mask & __GFP_IO);\n\tstruct zoneref *z;\n\tstruct zone *zone;\n\tenum compact_result rc = COMPACT_SKIPPED;\n\n\t \n\tif (!may_perform_io)\n\t\treturn COMPACT_SKIPPED;\n\n\ttrace_mm_compaction_try_to_compact_pages(order, gfp_mask, prio);\n\n\t \n\tfor_each_zone_zonelist_nodemask(zone, z, ac->zonelist,\n\t\t\t\t\tac->highest_zoneidx, ac->nodemask) {\n\t\tenum compact_result status;\n\n\t\tif (prio > MIN_COMPACT_PRIORITY\n\t\t\t\t\t&& compaction_deferred(zone, order)) {\n\t\t\trc = max_t(enum compact_result, COMPACT_DEFERRED, rc);\n\t\t\tcontinue;\n\t\t}\n\n\t\tstatus = compact_zone_order(zone, order, gfp_mask, prio,\n\t\t\t\talloc_flags, ac->highest_zoneidx, capture);\n\t\trc = max(status, rc);\n\n\t\t \n\t\tif (status == COMPACT_SUCCESS) {\n\t\t\t \n\t\t\tcompaction_defer_reset(zone, order, false);\n\n\t\t\tbreak;\n\t\t}\n\n\t\tif (prio != COMPACT_PRIO_ASYNC && (status == COMPACT_COMPLETE ||\n\t\t\t\t\tstatus == COMPACT_PARTIAL_SKIPPED))\n\t\t\t \n\t\t\tdefer_compaction(zone, order);\n\n\t\t \n\t\tif ((prio == COMPACT_PRIO_ASYNC && need_resched())\n\t\t\t\t\t|| fatal_signal_pending(current))\n\t\t\tbreak;\n\t}\n\n\treturn rc;\n}\n\n \nstatic void proactive_compact_node(pg_data_t *pgdat)\n{\n\tint zoneid;\n\tstruct zone *zone;\n\tstruct compact_control cc = {\n\t\t.order = -1,\n\t\t.mode = MIGRATE_SYNC_LIGHT,\n\t\t.ignore_skip_hint = true,\n\t\t.whole_zone = true,\n\t\t.gfp_mask = GFP_KERNEL,\n\t\t.proactive_compaction = true,\n\t};\n\n\tfor (zoneid = 0; zoneid < MAX_NR_ZONES; zoneid++) {\n\t\tzone = &pgdat->node_zones[zoneid];\n\t\tif (!populated_zone(zone))\n\t\t\tcontinue;\n\n\t\tcc.zone = zone;\n\n\t\tcompact_zone(&cc, NULL);\n\n\t\tcount_compact_events(KCOMPACTD_MIGRATE_SCANNED,\n\t\t\t\t     cc.total_migrate_scanned);\n\t\tcount_compact_events(KCOMPACTD_FREE_SCANNED,\n\t\t\t\t     cc.total_free_scanned);\n\t}\n}\n\n \nstatic void compact_node(int nid)\n{\n\tpg_data_t *pgdat = NODE_DATA(nid);\n\tint zoneid;\n\tstruct zone *zone;\n\tstruct compact_control cc = {\n\t\t.order = -1,\n\t\t.mode = MIGRATE_SYNC,\n\t\t.ignore_skip_hint = true,\n\t\t.whole_zone = true,\n\t\t.gfp_mask = GFP_KERNEL,\n\t};\n\n\n\tfor (zoneid = 0; zoneid < MAX_NR_ZONES; zoneid++) {\n\n\t\tzone = &pgdat->node_zones[zoneid];\n\t\tif (!populated_zone(zone))\n\t\t\tcontinue;\n\n\t\tcc.zone = zone;\n\n\t\tcompact_zone(&cc, NULL);\n\t}\n}\n\n \nstatic void compact_nodes(void)\n{\n\tint nid;\n\n\t \n\tlru_add_drain_all();\n\n\tfor_each_online_node(nid)\n\t\tcompact_node(nid);\n}\n\nstatic int compaction_proactiveness_sysctl_handler(struct ctl_table *table, int write,\n\t\tvoid *buffer, size_t *length, loff_t *ppos)\n{\n\tint rc, nid;\n\n\trc = proc_dointvec_minmax(table, write, buffer, length, ppos);\n\tif (rc)\n\t\treturn rc;\n\n\tif (write && sysctl_compaction_proactiveness) {\n\t\tfor_each_online_node(nid) {\n\t\t\tpg_data_t *pgdat = NODE_DATA(nid);\n\n\t\t\tif (pgdat->proactive_compact_trigger)\n\t\t\t\tcontinue;\n\n\t\t\tpgdat->proactive_compact_trigger = true;\n\t\t\ttrace_mm_compaction_wakeup_kcompactd(pgdat->node_id, -1,\n\t\t\t\t\t\t\t     pgdat->nr_zones - 1);\n\t\t\twake_up_interruptible(&pgdat->kcompactd_wait);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic int sysctl_compaction_handler(struct ctl_table *table, int write,\n\t\t\tvoid *buffer, size_t *length, loff_t *ppos)\n{\n\tint ret;\n\n\tret = proc_dointvec(table, write, buffer, length, ppos);\n\tif (ret)\n\t\treturn ret;\n\n\tif (sysctl_compact_memory != 1)\n\t\treturn -EINVAL;\n\n\tif (write)\n\t\tcompact_nodes();\n\n\treturn 0;\n}\n\n#if defined(CONFIG_SYSFS) && defined(CONFIG_NUMA)\nstatic ssize_t compact_store(struct device *dev,\n\t\t\t     struct device_attribute *attr,\n\t\t\t     const char *buf, size_t count)\n{\n\tint nid = dev->id;\n\n\tif (nid >= 0 && nid < nr_node_ids && node_online(nid)) {\n\t\t \n\t\tlru_add_drain_all();\n\n\t\tcompact_node(nid);\n\t}\n\n\treturn count;\n}\nstatic DEVICE_ATTR_WO(compact);\n\nint compaction_register_node(struct node *node)\n{\n\treturn device_create_file(&node->dev, &dev_attr_compact);\n}\n\nvoid compaction_unregister_node(struct node *node)\n{\n\tdevice_remove_file(&node->dev, &dev_attr_compact);\n}\n#endif  \n\nstatic inline bool kcompactd_work_requested(pg_data_t *pgdat)\n{\n\treturn pgdat->kcompactd_max_order > 0 || kthread_should_stop() ||\n\t\tpgdat->proactive_compact_trigger;\n}\n\nstatic bool kcompactd_node_suitable(pg_data_t *pgdat)\n{\n\tint zoneid;\n\tstruct zone *zone;\n\tenum zone_type highest_zoneidx = pgdat->kcompactd_highest_zoneidx;\n\n\tfor (zoneid = 0; zoneid <= highest_zoneidx; zoneid++) {\n\t\tzone = &pgdat->node_zones[zoneid];\n\n\t\tif (!populated_zone(zone))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (zone_watermark_ok(zone, pgdat->kcompactd_max_order,\n\t\t\t\t      min_wmark_pages(zone),\n\t\t\t\t      highest_zoneidx, 0))\n\t\t\tcontinue;\n\n\t\tif (compaction_suitable(zone, pgdat->kcompactd_max_order,\n\t\t\t\t\thighest_zoneidx))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic void kcompactd_do_work(pg_data_t *pgdat)\n{\n\t \n\tint zoneid;\n\tstruct zone *zone;\n\tstruct compact_control cc = {\n\t\t.order = pgdat->kcompactd_max_order,\n\t\t.search_order = pgdat->kcompactd_max_order,\n\t\t.highest_zoneidx = pgdat->kcompactd_highest_zoneidx,\n\t\t.mode = MIGRATE_SYNC_LIGHT,\n\t\t.ignore_skip_hint = false,\n\t\t.gfp_mask = GFP_KERNEL,\n\t};\n\ttrace_mm_compaction_kcompactd_wake(pgdat->node_id, cc.order,\n\t\t\t\t\t\t\tcc.highest_zoneidx);\n\tcount_compact_event(KCOMPACTD_WAKE);\n\n\tfor (zoneid = 0; zoneid <= cc.highest_zoneidx; zoneid++) {\n\t\tint status;\n\n\t\tzone = &pgdat->node_zones[zoneid];\n\t\tif (!populated_zone(zone))\n\t\t\tcontinue;\n\n\t\tif (compaction_deferred(zone, cc.order))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (zone_watermark_ok(zone, cc.order,\n\t\t\t\t      min_wmark_pages(zone), zoneid, 0))\n\t\t\tcontinue;\n\n\t\tif (!compaction_suitable(zone, cc.order, zoneid))\n\t\t\tcontinue;\n\n\t\tif (kthread_should_stop())\n\t\t\treturn;\n\n\t\tcc.zone = zone;\n\t\tstatus = compact_zone(&cc, NULL);\n\n\t\tif (status == COMPACT_SUCCESS) {\n\t\t\tcompaction_defer_reset(zone, cc.order, false);\n\t\t} else if (status == COMPACT_PARTIAL_SKIPPED || status == COMPACT_COMPLETE) {\n\t\t\t \n\t\t\tdrain_all_pages(zone);\n\n\t\t\t \n\t\t\tdefer_compaction(zone, cc.order);\n\t\t}\n\n\t\tcount_compact_events(KCOMPACTD_MIGRATE_SCANNED,\n\t\t\t\t     cc.total_migrate_scanned);\n\t\tcount_compact_events(KCOMPACTD_FREE_SCANNED,\n\t\t\t\t     cc.total_free_scanned);\n\t}\n\n\t \n\tif (pgdat->kcompactd_max_order <= cc.order)\n\t\tpgdat->kcompactd_max_order = 0;\n\tif (pgdat->kcompactd_highest_zoneidx >= cc.highest_zoneidx)\n\t\tpgdat->kcompactd_highest_zoneidx = pgdat->nr_zones - 1;\n}\n\nvoid wakeup_kcompactd(pg_data_t *pgdat, int order, int highest_zoneidx)\n{\n\tif (!order)\n\t\treturn;\n\n\tif (pgdat->kcompactd_max_order < order)\n\t\tpgdat->kcompactd_max_order = order;\n\n\tif (pgdat->kcompactd_highest_zoneidx > highest_zoneidx)\n\t\tpgdat->kcompactd_highest_zoneidx = highest_zoneidx;\n\n\t \n\tif (!wq_has_sleeper(&pgdat->kcompactd_wait))\n\t\treturn;\n\n\tif (!kcompactd_node_suitable(pgdat))\n\t\treturn;\n\n\ttrace_mm_compaction_wakeup_kcompactd(pgdat->node_id, order,\n\t\t\t\t\t\t\thighest_zoneidx);\n\twake_up_interruptible(&pgdat->kcompactd_wait);\n}\n\n \nstatic int kcompactd(void *p)\n{\n\tpg_data_t *pgdat = (pg_data_t *)p;\n\tstruct task_struct *tsk = current;\n\tlong default_timeout = msecs_to_jiffies(HPAGE_FRAG_CHECK_INTERVAL_MSEC);\n\tlong timeout = default_timeout;\n\n\tconst struct cpumask *cpumask = cpumask_of_node(pgdat->node_id);\n\n\tif (!cpumask_empty(cpumask))\n\t\tset_cpus_allowed_ptr(tsk, cpumask);\n\n\tset_freezable();\n\n\tpgdat->kcompactd_max_order = 0;\n\tpgdat->kcompactd_highest_zoneidx = pgdat->nr_zones - 1;\n\n\twhile (!kthread_should_stop()) {\n\t\tunsigned long pflags;\n\n\t\t \n\t\tif (!sysctl_compaction_proactiveness)\n\t\t\ttimeout = MAX_SCHEDULE_TIMEOUT;\n\t\ttrace_mm_compaction_kcompactd_sleep(pgdat->node_id);\n\t\tif (wait_event_freezable_timeout(pgdat->kcompactd_wait,\n\t\t\tkcompactd_work_requested(pgdat), timeout) &&\n\t\t\t!pgdat->proactive_compact_trigger) {\n\n\t\t\tpsi_memstall_enter(&pflags);\n\t\t\tkcompactd_do_work(pgdat);\n\t\t\tpsi_memstall_leave(&pflags);\n\t\t\t \n\t\t\ttimeout = default_timeout;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\ttimeout = default_timeout;\n\t\tif (should_proactive_compact_node(pgdat)) {\n\t\t\tunsigned int prev_score, score;\n\n\t\t\tprev_score = fragmentation_score_node(pgdat);\n\t\t\tproactive_compact_node(pgdat);\n\t\t\tscore = fragmentation_score_node(pgdat);\n\t\t\t \n\t\t\tif (unlikely(score >= prev_score))\n\t\t\t\ttimeout =\n\t\t\t\t   default_timeout << COMPACT_MAX_DEFER_SHIFT;\n\t\t}\n\t\tif (unlikely(pgdat->proactive_compact_trigger))\n\t\t\tpgdat->proactive_compact_trigger = false;\n\t}\n\n\treturn 0;\n}\n\n \nvoid __meminit kcompactd_run(int nid)\n{\n\tpg_data_t *pgdat = NODE_DATA(nid);\n\n\tif (pgdat->kcompactd)\n\t\treturn;\n\n\tpgdat->kcompactd = kthread_run(kcompactd, pgdat, \"kcompactd%d\", nid);\n\tif (IS_ERR(pgdat->kcompactd)) {\n\t\tpr_err(\"Failed to start kcompactd on node %d\\n\", nid);\n\t\tpgdat->kcompactd = NULL;\n\t}\n}\n\n \nvoid __meminit kcompactd_stop(int nid)\n{\n\tstruct task_struct *kcompactd = NODE_DATA(nid)->kcompactd;\n\n\tif (kcompactd) {\n\t\tkthread_stop(kcompactd);\n\t\tNODE_DATA(nid)->kcompactd = NULL;\n\t}\n}\n\n \nstatic int kcompactd_cpu_online(unsigned int cpu)\n{\n\tint nid;\n\n\tfor_each_node_state(nid, N_MEMORY) {\n\t\tpg_data_t *pgdat = NODE_DATA(nid);\n\t\tconst struct cpumask *mask;\n\n\t\tmask = cpumask_of_node(pgdat->node_id);\n\n\t\tif (cpumask_any_and(cpu_online_mask, mask) < nr_cpu_ids)\n\t\t\t \n\t\t\tif (pgdat->kcompactd)\n\t\t\t\tset_cpus_allowed_ptr(pgdat->kcompactd, mask);\n\t}\n\treturn 0;\n}\n\nstatic int proc_dointvec_minmax_warn_RT_change(struct ctl_table *table,\n\t\tint write, void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tint ret, old;\n\n\tif (!IS_ENABLED(CONFIG_PREEMPT_RT) || !write)\n\t\treturn proc_dointvec_minmax(table, write, buffer, lenp, ppos);\n\n\told = *(int *)table->data;\n\tret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);\n\tif (ret)\n\t\treturn ret;\n\tif (old != *(int *)table->data)\n\t\tpr_warn_once(\"sysctl attribute %s changed by %s[%d]\\n\",\n\t\t\t     table->procname, current->comm,\n\t\t\t     task_pid_nr(current));\n\treturn ret;\n}\n\nstatic struct ctl_table vm_compaction[] = {\n\t{\n\t\t.procname\t= \"compact_memory\",\n\t\t.data\t\t= &sysctl_compact_memory,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0200,\n\t\t.proc_handler\t= sysctl_compaction_handler,\n\t},\n\t{\n\t\t.procname\t= \"compaction_proactiveness\",\n\t\t.data\t\t= &sysctl_compaction_proactiveness,\n\t\t.maxlen\t\t= sizeof(sysctl_compaction_proactiveness),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= compaction_proactiveness_sysctl_handler,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_ONE_HUNDRED,\n\t},\n\t{\n\t\t.procname\t= \"extfrag_threshold\",\n\t\t.data\t\t= &sysctl_extfrag_threshold,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_ONE_THOUSAND,\n\t},\n\t{\n\t\t.procname\t= \"compact_unevictable_allowed\",\n\t\t.data\t\t= &sysctl_compact_unevictable_allowed,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax_warn_RT_change,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_ONE,\n\t},\n\t{ }\n};\n\nstatic int __init kcompactd_init(void)\n{\n\tint nid;\n\tint ret;\n\n\tret = cpuhp_setup_state_nocalls(CPUHP_AP_ONLINE_DYN,\n\t\t\t\t\t\"mm/compaction:online\",\n\t\t\t\t\tkcompactd_cpu_online, NULL);\n\tif (ret < 0) {\n\t\tpr_err(\"kcompactd: failed to register hotplug callbacks.\\n\");\n\t\treturn ret;\n\t}\n\n\tfor_each_node_state(nid, N_MEMORY)\n\t\tkcompactd_run(nid);\n\tregister_sysctl_init(\"vm\", vm_compaction);\n\treturn 0;\n}\nsubsys_initcall(kcompactd_init)\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}