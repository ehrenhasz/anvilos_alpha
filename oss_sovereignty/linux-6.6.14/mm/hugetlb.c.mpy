{
  "module_name": "hugetlb.c",
  "hash_id": "00122813c86e99f2b98fc9d96ed8b163ac096919bf2282be240be399538f72de",
  "original_prompt": "Ingested from linux-6.6.14/mm/hugetlb.c",
  "human_readable_source": "\n \n#include <linux/list.h>\n#include <linux/init.h>\n#include <linux/mm.h>\n#include <linux/seq_file.h>\n#include <linux/sysctl.h>\n#include <linux/highmem.h>\n#include <linux/mmu_notifier.h>\n#include <linux/nodemask.h>\n#include <linux/pagemap.h>\n#include <linux/mempolicy.h>\n#include <linux/compiler.h>\n#include <linux/cpuset.h>\n#include <linux/mutex.h>\n#include <linux/memblock.h>\n#include <linux/sysfs.h>\n#include <linux/slab.h>\n#include <linux/sched/mm.h>\n#include <linux/mmdebug.h>\n#include <linux/sched/signal.h>\n#include <linux/rmap.h>\n#include <linux/string_helpers.h>\n#include <linux/swap.h>\n#include <linux/swapops.h>\n#include <linux/jhash.h>\n#include <linux/numa.h>\n#include <linux/llist.h>\n#include <linux/cma.h>\n#include <linux/migrate.h>\n#include <linux/nospec.h>\n#include <linux/delayacct.h>\n#include <linux/memory.h>\n#include <linux/mm_inline.h>\n\n#include <asm/page.h>\n#include <asm/pgalloc.h>\n#include <asm/tlb.h>\n\n#include <linux/io.h>\n#include <linux/hugetlb.h>\n#include <linux/hugetlb_cgroup.h>\n#include <linux/node.h>\n#include <linux/page_owner.h>\n#include \"internal.h\"\n#include \"hugetlb_vmemmap.h\"\n\nint hugetlb_max_hstate __read_mostly;\nunsigned int default_hstate_idx;\nstruct hstate hstates[HUGE_MAX_HSTATE];\n\n#ifdef CONFIG_CMA\nstatic struct cma *hugetlb_cma[MAX_NUMNODES];\nstatic unsigned long hugetlb_cma_size_in_node[MAX_NUMNODES] __initdata;\nstatic bool hugetlb_cma_folio(struct folio *folio, unsigned int order)\n{\n\treturn cma_pages_valid(hugetlb_cma[folio_nid(folio)], &folio->page,\n\t\t\t\t1 << order);\n}\n#else\nstatic bool hugetlb_cma_folio(struct folio *folio, unsigned int order)\n{\n\treturn false;\n}\n#endif\nstatic unsigned long hugetlb_cma_size __initdata;\n\n__initdata LIST_HEAD(huge_boot_pages);\n\n \nstatic struct hstate * __initdata parsed_hstate;\nstatic unsigned long __initdata default_hstate_max_huge_pages;\nstatic bool __initdata parsed_valid_hugepagesz = true;\nstatic bool __initdata parsed_default_hugepagesz;\nstatic unsigned int default_hugepages_in_node[MAX_NUMNODES] __initdata;\n\n \nDEFINE_SPINLOCK(hugetlb_lock);\n\n \nstatic int num_fault_mutexes;\nstruct mutex *hugetlb_fault_mutex_table ____cacheline_aligned_in_smp;\n\n \nstatic int hugetlb_acct_memory(struct hstate *h, long delta);\nstatic void hugetlb_vma_lock_free(struct vm_area_struct *vma);\nstatic void hugetlb_vma_lock_alloc(struct vm_area_struct *vma);\nstatic void __hugetlb_vma_unlock_write_free(struct vm_area_struct *vma);\nstatic void hugetlb_unshare_pmds(struct vm_area_struct *vma,\n\t\tunsigned long start, unsigned long end);\nstatic struct resv_map *vma_resv_map(struct vm_area_struct *vma);\n\nstatic inline bool subpool_is_free(struct hugepage_subpool *spool)\n{\n\tif (spool->count)\n\t\treturn false;\n\tif (spool->max_hpages != -1)\n\t\treturn spool->used_hpages == 0;\n\tif (spool->min_hpages != -1)\n\t\treturn spool->rsv_hpages == spool->min_hpages;\n\n\treturn true;\n}\n\nstatic inline void unlock_or_release_subpool(struct hugepage_subpool *spool,\n\t\t\t\t\t\tunsigned long irq_flags)\n{\n\tspin_unlock_irqrestore(&spool->lock, irq_flags);\n\n\t \n\tif (subpool_is_free(spool)) {\n\t\tif (spool->min_hpages != -1)\n\t\t\thugetlb_acct_memory(spool->hstate,\n\t\t\t\t\t\t-spool->min_hpages);\n\t\tkfree(spool);\n\t}\n}\n\nstruct hugepage_subpool *hugepage_new_subpool(struct hstate *h, long max_hpages,\n\t\t\t\t\t\tlong min_hpages)\n{\n\tstruct hugepage_subpool *spool;\n\n\tspool = kzalloc(sizeof(*spool), GFP_KERNEL);\n\tif (!spool)\n\t\treturn NULL;\n\n\tspin_lock_init(&spool->lock);\n\tspool->count = 1;\n\tspool->max_hpages = max_hpages;\n\tspool->hstate = h;\n\tspool->min_hpages = min_hpages;\n\n\tif (min_hpages != -1 && hugetlb_acct_memory(h, min_hpages)) {\n\t\tkfree(spool);\n\t\treturn NULL;\n\t}\n\tspool->rsv_hpages = min_hpages;\n\n\treturn spool;\n}\n\nvoid hugepage_put_subpool(struct hugepage_subpool *spool)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&spool->lock, flags);\n\tBUG_ON(!spool->count);\n\tspool->count--;\n\tunlock_or_release_subpool(spool, flags);\n}\n\n \nstatic long hugepage_subpool_get_pages(struct hugepage_subpool *spool,\n\t\t\t\t      long delta)\n{\n\tlong ret = delta;\n\n\tif (!spool)\n\t\treturn ret;\n\n\tspin_lock_irq(&spool->lock);\n\n\tif (spool->max_hpages != -1) {\t\t \n\t\tif ((spool->used_hpages + delta) <= spool->max_hpages)\n\t\t\tspool->used_hpages += delta;\n\t\telse {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto unlock_ret;\n\t\t}\n\t}\n\n\t \n\tif (spool->min_hpages != -1 && spool->rsv_hpages) {\n\t\tif (delta > spool->rsv_hpages) {\n\t\t\t \n\t\t\tret = delta - spool->rsv_hpages;\n\t\t\tspool->rsv_hpages = 0;\n\t\t} else {\n\t\t\tret = 0;\t \n\t\t\tspool->rsv_hpages -= delta;\n\t\t}\n\t}\n\nunlock_ret:\n\tspin_unlock_irq(&spool->lock);\n\treturn ret;\n}\n\n \nstatic long hugepage_subpool_put_pages(struct hugepage_subpool *spool,\n\t\t\t\t       long delta)\n{\n\tlong ret = delta;\n\tunsigned long flags;\n\n\tif (!spool)\n\t\treturn delta;\n\n\tspin_lock_irqsave(&spool->lock, flags);\n\n\tif (spool->max_hpages != -1)\t\t \n\t\tspool->used_hpages -= delta;\n\n\t  \n\tif (spool->min_hpages != -1 && spool->used_hpages < spool->min_hpages) {\n\t\tif (spool->rsv_hpages + delta <= spool->min_hpages)\n\t\t\tret = 0;\n\t\telse\n\t\t\tret = spool->rsv_hpages + delta - spool->min_hpages;\n\n\t\tspool->rsv_hpages += delta;\n\t\tif (spool->rsv_hpages > spool->min_hpages)\n\t\t\tspool->rsv_hpages = spool->min_hpages;\n\t}\n\n\t \n\tunlock_or_release_subpool(spool, flags);\n\n\treturn ret;\n}\n\nstatic inline struct hugepage_subpool *subpool_inode(struct inode *inode)\n{\n\treturn HUGETLBFS_SB(inode->i_sb)->spool;\n}\n\nstatic inline struct hugepage_subpool *subpool_vma(struct vm_area_struct *vma)\n{\n\treturn subpool_inode(file_inode(vma->vm_file));\n}\n\n \nvoid hugetlb_vma_lock_read(struct vm_area_struct *vma)\n{\n\tif (__vma_shareable_lock(vma)) {\n\t\tstruct hugetlb_vma_lock *vma_lock = vma->vm_private_data;\n\n\t\tdown_read(&vma_lock->rw_sema);\n\t} else if (__vma_private_lock(vma)) {\n\t\tstruct resv_map *resv_map = vma_resv_map(vma);\n\n\t\tdown_read(&resv_map->rw_sema);\n\t}\n}\n\nvoid hugetlb_vma_unlock_read(struct vm_area_struct *vma)\n{\n\tif (__vma_shareable_lock(vma)) {\n\t\tstruct hugetlb_vma_lock *vma_lock = vma->vm_private_data;\n\n\t\tup_read(&vma_lock->rw_sema);\n\t} else if (__vma_private_lock(vma)) {\n\t\tstruct resv_map *resv_map = vma_resv_map(vma);\n\n\t\tup_read(&resv_map->rw_sema);\n\t}\n}\n\nvoid hugetlb_vma_lock_write(struct vm_area_struct *vma)\n{\n\tif (__vma_shareable_lock(vma)) {\n\t\tstruct hugetlb_vma_lock *vma_lock = vma->vm_private_data;\n\n\t\tdown_write(&vma_lock->rw_sema);\n\t} else if (__vma_private_lock(vma)) {\n\t\tstruct resv_map *resv_map = vma_resv_map(vma);\n\n\t\tdown_write(&resv_map->rw_sema);\n\t}\n}\n\nvoid hugetlb_vma_unlock_write(struct vm_area_struct *vma)\n{\n\tif (__vma_shareable_lock(vma)) {\n\t\tstruct hugetlb_vma_lock *vma_lock = vma->vm_private_data;\n\n\t\tup_write(&vma_lock->rw_sema);\n\t} else if (__vma_private_lock(vma)) {\n\t\tstruct resv_map *resv_map = vma_resv_map(vma);\n\n\t\tup_write(&resv_map->rw_sema);\n\t}\n}\n\nint hugetlb_vma_trylock_write(struct vm_area_struct *vma)\n{\n\n\tif (__vma_shareable_lock(vma)) {\n\t\tstruct hugetlb_vma_lock *vma_lock = vma->vm_private_data;\n\n\t\treturn down_write_trylock(&vma_lock->rw_sema);\n\t} else if (__vma_private_lock(vma)) {\n\t\tstruct resv_map *resv_map = vma_resv_map(vma);\n\n\t\treturn down_write_trylock(&resv_map->rw_sema);\n\t}\n\n\treturn 1;\n}\n\nvoid hugetlb_vma_assert_locked(struct vm_area_struct *vma)\n{\n\tif (__vma_shareable_lock(vma)) {\n\t\tstruct hugetlb_vma_lock *vma_lock = vma->vm_private_data;\n\n\t\tlockdep_assert_held(&vma_lock->rw_sema);\n\t} else if (__vma_private_lock(vma)) {\n\t\tstruct resv_map *resv_map = vma_resv_map(vma);\n\n\t\tlockdep_assert_held(&resv_map->rw_sema);\n\t}\n}\n\nvoid hugetlb_vma_lock_release(struct kref *kref)\n{\n\tstruct hugetlb_vma_lock *vma_lock = container_of(kref,\n\t\t\tstruct hugetlb_vma_lock, refs);\n\n\tkfree(vma_lock);\n}\n\nstatic void __hugetlb_vma_unlock_write_put(struct hugetlb_vma_lock *vma_lock)\n{\n\tstruct vm_area_struct *vma = vma_lock->vma;\n\n\t \n\tvma_lock->vma = NULL;\n\tvma->vm_private_data = NULL;\n\tup_write(&vma_lock->rw_sema);\n\tkref_put(&vma_lock->refs, hugetlb_vma_lock_release);\n}\n\nstatic void __hugetlb_vma_unlock_write_free(struct vm_area_struct *vma)\n{\n\tif (__vma_shareable_lock(vma)) {\n\t\tstruct hugetlb_vma_lock *vma_lock = vma->vm_private_data;\n\n\t\t__hugetlb_vma_unlock_write_put(vma_lock);\n\t} else if (__vma_private_lock(vma)) {\n\t\tstruct resv_map *resv_map = vma_resv_map(vma);\n\n\t\t \n\t\tup_write(&resv_map->rw_sema);\n\t}\n}\n\nstatic void hugetlb_vma_lock_free(struct vm_area_struct *vma)\n{\n\t \n\tif (!vma || !__vma_shareable_lock(vma))\n\t\treturn;\n\n\tif (vma->vm_private_data) {\n\t\tstruct hugetlb_vma_lock *vma_lock = vma->vm_private_data;\n\n\t\tdown_write(&vma_lock->rw_sema);\n\t\t__hugetlb_vma_unlock_write_put(vma_lock);\n\t}\n}\n\nstatic void hugetlb_vma_lock_alloc(struct vm_area_struct *vma)\n{\n\tstruct hugetlb_vma_lock *vma_lock;\n\n\t \n\tif (!vma || !(vma->vm_flags & VM_MAYSHARE))\n\t\treturn;\n\n\t \n\tif (vma->vm_private_data)\n\t\treturn;\n\n\tvma_lock = kmalloc(sizeof(*vma_lock), GFP_KERNEL);\n\tif (!vma_lock) {\n\t\t \n\t\tpr_warn_once(\"HugeTLB: unable to allocate vma specific lock\\n\");\n\t\treturn;\n\t}\n\n\tkref_init(&vma_lock->refs);\n\tinit_rwsem(&vma_lock->rw_sema);\n\tvma_lock->vma = vma;\n\tvma->vm_private_data = vma_lock;\n}\n\n \nstatic struct file_region *\nget_file_region_entry_from_cache(struct resv_map *resv, long from, long to)\n{\n\tstruct file_region *nrg;\n\n\tVM_BUG_ON(resv->region_cache_count <= 0);\n\n\tresv->region_cache_count--;\n\tnrg = list_first_entry(&resv->region_cache, struct file_region, link);\n\tlist_del(&nrg->link);\n\n\tnrg->from = from;\n\tnrg->to = to;\n\n\treturn nrg;\n}\n\nstatic void copy_hugetlb_cgroup_uncharge_info(struct file_region *nrg,\n\t\t\t\t\t      struct file_region *rg)\n{\n#ifdef CONFIG_CGROUP_HUGETLB\n\tnrg->reservation_counter = rg->reservation_counter;\n\tnrg->css = rg->css;\n\tif (rg->css)\n\t\tcss_get(rg->css);\n#endif\n}\n\n \nstatic void record_hugetlb_cgroup_uncharge_info(struct hugetlb_cgroup *h_cg,\n\t\t\t\t\t\tstruct hstate *h,\n\t\t\t\t\t\tstruct resv_map *resv,\n\t\t\t\t\t\tstruct file_region *nrg)\n{\n#ifdef CONFIG_CGROUP_HUGETLB\n\tif (h_cg) {\n\t\tnrg->reservation_counter =\n\t\t\t&h_cg->rsvd_hugepage[hstate_index(h)];\n\t\tnrg->css = &h_cg->css;\n\t\t \n\t\tcss_get(&h_cg->css);\n\t\tif (!resv->pages_per_hpage)\n\t\t\tresv->pages_per_hpage = pages_per_huge_page(h);\n\t\t \n\t\tVM_BUG_ON(resv->pages_per_hpage != pages_per_huge_page(h));\n\t} else {\n\t\tnrg->reservation_counter = NULL;\n\t\tnrg->css = NULL;\n\t}\n#endif\n}\n\nstatic void put_uncharge_info(struct file_region *rg)\n{\n#ifdef CONFIG_CGROUP_HUGETLB\n\tif (rg->css)\n\t\tcss_put(rg->css);\n#endif\n}\n\nstatic bool has_same_uncharge_info(struct file_region *rg,\n\t\t\t\t   struct file_region *org)\n{\n#ifdef CONFIG_CGROUP_HUGETLB\n\treturn rg->reservation_counter == org->reservation_counter &&\n\t       rg->css == org->css;\n\n#else\n\treturn true;\n#endif\n}\n\nstatic void coalesce_file_region(struct resv_map *resv, struct file_region *rg)\n{\n\tstruct file_region *nrg, *prg;\n\n\tprg = list_prev_entry(rg, link);\n\tif (&prg->link != &resv->regions && prg->to == rg->from &&\n\t    has_same_uncharge_info(prg, rg)) {\n\t\tprg->to = rg->to;\n\n\t\tlist_del(&rg->link);\n\t\tput_uncharge_info(rg);\n\t\tkfree(rg);\n\n\t\trg = prg;\n\t}\n\n\tnrg = list_next_entry(rg, link);\n\tif (&nrg->link != &resv->regions && nrg->from == rg->to &&\n\t    has_same_uncharge_info(nrg, rg)) {\n\t\tnrg->from = rg->from;\n\n\t\tlist_del(&rg->link);\n\t\tput_uncharge_info(rg);\n\t\tkfree(rg);\n\t}\n}\n\nstatic inline long\nhugetlb_resv_map_add(struct resv_map *map, struct list_head *rg, long from,\n\t\t     long to, struct hstate *h, struct hugetlb_cgroup *cg,\n\t\t     long *regions_needed)\n{\n\tstruct file_region *nrg;\n\n\tif (!regions_needed) {\n\t\tnrg = get_file_region_entry_from_cache(map, from, to);\n\t\trecord_hugetlb_cgroup_uncharge_info(cg, h, map, nrg);\n\t\tlist_add(&nrg->link, rg);\n\t\tcoalesce_file_region(map, nrg);\n\t} else\n\t\t*regions_needed += 1;\n\n\treturn to - from;\n}\n\n \nstatic long add_reservation_in_range(struct resv_map *resv, long f, long t,\n\t\t\t\t     struct hugetlb_cgroup *h_cg,\n\t\t\t\t     struct hstate *h, long *regions_needed)\n{\n\tlong add = 0;\n\tstruct list_head *head = &resv->regions;\n\tlong last_accounted_offset = f;\n\tstruct file_region *iter, *trg = NULL;\n\tstruct list_head *rg = NULL;\n\n\tif (regions_needed)\n\t\t*regions_needed = 0;\n\n\t \n\tlist_for_each_entry_safe(iter, trg, head, link) {\n\t\t \n\t\tif (iter->from < f) {\n\t\t\t \n\t\t\tif (iter->to > last_accounted_offset)\n\t\t\t\tlast_accounted_offset = iter->to;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (iter->from >= t) {\n\t\t\trg = iter->link.prev;\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (iter->from > last_accounted_offset)\n\t\t\tadd += hugetlb_resv_map_add(resv, iter->link.prev,\n\t\t\t\t\t\t    last_accounted_offset,\n\t\t\t\t\t\t    iter->from, h, h_cg,\n\t\t\t\t\t\t    regions_needed);\n\n\t\tlast_accounted_offset = iter->to;\n\t}\n\n\t \n\tif (!rg)\n\t\trg = head->prev;\n\tif (last_accounted_offset < t)\n\t\tadd += hugetlb_resv_map_add(resv, rg, last_accounted_offset,\n\t\t\t\t\t    t, h, h_cg, regions_needed);\n\n\treturn add;\n}\n\n \nstatic int allocate_file_region_entries(struct resv_map *resv,\n\t\t\t\t\tint regions_needed)\n\t__must_hold(&resv->lock)\n{\n\tLIST_HEAD(allocated_regions);\n\tint to_allocate = 0, i = 0;\n\tstruct file_region *trg = NULL, *rg = NULL;\n\n\tVM_BUG_ON(regions_needed < 0);\n\n\t \n\twhile (resv->region_cache_count <\n\t       (resv->adds_in_progress + regions_needed)) {\n\t\tto_allocate = resv->adds_in_progress + regions_needed -\n\t\t\t      resv->region_cache_count;\n\n\t\t \n\t\tVM_BUG_ON(resv->region_cache_count < resv->adds_in_progress);\n\n\t\tspin_unlock(&resv->lock);\n\t\tfor (i = 0; i < to_allocate; i++) {\n\t\t\ttrg = kmalloc(sizeof(*trg), GFP_KERNEL);\n\t\t\tif (!trg)\n\t\t\t\tgoto out_of_memory;\n\t\t\tlist_add(&trg->link, &allocated_regions);\n\t\t}\n\n\t\tspin_lock(&resv->lock);\n\n\t\tlist_splice(&allocated_regions, &resv->region_cache);\n\t\tresv->region_cache_count += to_allocate;\n\t}\n\n\treturn 0;\n\nout_of_memory:\n\tlist_for_each_entry_safe(rg, trg, &allocated_regions, link) {\n\t\tlist_del(&rg->link);\n\t\tkfree(rg);\n\t}\n\treturn -ENOMEM;\n}\n\n \nstatic long region_add(struct resv_map *resv, long f, long t,\n\t\t       long in_regions_needed, struct hstate *h,\n\t\t       struct hugetlb_cgroup *h_cg)\n{\n\tlong add = 0, actual_regions_needed = 0;\n\n\tspin_lock(&resv->lock);\nretry:\n\n\t \n\tadd_reservation_in_range(resv, f, t, NULL, NULL,\n\t\t\t\t &actual_regions_needed);\n\n\t \n\tif (actual_regions_needed > in_regions_needed &&\n\t    resv->region_cache_count <\n\t\t    resv->adds_in_progress +\n\t\t\t    (actual_regions_needed - in_regions_needed)) {\n\t\t \n\t\tVM_BUG_ON(t - f <= 1);\n\n\t\tif (allocate_file_region_entries(\n\t\t\t    resv, actual_regions_needed - in_regions_needed)) {\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tgoto retry;\n\t}\n\n\tadd = add_reservation_in_range(resv, f, t, h_cg, h, NULL);\n\n\tresv->adds_in_progress -= in_regions_needed;\n\n\tspin_unlock(&resv->lock);\n\treturn add;\n}\n\n \nstatic long region_chg(struct resv_map *resv, long f, long t,\n\t\t       long *out_regions_needed)\n{\n\tlong chg = 0;\n\n\tspin_lock(&resv->lock);\n\n\t \n\tchg = add_reservation_in_range(resv, f, t, NULL, NULL,\n\t\t\t\t       out_regions_needed);\n\n\tif (*out_regions_needed == 0)\n\t\t*out_regions_needed = 1;\n\n\tif (allocate_file_region_entries(resv, *out_regions_needed))\n\t\treturn -ENOMEM;\n\n\tresv->adds_in_progress += *out_regions_needed;\n\n\tspin_unlock(&resv->lock);\n\treturn chg;\n}\n\n \nstatic void region_abort(struct resv_map *resv, long f, long t,\n\t\t\t long regions_needed)\n{\n\tspin_lock(&resv->lock);\n\tVM_BUG_ON(!resv->region_cache_count);\n\tresv->adds_in_progress -= regions_needed;\n\tspin_unlock(&resv->lock);\n}\n\n \nstatic long region_del(struct resv_map *resv, long f, long t)\n{\n\tstruct list_head *head = &resv->regions;\n\tstruct file_region *rg, *trg;\n\tstruct file_region *nrg = NULL;\n\tlong del = 0;\n\nretry:\n\tspin_lock(&resv->lock);\n\tlist_for_each_entry_safe(rg, trg, head, link) {\n\t\t \n\t\tif (rg->to <= f && (rg->to != rg->from || rg->to != f))\n\t\t\tcontinue;\n\n\t\tif (rg->from >= t)\n\t\t\tbreak;\n\n\t\tif (f > rg->from && t < rg->to) {  \n\t\t\t \n\t\t\tif (!nrg &&\n\t\t\t    resv->region_cache_count > resv->adds_in_progress) {\n\t\t\t\tnrg = list_first_entry(&resv->region_cache,\n\t\t\t\t\t\t\tstruct file_region,\n\t\t\t\t\t\t\tlink);\n\t\t\t\tlist_del(&nrg->link);\n\t\t\t\tresv->region_cache_count--;\n\t\t\t}\n\n\t\t\tif (!nrg) {\n\t\t\t\tspin_unlock(&resv->lock);\n\t\t\t\tnrg = kmalloc(sizeof(*nrg), GFP_KERNEL);\n\t\t\t\tif (!nrg)\n\t\t\t\t\treturn -ENOMEM;\n\t\t\t\tgoto retry;\n\t\t\t}\n\n\t\t\tdel += t - f;\n\t\t\thugetlb_cgroup_uncharge_file_region(\n\t\t\t\tresv, rg, t - f, false);\n\n\t\t\t \n\t\t\tnrg->from = t;\n\t\t\tnrg->to = rg->to;\n\n\t\t\tcopy_hugetlb_cgroup_uncharge_info(nrg, rg);\n\n\t\t\tINIT_LIST_HEAD(&nrg->link);\n\n\t\t\t \n\t\t\trg->to = f;\n\n\t\t\tlist_add(&nrg->link, &rg->link);\n\t\t\tnrg = NULL;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (f <= rg->from && t >= rg->to) {  \n\t\t\tdel += rg->to - rg->from;\n\t\t\thugetlb_cgroup_uncharge_file_region(resv, rg,\n\t\t\t\t\t\t\t    rg->to - rg->from, true);\n\t\t\tlist_del(&rg->link);\n\t\t\tkfree(rg);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (f <= rg->from) {\t \n\t\t\thugetlb_cgroup_uncharge_file_region(resv, rg,\n\t\t\t\t\t\t\t    t - rg->from, false);\n\n\t\t\tdel += t - rg->from;\n\t\t\trg->from = t;\n\t\t} else {\t\t \n\t\t\thugetlb_cgroup_uncharge_file_region(resv, rg,\n\t\t\t\t\t\t\t    rg->to - f, false);\n\n\t\t\tdel += rg->to - f;\n\t\t\trg->to = f;\n\t\t}\n\t}\n\n\tspin_unlock(&resv->lock);\n\tkfree(nrg);\n\treturn del;\n}\n\n \nvoid hugetlb_fix_reserve_counts(struct inode *inode)\n{\n\tstruct hugepage_subpool *spool = subpool_inode(inode);\n\tlong rsv_adjust;\n\tbool reserved = false;\n\n\trsv_adjust = hugepage_subpool_get_pages(spool, 1);\n\tif (rsv_adjust > 0) {\n\t\tstruct hstate *h = hstate_inode(inode);\n\n\t\tif (!hugetlb_acct_memory(h, 1))\n\t\t\treserved = true;\n\t} else if (!rsv_adjust) {\n\t\treserved = true;\n\t}\n\n\tif (!reserved)\n\t\tpr_warn(\"hugetlb: Huge Page Reserved count may go negative.\\n\");\n}\n\n \nstatic long region_count(struct resv_map *resv, long f, long t)\n{\n\tstruct list_head *head = &resv->regions;\n\tstruct file_region *rg;\n\tlong chg = 0;\n\n\tspin_lock(&resv->lock);\n\t \n\tlist_for_each_entry(rg, head, link) {\n\t\tlong seg_from;\n\t\tlong seg_to;\n\n\t\tif (rg->to <= f)\n\t\t\tcontinue;\n\t\tif (rg->from >= t)\n\t\t\tbreak;\n\n\t\tseg_from = max(rg->from, f);\n\t\tseg_to = min(rg->to, t);\n\n\t\tchg += seg_to - seg_from;\n\t}\n\tspin_unlock(&resv->lock);\n\n\treturn chg;\n}\n\n \nstatic pgoff_t vma_hugecache_offset(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long address)\n{\n\treturn ((address - vma->vm_start) >> huge_page_shift(h)) +\n\t\t\t(vma->vm_pgoff >> huge_page_order(h));\n}\n\npgoff_t linear_hugepage_index(struct vm_area_struct *vma,\n\t\t\t\t     unsigned long address)\n{\n\treturn vma_hugecache_offset(hstate_vma(vma), vma, address);\n}\nEXPORT_SYMBOL_GPL(linear_hugepage_index);\n\n \nunsigned long vma_kernel_pagesize(struct vm_area_struct *vma)\n{\n\tif (vma->vm_ops && vma->vm_ops->pagesize)\n\t\treturn vma->vm_ops->pagesize(vma);\n\treturn PAGE_SIZE;\n}\nEXPORT_SYMBOL_GPL(vma_kernel_pagesize);\n\n \n__weak unsigned long vma_mmu_pagesize(struct vm_area_struct *vma)\n{\n\treturn vma_kernel_pagesize(vma);\n}\n\n \n#define HPAGE_RESV_OWNER    (1UL << 0)\n#define HPAGE_RESV_UNMAPPED (1UL << 1)\n#define HPAGE_RESV_MASK (HPAGE_RESV_OWNER | HPAGE_RESV_UNMAPPED)\n\n \nstatic unsigned long get_vma_private_data(struct vm_area_struct *vma)\n{\n\treturn (unsigned long)vma->vm_private_data;\n}\n\nstatic void set_vma_private_data(struct vm_area_struct *vma,\n\t\t\t\t\t\t\tunsigned long value)\n{\n\tvma->vm_private_data = (void *)value;\n}\n\nstatic void\nresv_map_set_hugetlb_cgroup_uncharge_info(struct resv_map *resv_map,\n\t\t\t\t\t  struct hugetlb_cgroup *h_cg,\n\t\t\t\t\t  struct hstate *h)\n{\n#ifdef CONFIG_CGROUP_HUGETLB\n\tif (!h_cg || !h) {\n\t\tresv_map->reservation_counter = NULL;\n\t\tresv_map->pages_per_hpage = 0;\n\t\tresv_map->css = NULL;\n\t} else {\n\t\tresv_map->reservation_counter =\n\t\t\t&h_cg->rsvd_hugepage[hstate_index(h)];\n\t\tresv_map->pages_per_hpage = pages_per_huge_page(h);\n\t\tresv_map->css = &h_cg->css;\n\t}\n#endif\n}\n\nstruct resv_map *resv_map_alloc(void)\n{\n\tstruct resv_map *resv_map = kmalloc(sizeof(*resv_map), GFP_KERNEL);\n\tstruct file_region *rg = kmalloc(sizeof(*rg), GFP_KERNEL);\n\n\tif (!resv_map || !rg) {\n\t\tkfree(resv_map);\n\t\tkfree(rg);\n\t\treturn NULL;\n\t}\n\n\tkref_init(&resv_map->refs);\n\tspin_lock_init(&resv_map->lock);\n\tINIT_LIST_HEAD(&resv_map->regions);\n\tinit_rwsem(&resv_map->rw_sema);\n\n\tresv_map->adds_in_progress = 0;\n\t \n\tresv_map_set_hugetlb_cgroup_uncharge_info(resv_map, NULL, NULL);\n\n\tINIT_LIST_HEAD(&resv_map->region_cache);\n\tlist_add(&rg->link, &resv_map->region_cache);\n\tresv_map->region_cache_count = 1;\n\n\treturn resv_map;\n}\n\nvoid resv_map_release(struct kref *ref)\n{\n\tstruct resv_map *resv_map = container_of(ref, struct resv_map, refs);\n\tstruct list_head *head = &resv_map->region_cache;\n\tstruct file_region *rg, *trg;\n\n\t \n\tregion_del(resv_map, 0, LONG_MAX);\n\n\t \n\tlist_for_each_entry_safe(rg, trg, head, link) {\n\t\tlist_del(&rg->link);\n\t\tkfree(rg);\n\t}\n\n\tVM_BUG_ON(resv_map->adds_in_progress);\n\n\tkfree(resv_map);\n}\n\nstatic inline struct resv_map *inode_resv_map(struct inode *inode)\n{\n\t \n\treturn (struct resv_map *)(&inode->i_data)->private_data;\n}\n\nstatic struct resv_map *vma_resv_map(struct vm_area_struct *vma)\n{\n\tVM_BUG_ON_VMA(!is_vm_hugetlb_page(vma), vma);\n\tif (vma->vm_flags & VM_MAYSHARE) {\n\t\tstruct address_space *mapping = vma->vm_file->f_mapping;\n\t\tstruct inode *inode = mapping->host;\n\n\t\treturn inode_resv_map(inode);\n\n\t} else {\n\t\treturn (struct resv_map *)(get_vma_private_data(vma) &\n\t\t\t\t\t\t\t~HPAGE_RESV_MASK);\n\t}\n}\n\nstatic void set_vma_resv_map(struct vm_area_struct *vma, struct resv_map *map)\n{\n\tVM_BUG_ON_VMA(!is_vm_hugetlb_page(vma), vma);\n\tVM_BUG_ON_VMA(vma->vm_flags & VM_MAYSHARE, vma);\n\n\tset_vma_private_data(vma, (unsigned long)map);\n}\n\nstatic void set_vma_resv_flags(struct vm_area_struct *vma, unsigned long flags)\n{\n\tVM_BUG_ON_VMA(!is_vm_hugetlb_page(vma), vma);\n\tVM_BUG_ON_VMA(vma->vm_flags & VM_MAYSHARE, vma);\n\n\tset_vma_private_data(vma, get_vma_private_data(vma) | flags);\n}\n\nstatic int is_vma_resv_set(struct vm_area_struct *vma, unsigned long flag)\n{\n\tVM_BUG_ON_VMA(!is_vm_hugetlb_page(vma), vma);\n\n\treturn (get_vma_private_data(vma) & flag) != 0;\n}\n\nbool __vma_private_lock(struct vm_area_struct *vma)\n{\n\treturn !(vma->vm_flags & VM_MAYSHARE) &&\n\t\tget_vma_private_data(vma) & ~HPAGE_RESV_MASK &&\n\t\tis_vma_resv_set(vma, HPAGE_RESV_OWNER);\n}\n\nvoid hugetlb_dup_vma_private(struct vm_area_struct *vma)\n{\n\tVM_BUG_ON_VMA(!is_vm_hugetlb_page(vma), vma);\n\t \n\tif (vma->vm_flags & VM_MAYSHARE) {\n\t\tstruct hugetlb_vma_lock *vma_lock = vma->vm_private_data;\n\n\t\tif (vma_lock && vma_lock->vma != vma)\n\t\t\tvma->vm_private_data = NULL;\n\t} else\n\t\tvma->vm_private_data = NULL;\n}\n\n \nvoid clear_vma_resv_huge_pages(struct vm_area_struct *vma)\n{\n\t \n\tstruct resv_map *reservations = vma_resv_map(vma);\n\n\tif (reservations && is_vma_resv_set(vma, HPAGE_RESV_OWNER)) {\n\t\tresv_map_put_hugetlb_cgroup_uncharge_info(reservations);\n\t\tkref_put(&reservations->refs, resv_map_release);\n\t}\n\n\thugetlb_dup_vma_private(vma);\n}\n\n \nstatic bool vma_has_reserves(struct vm_area_struct *vma, long chg)\n{\n\tif (vma->vm_flags & VM_NORESERVE) {\n\t\t \n\t\tif (vma->vm_flags & VM_MAYSHARE && chg == 0)\n\t\t\treturn true;\n\t\telse\n\t\t\treturn false;\n\t}\n\n\t \n\tif (vma->vm_flags & VM_MAYSHARE) {\n\t\t \n\t\tif (chg)\n\t\t\treturn false;\n\t\telse\n\t\t\treturn true;\n\t}\n\n\t \n\tif (is_vma_resv_set(vma, HPAGE_RESV_OWNER)) {\n\t\t \n\t\tif (chg)\n\t\t\treturn false;\n\t\telse\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic void enqueue_hugetlb_folio(struct hstate *h, struct folio *folio)\n{\n\tint nid = folio_nid(folio);\n\n\tlockdep_assert_held(&hugetlb_lock);\n\tVM_BUG_ON_FOLIO(folio_ref_count(folio), folio);\n\n\tlist_move(&folio->lru, &h->hugepage_freelists[nid]);\n\th->free_huge_pages++;\n\th->free_huge_pages_node[nid]++;\n\tfolio_set_hugetlb_freed(folio);\n}\n\nstatic struct folio *dequeue_hugetlb_folio_node_exact(struct hstate *h,\n\t\t\t\t\t\t\t\tint nid)\n{\n\tstruct folio *folio;\n\tbool pin = !!(current->flags & PF_MEMALLOC_PIN);\n\n\tlockdep_assert_held(&hugetlb_lock);\n\tlist_for_each_entry(folio, &h->hugepage_freelists[nid], lru) {\n\t\tif (pin && !folio_is_longterm_pinnable(folio))\n\t\t\tcontinue;\n\n\t\tif (folio_test_hwpoison(folio))\n\t\t\tcontinue;\n\n\t\tlist_move(&folio->lru, &h->hugepage_activelist);\n\t\tfolio_ref_unfreeze(folio, 1);\n\t\tfolio_clear_hugetlb_freed(folio);\n\t\th->free_huge_pages--;\n\t\th->free_huge_pages_node[nid]--;\n\t\treturn folio;\n\t}\n\n\treturn NULL;\n}\n\nstatic struct folio *dequeue_hugetlb_folio_nodemask(struct hstate *h, gfp_t gfp_mask,\n\t\t\t\t\t\t\tint nid, nodemask_t *nmask)\n{\n\tunsigned int cpuset_mems_cookie;\n\tstruct zonelist *zonelist;\n\tstruct zone *zone;\n\tstruct zoneref *z;\n\tint node = NUMA_NO_NODE;\n\n\tzonelist = node_zonelist(nid, gfp_mask);\n\nretry_cpuset:\n\tcpuset_mems_cookie = read_mems_allowed_begin();\n\tfor_each_zone_zonelist_nodemask(zone, z, zonelist, gfp_zone(gfp_mask), nmask) {\n\t\tstruct folio *folio;\n\n\t\tif (!cpuset_zone_allowed(zone, gfp_mask))\n\t\t\tcontinue;\n\t\t \n\t\tif (zone_to_nid(zone) == node)\n\t\t\tcontinue;\n\t\tnode = zone_to_nid(zone);\n\n\t\tfolio = dequeue_hugetlb_folio_node_exact(h, node);\n\t\tif (folio)\n\t\t\treturn folio;\n\t}\n\tif (unlikely(read_mems_allowed_retry(cpuset_mems_cookie)))\n\t\tgoto retry_cpuset;\n\n\treturn NULL;\n}\n\nstatic unsigned long available_huge_pages(struct hstate *h)\n{\n\treturn h->free_huge_pages - h->resv_huge_pages;\n}\n\nstatic struct folio *dequeue_hugetlb_folio_vma(struct hstate *h,\n\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\tunsigned long address, int avoid_reserve,\n\t\t\t\tlong chg)\n{\n\tstruct folio *folio = NULL;\n\tstruct mempolicy *mpol;\n\tgfp_t gfp_mask;\n\tnodemask_t *nodemask;\n\tint nid;\n\n\t \n\tif (!vma_has_reserves(vma, chg) && !available_huge_pages(h))\n\t\tgoto err;\n\n\t \n\tif (avoid_reserve && !available_huge_pages(h))\n\t\tgoto err;\n\n\tgfp_mask = htlb_alloc_mask(h);\n\tnid = huge_node(vma, address, gfp_mask, &mpol, &nodemask);\n\n\tif (mpol_is_preferred_many(mpol)) {\n\t\tfolio = dequeue_hugetlb_folio_nodemask(h, gfp_mask,\n\t\t\t\t\t\t\tnid, nodemask);\n\n\t\t \n\t\tnodemask = NULL;\n\t}\n\n\tif (!folio)\n\t\tfolio = dequeue_hugetlb_folio_nodemask(h, gfp_mask,\n\t\t\t\t\t\t\tnid, nodemask);\n\n\tif (folio && !avoid_reserve && vma_has_reserves(vma, chg)) {\n\t\tfolio_set_hugetlb_restore_reserve(folio);\n\t\th->resv_huge_pages--;\n\t}\n\n\tmpol_cond_put(mpol);\n\treturn folio;\n\nerr:\n\treturn NULL;\n}\n\n \nstatic int next_node_allowed(int nid, nodemask_t *nodes_allowed)\n{\n\tnid = next_node_in(nid, *nodes_allowed);\n\tVM_BUG_ON(nid >= MAX_NUMNODES);\n\n\treturn nid;\n}\n\nstatic int get_valid_node_allowed(int nid, nodemask_t *nodes_allowed)\n{\n\tif (!node_isset(nid, *nodes_allowed))\n\t\tnid = next_node_allowed(nid, nodes_allowed);\n\treturn nid;\n}\n\n \nstatic int hstate_next_node_to_alloc(struct hstate *h,\n\t\t\t\t\tnodemask_t *nodes_allowed)\n{\n\tint nid;\n\n\tVM_BUG_ON(!nodes_allowed);\n\n\tnid = get_valid_node_allowed(h->next_nid_to_alloc, nodes_allowed);\n\th->next_nid_to_alloc = next_node_allowed(nid, nodes_allowed);\n\n\treturn nid;\n}\n\n \nstatic int hstate_next_node_to_free(struct hstate *h, nodemask_t *nodes_allowed)\n{\n\tint nid;\n\n\tVM_BUG_ON(!nodes_allowed);\n\n\tnid = get_valid_node_allowed(h->next_nid_to_free, nodes_allowed);\n\th->next_nid_to_free = next_node_allowed(nid, nodes_allowed);\n\n\treturn nid;\n}\n\n#define for_each_node_mask_to_alloc(hs, nr_nodes, node, mask)\t\t\\\n\tfor (nr_nodes = nodes_weight(*mask);\t\t\t\t\\\n\t\tnr_nodes > 0 &&\t\t\t\t\t\t\\\n\t\t((node = hstate_next_node_to_alloc(hs, mask)) || 1);\t\\\n\t\tnr_nodes--)\n\n#define for_each_node_mask_to_free(hs, nr_nodes, node, mask)\t\t\\\n\tfor (nr_nodes = nodes_weight(*mask);\t\t\t\t\\\n\t\tnr_nodes > 0 &&\t\t\t\t\t\t\\\n\t\t((node = hstate_next_node_to_free(hs, mask)) || 1);\t\\\n\t\tnr_nodes--)\n\n \nstatic void __destroy_compound_gigantic_folio(struct folio *folio,\n\t\t\t\t\tunsigned int order, bool demote)\n{\n\tint i;\n\tint nr_pages = 1 << order;\n\tstruct page *p;\n\n\tatomic_set(&folio->_entire_mapcount, 0);\n\tatomic_set(&folio->_nr_pages_mapped, 0);\n\tatomic_set(&folio->_pincount, 0);\n\n\tfor (i = 1; i < nr_pages; i++) {\n\t\tp = folio_page(folio, i);\n\t\tp->flags &= ~PAGE_FLAGS_CHECK_AT_FREE;\n\t\tp->mapping = NULL;\n\t\tclear_compound_head(p);\n\t\tif (!demote)\n\t\t\tset_page_refcounted(p);\n\t}\n\n\t__folio_clear_head(folio);\n}\n\nstatic void destroy_compound_hugetlb_folio_for_demote(struct folio *folio,\n\t\t\t\t\tunsigned int order)\n{\n\t__destroy_compound_gigantic_folio(folio, order, true);\n}\n\n#ifdef CONFIG_ARCH_HAS_GIGANTIC_PAGE\nstatic void destroy_compound_gigantic_folio(struct folio *folio,\n\t\t\t\t\tunsigned int order)\n{\n\t__destroy_compound_gigantic_folio(folio, order, false);\n}\n\nstatic void free_gigantic_folio(struct folio *folio, unsigned int order)\n{\n\t \n#ifdef CONFIG_CMA\n\tint nid = folio_nid(folio);\n\n\tif (cma_release(hugetlb_cma[nid], &folio->page, 1 << order))\n\t\treturn;\n#endif\n\n\tfree_contig_range(folio_pfn(folio), 1 << order);\n}\n\n#ifdef CONFIG_CONTIG_ALLOC\nstatic struct folio *alloc_gigantic_folio(struct hstate *h, gfp_t gfp_mask,\n\t\tint nid, nodemask_t *nodemask)\n{\n\tstruct page *page;\n\tunsigned long nr_pages = pages_per_huge_page(h);\n\tif (nid == NUMA_NO_NODE)\n\t\tnid = numa_mem_id();\n\n#ifdef CONFIG_CMA\n\t{\n\t\tint node;\n\n\t\tif (hugetlb_cma[nid]) {\n\t\t\tpage = cma_alloc(hugetlb_cma[nid], nr_pages,\n\t\t\t\t\thuge_page_order(h), true);\n\t\t\tif (page)\n\t\t\t\treturn page_folio(page);\n\t\t}\n\n\t\tif (!(gfp_mask & __GFP_THISNODE)) {\n\t\t\tfor_each_node_mask(node, *nodemask) {\n\t\t\t\tif (node == nid || !hugetlb_cma[node])\n\t\t\t\t\tcontinue;\n\n\t\t\t\tpage = cma_alloc(hugetlb_cma[node], nr_pages,\n\t\t\t\t\t\thuge_page_order(h), true);\n\t\t\t\tif (page)\n\t\t\t\t\treturn page_folio(page);\n\t\t\t}\n\t\t}\n\t}\n#endif\n\n\tpage = alloc_contig_pages(nr_pages, gfp_mask, nid, nodemask);\n\treturn page ? page_folio(page) : NULL;\n}\n\n#else  \nstatic struct folio *alloc_gigantic_folio(struct hstate *h, gfp_t gfp_mask,\n\t\t\t\t\tint nid, nodemask_t *nodemask)\n{\n\treturn NULL;\n}\n#endif  \n\n#else  \nstatic struct folio *alloc_gigantic_folio(struct hstate *h, gfp_t gfp_mask,\n\t\t\t\t\tint nid, nodemask_t *nodemask)\n{\n\treturn NULL;\n}\nstatic inline void free_gigantic_folio(struct folio *folio,\n\t\t\t\t\t\tunsigned int order) { }\nstatic inline void destroy_compound_gigantic_folio(struct folio *folio,\n\t\t\t\t\t\tunsigned int order) { }\n#endif\n\nstatic inline void __clear_hugetlb_destructor(struct hstate *h,\n\t\t\t\t\t\tstruct folio *folio)\n{\n\tlockdep_assert_held(&hugetlb_lock);\n\n\tfolio_clear_hugetlb(folio);\n}\n\n \nstatic void __remove_hugetlb_folio(struct hstate *h, struct folio *folio,\n\t\t\t\t\t\t\tbool adjust_surplus,\n\t\t\t\t\t\t\tbool demote)\n{\n\tint nid = folio_nid(folio);\n\n\tVM_BUG_ON_FOLIO(hugetlb_cgroup_from_folio(folio), folio);\n\tVM_BUG_ON_FOLIO(hugetlb_cgroup_from_folio_rsvd(folio), folio);\n\n\tlockdep_assert_held(&hugetlb_lock);\n\tif (hstate_is_gigantic(h) && !gigantic_page_runtime_supported())\n\t\treturn;\n\n\tlist_del(&folio->lru);\n\n\tif (folio_test_hugetlb_freed(folio)) {\n\t\th->free_huge_pages--;\n\t\th->free_huge_pages_node[nid]--;\n\t}\n\tif (adjust_surplus) {\n\t\th->surplus_huge_pages--;\n\t\th->surplus_huge_pages_node[nid]--;\n\t}\n\n\t \n\tif (!folio_test_hugetlb_vmemmap_optimized(folio))\n\t\t__clear_hugetlb_destructor(h, folio);\n\n\t  \n\tif (!demote)\n\t\tfolio_ref_unfreeze(folio, 1);\n\n\th->nr_huge_pages--;\n\th->nr_huge_pages_node[nid]--;\n}\n\nstatic void remove_hugetlb_folio(struct hstate *h, struct folio *folio,\n\t\t\t\t\t\t\tbool adjust_surplus)\n{\n\t__remove_hugetlb_folio(h, folio, adjust_surplus, false);\n}\n\nstatic void remove_hugetlb_folio_for_demote(struct hstate *h, struct folio *folio,\n\t\t\t\t\t\t\tbool adjust_surplus)\n{\n\t__remove_hugetlb_folio(h, folio, adjust_surplus, true);\n}\n\nstatic void add_hugetlb_folio(struct hstate *h, struct folio *folio,\n\t\t\t     bool adjust_surplus)\n{\n\tint zeroed;\n\tint nid = folio_nid(folio);\n\n\tVM_BUG_ON_FOLIO(!folio_test_hugetlb_vmemmap_optimized(folio), folio);\n\n\tlockdep_assert_held(&hugetlb_lock);\n\n\tINIT_LIST_HEAD(&folio->lru);\n\th->nr_huge_pages++;\n\th->nr_huge_pages_node[nid]++;\n\n\tif (adjust_surplus) {\n\t\th->surplus_huge_pages++;\n\t\th->surplus_huge_pages_node[nid]++;\n\t}\n\n\tfolio_set_hugetlb(folio);\n\tfolio_change_private(folio, NULL);\n\t \n\tfolio_set_hugetlb_vmemmap_optimized(folio);\n\n\t \n\tzeroed = folio_put_testzero(folio);\n\tif (unlikely(!zeroed))\n\t\t \n\t\treturn;\n\n\tarch_clear_hugepage_flags(&folio->page);\n\tenqueue_hugetlb_folio(h, folio);\n}\n\nstatic void __update_and_free_hugetlb_folio(struct hstate *h,\n\t\t\t\t\t\tstruct folio *folio)\n{\n\tbool clear_dtor = folio_test_hugetlb_vmemmap_optimized(folio);\n\n\tif (hstate_is_gigantic(h) && !gigantic_page_runtime_supported())\n\t\treturn;\n\n\t \n\tif (folio_test_hugetlb_raw_hwp_unreliable(folio))\n\t\treturn;\n\n\tif (hugetlb_vmemmap_restore(h, &folio->page)) {\n\t\tspin_lock_irq(&hugetlb_lock);\n\t\t \n\t\tadd_hugetlb_folio(h, folio, true);\n\t\tspin_unlock_irq(&hugetlb_lock);\n\t\treturn;\n\t}\n\n\t \n\tif (unlikely(folio_test_hwpoison(folio)))\n\t\tfolio_clear_hugetlb_hwpoison(folio);\n\n\t \n\tif (clear_dtor) {\n\t\tspin_lock_irq(&hugetlb_lock);\n\t\t__clear_hugetlb_destructor(h, folio);\n\t\tspin_unlock_irq(&hugetlb_lock);\n\t}\n\n\t \n\tif (hstate_is_gigantic(h) ||\n\t    hugetlb_cma_folio(folio, huge_page_order(h))) {\n\t\tdestroy_compound_gigantic_folio(folio, huge_page_order(h));\n\t\tfree_gigantic_folio(folio, huge_page_order(h));\n\t} else {\n\t\t__free_pages(&folio->page, huge_page_order(h));\n\t}\n}\n\n \nstatic LLIST_HEAD(hpage_freelist);\n\nstatic void free_hpage_workfn(struct work_struct *work)\n{\n\tstruct llist_node *node;\n\n\tnode = llist_del_all(&hpage_freelist);\n\n\twhile (node) {\n\t\tstruct page *page;\n\t\tstruct hstate *h;\n\n\t\tpage = container_of((struct address_space **)node,\n\t\t\t\t     struct page, mapping);\n\t\tnode = node->next;\n\t\tpage->mapping = NULL;\n\t\t \n\t\th = size_to_hstate(page_size(page));\n\n\t\t__update_and_free_hugetlb_folio(h, page_folio(page));\n\n\t\tcond_resched();\n\t}\n}\nstatic DECLARE_WORK(free_hpage_work, free_hpage_workfn);\n\nstatic inline void flush_free_hpage_work(struct hstate *h)\n{\n\tif (hugetlb_vmemmap_optimizable(h))\n\t\tflush_work(&free_hpage_work);\n}\n\nstatic void update_and_free_hugetlb_folio(struct hstate *h, struct folio *folio,\n\t\t\t\t bool atomic)\n{\n\tif (!folio_test_hugetlb_vmemmap_optimized(folio) || !atomic) {\n\t\t__update_and_free_hugetlb_folio(h, folio);\n\t\treturn;\n\t}\n\n\t \n\tif (llist_add((struct llist_node *)&folio->mapping, &hpage_freelist))\n\t\tschedule_work(&free_hpage_work);\n}\n\nstatic void update_and_free_pages_bulk(struct hstate *h, struct list_head *list)\n{\n\tstruct page *page, *t_page;\n\tstruct folio *folio;\n\n\tlist_for_each_entry_safe(page, t_page, list, lru) {\n\t\tfolio = page_folio(page);\n\t\tupdate_and_free_hugetlb_folio(h, folio, false);\n\t\tcond_resched();\n\t}\n}\n\nstruct hstate *size_to_hstate(unsigned long size)\n{\n\tstruct hstate *h;\n\n\tfor_each_hstate(h) {\n\t\tif (huge_page_size(h) == size)\n\t\t\treturn h;\n\t}\n\treturn NULL;\n}\n\nvoid free_huge_folio(struct folio *folio)\n{\n\t \n\tstruct hstate *h = folio_hstate(folio);\n\tint nid = folio_nid(folio);\n\tstruct hugepage_subpool *spool = hugetlb_folio_subpool(folio);\n\tbool restore_reserve;\n\tunsigned long flags;\n\n\tVM_BUG_ON_FOLIO(folio_ref_count(folio), folio);\n\tVM_BUG_ON_FOLIO(folio_mapcount(folio), folio);\n\n\thugetlb_set_folio_subpool(folio, NULL);\n\tif (folio_test_anon(folio))\n\t\t__ClearPageAnonExclusive(&folio->page);\n\tfolio->mapping = NULL;\n\trestore_reserve = folio_test_hugetlb_restore_reserve(folio);\n\tfolio_clear_hugetlb_restore_reserve(folio);\n\n\t \n\tif (!restore_reserve) {\n\t\t \n\t\tif (hugepage_subpool_put_pages(spool, 1) == 0)\n\t\t\trestore_reserve = true;\n\t}\n\n\tspin_lock_irqsave(&hugetlb_lock, flags);\n\tfolio_clear_hugetlb_migratable(folio);\n\thugetlb_cgroup_uncharge_folio(hstate_index(h),\n\t\t\t\t     pages_per_huge_page(h), folio);\n\thugetlb_cgroup_uncharge_folio_rsvd(hstate_index(h),\n\t\t\t\t\t  pages_per_huge_page(h), folio);\n\tif (restore_reserve)\n\t\th->resv_huge_pages++;\n\n\tif (folio_test_hugetlb_temporary(folio)) {\n\t\tremove_hugetlb_folio(h, folio, false);\n\t\tspin_unlock_irqrestore(&hugetlb_lock, flags);\n\t\tupdate_and_free_hugetlb_folio(h, folio, true);\n\t} else if (h->surplus_huge_pages_node[nid]) {\n\t\t \n\t\tremove_hugetlb_folio(h, folio, true);\n\t\tspin_unlock_irqrestore(&hugetlb_lock, flags);\n\t\tupdate_and_free_hugetlb_folio(h, folio, true);\n\t} else {\n\t\tarch_clear_hugepage_flags(&folio->page);\n\t\tenqueue_hugetlb_folio(h, folio);\n\t\tspin_unlock_irqrestore(&hugetlb_lock, flags);\n\t}\n}\n\n \nstatic void __prep_account_new_huge_page(struct hstate *h, int nid)\n{\n\tlockdep_assert_held(&hugetlb_lock);\n\th->nr_huge_pages++;\n\th->nr_huge_pages_node[nid]++;\n}\n\nstatic void __prep_new_hugetlb_folio(struct hstate *h, struct folio *folio)\n{\n\thugetlb_vmemmap_optimize(h, &folio->page);\n\tINIT_LIST_HEAD(&folio->lru);\n\tfolio_set_hugetlb(folio);\n\thugetlb_set_folio_subpool(folio, NULL);\n\tset_hugetlb_cgroup(folio, NULL);\n\tset_hugetlb_cgroup_rsvd(folio, NULL);\n}\n\nstatic void prep_new_hugetlb_folio(struct hstate *h, struct folio *folio, int nid)\n{\n\t__prep_new_hugetlb_folio(h, folio);\n\tspin_lock_irq(&hugetlb_lock);\n\t__prep_account_new_huge_page(h, nid);\n\tspin_unlock_irq(&hugetlb_lock);\n}\n\nstatic bool __prep_compound_gigantic_folio(struct folio *folio,\n\t\t\t\t\tunsigned int order, bool demote)\n{\n\tint i, j;\n\tint nr_pages = 1 << order;\n\tstruct page *p;\n\n\t__folio_clear_reserved(folio);\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tp = folio_page(folio, i);\n\n\t\t \n\t\tif (i != 0)\t \n\t\t\t__ClearPageReserved(p);\n\t\t \n\t\tif (!demote) {\n\t\t\tif (!page_ref_freeze(p, 1)) {\n\t\t\t\tpr_warn(\"HugeTLB page can not be used due to unexpected inflated ref count\\n\");\n\t\t\t\tgoto out_error;\n\t\t\t}\n\t\t} else {\n\t\t\tVM_BUG_ON_PAGE(page_count(p), p);\n\t\t}\n\t\tif (i != 0)\n\t\t\tset_compound_head(p, &folio->page);\n\t}\n\t__folio_set_head(folio);\n\t \n\tfolio_set_order(folio, order);\n\tatomic_set(&folio->_entire_mapcount, -1);\n\tatomic_set(&folio->_nr_pages_mapped, 0);\n\tatomic_set(&folio->_pincount, 0);\n\treturn true;\n\nout_error:\n\t \n\tfor (j = 0; j < i; j++) {\n\t\tp = folio_page(folio, j);\n\t\tif (j != 0)\n\t\t\tclear_compound_head(p);\n\t\tset_page_refcounted(p);\n\t}\n\t \n\tfor (; j < nr_pages; j++) {\n\t\tp = folio_page(folio, j);\n\t\t__ClearPageReserved(p);\n\t}\n\treturn false;\n}\n\nstatic bool prep_compound_gigantic_folio(struct folio *folio,\n\t\t\t\t\t\t\tunsigned int order)\n{\n\treturn __prep_compound_gigantic_folio(folio, order, false);\n}\n\nstatic bool prep_compound_gigantic_folio_for_demote(struct folio *folio,\n\t\t\t\t\t\t\tunsigned int order)\n{\n\treturn __prep_compound_gigantic_folio(folio, order, true);\n}\n\n \nint PageHuge(struct page *page)\n{\n\tstruct folio *folio;\n\n\tif (!PageCompound(page))\n\t\treturn 0;\n\tfolio = page_folio(page);\n\treturn folio_test_hugetlb(folio);\n}\nEXPORT_SYMBOL_GPL(PageHuge);\n\n \nstruct address_space *hugetlb_page_mapping_lock_write(struct page *hpage)\n{\n\tstruct address_space *mapping = page_mapping(hpage);\n\n\tif (!mapping)\n\t\treturn mapping;\n\n\tif (i_mmap_trylock_write(mapping))\n\t\treturn mapping;\n\n\treturn NULL;\n}\n\npgoff_t hugetlb_basepage_index(struct page *page)\n{\n\tstruct page *page_head = compound_head(page);\n\tpgoff_t index = page_index(page_head);\n\tunsigned long compound_idx;\n\n\tif (compound_order(page_head) > MAX_ORDER)\n\t\tcompound_idx = page_to_pfn(page) - page_to_pfn(page_head);\n\telse\n\t\tcompound_idx = page - page_head;\n\n\treturn (index << compound_order(page_head)) + compound_idx;\n}\n\nstatic struct folio *alloc_buddy_hugetlb_folio(struct hstate *h,\n\t\tgfp_t gfp_mask, int nid, nodemask_t *nmask,\n\t\tnodemask_t *node_alloc_noretry)\n{\n\tint order = huge_page_order(h);\n\tstruct page *page;\n\tbool alloc_try_hard = true;\n\tbool retry = true;\n\n\t \n\tif (node_alloc_noretry && node_isset(nid, *node_alloc_noretry))\n\t\talloc_try_hard = false;\n\tgfp_mask |= __GFP_COMP|__GFP_NOWARN;\n\tif (alloc_try_hard)\n\t\tgfp_mask |= __GFP_RETRY_MAYFAIL;\n\tif (nid == NUMA_NO_NODE)\n\t\tnid = numa_mem_id();\nretry:\n\tpage = __alloc_pages(gfp_mask, order, nid, nmask);\n\n\t \n\tif (page && !page_ref_freeze(page, 1)) {\n\t\t__free_pages(page, order);\n\t\tif (retry) {\t \n\t\t\tretry = false;\n\t\t\tgoto retry;\n\t\t}\n\t\t \n\t\tpr_warn(\"HugeTLB head page unexpected inflated ref count\\n\");\n\t\tpage = NULL;\n\t}\n\n\t \n\tif (node_alloc_noretry && page && !alloc_try_hard)\n\t\tnode_clear(nid, *node_alloc_noretry);\n\n\t \n\tif (node_alloc_noretry && !page && alloc_try_hard)\n\t\tnode_set(nid, *node_alloc_noretry);\n\n\tif (!page) {\n\t\t__count_vm_event(HTLB_BUDDY_PGALLOC_FAIL);\n\t\treturn NULL;\n\t}\n\n\t__count_vm_event(HTLB_BUDDY_PGALLOC);\n\treturn page_folio(page);\n}\n\n \nstatic struct folio *alloc_fresh_hugetlb_folio(struct hstate *h,\n\t\tgfp_t gfp_mask, int nid, nodemask_t *nmask,\n\t\tnodemask_t *node_alloc_noretry)\n{\n\tstruct folio *folio;\n\tbool retry = false;\n\nretry:\n\tif (hstate_is_gigantic(h))\n\t\tfolio = alloc_gigantic_folio(h, gfp_mask, nid, nmask);\n\telse\n\t\tfolio = alloc_buddy_hugetlb_folio(h, gfp_mask,\n\t\t\t\tnid, nmask, node_alloc_noretry);\n\tif (!folio)\n\t\treturn NULL;\n\tif (hstate_is_gigantic(h)) {\n\t\tif (!prep_compound_gigantic_folio(folio, huge_page_order(h))) {\n\t\t\t \n\t\t\tfree_gigantic_folio(folio, huge_page_order(h));\n\t\t\tif (!retry) {\n\t\t\t\tretry = true;\n\t\t\t\tgoto retry;\n\t\t\t}\n\t\t\treturn NULL;\n\t\t}\n\t}\n\tprep_new_hugetlb_folio(h, folio, folio_nid(folio));\n\n\treturn folio;\n}\n\n \nstatic int alloc_pool_huge_page(struct hstate *h, nodemask_t *nodes_allowed,\n\t\t\t\tnodemask_t *node_alloc_noretry)\n{\n\tstruct folio *folio;\n\tint nr_nodes, node;\n\tgfp_t gfp_mask = htlb_alloc_mask(h) | __GFP_THISNODE;\n\n\tfor_each_node_mask_to_alloc(h, nr_nodes, node, nodes_allowed) {\n\t\tfolio = alloc_fresh_hugetlb_folio(h, gfp_mask, node,\n\t\t\t\t\tnodes_allowed, node_alloc_noretry);\n\t\tif (folio) {\n\t\t\tfree_huge_folio(folio);  \n\t\t\treturn 1;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic struct page *remove_pool_huge_page(struct hstate *h,\n\t\t\t\t\t\tnodemask_t *nodes_allowed,\n\t\t\t\t\t\t bool acct_surplus)\n{\n\tint nr_nodes, node;\n\tstruct page *page = NULL;\n\tstruct folio *folio;\n\n\tlockdep_assert_held(&hugetlb_lock);\n\tfor_each_node_mask_to_free(h, nr_nodes, node, nodes_allowed) {\n\t\t \n\t\tif ((!acct_surplus || h->surplus_huge_pages_node[node]) &&\n\t\t    !list_empty(&h->hugepage_freelists[node])) {\n\t\t\tpage = list_entry(h->hugepage_freelists[node].next,\n\t\t\t\t\t  struct page, lru);\n\t\t\tfolio = page_folio(page);\n\t\t\tremove_hugetlb_folio(h, folio, acct_surplus);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn page;\n}\n\n \nint dissolve_free_huge_page(struct page *page)\n{\n\tint rc = -EBUSY;\n\tstruct folio *folio = page_folio(page);\n\nretry:\n\t \n\tif (!folio_test_hugetlb(folio))\n\t\treturn 0;\n\n\tspin_lock_irq(&hugetlb_lock);\n\tif (!folio_test_hugetlb(folio)) {\n\t\trc = 0;\n\t\tgoto out;\n\t}\n\n\tif (!folio_ref_count(folio)) {\n\t\tstruct hstate *h = folio_hstate(folio);\n\t\tif (!available_huge_pages(h))\n\t\t\tgoto out;\n\n\t\t \n\t\tif (unlikely(!folio_test_hugetlb_freed(folio))) {\n\t\t\tspin_unlock_irq(&hugetlb_lock);\n\t\t\tcond_resched();\n\n\t\t\t \n\t\t\tgoto retry;\n\t\t}\n\n\t\tremove_hugetlb_folio(h, folio, false);\n\t\th->max_huge_pages--;\n\t\tspin_unlock_irq(&hugetlb_lock);\n\n\t\t \n\t\trc = hugetlb_vmemmap_restore(h, &folio->page);\n\t\tif (!rc) {\n\t\t\tupdate_and_free_hugetlb_folio(h, folio, false);\n\t\t} else {\n\t\t\tspin_lock_irq(&hugetlb_lock);\n\t\t\tadd_hugetlb_folio(h, folio, false);\n\t\t\th->max_huge_pages++;\n\t\t\tspin_unlock_irq(&hugetlb_lock);\n\t\t}\n\n\t\treturn rc;\n\t}\nout:\n\tspin_unlock_irq(&hugetlb_lock);\n\treturn rc;\n}\n\n \nint dissolve_free_huge_pages(unsigned long start_pfn, unsigned long end_pfn)\n{\n\tunsigned long pfn;\n\tstruct page *page;\n\tint rc = 0;\n\tunsigned int order;\n\tstruct hstate *h;\n\n\tif (!hugepages_supported())\n\t\treturn rc;\n\n\torder = huge_page_order(&default_hstate);\n\tfor_each_hstate(h)\n\t\torder = min(order, huge_page_order(h));\n\n\tfor (pfn = start_pfn; pfn < end_pfn; pfn += 1 << order) {\n\t\tpage = pfn_to_page(pfn);\n\t\trc = dissolve_free_huge_page(page);\n\t\tif (rc)\n\t\t\tbreak;\n\t}\n\n\treturn rc;\n}\n\n \nstatic struct folio *alloc_surplus_hugetlb_folio(struct hstate *h,\n\t\t\t\tgfp_t gfp_mask,\tint nid, nodemask_t *nmask)\n{\n\tstruct folio *folio = NULL;\n\n\tif (hstate_is_gigantic(h))\n\t\treturn NULL;\n\n\tspin_lock_irq(&hugetlb_lock);\n\tif (h->surplus_huge_pages >= h->nr_overcommit_huge_pages)\n\t\tgoto out_unlock;\n\tspin_unlock_irq(&hugetlb_lock);\n\n\tfolio = alloc_fresh_hugetlb_folio(h, gfp_mask, nid, nmask, NULL);\n\tif (!folio)\n\t\treturn NULL;\n\n\tspin_lock_irq(&hugetlb_lock);\n\t \n\tif (h->surplus_huge_pages >= h->nr_overcommit_huge_pages) {\n\t\tfolio_set_hugetlb_temporary(folio);\n\t\tspin_unlock_irq(&hugetlb_lock);\n\t\tfree_huge_folio(folio);\n\t\treturn NULL;\n\t}\n\n\th->surplus_huge_pages++;\n\th->surplus_huge_pages_node[folio_nid(folio)]++;\n\nout_unlock:\n\tspin_unlock_irq(&hugetlb_lock);\n\n\treturn folio;\n}\n\nstatic struct folio *alloc_migrate_hugetlb_folio(struct hstate *h, gfp_t gfp_mask,\n\t\t\t\t     int nid, nodemask_t *nmask)\n{\n\tstruct folio *folio;\n\n\tif (hstate_is_gigantic(h))\n\t\treturn NULL;\n\n\tfolio = alloc_fresh_hugetlb_folio(h, gfp_mask, nid, nmask, NULL);\n\tif (!folio)\n\t\treturn NULL;\n\n\t \n\tfolio_ref_unfreeze(folio, 1);\n\t \n\tfolio_set_hugetlb_temporary(folio);\n\n\treturn folio;\n}\n\n \nstatic\nstruct folio *alloc_buddy_hugetlb_folio_with_mpol(struct hstate *h,\n\t\tstruct vm_area_struct *vma, unsigned long addr)\n{\n\tstruct folio *folio = NULL;\n\tstruct mempolicy *mpol;\n\tgfp_t gfp_mask = htlb_alloc_mask(h);\n\tint nid;\n\tnodemask_t *nodemask;\n\n\tnid = huge_node(vma, addr, gfp_mask, &mpol, &nodemask);\n\tif (mpol_is_preferred_many(mpol)) {\n\t\tgfp_t gfp = gfp_mask | __GFP_NOWARN;\n\n\t\tgfp &=  ~(__GFP_DIRECT_RECLAIM | __GFP_NOFAIL);\n\t\tfolio = alloc_surplus_hugetlb_folio(h, gfp, nid, nodemask);\n\n\t\t \n\t\tnodemask = NULL;\n\t}\n\n\tif (!folio)\n\t\tfolio = alloc_surplus_hugetlb_folio(h, gfp_mask, nid, nodemask);\n\tmpol_cond_put(mpol);\n\treturn folio;\n}\n\n \nstruct folio *alloc_hugetlb_folio_nodemask(struct hstate *h, int preferred_nid,\n\t\tnodemask_t *nmask, gfp_t gfp_mask)\n{\n\tspin_lock_irq(&hugetlb_lock);\n\tif (available_huge_pages(h)) {\n\t\tstruct folio *folio;\n\n\t\tfolio = dequeue_hugetlb_folio_nodemask(h, gfp_mask,\n\t\t\t\t\t\tpreferred_nid, nmask);\n\t\tif (folio) {\n\t\t\tspin_unlock_irq(&hugetlb_lock);\n\t\t\treturn folio;\n\t\t}\n\t}\n\tspin_unlock_irq(&hugetlb_lock);\n\n\treturn alloc_migrate_hugetlb_folio(h, gfp_mask, preferred_nid, nmask);\n}\n\n \nstruct folio *alloc_hugetlb_folio_vma(struct hstate *h, struct vm_area_struct *vma,\n\t\tunsigned long address)\n{\n\tstruct mempolicy *mpol;\n\tnodemask_t *nodemask;\n\tstruct folio *folio;\n\tgfp_t gfp_mask;\n\tint node;\n\n\tgfp_mask = htlb_alloc_mask(h);\n\tnode = huge_node(vma, address, gfp_mask, &mpol, &nodemask);\n\tfolio = alloc_hugetlb_folio_nodemask(h, node, nodemask, gfp_mask);\n\tmpol_cond_put(mpol);\n\n\treturn folio;\n}\n\n \nstatic int gather_surplus_pages(struct hstate *h, long delta)\n\t__must_hold(&hugetlb_lock)\n{\n\tLIST_HEAD(surplus_list);\n\tstruct folio *folio, *tmp;\n\tint ret;\n\tlong i;\n\tlong needed, allocated;\n\tbool alloc_ok = true;\n\n\tlockdep_assert_held(&hugetlb_lock);\n\tneeded = (h->resv_huge_pages + delta) - h->free_huge_pages;\n\tif (needed <= 0) {\n\t\th->resv_huge_pages += delta;\n\t\treturn 0;\n\t}\n\n\tallocated = 0;\n\n\tret = -ENOMEM;\nretry:\n\tspin_unlock_irq(&hugetlb_lock);\n\tfor (i = 0; i < needed; i++) {\n\t\tfolio = alloc_surplus_hugetlb_folio(h, htlb_alloc_mask(h),\n\t\t\t\tNUMA_NO_NODE, NULL);\n\t\tif (!folio) {\n\t\t\talloc_ok = false;\n\t\t\tbreak;\n\t\t}\n\t\tlist_add(&folio->lru, &surplus_list);\n\t\tcond_resched();\n\t}\n\tallocated += i;\n\n\t \n\tspin_lock_irq(&hugetlb_lock);\n\tneeded = (h->resv_huge_pages + delta) -\n\t\t\t(h->free_huge_pages + allocated);\n\tif (needed > 0) {\n\t\tif (alloc_ok)\n\t\t\tgoto retry;\n\t\t \n\t\tgoto free;\n\t}\n\t \n\tneeded += allocated;\n\th->resv_huge_pages += delta;\n\tret = 0;\n\n\t \n\tlist_for_each_entry_safe(folio, tmp, &surplus_list, lru) {\n\t\tif ((--needed) < 0)\n\t\t\tbreak;\n\t\t \n\t\tenqueue_hugetlb_folio(h, folio);\n\t}\nfree:\n\tspin_unlock_irq(&hugetlb_lock);\n\n\t \n\tlist_for_each_entry_safe(folio, tmp, &surplus_list, lru)\n\t\tfree_huge_folio(folio);\n\tspin_lock_irq(&hugetlb_lock);\n\n\treturn ret;\n}\n\n \nstatic void return_unused_surplus_pages(struct hstate *h,\n\t\t\t\t\tunsigned long unused_resv_pages)\n{\n\tunsigned long nr_pages;\n\tstruct page *page;\n\tLIST_HEAD(page_list);\n\n\tlockdep_assert_held(&hugetlb_lock);\n\t \n\th->resv_huge_pages -= unused_resv_pages;\n\n\tif (hstate_is_gigantic(h) && !gigantic_page_runtime_supported())\n\t\tgoto out;\n\n\t \n\tnr_pages = min(unused_resv_pages, h->surplus_huge_pages);\n\n\t \n\twhile (nr_pages--) {\n\t\tpage = remove_pool_huge_page(h, &node_states[N_MEMORY], 1);\n\t\tif (!page)\n\t\t\tgoto out;\n\n\t\tlist_add(&page->lru, &page_list);\n\t}\n\nout:\n\tspin_unlock_irq(&hugetlb_lock);\n\tupdate_and_free_pages_bulk(h, &page_list);\n\tspin_lock_irq(&hugetlb_lock);\n}\n\n\n \nenum vma_resv_mode {\n\tVMA_NEEDS_RESV,\n\tVMA_COMMIT_RESV,\n\tVMA_END_RESV,\n\tVMA_ADD_RESV,\n\tVMA_DEL_RESV,\n};\nstatic long __vma_reservation_common(struct hstate *h,\n\t\t\t\tstruct vm_area_struct *vma, unsigned long addr,\n\t\t\t\tenum vma_resv_mode mode)\n{\n\tstruct resv_map *resv;\n\tpgoff_t idx;\n\tlong ret;\n\tlong dummy_out_regions_needed;\n\n\tresv = vma_resv_map(vma);\n\tif (!resv)\n\t\treturn 1;\n\n\tidx = vma_hugecache_offset(h, vma, addr);\n\tswitch (mode) {\n\tcase VMA_NEEDS_RESV:\n\t\tret = region_chg(resv, idx, idx + 1, &dummy_out_regions_needed);\n\t\t \n\t\tVM_BUG_ON(dummy_out_regions_needed != 1);\n\t\tbreak;\n\tcase VMA_COMMIT_RESV:\n\t\tret = region_add(resv, idx, idx + 1, 1, NULL, NULL);\n\t\t \n\t\tVM_BUG_ON(ret < 0);\n\t\tbreak;\n\tcase VMA_END_RESV:\n\t\tregion_abort(resv, idx, idx + 1, 1);\n\t\tret = 0;\n\t\tbreak;\n\tcase VMA_ADD_RESV:\n\t\tif (vma->vm_flags & VM_MAYSHARE) {\n\t\t\tret = region_add(resv, idx, idx + 1, 1, NULL, NULL);\n\t\t\t \n\t\t\tVM_BUG_ON(ret < 0);\n\t\t} else {\n\t\t\tregion_abort(resv, idx, idx + 1, 1);\n\t\t\tret = region_del(resv, idx, idx + 1);\n\t\t}\n\t\tbreak;\n\tcase VMA_DEL_RESV:\n\t\tif (vma->vm_flags & VM_MAYSHARE) {\n\t\t\tregion_abort(resv, idx, idx + 1, 1);\n\t\t\tret = region_del(resv, idx, idx + 1);\n\t\t} else {\n\t\t\tret = region_add(resv, idx, idx + 1, 1, NULL, NULL);\n\t\t\t \n\t\t\tVM_BUG_ON(ret < 0);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\tif (vma->vm_flags & VM_MAYSHARE || mode == VMA_DEL_RESV)\n\t\treturn ret;\n\t \n\tif (ret > 0)\n\t\treturn 0;\n\tif (ret == 0)\n\t\treturn 1;\n\treturn ret;\n}\n\nstatic long vma_needs_reservation(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long addr)\n{\n\treturn __vma_reservation_common(h, vma, addr, VMA_NEEDS_RESV);\n}\n\nstatic long vma_commit_reservation(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long addr)\n{\n\treturn __vma_reservation_common(h, vma, addr, VMA_COMMIT_RESV);\n}\n\nstatic void vma_end_reservation(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long addr)\n{\n\t(void)__vma_reservation_common(h, vma, addr, VMA_END_RESV);\n}\n\nstatic long vma_add_reservation(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long addr)\n{\n\treturn __vma_reservation_common(h, vma, addr, VMA_ADD_RESV);\n}\n\nstatic long vma_del_reservation(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long addr)\n{\n\treturn __vma_reservation_common(h, vma, addr, VMA_DEL_RESV);\n}\n\n \nvoid restore_reserve_on_error(struct hstate *h, struct vm_area_struct *vma,\n\t\t\tunsigned long address, struct folio *folio)\n{\n\tlong rc = vma_needs_reservation(h, vma, address);\n\n\tif (folio_test_hugetlb_restore_reserve(folio)) {\n\t\tif (unlikely(rc < 0))\n\t\t\t \n\t\t\tfolio_clear_hugetlb_restore_reserve(folio);\n\t\telse if (rc)\n\t\t\t(void)vma_add_reservation(h, vma, address);\n\t\telse\n\t\t\tvma_end_reservation(h, vma, address);\n\t} else {\n\t\tif (!rc) {\n\t\t\t \n\t\t\trc = vma_del_reservation(h, vma, address);\n\t\t\tif (rc < 0)\n\t\t\t\t \n\t\t\t\tfolio_set_hugetlb_restore_reserve(folio);\n\t\t} else if (rc < 0) {\n\t\t\t \n\t\t\tif (!(vma->vm_flags & VM_MAYSHARE))\n\t\t\t\t \n\t\t\t\tfolio_set_hugetlb_restore_reserve(folio);\n\t\t} else\n\t\t\t \n\t\t\t vma_end_reservation(h, vma, address);\n\t}\n}\n\n \nstatic int alloc_and_dissolve_hugetlb_folio(struct hstate *h,\n\t\t\tstruct folio *old_folio, struct list_head *list)\n{\n\tgfp_t gfp_mask = htlb_alloc_mask(h) | __GFP_THISNODE;\n\tint nid = folio_nid(old_folio);\n\tstruct folio *new_folio;\n\tint ret = 0;\n\n\t \n\tnew_folio = alloc_buddy_hugetlb_folio(h, gfp_mask, nid, NULL, NULL);\n\tif (!new_folio)\n\t\treturn -ENOMEM;\n\t__prep_new_hugetlb_folio(h, new_folio);\n\nretry:\n\tspin_lock_irq(&hugetlb_lock);\n\tif (!folio_test_hugetlb(old_folio)) {\n\t\t \n\t\tgoto free_new;\n\t} else if (folio_ref_count(old_folio)) {\n\t\tbool isolated;\n\n\t\t \n\t\tspin_unlock_irq(&hugetlb_lock);\n\t\tisolated = isolate_hugetlb(old_folio, list);\n\t\tret = isolated ? 0 : -EBUSY;\n\t\tspin_lock_irq(&hugetlb_lock);\n\t\tgoto free_new;\n\t} else if (!folio_test_hugetlb_freed(old_folio)) {\n\t\t \n\t\tspin_unlock_irq(&hugetlb_lock);\n\t\tcond_resched();\n\t\tgoto retry;\n\t} else {\n\t\t \n\t\tremove_hugetlb_folio(h, old_folio, false);\n\n\t\t \n\t\t__prep_account_new_huge_page(h, nid);\n\t\tenqueue_hugetlb_folio(h, new_folio);\n\n\t\t \n\t\tspin_unlock_irq(&hugetlb_lock);\n\t\tupdate_and_free_hugetlb_folio(h, old_folio, false);\n\t}\n\n\treturn ret;\n\nfree_new:\n\tspin_unlock_irq(&hugetlb_lock);\n\t \n\tfolio_ref_unfreeze(new_folio, 1);\n\tupdate_and_free_hugetlb_folio(h, new_folio, false);\n\n\treturn ret;\n}\n\nint isolate_or_dissolve_huge_page(struct page *page, struct list_head *list)\n{\n\tstruct hstate *h;\n\tstruct folio *folio = page_folio(page);\n\tint ret = -EBUSY;\n\n\t \n\tspin_lock_irq(&hugetlb_lock);\n\tif (folio_test_hugetlb(folio)) {\n\t\th = folio_hstate(folio);\n\t} else {\n\t\tspin_unlock_irq(&hugetlb_lock);\n\t\treturn 0;\n\t}\n\tspin_unlock_irq(&hugetlb_lock);\n\n\t \n\tif (hstate_is_gigantic(h))\n\t\treturn -ENOMEM;\n\n\tif (folio_ref_count(folio) && isolate_hugetlb(folio, list))\n\t\tret = 0;\n\telse if (!folio_ref_count(folio))\n\t\tret = alloc_and_dissolve_hugetlb_folio(h, folio, list);\n\n\treturn ret;\n}\n\nstruct folio *alloc_hugetlb_folio(struct vm_area_struct *vma,\n\t\t\t\t    unsigned long addr, int avoid_reserve)\n{\n\tstruct hugepage_subpool *spool = subpool_vma(vma);\n\tstruct hstate *h = hstate_vma(vma);\n\tstruct folio *folio;\n\tlong map_chg, map_commit;\n\tlong gbl_chg;\n\tint ret, idx;\n\tstruct hugetlb_cgroup *h_cg = NULL;\n\tbool deferred_reserve;\n\n\tidx = hstate_index(h);\n\t \n\tmap_chg = gbl_chg = vma_needs_reservation(h, vma, addr);\n\tif (map_chg < 0)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\t \n\tif (map_chg || avoid_reserve) {\n\t\tgbl_chg = hugepage_subpool_get_pages(spool, 1);\n\t\tif (gbl_chg < 0) {\n\t\t\tvma_end_reservation(h, vma, addr);\n\t\t\treturn ERR_PTR(-ENOSPC);\n\t\t}\n\n\t\t \n\t\tif (avoid_reserve)\n\t\t\tgbl_chg = 1;\n\t}\n\n\t \n\tdeferred_reserve = map_chg || avoid_reserve;\n\tif (deferred_reserve) {\n\t\tret = hugetlb_cgroup_charge_cgroup_rsvd(\n\t\t\tidx, pages_per_huge_page(h), &h_cg);\n\t\tif (ret)\n\t\t\tgoto out_subpool_put;\n\t}\n\n\tret = hugetlb_cgroup_charge_cgroup(idx, pages_per_huge_page(h), &h_cg);\n\tif (ret)\n\t\tgoto out_uncharge_cgroup_reservation;\n\n\tspin_lock_irq(&hugetlb_lock);\n\t \n\tfolio = dequeue_hugetlb_folio_vma(h, vma, addr, avoid_reserve, gbl_chg);\n\tif (!folio) {\n\t\tspin_unlock_irq(&hugetlb_lock);\n\t\tfolio = alloc_buddy_hugetlb_folio_with_mpol(h, vma, addr);\n\t\tif (!folio)\n\t\t\tgoto out_uncharge_cgroup;\n\t\tspin_lock_irq(&hugetlb_lock);\n\t\tif (!avoid_reserve && vma_has_reserves(vma, gbl_chg)) {\n\t\t\tfolio_set_hugetlb_restore_reserve(folio);\n\t\t\th->resv_huge_pages--;\n\t\t}\n\t\tlist_add(&folio->lru, &h->hugepage_activelist);\n\t\tfolio_ref_unfreeze(folio, 1);\n\t\t \n\t}\n\n\thugetlb_cgroup_commit_charge(idx, pages_per_huge_page(h), h_cg, folio);\n\t \n\tif (deferred_reserve) {\n\t\thugetlb_cgroup_commit_charge_rsvd(idx, pages_per_huge_page(h),\n\t\t\t\t\t\t  h_cg, folio);\n\t}\n\n\tspin_unlock_irq(&hugetlb_lock);\n\n\thugetlb_set_folio_subpool(folio, spool);\n\n\tmap_commit = vma_commit_reservation(h, vma, addr);\n\tif (unlikely(map_chg > map_commit)) {\n\t\t \n\t\tlong rsv_adjust;\n\n\t\trsv_adjust = hugepage_subpool_put_pages(spool, 1);\n\t\thugetlb_acct_memory(h, -rsv_adjust);\n\t\tif (deferred_reserve)\n\t\t\thugetlb_cgroup_uncharge_folio_rsvd(hstate_index(h),\n\t\t\t\t\tpages_per_huge_page(h), folio);\n\t}\n\treturn folio;\n\nout_uncharge_cgroup:\n\thugetlb_cgroup_uncharge_cgroup(idx, pages_per_huge_page(h), h_cg);\nout_uncharge_cgroup_reservation:\n\tif (deferred_reserve)\n\t\thugetlb_cgroup_uncharge_cgroup_rsvd(idx, pages_per_huge_page(h),\n\t\t\t\t\t\t    h_cg);\nout_subpool_put:\n\tif (map_chg || avoid_reserve)\n\t\thugepage_subpool_put_pages(spool, 1);\n\tvma_end_reservation(h, vma, addr);\n\treturn ERR_PTR(-ENOSPC);\n}\n\nint alloc_bootmem_huge_page(struct hstate *h, int nid)\n\t__attribute__ ((weak, alias(\"__alloc_bootmem_huge_page\")));\nint __alloc_bootmem_huge_page(struct hstate *h, int nid)\n{\n\tstruct huge_bootmem_page *m = NULL;  \n\tint nr_nodes, node;\n\n\t \n\tif (nid != NUMA_NO_NODE) {\n\t\tm = memblock_alloc_try_nid_raw(huge_page_size(h), huge_page_size(h),\n\t\t\t\t0, MEMBLOCK_ALLOC_ACCESSIBLE, nid);\n\t\tif (!m)\n\t\t\treturn 0;\n\t\tgoto found;\n\t}\n\t \n\tfor_each_node_mask_to_alloc(h, nr_nodes, node, &node_states[N_MEMORY]) {\n\t\tm = memblock_alloc_try_nid_raw(\n\t\t\t\thuge_page_size(h), huge_page_size(h),\n\t\t\t\t0, MEMBLOCK_ALLOC_ACCESSIBLE, node);\n\t\t \n\t\tif (!m)\n\t\t\treturn 0;\n\t\tgoto found;\n\t}\n\nfound:\n\t \n\tINIT_LIST_HEAD(&m->list);\n\tlist_add(&m->list, &huge_boot_pages);\n\tm->hstate = h;\n\treturn 1;\n}\n\n \nstatic void __init gather_bootmem_prealloc(void)\n{\n\tstruct huge_bootmem_page *m;\n\n\tlist_for_each_entry(m, &huge_boot_pages, list) {\n\t\tstruct page *page = virt_to_page(m);\n\t\tstruct folio *folio = page_folio(page);\n\t\tstruct hstate *h = m->hstate;\n\n\t\tVM_BUG_ON(!hstate_is_gigantic(h));\n\t\tWARN_ON(folio_ref_count(folio) != 1);\n\t\tif (prep_compound_gigantic_folio(folio, huge_page_order(h))) {\n\t\t\tWARN_ON(folio_test_reserved(folio));\n\t\t\tprep_new_hugetlb_folio(h, folio, folio_nid(folio));\n\t\t\tfree_huge_folio(folio);  \n\t\t} else {\n\t\t\t \n\t\t\tfree_gigantic_folio(folio, huge_page_order(h));\n\t\t}\n\n\t\t \n\t\tadjust_managed_page_count(page, pages_per_huge_page(h));\n\t\tcond_resched();\n\t}\n}\nstatic void __init hugetlb_hstate_alloc_pages_onenode(struct hstate *h, int nid)\n{\n\tunsigned long i;\n\tchar buf[32];\n\n\tfor (i = 0; i < h->max_huge_pages_node[nid]; ++i) {\n\t\tif (hstate_is_gigantic(h)) {\n\t\t\tif (!alloc_bootmem_huge_page(h, nid))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tstruct folio *folio;\n\t\t\tgfp_t gfp_mask = htlb_alloc_mask(h) | __GFP_THISNODE;\n\n\t\t\tfolio = alloc_fresh_hugetlb_folio(h, gfp_mask, nid,\n\t\t\t\t\t&node_states[N_MEMORY], NULL);\n\t\t\tif (!folio)\n\t\t\t\tbreak;\n\t\t\tfree_huge_folio(folio);  \n\t\t}\n\t\tcond_resched();\n\t}\n\tif (i == h->max_huge_pages_node[nid])\n\t\treturn;\n\n\tstring_get_size(huge_page_size(h), 1, STRING_UNITS_2, buf, 32);\n\tpr_warn(\"HugeTLB: allocating %u of page size %s failed node%d.  Only allocated %lu hugepages.\\n\",\n\t\th->max_huge_pages_node[nid], buf, nid, i);\n\th->max_huge_pages -= (h->max_huge_pages_node[nid] - i);\n\th->max_huge_pages_node[nid] = i;\n}\n\nstatic void __init hugetlb_hstate_alloc_pages(struct hstate *h)\n{\n\tunsigned long i;\n\tnodemask_t *node_alloc_noretry;\n\tbool node_specific_alloc = false;\n\n\t \n\tif (hstate_is_gigantic(h) && hugetlb_cma_size) {\n\t\tpr_warn_once(\"HugeTLB: hugetlb_cma is enabled, skip boot time allocation\\n\");\n\t\treturn;\n\t}\n\n\t \n\tfor_each_online_node(i) {\n\t\tif (h->max_huge_pages_node[i] > 0) {\n\t\t\thugetlb_hstate_alloc_pages_onenode(h, i);\n\t\t\tnode_specific_alloc = true;\n\t\t}\n\t}\n\n\tif (node_specific_alloc)\n\t\treturn;\n\n\t \n\tif (!hstate_is_gigantic(h)) {\n\t\t \n\t\tnode_alloc_noretry = kmalloc(sizeof(*node_alloc_noretry),\n\t\t\t\t\t\tGFP_KERNEL);\n\t} else {\n\t\t \n\t\tnode_alloc_noretry = NULL;\n\t}\n\n\t \n\tif (node_alloc_noretry)\n\t\tnodes_clear(*node_alloc_noretry);\n\n\tfor (i = 0; i < h->max_huge_pages; ++i) {\n\t\tif (hstate_is_gigantic(h)) {\n\t\t\tif (!alloc_bootmem_huge_page(h, NUMA_NO_NODE))\n\t\t\t\tbreak;\n\t\t} else if (!alloc_pool_huge_page(h,\n\t\t\t\t\t &node_states[N_MEMORY],\n\t\t\t\t\t node_alloc_noretry))\n\t\t\tbreak;\n\t\tcond_resched();\n\t}\n\tif (i < h->max_huge_pages) {\n\t\tchar buf[32];\n\n\t\tstring_get_size(huge_page_size(h), 1, STRING_UNITS_2, buf, 32);\n\t\tpr_warn(\"HugeTLB: allocating %lu of page size %s failed.  Only allocated %lu hugepages.\\n\",\n\t\t\th->max_huge_pages, buf, i);\n\t\th->max_huge_pages = i;\n\t}\n\tkfree(node_alloc_noretry);\n}\n\nstatic void __init hugetlb_init_hstates(void)\n{\n\tstruct hstate *h, *h2;\n\n\tfor_each_hstate(h) {\n\t\t \n\t\tif (!hstate_is_gigantic(h))\n\t\t\thugetlb_hstate_alloc_pages(h);\n\n\t\t \n\t\tif (hstate_is_gigantic(h) && !gigantic_page_runtime_supported())\n\t\t\tcontinue;\n\t\tif (hugetlb_cma_size && h->order <= HUGETLB_PAGE_ORDER)\n\t\t\tcontinue;\n\t\tfor_each_hstate(h2) {\n\t\t\tif (h2 == h)\n\t\t\t\tcontinue;\n\t\t\tif (h2->order < h->order &&\n\t\t\t    h2->order > h->demote_order)\n\t\t\t\th->demote_order = h2->order;\n\t\t}\n\t}\n}\n\nstatic void __init report_hugepages(void)\n{\n\tstruct hstate *h;\n\n\tfor_each_hstate(h) {\n\t\tchar buf[32];\n\n\t\tstring_get_size(huge_page_size(h), 1, STRING_UNITS_2, buf, 32);\n\t\tpr_info(\"HugeTLB: registered %s page size, pre-allocated %ld pages\\n\",\n\t\t\tbuf, h->free_huge_pages);\n\t\tpr_info(\"HugeTLB: %d KiB vmemmap can be freed for a %s page\\n\",\n\t\t\thugetlb_vmemmap_optimizable_size(h) / SZ_1K, buf);\n\t}\n}\n\n#ifdef CONFIG_HIGHMEM\nstatic void try_to_free_low(struct hstate *h, unsigned long count,\n\t\t\t\t\t\tnodemask_t *nodes_allowed)\n{\n\tint i;\n\tLIST_HEAD(page_list);\n\n\tlockdep_assert_held(&hugetlb_lock);\n\tif (hstate_is_gigantic(h))\n\t\treturn;\n\n\t \n\tfor_each_node_mask(i, *nodes_allowed) {\n\t\tstruct page *page, *next;\n\t\tstruct list_head *freel = &h->hugepage_freelists[i];\n\t\tlist_for_each_entry_safe(page, next, freel, lru) {\n\t\t\tif (count >= h->nr_huge_pages)\n\t\t\t\tgoto out;\n\t\t\tif (PageHighMem(page))\n\t\t\t\tcontinue;\n\t\t\tremove_hugetlb_folio(h, page_folio(page), false);\n\t\t\tlist_add(&page->lru, &page_list);\n\t\t}\n\t}\n\nout:\n\tspin_unlock_irq(&hugetlb_lock);\n\tupdate_and_free_pages_bulk(h, &page_list);\n\tspin_lock_irq(&hugetlb_lock);\n}\n#else\nstatic inline void try_to_free_low(struct hstate *h, unsigned long count,\n\t\t\t\t\t\tnodemask_t *nodes_allowed)\n{\n}\n#endif\n\n \nstatic int adjust_pool_surplus(struct hstate *h, nodemask_t *nodes_allowed,\n\t\t\t\tint delta)\n{\n\tint nr_nodes, node;\n\n\tlockdep_assert_held(&hugetlb_lock);\n\tVM_BUG_ON(delta != -1 && delta != 1);\n\n\tif (delta < 0) {\n\t\tfor_each_node_mask_to_alloc(h, nr_nodes, node, nodes_allowed) {\n\t\t\tif (h->surplus_huge_pages_node[node])\n\t\t\t\tgoto found;\n\t\t}\n\t} else {\n\t\tfor_each_node_mask_to_free(h, nr_nodes, node, nodes_allowed) {\n\t\t\tif (h->surplus_huge_pages_node[node] <\n\t\t\t\t\th->nr_huge_pages_node[node])\n\t\t\t\tgoto found;\n\t\t}\n\t}\n\treturn 0;\n\nfound:\n\th->surplus_huge_pages += delta;\n\th->surplus_huge_pages_node[node] += delta;\n\treturn 1;\n}\n\n#define persistent_huge_pages(h) (h->nr_huge_pages - h->surplus_huge_pages)\nstatic int set_max_huge_pages(struct hstate *h, unsigned long count, int nid,\n\t\t\t      nodemask_t *nodes_allowed)\n{\n\tunsigned long min_count, ret;\n\tstruct page *page;\n\tLIST_HEAD(page_list);\n\tNODEMASK_ALLOC(nodemask_t, node_alloc_noretry, GFP_KERNEL);\n\n\t \n\tif (node_alloc_noretry)\n\t\tnodes_clear(*node_alloc_noretry);\n\telse\n\t\treturn -ENOMEM;\n\n\t \n\tmutex_lock(&h->resize_lock);\n\tflush_free_hpage_work(h);\n\tspin_lock_irq(&hugetlb_lock);\n\n\t \n\tif (nid != NUMA_NO_NODE) {\n\t\tunsigned long old_count = count;\n\n\t\tcount += h->nr_huge_pages - h->nr_huge_pages_node[nid];\n\t\t \n\t\tif (count < old_count)\n\t\t\tcount = ULONG_MAX;\n\t}\n\n\t \n\tif (hstate_is_gigantic(h) && !IS_ENABLED(CONFIG_CONTIG_ALLOC)) {\n\t\tif (count > persistent_huge_pages(h)) {\n\t\t\tspin_unlock_irq(&hugetlb_lock);\n\t\t\tmutex_unlock(&h->resize_lock);\n\t\t\tNODEMASK_FREE(node_alloc_noretry);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\t \n\t}\n\n\t \n\twhile (h->surplus_huge_pages && count > persistent_huge_pages(h)) {\n\t\tif (!adjust_pool_surplus(h, nodes_allowed, -1))\n\t\t\tbreak;\n\t}\n\n\twhile (count > persistent_huge_pages(h)) {\n\t\t \n\t\tspin_unlock_irq(&hugetlb_lock);\n\n\t\t \n\t\tcond_resched();\n\n\t\tret = alloc_pool_huge_page(h, nodes_allowed,\n\t\t\t\t\t\tnode_alloc_noretry);\n\t\tspin_lock_irq(&hugetlb_lock);\n\t\tif (!ret)\n\t\t\tgoto out;\n\n\t\t \n\t\tif (signal_pending(current))\n\t\t\tgoto out;\n\t}\n\n\t \n\tmin_count = h->resv_huge_pages + h->nr_huge_pages - h->free_huge_pages;\n\tmin_count = max(count, min_count);\n\ttry_to_free_low(h, min_count, nodes_allowed);\n\n\t \n\twhile (min_count < persistent_huge_pages(h)) {\n\t\tpage = remove_pool_huge_page(h, nodes_allowed, 0);\n\t\tif (!page)\n\t\t\tbreak;\n\n\t\tlist_add(&page->lru, &page_list);\n\t}\n\t \n\tspin_unlock_irq(&hugetlb_lock);\n\tupdate_and_free_pages_bulk(h, &page_list);\n\tflush_free_hpage_work(h);\n\tspin_lock_irq(&hugetlb_lock);\n\n\twhile (count < persistent_huge_pages(h)) {\n\t\tif (!adjust_pool_surplus(h, nodes_allowed, 1))\n\t\t\tbreak;\n\t}\nout:\n\th->max_huge_pages = persistent_huge_pages(h);\n\tspin_unlock_irq(&hugetlb_lock);\n\tmutex_unlock(&h->resize_lock);\n\n\tNODEMASK_FREE(node_alloc_noretry);\n\n\treturn 0;\n}\n\nstatic int demote_free_hugetlb_folio(struct hstate *h, struct folio *folio)\n{\n\tint i, nid = folio_nid(folio);\n\tstruct hstate *target_hstate;\n\tstruct page *subpage;\n\tstruct folio *inner_folio;\n\tint rc = 0;\n\n\ttarget_hstate = size_to_hstate(PAGE_SIZE << h->demote_order);\n\n\tremove_hugetlb_folio_for_demote(h, folio, false);\n\tspin_unlock_irq(&hugetlb_lock);\n\n\trc = hugetlb_vmemmap_restore(h, &folio->page);\n\tif (rc) {\n\t\t \n\t\tspin_lock_irq(&hugetlb_lock);\n\t\tfolio_ref_unfreeze(folio, 1);\n\t\tadd_hugetlb_folio(h, folio, false);\n\t\treturn rc;\n\t}\n\n\t \n\tdestroy_compound_hugetlb_folio_for_demote(folio, huge_page_order(h));\n\n\t \n\tmutex_lock(&target_hstate->resize_lock);\n\tfor (i = 0; i < pages_per_huge_page(h);\n\t\t\t\ti += pages_per_huge_page(target_hstate)) {\n\t\tsubpage = folio_page(folio, i);\n\t\tinner_folio = page_folio(subpage);\n\t\tif (hstate_is_gigantic(target_hstate))\n\t\t\tprep_compound_gigantic_folio_for_demote(inner_folio,\n\t\t\t\t\t\t\ttarget_hstate->order);\n\t\telse\n\t\t\tprep_compound_page(subpage, target_hstate->order);\n\t\tfolio_change_private(inner_folio, NULL);\n\t\tprep_new_hugetlb_folio(target_hstate, inner_folio, nid);\n\t\tfree_huge_folio(inner_folio);\n\t}\n\tmutex_unlock(&target_hstate->resize_lock);\n\n\tspin_lock_irq(&hugetlb_lock);\n\n\t \n\th->max_huge_pages--;\n\ttarget_hstate->max_huge_pages +=\n\t\tpages_per_huge_page(h) / pages_per_huge_page(target_hstate);\n\n\treturn rc;\n}\n\nstatic int demote_pool_huge_page(struct hstate *h, nodemask_t *nodes_allowed)\n\t__must_hold(&hugetlb_lock)\n{\n\tint nr_nodes, node;\n\tstruct folio *folio;\n\n\tlockdep_assert_held(&hugetlb_lock);\n\n\t \n\tif (!h->demote_order) {\n\t\tpr_warn(\"HugeTLB: NULL demote order passed to demote_pool_huge_page.\\n\");\n\t\treturn -EINVAL;\t\t \n\t}\n\n\tfor_each_node_mask_to_free(h, nr_nodes, node, nodes_allowed) {\n\t\tlist_for_each_entry(folio, &h->hugepage_freelists[node], lru) {\n\t\t\tif (folio_test_hwpoison(folio))\n\t\t\t\tcontinue;\n\t\t\treturn demote_free_hugetlb_folio(h, folio);\n\t\t}\n\t}\n\n\t \n\treturn -EBUSY;\n}\n\n#define HSTATE_ATTR_RO(_name) \\\n\tstatic struct kobj_attribute _name##_attr = __ATTR_RO(_name)\n\n#define HSTATE_ATTR_WO(_name) \\\n\tstatic struct kobj_attribute _name##_attr = __ATTR_WO(_name)\n\n#define HSTATE_ATTR(_name) \\\n\tstatic struct kobj_attribute _name##_attr = __ATTR_RW(_name)\n\nstatic struct kobject *hugepages_kobj;\nstatic struct kobject *hstate_kobjs[HUGE_MAX_HSTATE];\n\nstatic struct hstate *kobj_to_node_hstate(struct kobject *kobj, int *nidp);\n\nstatic struct hstate *kobj_to_hstate(struct kobject *kobj, int *nidp)\n{\n\tint i;\n\n\tfor (i = 0; i < HUGE_MAX_HSTATE; i++)\n\t\tif (hstate_kobjs[i] == kobj) {\n\t\t\tif (nidp)\n\t\t\t\t*nidp = NUMA_NO_NODE;\n\t\t\treturn &hstates[i];\n\t\t}\n\n\treturn kobj_to_node_hstate(kobj, nidp);\n}\n\nstatic ssize_t nr_hugepages_show_common(struct kobject *kobj,\n\t\t\t\t\tstruct kobj_attribute *attr, char *buf)\n{\n\tstruct hstate *h;\n\tunsigned long nr_huge_pages;\n\tint nid;\n\n\th = kobj_to_hstate(kobj, &nid);\n\tif (nid == NUMA_NO_NODE)\n\t\tnr_huge_pages = h->nr_huge_pages;\n\telse\n\t\tnr_huge_pages = h->nr_huge_pages_node[nid];\n\n\treturn sysfs_emit(buf, \"%lu\\n\", nr_huge_pages);\n}\n\nstatic ssize_t __nr_hugepages_store_common(bool obey_mempolicy,\n\t\t\t\t\t   struct hstate *h, int nid,\n\t\t\t\t\t   unsigned long count, size_t len)\n{\n\tint err;\n\tnodemask_t nodes_allowed, *n_mask;\n\n\tif (hstate_is_gigantic(h) && !gigantic_page_runtime_supported())\n\t\treturn -EINVAL;\n\n\tif (nid == NUMA_NO_NODE) {\n\t\t \n\t\tif (!(obey_mempolicy &&\n\t\t\t\tinit_nodemask_of_mempolicy(&nodes_allowed)))\n\t\t\tn_mask = &node_states[N_MEMORY];\n\t\telse\n\t\t\tn_mask = &nodes_allowed;\n\t} else {\n\t\t \n\t\tinit_nodemask_of_node(&nodes_allowed, nid);\n\t\tn_mask = &nodes_allowed;\n\t}\n\n\terr = set_max_huge_pages(h, count, nid, n_mask);\n\n\treturn err ? err : len;\n}\n\nstatic ssize_t nr_hugepages_store_common(bool obey_mempolicy,\n\t\t\t\t\t struct kobject *kobj, const char *buf,\n\t\t\t\t\t size_t len)\n{\n\tstruct hstate *h;\n\tunsigned long count;\n\tint nid;\n\tint err;\n\n\terr = kstrtoul(buf, 10, &count);\n\tif (err)\n\t\treturn err;\n\n\th = kobj_to_hstate(kobj, &nid);\n\treturn __nr_hugepages_store_common(obey_mempolicy, h, nid, count, len);\n}\n\nstatic ssize_t nr_hugepages_show(struct kobject *kobj,\n\t\t\t\t       struct kobj_attribute *attr, char *buf)\n{\n\treturn nr_hugepages_show_common(kobj, attr, buf);\n}\n\nstatic ssize_t nr_hugepages_store(struct kobject *kobj,\n\t       struct kobj_attribute *attr, const char *buf, size_t len)\n{\n\treturn nr_hugepages_store_common(false, kobj, buf, len);\n}\nHSTATE_ATTR(nr_hugepages);\n\n#ifdef CONFIG_NUMA\n\n \nstatic ssize_t nr_hugepages_mempolicy_show(struct kobject *kobj,\n\t\t\t\t\t   struct kobj_attribute *attr,\n\t\t\t\t\t   char *buf)\n{\n\treturn nr_hugepages_show_common(kobj, attr, buf);\n}\n\nstatic ssize_t nr_hugepages_mempolicy_store(struct kobject *kobj,\n\t       struct kobj_attribute *attr, const char *buf, size_t len)\n{\n\treturn nr_hugepages_store_common(true, kobj, buf, len);\n}\nHSTATE_ATTR(nr_hugepages_mempolicy);\n#endif\n\n\nstatic ssize_t nr_overcommit_hugepages_show(struct kobject *kobj,\n\t\t\t\t\tstruct kobj_attribute *attr, char *buf)\n{\n\tstruct hstate *h = kobj_to_hstate(kobj, NULL);\n\treturn sysfs_emit(buf, \"%lu\\n\", h->nr_overcommit_huge_pages);\n}\n\nstatic ssize_t nr_overcommit_hugepages_store(struct kobject *kobj,\n\t\tstruct kobj_attribute *attr, const char *buf, size_t count)\n{\n\tint err;\n\tunsigned long input;\n\tstruct hstate *h = kobj_to_hstate(kobj, NULL);\n\n\tif (hstate_is_gigantic(h))\n\t\treturn -EINVAL;\n\n\terr = kstrtoul(buf, 10, &input);\n\tif (err)\n\t\treturn err;\n\n\tspin_lock_irq(&hugetlb_lock);\n\th->nr_overcommit_huge_pages = input;\n\tspin_unlock_irq(&hugetlb_lock);\n\n\treturn count;\n}\nHSTATE_ATTR(nr_overcommit_hugepages);\n\nstatic ssize_t free_hugepages_show(struct kobject *kobj,\n\t\t\t\t\tstruct kobj_attribute *attr, char *buf)\n{\n\tstruct hstate *h;\n\tunsigned long free_huge_pages;\n\tint nid;\n\n\th = kobj_to_hstate(kobj, &nid);\n\tif (nid == NUMA_NO_NODE)\n\t\tfree_huge_pages = h->free_huge_pages;\n\telse\n\t\tfree_huge_pages = h->free_huge_pages_node[nid];\n\n\treturn sysfs_emit(buf, \"%lu\\n\", free_huge_pages);\n}\nHSTATE_ATTR_RO(free_hugepages);\n\nstatic ssize_t resv_hugepages_show(struct kobject *kobj,\n\t\t\t\t\tstruct kobj_attribute *attr, char *buf)\n{\n\tstruct hstate *h = kobj_to_hstate(kobj, NULL);\n\treturn sysfs_emit(buf, \"%lu\\n\", h->resv_huge_pages);\n}\nHSTATE_ATTR_RO(resv_hugepages);\n\nstatic ssize_t surplus_hugepages_show(struct kobject *kobj,\n\t\t\t\t\tstruct kobj_attribute *attr, char *buf)\n{\n\tstruct hstate *h;\n\tunsigned long surplus_huge_pages;\n\tint nid;\n\n\th = kobj_to_hstate(kobj, &nid);\n\tif (nid == NUMA_NO_NODE)\n\t\tsurplus_huge_pages = h->surplus_huge_pages;\n\telse\n\t\tsurplus_huge_pages = h->surplus_huge_pages_node[nid];\n\n\treturn sysfs_emit(buf, \"%lu\\n\", surplus_huge_pages);\n}\nHSTATE_ATTR_RO(surplus_hugepages);\n\nstatic ssize_t demote_store(struct kobject *kobj,\n\t       struct kobj_attribute *attr, const char *buf, size_t len)\n{\n\tunsigned long nr_demote;\n\tunsigned long nr_available;\n\tnodemask_t nodes_allowed, *n_mask;\n\tstruct hstate *h;\n\tint err;\n\tint nid;\n\n\terr = kstrtoul(buf, 10, &nr_demote);\n\tif (err)\n\t\treturn err;\n\th = kobj_to_hstate(kobj, &nid);\n\n\tif (nid != NUMA_NO_NODE) {\n\t\tinit_nodemask_of_node(&nodes_allowed, nid);\n\t\tn_mask = &nodes_allowed;\n\t} else {\n\t\tn_mask = &node_states[N_MEMORY];\n\t}\n\n\t \n\tmutex_lock(&h->resize_lock);\n\tspin_lock_irq(&hugetlb_lock);\n\n\twhile (nr_demote) {\n\t\t \n\t\tif (nid != NUMA_NO_NODE)\n\t\t\tnr_available = h->free_huge_pages_node[nid];\n\t\telse\n\t\t\tnr_available = h->free_huge_pages;\n\t\tnr_available -= h->resv_huge_pages;\n\t\tif (!nr_available)\n\t\t\tbreak;\n\n\t\terr = demote_pool_huge_page(h, n_mask);\n\t\tif (err)\n\t\t\tbreak;\n\n\t\tnr_demote--;\n\t}\n\n\tspin_unlock_irq(&hugetlb_lock);\n\tmutex_unlock(&h->resize_lock);\n\n\tif (err)\n\t\treturn err;\n\treturn len;\n}\nHSTATE_ATTR_WO(demote);\n\nstatic ssize_t demote_size_show(struct kobject *kobj,\n\t\t\t\t\tstruct kobj_attribute *attr, char *buf)\n{\n\tstruct hstate *h = kobj_to_hstate(kobj, NULL);\n\tunsigned long demote_size = (PAGE_SIZE << h->demote_order) / SZ_1K;\n\n\treturn sysfs_emit(buf, \"%lukB\\n\", demote_size);\n}\n\nstatic ssize_t demote_size_store(struct kobject *kobj,\n\t\t\t\t\tstruct kobj_attribute *attr,\n\t\t\t\t\tconst char *buf, size_t count)\n{\n\tstruct hstate *h, *demote_hstate;\n\tunsigned long demote_size;\n\tunsigned int demote_order;\n\n\tdemote_size = (unsigned long)memparse(buf, NULL);\n\n\tdemote_hstate = size_to_hstate(demote_size);\n\tif (!demote_hstate)\n\t\treturn -EINVAL;\n\tdemote_order = demote_hstate->order;\n\tif (demote_order < HUGETLB_PAGE_ORDER)\n\t\treturn -EINVAL;\n\n\t \n\th = kobj_to_hstate(kobj, NULL);\n\tif (demote_order >= h->order)\n\t\treturn -EINVAL;\n\n\t \n\tmutex_lock(&h->resize_lock);\n\th->demote_order = demote_order;\n\tmutex_unlock(&h->resize_lock);\n\n\treturn count;\n}\nHSTATE_ATTR(demote_size);\n\nstatic struct attribute *hstate_attrs[] = {\n\t&nr_hugepages_attr.attr,\n\t&nr_overcommit_hugepages_attr.attr,\n\t&free_hugepages_attr.attr,\n\t&resv_hugepages_attr.attr,\n\t&surplus_hugepages_attr.attr,\n#ifdef CONFIG_NUMA\n\t&nr_hugepages_mempolicy_attr.attr,\n#endif\n\tNULL,\n};\n\nstatic const struct attribute_group hstate_attr_group = {\n\t.attrs = hstate_attrs,\n};\n\nstatic struct attribute *hstate_demote_attrs[] = {\n\t&demote_size_attr.attr,\n\t&demote_attr.attr,\n\tNULL,\n};\n\nstatic const struct attribute_group hstate_demote_attr_group = {\n\t.attrs = hstate_demote_attrs,\n};\n\nstatic int hugetlb_sysfs_add_hstate(struct hstate *h, struct kobject *parent,\n\t\t\t\t    struct kobject **hstate_kobjs,\n\t\t\t\t    const struct attribute_group *hstate_attr_group)\n{\n\tint retval;\n\tint hi = hstate_index(h);\n\n\thstate_kobjs[hi] = kobject_create_and_add(h->name, parent);\n\tif (!hstate_kobjs[hi])\n\t\treturn -ENOMEM;\n\n\tretval = sysfs_create_group(hstate_kobjs[hi], hstate_attr_group);\n\tif (retval) {\n\t\tkobject_put(hstate_kobjs[hi]);\n\t\thstate_kobjs[hi] = NULL;\n\t\treturn retval;\n\t}\n\n\tif (h->demote_order) {\n\t\tretval = sysfs_create_group(hstate_kobjs[hi],\n\t\t\t\t\t    &hstate_demote_attr_group);\n\t\tif (retval) {\n\t\t\tpr_warn(\"HugeTLB unable to create demote interfaces for %s\\n\", h->name);\n\t\t\tsysfs_remove_group(hstate_kobjs[hi], hstate_attr_group);\n\t\t\tkobject_put(hstate_kobjs[hi]);\n\t\t\thstate_kobjs[hi] = NULL;\n\t\t\treturn retval;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n#ifdef CONFIG_NUMA\nstatic bool hugetlb_sysfs_initialized __ro_after_init;\n\n \nstruct node_hstate {\n\tstruct kobject\t\t*hugepages_kobj;\n\tstruct kobject\t\t*hstate_kobjs[HUGE_MAX_HSTATE];\n};\nstatic struct node_hstate node_hstates[MAX_NUMNODES];\n\n \nstatic struct attribute *per_node_hstate_attrs[] = {\n\t&nr_hugepages_attr.attr,\n\t&free_hugepages_attr.attr,\n\t&surplus_hugepages_attr.attr,\n\tNULL,\n};\n\nstatic const struct attribute_group per_node_hstate_attr_group = {\n\t.attrs = per_node_hstate_attrs,\n};\n\n \nstatic struct hstate *kobj_to_node_hstate(struct kobject *kobj, int *nidp)\n{\n\tint nid;\n\n\tfor (nid = 0; nid < nr_node_ids; nid++) {\n\t\tstruct node_hstate *nhs = &node_hstates[nid];\n\t\tint i;\n\t\tfor (i = 0; i < HUGE_MAX_HSTATE; i++)\n\t\t\tif (nhs->hstate_kobjs[i] == kobj) {\n\t\t\t\tif (nidp)\n\t\t\t\t\t*nidp = nid;\n\t\t\t\treturn &hstates[i];\n\t\t\t}\n\t}\n\n\tBUG();\n\treturn NULL;\n}\n\n \nvoid hugetlb_unregister_node(struct node *node)\n{\n\tstruct hstate *h;\n\tstruct node_hstate *nhs = &node_hstates[node->dev.id];\n\n\tif (!nhs->hugepages_kobj)\n\t\treturn;\t\t \n\n\tfor_each_hstate(h) {\n\t\tint idx = hstate_index(h);\n\t\tstruct kobject *hstate_kobj = nhs->hstate_kobjs[idx];\n\n\t\tif (!hstate_kobj)\n\t\t\tcontinue;\n\t\tif (h->demote_order)\n\t\t\tsysfs_remove_group(hstate_kobj, &hstate_demote_attr_group);\n\t\tsysfs_remove_group(hstate_kobj, &per_node_hstate_attr_group);\n\t\tkobject_put(hstate_kobj);\n\t\tnhs->hstate_kobjs[idx] = NULL;\n\t}\n\n\tkobject_put(nhs->hugepages_kobj);\n\tnhs->hugepages_kobj = NULL;\n}\n\n\n \nvoid hugetlb_register_node(struct node *node)\n{\n\tstruct hstate *h;\n\tstruct node_hstate *nhs = &node_hstates[node->dev.id];\n\tint err;\n\n\tif (!hugetlb_sysfs_initialized)\n\t\treturn;\n\n\tif (nhs->hugepages_kobj)\n\t\treturn;\t\t \n\n\tnhs->hugepages_kobj = kobject_create_and_add(\"hugepages\",\n\t\t\t\t\t\t\t&node->dev.kobj);\n\tif (!nhs->hugepages_kobj)\n\t\treturn;\n\n\tfor_each_hstate(h) {\n\t\terr = hugetlb_sysfs_add_hstate(h, nhs->hugepages_kobj,\n\t\t\t\t\t\tnhs->hstate_kobjs,\n\t\t\t\t\t\t&per_node_hstate_attr_group);\n\t\tif (err) {\n\t\t\tpr_err(\"HugeTLB: Unable to add hstate %s for node %d\\n\",\n\t\t\t\th->name, node->dev.id);\n\t\t\thugetlb_unregister_node(node);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\n \nstatic void __init hugetlb_register_all_nodes(void)\n{\n\tint nid;\n\n\tfor_each_online_node(nid)\n\t\thugetlb_register_node(node_devices[nid]);\n}\n#else\t \n\nstatic struct hstate *kobj_to_node_hstate(struct kobject *kobj, int *nidp)\n{\n\tBUG();\n\tif (nidp)\n\t\t*nidp = -1;\n\treturn NULL;\n}\n\nstatic void hugetlb_register_all_nodes(void) { }\n\n#endif\n\n#ifdef CONFIG_CMA\nstatic void __init hugetlb_cma_check(void);\n#else\nstatic inline __init void hugetlb_cma_check(void)\n{\n}\n#endif\n\nstatic void __init hugetlb_sysfs_init(void)\n{\n\tstruct hstate *h;\n\tint err;\n\n\thugepages_kobj = kobject_create_and_add(\"hugepages\", mm_kobj);\n\tif (!hugepages_kobj)\n\t\treturn;\n\n\tfor_each_hstate(h) {\n\t\terr = hugetlb_sysfs_add_hstate(h, hugepages_kobj,\n\t\t\t\t\t hstate_kobjs, &hstate_attr_group);\n\t\tif (err)\n\t\t\tpr_err(\"HugeTLB: Unable to add hstate %s\", h->name);\n\t}\n\n#ifdef CONFIG_NUMA\n\thugetlb_sysfs_initialized = true;\n#endif\n\thugetlb_register_all_nodes();\n}\n\n#ifdef CONFIG_SYSCTL\nstatic void hugetlb_sysctl_init(void);\n#else\nstatic inline void hugetlb_sysctl_init(void) { }\n#endif\n\nstatic int __init hugetlb_init(void)\n{\n\tint i;\n\n\tBUILD_BUG_ON(sizeof_field(struct page, private) * BITS_PER_BYTE <\n\t\t\t__NR_HPAGEFLAGS);\n\n\tif (!hugepages_supported()) {\n\t\tif (hugetlb_max_hstate || default_hstate_max_huge_pages)\n\t\t\tpr_warn(\"HugeTLB: huge pages not supported, ignoring associated command-line parameters\\n\");\n\t\treturn 0;\n\t}\n\n\t \n\thugetlb_add_hstate(HUGETLB_PAGE_ORDER);\n\tif (!parsed_default_hugepagesz) {\n\t\t \n\t\tdefault_hstate_idx = hstate_index(size_to_hstate(HPAGE_SIZE));\n\t\tif (default_hstate_max_huge_pages) {\n\t\t\tif (default_hstate.max_huge_pages) {\n\t\t\t\tchar buf[32];\n\n\t\t\t\tstring_get_size(huge_page_size(&default_hstate),\n\t\t\t\t\t1, STRING_UNITS_2, buf, 32);\n\t\t\t\tpr_warn(\"HugeTLB: Ignoring hugepages=%lu associated with %s page size\\n\",\n\t\t\t\t\tdefault_hstate.max_huge_pages, buf);\n\t\t\t\tpr_warn(\"HugeTLB: Using hugepages=%lu for number of default huge pages\\n\",\n\t\t\t\t\tdefault_hstate_max_huge_pages);\n\t\t\t}\n\t\t\tdefault_hstate.max_huge_pages =\n\t\t\t\tdefault_hstate_max_huge_pages;\n\n\t\t\tfor_each_online_node(i)\n\t\t\t\tdefault_hstate.max_huge_pages_node[i] =\n\t\t\t\t\tdefault_hugepages_in_node[i];\n\t\t}\n\t}\n\n\thugetlb_cma_check();\n\thugetlb_init_hstates();\n\tgather_bootmem_prealloc();\n\treport_hugepages();\n\n\thugetlb_sysfs_init();\n\thugetlb_cgroup_file_init();\n\thugetlb_sysctl_init();\n\n#ifdef CONFIG_SMP\n\tnum_fault_mutexes = roundup_pow_of_two(8 * num_possible_cpus());\n#else\n\tnum_fault_mutexes = 1;\n#endif\n\thugetlb_fault_mutex_table =\n\t\tkmalloc_array(num_fault_mutexes, sizeof(struct mutex),\n\t\t\t      GFP_KERNEL);\n\tBUG_ON(!hugetlb_fault_mutex_table);\n\n\tfor (i = 0; i < num_fault_mutexes; i++)\n\t\tmutex_init(&hugetlb_fault_mutex_table[i]);\n\treturn 0;\n}\nsubsys_initcall(hugetlb_init);\n\n \nbool __init __attribute((weak)) arch_hugetlb_valid_size(unsigned long size)\n{\n\treturn size == HPAGE_SIZE;\n}\n\nvoid __init hugetlb_add_hstate(unsigned int order)\n{\n\tstruct hstate *h;\n\tunsigned long i;\n\n\tif (size_to_hstate(PAGE_SIZE << order)) {\n\t\treturn;\n\t}\n\tBUG_ON(hugetlb_max_hstate >= HUGE_MAX_HSTATE);\n\tBUG_ON(order == 0);\n\th = &hstates[hugetlb_max_hstate++];\n\tmutex_init(&h->resize_lock);\n\th->order = order;\n\th->mask = ~(huge_page_size(h) - 1);\n\tfor (i = 0; i < MAX_NUMNODES; ++i)\n\t\tINIT_LIST_HEAD(&h->hugepage_freelists[i]);\n\tINIT_LIST_HEAD(&h->hugepage_activelist);\n\th->next_nid_to_alloc = first_memory_node;\n\th->next_nid_to_free = first_memory_node;\n\tsnprintf(h->name, HSTATE_NAME_LEN, \"hugepages-%lukB\",\n\t\t\t\t\thuge_page_size(h)/SZ_1K);\n\n\tparsed_hstate = h;\n}\n\nbool __init __weak hugetlb_node_alloc_supported(void)\n{\n\treturn true;\n}\n\nstatic void __init hugepages_clear_pages_in_node(void)\n{\n\tif (!hugetlb_max_hstate) {\n\t\tdefault_hstate_max_huge_pages = 0;\n\t\tmemset(default_hugepages_in_node, 0,\n\t\t\tsizeof(default_hugepages_in_node));\n\t} else {\n\t\tparsed_hstate->max_huge_pages = 0;\n\t\tmemset(parsed_hstate->max_huge_pages_node, 0,\n\t\t\tsizeof(parsed_hstate->max_huge_pages_node));\n\t}\n}\n\n \nstatic int __init hugepages_setup(char *s)\n{\n\tunsigned long *mhp;\n\tstatic unsigned long *last_mhp;\n\tint node = NUMA_NO_NODE;\n\tint count;\n\tunsigned long tmp;\n\tchar *p = s;\n\n\tif (!parsed_valid_hugepagesz) {\n\t\tpr_warn(\"HugeTLB: hugepages=%s does not follow a valid hugepagesz, ignoring\\n\", s);\n\t\tparsed_valid_hugepagesz = true;\n\t\treturn 1;\n\t}\n\n\t \n\telse if (!hugetlb_max_hstate)\n\t\tmhp = &default_hstate_max_huge_pages;\n\telse\n\t\tmhp = &parsed_hstate->max_huge_pages;\n\n\tif (mhp == last_mhp) {\n\t\tpr_warn(\"HugeTLB: hugepages= specified twice without interleaving hugepagesz=, ignoring hugepages=%s\\n\", s);\n\t\treturn 1;\n\t}\n\n\twhile (*p) {\n\t\tcount = 0;\n\t\tif (sscanf(p, \"%lu%n\", &tmp, &count) != 1)\n\t\t\tgoto invalid;\n\t\t \n\t\tif (p[count] == ':') {\n\t\t\tif (!hugetlb_node_alloc_supported()) {\n\t\t\t\tpr_warn(\"HugeTLB: architecture can't support node specific alloc, ignoring!\\n\");\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\tif (tmp >= MAX_NUMNODES || !node_online(tmp))\n\t\t\t\tgoto invalid;\n\t\t\tnode = array_index_nospec(tmp, MAX_NUMNODES);\n\t\t\tp += count + 1;\n\t\t\t \n\t\t\tif (sscanf(p, \"%lu%n\", &tmp, &count) != 1)\n\t\t\t\tgoto invalid;\n\t\t\tif (!hugetlb_max_hstate)\n\t\t\t\tdefault_hugepages_in_node[node] = tmp;\n\t\t\telse\n\t\t\t\tparsed_hstate->max_huge_pages_node[node] = tmp;\n\t\t\t*mhp += tmp;\n\t\t\t \n\t\t\tif (p[count] == ',')\n\t\t\t\tp += count + 1;\n\t\t\telse\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tif (p != s)\n\t\t\t\tgoto invalid;\n\t\t\t*mhp = tmp;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\tif (hugetlb_max_hstate && hstate_is_gigantic(parsed_hstate))\n\t\thugetlb_hstate_alloc_pages(parsed_hstate);\n\n\tlast_mhp = mhp;\n\n\treturn 1;\n\ninvalid:\n\tpr_warn(\"HugeTLB: Invalid hugepages parameter %s\\n\", p);\n\thugepages_clear_pages_in_node();\n\treturn 1;\n}\n__setup(\"hugepages=\", hugepages_setup);\n\n \nstatic int __init hugepagesz_setup(char *s)\n{\n\tunsigned long size;\n\tstruct hstate *h;\n\n\tparsed_valid_hugepagesz = false;\n\tsize = (unsigned long)memparse(s, NULL);\n\n\tif (!arch_hugetlb_valid_size(size)) {\n\t\tpr_err(\"HugeTLB: unsupported hugepagesz=%s\\n\", s);\n\t\treturn 1;\n\t}\n\n\th = size_to_hstate(size);\n\tif (h) {\n\t\t \n\t\tif (!parsed_default_hugepagesz ||  h != &default_hstate ||\n\t\t    default_hstate.max_huge_pages) {\n\t\t\tpr_warn(\"HugeTLB: hugepagesz=%s specified twice, ignoring\\n\", s);\n\t\t\treturn 1;\n\t\t}\n\n\t\t \n\t\tparsed_hstate = h;\n\t\tparsed_valid_hugepagesz = true;\n\t\treturn 1;\n\t}\n\n\thugetlb_add_hstate(ilog2(size) - PAGE_SHIFT);\n\tparsed_valid_hugepagesz = true;\n\treturn 1;\n}\n__setup(\"hugepagesz=\", hugepagesz_setup);\n\n \nstatic int __init default_hugepagesz_setup(char *s)\n{\n\tunsigned long size;\n\tint i;\n\n\tparsed_valid_hugepagesz = false;\n\tif (parsed_default_hugepagesz) {\n\t\tpr_err(\"HugeTLB: default_hugepagesz previously specified, ignoring %s\\n\", s);\n\t\treturn 1;\n\t}\n\n\tsize = (unsigned long)memparse(s, NULL);\n\n\tif (!arch_hugetlb_valid_size(size)) {\n\t\tpr_err(\"HugeTLB: unsupported default_hugepagesz=%s\\n\", s);\n\t\treturn 1;\n\t}\n\n\thugetlb_add_hstate(ilog2(size) - PAGE_SHIFT);\n\tparsed_valid_hugepagesz = true;\n\tparsed_default_hugepagesz = true;\n\tdefault_hstate_idx = hstate_index(size_to_hstate(size));\n\n\t \n\tif (default_hstate_max_huge_pages) {\n\t\tdefault_hstate.max_huge_pages = default_hstate_max_huge_pages;\n\t\tfor_each_online_node(i)\n\t\t\tdefault_hstate.max_huge_pages_node[i] =\n\t\t\t\tdefault_hugepages_in_node[i];\n\t\tif (hstate_is_gigantic(&default_hstate))\n\t\t\thugetlb_hstate_alloc_pages(&default_hstate);\n\t\tdefault_hstate_max_huge_pages = 0;\n\t}\n\n\treturn 1;\n}\n__setup(\"default_hugepagesz=\", default_hugepagesz_setup);\n\nstatic nodemask_t *policy_mbind_nodemask(gfp_t gfp)\n{\n#ifdef CONFIG_NUMA\n\tstruct mempolicy *mpol = get_task_policy(current);\n\n\t \n\tif (mpol->mode == MPOL_BIND &&\n\t\t(apply_policy_zone(mpol, gfp_zone(gfp)) &&\n\t\t cpuset_nodemask_valid_mems_allowed(&mpol->nodes)))\n\t\treturn &mpol->nodes;\n#endif\n\treturn NULL;\n}\n\nstatic unsigned int allowed_mems_nr(struct hstate *h)\n{\n\tint node;\n\tunsigned int nr = 0;\n\tnodemask_t *mbind_nodemask;\n\tunsigned int *array = h->free_huge_pages_node;\n\tgfp_t gfp_mask = htlb_alloc_mask(h);\n\n\tmbind_nodemask = policy_mbind_nodemask(gfp_mask);\n\tfor_each_node_mask(node, cpuset_current_mems_allowed) {\n\t\tif (!mbind_nodemask || node_isset(node, *mbind_nodemask))\n\t\t\tnr += array[node];\n\t}\n\n\treturn nr;\n}\n\n#ifdef CONFIG_SYSCTL\nstatic int proc_hugetlb_doulongvec_minmax(struct ctl_table *table, int write,\n\t\t\t\t\t  void *buffer, size_t *length,\n\t\t\t\t\t  loff_t *ppos, unsigned long *out)\n{\n\tstruct ctl_table dup_table;\n\n\t \n\tdup_table = *table;\n\tdup_table.data = out;\n\n\treturn proc_doulongvec_minmax(&dup_table, write, buffer, length, ppos);\n}\n\nstatic int hugetlb_sysctl_handler_common(bool obey_mempolicy,\n\t\t\t struct ctl_table *table, int write,\n\t\t\t void *buffer, size_t *length, loff_t *ppos)\n{\n\tstruct hstate *h = &default_hstate;\n\tunsigned long tmp = h->max_huge_pages;\n\tint ret;\n\n\tif (!hugepages_supported())\n\t\treturn -EOPNOTSUPP;\n\n\tret = proc_hugetlb_doulongvec_minmax(table, write, buffer, length, ppos,\n\t\t\t\t\t     &tmp);\n\tif (ret)\n\t\tgoto out;\n\n\tif (write)\n\t\tret = __nr_hugepages_store_common(obey_mempolicy, h,\n\t\t\t\t\t\t  NUMA_NO_NODE, tmp, *length);\nout:\n\treturn ret;\n}\n\nstatic int hugetlb_sysctl_handler(struct ctl_table *table, int write,\n\t\t\t  void *buffer, size_t *length, loff_t *ppos)\n{\n\n\treturn hugetlb_sysctl_handler_common(false, table, write,\n\t\t\t\t\t\t\tbuffer, length, ppos);\n}\n\n#ifdef CONFIG_NUMA\nstatic int hugetlb_mempolicy_sysctl_handler(struct ctl_table *table, int write,\n\t\t\t  void *buffer, size_t *length, loff_t *ppos)\n{\n\treturn hugetlb_sysctl_handler_common(true, table, write,\n\t\t\t\t\t\t\tbuffer, length, ppos);\n}\n#endif  \n\nstatic int hugetlb_overcommit_handler(struct ctl_table *table, int write,\n\t\tvoid *buffer, size_t *length, loff_t *ppos)\n{\n\tstruct hstate *h = &default_hstate;\n\tunsigned long tmp;\n\tint ret;\n\n\tif (!hugepages_supported())\n\t\treturn -EOPNOTSUPP;\n\n\ttmp = h->nr_overcommit_huge_pages;\n\n\tif (write && hstate_is_gigantic(h))\n\t\treturn -EINVAL;\n\n\tret = proc_hugetlb_doulongvec_minmax(table, write, buffer, length, ppos,\n\t\t\t\t\t     &tmp);\n\tif (ret)\n\t\tgoto out;\n\n\tif (write) {\n\t\tspin_lock_irq(&hugetlb_lock);\n\t\th->nr_overcommit_huge_pages = tmp;\n\t\tspin_unlock_irq(&hugetlb_lock);\n\t}\nout:\n\treturn ret;\n}\n\nstatic struct ctl_table hugetlb_table[] = {\n\t{\n\t\t.procname\t= \"nr_hugepages\",\n\t\t.data\t\t= NULL,\n\t\t.maxlen\t\t= sizeof(unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= hugetlb_sysctl_handler,\n\t},\n#ifdef CONFIG_NUMA\n\t{\n\t\t.procname       = \"nr_hugepages_mempolicy\",\n\t\t.data           = NULL,\n\t\t.maxlen         = sizeof(unsigned long),\n\t\t.mode           = 0644,\n\t\t.proc_handler   = &hugetlb_mempolicy_sysctl_handler,\n\t},\n#endif\n\t{\n\t\t.procname\t= \"hugetlb_shm_group\",\n\t\t.data\t\t= &sysctl_hugetlb_shm_group,\n\t\t.maxlen\t\t= sizeof(gid_t),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"nr_overcommit_hugepages\",\n\t\t.data\t\t= NULL,\n\t\t.maxlen\t\t= sizeof(unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= hugetlb_overcommit_handler,\n\t},\n\t{ }\n};\n\nstatic void hugetlb_sysctl_init(void)\n{\n\tregister_sysctl_init(\"vm\", hugetlb_table);\n}\n#endif  \n\nvoid hugetlb_report_meminfo(struct seq_file *m)\n{\n\tstruct hstate *h;\n\tunsigned long total = 0;\n\n\tif (!hugepages_supported())\n\t\treturn;\n\n\tfor_each_hstate(h) {\n\t\tunsigned long count = h->nr_huge_pages;\n\n\t\ttotal += huge_page_size(h) * count;\n\n\t\tif (h == &default_hstate)\n\t\t\tseq_printf(m,\n\t\t\t\t   \"HugePages_Total:   %5lu\\n\"\n\t\t\t\t   \"HugePages_Free:    %5lu\\n\"\n\t\t\t\t   \"HugePages_Rsvd:    %5lu\\n\"\n\t\t\t\t   \"HugePages_Surp:    %5lu\\n\"\n\t\t\t\t   \"Hugepagesize:   %8lu kB\\n\",\n\t\t\t\t   count,\n\t\t\t\t   h->free_huge_pages,\n\t\t\t\t   h->resv_huge_pages,\n\t\t\t\t   h->surplus_huge_pages,\n\t\t\t\t   huge_page_size(h) / SZ_1K);\n\t}\n\n\tseq_printf(m, \"Hugetlb:        %8lu kB\\n\", total / SZ_1K);\n}\n\nint hugetlb_report_node_meminfo(char *buf, int len, int nid)\n{\n\tstruct hstate *h = &default_hstate;\n\n\tif (!hugepages_supported())\n\t\treturn 0;\n\n\treturn sysfs_emit_at(buf, len,\n\t\t\t     \"Node %d HugePages_Total: %5u\\n\"\n\t\t\t     \"Node %d HugePages_Free:  %5u\\n\"\n\t\t\t     \"Node %d HugePages_Surp:  %5u\\n\",\n\t\t\t     nid, h->nr_huge_pages_node[nid],\n\t\t\t     nid, h->free_huge_pages_node[nid],\n\t\t\t     nid, h->surplus_huge_pages_node[nid]);\n}\n\nvoid hugetlb_show_meminfo_node(int nid)\n{\n\tstruct hstate *h;\n\n\tif (!hugepages_supported())\n\t\treturn;\n\n\tfor_each_hstate(h)\n\t\tprintk(\"Node %d hugepages_total=%u hugepages_free=%u hugepages_surp=%u hugepages_size=%lukB\\n\",\n\t\t\tnid,\n\t\t\th->nr_huge_pages_node[nid],\n\t\t\th->free_huge_pages_node[nid],\n\t\t\th->surplus_huge_pages_node[nid],\n\t\t\thuge_page_size(h) / SZ_1K);\n}\n\nvoid hugetlb_report_usage(struct seq_file *m, struct mm_struct *mm)\n{\n\tseq_printf(m, \"HugetlbPages:\\t%8lu kB\\n\",\n\t\t   K(atomic_long_read(&mm->hugetlb_usage)));\n}\n\n \nunsigned long hugetlb_total_pages(void)\n{\n\tstruct hstate *h;\n\tunsigned long nr_total_pages = 0;\n\n\tfor_each_hstate(h)\n\t\tnr_total_pages += h->nr_huge_pages * pages_per_huge_page(h);\n\treturn nr_total_pages;\n}\n\nstatic int hugetlb_acct_memory(struct hstate *h, long delta)\n{\n\tint ret = -ENOMEM;\n\n\tif (!delta)\n\t\treturn 0;\n\n\tspin_lock_irq(&hugetlb_lock);\n\t \n\tif (delta > 0) {\n\t\tif (gather_surplus_pages(h, delta) < 0)\n\t\t\tgoto out;\n\n\t\tif (delta > allowed_mems_nr(h)) {\n\t\t\treturn_unused_surplus_pages(h, delta);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tret = 0;\n\tif (delta < 0)\n\t\treturn_unused_surplus_pages(h, (unsigned long) -delta);\n\nout:\n\tspin_unlock_irq(&hugetlb_lock);\n\treturn ret;\n}\n\nstatic void hugetlb_vm_op_open(struct vm_area_struct *vma)\n{\n\tstruct resv_map *resv = vma_resv_map(vma);\n\n\t \n\tif (resv && is_vma_resv_set(vma, HPAGE_RESV_OWNER)) {\n\t\tresv_map_dup_hugetlb_cgroup_uncharge_info(resv);\n\t\tkref_get(&resv->refs);\n\t}\n\n\t \n\tif (vma->vm_flags & VM_MAYSHARE) {\n\t\tstruct hugetlb_vma_lock *vma_lock = vma->vm_private_data;\n\n\t\tif (vma_lock) {\n\t\t\tif (vma_lock->vma != vma) {\n\t\t\t\tvma->vm_private_data = NULL;\n\t\t\t\thugetlb_vma_lock_alloc(vma);\n\t\t\t} else\n\t\t\t\tpr_warn(\"HugeTLB: vma_lock already exists in %s.\\n\", __func__);\n\t\t} else\n\t\t\thugetlb_vma_lock_alloc(vma);\n\t}\n}\n\nstatic void hugetlb_vm_op_close(struct vm_area_struct *vma)\n{\n\tstruct hstate *h = hstate_vma(vma);\n\tstruct resv_map *resv;\n\tstruct hugepage_subpool *spool = subpool_vma(vma);\n\tunsigned long reserve, start, end;\n\tlong gbl_reserve;\n\n\thugetlb_vma_lock_free(vma);\n\n\tresv = vma_resv_map(vma);\n\tif (!resv || !is_vma_resv_set(vma, HPAGE_RESV_OWNER))\n\t\treturn;\n\n\tstart = vma_hugecache_offset(h, vma, vma->vm_start);\n\tend = vma_hugecache_offset(h, vma, vma->vm_end);\n\n\treserve = (end - start) - region_count(resv, start, end);\n\thugetlb_cgroup_uncharge_counter(resv, start, end);\n\tif (reserve) {\n\t\t \n\t\tgbl_reserve = hugepage_subpool_put_pages(spool, reserve);\n\t\thugetlb_acct_memory(h, -gbl_reserve);\n\t}\n\n\tkref_put(&resv->refs, resv_map_release);\n}\n\nstatic int hugetlb_vm_op_split(struct vm_area_struct *vma, unsigned long addr)\n{\n\tif (addr & ~(huge_page_mask(hstate_vma(vma))))\n\t\treturn -EINVAL;\n\n\t \n\tif (addr & ~PUD_MASK) {\n\t\t \n\t\tunsigned long floor = addr & PUD_MASK;\n\t\tunsigned long ceil = floor + PUD_SIZE;\n\n\t\tif (floor >= vma->vm_start && ceil <= vma->vm_end)\n\t\t\thugetlb_unshare_pmds(vma, floor, ceil);\n\t}\n\n\treturn 0;\n}\n\nstatic unsigned long hugetlb_vm_op_pagesize(struct vm_area_struct *vma)\n{\n\treturn huge_page_size(hstate_vma(vma));\n}\n\n \nstatic vm_fault_t hugetlb_vm_op_fault(struct vm_fault *vmf)\n{\n\tBUG();\n\treturn 0;\n}\n\n \nconst struct vm_operations_struct hugetlb_vm_ops = {\n\t.fault = hugetlb_vm_op_fault,\n\t.open = hugetlb_vm_op_open,\n\t.close = hugetlb_vm_op_close,\n\t.may_split = hugetlb_vm_op_split,\n\t.pagesize = hugetlb_vm_op_pagesize,\n};\n\nstatic pte_t make_huge_pte(struct vm_area_struct *vma, struct page *page,\n\t\t\t\tint writable)\n{\n\tpte_t entry;\n\tunsigned int shift = huge_page_shift(hstate_vma(vma));\n\n\tif (writable) {\n\t\tentry = huge_pte_mkwrite(huge_pte_mkdirty(mk_huge_pte(page,\n\t\t\t\t\t vma->vm_page_prot)));\n\t} else {\n\t\tentry = huge_pte_wrprotect(mk_huge_pte(page,\n\t\t\t\t\t   vma->vm_page_prot));\n\t}\n\tentry = pte_mkyoung(entry);\n\tentry = arch_make_huge_pte(entry, shift, vma->vm_flags);\n\n\treturn entry;\n}\n\nstatic void set_huge_ptep_writable(struct vm_area_struct *vma,\n\t\t\t\t   unsigned long address, pte_t *ptep)\n{\n\tpte_t entry;\n\n\tentry = huge_pte_mkwrite(huge_pte_mkdirty(huge_ptep_get(ptep)));\n\tif (huge_ptep_set_access_flags(vma, address, ptep, entry, 1))\n\t\tupdate_mmu_cache(vma, address, ptep);\n}\n\nbool is_hugetlb_entry_migration(pte_t pte)\n{\n\tswp_entry_t swp;\n\n\tif (huge_pte_none(pte) || pte_present(pte))\n\t\treturn false;\n\tswp = pte_to_swp_entry(pte);\n\tif (is_migration_entry(swp))\n\t\treturn true;\n\telse\n\t\treturn false;\n}\n\nstatic bool is_hugetlb_entry_hwpoisoned(pte_t pte)\n{\n\tswp_entry_t swp;\n\n\tif (huge_pte_none(pte) || pte_present(pte))\n\t\treturn false;\n\tswp = pte_to_swp_entry(pte);\n\tif (is_hwpoison_entry(swp))\n\t\treturn true;\n\telse\n\t\treturn false;\n}\n\nstatic void\nhugetlb_install_folio(struct vm_area_struct *vma, pte_t *ptep, unsigned long addr,\n\t\t      struct folio *new_folio, pte_t old, unsigned long sz)\n{\n\tpte_t newpte = make_huge_pte(vma, &new_folio->page, 1);\n\n\t__folio_mark_uptodate(new_folio);\n\thugepage_add_new_anon_rmap(new_folio, vma, addr);\n\tif (userfaultfd_wp(vma) && huge_pte_uffd_wp(old))\n\t\tnewpte = huge_pte_mkuffd_wp(newpte);\n\tset_huge_pte_at(vma->vm_mm, addr, ptep, newpte, sz);\n\thugetlb_count_add(pages_per_huge_page(hstate_vma(vma)), vma->vm_mm);\n\tfolio_set_hugetlb_migratable(new_folio);\n}\n\nint copy_hugetlb_page_range(struct mm_struct *dst, struct mm_struct *src,\n\t\t\t    struct vm_area_struct *dst_vma,\n\t\t\t    struct vm_area_struct *src_vma)\n{\n\tpte_t *src_pte, *dst_pte, entry;\n\tstruct folio *pte_folio;\n\tunsigned long addr;\n\tbool cow = is_cow_mapping(src_vma->vm_flags);\n\tstruct hstate *h = hstate_vma(src_vma);\n\tunsigned long sz = huge_page_size(h);\n\tunsigned long npages = pages_per_huge_page(h);\n\tstruct mmu_notifier_range range;\n\tunsigned long last_addr_mask;\n\tint ret = 0;\n\n\tif (cow) {\n\t\tmmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, src,\n\t\t\t\t\tsrc_vma->vm_start,\n\t\t\t\t\tsrc_vma->vm_end);\n\t\tmmu_notifier_invalidate_range_start(&range);\n\t\tvma_assert_write_locked(src_vma);\n\t\traw_write_seqcount_begin(&src->write_protect_seq);\n\t} else {\n\t\t \n\t\thugetlb_vma_lock_read(src_vma);\n\t}\n\n\tlast_addr_mask = hugetlb_mask_last_page(h);\n\tfor (addr = src_vma->vm_start; addr < src_vma->vm_end; addr += sz) {\n\t\tspinlock_t *src_ptl, *dst_ptl;\n\t\tsrc_pte = hugetlb_walk(src_vma, addr, sz);\n\t\tif (!src_pte) {\n\t\t\taddr |= last_addr_mask;\n\t\t\tcontinue;\n\t\t}\n\t\tdst_pte = huge_pte_alloc(dst, dst_vma, addr, sz);\n\t\tif (!dst_pte) {\n\t\t\tret = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (page_count(virt_to_page(dst_pte)) > 1) {\n\t\t\taddr |= last_addr_mask;\n\t\t\tcontinue;\n\t\t}\n\n\t\tdst_ptl = huge_pte_lock(h, dst, dst_pte);\n\t\tsrc_ptl = huge_pte_lockptr(h, src, src_pte);\n\t\tspin_lock_nested(src_ptl, SINGLE_DEPTH_NESTING);\n\t\tentry = huge_ptep_get(src_pte);\nagain:\n\t\tif (huge_pte_none(entry)) {\n\t\t\t \n\t\t\t;\n\t\t} else if (unlikely(is_hugetlb_entry_hwpoisoned(entry))) {\n\t\t\tif (!userfaultfd_wp(dst_vma))\n\t\t\t\tentry = huge_pte_clear_uffd_wp(entry);\n\t\t\tset_huge_pte_at(dst, addr, dst_pte, entry, sz);\n\t\t} else if (unlikely(is_hugetlb_entry_migration(entry))) {\n\t\t\tswp_entry_t swp_entry = pte_to_swp_entry(entry);\n\t\t\tbool uffd_wp = pte_swp_uffd_wp(entry);\n\n\t\t\tif (!is_readable_migration_entry(swp_entry) && cow) {\n\t\t\t\t \n\t\t\t\tswp_entry = make_readable_migration_entry(\n\t\t\t\t\t\t\tswp_offset(swp_entry));\n\t\t\t\tentry = swp_entry_to_pte(swp_entry);\n\t\t\t\tif (userfaultfd_wp(src_vma) && uffd_wp)\n\t\t\t\t\tentry = pte_swp_mkuffd_wp(entry);\n\t\t\t\tset_huge_pte_at(src, addr, src_pte, entry, sz);\n\t\t\t}\n\t\t\tif (!userfaultfd_wp(dst_vma))\n\t\t\t\tentry = huge_pte_clear_uffd_wp(entry);\n\t\t\tset_huge_pte_at(dst, addr, dst_pte, entry, sz);\n\t\t} else if (unlikely(is_pte_marker(entry))) {\n\t\t\tpte_marker marker = copy_pte_marker(\n\t\t\t\tpte_to_swp_entry(entry), dst_vma);\n\n\t\t\tif (marker)\n\t\t\t\tset_huge_pte_at(dst, addr, dst_pte,\n\t\t\t\t\t\tmake_pte_marker(marker), sz);\n\t\t} else {\n\t\t\tentry = huge_ptep_get(src_pte);\n\t\t\tpte_folio = page_folio(pte_page(entry));\n\t\t\tfolio_get(pte_folio);\n\n\t\t\t \n\t\t\tif (!folio_test_anon(pte_folio)) {\n\t\t\t\tpage_dup_file_rmap(&pte_folio->page, true);\n\t\t\t} else if (page_try_dup_anon_rmap(&pte_folio->page,\n\t\t\t\t\t\t\t  true, src_vma)) {\n\t\t\t\tpte_t src_pte_old = entry;\n\t\t\t\tstruct folio *new_folio;\n\n\t\t\t\tspin_unlock(src_ptl);\n\t\t\t\tspin_unlock(dst_ptl);\n\t\t\t\t \n\t\t\t\tnew_folio = alloc_hugetlb_folio(dst_vma, addr, 1);\n\t\t\t\tif (IS_ERR(new_folio)) {\n\t\t\t\t\tfolio_put(pte_folio);\n\t\t\t\t\tret = PTR_ERR(new_folio);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tret = copy_user_large_folio(new_folio,\n\t\t\t\t\t\t\t    pte_folio,\n\t\t\t\t\t\t\t    addr, dst_vma);\n\t\t\t\tfolio_put(pte_folio);\n\t\t\t\tif (ret) {\n\t\t\t\t\tfolio_put(new_folio);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\t \n\t\t\t\tdst_ptl = huge_pte_lock(h, dst, dst_pte);\n\t\t\t\tsrc_ptl = huge_pte_lockptr(h, src, src_pte);\n\t\t\t\tspin_lock_nested(src_ptl, SINGLE_DEPTH_NESTING);\n\t\t\t\tentry = huge_ptep_get(src_pte);\n\t\t\t\tif (!pte_same(src_pte_old, entry)) {\n\t\t\t\t\trestore_reserve_on_error(h, dst_vma, addr,\n\t\t\t\t\t\t\t\tnew_folio);\n\t\t\t\t\tfolio_put(new_folio);\n\t\t\t\t\t \n\t\t\t\t\tgoto again;\n\t\t\t\t}\n\t\t\t\thugetlb_install_folio(dst_vma, dst_pte, addr,\n\t\t\t\t\t\t      new_folio, src_pte_old, sz);\n\t\t\t\tspin_unlock(src_ptl);\n\t\t\t\tspin_unlock(dst_ptl);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (cow) {\n\t\t\t\t \n\t\t\t\thuge_ptep_set_wrprotect(src, addr, src_pte);\n\t\t\t\tentry = huge_pte_wrprotect(entry);\n\t\t\t}\n\n\t\t\tif (!userfaultfd_wp(dst_vma))\n\t\t\t\tentry = huge_pte_clear_uffd_wp(entry);\n\n\t\t\tset_huge_pte_at(dst, addr, dst_pte, entry, sz);\n\t\t\thugetlb_count_add(npages, dst);\n\t\t}\n\t\tspin_unlock(src_ptl);\n\t\tspin_unlock(dst_ptl);\n\t}\n\n\tif (cow) {\n\t\traw_write_seqcount_end(&src->write_protect_seq);\n\t\tmmu_notifier_invalidate_range_end(&range);\n\t} else {\n\t\thugetlb_vma_unlock_read(src_vma);\n\t}\n\n\treturn ret;\n}\n\nstatic void move_huge_pte(struct vm_area_struct *vma, unsigned long old_addr,\n\t\t\t  unsigned long new_addr, pte_t *src_pte, pte_t *dst_pte,\n\t\t\t  unsigned long sz)\n{\n\tstruct hstate *h = hstate_vma(vma);\n\tstruct mm_struct *mm = vma->vm_mm;\n\tspinlock_t *src_ptl, *dst_ptl;\n\tpte_t pte;\n\n\tdst_ptl = huge_pte_lock(h, mm, dst_pte);\n\tsrc_ptl = huge_pte_lockptr(h, mm, src_pte);\n\n\t \n\tif (src_ptl != dst_ptl)\n\t\tspin_lock_nested(src_ptl, SINGLE_DEPTH_NESTING);\n\n\tpte = huge_ptep_get_and_clear(mm, old_addr, src_pte);\n\tset_huge_pte_at(mm, new_addr, dst_pte, pte, sz);\n\n\tif (src_ptl != dst_ptl)\n\t\tspin_unlock(src_ptl);\n\tspin_unlock(dst_ptl);\n}\n\nint move_hugetlb_page_tables(struct vm_area_struct *vma,\n\t\t\t     struct vm_area_struct *new_vma,\n\t\t\t     unsigned long old_addr, unsigned long new_addr,\n\t\t\t     unsigned long len)\n{\n\tstruct hstate *h = hstate_vma(vma);\n\tstruct address_space *mapping = vma->vm_file->f_mapping;\n\tunsigned long sz = huge_page_size(h);\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long old_end = old_addr + len;\n\tunsigned long last_addr_mask;\n\tpte_t *src_pte, *dst_pte;\n\tstruct mmu_notifier_range range;\n\tbool shared_pmd = false;\n\n\tmmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, mm, old_addr,\n\t\t\t\told_end);\n\tadjust_range_if_pmd_sharing_possible(vma, &range.start, &range.end);\n\t \n\tflush_cache_range(vma, range.start, range.end);\n\n\tmmu_notifier_invalidate_range_start(&range);\n\tlast_addr_mask = hugetlb_mask_last_page(h);\n\t \n\thugetlb_vma_lock_write(vma);\n\ti_mmap_lock_write(mapping);\n\tfor (; old_addr < old_end; old_addr += sz, new_addr += sz) {\n\t\tsrc_pte = hugetlb_walk(vma, old_addr, sz);\n\t\tif (!src_pte) {\n\t\t\told_addr |= last_addr_mask;\n\t\t\tnew_addr |= last_addr_mask;\n\t\t\tcontinue;\n\t\t}\n\t\tif (huge_pte_none(huge_ptep_get(src_pte)))\n\t\t\tcontinue;\n\n\t\tif (huge_pmd_unshare(mm, vma, old_addr, src_pte)) {\n\t\t\tshared_pmd = true;\n\t\t\told_addr |= last_addr_mask;\n\t\t\tnew_addr |= last_addr_mask;\n\t\t\tcontinue;\n\t\t}\n\n\t\tdst_pte = huge_pte_alloc(mm, new_vma, new_addr, sz);\n\t\tif (!dst_pte)\n\t\t\tbreak;\n\n\t\tmove_huge_pte(vma, old_addr, new_addr, src_pte, dst_pte, sz);\n\t}\n\n\tif (shared_pmd)\n\t\tflush_hugetlb_tlb_range(vma, range.start, range.end);\n\telse\n\t\tflush_hugetlb_tlb_range(vma, old_end - len, old_end);\n\tmmu_notifier_invalidate_range_end(&range);\n\ti_mmap_unlock_write(mapping);\n\thugetlb_vma_unlock_write(vma);\n\n\treturn len + old_addr - old_end;\n}\n\nvoid __unmap_hugepage_range(struct mmu_gather *tlb, struct vm_area_struct *vma,\n\t\t\t    unsigned long start, unsigned long end,\n\t\t\t    struct page *ref_page, zap_flags_t zap_flags)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long address;\n\tpte_t *ptep;\n\tpte_t pte;\n\tspinlock_t *ptl;\n\tstruct page *page;\n\tstruct hstate *h = hstate_vma(vma);\n\tunsigned long sz = huge_page_size(h);\n\tunsigned long last_addr_mask;\n\tbool force_flush = false;\n\n\tWARN_ON(!is_vm_hugetlb_page(vma));\n\tBUG_ON(start & ~huge_page_mask(h));\n\tBUG_ON(end & ~huge_page_mask(h));\n\n\t \n\ttlb_change_page_size(tlb, sz);\n\ttlb_start_vma(tlb, vma);\n\n\tlast_addr_mask = hugetlb_mask_last_page(h);\n\taddress = start;\n\tfor (; address < end; address += sz) {\n\t\tptep = hugetlb_walk(vma, address, sz);\n\t\tif (!ptep) {\n\t\t\taddress |= last_addr_mask;\n\t\t\tcontinue;\n\t\t}\n\n\t\tptl = huge_pte_lock(h, mm, ptep);\n\t\tif (huge_pmd_unshare(mm, vma, address, ptep)) {\n\t\t\tspin_unlock(ptl);\n\t\t\ttlb_flush_pmd_range(tlb, address & PUD_MASK, PUD_SIZE);\n\t\t\tforce_flush = true;\n\t\t\taddress |= last_addr_mask;\n\t\t\tcontinue;\n\t\t}\n\n\t\tpte = huge_ptep_get(ptep);\n\t\tif (huge_pte_none(pte)) {\n\t\t\tspin_unlock(ptl);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (unlikely(!pte_present(pte))) {\n\t\t\t \n\t\t\tif (pte_swp_uffd_wp_any(pte) &&\n\t\t\t    !(zap_flags & ZAP_FLAG_DROP_MARKER))\n\t\t\t\tset_huge_pte_at(mm, address, ptep,\n\t\t\t\t\t\tmake_pte_marker(PTE_MARKER_UFFD_WP),\n\t\t\t\t\t\tsz);\n\t\t\telse\n\t\t\t\thuge_pte_clear(mm, address, ptep, sz);\n\t\t\tspin_unlock(ptl);\n\t\t\tcontinue;\n\t\t}\n\n\t\tpage = pte_page(pte);\n\t\t \n\t\tif (ref_page) {\n\t\t\tif (page != ref_page) {\n\t\t\t\tspin_unlock(ptl);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\t \n\t\t\tset_vma_resv_flags(vma, HPAGE_RESV_UNMAPPED);\n\t\t}\n\n\t\tpte = huge_ptep_get_and_clear(mm, address, ptep);\n\t\ttlb_remove_huge_tlb_entry(h, tlb, ptep, address);\n\t\tif (huge_pte_dirty(pte))\n\t\t\tset_page_dirty(page);\n\t\t \n\t\tif (huge_pte_uffd_wp(pte) &&\n\t\t    !(zap_flags & ZAP_FLAG_DROP_MARKER))\n\t\t\tset_huge_pte_at(mm, address, ptep,\n\t\t\t\t\tmake_pte_marker(PTE_MARKER_UFFD_WP),\n\t\t\t\t\tsz);\n\t\thugetlb_count_sub(pages_per_huge_page(h), mm);\n\t\tpage_remove_rmap(page, vma, true);\n\n\t\tspin_unlock(ptl);\n\t\ttlb_remove_page_size(tlb, page, huge_page_size(h));\n\t\t \n\t\tif (ref_page)\n\t\t\tbreak;\n\t}\n\ttlb_end_vma(tlb, vma);\n\n\t \n\tif (force_flush)\n\t\ttlb_flush_mmu_tlbonly(tlb);\n}\n\nvoid __hugetlb_zap_begin(struct vm_area_struct *vma,\n\t\t\t unsigned long *start, unsigned long *end)\n{\n\tif (!vma->vm_file)\t \n\t\treturn;\n\n\tadjust_range_if_pmd_sharing_possible(vma, start, end);\n\thugetlb_vma_lock_write(vma);\n\tif (vma->vm_file)\n\t\ti_mmap_lock_write(vma->vm_file->f_mapping);\n}\n\nvoid __hugetlb_zap_end(struct vm_area_struct *vma,\n\t\t       struct zap_details *details)\n{\n\tzap_flags_t zap_flags = details ? details->zap_flags : 0;\n\n\tif (!vma->vm_file)\t \n\t\treturn;\n\n\tif (zap_flags & ZAP_FLAG_UNMAP) {\t \n\t\t \n\t\t__hugetlb_vma_unlock_write_free(vma);\n\t} else {\n\t\thugetlb_vma_unlock_write(vma);\n\t}\n\n\tif (vma->vm_file)\n\t\ti_mmap_unlock_write(vma->vm_file->f_mapping);\n}\n\nvoid unmap_hugepage_range(struct vm_area_struct *vma, unsigned long start,\n\t\t\t  unsigned long end, struct page *ref_page,\n\t\t\t  zap_flags_t zap_flags)\n{\n\tstruct mmu_notifier_range range;\n\tstruct mmu_gather tlb;\n\n\tmmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, vma->vm_mm,\n\t\t\t\tstart, end);\n\tadjust_range_if_pmd_sharing_possible(vma, &range.start, &range.end);\n\tmmu_notifier_invalidate_range_start(&range);\n\ttlb_gather_mmu(&tlb, vma->vm_mm);\n\n\t__unmap_hugepage_range(&tlb, vma, start, end, ref_page, zap_flags);\n\n\tmmu_notifier_invalidate_range_end(&range);\n\ttlb_finish_mmu(&tlb);\n}\n\n \nstatic void unmap_ref_private(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\t      struct page *page, unsigned long address)\n{\n\tstruct hstate *h = hstate_vma(vma);\n\tstruct vm_area_struct *iter_vma;\n\tstruct address_space *mapping;\n\tpgoff_t pgoff;\n\n\t \n\taddress = address & huge_page_mask(h);\n\tpgoff = ((address - vma->vm_start) >> PAGE_SHIFT) +\n\t\t\tvma->vm_pgoff;\n\tmapping = vma->vm_file->f_mapping;\n\n\t \n\ti_mmap_lock_write(mapping);\n\tvma_interval_tree_foreach(iter_vma, &mapping->i_mmap, pgoff, pgoff) {\n\t\t \n\t\tif (iter_vma == vma)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (iter_vma->vm_flags & VM_MAYSHARE)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (!is_vma_resv_set(iter_vma, HPAGE_RESV_OWNER))\n\t\t\tunmap_hugepage_range(iter_vma, address,\n\t\t\t\t\t     address + huge_page_size(h), page, 0);\n\t}\n\ti_mmap_unlock_write(mapping);\n}\n\n \nstatic vm_fault_t hugetlb_wp(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t       unsigned long address, pte_t *ptep, unsigned int flags,\n\t\t       struct folio *pagecache_folio, spinlock_t *ptl)\n{\n\tconst bool unshare = flags & FAULT_FLAG_UNSHARE;\n\tpte_t pte = huge_ptep_get(ptep);\n\tstruct hstate *h = hstate_vma(vma);\n\tstruct folio *old_folio;\n\tstruct folio *new_folio;\n\tint outside_reserve = 0;\n\tvm_fault_t ret = 0;\n\tunsigned long haddr = address & huge_page_mask(h);\n\tstruct mmu_notifier_range range;\n\n\t \n\tif (!unshare && huge_pte_uffd_wp(pte))\n\t\treturn 0;\n\n\t \n\tif (WARN_ON_ONCE(!unshare && !(vma->vm_flags & VM_WRITE)))\n\t\treturn VM_FAULT_SIGSEGV;\n\n\t \n\tif (vma->vm_flags & VM_MAYSHARE) {\n\t\tset_huge_ptep_writable(vma, haddr, ptep);\n\t\treturn 0;\n\t}\n\n\told_folio = page_folio(pte_page(pte));\n\n\tdelayacct_wpcopy_start();\n\nretry_avoidcopy:\n\t \n\tif (folio_mapcount(old_folio) == 1 && folio_test_anon(old_folio)) {\n\t\tif (!PageAnonExclusive(&old_folio->page))\n\t\t\tpage_move_anon_rmap(&old_folio->page, vma);\n\t\tif (likely(!unshare))\n\t\t\tset_huge_ptep_writable(vma, haddr, ptep);\n\n\t\tdelayacct_wpcopy_end();\n\t\treturn 0;\n\t}\n\tVM_BUG_ON_PAGE(folio_test_anon(old_folio) &&\n\t\t       PageAnonExclusive(&old_folio->page), &old_folio->page);\n\n\t \n\tif (is_vma_resv_set(vma, HPAGE_RESV_OWNER) &&\n\t\t\told_folio != pagecache_folio)\n\t\toutside_reserve = 1;\n\n\tfolio_get(old_folio);\n\n\t \n\tspin_unlock(ptl);\n\tnew_folio = alloc_hugetlb_folio(vma, haddr, outside_reserve);\n\n\tif (IS_ERR(new_folio)) {\n\t\t \n\t\tif (outside_reserve) {\n\t\t\tstruct address_space *mapping = vma->vm_file->f_mapping;\n\t\t\tpgoff_t idx;\n\t\t\tu32 hash;\n\n\t\t\tfolio_put(old_folio);\n\t\t\t \n\t\t\tidx = vma_hugecache_offset(h, vma, haddr);\n\t\t\thash = hugetlb_fault_mutex_hash(mapping, idx);\n\t\t\thugetlb_vma_unlock_read(vma);\n\t\t\tmutex_unlock(&hugetlb_fault_mutex_table[hash]);\n\n\t\t\tunmap_ref_private(mm, vma, &old_folio->page, haddr);\n\n\t\t\tmutex_lock(&hugetlb_fault_mutex_table[hash]);\n\t\t\thugetlb_vma_lock_read(vma);\n\t\t\tspin_lock(ptl);\n\t\t\tptep = hugetlb_walk(vma, haddr, huge_page_size(h));\n\t\t\tif (likely(ptep &&\n\t\t\t\t   pte_same(huge_ptep_get(ptep), pte)))\n\t\t\t\tgoto retry_avoidcopy;\n\t\t\t \n\t\t\tdelayacct_wpcopy_end();\n\t\t\treturn 0;\n\t\t}\n\n\t\tret = vmf_error(PTR_ERR(new_folio));\n\t\tgoto out_release_old;\n\t}\n\n\t \n\tif (unlikely(anon_vma_prepare(vma))) {\n\t\tret = VM_FAULT_OOM;\n\t\tgoto out_release_all;\n\t}\n\n\tif (copy_user_large_folio(new_folio, old_folio, address, vma)) {\n\t\tret = VM_FAULT_HWPOISON_LARGE;\n\t\tgoto out_release_all;\n\t}\n\t__folio_mark_uptodate(new_folio);\n\n\tmmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, mm, haddr,\n\t\t\t\thaddr + huge_page_size(h));\n\tmmu_notifier_invalidate_range_start(&range);\n\n\t \n\tspin_lock(ptl);\n\tptep = hugetlb_walk(vma, haddr, huge_page_size(h));\n\tif (likely(ptep && pte_same(huge_ptep_get(ptep), pte))) {\n\t\tpte_t newpte = make_huge_pte(vma, &new_folio->page, !unshare);\n\n\t\t \n\t\thuge_ptep_clear_flush(vma, haddr, ptep);\n\t\tpage_remove_rmap(&old_folio->page, vma, true);\n\t\thugepage_add_new_anon_rmap(new_folio, vma, haddr);\n\t\tif (huge_pte_uffd_wp(pte))\n\t\t\tnewpte = huge_pte_mkuffd_wp(newpte);\n\t\tset_huge_pte_at(mm, haddr, ptep, newpte, huge_page_size(h));\n\t\tfolio_set_hugetlb_migratable(new_folio);\n\t\t \n\t\tnew_folio = old_folio;\n\t}\n\tspin_unlock(ptl);\n\tmmu_notifier_invalidate_range_end(&range);\nout_release_all:\n\t \n\tif (new_folio != old_folio)\n\t\trestore_reserve_on_error(h, vma, haddr, new_folio);\n\tfolio_put(new_folio);\nout_release_old:\n\tfolio_put(old_folio);\n\n\tspin_lock(ptl);  \n\n\tdelayacct_wpcopy_end();\n\treturn ret;\n}\n\n \nstatic bool hugetlbfs_pagecache_present(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long address)\n{\n\tstruct address_space *mapping = vma->vm_file->f_mapping;\n\tpgoff_t idx = vma_hugecache_offset(h, vma, address);\n\tstruct folio *folio;\n\n\tfolio = filemap_get_folio(mapping, idx);\n\tif (IS_ERR(folio))\n\t\treturn false;\n\tfolio_put(folio);\n\treturn true;\n}\n\nint hugetlb_add_to_page_cache(struct folio *folio, struct address_space *mapping,\n\t\t\t   pgoff_t idx)\n{\n\tstruct inode *inode = mapping->host;\n\tstruct hstate *h = hstate_inode(inode);\n\tint err;\n\n\t__folio_set_locked(folio);\n\terr = __filemap_add_folio(mapping, folio, idx, GFP_KERNEL, NULL);\n\n\tif (unlikely(err)) {\n\t\t__folio_clear_locked(folio);\n\t\treturn err;\n\t}\n\tfolio_clear_hugetlb_restore_reserve(folio);\n\n\t \n\tfolio_mark_dirty(folio);\n\n\tspin_lock(&inode->i_lock);\n\tinode->i_blocks += blocks_per_huge_page(h);\n\tspin_unlock(&inode->i_lock);\n\treturn 0;\n}\n\nstatic inline vm_fault_t hugetlb_handle_userfault(struct vm_area_struct *vma,\n\t\t\t\t\t\t  struct address_space *mapping,\n\t\t\t\t\t\t  pgoff_t idx,\n\t\t\t\t\t\t  unsigned int flags,\n\t\t\t\t\t\t  unsigned long haddr,\n\t\t\t\t\t\t  unsigned long addr,\n\t\t\t\t\t\t  unsigned long reason)\n{\n\tu32 hash;\n\tstruct vm_fault vmf = {\n\t\t.vma = vma,\n\t\t.address = haddr,\n\t\t.real_address = addr,\n\t\t.flags = flags,\n\n\t\t \n\t};\n\n\t \n\thugetlb_vma_unlock_read(vma);\n\thash = hugetlb_fault_mutex_hash(mapping, idx);\n\tmutex_unlock(&hugetlb_fault_mutex_table[hash]);\n\treturn handle_userfault(&vmf, reason);\n}\n\n \nstatic bool hugetlb_pte_stable(struct hstate *h, struct mm_struct *mm,\n\t\t\t       pte_t *ptep, pte_t old_pte)\n{\n\tspinlock_t *ptl;\n\tbool same;\n\n\tptl = huge_pte_lock(h, mm, ptep);\n\tsame = pte_same(huge_ptep_get(ptep), old_pte);\n\tspin_unlock(ptl);\n\n\treturn same;\n}\n\nstatic vm_fault_t hugetlb_no_page(struct mm_struct *mm,\n\t\t\tstruct vm_area_struct *vma,\n\t\t\tstruct address_space *mapping, pgoff_t idx,\n\t\t\tunsigned long address, pte_t *ptep,\n\t\t\tpte_t old_pte, unsigned int flags)\n{\n\tstruct hstate *h = hstate_vma(vma);\n\tvm_fault_t ret = VM_FAULT_SIGBUS;\n\tint anon_rmap = 0;\n\tunsigned long size;\n\tstruct folio *folio;\n\tpte_t new_pte;\n\tspinlock_t *ptl;\n\tunsigned long haddr = address & huge_page_mask(h);\n\tbool new_folio, new_pagecache_folio = false;\n\tu32 hash = hugetlb_fault_mutex_hash(mapping, idx);\n\n\t \n\tif (is_vma_resv_set(vma, HPAGE_RESV_UNMAPPED)) {\n\t\tpr_warn_ratelimited(\"PID %d killed due to inadequate hugepage pool\\n\",\n\t\t\t   current->pid);\n\t\tgoto out;\n\t}\n\n\t \n\tnew_folio = false;\n\tfolio = filemap_lock_folio(mapping, idx);\n\tif (IS_ERR(folio)) {\n\t\tsize = i_size_read(mapping->host) >> huge_page_shift(h);\n\t\tif (idx >= size)\n\t\t\tgoto out;\n\t\t \n\t\tif (userfaultfd_missing(vma)) {\n\t\t\t \n\t\t\tif (!hugetlb_pte_stable(h, mm, ptep, old_pte)) {\n\t\t\t\tret = 0;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\treturn hugetlb_handle_userfault(vma, mapping, idx, flags,\n\t\t\t\t\t\t\thaddr, address,\n\t\t\t\t\t\t\tVM_UFFD_MISSING);\n\t\t}\n\n\t\tfolio = alloc_hugetlb_folio(vma, haddr, 0);\n\t\tif (IS_ERR(folio)) {\n\t\t\t \n\t\t\tif (hugetlb_pte_stable(h, mm, ptep, old_pte))\n\t\t\t\tret = vmf_error(PTR_ERR(folio));\n\t\t\telse\n\t\t\t\tret = 0;\n\t\t\tgoto out;\n\t\t}\n\t\tclear_huge_page(&folio->page, address, pages_per_huge_page(h));\n\t\t__folio_mark_uptodate(folio);\n\t\tnew_folio = true;\n\n\t\tif (vma->vm_flags & VM_MAYSHARE) {\n\t\t\tint err = hugetlb_add_to_page_cache(folio, mapping, idx);\n\t\t\tif (err) {\n\t\t\t\t \n\t\t\t\trestore_reserve_on_error(h, vma, haddr, folio);\n\t\t\t\tfolio_put(folio);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tnew_pagecache_folio = true;\n\t\t} else {\n\t\t\tfolio_lock(folio);\n\t\t\tif (unlikely(anon_vma_prepare(vma))) {\n\t\t\t\tret = VM_FAULT_OOM;\n\t\t\t\tgoto backout_unlocked;\n\t\t\t}\n\t\t\tanon_rmap = 1;\n\t\t}\n\t} else {\n\t\t \n\t\tif (unlikely(folio_test_hwpoison(folio))) {\n\t\t\tret = VM_FAULT_HWPOISON_LARGE |\n\t\t\t\tVM_FAULT_SET_HINDEX(hstate_index(h));\n\t\t\tgoto backout_unlocked;\n\t\t}\n\n\t\t \n\t\tif (userfaultfd_minor(vma)) {\n\t\t\tfolio_unlock(folio);\n\t\t\tfolio_put(folio);\n\t\t\t \n\t\t\tif (!hugetlb_pte_stable(h, mm, ptep, old_pte)) {\n\t\t\t\tret = 0;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\treturn hugetlb_handle_userfault(vma, mapping, idx, flags,\n\t\t\t\t\t\t\thaddr, address,\n\t\t\t\t\t\t\tVM_UFFD_MINOR);\n\t\t}\n\t}\n\n\t \n\tif ((flags & FAULT_FLAG_WRITE) && !(vma->vm_flags & VM_SHARED)) {\n\t\tif (vma_needs_reservation(h, vma, haddr) < 0) {\n\t\t\tret = VM_FAULT_OOM;\n\t\t\tgoto backout_unlocked;\n\t\t}\n\t\t \n\t\tvma_end_reservation(h, vma, haddr);\n\t}\n\n\tptl = huge_pte_lock(h, mm, ptep);\n\tret = 0;\n\t \n\tif (!pte_same(huge_ptep_get(ptep), old_pte))\n\t\tgoto backout;\n\n\tif (anon_rmap)\n\t\thugepage_add_new_anon_rmap(folio, vma, haddr);\n\telse\n\t\tpage_dup_file_rmap(&folio->page, true);\n\tnew_pte = make_huge_pte(vma, &folio->page, ((vma->vm_flags & VM_WRITE)\n\t\t\t\t&& (vma->vm_flags & VM_SHARED)));\n\t \n\tif (unlikely(pte_marker_uffd_wp(old_pte)))\n\t\tnew_pte = huge_pte_mkuffd_wp(new_pte);\n\tset_huge_pte_at(mm, haddr, ptep, new_pte, huge_page_size(h));\n\n\thugetlb_count_add(pages_per_huge_page(h), mm);\n\tif ((flags & FAULT_FLAG_WRITE) && !(vma->vm_flags & VM_SHARED)) {\n\t\t \n\t\tret = hugetlb_wp(mm, vma, address, ptep, flags, folio, ptl);\n\t}\n\n\tspin_unlock(ptl);\n\n\t \n\tif (new_folio)\n\t\tfolio_set_hugetlb_migratable(folio);\n\n\tfolio_unlock(folio);\nout:\n\thugetlb_vma_unlock_read(vma);\n\tmutex_unlock(&hugetlb_fault_mutex_table[hash]);\n\treturn ret;\n\nbackout:\n\tspin_unlock(ptl);\nbackout_unlocked:\n\tif (new_folio && !new_pagecache_folio)\n\t\trestore_reserve_on_error(h, vma, haddr, folio);\n\n\tfolio_unlock(folio);\n\tfolio_put(folio);\n\tgoto out;\n}\n\n#ifdef CONFIG_SMP\nu32 hugetlb_fault_mutex_hash(struct address_space *mapping, pgoff_t idx)\n{\n\tunsigned long key[2];\n\tu32 hash;\n\n\tkey[0] = (unsigned long) mapping;\n\tkey[1] = idx;\n\n\thash = jhash2((u32 *)&key, sizeof(key)/(sizeof(u32)), 0);\n\n\treturn hash & (num_fault_mutexes - 1);\n}\n#else\n \nu32 hugetlb_fault_mutex_hash(struct address_space *mapping, pgoff_t idx)\n{\n\treturn 0;\n}\n#endif\n\nvm_fault_t hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\tunsigned long address, unsigned int flags)\n{\n\tpte_t *ptep, entry;\n\tspinlock_t *ptl;\n\tvm_fault_t ret;\n\tu32 hash;\n\tpgoff_t idx;\n\tstruct folio *folio = NULL;\n\tstruct folio *pagecache_folio = NULL;\n\tstruct hstate *h = hstate_vma(vma);\n\tstruct address_space *mapping;\n\tint need_wait_lock = 0;\n\tunsigned long haddr = address & huge_page_mask(h);\n\n\t \n\tif (flags & FAULT_FLAG_VMA_LOCK) {\n\t\tvma_end_read(vma);\n\t\treturn VM_FAULT_RETRY;\n\t}\n\n\t \n\tmapping = vma->vm_file->f_mapping;\n\tidx = vma_hugecache_offset(h, vma, haddr);\n\thash = hugetlb_fault_mutex_hash(mapping, idx);\n\tmutex_lock(&hugetlb_fault_mutex_table[hash]);\n\n\t \n\thugetlb_vma_lock_read(vma);\n\tptep = huge_pte_alloc(mm, vma, haddr, huge_page_size(h));\n\tif (!ptep) {\n\t\thugetlb_vma_unlock_read(vma);\n\t\tmutex_unlock(&hugetlb_fault_mutex_table[hash]);\n\t\treturn VM_FAULT_OOM;\n\t}\n\n\tentry = huge_ptep_get(ptep);\n\tif (huge_pte_none_mostly(entry)) {\n\t\tif (is_pte_marker(entry)) {\n\t\t\tpte_marker marker =\n\t\t\t\tpte_marker_get(pte_to_swp_entry(entry));\n\n\t\t\tif (marker & PTE_MARKER_POISONED) {\n\t\t\t\tret = VM_FAULT_HWPOISON_LARGE;\n\t\t\t\tgoto out_mutex;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\treturn hugetlb_no_page(mm, vma, mapping, idx, address, ptep,\n\t\t\t\t      entry, flags);\n\t}\n\n\tret = 0;\n\n\t \n\tif (!pte_present(entry)) {\n\t\tif (unlikely(is_hugetlb_entry_migration(entry))) {\n\t\t\t \n\t\t\tmutex_unlock(&hugetlb_fault_mutex_table[hash]);\n\t\t\tmigration_entry_wait_huge(vma, ptep);\n\t\t\treturn 0;\n\t\t} else if (unlikely(is_hugetlb_entry_hwpoisoned(entry)))\n\t\t\tret = VM_FAULT_HWPOISON_LARGE |\n\t\t\t    VM_FAULT_SET_HINDEX(hstate_index(h));\n\t\tgoto out_mutex;\n\t}\n\n\t \n\tif ((flags & (FAULT_FLAG_WRITE|FAULT_FLAG_UNSHARE)) &&\n\t    !(vma->vm_flags & VM_MAYSHARE) && !huge_pte_write(entry)) {\n\t\tif (vma_needs_reservation(h, vma, haddr) < 0) {\n\t\t\tret = VM_FAULT_OOM;\n\t\t\tgoto out_mutex;\n\t\t}\n\t\t \n\t\tvma_end_reservation(h, vma, haddr);\n\n\t\tpagecache_folio = filemap_lock_folio(mapping, idx);\n\t\tif (IS_ERR(pagecache_folio))\n\t\t\tpagecache_folio = NULL;\n\t}\n\n\tptl = huge_pte_lock(h, mm, ptep);\n\n\t \n\tif (unlikely(!pte_same(entry, huge_ptep_get(ptep))))\n\t\tgoto out_ptl;\n\n\t \n\tif (userfaultfd_wp(vma) && huge_pte_uffd_wp(huge_ptep_get(ptep)) &&\n\t    (flags & FAULT_FLAG_WRITE) && !huge_pte_write(entry)) {\n\t\tstruct vm_fault vmf = {\n\t\t\t.vma = vma,\n\t\t\t.address = haddr,\n\t\t\t.real_address = address,\n\t\t\t.flags = flags,\n\t\t};\n\n\t\tspin_unlock(ptl);\n\t\tif (pagecache_folio) {\n\t\t\tfolio_unlock(pagecache_folio);\n\t\t\tfolio_put(pagecache_folio);\n\t\t}\n\t\thugetlb_vma_unlock_read(vma);\n\t\tmutex_unlock(&hugetlb_fault_mutex_table[hash]);\n\t\treturn handle_userfault(&vmf, VM_UFFD_WP);\n\t}\n\n\t \n\tfolio = page_folio(pte_page(entry));\n\tif (folio != pagecache_folio)\n\t\tif (!folio_trylock(folio)) {\n\t\t\tneed_wait_lock = 1;\n\t\t\tgoto out_ptl;\n\t\t}\n\n\tfolio_get(folio);\n\n\tif (flags & (FAULT_FLAG_WRITE|FAULT_FLAG_UNSHARE)) {\n\t\tif (!huge_pte_write(entry)) {\n\t\t\tret = hugetlb_wp(mm, vma, address, ptep, flags,\n\t\t\t\t\t pagecache_folio, ptl);\n\t\t\tgoto out_put_page;\n\t\t} else if (likely(flags & FAULT_FLAG_WRITE)) {\n\t\t\tentry = huge_pte_mkdirty(entry);\n\t\t}\n\t}\n\tentry = pte_mkyoung(entry);\n\tif (huge_ptep_set_access_flags(vma, haddr, ptep, entry,\n\t\t\t\t\t\tflags & FAULT_FLAG_WRITE))\n\t\tupdate_mmu_cache(vma, haddr, ptep);\nout_put_page:\n\tif (folio != pagecache_folio)\n\t\tfolio_unlock(folio);\n\tfolio_put(folio);\nout_ptl:\n\tspin_unlock(ptl);\n\n\tif (pagecache_folio) {\n\t\tfolio_unlock(pagecache_folio);\n\t\tfolio_put(pagecache_folio);\n\t}\nout_mutex:\n\thugetlb_vma_unlock_read(vma);\n\tmutex_unlock(&hugetlb_fault_mutex_table[hash]);\n\t \n\tif (need_wait_lock)\n\t\tfolio_wait_locked(folio);\n\treturn ret;\n}\n\n#ifdef CONFIG_USERFAULTFD\n \nint hugetlb_mfill_atomic_pte(pte_t *dst_pte,\n\t\t\t     struct vm_area_struct *dst_vma,\n\t\t\t     unsigned long dst_addr,\n\t\t\t     unsigned long src_addr,\n\t\t\t     uffd_flags_t flags,\n\t\t\t     struct folio **foliop)\n{\n\tstruct mm_struct *dst_mm = dst_vma->vm_mm;\n\tbool is_continue = uffd_flags_mode_is(flags, MFILL_ATOMIC_CONTINUE);\n\tbool wp_enabled = (flags & MFILL_ATOMIC_WP);\n\tstruct hstate *h = hstate_vma(dst_vma);\n\tstruct address_space *mapping = dst_vma->vm_file->f_mapping;\n\tpgoff_t idx = vma_hugecache_offset(h, dst_vma, dst_addr);\n\tunsigned long size;\n\tint vm_shared = dst_vma->vm_flags & VM_SHARED;\n\tpte_t _dst_pte;\n\tspinlock_t *ptl;\n\tint ret = -ENOMEM;\n\tstruct folio *folio;\n\tint writable;\n\tbool folio_in_pagecache = false;\n\n\tif (uffd_flags_mode_is(flags, MFILL_ATOMIC_POISON)) {\n\t\tptl = huge_pte_lock(h, dst_mm, dst_pte);\n\n\t\t \n\t\tif (!huge_pte_none(huge_ptep_get(dst_pte))) {\n\t\t\tspin_unlock(ptl);\n\t\t\treturn -EEXIST;\n\t\t}\n\n\t\t_dst_pte = make_pte_marker(PTE_MARKER_POISONED);\n\t\tset_huge_pte_at(dst_mm, dst_addr, dst_pte, _dst_pte,\n\t\t\t\thuge_page_size(h));\n\n\t\t \n\t\tupdate_mmu_cache(dst_vma, dst_addr, dst_pte);\n\n\t\tspin_unlock(ptl);\n\t\treturn 0;\n\t}\n\n\tif (is_continue) {\n\t\tret = -EFAULT;\n\t\tfolio = filemap_lock_folio(mapping, idx);\n\t\tif (IS_ERR(folio))\n\t\t\tgoto out;\n\t\tfolio_in_pagecache = true;\n\t} else if (!*foliop) {\n\t\t \n\t\tif (vm_shared &&\n\t\t    hugetlbfs_pagecache_present(h, dst_vma, dst_addr)) {\n\t\t\tret = -EEXIST;\n\t\t\tgoto out;\n\t\t}\n\n\t\tfolio = alloc_hugetlb_folio(dst_vma, dst_addr, 0);\n\t\tif (IS_ERR(folio)) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\n\t\tret = copy_folio_from_user(folio, (const void __user *) src_addr,\n\t\t\t\t\t   false);\n\n\t\t \n\t\tif (unlikely(ret)) {\n\t\t\tret = -ENOENT;\n\t\t\t \n\t\t\trestore_reserve_on_error(h, dst_vma, dst_addr, folio);\n\t\t\tfolio_put(folio);\n\n\t\t\t \n\t\t\tfolio = alloc_hugetlb_folio_vma(h, dst_vma, dst_addr);\n\t\t\tif (!folio) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\t*foliop = folio;\n\t\t\t \n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\tif (vm_shared &&\n\t\t    hugetlbfs_pagecache_present(h, dst_vma, dst_addr)) {\n\t\t\tfolio_put(*foliop);\n\t\t\tret = -EEXIST;\n\t\t\t*foliop = NULL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tfolio = alloc_hugetlb_folio(dst_vma, dst_addr, 0);\n\t\tif (IS_ERR(folio)) {\n\t\t\tfolio_put(*foliop);\n\t\t\tret = -ENOMEM;\n\t\t\t*foliop = NULL;\n\t\t\tgoto out;\n\t\t}\n\t\tret = copy_user_large_folio(folio, *foliop, dst_addr, dst_vma);\n\t\tfolio_put(*foliop);\n\t\t*foliop = NULL;\n\t\tif (ret) {\n\t\t\tfolio_put(folio);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t \n\t__folio_mark_uptodate(folio);\n\n\t \n\tif (vm_shared && !is_continue) {\n\t\tsize = i_size_read(mapping->host) >> huge_page_shift(h);\n\t\tret = -EFAULT;\n\t\tif (idx >= size)\n\t\t\tgoto out_release_nounlock;\n\n\t\t \n\t\tret = hugetlb_add_to_page_cache(folio, mapping, idx);\n\t\tif (ret)\n\t\t\tgoto out_release_nounlock;\n\t\tfolio_in_pagecache = true;\n\t}\n\n\tptl = huge_pte_lock(h, dst_mm, dst_pte);\n\n\tret = -EIO;\n\tif (folio_test_hwpoison(folio))\n\t\tgoto out_release_unlock;\n\n\t \n\tret = -EEXIST;\n\tif (!huge_pte_none_mostly(huge_ptep_get(dst_pte)))\n\t\tgoto out_release_unlock;\n\n\tif (folio_in_pagecache)\n\t\tpage_dup_file_rmap(&folio->page, true);\n\telse\n\t\thugepage_add_new_anon_rmap(folio, dst_vma, dst_addr);\n\n\t \n\tif (wp_enabled || (is_continue && !vm_shared))\n\t\twritable = 0;\n\telse\n\t\twritable = dst_vma->vm_flags & VM_WRITE;\n\n\t_dst_pte = make_huge_pte(dst_vma, &folio->page, writable);\n\t \n\t_dst_pte = huge_pte_mkdirty(_dst_pte);\n\t_dst_pte = pte_mkyoung(_dst_pte);\n\n\tif (wp_enabled)\n\t\t_dst_pte = huge_pte_mkuffd_wp(_dst_pte);\n\n\tset_huge_pte_at(dst_mm, dst_addr, dst_pte, _dst_pte, huge_page_size(h));\n\n\thugetlb_count_add(pages_per_huge_page(h), dst_mm);\n\n\t \n\tupdate_mmu_cache(dst_vma, dst_addr, dst_pte);\n\n\tspin_unlock(ptl);\n\tif (!is_continue)\n\t\tfolio_set_hugetlb_migratable(folio);\n\tif (vm_shared || is_continue)\n\t\tfolio_unlock(folio);\n\tret = 0;\nout:\n\treturn ret;\nout_release_unlock:\n\tspin_unlock(ptl);\n\tif (vm_shared || is_continue)\n\t\tfolio_unlock(folio);\nout_release_nounlock:\n\tif (!folio_in_pagecache)\n\t\trestore_reserve_on_error(h, dst_vma, dst_addr, folio);\n\tfolio_put(folio);\n\tgoto out;\n}\n#endif  \n\nstruct page *hugetlb_follow_page_mask(struct vm_area_struct *vma,\n\t\t\t\t      unsigned long address, unsigned int flags,\n\t\t\t\t      unsigned int *page_mask)\n{\n\tstruct hstate *h = hstate_vma(vma);\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long haddr = address & huge_page_mask(h);\n\tstruct page *page = NULL;\n\tspinlock_t *ptl;\n\tpte_t *pte, entry;\n\tint ret;\n\n\thugetlb_vma_lock_read(vma);\n\tpte = hugetlb_walk(vma, haddr, huge_page_size(h));\n\tif (!pte)\n\t\tgoto out_unlock;\n\n\tptl = huge_pte_lock(h, mm, pte);\n\tentry = huge_ptep_get(pte);\n\tif (pte_present(entry)) {\n\t\tpage = pte_page(entry);\n\n\t\tif (!huge_pte_write(entry)) {\n\t\t\tif (flags & FOLL_WRITE) {\n\t\t\t\tpage = NULL;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tif (gup_must_unshare(vma, flags, page)) {\n\t\t\t\t \n\t\t\t\tpage = ERR_PTR(-EMLINK);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\tpage = nth_page(page, ((address & ~huge_page_mask(h)) >> PAGE_SHIFT));\n\n\t\t \n\t\tret = try_grab_page(page, flags);\n\n\t\tif (WARN_ON_ONCE(ret)) {\n\t\t\tpage = ERR_PTR(ret);\n\t\t\tgoto out;\n\t\t}\n\n\t\t*page_mask = (1U << huge_page_order(h)) - 1;\n\t}\nout:\n\tspin_unlock(ptl);\nout_unlock:\n\thugetlb_vma_unlock_read(vma);\n\n\t \n\tif (!page && (flags & FOLL_DUMP) &&\n\t    !hugetlbfs_pagecache_present(h, vma, address))\n\t\tpage = ERR_PTR(-EFAULT);\n\n\treturn page;\n}\n\nlong hugetlb_change_protection(struct vm_area_struct *vma,\n\t\tunsigned long address, unsigned long end,\n\t\tpgprot_t newprot, unsigned long cp_flags)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long start = address;\n\tpte_t *ptep;\n\tpte_t pte;\n\tstruct hstate *h = hstate_vma(vma);\n\tlong pages = 0, psize = huge_page_size(h);\n\tbool shared_pmd = false;\n\tstruct mmu_notifier_range range;\n\tunsigned long last_addr_mask;\n\tbool uffd_wp = cp_flags & MM_CP_UFFD_WP;\n\tbool uffd_wp_resolve = cp_flags & MM_CP_UFFD_WP_RESOLVE;\n\n\t \n\tmmu_notifier_range_init(&range, MMU_NOTIFY_PROTECTION_VMA,\n\t\t\t\t0, mm, start, end);\n\tadjust_range_if_pmd_sharing_possible(vma, &range.start, &range.end);\n\n\tBUG_ON(address >= end);\n\tflush_cache_range(vma, range.start, range.end);\n\n\tmmu_notifier_invalidate_range_start(&range);\n\thugetlb_vma_lock_write(vma);\n\ti_mmap_lock_write(vma->vm_file->f_mapping);\n\tlast_addr_mask = hugetlb_mask_last_page(h);\n\tfor (; address < end; address += psize) {\n\t\tspinlock_t *ptl;\n\t\tptep = hugetlb_walk(vma, address, psize);\n\t\tif (!ptep) {\n\t\t\tif (!uffd_wp) {\n\t\t\t\taddress |= last_addr_mask;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\t \n\t\t\tptep = huge_pte_alloc(mm, vma, address, psize);\n\t\t\tif (!ptep) {\n\t\t\t\tpages = -ENOMEM;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tptl = huge_pte_lock(h, mm, ptep);\n\t\tif (huge_pmd_unshare(mm, vma, address, ptep)) {\n\t\t\t \n\t\t\tWARN_ON_ONCE(uffd_wp || uffd_wp_resolve);\n\t\t\tpages++;\n\t\t\tspin_unlock(ptl);\n\t\t\tshared_pmd = true;\n\t\t\taddress |= last_addr_mask;\n\t\t\tcontinue;\n\t\t}\n\t\tpte = huge_ptep_get(ptep);\n\t\tif (unlikely(is_hugetlb_entry_hwpoisoned(pte))) {\n\t\t\t \n\t\t} else if (unlikely(is_hugetlb_entry_migration(pte))) {\n\t\t\tswp_entry_t entry = pte_to_swp_entry(pte);\n\t\t\tstruct page *page = pfn_swap_entry_to_page(entry);\n\t\t\tpte_t newpte = pte;\n\n\t\t\tif (is_writable_migration_entry(entry)) {\n\t\t\t\tif (PageAnon(page))\n\t\t\t\t\tentry = make_readable_exclusive_migration_entry(\n\t\t\t\t\t\t\t\tswp_offset(entry));\n\t\t\t\telse\n\t\t\t\t\tentry = make_readable_migration_entry(\n\t\t\t\t\t\t\t\tswp_offset(entry));\n\t\t\t\tnewpte = swp_entry_to_pte(entry);\n\t\t\t\tpages++;\n\t\t\t}\n\n\t\t\tif (uffd_wp)\n\t\t\t\tnewpte = pte_swp_mkuffd_wp(newpte);\n\t\t\telse if (uffd_wp_resolve)\n\t\t\t\tnewpte = pte_swp_clear_uffd_wp(newpte);\n\t\t\tif (!pte_same(pte, newpte))\n\t\t\t\tset_huge_pte_at(mm, address, ptep, newpte, psize);\n\t\t} else if (unlikely(is_pte_marker(pte))) {\n\t\t\t \n\t\t\tWARN_ON_ONCE(!pte_marker_uffd_wp(pte));\n\t\t\tif (uffd_wp_resolve)\n\t\t\t\t \n\t\t\t\thuge_pte_clear(mm, address, ptep, psize);\n\t\t} else if (!huge_pte_none(pte)) {\n\t\t\tpte_t old_pte;\n\t\t\tunsigned int shift = huge_page_shift(hstate_vma(vma));\n\n\t\t\told_pte = huge_ptep_modify_prot_start(vma, address, ptep);\n\t\t\tpte = huge_pte_modify(old_pte, newprot);\n\t\t\tpte = arch_make_huge_pte(pte, shift, vma->vm_flags);\n\t\t\tif (uffd_wp)\n\t\t\t\tpte = huge_pte_mkuffd_wp(pte);\n\t\t\telse if (uffd_wp_resolve)\n\t\t\t\tpte = huge_pte_clear_uffd_wp(pte);\n\t\t\thuge_ptep_modify_prot_commit(vma, address, ptep, old_pte, pte);\n\t\t\tpages++;\n\t\t} else {\n\t\t\t \n\t\t\tif (unlikely(uffd_wp))\n\t\t\t\t \n\t\t\t\tset_huge_pte_at(mm, address, ptep,\n\t\t\t\t\t\tmake_pte_marker(PTE_MARKER_UFFD_WP),\n\t\t\t\t\t\tpsize);\n\t\t}\n\t\tspin_unlock(ptl);\n\t}\n\t \n\tif (shared_pmd)\n\t\tflush_hugetlb_tlb_range(vma, range.start, range.end);\n\telse\n\t\tflush_hugetlb_tlb_range(vma, start, end);\n\t \n\ti_mmap_unlock_write(vma->vm_file->f_mapping);\n\thugetlb_vma_unlock_write(vma);\n\tmmu_notifier_invalidate_range_end(&range);\n\n\treturn pages > 0 ? (pages << h->order) : pages;\n}\n\n \nbool hugetlb_reserve_pages(struct inode *inode,\n\t\t\t\t\tlong from, long to,\n\t\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\t\tvm_flags_t vm_flags)\n{\n\tlong chg = -1, add = -1;\n\tstruct hstate *h = hstate_inode(inode);\n\tstruct hugepage_subpool *spool = subpool_inode(inode);\n\tstruct resv_map *resv_map;\n\tstruct hugetlb_cgroup *h_cg = NULL;\n\tlong gbl_reserve, regions_needed = 0;\n\n\t \n\tif (from > to) {\n\t\tVM_WARN(1, \"%s called with a negative range\\n\", __func__);\n\t\treturn false;\n\t}\n\n\t \n\thugetlb_vma_lock_alloc(vma);\n\n\t \n\tif (vm_flags & VM_NORESERVE)\n\t\treturn true;\n\n\t \n\tif (!vma || vma->vm_flags & VM_MAYSHARE) {\n\t\t \n\t\tresv_map = inode_resv_map(inode);\n\n\t\tchg = region_chg(resv_map, from, to, &regions_needed);\n\t} else {\n\t\t \n\t\tresv_map = resv_map_alloc();\n\t\tif (!resv_map)\n\t\t\tgoto out_err;\n\n\t\tchg = to - from;\n\n\t\tset_vma_resv_map(vma, resv_map);\n\t\tset_vma_resv_flags(vma, HPAGE_RESV_OWNER);\n\t}\n\n\tif (chg < 0)\n\t\tgoto out_err;\n\n\tif (hugetlb_cgroup_charge_cgroup_rsvd(hstate_index(h),\n\t\t\t\tchg * pages_per_huge_page(h), &h_cg) < 0)\n\t\tgoto out_err;\n\n\tif (vma && !(vma->vm_flags & VM_MAYSHARE) && h_cg) {\n\t\t \n\t\tresv_map_set_hugetlb_cgroup_uncharge_info(resv_map, h_cg, h);\n\t}\n\n\t \n\tgbl_reserve = hugepage_subpool_get_pages(spool, chg);\n\tif (gbl_reserve < 0)\n\t\tgoto out_uncharge_cgroup;\n\n\t \n\tif (hugetlb_acct_memory(h, gbl_reserve) < 0)\n\t\tgoto out_put_pages;\n\n\t \n\tif (!vma || vma->vm_flags & VM_MAYSHARE) {\n\t\tadd = region_add(resv_map, from, to, regions_needed, h, h_cg);\n\n\t\tif (unlikely(add < 0)) {\n\t\t\thugetlb_acct_memory(h, -gbl_reserve);\n\t\t\tgoto out_put_pages;\n\t\t} else if (unlikely(chg > add)) {\n\t\t\t \n\t\t\tlong rsv_adjust;\n\n\t\t\t \n\t\t\thugetlb_cgroup_uncharge_cgroup_rsvd(\n\t\t\t\thstate_index(h),\n\t\t\t\t(chg - add) * pages_per_huge_page(h), h_cg);\n\n\t\t\trsv_adjust = hugepage_subpool_put_pages(spool,\n\t\t\t\t\t\t\t\tchg - add);\n\t\t\thugetlb_acct_memory(h, -rsv_adjust);\n\t\t} else if (h_cg) {\n\t\t\t \n\t\t\thugetlb_cgroup_put_rsvd_cgroup(h_cg);\n\t\t}\n\t}\n\treturn true;\n\nout_put_pages:\n\t \n\t(void)hugepage_subpool_put_pages(spool, chg);\nout_uncharge_cgroup:\n\thugetlb_cgroup_uncharge_cgroup_rsvd(hstate_index(h),\n\t\t\t\t\t    chg * pages_per_huge_page(h), h_cg);\nout_err:\n\thugetlb_vma_lock_free(vma);\n\tif (!vma || vma->vm_flags & VM_MAYSHARE)\n\t\t \n\t\tif (chg >= 0 && add < 0)\n\t\t\tregion_abort(resv_map, from, to, regions_needed);\n\tif (vma && is_vma_resv_set(vma, HPAGE_RESV_OWNER)) {\n\t\tkref_put(&resv_map->refs, resv_map_release);\n\t\tset_vma_resv_map(vma, NULL);\n\t}\n\treturn false;\n}\n\nlong hugetlb_unreserve_pages(struct inode *inode, long start, long end,\n\t\t\t\t\t\t\t\tlong freed)\n{\n\tstruct hstate *h = hstate_inode(inode);\n\tstruct resv_map *resv_map = inode_resv_map(inode);\n\tlong chg = 0;\n\tstruct hugepage_subpool *spool = subpool_inode(inode);\n\tlong gbl_reserve;\n\n\t \n\tif (resv_map) {\n\t\tchg = region_del(resv_map, start, end);\n\t\t \n\t\tif (chg < 0)\n\t\t\treturn chg;\n\t}\n\n\tspin_lock(&inode->i_lock);\n\tinode->i_blocks -= (blocks_per_huge_page(h) * freed);\n\tspin_unlock(&inode->i_lock);\n\n\t \n\tgbl_reserve = hugepage_subpool_put_pages(spool, (chg - freed));\n\thugetlb_acct_memory(h, -gbl_reserve);\n\n\treturn 0;\n}\n\n#ifdef CONFIG_ARCH_WANT_HUGE_PMD_SHARE\nstatic unsigned long page_table_shareable(struct vm_area_struct *svma,\n\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\tunsigned long addr, pgoff_t idx)\n{\n\tunsigned long saddr = ((idx - svma->vm_pgoff) << PAGE_SHIFT) +\n\t\t\t\tsvma->vm_start;\n\tunsigned long sbase = saddr & PUD_MASK;\n\tunsigned long s_end = sbase + PUD_SIZE;\n\n\t \n\tunsigned long vm_flags = vma->vm_flags & ~VM_LOCKED_MASK;\n\tunsigned long svm_flags = svma->vm_flags & ~VM_LOCKED_MASK;\n\n\t \n\tif (pmd_index(addr) != pmd_index(saddr) ||\n\t    vm_flags != svm_flags ||\n\t    !range_in_vma(svma, sbase, s_end) ||\n\t    !svma->vm_private_data)\n\t\treturn 0;\n\n\treturn saddr;\n}\n\nbool want_pmd_share(struct vm_area_struct *vma, unsigned long addr)\n{\n\tunsigned long start = addr & PUD_MASK;\n\tunsigned long end = start + PUD_SIZE;\n\n#ifdef CONFIG_USERFAULTFD\n\tif (uffd_disable_huge_pmd_share(vma))\n\t\treturn false;\n#endif\n\t \n\tif (!(vma->vm_flags & VM_MAYSHARE))\n\t\treturn false;\n\tif (!vma->vm_private_data)\t \n\t\treturn false;\n\tif (!range_in_vma(vma, start, end))\n\t\treturn false;\n\treturn true;\n}\n\n \nvoid adjust_range_if_pmd_sharing_possible(struct vm_area_struct *vma,\n\t\t\t\tunsigned long *start, unsigned long *end)\n{\n\tunsigned long v_start = ALIGN(vma->vm_start, PUD_SIZE),\n\t\tv_end = ALIGN_DOWN(vma->vm_end, PUD_SIZE);\n\n\t \n\tif (!(vma->vm_flags & VM_MAYSHARE) || !(v_end > v_start) ||\n\t\t(*end <= v_start) || (*start >= v_end))\n\t\treturn;\n\n\t \n\tif (*start > v_start)\n\t\t*start = ALIGN_DOWN(*start, PUD_SIZE);\n\n\tif (*end < v_end)\n\t\t*end = ALIGN(*end, PUD_SIZE);\n}\n\n \npte_t *huge_pmd_share(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t      unsigned long addr, pud_t *pud)\n{\n\tstruct address_space *mapping = vma->vm_file->f_mapping;\n\tpgoff_t idx = ((addr - vma->vm_start) >> PAGE_SHIFT) +\n\t\t\tvma->vm_pgoff;\n\tstruct vm_area_struct *svma;\n\tunsigned long saddr;\n\tpte_t *spte = NULL;\n\tpte_t *pte;\n\n\ti_mmap_lock_read(mapping);\n\tvma_interval_tree_foreach(svma, &mapping->i_mmap, idx, idx) {\n\t\tif (svma == vma)\n\t\t\tcontinue;\n\n\t\tsaddr = page_table_shareable(svma, vma, addr, idx);\n\t\tif (saddr) {\n\t\t\tspte = hugetlb_walk(svma, saddr,\n\t\t\t\t\t    vma_mmu_pagesize(svma));\n\t\t\tif (spte) {\n\t\t\t\tget_page(virt_to_page(spte));\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (!spte)\n\t\tgoto out;\n\n\tspin_lock(&mm->page_table_lock);\n\tif (pud_none(*pud)) {\n\t\tpud_populate(mm, pud,\n\t\t\t\t(pmd_t *)((unsigned long)spte & PAGE_MASK));\n\t\tmm_inc_nr_pmds(mm);\n\t} else {\n\t\tput_page(virt_to_page(spte));\n\t}\n\tspin_unlock(&mm->page_table_lock);\nout:\n\tpte = (pte_t *)pmd_alloc(mm, pud, addr);\n\ti_mmap_unlock_read(mapping);\n\treturn pte;\n}\n\n \nint huge_pmd_unshare(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\t\t\tunsigned long addr, pte_t *ptep)\n{\n\tpgd_t *pgd = pgd_offset(mm, addr);\n\tp4d_t *p4d = p4d_offset(pgd, addr);\n\tpud_t *pud = pud_offset(p4d, addr);\n\n\ti_mmap_assert_write_locked(vma->vm_file->f_mapping);\n\thugetlb_vma_assert_locked(vma);\n\tBUG_ON(page_count(virt_to_page(ptep)) == 0);\n\tif (page_count(virt_to_page(ptep)) == 1)\n\t\treturn 0;\n\n\tpud_clear(pud);\n\tput_page(virt_to_page(ptep));\n\tmm_dec_nr_pmds(mm);\n\treturn 1;\n}\n\n#else  \n\npte_t *huge_pmd_share(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t      unsigned long addr, pud_t *pud)\n{\n\treturn NULL;\n}\n\nint huge_pmd_unshare(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\t\tunsigned long addr, pte_t *ptep)\n{\n\treturn 0;\n}\n\nvoid adjust_range_if_pmd_sharing_possible(struct vm_area_struct *vma,\n\t\t\t\tunsigned long *start, unsigned long *end)\n{\n}\n\nbool want_pmd_share(struct vm_area_struct *vma, unsigned long addr)\n{\n\treturn false;\n}\n#endif  \n\n#ifdef CONFIG_ARCH_WANT_GENERAL_HUGETLB\npte_t *huge_pte_alloc(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\tunsigned long addr, unsigned long sz)\n{\n\tpgd_t *pgd;\n\tp4d_t *p4d;\n\tpud_t *pud;\n\tpte_t *pte = NULL;\n\n\tpgd = pgd_offset(mm, addr);\n\tp4d = p4d_alloc(mm, pgd, addr);\n\tif (!p4d)\n\t\treturn NULL;\n\tpud = pud_alloc(mm, p4d, addr);\n\tif (pud) {\n\t\tif (sz == PUD_SIZE) {\n\t\t\tpte = (pte_t *)pud;\n\t\t} else {\n\t\t\tBUG_ON(sz != PMD_SIZE);\n\t\t\tif (want_pmd_share(vma, addr) && pud_none(*pud))\n\t\t\t\tpte = huge_pmd_share(mm, vma, addr, pud);\n\t\t\telse\n\t\t\t\tpte = (pte_t *)pmd_alloc(mm, pud, addr);\n\t\t}\n\t}\n\n\tif (pte) {\n\t\tpte_t pteval = ptep_get_lockless(pte);\n\n\t\tBUG_ON(pte_present(pteval) && !pte_huge(pteval));\n\t}\n\n\treturn pte;\n}\n\n \npte_t *huge_pte_offset(struct mm_struct *mm,\n\t\t       unsigned long addr, unsigned long sz)\n{\n\tpgd_t *pgd;\n\tp4d_t *p4d;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\n\tpgd = pgd_offset(mm, addr);\n\tif (!pgd_present(*pgd))\n\t\treturn NULL;\n\tp4d = p4d_offset(pgd, addr);\n\tif (!p4d_present(*p4d))\n\t\treturn NULL;\n\n\tpud = pud_offset(p4d, addr);\n\tif (sz == PUD_SIZE)\n\t\t \n\t\treturn (pte_t *)pud;\n\tif (!pud_present(*pud))\n\t\treturn NULL;\n\t \n\n\tpmd = pmd_offset(pud, addr);\n\t \n\treturn (pte_t *)pmd;\n}\n\n \nunsigned long hugetlb_mask_last_page(struct hstate *h)\n{\n\tunsigned long hp_size = huge_page_size(h);\n\n\tif (hp_size == PUD_SIZE)\n\t\treturn P4D_SIZE - PUD_SIZE;\n\telse if (hp_size == PMD_SIZE)\n\t\treturn PUD_SIZE - PMD_SIZE;\n\telse\n\t\treturn 0UL;\n}\n\n#else\n\n \n__weak unsigned long hugetlb_mask_last_page(struct hstate *h)\n{\n#ifdef CONFIG_ARCH_WANT_HUGE_PMD_SHARE\n\tif (huge_page_size(h) == PMD_SIZE)\n\t\treturn PUD_SIZE - PMD_SIZE;\n#endif\n\treturn 0UL;\n}\n\n#endif  \n\n \nbool isolate_hugetlb(struct folio *folio, struct list_head *list)\n{\n\tbool ret = true;\n\n\tspin_lock_irq(&hugetlb_lock);\n\tif (!folio_test_hugetlb(folio) ||\n\t    !folio_test_hugetlb_migratable(folio) ||\n\t    !folio_try_get(folio)) {\n\t\tret = false;\n\t\tgoto unlock;\n\t}\n\tfolio_clear_hugetlb_migratable(folio);\n\tlist_move_tail(&folio->lru, list);\nunlock:\n\tspin_unlock_irq(&hugetlb_lock);\n\treturn ret;\n}\n\nint get_hwpoison_hugetlb_folio(struct folio *folio, bool *hugetlb, bool unpoison)\n{\n\tint ret = 0;\n\n\t*hugetlb = false;\n\tspin_lock_irq(&hugetlb_lock);\n\tif (folio_test_hugetlb(folio)) {\n\t\t*hugetlb = true;\n\t\tif (folio_test_hugetlb_freed(folio))\n\t\t\tret = 0;\n\t\telse if (folio_test_hugetlb_migratable(folio) || unpoison)\n\t\t\tret = folio_try_get(folio);\n\t\telse\n\t\t\tret = -EBUSY;\n\t}\n\tspin_unlock_irq(&hugetlb_lock);\n\treturn ret;\n}\n\nint get_huge_page_for_hwpoison(unsigned long pfn, int flags,\n\t\t\t\tbool *migratable_cleared)\n{\n\tint ret;\n\n\tspin_lock_irq(&hugetlb_lock);\n\tret = __get_huge_page_for_hwpoison(pfn, flags, migratable_cleared);\n\tspin_unlock_irq(&hugetlb_lock);\n\treturn ret;\n}\n\nvoid folio_putback_active_hugetlb(struct folio *folio)\n{\n\tspin_lock_irq(&hugetlb_lock);\n\tfolio_set_hugetlb_migratable(folio);\n\tlist_move_tail(&folio->lru, &(folio_hstate(folio))->hugepage_activelist);\n\tspin_unlock_irq(&hugetlb_lock);\n\tfolio_put(folio);\n}\n\nvoid move_hugetlb_state(struct folio *old_folio, struct folio *new_folio, int reason)\n{\n\tstruct hstate *h = folio_hstate(old_folio);\n\n\thugetlb_cgroup_migrate(old_folio, new_folio);\n\tset_page_owner_migrate_reason(&new_folio->page, reason);\n\n\t \n\tif (folio_test_hugetlb_temporary(new_folio)) {\n\t\tint old_nid = folio_nid(old_folio);\n\t\tint new_nid = folio_nid(new_folio);\n\n\t\tfolio_set_hugetlb_temporary(old_folio);\n\t\tfolio_clear_hugetlb_temporary(new_folio);\n\n\n\t\t \n\t\tif (new_nid == old_nid)\n\t\t\treturn;\n\t\tspin_lock_irq(&hugetlb_lock);\n\t\tif (h->surplus_huge_pages_node[old_nid]) {\n\t\t\th->surplus_huge_pages_node[old_nid]--;\n\t\t\th->surplus_huge_pages_node[new_nid]++;\n\t\t}\n\t\tspin_unlock_irq(&hugetlb_lock);\n\t}\n}\n\nstatic void hugetlb_unshare_pmds(struct vm_area_struct *vma,\n\t\t\t\t   unsigned long start,\n\t\t\t\t   unsigned long end)\n{\n\tstruct hstate *h = hstate_vma(vma);\n\tunsigned long sz = huge_page_size(h);\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct mmu_notifier_range range;\n\tunsigned long address;\n\tspinlock_t *ptl;\n\tpte_t *ptep;\n\n\tif (!(vma->vm_flags & VM_MAYSHARE))\n\t\treturn;\n\n\tif (start >= end)\n\t\treturn;\n\n\tflush_cache_range(vma, start, end);\n\t \n\tmmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, mm,\n\t\t\t\tstart, end);\n\tmmu_notifier_invalidate_range_start(&range);\n\thugetlb_vma_lock_write(vma);\n\ti_mmap_lock_write(vma->vm_file->f_mapping);\n\tfor (address = start; address < end; address += PUD_SIZE) {\n\t\tptep = hugetlb_walk(vma, address, sz);\n\t\tif (!ptep)\n\t\t\tcontinue;\n\t\tptl = huge_pte_lock(h, mm, ptep);\n\t\thuge_pmd_unshare(mm, vma, address, ptep);\n\t\tspin_unlock(ptl);\n\t}\n\tflush_hugetlb_tlb_range(vma, start, end);\n\ti_mmap_unlock_write(vma->vm_file->f_mapping);\n\thugetlb_vma_unlock_write(vma);\n\t \n\tmmu_notifier_invalidate_range_end(&range);\n}\n\n \nvoid hugetlb_unshare_all_pmds(struct vm_area_struct *vma)\n{\n\thugetlb_unshare_pmds(vma, ALIGN(vma->vm_start, PUD_SIZE),\n\t\t\tALIGN_DOWN(vma->vm_end, PUD_SIZE));\n}\n\n#ifdef CONFIG_CMA\nstatic bool cma_reserve_called __initdata;\n\nstatic int __init cmdline_parse_hugetlb_cma(char *p)\n{\n\tint nid, count = 0;\n\tunsigned long tmp;\n\tchar *s = p;\n\n\twhile (*s) {\n\t\tif (sscanf(s, \"%lu%n\", &tmp, &count) != 1)\n\t\t\tbreak;\n\n\t\tif (s[count] == ':') {\n\t\t\tif (tmp >= MAX_NUMNODES)\n\t\t\t\tbreak;\n\t\t\tnid = array_index_nospec(tmp, MAX_NUMNODES);\n\n\t\t\ts += count + 1;\n\t\t\ttmp = memparse(s, &s);\n\t\t\thugetlb_cma_size_in_node[nid] = tmp;\n\t\t\thugetlb_cma_size += tmp;\n\n\t\t\t \n\t\t\tif (*s == ',')\n\t\t\t\ts++;\n\t\t\telse\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\thugetlb_cma_size = memparse(p, &p);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nearly_param(\"hugetlb_cma\", cmdline_parse_hugetlb_cma);\n\nvoid __init hugetlb_cma_reserve(int order)\n{\n\tunsigned long size, reserved, per_node;\n\tbool node_specific_cma_alloc = false;\n\tint nid;\n\n\tcma_reserve_called = true;\n\n\tif (!hugetlb_cma_size)\n\t\treturn;\n\n\tfor (nid = 0; nid < MAX_NUMNODES; nid++) {\n\t\tif (hugetlb_cma_size_in_node[nid] == 0)\n\t\t\tcontinue;\n\n\t\tif (!node_online(nid)) {\n\t\t\tpr_warn(\"hugetlb_cma: invalid node %d specified\\n\", nid);\n\t\t\thugetlb_cma_size -= hugetlb_cma_size_in_node[nid];\n\t\t\thugetlb_cma_size_in_node[nid] = 0;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (hugetlb_cma_size_in_node[nid] < (PAGE_SIZE << order)) {\n\t\t\tpr_warn(\"hugetlb_cma: cma area of node %d should be at least %lu MiB\\n\",\n\t\t\t\tnid, (PAGE_SIZE << order) / SZ_1M);\n\t\t\thugetlb_cma_size -= hugetlb_cma_size_in_node[nid];\n\t\t\thugetlb_cma_size_in_node[nid] = 0;\n\t\t} else {\n\t\t\tnode_specific_cma_alloc = true;\n\t\t}\n\t}\n\n\t \n\tif (!hugetlb_cma_size)\n\t\treturn;\n\n\tif (hugetlb_cma_size < (PAGE_SIZE << order)) {\n\t\tpr_warn(\"hugetlb_cma: cma area should be at least %lu MiB\\n\",\n\t\t\t(PAGE_SIZE << order) / SZ_1M);\n\t\thugetlb_cma_size = 0;\n\t\treturn;\n\t}\n\n\tif (!node_specific_cma_alloc) {\n\t\t \n\t\tper_node = DIV_ROUND_UP(hugetlb_cma_size, nr_online_nodes);\n\t\tpr_info(\"hugetlb_cma: reserve %lu MiB, up to %lu MiB per node\\n\",\n\t\t\thugetlb_cma_size / SZ_1M, per_node / SZ_1M);\n\t}\n\n\treserved = 0;\n\tfor_each_online_node(nid) {\n\t\tint res;\n\t\tchar name[CMA_MAX_NAME];\n\n\t\tif (node_specific_cma_alloc) {\n\t\t\tif (hugetlb_cma_size_in_node[nid] == 0)\n\t\t\t\tcontinue;\n\n\t\t\tsize = hugetlb_cma_size_in_node[nid];\n\t\t} else {\n\t\t\tsize = min(per_node, hugetlb_cma_size - reserved);\n\t\t}\n\n\t\tsize = round_up(size, PAGE_SIZE << order);\n\n\t\tsnprintf(name, sizeof(name), \"hugetlb%d\", nid);\n\t\t \n\t\tres = cma_declare_contiguous_nid(0, size, 0,\n\t\t\t\t\t\tPAGE_SIZE << HUGETLB_PAGE_ORDER,\n\t\t\t\t\t\t 0, false, name,\n\t\t\t\t\t\t &hugetlb_cma[nid], nid);\n\t\tif (res) {\n\t\t\tpr_warn(\"hugetlb_cma: reservation failed: err %d, node %d\",\n\t\t\t\tres, nid);\n\t\t\tcontinue;\n\t\t}\n\n\t\treserved += size;\n\t\tpr_info(\"hugetlb_cma: reserved %lu MiB on node %d\\n\",\n\t\t\tsize / SZ_1M, nid);\n\n\t\tif (reserved >= hugetlb_cma_size)\n\t\t\tbreak;\n\t}\n\n\tif (!reserved)\n\t\t \n\t\thugetlb_cma_size = 0;\n}\n\nstatic void __init hugetlb_cma_check(void)\n{\n\tif (!hugetlb_cma_size || cma_reserve_called)\n\t\treturn;\n\n\tpr_warn(\"hugetlb_cma: the option isn't supported by current arch\\n\");\n}\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}