{
  "module_name": "percpu.c",
  "hash_id": "21d68b265150a5dd9b084cf0e498619dbaece74328f246a1e95ae031801e7e97",
  "original_prompt": "Ingested from linux-6.6.14/mm/percpu.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/bitmap.h>\n#include <linux/cpumask.h>\n#include <linux/memblock.h>\n#include <linux/err.h>\n#include <linux/list.h>\n#include <linux/log2.h>\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <linux/mutex.h>\n#include <linux/percpu.h>\n#include <linux/pfn.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n#include <linux/vmalloc.h>\n#include <linux/workqueue.h>\n#include <linux/kmemleak.h>\n#include <linux/sched.h>\n#include <linux/sched/mm.h>\n#include <linux/memcontrol.h>\n\n#include <asm/cacheflush.h>\n#include <asm/sections.h>\n#include <asm/tlbflush.h>\n#include <asm/io.h>\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/percpu.h>\n\n#include \"percpu-internal.h\"\n\n \n#define PCPU_SLOT_BASE_SHIFT\t\t5\n \n#define PCPU_SLOT_FAIL_THRESHOLD\t3\n\n#define PCPU_EMPTY_POP_PAGES_LOW\t2\n#define PCPU_EMPTY_POP_PAGES_HIGH\t4\n\n#ifdef CONFIG_SMP\n \n#ifndef __addr_to_pcpu_ptr\n#define __addr_to_pcpu_ptr(addr)\t\t\t\t\t\\\n\t(void __percpu *)((unsigned long)(addr) -\t\t\t\\\n\t\t\t  (unsigned long)pcpu_base_addr\t+\t\t\\\n\t\t\t  (unsigned long)__per_cpu_start)\n#endif\n#ifndef __pcpu_ptr_to_addr\n#define __pcpu_ptr_to_addr(ptr)\t\t\t\t\t\t\\\n\t(void __force *)((unsigned long)(ptr) +\t\t\t\t\\\n\t\t\t (unsigned long)pcpu_base_addr -\t\t\\\n\t\t\t (unsigned long)__per_cpu_start)\n#endif\n#else\t \n \n#define __addr_to_pcpu_ptr(addr)\t(void __percpu *)(addr)\n#define __pcpu_ptr_to_addr(ptr)\t\t(void __force *)(ptr)\n#endif\t \n\nstatic int pcpu_unit_pages __ro_after_init;\nstatic int pcpu_unit_size __ro_after_init;\nstatic int pcpu_nr_units __ro_after_init;\nstatic int pcpu_atom_size __ro_after_init;\nint pcpu_nr_slots __ro_after_init;\nstatic int pcpu_free_slot __ro_after_init;\nint pcpu_sidelined_slot __ro_after_init;\nint pcpu_to_depopulate_slot __ro_after_init;\nstatic size_t pcpu_chunk_struct_size __ro_after_init;\n\n \nstatic unsigned int pcpu_low_unit_cpu __ro_after_init;\nstatic unsigned int pcpu_high_unit_cpu __ro_after_init;\n\n \nvoid *pcpu_base_addr __ro_after_init;\n\nstatic const int *pcpu_unit_map __ro_after_init;\t\t \nconst unsigned long *pcpu_unit_offsets __ro_after_init;\t \n\n \nstatic int pcpu_nr_groups __ro_after_init;\nstatic const unsigned long *pcpu_group_offsets __ro_after_init;\nstatic const size_t *pcpu_group_sizes __ro_after_init;\n\n \nstruct pcpu_chunk *pcpu_first_chunk __ro_after_init;\n\n \nstruct pcpu_chunk *pcpu_reserved_chunk __ro_after_init;\n\nDEFINE_SPINLOCK(pcpu_lock);\t \nstatic DEFINE_MUTEX(pcpu_alloc_mutex);\t \n\nstruct list_head *pcpu_chunk_lists __ro_after_init;  \n\n \nint pcpu_nr_empty_pop_pages;\n\n \nstatic unsigned long pcpu_nr_populated;\n\n \nstatic void pcpu_balance_workfn(struct work_struct *work);\nstatic DECLARE_WORK(pcpu_balance_work, pcpu_balance_workfn);\nstatic bool pcpu_async_enabled __read_mostly;\nstatic bool pcpu_atomic_alloc_failed;\n\nstatic void pcpu_schedule_balance_work(void)\n{\n\tif (pcpu_async_enabled)\n\t\tschedule_work(&pcpu_balance_work);\n}\n\n \nstatic bool pcpu_addr_in_chunk(struct pcpu_chunk *chunk, void *addr)\n{\n\tvoid *start_addr, *end_addr;\n\n\tif (!chunk)\n\t\treturn false;\n\n\tstart_addr = chunk->base_addr + chunk->start_offset;\n\tend_addr = chunk->base_addr + chunk->nr_pages * PAGE_SIZE -\n\t\t   chunk->end_offset;\n\n\treturn addr >= start_addr && addr < end_addr;\n}\n\nstatic int __pcpu_size_to_slot(int size)\n{\n\tint highbit = fls(size);\t \n\treturn max(highbit - PCPU_SLOT_BASE_SHIFT + 2, 1);\n}\n\nstatic int pcpu_size_to_slot(int size)\n{\n\tif (size == pcpu_unit_size)\n\t\treturn pcpu_free_slot;\n\treturn __pcpu_size_to_slot(size);\n}\n\nstatic int pcpu_chunk_slot(const struct pcpu_chunk *chunk)\n{\n\tconst struct pcpu_block_md *chunk_md = &chunk->chunk_md;\n\n\tif (chunk->free_bytes < PCPU_MIN_ALLOC_SIZE ||\n\t    chunk_md->contig_hint == 0)\n\t\treturn 0;\n\n\treturn pcpu_size_to_slot(chunk_md->contig_hint * PCPU_MIN_ALLOC_SIZE);\n}\n\n \nstatic void pcpu_set_page_chunk(struct page *page, struct pcpu_chunk *pcpu)\n{\n\tpage->index = (unsigned long)pcpu;\n}\n\n \nstatic struct pcpu_chunk *pcpu_get_page_chunk(struct page *page)\n{\n\treturn (struct pcpu_chunk *)page->index;\n}\n\nstatic int __maybe_unused pcpu_page_idx(unsigned int cpu, int page_idx)\n{\n\treturn pcpu_unit_map[cpu] * pcpu_unit_pages + page_idx;\n}\n\nstatic unsigned long pcpu_unit_page_offset(unsigned int cpu, int page_idx)\n{\n\treturn pcpu_unit_offsets[cpu] + (page_idx << PAGE_SHIFT);\n}\n\nstatic unsigned long pcpu_chunk_addr(struct pcpu_chunk *chunk,\n\t\t\t\t     unsigned int cpu, int page_idx)\n{\n\treturn (unsigned long)chunk->base_addr +\n\t       pcpu_unit_page_offset(cpu, page_idx);\n}\n\n \nstatic unsigned long *pcpu_index_alloc_map(struct pcpu_chunk *chunk, int index)\n{\n\treturn chunk->alloc_map +\n\t       (index * PCPU_BITMAP_BLOCK_BITS / BITS_PER_LONG);\n}\n\nstatic unsigned long pcpu_off_to_block_index(int off)\n{\n\treturn off / PCPU_BITMAP_BLOCK_BITS;\n}\n\nstatic unsigned long pcpu_off_to_block_off(int off)\n{\n\treturn off & (PCPU_BITMAP_BLOCK_BITS - 1);\n}\n\nstatic unsigned long pcpu_block_off_to_off(int index, int off)\n{\n\treturn index * PCPU_BITMAP_BLOCK_BITS + off;\n}\n\n \nstatic bool pcpu_check_block_hint(struct pcpu_block_md *block, int bits,\n\t\t\t\t  size_t align)\n{\n\tint bit_off = ALIGN(block->contig_hint_start, align) -\n\t\tblock->contig_hint_start;\n\n\treturn bit_off + bits <= block->contig_hint;\n}\n\n \nstatic int pcpu_next_hint(struct pcpu_block_md *block, int alloc_bits)\n{\n\t \n\tif (block->scan_hint &&\n\t    block->contig_hint_start > block->scan_hint_start &&\n\t    alloc_bits > block->scan_hint)\n\t\treturn block->scan_hint_start + block->scan_hint;\n\n\treturn block->first_free;\n}\n\n \nstatic void pcpu_next_md_free_region(struct pcpu_chunk *chunk, int *bit_off,\n\t\t\t\t     int *bits)\n{\n\tint i = pcpu_off_to_block_index(*bit_off);\n\tint block_off = pcpu_off_to_block_off(*bit_off);\n\tstruct pcpu_block_md *block;\n\n\t*bits = 0;\n\tfor (block = chunk->md_blocks + i; i < pcpu_chunk_nr_blocks(chunk);\n\t     block++, i++) {\n\t\t \n\t\tif (*bits) {\n\t\t\t*bits += block->left_free;\n\t\t\tif (block->left_free == PCPU_BITMAP_BLOCK_BITS)\n\t\t\t\tcontinue;\n\t\t\treturn;\n\t\t}\n\n\t\t \n\t\t*bits = block->contig_hint;\n\t\tif (*bits && block->contig_hint_start >= block_off &&\n\t\t    *bits + block->contig_hint_start < PCPU_BITMAP_BLOCK_BITS) {\n\t\t\t*bit_off = pcpu_block_off_to_off(i,\n\t\t\t\t\tblock->contig_hint_start);\n\t\t\treturn;\n\t\t}\n\t\t \n\t\tblock_off = 0;\n\n\t\t*bits = block->right_free;\n\t\t*bit_off = (i + 1) * PCPU_BITMAP_BLOCK_BITS - block->right_free;\n\t}\n}\n\n \nstatic void pcpu_next_fit_region(struct pcpu_chunk *chunk, int alloc_bits,\n\t\t\t\t int align, int *bit_off, int *bits)\n{\n\tint i = pcpu_off_to_block_index(*bit_off);\n\tint block_off = pcpu_off_to_block_off(*bit_off);\n\tstruct pcpu_block_md *block;\n\n\t*bits = 0;\n\tfor (block = chunk->md_blocks + i; i < pcpu_chunk_nr_blocks(chunk);\n\t     block++, i++) {\n\t\t \n\t\tif (*bits) {\n\t\t\t*bits += block->left_free;\n\t\t\tif (*bits >= alloc_bits)\n\t\t\t\treturn;\n\t\t\tif (block->left_free == PCPU_BITMAP_BLOCK_BITS)\n\t\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\t*bits = ALIGN(block->contig_hint_start, align) -\n\t\t\tblock->contig_hint_start;\n\t\t \n\t\tif (block->contig_hint &&\n\t\t    block->contig_hint_start >= block_off &&\n\t\t    block->contig_hint >= *bits + alloc_bits) {\n\t\t\tint start = pcpu_next_hint(block, alloc_bits);\n\n\t\t\t*bits += alloc_bits + block->contig_hint_start -\n\t\t\t\t start;\n\t\t\t*bit_off = pcpu_block_off_to_off(i, start);\n\t\t\treturn;\n\t\t}\n\t\t \n\t\tblock_off = 0;\n\n\t\t*bit_off = ALIGN(PCPU_BITMAP_BLOCK_BITS - block->right_free,\n\t\t\t\t align);\n\t\t*bits = PCPU_BITMAP_BLOCK_BITS - *bit_off;\n\t\t*bit_off = pcpu_block_off_to_off(i, *bit_off);\n\t\tif (*bits >= alloc_bits)\n\t\t\treturn;\n\t}\n\n\t \n\t*bit_off = pcpu_chunk_map_bits(chunk);\n}\n\n \n#define pcpu_for_each_md_free_region(chunk, bit_off, bits)\t\t\\\n\tfor (pcpu_next_md_free_region((chunk), &(bit_off), &(bits));\t\\\n\t     (bit_off) < pcpu_chunk_map_bits((chunk));\t\t\t\\\n\t     (bit_off) += (bits) + 1,\t\t\t\t\t\\\n\t     pcpu_next_md_free_region((chunk), &(bit_off), &(bits)))\n\n#define pcpu_for_each_fit_region(chunk, alloc_bits, align, bit_off, bits)     \\\n\tfor (pcpu_next_fit_region((chunk), (alloc_bits), (align), &(bit_off), \\\n\t\t\t\t  &(bits));\t\t\t\t      \\\n\t     (bit_off) < pcpu_chunk_map_bits((chunk));\t\t\t      \\\n\t     (bit_off) += (bits),\t\t\t\t\t      \\\n\t     pcpu_next_fit_region((chunk), (alloc_bits), (align), &(bit_off), \\\n\t\t\t\t  &(bits)))\n\n \nstatic void *pcpu_mem_zalloc(size_t size, gfp_t gfp)\n{\n\tif (WARN_ON_ONCE(!slab_is_available()))\n\t\treturn NULL;\n\n\tif (size <= PAGE_SIZE)\n\t\treturn kzalloc(size, gfp);\n\telse\n\t\treturn __vmalloc(size, gfp | __GFP_ZERO);\n}\n\n \nstatic void pcpu_mem_free(void *ptr)\n{\n\tkvfree(ptr);\n}\n\nstatic void __pcpu_chunk_move(struct pcpu_chunk *chunk, int slot,\n\t\t\t      bool move_front)\n{\n\tif (chunk != pcpu_reserved_chunk) {\n\t\tif (move_front)\n\t\t\tlist_move(&chunk->list, &pcpu_chunk_lists[slot]);\n\t\telse\n\t\t\tlist_move_tail(&chunk->list, &pcpu_chunk_lists[slot]);\n\t}\n}\n\nstatic void pcpu_chunk_move(struct pcpu_chunk *chunk, int slot)\n{\n\t__pcpu_chunk_move(chunk, slot, true);\n}\n\n \nstatic void pcpu_chunk_relocate(struct pcpu_chunk *chunk, int oslot)\n{\n\tint nslot = pcpu_chunk_slot(chunk);\n\n\t \n\tif (chunk->isolated)\n\t\treturn;\n\n\tif (oslot != nslot)\n\t\t__pcpu_chunk_move(chunk, nslot, oslot < nslot);\n}\n\nstatic void pcpu_isolate_chunk(struct pcpu_chunk *chunk)\n{\n\tlockdep_assert_held(&pcpu_lock);\n\n\tif (!chunk->isolated) {\n\t\tchunk->isolated = true;\n\t\tpcpu_nr_empty_pop_pages -= chunk->nr_empty_pop_pages;\n\t}\n\tlist_move(&chunk->list, &pcpu_chunk_lists[pcpu_to_depopulate_slot]);\n}\n\nstatic void pcpu_reintegrate_chunk(struct pcpu_chunk *chunk)\n{\n\tlockdep_assert_held(&pcpu_lock);\n\n\tif (chunk->isolated) {\n\t\tchunk->isolated = false;\n\t\tpcpu_nr_empty_pop_pages += chunk->nr_empty_pop_pages;\n\t\tpcpu_chunk_relocate(chunk, -1);\n\t}\n}\n\n \nstatic inline void pcpu_update_empty_pages(struct pcpu_chunk *chunk, int nr)\n{\n\tchunk->nr_empty_pop_pages += nr;\n\tif (chunk != pcpu_reserved_chunk && !chunk->isolated)\n\t\tpcpu_nr_empty_pop_pages += nr;\n}\n\n \nstatic inline bool pcpu_region_overlap(int a, int b, int x, int y)\n{\n\treturn (a < y) && (x < b);\n}\n\n \nstatic void pcpu_block_update(struct pcpu_block_md *block, int start, int end)\n{\n\tint contig = end - start;\n\n\tblock->first_free = min(block->first_free, start);\n\tif (start == 0)\n\t\tblock->left_free = contig;\n\n\tif (end == block->nr_bits)\n\t\tblock->right_free = contig;\n\n\tif (contig > block->contig_hint) {\n\t\t \n\t\tif (start > block->contig_hint_start) {\n\t\t\tif (block->contig_hint > block->scan_hint) {\n\t\t\t\tblock->scan_hint_start =\n\t\t\t\t\tblock->contig_hint_start;\n\t\t\t\tblock->scan_hint = block->contig_hint;\n\t\t\t} else if (start < block->scan_hint_start) {\n\t\t\t\t \n\t\t\t\tblock->scan_hint = 0;\n\t\t\t}\n\t\t} else {\n\t\t\tblock->scan_hint = 0;\n\t\t}\n\t\tblock->contig_hint_start = start;\n\t\tblock->contig_hint = contig;\n\t} else if (contig == block->contig_hint) {\n\t\tif (block->contig_hint_start &&\n\t\t    (!start ||\n\t\t     __ffs(start) > __ffs(block->contig_hint_start))) {\n\t\t\t \n\t\t\tblock->contig_hint_start = start;\n\t\t\tif (start < block->scan_hint_start &&\n\t\t\t    block->contig_hint > block->scan_hint)\n\t\t\t\tblock->scan_hint = 0;\n\t\t} else if (start > block->scan_hint_start ||\n\t\t\t   block->contig_hint > block->scan_hint) {\n\t\t\t \n\t\t\tblock->scan_hint_start = start;\n\t\t\tblock->scan_hint = contig;\n\t\t}\n\t} else {\n\t\t \n\t\tif ((start < block->contig_hint_start &&\n\t\t     (contig > block->scan_hint ||\n\t\t      (contig == block->scan_hint &&\n\t\t       start > block->scan_hint_start)))) {\n\t\t\tblock->scan_hint_start = start;\n\t\t\tblock->scan_hint = contig;\n\t\t}\n\t}\n}\n\n \nstatic void pcpu_block_update_scan(struct pcpu_chunk *chunk, int bit_off,\n\t\t\t\t   int bits)\n{\n\tint s_off = pcpu_off_to_block_off(bit_off);\n\tint e_off = s_off + bits;\n\tint s_index, l_bit;\n\tstruct pcpu_block_md *block;\n\n\tif (e_off > PCPU_BITMAP_BLOCK_BITS)\n\t\treturn;\n\n\ts_index = pcpu_off_to_block_index(bit_off);\n\tblock = chunk->md_blocks + s_index;\n\n\t \n\tl_bit = find_last_bit(pcpu_index_alloc_map(chunk, s_index), s_off);\n\ts_off = (s_off == l_bit) ? 0 : l_bit + 1;\n\n\tpcpu_block_update(block, s_off, e_off);\n}\n\n \nstatic void pcpu_chunk_refresh_hint(struct pcpu_chunk *chunk, bool full_scan)\n{\n\tstruct pcpu_block_md *chunk_md = &chunk->chunk_md;\n\tint bit_off, bits;\n\n\t \n\tif (!full_scan && chunk_md->scan_hint) {\n\t\tbit_off = chunk_md->scan_hint_start + chunk_md->scan_hint;\n\t\tchunk_md->contig_hint_start = chunk_md->scan_hint_start;\n\t\tchunk_md->contig_hint = chunk_md->scan_hint;\n\t\tchunk_md->scan_hint = 0;\n\t} else {\n\t\tbit_off = chunk_md->first_free;\n\t\tchunk_md->contig_hint = 0;\n\t}\n\n\tbits = 0;\n\tpcpu_for_each_md_free_region(chunk, bit_off, bits)\n\t\tpcpu_block_update(chunk_md, bit_off, bit_off + bits);\n}\n\n \nstatic void pcpu_block_refresh_hint(struct pcpu_chunk *chunk, int index)\n{\n\tstruct pcpu_block_md *block = chunk->md_blocks + index;\n\tunsigned long *alloc_map = pcpu_index_alloc_map(chunk, index);\n\tunsigned int start, end;\t \n\n\t \n\tif (block->scan_hint) {\n\t\tstart = block->scan_hint_start + block->scan_hint;\n\t\tblock->contig_hint_start = block->scan_hint_start;\n\t\tblock->contig_hint = block->scan_hint;\n\t\tblock->scan_hint = 0;\n\t} else {\n\t\tstart = block->first_free;\n\t\tblock->contig_hint = 0;\n\t}\n\n\tblock->right_free = 0;\n\n\t \n\tfor_each_clear_bitrange_from(start, end, alloc_map, PCPU_BITMAP_BLOCK_BITS)\n\t\tpcpu_block_update(block, start, end);\n}\n\n \nstatic void pcpu_block_update_hint_alloc(struct pcpu_chunk *chunk, int bit_off,\n\t\t\t\t\t int bits)\n{\n\tstruct pcpu_block_md *chunk_md = &chunk->chunk_md;\n\tint nr_empty_pages = 0;\n\tstruct pcpu_block_md *s_block, *e_block, *block;\n\tint s_index, e_index;\t \n\tint s_off, e_off;\t \n\n\t \n\ts_index = pcpu_off_to_block_index(bit_off);\n\te_index = pcpu_off_to_block_index(bit_off + bits - 1);\n\ts_off = pcpu_off_to_block_off(bit_off);\n\te_off = pcpu_off_to_block_off(bit_off + bits - 1) + 1;\n\n\ts_block = chunk->md_blocks + s_index;\n\te_block = chunk->md_blocks + e_index;\n\n\t \n\tif (s_block->contig_hint == PCPU_BITMAP_BLOCK_BITS)\n\t\tnr_empty_pages++;\n\n\t \n\tif (s_off == s_block->first_free)\n\t\ts_block->first_free = find_next_zero_bit(\n\t\t\t\t\tpcpu_index_alloc_map(chunk, s_index),\n\t\t\t\t\tPCPU_BITMAP_BLOCK_BITS,\n\t\t\t\t\ts_off + bits);\n\n\tif (pcpu_region_overlap(s_block->scan_hint_start,\n\t\t\t\ts_block->scan_hint_start + s_block->scan_hint,\n\t\t\t\ts_off,\n\t\t\t\ts_off + bits))\n\t\ts_block->scan_hint = 0;\n\n\tif (pcpu_region_overlap(s_block->contig_hint_start,\n\t\t\t\ts_block->contig_hint_start +\n\t\t\t\ts_block->contig_hint,\n\t\t\t\ts_off,\n\t\t\t\ts_off + bits)) {\n\t\t \n\t\tif (!s_off)\n\t\t\ts_block->left_free = 0;\n\t\tpcpu_block_refresh_hint(chunk, s_index);\n\t} else {\n\t\t \n\t\ts_block->left_free = min(s_block->left_free, s_off);\n\t\tif (s_index == e_index)\n\t\t\ts_block->right_free = min_t(int, s_block->right_free,\n\t\t\t\t\tPCPU_BITMAP_BLOCK_BITS - e_off);\n\t\telse\n\t\t\ts_block->right_free = 0;\n\t}\n\n\t \n\tif (s_index != e_index) {\n\t\tif (e_block->contig_hint == PCPU_BITMAP_BLOCK_BITS)\n\t\t\tnr_empty_pages++;\n\n\t\t \n\t\te_block->first_free = find_next_zero_bit(\n\t\t\t\tpcpu_index_alloc_map(chunk, e_index),\n\t\t\t\tPCPU_BITMAP_BLOCK_BITS, e_off);\n\n\t\tif (e_off == PCPU_BITMAP_BLOCK_BITS) {\n\t\t\t \n\t\t\te_block++;\n\t\t} else {\n\t\t\tif (e_off > e_block->scan_hint_start)\n\t\t\t\te_block->scan_hint = 0;\n\n\t\t\te_block->left_free = 0;\n\t\t\tif (e_off > e_block->contig_hint_start) {\n\t\t\t\t \n\t\t\t\tpcpu_block_refresh_hint(chunk, e_index);\n\t\t\t} else {\n\t\t\t\te_block->right_free =\n\t\t\t\t\tmin_t(int, e_block->right_free,\n\t\t\t\t\t      PCPU_BITMAP_BLOCK_BITS - e_off);\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tnr_empty_pages += (e_index - s_index - 1);\n\t\tfor (block = s_block + 1; block < e_block; block++) {\n\t\t\tblock->scan_hint = 0;\n\t\t\tblock->contig_hint = 0;\n\t\t\tblock->left_free = 0;\n\t\t\tblock->right_free = 0;\n\t\t}\n\t}\n\n\t \n\tif (nr_empty_pages)\n\t\tpcpu_update_empty_pages(chunk, -nr_empty_pages);\n\n\tif (pcpu_region_overlap(chunk_md->scan_hint_start,\n\t\t\t\tchunk_md->scan_hint_start +\n\t\t\t\tchunk_md->scan_hint,\n\t\t\t\tbit_off,\n\t\t\t\tbit_off + bits))\n\t\tchunk_md->scan_hint = 0;\n\n\t \n\tif (pcpu_region_overlap(chunk_md->contig_hint_start,\n\t\t\t\tchunk_md->contig_hint_start +\n\t\t\t\tchunk_md->contig_hint,\n\t\t\t\tbit_off,\n\t\t\t\tbit_off + bits))\n\t\tpcpu_chunk_refresh_hint(chunk, false);\n}\n\n \nstatic void pcpu_block_update_hint_free(struct pcpu_chunk *chunk, int bit_off,\n\t\t\t\t\tint bits)\n{\n\tint nr_empty_pages = 0;\n\tstruct pcpu_block_md *s_block, *e_block, *block;\n\tint s_index, e_index;\t \n\tint s_off, e_off;\t \n\tint start, end;\t\t \n\n\t \n\ts_index = pcpu_off_to_block_index(bit_off);\n\te_index = pcpu_off_to_block_index(bit_off + bits - 1);\n\ts_off = pcpu_off_to_block_off(bit_off);\n\te_off = pcpu_off_to_block_off(bit_off + bits - 1) + 1;\n\n\ts_block = chunk->md_blocks + s_index;\n\te_block = chunk->md_blocks + e_index;\n\n\t \n\tstart = s_off;\n\tif (s_off == s_block->contig_hint + s_block->contig_hint_start) {\n\t\tstart = s_block->contig_hint_start;\n\t} else {\n\t\t \n\t\tint l_bit = find_last_bit(pcpu_index_alloc_map(chunk, s_index),\n\t\t\t\t\t  start);\n\t\tstart = (start == l_bit) ? 0 : l_bit + 1;\n\t}\n\n\tend = e_off;\n\tif (e_off == e_block->contig_hint_start)\n\t\tend = e_block->contig_hint_start + e_block->contig_hint;\n\telse\n\t\tend = find_next_bit(pcpu_index_alloc_map(chunk, e_index),\n\t\t\t\t    PCPU_BITMAP_BLOCK_BITS, end);\n\n\t \n\te_off = (s_index == e_index) ? end : PCPU_BITMAP_BLOCK_BITS;\n\tif (!start && e_off == PCPU_BITMAP_BLOCK_BITS)\n\t\tnr_empty_pages++;\n\tpcpu_block_update(s_block, start, e_off);\n\n\t \n\tif (s_index != e_index) {\n\t\t \n\t\tif (end == PCPU_BITMAP_BLOCK_BITS)\n\t\t\tnr_empty_pages++;\n\t\tpcpu_block_update(e_block, 0, end);\n\n\t\t \n\t\tnr_empty_pages += (e_index - s_index - 1);\n\t\tfor (block = s_block + 1; block < e_block; block++) {\n\t\t\tblock->first_free = 0;\n\t\t\tblock->scan_hint = 0;\n\t\t\tblock->contig_hint_start = 0;\n\t\t\tblock->contig_hint = PCPU_BITMAP_BLOCK_BITS;\n\t\t\tblock->left_free = PCPU_BITMAP_BLOCK_BITS;\n\t\t\tblock->right_free = PCPU_BITMAP_BLOCK_BITS;\n\t\t}\n\t}\n\n\tif (nr_empty_pages)\n\t\tpcpu_update_empty_pages(chunk, nr_empty_pages);\n\n\t \n\tif (((end - start) >= PCPU_BITMAP_BLOCK_BITS) || s_index != e_index)\n\t\tpcpu_chunk_refresh_hint(chunk, true);\n\telse\n\t\tpcpu_block_update(&chunk->chunk_md,\n\t\t\t\t  pcpu_block_off_to_off(s_index, start),\n\t\t\t\t  end);\n}\n\n \nstatic bool pcpu_is_populated(struct pcpu_chunk *chunk, int bit_off, int bits,\n\t\t\t      int *next_off)\n{\n\tunsigned int start, end;\n\n\tstart = PFN_DOWN(bit_off * PCPU_MIN_ALLOC_SIZE);\n\tend = PFN_UP((bit_off + bits) * PCPU_MIN_ALLOC_SIZE);\n\n\tstart = find_next_zero_bit(chunk->populated, end, start);\n\tif (start >= end)\n\t\treturn true;\n\n\tend = find_next_bit(chunk->populated, end, start + 1);\n\n\t*next_off = end * PAGE_SIZE / PCPU_MIN_ALLOC_SIZE;\n\treturn false;\n}\n\n \nstatic int pcpu_find_block_fit(struct pcpu_chunk *chunk, int alloc_bits,\n\t\t\t       size_t align, bool pop_only)\n{\n\tstruct pcpu_block_md *chunk_md = &chunk->chunk_md;\n\tint bit_off, bits, next_off;\n\n\t \n\tif (!pcpu_check_block_hint(chunk_md, alloc_bits, align))\n\t\treturn -1;\n\n\tbit_off = pcpu_next_hint(chunk_md, alloc_bits);\n\tbits = 0;\n\tpcpu_for_each_fit_region(chunk, alloc_bits, align, bit_off, bits) {\n\t\tif (!pop_only || pcpu_is_populated(chunk, bit_off, bits,\n\t\t\t\t\t\t   &next_off))\n\t\t\tbreak;\n\n\t\tbit_off = next_off;\n\t\tbits = 0;\n\t}\n\n\tif (bit_off == pcpu_chunk_map_bits(chunk))\n\t\treturn -1;\n\n\treturn bit_off;\n}\n\n \nstatic unsigned long pcpu_find_zero_area(unsigned long *map,\n\t\t\t\t\t unsigned long size,\n\t\t\t\t\t unsigned long start,\n\t\t\t\t\t unsigned long nr,\n\t\t\t\t\t unsigned long align_mask,\n\t\t\t\t\t unsigned long *largest_off,\n\t\t\t\t\t unsigned long *largest_bits)\n{\n\tunsigned long index, end, i, area_off, area_bits;\nagain:\n\tindex = find_next_zero_bit(map, size, start);\n\n\t \n\tindex = __ALIGN_MASK(index, align_mask);\n\tarea_off = index;\n\n\tend = index + nr;\n\tif (end > size)\n\t\treturn end;\n\ti = find_next_bit(map, end, index);\n\tif (i < end) {\n\t\tarea_bits = i - area_off;\n\t\t \n\t\tif (area_bits > *largest_bits ||\n\t\t    (area_bits == *largest_bits && *largest_off &&\n\t\t     (!area_off || __ffs(area_off) > __ffs(*largest_off)))) {\n\t\t\t*largest_off = area_off;\n\t\t\t*largest_bits = area_bits;\n\t\t}\n\n\t\tstart = i + 1;\n\t\tgoto again;\n\t}\n\treturn index;\n}\n\n \nstatic int pcpu_alloc_area(struct pcpu_chunk *chunk, int alloc_bits,\n\t\t\t   size_t align, int start)\n{\n\tstruct pcpu_block_md *chunk_md = &chunk->chunk_md;\n\tsize_t align_mask = (align) ? (align - 1) : 0;\n\tunsigned long area_off = 0, area_bits = 0;\n\tint bit_off, end, oslot;\n\n\tlockdep_assert_held(&pcpu_lock);\n\n\toslot = pcpu_chunk_slot(chunk);\n\n\t \n\tend = min_t(int, start + alloc_bits + PCPU_BITMAP_BLOCK_BITS,\n\t\t    pcpu_chunk_map_bits(chunk));\n\tbit_off = pcpu_find_zero_area(chunk->alloc_map, end, start, alloc_bits,\n\t\t\t\t      align_mask, &area_off, &area_bits);\n\tif (bit_off >= end)\n\t\treturn -1;\n\n\tif (area_bits)\n\t\tpcpu_block_update_scan(chunk, area_off, area_bits);\n\n\t \n\tbitmap_set(chunk->alloc_map, bit_off, alloc_bits);\n\n\t \n\tset_bit(bit_off, chunk->bound_map);\n\tbitmap_clear(chunk->bound_map, bit_off + 1, alloc_bits - 1);\n\tset_bit(bit_off + alloc_bits, chunk->bound_map);\n\n\tchunk->free_bytes -= alloc_bits * PCPU_MIN_ALLOC_SIZE;\n\n\t \n\tif (bit_off == chunk_md->first_free)\n\t\tchunk_md->first_free = find_next_zero_bit(\n\t\t\t\t\tchunk->alloc_map,\n\t\t\t\t\tpcpu_chunk_map_bits(chunk),\n\t\t\t\t\tbit_off + alloc_bits);\n\n\tpcpu_block_update_hint_alloc(chunk, bit_off, alloc_bits);\n\n\tpcpu_chunk_relocate(chunk, oslot);\n\n\treturn bit_off * PCPU_MIN_ALLOC_SIZE;\n}\n\n \nstatic int pcpu_free_area(struct pcpu_chunk *chunk, int off)\n{\n\tstruct pcpu_block_md *chunk_md = &chunk->chunk_md;\n\tint bit_off, bits, end, oslot, freed;\n\n\tlockdep_assert_held(&pcpu_lock);\n\tpcpu_stats_area_dealloc(chunk);\n\n\toslot = pcpu_chunk_slot(chunk);\n\n\tbit_off = off / PCPU_MIN_ALLOC_SIZE;\n\n\t \n\tend = find_next_bit(chunk->bound_map, pcpu_chunk_map_bits(chunk),\n\t\t\t    bit_off + 1);\n\tbits = end - bit_off;\n\tbitmap_clear(chunk->alloc_map, bit_off, bits);\n\n\tfreed = bits * PCPU_MIN_ALLOC_SIZE;\n\n\t \n\tchunk->free_bytes += freed;\n\n\t \n\tchunk_md->first_free = min(chunk_md->first_free, bit_off);\n\n\tpcpu_block_update_hint_free(chunk, bit_off, bits);\n\n\tpcpu_chunk_relocate(chunk, oslot);\n\n\treturn freed;\n}\n\nstatic void pcpu_init_md_block(struct pcpu_block_md *block, int nr_bits)\n{\n\tblock->scan_hint = 0;\n\tblock->contig_hint = nr_bits;\n\tblock->left_free = nr_bits;\n\tblock->right_free = nr_bits;\n\tblock->first_free = 0;\n\tblock->nr_bits = nr_bits;\n}\n\nstatic void pcpu_init_md_blocks(struct pcpu_chunk *chunk)\n{\n\tstruct pcpu_block_md *md_block;\n\n\t \n\tpcpu_init_md_block(&chunk->chunk_md, pcpu_chunk_map_bits(chunk));\n\n\tfor (md_block = chunk->md_blocks;\n\t     md_block != chunk->md_blocks + pcpu_chunk_nr_blocks(chunk);\n\t     md_block++)\n\t\tpcpu_init_md_block(md_block, PCPU_BITMAP_BLOCK_BITS);\n}\n\n \nstatic struct pcpu_chunk * __init pcpu_alloc_first_chunk(unsigned long tmp_addr,\n\t\t\t\t\t\t\t int map_size)\n{\n\tstruct pcpu_chunk *chunk;\n\tunsigned long aligned_addr;\n\tint start_offset, offset_bits, region_size, region_bits;\n\tsize_t alloc_size;\n\n\t \n\taligned_addr = tmp_addr & PAGE_MASK;\n\n\tstart_offset = tmp_addr - aligned_addr;\n\tregion_size = ALIGN(start_offset + map_size, PAGE_SIZE);\n\n\t \n\talloc_size = struct_size(chunk, populated,\n\t\t\t\t BITS_TO_LONGS(region_size >> PAGE_SHIFT));\n\tchunk = memblock_alloc(alloc_size, SMP_CACHE_BYTES);\n\tif (!chunk)\n\t\tpanic(\"%s: Failed to allocate %zu bytes\\n\", __func__,\n\t\t      alloc_size);\n\n\tINIT_LIST_HEAD(&chunk->list);\n\n\tchunk->base_addr = (void *)aligned_addr;\n\tchunk->start_offset = start_offset;\n\tchunk->end_offset = region_size - chunk->start_offset - map_size;\n\n\tchunk->nr_pages = region_size >> PAGE_SHIFT;\n\tregion_bits = pcpu_chunk_map_bits(chunk);\n\n\talloc_size = BITS_TO_LONGS(region_bits) * sizeof(chunk->alloc_map[0]);\n\tchunk->alloc_map = memblock_alloc(alloc_size, SMP_CACHE_BYTES);\n\tif (!chunk->alloc_map)\n\t\tpanic(\"%s: Failed to allocate %zu bytes\\n\", __func__,\n\t\t      alloc_size);\n\n\talloc_size =\n\t\tBITS_TO_LONGS(region_bits + 1) * sizeof(chunk->bound_map[0]);\n\tchunk->bound_map = memblock_alloc(alloc_size, SMP_CACHE_BYTES);\n\tif (!chunk->bound_map)\n\t\tpanic(\"%s: Failed to allocate %zu bytes\\n\", __func__,\n\t\t      alloc_size);\n\n\talloc_size = pcpu_chunk_nr_blocks(chunk) * sizeof(chunk->md_blocks[0]);\n\tchunk->md_blocks = memblock_alloc(alloc_size, SMP_CACHE_BYTES);\n\tif (!chunk->md_blocks)\n\t\tpanic(\"%s: Failed to allocate %zu bytes\\n\", __func__,\n\t\t      alloc_size);\n\n#ifdef CONFIG_MEMCG_KMEM\n\t \n\tchunk->obj_cgroups = NULL;\n#endif\n\tpcpu_init_md_blocks(chunk);\n\n\t \n\tchunk->immutable = true;\n\tbitmap_fill(chunk->populated, chunk->nr_pages);\n\tchunk->nr_populated = chunk->nr_pages;\n\tchunk->nr_empty_pop_pages = chunk->nr_pages;\n\n\tchunk->free_bytes = map_size;\n\n\tif (chunk->start_offset) {\n\t\t \n\t\toffset_bits = chunk->start_offset / PCPU_MIN_ALLOC_SIZE;\n\t\tbitmap_set(chunk->alloc_map, 0, offset_bits);\n\t\tset_bit(0, chunk->bound_map);\n\t\tset_bit(offset_bits, chunk->bound_map);\n\n\t\tchunk->chunk_md.first_free = offset_bits;\n\n\t\tpcpu_block_update_hint_alloc(chunk, 0, offset_bits);\n\t}\n\n\tif (chunk->end_offset) {\n\t\t \n\t\toffset_bits = chunk->end_offset / PCPU_MIN_ALLOC_SIZE;\n\t\tbitmap_set(chunk->alloc_map,\n\t\t\t   pcpu_chunk_map_bits(chunk) - offset_bits,\n\t\t\t   offset_bits);\n\t\tset_bit((start_offset + map_size) / PCPU_MIN_ALLOC_SIZE,\n\t\t\tchunk->bound_map);\n\t\tset_bit(region_bits, chunk->bound_map);\n\n\t\tpcpu_block_update_hint_alloc(chunk, pcpu_chunk_map_bits(chunk)\n\t\t\t\t\t     - offset_bits, offset_bits);\n\t}\n\n\treturn chunk;\n}\n\nstatic struct pcpu_chunk *pcpu_alloc_chunk(gfp_t gfp)\n{\n\tstruct pcpu_chunk *chunk;\n\tint region_bits;\n\n\tchunk = pcpu_mem_zalloc(pcpu_chunk_struct_size, gfp);\n\tif (!chunk)\n\t\treturn NULL;\n\n\tINIT_LIST_HEAD(&chunk->list);\n\tchunk->nr_pages = pcpu_unit_pages;\n\tregion_bits = pcpu_chunk_map_bits(chunk);\n\n\tchunk->alloc_map = pcpu_mem_zalloc(BITS_TO_LONGS(region_bits) *\n\t\t\t\t\t   sizeof(chunk->alloc_map[0]), gfp);\n\tif (!chunk->alloc_map)\n\t\tgoto alloc_map_fail;\n\n\tchunk->bound_map = pcpu_mem_zalloc(BITS_TO_LONGS(region_bits + 1) *\n\t\t\t\t\t   sizeof(chunk->bound_map[0]), gfp);\n\tif (!chunk->bound_map)\n\t\tgoto bound_map_fail;\n\n\tchunk->md_blocks = pcpu_mem_zalloc(pcpu_chunk_nr_blocks(chunk) *\n\t\t\t\t\t   sizeof(chunk->md_blocks[0]), gfp);\n\tif (!chunk->md_blocks)\n\t\tgoto md_blocks_fail;\n\n#ifdef CONFIG_MEMCG_KMEM\n\tif (!mem_cgroup_kmem_disabled()) {\n\t\tchunk->obj_cgroups =\n\t\t\tpcpu_mem_zalloc(pcpu_chunk_map_bits(chunk) *\n\t\t\t\t\tsizeof(struct obj_cgroup *), gfp);\n\t\tif (!chunk->obj_cgroups)\n\t\t\tgoto objcg_fail;\n\t}\n#endif\n\n\tpcpu_init_md_blocks(chunk);\n\n\t \n\tchunk->free_bytes = chunk->nr_pages * PAGE_SIZE;\n\n\treturn chunk;\n\n#ifdef CONFIG_MEMCG_KMEM\nobjcg_fail:\n\tpcpu_mem_free(chunk->md_blocks);\n#endif\nmd_blocks_fail:\n\tpcpu_mem_free(chunk->bound_map);\nbound_map_fail:\n\tpcpu_mem_free(chunk->alloc_map);\nalloc_map_fail:\n\tpcpu_mem_free(chunk);\n\n\treturn NULL;\n}\n\nstatic void pcpu_free_chunk(struct pcpu_chunk *chunk)\n{\n\tif (!chunk)\n\t\treturn;\n#ifdef CONFIG_MEMCG_KMEM\n\tpcpu_mem_free(chunk->obj_cgroups);\n#endif\n\tpcpu_mem_free(chunk->md_blocks);\n\tpcpu_mem_free(chunk->bound_map);\n\tpcpu_mem_free(chunk->alloc_map);\n\tpcpu_mem_free(chunk);\n}\n\n \nstatic void pcpu_chunk_populated(struct pcpu_chunk *chunk, int page_start,\n\t\t\t\t int page_end)\n{\n\tint nr = page_end - page_start;\n\n\tlockdep_assert_held(&pcpu_lock);\n\n\tbitmap_set(chunk->populated, page_start, nr);\n\tchunk->nr_populated += nr;\n\tpcpu_nr_populated += nr;\n\n\tpcpu_update_empty_pages(chunk, nr);\n}\n\n \nstatic void pcpu_chunk_depopulated(struct pcpu_chunk *chunk,\n\t\t\t\t   int page_start, int page_end)\n{\n\tint nr = page_end - page_start;\n\n\tlockdep_assert_held(&pcpu_lock);\n\n\tbitmap_clear(chunk->populated, page_start, nr);\n\tchunk->nr_populated -= nr;\n\tpcpu_nr_populated -= nr;\n\n\tpcpu_update_empty_pages(chunk, -nr);\n}\n\n \nstatic int pcpu_populate_chunk(struct pcpu_chunk *chunk,\n\t\t\t       int page_start, int page_end, gfp_t gfp);\nstatic void pcpu_depopulate_chunk(struct pcpu_chunk *chunk,\n\t\t\t\t  int page_start, int page_end);\nstatic void pcpu_post_unmap_tlb_flush(struct pcpu_chunk *chunk,\n\t\t\t\t      int page_start, int page_end);\nstatic struct pcpu_chunk *pcpu_create_chunk(gfp_t gfp);\nstatic void pcpu_destroy_chunk(struct pcpu_chunk *chunk);\nstatic struct page *pcpu_addr_to_page(void *addr);\nstatic int __init pcpu_verify_alloc_info(const struct pcpu_alloc_info *ai);\n\n#ifdef CONFIG_NEED_PER_CPU_KM\n#include \"percpu-km.c\"\n#else\n#include \"percpu-vm.c\"\n#endif\n\n \nstatic struct pcpu_chunk *pcpu_chunk_addr_search(void *addr)\n{\n\t \n\tif (pcpu_addr_in_chunk(pcpu_first_chunk, addr))\n\t\treturn pcpu_first_chunk;\n\n\t \n\tif (pcpu_addr_in_chunk(pcpu_reserved_chunk, addr))\n\t\treturn pcpu_reserved_chunk;\n\n\t \n\taddr += pcpu_unit_offsets[raw_smp_processor_id()];\n\treturn pcpu_get_page_chunk(pcpu_addr_to_page(addr));\n}\n\n#ifdef CONFIG_MEMCG_KMEM\nstatic bool pcpu_memcg_pre_alloc_hook(size_t size, gfp_t gfp,\n\t\t\t\t      struct obj_cgroup **objcgp)\n{\n\tstruct obj_cgroup *objcg;\n\n\tif (!memcg_kmem_online() || !(gfp & __GFP_ACCOUNT))\n\t\treturn true;\n\n\tobjcg = get_obj_cgroup_from_current();\n\tif (!objcg)\n\t\treturn true;\n\n\tif (obj_cgroup_charge(objcg, gfp, pcpu_obj_full_size(size))) {\n\t\tobj_cgroup_put(objcg);\n\t\treturn false;\n\t}\n\n\t*objcgp = objcg;\n\treturn true;\n}\n\nstatic void pcpu_memcg_post_alloc_hook(struct obj_cgroup *objcg,\n\t\t\t\t       struct pcpu_chunk *chunk, int off,\n\t\t\t\t       size_t size)\n{\n\tif (!objcg)\n\t\treturn;\n\n\tif (likely(chunk && chunk->obj_cgroups)) {\n\t\tchunk->obj_cgroups[off >> PCPU_MIN_ALLOC_SHIFT] = objcg;\n\n\t\trcu_read_lock();\n\t\tmod_memcg_state(obj_cgroup_memcg(objcg), MEMCG_PERCPU_B,\n\t\t\t\tpcpu_obj_full_size(size));\n\t\trcu_read_unlock();\n\t} else {\n\t\tobj_cgroup_uncharge(objcg, pcpu_obj_full_size(size));\n\t\tobj_cgroup_put(objcg);\n\t}\n}\n\nstatic void pcpu_memcg_free_hook(struct pcpu_chunk *chunk, int off, size_t size)\n{\n\tstruct obj_cgroup *objcg;\n\n\tif (unlikely(!chunk->obj_cgroups))\n\t\treturn;\n\n\tobjcg = chunk->obj_cgroups[off >> PCPU_MIN_ALLOC_SHIFT];\n\tif (!objcg)\n\t\treturn;\n\tchunk->obj_cgroups[off >> PCPU_MIN_ALLOC_SHIFT] = NULL;\n\n\tobj_cgroup_uncharge(objcg, pcpu_obj_full_size(size));\n\n\trcu_read_lock();\n\tmod_memcg_state(obj_cgroup_memcg(objcg), MEMCG_PERCPU_B,\n\t\t\t-pcpu_obj_full_size(size));\n\trcu_read_unlock();\n\n\tobj_cgroup_put(objcg);\n}\n\n#else  \nstatic bool\npcpu_memcg_pre_alloc_hook(size_t size, gfp_t gfp, struct obj_cgroup **objcgp)\n{\n\treturn true;\n}\n\nstatic void pcpu_memcg_post_alloc_hook(struct obj_cgroup *objcg,\n\t\t\t\t       struct pcpu_chunk *chunk, int off,\n\t\t\t\t       size_t size)\n{\n}\n\nstatic void pcpu_memcg_free_hook(struct pcpu_chunk *chunk, int off, size_t size)\n{\n}\n#endif  \n\n \nstatic void __percpu *pcpu_alloc(size_t size, size_t align, bool reserved,\n\t\t\t\t gfp_t gfp)\n{\n\tgfp_t pcpu_gfp;\n\tbool is_atomic;\n\tbool do_warn;\n\tstruct obj_cgroup *objcg = NULL;\n\tstatic int warn_limit = 10;\n\tstruct pcpu_chunk *chunk, *next;\n\tconst char *err;\n\tint slot, off, cpu, ret;\n\tunsigned long flags;\n\tvoid __percpu *ptr;\n\tsize_t bits, bit_align;\n\n\tgfp = current_gfp_context(gfp);\n\t \n\tpcpu_gfp = gfp & (GFP_KERNEL | __GFP_NORETRY | __GFP_NOWARN);\n\tis_atomic = (gfp & GFP_KERNEL) != GFP_KERNEL;\n\tdo_warn = !(gfp & __GFP_NOWARN);\n\n\t \n\tif (unlikely(align < PCPU_MIN_ALLOC_SIZE))\n\t\talign = PCPU_MIN_ALLOC_SIZE;\n\n\tsize = ALIGN(size, PCPU_MIN_ALLOC_SIZE);\n\tbits = size >> PCPU_MIN_ALLOC_SHIFT;\n\tbit_align = align >> PCPU_MIN_ALLOC_SHIFT;\n\n\tif (unlikely(!size || size > PCPU_MIN_UNIT_SIZE || align > PAGE_SIZE ||\n\t\t     !is_power_of_2(align))) {\n\t\tWARN(do_warn, \"illegal size (%zu) or align (%zu) for percpu allocation\\n\",\n\t\t     size, align);\n\t\treturn NULL;\n\t}\n\n\tif (unlikely(!pcpu_memcg_pre_alloc_hook(size, gfp, &objcg)))\n\t\treturn NULL;\n\n\tif (!is_atomic) {\n\t\t \n\t\tif (gfp & __GFP_NOFAIL) {\n\t\t\tmutex_lock(&pcpu_alloc_mutex);\n\t\t} else if (mutex_lock_killable(&pcpu_alloc_mutex)) {\n\t\t\tpcpu_memcg_post_alloc_hook(objcg, NULL, 0, size);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\n\tspin_lock_irqsave(&pcpu_lock, flags);\n\n\t \n\tif (reserved && pcpu_reserved_chunk) {\n\t\tchunk = pcpu_reserved_chunk;\n\n\t\toff = pcpu_find_block_fit(chunk, bits, bit_align, is_atomic);\n\t\tif (off < 0) {\n\t\t\terr = \"alloc from reserved chunk failed\";\n\t\t\tgoto fail_unlock;\n\t\t}\n\n\t\toff = pcpu_alloc_area(chunk, bits, bit_align, off);\n\t\tif (off >= 0)\n\t\t\tgoto area_found;\n\n\t\terr = \"alloc from reserved chunk failed\";\n\t\tgoto fail_unlock;\n\t}\n\nrestart:\n\t \n\tfor (slot = pcpu_size_to_slot(size); slot <= pcpu_free_slot; slot++) {\n\t\tlist_for_each_entry_safe(chunk, next, &pcpu_chunk_lists[slot],\n\t\t\t\t\t list) {\n\t\t\toff = pcpu_find_block_fit(chunk, bits, bit_align,\n\t\t\t\t\t\t  is_atomic);\n\t\t\tif (off < 0) {\n\t\t\t\tif (slot < PCPU_SLOT_FAIL_THRESHOLD)\n\t\t\t\t\tpcpu_chunk_move(chunk, 0);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\toff = pcpu_alloc_area(chunk, bits, bit_align, off);\n\t\t\tif (off >= 0) {\n\t\t\t\tpcpu_reintegrate_chunk(chunk);\n\t\t\t\tgoto area_found;\n\t\t\t}\n\t\t}\n\t}\n\n\tspin_unlock_irqrestore(&pcpu_lock, flags);\n\n\tif (is_atomic) {\n\t\terr = \"atomic alloc failed, no space left\";\n\t\tgoto fail;\n\t}\n\n\t \n\tif (list_empty(&pcpu_chunk_lists[pcpu_free_slot])) {\n\t\tchunk = pcpu_create_chunk(pcpu_gfp);\n\t\tif (!chunk) {\n\t\t\terr = \"failed to allocate new chunk\";\n\t\t\tgoto fail;\n\t\t}\n\n\t\tspin_lock_irqsave(&pcpu_lock, flags);\n\t\tpcpu_chunk_relocate(chunk, -1);\n\t} else {\n\t\tspin_lock_irqsave(&pcpu_lock, flags);\n\t}\n\n\tgoto restart;\n\narea_found:\n\tpcpu_stats_area_alloc(chunk, size);\n\tspin_unlock_irqrestore(&pcpu_lock, flags);\n\n\t \n\tif (!is_atomic) {\n\t\tunsigned int page_end, rs, re;\n\n\t\trs = PFN_DOWN(off);\n\t\tpage_end = PFN_UP(off + size);\n\n\t\tfor_each_clear_bitrange_from(rs, re, chunk->populated, page_end) {\n\t\t\tWARN_ON(chunk->immutable);\n\n\t\t\tret = pcpu_populate_chunk(chunk, rs, re, pcpu_gfp);\n\n\t\t\tspin_lock_irqsave(&pcpu_lock, flags);\n\t\t\tif (ret) {\n\t\t\t\tpcpu_free_area(chunk, off);\n\t\t\t\terr = \"failed to populate\";\n\t\t\t\tgoto fail_unlock;\n\t\t\t}\n\t\t\tpcpu_chunk_populated(chunk, rs, re);\n\t\t\tspin_unlock_irqrestore(&pcpu_lock, flags);\n\t\t}\n\n\t\tmutex_unlock(&pcpu_alloc_mutex);\n\t}\n\n\tif (pcpu_nr_empty_pop_pages < PCPU_EMPTY_POP_PAGES_LOW)\n\t\tpcpu_schedule_balance_work();\n\n\t \n\tfor_each_possible_cpu(cpu)\n\t\tmemset((void *)pcpu_chunk_addr(chunk, cpu, 0) + off, 0, size);\n\n\tptr = __addr_to_pcpu_ptr(chunk->base_addr + off);\n\tkmemleak_alloc_percpu(ptr, size, gfp);\n\n\ttrace_percpu_alloc_percpu(_RET_IP_, reserved, is_atomic, size, align,\n\t\t\t\t  chunk->base_addr, off, ptr,\n\t\t\t\t  pcpu_obj_full_size(size), gfp);\n\n\tpcpu_memcg_post_alloc_hook(objcg, chunk, off, size);\n\n\treturn ptr;\n\nfail_unlock:\n\tspin_unlock_irqrestore(&pcpu_lock, flags);\nfail:\n\ttrace_percpu_alloc_percpu_fail(reserved, is_atomic, size, align);\n\n\tif (do_warn && warn_limit) {\n\t\tpr_warn(\"allocation failed, size=%zu align=%zu atomic=%d, %s\\n\",\n\t\t\tsize, align, is_atomic, err);\n\t\tif (!is_atomic)\n\t\t\tdump_stack();\n\t\tif (!--warn_limit)\n\t\t\tpr_info(\"limit reached, disable warning\\n\");\n\t}\n\n\tif (is_atomic) {\n\t\t \n\t\tpcpu_atomic_alloc_failed = true;\n\t\tpcpu_schedule_balance_work();\n\t} else {\n\t\tmutex_unlock(&pcpu_alloc_mutex);\n\t}\n\n\tpcpu_memcg_post_alloc_hook(objcg, NULL, 0, size);\n\n\treturn NULL;\n}\n\n \nvoid __percpu *__alloc_percpu_gfp(size_t size, size_t align, gfp_t gfp)\n{\n\treturn pcpu_alloc(size, align, false, gfp);\n}\nEXPORT_SYMBOL_GPL(__alloc_percpu_gfp);\n\n \nvoid __percpu *__alloc_percpu(size_t size, size_t align)\n{\n\treturn pcpu_alloc(size, align, false, GFP_KERNEL);\n}\nEXPORT_SYMBOL_GPL(__alloc_percpu);\n\n \nvoid __percpu *__alloc_reserved_percpu(size_t size, size_t align)\n{\n\treturn pcpu_alloc(size, align, true, GFP_KERNEL);\n}\n\n \nstatic void pcpu_balance_free(bool empty_only)\n{\n\tLIST_HEAD(to_free);\n\tstruct list_head *free_head = &pcpu_chunk_lists[pcpu_free_slot];\n\tstruct pcpu_chunk *chunk, *next;\n\n\tlockdep_assert_held(&pcpu_lock);\n\n\t \n\tlist_for_each_entry_safe(chunk, next, free_head, list) {\n\t\tWARN_ON(chunk->immutable);\n\n\t\t \n\t\tif (chunk == list_first_entry(free_head, struct pcpu_chunk, list))\n\t\t\tcontinue;\n\n\t\tif (!empty_only || chunk->nr_empty_pop_pages == 0)\n\t\t\tlist_move(&chunk->list, &to_free);\n\t}\n\n\tif (list_empty(&to_free))\n\t\treturn;\n\n\tspin_unlock_irq(&pcpu_lock);\n\tlist_for_each_entry_safe(chunk, next, &to_free, list) {\n\t\tunsigned int rs, re;\n\n\t\tfor_each_set_bitrange(rs, re, chunk->populated, chunk->nr_pages) {\n\t\t\tpcpu_depopulate_chunk(chunk, rs, re);\n\t\t\tspin_lock_irq(&pcpu_lock);\n\t\t\tpcpu_chunk_depopulated(chunk, rs, re);\n\t\t\tspin_unlock_irq(&pcpu_lock);\n\t\t}\n\t\tpcpu_destroy_chunk(chunk);\n\t\tcond_resched();\n\t}\n\tspin_lock_irq(&pcpu_lock);\n}\n\n \nstatic void pcpu_balance_populated(void)\n{\n\t \n\tconst gfp_t gfp = GFP_KERNEL | __GFP_NORETRY | __GFP_NOWARN;\n\tstruct pcpu_chunk *chunk;\n\tint slot, nr_to_pop, ret;\n\n\tlockdep_assert_held(&pcpu_lock);\n\n\t \nretry_pop:\n\tif (pcpu_atomic_alloc_failed) {\n\t\tnr_to_pop = PCPU_EMPTY_POP_PAGES_HIGH;\n\t\t \n\t\tpcpu_atomic_alloc_failed = false;\n\t} else {\n\t\tnr_to_pop = clamp(PCPU_EMPTY_POP_PAGES_HIGH -\n\t\t\t\t  pcpu_nr_empty_pop_pages,\n\t\t\t\t  0, PCPU_EMPTY_POP_PAGES_HIGH);\n\t}\n\n\tfor (slot = pcpu_size_to_slot(PAGE_SIZE); slot <= pcpu_free_slot; slot++) {\n\t\tunsigned int nr_unpop = 0, rs, re;\n\n\t\tif (!nr_to_pop)\n\t\t\tbreak;\n\n\t\tlist_for_each_entry(chunk, &pcpu_chunk_lists[slot], list) {\n\t\t\tnr_unpop = chunk->nr_pages - chunk->nr_populated;\n\t\t\tif (nr_unpop)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (!nr_unpop)\n\t\t\tcontinue;\n\n\t\t \n\t\tfor_each_clear_bitrange(rs, re, chunk->populated, chunk->nr_pages) {\n\t\t\tint nr = min_t(int, re - rs, nr_to_pop);\n\n\t\t\tspin_unlock_irq(&pcpu_lock);\n\t\t\tret = pcpu_populate_chunk(chunk, rs, rs + nr, gfp);\n\t\t\tcond_resched();\n\t\t\tspin_lock_irq(&pcpu_lock);\n\t\t\tif (!ret) {\n\t\t\t\tnr_to_pop -= nr;\n\t\t\t\tpcpu_chunk_populated(chunk, rs, rs + nr);\n\t\t\t} else {\n\t\t\t\tnr_to_pop = 0;\n\t\t\t}\n\n\t\t\tif (!nr_to_pop)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (nr_to_pop) {\n\t\t \n\t\tspin_unlock_irq(&pcpu_lock);\n\t\tchunk = pcpu_create_chunk(gfp);\n\t\tcond_resched();\n\t\tspin_lock_irq(&pcpu_lock);\n\t\tif (chunk) {\n\t\t\tpcpu_chunk_relocate(chunk, -1);\n\t\t\tgoto retry_pop;\n\t\t}\n\t}\n}\n\n \nstatic void pcpu_reclaim_populated(void)\n{\n\tstruct pcpu_chunk *chunk;\n\tstruct pcpu_block_md *block;\n\tint freed_page_start, freed_page_end;\n\tint i, end;\n\tbool reintegrate;\n\n\tlockdep_assert_held(&pcpu_lock);\n\n\t \n\twhile ((chunk = list_first_entry_or_null(\n\t\t\t&pcpu_chunk_lists[pcpu_to_depopulate_slot],\n\t\t\tstruct pcpu_chunk, list))) {\n\t\tWARN_ON(chunk->immutable);\n\n\t\t \n\t\tfreed_page_start = chunk->nr_pages;\n\t\tfreed_page_end = 0;\n\t\treintegrate = false;\n\t\tfor (i = chunk->nr_pages - 1, end = -1; i >= 0; i--) {\n\t\t\t \n\t\t\tif (chunk->nr_empty_pop_pages == 0)\n\t\t\t\tbreak;\n\n\t\t\t \n\t\t\tif (pcpu_nr_empty_pop_pages < PCPU_EMPTY_POP_PAGES_HIGH) {\n\t\t\t\treintegrate = true;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t \n\t\t\tblock = chunk->md_blocks + i;\n\t\t\tif (block->contig_hint == PCPU_BITMAP_BLOCK_BITS &&\n\t\t\t    test_bit(i, chunk->populated)) {\n\t\t\t\tif (end == -1)\n\t\t\t\t\tend = i;\n\t\t\t\tif (i > 0)\n\t\t\t\t\tcontinue;\n\t\t\t\ti--;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (end == -1)\n\t\t\t\tcontinue;\n\n\t\t\tspin_unlock_irq(&pcpu_lock);\n\t\t\tpcpu_depopulate_chunk(chunk, i + 1, end + 1);\n\t\t\tcond_resched();\n\t\t\tspin_lock_irq(&pcpu_lock);\n\n\t\t\tpcpu_chunk_depopulated(chunk, i + 1, end + 1);\n\t\t\tfreed_page_start = min(freed_page_start, i + 1);\n\t\t\tfreed_page_end = max(freed_page_end, end + 1);\n\n\t\t\t \n\t\t\tend = -1;\n\t\t}\n\n\t\t \n\t\tif (freed_page_start < freed_page_end) {\n\t\t\tspin_unlock_irq(&pcpu_lock);\n\t\t\tpcpu_post_unmap_tlb_flush(chunk,\n\t\t\t\t\t\t  freed_page_start,\n\t\t\t\t\t\t  freed_page_end);\n\t\t\tcond_resched();\n\t\t\tspin_lock_irq(&pcpu_lock);\n\t\t}\n\n\t\tif (reintegrate || chunk->free_bytes == pcpu_unit_size)\n\t\t\tpcpu_reintegrate_chunk(chunk);\n\t\telse\n\t\t\tlist_move_tail(&chunk->list,\n\t\t\t\t       &pcpu_chunk_lists[pcpu_sidelined_slot]);\n\t}\n}\n\n \nstatic void pcpu_balance_workfn(struct work_struct *work)\n{\n\t \n\tmutex_lock(&pcpu_alloc_mutex);\n\tspin_lock_irq(&pcpu_lock);\n\n\tpcpu_balance_free(false);\n\tpcpu_reclaim_populated();\n\tpcpu_balance_populated();\n\tpcpu_balance_free(true);\n\n\tspin_unlock_irq(&pcpu_lock);\n\tmutex_unlock(&pcpu_alloc_mutex);\n}\n\n \nvoid free_percpu(void __percpu *ptr)\n{\n\tvoid *addr;\n\tstruct pcpu_chunk *chunk;\n\tunsigned long flags;\n\tint size, off;\n\tbool need_balance = false;\n\n\tif (!ptr)\n\t\treturn;\n\n\tkmemleak_free_percpu(ptr);\n\n\taddr = __pcpu_ptr_to_addr(ptr);\n\n\tspin_lock_irqsave(&pcpu_lock, flags);\n\n\tchunk = pcpu_chunk_addr_search(addr);\n\toff = addr - chunk->base_addr;\n\n\tsize = pcpu_free_area(chunk, off);\n\n\tpcpu_memcg_free_hook(chunk, off, size);\n\n\t \n\tif (!chunk->isolated && chunk->free_bytes == pcpu_unit_size) {\n\t\tstruct pcpu_chunk *pos;\n\n\t\tlist_for_each_entry(pos, &pcpu_chunk_lists[pcpu_free_slot], list)\n\t\t\tif (pos != chunk) {\n\t\t\t\tneed_balance = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t} else if (pcpu_should_reclaim_chunk(chunk)) {\n\t\tpcpu_isolate_chunk(chunk);\n\t\tneed_balance = true;\n\t}\n\n\ttrace_percpu_free_percpu(chunk->base_addr, off, ptr);\n\n\tspin_unlock_irqrestore(&pcpu_lock, flags);\n\n\tif (need_balance)\n\t\tpcpu_schedule_balance_work();\n}\nEXPORT_SYMBOL_GPL(free_percpu);\n\nbool __is_kernel_percpu_address(unsigned long addr, unsigned long *can_addr)\n{\n#ifdef CONFIG_SMP\n\tconst size_t static_size = __per_cpu_end - __per_cpu_start;\n\tvoid __percpu *base = __addr_to_pcpu_ptr(pcpu_base_addr);\n\tunsigned int cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tvoid *start = per_cpu_ptr(base, cpu);\n\t\tvoid *va = (void *)addr;\n\n\t\tif (va >= start && va < start + static_size) {\n\t\t\tif (can_addr) {\n\t\t\t\t*can_addr = (unsigned long) (va - start);\n\t\t\t\t*can_addr += (unsigned long)\n\t\t\t\t\tper_cpu_ptr(base, get_boot_cpu_id());\n\t\t\t}\n\t\t\treturn true;\n\t\t}\n\t}\n#endif\n\t \n\treturn false;\n}\n\n \nbool is_kernel_percpu_address(unsigned long addr)\n{\n\treturn __is_kernel_percpu_address(addr, NULL);\n}\n\n \nphys_addr_t per_cpu_ptr_to_phys(void *addr)\n{\n\tvoid __percpu *base = __addr_to_pcpu_ptr(pcpu_base_addr);\n\tbool in_first_chunk = false;\n\tunsigned long first_low, first_high;\n\tunsigned int cpu;\n\n\t \n\tfirst_low = (unsigned long)pcpu_base_addr +\n\t\t    pcpu_unit_page_offset(pcpu_low_unit_cpu, 0);\n\tfirst_high = (unsigned long)pcpu_base_addr +\n\t\t     pcpu_unit_page_offset(pcpu_high_unit_cpu, pcpu_unit_pages);\n\tif ((unsigned long)addr >= first_low &&\n\t    (unsigned long)addr < first_high) {\n\t\tfor_each_possible_cpu(cpu) {\n\t\t\tvoid *start = per_cpu_ptr(base, cpu);\n\n\t\t\tif (addr >= start && addr < start + pcpu_unit_size) {\n\t\t\t\tin_first_chunk = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (in_first_chunk) {\n\t\tif (!is_vmalloc_addr(addr))\n\t\t\treturn __pa(addr);\n\t\telse\n\t\t\treturn page_to_phys(vmalloc_to_page(addr)) +\n\t\t\t       offset_in_page(addr);\n\t} else\n\t\treturn page_to_phys(pcpu_addr_to_page(addr)) +\n\t\t       offset_in_page(addr);\n}\n\n \nstruct pcpu_alloc_info * __init pcpu_alloc_alloc_info(int nr_groups,\n\t\t\t\t\t\t      int nr_units)\n{\n\tstruct pcpu_alloc_info *ai;\n\tsize_t base_size, ai_size;\n\tvoid *ptr;\n\tint unit;\n\n\tbase_size = ALIGN(struct_size(ai, groups, nr_groups),\n\t\t\t  __alignof__(ai->groups[0].cpu_map[0]));\n\tai_size = base_size + nr_units * sizeof(ai->groups[0].cpu_map[0]);\n\n\tptr = memblock_alloc(PFN_ALIGN(ai_size), PAGE_SIZE);\n\tif (!ptr)\n\t\treturn NULL;\n\tai = ptr;\n\tptr += base_size;\n\n\tai->groups[0].cpu_map = ptr;\n\n\tfor (unit = 0; unit < nr_units; unit++)\n\t\tai->groups[0].cpu_map[unit] = NR_CPUS;\n\n\tai->nr_groups = nr_groups;\n\tai->__ai_size = PFN_ALIGN(ai_size);\n\n\treturn ai;\n}\n\n \nvoid __init pcpu_free_alloc_info(struct pcpu_alloc_info *ai)\n{\n\tmemblock_free(ai, ai->__ai_size);\n}\n\n \nstatic void pcpu_dump_alloc_info(const char *lvl,\n\t\t\t\t const struct pcpu_alloc_info *ai)\n{\n\tint group_width = 1, cpu_width = 1, width;\n\tchar empty_str[] = \"--------\";\n\tint alloc = 0, alloc_end = 0;\n\tint group, v;\n\tint upa, apl;\t \n\n\tv = ai->nr_groups;\n\twhile (v /= 10)\n\t\tgroup_width++;\n\n\tv = num_possible_cpus();\n\twhile (v /= 10)\n\t\tcpu_width++;\n\tempty_str[min_t(int, cpu_width, sizeof(empty_str) - 1)] = '\\0';\n\n\tupa = ai->alloc_size / ai->unit_size;\n\twidth = upa * (cpu_width + 1) + group_width + 3;\n\tapl = rounddown_pow_of_two(max(60 / width, 1));\n\n\tprintk(\"%spcpu-alloc: s%zu r%zu d%zu u%zu alloc=%zu*%zu\",\n\t       lvl, ai->static_size, ai->reserved_size, ai->dyn_size,\n\t       ai->unit_size, ai->alloc_size / ai->atom_size, ai->atom_size);\n\n\tfor (group = 0; group < ai->nr_groups; group++) {\n\t\tconst struct pcpu_group_info *gi = &ai->groups[group];\n\t\tint unit = 0, unit_end = 0;\n\n\t\tBUG_ON(gi->nr_units % upa);\n\t\tfor (alloc_end += gi->nr_units / upa;\n\t\t     alloc < alloc_end; alloc++) {\n\t\t\tif (!(alloc % apl)) {\n\t\t\t\tpr_cont(\"\\n\");\n\t\t\t\tprintk(\"%spcpu-alloc: \", lvl);\n\t\t\t}\n\t\t\tpr_cont(\"[%0*d] \", group_width, group);\n\n\t\t\tfor (unit_end += upa; unit < unit_end; unit++)\n\t\t\t\tif (gi->cpu_map[unit] != NR_CPUS)\n\t\t\t\t\tpr_cont(\"%0*d \",\n\t\t\t\t\t\tcpu_width, gi->cpu_map[unit]);\n\t\t\t\telse\n\t\t\t\t\tpr_cont(\"%s \", empty_str);\n\t\t}\n\t}\n\tpr_cont(\"\\n\");\n}\n\n \nvoid __init pcpu_setup_first_chunk(const struct pcpu_alloc_info *ai,\n\t\t\t\t   void *base_addr)\n{\n\tsize_t size_sum = ai->static_size + ai->reserved_size + ai->dyn_size;\n\tsize_t static_size, dyn_size;\n\tunsigned long *group_offsets;\n\tsize_t *group_sizes;\n\tunsigned long *unit_off;\n\tunsigned int cpu;\n\tint *unit_map;\n\tint group, unit, i;\n\tunsigned long tmp_addr;\n\tsize_t alloc_size;\n\n#define PCPU_SETUP_BUG_ON(cond)\tdo {\t\t\t\t\t\\\n\tif (unlikely(cond)) {\t\t\t\t\t\t\\\n\t\tpr_emerg(\"failed to initialize, %s\\n\", #cond);\t\t\\\n\t\tpr_emerg(\"cpu_possible_mask=%*pb\\n\",\t\t\t\\\n\t\t\t cpumask_pr_args(cpu_possible_mask));\t\t\\\n\t\tpcpu_dump_alloc_info(KERN_EMERG, ai);\t\t\t\\\n\t\tBUG();\t\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (0)\n\n\t \n\tPCPU_SETUP_BUG_ON(ai->nr_groups <= 0);\n#ifdef CONFIG_SMP\n\tPCPU_SETUP_BUG_ON(!ai->static_size);\n\tPCPU_SETUP_BUG_ON(offset_in_page(__per_cpu_start));\n#endif\n\tPCPU_SETUP_BUG_ON(!base_addr);\n\tPCPU_SETUP_BUG_ON(offset_in_page(base_addr));\n\tPCPU_SETUP_BUG_ON(ai->unit_size < size_sum);\n\tPCPU_SETUP_BUG_ON(offset_in_page(ai->unit_size));\n\tPCPU_SETUP_BUG_ON(ai->unit_size < PCPU_MIN_UNIT_SIZE);\n\tPCPU_SETUP_BUG_ON(!IS_ALIGNED(ai->unit_size, PCPU_BITMAP_BLOCK_SIZE));\n\tPCPU_SETUP_BUG_ON(ai->dyn_size < PERCPU_DYNAMIC_EARLY_SIZE);\n\tPCPU_SETUP_BUG_ON(!IS_ALIGNED(ai->reserved_size, PCPU_MIN_ALLOC_SIZE));\n\tPCPU_SETUP_BUG_ON(!(IS_ALIGNED(PCPU_BITMAP_BLOCK_SIZE, PAGE_SIZE) ||\n\t\t\t    IS_ALIGNED(PAGE_SIZE, PCPU_BITMAP_BLOCK_SIZE)));\n\tPCPU_SETUP_BUG_ON(pcpu_verify_alloc_info(ai) < 0);\n\n\t \n\talloc_size = ai->nr_groups * sizeof(group_offsets[0]);\n\tgroup_offsets = memblock_alloc(alloc_size, SMP_CACHE_BYTES);\n\tif (!group_offsets)\n\t\tpanic(\"%s: Failed to allocate %zu bytes\\n\", __func__,\n\t\t      alloc_size);\n\n\talloc_size = ai->nr_groups * sizeof(group_sizes[0]);\n\tgroup_sizes = memblock_alloc(alloc_size, SMP_CACHE_BYTES);\n\tif (!group_sizes)\n\t\tpanic(\"%s: Failed to allocate %zu bytes\\n\", __func__,\n\t\t      alloc_size);\n\n\talloc_size = nr_cpu_ids * sizeof(unit_map[0]);\n\tunit_map = memblock_alloc(alloc_size, SMP_CACHE_BYTES);\n\tif (!unit_map)\n\t\tpanic(\"%s: Failed to allocate %zu bytes\\n\", __func__,\n\t\t      alloc_size);\n\n\talloc_size = nr_cpu_ids * sizeof(unit_off[0]);\n\tunit_off = memblock_alloc(alloc_size, SMP_CACHE_BYTES);\n\tif (!unit_off)\n\t\tpanic(\"%s: Failed to allocate %zu bytes\\n\", __func__,\n\t\t      alloc_size);\n\n\tfor (cpu = 0; cpu < nr_cpu_ids; cpu++)\n\t\tunit_map[cpu] = UINT_MAX;\n\n\tpcpu_low_unit_cpu = NR_CPUS;\n\tpcpu_high_unit_cpu = NR_CPUS;\n\n\tfor (group = 0, unit = 0; group < ai->nr_groups; group++, unit += i) {\n\t\tconst struct pcpu_group_info *gi = &ai->groups[group];\n\n\t\tgroup_offsets[group] = gi->base_offset;\n\t\tgroup_sizes[group] = gi->nr_units * ai->unit_size;\n\n\t\tfor (i = 0; i < gi->nr_units; i++) {\n\t\t\tcpu = gi->cpu_map[i];\n\t\t\tif (cpu == NR_CPUS)\n\t\t\t\tcontinue;\n\n\t\t\tPCPU_SETUP_BUG_ON(cpu >= nr_cpu_ids);\n\t\t\tPCPU_SETUP_BUG_ON(!cpu_possible(cpu));\n\t\t\tPCPU_SETUP_BUG_ON(unit_map[cpu] != UINT_MAX);\n\n\t\t\tunit_map[cpu] = unit + i;\n\t\t\tunit_off[cpu] = gi->base_offset + i * ai->unit_size;\n\n\t\t\t \n\t\t\tif (pcpu_low_unit_cpu == NR_CPUS ||\n\t\t\t    unit_off[cpu] < unit_off[pcpu_low_unit_cpu])\n\t\t\t\tpcpu_low_unit_cpu = cpu;\n\t\t\tif (pcpu_high_unit_cpu == NR_CPUS ||\n\t\t\t    unit_off[cpu] > unit_off[pcpu_high_unit_cpu])\n\t\t\t\tpcpu_high_unit_cpu = cpu;\n\t\t}\n\t}\n\tpcpu_nr_units = unit;\n\n\tfor_each_possible_cpu(cpu)\n\t\tPCPU_SETUP_BUG_ON(unit_map[cpu] == UINT_MAX);\n\n\t \n#undef PCPU_SETUP_BUG_ON\n\tpcpu_dump_alloc_info(KERN_DEBUG, ai);\n\n\tpcpu_nr_groups = ai->nr_groups;\n\tpcpu_group_offsets = group_offsets;\n\tpcpu_group_sizes = group_sizes;\n\tpcpu_unit_map = unit_map;\n\tpcpu_unit_offsets = unit_off;\n\n\t \n\tpcpu_unit_pages = ai->unit_size >> PAGE_SHIFT;\n\tpcpu_unit_size = pcpu_unit_pages << PAGE_SHIFT;\n\tpcpu_atom_size = ai->atom_size;\n\tpcpu_chunk_struct_size = struct_size((struct pcpu_chunk *)0, populated,\n\t\t\t\t\t     BITS_TO_LONGS(pcpu_unit_pages));\n\n\tpcpu_stats_save_ai(ai);\n\n\t \n\tpcpu_sidelined_slot = __pcpu_size_to_slot(pcpu_unit_size) + 1;\n\tpcpu_free_slot = pcpu_sidelined_slot + 1;\n\tpcpu_to_depopulate_slot = pcpu_free_slot + 1;\n\tpcpu_nr_slots = pcpu_to_depopulate_slot + 1;\n\tpcpu_chunk_lists = memblock_alloc(pcpu_nr_slots *\n\t\t\t\t\t  sizeof(pcpu_chunk_lists[0]),\n\t\t\t\t\t  SMP_CACHE_BYTES);\n\tif (!pcpu_chunk_lists)\n\t\tpanic(\"%s: Failed to allocate %zu bytes\\n\", __func__,\n\t\t      pcpu_nr_slots * sizeof(pcpu_chunk_lists[0]));\n\n\tfor (i = 0; i < pcpu_nr_slots; i++)\n\t\tINIT_LIST_HEAD(&pcpu_chunk_lists[i]);\n\n\t \n\tstatic_size = ALIGN(ai->static_size, PCPU_MIN_ALLOC_SIZE);\n\tdyn_size = ai->dyn_size - (static_size - ai->static_size);\n\n\t \n\ttmp_addr = (unsigned long)base_addr + static_size;\n\tif (ai->reserved_size)\n\t\tpcpu_reserved_chunk = pcpu_alloc_first_chunk(tmp_addr,\n\t\t\t\t\t\tai->reserved_size);\n\ttmp_addr = (unsigned long)base_addr + static_size + ai->reserved_size;\n\tpcpu_first_chunk = pcpu_alloc_first_chunk(tmp_addr, dyn_size);\n\n\tpcpu_nr_empty_pop_pages = pcpu_first_chunk->nr_empty_pop_pages;\n\tpcpu_chunk_relocate(pcpu_first_chunk, -1);\n\n\t \n\tpcpu_nr_populated += PFN_DOWN(size_sum);\n\n\tpcpu_stats_chunk_alloc();\n\ttrace_percpu_create_chunk(base_addr);\n\n\t \n\tpcpu_base_addr = base_addr;\n}\n\n#ifdef CONFIG_SMP\n\nconst char * const pcpu_fc_names[PCPU_FC_NR] __initconst = {\n\t[PCPU_FC_AUTO]\t= \"auto\",\n\t[PCPU_FC_EMBED]\t= \"embed\",\n\t[PCPU_FC_PAGE]\t= \"page\",\n};\n\nenum pcpu_fc pcpu_chosen_fc __initdata = PCPU_FC_AUTO;\n\nstatic int __init percpu_alloc_setup(char *str)\n{\n\tif (!str)\n\t\treturn -EINVAL;\n\n\tif (0)\n\t\t ;\n#ifdef CONFIG_NEED_PER_CPU_EMBED_FIRST_CHUNK\n\telse if (!strcmp(str, \"embed\"))\n\t\tpcpu_chosen_fc = PCPU_FC_EMBED;\n#endif\n#ifdef CONFIG_NEED_PER_CPU_PAGE_FIRST_CHUNK\n\telse if (!strcmp(str, \"page\"))\n\t\tpcpu_chosen_fc = PCPU_FC_PAGE;\n#endif\n\telse\n\t\tpr_warn(\"unknown allocator %s specified\\n\", str);\n\n\treturn 0;\n}\nearly_param(\"percpu_alloc\", percpu_alloc_setup);\n\n \n#if defined(CONFIG_NEED_PER_CPU_EMBED_FIRST_CHUNK) || \\\n\t!defined(CONFIG_HAVE_SETUP_PER_CPU_AREA)\n#define BUILD_EMBED_FIRST_CHUNK\n#endif\n\n \n#if defined(CONFIG_NEED_PER_CPU_PAGE_FIRST_CHUNK)\n#define BUILD_PAGE_FIRST_CHUNK\n#endif\n\n \n#if defined(BUILD_EMBED_FIRST_CHUNK) || defined(BUILD_PAGE_FIRST_CHUNK)\n \nstatic struct pcpu_alloc_info * __init __flatten pcpu_build_alloc_info(\n\t\t\t\tsize_t reserved_size, size_t dyn_size,\n\t\t\t\tsize_t atom_size,\n\t\t\t\tpcpu_fc_cpu_distance_fn_t cpu_distance_fn)\n{\n\tstatic int group_map[NR_CPUS] __initdata;\n\tstatic int group_cnt[NR_CPUS] __initdata;\n\tstatic struct cpumask mask __initdata;\n\tconst size_t static_size = __per_cpu_end - __per_cpu_start;\n\tint nr_groups = 1, nr_units = 0;\n\tsize_t size_sum, min_unit_size, alloc_size;\n\tint upa, max_upa, best_upa;\t \n\tint last_allocs, group, unit;\n\tunsigned int cpu, tcpu;\n\tstruct pcpu_alloc_info *ai;\n\tunsigned int *cpu_map;\n\n\t \n\tmemset(group_map, 0, sizeof(group_map));\n\tmemset(group_cnt, 0, sizeof(group_cnt));\n\tcpumask_clear(&mask);\n\n\t \n\tsize_sum = PFN_ALIGN(static_size + reserved_size +\n\t\t\t    max_t(size_t, dyn_size, PERCPU_DYNAMIC_EARLY_SIZE));\n\tdyn_size = size_sum - static_size - reserved_size;\n\n\t \n\tmin_unit_size = max_t(size_t, size_sum, PCPU_MIN_UNIT_SIZE);\n\n\t \n\talloc_size = roundup(min_unit_size, atom_size);\n\tupa = alloc_size / min_unit_size;\n\twhile (alloc_size % upa || (offset_in_page(alloc_size / upa)))\n\t\tupa--;\n\tmax_upa = upa;\n\n\tcpumask_copy(&mask, cpu_possible_mask);\n\n\t \n\tfor (group = 0; !cpumask_empty(&mask); group++) {\n\t\t \n\t\tcpu = cpumask_first(&mask);\n\t\tgroup_map[cpu] = group;\n\t\tgroup_cnt[group]++;\n\t\tcpumask_clear_cpu(cpu, &mask);\n\n\t\tfor_each_cpu(tcpu, &mask) {\n\t\t\tif (!cpu_distance_fn ||\n\t\t\t    (cpu_distance_fn(cpu, tcpu) == LOCAL_DISTANCE &&\n\t\t\t     cpu_distance_fn(tcpu, cpu) == LOCAL_DISTANCE)) {\n\t\t\t\tgroup_map[tcpu] = group;\n\t\t\t\tgroup_cnt[group]++;\n\t\t\t\tcpumask_clear_cpu(tcpu, &mask);\n\t\t\t}\n\t\t}\n\t}\n\tnr_groups = group;\n\n\t \n\tlast_allocs = INT_MAX;\n\tbest_upa = 0;\n\tfor (upa = max_upa; upa; upa--) {\n\t\tint allocs = 0, wasted = 0;\n\n\t\tif (alloc_size % upa || (offset_in_page(alloc_size / upa)))\n\t\t\tcontinue;\n\n\t\tfor (group = 0; group < nr_groups; group++) {\n\t\t\tint this_allocs = DIV_ROUND_UP(group_cnt[group], upa);\n\t\t\tallocs += this_allocs;\n\t\t\twasted += this_allocs * upa - group_cnt[group];\n\t\t}\n\n\t\t \n\t\tif (wasted > num_possible_cpus() / 3)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (allocs > last_allocs)\n\t\t\tbreak;\n\t\tlast_allocs = allocs;\n\t\tbest_upa = upa;\n\t}\n\tBUG_ON(!best_upa);\n\tupa = best_upa;\n\n\t \n\tfor (group = 0; group < nr_groups; group++)\n\t\tnr_units += roundup(group_cnt[group], upa);\n\n\tai = pcpu_alloc_alloc_info(nr_groups, nr_units);\n\tif (!ai)\n\t\treturn ERR_PTR(-ENOMEM);\n\tcpu_map = ai->groups[0].cpu_map;\n\n\tfor (group = 0; group < nr_groups; group++) {\n\t\tai->groups[group].cpu_map = cpu_map;\n\t\tcpu_map += roundup(group_cnt[group], upa);\n\t}\n\n\tai->static_size = static_size;\n\tai->reserved_size = reserved_size;\n\tai->dyn_size = dyn_size;\n\tai->unit_size = alloc_size / upa;\n\tai->atom_size = atom_size;\n\tai->alloc_size = alloc_size;\n\n\tfor (group = 0, unit = 0; group < nr_groups; group++) {\n\t\tstruct pcpu_group_info *gi = &ai->groups[group];\n\n\t\t \n\t\tgi->base_offset = unit * ai->unit_size;\n\n\t\tfor_each_possible_cpu(cpu)\n\t\t\tif (group_map[cpu] == group)\n\t\t\t\tgi->cpu_map[gi->nr_units++] = cpu;\n\t\tgi->nr_units = roundup(gi->nr_units, upa);\n\t\tunit += gi->nr_units;\n\t}\n\tBUG_ON(unit != nr_units);\n\n\treturn ai;\n}\n\nstatic void * __init pcpu_fc_alloc(unsigned int cpu, size_t size, size_t align,\n\t\t\t\t   pcpu_fc_cpu_to_node_fn_t cpu_to_nd_fn)\n{\n\tconst unsigned long goal = __pa(MAX_DMA_ADDRESS);\n#ifdef CONFIG_NUMA\n\tint node = NUMA_NO_NODE;\n\tvoid *ptr;\n\n\tif (cpu_to_nd_fn)\n\t\tnode = cpu_to_nd_fn(cpu);\n\n\tif (node == NUMA_NO_NODE || !node_online(node) || !NODE_DATA(node)) {\n\t\tptr = memblock_alloc_from(size, align, goal);\n\t\tpr_info(\"cpu %d has no node %d or node-local memory\\n\",\n\t\t\tcpu, node);\n\t\tpr_debug(\"per cpu data for cpu%d %zu bytes at 0x%llx\\n\",\n\t\t\t cpu, size, (u64)__pa(ptr));\n\t} else {\n\t\tptr = memblock_alloc_try_nid(size, align, goal,\n\t\t\t\t\t     MEMBLOCK_ALLOC_ACCESSIBLE,\n\t\t\t\t\t     node);\n\n\t\tpr_debug(\"per cpu data for cpu%d %zu bytes on node%d at 0x%llx\\n\",\n\t\t\t cpu, size, node, (u64)__pa(ptr));\n\t}\n\treturn ptr;\n#else\n\treturn memblock_alloc_from(size, align, goal);\n#endif\n}\n\nstatic void __init pcpu_fc_free(void *ptr, size_t size)\n{\n\tmemblock_free(ptr, size);\n}\n#endif  \n\n#if defined(BUILD_EMBED_FIRST_CHUNK)\n \nint __init pcpu_embed_first_chunk(size_t reserved_size, size_t dyn_size,\n\t\t\t\t  size_t atom_size,\n\t\t\t\t  pcpu_fc_cpu_distance_fn_t cpu_distance_fn,\n\t\t\t\t  pcpu_fc_cpu_to_node_fn_t cpu_to_nd_fn)\n{\n\tvoid *base = (void *)ULONG_MAX;\n\tvoid **areas = NULL;\n\tstruct pcpu_alloc_info *ai;\n\tsize_t size_sum, areas_size;\n\tunsigned long max_distance;\n\tint group, i, highest_group, rc = 0;\n\n\tai = pcpu_build_alloc_info(reserved_size, dyn_size, atom_size,\n\t\t\t\t   cpu_distance_fn);\n\tif (IS_ERR(ai))\n\t\treturn PTR_ERR(ai);\n\n\tsize_sum = ai->static_size + ai->reserved_size + ai->dyn_size;\n\tareas_size = PFN_ALIGN(ai->nr_groups * sizeof(void *));\n\n\tareas = memblock_alloc(areas_size, SMP_CACHE_BYTES);\n\tif (!areas) {\n\t\trc = -ENOMEM;\n\t\tgoto out_free;\n\t}\n\n\t \n\thighest_group = 0;\n\tfor (group = 0; group < ai->nr_groups; group++) {\n\t\tstruct pcpu_group_info *gi = &ai->groups[group];\n\t\tunsigned int cpu = NR_CPUS;\n\t\tvoid *ptr;\n\n\t\tfor (i = 0; i < gi->nr_units && cpu == NR_CPUS; i++)\n\t\t\tcpu = gi->cpu_map[i];\n\t\tBUG_ON(cpu == NR_CPUS);\n\n\t\t \n\t\tptr = pcpu_fc_alloc(cpu, gi->nr_units * ai->unit_size, atom_size, cpu_to_nd_fn);\n\t\tif (!ptr) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto out_free_areas;\n\t\t}\n\t\t \n\t\tkmemleak_ignore_phys(__pa(ptr));\n\t\tareas[group] = ptr;\n\n\t\tbase = min(ptr, base);\n\t\tif (ptr > areas[highest_group])\n\t\t\thighest_group = group;\n\t}\n\tmax_distance = areas[highest_group] - base;\n\tmax_distance += ai->unit_size * ai->groups[highest_group].nr_units;\n\n\t \n\tif (max_distance > VMALLOC_TOTAL * 3 / 4) {\n\t\tpr_warn(\"max_distance=0x%lx too large for vmalloc space 0x%lx\\n\",\n\t\t\t\tmax_distance, VMALLOC_TOTAL);\n#ifdef CONFIG_NEED_PER_CPU_PAGE_FIRST_CHUNK\n\t\t \n\t\trc = -EINVAL;\n\t\tgoto out_free_areas;\n#endif\n\t}\n\n\t \n\tfor (group = 0; group < ai->nr_groups; group++) {\n\t\tstruct pcpu_group_info *gi = &ai->groups[group];\n\t\tvoid *ptr = areas[group];\n\n\t\tfor (i = 0; i < gi->nr_units; i++, ptr += ai->unit_size) {\n\t\t\tif (gi->cpu_map[i] == NR_CPUS) {\n\t\t\t\t \n\t\t\t\tpcpu_fc_free(ptr, ai->unit_size);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\t \n\t\t\tmemcpy(ptr, __per_cpu_load, ai->static_size);\n\t\t\tpcpu_fc_free(ptr + size_sum, ai->unit_size - size_sum);\n\t\t}\n\t}\n\n\t \n\tfor (group = 0; group < ai->nr_groups; group++) {\n\t\tai->groups[group].base_offset = areas[group] - base;\n\t}\n\n\tpr_info(\"Embedded %zu pages/cpu s%zu r%zu d%zu u%zu\\n\",\n\t\tPFN_DOWN(size_sum), ai->static_size, ai->reserved_size,\n\t\tai->dyn_size, ai->unit_size);\n\n\tpcpu_setup_first_chunk(ai, base);\n\tgoto out_free;\n\nout_free_areas:\n\tfor (group = 0; group < ai->nr_groups; group++)\n\t\tif (areas[group])\n\t\t\tpcpu_fc_free(areas[group],\n\t\t\t\tai->groups[group].nr_units * ai->unit_size);\nout_free:\n\tpcpu_free_alloc_info(ai);\n\tif (areas)\n\t\tmemblock_free(areas, areas_size);\n\treturn rc;\n}\n#endif  \n\n#ifdef BUILD_PAGE_FIRST_CHUNK\n#include <asm/pgalloc.h>\n\n#ifndef P4D_TABLE_SIZE\n#define P4D_TABLE_SIZE PAGE_SIZE\n#endif\n\n#ifndef PUD_TABLE_SIZE\n#define PUD_TABLE_SIZE PAGE_SIZE\n#endif\n\n#ifndef PMD_TABLE_SIZE\n#define PMD_TABLE_SIZE PAGE_SIZE\n#endif\n\n#ifndef PTE_TABLE_SIZE\n#define PTE_TABLE_SIZE PAGE_SIZE\n#endif\nvoid __init __weak pcpu_populate_pte(unsigned long addr)\n{\n\tpgd_t *pgd = pgd_offset_k(addr);\n\tp4d_t *p4d;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\n\tif (pgd_none(*pgd)) {\n\t\tp4d = memblock_alloc(P4D_TABLE_SIZE, P4D_TABLE_SIZE);\n\t\tif (!p4d)\n\t\t\tgoto err_alloc;\n\t\tpgd_populate(&init_mm, pgd, p4d);\n\t}\n\n\tp4d = p4d_offset(pgd, addr);\n\tif (p4d_none(*p4d)) {\n\t\tpud = memblock_alloc(PUD_TABLE_SIZE, PUD_TABLE_SIZE);\n\t\tif (!pud)\n\t\t\tgoto err_alloc;\n\t\tp4d_populate(&init_mm, p4d, pud);\n\t}\n\n\tpud = pud_offset(p4d, addr);\n\tif (pud_none(*pud)) {\n\t\tpmd = memblock_alloc(PMD_TABLE_SIZE, PMD_TABLE_SIZE);\n\t\tif (!pmd)\n\t\t\tgoto err_alloc;\n\t\tpud_populate(&init_mm, pud, pmd);\n\t}\n\n\tpmd = pmd_offset(pud, addr);\n\tif (!pmd_present(*pmd)) {\n\t\tpte_t *new;\n\n\t\tnew = memblock_alloc(PTE_TABLE_SIZE, PTE_TABLE_SIZE);\n\t\tif (!new)\n\t\t\tgoto err_alloc;\n\t\tpmd_populate_kernel(&init_mm, pmd, new);\n\t}\n\n\treturn;\n\nerr_alloc:\n\tpanic(\"%s: Failed to allocate memory\\n\", __func__);\n}\n\n \nint __init pcpu_page_first_chunk(size_t reserved_size, pcpu_fc_cpu_to_node_fn_t cpu_to_nd_fn)\n{\n\tstatic struct vm_struct vm;\n\tstruct pcpu_alloc_info *ai;\n\tchar psize_str[16];\n\tint unit_pages;\n\tsize_t pages_size;\n\tstruct page **pages;\n\tint unit, i, j, rc = 0;\n\tint upa;\n\tint nr_g0_units;\n\n\tsnprintf(psize_str, sizeof(psize_str), \"%luK\", PAGE_SIZE >> 10);\n\n\tai = pcpu_build_alloc_info(reserved_size, 0, PAGE_SIZE, NULL);\n\tif (IS_ERR(ai))\n\t\treturn PTR_ERR(ai);\n\tBUG_ON(ai->nr_groups != 1);\n\tupa = ai->alloc_size/ai->unit_size;\n\tnr_g0_units = roundup(num_possible_cpus(), upa);\n\tif (WARN_ON(ai->groups[0].nr_units != nr_g0_units)) {\n\t\tpcpu_free_alloc_info(ai);\n\t\treturn -EINVAL;\n\t}\n\n\tunit_pages = ai->unit_size >> PAGE_SHIFT;\n\n\t \n\tpages_size = PFN_ALIGN(unit_pages * num_possible_cpus() *\n\t\t\t       sizeof(pages[0]));\n\tpages = memblock_alloc(pages_size, SMP_CACHE_BYTES);\n\tif (!pages)\n\t\tpanic(\"%s: Failed to allocate %zu bytes\\n\", __func__,\n\t\t      pages_size);\n\n\t \n\tj = 0;\n\tfor (unit = 0; unit < num_possible_cpus(); unit++) {\n\t\tunsigned int cpu = ai->groups[0].cpu_map[unit];\n\t\tfor (i = 0; i < unit_pages; i++) {\n\t\t\tvoid *ptr;\n\n\t\t\tptr = pcpu_fc_alloc(cpu, PAGE_SIZE, PAGE_SIZE, cpu_to_nd_fn);\n\t\t\tif (!ptr) {\n\t\t\t\tpr_warn(\"failed to allocate %s page for cpu%u\\n\",\n\t\t\t\t\t\tpsize_str, cpu);\n\t\t\t\tgoto enomem;\n\t\t\t}\n\t\t\t \n\t\t\tkmemleak_ignore_phys(__pa(ptr));\n\t\t\tpages[j++] = virt_to_page(ptr);\n\t\t}\n\t}\n\n\t \n\tvm.flags = VM_ALLOC;\n\tvm.size = num_possible_cpus() * ai->unit_size;\n\tvm_area_register_early(&vm, PAGE_SIZE);\n\n\tfor (unit = 0; unit < num_possible_cpus(); unit++) {\n\t\tunsigned long unit_addr =\n\t\t\t(unsigned long)vm.addr + unit * ai->unit_size;\n\n\t\tfor (i = 0; i < unit_pages; i++)\n\t\t\tpcpu_populate_pte(unit_addr + (i << PAGE_SHIFT));\n\n\t\t \n\t\trc = __pcpu_map_pages(unit_addr, &pages[unit * unit_pages],\n\t\t\t\t      unit_pages);\n\t\tif (rc < 0)\n\t\t\tpanic(\"failed to map percpu area, err=%d\\n\", rc);\n\n\t\t \n\n\t\t \n\t\tmemcpy((void *)unit_addr, __per_cpu_load, ai->static_size);\n\t}\n\n\t \n\tpr_info(\"%d %s pages/cpu s%zu r%zu d%zu\\n\",\n\t\tunit_pages, psize_str, ai->static_size,\n\t\tai->reserved_size, ai->dyn_size);\n\n\tpcpu_setup_first_chunk(ai, vm.addr);\n\tgoto out_free_ar;\n\nenomem:\n\twhile (--j >= 0)\n\t\tpcpu_fc_free(page_address(pages[j]), PAGE_SIZE);\n\trc = -ENOMEM;\nout_free_ar:\n\tmemblock_free(pages, pages_size);\n\tpcpu_free_alloc_info(ai);\n\treturn rc;\n}\n#endif  \n\n#ifndef\tCONFIG_HAVE_SETUP_PER_CPU_AREA\n \nunsigned long __per_cpu_offset[NR_CPUS] __read_mostly;\nEXPORT_SYMBOL(__per_cpu_offset);\n\nvoid __init setup_per_cpu_areas(void)\n{\n\tunsigned long delta;\n\tunsigned int cpu;\n\tint rc;\n\n\t \n\trc = pcpu_embed_first_chunk(PERCPU_MODULE_RESERVE, PERCPU_DYNAMIC_RESERVE,\n\t\t\t\t    PAGE_SIZE, NULL, NULL);\n\tif (rc < 0)\n\t\tpanic(\"Failed to initialize percpu areas.\");\n\n\tdelta = (unsigned long)pcpu_base_addr - (unsigned long)__per_cpu_start;\n\tfor_each_possible_cpu(cpu)\n\t\t__per_cpu_offset[cpu] = delta + pcpu_unit_offsets[cpu];\n}\n#endif\t \n\n#else\t \n\n \nvoid __init setup_per_cpu_areas(void)\n{\n\tconst size_t unit_size =\n\t\troundup_pow_of_two(max_t(size_t, PCPU_MIN_UNIT_SIZE,\n\t\t\t\t\t PERCPU_DYNAMIC_RESERVE));\n\tstruct pcpu_alloc_info *ai;\n\tvoid *fc;\n\n\tai = pcpu_alloc_alloc_info(1, 1);\n\tfc = memblock_alloc_from(unit_size, PAGE_SIZE, __pa(MAX_DMA_ADDRESS));\n\tif (!ai || !fc)\n\t\tpanic(\"Failed to allocate memory for percpu areas.\");\n\t \n\tkmemleak_ignore_phys(__pa(fc));\n\n\tai->dyn_size = unit_size;\n\tai->unit_size = unit_size;\n\tai->atom_size = unit_size;\n\tai->alloc_size = unit_size;\n\tai->groups[0].nr_units = 1;\n\tai->groups[0].cpu_map[0] = 0;\n\n\tpcpu_setup_first_chunk(ai, fc);\n\tpcpu_free_alloc_info(ai);\n}\n\n#endif\t \n\n \nunsigned long pcpu_nr_pages(void)\n{\n\treturn pcpu_nr_populated * pcpu_nr_units;\n}\n\n \nstatic int __init percpu_enable_async(void)\n{\n\tpcpu_async_enabled = true;\n\treturn 0;\n}\nsubsys_initcall(percpu_enable_async);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}