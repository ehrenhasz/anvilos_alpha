{
  "module_name": "madvise.c",
  "hash_id": "a0c7c12f47a098b40d026a194ace6e0dee8ec9ed60a3d419ef5e985e56c00abe",
  "original_prompt": "Ingested from linux-6.6.14/mm/madvise.c",
  "human_readable_source": "\n \n\n#include <linux/mman.h>\n#include <linux/pagemap.h>\n#include <linux/syscalls.h>\n#include <linux/mempolicy.h>\n#include <linux/page-isolation.h>\n#include <linux/page_idle.h>\n#include <linux/userfaultfd_k.h>\n#include <linux/hugetlb.h>\n#include <linux/falloc.h>\n#include <linux/fadvise.h>\n#include <linux/sched.h>\n#include <linux/sched/mm.h>\n#include <linux/mm_inline.h>\n#include <linux/string.h>\n#include <linux/uio.h>\n#include <linux/ksm.h>\n#include <linux/fs.h>\n#include <linux/file.h>\n#include <linux/blkdev.h>\n#include <linux/backing-dev.h>\n#include <linux/pagewalk.h>\n#include <linux/swap.h>\n#include <linux/swapops.h>\n#include <linux/shmem_fs.h>\n#include <linux/mmu_notifier.h>\n\n#include <asm/tlb.h>\n\n#include \"internal.h\"\n#include \"swap.h\"\n\nstruct madvise_walk_private {\n\tstruct mmu_gather *tlb;\n\tbool pageout;\n};\n\n \nstatic int madvise_need_mmap_write(int behavior)\n{\n\tswitch (behavior) {\n\tcase MADV_REMOVE:\n\tcase MADV_WILLNEED:\n\tcase MADV_DONTNEED:\n\tcase MADV_DONTNEED_LOCKED:\n\tcase MADV_COLD:\n\tcase MADV_PAGEOUT:\n\tcase MADV_FREE:\n\tcase MADV_POPULATE_READ:\n\tcase MADV_POPULATE_WRITE:\n\tcase MADV_COLLAPSE:\n\t\treturn 0;\n\tdefault:\n\t\t \n\t\treturn 1;\n\t}\n}\n\n#ifdef CONFIG_ANON_VMA_NAME\nstruct anon_vma_name *anon_vma_name_alloc(const char *name)\n{\n\tstruct anon_vma_name *anon_name;\n\tsize_t count;\n\n\t \n\tcount = strlen(name) + 1;\n\tanon_name = kmalloc(struct_size(anon_name, name, count), GFP_KERNEL);\n\tif (anon_name) {\n\t\tkref_init(&anon_name->kref);\n\t\tmemcpy(anon_name->name, name, count);\n\t}\n\n\treturn anon_name;\n}\n\nvoid anon_vma_name_free(struct kref *kref)\n{\n\tstruct anon_vma_name *anon_name =\n\t\t\tcontainer_of(kref, struct anon_vma_name, kref);\n\tkfree(anon_name);\n}\n\nstruct anon_vma_name *anon_vma_name(struct vm_area_struct *vma)\n{\n\tmmap_assert_locked(vma->vm_mm);\n\n\treturn vma->anon_name;\n}\n\n \nstatic int replace_anon_vma_name(struct vm_area_struct *vma,\n\t\t\t\t struct anon_vma_name *anon_name)\n{\n\tstruct anon_vma_name *orig_name = anon_vma_name(vma);\n\n\tif (!anon_name) {\n\t\tvma->anon_name = NULL;\n\t\tanon_vma_name_put(orig_name);\n\t\treturn 0;\n\t}\n\n\tif (anon_vma_name_eq(orig_name, anon_name))\n\t\treturn 0;\n\n\tvma->anon_name = anon_vma_name_reuse(anon_name);\n\tanon_vma_name_put(orig_name);\n\n\treturn 0;\n}\n#else  \nstatic int replace_anon_vma_name(struct vm_area_struct *vma,\n\t\t\t\t struct anon_vma_name *anon_name)\n{\n\tif (anon_name)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n#endif  \n \nstatic int madvise_update_vma(struct vm_area_struct *vma,\n\t\t\t      struct vm_area_struct **prev, unsigned long start,\n\t\t\t      unsigned long end, unsigned long new_flags,\n\t\t\t      struct anon_vma_name *anon_name)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tint error;\n\tpgoff_t pgoff;\n\tVMA_ITERATOR(vmi, mm, start);\n\n\tif (new_flags == vma->vm_flags && anon_vma_name_eq(anon_vma_name(vma), anon_name)) {\n\t\t*prev = vma;\n\t\treturn 0;\n\t}\n\n\tpgoff = vma->vm_pgoff + ((start - vma->vm_start) >> PAGE_SHIFT);\n\t*prev = vma_merge(&vmi, mm, *prev, start, end, new_flags,\n\t\t\t  vma->anon_vma, vma->vm_file, pgoff, vma_policy(vma),\n\t\t\t  vma->vm_userfaultfd_ctx, anon_name);\n\tif (*prev) {\n\t\tvma = *prev;\n\t\tgoto success;\n\t}\n\n\t*prev = vma;\n\n\tif (start != vma->vm_start) {\n\t\terror = split_vma(&vmi, vma, start, 1);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\n\tif (end != vma->vm_end) {\n\t\terror = split_vma(&vmi, vma, end, 0);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\nsuccess:\n\t \n\tvma_start_write(vma);\n\tvm_flags_reset(vma, new_flags);\n\tif (!vma->vm_file || vma_is_anon_shmem(vma)) {\n\t\terror = replace_anon_vma_name(vma, anon_name);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\n\treturn 0;\n}\n\n#ifdef CONFIG_SWAP\nstatic int swapin_walk_pmd_entry(pmd_t *pmd, unsigned long start,\n\t\tunsigned long end, struct mm_walk *walk)\n{\n\tstruct vm_area_struct *vma = walk->private;\n\tstruct swap_iocb *splug = NULL;\n\tpte_t *ptep = NULL;\n\tspinlock_t *ptl;\n\tunsigned long addr;\n\n\tfor (addr = start; addr < end; addr += PAGE_SIZE) {\n\t\tpte_t pte;\n\t\tswp_entry_t entry;\n\t\tstruct page *page;\n\n\t\tif (!ptep++) {\n\t\t\tptep = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);\n\t\t\tif (!ptep)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tpte = ptep_get(ptep);\n\t\tif (!is_swap_pte(pte))\n\t\t\tcontinue;\n\t\tentry = pte_to_swp_entry(pte);\n\t\tif (unlikely(non_swap_entry(entry)))\n\t\t\tcontinue;\n\n\t\tpte_unmap_unlock(ptep, ptl);\n\t\tptep = NULL;\n\n\t\tpage = read_swap_cache_async(entry, GFP_HIGHUSER_MOVABLE,\n\t\t\t\t\t     vma, addr, &splug);\n\t\tif (page)\n\t\t\tput_page(page);\n\t}\n\n\tif (ptep)\n\t\tpte_unmap_unlock(ptep, ptl);\n\tswap_read_unplug(splug);\n\tcond_resched();\n\n\treturn 0;\n}\n\nstatic const struct mm_walk_ops swapin_walk_ops = {\n\t.pmd_entry\t\t= swapin_walk_pmd_entry,\n\t.walk_lock\t\t= PGWALK_RDLOCK,\n};\n\nstatic void shmem_swapin_range(struct vm_area_struct *vma,\n\t\tunsigned long start, unsigned long end,\n\t\tstruct address_space *mapping)\n{\n\tXA_STATE(xas, &mapping->i_pages, linear_page_index(vma, start));\n\tpgoff_t end_index = linear_page_index(vma, end) - 1;\n\tstruct page *page;\n\tstruct swap_iocb *splug = NULL;\n\n\trcu_read_lock();\n\txas_for_each(&xas, page, end_index) {\n\t\tunsigned long addr;\n\t\tswp_entry_t entry;\n\n\t\tif (!xa_is_value(page))\n\t\t\tcontinue;\n\t\tentry = radix_to_swp_entry(page);\n\t\t \n\t\tif (non_swap_entry(entry))\n\t\t\tcontinue;\n\n\t\taddr = vma->vm_start +\n\t\t\t((xas.xa_index - vma->vm_pgoff) << PAGE_SHIFT);\n\t\txas_pause(&xas);\n\t\trcu_read_unlock();\n\n\t\tpage = read_swap_cache_async(entry, mapping_gfp_mask(mapping),\n\t\t\t\t\t     vma, addr, &splug);\n\t\tif (page)\n\t\t\tput_page(page);\n\n\t\trcu_read_lock();\n\t}\n\trcu_read_unlock();\n\tswap_read_unplug(splug);\n}\n#endif\t\t \n\n \nstatic long madvise_willneed(struct vm_area_struct *vma,\n\t\t\t     struct vm_area_struct **prev,\n\t\t\t     unsigned long start, unsigned long end)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct file *file = vma->vm_file;\n\tloff_t offset;\n\n\t*prev = vma;\n#ifdef CONFIG_SWAP\n\tif (!file) {\n\t\twalk_page_range(vma->vm_mm, start, end, &swapin_walk_ops, vma);\n\t\tlru_add_drain();  \n\t\treturn 0;\n\t}\n\n\tif (shmem_mapping(file->f_mapping)) {\n\t\tshmem_swapin_range(vma, start, end, file->f_mapping);\n\t\tlru_add_drain();  \n\t\treturn 0;\n\t}\n#else\n\tif (!file)\n\t\treturn -EBADF;\n#endif\n\n\tif (IS_DAX(file_inode(file))) {\n\t\t \n\t\treturn 0;\n\t}\n\n\t \n\t*prev = NULL;\t \n\tget_file(file);\n\toffset = (loff_t)(start - vma->vm_start)\n\t\t\t+ ((loff_t)vma->vm_pgoff << PAGE_SHIFT);\n\tmmap_read_unlock(mm);\n\tvfs_fadvise(file, offset, end - start, POSIX_FADV_WILLNEED);\n\tfput(file);\n\tmmap_read_lock(mm);\n\treturn 0;\n}\n\nstatic inline bool can_do_file_pageout(struct vm_area_struct *vma)\n{\n\tif (!vma->vm_file)\n\t\treturn false;\n\t \n\treturn inode_owner_or_capable(&nop_mnt_idmap,\n\t\t\t\t      file_inode(vma->vm_file)) ||\n\t       file_permission(vma->vm_file, MAY_WRITE) == 0;\n}\n\nstatic int madvise_cold_or_pageout_pte_range(pmd_t *pmd,\n\t\t\t\tunsigned long addr, unsigned long end,\n\t\t\t\tstruct mm_walk *walk)\n{\n\tstruct madvise_walk_private *private = walk->private;\n\tstruct mmu_gather *tlb = private->tlb;\n\tbool pageout = private->pageout;\n\tstruct mm_struct *mm = tlb->mm;\n\tstruct vm_area_struct *vma = walk->vma;\n\tpte_t *start_pte, *pte, ptent;\n\tspinlock_t *ptl;\n\tstruct folio *folio = NULL;\n\tLIST_HEAD(folio_list);\n\tbool pageout_anon_only_filter;\n\n\tif (fatal_signal_pending(current))\n\t\treturn -EINTR;\n\n\tpageout_anon_only_filter = pageout && !vma_is_anonymous(vma) &&\n\t\t\t\t\t!can_do_file_pageout(vma);\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tif (pmd_trans_huge(*pmd)) {\n\t\tpmd_t orig_pmd;\n\t\tunsigned long next = pmd_addr_end(addr, end);\n\n\t\ttlb_change_page_size(tlb, HPAGE_PMD_SIZE);\n\t\tptl = pmd_trans_huge_lock(pmd, vma);\n\t\tif (!ptl)\n\t\t\treturn 0;\n\n\t\torig_pmd = *pmd;\n\t\tif (is_huge_zero_pmd(orig_pmd))\n\t\t\tgoto huge_unlock;\n\n\t\tif (unlikely(!pmd_present(orig_pmd))) {\n\t\t\tVM_BUG_ON(thp_migration_supported() &&\n\t\t\t\t\t!is_pmd_migration_entry(orig_pmd));\n\t\t\tgoto huge_unlock;\n\t\t}\n\n\t\tfolio = pfn_folio(pmd_pfn(orig_pmd));\n\n\t\t \n\t\tif (folio_estimated_sharers(folio) != 1)\n\t\t\tgoto huge_unlock;\n\n\t\tif (pageout_anon_only_filter && !folio_test_anon(folio))\n\t\t\tgoto huge_unlock;\n\n\t\tif (next - addr != HPAGE_PMD_SIZE) {\n\t\t\tint err;\n\n\t\t\tfolio_get(folio);\n\t\t\tspin_unlock(ptl);\n\t\t\tfolio_lock(folio);\n\t\t\terr = split_folio(folio);\n\t\t\tfolio_unlock(folio);\n\t\t\tfolio_put(folio);\n\t\t\tif (!err)\n\t\t\t\tgoto regular_folio;\n\t\t\treturn 0;\n\t\t}\n\n\t\tif (pmd_young(orig_pmd)) {\n\t\t\tpmdp_invalidate(vma, addr, pmd);\n\t\t\torig_pmd = pmd_mkold(orig_pmd);\n\n\t\t\tset_pmd_at(mm, addr, pmd, orig_pmd);\n\t\t\ttlb_remove_pmd_tlb_entry(tlb, pmd, addr);\n\t\t}\n\n\t\tfolio_clear_referenced(folio);\n\t\tfolio_test_clear_young(folio);\n\t\tif (folio_test_active(folio))\n\t\t\tfolio_set_workingset(folio);\n\t\tif (pageout) {\n\t\t\tif (folio_isolate_lru(folio)) {\n\t\t\t\tif (folio_test_unevictable(folio))\n\t\t\t\t\tfolio_putback_lru(folio);\n\t\t\t\telse\n\t\t\t\t\tlist_add(&folio->lru, &folio_list);\n\t\t\t}\n\t\t} else\n\t\t\tfolio_deactivate(folio);\nhuge_unlock:\n\t\tspin_unlock(ptl);\n\t\tif (pageout)\n\t\t\treclaim_pages(&folio_list);\n\t\treturn 0;\n\t}\n\nregular_folio:\n#endif\n\ttlb_change_page_size(tlb, PAGE_SIZE);\n\tstart_pte = pte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);\n\tif (!start_pte)\n\t\treturn 0;\n\tflush_tlb_batched_pending(mm);\n\tarch_enter_lazy_mmu_mode();\n\tfor (; addr < end; pte++, addr += PAGE_SIZE) {\n\t\tptent = ptep_get(pte);\n\n\t\tif (pte_none(ptent))\n\t\t\tcontinue;\n\n\t\tif (!pte_present(ptent))\n\t\t\tcontinue;\n\n\t\tfolio = vm_normal_folio(vma, addr, ptent);\n\t\tif (!folio || folio_is_zone_device(folio))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (folio_test_large(folio)) {\n\t\t\tint err;\n\n\t\t\tif (folio_estimated_sharers(folio) != 1)\n\t\t\t\tbreak;\n\t\t\tif (pageout_anon_only_filter && !folio_test_anon(folio))\n\t\t\t\tbreak;\n\t\t\tif (!folio_trylock(folio))\n\t\t\t\tbreak;\n\t\t\tfolio_get(folio);\n\t\t\tarch_leave_lazy_mmu_mode();\n\t\t\tpte_unmap_unlock(start_pte, ptl);\n\t\t\tstart_pte = NULL;\n\t\t\terr = split_folio(folio);\n\t\t\tfolio_unlock(folio);\n\t\t\tfolio_put(folio);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tstart_pte = pte =\n\t\t\t\tpte_offset_map_lock(mm, pmd, addr, &ptl);\n\t\t\tif (!start_pte)\n\t\t\t\tbreak;\n\t\t\tarch_enter_lazy_mmu_mode();\n\t\t\tpte--;\n\t\t\taddr -= PAGE_SIZE;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (!folio_test_lru(folio) || folio_mapcount(folio) != 1)\n\t\t\tcontinue;\n\n\t\tif (pageout_anon_only_filter && !folio_test_anon(folio))\n\t\t\tcontinue;\n\n\t\tVM_BUG_ON_FOLIO(folio_test_large(folio), folio);\n\n\t\tif (pte_young(ptent)) {\n\t\t\tptent = ptep_get_and_clear_full(mm, addr, pte,\n\t\t\t\t\t\t\ttlb->fullmm);\n\t\t\tptent = pte_mkold(ptent);\n\t\t\tset_pte_at(mm, addr, pte, ptent);\n\t\t\ttlb_remove_tlb_entry(tlb, pte, addr);\n\t\t}\n\n\t\t \n\t\tfolio_clear_referenced(folio);\n\t\tfolio_test_clear_young(folio);\n\t\tif (folio_test_active(folio))\n\t\t\tfolio_set_workingset(folio);\n\t\tif (pageout) {\n\t\t\tif (folio_isolate_lru(folio)) {\n\t\t\t\tif (folio_test_unevictable(folio))\n\t\t\t\t\tfolio_putback_lru(folio);\n\t\t\t\telse\n\t\t\t\t\tlist_add(&folio->lru, &folio_list);\n\t\t\t}\n\t\t} else\n\t\t\tfolio_deactivate(folio);\n\t}\n\n\tif (start_pte) {\n\t\tarch_leave_lazy_mmu_mode();\n\t\tpte_unmap_unlock(start_pte, ptl);\n\t}\n\tif (pageout)\n\t\treclaim_pages(&folio_list);\n\tcond_resched();\n\n\treturn 0;\n}\n\nstatic const struct mm_walk_ops cold_walk_ops = {\n\t.pmd_entry = madvise_cold_or_pageout_pte_range,\n\t.walk_lock = PGWALK_RDLOCK,\n};\n\nstatic void madvise_cold_page_range(struct mmu_gather *tlb,\n\t\t\t     struct vm_area_struct *vma,\n\t\t\t     unsigned long addr, unsigned long end)\n{\n\tstruct madvise_walk_private walk_private = {\n\t\t.pageout = false,\n\t\t.tlb = tlb,\n\t};\n\n\ttlb_start_vma(tlb, vma);\n\twalk_page_range(vma->vm_mm, addr, end, &cold_walk_ops, &walk_private);\n\ttlb_end_vma(tlb, vma);\n}\n\nstatic inline bool can_madv_lru_vma(struct vm_area_struct *vma)\n{\n\treturn !(vma->vm_flags & (VM_LOCKED|VM_PFNMAP|VM_HUGETLB));\n}\n\nstatic long madvise_cold(struct vm_area_struct *vma,\n\t\t\tstruct vm_area_struct **prev,\n\t\t\tunsigned long start_addr, unsigned long end_addr)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct mmu_gather tlb;\n\n\t*prev = vma;\n\tif (!can_madv_lru_vma(vma))\n\t\treturn -EINVAL;\n\n\tlru_add_drain();\n\ttlb_gather_mmu(&tlb, mm);\n\tmadvise_cold_page_range(&tlb, vma, start_addr, end_addr);\n\ttlb_finish_mmu(&tlb);\n\n\treturn 0;\n}\n\nstatic void madvise_pageout_page_range(struct mmu_gather *tlb,\n\t\t\t     struct vm_area_struct *vma,\n\t\t\t     unsigned long addr, unsigned long end)\n{\n\tstruct madvise_walk_private walk_private = {\n\t\t.pageout = true,\n\t\t.tlb = tlb,\n\t};\n\n\ttlb_start_vma(tlb, vma);\n\twalk_page_range(vma->vm_mm, addr, end, &cold_walk_ops, &walk_private);\n\ttlb_end_vma(tlb, vma);\n}\n\nstatic long madvise_pageout(struct vm_area_struct *vma,\n\t\t\tstruct vm_area_struct **prev,\n\t\t\tunsigned long start_addr, unsigned long end_addr)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct mmu_gather tlb;\n\n\t*prev = vma;\n\tif (!can_madv_lru_vma(vma))\n\t\treturn -EINVAL;\n\n\t \n\tif (!vma_is_anonymous(vma) && (!can_do_file_pageout(vma) &&\n\t\t\t\t(vma->vm_flags & VM_MAYSHARE)))\n\t\treturn 0;\n\n\tlru_add_drain();\n\ttlb_gather_mmu(&tlb, mm);\n\tmadvise_pageout_page_range(&tlb, vma, start_addr, end_addr);\n\ttlb_finish_mmu(&tlb);\n\n\treturn 0;\n}\n\nstatic int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,\n\t\t\t\tunsigned long end, struct mm_walk *walk)\n\n{\n\tstruct mmu_gather *tlb = walk->private;\n\tstruct mm_struct *mm = tlb->mm;\n\tstruct vm_area_struct *vma = walk->vma;\n\tspinlock_t *ptl;\n\tpte_t *start_pte, *pte, ptent;\n\tstruct folio *folio;\n\tint nr_swap = 0;\n\tunsigned long next;\n\n\tnext = pmd_addr_end(addr, end);\n\tif (pmd_trans_huge(*pmd))\n\t\tif (madvise_free_huge_pmd(tlb, vma, pmd, addr, next))\n\t\t\treturn 0;\n\n\ttlb_change_page_size(tlb, PAGE_SIZE);\n\tstart_pte = pte = pte_offset_map_lock(mm, pmd, addr, &ptl);\n\tif (!start_pte)\n\t\treturn 0;\n\tflush_tlb_batched_pending(mm);\n\tarch_enter_lazy_mmu_mode();\n\tfor (; addr != end; pte++, addr += PAGE_SIZE) {\n\t\tptent = ptep_get(pte);\n\n\t\tif (pte_none(ptent))\n\t\t\tcontinue;\n\t\t \n\t\tif (!pte_present(ptent)) {\n\t\t\tswp_entry_t entry;\n\n\t\t\tentry = pte_to_swp_entry(ptent);\n\t\t\tif (!non_swap_entry(entry)) {\n\t\t\t\tnr_swap--;\n\t\t\t\tfree_swap_and_cache(entry);\n\t\t\t\tpte_clear_not_present_full(mm, addr, pte, tlb->fullmm);\n\t\t\t} else if (is_hwpoison_entry(entry) ||\n\t\t\t\t   is_poisoned_swp_entry(entry)) {\n\t\t\t\tpte_clear_not_present_full(mm, addr, pte, tlb->fullmm);\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\tfolio = vm_normal_folio(vma, addr, ptent);\n\t\tif (!folio || folio_is_zone_device(folio))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (folio_test_large(folio)) {\n\t\t\tint err;\n\n\t\t\tif (folio_estimated_sharers(folio) != 1)\n\t\t\t\tbreak;\n\t\t\tif (!folio_trylock(folio))\n\t\t\t\tbreak;\n\t\t\tfolio_get(folio);\n\t\t\tarch_leave_lazy_mmu_mode();\n\t\t\tpte_unmap_unlock(start_pte, ptl);\n\t\t\tstart_pte = NULL;\n\t\t\terr = split_folio(folio);\n\t\t\tfolio_unlock(folio);\n\t\t\tfolio_put(folio);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tstart_pte = pte =\n\t\t\t\tpte_offset_map_lock(mm, pmd, addr, &ptl);\n\t\t\tif (!start_pte)\n\t\t\t\tbreak;\n\t\t\tarch_enter_lazy_mmu_mode();\n\t\t\tpte--;\n\t\t\taddr -= PAGE_SIZE;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (folio_test_swapcache(folio) || folio_test_dirty(folio)) {\n\t\t\tif (!folio_trylock(folio))\n\t\t\t\tcontinue;\n\t\t\t \n\t\t\tif (folio_mapcount(folio) != 1) {\n\t\t\t\tfolio_unlock(folio);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (folio_test_swapcache(folio) &&\n\t\t\t    !folio_free_swap(folio)) {\n\t\t\t\tfolio_unlock(folio);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tfolio_clear_dirty(folio);\n\t\t\tfolio_unlock(folio);\n\t\t}\n\n\t\tif (pte_young(ptent) || pte_dirty(ptent)) {\n\t\t\t \n\t\t\tptent = ptep_get_and_clear_full(mm, addr, pte,\n\t\t\t\t\t\t\ttlb->fullmm);\n\n\t\t\tptent = pte_mkold(ptent);\n\t\t\tptent = pte_mkclean(ptent);\n\t\t\tset_pte_at(mm, addr, pte, ptent);\n\t\t\ttlb_remove_tlb_entry(tlb, pte, addr);\n\t\t}\n\t\tfolio_mark_lazyfree(folio);\n\t}\n\n\tif (nr_swap) {\n\t\tif (current->mm == mm)\n\t\t\tsync_mm_rss(mm);\n\t\tadd_mm_counter(mm, MM_SWAPENTS, nr_swap);\n\t}\n\tif (start_pte) {\n\t\tarch_leave_lazy_mmu_mode();\n\t\tpte_unmap_unlock(start_pte, ptl);\n\t}\n\tcond_resched();\n\n\treturn 0;\n}\n\nstatic const struct mm_walk_ops madvise_free_walk_ops = {\n\t.pmd_entry\t\t= madvise_free_pte_range,\n\t.walk_lock\t\t= PGWALK_RDLOCK,\n};\n\nstatic int madvise_free_single_vma(struct vm_area_struct *vma,\n\t\t\tunsigned long start_addr, unsigned long end_addr)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct mmu_notifier_range range;\n\tstruct mmu_gather tlb;\n\n\t \n\tif (!vma_is_anonymous(vma))\n\t\treturn -EINVAL;\n\n\trange.start = max(vma->vm_start, start_addr);\n\tif (range.start >= vma->vm_end)\n\t\treturn -EINVAL;\n\trange.end = min(vma->vm_end, end_addr);\n\tif (range.end <= vma->vm_start)\n\t\treturn -EINVAL;\n\tmmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, mm,\n\t\t\t\trange.start, range.end);\n\n\tlru_add_drain();\n\ttlb_gather_mmu(&tlb, mm);\n\tupdate_hiwater_rss(mm);\n\n\tmmu_notifier_invalidate_range_start(&range);\n\ttlb_start_vma(&tlb, vma);\n\twalk_page_range(vma->vm_mm, range.start, range.end,\n\t\t\t&madvise_free_walk_ops, &tlb);\n\ttlb_end_vma(&tlb, vma);\n\tmmu_notifier_invalidate_range_end(&range);\n\ttlb_finish_mmu(&tlb);\n\n\treturn 0;\n}\n\n \nstatic long madvise_dontneed_single_vma(struct vm_area_struct *vma,\n\t\t\t\t\tunsigned long start, unsigned long end)\n{\n\tzap_page_range_single(vma, start, end - start, NULL);\n\treturn 0;\n}\n\nstatic bool madvise_dontneed_free_valid_vma(struct vm_area_struct *vma,\n\t\t\t\t\t    unsigned long start,\n\t\t\t\t\t    unsigned long *end,\n\t\t\t\t\t    int behavior)\n{\n\tif (!is_vm_hugetlb_page(vma)) {\n\t\tunsigned int forbidden = VM_PFNMAP;\n\n\t\tif (behavior != MADV_DONTNEED_LOCKED)\n\t\t\tforbidden |= VM_LOCKED;\n\n\t\treturn !(vma->vm_flags & forbidden);\n\t}\n\n\tif (behavior != MADV_DONTNEED && behavior != MADV_DONTNEED_LOCKED)\n\t\treturn false;\n\tif (start & ~huge_page_mask(hstate_vma(vma)))\n\t\treturn false;\n\n\t \n\t*end = ALIGN_DOWN(*end, huge_page_size(hstate_vma(vma)));\n\n\treturn true;\n}\n\nstatic long madvise_dontneed_free(struct vm_area_struct *vma,\n\t\t\t\t  struct vm_area_struct **prev,\n\t\t\t\t  unsigned long start, unsigned long end,\n\t\t\t\t  int behavior)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\n\t*prev = vma;\n\tif (!madvise_dontneed_free_valid_vma(vma, start, &end, behavior))\n\t\treturn -EINVAL;\n\n\tif (start == end)\n\t\treturn 0;\n\n\tif (!userfaultfd_remove(vma, start, end)) {\n\t\t*prev = NULL;  \n\n\t\tmmap_read_lock(mm);\n\t\tvma = vma_lookup(mm, start);\n\t\tif (!vma)\n\t\t\treturn -ENOMEM;\n\t\t \n\t\tif (!madvise_dontneed_free_valid_vma(vma, start, &end,\n\t\t\t\t\t\t     behavior))\n\t\t\treturn -EINVAL;\n\t\tif (end > vma->vm_end) {\n\t\t\t \n\t\t\tend = vma->vm_end;\n\t\t}\n\t\tVM_WARN_ON(start >= end);\n\t}\n\n\tif (behavior == MADV_DONTNEED || behavior == MADV_DONTNEED_LOCKED)\n\t\treturn madvise_dontneed_single_vma(vma, start, end);\n\telse if (behavior == MADV_FREE)\n\t\treturn madvise_free_single_vma(vma, start, end);\n\telse\n\t\treturn -EINVAL;\n}\n\nstatic long madvise_populate(struct vm_area_struct *vma,\n\t\t\t     struct vm_area_struct **prev,\n\t\t\t     unsigned long start, unsigned long end,\n\t\t\t     int behavior)\n{\n\tconst bool write = behavior == MADV_POPULATE_WRITE;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long tmp_end;\n\tint locked = 1;\n\tlong pages;\n\n\t*prev = vma;\n\n\twhile (start < end) {\n\t\t \n\t\tif (!vma || start >= vma->vm_end) {\n\t\t\tvma = vma_lookup(mm, start);\n\t\t\tif (!vma)\n\t\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\ttmp_end = min_t(unsigned long, end, vma->vm_end);\n\t\t \n\t\tpages = faultin_vma_page_range(vma, start, tmp_end, write,\n\t\t\t\t\t       &locked);\n\t\tif (!locked) {\n\t\t\tmmap_read_lock(mm);\n\t\t\tlocked = 1;\n\t\t\t*prev = NULL;\n\t\t\tvma = NULL;\n\t\t}\n\t\tif (pages < 0) {\n\t\t\tswitch (pages) {\n\t\t\tcase -EINTR:\n\t\t\t\treturn -EINTR;\n\t\t\tcase -EINVAL:  \n\t\t\t\treturn -EINVAL;\n\t\t\tcase -EHWPOISON:\n\t\t\t\treturn -EHWPOISON;\n\t\t\tcase -EFAULT:  \n\t\t\t\treturn -EFAULT;\n\t\t\tdefault:\n\t\t\t\tpr_warn_once(\"%s: unhandled return value: %ld\\n\",\n\t\t\t\t\t     __func__, pages);\n\t\t\t\tfallthrough;\n\t\t\tcase -ENOMEM:\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t}\n\t\tstart += pages * PAGE_SIZE;\n\t}\n\treturn 0;\n}\n\n \nstatic long madvise_remove(struct vm_area_struct *vma,\n\t\t\t\tstruct vm_area_struct **prev,\n\t\t\t\tunsigned long start, unsigned long end)\n{\n\tloff_t offset;\n\tint error;\n\tstruct file *f;\n\tstruct mm_struct *mm = vma->vm_mm;\n\n\t*prev = NULL;\t \n\n\tif (vma->vm_flags & VM_LOCKED)\n\t\treturn -EINVAL;\n\n\tf = vma->vm_file;\n\n\tif (!f || !f->f_mapping || !f->f_mapping->host) {\n\t\t\treturn -EINVAL;\n\t}\n\n\tif ((vma->vm_flags & (VM_SHARED|VM_WRITE)) != (VM_SHARED|VM_WRITE))\n\t\treturn -EACCES;\n\n\toffset = (loff_t)(start - vma->vm_start)\n\t\t\t+ ((loff_t)vma->vm_pgoff << PAGE_SHIFT);\n\n\t \n\tget_file(f);\n\tif (userfaultfd_remove(vma, start, end)) {\n\t\t \n\t\tmmap_read_unlock(mm);\n\t}\n\terror = vfs_fallocate(f,\n\t\t\t\tFALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,\n\t\t\t\toffset, end - start);\n\tfput(f);\n\tmmap_read_lock(mm);\n\treturn error;\n}\n\n \nstatic int madvise_vma_behavior(struct vm_area_struct *vma,\n\t\t\t\tstruct vm_area_struct **prev,\n\t\t\t\tunsigned long start, unsigned long end,\n\t\t\t\tunsigned long behavior)\n{\n\tint error;\n\tstruct anon_vma_name *anon_name;\n\tunsigned long new_flags = vma->vm_flags;\n\n\tswitch (behavior) {\n\tcase MADV_REMOVE:\n\t\treturn madvise_remove(vma, prev, start, end);\n\tcase MADV_WILLNEED:\n\t\treturn madvise_willneed(vma, prev, start, end);\n\tcase MADV_COLD:\n\t\treturn madvise_cold(vma, prev, start, end);\n\tcase MADV_PAGEOUT:\n\t\treturn madvise_pageout(vma, prev, start, end);\n\tcase MADV_FREE:\n\tcase MADV_DONTNEED:\n\tcase MADV_DONTNEED_LOCKED:\n\t\treturn madvise_dontneed_free(vma, prev, start, end, behavior);\n\tcase MADV_POPULATE_READ:\n\tcase MADV_POPULATE_WRITE:\n\t\treturn madvise_populate(vma, prev, start, end, behavior);\n\tcase MADV_NORMAL:\n\t\tnew_flags = new_flags & ~VM_RAND_READ & ~VM_SEQ_READ;\n\t\tbreak;\n\tcase MADV_SEQUENTIAL:\n\t\tnew_flags = (new_flags & ~VM_RAND_READ) | VM_SEQ_READ;\n\t\tbreak;\n\tcase MADV_RANDOM:\n\t\tnew_flags = (new_flags & ~VM_SEQ_READ) | VM_RAND_READ;\n\t\tbreak;\n\tcase MADV_DONTFORK:\n\t\tnew_flags |= VM_DONTCOPY;\n\t\tbreak;\n\tcase MADV_DOFORK:\n\t\tif (vma->vm_flags & VM_IO)\n\t\t\treturn -EINVAL;\n\t\tnew_flags &= ~VM_DONTCOPY;\n\t\tbreak;\n\tcase MADV_WIPEONFORK:\n\t\t \n\t\tif (vma->vm_file || vma->vm_flags & VM_SHARED)\n\t\t\treturn -EINVAL;\n\t\tnew_flags |= VM_WIPEONFORK;\n\t\tbreak;\n\tcase MADV_KEEPONFORK:\n\t\tnew_flags &= ~VM_WIPEONFORK;\n\t\tbreak;\n\tcase MADV_DONTDUMP:\n\t\tnew_flags |= VM_DONTDUMP;\n\t\tbreak;\n\tcase MADV_DODUMP:\n\t\tif (!is_vm_hugetlb_page(vma) && new_flags & VM_SPECIAL)\n\t\t\treturn -EINVAL;\n\t\tnew_flags &= ~VM_DONTDUMP;\n\t\tbreak;\n\tcase MADV_MERGEABLE:\n\tcase MADV_UNMERGEABLE:\n\t\terror = ksm_madvise(vma, start, end, behavior, &new_flags);\n\t\tif (error)\n\t\t\tgoto out;\n\t\tbreak;\n\tcase MADV_HUGEPAGE:\n\tcase MADV_NOHUGEPAGE:\n\t\terror = hugepage_madvise(vma, &new_flags, behavior);\n\t\tif (error)\n\t\t\tgoto out;\n\t\tbreak;\n\tcase MADV_COLLAPSE:\n\t\treturn madvise_collapse(vma, prev, start, end);\n\t}\n\n\tanon_name = anon_vma_name(vma);\n\tanon_vma_name_get(anon_name);\n\terror = madvise_update_vma(vma, prev, start, end, new_flags,\n\t\t\t\t   anon_name);\n\tanon_vma_name_put(anon_name);\n\nout:\n\t \n\tif (error == -ENOMEM)\n\t\terror = -EAGAIN;\n\treturn error;\n}\n\n#ifdef CONFIG_MEMORY_FAILURE\n \nstatic int madvise_inject_error(int behavior,\n\t\tunsigned long start, unsigned long end)\n{\n\tunsigned long size;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\n\tfor (; start < end; start += size) {\n\t\tunsigned long pfn;\n\t\tstruct page *page;\n\t\tint ret;\n\n\t\tret = get_user_pages_fast(start, 1, 0, &page);\n\t\tif (ret != 1)\n\t\t\treturn ret;\n\t\tpfn = page_to_pfn(page);\n\n\t\t \n\t\tsize = page_size(compound_head(page));\n\n\t\tif (behavior == MADV_SOFT_OFFLINE) {\n\t\t\tpr_info(\"Soft offlining pfn %#lx at process virtual address %#lx\\n\",\n\t\t\t\t pfn, start);\n\t\t\tret = soft_offline_page(pfn, MF_COUNT_INCREASED);\n\t\t} else {\n\t\t\tpr_info(\"Injecting memory failure for pfn %#lx at process virtual address %#lx\\n\",\n\t\t\t\t pfn, start);\n\t\t\tret = memory_failure(pfn, MF_COUNT_INCREASED | MF_SW_SIMULATED);\n\t\t\tif (ret == -EOPNOTSUPP)\n\t\t\t\tret = 0;\n\t\t}\n\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n#endif\n\nstatic bool\nmadvise_behavior_valid(int behavior)\n{\n\tswitch (behavior) {\n\tcase MADV_DOFORK:\n\tcase MADV_DONTFORK:\n\tcase MADV_NORMAL:\n\tcase MADV_SEQUENTIAL:\n\tcase MADV_RANDOM:\n\tcase MADV_REMOVE:\n\tcase MADV_WILLNEED:\n\tcase MADV_DONTNEED:\n\tcase MADV_DONTNEED_LOCKED:\n\tcase MADV_FREE:\n\tcase MADV_COLD:\n\tcase MADV_PAGEOUT:\n\tcase MADV_POPULATE_READ:\n\tcase MADV_POPULATE_WRITE:\n#ifdef CONFIG_KSM\n\tcase MADV_MERGEABLE:\n\tcase MADV_UNMERGEABLE:\n#endif\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tcase MADV_HUGEPAGE:\n\tcase MADV_NOHUGEPAGE:\n\tcase MADV_COLLAPSE:\n#endif\n\tcase MADV_DONTDUMP:\n\tcase MADV_DODUMP:\n\tcase MADV_WIPEONFORK:\n\tcase MADV_KEEPONFORK:\n#ifdef CONFIG_MEMORY_FAILURE\n\tcase MADV_SOFT_OFFLINE:\n\tcase MADV_HWPOISON:\n#endif\n\t\treturn true;\n\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic bool process_madvise_behavior_valid(int behavior)\n{\n\tswitch (behavior) {\n\tcase MADV_COLD:\n\tcase MADV_PAGEOUT:\n\tcase MADV_WILLNEED:\n\tcase MADV_COLLAPSE:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n \nstatic\nint madvise_walk_vmas(struct mm_struct *mm, unsigned long start,\n\t\t      unsigned long end, unsigned long arg,\n\t\t      int (*visit)(struct vm_area_struct *vma,\n\t\t\t\t   struct vm_area_struct **prev, unsigned long start,\n\t\t\t\t   unsigned long end, unsigned long arg))\n{\n\tstruct vm_area_struct *vma;\n\tstruct vm_area_struct *prev;\n\tunsigned long tmp;\n\tint unmapped_error = 0;\n\n\t \n\tvma = find_vma_prev(mm, start, &prev);\n\tif (vma && start > vma->vm_start)\n\t\tprev = vma;\n\n\tfor (;;) {\n\t\tint error;\n\n\t\t \n\t\tif (!vma)\n\t\t\treturn -ENOMEM;\n\n\t\t \n\t\tif (start < vma->vm_start) {\n\t\t\tunmapped_error = -ENOMEM;\n\t\t\tstart = vma->vm_start;\n\t\t\tif (start >= end)\n\t\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\ttmp = vma->vm_end;\n\t\tif (end < tmp)\n\t\t\ttmp = end;\n\n\t\t \n\t\terror = visit(vma, &prev, start, tmp, arg);\n\t\tif (error)\n\t\t\treturn error;\n\t\tstart = tmp;\n\t\tif (prev && start < prev->vm_end)\n\t\t\tstart = prev->vm_end;\n\t\tif (start >= end)\n\t\t\tbreak;\n\t\tif (prev)\n\t\t\tvma = find_vma(mm, prev->vm_end);\n\t\telse\t \n\t\t\tvma = find_vma(mm, start);\n\t}\n\n\treturn unmapped_error;\n}\n\n#ifdef CONFIG_ANON_VMA_NAME\nstatic int madvise_vma_anon_name(struct vm_area_struct *vma,\n\t\t\t\t struct vm_area_struct **prev,\n\t\t\t\t unsigned long start, unsigned long end,\n\t\t\t\t unsigned long anon_name)\n{\n\tint error;\n\n\t \n\tif (vma->vm_file && !vma_is_anon_shmem(vma))\n\t\treturn -EBADF;\n\n\terror = madvise_update_vma(vma, prev, start, end, vma->vm_flags,\n\t\t\t\t   (struct anon_vma_name *)anon_name);\n\n\t \n\tif (error == -ENOMEM)\n\t\terror = -EAGAIN;\n\treturn error;\n}\n\nint madvise_set_anon_name(struct mm_struct *mm, unsigned long start,\n\t\t\t  unsigned long len_in, struct anon_vma_name *anon_name)\n{\n\tunsigned long end;\n\tunsigned long len;\n\n\tif (start & ~PAGE_MASK)\n\t\treturn -EINVAL;\n\tlen = (len_in + ~PAGE_MASK) & PAGE_MASK;\n\n\t \n\tif (len_in && !len)\n\t\treturn -EINVAL;\n\n\tend = start + len;\n\tif (end < start)\n\t\treturn -EINVAL;\n\n\tif (end == start)\n\t\treturn 0;\n\n\treturn madvise_walk_vmas(mm, start, end, (unsigned long)anon_name,\n\t\t\t\t madvise_vma_anon_name);\n}\n#endif  \n \nint do_madvise(struct mm_struct *mm, unsigned long start, size_t len_in, int behavior)\n{\n\tunsigned long end;\n\tint error;\n\tint write;\n\tsize_t len;\n\tstruct blk_plug plug;\n\n\tif (!madvise_behavior_valid(behavior))\n\t\treturn -EINVAL;\n\n\tif (!PAGE_ALIGNED(start))\n\t\treturn -EINVAL;\n\tlen = PAGE_ALIGN(len_in);\n\n\t \n\tif (len_in && !len)\n\t\treturn -EINVAL;\n\n\tend = start + len;\n\tif (end < start)\n\t\treturn -EINVAL;\n\n\tif (end == start)\n\t\treturn 0;\n\n#ifdef CONFIG_MEMORY_FAILURE\n\tif (behavior == MADV_HWPOISON || behavior == MADV_SOFT_OFFLINE)\n\t\treturn madvise_inject_error(behavior, start, start + len_in);\n#endif\n\n\twrite = madvise_need_mmap_write(behavior);\n\tif (write) {\n\t\tif (mmap_write_lock_killable(mm))\n\t\t\treturn -EINTR;\n\t} else {\n\t\tmmap_read_lock(mm);\n\t}\n\n\tstart = untagged_addr_remote(mm, start);\n\tend = start + len;\n\n\tblk_start_plug(&plug);\n\terror = madvise_walk_vmas(mm, start, end, behavior,\n\t\t\tmadvise_vma_behavior);\n\tblk_finish_plug(&plug);\n\tif (write)\n\t\tmmap_write_unlock(mm);\n\telse\n\t\tmmap_read_unlock(mm);\n\n\treturn error;\n}\n\nSYSCALL_DEFINE3(madvise, unsigned long, start, size_t, len_in, int, behavior)\n{\n\treturn do_madvise(current->mm, start, len_in, behavior);\n}\n\nSYSCALL_DEFINE5(process_madvise, int, pidfd, const struct iovec __user *, vec,\n\t\tsize_t, vlen, int, behavior, unsigned int, flags)\n{\n\tssize_t ret;\n\tstruct iovec iovstack[UIO_FASTIOV];\n\tstruct iovec *iov = iovstack;\n\tstruct iov_iter iter;\n\tstruct task_struct *task;\n\tstruct mm_struct *mm;\n\tsize_t total_len;\n\tunsigned int f_flags;\n\n\tif (flags != 0) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tret = import_iovec(ITER_DEST, vec, vlen, ARRAY_SIZE(iovstack), &iov, &iter);\n\tif (ret < 0)\n\t\tgoto out;\n\n\ttask = pidfd_get_task(pidfd, &f_flags);\n\tif (IS_ERR(task)) {\n\t\tret = PTR_ERR(task);\n\t\tgoto free_iov;\n\t}\n\n\tif (!process_madvise_behavior_valid(behavior)) {\n\t\tret = -EINVAL;\n\t\tgoto release_task;\n\t}\n\n\t \n\tmm = mm_access(task, PTRACE_MODE_READ_FSCREDS);\n\tif (IS_ERR_OR_NULL(mm)) {\n\t\tret = IS_ERR(mm) ? PTR_ERR(mm) : -ESRCH;\n\t\tgoto release_task;\n\t}\n\n\t \n\tif (!capable(CAP_SYS_NICE)) {\n\t\tret = -EPERM;\n\t\tgoto release_mm;\n\t}\n\n\ttotal_len = iov_iter_count(&iter);\n\n\twhile (iov_iter_count(&iter)) {\n\t\tret = do_madvise(mm, (unsigned long)iter_iov_addr(&iter),\n\t\t\t\t\titer_iov_len(&iter), behavior);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\t\tiov_iter_advance(&iter, iter_iov_len(&iter));\n\t}\n\n\tret = (total_len - iov_iter_count(&iter)) ? : ret;\n\nrelease_mm:\n\tmmput(mm);\nrelease_task:\n\tput_task_struct(task);\nfree_iov:\n\tkfree(iov);\nout:\n\treturn ret;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}