{
  "module_name": "filemap.c",
  "hash_id": "74be8f29ebc13dcda488ffdcbd79f4307b84933f79e6d3df5582bfb2f1cb2045",
  "original_prompt": "Ingested from linux-6.6.14/mm/filemap.c",
  "human_readable_source": "\n \n\n \n#include <linux/export.h>\n#include <linux/compiler.h>\n#include <linux/dax.h>\n#include <linux/fs.h>\n#include <linux/sched/signal.h>\n#include <linux/uaccess.h>\n#include <linux/capability.h>\n#include <linux/kernel_stat.h>\n#include <linux/gfp.h>\n#include <linux/mm.h>\n#include <linux/swap.h>\n#include <linux/swapops.h>\n#include <linux/syscalls.h>\n#include <linux/mman.h>\n#include <linux/pagemap.h>\n#include <linux/file.h>\n#include <linux/uio.h>\n#include <linux/error-injection.h>\n#include <linux/hash.h>\n#include <linux/writeback.h>\n#include <linux/backing-dev.h>\n#include <linux/pagevec.h>\n#include <linux/security.h>\n#include <linux/cpuset.h>\n#include <linux/hugetlb.h>\n#include <linux/memcontrol.h>\n#include <linux/shmem_fs.h>\n#include <linux/rmap.h>\n#include <linux/delayacct.h>\n#include <linux/psi.h>\n#include <linux/ramfs.h>\n#include <linux/page_idle.h>\n#include <linux/migrate.h>\n#include <linux/pipe_fs_i.h>\n#include <linux/splice.h>\n#include <asm/pgalloc.h>\n#include <asm/tlbflush.h>\n#include \"internal.h\"\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/filemap.h>\n\n \n#include <linux/buffer_head.h>  \n\n#include <asm/mman.h>\n\n#include \"swap.h\"\n\n \n\n \n\nstatic void page_cache_delete(struct address_space *mapping,\n\t\t\t\t   struct folio *folio, void *shadow)\n{\n\tXA_STATE(xas, &mapping->i_pages, folio->index);\n\tlong nr = 1;\n\n\tmapping_set_update(&xas, mapping);\n\n\t \n\tif (!folio_test_hugetlb(folio)) {\n\t\txas_set_order(&xas, folio->index, folio_order(folio));\n\t\tnr = folio_nr_pages(folio);\n\t}\n\n\tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n\n\txas_store(&xas, shadow);\n\txas_init_marks(&xas);\n\n\tfolio->mapping = NULL;\n\t \n\tmapping->nrpages -= nr;\n}\n\nstatic void filemap_unaccount_folio(struct address_space *mapping,\n\t\tstruct folio *folio)\n{\n\tlong nr;\n\n\tVM_BUG_ON_FOLIO(folio_mapped(folio), folio);\n\tif (!IS_ENABLED(CONFIG_DEBUG_VM) && unlikely(folio_mapped(folio))) {\n\t\tpr_alert(\"BUG: Bad page cache in process %s  pfn:%05lx\\n\",\n\t\t\t current->comm, folio_pfn(folio));\n\t\tdump_page(&folio->page, \"still mapped when deleted\");\n\t\tdump_stack();\n\t\tadd_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);\n\n\t\tif (mapping_exiting(mapping) && !folio_test_large(folio)) {\n\t\t\tint mapcount = page_mapcount(&folio->page);\n\n\t\t\tif (folio_ref_count(folio) >= mapcount + 2) {\n\t\t\t\t \n\t\t\t\tpage_mapcount_reset(&folio->page);\n\t\t\t\tfolio_ref_sub(folio, mapcount);\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tif (folio_test_hugetlb(folio))\n\t\treturn;\n\n\tnr = folio_nr_pages(folio);\n\n\t__lruvec_stat_mod_folio(folio, NR_FILE_PAGES, -nr);\n\tif (folio_test_swapbacked(folio)) {\n\t\t__lruvec_stat_mod_folio(folio, NR_SHMEM, -nr);\n\t\tif (folio_test_pmd_mappable(folio))\n\t\t\t__lruvec_stat_mod_folio(folio, NR_SHMEM_THPS, -nr);\n\t} else if (folio_test_pmd_mappable(folio)) {\n\t\t__lruvec_stat_mod_folio(folio, NR_FILE_THPS, -nr);\n\t\tfilemap_nr_thps_dec(mapping);\n\t}\n\n\t \n\tif (WARN_ON_ONCE(folio_test_dirty(folio) &&\n\t\t\t mapping_can_writeback(mapping)))\n\t\tfolio_account_cleaned(folio, inode_to_wb(mapping->host));\n}\n\n \nvoid __filemap_remove_folio(struct folio *folio, void *shadow)\n{\n\tstruct address_space *mapping = folio->mapping;\n\n\ttrace_mm_filemap_delete_from_page_cache(folio);\n\tfilemap_unaccount_folio(mapping, folio);\n\tpage_cache_delete(mapping, folio, shadow);\n}\n\nvoid filemap_free_folio(struct address_space *mapping, struct folio *folio)\n{\n\tvoid (*free_folio)(struct folio *);\n\tint refs = 1;\n\n\tfree_folio = mapping->a_ops->free_folio;\n\tif (free_folio)\n\t\tfree_folio(folio);\n\n\tif (folio_test_large(folio) && !folio_test_hugetlb(folio))\n\t\trefs = folio_nr_pages(folio);\n\tfolio_put_refs(folio, refs);\n}\n\n \nvoid filemap_remove_folio(struct folio *folio)\n{\n\tstruct address_space *mapping = folio->mapping;\n\n\tBUG_ON(!folio_test_locked(folio));\n\tspin_lock(&mapping->host->i_lock);\n\txa_lock_irq(&mapping->i_pages);\n\t__filemap_remove_folio(folio, NULL);\n\txa_unlock_irq(&mapping->i_pages);\n\tif (mapping_shrinkable(mapping))\n\t\tinode_add_lru(mapping->host);\n\tspin_unlock(&mapping->host->i_lock);\n\n\tfilemap_free_folio(mapping, folio);\n}\n\n \nstatic void page_cache_delete_batch(struct address_space *mapping,\n\t\t\t     struct folio_batch *fbatch)\n{\n\tXA_STATE(xas, &mapping->i_pages, fbatch->folios[0]->index);\n\tlong total_pages = 0;\n\tint i = 0;\n\tstruct folio *folio;\n\n\tmapping_set_update(&xas, mapping);\n\txas_for_each(&xas, folio, ULONG_MAX) {\n\t\tif (i >= folio_batch_count(fbatch))\n\t\t\tbreak;\n\n\t\t \n\t\tif (xa_is_value(folio))\n\t\t\tcontinue;\n\t\t \n\t\tif (folio != fbatch->folios[i]) {\n\t\t\tVM_BUG_ON_FOLIO(folio->index >\n\t\t\t\t\tfbatch->folios[i]->index, folio);\n\t\t\tcontinue;\n\t\t}\n\n\t\tWARN_ON_ONCE(!folio_test_locked(folio));\n\n\t\tfolio->mapping = NULL;\n\t\t \n\n\t\ti++;\n\t\txas_store(&xas, NULL);\n\t\ttotal_pages += folio_nr_pages(folio);\n\t}\n\tmapping->nrpages -= total_pages;\n}\n\nvoid delete_from_page_cache_batch(struct address_space *mapping,\n\t\t\t\t  struct folio_batch *fbatch)\n{\n\tint i;\n\n\tif (!folio_batch_count(fbatch))\n\t\treturn;\n\n\tspin_lock(&mapping->host->i_lock);\n\txa_lock_irq(&mapping->i_pages);\n\tfor (i = 0; i < folio_batch_count(fbatch); i++) {\n\t\tstruct folio *folio = fbatch->folios[i];\n\n\t\ttrace_mm_filemap_delete_from_page_cache(folio);\n\t\tfilemap_unaccount_folio(mapping, folio);\n\t}\n\tpage_cache_delete_batch(mapping, fbatch);\n\txa_unlock_irq(&mapping->i_pages);\n\tif (mapping_shrinkable(mapping))\n\t\tinode_add_lru(mapping->host);\n\tspin_unlock(&mapping->host->i_lock);\n\n\tfor (i = 0; i < folio_batch_count(fbatch); i++)\n\t\tfilemap_free_folio(mapping, fbatch->folios[i]);\n}\n\nint filemap_check_errors(struct address_space *mapping)\n{\n\tint ret = 0;\n\t \n\tif (test_bit(AS_ENOSPC, &mapping->flags) &&\n\t    test_and_clear_bit(AS_ENOSPC, &mapping->flags))\n\t\tret = -ENOSPC;\n\tif (test_bit(AS_EIO, &mapping->flags) &&\n\t    test_and_clear_bit(AS_EIO, &mapping->flags))\n\t\tret = -EIO;\n\treturn ret;\n}\nEXPORT_SYMBOL(filemap_check_errors);\n\nstatic int filemap_check_and_keep_errors(struct address_space *mapping)\n{\n\t \n\tif (test_bit(AS_EIO, &mapping->flags))\n\t\treturn -EIO;\n\tif (test_bit(AS_ENOSPC, &mapping->flags))\n\t\treturn -ENOSPC;\n\treturn 0;\n}\n\n \nint filemap_fdatawrite_wbc(struct address_space *mapping,\n\t\t\t   struct writeback_control *wbc)\n{\n\tint ret;\n\n\tif (!mapping_can_writeback(mapping) ||\n\t    !mapping_tagged(mapping, PAGECACHE_TAG_DIRTY))\n\t\treturn 0;\n\n\twbc_attach_fdatawrite_inode(wbc, mapping->host);\n\tret = do_writepages(mapping, wbc);\n\twbc_detach_inode(wbc);\n\treturn ret;\n}\nEXPORT_SYMBOL(filemap_fdatawrite_wbc);\n\n \nint __filemap_fdatawrite_range(struct address_space *mapping, loff_t start,\n\t\t\t\tloff_t end, int sync_mode)\n{\n\tstruct writeback_control wbc = {\n\t\t.sync_mode = sync_mode,\n\t\t.nr_to_write = LONG_MAX,\n\t\t.range_start = start,\n\t\t.range_end = end,\n\t};\n\n\treturn filemap_fdatawrite_wbc(mapping, &wbc);\n}\n\nstatic inline int __filemap_fdatawrite(struct address_space *mapping,\n\tint sync_mode)\n{\n\treturn __filemap_fdatawrite_range(mapping, 0, LLONG_MAX, sync_mode);\n}\n\nint filemap_fdatawrite(struct address_space *mapping)\n{\n\treturn __filemap_fdatawrite(mapping, WB_SYNC_ALL);\n}\nEXPORT_SYMBOL(filemap_fdatawrite);\n\nint filemap_fdatawrite_range(struct address_space *mapping, loff_t start,\n\t\t\t\tloff_t end)\n{\n\treturn __filemap_fdatawrite_range(mapping, start, end, WB_SYNC_ALL);\n}\nEXPORT_SYMBOL(filemap_fdatawrite_range);\n\n \nint filemap_flush(struct address_space *mapping)\n{\n\treturn __filemap_fdatawrite(mapping, WB_SYNC_NONE);\n}\nEXPORT_SYMBOL(filemap_flush);\n\n \nbool filemap_range_has_page(struct address_space *mapping,\n\t\t\t   loff_t start_byte, loff_t end_byte)\n{\n\tstruct folio *folio;\n\tXA_STATE(xas, &mapping->i_pages, start_byte >> PAGE_SHIFT);\n\tpgoff_t max = end_byte >> PAGE_SHIFT;\n\n\tif (end_byte < start_byte)\n\t\treturn false;\n\n\trcu_read_lock();\n\tfor (;;) {\n\t\tfolio = xas_find(&xas, max);\n\t\tif (xas_retry(&xas, folio))\n\t\t\tcontinue;\n\t\t \n\t\tif (xa_is_value(folio))\n\t\t\tcontinue;\n\t\t \n\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\treturn folio != NULL;\n}\nEXPORT_SYMBOL(filemap_range_has_page);\n\nstatic void __filemap_fdatawait_range(struct address_space *mapping,\n\t\t\t\t     loff_t start_byte, loff_t end_byte)\n{\n\tpgoff_t index = start_byte >> PAGE_SHIFT;\n\tpgoff_t end = end_byte >> PAGE_SHIFT;\n\tstruct folio_batch fbatch;\n\tunsigned nr_folios;\n\n\tfolio_batch_init(&fbatch);\n\n\twhile (index <= end) {\n\t\tunsigned i;\n\n\t\tnr_folios = filemap_get_folios_tag(mapping, &index, end,\n\t\t\t\tPAGECACHE_TAG_WRITEBACK, &fbatch);\n\n\t\tif (!nr_folios)\n\t\t\tbreak;\n\n\t\tfor (i = 0; i < nr_folios; i++) {\n\t\t\tstruct folio *folio = fbatch.folios[i];\n\n\t\t\tfolio_wait_writeback(folio);\n\t\t\tfolio_clear_error(folio);\n\t\t}\n\t\tfolio_batch_release(&fbatch);\n\t\tcond_resched();\n\t}\n}\n\n \nint filemap_fdatawait_range(struct address_space *mapping, loff_t start_byte,\n\t\t\t    loff_t end_byte)\n{\n\t__filemap_fdatawait_range(mapping, start_byte, end_byte);\n\treturn filemap_check_errors(mapping);\n}\nEXPORT_SYMBOL(filemap_fdatawait_range);\n\n \nint filemap_fdatawait_range_keep_errors(struct address_space *mapping,\n\t\tloff_t start_byte, loff_t end_byte)\n{\n\t__filemap_fdatawait_range(mapping, start_byte, end_byte);\n\treturn filemap_check_and_keep_errors(mapping);\n}\nEXPORT_SYMBOL(filemap_fdatawait_range_keep_errors);\n\n \nint file_fdatawait_range(struct file *file, loff_t start_byte, loff_t end_byte)\n{\n\tstruct address_space *mapping = file->f_mapping;\n\n\t__filemap_fdatawait_range(mapping, start_byte, end_byte);\n\treturn file_check_and_advance_wb_err(file);\n}\nEXPORT_SYMBOL(file_fdatawait_range);\n\n \nint filemap_fdatawait_keep_errors(struct address_space *mapping)\n{\n\t__filemap_fdatawait_range(mapping, 0, LLONG_MAX);\n\treturn filemap_check_and_keep_errors(mapping);\n}\nEXPORT_SYMBOL(filemap_fdatawait_keep_errors);\n\n \nstatic bool mapping_needs_writeback(struct address_space *mapping)\n{\n\treturn mapping->nrpages;\n}\n\nbool filemap_range_has_writeback(struct address_space *mapping,\n\t\t\t\t loff_t start_byte, loff_t end_byte)\n{\n\tXA_STATE(xas, &mapping->i_pages, start_byte >> PAGE_SHIFT);\n\tpgoff_t max = end_byte >> PAGE_SHIFT;\n\tstruct folio *folio;\n\n\tif (end_byte < start_byte)\n\t\treturn false;\n\n\trcu_read_lock();\n\txas_for_each(&xas, folio, max) {\n\t\tif (xas_retry(&xas, folio))\n\t\t\tcontinue;\n\t\tif (xa_is_value(folio))\n\t\t\tcontinue;\n\t\tif (folio_test_dirty(folio) || folio_test_locked(folio) ||\n\t\t\t\tfolio_test_writeback(folio))\n\t\t\tbreak;\n\t}\n\trcu_read_unlock();\n\treturn folio != NULL;\n}\nEXPORT_SYMBOL_GPL(filemap_range_has_writeback);\n\n \nint filemap_write_and_wait_range(struct address_space *mapping,\n\t\t\t\t loff_t lstart, loff_t lend)\n{\n\tint err = 0, err2;\n\n\tif (lend < lstart)\n\t\treturn 0;\n\n\tif (mapping_needs_writeback(mapping)) {\n\t\terr = __filemap_fdatawrite_range(mapping, lstart, lend,\n\t\t\t\t\t\t WB_SYNC_ALL);\n\t\t \n\t\tif (err != -EIO)\n\t\t\t__filemap_fdatawait_range(mapping, lstart, lend);\n\t}\n\terr2 = filemap_check_errors(mapping);\n\tif (!err)\n\t\terr = err2;\n\treturn err;\n}\nEXPORT_SYMBOL(filemap_write_and_wait_range);\n\nvoid __filemap_set_wb_err(struct address_space *mapping, int err)\n{\n\terrseq_t eseq = errseq_set(&mapping->wb_err, err);\n\n\ttrace_filemap_set_wb_err(mapping, eseq);\n}\nEXPORT_SYMBOL(__filemap_set_wb_err);\n\n \nint file_check_and_advance_wb_err(struct file *file)\n{\n\tint err = 0;\n\terrseq_t old = READ_ONCE(file->f_wb_err);\n\tstruct address_space *mapping = file->f_mapping;\n\n\t \n\tif (errseq_check(&mapping->wb_err, old)) {\n\t\t \n\t\tspin_lock(&file->f_lock);\n\t\told = file->f_wb_err;\n\t\terr = errseq_check_and_advance(&mapping->wb_err,\n\t\t\t\t\t\t&file->f_wb_err);\n\t\ttrace_file_check_and_advance_wb_err(file, old);\n\t\tspin_unlock(&file->f_lock);\n\t}\n\n\t \n\tclear_bit(AS_EIO, &mapping->flags);\n\tclear_bit(AS_ENOSPC, &mapping->flags);\n\treturn err;\n}\nEXPORT_SYMBOL(file_check_and_advance_wb_err);\n\n \nint file_write_and_wait_range(struct file *file, loff_t lstart, loff_t lend)\n{\n\tint err = 0, err2;\n\tstruct address_space *mapping = file->f_mapping;\n\n\tif (lend < lstart)\n\t\treturn 0;\n\n\tif (mapping_needs_writeback(mapping)) {\n\t\terr = __filemap_fdatawrite_range(mapping, lstart, lend,\n\t\t\t\t\t\t WB_SYNC_ALL);\n\t\t \n\t\tif (err != -EIO)\n\t\t\t__filemap_fdatawait_range(mapping, lstart, lend);\n\t}\n\terr2 = file_check_and_advance_wb_err(file);\n\tif (!err)\n\t\terr = err2;\n\treturn err;\n}\nEXPORT_SYMBOL(file_write_and_wait_range);\n\n \nvoid replace_page_cache_folio(struct folio *old, struct folio *new)\n{\n\tstruct address_space *mapping = old->mapping;\n\tvoid (*free_folio)(struct folio *) = mapping->a_ops->free_folio;\n\tpgoff_t offset = old->index;\n\tXA_STATE(xas, &mapping->i_pages, offset);\n\n\tVM_BUG_ON_FOLIO(!folio_test_locked(old), old);\n\tVM_BUG_ON_FOLIO(!folio_test_locked(new), new);\n\tVM_BUG_ON_FOLIO(new->mapping, new);\n\n\tfolio_get(new);\n\tnew->mapping = mapping;\n\tnew->index = offset;\n\n\tmem_cgroup_migrate(old, new);\n\n\txas_lock_irq(&xas);\n\txas_store(&xas, new);\n\n\told->mapping = NULL;\n\t \n\tif (!folio_test_hugetlb(old))\n\t\t__lruvec_stat_sub_folio(old, NR_FILE_PAGES);\n\tif (!folio_test_hugetlb(new))\n\t\t__lruvec_stat_add_folio(new, NR_FILE_PAGES);\n\tif (folio_test_swapbacked(old))\n\t\t__lruvec_stat_sub_folio(old, NR_SHMEM);\n\tif (folio_test_swapbacked(new))\n\t\t__lruvec_stat_add_folio(new, NR_SHMEM);\n\txas_unlock_irq(&xas);\n\tif (free_folio)\n\t\tfree_folio(old);\n\tfolio_put(old);\n}\nEXPORT_SYMBOL_GPL(replace_page_cache_folio);\n\nnoinline int __filemap_add_folio(struct address_space *mapping,\n\t\tstruct folio *folio, pgoff_t index, gfp_t gfp, void **shadowp)\n{\n\tXA_STATE(xas, &mapping->i_pages, index);\n\tint huge = folio_test_hugetlb(folio);\n\tbool charged = false;\n\tlong nr = 1;\n\n\tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n\tVM_BUG_ON_FOLIO(folio_test_swapbacked(folio), folio);\n\tmapping_set_update(&xas, mapping);\n\n\tif (!huge) {\n\t\tint error = mem_cgroup_charge(folio, NULL, gfp);\n\t\tVM_BUG_ON_FOLIO(index & (folio_nr_pages(folio) - 1), folio);\n\t\tif (error)\n\t\t\treturn error;\n\t\tcharged = true;\n\t\txas_set_order(&xas, index, folio_order(folio));\n\t\tnr = folio_nr_pages(folio);\n\t}\n\n\tgfp &= GFP_RECLAIM_MASK;\n\tfolio_ref_add(folio, nr);\n\tfolio->mapping = mapping;\n\tfolio->index = xas.xa_index;\n\n\tdo {\n\t\tunsigned int order = xa_get_order(xas.xa, xas.xa_index);\n\t\tvoid *entry, *old = NULL;\n\n\t\tif (order > folio_order(folio))\n\t\t\txas_split_alloc(&xas, xa_load(xas.xa, xas.xa_index),\n\t\t\t\t\torder, gfp);\n\t\txas_lock_irq(&xas);\n\t\txas_for_each_conflict(&xas, entry) {\n\t\t\told = entry;\n\t\t\tif (!xa_is_value(entry)) {\n\t\t\t\txas_set_err(&xas, -EEXIST);\n\t\t\t\tgoto unlock;\n\t\t\t}\n\t\t}\n\n\t\tif (old) {\n\t\t\tif (shadowp)\n\t\t\t\t*shadowp = old;\n\t\t\t \n\t\t\torder = xa_get_order(xas.xa, xas.xa_index);\n\t\t\tif (order > folio_order(folio)) {\n\t\t\t\t \n\t\t\t\tBUG_ON(shmem_mapping(mapping));\n\t\t\t\txas_split(&xas, old, order);\n\t\t\t\txas_reset(&xas);\n\t\t\t}\n\t\t}\n\n\t\txas_store(&xas, folio);\n\t\tif (xas_error(&xas))\n\t\t\tgoto unlock;\n\n\t\tmapping->nrpages += nr;\n\n\t\t \n\t\tif (!huge) {\n\t\t\t__lruvec_stat_mod_folio(folio, NR_FILE_PAGES, nr);\n\t\t\tif (folio_test_pmd_mappable(folio))\n\t\t\t\t__lruvec_stat_mod_folio(folio,\n\t\t\t\t\t\tNR_FILE_THPS, nr);\n\t\t}\nunlock:\n\t\txas_unlock_irq(&xas);\n\t} while (xas_nomem(&xas, gfp));\n\n\tif (xas_error(&xas))\n\t\tgoto error;\n\n\ttrace_mm_filemap_add_to_page_cache(folio);\n\treturn 0;\nerror:\n\tif (charged)\n\t\tmem_cgroup_uncharge(folio);\n\tfolio->mapping = NULL;\n\t \n\tfolio_put_refs(folio, nr);\n\treturn xas_error(&xas);\n}\nALLOW_ERROR_INJECTION(__filemap_add_folio, ERRNO);\n\nint filemap_add_folio(struct address_space *mapping, struct folio *folio,\n\t\t\t\tpgoff_t index, gfp_t gfp)\n{\n\tvoid *shadow = NULL;\n\tint ret;\n\n\t__folio_set_locked(folio);\n\tret = __filemap_add_folio(mapping, folio, index, gfp, &shadow);\n\tif (unlikely(ret))\n\t\t__folio_clear_locked(folio);\n\telse {\n\t\t \n\t\tWARN_ON_ONCE(folio_test_active(folio));\n\t\tif (!(gfp & __GFP_WRITE) && shadow)\n\t\t\tworkingset_refault(folio, shadow);\n\t\tfolio_add_lru(folio);\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(filemap_add_folio);\n\n#ifdef CONFIG_NUMA\nstruct folio *filemap_alloc_folio(gfp_t gfp, unsigned int order)\n{\n\tint n;\n\tstruct folio *folio;\n\n\tif (cpuset_do_page_mem_spread()) {\n\t\tunsigned int cpuset_mems_cookie;\n\t\tdo {\n\t\t\tcpuset_mems_cookie = read_mems_allowed_begin();\n\t\t\tn = cpuset_mem_spread_node();\n\t\t\tfolio = __folio_alloc_node(gfp, order, n);\n\t\t} while (!folio && read_mems_allowed_retry(cpuset_mems_cookie));\n\n\t\treturn folio;\n\t}\n\treturn folio_alloc(gfp, order);\n}\nEXPORT_SYMBOL(filemap_alloc_folio);\n#endif\n\n \nvoid filemap_invalidate_lock_two(struct address_space *mapping1,\n\t\t\t\t struct address_space *mapping2)\n{\n\tif (mapping1 > mapping2)\n\t\tswap(mapping1, mapping2);\n\tif (mapping1)\n\t\tdown_write(&mapping1->invalidate_lock);\n\tif (mapping2 && mapping1 != mapping2)\n\t\tdown_write_nested(&mapping2->invalidate_lock, 1);\n}\nEXPORT_SYMBOL(filemap_invalidate_lock_two);\n\n \nvoid filemap_invalidate_unlock_two(struct address_space *mapping1,\n\t\t\t\t   struct address_space *mapping2)\n{\n\tif (mapping1)\n\t\tup_write(&mapping1->invalidate_lock);\n\tif (mapping2 && mapping1 != mapping2)\n\t\tup_write(&mapping2->invalidate_lock);\n}\nEXPORT_SYMBOL(filemap_invalidate_unlock_two);\n\n \n#define PAGE_WAIT_TABLE_BITS 8\n#define PAGE_WAIT_TABLE_SIZE (1 << PAGE_WAIT_TABLE_BITS)\nstatic wait_queue_head_t folio_wait_table[PAGE_WAIT_TABLE_SIZE] __cacheline_aligned;\n\nstatic wait_queue_head_t *folio_waitqueue(struct folio *folio)\n{\n\treturn &folio_wait_table[hash_ptr(folio, PAGE_WAIT_TABLE_BITS)];\n}\n\nvoid __init pagecache_init(void)\n{\n\tint i;\n\n\tfor (i = 0; i < PAGE_WAIT_TABLE_SIZE; i++)\n\t\tinit_waitqueue_head(&folio_wait_table[i]);\n\n\tpage_writeback_init();\n}\n\n \nstatic int wake_page_function(wait_queue_entry_t *wait, unsigned mode, int sync, void *arg)\n{\n\tunsigned int flags;\n\tstruct wait_page_key *key = arg;\n\tstruct wait_page_queue *wait_page\n\t\t= container_of(wait, struct wait_page_queue, wait);\n\n\tif (!wake_page_match(wait_page, key))\n\t\treturn 0;\n\n\t \n\tflags = wait->flags;\n\tif (flags & WQ_FLAG_EXCLUSIVE) {\n\t\tif (test_bit(key->bit_nr, &key->folio->flags))\n\t\t\treturn -1;\n\t\tif (flags & WQ_FLAG_CUSTOM) {\n\t\t\tif (test_and_set_bit(key->bit_nr, &key->folio->flags))\n\t\t\t\treturn -1;\n\t\t\tflags |= WQ_FLAG_DONE;\n\t\t}\n\t}\n\n\t \n\tsmp_store_release(&wait->flags, flags | WQ_FLAG_WOKEN);\n\twake_up_state(wait->private, mode);\n\n\t \n\tlist_del_init_careful(&wait->entry);\n\treturn (flags & WQ_FLAG_EXCLUSIVE) != 0;\n}\n\nstatic void folio_wake_bit(struct folio *folio, int bit_nr)\n{\n\twait_queue_head_t *q = folio_waitqueue(folio);\n\tstruct wait_page_key key;\n\tunsigned long flags;\n\twait_queue_entry_t bookmark;\n\n\tkey.folio = folio;\n\tkey.bit_nr = bit_nr;\n\tkey.page_match = 0;\n\n\tbookmark.flags = 0;\n\tbookmark.private = NULL;\n\tbookmark.func = NULL;\n\tINIT_LIST_HEAD(&bookmark.entry);\n\n\tspin_lock_irqsave(&q->lock, flags);\n\t__wake_up_locked_key_bookmark(q, TASK_NORMAL, &key, &bookmark);\n\n\twhile (bookmark.flags & WQ_FLAG_BOOKMARK) {\n\t\t \n\t\tspin_unlock_irqrestore(&q->lock, flags);\n\t\tcpu_relax();\n\t\tspin_lock_irqsave(&q->lock, flags);\n\t\t__wake_up_locked_key_bookmark(q, TASK_NORMAL, &key, &bookmark);\n\t}\n\n\t \n\tif (!waitqueue_active(q) || !key.page_match)\n\t\tfolio_clear_waiters(folio);\n\n\tspin_unlock_irqrestore(&q->lock, flags);\n}\n\nstatic void folio_wake(struct folio *folio, int bit)\n{\n\tif (!folio_test_waiters(folio))\n\t\treturn;\n\tfolio_wake_bit(folio, bit);\n}\n\n \nenum behavior {\n\tEXCLUSIVE,\t \n\tSHARED,\t\t \n\tDROP,\t\t \n};\n\n \nstatic inline bool folio_trylock_flag(struct folio *folio, int bit_nr,\n\t\t\t\t\tstruct wait_queue_entry *wait)\n{\n\tif (wait->flags & WQ_FLAG_EXCLUSIVE) {\n\t\tif (test_and_set_bit(bit_nr, &folio->flags))\n\t\t\treturn false;\n\t} else if (test_bit(bit_nr, &folio->flags))\n\t\treturn false;\n\n\twait->flags |= WQ_FLAG_WOKEN | WQ_FLAG_DONE;\n\treturn true;\n}\n\n \nint sysctl_page_lock_unfairness = 5;\n\nstatic inline int folio_wait_bit_common(struct folio *folio, int bit_nr,\n\t\tint state, enum behavior behavior)\n{\n\twait_queue_head_t *q = folio_waitqueue(folio);\n\tint unfairness = sysctl_page_lock_unfairness;\n\tstruct wait_page_queue wait_page;\n\twait_queue_entry_t *wait = &wait_page.wait;\n\tbool thrashing = false;\n\tunsigned long pflags;\n\tbool in_thrashing;\n\n\tif (bit_nr == PG_locked &&\n\t    !folio_test_uptodate(folio) && folio_test_workingset(folio)) {\n\t\tdelayacct_thrashing_start(&in_thrashing);\n\t\tpsi_memstall_enter(&pflags);\n\t\tthrashing = true;\n\t}\n\n\tinit_wait(wait);\n\twait->func = wake_page_function;\n\twait_page.folio = folio;\n\twait_page.bit_nr = bit_nr;\n\nrepeat:\n\twait->flags = 0;\n\tif (behavior == EXCLUSIVE) {\n\t\twait->flags = WQ_FLAG_EXCLUSIVE;\n\t\tif (--unfairness < 0)\n\t\t\twait->flags |= WQ_FLAG_CUSTOM;\n\t}\n\n\t \n\tspin_lock_irq(&q->lock);\n\tfolio_set_waiters(folio);\n\tif (!folio_trylock_flag(folio, bit_nr, wait))\n\t\t__add_wait_queue_entry_tail(q, wait);\n\tspin_unlock_irq(&q->lock);\n\n\t \n\tif (behavior == DROP)\n\t\tfolio_put(folio);\n\n\t \n\tfor (;;) {\n\t\tunsigned int flags;\n\n\t\tset_current_state(state);\n\n\t\t \n\t\tflags = smp_load_acquire(&wait->flags);\n\t\tif (!(flags & WQ_FLAG_WOKEN)) {\n\t\t\tif (signal_pending_state(state, current))\n\t\t\t\tbreak;\n\n\t\t\tio_schedule();\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (behavior != EXCLUSIVE)\n\t\t\tbreak;\n\n\t\t \n\t\tif (flags & WQ_FLAG_DONE)\n\t\t\tbreak;\n\n\t\t \n\t\tif (unlikely(test_and_set_bit(bit_nr, folio_flags(folio, 0))))\n\t\t\tgoto repeat;\n\n\t\twait->flags |= WQ_FLAG_DONE;\n\t\tbreak;\n\t}\n\n\t \n\tfinish_wait(q, wait);\n\n\tif (thrashing) {\n\t\tdelayacct_thrashing_end(&in_thrashing);\n\t\tpsi_memstall_leave(&pflags);\n\t}\n\n\t \n\tif (behavior == EXCLUSIVE)\n\t\treturn wait->flags & WQ_FLAG_DONE ? 0 : -EINTR;\n\n\treturn wait->flags & WQ_FLAG_WOKEN ? 0 : -EINTR;\n}\n\n#ifdef CONFIG_MIGRATION\n \nvoid migration_entry_wait_on_locked(swp_entry_t entry, spinlock_t *ptl)\n\t__releases(ptl)\n{\n\tstruct wait_page_queue wait_page;\n\twait_queue_entry_t *wait = &wait_page.wait;\n\tbool thrashing = false;\n\tunsigned long pflags;\n\tbool in_thrashing;\n\twait_queue_head_t *q;\n\tstruct folio *folio = page_folio(pfn_swap_entry_to_page(entry));\n\n\tq = folio_waitqueue(folio);\n\tif (!folio_test_uptodate(folio) && folio_test_workingset(folio)) {\n\t\tdelayacct_thrashing_start(&in_thrashing);\n\t\tpsi_memstall_enter(&pflags);\n\t\tthrashing = true;\n\t}\n\n\tinit_wait(wait);\n\twait->func = wake_page_function;\n\twait_page.folio = folio;\n\twait_page.bit_nr = PG_locked;\n\twait->flags = 0;\n\n\tspin_lock_irq(&q->lock);\n\tfolio_set_waiters(folio);\n\tif (!folio_trylock_flag(folio, PG_locked, wait))\n\t\t__add_wait_queue_entry_tail(q, wait);\n\tspin_unlock_irq(&q->lock);\n\n\t \n\tspin_unlock(ptl);\n\n\tfor (;;) {\n\t\tunsigned int flags;\n\n\t\tset_current_state(TASK_UNINTERRUPTIBLE);\n\n\t\t \n\t\tflags = smp_load_acquire(&wait->flags);\n\t\tif (!(flags & WQ_FLAG_WOKEN)) {\n\t\t\tif (signal_pending_state(TASK_UNINTERRUPTIBLE, current))\n\t\t\t\tbreak;\n\n\t\t\tio_schedule();\n\t\t\tcontinue;\n\t\t}\n\t\tbreak;\n\t}\n\n\tfinish_wait(q, wait);\n\n\tif (thrashing) {\n\t\tdelayacct_thrashing_end(&in_thrashing);\n\t\tpsi_memstall_leave(&pflags);\n\t}\n}\n#endif\n\nvoid folio_wait_bit(struct folio *folio, int bit_nr)\n{\n\tfolio_wait_bit_common(folio, bit_nr, TASK_UNINTERRUPTIBLE, SHARED);\n}\nEXPORT_SYMBOL(folio_wait_bit);\n\nint folio_wait_bit_killable(struct folio *folio, int bit_nr)\n{\n\treturn folio_wait_bit_common(folio, bit_nr, TASK_KILLABLE, SHARED);\n}\nEXPORT_SYMBOL(folio_wait_bit_killable);\n\n \nstatic int folio_put_wait_locked(struct folio *folio, int state)\n{\n\treturn folio_wait_bit_common(folio, PG_locked, state, DROP);\n}\n\n \nvoid folio_add_wait_queue(struct folio *folio, wait_queue_entry_t *waiter)\n{\n\twait_queue_head_t *q = folio_waitqueue(folio);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&q->lock, flags);\n\t__add_wait_queue_entry_tail(q, waiter);\n\tfolio_set_waiters(folio);\n\tspin_unlock_irqrestore(&q->lock, flags);\n}\nEXPORT_SYMBOL_GPL(folio_add_wait_queue);\n\n#ifndef clear_bit_unlock_is_negative_byte\n\n \nstatic inline bool clear_bit_unlock_is_negative_byte(long nr, volatile void *mem)\n{\n\tclear_bit_unlock(nr, mem);\n\t \n\treturn test_bit(PG_waiters, mem);\n}\n\n#endif\n\n \nvoid folio_unlock(struct folio *folio)\n{\n\t \n\tBUILD_BUG_ON(PG_waiters != 7);\n\tBUILD_BUG_ON(PG_locked > 7);\n\tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n\tif (clear_bit_unlock_is_negative_byte(PG_locked, folio_flags(folio, 0)))\n\t\tfolio_wake_bit(folio, PG_locked);\n}\nEXPORT_SYMBOL(folio_unlock);\n\n \nvoid folio_end_private_2(struct folio *folio)\n{\n\tVM_BUG_ON_FOLIO(!folio_test_private_2(folio), folio);\n\tclear_bit_unlock(PG_private_2, folio_flags(folio, 0));\n\tfolio_wake_bit(folio, PG_private_2);\n\tfolio_put(folio);\n}\nEXPORT_SYMBOL(folio_end_private_2);\n\n \nvoid folio_wait_private_2(struct folio *folio)\n{\n\twhile (folio_test_private_2(folio))\n\t\tfolio_wait_bit(folio, PG_private_2);\n}\nEXPORT_SYMBOL(folio_wait_private_2);\n\n \nint folio_wait_private_2_killable(struct folio *folio)\n{\n\tint ret = 0;\n\n\twhile (folio_test_private_2(folio)) {\n\t\tret = folio_wait_bit_killable(folio, PG_private_2);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\t}\n\n\treturn ret;\n}\nEXPORT_SYMBOL(folio_wait_private_2_killable);\n\n \nvoid folio_end_writeback(struct folio *folio)\n{\n\t \n\tif (folio_test_reclaim(folio)) {\n\t\tfolio_clear_reclaim(folio);\n\t\tfolio_rotate_reclaimable(folio);\n\t}\n\n\t \n\tfolio_get(folio);\n\tif (!__folio_end_writeback(folio))\n\t\tBUG();\n\n\tsmp_mb__after_atomic();\n\tfolio_wake(folio, PG_writeback);\n\tacct_reclaim_writeback(folio);\n\tfolio_put(folio);\n}\nEXPORT_SYMBOL(folio_end_writeback);\n\n \nvoid __folio_lock(struct folio *folio)\n{\n\tfolio_wait_bit_common(folio, PG_locked, TASK_UNINTERRUPTIBLE,\n\t\t\t\tEXCLUSIVE);\n}\nEXPORT_SYMBOL(__folio_lock);\n\nint __folio_lock_killable(struct folio *folio)\n{\n\treturn folio_wait_bit_common(folio, PG_locked, TASK_KILLABLE,\n\t\t\t\t\tEXCLUSIVE);\n}\nEXPORT_SYMBOL_GPL(__folio_lock_killable);\n\nstatic int __folio_lock_async(struct folio *folio, struct wait_page_queue *wait)\n{\n\tstruct wait_queue_head *q = folio_waitqueue(folio);\n\tint ret = 0;\n\n\twait->folio = folio;\n\twait->bit_nr = PG_locked;\n\n\tspin_lock_irq(&q->lock);\n\t__add_wait_queue_entry_tail(q, &wait->wait);\n\tfolio_set_waiters(folio);\n\tret = !folio_trylock(folio);\n\t \n\tif (!ret)\n\t\t__remove_wait_queue(q, &wait->wait);\n\telse\n\t\tret = -EIOCBQUEUED;\n\tspin_unlock_irq(&q->lock);\n\treturn ret;\n}\n\n \nvm_fault_t __folio_lock_or_retry(struct folio *folio, struct vm_fault *vmf)\n{\n\tunsigned int flags = vmf->flags;\n\n\tif (fault_flag_allow_retry_first(flags)) {\n\t\t \n\t\tif (flags & FAULT_FLAG_RETRY_NOWAIT)\n\t\t\treturn VM_FAULT_RETRY;\n\n\t\trelease_fault_lock(vmf);\n\t\tif (flags & FAULT_FLAG_KILLABLE)\n\t\t\tfolio_wait_locked_killable(folio);\n\t\telse\n\t\t\tfolio_wait_locked(folio);\n\t\treturn VM_FAULT_RETRY;\n\t}\n\tif (flags & FAULT_FLAG_KILLABLE) {\n\t\tbool ret;\n\n\t\tret = __folio_lock_killable(folio);\n\t\tif (ret) {\n\t\t\trelease_fault_lock(vmf);\n\t\t\treturn VM_FAULT_RETRY;\n\t\t}\n\t} else {\n\t\t__folio_lock(folio);\n\t}\n\n\treturn 0;\n}\n\n \npgoff_t page_cache_next_miss(struct address_space *mapping,\n\t\t\t     pgoff_t index, unsigned long max_scan)\n{\n\tXA_STATE(xas, &mapping->i_pages, index);\n\n\twhile (max_scan--) {\n\t\tvoid *entry = xas_next(&xas);\n\t\tif (!entry || xa_is_value(entry))\n\t\t\tbreak;\n\t\tif (xas.xa_index == 0)\n\t\t\tbreak;\n\t}\n\n\treturn xas.xa_index;\n}\nEXPORT_SYMBOL(page_cache_next_miss);\n\n \npgoff_t page_cache_prev_miss(struct address_space *mapping,\n\t\t\t     pgoff_t index, unsigned long max_scan)\n{\n\tXA_STATE(xas, &mapping->i_pages, index);\n\n\twhile (max_scan--) {\n\t\tvoid *entry = xas_prev(&xas);\n\t\tif (!entry || xa_is_value(entry))\n\t\t\tbreak;\n\t\tif (xas.xa_index == ULONG_MAX)\n\t\t\tbreak;\n\t}\n\n\treturn xas.xa_index;\n}\nEXPORT_SYMBOL(page_cache_prev_miss);\n\n \n\n \nvoid *filemap_get_entry(struct address_space *mapping, pgoff_t index)\n{\n\tXA_STATE(xas, &mapping->i_pages, index);\n\tstruct folio *folio;\n\n\trcu_read_lock();\nrepeat:\n\txas_reset(&xas);\n\tfolio = xas_load(&xas);\n\tif (xas_retry(&xas, folio))\n\t\tgoto repeat;\n\t \n\tif (!folio || xa_is_value(folio))\n\t\tgoto out;\n\n\tif (!folio_try_get_rcu(folio))\n\t\tgoto repeat;\n\n\tif (unlikely(folio != xas_reload(&xas))) {\n\t\tfolio_put(folio);\n\t\tgoto repeat;\n\t}\nout:\n\trcu_read_unlock();\n\n\treturn folio;\n}\n\n \nstruct folio *__filemap_get_folio(struct address_space *mapping, pgoff_t index,\n\t\tfgf_t fgp_flags, gfp_t gfp)\n{\n\tstruct folio *folio;\n\nrepeat:\n\tfolio = filemap_get_entry(mapping, index);\n\tif (xa_is_value(folio))\n\t\tfolio = NULL;\n\tif (!folio)\n\t\tgoto no_page;\n\n\tif (fgp_flags & FGP_LOCK) {\n\t\tif (fgp_flags & FGP_NOWAIT) {\n\t\t\tif (!folio_trylock(folio)) {\n\t\t\t\tfolio_put(folio);\n\t\t\t\treturn ERR_PTR(-EAGAIN);\n\t\t\t}\n\t\t} else {\n\t\t\tfolio_lock(folio);\n\t\t}\n\n\t\t \n\t\tif (unlikely(folio->mapping != mapping)) {\n\t\t\tfolio_unlock(folio);\n\t\t\tfolio_put(folio);\n\t\t\tgoto repeat;\n\t\t}\n\t\tVM_BUG_ON_FOLIO(!folio_contains(folio, index), folio);\n\t}\n\n\tif (fgp_flags & FGP_ACCESSED)\n\t\tfolio_mark_accessed(folio);\n\telse if (fgp_flags & FGP_WRITE) {\n\t\t \n\t\tif (folio_test_idle(folio))\n\t\t\tfolio_clear_idle(folio);\n\t}\n\n\tif (fgp_flags & FGP_STABLE)\n\t\tfolio_wait_stable(folio);\nno_page:\n\tif (!folio && (fgp_flags & FGP_CREAT)) {\n\t\tunsigned order = FGF_GET_ORDER(fgp_flags);\n\t\tint err;\n\n\t\tif ((fgp_flags & FGP_WRITE) && mapping_can_writeback(mapping))\n\t\t\tgfp |= __GFP_WRITE;\n\t\tif (fgp_flags & FGP_NOFS)\n\t\t\tgfp &= ~__GFP_FS;\n\t\tif (fgp_flags & FGP_NOWAIT) {\n\t\t\tgfp &= ~GFP_KERNEL;\n\t\t\tgfp |= GFP_NOWAIT | __GFP_NOWARN;\n\t\t}\n\t\tif (WARN_ON_ONCE(!(fgp_flags & (FGP_LOCK | FGP_FOR_MMAP))))\n\t\t\tfgp_flags |= FGP_LOCK;\n\n\t\tif (!mapping_large_folio_support(mapping))\n\t\t\torder = 0;\n\t\tif (order > MAX_PAGECACHE_ORDER)\n\t\t\torder = MAX_PAGECACHE_ORDER;\n\t\t \n\t\tif (index & ((1UL << order) - 1))\n\t\t\torder = __ffs(index);\n\n\t\tdo {\n\t\t\tgfp_t alloc_gfp = gfp;\n\n\t\t\terr = -ENOMEM;\n\t\t\tif (order == 1)\n\t\t\t\torder = 0;\n\t\t\tif (order > 0)\n\t\t\t\talloc_gfp |= __GFP_NORETRY | __GFP_NOWARN;\n\t\t\tfolio = filemap_alloc_folio(alloc_gfp, order);\n\t\t\tif (!folio)\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tif (fgp_flags & FGP_ACCESSED)\n\t\t\t\t__folio_set_referenced(folio);\n\n\t\t\terr = filemap_add_folio(mapping, folio, index, gfp);\n\t\t\tif (!err)\n\t\t\t\tbreak;\n\t\t\tfolio_put(folio);\n\t\t\tfolio = NULL;\n\t\t} while (order-- > 0);\n\n\t\tif (err == -EEXIST)\n\t\t\tgoto repeat;\n\t\tif (err)\n\t\t\treturn ERR_PTR(err);\n\t\t \n\t\tif (folio && (fgp_flags & FGP_FOR_MMAP))\n\t\t\tfolio_unlock(folio);\n\t}\n\n\tif (!folio)\n\t\treturn ERR_PTR(-ENOENT);\n\treturn folio;\n}\nEXPORT_SYMBOL(__filemap_get_folio);\n\nstatic inline struct folio *find_get_entry(struct xa_state *xas, pgoff_t max,\n\t\txa_mark_t mark)\n{\n\tstruct folio *folio;\n\nretry:\n\tif (mark == XA_PRESENT)\n\t\tfolio = xas_find(xas, max);\n\telse\n\t\tfolio = xas_find_marked(xas, max, mark);\n\n\tif (xas_retry(xas, folio))\n\t\tgoto retry;\n\t \n\tif (!folio || xa_is_value(folio))\n\t\treturn folio;\n\n\tif (!folio_try_get_rcu(folio))\n\t\tgoto reset;\n\n\tif (unlikely(folio != xas_reload(xas))) {\n\t\tfolio_put(folio);\n\t\tgoto reset;\n\t}\n\n\treturn folio;\nreset:\n\txas_reset(xas);\n\tgoto retry;\n}\n\n \nunsigned find_get_entries(struct address_space *mapping, pgoff_t *start,\n\t\tpgoff_t end, struct folio_batch *fbatch, pgoff_t *indices)\n{\n\tXA_STATE(xas, &mapping->i_pages, *start);\n\tstruct folio *folio;\n\n\trcu_read_lock();\n\twhile ((folio = find_get_entry(&xas, end, XA_PRESENT)) != NULL) {\n\t\tindices[fbatch->nr] = xas.xa_index;\n\t\tif (!folio_batch_add(fbatch, folio))\n\t\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\tif (folio_batch_count(fbatch)) {\n\t\tunsigned long nr = 1;\n\t\tint idx = folio_batch_count(fbatch) - 1;\n\n\t\tfolio = fbatch->folios[idx];\n\t\tif (!xa_is_value(folio) && !folio_test_hugetlb(folio))\n\t\t\tnr = folio_nr_pages(folio);\n\t\t*start = indices[idx] + nr;\n\t}\n\treturn folio_batch_count(fbatch);\n}\n\n \nunsigned find_lock_entries(struct address_space *mapping, pgoff_t *start,\n\t\tpgoff_t end, struct folio_batch *fbatch, pgoff_t *indices)\n{\n\tXA_STATE(xas, &mapping->i_pages, *start);\n\tstruct folio *folio;\n\n\trcu_read_lock();\n\twhile ((folio = find_get_entry(&xas, end, XA_PRESENT))) {\n\t\tif (!xa_is_value(folio)) {\n\t\t\tif (folio->index < *start)\n\t\t\t\tgoto put;\n\t\t\tif (folio_next_index(folio) - 1 > end)\n\t\t\t\tgoto put;\n\t\t\tif (!folio_trylock(folio))\n\t\t\t\tgoto put;\n\t\t\tif (folio->mapping != mapping ||\n\t\t\t    folio_test_writeback(folio))\n\t\t\t\tgoto unlock;\n\t\t\tVM_BUG_ON_FOLIO(!folio_contains(folio, xas.xa_index),\n\t\t\t\t\tfolio);\n\t\t}\n\t\tindices[fbatch->nr] = xas.xa_index;\n\t\tif (!folio_batch_add(fbatch, folio))\n\t\t\tbreak;\n\t\tcontinue;\nunlock:\n\t\tfolio_unlock(folio);\nput:\n\t\tfolio_put(folio);\n\t}\n\trcu_read_unlock();\n\n\tif (folio_batch_count(fbatch)) {\n\t\tunsigned long nr = 1;\n\t\tint idx = folio_batch_count(fbatch) - 1;\n\n\t\tfolio = fbatch->folios[idx];\n\t\tif (!xa_is_value(folio) && !folio_test_hugetlb(folio))\n\t\t\tnr = folio_nr_pages(folio);\n\t\t*start = indices[idx] + nr;\n\t}\n\treturn folio_batch_count(fbatch);\n}\n\n \nunsigned filemap_get_folios(struct address_space *mapping, pgoff_t *start,\n\t\tpgoff_t end, struct folio_batch *fbatch)\n{\n\tXA_STATE(xas, &mapping->i_pages, *start);\n\tstruct folio *folio;\n\n\trcu_read_lock();\n\twhile ((folio = find_get_entry(&xas, end, XA_PRESENT)) != NULL) {\n\t\t \n\t\tif (xa_is_value(folio))\n\t\t\tcontinue;\n\t\tif (!folio_batch_add(fbatch, folio)) {\n\t\t\tunsigned long nr = folio_nr_pages(folio);\n\n\t\t\tif (folio_test_hugetlb(folio))\n\t\t\t\tnr = 1;\n\t\t\t*start = folio->index + nr;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t \n\tif (end == (pgoff_t)-1)\n\t\t*start = (pgoff_t)-1;\n\telse\n\t\t*start = end + 1;\nout:\n\trcu_read_unlock();\n\n\treturn folio_batch_count(fbatch);\n}\nEXPORT_SYMBOL(filemap_get_folios);\n\n \n\nunsigned filemap_get_folios_contig(struct address_space *mapping,\n\t\tpgoff_t *start, pgoff_t end, struct folio_batch *fbatch)\n{\n\tXA_STATE(xas, &mapping->i_pages, *start);\n\tunsigned long nr;\n\tstruct folio *folio;\n\n\trcu_read_lock();\n\n\tfor (folio = xas_load(&xas); folio && xas.xa_index <= end;\n\t\t\tfolio = xas_next(&xas)) {\n\t\tif (xas_retry(&xas, folio))\n\t\t\tcontinue;\n\t\t \n\t\tif (xa_is_value(folio))\n\t\t\tgoto update_start;\n\n\t\tif (!folio_try_get_rcu(folio))\n\t\t\tgoto retry;\n\n\t\tif (unlikely(folio != xas_reload(&xas)))\n\t\t\tgoto put_folio;\n\n\t\tif (!folio_batch_add(fbatch, folio)) {\n\t\t\tnr = folio_nr_pages(folio);\n\n\t\t\tif (folio_test_hugetlb(folio))\n\t\t\t\tnr = 1;\n\t\t\t*start = folio->index + nr;\n\t\t\tgoto out;\n\t\t}\n\t\tcontinue;\nput_folio:\n\t\tfolio_put(folio);\n\nretry:\n\t\txas_reset(&xas);\n\t}\n\nupdate_start:\n\tnr = folio_batch_count(fbatch);\n\n\tif (nr) {\n\t\tfolio = fbatch->folios[nr - 1];\n\t\tif (folio_test_hugetlb(folio))\n\t\t\t*start = folio->index + 1;\n\t\telse\n\t\t\t*start = folio_next_index(folio);\n\t}\nout:\n\trcu_read_unlock();\n\treturn folio_batch_count(fbatch);\n}\nEXPORT_SYMBOL(filemap_get_folios_contig);\n\n \nunsigned filemap_get_folios_tag(struct address_space *mapping, pgoff_t *start,\n\t\t\tpgoff_t end, xa_mark_t tag, struct folio_batch *fbatch)\n{\n\tXA_STATE(xas, &mapping->i_pages, *start);\n\tstruct folio *folio;\n\n\trcu_read_lock();\n\twhile ((folio = find_get_entry(&xas, end, tag)) != NULL) {\n\t\t \n\t\tif (xa_is_value(folio))\n\t\t\tcontinue;\n\t\tif (!folio_batch_add(fbatch, folio)) {\n\t\t\tunsigned long nr = folio_nr_pages(folio);\n\n\t\t\tif (folio_test_hugetlb(folio))\n\t\t\t\tnr = 1;\n\t\t\t*start = folio->index + nr;\n\t\t\tgoto out;\n\t\t}\n\t}\n\t \n\tif (end == (pgoff_t)-1)\n\t\t*start = (pgoff_t)-1;\n\telse\n\t\t*start = end + 1;\nout:\n\trcu_read_unlock();\n\n\treturn folio_batch_count(fbatch);\n}\nEXPORT_SYMBOL(filemap_get_folios_tag);\n\n \nstatic void shrink_readahead_size_eio(struct file_ra_state *ra)\n{\n\tra->ra_pages /= 4;\n}\n\n \nstatic void filemap_get_read_batch(struct address_space *mapping,\n\t\tpgoff_t index, pgoff_t max, struct folio_batch *fbatch)\n{\n\tXA_STATE(xas, &mapping->i_pages, index);\n\tstruct folio *folio;\n\n\trcu_read_lock();\n\tfor (folio = xas_load(&xas); folio; folio = xas_next(&xas)) {\n\t\tif (xas_retry(&xas, folio))\n\t\t\tcontinue;\n\t\tif (xas.xa_index > max || xa_is_value(folio))\n\t\t\tbreak;\n\t\tif (xa_is_sibling(folio))\n\t\t\tbreak;\n\t\tif (!folio_try_get_rcu(folio))\n\t\t\tgoto retry;\n\n\t\tif (unlikely(folio != xas_reload(&xas)))\n\t\t\tgoto put_folio;\n\n\t\tif (!folio_batch_add(fbatch, folio))\n\t\t\tbreak;\n\t\tif (!folio_test_uptodate(folio))\n\t\t\tbreak;\n\t\tif (folio_test_readahead(folio))\n\t\t\tbreak;\n\t\txas_advance(&xas, folio_next_index(folio) - 1);\n\t\tcontinue;\nput_folio:\n\t\tfolio_put(folio);\nretry:\n\t\txas_reset(&xas);\n\t}\n\trcu_read_unlock();\n}\n\nstatic int filemap_read_folio(struct file *file, filler_t filler,\n\t\tstruct folio *folio)\n{\n\tbool workingset = folio_test_workingset(folio);\n\tunsigned long pflags;\n\tint error;\n\n\t \n\tfolio_clear_error(folio);\n\n\t \n\tif (unlikely(workingset))\n\t\tpsi_memstall_enter(&pflags);\n\terror = filler(file, folio);\n\tif (unlikely(workingset))\n\t\tpsi_memstall_leave(&pflags);\n\tif (error)\n\t\treturn error;\n\n\terror = folio_wait_locked_killable(folio);\n\tif (error)\n\t\treturn error;\n\tif (folio_test_uptodate(folio))\n\t\treturn 0;\n\tif (file)\n\t\tshrink_readahead_size_eio(&file->f_ra);\n\treturn -EIO;\n}\n\nstatic bool filemap_range_uptodate(struct address_space *mapping,\n\t\tloff_t pos, size_t count, struct folio *folio,\n\t\tbool need_uptodate)\n{\n\tif (folio_test_uptodate(folio))\n\t\treturn true;\n\t \n\tif (need_uptodate)\n\t\treturn false;\n\tif (!mapping->a_ops->is_partially_uptodate)\n\t\treturn false;\n\tif (mapping->host->i_blkbits >= folio_shift(folio))\n\t\treturn false;\n\n\tif (folio_pos(folio) > pos) {\n\t\tcount -= folio_pos(folio) - pos;\n\t\tpos = 0;\n\t} else {\n\t\tpos -= folio_pos(folio);\n\t}\n\n\treturn mapping->a_ops->is_partially_uptodate(folio, pos, count);\n}\n\nstatic int filemap_update_page(struct kiocb *iocb,\n\t\tstruct address_space *mapping, size_t count,\n\t\tstruct folio *folio, bool need_uptodate)\n{\n\tint error;\n\n\tif (iocb->ki_flags & IOCB_NOWAIT) {\n\t\tif (!filemap_invalidate_trylock_shared(mapping))\n\t\t\treturn -EAGAIN;\n\t} else {\n\t\tfilemap_invalidate_lock_shared(mapping);\n\t}\n\n\tif (!folio_trylock(folio)) {\n\t\terror = -EAGAIN;\n\t\tif (iocb->ki_flags & (IOCB_NOWAIT | IOCB_NOIO))\n\t\t\tgoto unlock_mapping;\n\t\tif (!(iocb->ki_flags & IOCB_WAITQ)) {\n\t\t\tfilemap_invalidate_unlock_shared(mapping);\n\t\t\t \n\t\t\tfolio_put_wait_locked(folio, TASK_KILLABLE);\n\t\t\treturn AOP_TRUNCATED_PAGE;\n\t\t}\n\t\terror = __folio_lock_async(folio, iocb->ki_waitq);\n\t\tif (error)\n\t\t\tgoto unlock_mapping;\n\t}\n\n\terror = AOP_TRUNCATED_PAGE;\n\tif (!folio->mapping)\n\t\tgoto unlock;\n\n\terror = 0;\n\tif (filemap_range_uptodate(mapping, iocb->ki_pos, count, folio,\n\t\t\t\t   need_uptodate))\n\t\tgoto unlock;\n\n\terror = -EAGAIN;\n\tif (iocb->ki_flags & (IOCB_NOIO | IOCB_NOWAIT | IOCB_WAITQ))\n\t\tgoto unlock;\n\n\terror = filemap_read_folio(iocb->ki_filp, mapping->a_ops->read_folio,\n\t\t\tfolio);\n\tgoto unlock_mapping;\nunlock:\n\tfolio_unlock(folio);\nunlock_mapping:\n\tfilemap_invalidate_unlock_shared(mapping);\n\tif (error == AOP_TRUNCATED_PAGE)\n\t\tfolio_put(folio);\n\treturn error;\n}\n\nstatic int filemap_create_folio(struct file *file,\n\t\tstruct address_space *mapping, pgoff_t index,\n\t\tstruct folio_batch *fbatch)\n{\n\tstruct folio *folio;\n\tint error;\n\n\tfolio = filemap_alloc_folio(mapping_gfp_mask(mapping), 0);\n\tif (!folio)\n\t\treturn -ENOMEM;\n\n\t \n\tfilemap_invalidate_lock_shared(mapping);\n\terror = filemap_add_folio(mapping, folio, index,\n\t\t\tmapping_gfp_constraint(mapping, GFP_KERNEL));\n\tif (error == -EEXIST)\n\t\terror = AOP_TRUNCATED_PAGE;\n\tif (error)\n\t\tgoto error;\n\n\terror = filemap_read_folio(file, mapping->a_ops->read_folio, folio);\n\tif (error)\n\t\tgoto error;\n\n\tfilemap_invalidate_unlock_shared(mapping);\n\tfolio_batch_add(fbatch, folio);\n\treturn 0;\nerror:\n\tfilemap_invalidate_unlock_shared(mapping);\n\tfolio_put(folio);\n\treturn error;\n}\n\nstatic int filemap_readahead(struct kiocb *iocb, struct file *file,\n\t\tstruct address_space *mapping, struct folio *folio,\n\t\tpgoff_t last_index)\n{\n\tDEFINE_READAHEAD(ractl, file, &file->f_ra, mapping, folio->index);\n\n\tif (iocb->ki_flags & IOCB_NOIO)\n\t\treturn -EAGAIN;\n\tpage_cache_async_ra(&ractl, folio, last_index - folio->index);\n\treturn 0;\n}\n\nstatic int filemap_get_pages(struct kiocb *iocb, size_t count,\n\t\tstruct folio_batch *fbatch, bool need_uptodate)\n{\n\tstruct file *filp = iocb->ki_filp;\n\tstruct address_space *mapping = filp->f_mapping;\n\tstruct file_ra_state *ra = &filp->f_ra;\n\tpgoff_t index = iocb->ki_pos >> PAGE_SHIFT;\n\tpgoff_t last_index;\n\tstruct folio *folio;\n\tint err = 0;\n\n\t \n\tlast_index = DIV_ROUND_UP(iocb->ki_pos + count, PAGE_SIZE);\nretry:\n\tif (fatal_signal_pending(current))\n\t\treturn -EINTR;\n\n\tfilemap_get_read_batch(mapping, index, last_index - 1, fbatch);\n\tif (!folio_batch_count(fbatch)) {\n\t\tif (iocb->ki_flags & IOCB_NOIO)\n\t\t\treturn -EAGAIN;\n\t\tpage_cache_sync_readahead(mapping, ra, filp, index,\n\t\t\t\tlast_index - index);\n\t\tfilemap_get_read_batch(mapping, index, last_index - 1, fbatch);\n\t}\n\tif (!folio_batch_count(fbatch)) {\n\t\tif (iocb->ki_flags & (IOCB_NOWAIT | IOCB_WAITQ))\n\t\t\treturn -EAGAIN;\n\t\terr = filemap_create_folio(filp, mapping,\n\t\t\t\tiocb->ki_pos >> PAGE_SHIFT, fbatch);\n\t\tif (err == AOP_TRUNCATED_PAGE)\n\t\t\tgoto retry;\n\t\treturn err;\n\t}\n\n\tfolio = fbatch->folios[folio_batch_count(fbatch) - 1];\n\tif (folio_test_readahead(folio)) {\n\t\terr = filemap_readahead(iocb, filp, mapping, folio, last_index);\n\t\tif (err)\n\t\t\tgoto err;\n\t}\n\tif (!folio_test_uptodate(folio)) {\n\t\tif ((iocb->ki_flags & IOCB_WAITQ) &&\n\t\t    folio_batch_count(fbatch) > 1)\n\t\t\tiocb->ki_flags |= IOCB_NOWAIT;\n\t\terr = filemap_update_page(iocb, mapping, count, folio,\n\t\t\t\t\t  need_uptodate);\n\t\tif (err)\n\t\t\tgoto err;\n\t}\n\n\treturn 0;\nerr:\n\tif (err < 0)\n\t\tfolio_put(folio);\n\tif (likely(--fbatch->nr))\n\t\treturn 0;\n\tif (err == AOP_TRUNCATED_PAGE)\n\t\tgoto retry;\n\treturn err;\n}\n\nstatic inline bool pos_same_folio(loff_t pos1, loff_t pos2, struct folio *folio)\n{\n\tunsigned int shift = folio_shift(folio);\n\n\treturn (pos1 >> shift == pos2 >> shift);\n}\n\n \nssize_t filemap_read(struct kiocb *iocb, struct iov_iter *iter,\n\t\tssize_t already_read)\n{\n\tstruct file *filp = iocb->ki_filp;\n\tstruct file_ra_state *ra = &filp->f_ra;\n\tstruct address_space *mapping = filp->f_mapping;\n\tstruct inode *inode = mapping->host;\n\tstruct folio_batch fbatch;\n\tint i, error = 0;\n\tbool writably_mapped;\n\tloff_t isize, end_offset;\n\tloff_t last_pos = ra->prev_pos;\n\n\tif (unlikely(iocb->ki_pos >= inode->i_sb->s_maxbytes))\n\t\treturn 0;\n\tif (unlikely(!iov_iter_count(iter)))\n\t\treturn 0;\n\n\tiov_iter_truncate(iter, inode->i_sb->s_maxbytes);\n\tfolio_batch_init(&fbatch);\n\n\tdo {\n\t\tcond_resched();\n\n\t\t \n\t\tif ((iocb->ki_flags & IOCB_WAITQ) && already_read)\n\t\t\tiocb->ki_flags |= IOCB_NOWAIT;\n\n\t\tif (unlikely(iocb->ki_pos >= i_size_read(inode)))\n\t\t\tbreak;\n\n\t\terror = filemap_get_pages(iocb, iter->count, &fbatch, false);\n\t\tif (error < 0)\n\t\t\tbreak;\n\n\t\t \n\t\tisize = i_size_read(inode);\n\t\tif (unlikely(iocb->ki_pos >= isize))\n\t\t\tgoto put_folios;\n\t\tend_offset = min_t(loff_t, isize, iocb->ki_pos + iter->count);\n\n\t\t \n\t\tsmp_rmb();\n\n\t\t \n\t\twritably_mapped = mapping_writably_mapped(mapping);\n\n\t\t \n\t\tif (!pos_same_folio(iocb->ki_pos, last_pos - 1,\n\t\t\t\t    fbatch.folios[0]))\n\t\t\tfolio_mark_accessed(fbatch.folios[0]);\n\n\t\tfor (i = 0; i < folio_batch_count(&fbatch); i++) {\n\t\t\tstruct folio *folio = fbatch.folios[i];\n\t\t\tsize_t fsize = folio_size(folio);\n\t\t\tsize_t offset = iocb->ki_pos & (fsize - 1);\n\t\t\tsize_t bytes = min_t(loff_t, end_offset - iocb->ki_pos,\n\t\t\t\t\t     fsize - offset);\n\t\t\tsize_t copied;\n\n\t\t\tif (end_offset < folio_pos(folio))\n\t\t\t\tbreak;\n\t\t\tif (i > 0)\n\t\t\t\tfolio_mark_accessed(folio);\n\t\t\t \n\t\t\tif (writably_mapped)\n\t\t\t\tflush_dcache_folio(folio);\n\n\t\t\tcopied = copy_folio_to_iter(folio, offset, bytes, iter);\n\n\t\t\talready_read += copied;\n\t\t\tiocb->ki_pos += copied;\n\t\t\tlast_pos = iocb->ki_pos;\n\n\t\t\tif (copied < bytes) {\n\t\t\t\terror = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\nput_folios:\n\t\tfor (i = 0; i < folio_batch_count(&fbatch); i++)\n\t\t\tfolio_put(fbatch.folios[i]);\n\t\tfolio_batch_init(&fbatch);\n\t} while (iov_iter_count(iter) && iocb->ki_pos < isize && !error);\n\n\tfile_accessed(filp);\n\tra->prev_pos = last_pos;\n\treturn already_read ? already_read : error;\n}\nEXPORT_SYMBOL_GPL(filemap_read);\n\nint kiocb_write_and_wait(struct kiocb *iocb, size_t count)\n{\n\tstruct address_space *mapping = iocb->ki_filp->f_mapping;\n\tloff_t pos = iocb->ki_pos;\n\tloff_t end = pos + count - 1;\n\n\tif (iocb->ki_flags & IOCB_NOWAIT) {\n\t\tif (filemap_range_needs_writeback(mapping, pos, end))\n\t\t\treturn -EAGAIN;\n\t\treturn 0;\n\t}\n\n\treturn filemap_write_and_wait_range(mapping, pos, end);\n}\n\nint kiocb_invalidate_pages(struct kiocb *iocb, size_t count)\n{\n\tstruct address_space *mapping = iocb->ki_filp->f_mapping;\n\tloff_t pos = iocb->ki_pos;\n\tloff_t end = pos + count - 1;\n\tint ret;\n\n\tif (iocb->ki_flags & IOCB_NOWAIT) {\n\t\t \n\t\tif (filemap_range_has_page(mapping, pos, end))\n\t\t\treturn -EAGAIN;\n\t} else {\n\t\tret = filemap_write_and_wait_range(mapping, pos, end);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t \n\treturn invalidate_inode_pages2_range(mapping, pos >> PAGE_SHIFT,\n\t\t\t\t\t     end >> PAGE_SHIFT);\n}\n\n \nssize_t\ngeneric_file_read_iter(struct kiocb *iocb, struct iov_iter *iter)\n{\n\tsize_t count = iov_iter_count(iter);\n\tssize_t retval = 0;\n\n\tif (!count)\n\t\treturn 0;  \n\n\tif (iocb->ki_flags & IOCB_DIRECT) {\n\t\tstruct file *file = iocb->ki_filp;\n\t\tstruct address_space *mapping = file->f_mapping;\n\t\tstruct inode *inode = mapping->host;\n\n\t\tretval = kiocb_write_and_wait(iocb, count);\n\t\tif (retval < 0)\n\t\t\treturn retval;\n\t\tfile_accessed(file);\n\n\t\tretval = mapping->a_ops->direct_IO(iocb, iter);\n\t\tif (retval >= 0) {\n\t\t\tiocb->ki_pos += retval;\n\t\t\tcount -= retval;\n\t\t}\n\t\tif (retval != -EIOCBQUEUED)\n\t\t\tiov_iter_revert(iter, count - iov_iter_count(iter));\n\n\t\t \n\t\tif (retval < 0 || !count || IS_DAX(inode))\n\t\t\treturn retval;\n\t\tif (iocb->ki_pos >= i_size_read(inode))\n\t\t\treturn retval;\n\t}\n\n\treturn filemap_read(iocb, iter, retval);\n}\nEXPORT_SYMBOL(generic_file_read_iter);\n\n \nsize_t splice_folio_into_pipe(struct pipe_inode_info *pipe,\n\t\t\t      struct folio *folio, loff_t fpos, size_t size)\n{\n\tstruct page *page;\n\tsize_t spliced = 0, offset = offset_in_folio(folio, fpos);\n\n\tpage = folio_page(folio, offset / PAGE_SIZE);\n\tsize = min(size, folio_size(folio) - offset);\n\toffset %= PAGE_SIZE;\n\n\twhile (spliced < size &&\n\t       !pipe_full(pipe->head, pipe->tail, pipe->max_usage)) {\n\t\tstruct pipe_buffer *buf = pipe_head_buf(pipe);\n\t\tsize_t part = min_t(size_t, PAGE_SIZE - offset, size - spliced);\n\n\t\t*buf = (struct pipe_buffer) {\n\t\t\t.ops\t= &page_cache_pipe_buf_ops,\n\t\t\t.page\t= page,\n\t\t\t.offset\t= offset,\n\t\t\t.len\t= part,\n\t\t};\n\t\tfolio_get(folio);\n\t\tpipe->head++;\n\t\tpage++;\n\t\tspliced += part;\n\t\toffset = 0;\n\t}\n\n\treturn spliced;\n}\n\n \nssize_t filemap_splice_read(struct file *in, loff_t *ppos,\n\t\t\t    struct pipe_inode_info *pipe,\n\t\t\t    size_t len, unsigned int flags)\n{\n\tstruct folio_batch fbatch;\n\tstruct kiocb iocb;\n\tsize_t total_spliced = 0, used, npages;\n\tloff_t isize, end_offset;\n\tbool writably_mapped;\n\tint i, error = 0;\n\n\tif (unlikely(*ppos >= in->f_mapping->host->i_sb->s_maxbytes))\n\t\treturn 0;\n\n\tinit_sync_kiocb(&iocb, in);\n\tiocb.ki_pos = *ppos;\n\n\t \n\tused = pipe_occupancy(pipe->head, pipe->tail);\n\tnpages = max_t(ssize_t, pipe->max_usage - used, 0);\n\tlen = min_t(size_t, len, npages * PAGE_SIZE);\n\n\tfolio_batch_init(&fbatch);\n\n\tdo {\n\t\tcond_resched();\n\n\t\tif (*ppos >= i_size_read(in->f_mapping->host))\n\t\t\tbreak;\n\n\t\tiocb.ki_pos = *ppos;\n\t\terror = filemap_get_pages(&iocb, len, &fbatch, true);\n\t\tif (error < 0)\n\t\t\tbreak;\n\n\t\t \n\t\tisize = i_size_read(in->f_mapping->host);\n\t\tif (unlikely(*ppos >= isize))\n\t\t\tbreak;\n\t\tend_offset = min_t(loff_t, isize, *ppos + len);\n\n\t\t \n\t\twritably_mapped = mapping_writably_mapped(in->f_mapping);\n\n\t\tfor (i = 0; i < folio_batch_count(&fbatch); i++) {\n\t\t\tstruct folio *folio = fbatch.folios[i];\n\t\t\tsize_t n;\n\n\t\t\tif (folio_pos(folio) >= end_offset)\n\t\t\t\tgoto out;\n\t\t\tfolio_mark_accessed(folio);\n\n\t\t\t \n\t\t\tif (writably_mapped)\n\t\t\t\tflush_dcache_folio(folio);\n\n\t\t\tn = min_t(loff_t, len, isize - *ppos);\n\t\t\tn = splice_folio_into_pipe(pipe, folio, *ppos, n);\n\t\t\tif (!n)\n\t\t\t\tgoto out;\n\t\t\tlen -= n;\n\t\t\ttotal_spliced += n;\n\t\t\t*ppos += n;\n\t\t\tin->f_ra.prev_pos = *ppos;\n\t\t\tif (pipe_full(pipe->head, pipe->tail, pipe->max_usage))\n\t\t\t\tgoto out;\n\t\t}\n\n\t\tfolio_batch_release(&fbatch);\n\t} while (len);\n\nout:\n\tfolio_batch_release(&fbatch);\n\tfile_accessed(in);\n\n\treturn total_spliced ? total_spliced : error;\n}\nEXPORT_SYMBOL(filemap_splice_read);\n\nstatic inline loff_t folio_seek_hole_data(struct xa_state *xas,\n\t\tstruct address_space *mapping, struct folio *folio,\n\t\tloff_t start, loff_t end, bool seek_data)\n{\n\tconst struct address_space_operations *ops = mapping->a_ops;\n\tsize_t offset, bsz = i_blocksize(mapping->host);\n\n\tif (xa_is_value(folio) || folio_test_uptodate(folio))\n\t\treturn seek_data ? start : end;\n\tif (!ops->is_partially_uptodate)\n\t\treturn seek_data ? end : start;\n\n\txas_pause(xas);\n\trcu_read_unlock();\n\tfolio_lock(folio);\n\tif (unlikely(folio->mapping != mapping))\n\t\tgoto unlock;\n\n\toffset = offset_in_folio(folio, start) & ~(bsz - 1);\n\n\tdo {\n\t\tif (ops->is_partially_uptodate(folio, offset, bsz) ==\n\t\t\t\t\t\t\tseek_data)\n\t\t\tbreak;\n\t\tstart = (start + bsz) & ~(bsz - 1);\n\t\toffset += bsz;\n\t} while (offset < folio_size(folio));\nunlock:\n\tfolio_unlock(folio);\n\trcu_read_lock();\n\treturn start;\n}\n\nstatic inline size_t seek_folio_size(struct xa_state *xas, struct folio *folio)\n{\n\tif (xa_is_value(folio))\n\t\treturn PAGE_SIZE << xa_get_order(xas->xa, xas->xa_index);\n\treturn folio_size(folio);\n}\n\n \nloff_t mapping_seek_hole_data(struct address_space *mapping, loff_t start,\n\t\tloff_t end, int whence)\n{\n\tXA_STATE(xas, &mapping->i_pages, start >> PAGE_SHIFT);\n\tpgoff_t max = (end - 1) >> PAGE_SHIFT;\n\tbool seek_data = (whence == SEEK_DATA);\n\tstruct folio *folio;\n\n\tif (end <= start)\n\t\treturn -ENXIO;\n\n\trcu_read_lock();\n\twhile ((folio = find_get_entry(&xas, max, XA_PRESENT))) {\n\t\tloff_t pos = (u64)xas.xa_index << PAGE_SHIFT;\n\t\tsize_t seek_size;\n\n\t\tif (start < pos) {\n\t\t\tif (!seek_data)\n\t\t\t\tgoto unlock;\n\t\t\tstart = pos;\n\t\t}\n\n\t\tseek_size = seek_folio_size(&xas, folio);\n\t\tpos = round_up((u64)pos + 1, seek_size);\n\t\tstart = folio_seek_hole_data(&xas, mapping, folio, start, pos,\n\t\t\t\tseek_data);\n\t\tif (start < pos)\n\t\t\tgoto unlock;\n\t\tif (start >= end)\n\t\t\tbreak;\n\t\tif (seek_size > PAGE_SIZE)\n\t\t\txas_set(&xas, pos >> PAGE_SHIFT);\n\t\tif (!xa_is_value(folio))\n\t\t\tfolio_put(folio);\n\t}\n\tif (seek_data)\n\t\tstart = -ENXIO;\nunlock:\n\trcu_read_unlock();\n\tif (folio && !xa_is_value(folio))\n\t\tfolio_put(folio);\n\tif (start > end)\n\t\treturn end;\n\treturn start;\n}\n\n#ifdef CONFIG_MMU\n#define MMAP_LOTSAMISS  (100)\n \nstatic int lock_folio_maybe_drop_mmap(struct vm_fault *vmf, struct folio *folio,\n\t\t\t\t     struct file **fpin)\n{\n\tif (folio_trylock(folio))\n\t\treturn 1;\n\n\t \n\tif (vmf->flags & FAULT_FLAG_RETRY_NOWAIT)\n\t\treturn 0;\n\n\t*fpin = maybe_unlock_mmap_for_io(vmf, *fpin);\n\tif (vmf->flags & FAULT_FLAG_KILLABLE) {\n\t\tif (__folio_lock_killable(folio)) {\n\t\t\t \n\t\t\tif (*fpin == NULL)\n\t\t\t\tmmap_read_unlock(vmf->vma->vm_mm);\n\t\t\treturn 0;\n\t\t}\n\t} else\n\t\t__folio_lock(folio);\n\n\treturn 1;\n}\n\n \nstatic struct file *do_sync_mmap_readahead(struct vm_fault *vmf)\n{\n\tstruct file *file = vmf->vma->vm_file;\n\tstruct file_ra_state *ra = &file->f_ra;\n\tstruct address_space *mapping = file->f_mapping;\n\tDEFINE_READAHEAD(ractl, file, ra, mapping, vmf->pgoff);\n\tstruct file *fpin = NULL;\n\tunsigned long vm_flags = vmf->vma->vm_flags;\n\tunsigned int mmap_miss;\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\t \n\tif (vm_flags & VM_HUGEPAGE) {\n\t\tfpin = maybe_unlock_mmap_for_io(vmf, fpin);\n\t\tractl._index &= ~((unsigned long)HPAGE_PMD_NR - 1);\n\t\tra->size = HPAGE_PMD_NR;\n\t\t \n\t\tif (!(vm_flags & VM_RAND_READ))\n\t\t\tra->size *= 2;\n\t\tra->async_size = HPAGE_PMD_NR;\n\t\tpage_cache_ra_order(&ractl, ra, HPAGE_PMD_ORDER);\n\t\treturn fpin;\n\t}\n#endif\n\n\t \n\tif (vm_flags & VM_RAND_READ)\n\t\treturn fpin;\n\tif (!ra->ra_pages)\n\t\treturn fpin;\n\n\tif (vm_flags & VM_SEQ_READ) {\n\t\tfpin = maybe_unlock_mmap_for_io(vmf, fpin);\n\t\tpage_cache_sync_ra(&ractl, ra->ra_pages);\n\t\treturn fpin;\n\t}\n\n\t \n\tmmap_miss = READ_ONCE(ra->mmap_miss);\n\tif (mmap_miss < MMAP_LOTSAMISS * 10)\n\t\tWRITE_ONCE(ra->mmap_miss, ++mmap_miss);\n\n\t \n\tif (mmap_miss > MMAP_LOTSAMISS)\n\t\treturn fpin;\n\n\t \n\tfpin = maybe_unlock_mmap_for_io(vmf, fpin);\n\tra->start = max_t(long, 0, vmf->pgoff - ra->ra_pages / 2);\n\tra->size = ra->ra_pages;\n\tra->async_size = ra->ra_pages / 4;\n\tractl._index = ra->start;\n\tpage_cache_ra_order(&ractl, ra, 0);\n\treturn fpin;\n}\n\n \nstatic struct file *do_async_mmap_readahead(struct vm_fault *vmf,\n\t\t\t\t\t    struct folio *folio)\n{\n\tstruct file *file = vmf->vma->vm_file;\n\tstruct file_ra_state *ra = &file->f_ra;\n\tDEFINE_READAHEAD(ractl, file, ra, file->f_mapping, vmf->pgoff);\n\tstruct file *fpin = NULL;\n\tunsigned int mmap_miss;\n\n\t \n\tif (vmf->vma->vm_flags & VM_RAND_READ || !ra->ra_pages)\n\t\treturn fpin;\n\n\tmmap_miss = READ_ONCE(ra->mmap_miss);\n\tif (mmap_miss)\n\t\tWRITE_ONCE(ra->mmap_miss, --mmap_miss);\n\n\tif (folio_test_readahead(folio)) {\n\t\tfpin = maybe_unlock_mmap_for_io(vmf, fpin);\n\t\tpage_cache_async_ra(&ractl, folio, ra->ra_pages);\n\t}\n\treturn fpin;\n}\n\n \nvm_fault_t filemap_fault(struct vm_fault *vmf)\n{\n\tint error;\n\tstruct file *file = vmf->vma->vm_file;\n\tstruct file *fpin = NULL;\n\tstruct address_space *mapping = file->f_mapping;\n\tstruct inode *inode = mapping->host;\n\tpgoff_t max_idx, index = vmf->pgoff;\n\tstruct folio *folio;\n\tvm_fault_t ret = 0;\n\tbool mapping_locked = false;\n\n\tmax_idx = DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE);\n\tif (unlikely(index >= max_idx))\n\t\treturn VM_FAULT_SIGBUS;\n\n\t \n\tfolio = filemap_get_folio(mapping, index);\n\tif (likely(!IS_ERR(folio))) {\n\t\t \n\t\tif (!(vmf->flags & FAULT_FLAG_TRIED))\n\t\t\tfpin = do_async_mmap_readahead(vmf, folio);\n\t\tif (unlikely(!folio_test_uptodate(folio))) {\n\t\t\tfilemap_invalidate_lock_shared(mapping);\n\t\t\tmapping_locked = true;\n\t\t}\n\t} else {\n\t\t \n\t\tcount_vm_event(PGMAJFAULT);\n\t\tcount_memcg_event_mm(vmf->vma->vm_mm, PGMAJFAULT);\n\t\tret = VM_FAULT_MAJOR;\n\t\tfpin = do_sync_mmap_readahead(vmf);\nretry_find:\n\t\t \n\t\tif (!mapping_locked) {\n\t\t\tfilemap_invalidate_lock_shared(mapping);\n\t\t\tmapping_locked = true;\n\t\t}\n\t\tfolio = __filemap_get_folio(mapping, index,\n\t\t\t\t\t  FGP_CREAT|FGP_FOR_MMAP,\n\t\t\t\t\t  vmf->gfp_mask);\n\t\tif (IS_ERR(folio)) {\n\t\t\tif (fpin)\n\t\t\t\tgoto out_retry;\n\t\t\tfilemap_invalidate_unlock_shared(mapping);\n\t\t\treturn VM_FAULT_OOM;\n\t\t}\n\t}\n\n\tif (!lock_folio_maybe_drop_mmap(vmf, folio, &fpin))\n\t\tgoto out_retry;\n\n\t \n\tif (unlikely(folio->mapping != mapping)) {\n\t\tfolio_unlock(folio);\n\t\tfolio_put(folio);\n\t\tgoto retry_find;\n\t}\n\tVM_BUG_ON_FOLIO(!folio_contains(folio, index), folio);\n\n\t \n\tif (unlikely(!folio_test_uptodate(folio))) {\n\t\t \n\t\tif (!mapping_locked) {\n\t\t\tfolio_unlock(folio);\n\t\t\tfolio_put(folio);\n\t\t\tgoto retry_find;\n\t\t}\n\t\tgoto page_not_uptodate;\n\t}\n\n\t \n\tif (fpin) {\n\t\tfolio_unlock(folio);\n\t\tgoto out_retry;\n\t}\n\tif (mapping_locked)\n\t\tfilemap_invalidate_unlock_shared(mapping);\n\n\t \n\tmax_idx = DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE);\n\tif (unlikely(index >= max_idx)) {\n\t\tfolio_unlock(folio);\n\t\tfolio_put(folio);\n\t\treturn VM_FAULT_SIGBUS;\n\t}\n\n\tvmf->page = folio_file_page(folio, index);\n\treturn ret | VM_FAULT_LOCKED;\n\npage_not_uptodate:\n\t \n\tfpin = maybe_unlock_mmap_for_io(vmf, fpin);\n\terror = filemap_read_folio(file, mapping->a_ops->read_folio, folio);\n\tif (fpin)\n\t\tgoto out_retry;\n\tfolio_put(folio);\n\n\tif (!error || error == AOP_TRUNCATED_PAGE)\n\t\tgoto retry_find;\n\tfilemap_invalidate_unlock_shared(mapping);\n\n\treturn VM_FAULT_SIGBUS;\n\nout_retry:\n\t \n\tif (!IS_ERR(folio))\n\t\tfolio_put(folio);\n\tif (mapping_locked)\n\t\tfilemap_invalidate_unlock_shared(mapping);\n\tif (fpin)\n\t\tfput(fpin);\n\treturn ret | VM_FAULT_RETRY;\n}\nEXPORT_SYMBOL(filemap_fault);\n\nstatic bool filemap_map_pmd(struct vm_fault *vmf, struct folio *folio,\n\t\tpgoff_t start)\n{\n\tstruct mm_struct *mm = vmf->vma->vm_mm;\n\n\t \n\tif (pmd_trans_huge(*vmf->pmd)) {\n\t\tfolio_unlock(folio);\n\t\tfolio_put(folio);\n\t\treturn true;\n\t}\n\n\tif (pmd_none(*vmf->pmd) && folio_test_pmd_mappable(folio)) {\n\t\tstruct page *page = folio_file_page(folio, start);\n\t\tvm_fault_t ret = do_set_pmd(vmf, page);\n\t\tif (!ret) {\n\t\t\t \n\t\t\tfolio_unlock(folio);\n\t\t\treturn true;\n\t\t}\n\t}\n\n\tif (pmd_none(*vmf->pmd) && vmf->prealloc_pte)\n\t\tpmd_install(mm, vmf->pmd, &vmf->prealloc_pte);\n\n\treturn false;\n}\n\nstatic struct folio *next_uptodate_folio(struct xa_state *xas,\n\t\tstruct address_space *mapping, pgoff_t end_pgoff)\n{\n\tstruct folio *folio = xas_next_entry(xas, end_pgoff);\n\tunsigned long max_idx;\n\n\tdo {\n\t\tif (!folio)\n\t\t\treturn NULL;\n\t\tif (xas_retry(xas, folio))\n\t\t\tcontinue;\n\t\tif (xa_is_value(folio))\n\t\t\tcontinue;\n\t\tif (folio_test_locked(folio))\n\t\t\tcontinue;\n\t\tif (!folio_try_get_rcu(folio))\n\t\t\tcontinue;\n\t\t \n\t\tif (unlikely(folio != xas_reload(xas)))\n\t\t\tgoto skip;\n\t\tif (!folio_test_uptodate(folio) || folio_test_readahead(folio))\n\t\t\tgoto skip;\n\t\tif (!folio_trylock(folio))\n\t\t\tgoto skip;\n\t\tif (folio->mapping != mapping)\n\t\t\tgoto unlock;\n\t\tif (!folio_test_uptodate(folio))\n\t\t\tgoto unlock;\n\t\tmax_idx = DIV_ROUND_UP(i_size_read(mapping->host), PAGE_SIZE);\n\t\tif (xas->xa_index >= max_idx)\n\t\t\tgoto unlock;\n\t\treturn folio;\nunlock:\n\t\tfolio_unlock(folio);\nskip:\n\t\tfolio_put(folio);\n\t} while ((folio = xas_next_entry(xas, end_pgoff)) != NULL);\n\n\treturn NULL;\n}\n\n \nstatic vm_fault_t filemap_map_folio_range(struct vm_fault *vmf,\n\t\t\tstruct folio *folio, unsigned long start,\n\t\t\tunsigned long addr, unsigned int nr_pages,\n\t\t\tunsigned int *mmap_miss)\n{\n\tvm_fault_t ret = 0;\n\tstruct page *page = folio_page(folio, start);\n\tunsigned int count = 0;\n\tpte_t *old_ptep = vmf->pte;\n\n\tdo {\n\t\tif (PageHWPoison(page + count))\n\t\t\tgoto skip;\n\n\t\t(*mmap_miss)++;\n\n\t\t \n\t\tif (!pte_none(vmf->pte[count]))\n\t\t\tgoto skip;\n\n\t\tcount++;\n\t\tcontinue;\nskip:\n\t\tif (count) {\n\t\t\tset_pte_range(vmf, folio, page, count, addr);\n\t\t\tfolio_ref_add(folio, count);\n\t\t\tif (in_range(vmf->address, addr, count * PAGE_SIZE))\n\t\t\t\tret = VM_FAULT_NOPAGE;\n\t\t}\n\n\t\tcount++;\n\t\tpage += count;\n\t\tvmf->pte += count;\n\t\taddr += count * PAGE_SIZE;\n\t\tcount = 0;\n\t} while (--nr_pages > 0);\n\n\tif (count) {\n\t\tset_pte_range(vmf, folio, page, count, addr);\n\t\tfolio_ref_add(folio, count);\n\t\tif (in_range(vmf->address, addr, count * PAGE_SIZE))\n\t\t\tret = VM_FAULT_NOPAGE;\n\t}\n\n\tvmf->pte = old_ptep;\n\n\treturn ret;\n}\n\nstatic vm_fault_t filemap_map_order0_folio(struct vm_fault *vmf,\n\t\tstruct folio *folio, unsigned long addr,\n\t\tunsigned int *mmap_miss)\n{\n\tvm_fault_t ret = 0;\n\tstruct page *page = &folio->page;\n\n\tif (PageHWPoison(page))\n\t\treturn ret;\n\n\t(*mmap_miss)++;\n\n\t \n\tif (!pte_none(ptep_get(vmf->pte)))\n\t\treturn ret;\n\n\tif (vmf->address == addr)\n\t\tret = VM_FAULT_NOPAGE;\n\n\tset_pte_range(vmf, folio, page, 1, addr);\n\tfolio_ref_inc(folio);\n\n\treturn ret;\n}\n\nvm_fault_t filemap_map_pages(struct vm_fault *vmf,\n\t\t\t     pgoff_t start_pgoff, pgoff_t end_pgoff)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct file *file = vma->vm_file;\n\tstruct address_space *mapping = file->f_mapping;\n\tpgoff_t last_pgoff = start_pgoff;\n\tunsigned long addr;\n\tXA_STATE(xas, &mapping->i_pages, start_pgoff);\n\tstruct folio *folio;\n\tvm_fault_t ret = 0;\n\tunsigned int nr_pages = 0, mmap_miss = 0, mmap_miss_saved;\n\n\trcu_read_lock();\n\tfolio = next_uptodate_folio(&xas, mapping, end_pgoff);\n\tif (!folio)\n\t\tgoto out;\n\n\tif (filemap_map_pmd(vmf, folio, start_pgoff)) {\n\t\tret = VM_FAULT_NOPAGE;\n\t\tgoto out;\n\t}\n\n\taddr = vma->vm_start + ((start_pgoff - vma->vm_pgoff) << PAGE_SHIFT);\n\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd, addr, &vmf->ptl);\n\tif (!vmf->pte) {\n\t\tfolio_unlock(folio);\n\t\tfolio_put(folio);\n\t\tgoto out;\n\t}\n\tdo {\n\t\tunsigned long end;\n\n\t\taddr += (xas.xa_index - last_pgoff) << PAGE_SHIFT;\n\t\tvmf->pte += xas.xa_index - last_pgoff;\n\t\tlast_pgoff = xas.xa_index;\n\t\tend = folio->index + folio_nr_pages(folio) - 1;\n\t\tnr_pages = min(end, end_pgoff) - xas.xa_index + 1;\n\n\t\tif (!folio_test_large(folio))\n\t\t\tret |= filemap_map_order0_folio(vmf,\n\t\t\t\t\tfolio, addr, &mmap_miss);\n\t\telse\n\t\t\tret |= filemap_map_folio_range(vmf, folio,\n\t\t\t\t\txas.xa_index - folio->index, addr,\n\t\t\t\t\tnr_pages, &mmap_miss);\n\n\t\tfolio_unlock(folio);\n\t\tfolio_put(folio);\n\t} while ((folio = next_uptodate_folio(&xas, mapping, end_pgoff)) != NULL);\n\tpte_unmap_unlock(vmf->pte, vmf->ptl);\nout:\n\trcu_read_unlock();\n\n\tmmap_miss_saved = READ_ONCE(file->f_ra.mmap_miss);\n\tif (mmap_miss >= mmap_miss_saved)\n\t\tWRITE_ONCE(file->f_ra.mmap_miss, 0);\n\telse\n\t\tWRITE_ONCE(file->f_ra.mmap_miss, mmap_miss_saved - mmap_miss);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(filemap_map_pages);\n\nvm_fault_t filemap_page_mkwrite(struct vm_fault *vmf)\n{\n\tstruct address_space *mapping = vmf->vma->vm_file->f_mapping;\n\tstruct folio *folio = page_folio(vmf->page);\n\tvm_fault_t ret = VM_FAULT_LOCKED;\n\n\tsb_start_pagefault(mapping->host->i_sb);\n\tfile_update_time(vmf->vma->vm_file);\n\tfolio_lock(folio);\n\tif (folio->mapping != mapping) {\n\t\tfolio_unlock(folio);\n\t\tret = VM_FAULT_NOPAGE;\n\t\tgoto out;\n\t}\n\t \n\tfolio_mark_dirty(folio);\n\tfolio_wait_stable(folio);\nout:\n\tsb_end_pagefault(mapping->host->i_sb);\n\treturn ret;\n}\n\nconst struct vm_operations_struct generic_file_vm_ops = {\n\t.fault\t\t= filemap_fault,\n\t.map_pages\t= filemap_map_pages,\n\t.page_mkwrite\t= filemap_page_mkwrite,\n};\n\n \n\nint generic_file_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\tstruct address_space *mapping = file->f_mapping;\n\n\tif (!mapping->a_ops->read_folio)\n\t\treturn -ENOEXEC;\n\tfile_accessed(file);\n\tvma->vm_ops = &generic_file_vm_ops;\n\treturn 0;\n}\n\n \nint generic_file_readonly_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\tif ((vma->vm_flags & VM_SHARED) && (vma->vm_flags & VM_MAYWRITE))\n\t\treturn -EINVAL;\n\treturn generic_file_mmap(file, vma);\n}\n#else\nvm_fault_t filemap_page_mkwrite(struct vm_fault *vmf)\n{\n\treturn VM_FAULT_SIGBUS;\n}\nint generic_file_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\treturn -ENOSYS;\n}\nint generic_file_readonly_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\treturn -ENOSYS;\n}\n#endif  \n\nEXPORT_SYMBOL(filemap_page_mkwrite);\nEXPORT_SYMBOL(generic_file_mmap);\nEXPORT_SYMBOL(generic_file_readonly_mmap);\n\nstatic struct folio *do_read_cache_folio(struct address_space *mapping,\n\t\tpgoff_t index, filler_t filler, struct file *file, gfp_t gfp)\n{\n\tstruct folio *folio;\n\tint err;\n\n\tif (!filler)\n\t\tfiller = mapping->a_ops->read_folio;\nrepeat:\n\tfolio = filemap_get_folio(mapping, index);\n\tif (IS_ERR(folio)) {\n\t\tfolio = filemap_alloc_folio(gfp, 0);\n\t\tif (!folio)\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\terr = filemap_add_folio(mapping, folio, index, gfp);\n\t\tif (unlikely(err)) {\n\t\t\tfolio_put(folio);\n\t\t\tif (err == -EEXIST)\n\t\t\t\tgoto repeat;\n\t\t\t \n\t\t\treturn ERR_PTR(err);\n\t\t}\n\n\t\tgoto filler;\n\t}\n\tif (folio_test_uptodate(folio))\n\t\tgoto out;\n\n\tif (!folio_trylock(folio)) {\n\t\tfolio_put_wait_locked(folio, TASK_UNINTERRUPTIBLE);\n\t\tgoto repeat;\n\t}\n\n\t \n\tif (!folio->mapping) {\n\t\tfolio_unlock(folio);\n\t\tfolio_put(folio);\n\t\tgoto repeat;\n\t}\n\n\t \n\tif (folio_test_uptodate(folio)) {\n\t\tfolio_unlock(folio);\n\t\tgoto out;\n\t}\n\nfiller:\n\terr = filemap_read_folio(file, filler, folio);\n\tif (err) {\n\t\tfolio_put(folio);\n\t\tif (err == AOP_TRUNCATED_PAGE)\n\t\t\tgoto repeat;\n\t\treturn ERR_PTR(err);\n\t}\n\nout:\n\tfolio_mark_accessed(folio);\n\treturn folio;\n}\n\n \nstruct folio *read_cache_folio(struct address_space *mapping, pgoff_t index,\n\t\tfiller_t filler, struct file *file)\n{\n\treturn do_read_cache_folio(mapping, index, filler, file,\n\t\t\tmapping_gfp_mask(mapping));\n}\nEXPORT_SYMBOL(read_cache_folio);\n\n \nstruct folio *mapping_read_folio_gfp(struct address_space *mapping,\n\t\tpgoff_t index, gfp_t gfp)\n{\n\treturn do_read_cache_folio(mapping, index, NULL, NULL, gfp);\n}\nEXPORT_SYMBOL(mapping_read_folio_gfp);\n\nstatic struct page *do_read_cache_page(struct address_space *mapping,\n\t\tpgoff_t index, filler_t *filler, struct file *file, gfp_t gfp)\n{\n\tstruct folio *folio;\n\n\tfolio = do_read_cache_folio(mapping, index, filler, file, gfp);\n\tif (IS_ERR(folio))\n\t\treturn &folio->page;\n\treturn folio_file_page(folio, index);\n}\n\nstruct page *read_cache_page(struct address_space *mapping,\n\t\t\tpgoff_t index, filler_t *filler, struct file *file)\n{\n\treturn do_read_cache_page(mapping, index, filler, file,\n\t\t\tmapping_gfp_mask(mapping));\n}\nEXPORT_SYMBOL(read_cache_page);\n\n \nstruct page *read_cache_page_gfp(struct address_space *mapping,\n\t\t\t\tpgoff_t index,\n\t\t\t\tgfp_t gfp)\n{\n\treturn do_read_cache_page(mapping, index, NULL, NULL, gfp);\n}\nEXPORT_SYMBOL(read_cache_page_gfp);\n\n \nstatic void dio_warn_stale_pagecache(struct file *filp)\n{\n\tstatic DEFINE_RATELIMIT_STATE(_rs, 86400 * HZ, DEFAULT_RATELIMIT_BURST);\n\tchar pathname[128];\n\tchar *path;\n\n\terrseq_set(&filp->f_mapping->wb_err, -EIO);\n\tif (__ratelimit(&_rs)) {\n\t\tpath = file_path(filp, pathname, sizeof(pathname));\n\t\tif (IS_ERR(path))\n\t\t\tpath = \"(unknown)\";\n\t\tpr_crit(\"Page cache invalidation failure on direct I/O.  Possible data corruption due to collision with buffered I/O!\\n\");\n\t\tpr_crit(\"File: %s PID: %d Comm: %.20s\\n\", path, current->pid,\n\t\t\tcurrent->comm);\n\t}\n}\n\nvoid kiocb_invalidate_post_direct_write(struct kiocb *iocb, size_t count)\n{\n\tstruct address_space *mapping = iocb->ki_filp->f_mapping;\n\n\tif (mapping->nrpages &&\n\t    invalidate_inode_pages2_range(mapping,\n\t\t\tiocb->ki_pos >> PAGE_SHIFT,\n\t\t\t(iocb->ki_pos + count - 1) >> PAGE_SHIFT))\n\t\tdio_warn_stale_pagecache(iocb->ki_filp);\n}\n\nssize_t\ngeneric_file_direct_write(struct kiocb *iocb, struct iov_iter *from)\n{\n\tstruct address_space *mapping = iocb->ki_filp->f_mapping;\n\tsize_t write_len = iov_iter_count(from);\n\tssize_t written;\n\n\t \n\twritten = kiocb_invalidate_pages(iocb, write_len);\n\tif (written) {\n\t\tif (written == -EBUSY)\n\t\t\treturn 0;\n\t\treturn written;\n\t}\n\n\twritten = mapping->a_ops->direct_IO(iocb, from);\n\n\t \n\tif (written > 0) {\n\t\tstruct inode *inode = mapping->host;\n\t\tloff_t pos = iocb->ki_pos;\n\n\t\tkiocb_invalidate_post_direct_write(iocb, written);\n\t\tpos += written;\n\t\twrite_len -= written;\n\t\tif (pos > i_size_read(inode) && !S_ISBLK(inode->i_mode)) {\n\t\t\ti_size_write(inode, pos);\n\t\t\tmark_inode_dirty(inode);\n\t\t}\n\t\tiocb->ki_pos = pos;\n\t}\n\tif (written != -EIOCBQUEUED)\n\t\tiov_iter_revert(from, write_len - iov_iter_count(from));\n\treturn written;\n}\nEXPORT_SYMBOL(generic_file_direct_write);\n\nssize_t generic_perform_write(struct kiocb *iocb, struct iov_iter *i)\n{\n\tstruct file *file = iocb->ki_filp;\n\tloff_t pos = iocb->ki_pos;\n\tstruct address_space *mapping = file->f_mapping;\n\tconst struct address_space_operations *a_ops = mapping->a_ops;\n\tlong status = 0;\n\tssize_t written = 0;\n\n\tdo {\n\t\tstruct page *page;\n\t\tunsigned long offset;\t \n\t\tunsigned long bytes;\t \n\t\tsize_t copied;\t\t \n\t\tvoid *fsdata = NULL;\n\n\t\toffset = (pos & (PAGE_SIZE - 1));\n\t\tbytes = min_t(unsigned long, PAGE_SIZE - offset,\n\t\t\t\t\t\tiov_iter_count(i));\n\nagain:\n\t\t \n\t\tif (unlikely(fault_in_iov_iter_readable(i, bytes) == bytes)) {\n\t\t\tstatus = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (fatal_signal_pending(current)) {\n\t\t\tstatus = -EINTR;\n\t\t\tbreak;\n\t\t}\n\n\t\tstatus = a_ops->write_begin(file, mapping, pos, bytes,\n\t\t\t\t\t\t&page, &fsdata);\n\t\tif (unlikely(status < 0))\n\t\t\tbreak;\n\n\t\tif (mapping_writably_mapped(mapping))\n\t\t\tflush_dcache_page(page);\n\n\t\tcopied = copy_page_from_iter_atomic(page, offset, bytes, i);\n\t\tflush_dcache_page(page);\n\n\t\tstatus = a_ops->write_end(file, mapping, pos, bytes, copied,\n\t\t\t\t\t\tpage, fsdata);\n\t\tif (unlikely(status != copied)) {\n\t\t\tiov_iter_revert(i, copied - max(status, 0L));\n\t\t\tif (unlikely(status < 0))\n\t\t\t\tbreak;\n\t\t}\n\t\tcond_resched();\n\n\t\tif (unlikely(status == 0)) {\n\t\t\t \n\t\t\tif (copied)\n\t\t\t\tbytes = copied;\n\t\t\tgoto again;\n\t\t}\n\t\tpos += status;\n\t\twritten += status;\n\n\t\tbalance_dirty_pages_ratelimited(mapping);\n\t} while (iov_iter_count(i));\n\n\tif (!written)\n\t\treturn status;\n\tiocb->ki_pos += written;\n\treturn written;\n}\nEXPORT_SYMBOL(generic_perform_write);\n\n \nssize_t __generic_file_write_iter(struct kiocb *iocb, struct iov_iter *from)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct address_space *mapping = file->f_mapping;\n\tstruct inode *inode = mapping->host;\n\tssize_t ret;\n\n\tret = file_remove_privs(file);\n\tif (ret)\n\t\treturn ret;\n\n\tret = file_update_time(file);\n\tif (ret)\n\t\treturn ret;\n\n\tif (iocb->ki_flags & IOCB_DIRECT) {\n\t\tret = generic_file_direct_write(iocb, from);\n\t\t \n\t\tif (ret < 0 || !iov_iter_count(from) || IS_DAX(inode))\n\t\t\treturn ret;\n\t\treturn direct_write_fallback(iocb, from, ret,\n\t\t\t\tgeneric_perform_write(iocb, from));\n\t}\n\n\treturn generic_perform_write(iocb, from);\n}\nEXPORT_SYMBOL(__generic_file_write_iter);\n\n \nssize_t generic_file_write_iter(struct kiocb *iocb, struct iov_iter *from)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct inode *inode = file->f_mapping->host;\n\tssize_t ret;\n\n\tinode_lock(inode);\n\tret = generic_write_checks(iocb, from);\n\tif (ret > 0)\n\t\tret = __generic_file_write_iter(iocb, from);\n\tinode_unlock(inode);\n\n\tif (ret > 0)\n\t\tret = generic_write_sync(iocb, ret);\n\treturn ret;\n}\nEXPORT_SYMBOL(generic_file_write_iter);\n\n \nbool filemap_release_folio(struct folio *folio, gfp_t gfp)\n{\n\tstruct address_space * const mapping = folio->mapping;\n\n\tBUG_ON(!folio_test_locked(folio));\n\tif (!folio_needs_release(folio))\n\t\treturn true;\n\tif (folio_test_writeback(folio))\n\t\treturn false;\n\n\tif (mapping && mapping->a_ops->release_folio)\n\t\treturn mapping->a_ops->release_folio(folio, gfp);\n\treturn try_to_free_buffers(folio);\n}\nEXPORT_SYMBOL(filemap_release_folio);\n\n#ifdef CONFIG_CACHESTAT_SYSCALL\n \nstatic void filemap_cachestat(struct address_space *mapping,\n\t\tpgoff_t first_index, pgoff_t last_index, struct cachestat *cs)\n{\n\tXA_STATE(xas, &mapping->i_pages, first_index);\n\tstruct folio *folio;\n\n\trcu_read_lock();\n\txas_for_each(&xas, folio, last_index) {\n\t\tunsigned long nr_pages;\n\t\tpgoff_t folio_first_index, folio_last_index;\n\n\t\tif (xas_retry(&xas, folio))\n\t\t\tcontinue;\n\n\t\tif (xa_is_value(folio)) {\n\t\t\t \n\t\t\tvoid *shadow = (void *)folio;\n\t\t\tbool workingset;  \n\t\t\tint order = xa_get_order(xas.xa, xas.xa_index);\n\n\t\t\tnr_pages = 1 << order;\n\t\t\tfolio_first_index = round_down(xas.xa_index, 1 << order);\n\t\t\tfolio_last_index = folio_first_index + nr_pages - 1;\n\n\t\t\t \n\t\t\tif (folio_first_index < first_index)\n\t\t\t\tnr_pages -= first_index - folio_first_index;\n\n\t\t\tif (folio_last_index > last_index)\n\t\t\t\tnr_pages -= folio_last_index - last_index;\n\n\t\t\tcs->nr_evicted += nr_pages;\n\n#ifdef CONFIG_SWAP  \n\t\t\tif (shmem_mapping(mapping)) {\n\t\t\t\t \n\t\t\t\tswp_entry_t swp = radix_to_swp_entry(folio);\n\n\t\t\t\tshadow = get_shadow_from_swap_cache(swp);\n\t\t\t}\n#endif\n\t\t\tif (workingset_test_recent(shadow, true, &workingset))\n\t\t\t\tcs->nr_recently_evicted += nr_pages;\n\n\t\t\tgoto resched;\n\t\t}\n\n\t\tnr_pages = folio_nr_pages(folio);\n\t\tfolio_first_index = folio_pgoff(folio);\n\t\tfolio_last_index = folio_first_index + nr_pages - 1;\n\n\t\t \n\t\tif (folio_first_index < first_index)\n\t\t\tnr_pages -= first_index - folio_first_index;\n\n\t\tif (folio_last_index > last_index)\n\t\t\tnr_pages -= folio_last_index - last_index;\n\n\t\t \n\t\tcs->nr_cache += nr_pages;\n\n\t\tif (folio_test_dirty(folio))\n\t\t\tcs->nr_dirty += nr_pages;\n\n\t\tif (folio_test_writeback(folio))\n\t\t\tcs->nr_writeback += nr_pages;\n\nresched:\n\t\tif (need_resched()) {\n\t\t\txas_pause(&xas);\n\t\t\tcond_resched_rcu();\n\t\t}\n\t}\n\trcu_read_unlock();\n}\n\n \nSYSCALL_DEFINE4(cachestat, unsigned int, fd,\n\t\tstruct cachestat_range __user *, cstat_range,\n\t\tstruct cachestat __user *, cstat, unsigned int, flags)\n{\n\tstruct fd f = fdget(fd);\n\tstruct address_space *mapping;\n\tstruct cachestat_range csr;\n\tstruct cachestat cs;\n\tpgoff_t first_index, last_index;\n\n\tif (!f.file)\n\t\treturn -EBADF;\n\n\tif (copy_from_user(&csr, cstat_range,\n\t\t\tsizeof(struct cachestat_range))) {\n\t\tfdput(f);\n\t\treturn -EFAULT;\n\t}\n\n\t \n\tif (is_file_hugepages(f.file)) {\n\t\tfdput(f);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (flags != 0) {\n\t\tfdput(f);\n\t\treturn -EINVAL;\n\t}\n\n\tfirst_index = csr.off >> PAGE_SHIFT;\n\tlast_index =\n\t\tcsr.len == 0 ? ULONG_MAX : (csr.off + csr.len - 1) >> PAGE_SHIFT;\n\tmemset(&cs, 0, sizeof(struct cachestat));\n\tmapping = f.file->f_mapping;\n\tfilemap_cachestat(mapping, first_index, last_index, &cs);\n\tfdput(f);\n\n\tif (copy_to_user(cstat, &cs, sizeof(struct cachestat)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}