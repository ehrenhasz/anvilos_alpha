{
  "module_name": "khugepaged.c",
  "hash_id": "bb7d598e01307a9d53028119607d9930cbe7acd1fdbff3bd2bf878fbc22780ad",
  "original_prompt": "Ingested from linux-6.6.14/mm/khugepaged.c",
  "human_readable_source": "\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/mm.h>\n#include <linux/sched.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/coredump.h>\n#include <linux/mmu_notifier.h>\n#include <linux/rmap.h>\n#include <linux/swap.h>\n#include <linux/mm_inline.h>\n#include <linux/kthread.h>\n#include <linux/khugepaged.h>\n#include <linux/freezer.h>\n#include <linux/mman.h>\n#include <linux/hashtable.h>\n#include <linux/userfaultfd_k.h>\n#include <linux/page_idle.h>\n#include <linux/page_table_check.h>\n#include <linux/swapops.h>\n#include <linux/shmem_fs.h>\n#include <linux/ksm.h>\n\n#include <asm/tlb.h>\n#include <asm/pgalloc.h>\n#include \"internal.h\"\n#include \"mm_slot.h\"\n\nenum scan_result {\n\tSCAN_FAIL,\n\tSCAN_SUCCEED,\n\tSCAN_PMD_NULL,\n\tSCAN_PMD_NONE,\n\tSCAN_PMD_MAPPED,\n\tSCAN_EXCEED_NONE_PTE,\n\tSCAN_EXCEED_SWAP_PTE,\n\tSCAN_EXCEED_SHARED_PTE,\n\tSCAN_PTE_NON_PRESENT,\n\tSCAN_PTE_UFFD_WP,\n\tSCAN_PTE_MAPPED_HUGEPAGE,\n\tSCAN_PAGE_RO,\n\tSCAN_LACK_REFERENCED_PAGE,\n\tSCAN_PAGE_NULL,\n\tSCAN_SCAN_ABORT,\n\tSCAN_PAGE_COUNT,\n\tSCAN_PAGE_LRU,\n\tSCAN_PAGE_LOCK,\n\tSCAN_PAGE_ANON,\n\tSCAN_PAGE_COMPOUND,\n\tSCAN_ANY_PROCESS,\n\tSCAN_VMA_NULL,\n\tSCAN_VMA_CHECK,\n\tSCAN_ADDRESS_RANGE,\n\tSCAN_DEL_PAGE_LRU,\n\tSCAN_ALLOC_HUGE_PAGE_FAIL,\n\tSCAN_CGROUP_CHARGE_FAIL,\n\tSCAN_TRUNCATED,\n\tSCAN_PAGE_HAS_PRIVATE,\n\tSCAN_STORE_FAILED,\n\tSCAN_COPY_MC,\n\tSCAN_PAGE_FILLED,\n};\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/huge_memory.h>\n\nstatic struct task_struct *khugepaged_thread __read_mostly;\nstatic DEFINE_MUTEX(khugepaged_mutex);\n\n \nstatic unsigned int khugepaged_pages_to_scan __read_mostly;\nstatic unsigned int khugepaged_pages_collapsed;\nstatic unsigned int khugepaged_full_scans;\nstatic unsigned int khugepaged_scan_sleep_millisecs __read_mostly = 10000;\n \nstatic unsigned int khugepaged_alloc_sleep_millisecs __read_mostly = 60000;\nstatic unsigned long khugepaged_sleep_expire;\nstatic DEFINE_SPINLOCK(khugepaged_mm_lock);\nstatic DECLARE_WAIT_QUEUE_HEAD(khugepaged_wait);\n \nstatic unsigned int khugepaged_max_ptes_none __read_mostly;\nstatic unsigned int khugepaged_max_ptes_swap __read_mostly;\nstatic unsigned int khugepaged_max_ptes_shared __read_mostly;\n\n#define MM_SLOTS_HASH_BITS 10\nstatic DEFINE_READ_MOSTLY_HASHTABLE(mm_slots_hash, MM_SLOTS_HASH_BITS);\n\nstatic struct kmem_cache *mm_slot_cache __read_mostly;\n\nstruct collapse_control {\n\tbool is_khugepaged;\n\n\t \n\tu32 node_load[MAX_NUMNODES];\n\n\t \n\tnodemask_t alloc_nmask;\n};\n\n \nstruct khugepaged_mm_slot {\n\tstruct mm_slot slot;\n};\n\n \nstruct khugepaged_scan {\n\tstruct list_head mm_head;\n\tstruct khugepaged_mm_slot *mm_slot;\n\tunsigned long address;\n};\n\nstatic struct khugepaged_scan khugepaged_scan = {\n\t.mm_head = LIST_HEAD_INIT(khugepaged_scan.mm_head),\n};\n\n#ifdef CONFIG_SYSFS\nstatic ssize_t scan_sleep_millisecs_show(struct kobject *kobj,\n\t\t\t\t\t struct kobj_attribute *attr,\n\t\t\t\t\t char *buf)\n{\n\treturn sysfs_emit(buf, \"%u\\n\", khugepaged_scan_sleep_millisecs);\n}\n\nstatic ssize_t scan_sleep_millisecs_store(struct kobject *kobj,\n\t\t\t\t\t  struct kobj_attribute *attr,\n\t\t\t\t\t  const char *buf, size_t count)\n{\n\tunsigned int msecs;\n\tint err;\n\n\terr = kstrtouint(buf, 10, &msecs);\n\tif (err)\n\t\treturn -EINVAL;\n\n\tkhugepaged_scan_sleep_millisecs = msecs;\n\tkhugepaged_sleep_expire = 0;\n\twake_up_interruptible(&khugepaged_wait);\n\n\treturn count;\n}\nstatic struct kobj_attribute scan_sleep_millisecs_attr =\n\t__ATTR_RW(scan_sleep_millisecs);\n\nstatic ssize_t alloc_sleep_millisecs_show(struct kobject *kobj,\n\t\t\t\t\t  struct kobj_attribute *attr,\n\t\t\t\t\t  char *buf)\n{\n\treturn sysfs_emit(buf, \"%u\\n\", khugepaged_alloc_sleep_millisecs);\n}\n\nstatic ssize_t alloc_sleep_millisecs_store(struct kobject *kobj,\n\t\t\t\t\t   struct kobj_attribute *attr,\n\t\t\t\t\t   const char *buf, size_t count)\n{\n\tunsigned int msecs;\n\tint err;\n\n\terr = kstrtouint(buf, 10, &msecs);\n\tif (err)\n\t\treturn -EINVAL;\n\n\tkhugepaged_alloc_sleep_millisecs = msecs;\n\tkhugepaged_sleep_expire = 0;\n\twake_up_interruptible(&khugepaged_wait);\n\n\treturn count;\n}\nstatic struct kobj_attribute alloc_sleep_millisecs_attr =\n\t__ATTR_RW(alloc_sleep_millisecs);\n\nstatic ssize_t pages_to_scan_show(struct kobject *kobj,\n\t\t\t\t  struct kobj_attribute *attr,\n\t\t\t\t  char *buf)\n{\n\treturn sysfs_emit(buf, \"%u\\n\", khugepaged_pages_to_scan);\n}\nstatic ssize_t pages_to_scan_store(struct kobject *kobj,\n\t\t\t\t   struct kobj_attribute *attr,\n\t\t\t\t   const char *buf, size_t count)\n{\n\tunsigned int pages;\n\tint err;\n\n\terr = kstrtouint(buf, 10, &pages);\n\tif (err || !pages)\n\t\treturn -EINVAL;\n\n\tkhugepaged_pages_to_scan = pages;\n\n\treturn count;\n}\nstatic struct kobj_attribute pages_to_scan_attr =\n\t__ATTR_RW(pages_to_scan);\n\nstatic ssize_t pages_collapsed_show(struct kobject *kobj,\n\t\t\t\t    struct kobj_attribute *attr,\n\t\t\t\t    char *buf)\n{\n\treturn sysfs_emit(buf, \"%u\\n\", khugepaged_pages_collapsed);\n}\nstatic struct kobj_attribute pages_collapsed_attr =\n\t__ATTR_RO(pages_collapsed);\n\nstatic ssize_t full_scans_show(struct kobject *kobj,\n\t\t\t       struct kobj_attribute *attr,\n\t\t\t       char *buf)\n{\n\treturn sysfs_emit(buf, \"%u\\n\", khugepaged_full_scans);\n}\nstatic struct kobj_attribute full_scans_attr =\n\t__ATTR_RO(full_scans);\n\nstatic ssize_t defrag_show(struct kobject *kobj,\n\t\t\t   struct kobj_attribute *attr, char *buf)\n{\n\treturn single_hugepage_flag_show(kobj, attr, buf,\n\t\t\t\t\t TRANSPARENT_HUGEPAGE_DEFRAG_KHUGEPAGED_FLAG);\n}\nstatic ssize_t defrag_store(struct kobject *kobj,\n\t\t\t    struct kobj_attribute *attr,\n\t\t\t    const char *buf, size_t count)\n{\n\treturn single_hugepage_flag_store(kobj, attr, buf, count,\n\t\t\t\t TRANSPARENT_HUGEPAGE_DEFRAG_KHUGEPAGED_FLAG);\n}\nstatic struct kobj_attribute khugepaged_defrag_attr =\n\t__ATTR_RW(defrag);\n\n \nstatic ssize_t max_ptes_none_show(struct kobject *kobj,\n\t\t\t\t  struct kobj_attribute *attr,\n\t\t\t\t  char *buf)\n{\n\treturn sysfs_emit(buf, \"%u\\n\", khugepaged_max_ptes_none);\n}\nstatic ssize_t max_ptes_none_store(struct kobject *kobj,\n\t\t\t\t   struct kobj_attribute *attr,\n\t\t\t\t   const char *buf, size_t count)\n{\n\tint err;\n\tunsigned long max_ptes_none;\n\n\terr = kstrtoul(buf, 10, &max_ptes_none);\n\tif (err || max_ptes_none > HPAGE_PMD_NR - 1)\n\t\treturn -EINVAL;\n\n\tkhugepaged_max_ptes_none = max_ptes_none;\n\n\treturn count;\n}\nstatic struct kobj_attribute khugepaged_max_ptes_none_attr =\n\t__ATTR_RW(max_ptes_none);\n\nstatic ssize_t max_ptes_swap_show(struct kobject *kobj,\n\t\t\t\t  struct kobj_attribute *attr,\n\t\t\t\t  char *buf)\n{\n\treturn sysfs_emit(buf, \"%u\\n\", khugepaged_max_ptes_swap);\n}\n\nstatic ssize_t max_ptes_swap_store(struct kobject *kobj,\n\t\t\t\t   struct kobj_attribute *attr,\n\t\t\t\t   const char *buf, size_t count)\n{\n\tint err;\n\tunsigned long max_ptes_swap;\n\n\terr  = kstrtoul(buf, 10, &max_ptes_swap);\n\tif (err || max_ptes_swap > HPAGE_PMD_NR - 1)\n\t\treturn -EINVAL;\n\n\tkhugepaged_max_ptes_swap = max_ptes_swap;\n\n\treturn count;\n}\n\nstatic struct kobj_attribute khugepaged_max_ptes_swap_attr =\n\t__ATTR_RW(max_ptes_swap);\n\nstatic ssize_t max_ptes_shared_show(struct kobject *kobj,\n\t\t\t\t    struct kobj_attribute *attr,\n\t\t\t\t    char *buf)\n{\n\treturn sysfs_emit(buf, \"%u\\n\", khugepaged_max_ptes_shared);\n}\n\nstatic ssize_t max_ptes_shared_store(struct kobject *kobj,\n\t\t\t\t     struct kobj_attribute *attr,\n\t\t\t\t     const char *buf, size_t count)\n{\n\tint err;\n\tunsigned long max_ptes_shared;\n\n\terr  = kstrtoul(buf, 10, &max_ptes_shared);\n\tif (err || max_ptes_shared > HPAGE_PMD_NR - 1)\n\t\treturn -EINVAL;\n\n\tkhugepaged_max_ptes_shared = max_ptes_shared;\n\n\treturn count;\n}\n\nstatic struct kobj_attribute khugepaged_max_ptes_shared_attr =\n\t__ATTR_RW(max_ptes_shared);\n\nstatic struct attribute *khugepaged_attr[] = {\n\t&khugepaged_defrag_attr.attr,\n\t&khugepaged_max_ptes_none_attr.attr,\n\t&khugepaged_max_ptes_swap_attr.attr,\n\t&khugepaged_max_ptes_shared_attr.attr,\n\t&pages_to_scan_attr.attr,\n\t&pages_collapsed_attr.attr,\n\t&full_scans_attr.attr,\n\t&scan_sleep_millisecs_attr.attr,\n\t&alloc_sleep_millisecs_attr.attr,\n\tNULL,\n};\n\nstruct attribute_group khugepaged_attr_group = {\n\t.attrs = khugepaged_attr,\n\t.name = \"khugepaged\",\n};\n#endif  \n\nint hugepage_madvise(struct vm_area_struct *vma,\n\t\t     unsigned long *vm_flags, int advice)\n{\n\tswitch (advice) {\n\tcase MADV_HUGEPAGE:\n#ifdef CONFIG_S390\n\t\t \n\t\tif (mm_has_pgste(vma->vm_mm))\n\t\t\treturn 0;\n#endif\n\t\t*vm_flags &= ~VM_NOHUGEPAGE;\n\t\t*vm_flags |= VM_HUGEPAGE;\n\t\t \n\t\tkhugepaged_enter_vma(vma, *vm_flags);\n\t\tbreak;\n\tcase MADV_NOHUGEPAGE:\n\t\t*vm_flags &= ~VM_HUGEPAGE;\n\t\t*vm_flags |= VM_NOHUGEPAGE;\n\t\t \n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nint __init khugepaged_init(void)\n{\n\tmm_slot_cache = kmem_cache_create(\"khugepaged_mm_slot\",\n\t\t\t\t\t  sizeof(struct khugepaged_mm_slot),\n\t\t\t\t\t  __alignof__(struct khugepaged_mm_slot),\n\t\t\t\t\t  0, NULL);\n\tif (!mm_slot_cache)\n\t\treturn -ENOMEM;\n\n\tkhugepaged_pages_to_scan = HPAGE_PMD_NR * 8;\n\tkhugepaged_max_ptes_none = HPAGE_PMD_NR - 1;\n\tkhugepaged_max_ptes_swap = HPAGE_PMD_NR / 8;\n\tkhugepaged_max_ptes_shared = HPAGE_PMD_NR / 2;\n\n\treturn 0;\n}\n\nvoid __init khugepaged_destroy(void)\n{\n\tkmem_cache_destroy(mm_slot_cache);\n}\n\nstatic inline int hpage_collapse_test_exit(struct mm_struct *mm)\n{\n\treturn atomic_read(&mm->mm_users) == 0;\n}\n\nvoid __khugepaged_enter(struct mm_struct *mm)\n{\n\tstruct khugepaged_mm_slot *mm_slot;\n\tstruct mm_slot *slot;\n\tint wakeup;\n\n\t \n\tVM_BUG_ON_MM(hpage_collapse_test_exit(mm), mm);\n\tif (unlikely(test_and_set_bit(MMF_VM_HUGEPAGE, &mm->flags)))\n\t\treturn;\n\n\tmm_slot = mm_slot_alloc(mm_slot_cache);\n\tif (!mm_slot)\n\t\treturn;\n\n\tslot = &mm_slot->slot;\n\n\tspin_lock(&khugepaged_mm_lock);\n\tmm_slot_insert(mm_slots_hash, mm, slot);\n\t \n\twakeup = list_empty(&khugepaged_scan.mm_head);\n\tlist_add_tail(&slot->mm_node, &khugepaged_scan.mm_head);\n\tspin_unlock(&khugepaged_mm_lock);\n\n\tmmgrab(mm);\n\tif (wakeup)\n\t\twake_up_interruptible(&khugepaged_wait);\n}\n\nvoid khugepaged_enter_vma(struct vm_area_struct *vma,\n\t\t\t  unsigned long vm_flags)\n{\n\tif (!test_bit(MMF_VM_HUGEPAGE, &vma->vm_mm->flags) &&\n\t    hugepage_flags_enabled()) {\n\t\tif (hugepage_vma_check(vma, vm_flags, false, false, true))\n\t\t\t__khugepaged_enter(vma->vm_mm);\n\t}\n}\n\nvoid __khugepaged_exit(struct mm_struct *mm)\n{\n\tstruct khugepaged_mm_slot *mm_slot;\n\tstruct mm_slot *slot;\n\tint free = 0;\n\n\tspin_lock(&khugepaged_mm_lock);\n\tslot = mm_slot_lookup(mm_slots_hash, mm);\n\tmm_slot = mm_slot_entry(slot, struct khugepaged_mm_slot, slot);\n\tif (mm_slot && khugepaged_scan.mm_slot != mm_slot) {\n\t\thash_del(&slot->hash);\n\t\tlist_del(&slot->mm_node);\n\t\tfree = 1;\n\t}\n\tspin_unlock(&khugepaged_mm_lock);\n\n\tif (free) {\n\t\tclear_bit(MMF_VM_HUGEPAGE, &mm->flags);\n\t\tmm_slot_free(mm_slot_cache, mm_slot);\n\t\tmmdrop(mm);\n\t} else if (mm_slot) {\n\t\t \n\t\tmmap_write_lock(mm);\n\t\tmmap_write_unlock(mm);\n\t}\n}\n\nstatic void release_pte_folio(struct folio *folio)\n{\n\tnode_stat_mod_folio(folio,\n\t\t\tNR_ISOLATED_ANON + folio_is_file_lru(folio),\n\t\t\t-folio_nr_pages(folio));\n\tfolio_unlock(folio);\n\tfolio_putback_lru(folio);\n}\n\nstatic void release_pte_page(struct page *page)\n{\n\trelease_pte_folio(page_folio(page));\n}\n\nstatic void release_pte_pages(pte_t *pte, pte_t *_pte,\n\t\tstruct list_head *compound_pagelist)\n{\n\tstruct folio *folio, *tmp;\n\n\twhile (--_pte >= pte) {\n\t\tpte_t pteval = ptep_get(_pte);\n\t\tunsigned long pfn;\n\n\t\tif (pte_none(pteval))\n\t\t\tcontinue;\n\t\tpfn = pte_pfn(pteval);\n\t\tif (is_zero_pfn(pfn))\n\t\t\tcontinue;\n\t\tfolio = pfn_folio(pfn);\n\t\tif (folio_test_large(folio))\n\t\t\tcontinue;\n\t\trelease_pte_folio(folio);\n\t}\n\n\tlist_for_each_entry_safe(folio, tmp, compound_pagelist, lru) {\n\t\tlist_del(&folio->lru);\n\t\trelease_pte_folio(folio);\n\t}\n}\n\nstatic bool is_refcount_suitable(struct page *page)\n{\n\tint expected_refcount;\n\n\texpected_refcount = total_mapcount(page);\n\tif (PageSwapCache(page))\n\t\texpected_refcount += compound_nr(page);\n\n\treturn page_count(page) == expected_refcount;\n}\n\nstatic int __collapse_huge_page_isolate(struct vm_area_struct *vma,\n\t\t\t\t\tunsigned long address,\n\t\t\t\t\tpte_t *pte,\n\t\t\t\t\tstruct collapse_control *cc,\n\t\t\t\t\tstruct list_head *compound_pagelist)\n{\n\tstruct page *page = NULL;\n\tpte_t *_pte;\n\tint none_or_zero = 0, shared = 0, result = SCAN_FAIL, referenced = 0;\n\tbool writable = false;\n\n\tfor (_pte = pte; _pte < pte + HPAGE_PMD_NR;\n\t     _pte++, address += PAGE_SIZE) {\n\t\tpte_t pteval = ptep_get(_pte);\n\t\tif (pte_none(pteval) || (pte_present(pteval) &&\n\t\t\t\tis_zero_pfn(pte_pfn(pteval)))) {\n\t\t\t++none_or_zero;\n\t\t\tif (!userfaultfd_armed(vma) &&\n\t\t\t    (!cc->is_khugepaged ||\n\t\t\t     none_or_zero <= khugepaged_max_ptes_none)) {\n\t\t\t\tcontinue;\n\t\t\t} else {\n\t\t\t\tresult = SCAN_EXCEED_NONE_PTE;\n\t\t\t\tcount_vm_event(THP_SCAN_EXCEED_NONE_PTE);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\tif (!pte_present(pteval)) {\n\t\t\tresult = SCAN_PTE_NON_PRESENT;\n\t\t\tgoto out;\n\t\t}\n\t\tif (pte_uffd_wp(pteval)) {\n\t\t\tresult = SCAN_PTE_UFFD_WP;\n\t\t\tgoto out;\n\t\t}\n\t\tpage = vm_normal_page(vma, address, pteval);\n\t\tif (unlikely(!page) || unlikely(is_zone_device_page(page))) {\n\t\t\tresult = SCAN_PAGE_NULL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tVM_BUG_ON_PAGE(!PageAnon(page), page);\n\n\t\tif (page_mapcount(page) > 1) {\n\t\t\t++shared;\n\t\t\tif (cc->is_khugepaged &&\n\t\t\t    shared > khugepaged_max_ptes_shared) {\n\t\t\t\tresult = SCAN_EXCEED_SHARED_PTE;\n\t\t\t\tcount_vm_event(THP_SCAN_EXCEED_SHARED_PTE);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\tif (PageCompound(page)) {\n\t\t\tstruct page *p;\n\t\t\tpage = compound_head(page);\n\n\t\t\t \n\t\t\tlist_for_each_entry(p, compound_pagelist, lru) {\n\t\t\t\tif (page == p)\n\t\t\t\t\tgoto next;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (!trylock_page(page)) {\n\t\t\tresult = SCAN_PAGE_LOCK;\n\t\t\tgoto out;\n\t\t}\n\n\t\t \n\t\tif (!is_refcount_suitable(page)) {\n\t\t\tunlock_page(page);\n\t\t\tresult = SCAN_PAGE_COUNT;\n\t\t\tgoto out;\n\t\t}\n\n\t\t \n\t\tif (!isolate_lru_page(page)) {\n\t\t\tunlock_page(page);\n\t\t\tresult = SCAN_DEL_PAGE_LRU;\n\t\t\tgoto out;\n\t\t}\n\t\tmod_node_page_state(page_pgdat(page),\n\t\t\t\tNR_ISOLATED_ANON + page_is_file_lru(page),\n\t\t\t\tcompound_nr(page));\n\t\tVM_BUG_ON_PAGE(!PageLocked(page), page);\n\t\tVM_BUG_ON_PAGE(PageLRU(page), page);\n\n\t\tif (PageCompound(page))\n\t\t\tlist_add_tail(&page->lru, compound_pagelist);\nnext:\n\t\t \n\t\tif (cc->is_khugepaged &&\n\t\t    (pte_young(pteval) || page_is_young(page) ||\n\t\t     PageReferenced(page) || mmu_notifier_test_young(vma->vm_mm,\n\t\t\t\t\t\t\t\t     address)))\n\t\t\treferenced++;\n\n\t\tif (pte_write(pteval))\n\t\t\twritable = true;\n\t}\n\n\tif (unlikely(!writable)) {\n\t\tresult = SCAN_PAGE_RO;\n\t} else if (unlikely(cc->is_khugepaged && !referenced)) {\n\t\tresult = SCAN_LACK_REFERENCED_PAGE;\n\t} else {\n\t\tresult = SCAN_SUCCEED;\n\t\ttrace_mm_collapse_huge_page_isolate(page, none_or_zero,\n\t\t\t\t\t\t    referenced, writable, result);\n\t\treturn result;\n\t}\nout:\n\trelease_pte_pages(pte, _pte, compound_pagelist);\n\ttrace_mm_collapse_huge_page_isolate(page, none_or_zero,\n\t\t\t\t\t    referenced, writable, result);\n\treturn result;\n}\n\nstatic void __collapse_huge_page_copy_succeeded(pte_t *pte,\n\t\t\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\t\t\tunsigned long address,\n\t\t\t\t\t\tspinlock_t *ptl,\n\t\t\t\t\t\tstruct list_head *compound_pagelist)\n{\n\tstruct page *src_page;\n\tstruct page *tmp;\n\tpte_t *_pte;\n\tpte_t pteval;\n\n\tfor (_pte = pte; _pte < pte + HPAGE_PMD_NR;\n\t     _pte++, address += PAGE_SIZE) {\n\t\tpteval = ptep_get(_pte);\n\t\tif (pte_none(pteval) || is_zero_pfn(pte_pfn(pteval))) {\n\t\t\tadd_mm_counter(vma->vm_mm, MM_ANONPAGES, 1);\n\t\t\tif (is_zero_pfn(pte_pfn(pteval))) {\n\t\t\t\t \n\t\t\t\tspin_lock(ptl);\n\t\t\t\tptep_clear(vma->vm_mm, address, _pte);\n\t\t\t\tspin_unlock(ptl);\n\t\t\t\tksm_might_unmap_zero_page(vma->vm_mm, pteval);\n\t\t\t}\n\t\t} else {\n\t\t\tsrc_page = pte_page(pteval);\n\t\t\tif (!PageCompound(src_page))\n\t\t\t\trelease_pte_page(src_page);\n\t\t\t \n\t\t\tspin_lock(ptl);\n\t\t\tptep_clear(vma->vm_mm, address, _pte);\n\t\t\tpage_remove_rmap(src_page, vma, false);\n\t\t\tspin_unlock(ptl);\n\t\t\tfree_page_and_swap_cache(src_page);\n\t\t}\n\t}\n\n\tlist_for_each_entry_safe(src_page, tmp, compound_pagelist, lru) {\n\t\tlist_del(&src_page->lru);\n\t\tmod_node_page_state(page_pgdat(src_page),\n\t\t\t\t    NR_ISOLATED_ANON + page_is_file_lru(src_page),\n\t\t\t\t    -compound_nr(src_page));\n\t\tunlock_page(src_page);\n\t\tfree_swap_cache(src_page);\n\t\tputback_lru_page(src_page);\n\t}\n}\n\nstatic void __collapse_huge_page_copy_failed(pte_t *pte,\n\t\t\t\t\t     pmd_t *pmd,\n\t\t\t\t\t     pmd_t orig_pmd,\n\t\t\t\t\t     struct vm_area_struct *vma,\n\t\t\t\t\t     struct list_head *compound_pagelist)\n{\n\tspinlock_t *pmd_ptl;\n\n\t \n\tpmd_ptl = pmd_lock(vma->vm_mm, pmd);\n\tpmd_populate(vma->vm_mm, pmd, pmd_pgtable(orig_pmd));\n\tspin_unlock(pmd_ptl);\n\t \n\trelease_pte_pages(pte, pte + HPAGE_PMD_NR, compound_pagelist);\n}\n\n \nstatic int __collapse_huge_page_copy(pte_t *pte,\n\t\t\t\t     struct page *page,\n\t\t\t\t     pmd_t *pmd,\n\t\t\t\t     pmd_t orig_pmd,\n\t\t\t\t     struct vm_area_struct *vma,\n\t\t\t\t     unsigned long address,\n\t\t\t\t     spinlock_t *ptl,\n\t\t\t\t     struct list_head *compound_pagelist)\n{\n\tstruct page *src_page;\n\tpte_t *_pte;\n\tpte_t pteval;\n\tunsigned long _address;\n\tint result = SCAN_SUCCEED;\n\n\t \n\tfor (_pte = pte, _address = address; _pte < pte + HPAGE_PMD_NR;\n\t     _pte++, page++, _address += PAGE_SIZE) {\n\t\tpteval = ptep_get(_pte);\n\t\tif (pte_none(pteval) || is_zero_pfn(pte_pfn(pteval))) {\n\t\t\tclear_user_highpage(page, _address);\n\t\t\tcontinue;\n\t\t}\n\t\tsrc_page = pte_page(pteval);\n\t\tif (copy_mc_user_highpage(page, src_page, _address, vma) > 0) {\n\t\t\tresult = SCAN_COPY_MC;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (likely(result == SCAN_SUCCEED))\n\t\t__collapse_huge_page_copy_succeeded(pte, vma, address, ptl,\n\t\t\t\t\t\t    compound_pagelist);\n\telse\n\t\t__collapse_huge_page_copy_failed(pte, pmd, orig_pmd, vma,\n\t\t\t\t\t\t compound_pagelist);\n\n\treturn result;\n}\n\nstatic void khugepaged_alloc_sleep(void)\n{\n\tDEFINE_WAIT(wait);\n\n\tadd_wait_queue(&khugepaged_wait, &wait);\n\t__set_current_state(TASK_INTERRUPTIBLE|TASK_FREEZABLE);\n\tschedule_timeout(msecs_to_jiffies(khugepaged_alloc_sleep_millisecs));\n\tremove_wait_queue(&khugepaged_wait, &wait);\n}\n\nstruct collapse_control khugepaged_collapse_control = {\n\t.is_khugepaged = true,\n};\n\nstatic bool hpage_collapse_scan_abort(int nid, struct collapse_control *cc)\n{\n\tint i;\n\n\t \n\tif (!node_reclaim_enabled())\n\t\treturn false;\n\n\t \n\tif (cc->node_load[nid])\n\t\treturn false;\n\n\tfor (i = 0; i < MAX_NUMNODES; i++) {\n\t\tif (!cc->node_load[i])\n\t\t\tcontinue;\n\t\tif (node_distance(nid, i) > node_reclaim_distance)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\n#define khugepaged_defrag()\t\t\t\t\t\\\n\t(transparent_hugepage_flags &\t\t\t\t\\\n\t (1<<TRANSPARENT_HUGEPAGE_DEFRAG_KHUGEPAGED_FLAG))\n\n \nstatic inline gfp_t alloc_hugepage_khugepaged_gfpmask(void)\n{\n\treturn khugepaged_defrag() ? GFP_TRANSHUGE : GFP_TRANSHUGE_LIGHT;\n}\n\n#ifdef CONFIG_NUMA\nstatic int hpage_collapse_find_target_node(struct collapse_control *cc)\n{\n\tint nid, target_node = 0, max_value = 0;\n\n\t \n\tfor (nid = 0; nid < MAX_NUMNODES; nid++)\n\t\tif (cc->node_load[nid] > max_value) {\n\t\t\tmax_value = cc->node_load[nid];\n\t\t\ttarget_node = nid;\n\t\t}\n\n\tfor_each_online_node(nid) {\n\t\tif (max_value == cc->node_load[nid])\n\t\t\tnode_set(nid, cc->alloc_nmask);\n\t}\n\n\treturn target_node;\n}\n#else\nstatic int hpage_collapse_find_target_node(struct collapse_control *cc)\n{\n\treturn 0;\n}\n#endif\n\nstatic bool hpage_collapse_alloc_page(struct page **hpage, gfp_t gfp, int node,\n\t\t\t\t      nodemask_t *nmask)\n{\n\t*hpage = __alloc_pages(gfp, HPAGE_PMD_ORDER, node, nmask);\n\tif (unlikely(!*hpage)) {\n\t\tcount_vm_event(THP_COLLAPSE_ALLOC_FAILED);\n\t\treturn false;\n\t}\n\n\tfolio_prep_large_rmappable((struct folio *)*hpage);\n\tcount_vm_event(THP_COLLAPSE_ALLOC);\n\treturn true;\n}\n\n \n\nstatic int hugepage_vma_revalidate(struct mm_struct *mm, unsigned long address,\n\t\t\t\t   bool expect_anon,\n\t\t\t\t   struct vm_area_struct **vmap,\n\t\t\t\t   struct collapse_control *cc)\n{\n\tstruct vm_area_struct *vma;\n\n\tif (unlikely(hpage_collapse_test_exit(mm)))\n\t\treturn SCAN_ANY_PROCESS;\n\n\t*vmap = vma = find_vma(mm, address);\n\tif (!vma)\n\t\treturn SCAN_VMA_NULL;\n\n\tif (!transhuge_vma_suitable(vma, address))\n\t\treturn SCAN_ADDRESS_RANGE;\n\tif (!hugepage_vma_check(vma, vma->vm_flags, false, false,\n\t\t\t\tcc->is_khugepaged))\n\t\treturn SCAN_VMA_CHECK;\n\t \n\tif (expect_anon && (!(*vmap)->anon_vma || !vma_is_anonymous(*vmap)))\n\t\treturn SCAN_PAGE_ANON;\n\treturn SCAN_SUCCEED;\n}\n\nstatic int find_pmd_or_thp_or_none(struct mm_struct *mm,\n\t\t\t\t   unsigned long address,\n\t\t\t\t   pmd_t **pmd)\n{\n\tpmd_t pmde;\n\n\t*pmd = mm_find_pmd(mm, address);\n\tif (!*pmd)\n\t\treturn SCAN_PMD_NULL;\n\n\tpmde = pmdp_get_lockless(*pmd);\n\tif (pmd_none(pmde))\n\t\treturn SCAN_PMD_NONE;\n\tif (!pmd_present(pmde))\n\t\treturn SCAN_PMD_NULL;\n\tif (pmd_trans_huge(pmde))\n\t\treturn SCAN_PMD_MAPPED;\n\tif (pmd_devmap(pmde))\n\t\treturn SCAN_PMD_NULL;\n\tif (pmd_bad(pmde))\n\t\treturn SCAN_PMD_NULL;\n\treturn SCAN_SUCCEED;\n}\n\nstatic int check_pmd_still_valid(struct mm_struct *mm,\n\t\t\t\t unsigned long address,\n\t\t\t\t pmd_t *pmd)\n{\n\tpmd_t *new_pmd;\n\tint result = find_pmd_or_thp_or_none(mm, address, &new_pmd);\n\n\tif (result != SCAN_SUCCEED)\n\t\treturn result;\n\tif (new_pmd != pmd)\n\t\treturn SCAN_FAIL;\n\treturn SCAN_SUCCEED;\n}\n\n \nstatic int __collapse_huge_page_swapin(struct mm_struct *mm,\n\t\t\t\t       struct vm_area_struct *vma,\n\t\t\t\t       unsigned long haddr, pmd_t *pmd,\n\t\t\t\t       int referenced)\n{\n\tint swapped_in = 0;\n\tvm_fault_t ret = 0;\n\tunsigned long address, end = haddr + (HPAGE_PMD_NR * PAGE_SIZE);\n\tint result;\n\tpte_t *pte = NULL;\n\tspinlock_t *ptl;\n\n\tfor (address = haddr; address < end; address += PAGE_SIZE) {\n\t\tstruct vm_fault vmf = {\n\t\t\t.vma = vma,\n\t\t\t.address = address,\n\t\t\t.pgoff = linear_page_index(vma, address),\n\t\t\t.flags = FAULT_FLAG_ALLOW_RETRY,\n\t\t\t.pmd = pmd,\n\t\t};\n\n\t\tif (!pte++) {\n\t\t\tpte = pte_offset_map_nolock(mm, pmd, address, &ptl);\n\t\t\tif (!pte) {\n\t\t\t\tmmap_read_unlock(mm);\n\t\t\t\tresult = SCAN_PMD_NULL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\tvmf.orig_pte = ptep_get_lockless(pte);\n\t\tif (!is_swap_pte(vmf.orig_pte))\n\t\t\tcontinue;\n\n\t\tvmf.pte = pte;\n\t\tvmf.ptl = ptl;\n\t\tret = do_swap_page(&vmf);\n\t\t \n\t\tpte = NULL;\n\n\t\t \n\t\tif (ret & VM_FAULT_RETRY) {\n\t\t\t \n\t\t\tresult = SCAN_PAGE_LOCK;\n\t\t\tgoto out;\n\t\t}\n\t\tif (ret & VM_FAULT_ERROR) {\n\t\t\tmmap_read_unlock(mm);\n\t\t\tresult = SCAN_FAIL;\n\t\t\tgoto out;\n\t\t}\n\t\tswapped_in++;\n\t}\n\n\tif (pte)\n\t\tpte_unmap(pte);\n\n\t \n\tif (swapped_in)\n\t\tlru_add_drain();\n\n\tresult = SCAN_SUCCEED;\nout:\n\ttrace_mm_collapse_huge_page_swapin(mm, swapped_in, referenced, result);\n\treturn result;\n}\n\nstatic int alloc_charge_hpage(struct page **hpage, struct mm_struct *mm,\n\t\t\t      struct collapse_control *cc)\n{\n\tgfp_t gfp = (cc->is_khugepaged ? alloc_hugepage_khugepaged_gfpmask() :\n\t\t     GFP_TRANSHUGE);\n\tint node = hpage_collapse_find_target_node(cc);\n\tstruct folio *folio;\n\n\tif (!hpage_collapse_alloc_page(hpage, gfp, node, &cc->alloc_nmask))\n\t\treturn SCAN_ALLOC_HUGE_PAGE_FAIL;\n\n\tfolio = page_folio(*hpage);\n\tif (unlikely(mem_cgroup_charge(folio, mm, gfp))) {\n\t\tfolio_put(folio);\n\t\t*hpage = NULL;\n\t\treturn SCAN_CGROUP_CHARGE_FAIL;\n\t}\n\tcount_memcg_page_event(*hpage, THP_COLLAPSE_ALLOC);\n\n\treturn SCAN_SUCCEED;\n}\n\nstatic int collapse_huge_page(struct mm_struct *mm, unsigned long address,\n\t\t\t      int referenced, int unmapped,\n\t\t\t      struct collapse_control *cc)\n{\n\tLIST_HEAD(compound_pagelist);\n\tpmd_t *pmd, _pmd;\n\tpte_t *pte;\n\tpgtable_t pgtable;\n\tstruct page *hpage;\n\tspinlock_t *pmd_ptl, *pte_ptl;\n\tint result = SCAN_FAIL;\n\tstruct vm_area_struct *vma;\n\tstruct mmu_notifier_range range;\n\n\tVM_BUG_ON(address & ~HPAGE_PMD_MASK);\n\n\t \n\tmmap_read_unlock(mm);\n\n\tresult = alloc_charge_hpage(&hpage, mm, cc);\n\tif (result != SCAN_SUCCEED)\n\t\tgoto out_nolock;\n\n\tmmap_read_lock(mm);\n\tresult = hugepage_vma_revalidate(mm, address, true, &vma, cc);\n\tif (result != SCAN_SUCCEED) {\n\t\tmmap_read_unlock(mm);\n\t\tgoto out_nolock;\n\t}\n\n\tresult = find_pmd_or_thp_or_none(mm, address, &pmd);\n\tif (result != SCAN_SUCCEED) {\n\t\tmmap_read_unlock(mm);\n\t\tgoto out_nolock;\n\t}\n\n\tif (unmapped) {\n\t\t \n\t\tresult = __collapse_huge_page_swapin(mm, vma, address, pmd,\n\t\t\t\t\t\t     referenced);\n\t\tif (result != SCAN_SUCCEED)\n\t\t\tgoto out_nolock;\n\t}\n\n\tmmap_read_unlock(mm);\n\t \n\tmmap_write_lock(mm);\n\tresult = hugepage_vma_revalidate(mm, address, true, &vma, cc);\n\tif (result != SCAN_SUCCEED)\n\t\tgoto out_up_write;\n\t \n\tresult = check_pmd_still_valid(mm, address, pmd);\n\tif (result != SCAN_SUCCEED)\n\t\tgoto out_up_write;\n\n\tvma_start_write(vma);\n\tanon_vma_lock_write(vma->anon_vma);\n\n\tmmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, mm, address,\n\t\t\t\taddress + HPAGE_PMD_SIZE);\n\tmmu_notifier_invalidate_range_start(&range);\n\n\tpmd_ptl = pmd_lock(mm, pmd);  \n\t \n\t_pmd = pmdp_collapse_flush(vma, address, pmd);\n\tspin_unlock(pmd_ptl);\n\tmmu_notifier_invalidate_range_end(&range);\n\ttlb_remove_table_sync_one();\n\n\tpte = pte_offset_map_lock(mm, &_pmd, address, &pte_ptl);\n\tif (pte) {\n\t\tresult = __collapse_huge_page_isolate(vma, address, pte, cc,\n\t\t\t\t\t\t      &compound_pagelist);\n\t\tspin_unlock(pte_ptl);\n\t} else {\n\t\tresult = SCAN_PMD_NULL;\n\t}\n\n\tif (unlikely(result != SCAN_SUCCEED)) {\n\t\tif (pte)\n\t\t\tpte_unmap(pte);\n\t\tspin_lock(pmd_ptl);\n\t\tBUG_ON(!pmd_none(*pmd));\n\t\t \n\t\tpmd_populate(mm, pmd, pmd_pgtable(_pmd));\n\t\tspin_unlock(pmd_ptl);\n\t\tanon_vma_unlock_write(vma->anon_vma);\n\t\tgoto out_up_write;\n\t}\n\n\t \n\tanon_vma_unlock_write(vma->anon_vma);\n\n\tresult = __collapse_huge_page_copy(pte, hpage, pmd, _pmd,\n\t\t\t\t\t   vma, address, pte_ptl,\n\t\t\t\t\t   &compound_pagelist);\n\tpte_unmap(pte);\n\tif (unlikely(result != SCAN_SUCCEED))\n\t\tgoto out_up_write;\n\n\t \n\t__SetPageUptodate(hpage);\n\tpgtable = pmd_pgtable(_pmd);\n\n\t_pmd = mk_huge_pmd(hpage, vma->vm_page_prot);\n\t_pmd = maybe_pmd_mkwrite(pmd_mkdirty(_pmd), vma);\n\n\tspin_lock(pmd_ptl);\n\tBUG_ON(!pmd_none(*pmd));\n\tpage_add_new_anon_rmap(hpage, vma, address);\n\tlru_cache_add_inactive_or_unevictable(hpage, vma);\n\tpgtable_trans_huge_deposit(mm, pmd, pgtable);\n\tset_pmd_at(mm, address, pmd, _pmd);\n\tupdate_mmu_cache_pmd(vma, address, pmd);\n\tspin_unlock(pmd_ptl);\n\n\thpage = NULL;\n\n\tresult = SCAN_SUCCEED;\nout_up_write:\n\tmmap_write_unlock(mm);\nout_nolock:\n\tif (hpage)\n\t\tput_page(hpage);\n\ttrace_mm_collapse_huge_page(mm, result == SCAN_SUCCEED, result);\n\treturn result;\n}\n\nstatic int hpage_collapse_scan_pmd(struct mm_struct *mm,\n\t\t\t\t   struct vm_area_struct *vma,\n\t\t\t\t   unsigned long address, bool *mmap_locked,\n\t\t\t\t   struct collapse_control *cc)\n{\n\tpmd_t *pmd;\n\tpte_t *pte, *_pte;\n\tint result = SCAN_FAIL, referenced = 0;\n\tint none_or_zero = 0, shared = 0;\n\tstruct page *page = NULL;\n\tunsigned long _address;\n\tspinlock_t *ptl;\n\tint node = NUMA_NO_NODE, unmapped = 0;\n\tbool writable = false;\n\n\tVM_BUG_ON(address & ~HPAGE_PMD_MASK);\n\n\tresult = find_pmd_or_thp_or_none(mm, address, &pmd);\n\tif (result != SCAN_SUCCEED)\n\t\tgoto out;\n\n\tmemset(cc->node_load, 0, sizeof(cc->node_load));\n\tnodes_clear(cc->alloc_nmask);\n\tpte = pte_offset_map_lock(mm, pmd, address, &ptl);\n\tif (!pte) {\n\t\tresult = SCAN_PMD_NULL;\n\t\tgoto out;\n\t}\n\n\tfor (_address = address, _pte = pte; _pte < pte + HPAGE_PMD_NR;\n\t     _pte++, _address += PAGE_SIZE) {\n\t\tpte_t pteval = ptep_get(_pte);\n\t\tif (is_swap_pte(pteval)) {\n\t\t\t++unmapped;\n\t\t\tif (!cc->is_khugepaged ||\n\t\t\t    unmapped <= khugepaged_max_ptes_swap) {\n\t\t\t\t \n\t\t\t\tif (pte_swp_uffd_wp_any(pteval)) {\n\t\t\t\t\tresult = SCAN_PTE_UFFD_WP;\n\t\t\t\t\tgoto out_unmap;\n\t\t\t\t}\n\t\t\t\tcontinue;\n\t\t\t} else {\n\t\t\t\tresult = SCAN_EXCEED_SWAP_PTE;\n\t\t\t\tcount_vm_event(THP_SCAN_EXCEED_SWAP_PTE);\n\t\t\t\tgoto out_unmap;\n\t\t\t}\n\t\t}\n\t\tif (pte_none(pteval) || is_zero_pfn(pte_pfn(pteval))) {\n\t\t\t++none_or_zero;\n\t\t\tif (!userfaultfd_armed(vma) &&\n\t\t\t    (!cc->is_khugepaged ||\n\t\t\t     none_or_zero <= khugepaged_max_ptes_none)) {\n\t\t\t\tcontinue;\n\t\t\t} else {\n\t\t\t\tresult = SCAN_EXCEED_NONE_PTE;\n\t\t\t\tcount_vm_event(THP_SCAN_EXCEED_NONE_PTE);\n\t\t\t\tgoto out_unmap;\n\t\t\t}\n\t\t}\n\t\tif (pte_uffd_wp(pteval)) {\n\t\t\t \n\t\t\tresult = SCAN_PTE_UFFD_WP;\n\t\t\tgoto out_unmap;\n\t\t}\n\t\tif (pte_write(pteval))\n\t\t\twritable = true;\n\n\t\tpage = vm_normal_page(vma, _address, pteval);\n\t\tif (unlikely(!page) || unlikely(is_zone_device_page(page))) {\n\t\t\tresult = SCAN_PAGE_NULL;\n\t\t\tgoto out_unmap;\n\t\t}\n\n\t\tif (page_mapcount(page) > 1) {\n\t\t\t++shared;\n\t\t\tif (cc->is_khugepaged &&\n\t\t\t    shared > khugepaged_max_ptes_shared) {\n\t\t\t\tresult = SCAN_EXCEED_SHARED_PTE;\n\t\t\t\tcount_vm_event(THP_SCAN_EXCEED_SHARED_PTE);\n\t\t\t\tgoto out_unmap;\n\t\t\t}\n\t\t}\n\n\t\tpage = compound_head(page);\n\n\t\t \n\t\tnode = page_to_nid(page);\n\t\tif (hpage_collapse_scan_abort(node, cc)) {\n\t\t\tresult = SCAN_SCAN_ABORT;\n\t\t\tgoto out_unmap;\n\t\t}\n\t\tcc->node_load[node]++;\n\t\tif (!PageLRU(page)) {\n\t\t\tresult = SCAN_PAGE_LRU;\n\t\t\tgoto out_unmap;\n\t\t}\n\t\tif (PageLocked(page)) {\n\t\t\tresult = SCAN_PAGE_LOCK;\n\t\t\tgoto out_unmap;\n\t\t}\n\t\tif (!PageAnon(page)) {\n\t\t\tresult = SCAN_PAGE_ANON;\n\t\t\tgoto out_unmap;\n\t\t}\n\n\t\t \n\t\tif (!is_refcount_suitable(page)) {\n\t\t\tresult = SCAN_PAGE_COUNT;\n\t\t\tgoto out_unmap;\n\t\t}\n\n\t\t \n\t\tif (cc->is_khugepaged &&\n\t\t    (pte_young(pteval) || page_is_young(page) ||\n\t\t     PageReferenced(page) || mmu_notifier_test_young(vma->vm_mm,\n\t\t\t\t\t\t\t\t     address)))\n\t\t\treferenced++;\n\t}\n\tif (!writable) {\n\t\tresult = SCAN_PAGE_RO;\n\t} else if (cc->is_khugepaged &&\n\t\t   (!referenced ||\n\t\t    (unmapped && referenced < HPAGE_PMD_NR / 2))) {\n\t\tresult = SCAN_LACK_REFERENCED_PAGE;\n\t} else {\n\t\tresult = SCAN_SUCCEED;\n\t}\nout_unmap:\n\tpte_unmap_unlock(pte, ptl);\n\tif (result == SCAN_SUCCEED) {\n\t\tresult = collapse_huge_page(mm, address, referenced,\n\t\t\t\t\t    unmapped, cc);\n\t\t \n\t\t*mmap_locked = false;\n\t}\nout:\n\ttrace_mm_khugepaged_scan_pmd(mm, page, writable, referenced,\n\t\t\t\t     none_or_zero, result, unmapped);\n\treturn result;\n}\n\nstatic void collect_mm_slot(struct khugepaged_mm_slot *mm_slot)\n{\n\tstruct mm_slot *slot = &mm_slot->slot;\n\tstruct mm_struct *mm = slot->mm;\n\n\tlockdep_assert_held(&khugepaged_mm_lock);\n\n\tif (hpage_collapse_test_exit(mm)) {\n\t\t \n\t\thash_del(&slot->hash);\n\t\tlist_del(&slot->mm_node);\n\n\t\t \n\n\t\t \n\t\tmm_slot_free(mm_slot_cache, mm_slot);\n\t\tmmdrop(mm);\n\t}\n}\n\n#ifdef CONFIG_SHMEM\n \nstatic int set_huge_pmd(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tpmd_t *pmdp, struct page *hpage)\n{\n\tstruct vm_fault vmf = {\n\t\t.vma = vma,\n\t\t.address = addr,\n\t\t.flags = 0,\n\t\t.pmd = pmdp,\n\t};\n\n\tVM_BUG_ON(!PageTransHuge(hpage));\n\tmmap_assert_locked(vma->vm_mm);\n\n\tif (do_set_pmd(&vmf, hpage))\n\t\treturn SCAN_FAIL;\n\n\tget_page(hpage);\n\treturn SCAN_SUCCEED;\n}\n\n \nint collapse_pte_mapped_thp(struct mm_struct *mm, unsigned long addr,\n\t\t\t    bool install_pmd)\n{\n\tstruct mmu_notifier_range range;\n\tbool notified = false;\n\tunsigned long haddr = addr & HPAGE_PMD_MASK;\n\tstruct vm_area_struct *vma = vma_lookup(mm, haddr);\n\tstruct page *hpage;\n\tpte_t *start_pte, *pte;\n\tpmd_t *pmd, pgt_pmd;\n\tspinlock_t *pml = NULL, *ptl;\n\tint nr_ptes = 0, result = SCAN_FAIL;\n\tint i;\n\n\tmmap_assert_locked(mm);\n\n\t \n\tif (!vma || !vma->vm_file ||\n\t    !range_in_vma(vma, haddr, haddr + HPAGE_PMD_SIZE))\n\t\treturn SCAN_VMA_CHECK;\n\n\t \n\tresult = find_pmd_or_thp_or_none(mm, haddr, &pmd);\n\tif (result == SCAN_PMD_MAPPED)\n\t\treturn result;\n\n\t \n\tif (!hugepage_vma_check(vma, vma->vm_flags, false, false, false))\n\t\treturn SCAN_VMA_CHECK;\n\n\t \n\tif (userfaultfd_wp(vma))\n\t\treturn SCAN_PTE_UFFD_WP;\n\n\thpage = find_lock_page(vma->vm_file->f_mapping,\n\t\t\t       linear_page_index(vma, haddr));\n\tif (!hpage)\n\t\treturn SCAN_PAGE_NULL;\n\n\tif (!PageHead(hpage)) {\n\t\tresult = SCAN_FAIL;\n\t\tgoto drop_hpage;\n\t}\n\n\tif (compound_order(hpage) != HPAGE_PMD_ORDER) {\n\t\tresult = SCAN_PAGE_COMPOUND;\n\t\tgoto drop_hpage;\n\t}\n\n\tresult = find_pmd_or_thp_or_none(mm, haddr, &pmd);\n\tswitch (result) {\n\tcase SCAN_SUCCEED:\n\t\tbreak;\n\tcase SCAN_PMD_NONE:\n\t\t \n\t\tgoto maybe_install_pmd;\n\tdefault:\n\t\tgoto drop_hpage;\n\t}\n\n\tresult = SCAN_FAIL;\n\tstart_pte = pte_offset_map_lock(mm, pmd, haddr, &ptl);\n\tif (!start_pte)\t\t \n\t\tgoto drop_hpage;\n\n\t \n\tfor (i = 0, addr = haddr, pte = start_pte;\n\t     i < HPAGE_PMD_NR; i++, addr += PAGE_SIZE, pte++) {\n\t\tstruct page *page;\n\t\tpte_t ptent = ptep_get(pte);\n\n\t\t \n\t\tif (pte_none(ptent))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (!pte_present(ptent)) {\n\t\t\tresult = SCAN_PTE_NON_PRESENT;\n\t\t\tgoto abort;\n\t\t}\n\n\t\tpage = vm_normal_page(vma, addr, ptent);\n\t\tif (WARN_ON_ONCE(page && is_zone_device_page(page)))\n\t\t\tpage = NULL;\n\t\t \n\t\tif (hpage + i != page)\n\t\t\tgoto abort;\n\t}\n\n\tpte_unmap_unlock(start_pte, ptl);\n\tmmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, mm,\n\t\t\t\thaddr, haddr + HPAGE_PMD_SIZE);\n\tmmu_notifier_invalidate_range_start(&range);\n\tnotified = true;\n\n\t \n\tif (userfaultfd_armed(vma) && !(vma->vm_flags & VM_SHARED))\n\t\tpml = pmd_lock(mm, pmd);\n\n\tstart_pte = pte_offset_map_nolock(mm, pmd, haddr, &ptl);\n\tif (!start_pte)\t\t \n\t\tgoto abort;\n\tif (!pml)\n\t\tspin_lock(ptl);\n\telse if (ptl != pml)\n\t\tspin_lock_nested(ptl, SINGLE_DEPTH_NESTING);\n\n\t \n\tfor (i = 0, addr = haddr, pte = start_pte;\n\t     i < HPAGE_PMD_NR; i++, addr += PAGE_SIZE, pte++) {\n\t\tstruct page *page;\n\t\tpte_t ptent = ptep_get(pte);\n\n\t\tif (pte_none(ptent))\n\t\t\tcontinue;\n\t\t \n\t\tif (!pte_present(ptent)) {\n\t\t\tresult = SCAN_PTE_NON_PRESENT;\n\t\t\tgoto abort;\n\t\t}\n\t\tpage = vm_normal_page(vma, addr, ptent);\n\t\tif (hpage + i != page)\n\t\t\tgoto abort;\n\n\t\t \n\t\tptep_clear(mm, addr, pte);\n\t\tpage_remove_rmap(page, vma, false);\n\t\tnr_ptes++;\n\t}\n\n\tpte_unmap(start_pte);\n\tif (!pml)\n\t\tspin_unlock(ptl);\n\n\t \n\tif (nr_ptes) {\n\t\tpage_ref_sub(hpage, nr_ptes);\n\t\tadd_mm_counter(mm, mm_counter_file(hpage), -nr_ptes);\n\t}\n\n\t \n\tif (!pml) {\n\t\tpml = pmd_lock(mm, pmd);\n\t\tif (ptl != pml)\n\t\t\tspin_lock_nested(ptl, SINGLE_DEPTH_NESTING);\n\t}\n\tpgt_pmd = pmdp_collapse_flush(vma, haddr, pmd);\n\tpmdp_get_lockless_sync();\n\tif (ptl != pml)\n\t\tspin_unlock(ptl);\n\tspin_unlock(pml);\n\n\tmmu_notifier_invalidate_range_end(&range);\n\n\tmm_dec_nr_ptes(mm);\n\tpage_table_check_pte_clear_range(mm, haddr, pgt_pmd);\n\tpte_free_defer(mm, pmd_pgtable(pgt_pmd));\n\nmaybe_install_pmd:\n\t \n\tresult = install_pmd\n\t\t\t? set_huge_pmd(vma, haddr, pmd, hpage)\n\t\t\t: SCAN_SUCCEED;\n\tgoto drop_hpage;\nabort:\n\tif (nr_ptes) {\n\t\tflush_tlb_mm(mm);\n\t\tpage_ref_sub(hpage, nr_ptes);\n\t\tadd_mm_counter(mm, mm_counter_file(hpage), -nr_ptes);\n\t}\n\tif (start_pte)\n\t\tpte_unmap_unlock(start_pte, ptl);\n\tif (pml && pml != ptl)\n\t\tspin_unlock(pml);\n\tif (notified)\n\t\tmmu_notifier_invalidate_range_end(&range);\ndrop_hpage:\n\tunlock_page(hpage);\n\tput_page(hpage);\n\treturn result;\n}\n\nstatic void retract_page_tables(struct address_space *mapping, pgoff_t pgoff)\n{\n\tstruct vm_area_struct *vma;\n\n\ti_mmap_lock_read(mapping);\n\tvma_interval_tree_foreach(vma, &mapping->i_mmap, pgoff, pgoff) {\n\t\tstruct mmu_notifier_range range;\n\t\tstruct mm_struct *mm;\n\t\tunsigned long addr;\n\t\tpmd_t *pmd, pgt_pmd;\n\t\tspinlock_t *pml;\n\t\tspinlock_t *ptl;\n\t\tbool skipped_uffd = false;\n\n\t\t \n\t\tif (READ_ONCE(vma->anon_vma))\n\t\t\tcontinue;\n\n\t\taddr = vma->vm_start + ((pgoff - vma->vm_pgoff) << PAGE_SHIFT);\n\t\tif (addr & ~HPAGE_PMD_MASK ||\n\t\t    vma->vm_end < addr + HPAGE_PMD_SIZE)\n\t\t\tcontinue;\n\n\t\tmm = vma->vm_mm;\n\t\tif (find_pmd_or_thp_or_none(mm, addr, &pmd) != SCAN_SUCCEED)\n\t\t\tcontinue;\n\n\t\tif (hpage_collapse_test_exit(mm))\n\t\t\tcontinue;\n\t\t \n\t\tif (userfaultfd_wp(vma))\n\t\t\tcontinue;\n\n\t\t \n\t\tmmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, mm,\n\t\t\t\t\taddr, addr + HPAGE_PMD_SIZE);\n\t\tmmu_notifier_invalidate_range_start(&range);\n\n\t\tpml = pmd_lock(mm, pmd);\n\t\tptl = pte_lockptr(mm, pmd);\n\t\tif (ptl != pml)\n\t\t\tspin_lock_nested(ptl, SINGLE_DEPTH_NESTING);\n\n\t\t \n\t\tif (unlikely(vma->anon_vma || userfaultfd_wp(vma))) {\n\t\t\tskipped_uffd = true;\n\t\t} else {\n\t\t\tpgt_pmd = pmdp_collapse_flush(vma, addr, pmd);\n\t\t\tpmdp_get_lockless_sync();\n\t\t}\n\n\t\tif (ptl != pml)\n\t\t\tspin_unlock(ptl);\n\t\tspin_unlock(pml);\n\n\t\tmmu_notifier_invalidate_range_end(&range);\n\n\t\tif (!skipped_uffd) {\n\t\t\tmm_dec_nr_ptes(mm);\n\t\t\tpage_table_check_pte_clear_range(mm, addr, pgt_pmd);\n\t\t\tpte_free_defer(mm, pmd_pgtable(pgt_pmd));\n\t\t}\n\t}\n\ti_mmap_unlock_read(mapping);\n}\n\n \nstatic int collapse_file(struct mm_struct *mm, unsigned long addr,\n\t\t\t struct file *file, pgoff_t start,\n\t\t\t struct collapse_control *cc)\n{\n\tstruct address_space *mapping = file->f_mapping;\n\tstruct page *hpage;\n\tstruct page *page;\n\tstruct page *tmp;\n\tstruct folio *folio;\n\tpgoff_t index = 0, end = start + HPAGE_PMD_NR;\n\tLIST_HEAD(pagelist);\n\tXA_STATE_ORDER(xas, &mapping->i_pages, start, HPAGE_PMD_ORDER);\n\tint nr_none = 0, result = SCAN_SUCCEED;\n\tbool is_shmem = shmem_file(file);\n\tint nr = 0;\n\n\tVM_BUG_ON(!IS_ENABLED(CONFIG_READ_ONLY_THP_FOR_FS) && !is_shmem);\n\tVM_BUG_ON(start & (HPAGE_PMD_NR - 1));\n\n\tresult = alloc_charge_hpage(&hpage, mm, cc);\n\tif (result != SCAN_SUCCEED)\n\t\tgoto out;\n\n\t__SetPageLocked(hpage);\n\tif (is_shmem)\n\t\t__SetPageSwapBacked(hpage);\n\thpage->index = start;\n\thpage->mapping = mapping;\n\n\t \n\tdo {\n\t\txas_lock_irq(&xas);\n\t\txas_create_range(&xas);\n\t\tif (!xas_error(&xas))\n\t\t\tbreak;\n\t\txas_unlock_irq(&xas);\n\t\tif (!xas_nomem(&xas, GFP_KERNEL)) {\n\t\t\tresult = SCAN_FAIL;\n\t\t\tgoto rollback;\n\t\t}\n\t} while (1);\n\n\tfor (index = start; index < end; index++) {\n\t\txas_set(&xas, index);\n\t\tpage = xas_load(&xas);\n\n\t\tVM_BUG_ON(index != xas.xa_index);\n\t\tif (is_shmem) {\n\t\t\tif (!page) {\n\t\t\t\t \n\t\t\t\tif (index == start) {\n\t\t\t\t\tif (!xas_next_entry(&xas, end - 1)) {\n\t\t\t\t\t\tresult = SCAN_TRUNCATED;\n\t\t\t\t\t\tgoto xa_locked;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tnr_none++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (xa_is_value(page) || !PageUptodate(page)) {\n\t\t\t\txas_unlock_irq(&xas);\n\t\t\t\t \n\t\t\t\tif (shmem_get_folio(mapping->host, index,\n\t\t\t\t\t\t&folio, SGP_NOALLOC)) {\n\t\t\t\t\tresult = SCAN_FAIL;\n\t\t\t\t\tgoto xa_unlocked;\n\t\t\t\t}\n\t\t\t\t \n\t\t\t\tlru_add_drain();\n\t\t\t\tpage = folio_file_page(folio, index);\n\t\t\t} else if (trylock_page(page)) {\n\t\t\t\tget_page(page);\n\t\t\t\txas_unlock_irq(&xas);\n\t\t\t} else {\n\t\t\t\tresult = SCAN_PAGE_LOCK;\n\t\t\t\tgoto xa_locked;\n\t\t\t}\n\t\t} else {\t \n\t\t\tif (!page || xa_is_value(page)) {\n\t\t\t\txas_unlock_irq(&xas);\n\t\t\t\tpage_cache_sync_readahead(mapping, &file->f_ra,\n\t\t\t\t\t\t\t  file, index,\n\t\t\t\t\t\t\t  end - index);\n\t\t\t\t \n\t\t\t\tlru_add_drain();\n\t\t\t\tpage = find_lock_page(mapping, index);\n\t\t\t\tif (unlikely(page == NULL)) {\n\t\t\t\t\tresult = SCAN_FAIL;\n\t\t\t\t\tgoto xa_unlocked;\n\t\t\t\t}\n\t\t\t} else if (PageDirty(page)) {\n\t\t\t\t \n\t\t\t\txas_unlock_irq(&xas);\n\t\t\t\tfilemap_flush(mapping);\n\t\t\t\tresult = SCAN_FAIL;\n\t\t\t\tgoto xa_unlocked;\n\t\t\t} else if (PageWriteback(page)) {\n\t\t\t\txas_unlock_irq(&xas);\n\t\t\t\tresult = SCAN_FAIL;\n\t\t\t\tgoto xa_unlocked;\n\t\t\t} else if (trylock_page(page)) {\n\t\t\t\tget_page(page);\n\t\t\t\txas_unlock_irq(&xas);\n\t\t\t} else {\n\t\t\t\tresult = SCAN_PAGE_LOCK;\n\t\t\t\tgoto xa_locked;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tVM_BUG_ON_PAGE(!PageLocked(page), page);\n\n\t\t \n\t\tif (unlikely(!PageUptodate(page))) {\n\t\t\tresult = SCAN_FAIL;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\t \n\t\tif (PageTransCompound(page)) {\n\t\t\tstruct page *head = compound_head(page);\n\n\t\t\tresult = compound_order(head) == HPAGE_PMD_ORDER &&\n\t\t\t\t\thead->index == start\n\t\t\t\t\t \n\t\t\t\t\t? SCAN_PTE_MAPPED_HUGEPAGE\n\t\t\t\t\t: SCAN_PAGE_COMPOUND;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tfolio = page_folio(page);\n\n\t\tif (folio_mapping(folio) != mapping) {\n\t\t\tresult = SCAN_TRUNCATED;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tif (!is_shmem && (folio_test_dirty(folio) ||\n\t\t\t\t  folio_test_writeback(folio))) {\n\t\t\t \n\t\t\tresult = SCAN_FAIL;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tif (!folio_isolate_lru(folio)) {\n\t\t\tresult = SCAN_DEL_PAGE_LRU;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tif (!filemap_release_folio(folio, GFP_KERNEL)) {\n\t\t\tresult = SCAN_PAGE_HAS_PRIVATE;\n\t\t\tfolio_putback_lru(folio);\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tif (folio_mapped(folio))\n\t\t\ttry_to_unmap(folio,\n\t\t\t\t\tTTU_IGNORE_MLOCK | TTU_BATCH_FLUSH);\n\n\t\txas_lock_irq(&xas);\n\n\t\tVM_BUG_ON_PAGE(page != xa_load(xas.xa, index), page);\n\n\t\t \n\t\tif (page_count(page) != 3) {\n\t\t\tresult = SCAN_PAGE_COUNT;\n\t\t\txas_unlock_irq(&xas);\n\t\t\tputback_lru_page(page);\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\t \n\t\tlist_add_tail(&page->lru, &pagelist);\n\t\tcontinue;\nout_unlock:\n\t\tunlock_page(page);\n\t\tput_page(page);\n\t\tgoto xa_unlocked;\n\t}\n\n\tif (!is_shmem) {\n\t\tfilemap_nr_thps_inc(mapping);\n\t\t \n\t\tsmp_mb();\n\t\tif (inode_is_open_for_write(mapping->host)) {\n\t\t\tresult = SCAN_FAIL;\n\t\t\tfilemap_nr_thps_dec(mapping);\n\t\t}\n\t}\n\nxa_locked:\n\txas_unlock_irq(&xas);\nxa_unlocked:\n\n\t \n\ttry_to_unmap_flush();\n\n\tif (result == SCAN_SUCCEED && nr_none &&\n\t    !shmem_charge(mapping->host, nr_none))\n\t\tresult = SCAN_FAIL;\n\tif (result != SCAN_SUCCEED) {\n\t\tnr_none = 0;\n\t\tgoto rollback;\n\t}\n\n\t \n\tindex = start;\n\tlist_for_each_entry(page, &pagelist, lru) {\n\t\twhile (index < page->index) {\n\t\t\tclear_highpage(hpage + (index % HPAGE_PMD_NR));\n\t\t\tindex++;\n\t\t}\n\t\tif (copy_mc_highpage(hpage + (page->index % HPAGE_PMD_NR), page) > 0) {\n\t\t\tresult = SCAN_COPY_MC;\n\t\t\tgoto rollback;\n\t\t}\n\t\tindex++;\n\t}\n\twhile (index < end) {\n\t\tclear_highpage(hpage + (index % HPAGE_PMD_NR));\n\t\tindex++;\n\t}\n\n\tif (nr_none) {\n\t\tstruct vm_area_struct *vma;\n\t\tint nr_none_check = 0;\n\n\t\ti_mmap_lock_read(mapping);\n\t\txas_lock_irq(&xas);\n\n\t\txas_set(&xas, start);\n\t\tfor (index = start; index < end; index++) {\n\t\t\tif (!xas_next(&xas)) {\n\t\t\t\txas_store(&xas, XA_RETRY_ENTRY);\n\t\t\t\tif (xas_error(&xas)) {\n\t\t\t\t\tresult = SCAN_STORE_FAILED;\n\t\t\t\t\tgoto immap_locked;\n\t\t\t\t}\n\t\t\t\tnr_none_check++;\n\t\t\t}\n\t\t}\n\n\t\tif (nr_none != nr_none_check) {\n\t\t\tresult = SCAN_PAGE_FILLED;\n\t\t\tgoto immap_locked;\n\t\t}\n\n\t\t \n\t\tvma_interval_tree_foreach(vma, &mapping->i_mmap, start, end) {\n\t\t\tif (userfaultfd_missing(vma)) {\n\t\t\t\tresult = SCAN_EXCEED_NONE_PTE;\n\t\t\t\tgoto immap_locked;\n\t\t\t}\n\t\t}\n\nimmap_locked:\n\t\ti_mmap_unlock_read(mapping);\n\t\tif (result != SCAN_SUCCEED) {\n\t\t\txas_set(&xas, start);\n\t\t\tfor (index = start; index < end; index++) {\n\t\t\t\tif (xas_next(&xas) == XA_RETRY_ENTRY)\n\t\t\t\t\txas_store(&xas, NULL);\n\t\t\t}\n\n\t\t\txas_unlock_irq(&xas);\n\t\t\tgoto rollback;\n\t\t}\n\t} else {\n\t\txas_lock_irq(&xas);\n\t}\n\n\tnr = thp_nr_pages(hpage);\n\tif (is_shmem)\n\t\t__mod_lruvec_page_state(hpage, NR_SHMEM_THPS, nr);\n\telse\n\t\t__mod_lruvec_page_state(hpage, NR_FILE_THPS, nr);\n\n\tif (nr_none) {\n\t\t__mod_lruvec_page_state(hpage, NR_FILE_PAGES, nr_none);\n\t\t \n\t\t__mod_lruvec_page_state(hpage, NR_SHMEM, nr_none);\n\t}\n\n\t \n\tfolio = page_folio(hpage);\n\tfolio_mark_uptodate(folio);\n\tfolio_ref_add(folio, HPAGE_PMD_NR - 1);\n\n\tif (is_shmem)\n\t\tfolio_mark_dirty(folio);\n\tfolio_add_lru(folio);\n\n\t \n\txas_set_order(&xas, start, HPAGE_PMD_ORDER);\n\txas_store(&xas, hpage);\n\tWARN_ON_ONCE(xas_error(&xas));\n\txas_unlock_irq(&xas);\n\n\t \n\tretract_page_tables(mapping, start);\n\tif (cc && !cc->is_khugepaged)\n\t\tresult = SCAN_PTE_MAPPED_HUGEPAGE;\n\tunlock_page(hpage);\n\n\t \n\tlist_for_each_entry_safe(page, tmp, &pagelist, lru) {\n\t\tlist_del(&page->lru);\n\t\tpage->mapping = NULL;\n\t\tClearPageActive(page);\n\t\tClearPageUnevictable(page);\n\t\tunlock_page(page);\n\t\tfolio_put_refs(page_folio(page), 3);\n\t}\n\n\tgoto out;\n\nrollback:\n\t \n\tif (nr_none) {\n\t\txas_lock_irq(&xas);\n\t\tmapping->nrpages -= nr_none;\n\t\txas_unlock_irq(&xas);\n\t\tshmem_uncharge(mapping->host, nr_none);\n\t}\n\n\tlist_for_each_entry_safe(page, tmp, &pagelist, lru) {\n\t\tlist_del(&page->lru);\n\t\tunlock_page(page);\n\t\tputback_lru_page(page);\n\t\tput_page(page);\n\t}\n\t \n\tif (!is_shmem && result == SCAN_COPY_MC) {\n\t\tfilemap_nr_thps_dec(mapping);\n\t\t \n\t\tsmp_mb();\n\t}\n\n\thpage->mapping = NULL;\n\n\tunlock_page(hpage);\n\tput_page(hpage);\nout:\n\tVM_BUG_ON(!list_empty(&pagelist));\n\ttrace_mm_khugepaged_collapse_file(mm, hpage, index, is_shmem, addr, file, nr, result);\n\treturn result;\n}\n\nstatic int hpage_collapse_scan_file(struct mm_struct *mm, unsigned long addr,\n\t\t\t\t    struct file *file, pgoff_t start,\n\t\t\t\t    struct collapse_control *cc)\n{\n\tstruct page *page = NULL;\n\tstruct address_space *mapping = file->f_mapping;\n\tXA_STATE(xas, &mapping->i_pages, start);\n\tint present, swap;\n\tint node = NUMA_NO_NODE;\n\tint result = SCAN_SUCCEED;\n\n\tpresent = 0;\n\tswap = 0;\n\tmemset(cc->node_load, 0, sizeof(cc->node_load));\n\tnodes_clear(cc->alloc_nmask);\n\trcu_read_lock();\n\txas_for_each(&xas, page, start + HPAGE_PMD_NR - 1) {\n\t\tif (xas_retry(&xas, page))\n\t\t\tcontinue;\n\n\t\tif (xa_is_value(page)) {\n\t\t\t++swap;\n\t\t\tif (cc->is_khugepaged &&\n\t\t\t    swap > khugepaged_max_ptes_swap) {\n\t\t\t\tresult = SCAN_EXCEED_SWAP_PTE;\n\t\t\t\tcount_vm_event(THP_SCAN_EXCEED_SWAP_PTE);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (PageTransCompound(page)) {\n\t\t\tstruct page *head = compound_head(page);\n\n\t\t\tresult = compound_order(head) == HPAGE_PMD_ORDER &&\n\t\t\t\t\thead->index == start\n\t\t\t\t\t \n\t\t\t\t\t? SCAN_PTE_MAPPED_HUGEPAGE\n\t\t\t\t\t: SCAN_PAGE_COMPOUND;\n\t\t\t \n\t\t\tbreak;\n\t\t}\n\n\t\tnode = page_to_nid(page);\n\t\tif (hpage_collapse_scan_abort(node, cc)) {\n\t\t\tresult = SCAN_SCAN_ABORT;\n\t\t\tbreak;\n\t\t}\n\t\tcc->node_load[node]++;\n\n\t\tif (!PageLRU(page)) {\n\t\t\tresult = SCAN_PAGE_LRU;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (page_count(page) !=\n\t\t    1 + page_mapcount(page) + page_has_private(page)) {\n\t\t\tresult = SCAN_PAGE_COUNT;\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\n\t\tpresent++;\n\n\t\tif (need_resched()) {\n\t\t\txas_pause(&xas);\n\t\t\tcond_resched_rcu();\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\tif (result == SCAN_SUCCEED) {\n\t\tif (cc->is_khugepaged &&\n\t\t    present < HPAGE_PMD_NR - khugepaged_max_ptes_none) {\n\t\t\tresult = SCAN_EXCEED_NONE_PTE;\n\t\t\tcount_vm_event(THP_SCAN_EXCEED_NONE_PTE);\n\t\t} else {\n\t\t\tresult = collapse_file(mm, addr, file, start, cc);\n\t\t}\n\t}\n\n\ttrace_mm_khugepaged_scan_file(mm, page, file, present, swap, result);\n\treturn result;\n}\n#else\nstatic int hpage_collapse_scan_file(struct mm_struct *mm, unsigned long addr,\n\t\t\t\t    struct file *file, pgoff_t start,\n\t\t\t\t    struct collapse_control *cc)\n{\n\tBUILD_BUG();\n}\n#endif\n\nstatic unsigned int khugepaged_scan_mm_slot(unsigned int pages, int *result,\n\t\t\t\t\t    struct collapse_control *cc)\n\t__releases(&khugepaged_mm_lock)\n\t__acquires(&khugepaged_mm_lock)\n{\n\tstruct vma_iterator vmi;\n\tstruct khugepaged_mm_slot *mm_slot;\n\tstruct mm_slot *slot;\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct *vma;\n\tint progress = 0;\n\n\tVM_BUG_ON(!pages);\n\tlockdep_assert_held(&khugepaged_mm_lock);\n\t*result = SCAN_FAIL;\n\n\tif (khugepaged_scan.mm_slot) {\n\t\tmm_slot = khugepaged_scan.mm_slot;\n\t\tslot = &mm_slot->slot;\n\t} else {\n\t\tslot = list_entry(khugepaged_scan.mm_head.next,\n\t\t\t\t     struct mm_slot, mm_node);\n\t\tmm_slot = mm_slot_entry(slot, struct khugepaged_mm_slot, slot);\n\t\tkhugepaged_scan.address = 0;\n\t\tkhugepaged_scan.mm_slot = mm_slot;\n\t}\n\tspin_unlock(&khugepaged_mm_lock);\n\n\tmm = slot->mm;\n\t \n\tvma = NULL;\n\tif (unlikely(!mmap_read_trylock(mm)))\n\t\tgoto breakouterloop_mmap_lock;\n\n\tprogress++;\n\tif (unlikely(hpage_collapse_test_exit(mm)))\n\t\tgoto breakouterloop;\n\n\tvma_iter_init(&vmi, mm, khugepaged_scan.address);\n\tfor_each_vma(vmi, vma) {\n\t\tunsigned long hstart, hend;\n\n\t\tcond_resched();\n\t\tif (unlikely(hpage_collapse_test_exit(mm))) {\n\t\t\tprogress++;\n\t\t\tbreak;\n\t\t}\n\t\tif (!hugepage_vma_check(vma, vma->vm_flags, false, false, true)) {\nskip:\n\t\t\tprogress++;\n\t\t\tcontinue;\n\t\t}\n\t\thstart = round_up(vma->vm_start, HPAGE_PMD_SIZE);\n\t\thend = round_down(vma->vm_end, HPAGE_PMD_SIZE);\n\t\tif (khugepaged_scan.address > hend)\n\t\t\tgoto skip;\n\t\tif (khugepaged_scan.address < hstart)\n\t\t\tkhugepaged_scan.address = hstart;\n\t\tVM_BUG_ON(khugepaged_scan.address & ~HPAGE_PMD_MASK);\n\n\t\twhile (khugepaged_scan.address < hend) {\n\t\t\tbool mmap_locked = true;\n\n\t\t\tcond_resched();\n\t\t\tif (unlikely(hpage_collapse_test_exit(mm)))\n\t\t\t\tgoto breakouterloop;\n\n\t\t\tVM_BUG_ON(khugepaged_scan.address < hstart ||\n\t\t\t\t  khugepaged_scan.address + HPAGE_PMD_SIZE >\n\t\t\t\t  hend);\n\t\t\tif (IS_ENABLED(CONFIG_SHMEM) && vma->vm_file) {\n\t\t\t\tstruct file *file = get_file(vma->vm_file);\n\t\t\t\tpgoff_t pgoff = linear_page_index(vma,\n\t\t\t\t\t\tkhugepaged_scan.address);\n\n\t\t\t\tmmap_read_unlock(mm);\n\t\t\t\tmmap_locked = false;\n\t\t\t\t*result = hpage_collapse_scan_file(mm,\n\t\t\t\t\tkhugepaged_scan.address, file, pgoff, cc);\n\t\t\t\tfput(file);\n\t\t\t\tif (*result == SCAN_PTE_MAPPED_HUGEPAGE) {\n\t\t\t\t\tmmap_read_lock(mm);\n\t\t\t\t\tif (hpage_collapse_test_exit(mm))\n\t\t\t\t\t\tgoto breakouterloop;\n\t\t\t\t\t*result = collapse_pte_mapped_thp(mm,\n\t\t\t\t\t\tkhugepaged_scan.address, false);\n\t\t\t\t\tif (*result == SCAN_PMD_MAPPED)\n\t\t\t\t\t\t*result = SCAN_SUCCEED;\n\t\t\t\t\tmmap_read_unlock(mm);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t*result = hpage_collapse_scan_pmd(mm, vma,\n\t\t\t\t\tkhugepaged_scan.address, &mmap_locked, cc);\n\t\t\t}\n\n\t\t\tif (*result == SCAN_SUCCEED)\n\t\t\t\t++khugepaged_pages_collapsed;\n\n\t\t\t \n\t\t\tkhugepaged_scan.address += HPAGE_PMD_SIZE;\n\t\t\tprogress += HPAGE_PMD_NR;\n\t\t\tif (!mmap_locked)\n\t\t\t\t \n\t\t\t\tgoto breakouterloop_mmap_lock;\n\t\t\tif (progress >= pages)\n\t\t\t\tgoto breakouterloop;\n\t\t}\n\t}\nbreakouterloop:\n\tmmap_read_unlock(mm);  \nbreakouterloop_mmap_lock:\n\n\tspin_lock(&khugepaged_mm_lock);\n\tVM_BUG_ON(khugepaged_scan.mm_slot != mm_slot);\n\t \n\tif (hpage_collapse_test_exit(mm) || !vma) {\n\t\t \n\t\tif (slot->mm_node.next != &khugepaged_scan.mm_head) {\n\t\t\tslot = list_entry(slot->mm_node.next,\n\t\t\t\t\t  struct mm_slot, mm_node);\n\t\t\tkhugepaged_scan.mm_slot =\n\t\t\t\tmm_slot_entry(slot, struct khugepaged_mm_slot, slot);\n\t\t\tkhugepaged_scan.address = 0;\n\t\t} else {\n\t\t\tkhugepaged_scan.mm_slot = NULL;\n\t\t\tkhugepaged_full_scans++;\n\t\t}\n\n\t\tcollect_mm_slot(mm_slot);\n\t}\n\n\treturn progress;\n}\n\nstatic int khugepaged_has_work(void)\n{\n\treturn !list_empty(&khugepaged_scan.mm_head) &&\n\t\thugepage_flags_enabled();\n}\n\nstatic int khugepaged_wait_event(void)\n{\n\treturn !list_empty(&khugepaged_scan.mm_head) ||\n\t\tkthread_should_stop();\n}\n\nstatic void khugepaged_do_scan(struct collapse_control *cc)\n{\n\tunsigned int progress = 0, pass_through_head = 0;\n\tunsigned int pages = READ_ONCE(khugepaged_pages_to_scan);\n\tbool wait = true;\n\tint result = SCAN_SUCCEED;\n\n\tlru_add_drain_all();\n\n\twhile (true) {\n\t\tcond_resched();\n\n\t\tif (unlikely(kthread_should_stop() || try_to_freeze()))\n\t\t\tbreak;\n\n\t\tspin_lock(&khugepaged_mm_lock);\n\t\tif (!khugepaged_scan.mm_slot)\n\t\t\tpass_through_head++;\n\t\tif (khugepaged_has_work() &&\n\t\t    pass_through_head < 2)\n\t\t\tprogress += khugepaged_scan_mm_slot(pages - progress,\n\t\t\t\t\t\t\t    &result, cc);\n\t\telse\n\t\t\tprogress = pages;\n\t\tspin_unlock(&khugepaged_mm_lock);\n\n\t\tif (progress >= pages)\n\t\t\tbreak;\n\n\t\tif (result == SCAN_ALLOC_HUGE_PAGE_FAIL) {\n\t\t\t \n\t\t\tif (!wait)\n\t\t\t\tbreak;\n\t\t\twait = false;\n\t\t\tkhugepaged_alloc_sleep();\n\t\t}\n\t}\n}\n\nstatic bool khugepaged_should_wakeup(void)\n{\n\treturn kthread_should_stop() ||\n\t       time_after_eq(jiffies, khugepaged_sleep_expire);\n}\n\nstatic void khugepaged_wait_work(void)\n{\n\tif (khugepaged_has_work()) {\n\t\tconst unsigned long scan_sleep_jiffies =\n\t\t\tmsecs_to_jiffies(khugepaged_scan_sleep_millisecs);\n\n\t\tif (!scan_sleep_jiffies)\n\t\t\treturn;\n\n\t\tkhugepaged_sleep_expire = jiffies + scan_sleep_jiffies;\n\t\twait_event_freezable_timeout(khugepaged_wait,\n\t\t\t\t\t     khugepaged_should_wakeup(),\n\t\t\t\t\t     scan_sleep_jiffies);\n\t\treturn;\n\t}\n\n\tif (hugepage_flags_enabled())\n\t\twait_event_freezable(khugepaged_wait, khugepaged_wait_event());\n}\n\nstatic int khugepaged(void *none)\n{\n\tstruct khugepaged_mm_slot *mm_slot;\n\n\tset_freezable();\n\tset_user_nice(current, MAX_NICE);\n\n\twhile (!kthread_should_stop()) {\n\t\tkhugepaged_do_scan(&khugepaged_collapse_control);\n\t\tkhugepaged_wait_work();\n\t}\n\n\tspin_lock(&khugepaged_mm_lock);\n\tmm_slot = khugepaged_scan.mm_slot;\n\tkhugepaged_scan.mm_slot = NULL;\n\tif (mm_slot)\n\t\tcollect_mm_slot(mm_slot);\n\tspin_unlock(&khugepaged_mm_lock);\n\treturn 0;\n}\n\nstatic void set_recommended_min_free_kbytes(void)\n{\n\tstruct zone *zone;\n\tint nr_zones = 0;\n\tunsigned long recommended_min;\n\n\tif (!hugepage_flags_enabled()) {\n\t\tcalculate_min_free_kbytes();\n\t\tgoto update_wmarks;\n\t}\n\n\tfor_each_populated_zone(zone) {\n\t\t \n\t\tif (zone_idx(zone) > gfp_zone(GFP_USER))\n\t\t\tcontinue;\n\n\t\tnr_zones++;\n\t}\n\n\t \n\trecommended_min = pageblock_nr_pages * nr_zones * 2;\n\n\t \n\trecommended_min += pageblock_nr_pages * nr_zones *\n\t\t\t   MIGRATE_PCPTYPES * MIGRATE_PCPTYPES;\n\n\t \n\trecommended_min = min(recommended_min,\n\t\t\t      (unsigned long) nr_free_buffer_pages() / 20);\n\trecommended_min <<= (PAGE_SHIFT-10);\n\n\tif (recommended_min > min_free_kbytes) {\n\t\tif (user_min_free_kbytes >= 0)\n\t\t\tpr_info(\"raising min_free_kbytes from %d to %lu to help transparent hugepage allocations\\n\",\n\t\t\t\tmin_free_kbytes, recommended_min);\n\n\t\tmin_free_kbytes = recommended_min;\n\t}\n\nupdate_wmarks:\n\tsetup_per_zone_wmarks();\n}\n\nint start_stop_khugepaged(void)\n{\n\tint err = 0;\n\n\tmutex_lock(&khugepaged_mutex);\n\tif (hugepage_flags_enabled()) {\n\t\tif (!khugepaged_thread)\n\t\t\tkhugepaged_thread = kthread_run(khugepaged, NULL,\n\t\t\t\t\t\t\t\"khugepaged\");\n\t\tif (IS_ERR(khugepaged_thread)) {\n\t\t\tpr_err(\"khugepaged: kthread_run(khugepaged) failed\\n\");\n\t\t\terr = PTR_ERR(khugepaged_thread);\n\t\t\tkhugepaged_thread = NULL;\n\t\t\tgoto fail;\n\t\t}\n\n\t\tif (!list_empty(&khugepaged_scan.mm_head))\n\t\t\twake_up_interruptible(&khugepaged_wait);\n\t} else if (khugepaged_thread) {\n\t\tkthread_stop(khugepaged_thread);\n\t\tkhugepaged_thread = NULL;\n\t}\n\tset_recommended_min_free_kbytes();\nfail:\n\tmutex_unlock(&khugepaged_mutex);\n\treturn err;\n}\n\nvoid khugepaged_min_free_kbytes_update(void)\n{\n\tmutex_lock(&khugepaged_mutex);\n\tif (hugepage_flags_enabled() && khugepaged_thread)\n\t\tset_recommended_min_free_kbytes();\n\tmutex_unlock(&khugepaged_mutex);\n}\n\nbool current_is_khugepaged(void)\n{\n\treturn kthread_func(current) == khugepaged;\n}\n\nstatic int madvise_collapse_errno(enum scan_result r)\n{\n\t \n\tswitch (r) {\n\tcase SCAN_ALLOC_HUGE_PAGE_FAIL:\n\t\treturn -ENOMEM;\n\tcase SCAN_CGROUP_CHARGE_FAIL:\n\tcase SCAN_EXCEED_NONE_PTE:\n\t\treturn -EBUSY;\n\t \n\tcase SCAN_PAGE_COUNT:\n\tcase SCAN_PAGE_LOCK:\n\tcase SCAN_PAGE_LRU:\n\tcase SCAN_DEL_PAGE_LRU:\n\tcase SCAN_PAGE_FILLED:\n\t\treturn -EAGAIN;\n\t \n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nint madvise_collapse(struct vm_area_struct *vma, struct vm_area_struct **prev,\n\t\t     unsigned long start, unsigned long end)\n{\n\tstruct collapse_control *cc;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long hstart, hend, addr;\n\tint thps = 0, last_fail = SCAN_FAIL;\n\tbool mmap_locked = true;\n\n\tBUG_ON(vma->vm_start > start);\n\tBUG_ON(vma->vm_end < end);\n\n\t*prev = vma;\n\n\tif (!hugepage_vma_check(vma, vma->vm_flags, false, false, false))\n\t\treturn -EINVAL;\n\n\tcc = kmalloc(sizeof(*cc), GFP_KERNEL);\n\tif (!cc)\n\t\treturn -ENOMEM;\n\tcc->is_khugepaged = false;\n\n\tmmgrab(mm);\n\tlru_add_drain_all();\n\n\thstart = (start + ~HPAGE_PMD_MASK) & HPAGE_PMD_MASK;\n\thend = end & HPAGE_PMD_MASK;\n\n\tfor (addr = hstart; addr < hend; addr += HPAGE_PMD_SIZE) {\n\t\tint result = SCAN_FAIL;\n\n\t\tif (!mmap_locked) {\n\t\t\tcond_resched();\n\t\t\tmmap_read_lock(mm);\n\t\t\tmmap_locked = true;\n\t\t\tresult = hugepage_vma_revalidate(mm, addr, false, &vma,\n\t\t\t\t\t\t\t cc);\n\t\t\tif (result  != SCAN_SUCCEED) {\n\t\t\t\tlast_fail = result;\n\t\t\t\tgoto out_nolock;\n\t\t\t}\n\n\t\t\thend = min(hend, vma->vm_end & HPAGE_PMD_MASK);\n\t\t}\n\t\tmmap_assert_locked(mm);\n\t\tmemset(cc->node_load, 0, sizeof(cc->node_load));\n\t\tnodes_clear(cc->alloc_nmask);\n\t\tif (IS_ENABLED(CONFIG_SHMEM) && vma->vm_file) {\n\t\t\tstruct file *file = get_file(vma->vm_file);\n\t\t\tpgoff_t pgoff = linear_page_index(vma, addr);\n\n\t\t\tmmap_read_unlock(mm);\n\t\t\tmmap_locked = false;\n\t\t\tresult = hpage_collapse_scan_file(mm, addr, file, pgoff,\n\t\t\t\t\t\t\t  cc);\n\t\t\tfput(file);\n\t\t} else {\n\t\t\tresult = hpage_collapse_scan_pmd(mm, vma, addr,\n\t\t\t\t\t\t\t &mmap_locked, cc);\n\t\t}\n\t\tif (!mmap_locked)\n\t\t\t*prev = NULL;   \n\nhandle_result:\n\t\tswitch (result) {\n\t\tcase SCAN_SUCCEED:\n\t\tcase SCAN_PMD_MAPPED:\n\t\t\t++thps;\n\t\t\tbreak;\n\t\tcase SCAN_PTE_MAPPED_HUGEPAGE:\n\t\t\tBUG_ON(mmap_locked);\n\t\t\tBUG_ON(*prev);\n\t\t\tmmap_read_lock(mm);\n\t\t\tresult = collapse_pte_mapped_thp(mm, addr, true);\n\t\t\tmmap_read_unlock(mm);\n\t\t\tgoto handle_result;\n\t\t \n\t\tcase SCAN_PMD_NULL:\n\t\tcase SCAN_PTE_NON_PRESENT:\n\t\tcase SCAN_PTE_UFFD_WP:\n\t\tcase SCAN_PAGE_RO:\n\t\tcase SCAN_LACK_REFERENCED_PAGE:\n\t\tcase SCAN_PAGE_NULL:\n\t\tcase SCAN_PAGE_COUNT:\n\t\tcase SCAN_PAGE_LOCK:\n\t\tcase SCAN_PAGE_COMPOUND:\n\t\tcase SCAN_PAGE_LRU:\n\t\tcase SCAN_DEL_PAGE_LRU:\n\t\t\tlast_fail = result;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tlast_fail = result;\n\t\t\t \n\t\t\tgoto out_maybelock;\n\t\t}\n\t}\n\nout_maybelock:\n\t \n\tif (!mmap_locked)\n\t\tmmap_read_lock(mm);\nout_nolock:\n\tmmap_assert_locked(mm);\n\tmmdrop(mm);\n\tkfree(cc);\n\n\treturn thps == ((hend - hstart) >> HPAGE_PMD_SHIFT) ? 0\n\t\t\t: madvise_collapse_errno(last_fail);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}