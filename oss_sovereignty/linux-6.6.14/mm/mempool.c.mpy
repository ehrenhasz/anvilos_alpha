{
  "module_name": "mempool.c",
  "hash_id": "892883e597008acbdb0ef78b9e9d5bb2a99f12af85d3d87f97d4c589c1f02ef1",
  "original_prompt": "Ingested from linux-6.6.14/mm/mempool.c",
  "human_readable_source": "\n \n\n#include <linux/mm.h>\n#include <linux/slab.h>\n#include <linux/highmem.h>\n#include <linux/kasan.h>\n#include <linux/kmemleak.h>\n#include <linux/export.h>\n#include <linux/mempool.h>\n#include <linux/writeback.h>\n#include \"slab.h\"\n\n#if defined(CONFIG_DEBUG_SLAB) || defined(CONFIG_SLUB_DEBUG_ON)\nstatic void poison_error(mempool_t *pool, void *element, size_t size,\n\t\t\t size_t byte)\n{\n\tconst int nr = pool->curr_nr;\n\tconst int start = max_t(int, byte - (BITS_PER_LONG / 8), 0);\n\tconst int end = min_t(int, byte + (BITS_PER_LONG / 8), size);\n\tint i;\n\n\tpr_err(\"BUG: mempool element poison mismatch\\n\");\n\tpr_err(\"Mempool %p size %zu\\n\", pool, size);\n\tpr_err(\" nr=%d @ %p: %s0x\", nr, element, start > 0 ? \"... \" : \"\");\n\tfor (i = start; i < end; i++)\n\t\tpr_cont(\"%x \", *(u8 *)(element + i));\n\tpr_cont(\"%s\\n\", end < size ? \"...\" : \"\");\n\tdump_stack();\n}\n\nstatic void __check_element(mempool_t *pool, void *element, size_t size)\n{\n\tu8 *obj = element;\n\tsize_t i;\n\n\tfor (i = 0; i < size; i++) {\n\t\tu8 exp = (i < size - 1) ? POISON_FREE : POISON_END;\n\n\t\tif (obj[i] != exp) {\n\t\t\tpoison_error(pool, element, size, i);\n\t\t\treturn;\n\t\t}\n\t}\n\tmemset(obj, POISON_INUSE, size);\n}\n\nstatic void check_element(mempool_t *pool, void *element)\n{\n\t \n\tif (pool->free == mempool_kfree) {\n\t\t__check_element(pool, element, (size_t)pool->pool_data);\n\t} else if (pool->free == mempool_free_slab) {\n\t\t__check_element(pool, element, kmem_cache_size(pool->pool_data));\n\t} else if (pool->free == mempool_free_pages) {\n\t\t \n\t\tint order = (int)(long)pool->pool_data;\n\t\tvoid *addr = kmap_atomic((struct page *)element);\n\n\t\t__check_element(pool, addr, 1UL << (PAGE_SHIFT + order));\n\t\tkunmap_atomic(addr);\n\t}\n}\n\nstatic void __poison_element(void *element, size_t size)\n{\n\tu8 *obj = element;\n\n\tmemset(obj, POISON_FREE, size - 1);\n\tobj[size - 1] = POISON_END;\n}\n\nstatic void poison_element(mempool_t *pool, void *element)\n{\n\t \n\tif (pool->alloc == mempool_kmalloc) {\n\t\t__poison_element(element, (size_t)pool->pool_data);\n\t} else if (pool->alloc == mempool_alloc_slab) {\n\t\t__poison_element(element, kmem_cache_size(pool->pool_data));\n\t} else if (pool->alloc == mempool_alloc_pages) {\n\t\t \n\t\tint order = (int)(long)pool->pool_data;\n\t\tvoid *addr = kmap_atomic((struct page *)element);\n\n\t\t__poison_element(addr, 1UL << (PAGE_SHIFT + order));\n\t\tkunmap_atomic(addr);\n\t}\n}\n#else  \nstatic inline void check_element(mempool_t *pool, void *element)\n{\n}\nstatic inline void poison_element(mempool_t *pool, void *element)\n{\n}\n#endif  \n\nstatic __always_inline void kasan_poison_element(mempool_t *pool, void *element)\n{\n\tif (pool->alloc == mempool_alloc_slab || pool->alloc == mempool_kmalloc)\n\t\tkasan_slab_free_mempool(element);\n\telse if (pool->alloc == mempool_alloc_pages)\n\t\tkasan_poison_pages(element, (unsigned long)pool->pool_data,\n\t\t\t\t   false);\n}\n\nstatic void kasan_unpoison_element(mempool_t *pool, void *element)\n{\n\tif (pool->alloc == mempool_kmalloc)\n\t\tkasan_unpoison_range(element, (size_t)pool->pool_data);\n\telse if (pool->alloc == mempool_alloc_slab)\n\t\tkasan_unpoison_range(element, kmem_cache_size(pool->pool_data));\n\telse if (pool->alloc == mempool_alloc_pages)\n\t\tkasan_unpoison_pages(element, (unsigned long)pool->pool_data,\n\t\t\t\t     false);\n}\n\nstatic __always_inline void add_element(mempool_t *pool, void *element)\n{\n\tBUG_ON(pool->curr_nr >= pool->min_nr);\n\tpoison_element(pool, element);\n\tkasan_poison_element(pool, element);\n\tpool->elements[pool->curr_nr++] = element;\n}\n\nstatic void *remove_element(mempool_t *pool)\n{\n\tvoid *element = pool->elements[--pool->curr_nr];\n\n\tBUG_ON(pool->curr_nr < 0);\n\tkasan_unpoison_element(pool, element);\n\tcheck_element(pool, element);\n\treturn element;\n}\n\n \nvoid mempool_exit(mempool_t *pool)\n{\n\twhile (pool->curr_nr) {\n\t\tvoid *element = remove_element(pool);\n\t\tpool->free(element, pool->pool_data);\n\t}\n\tkfree(pool->elements);\n\tpool->elements = NULL;\n}\nEXPORT_SYMBOL(mempool_exit);\n\n \nvoid mempool_destroy(mempool_t *pool)\n{\n\tif (unlikely(!pool))\n\t\treturn;\n\n\tmempool_exit(pool);\n\tkfree(pool);\n}\nEXPORT_SYMBOL(mempool_destroy);\n\nint mempool_init_node(mempool_t *pool, int min_nr, mempool_alloc_t *alloc_fn,\n\t\t      mempool_free_t *free_fn, void *pool_data,\n\t\t      gfp_t gfp_mask, int node_id)\n{\n\tspin_lock_init(&pool->lock);\n\tpool->min_nr\t= min_nr;\n\tpool->pool_data = pool_data;\n\tpool->alloc\t= alloc_fn;\n\tpool->free\t= free_fn;\n\tinit_waitqueue_head(&pool->wait);\n\n\tpool->elements = kmalloc_array_node(min_nr, sizeof(void *),\n\t\t\t\t\t    gfp_mask, node_id);\n\tif (!pool->elements)\n\t\treturn -ENOMEM;\n\n\t \n\twhile (pool->curr_nr < pool->min_nr) {\n\t\tvoid *element;\n\n\t\telement = pool->alloc(gfp_mask, pool->pool_data);\n\t\tif (unlikely(!element)) {\n\t\t\tmempool_exit(pool);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tadd_element(pool, element);\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(mempool_init_node);\n\n \nint mempool_init(mempool_t *pool, int min_nr, mempool_alloc_t *alloc_fn,\n\t\t mempool_free_t *free_fn, void *pool_data)\n{\n\treturn mempool_init_node(pool, min_nr, alloc_fn, free_fn,\n\t\t\t\t pool_data, GFP_KERNEL, NUMA_NO_NODE);\n\n}\nEXPORT_SYMBOL(mempool_init);\n\n \nmempool_t *mempool_create(int min_nr, mempool_alloc_t *alloc_fn,\n\t\t\t\tmempool_free_t *free_fn, void *pool_data)\n{\n\treturn mempool_create_node(min_nr, alloc_fn, free_fn, pool_data,\n\t\t\t\t   GFP_KERNEL, NUMA_NO_NODE);\n}\nEXPORT_SYMBOL(mempool_create);\n\nmempool_t *mempool_create_node(int min_nr, mempool_alloc_t *alloc_fn,\n\t\t\t       mempool_free_t *free_fn, void *pool_data,\n\t\t\t       gfp_t gfp_mask, int node_id)\n{\n\tmempool_t *pool;\n\n\tpool = kzalloc_node(sizeof(*pool), gfp_mask, node_id);\n\tif (!pool)\n\t\treturn NULL;\n\n\tif (mempool_init_node(pool, min_nr, alloc_fn, free_fn, pool_data,\n\t\t\t      gfp_mask, node_id)) {\n\t\tkfree(pool);\n\t\treturn NULL;\n\t}\n\n\treturn pool;\n}\nEXPORT_SYMBOL(mempool_create_node);\n\n \nint mempool_resize(mempool_t *pool, int new_min_nr)\n{\n\tvoid *element;\n\tvoid **new_elements;\n\tunsigned long flags;\n\n\tBUG_ON(new_min_nr <= 0);\n\tmight_sleep();\n\n\tspin_lock_irqsave(&pool->lock, flags);\n\tif (new_min_nr <= pool->min_nr) {\n\t\twhile (new_min_nr < pool->curr_nr) {\n\t\t\telement = remove_element(pool);\n\t\t\tspin_unlock_irqrestore(&pool->lock, flags);\n\t\t\tpool->free(element, pool->pool_data);\n\t\t\tspin_lock_irqsave(&pool->lock, flags);\n\t\t}\n\t\tpool->min_nr = new_min_nr;\n\t\tgoto out_unlock;\n\t}\n\tspin_unlock_irqrestore(&pool->lock, flags);\n\n\t \n\tnew_elements = kmalloc_array(new_min_nr, sizeof(*new_elements),\n\t\t\t\t     GFP_KERNEL);\n\tif (!new_elements)\n\t\treturn -ENOMEM;\n\n\tspin_lock_irqsave(&pool->lock, flags);\n\tif (unlikely(new_min_nr <= pool->min_nr)) {\n\t\t \n\t\tspin_unlock_irqrestore(&pool->lock, flags);\n\t\tkfree(new_elements);\n\t\tgoto out;\n\t}\n\tmemcpy(new_elements, pool->elements,\n\t\t\tpool->curr_nr * sizeof(*new_elements));\n\tkfree(pool->elements);\n\tpool->elements = new_elements;\n\tpool->min_nr = new_min_nr;\n\n\twhile (pool->curr_nr < pool->min_nr) {\n\t\tspin_unlock_irqrestore(&pool->lock, flags);\n\t\telement = pool->alloc(GFP_KERNEL, pool->pool_data);\n\t\tif (!element)\n\t\t\tgoto out;\n\t\tspin_lock_irqsave(&pool->lock, flags);\n\t\tif (pool->curr_nr < pool->min_nr) {\n\t\t\tadd_element(pool, element);\n\t\t} else {\n\t\t\tspin_unlock_irqrestore(&pool->lock, flags);\n\t\t\tpool->free(element, pool->pool_data);\t \n\t\t\tgoto out;\n\t\t}\n\t}\nout_unlock:\n\tspin_unlock_irqrestore(&pool->lock, flags);\nout:\n\treturn 0;\n}\nEXPORT_SYMBOL(mempool_resize);\n\n \nvoid *mempool_alloc(mempool_t *pool, gfp_t gfp_mask)\n{\n\tvoid *element;\n\tunsigned long flags;\n\twait_queue_entry_t wait;\n\tgfp_t gfp_temp;\n\n\tVM_WARN_ON_ONCE(gfp_mask & __GFP_ZERO);\n\tmight_alloc(gfp_mask);\n\n\tgfp_mask |= __GFP_NOMEMALLOC;\t \n\tgfp_mask |= __GFP_NORETRY;\t \n\tgfp_mask |= __GFP_NOWARN;\t \n\n\tgfp_temp = gfp_mask & ~(__GFP_DIRECT_RECLAIM|__GFP_IO);\n\nrepeat_alloc:\n\n\telement = pool->alloc(gfp_temp, pool->pool_data);\n\tif (likely(element != NULL))\n\t\treturn element;\n\n\tspin_lock_irqsave(&pool->lock, flags);\n\tif (likely(pool->curr_nr)) {\n\t\telement = remove_element(pool);\n\t\tspin_unlock_irqrestore(&pool->lock, flags);\n\t\t \n\t\tsmp_wmb();\n\t\t \n\t\tkmemleak_update_trace(element);\n\t\treturn element;\n\t}\n\n\t \n\tif (gfp_temp != gfp_mask) {\n\t\tspin_unlock_irqrestore(&pool->lock, flags);\n\t\tgfp_temp = gfp_mask;\n\t\tgoto repeat_alloc;\n\t}\n\n\t \n\tif (!(gfp_mask & __GFP_DIRECT_RECLAIM)) {\n\t\tspin_unlock_irqrestore(&pool->lock, flags);\n\t\treturn NULL;\n\t}\n\n\t \n\tinit_wait(&wait);\n\tprepare_to_wait(&pool->wait, &wait, TASK_UNINTERRUPTIBLE);\n\n\tspin_unlock_irqrestore(&pool->lock, flags);\n\n\t \n\tio_schedule_timeout(5*HZ);\n\n\tfinish_wait(&pool->wait, &wait);\n\tgoto repeat_alloc;\n}\nEXPORT_SYMBOL(mempool_alloc);\n\n \nvoid mempool_free(void *element, mempool_t *pool)\n{\n\tunsigned long flags;\n\n\tif (unlikely(element == NULL))\n\t\treturn;\n\n\t \n\tsmp_rmb();\n\n\t \n\tif (unlikely(READ_ONCE(pool->curr_nr) < pool->min_nr)) {\n\t\tspin_lock_irqsave(&pool->lock, flags);\n\t\tif (likely(pool->curr_nr < pool->min_nr)) {\n\t\t\tadd_element(pool, element);\n\t\t\tspin_unlock_irqrestore(&pool->lock, flags);\n\t\t\twake_up(&pool->wait);\n\t\t\treturn;\n\t\t}\n\t\tspin_unlock_irqrestore(&pool->lock, flags);\n\t}\n\tpool->free(element, pool->pool_data);\n}\nEXPORT_SYMBOL(mempool_free);\n\n \nvoid *mempool_alloc_slab(gfp_t gfp_mask, void *pool_data)\n{\n\tstruct kmem_cache *mem = pool_data;\n\tVM_BUG_ON(mem->ctor);\n\treturn kmem_cache_alloc(mem, gfp_mask);\n}\nEXPORT_SYMBOL(mempool_alloc_slab);\n\nvoid mempool_free_slab(void *element, void *pool_data)\n{\n\tstruct kmem_cache *mem = pool_data;\n\tkmem_cache_free(mem, element);\n}\nEXPORT_SYMBOL(mempool_free_slab);\n\n \nvoid *mempool_kmalloc(gfp_t gfp_mask, void *pool_data)\n{\n\tsize_t size = (size_t)pool_data;\n\treturn kmalloc(size, gfp_mask);\n}\nEXPORT_SYMBOL(mempool_kmalloc);\n\nvoid mempool_kfree(void *element, void *pool_data)\n{\n\tkfree(element);\n}\nEXPORT_SYMBOL(mempool_kfree);\n\n \nvoid *mempool_alloc_pages(gfp_t gfp_mask, void *pool_data)\n{\n\tint order = (int)(long)pool_data;\n\treturn alloc_pages(gfp_mask, order);\n}\nEXPORT_SYMBOL(mempool_alloc_pages);\n\nvoid mempool_free_pages(void *element, void *pool_data)\n{\n\tint order = (int)(long)pool_data;\n\t__free_pages(element, order);\n}\nEXPORT_SYMBOL(mempool_free_pages);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}