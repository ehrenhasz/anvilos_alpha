{
  "module_name": "cma.c",
  "hash_id": "2ed2c34fdf1c46908f0b54aa7cc449915819b974872a25fd9fea2b5eff2703a0",
  "original_prompt": "Ingested from linux-6.6.14/mm/cma.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) \"cma: \" fmt\n\n#ifdef CONFIG_CMA_DEBUG\n#ifndef DEBUG\n#  define DEBUG\n#endif\n#endif\n#define CREATE_TRACE_POINTS\n\n#include <linux/memblock.h>\n#include <linux/err.h>\n#include <linux/mm.h>\n#include <linux/sizes.h>\n#include <linux/slab.h>\n#include <linux/log2.h>\n#include <linux/cma.h>\n#include <linux/highmem.h>\n#include <linux/io.h>\n#include <linux/kmemleak.h>\n#include <trace/events/cma.h>\n\n#include \"internal.h\"\n#include \"cma.h\"\n\nstruct cma cma_areas[MAX_CMA_AREAS];\nunsigned cma_area_count;\nstatic DEFINE_MUTEX(cma_mutex);\n\nphys_addr_t cma_get_base(const struct cma *cma)\n{\n\treturn PFN_PHYS(cma->base_pfn);\n}\n\nunsigned long cma_get_size(const struct cma *cma)\n{\n\treturn cma->count << PAGE_SHIFT;\n}\n\nconst char *cma_get_name(const struct cma *cma)\n{\n\treturn cma->name;\n}\n\nstatic unsigned long cma_bitmap_aligned_mask(const struct cma *cma,\n\t\t\t\t\t     unsigned int align_order)\n{\n\tif (align_order <= cma->order_per_bit)\n\t\treturn 0;\n\treturn (1UL << (align_order - cma->order_per_bit)) - 1;\n}\n\n \nstatic unsigned long cma_bitmap_aligned_offset(const struct cma *cma,\n\t\t\t\t\t       unsigned int align_order)\n{\n\treturn (cma->base_pfn & ((1UL << align_order) - 1))\n\t\t>> cma->order_per_bit;\n}\n\nstatic unsigned long cma_bitmap_pages_to_bits(const struct cma *cma,\n\t\t\t\t\t      unsigned long pages)\n{\n\treturn ALIGN(pages, 1UL << cma->order_per_bit) >> cma->order_per_bit;\n}\n\nstatic void cma_clear_bitmap(struct cma *cma, unsigned long pfn,\n\t\t\t     unsigned long count)\n{\n\tunsigned long bitmap_no, bitmap_count;\n\tunsigned long flags;\n\n\tbitmap_no = (pfn - cma->base_pfn) >> cma->order_per_bit;\n\tbitmap_count = cma_bitmap_pages_to_bits(cma, count);\n\n\tspin_lock_irqsave(&cma->lock, flags);\n\tbitmap_clear(cma->bitmap, bitmap_no, bitmap_count);\n\tspin_unlock_irqrestore(&cma->lock, flags);\n}\n\nstatic void __init cma_activate_area(struct cma *cma)\n{\n\tunsigned long base_pfn = cma->base_pfn, pfn;\n\tstruct zone *zone;\n\n\tcma->bitmap = bitmap_zalloc(cma_bitmap_maxno(cma), GFP_KERNEL);\n\tif (!cma->bitmap)\n\t\tgoto out_error;\n\n\t \n\tWARN_ON_ONCE(!pfn_valid(base_pfn));\n\tzone = page_zone(pfn_to_page(base_pfn));\n\tfor (pfn = base_pfn + 1; pfn < base_pfn + cma->count; pfn++) {\n\t\tWARN_ON_ONCE(!pfn_valid(pfn));\n\t\tif (page_zone(pfn_to_page(pfn)) != zone)\n\t\t\tgoto not_in_zone;\n\t}\n\n\tfor (pfn = base_pfn; pfn < base_pfn + cma->count;\n\t     pfn += pageblock_nr_pages)\n\t\tinit_cma_reserved_pageblock(pfn_to_page(pfn));\n\n\tspin_lock_init(&cma->lock);\n\n#ifdef CONFIG_CMA_DEBUGFS\n\tINIT_HLIST_HEAD(&cma->mem_head);\n\tspin_lock_init(&cma->mem_head_lock);\n#endif\n\n\treturn;\n\nnot_in_zone:\n\tbitmap_free(cma->bitmap);\nout_error:\n\t \n\tif (!cma->reserve_pages_on_error) {\n\t\tfor (pfn = base_pfn; pfn < base_pfn + cma->count; pfn++)\n\t\t\tfree_reserved_page(pfn_to_page(pfn));\n\t}\n\ttotalcma_pages -= cma->count;\n\tcma->count = 0;\n\tpr_err(\"CMA area %s could not be activated\\n\", cma->name);\n\treturn;\n}\n\nstatic int __init cma_init_reserved_areas(void)\n{\n\tint i;\n\n\tfor (i = 0; i < cma_area_count; i++)\n\t\tcma_activate_area(&cma_areas[i]);\n\n\treturn 0;\n}\ncore_initcall(cma_init_reserved_areas);\n\nvoid __init cma_reserve_pages_on_error(struct cma *cma)\n{\n\tcma->reserve_pages_on_error = true;\n}\n\n \nint __init cma_init_reserved_mem(phys_addr_t base, phys_addr_t size,\n\t\t\t\t unsigned int order_per_bit,\n\t\t\t\t const char *name,\n\t\t\t\t struct cma **res_cma)\n{\n\tstruct cma *cma;\n\n\t \n\tif (cma_area_count == ARRAY_SIZE(cma_areas)) {\n\t\tpr_err(\"Not enough slots for CMA reserved regions!\\n\");\n\t\treturn -ENOSPC;\n\t}\n\n\tif (!size || !memblock_is_region_reserved(base, size))\n\t\treturn -EINVAL;\n\n\t \n\tif (!IS_ALIGNED(CMA_MIN_ALIGNMENT_PAGES, 1 << order_per_bit))\n\t\treturn -EINVAL;\n\n\t \n\tif (!IS_ALIGNED(base | size, CMA_MIN_ALIGNMENT_BYTES))\n\t\treturn -EINVAL;\n\n\t \n\tcma = &cma_areas[cma_area_count];\n\n\tif (name)\n\t\tsnprintf(cma->name, CMA_MAX_NAME, name);\n\telse\n\t\tsnprintf(cma->name, CMA_MAX_NAME,  \"cma%d\\n\", cma_area_count);\n\n\tcma->base_pfn = PFN_DOWN(base);\n\tcma->count = size >> PAGE_SHIFT;\n\tcma->order_per_bit = order_per_bit;\n\t*res_cma = cma;\n\tcma_area_count++;\n\ttotalcma_pages += (size / PAGE_SIZE);\n\n\treturn 0;\n}\n\n \nint __init cma_declare_contiguous_nid(phys_addr_t base,\n\t\t\tphys_addr_t size, phys_addr_t limit,\n\t\t\tphys_addr_t alignment, unsigned int order_per_bit,\n\t\t\tbool fixed, const char *name, struct cma **res_cma,\n\t\t\tint nid)\n{\n\tphys_addr_t memblock_end = memblock_end_of_DRAM();\n\tphys_addr_t highmem_start;\n\tint ret = 0;\n\n\t \n\thighmem_start = __pa(high_memory - 1) + 1;\n\tpr_debug(\"%s(size %pa, base %pa, limit %pa alignment %pa)\\n\",\n\t\t__func__, &size, &base, &limit, &alignment);\n\n\tif (cma_area_count == ARRAY_SIZE(cma_areas)) {\n\t\tpr_err(\"Not enough slots for CMA reserved regions!\\n\");\n\t\treturn -ENOSPC;\n\t}\n\n\tif (!size)\n\t\treturn -EINVAL;\n\n\tif (alignment && !is_power_of_2(alignment))\n\t\treturn -EINVAL;\n\n\tif (!IS_ENABLED(CONFIG_NUMA))\n\t\tnid = NUMA_NO_NODE;\n\n\t \n\talignment = max_t(phys_addr_t, alignment, CMA_MIN_ALIGNMENT_BYTES);\n\tif (fixed && base & (alignment - 1)) {\n\t\tret = -EINVAL;\n\t\tpr_err(\"Region at %pa must be aligned to %pa bytes\\n\",\n\t\t\t&base, &alignment);\n\t\tgoto err;\n\t}\n\tbase = ALIGN(base, alignment);\n\tsize = ALIGN(size, alignment);\n\tlimit &= ~(alignment - 1);\n\n\tif (!base)\n\t\tfixed = false;\n\n\t \n\tif (!IS_ALIGNED(size >> PAGE_SHIFT, 1 << order_per_bit))\n\t\treturn -EINVAL;\n\n\t \n\tif (fixed && base < highmem_start && base + size > highmem_start) {\n\t\tret = -EINVAL;\n\t\tpr_err(\"Region at %pa defined on low/high memory boundary (%pa)\\n\",\n\t\t\t&base, &highmem_start);\n\t\tgoto err;\n\t}\n\n\t \n\tif (limit == 0 || limit > memblock_end)\n\t\tlimit = memblock_end;\n\n\tif (base + size > limit) {\n\t\tret = -EINVAL;\n\t\tpr_err(\"Size (%pa) of region at %pa exceeds limit (%pa)\\n\",\n\t\t\t&size, &base, &limit);\n\t\tgoto err;\n\t}\n\n\t \n\tif (fixed) {\n\t\tif (memblock_is_region_reserved(base, size) ||\n\t\t    memblock_reserve(base, size) < 0) {\n\t\t\tret = -EBUSY;\n\t\t\tgoto err;\n\t\t}\n\t} else {\n\t\tphys_addr_t addr = 0;\n\n\t\t \n#ifdef CONFIG_PHYS_ADDR_T_64BIT\n\t\tif (!memblock_bottom_up() && memblock_end >= SZ_4G + size) {\n\t\t\tmemblock_set_bottom_up(true);\n\t\t\taddr = memblock_alloc_range_nid(size, alignment, SZ_4G,\n\t\t\t\t\t\t\tlimit, nid, true);\n\t\t\tmemblock_set_bottom_up(false);\n\t\t}\n#endif\n\n\t\t \n\t\tif (!addr && base < highmem_start && limit > highmem_start) {\n\t\t\taddr = memblock_alloc_range_nid(size, alignment,\n\t\t\t\t\thighmem_start, limit, nid, true);\n\t\t\tlimit = highmem_start;\n\t\t}\n\n\t\tif (!addr) {\n\t\t\taddr = memblock_alloc_range_nid(size, alignment, base,\n\t\t\t\t\tlimit, nid, true);\n\t\t\tif (!addr) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tkmemleak_ignore_phys(addr);\n\t\tbase = addr;\n\t}\n\n\tret = cma_init_reserved_mem(base, size, order_per_bit, name, res_cma);\n\tif (ret)\n\t\tgoto free_mem;\n\n\tpr_info(\"Reserved %ld MiB at %pa on node %d\\n\", (unsigned long)size / SZ_1M,\n\t\t&base, nid);\n\treturn 0;\n\nfree_mem:\n\tmemblock_phys_free(base, size);\nerr:\n\tpr_err(\"Failed to reserve %ld MiB on node %d\\n\", (unsigned long)size / SZ_1M,\n\t       nid);\n\treturn ret;\n}\n\n#ifdef CONFIG_CMA_DEBUG\nstatic void cma_debug_show_areas(struct cma *cma)\n{\n\tunsigned long next_zero_bit, next_set_bit, nr_zero;\n\tunsigned long start = 0;\n\tunsigned long nr_part, nr_total = 0;\n\tunsigned long nbits = cma_bitmap_maxno(cma);\n\n\tspin_lock_irq(&cma->lock);\n\tpr_info(\"number of available pages: \");\n\tfor (;;) {\n\t\tnext_zero_bit = find_next_zero_bit(cma->bitmap, nbits, start);\n\t\tif (next_zero_bit >= nbits)\n\t\t\tbreak;\n\t\tnext_set_bit = find_next_bit(cma->bitmap, nbits, next_zero_bit);\n\t\tnr_zero = next_set_bit - next_zero_bit;\n\t\tnr_part = nr_zero << cma->order_per_bit;\n\t\tpr_cont(\"%s%lu@%lu\", nr_total ? \"+\" : \"\", nr_part,\n\t\t\tnext_zero_bit);\n\t\tnr_total += nr_part;\n\t\tstart = next_zero_bit + nr_zero;\n\t}\n\tpr_cont(\"=> %lu free of %lu total pages\\n\", nr_total, cma->count);\n\tspin_unlock_irq(&cma->lock);\n}\n#else\nstatic inline void cma_debug_show_areas(struct cma *cma) { }\n#endif\n\n \nstruct page *cma_alloc(struct cma *cma, unsigned long count,\n\t\t       unsigned int align, bool no_warn)\n{\n\tunsigned long mask, offset;\n\tunsigned long pfn = -1;\n\tunsigned long start = 0;\n\tunsigned long bitmap_maxno, bitmap_no, bitmap_count;\n\tunsigned long i;\n\tstruct page *page = NULL;\n\tint ret = -ENOMEM;\n\n\tif (!cma || !cma->count || !cma->bitmap)\n\t\tgoto out;\n\n\tpr_debug(\"%s(cma %p, name: %s, count %lu, align %d)\\n\", __func__,\n\t\t(void *)cma, cma->name, count, align);\n\n\tif (!count)\n\t\tgoto out;\n\n\ttrace_cma_alloc_start(cma->name, count, align);\n\n\tmask = cma_bitmap_aligned_mask(cma, align);\n\toffset = cma_bitmap_aligned_offset(cma, align);\n\tbitmap_maxno = cma_bitmap_maxno(cma);\n\tbitmap_count = cma_bitmap_pages_to_bits(cma, count);\n\n\tif (bitmap_count > bitmap_maxno)\n\t\tgoto out;\n\n\tfor (;;) {\n\t\tspin_lock_irq(&cma->lock);\n\t\tbitmap_no = bitmap_find_next_zero_area_off(cma->bitmap,\n\t\t\t\tbitmap_maxno, start, bitmap_count, mask,\n\t\t\t\toffset);\n\t\tif (bitmap_no >= bitmap_maxno) {\n\t\t\tspin_unlock_irq(&cma->lock);\n\t\t\tbreak;\n\t\t}\n\t\tbitmap_set(cma->bitmap, bitmap_no, bitmap_count);\n\t\t \n\t\tspin_unlock_irq(&cma->lock);\n\n\t\tpfn = cma->base_pfn + (bitmap_no << cma->order_per_bit);\n\t\tmutex_lock(&cma_mutex);\n\t\tret = alloc_contig_range(pfn, pfn + count, MIGRATE_CMA,\n\t\t\t\t     GFP_KERNEL | (no_warn ? __GFP_NOWARN : 0));\n\t\tmutex_unlock(&cma_mutex);\n\t\tif (ret == 0) {\n\t\t\tpage = pfn_to_page(pfn);\n\t\t\tbreak;\n\t\t}\n\n\t\tcma_clear_bitmap(cma, pfn, count);\n\t\tif (ret != -EBUSY)\n\t\t\tbreak;\n\n\t\tpr_debug(\"%s(): memory range at pfn 0x%lx %p is busy, retrying\\n\",\n\t\t\t __func__, pfn, pfn_to_page(pfn));\n\n\t\ttrace_cma_alloc_busy_retry(cma->name, pfn, pfn_to_page(pfn),\n\t\t\t\t\t   count, align);\n\t\t \n\t\tstart = bitmap_no + mask + 1;\n\t}\n\n\ttrace_cma_alloc_finish(cma->name, pfn, page, count, align, ret);\n\n\t \n\tif (page) {\n\t\tfor (i = 0; i < count; i++)\n\t\t\tpage_kasan_tag_reset(nth_page(page, i));\n\t}\n\n\tif (ret && !no_warn) {\n\t\tpr_err_ratelimited(\"%s: %s: alloc failed, req-size: %lu pages, ret: %d\\n\",\n\t\t\t\t   __func__, cma->name, count, ret);\n\t\tcma_debug_show_areas(cma);\n\t}\n\n\tpr_debug(\"%s(): returned %p\\n\", __func__, page);\nout:\n\tif (page) {\n\t\tcount_vm_event(CMA_ALLOC_SUCCESS);\n\t\tcma_sysfs_account_success_pages(cma, count);\n\t} else {\n\t\tcount_vm_event(CMA_ALLOC_FAIL);\n\t\tif (cma)\n\t\t\tcma_sysfs_account_fail_pages(cma, count);\n\t}\n\n\treturn page;\n}\n\nbool cma_pages_valid(struct cma *cma, const struct page *pages,\n\t\t     unsigned long count)\n{\n\tunsigned long pfn;\n\n\tif (!cma || !pages)\n\t\treturn false;\n\n\tpfn = page_to_pfn(pages);\n\n\tif (pfn < cma->base_pfn || pfn >= cma->base_pfn + cma->count) {\n\t\tpr_debug(\"%s(page %p, count %lu)\\n\", __func__,\n\t\t\t\t\t\t(void *)pages, count);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\n \nbool cma_release(struct cma *cma, const struct page *pages,\n\t\t unsigned long count)\n{\n\tunsigned long pfn;\n\n\tif (!cma_pages_valid(cma, pages, count))\n\t\treturn false;\n\n\tpr_debug(\"%s(page %p, count %lu)\\n\", __func__, (void *)pages, count);\n\n\tpfn = page_to_pfn(pages);\n\n\tVM_BUG_ON(pfn + count > cma->base_pfn + cma->count);\n\n\tfree_contig_range(pfn, count);\n\tcma_clear_bitmap(cma, pfn, count);\n\ttrace_cma_release(cma->name, pfn, pages, count);\n\n\treturn true;\n}\n\nint cma_for_each_area(int (*it)(struct cma *cma, void *data), void *data)\n{\n\tint i;\n\n\tfor (i = 0; i < cma_area_count; i++) {\n\t\tint ret = it(&cma_areas[i], data);\n\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}