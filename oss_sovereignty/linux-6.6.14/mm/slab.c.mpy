{
  "module_name": "slab.c",
  "hash_id": "275148d41783fcfac08fa8875ee2dcc86c2a0992b9310b8820a3bb557f277b31",
  "original_prompt": "Ingested from linux-6.6.14/mm/slab.c",
  "human_readable_source": "\n \n\n#include\t<linux/slab.h>\n#include\t<linux/mm.h>\n#include\t<linux/poison.h>\n#include\t<linux/swap.h>\n#include\t<linux/cache.h>\n#include\t<linux/interrupt.h>\n#include\t<linux/init.h>\n#include\t<linux/compiler.h>\n#include\t<linux/cpuset.h>\n#include\t<linux/proc_fs.h>\n#include\t<linux/seq_file.h>\n#include\t<linux/notifier.h>\n#include\t<linux/kallsyms.h>\n#include\t<linux/kfence.h>\n#include\t<linux/cpu.h>\n#include\t<linux/sysctl.h>\n#include\t<linux/module.h>\n#include\t<linux/rcupdate.h>\n#include\t<linux/string.h>\n#include\t<linux/uaccess.h>\n#include\t<linux/nodemask.h>\n#include\t<linux/kmemleak.h>\n#include\t<linux/mempolicy.h>\n#include\t<linux/mutex.h>\n#include\t<linux/fault-inject.h>\n#include\t<linux/rtmutex.h>\n#include\t<linux/reciprocal_div.h>\n#include\t<linux/debugobjects.h>\n#include\t<linux/memory.h>\n#include\t<linux/prefetch.h>\n#include\t<linux/sched/task_stack.h>\n\n#include\t<net/sock.h>\n\n#include\t<asm/cacheflush.h>\n#include\t<asm/tlbflush.h>\n#include\t<asm/page.h>\n\n#include <trace/events/kmem.h>\n\n#include\t\"internal.h\"\n\n#include\t\"slab.h\"\n\n \n\n#ifdef CONFIG_DEBUG_SLAB\n#define\tDEBUG\t\t1\n#define\tSTATS\t\t1\n#define\tFORCED_DEBUG\t1\n#else\n#define\tDEBUG\t\t0\n#define\tSTATS\t\t0\n#define\tFORCED_DEBUG\t0\n#endif\n\n \n#define\tBYTES_PER_WORD\t\tsizeof(void *)\n#define\tREDZONE_ALIGN\t\tmax(BYTES_PER_WORD, __alignof__(unsigned long long))\n\n#ifndef ARCH_KMALLOC_FLAGS\n#define ARCH_KMALLOC_FLAGS SLAB_HWCACHE_ALIGN\n#endif\n\n#define FREELIST_BYTE_INDEX (((PAGE_SIZE >> BITS_PER_BYTE) \\\n\t\t\t\t<= SLAB_OBJ_MIN_SIZE) ? 1 : 0)\n\n#if FREELIST_BYTE_INDEX\ntypedef unsigned char freelist_idx_t;\n#else\ntypedef unsigned short freelist_idx_t;\n#endif\n\n#define SLAB_OBJ_MAX_NUM ((1 << sizeof(freelist_idx_t) * BITS_PER_BYTE) - 1)\n\n \nstruct array_cache {\n\tunsigned int avail;\n\tunsigned int limit;\n\tunsigned int batchcount;\n\tunsigned int touched;\n\tvoid *entry[];\t \n};\n\nstruct alien_cache {\n\tspinlock_t lock;\n\tstruct array_cache ac;\n};\n\n \n#define NUM_INIT_LISTS (2 * MAX_NUMNODES)\nstatic struct kmem_cache_node __initdata init_kmem_cache_node[NUM_INIT_LISTS];\n#define\tCACHE_CACHE 0\n#define\tSIZE_NODE (MAX_NUMNODES)\n\nstatic int drain_freelist(struct kmem_cache *cache,\n\t\t\tstruct kmem_cache_node *n, int tofree);\nstatic void free_block(struct kmem_cache *cachep, void **objpp, int len,\n\t\t\tint node, struct list_head *list);\nstatic void slabs_destroy(struct kmem_cache *cachep, struct list_head *list);\nstatic int enable_cpucache(struct kmem_cache *cachep, gfp_t gfp);\nstatic void cache_reap(struct work_struct *unused);\n\nstatic inline void fixup_objfreelist_debug(struct kmem_cache *cachep,\n\t\t\t\t\t\tvoid **list);\nstatic inline void fixup_slab_list(struct kmem_cache *cachep,\n\t\t\t\tstruct kmem_cache_node *n, struct slab *slab,\n\t\t\t\tvoid **list);\n\n#define INDEX_NODE kmalloc_index(sizeof(struct kmem_cache_node))\n\nstatic void kmem_cache_node_init(struct kmem_cache_node *parent)\n{\n\tINIT_LIST_HEAD(&parent->slabs_full);\n\tINIT_LIST_HEAD(&parent->slabs_partial);\n\tINIT_LIST_HEAD(&parent->slabs_free);\n\tparent->total_slabs = 0;\n\tparent->free_slabs = 0;\n\tparent->shared = NULL;\n\tparent->alien = NULL;\n\tparent->colour_next = 0;\n\traw_spin_lock_init(&parent->list_lock);\n\tparent->free_objects = 0;\n\tparent->free_touched = 0;\n}\n\n#define MAKE_LIST(cachep, listp, slab, nodeid)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tINIT_LIST_HEAD(listp);\t\t\t\t\t\\\n\t\tlist_splice(&get_node(cachep, nodeid)->slab, listp);\t\\\n\t} while (0)\n\n#define\tMAKE_ALL_LISTS(cachep, ptr, nodeid)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\tMAKE_LIST((cachep), (&(ptr)->slabs_full), slabs_full, nodeid);\t\\\n\tMAKE_LIST((cachep), (&(ptr)->slabs_partial), slabs_partial, nodeid); \\\n\tMAKE_LIST((cachep), (&(ptr)->slabs_free), slabs_free, nodeid);\t\\\n\t} while (0)\n\n#define CFLGS_OBJFREELIST_SLAB\t((slab_flags_t __force)0x40000000U)\n#define CFLGS_OFF_SLAB\t\t((slab_flags_t __force)0x80000000U)\n#define\tOBJFREELIST_SLAB(x)\t((x)->flags & CFLGS_OBJFREELIST_SLAB)\n#define\tOFF_SLAB(x)\t((x)->flags & CFLGS_OFF_SLAB)\n\n#define BATCHREFILL_LIMIT\t16\n \n#define REAPTIMEOUT_AC\t\t(2*HZ)\n#define REAPTIMEOUT_NODE\t(4*HZ)\n\n#if STATS\n#define\tSTATS_INC_ACTIVE(x)\t((x)->num_active++)\n#define\tSTATS_DEC_ACTIVE(x)\t((x)->num_active--)\n#define\tSTATS_INC_ALLOCED(x)\t((x)->num_allocations++)\n#define\tSTATS_INC_GROWN(x)\t((x)->grown++)\n#define\tSTATS_ADD_REAPED(x, y)\t((x)->reaped += (y))\n#define\tSTATS_SET_HIGH(x)\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif ((x)->num_active > (x)->high_mark)\t\t\t\\\n\t\t\t(x)->high_mark = (x)->num_active;\t\t\\\n\t} while (0)\n#define\tSTATS_INC_ERR(x)\t((x)->errors++)\n#define\tSTATS_INC_NODEALLOCS(x)\t((x)->node_allocs++)\n#define\tSTATS_INC_NODEFREES(x)\t((x)->node_frees++)\n#define STATS_INC_ACOVERFLOW(x)   ((x)->node_overflow++)\n#define\tSTATS_SET_FREEABLE(x, i)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif ((x)->max_freeable < i)\t\t\t\t\\\n\t\t\t(x)->max_freeable = i;\t\t\t\t\\\n\t} while (0)\n#define STATS_INC_ALLOCHIT(x)\tatomic_inc(&(x)->allochit)\n#define STATS_INC_ALLOCMISS(x)\tatomic_inc(&(x)->allocmiss)\n#define STATS_INC_FREEHIT(x)\tatomic_inc(&(x)->freehit)\n#define STATS_INC_FREEMISS(x)\tatomic_inc(&(x)->freemiss)\n#else\n#define\tSTATS_INC_ACTIVE(x)\tdo { } while (0)\n#define\tSTATS_DEC_ACTIVE(x)\tdo { } while (0)\n#define\tSTATS_INC_ALLOCED(x)\tdo { } while (0)\n#define\tSTATS_INC_GROWN(x)\tdo { } while (0)\n#define\tSTATS_ADD_REAPED(x, y)\tdo { (void)(y); } while (0)\n#define\tSTATS_SET_HIGH(x)\tdo { } while (0)\n#define\tSTATS_INC_ERR(x)\tdo { } while (0)\n#define\tSTATS_INC_NODEALLOCS(x)\tdo { } while (0)\n#define\tSTATS_INC_NODEFREES(x)\tdo { } while (0)\n#define STATS_INC_ACOVERFLOW(x)   do { } while (0)\n#define\tSTATS_SET_FREEABLE(x, i) do { } while (0)\n#define STATS_INC_ALLOCHIT(x)\tdo { } while (0)\n#define STATS_INC_ALLOCMISS(x)\tdo { } while (0)\n#define STATS_INC_FREEHIT(x)\tdo { } while (0)\n#define STATS_INC_FREEMISS(x)\tdo { } while (0)\n#endif\n\n#if DEBUG\n\n \nstatic int obj_offset(struct kmem_cache *cachep)\n{\n\treturn cachep->obj_offset;\n}\n\nstatic unsigned long long *dbg_redzone1(struct kmem_cache *cachep, void *objp)\n{\n\tBUG_ON(!(cachep->flags & SLAB_RED_ZONE));\n\treturn (unsigned long long *) (objp + obj_offset(cachep) -\n\t\t\t\t      sizeof(unsigned long long));\n}\n\nstatic unsigned long long *dbg_redzone2(struct kmem_cache *cachep, void *objp)\n{\n\tBUG_ON(!(cachep->flags & SLAB_RED_ZONE));\n\tif (cachep->flags & SLAB_STORE_USER)\n\t\treturn (unsigned long long *)(objp + cachep->size -\n\t\t\t\t\t      sizeof(unsigned long long) -\n\t\t\t\t\t      REDZONE_ALIGN);\n\treturn (unsigned long long *) (objp + cachep->size -\n\t\t\t\t       sizeof(unsigned long long));\n}\n\nstatic void **dbg_userword(struct kmem_cache *cachep, void *objp)\n{\n\tBUG_ON(!(cachep->flags & SLAB_STORE_USER));\n\treturn (void **)(objp + cachep->size - BYTES_PER_WORD);\n}\n\n#else\n\n#define obj_offset(x)\t\t\t0\n#define dbg_redzone1(cachep, objp)\t({BUG(); (unsigned long long *)NULL;})\n#define dbg_redzone2(cachep, objp)\t({BUG(); (unsigned long long *)NULL;})\n#define dbg_userword(cachep, objp)\t({BUG(); (void **)NULL;})\n\n#endif\n\n \n#define\tSLAB_MAX_ORDER_HI\t1\n#define\tSLAB_MAX_ORDER_LO\t0\nstatic int slab_max_order = SLAB_MAX_ORDER_LO;\nstatic bool slab_max_order_set __initdata;\n\nstatic inline void *index_to_obj(struct kmem_cache *cache,\n\t\t\t\t const struct slab *slab, unsigned int idx)\n{\n\treturn slab->s_mem + cache->size * idx;\n}\n\n#define BOOT_CPUCACHE_ENTRIES\t1\n \nstatic struct kmem_cache kmem_cache_boot = {\n\t.batchcount = 1,\n\t.limit = BOOT_CPUCACHE_ENTRIES,\n\t.shared = 1,\n\t.size = sizeof(struct kmem_cache),\n\t.name = \"kmem_cache\",\n};\n\nstatic DEFINE_PER_CPU(struct delayed_work, slab_reap_work);\n\nstatic inline struct array_cache *cpu_cache_get(struct kmem_cache *cachep)\n{\n\treturn this_cpu_ptr(cachep->cpu_cache);\n}\n\n \nstatic unsigned int cache_estimate(unsigned long gfporder, size_t buffer_size,\n\t\tslab_flags_t flags, size_t *left_over)\n{\n\tunsigned int num;\n\tsize_t slab_size = PAGE_SIZE << gfporder;\n\n\t \n\tif (flags & (CFLGS_OBJFREELIST_SLAB | CFLGS_OFF_SLAB)) {\n\t\tnum = slab_size / buffer_size;\n\t\t*left_over = slab_size % buffer_size;\n\t} else {\n\t\tnum = slab_size / (buffer_size + sizeof(freelist_idx_t));\n\t\t*left_over = slab_size %\n\t\t\t(buffer_size + sizeof(freelist_idx_t));\n\t}\n\n\treturn num;\n}\n\n#if DEBUG\n#define slab_error(cachep, msg) __slab_error(__func__, cachep, msg)\n\nstatic void __slab_error(const char *function, struct kmem_cache *cachep,\n\t\t\tchar *msg)\n{\n\tpr_err(\"slab error in %s(): cache `%s': %s\\n\",\n\t       function, cachep->name, msg);\n\tdump_stack();\n\tadd_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);\n}\n#endif\n\n \n\nstatic int use_alien_caches __read_mostly = 1;\nstatic int __init noaliencache_setup(char *s)\n{\n\tuse_alien_caches = 0;\n\treturn 1;\n}\n__setup(\"noaliencache\", noaliencache_setup);\n\nstatic int __init slab_max_order_setup(char *str)\n{\n\tget_option(&str, &slab_max_order);\n\tslab_max_order = slab_max_order < 0 ? 0 :\n\t\t\t\tmin(slab_max_order, MAX_ORDER);\n\tslab_max_order_set = true;\n\n\treturn 1;\n}\n__setup(\"slab_max_order=\", slab_max_order_setup);\n\n#ifdef CONFIG_NUMA\n \nstatic DEFINE_PER_CPU(unsigned long, slab_reap_node);\n\nstatic void init_reap_node(int cpu)\n{\n\tper_cpu(slab_reap_node, cpu) = next_node_in(cpu_to_mem(cpu),\n\t\t\t\t\t\t    node_online_map);\n}\n\nstatic void next_reap_node(void)\n{\n\tint node = __this_cpu_read(slab_reap_node);\n\n\tnode = next_node_in(node, node_online_map);\n\t__this_cpu_write(slab_reap_node, node);\n}\n\n#else\n#define init_reap_node(cpu) do { } while (0)\n#define next_reap_node(void) do { } while (0)\n#endif\n\n \nstatic void start_cpu_timer(int cpu)\n{\n\tstruct delayed_work *reap_work = &per_cpu(slab_reap_work, cpu);\n\n\tif (reap_work->work.func == NULL) {\n\t\tinit_reap_node(cpu);\n\t\tINIT_DEFERRABLE_WORK(reap_work, cache_reap);\n\t\tschedule_delayed_work_on(cpu, reap_work,\n\t\t\t\t\t__round_jiffies_relative(HZ, cpu));\n\t}\n}\n\nstatic void init_arraycache(struct array_cache *ac, int limit, int batch)\n{\n\tif (ac) {\n\t\tac->avail = 0;\n\t\tac->limit = limit;\n\t\tac->batchcount = batch;\n\t\tac->touched = 0;\n\t}\n}\n\nstatic struct array_cache *alloc_arraycache(int node, int entries,\n\t\t\t\t\t    int batchcount, gfp_t gfp)\n{\n\tsize_t memsize = sizeof(void *) * entries + sizeof(struct array_cache);\n\tstruct array_cache *ac = NULL;\n\n\tac = kmalloc_node(memsize, gfp, node);\n\t \n\tkmemleak_no_scan(ac);\n\tinit_arraycache(ac, entries, batchcount);\n\treturn ac;\n}\n\nstatic noinline void cache_free_pfmemalloc(struct kmem_cache *cachep,\n\t\t\t\t\tstruct slab *slab, void *objp)\n{\n\tstruct kmem_cache_node *n;\n\tint slab_node;\n\tLIST_HEAD(list);\n\n\tslab_node = slab_nid(slab);\n\tn = get_node(cachep, slab_node);\n\n\traw_spin_lock(&n->list_lock);\n\tfree_block(cachep, &objp, 1, slab_node, &list);\n\traw_spin_unlock(&n->list_lock);\n\n\tslabs_destroy(cachep, &list);\n}\n\n \nstatic int transfer_objects(struct array_cache *to,\n\t\tstruct array_cache *from, unsigned int max)\n{\n\t \n\tint nr = min3(from->avail, max, to->limit - to->avail);\n\n\tif (!nr)\n\t\treturn 0;\n\n\tmemcpy(to->entry + to->avail, from->entry + from->avail - nr,\n\t\t\tsizeof(void *) *nr);\n\n\tfrom->avail -= nr;\n\tto->avail += nr;\n\treturn nr;\n}\n\n \nstatic __always_inline void __free_one(struct array_cache *ac, void *objp)\n{\n\t \n\tif (IS_ENABLED(CONFIG_SLAB_FREELIST_HARDENED) &&\n\t    WARN_ON_ONCE(ac->avail > 0 && ac->entry[ac->avail - 1] == objp))\n\t\treturn;\n\tac->entry[ac->avail++] = objp;\n}\n\n#ifndef CONFIG_NUMA\n\n#define drain_alien_cache(cachep, alien) do { } while (0)\n#define reap_alien(cachep, n) do { } while (0)\n\nstatic inline struct alien_cache **alloc_alien_cache(int node,\n\t\t\t\t\t\tint limit, gfp_t gfp)\n{\n\treturn NULL;\n}\n\nstatic inline void free_alien_cache(struct alien_cache **ac_ptr)\n{\n}\n\nstatic inline int cache_free_alien(struct kmem_cache *cachep, void *objp)\n{\n\treturn 0;\n}\n\nstatic inline gfp_t gfp_exact_node(gfp_t flags)\n{\n\treturn flags & ~__GFP_NOFAIL;\n}\n\n#else\t \n\nstatic struct alien_cache *__alloc_alien_cache(int node, int entries,\n\t\t\t\t\t\tint batch, gfp_t gfp)\n{\n\tsize_t memsize = sizeof(void *) * entries + sizeof(struct alien_cache);\n\tstruct alien_cache *alc = NULL;\n\n\talc = kmalloc_node(memsize, gfp, node);\n\tif (alc) {\n\t\tkmemleak_no_scan(alc);\n\t\tinit_arraycache(&alc->ac, entries, batch);\n\t\tspin_lock_init(&alc->lock);\n\t}\n\treturn alc;\n}\n\nstatic struct alien_cache **alloc_alien_cache(int node, int limit, gfp_t gfp)\n{\n\tstruct alien_cache **alc_ptr;\n\tint i;\n\n\tif (limit > 1)\n\t\tlimit = 12;\n\talc_ptr = kcalloc_node(nr_node_ids, sizeof(void *), gfp, node);\n\tif (!alc_ptr)\n\t\treturn NULL;\n\n\tfor_each_node(i) {\n\t\tif (i == node || !node_online(i))\n\t\t\tcontinue;\n\t\talc_ptr[i] = __alloc_alien_cache(node, limit, 0xbaadf00d, gfp);\n\t\tif (!alc_ptr[i]) {\n\t\t\tfor (i--; i >= 0; i--)\n\t\t\t\tkfree(alc_ptr[i]);\n\t\t\tkfree(alc_ptr);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\treturn alc_ptr;\n}\n\nstatic void free_alien_cache(struct alien_cache **alc_ptr)\n{\n\tint i;\n\n\tif (!alc_ptr)\n\t\treturn;\n\tfor_each_node(i)\n\t    kfree(alc_ptr[i]);\n\tkfree(alc_ptr);\n}\n\nstatic void __drain_alien_cache(struct kmem_cache *cachep,\n\t\t\t\tstruct array_cache *ac, int node,\n\t\t\t\tstruct list_head *list)\n{\n\tstruct kmem_cache_node *n = get_node(cachep, node);\n\n\tif (ac->avail) {\n\t\traw_spin_lock(&n->list_lock);\n\t\t \n\t\tif (n->shared)\n\t\t\ttransfer_objects(n->shared, ac, ac->limit);\n\n\t\tfree_block(cachep, ac->entry, ac->avail, node, list);\n\t\tac->avail = 0;\n\t\traw_spin_unlock(&n->list_lock);\n\t}\n}\n\n \nstatic void reap_alien(struct kmem_cache *cachep, struct kmem_cache_node *n)\n{\n\tint node = __this_cpu_read(slab_reap_node);\n\n\tif (n->alien) {\n\t\tstruct alien_cache *alc = n->alien[node];\n\t\tstruct array_cache *ac;\n\n\t\tif (alc) {\n\t\t\tac = &alc->ac;\n\t\t\tif (ac->avail && spin_trylock_irq(&alc->lock)) {\n\t\t\t\tLIST_HEAD(list);\n\n\t\t\t\t__drain_alien_cache(cachep, ac, node, &list);\n\t\t\t\tspin_unlock_irq(&alc->lock);\n\t\t\t\tslabs_destroy(cachep, &list);\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic void drain_alien_cache(struct kmem_cache *cachep,\n\t\t\t\tstruct alien_cache **alien)\n{\n\tint i = 0;\n\tstruct alien_cache *alc;\n\tstruct array_cache *ac;\n\tunsigned long flags;\n\n\tfor_each_online_node(i) {\n\t\talc = alien[i];\n\t\tif (alc) {\n\t\t\tLIST_HEAD(list);\n\n\t\t\tac = &alc->ac;\n\t\t\tspin_lock_irqsave(&alc->lock, flags);\n\t\t\t__drain_alien_cache(cachep, ac, i, &list);\n\t\t\tspin_unlock_irqrestore(&alc->lock, flags);\n\t\t\tslabs_destroy(cachep, &list);\n\t\t}\n\t}\n}\n\nstatic int __cache_free_alien(struct kmem_cache *cachep, void *objp,\n\t\t\t\tint node, int slab_node)\n{\n\tstruct kmem_cache_node *n;\n\tstruct alien_cache *alien = NULL;\n\tstruct array_cache *ac;\n\tLIST_HEAD(list);\n\n\tn = get_node(cachep, node);\n\tSTATS_INC_NODEFREES(cachep);\n\tif (n->alien && n->alien[slab_node]) {\n\t\talien = n->alien[slab_node];\n\t\tac = &alien->ac;\n\t\tspin_lock(&alien->lock);\n\t\tif (unlikely(ac->avail == ac->limit)) {\n\t\t\tSTATS_INC_ACOVERFLOW(cachep);\n\t\t\t__drain_alien_cache(cachep, ac, slab_node, &list);\n\t\t}\n\t\t__free_one(ac, objp);\n\t\tspin_unlock(&alien->lock);\n\t\tslabs_destroy(cachep, &list);\n\t} else {\n\t\tn = get_node(cachep, slab_node);\n\t\traw_spin_lock(&n->list_lock);\n\t\tfree_block(cachep, &objp, 1, slab_node, &list);\n\t\traw_spin_unlock(&n->list_lock);\n\t\tslabs_destroy(cachep, &list);\n\t}\n\treturn 1;\n}\n\nstatic inline int cache_free_alien(struct kmem_cache *cachep, void *objp)\n{\n\tint slab_node = slab_nid(virt_to_slab(objp));\n\tint node = numa_mem_id();\n\t \n\tif (likely(node == slab_node))\n\t\treturn 0;\n\n\treturn __cache_free_alien(cachep, objp, node, slab_node);\n}\n\n \nstatic inline gfp_t gfp_exact_node(gfp_t flags)\n{\n\treturn (flags | __GFP_THISNODE | __GFP_NOWARN) & ~(__GFP_RECLAIM|__GFP_NOFAIL);\n}\n#endif\n\nstatic int init_cache_node(struct kmem_cache *cachep, int node, gfp_t gfp)\n{\n\tstruct kmem_cache_node *n;\n\n\t \n\tn = get_node(cachep, node);\n\tif (n) {\n\t\traw_spin_lock_irq(&n->list_lock);\n\t\tn->free_limit = (1 + nr_cpus_node(node)) * cachep->batchcount +\n\t\t\t\tcachep->num;\n\t\traw_spin_unlock_irq(&n->list_lock);\n\n\t\treturn 0;\n\t}\n\n\tn = kmalloc_node(sizeof(struct kmem_cache_node), gfp, node);\n\tif (!n)\n\t\treturn -ENOMEM;\n\n\tkmem_cache_node_init(n);\n\tn->next_reap = jiffies + REAPTIMEOUT_NODE +\n\t\t    ((unsigned long)cachep) % REAPTIMEOUT_NODE;\n\n\tn->free_limit =\n\t\t(1 + nr_cpus_node(node)) * cachep->batchcount + cachep->num;\n\n\t \n\tcachep->node[node] = n;\n\n\treturn 0;\n}\n\n#if defined(CONFIG_NUMA) || defined(CONFIG_SMP)\n \nstatic int init_cache_node_node(int node)\n{\n\tint ret;\n\tstruct kmem_cache *cachep;\n\n\tlist_for_each_entry(cachep, &slab_caches, list) {\n\t\tret = init_cache_node(cachep, node, GFP_KERNEL);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n#endif\n\nstatic int setup_kmem_cache_node(struct kmem_cache *cachep,\n\t\t\t\tint node, gfp_t gfp, bool force_change)\n{\n\tint ret = -ENOMEM;\n\tstruct kmem_cache_node *n;\n\tstruct array_cache *old_shared = NULL;\n\tstruct array_cache *new_shared = NULL;\n\tstruct alien_cache **new_alien = NULL;\n\tLIST_HEAD(list);\n\n\tif (use_alien_caches) {\n\t\tnew_alien = alloc_alien_cache(node, cachep->limit, gfp);\n\t\tif (!new_alien)\n\t\t\tgoto fail;\n\t}\n\n\tif (cachep->shared) {\n\t\tnew_shared = alloc_arraycache(node,\n\t\t\tcachep->shared * cachep->batchcount, 0xbaadf00d, gfp);\n\t\tif (!new_shared)\n\t\t\tgoto fail;\n\t}\n\n\tret = init_cache_node(cachep, node, gfp);\n\tif (ret)\n\t\tgoto fail;\n\n\tn = get_node(cachep, node);\n\traw_spin_lock_irq(&n->list_lock);\n\tif (n->shared && force_change) {\n\t\tfree_block(cachep, n->shared->entry,\n\t\t\t\tn->shared->avail, node, &list);\n\t\tn->shared->avail = 0;\n\t}\n\n\tif (!n->shared || force_change) {\n\t\told_shared = n->shared;\n\t\tn->shared = new_shared;\n\t\tnew_shared = NULL;\n\t}\n\n\tif (!n->alien) {\n\t\tn->alien = new_alien;\n\t\tnew_alien = NULL;\n\t}\n\n\traw_spin_unlock_irq(&n->list_lock);\n\tslabs_destroy(cachep, &list);\n\n\t \n\tif (old_shared && force_change)\n\t\tsynchronize_rcu();\n\nfail:\n\tkfree(old_shared);\n\tkfree(new_shared);\n\tfree_alien_cache(new_alien);\n\n\treturn ret;\n}\n\n#ifdef CONFIG_SMP\n\nstatic void cpuup_canceled(long cpu)\n{\n\tstruct kmem_cache *cachep;\n\tstruct kmem_cache_node *n = NULL;\n\tint node = cpu_to_mem(cpu);\n\tconst struct cpumask *mask = cpumask_of_node(node);\n\n\tlist_for_each_entry(cachep, &slab_caches, list) {\n\t\tstruct array_cache *nc;\n\t\tstruct array_cache *shared;\n\t\tstruct alien_cache **alien;\n\t\tLIST_HEAD(list);\n\n\t\tn = get_node(cachep, node);\n\t\tif (!n)\n\t\t\tcontinue;\n\n\t\traw_spin_lock_irq(&n->list_lock);\n\n\t\t \n\t\tn->free_limit -= cachep->batchcount;\n\n\t\t \n\t\tnc = per_cpu_ptr(cachep->cpu_cache, cpu);\n\t\tfree_block(cachep, nc->entry, nc->avail, node, &list);\n\t\tnc->avail = 0;\n\n\t\tif (!cpumask_empty(mask)) {\n\t\t\traw_spin_unlock_irq(&n->list_lock);\n\t\t\tgoto free_slab;\n\t\t}\n\n\t\tshared = n->shared;\n\t\tif (shared) {\n\t\t\tfree_block(cachep, shared->entry,\n\t\t\t\t   shared->avail, node, &list);\n\t\t\tn->shared = NULL;\n\t\t}\n\n\t\talien = n->alien;\n\t\tn->alien = NULL;\n\n\t\traw_spin_unlock_irq(&n->list_lock);\n\n\t\tkfree(shared);\n\t\tif (alien) {\n\t\t\tdrain_alien_cache(cachep, alien);\n\t\t\tfree_alien_cache(alien);\n\t\t}\n\nfree_slab:\n\t\tslabs_destroy(cachep, &list);\n\t}\n\t \n\tlist_for_each_entry(cachep, &slab_caches, list) {\n\t\tn = get_node(cachep, node);\n\t\tif (!n)\n\t\t\tcontinue;\n\t\tdrain_freelist(cachep, n, INT_MAX);\n\t}\n}\n\nstatic int cpuup_prepare(long cpu)\n{\n\tstruct kmem_cache *cachep;\n\tint node = cpu_to_mem(cpu);\n\tint err;\n\n\t \n\terr = init_cache_node_node(node);\n\tif (err < 0)\n\t\tgoto bad;\n\n\t \n\tlist_for_each_entry(cachep, &slab_caches, list) {\n\t\terr = setup_kmem_cache_node(cachep, node, GFP_KERNEL, false);\n\t\tif (err)\n\t\t\tgoto bad;\n\t}\n\n\treturn 0;\nbad:\n\tcpuup_canceled(cpu);\n\treturn -ENOMEM;\n}\n\nint slab_prepare_cpu(unsigned int cpu)\n{\n\tint err;\n\n\tmutex_lock(&slab_mutex);\n\terr = cpuup_prepare(cpu);\n\tmutex_unlock(&slab_mutex);\n\treturn err;\n}\n\n \nint slab_dead_cpu(unsigned int cpu)\n{\n\tmutex_lock(&slab_mutex);\n\tcpuup_canceled(cpu);\n\tmutex_unlock(&slab_mutex);\n\treturn 0;\n}\n#endif\n\nstatic int slab_online_cpu(unsigned int cpu)\n{\n\tstart_cpu_timer(cpu);\n\treturn 0;\n}\n\nstatic int slab_offline_cpu(unsigned int cpu)\n{\n\t \n\tcancel_delayed_work_sync(&per_cpu(slab_reap_work, cpu));\n\t \n\tper_cpu(slab_reap_work, cpu).work.func = NULL;\n\treturn 0;\n}\n\n#if defined(CONFIG_NUMA)\n \nstatic int __meminit drain_cache_node_node(int node)\n{\n\tstruct kmem_cache *cachep;\n\tint ret = 0;\n\n\tlist_for_each_entry(cachep, &slab_caches, list) {\n\t\tstruct kmem_cache_node *n;\n\n\t\tn = get_node(cachep, node);\n\t\tif (!n)\n\t\t\tcontinue;\n\n\t\tdrain_freelist(cachep, n, INT_MAX);\n\n\t\tif (!list_empty(&n->slabs_full) ||\n\t\t    !list_empty(&n->slabs_partial)) {\n\t\t\tret = -EBUSY;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn ret;\n}\n\nstatic int __meminit slab_memory_callback(struct notifier_block *self,\n\t\t\t\t\tunsigned long action, void *arg)\n{\n\tstruct memory_notify *mnb = arg;\n\tint ret = 0;\n\tint nid;\n\n\tnid = mnb->status_change_nid;\n\tif (nid < 0)\n\t\tgoto out;\n\n\tswitch (action) {\n\tcase MEM_GOING_ONLINE:\n\t\tmutex_lock(&slab_mutex);\n\t\tret = init_cache_node_node(nid);\n\t\tmutex_unlock(&slab_mutex);\n\t\tbreak;\n\tcase MEM_GOING_OFFLINE:\n\t\tmutex_lock(&slab_mutex);\n\t\tret = drain_cache_node_node(nid);\n\t\tmutex_unlock(&slab_mutex);\n\t\tbreak;\n\tcase MEM_ONLINE:\n\tcase MEM_OFFLINE:\n\tcase MEM_CANCEL_ONLINE:\n\tcase MEM_CANCEL_OFFLINE:\n\t\tbreak;\n\t}\nout:\n\treturn notifier_from_errno(ret);\n}\n#endif  \n\n \nstatic void __init init_list(struct kmem_cache *cachep, struct kmem_cache_node *list,\n\t\t\t\tint nodeid)\n{\n\tstruct kmem_cache_node *ptr;\n\n\tptr = kmalloc_node(sizeof(struct kmem_cache_node), GFP_NOWAIT, nodeid);\n\tBUG_ON(!ptr);\n\n\tmemcpy(ptr, list, sizeof(struct kmem_cache_node));\n\t \n\traw_spin_lock_init(&ptr->list_lock);\n\n\tMAKE_ALL_LISTS(cachep, ptr, nodeid);\n\tcachep->node[nodeid] = ptr;\n}\n\n \nstatic void __init set_up_node(struct kmem_cache *cachep, int index)\n{\n\tint node;\n\n\tfor_each_online_node(node) {\n\t\tcachep->node[node] = &init_kmem_cache_node[index + node];\n\t\tcachep->node[node]->next_reap = jiffies +\n\t\t    REAPTIMEOUT_NODE +\n\t\t    ((unsigned long)cachep) % REAPTIMEOUT_NODE;\n\t}\n}\n\n \nvoid __init kmem_cache_init(void)\n{\n\tint i;\n\n\tkmem_cache = &kmem_cache_boot;\n\n\tif (!IS_ENABLED(CONFIG_NUMA) || num_possible_nodes() == 1)\n\t\tuse_alien_caches = 0;\n\n\tfor (i = 0; i < NUM_INIT_LISTS; i++)\n\t\tkmem_cache_node_init(&init_kmem_cache_node[i]);\n\n\t \n\tif (!slab_max_order_set && totalram_pages() > (32 << 20) >> PAGE_SHIFT)\n\t\tslab_max_order = SLAB_MAX_ORDER_HI;\n\n\t \n\n\t \n\n\t \n\tcreate_boot_cache(kmem_cache, \"kmem_cache\",\n\t\toffsetof(struct kmem_cache, node) +\n\t\t\t\t  nr_node_ids * sizeof(struct kmem_cache_node *),\n\t\t\t\t  SLAB_HWCACHE_ALIGN, 0, 0);\n\tlist_add(&kmem_cache->list, &slab_caches);\n\tslab_state = PARTIAL;\n\n\t \n\tnew_kmalloc_cache(INDEX_NODE, KMALLOC_NORMAL, ARCH_KMALLOC_FLAGS);\n\tslab_state = PARTIAL_NODE;\n\tsetup_kmalloc_cache_index_table();\n\n\t \n\t{\n\t\tint nid;\n\n\t\tfor_each_online_node(nid) {\n\t\t\tinit_list(kmem_cache, &init_kmem_cache_node[CACHE_CACHE + nid], nid);\n\n\t\t\tinit_list(kmalloc_caches[KMALLOC_NORMAL][INDEX_NODE],\n\t\t\t\t\t  &init_kmem_cache_node[SIZE_NODE + nid], nid);\n\t\t}\n\t}\n\n\tcreate_kmalloc_caches(ARCH_KMALLOC_FLAGS);\n}\n\nvoid __init kmem_cache_init_late(void)\n{\n\tstruct kmem_cache *cachep;\n\n\t \n\tmutex_lock(&slab_mutex);\n\tlist_for_each_entry(cachep, &slab_caches, list)\n\t\tif (enable_cpucache(cachep, GFP_NOWAIT))\n\t\t\tBUG();\n\tmutex_unlock(&slab_mutex);\n\n\t \n\tslab_state = FULL;\n\n#ifdef CONFIG_NUMA\n\t \n\thotplug_memory_notifier(slab_memory_callback, SLAB_CALLBACK_PRI);\n#endif\n\n\t \n}\n\nstatic int __init cpucache_init(void)\n{\n\tint ret;\n\n\t \n\tret = cpuhp_setup_state(CPUHP_AP_ONLINE_DYN, \"SLAB online\",\n\t\t\t\tslab_online_cpu, slab_offline_cpu);\n\tWARN_ON(ret < 0);\n\n\treturn 0;\n}\n__initcall(cpucache_init);\n\nstatic noinline void\nslab_out_of_memory(struct kmem_cache *cachep, gfp_t gfpflags, int nodeid)\n{\n#if DEBUG\n\tstruct kmem_cache_node *n;\n\tunsigned long flags;\n\tint node;\n\tstatic DEFINE_RATELIMIT_STATE(slab_oom_rs, DEFAULT_RATELIMIT_INTERVAL,\n\t\t\t\t      DEFAULT_RATELIMIT_BURST);\n\n\tif ((gfpflags & __GFP_NOWARN) || !__ratelimit(&slab_oom_rs))\n\t\treturn;\n\n\tpr_warn(\"SLAB: Unable to allocate memory on node %d, gfp=%#x(%pGg)\\n\",\n\t\tnodeid, gfpflags, &gfpflags);\n\tpr_warn(\"  cache: %s, object size: %d, order: %d\\n\",\n\t\tcachep->name, cachep->size, cachep->gfporder);\n\n\tfor_each_kmem_cache_node(cachep, node, n) {\n\t\tunsigned long total_slabs, free_slabs, free_objs;\n\n\t\traw_spin_lock_irqsave(&n->list_lock, flags);\n\t\ttotal_slabs = n->total_slabs;\n\t\tfree_slabs = n->free_slabs;\n\t\tfree_objs = n->free_objects;\n\t\traw_spin_unlock_irqrestore(&n->list_lock, flags);\n\n\t\tpr_warn(\"  node %d: slabs: %ld/%ld, objs: %ld/%ld\\n\",\n\t\t\tnode, total_slabs - free_slabs, total_slabs,\n\t\t\t(total_slabs * cachep->num) - free_objs,\n\t\t\ttotal_slabs * cachep->num);\n\t}\n#endif\n}\n\n \nstatic struct slab *kmem_getpages(struct kmem_cache *cachep, gfp_t flags,\n\t\t\t\t\t\t\t\tint nodeid)\n{\n\tstruct folio *folio;\n\tstruct slab *slab;\n\n\tflags |= cachep->allocflags;\n\n\tfolio = (struct folio *) __alloc_pages_node(nodeid, flags, cachep->gfporder);\n\tif (!folio) {\n\t\tslab_out_of_memory(cachep, flags, nodeid);\n\t\treturn NULL;\n\t}\n\n\tslab = folio_slab(folio);\n\n\taccount_slab(slab, cachep->gfporder, cachep, flags);\n\t__folio_set_slab(folio);\n\t \n\tsmp_wmb();\n\t \n\tif (sk_memalloc_socks() && folio_is_pfmemalloc(folio))\n\t\tslab_set_pfmemalloc(slab);\n\n\treturn slab;\n}\n\n \nstatic void kmem_freepages(struct kmem_cache *cachep, struct slab *slab)\n{\n\tint order = cachep->gfporder;\n\tstruct folio *folio = slab_folio(slab);\n\n\tBUG_ON(!folio_test_slab(folio));\n\t__slab_clear_pfmemalloc(slab);\n\tpage_mapcount_reset(&folio->page);\n\tfolio->mapping = NULL;\n\t \n\tsmp_wmb();\n\t__folio_clear_slab(folio);\n\n\tmm_account_reclaimed_pages(1 << order);\n\tunaccount_slab(slab, order, cachep);\n\t__free_pages(&folio->page, order);\n}\n\nstatic void kmem_rcu_free(struct rcu_head *head)\n{\n\tstruct kmem_cache *cachep;\n\tstruct slab *slab;\n\n\tslab = container_of(head, struct slab, rcu_head);\n\tcachep = slab->slab_cache;\n\n\tkmem_freepages(cachep, slab);\n}\n\n#if DEBUG\nstatic inline bool is_debug_pagealloc_cache(struct kmem_cache *cachep)\n{\n\treturn debug_pagealloc_enabled_static() && OFF_SLAB(cachep) &&\n\t\t\t((cachep->size % PAGE_SIZE) == 0);\n}\n\n#ifdef CONFIG_DEBUG_PAGEALLOC\nstatic void slab_kernel_map(struct kmem_cache *cachep, void *objp, int map)\n{\n\tif (!is_debug_pagealloc_cache(cachep))\n\t\treturn;\n\n\t__kernel_map_pages(virt_to_page(objp), cachep->size / PAGE_SIZE, map);\n}\n\n#else\nstatic inline void slab_kernel_map(struct kmem_cache *cachep, void *objp,\n\t\t\t\tint map) {}\n\n#endif\n\nstatic void poison_obj(struct kmem_cache *cachep, void *addr, unsigned char val)\n{\n\tint size = cachep->object_size;\n\taddr = &((char *)addr)[obj_offset(cachep)];\n\n\tmemset(addr, val, size);\n\t*(unsigned char *)(addr + size - 1) = POISON_END;\n}\n\nstatic void dump_line(char *data, int offset, int limit)\n{\n\tint i;\n\tunsigned char error = 0;\n\tint bad_count = 0;\n\n\tpr_err(\"%03x: \", offset);\n\tfor (i = 0; i < limit; i++) {\n\t\tif (data[offset + i] != POISON_FREE) {\n\t\t\terror = data[offset + i];\n\t\t\tbad_count++;\n\t\t}\n\t}\n\tprint_hex_dump(KERN_CONT, \"\", 0, 16, 1,\n\t\t\t&data[offset], limit, 1);\n\n\tif (bad_count == 1) {\n\t\terror ^= POISON_FREE;\n\t\tif (!(error & (error - 1))) {\n\t\t\tpr_err(\"Single bit error detected. Probably bad RAM.\\n\");\n#ifdef CONFIG_X86\n\t\t\tpr_err(\"Run memtest86+ or a similar memory test tool.\\n\");\n#else\n\t\t\tpr_err(\"Run a memory test tool.\\n\");\n#endif\n\t\t}\n\t}\n}\n#endif\n\n#if DEBUG\n\nstatic void print_objinfo(struct kmem_cache *cachep, void *objp, int lines)\n{\n\tint i, size;\n\tchar *realobj;\n\n\tif (cachep->flags & SLAB_RED_ZONE) {\n\t\tpr_err(\"Redzone: 0x%llx/0x%llx\\n\",\n\t\t       *dbg_redzone1(cachep, objp),\n\t\t       *dbg_redzone2(cachep, objp));\n\t}\n\n\tif (cachep->flags & SLAB_STORE_USER)\n\t\tpr_err(\"Last user: (%pSR)\\n\", *dbg_userword(cachep, objp));\n\trealobj = (char *)objp + obj_offset(cachep);\n\tsize = cachep->object_size;\n\tfor (i = 0; i < size && lines; i += 16, lines--) {\n\t\tint limit;\n\t\tlimit = 16;\n\t\tif (i + limit > size)\n\t\t\tlimit = size - i;\n\t\tdump_line(realobj, i, limit);\n\t}\n}\n\nstatic void check_poison_obj(struct kmem_cache *cachep, void *objp)\n{\n\tchar *realobj;\n\tint size, i;\n\tint lines = 0;\n\n\tif (is_debug_pagealloc_cache(cachep))\n\t\treturn;\n\n\trealobj = (char *)objp + obj_offset(cachep);\n\tsize = cachep->object_size;\n\n\tfor (i = 0; i < size; i++) {\n\t\tchar exp = POISON_FREE;\n\t\tif (i == size - 1)\n\t\t\texp = POISON_END;\n\t\tif (realobj[i] != exp) {\n\t\t\tint limit;\n\t\t\t \n\t\t\t \n\t\t\tif (lines == 0) {\n\t\t\t\tpr_err(\"Slab corruption (%s): %s start=%px, len=%d\\n\",\n\t\t\t\t       print_tainted(), cachep->name,\n\t\t\t\t       realobj, size);\n\t\t\t\tprint_objinfo(cachep, objp, 0);\n\t\t\t}\n\t\t\t \n\t\t\ti = (i / 16) * 16;\n\t\t\tlimit = 16;\n\t\t\tif (i + limit > size)\n\t\t\t\tlimit = size - i;\n\t\t\tdump_line(realobj, i, limit);\n\t\t\ti += 16;\n\t\t\tlines++;\n\t\t\t \n\t\t\tif (lines > 5)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tif (lines != 0) {\n\t\t \n\t\tstruct slab *slab = virt_to_slab(objp);\n\t\tunsigned int objnr;\n\n\t\tobjnr = obj_to_index(cachep, slab, objp);\n\t\tif (objnr) {\n\t\t\tobjp = index_to_obj(cachep, slab, objnr - 1);\n\t\t\trealobj = (char *)objp + obj_offset(cachep);\n\t\t\tpr_err(\"Prev obj: start=%px, len=%d\\n\", realobj, size);\n\t\t\tprint_objinfo(cachep, objp, 2);\n\t\t}\n\t\tif (objnr + 1 < cachep->num) {\n\t\t\tobjp = index_to_obj(cachep, slab, objnr + 1);\n\t\t\trealobj = (char *)objp + obj_offset(cachep);\n\t\t\tpr_err(\"Next obj: start=%px, len=%d\\n\", realobj, size);\n\t\t\tprint_objinfo(cachep, objp, 2);\n\t\t}\n\t}\n}\n#endif\n\n#if DEBUG\nstatic void slab_destroy_debugcheck(struct kmem_cache *cachep,\n\t\t\t\t\t\tstruct slab *slab)\n{\n\tint i;\n\n\tif (OBJFREELIST_SLAB(cachep) && cachep->flags & SLAB_POISON) {\n\t\tpoison_obj(cachep, slab->freelist - obj_offset(cachep),\n\t\t\tPOISON_FREE);\n\t}\n\n\tfor (i = 0; i < cachep->num; i++) {\n\t\tvoid *objp = index_to_obj(cachep, slab, i);\n\n\t\tif (cachep->flags & SLAB_POISON) {\n\t\t\tcheck_poison_obj(cachep, objp);\n\t\t\tslab_kernel_map(cachep, objp, 1);\n\t\t}\n\t\tif (cachep->flags & SLAB_RED_ZONE) {\n\t\t\tif (*dbg_redzone1(cachep, objp) != RED_INACTIVE)\n\t\t\t\tslab_error(cachep, \"start of a freed object was overwritten\");\n\t\t\tif (*dbg_redzone2(cachep, objp) != RED_INACTIVE)\n\t\t\t\tslab_error(cachep, \"end of a freed object was overwritten\");\n\t\t}\n\t}\n}\n#else\nstatic void slab_destroy_debugcheck(struct kmem_cache *cachep,\n\t\t\t\t\t\tstruct slab *slab)\n{\n}\n#endif\n\n \nstatic void slab_destroy(struct kmem_cache *cachep, struct slab *slab)\n{\n\tvoid *freelist;\n\n\tfreelist = slab->freelist;\n\tslab_destroy_debugcheck(cachep, slab);\n\tif (unlikely(cachep->flags & SLAB_TYPESAFE_BY_RCU))\n\t\tcall_rcu(&slab->rcu_head, kmem_rcu_free);\n\telse\n\t\tkmem_freepages(cachep, slab);\n\n\t \n\tif (OFF_SLAB(cachep))\n\t\tkfree(freelist);\n}\n\n \nstatic void slabs_destroy(struct kmem_cache *cachep, struct list_head *list)\n{\n\tstruct slab *slab, *n;\n\n\tlist_for_each_entry_safe(slab, n, list, slab_list) {\n\t\tlist_del(&slab->slab_list);\n\t\tslab_destroy(cachep, slab);\n\t}\n}\n\n \nstatic size_t calculate_slab_order(struct kmem_cache *cachep,\n\t\t\t\tsize_t size, slab_flags_t flags)\n{\n\tsize_t left_over = 0;\n\tint gfporder;\n\n\tfor (gfporder = 0; gfporder <= KMALLOC_MAX_ORDER; gfporder++) {\n\t\tunsigned int num;\n\t\tsize_t remainder;\n\n\t\tnum = cache_estimate(gfporder, size, flags, &remainder);\n\t\tif (!num)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (num > SLAB_OBJ_MAX_NUM)\n\t\t\tbreak;\n\n\t\tif (flags & CFLGS_OFF_SLAB) {\n\t\t\tstruct kmem_cache *freelist_cache;\n\t\t\tsize_t freelist_size;\n\t\t\tsize_t freelist_cache_size;\n\n\t\t\tfreelist_size = num * sizeof(freelist_idx_t);\n\t\t\tif (freelist_size > KMALLOC_MAX_CACHE_SIZE) {\n\t\t\t\tfreelist_cache_size = PAGE_SIZE << get_order(freelist_size);\n\t\t\t} else {\n\t\t\t\tfreelist_cache = kmalloc_slab(freelist_size, 0u, _RET_IP_);\n\t\t\t\tif (!freelist_cache)\n\t\t\t\t\tcontinue;\n\t\t\t\tfreelist_cache_size = freelist_cache->size;\n\n\t\t\t\t \n\t\t\t\tif (OFF_SLAB(freelist_cache))\n\t\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (freelist_cache_size > cachep->size / 2)\n\t\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tcachep->num = num;\n\t\tcachep->gfporder = gfporder;\n\t\tleft_over = remainder;\n\n\t\t \n\t\tif (flags & SLAB_RECLAIM_ACCOUNT)\n\t\t\tbreak;\n\n\t\t \n\t\tif (gfporder >= slab_max_order)\n\t\t\tbreak;\n\n\t\t \n\t\tif (left_over * 8 <= (PAGE_SIZE << gfporder))\n\t\t\tbreak;\n\t}\n\treturn left_over;\n}\n\nstatic struct array_cache __percpu *alloc_kmem_cache_cpus(\n\t\tstruct kmem_cache *cachep, int entries, int batchcount)\n{\n\tint cpu;\n\tsize_t size;\n\tstruct array_cache __percpu *cpu_cache;\n\n\tsize = sizeof(void *) * entries + sizeof(struct array_cache);\n\tcpu_cache = __alloc_percpu(size, sizeof(void *));\n\n\tif (!cpu_cache)\n\t\treturn NULL;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tinit_arraycache(per_cpu_ptr(cpu_cache, cpu),\n\t\t\t\tentries, batchcount);\n\t}\n\n\treturn cpu_cache;\n}\n\nstatic int __ref setup_cpu_cache(struct kmem_cache *cachep, gfp_t gfp)\n{\n\tif (slab_state >= FULL)\n\t\treturn enable_cpucache(cachep, gfp);\n\n\tcachep->cpu_cache = alloc_kmem_cache_cpus(cachep, 1, 1);\n\tif (!cachep->cpu_cache)\n\t\treturn 1;\n\n\tif (slab_state == DOWN) {\n\t\t \n\t\tset_up_node(kmem_cache, CACHE_CACHE);\n\t} else if (slab_state == PARTIAL) {\n\t\t \n\t\tset_up_node(cachep, SIZE_NODE);\n\t} else {\n\t\tint node;\n\n\t\tfor_each_online_node(node) {\n\t\t\tcachep->node[node] = kmalloc_node(\n\t\t\t\tsizeof(struct kmem_cache_node), gfp, node);\n\t\t\tBUG_ON(!cachep->node[node]);\n\t\t\tkmem_cache_node_init(cachep->node[node]);\n\t\t}\n\t}\n\n\tcachep->node[numa_mem_id()]->next_reap =\n\t\t\tjiffies + REAPTIMEOUT_NODE +\n\t\t\t((unsigned long)cachep) % REAPTIMEOUT_NODE;\n\n\tcpu_cache_get(cachep)->avail = 0;\n\tcpu_cache_get(cachep)->limit = BOOT_CPUCACHE_ENTRIES;\n\tcpu_cache_get(cachep)->batchcount = 1;\n\tcpu_cache_get(cachep)->touched = 0;\n\tcachep->batchcount = 1;\n\tcachep->limit = BOOT_CPUCACHE_ENTRIES;\n\treturn 0;\n}\n\nslab_flags_t kmem_cache_flags(unsigned int object_size,\n\tslab_flags_t flags, const char *name)\n{\n\treturn flags;\n}\n\nstruct kmem_cache *\n__kmem_cache_alias(const char *name, unsigned int size, unsigned int align,\n\t\t   slab_flags_t flags, void (*ctor)(void *))\n{\n\tstruct kmem_cache *cachep;\n\n\tcachep = find_mergeable(size, align, flags, name, ctor);\n\tif (cachep) {\n\t\tcachep->refcount++;\n\n\t\t \n\t\tcachep->object_size = max_t(int, cachep->object_size, size);\n\t}\n\treturn cachep;\n}\n\nstatic bool set_objfreelist_slab_cache(struct kmem_cache *cachep,\n\t\t\tsize_t size, slab_flags_t flags)\n{\n\tsize_t left;\n\n\tcachep->num = 0;\n\n\t \n\tif (unlikely(slab_want_init_on_free(cachep)))\n\t\treturn false;\n\n\tif (cachep->ctor || flags & SLAB_TYPESAFE_BY_RCU)\n\t\treturn false;\n\n\tleft = calculate_slab_order(cachep, size,\n\t\t\tflags | CFLGS_OBJFREELIST_SLAB);\n\tif (!cachep->num)\n\t\treturn false;\n\n\tif (cachep->num * sizeof(freelist_idx_t) > cachep->object_size)\n\t\treturn false;\n\n\tcachep->colour = left / cachep->colour_off;\n\n\treturn true;\n}\n\nstatic bool set_off_slab_cache(struct kmem_cache *cachep,\n\t\t\tsize_t size, slab_flags_t flags)\n{\n\tsize_t left;\n\n\tcachep->num = 0;\n\n\t \n\tif (flags & SLAB_NOLEAKTRACE)\n\t\treturn false;\n\n\t \n\tleft = calculate_slab_order(cachep, size, flags | CFLGS_OFF_SLAB);\n\tif (!cachep->num)\n\t\treturn false;\n\n\t \n\tif (left >= cachep->num * sizeof(freelist_idx_t))\n\t\treturn false;\n\n\tcachep->colour = left / cachep->colour_off;\n\n\treturn true;\n}\n\nstatic bool set_on_slab_cache(struct kmem_cache *cachep,\n\t\t\tsize_t size, slab_flags_t flags)\n{\n\tsize_t left;\n\n\tcachep->num = 0;\n\n\tleft = calculate_slab_order(cachep, size, flags);\n\tif (!cachep->num)\n\t\treturn false;\n\n\tcachep->colour = left / cachep->colour_off;\n\n\treturn true;\n}\n\n \nint __kmem_cache_create(struct kmem_cache *cachep, slab_flags_t flags)\n{\n\tsize_t ralign = BYTES_PER_WORD;\n\tgfp_t gfp;\n\tint err;\n\tunsigned int size = cachep->size;\n\n#if DEBUG\n#if FORCED_DEBUG\n\t \n\tif (size < 4096 || fls(size - 1) == fls(size-1 + REDZONE_ALIGN +\n\t\t\t\t\t\t2 * sizeof(unsigned long long)))\n\t\tflags |= SLAB_RED_ZONE | SLAB_STORE_USER;\n\tif (!(flags & SLAB_TYPESAFE_BY_RCU))\n\t\tflags |= SLAB_POISON;\n#endif\n#endif\n\n\t \n\tsize = ALIGN(size, BYTES_PER_WORD);\n\n\tif (flags & SLAB_RED_ZONE) {\n\t\tralign = REDZONE_ALIGN;\n\t\t \n\t\tsize = ALIGN(size, REDZONE_ALIGN);\n\t}\n\n\t \n\tif (ralign < cachep->align) {\n\t\tralign = cachep->align;\n\t}\n\t \n\tif (ralign > __alignof__(unsigned long long))\n\t\tflags &= ~(SLAB_RED_ZONE | SLAB_STORE_USER);\n\t \n\tcachep->align = ralign;\n\tcachep->colour_off = cache_line_size();\n\t \n\tif (cachep->colour_off < cachep->align)\n\t\tcachep->colour_off = cachep->align;\n\n\tif (slab_is_available())\n\t\tgfp = GFP_KERNEL;\n\telse\n\t\tgfp = GFP_NOWAIT;\n\n#if DEBUG\n\n\t \n\tif (flags & SLAB_RED_ZONE) {\n\t\t \n\t\tcachep->obj_offset += sizeof(unsigned long long);\n\t\tsize += 2 * sizeof(unsigned long long);\n\t}\n\tif (flags & SLAB_STORE_USER) {\n\t\t \n\t\tif (flags & SLAB_RED_ZONE)\n\t\t\tsize += REDZONE_ALIGN;\n\t\telse\n\t\t\tsize += BYTES_PER_WORD;\n\t}\n#endif\n\n\tkasan_cache_create(cachep, &size, &flags);\n\n\tsize = ALIGN(size, cachep->align);\n\t \n\tif (FREELIST_BYTE_INDEX && size < SLAB_OBJ_MIN_SIZE)\n\t\tsize = ALIGN(SLAB_OBJ_MIN_SIZE, cachep->align);\n\n#if DEBUG\n\t \n\tif (debug_pagealloc_enabled_static() && (flags & SLAB_POISON) &&\n\t\tsize >= 256 && cachep->object_size > cache_line_size()) {\n\t\tif (size < PAGE_SIZE || size % PAGE_SIZE == 0) {\n\t\t\tsize_t tmp_size = ALIGN(size, PAGE_SIZE);\n\n\t\t\tif (set_off_slab_cache(cachep, tmp_size, flags)) {\n\t\t\t\tflags |= CFLGS_OFF_SLAB;\n\t\t\t\tcachep->obj_offset += tmp_size - size;\n\t\t\t\tsize = tmp_size;\n\t\t\t\tgoto done;\n\t\t\t}\n\t\t}\n\t}\n#endif\n\n\tif (set_objfreelist_slab_cache(cachep, size, flags)) {\n\t\tflags |= CFLGS_OBJFREELIST_SLAB;\n\t\tgoto done;\n\t}\n\n\tif (set_off_slab_cache(cachep, size, flags)) {\n\t\tflags |= CFLGS_OFF_SLAB;\n\t\tgoto done;\n\t}\n\n\tif (set_on_slab_cache(cachep, size, flags))\n\t\tgoto done;\n\n\treturn -E2BIG;\n\ndone:\n\tcachep->freelist_size = cachep->num * sizeof(freelist_idx_t);\n\tcachep->flags = flags;\n\tcachep->allocflags = __GFP_COMP;\n\tif (flags & SLAB_CACHE_DMA)\n\t\tcachep->allocflags |= GFP_DMA;\n\tif (flags & SLAB_CACHE_DMA32)\n\t\tcachep->allocflags |= GFP_DMA32;\n\tif (flags & SLAB_RECLAIM_ACCOUNT)\n\t\tcachep->allocflags |= __GFP_RECLAIMABLE;\n\tcachep->size = size;\n\tcachep->reciprocal_buffer_size = reciprocal_value(size);\n\n#if DEBUG\n\t \n\tif (IS_ENABLED(CONFIG_PAGE_POISONING) &&\n\t\t(cachep->flags & SLAB_POISON) &&\n\t\tis_debug_pagealloc_cache(cachep))\n\t\tcachep->flags &= ~(SLAB_RED_ZONE | SLAB_STORE_USER);\n#endif\n\n\terr = setup_cpu_cache(cachep, gfp);\n\tif (err) {\n\t\t__kmem_cache_release(cachep);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\n#if DEBUG\nstatic void check_irq_off(void)\n{\n\tBUG_ON(!irqs_disabled());\n}\n\nstatic void check_irq_on(void)\n{\n\tBUG_ON(irqs_disabled());\n}\n\nstatic void check_mutex_acquired(void)\n{\n\tBUG_ON(!mutex_is_locked(&slab_mutex));\n}\n\nstatic void check_spinlock_acquired(struct kmem_cache *cachep)\n{\n#ifdef CONFIG_SMP\n\tcheck_irq_off();\n\tassert_raw_spin_locked(&get_node(cachep, numa_mem_id())->list_lock);\n#endif\n}\n\nstatic void check_spinlock_acquired_node(struct kmem_cache *cachep, int node)\n{\n#ifdef CONFIG_SMP\n\tcheck_irq_off();\n\tassert_raw_spin_locked(&get_node(cachep, node)->list_lock);\n#endif\n}\n\n#else\n#define check_irq_off()\tdo { } while(0)\n#define check_irq_on()\tdo { } while(0)\n#define check_mutex_acquired()\tdo { } while(0)\n#define check_spinlock_acquired(x) do { } while(0)\n#define check_spinlock_acquired_node(x, y) do { } while(0)\n#endif\n\nstatic void drain_array_locked(struct kmem_cache *cachep, struct array_cache *ac,\n\t\t\t\tint node, bool free_all, struct list_head *list)\n{\n\tint tofree;\n\n\tif (!ac || !ac->avail)\n\t\treturn;\n\n\ttofree = free_all ? ac->avail : (ac->limit + 4) / 5;\n\tif (tofree > ac->avail)\n\t\ttofree = (ac->avail + 1) / 2;\n\n\tfree_block(cachep, ac->entry, tofree, node, list);\n\tac->avail -= tofree;\n\tmemmove(ac->entry, &(ac->entry[tofree]), sizeof(void *) * ac->avail);\n}\n\nstatic void do_drain(void *arg)\n{\n\tstruct kmem_cache *cachep = arg;\n\tstruct array_cache *ac;\n\tint node = numa_mem_id();\n\tstruct kmem_cache_node *n;\n\tLIST_HEAD(list);\n\n\tcheck_irq_off();\n\tac = cpu_cache_get(cachep);\n\tn = get_node(cachep, node);\n\traw_spin_lock(&n->list_lock);\n\tfree_block(cachep, ac->entry, ac->avail, node, &list);\n\traw_spin_unlock(&n->list_lock);\n\tac->avail = 0;\n\tslabs_destroy(cachep, &list);\n}\n\nstatic void drain_cpu_caches(struct kmem_cache *cachep)\n{\n\tstruct kmem_cache_node *n;\n\tint node;\n\tLIST_HEAD(list);\n\n\ton_each_cpu(do_drain, cachep, 1);\n\tcheck_irq_on();\n\tfor_each_kmem_cache_node(cachep, node, n)\n\t\tif (n->alien)\n\t\t\tdrain_alien_cache(cachep, n->alien);\n\n\tfor_each_kmem_cache_node(cachep, node, n) {\n\t\traw_spin_lock_irq(&n->list_lock);\n\t\tdrain_array_locked(cachep, n->shared, node, true, &list);\n\t\traw_spin_unlock_irq(&n->list_lock);\n\n\t\tslabs_destroy(cachep, &list);\n\t}\n}\n\n \nstatic int drain_freelist(struct kmem_cache *cache,\n\t\t\tstruct kmem_cache_node *n, int tofree)\n{\n\tstruct list_head *p;\n\tint nr_freed;\n\tstruct slab *slab;\n\n\tnr_freed = 0;\n\twhile (nr_freed < tofree && !list_empty(&n->slabs_free)) {\n\n\t\traw_spin_lock_irq(&n->list_lock);\n\t\tp = n->slabs_free.prev;\n\t\tif (p == &n->slabs_free) {\n\t\t\traw_spin_unlock_irq(&n->list_lock);\n\t\t\tgoto out;\n\t\t}\n\n\t\tslab = list_entry(p, struct slab, slab_list);\n\t\tlist_del(&slab->slab_list);\n\t\tn->free_slabs--;\n\t\tn->total_slabs--;\n\t\t \n\t\tn->free_objects -= cache->num;\n\t\traw_spin_unlock_irq(&n->list_lock);\n\t\tslab_destroy(cache, slab);\n\t\tnr_freed++;\n\n\t\tcond_resched();\n\t}\nout:\n\treturn nr_freed;\n}\n\nbool __kmem_cache_empty(struct kmem_cache *s)\n{\n\tint node;\n\tstruct kmem_cache_node *n;\n\n\tfor_each_kmem_cache_node(s, node, n)\n\t\tif (!list_empty(&n->slabs_full) ||\n\t\t    !list_empty(&n->slabs_partial))\n\t\t\treturn false;\n\treturn true;\n}\n\nint __kmem_cache_shrink(struct kmem_cache *cachep)\n{\n\tint ret = 0;\n\tint node;\n\tstruct kmem_cache_node *n;\n\n\tdrain_cpu_caches(cachep);\n\n\tcheck_irq_on();\n\tfor_each_kmem_cache_node(cachep, node, n) {\n\t\tdrain_freelist(cachep, n, INT_MAX);\n\n\t\tret += !list_empty(&n->slabs_full) ||\n\t\t\t!list_empty(&n->slabs_partial);\n\t}\n\treturn (ret ? 1 : 0);\n}\n\nint __kmem_cache_shutdown(struct kmem_cache *cachep)\n{\n\treturn __kmem_cache_shrink(cachep);\n}\n\nvoid __kmem_cache_release(struct kmem_cache *cachep)\n{\n\tint i;\n\tstruct kmem_cache_node *n;\n\n\tcache_random_seq_destroy(cachep);\n\n\tfree_percpu(cachep->cpu_cache);\n\n\t \n\tfor_each_kmem_cache_node(cachep, i, n) {\n\t\tkfree(n->shared);\n\t\tfree_alien_cache(n->alien);\n\t\tkfree(n);\n\t\tcachep->node[i] = NULL;\n\t}\n}\n\n \nstatic void *alloc_slabmgmt(struct kmem_cache *cachep,\n\t\t\t\t   struct slab *slab, int colour_off,\n\t\t\t\t   gfp_t local_flags, int nodeid)\n{\n\tvoid *freelist;\n\tvoid *addr = slab_address(slab);\n\n\tslab->s_mem = addr + colour_off;\n\tslab->active = 0;\n\n\tif (OBJFREELIST_SLAB(cachep))\n\t\tfreelist = NULL;\n\telse if (OFF_SLAB(cachep)) {\n\t\t \n\t\tfreelist = kmalloc_node(cachep->freelist_size,\n\t\t\t\t\t      local_flags, nodeid);\n\t} else {\n\t\t \n\t\tfreelist = addr + (PAGE_SIZE << cachep->gfporder) -\n\t\t\t\tcachep->freelist_size;\n\t}\n\n\treturn freelist;\n}\n\nstatic inline freelist_idx_t get_free_obj(struct slab *slab, unsigned int idx)\n{\n\treturn ((freelist_idx_t *) slab->freelist)[idx];\n}\n\nstatic inline void set_free_obj(struct slab *slab,\n\t\t\t\t\tunsigned int idx, freelist_idx_t val)\n{\n\t((freelist_idx_t *)(slab->freelist))[idx] = val;\n}\n\nstatic void cache_init_objs_debug(struct kmem_cache *cachep, struct slab *slab)\n{\n#if DEBUG\n\tint i;\n\n\tfor (i = 0; i < cachep->num; i++) {\n\t\tvoid *objp = index_to_obj(cachep, slab, i);\n\n\t\tif (cachep->flags & SLAB_STORE_USER)\n\t\t\t*dbg_userword(cachep, objp) = NULL;\n\n\t\tif (cachep->flags & SLAB_RED_ZONE) {\n\t\t\t*dbg_redzone1(cachep, objp) = RED_INACTIVE;\n\t\t\t*dbg_redzone2(cachep, objp) = RED_INACTIVE;\n\t\t}\n\t\t \n\t\tif (cachep->ctor && !(cachep->flags & SLAB_POISON)) {\n\t\t\tkasan_unpoison_object_data(cachep,\n\t\t\t\t\t\t   objp + obj_offset(cachep));\n\t\t\tcachep->ctor(objp + obj_offset(cachep));\n\t\t\tkasan_poison_object_data(\n\t\t\t\tcachep, objp + obj_offset(cachep));\n\t\t}\n\n\t\tif (cachep->flags & SLAB_RED_ZONE) {\n\t\t\tif (*dbg_redzone2(cachep, objp) != RED_INACTIVE)\n\t\t\t\tslab_error(cachep, \"constructor overwrote the end of an object\");\n\t\t\tif (*dbg_redzone1(cachep, objp) != RED_INACTIVE)\n\t\t\t\tslab_error(cachep, \"constructor overwrote the start of an object\");\n\t\t}\n\t\t \n\t\tif (cachep->flags & SLAB_POISON) {\n\t\t\tpoison_obj(cachep, objp, POISON_FREE);\n\t\t\tslab_kernel_map(cachep, objp, 0);\n\t\t}\n\t}\n#endif\n}\n\n#ifdef CONFIG_SLAB_FREELIST_RANDOM\n \nstruct freelist_init_state {\n\tunsigned int pos;\n\tunsigned int *list;\n\tunsigned int count;\n};\n\n \nstatic bool freelist_state_initialize(struct freelist_init_state *state,\n\t\t\t\tstruct kmem_cache *cachep,\n\t\t\t\tunsigned int count)\n{\n\tbool ret;\n\tif (!cachep->random_seq) {\n\t\tret = false;\n\t} else {\n\t\tstate->list = cachep->random_seq;\n\t\tstate->count = count;\n\t\tstate->pos = get_random_u32_below(count);\n\t\tret = true;\n\t}\n\treturn ret;\n}\n\n \nstatic freelist_idx_t next_random_slot(struct freelist_init_state *state)\n{\n\tif (state->pos >= state->count)\n\t\tstate->pos = 0;\n\treturn state->list[state->pos++];\n}\n\n \nstatic void swap_free_obj(struct slab *slab, unsigned int a, unsigned int b)\n{\n\tswap(((freelist_idx_t *) slab->freelist)[a],\n\t\t((freelist_idx_t *) slab->freelist)[b]);\n}\n\n \nstatic bool shuffle_freelist(struct kmem_cache *cachep, struct slab *slab)\n{\n\tunsigned int objfreelist = 0, i, rand, count = cachep->num;\n\tstruct freelist_init_state state;\n\tbool precomputed;\n\n\tif (count < 2)\n\t\treturn false;\n\n\tprecomputed = freelist_state_initialize(&state, cachep, count);\n\n\t \n\tif (OBJFREELIST_SLAB(cachep)) {\n\t\tif (!precomputed)\n\t\t\tobjfreelist = count - 1;\n\t\telse\n\t\t\tobjfreelist = next_random_slot(&state);\n\t\tslab->freelist = index_to_obj(cachep, slab, objfreelist) +\n\t\t\t\t\t\tobj_offset(cachep);\n\t\tcount--;\n\t}\n\n\t \n\tif (!precomputed) {\n\t\tfor (i = 0; i < count; i++)\n\t\t\tset_free_obj(slab, i, i);\n\n\t\t \n\t\tfor (i = count - 1; i > 0; i--) {\n\t\t\trand = get_random_u32_below(i + 1);\n\t\t\tswap_free_obj(slab, i, rand);\n\t\t}\n\t} else {\n\t\tfor (i = 0; i < count; i++)\n\t\t\tset_free_obj(slab, i, next_random_slot(&state));\n\t}\n\n\tif (OBJFREELIST_SLAB(cachep))\n\t\tset_free_obj(slab, cachep->num - 1, objfreelist);\n\n\treturn true;\n}\n#else\nstatic inline bool shuffle_freelist(struct kmem_cache *cachep,\n\t\t\t\tstruct slab *slab)\n{\n\treturn false;\n}\n#endif  \n\nstatic void cache_init_objs(struct kmem_cache *cachep,\n\t\t\t    struct slab *slab)\n{\n\tint i;\n\tvoid *objp;\n\tbool shuffled;\n\n\tcache_init_objs_debug(cachep, slab);\n\n\t \n\tshuffled = shuffle_freelist(cachep, slab);\n\n\tif (!shuffled && OBJFREELIST_SLAB(cachep)) {\n\t\tslab->freelist = index_to_obj(cachep, slab, cachep->num - 1) +\n\t\t\t\t\t\tobj_offset(cachep);\n\t}\n\n\tfor (i = 0; i < cachep->num; i++) {\n\t\tobjp = index_to_obj(cachep, slab, i);\n\t\tobjp = kasan_init_slab_obj(cachep, objp);\n\n\t\t \n\t\tif (DEBUG == 0 && cachep->ctor) {\n\t\t\tkasan_unpoison_object_data(cachep, objp);\n\t\t\tcachep->ctor(objp);\n\t\t\tkasan_poison_object_data(cachep, objp);\n\t\t}\n\n\t\tif (!shuffled)\n\t\t\tset_free_obj(slab, i, i);\n\t}\n}\n\nstatic void *slab_get_obj(struct kmem_cache *cachep, struct slab *slab)\n{\n\tvoid *objp;\n\n\tobjp = index_to_obj(cachep, slab, get_free_obj(slab, slab->active));\n\tslab->active++;\n\n\treturn objp;\n}\n\nstatic void slab_put_obj(struct kmem_cache *cachep,\n\t\t\tstruct slab *slab, void *objp)\n{\n\tunsigned int objnr = obj_to_index(cachep, slab, objp);\n#if DEBUG\n\tunsigned int i;\n\n\t \n\tfor (i = slab->active; i < cachep->num; i++) {\n\t\tif (get_free_obj(slab, i) == objnr) {\n\t\t\tpr_err(\"slab: double free detected in cache '%s', objp %px\\n\",\n\t\t\t       cachep->name, objp);\n\t\t\tBUG();\n\t\t}\n\t}\n#endif\n\tslab->active--;\n\tif (!slab->freelist)\n\t\tslab->freelist = objp + obj_offset(cachep);\n\n\tset_free_obj(slab, slab->active, objnr);\n}\n\n \nstatic struct slab *cache_grow_begin(struct kmem_cache *cachep,\n\t\t\t\tgfp_t flags, int nodeid)\n{\n\tvoid *freelist;\n\tsize_t offset;\n\tgfp_t local_flags;\n\tint slab_node;\n\tstruct kmem_cache_node *n;\n\tstruct slab *slab;\n\n\t \n\tif (unlikely(flags & GFP_SLAB_BUG_MASK))\n\t\tflags = kmalloc_fix_flags(flags);\n\n\tWARN_ON_ONCE(cachep->ctor && (flags & __GFP_ZERO));\n\tlocal_flags = flags & (GFP_CONSTRAINT_MASK|GFP_RECLAIM_MASK);\n\n\tcheck_irq_off();\n\tif (gfpflags_allow_blocking(local_flags))\n\t\tlocal_irq_enable();\n\n\t \n\tslab = kmem_getpages(cachep, local_flags, nodeid);\n\tif (!slab)\n\t\tgoto failed;\n\n\tslab_node = slab_nid(slab);\n\tn = get_node(cachep, slab_node);\n\n\t \n\tn->colour_next++;\n\tif (n->colour_next >= cachep->colour)\n\t\tn->colour_next = 0;\n\n\toffset = n->colour_next;\n\tif (offset >= cachep->colour)\n\t\toffset = 0;\n\n\toffset *= cachep->colour_off;\n\n\t \n\tkasan_poison_slab(slab);\n\n\t \n\tfreelist = alloc_slabmgmt(cachep, slab, offset,\n\t\t\tlocal_flags & ~GFP_CONSTRAINT_MASK, slab_node);\n\tif (OFF_SLAB(cachep) && !freelist)\n\t\tgoto opps1;\n\n\tslab->slab_cache = cachep;\n\tslab->freelist = freelist;\n\n\tcache_init_objs(cachep, slab);\n\n\tif (gfpflags_allow_blocking(local_flags))\n\t\tlocal_irq_disable();\n\n\treturn slab;\n\nopps1:\n\tkmem_freepages(cachep, slab);\nfailed:\n\tif (gfpflags_allow_blocking(local_flags))\n\t\tlocal_irq_disable();\n\treturn NULL;\n}\n\nstatic void cache_grow_end(struct kmem_cache *cachep, struct slab *slab)\n{\n\tstruct kmem_cache_node *n;\n\tvoid *list = NULL;\n\n\tcheck_irq_off();\n\n\tif (!slab)\n\t\treturn;\n\n\tINIT_LIST_HEAD(&slab->slab_list);\n\tn = get_node(cachep, slab_nid(slab));\n\n\traw_spin_lock(&n->list_lock);\n\tn->total_slabs++;\n\tif (!slab->active) {\n\t\tlist_add_tail(&slab->slab_list, &n->slabs_free);\n\t\tn->free_slabs++;\n\t} else\n\t\tfixup_slab_list(cachep, n, slab, &list);\n\n\tSTATS_INC_GROWN(cachep);\n\tn->free_objects += cachep->num - slab->active;\n\traw_spin_unlock(&n->list_lock);\n\n\tfixup_objfreelist_debug(cachep, &list);\n}\n\n#if DEBUG\n\n \nstatic void kfree_debugcheck(const void *objp)\n{\n\tif (!virt_addr_valid(objp)) {\n\t\tpr_err(\"kfree_debugcheck: out of range ptr %lxh\\n\",\n\t\t       (unsigned long)objp);\n\t\tBUG();\n\t}\n}\n\nstatic inline void verify_redzone_free(struct kmem_cache *cache, void *obj)\n{\n\tunsigned long long redzone1, redzone2;\n\n\tredzone1 = *dbg_redzone1(cache, obj);\n\tredzone2 = *dbg_redzone2(cache, obj);\n\n\t \n\tif (redzone1 == RED_ACTIVE && redzone2 == RED_ACTIVE)\n\t\treturn;\n\n\tif (redzone1 == RED_INACTIVE && redzone2 == RED_INACTIVE)\n\t\tslab_error(cache, \"double free detected\");\n\telse\n\t\tslab_error(cache, \"memory outside object was overwritten\");\n\n\tpr_err(\"%px: redzone 1:0x%llx, redzone 2:0x%llx\\n\",\n\t       obj, redzone1, redzone2);\n}\n\nstatic void *cache_free_debugcheck(struct kmem_cache *cachep, void *objp,\n\t\t\t\t   unsigned long caller)\n{\n\tunsigned int objnr;\n\tstruct slab *slab;\n\n\tBUG_ON(virt_to_cache(objp) != cachep);\n\n\tobjp -= obj_offset(cachep);\n\tkfree_debugcheck(objp);\n\tslab = virt_to_slab(objp);\n\n\tif (cachep->flags & SLAB_RED_ZONE) {\n\t\tverify_redzone_free(cachep, objp);\n\t\t*dbg_redzone1(cachep, objp) = RED_INACTIVE;\n\t\t*dbg_redzone2(cachep, objp) = RED_INACTIVE;\n\t}\n\tif (cachep->flags & SLAB_STORE_USER)\n\t\t*dbg_userword(cachep, objp) = (void *)caller;\n\n\tobjnr = obj_to_index(cachep, slab, objp);\n\n\tBUG_ON(objnr >= cachep->num);\n\tBUG_ON(objp != index_to_obj(cachep, slab, objnr));\n\n\tif (cachep->flags & SLAB_POISON) {\n\t\tpoison_obj(cachep, objp, POISON_FREE);\n\t\tslab_kernel_map(cachep, objp, 0);\n\t}\n\treturn objp;\n}\n\n#else\n#define kfree_debugcheck(x) do { } while(0)\n#define cache_free_debugcheck(x, objp, z) (objp)\n#endif\n\nstatic inline void fixup_objfreelist_debug(struct kmem_cache *cachep,\n\t\t\t\t\t\tvoid **list)\n{\n#if DEBUG\n\tvoid *next = *list;\n\tvoid *objp;\n\n\twhile (next) {\n\t\tobjp = next - obj_offset(cachep);\n\t\tnext = *(void **)next;\n\t\tpoison_obj(cachep, objp, POISON_FREE);\n\t}\n#endif\n}\n\nstatic inline void fixup_slab_list(struct kmem_cache *cachep,\n\t\t\t\tstruct kmem_cache_node *n, struct slab *slab,\n\t\t\t\tvoid **list)\n{\n\t \n\tlist_del(&slab->slab_list);\n\tif (slab->active == cachep->num) {\n\t\tlist_add(&slab->slab_list, &n->slabs_full);\n\t\tif (OBJFREELIST_SLAB(cachep)) {\n#if DEBUG\n\t\t\t \n\t\t\tif (cachep->flags & SLAB_POISON) {\n\t\t\t\tvoid **objp = slab->freelist;\n\n\t\t\t\t*objp = *list;\n\t\t\t\t*list = objp;\n\t\t\t}\n#endif\n\t\t\tslab->freelist = NULL;\n\t\t}\n\t} else\n\t\tlist_add(&slab->slab_list, &n->slabs_partial);\n}\n\n \nstatic noinline struct slab *get_valid_first_slab(struct kmem_cache_node *n,\n\t\t\t\t\tstruct slab *slab, bool pfmemalloc)\n{\n\tif (!slab)\n\t\treturn NULL;\n\n\tif (pfmemalloc)\n\t\treturn slab;\n\n\tif (!slab_test_pfmemalloc(slab))\n\t\treturn slab;\n\n\t \n\tif (n->free_objects > n->free_limit) {\n\t\tslab_clear_pfmemalloc(slab);\n\t\treturn slab;\n\t}\n\n\t \n\tlist_del(&slab->slab_list);\n\tif (!slab->active) {\n\t\tlist_add_tail(&slab->slab_list, &n->slabs_free);\n\t\tn->free_slabs++;\n\t} else\n\t\tlist_add_tail(&slab->slab_list, &n->slabs_partial);\n\n\tlist_for_each_entry(slab, &n->slabs_partial, slab_list) {\n\t\tif (!slab_test_pfmemalloc(slab))\n\t\t\treturn slab;\n\t}\n\n\tn->free_touched = 1;\n\tlist_for_each_entry(slab, &n->slabs_free, slab_list) {\n\t\tif (!slab_test_pfmemalloc(slab)) {\n\t\t\tn->free_slabs--;\n\t\t\treturn slab;\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\nstatic struct slab *get_first_slab(struct kmem_cache_node *n, bool pfmemalloc)\n{\n\tstruct slab *slab;\n\n\tassert_raw_spin_locked(&n->list_lock);\n\tslab = list_first_entry_or_null(&n->slabs_partial, struct slab,\n\t\t\t\t\tslab_list);\n\tif (!slab) {\n\t\tn->free_touched = 1;\n\t\tslab = list_first_entry_or_null(&n->slabs_free, struct slab,\n\t\t\t\t\t\tslab_list);\n\t\tif (slab)\n\t\t\tn->free_slabs--;\n\t}\n\n\tif (sk_memalloc_socks())\n\t\tslab = get_valid_first_slab(n, slab, pfmemalloc);\n\n\treturn slab;\n}\n\nstatic noinline void *cache_alloc_pfmemalloc(struct kmem_cache *cachep,\n\t\t\t\tstruct kmem_cache_node *n, gfp_t flags)\n{\n\tstruct slab *slab;\n\tvoid *obj;\n\tvoid *list = NULL;\n\n\tif (!gfp_pfmemalloc_allowed(flags))\n\t\treturn NULL;\n\n\traw_spin_lock(&n->list_lock);\n\tslab = get_first_slab(n, true);\n\tif (!slab) {\n\t\traw_spin_unlock(&n->list_lock);\n\t\treturn NULL;\n\t}\n\n\tobj = slab_get_obj(cachep, slab);\n\tn->free_objects--;\n\n\tfixup_slab_list(cachep, n, slab, &list);\n\n\traw_spin_unlock(&n->list_lock);\n\tfixup_objfreelist_debug(cachep, &list);\n\n\treturn obj;\n}\n\n \nstatic __always_inline int alloc_block(struct kmem_cache *cachep,\n\t\tstruct array_cache *ac, struct slab *slab, int batchcount)\n{\n\t \n\tBUG_ON(slab->active >= cachep->num);\n\n\twhile (slab->active < cachep->num && batchcount--) {\n\t\tSTATS_INC_ALLOCED(cachep);\n\t\tSTATS_INC_ACTIVE(cachep);\n\t\tSTATS_SET_HIGH(cachep);\n\n\t\tac->entry[ac->avail++] = slab_get_obj(cachep, slab);\n\t}\n\n\treturn batchcount;\n}\n\nstatic void *cache_alloc_refill(struct kmem_cache *cachep, gfp_t flags)\n{\n\tint batchcount;\n\tstruct kmem_cache_node *n;\n\tstruct array_cache *ac, *shared;\n\tint node;\n\tvoid *list = NULL;\n\tstruct slab *slab;\n\n\tcheck_irq_off();\n\tnode = numa_mem_id();\n\n\tac = cpu_cache_get(cachep);\n\tbatchcount = ac->batchcount;\n\tif (!ac->touched && batchcount > BATCHREFILL_LIMIT) {\n\t\t \n\t\tbatchcount = BATCHREFILL_LIMIT;\n\t}\n\tn = get_node(cachep, node);\n\n\tBUG_ON(ac->avail > 0 || !n);\n\tshared = READ_ONCE(n->shared);\n\tif (!n->free_objects && (!shared || !shared->avail))\n\t\tgoto direct_grow;\n\n\traw_spin_lock(&n->list_lock);\n\tshared = READ_ONCE(n->shared);\n\n\t \n\tif (shared && transfer_objects(ac, shared, batchcount)) {\n\t\tshared->touched = 1;\n\t\tgoto alloc_done;\n\t}\n\n\twhile (batchcount > 0) {\n\t\t \n\t\tslab = get_first_slab(n, false);\n\t\tif (!slab)\n\t\t\tgoto must_grow;\n\n\t\tcheck_spinlock_acquired(cachep);\n\n\t\tbatchcount = alloc_block(cachep, ac, slab, batchcount);\n\t\tfixup_slab_list(cachep, n, slab, &list);\n\t}\n\nmust_grow:\n\tn->free_objects -= ac->avail;\nalloc_done:\n\traw_spin_unlock(&n->list_lock);\n\tfixup_objfreelist_debug(cachep, &list);\n\ndirect_grow:\n\tif (unlikely(!ac->avail)) {\n\t\t \n\t\tif (sk_memalloc_socks()) {\n\t\t\tvoid *obj = cache_alloc_pfmemalloc(cachep, n, flags);\n\n\t\t\tif (obj)\n\t\t\t\treturn obj;\n\t\t}\n\n\t\tslab = cache_grow_begin(cachep, gfp_exact_node(flags), node);\n\n\t\t \n\t\tac = cpu_cache_get(cachep);\n\t\tif (!ac->avail && slab)\n\t\t\talloc_block(cachep, ac, slab, batchcount);\n\t\tcache_grow_end(cachep, slab);\n\n\t\tif (!ac->avail)\n\t\t\treturn NULL;\n\t}\n\tac->touched = 1;\n\n\treturn ac->entry[--ac->avail];\n}\n\n#if DEBUG\nstatic void *cache_alloc_debugcheck_after(struct kmem_cache *cachep,\n\t\t\t\tgfp_t flags, void *objp, unsigned long caller)\n{\n\tWARN_ON_ONCE(cachep->ctor && (flags & __GFP_ZERO));\n\tif (!objp || is_kfence_address(objp))\n\t\treturn objp;\n\tif (cachep->flags & SLAB_POISON) {\n\t\tcheck_poison_obj(cachep, objp);\n\t\tslab_kernel_map(cachep, objp, 1);\n\t\tpoison_obj(cachep, objp, POISON_INUSE);\n\t}\n\tif (cachep->flags & SLAB_STORE_USER)\n\t\t*dbg_userword(cachep, objp) = (void *)caller;\n\n\tif (cachep->flags & SLAB_RED_ZONE) {\n\t\tif (*dbg_redzone1(cachep, objp) != RED_INACTIVE ||\n\t\t\t\t*dbg_redzone2(cachep, objp) != RED_INACTIVE) {\n\t\t\tslab_error(cachep, \"double free, or memory outside object was overwritten\");\n\t\t\tpr_err(\"%px: redzone 1:0x%llx, redzone 2:0x%llx\\n\",\n\t\t\t       objp, *dbg_redzone1(cachep, objp),\n\t\t\t       *dbg_redzone2(cachep, objp));\n\t\t}\n\t\t*dbg_redzone1(cachep, objp) = RED_ACTIVE;\n\t\t*dbg_redzone2(cachep, objp) = RED_ACTIVE;\n\t}\n\n\tobjp += obj_offset(cachep);\n\tif (cachep->ctor && cachep->flags & SLAB_POISON)\n\t\tcachep->ctor(objp);\n\tif ((unsigned long)objp & (arch_slab_minalign() - 1)) {\n\t\tpr_err(\"0x%px: not aligned to arch_slab_minalign()=%u\\n\", objp,\n\t\t       arch_slab_minalign());\n\t}\n\treturn objp;\n}\n#else\n#define cache_alloc_debugcheck_after(a, b, objp, d) (objp)\n#endif\n\nstatic inline void *____cache_alloc(struct kmem_cache *cachep, gfp_t flags)\n{\n\tvoid *objp;\n\tstruct array_cache *ac;\n\n\tcheck_irq_off();\n\n\tac = cpu_cache_get(cachep);\n\tif (likely(ac->avail)) {\n\t\tac->touched = 1;\n\t\tobjp = ac->entry[--ac->avail];\n\n\t\tSTATS_INC_ALLOCHIT(cachep);\n\t\tgoto out;\n\t}\n\n\tSTATS_INC_ALLOCMISS(cachep);\n\tobjp = cache_alloc_refill(cachep, flags);\n\t \n\tac = cpu_cache_get(cachep);\n\nout:\n\t \n\tif (objp)\n\t\tkmemleak_erase(&ac->entry[ac->avail]);\n\treturn objp;\n}\n\n#ifdef CONFIG_NUMA\nstatic void *____cache_alloc_node(struct kmem_cache *, gfp_t, int);\n\n \nstatic void *alternate_node_alloc(struct kmem_cache *cachep, gfp_t flags)\n{\n\tint nid_alloc, nid_here;\n\n\tif (in_interrupt() || (flags & __GFP_THISNODE))\n\t\treturn NULL;\n\tnid_alloc = nid_here = numa_mem_id();\n\tif (cpuset_do_slab_mem_spread() && (cachep->flags & SLAB_MEM_SPREAD))\n\t\tnid_alloc = cpuset_slab_spread_node();\n\telse if (current->mempolicy)\n\t\tnid_alloc = mempolicy_slab_node();\n\tif (nid_alloc != nid_here)\n\t\treturn ____cache_alloc_node(cachep, flags, nid_alloc);\n\treturn NULL;\n}\n\n \nstatic void *fallback_alloc(struct kmem_cache *cache, gfp_t flags)\n{\n\tstruct zonelist *zonelist;\n\tstruct zoneref *z;\n\tstruct zone *zone;\n\tenum zone_type highest_zoneidx = gfp_zone(flags);\n\tvoid *obj = NULL;\n\tstruct slab *slab;\n\tint nid;\n\tunsigned int cpuset_mems_cookie;\n\n\tif (flags & __GFP_THISNODE)\n\t\treturn NULL;\n\nretry_cpuset:\n\tcpuset_mems_cookie = read_mems_allowed_begin();\n\tzonelist = node_zonelist(mempolicy_slab_node(), flags);\n\nretry:\n\t \n\tfor_each_zone_zonelist(zone, z, zonelist, highest_zoneidx) {\n\t\tnid = zone_to_nid(zone);\n\n\t\tif (cpuset_zone_allowed(zone, flags) &&\n\t\t\tget_node(cache, nid) &&\n\t\t\tget_node(cache, nid)->free_objects) {\n\t\t\t\tobj = ____cache_alloc_node(cache,\n\t\t\t\t\tgfp_exact_node(flags), nid);\n\t\t\t\tif (obj)\n\t\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!obj) {\n\t\t \n\t\tslab = cache_grow_begin(cache, flags, numa_mem_id());\n\t\tcache_grow_end(cache, slab);\n\t\tif (slab) {\n\t\t\tnid = slab_nid(slab);\n\t\t\tobj = ____cache_alloc_node(cache,\n\t\t\t\tgfp_exact_node(flags), nid);\n\n\t\t\t \n\t\t\tif (!obj)\n\t\t\t\tgoto retry;\n\t\t}\n\t}\n\n\tif (unlikely(!obj && read_mems_allowed_retry(cpuset_mems_cookie)))\n\t\tgoto retry_cpuset;\n\treturn obj;\n}\n\n \nstatic void *____cache_alloc_node(struct kmem_cache *cachep, gfp_t flags,\n\t\t\t\tint nodeid)\n{\n\tstruct slab *slab;\n\tstruct kmem_cache_node *n;\n\tvoid *obj = NULL;\n\tvoid *list = NULL;\n\n\tVM_BUG_ON(nodeid < 0 || nodeid >= MAX_NUMNODES);\n\tn = get_node(cachep, nodeid);\n\tBUG_ON(!n);\n\n\tcheck_irq_off();\n\traw_spin_lock(&n->list_lock);\n\tslab = get_first_slab(n, false);\n\tif (!slab)\n\t\tgoto must_grow;\n\n\tcheck_spinlock_acquired_node(cachep, nodeid);\n\n\tSTATS_INC_NODEALLOCS(cachep);\n\tSTATS_INC_ACTIVE(cachep);\n\tSTATS_SET_HIGH(cachep);\n\n\tBUG_ON(slab->active == cachep->num);\n\n\tobj = slab_get_obj(cachep, slab);\n\tn->free_objects--;\n\n\tfixup_slab_list(cachep, n, slab, &list);\n\n\traw_spin_unlock(&n->list_lock);\n\tfixup_objfreelist_debug(cachep, &list);\n\treturn obj;\n\nmust_grow:\n\traw_spin_unlock(&n->list_lock);\n\tslab = cache_grow_begin(cachep, gfp_exact_node(flags), nodeid);\n\tif (slab) {\n\t\t \n\t\tobj = slab_get_obj(cachep, slab);\n\t}\n\tcache_grow_end(cachep, slab);\n\n\treturn obj ? obj : fallback_alloc(cachep, flags);\n}\n\nstatic __always_inline void *\n__do_cache_alloc(struct kmem_cache *cachep, gfp_t flags, int nodeid)\n{\n\tvoid *objp = NULL;\n\tint slab_node = numa_mem_id();\n\n\tif (nodeid == NUMA_NO_NODE) {\n\t\tif (current->mempolicy || cpuset_do_slab_mem_spread()) {\n\t\t\tobjp = alternate_node_alloc(cachep, flags);\n\t\t\tif (objp)\n\t\t\t\tgoto out;\n\t\t}\n\t\t \n\t\tobjp = ____cache_alloc(cachep, flags);\n\t\tnodeid = slab_node;\n\t} else if (nodeid == slab_node) {\n\t\tobjp = ____cache_alloc(cachep, flags);\n\t} else if (!get_node(cachep, nodeid)) {\n\t\t \n\t\tobjp = fallback_alloc(cachep, flags);\n\t\tgoto out;\n\t}\n\n\t \n\tif (!objp)\n\t\tobjp = ____cache_alloc_node(cachep, flags, nodeid);\nout:\n\treturn objp;\n}\n#else\n\nstatic __always_inline void *\n__do_cache_alloc(struct kmem_cache *cachep, gfp_t flags, int nodeid __maybe_unused)\n{\n\treturn ____cache_alloc(cachep, flags);\n}\n\n#endif  \n\nstatic __always_inline void *\nslab_alloc_node(struct kmem_cache *cachep, struct list_lru *lru, gfp_t flags,\n\t\tint nodeid, size_t orig_size, unsigned long caller)\n{\n\tunsigned long save_flags;\n\tvoid *objp;\n\tstruct obj_cgroup *objcg = NULL;\n\tbool init = false;\n\n\tflags &= gfp_allowed_mask;\n\tcachep = slab_pre_alloc_hook(cachep, lru, &objcg, 1, flags);\n\tif (unlikely(!cachep))\n\t\treturn NULL;\n\n\tobjp = kfence_alloc(cachep, orig_size, flags);\n\tif (unlikely(objp))\n\t\tgoto out;\n\n\tlocal_irq_save(save_flags);\n\tobjp = __do_cache_alloc(cachep, flags, nodeid);\n\tlocal_irq_restore(save_flags);\n\tobjp = cache_alloc_debugcheck_after(cachep, flags, objp, caller);\n\tprefetchw(objp);\n\tinit = slab_want_init_on_alloc(flags, cachep);\n\nout:\n\tslab_post_alloc_hook(cachep, objcg, flags, 1, &objp, init,\n\t\t\t\tcachep->object_size);\n\treturn objp;\n}\n\nstatic __always_inline void *\nslab_alloc(struct kmem_cache *cachep, struct list_lru *lru, gfp_t flags,\n\t   size_t orig_size, unsigned long caller)\n{\n\treturn slab_alloc_node(cachep, lru, flags, NUMA_NO_NODE, orig_size,\n\t\t\t       caller);\n}\n\n \nstatic void free_block(struct kmem_cache *cachep, void **objpp,\n\t\t\tint nr_objects, int node, struct list_head *list)\n{\n\tint i;\n\tstruct kmem_cache_node *n = get_node(cachep, node);\n\tstruct slab *slab;\n\n\tn->free_objects += nr_objects;\n\n\tfor (i = 0; i < nr_objects; i++) {\n\t\tvoid *objp;\n\t\tstruct slab *slab;\n\n\t\tobjp = objpp[i];\n\n\t\tslab = virt_to_slab(objp);\n\t\tlist_del(&slab->slab_list);\n\t\tcheck_spinlock_acquired_node(cachep, node);\n\t\tslab_put_obj(cachep, slab, objp);\n\t\tSTATS_DEC_ACTIVE(cachep);\n\n\t\t \n\t\tif (slab->active == 0) {\n\t\t\tlist_add(&slab->slab_list, &n->slabs_free);\n\t\t\tn->free_slabs++;\n\t\t} else {\n\t\t\t \n\t\t\tlist_add_tail(&slab->slab_list, &n->slabs_partial);\n\t\t}\n\t}\n\n\twhile (n->free_objects > n->free_limit && !list_empty(&n->slabs_free)) {\n\t\tn->free_objects -= cachep->num;\n\n\t\tslab = list_last_entry(&n->slabs_free, struct slab, slab_list);\n\t\tlist_move(&slab->slab_list, list);\n\t\tn->free_slabs--;\n\t\tn->total_slabs--;\n\t}\n}\n\nstatic void cache_flusharray(struct kmem_cache *cachep, struct array_cache *ac)\n{\n\tint batchcount;\n\tstruct kmem_cache_node *n;\n\tint node = numa_mem_id();\n\tLIST_HEAD(list);\n\n\tbatchcount = ac->batchcount;\n\n\tcheck_irq_off();\n\tn = get_node(cachep, node);\n\traw_spin_lock(&n->list_lock);\n\tif (n->shared) {\n\t\tstruct array_cache *shared_array = n->shared;\n\t\tint max = shared_array->limit - shared_array->avail;\n\t\tif (max) {\n\t\t\tif (batchcount > max)\n\t\t\t\tbatchcount = max;\n\t\t\tmemcpy(&(shared_array->entry[shared_array->avail]),\n\t\t\t       ac->entry, sizeof(void *) * batchcount);\n\t\t\tshared_array->avail += batchcount;\n\t\t\tgoto free_done;\n\t\t}\n\t}\n\n\tfree_block(cachep, ac->entry, batchcount, node, &list);\nfree_done:\n#if STATS\n\t{\n\t\tint i = 0;\n\t\tstruct slab *slab;\n\n\t\tlist_for_each_entry(slab, &n->slabs_free, slab_list) {\n\t\t\tBUG_ON(slab->active);\n\n\t\t\ti++;\n\t\t}\n\t\tSTATS_SET_FREEABLE(cachep, i);\n\t}\n#endif\n\traw_spin_unlock(&n->list_lock);\n\tac->avail -= batchcount;\n\tmemmove(ac->entry, &(ac->entry[batchcount]), sizeof(void *)*ac->avail);\n\tslabs_destroy(cachep, &list);\n}\n\n \nstatic __always_inline void __cache_free(struct kmem_cache *cachep, void *objp,\n\t\t\t\t\t unsigned long caller)\n{\n\tbool init;\n\n\tmemcg_slab_free_hook(cachep, virt_to_slab(objp), &objp, 1);\n\n\tif (is_kfence_address(objp)) {\n\t\tkmemleak_free_recursive(objp, cachep->flags);\n\t\t__kfence_free(objp);\n\t\treturn;\n\t}\n\n\t \n\tinit = slab_want_init_on_free(cachep);\n\tif (init && !kasan_has_integrated_init())\n\t\tmemset(objp, 0, cachep->object_size);\n\t \n\tif (kasan_slab_free(cachep, objp, init))\n\t\treturn;\n\n\t \n\tif (!(cachep->flags & SLAB_TYPESAFE_BY_RCU))\n\t\t__kcsan_check_access(objp, cachep->object_size,\n\t\t\t\t     KCSAN_ACCESS_WRITE | KCSAN_ACCESS_ASSERT);\n\n\t___cache_free(cachep, objp, caller);\n}\n\nvoid ___cache_free(struct kmem_cache *cachep, void *objp,\n\t\tunsigned long caller)\n{\n\tstruct array_cache *ac = cpu_cache_get(cachep);\n\n\tcheck_irq_off();\n\tkmemleak_free_recursive(objp, cachep->flags);\n\tobjp = cache_free_debugcheck(cachep, objp, caller);\n\n\t \n\tif (nr_online_nodes > 1 && cache_free_alien(cachep, objp))\n\t\treturn;\n\n\tif (ac->avail < ac->limit) {\n\t\tSTATS_INC_FREEHIT(cachep);\n\t} else {\n\t\tSTATS_INC_FREEMISS(cachep);\n\t\tcache_flusharray(cachep, ac);\n\t}\n\n\tif (sk_memalloc_socks()) {\n\t\tstruct slab *slab = virt_to_slab(objp);\n\n\t\tif (unlikely(slab_test_pfmemalloc(slab))) {\n\t\t\tcache_free_pfmemalloc(cachep, slab, objp);\n\t\t\treturn;\n\t\t}\n\t}\n\n\t__free_one(ac, objp);\n}\n\nstatic __always_inline\nvoid *__kmem_cache_alloc_lru(struct kmem_cache *cachep, struct list_lru *lru,\n\t\t\t     gfp_t flags)\n{\n\tvoid *ret = slab_alloc(cachep, lru, flags, cachep->object_size, _RET_IP_);\n\n\ttrace_kmem_cache_alloc(_RET_IP_, ret, cachep, flags, NUMA_NO_NODE);\n\n\treturn ret;\n}\n\nvoid *kmem_cache_alloc(struct kmem_cache *cachep, gfp_t flags)\n{\n\treturn __kmem_cache_alloc_lru(cachep, NULL, flags);\n}\nEXPORT_SYMBOL(kmem_cache_alloc);\n\nvoid *kmem_cache_alloc_lru(struct kmem_cache *cachep, struct list_lru *lru,\n\t\t\t   gfp_t flags)\n{\n\treturn __kmem_cache_alloc_lru(cachep, lru, flags);\n}\nEXPORT_SYMBOL(kmem_cache_alloc_lru);\n\nstatic __always_inline void\ncache_alloc_debugcheck_after_bulk(struct kmem_cache *s, gfp_t flags,\n\t\t\t\t  size_t size, void **p, unsigned long caller)\n{\n\tsize_t i;\n\n\tfor (i = 0; i < size; i++)\n\t\tp[i] = cache_alloc_debugcheck_after(s, flags, p[i], caller);\n}\n\nint kmem_cache_alloc_bulk(struct kmem_cache *s, gfp_t flags, size_t size,\n\t\t\t  void **p)\n{\n\tstruct obj_cgroup *objcg = NULL;\n\tunsigned long irqflags;\n\tsize_t i;\n\n\ts = slab_pre_alloc_hook(s, NULL, &objcg, size, flags);\n\tif (!s)\n\t\treturn 0;\n\n\tlocal_irq_save(irqflags);\n\tfor (i = 0; i < size; i++) {\n\t\tvoid *objp = kfence_alloc(s, s->object_size, flags) ?:\n\t\t\t     __do_cache_alloc(s, flags, NUMA_NO_NODE);\n\n\t\tif (unlikely(!objp))\n\t\t\tgoto error;\n\t\tp[i] = objp;\n\t}\n\tlocal_irq_restore(irqflags);\n\n\tcache_alloc_debugcheck_after_bulk(s, flags, size, p, _RET_IP_);\n\n\t \n\tslab_post_alloc_hook(s, objcg, flags, size, p,\n\t\t\tslab_want_init_on_alloc(flags, s), s->object_size);\n\t \n\treturn size;\nerror:\n\tlocal_irq_restore(irqflags);\n\tcache_alloc_debugcheck_after_bulk(s, flags, i, p, _RET_IP_);\n\tslab_post_alloc_hook(s, objcg, flags, i, p, false, s->object_size);\n\tkmem_cache_free_bulk(s, i, p);\n\treturn 0;\n}\nEXPORT_SYMBOL(kmem_cache_alloc_bulk);\n\n \nvoid *kmem_cache_alloc_node(struct kmem_cache *cachep, gfp_t flags, int nodeid)\n{\n\tvoid *ret = slab_alloc_node(cachep, NULL, flags, nodeid, cachep->object_size, _RET_IP_);\n\n\ttrace_kmem_cache_alloc(_RET_IP_, ret, cachep, flags, nodeid);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(kmem_cache_alloc_node);\n\nvoid *__kmem_cache_alloc_node(struct kmem_cache *cachep, gfp_t flags,\n\t\t\t     int nodeid, size_t orig_size,\n\t\t\t     unsigned long caller)\n{\n\treturn slab_alloc_node(cachep, NULL, flags, nodeid,\n\t\t\t       orig_size, caller);\n}\n\n#ifdef CONFIG_PRINTK\nvoid __kmem_obj_info(struct kmem_obj_info *kpp, void *object, struct slab *slab)\n{\n\tstruct kmem_cache *cachep;\n\tunsigned int objnr;\n\tvoid *objp;\n\n\tkpp->kp_ptr = object;\n\tkpp->kp_slab = slab;\n\tcachep = slab->slab_cache;\n\tkpp->kp_slab_cache = cachep;\n\tobjp = object - obj_offset(cachep);\n\tkpp->kp_data_offset = obj_offset(cachep);\n\tslab = virt_to_slab(objp);\n\tobjnr = obj_to_index(cachep, slab, objp);\n\tobjp = index_to_obj(cachep, slab, objnr);\n\tkpp->kp_objp = objp;\n\tif (DEBUG && cachep->flags & SLAB_STORE_USER)\n\t\tkpp->kp_ret = *dbg_userword(cachep, objp);\n}\n#endif\n\nstatic __always_inline\nvoid __do_kmem_cache_free(struct kmem_cache *cachep, void *objp,\n\t\t\t  unsigned long caller)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tdebug_check_no_locks_freed(objp, cachep->object_size);\n\tif (!(cachep->flags & SLAB_DEBUG_OBJECTS))\n\t\tdebug_check_no_obj_freed(objp, cachep->object_size);\n\t__cache_free(cachep, objp, caller);\n\tlocal_irq_restore(flags);\n}\n\nvoid __kmem_cache_free(struct kmem_cache *cachep, void *objp,\n\t\t       unsigned long caller)\n{\n\t__do_kmem_cache_free(cachep, objp, caller);\n}\n\n \nvoid kmem_cache_free(struct kmem_cache *cachep, void *objp)\n{\n\tcachep = cache_from_obj(cachep, objp);\n\tif (!cachep)\n\t\treturn;\n\n\ttrace_kmem_cache_free(_RET_IP_, objp, cachep);\n\t__do_kmem_cache_free(cachep, objp, _RET_IP_);\n}\nEXPORT_SYMBOL(kmem_cache_free);\n\nvoid kmem_cache_free_bulk(struct kmem_cache *orig_s, size_t size, void **p)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tfor (int i = 0; i < size; i++) {\n\t\tvoid *objp = p[i];\n\t\tstruct kmem_cache *s;\n\n\t\tif (!orig_s) {\n\t\t\tstruct folio *folio = virt_to_folio(objp);\n\n\t\t\t \n\t\t\tif (!folio_test_slab(folio)) {\n\t\t\t\tlocal_irq_restore(flags);\n\t\t\t\tfree_large_kmalloc(folio, objp);\n\t\t\t\tlocal_irq_save(flags);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\ts = folio_slab(folio)->slab_cache;\n\t\t} else {\n\t\t\ts = cache_from_obj(orig_s, objp);\n\t\t}\n\n\t\tif (!s)\n\t\t\tcontinue;\n\n\t\tdebug_check_no_locks_freed(objp, s->object_size);\n\t\tif (!(s->flags & SLAB_DEBUG_OBJECTS))\n\t\t\tdebug_check_no_obj_freed(objp, s->object_size);\n\n\t\t__cache_free(s, objp, _RET_IP_);\n\t}\n\tlocal_irq_restore(flags);\n\n\t \n}\nEXPORT_SYMBOL(kmem_cache_free_bulk);\n\n \nstatic int setup_kmem_cache_nodes(struct kmem_cache *cachep, gfp_t gfp)\n{\n\tint ret;\n\tint node;\n\tstruct kmem_cache_node *n;\n\n\tfor_each_online_node(node) {\n\t\tret = setup_kmem_cache_node(cachep, node, gfp, true);\n\t\tif (ret)\n\t\t\tgoto fail;\n\n\t}\n\n\treturn 0;\n\nfail:\n\tif (!cachep->list.next) {\n\t\t \n\t\tnode--;\n\t\twhile (node >= 0) {\n\t\t\tn = get_node(cachep, node);\n\t\t\tif (n) {\n\t\t\t\tkfree(n->shared);\n\t\t\t\tfree_alien_cache(n->alien);\n\t\t\t\tkfree(n);\n\t\t\t\tcachep->node[node] = NULL;\n\t\t\t}\n\t\t\tnode--;\n\t\t}\n\t}\n\treturn -ENOMEM;\n}\n\n \nstatic int do_tune_cpucache(struct kmem_cache *cachep, int limit,\n\t\t\t    int batchcount, int shared, gfp_t gfp)\n{\n\tstruct array_cache __percpu *cpu_cache, *prev;\n\tint cpu;\n\n\tcpu_cache = alloc_kmem_cache_cpus(cachep, limit, batchcount);\n\tif (!cpu_cache)\n\t\treturn -ENOMEM;\n\n\tprev = cachep->cpu_cache;\n\tcachep->cpu_cache = cpu_cache;\n\t \n\tif (prev)\n\t\tkick_all_cpus_sync();\n\n\tcheck_irq_on();\n\tcachep->batchcount = batchcount;\n\tcachep->limit = limit;\n\tcachep->shared = shared;\n\n\tif (!prev)\n\t\tgoto setup_node;\n\n\tfor_each_online_cpu(cpu) {\n\t\tLIST_HEAD(list);\n\t\tint node;\n\t\tstruct kmem_cache_node *n;\n\t\tstruct array_cache *ac = per_cpu_ptr(prev, cpu);\n\n\t\tnode = cpu_to_mem(cpu);\n\t\tn = get_node(cachep, node);\n\t\traw_spin_lock_irq(&n->list_lock);\n\t\tfree_block(cachep, ac->entry, ac->avail, node, &list);\n\t\traw_spin_unlock_irq(&n->list_lock);\n\t\tslabs_destroy(cachep, &list);\n\t}\n\tfree_percpu(prev);\n\nsetup_node:\n\treturn setup_kmem_cache_nodes(cachep, gfp);\n}\n\n \nstatic int enable_cpucache(struct kmem_cache *cachep, gfp_t gfp)\n{\n\tint err;\n\tint limit = 0;\n\tint shared = 0;\n\tint batchcount = 0;\n\n\terr = cache_random_seq_create(cachep, cachep->num, gfp);\n\tif (err)\n\t\tgoto end;\n\n\t \n\tif (cachep->size > 131072)\n\t\tlimit = 1;\n\telse if (cachep->size > PAGE_SIZE)\n\t\tlimit = 8;\n\telse if (cachep->size > 1024)\n\t\tlimit = 24;\n\telse if (cachep->size > 256)\n\t\tlimit = 54;\n\telse\n\t\tlimit = 120;\n\n\t \n\tshared = 0;\n\tif (cachep->size <= PAGE_SIZE && num_possible_cpus() > 1)\n\t\tshared = 8;\n\n#if DEBUG\n\t \n\tif (limit > 32)\n\t\tlimit = 32;\n#endif\n\tbatchcount = (limit + 1) / 2;\n\terr = do_tune_cpucache(cachep, limit, batchcount, shared, gfp);\nend:\n\tif (err)\n\t\tpr_err(\"enable_cpucache failed for %s, error %d\\n\",\n\t\t       cachep->name, -err);\n\treturn err;\n}\n\n \nstatic void drain_array(struct kmem_cache *cachep, struct kmem_cache_node *n,\n\t\t\t struct array_cache *ac, int node)\n{\n\tLIST_HEAD(list);\n\n\t \n\tcheck_mutex_acquired();\n\n\tif (!ac || !ac->avail)\n\t\treturn;\n\n\tif (ac->touched) {\n\t\tac->touched = 0;\n\t\treturn;\n\t}\n\n\traw_spin_lock_irq(&n->list_lock);\n\tdrain_array_locked(cachep, ac, node, false, &list);\n\traw_spin_unlock_irq(&n->list_lock);\n\n\tslabs_destroy(cachep, &list);\n}\n\n \nstatic void cache_reap(struct work_struct *w)\n{\n\tstruct kmem_cache *searchp;\n\tstruct kmem_cache_node *n;\n\tint node = numa_mem_id();\n\tstruct delayed_work *work = to_delayed_work(w);\n\n\tif (!mutex_trylock(&slab_mutex))\n\t\t \n\t\tgoto out;\n\n\tlist_for_each_entry(searchp, &slab_caches, list) {\n\t\tcheck_irq_on();\n\n\t\t \n\t\tn = get_node(searchp, node);\n\n\t\treap_alien(searchp, n);\n\n\t\tdrain_array(searchp, n, cpu_cache_get(searchp), node);\n\n\t\t \n\t\tif (time_after(n->next_reap, jiffies))\n\t\t\tgoto next;\n\n\t\tn->next_reap = jiffies + REAPTIMEOUT_NODE;\n\n\t\tdrain_array(searchp, n, n->shared, node);\n\n\t\tif (n->free_touched)\n\t\t\tn->free_touched = 0;\n\t\telse {\n\t\t\tint freed;\n\n\t\t\tfreed = drain_freelist(searchp, n, (n->free_limit +\n\t\t\t\t5 * searchp->num - 1) / (5 * searchp->num));\n\t\t\tSTATS_ADD_REAPED(searchp, freed);\n\t\t}\nnext:\n\t\tcond_resched();\n\t}\n\tcheck_irq_on();\n\tmutex_unlock(&slab_mutex);\n\tnext_reap_node();\nout:\n\t \n\tschedule_delayed_work_on(smp_processor_id(), work,\n\t\t\t\tround_jiffies_relative(REAPTIMEOUT_AC));\n}\n\nvoid get_slabinfo(struct kmem_cache *cachep, struct slabinfo *sinfo)\n{\n\tunsigned long active_objs, num_objs, active_slabs;\n\tunsigned long total_slabs = 0, free_objs = 0, shared_avail = 0;\n\tunsigned long free_slabs = 0;\n\tint node;\n\tstruct kmem_cache_node *n;\n\n\tfor_each_kmem_cache_node(cachep, node, n) {\n\t\tcheck_irq_on();\n\t\traw_spin_lock_irq(&n->list_lock);\n\n\t\ttotal_slabs += n->total_slabs;\n\t\tfree_slabs += n->free_slabs;\n\t\tfree_objs += n->free_objects;\n\n\t\tif (n->shared)\n\t\t\tshared_avail += n->shared->avail;\n\n\t\traw_spin_unlock_irq(&n->list_lock);\n\t}\n\tnum_objs = total_slabs * cachep->num;\n\tactive_slabs = total_slabs - free_slabs;\n\tactive_objs = num_objs - free_objs;\n\n\tsinfo->active_objs = active_objs;\n\tsinfo->num_objs = num_objs;\n\tsinfo->active_slabs = active_slabs;\n\tsinfo->num_slabs = total_slabs;\n\tsinfo->shared_avail = shared_avail;\n\tsinfo->limit = cachep->limit;\n\tsinfo->batchcount = cachep->batchcount;\n\tsinfo->shared = cachep->shared;\n\tsinfo->objects_per_slab = cachep->num;\n\tsinfo->cache_order = cachep->gfporder;\n}\n\nvoid slabinfo_show_stats(struct seq_file *m, struct kmem_cache *cachep)\n{\n#if STATS\n\t{\t\t\t \n\t\tunsigned long high = cachep->high_mark;\n\t\tunsigned long allocs = cachep->num_allocations;\n\t\tunsigned long grown = cachep->grown;\n\t\tunsigned long reaped = cachep->reaped;\n\t\tunsigned long errors = cachep->errors;\n\t\tunsigned long max_freeable = cachep->max_freeable;\n\t\tunsigned long node_allocs = cachep->node_allocs;\n\t\tunsigned long node_frees = cachep->node_frees;\n\t\tunsigned long overflows = cachep->node_overflow;\n\n\t\tseq_printf(m, \" : globalstat %7lu %6lu %5lu %4lu %4lu %4lu %4lu %4lu %4lu\",\n\t\t\t   allocs, high, grown,\n\t\t\t   reaped, errors, max_freeable, node_allocs,\n\t\t\t   node_frees, overflows);\n\t}\n\t \n\t{\n\t\tunsigned long allochit = atomic_read(&cachep->allochit);\n\t\tunsigned long allocmiss = atomic_read(&cachep->allocmiss);\n\t\tunsigned long freehit = atomic_read(&cachep->freehit);\n\t\tunsigned long freemiss = atomic_read(&cachep->freemiss);\n\n\t\tseq_printf(m, \" : cpustat %6lu %6lu %6lu %6lu\",\n\t\t\t   allochit, allocmiss, freehit, freemiss);\n\t}\n#endif\n}\n\n#define MAX_SLABINFO_WRITE 128\n \nssize_t slabinfo_write(struct file *file, const char __user *buffer,\n\t\t       size_t count, loff_t *ppos)\n{\n\tchar kbuf[MAX_SLABINFO_WRITE + 1], *tmp;\n\tint limit, batchcount, shared, res;\n\tstruct kmem_cache *cachep;\n\n\tif (count > MAX_SLABINFO_WRITE)\n\t\treturn -EINVAL;\n\tif (copy_from_user(&kbuf, buffer, count))\n\t\treturn -EFAULT;\n\tkbuf[MAX_SLABINFO_WRITE] = '\\0';\n\n\ttmp = strchr(kbuf, ' ');\n\tif (!tmp)\n\t\treturn -EINVAL;\n\t*tmp = '\\0';\n\ttmp++;\n\tif (sscanf(tmp, \" %d %d %d\", &limit, &batchcount, &shared) != 3)\n\t\treturn -EINVAL;\n\n\t \n\tmutex_lock(&slab_mutex);\n\tres = -EINVAL;\n\tlist_for_each_entry(cachep, &slab_caches, list) {\n\t\tif (!strcmp(cachep->name, kbuf)) {\n\t\t\tif (limit < 1 || batchcount < 1 ||\n\t\t\t\t\tbatchcount > limit || shared < 0) {\n\t\t\t\tres = 0;\n\t\t\t} else {\n\t\t\t\tres = do_tune_cpucache(cachep, limit,\n\t\t\t\t\t\t       batchcount, shared,\n\t\t\t\t\t\t       GFP_KERNEL);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&slab_mutex);\n\tif (res >= 0)\n\t\tres = count;\n\treturn res;\n}\n\n#ifdef CONFIG_HARDENED_USERCOPY\n \nvoid __check_heap_object(const void *ptr, unsigned long n,\n\t\t\t const struct slab *slab, bool to_user)\n{\n\tstruct kmem_cache *cachep;\n\tunsigned int objnr;\n\tunsigned long offset;\n\n\tptr = kasan_reset_tag(ptr);\n\n\t \n\tcachep = slab->slab_cache;\n\tobjnr = obj_to_index(cachep, slab, (void *)ptr);\n\tBUG_ON(objnr >= cachep->num);\n\n\t \n\tif (is_kfence_address(ptr))\n\t\toffset = ptr - kfence_object_start(ptr);\n\telse\n\t\toffset = ptr - index_to_obj(cachep, slab, objnr) - obj_offset(cachep);\n\n\t \n\tif (offset >= cachep->useroffset &&\n\t    offset - cachep->useroffset <= cachep->usersize &&\n\t    n <= cachep->useroffset - offset + cachep->usersize)\n\t\treturn;\n\n\tusercopy_abort(\"SLAB object\", cachep->name, to_user, offset, n);\n}\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}