{
  "module_name": "mprotect.c",
  "hash_id": "374b27e86bc555e13a07bdfaddb9a7b3dd7d79fe3e129b01274ac2ede23d3de7",
  "original_prompt": "Ingested from linux-6.6.14/mm/mprotect.c",
  "human_readable_source": "\n \n\n#include <linux/pagewalk.h>\n#include <linux/hugetlb.h>\n#include <linux/shm.h>\n#include <linux/mman.h>\n#include <linux/fs.h>\n#include <linux/highmem.h>\n#include <linux/security.h>\n#include <linux/mempolicy.h>\n#include <linux/personality.h>\n#include <linux/syscalls.h>\n#include <linux/swap.h>\n#include <linux/swapops.h>\n#include <linux/mmu_notifier.h>\n#include <linux/migrate.h>\n#include <linux/perf_event.h>\n#include <linux/pkeys.h>\n#include <linux/ksm.h>\n#include <linux/uaccess.h>\n#include <linux/mm_inline.h>\n#include <linux/pgtable.h>\n#include <linux/sched/sysctl.h>\n#include <linux/userfaultfd_k.h>\n#include <linux/memory-tiers.h>\n#include <asm/cacheflush.h>\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n\n#include \"internal.h\"\n\nbool can_change_pte_writable(struct vm_area_struct *vma, unsigned long addr,\n\t\t\t     pte_t pte)\n{\n\tstruct page *page;\n\n\tif (WARN_ON_ONCE(!(vma->vm_flags & VM_WRITE)))\n\t\treturn false;\n\n\t \n\tif (pte_protnone(pte))\n\t\treturn false;\n\n\t \n\tif (vma_soft_dirty_enabled(vma) && !pte_soft_dirty(pte))\n\t\treturn false;\n\n\t \n\tif (userfaultfd_pte_wp(vma, pte))\n\t\treturn false;\n\n\tif (!(vma->vm_flags & VM_SHARED)) {\n\t\t \n\t\tpage = vm_normal_page(vma, addr, pte);\n\t\treturn page && PageAnon(page) && PageAnonExclusive(page);\n\t}\n\n\t \n\treturn pte_dirty(pte);\n}\n\nstatic long change_pte_range(struct mmu_gather *tlb,\n\t\tstruct vm_area_struct *vma, pmd_t *pmd, unsigned long addr,\n\t\tunsigned long end, pgprot_t newprot, unsigned long cp_flags)\n{\n\tpte_t *pte, oldpte;\n\tspinlock_t *ptl;\n\tlong pages = 0;\n\tint target_node = NUMA_NO_NODE;\n\tbool prot_numa = cp_flags & MM_CP_PROT_NUMA;\n\tbool uffd_wp = cp_flags & MM_CP_UFFD_WP;\n\tbool uffd_wp_resolve = cp_flags & MM_CP_UFFD_WP_RESOLVE;\n\n\ttlb_change_page_size(tlb, PAGE_SIZE);\n\tpte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);\n\tif (!pte)\n\t\treturn -EAGAIN;\n\n\t \n\tif (prot_numa && !(vma->vm_flags & VM_SHARED) &&\n\t    atomic_read(&vma->vm_mm->mm_users) == 1)\n\t\ttarget_node = numa_node_id();\n\n\tflush_tlb_batched_pending(vma->vm_mm);\n\tarch_enter_lazy_mmu_mode();\n\tdo {\n\t\toldpte = ptep_get(pte);\n\t\tif (pte_present(oldpte)) {\n\t\t\tpte_t ptent;\n\n\t\t\t \n\t\t\tif (prot_numa) {\n\t\t\t\tstruct page *page;\n\t\t\t\tint nid;\n\t\t\t\tbool toptier;\n\n\t\t\t\t \n\t\t\t\tif (pte_protnone(oldpte))\n\t\t\t\t\tcontinue;\n\n\t\t\t\tpage = vm_normal_page(vma, addr, oldpte);\n\t\t\t\tif (!page || is_zone_device_page(page) || PageKsm(page))\n\t\t\t\t\tcontinue;\n\n\t\t\t\t \n\t\t\t\tif (is_cow_mapping(vma->vm_flags) &&\n\t\t\t\t    page_count(page) != 1)\n\t\t\t\t\tcontinue;\n\n\t\t\t\t \n\t\t\t\tif (page_is_file_lru(page) && PageDirty(page))\n\t\t\t\t\tcontinue;\n\n\t\t\t\t \n\t\t\t\tnid = page_to_nid(page);\n\t\t\t\tif (target_node == nid)\n\t\t\t\t\tcontinue;\n\t\t\t\ttoptier = node_is_toptier(nid);\n\n\t\t\t\t \n\t\t\t\tif (!(sysctl_numa_balancing_mode & NUMA_BALANCING_NORMAL) &&\n\t\t\t\t    toptier)\n\t\t\t\t\tcontinue;\n\t\t\t\tif (sysctl_numa_balancing_mode & NUMA_BALANCING_MEMORY_TIERING &&\n\t\t\t\t    !toptier)\n\t\t\t\t\txchg_page_access_time(page,\n\t\t\t\t\t\tjiffies_to_msecs(jiffies));\n\t\t\t}\n\n\t\t\toldpte = ptep_modify_prot_start(vma, addr, pte);\n\t\t\tptent = pte_modify(oldpte, newprot);\n\n\t\t\tif (uffd_wp)\n\t\t\t\tptent = pte_mkuffd_wp(ptent);\n\t\t\telse if (uffd_wp_resolve)\n\t\t\t\tptent = pte_clear_uffd_wp(ptent);\n\n\t\t\t \n\t\t\tif ((cp_flags & MM_CP_TRY_CHANGE_WRITABLE) &&\n\t\t\t    !pte_write(ptent) &&\n\t\t\t    can_change_pte_writable(vma, addr, ptent))\n\t\t\t\tptent = pte_mkwrite(ptent, vma);\n\n\t\t\tptep_modify_prot_commit(vma, addr, pte, oldpte, ptent);\n\t\t\tif (pte_needs_flush(oldpte, ptent))\n\t\t\t\ttlb_flush_pte_range(tlb, addr, PAGE_SIZE);\n\t\t\tpages++;\n\t\t} else if (is_swap_pte(oldpte)) {\n\t\t\tswp_entry_t entry = pte_to_swp_entry(oldpte);\n\t\t\tpte_t newpte;\n\n\t\t\tif (is_writable_migration_entry(entry)) {\n\t\t\t\tstruct page *page = pfn_swap_entry_to_page(entry);\n\n\t\t\t\t \n\t\t\t\tif (PageAnon(page))\n\t\t\t\t\tentry = make_readable_exclusive_migration_entry(\n\t\t\t\t\t\t\t     swp_offset(entry));\n\t\t\t\telse\n\t\t\t\t\tentry = make_readable_migration_entry(swp_offset(entry));\n\t\t\t\tnewpte = swp_entry_to_pte(entry);\n\t\t\t\tif (pte_swp_soft_dirty(oldpte))\n\t\t\t\t\tnewpte = pte_swp_mksoft_dirty(newpte);\n\t\t\t} else if (is_writable_device_private_entry(entry)) {\n\t\t\t\t \n\t\t\t\tentry = make_readable_device_private_entry(\n\t\t\t\t\t\t\tswp_offset(entry));\n\t\t\t\tnewpte = swp_entry_to_pte(entry);\n\t\t\t\tif (pte_swp_uffd_wp(oldpte))\n\t\t\t\t\tnewpte = pte_swp_mkuffd_wp(newpte);\n\t\t\t} else if (is_writable_device_exclusive_entry(entry)) {\n\t\t\t\tentry = make_readable_device_exclusive_entry(\n\t\t\t\t\t\t\tswp_offset(entry));\n\t\t\t\tnewpte = swp_entry_to_pte(entry);\n\t\t\t\tif (pte_swp_soft_dirty(oldpte))\n\t\t\t\t\tnewpte = pte_swp_mksoft_dirty(newpte);\n\t\t\t\tif (pte_swp_uffd_wp(oldpte))\n\t\t\t\t\tnewpte = pte_swp_mkuffd_wp(newpte);\n\t\t\t} else if (is_pte_marker_entry(entry)) {\n\t\t\t\t \n\t\t\t\tif (is_poisoned_swp_entry(entry))\n\t\t\t\t\tcontinue;\n\t\t\t\t \n\t\t\t\tif (uffd_wp_resolve) {\n\t\t\t\t\tpte_clear(vma->vm_mm, addr, pte);\n\t\t\t\t\tpages++;\n\t\t\t\t}\n\t\t\t\tcontinue;\n\t\t\t} else {\n\t\t\t\tnewpte = oldpte;\n\t\t\t}\n\n\t\t\tif (uffd_wp)\n\t\t\t\tnewpte = pte_swp_mkuffd_wp(newpte);\n\t\t\telse if (uffd_wp_resolve)\n\t\t\t\tnewpte = pte_swp_clear_uffd_wp(newpte);\n\n\t\t\tif (!pte_same(oldpte, newpte)) {\n\t\t\t\tset_pte_at(vma->vm_mm, addr, pte, newpte);\n\t\t\t\tpages++;\n\t\t\t}\n\t\t} else {\n\t\t\t \n\t\t\tWARN_ON_ONCE(!pte_none(oldpte));\n\n\t\t\t \n\t\t\tif (likely(!uffd_wp))\n\t\t\t\tcontinue;\n\n\t\t\tif (userfaultfd_wp_use_markers(vma)) {\n\t\t\t\t \n\t\t\t\tset_pte_at(vma->vm_mm, addr, pte,\n\t\t\t\t\t   make_pte_marker(PTE_MARKER_UFFD_WP));\n\t\t\t\tpages++;\n\t\t\t}\n\t\t}\n\t} while (pte++, addr += PAGE_SIZE, addr != end);\n\tarch_leave_lazy_mmu_mode();\n\tpte_unmap_unlock(pte - 1, ptl);\n\n\treturn pages;\n}\n\n \nstatic inline bool\npgtable_split_needed(struct vm_area_struct *vma, unsigned long cp_flags)\n{\n\t \n\treturn (cp_flags & MM_CP_UFFD_WP) && !vma_is_anonymous(vma);\n}\n\n \nstatic inline bool\npgtable_populate_needed(struct vm_area_struct *vma, unsigned long cp_flags)\n{\n\t \n\tif (!(cp_flags & MM_CP_UFFD_WP))\n\t\treturn false;\n\n\t \n\treturn userfaultfd_wp_use_markers(vma);\n}\n\n \n#define  change_pmd_prepare(vma, pmd, cp_flags)\t\t\t\t\\\n\t({\t\t\t\t\t\t\t\t\\\n\t\tlong err = 0;\t\t\t\t\t\t\\\n\t\tif (unlikely(pgtable_populate_needed(vma, cp_flags))) {\t\\\n\t\t\tif (pte_alloc(vma->vm_mm, pmd))\t\t\t\\\n\t\t\t\terr = -ENOMEM;\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t\terr;\t\t\t\t\t\t\t\\\n\t})\n\n \n#define  change_prepare(vma, high, low, addr, cp_flags)\t\t\t\\\n\t  ({\t\t\t\t\t\t\t\t\\\n\t\tlong err = 0;\t\t\t\t\t\t\\\n\t\tif (unlikely(pgtable_populate_needed(vma, cp_flags))) {\t\\\n\t\t\tlow##_t *p = low##_alloc(vma->vm_mm, high, addr); \\\n\t\t\tif (p == NULL)\t\t\t\t\t\\\n\t\t\t\terr = -ENOMEM;\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t\terr;\t\t\t\t\t\t\t\\\n\t})\n\nstatic inline long change_pmd_range(struct mmu_gather *tlb,\n\t\tstruct vm_area_struct *vma, pud_t *pud, unsigned long addr,\n\t\tunsigned long end, pgprot_t newprot, unsigned long cp_flags)\n{\n\tpmd_t *pmd;\n\tunsigned long next;\n\tlong pages = 0;\n\tunsigned long nr_huge_updates = 0;\n\tstruct mmu_notifier_range range;\n\n\trange.start = 0;\n\n\tpmd = pmd_offset(pud, addr);\n\tdo {\n\t\tlong ret;\n\t\tpmd_t _pmd;\nagain:\n\t\tnext = pmd_addr_end(addr, end);\n\n\t\tret = change_pmd_prepare(vma, pmd, cp_flags);\n\t\tif (ret) {\n\t\t\tpages = ret;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (pmd_none(*pmd))\n\t\t\tgoto next;\n\n\t\t \n\t\tif (!range.start) {\n\t\t\tmmu_notifier_range_init(&range,\n\t\t\t\tMMU_NOTIFY_PROTECTION_VMA, 0,\n\t\t\t\tvma->vm_mm, addr, end);\n\t\t\tmmu_notifier_invalidate_range_start(&range);\n\t\t}\n\n\t\t_pmd = pmdp_get_lockless(pmd);\n\t\tif (is_swap_pmd(_pmd) || pmd_trans_huge(_pmd) || pmd_devmap(_pmd)) {\n\t\t\tif ((next - addr != HPAGE_PMD_SIZE) ||\n\t\t\t    pgtable_split_needed(vma, cp_flags)) {\n\t\t\t\t__split_huge_pmd(vma, pmd, addr, false, NULL);\n\t\t\t\t \n\t\t\t\tret = change_pmd_prepare(vma, pmd, cp_flags);\n\t\t\t\tif (ret) {\n\t\t\t\t\tpages = ret;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tret = change_huge_pmd(tlb, vma, pmd,\n\t\t\t\t\t\taddr, newprot, cp_flags);\n\t\t\t\tif (ret) {\n\t\t\t\t\tif (ret == HPAGE_PMD_NR) {\n\t\t\t\t\t\tpages += HPAGE_PMD_NR;\n\t\t\t\t\t\tnr_huge_updates++;\n\t\t\t\t\t}\n\n\t\t\t\t\t \n\t\t\t\t\tgoto next;\n\t\t\t\t}\n\t\t\t}\n\t\t\t \n\t\t}\n\n\t\tret = change_pte_range(tlb, vma, pmd, addr, next, newprot,\n\t\t\t\t       cp_flags);\n\t\tif (ret < 0)\n\t\t\tgoto again;\n\t\tpages += ret;\nnext:\n\t\tcond_resched();\n\t} while (pmd++, addr = next, addr != end);\n\n\tif (range.start)\n\t\tmmu_notifier_invalidate_range_end(&range);\n\n\tif (nr_huge_updates)\n\t\tcount_vm_numa_events(NUMA_HUGE_PTE_UPDATES, nr_huge_updates);\n\treturn pages;\n}\n\nstatic inline long change_pud_range(struct mmu_gather *tlb,\n\t\tstruct vm_area_struct *vma, p4d_t *p4d, unsigned long addr,\n\t\tunsigned long end, pgprot_t newprot, unsigned long cp_flags)\n{\n\tpud_t *pud;\n\tunsigned long next;\n\tlong pages = 0, ret;\n\n\tpud = pud_offset(p4d, addr);\n\tdo {\n\t\tnext = pud_addr_end(addr, end);\n\t\tret = change_prepare(vma, pud, pmd, addr, cp_flags);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tif (pud_none_or_clear_bad(pud))\n\t\t\tcontinue;\n\t\tpages += change_pmd_range(tlb, vma, pud, addr, next, newprot,\n\t\t\t\t\t  cp_flags);\n\t} while (pud++, addr = next, addr != end);\n\n\treturn pages;\n}\n\nstatic inline long change_p4d_range(struct mmu_gather *tlb,\n\t\tstruct vm_area_struct *vma, pgd_t *pgd, unsigned long addr,\n\t\tunsigned long end, pgprot_t newprot, unsigned long cp_flags)\n{\n\tp4d_t *p4d;\n\tunsigned long next;\n\tlong pages = 0, ret;\n\n\tp4d = p4d_offset(pgd, addr);\n\tdo {\n\t\tnext = p4d_addr_end(addr, end);\n\t\tret = change_prepare(vma, p4d, pud, addr, cp_flags);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tif (p4d_none_or_clear_bad(p4d))\n\t\t\tcontinue;\n\t\tpages += change_pud_range(tlb, vma, p4d, addr, next, newprot,\n\t\t\t\t\t  cp_flags);\n\t} while (p4d++, addr = next, addr != end);\n\n\treturn pages;\n}\n\nstatic long change_protection_range(struct mmu_gather *tlb,\n\t\tstruct vm_area_struct *vma, unsigned long addr,\n\t\tunsigned long end, pgprot_t newprot, unsigned long cp_flags)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tpgd_t *pgd;\n\tunsigned long next;\n\tlong pages = 0, ret;\n\n\tBUG_ON(addr >= end);\n\tpgd = pgd_offset(mm, addr);\n\ttlb_start_vma(tlb, vma);\n\tdo {\n\t\tnext = pgd_addr_end(addr, end);\n\t\tret = change_prepare(vma, pgd, p4d, addr, cp_flags);\n\t\tif (ret) {\n\t\t\tpages = ret;\n\t\t\tbreak;\n\t\t}\n\t\tif (pgd_none_or_clear_bad(pgd))\n\t\t\tcontinue;\n\t\tpages += change_p4d_range(tlb, vma, pgd, addr, next, newprot,\n\t\t\t\t\t  cp_flags);\n\t} while (pgd++, addr = next, addr != end);\n\n\ttlb_end_vma(tlb, vma);\n\n\treturn pages;\n}\n\nlong change_protection(struct mmu_gather *tlb,\n\t\t       struct vm_area_struct *vma, unsigned long start,\n\t\t       unsigned long end, unsigned long cp_flags)\n{\n\tpgprot_t newprot = vma->vm_page_prot;\n\tlong pages;\n\n\tBUG_ON((cp_flags & MM_CP_UFFD_WP_ALL) == MM_CP_UFFD_WP_ALL);\n\n#ifdef CONFIG_NUMA_BALANCING\n\t \n\tif (cp_flags & MM_CP_PROT_NUMA)\n\t\tnewprot = PAGE_NONE;\n#else\n\tWARN_ON_ONCE(cp_flags & MM_CP_PROT_NUMA);\n#endif\n\n\tif (is_vm_hugetlb_page(vma))\n\t\tpages = hugetlb_change_protection(vma, start, end, newprot,\n\t\t\t\t\t\t  cp_flags);\n\telse\n\t\tpages = change_protection_range(tlb, vma, start, end, newprot,\n\t\t\t\t\t\tcp_flags);\n\n\treturn pages;\n}\n\nstatic int prot_none_pte_entry(pte_t *pte, unsigned long addr,\n\t\t\t       unsigned long next, struct mm_walk *walk)\n{\n\treturn pfn_modify_allowed(pte_pfn(ptep_get(pte)),\n\t\t\t\t  *(pgprot_t *)(walk->private)) ?\n\t\t0 : -EACCES;\n}\n\nstatic int prot_none_hugetlb_entry(pte_t *pte, unsigned long hmask,\n\t\t\t\t   unsigned long addr, unsigned long next,\n\t\t\t\t   struct mm_walk *walk)\n{\n\treturn pfn_modify_allowed(pte_pfn(ptep_get(pte)),\n\t\t\t\t  *(pgprot_t *)(walk->private)) ?\n\t\t0 : -EACCES;\n}\n\nstatic int prot_none_test(unsigned long addr, unsigned long next,\n\t\t\t  struct mm_walk *walk)\n{\n\treturn 0;\n}\n\nstatic const struct mm_walk_ops prot_none_walk_ops = {\n\t.pte_entry\t\t= prot_none_pte_entry,\n\t.hugetlb_entry\t\t= prot_none_hugetlb_entry,\n\t.test_walk\t\t= prot_none_test,\n\t.walk_lock\t\t= PGWALK_WRLOCK,\n};\n\nint\nmprotect_fixup(struct vma_iterator *vmi, struct mmu_gather *tlb,\n\t       struct vm_area_struct *vma, struct vm_area_struct **pprev,\n\t       unsigned long start, unsigned long end, unsigned long newflags)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long oldflags = vma->vm_flags;\n\tlong nrpages = (end - start) >> PAGE_SHIFT;\n\tunsigned int mm_cp_flags = 0;\n\tunsigned long charged = 0;\n\tpgoff_t pgoff;\n\tint error;\n\n\tif (newflags == oldflags) {\n\t\t*pprev = vma;\n\t\treturn 0;\n\t}\n\n\t \n\tif (arch_has_pfn_modify_check() &&\n\t    (vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP)) &&\n\t    (newflags & VM_ACCESS_FLAGS) == 0) {\n\t\tpgprot_t new_pgprot = vm_get_page_prot(newflags);\n\n\t\terror = walk_page_range(current->mm, start, end,\n\t\t\t\t&prot_none_walk_ops, &new_pgprot);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\n\t \n\tif (newflags & VM_WRITE) {\n\t\t \n\t\tif (!may_expand_vm(mm, newflags, nrpages) &&\n\t\t\t\tmay_expand_vm(mm, oldflags, nrpages))\n\t\t\treturn -ENOMEM;\n\t\tif (!(oldflags & (VM_ACCOUNT|VM_WRITE|VM_HUGETLB|\n\t\t\t\t\t\tVM_SHARED|VM_NORESERVE))) {\n\t\t\tcharged = nrpages;\n\t\t\tif (security_vm_enough_memory_mm(mm, charged))\n\t\t\t\treturn -ENOMEM;\n\t\t\tnewflags |= VM_ACCOUNT;\n\t\t}\n\t}\n\n\t \n\tpgoff = vma->vm_pgoff + ((start - vma->vm_start) >> PAGE_SHIFT);\n\t*pprev = vma_merge(vmi, mm, *pprev, start, end, newflags,\n\t\t\t   vma->anon_vma, vma->vm_file, pgoff, vma_policy(vma),\n\t\t\t   vma->vm_userfaultfd_ctx, anon_vma_name(vma));\n\tif (*pprev) {\n\t\tvma = *pprev;\n\t\tVM_WARN_ON((vma->vm_flags ^ newflags) & ~VM_SOFTDIRTY);\n\t\tgoto success;\n\t}\n\n\t*pprev = vma;\n\n\tif (start != vma->vm_start) {\n\t\terror = split_vma(vmi, vma, start, 1);\n\t\tif (error)\n\t\t\tgoto fail;\n\t}\n\n\tif (end != vma->vm_end) {\n\t\terror = split_vma(vmi, vma, end, 0);\n\t\tif (error)\n\t\t\tgoto fail;\n\t}\n\nsuccess:\n\t \n\tvma_start_write(vma);\n\tvm_flags_reset(vma, newflags);\n\tif (vma_wants_manual_pte_write_upgrade(vma))\n\t\tmm_cp_flags |= MM_CP_TRY_CHANGE_WRITABLE;\n\tvma_set_page_prot(vma);\n\n\tchange_protection(tlb, vma, start, end, mm_cp_flags);\n\n\t \n\tif ((oldflags & (VM_WRITE | VM_SHARED | VM_LOCKED)) == VM_LOCKED &&\n\t\t\t(newflags & VM_WRITE)) {\n\t\tpopulate_vma_page_range(vma, start, end, NULL);\n\t}\n\n\tvm_stat_account(mm, oldflags, -nrpages);\n\tvm_stat_account(mm, newflags, nrpages);\n\tperf_event_mmap(vma);\n\treturn 0;\n\nfail:\n\tvm_unacct_memory(charged);\n\treturn error;\n}\n\n \nstatic int do_mprotect_pkey(unsigned long start, size_t len,\n\t\tunsigned long prot, int pkey)\n{\n\tunsigned long nstart, end, tmp, reqprot;\n\tstruct vm_area_struct *vma, *prev;\n\tint error;\n\tconst int grows = prot & (PROT_GROWSDOWN|PROT_GROWSUP);\n\tconst bool rier = (current->personality & READ_IMPLIES_EXEC) &&\n\t\t\t\t(prot & PROT_READ);\n\tstruct mmu_gather tlb;\n\tstruct vma_iterator vmi;\n\n\tstart = untagged_addr(start);\n\n\tprot &= ~(PROT_GROWSDOWN|PROT_GROWSUP);\n\tif (grows == (PROT_GROWSDOWN|PROT_GROWSUP))  \n\t\treturn -EINVAL;\n\n\tif (start & ~PAGE_MASK)\n\t\treturn -EINVAL;\n\tif (!len)\n\t\treturn 0;\n\tlen = PAGE_ALIGN(len);\n\tend = start + len;\n\tif (end <= start)\n\t\treturn -ENOMEM;\n\tif (!arch_validate_prot(prot, start))\n\t\treturn -EINVAL;\n\n\treqprot = prot;\n\n\tif (mmap_write_lock_killable(current->mm))\n\t\treturn -EINTR;\n\n\t \n\terror = -EINVAL;\n\tif ((pkey != -1) && !mm_pkey_is_allocated(current->mm, pkey))\n\t\tgoto out;\n\n\tvma_iter_init(&vmi, current->mm, start);\n\tvma = vma_find(&vmi, end);\n\terror = -ENOMEM;\n\tif (!vma)\n\t\tgoto out;\n\n\tif (unlikely(grows & PROT_GROWSDOWN)) {\n\t\tif (vma->vm_start >= end)\n\t\t\tgoto out;\n\t\tstart = vma->vm_start;\n\t\terror = -EINVAL;\n\t\tif (!(vma->vm_flags & VM_GROWSDOWN))\n\t\t\tgoto out;\n\t} else {\n\t\tif (vma->vm_start > start)\n\t\t\tgoto out;\n\t\tif (unlikely(grows & PROT_GROWSUP)) {\n\t\t\tend = vma->vm_end;\n\t\t\terror = -EINVAL;\n\t\t\tif (!(vma->vm_flags & VM_GROWSUP))\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\n\tprev = vma_prev(&vmi);\n\tif (start > vma->vm_start)\n\t\tprev = vma;\n\n\ttlb_gather_mmu(&tlb, current->mm);\n\tnstart = start;\n\ttmp = vma->vm_start;\n\tfor_each_vma_range(vmi, vma, end) {\n\t\tunsigned long mask_off_old_flags;\n\t\tunsigned long newflags;\n\t\tint new_vma_pkey;\n\n\t\tif (vma->vm_start != tmp) {\n\t\t\terror = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (rier && (vma->vm_flags & VM_MAYEXEC))\n\t\t\tprot |= PROT_EXEC;\n\n\t\t \n\t\tmask_off_old_flags = VM_ACCESS_FLAGS | VM_FLAGS_CLEAR;\n\n\t\tnew_vma_pkey = arch_override_mprotect_pkey(vma, prot, pkey);\n\t\tnewflags = calc_vm_prot_bits(prot, new_vma_pkey);\n\t\tnewflags |= (vma->vm_flags & ~mask_off_old_flags);\n\n\t\t \n\t\tif ((newflags & ~(newflags >> 4)) & VM_ACCESS_FLAGS) {\n\t\t\terror = -EACCES;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (map_deny_write_exec(vma, newflags)) {\n\t\t\terror = -EACCES;\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (!arch_validate_flags(newflags)) {\n\t\t\terror = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\terror = security_file_mprotect(vma, reqprot, prot);\n\t\tif (error)\n\t\t\tbreak;\n\n\t\ttmp = vma->vm_end;\n\t\tif (tmp > end)\n\t\t\ttmp = end;\n\n\t\tif (vma->vm_ops && vma->vm_ops->mprotect) {\n\t\t\terror = vma->vm_ops->mprotect(vma, nstart, tmp, newflags);\n\t\t\tif (error)\n\t\t\t\tbreak;\n\t\t}\n\n\t\terror = mprotect_fixup(&vmi, &tlb, vma, &prev, nstart, tmp, newflags);\n\t\tif (error)\n\t\t\tbreak;\n\n\t\ttmp = vma_iter_end(&vmi);\n\t\tnstart = tmp;\n\t\tprot = reqprot;\n\t}\n\ttlb_finish_mmu(&tlb);\n\n\tif (!error && tmp < end)\n\t\terror = -ENOMEM;\n\nout:\n\tmmap_write_unlock(current->mm);\n\treturn error;\n}\n\nSYSCALL_DEFINE3(mprotect, unsigned long, start, size_t, len,\n\t\tunsigned long, prot)\n{\n\treturn do_mprotect_pkey(start, len, prot, -1);\n}\n\n#ifdef CONFIG_ARCH_HAS_PKEYS\n\nSYSCALL_DEFINE4(pkey_mprotect, unsigned long, start, size_t, len,\n\t\tunsigned long, prot, int, pkey)\n{\n\treturn do_mprotect_pkey(start, len, prot, pkey);\n}\n\nSYSCALL_DEFINE2(pkey_alloc, unsigned long, flags, unsigned long, init_val)\n{\n\tint pkey;\n\tint ret;\n\n\t \n\tif (flags)\n\t\treturn -EINVAL;\n\t \n\tif (init_val & ~PKEY_ACCESS_MASK)\n\t\treturn -EINVAL;\n\n\tmmap_write_lock(current->mm);\n\tpkey = mm_pkey_alloc(current->mm);\n\n\tret = -ENOSPC;\n\tif (pkey == -1)\n\t\tgoto out;\n\n\tret = arch_set_user_pkey_access(current, pkey, init_val);\n\tif (ret) {\n\t\tmm_pkey_free(current->mm, pkey);\n\t\tgoto out;\n\t}\n\tret = pkey;\nout:\n\tmmap_write_unlock(current->mm);\n\treturn ret;\n}\n\nSYSCALL_DEFINE1(pkey_free, int, pkey)\n{\n\tint ret;\n\n\tmmap_write_lock(current->mm);\n\tret = mm_pkey_free(current->mm, pkey);\n\tmmap_write_unlock(current->mm);\n\n\t \n\treturn ret;\n}\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}