{
  "module_name": "list_lru.c",
  "hash_id": "c2a46b8a62dd3c89895a7a970ad53059ce07ff53581bf7cc5c3b6535e12ef202",
  "original_prompt": "Ingested from linux-6.6.14/mm/list_lru.c",
  "human_readable_source": "\n \n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/mm.h>\n#include <linux/list_lru.h>\n#include <linux/slab.h>\n#include <linux/mutex.h>\n#include <linux/memcontrol.h>\n#include \"slab.h\"\n#include \"internal.h\"\n\n#ifdef CONFIG_MEMCG_KMEM\nstatic LIST_HEAD(memcg_list_lrus);\nstatic DEFINE_MUTEX(list_lrus_mutex);\n\nstatic inline bool list_lru_memcg_aware(struct list_lru *lru)\n{\n\treturn lru->memcg_aware;\n}\n\nstatic void list_lru_register(struct list_lru *lru)\n{\n\tif (!list_lru_memcg_aware(lru))\n\t\treturn;\n\n\tmutex_lock(&list_lrus_mutex);\n\tlist_add(&lru->list, &memcg_list_lrus);\n\tmutex_unlock(&list_lrus_mutex);\n}\n\nstatic void list_lru_unregister(struct list_lru *lru)\n{\n\tif (!list_lru_memcg_aware(lru))\n\t\treturn;\n\n\tmutex_lock(&list_lrus_mutex);\n\tlist_del(&lru->list);\n\tmutex_unlock(&list_lrus_mutex);\n}\n\nstatic int lru_shrinker_id(struct list_lru *lru)\n{\n\treturn lru->shrinker_id;\n}\n\nstatic inline struct list_lru_one *\nlist_lru_from_memcg_idx(struct list_lru *lru, int nid, int idx)\n{\n\tif (list_lru_memcg_aware(lru) && idx >= 0) {\n\t\tstruct list_lru_memcg *mlru = xa_load(&lru->xa, idx);\n\n\t\treturn mlru ? &mlru->node[nid] : NULL;\n\t}\n\treturn &lru->node[nid].lru;\n}\n\nstatic inline struct list_lru_one *\nlist_lru_from_kmem(struct list_lru *lru, int nid, void *ptr,\n\t\t   struct mem_cgroup **memcg_ptr)\n{\n\tstruct list_lru_node *nlru = &lru->node[nid];\n\tstruct list_lru_one *l = &nlru->lru;\n\tstruct mem_cgroup *memcg = NULL;\n\n\tif (!list_lru_memcg_aware(lru))\n\t\tgoto out;\n\n\tmemcg = mem_cgroup_from_slab_obj(ptr);\n\tif (!memcg)\n\t\tgoto out;\n\n\tl = list_lru_from_memcg_idx(lru, nid, memcg_kmem_id(memcg));\nout:\n\tif (memcg_ptr)\n\t\t*memcg_ptr = memcg;\n\treturn l;\n}\n#else\nstatic void list_lru_register(struct list_lru *lru)\n{\n}\n\nstatic void list_lru_unregister(struct list_lru *lru)\n{\n}\n\nstatic int lru_shrinker_id(struct list_lru *lru)\n{\n\treturn -1;\n}\n\nstatic inline bool list_lru_memcg_aware(struct list_lru *lru)\n{\n\treturn false;\n}\n\nstatic inline struct list_lru_one *\nlist_lru_from_memcg_idx(struct list_lru *lru, int nid, int idx)\n{\n\treturn &lru->node[nid].lru;\n}\n\nstatic inline struct list_lru_one *\nlist_lru_from_kmem(struct list_lru *lru, int nid, void *ptr,\n\t\t   struct mem_cgroup **memcg_ptr)\n{\n\tif (memcg_ptr)\n\t\t*memcg_ptr = NULL;\n\treturn &lru->node[nid].lru;\n}\n#endif  \n\nbool list_lru_add(struct list_lru *lru, struct list_head *item)\n{\n\tint nid = page_to_nid(virt_to_page(item));\n\tstruct list_lru_node *nlru = &lru->node[nid];\n\tstruct mem_cgroup *memcg;\n\tstruct list_lru_one *l;\n\n\tspin_lock(&nlru->lock);\n\tif (list_empty(item)) {\n\t\tl = list_lru_from_kmem(lru, nid, item, &memcg);\n\t\tlist_add_tail(item, &l->list);\n\t\t \n\t\tif (!l->nr_items++)\n\t\t\tset_shrinker_bit(memcg, nid,\n\t\t\t\t\t lru_shrinker_id(lru));\n\t\tnlru->nr_items++;\n\t\tspin_unlock(&nlru->lock);\n\t\treturn true;\n\t}\n\tspin_unlock(&nlru->lock);\n\treturn false;\n}\nEXPORT_SYMBOL_GPL(list_lru_add);\n\nbool list_lru_del(struct list_lru *lru, struct list_head *item)\n{\n\tint nid = page_to_nid(virt_to_page(item));\n\tstruct list_lru_node *nlru = &lru->node[nid];\n\tstruct list_lru_one *l;\n\n\tspin_lock(&nlru->lock);\n\tif (!list_empty(item)) {\n\t\tl = list_lru_from_kmem(lru, nid, item, NULL);\n\t\tlist_del_init(item);\n\t\tl->nr_items--;\n\t\tnlru->nr_items--;\n\t\tspin_unlock(&nlru->lock);\n\t\treturn true;\n\t}\n\tspin_unlock(&nlru->lock);\n\treturn false;\n}\nEXPORT_SYMBOL_GPL(list_lru_del);\n\nvoid list_lru_isolate(struct list_lru_one *list, struct list_head *item)\n{\n\tlist_del_init(item);\n\tlist->nr_items--;\n}\nEXPORT_SYMBOL_GPL(list_lru_isolate);\n\nvoid list_lru_isolate_move(struct list_lru_one *list, struct list_head *item,\n\t\t\t   struct list_head *head)\n{\n\tlist_move(item, head);\n\tlist->nr_items--;\n}\nEXPORT_SYMBOL_GPL(list_lru_isolate_move);\n\nunsigned long list_lru_count_one(struct list_lru *lru,\n\t\t\t\t int nid, struct mem_cgroup *memcg)\n{\n\tstruct list_lru_one *l;\n\tlong count;\n\n\trcu_read_lock();\n\tl = list_lru_from_memcg_idx(lru, nid, memcg_kmem_id(memcg));\n\tcount = l ? READ_ONCE(l->nr_items) : 0;\n\trcu_read_unlock();\n\n\tif (unlikely(count < 0))\n\t\tcount = 0;\n\n\treturn count;\n}\nEXPORT_SYMBOL_GPL(list_lru_count_one);\n\nunsigned long list_lru_count_node(struct list_lru *lru, int nid)\n{\n\tstruct list_lru_node *nlru;\n\n\tnlru = &lru->node[nid];\n\treturn nlru->nr_items;\n}\nEXPORT_SYMBOL_GPL(list_lru_count_node);\n\nstatic unsigned long\n__list_lru_walk_one(struct list_lru *lru, int nid, int memcg_idx,\n\t\t    list_lru_walk_cb isolate, void *cb_arg,\n\t\t    unsigned long *nr_to_walk)\n{\n\tstruct list_lru_node *nlru = &lru->node[nid];\n\tstruct list_lru_one *l;\n\tstruct list_head *item, *n;\n\tunsigned long isolated = 0;\n\nrestart:\n\tl = list_lru_from_memcg_idx(lru, nid, memcg_idx);\n\tif (!l)\n\t\tgoto out;\n\n\tlist_for_each_safe(item, n, &l->list) {\n\t\tenum lru_status ret;\n\n\t\t \n\t\tif (!*nr_to_walk)\n\t\t\tbreak;\n\t\t--*nr_to_walk;\n\n\t\tret = isolate(item, l, &nlru->lock, cb_arg);\n\t\tswitch (ret) {\n\t\tcase LRU_REMOVED_RETRY:\n\t\t\tassert_spin_locked(&nlru->lock);\n\t\t\tfallthrough;\n\t\tcase LRU_REMOVED:\n\t\t\tisolated++;\n\t\t\tnlru->nr_items--;\n\t\t\t \n\t\t\tif (ret == LRU_REMOVED_RETRY)\n\t\t\t\tgoto restart;\n\t\t\tbreak;\n\t\tcase LRU_ROTATE:\n\t\t\tlist_move_tail(item, &l->list);\n\t\t\tbreak;\n\t\tcase LRU_SKIP:\n\t\t\tbreak;\n\t\tcase LRU_RETRY:\n\t\t\t \n\t\t\tassert_spin_locked(&nlru->lock);\n\t\t\tgoto restart;\n\t\tdefault:\n\t\t\tBUG();\n\t\t}\n\t}\nout:\n\treturn isolated;\n}\n\nunsigned long\nlist_lru_walk_one(struct list_lru *lru, int nid, struct mem_cgroup *memcg,\n\t\t  list_lru_walk_cb isolate, void *cb_arg,\n\t\t  unsigned long *nr_to_walk)\n{\n\tstruct list_lru_node *nlru = &lru->node[nid];\n\tunsigned long ret;\n\n\tspin_lock(&nlru->lock);\n\tret = __list_lru_walk_one(lru, nid, memcg_kmem_id(memcg), isolate,\n\t\t\t\t  cb_arg, nr_to_walk);\n\tspin_unlock(&nlru->lock);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(list_lru_walk_one);\n\nunsigned long\nlist_lru_walk_one_irq(struct list_lru *lru, int nid, struct mem_cgroup *memcg,\n\t\t      list_lru_walk_cb isolate, void *cb_arg,\n\t\t      unsigned long *nr_to_walk)\n{\n\tstruct list_lru_node *nlru = &lru->node[nid];\n\tunsigned long ret;\n\n\tspin_lock_irq(&nlru->lock);\n\tret = __list_lru_walk_one(lru, nid, memcg_kmem_id(memcg), isolate,\n\t\t\t\t  cb_arg, nr_to_walk);\n\tspin_unlock_irq(&nlru->lock);\n\treturn ret;\n}\n\nunsigned long list_lru_walk_node(struct list_lru *lru, int nid,\n\t\t\t\t list_lru_walk_cb isolate, void *cb_arg,\n\t\t\t\t unsigned long *nr_to_walk)\n{\n\tlong isolated = 0;\n\n\tisolated += list_lru_walk_one(lru, nid, NULL, isolate, cb_arg,\n\t\t\t\t      nr_to_walk);\n\n#ifdef CONFIG_MEMCG_KMEM\n\tif (*nr_to_walk > 0 && list_lru_memcg_aware(lru)) {\n\t\tstruct list_lru_memcg *mlru;\n\t\tunsigned long index;\n\n\t\txa_for_each(&lru->xa, index, mlru) {\n\t\t\tstruct list_lru_node *nlru = &lru->node[nid];\n\n\t\t\tspin_lock(&nlru->lock);\n\t\t\tisolated += __list_lru_walk_one(lru, nid, index,\n\t\t\t\t\t\t\tisolate, cb_arg,\n\t\t\t\t\t\t\tnr_to_walk);\n\t\t\tspin_unlock(&nlru->lock);\n\n\t\t\tif (*nr_to_walk <= 0)\n\t\t\t\tbreak;\n\t\t}\n\t}\n#endif\n\n\treturn isolated;\n}\nEXPORT_SYMBOL_GPL(list_lru_walk_node);\n\nstatic void init_one_lru(struct list_lru_one *l)\n{\n\tINIT_LIST_HEAD(&l->list);\n\tl->nr_items = 0;\n}\n\n#ifdef CONFIG_MEMCG_KMEM\nstatic struct list_lru_memcg *memcg_init_list_lru_one(gfp_t gfp)\n{\n\tint nid;\n\tstruct list_lru_memcg *mlru;\n\n\tmlru = kmalloc(struct_size(mlru, node, nr_node_ids), gfp);\n\tif (!mlru)\n\t\treturn NULL;\n\n\tfor_each_node(nid)\n\t\tinit_one_lru(&mlru->node[nid]);\n\n\treturn mlru;\n}\n\nstatic void memcg_list_lru_free(struct list_lru *lru, int src_idx)\n{\n\tstruct list_lru_memcg *mlru = xa_erase_irq(&lru->xa, src_idx);\n\n\t \n\tif (mlru)\n\t\tkvfree_rcu(mlru, rcu);\n}\n\nstatic inline void memcg_init_list_lru(struct list_lru *lru, bool memcg_aware)\n{\n\tif (memcg_aware)\n\t\txa_init_flags(&lru->xa, XA_FLAGS_LOCK_IRQ);\n\tlru->memcg_aware = memcg_aware;\n}\n\nstatic void memcg_destroy_list_lru(struct list_lru *lru)\n{\n\tXA_STATE(xas, &lru->xa, 0);\n\tstruct list_lru_memcg *mlru;\n\n\tif (!list_lru_memcg_aware(lru))\n\t\treturn;\n\n\txas_lock_irq(&xas);\n\txas_for_each(&xas, mlru, ULONG_MAX) {\n\t\tkfree(mlru);\n\t\txas_store(&xas, NULL);\n\t}\n\txas_unlock_irq(&xas);\n}\n\nstatic void memcg_reparent_list_lru_node(struct list_lru *lru, int nid,\n\t\t\t\t\t int src_idx, struct mem_cgroup *dst_memcg)\n{\n\tstruct list_lru_node *nlru = &lru->node[nid];\n\tint dst_idx = dst_memcg->kmemcg_id;\n\tstruct list_lru_one *src, *dst;\n\n\t \n\tspin_lock_irq(&nlru->lock);\n\n\tsrc = list_lru_from_memcg_idx(lru, nid, src_idx);\n\tif (!src)\n\t\tgoto out;\n\tdst = list_lru_from_memcg_idx(lru, nid, dst_idx);\n\n\tlist_splice_init(&src->list, &dst->list);\n\n\tif (src->nr_items) {\n\t\tdst->nr_items += src->nr_items;\n\t\tset_shrinker_bit(dst_memcg, nid, lru_shrinker_id(lru));\n\t\tsrc->nr_items = 0;\n\t}\nout:\n\tspin_unlock_irq(&nlru->lock);\n}\n\nstatic void memcg_reparent_list_lru(struct list_lru *lru,\n\t\t\t\t    int src_idx, struct mem_cgroup *dst_memcg)\n{\n\tint i;\n\n\tfor_each_node(i)\n\t\tmemcg_reparent_list_lru_node(lru, i, src_idx, dst_memcg);\n\n\tmemcg_list_lru_free(lru, src_idx);\n}\n\nvoid memcg_reparent_list_lrus(struct mem_cgroup *memcg, struct mem_cgroup *parent)\n{\n\tstruct cgroup_subsys_state *css;\n\tstruct list_lru *lru;\n\tint src_idx = memcg->kmemcg_id;\n\n\t \n\trcu_read_lock();\n\tcss_for_each_descendant_pre(css, &memcg->css) {\n\t\tstruct mem_cgroup *child;\n\n\t\tchild = mem_cgroup_from_css(css);\n\t\tWRITE_ONCE(child->kmemcg_id, parent->kmemcg_id);\n\t}\n\trcu_read_unlock();\n\n\tmutex_lock(&list_lrus_mutex);\n\tlist_for_each_entry(lru, &memcg_list_lrus, list)\n\t\tmemcg_reparent_list_lru(lru, src_idx, parent);\n\tmutex_unlock(&list_lrus_mutex);\n}\n\nstatic inline bool memcg_list_lru_allocated(struct mem_cgroup *memcg,\n\t\t\t\t\t    struct list_lru *lru)\n{\n\tint idx = memcg->kmemcg_id;\n\n\treturn idx < 0 || xa_load(&lru->xa, idx);\n}\n\nint memcg_list_lru_alloc(struct mem_cgroup *memcg, struct list_lru *lru,\n\t\t\t gfp_t gfp)\n{\n\tint i;\n\tunsigned long flags;\n\tstruct list_lru_memcg_table {\n\t\tstruct list_lru_memcg *mlru;\n\t\tstruct mem_cgroup *memcg;\n\t} *table;\n\tXA_STATE(xas, &lru->xa, 0);\n\n\tif (!list_lru_memcg_aware(lru) || memcg_list_lru_allocated(memcg, lru))\n\t\treturn 0;\n\n\tgfp &= GFP_RECLAIM_MASK;\n\ttable = kmalloc_array(memcg->css.cgroup->level, sizeof(*table), gfp);\n\tif (!table)\n\t\treturn -ENOMEM;\n\n\t \n\tfor (i = 0; memcg; memcg = parent_mem_cgroup(memcg), i++) {\n\t\tif (memcg_list_lru_allocated(memcg, lru))\n\t\t\tbreak;\n\n\t\ttable[i].memcg = memcg;\n\t\ttable[i].mlru = memcg_init_list_lru_one(gfp);\n\t\tif (!table[i].mlru) {\n\t\t\twhile (i--)\n\t\t\t\tkfree(table[i].mlru);\n\t\t\tkfree(table);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\txas_lock_irqsave(&xas, flags);\n\twhile (i--) {\n\t\tint index = READ_ONCE(table[i].memcg->kmemcg_id);\n\t\tstruct list_lru_memcg *mlru = table[i].mlru;\n\n\t\txas_set(&xas, index);\nretry:\n\t\tif (unlikely(index < 0 || xas_error(&xas) || xas_load(&xas))) {\n\t\t\tkfree(mlru);\n\t\t} else {\n\t\t\txas_store(&xas, mlru);\n\t\t\tif (xas_error(&xas) == -ENOMEM) {\n\t\t\t\txas_unlock_irqrestore(&xas, flags);\n\t\t\t\tif (xas_nomem(&xas, gfp))\n\t\t\t\t\txas_set_err(&xas, 0);\n\t\t\t\txas_lock_irqsave(&xas, flags);\n\t\t\t\t \n\t\t\t\tindex = READ_ONCE(table[i].memcg->kmemcg_id);\n\t\t\t\tif (index < 0)\n\t\t\t\t\txas_set_err(&xas, 0);\n\t\t\t\telse if (!xas_error(&xas) && index != xas.xa_index)\n\t\t\t\t\txas_set(&xas, index);\n\t\t\t\tgoto retry;\n\t\t\t}\n\t\t}\n\t}\n\t \n\tif (xas.xa_alloc)\n\t\txas_nomem(&xas, gfp);\n\txas_unlock_irqrestore(&xas, flags);\n\tkfree(table);\n\n\treturn xas_error(&xas);\n}\n#else\nstatic inline void memcg_init_list_lru(struct list_lru *lru, bool memcg_aware)\n{\n}\n\nstatic void memcg_destroy_list_lru(struct list_lru *lru)\n{\n}\n#endif  \n\nint __list_lru_init(struct list_lru *lru, bool memcg_aware,\n\t\t    struct lock_class_key *key, struct shrinker *shrinker)\n{\n\tint i;\n\n#ifdef CONFIG_MEMCG_KMEM\n\tif (shrinker)\n\t\tlru->shrinker_id = shrinker->id;\n\telse\n\t\tlru->shrinker_id = -1;\n#endif\n\n\tlru->node = kcalloc(nr_node_ids, sizeof(*lru->node), GFP_KERNEL);\n\tif (!lru->node)\n\t\treturn -ENOMEM;\n\n\tfor_each_node(i) {\n\t\tspin_lock_init(&lru->node[i].lock);\n\t\tif (key)\n\t\t\tlockdep_set_class(&lru->node[i].lock, key);\n\t\tinit_one_lru(&lru->node[i].lru);\n\t}\n\n\tmemcg_init_list_lru(lru, memcg_aware);\n\tlist_lru_register(lru);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(__list_lru_init);\n\nvoid list_lru_destroy(struct list_lru *lru)\n{\n\t \n\tif (!lru->node)\n\t\treturn;\n\n\tlist_lru_unregister(lru);\n\n\tmemcg_destroy_list_lru(lru);\n\tkfree(lru->node);\n\tlru->node = NULL;\n\n#ifdef CONFIG_MEMCG_KMEM\n\tlru->shrinker_id = -1;\n#endif\n}\nEXPORT_SYMBOL_GPL(list_lru_destroy);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}