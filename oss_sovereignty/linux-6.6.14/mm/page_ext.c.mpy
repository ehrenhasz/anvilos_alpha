{
  "module_name": "page_ext.c",
  "hash_id": "7c378e0d76a52316e97f6be1f0cb13974aff6f7c4789db8859b05af61402d56b",
  "original_prompt": "Ingested from linux-6.6.14/mm/page_ext.c",
  "human_readable_source": "\n#include <linux/mm.h>\n#include <linux/mmzone.h>\n#include <linux/memblock.h>\n#include <linux/page_ext.h>\n#include <linux/memory.h>\n#include <linux/vmalloc.h>\n#include <linux/kmemleak.h>\n#include <linux/page_owner.h>\n#include <linux/page_idle.h>\n#include <linux/page_table_check.h>\n#include <linux/rcupdate.h>\n\n \n\n#ifdef CONFIG_SPARSEMEM\n#define PAGE_EXT_INVALID       (0x1)\n#endif\n\n#if defined(CONFIG_PAGE_IDLE_FLAG) && !defined(CONFIG_64BIT)\nstatic bool need_page_idle(void)\n{\n\treturn true;\n}\nstatic struct page_ext_operations page_idle_ops __initdata = {\n\t.need = need_page_idle,\n\t.need_shared_flags = true,\n};\n#endif\n\nstatic struct page_ext_operations *page_ext_ops[] __initdata = {\n#ifdef CONFIG_PAGE_OWNER\n\t&page_owner_ops,\n#endif\n#if defined(CONFIG_PAGE_IDLE_FLAG) && !defined(CONFIG_64BIT)\n\t&page_idle_ops,\n#endif\n#ifdef CONFIG_PAGE_TABLE_CHECK\n\t&page_table_check_ops,\n#endif\n};\n\nunsigned long page_ext_size;\n\nstatic unsigned long total_usage;\n\nbool early_page_ext __meminitdata;\nstatic int __init setup_early_page_ext(char *str)\n{\n\tearly_page_ext = true;\n\treturn 0;\n}\nearly_param(\"early_page_ext\", setup_early_page_ext);\n\nstatic bool __init invoke_need_callbacks(void)\n{\n\tint i;\n\tint entries = ARRAY_SIZE(page_ext_ops);\n\tbool need = false;\n\n\tfor (i = 0; i < entries; i++) {\n\t\tif (page_ext_ops[i]->need()) {\n\t\t\tif (page_ext_ops[i]->need_shared_flags) {\n\t\t\t\tpage_ext_size = sizeof(struct page_ext);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tfor (i = 0; i < entries; i++) {\n\t\tif (page_ext_ops[i]->need()) {\n\t\t\tpage_ext_ops[i]->offset = page_ext_size;\n\t\t\tpage_ext_size += page_ext_ops[i]->size;\n\t\t\tneed = true;\n\t\t}\n\t}\n\n\treturn need;\n}\n\nstatic void __init invoke_init_callbacks(void)\n{\n\tint i;\n\tint entries = ARRAY_SIZE(page_ext_ops);\n\n\tfor (i = 0; i < entries; i++) {\n\t\tif (page_ext_ops[i]->init)\n\t\t\tpage_ext_ops[i]->init();\n\t}\n}\n\nstatic inline struct page_ext *get_entry(void *base, unsigned long index)\n{\n\treturn base + page_ext_size * index;\n}\n\n#ifndef CONFIG_SPARSEMEM\nvoid __init page_ext_init_flatmem_late(void)\n{\n\tinvoke_init_callbacks();\n}\n\nvoid __meminit pgdat_page_ext_init(struct pglist_data *pgdat)\n{\n\tpgdat->node_page_ext = NULL;\n}\n\nstatic struct page_ext *lookup_page_ext(const struct page *page)\n{\n\tunsigned long pfn = page_to_pfn(page);\n\tunsigned long index;\n\tstruct page_ext *base;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\tbase = NODE_DATA(page_to_nid(page))->node_page_ext;\n\t \n\tif (unlikely(!base))\n\t\treturn NULL;\n\tindex = pfn - round_down(node_start_pfn(page_to_nid(page)),\n\t\t\t\t\tMAX_ORDER_NR_PAGES);\n\treturn get_entry(base, index);\n}\n\nstatic int __init alloc_node_page_ext(int nid)\n{\n\tstruct page_ext *base;\n\tunsigned long table_size;\n\tunsigned long nr_pages;\n\n\tnr_pages = NODE_DATA(nid)->node_spanned_pages;\n\tif (!nr_pages)\n\t\treturn 0;\n\n\t \n\tif (!IS_ALIGNED(node_start_pfn(nid), MAX_ORDER_NR_PAGES) ||\n\t\t!IS_ALIGNED(node_end_pfn(nid), MAX_ORDER_NR_PAGES))\n\t\tnr_pages += MAX_ORDER_NR_PAGES;\n\n\ttable_size = page_ext_size * nr_pages;\n\n\tbase = memblock_alloc_try_nid(\n\t\t\ttable_size, PAGE_SIZE, __pa(MAX_DMA_ADDRESS),\n\t\t\tMEMBLOCK_ALLOC_ACCESSIBLE, nid);\n\tif (!base)\n\t\treturn -ENOMEM;\n\tNODE_DATA(nid)->node_page_ext = base;\n\ttotal_usage += table_size;\n\treturn 0;\n}\n\nvoid __init page_ext_init_flatmem(void)\n{\n\n\tint nid, fail;\n\n\tif (!invoke_need_callbacks())\n\t\treturn;\n\n\tfor_each_online_node(nid)  {\n\t\tfail = alloc_node_page_ext(nid);\n\t\tif (fail)\n\t\t\tgoto fail;\n\t}\n\tpr_info(\"allocated %ld bytes of page_ext\\n\", total_usage);\n\treturn;\n\nfail:\n\tpr_crit(\"allocation of page_ext failed.\\n\");\n\tpanic(\"Out of memory\");\n}\n\n#else  \nstatic bool page_ext_invalid(struct page_ext *page_ext)\n{\n\treturn !page_ext || (((unsigned long)page_ext & PAGE_EXT_INVALID) == PAGE_EXT_INVALID);\n}\n\nstatic struct page_ext *lookup_page_ext(const struct page *page)\n{\n\tunsigned long pfn = page_to_pfn(page);\n\tstruct mem_section *section = __pfn_to_section(pfn);\n\tstruct page_ext *page_ext = READ_ONCE(section->page_ext);\n\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\t \n\tif (page_ext_invalid(page_ext))\n\t\treturn NULL;\n\treturn get_entry(page_ext, pfn);\n}\n\nstatic void *__meminit alloc_page_ext(size_t size, int nid)\n{\n\tgfp_t flags = GFP_KERNEL | __GFP_ZERO | __GFP_NOWARN;\n\tvoid *addr = NULL;\n\n\taddr = alloc_pages_exact_nid(nid, size, flags);\n\tif (addr) {\n\t\tkmemleak_alloc(addr, size, 1, flags);\n\t\treturn addr;\n\t}\n\n\taddr = vzalloc_node(size, nid);\n\n\treturn addr;\n}\n\nstatic int __meminit init_section_page_ext(unsigned long pfn, int nid)\n{\n\tstruct mem_section *section;\n\tstruct page_ext *base;\n\tunsigned long table_size;\n\n\tsection = __pfn_to_section(pfn);\n\n\tif (section->page_ext)\n\t\treturn 0;\n\n\ttable_size = page_ext_size * PAGES_PER_SECTION;\n\tbase = alloc_page_ext(table_size, nid);\n\n\t \n\tkmemleak_not_leak(base);\n\n\tif (!base) {\n\t\tpr_err(\"page ext allocation failure\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tpfn &= PAGE_SECTION_MASK;\n\tsection->page_ext = (void *)base - page_ext_size * pfn;\n\ttotal_usage += table_size;\n\treturn 0;\n}\n\nstatic void free_page_ext(void *addr)\n{\n\tif (is_vmalloc_addr(addr)) {\n\t\tvfree(addr);\n\t} else {\n\t\tstruct page *page = virt_to_page(addr);\n\t\tsize_t table_size;\n\n\t\ttable_size = page_ext_size * PAGES_PER_SECTION;\n\n\t\tBUG_ON(PageReserved(page));\n\t\tkmemleak_free(addr);\n\t\tfree_pages_exact(addr, table_size);\n\t}\n}\n\nstatic void __free_page_ext(unsigned long pfn)\n{\n\tstruct mem_section *ms;\n\tstruct page_ext *base;\n\n\tms = __pfn_to_section(pfn);\n\tif (!ms || !ms->page_ext)\n\t\treturn;\n\n\tbase = READ_ONCE(ms->page_ext);\n\t \n\tif (page_ext_invalid(base))\n\t\tbase = (void *)base - PAGE_EXT_INVALID;\n\tWRITE_ONCE(ms->page_ext, NULL);\n\n\tbase = get_entry(base, pfn);\n\tfree_page_ext(base);\n}\n\nstatic void __invalidate_page_ext(unsigned long pfn)\n{\n\tstruct mem_section *ms;\n\tvoid *val;\n\n\tms = __pfn_to_section(pfn);\n\tif (!ms || !ms->page_ext)\n\t\treturn;\n\tval = (void *)ms->page_ext + PAGE_EXT_INVALID;\n\tWRITE_ONCE(ms->page_ext, val);\n}\n\nstatic int __meminit online_page_ext(unsigned long start_pfn,\n\t\t\t\tunsigned long nr_pages,\n\t\t\t\tint nid)\n{\n\tunsigned long start, end, pfn;\n\tint fail = 0;\n\n\tstart = SECTION_ALIGN_DOWN(start_pfn);\n\tend = SECTION_ALIGN_UP(start_pfn + nr_pages);\n\n\tif (nid == NUMA_NO_NODE) {\n\t\t \n\t\tnid = pfn_to_nid(start_pfn);\n\t\tVM_BUG_ON(!node_online(nid));\n\t}\n\n\tfor (pfn = start; !fail && pfn < end; pfn += PAGES_PER_SECTION)\n\t\tfail = init_section_page_ext(pfn, nid);\n\tif (!fail)\n\t\treturn 0;\n\n\t \n\tend = pfn - PAGES_PER_SECTION;\n\tfor (pfn = start; pfn < end; pfn += PAGES_PER_SECTION)\n\t\t__free_page_ext(pfn);\n\n\treturn -ENOMEM;\n}\n\nstatic void __meminit offline_page_ext(unsigned long start_pfn,\n\t\t\t\tunsigned long nr_pages)\n{\n\tunsigned long start, end, pfn;\n\n\tstart = SECTION_ALIGN_DOWN(start_pfn);\n\tend = SECTION_ALIGN_UP(start_pfn + nr_pages);\n\n\t \n\tfor (pfn = start; pfn < end; pfn += PAGES_PER_SECTION)\n\t\t__invalidate_page_ext(pfn);\n\n\tsynchronize_rcu();\n\n\tfor (pfn = start; pfn < end; pfn += PAGES_PER_SECTION)\n\t\t__free_page_ext(pfn);\n}\n\nstatic int __meminit page_ext_callback(struct notifier_block *self,\n\t\t\t       unsigned long action, void *arg)\n{\n\tstruct memory_notify *mn = arg;\n\tint ret = 0;\n\n\tswitch (action) {\n\tcase MEM_GOING_ONLINE:\n\t\tret = online_page_ext(mn->start_pfn,\n\t\t\t\t   mn->nr_pages, mn->status_change_nid);\n\t\tbreak;\n\tcase MEM_OFFLINE:\n\t\toffline_page_ext(mn->start_pfn,\n\t\t\t\tmn->nr_pages);\n\t\tbreak;\n\tcase MEM_CANCEL_ONLINE:\n\t\toffline_page_ext(mn->start_pfn,\n\t\t\t\tmn->nr_pages);\n\t\tbreak;\n\tcase MEM_GOING_OFFLINE:\n\t\tbreak;\n\tcase MEM_ONLINE:\n\tcase MEM_CANCEL_OFFLINE:\n\t\tbreak;\n\t}\n\n\treturn notifier_from_errno(ret);\n}\n\nvoid __init page_ext_init(void)\n{\n\tunsigned long pfn;\n\tint nid;\n\n\tif (!invoke_need_callbacks())\n\t\treturn;\n\n\tfor_each_node_state(nid, N_MEMORY) {\n\t\tunsigned long start_pfn, end_pfn;\n\n\t\tstart_pfn = node_start_pfn(nid);\n\t\tend_pfn = node_end_pfn(nid);\n\t\t \n\t\tfor (pfn = start_pfn; pfn < end_pfn;\n\t\t\tpfn = ALIGN(pfn + 1, PAGES_PER_SECTION)) {\n\n\t\t\tif (!pfn_valid(pfn))\n\t\t\t\tcontinue;\n\t\t\t \n\t\t\tif (pfn_to_nid(pfn) != nid)\n\t\t\t\tcontinue;\n\t\t\tif (init_section_page_ext(pfn, nid))\n\t\t\t\tgoto oom;\n\t\t\tcond_resched();\n\t\t}\n\t}\n\thotplug_memory_notifier(page_ext_callback, DEFAULT_CALLBACK_PRI);\n\tpr_info(\"allocated %ld bytes of page_ext\\n\", total_usage);\n\tinvoke_init_callbacks();\n\treturn;\n\noom:\n\tpanic(\"Out of memory\");\n}\n\nvoid __meminit pgdat_page_ext_init(struct pglist_data *pgdat)\n{\n}\n\n#endif\n\n \nstruct page_ext *page_ext_get(struct page *page)\n{\n\tstruct page_ext *page_ext;\n\n\trcu_read_lock();\n\tpage_ext = lookup_page_ext(page);\n\tif (!page_ext) {\n\t\trcu_read_unlock();\n\t\treturn NULL;\n\t}\n\n\treturn page_ext;\n}\n\n \nvoid page_ext_put(struct page_ext *page_ext)\n{\n\tif (unlikely(!page_ext))\n\t\treturn;\n\n\trcu_read_unlock();\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}