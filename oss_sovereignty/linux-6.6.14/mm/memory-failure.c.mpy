{
  "module_name": "memory-failure.c",
  "hash_id": "8693d347970952a8273e993b2af46e20e226865f2e5924e6b5f143f6f917c7d5",
  "original_prompt": "Ingested from linux-6.6.14/mm/memory-failure.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) \"Memory failure: \" fmt\n\n#include <linux/kernel.h>\n#include <linux/mm.h>\n#include <linux/page-flags.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/task.h>\n#include <linux/dax.h>\n#include <linux/ksm.h>\n#include <linux/rmap.h>\n#include <linux/export.h>\n#include <linux/pagemap.h>\n#include <linux/swap.h>\n#include <linux/backing-dev.h>\n#include <linux/migrate.h>\n#include <linux/slab.h>\n#include <linux/swapops.h>\n#include <linux/hugetlb.h>\n#include <linux/memory_hotplug.h>\n#include <linux/mm_inline.h>\n#include <linux/memremap.h>\n#include <linux/kfifo.h>\n#include <linux/ratelimit.h>\n#include <linux/pagewalk.h>\n#include <linux/shmem_fs.h>\n#include <linux/sysctl.h>\n#include \"swap.h\"\n#include \"internal.h\"\n#include \"ras/ras_event.h\"\n\nstatic int sysctl_memory_failure_early_kill __read_mostly;\n\nstatic int sysctl_memory_failure_recovery __read_mostly = 1;\n\natomic_long_t num_poisoned_pages __read_mostly = ATOMIC_LONG_INIT(0);\n\nstatic bool hw_memory_failure __read_mostly = false;\n\nstatic DEFINE_MUTEX(mf_mutex);\n\nvoid num_poisoned_pages_inc(unsigned long pfn)\n{\n\tatomic_long_inc(&num_poisoned_pages);\n\tmemblk_nr_poison_inc(pfn);\n}\n\nvoid num_poisoned_pages_sub(unsigned long pfn, long i)\n{\n\tatomic_long_sub(i, &num_poisoned_pages);\n\tif (pfn != -1UL)\n\t\tmemblk_nr_poison_sub(pfn, i);\n}\n\n \n#define MF_ATTR_RO(_name)\t\t\t\t\t\\\nstatic ssize_t _name##_show(struct device *dev,\t\t\t\\\n\t\t\t    struct device_attribute *attr,\t\\\n\t\t\t    char *buf)\t\t\t\t\\\n{\t\t\t\t\t\t\t\t\\\n\tstruct memory_failure_stats *mf_stats =\t\t\t\\\n\t\t&NODE_DATA(dev->id)->mf_stats;\t\t\t\\\n\treturn sprintf(buf, \"%lu\\n\", mf_stats->_name);\t\t\\\n}\t\t\t\t\t\t\t\t\\\nstatic DEVICE_ATTR_RO(_name)\n\nMF_ATTR_RO(total);\nMF_ATTR_RO(ignored);\nMF_ATTR_RO(failed);\nMF_ATTR_RO(delayed);\nMF_ATTR_RO(recovered);\n\nstatic struct attribute *memory_failure_attr[] = {\n\t&dev_attr_total.attr,\n\t&dev_attr_ignored.attr,\n\t&dev_attr_failed.attr,\n\t&dev_attr_delayed.attr,\n\t&dev_attr_recovered.attr,\n\tNULL,\n};\n\nconst struct attribute_group memory_failure_attr_group = {\n\t.name = \"memory_failure\",\n\t.attrs = memory_failure_attr,\n};\n\nstatic struct ctl_table memory_failure_table[] = {\n\t{\n\t\t.procname\t= \"memory_failure_early_kill\",\n\t\t.data\t\t= &sysctl_memory_failure_early_kill,\n\t\t.maxlen\t\t= sizeof(sysctl_memory_failure_early_kill),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_ONE,\n\t},\n\t{\n\t\t.procname\t= \"memory_failure_recovery\",\n\t\t.data\t\t= &sysctl_memory_failure_recovery,\n\t\t.maxlen\t\t= sizeof(sysctl_memory_failure_recovery),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_ONE,\n\t},\n\t{ }\n};\n\n \nstatic int __page_handle_poison(struct page *page)\n{\n\tint ret;\n\n\tzone_pcp_disable(page_zone(page));\n\tret = dissolve_free_huge_page(page);\n\tif (!ret)\n\t\tret = take_page_off_buddy(page);\n\tzone_pcp_enable(page_zone(page));\n\n\treturn ret;\n}\n\nstatic bool page_handle_poison(struct page *page, bool hugepage_or_freepage, bool release)\n{\n\tif (hugepage_or_freepage) {\n\t\t \n\t\tif (__page_handle_poison(page) <= 0)\n\t\t\t \n\t\t\treturn false;\n\t}\n\n\tSetPageHWPoison(page);\n\tif (release)\n\t\tput_page(page);\n\tpage_ref_inc(page);\n\tnum_poisoned_pages_inc(page_to_pfn(page));\n\n\treturn true;\n}\n\n#if IS_ENABLED(CONFIG_HWPOISON_INJECT)\n\nu32 hwpoison_filter_enable = 0;\nu32 hwpoison_filter_dev_major = ~0U;\nu32 hwpoison_filter_dev_minor = ~0U;\nu64 hwpoison_filter_flags_mask;\nu64 hwpoison_filter_flags_value;\nEXPORT_SYMBOL_GPL(hwpoison_filter_enable);\nEXPORT_SYMBOL_GPL(hwpoison_filter_dev_major);\nEXPORT_SYMBOL_GPL(hwpoison_filter_dev_minor);\nEXPORT_SYMBOL_GPL(hwpoison_filter_flags_mask);\nEXPORT_SYMBOL_GPL(hwpoison_filter_flags_value);\n\nstatic int hwpoison_filter_dev(struct page *p)\n{\n\tstruct address_space *mapping;\n\tdev_t dev;\n\n\tif (hwpoison_filter_dev_major == ~0U &&\n\t    hwpoison_filter_dev_minor == ~0U)\n\t\treturn 0;\n\n\tmapping = page_mapping(p);\n\tif (mapping == NULL || mapping->host == NULL)\n\t\treturn -EINVAL;\n\n\tdev = mapping->host->i_sb->s_dev;\n\tif (hwpoison_filter_dev_major != ~0U &&\n\t    hwpoison_filter_dev_major != MAJOR(dev))\n\t\treturn -EINVAL;\n\tif (hwpoison_filter_dev_minor != ~0U &&\n\t    hwpoison_filter_dev_minor != MINOR(dev))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int hwpoison_filter_flags(struct page *p)\n{\n\tif (!hwpoison_filter_flags_mask)\n\t\treturn 0;\n\n\tif ((stable_page_flags(p) & hwpoison_filter_flags_mask) ==\n\t\t\t\t    hwpoison_filter_flags_value)\n\t\treturn 0;\n\telse\n\t\treturn -EINVAL;\n}\n\n \n#ifdef CONFIG_MEMCG\nu64 hwpoison_filter_memcg;\nEXPORT_SYMBOL_GPL(hwpoison_filter_memcg);\nstatic int hwpoison_filter_task(struct page *p)\n{\n\tif (!hwpoison_filter_memcg)\n\t\treturn 0;\n\n\tif (page_cgroup_ino(p) != hwpoison_filter_memcg)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n#else\nstatic int hwpoison_filter_task(struct page *p) { return 0; }\n#endif\n\nint hwpoison_filter(struct page *p)\n{\n\tif (!hwpoison_filter_enable)\n\t\treturn 0;\n\n\tif (hwpoison_filter_dev(p))\n\t\treturn -EINVAL;\n\n\tif (hwpoison_filter_flags(p))\n\t\treturn -EINVAL;\n\n\tif (hwpoison_filter_task(p))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n#else\nint hwpoison_filter(struct page *p)\n{\n\treturn 0;\n}\n#endif\n\nEXPORT_SYMBOL_GPL(hwpoison_filter);\n\n \n\nstruct to_kill {\n\tstruct list_head nd;\n\tstruct task_struct *tsk;\n\tunsigned long addr;\n\tshort size_shift;\n};\n\n \nstatic int kill_proc(struct to_kill *tk, unsigned long pfn, int flags)\n{\n\tstruct task_struct *t = tk->tsk;\n\tshort addr_lsb = tk->size_shift;\n\tint ret = 0;\n\n\tpr_err(\"%#lx: Sending SIGBUS to %s:%d due to hardware memory corruption\\n\",\n\t\t\tpfn, t->comm, t->pid);\n\n\tif ((flags & MF_ACTION_REQUIRED) && (t == current))\n\t\tret = force_sig_mceerr(BUS_MCEERR_AR,\n\t\t\t\t (void __user *)tk->addr, addr_lsb);\n\telse\n\t\t \n\t\tret = send_sig_mceerr(BUS_MCEERR_AO, (void __user *)tk->addr,\n\t\t\t\t      addr_lsb, t);\n\tif (ret < 0)\n\t\tpr_info(\"Error sending signal to %s:%d: %d\\n\",\n\t\t\tt->comm, t->pid, ret);\n\treturn ret;\n}\n\n \nvoid shake_page(struct page *p)\n{\n\tif (PageHuge(p))\n\t\treturn;\n\t \n\tif (PageSlab(p))\n\t\treturn;\n\n\tlru_add_drain_all();\n}\nEXPORT_SYMBOL_GPL(shake_page);\n\nstatic unsigned long dev_pagemap_mapping_shift(struct vm_area_struct *vma,\n\t\tunsigned long address)\n{\n\tunsigned long ret = 0;\n\tpgd_t *pgd;\n\tp4d_t *p4d;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\tpte_t *pte;\n\tpte_t ptent;\n\n\tVM_BUG_ON_VMA(address == -EFAULT, vma);\n\tpgd = pgd_offset(vma->vm_mm, address);\n\tif (!pgd_present(*pgd))\n\t\treturn 0;\n\tp4d = p4d_offset(pgd, address);\n\tif (!p4d_present(*p4d))\n\t\treturn 0;\n\tpud = pud_offset(p4d, address);\n\tif (!pud_present(*pud))\n\t\treturn 0;\n\tif (pud_devmap(*pud))\n\t\treturn PUD_SHIFT;\n\tpmd = pmd_offset(pud, address);\n\tif (!pmd_present(*pmd))\n\t\treturn 0;\n\tif (pmd_devmap(*pmd))\n\t\treturn PMD_SHIFT;\n\tpte = pte_offset_map(pmd, address);\n\tif (!pte)\n\t\treturn 0;\n\tptent = ptep_get(pte);\n\tif (pte_present(ptent) && pte_devmap(ptent))\n\t\tret = PAGE_SHIFT;\n\tpte_unmap(pte);\n\treturn ret;\n}\n\n \n\n#define FSDAX_INVALID_PGOFF ULONG_MAX\n\n \nstatic void __add_to_kill(struct task_struct *tsk, struct page *p,\n\t\t\t  struct vm_area_struct *vma, struct list_head *to_kill,\n\t\t\t  unsigned long ksm_addr, pgoff_t fsdax_pgoff)\n{\n\tstruct to_kill *tk;\n\n\ttk = kmalloc(sizeof(struct to_kill), GFP_ATOMIC);\n\tif (!tk) {\n\t\tpr_err(\"Out of memory while machine check handling\\n\");\n\t\treturn;\n\t}\n\n\ttk->addr = ksm_addr ? ksm_addr : page_address_in_vma(p, vma);\n\tif (is_zone_device_page(p)) {\n\t\tif (fsdax_pgoff != FSDAX_INVALID_PGOFF)\n\t\t\ttk->addr = vma_pgoff_address(fsdax_pgoff, 1, vma);\n\t\ttk->size_shift = dev_pagemap_mapping_shift(vma, tk->addr);\n\t} else\n\t\ttk->size_shift = page_shift(compound_head(p));\n\n\t \n\tif (tk->addr == -EFAULT) {\n\t\tpr_info(\"Unable to find user space address %lx in %s\\n\",\n\t\t\tpage_to_pfn(p), tsk->comm);\n\t} else if (tk->size_shift == 0) {\n\t\tkfree(tk);\n\t\treturn;\n\t}\n\n\tget_task_struct(tsk);\n\ttk->tsk = tsk;\n\tlist_add_tail(&tk->nd, to_kill);\n}\n\nstatic void add_to_kill_anon_file(struct task_struct *tsk, struct page *p,\n\t\t\t\t  struct vm_area_struct *vma,\n\t\t\t\t  struct list_head *to_kill)\n{\n\t__add_to_kill(tsk, p, vma, to_kill, 0, FSDAX_INVALID_PGOFF);\n}\n\n#ifdef CONFIG_KSM\nstatic bool task_in_to_kill_list(struct list_head *to_kill,\n\t\t\t\t struct task_struct *tsk)\n{\n\tstruct to_kill *tk, *next;\n\n\tlist_for_each_entry_safe(tk, next, to_kill, nd) {\n\t\tif (tk->tsk == tsk)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\nvoid add_to_kill_ksm(struct task_struct *tsk, struct page *p,\n\t\t     struct vm_area_struct *vma, struct list_head *to_kill,\n\t\t     unsigned long ksm_addr)\n{\n\tif (!task_in_to_kill_list(to_kill, tsk))\n\t\t__add_to_kill(tsk, p, vma, to_kill, ksm_addr, FSDAX_INVALID_PGOFF);\n}\n#endif\n \nstatic void kill_procs(struct list_head *to_kill, int forcekill, bool fail,\n\t\tunsigned long pfn, int flags)\n{\n\tstruct to_kill *tk, *next;\n\n\tlist_for_each_entry_safe(tk, next, to_kill, nd) {\n\t\tif (forcekill) {\n\t\t\t \n\t\t\tif (fail || tk->addr == -EFAULT) {\n\t\t\t\tpr_err(\"%#lx: forcibly killing %s:%d because of failure to unmap corrupted page\\n\",\n\t\t\t\t       pfn, tk->tsk->comm, tk->tsk->pid);\n\t\t\t\tdo_send_sig_info(SIGKILL, SEND_SIG_PRIV,\n\t\t\t\t\t\t tk->tsk, PIDTYPE_PID);\n\t\t\t}\n\n\t\t\t \n\t\t\telse if (kill_proc(tk, pfn, flags) < 0)\n\t\t\t\tpr_err(\"%#lx: Cannot send advisory machine check signal to %s:%d\\n\",\n\t\t\t\t       pfn, tk->tsk->comm, tk->tsk->pid);\n\t\t}\n\t\tlist_del(&tk->nd);\n\t\tput_task_struct(tk->tsk);\n\t\tkfree(tk);\n\t}\n}\n\n \nstatic struct task_struct *find_early_kill_thread(struct task_struct *tsk)\n{\n\tstruct task_struct *t;\n\n\tfor_each_thread(tsk, t) {\n\t\tif (t->flags & PF_MCE_PROCESS) {\n\t\t\tif (t->flags & PF_MCE_EARLY)\n\t\t\t\treturn t;\n\t\t} else {\n\t\t\tif (sysctl_memory_failure_early_kill)\n\t\t\t\treturn t;\n\t\t}\n\t}\n\treturn NULL;\n}\n\n \nstruct task_struct *task_early_kill(struct task_struct *tsk, int force_early)\n{\n\tif (!tsk->mm)\n\t\treturn NULL;\n\t \n\tif (force_early && tsk->mm == current->mm)\n\t\treturn current;\n\n\treturn find_early_kill_thread(tsk);\n}\n\n \nstatic void collect_procs_anon(struct folio *folio, struct page *page,\n\t\tstruct list_head *to_kill, int force_early)\n{\n\tstruct vm_area_struct *vma;\n\tstruct task_struct *tsk;\n\tstruct anon_vma *av;\n\tpgoff_t pgoff;\n\n\tav = folio_lock_anon_vma_read(folio, NULL);\n\tif (av == NULL)\t \n\t\treturn;\n\n\tpgoff = page_to_pgoff(page);\n\trcu_read_lock();\n\tfor_each_process(tsk) {\n\t\tstruct anon_vma_chain *vmac;\n\t\tstruct task_struct *t = task_early_kill(tsk, force_early);\n\n\t\tif (!t)\n\t\t\tcontinue;\n\t\tanon_vma_interval_tree_foreach(vmac, &av->rb_root,\n\t\t\t\t\t       pgoff, pgoff) {\n\t\t\tvma = vmac->vma;\n\t\t\tif (vma->vm_mm != t->mm)\n\t\t\t\tcontinue;\n\t\t\tif (!page_mapped_in_vma(page, vma))\n\t\t\t\tcontinue;\n\t\t\tadd_to_kill_anon_file(t, page, vma, to_kill);\n\t\t}\n\t}\n\trcu_read_unlock();\n\tanon_vma_unlock_read(av);\n}\n\n \nstatic void collect_procs_file(struct folio *folio, struct page *page,\n\t\tstruct list_head *to_kill, int force_early)\n{\n\tstruct vm_area_struct *vma;\n\tstruct task_struct *tsk;\n\tstruct address_space *mapping = folio->mapping;\n\tpgoff_t pgoff;\n\n\ti_mmap_lock_read(mapping);\n\trcu_read_lock();\n\tpgoff = page_to_pgoff(page);\n\tfor_each_process(tsk) {\n\t\tstruct task_struct *t = task_early_kill(tsk, force_early);\n\n\t\tif (!t)\n\t\t\tcontinue;\n\t\tvma_interval_tree_foreach(vma, &mapping->i_mmap, pgoff,\n\t\t\t\t      pgoff) {\n\t\t\t \n\t\t\tif (vma->vm_mm == t->mm)\n\t\t\t\tadd_to_kill_anon_file(t, page, vma, to_kill);\n\t\t}\n\t}\n\trcu_read_unlock();\n\ti_mmap_unlock_read(mapping);\n}\n\n#ifdef CONFIG_FS_DAX\nstatic void add_to_kill_fsdax(struct task_struct *tsk, struct page *p,\n\t\t\t      struct vm_area_struct *vma,\n\t\t\t      struct list_head *to_kill, pgoff_t pgoff)\n{\n\t__add_to_kill(tsk, p, vma, to_kill, 0, pgoff);\n}\n\n \nstatic void collect_procs_fsdax(struct page *page,\n\t\tstruct address_space *mapping, pgoff_t pgoff,\n\t\tstruct list_head *to_kill)\n{\n\tstruct vm_area_struct *vma;\n\tstruct task_struct *tsk;\n\n\ti_mmap_lock_read(mapping);\n\trcu_read_lock();\n\tfor_each_process(tsk) {\n\t\tstruct task_struct *t = task_early_kill(tsk, true);\n\n\t\tif (!t)\n\t\t\tcontinue;\n\t\tvma_interval_tree_foreach(vma, &mapping->i_mmap, pgoff, pgoff) {\n\t\t\tif (vma->vm_mm == t->mm)\n\t\t\t\tadd_to_kill_fsdax(t, page, vma, to_kill, pgoff);\n\t\t}\n\t}\n\trcu_read_unlock();\n\ti_mmap_unlock_read(mapping);\n}\n#endif  \n\n \nstatic void collect_procs(struct folio *folio, struct page *page,\n\t\tstruct list_head *tokill, int force_early)\n{\n\tif (!folio->mapping)\n\t\treturn;\n\tif (unlikely(PageKsm(page)))\n\t\tcollect_procs_ksm(page, tokill, force_early);\n\telse if (PageAnon(page))\n\t\tcollect_procs_anon(folio, page, tokill, force_early);\n\telse\n\t\tcollect_procs_file(folio, page, tokill, force_early);\n}\n\nstruct hwpoison_walk {\n\tstruct to_kill tk;\n\tunsigned long pfn;\n\tint flags;\n};\n\nstatic void set_to_kill(struct to_kill *tk, unsigned long addr, short shift)\n{\n\ttk->addr = addr;\n\ttk->size_shift = shift;\n}\n\nstatic int check_hwpoisoned_entry(pte_t pte, unsigned long addr, short shift,\n\t\t\t\tunsigned long poisoned_pfn, struct to_kill *tk)\n{\n\tunsigned long pfn = 0;\n\n\tif (pte_present(pte)) {\n\t\tpfn = pte_pfn(pte);\n\t} else {\n\t\tswp_entry_t swp = pte_to_swp_entry(pte);\n\n\t\tif (is_hwpoison_entry(swp))\n\t\t\tpfn = swp_offset_pfn(swp);\n\t}\n\n\tif (!pfn || pfn != poisoned_pfn)\n\t\treturn 0;\n\n\tset_to_kill(tk, addr, shift);\n\treturn 1;\n}\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\nstatic int check_hwpoisoned_pmd_entry(pmd_t *pmdp, unsigned long addr,\n\t\t\t\t      struct hwpoison_walk *hwp)\n{\n\tpmd_t pmd = *pmdp;\n\tunsigned long pfn;\n\tunsigned long hwpoison_vaddr;\n\n\tif (!pmd_present(pmd))\n\t\treturn 0;\n\tpfn = pmd_pfn(pmd);\n\tif (pfn <= hwp->pfn && hwp->pfn < pfn + HPAGE_PMD_NR) {\n\t\thwpoison_vaddr = addr + ((hwp->pfn - pfn) << PAGE_SHIFT);\n\t\tset_to_kill(&hwp->tk, hwpoison_vaddr, PAGE_SHIFT);\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n#else\nstatic int check_hwpoisoned_pmd_entry(pmd_t *pmdp, unsigned long addr,\n\t\t\t\t      struct hwpoison_walk *hwp)\n{\n\treturn 0;\n}\n#endif\n\nstatic int hwpoison_pte_range(pmd_t *pmdp, unsigned long addr,\n\t\t\t      unsigned long end, struct mm_walk *walk)\n{\n\tstruct hwpoison_walk *hwp = walk->private;\n\tint ret = 0;\n\tpte_t *ptep, *mapped_pte;\n\tspinlock_t *ptl;\n\n\tptl = pmd_trans_huge_lock(pmdp, walk->vma);\n\tif (ptl) {\n\t\tret = check_hwpoisoned_pmd_entry(pmdp, addr, hwp);\n\t\tspin_unlock(ptl);\n\t\tgoto out;\n\t}\n\n\tmapped_pte = ptep = pte_offset_map_lock(walk->vma->vm_mm, pmdp,\n\t\t\t\t\t\taddr, &ptl);\n\tif (!ptep)\n\t\tgoto out;\n\n\tfor (; addr != end; ptep++, addr += PAGE_SIZE) {\n\t\tret = check_hwpoisoned_entry(ptep_get(ptep), addr, PAGE_SHIFT,\n\t\t\t\t\t     hwp->pfn, &hwp->tk);\n\t\tif (ret == 1)\n\t\t\tbreak;\n\t}\n\tpte_unmap_unlock(mapped_pte, ptl);\nout:\n\tcond_resched();\n\treturn ret;\n}\n\n#ifdef CONFIG_HUGETLB_PAGE\nstatic int hwpoison_hugetlb_range(pte_t *ptep, unsigned long hmask,\n\t\t\t    unsigned long addr, unsigned long end,\n\t\t\t    struct mm_walk *walk)\n{\n\tstruct hwpoison_walk *hwp = walk->private;\n\tpte_t pte = huge_ptep_get(ptep);\n\tstruct hstate *h = hstate_vma(walk->vma);\n\n\treturn check_hwpoisoned_entry(pte, addr, huge_page_shift(h),\n\t\t\t\t      hwp->pfn, &hwp->tk);\n}\n#else\n#define hwpoison_hugetlb_range\tNULL\n#endif\n\nstatic const struct mm_walk_ops hwpoison_walk_ops = {\n\t.pmd_entry = hwpoison_pte_range,\n\t.hugetlb_entry = hwpoison_hugetlb_range,\n\t.walk_lock = PGWALK_RDLOCK,\n};\n\n \nstatic int kill_accessing_process(struct task_struct *p, unsigned long pfn,\n\t\t\t\t  int flags)\n{\n\tint ret;\n\tstruct hwpoison_walk priv = {\n\t\t.pfn = pfn,\n\t};\n\tpriv.tk.tsk = p;\n\n\tif (!p->mm)\n\t\treturn -EFAULT;\n\n\tmmap_read_lock(p->mm);\n\tret = walk_page_range(p->mm, 0, TASK_SIZE, &hwpoison_walk_ops,\n\t\t\t      (void *)&priv);\n\tif (ret == 1 && priv.tk.addr)\n\t\tkill_proc(&priv.tk, pfn, flags);\n\telse\n\t\tret = 0;\n\tmmap_read_unlock(p->mm);\n\treturn ret > 0 ? -EHWPOISON : -EFAULT;\n}\n\nstatic const char *action_name[] = {\n\t[MF_IGNORED] = \"Ignored\",\n\t[MF_FAILED] = \"Failed\",\n\t[MF_DELAYED] = \"Delayed\",\n\t[MF_RECOVERED] = \"Recovered\",\n};\n\nstatic const char * const action_page_types[] = {\n\t[MF_MSG_KERNEL]\t\t\t= \"reserved kernel page\",\n\t[MF_MSG_KERNEL_HIGH_ORDER]\t= \"high-order kernel page\",\n\t[MF_MSG_SLAB]\t\t\t= \"kernel slab page\",\n\t[MF_MSG_DIFFERENT_COMPOUND]\t= \"different compound page after locking\",\n\t[MF_MSG_HUGE]\t\t\t= \"huge page\",\n\t[MF_MSG_FREE_HUGE]\t\t= \"free huge page\",\n\t[MF_MSG_UNMAP_FAILED]\t\t= \"unmapping failed page\",\n\t[MF_MSG_DIRTY_SWAPCACHE]\t= \"dirty swapcache page\",\n\t[MF_MSG_CLEAN_SWAPCACHE]\t= \"clean swapcache page\",\n\t[MF_MSG_DIRTY_MLOCKED_LRU]\t= \"dirty mlocked LRU page\",\n\t[MF_MSG_CLEAN_MLOCKED_LRU]\t= \"clean mlocked LRU page\",\n\t[MF_MSG_DIRTY_UNEVICTABLE_LRU]\t= \"dirty unevictable LRU page\",\n\t[MF_MSG_CLEAN_UNEVICTABLE_LRU]\t= \"clean unevictable LRU page\",\n\t[MF_MSG_DIRTY_LRU]\t\t= \"dirty LRU page\",\n\t[MF_MSG_CLEAN_LRU]\t\t= \"clean LRU page\",\n\t[MF_MSG_TRUNCATED_LRU]\t\t= \"already truncated LRU page\",\n\t[MF_MSG_BUDDY]\t\t\t= \"free buddy page\",\n\t[MF_MSG_DAX]\t\t\t= \"dax page\",\n\t[MF_MSG_UNSPLIT_THP]\t\t= \"unsplit thp\",\n\t[MF_MSG_UNKNOWN]\t\t= \"unknown page\",\n};\n\n \nstatic int delete_from_lru_cache(struct page *p)\n{\n\tif (isolate_lru_page(p)) {\n\t\t \n\t\tClearPageActive(p);\n\t\tClearPageUnevictable(p);\n\n\t\t \n\t\tmem_cgroup_uncharge(page_folio(p));\n\n\t\t \n\t\tput_page(p);\n\t\treturn 0;\n\t}\n\treturn -EIO;\n}\n\nstatic int truncate_error_page(struct page *p, unsigned long pfn,\n\t\t\t\tstruct address_space *mapping)\n{\n\tint ret = MF_FAILED;\n\n\tif (mapping->a_ops->error_remove_page) {\n\t\tstruct folio *folio = page_folio(p);\n\t\tint err = mapping->a_ops->error_remove_page(mapping, p);\n\n\t\tif (err != 0)\n\t\t\tpr_info(\"%#lx: Failed to punch page: %d\\n\", pfn, err);\n\t\telse if (!filemap_release_folio(folio, GFP_NOIO))\n\t\t\tpr_info(\"%#lx: failed to release buffers\\n\", pfn);\n\t\telse\n\t\t\tret = MF_RECOVERED;\n\t} else {\n\t\t \n\t\tif (invalidate_inode_page(p))\n\t\t\tret = MF_RECOVERED;\n\t\telse\n\t\t\tpr_info(\"%#lx: Failed to invalidate\\n\",\tpfn);\n\t}\n\n\treturn ret;\n}\n\nstruct page_state {\n\tunsigned long mask;\n\tunsigned long res;\n\tenum mf_action_page_type type;\n\n\t \n\tint (*action)(struct page_state *ps, struct page *p);\n};\n\n \nstatic bool has_extra_refcount(struct page_state *ps, struct page *p,\n\t\t\t       bool extra_pins)\n{\n\tint count = page_count(p) - 1;\n\n\tif (extra_pins)\n\t\tcount -= 1;\n\n\tif (count > 0) {\n\t\tpr_err(\"%#lx: %s still referenced by %d users\\n\",\n\t\t       page_to_pfn(p), action_page_types[ps->type], count);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nstatic int me_kernel(struct page_state *ps, struct page *p)\n{\n\tunlock_page(p);\n\treturn MF_IGNORED;\n}\n\n \nstatic int me_unknown(struct page_state *ps, struct page *p)\n{\n\tpr_err(\"%#lx: Unknown page state\\n\", page_to_pfn(p));\n\tunlock_page(p);\n\treturn MF_FAILED;\n}\n\n \nstatic int me_pagecache_clean(struct page_state *ps, struct page *p)\n{\n\tint ret;\n\tstruct address_space *mapping;\n\tbool extra_pins;\n\n\tdelete_from_lru_cache(p);\n\n\t \n\tif (PageAnon(p)) {\n\t\tret = MF_RECOVERED;\n\t\tgoto out;\n\t}\n\n\t \n\tmapping = page_mapping(p);\n\tif (!mapping) {\n\t\t \n\t\tret = MF_FAILED;\n\t\tgoto out;\n\t}\n\n\t \n\textra_pins = shmem_mapping(mapping);\n\n\t \n\tret = truncate_error_page(p, page_to_pfn(p), mapping);\n\tif (has_extra_refcount(ps, p, extra_pins))\n\t\tret = MF_FAILED;\n\nout:\n\tunlock_page(p);\n\n\treturn ret;\n}\n\n \nstatic int me_pagecache_dirty(struct page_state *ps, struct page *p)\n{\n\tstruct address_space *mapping = page_mapping(p);\n\n\tSetPageError(p);\n\t \n\tif (mapping) {\n\t\t \n\t\tmapping_set_error(mapping, -EIO);\n\t}\n\n\treturn me_pagecache_clean(ps, p);\n}\n\n \nstatic int me_swapcache_dirty(struct page_state *ps, struct page *p)\n{\n\tint ret;\n\tbool extra_pins = false;\n\n\tClearPageDirty(p);\n\t \n\tClearPageUptodate(p);\n\n\tret = delete_from_lru_cache(p) ? MF_FAILED : MF_DELAYED;\n\tunlock_page(p);\n\n\tif (ret == MF_DELAYED)\n\t\textra_pins = true;\n\n\tif (has_extra_refcount(ps, p, extra_pins))\n\t\tret = MF_FAILED;\n\n\treturn ret;\n}\n\nstatic int me_swapcache_clean(struct page_state *ps, struct page *p)\n{\n\tstruct folio *folio = page_folio(p);\n\tint ret;\n\n\tdelete_from_swap_cache(folio);\n\n\tret = delete_from_lru_cache(p) ? MF_FAILED : MF_RECOVERED;\n\tfolio_unlock(folio);\n\n\tif (has_extra_refcount(ps, p, false))\n\t\tret = MF_FAILED;\n\n\treturn ret;\n}\n\n \nstatic int me_huge_page(struct page_state *ps, struct page *p)\n{\n\tint res;\n\tstruct page *hpage = compound_head(p);\n\tstruct address_space *mapping;\n\tbool extra_pins = false;\n\n\tmapping = page_mapping(hpage);\n\tif (mapping) {\n\t\tres = truncate_error_page(hpage, page_to_pfn(p), mapping);\n\t\t \n\t\textra_pins = true;\n\t\tunlock_page(hpage);\n\t} else {\n\t\tunlock_page(hpage);\n\t\t \n\t\tput_page(hpage);\n\t\tif (__page_handle_poison(p) >= 0) {\n\t\t\tpage_ref_inc(p);\n\t\t\tres = MF_RECOVERED;\n\t\t} else {\n\t\t\tres = MF_FAILED;\n\t\t}\n\t}\n\n\tif (has_extra_refcount(ps, p, extra_pins))\n\t\tres = MF_FAILED;\n\n\treturn res;\n}\n\n \n\n#define dirty\t\t(1UL << PG_dirty)\n#define sc\t\t((1UL << PG_swapcache) | (1UL << PG_swapbacked))\n#define unevict\t\t(1UL << PG_unevictable)\n#define mlock\t\t(1UL << PG_mlocked)\n#define lru\t\t(1UL << PG_lru)\n#define head\t\t(1UL << PG_head)\n#define slab\t\t(1UL << PG_slab)\n#define reserved\t(1UL << PG_reserved)\n\nstatic struct page_state error_states[] = {\n\t{ reserved,\treserved,\tMF_MSG_KERNEL,\tme_kernel },\n\t \n\n\t \n\t{ slab,\t\tslab,\t\tMF_MSG_SLAB,\tme_kernel },\n\n\t{ head,\t\thead,\t\tMF_MSG_HUGE,\t\tme_huge_page },\n\n\t{ sc|dirty,\tsc|dirty,\tMF_MSG_DIRTY_SWAPCACHE,\tme_swapcache_dirty },\n\t{ sc|dirty,\tsc,\t\tMF_MSG_CLEAN_SWAPCACHE,\tme_swapcache_clean },\n\n\t{ mlock|dirty,\tmlock|dirty,\tMF_MSG_DIRTY_MLOCKED_LRU,\tme_pagecache_dirty },\n\t{ mlock|dirty,\tmlock,\t\tMF_MSG_CLEAN_MLOCKED_LRU,\tme_pagecache_clean },\n\n\t{ unevict|dirty, unevict|dirty,\tMF_MSG_DIRTY_UNEVICTABLE_LRU,\tme_pagecache_dirty },\n\t{ unevict|dirty, unevict,\tMF_MSG_CLEAN_UNEVICTABLE_LRU,\tme_pagecache_clean },\n\n\t{ lru|dirty,\tlru|dirty,\tMF_MSG_DIRTY_LRU,\tme_pagecache_dirty },\n\t{ lru|dirty,\tlru,\t\tMF_MSG_CLEAN_LRU,\tme_pagecache_clean },\n\n\t \n\t{ 0,\t\t0,\t\tMF_MSG_UNKNOWN,\tme_unknown },\n};\n\n#undef dirty\n#undef sc\n#undef unevict\n#undef mlock\n#undef lru\n#undef head\n#undef slab\n#undef reserved\n\nstatic void update_per_node_mf_stats(unsigned long pfn,\n\t\t\t\t     enum mf_result result)\n{\n\tint nid = MAX_NUMNODES;\n\tstruct memory_failure_stats *mf_stats = NULL;\n\n\tnid = pfn_to_nid(pfn);\n\tif (unlikely(nid < 0 || nid >= MAX_NUMNODES)) {\n\t\tWARN_ONCE(1, \"Memory failure: pfn=%#lx, invalid nid=%d\", pfn, nid);\n\t\treturn;\n\t}\n\n\tmf_stats = &NODE_DATA(nid)->mf_stats;\n\tswitch (result) {\n\tcase MF_IGNORED:\n\t\t++mf_stats->ignored;\n\t\tbreak;\n\tcase MF_FAILED:\n\t\t++mf_stats->failed;\n\t\tbreak;\n\tcase MF_DELAYED:\n\t\t++mf_stats->delayed;\n\t\tbreak;\n\tcase MF_RECOVERED:\n\t\t++mf_stats->recovered;\n\t\tbreak;\n\tdefault:\n\t\tWARN_ONCE(1, \"Memory failure: mf_result=%d is not properly handled\", result);\n\t\tbreak;\n\t}\n\t++mf_stats->total;\n}\n\n \nstatic int action_result(unsigned long pfn, enum mf_action_page_type type,\n\t\t\t enum mf_result result)\n{\n\ttrace_memory_failure_event(pfn, type, result);\n\n\tnum_poisoned_pages_inc(pfn);\n\n\tupdate_per_node_mf_stats(pfn, result);\n\n\tpr_err(\"%#lx: recovery action for %s: %s\\n\",\n\t\tpfn, action_page_types[type], action_name[result]);\n\n\treturn (result == MF_RECOVERED || result == MF_DELAYED) ? 0 : -EBUSY;\n}\n\nstatic int page_action(struct page_state *ps, struct page *p,\n\t\t\tunsigned long pfn)\n{\n\tint result;\n\n\t \n\tresult = ps->action(ps, p);\n\n\t \n\t \n\n\treturn action_result(pfn, ps->type, result);\n}\n\nstatic inline bool PageHWPoisonTakenOff(struct page *page)\n{\n\treturn PageHWPoison(page) && page_private(page) == MAGIC_HWPOISON;\n}\n\nvoid SetPageHWPoisonTakenOff(struct page *page)\n{\n\tset_page_private(page, MAGIC_HWPOISON);\n}\n\nvoid ClearPageHWPoisonTakenOff(struct page *page)\n{\n\tif (PageHWPoison(page))\n\t\tset_page_private(page, 0);\n}\n\n \nstatic inline bool HWPoisonHandlable(struct page *page, unsigned long flags)\n{\n\t \n\tif ((flags & MF_SOFT_OFFLINE) && __PageMovable(page))\n\t\treturn true;\n\n\treturn PageLRU(page) || is_free_buddy_page(page);\n}\n\nstatic int __get_hwpoison_page(struct page *page, unsigned long flags)\n{\n\tstruct folio *folio = page_folio(page);\n\tint ret = 0;\n\tbool hugetlb = false;\n\n\tret = get_hwpoison_hugetlb_folio(folio, &hugetlb, false);\n\tif (hugetlb) {\n\t\t \n\t\tif (folio == page_folio(page))\n\t\t\treturn ret;\n\t\tif (ret > 0) {\n\t\t\tfolio_put(folio);\n\t\t\tfolio = page_folio(page);\n\t\t}\n\t}\n\n\t \n\tif (!HWPoisonHandlable(&folio->page, flags))\n\t\treturn -EBUSY;\n\n\tif (folio_try_get(folio)) {\n\t\tif (folio == page_folio(page))\n\t\t\treturn 1;\n\n\t\tpr_info(\"%#lx cannot catch tail\\n\", page_to_pfn(page));\n\t\tfolio_put(folio);\n\t}\n\n\treturn 0;\n}\n\nstatic int get_any_page(struct page *p, unsigned long flags)\n{\n\tint ret = 0, pass = 0;\n\tbool count_increased = false;\n\n\tif (flags & MF_COUNT_INCREASED)\n\t\tcount_increased = true;\n\ntry_again:\n\tif (!count_increased) {\n\t\tret = __get_hwpoison_page(p, flags);\n\t\tif (!ret) {\n\t\t\tif (page_count(p)) {\n\t\t\t\t \n\t\t\t\tif (pass++ < 3)\n\t\t\t\t\tgoto try_again;\n\t\t\t\tret = -EBUSY;\n\t\t\t} else if (!PageHuge(p) && !is_free_buddy_page(p)) {\n\t\t\t\t \n\t\t\t\tif (pass++ < 3)\n\t\t\t\t\tgoto try_again;\n\t\t\t\tret = -EIO;\n\t\t\t}\n\t\t\tgoto out;\n\t\t} else if (ret == -EBUSY) {\n\t\t\t \n\t\t\tif (pass++ < 3) {\n\t\t\t\tshake_page(p);\n\t\t\t\tgoto try_again;\n\t\t\t}\n\t\t\tret = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (PageHuge(p) || HWPoisonHandlable(p, flags)) {\n\t\tret = 1;\n\t} else {\n\t\t \n\t\tif (pass++ < 3) {\n\t\t\tput_page(p);\n\t\t\tshake_page(p);\n\t\t\tcount_increased = false;\n\t\t\tgoto try_again;\n\t\t}\n\t\tput_page(p);\n\t\tret = -EIO;\n\t}\nout:\n\tif (ret == -EIO)\n\t\tpr_err(\"%#lx: unhandlable page.\\n\", page_to_pfn(p));\n\n\treturn ret;\n}\n\nstatic int __get_unpoison_page(struct page *page)\n{\n\tstruct folio *folio = page_folio(page);\n\tint ret = 0;\n\tbool hugetlb = false;\n\n\tret = get_hwpoison_hugetlb_folio(folio, &hugetlb, true);\n\tif (hugetlb) {\n\t\t \n\t\tif (folio == page_folio(page))\n\t\t\treturn ret;\n\t\tif (ret > 0)\n\t\t\tfolio_put(folio);\n\t}\n\n\t \n\tif (PageHWPoisonTakenOff(page))\n\t\treturn -EHWPOISON;\n\n\treturn get_page_unless_zero(page) ? 1 : 0;\n}\n\n \nstatic int get_hwpoison_page(struct page *p, unsigned long flags)\n{\n\tint ret;\n\n\tzone_pcp_disable(page_zone(p));\n\tif (flags & MF_UNPOISON)\n\t\tret = __get_unpoison_page(p);\n\telse\n\t\tret = get_any_page(p, flags);\n\tzone_pcp_enable(page_zone(p));\n\n\treturn ret;\n}\n\n \nstatic bool hwpoison_user_mappings(struct page *p, unsigned long pfn,\n\t\t\t\t  int flags, struct page *hpage)\n{\n\tstruct folio *folio = page_folio(hpage);\n\tenum ttu_flags ttu = TTU_IGNORE_MLOCK | TTU_SYNC | TTU_HWPOISON;\n\tstruct address_space *mapping;\n\tLIST_HEAD(tokill);\n\tbool unmap_success;\n\tint forcekill;\n\tbool mlocked = PageMlocked(hpage);\n\n\t \n\tif (PageReserved(p) || PageSlab(p) || PageTable(p) || PageOffline(p))\n\t\treturn true;\n\tif (!(PageLRU(hpage) || PageHuge(p)))\n\t\treturn true;\n\n\t \n\tif (!page_mapped(p))\n\t\treturn true;\n\n\tif (PageSwapCache(p)) {\n\t\tpr_err(\"%#lx: keeping poisoned page in swap cache\\n\", pfn);\n\t\tttu &= ~TTU_HWPOISON;\n\t}\n\n\t \n\tmapping = page_mapping(hpage);\n\tif (!(flags & MF_MUST_KILL) && !PageDirty(hpage) && mapping &&\n\t    mapping_can_writeback(mapping)) {\n\t\tif (page_mkclean(hpage)) {\n\t\t\tSetPageDirty(hpage);\n\t\t} else {\n\t\t\tttu &= ~TTU_HWPOISON;\n\t\t\tpr_info(\"%#lx: corrupted page was clean: dropped without side effects\\n\",\n\t\t\t\tpfn);\n\t\t}\n\t}\n\n\t \n\tcollect_procs(folio, p, &tokill, flags & MF_ACTION_REQUIRED);\n\n\tif (PageHuge(hpage) && !PageAnon(hpage)) {\n\t\t \n\t\tmapping = hugetlb_page_mapping_lock_write(hpage);\n\t\tif (mapping) {\n\t\t\ttry_to_unmap(folio, ttu|TTU_RMAP_LOCKED);\n\t\t\ti_mmap_unlock_write(mapping);\n\t\t} else\n\t\t\tpr_info(\"%#lx: could not lock mapping for mapped huge page\\n\", pfn);\n\t} else {\n\t\ttry_to_unmap(folio, ttu);\n\t}\n\n\tunmap_success = !page_mapped(p);\n\tif (!unmap_success)\n\t\tpr_err(\"%#lx: failed to unmap page (mapcount=%d)\\n\",\n\t\t       pfn, page_mapcount(p));\n\n\t \n\tif (mlocked)\n\t\tshake_page(hpage);\n\n\t \n\tforcekill = PageDirty(hpage) || (flags & MF_MUST_KILL) ||\n\t\t    !unmap_success;\n\tkill_procs(&tokill, forcekill, !unmap_success, pfn, flags);\n\n\treturn unmap_success;\n}\n\nstatic int identify_page_state(unsigned long pfn, struct page *p,\n\t\t\t\tunsigned long page_flags)\n{\n\tstruct page_state *ps;\n\n\t \n\tfor (ps = error_states;; ps++)\n\t\tif ((p->flags & ps->mask) == ps->res)\n\t\t\tbreak;\n\n\tpage_flags |= (p->flags & (1UL << PG_dirty));\n\n\tif (!ps->mask)\n\t\tfor (ps = error_states;; ps++)\n\t\t\tif ((page_flags & ps->mask) == ps->res)\n\t\t\t\tbreak;\n\treturn page_action(ps, p, pfn);\n}\n\nstatic int try_to_split_thp_page(struct page *page)\n{\n\tint ret;\n\n\tlock_page(page);\n\tret = split_huge_page(page);\n\tunlock_page(page);\n\n\tif (unlikely(ret))\n\t\tput_page(page);\n\n\treturn ret;\n}\n\nstatic void unmap_and_kill(struct list_head *to_kill, unsigned long pfn,\n\t\tstruct address_space *mapping, pgoff_t index, int flags)\n{\n\tstruct to_kill *tk;\n\tunsigned long size = 0;\n\n\tlist_for_each_entry(tk, to_kill, nd)\n\t\tif (tk->size_shift)\n\t\t\tsize = max(size, 1UL << tk->size_shift);\n\n\tif (size) {\n\t\t \n\t\tloff_t start = ((loff_t)index << PAGE_SHIFT) & ~(size - 1);\n\n\t\tunmap_mapping_range(mapping, start, size, 0);\n\t}\n\n\tkill_procs(to_kill, flags & MF_MUST_KILL, false, pfn, flags);\n}\n\n \nstatic int mf_generic_kill_procs(unsigned long long pfn, int flags,\n\t\tstruct dev_pagemap *pgmap)\n{\n\tstruct folio *folio = pfn_folio(pfn);\n\tLIST_HEAD(to_kill);\n\tdax_entry_t cookie;\n\tint rc = 0;\n\n\t \n\tcookie = dax_lock_folio(folio);\n\tif (!cookie)\n\t\treturn -EBUSY;\n\n\tif (hwpoison_filter(&folio->page)) {\n\t\trc = -EOPNOTSUPP;\n\t\tgoto unlock;\n\t}\n\n\tswitch (pgmap->type) {\n\tcase MEMORY_DEVICE_PRIVATE:\n\tcase MEMORY_DEVICE_COHERENT:\n\t\t \n\t\trc = -ENXIO;\n\t\tgoto unlock;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t \n\tSetPageHWPoison(&folio->page);\n\n\t \n\tflags |= MF_ACTION_REQUIRED | MF_MUST_KILL;\n\tcollect_procs(folio, &folio->page, &to_kill, true);\n\n\tunmap_and_kill(&to_kill, pfn, folio->mapping, folio->index, flags);\nunlock:\n\tdax_unlock_folio(folio, cookie);\n\treturn rc;\n}\n\n#ifdef CONFIG_FS_DAX\n \nint mf_dax_kill_procs(struct address_space *mapping, pgoff_t index,\n\t\tunsigned long count, int mf_flags)\n{\n\tLIST_HEAD(to_kill);\n\tdax_entry_t cookie;\n\tstruct page *page;\n\tsize_t end = index + count;\n\n\tmf_flags |= MF_ACTION_REQUIRED | MF_MUST_KILL;\n\n\tfor (; index < end; index++) {\n\t\tpage = NULL;\n\t\tcookie = dax_lock_mapping_entry(mapping, index, &page);\n\t\tif (!cookie)\n\t\t\treturn -EBUSY;\n\t\tif (!page)\n\t\t\tgoto unlock;\n\n\t\tSetPageHWPoison(page);\n\n\t\tcollect_procs_fsdax(page, mapping, index, &to_kill);\n\t\tunmap_and_kill(&to_kill, page_to_pfn(page), mapping,\n\t\t\t\tindex, mf_flags);\nunlock:\n\t\tdax_unlock_mapping_entry(mapping, index, cookie);\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(mf_dax_kill_procs);\n#endif  \n\n#ifdef CONFIG_HUGETLB_PAGE\n\n \nstruct raw_hwp_page {\n\tstruct llist_node node;\n\tstruct page *page;\n};\n\nstatic inline struct llist_head *raw_hwp_list_head(struct folio *folio)\n{\n\treturn (struct llist_head *)&folio->_hugetlb_hwpoison;\n}\n\nbool is_raw_hwpoison_page_in_hugepage(struct page *page)\n{\n\tstruct llist_head *raw_hwp_head;\n\tstruct raw_hwp_page *p;\n\tstruct folio *folio = page_folio(page);\n\tbool ret = false;\n\n\tif (!folio_test_hwpoison(folio))\n\t\treturn false;\n\n\tif (!folio_test_hugetlb(folio))\n\t\treturn PageHWPoison(page);\n\n\t \n\tif (folio_test_hugetlb_raw_hwp_unreliable(folio))\n\t\treturn true;\n\n\tmutex_lock(&mf_mutex);\n\n\traw_hwp_head = raw_hwp_list_head(folio);\n\tllist_for_each_entry(p, raw_hwp_head->first, node) {\n\t\tif (page == p->page) {\n\t\t\tret = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tmutex_unlock(&mf_mutex);\n\n\treturn ret;\n}\n\nstatic unsigned long __folio_free_raw_hwp(struct folio *folio, bool move_flag)\n{\n\tstruct llist_node *head;\n\tstruct raw_hwp_page *p, *next;\n\tunsigned long count = 0;\n\n\thead = llist_del_all(raw_hwp_list_head(folio));\n\tllist_for_each_entry_safe(p, next, head, node) {\n\t\tif (move_flag)\n\t\t\tSetPageHWPoison(p->page);\n\t\telse\n\t\t\tnum_poisoned_pages_sub(page_to_pfn(p->page), 1);\n\t\tkfree(p);\n\t\tcount++;\n\t}\n\treturn count;\n}\n\nstatic int folio_set_hugetlb_hwpoison(struct folio *folio, struct page *page)\n{\n\tstruct llist_head *head;\n\tstruct raw_hwp_page *raw_hwp;\n\tstruct raw_hwp_page *p, *next;\n\tint ret = folio_test_set_hwpoison(folio) ? -EHWPOISON : 0;\n\n\t \n\tif (folio_test_hugetlb_raw_hwp_unreliable(folio))\n\t\treturn -EHWPOISON;\n\thead = raw_hwp_list_head(folio);\n\tllist_for_each_entry_safe(p, next, head->first, node) {\n\t\tif (p->page == page)\n\t\t\treturn -EHWPOISON;\n\t}\n\n\traw_hwp = kmalloc(sizeof(struct raw_hwp_page), GFP_ATOMIC);\n\tif (raw_hwp) {\n\t\traw_hwp->page = page;\n\t\tllist_add(&raw_hwp->node, head);\n\t\t \n\t\tif (ret)\n\t\t\tnum_poisoned_pages_inc(page_to_pfn(page));\n\t} else {\n\t\t \n\t\tfolio_set_hugetlb_raw_hwp_unreliable(folio);\n\t\t \n\t\t__folio_free_raw_hwp(folio, false);\n\t}\n\treturn ret;\n}\n\nstatic unsigned long folio_free_raw_hwp(struct folio *folio, bool move_flag)\n{\n\t \n\tif (move_flag && folio_test_hugetlb_vmemmap_optimized(folio))\n\t\treturn 0;\n\n\t \n\tif (folio_test_hugetlb_raw_hwp_unreliable(folio))\n\t\treturn 0;\n\n\treturn __folio_free_raw_hwp(folio, move_flag);\n}\n\nvoid folio_clear_hugetlb_hwpoison(struct folio *folio)\n{\n\tif (folio_test_hugetlb_raw_hwp_unreliable(folio))\n\t\treturn;\n\tif (folio_test_hugetlb_vmemmap_optimized(folio))\n\t\treturn;\n\tfolio_clear_hwpoison(folio);\n\tfolio_free_raw_hwp(folio, true);\n}\n\n \nint __get_huge_page_for_hwpoison(unsigned long pfn, int flags,\n\t\t\t\t bool *migratable_cleared)\n{\n\tstruct page *page = pfn_to_page(pfn);\n\tstruct folio *folio = page_folio(page);\n\tint ret = 2;\t \n\tbool count_increased = false;\n\n\tif (!folio_test_hugetlb(folio))\n\t\tgoto out;\n\n\tif (flags & MF_COUNT_INCREASED) {\n\t\tret = 1;\n\t\tcount_increased = true;\n\t} else if (folio_test_hugetlb_freed(folio)) {\n\t\tret = 0;\n\t} else if (folio_test_hugetlb_migratable(folio)) {\n\t\tret = folio_try_get(folio);\n\t\tif (ret)\n\t\t\tcount_increased = true;\n\t} else {\n\t\tret = -EBUSY;\n\t\tif (!(flags & MF_NO_RETRY))\n\t\t\tgoto out;\n\t}\n\n\tif (folio_set_hugetlb_hwpoison(folio, page)) {\n\t\tret = -EHWPOISON;\n\t\tgoto out;\n\t}\n\n\t \n\tif (count_increased && folio_test_hugetlb_migratable(folio)) {\n\t\tfolio_clear_hugetlb_migratable(folio);\n\t\t*migratable_cleared = true;\n\t}\n\n\treturn ret;\nout:\n\tif (count_increased)\n\t\tfolio_put(folio);\n\treturn ret;\n}\n\n \nstatic int try_memory_failure_hugetlb(unsigned long pfn, int flags, int *hugetlb)\n{\n\tint res;\n\tstruct page *p = pfn_to_page(pfn);\n\tstruct folio *folio;\n\tunsigned long page_flags;\n\tbool migratable_cleared = false;\n\n\t*hugetlb = 1;\nretry:\n\tres = get_huge_page_for_hwpoison(pfn, flags, &migratable_cleared);\n\tif (res == 2) {  \n\t\t*hugetlb = 0;\n\t\treturn 0;\n\t} else if (res == -EHWPOISON) {\n\t\tpr_err(\"%#lx: already hardware poisoned\\n\", pfn);\n\t\tif (flags & MF_ACTION_REQUIRED) {\n\t\t\tfolio = page_folio(p);\n\t\t\tres = kill_accessing_process(current, folio_pfn(folio), flags);\n\t\t}\n\t\treturn res;\n\t} else if (res == -EBUSY) {\n\t\tif (!(flags & MF_NO_RETRY)) {\n\t\t\tflags |= MF_NO_RETRY;\n\t\t\tgoto retry;\n\t\t}\n\t\treturn action_result(pfn, MF_MSG_UNKNOWN, MF_IGNORED);\n\t}\n\n\tfolio = page_folio(p);\n\tfolio_lock(folio);\n\n\tif (hwpoison_filter(p)) {\n\t\tfolio_clear_hugetlb_hwpoison(folio);\n\t\tif (migratable_cleared)\n\t\t\tfolio_set_hugetlb_migratable(folio);\n\t\tfolio_unlock(folio);\n\t\tif (res == 1)\n\t\t\tfolio_put(folio);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\t \n\tif (res == 0) {\n\t\tfolio_unlock(folio);\n\t\tif (__page_handle_poison(p) >= 0) {\n\t\t\tpage_ref_inc(p);\n\t\t\tres = MF_RECOVERED;\n\t\t} else {\n\t\t\tres = MF_FAILED;\n\t\t}\n\t\treturn action_result(pfn, MF_MSG_FREE_HUGE, res);\n\t}\n\n\tpage_flags = folio->flags;\n\n\tif (!hwpoison_user_mappings(p, pfn, flags, &folio->page)) {\n\t\tfolio_unlock(folio);\n\t\treturn action_result(pfn, MF_MSG_UNMAP_FAILED, MF_IGNORED);\n\t}\n\n\treturn identify_page_state(pfn, p, page_flags);\n}\n\n#else\nstatic inline int try_memory_failure_hugetlb(unsigned long pfn, int flags, int *hugetlb)\n{\n\treturn 0;\n}\n\nstatic inline unsigned long folio_free_raw_hwp(struct folio *folio, bool flag)\n{\n\treturn 0;\n}\n#endif\t \n\n \nstatic void put_ref_page(unsigned long pfn, int flags)\n{\n\tstruct page *page;\n\n\tif (!(flags & MF_COUNT_INCREASED))\n\t\treturn;\n\n\tpage = pfn_to_page(pfn);\n\tif (page)\n\t\tput_page(page);\n}\n\nstatic int memory_failure_dev_pagemap(unsigned long pfn, int flags,\n\t\tstruct dev_pagemap *pgmap)\n{\n\tint rc = -ENXIO;\n\n\t \n\tif (!pgmap_pfn_valid(pgmap, pfn))\n\t\tgoto out;\n\n\t \n\tif (pgmap_has_memory_failure(pgmap)) {\n\t\trc = pgmap->ops->memory_failure(pgmap, pfn, 1, flags);\n\t\t \n\t\tif (rc != -EOPNOTSUPP)\n\t\t\tgoto out;\n\t}\n\n\trc = mf_generic_kill_procs(pfn, flags, pgmap);\nout:\n\t \n\tput_dev_pagemap(pgmap);\n\tif (rc != -EOPNOTSUPP)\n\t\taction_result(pfn, MF_MSG_DAX, rc ? MF_FAILED : MF_RECOVERED);\n\treturn rc;\n}\n\n \nint memory_failure(unsigned long pfn, int flags)\n{\n\tstruct page *p;\n\tstruct page *hpage;\n\tstruct dev_pagemap *pgmap;\n\tint res = 0;\n\tunsigned long page_flags;\n\tbool retry = true;\n\tint hugetlb = 0;\n\n\tif (!sysctl_memory_failure_recovery)\n\t\tpanic(\"Memory failure on page %lx\", pfn);\n\n\tmutex_lock(&mf_mutex);\n\n\tif (!(flags & MF_SW_SIMULATED))\n\t\thw_memory_failure = true;\n\n\tp = pfn_to_online_page(pfn);\n\tif (!p) {\n\t\tres = arch_memory_failure(pfn, flags);\n\t\tif (res == 0)\n\t\t\tgoto unlock_mutex;\n\n\t\tif (pfn_valid(pfn)) {\n\t\t\tpgmap = get_dev_pagemap(pfn, NULL);\n\t\t\tput_ref_page(pfn, flags);\n\t\t\tif (pgmap) {\n\t\t\t\tres = memory_failure_dev_pagemap(pfn, flags,\n\t\t\t\t\t\t\t\t pgmap);\n\t\t\t\tgoto unlock_mutex;\n\t\t\t}\n\t\t}\n\t\tpr_err(\"%#lx: memory outside kernel control\\n\", pfn);\n\t\tres = -ENXIO;\n\t\tgoto unlock_mutex;\n\t}\n\ntry_again:\n\tres = try_memory_failure_hugetlb(pfn, flags, &hugetlb);\n\tif (hugetlb)\n\t\tgoto unlock_mutex;\n\n\tif (TestSetPageHWPoison(p)) {\n\t\tpr_err(\"%#lx: already hardware poisoned\\n\", pfn);\n\t\tres = -EHWPOISON;\n\t\tif (flags & MF_ACTION_REQUIRED)\n\t\t\tres = kill_accessing_process(current, pfn, flags);\n\t\tif (flags & MF_COUNT_INCREASED)\n\t\t\tput_page(p);\n\t\tgoto unlock_mutex;\n\t}\n\n\t \n\tif (!(flags & MF_COUNT_INCREASED)) {\n\t\tres = get_hwpoison_page(p, flags);\n\t\tif (!res) {\n\t\t\tif (is_free_buddy_page(p)) {\n\t\t\t\tif (take_page_off_buddy(p)) {\n\t\t\t\t\tpage_ref_inc(p);\n\t\t\t\t\tres = MF_RECOVERED;\n\t\t\t\t} else {\n\t\t\t\t\t \n\t\t\t\t\tif (retry) {\n\t\t\t\t\t\tClearPageHWPoison(p);\n\t\t\t\t\t\tretry = false;\n\t\t\t\t\t\tgoto try_again;\n\t\t\t\t\t}\n\t\t\t\t\tres = MF_FAILED;\n\t\t\t\t}\n\t\t\t\tres = action_result(pfn, MF_MSG_BUDDY, res);\n\t\t\t} else {\n\t\t\t\tres = action_result(pfn, MF_MSG_KERNEL_HIGH_ORDER, MF_IGNORED);\n\t\t\t}\n\t\t\tgoto unlock_mutex;\n\t\t} else if (res < 0) {\n\t\t\tres = action_result(pfn, MF_MSG_UNKNOWN, MF_IGNORED);\n\t\t\tgoto unlock_mutex;\n\t\t}\n\t}\n\n\thpage = compound_head(p);\n\tif (PageTransHuge(hpage)) {\n\t\t \n\t\tSetPageHasHWPoisoned(hpage);\n\t\tif (try_to_split_thp_page(p) < 0) {\n\t\t\tres = action_result(pfn, MF_MSG_UNSPLIT_THP, MF_IGNORED);\n\t\t\tgoto unlock_mutex;\n\t\t}\n\t\tVM_BUG_ON_PAGE(!page_count(p), p);\n\t}\n\n\t \n\tshake_page(p);\n\n\tlock_page(p);\n\n\t \n\tif (PageCompound(p)) {\n\t\tif (retry) {\n\t\t\tClearPageHWPoison(p);\n\t\t\tunlock_page(p);\n\t\t\tput_page(p);\n\t\t\tflags &= ~MF_COUNT_INCREASED;\n\t\t\tretry = false;\n\t\t\tgoto try_again;\n\t\t}\n\t\tres = action_result(pfn, MF_MSG_DIFFERENT_COMPOUND, MF_IGNORED);\n\t\tgoto unlock_page;\n\t}\n\n\t \n\tpage_flags = p->flags;\n\n\tif (hwpoison_filter(p)) {\n\t\tClearPageHWPoison(p);\n\t\tunlock_page(p);\n\t\tput_page(p);\n\t\tres = -EOPNOTSUPP;\n\t\tgoto unlock_mutex;\n\t}\n\n\t \n\tif (!PageLRU(p) && !PageWriteback(p))\n\t\tgoto identify_page_state;\n\n\t \n\twait_on_page_writeback(p);\n\n\t \n\tif (!hwpoison_user_mappings(p, pfn, flags, p)) {\n\t\tres = action_result(pfn, MF_MSG_UNMAP_FAILED, MF_IGNORED);\n\t\tgoto unlock_page;\n\t}\n\n\t \n\tif (PageLRU(p) && !PageSwapCache(p) && p->mapping == NULL) {\n\t\tres = action_result(pfn, MF_MSG_TRUNCATED_LRU, MF_IGNORED);\n\t\tgoto unlock_page;\n\t}\n\nidentify_page_state:\n\tres = identify_page_state(pfn, p, page_flags);\n\tmutex_unlock(&mf_mutex);\n\treturn res;\nunlock_page:\n\tunlock_page(p);\nunlock_mutex:\n\tmutex_unlock(&mf_mutex);\n\treturn res;\n}\nEXPORT_SYMBOL_GPL(memory_failure);\n\n#define MEMORY_FAILURE_FIFO_ORDER\t4\n#define MEMORY_FAILURE_FIFO_SIZE\t(1 << MEMORY_FAILURE_FIFO_ORDER)\n\nstruct memory_failure_entry {\n\tunsigned long pfn;\n\tint flags;\n};\n\nstruct memory_failure_cpu {\n\tDECLARE_KFIFO(fifo, struct memory_failure_entry,\n\t\t      MEMORY_FAILURE_FIFO_SIZE);\n\tspinlock_t lock;\n\tstruct work_struct work;\n};\n\nstatic DEFINE_PER_CPU(struct memory_failure_cpu, memory_failure_cpu);\n\n \nvoid memory_failure_queue(unsigned long pfn, int flags)\n{\n\tstruct memory_failure_cpu *mf_cpu;\n\tunsigned long proc_flags;\n\tstruct memory_failure_entry entry = {\n\t\t.pfn =\t\tpfn,\n\t\t.flags =\tflags,\n\t};\n\n\tmf_cpu = &get_cpu_var(memory_failure_cpu);\n\tspin_lock_irqsave(&mf_cpu->lock, proc_flags);\n\tif (kfifo_put(&mf_cpu->fifo, entry))\n\t\tschedule_work_on(smp_processor_id(), &mf_cpu->work);\n\telse\n\t\tpr_err(\"buffer overflow when queuing memory failure at %#lx\\n\",\n\t\t       pfn);\n\tspin_unlock_irqrestore(&mf_cpu->lock, proc_flags);\n\tput_cpu_var(memory_failure_cpu);\n}\nEXPORT_SYMBOL_GPL(memory_failure_queue);\n\nstatic void memory_failure_work_func(struct work_struct *work)\n{\n\tstruct memory_failure_cpu *mf_cpu;\n\tstruct memory_failure_entry entry = { 0, };\n\tunsigned long proc_flags;\n\tint gotten;\n\n\tmf_cpu = container_of(work, struct memory_failure_cpu, work);\n\tfor (;;) {\n\t\tspin_lock_irqsave(&mf_cpu->lock, proc_flags);\n\t\tgotten = kfifo_get(&mf_cpu->fifo, &entry);\n\t\tspin_unlock_irqrestore(&mf_cpu->lock, proc_flags);\n\t\tif (!gotten)\n\t\t\tbreak;\n\t\tif (entry.flags & MF_SOFT_OFFLINE)\n\t\t\tsoft_offline_page(entry.pfn, entry.flags);\n\t\telse\n\t\t\tmemory_failure(entry.pfn, entry.flags);\n\t}\n}\n\n \nvoid memory_failure_queue_kick(int cpu)\n{\n\tstruct memory_failure_cpu *mf_cpu;\n\n\tmf_cpu = &per_cpu(memory_failure_cpu, cpu);\n\tcancel_work_sync(&mf_cpu->work);\n\tmemory_failure_work_func(&mf_cpu->work);\n}\n\nstatic int __init memory_failure_init(void)\n{\n\tstruct memory_failure_cpu *mf_cpu;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tmf_cpu = &per_cpu(memory_failure_cpu, cpu);\n\t\tspin_lock_init(&mf_cpu->lock);\n\t\tINIT_KFIFO(mf_cpu->fifo);\n\t\tINIT_WORK(&mf_cpu->work, memory_failure_work_func);\n\t}\n\n\tregister_sysctl_init(\"vm\", memory_failure_table);\n\n\treturn 0;\n}\ncore_initcall(memory_failure_init);\n\n#undef pr_fmt\n#define pr_fmt(fmt)\t\"\" fmt\n#define unpoison_pr_info(fmt, pfn, rs)\t\t\t\\\n({\t\t\t\t\t\t\t\\\n\tif (__ratelimit(rs))\t\t\t\t\\\n\t\tpr_info(fmt, pfn);\t\t\t\\\n})\n\n \nint unpoison_memory(unsigned long pfn)\n{\n\tstruct folio *folio;\n\tstruct page *p;\n\tint ret = -EBUSY, ghp;\n\tunsigned long count = 1;\n\tbool huge = false;\n\tstatic DEFINE_RATELIMIT_STATE(unpoison_rs, DEFAULT_RATELIMIT_INTERVAL,\n\t\t\t\t\tDEFAULT_RATELIMIT_BURST);\n\n\tif (!pfn_valid(pfn))\n\t\treturn -ENXIO;\n\n\tp = pfn_to_page(pfn);\n\tfolio = page_folio(p);\n\n\tmutex_lock(&mf_mutex);\n\n\tif (hw_memory_failure) {\n\t\tunpoison_pr_info(\"Unpoison: Disabled after HW memory failure %#lx\\n\",\n\t\t\t\t pfn, &unpoison_rs);\n\t\tret = -EOPNOTSUPP;\n\t\tgoto unlock_mutex;\n\t}\n\n\tif (!PageHWPoison(p)) {\n\t\tunpoison_pr_info(\"Unpoison: Page was already unpoisoned %#lx\\n\",\n\t\t\t\t pfn, &unpoison_rs);\n\t\tgoto unlock_mutex;\n\t}\n\n\tif (folio_ref_count(folio) > 1) {\n\t\tunpoison_pr_info(\"Unpoison: Someone grabs the hwpoison page %#lx\\n\",\n\t\t\t\t pfn, &unpoison_rs);\n\t\tgoto unlock_mutex;\n\t}\n\n\tif (folio_test_slab(folio) || PageTable(&folio->page) ||\n\t    folio_test_reserved(folio) || PageOffline(&folio->page))\n\t\tgoto unlock_mutex;\n\n\t \n\tif (folio_mapped(folio)) {\n\t\tunpoison_pr_info(\"Unpoison: Someone maps the hwpoison page %#lx\\n\",\n\t\t\t\t pfn, &unpoison_rs);\n\t\tgoto unlock_mutex;\n\t}\n\n\tif (folio_mapping(folio)) {\n\t\tunpoison_pr_info(\"Unpoison: the hwpoison page has non-NULL mapping %#lx\\n\",\n\t\t\t\t pfn, &unpoison_rs);\n\t\tgoto unlock_mutex;\n\t}\n\n\tghp = get_hwpoison_page(p, MF_UNPOISON);\n\tif (!ghp) {\n\t\tif (PageHuge(p)) {\n\t\t\thuge = true;\n\t\t\tcount = folio_free_raw_hwp(folio, false);\n\t\t\tif (count == 0)\n\t\t\t\tgoto unlock_mutex;\n\t\t}\n\t\tret = folio_test_clear_hwpoison(folio) ? 0 : -EBUSY;\n\t} else if (ghp < 0) {\n\t\tif (ghp == -EHWPOISON) {\n\t\t\tret = put_page_back_buddy(p) ? 0 : -EBUSY;\n\t\t} else {\n\t\t\tret = ghp;\n\t\t\tunpoison_pr_info(\"Unpoison: failed to grab page %#lx\\n\",\n\t\t\t\t\t pfn, &unpoison_rs);\n\t\t}\n\t} else {\n\t\tif (PageHuge(p)) {\n\t\t\thuge = true;\n\t\t\tcount = folio_free_raw_hwp(folio, false);\n\t\t\tif (count == 0) {\n\t\t\t\tfolio_put(folio);\n\t\t\t\tgoto unlock_mutex;\n\t\t\t}\n\t\t}\n\n\t\tfolio_put(folio);\n\t\tif (TestClearPageHWPoison(p)) {\n\t\t\tfolio_put(folio);\n\t\t\tret = 0;\n\t\t}\n\t}\n\nunlock_mutex:\n\tmutex_unlock(&mf_mutex);\n\tif (!ret) {\n\t\tif (!huge)\n\t\t\tnum_poisoned_pages_sub(pfn, 1);\n\t\tunpoison_pr_info(\"Unpoison: Software-unpoisoned page %#lx\\n\",\n\t\t\t\t page_to_pfn(p), &unpoison_rs);\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL(unpoison_memory);\n\nstatic bool isolate_page(struct page *page, struct list_head *pagelist)\n{\n\tbool isolated = false;\n\n\tif (PageHuge(page)) {\n\t\tisolated = isolate_hugetlb(page_folio(page), pagelist);\n\t} else {\n\t\tbool lru = !__PageMovable(page);\n\n\t\tif (lru)\n\t\t\tisolated = isolate_lru_page(page);\n\t\telse\n\t\t\tisolated = isolate_movable_page(page,\n\t\t\t\t\t\t\tISOLATE_UNEVICTABLE);\n\n\t\tif (isolated) {\n\t\t\tlist_add(&page->lru, pagelist);\n\t\t\tif (lru)\n\t\t\t\tinc_node_page_state(page, NR_ISOLATED_ANON +\n\t\t\t\t\t\t    page_is_file_lru(page));\n\t\t}\n\t}\n\n\t \n\tput_page(page);\n\treturn isolated;\n}\n\n \nstatic int soft_offline_in_use_page(struct page *page)\n{\n\tlong ret = 0;\n\tunsigned long pfn = page_to_pfn(page);\n\tstruct page *hpage = compound_head(page);\n\tchar const *msg_page[] = {\"page\", \"hugepage\"};\n\tbool huge = PageHuge(page);\n\tLIST_HEAD(pagelist);\n\tstruct migration_target_control mtc = {\n\t\t.nid = NUMA_NO_NODE,\n\t\t.gfp_mask = GFP_USER | __GFP_MOVABLE | __GFP_RETRY_MAYFAIL,\n\t};\n\n\tif (!huge && PageTransHuge(hpage)) {\n\t\tif (try_to_split_thp_page(page)) {\n\t\t\tpr_info(\"soft offline: %#lx: thp split failed\\n\", pfn);\n\t\t\treturn -EBUSY;\n\t\t}\n\t\thpage = page;\n\t}\n\n\tlock_page(page);\n\tif (!huge)\n\t\twait_on_page_writeback(page);\n\tif (PageHWPoison(page)) {\n\t\tunlock_page(page);\n\t\tput_page(page);\n\t\tpr_info(\"soft offline: %#lx page already poisoned\\n\", pfn);\n\t\treturn 0;\n\t}\n\n\tif (!huge && PageLRU(page) && !PageSwapCache(page))\n\t\t \n\t\tret = invalidate_inode_page(page);\n\tunlock_page(page);\n\n\tif (ret) {\n\t\tpr_info(\"soft_offline: %#lx: invalidated\\n\", pfn);\n\t\tpage_handle_poison(page, false, true);\n\t\treturn 0;\n\t}\n\n\tif (isolate_page(hpage, &pagelist)) {\n\t\tret = migrate_pages(&pagelist, alloc_migration_target, NULL,\n\t\t\t(unsigned long)&mtc, MIGRATE_SYNC, MR_MEMORY_FAILURE, NULL);\n\t\tif (!ret) {\n\t\t\tbool release = !huge;\n\n\t\t\tif (!page_handle_poison(page, huge, release))\n\t\t\t\tret = -EBUSY;\n\t\t} else {\n\t\t\tif (!list_empty(&pagelist))\n\t\t\t\tputback_movable_pages(&pagelist);\n\n\t\t\tpr_info(\"soft offline: %#lx: %s migration failed %ld, type %pGp\\n\",\n\t\t\t\tpfn, msg_page[huge], ret, &page->flags);\n\t\t\tif (ret > 0)\n\t\t\t\tret = -EBUSY;\n\t\t}\n\t} else {\n\t\tpr_info(\"soft offline: %#lx: %s isolation failed, page count %d, type %pGp\\n\",\n\t\t\tpfn, msg_page[huge], page_count(page), &page->flags);\n\t\tret = -EBUSY;\n\t}\n\treturn ret;\n}\n\n \nint soft_offline_page(unsigned long pfn, int flags)\n{\n\tint ret;\n\tbool try_again = true;\n\tstruct page *page;\n\n\tif (!pfn_valid(pfn)) {\n\t\tWARN_ON_ONCE(flags & MF_COUNT_INCREASED);\n\t\treturn -ENXIO;\n\t}\n\n\t \n\tpage = pfn_to_online_page(pfn);\n\tif (!page) {\n\t\tput_ref_page(pfn, flags);\n\t\treturn -EIO;\n\t}\n\n\tmutex_lock(&mf_mutex);\n\n\tif (PageHWPoison(page)) {\n\t\tpr_info(\"%s: %#lx page already poisoned\\n\", __func__, pfn);\n\t\tput_ref_page(pfn, flags);\n\t\tmutex_unlock(&mf_mutex);\n\t\treturn 0;\n\t}\n\nretry:\n\tget_online_mems();\n\tret = get_hwpoison_page(page, flags | MF_SOFT_OFFLINE);\n\tput_online_mems();\n\n\tif (hwpoison_filter(page)) {\n\t\tif (ret > 0)\n\t\t\tput_page(page);\n\n\t\tmutex_unlock(&mf_mutex);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (ret > 0) {\n\t\tret = soft_offline_in_use_page(page);\n\t} else if (ret == 0) {\n\t\tif (!page_handle_poison(page, true, false)) {\n\t\t\tif (try_again) {\n\t\t\t\ttry_again = false;\n\t\t\t\tflags &= ~MF_COUNT_INCREASED;\n\t\t\t\tgoto retry;\n\t\t\t}\n\t\t\tret = -EBUSY;\n\t\t}\n\t}\n\n\tmutex_unlock(&mf_mutex);\n\n\treturn ret;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}