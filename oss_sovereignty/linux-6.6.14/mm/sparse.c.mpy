{
  "module_name": "sparse.c",
  "hash_id": "32036d4821fd98b47edecdc014f41df049513361690011bd9efbc15346084afa",
  "original_prompt": "Ingested from linux-6.6.14/mm/sparse.c",
  "human_readable_source": "\n \n#include <linux/mm.h>\n#include <linux/slab.h>\n#include <linux/mmzone.h>\n#include <linux/memblock.h>\n#include <linux/compiler.h>\n#include <linux/highmem.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n#include <linux/vmalloc.h>\n#include <linux/swap.h>\n#include <linux/swapops.h>\n#include <linux/bootmem_info.h>\n\n#include \"internal.h\"\n#include <asm/dma.h>\n\n \n#ifdef CONFIG_SPARSEMEM_EXTREME\nstruct mem_section **mem_section;\n#else\nstruct mem_section mem_section[NR_SECTION_ROOTS][SECTIONS_PER_ROOT]\n\t____cacheline_internodealigned_in_smp;\n#endif\nEXPORT_SYMBOL(mem_section);\n\n#ifdef NODE_NOT_IN_PAGE_FLAGS\n \n#if MAX_NUMNODES <= 256\nstatic u8 section_to_node_table[NR_MEM_SECTIONS] __cacheline_aligned;\n#else\nstatic u16 section_to_node_table[NR_MEM_SECTIONS] __cacheline_aligned;\n#endif\n\nint page_to_nid(const struct page *page)\n{\n\treturn section_to_node_table[page_to_section(page)];\n}\nEXPORT_SYMBOL(page_to_nid);\n\nstatic void set_section_nid(unsigned long section_nr, int nid)\n{\n\tsection_to_node_table[section_nr] = nid;\n}\n#else  \nstatic inline void set_section_nid(unsigned long section_nr, int nid)\n{\n}\n#endif\n\n#ifdef CONFIG_SPARSEMEM_EXTREME\nstatic noinline struct mem_section __ref *sparse_index_alloc(int nid)\n{\n\tstruct mem_section *section = NULL;\n\tunsigned long array_size = SECTIONS_PER_ROOT *\n\t\t\t\t   sizeof(struct mem_section);\n\n\tif (slab_is_available()) {\n\t\tsection = kzalloc_node(array_size, GFP_KERNEL, nid);\n\t} else {\n\t\tsection = memblock_alloc_node(array_size, SMP_CACHE_BYTES,\n\t\t\t\t\t      nid);\n\t\tif (!section)\n\t\t\tpanic(\"%s: Failed to allocate %lu bytes nid=%d\\n\",\n\t\t\t      __func__, array_size, nid);\n\t}\n\n\treturn section;\n}\n\nstatic int __meminit sparse_index_init(unsigned long section_nr, int nid)\n{\n\tunsigned long root = SECTION_NR_TO_ROOT(section_nr);\n\tstruct mem_section *section;\n\n\t \n\tif (mem_section[root])\n\t\treturn 0;\n\n\tsection = sparse_index_alloc(nid);\n\tif (!section)\n\t\treturn -ENOMEM;\n\n\tmem_section[root] = section;\n\n\treturn 0;\n}\n#else  \nstatic inline int sparse_index_init(unsigned long section_nr, int nid)\n{\n\treturn 0;\n}\n#endif\n\n \nstatic inline unsigned long sparse_encode_early_nid(int nid)\n{\n\treturn ((unsigned long)nid << SECTION_NID_SHIFT);\n}\n\nstatic inline int sparse_early_nid(struct mem_section *section)\n{\n\treturn (section->section_mem_map >> SECTION_NID_SHIFT);\n}\n\n \nstatic void __meminit mminit_validate_memmodel_limits(unsigned long *start_pfn,\n\t\t\t\t\t\tunsigned long *end_pfn)\n{\n\tunsigned long max_sparsemem_pfn = 1UL << (MAX_PHYSMEM_BITS-PAGE_SHIFT);\n\n\t \n\tif (*start_pfn > max_sparsemem_pfn) {\n\t\tmminit_dprintk(MMINIT_WARNING, \"pfnvalidation\",\n\t\t\t\"Start of range %lu -> %lu exceeds SPARSEMEM max %lu\\n\",\n\t\t\t*start_pfn, *end_pfn, max_sparsemem_pfn);\n\t\tWARN_ON_ONCE(1);\n\t\t*start_pfn = max_sparsemem_pfn;\n\t\t*end_pfn = max_sparsemem_pfn;\n\t} else if (*end_pfn > max_sparsemem_pfn) {\n\t\tmminit_dprintk(MMINIT_WARNING, \"pfnvalidation\",\n\t\t\t\"End of range %lu -> %lu exceeds SPARSEMEM max %lu\\n\",\n\t\t\t*start_pfn, *end_pfn, max_sparsemem_pfn);\n\t\tWARN_ON_ONCE(1);\n\t\t*end_pfn = max_sparsemem_pfn;\n\t}\n}\n\n \nunsigned long __highest_present_section_nr;\nstatic void __section_mark_present(struct mem_section *ms,\n\t\tunsigned long section_nr)\n{\n\tif (section_nr > __highest_present_section_nr)\n\t\t__highest_present_section_nr = section_nr;\n\n\tms->section_mem_map |= SECTION_MARKED_PRESENT;\n}\n\n#define for_each_present_section_nr(start, section_nr)\t\t\\\n\tfor (section_nr = next_present_section_nr(start-1);\t\\\n\t     section_nr != -1;\t\t\t\t\t\t\t\t\\\n\t     section_nr = next_present_section_nr(section_nr))\n\nstatic inline unsigned long first_present_section_nr(void)\n{\n\treturn next_present_section_nr(-1);\n}\n\n#ifdef CONFIG_SPARSEMEM_VMEMMAP\nstatic void subsection_mask_set(unsigned long *map, unsigned long pfn,\n\t\tunsigned long nr_pages)\n{\n\tint idx = subsection_map_index(pfn);\n\tint end = subsection_map_index(pfn + nr_pages - 1);\n\n\tbitmap_set(map, idx, end - idx + 1);\n}\n\nvoid __init subsection_map_init(unsigned long pfn, unsigned long nr_pages)\n{\n\tint end_sec = pfn_to_section_nr(pfn + nr_pages - 1);\n\tunsigned long nr, start_sec = pfn_to_section_nr(pfn);\n\n\tif (!nr_pages)\n\t\treturn;\n\n\tfor (nr = start_sec; nr <= end_sec; nr++) {\n\t\tstruct mem_section *ms;\n\t\tunsigned long pfns;\n\n\t\tpfns = min(nr_pages, PAGES_PER_SECTION\n\t\t\t\t- (pfn & ~PAGE_SECTION_MASK));\n\t\tms = __nr_to_section(nr);\n\t\tsubsection_mask_set(ms->usage->subsection_map, pfn, pfns);\n\n\t\tpr_debug(\"%s: sec: %lu pfns: %lu set(%d, %d)\\n\", __func__, nr,\n\t\t\t\tpfns, subsection_map_index(pfn),\n\t\t\t\tsubsection_map_index(pfn + pfns - 1));\n\n\t\tpfn += pfns;\n\t\tnr_pages -= pfns;\n\t}\n}\n#else\nvoid __init subsection_map_init(unsigned long pfn, unsigned long nr_pages)\n{\n}\n#endif\n\n \nstatic void __init memory_present(int nid, unsigned long start, unsigned long end)\n{\n\tunsigned long pfn;\n\n#ifdef CONFIG_SPARSEMEM_EXTREME\n\tif (unlikely(!mem_section)) {\n\t\tunsigned long size, align;\n\n\t\tsize = sizeof(struct mem_section *) * NR_SECTION_ROOTS;\n\t\talign = 1 << (INTERNODE_CACHE_SHIFT);\n\t\tmem_section = memblock_alloc(size, align);\n\t\tif (!mem_section)\n\t\t\tpanic(\"%s: Failed to allocate %lu bytes align=0x%lx\\n\",\n\t\t\t      __func__, size, align);\n\t}\n#endif\n\n\tstart &= PAGE_SECTION_MASK;\n\tmminit_validate_memmodel_limits(&start, &end);\n\tfor (pfn = start; pfn < end; pfn += PAGES_PER_SECTION) {\n\t\tunsigned long section = pfn_to_section_nr(pfn);\n\t\tstruct mem_section *ms;\n\n\t\tsparse_index_init(section, nid);\n\t\tset_section_nid(section, nid);\n\n\t\tms = __nr_to_section(section);\n\t\tif (!ms->section_mem_map) {\n\t\t\tms->section_mem_map = sparse_encode_early_nid(nid) |\n\t\t\t\t\t\t\tSECTION_IS_ONLINE;\n\t\t\t__section_mark_present(ms, section);\n\t\t}\n\t}\n}\n\n \nstatic void __init memblocks_present(void)\n{\n\tunsigned long start, end;\n\tint i, nid;\n\n\tfor_each_mem_pfn_range(i, MAX_NUMNODES, &start, &end, &nid)\n\t\tmemory_present(nid, start, end);\n}\n\n \nstatic unsigned long sparse_encode_mem_map(struct page *mem_map, unsigned long pnum)\n{\n\tunsigned long coded_mem_map =\n\t\t(unsigned long)(mem_map - (section_nr_to_pfn(pnum)));\n\tBUILD_BUG_ON(SECTION_MAP_LAST_BIT > PFN_SECTION_SHIFT);\n\tBUG_ON(coded_mem_map & ~SECTION_MAP_MASK);\n\treturn coded_mem_map;\n}\n\n#ifdef CONFIG_MEMORY_HOTPLUG\n \nstruct page *sparse_decode_mem_map(unsigned long coded_mem_map, unsigned long pnum)\n{\n\t \n\tcoded_mem_map &= SECTION_MAP_MASK;\n\treturn ((struct page *)coded_mem_map) + section_nr_to_pfn(pnum);\n}\n#endif  \n\nstatic void __meminit sparse_init_one_section(struct mem_section *ms,\n\t\tunsigned long pnum, struct page *mem_map,\n\t\tstruct mem_section_usage *usage, unsigned long flags)\n{\n\tms->section_mem_map &= ~SECTION_MAP_MASK;\n\tms->section_mem_map |= sparse_encode_mem_map(mem_map, pnum)\n\t\t| SECTION_HAS_MEM_MAP | flags;\n\tms->usage = usage;\n}\n\nstatic unsigned long usemap_size(void)\n{\n\treturn BITS_TO_LONGS(SECTION_BLOCKFLAGS_BITS) * sizeof(unsigned long);\n}\n\nsize_t mem_section_usage_size(void)\n{\n\treturn sizeof(struct mem_section_usage) + usemap_size();\n}\n\n#ifdef CONFIG_MEMORY_HOTREMOVE\nstatic inline phys_addr_t pgdat_to_phys(struct pglist_data *pgdat)\n{\n#ifndef CONFIG_NUMA\n\tVM_BUG_ON(pgdat != &contig_page_data);\n\treturn __pa_symbol(&contig_page_data);\n#else\n\treturn __pa(pgdat);\n#endif\n}\n\nstatic struct mem_section_usage * __init\nsparse_early_usemaps_alloc_pgdat_section(struct pglist_data *pgdat,\n\t\t\t\t\t unsigned long size)\n{\n\tstruct mem_section_usage *usage;\n\tunsigned long goal, limit;\n\tint nid;\n\t \n\tgoal = pgdat_to_phys(pgdat) & (PAGE_SECTION_MASK << PAGE_SHIFT);\n\tlimit = goal + (1UL << PA_SECTION_SHIFT);\n\tnid = early_pfn_to_nid(goal >> PAGE_SHIFT);\nagain:\n\tusage = memblock_alloc_try_nid(size, SMP_CACHE_BYTES, goal, limit, nid);\n\tif (!usage && limit) {\n\t\tlimit = 0;\n\t\tgoto again;\n\t}\n\treturn usage;\n}\n\nstatic void __init check_usemap_section_nr(int nid,\n\t\tstruct mem_section_usage *usage)\n{\n\tunsigned long usemap_snr, pgdat_snr;\n\tstatic unsigned long old_usemap_snr;\n\tstatic unsigned long old_pgdat_snr;\n\tstruct pglist_data *pgdat = NODE_DATA(nid);\n\tint usemap_nid;\n\n\t \n\tif (!old_usemap_snr) {\n\t\told_usemap_snr = NR_MEM_SECTIONS;\n\t\told_pgdat_snr = NR_MEM_SECTIONS;\n\t}\n\n\tusemap_snr = pfn_to_section_nr(__pa(usage) >> PAGE_SHIFT);\n\tpgdat_snr = pfn_to_section_nr(pgdat_to_phys(pgdat) >> PAGE_SHIFT);\n\tif (usemap_snr == pgdat_snr)\n\t\treturn;\n\n\tif (old_usemap_snr == usemap_snr && old_pgdat_snr == pgdat_snr)\n\t\t \n\t\treturn;\n\n\told_usemap_snr = usemap_snr;\n\told_pgdat_snr = pgdat_snr;\n\n\tusemap_nid = sparse_early_nid(__nr_to_section(usemap_snr));\n\tif (usemap_nid != nid) {\n\t\tpr_info(\"node %d must be removed before remove section %ld\\n\",\n\t\t\tnid, usemap_snr);\n\t\treturn;\n\t}\n\t \n\tpr_info(\"Section %ld and %ld (node %d) have a circular dependency on usemap and pgdat allocations\\n\",\n\t\tusemap_snr, pgdat_snr, nid);\n}\n#else\nstatic struct mem_section_usage * __init\nsparse_early_usemaps_alloc_pgdat_section(struct pglist_data *pgdat,\n\t\t\t\t\t unsigned long size)\n{\n\treturn memblock_alloc_node(size, SMP_CACHE_BYTES, pgdat->node_id);\n}\n\nstatic void __init check_usemap_section_nr(int nid,\n\t\tstruct mem_section_usage *usage)\n{\n}\n#endif  \n\n#ifdef CONFIG_SPARSEMEM_VMEMMAP\nstatic unsigned long __init section_map_size(void)\n{\n\treturn ALIGN(sizeof(struct page) * PAGES_PER_SECTION, PMD_SIZE);\n}\n\n#else\nstatic unsigned long __init section_map_size(void)\n{\n\treturn PAGE_ALIGN(sizeof(struct page) * PAGES_PER_SECTION);\n}\n\nstruct page __init *__populate_section_memmap(unsigned long pfn,\n\t\tunsigned long nr_pages, int nid, struct vmem_altmap *altmap,\n\t\tstruct dev_pagemap *pgmap)\n{\n\tunsigned long size = section_map_size();\n\tstruct page *map = sparse_buffer_alloc(size);\n\tphys_addr_t addr = __pa(MAX_DMA_ADDRESS);\n\n\tif (map)\n\t\treturn map;\n\n\tmap = memmap_alloc(size, size, addr, nid, false);\n\tif (!map)\n\t\tpanic(\"%s: Failed to allocate %lu bytes align=0x%lx nid=%d from=%pa\\n\",\n\t\t      __func__, size, PAGE_SIZE, nid, &addr);\n\n\treturn map;\n}\n#endif  \n\nstatic void *sparsemap_buf __meminitdata;\nstatic void *sparsemap_buf_end __meminitdata;\n\nstatic inline void __meminit sparse_buffer_free(unsigned long size)\n{\n\tWARN_ON(!sparsemap_buf || size == 0);\n\tmemblock_free(sparsemap_buf, size);\n}\n\nstatic void __init sparse_buffer_init(unsigned long size, int nid)\n{\n\tphys_addr_t addr = __pa(MAX_DMA_ADDRESS);\n\tWARN_ON(sparsemap_buf);\t \n\t \n\tsparsemap_buf = memmap_alloc(size, section_map_size(), addr, nid, true);\n\tsparsemap_buf_end = sparsemap_buf + size;\n}\n\nstatic void __init sparse_buffer_fini(void)\n{\n\tunsigned long size = sparsemap_buf_end - sparsemap_buf;\n\n\tif (sparsemap_buf && size > 0)\n\t\tsparse_buffer_free(size);\n\tsparsemap_buf = NULL;\n}\n\nvoid * __meminit sparse_buffer_alloc(unsigned long size)\n{\n\tvoid *ptr = NULL;\n\n\tif (sparsemap_buf) {\n\t\tptr = (void *) roundup((unsigned long)sparsemap_buf, size);\n\t\tif (ptr + size > sparsemap_buf_end)\n\t\t\tptr = NULL;\n\t\telse {\n\t\t\t \n\t\t\tif ((unsigned long)(ptr - sparsemap_buf) > 0)\n\t\t\t\tsparse_buffer_free((unsigned long)(ptr - sparsemap_buf));\n\t\t\tsparsemap_buf = ptr + size;\n\t\t}\n\t}\n\treturn ptr;\n}\n\nvoid __weak __meminit vmemmap_populate_print_last(void)\n{\n}\n\n \nstatic void __init sparse_init_nid(int nid, unsigned long pnum_begin,\n\t\t\t\t   unsigned long pnum_end,\n\t\t\t\t   unsigned long map_count)\n{\n\tstruct mem_section_usage *usage;\n\tunsigned long pnum;\n\tstruct page *map;\n\n\tusage = sparse_early_usemaps_alloc_pgdat_section(NODE_DATA(nid),\n\t\t\tmem_section_usage_size() * map_count);\n\tif (!usage) {\n\t\tpr_err(\"%s: node[%d] usemap allocation failed\", __func__, nid);\n\t\tgoto failed;\n\t}\n\tsparse_buffer_init(map_count * section_map_size(), nid);\n\tfor_each_present_section_nr(pnum_begin, pnum) {\n\t\tunsigned long pfn = section_nr_to_pfn(pnum);\n\n\t\tif (pnum >= pnum_end)\n\t\t\tbreak;\n\n\t\tmap = __populate_section_memmap(pfn, PAGES_PER_SECTION,\n\t\t\t\tnid, NULL, NULL);\n\t\tif (!map) {\n\t\t\tpr_err(\"%s: node[%d] memory map backing failed. Some memory will not be available.\",\n\t\t\t       __func__, nid);\n\t\t\tpnum_begin = pnum;\n\t\t\tsparse_buffer_fini();\n\t\t\tgoto failed;\n\t\t}\n\t\tcheck_usemap_section_nr(nid, usage);\n\t\tsparse_init_one_section(__nr_to_section(pnum), pnum, map, usage,\n\t\t\t\tSECTION_IS_EARLY);\n\t\tusage = (void *) usage + mem_section_usage_size();\n\t}\n\tsparse_buffer_fini();\n\treturn;\nfailed:\n\t \n\tfor_each_present_section_nr(pnum_begin, pnum) {\n\t\tstruct mem_section *ms;\n\n\t\tif (pnum >= pnum_end)\n\t\t\tbreak;\n\t\tms = __nr_to_section(pnum);\n\t\tms->section_mem_map = 0;\n\t}\n}\n\n \nvoid __init sparse_init(void)\n{\n\tunsigned long pnum_end, pnum_begin, map_count = 1;\n\tint nid_begin;\n\n\tmemblocks_present();\n\n\tpnum_begin = first_present_section_nr();\n\tnid_begin = sparse_early_nid(__nr_to_section(pnum_begin));\n\n\t \n\tset_pageblock_order();\n\n\tfor_each_present_section_nr(pnum_begin + 1, pnum_end) {\n\t\tint nid = sparse_early_nid(__nr_to_section(pnum_end));\n\n\t\tif (nid == nid_begin) {\n\t\t\tmap_count++;\n\t\t\tcontinue;\n\t\t}\n\t\t \n\t\tsparse_init_nid(nid_begin, pnum_begin, pnum_end, map_count);\n\t\tnid_begin = nid;\n\t\tpnum_begin = pnum_end;\n\t\tmap_count = 1;\n\t}\n\t \n\tsparse_init_nid(nid_begin, pnum_begin, pnum_end, map_count);\n\tvmemmap_populate_print_last();\n}\n\n#ifdef CONFIG_MEMORY_HOTPLUG\n\n \nvoid online_mem_sections(unsigned long start_pfn, unsigned long end_pfn)\n{\n\tunsigned long pfn;\n\n\tfor (pfn = start_pfn; pfn < end_pfn; pfn += PAGES_PER_SECTION) {\n\t\tunsigned long section_nr = pfn_to_section_nr(pfn);\n\t\tstruct mem_section *ms;\n\n\t\t \n\t\tif (WARN_ON(!valid_section_nr(section_nr)))\n\t\t\tcontinue;\n\n\t\tms = __nr_to_section(section_nr);\n\t\tms->section_mem_map |= SECTION_IS_ONLINE;\n\t}\n}\n\n \nvoid offline_mem_sections(unsigned long start_pfn, unsigned long end_pfn)\n{\n\tunsigned long pfn;\n\n\tfor (pfn = start_pfn; pfn < end_pfn; pfn += PAGES_PER_SECTION) {\n\t\tunsigned long section_nr = pfn_to_section_nr(pfn);\n\t\tstruct mem_section *ms;\n\n\t\t \n\t\tif (WARN_ON(!valid_section_nr(section_nr)))\n\t\t\tcontinue;\n\n\t\tms = __nr_to_section(section_nr);\n\t\tms->section_mem_map &= ~SECTION_IS_ONLINE;\n\t}\n}\n\n#ifdef CONFIG_SPARSEMEM_VMEMMAP\nstatic struct page * __meminit populate_section_memmap(unsigned long pfn,\n\t\tunsigned long nr_pages, int nid, struct vmem_altmap *altmap,\n\t\tstruct dev_pagemap *pgmap)\n{\n\treturn __populate_section_memmap(pfn, nr_pages, nid, altmap, pgmap);\n}\n\nstatic void depopulate_section_memmap(unsigned long pfn, unsigned long nr_pages,\n\t\tstruct vmem_altmap *altmap)\n{\n\tunsigned long start = (unsigned long) pfn_to_page(pfn);\n\tunsigned long end = start + nr_pages * sizeof(struct page);\n\n\tvmemmap_free(start, end, altmap);\n}\nstatic void free_map_bootmem(struct page *memmap)\n{\n\tunsigned long start = (unsigned long)memmap;\n\tunsigned long end = (unsigned long)(memmap + PAGES_PER_SECTION);\n\n\tvmemmap_free(start, end, NULL);\n}\n\nstatic int clear_subsection_map(unsigned long pfn, unsigned long nr_pages)\n{\n\tDECLARE_BITMAP(map, SUBSECTIONS_PER_SECTION) = { 0 };\n\tDECLARE_BITMAP(tmp, SUBSECTIONS_PER_SECTION) = { 0 };\n\tstruct mem_section *ms = __pfn_to_section(pfn);\n\tunsigned long *subsection_map = ms->usage\n\t\t? &ms->usage->subsection_map[0] : NULL;\n\n\tsubsection_mask_set(map, pfn, nr_pages);\n\tif (subsection_map)\n\t\tbitmap_and(tmp, map, subsection_map, SUBSECTIONS_PER_SECTION);\n\n\tif (WARN(!subsection_map || !bitmap_equal(tmp, map, SUBSECTIONS_PER_SECTION),\n\t\t\t\t\"section already deactivated (%#lx + %ld)\\n\",\n\t\t\t\tpfn, nr_pages))\n\t\treturn -EINVAL;\n\n\tbitmap_xor(subsection_map, map, subsection_map, SUBSECTIONS_PER_SECTION);\n\treturn 0;\n}\n\nstatic bool is_subsection_map_empty(struct mem_section *ms)\n{\n\treturn bitmap_empty(&ms->usage->subsection_map[0],\n\t\t\t    SUBSECTIONS_PER_SECTION);\n}\n\nstatic int fill_subsection_map(unsigned long pfn, unsigned long nr_pages)\n{\n\tstruct mem_section *ms = __pfn_to_section(pfn);\n\tDECLARE_BITMAP(map, SUBSECTIONS_PER_SECTION) = { 0 };\n\tunsigned long *subsection_map;\n\tint rc = 0;\n\n\tsubsection_mask_set(map, pfn, nr_pages);\n\n\tsubsection_map = &ms->usage->subsection_map[0];\n\n\tif (bitmap_empty(map, SUBSECTIONS_PER_SECTION))\n\t\trc = -EINVAL;\n\telse if (bitmap_intersects(map, subsection_map, SUBSECTIONS_PER_SECTION))\n\t\trc = -EEXIST;\n\telse\n\t\tbitmap_or(subsection_map, map, subsection_map,\n\t\t\t\tSUBSECTIONS_PER_SECTION);\n\n\treturn rc;\n}\n#else\nstatic struct page * __meminit populate_section_memmap(unsigned long pfn,\n\t\tunsigned long nr_pages, int nid, struct vmem_altmap *altmap,\n\t\tstruct dev_pagemap *pgmap)\n{\n\treturn kvmalloc_node(array_size(sizeof(struct page),\n\t\t\t\t\tPAGES_PER_SECTION), GFP_KERNEL, nid);\n}\n\nstatic void depopulate_section_memmap(unsigned long pfn, unsigned long nr_pages,\n\t\tstruct vmem_altmap *altmap)\n{\n\tkvfree(pfn_to_page(pfn));\n}\n\nstatic void free_map_bootmem(struct page *memmap)\n{\n\tunsigned long maps_section_nr, removing_section_nr, i;\n\tunsigned long magic, nr_pages;\n\tstruct page *page = virt_to_page(memmap);\n\n\tnr_pages = PAGE_ALIGN(PAGES_PER_SECTION * sizeof(struct page))\n\t\t>> PAGE_SHIFT;\n\n\tfor (i = 0; i < nr_pages; i++, page++) {\n\t\tmagic = page->index;\n\n\t\tBUG_ON(magic == NODE_INFO);\n\n\t\tmaps_section_nr = pfn_to_section_nr(page_to_pfn(page));\n\t\tremoving_section_nr = page_private(page);\n\n\t\t \n\t\tif (maps_section_nr != removing_section_nr)\n\t\t\tput_page_bootmem(page);\n\t}\n}\n\nstatic int clear_subsection_map(unsigned long pfn, unsigned long nr_pages)\n{\n\treturn 0;\n}\n\nstatic bool is_subsection_map_empty(struct mem_section *ms)\n{\n\treturn true;\n}\n\nstatic int fill_subsection_map(unsigned long pfn, unsigned long nr_pages)\n{\n\treturn 0;\n}\n#endif  \n\n \nstatic void section_deactivate(unsigned long pfn, unsigned long nr_pages,\n\t\tstruct vmem_altmap *altmap)\n{\n\tstruct mem_section *ms = __pfn_to_section(pfn);\n\tbool section_is_early = early_section(ms);\n\tstruct page *memmap = NULL;\n\tbool empty;\n\n\tif (clear_subsection_map(pfn, nr_pages))\n\t\treturn;\n\n\tempty = is_subsection_map_empty(ms);\n\tif (empty) {\n\t\tunsigned long section_nr = pfn_to_section_nr(pfn);\n\n\t\t \n\t\tif (!PageReserved(virt_to_page(ms->usage))) {\n\t\t\tkfree(ms->usage);\n\t\t\tms->usage = NULL;\n\t\t}\n\t\tmemmap = sparse_decode_mem_map(ms->section_mem_map, section_nr);\n\t\t \n\t\tms->section_mem_map &= ~SECTION_HAS_MEM_MAP;\n\t}\n\n\t \n\tif (!section_is_early)\n\t\tdepopulate_section_memmap(pfn, nr_pages, altmap);\n\telse if (memmap)\n\t\tfree_map_bootmem(memmap);\n\n\tif (empty)\n\t\tms->section_mem_map = (unsigned long)NULL;\n}\n\nstatic struct page * __meminit section_activate(int nid, unsigned long pfn,\n\t\tunsigned long nr_pages, struct vmem_altmap *altmap,\n\t\tstruct dev_pagemap *pgmap)\n{\n\tstruct mem_section *ms = __pfn_to_section(pfn);\n\tstruct mem_section_usage *usage = NULL;\n\tstruct page *memmap;\n\tint rc;\n\n\tif (!ms->usage) {\n\t\tusage = kzalloc(mem_section_usage_size(), GFP_KERNEL);\n\t\tif (!usage)\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\tms->usage = usage;\n\t}\n\n\trc = fill_subsection_map(pfn, nr_pages);\n\tif (rc) {\n\t\tif (usage)\n\t\t\tms->usage = NULL;\n\t\tkfree(usage);\n\t\treturn ERR_PTR(rc);\n\t}\n\n\t \n\tif (nr_pages < PAGES_PER_SECTION && early_section(ms))\n\t\treturn pfn_to_page(pfn);\n\n\tmemmap = populate_section_memmap(pfn, nr_pages, nid, altmap, pgmap);\n\tif (!memmap) {\n\t\tsection_deactivate(pfn, nr_pages, altmap);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\treturn memmap;\n}\n\n \nint __meminit sparse_add_section(int nid, unsigned long start_pfn,\n\t\tunsigned long nr_pages, struct vmem_altmap *altmap,\n\t\tstruct dev_pagemap *pgmap)\n{\n\tunsigned long section_nr = pfn_to_section_nr(start_pfn);\n\tstruct mem_section *ms;\n\tstruct page *memmap;\n\tint ret;\n\n\tret = sparse_index_init(section_nr, nid);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tmemmap = section_activate(nid, start_pfn, nr_pages, altmap, pgmap);\n\tif (IS_ERR(memmap))\n\t\treturn PTR_ERR(memmap);\n\n\t \n\tpage_init_poison(memmap, sizeof(struct page) * nr_pages);\n\n\tms = __nr_to_section(section_nr);\n\tset_section_nid(section_nr, nid);\n\t__section_mark_present(ms, section_nr);\n\n\t \n\tif (section_nr_to_pfn(section_nr) != start_pfn)\n\t\tmemmap = pfn_to_page(section_nr_to_pfn(section_nr));\n\tsparse_init_one_section(ms, section_nr, memmap, ms->usage, 0);\n\n\treturn 0;\n}\n\nvoid sparse_remove_section(unsigned long pfn, unsigned long nr_pages,\n\t\t\t   struct vmem_altmap *altmap)\n{\n\tstruct mem_section *ms = __pfn_to_section(pfn);\n\n\tif (WARN_ON_ONCE(!valid_section(ms)))\n\t\treturn;\n\n\tsection_deactivate(pfn, nr_pages, altmap);\n}\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}