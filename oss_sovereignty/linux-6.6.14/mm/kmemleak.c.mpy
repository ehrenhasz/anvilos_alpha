{
  "module_name": "kmemleak.c",
  "hash_id": "6019db8d0c0ee10ee69fd8cae26799aa1d9794672920c2c373c61dc543929386",
  "original_prompt": "Ingested from linux-6.6.14/mm/kmemleak.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/list.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/task.h>\n#include <linux/sched/task_stack.h>\n#include <linux/jiffies.h>\n#include <linux/delay.h>\n#include <linux/export.h>\n#include <linux/kthread.h>\n#include <linux/rbtree.h>\n#include <linux/fs.h>\n#include <linux/debugfs.h>\n#include <linux/seq_file.h>\n#include <linux/cpumask.h>\n#include <linux/spinlock.h>\n#include <linux/module.h>\n#include <linux/mutex.h>\n#include <linux/rcupdate.h>\n#include <linux/stacktrace.h>\n#include <linux/stackdepot.h>\n#include <linux/cache.h>\n#include <linux/percpu.h>\n#include <linux/memblock.h>\n#include <linux/pfn.h>\n#include <linux/mmzone.h>\n#include <linux/slab.h>\n#include <linux/thread_info.h>\n#include <linux/err.h>\n#include <linux/uaccess.h>\n#include <linux/string.h>\n#include <linux/nodemask.h>\n#include <linux/mm.h>\n#include <linux/workqueue.h>\n#include <linux/crc32.h>\n\n#include <asm/sections.h>\n#include <asm/processor.h>\n#include <linux/atomic.h>\n\n#include <linux/kasan.h>\n#include <linux/kfence.h>\n#include <linux/kmemleak.h>\n#include <linux/memory_hotplug.h>\n\n \n#define MAX_TRACE\t\t16\t \n#define MSECS_MIN_AGE\t\t5000\t \n#define SECS_FIRST_SCAN\t\t60\t \n#define SECS_SCAN_WAIT\t\t600\t \n#define MAX_SCAN_SIZE\t\t4096\t \n\n#define BYTES_PER_POINTER\tsizeof(void *)\n\n \n#define gfp_kmemleak_mask(gfp)\t(((gfp) & (GFP_KERNEL | GFP_ATOMIC | \\\n\t\t\t\t\t   __GFP_NOLOCKDEP)) | \\\n\t\t\t\t __GFP_NORETRY | __GFP_NOMEMALLOC | \\\n\t\t\t\t __GFP_NOWARN)\n\n \nstruct kmemleak_scan_area {\n\tstruct hlist_node node;\n\tunsigned long start;\n\tsize_t size;\n};\n\n#define KMEMLEAK_GREY\t0\n#define KMEMLEAK_BLACK\t-1\n\n \nstruct kmemleak_object {\n\traw_spinlock_t lock;\n\tunsigned int flags;\t\t \n\tstruct list_head object_list;\n\tstruct list_head gray_list;\n\tstruct rb_node rb_node;\n\tstruct rcu_head rcu;\t\t \n\t \n\tatomic_t use_count;\n\tunsigned int del_state;\t\t \n\tunsigned long pointer;\n\tsize_t size;\n\t \n\tunsigned long excess_ref;\n\t \n\tint min_count;\n\t \n\tint count;\n\t \n\tu32 checksum;\n\t \n\tstruct hlist_head area_list;\n\tdepot_stack_handle_t trace_handle;\n\tunsigned long jiffies;\t\t \n\tpid_t pid;\t\t\t \n\tchar comm[TASK_COMM_LEN];\t \n};\n\n \n#define OBJECT_ALLOCATED\t(1 << 0)\n \n#define OBJECT_REPORTED\t\t(1 << 1)\n \n#define OBJECT_NO_SCAN\t\t(1 << 2)\n \n#define OBJECT_FULL_SCAN\t(1 << 3)\n \n#define OBJECT_PHYS\t\t(1 << 4)\n\n \n#define DELSTATE_REMOVED\t(1 << 0)\n \n#define DELSTATE_NO_DELETE\t(1 << 1)\n\n#define HEX_PREFIX\t\t\"    \"\n \n#define HEX_ROW_SIZE\t\t16\n \n#define HEX_GROUP_SIZE\t\t1\n \n#define HEX_ASCII\t\t1\n \n#define HEX_MAX_LINES\t\t2\n\n \nstatic LIST_HEAD(object_list);\n \nstatic LIST_HEAD(gray_list);\n \nstatic struct kmemleak_object mem_pool[CONFIG_DEBUG_KMEMLEAK_MEM_POOL_SIZE];\nstatic int mem_pool_free_count = ARRAY_SIZE(mem_pool);\nstatic LIST_HEAD(mem_pool_free_list);\n \nstatic struct rb_root object_tree_root = RB_ROOT;\n \nstatic struct rb_root object_phys_tree_root = RB_ROOT;\n \nstatic DEFINE_RAW_SPINLOCK(kmemleak_lock);\n\n \nstatic struct kmem_cache *object_cache;\nstatic struct kmem_cache *scan_area_cache;\n\n \nstatic int kmemleak_enabled = 1;\n \nstatic int kmemleak_free_enabled = 1;\n \nstatic int kmemleak_late_initialized;\n \nstatic int kmemleak_warning;\n \nstatic int kmemleak_error;\n\n \nstatic unsigned long min_addr = ULONG_MAX;\nstatic unsigned long max_addr;\n\nstatic struct task_struct *scan_thread;\n \nstatic unsigned long jiffies_min_age;\nstatic unsigned long jiffies_last_scan;\n \nstatic unsigned long jiffies_scan_wait;\n \nstatic int kmemleak_stack_scan = 1;\n \nstatic DEFINE_MUTEX(scan_mutex);\n \nstatic int kmemleak_skip_disable;\n \nstatic bool kmemleak_found_leaks;\n\nstatic bool kmemleak_verbose;\nmodule_param_named(verbose, kmemleak_verbose, bool, 0600);\n\nstatic void kmemleak_disable(void);\n\n \n#define kmemleak_warn(x...)\tdo {\t\t\\\n\tpr_warn(x);\t\t\t\t\\\n\tdump_stack();\t\t\t\t\\\n\tkmemleak_warning = 1;\t\t\t\\\n} while (0)\n\n \n#define kmemleak_stop(x...)\tdo {\t\\\n\tkmemleak_warn(x);\t\t\\\n\tkmemleak_disable();\t\t\\\n} while (0)\n\n#define warn_or_seq_printf(seq, fmt, ...)\tdo {\t\\\n\tif (seq)\t\t\t\t\t\\\n\t\tseq_printf(seq, fmt, ##__VA_ARGS__);\t\\\n\telse\t\t\t\t\t\t\\\n\t\tpr_warn(fmt, ##__VA_ARGS__);\t\t\\\n} while (0)\n\nstatic void warn_or_seq_hex_dump(struct seq_file *seq, int prefix_type,\n\t\t\t\t int rowsize, int groupsize, const void *buf,\n\t\t\t\t size_t len, bool ascii)\n{\n\tif (seq)\n\t\tseq_hex_dump(seq, HEX_PREFIX, prefix_type, rowsize, groupsize,\n\t\t\t     buf, len, ascii);\n\telse\n\t\tprint_hex_dump(KERN_WARNING, pr_fmt(HEX_PREFIX), prefix_type,\n\t\t\t       rowsize, groupsize, buf, len, ascii);\n}\n\n \nstatic void hex_dump_object(struct seq_file *seq,\n\t\t\t    struct kmemleak_object *object)\n{\n\tconst u8 *ptr = (const u8 *)object->pointer;\n\tsize_t len;\n\n\tif (WARN_ON_ONCE(object->flags & OBJECT_PHYS))\n\t\treturn;\n\n\t \n\tlen = min_t(size_t, object->size, HEX_MAX_LINES * HEX_ROW_SIZE);\n\n\twarn_or_seq_printf(seq, \"  hex dump (first %zu bytes):\\n\", len);\n\tkasan_disable_current();\n\twarn_or_seq_hex_dump(seq, DUMP_PREFIX_NONE, HEX_ROW_SIZE,\n\t\t\t     HEX_GROUP_SIZE, kasan_reset_tag((void *)ptr), len, HEX_ASCII);\n\tkasan_enable_current();\n}\n\n \nstatic bool color_white(const struct kmemleak_object *object)\n{\n\treturn object->count != KMEMLEAK_BLACK &&\n\t\tobject->count < object->min_count;\n}\n\nstatic bool color_gray(const struct kmemleak_object *object)\n{\n\treturn object->min_count != KMEMLEAK_BLACK &&\n\t\tobject->count >= object->min_count;\n}\n\n \nstatic bool unreferenced_object(struct kmemleak_object *object)\n{\n\treturn (color_white(object) && object->flags & OBJECT_ALLOCATED) &&\n\t\ttime_before_eq(object->jiffies + jiffies_min_age,\n\t\t\t       jiffies_last_scan);\n}\n\n \nstatic void print_unreferenced(struct seq_file *seq,\n\t\t\t       struct kmemleak_object *object)\n{\n\tint i;\n\tunsigned long *entries;\n\tunsigned int nr_entries;\n\tunsigned int msecs_age = jiffies_to_msecs(jiffies - object->jiffies);\n\n\tnr_entries = stack_depot_fetch(object->trace_handle, &entries);\n\twarn_or_seq_printf(seq, \"unreferenced object 0x%08lx (size %zu):\\n\",\n\t\t\t  object->pointer, object->size);\n\twarn_or_seq_printf(seq, \"  comm \\\"%s\\\", pid %d, jiffies %lu (age %d.%03ds)\\n\",\n\t\t\t   object->comm, object->pid, object->jiffies,\n\t\t\t   msecs_age / 1000, msecs_age % 1000);\n\thex_dump_object(seq, object);\n\twarn_or_seq_printf(seq, \"  backtrace:\\n\");\n\n\tfor (i = 0; i < nr_entries; i++) {\n\t\tvoid *ptr = (void *)entries[i];\n\t\twarn_or_seq_printf(seq, \"    [<%pK>] %pS\\n\", ptr, ptr);\n\t}\n}\n\n \nstatic void dump_object_info(struct kmemleak_object *object)\n{\n\tpr_notice(\"Object 0x%08lx (size %zu):\\n\",\n\t\t\tobject->pointer, object->size);\n\tpr_notice(\"  comm \\\"%s\\\", pid %d, jiffies %lu\\n\",\n\t\t\tobject->comm, object->pid, object->jiffies);\n\tpr_notice(\"  min_count = %d\\n\", object->min_count);\n\tpr_notice(\"  count = %d\\n\", object->count);\n\tpr_notice(\"  flags = 0x%x\\n\", object->flags);\n\tpr_notice(\"  checksum = %u\\n\", object->checksum);\n\tpr_notice(\"  backtrace:\\n\");\n\tif (object->trace_handle)\n\t\tstack_depot_print(object->trace_handle);\n}\n\n \nstatic struct kmemleak_object *__lookup_object(unsigned long ptr, int alias,\n\t\t\t\t\t       bool is_phys)\n{\n\tstruct rb_node *rb = is_phys ? object_phys_tree_root.rb_node :\n\t\t\t     object_tree_root.rb_node;\n\tunsigned long untagged_ptr = (unsigned long)kasan_reset_tag((void *)ptr);\n\n\twhile (rb) {\n\t\tstruct kmemleak_object *object;\n\t\tunsigned long untagged_objp;\n\n\t\tobject = rb_entry(rb, struct kmemleak_object, rb_node);\n\t\tuntagged_objp = (unsigned long)kasan_reset_tag((void *)object->pointer);\n\n\t\tif (untagged_ptr < untagged_objp)\n\t\t\trb = object->rb_node.rb_left;\n\t\telse if (untagged_objp + object->size <= untagged_ptr)\n\t\t\trb = object->rb_node.rb_right;\n\t\telse if (untagged_objp == untagged_ptr || alias)\n\t\t\treturn object;\n\t\telse {\n\t\t\tkmemleak_warn(\"Found object by alias at 0x%08lx\\n\",\n\t\t\t\t      ptr);\n\t\t\tdump_object_info(object);\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn NULL;\n}\n\n \nstatic struct kmemleak_object *lookup_object(unsigned long ptr, int alias)\n{\n\treturn __lookup_object(ptr, alias, false);\n}\n\n \nstatic int get_object(struct kmemleak_object *object)\n{\n\treturn atomic_inc_not_zero(&object->use_count);\n}\n\n \nstatic struct kmemleak_object *mem_pool_alloc(gfp_t gfp)\n{\n\tunsigned long flags;\n\tstruct kmemleak_object *object;\n\n\t \n\tif (object_cache) {\n\t\tobject = kmem_cache_alloc(object_cache, gfp_kmemleak_mask(gfp));\n\t\tif (object)\n\t\t\treturn object;\n\t}\n\n\t \n\traw_spin_lock_irqsave(&kmemleak_lock, flags);\n\tobject = list_first_entry_or_null(&mem_pool_free_list,\n\t\t\t\t\t  typeof(*object), object_list);\n\tif (object)\n\t\tlist_del(&object->object_list);\n\telse if (mem_pool_free_count)\n\t\tobject = &mem_pool[--mem_pool_free_count];\n\telse\n\t\tpr_warn_once(\"Memory pool empty, consider increasing CONFIG_DEBUG_KMEMLEAK_MEM_POOL_SIZE\\n\");\n\traw_spin_unlock_irqrestore(&kmemleak_lock, flags);\n\n\treturn object;\n}\n\n \nstatic void mem_pool_free(struct kmemleak_object *object)\n{\n\tunsigned long flags;\n\n\tif (object < mem_pool || object >= mem_pool + ARRAY_SIZE(mem_pool)) {\n\t\tkmem_cache_free(object_cache, object);\n\t\treturn;\n\t}\n\n\t \n\traw_spin_lock_irqsave(&kmemleak_lock, flags);\n\tlist_add(&object->object_list, &mem_pool_free_list);\n\traw_spin_unlock_irqrestore(&kmemleak_lock, flags);\n}\n\n \nstatic void free_object_rcu(struct rcu_head *rcu)\n{\n\tstruct hlist_node *tmp;\n\tstruct kmemleak_scan_area *area;\n\tstruct kmemleak_object *object =\n\t\tcontainer_of(rcu, struct kmemleak_object, rcu);\n\n\t \n\thlist_for_each_entry_safe(area, tmp, &object->area_list, node) {\n\t\thlist_del(&area->node);\n\t\tkmem_cache_free(scan_area_cache, area);\n\t}\n\tmem_pool_free(object);\n}\n\n \nstatic void put_object(struct kmemleak_object *object)\n{\n\tif (!atomic_dec_and_test(&object->use_count))\n\t\treturn;\n\n\t \n\tWARN_ON(object->flags & OBJECT_ALLOCATED);\n\n\t \n\tif (object_cache)\n\t\tcall_rcu(&object->rcu, free_object_rcu);\n\telse\n\t\tfree_object_rcu(&object->rcu);\n}\n\n \nstatic struct kmemleak_object *__find_and_get_object(unsigned long ptr, int alias,\n\t\t\t\t\t\t     bool is_phys)\n{\n\tunsigned long flags;\n\tstruct kmemleak_object *object;\n\n\trcu_read_lock();\n\traw_spin_lock_irqsave(&kmemleak_lock, flags);\n\tobject = __lookup_object(ptr, alias, is_phys);\n\traw_spin_unlock_irqrestore(&kmemleak_lock, flags);\n\n\t \n\tif (object && !get_object(object))\n\t\tobject = NULL;\n\trcu_read_unlock();\n\n\treturn object;\n}\n\n \nstatic struct kmemleak_object *find_and_get_object(unsigned long ptr, int alias)\n{\n\treturn __find_and_get_object(ptr, alias, false);\n}\n\n \nstatic void __remove_object(struct kmemleak_object *object)\n{\n\trb_erase(&object->rb_node, object->flags & OBJECT_PHYS ?\n\t\t\t\t   &object_phys_tree_root :\n\t\t\t\t   &object_tree_root);\n\tif (!(object->del_state & DELSTATE_NO_DELETE))\n\t\tlist_del_rcu(&object->object_list);\n\tobject->del_state |= DELSTATE_REMOVED;\n}\n\n \nstatic struct kmemleak_object *find_and_remove_object(unsigned long ptr, int alias,\n\t\t\t\t\t\t      bool is_phys)\n{\n\tunsigned long flags;\n\tstruct kmemleak_object *object;\n\n\traw_spin_lock_irqsave(&kmemleak_lock, flags);\n\tobject = __lookup_object(ptr, alias, is_phys);\n\tif (object)\n\t\t__remove_object(object);\n\traw_spin_unlock_irqrestore(&kmemleak_lock, flags);\n\n\treturn object;\n}\n\nstatic noinline depot_stack_handle_t set_track_prepare(void)\n{\n\tdepot_stack_handle_t trace_handle;\n\tunsigned long entries[MAX_TRACE];\n\tunsigned int nr_entries;\n\n\t \n\tif (!object_cache)\n\t\treturn 0;\n\tnr_entries = stack_trace_save(entries, ARRAY_SIZE(entries), 3);\n\ttrace_handle = stack_depot_save(entries, nr_entries, GFP_NOWAIT);\n\n\treturn trace_handle;\n}\n\n \nstatic void __create_object(unsigned long ptr, size_t size,\n\t\t\t    int min_count, gfp_t gfp, bool is_phys)\n{\n\tunsigned long flags;\n\tstruct kmemleak_object *object, *parent;\n\tstruct rb_node **link, *rb_parent;\n\tunsigned long untagged_ptr;\n\tunsigned long untagged_objp;\n\n\tobject = mem_pool_alloc(gfp);\n\tif (!object) {\n\t\tpr_warn(\"Cannot allocate a kmemleak_object structure\\n\");\n\t\tkmemleak_disable();\n\t\treturn;\n\t}\n\n\tINIT_LIST_HEAD(&object->object_list);\n\tINIT_LIST_HEAD(&object->gray_list);\n\tINIT_HLIST_HEAD(&object->area_list);\n\traw_spin_lock_init(&object->lock);\n\tatomic_set(&object->use_count, 1);\n\tobject->flags = OBJECT_ALLOCATED | (is_phys ? OBJECT_PHYS : 0);\n\tobject->pointer = ptr;\n\tobject->size = kfence_ksize((void *)ptr) ?: size;\n\tobject->excess_ref = 0;\n\tobject->min_count = min_count;\n\tobject->count = 0;\t\t\t \n\tobject->jiffies = jiffies;\n\tobject->checksum = 0;\n\tobject->del_state = 0;\n\n\t \n\tif (in_hardirq()) {\n\t\tobject->pid = 0;\n\t\tstrncpy(object->comm, \"hardirq\", sizeof(object->comm));\n\t} else if (in_serving_softirq()) {\n\t\tobject->pid = 0;\n\t\tstrncpy(object->comm, \"softirq\", sizeof(object->comm));\n\t} else {\n\t\tobject->pid = current->pid;\n\t\t \n\t\tstrncpy(object->comm, current->comm, sizeof(object->comm));\n\t}\n\n\t \n\tobject->trace_handle = set_track_prepare();\n\n\traw_spin_lock_irqsave(&kmemleak_lock, flags);\n\n\tuntagged_ptr = (unsigned long)kasan_reset_tag((void *)ptr);\n\t \n\tif (!is_phys) {\n\t\tmin_addr = min(min_addr, untagged_ptr);\n\t\tmax_addr = max(max_addr, untagged_ptr + size);\n\t}\n\tlink = is_phys ? &object_phys_tree_root.rb_node :\n\t\t&object_tree_root.rb_node;\n\trb_parent = NULL;\n\twhile (*link) {\n\t\trb_parent = *link;\n\t\tparent = rb_entry(rb_parent, struct kmemleak_object, rb_node);\n\t\tuntagged_objp = (unsigned long)kasan_reset_tag((void *)parent->pointer);\n\t\tif (untagged_ptr + size <= untagged_objp)\n\t\t\tlink = &parent->rb_node.rb_left;\n\t\telse if (untagged_objp + parent->size <= untagged_ptr)\n\t\t\tlink = &parent->rb_node.rb_right;\n\t\telse {\n\t\t\tkmemleak_stop(\"Cannot insert 0x%lx into the object search tree (overlaps existing)\\n\",\n\t\t\t\t      ptr);\n\t\t\t \n\t\t\tdump_object_info(parent);\n\t\t\tkmem_cache_free(object_cache, object);\n\t\t\tgoto out;\n\t\t}\n\t}\n\trb_link_node(&object->rb_node, rb_parent, link);\n\trb_insert_color(&object->rb_node, is_phys ? &object_phys_tree_root :\n\t\t\t\t\t  &object_tree_root);\n\tlist_add_tail_rcu(&object->object_list, &object_list);\nout:\n\traw_spin_unlock_irqrestore(&kmemleak_lock, flags);\n}\n\n \nstatic void create_object(unsigned long ptr, size_t size,\n\t\t\t  int min_count, gfp_t gfp)\n{\n\t__create_object(ptr, size, min_count, gfp, false);\n}\n\n \nstatic void create_object_phys(unsigned long ptr, size_t size,\n\t\t\t       int min_count, gfp_t gfp)\n{\n\t__create_object(ptr, size, min_count, gfp, true);\n}\n\n \nstatic void __delete_object(struct kmemleak_object *object)\n{\n\tunsigned long flags;\n\n\tWARN_ON(!(object->flags & OBJECT_ALLOCATED));\n\tWARN_ON(atomic_read(&object->use_count) < 1);\n\n\t \n\traw_spin_lock_irqsave(&object->lock, flags);\n\tobject->flags &= ~OBJECT_ALLOCATED;\n\traw_spin_unlock_irqrestore(&object->lock, flags);\n\tput_object(object);\n}\n\n \nstatic void delete_object_full(unsigned long ptr)\n{\n\tstruct kmemleak_object *object;\n\n\tobject = find_and_remove_object(ptr, 0, false);\n\tif (!object) {\n#ifdef DEBUG\n\t\tkmemleak_warn(\"Freeing unknown object at 0x%08lx\\n\",\n\t\t\t      ptr);\n#endif\n\t\treturn;\n\t}\n\t__delete_object(object);\n}\n\n \nstatic void delete_object_part(unsigned long ptr, size_t size, bool is_phys)\n{\n\tstruct kmemleak_object *object;\n\tunsigned long start, end;\n\n\tobject = find_and_remove_object(ptr, 1, is_phys);\n\tif (!object) {\n#ifdef DEBUG\n\t\tkmemleak_warn(\"Partially freeing unknown object at 0x%08lx (size %zu)\\n\",\n\t\t\t      ptr, size);\n#endif\n\t\treturn;\n\t}\n\n\t \n\tstart = object->pointer;\n\tend = object->pointer + object->size;\n\tif (ptr > start)\n\t\t__create_object(start, ptr - start, object->min_count,\n\t\t\t      GFP_KERNEL, is_phys);\n\tif (ptr + size < end)\n\t\t__create_object(ptr + size, end - ptr - size, object->min_count,\n\t\t\t      GFP_KERNEL, is_phys);\n\n\t__delete_object(object);\n}\n\nstatic void __paint_it(struct kmemleak_object *object, int color)\n{\n\tobject->min_count = color;\n\tif (color == KMEMLEAK_BLACK)\n\t\tobject->flags |= OBJECT_NO_SCAN;\n}\n\nstatic void paint_it(struct kmemleak_object *object, int color)\n{\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&object->lock, flags);\n\t__paint_it(object, color);\n\traw_spin_unlock_irqrestore(&object->lock, flags);\n}\n\nstatic void paint_ptr(unsigned long ptr, int color, bool is_phys)\n{\n\tstruct kmemleak_object *object;\n\n\tobject = __find_and_get_object(ptr, 0, is_phys);\n\tif (!object) {\n\t\tkmemleak_warn(\"Trying to color unknown object at 0x%08lx as %s\\n\",\n\t\t\t      ptr,\n\t\t\t      (color == KMEMLEAK_GREY) ? \"Grey\" :\n\t\t\t      (color == KMEMLEAK_BLACK) ? \"Black\" : \"Unknown\");\n\t\treturn;\n\t}\n\tpaint_it(object, color);\n\tput_object(object);\n}\n\n \nstatic void make_gray_object(unsigned long ptr)\n{\n\tpaint_ptr(ptr, KMEMLEAK_GREY, false);\n}\n\n \nstatic void make_black_object(unsigned long ptr, bool is_phys)\n{\n\tpaint_ptr(ptr, KMEMLEAK_BLACK, is_phys);\n}\n\n \nstatic void add_scan_area(unsigned long ptr, size_t size, gfp_t gfp)\n{\n\tunsigned long flags;\n\tstruct kmemleak_object *object;\n\tstruct kmemleak_scan_area *area = NULL;\n\tunsigned long untagged_ptr;\n\tunsigned long untagged_objp;\n\n\tobject = find_and_get_object(ptr, 1);\n\tif (!object) {\n\t\tkmemleak_warn(\"Adding scan area to unknown object at 0x%08lx\\n\",\n\t\t\t      ptr);\n\t\treturn;\n\t}\n\n\tuntagged_ptr = (unsigned long)kasan_reset_tag((void *)ptr);\n\tuntagged_objp = (unsigned long)kasan_reset_tag((void *)object->pointer);\n\n\tif (scan_area_cache)\n\t\tarea = kmem_cache_alloc(scan_area_cache, gfp_kmemleak_mask(gfp));\n\n\traw_spin_lock_irqsave(&object->lock, flags);\n\tif (!area) {\n\t\tpr_warn_once(\"Cannot allocate a scan area, scanning the full object\\n\");\n\t\t \n\t\tobject->flags |= OBJECT_FULL_SCAN;\n\t\tgoto out_unlock;\n\t}\n\tif (size == SIZE_MAX) {\n\t\tsize = untagged_objp + object->size - untagged_ptr;\n\t} else if (untagged_ptr + size > untagged_objp + object->size) {\n\t\tkmemleak_warn(\"Scan area larger than object 0x%08lx\\n\", ptr);\n\t\tdump_object_info(object);\n\t\tkmem_cache_free(scan_area_cache, area);\n\t\tgoto out_unlock;\n\t}\n\n\tINIT_HLIST_NODE(&area->node);\n\tarea->start = ptr;\n\tarea->size = size;\n\n\thlist_add_head(&area->node, &object->area_list);\nout_unlock:\n\traw_spin_unlock_irqrestore(&object->lock, flags);\n\tput_object(object);\n}\n\n \nstatic void object_set_excess_ref(unsigned long ptr, unsigned long excess_ref)\n{\n\tunsigned long flags;\n\tstruct kmemleak_object *object;\n\n\tobject = find_and_get_object(ptr, 0);\n\tif (!object) {\n\t\tkmemleak_warn(\"Setting excess_ref on unknown object at 0x%08lx\\n\",\n\t\t\t      ptr);\n\t\treturn;\n\t}\n\n\traw_spin_lock_irqsave(&object->lock, flags);\n\tobject->excess_ref = excess_ref;\n\traw_spin_unlock_irqrestore(&object->lock, flags);\n\tput_object(object);\n}\n\n \nstatic void object_no_scan(unsigned long ptr)\n{\n\tunsigned long flags;\n\tstruct kmemleak_object *object;\n\n\tobject = find_and_get_object(ptr, 0);\n\tif (!object) {\n\t\tkmemleak_warn(\"Not scanning unknown object at 0x%08lx\\n\", ptr);\n\t\treturn;\n\t}\n\n\traw_spin_lock_irqsave(&object->lock, flags);\n\tobject->flags |= OBJECT_NO_SCAN;\n\traw_spin_unlock_irqrestore(&object->lock, flags);\n\tput_object(object);\n}\n\n \nvoid __ref kmemleak_alloc(const void *ptr, size_t size, int min_count,\n\t\t\t  gfp_t gfp)\n{\n\tpr_debug(\"%s(0x%p, %zu, %d)\\n\", __func__, ptr, size, min_count);\n\n\tif (kmemleak_enabled && ptr && !IS_ERR(ptr))\n\t\tcreate_object((unsigned long)ptr, size, min_count, gfp);\n}\nEXPORT_SYMBOL_GPL(kmemleak_alloc);\n\n \nvoid __ref kmemleak_alloc_percpu(const void __percpu *ptr, size_t size,\n\t\t\t\t gfp_t gfp)\n{\n\tunsigned int cpu;\n\n\tpr_debug(\"%s(0x%p, %zu)\\n\", __func__, ptr, size);\n\n\t \n\tif (kmemleak_enabled && ptr && !IS_ERR(ptr))\n\t\tfor_each_possible_cpu(cpu)\n\t\t\tcreate_object((unsigned long)per_cpu_ptr(ptr, cpu),\n\t\t\t\t      size, 0, gfp);\n}\nEXPORT_SYMBOL_GPL(kmemleak_alloc_percpu);\n\n \nvoid __ref kmemleak_vmalloc(const struct vm_struct *area, size_t size, gfp_t gfp)\n{\n\tpr_debug(\"%s(0x%p, %zu)\\n\", __func__, area, size);\n\n\t \n\tif (kmemleak_enabled) {\n\t\tcreate_object((unsigned long)area->addr, size, 2, gfp);\n\t\tobject_set_excess_ref((unsigned long)area,\n\t\t\t\t      (unsigned long)area->addr);\n\t}\n}\nEXPORT_SYMBOL_GPL(kmemleak_vmalloc);\n\n \nvoid __ref kmemleak_free(const void *ptr)\n{\n\tpr_debug(\"%s(0x%p)\\n\", __func__, ptr);\n\n\tif (kmemleak_free_enabled && ptr && !IS_ERR(ptr))\n\t\tdelete_object_full((unsigned long)ptr);\n}\nEXPORT_SYMBOL_GPL(kmemleak_free);\n\n \nvoid __ref kmemleak_free_part(const void *ptr, size_t size)\n{\n\tpr_debug(\"%s(0x%p)\\n\", __func__, ptr);\n\n\tif (kmemleak_enabled && ptr && !IS_ERR(ptr))\n\t\tdelete_object_part((unsigned long)ptr, size, false);\n}\nEXPORT_SYMBOL_GPL(kmemleak_free_part);\n\n \nvoid __ref kmemleak_free_percpu(const void __percpu *ptr)\n{\n\tunsigned int cpu;\n\n\tpr_debug(\"%s(0x%p)\\n\", __func__, ptr);\n\n\tif (kmemleak_free_enabled && ptr && !IS_ERR(ptr))\n\t\tfor_each_possible_cpu(cpu)\n\t\t\tdelete_object_full((unsigned long)per_cpu_ptr(ptr,\n\t\t\t\t\t\t\t\t      cpu));\n}\nEXPORT_SYMBOL_GPL(kmemleak_free_percpu);\n\n \nvoid __ref kmemleak_update_trace(const void *ptr)\n{\n\tstruct kmemleak_object *object;\n\tunsigned long flags;\n\n\tpr_debug(\"%s(0x%p)\\n\", __func__, ptr);\n\n\tif (!kmemleak_enabled || IS_ERR_OR_NULL(ptr))\n\t\treturn;\n\n\tobject = find_and_get_object((unsigned long)ptr, 1);\n\tif (!object) {\n#ifdef DEBUG\n\t\tkmemleak_warn(\"Updating stack trace for unknown object at %p\\n\",\n\t\t\t      ptr);\n#endif\n\t\treturn;\n\t}\n\n\traw_spin_lock_irqsave(&object->lock, flags);\n\tobject->trace_handle = set_track_prepare();\n\traw_spin_unlock_irqrestore(&object->lock, flags);\n\n\tput_object(object);\n}\nEXPORT_SYMBOL(kmemleak_update_trace);\n\n \nvoid __ref kmemleak_not_leak(const void *ptr)\n{\n\tpr_debug(\"%s(0x%p)\\n\", __func__, ptr);\n\n\tif (kmemleak_enabled && ptr && !IS_ERR(ptr))\n\t\tmake_gray_object((unsigned long)ptr);\n}\nEXPORT_SYMBOL(kmemleak_not_leak);\n\n \nvoid __ref kmemleak_ignore(const void *ptr)\n{\n\tpr_debug(\"%s(0x%p)\\n\", __func__, ptr);\n\n\tif (kmemleak_enabled && ptr && !IS_ERR(ptr))\n\t\tmake_black_object((unsigned long)ptr, false);\n}\nEXPORT_SYMBOL(kmemleak_ignore);\n\n \nvoid __ref kmemleak_scan_area(const void *ptr, size_t size, gfp_t gfp)\n{\n\tpr_debug(\"%s(0x%p)\\n\", __func__, ptr);\n\n\tif (kmemleak_enabled && ptr && size && !IS_ERR(ptr))\n\t\tadd_scan_area((unsigned long)ptr, size, gfp);\n}\nEXPORT_SYMBOL(kmemleak_scan_area);\n\n \nvoid __ref kmemleak_no_scan(const void *ptr)\n{\n\tpr_debug(\"%s(0x%p)\\n\", __func__, ptr);\n\n\tif (kmemleak_enabled && ptr && !IS_ERR(ptr))\n\t\tobject_no_scan((unsigned long)ptr);\n}\nEXPORT_SYMBOL(kmemleak_no_scan);\n\n \nvoid __ref kmemleak_alloc_phys(phys_addr_t phys, size_t size, gfp_t gfp)\n{\n\tpr_debug(\"%s(0x%pa, %zu)\\n\", __func__, &phys, size);\n\n\tif (kmemleak_enabled)\n\t\t \n\t\tcreate_object_phys((unsigned long)phys, size, 0, gfp);\n}\nEXPORT_SYMBOL(kmemleak_alloc_phys);\n\n \nvoid __ref kmemleak_free_part_phys(phys_addr_t phys, size_t size)\n{\n\tpr_debug(\"%s(0x%pa)\\n\", __func__, &phys);\n\n\tif (kmemleak_enabled)\n\t\tdelete_object_part((unsigned long)phys, size, true);\n}\nEXPORT_SYMBOL(kmemleak_free_part_phys);\n\n \nvoid __ref kmemleak_ignore_phys(phys_addr_t phys)\n{\n\tpr_debug(\"%s(0x%pa)\\n\", __func__, &phys);\n\n\tif (kmemleak_enabled)\n\t\tmake_black_object((unsigned long)phys, true);\n}\nEXPORT_SYMBOL(kmemleak_ignore_phys);\n\n \nstatic bool update_checksum(struct kmemleak_object *object)\n{\n\tu32 old_csum = object->checksum;\n\n\tif (WARN_ON_ONCE(object->flags & OBJECT_PHYS))\n\t\treturn false;\n\n\tkasan_disable_current();\n\tkcsan_disable_current();\n\tobject->checksum = crc32(0, kasan_reset_tag((void *)object->pointer), object->size);\n\tkasan_enable_current();\n\tkcsan_enable_current();\n\n\treturn object->checksum != old_csum;\n}\n\n \nstatic void update_refs(struct kmemleak_object *object)\n{\n\tif (!color_white(object)) {\n\t\t \n\t\treturn;\n\t}\n\n\t \n\tobject->count++;\n\tif (color_gray(object)) {\n\t\t \n\t\tWARN_ON(!get_object(object));\n\t\tlist_add_tail(&object->gray_list, &gray_list);\n\t}\n}\n\n \nstatic int scan_should_stop(void)\n{\n\tif (!kmemleak_enabled)\n\t\treturn 1;\n\n\t \n\tif (current->mm)\n\t\treturn signal_pending(current);\n\telse\n\t\treturn kthread_should_stop();\n\n\treturn 0;\n}\n\n \nstatic void scan_block(void *_start, void *_end,\n\t\t       struct kmemleak_object *scanned)\n{\n\tunsigned long *ptr;\n\tunsigned long *start = PTR_ALIGN(_start, BYTES_PER_POINTER);\n\tunsigned long *end = _end - (BYTES_PER_POINTER - 1);\n\tunsigned long flags;\n\tunsigned long untagged_ptr;\n\n\traw_spin_lock_irqsave(&kmemleak_lock, flags);\n\tfor (ptr = start; ptr < end; ptr++) {\n\t\tstruct kmemleak_object *object;\n\t\tunsigned long pointer;\n\t\tunsigned long excess_ref;\n\n\t\tif (scan_should_stop())\n\t\t\tbreak;\n\n\t\tkasan_disable_current();\n\t\tpointer = *(unsigned long *)kasan_reset_tag((void *)ptr);\n\t\tkasan_enable_current();\n\n\t\tuntagged_ptr = (unsigned long)kasan_reset_tag((void *)pointer);\n\t\tif (untagged_ptr < min_addr || untagged_ptr >= max_addr)\n\t\t\tcontinue;\n\n\t\t \n\t\tobject = lookup_object(pointer, 1);\n\t\tif (!object)\n\t\t\tcontinue;\n\t\tif (object == scanned)\n\t\t\t \n\t\t\tcontinue;\n\n\t\t \n\t\traw_spin_lock_nested(&object->lock, SINGLE_DEPTH_NESTING);\n\t\t \n\t\tif (color_gray(object)) {\n\t\t\texcess_ref = object->excess_ref;\n\t\t\t \n\t\t} else {\n\t\t\texcess_ref = 0;\n\t\t\tupdate_refs(object);\n\t\t}\n\t\traw_spin_unlock(&object->lock);\n\n\t\tif (excess_ref) {\n\t\t\tobject = lookup_object(excess_ref, 0);\n\t\t\tif (!object)\n\t\t\t\tcontinue;\n\t\t\tif (object == scanned)\n\t\t\t\t \n\t\t\t\tcontinue;\n\t\t\traw_spin_lock_nested(&object->lock, SINGLE_DEPTH_NESTING);\n\t\t\tupdate_refs(object);\n\t\t\traw_spin_unlock(&object->lock);\n\t\t}\n\t}\n\traw_spin_unlock_irqrestore(&kmemleak_lock, flags);\n}\n\n \n#ifdef CONFIG_SMP\nstatic void scan_large_block(void *start, void *end)\n{\n\tvoid *next;\n\n\twhile (start < end) {\n\t\tnext = min(start + MAX_SCAN_SIZE, end);\n\t\tscan_block(start, next, NULL);\n\t\tstart = next;\n\t\tcond_resched();\n\t}\n}\n#endif\n\n \nstatic void scan_object(struct kmemleak_object *object)\n{\n\tstruct kmemleak_scan_area *area;\n\tunsigned long flags;\n\tvoid *obj_ptr;\n\n\t \n\traw_spin_lock_irqsave(&object->lock, flags);\n\tif (object->flags & OBJECT_NO_SCAN)\n\t\tgoto out;\n\tif (!(object->flags & OBJECT_ALLOCATED))\n\t\t \n\t\tgoto out;\n\n\tobj_ptr = object->flags & OBJECT_PHYS ?\n\t\t  __va((phys_addr_t)object->pointer) :\n\t\t  (void *)object->pointer;\n\n\tif (hlist_empty(&object->area_list) ||\n\t    object->flags & OBJECT_FULL_SCAN) {\n\t\tvoid *start = obj_ptr;\n\t\tvoid *end = obj_ptr + object->size;\n\t\tvoid *next;\n\n\t\tdo {\n\t\t\tnext = min(start + MAX_SCAN_SIZE, end);\n\t\t\tscan_block(start, next, object);\n\n\t\t\tstart = next;\n\t\t\tif (start >= end)\n\t\t\t\tbreak;\n\n\t\t\traw_spin_unlock_irqrestore(&object->lock, flags);\n\t\t\tcond_resched();\n\t\t\traw_spin_lock_irqsave(&object->lock, flags);\n\t\t} while (object->flags & OBJECT_ALLOCATED);\n\t} else\n\t\thlist_for_each_entry(area, &object->area_list, node)\n\t\t\tscan_block((void *)area->start,\n\t\t\t\t   (void *)(area->start + area->size),\n\t\t\t\t   object);\nout:\n\traw_spin_unlock_irqrestore(&object->lock, flags);\n}\n\n \nstatic void scan_gray_list(void)\n{\n\tstruct kmemleak_object *object, *tmp;\n\n\t \n\tobject = list_entry(gray_list.next, typeof(*object), gray_list);\n\twhile (&object->gray_list != &gray_list) {\n\t\tcond_resched();\n\n\t\t \n\t\tif (!scan_should_stop())\n\t\t\tscan_object(object);\n\n\t\ttmp = list_entry(object->gray_list.next, typeof(*object),\n\t\t\t\t gray_list);\n\n\t\t \n\t\tlist_del(&object->gray_list);\n\t\tput_object(object);\n\n\t\tobject = tmp;\n\t}\n\tWARN_ON(!list_empty(&gray_list));\n}\n\n \nstatic void kmemleak_cond_resched(struct kmemleak_object *object)\n{\n\tif (!get_object(object))\n\t\treturn;\t \n\n\traw_spin_lock_irq(&kmemleak_lock);\n\tif (object->del_state & DELSTATE_REMOVED)\n\t\tgoto unlock_put;\t \n\tobject->del_state |= DELSTATE_NO_DELETE;\n\traw_spin_unlock_irq(&kmemleak_lock);\n\n\trcu_read_unlock();\n\tcond_resched();\n\trcu_read_lock();\n\n\traw_spin_lock_irq(&kmemleak_lock);\n\tif (object->del_state & DELSTATE_REMOVED)\n\t\tlist_del_rcu(&object->object_list);\n\tobject->del_state &= ~DELSTATE_NO_DELETE;\nunlock_put:\n\traw_spin_unlock_irq(&kmemleak_lock);\n\tput_object(object);\n}\n\n \nstatic void kmemleak_scan(void)\n{\n\tstruct kmemleak_object *object;\n\tstruct zone *zone;\n\tint __maybe_unused i;\n\tint new_leaks = 0;\n\n\tjiffies_last_scan = jiffies;\n\n\t \n\trcu_read_lock();\n\tlist_for_each_entry_rcu(object, &object_list, object_list) {\n\t\traw_spin_lock_irq(&object->lock);\n#ifdef DEBUG\n\t\t \n\t\tif (atomic_read(&object->use_count) > 1) {\n\t\t\tpr_debug(\"object->use_count = %d\\n\",\n\t\t\t\t atomic_read(&object->use_count));\n\t\t\tdump_object_info(object);\n\t\t}\n#endif\n\n\t\t \n\t\tif ((object->flags & OBJECT_PHYS) &&\n\t\t   !(object->flags & OBJECT_NO_SCAN)) {\n\t\t\tunsigned long phys = object->pointer;\n\n\t\t\tif (PHYS_PFN(phys) < min_low_pfn ||\n\t\t\t    PHYS_PFN(phys + object->size) >= max_low_pfn)\n\t\t\t\t__paint_it(object, KMEMLEAK_BLACK);\n\t\t}\n\n\t\t \n\t\tobject->count = 0;\n\t\tif (color_gray(object) && get_object(object))\n\t\t\tlist_add_tail(&object->gray_list, &gray_list);\n\n\t\traw_spin_unlock_irq(&object->lock);\n\n\t\tif (need_resched())\n\t\t\tkmemleak_cond_resched(object);\n\t}\n\trcu_read_unlock();\n\n#ifdef CONFIG_SMP\n\t \n\tfor_each_possible_cpu(i)\n\t\tscan_large_block(__per_cpu_start + per_cpu_offset(i),\n\t\t\t\t __per_cpu_end + per_cpu_offset(i));\n#endif\n\n\t \n\tget_online_mems();\n\tfor_each_populated_zone(zone) {\n\t\tunsigned long start_pfn = zone->zone_start_pfn;\n\t\tunsigned long end_pfn = zone_end_pfn(zone);\n\t\tunsigned long pfn;\n\n\t\tfor (pfn = start_pfn; pfn < end_pfn; pfn++) {\n\t\t\tstruct page *page = pfn_to_online_page(pfn);\n\n\t\t\tif (!(pfn & 63))\n\t\t\t\tcond_resched();\n\n\t\t\tif (!page)\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tif (page_zone(page) != zone)\n\t\t\t\tcontinue;\n\t\t\t \n\t\t\tif (page_count(page) == 0)\n\t\t\t\tcontinue;\n\t\t\tscan_block(page, page + 1, NULL);\n\t\t}\n\t}\n\tput_online_mems();\n\n\t \n\tif (kmemleak_stack_scan) {\n\t\tstruct task_struct *p, *g;\n\n\t\trcu_read_lock();\n\t\tfor_each_process_thread(g, p) {\n\t\t\tvoid *stack = try_get_task_stack(p);\n\t\t\tif (stack) {\n\t\t\t\tscan_block(stack, stack + THREAD_SIZE, NULL);\n\t\t\t\tput_task_stack(p);\n\t\t\t}\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\n\t \n\tscan_gray_list();\n\n\t \n\trcu_read_lock();\n\tlist_for_each_entry_rcu(object, &object_list, object_list) {\n\t\tif (need_resched())\n\t\t\tkmemleak_cond_resched(object);\n\n\t\t \n\t\tif (!color_white(object))\n\t\t\tcontinue;\n\t\traw_spin_lock_irq(&object->lock);\n\t\tif (color_white(object) && (object->flags & OBJECT_ALLOCATED)\n\t\t    && update_checksum(object) && get_object(object)) {\n\t\t\t \n\t\t\tobject->count = object->min_count;\n\t\t\tlist_add_tail(&object->gray_list, &gray_list);\n\t\t}\n\t\traw_spin_unlock_irq(&object->lock);\n\t}\n\trcu_read_unlock();\n\n\t \n\tscan_gray_list();\n\n\t \n\tif (scan_should_stop())\n\t\treturn;\n\n\t \n\trcu_read_lock();\n\tlist_for_each_entry_rcu(object, &object_list, object_list) {\n\t\tif (need_resched())\n\t\t\tkmemleak_cond_resched(object);\n\n\t\t \n\t\tif (!color_white(object))\n\t\t\tcontinue;\n\t\traw_spin_lock_irq(&object->lock);\n\t\tif (unreferenced_object(object) &&\n\t\t    !(object->flags & OBJECT_REPORTED)) {\n\t\t\tobject->flags |= OBJECT_REPORTED;\n\n\t\t\tif (kmemleak_verbose)\n\t\t\t\tprint_unreferenced(NULL, object);\n\n\t\t\tnew_leaks++;\n\t\t}\n\t\traw_spin_unlock_irq(&object->lock);\n\t}\n\trcu_read_unlock();\n\n\tif (new_leaks) {\n\t\tkmemleak_found_leaks = true;\n\n\t\tpr_info(\"%d new suspected memory leaks (see /sys/kernel/debug/kmemleak)\\n\",\n\t\t\tnew_leaks);\n\t}\n\n}\n\n \nstatic int kmemleak_scan_thread(void *arg)\n{\n\tstatic int first_run = IS_ENABLED(CONFIG_DEBUG_KMEMLEAK_AUTO_SCAN);\n\n\tpr_info(\"Automatic memory scanning thread started\\n\");\n\tset_user_nice(current, 10);\n\n\t \n\tif (first_run) {\n\t\tsigned long timeout = msecs_to_jiffies(SECS_FIRST_SCAN * 1000);\n\t\tfirst_run = 0;\n\t\twhile (timeout && !kthread_should_stop())\n\t\t\ttimeout = schedule_timeout_interruptible(timeout);\n\t}\n\n\twhile (!kthread_should_stop()) {\n\t\tsigned long timeout = READ_ONCE(jiffies_scan_wait);\n\n\t\tmutex_lock(&scan_mutex);\n\t\tkmemleak_scan();\n\t\tmutex_unlock(&scan_mutex);\n\n\t\t \n\t\twhile (timeout && !kthread_should_stop())\n\t\t\ttimeout = schedule_timeout_interruptible(timeout);\n\t}\n\n\tpr_info(\"Automatic memory scanning thread ended\\n\");\n\n\treturn 0;\n}\n\n \nstatic void start_scan_thread(void)\n{\n\tif (scan_thread)\n\t\treturn;\n\tscan_thread = kthread_run(kmemleak_scan_thread, NULL, \"kmemleak\");\n\tif (IS_ERR(scan_thread)) {\n\t\tpr_warn(\"Failed to create the scan thread\\n\");\n\t\tscan_thread = NULL;\n\t}\n}\n\n \nstatic void stop_scan_thread(void)\n{\n\tif (scan_thread) {\n\t\tkthread_stop(scan_thread);\n\t\tscan_thread = NULL;\n\t}\n}\n\n \nstatic void *kmemleak_seq_start(struct seq_file *seq, loff_t *pos)\n{\n\tstruct kmemleak_object *object;\n\tloff_t n = *pos;\n\tint err;\n\n\terr = mutex_lock_interruptible(&scan_mutex);\n\tif (err < 0)\n\t\treturn ERR_PTR(err);\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(object, &object_list, object_list) {\n\t\tif (n-- > 0)\n\t\t\tcontinue;\n\t\tif (get_object(object))\n\t\t\tgoto out;\n\t}\n\tobject = NULL;\nout:\n\treturn object;\n}\n\n \nstatic void *kmemleak_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\tstruct kmemleak_object *prev_obj = v;\n\tstruct kmemleak_object *next_obj = NULL;\n\tstruct kmemleak_object *obj = prev_obj;\n\n\t++(*pos);\n\n\tlist_for_each_entry_continue_rcu(obj, &object_list, object_list) {\n\t\tif (get_object(obj)) {\n\t\t\tnext_obj = obj;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tput_object(prev_obj);\n\treturn next_obj;\n}\n\n \nstatic void kmemleak_seq_stop(struct seq_file *seq, void *v)\n{\n\tif (!IS_ERR(v)) {\n\t\t \n\t\trcu_read_unlock();\n\t\tmutex_unlock(&scan_mutex);\n\t\tif (v)\n\t\t\tput_object(v);\n\t}\n}\n\n \nstatic int kmemleak_seq_show(struct seq_file *seq, void *v)\n{\n\tstruct kmemleak_object *object = v;\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&object->lock, flags);\n\tif ((object->flags & OBJECT_REPORTED) && unreferenced_object(object))\n\t\tprint_unreferenced(seq, object);\n\traw_spin_unlock_irqrestore(&object->lock, flags);\n\treturn 0;\n}\n\nstatic const struct seq_operations kmemleak_seq_ops = {\n\t.start = kmemleak_seq_start,\n\t.next  = kmemleak_seq_next,\n\t.stop  = kmemleak_seq_stop,\n\t.show  = kmemleak_seq_show,\n};\n\nstatic int kmemleak_open(struct inode *inode, struct file *file)\n{\n\treturn seq_open(file, &kmemleak_seq_ops);\n}\n\nstatic int dump_str_object_info(const char *str)\n{\n\tunsigned long flags;\n\tstruct kmemleak_object *object;\n\tunsigned long addr;\n\n\tif (kstrtoul(str, 0, &addr))\n\t\treturn -EINVAL;\n\tobject = find_and_get_object(addr, 0);\n\tif (!object) {\n\t\tpr_info(\"Unknown object at 0x%08lx\\n\", addr);\n\t\treturn -EINVAL;\n\t}\n\n\traw_spin_lock_irqsave(&object->lock, flags);\n\tdump_object_info(object);\n\traw_spin_unlock_irqrestore(&object->lock, flags);\n\n\tput_object(object);\n\treturn 0;\n}\n\n \nstatic void kmemleak_clear(void)\n{\n\tstruct kmemleak_object *object;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(object, &object_list, object_list) {\n\t\traw_spin_lock_irq(&object->lock);\n\t\tif ((object->flags & OBJECT_REPORTED) &&\n\t\t    unreferenced_object(object))\n\t\t\t__paint_it(object, KMEMLEAK_GREY);\n\t\traw_spin_unlock_irq(&object->lock);\n\t}\n\trcu_read_unlock();\n\n\tkmemleak_found_leaks = false;\n}\n\nstatic void __kmemleak_do_cleanup(void);\n\n \nstatic ssize_t kmemleak_write(struct file *file, const char __user *user_buf,\n\t\t\t      size_t size, loff_t *ppos)\n{\n\tchar buf[64];\n\tint buf_size;\n\tint ret;\n\n\tbuf_size = min(size, (sizeof(buf) - 1));\n\tif (strncpy_from_user(buf, user_buf, buf_size) < 0)\n\t\treturn -EFAULT;\n\tbuf[buf_size] = 0;\n\n\tret = mutex_lock_interruptible(&scan_mutex);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (strncmp(buf, \"clear\", 5) == 0) {\n\t\tif (kmemleak_enabled)\n\t\t\tkmemleak_clear();\n\t\telse\n\t\t\t__kmemleak_do_cleanup();\n\t\tgoto out;\n\t}\n\n\tif (!kmemleak_enabled) {\n\t\tret = -EPERM;\n\t\tgoto out;\n\t}\n\n\tif (strncmp(buf, \"off\", 3) == 0)\n\t\tkmemleak_disable();\n\telse if (strncmp(buf, \"stack=on\", 8) == 0)\n\t\tkmemleak_stack_scan = 1;\n\telse if (strncmp(buf, \"stack=off\", 9) == 0)\n\t\tkmemleak_stack_scan = 0;\n\telse if (strncmp(buf, \"scan=on\", 7) == 0)\n\t\tstart_scan_thread();\n\telse if (strncmp(buf, \"scan=off\", 8) == 0)\n\t\tstop_scan_thread();\n\telse if (strncmp(buf, \"scan=\", 5) == 0) {\n\t\tunsigned secs;\n\t\tunsigned long msecs;\n\n\t\tret = kstrtouint(buf + 5, 0, &secs);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\n\t\tmsecs = secs * MSEC_PER_SEC;\n\t\tif (msecs > UINT_MAX)\n\t\t\tmsecs = UINT_MAX;\n\n\t\tstop_scan_thread();\n\t\tif (msecs) {\n\t\t\tWRITE_ONCE(jiffies_scan_wait, msecs_to_jiffies(msecs));\n\t\t\tstart_scan_thread();\n\t\t}\n\t} else if (strncmp(buf, \"scan\", 4) == 0)\n\t\tkmemleak_scan();\n\telse if (strncmp(buf, \"dump=\", 5) == 0)\n\t\tret = dump_str_object_info(buf + 5);\n\telse\n\t\tret = -EINVAL;\n\nout:\n\tmutex_unlock(&scan_mutex);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t \n\t*ppos += size;\n\treturn size;\n}\n\nstatic const struct file_operations kmemleak_fops = {\n\t.owner\t\t= THIS_MODULE,\n\t.open\t\t= kmemleak_open,\n\t.read\t\t= seq_read,\n\t.write\t\t= kmemleak_write,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= seq_release,\n};\n\nstatic void __kmemleak_do_cleanup(void)\n{\n\tstruct kmemleak_object *object, *tmp;\n\n\t \n\tlist_for_each_entry_safe(object, tmp, &object_list, object_list) {\n\t\t__remove_object(object);\n\t\t__delete_object(object);\n\t}\n}\n\n \nstatic void kmemleak_do_cleanup(struct work_struct *work)\n{\n\tstop_scan_thread();\n\n\tmutex_lock(&scan_mutex);\n\t \n\tkmemleak_free_enabled = 0;\n\tmutex_unlock(&scan_mutex);\n\n\tif (!kmemleak_found_leaks)\n\t\t__kmemleak_do_cleanup();\n\telse\n\t\tpr_info(\"Kmemleak disabled without freeing internal data. Reclaim the memory with \\\"echo clear > /sys/kernel/debug/kmemleak\\\".\\n\");\n}\n\nstatic DECLARE_WORK(cleanup_work, kmemleak_do_cleanup);\n\n \nstatic void kmemleak_disable(void)\n{\n\t \n\tif (cmpxchg(&kmemleak_error, 0, 1))\n\t\treturn;\n\n\t \n\tkmemleak_enabled = 0;\n\n\t \n\tif (kmemleak_late_initialized)\n\t\tschedule_work(&cleanup_work);\n\telse\n\t\tkmemleak_free_enabled = 0;\n\n\tpr_info(\"Kernel memory leak detector disabled\\n\");\n}\n\n \nstatic int __init kmemleak_boot_config(char *str)\n{\n\tif (!str)\n\t\treturn -EINVAL;\n\tif (strcmp(str, \"off\") == 0)\n\t\tkmemleak_disable();\n\telse if (strcmp(str, \"on\") == 0) {\n\t\tkmemleak_skip_disable = 1;\n\t\tstack_depot_request_early_init();\n\t}\n\telse\n\t\treturn -EINVAL;\n\treturn 0;\n}\nearly_param(\"kmemleak\", kmemleak_boot_config);\n\n \nvoid __init kmemleak_init(void)\n{\n#ifdef CONFIG_DEBUG_KMEMLEAK_DEFAULT_OFF\n\tif (!kmemleak_skip_disable) {\n\t\tkmemleak_disable();\n\t\treturn;\n\t}\n#endif\n\n\tif (kmemleak_error)\n\t\treturn;\n\n\tjiffies_min_age = msecs_to_jiffies(MSECS_MIN_AGE);\n\tjiffies_scan_wait = msecs_to_jiffies(SECS_SCAN_WAIT * 1000);\n\n\tobject_cache = KMEM_CACHE(kmemleak_object, SLAB_NOLEAKTRACE);\n\tscan_area_cache = KMEM_CACHE(kmemleak_scan_area, SLAB_NOLEAKTRACE);\n\n\t \n\tcreate_object((unsigned long)_sdata, _edata - _sdata,\n\t\t      KMEMLEAK_GREY, GFP_ATOMIC);\n\tcreate_object((unsigned long)__bss_start, __bss_stop - __bss_start,\n\t\t      KMEMLEAK_GREY, GFP_ATOMIC);\n\t \n\tif (&__start_ro_after_init < &_sdata || &__end_ro_after_init > &_edata)\n\t\tcreate_object((unsigned long)__start_ro_after_init,\n\t\t\t      __end_ro_after_init - __start_ro_after_init,\n\t\t\t      KMEMLEAK_GREY, GFP_ATOMIC);\n}\n\n \nstatic int __init kmemleak_late_init(void)\n{\n\tkmemleak_late_initialized = 1;\n\n\tdebugfs_create_file(\"kmemleak\", 0644, NULL, NULL, &kmemleak_fops);\n\n\tif (kmemleak_error) {\n\t\t \n\t\tschedule_work(&cleanup_work);\n\t\treturn -ENOMEM;\n\t}\n\n\tif (IS_ENABLED(CONFIG_DEBUG_KMEMLEAK_AUTO_SCAN)) {\n\t\tmutex_lock(&scan_mutex);\n\t\tstart_scan_thread();\n\t\tmutex_unlock(&scan_mutex);\n\t}\n\n\tpr_info(\"Kernel memory leak detector initialized (mem pool available: %d)\\n\",\n\t\tmem_pool_free_count);\n\n\treturn 0;\n}\nlate_initcall(kmemleak_late_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}