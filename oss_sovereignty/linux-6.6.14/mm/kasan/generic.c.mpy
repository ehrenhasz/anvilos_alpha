{
  "module_name": "generic.c",
  "hash_id": "ece5db2dcd6d72a474e09263ed5ebbfbe80ebe4a4966ddc4b3d07743790ba6e1",
  "original_prompt": "Ingested from linux-6.6.14/mm/kasan/generic.c",
  "human_readable_source": "\n \n\n#include <linux/export.h>\n#include <linux/interrupt.h>\n#include <linux/init.h>\n#include <linux/kasan.h>\n#include <linux/kernel.h>\n#include <linux/kfence.h>\n#include <linux/kmemleak.h>\n#include <linux/linkage.h>\n#include <linux/memblock.h>\n#include <linux/memory.h>\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <linux/printk.h>\n#include <linux/sched.h>\n#include <linux/sched/task_stack.h>\n#include <linux/slab.h>\n#include <linux/stacktrace.h>\n#include <linux/string.h>\n#include <linux/types.h>\n#include <linux/vmalloc.h>\n#include <linux/bug.h>\n\n#include \"kasan.h\"\n#include \"../slab.h\"\n\n \n\nstatic __always_inline bool memory_is_poisoned_1(const void *addr)\n{\n\ts8 shadow_value = *(s8 *)kasan_mem_to_shadow(addr);\n\n\tif (unlikely(shadow_value)) {\n\t\ts8 last_accessible_byte = (unsigned long)addr & KASAN_GRANULE_MASK;\n\t\treturn unlikely(last_accessible_byte >= shadow_value);\n\t}\n\n\treturn false;\n}\n\nstatic __always_inline bool memory_is_poisoned_2_4_8(const void *addr,\n\t\t\t\t\t\tunsigned long size)\n{\n\tu8 *shadow_addr = (u8 *)kasan_mem_to_shadow(addr);\n\n\t \n\tif (unlikely((((unsigned long)addr + size - 1) & KASAN_GRANULE_MASK) < size - 1))\n\t\treturn *shadow_addr || memory_is_poisoned_1(addr + size - 1);\n\n\treturn memory_is_poisoned_1(addr + size - 1);\n}\n\nstatic __always_inline bool memory_is_poisoned_16(const void *addr)\n{\n\tu16 *shadow_addr = (u16 *)kasan_mem_to_shadow(addr);\n\n\t \n\tif (unlikely(!IS_ALIGNED((unsigned long)addr, KASAN_GRANULE_SIZE)))\n\t\treturn *shadow_addr || memory_is_poisoned_1(addr + 15);\n\n\treturn *shadow_addr;\n}\n\nstatic __always_inline unsigned long bytes_is_nonzero(const u8 *start,\n\t\t\t\t\tsize_t size)\n{\n\twhile (size) {\n\t\tif (unlikely(*start))\n\t\t\treturn (unsigned long)start;\n\t\tstart++;\n\t\tsize--;\n\t}\n\n\treturn 0;\n}\n\nstatic __always_inline unsigned long memory_is_nonzero(const void *start,\n\t\t\t\t\t\tconst void *end)\n{\n\tunsigned int words;\n\tunsigned long ret;\n\tunsigned int prefix = (unsigned long)start % 8;\n\n\tif (end - start <= 16)\n\t\treturn bytes_is_nonzero(start, end - start);\n\n\tif (prefix) {\n\t\tprefix = 8 - prefix;\n\t\tret = bytes_is_nonzero(start, prefix);\n\t\tif (unlikely(ret))\n\t\t\treturn ret;\n\t\tstart += prefix;\n\t}\n\n\twords = (end - start) / 8;\n\twhile (words) {\n\t\tif (unlikely(*(u64 *)start))\n\t\t\treturn bytes_is_nonzero(start, 8);\n\t\tstart += 8;\n\t\twords--;\n\t}\n\n\treturn bytes_is_nonzero(start, (end - start) % 8);\n}\n\nstatic __always_inline bool memory_is_poisoned_n(const void *addr, size_t size)\n{\n\tunsigned long ret;\n\n\tret = memory_is_nonzero(kasan_mem_to_shadow(addr),\n\t\t\tkasan_mem_to_shadow(addr + size - 1) + 1);\n\n\tif (unlikely(ret)) {\n\t\tconst void *last_byte = addr + size - 1;\n\t\ts8 *last_shadow = (s8 *)kasan_mem_to_shadow(last_byte);\n\t\ts8 last_accessible_byte = (unsigned long)last_byte & KASAN_GRANULE_MASK;\n\n\t\tif (unlikely(ret != (unsigned long)last_shadow ||\n\t\t\t     last_accessible_byte >= *last_shadow))\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic __always_inline bool memory_is_poisoned(const void *addr, size_t size)\n{\n\tif (__builtin_constant_p(size)) {\n\t\tswitch (size) {\n\t\tcase 1:\n\t\t\treturn memory_is_poisoned_1(addr);\n\t\tcase 2:\n\t\tcase 4:\n\t\tcase 8:\n\t\t\treturn memory_is_poisoned_2_4_8(addr, size);\n\t\tcase 16:\n\t\t\treturn memory_is_poisoned_16(addr);\n\t\tdefault:\n\t\t\tBUILD_BUG();\n\t\t}\n\t}\n\n\treturn memory_is_poisoned_n(addr, size);\n}\n\nstatic __always_inline bool check_region_inline(const void *addr,\n\t\t\t\t\t\tsize_t size, bool write,\n\t\t\t\t\t\tunsigned long ret_ip)\n{\n\tif (!kasan_arch_is_ready())\n\t\treturn true;\n\n\tif (unlikely(size == 0))\n\t\treturn true;\n\n\tif (unlikely(addr + size < addr))\n\t\treturn !kasan_report(addr, size, write, ret_ip);\n\n\tif (unlikely(!addr_has_metadata(addr)))\n\t\treturn !kasan_report(addr, size, write, ret_ip);\n\n\tif (likely(!memory_is_poisoned(addr, size)))\n\t\treturn true;\n\n\treturn !kasan_report(addr, size, write, ret_ip);\n}\n\nbool kasan_check_range(const void *addr, size_t size, bool write,\n\t\t\t\t\tunsigned long ret_ip)\n{\n\treturn check_region_inline(addr, size, write, ret_ip);\n}\n\nbool kasan_byte_accessible(const void *addr)\n{\n\ts8 shadow_byte;\n\n\tif (!kasan_arch_is_ready())\n\t\treturn true;\n\n\tshadow_byte = READ_ONCE(*(s8 *)kasan_mem_to_shadow(addr));\n\n\treturn shadow_byte >= 0 && shadow_byte < KASAN_GRANULE_SIZE;\n}\n\nvoid kasan_cache_shrink(struct kmem_cache *cache)\n{\n\tkasan_quarantine_remove_cache(cache);\n}\n\nvoid kasan_cache_shutdown(struct kmem_cache *cache)\n{\n\tif (!__kmem_cache_empty(cache))\n\t\tkasan_quarantine_remove_cache(cache);\n}\n\nstatic void register_global(struct kasan_global *global)\n{\n\tsize_t aligned_size = round_up(global->size, KASAN_GRANULE_SIZE);\n\n\tkasan_unpoison(global->beg, global->size, false);\n\n\tkasan_poison(global->beg + aligned_size,\n\t\t     global->size_with_redzone - aligned_size,\n\t\t     KASAN_GLOBAL_REDZONE, false);\n}\n\nvoid __asan_register_globals(void *ptr, ssize_t size)\n{\n\tint i;\n\tstruct kasan_global *globals = ptr;\n\n\tfor (i = 0; i < size; i++)\n\t\tregister_global(&globals[i]);\n}\nEXPORT_SYMBOL(__asan_register_globals);\n\nvoid __asan_unregister_globals(void *ptr, ssize_t size)\n{\n}\nEXPORT_SYMBOL(__asan_unregister_globals);\n\n#define DEFINE_ASAN_LOAD_STORE(size)\t\t\t\t\t\\\n\tvoid __asan_load##size(void *addr)\t\t\t\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\tcheck_region_inline(addr, size, false, _RET_IP_);\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tEXPORT_SYMBOL(__asan_load##size);\t\t\t\t\\\n\t__alias(__asan_load##size)\t\t\t\t\t\\\n\tvoid __asan_load##size##_noabort(void *);\t\t\t\\\n\tEXPORT_SYMBOL(__asan_load##size##_noabort);\t\t\t\\\n\tvoid __asan_store##size(void *addr)\t\t\t\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\tcheck_region_inline(addr, size, true, _RET_IP_);\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tEXPORT_SYMBOL(__asan_store##size);\t\t\t\t\\\n\t__alias(__asan_store##size)\t\t\t\t\t\\\n\tvoid __asan_store##size##_noabort(void *);\t\t\t\\\n\tEXPORT_SYMBOL(__asan_store##size##_noabort)\n\nDEFINE_ASAN_LOAD_STORE(1);\nDEFINE_ASAN_LOAD_STORE(2);\nDEFINE_ASAN_LOAD_STORE(4);\nDEFINE_ASAN_LOAD_STORE(8);\nDEFINE_ASAN_LOAD_STORE(16);\n\nvoid __asan_loadN(void *addr, ssize_t size)\n{\n\tkasan_check_range(addr, size, false, _RET_IP_);\n}\nEXPORT_SYMBOL(__asan_loadN);\n\n__alias(__asan_loadN)\nvoid __asan_loadN_noabort(void *, ssize_t);\nEXPORT_SYMBOL(__asan_loadN_noabort);\n\nvoid __asan_storeN(void *addr, ssize_t size)\n{\n\tkasan_check_range(addr, size, true, _RET_IP_);\n}\nEXPORT_SYMBOL(__asan_storeN);\n\n__alias(__asan_storeN)\nvoid __asan_storeN_noabort(void *, ssize_t);\nEXPORT_SYMBOL(__asan_storeN_noabort);\n\n \nvoid __asan_handle_no_return(void) {}\nEXPORT_SYMBOL(__asan_handle_no_return);\n\n \nvoid __asan_alloca_poison(void *addr, ssize_t size)\n{\n\tsize_t rounded_up_size = round_up(size, KASAN_GRANULE_SIZE);\n\tsize_t padding_size = round_up(size, KASAN_ALLOCA_REDZONE_SIZE) -\n\t\t\trounded_up_size;\n\tsize_t rounded_down_size = round_down(size, KASAN_GRANULE_SIZE);\n\n\tconst void *left_redzone = (const void *)(addr -\n\t\t\tKASAN_ALLOCA_REDZONE_SIZE);\n\tconst void *right_redzone = (const void *)(addr + rounded_up_size);\n\n\tWARN_ON(!IS_ALIGNED((unsigned long)addr, KASAN_ALLOCA_REDZONE_SIZE));\n\n\tkasan_unpoison((const void *)(addr + rounded_down_size),\n\t\t\tsize - rounded_down_size, false);\n\tkasan_poison(left_redzone, KASAN_ALLOCA_REDZONE_SIZE,\n\t\t     KASAN_ALLOCA_LEFT, false);\n\tkasan_poison(right_redzone, padding_size + KASAN_ALLOCA_REDZONE_SIZE,\n\t\t     KASAN_ALLOCA_RIGHT, false);\n}\nEXPORT_SYMBOL(__asan_alloca_poison);\n\n \nvoid __asan_allocas_unpoison(void *stack_top, ssize_t stack_bottom)\n{\n\tif (unlikely(!stack_top || stack_top > (void *)stack_bottom))\n\t\treturn;\n\n\tkasan_unpoison(stack_top, (void *)stack_bottom - stack_top, false);\n}\nEXPORT_SYMBOL(__asan_allocas_unpoison);\n\n \n#define DEFINE_ASAN_SET_SHADOW(byte) \\\n\tvoid __asan_set_shadow_##byte(const void *addr, ssize_t size)\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\t__memset((void *)addr, 0x##byte, size);\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tEXPORT_SYMBOL(__asan_set_shadow_##byte)\n\nDEFINE_ASAN_SET_SHADOW(00);\nDEFINE_ASAN_SET_SHADOW(f1);\nDEFINE_ASAN_SET_SHADOW(f2);\nDEFINE_ASAN_SET_SHADOW(f3);\nDEFINE_ASAN_SET_SHADOW(f5);\nDEFINE_ASAN_SET_SHADOW(f8);\n\n \nslab_flags_t kasan_never_merge(void)\n{\n\tif (!kasan_requires_meta())\n\t\treturn 0;\n\treturn SLAB_KASAN;\n}\n\n \nstatic inline unsigned int optimal_redzone(unsigned int object_size)\n{\n\treturn\n\t\tobject_size <= 64        - 16   ? 16 :\n\t\tobject_size <= 128       - 32   ? 32 :\n\t\tobject_size <= 512       - 64   ? 64 :\n\t\tobject_size <= 4096      - 128  ? 128 :\n\t\tobject_size <= (1 << 14) - 256  ? 256 :\n\t\tobject_size <= (1 << 15) - 512  ? 512 :\n\t\tobject_size <= (1 << 16) - 1024 ? 1024 : 2048;\n}\n\nvoid kasan_cache_create(struct kmem_cache *cache, unsigned int *size,\n\t\t\t  slab_flags_t *flags)\n{\n\tunsigned int ok_size;\n\tunsigned int optimal_size;\n\n\tif (!kasan_requires_meta())\n\t\treturn;\n\n\t \n\t*flags |= SLAB_KASAN;\n\n\tok_size = *size;\n\n\t \n\tcache->kasan_info.alloc_meta_offset = *size;\n\t*size += sizeof(struct kasan_alloc_meta);\n\n\t \n\tif (*size > KMALLOC_MAX_SIZE) {\n\t\tcache->kasan_info.alloc_meta_offset = 0;\n\t\t*size = ok_size;\n\t\t \n\t}\n\n\t \n\tif ((cache->flags & SLAB_TYPESAFE_BY_RCU) || cache->ctor ||\n\t    cache->object_size < sizeof(struct kasan_free_meta)) {\n\t\tok_size = *size;\n\n\t\tcache->kasan_info.free_meta_offset = *size;\n\t\t*size += sizeof(struct kasan_free_meta);\n\n\t\t \n\t\tif (*size > KMALLOC_MAX_SIZE) {\n\t\t\tcache->kasan_info.free_meta_offset = KASAN_NO_FREE_META;\n\t\t\t*size = ok_size;\n\t\t}\n\t}\n\n\t \n\toptimal_size = cache->object_size + optimal_redzone(cache->object_size);\n\t \n\tif (optimal_size > KMALLOC_MAX_SIZE)\n\t\toptimal_size = KMALLOC_MAX_SIZE;\n\t \n\tif (*size < optimal_size)\n\t\t*size = optimal_size;\n}\n\nstruct kasan_alloc_meta *kasan_get_alloc_meta(struct kmem_cache *cache,\n\t\t\t\t\t      const void *object)\n{\n\tif (!cache->kasan_info.alloc_meta_offset)\n\t\treturn NULL;\n\treturn (void *)object + cache->kasan_info.alloc_meta_offset;\n}\n\nstruct kasan_free_meta *kasan_get_free_meta(struct kmem_cache *cache,\n\t\t\t\t\t    const void *object)\n{\n\tBUILD_BUG_ON(sizeof(struct kasan_free_meta) > 32);\n\tif (cache->kasan_info.free_meta_offset == KASAN_NO_FREE_META)\n\t\treturn NULL;\n\treturn (void *)object + cache->kasan_info.free_meta_offset;\n}\n\nvoid kasan_init_object_meta(struct kmem_cache *cache, const void *object)\n{\n\tstruct kasan_alloc_meta *alloc_meta;\n\n\talloc_meta = kasan_get_alloc_meta(cache, object);\n\tif (alloc_meta)\n\t\t__memset(alloc_meta, 0, sizeof(*alloc_meta));\n}\n\nsize_t kasan_metadata_size(struct kmem_cache *cache, bool in_object)\n{\n\tstruct kasan_cache *info = &cache->kasan_info;\n\n\tif (!kasan_requires_meta())\n\t\treturn 0;\n\n\tif (in_object)\n\t\treturn (info->free_meta_offset ?\n\t\t\t0 : sizeof(struct kasan_free_meta));\n\telse\n\t\treturn (info->alloc_meta_offset ?\n\t\t\tsizeof(struct kasan_alloc_meta) : 0) +\n\t\t\t((info->free_meta_offset &&\n\t\t\tinfo->free_meta_offset != KASAN_NO_FREE_META) ?\n\t\t\tsizeof(struct kasan_free_meta) : 0);\n}\n\nstatic void __kasan_record_aux_stack(void *addr, bool can_alloc)\n{\n\tstruct slab *slab = kasan_addr_to_slab(addr);\n\tstruct kmem_cache *cache;\n\tstruct kasan_alloc_meta *alloc_meta;\n\tvoid *object;\n\n\tif (is_kfence_address(addr) || !slab)\n\t\treturn;\n\n\tcache = slab->slab_cache;\n\tobject = nearest_obj(cache, slab, addr);\n\talloc_meta = kasan_get_alloc_meta(cache, object);\n\tif (!alloc_meta)\n\t\treturn;\n\n\talloc_meta->aux_stack[1] = alloc_meta->aux_stack[0];\n\talloc_meta->aux_stack[0] = kasan_save_stack(0, can_alloc);\n}\n\nvoid kasan_record_aux_stack(void *addr)\n{\n\treturn __kasan_record_aux_stack(addr, true);\n}\n\nvoid kasan_record_aux_stack_noalloc(void *addr)\n{\n\treturn __kasan_record_aux_stack(addr, false);\n}\n\nvoid kasan_save_alloc_info(struct kmem_cache *cache, void *object, gfp_t flags)\n{\n\tstruct kasan_alloc_meta *alloc_meta;\n\n\talloc_meta = kasan_get_alloc_meta(cache, object);\n\tif (alloc_meta)\n\t\tkasan_set_track(&alloc_meta->alloc_track, flags);\n}\n\nvoid kasan_save_free_info(struct kmem_cache *cache, void *object)\n{\n\tstruct kasan_free_meta *free_meta;\n\n\tfree_meta = kasan_get_free_meta(cache, object);\n\tif (!free_meta)\n\t\treturn;\n\n\tkasan_set_track(&free_meta->free_track, 0);\n\t \n\t*(u8 *)kasan_mem_to_shadow(object) = KASAN_SLAB_FREETRACK;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}