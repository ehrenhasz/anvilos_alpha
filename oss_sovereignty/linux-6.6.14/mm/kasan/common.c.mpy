{
  "module_name": "common.c",
  "hash_id": "f869dea97fac0f6dac80e730915fea0d438ec336614c172091e230f1f2e6b681",
  "original_prompt": "Ingested from linux-6.6.14/mm/kasan/common.c",
  "human_readable_source": "\n \n\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/kasan.h>\n#include <linux/kernel.h>\n#include <linux/linkage.h>\n#include <linux/memblock.h>\n#include <linux/memory.h>\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <linux/printk.h>\n#include <linux/sched.h>\n#include <linux/sched/task_stack.h>\n#include <linux/slab.h>\n#include <linux/stacktrace.h>\n#include <linux/string.h>\n#include <linux/types.h>\n#include <linux/bug.h>\n\n#include \"kasan.h\"\n#include \"../slab.h\"\n\nstruct slab *kasan_addr_to_slab(const void *addr)\n{\n\tif (virt_addr_valid(addr))\n\t\treturn virt_to_slab(addr);\n\treturn NULL;\n}\n\ndepot_stack_handle_t kasan_save_stack(gfp_t flags, bool can_alloc)\n{\n\tunsigned long entries[KASAN_STACK_DEPTH];\n\tunsigned int nr_entries;\n\n\tnr_entries = stack_trace_save(entries, ARRAY_SIZE(entries), 0);\n\treturn __stack_depot_save(entries, nr_entries, flags, can_alloc);\n}\n\nvoid kasan_set_track(struct kasan_track *track, gfp_t flags)\n{\n\ttrack->pid = current->pid;\n\ttrack->stack = kasan_save_stack(flags, true);\n}\n\n#if defined(CONFIG_KASAN_GENERIC) || defined(CONFIG_KASAN_SW_TAGS)\nvoid kasan_enable_current(void)\n{\n\tcurrent->kasan_depth++;\n}\nEXPORT_SYMBOL(kasan_enable_current);\n\nvoid kasan_disable_current(void)\n{\n\tcurrent->kasan_depth--;\n}\nEXPORT_SYMBOL(kasan_disable_current);\n\n#endif  \n\nvoid __kasan_unpoison_range(const void *address, size_t size)\n{\n\tkasan_unpoison(address, size, false);\n}\n\n#ifdef CONFIG_KASAN_STACK\n \nvoid kasan_unpoison_task_stack(struct task_struct *task)\n{\n\tvoid *base = task_stack_page(task);\n\n\tkasan_unpoison(base, THREAD_SIZE, false);\n}\n\n \nasmlinkage void kasan_unpoison_task_stack_below(const void *watermark)\n{\n\t \n\tvoid *base = (void *)((unsigned long)watermark & ~(THREAD_SIZE - 1));\n\n\tkasan_unpoison(base, watermark - base, false);\n}\n#endif  \n\nbool __kasan_unpoison_pages(struct page *page, unsigned int order, bool init)\n{\n\tu8 tag;\n\tunsigned long i;\n\n\tif (unlikely(PageHighMem(page)))\n\t\treturn false;\n\n\tif (!kasan_sample_page_alloc(order))\n\t\treturn false;\n\n\ttag = kasan_random_tag();\n\tkasan_unpoison(set_tag(page_address(page), tag),\n\t\t       PAGE_SIZE << order, init);\n\tfor (i = 0; i < (1 << order); i++)\n\t\tpage_kasan_tag_set(page + i, tag);\n\n\treturn true;\n}\n\nvoid __kasan_poison_pages(struct page *page, unsigned int order, bool init)\n{\n\tif (likely(!PageHighMem(page)))\n\t\tkasan_poison(page_address(page), PAGE_SIZE << order,\n\t\t\t     KASAN_PAGE_FREE, init);\n}\n\nvoid __kasan_poison_slab(struct slab *slab)\n{\n\tstruct page *page = slab_page(slab);\n\tunsigned long i;\n\n\tfor (i = 0; i < compound_nr(page); i++)\n\t\tpage_kasan_tag_reset(page + i);\n\tkasan_poison(page_address(page), page_size(page),\n\t\t     KASAN_SLAB_REDZONE, false);\n}\n\nvoid __kasan_unpoison_object_data(struct kmem_cache *cache, void *object)\n{\n\tkasan_unpoison(object, cache->object_size, false);\n}\n\nvoid __kasan_poison_object_data(struct kmem_cache *cache, void *object)\n{\n\tkasan_poison(object, round_up(cache->object_size, KASAN_GRANULE_SIZE),\n\t\t\tKASAN_SLAB_REDZONE, false);\n}\n\n \nstatic inline u8 assign_tag(struct kmem_cache *cache,\n\t\t\t\t\tconst void *object, bool init)\n{\n\tif (IS_ENABLED(CONFIG_KASAN_GENERIC))\n\t\treturn 0xff;\n\n\t \n\tif (!cache->ctor && !(cache->flags & SLAB_TYPESAFE_BY_RCU))\n\t\treturn init ? KASAN_TAG_KERNEL : kasan_random_tag();\n\n\t \n#ifdef CONFIG_SLAB\n\t \n\treturn (u8)obj_to_index(cache, virt_to_slab(object), (void *)object);\n#else\n\t \n\treturn init ? kasan_random_tag() : get_tag(object);\n#endif\n}\n\nvoid * __must_check __kasan_init_slab_obj(struct kmem_cache *cache,\n\t\t\t\t\t\tconst void *object)\n{\n\t \n\tif (kasan_requires_meta())\n\t\tkasan_init_object_meta(cache, object);\n\n\t \n\tobject = set_tag(object, assign_tag(cache, object, true));\n\n\treturn (void *)object;\n}\n\nstatic inline bool ____kasan_slab_free(struct kmem_cache *cache, void *object,\n\t\t\t\tunsigned long ip, bool quarantine, bool init)\n{\n\tvoid *tagged_object;\n\n\tif (!kasan_arch_is_ready())\n\t\treturn false;\n\n\ttagged_object = object;\n\tobject = kasan_reset_tag(object);\n\n\tif (is_kfence_address(object))\n\t\treturn false;\n\n\tif (unlikely(nearest_obj(cache, virt_to_slab(object), object) !=\n\t    object)) {\n\t\tkasan_report_invalid_free(tagged_object, ip, KASAN_REPORT_INVALID_FREE);\n\t\treturn true;\n\t}\n\n\t \n\tif (unlikely(cache->flags & SLAB_TYPESAFE_BY_RCU))\n\t\treturn false;\n\n\tif (!kasan_byte_accessible(tagged_object)) {\n\t\tkasan_report_invalid_free(tagged_object, ip, KASAN_REPORT_DOUBLE_FREE);\n\t\treturn true;\n\t}\n\n\tkasan_poison(object, round_up(cache->object_size, KASAN_GRANULE_SIZE),\n\t\t\tKASAN_SLAB_FREE, init);\n\n\tif ((IS_ENABLED(CONFIG_KASAN_GENERIC) && !quarantine))\n\t\treturn false;\n\n\tif (kasan_stack_collection_enabled())\n\t\tkasan_save_free_info(cache, tagged_object);\n\n\treturn kasan_quarantine_put(cache, object);\n}\n\nbool __kasan_slab_free(struct kmem_cache *cache, void *object,\n\t\t\t\tunsigned long ip, bool init)\n{\n\treturn ____kasan_slab_free(cache, object, ip, true, init);\n}\n\nstatic inline bool ____kasan_kfree_large(void *ptr, unsigned long ip)\n{\n\tif (!kasan_arch_is_ready())\n\t\treturn false;\n\n\tif (ptr != page_address(virt_to_head_page(ptr))) {\n\t\tkasan_report_invalid_free(ptr, ip, KASAN_REPORT_INVALID_FREE);\n\t\treturn true;\n\t}\n\n\tif (!kasan_byte_accessible(ptr)) {\n\t\tkasan_report_invalid_free(ptr, ip, KASAN_REPORT_DOUBLE_FREE);\n\t\treturn true;\n\t}\n\n\t \n\n\treturn false;\n}\n\nvoid __kasan_kfree_large(void *ptr, unsigned long ip)\n{\n\t____kasan_kfree_large(ptr, ip);\n}\n\nvoid __kasan_slab_free_mempool(void *ptr, unsigned long ip)\n{\n\tstruct folio *folio;\n\n\tfolio = virt_to_folio(ptr);\n\n\t \n\tif (unlikely(!folio_test_slab(folio))) {\n\t\tif (____kasan_kfree_large(ptr, ip))\n\t\t\treturn;\n\t\tkasan_poison(ptr, folio_size(folio), KASAN_PAGE_FREE, false);\n\t} else {\n\t\tstruct slab *slab = folio_slab(folio);\n\n\t\t____kasan_slab_free(slab->slab_cache, ptr, ip, false, false);\n\t}\n}\n\nvoid * __must_check __kasan_slab_alloc(struct kmem_cache *cache,\n\t\t\t\t\tvoid *object, gfp_t flags, bool init)\n{\n\tu8 tag;\n\tvoid *tagged_object;\n\n\tif (gfpflags_allow_blocking(flags))\n\t\tkasan_quarantine_reduce();\n\n\tif (unlikely(object == NULL))\n\t\treturn NULL;\n\n\tif (is_kfence_address(object))\n\t\treturn (void *)object;\n\n\t \n\ttag = assign_tag(cache, object, false);\n\ttagged_object = set_tag(object, tag);\n\n\t \n\tkasan_unpoison(tagged_object, cache->object_size, init);\n\n\t \n\tif (kasan_stack_collection_enabled() && !is_kmalloc_cache(cache))\n\t\tkasan_save_alloc_info(cache, tagged_object, flags);\n\n\treturn tagged_object;\n}\n\nstatic inline void *____kasan_kmalloc(struct kmem_cache *cache,\n\t\t\t\tconst void *object, size_t size, gfp_t flags)\n{\n\tunsigned long redzone_start;\n\tunsigned long redzone_end;\n\n\tif (gfpflags_allow_blocking(flags))\n\t\tkasan_quarantine_reduce();\n\n\tif (unlikely(object == NULL))\n\t\treturn NULL;\n\n\tif (is_kfence_address(kasan_reset_tag(object)))\n\t\treturn (void *)object;\n\n\t \n\n\t \n\tif (IS_ENABLED(CONFIG_KASAN_GENERIC))\n\t\tkasan_poison_last_granule((void *)object, size);\n\n\t \n\tredzone_start = round_up((unsigned long)(object + size),\n\t\t\t\tKASAN_GRANULE_SIZE);\n\tredzone_end = round_up((unsigned long)(object + cache->object_size),\n\t\t\t\tKASAN_GRANULE_SIZE);\n\tkasan_poison((void *)redzone_start, redzone_end - redzone_start,\n\t\t\t   KASAN_SLAB_REDZONE, false);\n\n\t \n\tif (kasan_stack_collection_enabled() && is_kmalloc_cache(cache))\n\t\tkasan_save_alloc_info(cache, (void *)object, flags);\n\n\t \n\treturn (void *)object;\n}\n\nvoid * __must_check __kasan_kmalloc(struct kmem_cache *cache, const void *object,\n\t\t\t\t\tsize_t size, gfp_t flags)\n{\n\treturn ____kasan_kmalloc(cache, object, size, flags);\n}\nEXPORT_SYMBOL(__kasan_kmalloc);\n\nvoid * __must_check __kasan_kmalloc_large(const void *ptr, size_t size,\n\t\t\t\t\t\tgfp_t flags)\n{\n\tunsigned long redzone_start;\n\tunsigned long redzone_end;\n\n\tif (gfpflags_allow_blocking(flags))\n\t\tkasan_quarantine_reduce();\n\n\tif (unlikely(ptr == NULL))\n\t\treturn NULL;\n\n\t \n\n\t \n\tif (IS_ENABLED(CONFIG_KASAN_GENERIC))\n\t\tkasan_poison_last_granule(ptr, size);\n\n\t \n\tredzone_start = round_up((unsigned long)(ptr + size),\n\t\t\t\tKASAN_GRANULE_SIZE);\n\tredzone_end = (unsigned long)ptr + page_size(virt_to_page(ptr));\n\tkasan_poison((void *)redzone_start, redzone_end - redzone_start,\n\t\t     KASAN_PAGE_REDZONE, false);\n\n\treturn (void *)ptr;\n}\n\nvoid * __must_check __kasan_krealloc(const void *object, size_t size, gfp_t flags)\n{\n\tstruct slab *slab;\n\n\tif (unlikely(object == ZERO_SIZE_PTR))\n\t\treturn (void *)object;\n\n\t \n\tkasan_unpoison(object, size, false);\n\n\tslab = virt_to_slab(object);\n\n\t \n\tif (unlikely(!slab))\n\t\treturn __kasan_kmalloc_large(object, size, flags);\n\telse\n\t\treturn ____kasan_kmalloc(slab->slab_cache, object, size, flags);\n}\n\nbool __kasan_check_byte(const void *address, unsigned long ip)\n{\n\tif (!kasan_byte_accessible(address)) {\n\t\tkasan_report(address, 1, false, ip);\n\t\treturn false;\n\t}\n\treturn true;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}