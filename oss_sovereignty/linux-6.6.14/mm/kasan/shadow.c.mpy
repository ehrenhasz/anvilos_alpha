{
  "module_name": "shadow.c",
  "hash_id": "3f43c3b3f158cf01412afe1db3c7c2b649d0ca67b4acf9f9675ab63104e14d46",
  "original_prompt": "Ingested from linux-6.6.14/mm/kasan/shadow.c",
  "human_readable_source": "\n \n\n#include <linux/init.h>\n#include <linux/kasan.h>\n#include <linux/kernel.h>\n#include <linux/kfence.h>\n#include <linux/kmemleak.h>\n#include <linux/memory.h>\n#include <linux/mm.h>\n#include <linux/string.h>\n#include <linux/types.h>\n#include <linux/vmalloc.h>\n\n#include <asm/cacheflush.h>\n#include <asm/tlbflush.h>\n\n#include \"kasan.h\"\n\nbool __kasan_check_read(const volatile void *p, unsigned int size)\n{\n\treturn kasan_check_range((void *)p, size, false, _RET_IP_);\n}\nEXPORT_SYMBOL(__kasan_check_read);\n\nbool __kasan_check_write(const volatile void *p, unsigned int size)\n{\n\treturn kasan_check_range((void *)p, size, true, _RET_IP_);\n}\nEXPORT_SYMBOL(__kasan_check_write);\n\n#if !defined(CONFIG_CC_HAS_KASAN_MEMINTRINSIC_PREFIX) && !defined(CONFIG_GENERIC_ENTRY)\n \n#undef memset\nvoid *memset(void *addr, int c, size_t len)\n{\n\tif (!kasan_check_range(addr, len, true, _RET_IP_))\n\t\treturn NULL;\n\n\treturn __memset(addr, c, len);\n}\n\n#ifdef __HAVE_ARCH_MEMMOVE\n#undef memmove\nvoid *memmove(void *dest, const void *src, size_t len)\n{\n\tif (!kasan_check_range(src, len, false, _RET_IP_) ||\n\t    !kasan_check_range(dest, len, true, _RET_IP_))\n\t\treturn NULL;\n\n\treturn __memmove(dest, src, len);\n}\n#endif\n\n#undef memcpy\nvoid *memcpy(void *dest, const void *src, size_t len)\n{\n\tif (!kasan_check_range(src, len, false, _RET_IP_) ||\n\t    !kasan_check_range(dest, len, true, _RET_IP_))\n\t\treturn NULL;\n\n\treturn __memcpy(dest, src, len);\n}\n#endif\n\nvoid *__asan_memset(void *addr, int c, ssize_t len)\n{\n\tif (!kasan_check_range(addr, len, true, _RET_IP_))\n\t\treturn NULL;\n\n\treturn __memset(addr, c, len);\n}\nEXPORT_SYMBOL(__asan_memset);\n\n#ifdef __HAVE_ARCH_MEMMOVE\nvoid *__asan_memmove(void *dest, const void *src, ssize_t len)\n{\n\tif (!kasan_check_range(src, len, false, _RET_IP_) ||\n\t    !kasan_check_range(dest, len, true, _RET_IP_))\n\t\treturn NULL;\n\n\treturn __memmove(dest, src, len);\n}\nEXPORT_SYMBOL(__asan_memmove);\n#endif\n\nvoid *__asan_memcpy(void *dest, const void *src, ssize_t len)\n{\n\tif (!kasan_check_range(src, len, false, _RET_IP_) ||\n\t    !kasan_check_range(dest, len, true, _RET_IP_))\n\t\treturn NULL;\n\n\treturn __memcpy(dest, src, len);\n}\nEXPORT_SYMBOL(__asan_memcpy);\n\n#ifdef CONFIG_KASAN_SW_TAGS\nvoid *__hwasan_memset(void *addr, int c, ssize_t len) __alias(__asan_memset);\nEXPORT_SYMBOL(__hwasan_memset);\n#ifdef __HAVE_ARCH_MEMMOVE\nvoid *__hwasan_memmove(void *dest, const void *src, ssize_t len) __alias(__asan_memmove);\nEXPORT_SYMBOL(__hwasan_memmove);\n#endif\nvoid *__hwasan_memcpy(void *dest, const void *src, ssize_t len) __alias(__asan_memcpy);\nEXPORT_SYMBOL(__hwasan_memcpy);\n#endif\n\nvoid kasan_poison(const void *addr, size_t size, u8 value, bool init)\n{\n\tvoid *shadow_start, *shadow_end;\n\n\tif (!kasan_arch_is_ready())\n\t\treturn;\n\n\t \n\taddr = kasan_reset_tag(addr);\n\n\t \n\tif (is_kfence_address(addr))\n\t\treturn;\n\n\tif (WARN_ON((unsigned long)addr & KASAN_GRANULE_MASK))\n\t\treturn;\n\tif (WARN_ON(size & KASAN_GRANULE_MASK))\n\t\treturn;\n\n\tshadow_start = kasan_mem_to_shadow(addr);\n\tshadow_end = kasan_mem_to_shadow(addr + size);\n\n\t__memset(shadow_start, value, shadow_end - shadow_start);\n}\nEXPORT_SYMBOL(kasan_poison);\n\n#ifdef CONFIG_KASAN_GENERIC\nvoid kasan_poison_last_granule(const void *addr, size_t size)\n{\n\tif (!kasan_arch_is_ready())\n\t\treturn;\n\n\tif (size & KASAN_GRANULE_MASK) {\n\t\tu8 *shadow = (u8 *)kasan_mem_to_shadow(addr + size);\n\t\t*shadow = size & KASAN_GRANULE_MASK;\n\t}\n}\n#endif\n\nvoid kasan_unpoison(const void *addr, size_t size, bool init)\n{\n\tu8 tag = get_tag(addr);\n\n\t \n\taddr = kasan_reset_tag(addr);\n\n\t \n\tif (is_kfence_address(addr))\n\t\treturn;\n\n\tif (WARN_ON((unsigned long)addr & KASAN_GRANULE_MASK))\n\t\treturn;\n\n\t \n\tkasan_poison(addr, round_up(size, KASAN_GRANULE_SIZE), tag, false);\n\n\t \n\tif (IS_ENABLED(CONFIG_KASAN_GENERIC))\n\t\tkasan_poison_last_granule(addr, size);\n}\n\n#ifdef CONFIG_MEMORY_HOTPLUG\nstatic bool shadow_mapped(unsigned long addr)\n{\n\tpgd_t *pgd = pgd_offset_k(addr);\n\tp4d_t *p4d;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\tpte_t *pte;\n\n\tif (pgd_none(*pgd))\n\t\treturn false;\n\tp4d = p4d_offset(pgd, addr);\n\tif (p4d_none(*p4d))\n\t\treturn false;\n\tpud = pud_offset(p4d, addr);\n\tif (pud_none(*pud))\n\t\treturn false;\n\n\t \n\tif (pud_bad(*pud))\n\t\treturn true;\n\tpmd = pmd_offset(pud, addr);\n\tif (pmd_none(*pmd))\n\t\treturn false;\n\n\tif (pmd_bad(*pmd))\n\t\treturn true;\n\tpte = pte_offset_kernel(pmd, addr);\n\treturn !pte_none(ptep_get(pte));\n}\n\nstatic int __meminit kasan_mem_notifier(struct notifier_block *nb,\n\t\t\tunsigned long action, void *data)\n{\n\tstruct memory_notify *mem_data = data;\n\tunsigned long nr_shadow_pages, start_kaddr, shadow_start;\n\tunsigned long shadow_end, shadow_size;\n\n\tnr_shadow_pages = mem_data->nr_pages >> KASAN_SHADOW_SCALE_SHIFT;\n\tstart_kaddr = (unsigned long)pfn_to_kaddr(mem_data->start_pfn);\n\tshadow_start = (unsigned long)kasan_mem_to_shadow((void *)start_kaddr);\n\tshadow_size = nr_shadow_pages << PAGE_SHIFT;\n\tshadow_end = shadow_start + shadow_size;\n\n\tif (WARN_ON(mem_data->nr_pages % KASAN_GRANULE_SIZE) ||\n\t\tWARN_ON(start_kaddr % KASAN_MEMORY_PER_SHADOW_PAGE))\n\t\treturn NOTIFY_BAD;\n\n\tswitch (action) {\n\tcase MEM_GOING_ONLINE: {\n\t\tvoid *ret;\n\n\t\t \n\t\tif (shadow_mapped(shadow_start))\n\t\t\treturn NOTIFY_OK;\n\n\t\tret = __vmalloc_node_range(shadow_size, PAGE_SIZE, shadow_start,\n\t\t\t\t\tshadow_end, GFP_KERNEL,\n\t\t\t\t\tPAGE_KERNEL, VM_NO_GUARD,\n\t\t\t\t\tpfn_to_nid(mem_data->start_pfn),\n\t\t\t\t\t__builtin_return_address(0));\n\t\tif (!ret)\n\t\t\treturn NOTIFY_BAD;\n\n\t\tkmemleak_ignore(ret);\n\t\treturn NOTIFY_OK;\n\t}\n\tcase MEM_CANCEL_ONLINE:\n\tcase MEM_OFFLINE: {\n\t\tstruct vm_struct *vm;\n\n\t\t \n\t\tvm = find_vm_area((void *)shadow_start);\n\t\tif (vm)\n\t\t\tvfree((void *)shadow_start);\n\t}\n\t}\n\n\treturn NOTIFY_OK;\n}\n\nstatic int __init kasan_memhotplug_init(void)\n{\n\thotplug_memory_notifier(kasan_mem_notifier, DEFAULT_CALLBACK_PRI);\n\n\treturn 0;\n}\n\ncore_initcall(kasan_memhotplug_init);\n#endif\n\n#ifdef CONFIG_KASAN_VMALLOC\n\nvoid __init __weak kasan_populate_early_vm_area_shadow(void *start,\n\t\t\t\t\t\t       unsigned long size)\n{\n}\n\nstatic int kasan_populate_vmalloc_pte(pte_t *ptep, unsigned long addr,\n\t\t\t\t      void *unused)\n{\n\tunsigned long page;\n\tpte_t pte;\n\n\tif (likely(!pte_none(ptep_get(ptep))))\n\t\treturn 0;\n\n\tpage = __get_free_page(GFP_KERNEL);\n\tif (!page)\n\t\treturn -ENOMEM;\n\n\tmemset((void *)page, KASAN_VMALLOC_INVALID, PAGE_SIZE);\n\tpte = pfn_pte(PFN_DOWN(__pa(page)), PAGE_KERNEL);\n\n\tspin_lock(&init_mm.page_table_lock);\n\tif (likely(pte_none(ptep_get(ptep)))) {\n\t\tset_pte_at(&init_mm, addr, ptep, pte);\n\t\tpage = 0;\n\t}\n\tspin_unlock(&init_mm.page_table_lock);\n\tif (page)\n\t\tfree_page(page);\n\treturn 0;\n}\n\nint kasan_populate_vmalloc(unsigned long addr, unsigned long size)\n{\n\tunsigned long shadow_start, shadow_end;\n\tint ret;\n\n\tif (!kasan_arch_is_ready())\n\t\treturn 0;\n\n\tif (!is_vmalloc_or_module_addr((void *)addr))\n\t\treturn 0;\n\n\tshadow_start = (unsigned long)kasan_mem_to_shadow((void *)addr);\n\tshadow_end = (unsigned long)kasan_mem_to_shadow((void *)addr + size);\n\n\t \n\tif (IS_ENABLED(CONFIG_UML)) {\n\t\t__memset((void *)shadow_start, KASAN_VMALLOC_INVALID, shadow_end - shadow_start);\n\t\treturn 0;\n\t}\n\n\tshadow_start = PAGE_ALIGN_DOWN(shadow_start);\n\tshadow_end = PAGE_ALIGN(shadow_end);\n\n\tret = apply_to_page_range(&init_mm, shadow_start,\n\t\t\t\t  shadow_end - shadow_start,\n\t\t\t\t  kasan_populate_vmalloc_pte, NULL);\n\tif (ret)\n\t\treturn ret;\n\n\tflush_cache_vmap(shadow_start, shadow_end);\n\n\t \n\n\treturn 0;\n}\n\nstatic int kasan_depopulate_vmalloc_pte(pte_t *ptep, unsigned long addr,\n\t\t\t\t\tvoid *unused)\n{\n\tunsigned long page;\n\n\tpage = (unsigned long)__va(pte_pfn(ptep_get(ptep)) << PAGE_SHIFT);\n\n\tspin_lock(&init_mm.page_table_lock);\n\n\tif (likely(!pte_none(ptep_get(ptep)))) {\n\t\tpte_clear(&init_mm, addr, ptep);\n\t\tfree_page(page);\n\t}\n\tspin_unlock(&init_mm.page_table_lock);\n\n\treturn 0;\n}\n\n \nvoid kasan_release_vmalloc(unsigned long start, unsigned long end,\n\t\t\t   unsigned long free_region_start,\n\t\t\t   unsigned long free_region_end)\n{\n\tvoid *shadow_start, *shadow_end;\n\tunsigned long region_start, region_end;\n\tunsigned long size;\n\n\tif (!kasan_arch_is_ready())\n\t\treturn;\n\n\tregion_start = ALIGN(start, KASAN_MEMORY_PER_SHADOW_PAGE);\n\tregion_end = ALIGN_DOWN(end, KASAN_MEMORY_PER_SHADOW_PAGE);\n\n\tfree_region_start = ALIGN(free_region_start, KASAN_MEMORY_PER_SHADOW_PAGE);\n\n\tif (start != region_start &&\n\t    free_region_start < region_start)\n\t\tregion_start -= KASAN_MEMORY_PER_SHADOW_PAGE;\n\n\tfree_region_end = ALIGN_DOWN(free_region_end, KASAN_MEMORY_PER_SHADOW_PAGE);\n\n\tif (end != region_end &&\n\t    free_region_end > region_end)\n\t\tregion_end += KASAN_MEMORY_PER_SHADOW_PAGE;\n\n\tshadow_start = kasan_mem_to_shadow((void *)region_start);\n\tshadow_end = kasan_mem_to_shadow((void *)region_end);\n\n\tif (shadow_end > shadow_start) {\n\t\tsize = shadow_end - shadow_start;\n\t\tif (IS_ENABLED(CONFIG_UML)) {\n\t\t\t__memset(shadow_start, KASAN_SHADOW_INIT, shadow_end - shadow_start);\n\t\t\treturn;\n\t\t}\n\t\tapply_to_existing_page_range(&init_mm,\n\t\t\t\t\t     (unsigned long)shadow_start,\n\t\t\t\t\t     size, kasan_depopulate_vmalloc_pte,\n\t\t\t\t\t     NULL);\n\t\tflush_tlb_kernel_range((unsigned long)shadow_start,\n\t\t\t\t       (unsigned long)shadow_end);\n\t}\n}\n\nvoid *__kasan_unpoison_vmalloc(const void *start, unsigned long size,\n\t\t\t       kasan_vmalloc_flags_t flags)\n{\n\t \n\n\tif (!kasan_arch_is_ready())\n\t\treturn (void *)start;\n\n\tif (!is_vmalloc_or_module_addr(start))\n\t\treturn (void *)start;\n\n\t \n\tif (IS_ENABLED(CONFIG_KASAN_SW_TAGS) &&\n\t    !(flags & KASAN_VMALLOC_PROT_NORMAL))\n\t\treturn (void *)start;\n\n\tstart = set_tag(start, kasan_random_tag());\n\tkasan_unpoison(start, size, false);\n\treturn (void *)start;\n}\n\n \nvoid __kasan_poison_vmalloc(const void *start, unsigned long size)\n{\n\tif (!kasan_arch_is_ready())\n\t\treturn;\n\n\tif (!is_vmalloc_or_module_addr(start))\n\t\treturn;\n\n\tsize = round_up(size, KASAN_GRANULE_SIZE);\n\tkasan_poison(start, size, KASAN_VMALLOC_INVALID, false);\n}\n\n#else  \n\nint kasan_alloc_module_shadow(void *addr, size_t size, gfp_t gfp_mask)\n{\n\tvoid *ret;\n\tsize_t scaled_size;\n\tsize_t shadow_size;\n\tunsigned long shadow_start;\n\n\tshadow_start = (unsigned long)kasan_mem_to_shadow(addr);\n\tscaled_size = (size + KASAN_GRANULE_SIZE - 1) >>\n\t\t\t\tKASAN_SHADOW_SCALE_SHIFT;\n\tshadow_size = round_up(scaled_size, PAGE_SIZE);\n\n\tif (WARN_ON(!PAGE_ALIGNED(shadow_start)))\n\t\treturn -EINVAL;\n\n\tif (IS_ENABLED(CONFIG_UML)) {\n\t\t__memset((void *)shadow_start, KASAN_SHADOW_INIT, shadow_size);\n\t\treturn 0;\n\t}\n\n\tret = __vmalloc_node_range(shadow_size, 1, shadow_start,\n\t\t\tshadow_start + shadow_size,\n\t\t\tGFP_KERNEL,\n\t\t\tPAGE_KERNEL, VM_NO_GUARD, NUMA_NO_NODE,\n\t\t\t__builtin_return_address(0));\n\n\tif (ret) {\n\t\tstruct vm_struct *vm = find_vm_area(addr);\n\t\t__memset(ret, KASAN_SHADOW_INIT, shadow_size);\n\t\tvm->flags |= VM_KASAN;\n\t\tkmemleak_ignore(ret);\n\n\t\tif (vm->flags & VM_DEFER_KMEMLEAK)\n\t\t\tkmemleak_vmalloc(vm, size, gfp_mask);\n\n\t\treturn 0;\n\t}\n\n\treturn -ENOMEM;\n}\n\nvoid kasan_free_module_shadow(const struct vm_struct *vm)\n{\n\tif (IS_ENABLED(CONFIG_UML))\n\t\treturn;\n\n\tif (vm->flags & VM_KASAN)\n\t\tvfree(kasan_mem_to_shadow(vm->addr));\n}\n\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}