{
  "module_name": "memory_hotplug.c",
  "hash_id": "b70d76cc90f2352b925a7627a369657b6ef2b60fbf1aedc4d98f4997e0c13482",
  "original_prompt": "Ingested from linux-6.6.14/mm/memory_hotplug.c",
  "human_readable_source": "\n \n\n#include <linux/stddef.h>\n#include <linux/mm.h>\n#include <linux/sched/signal.h>\n#include <linux/swap.h>\n#include <linux/interrupt.h>\n#include <linux/pagemap.h>\n#include <linux/compiler.h>\n#include <linux/export.h>\n#include <linux/writeback.h>\n#include <linux/slab.h>\n#include <linux/sysctl.h>\n#include <linux/cpu.h>\n#include <linux/memory.h>\n#include <linux/memremap.h>\n#include <linux/memory_hotplug.h>\n#include <linux/vmalloc.h>\n#include <linux/ioport.h>\n#include <linux/delay.h>\n#include <linux/migrate.h>\n#include <linux/page-isolation.h>\n#include <linux/pfn.h>\n#include <linux/suspend.h>\n#include <linux/mm_inline.h>\n#include <linux/firmware-map.h>\n#include <linux/stop_machine.h>\n#include <linux/hugetlb.h>\n#include <linux/memblock.h>\n#include <linux/compaction.h>\n#include <linux/rmap.h>\n#include <linux/module.h>\n\n#include <asm/tlbflush.h>\n\n#include \"internal.h\"\n#include \"shuffle.h\"\n\nenum {\n\tMEMMAP_ON_MEMORY_DISABLE = 0,\n\tMEMMAP_ON_MEMORY_ENABLE,\n\tMEMMAP_ON_MEMORY_FORCE,\n};\n\nstatic int memmap_mode __read_mostly = MEMMAP_ON_MEMORY_DISABLE;\n\nstatic inline unsigned long memory_block_memmap_size(void)\n{\n\treturn PHYS_PFN(memory_block_size_bytes()) * sizeof(struct page);\n}\n\nstatic inline unsigned long memory_block_memmap_on_memory_pages(void)\n{\n\tunsigned long nr_pages = PFN_UP(memory_block_memmap_size());\n\n\t \n\tif (memmap_mode == MEMMAP_ON_MEMORY_FORCE)\n\t\treturn pageblock_align(nr_pages);\n\treturn nr_pages;\n}\n\n#ifdef CONFIG_MHP_MEMMAP_ON_MEMORY\n \nstatic int set_memmap_mode(const char *val, const struct kernel_param *kp)\n{\n\tint ret, mode;\n\tbool enabled;\n\n\tif (sysfs_streq(val, \"force\") ||  sysfs_streq(val, \"FORCE\")) {\n\t\tmode = MEMMAP_ON_MEMORY_FORCE;\n\t} else {\n\t\tret = kstrtobool(val, &enabled);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\tif (enabled)\n\t\t\tmode = MEMMAP_ON_MEMORY_ENABLE;\n\t\telse\n\t\t\tmode = MEMMAP_ON_MEMORY_DISABLE;\n\t}\n\t*((int *)kp->arg) = mode;\n\tif (mode == MEMMAP_ON_MEMORY_FORCE) {\n\t\tunsigned long memmap_pages = memory_block_memmap_on_memory_pages();\n\n\t\tpr_info_once(\"Memory hotplug will waste %ld pages in each memory block\\n\",\n\t\t\t     memmap_pages - PFN_UP(memory_block_memmap_size()));\n\t}\n\treturn 0;\n}\n\nstatic int get_memmap_mode(char *buffer, const struct kernel_param *kp)\n{\n\tint mode = *((int *)kp->arg);\n\n\tif (mode == MEMMAP_ON_MEMORY_FORCE)\n\t\treturn sprintf(buffer, \"force\\n\");\n\treturn sprintf(buffer, \"%c\\n\", mode ? 'Y' : 'N');\n}\n\nstatic const struct kernel_param_ops memmap_mode_ops = {\n\t.set = set_memmap_mode,\n\t.get = get_memmap_mode,\n};\nmodule_param_cb(memmap_on_memory, &memmap_mode_ops, &memmap_mode, 0444);\nMODULE_PARM_DESC(memmap_on_memory, \"Enable memmap on memory for memory hotplug\\n\"\n\t\t \"With value \\\"force\\\" it could result in memory wastage due \"\n\t\t \"to memmap size limitations (Y/N/force)\");\n\nstatic inline bool mhp_memmap_on_memory(void)\n{\n\treturn memmap_mode != MEMMAP_ON_MEMORY_DISABLE;\n}\n#else\nstatic inline bool mhp_memmap_on_memory(void)\n{\n\treturn false;\n}\n#endif\n\nenum {\n\tONLINE_POLICY_CONTIG_ZONES = 0,\n\tONLINE_POLICY_AUTO_MOVABLE,\n};\n\nstatic const char * const online_policy_to_str[] = {\n\t[ONLINE_POLICY_CONTIG_ZONES] = \"contig-zones\",\n\t[ONLINE_POLICY_AUTO_MOVABLE] = \"auto-movable\",\n};\n\nstatic int set_online_policy(const char *val, const struct kernel_param *kp)\n{\n\tint ret = sysfs_match_string(online_policy_to_str, val);\n\n\tif (ret < 0)\n\t\treturn ret;\n\t*((int *)kp->arg) = ret;\n\treturn 0;\n}\n\nstatic int get_online_policy(char *buffer, const struct kernel_param *kp)\n{\n\treturn sprintf(buffer, \"%s\\n\", online_policy_to_str[*((int *)kp->arg)]);\n}\n\n \nstatic int online_policy __read_mostly = ONLINE_POLICY_CONTIG_ZONES;\nstatic const struct kernel_param_ops online_policy_ops = {\n\t.set = set_online_policy,\n\t.get = get_online_policy,\n};\nmodule_param_cb(online_policy, &online_policy_ops, &online_policy, 0644);\nMODULE_PARM_DESC(online_policy,\n\t\t\"Set the online policy (\\\"contig-zones\\\", \\\"auto-movable\\\") \"\n\t\t\"Default: \\\"contig-zones\\\"\");\n\n \nstatic unsigned int auto_movable_ratio __read_mostly = 301;\nmodule_param(auto_movable_ratio, uint, 0644);\nMODULE_PARM_DESC(auto_movable_ratio,\n\t\t\"Set the maximum ratio of MOVABLE:KERNEL memory in the system \"\n\t\t\"in percent for \\\"auto-movable\\\" online policy. Default: 301\");\n\n \n#ifdef CONFIG_NUMA\nstatic bool auto_movable_numa_aware __read_mostly = true;\nmodule_param(auto_movable_numa_aware, bool, 0644);\nMODULE_PARM_DESC(auto_movable_numa_aware,\n\t\t\"Consider numa node stats in addition to global stats in \"\n\t\t\"\\\"auto-movable\\\" online policy. Default: true\");\n#endif  \n\n \n\nstatic online_page_callback_t online_page_callback = generic_online_page;\nstatic DEFINE_MUTEX(online_page_callback_lock);\n\nDEFINE_STATIC_PERCPU_RWSEM(mem_hotplug_lock);\n\nvoid get_online_mems(void)\n{\n\tpercpu_down_read(&mem_hotplug_lock);\n}\n\nvoid put_online_mems(void)\n{\n\tpercpu_up_read(&mem_hotplug_lock);\n}\n\nbool movable_node_enabled = false;\n\n#ifndef CONFIG_MEMORY_HOTPLUG_DEFAULT_ONLINE\nint mhp_default_online_type = MMOP_OFFLINE;\n#else\nint mhp_default_online_type = MMOP_ONLINE;\n#endif\n\nstatic int __init setup_memhp_default_state(char *str)\n{\n\tconst int online_type = mhp_online_type_from_str(str);\n\n\tif (online_type >= 0)\n\t\tmhp_default_online_type = online_type;\n\n\treturn 1;\n}\n__setup(\"memhp_default_state=\", setup_memhp_default_state);\n\nvoid mem_hotplug_begin(void)\n{\n\tcpus_read_lock();\n\tpercpu_down_write(&mem_hotplug_lock);\n}\n\nvoid mem_hotplug_done(void)\n{\n\tpercpu_up_write(&mem_hotplug_lock);\n\tcpus_read_unlock();\n}\n\nu64 max_mem_size = U64_MAX;\n\n \nstatic struct resource *register_memory_resource(u64 start, u64 size,\n\t\t\t\t\t\t const char *resource_name)\n{\n\tstruct resource *res;\n\tunsigned long flags =  IORESOURCE_SYSTEM_RAM | IORESOURCE_BUSY;\n\n\tif (strcmp(resource_name, \"System RAM\"))\n\t\tflags |= IORESOURCE_SYSRAM_DRIVER_MANAGED;\n\n\tif (!mhp_range_allowed(start, size, true))\n\t\treturn ERR_PTR(-E2BIG);\n\n\t \n\tif (start + size > max_mem_size && system_state < SYSTEM_RUNNING)\n\t\treturn ERR_PTR(-E2BIG);\n\n\t \n\tres = __request_region(&iomem_resource, start, size,\n\t\t\t       resource_name, flags);\n\n\tif (!res) {\n\t\tpr_debug(\"Unable to reserve System RAM region: %016llx->%016llx\\n\",\n\t\t\t\tstart, start + size);\n\t\treturn ERR_PTR(-EEXIST);\n\t}\n\treturn res;\n}\n\nstatic void release_memory_resource(struct resource *res)\n{\n\tif (!res)\n\t\treturn;\n\trelease_resource(res);\n\tkfree(res);\n}\n\nstatic int check_pfn_span(unsigned long pfn, unsigned long nr_pages)\n{\n\t \n\tunsigned long min_align;\n\n\tif (IS_ENABLED(CONFIG_SPARSEMEM_VMEMMAP))\n\t\tmin_align = PAGES_PER_SUBSECTION;\n\telse\n\t\tmin_align = PAGES_PER_SECTION;\n\tif (!IS_ALIGNED(pfn | nr_pages, min_align))\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\n \nstruct page *pfn_to_online_page(unsigned long pfn)\n{\n\tunsigned long nr = pfn_to_section_nr(pfn);\n\tstruct dev_pagemap *pgmap;\n\tstruct mem_section *ms;\n\n\tif (nr >= NR_MEM_SECTIONS)\n\t\treturn NULL;\n\n\tms = __nr_to_section(nr);\n\tif (!online_section(ms))\n\t\treturn NULL;\n\n\t \n\tif (IS_ENABLED(CONFIG_HAVE_ARCH_PFN_VALID) && !pfn_valid(pfn))\n\t\treturn NULL;\n\n\tif (!pfn_section_valid(ms, pfn))\n\t\treturn NULL;\n\n\tif (!online_device_section(ms))\n\t\treturn pfn_to_page(pfn);\n\n\t \n\tpgmap = get_dev_pagemap(pfn, NULL);\n\tput_dev_pagemap(pgmap);\n\n\t \n\tif (pgmap)\n\t\treturn NULL;\n\n\treturn pfn_to_page(pfn);\n}\nEXPORT_SYMBOL_GPL(pfn_to_online_page);\n\nint __ref __add_pages(int nid, unsigned long pfn, unsigned long nr_pages,\n\t\tstruct mhp_params *params)\n{\n\tconst unsigned long end_pfn = pfn + nr_pages;\n\tunsigned long cur_nr_pages;\n\tint err;\n\tstruct vmem_altmap *altmap = params->altmap;\n\n\tif (WARN_ON_ONCE(!pgprot_val(params->pgprot)))\n\t\treturn -EINVAL;\n\n\tVM_BUG_ON(!mhp_range_allowed(PFN_PHYS(pfn), nr_pages * PAGE_SIZE, false));\n\n\tif (altmap) {\n\t\t \n\t\tif (altmap->base_pfn != pfn\n\t\t\t\t|| vmem_altmap_offset(altmap) > nr_pages) {\n\t\t\tpr_warn_once(\"memory add fail, invalid altmap\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\taltmap->alloc = 0;\n\t}\n\n\tif (check_pfn_span(pfn, nr_pages)) {\n\t\tWARN(1, \"Misaligned %s start: %#lx end: %#lx\\n\", __func__, pfn, pfn + nr_pages - 1);\n\t\treturn -EINVAL;\n\t}\n\n\tfor (; pfn < end_pfn; pfn += cur_nr_pages) {\n\t\t \n\t\tcur_nr_pages = min(end_pfn - pfn,\n\t\t\t\t   SECTION_ALIGN_UP(pfn + 1) - pfn);\n\t\terr = sparse_add_section(nid, pfn, cur_nr_pages, altmap,\n\t\t\t\t\t params->pgmap);\n\t\tif (err)\n\t\t\tbreak;\n\t\tcond_resched();\n\t}\n\tvmemmap_populate_print_last();\n\treturn err;\n}\n\n \nstatic unsigned long find_smallest_section_pfn(int nid, struct zone *zone,\n\t\t\t\t     unsigned long start_pfn,\n\t\t\t\t     unsigned long end_pfn)\n{\n\tfor (; start_pfn < end_pfn; start_pfn += PAGES_PER_SUBSECTION) {\n\t\tif (unlikely(!pfn_to_online_page(start_pfn)))\n\t\t\tcontinue;\n\n\t\tif (unlikely(pfn_to_nid(start_pfn) != nid))\n\t\t\tcontinue;\n\n\t\tif (zone != page_zone(pfn_to_page(start_pfn)))\n\t\t\tcontinue;\n\n\t\treturn start_pfn;\n\t}\n\n\treturn 0;\n}\n\n \nstatic unsigned long find_biggest_section_pfn(int nid, struct zone *zone,\n\t\t\t\t    unsigned long start_pfn,\n\t\t\t\t    unsigned long end_pfn)\n{\n\tunsigned long pfn;\n\n\t \n\tpfn = end_pfn - 1;\n\tfor (; pfn >= start_pfn; pfn -= PAGES_PER_SUBSECTION) {\n\t\tif (unlikely(!pfn_to_online_page(pfn)))\n\t\t\tcontinue;\n\n\t\tif (unlikely(pfn_to_nid(pfn) != nid))\n\t\t\tcontinue;\n\n\t\tif (zone != page_zone(pfn_to_page(pfn)))\n\t\t\tcontinue;\n\n\t\treturn pfn;\n\t}\n\n\treturn 0;\n}\n\nstatic void shrink_zone_span(struct zone *zone, unsigned long start_pfn,\n\t\t\t     unsigned long end_pfn)\n{\n\tunsigned long pfn;\n\tint nid = zone_to_nid(zone);\n\n\tif (zone->zone_start_pfn == start_pfn) {\n\t\t \n\t\tpfn = find_smallest_section_pfn(nid, zone, end_pfn,\n\t\t\t\t\t\tzone_end_pfn(zone));\n\t\tif (pfn) {\n\t\t\tzone->spanned_pages = zone_end_pfn(zone) - pfn;\n\t\t\tzone->zone_start_pfn = pfn;\n\t\t} else {\n\t\t\tzone->zone_start_pfn = 0;\n\t\t\tzone->spanned_pages = 0;\n\t\t}\n\t} else if (zone_end_pfn(zone) == end_pfn) {\n\t\t \n\t\tpfn = find_biggest_section_pfn(nid, zone, zone->zone_start_pfn,\n\t\t\t\t\t       start_pfn);\n\t\tif (pfn)\n\t\t\tzone->spanned_pages = pfn - zone->zone_start_pfn + 1;\n\t\telse {\n\t\t\tzone->zone_start_pfn = 0;\n\t\t\tzone->spanned_pages = 0;\n\t\t}\n\t}\n}\n\nstatic void update_pgdat_span(struct pglist_data *pgdat)\n{\n\tunsigned long node_start_pfn = 0, node_end_pfn = 0;\n\tstruct zone *zone;\n\n\tfor (zone = pgdat->node_zones;\n\t     zone < pgdat->node_zones + MAX_NR_ZONES; zone++) {\n\t\tunsigned long end_pfn = zone_end_pfn(zone);\n\n\t\t \n\t\tif (!zone->spanned_pages)\n\t\t\tcontinue;\n\t\tif (!node_end_pfn) {\n\t\t\tnode_start_pfn = zone->zone_start_pfn;\n\t\t\tnode_end_pfn = end_pfn;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (end_pfn > node_end_pfn)\n\t\t\tnode_end_pfn = end_pfn;\n\t\tif (zone->zone_start_pfn < node_start_pfn)\n\t\t\tnode_start_pfn = zone->zone_start_pfn;\n\t}\n\n\tpgdat->node_start_pfn = node_start_pfn;\n\tpgdat->node_spanned_pages = node_end_pfn - node_start_pfn;\n}\n\nvoid __ref remove_pfn_range_from_zone(struct zone *zone,\n\t\t\t\t      unsigned long start_pfn,\n\t\t\t\t      unsigned long nr_pages)\n{\n\tconst unsigned long end_pfn = start_pfn + nr_pages;\n\tstruct pglist_data *pgdat = zone->zone_pgdat;\n\tunsigned long pfn, cur_nr_pages;\n\n\t \n\tfor (pfn = start_pfn; pfn < end_pfn; pfn += cur_nr_pages) {\n\t\tcond_resched();\n\n\t\t \n\t\tcur_nr_pages =\n\t\t\tmin(end_pfn - pfn, SECTION_ALIGN_UP(pfn + 1) - pfn);\n\t\tpage_init_poison(pfn_to_page(pfn),\n\t\t\t\t sizeof(struct page) * cur_nr_pages);\n\t}\n\n\t \n\tif (zone_is_zone_device(zone))\n\t\treturn;\n\n\tclear_zone_contiguous(zone);\n\n\tshrink_zone_span(zone, start_pfn, start_pfn + nr_pages);\n\tupdate_pgdat_span(pgdat);\n\n\tset_zone_contiguous(zone);\n}\n\n \nvoid __remove_pages(unsigned long pfn, unsigned long nr_pages,\n\t\t    struct vmem_altmap *altmap)\n{\n\tconst unsigned long end_pfn = pfn + nr_pages;\n\tunsigned long cur_nr_pages;\n\n\tif (check_pfn_span(pfn, nr_pages)) {\n\t\tWARN(1, \"Misaligned %s start: %#lx end: %#lx\\n\", __func__, pfn, pfn + nr_pages - 1);\n\t\treturn;\n\t}\n\n\tfor (; pfn < end_pfn; pfn += cur_nr_pages) {\n\t\tcond_resched();\n\t\t \n\t\tcur_nr_pages = min(end_pfn - pfn,\n\t\t\t\t   SECTION_ALIGN_UP(pfn + 1) - pfn);\n\t\tsparse_remove_section(pfn, cur_nr_pages, altmap);\n\t}\n}\n\nint set_online_page_callback(online_page_callback_t callback)\n{\n\tint rc = -EINVAL;\n\n\tget_online_mems();\n\tmutex_lock(&online_page_callback_lock);\n\n\tif (online_page_callback == generic_online_page) {\n\t\tonline_page_callback = callback;\n\t\trc = 0;\n\t}\n\n\tmutex_unlock(&online_page_callback_lock);\n\tput_online_mems();\n\n\treturn rc;\n}\nEXPORT_SYMBOL_GPL(set_online_page_callback);\n\nint restore_online_page_callback(online_page_callback_t callback)\n{\n\tint rc = -EINVAL;\n\n\tget_online_mems();\n\tmutex_lock(&online_page_callback_lock);\n\n\tif (online_page_callback == callback) {\n\t\tonline_page_callback = generic_online_page;\n\t\trc = 0;\n\t}\n\n\tmutex_unlock(&online_page_callback_lock);\n\tput_online_mems();\n\n\treturn rc;\n}\nEXPORT_SYMBOL_GPL(restore_online_page_callback);\n\nvoid generic_online_page(struct page *page, unsigned int order)\n{\n\t \n\tdebug_pagealloc_map_pages(page, 1 << order);\n\t__free_pages_core(page, order);\n\ttotalram_pages_add(1UL << order);\n}\nEXPORT_SYMBOL_GPL(generic_online_page);\n\nstatic void online_pages_range(unsigned long start_pfn, unsigned long nr_pages)\n{\n\tconst unsigned long end_pfn = start_pfn + nr_pages;\n\tunsigned long pfn;\n\n\t \n\tfor (pfn = start_pfn; pfn < end_pfn;) {\n\t\tint order;\n\n\t\t \n\t\tif (pfn)\n\t\t\torder = min_t(int, MAX_ORDER, __ffs(pfn));\n\t\telse\n\t\t\torder = MAX_ORDER;\n\n\t\t(*online_page_callback)(pfn_to_page(pfn), order);\n\t\tpfn += (1UL << order);\n\t}\n\n\t \n\tonline_mem_sections(start_pfn, end_pfn);\n}\n\n \nstatic void node_states_check_changes_online(unsigned long nr_pages,\n\tstruct zone *zone, struct memory_notify *arg)\n{\n\tint nid = zone_to_nid(zone);\n\n\targ->status_change_nid = NUMA_NO_NODE;\n\targ->status_change_nid_normal = NUMA_NO_NODE;\n\n\tif (!node_state(nid, N_MEMORY))\n\t\targ->status_change_nid = nid;\n\tif (zone_idx(zone) <= ZONE_NORMAL && !node_state(nid, N_NORMAL_MEMORY))\n\t\targ->status_change_nid_normal = nid;\n}\n\nstatic void node_states_set_node(int node, struct memory_notify *arg)\n{\n\tif (arg->status_change_nid_normal >= 0)\n\t\tnode_set_state(node, N_NORMAL_MEMORY);\n\n\tif (arg->status_change_nid >= 0)\n\t\tnode_set_state(node, N_MEMORY);\n}\n\nstatic void __meminit resize_zone_range(struct zone *zone, unsigned long start_pfn,\n\t\tunsigned long nr_pages)\n{\n\tunsigned long old_end_pfn = zone_end_pfn(zone);\n\n\tif (zone_is_empty(zone) || start_pfn < zone->zone_start_pfn)\n\t\tzone->zone_start_pfn = start_pfn;\n\n\tzone->spanned_pages = max(start_pfn + nr_pages, old_end_pfn) - zone->zone_start_pfn;\n}\n\nstatic void __meminit resize_pgdat_range(struct pglist_data *pgdat, unsigned long start_pfn,\n                                     unsigned long nr_pages)\n{\n\tunsigned long old_end_pfn = pgdat_end_pfn(pgdat);\n\n\tif (!pgdat->node_spanned_pages || start_pfn < pgdat->node_start_pfn)\n\t\tpgdat->node_start_pfn = start_pfn;\n\n\tpgdat->node_spanned_pages = max(start_pfn + nr_pages, old_end_pfn) - pgdat->node_start_pfn;\n\n}\n\n#ifdef CONFIG_ZONE_DEVICE\nstatic void section_taint_zone_device(unsigned long pfn)\n{\n\tstruct mem_section *ms = __pfn_to_section(pfn);\n\n\tms->section_mem_map |= SECTION_TAINT_ZONE_DEVICE;\n}\n#else\nstatic inline void section_taint_zone_device(unsigned long pfn)\n{\n}\n#endif\n\n \nvoid __ref move_pfn_range_to_zone(struct zone *zone, unsigned long start_pfn,\n\t\t\t\t  unsigned long nr_pages,\n\t\t\t\t  struct vmem_altmap *altmap, int migratetype)\n{\n\tstruct pglist_data *pgdat = zone->zone_pgdat;\n\tint nid = pgdat->node_id;\n\n\tclear_zone_contiguous(zone);\n\n\tif (zone_is_empty(zone))\n\t\tinit_currently_empty_zone(zone, start_pfn, nr_pages);\n\tresize_zone_range(zone, start_pfn, nr_pages);\n\tresize_pgdat_range(pgdat, start_pfn, nr_pages);\n\n\t \n\tif (zone_is_zone_device(zone)) {\n\t\tif (!IS_ALIGNED(start_pfn, PAGES_PER_SECTION))\n\t\t\tsection_taint_zone_device(start_pfn);\n\t\tif (!IS_ALIGNED(start_pfn + nr_pages, PAGES_PER_SECTION))\n\t\t\tsection_taint_zone_device(start_pfn + nr_pages);\n\t}\n\n\t \n\tmemmap_init_range(nr_pages, nid, zone_idx(zone), start_pfn, 0,\n\t\t\t MEMINIT_HOTPLUG, altmap, migratetype);\n\n\tset_zone_contiguous(zone);\n}\n\nstruct auto_movable_stats {\n\tunsigned long kernel_early_pages;\n\tunsigned long movable_pages;\n};\n\nstatic void auto_movable_stats_account_zone(struct auto_movable_stats *stats,\n\t\t\t\t\t    struct zone *zone)\n{\n\tif (zone_idx(zone) == ZONE_MOVABLE) {\n\t\tstats->movable_pages += zone->present_pages;\n\t} else {\n\t\tstats->kernel_early_pages += zone->present_early_pages;\n#ifdef CONFIG_CMA\n\t\t \n\t\tstats->movable_pages += zone->cma_pages;\n\t\tstats->kernel_early_pages -= zone->cma_pages;\n#endif  \n\t}\n}\nstruct auto_movable_group_stats {\n\tunsigned long movable_pages;\n\tunsigned long req_kernel_early_pages;\n};\n\nstatic int auto_movable_stats_account_group(struct memory_group *group,\n\t\t\t\t\t   void *arg)\n{\n\tconst int ratio = READ_ONCE(auto_movable_ratio);\n\tstruct auto_movable_group_stats *stats = arg;\n\tlong pages;\n\n\t \n\tif (!ratio)\n\t\treturn 0;\n\n\t \n\tpages = group->present_movable_pages * 100 / ratio;\n\tpages -= group->present_kernel_pages;\n\n\tif (pages > 0)\n\t\tstats->req_kernel_early_pages += pages;\n\tstats->movable_pages += group->present_movable_pages;\n\treturn 0;\n}\n\nstatic bool auto_movable_can_online_movable(int nid, struct memory_group *group,\n\t\t\t\t\t    unsigned long nr_pages)\n{\n\tunsigned long kernel_early_pages, movable_pages;\n\tstruct auto_movable_group_stats group_stats = {};\n\tstruct auto_movable_stats stats = {};\n\tpg_data_t *pgdat = NODE_DATA(nid);\n\tstruct zone *zone;\n\tint i;\n\n\t \n\tif (nid == NUMA_NO_NODE) {\n\t\t \n\t\tfor_each_populated_zone(zone)\n\t\t\tauto_movable_stats_account_zone(&stats, zone);\n\t} else {\n\t\tfor (i = 0; i < MAX_NR_ZONES; i++) {\n\t\t\tzone = pgdat->node_zones + i;\n\t\t\tif (populated_zone(zone))\n\t\t\t\tauto_movable_stats_account_zone(&stats, zone);\n\t\t}\n\t}\n\n\tkernel_early_pages = stats.kernel_early_pages;\n\tmovable_pages = stats.movable_pages;\n\n\t \n\twalk_dynamic_memory_groups(nid, auto_movable_stats_account_group,\n\t\t\t\t   group, &group_stats);\n\tif (kernel_early_pages <= group_stats.req_kernel_early_pages)\n\t\treturn false;\n\tkernel_early_pages -= group_stats.req_kernel_early_pages;\n\tmovable_pages -= group_stats.movable_pages;\n\n\tif (group && group->is_dynamic)\n\t\tkernel_early_pages += group->present_kernel_pages;\n\n\t \n\tmovable_pages += nr_pages;\n\treturn movable_pages <= (auto_movable_ratio * kernel_early_pages) / 100;\n}\n\n \nstatic struct zone *default_kernel_zone_for_pfn(int nid, unsigned long start_pfn,\n\t\tunsigned long nr_pages)\n{\n\tstruct pglist_data *pgdat = NODE_DATA(nid);\n\tint zid;\n\n\tfor (zid = 0; zid < ZONE_NORMAL; zid++) {\n\t\tstruct zone *zone = &pgdat->node_zones[zid];\n\n\t\tif (zone_intersects(zone, start_pfn, nr_pages))\n\t\t\treturn zone;\n\t}\n\n\treturn &pgdat->node_zones[ZONE_NORMAL];\n}\n\n \nstatic struct zone *auto_movable_zone_for_pfn(int nid,\n\t\t\t\t\t      struct memory_group *group,\n\t\t\t\t\t      unsigned long pfn,\n\t\t\t\t\t      unsigned long nr_pages)\n{\n\tunsigned long online_pages = 0, max_pages, end_pfn;\n\tstruct page *page;\n\n\tif (!auto_movable_ratio)\n\t\tgoto kernel_zone;\n\n\tif (group && !group->is_dynamic) {\n\t\tmax_pages = group->s.max_pages;\n\t\tonline_pages = group->present_movable_pages;\n\n\t\t \n\t\tif (group->present_kernel_pages)\n\t\t\tgoto kernel_zone;\n\t} else if (!group || group->d.unit_pages == nr_pages) {\n\t\tmax_pages = nr_pages;\n\t} else {\n\t\tmax_pages = group->d.unit_pages;\n\t\t \n\t\tpfn = ALIGN_DOWN(pfn, group->d.unit_pages);\n\t\tend_pfn = pfn + group->d.unit_pages;\n\t\tfor (; pfn < end_pfn; pfn += PAGES_PER_SECTION) {\n\t\t\tpage = pfn_to_online_page(pfn);\n\t\t\tif (!page)\n\t\t\t\tcontinue;\n\t\t\t \n\t\t\tif (!is_zone_movable_page(page))\n\t\t\t\tgoto kernel_zone;\n\t\t\tonline_pages += PAGES_PER_SECTION;\n\t\t}\n\t}\n\n\t \n\tnr_pages = max_pages - online_pages;\n\tif (!auto_movable_can_online_movable(NUMA_NO_NODE, group, nr_pages))\n\t\tgoto kernel_zone;\n\n#ifdef CONFIG_NUMA\n\tif (auto_movable_numa_aware &&\n\t    !auto_movable_can_online_movable(nid, group, nr_pages))\n\t\tgoto kernel_zone;\n#endif  \n\n\treturn &NODE_DATA(nid)->node_zones[ZONE_MOVABLE];\nkernel_zone:\n\treturn default_kernel_zone_for_pfn(nid, pfn, nr_pages);\n}\n\nstatic inline struct zone *default_zone_for_pfn(int nid, unsigned long start_pfn,\n\t\tunsigned long nr_pages)\n{\n\tstruct zone *kernel_zone = default_kernel_zone_for_pfn(nid, start_pfn,\n\t\t\tnr_pages);\n\tstruct zone *movable_zone = &NODE_DATA(nid)->node_zones[ZONE_MOVABLE];\n\tbool in_kernel = zone_intersects(kernel_zone, start_pfn, nr_pages);\n\tbool in_movable = zone_intersects(movable_zone, start_pfn, nr_pages);\n\n\t \n\tif (in_kernel ^ in_movable)\n\t\treturn (in_kernel) ? kernel_zone : movable_zone;\n\n\t \n\treturn movable_node_enabled ? movable_zone : kernel_zone;\n}\n\nstruct zone *zone_for_pfn_range(int online_type, int nid,\n\t\tstruct memory_group *group, unsigned long start_pfn,\n\t\tunsigned long nr_pages)\n{\n\tif (online_type == MMOP_ONLINE_KERNEL)\n\t\treturn default_kernel_zone_for_pfn(nid, start_pfn, nr_pages);\n\n\tif (online_type == MMOP_ONLINE_MOVABLE)\n\t\treturn &NODE_DATA(nid)->node_zones[ZONE_MOVABLE];\n\n\tif (online_policy == ONLINE_POLICY_AUTO_MOVABLE)\n\t\treturn auto_movable_zone_for_pfn(nid, group, start_pfn, nr_pages);\n\n\treturn default_zone_for_pfn(nid, start_pfn, nr_pages);\n}\n\n \nvoid adjust_present_page_count(struct page *page, struct memory_group *group,\n\t\t\t       long nr_pages)\n{\n\tstruct zone *zone = page_zone(page);\n\tconst bool movable = zone_idx(zone) == ZONE_MOVABLE;\n\n\t \n\tif (early_section(__pfn_to_section(page_to_pfn(page))))\n\t\tzone->present_early_pages += nr_pages;\n\tzone->present_pages += nr_pages;\n\tzone->zone_pgdat->node_present_pages += nr_pages;\n\n\tif (group && movable)\n\t\tgroup->present_movable_pages += nr_pages;\n\telse if (group && !movable)\n\t\tgroup->present_kernel_pages += nr_pages;\n}\n\nint mhp_init_memmap_on_memory(unsigned long pfn, unsigned long nr_pages,\n\t\t\t      struct zone *zone)\n{\n\tunsigned long end_pfn = pfn + nr_pages;\n\tint ret, i;\n\n\tret = kasan_add_zero_shadow(__va(PFN_PHYS(pfn)), PFN_PHYS(nr_pages));\n\tif (ret)\n\t\treturn ret;\n\n\tmove_pfn_range_to_zone(zone, pfn, nr_pages, NULL, MIGRATE_UNMOVABLE);\n\n\tfor (i = 0; i < nr_pages; i++)\n\t\tSetPageVmemmapSelfHosted(pfn_to_page(pfn + i));\n\n\t \n\tif (nr_pages >= PAGES_PER_SECTION)\n\t        online_mem_sections(pfn, ALIGN_DOWN(end_pfn, PAGES_PER_SECTION));\n\n\treturn ret;\n}\n\nvoid mhp_deinit_memmap_on_memory(unsigned long pfn, unsigned long nr_pages)\n{\n\tunsigned long end_pfn = pfn + nr_pages;\n\n\t \n\tif (nr_pages >= PAGES_PER_SECTION)\n\t\toffline_mem_sections(pfn, ALIGN_DOWN(end_pfn, PAGES_PER_SECTION));\n\n         \n\tremove_pfn_range_from_zone(page_zone(pfn_to_page(pfn)), pfn, nr_pages);\n\tkasan_remove_zero_shadow(__va(PFN_PHYS(pfn)), PFN_PHYS(nr_pages));\n}\n\n \nint __ref online_pages(unsigned long pfn, unsigned long nr_pages,\n\t\t       struct zone *zone, struct memory_group *group)\n{\n\tunsigned long flags;\n\tint need_zonelists_rebuild = 0;\n\tconst int nid = zone_to_nid(zone);\n\tint ret;\n\tstruct memory_notify arg;\n\n\t \n\tif (WARN_ON_ONCE(!nr_pages || !pageblock_aligned(pfn) ||\n\t\t\t !IS_ALIGNED(pfn + nr_pages, PAGES_PER_SECTION)))\n\t\treturn -EINVAL;\n\n\n\t \n\tmove_pfn_range_to_zone(zone, pfn, nr_pages, NULL, MIGRATE_ISOLATE);\n\n\targ.start_pfn = pfn;\n\targ.nr_pages = nr_pages;\n\tnode_states_check_changes_online(nr_pages, zone, &arg);\n\n\tret = memory_notify(MEM_GOING_ONLINE, &arg);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\tgoto failed_addition;\n\n\t \n\tspin_lock_irqsave(&zone->lock, flags);\n\tzone->nr_isolate_pageblock += nr_pages / pageblock_nr_pages;\n\tspin_unlock_irqrestore(&zone->lock, flags);\n\n\t \n\tif (!populated_zone(zone)) {\n\t\tneed_zonelists_rebuild = 1;\n\t\tsetup_zone_pageset(zone);\n\t}\n\n\tonline_pages_range(pfn, nr_pages);\n\tadjust_present_page_count(pfn_to_page(pfn), group, nr_pages);\n\n\tnode_states_set_node(nid, &arg);\n\tif (need_zonelists_rebuild)\n\t\tbuild_all_zonelists(NULL);\n\n\t \n\tundo_isolate_page_range(pfn, pfn + nr_pages, MIGRATE_MOVABLE);\n\n\t \n\tshuffle_zone(zone);\n\n\t \n\tinit_per_zone_wmark_min();\n\n\tkswapd_run(nid);\n\tkcompactd_run(nid);\n\n\twriteback_set_ratelimit();\n\n\tmemory_notify(MEM_ONLINE, &arg);\n\treturn 0;\n\nfailed_addition:\n\tpr_debug(\"online_pages [mem %#010llx-%#010llx] failed\\n\",\n\t\t (unsigned long long) pfn << PAGE_SHIFT,\n\t\t (((unsigned long long) pfn + nr_pages) << PAGE_SHIFT) - 1);\n\tmemory_notify(MEM_CANCEL_ONLINE, &arg);\n\tremove_pfn_range_from_zone(zone, pfn, nr_pages);\n\treturn ret;\n}\n\n \nstatic pg_data_t __ref *hotadd_init_pgdat(int nid)\n{\n\tstruct pglist_data *pgdat;\n\n\t \n\tpgdat = NODE_DATA(nid);\n\n\t \n\tfree_area_init_core_hotplug(pgdat);\n\n\t \n\tbuild_all_zonelists(pgdat);\n\n\treturn pgdat;\n}\n\n \nstatic int __try_online_node(int nid, bool set_node_online)\n{\n\tpg_data_t *pgdat;\n\tint ret = 1;\n\n\tif (node_online(nid))\n\t\treturn 0;\n\n\tpgdat = hotadd_init_pgdat(nid);\n\tif (!pgdat) {\n\t\tpr_err(\"Cannot online node %d due to NULL pgdat\\n\", nid);\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tif (set_node_online) {\n\t\tnode_set_online(nid);\n\t\tret = register_one_node(nid);\n\t\tBUG_ON(ret);\n\t}\nout:\n\treturn ret;\n}\n\n \nint try_online_node(int nid)\n{\n\tint ret;\n\n\tmem_hotplug_begin();\n\tret =  __try_online_node(nid, true);\n\tmem_hotplug_done();\n\treturn ret;\n}\n\nstatic int check_hotplug_memory_range(u64 start, u64 size)\n{\n\t \n\tif (!size || !IS_ALIGNED(start, memory_block_size_bytes()) ||\n\t    !IS_ALIGNED(size, memory_block_size_bytes())) {\n\t\tpr_err(\"Block size [%#lx] unaligned hotplug range: start %#llx, size %#llx\",\n\t\t       memory_block_size_bytes(), start, size);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int online_memory_block(struct memory_block *mem, void *arg)\n{\n\tmem->online_type = mhp_default_online_type;\n\treturn device_online(&mem->dev);\n}\n\n#ifndef arch_supports_memmap_on_memory\nstatic inline bool arch_supports_memmap_on_memory(unsigned long vmemmap_size)\n{\n\t \n\treturn IS_ALIGNED(vmemmap_size, PMD_SIZE);\n}\n#endif\n\nstatic bool mhp_supports_memmap_on_memory(unsigned long size)\n{\n\tunsigned long vmemmap_size = memory_block_memmap_size();\n\tunsigned long memmap_pages = memory_block_memmap_on_memory_pages();\n\n\t \n\tif (!mhp_memmap_on_memory() || size != memory_block_size_bytes())\n\t\treturn false;\n\n\t \n\tif (!IS_ALIGNED(vmemmap_size, PAGE_SIZE))\n\t\treturn false;\n\n\t \n\tif (!pageblock_aligned(memmap_pages))\n\t\treturn false;\n\n\tif (memmap_pages == PHYS_PFN(memory_block_size_bytes()))\n\t\t \n\t\treturn false;\n\n\treturn arch_supports_memmap_on_memory(vmemmap_size);\n}\n\n \nint __ref add_memory_resource(int nid, struct resource *res, mhp_t mhp_flags)\n{\n\tstruct mhp_params params = { .pgprot = pgprot_mhp(PAGE_KERNEL) };\n\tenum memblock_flags memblock_flags = MEMBLOCK_NONE;\n\tstruct vmem_altmap mhp_altmap = {\n\t\t.base_pfn =  PHYS_PFN(res->start),\n\t\t.end_pfn  =  PHYS_PFN(res->end),\n\t};\n\tstruct memory_group *group = NULL;\n\tu64 start, size;\n\tbool new_node = false;\n\tint ret;\n\n\tstart = res->start;\n\tsize = resource_size(res);\n\n\tret = check_hotplug_memory_range(start, size);\n\tif (ret)\n\t\treturn ret;\n\n\tif (mhp_flags & MHP_NID_IS_MGID) {\n\t\tgroup = memory_group_find_by_id(nid);\n\t\tif (!group)\n\t\t\treturn -EINVAL;\n\t\tnid = group->nid;\n\t}\n\n\tif (!node_possible(nid)) {\n\t\tWARN(1, \"node %d was absent from the node_possible_map\\n\", nid);\n\t\treturn -EINVAL;\n\t}\n\n\tmem_hotplug_begin();\n\n\tif (IS_ENABLED(CONFIG_ARCH_KEEP_MEMBLOCK)) {\n\t\tif (res->flags & IORESOURCE_SYSRAM_DRIVER_MANAGED)\n\t\t\tmemblock_flags = MEMBLOCK_DRIVER_MANAGED;\n\t\tret = memblock_add_node(start, size, nid, memblock_flags);\n\t\tif (ret)\n\t\t\tgoto error_mem_hotplug_end;\n\t}\n\n\tret = __try_online_node(nid, false);\n\tif (ret < 0)\n\t\tgoto error;\n\tnew_node = ret;\n\n\t \n\tif (mhp_flags & MHP_MEMMAP_ON_MEMORY) {\n\t\tif (mhp_supports_memmap_on_memory(size)) {\n\t\t\tmhp_altmap.free = memory_block_memmap_on_memory_pages();\n\t\t\tparams.altmap = kmalloc(sizeof(struct vmem_altmap), GFP_KERNEL);\n\t\t\tif (!params.altmap) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto error;\n\t\t\t}\n\n\t\t\tmemcpy(params.altmap, &mhp_altmap, sizeof(mhp_altmap));\n\t\t}\n\t\t \n\t}\n\n\t \n\tret = arch_add_memory(nid, start, size, &params);\n\tif (ret < 0)\n\t\tgoto error_free;\n\n\t \n\tret = create_memory_block_devices(start, size, params.altmap, group);\n\tif (ret) {\n\t\tarch_remove_memory(start, size, params.altmap);\n\t\tgoto error_free;\n\t}\n\n\tif (new_node) {\n\t\t \n\t\tnode_set_online(nid);\n\t\tret = __register_one_node(nid);\n\t\tBUG_ON(ret);\n\t}\n\n\tregister_memory_blocks_under_node(nid, PFN_DOWN(start),\n\t\t\t\t\t  PFN_UP(start + size - 1),\n\t\t\t\t\t  MEMINIT_HOTPLUG);\n\n\t \n\tif (!strcmp(res->name, \"System RAM\"))\n\t\tfirmware_map_add_hotplug(start, start + size, \"System RAM\");\n\n\t \n\tmem_hotplug_done();\n\n\t \n\tif (mhp_flags & MHP_MERGE_RESOURCE)\n\t\tmerge_system_ram_resource(res);\n\n\t \n\tif (mhp_default_online_type != MMOP_OFFLINE)\n\t\twalk_memory_blocks(start, size, NULL, online_memory_block);\n\n\treturn ret;\nerror_free:\n\tkfree(params.altmap);\nerror:\n\tif (IS_ENABLED(CONFIG_ARCH_KEEP_MEMBLOCK))\n\t\tmemblock_remove(start, size);\nerror_mem_hotplug_end:\n\tmem_hotplug_done();\n\treturn ret;\n}\n\n \nint __ref __add_memory(int nid, u64 start, u64 size, mhp_t mhp_flags)\n{\n\tstruct resource *res;\n\tint ret;\n\n\tres = register_memory_resource(start, size, \"System RAM\");\n\tif (IS_ERR(res))\n\t\treturn PTR_ERR(res);\n\n\tret = add_memory_resource(nid, res, mhp_flags);\n\tif (ret < 0)\n\t\trelease_memory_resource(res);\n\treturn ret;\n}\n\nint add_memory(int nid, u64 start, u64 size, mhp_t mhp_flags)\n{\n\tint rc;\n\n\tlock_device_hotplug();\n\trc = __add_memory(nid, start, size, mhp_flags);\n\tunlock_device_hotplug();\n\n\treturn rc;\n}\nEXPORT_SYMBOL_GPL(add_memory);\n\n \nint add_memory_driver_managed(int nid, u64 start, u64 size,\n\t\t\t      const char *resource_name, mhp_t mhp_flags)\n{\n\tstruct resource *res;\n\tint rc;\n\n\tif (!resource_name ||\n\t    strstr(resource_name, \"System RAM (\") != resource_name ||\n\t    resource_name[strlen(resource_name) - 1] != ')')\n\t\treturn -EINVAL;\n\n\tlock_device_hotplug();\n\n\tres = register_memory_resource(start, size, resource_name);\n\tif (IS_ERR(res)) {\n\t\trc = PTR_ERR(res);\n\t\tgoto out_unlock;\n\t}\n\n\trc = add_memory_resource(nid, res, mhp_flags);\n\tif (rc < 0)\n\t\trelease_memory_resource(res);\n\nout_unlock:\n\tunlock_device_hotplug();\n\treturn rc;\n}\nEXPORT_SYMBOL_GPL(add_memory_driver_managed);\n\n \nstruct range __weak arch_get_mappable_range(void)\n{\n\tstruct range mhp_range = {\n\t\t.start = 0UL,\n\t\t.end = -1ULL,\n\t};\n\treturn mhp_range;\n}\n\nstruct range mhp_get_pluggable_range(bool need_mapping)\n{\n\tconst u64 max_phys = (1ULL << MAX_PHYSMEM_BITS) - 1;\n\tstruct range mhp_range;\n\n\tif (need_mapping) {\n\t\tmhp_range = arch_get_mappable_range();\n\t\tif (mhp_range.start > max_phys) {\n\t\t\tmhp_range.start = 0;\n\t\t\tmhp_range.end = 0;\n\t\t}\n\t\tmhp_range.end = min_t(u64, mhp_range.end, max_phys);\n\t} else {\n\t\tmhp_range.start = 0;\n\t\tmhp_range.end = max_phys;\n\t}\n\treturn mhp_range;\n}\nEXPORT_SYMBOL_GPL(mhp_get_pluggable_range);\n\nbool mhp_range_allowed(u64 start, u64 size, bool need_mapping)\n{\n\tstruct range mhp_range = mhp_get_pluggable_range(need_mapping);\n\tu64 end = start + size;\n\n\tif (start < end && start >= mhp_range.start && (end - 1) <= mhp_range.end)\n\t\treturn true;\n\n\tpr_warn(\"Hotplug memory [%#llx-%#llx] exceeds maximum addressable range [%#llx-%#llx]\\n\",\n\t\tstart, end, mhp_range.start, mhp_range.end);\n\treturn false;\n}\n\n#ifdef CONFIG_MEMORY_HOTREMOVE\n \nstatic int scan_movable_pages(unsigned long start, unsigned long end,\n\t\t\t      unsigned long *movable_pfn)\n{\n\tunsigned long pfn;\n\n\tfor (pfn = start; pfn < end; pfn++) {\n\t\tstruct page *page, *head;\n\t\tunsigned long skip;\n\n\t\tif (!pfn_valid(pfn))\n\t\t\tcontinue;\n\t\tpage = pfn_to_page(pfn);\n\t\tif (PageLRU(page))\n\t\t\tgoto found;\n\t\tif (__PageMovable(page))\n\t\t\tgoto found;\n\n\t\t \n\t\tif (PageOffline(page) && page_count(page))\n\t\t\treturn -EBUSY;\n\n\t\tif (!PageHuge(page))\n\t\t\tcontinue;\n\t\thead = compound_head(page);\n\t\t \n\t\tif (HPageMigratable(head))\n\t\t\tgoto found;\n\t\tskip = compound_nr(head) - (pfn - page_to_pfn(head));\n\t\tpfn += skip - 1;\n\t}\n\treturn -ENOENT;\nfound:\n\t*movable_pfn = pfn;\n\treturn 0;\n}\n\nstatic void do_migrate_range(unsigned long start_pfn, unsigned long end_pfn)\n{\n\tunsigned long pfn;\n\tstruct page *page, *head;\n\tLIST_HEAD(source);\n\tstatic DEFINE_RATELIMIT_STATE(migrate_rs, DEFAULT_RATELIMIT_INTERVAL,\n\t\t\t\t      DEFAULT_RATELIMIT_BURST);\n\n\tfor (pfn = start_pfn; pfn < end_pfn; pfn++) {\n\t\tstruct folio *folio;\n\t\tbool isolated;\n\n\t\tif (!pfn_valid(pfn))\n\t\t\tcontinue;\n\t\tpage = pfn_to_page(pfn);\n\t\tfolio = page_folio(page);\n\t\thead = &folio->page;\n\n\t\tif (PageHuge(page)) {\n\t\t\tpfn = page_to_pfn(head) + compound_nr(head) - 1;\n\t\t\tisolate_hugetlb(folio, &source);\n\t\t\tcontinue;\n\t\t} else if (PageTransHuge(page))\n\t\t\tpfn = page_to_pfn(head) + thp_nr_pages(page) - 1;\n\n\t\t \n\t\tif (PageHWPoison(page)) {\n\t\t\tif (WARN_ON(folio_test_lru(folio)))\n\t\t\t\tfolio_isolate_lru(folio);\n\t\t\tif (folio_mapped(folio))\n\t\t\t\ttry_to_unmap(folio, TTU_IGNORE_MLOCK);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!get_page_unless_zero(page))\n\t\t\tcontinue;\n\t\t \n\t\tif (PageLRU(page))\n\t\t\tisolated = isolate_lru_page(page);\n\t\telse\n\t\t\tisolated = isolate_movable_page(page, ISOLATE_UNEVICTABLE);\n\t\tif (isolated) {\n\t\t\tlist_add_tail(&page->lru, &source);\n\t\t\tif (!__PageMovable(page))\n\t\t\t\tinc_node_page_state(page, NR_ISOLATED_ANON +\n\t\t\t\t\t\t    page_is_file_lru(page));\n\n\t\t} else {\n\t\t\tif (__ratelimit(&migrate_rs)) {\n\t\t\t\tpr_warn(\"failed to isolate pfn %lx\\n\", pfn);\n\t\t\t\tdump_page(page, \"isolation failed\");\n\t\t\t}\n\t\t}\n\t\tput_page(page);\n\t}\n\tif (!list_empty(&source)) {\n\t\tnodemask_t nmask = node_states[N_MEMORY];\n\t\tstruct migration_target_control mtc = {\n\t\t\t.nmask = &nmask,\n\t\t\t.gfp_mask = GFP_USER | __GFP_MOVABLE | __GFP_RETRY_MAYFAIL,\n\t\t};\n\t\tint ret;\n\n\t\t \n\t\tmtc.nid = page_to_nid(list_first_entry(&source, struct page, lru));\n\n\t\t \n\t\tnode_clear(mtc.nid, nmask);\n\t\tif (nodes_empty(nmask))\n\t\t\tnode_set(mtc.nid, nmask);\n\t\tret = migrate_pages(&source, alloc_migration_target, NULL,\n\t\t\t(unsigned long)&mtc, MIGRATE_SYNC, MR_MEMORY_HOTPLUG, NULL);\n\t\tif (ret) {\n\t\t\tlist_for_each_entry(page, &source, lru) {\n\t\t\t\tif (__ratelimit(&migrate_rs)) {\n\t\t\t\t\tpr_warn(\"migrating pfn %lx failed ret:%d\\n\",\n\t\t\t\t\t\tpage_to_pfn(page), ret);\n\t\t\t\t\tdump_page(page, \"migration failure\");\n\t\t\t\t}\n\t\t\t}\n\t\t\tputback_movable_pages(&source);\n\t\t}\n\t}\n}\n\nstatic int __init cmdline_parse_movable_node(char *p)\n{\n\tmovable_node_enabled = true;\n\treturn 0;\n}\nearly_param(\"movable_node\", cmdline_parse_movable_node);\n\n \nstatic void node_states_check_changes_offline(unsigned long nr_pages,\n\t\tstruct zone *zone, struct memory_notify *arg)\n{\n\tstruct pglist_data *pgdat = zone->zone_pgdat;\n\tunsigned long present_pages = 0;\n\tenum zone_type zt;\n\n\targ->status_change_nid = NUMA_NO_NODE;\n\targ->status_change_nid_normal = NUMA_NO_NODE;\n\n\t \n\tfor (zt = 0; zt <= ZONE_NORMAL; zt++)\n\t\tpresent_pages += pgdat->node_zones[zt].present_pages;\n\tif (zone_idx(zone) <= ZONE_NORMAL && nr_pages >= present_pages)\n\t\targ->status_change_nid_normal = zone_to_nid(zone);\n\n\t \n\tpresent_pages += pgdat->node_zones[ZONE_MOVABLE].present_pages;\n\n\tif (nr_pages >= present_pages)\n\t\targ->status_change_nid = zone_to_nid(zone);\n}\n\nstatic void node_states_clear_node(int node, struct memory_notify *arg)\n{\n\tif (arg->status_change_nid_normal >= 0)\n\t\tnode_clear_state(node, N_NORMAL_MEMORY);\n\n\tif (arg->status_change_nid >= 0)\n\t\tnode_clear_state(node, N_MEMORY);\n}\n\nstatic int count_system_ram_pages_cb(unsigned long start_pfn,\n\t\t\t\t     unsigned long nr_pages, void *data)\n{\n\tunsigned long *nr_system_ram_pages = data;\n\n\t*nr_system_ram_pages += nr_pages;\n\treturn 0;\n}\n\n \nint __ref offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n\t\t\tstruct zone *zone, struct memory_group *group)\n{\n\tconst unsigned long end_pfn = start_pfn + nr_pages;\n\tunsigned long pfn, system_ram_pages = 0;\n\tconst int node = zone_to_nid(zone);\n\tunsigned long flags;\n\tstruct memory_notify arg;\n\tchar *reason;\n\tint ret;\n\n\t \n\tif (WARN_ON_ONCE(!nr_pages || !pageblock_aligned(start_pfn) ||\n\t\t\t !IS_ALIGNED(start_pfn + nr_pages, PAGES_PER_SECTION)))\n\t\treturn -EINVAL;\n\n\t \n\twalk_system_ram_range(start_pfn, nr_pages, &system_ram_pages,\n\t\t\t      count_system_ram_pages_cb);\n\tif (system_ram_pages != nr_pages) {\n\t\tret = -EINVAL;\n\t\treason = \"memory holes\";\n\t\tgoto failed_removal;\n\t}\n\n\t \n\tif (WARN_ON_ONCE(page_zone(pfn_to_page(start_pfn)) != zone ||\n\t\t\t page_zone(pfn_to_page(end_pfn - 1)) != zone)) {\n\t\tret = -EINVAL;\n\t\treason = \"multizone range\";\n\t\tgoto failed_removal;\n\t}\n\n\t \n\tzone_pcp_disable(zone);\n\tlru_cache_disable();\n\n\t \n\tret = start_isolate_page_range(start_pfn, end_pfn,\n\t\t\t\t       MIGRATE_MOVABLE,\n\t\t\t\t       MEMORY_OFFLINE | REPORT_FAILURE,\n\t\t\t\t       GFP_USER | __GFP_MOVABLE | __GFP_RETRY_MAYFAIL);\n\tif (ret) {\n\t\treason = \"failure to isolate range\";\n\t\tgoto failed_removal_pcplists_disabled;\n\t}\n\n\targ.start_pfn = start_pfn;\n\targ.nr_pages = nr_pages;\n\tnode_states_check_changes_offline(nr_pages, zone, &arg);\n\n\tret = memory_notify(MEM_GOING_OFFLINE, &arg);\n\tret = notifier_to_errno(ret);\n\tif (ret) {\n\t\treason = \"notifier failure\";\n\t\tgoto failed_removal_isolated;\n\t}\n\n\tdo {\n\t\tpfn = start_pfn;\n\t\tdo {\n\t\t\t \n\t\t\tif (signal_pending(current)) {\n\t\t\t\tret = -EINTR;\n\t\t\t\treason = \"signal backoff\";\n\t\t\t\tgoto failed_removal_isolated;\n\t\t\t}\n\n\t\t\tcond_resched();\n\n\t\t\tret = scan_movable_pages(pfn, end_pfn, &pfn);\n\t\t\tif (!ret) {\n\t\t\t\t \n\t\t\t\tdo_migrate_range(pfn, end_pfn);\n\t\t\t}\n\t\t} while (!ret);\n\n\t\tif (ret != -ENOENT) {\n\t\t\treason = \"unmovable page\";\n\t\t\tgoto failed_removal_isolated;\n\t\t}\n\n\t\t \n\t\tret = dissolve_free_huge_pages(start_pfn, end_pfn);\n\t\tif (ret) {\n\t\t\treason = \"failure to dissolve huge pages\";\n\t\t\tgoto failed_removal_isolated;\n\t\t}\n\n\t\tret = test_pages_isolated(start_pfn, end_pfn, MEMORY_OFFLINE);\n\n\t} while (ret);\n\n\t \n\t__offline_isolated_pages(start_pfn, end_pfn);\n\tpr_debug(\"Offlined Pages %ld\\n\", nr_pages);\n\n\t \n\tspin_lock_irqsave(&zone->lock, flags);\n\tzone->nr_isolate_pageblock -= nr_pages / pageblock_nr_pages;\n\tspin_unlock_irqrestore(&zone->lock, flags);\n\n\tlru_cache_enable();\n\tzone_pcp_enable(zone);\n\n\t \n\tadjust_managed_page_count(pfn_to_page(start_pfn), -nr_pages);\n\tadjust_present_page_count(pfn_to_page(start_pfn), group, -nr_pages);\n\n\t \n\tinit_per_zone_wmark_min();\n\n\tif (!populated_zone(zone)) {\n\t\tzone_pcp_reset(zone);\n\t\tbuild_all_zonelists(NULL);\n\t}\n\n\tnode_states_clear_node(node, &arg);\n\tif (arg.status_change_nid >= 0) {\n\t\tkcompactd_stop(node);\n\t\tkswapd_stop(node);\n\t}\n\n\twriteback_set_ratelimit();\n\n\tmemory_notify(MEM_OFFLINE, &arg);\n\tremove_pfn_range_from_zone(zone, start_pfn, nr_pages);\n\treturn 0;\n\nfailed_removal_isolated:\n\t \n\tundo_isolate_page_range(start_pfn, end_pfn, MIGRATE_MOVABLE);\n\tmemory_notify(MEM_CANCEL_OFFLINE, &arg);\nfailed_removal_pcplists_disabled:\n\tlru_cache_enable();\n\tzone_pcp_enable(zone);\nfailed_removal:\n\tpr_debug(\"memory offlining [mem %#010llx-%#010llx] failed due to %s\\n\",\n\t\t (unsigned long long) start_pfn << PAGE_SHIFT,\n\t\t ((unsigned long long) end_pfn << PAGE_SHIFT) - 1,\n\t\t reason);\n\treturn ret;\n}\n\nstatic int check_memblock_offlined_cb(struct memory_block *mem, void *arg)\n{\n\tint *nid = arg;\n\n\t*nid = mem->nid;\n\tif (unlikely(mem->state != MEM_OFFLINE)) {\n\t\tphys_addr_t beginpa, endpa;\n\n\t\tbeginpa = PFN_PHYS(section_nr_to_pfn(mem->start_section_nr));\n\t\tendpa = beginpa + memory_block_size_bytes() - 1;\n\t\tpr_warn(\"removing memory fails, because memory [%pa-%pa] is onlined\\n\",\n\t\t\t&beginpa, &endpa);\n\n\t\treturn -EBUSY;\n\t}\n\treturn 0;\n}\n\nstatic int test_has_altmap_cb(struct memory_block *mem, void *arg)\n{\n\tstruct memory_block **mem_ptr = (struct memory_block **)arg;\n\t \n\tif (mem->altmap) {\n\t\t*mem_ptr = mem;\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic int check_cpu_on_node(int nid)\n{\n\tint cpu;\n\n\tfor_each_present_cpu(cpu) {\n\t\tif (cpu_to_node(cpu) == nid)\n\t\t\t \n\t\t\treturn -EBUSY;\n\t}\n\n\treturn 0;\n}\n\nstatic int check_no_memblock_for_node_cb(struct memory_block *mem, void *arg)\n{\n\tint nid = *(int *)arg;\n\n\t \n\treturn mem->nid == nid ? -EEXIST : 0;\n}\n\n \nvoid try_offline_node(int nid)\n{\n\tint rc;\n\n\t \n\tif (node_spanned_pages(nid))\n\t\treturn;\n\n\t \n\trc = for_each_memory_block(&nid, check_no_memblock_for_node_cb);\n\tif (rc)\n\t\treturn;\n\n\tif (check_cpu_on_node(nid))\n\t\treturn;\n\n\t \n\tnode_set_offline(nid);\n\tunregister_one_node(nid);\n}\nEXPORT_SYMBOL(try_offline_node);\n\nstatic int __ref try_remove_memory(u64 start, u64 size)\n{\n\tstruct memory_block *mem;\n\tint rc = 0, nid = NUMA_NO_NODE;\n\tstruct vmem_altmap *altmap = NULL;\n\n\tBUG_ON(check_hotplug_memory_range(start, size));\n\n\t \n\trc = walk_memory_blocks(start, size, &nid, check_memblock_offlined_cb);\n\tif (rc)\n\t\treturn rc;\n\n\t \n\tif (mhp_memmap_on_memory()) {\n\t\trc = walk_memory_blocks(start, size, &mem, test_has_altmap_cb);\n\t\tif (rc) {\n\t\t\tif (size != memory_block_size_bytes()) {\n\t\t\t\tpr_warn(\"Refuse to remove %#llx - %#llx,\"\n\t\t\t\t\t\"wrong granularity\\n\",\n\t\t\t\t\tstart, start + size);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\taltmap = mem->altmap;\n\t\t\t \n\t\t\tmem->altmap = NULL;\n\t\t}\n\t}\n\n\t \n\tfirmware_map_remove(start, start + size, \"System RAM\");\n\n\t \n\tremove_memory_block_devices(start, size);\n\n\tmem_hotplug_begin();\n\n\tarch_remove_memory(start, size, altmap);\n\n\t \n\tif (altmap) {\n\t\tWARN(altmap->alloc, \"Altmap not fully unmapped\");\n\t\tkfree(altmap);\n\t}\n\n\tif (IS_ENABLED(CONFIG_ARCH_KEEP_MEMBLOCK)) {\n\t\tmemblock_phys_free(start, size);\n\t\tmemblock_remove(start, size);\n\t}\n\n\trelease_mem_region_adjustable(start, size);\n\n\tif (nid != NUMA_NO_NODE)\n\t\ttry_offline_node(nid);\n\n\tmem_hotplug_done();\n\treturn 0;\n}\n\n \nvoid __remove_memory(u64 start, u64 size)\n{\n\n\t \n\tif (try_remove_memory(start, size))\n\t\tBUG();\n}\n\n \nint remove_memory(u64 start, u64 size)\n{\n\tint rc;\n\n\tlock_device_hotplug();\n\trc = try_remove_memory(start, size);\n\tunlock_device_hotplug();\n\n\treturn rc;\n}\nEXPORT_SYMBOL_GPL(remove_memory);\n\nstatic int try_offline_memory_block(struct memory_block *mem, void *arg)\n{\n\tuint8_t online_type = MMOP_ONLINE_KERNEL;\n\tuint8_t **online_types = arg;\n\tstruct page *page;\n\tint rc;\n\n\t \n\tpage = pfn_to_online_page(section_nr_to_pfn(mem->start_section_nr));\n\tif (page && zone_idx(page_zone(page)) == ZONE_MOVABLE)\n\t\tonline_type = MMOP_ONLINE_MOVABLE;\n\n\trc = device_offline(&mem->dev);\n\t \n\tif (!rc)\n\t\t**online_types = online_type;\n\n\t(*online_types)++;\n\t \n\treturn rc < 0 ? rc : 0;\n}\n\nstatic int try_reonline_memory_block(struct memory_block *mem, void *arg)\n{\n\tuint8_t **online_types = arg;\n\tint rc;\n\n\tif (**online_types != MMOP_OFFLINE) {\n\t\tmem->online_type = **online_types;\n\t\trc = device_online(&mem->dev);\n\t\tif (rc < 0)\n\t\t\tpr_warn(\"%s: Failed to re-online memory: %d\",\n\t\t\t\t__func__, rc);\n\t}\n\n\t \n\t(*online_types)++;\n\treturn 0;\n}\n\n \nint offline_and_remove_memory(u64 start, u64 size)\n{\n\tconst unsigned long mb_count = size / memory_block_size_bytes();\n\tuint8_t *online_types, *tmp;\n\tint rc;\n\n\tif (!IS_ALIGNED(start, memory_block_size_bytes()) ||\n\t    !IS_ALIGNED(size, memory_block_size_bytes()) || !size)\n\t\treturn -EINVAL;\n\n\t \n\tonline_types = kmalloc_array(mb_count, sizeof(*online_types),\n\t\t\t\t     GFP_KERNEL);\n\tif (!online_types)\n\t\treturn -ENOMEM;\n\t \n\tmemset(online_types, MMOP_OFFLINE, mb_count);\n\n\tlock_device_hotplug();\n\n\ttmp = online_types;\n\trc = walk_memory_blocks(start, size, &tmp, try_offline_memory_block);\n\n\t \n\tif (!rc) {\n\t\trc = try_remove_memory(start, size);\n\t\tif (rc)\n\t\t\tpr_err(\"%s: Failed to remove memory: %d\", __func__, rc);\n\t}\n\n\t \n\tif (rc) {\n\t\ttmp = online_types;\n\t\twalk_memory_blocks(start, size, &tmp,\n\t\t\t\t   try_reonline_memory_block);\n\t}\n\tunlock_device_hotplug();\n\n\tkfree(online_types);\n\treturn rc;\n}\nEXPORT_SYMBOL_GPL(offline_and_remove_memory);\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}