{
  "module_name": "slab.h",
  "hash_id": "b68516a76be2d3db2895b4ea1bc20b37f519fa99e95ef9e4f80de5d3d0271548",
  "original_prompt": "Ingested from linux-6.6.14/mm/slab.h",
  "human_readable_source": " \n#ifndef MM_SLAB_H\n#define MM_SLAB_H\n \nvoid __init kmem_cache_init(void);\n\n#ifdef CONFIG_64BIT\n# ifdef system_has_cmpxchg128\n# define system_has_freelist_aba()\tsystem_has_cmpxchg128()\n# define try_cmpxchg_freelist\t\ttry_cmpxchg128\n# endif\n#define this_cpu_try_cmpxchg_freelist\tthis_cpu_try_cmpxchg128\ntypedef u128 freelist_full_t;\n#else  \n# ifdef system_has_cmpxchg64\n# define system_has_freelist_aba()\tsystem_has_cmpxchg64()\n# define try_cmpxchg_freelist\t\ttry_cmpxchg64\n# endif\n#define this_cpu_try_cmpxchg_freelist\tthis_cpu_try_cmpxchg64\ntypedef u64 freelist_full_t;\n#endif  \n\n#if defined(system_has_freelist_aba) && !defined(CONFIG_HAVE_ALIGNED_STRUCT_PAGE)\n#undef system_has_freelist_aba\n#endif\n\n \ntypedef union {\n\tstruct {\n\t\tvoid *freelist;\n\t\tunsigned long counter;\n\t};\n\tfreelist_full_t full;\n} freelist_aba_t;\n\n \nstruct slab {\n\tunsigned long __page_flags;\n\n#if defined(CONFIG_SLAB)\n\n\tstruct kmem_cache *slab_cache;\n\tunion {\n\t\tstruct {\n\t\t\tstruct list_head slab_list;\n\t\t\tvoid *freelist;\t \n\t\t\tvoid *s_mem;\t \n\t\t};\n\t\tstruct rcu_head rcu_head;\n\t};\n\tunsigned int active;\n\n#elif defined(CONFIG_SLUB)\n\n\tstruct kmem_cache *slab_cache;\n\tunion {\n\t\tstruct {\n\t\t\tunion {\n\t\t\t\tstruct list_head slab_list;\n#ifdef CONFIG_SLUB_CPU_PARTIAL\n\t\t\t\tstruct {\n\t\t\t\t\tstruct slab *next;\n\t\t\t\t\tint slabs;\t \n\t\t\t\t};\n#endif\n\t\t\t};\n\t\t\t \n\t\t\tunion {\n\t\t\t\tstruct {\n\t\t\t\t\tvoid *freelist;\t\t \n\t\t\t\t\tunion {\n\t\t\t\t\t\tunsigned long counters;\n\t\t\t\t\t\tstruct {\n\t\t\t\t\t\t\tunsigned inuse:16;\n\t\t\t\t\t\t\tunsigned objects:15;\n\t\t\t\t\t\t\tunsigned frozen:1;\n\t\t\t\t\t\t};\n\t\t\t\t\t};\n\t\t\t\t};\n#ifdef system_has_freelist_aba\n\t\t\t\tfreelist_aba_t freelist_counter;\n#endif\n\t\t\t};\n\t\t};\n\t\tstruct rcu_head rcu_head;\n\t};\n\tunsigned int __unused;\n\n#else\n#error \"Unexpected slab allocator configured\"\n#endif\n\n\tatomic_t __page_refcount;\n#ifdef CONFIG_MEMCG\n\tunsigned long memcg_data;\n#endif\n};\n\n#define SLAB_MATCH(pg, sl)\t\t\t\t\t\t\\\n\tstatic_assert(offsetof(struct page, pg) == offsetof(struct slab, sl))\nSLAB_MATCH(flags, __page_flags);\nSLAB_MATCH(compound_head, slab_cache);\t \nSLAB_MATCH(_refcount, __page_refcount);\n#ifdef CONFIG_MEMCG\nSLAB_MATCH(memcg_data, memcg_data);\n#endif\n#undef SLAB_MATCH\nstatic_assert(sizeof(struct slab) <= sizeof(struct page));\n#if defined(system_has_freelist_aba) && defined(CONFIG_SLUB)\nstatic_assert(IS_ALIGNED(offsetof(struct slab, freelist), sizeof(freelist_aba_t)));\n#endif\n\n \n#define folio_slab(folio)\t(_Generic((folio),\t\t\t\\\n\tconst struct folio *:\t(const struct slab *)(folio),\t\t\\\n\tstruct folio *:\t\t(struct slab *)(folio)))\n\n \n#define slab_folio(s)\t\t(_Generic((s),\t\t\t\t\\\n\tconst struct slab *:\t(const struct folio *)s,\t\t\\\n\tstruct slab *:\t\t(struct folio *)s))\n\n \n#define page_slab(p)\t\t(_Generic((p),\t\t\t\t\\\n\tconst struct page *:\t(const struct slab *)(p),\t\t\\\n\tstruct page *:\t\t(struct slab *)(p)))\n\n \n#define slab_page(s) folio_page(slab_folio(s), 0)\n\n \nstatic inline bool slab_test_pfmemalloc(const struct slab *slab)\n{\n\treturn folio_test_active((struct folio *)slab_folio(slab));\n}\n\nstatic inline void slab_set_pfmemalloc(struct slab *slab)\n{\n\tfolio_set_active(slab_folio(slab));\n}\n\nstatic inline void slab_clear_pfmemalloc(struct slab *slab)\n{\n\tfolio_clear_active(slab_folio(slab));\n}\n\nstatic inline void __slab_clear_pfmemalloc(struct slab *slab)\n{\n\t__folio_clear_active(slab_folio(slab));\n}\n\nstatic inline void *slab_address(const struct slab *slab)\n{\n\treturn folio_address(slab_folio(slab));\n}\n\nstatic inline int slab_nid(const struct slab *slab)\n{\n\treturn folio_nid(slab_folio(slab));\n}\n\nstatic inline pg_data_t *slab_pgdat(const struct slab *slab)\n{\n\treturn folio_pgdat(slab_folio(slab));\n}\n\nstatic inline struct slab *virt_to_slab(const void *addr)\n{\n\tstruct folio *folio = virt_to_folio(addr);\n\n\tif (!folio_test_slab(folio))\n\t\treturn NULL;\n\n\treturn folio_slab(folio);\n}\n\nstatic inline int slab_order(const struct slab *slab)\n{\n\treturn folio_order((struct folio *)slab_folio(slab));\n}\n\nstatic inline size_t slab_size(const struct slab *slab)\n{\n\treturn PAGE_SIZE << slab_order(slab);\n}\n\n#ifdef CONFIG_SLAB\n#include <linux/slab_def.h>\n#endif\n\n#ifdef CONFIG_SLUB\n#include <linux/slub_def.h>\n#endif\n\n#include <linux/memcontrol.h>\n#include <linux/fault-inject.h>\n#include <linux/kasan.h>\n#include <linux/kmemleak.h>\n#include <linux/random.h>\n#include <linux/sched/mm.h>\n#include <linux/list_lru.h>\n\n \nenum slab_state {\n\tDOWN,\t\t\t \n\tPARTIAL,\t\t \n\tPARTIAL_NODE,\t\t \n\tUP,\t\t\t \n\tFULL\t\t\t \n};\n\nextern enum slab_state slab_state;\n\n \nextern struct mutex slab_mutex;\n\n \nextern struct list_head slab_caches;\n\n \nextern struct kmem_cache *kmem_cache;\n\n \nextern const struct kmalloc_info_struct {\n\tconst char *name[NR_KMALLOC_TYPES];\n\tunsigned int size;\n} kmalloc_info[];\n\n \nvoid setup_kmalloc_cache_index_table(void);\nvoid create_kmalloc_caches(slab_flags_t);\n\n \nstruct kmem_cache *kmalloc_slab(size_t size, gfp_t flags, unsigned long caller);\n\nvoid *__kmem_cache_alloc_node(struct kmem_cache *s, gfp_t gfpflags,\n\t\t\t      int node, size_t orig_size,\n\t\t\t      unsigned long caller);\nvoid __kmem_cache_free(struct kmem_cache *s, void *x, unsigned long caller);\n\ngfp_t kmalloc_fix_flags(gfp_t flags);\n\n \nint __kmem_cache_create(struct kmem_cache *, slab_flags_t flags);\n\nvoid __init new_kmalloc_cache(int idx, enum kmalloc_cache_type type,\n\t\t\t      slab_flags_t flags);\nextern void create_boot_cache(struct kmem_cache *, const char *name,\n\t\t\tunsigned int size, slab_flags_t flags,\n\t\t\tunsigned int useroffset, unsigned int usersize);\n\nint slab_unmergeable(struct kmem_cache *s);\nstruct kmem_cache *find_mergeable(unsigned size, unsigned align,\n\t\tslab_flags_t flags, const char *name, void (*ctor)(void *));\nstruct kmem_cache *\n__kmem_cache_alias(const char *name, unsigned int size, unsigned int align,\n\t\t   slab_flags_t flags, void (*ctor)(void *));\n\nslab_flags_t kmem_cache_flags(unsigned int object_size,\n\tslab_flags_t flags, const char *name);\n\nstatic inline bool is_kmalloc_cache(struct kmem_cache *s)\n{\n\treturn (s->flags & SLAB_KMALLOC);\n}\n\n \n#define SLAB_CORE_FLAGS (SLAB_HWCACHE_ALIGN | SLAB_CACHE_DMA | \\\n\t\t\t SLAB_CACHE_DMA32 | SLAB_PANIC | \\\n\t\t\t SLAB_TYPESAFE_BY_RCU | SLAB_DEBUG_OBJECTS )\n\n#if defined(CONFIG_DEBUG_SLAB)\n#define SLAB_DEBUG_FLAGS (SLAB_RED_ZONE | SLAB_POISON | SLAB_STORE_USER)\n#elif defined(CONFIG_SLUB_DEBUG)\n#define SLAB_DEBUG_FLAGS (SLAB_RED_ZONE | SLAB_POISON | SLAB_STORE_USER | \\\n\t\t\t  SLAB_TRACE | SLAB_CONSISTENCY_CHECKS)\n#else\n#define SLAB_DEBUG_FLAGS (0)\n#endif\n\n#if defined(CONFIG_SLAB)\n#define SLAB_CACHE_FLAGS (SLAB_MEM_SPREAD | SLAB_NOLEAKTRACE | \\\n\t\t\t  SLAB_RECLAIM_ACCOUNT | SLAB_TEMPORARY | \\\n\t\t\t  SLAB_ACCOUNT | SLAB_NO_MERGE)\n#elif defined(CONFIG_SLUB)\n#define SLAB_CACHE_FLAGS (SLAB_NOLEAKTRACE | SLAB_RECLAIM_ACCOUNT | \\\n\t\t\t  SLAB_TEMPORARY | SLAB_ACCOUNT | \\\n\t\t\t  SLAB_NO_USER_FLAGS | SLAB_KMALLOC | SLAB_NO_MERGE)\n#else\n#define SLAB_CACHE_FLAGS (SLAB_NOLEAKTRACE)\n#endif\n\n \n#define CACHE_CREATE_MASK (SLAB_CORE_FLAGS | SLAB_DEBUG_FLAGS | SLAB_CACHE_FLAGS)\n\n \n#define SLAB_FLAGS_PERMITTED (SLAB_CORE_FLAGS | \\\n\t\t\t      SLAB_RED_ZONE | \\\n\t\t\t      SLAB_POISON | \\\n\t\t\t      SLAB_STORE_USER | \\\n\t\t\t      SLAB_TRACE | \\\n\t\t\t      SLAB_CONSISTENCY_CHECKS | \\\n\t\t\t      SLAB_MEM_SPREAD | \\\n\t\t\t      SLAB_NOLEAKTRACE | \\\n\t\t\t      SLAB_RECLAIM_ACCOUNT | \\\n\t\t\t      SLAB_TEMPORARY | \\\n\t\t\t      SLAB_ACCOUNT | \\\n\t\t\t      SLAB_KMALLOC | \\\n\t\t\t      SLAB_NO_MERGE | \\\n\t\t\t      SLAB_NO_USER_FLAGS)\n\nbool __kmem_cache_empty(struct kmem_cache *);\nint __kmem_cache_shutdown(struct kmem_cache *);\nvoid __kmem_cache_release(struct kmem_cache *);\nint __kmem_cache_shrink(struct kmem_cache *);\nvoid slab_kmem_cache_release(struct kmem_cache *);\n\nstruct seq_file;\nstruct file;\n\nstruct slabinfo {\n\tunsigned long active_objs;\n\tunsigned long num_objs;\n\tunsigned long active_slabs;\n\tunsigned long num_slabs;\n\tunsigned long shared_avail;\n\tunsigned int limit;\n\tunsigned int batchcount;\n\tunsigned int shared;\n\tunsigned int objects_per_slab;\n\tunsigned int cache_order;\n};\n\nvoid get_slabinfo(struct kmem_cache *s, struct slabinfo *sinfo);\nvoid slabinfo_show_stats(struct seq_file *m, struct kmem_cache *s);\nssize_t slabinfo_write(struct file *file, const char __user *buffer,\n\t\t       size_t count, loff_t *ppos);\n\nstatic inline enum node_stat_item cache_vmstat_idx(struct kmem_cache *s)\n{\n\treturn (s->flags & SLAB_RECLAIM_ACCOUNT) ?\n\t\tNR_SLAB_RECLAIMABLE_B : NR_SLAB_UNRECLAIMABLE_B;\n}\n\n#ifdef CONFIG_SLUB_DEBUG\n#ifdef CONFIG_SLUB_DEBUG_ON\nDECLARE_STATIC_KEY_TRUE(slub_debug_enabled);\n#else\nDECLARE_STATIC_KEY_FALSE(slub_debug_enabled);\n#endif\nextern void print_tracking(struct kmem_cache *s, void *object);\nlong validate_slab_cache(struct kmem_cache *s);\nstatic inline bool __slub_debug_enabled(void)\n{\n\treturn static_branch_unlikely(&slub_debug_enabled);\n}\n#else\nstatic inline void print_tracking(struct kmem_cache *s, void *object)\n{\n}\nstatic inline bool __slub_debug_enabled(void)\n{\n\treturn false;\n}\n#endif\n\n \nstatic inline bool kmem_cache_debug_flags(struct kmem_cache *s, slab_flags_t flags)\n{\n\tif (IS_ENABLED(CONFIG_SLUB_DEBUG))\n\t\tVM_WARN_ON_ONCE(!(flags & SLAB_DEBUG_FLAGS));\n\tif (__slub_debug_enabled())\n\t\treturn s->flags & flags;\n\treturn false;\n}\n\n#ifdef CONFIG_MEMCG_KMEM\n \nstatic inline struct obj_cgroup **slab_objcgs(struct slab *slab)\n{\n\tunsigned long memcg_data = READ_ONCE(slab->memcg_data);\n\n\tVM_BUG_ON_PAGE(memcg_data && !(memcg_data & MEMCG_DATA_OBJCGS),\n\t\t\t\t\t\t\tslab_page(slab));\n\tVM_BUG_ON_PAGE(memcg_data & MEMCG_DATA_KMEM, slab_page(slab));\n\n\treturn (struct obj_cgroup **)(memcg_data & ~MEMCG_DATA_FLAGS_MASK);\n}\n\nint memcg_alloc_slab_cgroups(struct slab *slab, struct kmem_cache *s,\n\t\t\t\t gfp_t gfp, bool new_slab);\nvoid mod_objcg_state(struct obj_cgroup *objcg, struct pglist_data *pgdat,\n\t\t     enum node_stat_item idx, int nr);\n\nstatic inline void memcg_free_slab_cgroups(struct slab *slab)\n{\n\tkfree(slab_objcgs(slab));\n\tslab->memcg_data = 0;\n}\n\nstatic inline size_t obj_full_size(struct kmem_cache *s)\n{\n\t \n\treturn s->size + sizeof(struct obj_cgroup *);\n}\n\n \nstatic inline bool memcg_slab_pre_alloc_hook(struct kmem_cache *s,\n\t\t\t\t\t     struct list_lru *lru,\n\t\t\t\t\t     struct obj_cgroup **objcgp,\n\t\t\t\t\t     size_t objects, gfp_t flags)\n{\n\tstruct obj_cgroup *objcg;\n\n\tif (!memcg_kmem_online())\n\t\treturn true;\n\n\tif (!(flags & __GFP_ACCOUNT) && !(s->flags & SLAB_ACCOUNT))\n\t\treturn true;\n\n\tobjcg = get_obj_cgroup_from_current();\n\tif (!objcg)\n\t\treturn true;\n\n\tif (lru) {\n\t\tint ret;\n\t\tstruct mem_cgroup *memcg;\n\n\t\tmemcg = get_mem_cgroup_from_objcg(objcg);\n\t\tret = memcg_list_lru_alloc(memcg, lru, flags);\n\t\tcss_put(&memcg->css);\n\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tif (obj_cgroup_charge(objcg, flags, objects * obj_full_size(s)))\n\t\tgoto out;\n\n\t*objcgp = objcg;\n\treturn true;\nout:\n\tobj_cgroup_put(objcg);\n\treturn false;\n}\n\nstatic inline void memcg_slab_post_alloc_hook(struct kmem_cache *s,\n\t\t\t\t\t      struct obj_cgroup *objcg,\n\t\t\t\t\t      gfp_t flags, size_t size,\n\t\t\t\t\t      void **p)\n{\n\tstruct slab *slab;\n\tunsigned long off;\n\tsize_t i;\n\n\tif (!memcg_kmem_online() || !objcg)\n\t\treturn;\n\n\tfor (i = 0; i < size; i++) {\n\t\tif (likely(p[i])) {\n\t\t\tslab = virt_to_slab(p[i]);\n\n\t\t\tif (!slab_objcgs(slab) &&\n\t\t\t    memcg_alloc_slab_cgroups(slab, s, flags,\n\t\t\t\t\t\t\t false)) {\n\t\t\t\tobj_cgroup_uncharge(objcg, obj_full_size(s));\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\toff = obj_to_index(s, slab, p[i]);\n\t\t\tobj_cgroup_get(objcg);\n\t\t\tslab_objcgs(slab)[off] = objcg;\n\t\t\tmod_objcg_state(objcg, slab_pgdat(slab),\n\t\t\t\t\tcache_vmstat_idx(s), obj_full_size(s));\n\t\t} else {\n\t\t\tobj_cgroup_uncharge(objcg, obj_full_size(s));\n\t\t}\n\t}\n\tobj_cgroup_put(objcg);\n}\n\nstatic inline void memcg_slab_free_hook(struct kmem_cache *s, struct slab *slab,\n\t\t\t\t\tvoid **p, int objects)\n{\n\tstruct obj_cgroup **objcgs;\n\tint i;\n\n\tif (!memcg_kmem_online())\n\t\treturn;\n\n\tobjcgs = slab_objcgs(slab);\n\tif (!objcgs)\n\t\treturn;\n\n\tfor (i = 0; i < objects; i++) {\n\t\tstruct obj_cgroup *objcg;\n\t\tunsigned int off;\n\n\t\toff = obj_to_index(s, slab, p[i]);\n\t\tobjcg = objcgs[off];\n\t\tif (!objcg)\n\t\t\tcontinue;\n\n\t\tobjcgs[off] = NULL;\n\t\tobj_cgroup_uncharge(objcg, obj_full_size(s));\n\t\tmod_objcg_state(objcg, slab_pgdat(slab), cache_vmstat_idx(s),\n\t\t\t\t-obj_full_size(s));\n\t\tobj_cgroup_put(objcg);\n\t}\n}\n\n#else  \nstatic inline struct obj_cgroup **slab_objcgs(struct slab *slab)\n{\n\treturn NULL;\n}\n\nstatic inline struct mem_cgroup *memcg_from_slab_obj(void *ptr)\n{\n\treturn NULL;\n}\n\nstatic inline int memcg_alloc_slab_cgroups(struct slab *slab,\n\t\t\t\t\t       struct kmem_cache *s, gfp_t gfp,\n\t\t\t\t\t       bool new_slab)\n{\n\treturn 0;\n}\n\nstatic inline void memcg_free_slab_cgroups(struct slab *slab)\n{\n}\n\nstatic inline bool memcg_slab_pre_alloc_hook(struct kmem_cache *s,\n\t\t\t\t\t     struct list_lru *lru,\n\t\t\t\t\t     struct obj_cgroup **objcgp,\n\t\t\t\t\t     size_t objects, gfp_t flags)\n{\n\treturn true;\n}\n\nstatic inline void memcg_slab_post_alloc_hook(struct kmem_cache *s,\n\t\t\t\t\t      struct obj_cgroup *objcg,\n\t\t\t\t\t      gfp_t flags, size_t size,\n\t\t\t\t\t      void **p)\n{\n}\n\nstatic inline void memcg_slab_free_hook(struct kmem_cache *s, struct slab *slab,\n\t\t\t\t\tvoid **p, int objects)\n{\n}\n#endif  \n\nstatic inline struct kmem_cache *virt_to_cache(const void *obj)\n{\n\tstruct slab *slab;\n\n\tslab = virt_to_slab(obj);\n\tif (WARN_ONCE(!slab, \"%s: Object is not a Slab page!\\n\",\n\t\t\t\t\t__func__))\n\t\treturn NULL;\n\treturn slab->slab_cache;\n}\n\nstatic __always_inline void account_slab(struct slab *slab, int order,\n\t\t\t\t\t struct kmem_cache *s, gfp_t gfp)\n{\n\tif (memcg_kmem_online() && (s->flags & SLAB_ACCOUNT))\n\t\tmemcg_alloc_slab_cgroups(slab, s, gfp, true);\n\n\tmod_node_page_state(slab_pgdat(slab), cache_vmstat_idx(s),\n\t\t\t    PAGE_SIZE << order);\n}\n\nstatic __always_inline void unaccount_slab(struct slab *slab, int order,\n\t\t\t\t\t   struct kmem_cache *s)\n{\n\tif (memcg_kmem_online())\n\t\tmemcg_free_slab_cgroups(slab);\n\n\tmod_node_page_state(slab_pgdat(slab), cache_vmstat_idx(s),\n\t\t\t    -(PAGE_SIZE << order));\n}\n\nstatic inline struct kmem_cache *cache_from_obj(struct kmem_cache *s, void *x)\n{\n\tstruct kmem_cache *cachep;\n\n\tif (!IS_ENABLED(CONFIG_SLAB_FREELIST_HARDENED) &&\n\t    !kmem_cache_debug_flags(s, SLAB_CONSISTENCY_CHECKS))\n\t\treturn s;\n\n\tcachep = virt_to_cache(x);\n\tif (WARN(cachep && cachep != s,\n\t\t  \"%s: Wrong slab cache. %s but object is from %s\\n\",\n\t\t  __func__, s->name, cachep->name))\n\t\tprint_tracking(cachep, x);\n\treturn cachep;\n}\n\nvoid free_large_kmalloc(struct folio *folio, void *object);\n\nsize_t __ksize(const void *objp);\n\nstatic inline size_t slab_ksize(const struct kmem_cache *s)\n{\n#ifndef CONFIG_SLUB\n\treturn s->object_size;\n\n#else  \n# ifdef CONFIG_SLUB_DEBUG\n\t \n\tif (s->flags & (SLAB_RED_ZONE | SLAB_POISON))\n\t\treturn s->object_size;\n# endif\n\tif (s->flags & SLAB_KASAN)\n\t\treturn s->object_size;\n\t \n\tif (s->flags & (SLAB_TYPESAFE_BY_RCU | SLAB_STORE_USER))\n\t\treturn s->inuse;\n\t \n\treturn s->size;\n#endif\n}\n\nstatic inline struct kmem_cache *slab_pre_alloc_hook(struct kmem_cache *s,\n\t\t\t\t\t\t     struct list_lru *lru,\n\t\t\t\t\t\t     struct obj_cgroup **objcgp,\n\t\t\t\t\t\t     size_t size, gfp_t flags)\n{\n\tflags &= gfp_allowed_mask;\n\n\tmight_alloc(flags);\n\n\tif (should_failslab(s, flags))\n\t\treturn NULL;\n\n\tif (!memcg_slab_pre_alloc_hook(s, lru, objcgp, size, flags))\n\t\treturn NULL;\n\n\treturn s;\n}\n\nstatic inline void slab_post_alloc_hook(struct kmem_cache *s,\n\t\t\t\t\tstruct obj_cgroup *objcg, gfp_t flags,\n\t\t\t\t\tsize_t size, void **p, bool init,\n\t\t\t\t\tunsigned int orig_size)\n{\n\tunsigned int zero_size = s->object_size;\n\tbool kasan_init = init;\n\tsize_t i;\n\n\tflags &= gfp_allowed_mask;\n\n\t \n\tif (kmem_cache_debug_flags(s, SLAB_STORE_USER | SLAB_RED_ZONE) &&\n\t    (s->flags & SLAB_KMALLOC))\n\t\tzero_size = orig_size;\n\n\t \n\tif (__slub_debug_enabled())\n\t\tkasan_init = false;\n\n\t \n\tfor (i = 0; i < size; i++) {\n\t\tp[i] = kasan_slab_alloc(s, p[i], flags, kasan_init);\n\t\tif (p[i] && init && (!kasan_init || !kasan_has_integrated_init()))\n\t\t\tmemset(p[i], 0, zero_size);\n\t\tkmemleak_alloc_recursive(p[i], s->object_size, 1,\n\t\t\t\t\t s->flags, flags);\n\t\tkmsan_slab_alloc(s, p[i], flags);\n\t}\n\n\tmemcg_slab_post_alloc_hook(s, objcg, flags, size, p);\n}\n\n \nstruct kmem_cache_node {\n#ifdef CONFIG_SLAB\n\traw_spinlock_t list_lock;\n\tstruct list_head slabs_partial;\t \n\tstruct list_head slabs_full;\n\tstruct list_head slabs_free;\n\tunsigned long total_slabs;\t \n\tunsigned long free_slabs;\t \n\tunsigned long free_objects;\n\tunsigned int free_limit;\n\tunsigned int colour_next;\t \n\tstruct array_cache *shared;\t \n\tstruct alien_cache **alien;\t \n\tunsigned long next_reap;\t \n\tint free_touched;\t\t \n#endif\n\n#ifdef CONFIG_SLUB\n\tspinlock_t list_lock;\n\tunsigned long nr_partial;\n\tstruct list_head partial;\n#ifdef CONFIG_SLUB_DEBUG\n\tatomic_long_t nr_slabs;\n\tatomic_long_t total_objects;\n\tstruct list_head full;\n#endif\n#endif\n\n};\n\nstatic inline struct kmem_cache_node *get_node(struct kmem_cache *s, int node)\n{\n\treturn s->node[node];\n}\n\n \n#define for_each_kmem_cache_node(__s, __node, __n) \\\n\tfor (__node = 0; __node < nr_node_ids; __node++) \\\n\t\t if ((__n = get_node(__s, __node)))\n\n\n#if defined(CONFIG_SLAB) || defined(CONFIG_SLUB_DEBUG)\nvoid dump_unreclaimable_slab(void);\n#else\nstatic inline void dump_unreclaimable_slab(void)\n{\n}\n#endif\n\nvoid ___cache_free(struct kmem_cache *cache, void *x, unsigned long addr);\n\n#ifdef CONFIG_SLAB_FREELIST_RANDOM\nint cache_random_seq_create(struct kmem_cache *cachep, unsigned int count,\n\t\t\tgfp_t gfp);\nvoid cache_random_seq_destroy(struct kmem_cache *cachep);\n#else\nstatic inline int cache_random_seq_create(struct kmem_cache *cachep,\n\t\t\t\t\tunsigned int count, gfp_t gfp)\n{\n\treturn 0;\n}\nstatic inline void cache_random_seq_destroy(struct kmem_cache *cachep) { }\n#endif  \n\nstatic inline bool slab_want_init_on_alloc(gfp_t flags, struct kmem_cache *c)\n{\n\tif (static_branch_maybe(CONFIG_INIT_ON_ALLOC_DEFAULT_ON,\n\t\t\t\t&init_on_alloc)) {\n\t\tif (c->ctor)\n\t\t\treturn false;\n\t\tif (c->flags & (SLAB_TYPESAFE_BY_RCU | SLAB_POISON))\n\t\t\treturn flags & __GFP_ZERO;\n\t\treturn true;\n\t}\n\treturn flags & __GFP_ZERO;\n}\n\nstatic inline bool slab_want_init_on_free(struct kmem_cache *c)\n{\n\tif (static_branch_maybe(CONFIG_INIT_ON_FREE_DEFAULT_ON,\n\t\t\t\t&init_on_free))\n\t\treturn !(c->ctor ||\n\t\t\t (c->flags & (SLAB_TYPESAFE_BY_RCU | SLAB_POISON)));\n\treturn false;\n}\n\n#if defined(CONFIG_DEBUG_FS) && defined(CONFIG_SLUB_DEBUG)\nvoid debugfs_slab_release(struct kmem_cache *);\n#else\nstatic inline void debugfs_slab_release(struct kmem_cache *s) { }\n#endif\n\n#ifdef CONFIG_PRINTK\n#define KS_ADDRS_COUNT 16\nstruct kmem_obj_info {\n\tvoid *kp_ptr;\n\tstruct slab *kp_slab;\n\tvoid *kp_objp;\n\tunsigned long kp_data_offset;\n\tstruct kmem_cache *kp_slab_cache;\n\tvoid *kp_ret;\n\tvoid *kp_stack[KS_ADDRS_COUNT];\n\tvoid *kp_free_stack[KS_ADDRS_COUNT];\n};\nvoid __kmem_obj_info(struct kmem_obj_info *kpp, void *object, struct slab *slab);\n#endif\n\nvoid __check_heap_object(const void *ptr, unsigned long n,\n\t\t\t const struct slab *slab, bool to_user);\n\n#ifdef CONFIG_SLUB_DEBUG\nvoid skip_orig_size_check(struct kmem_cache *s, const void *object);\n#endif\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}