{
  "module_name": "vmalloc.c",
  "hash_id": "c2ac6562f545c67d80157bf0b353324752937eaedcce846b525f9cc170c40947",
  "original_prompt": "Ingested from linux-6.6.14/mm/vmalloc.c",
  "human_readable_source": "\n \n\n#include <linux/vmalloc.h>\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <linux/highmem.h>\n#include <linux/sched/signal.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n#include <linux/interrupt.h>\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <linux/set_memory.h>\n#include <linux/debugobjects.h>\n#include <linux/kallsyms.h>\n#include <linux/list.h>\n#include <linux/notifier.h>\n#include <linux/rbtree.h>\n#include <linux/xarray.h>\n#include <linux/io.h>\n#include <linux/rcupdate.h>\n#include <linux/pfn.h>\n#include <linux/kmemleak.h>\n#include <linux/atomic.h>\n#include <linux/compiler.h>\n#include <linux/memcontrol.h>\n#include <linux/llist.h>\n#include <linux/uio.h>\n#include <linux/bitops.h>\n#include <linux/rbtree_augmented.h>\n#include <linux/overflow.h>\n#include <linux/pgtable.h>\n#include <linux/hugetlb.h>\n#include <linux/sched/mm.h>\n#include <asm/tlbflush.h>\n#include <asm/shmparam.h>\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/vmalloc.h>\n\n#include \"internal.h\"\n#include \"pgalloc-track.h\"\n\n#ifdef CONFIG_HAVE_ARCH_HUGE_VMAP\nstatic unsigned int __ro_after_init ioremap_max_page_shift = BITS_PER_LONG - 1;\n\nstatic int __init set_nohugeiomap(char *str)\n{\n\tioremap_max_page_shift = PAGE_SHIFT;\n\treturn 0;\n}\nearly_param(\"nohugeiomap\", set_nohugeiomap);\n#else  \nstatic const unsigned int ioremap_max_page_shift = PAGE_SHIFT;\n#endif\t \n\n#ifdef CONFIG_HAVE_ARCH_HUGE_VMALLOC\nstatic bool __ro_after_init vmap_allow_huge = true;\n\nstatic int __init set_nohugevmalloc(char *str)\n{\n\tvmap_allow_huge = false;\n\treturn 0;\n}\nearly_param(\"nohugevmalloc\", set_nohugevmalloc);\n#else  \nstatic const bool vmap_allow_huge = false;\n#endif\t \n\nbool is_vmalloc_addr(const void *x)\n{\n\tunsigned long addr = (unsigned long)kasan_reset_tag(x);\n\n\treturn addr >= VMALLOC_START && addr < VMALLOC_END;\n}\nEXPORT_SYMBOL(is_vmalloc_addr);\n\nstruct vfree_deferred {\n\tstruct llist_head list;\n\tstruct work_struct wq;\n};\nstatic DEFINE_PER_CPU(struct vfree_deferred, vfree_deferred);\n\n \nstatic int vmap_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,\n\t\t\tphys_addr_t phys_addr, pgprot_t prot,\n\t\t\tunsigned int max_page_shift, pgtbl_mod_mask *mask)\n{\n\tpte_t *pte;\n\tu64 pfn;\n\tunsigned long size = PAGE_SIZE;\n\n\tpfn = phys_addr >> PAGE_SHIFT;\n\tpte = pte_alloc_kernel_track(pmd, addr, mask);\n\tif (!pte)\n\t\treturn -ENOMEM;\n\tdo {\n\t\tBUG_ON(!pte_none(ptep_get(pte)));\n\n#ifdef CONFIG_HUGETLB_PAGE\n\t\tsize = arch_vmap_pte_range_map_size(addr, end, pfn, max_page_shift);\n\t\tif (size != PAGE_SIZE) {\n\t\t\tpte_t entry = pfn_pte(pfn, prot);\n\n\t\t\tentry = arch_make_huge_pte(entry, ilog2(size), 0);\n\t\t\tset_huge_pte_at(&init_mm, addr, pte, entry, size);\n\t\t\tpfn += PFN_DOWN(size);\n\t\t\tcontinue;\n\t\t}\n#endif\n\t\tset_pte_at(&init_mm, addr, pte, pfn_pte(pfn, prot));\n\t\tpfn++;\n\t} while (pte += PFN_DOWN(size), addr += size, addr != end);\n\t*mask |= PGTBL_PTE_MODIFIED;\n\treturn 0;\n}\n\nstatic int vmap_try_huge_pmd(pmd_t *pmd, unsigned long addr, unsigned long end,\n\t\t\tphys_addr_t phys_addr, pgprot_t prot,\n\t\t\tunsigned int max_page_shift)\n{\n\tif (max_page_shift < PMD_SHIFT)\n\t\treturn 0;\n\n\tif (!arch_vmap_pmd_supported(prot))\n\t\treturn 0;\n\n\tif ((end - addr) != PMD_SIZE)\n\t\treturn 0;\n\n\tif (!IS_ALIGNED(addr, PMD_SIZE))\n\t\treturn 0;\n\n\tif (!IS_ALIGNED(phys_addr, PMD_SIZE))\n\t\treturn 0;\n\n\tif (pmd_present(*pmd) && !pmd_free_pte_page(pmd, addr))\n\t\treturn 0;\n\n\treturn pmd_set_huge(pmd, phys_addr, prot);\n}\n\nstatic int vmap_pmd_range(pud_t *pud, unsigned long addr, unsigned long end,\n\t\t\tphys_addr_t phys_addr, pgprot_t prot,\n\t\t\tunsigned int max_page_shift, pgtbl_mod_mask *mask)\n{\n\tpmd_t *pmd;\n\tunsigned long next;\n\n\tpmd = pmd_alloc_track(&init_mm, pud, addr, mask);\n\tif (!pmd)\n\t\treturn -ENOMEM;\n\tdo {\n\t\tnext = pmd_addr_end(addr, end);\n\n\t\tif (vmap_try_huge_pmd(pmd, addr, next, phys_addr, prot,\n\t\t\t\t\tmax_page_shift)) {\n\t\t\t*mask |= PGTBL_PMD_MODIFIED;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (vmap_pte_range(pmd, addr, next, phys_addr, prot, max_page_shift, mask))\n\t\t\treturn -ENOMEM;\n\t} while (pmd++, phys_addr += (next - addr), addr = next, addr != end);\n\treturn 0;\n}\n\nstatic int vmap_try_huge_pud(pud_t *pud, unsigned long addr, unsigned long end,\n\t\t\tphys_addr_t phys_addr, pgprot_t prot,\n\t\t\tunsigned int max_page_shift)\n{\n\tif (max_page_shift < PUD_SHIFT)\n\t\treturn 0;\n\n\tif (!arch_vmap_pud_supported(prot))\n\t\treturn 0;\n\n\tif ((end - addr) != PUD_SIZE)\n\t\treturn 0;\n\n\tif (!IS_ALIGNED(addr, PUD_SIZE))\n\t\treturn 0;\n\n\tif (!IS_ALIGNED(phys_addr, PUD_SIZE))\n\t\treturn 0;\n\n\tif (pud_present(*pud) && !pud_free_pmd_page(pud, addr))\n\t\treturn 0;\n\n\treturn pud_set_huge(pud, phys_addr, prot);\n}\n\nstatic int vmap_pud_range(p4d_t *p4d, unsigned long addr, unsigned long end,\n\t\t\tphys_addr_t phys_addr, pgprot_t prot,\n\t\t\tunsigned int max_page_shift, pgtbl_mod_mask *mask)\n{\n\tpud_t *pud;\n\tunsigned long next;\n\n\tpud = pud_alloc_track(&init_mm, p4d, addr, mask);\n\tif (!pud)\n\t\treturn -ENOMEM;\n\tdo {\n\t\tnext = pud_addr_end(addr, end);\n\n\t\tif (vmap_try_huge_pud(pud, addr, next, phys_addr, prot,\n\t\t\t\t\tmax_page_shift)) {\n\t\t\t*mask |= PGTBL_PUD_MODIFIED;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (vmap_pmd_range(pud, addr, next, phys_addr, prot,\n\t\t\t\t\tmax_page_shift, mask))\n\t\t\treturn -ENOMEM;\n\t} while (pud++, phys_addr += (next - addr), addr = next, addr != end);\n\treturn 0;\n}\n\nstatic int vmap_try_huge_p4d(p4d_t *p4d, unsigned long addr, unsigned long end,\n\t\t\tphys_addr_t phys_addr, pgprot_t prot,\n\t\t\tunsigned int max_page_shift)\n{\n\tif (max_page_shift < P4D_SHIFT)\n\t\treturn 0;\n\n\tif (!arch_vmap_p4d_supported(prot))\n\t\treturn 0;\n\n\tif ((end - addr) != P4D_SIZE)\n\t\treturn 0;\n\n\tif (!IS_ALIGNED(addr, P4D_SIZE))\n\t\treturn 0;\n\n\tif (!IS_ALIGNED(phys_addr, P4D_SIZE))\n\t\treturn 0;\n\n\tif (p4d_present(*p4d) && !p4d_free_pud_page(p4d, addr))\n\t\treturn 0;\n\n\treturn p4d_set_huge(p4d, phys_addr, prot);\n}\n\nstatic int vmap_p4d_range(pgd_t *pgd, unsigned long addr, unsigned long end,\n\t\t\tphys_addr_t phys_addr, pgprot_t prot,\n\t\t\tunsigned int max_page_shift, pgtbl_mod_mask *mask)\n{\n\tp4d_t *p4d;\n\tunsigned long next;\n\n\tp4d = p4d_alloc_track(&init_mm, pgd, addr, mask);\n\tif (!p4d)\n\t\treturn -ENOMEM;\n\tdo {\n\t\tnext = p4d_addr_end(addr, end);\n\n\t\tif (vmap_try_huge_p4d(p4d, addr, next, phys_addr, prot,\n\t\t\t\t\tmax_page_shift)) {\n\t\t\t*mask |= PGTBL_P4D_MODIFIED;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (vmap_pud_range(p4d, addr, next, phys_addr, prot,\n\t\t\t\t\tmax_page_shift, mask))\n\t\t\treturn -ENOMEM;\n\t} while (p4d++, phys_addr += (next - addr), addr = next, addr != end);\n\treturn 0;\n}\n\nstatic int vmap_range_noflush(unsigned long addr, unsigned long end,\n\t\t\tphys_addr_t phys_addr, pgprot_t prot,\n\t\t\tunsigned int max_page_shift)\n{\n\tpgd_t *pgd;\n\tunsigned long start;\n\tunsigned long next;\n\tint err;\n\tpgtbl_mod_mask mask = 0;\n\n\tmight_sleep();\n\tBUG_ON(addr >= end);\n\n\tstart = addr;\n\tpgd = pgd_offset_k(addr);\n\tdo {\n\t\tnext = pgd_addr_end(addr, end);\n\t\terr = vmap_p4d_range(pgd, addr, next, phys_addr, prot,\n\t\t\t\t\tmax_page_shift, &mask);\n\t\tif (err)\n\t\t\tbreak;\n\t} while (pgd++, phys_addr += (next - addr), addr = next, addr != end);\n\n\tif (mask & ARCH_PAGE_TABLE_SYNC_MASK)\n\t\tarch_sync_kernel_mappings(start, end);\n\n\treturn err;\n}\n\nint ioremap_page_range(unsigned long addr, unsigned long end,\n\t\tphys_addr_t phys_addr, pgprot_t prot)\n{\n\tint err;\n\n\terr = vmap_range_noflush(addr, end, phys_addr, pgprot_nx(prot),\n\t\t\t\t ioremap_max_page_shift);\n\tflush_cache_vmap(addr, end);\n\tif (!err)\n\t\terr = kmsan_ioremap_page_range(addr, end, phys_addr, prot,\n\t\t\t\t\t       ioremap_max_page_shift);\n\treturn err;\n}\n\nstatic void vunmap_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,\n\t\t\t     pgtbl_mod_mask *mask)\n{\n\tpte_t *pte;\n\n\tpte = pte_offset_kernel(pmd, addr);\n\tdo {\n\t\tpte_t ptent = ptep_get_and_clear(&init_mm, addr, pte);\n\t\tWARN_ON(!pte_none(ptent) && !pte_present(ptent));\n\t} while (pte++, addr += PAGE_SIZE, addr != end);\n\t*mask |= PGTBL_PTE_MODIFIED;\n}\n\nstatic void vunmap_pmd_range(pud_t *pud, unsigned long addr, unsigned long end,\n\t\t\t     pgtbl_mod_mask *mask)\n{\n\tpmd_t *pmd;\n\tunsigned long next;\n\tint cleared;\n\n\tpmd = pmd_offset(pud, addr);\n\tdo {\n\t\tnext = pmd_addr_end(addr, end);\n\n\t\tcleared = pmd_clear_huge(pmd);\n\t\tif (cleared || pmd_bad(*pmd))\n\t\t\t*mask |= PGTBL_PMD_MODIFIED;\n\n\t\tif (cleared)\n\t\t\tcontinue;\n\t\tif (pmd_none_or_clear_bad(pmd))\n\t\t\tcontinue;\n\t\tvunmap_pte_range(pmd, addr, next, mask);\n\n\t\tcond_resched();\n\t} while (pmd++, addr = next, addr != end);\n}\n\nstatic void vunmap_pud_range(p4d_t *p4d, unsigned long addr, unsigned long end,\n\t\t\t     pgtbl_mod_mask *mask)\n{\n\tpud_t *pud;\n\tunsigned long next;\n\tint cleared;\n\n\tpud = pud_offset(p4d, addr);\n\tdo {\n\t\tnext = pud_addr_end(addr, end);\n\n\t\tcleared = pud_clear_huge(pud);\n\t\tif (cleared || pud_bad(*pud))\n\t\t\t*mask |= PGTBL_PUD_MODIFIED;\n\n\t\tif (cleared)\n\t\t\tcontinue;\n\t\tif (pud_none_or_clear_bad(pud))\n\t\t\tcontinue;\n\t\tvunmap_pmd_range(pud, addr, next, mask);\n\t} while (pud++, addr = next, addr != end);\n}\n\nstatic void vunmap_p4d_range(pgd_t *pgd, unsigned long addr, unsigned long end,\n\t\t\t     pgtbl_mod_mask *mask)\n{\n\tp4d_t *p4d;\n\tunsigned long next;\n\n\tp4d = p4d_offset(pgd, addr);\n\tdo {\n\t\tnext = p4d_addr_end(addr, end);\n\n\t\tp4d_clear_huge(p4d);\n\t\tif (p4d_bad(*p4d))\n\t\t\t*mask |= PGTBL_P4D_MODIFIED;\n\n\t\tif (p4d_none_or_clear_bad(p4d))\n\t\t\tcontinue;\n\t\tvunmap_pud_range(p4d, addr, next, mask);\n\t} while (p4d++, addr = next, addr != end);\n}\n\n \nvoid __vunmap_range_noflush(unsigned long start, unsigned long end)\n{\n\tunsigned long next;\n\tpgd_t *pgd;\n\tunsigned long addr = start;\n\tpgtbl_mod_mask mask = 0;\n\n\tBUG_ON(addr >= end);\n\tpgd = pgd_offset_k(addr);\n\tdo {\n\t\tnext = pgd_addr_end(addr, end);\n\t\tif (pgd_bad(*pgd))\n\t\t\tmask |= PGTBL_PGD_MODIFIED;\n\t\tif (pgd_none_or_clear_bad(pgd))\n\t\t\tcontinue;\n\t\tvunmap_p4d_range(pgd, addr, next, &mask);\n\t} while (pgd++, addr = next, addr != end);\n\n\tif (mask & ARCH_PAGE_TABLE_SYNC_MASK)\n\t\tarch_sync_kernel_mappings(start, end);\n}\n\nvoid vunmap_range_noflush(unsigned long start, unsigned long end)\n{\n\tkmsan_vunmap_range_noflush(start, end);\n\t__vunmap_range_noflush(start, end);\n}\n\n \nvoid vunmap_range(unsigned long addr, unsigned long end)\n{\n\tflush_cache_vunmap(addr, end);\n\tvunmap_range_noflush(addr, end);\n\tflush_tlb_kernel_range(addr, end);\n}\n\nstatic int vmap_pages_pte_range(pmd_t *pmd, unsigned long addr,\n\t\tunsigned long end, pgprot_t prot, struct page **pages, int *nr,\n\t\tpgtbl_mod_mask *mask)\n{\n\tpte_t *pte;\n\n\t \n\n\tpte = pte_alloc_kernel_track(pmd, addr, mask);\n\tif (!pte)\n\t\treturn -ENOMEM;\n\tdo {\n\t\tstruct page *page = pages[*nr];\n\n\t\tif (WARN_ON(!pte_none(ptep_get(pte))))\n\t\t\treturn -EBUSY;\n\t\tif (WARN_ON(!page))\n\t\t\treturn -ENOMEM;\n\t\tif (WARN_ON(!pfn_valid(page_to_pfn(page))))\n\t\t\treturn -EINVAL;\n\n\t\tset_pte_at(&init_mm, addr, pte, mk_pte(page, prot));\n\t\t(*nr)++;\n\t} while (pte++, addr += PAGE_SIZE, addr != end);\n\t*mask |= PGTBL_PTE_MODIFIED;\n\treturn 0;\n}\n\nstatic int vmap_pages_pmd_range(pud_t *pud, unsigned long addr,\n\t\tunsigned long end, pgprot_t prot, struct page **pages, int *nr,\n\t\tpgtbl_mod_mask *mask)\n{\n\tpmd_t *pmd;\n\tunsigned long next;\n\n\tpmd = pmd_alloc_track(&init_mm, pud, addr, mask);\n\tif (!pmd)\n\t\treturn -ENOMEM;\n\tdo {\n\t\tnext = pmd_addr_end(addr, end);\n\t\tif (vmap_pages_pte_range(pmd, addr, next, prot, pages, nr, mask))\n\t\t\treturn -ENOMEM;\n\t} while (pmd++, addr = next, addr != end);\n\treturn 0;\n}\n\nstatic int vmap_pages_pud_range(p4d_t *p4d, unsigned long addr,\n\t\tunsigned long end, pgprot_t prot, struct page **pages, int *nr,\n\t\tpgtbl_mod_mask *mask)\n{\n\tpud_t *pud;\n\tunsigned long next;\n\n\tpud = pud_alloc_track(&init_mm, p4d, addr, mask);\n\tif (!pud)\n\t\treturn -ENOMEM;\n\tdo {\n\t\tnext = pud_addr_end(addr, end);\n\t\tif (vmap_pages_pmd_range(pud, addr, next, prot, pages, nr, mask))\n\t\t\treturn -ENOMEM;\n\t} while (pud++, addr = next, addr != end);\n\treturn 0;\n}\n\nstatic int vmap_pages_p4d_range(pgd_t *pgd, unsigned long addr,\n\t\tunsigned long end, pgprot_t prot, struct page **pages, int *nr,\n\t\tpgtbl_mod_mask *mask)\n{\n\tp4d_t *p4d;\n\tunsigned long next;\n\n\tp4d = p4d_alloc_track(&init_mm, pgd, addr, mask);\n\tif (!p4d)\n\t\treturn -ENOMEM;\n\tdo {\n\t\tnext = p4d_addr_end(addr, end);\n\t\tif (vmap_pages_pud_range(p4d, addr, next, prot, pages, nr, mask))\n\t\t\treturn -ENOMEM;\n\t} while (p4d++, addr = next, addr != end);\n\treturn 0;\n}\n\nstatic int vmap_small_pages_range_noflush(unsigned long addr, unsigned long end,\n\t\tpgprot_t prot, struct page **pages)\n{\n\tunsigned long start = addr;\n\tpgd_t *pgd;\n\tunsigned long next;\n\tint err = 0;\n\tint nr = 0;\n\tpgtbl_mod_mask mask = 0;\n\n\tBUG_ON(addr >= end);\n\tpgd = pgd_offset_k(addr);\n\tdo {\n\t\tnext = pgd_addr_end(addr, end);\n\t\tif (pgd_bad(*pgd))\n\t\t\tmask |= PGTBL_PGD_MODIFIED;\n\t\terr = vmap_pages_p4d_range(pgd, addr, next, prot, pages, &nr, &mask);\n\t\tif (err)\n\t\t\treturn err;\n\t} while (pgd++, addr = next, addr != end);\n\n\tif (mask & ARCH_PAGE_TABLE_SYNC_MASK)\n\t\tarch_sync_kernel_mappings(start, end);\n\n\treturn 0;\n}\n\n \nint __vmap_pages_range_noflush(unsigned long addr, unsigned long end,\n\t\tpgprot_t prot, struct page **pages, unsigned int page_shift)\n{\n\tunsigned int i, nr = (end - addr) >> PAGE_SHIFT;\n\n\tWARN_ON(page_shift < PAGE_SHIFT);\n\n\tif (!IS_ENABLED(CONFIG_HAVE_ARCH_HUGE_VMALLOC) ||\n\t\t\tpage_shift == PAGE_SHIFT)\n\t\treturn vmap_small_pages_range_noflush(addr, end, prot, pages);\n\n\tfor (i = 0; i < nr; i += 1U << (page_shift - PAGE_SHIFT)) {\n\t\tint err;\n\n\t\terr = vmap_range_noflush(addr, addr + (1UL << page_shift),\n\t\t\t\t\tpage_to_phys(pages[i]), prot,\n\t\t\t\t\tpage_shift);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\taddr += 1UL << page_shift;\n\t}\n\n\treturn 0;\n}\n\nint vmap_pages_range_noflush(unsigned long addr, unsigned long end,\n\t\tpgprot_t prot, struct page **pages, unsigned int page_shift)\n{\n\tint ret = kmsan_vmap_pages_range_noflush(addr, end, prot, pages,\n\t\t\t\t\t\t page_shift);\n\n\tif (ret)\n\t\treturn ret;\n\treturn __vmap_pages_range_noflush(addr, end, prot, pages, page_shift);\n}\n\n \nstatic int vmap_pages_range(unsigned long addr, unsigned long end,\n\t\tpgprot_t prot, struct page **pages, unsigned int page_shift)\n{\n\tint err;\n\n\terr = vmap_pages_range_noflush(addr, end, prot, pages, page_shift);\n\tflush_cache_vmap(addr, end);\n\treturn err;\n}\n\nint is_vmalloc_or_module_addr(const void *x)\n{\n\t \n#if defined(CONFIG_MODULES) && defined(MODULES_VADDR)\n\tunsigned long addr = (unsigned long)kasan_reset_tag(x);\n\tif (addr >= MODULES_VADDR && addr < MODULES_END)\n\t\treturn 1;\n#endif\n\treturn is_vmalloc_addr(x);\n}\nEXPORT_SYMBOL_GPL(is_vmalloc_or_module_addr);\n\n \nstruct page *vmalloc_to_page(const void *vmalloc_addr)\n{\n\tunsigned long addr = (unsigned long) vmalloc_addr;\n\tstruct page *page = NULL;\n\tpgd_t *pgd = pgd_offset_k(addr);\n\tp4d_t *p4d;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\tpte_t *ptep, pte;\n\n\t \n\tVIRTUAL_BUG_ON(!is_vmalloc_or_module_addr(vmalloc_addr));\n\n\tif (pgd_none(*pgd))\n\t\treturn NULL;\n\tif (WARN_ON_ONCE(pgd_leaf(*pgd)))\n\t\treturn NULL;  \n\tif (WARN_ON_ONCE(pgd_bad(*pgd)))\n\t\treturn NULL;\n\n\tp4d = p4d_offset(pgd, addr);\n\tif (p4d_none(*p4d))\n\t\treturn NULL;\n\tif (p4d_leaf(*p4d))\n\t\treturn p4d_page(*p4d) + ((addr & ~P4D_MASK) >> PAGE_SHIFT);\n\tif (WARN_ON_ONCE(p4d_bad(*p4d)))\n\t\treturn NULL;\n\n\tpud = pud_offset(p4d, addr);\n\tif (pud_none(*pud))\n\t\treturn NULL;\n\tif (pud_leaf(*pud))\n\t\treturn pud_page(*pud) + ((addr & ~PUD_MASK) >> PAGE_SHIFT);\n\tif (WARN_ON_ONCE(pud_bad(*pud)))\n\t\treturn NULL;\n\n\tpmd = pmd_offset(pud, addr);\n\tif (pmd_none(*pmd))\n\t\treturn NULL;\n\tif (pmd_leaf(*pmd))\n\t\treturn pmd_page(*pmd) + ((addr & ~PMD_MASK) >> PAGE_SHIFT);\n\tif (WARN_ON_ONCE(pmd_bad(*pmd)))\n\t\treturn NULL;\n\n\tptep = pte_offset_kernel(pmd, addr);\n\tpte = ptep_get(ptep);\n\tif (pte_present(pte))\n\t\tpage = pte_page(pte);\n\n\treturn page;\n}\nEXPORT_SYMBOL(vmalloc_to_page);\n\n \nunsigned long vmalloc_to_pfn(const void *vmalloc_addr)\n{\n\treturn page_to_pfn(vmalloc_to_page(vmalloc_addr));\n}\nEXPORT_SYMBOL(vmalloc_to_pfn);\n\n\n \n\n#define DEBUG_AUGMENT_PROPAGATE_CHECK 0\n#define DEBUG_AUGMENT_LOWEST_MATCH_CHECK 0\n\n\nstatic DEFINE_SPINLOCK(vmap_area_lock);\nstatic DEFINE_SPINLOCK(free_vmap_area_lock);\n \nLIST_HEAD(vmap_area_list);\nstatic struct rb_root vmap_area_root = RB_ROOT;\nstatic bool vmap_initialized __read_mostly;\n\nstatic struct rb_root purge_vmap_area_root = RB_ROOT;\nstatic LIST_HEAD(purge_vmap_area_list);\nstatic DEFINE_SPINLOCK(purge_vmap_area_lock);\n\n \nstatic struct kmem_cache *vmap_area_cachep;\n\n \nstatic LIST_HEAD(free_vmap_area_list);\n\n \nstatic struct rb_root free_vmap_area_root = RB_ROOT;\n\n \nstatic DEFINE_PER_CPU(struct vmap_area *, ne_fit_preload_node);\n\nstatic __always_inline unsigned long\nva_size(struct vmap_area *va)\n{\n\treturn (va->va_end - va->va_start);\n}\n\nstatic __always_inline unsigned long\nget_subtree_max_size(struct rb_node *node)\n{\n\tstruct vmap_area *va;\n\n\tva = rb_entry_safe(node, struct vmap_area, rb_node);\n\treturn va ? va->subtree_max_size : 0;\n}\n\nRB_DECLARE_CALLBACKS_MAX(static, free_vmap_area_rb_augment_cb,\n\tstruct vmap_area, rb_node, unsigned long, subtree_max_size, va_size)\n\nstatic void reclaim_and_purge_vmap_areas(void);\nstatic BLOCKING_NOTIFIER_HEAD(vmap_notify_list);\nstatic void drain_vmap_area_work(struct work_struct *work);\nstatic DECLARE_WORK(drain_vmap_work, drain_vmap_area_work);\n\nstatic atomic_long_t nr_vmalloc_pages;\n\nunsigned long vmalloc_nr_pages(void)\n{\n\treturn atomic_long_read(&nr_vmalloc_pages);\n}\n\n \nstatic struct vmap_area *find_vmap_area_exceed_addr(unsigned long addr)\n{\n\tstruct vmap_area *va = NULL;\n\tstruct rb_node *n = vmap_area_root.rb_node;\n\n\taddr = (unsigned long)kasan_reset_tag((void *)addr);\n\n\twhile (n) {\n\t\tstruct vmap_area *tmp;\n\n\t\ttmp = rb_entry(n, struct vmap_area, rb_node);\n\t\tif (tmp->va_end > addr) {\n\t\t\tva = tmp;\n\t\t\tif (tmp->va_start <= addr)\n\t\t\t\tbreak;\n\n\t\t\tn = n->rb_left;\n\t\t} else\n\t\t\tn = n->rb_right;\n\t}\n\n\treturn va;\n}\n\nstatic struct vmap_area *__find_vmap_area(unsigned long addr, struct rb_root *root)\n{\n\tstruct rb_node *n = root->rb_node;\n\n\taddr = (unsigned long)kasan_reset_tag((void *)addr);\n\n\twhile (n) {\n\t\tstruct vmap_area *va;\n\n\t\tva = rb_entry(n, struct vmap_area, rb_node);\n\t\tif (addr < va->va_start)\n\t\t\tn = n->rb_left;\n\t\telse if (addr >= va->va_end)\n\t\t\tn = n->rb_right;\n\t\telse\n\t\t\treturn va;\n\t}\n\n\treturn NULL;\n}\n\n \nstatic __always_inline struct rb_node **\nfind_va_links(struct vmap_area *va,\n\tstruct rb_root *root, struct rb_node *from,\n\tstruct rb_node **parent)\n{\n\tstruct vmap_area *tmp_va;\n\tstruct rb_node **link;\n\n\tif (root) {\n\t\tlink = &root->rb_node;\n\t\tif (unlikely(!*link)) {\n\t\t\t*parent = NULL;\n\t\t\treturn link;\n\t\t}\n\t} else {\n\t\tlink = &from;\n\t}\n\n\t \n\tdo {\n\t\ttmp_va = rb_entry(*link, struct vmap_area, rb_node);\n\n\t\t \n\t\tif (va->va_end <= tmp_va->va_start)\n\t\t\tlink = &(*link)->rb_left;\n\t\telse if (va->va_start >= tmp_va->va_end)\n\t\t\tlink = &(*link)->rb_right;\n\t\telse {\n\t\t\tWARN(1, \"vmalloc bug: 0x%lx-0x%lx overlaps with 0x%lx-0x%lx\\n\",\n\t\t\t\tva->va_start, va->va_end, tmp_va->va_start, tmp_va->va_end);\n\n\t\t\treturn NULL;\n\t\t}\n\t} while (*link);\n\n\t*parent = &tmp_va->rb_node;\n\treturn link;\n}\n\nstatic __always_inline struct list_head *\nget_va_next_sibling(struct rb_node *parent, struct rb_node **link)\n{\n\tstruct list_head *list;\n\n\tif (unlikely(!parent))\n\t\t \n\t\treturn NULL;\n\n\tlist = &rb_entry(parent, struct vmap_area, rb_node)->list;\n\treturn (&parent->rb_right == link ? list->next : list);\n}\n\nstatic __always_inline void\n__link_va(struct vmap_area *va, struct rb_root *root,\n\tstruct rb_node *parent, struct rb_node **link,\n\tstruct list_head *head, bool augment)\n{\n\t \n\tif (likely(parent)) {\n\t\thead = &rb_entry(parent, struct vmap_area, rb_node)->list;\n\t\tif (&parent->rb_right != link)\n\t\t\thead = head->prev;\n\t}\n\n\t \n\trb_link_node(&va->rb_node, parent, link);\n\tif (augment) {\n\t\t \n\t\trb_insert_augmented(&va->rb_node,\n\t\t\troot, &free_vmap_area_rb_augment_cb);\n\t\tva->subtree_max_size = 0;\n\t} else {\n\t\trb_insert_color(&va->rb_node, root);\n\t}\n\n\t \n\tlist_add(&va->list, head);\n}\n\nstatic __always_inline void\nlink_va(struct vmap_area *va, struct rb_root *root,\n\tstruct rb_node *parent, struct rb_node **link,\n\tstruct list_head *head)\n{\n\t__link_va(va, root, parent, link, head, false);\n}\n\nstatic __always_inline void\nlink_va_augment(struct vmap_area *va, struct rb_root *root,\n\tstruct rb_node *parent, struct rb_node **link,\n\tstruct list_head *head)\n{\n\t__link_va(va, root, parent, link, head, true);\n}\n\nstatic __always_inline void\n__unlink_va(struct vmap_area *va, struct rb_root *root, bool augment)\n{\n\tif (WARN_ON(RB_EMPTY_NODE(&va->rb_node)))\n\t\treturn;\n\n\tif (augment)\n\t\trb_erase_augmented(&va->rb_node,\n\t\t\troot, &free_vmap_area_rb_augment_cb);\n\telse\n\t\trb_erase(&va->rb_node, root);\n\n\tlist_del_init(&va->list);\n\tRB_CLEAR_NODE(&va->rb_node);\n}\n\nstatic __always_inline void\nunlink_va(struct vmap_area *va, struct rb_root *root)\n{\n\t__unlink_va(va, root, false);\n}\n\nstatic __always_inline void\nunlink_va_augment(struct vmap_area *va, struct rb_root *root)\n{\n\t__unlink_va(va, root, true);\n}\n\n#if DEBUG_AUGMENT_PROPAGATE_CHECK\n \nstatic __always_inline unsigned long\ncompute_subtree_max_size(struct vmap_area *va)\n{\n\treturn max3(va_size(va),\n\t\tget_subtree_max_size(va->rb_node.rb_left),\n\t\tget_subtree_max_size(va->rb_node.rb_right));\n}\n\nstatic void\naugment_tree_propagate_check(void)\n{\n\tstruct vmap_area *va;\n\tunsigned long computed_size;\n\n\tlist_for_each_entry(va, &free_vmap_area_list, list) {\n\t\tcomputed_size = compute_subtree_max_size(va);\n\t\tif (computed_size != va->subtree_max_size)\n\t\t\tpr_emerg(\"tree is corrupted: %lu, %lu\\n\",\n\t\t\t\tva_size(va), va->subtree_max_size);\n\t}\n}\n#endif\n\n \nstatic __always_inline void\naugment_tree_propagate_from(struct vmap_area *va)\n{\n\t \n\tfree_vmap_area_rb_augment_cb_propagate(&va->rb_node, NULL);\n\n#if DEBUG_AUGMENT_PROPAGATE_CHECK\n\taugment_tree_propagate_check();\n#endif\n}\n\nstatic void\ninsert_vmap_area(struct vmap_area *va,\n\tstruct rb_root *root, struct list_head *head)\n{\n\tstruct rb_node **link;\n\tstruct rb_node *parent;\n\n\tlink = find_va_links(va, root, NULL, &parent);\n\tif (link)\n\t\tlink_va(va, root, parent, link, head);\n}\n\nstatic void\ninsert_vmap_area_augment(struct vmap_area *va,\n\tstruct rb_node *from, struct rb_root *root,\n\tstruct list_head *head)\n{\n\tstruct rb_node **link;\n\tstruct rb_node *parent;\n\n\tif (from)\n\t\tlink = find_va_links(va, NULL, from, &parent);\n\telse\n\t\tlink = find_va_links(va, root, NULL, &parent);\n\n\tif (link) {\n\t\tlink_va_augment(va, root, parent, link, head);\n\t\taugment_tree_propagate_from(va);\n\t}\n}\n\n \nstatic __always_inline struct vmap_area *\n__merge_or_add_vmap_area(struct vmap_area *va,\n\tstruct rb_root *root, struct list_head *head, bool augment)\n{\n\tstruct vmap_area *sibling;\n\tstruct list_head *next;\n\tstruct rb_node **link;\n\tstruct rb_node *parent;\n\tbool merged = false;\n\n\t \n\tlink = find_va_links(va, root, NULL, &parent);\n\tif (!link)\n\t\treturn NULL;\n\n\t \n\tnext = get_va_next_sibling(parent, link);\n\tif (unlikely(next == NULL))\n\t\tgoto insert;\n\n\t \n\tif (next != head) {\n\t\tsibling = list_entry(next, struct vmap_area, list);\n\t\tif (sibling->va_start == va->va_end) {\n\t\t\tsibling->va_start = va->va_start;\n\n\t\t\t \n\t\t\tkmem_cache_free(vmap_area_cachep, va);\n\n\t\t\t \n\t\t\tva = sibling;\n\t\t\tmerged = true;\n\t\t}\n\t}\n\n\t \n\tif (next->prev != head) {\n\t\tsibling = list_entry(next->prev, struct vmap_area, list);\n\t\tif (sibling->va_end == va->va_start) {\n\t\t\t \n\t\t\tif (merged)\n\t\t\t\t__unlink_va(va, root, augment);\n\n\t\t\tsibling->va_end = va->va_end;\n\n\t\t\t \n\t\t\tkmem_cache_free(vmap_area_cachep, va);\n\n\t\t\t \n\t\t\tva = sibling;\n\t\t\tmerged = true;\n\t\t}\n\t}\n\ninsert:\n\tif (!merged)\n\t\t__link_va(va, root, parent, link, head, augment);\n\n\treturn va;\n}\n\nstatic __always_inline struct vmap_area *\nmerge_or_add_vmap_area(struct vmap_area *va,\n\tstruct rb_root *root, struct list_head *head)\n{\n\treturn __merge_or_add_vmap_area(va, root, head, false);\n}\n\nstatic __always_inline struct vmap_area *\nmerge_or_add_vmap_area_augment(struct vmap_area *va,\n\tstruct rb_root *root, struct list_head *head)\n{\n\tva = __merge_or_add_vmap_area(va, root, head, true);\n\tif (va)\n\t\taugment_tree_propagate_from(va);\n\n\treturn va;\n}\n\nstatic __always_inline bool\nis_within_this_va(struct vmap_area *va, unsigned long size,\n\tunsigned long align, unsigned long vstart)\n{\n\tunsigned long nva_start_addr;\n\n\tif (va->va_start > vstart)\n\t\tnva_start_addr = ALIGN(va->va_start, align);\n\telse\n\t\tnva_start_addr = ALIGN(vstart, align);\n\n\t \n\tif (nva_start_addr + size < nva_start_addr ||\n\t\t\tnva_start_addr < vstart)\n\t\treturn false;\n\n\treturn (nva_start_addr + size <= va->va_end);\n}\n\n \nstatic __always_inline struct vmap_area *\nfind_vmap_lowest_match(struct rb_root *root, unsigned long size,\n\tunsigned long align, unsigned long vstart, bool adjust_search_size)\n{\n\tstruct vmap_area *va;\n\tstruct rb_node *node;\n\tunsigned long length;\n\n\t \n\tnode = root->rb_node;\n\n\t \n\tlength = adjust_search_size ? size + align - 1 : size;\n\n\twhile (node) {\n\t\tva = rb_entry(node, struct vmap_area, rb_node);\n\n\t\tif (get_subtree_max_size(node->rb_left) >= length &&\n\t\t\t\tvstart < va->va_start) {\n\t\t\tnode = node->rb_left;\n\t\t} else {\n\t\t\tif (is_within_this_va(va, size, align, vstart))\n\t\t\t\treturn va;\n\n\t\t\t \n\t\t\tif (get_subtree_max_size(node->rb_right) >= length) {\n\t\t\t\tnode = node->rb_right;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t \n\t\t\twhile ((node = rb_parent(node))) {\n\t\t\t\tva = rb_entry(node, struct vmap_area, rb_node);\n\t\t\t\tif (is_within_this_va(va, size, align, vstart))\n\t\t\t\t\treturn va;\n\n\t\t\t\tif (get_subtree_max_size(node->rb_right) >= length &&\n\t\t\t\t\t\tvstart <= va->va_start) {\n\t\t\t\t\t \n\t\t\t\t\tvstart = va->va_start + 1;\n\t\t\t\t\tnode = node->rb_right;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\n#if DEBUG_AUGMENT_LOWEST_MATCH_CHECK\n#include <linux/random.h>\n\nstatic struct vmap_area *\nfind_vmap_lowest_linear_match(struct list_head *head, unsigned long size,\n\tunsigned long align, unsigned long vstart)\n{\n\tstruct vmap_area *va;\n\n\tlist_for_each_entry(va, head, list) {\n\t\tif (!is_within_this_va(va, size, align, vstart))\n\t\t\tcontinue;\n\n\t\treturn va;\n\t}\n\n\treturn NULL;\n}\n\nstatic void\nfind_vmap_lowest_match_check(struct rb_root *root, struct list_head *head,\n\t\t\t     unsigned long size, unsigned long align)\n{\n\tstruct vmap_area *va_1, *va_2;\n\tunsigned long vstart;\n\tunsigned int rnd;\n\n\tget_random_bytes(&rnd, sizeof(rnd));\n\tvstart = VMALLOC_START + rnd;\n\n\tva_1 = find_vmap_lowest_match(root, size, align, vstart, false);\n\tva_2 = find_vmap_lowest_linear_match(head, size, align, vstart);\n\n\tif (va_1 != va_2)\n\t\tpr_emerg(\"not lowest: t: 0x%p, l: 0x%p, v: 0x%lx\\n\",\n\t\t\tva_1, va_2, vstart);\n}\n#endif\n\nenum fit_type {\n\tNOTHING_FIT = 0,\n\tFL_FIT_TYPE = 1,\t \n\tLE_FIT_TYPE = 2,\t \n\tRE_FIT_TYPE = 3,\t \n\tNE_FIT_TYPE = 4\t\t \n};\n\nstatic __always_inline enum fit_type\nclassify_va_fit_type(struct vmap_area *va,\n\tunsigned long nva_start_addr, unsigned long size)\n{\n\tenum fit_type type;\n\n\t \n\tif (nva_start_addr < va->va_start ||\n\t\t\tnva_start_addr + size > va->va_end)\n\t\treturn NOTHING_FIT;\n\n\t \n\tif (va->va_start == nva_start_addr) {\n\t\tif (va->va_end == nva_start_addr + size)\n\t\t\ttype = FL_FIT_TYPE;\n\t\telse\n\t\t\ttype = LE_FIT_TYPE;\n\t} else if (va->va_end == nva_start_addr + size) {\n\t\ttype = RE_FIT_TYPE;\n\t} else {\n\t\ttype = NE_FIT_TYPE;\n\t}\n\n\treturn type;\n}\n\nstatic __always_inline int\nadjust_va_to_fit_type(struct rb_root *root, struct list_head *head,\n\t\t      struct vmap_area *va, unsigned long nva_start_addr,\n\t\t      unsigned long size)\n{\n\tstruct vmap_area *lva = NULL;\n\tenum fit_type type = classify_va_fit_type(va, nva_start_addr, size);\n\n\tif (type == FL_FIT_TYPE) {\n\t\t \n\t\tunlink_va_augment(va, root);\n\t\tkmem_cache_free(vmap_area_cachep, va);\n\t} else if (type == LE_FIT_TYPE) {\n\t\t \n\t\tva->va_start += size;\n\t} else if (type == RE_FIT_TYPE) {\n\t\t \n\t\tva->va_end = nva_start_addr;\n\t} else if (type == NE_FIT_TYPE) {\n\t\t \n\t\tlva = __this_cpu_xchg(ne_fit_preload_node, NULL);\n\t\tif (unlikely(!lva)) {\n\t\t\t \n\t\t\tlva = kmem_cache_alloc(vmap_area_cachep, GFP_NOWAIT);\n\t\t\tif (!lva)\n\t\t\t\treturn -1;\n\t\t}\n\n\t\t \n\t\tlva->va_start = va->va_start;\n\t\tlva->va_end = nva_start_addr;\n\n\t\t \n\t\tva->va_start = nva_start_addr + size;\n\t} else {\n\t\treturn -1;\n\t}\n\n\tif (type != FL_FIT_TYPE) {\n\t\taugment_tree_propagate_from(va);\n\n\t\tif (lva)\t \n\t\t\tinsert_vmap_area_augment(lva, &va->rb_node, root, head);\n\t}\n\n\treturn 0;\n}\n\n \nstatic __always_inline unsigned long\n__alloc_vmap_area(struct rb_root *root, struct list_head *head,\n\tunsigned long size, unsigned long align,\n\tunsigned long vstart, unsigned long vend)\n{\n\tbool adjust_search_size = true;\n\tunsigned long nva_start_addr;\n\tstruct vmap_area *va;\n\tint ret;\n\n\t \n\tif (align <= PAGE_SIZE || (align > PAGE_SIZE && (vend - vstart) == size))\n\t\tadjust_search_size = false;\n\n\tva = find_vmap_lowest_match(root, size, align, vstart, adjust_search_size);\n\tif (unlikely(!va))\n\t\treturn vend;\n\n\tif (va->va_start > vstart)\n\t\tnva_start_addr = ALIGN(va->va_start, align);\n\telse\n\t\tnva_start_addr = ALIGN(vstart, align);\n\n\t \n\tif (nva_start_addr + size > vend)\n\t\treturn vend;\n\n\t \n\tret = adjust_va_to_fit_type(root, head, va, nva_start_addr, size);\n\tif (WARN_ON_ONCE(ret))\n\t\treturn vend;\n\n#if DEBUG_AUGMENT_LOWEST_MATCH_CHECK\n\tfind_vmap_lowest_match_check(root, head, size, align);\n#endif\n\n\treturn nva_start_addr;\n}\n\n \nstatic void free_vmap_area(struct vmap_area *va)\n{\n\t \n\tspin_lock(&vmap_area_lock);\n\tunlink_va(va, &vmap_area_root);\n\tspin_unlock(&vmap_area_lock);\n\n\t \n\tspin_lock(&free_vmap_area_lock);\n\tmerge_or_add_vmap_area_augment(va, &free_vmap_area_root, &free_vmap_area_list);\n\tspin_unlock(&free_vmap_area_lock);\n}\n\nstatic inline void\npreload_this_cpu_lock(spinlock_t *lock, gfp_t gfp_mask, int node)\n{\n\tstruct vmap_area *va = NULL;\n\n\t \n\tif (!this_cpu_read(ne_fit_preload_node))\n\t\tva = kmem_cache_alloc_node(vmap_area_cachep, gfp_mask, node);\n\n\tspin_lock(lock);\n\n\tif (va && __this_cpu_cmpxchg(ne_fit_preload_node, NULL, va))\n\t\tkmem_cache_free(vmap_area_cachep, va);\n}\n\n \nstatic struct vmap_area *alloc_vmap_area(unsigned long size,\n\t\t\t\tunsigned long align,\n\t\t\t\tunsigned long vstart, unsigned long vend,\n\t\t\t\tint node, gfp_t gfp_mask,\n\t\t\t\tunsigned long va_flags)\n{\n\tstruct vmap_area *va;\n\tunsigned long freed;\n\tunsigned long addr;\n\tint purged = 0;\n\tint ret;\n\n\tif (unlikely(!size || offset_in_page(size) || !is_power_of_2(align)))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (unlikely(!vmap_initialized))\n\t\treturn ERR_PTR(-EBUSY);\n\n\tmight_sleep();\n\tgfp_mask = gfp_mask & GFP_RECLAIM_MASK;\n\n\tva = kmem_cache_alloc_node(vmap_area_cachep, gfp_mask, node);\n\tif (unlikely(!va))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\t \n\tkmemleak_scan_area(&va->rb_node, SIZE_MAX, gfp_mask);\n\nretry:\n\tpreload_this_cpu_lock(&free_vmap_area_lock, gfp_mask, node);\n\taddr = __alloc_vmap_area(&free_vmap_area_root, &free_vmap_area_list,\n\t\tsize, align, vstart, vend);\n\tspin_unlock(&free_vmap_area_lock);\n\n\ttrace_alloc_vmap_area(addr, size, align, vstart, vend, addr == vend);\n\n\t \n\tif (unlikely(addr == vend))\n\t\tgoto overflow;\n\n\tva->va_start = addr;\n\tva->va_end = addr + size;\n\tva->vm = NULL;\n\tva->flags = va_flags;\n\n\tspin_lock(&vmap_area_lock);\n\tinsert_vmap_area(va, &vmap_area_root, &vmap_area_list);\n\tspin_unlock(&vmap_area_lock);\n\n\tBUG_ON(!IS_ALIGNED(va->va_start, align));\n\tBUG_ON(va->va_start < vstart);\n\tBUG_ON(va->va_end > vend);\n\n\tret = kasan_populate_vmalloc(addr, size);\n\tif (ret) {\n\t\tfree_vmap_area(va);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\treturn va;\n\noverflow:\n\tif (!purged) {\n\t\treclaim_and_purge_vmap_areas();\n\t\tpurged = 1;\n\t\tgoto retry;\n\t}\n\n\tfreed = 0;\n\tblocking_notifier_call_chain(&vmap_notify_list, 0, &freed);\n\n\tif (freed > 0) {\n\t\tpurged = 0;\n\t\tgoto retry;\n\t}\n\n\tif (!(gfp_mask & __GFP_NOWARN) && printk_ratelimit())\n\t\tpr_warn(\"vmap allocation for size %lu failed: use vmalloc=<size> to increase size\\n\",\n\t\t\tsize);\n\n\tkmem_cache_free(vmap_area_cachep, va);\n\treturn ERR_PTR(-EBUSY);\n}\n\nint register_vmap_purge_notifier(struct notifier_block *nb)\n{\n\treturn blocking_notifier_chain_register(&vmap_notify_list, nb);\n}\nEXPORT_SYMBOL_GPL(register_vmap_purge_notifier);\n\nint unregister_vmap_purge_notifier(struct notifier_block *nb)\n{\n\treturn blocking_notifier_chain_unregister(&vmap_notify_list, nb);\n}\nEXPORT_SYMBOL_GPL(unregister_vmap_purge_notifier);\n\n \nstatic unsigned long lazy_max_pages(void)\n{\n\tunsigned int log;\n\n\tlog = fls(num_online_cpus());\n\n\treturn log * (32UL * 1024 * 1024 / PAGE_SIZE);\n}\n\nstatic atomic_long_t vmap_lazy_nr = ATOMIC_LONG_INIT(0);\n\n \nstatic DEFINE_MUTEX(vmap_purge_lock);\n\n \nstatic void purge_fragmented_blocks_allcpus(void);\n\n \nstatic bool __purge_vmap_area_lazy(unsigned long start, unsigned long end)\n{\n\tunsigned long resched_threshold;\n\tunsigned int num_purged_areas = 0;\n\tstruct list_head local_purge_list;\n\tstruct vmap_area *va, *n_va;\n\n\tlockdep_assert_held(&vmap_purge_lock);\n\n\tspin_lock(&purge_vmap_area_lock);\n\tpurge_vmap_area_root = RB_ROOT;\n\tlist_replace_init(&purge_vmap_area_list, &local_purge_list);\n\tspin_unlock(&purge_vmap_area_lock);\n\n\tif (unlikely(list_empty(&local_purge_list)))\n\t\tgoto out;\n\n\tstart = min(start,\n\t\tlist_first_entry(&local_purge_list,\n\t\t\tstruct vmap_area, list)->va_start);\n\n\tend = max(end,\n\t\tlist_last_entry(&local_purge_list,\n\t\t\tstruct vmap_area, list)->va_end);\n\n\tflush_tlb_kernel_range(start, end);\n\tresched_threshold = lazy_max_pages() << 1;\n\n\tspin_lock(&free_vmap_area_lock);\n\tlist_for_each_entry_safe(va, n_va, &local_purge_list, list) {\n\t\tunsigned long nr = (va->va_end - va->va_start) >> PAGE_SHIFT;\n\t\tunsigned long orig_start = va->va_start;\n\t\tunsigned long orig_end = va->va_end;\n\n\t\t \n\t\tva = merge_or_add_vmap_area_augment(va, &free_vmap_area_root,\n\t\t\t\t&free_vmap_area_list);\n\n\t\tif (!va)\n\t\t\tcontinue;\n\n\t\tif (is_vmalloc_or_module_addr((void *)orig_start))\n\t\t\tkasan_release_vmalloc(orig_start, orig_end,\n\t\t\t\t\t      va->va_start, va->va_end);\n\n\t\tatomic_long_sub(nr, &vmap_lazy_nr);\n\t\tnum_purged_areas++;\n\n\t\tif (atomic_long_read(&vmap_lazy_nr) < resched_threshold)\n\t\t\tcond_resched_lock(&free_vmap_area_lock);\n\t}\n\tspin_unlock(&free_vmap_area_lock);\n\nout:\n\ttrace_purge_vmap_area_lazy(start, end, num_purged_areas);\n\treturn num_purged_areas > 0;\n}\n\n \nstatic void reclaim_and_purge_vmap_areas(void)\n\n{\n\tmutex_lock(&vmap_purge_lock);\n\tpurge_fragmented_blocks_allcpus();\n\t__purge_vmap_area_lazy(ULONG_MAX, 0);\n\tmutex_unlock(&vmap_purge_lock);\n}\n\nstatic void drain_vmap_area_work(struct work_struct *work)\n{\n\tunsigned long nr_lazy;\n\n\tdo {\n\t\tmutex_lock(&vmap_purge_lock);\n\t\t__purge_vmap_area_lazy(ULONG_MAX, 0);\n\t\tmutex_unlock(&vmap_purge_lock);\n\n\t\t \n\t\tnr_lazy = atomic_long_read(&vmap_lazy_nr);\n\t} while (nr_lazy > lazy_max_pages());\n}\n\n \nstatic void free_vmap_area_noflush(struct vmap_area *va)\n{\n\tunsigned long nr_lazy_max = lazy_max_pages();\n\tunsigned long va_start = va->va_start;\n\tunsigned long nr_lazy;\n\n\tif (WARN_ON_ONCE(!list_empty(&va->list)))\n\t\treturn;\n\n\tnr_lazy = atomic_long_add_return((va->va_end - va->va_start) >>\n\t\t\t\tPAGE_SHIFT, &vmap_lazy_nr);\n\n\t \n\tspin_lock(&purge_vmap_area_lock);\n\tmerge_or_add_vmap_area(va,\n\t\t&purge_vmap_area_root, &purge_vmap_area_list);\n\tspin_unlock(&purge_vmap_area_lock);\n\n\ttrace_free_vmap_area_noflush(va_start, nr_lazy, nr_lazy_max);\n\n\t \n\tif (unlikely(nr_lazy > nr_lazy_max))\n\t\tschedule_work(&drain_vmap_work);\n}\n\n \nstatic void free_unmap_vmap_area(struct vmap_area *va)\n{\n\tflush_cache_vunmap(va->va_start, va->va_end);\n\tvunmap_range_noflush(va->va_start, va->va_end);\n\tif (debug_pagealloc_enabled_static())\n\t\tflush_tlb_kernel_range(va->va_start, va->va_end);\n\n\tfree_vmap_area_noflush(va);\n}\n\nstruct vmap_area *find_vmap_area(unsigned long addr)\n{\n\tstruct vmap_area *va;\n\n\tspin_lock(&vmap_area_lock);\n\tva = __find_vmap_area(addr, &vmap_area_root);\n\tspin_unlock(&vmap_area_lock);\n\n\treturn va;\n}\n\nstatic struct vmap_area *find_unlink_vmap_area(unsigned long addr)\n{\n\tstruct vmap_area *va;\n\n\tspin_lock(&vmap_area_lock);\n\tva = __find_vmap_area(addr, &vmap_area_root);\n\tif (va)\n\t\tunlink_va(va, &vmap_area_root);\n\tspin_unlock(&vmap_area_lock);\n\n\treturn va;\n}\n\n \n\n \n \n#if BITS_PER_LONG == 32\n#define VMALLOC_SPACE\t\t(128UL*1024*1024)\n#else\n#define VMALLOC_SPACE\t\t(128UL*1024*1024*1024)\n#endif\n\n#define VMALLOC_PAGES\t\t(VMALLOC_SPACE / PAGE_SIZE)\n#define VMAP_MAX_ALLOC\t\tBITS_PER_LONG\t \n#define VMAP_BBMAP_BITS_MAX\t1024\t \n#define VMAP_BBMAP_BITS_MIN\t(VMAP_MAX_ALLOC*2)\n#define VMAP_MIN(x, y)\t\t((x) < (y) ? (x) : (y))  \n#define VMAP_MAX(x, y)\t\t((x) > (y) ? (x) : (y))  \n#define VMAP_BBMAP_BITS\t\t\\\n\t\tVMAP_MIN(VMAP_BBMAP_BITS_MAX,\t\\\n\t\tVMAP_MAX(VMAP_BBMAP_BITS_MIN,\t\\\n\t\t\tVMALLOC_PAGES / roundup_pow_of_two(NR_CPUS) / 16))\n\n#define VMAP_BLOCK_SIZE\t\t(VMAP_BBMAP_BITS * PAGE_SIZE)\n\n \n#define VMAP_PURGE_THRESHOLD\t(VMAP_BBMAP_BITS / 4)\n\n#define VMAP_RAM\t\t0x1  \n#define VMAP_BLOCK\t\t0x2  \n#define VMAP_FLAGS_MASK\t\t0x3\n\nstruct vmap_block_queue {\n\tspinlock_t lock;\n\tstruct list_head free;\n\n\t \n\tstruct xarray vmap_blocks;\n};\n\nstruct vmap_block {\n\tspinlock_t lock;\n\tstruct vmap_area *va;\n\tunsigned long free, dirty;\n\tDECLARE_BITMAP(used_map, VMAP_BBMAP_BITS);\n\tunsigned long dirty_min, dirty_max;  \n\tstruct list_head free_list;\n\tstruct rcu_head rcu_head;\n\tstruct list_head purge;\n};\n\n \nstatic DEFINE_PER_CPU(struct vmap_block_queue, vmap_block_queue);\n\n \nstatic struct xarray *\naddr_to_vb_xa(unsigned long addr)\n{\n\tint index = (addr / VMAP_BLOCK_SIZE) % num_possible_cpus();\n\n\treturn &per_cpu(vmap_block_queue, index).vmap_blocks;\n}\n\n \n\nstatic unsigned long addr_to_vb_idx(unsigned long addr)\n{\n\taddr -= VMALLOC_START & ~(VMAP_BLOCK_SIZE-1);\n\taddr /= VMAP_BLOCK_SIZE;\n\treturn addr;\n}\n\nstatic void *vmap_block_vaddr(unsigned long va_start, unsigned long pages_off)\n{\n\tunsigned long addr;\n\n\taddr = va_start + (pages_off << PAGE_SHIFT);\n\tBUG_ON(addr_to_vb_idx(addr) != addr_to_vb_idx(va_start));\n\treturn (void *)addr;\n}\n\n \nstatic void *new_vmap_block(unsigned int order, gfp_t gfp_mask)\n{\n\tstruct vmap_block_queue *vbq;\n\tstruct vmap_block *vb;\n\tstruct vmap_area *va;\n\tstruct xarray *xa;\n\tunsigned long vb_idx;\n\tint node, err;\n\tvoid *vaddr;\n\n\tnode = numa_node_id();\n\n\tvb = kmalloc_node(sizeof(struct vmap_block),\n\t\t\tgfp_mask & GFP_RECLAIM_MASK, node);\n\tif (unlikely(!vb))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tva = alloc_vmap_area(VMAP_BLOCK_SIZE, VMAP_BLOCK_SIZE,\n\t\t\t\t\tVMALLOC_START, VMALLOC_END,\n\t\t\t\t\tnode, gfp_mask,\n\t\t\t\t\tVMAP_RAM|VMAP_BLOCK);\n\tif (IS_ERR(va)) {\n\t\tkfree(vb);\n\t\treturn ERR_CAST(va);\n\t}\n\n\tvaddr = vmap_block_vaddr(va->va_start, 0);\n\tspin_lock_init(&vb->lock);\n\tvb->va = va;\n\t \n\tBUG_ON(VMAP_BBMAP_BITS <= (1UL << order));\n\tbitmap_zero(vb->used_map, VMAP_BBMAP_BITS);\n\tvb->free = VMAP_BBMAP_BITS - (1UL << order);\n\tvb->dirty = 0;\n\tvb->dirty_min = VMAP_BBMAP_BITS;\n\tvb->dirty_max = 0;\n\tbitmap_set(vb->used_map, 0, (1UL << order));\n\tINIT_LIST_HEAD(&vb->free_list);\n\n\txa = addr_to_vb_xa(va->va_start);\n\tvb_idx = addr_to_vb_idx(va->va_start);\n\terr = xa_insert(xa, vb_idx, vb, gfp_mask);\n\tif (err) {\n\t\tkfree(vb);\n\t\tfree_vmap_area(va);\n\t\treturn ERR_PTR(err);\n\t}\n\n\tvbq = raw_cpu_ptr(&vmap_block_queue);\n\tspin_lock(&vbq->lock);\n\tlist_add_tail_rcu(&vb->free_list, &vbq->free);\n\tspin_unlock(&vbq->lock);\n\n\treturn vaddr;\n}\n\nstatic void free_vmap_block(struct vmap_block *vb)\n{\n\tstruct vmap_block *tmp;\n\tstruct xarray *xa;\n\n\txa = addr_to_vb_xa(vb->va->va_start);\n\ttmp = xa_erase(xa, addr_to_vb_idx(vb->va->va_start));\n\tBUG_ON(tmp != vb);\n\n\tspin_lock(&vmap_area_lock);\n\tunlink_va(vb->va, &vmap_area_root);\n\tspin_unlock(&vmap_area_lock);\n\n\tfree_vmap_area_noflush(vb->va);\n\tkfree_rcu(vb, rcu_head);\n}\n\nstatic bool purge_fragmented_block(struct vmap_block *vb,\n\t\tstruct vmap_block_queue *vbq, struct list_head *purge_list,\n\t\tbool force_purge)\n{\n\tif (vb->free + vb->dirty != VMAP_BBMAP_BITS ||\n\t    vb->dirty == VMAP_BBMAP_BITS)\n\t\treturn false;\n\n\t \n\tif (!(force_purge || vb->free < VMAP_PURGE_THRESHOLD))\n\t\treturn false;\n\n\t \n\tWRITE_ONCE(vb->free, 0);\n\t \n\tWRITE_ONCE(vb->dirty, VMAP_BBMAP_BITS);\n\tvb->dirty_min = 0;\n\tvb->dirty_max = VMAP_BBMAP_BITS;\n\tspin_lock(&vbq->lock);\n\tlist_del_rcu(&vb->free_list);\n\tspin_unlock(&vbq->lock);\n\tlist_add_tail(&vb->purge, purge_list);\n\treturn true;\n}\n\nstatic void free_purged_blocks(struct list_head *purge_list)\n{\n\tstruct vmap_block *vb, *n_vb;\n\n\tlist_for_each_entry_safe(vb, n_vb, purge_list, purge) {\n\t\tlist_del(&vb->purge);\n\t\tfree_vmap_block(vb);\n\t}\n}\n\nstatic void purge_fragmented_blocks(int cpu)\n{\n\tLIST_HEAD(purge);\n\tstruct vmap_block *vb;\n\tstruct vmap_block_queue *vbq = &per_cpu(vmap_block_queue, cpu);\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(vb, &vbq->free, free_list) {\n\t\tunsigned long free = READ_ONCE(vb->free);\n\t\tunsigned long dirty = READ_ONCE(vb->dirty);\n\n\t\tif (free + dirty != VMAP_BBMAP_BITS ||\n\t\t    dirty == VMAP_BBMAP_BITS)\n\t\t\tcontinue;\n\n\t\tspin_lock(&vb->lock);\n\t\tpurge_fragmented_block(vb, vbq, &purge, true);\n\t\tspin_unlock(&vb->lock);\n\t}\n\trcu_read_unlock();\n\tfree_purged_blocks(&purge);\n}\n\nstatic void purge_fragmented_blocks_allcpus(void)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu)\n\t\tpurge_fragmented_blocks(cpu);\n}\n\nstatic void *vb_alloc(unsigned long size, gfp_t gfp_mask)\n{\n\tstruct vmap_block_queue *vbq;\n\tstruct vmap_block *vb;\n\tvoid *vaddr = NULL;\n\tunsigned int order;\n\n\tBUG_ON(offset_in_page(size));\n\tBUG_ON(size > PAGE_SIZE*VMAP_MAX_ALLOC);\n\tif (WARN_ON(size == 0)) {\n\t\t \n\t\treturn NULL;\n\t}\n\torder = get_order(size);\n\n\trcu_read_lock();\n\tvbq = raw_cpu_ptr(&vmap_block_queue);\n\tlist_for_each_entry_rcu(vb, &vbq->free, free_list) {\n\t\tunsigned long pages_off;\n\n\t\tif (READ_ONCE(vb->free) < (1UL << order))\n\t\t\tcontinue;\n\n\t\tspin_lock(&vb->lock);\n\t\tif (vb->free < (1UL << order)) {\n\t\t\tspin_unlock(&vb->lock);\n\t\t\tcontinue;\n\t\t}\n\n\t\tpages_off = VMAP_BBMAP_BITS - vb->free;\n\t\tvaddr = vmap_block_vaddr(vb->va->va_start, pages_off);\n\t\tWRITE_ONCE(vb->free, vb->free - (1UL << order));\n\t\tbitmap_set(vb->used_map, pages_off, (1UL << order));\n\t\tif (vb->free == 0) {\n\t\t\tspin_lock(&vbq->lock);\n\t\t\tlist_del_rcu(&vb->free_list);\n\t\t\tspin_unlock(&vbq->lock);\n\t\t}\n\n\t\tspin_unlock(&vb->lock);\n\t\tbreak;\n\t}\n\n\trcu_read_unlock();\n\n\t \n\tif (!vaddr)\n\t\tvaddr = new_vmap_block(order, gfp_mask);\n\n\treturn vaddr;\n}\n\nstatic void vb_free(unsigned long addr, unsigned long size)\n{\n\tunsigned long offset;\n\tunsigned int order;\n\tstruct vmap_block *vb;\n\tstruct xarray *xa;\n\n\tBUG_ON(offset_in_page(size));\n\tBUG_ON(size > PAGE_SIZE*VMAP_MAX_ALLOC);\n\n\tflush_cache_vunmap(addr, addr + size);\n\n\torder = get_order(size);\n\toffset = (addr & (VMAP_BLOCK_SIZE - 1)) >> PAGE_SHIFT;\n\n\txa = addr_to_vb_xa(addr);\n\tvb = xa_load(xa, addr_to_vb_idx(addr));\n\n\tspin_lock(&vb->lock);\n\tbitmap_clear(vb->used_map, offset, (1UL << order));\n\tspin_unlock(&vb->lock);\n\n\tvunmap_range_noflush(addr, addr + size);\n\n\tif (debug_pagealloc_enabled_static())\n\t\tflush_tlb_kernel_range(addr, addr + size);\n\n\tspin_lock(&vb->lock);\n\n\t \n\tvb->dirty_min = min(vb->dirty_min, offset);\n\tvb->dirty_max = max(vb->dirty_max, offset + (1UL << order));\n\n\tWRITE_ONCE(vb->dirty, vb->dirty + (1UL << order));\n\tif (vb->dirty == VMAP_BBMAP_BITS) {\n\t\tBUG_ON(vb->free);\n\t\tspin_unlock(&vb->lock);\n\t\tfree_vmap_block(vb);\n\t} else\n\t\tspin_unlock(&vb->lock);\n}\n\nstatic void _vm_unmap_aliases(unsigned long start, unsigned long end, int flush)\n{\n\tLIST_HEAD(purge_list);\n\tint cpu;\n\n\tif (unlikely(!vmap_initialized))\n\t\treturn;\n\n\tmutex_lock(&vmap_purge_lock);\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct vmap_block_queue *vbq = &per_cpu(vmap_block_queue, cpu);\n\t\tstruct vmap_block *vb;\n\t\tunsigned long idx;\n\n\t\trcu_read_lock();\n\t\txa_for_each(&vbq->vmap_blocks, idx, vb) {\n\t\t\tspin_lock(&vb->lock);\n\n\t\t\t \n\t\t\tif (!purge_fragmented_block(vb, vbq, &purge_list, false) &&\n\t\t\t    vb->dirty_max && vb->dirty != VMAP_BBMAP_BITS) {\n\t\t\t\tunsigned long va_start = vb->va->va_start;\n\t\t\t\tunsigned long s, e;\n\n\t\t\t\ts = va_start + (vb->dirty_min << PAGE_SHIFT);\n\t\t\t\te = va_start + (vb->dirty_max << PAGE_SHIFT);\n\n\t\t\t\tstart = min(s, start);\n\t\t\t\tend   = max(e, end);\n\n\t\t\t\t \n\t\t\t\tvb->dirty_min = VMAP_BBMAP_BITS;\n\t\t\t\tvb->dirty_max = 0;\n\n\t\t\t\tflush = 1;\n\t\t\t}\n\t\t\tspin_unlock(&vb->lock);\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\tfree_purged_blocks(&purge_list);\n\n\tif (!__purge_vmap_area_lazy(start, end) && flush)\n\t\tflush_tlb_kernel_range(start, end);\n\tmutex_unlock(&vmap_purge_lock);\n}\n\n \nvoid vm_unmap_aliases(void)\n{\n\tunsigned long start = ULONG_MAX, end = 0;\n\tint flush = 0;\n\n\t_vm_unmap_aliases(start, end, flush);\n}\nEXPORT_SYMBOL_GPL(vm_unmap_aliases);\n\n \nvoid vm_unmap_ram(const void *mem, unsigned int count)\n{\n\tunsigned long size = (unsigned long)count << PAGE_SHIFT;\n\tunsigned long addr = (unsigned long)kasan_reset_tag(mem);\n\tstruct vmap_area *va;\n\n\tmight_sleep();\n\tBUG_ON(!addr);\n\tBUG_ON(addr < VMALLOC_START);\n\tBUG_ON(addr > VMALLOC_END);\n\tBUG_ON(!PAGE_ALIGNED(addr));\n\n\tkasan_poison_vmalloc(mem, size);\n\n\tif (likely(count <= VMAP_MAX_ALLOC)) {\n\t\tdebug_check_no_locks_freed(mem, size);\n\t\tvb_free(addr, size);\n\t\treturn;\n\t}\n\n\tva = find_unlink_vmap_area(addr);\n\tif (WARN_ON_ONCE(!va))\n\t\treturn;\n\n\tdebug_check_no_locks_freed((void *)va->va_start,\n\t\t\t\t    (va->va_end - va->va_start));\n\tfree_unmap_vmap_area(va);\n}\nEXPORT_SYMBOL(vm_unmap_ram);\n\n \nvoid *vm_map_ram(struct page **pages, unsigned int count, int node)\n{\n\tunsigned long size = (unsigned long)count << PAGE_SHIFT;\n\tunsigned long addr;\n\tvoid *mem;\n\n\tif (likely(count <= VMAP_MAX_ALLOC)) {\n\t\tmem = vb_alloc(size, GFP_KERNEL);\n\t\tif (IS_ERR(mem))\n\t\t\treturn NULL;\n\t\taddr = (unsigned long)mem;\n\t} else {\n\t\tstruct vmap_area *va;\n\t\tva = alloc_vmap_area(size, PAGE_SIZE,\n\t\t\t\tVMALLOC_START, VMALLOC_END,\n\t\t\t\tnode, GFP_KERNEL, VMAP_RAM);\n\t\tif (IS_ERR(va))\n\t\t\treturn NULL;\n\n\t\taddr = va->va_start;\n\t\tmem = (void *)addr;\n\t}\n\n\tif (vmap_pages_range(addr, addr + size, PAGE_KERNEL,\n\t\t\t\tpages, PAGE_SHIFT) < 0) {\n\t\tvm_unmap_ram(mem, count);\n\t\treturn NULL;\n\t}\n\n\t \n\tmem = kasan_unpoison_vmalloc(mem, size, KASAN_VMALLOC_PROT_NORMAL);\n\n\treturn mem;\n}\nEXPORT_SYMBOL(vm_map_ram);\n\nstatic struct vm_struct *vmlist __initdata;\n\nstatic inline unsigned int vm_area_page_order(struct vm_struct *vm)\n{\n#ifdef CONFIG_HAVE_ARCH_HUGE_VMALLOC\n\treturn vm->page_order;\n#else\n\treturn 0;\n#endif\n}\n\nstatic inline void set_vm_area_page_order(struct vm_struct *vm, unsigned int order)\n{\n#ifdef CONFIG_HAVE_ARCH_HUGE_VMALLOC\n\tvm->page_order = order;\n#else\n\tBUG_ON(order != 0);\n#endif\n}\n\n \nvoid __init vm_area_add_early(struct vm_struct *vm)\n{\n\tstruct vm_struct *tmp, **p;\n\n\tBUG_ON(vmap_initialized);\n\tfor (p = &vmlist; (tmp = *p) != NULL; p = &tmp->next) {\n\t\tif (tmp->addr >= vm->addr) {\n\t\t\tBUG_ON(tmp->addr < vm->addr + vm->size);\n\t\t\tbreak;\n\t\t} else\n\t\t\tBUG_ON(tmp->addr + tmp->size > vm->addr);\n\t}\n\tvm->next = *p;\n\t*p = vm;\n}\n\n \nvoid __init vm_area_register_early(struct vm_struct *vm, size_t align)\n{\n\tunsigned long addr = ALIGN(VMALLOC_START, align);\n\tstruct vm_struct *cur, **p;\n\n\tBUG_ON(vmap_initialized);\n\n\tfor (p = &vmlist; (cur = *p) != NULL; p = &cur->next) {\n\t\tif ((unsigned long)cur->addr - addr >= vm->size)\n\t\t\tbreak;\n\t\taddr = ALIGN((unsigned long)cur->addr + cur->size, align);\n\t}\n\n\tBUG_ON(addr > VMALLOC_END - vm->size);\n\tvm->addr = (void *)addr;\n\tvm->next = *p;\n\t*p = vm;\n\tkasan_populate_early_vm_area_shadow(vm->addr, vm->size);\n}\n\nstatic void vmap_init_free_space(void)\n{\n\tunsigned long vmap_start = 1;\n\tconst unsigned long vmap_end = ULONG_MAX;\n\tstruct vmap_area *busy, *free;\n\n\t \n\tlist_for_each_entry(busy, &vmap_area_list, list) {\n\t\tif (busy->va_start - vmap_start > 0) {\n\t\t\tfree = kmem_cache_zalloc(vmap_area_cachep, GFP_NOWAIT);\n\t\t\tif (!WARN_ON_ONCE(!free)) {\n\t\t\t\tfree->va_start = vmap_start;\n\t\t\t\tfree->va_end = busy->va_start;\n\n\t\t\t\tinsert_vmap_area_augment(free, NULL,\n\t\t\t\t\t&free_vmap_area_root,\n\t\t\t\t\t\t&free_vmap_area_list);\n\t\t\t}\n\t\t}\n\n\t\tvmap_start = busy->va_end;\n\t}\n\n\tif (vmap_end - vmap_start > 0) {\n\t\tfree = kmem_cache_zalloc(vmap_area_cachep, GFP_NOWAIT);\n\t\tif (!WARN_ON_ONCE(!free)) {\n\t\t\tfree->va_start = vmap_start;\n\t\t\tfree->va_end = vmap_end;\n\n\t\t\tinsert_vmap_area_augment(free, NULL,\n\t\t\t\t&free_vmap_area_root,\n\t\t\t\t\t&free_vmap_area_list);\n\t\t}\n\t}\n}\n\nstatic inline void setup_vmalloc_vm_locked(struct vm_struct *vm,\n\tstruct vmap_area *va, unsigned long flags, const void *caller)\n{\n\tvm->flags = flags;\n\tvm->addr = (void *)va->va_start;\n\tvm->size = va->va_end - va->va_start;\n\tvm->caller = caller;\n\tva->vm = vm;\n}\n\nstatic void setup_vmalloc_vm(struct vm_struct *vm, struct vmap_area *va,\n\t\t\t      unsigned long flags, const void *caller)\n{\n\tspin_lock(&vmap_area_lock);\n\tsetup_vmalloc_vm_locked(vm, va, flags, caller);\n\tspin_unlock(&vmap_area_lock);\n}\n\nstatic void clear_vm_uninitialized_flag(struct vm_struct *vm)\n{\n\t \n\tsmp_wmb();\n\tvm->flags &= ~VM_UNINITIALIZED;\n}\n\nstatic struct vm_struct *__get_vm_area_node(unsigned long size,\n\t\tunsigned long align, unsigned long shift, unsigned long flags,\n\t\tunsigned long start, unsigned long end, int node,\n\t\tgfp_t gfp_mask, const void *caller)\n{\n\tstruct vmap_area *va;\n\tstruct vm_struct *area;\n\tunsigned long requested_size = size;\n\n\tBUG_ON(in_interrupt());\n\tsize = ALIGN(size, 1ul << shift);\n\tif (unlikely(!size))\n\t\treturn NULL;\n\n\tif (flags & VM_IOREMAP)\n\t\talign = 1ul << clamp_t(int, get_count_order_long(size),\n\t\t\t\t       PAGE_SHIFT, IOREMAP_MAX_ORDER);\n\n\tarea = kzalloc_node(sizeof(*area), gfp_mask & GFP_RECLAIM_MASK, node);\n\tif (unlikely(!area))\n\t\treturn NULL;\n\n\tif (!(flags & VM_NO_GUARD))\n\t\tsize += PAGE_SIZE;\n\n\tva = alloc_vmap_area(size, align, start, end, node, gfp_mask, 0);\n\tif (IS_ERR(va)) {\n\t\tkfree(area);\n\t\treturn NULL;\n\t}\n\n\tsetup_vmalloc_vm(area, va, flags, caller);\n\n\t \n\tif (!(flags & VM_ALLOC))\n\t\tarea->addr = kasan_unpoison_vmalloc(area->addr, requested_size,\n\t\t\t\t\t\t    KASAN_VMALLOC_PROT_NORMAL);\n\n\treturn area;\n}\n\nstruct vm_struct *__get_vm_area_caller(unsigned long size, unsigned long flags,\n\t\t\t\t       unsigned long start, unsigned long end,\n\t\t\t\t       const void *caller)\n{\n\treturn __get_vm_area_node(size, 1, PAGE_SHIFT, flags, start, end,\n\t\t\t\t  NUMA_NO_NODE, GFP_KERNEL, caller);\n}\n\n \nstruct vm_struct *get_vm_area(unsigned long size, unsigned long flags)\n{\n\treturn __get_vm_area_node(size, 1, PAGE_SHIFT, flags,\n\t\t\t\t  VMALLOC_START, VMALLOC_END,\n\t\t\t\t  NUMA_NO_NODE, GFP_KERNEL,\n\t\t\t\t  __builtin_return_address(0));\n}\n\nstruct vm_struct *get_vm_area_caller(unsigned long size, unsigned long flags,\n\t\t\t\tconst void *caller)\n{\n\treturn __get_vm_area_node(size, 1, PAGE_SHIFT, flags,\n\t\t\t\t  VMALLOC_START, VMALLOC_END,\n\t\t\t\t  NUMA_NO_NODE, GFP_KERNEL, caller);\n}\n\n \nstruct vm_struct *find_vm_area(const void *addr)\n{\n\tstruct vmap_area *va;\n\n\tva = find_vmap_area((unsigned long)addr);\n\tif (!va)\n\t\treturn NULL;\n\n\treturn va->vm;\n}\n\n \nstruct vm_struct *remove_vm_area(const void *addr)\n{\n\tstruct vmap_area *va;\n\tstruct vm_struct *vm;\n\n\tmight_sleep();\n\n\tif (WARN(!PAGE_ALIGNED(addr), \"Trying to vfree() bad address (%p)\\n\",\n\t\t\taddr))\n\t\treturn NULL;\n\n\tva = find_unlink_vmap_area((unsigned long)addr);\n\tif (!va || !va->vm)\n\t\treturn NULL;\n\tvm = va->vm;\n\n\tdebug_check_no_locks_freed(vm->addr, get_vm_area_size(vm));\n\tdebug_check_no_obj_freed(vm->addr, get_vm_area_size(vm));\n\tkasan_free_module_shadow(vm);\n\tkasan_poison_vmalloc(vm->addr, get_vm_area_size(vm));\n\n\tfree_unmap_vmap_area(va);\n\treturn vm;\n}\n\nstatic inline void set_area_direct_map(const struct vm_struct *area,\n\t\t\t\t       int (*set_direct_map)(struct page *page))\n{\n\tint i;\n\n\t \n\tfor (i = 0; i < area->nr_pages; i++)\n\t\tif (page_address(area->pages[i]))\n\t\t\tset_direct_map(area->pages[i]);\n}\n\n \nstatic void vm_reset_perms(struct vm_struct *area)\n{\n\tunsigned long start = ULONG_MAX, end = 0;\n\tunsigned int page_order = vm_area_page_order(area);\n\tint flush_dmap = 0;\n\tint i;\n\n\t \n\tfor (i = 0; i < area->nr_pages; i += 1U << page_order) {\n\t\tunsigned long addr = (unsigned long)page_address(area->pages[i]);\n\n\t\tif (addr) {\n\t\t\tunsigned long page_size;\n\n\t\t\tpage_size = PAGE_SIZE << page_order;\n\t\t\tstart = min(addr, start);\n\t\t\tend = max(addr + page_size, end);\n\t\t\tflush_dmap = 1;\n\t\t}\n\t}\n\n\t \n\tset_area_direct_map(area, set_direct_map_invalid_noflush);\n\t_vm_unmap_aliases(start, end, flush_dmap);\n\tset_area_direct_map(area, set_direct_map_default_noflush);\n}\n\nstatic void delayed_vfree_work(struct work_struct *w)\n{\n\tstruct vfree_deferred *p = container_of(w, struct vfree_deferred, wq);\n\tstruct llist_node *t, *llnode;\n\n\tllist_for_each_safe(llnode, t, llist_del_all(&p->list))\n\t\tvfree(llnode);\n}\n\n \nvoid vfree_atomic(const void *addr)\n{\n\tstruct vfree_deferred *p = raw_cpu_ptr(&vfree_deferred);\n\n\tBUG_ON(in_nmi());\n\tkmemleak_free(addr);\n\n\t \n\tif (addr && llist_add((struct llist_node *)addr, &p->list))\n\t\tschedule_work(&p->wq);\n}\n\n \nvoid vfree(const void *addr)\n{\n\tstruct vm_struct *vm;\n\tint i;\n\n\tif (unlikely(in_interrupt())) {\n\t\tvfree_atomic(addr);\n\t\treturn;\n\t}\n\n\tBUG_ON(in_nmi());\n\tkmemleak_free(addr);\n\tmight_sleep();\n\n\tif (!addr)\n\t\treturn;\n\n\tvm = remove_vm_area(addr);\n\tif (unlikely(!vm)) {\n\t\tWARN(1, KERN_ERR \"Trying to vfree() nonexistent vm area (%p)\\n\",\n\t\t\t\taddr);\n\t\treturn;\n\t}\n\n\tif (unlikely(vm->flags & VM_FLUSH_RESET_PERMS))\n\t\tvm_reset_perms(vm);\n\tfor (i = 0; i < vm->nr_pages; i++) {\n\t\tstruct page *page = vm->pages[i];\n\n\t\tBUG_ON(!page);\n\t\tmod_memcg_page_state(page, MEMCG_VMALLOC, -1);\n\t\t \n\t\t__free_page(page);\n\t\tcond_resched();\n\t}\n\tatomic_long_sub(vm->nr_pages, &nr_vmalloc_pages);\n\tkvfree(vm->pages);\n\tkfree(vm);\n}\nEXPORT_SYMBOL(vfree);\n\n \nvoid vunmap(const void *addr)\n{\n\tstruct vm_struct *vm;\n\n\tBUG_ON(in_interrupt());\n\tmight_sleep();\n\n\tif (!addr)\n\t\treturn;\n\tvm = remove_vm_area(addr);\n\tif (unlikely(!vm)) {\n\t\tWARN(1, KERN_ERR \"Trying to vunmap() nonexistent vm area (%p)\\n\",\n\t\t\t\taddr);\n\t\treturn;\n\t}\n\tkfree(vm);\n}\nEXPORT_SYMBOL(vunmap);\n\n \nvoid *vmap(struct page **pages, unsigned int count,\n\t   unsigned long flags, pgprot_t prot)\n{\n\tstruct vm_struct *area;\n\tunsigned long addr;\n\tunsigned long size;\t\t \n\n\tmight_sleep();\n\n\tif (WARN_ON_ONCE(flags & VM_FLUSH_RESET_PERMS))\n\t\treturn NULL;\n\n\t \n\tif (WARN_ON_ONCE(flags & VM_NO_GUARD))\n\t\tflags &= ~VM_NO_GUARD;\n\n\tif (count > totalram_pages())\n\t\treturn NULL;\n\n\tsize = (unsigned long)count << PAGE_SHIFT;\n\tarea = get_vm_area_caller(size, flags, __builtin_return_address(0));\n\tif (!area)\n\t\treturn NULL;\n\n\taddr = (unsigned long)area->addr;\n\tif (vmap_pages_range(addr, addr + size, pgprot_nx(prot),\n\t\t\t\tpages, PAGE_SHIFT) < 0) {\n\t\tvunmap(area->addr);\n\t\treturn NULL;\n\t}\n\n\tif (flags & VM_MAP_PUT_PAGES) {\n\t\tarea->pages = pages;\n\t\tarea->nr_pages = count;\n\t}\n\treturn area->addr;\n}\nEXPORT_SYMBOL(vmap);\n\n#ifdef CONFIG_VMAP_PFN\nstruct vmap_pfn_data {\n\tunsigned long\t*pfns;\n\tpgprot_t\tprot;\n\tunsigned int\tidx;\n};\n\nstatic int vmap_pfn_apply(pte_t *pte, unsigned long addr, void *private)\n{\n\tstruct vmap_pfn_data *data = private;\n\tunsigned long pfn = data->pfns[data->idx];\n\tpte_t ptent;\n\n\tif (WARN_ON_ONCE(pfn_valid(pfn)))\n\t\treturn -EINVAL;\n\n\tptent = pte_mkspecial(pfn_pte(pfn, data->prot));\n\tset_pte_at(&init_mm, addr, pte, ptent);\n\n\tdata->idx++;\n\treturn 0;\n}\n\n \nvoid *vmap_pfn(unsigned long *pfns, unsigned int count, pgprot_t prot)\n{\n\tstruct vmap_pfn_data data = { .pfns = pfns, .prot = pgprot_nx(prot) };\n\tstruct vm_struct *area;\n\n\tarea = get_vm_area_caller(count * PAGE_SIZE, VM_IOREMAP,\n\t\t\t__builtin_return_address(0));\n\tif (!area)\n\t\treturn NULL;\n\tif (apply_to_page_range(&init_mm, (unsigned long)area->addr,\n\t\t\tcount * PAGE_SIZE, vmap_pfn_apply, &data)) {\n\t\tfree_vm_area(area);\n\t\treturn NULL;\n\t}\n\n\tflush_cache_vmap((unsigned long)area->addr,\n\t\t\t (unsigned long)area->addr + count * PAGE_SIZE);\n\n\treturn area->addr;\n}\nEXPORT_SYMBOL_GPL(vmap_pfn);\n#endif  \n\nstatic inline unsigned int\nvm_area_alloc_pages(gfp_t gfp, int nid,\n\t\tunsigned int order, unsigned int nr_pages, struct page **pages)\n{\n\tunsigned int nr_allocated = 0;\n\tgfp_t alloc_gfp = gfp;\n\tbool nofail = false;\n\tstruct page *page;\n\tint i;\n\n\t \n\tif (!order) {\n\t\t \n\t\tgfp_t bulk_gfp = gfp & ~__GFP_NOFAIL;\n\n\t\twhile (nr_allocated < nr_pages) {\n\t\t\tunsigned int nr, nr_pages_request;\n\n\t\t\t \n\t\t\tnr_pages_request = min(100U, nr_pages - nr_allocated);\n\n\t\t\t \n\t\t\tif (IS_ENABLED(CONFIG_NUMA) && nid == NUMA_NO_NODE)\n\t\t\t\tnr = alloc_pages_bulk_array_mempolicy(bulk_gfp,\n\t\t\t\t\t\t\tnr_pages_request,\n\t\t\t\t\t\t\tpages + nr_allocated);\n\n\t\t\telse\n\t\t\t\tnr = alloc_pages_bulk_array_node(bulk_gfp, nid,\n\t\t\t\t\t\t\tnr_pages_request,\n\t\t\t\t\t\t\tpages + nr_allocated);\n\n\t\t\tnr_allocated += nr;\n\t\t\tcond_resched();\n\n\t\t\t \n\t\t\tif (nr != nr_pages_request)\n\t\t\t\tbreak;\n\t\t}\n\t} else if (gfp & __GFP_NOFAIL) {\n\t\t \n\t\talloc_gfp &= ~__GFP_NOFAIL;\n\t\tnofail = true;\n\t}\n\n\t \n\twhile (nr_allocated < nr_pages) {\n\t\tif (fatal_signal_pending(current))\n\t\t\tbreak;\n\n\t\tif (nid == NUMA_NO_NODE)\n\t\t\tpage = alloc_pages(alloc_gfp, order);\n\t\telse\n\t\t\tpage = alloc_pages_node(nid, alloc_gfp, order);\n\t\tif (unlikely(!page)) {\n\t\t\tif (!nofail)\n\t\t\t\tbreak;\n\n\t\t\t \n\t\t\talloc_gfp |= __GFP_NOFAIL;\n\t\t\torder = 0;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (order)\n\t\t\tsplit_page(page, order);\n\n\t\t \n\t\tfor (i = 0; i < (1U << order); i++)\n\t\t\tpages[nr_allocated + i] = page + i;\n\n\t\tcond_resched();\n\t\tnr_allocated += 1U << order;\n\t}\n\n\treturn nr_allocated;\n}\n\nstatic void *__vmalloc_area_node(struct vm_struct *area, gfp_t gfp_mask,\n\t\t\t\t pgprot_t prot, unsigned int page_shift,\n\t\t\t\t int node)\n{\n\tconst gfp_t nested_gfp = (gfp_mask & GFP_RECLAIM_MASK) | __GFP_ZERO;\n\tbool nofail = gfp_mask & __GFP_NOFAIL;\n\tunsigned long addr = (unsigned long)area->addr;\n\tunsigned long size = get_vm_area_size(area);\n\tunsigned long array_size;\n\tunsigned int nr_small_pages = size >> PAGE_SHIFT;\n\tunsigned int page_order;\n\tunsigned int flags;\n\tint ret;\n\n\tarray_size = (unsigned long)nr_small_pages * sizeof(struct page *);\n\n\tif (!(gfp_mask & (GFP_DMA | GFP_DMA32)))\n\t\tgfp_mask |= __GFP_HIGHMEM;\n\n\t \n\tif (array_size > PAGE_SIZE) {\n\t\tarea->pages = __vmalloc_node(array_size, 1, nested_gfp, node,\n\t\t\t\t\tarea->caller);\n\t} else {\n\t\tarea->pages = kmalloc_node(array_size, nested_gfp, node);\n\t}\n\n\tif (!area->pages) {\n\t\twarn_alloc(gfp_mask, NULL,\n\t\t\t\"vmalloc error: size %lu, failed to allocated page array size %lu\",\n\t\t\tnr_small_pages * PAGE_SIZE, array_size);\n\t\tfree_vm_area(area);\n\t\treturn NULL;\n\t}\n\n\tset_vm_area_page_order(area, page_shift - PAGE_SHIFT);\n\tpage_order = vm_area_page_order(area);\n\n\tarea->nr_pages = vm_area_alloc_pages(gfp_mask | __GFP_NOWARN,\n\t\tnode, page_order, nr_small_pages, area->pages);\n\n\tatomic_long_add(area->nr_pages, &nr_vmalloc_pages);\n\tif (gfp_mask & __GFP_ACCOUNT) {\n\t\tint i;\n\n\t\tfor (i = 0; i < area->nr_pages; i++)\n\t\t\tmod_memcg_page_state(area->pages[i], MEMCG_VMALLOC, 1);\n\t}\n\n\t \n\tif (area->nr_pages != nr_small_pages) {\n\t\t \n\t\tif (!fatal_signal_pending(current) && page_order == 0)\n\t\t\twarn_alloc(gfp_mask, NULL,\n\t\t\t\t\"vmalloc error: size %lu, failed to allocate pages\",\n\t\t\t\tarea->nr_pages * PAGE_SIZE);\n\t\tgoto fail;\n\t}\n\n\t \n\tif ((gfp_mask & (__GFP_FS | __GFP_IO)) == __GFP_IO)\n\t\tflags = memalloc_nofs_save();\n\telse if ((gfp_mask & (__GFP_FS | __GFP_IO)) == 0)\n\t\tflags = memalloc_noio_save();\n\n\tdo {\n\t\tret = vmap_pages_range(addr, addr + size, prot, area->pages,\n\t\t\tpage_shift);\n\t\tif (nofail && (ret < 0))\n\t\t\tschedule_timeout_uninterruptible(1);\n\t} while (nofail && (ret < 0));\n\n\tif ((gfp_mask & (__GFP_FS | __GFP_IO)) == __GFP_IO)\n\t\tmemalloc_nofs_restore(flags);\n\telse if ((gfp_mask & (__GFP_FS | __GFP_IO)) == 0)\n\t\tmemalloc_noio_restore(flags);\n\n\tif (ret < 0) {\n\t\twarn_alloc(gfp_mask, NULL,\n\t\t\t\"vmalloc error: size %lu, failed to map pages\",\n\t\t\tarea->nr_pages * PAGE_SIZE);\n\t\tgoto fail;\n\t}\n\n\treturn area->addr;\n\nfail:\n\tvfree(area->addr);\n\treturn NULL;\n}\n\n \nvoid *__vmalloc_node_range(unsigned long size, unsigned long align,\n\t\t\tunsigned long start, unsigned long end, gfp_t gfp_mask,\n\t\t\tpgprot_t prot, unsigned long vm_flags, int node,\n\t\t\tconst void *caller)\n{\n\tstruct vm_struct *area;\n\tvoid *ret;\n\tkasan_vmalloc_flags_t kasan_flags = KASAN_VMALLOC_NONE;\n\tunsigned long real_size = size;\n\tunsigned long real_align = align;\n\tunsigned int shift = PAGE_SHIFT;\n\n\tif (WARN_ON_ONCE(!size))\n\t\treturn NULL;\n\n\tif ((size >> PAGE_SHIFT) > totalram_pages()) {\n\t\twarn_alloc(gfp_mask, NULL,\n\t\t\t\"vmalloc error: size %lu, exceeds total pages\",\n\t\t\treal_size);\n\t\treturn NULL;\n\t}\n\n\tif (vmap_allow_huge && (vm_flags & VM_ALLOW_HUGE_VMAP)) {\n\t\tunsigned long size_per_node;\n\n\t\t \n\n\t\tsize_per_node = size;\n\t\tif (node == NUMA_NO_NODE)\n\t\t\tsize_per_node /= num_online_nodes();\n\t\tif (arch_vmap_pmd_supported(prot) && size_per_node >= PMD_SIZE)\n\t\t\tshift = PMD_SHIFT;\n\t\telse\n\t\t\tshift = arch_vmap_pte_supported_shift(size_per_node);\n\n\t\talign = max(real_align, 1UL << shift);\n\t\tsize = ALIGN(real_size, 1UL << shift);\n\t}\n\nagain:\n\tarea = __get_vm_area_node(real_size, align, shift, VM_ALLOC |\n\t\t\t\t  VM_UNINITIALIZED | vm_flags, start, end, node,\n\t\t\t\t  gfp_mask, caller);\n\tif (!area) {\n\t\tbool nofail = gfp_mask & __GFP_NOFAIL;\n\t\twarn_alloc(gfp_mask, NULL,\n\t\t\t\"vmalloc error: size %lu, vm_struct allocation failed%s\",\n\t\t\treal_size, (nofail) ? \". Retrying.\" : \"\");\n\t\tif (nofail) {\n\t\t\tschedule_timeout_uninterruptible(1);\n\t\t\tgoto again;\n\t\t}\n\t\tgoto fail;\n\t}\n\n\t \n\tif (pgprot_val(prot) == pgprot_val(PAGE_KERNEL)) {\n\t\tif (kasan_hw_tags_enabled()) {\n\t\t\t \n\t\t\tprot = arch_vmap_pgprot_tagged(prot);\n\n\t\t\t \n\t\t\tgfp_mask |= __GFP_SKIP_KASAN | __GFP_SKIP_ZERO;\n\t\t}\n\n\t\t \n\t\tkasan_flags |= KASAN_VMALLOC_PROT_NORMAL;\n\t}\n\n\t \n\tret = __vmalloc_area_node(area, gfp_mask, prot, shift, node);\n\tif (!ret)\n\t\tgoto fail;\n\n\t \n\tkasan_flags |= KASAN_VMALLOC_VM_ALLOC;\n\tif (!want_init_on_free() && want_init_on_alloc(gfp_mask) &&\n\t    (gfp_mask & __GFP_SKIP_ZERO))\n\t\tkasan_flags |= KASAN_VMALLOC_INIT;\n\t \n\tarea->addr = kasan_unpoison_vmalloc(area->addr, real_size, kasan_flags);\n\n\t \n\tclear_vm_uninitialized_flag(area);\n\n\tsize = PAGE_ALIGN(size);\n\tif (!(vm_flags & VM_DEFER_KMEMLEAK))\n\t\tkmemleak_vmalloc(area, size, gfp_mask);\n\n\treturn area->addr;\n\nfail:\n\tif (shift > PAGE_SHIFT) {\n\t\tshift = PAGE_SHIFT;\n\t\talign = real_align;\n\t\tsize = real_size;\n\t\tgoto again;\n\t}\n\n\treturn NULL;\n}\n\n \nvoid *__vmalloc_node(unsigned long size, unsigned long align,\n\t\t\t    gfp_t gfp_mask, int node, const void *caller)\n{\n\treturn __vmalloc_node_range(size, align, VMALLOC_START, VMALLOC_END,\n\t\t\t\tgfp_mask, PAGE_KERNEL, 0, node, caller);\n}\n \n#ifdef CONFIG_TEST_VMALLOC_MODULE\nEXPORT_SYMBOL_GPL(__vmalloc_node);\n#endif\n\nvoid *__vmalloc(unsigned long size, gfp_t gfp_mask)\n{\n\treturn __vmalloc_node(size, 1, gfp_mask, NUMA_NO_NODE,\n\t\t\t\t__builtin_return_address(0));\n}\nEXPORT_SYMBOL(__vmalloc);\n\n \nvoid *vmalloc(unsigned long size)\n{\n\treturn __vmalloc_node(size, 1, GFP_KERNEL, NUMA_NO_NODE,\n\t\t\t\t__builtin_return_address(0));\n}\nEXPORT_SYMBOL(vmalloc);\n\n \nvoid *vmalloc_huge(unsigned long size, gfp_t gfp_mask)\n{\n\treturn __vmalloc_node_range(size, 1, VMALLOC_START, VMALLOC_END,\n\t\t\t\t    gfp_mask, PAGE_KERNEL, VM_ALLOW_HUGE_VMAP,\n\t\t\t\t    NUMA_NO_NODE, __builtin_return_address(0));\n}\nEXPORT_SYMBOL_GPL(vmalloc_huge);\n\n \nvoid *vzalloc(unsigned long size)\n{\n\treturn __vmalloc_node(size, 1, GFP_KERNEL | __GFP_ZERO, NUMA_NO_NODE,\n\t\t\t\t__builtin_return_address(0));\n}\nEXPORT_SYMBOL(vzalloc);\n\n \nvoid *vmalloc_user(unsigned long size)\n{\n\treturn __vmalloc_node_range(size, SHMLBA,  VMALLOC_START, VMALLOC_END,\n\t\t\t\t    GFP_KERNEL | __GFP_ZERO, PAGE_KERNEL,\n\t\t\t\t    VM_USERMAP, NUMA_NO_NODE,\n\t\t\t\t    __builtin_return_address(0));\n}\nEXPORT_SYMBOL(vmalloc_user);\n\n \nvoid *vmalloc_node(unsigned long size, int node)\n{\n\treturn __vmalloc_node(size, 1, GFP_KERNEL, node,\n\t\t\t__builtin_return_address(0));\n}\nEXPORT_SYMBOL(vmalloc_node);\n\n \nvoid *vzalloc_node(unsigned long size, int node)\n{\n\treturn __vmalloc_node(size, 1, GFP_KERNEL | __GFP_ZERO, node,\n\t\t\t\t__builtin_return_address(0));\n}\nEXPORT_SYMBOL(vzalloc_node);\n\n#if defined(CONFIG_64BIT) && defined(CONFIG_ZONE_DMA32)\n#define GFP_VMALLOC32 (GFP_DMA32 | GFP_KERNEL)\n#elif defined(CONFIG_64BIT) && defined(CONFIG_ZONE_DMA)\n#define GFP_VMALLOC32 (GFP_DMA | GFP_KERNEL)\n#else\n \n#define GFP_VMALLOC32 (GFP_DMA32 | GFP_KERNEL)\n#endif\n\n \nvoid *vmalloc_32(unsigned long size)\n{\n\treturn __vmalloc_node(size, 1, GFP_VMALLOC32, NUMA_NO_NODE,\n\t\t\t__builtin_return_address(0));\n}\nEXPORT_SYMBOL(vmalloc_32);\n\n \nvoid *vmalloc_32_user(unsigned long size)\n{\n\treturn __vmalloc_node_range(size, SHMLBA,  VMALLOC_START, VMALLOC_END,\n\t\t\t\t    GFP_VMALLOC32 | __GFP_ZERO, PAGE_KERNEL,\n\t\t\t\t    VM_USERMAP, NUMA_NO_NODE,\n\t\t\t\t    __builtin_return_address(0));\n}\nEXPORT_SYMBOL(vmalloc_32_user);\n\n \nstatic size_t zero_iter(struct iov_iter *iter, size_t count)\n{\n\tsize_t remains = count;\n\n\twhile (remains > 0) {\n\t\tsize_t num, copied;\n\n\t\tnum = min_t(size_t, remains, PAGE_SIZE);\n\t\tcopied = copy_page_to_iter_nofault(ZERO_PAGE(0), 0, num, iter);\n\t\tremains -= copied;\n\n\t\tif (copied < num)\n\t\t\tbreak;\n\t}\n\n\treturn count - remains;\n}\n\n \nstatic size_t aligned_vread_iter(struct iov_iter *iter,\n\t\t\t\t const char *addr, size_t count)\n{\n\tsize_t remains = count;\n\tstruct page *page;\n\n\twhile (remains > 0) {\n\t\tunsigned long offset, length;\n\t\tsize_t copied = 0;\n\n\t\toffset = offset_in_page(addr);\n\t\tlength = PAGE_SIZE - offset;\n\t\tif (length > remains)\n\t\t\tlength = remains;\n\t\tpage = vmalloc_to_page(addr);\n\t\t \n\t\tif (page)\n\t\t\tcopied = copy_page_to_iter_nofault(page, offset,\n\t\t\t\t\t\t\t   length, iter);\n\t\telse\n\t\t\tcopied = zero_iter(iter, length);\n\n\t\taddr += copied;\n\t\tremains -= copied;\n\n\t\tif (copied != length)\n\t\t\tbreak;\n\t}\n\n\treturn count - remains;\n}\n\n \nstatic size_t vmap_ram_vread_iter(struct iov_iter *iter, const char *addr,\n\t\t\t\t  size_t count, unsigned long flags)\n{\n\tchar *start;\n\tstruct vmap_block *vb;\n\tstruct xarray *xa;\n\tunsigned long offset;\n\tunsigned int rs, re;\n\tsize_t remains, n;\n\n\t \n\tif (!(flags & VMAP_BLOCK))\n\t\treturn aligned_vread_iter(iter, addr, count);\n\n\tremains = count;\n\n\t \n\txa = addr_to_vb_xa((unsigned long) addr);\n\tvb = xa_load(xa, addr_to_vb_idx((unsigned long)addr));\n\tif (!vb)\n\t\tgoto finished_zero;\n\n\tspin_lock(&vb->lock);\n\tif (bitmap_empty(vb->used_map, VMAP_BBMAP_BITS)) {\n\t\tspin_unlock(&vb->lock);\n\t\tgoto finished_zero;\n\t}\n\n\tfor_each_set_bitrange(rs, re, vb->used_map, VMAP_BBMAP_BITS) {\n\t\tsize_t copied;\n\n\t\tif (remains == 0)\n\t\t\tgoto finished;\n\n\t\tstart = vmap_block_vaddr(vb->va->va_start, rs);\n\n\t\tif (addr < start) {\n\t\t\tsize_t to_zero = min_t(size_t, start - addr, remains);\n\t\t\tsize_t zeroed = zero_iter(iter, to_zero);\n\n\t\t\taddr += zeroed;\n\t\t\tremains -= zeroed;\n\n\t\t\tif (remains == 0 || zeroed != to_zero)\n\t\t\t\tgoto finished;\n\t\t}\n\n\t\t \n\t\toffset = offset_in_page(addr);\n\t\tn = ((re - rs + 1) << PAGE_SHIFT) - offset;\n\t\tif (n > remains)\n\t\t\tn = remains;\n\n\t\tcopied = aligned_vread_iter(iter, start + offset, n);\n\n\t\taddr += copied;\n\t\tremains -= copied;\n\n\t\tif (copied != n)\n\t\t\tgoto finished;\n\t}\n\n\tspin_unlock(&vb->lock);\n\nfinished_zero:\n\t \n\treturn count - remains + zero_iter(iter, remains);\nfinished:\n\t \n\tspin_unlock(&vb->lock);\n\treturn count - remains;\n}\n\n \nlong vread_iter(struct iov_iter *iter, const char *addr, size_t count)\n{\n\tstruct vmap_area *va;\n\tstruct vm_struct *vm;\n\tchar *vaddr;\n\tsize_t n, size, flags, remains;\n\n\taddr = kasan_reset_tag(addr);\n\n\t \n\tif ((unsigned long) addr + count < count)\n\t\tcount = -(unsigned long) addr;\n\n\tremains = count;\n\n\tspin_lock(&vmap_area_lock);\n\tva = find_vmap_area_exceed_addr((unsigned long)addr);\n\tif (!va)\n\t\tgoto finished_zero;\n\n\t \n\tif ((unsigned long)addr + remains <= va->va_start)\n\t\tgoto finished_zero;\n\n\tlist_for_each_entry_from(va, &vmap_area_list, list) {\n\t\tsize_t copied;\n\n\t\tif (remains == 0)\n\t\t\tgoto finished;\n\n\t\tvm = va->vm;\n\t\tflags = va->flags & VMAP_FLAGS_MASK;\n\t\t \n\t\tWARN_ON(flags == VMAP_BLOCK);\n\n\t\tif (!vm && !flags)\n\t\t\tcontinue;\n\n\t\tif (vm && (vm->flags & VM_UNINITIALIZED))\n\t\t\tcontinue;\n\n\t\t \n\t\tsmp_rmb();\n\n\t\tvaddr = (char *) va->va_start;\n\t\tsize = vm ? get_vm_area_size(vm) : va_size(va);\n\n\t\tif (addr >= vaddr + size)\n\t\t\tcontinue;\n\n\t\tif (addr < vaddr) {\n\t\t\tsize_t to_zero = min_t(size_t, vaddr - addr, remains);\n\t\t\tsize_t zeroed = zero_iter(iter, to_zero);\n\n\t\t\taddr += zeroed;\n\t\t\tremains -= zeroed;\n\n\t\t\tif (remains == 0 || zeroed != to_zero)\n\t\t\t\tgoto finished;\n\t\t}\n\n\t\tn = vaddr + size - addr;\n\t\tif (n > remains)\n\t\t\tn = remains;\n\n\t\tif (flags & VMAP_RAM)\n\t\t\tcopied = vmap_ram_vread_iter(iter, addr, n, flags);\n\t\telse if (!(vm->flags & VM_IOREMAP))\n\t\t\tcopied = aligned_vread_iter(iter, addr, n);\n\t\telse  \n\t\t\tcopied = zero_iter(iter, n);\n\n\t\taddr += copied;\n\t\tremains -= copied;\n\n\t\tif (copied != n)\n\t\t\tgoto finished;\n\t}\n\nfinished_zero:\n\tspin_unlock(&vmap_area_lock);\n\t \n\treturn count - remains + zero_iter(iter, remains);\nfinished:\n\t \n\tspin_unlock(&vmap_area_lock);\n\n\treturn count - remains;\n}\n\n \nint remap_vmalloc_range_partial(struct vm_area_struct *vma, unsigned long uaddr,\n\t\t\t\tvoid *kaddr, unsigned long pgoff,\n\t\t\t\tunsigned long size)\n{\n\tstruct vm_struct *area;\n\tunsigned long off;\n\tunsigned long end_index;\n\n\tif (check_shl_overflow(pgoff, PAGE_SHIFT, &off))\n\t\treturn -EINVAL;\n\n\tsize = PAGE_ALIGN(size);\n\n\tif (!PAGE_ALIGNED(uaddr) || !PAGE_ALIGNED(kaddr))\n\t\treturn -EINVAL;\n\n\tarea = find_vm_area(kaddr);\n\tif (!area)\n\t\treturn -EINVAL;\n\n\tif (!(area->flags & (VM_USERMAP | VM_DMA_COHERENT)))\n\t\treturn -EINVAL;\n\n\tif (check_add_overflow(size, off, &end_index) ||\n\t    end_index > get_vm_area_size(area))\n\t\treturn -EINVAL;\n\tkaddr += off;\n\n\tdo {\n\t\tstruct page *page = vmalloc_to_page(kaddr);\n\t\tint ret;\n\n\t\tret = vm_insert_page(vma, uaddr, page);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tuaddr += PAGE_SIZE;\n\t\tkaddr += PAGE_SIZE;\n\t\tsize -= PAGE_SIZE;\n\t} while (size > 0);\n\n\tvm_flags_set(vma, VM_DONTEXPAND | VM_DONTDUMP);\n\n\treturn 0;\n}\n\n \nint remap_vmalloc_range(struct vm_area_struct *vma, void *addr,\n\t\t\t\t\t\tunsigned long pgoff)\n{\n\treturn remap_vmalloc_range_partial(vma, vma->vm_start,\n\t\t\t\t\t   addr, pgoff,\n\t\t\t\t\t   vma->vm_end - vma->vm_start);\n}\nEXPORT_SYMBOL(remap_vmalloc_range);\n\nvoid free_vm_area(struct vm_struct *area)\n{\n\tstruct vm_struct *ret;\n\tret = remove_vm_area(area->addr);\n\tBUG_ON(ret != area);\n\tkfree(area);\n}\nEXPORT_SYMBOL_GPL(free_vm_area);\n\n#ifdef CONFIG_SMP\nstatic struct vmap_area *node_to_va(struct rb_node *n)\n{\n\treturn rb_entry_safe(n, struct vmap_area, rb_node);\n}\n\n \nstatic struct vmap_area *\npvm_find_va_enclose_addr(unsigned long addr)\n{\n\tstruct vmap_area *va, *tmp;\n\tstruct rb_node *n;\n\n\tn = free_vmap_area_root.rb_node;\n\tva = NULL;\n\n\twhile (n) {\n\t\ttmp = rb_entry(n, struct vmap_area, rb_node);\n\t\tif (tmp->va_start <= addr) {\n\t\t\tva = tmp;\n\t\t\tif (tmp->va_end >= addr)\n\t\t\t\tbreak;\n\n\t\t\tn = n->rb_right;\n\t\t} else {\n\t\t\tn = n->rb_left;\n\t\t}\n\t}\n\n\treturn va;\n}\n\n \nstatic unsigned long\npvm_determine_end_from_reverse(struct vmap_area **va, unsigned long align)\n{\n\tunsigned long vmalloc_end = VMALLOC_END & ~(align - 1);\n\tunsigned long addr;\n\n\tif (likely(*va)) {\n\t\tlist_for_each_entry_from_reverse((*va),\n\t\t\t\t&free_vmap_area_list, list) {\n\t\t\taddr = min((*va)->va_end & ~(align - 1), vmalloc_end);\n\t\t\tif ((*va)->va_start < addr)\n\t\t\t\treturn addr;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstruct vm_struct **pcpu_get_vm_areas(const unsigned long *offsets,\n\t\t\t\t     const size_t *sizes, int nr_vms,\n\t\t\t\t     size_t align)\n{\n\tconst unsigned long vmalloc_start = ALIGN(VMALLOC_START, align);\n\tconst unsigned long vmalloc_end = VMALLOC_END & ~(align - 1);\n\tstruct vmap_area **vas, *va;\n\tstruct vm_struct **vms;\n\tint area, area2, last_area, term_area;\n\tunsigned long base, start, size, end, last_end, orig_start, orig_end;\n\tbool purged = false;\n\n\t \n\tBUG_ON(offset_in_page(align) || !is_power_of_2(align));\n\tfor (last_area = 0, area = 0; area < nr_vms; area++) {\n\t\tstart = offsets[area];\n\t\tend = start + sizes[area];\n\n\t\t \n\t\tBUG_ON(!IS_ALIGNED(offsets[area], align));\n\t\tBUG_ON(!IS_ALIGNED(sizes[area], align));\n\n\t\t \n\t\tif (start > offsets[last_area])\n\t\t\tlast_area = area;\n\n\t\tfor (area2 = area + 1; area2 < nr_vms; area2++) {\n\t\t\tunsigned long start2 = offsets[area2];\n\t\t\tunsigned long end2 = start2 + sizes[area2];\n\n\t\t\tBUG_ON(start2 < end && start < end2);\n\t\t}\n\t}\n\tlast_end = offsets[last_area] + sizes[last_area];\n\n\tif (vmalloc_end - vmalloc_start < last_end) {\n\t\tWARN_ON(true);\n\t\treturn NULL;\n\t}\n\n\tvms = kcalloc(nr_vms, sizeof(vms[0]), GFP_KERNEL);\n\tvas = kcalloc(nr_vms, sizeof(vas[0]), GFP_KERNEL);\n\tif (!vas || !vms)\n\t\tgoto err_free2;\n\n\tfor (area = 0; area < nr_vms; area++) {\n\t\tvas[area] = kmem_cache_zalloc(vmap_area_cachep, GFP_KERNEL);\n\t\tvms[area] = kzalloc(sizeof(struct vm_struct), GFP_KERNEL);\n\t\tif (!vas[area] || !vms[area])\n\t\t\tgoto err_free;\n\t}\nretry:\n\tspin_lock(&free_vmap_area_lock);\n\n\t \n\tarea = term_area = last_area;\n\tstart = offsets[area];\n\tend = start + sizes[area];\n\n\tva = pvm_find_va_enclose_addr(vmalloc_end);\n\tbase = pvm_determine_end_from_reverse(&va, align) - end;\n\n\twhile (true) {\n\t\t \n\t\tif (base + last_end < vmalloc_start + last_end)\n\t\t\tgoto overflow;\n\n\t\t \n\t\tif (va == NULL)\n\t\t\tgoto overflow;\n\n\t\t \n\t\tif (base + end > va->va_end) {\n\t\t\tbase = pvm_determine_end_from_reverse(&va, align) - end;\n\t\t\tterm_area = area;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (base + start < va->va_start) {\n\t\t\tva = node_to_va(rb_prev(&va->rb_node));\n\t\t\tbase = pvm_determine_end_from_reverse(&va, align) - end;\n\t\t\tterm_area = area;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tarea = (area + nr_vms - 1) % nr_vms;\n\t\tif (area == term_area)\n\t\t\tbreak;\n\n\t\tstart = offsets[area];\n\t\tend = start + sizes[area];\n\t\tva = pvm_find_va_enclose_addr(base + end);\n\t}\n\n\t \n\tfor (area = 0; area < nr_vms; area++) {\n\t\tint ret;\n\n\t\tstart = base + offsets[area];\n\t\tsize = sizes[area];\n\n\t\tva = pvm_find_va_enclose_addr(start);\n\t\tif (WARN_ON_ONCE(va == NULL))\n\t\t\t \n\t\t\tgoto recovery;\n\n\t\tret = adjust_va_to_fit_type(&free_vmap_area_root,\n\t\t\t\t\t    &free_vmap_area_list,\n\t\t\t\t\t    va, start, size);\n\t\tif (WARN_ON_ONCE(unlikely(ret)))\n\t\t\t \n\t\t\tgoto recovery;\n\n\t\t \n\t\tva = vas[area];\n\t\tva->va_start = start;\n\t\tva->va_end = start + size;\n\t}\n\n\tspin_unlock(&free_vmap_area_lock);\n\n\t \n\tfor (area = 0; area < nr_vms; area++) {\n\t\tif (kasan_populate_vmalloc(vas[area]->va_start, sizes[area]))\n\t\t\tgoto err_free_shadow;\n\t}\n\n\t \n\tspin_lock(&vmap_area_lock);\n\tfor (area = 0; area < nr_vms; area++) {\n\t\tinsert_vmap_area(vas[area], &vmap_area_root, &vmap_area_list);\n\n\t\tsetup_vmalloc_vm_locked(vms[area], vas[area], VM_ALLOC,\n\t\t\t\t pcpu_get_vm_areas);\n\t}\n\tspin_unlock(&vmap_area_lock);\n\n\t \n\tfor (area = 0; area < nr_vms; area++)\n\t\tvms[area]->addr = kasan_unpoison_vmalloc(vms[area]->addr,\n\t\t\t\tvms[area]->size, KASAN_VMALLOC_PROT_NORMAL);\n\n\tkfree(vas);\n\treturn vms;\n\nrecovery:\n\t \n\twhile (area--) {\n\t\torig_start = vas[area]->va_start;\n\t\torig_end = vas[area]->va_end;\n\t\tva = merge_or_add_vmap_area_augment(vas[area], &free_vmap_area_root,\n\t\t\t\t&free_vmap_area_list);\n\t\tif (va)\n\t\t\tkasan_release_vmalloc(orig_start, orig_end,\n\t\t\t\tva->va_start, va->va_end);\n\t\tvas[area] = NULL;\n\t}\n\noverflow:\n\tspin_unlock(&free_vmap_area_lock);\n\tif (!purged) {\n\t\treclaim_and_purge_vmap_areas();\n\t\tpurged = true;\n\n\t\t \n\t\tfor (area = 0; area < nr_vms; area++) {\n\t\t\tif (vas[area])\n\t\t\t\tcontinue;\n\n\t\t\tvas[area] = kmem_cache_zalloc(\n\t\t\t\tvmap_area_cachep, GFP_KERNEL);\n\t\t\tif (!vas[area])\n\t\t\t\tgoto err_free;\n\t\t}\n\n\t\tgoto retry;\n\t}\n\nerr_free:\n\tfor (area = 0; area < nr_vms; area++) {\n\t\tif (vas[area])\n\t\t\tkmem_cache_free(vmap_area_cachep, vas[area]);\n\n\t\tkfree(vms[area]);\n\t}\nerr_free2:\n\tkfree(vas);\n\tkfree(vms);\n\treturn NULL;\n\nerr_free_shadow:\n\tspin_lock(&free_vmap_area_lock);\n\t \n\tfor (area = 0; area < nr_vms; area++) {\n\t\torig_start = vas[area]->va_start;\n\t\torig_end = vas[area]->va_end;\n\t\tva = merge_or_add_vmap_area_augment(vas[area], &free_vmap_area_root,\n\t\t\t\t&free_vmap_area_list);\n\t\tif (va)\n\t\t\tkasan_release_vmalloc(orig_start, orig_end,\n\t\t\t\tva->va_start, va->va_end);\n\t\tvas[area] = NULL;\n\t\tkfree(vms[area]);\n\t}\n\tspin_unlock(&free_vmap_area_lock);\n\tkfree(vas);\n\tkfree(vms);\n\treturn NULL;\n}\n\n \nvoid pcpu_free_vm_areas(struct vm_struct **vms, int nr_vms)\n{\n\tint i;\n\n\tfor (i = 0; i < nr_vms; i++)\n\t\tfree_vm_area(vms[i]);\n\tkfree(vms);\n}\n#endif\t \n\n#ifdef CONFIG_PRINTK\nbool vmalloc_dump_obj(void *object)\n{\n\tvoid *objp = (void *)PAGE_ALIGN((unsigned long)object);\n\tconst void *caller;\n\tstruct vm_struct *vm;\n\tstruct vmap_area *va;\n\tunsigned long addr;\n\tunsigned int nr_pages;\n\n\tif (!spin_trylock(&vmap_area_lock))\n\t\treturn false;\n\tva = __find_vmap_area((unsigned long)objp, &vmap_area_root);\n\tif (!va) {\n\t\tspin_unlock(&vmap_area_lock);\n\t\treturn false;\n\t}\n\n\tvm = va->vm;\n\tif (!vm) {\n\t\tspin_unlock(&vmap_area_lock);\n\t\treturn false;\n\t}\n\taddr = (unsigned long)vm->addr;\n\tcaller = vm->caller;\n\tnr_pages = vm->nr_pages;\n\tspin_unlock(&vmap_area_lock);\n\tpr_cont(\" %u-page vmalloc region starting at %#lx allocated at %pS\\n\",\n\t\tnr_pages, addr, caller);\n\treturn true;\n}\n#endif\n\n#ifdef CONFIG_PROC_FS\nstatic void *s_start(struct seq_file *m, loff_t *pos)\n\t__acquires(&vmap_purge_lock)\n\t__acquires(&vmap_area_lock)\n{\n\tmutex_lock(&vmap_purge_lock);\n\tspin_lock(&vmap_area_lock);\n\n\treturn seq_list_start(&vmap_area_list, *pos);\n}\n\nstatic void *s_next(struct seq_file *m, void *p, loff_t *pos)\n{\n\treturn seq_list_next(p, &vmap_area_list, pos);\n}\n\nstatic void s_stop(struct seq_file *m, void *p)\n\t__releases(&vmap_area_lock)\n\t__releases(&vmap_purge_lock)\n{\n\tspin_unlock(&vmap_area_lock);\n\tmutex_unlock(&vmap_purge_lock);\n}\n\nstatic void show_numa_info(struct seq_file *m, struct vm_struct *v)\n{\n\tif (IS_ENABLED(CONFIG_NUMA)) {\n\t\tunsigned int nr, *counters = m->private;\n\t\tunsigned int step = 1U << vm_area_page_order(v);\n\n\t\tif (!counters)\n\t\t\treturn;\n\n\t\tif (v->flags & VM_UNINITIALIZED)\n\t\t\treturn;\n\t\t \n\t\tsmp_rmb();\n\n\t\tmemset(counters, 0, nr_node_ids * sizeof(unsigned int));\n\n\t\tfor (nr = 0; nr < v->nr_pages; nr += step)\n\t\t\tcounters[page_to_nid(v->pages[nr])] += step;\n\t\tfor_each_node_state(nr, N_HIGH_MEMORY)\n\t\t\tif (counters[nr])\n\t\t\t\tseq_printf(m, \" N%u=%u\", nr, counters[nr]);\n\t}\n}\n\nstatic void show_purge_info(struct seq_file *m)\n{\n\tstruct vmap_area *va;\n\n\tspin_lock(&purge_vmap_area_lock);\n\tlist_for_each_entry(va, &purge_vmap_area_list, list) {\n\t\tseq_printf(m, \"0x%pK-0x%pK %7ld unpurged vm_area\\n\",\n\t\t\t(void *)va->va_start, (void *)va->va_end,\n\t\t\tva->va_end - va->va_start);\n\t}\n\tspin_unlock(&purge_vmap_area_lock);\n}\n\nstatic int s_show(struct seq_file *m, void *p)\n{\n\tstruct vmap_area *va;\n\tstruct vm_struct *v;\n\n\tva = list_entry(p, struct vmap_area, list);\n\n\tif (!va->vm) {\n\t\tif (va->flags & VMAP_RAM)\n\t\t\tseq_printf(m, \"0x%pK-0x%pK %7ld vm_map_ram\\n\",\n\t\t\t\t(void *)va->va_start, (void *)va->va_end,\n\t\t\t\tva->va_end - va->va_start);\n\n\t\tgoto final;\n\t}\n\n\tv = va->vm;\n\n\tseq_printf(m, \"0x%pK-0x%pK %7ld\",\n\t\tv->addr, v->addr + v->size, v->size);\n\n\tif (v->caller)\n\t\tseq_printf(m, \" %pS\", v->caller);\n\n\tif (v->nr_pages)\n\t\tseq_printf(m, \" pages=%d\", v->nr_pages);\n\n\tif (v->phys_addr)\n\t\tseq_printf(m, \" phys=%pa\", &v->phys_addr);\n\n\tif (v->flags & VM_IOREMAP)\n\t\tseq_puts(m, \" ioremap\");\n\n\tif (v->flags & VM_ALLOC)\n\t\tseq_puts(m, \" vmalloc\");\n\n\tif (v->flags & VM_MAP)\n\t\tseq_puts(m, \" vmap\");\n\n\tif (v->flags & VM_USERMAP)\n\t\tseq_puts(m, \" user\");\n\n\tif (v->flags & VM_DMA_COHERENT)\n\t\tseq_puts(m, \" dma-coherent\");\n\n\tif (is_vmalloc_addr(v->pages))\n\t\tseq_puts(m, \" vpages\");\n\n\tshow_numa_info(m, v);\n\tseq_putc(m, '\\n');\n\n\t \nfinal:\n\tif (list_is_last(&va->list, &vmap_area_list))\n\t\tshow_purge_info(m);\n\n\treturn 0;\n}\n\nstatic const struct seq_operations vmalloc_op = {\n\t.start = s_start,\n\t.next = s_next,\n\t.stop = s_stop,\n\t.show = s_show,\n};\n\nstatic int __init proc_vmalloc_init(void)\n{\n\tif (IS_ENABLED(CONFIG_NUMA))\n\t\tproc_create_seq_private(\"vmallocinfo\", 0400, NULL,\n\t\t\t\t&vmalloc_op,\n\t\t\t\tnr_node_ids * sizeof(unsigned int), NULL);\n\telse\n\t\tproc_create_seq(\"vmallocinfo\", 0400, NULL, &vmalloc_op);\n\treturn 0;\n}\nmodule_init(proc_vmalloc_init);\n\n#endif\n\nvoid __init vmalloc_init(void)\n{\n\tstruct vmap_area *va;\n\tstruct vm_struct *tmp;\n\tint i;\n\n\t \n\tvmap_area_cachep = KMEM_CACHE(vmap_area, SLAB_PANIC);\n\n\tfor_each_possible_cpu(i) {\n\t\tstruct vmap_block_queue *vbq;\n\t\tstruct vfree_deferred *p;\n\n\t\tvbq = &per_cpu(vmap_block_queue, i);\n\t\tspin_lock_init(&vbq->lock);\n\t\tINIT_LIST_HEAD(&vbq->free);\n\t\tp = &per_cpu(vfree_deferred, i);\n\t\tinit_llist_head(&p->list);\n\t\tINIT_WORK(&p->wq, delayed_vfree_work);\n\t\txa_init(&vbq->vmap_blocks);\n\t}\n\n\t \n\tfor (tmp = vmlist; tmp; tmp = tmp->next) {\n\t\tva = kmem_cache_zalloc(vmap_area_cachep, GFP_NOWAIT);\n\t\tif (WARN_ON_ONCE(!va))\n\t\t\tcontinue;\n\n\t\tva->va_start = (unsigned long)tmp->addr;\n\t\tva->va_end = va->va_start + tmp->size;\n\t\tva->vm = tmp;\n\t\tinsert_vmap_area(va, &vmap_area_root, &vmap_area_list);\n\t}\n\n\t \n\tvmap_init_free_space();\n\tvmap_initialized = true;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}