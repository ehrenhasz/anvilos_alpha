{
  "module_name": "vaddr.c",
  "hash_id": "0d9ba9163dfd2367325499742ce46df2f6aaee81f5ebb149f1dc1e1ba4857363",
  "original_prompt": "Ingested from linux-6.6.14/mm/damon/vaddr.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) \"damon-va: \" fmt\n\n#include <asm-generic/mman-common.h>\n#include <linux/highmem.h>\n#include <linux/hugetlb.h>\n#include <linux/mmu_notifier.h>\n#include <linux/page_idle.h>\n#include <linux/pagewalk.h>\n#include <linux/sched/mm.h>\n\n#include \"ops-common.h\"\n\n#ifdef CONFIG_DAMON_VADDR_KUNIT_TEST\n#undef DAMON_MIN_REGION\n#define DAMON_MIN_REGION 1\n#endif\n\n \nstatic inline struct task_struct *damon_get_task_struct(struct damon_target *t)\n{\n\treturn get_pid_task(t->pid, PIDTYPE_PID);\n}\n\n \nstatic struct mm_struct *damon_get_mm(struct damon_target *t)\n{\n\tstruct task_struct *task;\n\tstruct mm_struct *mm;\n\n\ttask = damon_get_task_struct(t);\n\tif (!task)\n\t\treturn NULL;\n\n\tmm = get_task_mm(task);\n\tput_task_struct(task);\n\treturn mm;\n}\n\n \n\n \nstatic int damon_va_evenly_split_region(struct damon_target *t,\n\t\tstruct damon_region *r, unsigned int nr_pieces)\n{\n\tunsigned long sz_orig, sz_piece, orig_end;\n\tstruct damon_region *n = NULL, *next;\n\tunsigned long start;\n\n\tif (!r || !nr_pieces)\n\t\treturn -EINVAL;\n\n\torig_end = r->ar.end;\n\tsz_orig = damon_sz_region(r);\n\tsz_piece = ALIGN_DOWN(sz_orig / nr_pieces, DAMON_MIN_REGION);\n\n\tif (!sz_piece)\n\t\treturn -EINVAL;\n\n\tr->ar.end = r->ar.start + sz_piece;\n\tnext = damon_next_region(r);\n\tfor (start = r->ar.end; start + sz_piece <= orig_end;\n\t\t\tstart += sz_piece) {\n\t\tn = damon_new_region(start, start + sz_piece);\n\t\tif (!n)\n\t\t\treturn -ENOMEM;\n\t\tdamon_insert_region(n, r, next, t);\n\t\tr = n;\n\t}\n\t \n\tif (n)\n\t\tn->ar.end = orig_end;\n\n\treturn 0;\n}\n\nstatic unsigned long sz_range(struct damon_addr_range *r)\n{\n\treturn r->end - r->start;\n}\n\n \nstatic int __damon_va_three_regions(struct mm_struct *mm,\n\t\t\t\t       struct damon_addr_range regions[3])\n{\n\tstruct damon_addr_range first_gap = {0}, second_gap = {0};\n\tVMA_ITERATOR(vmi, mm, 0);\n\tstruct vm_area_struct *vma, *prev = NULL;\n\tunsigned long start;\n\n\t \n\tfor_each_vma(vmi, vma) {\n\t\tunsigned long gap;\n\n\t\tif (!prev) {\n\t\t\tstart = vma->vm_start;\n\t\t\tgoto next;\n\t\t}\n\t\tgap = vma->vm_start - prev->vm_end;\n\n\t\tif (gap > sz_range(&first_gap)) {\n\t\t\tsecond_gap = first_gap;\n\t\t\tfirst_gap.start = prev->vm_end;\n\t\t\tfirst_gap.end = vma->vm_start;\n\t\t} else if (gap > sz_range(&second_gap)) {\n\t\t\tsecond_gap.start = prev->vm_end;\n\t\t\tsecond_gap.end = vma->vm_start;\n\t\t}\nnext:\n\t\tprev = vma;\n\t}\n\n\tif (!sz_range(&second_gap) || !sz_range(&first_gap))\n\t\treturn -EINVAL;\n\n\t \n\tif (first_gap.start > second_gap.start)\n\t\tswap(first_gap, second_gap);\n\n\t \n\tregions[0].start = ALIGN(start, DAMON_MIN_REGION);\n\tregions[0].end = ALIGN(first_gap.start, DAMON_MIN_REGION);\n\tregions[1].start = ALIGN(first_gap.end, DAMON_MIN_REGION);\n\tregions[1].end = ALIGN(second_gap.start, DAMON_MIN_REGION);\n\tregions[2].start = ALIGN(second_gap.end, DAMON_MIN_REGION);\n\tregions[2].end = ALIGN(prev->vm_end, DAMON_MIN_REGION);\n\n\treturn 0;\n}\n\n \nstatic int damon_va_three_regions(struct damon_target *t,\n\t\t\t\tstruct damon_addr_range regions[3])\n{\n\tstruct mm_struct *mm;\n\tint rc;\n\n\tmm = damon_get_mm(t);\n\tif (!mm)\n\t\treturn -EINVAL;\n\n\tmmap_read_lock(mm);\n\trc = __damon_va_three_regions(mm, regions);\n\tmmap_read_unlock(mm);\n\n\tmmput(mm);\n\treturn rc;\n}\n\n \nstatic void __damon_va_init_regions(struct damon_ctx *ctx,\n\t\t\t\t     struct damon_target *t)\n{\n\tstruct damon_target *ti;\n\tstruct damon_region *r;\n\tstruct damon_addr_range regions[3];\n\tunsigned long sz = 0, nr_pieces;\n\tint i, tidx = 0;\n\n\tif (damon_va_three_regions(t, regions)) {\n\t\tdamon_for_each_target(ti, ctx) {\n\t\t\tif (ti == t)\n\t\t\t\tbreak;\n\t\t\ttidx++;\n\t\t}\n\t\tpr_debug(\"Failed to get three regions of %dth target\\n\", tidx);\n\t\treturn;\n\t}\n\n\tfor (i = 0; i < 3; i++)\n\t\tsz += regions[i].end - regions[i].start;\n\tif (ctx->attrs.min_nr_regions)\n\t\tsz /= ctx->attrs.min_nr_regions;\n\tif (sz < DAMON_MIN_REGION)\n\t\tsz = DAMON_MIN_REGION;\n\n\t \n\tfor (i = 0; i < 3; i++) {\n\t\tr = damon_new_region(regions[i].start, regions[i].end);\n\t\tif (!r) {\n\t\t\tpr_err(\"%d'th init region creation failed\\n\", i);\n\t\t\treturn;\n\t\t}\n\t\tdamon_add_region(r, t);\n\n\t\tnr_pieces = (regions[i].end - regions[i].start) / sz;\n\t\tdamon_va_evenly_split_region(t, r, nr_pieces);\n\t}\n}\n\n \nstatic void damon_va_init(struct damon_ctx *ctx)\n{\n\tstruct damon_target *t;\n\n\tdamon_for_each_target(t, ctx) {\n\t\t \n\t\tif (!damon_nr_regions(t))\n\t\t\t__damon_va_init_regions(ctx, t);\n\t}\n}\n\n \nstatic void damon_va_update(struct damon_ctx *ctx)\n{\n\tstruct damon_addr_range three_regions[3];\n\tstruct damon_target *t;\n\n\tdamon_for_each_target(t, ctx) {\n\t\tif (damon_va_three_regions(t, three_regions))\n\t\t\tcontinue;\n\t\tdamon_set_regions(t, three_regions, 3);\n\t}\n}\n\nstatic int damon_mkold_pmd_entry(pmd_t *pmd, unsigned long addr,\n\t\tunsigned long next, struct mm_walk *walk)\n{\n\tpte_t *pte;\n\tpmd_t pmde;\n\tspinlock_t *ptl;\n\n\tif (pmd_trans_huge(pmdp_get(pmd))) {\n\t\tptl = pmd_lock(walk->mm, pmd);\n\t\tpmde = pmdp_get(pmd);\n\n\t\tif (!pmd_present(pmde)) {\n\t\t\tspin_unlock(ptl);\n\t\t\treturn 0;\n\t\t}\n\n\t\tif (pmd_trans_huge(pmde)) {\n\t\t\tdamon_pmdp_mkold(pmd, walk->vma, addr);\n\t\t\tspin_unlock(ptl);\n\t\t\treturn 0;\n\t\t}\n\t\tspin_unlock(ptl);\n\t}\n\n\tpte = pte_offset_map_lock(walk->mm, pmd, addr, &ptl);\n\tif (!pte) {\n\t\twalk->action = ACTION_AGAIN;\n\t\treturn 0;\n\t}\n\tif (!pte_present(ptep_get(pte)))\n\t\tgoto out;\n\tdamon_ptep_mkold(pte, walk->vma, addr);\nout:\n\tpte_unmap_unlock(pte, ptl);\n\treturn 0;\n}\n\n#ifdef CONFIG_HUGETLB_PAGE\nstatic void damon_hugetlb_mkold(pte_t *pte, struct mm_struct *mm,\n\t\t\t\tstruct vm_area_struct *vma, unsigned long addr)\n{\n\tbool referenced = false;\n\tpte_t entry = huge_ptep_get(pte);\n\tstruct folio *folio = pfn_folio(pte_pfn(entry));\n\tunsigned long psize = huge_page_size(hstate_vma(vma));\n\n\tfolio_get(folio);\n\n\tif (pte_young(entry)) {\n\t\treferenced = true;\n\t\tentry = pte_mkold(entry);\n\t\tset_huge_pte_at(mm, addr, pte, entry, psize);\n\t}\n\n#ifdef CONFIG_MMU_NOTIFIER\n\tif (mmu_notifier_clear_young(mm, addr,\n\t\t\t\t     addr + huge_page_size(hstate_vma(vma))))\n\t\treferenced = true;\n#endif  \n\n\tif (referenced)\n\t\tfolio_set_young(folio);\n\n\tfolio_set_idle(folio);\n\tfolio_put(folio);\n}\n\nstatic int damon_mkold_hugetlb_entry(pte_t *pte, unsigned long hmask,\n\t\t\t\t     unsigned long addr, unsigned long end,\n\t\t\t\t     struct mm_walk *walk)\n{\n\tstruct hstate *h = hstate_vma(walk->vma);\n\tspinlock_t *ptl;\n\tpte_t entry;\n\n\tptl = huge_pte_lock(h, walk->mm, pte);\n\tentry = huge_ptep_get(pte);\n\tif (!pte_present(entry))\n\t\tgoto out;\n\n\tdamon_hugetlb_mkold(pte, walk->mm, walk->vma, addr);\n\nout:\n\tspin_unlock(ptl);\n\treturn 0;\n}\n#else\n#define damon_mkold_hugetlb_entry NULL\n#endif  \n\nstatic const struct mm_walk_ops damon_mkold_ops = {\n\t.pmd_entry = damon_mkold_pmd_entry,\n\t.hugetlb_entry = damon_mkold_hugetlb_entry,\n\t.walk_lock = PGWALK_RDLOCK,\n};\n\nstatic void damon_va_mkold(struct mm_struct *mm, unsigned long addr)\n{\n\tmmap_read_lock(mm);\n\twalk_page_range(mm, addr, addr + 1, &damon_mkold_ops, NULL);\n\tmmap_read_unlock(mm);\n}\n\n \n\nstatic void __damon_va_prepare_access_check(struct mm_struct *mm,\n\t\t\t\t\tstruct damon_region *r)\n{\n\tr->sampling_addr = damon_rand(r->ar.start, r->ar.end);\n\n\tdamon_va_mkold(mm, r->sampling_addr);\n}\n\nstatic void damon_va_prepare_access_checks(struct damon_ctx *ctx)\n{\n\tstruct damon_target *t;\n\tstruct mm_struct *mm;\n\tstruct damon_region *r;\n\n\tdamon_for_each_target(t, ctx) {\n\t\tmm = damon_get_mm(t);\n\t\tif (!mm)\n\t\t\tcontinue;\n\t\tdamon_for_each_region(r, t)\n\t\t\t__damon_va_prepare_access_check(mm, r);\n\t\tmmput(mm);\n\t}\n}\n\nstruct damon_young_walk_private {\n\t \n\tunsigned long *folio_sz;\n\tbool young;\n};\n\nstatic int damon_young_pmd_entry(pmd_t *pmd, unsigned long addr,\n\t\tunsigned long next, struct mm_walk *walk)\n{\n\tpte_t *pte;\n\tpte_t ptent;\n\tspinlock_t *ptl;\n\tstruct folio *folio;\n\tstruct damon_young_walk_private *priv = walk->private;\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tif (pmd_trans_huge(pmdp_get(pmd))) {\n\t\tpmd_t pmde;\n\n\t\tptl = pmd_lock(walk->mm, pmd);\n\t\tpmde = pmdp_get(pmd);\n\n\t\tif (!pmd_present(pmde)) {\n\t\t\tspin_unlock(ptl);\n\t\t\treturn 0;\n\t\t}\n\n\t\tif (!pmd_trans_huge(pmde)) {\n\t\t\tspin_unlock(ptl);\n\t\t\tgoto regular_page;\n\t\t}\n\t\tfolio = damon_get_folio(pmd_pfn(pmde));\n\t\tif (!folio)\n\t\t\tgoto huge_out;\n\t\tif (pmd_young(pmde) || !folio_test_idle(folio) ||\n\t\t\t\t\tmmu_notifier_test_young(walk->mm,\n\t\t\t\t\t\taddr))\n\t\t\tpriv->young = true;\n\t\t*priv->folio_sz = HPAGE_PMD_SIZE;\n\t\tfolio_put(folio);\nhuge_out:\n\t\tspin_unlock(ptl);\n\t\treturn 0;\n\t}\n\nregular_page:\n#endif\t \n\n\tpte = pte_offset_map_lock(walk->mm, pmd, addr, &ptl);\n\tif (!pte) {\n\t\twalk->action = ACTION_AGAIN;\n\t\treturn 0;\n\t}\n\tptent = ptep_get(pte);\n\tif (!pte_present(ptent))\n\t\tgoto out;\n\tfolio = damon_get_folio(pte_pfn(ptent));\n\tif (!folio)\n\t\tgoto out;\n\tif (pte_young(ptent) || !folio_test_idle(folio) ||\n\t\t\tmmu_notifier_test_young(walk->mm, addr))\n\t\tpriv->young = true;\n\t*priv->folio_sz = folio_size(folio);\n\tfolio_put(folio);\nout:\n\tpte_unmap_unlock(pte, ptl);\n\treturn 0;\n}\n\n#ifdef CONFIG_HUGETLB_PAGE\nstatic int damon_young_hugetlb_entry(pte_t *pte, unsigned long hmask,\n\t\t\t\t     unsigned long addr, unsigned long end,\n\t\t\t\t     struct mm_walk *walk)\n{\n\tstruct damon_young_walk_private *priv = walk->private;\n\tstruct hstate *h = hstate_vma(walk->vma);\n\tstruct folio *folio;\n\tspinlock_t *ptl;\n\tpte_t entry;\n\n\tptl = huge_pte_lock(h, walk->mm, pte);\n\tentry = huge_ptep_get(pte);\n\tif (!pte_present(entry))\n\t\tgoto out;\n\n\tfolio = pfn_folio(pte_pfn(entry));\n\tfolio_get(folio);\n\n\tif (pte_young(entry) || !folio_test_idle(folio) ||\n\t    mmu_notifier_test_young(walk->mm, addr))\n\t\tpriv->young = true;\n\t*priv->folio_sz = huge_page_size(h);\n\n\tfolio_put(folio);\n\nout:\n\tspin_unlock(ptl);\n\treturn 0;\n}\n#else\n#define damon_young_hugetlb_entry NULL\n#endif  \n\nstatic const struct mm_walk_ops damon_young_ops = {\n\t.pmd_entry = damon_young_pmd_entry,\n\t.hugetlb_entry = damon_young_hugetlb_entry,\n\t.walk_lock = PGWALK_RDLOCK,\n};\n\nstatic bool damon_va_young(struct mm_struct *mm, unsigned long addr,\n\t\tunsigned long *folio_sz)\n{\n\tstruct damon_young_walk_private arg = {\n\t\t.folio_sz = folio_sz,\n\t\t.young = false,\n\t};\n\n\tmmap_read_lock(mm);\n\twalk_page_range(mm, addr, addr + 1, &damon_young_ops, &arg);\n\tmmap_read_unlock(mm);\n\treturn arg.young;\n}\n\n \nstatic void __damon_va_check_access(struct mm_struct *mm,\n\t\t\t\tstruct damon_region *r, bool same_target)\n{\n\tstatic unsigned long last_addr;\n\tstatic unsigned long last_folio_sz = PAGE_SIZE;\n\tstatic bool last_accessed;\n\n\t \n\tif (same_target && (ALIGN_DOWN(last_addr, last_folio_sz) ==\n\t\t\t\tALIGN_DOWN(r->sampling_addr, last_folio_sz))) {\n\t\tif (last_accessed)\n\t\t\tr->nr_accesses++;\n\t\treturn;\n\t}\n\n\tlast_accessed = damon_va_young(mm, r->sampling_addr, &last_folio_sz);\n\tif (last_accessed)\n\t\tr->nr_accesses++;\n\n\tlast_addr = r->sampling_addr;\n}\n\nstatic unsigned int damon_va_check_accesses(struct damon_ctx *ctx)\n{\n\tstruct damon_target *t;\n\tstruct mm_struct *mm;\n\tstruct damon_region *r;\n\tunsigned int max_nr_accesses = 0;\n\tbool same_target;\n\n\tdamon_for_each_target(t, ctx) {\n\t\tmm = damon_get_mm(t);\n\t\tif (!mm)\n\t\t\tcontinue;\n\t\tsame_target = false;\n\t\tdamon_for_each_region(r, t) {\n\t\t\t__damon_va_check_access(mm, r, same_target);\n\t\t\tmax_nr_accesses = max(r->nr_accesses, max_nr_accesses);\n\t\t\tsame_target = true;\n\t\t}\n\t\tmmput(mm);\n\t}\n\n\treturn max_nr_accesses;\n}\n\n \n\nstatic bool damon_va_target_valid(struct damon_target *t)\n{\n\tstruct task_struct *task;\n\n\ttask = damon_get_task_struct(t);\n\tif (task) {\n\t\tput_task_struct(task);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n#ifndef CONFIG_ADVISE_SYSCALLS\nstatic unsigned long damos_madvise(struct damon_target *target,\n\t\tstruct damon_region *r, int behavior)\n{\n\treturn 0;\n}\n#else\nstatic unsigned long damos_madvise(struct damon_target *target,\n\t\tstruct damon_region *r, int behavior)\n{\n\tstruct mm_struct *mm;\n\tunsigned long start = PAGE_ALIGN(r->ar.start);\n\tunsigned long len = PAGE_ALIGN(damon_sz_region(r));\n\tunsigned long applied;\n\n\tmm = damon_get_mm(target);\n\tif (!mm)\n\t\treturn 0;\n\n\tapplied = do_madvise(mm, start, len, behavior) ? 0 : len;\n\tmmput(mm);\n\n\treturn applied;\n}\n#endif\t \n\nstatic unsigned long damon_va_apply_scheme(struct damon_ctx *ctx,\n\t\tstruct damon_target *t, struct damon_region *r,\n\t\tstruct damos *scheme)\n{\n\tint madv_action;\n\n\tswitch (scheme->action) {\n\tcase DAMOS_WILLNEED:\n\t\tmadv_action = MADV_WILLNEED;\n\t\tbreak;\n\tcase DAMOS_COLD:\n\t\tmadv_action = MADV_COLD;\n\t\tbreak;\n\tcase DAMOS_PAGEOUT:\n\t\tmadv_action = MADV_PAGEOUT;\n\t\tbreak;\n\tcase DAMOS_HUGEPAGE:\n\t\tmadv_action = MADV_HUGEPAGE;\n\t\tbreak;\n\tcase DAMOS_NOHUGEPAGE:\n\t\tmadv_action = MADV_NOHUGEPAGE;\n\t\tbreak;\n\tcase DAMOS_STAT:\n\t\treturn 0;\n\tdefault:\n\t\t \n\t\treturn 0;\n\t}\n\n\treturn damos_madvise(t, r, madv_action);\n}\n\nstatic int damon_va_scheme_score(struct damon_ctx *context,\n\t\tstruct damon_target *t, struct damon_region *r,\n\t\tstruct damos *scheme)\n{\n\n\tswitch (scheme->action) {\n\tcase DAMOS_PAGEOUT:\n\t\treturn damon_cold_score(context, r, scheme);\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn DAMOS_MAX_SCORE;\n}\n\nstatic int __init damon_va_initcall(void)\n{\n\tstruct damon_operations ops = {\n\t\t.id = DAMON_OPS_VADDR,\n\t\t.init = damon_va_init,\n\t\t.update = damon_va_update,\n\t\t.prepare_access_checks = damon_va_prepare_access_checks,\n\t\t.check_accesses = damon_va_check_accesses,\n\t\t.reset_aggregated = NULL,\n\t\t.target_valid = damon_va_target_valid,\n\t\t.cleanup = NULL,\n\t\t.apply_scheme = damon_va_apply_scheme,\n\t\t.get_scheme_score = damon_va_scheme_score,\n\t};\n\t \n\tstruct damon_operations ops_fvaddr = ops;\n\tint err;\n\n\t \n\tops_fvaddr.id = DAMON_OPS_FVADDR;\n\tops_fvaddr.init = NULL;\n\tops_fvaddr.update = NULL;\n\n\terr = damon_register_ops(&ops);\n\tif (err)\n\t\treturn err;\n\treturn damon_register_ops(&ops_fvaddr);\n};\n\nsubsys_initcall(damon_va_initcall);\n\n#include \"vaddr-test.h\"\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}