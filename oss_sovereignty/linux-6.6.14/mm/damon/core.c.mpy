{
  "module_name": "core.c",
  "hash_id": "f76fabf32cb2d7cd3b8a1a7d8400004da78cc4e6d0a2b407047ac4c02426a488",
  "original_prompt": "Ingested from linux-6.6.14/mm/damon/core.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) \"damon: \" fmt\n\n#include <linux/damon.h>\n#include <linux/delay.h>\n#include <linux/kthread.h>\n#include <linux/mm.h>\n#include <linux/slab.h>\n#include <linux/string.h>\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/damon.h>\n\n#ifdef CONFIG_DAMON_KUNIT_TEST\n#undef DAMON_MIN_REGION\n#define DAMON_MIN_REGION 1\n#endif\n\nstatic DEFINE_MUTEX(damon_lock);\nstatic int nr_running_ctxs;\nstatic bool running_exclusive_ctxs;\n\nstatic DEFINE_MUTEX(damon_ops_lock);\nstatic struct damon_operations damon_registered_ops[NR_DAMON_OPS];\n\nstatic struct kmem_cache *damon_region_cache __ro_after_init;\n\n \nstatic bool __damon_is_registered_ops(enum damon_ops_id id)\n{\n\tstruct damon_operations empty_ops = {};\n\n\tif (!memcmp(&empty_ops, &damon_registered_ops[id], sizeof(empty_ops)))\n\t\treturn false;\n\treturn true;\n}\n\n \nbool damon_is_registered_ops(enum damon_ops_id id)\n{\n\tbool registered;\n\n\tif (id >= NR_DAMON_OPS)\n\t\treturn false;\n\tmutex_lock(&damon_ops_lock);\n\tregistered = __damon_is_registered_ops(id);\n\tmutex_unlock(&damon_ops_lock);\n\treturn registered;\n}\n\n \nint damon_register_ops(struct damon_operations *ops)\n{\n\tint err = 0;\n\n\tif (ops->id >= NR_DAMON_OPS)\n\t\treturn -EINVAL;\n\tmutex_lock(&damon_ops_lock);\n\t \n\tif (__damon_is_registered_ops(ops->id)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\tdamon_registered_ops[ops->id] = *ops;\nout:\n\tmutex_unlock(&damon_ops_lock);\n\treturn err;\n}\n\n \nint damon_select_ops(struct damon_ctx *ctx, enum damon_ops_id id)\n{\n\tint err = 0;\n\n\tif (id >= NR_DAMON_OPS)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&damon_ops_lock);\n\tif (!__damon_is_registered_ops(id))\n\t\terr = -EINVAL;\n\telse\n\t\tctx->ops = damon_registered_ops[id];\n\tmutex_unlock(&damon_ops_lock);\n\treturn err;\n}\n\n \nstruct damon_region *damon_new_region(unsigned long start, unsigned long end)\n{\n\tstruct damon_region *region;\n\n\tregion = kmem_cache_alloc(damon_region_cache, GFP_KERNEL);\n\tif (!region)\n\t\treturn NULL;\n\n\tregion->ar.start = start;\n\tregion->ar.end = end;\n\tregion->nr_accesses = 0;\n\tINIT_LIST_HEAD(&region->list);\n\n\tregion->age = 0;\n\tregion->last_nr_accesses = 0;\n\n\treturn region;\n}\n\nvoid damon_add_region(struct damon_region *r, struct damon_target *t)\n{\n\tlist_add_tail(&r->list, &t->regions_list);\n\tt->nr_regions++;\n}\n\nstatic void damon_del_region(struct damon_region *r, struct damon_target *t)\n{\n\tlist_del(&r->list);\n\tt->nr_regions--;\n}\n\nstatic void damon_free_region(struct damon_region *r)\n{\n\tkmem_cache_free(damon_region_cache, r);\n}\n\nvoid damon_destroy_region(struct damon_region *r, struct damon_target *t)\n{\n\tdamon_del_region(r, t);\n\tdamon_free_region(r);\n}\n\n \nstatic bool damon_intersect(struct damon_region *r,\n\t\tstruct damon_addr_range *re)\n{\n\treturn !(r->ar.end <= re->start || re->end <= r->ar.start);\n}\n\n \nstatic int damon_fill_regions_holes(struct damon_region *first,\n\t\tstruct damon_region *last, struct damon_target *t)\n{\n\tstruct damon_region *r = first;\n\n\tdamon_for_each_region_from(r, t) {\n\t\tstruct damon_region *next, *newr;\n\n\t\tif (r == last)\n\t\t\tbreak;\n\t\tnext = damon_next_region(r);\n\t\tif (r->ar.end != next->ar.start) {\n\t\t\tnewr = damon_new_region(r->ar.end, next->ar.start);\n\t\t\tif (!newr)\n\t\t\t\treturn -ENOMEM;\n\t\t\tdamon_insert_region(newr, r, next, t);\n\t\t}\n\t}\n\treturn 0;\n}\n\n \nint damon_set_regions(struct damon_target *t, struct damon_addr_range *ranges,\n\t\tunsigned int nr_ranges)\n{\n\tstruct damon_region *r, *next;\n\tunsigned int i;\n\tint err;\n\n\t \n\tdamon_for_each_region_safe(r, next, t) {\n\t\tfor (i = 0; i < nr_ranges; i++) {\n\t\t\tif (damon_intersect(r, &ranges[i]))\n\t\t\t\tbreak;\n\t\t}\n\t\tif (i == nr_ranges)\n\t\t\tdamon_destroy_region(r, t);\n\t}\n\n\tr = damon_first_region(t);\n\t \n\tfor (i = 0; i < nr_ranges; i++) {\n\t\tstruct damon_region *first = NULL, *last, *newr;\n\t\tstruct damon_addr_range *range;\n\n\t\trange = &ranges[i];\n\t\t \n\t\tdamon_for_each_region_from(r, t) {\n\t\t\tif (damon_intersect(r, range)) {\n\t\t\t\tif (!first)\n\t\t\t\t\tfirst = r;\n\t\t\t\tlast = r;\n\t\t\t}\n\t\t\tif (r->ar.start >= range->end)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (!first) {\n\t\t\t \n\t\t\tnewr = damon_new_region(\n\t\t\t\t\tALIGN_DOWN(range->start,\n\t\t\t\t\t\tDAMON_MIN_REGION),\n\t\t\t\t\tALIGN(range->end, DAMON_MIN_REGION));\n\t\t\tif (!newr)\n\t\t\t\treturn -ENOMEM;\n\t\t\tdamon_insert_region(newr, damon_prev_region(r), r, t);\n\t\t} else {\n\t\t\t \n\t\t\tfirst->ar.start = ALIGN_DOWN(range->start,\n\t\t\t\t\tDAMON_MIN_REGION);\n\t\t\tlast->ar.end = ALIGN(range->end, DAMON_MIN_REGION);\n\n\t\t\t \n\t\t\terr = damon_fill_regions_holes(first, last, t);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstruct damos_filter *damos_new_filter(enum damos_filter_type type,\n\t\tbool matching)\n{\n\tstruct damos_filter *filter;\n\n\tfilter = kmalloc(sizeof(*filter), GFP_KERNEL);\n\tif (!filter)\n\t\treturn NULL;\n\tfilter->type = type;\n\tfilter->matching = matching;\n\tINIT_LIST_HEAD(&filter->list);\n\treturn filter;\n}\n\nvoid damos_add_filter(struct damos *s, struct damos_filter *f)\n{\n\tlist_add_tail(&f->list, &s->filters);\n}\n\nstatic void damos_del_filter(struct damos_filter *f)\n{\n\tlist_del(&f->list);\n}\n\nstatic void damos_free_filter(struct damos_filter *f)\n{\n\tkfree(f);\n}\n\nvoid damos_destroy_filter(struct damos_filter *f)\n{\n\tdamos_del_filter(f);\n\tdamos_free_filter(f);\n}\n\n \nstatic struct damos_quota *damos_quota_init_priv(struct damos_quota *quota)\n{\n\tquota->total_charged_sz = 0;\n\tquota->total_charged_ns = 0;\n\tquota->esz = 0;\n\tquota->charged_sz = 0;\n\tquota->charged_from = 0;\n\tquota->charge_target_from = NULL;\n\tquota->charge_addr_from = 0;\n\treturn quota;\n}\n\nstruct damos *damon_new_scheme(struct damos_access_pattern *pattern,\n\t\t\tenum damos_action action, struct damos_quota *quota,\n\t\t\tstruct damos_watermarks *wmarks)\n{\n\tstruct damos *scheme;\n\n\tscheme = kmalloc(sizeof(*scheme), GFP_KERNEL);\n\tif (!scheme)\n\t\treturn NULL;\n\tscheme->pattern = *pattern;\n\tscheme->action = action;\n\tINIT_LIST_HEAD(&scheme->filters);\n\tscheme->stat = (struct damos_stat){};\n\tINIT_LIST_HEAD(&scheme->list);\n\n\tscheme->quota = *(damos_quota_init_priv(quota));\n\n\tscheme->wmarks = *wmarks;\n\tscheme->wmarks.activated = true;\n\n\treturn scheme;\n}\n\nvoid damon_add_scheme(struct damon_ctx *ctx, struct damos *s)\n{\n\tlist_add_tail(&s->list, &ctx->schemes);\n}\n\nstatic void damon_del_scheme(struct damos *s)\n{\n\tlist_del(&s->list);\n}\n\nstatic void damon_free_scheme(struct damos *s)\n{\n\tkfree(s);\n}\n\nvoid damon_destroy_scheme(struct damos *s)\n{\n\tstruct damos_filter *f, *next;\n\n\tdamos_for_each_filter_safe(f, next, s)\n\t\tdamos_destroy_filter(f);\n\tdamon_del_scheme(s);\n\tdamon_free_scheme(s);\n}\n\n \nstruct damon_target *damon_new_target(void)\n{\n\tstruct damon_target *t;\n\n\tt = kmalloc(sizeof(*t), GFP_KERNEL);\n\tif (!t)\n\t\treturn NULL;\n\n\tt->pid = NULL;\n\tt->nr_regions = 0;\n\tINIT_LIST_HEAD(&t->regions_list);\n\tINIT_LIST_HEAD(&t->list);\n\n\treturn t;\n}\n\nvoid damon_add_target(struct damon_ctx *ctx, struct damon_target *t)\n{\n\tlist_add_tail(&t->list, &ctx->adaptive_targets);\n}\n\nbool damon_targets_empty(struct damon_ctx *ctx)\n{\n\treturn list_empty(&ctx->adaptive_targets);\n}\n\nstatic void damon_del_target(struct damon_target *t)\n{\n\tlist_del(&t->list);\n}\n\nvoid damon_free_target(struct damon_target *t)\n{\n\tstruct damon_region *r, *next;\n\n\tdamon_for_each_region_safe(r, next, t)\n\t\tdamon_free_region(r);\n\tkfree(t);\n}\n\nvoid damon_destroy_target(struct damon_target *t)\n{\n\tdamon_del_target(t);\n\tdamon_free_target(t);\n}\n\nunsigned int damon_nr_regions(struct damon_target *t)\n{\n\treturn t->nr_regions;\n}\n\nstruct damon_ctx *damon_new_ctx(void)\n{\n\tstruct damon_ctx *ctx;\n\n\tctx = kzalloc(sizeof(*ctx), GFP_KERNEL);\n\tif (!ctx)\n\t\treturn NULL;\n\n\tinit_completion(&ctx->kdamond_started);\n\n\tctx->attrs.sample_interval = 5 * 1000;\n\tctx->attrs.aggr_interval = 100 * 1000;\n\tctx->attrs.ops_update_interval = 60 * 1000 * 1000;\n\n\tctx->passed_sample_intervals = 0;\n\t \n\tctx->next_aggregation_sis = 0;\n\tctx->next_ops_update_sis = 0;\n\n\tmutex_init(&ctx->kdamond_lock);\n\n\tctx->attrs.min_nr_regions = 10;\n\tctx->attrs.max_nr_regions = 1000;\n\n\tINIT_LIST_HEAD(&ctx->adaptive_targets);\n\tINIT_LIST_HEAD(&ctx->schemes);\n\n\treturn ctx;\n}\n\nstatic void damon_destroy_targets(struct damon_ctx *ctx)\n{\n\tstruct damon_target *t, *next_t;\n\n\tif (ctx->ops.cleanup) {\n\t\tctx->ops.cleanup(ctx);\n\t\treturn;\n\t}\n\n\tdamon_for_each_target_safe(t, next_t, ctx)\n\t\tdamon_destroy_target(t);\n}\n\nvoid damon_destroy_ctx(struct damon_ctx *ctx)\n{\n\tstruct damos *s, *next_s;\n\n\tdamon_destroy_targets(ctx);\n\n\tdamon_for_each_scheme_safe(s, next_s, ctx)\n\t\tdamon_destroy_scheme(s);\n\n\tkfree(ctx);\n}\n\nstatic unsigned int damon_age_for_new_attrs(unsigned int age,\n\t\tstruct damon_attrs *old_attrs, struct damon_attrs *new_attrs)\n{\n\treturn age * old_attrs->aggr_interval / new_attrs->aggr_interval;\n}\n\n \nstatic unsigned int damon_accesses_bp_to_nr_accesses(\n\t\tunsigned int accesses_bp, struct damon_attrs *attrs)\n{\n\treturn accesses_bp * damon_max_nr_accesses(attrs) / 10000;\n}\n\n \nstatic unsigned int damon_nr_accesses_to_accesses_bp(\n\t\tunsigned int nr_accesses, struct damon_attrs *attrs)\n{\n\treturn nr_accesses * 10000 / damon_max_nr_accesses(attrs);\n}\n\nstatic unsigned int damon_nr_accesses_for_new_attrs(unsigned int nr_accesses,\n\t\tstruct damon_attrs *old_attrs, struct damon_attrs *new_attrs)\n{\n\treturn damon_accesses_bp_to_nr_accesses(\n\t\t\tdamon_nr_accesses_to_accesses_bp(\n\t\t\t\tnr_accesses, old_attrs),\n\t\t\tnew_attrs);\n}\n\nstatic void damon_update_monitoring_result(struct damon_region *r,\n\t\tstruct damon_attrs *old_attrs, struct damon_attrs *new_attrs)\n{\n\tr->nr_accesses = damon_nr_accesses_for_new_attrs(r->nr_accesses,\n\t\t\told_attrs, new_attrs);\n\tr->age = damon_age_for_new_attrs(r->age, old_attrs, new_attrs);\n}\n\n \nstatic void damon_update_monitoring_results(struct damon_ctx *ctx,\n\t\tstruct damon_attrs *new_attrs)\n{\n\tstruct damon_attrs *old_attrs = &ctx->attrs;\n\tstruct damon_target *t;\n\tstruct damon_region *r;\n\n\t \n\tif (!old_attrs->sample_interval || !old_attrs->aggr_interval ||\n\t\t\t!new_attrs->sample_interval ||\n\t\t\t!new_attrs->aggr_interval)\n\t\treturn;\n\n\tdamon_for_each_target(t, ctx)\n\t\tdamon_for_each_region(r, t)\n\t\t\tdamon_update_monitoring_result(\n\t\t\t\t\tr, old_attrs, new_attrs);\n}\n\n \nint damon_set_attrs(struct damon_ctx *ctx, struct damon_attrs *attrs)\n{\n\tunsigned long sample_interval = attrs->sample_interval ?\n\t\tattrs->sample_interval : 1;\n\n\tif (attrs->min_nr_regions < 3)\n\t\treturn -EINVAL;\n\tif (attrs->min_nr_regions > attrs->max_nr_regions)\n\t\treturn -EINVAL;\n\tif (attrs->sample_interval > attrs->aggr_interval)\n\t\treturn -EINVAL;\n\n\tctx->next_aggregation_sis = ctx->passed_sample_intervals +\n\t\tattrs->aggr_interval / sample_interval;\n\tctx->next_ops_update_sis = ctx->passed_sample_intervals +\n\t\tattrs->ops_update_interval / sample_interval;\n\n\tdamon_update_monitoring_results(ctx, attrs);\n\tctx->attrs = *attrs;\n\treturn 0;\n}\n\n \nvoid damon_set_schemes(struct damon_ctx *ctx, struct damos **schemes,\n\t\t\tssize_t nr_schemes)\n{\n\tstruct damos *s, *next;\n\tssize_t i;\n\n\tdamon_for_each_scheme_safe(s, next, ctx)\n\t\tdamon_destroy_scheme(s);\n\tfor (i = 0; i < nr_schemes; i++)\n\t\tdamon_add_scheme(ctx, schemes[i]);\n}\n\n \nint damon_nr_running_ctxs(void)\n{\n\tint nr_ctxs;\n\n\tmutex_lock(&damon_lock);\n\tnr_ctxs = nr_running_ctxs;\n\tmutex_unlock(&damon_lock);\n\n\treturn nr_ctxs;\n}\n\n \nstatic unsigned long damon_region_sz_limit(struct damon_ctx *ctx)\n{\n\tstruct damon_target *t;\n\tstruct damon_region *r;\n\tunsigned long sz = 0;\n\n\tdamon_for_each_target(t, ctx) {\n\t\tdamon_for_each_region(r, t)\n\t\t\tsz += damon_sz_region(r);\n\t}\n\n\tif (ctx->attrs.min_nr_regions)\n\t\tsz /= ctx->attrs.min_nr_regions;\n\tif (sz < DAMON_MIN_REGION)\n\t\tsz = DAMON_MIN_REGION;\n\n\treturn sz;\n}\n\nstatic int kdamond_fn(void *data);\n\n \nstatic int __damon_start(struct damon_ctx *ctx)\n{\n\tint err = -EBUSY;\n\n\tmutex_lock(&ctx->kdamond_lock);\n\tif (!ctx->kdamond) {\n\t\terr = 0;\n\t\treinit_completion(&ctx->kdamond_started);\n\t\tctx->kdamond = kthread_run(kdamond_fn, ctx, \"kdamond.%d\",\n\t\t\t\tnr_running_ctxs);\n\t\tif (IS_ERR(ctx->kdamond)) {\n\t\t\terr = PTR_ERR(ctx->kdamond);\n\t\t\tctx->kdamond = NULL;\n\t\t} else {\n\t\t\twait_for_completion(&ctx->kdamond_started);\n\t\t}\n\t}\n\tmutex_unlock(&ctx->kdamond_lock);\n\n\treturn err;\n}\n\n \nint damon_start(struct damon_ctx **ctxs, int nr_ctxs, bool exclusive)\n{\n\tint i;\n\tint err = 0;\n\n\tmutex_lock(&damon_lock);\n\tif ((exclusive && nr_running_ctxs) ||\n\t\t\t(!exclusive && running_exclusive_ctxs)) {\n\t\tmutex_unlock(&damon_lock);\n\t\treturn -EBUSY;\n\t}\n\n\tfor (i = 0; i < nr_ctxs; i++) {\n\t\terr = __damon_start(ctxs[i]);\n\t\tif (err)\n\t\t\tbreak;\n\t\tnr_running_ctxs++;\n\t}\n\tif (exclusive && nr_running_ctxs)\n\t\trunning_exclusive_ctxs = true;\n\tmutex_unlock(&damon_lock);\n\n\treturn err;\n}\n\n \nstatic int __damon_stop(struct damon_ctx *ctx)\n{\n\tstruct task_struct *tsk;\n\n\tmutex_lock(&ctx->kdamond_lock);\n\ttsk = ctx->kdamond;\n\tif (tsk) {\n\t\tget_task_struct(tsk);\n\t\tmutex_unlock(&ctx->kdamond_lock);\n\t\tkthread_stop(tsk);\n\t\tput_task_struct(tsk);\n\t\treturn 0;\n\t}\n\tmutex_unlock(&ctx->kdamond_lock);\n\n\treturn -EPERM;\n}\n\n \nint damon_stop(struct damon_ctx **ctxs, int nr_ctxs)\n{\n\tint i, err = 0;\n\n\tfor (i = 0; i < nr_ctxs; i++) {\n\t\t \n\t\terr = __damon_stop(ctxs[i]);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\treturn err;\n}\n\n \nstatic void kdamond_reset_aggregated(struct damon_ctx *c)\n{\n\tstruct damon_target *t;\n\tunsigned int ti = 0;\t \n\n\tdamon_for_each_target(t, c) {\n\t\tstruct damon_region *r;\n\n\t\tdamon_for_each_region(r, t) {\n\t\t\ttrace_damon_aggregated(t, ti, r, damon_nr_regions(t));\n\t\t\tr->last_nr_accesses = r->nr_accesses;\n\t\t\tr->nr_accesses = 0;\n\t\t}\n\t\tti++;\n\t}\n}\n\nstatic void damon_split_region_at(struct damon_target *t,\n\t\t\t\t  struct damon_region *r, unsigned long sz_r);\n\nstatic bool __damos_valid_target(struct damon_region *r, struct damos *s)\n{\n\tunsigned long sz;\n\n\tsz = damon_sz_region(r);\n\treturn s->pattern.min_sz_region <= sz &&\n\t\tsz <= s->pattern.max_sz_region &&\n\t\ts->pattern.min_nr_accesses <= r->nr_accesses &&\n\t\tr->nr_accesses <= s->pattern.max_nr_accesses &&\n\t\ts->pattern.min_age_region <= r->age &&\n\t\tr->age <= s->pattern.max_age_region;\n}\n\nstatic bool damos_valid_target(struct damon_ctx *c, struct damon_target *t,\n\t\tstruct damon_region *r, struct damos *s)\n{\n\tbool ret = __damos_valid_target(r, s);\n\n\tif (!ret || !s->quota.esz || !c->ops.get_scheme_score)\n\t\treturn ret;\n\n\treturn c->ops.get_scheme_score(c, t, r, s) >= s->quota.min_score;\n}\n\n \nstatic bool damos_skip_charged_region(struct damon_target *t,\n\t\tstruct damon_region **rp, struct damos *s)\n{\n\tstruct damon_region *r = *rp;\n\tstruct damos_quota *quota = &s->quota;\n\tunsigned long sz_to_skip;\n\n\t \n\tif (quota->charge_target_from) {\n\t\tif (t != quota->charge_target_from)\n\t\t\treturn true;\n\t\tif (r == damon_last_region(t)) {\n\t\t\tquota->charge_target_from = NULL;\n\t\t\tquota->charge_addr_from = 0;\n\t\t\treturn true;\n\t\t}\n\t\tif (quota->charge_addr_from &&\n\t\t\t\tr->ar.end <= quota->charge_addr_from)\n\t\t\treturn true;\n\n\t\tif (quota->charge_addr_from && r->ar.start <\n\t\t\t\tquota->charge_addr_from) {\n\t\t\tsz_to_skip = ALIGN_DOWN(quota->charge_addr_from -\n\t\t\t\t\tr->ar.start, DAMON_MIN_REGION);\n\t\t\tif (!sz_to_skip) {\n\t\t\t\tif (damon_sz_region(r) <= DAMON_MIN_REGION)\n\t\t\t\t\treturn true;\n\t\t\t\tsz_to_skip = DAMON_MIN_REGION;\n\t\t\t}\n\t\t\tdamon_split_region_at(t, r, sz_to_skip);\n\t\t\tr = damon_next_region(r);\n\t\t\t*rp = r;\n\t\t}\n\t\tquota->charge_target_from = NULL;\n\t\tquota->charge_addr_from = 0;\n\t}\n\treturn false;\n}\n\nstatic void damos_update_stat(struct damos *s,\n\t\tunsigned long sz_tried, unsigned long sz_applied)\n{\n\ts->stat.nr_tried++;\n\ts->stat.sz_tried += sz_tried;\n\tif (sz_applied)\n\t\ts->stat.nr_applied++;\n\ts->stat.sz_applied += sz_applied;\n}\n\nstatic bool __damos_filter_out(struct damon_ctx *ctx, struct damon_target *t,\n\t\tstruct damon_region *r, struct damos_filter *filter)\n{\n\tbool matched = false;\n\tstruct damon_target *ti;\n\tint target_idx = 0;\n\tunsigned long start, end;\n\n\tswitch (filter->type) {\n\tcase DAMOS_FILTER_TYPE_TARGET:\n\t\tdamon_for_each_target(ti, ctx) {\n\t\t\tif (ti == t)\n\t\t\t\tbreak;\n\t\t\ttarget_idx++;\n\t\t}\n\t\tmatched = target_idx == filter->target_idx;\n\t\tbreak;\n\tcase DAMOS_FILTER_TYPE_ADDR:\n\t\tstart = ALIGN_DOWN(filter->addr_range.start, DAMON_MIN_REGION);\n\t\tend = ALIGN_DOWN(filter->addr_range.end, DAMON_MIN_REGION);\n\n\t\t \n\t\tif (start <= r->ar.start && r->ar.end <= end) {\n\t\t\tmatched = true;\n\t\t\tbreak;\n\t\t}\n\t\t \n\t\tif (r->ar.end <= start || end <= r->ar.start) {\n\t\t\tmatched = false;\n\t\t\tbreak;\n\t\t}\n\t\t \n\t\tif (r->ar.start < start) {\n\t\t\tdamon_split_region_at(t, r, start - r->ar.start);\n\t\t\tmatched = false;\n\t\t\tbreak;\n\t\t}\n\t\t \n\t\tdamon_split_region_at(t, r, end - r->ar.start);\n\t\tmatched = true;\n\t\tbreak;\n\tdefault:\n\t\treturn false;\n\t}\n\n\treturn matched == filter->matching;\n}\n\nstatic bool damos_filter_out(struct damon_ctx *ctx, struct damon_target *t,\n\t\tstruct damon_region *r, struct damos *s)\n{\n\tstruct damos_filter *filter;\n\n\tdamos_for_each_filter(filter, s) {\n\t\tif (__damos_filter_out(ctx, t, r, filter))\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic void damos_apply_scheme(struct damon_ctx *c, struct damon_target *t,\n\t\tstruct damon_region *r, struct damos *s)\n{\n\tstruct damos_quota *quota = &s->quota;\n\tunsigned long sz = damon_sz_region(r);\n\tstruct timespec64 begin, end;\n\tunsigned long sz_applied = 0;\n\tint err = 0;\n\n\tif (c->ops.apply_scheme) {\n\t\tif (quota->esz && quota->charged_sz + sz > quota->esz) {\n\t\t\tsz = ALIGN_DOWN(quota->esz - quota->charged_sz,\n\t\t\t\t\tDAMON_MIN_REGION);\n\t\t\tif (!sz)\n\t\t\t\tgoto update_stat;\n\t\t\tdamon_split_region_at(t, r, sz);\n\t\t}\n\t\tif (damos_filter_out(c, t, r, s))\n\t\t\treturn;\n\t\tktime_get_coarse_ts64(&begin);\n\t\tif (c->callback.before_damos_apply)\n\t\t\terr = c->callback.before_damos_apply(c, t, r, s);\n\t\tif (!err)\n\t\t\tsz_applied = c->ops.apply_scheme(c, t, r, s);\n\t\tktime_get_coarse_ts64(&end);\n\t\tquota->total_charged_ns += timespec64_to_ns(&end) -\n\t\t\ttimespec64_to_ns(&begin);\n\t\tquota->charged_sz += sz;\n\t\tif (quota->esz && quota->charged_sz >= quota->esz) {\n\t\t\tquota->charge_target_from = t;\n\t\t\tquota->charge_addr_from = r->ar.end + 1;\n\t\t}\n\t}\n\tif (s->action != DAMOS_STAT)\n\t\tr->age = 0;\n\nupdate_stat:\n\tdamos_update_stat(s, sz, sz_applied);\n}\n\nstatic void damon_do_apply_schemes(struct damon_ctx *c,\n\t\t\t\t   struct damon_target *t,\n\t\t\t\t   struct damon_region *r)\n{\n\tstruct damos *s;\n\n\tdamon_for_each_scheme(s, c) {\n\t\tstruct damos_quota *quota = &s->quota;\n\n\t\tif (!s->wmarks.activated)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (quota->esz && quota->charged_sz >= quota->esz)\n\t\t\tcontinue;\n\n\t\tif (damos_skip_charged_region(t, &r, s))\n\t\t\tcontinue;\n\n\t\tif (!damos_valid_target(c, t, r, s))\n\t\t\tcontinue;\n\n\t\tdamos_apply_scheme(c, t, r, s);\n\t}\n}\n\n \nstatic void damos_set_effective_quota(struct damos_quota *quota)\n{\n\tunsigned long throughput;\n\tunsigned long esz;\n\n\tif (!quota->ms) {\n\t\tquota->esz = quota->sz;\n\t\treturn;\n\t}\n\n\tif (quota->total_charged_ns)\n\t\tthroughput = quota->total_charged_sz * 1000000 /\n\t\t\tquota->total_charged_ns;\n\telse\n\t\tthroughput = PAGE_SIZE * 1024;\n\tesz = throughput * quota->ms;\n\n\tif (quota->sz && quota->sz < esz)\n\t\tesz = quota->sz;\n\tquota->esz = esz;\n}\n\nstatic void damos_adjust_quota(struct damon_ctx *c, struct damos *s)\n{\n\tstruct damos_quota *quota = &s->quota;\n\tstruct damon_target *t;\n\tstruct damon_region *r;\n\tunsigned long cumulated_sz;\n\tunsigned int score, max_score = 0;\n\n\tif (!quota->ms && !quota->sz)\n\t\treturn;\n\n\t \n\tif (time_after_eq(jiffies, quota->charged_from +\n\t\t\t\tmsecs_to_jiffies(quota->reset_interval))) {\n\t\tif (quota->esz && quota->charged_sz >= quota->esz)\n\t\t\ts->stat.qt_exceeds++;\n\t\tquota->total_charged_sz += quota->charged_sz;\n\t\tquota->charged_from = jiffies;\n\t\tquota->charged_sz = 0;\n\t\tdamos_set_effective_quota(quota);\n\t}\n\n\tif (!c->ops.get_scheme_score)\n\t\treturn;\n\n\t \n\tmemset(quota->histogram, 0, sizeof(quota->histogram));\n\tdamon_for_each_target(t, c) {\n\t\tdamon_for_each_region(r, t) {\n\t\t\tif (!__damos_valid_target(r, s))\n\t\t\t\tcontinue;\n\t\t\tscore = c->ops.get_scheme_score(c, t, r, s);\n\t\t\tquota->histogram[score] += damon_sz_region(r);\n\t\t\tif (score > max_score)\n\t\t\t\tmax_score = score;\n\t\t}\n\t}\n\n\t \n\tfor (cumulated_sz = 0, score = max_score; ; score--) {\n\t\tcumulated_sz += quota->histogram[score];\n\t\tif (cumulated_sz >= quota->esz || !score)\n\t\t\tbreak;\n\t}\n\tquota->min_score = score;\n}\n\nstatic void kdamond_apply_schemes(struct damon_ctx *c)\n{\n\tstruct damon_target *t;\n\tstruct damon_region *r, *next_r;\n\tstruct damos *s;\n\n\tdamon_for_each_scheme(s, c) {\n\t\tif (!s->wmarks.activated)\n\t\t\tcontinue;\n\n\t\tdamos_adjust_quota(c, s);\n\t}\n\n\tdamon_for_each_target(t, c) {\n\t\tdamon_for_each_region_safe(r, next_r, t)\n\t\t\tdamon_do_apply_schemes(c, t, r);\n\t}\n}\n\n \nstatic void damon_merge_two_regions(struct damon_target *t,\n\t\tstruct damon_region *l, struct damon_region *r)\n{\n\tunsigned long sz_l = damon_sz_region(l), sz_r = damon_sz_region(r);\n\n\tl->nr_accesses = (l->nr_accesses * sz_l + r->nr_accesses * sz_r) /\n\t\t\t(sz_l + sz_r);\n\tl->age = (l->age * sz_l + r->age * sz_r) / (sz_l + sz_r);\n\tl->ar.end = r->ar.end;\n\tdamon_destroy_region(r, t);\n}\n\n \nstatic void damon_merge_regions_of(struct damon_target *t, unsigned int thres,\n\t\t\t\t   unsigned long sz_limit)\n{\n\tstruct damon_region *r, *prev = NULL, *next;\n\n\tdamon_for_each_region_safe(r, next, t) {\n\t\tif (abs(r->nr_accesses - r->last_nr_accesses) > thres)\n\t\t\tr->age = 0;\n\t\telse\n\t\t\tr->age++;\n\n\t\tif (prev && prev->ar.end == r->ar.start &&\n\t\t    abs(prev->nr_accesses - r->nr_accesses) <= thres &&\n\t\t    damon_sz_region(prev) + damon_sz_region(r) <= sz_limit)\n\t\t\tdamon_merge_two_regions(t, prev, r);\n\t\telse\n\t\t\tprev = r;\n\t}\n}\n\n \nstatic void kdamond_merge_regions(struct damon_ctx *c, unsigned int threshold,\n\t\t\t\t  unsigned long sz_limit)\n{\n\tstruct damon_target *t;\n\n\tdamon_for_each_target(t, c)\n\t\tdamon_merge_regions_of(t, threshold, sz_limit);\n}\n\n \nstatic void damon_split_region_at(struct damon_target *t,\n\t\t\t\t  struct damon_region *r, unsigned long sz_r)\n{\n\tstruct damon_region *new;\n\n\tnew = damon_new_region(r->ar.start + sz_r, r->ar.end);\n\tif (!new)\n\t\treturn;\n\n\tr->ar.end = new->ar.start;\n\n\tnew->age = r->age;\n\tnew->last_nr_accesses = r->last_nr_accesses;\n\n\tdamon_insert_region(new, r, damon_next_region(r), t);\n}\n\n \nstatic void damon_split_regions_of(struct damon_target *t, int nr_subs)\n{\n\tstruct damon_region *r, *next;\n\tunsigned long sz_region, sz_sub = 0;\n\tint i;\n\n\tdamon_for_each_region_safe(r, next, t) {\n\t\tsz_region = damon_sz_region(r);\n\n\t\tfor (i = 0; i < nr_subs - 1 &&\n\t\t\t\tsz_region > 2 * DAMON_MIN_REGION; i++) {\n\t\t\t \n\t\t\tsz_sub = ALIGN_DOWN(damon_rand(1, 10) *\n\t\t\t\t\tsz_region / 10, DAMON_MIN_REGION);\n\t\t\t \n\t\t\tif (sz_sub == 0 || sz_sub >= sz_region)\n\t\t\t\tcontinue;\n\n\t\t\tdamon_split_region_at(t, r, sz_sub);\n\t\t\tsz_region = sz_sub;\n\t\t}\n\t}\n}\n\n \nstatic void kdamond_split_regions(struct damon_ctx *ctx)\n{\n\tstruct damon_target *t;\n\tunsigned int nr_regions = 0;\n\tstatic unsigned int last_nr_regions;\n\tint nr_subregions = 2;\n\n\tdamon_for_each_target(t, ctx)\n\t\tnr_regions += damon_nr_regions(t);\n\n\tif (nr_regions > ctx->attrs.max_nr_regions / 2)\n\t\treturn;\n\n\t \n\tif (last_nr_regions == nr_regions &&\n\t\t\tnr_regions < ctx->attrs.max_nr_regions / 3)\n\t\tnr_subregions = 3;\n\n\tdamon_for_each_target(t, ctx)\n\t\tdamon_split_regions_of(t, nr_subregions);\n\n\tlast_nr_regions = nr_regions;\n}\n\n \nstatic bool kdamond_need_stop(struct damon_ctx *ctx)\n{\n\tstruct damon_target *t;\n\n\tif (kthread_should_stop())\n\t\treturn true;\n\n\tif (!ctx->ops.target_valid)\n\t\treturn false;\n\n\tdamon_for_each_target(t, ctx) {\n\t\tif (ctx->ops.target_valid(t))\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic unsigned long damos_wmark_metric_value(enum damos_wmark_metric metric)\n{\n\tstruct sysinfo i;\n\n\tswitch (metric) {\n\tcase DAMOS_WMARK_FREE_MEM_RATE:\n\t\tsi_meminfo(&i);\n\t\treturn i.freeram * 1000 / i.totalram;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn -EINVAL;\n}\n\n \nstatic unsigned long damos_wmark_wait_us(struct damos *scheme)\n{\n\tunsigned long metric;\n\n\tif (scheme->wmarks.metric == DAMOS_WMARK_NONE)\n\t\treturn 0;\n\n\tmetric = damos_wmark_metric_value(scheme->wmarks.metric);\n\t \n\tif (metric > scheme->wmarks.high || scheme->wmarks.low > metric) {\n\t\tif (scheme->wmarks.activated)\n\t\t\tpr_debug(\"deactivate a scheme (%d) for %s wmark\\n\",\n\t\t\t\t\tscheme->action,\n\t\t\t\t\tmetric > scheme->wmarks.high ?\n\t\t\t\t\t\"high\" : \"low\");\n\t\tscheme->wmarks.activated = false;\n\t\treturn scheme->wmarks.interval;\n\t}\n\n\t \n\tif ((scheme->wmarks.high >= metric && metric >= scheme->wmarks.mid) &&\n\t\t\t!scheme->wmarks.activated)\n\t\treturn scheme->wmarks.interval;\n\n\tif (!scheme->wmarks.activated)\n\t\tpr_debug(\"activate a scheme (%d)\\n\", scheme->action);\n\tscheme->wmarks.activated = true;\n\treturn 0;\n}\n\nstatic void kdamond_usleep(unsigned long usecs)\n{\n\t \n\tif (usecs > 20 * USEC_PER_MSEC)\n\t\tschedule_timeout_idle(usecs_to_jiffies(usecs));\n\telse\n\t\tusleep_idle_range(usecs, usecs + 1);\n}\n\n \nstatic int kdamond_wait_activation(struct damon_ctx *ctx)\n{\n\tstruct damos *s;\n\tunsigned long wait_time;\n\tunsigned long min_wait_time = 0;\n\tbool init_wait_time = false;\n\n\twhile (!kdamond_need_stop(ctx)) {\n\t\tdamon_for_each_scheme(s, ctx) {\n\t\t\twait_time = damos_wmark_wait_us(s);\n\t\t\tif (!init_wait_time || wait_time < min_wait_time) {\n\t\t\t\tinit_wait_time = true;\n\t\t\t\tmin_wait_time = wait_time;\n\t\t\t}\n\t\t}\n\t\tif (!min_wait_time)\n\t\t\treturn 0;\n\n\t\tkdamond_usleep(min_wait_time);\n\n\t\tif (ctx->callback.after_wmarks_check &&\n\t\t\t\tctx->callback.after_wmarks_check(ctx))\n\t\t\tbreak;\n\t}\n\treturn -EBUSY;\n}\n\nstatic void kdamond_init_intervals_sis(struct damon_ctx *ctx)\n{\n\tunsigned long sample_interval = ctx->attrs.sample_interval ?\n\t\tctx->attrs.sample_interval : 1;\n\n\tctx->passed_sample_intervals = 0;\n\tctx->next_aggregation_sis = ctx->attrs.aggr_interval / sample_interval;\n\tctx->next_ops_update_sis = ctx->attrs.ops_update_interval /\n\t\tsample_interval;\n}\n\n \nstatic int kdamond_fn(void *data)\n{\n\tstruct damon_ctx *ctx = data;\n\tstruct damon_target *t;\n\tstruct damon_region *r, *next;\n\tunsigned int max_nr_accesses = 0;\n\tunsigned long sz_limit = 0;\n\n\tpr_debug(\"kdamond (%d) starts\\n\", current->pid);\n\n\tcomplete(&ctx->kdamond_started);\n\tkdamond_init_intervals_sis(ctx);\n\n\tif (ctx->ops.init)\n\t\tctx->ops.init(ctx);\n\tif (ctx->callback.before_start && ctx->callback.before_start(ctx))\n\t\tgoto done;\n\n\tsz_limit = damon_region_sz_limit(ctx);\n\n\twhile (!kdamond_need_stop(ctx)) {\n\t\t \n\t\tunsigned long next_aggregation_sis = ctx->next_aggregation_sis;\n\t\tunsigned long next_ops_update_sis = ctx->next_ops_update_sis;\n\t\tunsigned long sample_interval = ctx->attrs.sample_interval;\n\n\t\tif (kdamond_wait_activation(ctx))\n\t\t\tbreak;\n\n\t\tif (ctx->ops.prepare_access_checks)\n\t\t\tctx->ops.prepare_access_checks(ctx);\n\t\tif (ctx->callback.after_sampling &&\n\t\t\t\tctx->callback.after_sampling(ctx))\n\t\t\tbreak;\n\n\t\tkdamond_usleep(sample_interval);\n\t\tctx->passed_sample_intervals++;\n\n\t\tif (ctx->ops.check_accesses)\n\t\t\tmax_nr_accesses = ctx->ops.check_accesses(ctx);\n\n\t\tsample_interval = ctx->attrs.sample_interval ?\n\t\t\tctx->attrs.sample_interval : 1;\n\t\tif (ctx->passed_sample_intervals == next_aggregation_sis) {\n\t\t\tctx->next_aggregation_sis = next_aggregation_sis +\n\t\t\t\tctx->attrs.aggr_interval / sample_interval;\n\t\t\tkdamond_merge_regions(ctx,\n\t\t\t\t\tmax_nr_accesses / 10,\n\t\t\t\t\tsz_limit);\n\t\t\tif (ctx->callback.after_aggregation &&\n\t\t\t\t\tctx->callback.after_aggregation(ctx))\n\t\t\t\tbreak;\n\t\t\tif (!list_empty(&ctx->schemes))\n\t\t\t\tkdamond_apply_schemes(ctx);\n\t\t\tkdamond_reset_aggregated(ctx);\n\t\t\tkdamond_split_regions(ctx);\n\t\t\tif (ctx->ops.reset_aggregated)\n\t\t\t\tctx->ops.reset_aggregated(ctx);\n\t\t}\n\n\t\tif (ctx->passed_sample_intervals == next_ops_update_sis) {\n\t\t\tctx->next_ops_update_sis = next_ops_update_sis +\n\t\t\t\tctx->attrs.ops_update_interval /\n\t\t\t\tsample_interval;\n\t\t\tif (ctx->ops.update)\n\t\t\t\tctx->ops.update(ctx);\n\t\t\tsz_limit = damon_region_sz_limit(ctx);\n\t\t}\n\t}\ndone:\n\tdamon_for_each_target(t, ctx) {\n\t\tdamon_for_each_region_safe(r, next, t)\n\t\t\tdamon_destroy_region(r, t);\n\t}\n\n\tif (ctx->callback.before_terminate)\n\t\tctx->callback.before_terminate(ctx);\n\tif (ctx->ops.cleanup)\n\t\tctx->ops.cleanup(ctx);\n\n\tpr_debug(\"kdamond (%d) finishes\\n\", current->pid);\n\tmutex_lock(&ctx->kdamond_lock);\n\tctx->kdamond = NULL;\n\tmutex_unlock(&ctx->kdamond_lock);\n\n\tmutex_lock(&damon_lock);\n\tnr_running_ctxs--;\n\tif (!nr_running_ctxs && running_exclusive_ctxs)\n\t\trunning_exclusive_ctxs = false;\n\tmutex_unlock(&damon_lock);\n\n\treturn 0;\n}\n\n \nstruct damon_system_ram_region {\n\tunsigned long start;\n\tunsigned long end;\n};\n\nstatic int walk_system_ram(struct resource *res, void *arg)\n{\n\tstruct damon_system_ram_region *a = arg;\n\n\tif (a->end - a->start < resource_size(res)) {\n\t\ta->start = res->start;\n\t\ta->end = res->end;\n\t}\n\treturn 0;\n}\n\n \nstatic bool damon_find_biggest_system_ram(unsigned long *start,\n\t\t\t\t\t\tunsigned long *end)\n\n{\n\tstruct damon_system_ram_region arg = {};\n\n\twalk_system_ram_res(0, ULONG_MAX, &arg, walk_system_ram);\n\tif (arg.end <= arg.start)\n\t\treturn false;\n\n\t*start = arg.start;\n\t*end = arg.end;\n\treturn true;\n}\n\n \nint damon_set_region_biggest_system_ram_default(struct damon_target *t,\n\t\t\tunsigned long *start, unsigned long *end)\n{\n\tstruct damon_addr_range addr_range;\n\n\tif (*start > *end)\n\t\treturn -EINVAL;\n\n\tif (!*start && !*end &&\n\t\t!damon_find_biggest_system_ram(start, end))\n\t\treturn -EINVAL;\n\n\taddr_range.start = *start;\n\taddr_range.end = *end;\n\treturn damon_set_regions(t, &addr_range, 1);\n}\n\nstatic int __init damon_init(void)\n{\n\tdamon_region_cache = KMEM_CACHE(damon_region, 0);\n\tif (unlikely(!damon_region_cache)) {\n\t\tpr_err(\"creating damon_region_cache fails\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nsubsys_initcall(damon_init);\n\n#include \"core-test.h\"\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}