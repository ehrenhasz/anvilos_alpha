{
  "module_name": "migrate.c",
  "hash_id": "dbc49025d2f36e74e606dec4785f8a57c3ee7ac0f9c2d8e48010c7ea8501da03",
  "original_prompt": "Ingested from linux-6.6.14/mm/migrate.c",
  "human_readable_source": "\n \n\n#include <linux/migrate.h>\n#include <linux/export.h>\n#include <linux/swap.h>\n#include <linux/swapops.h>\n#include <linux/pagemap.h>\n#include <linux/buffer_head.h>\n#include <linux/mm_inline.h>\n#include <linux/nsproxy.h>\n#include <linux/ksm.h>\n#include <linux/rmap.h>\n#include <linux/topology.h>\n#include <linux/cpu.h>\n#include <linux/cpuset.h>\n#include <linux/writeback.h>\n#include <linux/mempolicy.h>\n#include <linux/vmalloc.h>\n#include <linux/security.h>\n#include <linux/backing-dev.h>\n#include <linux/compaction.h>\n#include <linux/syscalls.h>\n#include <linux/compat.h>\n#include <linux/hugetlb.h>\n#include <linux/hugetlb_cgroup.h>\n#include <linux/gfp.h>\n#include <linux/pfn_t.h>\n#include <linux/memremap.h>\n#include <linux/userfaultfd_k.h>\n#include <linux/balloon_compaction.h>\n#include <linux/page_idle.h>\n#include <linux/page_owner.h>\n#include <linux/sched/mm.h>\n#include <linux/ptrace.h>\n#include <linux/oom.h>\n#include <linux/memory.h>\n#include <linux/random.h>\n#include <linux/sched/sysctl.h>\n#include <linux/memory-tiers.h>\n\n#include <asm/tlbflush.h>\n\n#include <trace/events/migrate.h>\n\n#include \"internal.h\"\n\nbool isolate_movable_page(struct page *page, isolate_mode_t mode)\n{\n\tstruct folio *folio = folio_get_nontail_page(page);\n\tconst struct movable_operations *mops;\n\n\t \n\tif (!folio)\n\t\tgoto out;\n\n\tif (unlikely(folio_test_slab(folio)))\n\t\tgoto out_putfolio;\n\t \n\tsmp_rmb();\n\t \n\tif (unlikely(!__folio_test_movable(folio)))\n\t\tgoto out_putfolio;\n\t \n\tsmp_rmb();\n\tif (unlikely(folio_test_slab(folio)))\n\t\tgoto out_putfolio;\n\n\t \n\tif (unlikely(!folio_trylock(folio)))\n\t\tgoto out_putfolio;\n\n\tif (!folio_test_movable(folio) || folio_test_isolated(folio))\n\t\tgoto out_no_isolated;\n\n\tmops = folio_movable_ops(folio);\n\tVM_BUG_ON_FOLIO(!mops, folio);\n\n\tif (!mops->isolate_page(&folio->page, mode))\n\t\tgoto out_no_isolated;\n\n\t \n\tWARN_ON_ONCE(folio_test_isolated(folio));\n\tfolio_set_isolated(folio);\n\tfolio_unlock(folio);\n\n\treturn true;\n\nout_no_isolated:\n\tfolio_unlock(folio);\nout_putfolio:\n\tfolio_put(folio);\nout:\n\treturn false;\n}\n\nstatic void putback_movable_folio(struct folio *folio)\n{\n\tconst struct movable_operations *mops = folio_movable_ops(folio);\n\n\tmops->putback_page(&folio->page);\n\tfolio_clear_isolated(folio);\n}\n\n \nvoid putback_movable_pages(struct list_head *l)\n{\n\tstruct folio *folio;\n\tstruct folio *folio2;\n\n\tlist_for_each_entry_safe(folio, folio2, l, lru) {\n\t\tif (unlikely(folio_test_hugetlb(folio))) {\n\t\t\tfolio_putback_active_hugetlb(folio);\n\t\t\tcontinue;\n\t\t}\n\t\tlist_del(&folio->lru);\n\t\t \n\t\tif (unlikely(__folio_test_movable(folio))) {\n\t\t\tVM_BUG_ON_FOLIO(!folio_test_isolated(folio), folio);\n\t\t\tfolio_lock(folio);\n\t\t\tif (folio_test_movable(folio))\n\t\t\t\tputback_movable_folio(folio);\n\t\t\telse\n\t\t\t\tfolio_clear_isolated(folio);\n\t\t\tfolio_unlock(folio);\n\t\t\tfolio_put(folio);\n\t\t} else {\n\t\t\tnode_stat_mod_folio(folio, NR_ISOLATED_ANON +\n\t\t\t\t\tfolio_is_file_lru(folio), -folio_nr_pages(folio));\n\t\t\tfolio_putback_lru(folio);\n\t\t}\n\t}\n}\n\n \nstatic bool remove_migration_pte(struct folio *folio,\n\t\tstruct vm_area_struct *vma, unsigned long addr, void *old)\n{\n\tDEFINE_FOLIO_VMA_WALK(pvmw, old, vma, addr, PVMW_SYNC | PVMW_MIGRATION);\n\n\twhile (page_vma_mapped_walk(&pvmw)) {\n\t\trmap_t rmap_flags = RMAP_NONE;\n\t\tpte_t old_pte;\n\t\tpte_t pte;\n\t\tswp_entry_t entry;\n\t\tstruct page *new;\n\t\tunsigned long idx = 0;\n\n\t\t \n\t\tif (folio_test_large(folio) && !folio_test_hugetlb(folio))\n\t\t\tidx = linear_page_index(vma, pvmw.address) - pvmw.pgoff;\n\t\tnew = folio_page(folio, idx);\n\n#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION\n\t\t \n\t\tif (!pvmw.pte) {\n\t\t\tVM_BUG_ON_FOLIO(folio_test_hugetlb(folio) ||\n\t\t\t\t\t!folio_test_pmd_mappable(folio), folio);\n\t\t\tremove_migration_pmd(&pvmw, new);\n\t\t\tcontinue;\n\t\t}\n#endif\n\n\t\tfolio_get(folio);\n\t\tpte = mk_pte(new, READ_ONCE(vma->vm_page_prot));\n\t\told_pte = ptep_get(pvmw.pte);\n\t\tif (pte_swp_soft_dirty(old_pte))\n\t\t\tpte = pte_mksoft_dirty(pte);\n\n\t\tentry = pte_to_swp_entry(old_pte);\n\t\tif (!is_migration_entry_young(entry))\n\t\t\tpte = pte_mkold(pte);\n\t\tif (folio_test_dirty(folio) && is_migration_entry_dirty(entry))\n\t\t\tpte = pte_mkdirty(pte);\n\t\tif (is_writable_migration_entry(entry))\n\t\t\tpte = pte_mkwrite(pte, vma);\n\t\telse if (pte_swp_uffd_wp(old_pte))\n\t\t\tpte = pte_mkuffd_wp(pte);\n\n\t\tif (folio_test_anon(folio) && !is_readable_migration_entry(entry))\n\t\t\trmap_flags |= RMAP_EXCLUSIVE;\n\n\t\tif (unlikely(is_device_private_page(new))) {\n\t\t\tif (pte_write(pte))\n\t\t\t\tentry = make_writable_device_private_entry(\n\t\t\t\t\t\t\tpage_to_pfn(new));\n\t\t\telse\n\t\t\t\tentry = make_readable_device_private_entry(\n\t\t\t\t\t\t\tpage_to_pfn(new));\n\t\t\tpte = swp_entry_to_pte(entry);\n\t\t\tif (pte_swp_soft_dirty(old_pte))\n\t\t\t\tpte = pte_swp_mksoft_dirty(pte);\n\t\t\tif (pte_swp_uffd_wp(old_pte))\n\t\t\t\tpte = pte_swp_mkuffd_wp(pte);\n\t\t}\n\n#ifdef CONFIG_HUGETLB_PAGE\n\t\tif (folio_test_hugetlb(folio)) {\n\t\t\tstruct hstate *h = hstate_vma(vma);\n\t\t\tunsigned int shift = huge_page_shift(h);\n\t\t\tunsigned long psize = huge_page_size(h);\n\n\t\t\tpte = arch_make_huge_pte(pte, shift, vma->vm_flags);\n\t\t\tif (folio_test_anon(folio))\n\t\t\t\thugepage_add_anon_rmap(new, vma, pvmw.address,\n\t\t\t\t\t\t       rmap_flags);\n\t\t\telse\n\t\t\t\tpage_dup_file_rmap(new, true);\n\t\t\tset_huge_pte_at(vma->vm_mm, pvmw.address, pvmw.pte, pte,\n\t\t\t\t\tpsize);\n\t\t} else\n#endif\n\t\t{\n\t\t\tif (folio_test_anon(folio))\n\t\t\t\tpage_add_anon_rmap(new, vma, pvmw.address,\n\t\t\t\t\t\t   rmap_flags);\n\t\t\telse\n\t\t\t\tpage_add_file_rmap(new, vma, false);\n\t\t\tset_pte_at(vma->vm_mm, pvmw.address, pvmw.pte, pte);\n\t\t}\n\t\tif (vma->vm_flags & VM_LOCKED)\n\t\t\tmlock_drain_local();\n\n\t\ttrace_remove_migration_pte(pvmw.address, pte_val(pte),\n\t\t\t\t\t   compound_order(new));\n\n\t\t \n\t\tupdate_mmu_cache(vma, pvmw.address, pvmw.pte);\n\t}\n\n\treturn true;\n}\n\n \nvoid remove_migration_ptes(struct folio *src, struct folio *dst, bool locked)\n{\n\tstruct rmap_walk_control rwc = {\n\t\t.rmap_one = remove_migration_pte,\n\t\t.arg = src,\n\t};\n\n\tif (locked)\n\t\trmap_walk_locked(dst, &rwc);\n\telse\n\t\trmap_walk(dst, &rwc);\n}\n\n \nvoid migration_entry_wait(struct mm_struct *mm, pmd_t *pmd,\n\t\t\t  unsigned long address)\n{\n\tspinlock_t *ptl;\n\tpte_t *ptep;\n\tpte_t pte;\n\tswp_entry_t entry;\n\n\tptep = pte_offset_map_lock(mm, pmd, address, &ptl);\n\tif (!ptep)\n\t\treturn;\n\n\tpte = ptep_get(ptep);\n\tpte_unmap(ptep);\n\n\tif (!is_swap_pte(pte))\n\t\tgoto out;\n\n\tentry = pte_to_swp_entry(pte);\n\tif (!is_migration_entry(entry))\n\t\tgoto out;\n\n\tmigration_entry_wait_on_locked(entry, ptl);\n\treturn;\nout:\n\tspin_unlock(ptl);\n}\n\n#ifdef CONFIG_HUGETLB_PAGE\n \nvoid migration_entry_wait_huge(struct vm_area_struct *vma, pte_t *ptep)\n{\n\tspinlock_t *ptl = huge_pte_lockptr(hstate_vma(vma), vma->vm_mm, ptep);\n\tpte_t pte;\n\n\thugetlb_vma_assert_locked(vma);\n\tspin_lock(ptl);\n\tpte = huge_ptep_get(ptep);\n\n\tif (unlikely(!is_hugetlb_entry_migration(pte))) {\n\t\tspin_unlock(ptl);\n\t\thugetlb_vma_unlock_read(vma);\n\t} else {\n\t\t \n\t\thugetlb_vma_unlock_read(vma);\n\t\tmigration_entry_wait_on_locked(pte_to_swp_entry(pte), ptl);\n\t}\n}\n#endif\n\n#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION\nvoid pmd_migration_entry_wait(struct mm_struct *mm, pmd_t *pmd)\n{\n\tspinlock_t *ptl;\n\n\tptl = pmd_lock(mm, pmd);\n\tif (!is_pmd_migration_entry(*pmd))\n\t\tgoto unlock;\n\tmigration_entry_wait_on_locked(pmd_to_swp_entry(*pmd), ptl);\n\treturn;\nunlock:\n\tspin_unlock(ptl);\n}\n#endif\n\nstatic int folio_expected_refs(struct address_space *mapping,\n\t\tstruct folio *folio)\n{\n\tint refs = 1;\n\tif (!mapping)\n\t\treturn refs;\n\n\trefs += folio_nr_pages(folio);\n\tif (folio_test_private(folio))\n\t\trefs++;\n\n\treturn refs;\n}\n\n \nint folio_migrate_mapping(struct address_space *mapping,\n\t\tstruct folio *newfolio, struct folio *folio, int extra_count)\n{\n\tXA_STATE(xas, &mapping->i_pages, folio_index(folio));\n\tstruct zone *oldzone, *newzone;\n\tint dirty;\n\tint expected_count = folio_expected_refs(mapping, folio) + extra_count;\n\tlong nr = folio_nr_pages(folio);\n\tlong entries, i;\n\n\tif (!mapping) {\n\t\t \n\t\tif (folio_ref_count(folio) != expected_count)\n\t\t\treturn -EAGAIN;\n\n\t\t \n\t\tnewfolio->index = folio->index;\n\t\tnewfolio->mapping = folio->mapping;\n\t\tif (folio_test_swapbacked(folio))\n\t\t\t__folio_set_swapbacked(newfolio);\n\n\t\treturn MIGRATEPAGE_SUCCESS;\n\t}\n\n\toldzone = folio_zone(folio);\n\tnewzone = folio_zone(newfolio);\n\n\txas_lock_irq(&xas);\n\tif (!folio_ref_freeze(folio, expected_count)) {\n\t\txas_unlock_irq(&xas);\n\t\treturn -EAGAIN;\n\t}\n\n\t \n\tnewfolio->index = folio->index;\n\tnewfolio->mapping = folio->mapping;\n\tfolio_ref_add(newfolio, nr);  \n\tif (folio_test_swapbacked(folio)) {\n\t\t__folio_set_swapbacked(newfolio);\n\t\tif (folio_test_swapcache(folio)) {\n\t\t\tfolio_set_swapcache(newfolio);\n\t\t\tnewfolio->private = folio_get_private(folio);\n\t\t}\n\t\tentries = nr;\n\t} else {\n\t\tVM_BUG_ON_FOLIO(folio_test_swapcache(folio), folio);\n\t\tentries = 1;\n\t}\n\n\t \n\tdirty = folio_test_dirty(folio);\n\tif (dirty) {\n\t\tfolio_clear_dirty(folio);\n\t\tfolio_set_dirty(newfolio);\n\t}\n\n\t \n\tfor (i = 0; i < entries; i++) {\n\t\txas_store(&xas, newfolio);\n\t\txas_next(&xas);\n\t}\n\n\t \n\tfolio_ref_unfreeze(folio, expected_count - nr);\n\n\txas_unlock(&xas);\n\t \n\n\t \n\tif (newzone != oldzone) {\n\t\tstruct lruvec *old_lruvec, *new_lruvec;\n\t\tstruct mem_cgroup *memcg;\n\n\t\tmemcg = folio_memcg(folio);\n\t\told_lruvec = mem_cgroup_lruvec(memcg, oldzone->zone_pgdat);\n\t\tnew_lruvec = mem_cgroup_lruvec(memcg, newzone->zone_pgdat);\n\n\t\t__mod_lruvec_state(old_lruvec, NR_FILE_PAGES, -nr);\n\t\t__mod_lruvec_state(new_lruvec, NR_FILE_PAGES, nr);\n\t\tif (folio_test_swapbacked(folio) && !folio_test_swapcache(folio)) {\n\t\t\t__mod_lruvec_state(old_lruvec, NR_SHMEM, -nr);\n\t\t\t__mod_lruvec_state(new_lruvec, NR_SHMEM, nr);\n\n\t\t\tif (folio_test_pmd_mappable(folio)) {\n\t\t\t\t__mod_lruvec_state(old_lruvec, NR_SHMEM_THPS, -nr);\n\t\t\t\t__mod_lruvec_state(new_lruvec, NR_SHMEM_THPS, nr);\n\t\t\t}\n\t\t}\n#ifdef CONFIG_SWAP\n\t\tif (folio_test_swapcache(folio)) {\n\t\t\t__mod_lruvec_state(old_lruvec, NR_SWAPCACHE, -nr);\n\t\t\t__mod_lruvec_state(new_lruvec, NR_SWAPCACHE, nr);\n\t\t}\n#endif\n\t\tif (dirty && mapping_can_writeback(mapping)) {\n\t\t\t__mod_lruvec_state(old_lruvec, NR_FILE_DIRTY, -nr);\n\t\t\t__mod_zone_page_state(oldzone, NR_ZONE_WRITE_PENDING, -nr);\n\t\t\t__mod_lruvec_state(new_lruvec, NR_FILE_DIRTY, nr);\n\t\t\t__mod_zone_page_state(newzone, NR_ZONE_WRITE_PENDING, nr);\n\t\t}\n\t}\n\tlocal_irq_enable();\n\n\treturn MIGRATEPAGE_SUCCESS;\n}\nEXPORT_SYMBOL(folio_migrate_mapping);\n\n \nint migrate_huge_page_move_mapping(struct address_space *mapping,\n\t\t\t\t   struct folio *dst, struct folio *src)\n{\n\tXA_STATE(xas, &mapping->i_pages, folio_index(src));\n\tint expected_count;\n\n\txas_lock_irq(&xas);\n\texpected_count = 2 + folio_has_private(src);\n\tif (!folio_ref_freeze(src, expected_count)) {\n\t\txas_unlock_irq(&xas);\n\t\treturn -EAGAIN;\n\t}\n\n\tdst->index = src->index;\n\tdst->mapping = src->mapping;\n\n\tfolio_get(dst);\n\n\txas_store(&xas, dst);\n\n\tfolio_ref_unfreeze(src, expected_count - 1);\n\n\txas_unlock_irq(&xas);\n\n\treturn MIGRATEPAGE_SUCCESS;\n}\n\n \nvoid folio_migrate_flags(struct folio *newfolio, struct folio *folio)\n{\n\tint cpupid;\n\n\tif (folio_test_error(folio))\n\t\tfolio_set_error(newfolio);\n\tif (folio_test_referenced(folio))\n\t\tfolio_set_referenced(newfolio);\n\tif (folio_test_uptodate(folio))\n\t\tfolio_mark_uptodate(newfolio);\n\tif (folio_test_clear_active(folio)) {\n\t\tVM_BUG_ON_FOLIO(folio_test_unevictable(folio), folio);\n\t\tfolio_set_active(newfolio);\n\t} else if (folio_test_clear_unevictable(folio))\n\t\tfolio_set_unevictable(newfolio);\n\tif (folio_test_workingset(folio))\n\t\tfolio_set_workingset(newfolio);\n\tif (folio_test_checked(folio))\n\t\tfolio_set_checked(newfolio);\n\t \n\tif (folio_test_mappedtodisk(folio))\n\t\tfolio_set_mappedtodisk(newfolio);\n\n\t \n\tif (folio_test_dirty(folio))\n\t\tfolio_set_dirty(newfolio);\n\n\tif (folio_test_young(folio))\n\t\tfolio_set_young(newfolio);\n\tif (folio_test_idle(folio))\n\t\tfolio_set_idle(newfolio);\n\n\t \n\tcpupid = page_cpupid_xchg_last(&folio->page, -1);\n\t \n\tif (sysctl_numa_balancing_mode & NUMA_BALANCING_MEMORY_TIERING) {\n\t\tbool f_toptier = node_is_toptier(page_to_nid(&folio->page));\n\t\tbool t_toptier = node_is_toptier(page_to_nid(&newfolio->page));\n\n\t\tif (f_toptier != t_toptier)\n\t\t\tcpupid = -1;\n\t}\n\tpage_cpupid_xchg_last(&newfolio->page, cpupid);\n\n\tfolio_migrate_ksm(newfolio, folio);\n\t \n\tif (folio_test_swapcache(folio))\n\t\tfolio_clear_swapcache(folio);\n\tfolio_clear_private(folio);\n\n\t \n\tif (!folio_test_hugetlb(folio))\n\t\tfolio->private = NULL;\n\n\t \n\tif (folio_test_writeback(newfolio))\n\t\tfolio_end_writeback(newfolio);\n\n\t \n\tif (folio_test_readahead(folio))\n\t\tfolio_set_readahead(newfolio);\n\n\tfolio_copy_owner(newfolio, folio);\n\n\tif (!folio_test_hugetlb(folio))\n\t\tmem_cgroup_migrate(folio, newfolio);\n}\nEXPORT_SYMBOL(folio_migrate_flags);\n\nvoid folio_migrate_copy(struct folio *newfolio, struct folio *folio)\n{\n\tfolio_copy(newfolio, folio);\n\tfolio_migrate_flags(newfolio, folio);\n}\nEXPORT_SYMBOL(folio_migrate_copy);\n\n \n\nint migrate_folio_extra(struct address_space *mapping, struct folio *dst,\n\t\tstruct folio *src, enum migrate_mode mode, int extra_count)\n{\n\tint rc;\n\n\tBUG_ON(folio_test_writeback(src));\t \n\n\trc = folio_migrate_mapping(mapping, dst, src, extra_count);\n\n\tif (rc != MIGRATEPAGE_SUCCESS)\n\t\treturn rc;\n\n\tif (mode != MIGRATE_SYNC_NO_COPY)\n\t\tfolio_migrate_copy(dst, src);\n\telse\n\t\tfolio_migrate_flags(dst, src);\n\treturn MIGRATEPAGE_SUCCESS;\n}\n\n \nint migrate_folio(struct address_space *mapping, struct folio *dst,\n\t\tstruct folio *src, enum migrate_mode mode)\n{\n\treturn migrate_folio_extra(mapping, dst, src, mode, 0);\n}\nEXPORT_SYMBOL(migrate_folio);\n\n#ifdef CONFIG_BUFFER_HEAD\n \nstatic bool buffer_migrate_lock_buffers(struct buffer_head *head,\n\t\t\t\t\t\t\tenum migrate_mode mode)\n{\n\tstruct buffer_head *bh = head;\n\tstruct buffer_head *failed_bh;\n\n\tdo {\n\t\tif (!trylock_buffer(bh)) {\n\t\t\tif (mode == MIGRATE_ASYNC)\n\t\t\t\tgoto unlock;\n\t\t\tif (mode == MIGRATE_SYNC_LIGHT && !buffer_uptodate(bh))\n\t\t\t\tgoto unlock;\n\t\t\tlock_buffer(bh);\n\t\t}\n\n\t\tbh = bh->b_this_page;\n\t} while (bh != head);\n\n\treturn true;\n\nunlock:\n\t \n\tfailed_bh = bh;\n\tbh = head;\n\twhile (bh != failed_bh) {\n\t\tunlock_buffer(bh);\n\t\tbh = bh->b_this_page;\n\t}\n\n\treturn false;\n}\n\nstatic int __buffer_migrate_folio(struct address_space *mapping,\n\t\tstruct folio *dst, struct folio *src, enum migrate_mode mode,\n\t\tbool check_refs)\n{\n\tstruct buffer_head *bh, *head;\n\tint rc;\n\tint expected_count;\n\n\thead = folio_buffers(src);\n\tif (!head)\n\t\treturn migrate_folio(mapping, dst, src, mode);\n\n\t \n\texpected_count = folio_expected_refs(mapping, src);\n\tif (folio_ref_count(src) != expected_count)\n\t\treturn -EAGAIN;\n\n\tif (!buffer_migrate_lock_buffers(head, mode))\n\t\treturn -EAGAIN;\n\n\tif (check_refs) {\n\t\tbool busy;\n\t\tbool invalidated = false;\n\nrecheck_buffers:\n\t\tbusy = false;\n\t\tspin_lock(&mapping->private_lock);\n\t\tbh = head;\n\t\tdo {\n\t\t\tif (atomic_read(&bh->b_count)) {\n\t\t\t\tbusy = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbh = bh->b_this_page;\n\t\t} while (bh != head);\n\t\tif (busy) {\n\t\t\tif (invalidated) {\n\t\t\t\trc = -EAGAIN;\n\t\t\t\tgoto unlock_buffers;\n\t\t\t}\n\t\t\tspin_unlock(&mapping->private_lock);\n\t\t\tinvalidate_bh_lrus();\n\t\t\tinvalidated = true;\n\t\t\tgoto recheck_buffers;\n\t\t}\n\t}\n\n\trc = folio_migrate_mapping(mapping, dst, src, 0);\n\tif (rc != MIGRATEPAGE_SUCCESS)\n\t\tgoto unlock_buffers;\n\n\tfolio_attach_private(dst, folio_detach_private(src));\n\n\tbh = head;\n\tdo {\n\t\tfolio_set_bh(bh, dst, bh_offset(bh));\n\t\tbh = bh->b_this_page;\n\t} while (bh != head);\n\n\tif (mode != MIGRATE_SYNC_NO_COPY)\n\t\tfolio_migrate_copy(dst, src);\n\telse\n\t\tfolio_migrate_flags(dst, src);\n\n\trc = MIGRATEPAGE_SUCCESS;\nunlock_buffers:\n\tif (check_refs)\n\t\tspin_unlock(&mapping->private_lock);\n\tbh = head;\n\tdo {\n\t\tunlock_buffer(bh);\n\t\tbh = bh->b_this_page;\n\t} while (bh != head);\n\n\treturn rc;\n}\n\n \nint buffer_migrate_folio(struct address_space *mapping,\n\t\tstruct folio *dst, struct folio *src, enum migrate_mode mode)\n{\n\treturn __buffer_migrate_folio(mapping, dst, src, mode, false);\n}\nEXPORT_SYMBOL(buffer_migrate_folio);\n\n \nint buffer_migrate_folio_norefs(struct address_space *mapping,\n\t\tstruct folio *dst, struct folio *src, enum migrate_mode mode)\n{\n\treturn __buffer_migrate_folio(mapping, dst, src, mode, true);\n}\nEXPORT_SYMBOL_GPL(buffer_migrate_folio_norefs);\n#endif  \n\nint filemap_migrate_folio(struct address_space *mapping,\n\t\tstruct folio *dst, struct folio *src, enum migrate_mode mode)\n{\n\tint ret;\n\n\tret = folio_migrate_mapping(mapping, dst, src, 0);\n\tif (ret != MIGRATEPAGE_SUCCESS)\n\t\treturn ret;\n\n\tif (folio_get_private(src))\n\t\tfolio_attach_private(dst, folio_detach_private(src));\n\n\tif (mode != MIGRATE_SYNC_NO_COPY)\n\t\tfolio_migrate_copy(dst, src);\n\telse\n\t\tfolio_migrate_flags(dst, src);\n\treturn MIGRATEPAGE_SUCCESS;\n}\nEXPORT_SYMBOL_GPL(filemap_migrate_folio);\n\n \nstatic int writeout(struct address_space *mapping, struct folio *folio)\n{\n\tstruct writeback_control wbc = {\n\t\t.sync_mode = WB_SYNC_NONE,\n\t\t.nr_to_write = 1,\n\t\t.range_start = 0,\n\t\t.range_end = LLONG_MAX,\n\t\t.for_reclaim = 1\n\t};\n\tint rc;\n\n\tif (!mapping->a_ops->writepage)\n\t\t \n\t\treturn -EINVAL;\n\n\tif (!folio_clear_dirty_for_io(folio))\n\t\t \n\t\treturn -EAGAIN;\n\n\t \n\tremove_migration_ptes(folio, folio, false);\n\n\trc = mapping->a_ops->writepage(&folio->page, &wbc);\n\n\tif (rc != AOP_WRITEPAGE_ACTIVATE)\n\t\t \n\t\tfolio_lock(folio);\n\n\treturn (rc < 0) ? -EIO : -EAGAIN;\n}\n\n \nstatic int fallback_migrate_folio(struct address_space *mapping,\n\t\tstruct folio *dst, struct folio *src, enum migrate_mode mode)\n{\n\tif (folio_test_dirty(src)) {\n\t\t \n\t\tswitch (mode) {\n\t\tcase MIGRATE_SYNC:\n\t\tcase MIGRATE_SYNC_NO_COPY:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EBUSY;\n\t\t}\n\t\treturn writeout(mapping, src);\n\t}\n\n\t \n\tif (!filemap_release_folio(src, GFP_KERNEL))\n\t\treturn mode == MIGRATE_SYNC ? -EAGAIN : -EBUSY;\n\n\treturn migrate_folio(mapping, dst, src, mode);\n}\n\n \nstatic int move_to_new_folio(struct folio *dst, struct folio *src,\n\t\t\t\tenum migrate_mode mode)\n{\n\tint rc = -EAGAIN;\n\tbool is_lru = !__PageMovable(&src->page);\n\n\tVM_BUG_ON_FOLIO(!folio_test_locked(src), src);\n\tVM_BUG_ON_FOLIO(!folio_test_locked(dst), dst);\n\n\tif (likely(is_lru)) {\n\t\tstruct address_space *mapping = folio_mapping(src);\n\n\t\tif (!mapping)\n\t\t\trc = migrate_folio(mapping, dst, src, mode);\n\t\telse if (mapping->a_ops->migrate_folio)\n\t\t\t \n\t\t\trc = mapping->a_ops->migrate_folio(mapping, dst, src,\n\t\t\t\t\t\t\t\tmode);\n\t\telse\n\t\t\trc = fallback_migrate_folio(mapping, dst, src, mode);\n\t} else {\n\t\tconst struct movable_operations *mops;\n\n\t\t \n\t\tVM_BUG_ON_FOLIO(!folio_test_isolated(src), src);\n\t\tif (!folio_test_movable(src)) {\n\t\t\trc = MIGRATEPAGE_SUCCESS;\n\t\t\tfolio_clear_isolated(src);\n\t\t\tgoto out;\n\t\t}\n\n\t\tmops = folio_movable_ops(src);\n\t\trc = mops->migrate_page(&dst->page, &src->page, mode);\n\t\tWARN_ON_ONCE(rc == MIGRATEPAGE_SUCCESS &&\n\t\t\t\t!folio_test_isolated(src));\n\t}\n\n\t \n\tif (rc == MIGRATEPAGE_SUCCESS) {\n\t\tif (__PageMovable(&src->page)) {\n\t\t\tVM_BUG_ON_FOLIO(!folio_test_isolated(src), src);\n\n\t\t\t \n\t\t\tfolio_clear_isolated(src);\n\t\t}\n\n\t\t \n\t\tif (!folio_mapping_flags(src))\n\t\t\tsrc->mapping = NULL;\n\n\t\tif (likely(!folio_is_zone_device(dst)))\n\t\t\tflush_dcache_folio(dst);\n\t}\nout:\n\treturn rc;\n}\n\n \nunion migration_ptr {\n\tstruct anon_vma *anon_vma;\n\tstruct address_space *mapping;\n};\nstatic void __migrate_folio_record(struct folio *dst,\n\t\t\t\t   unsigned long page_was_mapped,\n\t\t\t\t   struct anon_vma *anon_vma)\n{\n\tunion migration_ptr ptr = { .anon_vma = anon_vma };\n\tdst->mapping = ptr.mapping;\n\tdst->private = (void *)page_was_mapped;\n}\n\nstatic void __migrate_folio_extract(struct folio *dst,\n\t\t\t\t   int *page_was_mappedp,\n\t\t\t\t   struct anon_vma **anon_vmap)\n{\n\tunion migration_ptr ptr = { .mapping = dst->mapping };\n\t*anon_vmap = ptr.anon_vma;\n\t*page_was_mappedp = (unsigned long)dst->private;\n\tdst->mapping = NULL;\n\tdst->private = NULL;\n}\n\n \nstatic void migrate_folio_undo_src(struct folio *src,\n\t\t\t\t   int page_was_mapped,\n\t\t\t\t   struct anon_vma *anon_vma,\n\t\t\t\t   bool locked,\n\t\t\t\t   struct list_head *ret)\n{\n\tif (page_was_mapped)\n\t\tremove_migration_ptes(src, src, false);\n\t \n\tif (anon_vma)\n\t\tput_anon_vma(anon_vma);\n\tif (locked)\n\t\tfolio_unlock(src);\n\tif (ret)\n\t\tlist_move_tail(&src->lru, ret);\n}\n\n \nstatic void migrate_folio_undo_dst(struct folio *dst, bool locked,\n\t\tfree_folio_t put_new_folio, unsigned long private)\n{\n\tif (locked)\n\t\tfolio_unlock(dst);\n\tif (put_new_folio)\n\t\tput_new_folio(dst, private);\n\telse\n\t\tfolio_put(dst);\n}\n\n \nstatic void migrate_folio_done(struct folio *src,\n\t\t\t       enum migrate_reason reason)\n{\n\t \n\tif (likely(!__folio_test_movable(src)))\n\t\tmod_node_page_state(folio_pgdat(src), NR_ISOLATED_ANON +\n\t\t\t\t    folio_is_file_lru(src), -folio_nr_pages(src));\n\n\tif (reason != MR_MEMORY_FAILURE)\n\t\t \n\t\tfolio_put(src);\n}\n\n \nstatic int migrate_folio_unmap(new_folio_t get_new_folio,\n\t\tfree_folio_t put_new_folio, unsigned long private,\n\t\tstruct folio *src, struct folio **dstp, enum migrate_mode mode,\n\t\tenum migrate_reason reason, struct list_head *ret)\n{\n\tstruct folio *dst;\n\tint rc = -EAGAIN;\n\tint page_was_mapped = 0;\n\tstruct anon_vma *anon_vma = NULL;\n\tbool is_lru = !__PageMovable(&src->page);\n\tbool locked = false;\n\tbool dst_locked = false;\n\n\tif (folio_ref_count(src) == 1) {\n\t\t \n\t\tfolio_clear_active(src);\n\t\tfolio_clear_unevictable(src);\n\t\t \n\t\tlist_del(&src->lru);\n\t\tmigrate_folio_done(src, reason);\n\t\treturn MIGRATEPAGE_SUCCESS;\n\t}\n\n\tdst = get_new_folio(src, private);\n\tif (!dst)\n\t\treturn -ENOMEM;\n\t*dstp = dst;\n\n\tdst->private = NULL;\n\n\tif (!folio_trylock(src)) {\n\t\tif (mode == MIGRATE_ASYNC)\n\t\t\tgoto out;\n\n\t\t \n\t\tif (current->flags & PF_MEMALLOC)\n\t\t\tgoto out;\n\n\t\t \n\t\tif (mode == MIGRATE_SYNC_LIGHT && !folio_test_uptodate(src))\n\t\t\tgoto out;\n\n\t\tfolio_lock(src);\n\t}\n\tlocked = true;\n\n\tif (folio_test_writeback(src)) {\n\t\t \n\t\tswitch (mode) {\n\t\tcase MIGRATE_SYNC:\n\t\tcase MIGRATE_SYNC_NO_COPY:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\trc = -EBUSY;\n\t\t\tgoto out;\n\t\t}\n\t\tfolio_wait_writeback(src);\n\t}\n\n\t \n\tif (folio_test_anon(src) && !folio_test_ksm(src))\n\t\tanon_vma = folio_get_anon_vma(src);\n\n\t \n\tif (unlikely(!folio_trylock(dst)))\n\t\tgoto out;\n\tdst_locked = true;\n\n\tif (unlikely(!is_lru)) {\n\t\t__migrate_folio_record(dst, page_was_mapped, anon_vma);\n\t\treturn MIGRATEPAGE_UNMAP;\n\t}\n\n\t \n\tif (!src->mapping) {\n\t\tif (folio_test_private(src)) {\n\t\t\ttry_to_free_buffers(src);\n\t\t\tgoto out;\n\t\t}\n\t} else if (folio_mapped(src)) {\n\t\t \n\t\tVM_BUG_ON_FOLIO(folio_test_anon(src) &&\n\t\t\t       !folio_test_ksm(src) && !anon_vma, src);\n\t\ttry_to_migrate(src, mode == MIGRATE_ASYNC ? TTU_BATCH_FLUSH : 0);\n\t\tpage_was_mapped = 1;\n\t}\n\n\tif (!folio_mapped(src)) {\n\t\t__migrate_folio_record(dst, page_was_mapped, anon_vma);\n\t\treturn MIGRATEPAGE_UNMAP;\n\t}\n\nout:\n\t \n\tif (rc == -EAGAIN)\n\t\tret = NULL;\n\n\tmigrate_folio_undo_src(src, page_was_mapped, anon_vma, locked, ret);\n\tmigrate_folio_undo_dst(dst, dst_locked, put_new_folio, private);\n\n\treturn rc;\n}\n\n \nstatic int migrate_folio_move(free_folio_t put_new_folio, unsigned long private,\n\t\t\t      struct folio *src, struct folio *dst,\n\t\t\t      enum migrate_mode mode, enum migrate_reason reason,\n\t\t\t      struct list_head *ret)\n{\n\tint rc;\n\tint page_was_mapped = 0;\n\tstruct anon_vma *anon_vma = NULL;\n\tbool is_lru = !__PageMovable(&src->page);\n\tstruct list_head *prev;\n\n\t__migrate_folio_extract(dst, &page_was_mapped, &anon_vma);\n\tprev = dst->lru.prev;\n\tlist_del(&dst->lru);\n\n\trc = move_to_new_folio(dst, src, mode);\n\tif (rc)\n\t\tgoto out;\n\n\tif (unlikely(!is_lru))\n\t\tgoto out_unlock_both;\n\n\t \n\tfolio_add_lru(dst);\n\tif (page_was_mapped)\n\t\tlru_add_drain();\n\n\tif (page_was_mapped)\n\t\tremove_migration_ptes(src, dst, false);\n\nout_unlock_both:\n\tfolio_unlock(dst);\n\tset_page_owner_migrate_reason(&dst->page, reason);\n\t \n\tfolio_put(dst);\n\n\t \n\tlist_del(&src->lru);\n\t \n\tif (anon_vma)\n\t\tput_anon_vma(anon_vma);\n\tfolio_unlock(src);\n\tmigrate_folio_done(src, reason);\n\n\treturn rc;\nout:\n\t \n\tif (rc == -EAGAIN) {\n\t\tlist_add(&dst->lru, prev);\n\t\t__migrate_folio_record(dst, page_was_mapped, anon_vma);\n\t\treturn rc;\n\t}\n\n\tmigrate_folio_undo_src(src, page_was_mapped, anon_vma, true, ret);\n\tmigrate_folio_undo_dst(dst, true, put_new_folio, private);\n\n\treturn rc;\n}\n\n \nstatic int unmap_and_move_huge_page(new_folio_t get_new_folio,\n\t\tfree_folio_t put_new_folio, unsigned long private,\n\t\tstruct folio *src, int force, enum migrate_mode mode,\n\t\tint reason, struct list_head *ret)\n{\n\tstruct folio *dst;\n\tint rc = -EAGAIN;\n\tint page_was_mapped = 0;\n\tstruct anon_vma *anon_vma = NULL;\n\tstruct address_space *mapping = NULL;\n\n\tif (folio_ref_count(src) == 1) {\n\t\t \n\t\tfolio_putback_active_hugetlb(src);\n\t\treturn MIGRATEPAGE_SUCCESS;\n\t}\n\n\tdst = get_new_folio(src, private);\n\tif (!dst)\n\t\treturn -ENOMEM;\n\n\tif (!folio_trylock(src)) {\n\t\tif (!force)\n\t\t\tgoto out;\n\t\tswitch (mode) {\n\t\tcase MIGRATE_SYNC:\n\t\tcase MIGRATE_SYNC_NO_COPY:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto out;\n\t\t}\n\t\tfolio_lock(src);\n\t}\n\n\t \n\tif (hugetlb_folio_subpool(src) && !folio_mapping(src)) {\n\t\trc = -EBUSY;\n\t\tgoto out_unlock;\n\t}\n\n\tif (folio_test_anon(src))\n\t\tanon_vma = folio_get_anon_vma(src);\n\n\tif (unlikely(!folio_trylock(dst)))\n\t\tgoto put_anon;\n\n\tif (folio_mapped(src)) {\n\t\tenum ttu_flags ttu = 0;\n\n\t\tif (!folio_test_anon(src)) {\n\t\t\t \n\t\t\tmapping = hugetlb_page_mapping_lock_write(&src->page);\n\t\t\tif (unlikely(!mapping))\n\t\t\t\tgoto unlock_put_anon;\n\n\t\t\tttu = TTU_RMAP_LOCKED;\n\t\t}\n\n\t\ttry_to_migrate(src, ttu);\n\t\tpage_was_mapped = 1;\n\n\t\tif (ttu & TTU_RMAP_LOCKED)\n\t\t\ti_mmap_unlock_write(mapping);\n\t}\n\n\tif (!folio_mapped(src))\n\t\trc = move_to_new_folio(dst, src, mode);\n\n\tif (page_was_mapped)\n\t\tremove_migration_ptes(src,\n\t\t\trc == MIGRATEPAGE_SUCCESS ? dst : src, false);\n\nunlock_put_anon:\n\tfolio_unlock(dst);\n\nput_anon:\n\tif (anon_vma)\n\t\tput_anon_vma(anon_vma);\n\n\tif (rc == MIGRATEPAGE_SUCCESS) {\n\t\tmove_hugetlb_state(src, dst, reason);\n\t\tput_new_folio = NULL;\n\t}\n\nout_unlock:\n\tfolio_unlock(src);\nout:\n\tif (rc == MIGRATEPAGE_SUCCESS)\n\t\tfolio_putback_active_hugetlb(src);\n\telse if (rc != -EAGAIN)\n\t\tlist_move_tail(&src->lru, ret);\n\n\t \n\tif (put_new_folio)\n\t\tput_new_folio(dst, private);\n\telse\n\t\tfolio_putback_active_hugetlb(dst);\n\n\treturn rc;\n}\n\nstatic inline int try_split_folio(struct folio *folio, struct list_head *split_folios)\n{\n\tint rc;\n\n\tfolio_lock(folio);\n\trc = split_folio_to_list(folio, split_folios);\n\tfolio_unlock(folio);\n\tif (!rc)\n\t\tlist_move_tail(&folio->lru, split_folios);\n\n\treturn rc;\n}\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n#define NR_MAX_BATCHED_MIGRATION\tHPAGE_PMD_NR\n#else\n#define NR_MAX_BATCHED_MIGRATION\t512\n#endif\n#define NR_MAX_MIGRATE_PAGES_RETRY\t10\n#define NR_MAX_MIGRATE_ASYNC_RETRY\t3\n#define NR_MAX_MIGRATE_SYNC_RETRY\t\t\t\t\t\\\n\t(NR_MAX_MIGRATE_PAGES_RETRY - NR_MAX_MIGRATE_ASYNC_RETRY)\n\nstruct migrate_pages_stats {\n\tint nr_succeeded;\t \n\tint nr_failed_pages;\t \n\tint nr_thp_succeeded;\t \n\tint nr_thp_failed;\t \n\tint nr_thp_split;\t \n};\n\n \nstatic int migrate_hugetlbs(struct list_head *from, new_folio_t get_new_folio,\n\t\t\t    free_folio_t put_new_folio, unsigned long private,\n\t\t\t    enum migrate_mode mode, int reason,\n\t\t\t    struct migrate_pages_stats *stats,\n\t\t\t    struct list_head *ret_folios)\n{\n\tint retry = 1;\n\tint nr_failed = 0;\n\tint nr_retry_pages = 0;\n\tint pass = 0;\n\tstruct folio *folio, *folio2;\n\tint rc, nr_pages;\n\n\tfor (pass = 0; pass < NR_MAX_MIGRATE_PAGES_RETRY && retry; pass++) {\n\t\tretry = 0;\n\t\tnr_retry_pages = 0;\n\n\t\tlist_for_each_entry_safe(folio, folio2, from, lru) {\n\t\t\tif (!folio_test_hugetlb(folio))\n\t\t\t\tcontinue;\n\n\t\t\tnr_pages = folio_nr_pages(folio);\n\n\t\t\tcond_resched();\n\n\t\t\t \n\t\t\tif (!hugepage_migration_supported(folio_hstate(folio))) {\n\t\t\t\tnr_failed++;\n\t\t\t\tstats->nr_failed_pages += nr_pages;\n\t\t\t\tlist_move_tail(&folio->lru, ret_folios);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\trc = unmap_and_move_huge_page(get_new_folio,\n\t\t\t\t\t\t      put_new_folio, private,\n\t\t\t\t\t\t      folio, pass > 2, mode,\n\t\t\t\t\t\t      reason, ret_folios);\n\t\t\t \n\t\t\tswitch(rc) {\n\t\t\tcase -ENOMEM:\n\t\t\t\t \n\t\t\t\tstats->nr_failed_pages += nr_pages + nr_retry_pages;\n\t\t\t\treturn -ENOMEM;\n\t\t\tcase -EAGAIN:\n\t\t\t\tretry++;\n\t\t\t\tnr_retry_pages += nr_pages;\n\t\t\t\tbreak;\n\t\t\tcase MIGRATEPAGE_SUCCESS:\n\t\t\t\tstats->nr_succeeded += nr_pages;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\t \n\t\t\t\tnr_failed++;\n\t\t\t\tstats->nr_failed_pages += nr_pages;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\t \n\tnr_failed += retry;\n\tstats->nr_failed_pages += nr_retry_pages;\n\n\treturn nr_failed;\n}\n\n \nstatic int migrate_pages_batch(struct list_head *from,\n\t\tnew_folio_t get_new_folio, free_folio_t put_new_folio,\n\t\tunsigned long private, enum migrate_mode mode, int reason,\n\t\tstruct list_head *ret_folios, struct list_head *split_folios,\n\t\tstruct migrate_pages_stats *stats, int nr_pass)\n{\n\tint retry = 1;\n\tint thp_retry = 1;\n\tint nr_failed = 0;\n\tint nr_retry_pages = 0;\n\tint pass = 0;\n\tbool is_thp = false;\n\tstruct folio *folio, *folio2, *dst = NULL, *dst2;\n\tint rc, rc_saved = 0, nr_pages;\n\tLIST_HEAD(unmap_folios);\n\tLIST_HEAD(dst_folios);\n\tbool nosplit = (reason == MR_NUMA_MISPLACED);\n\n\tVM_WARN_ON_ONCE(mode != MIGRATE_ASYNC &&\n\t\t\t!list_empty(from) && !list_is_singular(from));\n\n\tfor (pass = 0; pass < nr_pass && retry; pass++) {\n\t\tretry = 0;\n\t\tthp_retry = 0;\n\t\tnr_retry_pages = 0;\n\n\t\tlist_for_each_entry_safe(folio, folio2, from, lru) {\n\t\t\tis_thp = folio_test_large(folio) && folio_test_pmd_mappable(folio);\n\t\t\tnr_pages = folio_nr_pages(folio);\n\n\t\t\tcond_resched();\n\n\t\t\t \n\t\t\tif (!thp_migration_supported() && is_thp) {\n\t\t\t\tnr_failed++;\n\t\t\t\tstats->nr_thp_failed++;\n\t\t\t\tif (!try_split_folio(folio, split_folios)) {\n\t\t\t\t\tstats->nr_thp_split++;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tstats->nr_failed_pages += nr_pages;\n\t\t\t\tlist_move_tail(&folio->lru, ret_folios);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\trc = migrate_folio_unmap(get_new_folio, put_new_folio,\n\t\t\t\t\tprivate, folio, &dst, mode, reason,\n\t\t\t\t\tret_folios);\n\t\t\t \n\t\t\tswitch(rc) {\n\t\t\tcase -ENOMEM:\n\t\t\t\t \n\t\t\t\tnr_failed++;\n\t\t\t\tstats->nr_thp_failed += is_thp;\n\t\t\t\t \n\t\t\t\tif (folio_test_large(folio) && !nosplit) {\n\t\t\t\t\tint ret = try_split_folio(folio, split_folios);\n\n\t\t\t\t\tif (!ret) {\n\t\t\t\t\t\tstats->nr_thp_split += is_thp;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t} else if (reason == MR_LONGTERM_PIN &&\n\t\t\t\t\t\t   ret == -EAGAIN) {\n\t\t\t\t\t\t \n\t\t\t\t\t\tretry++;\n\t\t\t\t\t\tthp_retry += is_thp;\n\t\t\t\t\t\tnr_retry_pages += nr_pages;\n\t\t\t\t\t\t \n\t\t\t\t\t\tnr_failed--;\n\t\t\t\t\t\tstats->nr_thp_failed -= is_thp;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tstats->nr_failed_pages += nr_pages + nr_retry_pages;\n\t\t\t\t \n\t\t\t\tstats->nr_thp_failed += thp_retry;\n\t\t\t\trc_saved = rc;\n\t\t\t\tif (list_empty(&unmap_folios))\n\t\t\t\t\tgoto out;\n\t\t\t\telse\n\t\t\t\t\tgoto move;\n\t\t\tcase -EAGAIN:\n\t\t\t\tretry++;\n\t\t\t\tthp_retry += is_thp;\n\t\t\t\tnr_retry_pages += nr_pages;\n\t\t\t\tbreak;\n\t\t\tcase MIGRATEPAGE_SUCCESS:\n\t\t\t\tstats->nr_succeeded += nr_pages;\n\t\t\t\tstats->nr_thp_succeeded += is_thp;\n\t\t\t\tbreak;\n\t\t\tcase MIGRATEPAGE_UNMAP:\n\t\t\t\tlist_move_tail(&folio->lru, &unmap_folios);\n\t\t\t\tlist_add_tail(&dst->lru, &dst_folios);\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\t \n\t\t\t\tnr_failed++;\n\t\t\t\tstats->nr_thp_failed += is_thp;\n\t\t\t\tstats->nr_failed_pages += nr_pages;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\tnr_failed += retry;\n\tstats->nr_thp_failed += thp_retry;\n\tstats->nr_failed_pages += nr_retry_pages;\nmove:\n\t \n\ttry_to_unmap_flush();\n\n\tretry = 1;\n\tfor (pass = 0; pass < nr_pass && retry; pass++) {\n\t\tretry = 0;\n\t\tthp_retry = 0;\n\t\tnr_retry_pages = 0;\n\n\t\tdst = list_first_entry(&dst_folios, struct folio, lru);\n\t\tdst2 = list_next_entry(dst, lru);\n\t\tlist_for_each_entry_safe(folio, folio2, &unmap_folios, lru) {\n\t\t\tis_thp = folio_test_large(folio) && folio_test_pmd_mappable(folio);\n\t\t\tnr_pages = folio_nr_pages(folio);\n\n\t\t\tcond_resched();\n\n\t\t\trc = migrate_folio_move(put_new_folio, private,\n\t\t\t\t\t\tfolio, dst, mode,\n\t\t\t\t\t\treason, ret_folios);\n\t\t\t \n\t\t\tswitch(rc) {\n\t\t\tcase -EAGAIN:\n\t\t\t\tretry++;\n\t\t\t\tthp_retry += is_thp;\n\t\t\t\tnr_retry_pages += nr_pages;\n\t\t\t\tbreak;\n\t\t\tcase MIGRATEPAGE_SUCCESS:\n\t\t\t\tstats->nr_succeeded += nr_pages;\n\t\t\t\tstats->nr_thp_succeeded += is_thp;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tnr_failed++;\n\t\t\t\tstats->nr_thp_failed += is_thp;\n\t\t\t\tstats->nr_failed_pages += nr_pages;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tdst = dst2;\n\t\t\tdst2 = list_next_entry(dst, lru);\n\t\t}\n\t}\n\tnr_failed += retry;\n\tstats->nr_thp_failed += thp_retry;\n\tstats->nr_failed_pages += nr_retry_pages;\n\n\trc = rc_saved ? : nr_failed;\nout:\n\t \n\tdst = list_first_entry(&dst_folios, struct folio, lru);\n\tdst2 = list_next_entry(dst, lru);\n\tlist_for_each_entry_safe(folio, folio2, &unmap_folios, lru) {\n\t\tint page_was_mapped = 0;\n\t\tstruct anon_vma *anon_vma = NULL;\n\n\t\t__migrate_folio_extract(dst, &page_was_mapped, &anon_vma);\n\t\tmigrate_folio_undo_src(folio, page_was_mapped, anon_vma,\n\t\t\t\t       true, ret_folios);\n\t\tlist_del(&dst->lru);\n\t\tmigrate_folio_undo_dst(dst, true, put_new_folio, private);\n\t\tdst = dst2;\n\t\tdst2 = list_next_entry(dst, lru);\n\t}\n\n\treturn rc;\n}\n\nstatic int migrate_pages_sync(struct list_head *from, new_folio_t get_new_folio,\n\t\tfree_folio_t put_new_folio, unsigned long private,\n\t\tenum migrate_mode mode, int reason,\n\t\tstruct list_head *ret_folios, struct list_head *split_folios,\n\t\tstruct migrate_pages_stats *stats)\n{\n\tint rc, nr_failed = 0;\n\tLIST_HEAD(folios);\n\tstruct migrate_pages_stats astats;\n\n\tmemset(&astats, 0, sizeof(astats));\n\t \n\trc = migrate_pages_batch(from, get_new_folio, put_new_folio, private, MIGRATE_ASYNC,\n\t\t\t\t reason, &folios, split_folios, &astats,\n\t\t\t\t NR_MAX_MIGRATE_ASYNC_RETRY);\n\tstats->nr_succeeded += astats.nr_succeeded;\n\tstats->nr_thp_succeeded += astats.nr_thp_succeeded;\n\tstats->nr_thp_split += astats.nr_thp_split;\n\tif (rc < 0) {\n\t\tstats->nr_failed_pages += astats.nr_failed_pages;\n\t\tstats->nr_thp_failed += astats.nr_thp_failed;\n\t\tlist_splice_tail(&folios, ret_folios);\n\t\treturn rc;\n\t}\n\tstats->nr_thp_failed += astats.nr_thp_split;\n\tnr_failed += astats.nr_thp_split;\n\t \n\tlist_splice_tail_init(&folios, from);\n\twhile (!list_empty(from)) {\n\t\tlist_move(from->next, &folios);\n\t\trc = migrate_pages_batch(&folios, get_new_folio, put_new_folio,\n\t\t\t\t\t private, mode, reason, ret_folios,\n\t\t\t\t\t split_folios, stats, NR_MAX_MIGRATE_SYNC_RETRY);\n\t\tlist_splice_tail_init(&folios, ret_folios);\n\t\tif (rc < 0)\n\t\t\treturn rc;\n\t\tnr_failed += rc;\n\t}\n\n\treturn nr_failed;\n}\n\n \nint migrate_pages(struct list_head *from, new_folio_t get_new_folio,\n\t\tfree_folio_t put_new_folio, unsigned long private,\n\t\tenum migrate_mode mode, int reason, unsigned int *ret_succeeded)\n{\n\tint rc, rc_gather;\n\tint nr_pages;\n\tstruct folio *folio, *folio2;\n\tLIST_HEAD(folios);\n\tLIST_HEAD(ret_folios);\n\tLIST_HEAD(split_folios);\n\tstruct migrate_pages_stats stats;\n\n\ttrace_mm_migrate_pages_start(mode, reason);\n\n\tmemset(&stats, 0, sizeof(stats));\n\n\trc_gather = migrate_hugetlbs(from, get_new_folio, put_new_folio, private,\n\t\t\t\t     mode, reason, &stats, &ret_folios);\n\tif (rc_gather < 0)\n\t\tgoto out;\n\nagain:\n\tnr_pages = 0;\n\tlist_for_each_entry_safe(folio, folio2, from, lru) {\n\t\t \n\t\tif (folio_test_hugetlb(folio)) {\n\t\t\tlist_move_tail(&folio->lru, &ret_folios);\n\t\t\tcontinue;\n\t\t}\n\n\t\tnr_pages += folio_nr_pages(folio);\n\t\tif (nr_pages >= NR_MAX_BATCHED_MIGRATION)\n\t\t\tbreak;\n\t}\n\tif (nr_pages >= NR_MAX_BATCHED_MIGRATION)\n\t\tlist_cut_before(&folios, from, &folio2->lru);\n\telse\n\t\tlist_splice_init(from, &folios);\n\tif (mode == MIGRATE_ASYNC)\n\t\trc = migrate_pages_batch(&folios, get_new_folio, put_new_folio,\n\t\t\t\tprivate, mode, reason, &ret_folios,\n\t\t\t\t&split_folios, &stats,\n\t\t\t\tNR_MAX_MIGRATE_PAGES_RETRY);\n\telse\n\t\trc = migrate_pages_sync(&folios, get_new_folio, put_new_folio,\n\t\t\t\tprivate, mode, reason, &ret_folios,\n\t\t\t\t&split_folios, &stats);\n\tlist_splice_tail_init(&folios, &ret_folios);\n\tif (rc < 0) {\n\t\trc_gather = rc;\n\t\tlist_splice_tail(&split_folios, &ret_folios);\n\t\tgoto out;\n\t}\n\tif (!list_empty(&split_folios)) {\n\t\t \n\t\tmigrate_pages_batch(&split_folios, get_new_folio,\n\t\t\t\tput_new_folio, private, MIGRATE_ASYNC, reason,\n\t\t\t\t&ret_folios, NULL, &stats, 1);\n\t\tlist_splice_tail_init(&split_folios, &ret_folios);\n\t}\n\trc_gather += rc;\n\tif (!list_empty(from))\n\t\tgoto again;\nout:\n\t \n\tlist_splice(&ret_folios, from);\n\n\t \n\tif (list_empty(from))\n\t\trc_gather = 0;\n\n\tcount_vm_events(PGMIGRATE_SUCCESS, stats.nr_succeeded);\n\tcount_vm_events(PGMIGRATE_FAIL, stats.nr_failed_pages);\n\tcount_vm_events(THP_MIGRATION_SUCCESS, stats.nr_thp_succeeded);\n\tcount_vm_events(THP_MIGRATION_FAIL, stats.nr_thp_failed);\n\tcount_vm_events(THP_MIGRATION_SPLIT, stats.nr_thp_split);\n\ttrace_mm_migrate_pages(stats.nr_succeeded, stats.nr_failed_pages,\n\t\t\t       stats.nr_thp_succeeded, stats.nr_thp_failed,\n\t\t\t       stats.nr_thp_split, mode, reason);\n\n\tif (ret_succeeded)\n\t\t*ret_succeeded = stats.nr_succeeded;\n\n\treturn rc_gather;\n}\n\nstruct folio *alloc_migration_target(struct folio *src, unsigned long private)\n{\n\tstruct migration_target_control *mtc;\n\tgfp_t gfp_mask;\n\tunsigned int order = 0;\n\tint nid;\n\tint zidx;\n\n\tmtc = (struct migration_target_control *)private;\n\tgfp_mask = mtc->gfp_mask;\n\tnid = mtc->nid;\n\tif (nid == NUMA_NO_NODE)\n\t\tnid = folio_nid(src);\n\n\tif (folio_test_hugetlb(src)) {\n\t\tstruct hstate *h = folio_hstate(src);\n\n\t\tgfp_mask = htlb_modify_alloc_mask(h, gfp_mask);\n\t\treturn alloc_hugetlb_folio_nodemask(h, nid,\n\t\t\t\t\t\tmtc->nmask, gfp_mask);\n\t}\n\n\tif (folio_test_large(src)) {\n\t\t \n\t\tgfp_mask &= ~__GFP_RECLAIM;\n\t\tgfp_mask |= GFP_TRANSHUGE;\n\t\torder = folio_order(src);\n\t}\n\tzidx = zone_idx(folio_zone(src));\n\tif (is_highmem_idx(zidx) || zidx == ZONE_MOVABLE)\n\t\tgfp_mask |= __GFP_HIGHMEM;\n\n\treturn __folio_alloc(gfp_mask, order, nid, mtc->nmask);\n}\n\n#ifdef CONFIG_NUMA\n\nstatic int store_status(int __user *status, int start, int value, int nr)\n{\n\twhile (nr-- > 0) {\n\t\tif (put_user(value, status + start))\n\t\t\treturn -EFAULT;\n\t\tstart++;\n\t}\n\n\treturn 0;\n}\n\nstatic int do_move_pages_to_node(struct mm_struct *mm,\n\t\tstruct list_head *pagelist, int node)\n{\n\tint err;\n\tstruct migration_target_control mtc = {\n\t\t.nid = node,\n\t\t.gfp_mask = GFP_HIGHUSER_MOVABLE | __GFP_THISNODE,\n\t};\n\n\terr = migrate_pages(pagelist, alloc_migration_target, NULL,\n\t\t(unsigned long)&mtc, MIGRATE_SYNC, MR_SYSCALL, NULL);\n\tif (err)\n\t\tputback_movable_pages(pagelist);\n\treturn err;\n}\n\n \nstatic int add_page_for_migration(struct mm_struct *mm, const void __user *p,\n\t\tint node, struct list_head *pagelist, bool migrate_all)\n{\n\tstruct vm_area_struct *vma;\n\tunsigned long addr;\n\tstruct page *page;\n\tint err;\n\tbool isolated;\n\n\tmmap_read_lock(mm);\n\taddr = (unsigned long)untagged_addr_remote(mm, p);\n\n\terr = -EFAULT;\n\tvma = vma_lookup(mm, addr);\n\tif (!vma || !vma_migratable(vma))\n\t\tgoto out;\n\n\t \n\tpage = follow_page(vma, addr, FOLL_GET | FOLL_DUMP);\n\n\terr = PTR_ERR(page);\n\tif (IS_ERR(page))\n\t\tgoto out;\n\n\terr = -ENOENT;\n\tif (!page)\n\t\tgoto out;\n\n\tif (is_zone_device_page(page))\n\t\tgoto out_putpage;\n\n\terr = 0;\n\tif (page_to_nid(page) == node)\n\t\tgoto out_putpage;\n\n\terr = -EACCES;\n\tif (page_mapcount(page) > 1 && !migrate_all)\n\t\tgoto out_putpage;\n\n\tif (PageHuge(page)) {\n\t\tif (PageHead(page)) {\n\t\t\tisolated = isolate_hugetlb(page_folio(page), pagelist);\n\t\t\terr = isolated ? 1 : -EBUSY;\n\t\t}\n\t} else {\n\t\tstruct page *head;\n\n\t\thead = compound_head(page);\n\t\tisolated = isolate_lru_page(head);\n\t\tif (!isolated) {\n\t\t\terr = -EBUSY;\n\t\t\tgoto out_putpage;\n\t\t}\n\n\t\terr = 1;\n\t\tlist_add_tail(&head->lru, pagelist);\n\t\tmod_node_page_state(page_pgdat(head),\n\t\t\tNR_ISOLATED_ANON + page_is_file_lru(head),\n\t\t\tthp_nr_pages(head));\n\t}\nout_putpage:\n\t \n\tput_page(page);\nout:\n\tmmap_read_unlock(mm);\n\treturn err;\n}\n\nstatic int move_pages_and_store_status(struct mm_struct *mm, int node,\n\t\tstruct list_head *pagelist, int __user *status,\n\t\tint start, int i, unsigned long nr_pages)\n{\n\tint err;\n\n\tif (list_empty(pagelist))\n\t\treturn 0;\n\n\terr = do_move_pages_to_node(mm, pagelist, node);\n\tif (err) {\n\t\t \n\t\tif (err > 0)\n\t\t\terr += nr_pages - i;\n\t\treturn err;\n\t}\n\treturn store_status(status, start, node, i - start);\n}\n\n \nstatic int do_pages_move(struct mm_struct *mm, nodemask_t task_nodes,\n\t\t\t unsigned long nr_pages,\n\t\t\t const void __user * __user *pages,\n\t\t\t const int __user *nodes,\n\t\t\t int __user *status, int flags)\n{\n\tcompat_uptr_t __user *compat_pages = (void __user *)pages;\n\tint current_node = NUMA_NO_NODE;\n\tLIST_HEAD(pagelist);\n\tint start, i;\n\tint err = 0, err1;\n\n\tlru_cache_disable();\n\n\tfor (i = start = 0; i < nr_pages; i++) {\n\t\tconst void __user *p;\n\t\tint node;\n\n\t\terr = -EFAULT;\n\t\tif (in_compat_syscall()) {\n\t\t\tcompat_uptr_t cp;\n\n\t\t\tif (get_user(cp, compat_pages + i))\n\t\t\t\tgoto out_flush;\n\n\t\t\tp = compat_ptr(cp);\n\t\t} else {\n\t\t\tif (get_user(p, pages + i))\n\t\t\t\tgoto out_flush;\n\t\t}\n\t\tif (get_user(node, nodes + i))\n\t\t\tgoto out_flush;\n\n\t\terr = -ENODEV;\n\t\tif (node < 0 || node >= MAX_NUMNODES)\n\t\t\tgoto out_flush;\n\t\tif (!node_state(node, N_MEMORY))\n\t\t\tgoto out_flush;\n\n\t\terr = -EACCES;\n\t\tif (!node_isset(node, task_nodes))\n\t\t\tgoto out_flush;\n\n\t\tif (current_node == NUMA_NO_NODE) {\n\t\t\tcurrent_node = node;\n\t\t\tstart = i;\n\t\t} else if (node != current_node) {\n\t\t\terr = move_pages_and_store_status(mm, current_node,\n\t\t\t\t\t&pagelist, status, start, i, nr_pages);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t\tstart = i;\n\t\t\tcurrent_node = node;\n\t\t}\n\n\t\t \n\t\terr = add_page_for_migration(mm, p, current_node, &pagelist,\n\t\t\t\t\t     flags & MPOL_MF_MOVE_ALL);\n\n\t\tif (err > 0) {\n\t\t\t \n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (err == -EEXIST)\n\t\t\terr = -EFAULT;\n\n\t\t \n\t\terr = store_status(status, i, err ? : current_node, 1);\n\t\tif (err)\n\t\t\tgoto out_flush;\n\n\t\terr = move_pages_and_store_status(mm, current_node, &pagelist,\n\t\t\t\tstatus, start, i, nr_pages);\n\t\tif (err) {\n\t\t\t \n\t\t\tif (err > 0)\n\t\t\t\terr--;\n\t\t\tgoto out;\n\t\t}\n\t\tcurrent_node = NUMA_NO_NODE;\n\t}\nout_flush:\n\t \n\terr1 = move_pages_and_store_status(mm, current_node, &pagelist,\n\t\t\t\tstatus, start, i, nr_pages);\n\tif (err >= 0)\n\t\terr = err1;\nout:\n\tlru_cache_enable();\n\treturn err;\n}\n\n \nstatic void do_pages_stat_array(struct mm_struct *mm, unsigned long nr_pages,\n\t\t\t\tconst void __user **pages, int *status)\n{\n\tunsigned long i;\n\n\tmmap_read_lock(mm);\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tunsigned long addr = (unsigned long)(*pages);\n\t\tstruct vm_area_struct *vma;\n\t\tstruct page *page;\n\t\tint err = -EFAULT;\n\n\t\tvma = vma_lookup(mm, addr);\n\t\tif (!vma)\n\t\t\tgoto set_status;\n\n\t\t \n\t\tpage = follow_page(vma, addr, FOLL_GET | FOLL_DUMP);\n\n\t\terr = PTR_ERR(page);\n\t\tif (IS_ERR(page))\n\t\t\tgoto set_status;\n\n\t\terr = -ENOENT;\n\t\tif (!page)\n\t\t\tgoto set_status;\n\n\t\tif (!is_zone_device_page(page))\n\t\t\terr = page_to_nid(page);\n\n\t\tput_page(page);\nset_status:\n\t\t*status = err;\n\n\t\tpages++;\n\t\tstatus++;\n\t}\n\n\tmmap_read_unlock(mm);\n}\n\nstatic int get_compat_pages_array(const void __user *chunk_pages[],\n\t\t\t\t  const void __user * __user *pages,\n\t\t\t\t  unsigned long chunk_nr)\n{\n\tcompat_uptr_t __user *pages32 = (compat_uptr_t __user *)pages;\n\tcompat_uptr_t p;\n\tint i;\n\n\tfor (i = 0; i < chunk_nr; i++) {\n\t\tif (get_user(p, pages32 + i))\n\t\t\treturn -EFAULT;\n\t\tchunk_pages[i] = compat_ptr(p);\n\t}\n\n\treturn 0;\n}\n\n \nstatic int do_pages_stat(struct mm_struct *mm, unsigned long nr_pages,\n\t\t\t const void __user * __user *pages,\n\t\t\t int __user *status)\n{\n#define DO_PAGES_STAT_CHUNK_NR 16UL\n\tconst void __user *chunk_pages[DO_PAGES_STAT_CHUNK_NR];\n\tint chunk_status[DO_PAGES_STAT_CHUNK_NR];\n\n\twhile (nr_pages) {\n\t\tunsigned long chunk_nr = min(nr_pages, DO_PAGES_STAT_CHUNK_NR);\n\n\t\tif (in_compat_syscall()) {\n\t\t\tif (get_compat_pages_array(chunk_pages, pages,\n\t\t\t\t\t\t   chunk_nr))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tif (copy_from_user(chunk_pages, pages,\n\t\t\t\t      chunk_nr * sizeof(*chunk_pages)))\n\t\t\t\tbreak;\n\t\t}\n\n\t\tdo_pages_stat_array(mm, chunk_nr, chunk_pages, chunk_status);\n\n\t\tif (copy_to_user(status, chunk_status, chunk_nr * sizeof(*status)))\n\t\t\tbreak;\n\n\t\tpages += chunk_nr;\n\t\tstatus += chunk_nr;\n\t\tnr_pages -= chunk_nr;\n\t}\n\treturn nr_pages ? -EFAULT : 0;\n}\n\nstatic struct mm_struct *find_mm_struct(pid_t pid, nodemask_t *mem_nodes)\n{\n\tstruct task_struct *task;\n\tstruct mm_struct *mm;\n\n\t \n\tif (!pid) {\n\t\tmmget(current->mm);\n\t\t*mem_nodes = cpuset_mems_allowed(current);\n\t\treturn current->mm;\n\t}\n\n\t \n\trcu_read_lock();\n\ttask = find_task_by_vpid(pid);\n\tif (!task) {\n\t\trcu_read_unlock();\n\t\treturn ERR_PTR(-ESRCH);\n\t}\n\tget_task_struct(task);\n\n\t \n\tif (!ptrace_may_access(task, PTRACE_MODE_READ_REALCREDS)) {\n\t\trcu_read_unlock();\n\t\tmm = ERR_PTR(-EPERM);\n\t\tgoto out;\n\t}\n\trcu_read_unlock();\n\n\tmm = ERR_PTR(security_task_movememory(task));\n\tif (IS_ERR(mm))\n\t\tgoto out;\n\t*mem_nodes = cpuset_mems_allowed(task);\n\tmm = get_task_mm(task);\nout:\n\tput_task_struct(task);\n\tif (!mm)\n\t\tmm = ERR_PTR(-EINVAL);\n\treturn mm;\n}\n\n \nstatic int kernel_move_pages(pid_t pid, unsigned long nr_pages,\n\t\t\t     const void __user * __user *pages,\n\t\t\t     const int __user *nodes,\n\t\t\t     int __user *status, int flags)\n{\n\tstruct mm_struct *mm;\n\tint err;\n\tnodemask_t task_nodes;\n\n\t \n\tif (flags & ~(MPOL_MF_MOVE|MPOL_MF_MOVE_ALL))\n\t\treturn -EINVAL;\n\n\tif ((flags & MPOL_MF_MOVE_ALL) && !capable(CAP_SYS_NICE))\n\t\treturn -EPERM;\n\n\tmm = find_mm_struct(pid, &task_nodes);\n\tif (IS_ERR(mm))\n\t\treturn PTR_ERR(mm);\n\n\tif (nodes)\n\t\terr = do_pages_move(mm, task_nodes, nr_pages, pages,\n\t\t\t\t    nodes, status, flags);\n\telse\n\t\terr = do_pages_stat(mm, nr_pages, pages, status);\n\n\tmmput(mm);\n\treturn err;\n}\n\nSYSCALL_DEFINE6(move_pages, pid_t, pid, unsigned long, nr_pages,\n\t\tconst void __user * __user *, pages,\n\t\tconst int __user *, nodes,\n\t\tint __user *, status, int, flags)\n{\n\treturn kernel_move_pages(pid, nr_pages, pages, nodes, status, flags);\n}\n\n#ifdef CONFIG_NUMA_BALANCING\n \nstatic bool migrate_balanced_pgdat(struct pglist_data *pgdat,\n\t\t\t\t   unsigned long nr_migrate_pages)\n{\n\tint z;\n\n\tfor (z = pgdat->nr_zones - 1; z >= 0; z--) {\n\t\tstruct zone *zone = pgdat->node_zones + z;\n\n\t\tif (!managed_zone(zone))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (!zone_watermark_ok(zone, 0,\n\t\t\t\t       high_wmark_pages(zone) +\n\t\t\t\t       nr_migrate_pages,\n\t\t\t\t       ZONE_MOVABLE, 0))\n\t\t\tcontinue;\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic struct folio *alloc_misplaced_dst_folio(struct folio *src,\n\t\t\t\t\t   unsigned long data)\n{\n\tint nid = (int) data;\n\tint order = folio_order(src);\n\tgfp_t gfp = __GFP_THISNODE;\n\n\tif (order > 0)\n\t\tgfp |= GFP_TRANSHUGE_LIGHT;\n\telse {\n\t\tgfp |= GFP_HIGHUSER_MOVABLE | __GFP_NOMEMALLOC | __GFP_NORETRY |\n\t\t\t__GFP_NOWARN;\n\t\tgfp &= ~__GFP_RECLAIM;\n\t}\n\treturn __folio_alloc_node(gfp, order, nid);\n}\n\nstatic int numamigrate_isolate_page(pg_data_t *pgdat, struct page *page)\n{\n\tint nr_pages = thp_nr_pages(page);\n\tint order = compound_order(page);\n\n\tVM_BUG_ON_PAGE(order && !PageTransHuge(page), page);\n\n\t \n\tif (PageTransHuge(page) && total_mapcount(page) > 1)\n\t\treturn 0;\n\n\t \n\tif (!migrate_balanced_pgdat(pgdat, nr_pages)) {\n\t\tint z;\n\n\t\tif (!(sysctl_numa_balancing_mode & NUMA_BALANCING_MEMORY_TIERING))\n\t\t\treturn 0;\n\t\tfor (z = pgdat->nr_zones - 1; z >= 0; z--) {\n\t\t\tif (managed_zone(pgdat->node_zones + z))\n\t\t\t\tbreak;\n\t\t}\n\t\twakeup_kswapd(pgdat->node_zones + z, 0, order, ZONE_MOVABLE);\n\t\treturn 0;\n\t}\n\n\tif (!isolate_lru_page(page))\n\t\treturn 0;\n\n\tmod_node_page_state(page_pgdat(page), NR_ISOLATED_ANON + page_is_file_lru(page),\n\t\t\t    nr_pages);\n\n\t \n\tput_page(page);\n\treturn 1;\n}\n\n \nint migrate_misplaced_page(struct page *page, struct vm_area_struct *vma,\n\t\t\t   int node)\n{\n\tpg_data_t *pgdat = NODE_DATA(node);\n\tint isolated;\n\tint nr_remaining;\n\tunsigned int nr_succeeded;\n\tLIST_HEAD(migratepages);\n\tint nr_pages = thp_nr_pages(page);\n\n\t \n\tif (page_mapcount(page) != 1 && page_is_file_lru(page) &&\n\t    (vma->vm_flags & VM_EXEC))\n\t\tgoto out;\n\n\t \n\tif (page_is_file_lru(page) && PageDirty(page))\n\t\tgoto out;\n\n\tisolated = numamigrate_isolate_page(pgdat, page);\n\tif (!isolated)\n\t\tgoto out;\n\n\tlist_add(&page->lru, &migratepages);\n\tnr_remaining = migrate_pages(&migratepages, alloc_misplaced_dst_folio,\n\t\t\t\t     NULL, node, MIGRATE_ASYNC,\n\t\t\t\t     MR_NUMA_MISPLACED, &nr_succeeded);\n\tif (nr_remaining) {\n\t\tif (!list_empty(&migratepages)) {\n\t\t\tlist_del(&page->lru);\n\t\t\tmod_node_page_state(page_pgdat(page), NR_ISOLATED_ANON +\n\t\t\t\t\tpage_is_file_lru(page), -nr_pages);\n\t\t\tputback_lru_page(page);\n\t\t}\n\t\tisolated = 0;\n\t}\n\tif (nr_succeeded) {\n\t\tcount_vm_numa_events(NUMA_PAGE_MIGRATE, nr_succeeded);\n\t\tif (!node_is_toptier(page_to_nid(page)) && node_is_toptier(node))\n\t\t\tmod_node_page_state(pgdat, PGPROMOTE_SUCCESS,\n\t\t\t\t\t    nr_succeeded);\n\t}\n\tBUG_ON(!list_empty(&migratepages));\n\treturn isolated;\n\nout:\n\tput_page(page);\n\treturn 0;\n}\n#endif  \n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}