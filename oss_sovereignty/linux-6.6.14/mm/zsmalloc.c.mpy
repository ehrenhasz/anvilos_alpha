{
  "module_name": "zsmalloc.c",
  "hash_id": "ae18b4657f9157719aa2ec3d085115f9d73fd77289066c2c694cbcb60b8397c6",
  "original_prompt": "Ingested from linux-6.6.14/mm/zsmalloc.c",
  "human_readable_source": " \n\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n \n\n#include <linux/module.h>\n#include <linux/kernel.h>\n#include <linux/sched.h>\n#include <linux/bitops.h>\n#include <linux/errno.h>\n#include <linux/highmem.h>\n#include <linux/string.h>\n#include <linux/slab.h>\n#include <linux/pgtable.h>\n#include <asm/tlbflush.h>\n#include <linux/cpumask.h>\n#include <linux/cpu.h>\n#include <linux/vmalloc.h>\n#include <linux/preempt.h>\n#include <linux/spinlock.h>\n#include <linux/shrinker.h>\n#include <linux/types.h>\n#include <linux/debugfs.h>\n#include <linux/zsmalloc.h>\n#include <linux/zpool.h>\n#include <linux/migrate.h>\n#include <linux/wait.h>\n#include <linux/pagemap.h>\n#include <linux/fs.h>\n#include <linux/local_lock.h>\n\n#define ZSPAGE_MAGIC\t0x58\n\n \n#define ZS_ALIGN\t\t8\n\n#define ZS_HANDLE_SIZE (sizeof(unsigned long))\n\n \n\n#ifndef MAX_POSSIBLE_PHYSMEM_BITS\n#ifdef MAX_PHYSMEM_BITS\n#define MAX_POSSIBLE_PHYSMEM_BITS MAX_PHYSMEM_BITS\n#else\n \n#define MAX_POSSIBLE_PHYSMEM_BITS BITS_PER_LONG\n#endif\n#endif\n\n#define _PFN_BITS\t\t(MAX_POSSIBLE_PHYSMEM_BITS - PAGE_SHIFT)\n\n \n#define OBJ_ALLOCATED_TAG 1\n\n#define OBJ_TAG_BITS\t1\n#define OBJ_TAG_MASK\tOBJ_ALLOCATED_TAG\n\n#define OBJ_INDEX_BITS\t(BITS_PER_LONG - _PFN_BITS - OBJ_TAG_BITS)\n#define OBJ_INDEX_MASK\t((_AC(1, UL) << OBJ_INDEX_BITS) - 1)\n\n#define HUGE_BITS\t1\n#define FULLNESS_BITS\t4\n#define CLASS_BITS\t8\n#define ISOLATED_BITS\t5\n#define MAGIC_VAL_BITS\t8\n\n#define MAX(a, b) ((a) >= (b) ? (a) : (b))\n\n#define ZS_MAX_PAGES_PER_ZSPAGE\t(_AC(CONFIG_ZSMALLOC_CHAIN_SIZE, UL))\n\n \n#define ZS_MIN_ALLOC_SIZE \\\n\tMAX(32, (ZS_MAX_PAGES_PER_ZSPAGE << PAGE_SHIFT >> OBJ_INDEX_BITS))\n \n#define ZS_MAX_ALLOC_SIZE\tPAGE_SIZE\n\n \n#define ZS_SIZE_CLASS_DELTA\t(PAGE_SIZE >> CLASS_BITS)\n#define ZS_SIZE_CLASSES\t(DIV_ROUND_UP(ZS_MAX_ALLOC_SIZE - ZS_MIN_ALLOC_SIZE, \\\n\t\t\t\t      ZS_SIZE_CLASS_DELTA) + 1)\n\n \nenum fullness_group {\n\tZS_INUSE_RATIO_0,\n\tZS_INUSE_RATIO_10,\n\t \n\tZS_INUSE_RATIO_99       = 10,\n\tZS_INUSE_RATIO_100,\n\tNR_FULLNESS_GROUPS,\n};\n\nenum class_stat_type {\n\t \n\tZS_OBJS_ALLOCATED       = NR_FULLNESS_GROUPS,\n\tZS_OBJS_INUSE,\n\tNR_CLASS_STAT_TYPES,\n};\n\nstruct zs_size_stat {\n\tunsigned long objs[NR_CLASS_STAT_TYPES];\n};\n\n#ifdef CONFIG_ZSMALLOC_STAT\nstatic struct dentry *zs_stat_root;\n#endif\n\nstatic size_t huge_class_size;\n\nstruct size_class {\n\tstruct list_head fullness_list[NR_FULLNESS_GROUPS];\n\t \n\tint size;\n\tint objs_per_zspage;\n\t \n\tint pages_per_zspage;\n\n\tunsigned int index;\n\tstruct zs_size_stat stats;\n};\n\n \nstruct link_free {\n\tunion {\n\t\t \n\t\tunsigned long next;\n\t\t \n\t\tunsigned long handle;\n\t};\n};\n\nstruct zs_pool {\n\tconst char *name;\n\n\tstruct size_class *size_class[ZS_SIZE_CLASSES];\n\tstruct kmem_cache *handle_cachep;\n\tstruct kmem_cache *zspage_cachep;\n\n\tatomic_long_t pages_allocated;\n\n\tstruct zs_pool_stats stats;\n\n\t \n\tstruct shrinker shrinker;\n\n#ifdef CONFIG_ZSMALLOC_STAT\n\tstruct dentry *stat_dentry;\n#endif\n#ifdef CONFIG_COMPACTION\n\tstruct work_struct free_work;\n#endif\n\tspinlock_t lock;\n\tatomic_t compaction_in_progress;\n};\n\nstruct zspage {\n\tstruct {\n\t\tunsigned int huge:HUGE_BITS;\n\t\tunsigned int fullness:FULLNESS_BITS;\n\t\tunsigned int class:CLASS_BITS + 1;\n\t\tunsigned int isolated:ISOLATED_BITS;\n\t\tunsigned int magic:MAGIC_VAL_BITS;\n\t};\n\tunsigned int inuse;\n\tunsigned int freeobj;\n\tstruct page *first_page;\n\tstruct list_head list;  \n\tstruct zs_pool *pool;\n\trwlock_t lock;\n};\n\nstruct mapping_area {\n\tlocal_lock_t lock;\n\tchar *vm_buf;  \n\tchar *vm_addr;  \n\tenum zs_mapmode vm_mm;  \n};\n\n \nstatic void SetZsHugePage(struct zspage *zspage)\n{\n\tzspage->huge = 1;\n}\n\nstatic bool ZsHugePage(struct zspage *zspage)\n{\n\treturn zspage->huge;\n}\n\nstatic void migrate_lock_init(struct zspage *zspage);\nstatic void migrate_read_lock(struct zspage *zspage);\nstatic void migrate_read_unlock(struct zspage *zspage);\n\n#ifdef CONFIG_COMPACTION\nstatic void migrate_write_lock(struct zspage *zspage);\nstatic void migrate_write_lock_nested(struct zspage *zspage);\nstatic void migrate_write_unlock(struct zspage *zspage);\nstatic void kick_deferred_free(struct zs_pool *pool);\nstatic void init_deferred_free(struct zs_pool *pool);\nstatic void SetZsPageMovable(struct zs_pool *pool, struct zspage *zspage);\n#else\nstatic void migrate_write_lock(struct zspage *zspage) {}\nstatic void migrate_write_lock_nested(struct zspage *zspage) {}\nstatic void migrate_write_unlock(struct zspage *zspage) {}\nstatic void kick_deferred_free(struct zs_pool *pool) {}\nstatic void init_deferred_free(struct zs_pool *pool) {}\nstatic void SetZsPageMovable(struct zs_pool *pool, struct zspage *zspage) {}\n#endif\n\nstatic int create_cache(struct zs_pool *pool)\n{\n\tpool->handle_cachep = kmem_cache_create(\"zs_handle\", ZS_HANDLE_SIZE,\n\t\t\t\t\t0, 0, NULL);\n\tif (!pool->handle_cachep)\n\t\treturn 1;\n\n\tpool->zspage_cachep = kmem_cache_create(\"zspage\", sizeof(struct zspage),\n\t\t\t\t\t0, 0, NULL);\n\tif (!pool->zspage_cachep) {\n\t\tkmem_cache_destroy(pool->handle_cachep);\n\t\tpool->handle_cachep = NULL;\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic void destroy_cache(struct zs_pool *pool)\n{\n\tkmem_cache_destroy(pool->handle_cachep);\n\tkmem_cache_destroy(pool->zspage_cachep);\n}\n\nstatic unsigned long cache_alloc_handle(struct zs_pool *pool, gfp_t gfp)\n{\n\treturn (unsigned long)kmem_cache_alloc(pool->handle_cachep,\n\t\t\tgfp & ~(__GFP_HIGHMEM|__GFP_MOVABLE));\n}\n\nstatic void cache_free_handle(struct zs_pool *pool, unsigned long handle)\n{\n\tkmem_cache_free(pool->handle_cachep, (void *)handle);\n}\n\nstatic struct zspage *cache_alloc_zspage(struct zs_pool *pool, gfp_t flags)\n{\n\treturn kmem_cache_zalloc(pool->zspage_cachep,\n\t\t\tflags & ~(__GFP_HIGHMEM|__GFP_MOVABLE));\n}\n\nstatic void cache_free_zspage(struct zs_pool *pool, struct zspage *zspage)\n{\n\tkmem_cache_free(pool->zspage_cachep, zspage);\n}\n\n \nstatic void record_obj(unsigned long handle, unsigned long obj)\n{\n\t*(unsigned long *)handle = obj;\n}\n\n \n\n#ifdef CONFIG_ZPOOL\n\nstatic void *zs_zpool_create(const char *name, gfp_t gfp)\n{\n\t \n\treturn zs_create_pool(name);\n}\n\nstatic void zs_zpool_destroy(void *pool)\n{\n\tzs_destroy_pool(pool);\n}\n\nstatic int zs_zpool_malloc(void *pool, size_t size, gfp_t gfp,\n\t\t\tunsigned long *handle)\n{\n\t*handle = zs_malloc(pool, size, gfp);\n\n\tif (IS_ERR_VALUE(*handle))\n\t\treturn PTR_ERR((void *)*handle);\n\treturn 0;\n}\nstatic void zs_zpool_free(void *pool, unsigned long handle)\n{\n\tzs_free(pool, handle);\n}\n\nstatic void *zs_zpool_map(void *pool, unsigned long handle,\n\t\t\tenum zpool_mapmode mm)\n{\n\tenum zs_mapmode zs_mm;\n\n\tswitch (mm) {\n\tcase ZPOOL_MM_RO:\n\t\tzs_mm = ZS_MM_RO;\n\t\tbreak;\n\tcase ZPOOL_MM_WO:\n\t\tzs_mm = ZS_MM_WO;\n\t\tbreak;\n\tcase ZPOOL_MM_RW:\n\tdefault:\n\t\tzs_mm = ZS_MM_RW;\n\t\tbreak;\n\t}\n\n\treturn zs_map_object(pool, handle, zs_mm);\n}\nstatic void zs_zpool_unmap(void *pool, unsigned long handle)\n{\n\tzs_unmap_object(pool, handle);\n}\n\nstatic u64 zs_zpool_total_size(void *pool)\n{\n\treturn zs_get_total_pages(pool) << PAGE_SHIFT;\n}\n\nstatic struct zpool_driver zs_zpool_driver = {\n\t.type =\t\t\t  \"zsmalloc\",\n\t.owner =\t\t  THIS_MODULE,\n\t.create =\t\t  zs_zpool_create,\n\t.destroy =\t\t  zs_zpool_destroy,\n\t.malloc_support_movable = true,\n\t.malloc =\t\t  zs_zpool_malloc,\n\t.free =\t\t\t  zs_zpool_free,\n\t.map =\t\t\t  zs_zpool_map,\n\t.unmap =\t\t  zs_zpool_unmap,\n\t.total_size =\t\t  zs_zpool_total_size,\n};\n\nMODULE_ALIAS(\"zpool-zsmalloc\");\n#endif  \n\n \nstatic DEFINE_PER_CPU(struct mapping_area, zs_map_area) = {\n\t.lock\t= INIT_LOCAL_LOCK(lock),\n};\n\nstatic __maybe_unused int is_first_page(struct page *page)\n{\n\treturn PagePrivate(page);\n}\n\n \nstatic inline int get_zspage_inuse(struct zspage *zspage)\n{\n\treturn zspage->inuse;\n}\n\n\nstatic inline void mod_zspage_inuse(struct zspage *zspage, int val)\n{\n\tzspage->inuse += val;\n}\n\nstatic inline struct page *get_first_page(struct zspage *zspage)\n{\n\tstruct page *first_page = zspage->first_page;\n\n\tVM_BUG_ON_PAGE(!is_first_page(first_page), first_page);\n\treturn first_page;\n}\n\nstatic inline unsigned int get_first_obj_offset(struct page *page)\n{\n\treturn page->page_type;\n}\n\nstatic inline void set_first_obj_offset(struct page *page, unsigned int offset)\n{\n\tpage->page_type = offset;\n}\n\nstatic inline unsigned int get_freeobj(struct zspage *zspage)\n{\n\treturn zspage->freeobj;\n}\n\nstatic inline void set_freeobj(struct zspage *zspage, unsigned int obj)\n{\n\tzspage->freeobj = obj;\n}\n\nstatic void get_zspage_mapping(struct zspage *zspage,\n\t\t\t       unsigned int *class_idx,\n\t\t\t       int *fullness)\n{\n\tBUG_ON(zspage->magic != ZSPAGE_MAGIC);\n\n\t*fullness = zspage->fullness;\n\t*class_idx = zspage->class;\n}\n\nstatic struct size_class *zspage_class(struct zs_pool *pool,\n\t\t\t\t       struct zspage *zspage)\n{\n\treturn pool->size_class[zspage->class];\n}\n\nstatic void set_zspage_mapping(struct zspage *zspage,\n\t\t\t       unsigned int class_idx,\n\t\t\t       int fullness)\n{\n\tzspage->class = class_idx;\n\tzspage->fullness = fullness;\n}\n\n \nstatic int get_size_class_index(int size)\n{\n\tint idx = 0;\n\n\tif (likely(size > ZS_MIN_ALLOC_SIZE))\n\t\tidx = DIV_ROUND_UP(size - ZS_MIN_ALLOC_SIZE,\n\t\t\t\tZS_SIZE_CLASS_DELTA);\n\n\treturn min_t(int, ZS_SIZE_CLASSES - 1, idx);\n}\n\nstatic inline void class_stat_inc(struct size_class *class,\n\t\t\t\tint type, unsigned long cnt)\n{\n\tclass->stats.objs[type] += cnt;\n}\n\nstatic inline void class_stat_dec(struct size_class *class,\n\t\t\t\tint type, unsigned long cnt)\n{\n\tclass->stats.objs[type] -= cnt;\n}\n\nstatic inline unsigned long zs_stat_get(struct size_class *class, int type)\n{\n\treturn class->stats.objs[type];\n}\n\n#ifdef CONFIG_ZSMALLOC_STAT\n\nstatic void __init zs_stat_init(void)\n{\n\tif (!debugfs_initialized()) {\n\t\tpr_warn(\"debugfs not available, stat dir not created\\n\");\n\t\treturn;\n\t}\n\n\tzs_stat_root = debugfs_create_dir(\"zsmalloc\", NULL);\n}\n\nstatic void __exit zs_stat_exit(void)\n{\n\tdebugfs_remove_recursive(zs_stat_root);\n}\n\nstatic unsigned long zs_can_compact(struct size_class *class);\n\nstatic int zs_stats_size_show(struct seq_file *s, void *v)\n{\n\tint i, fg;\n\tstruct zs_pool *pool = s->private;\n\tstruct size_class *class;\n\tint objs_per_zspage;\n\tunsigned long obj_allocated, obj_used, pages_used, freeable;\n\tunsigned long total_objs = 0, total_used_objs = 0, total_pages = 0;\n\tunsigned long total_freeable = 0;\n\tunsigned long inuse_totals[NR_FULLNESS_GROUPS] = {0, };\n\n\tseq_printf(s, \" %5s %5s %9s %9s %9s %9s %9s %9s %9s %9s %9s %9s %9s %13s %10s %10s %16s %8s\\n\",\n\t\t\t\"class\", \"size\", \"10%\", \"20%\", \"30%\", \"40%\",\n\t\t\t\"50%\", \"60%\", \"70%\", \"80%\", \"90%\", \"99%\", \"100%\",\n\t\t\t\"obj_allocated\", \"obj_used\", \"pages_used\",\n\t\t\t\"pages_per_zspage\", \"freeable\");\n\n\tfor (i = 0; i < ZS_SIZE_CLASSES; i++) {\n\n\t\tclass = pool->size_class[i];\n\n\t\tif (class->index != i)\n\t\t\tcontinue;\n\n\t\tspin_lock(&pool->lock);\n\n\t\tseq_printf(s, \" %5u %5u \", i, class->size);\n\t\tfor (fg = ZS_INUSE_RATIO_10; fg < NR_FULLNESS_GROUPS; fg++) {\n\t\t\tinuse_totals[fg] += zs_stat_get(class, fg);\n\t\t\tseq_printf(s, \"%9lu \", zs_stat_get(class, fg));\n\t\t}\n\n\t\tobj_allocated = zs_stat_get(class, ZS_OBJS_ALLOCATED);\n\t\tobj_used = zs_stat_get(class, ZS_OBJS_INUSE);\n\t\tfreeable = zs_can_compact(class);\n\t\tspin_unlock(&pool->lock);\n\n\t\tobjs_per_zspage = class->objs_per_zspage;\n\t\tpages_used = obj_allocated / objs_per_zspage *\n\t\t\t\tclass->pages_per_zspage;\n\n\t\tseq_printf(s, \"%13lu %10lu %10lu %16d %8lu\\n\",\n\t\t\t   obj_allocated, obj_used, pages_used,\n\t\t\t   class->pages_per_zspage, freeable);\n\n\t\ttotal_objs += obj_allocated;\n\t\ttotal_used_objs += obj_used;\n\t\ttotal_pages += pages_used;\n\t\ttotal_freeable += freeable;\n\t}\n\n\tseq_puts(s, \"\\n\");\n\tseq_printf(s, \" %5s %5s \", \"Total\", \"\");\n\n\tfor (fg = ZS_INUSE_RATIO_10; fg < NR_FULLNESS_GROUPS; fg++)\n\t\tseq_printf(s, \"%9lu \", inuse_totals[fg]);\n\n\tseq_printf(s, \"%13lu %10lu %10lu %16s %8lu\\n\",\n\t\t   total_objs, total_used_objs, total_pages, \"\",\n\t\t   total_freeable);\n\n\treturn 0;\n}\nDEFINE_SHOW_ATTRIBUTE(zs_stats_size);\n\nstatic void zs_pool_stat_create(struct zs_pool *pool, const char *name)\n{\n\tif (!zs_stat_root) {\n\t\tpr_warn(\"no root stat dir, not creating <%s> stat dir\\n\", name);\n\t\treturn;\n\t}\n\n\tpool->stat_dentry = debugfs_create_dir(name, zs_stat_root);\n\n\tdebugfs_create_file(\"classes\", S_IFREG | 0444, pool->stat_dentry, pool,\n\t\t\t    &zs_stats_size_fops);\n}\n\nstatic void zs_pool_stat_destroy(struct zs_pool *pool)\n{\n\tdebugfs_remove_recursive(pool->stat_dentry);\n}\n\n#else  \nstatic void __init zs_stat_init(void)\n{\n}\n\nstatic void __exit zs_stat_exit(void)\n{\n}\n\nstatic inline void zs_pool_stat_create(struct zs_pool *pool, const char *name)\n{\n}\n\nstatic inline void zs_pool_stat_destroy(struct zs_pool *pool)\n{\n}\n#endif\n\n\n \nstatic int get_fullness_group(struct size_class *class, struct zspage *zspage)\n{\n\tint inuse, objs_per_zspage, ratio;\n\n\tinuse = get_zspage_inuse(zspage);\n\tobjs_per_zspage = class->objs_per_zspage;\n\n\tif (inuse == 0)\n\t\treturn ZS_INUSE_RATIO_0;\n\tif (inuse == objs_per_zspage)\n\t\treturn ZS_INUSE_RATIO_100;\n\n\tratio = 100 * inuse / objs_per_zspage;\n\t \n\treturn ratio / 10 + 1;\n}\n\n \nstatic void insert_zspage(struct size_class *class,\n\t\t\t\tstruct zspage *zspage,\n\t\t\t\tint fullness)\n{\n\tclass_stat_inc(class, fullness, 1);\n\tlist_add(&zspage->list, &class->fullness_list[fullness]);\n}\n\n \nstatic void remove_zspage(struct size_class *class,\n\t\t\t\tstruct zspage *zspage,\n\t\t\t\tint fullness)\n{\n\tVM_BUG_ON(list_empty(&class->fullness_list[fullness]));\n\n\tlist_del_init(&zspage->list);\n\tclass_stat_dec(class, fullness, 1);\n}\n\n \nstatic int fix_fullness_group(struct size_class *class, struct zspage *zspage)\n{\n\tint class_idx;\n\tint currfg, newfg;\n\n\tget_zspage_mapping(zspage, &class_idx, &currfg);\n\tnewfg = get_fullness_group(class, zspage);\n\tif (newfg == currfg)\n\t\tgoto out;\n\n\tremove_zspage(class, zspage, currfg);\n\tinsert_zspage(class, zspage, newfg);\n\tset_zspage_mapping(zspage, class_idx, newfg);\nout:\n\treturn newfg;\n}\n\nstatic struct zspage *get_zspage(struct page *page)\n{\n\tstruct zspage *zspage = (struct zspage *)page_private(page);\n\n\tBUG_ON(zspage->magic != ZSPAGE_MAGIC);\n\treturn zspage;\n}\n\nstatic struct page *get_next_page(struct page *page)\n{\n\tstruct zspage *zspage = get_zspage(page);\n\n\tif (unlikely(ZsHugePage(zspage)))\n\t\treturn NULL;\n\n\treturn (struct page *)page->index;\n}\n\n \nstatic void obj_to_location(unsigned long obj, struct page **page,\n\t\t\t\tunsigned int *obj_idx)\n{\n\tobj >>= OBJ_TAG_BITS;\n\t*page = pfn_to_page(obj >> OBJ_INDEX_BITS);\n\t*obj_idx = (obj & OBJ_INDEX_MASK);\n}\n\nstatic void obj_to_page(unsigned long obj, struct page **page)\n{\n\tobj >>= OBJ_TAG_BITS;\n\t*page = pfn_to_page(obj >> OBJ_INDEX_BITS);\n}\n\n \nstatic unsigned long location_to_obj(struct page *page, unsigned int obj_idx)\n{\n\tunsigned long obj;\n\n\tobj = page_to_pfn(page) << OBJ_INDEX_BITS;\n\tobj |= obj_idx & OBJ_INDEX_MASK;\n\tobj <<= OBJ_TAG_BITS;\n\n\treturn obj;\n}\n\nstatic unsigned long handle_to_obj(unsigned long handle)\n{\n\treturn *(unsigned long *)handle;\n}\n\nstatic inline bool obj_allocated(struct page *page, void *obj,\n\t\t\t\t unsigned long *phandle)\n{\n\tunsigned long handle;\n\tstruct zspage *zspage = get_zspage(page);\n\n\tif (unlikely(ZsHugePage(zspage))) {\n\t\tVM_BUG_ON_PAGE(!is_first_page(page), page);\n\t\thandle = page->index;\n\t} else\n\t\thandle = *(unsigned long *)obj;\n\n\tif (!(handle & OBJ_ALLOCATED_TAG))\n\t\treturn false;\n\n\t \n\t*phandle = handle & ~OBJ_TAG_MASK;\n\treturn true;\n}\n\nstatic void reset_page(struct page *page)\n{\n\t__ClearPageMovable(page);\n\tClearPagePrivate(page);\n\tset_page_private(page, 0);\n\tpage_mapcount_reset(page);\n\tpage->index = 0;\n}\n\nstatic int trylock_zspage(struct zspage *zspage)\n{\n\tstruct page *cursor, *fail;\n\n\tfor (cursor = get_first_page(zspage); cursor != NULL; cursor =\n\t\t\t\t\tget_next_page(cursor)) {\n\t\tif (!trylock_page(cursor)) {\n\t\t\tfail = cursor;\n\t\t\tgoto unlock;\n\t\t}\n\t}\n\n\treturn 1;\nunlock:\n\tfor (cursor = get_first_page(zspage); cursor != fail; cursor =\n\t\t\t\t\tget_next_page(cursor))\n\t\tunlock_page(cursor);\n\n\treturn 0;\n}\n\nstatic void __free_zspage(struct zs_pool *pool, struct size_class *class,\n\t\t\t\tstruct zspage *zspage)\n{\n\tstruct page *page, *next;\n\tint fg;\n\tunsigned int class_idx;\n\n\tget_zspage_mapping(zspage, &class_idx, &fg);\n\n\tassert_spin_locked(&pool->lock);\n\n\tVM_BUG_ON(get_zspage_inuse(zspage));\n\tVM_BUG_ON(fg != ZS_INUSE_RATIO_0);\n\n\tnext = page = get_first_page(zspage);\n\tdo {\n\t\tVM_BUG_ON_PAGE(!PageLocked(page), page);\n\t\tnext = get_next_page(page);\n\t\treset_page(page);\n\t\tunlock_page(page);\n\t\tdec_zone_page_state(page, NR_ZSPAGES);\n\t\tput_page(page);\n\t\tpage = next;\n\t} while (page != NULL);\n\n\tcache_free_zspage(pool, zspage);\n\n\tclass_stat_dec(class, ZS_OBJS_ALLOCATED, class->objs_per_zspage);\n\tatomic_long_sub(class->pages_per_zspage, &pool->pages_allocated);\n}\n\nstatic void free_zspage(struct zs_pool *pool, struct size_class *class,\n\t\t\t\tstruct zspage *zspage)\n{\n\tVM_BUG_ON(get_zspage_inuse(zspage));\n\tVM_BUG_ON(list_empty(&zspage->list));\n\n\t \n\tif (!trylock_zspage(zspage)) {\n\t\tkick_deferred_free(pool);\n\t\treturn;\n\t}\n\n\tremove_zspage(class, zspage, ZS_INUSE_RATIO_0);\n\t__free_zspage(pool, class, zspage);\n}\n\n \nstatic void init_zspage(struct size_class *class, struct zspage *zspage)\n{\n\tunsigned int freeobj = 1;\n\tunsigned long off = 0;\n\tstruct page *page = get_first_page(zspage);\n\n\twhile (page) {\n\t\tstruct page *next_page;\n\t\tstruct link_free *link;\n\t\tvoid *vaddr;\n\n\t\tset_first_obj_offset(page, off);\n\n\t\tvaddr = kmap_atomic(page);\n\t\tlink = (struct link_free *)vaddr + off / sizeof(*link);\n\n\t\twhile ((off += class->size) < PAGE_SIZE) {\n\t\t\tlink->next = freeobj++ << OBJ_TAG_BITS;\n\t\t\tlink += class->size / sizeof(*link);\n\t\t}\n\n\t\t \n\t\tnext_page = get_next_page(page);\n\t\tif (next_page) {\n\t\t\tlink->next = freeobj++ << OBJ_TAG_BITS;\n\t\t} else {\n\t\t\t \n\t\t\tlink->next = -1UL << OBJ_TAG_BITS;\n\t\t}\n\t\tkunmap_atomic(vaddr);\n\t\tpage = next_page;\n\t\toff %= PAGE_SIZE;\n\t}\n\n\tset_freeobj(zspage, 0);\n}\n\nstatic void create_page_chain(struct size_class *class, struct zspage *zspage,\n\t\t\t\tstruct page *pages[])\n{\n\tint i;\n\tstruct page *page;\n\tstruct page *prev_page = NULL;\n\tint nr_pages = class->pages_per_zspage;\n\n\t \n\tfor (i = 0; i < nr_pages; i++) {\n\t\tpage = pages[i];\n\t\tset_page_private(page, (unsigned long)zspage);\n\t\tpage->index = 0;\n\t\tif (i == 0) {\n\t\t\tzspage->first_page = page;\n\t\t\tSetPagePrivate(page);\n\t\t\tif (unlikely(class->objs_per_zspage == 1 &&\n\t\t\t\t\tclass->pages_per_zspage == 1))\n\t\t\t\tSetZsHugePage(zspage);\n\t\t} else {\n\t\t\tprev_page->index = (unsigned long)page;\n\t\t}\n\t\tprev_page = page;\n\t}\n}\n\n \nstatic struct zspage *alloc_zspage(struct zs_pool *pool,\n\t\t\t\t\tstruct size_class *class,\n\t\t\t\t\tgfp_t gfp)\n{\n\tint i;\n\tstruct page *pages[ZS_MAX_PAGES_PER_ZSPAGE];\n\tstruct zspage *zspage = cache_alloc_zspage(pool, gfp);\n\n\tif (!zspage)\n\t\treturn NULL;\n\n\tzspage->magic = ZSPAGE_MAGIC;\n\tmigrate_lock_init(zspage);\n\n\tfor (i = 0; i < class->pages_per_zspage; i++) {\n\t\tstruct page *page;\n\n\t\tpage = alloc_page(gfp);\n\t\tif (!page) {\n\t\t\twhile (--i >= 0) {\n\t\t\t\tdec_zone_page_state(pages[i], NR_ZSPAGES);\n\t\t\t\t__free_page(pages[i]);\n\t\t\t}\n\t\t\tcache_free_zspage(pool, zspage);\n\t\t\treturn NULL;\n\t\t}\n\n\t\tinc_zone_page_state(page, NR_ZSPAGES);\n\t\tpages[i] = page;\n\t}\n\n\tcreate_page_chain(class, zspage, pages);\n\tinit_zspage(class, zspage);\n\tzspage->pool = pool;\n\n\treturn zspage;\n}\n\nstatic struct zspage *find_get_zspage(struct size_class *class)\n{\n\tint i;\n\tstruct zspage *zspage;\n\n\tfor (i = ZS_INUSE_RATIO_99; i >= ZS_INUSE_RATIO_0; i--) {\n\t\tzspage = list_first_entry_or_null(&class->fullness_list[i],\n\t\t\t\t\t\t  struct zspage, list);\n\t\tif (zspage)\n\t\t\tbreak;\n\t}\n\n\treturn zspage;\n}\n\nstatic inline int __zs_cpu_up(struct mapping_area *area)\n{\n\t \n\tif (area->vm_buf)\n\t\treturn 0;\n\tarea->vm_buf = kmalloc(ZS_MAX_ALLOC_SIZE, GFP_KERNEL);\n\tif (!area->vm_buf)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nstatic inline void __zs_cpu_down(struct mapping_area *area)\n{\n\tkfree(area->vm_buf);\n\tarea->vm_buf = NULL;\n}\n\nstatic void *__zs_map_object(struct mapping_area *area,\n\t\t\tstruct page *pages[2], int off, int size)\n{\n\tint sizes[2];\n\tvoid *addr;\n\tchar *buf = area->vm_buf;\n\n\t \n\tpagefault_disable();\n\n\t \n\tif (area->vm_mm == ZS_MM_WO)\n\t\tgoto out;\n\n\tsizes[0] = PAGE_SIZE - off;\n\tsizes[1] = size - sizes[0];\n\n\t \n\taddr = kmap_atomic(pages[0]);\n\tmemcpy(buf, addr + off, sizes[0]);\n\tkunmap_atomic(addr);\n\taddr = kmap_atomic(pages[1]);\n\tmemcpy(buf + sizes[0], addr, sizes[1]);\n\tkunmap_atomic(addr);\nout:\n\treturn area->vm_buf;\n}\n\nstatic void __zs_unmap_object(struct mapping_area *area,\n\t\t\tstruct page *pages[2], int off, int size)\n{\n\tint sizes[2];\n\tvoid *addr;\n\tchar *buf;\n\n\t \n\tif (area->vm_mm == ZS_MM_RO)\n\t\tgoto out;\n\n\tbuf = area->vm_buf;\n\tbuf = buf + ZS_HANDLE_SIZE;\n\tsize -= ZS_HANDLE_SIZE;\n\toff += ZS_HANDLE_SIZE;\n\n\tsizes[0] = PAGE_SIZE - off;\n\tsizes[1] = size - sizes[0];\n\n\t \n\taddr = kmap_atomic(pages[0]);\n\tmemcpy(addr + off, buf, sizes[0]);\n\tkunmap_atomic(addr);\n\taddr = kmap_atomic(pages[1]);\n\tmemcpy(addr, buf + sizes[0], sizes[1]);\n\tkunmap_atomic(addr);\n\nout:\n\t \n\tpagefault_enable();\n}\n\nstatic int zs_cpu_prepare(unsigned int cpu)\n{\n\tstruct mapping_area *area;\n\n\tarea = &per_cpu(zs_map_area, cpu);\n\treturn __zs_cpu_up(area);\n}\n\nstatic int zs_cpu_dead(unsigned int cpu)\n{\n\tstruct mapping_area *area;\n\n\tarea = &per_cpu(zs_map_area, cpu);\n\t__zs_cpu_down(area);\n\treturn 0;\n}\n\nstatic bool can_merge(struct size_class *prev, int pages_per_zspage,\n\t\t\t\t\tint objs_per_zspage)\n{\n\tif (prev->pages_per_zspage == pages_per_zspage &&\n\t\tprev->objs_per_zspage == objs_per_zspage)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic bool zspage_full(struct size_class *class, struct zspage *zspage)\n{\n\treturn get_zspage_inuse(zspage) == class->objs_per_zspage;\n}\n\nstatic bool zspage_empty(struct zspage *zspage)\n{\n\treturn get_zspage_inuse(zspage) == 0;\n}\n\n \nunsigned int zs_lookup_class_index(struct zs_pool *pool, unsigned int size)\n{\n\tstruct size_class *class;\n\n\tclass = pool->size_class[get_size_class_index(size)];\n\n\treturn class->index;\n}\nEXPORT_SYMBOL_GPL(zs_lookup_class_index);\n\nunsigned long zs_get_total_pages(struct zs_pool *pool)\n{\n\treturn atomic_long_read(&pool->pages_allocated);\n}\nEXPORT_SYMBOL_GPL(zs_get_total_pages);\n\n \nvoid *zs_map_object(struct zs_pool *pool, unsigned long handle,\n\t\t\tenum zs_mapmode mm)\n{\n\tstruct zspage *zspage;\n\tstruct page *page;\n\tunsigned long obj, off;\n\tunsigned int obj_idx;\n\n\tstruct size_class *class;\n\tstruct mapping_area *area;\n\tstruct page *pages[2];\n\tvoid *ret;\n\n\t \n\tBUG_ON(in_interrupt());\n\n\t \n\tspin_lock(&pool->lock);\n\tobj = handle_to_obj(handle);\n\tobj_to_location(obj, &page, &obj_idx);\n\tzspage = get_zspage(page);\n\n\t \n\tmigrate_read_lock(zspage);\n\tspin_unlock(&pool->lock);\n\n\tclass = zspage_class(pool, zspage);\n\toff = offset_in_page(class->size * obj_idx);\n\n\tlocal_lock(&zs_map_area.lock);\n\tarea = this_cpu_ptr(&zs_map_area);\n\tarea->vm_mm = mm;\n\tif (off + class->size <= PAGE_SIZE) {\n\t\t \n\t\tarea->vm_addr = kmap_atomic(page);\n\t\tret = area->vm_addr + off;\n\t\tgoto out;\n\t}\n\n\t \n\tpages[0] = page;\n\tpages[1] = get_next_page(page);\n\tBUG_ON(!pages[1]);\n\n\tret = __zs_map_object(area, pages, off, class->size);\nout:\n\tif (likely(!ZsHugePage(zspage)))\n\t\tret += ZS_HANDLE_SIZE;\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(zs_map_object);\n\nvoid zs_unmap_object(struct zs_pool *pool, unsigned long handle)\n{\n\tstruct zspage *zspage;\n\tstruct page *page;\n\tunsigned long obj, off;\n\tunsigned int obj_idx;\n\n\tstruct size_class *class;\n\tstruct mapping_area *area;\n\n\tobj = handle_to_obj(handle);\n\tobj_to_location(obj, &page, &obj_idx);\n\tzspage = get_zspage(page);\n\tclass = zspage_class(pool, zspage);\n\toff = offset_in_page(class->size * obj_idx);\n\n\tarea = this_cpu_ptr(&zs_map_area);\n\tif (off + class->size <= PAGE_SIZE)\n\t\tkunmap_atomic(area->vm_addr);\n\telse {\n\t\tstruct page *pages[2];\n\n\t\tpages[0] = page;\n\t\tpages[1] = get_next_page(page);\n\t\tBUG_ON(!pages[1]);\n\n\t\t__zs_unmap_object(area, pages, off, class->size);\n\t}\n\tlocal_unlock(&zs_map_area.lock);\n\n\tmigrate_read_unlock(zspage);\n}\nEXPORT_SYMBOL_GPL(zs_unmap_object);\n\n \nsize_t zs_huge_class_size(struct zs_pool *pool)\n{\n\treturn huge_class_size;\n}\nEXPORT_SYMBOL_GPL(zs_huge_class_size);\n\nstatic unsigned long obj_malloc(struct zs_pool *pool,\n\t\t\t\tstruct zspage *zspage, unsigned long handle)\n{\n\tint i, nr_page, offset;\n\tunsigned long obj;\n\tstruct link_free *link;\n\tstruct size_class *class;\n\n\tstruct page *m_page;\n\tunsigned long m_offset;\n\tvoid *vaddr;\n\n\tclass = pool->size_class[zspage->class];\n\thandle |= OBJ_ALLOCATED_TAG;\n\tobj = get_freeobj(zspage);\n\n\toffset = obj * class->size;\n\tnr_page = offset >> PAGE_SHIFT;\n\tm_offset = offset_in_page(offset);\n\tm_page = get_first_page(zspage);\n\n\tfor (i = 0; i < nr_page; i++)\n\t\tm_page = get_next_page(m_page);\n\n\tvaddr = kmap_atomic(m_page);\n\tlink = (struct link_free *)vaddr + m_offset / sizeof(*link);\n\tset_freeobj(zspage, link->next >> OBJ_TAG_BITS);\n\tif (likely(!ZsHugePage(zspage)))\n\t\t \n\t\tlink->handle = handle;\n\telse\n\t\t \n\t\tzspage->first_page->index = handle;\n\n\tkunmap_atomic(vaddr);\n\tmod_zspage_inuse(zspage, 1);\n\n\tobj = location_to_obj(m_page, obj);\n\n\treturn obj;\n}\n\n\n \nunsigned long zs_malloc(struct zs_pool *pool, size_t size, gfp_t gfp)\n{\n\tunsigned long handle, obj;\n\tstruct size_class *class;\n\tint newfg;\n\tstruct zspage *zspage;\n\n\tif (unlikely(!size || size > ZS_MAX_ALLOC_SIZE))\n\t\treturn (unsigned long)ERR_PTR(-EINVAL);\n\n\thandle = cache_alloc_handle(pool, gfp);\n\tif (!handle)\n\t\treturn (unsigned long)ERR_PTR(-ENOMEM);\n\n\t \n\tsize += ZS_HANDLE_SIZE;\n\tclass = pool->size_class[get_size_class_index(size)];\n\n\t \n\tspin_lock(&pool->lock);\n\tzspage = find_get_zspage(class);\n\tif (likely(zspage)) {\n\t\tobj = obj_malloc(pool, zspage, handle);\n\t\t \n\t\tfix_fullness_group(class, zspage);\n\t\trecord_obj(handle, obj);\n\t\tclass_stat_inc(class, ZS_OBJS_INUSE, 1);\n\n\t\tgoto out;\n\t}\n\n\tspin_unlock(&pool->lock);\n\n\tzspage = alloc_zspage(pool, class, gfp);\n\tif (!zspage) {\n\t\tcache_free_handle(pool, handle);\n\t\treturn (unsigned long)ERR_PTR(-ENOMEM);\n\t}\n\n\tspin_lock(&pool->lock);\n\tobj = obj_malloc(pool, zspage, handle);\n\tnewfg = get_fullness_group(class, zspage);\n\tinsert_zspage(class, zspage, newfg);\n\tset_zspage_mapping(zspage, class->index, newfg);\n\trecord_obj(handle, obj);\n\tatomic_long_add(class->pages_per_zspage, &pool->pages_allocated);\n\tclass_stat_inc(class, ZS_OBJS_ALLOCATED, class->objs_per_zspage);\n\tclass_stat_inc(class, ZS_OBJS_INUSE, 1);\n\n\t \n\tSetZsPageMovable(pool, zspage);\nout:\n\tspin_unlock(&pool->lock);\n\n\treturn handle;\n}\nEXPORT_SYMBOL_GPL(zs_malloc);\n\nstatic void obj_free(int class_size, unsigned long obj)\n{\n\tstruct link_free *link;\n\tstruct zspage *zspage;\n\tstruct page *f_page;\n\tunsigned long f_offset;\n\tunsigned int f_objidx;\n\tvoid *vaddr;\n\n\tobj_to_location(obj, &f_page, &f_objidx);\n\tf_offset = offset_in_page(class_size * f_objidx);\n\tzspage = get_zspage(f_page);\n\n\tvaddr = kmap_atomic(f_page);\n\tlink = (struct link_free *)(vaddr + f_offset);\n\n\t \n\tif (likely(!ZsHugePage(zspage)))\n\t\tlink->next = get_freeobj(zspage) << OBJ_TAG_BITS;\n\telse\n\t\tf_page->index = 0;\n\tset_freeobj(zspage, f_objidx);\n\n\tkunmap_atomic(vaddr);\n\tmod_zspage_inuse(zspage, -1);\n}\n\nvoid zs_free(struct zs_pool *pool, unsigned long handle)\n{\n\tstruct zspage *zspage;\n\tstruct page *f_page;\n\tunsigned long obj;\n\tstruct size_class *class;\n\tint fullness;\n\n\tif (IS_ERR_OR_NULL((void *)handle))\n\t\treturn;\n\n\t \n\tspin_lock(&pool->lock);\n\tobj = handle_to_obj(handle);\n\tobj_to_page(obj, &f_page);\n\tzspage = get_zspage(f_page);\n\tclass = zspage_class(pool, zspage);\n\n\tclass_stat_dec(class, ZS_OBJS_INUSE, 1);\n\tobj_free(class->size, obj);\n\n\tfullness = fix_fullness_group(class, zspage);\n\tif (fullness == ZS_INUSE_RATIO_0)\n\t\tfree_zspage(pool, class, zspage);\n\n\tspin_unlock(&pool->lock);\n\tcache_free_handle(pool, handle);\n}\nEXPORT_SYMBOL_GPL(zs_free);\n\nstatic void zs_object_copy(struct size_class *class, unsigned long dst,\n\t\t\t\tunsigned long src)\n{\n\tstruct page *s_page, *d_page;\n\tunsigned int s_objidx, d_objidx;\n\tunsigned long s_off, d_off;\n\tvoid *s_addr, *d_addr;\n\tint s_size, d_size, size;\n\tint written = 0;\n\n\ts_size = d_size = class->size;\n\n\tobj_to_location(src, &s_page, &s_objidx);\n\tobj_to_location(dst, &d_page, &d_objidx);\n\n\ts_off = offset_in_page(class->size * s_objidx);\n\td_off = offset_in_page(class->size * d_objidx);\n\n\tif (s_off + class->size > PAGE_SIZE)\n\t\ts_size = PAGE_SIZE - s_off;\n\n\tif (d_off + class->size > PAGE_SIZE)\n\t\td_size = PAGE_SIZE - d_off;\n\n\ts_addr = kmap_atomic(s_page);\n\td_addr = kmap_atomic(d_page);\n\n\twhile (1) {\n\t\tsize = min(s_size, d_size);\n\t\tmemcpy(d_addr + d_off, s_addr + s_off, size);\n\t\twritten += size;\n\n\t\tif (written == class->size)\n\t\t\tbreak;\n\n\t\ts_off += size;\n\t\ts_size -= size;\n\t\td_off += size;\n\t\td_size -= size;\n\n\t\t \n\t\tif (s_off >= PAGE_SIZE) {\n\t\t\tkunmap_atomic(d_addr);\n\t\t\tkunmap_atomic(s_addr);\n\t\t\ts_page = get_next_page(s_page);\n\t\t\ts_addr = kmap_atomic(s_page);\n\t\t\td_addr = kmap_atomic(d_page);\n\t\t\ts_size = class->size - written;\n\t\t\ts_off = 0;\n\t\t}\n\n\t\tif (d_off >= PAGE_SIZE) {\n\t\t\tkunmap_atomic(d_addr);\n\t\t\td_page = get_next_page(d_page);\n\t\t\td_addr = kmap_atomic(d_page);\n\t\t\td_size = class->size - written;\n\t\t\td_off = 0;\n\t\t}\n\t}\n\n\tkunmap_atomic(d_addr);\n\tkunmap_atomic(s_addr);\n}\n\n \nstatic unsigned long find_alloced_obj(struct size_class *class,\n\t\t\t\t      struct page *page, int *obj_idx)\n{\n\tunsigned int offset;\n\tint index = *obj_idx;\n\tunsigned long handle = 0;\n\tvoid *addr = kmap_atomic(page);\n\n\toffset = get_first_obj_offset(page);\n\toffset += class->size * index;\n\n\twhile (offset < PAGE_SIZE) {\n\t\tif (obj_allocated(page, addr + offset, &handle))\n\t\t\tbreak;\n\n\t\toffset += class->size;\n\t\tindex++;\n\t}\n\n\tkunmap_atomic(addr);\n\n\t*obj_idx = index;\n\n\treturn handle;\n}\n\nstatic void migrate_zspage(struct zs_pool *pool, struct zspage *src_zspage,\n\t\t\t   struct zspage *dst_zspage)\n{\n\tunsigned long used_obj, free_obj;\n\tunsigned long handle;\n\tint obj_idx = 0;\n\tstruct page *s_page = get_first_page(src_zspage);\n\tstruct size_class *class = pool->size_class[src_zspage->class];\n\n\twhile (1) {\n\t\thandle = find_alloced_obj(class, s_page, &obj_idx);\n\t\tif (!handle) {\n\t\t\ts_page = get_next_page(s_page);\n\t\t\tif (!s_page)\n\t\t\t\tbreak;\n\t\t\tobj_idx = 0;\n\t\t\tcontinue;\n\t\t}\n\n\t\tused_obj = handle_to_obj(handle);\n\t\tfree_obj = obj_malloc(pool, dst_zspage, handle);\n\t\tzs_object_copy(class, free_obj, used_obj);\n\t\tobj_idx++;\n\t\trecord_obj(handle, free_obj);\n\t\tobj_free(class->size, used_obj);\n\n\t\t \n\t\tif (zspage_full(class, dst_zspage))\n\t\t\tbreak;\n\n\t\t \n\t\tif (zspage_empty(src_zspage))\n\t\t\tbreak;\n\t}\n}\n\nstatic struct zspage *isolate_src_zspage(struct size_class *class)\n{\n\tstruct zspage *zspage;\n\tint fg;\n\n\tfor (fg = ZS_INUSE_RATIO_10; fg <= ZS_INUSE_RATIO_99; fg++) {\n\t\tzspage = list_first_entry_or_null(&class->fullness_list[fg],\n\t\t\t\t\t\t  struct zspage, list);\n\t\tif (zspage) {\n\t\t\tremove_zspage(class, zspage, fg);\n\t\t\treturn zspage;\n\t\t}\n\t}\n\n\treturn zspage;\n}\n\nstatic struct zspage *isolate_dst_zspage(struct size_class *class)\n{\n\tstruct zspage *zspage;\n\tint fg;\n\n\tfor (fg = ZS_INUSE_RATIO_99; fg >= ZS_INUSE_RATIO_10; fg--) {\n\t\tzspage = list_first_entry_or_null(&class->fullness_list[fg],\n\t\t\t\t\t\t  struct zspage, list);\n\t\tif (zspage) {\n\t\t\tremove_zspage(class, zspage, fg);\n\t\t\treturn zspage;\n\t\t}\n\t}\n\n\treturn zspage;\n}\n\n \nstatic int putback_zspage(struct size_class *class, struct zspage *zspage)\n{\n\tint fullness;\n\n\tfullness = get_fullness_group(class, zspage);\n\tinsert_zspage(class, zspage, fullness);\n\tset_zspage_mapping(zspage, class->index, fullness);\n\n\treturn fullness;\n}\n\n#ifdef CONFIG_COMPACTION\n \nstatic void lock_zspage(struct zspage *zspage)\n{\n\tstruct page *curr_page, *page;\n\n\t \n\twhile (1) {\n\t\tmigrate_read_lock(zspage);\n\t\tpage = get_first_page(zspage);\n\t\tif (trylock_page(page))\n\t\t\tbreak;\n\t\tget_page(page);\n\t\tmigrate_read_unlock(zspage);\n\t\twait_on_page_locked(page);\n\t\tput_page(page);\n\t}\n\n\tcurr_page = page;\n\twhile ((page = get_next_page(curr_page))) {\n\t\tif (trylock_page(page)) {\n\t\t\tcurr_page = page;\n\t\t} else {\n\t\t\tget_page(page);\n\t\t\tmigrate_read_unlock(zspage);\n\t\t\twait_on_page_locked(page);\n\t\t\tput_page(page);\n\t\t\tmigrate_read_lock(zspage);\n\t\t}\n\t}\n\tmigrate_read_unlock(zspage);\n}\n#endif  \n\nstatic void migrate_lock_init(struct zspage *zspage)\n{\n\trwlock_init(&zspage->lock);\n}\n\nstatic void migrate_read_lock(struct zspage *zspage) __acquires(&zspage->lock)\n{\n\tread_lock(&zspage->lock);\n}\n\nstatic void migrate_read_unlock(struct zspage *zspage) __releases(&zspage->lock)\n{\n\tread_unlock(&zspage->lock);\n}\n\n#ifdef CONFIG_COMPACTION\nstatic void migrate_write_lock(struct zspage *zspage)\n{\n\twrite_lock(&zspage->lock);\n}\n\nstatic void migrate_write_lock_nested(struct zspage *zspage)\n{\n\twrite_lock_nested(&zspage->lock, SINGLE_DEPTH_NESTING);\n}\n\nstatic void migrate_write_unlock(struct zspage *zspage)\n{\n\twrite_unlock(&zspage->lock);\n}\n\n \nstatic void inc_zspage_isolation(struct zspage *zspage)\n{\n\tzspage->isolated++;\n}\n\nstatic void dec_zspage_isolation(struct zspage *zspage)\n{\n\tVM_BUG_ON(zspage->isolated == 0);\n\tzspage->isolated--;\n}\n\nstatic const struct movable_operations zsmalloc_mops;\n\nstatic void replace_sub_page(struct size_class *class, struct zspage *zspage,\n\t\t\t\tstruct page *newpage, struct page *oldpage)\n{\n\tstruct page *page;\n\tstruct page *pages[ZS_MAX_PAGES_PER_ZSPAGE] = {NULL, };\n\tint idx = 0;\n\n\tpage = get_first_page(zspage);\n\tdo {\n\t\tif (page == oldpage)\n\t\t\tpages[idx] = newpage;\n\t\telse\n\t\t\tpages[idx] = page;\n\t\tidx++;\n\t} while ((page = get_next_page(page)) != NULL);\n\n\tcreate_page_chain(class, zspage, pages);\n\tset_first_obj_offset(newpage, get_first_obj_offset(oldpage));\n\tif (unlikely(ZsHugePage(zspage)))\n\t\tnewpage->index = oldpage->index;\n\t__SetPageMovable(newpage, &zsmalloc_mops);\n}\n\nstatic bool zs_page_isolate(struct page *page, isolate_mode_t mode)\n{\n\tstruct zs_pool *pool;\n\tstruct zspage *zspage;\n\n\t \n\tVM_BUG_ON_PAGE(PageIsolated(page), page);\n\n\tzspage = get_zspage(page);\n\tpool = zspage->pool;\n\tspin_lock(&pool->lock);\n\tinc_zspage_isolation(zspage);\n\tspin_unlock(&pool->lock);\n\n\treturn true;\n}\n\nstatic int zs_page_migrate(struct page *newpage, struct page *page,\n\t\tenum migrate_mode mode)\n{\n\tstruct zs_pool *pool;\n\tstruct size_class *class;\n\tstruct zspage *zspage;\n\tstruct page *dummy;\n\tvoid *s_addr, *d_addr, *addr;\n\tunsigned int offset;\n\tunsigned long handle;\n\tunsigned long old_obj, new_obj;\n\tunsigned int obj_idx;\n\n\t \n\tif (mode == MIGRATE_SYNC_NO_COPY)\n\t\treturn -EINVAL;\n\n\tVM_BUG_ON_PAGE(!PageIsolated(page), page);\n\n\t \n\tzspage = get_zspage(page);\n\tpool = zspage->pool;\n\n\t \n\tspin_lock(&pool->lock);\n\tclass = zspage_class(pool, zspage);\n\n\t \n\tmigrate_write_lock(zspage);\n\n\toffset = get_first_obj_offset(page);\n\ts_addr = kmap_atomic(page);\n\n\t \n\td_addr = kmap_atomic(newpage);\n\tmemcpy(d_addr, s_addr, PAGE_SIZE);\n\tkunmap_atomic(d_addr);\n\n\tfor (addr = s_addr + offset; addr < s_addr + PAGE_SIZE;\n\t\t\t\t\taddr += class->size) {\n\t\tif (obj_allocated(page, addr, &handle)) {\n\n\t\t\told_obj = handle_to_obj(handle);\n\t\t\tobj_to_location(old_obj, &dummy, &obj_idx);\n\t\t\tnew_obj = (unsigned long)location_to_obj(newpage,\n\t\t\t\t\t\t\t\tobj_idx);\n\t\t\trecord_obj(handle, new_obj);\n\t\t}\n\t}\n\tkunmap_atomic(s_addr);\n\n\treplace_sub_page(class, zspage, newpage, page);\n\tdec_zspage_isolation(zspage);\n\t \n\tspin_unlock(&pool->lock);\n\tmigrate_write_unlock(zspage);\n\n\tget_page(newpage);\n\tif (page_zone(newpage) != page_zone(page)) {\n\t\tdec_zone_page_state(page, NR_ZSPAGES);\n\t\tinc_zone_page_state(newpage, NR_ZSPAGES);\n\t}\n\n\treset_page(page);\n\tput_page(page);\n\n\treturn MIGRATEPAGE_SUCCESS;\n}\n\nstatic void zs_page_putback(struct page *page)\n{\n\tstruct zs_pool *pool;\n\tstruct zspage *zspage;\n\n\tVM_BUG_ON_PAGE(!PageIsolated(page), page);\n\n\tzspage = get_zspage(page);\n\tpool = zspage->pool;\n\tspin_lock(&pool->lock);\n\tdec_zspage_isolation(zspage);\n\tspin_unlock(&pool->lock);\n}\n\nstatic const struct movable_operations zsmalloc_mops = {\n\t.isolate_page = zs_page_isolate,\n\t.migrate_page = zs_page_migrate,\n\t.putback_page = zs_page_putback,\n};\n\n \nstatic void async_free_zspage(struct work_struct *work)\n{\n\tint i;\n\tstruct size_class *class;\n\tunsigned int class_idx;\n\tint fullness;\n\tstruct zspage *zspage, *tmp;\n\tLIST_HEAD(free_pages);\n\tstruct zs_pool *pool = container_of(work, struct zs_pool,\n\t\t\t\t\tfree_work);\n\n\tfor (i = 0; i < ZS_SIZE_CLASSES; i++) {\n\t\tclass = pool->size_class[i];\n\t\tif (class->index != i)\n\t\t\tcontinue;\n\n\t\tspin_lock(&pool->lock);\n\t\tlist_splice_init(&class->fullness_list[ZS_INUSE_RATIO_0],\n\t\t\t\t &free_pages);\n\t\tspin_unlock(&pool->lock);\n\t}\n\n\tlist_for_each_entry_safe(zspage, tmp, &free_pages, list) {\n\t\tlist_del(&zspage->list);\n\t\tlock_zspage(zspage);\n\n\t\tget_zspage_mapping(zspage, &class_idx, &fullness);\n\t\tVM_BUG_ON(fullness != ZS_INUSE_RATIO_0);\n\t\tclass = pool->size_class[class_idx];\n\t\tspin_lock(&pool->lock);\n\t\t__free_zspage(pool, class, zspage);\n\t\tspin_unlock(&pool->lock);\n\t}\n};\n\nstatic void kick_deferred_free(struct zs_pool *pool)\n{\n\tschedule_work(&pool->free_work);\n}\n\nstatic void zs_flush_migration(struct zs_pool *pool)\n{\n\tflush_work(&pool->free_work);\n}\n\nstatic void init_deferred_free(struct zs_pool *pool)\n{\n\tINIT_WORK(&pool->free_work, async_free_zspage);\n}\n\nstatic void SetZsPageMovable(struct zs_pool *pool, struct zspage *zspage)\n{\n\tstruct page *page = get_first_page(zspage);\n\n\tdo {\n\t\tWARN_ON(!trylock_page(page));\n\t\t__SetPageMovable(page, &zsmalloc_mops);\n\t\tunlock_page(page);\n\t} while ((page = get_next_page(page)) != NULL);\n}\n#else\nstatic inline void zs_flush_migration(struct zs_pool *pool) { }\n#endif\n\n \nstatic unsigned long zs_can_compact(struct size_class *class)\n{\n\tunsigned long obj_wasted;\n\tunsigned long obj_allocated = zs_stat_get(class, ZS_OBJS_ALLOCATED);\n\tunsigned long obj_used = zs_stat_get(class, ZS_OBJS_INUSE);\n\n\tif (obj_allocated <= obj_used)\n\t\treturn 0;\n\n\tobj_wasted = obj_allocated - obj_used;\n\tobj_wasted /= class->objs_per_zspage;\n\n\treturn obj_wasted * class->pages_per_zspage;\n}\n\nstatic unsigned long __zs_compact(struct zs_pool *pool,\n\t\t\t\t  struct size_class *class)\n{\n\tstruct zspage *src_zspage = NULL;\n\tstruct zspage *dst_zspage = NULL;\n\tunsigned long pages_freed = 0;\n\n\t \n\tspin_lock(&pool->lock);\n\twhile (zs_can_compact(class)) {\n\t\tint fg;\n\n\t\tif (!dst_zspage) {\n\t\t\tdst_zspage = isolate_dst_zspage(class);\n\t\t\tif (!dst_zspage)\n\t\t\t\tbreak;\n\t\t\tmigrate_write_lock(dst_zspage);\n\t\t}\n\n\t\tsrc_zspage = isolate_src_zspage(class);\n\t\tif (!src_zspage)\n\t\t\tbreak;\n\n\t\tmigrate_write_lock_nested(src_zspage);\n\n\t\tmigrate_zspage(pool, src_zspage, dst_zspage);\n\t\tfg = putback_zspage(class, src_zspage);\n\t\tmigrate_write_unlock(src_zspage);\n\n\t\tif (fg == ZS_INUSE_RATIO_0) {\n\t\t\tfree_zspage(pool, class, src_zspage);\n\t\t\tpages_freed += class->pages_per_zspage;\n\t\t}\n\t\tsrc_zspage = NULL;\n\n\t\tif (get_fullness_group(class, dst_zspage) == ZS_INUSE_RATIO_100\n\t\t    || spin_is_contended(&pool->lock)) {\n\t\t\tputback_zspage(class, dst_zspage);\n\t\t\tmigrate_write_unlock(dst_zspage);\n\t\t\tdst_zspage = NULL;\n\n\t\t\tspin_unlock(&pool->lock);\n\t\t\tcond_resched();\n\t\t\tspin_lock(&pool->lock);\n\t\t}\n\t}\n\n\tif (src_zspage) {\n\t\tputback_zspage(class, src_zspage);\n\t\tmigrate_write_unlock(src_zspage);\n\t}\n\n\tif (dst_zspage) {\n\t\tputback_zspage(class, dst_zspage);\n\t\tmigrate_write_unlock(dst_zspage);\n\t}\n\tspin_unlock(&pool->lock);\n\n\treturn pages_freed;\n}\n\nunsigned long zs_compact(struct zs_pool *pool)\n{\n\tint i;\n\tstruct size_class *class;\n\tunsigned long pages_freed = 0;\n\n\t \n\tif (atomic_xchg(&pool->compaction_in_progress, 1))\n\t\treturn 0;\n\n\tfor (i = ZS_SIZE_CLASSES - 1; i >= 0; i--) {\n\t\tclass = pool->size_class[i];\n\t\tif (class->index != i)\n\t\t\tcontinue;\n\t\tpages_freed += __zs_compact(pool, class);\n\t}\n\tatomic_long_add(pages_freed, &pool->stats.pages_compacted);\n\tatomic_set(&pool->compaction_in_progress, 0);\n\n\treturn pages_freed;\n}\nEXPORT_SYMBOL_GPL(zs_compact);\n\nvoid zs_pool_stats(struct zs_pool *pool, struct zs_pool_stats *stats)\n{\n\tmemcpy(stats, &pool->stats, sizeof(struct zs_pool_stats));\n}\nEXPORT_SYMBOL_GPL(zs_pool_stats);\n\nstatic unsigned long zs_shrinker_scan(struct shrinker *shrinker,\n\t\tstruct shrink_control *sc)\n{\n\tunsigned long pages_freed;\n\tstruct zs_pool *pool = container_of(shrinker, struct zs_pool,\n\t\t\tshrinker);\n\n\t \n\tpages_freed = zs_compact(pool);\n\n\treturn pages_freed ? pages_freed : SHRINK_STOP;\n}\n\nstatic unsigned long zs_shrinker_count(struct shrinker *shrinker,\n\t\tstruct shrink_control *sc)\n{\n\tint i;\n\tstruct size_class *class;\n\tunsigned long pages_to_free = 0;\n\tstruct zs_pool *pool = container_of(shrinker, struct zs_pool,\n\t\t\tshrinker);\n\n\tfor (i = ZS_SIZE_CLASSES - 1; i >= 0; i--) {\n\t\tclass = pool->size_class[i];\n\t\tif (class->index != i)\n\t\t\tcontinue;\n\n\t\tpages_to_free += zs_can_compact(class);\n\t}\n\n\treturn pages_to_free;\n}\n\nstatic void zs_unregister_shrinker(struct zs_pool *pool)\n{\n\tunregister_shrinker(&pool->shrinker);\n}\n\nstatic int zs_register_shrinker(struct zs_pool *pool)\n{\n\tpool->shrinker.scan_objects = zs_shrinker_scan;\n\tpool->shrinker.count_objects = zs_shrinker_count;\n\tpool->shrinker.batch = 0;\n\tpool->shrinker.seeks = DEFAULT_SEEKS;\n\n\treturn register_shrinker(&pool->shrinker, \"mm-zspool:%s\",\n\t\t\t\t pool->name);\n}\n\nstatic int calculate_zspage_chain_size(int class_size)\n{\n\tint i, min_waste = INT_MAX;\n\tint chain_size = 1;\n\n\tif (is_power_of_2(class_size))\n\t\treturn chain_size;\n\n\tfor (i = 1; i <= ZS_MAX_PAGES_PER_ZSPAGE; i++) {\n\t\tint waste;\n\n\t\twaste = (i * PAGE_SIZE) % class_size;\n\t\tif (waste < min_waste) {\n\t\t\tmin_waste = waste;\n\t\t\tchain_size = i;\n\t\t}\n\t}\n\n\treturn chain_size;\n}\n\n \nstruct zs_pool *zs_create_pool(const char *name)\n{\n\tint i;\n\tstruct zs_pool *pool;\n\tstruct size_class *prev_class = NULL;\n\n\tpool = kzalloc(sizeof(*pool), GFP_KERNEL);\n\tif (!pool)\n\t\treturn NULL;\n\n\tinit_deferred_free(pool);\n\tspin_lock_init(&pool->lock);\n\tatomic_set(&pool->compaction_in_progress, 0);\n\n\tpool->name = kstrdup(name, GFP_KERNEL);\n\tif (!pool->name)\n\t\tgoto err;\n\n\tif (create_cache(pool))\n\t\tgoto err;\n\n\t \n\tfor (i = ZS_SIZE_CLASSES - 1; i >= 0; i--) {\n\t\tint size;\n\t\tint pages_per_zspage;\n\t\tint objs_per_zspage;\n\t\tstruct size_class *class;\n\t\tint fullness;\n\n\t\tsize = ZS_MIN_ALLOC_SIZE + i * ZS_SIZE_CLASS_DELTA;\n\t\tif (size > ZS_MAX_ALLOC_SIZE)\n\t\t\tsize = ZS_MAX_ALLOC_SIZE;\n\t\tpages_per_zspage = calculate_zspage_chain_size(size);\n\t\tobjs_per_zspage = pages_per_zspage * PAGE_SIZE / size;\n\n\t\t \n\t\tif (pages_per_zspage != 1 && objs_per_zspage != 1 &&\n\t\t\t\t!huge_class_size) {\n\t\t\thuge_class_size = size;\n\t\t\t \n\t\t\thuge_class_size -= (ZS_HANDLE_SIZE - 1);\n\t\t}\n\n\t\t \n\t\tif (prev_class) {\n\t\t\tif (can_merge(prev_class, pages_per_zspage, objs_per_zspage)) {\n\t\t\t\tpool->size_class[i] = prev_class;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\tclass = kzalloc(sizeof(struct size_class), GFP_KERNEL);\n\t\tif (!class)\n\t\t\tgoto err;\n\n\t\tclass->size = size;\n\t\tclass->index = i;\n\t\tclass->pages_per_zspage = pages_per_zspage;\n\t\tclass->objs_per_zspage = objs_per_zspage;\n\t\tpool->size_class[i] = class;\n\n\t\tfullness = ZS_INUSE_RATIO_0;\n\t\twhile (fullness < NR_FULLNESS_GROUPS) {\n\t\t\tINIT_LIST_HEAD(&class->fullness_list[fullness]);\n\t\t\tfullness++;\n\t\t}\n\n\t\tprev_class = class;\n\t}\n\n\t \n\tzs_pool_stat_create(pool, name);\n\n\t \n\tzs_register_shrinker(pool);\n\n\treturn pool;\n\nerr:\n\tzs_destroy_pool(pool);\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(zs_create_pool);\n\nvoid zs_destroy_pool(struct zs_pool *pool)\n{\n\tint i;\n\n\tzs_unregister_shrinker(pool);\n\tzs_flush_migration(pool);\n\tzs_pool_stat_destroy(pool);\n\n\tfor (i = 0; i < ZS_SIZE_CLASSES; i++) {\n\t\tint fg;\n\t\tstruct size_class *class = pool->size_class[i];\n\n\t\tif (!class)\n\t\t\tcontinue;\n\n\t\tif (class->index != i)\n\t\t\tcontinue;\n\n\t\tfor (fg = ZS_INUSE_RATIO_0; fg < NR_FULLNESS_GROUPS; fg++) {\n\t\t\tif (list_empty(&class->fullness_list[fg]))\n\t\t\t\tcontinue;\n\n\t\t\tpr_err(\"Class-%d fullness group %d is not empty\\n\",\n\t\t\t       class->size, fg);\n\t\t}\n\t\tkfree(class);\n\t}\n\n\tdestroy_cache(pool);\n\tkfree(pool->name);\n\tkfree(pool);\n}\nEXPORT_SYMBOL_GPL(zs_destroy_pool);\n\nstatic int __init zs_init(void)\n{\n\tint ret;\n\n\tret = cpuhp_setup_state(CPUHP_MM_ZS_PREPARE, \"mm/zsmalloc:prepare\",\n\t\t\t\tzs_cpu_prepare, zs_cpu_dead);\n\tif (ret)\n\t\tgoto out;\n\n#ifdef CONFIG_ZPOOL\n\tzpool_register_driver(&zs_zpool_driver);\n#endif\n\n\tzs_stat_init();\n\n\treturn 0;\n\nout:\n\treturn ret;\n}\n\nstatic void __exit zs_exit(void)\n{\n#ifdef CONFIG_ZPOOL\n\tzpool_unregister_driver(&zs_zpool_driver);\n#endif\n\tcpuhp_remove_state(CPUHP_MM_ZS_PREPARE);\n\n\tzs_stat_exit();\n}\n\nmodule_init(zs_init);\nmodule_exit(zs_exit);\n\nMODULE_LICENSE(\"Dual BSD/GPL\");\nMODULE_AUTHOR(\"Nitin Gupta <ngupta@vflare.org>\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}