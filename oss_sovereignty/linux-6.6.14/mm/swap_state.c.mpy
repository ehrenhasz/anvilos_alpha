{
  "module_name": "swap_state.c",
  "hash_id": "1918480dff5c7e6f969bf296c56f695bc1346a3fa478a20a5ccefcfbce59c0b2",
  "original_prompt": "Ingested from linux-6.6.14/mm/swap_state.c",
  "human_readable_source": "\n \n#include <linux/mm.h>\n#include <linux/gfp.h>\n#include <linux/kernel_stat.h>\n#include <linux/swap.h>\n#include <linux/swapops.h>\n#include <linux/init.h>\n#include <linux/pagemap.h>\n#include <linux/backing-dev.h>\n#include <linux/blkdev.h>\n#include <linux/migrate.h>\n#include <linux/vmalloc.h>\n#include <linux/swap_slots.h>\n#include <linux/huge_mm.h>\n#include <linux/shmem_fs.h>\n#include \"internal.h\"\n#include \"swap.h\"\n\n \nstatic const struct address_space_operations swap_aops = {\n\t.writepage\t= swap_writepage,\n\t.dirty_folio\t= noop_dirty_folio,\n#ifdef CONFIG_MIGRATION\n\t.migrate_folio\t= migrate_folio,\n#endif\n};\n\nstruct address_space *swapper_spaces[MAX_SWAPFILES] __read_mostly;\nstatic unsigned int nr_swapper_spaces[MAX_SWAPFILES] __read_mostly;\nstatic bool enable_vma_readahead __read_mostly = true;\n\n#define SWAP_RA_WIN_SHIFT\t(PAGE_SHIFT / 2)\n#define SWAP_RA_HITS_MASK\t((1UL << SWAP_RA_WIN_SHIFT) - 1)\n#define SWAP_RA_HITS_MAX\tSWAP_RA_HITS_MASK\n#define SWAP_RA_WIN_MASK\t(~PAGE_MASK & ~SWAP_RA_HITS_MASK)\n\n#define SWAP_RA_HITS(v)\t\t((v) & SWAP_RA_HITS_MASK)\n#define SWAP_RA_WIN(v)\t\t(((v) & SWAP_RA_WIN_MASK) >> SWAP_RA_WIN_SHIFT)\n#define SWAP_RA_ADDR(v)\t\t((v) & PAGE_MASK)\n\n#define SWAP_RA_VAL(addr, win, hits)\t\t\t\t\\\n\t(((addr) & PAGE_MASK) |\t\t\t\t\t\\\n\t (((win) << SWAP_RA_WIN_SHIFT) & SWAP_RA_WIN_MASK) |\t\\\n\t ((hits) & SWAP_RA_HITS_MASK))\n\n \n#define GET_SWAP_RA_VAL(vma)\t\t\t\t\t\\\n\t(atomic_long_read(&(vma)->swap_readahead_info) ? : 4)\n\nstatic atomic_t swapin_readahead_hits = ATOMIC_INIT(4);\n\nvoid show_swap_cache_info(void)\n{\n\tprintk(\"%lu pages in swap cache\\n\", total_swapcache_pages());\n\tprintk(\"Free swap  = %ldkB\\n\", K(get_nr_swap_pages()));\n\tprintk(\"Total swap = %lukB\\n\", K(total_swap_pages));\n}\n\nvoid *get_shadow_from_swap_cache(swp_entry_t entry)\n{\n\tstruct address_space *address_space = swap_address_space(entry);\n\tpgoff_t idx = swp_offset(entry);\n\tstruct page *page;\n\n\tpage = xa_load(&address_space->i_pages, idx);\n\tif (xa_is_value(page))\n\t\treturn page;\n\treturn NULL;\n}\n\n \nint add_to_swap_cache(struct folio *folio, swp_entry_t entry,\n\t\t\tgfp_t gfp, void **shadowp)\n{\n\tstruct address_space *address_space = swap_address_space(entry);\n\tpgoff_t idx = swp_offset(entry);\n\tXA_STATE_ORDER(xas, &address_space->i_pages, idx, folio_order(folio));\n\tunsigned long i, nr = folio_nr_pages(folio);\n\tvoid *old;\n\n\txas_set_update(&xas, workingset_update_node);\n\n\tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n\tVM_BUG_ON_FOLIO(folio_test_swapcache(folio), folio);\n\tVM_BUG_ON_FOLIO(!folio_test_swapbacked(folio), folio);\n\n\tfolio_ref_add(folio, nr);\n\tfolio_set_swapcache(folio);\n\tfolio->swap = entry;\n\n\tdo {\n\t\txas_lock_irq(&xas);\n\t\txas_create_range(&xas);\n\t\tif (xas_error(&xas))\n\t\t\tgoto unlock;\n\t\tfor (i = 0; i < nr; i++) {\n\t\t\tVM_BUG_ON_FOLIO(xas.xa_index != idx + i, folio);\n\t\t\told = xas_load(&xas);\n\t\t\tif (xa_is_value(old)) {\n\t\t\t\tif (shadowp)\n\t\t\t\t\t*shadowp = old;\n\t\t\t}\n\t\t\txas_store(&xas, folio);\n\t\t\txas_next(&xas);\n\t\t}\n\t\taddress_space->nrpages += nr;\n\t\t__node_stat_mod_folio(folio, NR_FILE_PAGES, nr);\n\t\t__lruvec_stat_mod_folio(folio, NR_SWAPCACHE, nr);\nunlock:\n\t\txas_unlock_irq(&xas);\n\t} while (xas_nomem(&xas, gfp));\n\n\tif (!xas_error(&xas))\n\t\treturn 0;\n\n\tfolio_clear_swapcache(folio);\n\tfolio_ref_sub(folio, nr);\n\treturn xas_error(&xas);\n}\n\n \nvoid __delete_from_swap_cache(struct folio *folio,\n\t\t\tswp_entry_t entry, void *shadow)\n{\n\tstruct address_space *address_space = swap_address_space(entry);\n\tint i;\n\tlong nr = folio_nr_pages(folio);\n\tpgoff_t idx = swp_offset(entry);\n\tXA_STATE(xas, &address_space->i_pages, idx);\n\n\txas_set_update(&xas, workingset_update_node);\n\n\tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n\tVM_BUG_ON_FOLIO(!folio_test_swapcache(folio), folio);\n\tVM_BUG_ON_FOLIO(folio_test_writeback(folio), folio);\n\n\tfor (i = 0; i < nr; i++) {\n\t\tvoid *entry = xas_store(&xas, shadow);\n\t\tVM_BUG_ON_PAGE(entry != folio, entry);\n\t\txas_next(&xas);\n\t}\n\tfolio->swap.val = 0;\n\tfolio_clear_swapcache(folio);\n\taddress_space->nrpages -= nr;\n\t__node_stat_mod_folio(folio, NR_FILE_PAGES, -nr);\n\t__lruvec_stat_mod_folio(folio, NR_SWAPCACHE, -nr);\n}\n\n \nbool add_to_swap(struct folio *folio)\n{\n\tswp_entry_t entry;\n\tint err;\n\n\tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n\tVM_BUG_ON_FOLIO(!folio_test_uptodate(folio), folio);\n\n\tentry = folio_alloc_swap(folio);\n\tif (!entry.val)\n\t\treturn false;\n\n\t \n\t \n\terr = add_to_swap_cache(folio, entry,\n\t\t\t__GFP_HIGH|__GFP_NOMEMALLOC|__GFP_NOWARN, NULL);\n\tif (err)\n\t\t \n\t\tgoto fail;\n\t \n\tfolio_mark_dirty(folio);\n\n\treturn true;\n\nfail:\n\tput_swap_folio(folio, entry);\n\treturn false;\n}\n\n \nvoid delete_from_swap_cache(struct folio *folio)\n{\n\tswp_entry_t entry = folio->swap;\n\tstruct address_space *address_space = swap_address_space(entry);\n\n\txa_lock_irq(&address_space->i_pages);\n\t__delete_from_swap_cache(folio, entry, NULL);\n\txa_unlock_irq(&address_space->i_pages);\n\n\tput_swap_folio(folio, entry);\n\tfolio_ref_sub(folio, folio_nr_pages(folio));\n}\n\nvoid clear_shadow_from_swap_cache(int type, unsigned long begin,\n\t\t\t\tunsigned long end)\n{\n\tunsigned long curr = begin;\n\tvoid *old;\n\n\tfor (;;) {\n\t\tswp_entry_t entry = swp_entry(type, curr);\n\t\tstruct address_space *address_space = swap_address_space(entry);\n\t\tXA_STATE(xas, &address_space->i_pages, curr);\n\n\t\txas_set_update(&xas, workingset_update_node);\n\n\t\txa_lock_irq(&address_space->i_pages);\n\t\txas_for_each(&xas, old, end) {\n\t\t\tif (!xa_is_value(old))\n\t\t\t\tcontinue;\n\t\t\txas_store(&xas, NULL);\n\t\t}\n\t\txa_unlock_irq(&address_space->i_pages);\n\n\t\t \n\t\tcurr >>= SWAP_ADDRESS_SPACE_SHIFT;\n\t\tcurr++;\n\t\tcurr <<= SWAP_ADDRESS_SPACE_SHIFT;\n\t\tif (curr > end)\n\t\t\tbreak;\n\t}\n}\n\n \nvoid free_swap_cache(struct page *page)\n{\n\tstruct folio *folio = page_folio(page);\n\n\tif (folio_test_swapcache(folio) && !folio_mapped(folio) &&\n\t    folio_trylock(folio)) {\n\t\tfolio_free_swap(folio);\n\t\tfolio_unlock(folio);\n\t}\n}\n\n \nvoid free_page_and_swap_cache(struct page *page)\n{\n\tfree_swap_cache(page);\n\tif (!is_huge_zero_page(page))\n\t\tput_page(page);\n}\n\n \nvoid free_pages_and_swap_cache(struct encoded_page **pages, int nr)\n{\n\tlru_add_drain();\n\tfor (int i = 0; i < nr; i++)\n\t\tfree_swap_cache(encoded_page_ptr(pages[i]));\n\trelease_pages(pages, nr);\n}\n\nstatic inline bool swap_use_vma_readahead(void)\n{\n\treturn READ_ONCE(enable_vma_readahead) && !atomic_read(&nr_rotate_swap);\n}\n\n \nstruct folio *swap_cache_get_folio(swp_entry_t entry,\n\t\tstruct vm_area_struct *vma, unsigned long addr)\n{\n\tstruct folio *folio;\n\n\tfolio = filemap_get_folio(swap_address_space(entry), swp_offset(entry));\n\tif (!IS_ERR(folio)) {\n\t\tbool vma_ra = swap_use_vma_readahead();\n\t\tbool readahead;\n\n\t\t \n\t\tif (unlikely(folio_test_large(folio)))\n\t\t\treturn folio;\n\n\t\treadahead = folio_test_clear_readahead(folio);\n\t\tif (vma && vma_ra) {\n\t\t\tunsigned long ra_val;\n\t\t\tint win, hits;\n\n\t\t\tra_val = GET_SWAP_RA_VAL(vma);\n\t\t\twin = SWAP_RA_WIN(ra_val);\n\t\t\thits = SWAP_RA_HITS(ra_val);\n\t\t\tif (readahead)\n\t\t\t\thits = min_t(int, hits + 1, SWAP_RA_HITS_MAX);\n\t\t\tatomic_long_set(&vma->swap_readahead_info,\n\t\t\t\t\tSWAP_RA_VAL(addr, win, hits));\n\t\t}\n\n\t\tif (readahead) {\n\t\t\tcount_vm_event(SWAP_RA_HIT);\n\t\t\tif (!vma || !vma_ra)\n\t\t\t\tatomic_inc(&swapin_readahead_hits);\n\t\t}\n\t} else {\n\t\tfolio = NULL;\n\t}\n\n\treturn folio;\n}\n\n \nstruct folio *filemap_get_incore_folio(struct address_space *mapping,\n\t\tpgoff_t index)\n{\n\tswp_entry_t swp;\n\tstruct swap_info_struct *si;\n\tstruct folio *folio = filemap_get_entry(mapping, index);\n\n\tif (!folio)\n\t\treturn ERR_PTR(-ENOENT);\n\tif (!xa_is_value(folio))\n\t\treturn folio;\n\tif (!shmem_mapping(mapping))\n\t\treturn ERR_PTR(-ENOENT);\n\n\tswp = radix_to_swp_entry(folio);\n\t \n\tif (non_swap_entry(swp))\n\t\treturn ERR_PTR(-ENOENT);\n\t \n\tsi = get_swap_device(swp);\n\tif (!si)\n\t\treturn ERR_PTR(-ENOENT);\n\tindex = swp_offset(swp);\n\tfolio = filemap_get_folio(swap_address_space(swp), index);\n\tput_swap_device(si);\n\treturn folio;\n}\n\nstruct page *__read_swap_cache_async(swp_entry_t entry, gfp_t gfp_mask,\n\t\t\tstruct vm_area_struct *vma, unsigned long addr,\n\t\t\tbool *new_page_allocated)\n{\n\tstruct swap_info_struct *si;\n\tstruct folio *folio;\n\tstruct page *page;\n\tvoid *shadow = NULL;\n\n\t*new_page_allocated = false;\n\tsi = get_swap_device(entry);\n\tif (!si)\n\t\treturn NULL;\n\n\tfor (;;) {\n\t\tint err;\n\t\t \n\t\tfolio = filemap_get_folio(swap_address_space(entry),\n\t\t\t\t\t\tswp_offset(entry));\n\t\tif (!IS_ERR(folio)) {\n\t\t\tpage = folio_file_page(folio, swp_offset(entry));\n\t\t\tgoto got_page;\n\t\t}\n\n\t\t \n\t\tif (!swap_swapcount(si, entry) && swap_slot_cache_enabled)\n\t\t\tgoto fail_put_swap;\n\n\t\t \n\t\tfolio = vma_alloc_folio(gfp_mask, 0, vma, addr, false);\n\t\tif (!folio)\n                        goto fail_put_swap;\n\n\t\t \n\t\terr = swapcache_prepare(entry);\n\t\tif (!err)\n\t\t\tbreak;\n\n\t\tfolio_put(folio);\n\t\tif (err != -EEXIST)\n\t\t\tgoto fail_put_swap;\n\n\t\t \n\t\tschedule_timeout_uninterruptible(1);\n\t}\n\n\t \n\n\t__folio_set_locked(folio);\n\t__folio_set_swapbacked(folio);\n\n\tif (mem_cgroup_swapin_charge_folio(folio, NULL, gfp_mask, entry))\n\t\tgoto fail_unlock;\n\n\t \n\tif (add_to_swap_cache(folio, entry, gfp_mask & GFP_RECLAIM_MASK, &shadow))\n\t\tgoto fail_unlock;\n\n\tmem_cgroup_swapin_uncharge_swap(entry);\n\n\tif (shadow)\n\t\tworkingset_refault(folio, shadow);\n\n\t \n\tfolio_add_lru(folio);\n\t*new_page_allocated = true;\n\tpage = &folio->page;\ngot_page:\n\tput_swap_device(si);\n\treturn page;\n\nfail_unlock:\n\tput_swap_folio(folio, entry);\n\tfolio_unlock(folio);\n\tfolio_put(folio);\nfail_put_swap:\n\tput_swap_device(si);\n\treturn NULL;\n}\n\n \nstruct page *read_swap_cache_async(swp_entry_t entry, gfp_t gfp_mask,\n\t\t\t\t   struct vm_area_struct *vma,\n\t\t\t\t   unsigned long addr, struct swap_iocb **plug)\n{\n\tbool page_was_allocated;\n\tstruct page *retpage = __read_swap_cache_async(entry, gfp_mask,\n\t\t\tvma, addr, &page_was_allocated);\n\n\tif (page_was_allocated)\n\t\tswap_readpage(retpage, false, plug);\n\n\treturn retpage;\n}\n\nstatic unsigned int __swapin_nr_pages(unsigned long prev_offset,\n\t\t\t\t      unsigned long offset,\n\t\t\t\t      int hits,\n\t\t\t\t      int max_pages,\n\t\t\t\t      int prev_win)\n{\n\tunsigned int pages, last_ra;\n\n\t \n\tpages = hits + 2;\n\tif (pages == 2) {\n\t\t \n\t\tif (offset != prev_offset + 1 && offset != prev_offset - 1)\n\t\t\tpages = 1;\n\t} else {\n\t\tunsigned int roundup = 4;\n\t\twhile (roundup < pages)\n\t\t\troundup <<= 1;\n\t\tpages = roundup;\n\t}\n\n\tif (pages > max_pages)\n\t\tpages = max_pages;\n\n\t \n\tlast_ra = prev_win / 2;\n\tif (pages < last_ra)\n\t\tpages = last_ra;\n\n\treturn pages;\n}\n\nstatic unsigned long swapin_nr_pages(unsigned long offset)\n{\n\tstatic unsigned long prev_offset;\n\tunsigned int hits, pages, max_pages;\n\tstatic atomic_t last_readahead_pages;\n\n\tmax_pages = 1 << READ_ONCE(page_cluster);\n\tif (max_pages <= 1)\n\t\treturn 1;\n\n\thits = atomic_xchg(&swapin_readahead_hits, 0);\n\tpages = __swapin_nr_pages(READ_ONCE(prev_offset), offset, hits,\n\t\t\t\t  max_pages,\n\t\t\t\t  atomic_read(&last_readahead_pages));\n\tif (!hits)\n\t\tWRITE_ONCE(prev_offset, offset);\n\tatomic_set(&last_readahead_pages, pages);\n\n\treturn pages;\n}\n\n \nstruct page *swap_cluster_readahead(swp_entry_t entry, gfp_t gfp_mask,\n\t\t\t\tstruct vm_fault *vmf)\n{\n\tstruct page *page;\n\tunsigned long entry_offset = swp_offset(entry);\n\tunsigned long offset = entry_offset;\n\tunsigned long start_offset, end_offset;\n\tunsigned long mask;\n\tstruct swap_info_struct *si = swp_swap_info(entry);\n\tstruct blk_plug plug;\n\tstruct swap_iocb *splug = NULL;\n\tbool page_allocated;\n\tstruct vm_area_struct *vma = vmf->vma;\n\tunsigned long addr = vmf->address;\n\n\tmask = swapin_nr_pages(offset) - 1;\n\tif (!mask)\n\t\tgoto skip;\n\n\t \n\tstart_offset = offset & ~mask;\n\tend_offset = offset | mask;\n\tif (!start_offset)\t \n\t\tstart_offset++;\n\tif (end_offset >= si->max)\n\t\tend_offset = si->max - 1;\n\n\tblk_start_plug(&plug);\n\tfor (offset = start_offset; offset <= end_offset ; offset++) {\n\t\t \n\t\tpage = __read_swap_cache_async(\n\t\t\tswp_entry(swp_type(entry), offset),\n\t\t\tgfp_mask, vma, addr, &page_allocated);\n\t\tif (!page)\n\t\t\tcontinue;\n\t\tif (page_allocated) {\n\t\t\tswap_readpage(page, false, &splug);\n\t\t\tif (offset != entry_offset) {\n\t\t\t\tSetPageReadahead(page);\n\t\t\t\tcount_vm_event(SWAP_RA);\n\t\t\t}\n\t\t}\n\t\tput_page(page);\n\t}\n\tblk_finish_plug(&plug);\n\tswap_read_unplug(splug);\n\n\tlru_add_drain();\t \nskip:\n\t \n\treturn read_swap_cache_async(entry, gfp_mask, vma, addr, NULL);\n}\n\nint init_swap_address_space(unsigned int type, unsigned long nr_pages)\n{\n\tstruct address_space *spaces, *space;\n\tunsigned int i, nr;\n\n\tnr = DIV_ROUND_UP(nr_pages, SWAP_ADDRESS_SPACE_PAGES);\n\tspaces = kvcalloc(nr, sizeof(struct address_space), GFP_KERNEL);\n\tif (!spaces)\n\t\treturn -ENOMEM;\n\tfor (i = 0; i < nr; i++) {\n\t\tspace = spaces + i;\n\t\txa_init_flags(&space->i_pages, XA_FLAGS_LOCK_IRQ);\n\t\tatomic_set(&space->i_mmap_writable, 0);\n\t\tspace->a_ops = &swap_aops;\n\t\t \n\t\tmapping_set_no_writeback_tags(space);\n\t}\n\tnr_swapper_spaces[type] = nr;\n\tswapper_spaces[type] = spaces;\n\n\treturn 0;\n}\n\nvoid exit_swap_address_space(unsigned int type)\n{\n\tint i;\n\tstruct address_space *spaces = swapper_spaces[type];\n\n\tfor (i = 0; i < nr_swapper_spaces[type]; i++)\n\t\tVM_WARN_ON_ONCE(!mapping_empty(&spaces[i]));\n\tkvfree(spaces);\n\tnr_swapper_spaces[type] = 0;\n\tswapper_spaces[type] = NULL;\n}\n\n#define SWAP_RA_ORDER_CEILING\t5\n\nstruct vma_swap_readahead {\n\tunsigned short win;\n\tunsigned short offset;\n\tunsigned short nr_pte;\n};\n\nstatic void swap_ra_info(struct vm_fault *vmf,\n\t\t\t struct vma_swap_readahead *ra_info)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tunsigned long ra_val;\n\tunsigned long faddr, pfn, fpfn, lpfn, rpfn;\n\tunsigned long start, end;\n\tunsigned int max_win, hits, prev_win, win;\n\n\tmax_win = 1 << min_t(unsigned int, READ_ONCE(page_cluster),\n\t\t\t     SWAP_RA_ORDER_CEILING);\n\tif (max_win == 1) {\n\t\tra_info->win = 1;\n\t\treturn;\n\t}\n\n\tfaddr = vmf->address;\n\tfpfn = PFN_DOWN(faddr);\n\tra_val = GET_SWAP_RA_VAL(vma);\n\tpfn = PFN_DOWN(SWAP_RA_ADDR(ra_val));\n\tprev_win = SWAP_RA_WIN(ra_val);\n\thits = SWAP_RA_HITS(ra_val);\n\tra_info->win = win = __swapin_nr_pages(pfn, fpfn, hits,\n\t\t\t\t\t       max_win, prev_win);\n\tatomic_long_set(&vma->swap_readahead_info,\n\t\t\tSWAP_RA_VAL(faddr, win, 0));\n\tif (win == 1)\n\t\treturn;\n\n\tif (fpfn == pfn + 1) {\n\t\tlpfn = fpfn;\n\t\trpfn = fpfn + win;\n\t} else if (pfn == fpfn + 1) {\n\t\tlpfn = fpfn - win + 1;\n\t\trpfn = fpfn + 1;\n\t} else {\n\t\tunsigned int left = (win - 1) / 2;\n\n\t\tlpfn = fpfn - left;\n\t\trpfn = fpfn + win - left;\n\t}\n\tstart = max3(lpfn, PFN_DOWN(vma->vm_start),\n\t\t     PFN_DOWN(faddr & PMD_MASK));\n\tend = min3(rpfn, PFN_DOWN(vma->vm_end),\n\t\t   PFN_DOWN((faddr & PMD_MASK) + PMD_SIZE));\n\n\tra_info->nr_pte = end - start;\n\tra_info->offset = fpfn - start;\n}\n\n \nstatic struct page *swap_vma_readahead(swp_entry_t fentry, gfp_t gfp_mask,\n\t\t\t\t       struct vm_fault *vmf)\n{\n\tstruct blk_plug plug;\n\tstruct swap_iocb *splug = NULL;\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct page *page;\n\tpte_t *pte = NULL, pentry;\n\tunsigned long addr;\n\tswp_entry_t entry;\n\tunsigned int i;\n\tbool page_allocated;\n\tstruct vma_swap_readahead ra_info = {\n\t\t.win = 1,\n\t};\n\n\tswap_ra_info(vmf, &ra_info);\n\tif (ra_info.win == 1)\n\t\tgoto skip;\n\n\taddr = vmf->address - (ra_info.offset * PAGE_SIZE);\n\n\tblk_start_plug(&plug);\n\tfor (i = 0; i < ra_info.nr_pte; i++, addr += PAGE_SIZE) {\n\t\tif (!pte++) {\n\t\t\tpte = pte_offset_map(vmf->pmd, addr);\n\t\t\tif (!pte)\n\t\t\t\tbreak;\n\t\t}\n\t\tpentry = ptep_get_lockless(pte);\n\t\tif (!is_swap_pte(pentry))\n\t\t\tcontinue;\n\t\tentry = pte_to_swp_entry(pentry);\n\t\tif (unlikely(non_swap_entry(entry)))\n\t\t\tcontinue;\n\t\tpte_unmap(pte);\n\t\tpte = NULL;\n\t\tpage = __read_swap_cache_async(entry, gfp_mask, vma,\n\t\t\t\t\t       addr, &page_allocated);\n\t\tif (!page)\n\t\t\tcontinue;\n\t\tif (page_allocated) {\n\t\t\tswap_readpage(page, false, &splug);\n\t\t\tif (i != ra_info.offset) {\n\t\t\t\tSetPageReadahead(page);\n\t\t\t\tcount_vm_event(SWAP_RA);\n\t\t\t}\n\t\t}\n\t\tput_page(page);\n\t}\n\tif (pte)\n\t\tpte_unmap(pte);\n\tblk_finish_plug(&plug);\n\tswap_read_unplug(splug);\n\tlru_add_drain();\nskip:\n\t \n\treturn read_swap_cache_async(fentry, gfp_mask, vma, vmf->address,\n\t\t\t\t     NULL);\n}\n\n \nstruct page *swapin_readahead(swp_entry_t entry, gfp_t gfp_mask,\n\t\t\t\tstruct vm_fault *vmf)\n{\n\treturn swap_use_vma_readahead() ?\n\t\t\tswap_vma_readahead(entry, gfp_mask, vmf) :\n\t\t\tswap_cluster_readahead(entry, gfp_mask, vmf);\n}\n\n#ifdef CONFIG_SYSFS\nstatic ssize_t vma_ra_enabled_show(struct kobject *kobj,\n\t\t\t\t     struct kobj_attribute *attr, char *buf)\n{\n\treturn sysfs_emit(buf, \"%s\\n\",\n\t\t\t  enable_vma_readahead ? \"true\" : \"false\");\n}\nstatic ssize_t vma_ra_enabled_store(struct kobject *kobj,\n\t\t\t\t      struct kobj_attribute *attr,\n\t\t\t\t      const char *buf, size_t count)\n{\n\tssize_t ret;\n\n\tret = kstrtobool(buf, &enable_vma_readahead);\n\tif (ret)\n\t\treturn ret;\n\n\treturn count;\n}\nstatic struct kobj_attribute vma_ra_enabled_attr = __ATTR_RW(vma_ra_enabled);\n\nstatic struct attribute *swap_attrs[] = {\n\t&vma_ra_enabled_attr.attr,\n\tNULL,\n};\n\nstatic const struct attribute_group swap_attr_group = {\n\t.attrs = swap_attrs,\n};\n\nstatic int __init swap_init_sysfs(void)\n{\n\tint err;\n\tstruct kobject *swap_kobj;\n\n\tswap_kobj = kobject_create_and_add(\"swap\", mm_kobj);\n\tif (!swap_kobj) {\n\t\tpr_err(\"failed to create swap kobject\\n\");\n\t\treturn -ENOMEM;\n\t}\n\terr = sysfs_create_group(swap_kobj, &swap_attr_group);\n\tif (err) {\n\t\tpr_err(\"failed to register swap group\\n\");\n\t\tgoto delete_obj;\n\t}\n\treturn 0;\n\ndelete_obj:\n\tkobject_put(swap_kobj);\n\treturn err;\n}\nsubsys_initcall(swap_init_sysfs);\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}