{
  "module_name": "vmscan.c",
  "hash_id": "c941f9734d9e68cb5c7562d1b3545a21fbfa8373cc6ef265df2e18469f766422",
  "original_prompt": "Ingested from linux-6.6.14/mm/vmscan.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/mm.h>\n#include <linux/sched/mm.h>\n#include <linux/module.h>\n#include <linux/gfp.h>\n#include <linux/kernel_stat.h>\n#include <linux/swap.h>\n#include <linux/pagemap.h>\n#include <linux/init.h>\n#include <linux/highmem.h>\n#include <linux/vmpressure.h>\n#include <linux/vmstat.h>\n#include <linux/file.h>\n#include <linux/writeback.h>\n#include <linux/blkdev.h>\n#include <linux/buffer_head.h>\t \n#include <linux/mm_inline.h>\n#include <linux/backing-dev.h>\n#include <linux/rmap.h>\n#include <linux/topology.h>\n#include <linux/cpu.h>\n#include <linux/cpuset.h>\n#include <linux/compaction.h>\n#include <linux/notifier.h>\n#include <linux/rwsem.h>\n#include <linux/delay.h>\n#include <linux/kthread.h>\n#include <linux/freezer.h>\n#include <linux/memcontrol.h>\n#include <linux/migrate.h>\n#include <linux/delayacct.h>\n#include <linux/sysctl.h>\n#include <linux/memory-tiers.h>\n#include <linux/oom.h>\n#include <linux/pagevec.h>\n#include <linux/prefetch.h>\n#include <linux/printk.h>\n#include <linux/dax.h>\n#include <linux/psi.h>\n#include <linux/pagewalk.h>\n#include <linux/shmem_fs.h>\n#include <linux/ctype.h>\n#include <linux/debugfs.h>\n#include <linux/khugepaged.h>\n#include <linux/rculist_nulls.h>\n#include <linux/random.h>\n\n#include <asm/tlbflush.h>\n#include <asm/div64.h>\n\n#include <linux/swapops.h>\n#include <linux/balloon_compaction.h>\n#include <linux/sched/sysctl.h>\n\n#include \"internal.h\"\n#include \"swap.h\"\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/vmscan.h>\n\nstruct scan_control {\n\t \n\tunsigned long nr_to_reclaim;\n\n\t \n\tnodemask_t\t*nodemask;\n\n\t \n\tstruct mem_cgroup *target_mem_cgroup;\n\n\t \n\tunsigned long\tanon_cost;\n\tunsigned long\tfile_cost;\n\n\t \n#define DEACTIVATE_ANON 1\n#define DEACTIVATE_FILE 2\n\tunsigned int may_deactivate:2;\n\tunsigned int force_deactivate:1;\n\tunsigned int skipped_deactivate:1;\n\n\t \n\tunsigned int may_writepage:1;\n\n\t \n\tunsigned int may_unmap:1;\n\n\t \n\tunsigned int may_swap:1;\n\n\t \n\tunsigned int proactive:1;\n\n\t \n\tunsigned int memcg_low_reclaim:1;\n\tunsigned int memcg_low_skipped:1;\n\n\tunsigned int hibernation_mode:1;\n\n\t \n\tunsigned int compaction_ready:1;\n\n\t \n\tunsigned int cache_trim_mode:1;\n\n\t \n\tunsigned int file_is_tiny:1;\n\n\t \n\tunsigned int no_demotion:1;\n\n\t \n\ts8 order;\n\n\t \n\ts8 priority;\n\n\t \n\ts8 reclaim_idx;\n\n\t \n\tgfp_t gfp_mask;\n\n\t \n\tunsigned long nr_scanned;\n\n\t \n\tunsigned long nr_reclaimed;\n\n\tstruct {\n\t\tunsigned int dirty;\n\t\tunsigned int unqueued_dirty;\n\t\tunsigned int congested;\n\t\tunsigned int writeback;\n\t\tunsigned int immediate;\n\t\tunsigned int file_taken;\n\t\tunsigned int taken;\n\t} nr;\n\n\t \n\tstruct reclaim_state reclaim_state;\n};\n\n#ifdef ARCH_HAS_PREFETCHW\n#define prefetchw_prev_lru_folio(_folio, _base, _field)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif ((_folio)->lru.prev != _base) {\t\t\t\\\n\t\t\tstruct folio *prev;\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\t\tprev = lru_to_folio(&(_folio->lru));\t\t\\\n\t\t\tprefetchw(&prev->_field);\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t} while (0)\n#else\n#define prefetchw_prev_lru_folio(_folio, _base, _field) do { } while (0)\n#endif\n\n \nint vm_swappiness = 60;\n\nLIST_HEAD(shrinker_list);\nDECLARE_RWSEM(shrinker_rwsem);\n\n#ifdef CONFIG_MEMCG\nstatic int shrinker_nr_max;\n\n \nstatic inline int shrinker_map_size(int nr_items)\n{\n\treturn (DIV_ROUND_UP(nr_items, BITS_PER_LONG) * sizeof(unsigned long));\n}\n\nstatic inline int shrinker_defer_size(int nr_items)\n{\n\treturn (round_up(nr_items, BITS_PER_LONG) * sizeof(atomic_long_t));\n}\n\nstatic struct shrinker_info *shrinker_info_protected(struct mem_cgroup *memcg,\n\t\t\t\t\t\t     int nid)\n{\n\treturn rcu_dereference_protected(memcg->nodeinfo[nid]->shrinker_info,\n\t\t\t\t\t lockdep_is_held(&shrinker_rwsem));\n}\n\nstatic int expand_one_shrinker_info(struct mem_cgroup *memcg,\n\t\t\t\t    int map_size, int defer_size,\n\t\t\t\t    int old_map_size, int old_defer_size,\n\t\t\t\t    int new_nr_max)\n{\n\tstruct shrinker_info *new, *old;\n\tstruct mem_cgroup_per_node *pn;\n\tint nid;\n\tint size = map_size + defer_size;\n\n\tfor_each_node(nid) {\n\t\tpn = memcg->nodeinfo[nid];\n\t\told = shrinker_info_protected(memcg, nid);\n\t\t \n\t\tif (!old)\n\t\t\treturn 0;\n\n\t\t \n\t\tif (new_nr_max <= old->map_nr_max)\n\t\t\tcontinue;\n\n\t\tnew = kvmalloc_node(sizeof(*new) + size, GFP_KERNEL, nid);\n\t\tif (!new)\n\t\t\treturn -ENOMEM;\n\n\t\tnew->nr_deferred = (atomic_long_t *)(new + 1);\n\t\tnew->map = (void *)new->nr_deferred + defer_size;\n\t\tnew->map_nr_max = new_nr_max;\n\n\t\t \n\t\tmemset(new->map, (int)0xff, old_map_size);\n\t\tmemset((void *)new->map + old_map_size, 0, map_size - old_map_size);\n\t\t \n\t\tmemcpy(new->nr_deferred, old->nr_deferred, old_defer_size);\n\t\tmemset((void *)new->nr_deferred + old_defer_size, 0,\n\t\t       defer_size - old_defer_size);\n\n\t\trcu_assign_pointer(pn->shrinker_info, new);\n\t\tkvfree_rcu(old, rcu);\n\t}\n\n\treturn 0;\n}\n\nvoid free_shrinker_info(struct mem_cgroup *memcg)\n{\n\tstruct mem_cgroup_per_node *pn;\n\tstruct shrinker_info *info;\n\tint nid;\n\n\tfor_each_node(nid) {\n\t\tpn = memcg->nodeinfo[nid];\n\t\tinfo = rcu_dereference_protected(pn->shrinker_info, true);\n\t\tkvfree(info);\n\t\trcu_assign_pointer(pn->shrinker_info, NULL);\n\t}\n}\n\nint alloc_shrinker_info(struct mem_cgroup *memcg)\n{\n\tstruct shrinker_info *info;\n\tint nid, size, ret = 0;\n\tint map_size, defer_size = 0;\n\n\tdown_write(&shrinker_rwsem);\n\tmap_size = shrinker_map_size(shrinker_nr_max);\n\tdefer_size = shrinker_defer_size(shrinker_nr_max);\n\tsize = map_size + defer_size;\n\tfor_each_node(nid) {\n\t\tinfo = kvzalloc_node(sizeof(*info) + size, GFP_KERNEL, nid);\n\t\tif (!info) {\n\t\t\tfree_shrinker_info(memcg);\n\t\t\tret = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\t\tinfo->nr_deferred = (atomic_long_t *)(info + 1);\n\t\tinfo->map = (void *)info->nr_deferred + defer_size;\n\t\tinfo->map_nr_max = shrinker_nr_max;\n\t\trcu_assign_pointer(memcg->nodeinfo[nid]->shrinker_info, info);\n\t}\n\tup_write(&shrinker_rwsem);\n\n\treturn ret;\n}\n\nstatic int expand_shrinker_info(int new_id)\n{\n\tint ret = 0;\n\tint new_nr_max = round_up(new_id + 1, BITS_PER_LONG);\n\tint map_size, defer_size = 0;\n\tint old_map_size, old_defer_size = 0;\n\tstruct mem_cgroup *memcg;\n\n\tif (!root_mem_cgroup)\n\t\tgoto out;\n\n\tlockdep_assert_held(&shrinker_rwsem);\n\n\tmap_size = shrinker_map_size(new_nr_max);\n\tdefer_size = shrinker_defer_size(new_nr_max);\n\told_map_size = shrinker_map_size(shrinker_nr_max);\n\told_defer_size = shrinker_defer_size(shrinker_nr_max);\n\n\tmemcg = mem_cgroup_iter(NULL, NULL, NULL);\n\tdo {\n\t\tret = expand_one_shrinker_info(memcg, map_size, defer_size,\n\t\t\t\t\t       old_map_size, old_defer_size,\n\t\t\t\t\t       new_nr_max);\n\t\tif (ret) {\n\t\t\tmem_cgroup_iter_break(NULL, memcg);\n\t\t\tgoto out;\n\t\t}\n\t} while ((memcg = mem_cgroup_iter(NULL, memcg, NULL)) != NULL);\nout:\n\tif (!ret)\n\t\tshrinker_nr_max = new_nr_max;\n\n\treturn ret;\n}\n\nvoid set_shrinker_bit(struct mem_cgroup *memcg, int nid, int shrinker_id)\n{\n\tif (shrinker_id >= 0 && memcg && !mem_cgroup_is_root(memcg)) {\n\t\tstruct shrinker_info *info;\n\n\t\trcu_read_lock();\n\t\tinfo = rcu_dereference(memcg->nodeinfo[nid]->shrinker_info);\n\t\tif (!WARN_ON_ONCE(shrinker_id >= info->map_nr_max)) {\n\t\t\t \n\t\t\tsmp_mb__before_atomic();\n\t\t\tset_bit(shrinker_id, info->map);\n\t\t}\n\t\trcu_read_unlock();\n\t}\n}\n\nstatic DEFINE_IDR(shrinker_idr);\n\nstatic int prealloc_memcg_shrinker(struct shrinker *shrinker)\n{\n\tint id, ret = -ENOMEM;\n\n\tif (mem_cgroup_disabled())\n\t\treturn -ENOSYS;\n\n\tdown_write(&shrinker_rwsem);\n\t \n\tid = idr_alloc(&shrinker_idr, shrinker, 0, 0, GFP_KERNEL);\n\tif (id < 0)\n\t\tgoto unlock;\n\n\tif (id >= shrinker_nr_max) {\n\t\tif (expand_shrinker_info(id)) {\n\t\t\tidr_remove(&shrinker_idr, id);\n\t\t\tgoto unlock;\n\t\t}\n\t}\n\tshrinker->id = id;\n\tret = 0;\nunlock:\n\tup_write(&shrinker_rwsem);\n\treturn ret;\n}\n\nstatic void unregister_memcg_shrinker(struct shrinker *shrinker)\n{\n\tint id = shrinker->id;\n\n\tBUG_ON(id < 0);\n\n\tlockdep_assert_held(&shrinker_rwsem);\n\n\tidr_remove(&shrinker_idr, id);\n}\n\nstatic long xchg_nr_deferred_memcg(int nid, struct shrinker *shrinker,\n\t\t\t\t   struct mem_cgroup *memcg)\n{\n\tstruct shrinker_info *info;\n\n\tinfo = shrinker_info_protected(memcg, nid);\n\treturn atomic_long_xchg(&info->nr_deferred[shrinker->id], 0);\n}\n\nstatic long add_nr_deferred_memcg(long nr, int nid, struct shrinker *shrinker,\n\t\t\t\t  struct mem_cgroup *memcg)\n{\n\tstruct shrinker_info *info;\n\n\tinfo = shrinker_info_protected(memcg, nid);\n\treturn atomic_long_add_return(nr, &info->nr_deferred[shrinker->id]);\n}\n\nvoid reparent_shrinker_deferred(struct mem_cgroup *memcg)\n{\n\tint i, nid;\n\tlong nr;\n\tstruct mem_cgroup *parent;\n\tstruct shrinker_info *child_info, *parent_info;\n\n\tparent = parent_mem_cgroup(memcg);\n\tif (!parent)\n\t\tparent = root_mem_cgroup;\n\n\t \n\tdown_read(&shrinker_rwsem);\n\tfor_each_node(nid) {\n\t\tchild_info = shrinker_info_protected(memcg, nid);\n\t\tparent_info = shrinker_info_protected(parent, nid);\n\t\tfor (i = 0; i < child_info->map_nr_max; i++) {\n\t\t\tnr = atomic_long_read(&child_info->nr_deferred[i]);\n\t\t\tatomic_long_add(nr, &parent_info->nr_deferred[i]);\n\t\t}\n\t}\n\tup_read(&shrinker_rwsem);\n}\n\n \nstatic bool cgroup_reclaim(struct scan_control *sc)\n{\n\treturn sc->target_mem_cgroup;\n}\n\n \nstatic bool root_reclaim(struct scan_control *sc)\n{\n\treturn !sc->target_mem_cgroup || mem_cgroup_is_root(sc->target_mem_cgroup);\n}\n\n \nstatic bool writeback_throttling_sane(struct scan_control *sc)\n{\n\tif (!cgroup_reclaim(sc))\n\t\treturn true;\n#ifdef CONFIG_CGROUP_WRITEBACK\n\tif (cgroup_subsys_on_dfl(memory_cgrp_subsys))\n\t\treturn true;\n#endif\n\treturn false;\n}\n#else\nstatic int prealloc_memcg_shrinker(struct shrinker *shrinker)\n{\n\treturn -ENOSYS;\n}\n\nstatic void unregister_memcg_shrinker(struct shrinker *shrinker)\n{\n}\n\nstatic long xchg_nr_deferred_memcg(int nid, struct shrinker *shrinker,\n\t\t\t\t   struct mem_cgroup *memcg)\n{\n\treturn 0;\n}\n\nstatic long add_nr_deferred_memcg(long nr, int nid, struct shrinker *shrinker,\n\t\t\t\t  struct mem_cgroup *memcg)\n{\n\treturn 0;\n}\n\nstatic bool cgroup_reclaim(struct scan_control *sc)\n{\n\treturn false;\n}\n\nstatic bool root_reclaim(struct scan_control *sc)\n{\n\treturn true;\n}\n\nstatic bool writeback_throttling_sane(struct scan_control *sc)\n{\n\treturn true;\n}\n#endif\n\nstatic void set_task_reclaim_state(struct task_struct *task,\n\t\t\t\t   struct reclaim_state *rs)\n{\n\t \n\tWARN_ON_ONCE(rs && task->reclaim_state);\n\n\t \n\tWARN_ON_ONCE(!rs && !task->reclaim_state);\n\n\ttask->reclaim_state = rs;\n}\n\n \nstatic void flush_reclaim_state(struct scan_control *sc)\n{\n\t \n\tif (current->reclaim_state && root_reclaim(sc)) {\n\t\tsc->nr_reclaimed += current->reclaim_state->reclaimed;\n\t\tcurrent->reclaim_state->reclaimed = 0;\n\t}\n}\n\nstatic long xchg_nr_deferred(struct shrinker *shrinker,\n\t\t\t     struct shrink_control *sc)\n{\n\tint nid = sc->nid;\n\n\tif (!(shrinker->flags & SHRINKER_NUMA_AWARE))\n\t\tnid = 0;\n\n\tif (sc->memcg &&\n\t    (shrinker->flags & SHRINKER_MEMCG_AWARE))\n\t\treturn xchg_nr_deferred_memcg(nid, shrinker,\n\t\t\t\t\t      sc->memcg);\n\n\treturn atomic_long_xchg(&shrinker->nr_deferred[nid], 0);\n}\n\n\nstatic long add_nr_deferred(long nr, struct shrinker *shrinker,\n\t\t\t    struct shrink_control *sc)\n{\n\tint nid = sc->nid;\n\n\tif (!(shrinker->flags & SHRINKER_NUMA_AWARE))\n\t\tnid = 0;\n\n\tif (sc->memcg &&\n\t    (shrinker->flags & SHRINKER_MEMCG_AWARE))\n\t\treturn add_nr_deferred_memcg(nr, nid, shrinker,\n\t\t\t\t\t     sc->memcg);\n\n\treturn atomic_long_add_return(nr, &shrinker->nr_deferred[nid]);\n}\n\nstatic bool can_demote(int nid, struct scan_control *sc)\n{\n\tif (!numa_demotion_enabled)\n\t\treturn false;\n\tif (sc && sc->no_demotion)\n\t\treturn false;\n\tif (next_demotion_node(nid) == NUMA_NO_NODE)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic inline bool can_reclaim_anon_pages(struct mem_cgroup *memcg,\n\t\t\t\t\t  int nid,\n\t\t\t\t\t  struct scan_control *sc)\n{\n\tif (memcg == NULL) {\n\t\t \n\t\tif (get_nr_swap_pages() > 0)\n\t\t\treturn true;\n\t} else {\n\t\t \n\t\tif (mem_cgroup_get_nr_swap_pages(memcg) > 0)\n\t\t\treturn true;\n\t}\n\n\t \n\treturn can_demote(nid, sc);\n}\n\n \nunsigned long zone_reclaimable_pages(struct zone *zone)\n{\n\tunsigned long nr;\n\n\tnr = zone_page_state_snapshot(zone, NR_ZONE_INACTIVE_FILE) +\n\t\tzone_page_state_snapshot(zone, NR_ZONE_ACTIVE_FILE);\n\tif (can_reclaim_anon_pages(NULL, zone_to_nid(zone), NULL))\n\t\tnr += zone_page_state_snapshot(zone, NR_ZONE_INACTIVE_ANON) +\n\t\t\tzone_page_state_snapshot(zone, NR_ZONE_ACTIVE_ANON);\n\n\treturn nr;\n}\n\n \nstatic unsigned long lruvec_lru_size(struct lruvec *lruvec, enum lru_list lru,\n\t\t\t\t     int zone_idx)\n{\n\tunsigned long size = 0;\n\tint zid;\n\n\tfor (zid = 0; zid <= zone_idx; zid++) {\n\t\tstruct zone *zone = &lruvec_pgdat(lruvec)->node_zones[zid];\n\n\t\tif (!managed_zone(zone))\n\t\t\tcontinue;\n\n\t\tif (!mem_cgroup_disabled())\n\t\t\tsize += mem_cgroup_get_zone_lru_size(lruvec, lru, zid);\n\t\telse\n\t\t\tsize += zone_page_state(zone, NR_ZONE_LRU_BASE + lru);\n\t}\n\treturn size;\n}\n\n \nstatic int __prealloc_shrinker(struct shrinker *shrinker)\n{\n\tunsigned int size;\n\tint err;\n\n\tif (shrinker->flags & SHRINKER_MEMCG_AWARE) {\n\t\terr = prealloc_memcg_shrinker(shrinker);\n\t\tif (err != -ENOSYS)\n\t\t\treturn err;\n\n\t\tshrinker->flags &= ~SHRINKER_MEMCG_AWARE;\n\t}\n\n\tsize = sizeof(*shrinker->nr_deferred);\n\tif (shrinker->flags & SHRINKER_NUMA_AWARE)\n\t\tsize *= nr_node_ids;\n\n\tshrinker->nr_deferred = kzalloc(size, GFP_KERNEL);\n\tif (!shrinker->nr_deferred)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\n#ifdef CONFIG_SHRINKER_DEBUG\nint prealloc_shrinker(struct shrinker *shrinker, const char *fmt, ...)\n{\n\tva_list ap;\n\tint err;\n\n\tva_start(ap, fmt);\n\tshrinker->name = kvasprintf_const(GFP_KERNEL, fmt, ap);\n\tva_end(ap);\n\tif (!shrinker->name)\n\t\treturn -ENOMEM;\n\n\terr = __prealloc_shrinker(shrinker);\n\tif (err) {\n\t\tkfree_const(shrinker->name);\n\t\tshrinker->name = NULL;\n\t}\n\n\treturn err;\n}\n#else\nint prealloc_shrinker(struct shrinker *shrinker, const char *fmt, ...)\n{\n\treturn __prealloc_shrinker(shrinker);\n}\n#endif\n\nvoid free_prealloced_shrinker(struct shrinker *shrinker)\n{\n#ifdef CONFIG_SHRINKER_DEBUG\n\tkfree_const(shrinker->name);\n\tshrinker->name = NULL;\n#endif\n\tif (shrinker->flags & SHRINKER_MEMCG_AWARE) {\n\t\tdown_write(&shrinker_rwsem);\n\t\tunregister_memcg_shrinker(shrinker);\n\t\tup_write(&shrinker_rwsem);\n\t\treturn;\n\t}\n\n\tkfree(shrinker->nr_deferred);\n\tshrinker->nr_deferred = NULL;\n}\n\nvoid register_shrinker_prepared(struct shrinker *shrinker)\n{\n\tdown_write(&shrinker_rwsem);\n\tlist_add_tail(&shrinker->list, &shrinker_list);\n\tshrinker->flags |= SHRINKER_REGISTERED;\n\tshrinker_debugfs_add(shrinker);\n\tup_write(&shrinker_rwsem);\n}\n\nstatic int __register_shrinker(struct shrinker *shrinker)\n{\n\tint err = __prealloc_shrinker(shrinker);\n\n\tif (err)\n\t\treturn err;\n\tregister_shrinker_prepared(shrinker);\n\treturn 0;\n}\n\n#ifdef CONFIG_SHRINKER_DEBUG\nint register_shrinker(struct shrinker *shrinker, const char *fmt, ...)\n{\n\tva_list ap;\n\tint err;\n\n\tva_start(ap, fmt);\n\tshrinker->name = kvasprintf_const(GFP_KERNEL, fmt, ap);\n\tva_end(ap);\n\tif (!shrinker->name)\n\t\treturn -ENOMEM;\n\n\terr = __register_shrinker(shrinker);\n\tif (err) {\n\t\tkfree_const(shrinker->name);\n\t\tshrinker->name = NULL;\n\t}\n\treturn err;\n}\n#else\nint register_shrinker(struct shrinker *shrinker, const char *fmt, ...)\n{\n\treturn __register_shrinker(shrinker);\n}\n#endif\nEXPORT_SYMBOL(register_shrinker);\n\n \nvoid unregister_shrinker(struct shrinker *shrinker)\n{\n\tstruct dentry *debugfs_entry;\n\tint debugfs_id;\n\n\tif (!(shrinker->flags & SHRINKER_REGISTERED))\n\t\treturn;\n\n\tdown_write(&shrinker_rwsem);\n\tlist_del(&shrinker->list);\n\tshrinker->flags &= ~SHRINKER_REGISTERED;\n\tif (shrinker->flags & SHRINKER_MEMCG_AWARE)\n\t\tunregister_memcg_shrinker(shrinker);\n\tdebugfs_entry = shrinker_debugfs_detach(shrinker, &debugfs_id);\n\tup_write(&shrinker_rwsem);\n\n\tshrinker_debugfs_remove(debugfs_entry, debugfs_id);\n\n\tkfree(shrinker->nr_deferred);\n\tshrinker->nr_deferred = NULL;\n}\nEXPORT_SYMBOL(unregister_shrinker);\n\n \nvoid synchronize_shrinkers(void)\n{\n\tdown_write(&shrinker_rwsem);\n\tup_write(&shrinker_rwsem);\n}\nEXPORT_SYMBOL(synchronize_shrinkers);\n\n#define SHRINK_BATCH 128\n\nstatic unsigned long do_shrink_slab(struct shrink_control *shrinkctl,\n\t\t\t\t    struct shrinker *shrinker, int priority)\n{\n\tunsigned long freed = 0;\n\tunsigned long long delta;\n\tlong total_scan;\n\tlong freeable;\n\tlong nr;\n\tlong new_nr;\n\tlong batch_size = shrinker->batch ? shrinker->batch\n\t\t\t\t\t  : SHRINK_BATCH;\n\tlong scanned = 0, next_deferred;\n\n\tfreeable = shrinker->count_objects(shrinker, shrinkctl);\n\tif (freeable == 0 || freeable == SHRINK_EMPTY)\n\t\treturn freeable;\n\n\t \n\tnr = xchg_nr_deferred(shrinker, shrinkctl);\n\n\tif (shrinker->seeks) {\n\t\tdelta = freeable >> priority;\n\t\tdelta *= 4;\n\t\tdo_div(delta, shrinker->seeks);\n\t} else {\n\t\t \n\t\tdelta = freeable / 2;\n\t}\n\n\ttotal_scan = nr >> priority;\n\ttotal_scan += delta;\n\ttotal_scan = min(total_scan, (2 * freeable));\n\n\ttrace_mm_shrink_slab_start(shrinker, shrinkctl, nr,\n\t\t\t\t   freeable, delta, total_scan, priority);\n\n\t \n\twhile (total_scan >= batch_size ||\n\t       total_scan >= freeable) {\n\t\tunsigned long ret;\n\t\tunsigned long nr_to_scan = min(batch_size, total_scan);\n\n\t\tshrinkctl->nr_to_scan = nr_to_scan;\n\t\tshrinkctl->nr_scanned = nr_to_scan;\n\t\tret = shrinker->scan_objects(shrinker, shrinkctl);\n\t\tif (ret == SHRINK_STOP)\n\t\t\tbreak;\n\t\tfreed += ret;\n\n\t\tcount_vm_events(SLABS_SCANNED, shrinkctl->nr_scanned);\n\t\ttotal_scan -= shrinkctl->nr_scanned;\n\t\tscanned += shrinkctl->nr_scanned;\n\n\t\tcond_resched();\n\t}\n\n\t \n\tnext_deferred = max_t(long, (nr + delta - scanned), 0);\n\tnext_deferred = min(next_deferred, (2 * freeable));\n\n\t \n\tnew_nr = add_nr_deferred(next_deferred, shrinker, shrinkctl);\n\n\ttrace_mm_shrink_slab_end(shrinker, shrinkctl->nid, freed, nr, new_nr, total_scan);\n\treturn freed;\n}\n\n#ifdef CONFIG_MEMCG\nstatic unsigned long shrink_slab_memcg(gfp_t gfp_mask, int nid,\n\t\t\tstruct mem_cgroup *memcg, int priority)\n{\n\tstruct shrinker_info *info;\n\tunsigned long ret, freed = 0;\n\tint i;\n\n\tif (!mem_cgroup_online(memcg))\n\t\treturn 0;\n\n\tif (!down_read_trylock(&shrinker_rwsem))\n\t\treturn 0;\n\n\tinfo = shrinker_info_protected(memcg, nid);\n\tif (unlikely(!info))\n\t\tgoto unlock;\n\n\tfor_each_set_bit(i, info->map, info->map_nr_max) {\n\t\tstruct shrink_control sc = {\n\t\t\t.gfp_mask = gfp_mask,\n\t\t\t.nid = nid,\n\t\t\t.memcg = memcg,\n\t\t};\n\t\tstruct shrinker *shrinker;\n\n\t\tshrinker = idr_find(&shrinker_idr, i);\n\t\tif (unlikely(!shrinker || !(shrinker->flags & SHRINKER_REGISTERED))) {\n\t\t\tif (!shrinker)\n\t\t\t\tclear_bit(i, info->map);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (!memcg_kmem_online() &&\n\t\t    !(shrinker->flags & SHRINKER_NONSLAB))\n\t\t\tcontinue;\n\n\t\tret = do_shrink_slab(&sc, shrinker, priority);\n\t\tif (ret == SHRINK_EMPTY) {\n\t\t\tclear_bit(i, info->map);\n\t\t\t \n\t\t\tsmp_mb__after_atomic();\n\t\t\tret = do_shrink_slab(&sc, shrinker, priority);\n\t\t\tif (ret == SHRINK_EMPTY)\n\t\t\t\tret = 0;\n\t\t\telse\n\t\t\t\tset_shrinker_bit(memcg, nid, i);\n\t\t}\n\t\tfreed += ret;\n\n\t\tif (rwsem_is_contended(&shrinker_rwsem)) {\n\t\t\tfreed = freed ? : 1;\n\t\t\tbreak;\n\t\t}\n\t}\nunlock:\n\tup_read(&shrinker_rwsem);\n\treturn freed;\n}\n#else  \nstatic unsigned long shrink_slab_memcg(gfp_t gfp_mask, int nid,\n\t\t\tstruct mem_cgroup *memcg, int priority)\n{\n\treturn 0;\n}\n#endif  \n\n \nstatic unsigned long shrink_slab(gfp_t gfp_mask, int nid,\n\t\t\t\t struct mem_cgroup *memcg,\n\t\t\t\t int priority)\n{\n\tunsigned long ret, freed = 0;\n\tstruct shrinker *shrinker;\n\n\t \n\tif (!mem_cgroup_disabled() && !mem_cgroup_is_root(memcg))\n\t\treturn shrink_slab_memcg(gfp_mask, nid, memcg, priority);\n\n\tif (!down_read_trylock(&shrinker_rwsem))\n\t\tgoto out;\n\n\tlist_for_each_entry(shrinker, &shrinker_list, list) {\n\t\tstruct shrink_control sc = {\n\t\t\t.gfp_mask = gfp_mask,\n\t\t\t.nid = nid,\n\t\t\t.memcg = memcg,\n\t\t};\n\n\t\tret = do_shrink_slab(&sc, shrinker, priority);\n\t\tif (ret == SHRINK_EMPTY)\n\t\t\tret = 0;\n\t\tfreed += ret;\n\t\t \n\t\tif (rwsem_is_contended(&shrinker_rwsem)) {\n\t\t\tfreed = freed ? : 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tup_read(&shrinker_rwsem);\nout:\n\tcond_resched();\n\treturn freed;\n}\n\nstatic unsigned long drop_slab_node(int nid)\n{\n\tunsigned long freed = 0;\n\tstruct mem_cgroup *memcg = NULL;\n\n\tmemcg = mem_cgroup_iter(NULL, NULL, NULL);\n\tdo {\n\t\tfreed += shrink_slab(GFP_KERNEL, nid, memcg, 0);\n\t} while ((memcg = mem_cgroup_iter(NULL, memcg, NULL)) != NULL);\n\n\treturn freed;\n}\n\nvoid drop_slab(void)\n{\n\tint nid;\n\tint shift = 0;\n\tunsigned long freed;\n\n\tdo {\n\t\tfreed = 0;\n\t\tfor_each_online_node(nid) {\n\t\t\tif (fatal_signal_pending(current))\n\t\t\t\treturn;\n\n\t\t\tfreed += drop_slab_node(nid);\n\t\t}\n\t} while ((freed >> shift++) > 1);\n}\n\nstatic int reclaimer_offset(void)\n{\n\tBUILD_BUG_ON(PGSTEAL_DIRECT - PGSTEAL_KSWAPD !=\n\t\t\tPGDEMOTE_DIRECT - PGDEMOTE_KSWAPD);\n\tBUILD_BUG_ON(PGSTEAL_DIRECT - PGSTEAL_KSWAPD !=\n\t\t\tPGSCAN_DIRECT - PGSCAN_KSWAPD);\n\tBUILD_BUG_ON(PGSTEAL_KHUGEPAGED - PGSTEAL_KSWAPD !=\n\t\t\tPGDEMOTE_KHUGEPAGED - PGDEMOTE_KSWAPD);\n\tBUILD_BUG_ON(PGSTEAL_KHUGEPAGED - PGSTEAL_KSWAPD !=\n\t\t\tPGSCAN_KHUGEPAGED - PGSCAN_KSWAPD);\n\n\tif (current_is_kswapd())\n\t\treturn 0;\n\tif (current_is_khugepaged())\n\t\treturn PGSTEAL_KHUGEPAGED - PGSTEAL_KSWAPD;\n\treturn PGSTEAL_DIRECT - PGSTEAL_KSWAPD;\n}\n\nstatic inline int is_page_cache_freeable(struct folio *folio)\n{\n\t \n\treturn folio_ref_count(folio) - folio_test_private(folio) ==\n\t\t1 + folio_nr_pages(folio);\n}\n\n \nstatic void handle_write_error(struct address_space *mapping,\n\t\t\t\tstruct folio *folio, int error)\n{\n\tfolio_lock(folio);\n\tif (folio_mapping(folio) == mapping)\n\t\tmapping_set_error(mapping, error);\n\tfolio_unlock(folio);\n}\n\nstatic bool skip_throttle_noprogress(pg_data_t *pgdat)\n{\n\tint reclaimable = 0, write_pending = 0;\n\tint i;\n\n\t \n\tif (pgdat->kswapd_failures >= MAX_RECLAIM_RETRIES)\n\t\treturn true;\n\n\t \n\tfor (i = 0; i < MAX_NR_ZONES; i++) {\n\t\tstruct zone *zone = pgdat->node_zones + i;\n\n\t\tif (!managed_zone(zone))\n\t\t\tcontinue;\n\n\t\treclaimable += zone_reclaimable_pages(zone);\n\t\twrite_pending += zone_page_state_snapshot(zone,\n\t\t\t\t\t\t  NR_ZONE_WRITE_PENDING);\n\t}\n\tif (2 * write_pending <= reclaimable)\n\t\treturn true;\n\n\treturn false;\n}\n\nvoid reclaim_throttle(pg_data_t *pgdat, enum vmscan_throttle_state reason)\n{\n\twait_queue_head_t *wqh = &pgdat->reclaim_wait[reason];\n\tlong timeout, ret;\n\tDEFINE_WAIT(wait);\n\n\t \n\tif (!current_is_kswapd() &&\n\t    current->flags & (PF_USER_WORKER|PF_KTHREAD)) {\n\t\tcond_resched();\n\t\treturn;\n\t}\n\n\t \n\tswitch(reason) {\n\tcase VMSCAN_THROTTLE_WRITEBACK:\n\t\ttimeout = HZ/10;\n\n\t\tif (atomic_inc_return(&pgdat->nr_writeback_throttled) == 1) {\n\t\t\tWRITE_ONCE(pgdat->nr_reclaim_start,\n\t\t\t\tnode_page_state(pgdat, NR_THROTTLED_WRITTEN));\n\t\t}\n\n\t\tbreak;\n\tcase VMSCAN_THROTTLE_CONGESTED:\n\t\tfallthrough;\n\tcase VMSCAN_THROTTLE_NOPROGRESS:\n\t\tif (skip_throttle_noprogress(pgdat)) {\n\t\t\tcond_resched();\n\t\t\treturn;\n\t\t}\n\n\t\ttimeout = 1;\n\n\t\tbreak;\n\tcase VMSCAN_THROTTLE_ISOLATED:\n\t\ttimeout = HZ/50;\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\ttimeout = HZ;\n\t\tbreak;\n\t}\n\n\tprepare_to_wait(wqh, &wait, TASK_UNINTERRUPTIBLE);\n\tret = schedule_timeout(timeout);\n\tfinish_wait(wqh, &wait);\n\n\tif (reason == VMSCAN_THROTTLE_WRITEBACK)\n\t\tatomic_dec(&pgdat->nr_writeback_throttled);\n\n\ttrace_mm_vmscan_throttled(pgdat->node_id, jiffies_to_usecs(timeout),\n\t\t\t\tjiffies_to_usecs(timeout - ret),\n\t\t\t\treason);\n}\n\n \nvoid __acct_reclaim_writeback(pg_data_t *pgdat, struct folio *folio,\n\t\t\t\t\t\t\tint nr_throttled)\n{\n\tunsigned long nr_written;\n\n\tnode_stat_add_folio(folio, NR_THROTTLED_WRITTEN);\n\n\t \n\tnr_written = node_page_state(pgdat, NR_THROTTLED_WRITTEN) -\n\t\tREAD_ONCE(pgdat->nr_reclaim_start);\n\n\tif (nr_written > SWAP_CLUSTER_MAX * nr_throttled)\n\t\twake_up(&pgdat->reclaim_wait[VMSCAN_THROTTLE_WRITEBACK]);\n}\n\n \ntypedef enum {\n\t \n\tPAGE_KEEP,\n\t \n\tPAGE_ACTIVATE,\n\t \n\tPAGE_SUCCESS,\n\t \n\tPAGE_CLEAN,\n} pageout_t;\n\n \nstatic pageout_t pageout(struct folio *folio, struct address_space *mapping,\n\t\t\t struct swap_iocb **plug)\n{\n\t \n\tif (!is_page_cache_freeable(folio))\n\t\treturn PAGE_KEEP;\n\tif (!mapping) {\n\t\t \n\t\tif (folio_test_private(folio)) {\n\t\t\tif (try_to_free_buffers(folio)) {\n\t\t\t\tfolio_clear_dirty(folio);\n\t\t\t\tpr_info(\"%s: orphaned folio\\n\", __func__);\n\t\t\t\treturn PAGE_CLEAN;\n\t\t\t}\n\t\t}\n\t\treturn PAGE_KEEP;\n\t}\n\tif (mapping->a_ops->writepage == NULL)\n\t\treturn PAGE_ACTIVATE;\n\n\tif (folio_clear_dirty_for_io(folio)) {\n\t\tint res;\n\t\tstruct writeback_control wbc = {\n\t\t\t.sync_mode = WB_SYNC_NONE,\n\t\t\t.nr_to_write = SWAP_CLUSTER_MAX,\n\t\t\t.range_start = 0,\n\t\t\t.range_end = LLONG_MAX,\n\t\t\t.for_reclaim = 1,\n\t\t\t.swap_plug = plug,\n\t\t};\n\n\t\tfolio_set_reclaim(folio);\n\t\tres = mapping->a_ops->writepage(&folio->page, &wbc);\n\t\tif (res < 0)\n\t\t\thandle_write_error(mapping, folio, res);\n\t\tif (res == AOP_WRITEPAGE_ACTIVATE) {\n\t\t\tfolio_clear_reclaim(folio);\n\t\t\treturn PAGE_ACTIVATE;\n\t\t}\n\n\t\tif (!folio_test_writeback(folio)) {\n\t\t\t \n\t\t\tfolio_clear_reclaim(folio);\n\t\t}\n\t\ttrace_mm_vmscan_write_folio(folio);\n\t\tnode_stat_add_folio(folio, NR_VMSCAN_WRITE);\n\t\treturn PAGE_SUCCESS;\n\t}\n\n\treturn PAGE_CLEAN;\n}\n\n \nstatic int __remove_mapping(struct address_space *mapping, struct folio *folio,\n\t\t\t    bool reclaimed, struct mem_cgroup *target_memcg)\n{\n\tint refcount;\n\tvoid *shadow = NULL;\n\n\tBUG_ON(!folio_test_locked(folio));\n\tBUG_ON(mapping != folio_mapping(folio));\n\n\tif (!folio_test_swapcache(folio))\n\t\tspin_lock(&mapping->host->i_lock);\n\txa_lock_irq(&mapping->i_pages);\n\t \n\trefcount = 1 + folio_nr_pages(folio);\n\tif (!folio_ref_freeze(folio, refcount))\n\t\tgoto cannot_free;\n\t \n\tif (unlikely(folio_test_dirty(folio))) {\n\t\tfolio_ref_unfreeze(folio, refcount);\n\t\tgoto cannot_free;\n\t}\n\n\tif (folio_test_swapcache(folio)) {\n\t\tswp_entry_t swap = folio->swap;\n\n\t\tif (reclaimed && !mapping_exiting(mapping))\n\t\t\tshadow = workingset_eviction(folio, target_memcg);\n\t\t__delete_from_swap_cache(folio, swap, shadow);\n\t\tmem_cgroup_swapout(folio, swap);\n\t\txa_unlock_irq(&mapping->i_pages);\n\t\tput_swap_folio(folio, swap);\n\t} else {\n\t\tvoid (*free_folio)(struct folio *);\n\n\t\tfree_folio = mapping->a_ops->free_folio;\n\t\t \n\t\tif (reclaimed && folio_is_file_lru(folio) &&\n\t\t    !mapping_exiting(mapping) && !dax_mapping(mapping))\n\t\t\tshadow = workingset_eviction(folio, target_memcg);\n\t\t__filemap_remove_folio(folio, shadow);\n\t\txa_unlock_irq(&mapping->i_pages);\n\t\tif (mapping_shrinkable(mapping))\n\t\t\tinode_add_lru(mapping->host);\n\t\tspin_unlock(&mapping->host->i_lock);\n\n\t\tif (free_folio)\n\t\t\tfree_folio(folio);\n\t}\n\n\treturn 1;\n\ncannot_free:\n\txa_unlock_irq(&mapping->i_pages);\n\tif (!folio_test_swapcache(folio))\n\t\tspin_unlock(&mapping->host->i_lock);\n\treturn 0;\n}\n\n \nlong remove_mapping(struct address_space *mapping, struct folio *folio)\n{\n\tif (__remove_mapping(mapping, folio, false, NULL)) {\n\t\t \n\t\tfolio_ref_unfreeze(folio, 1);\n\t\treturn folio_nr_pages(folio);\n\t}\n\treturn 0;\n}\n\n \nvoid folio_putback_lru(struct folio *folio)\n{\n\tfolio_add_lru(folio);\n\tfolio_put(folio);\t\t \n}\n\nenum folio_references {\n\tFOLIOREF_RECLAIM,\n\tFOLIOREF_RECLAIM_CLEAN,\n\tFOLIOREF_KEEP,\n\tFOLIOREF_ACTIVATE,\n};\n\nstatic enum folio_references folio_check_references(struct folio *folio,\n\t\t\t\t\t\t  struct scan_control *sc)\n{\n\tint referenced_ptes, referenced_folio;\n\tunsigned long vm_flags;\n\n\treferenced_ptes = folio_referenced(folio, 1, sc->target_mem_cgroup,\n\t\t\t\t\t   &vm_flags);\n\treferenced_folio = folio_test_clear_referenced(folio);\n\n\t \n\tif (vm_flags & VM_LOCKED)\n\t\treturn FOLIOREF_ACTIVATE;\n\n\t \n\tif (referenced_ptes == -1)\n\t\treturn FOLIOREF_KEEP;\n\n\tif (referenced_ptes) {\n\t\t \n\t\tfolio_set_referenced(folio);\n\n\t\tif (referenced_folio || referenced_ptes > 1)\n\t\t\treturn FOLIOREF_ACTIVATE;\n\n\t\t \n\t\tif ((vm_flags & VM_EXEC) && folio_is_file_lru(folio))\n\t\t\treturn FOLIOREF_ACTIVATE;\n\n\t\treturn FOLIOREF_KEEP;\n\t}\n\n\t \n\tif (referenced_folio && folio_is_file_lru(folio))\n\t\treturn FOLIOREF_RECLAIM_CLEAN;\n\n\treturn FOLIOREF_RECLAIM;\n}\n\n \nstatic void folio_check_dirty_writeback(struct folio *folio,\n\t\t\t\t       bool *dirty, bool *writeback)\n{\n\tstruct address_space *mapping;\n\n\t \n\tif (!folio_is_file_lru(folio) ||\n\t    (folio_test_anon(folio) && !folio_test_swapbacked(folio))) {\n\t\t*dirty = false;\n\t\t*writeback = false;\n\t\treturn;\n\t}\n\n\t \n\t*dirty = folio_test_dirty(folio);\n\t*writeback = folio_test_writeback(folio);\n\n\t \n\tif (!folio_test_private(folio))\n\t\treturn;\n\n\tmapping = folio_mapping(folio);\n\tif (mapping && mapping->a_ops->is_dirty_writeback)\n\t\tmapping->a_ops->is_dirty_writeback(folio, dirty, writeback);\n}\n\nstatic struct folio *alloc_demote_folio(struct folio *src,\n\t\tunsigned long private)\n{\n\tstruct folio *dst;\n\tnodemask_t *allowed_mask;\n\tstruct migration_target_control *mtc;\n\n\tmtc = (struct migration_target_control *)private;\n\n\tallowed_mask = mtc->nmask;\n\t \n\tmtc->nmask = NULL;\n\tmtc->gfp_mask |= __GFP_THISNODE;\n\tdst = alloc_migration_target(src, (unsigned long)mtc);\n\tif (dst)\n\t\treturn dst;\n\n\tmtc->gfp_mask &= ~__GFP_THISNODE;\n\tmtc->nmask = allowed_mask;\n\n\treturn alloc_migration_target(src, (unsigned long)mtc);\n}\n\n \nstatic unsigned int demote_folio_list(struct list_head *demote_folios,\n\t\t\t\t     struct pglist_data *pgdat)\n{\n\tint target_nid = next_demotion_node(pgdat->node_id);\n\tunsigned int nr_succeeded;\n\tnodemask_t allowed_mask;\n\n\tstruct migration_target_control mtc = {\n\t\t \n\t\t.gfp_mask = (GFP_HIGHUSER_MOVABLE & ~__GFP_RECLAIM) | __GFP_NOWARN |\n\t\t\t__GFP_NOMEMALLOC | GFP_NOWAIT,\n\t\t.nid = target_nid,\n\t\t.nmask = &allowed_mask\n\t};\n\n\tif (list_empty(demote_folios))\n\t\treturn 0;\n\n\tif (target_nid == NUMA_NO_NODE)\n\t\treturn 0;\n\n\tnode_get_allowed_targets(pgdat, &allowed_mask);\n\n\t \n\tmigrate_pages(demote_folios, alloc_demote_folio, NULL,\n\t\t      (unsigned long)&mtc, MIGRATE_ASYNC, MR_DEMOTION,\n\t\t      &nr_succeeded);\n\n\t__count_vm_events(PGDEMOTE_KSWAPD + reclaimer_offset(), nr_succeeded);\n\n\treturn nr_succeeded;\n}\n\nstatic bool may_enter_fs(struct folio *folio, gfp_t gfp_mask)\n{\n\tif (gfp_mask & __GFP_FS)\n\t\treturn true;\n\tif (!folio_test_swapcache(folio) || !(gfp_mask & __GFP_IO))\n\t\treturn false;\n\t \n\treturn !data_race(folio_swap_flags(folio) & SWP_FS_OPS);\n}\n\n \nstatic unsigned int shrink_folio_list(struct list_head *folio_list,\n\t\tstruct pglist_data *pgdat, struct scan_control *sc,\n\t\tstruct reclaim_stat *stat, bool ignore_references)\n{\n\tLIST_HEAD(ret_folios);\n\tLIST_HEAD(free_folios);\n\tLIST_HEAD(demote_folios);\n\tunsigned int nr_reclaimed = 0;\n\tunsigned int pgactivate = 0;\n\tbool do_demote_pass;\n\tstruct swap_iocb *plug = NULL;\n\n\tmemset(stat, 0, sizeof(*stat));\n\tcond_resched();\n\tdo_demote_pass = can_demote(pgdat->node_id, sc);\n\nretry:\n\twhile (!list_empty(folio_list)) {\n\t\tstruct address_space *mapping;\n\t\tstruct folio *folio;\n\t\tenum folio_references references = FOLIOREF_RECLAIM;\n\t\tbool dirty, writeback;\n\t\tunsigned int nr_pages;\n\n\t\tcond_resched();\n\n\t\tfolio = lru_to_folio(folio_list);\n\t\tlist_del(&folio->lru);\n\n\t\tif (!folio_trylock(folio))\n\t\t\tgoto keep;\n\n\t\tVM_BUG_ON_FOLIO(folio_test_active(folio), folio);\n\n\t\tnr_pages = folio_nr_pages(folio);\n\n\t\t \n\t\tsc->nr_scanned += nr_pages;\n\n\t\tif (unlikely(!folio_evictable(folio)))\n\t\t\tgoto activate_locked;\n\n\t\tif (!sc->may_unmap && folio_mapped(folio))\n\t\t\tgoto keep_locked;\n\n\t\t \n\t\tif (lru_gen_enabled() && !ignore_references &&\n\t\t    folio_mapped(folio) && folio_test_referenced(folio))\n\t\t\tgoto keep_locked;\n\n\t\t \n\t\tfolio_check_dirty_writeback(folio, &dirty, &writeback);\n\t\tif (dirty || writeback)\n\t\t\tstat->nr_dirty += nr_pages;\n\n\t\tif (dirty && !writeback)\n\t\t\tstat->nr_unqueued_dirty += nr_pages;\n\n\t\t \n\t\tif (writeback && folio_test_reclaim(folio))\n\t\t\tstat->nr_congested += nr_pages;\n\n\t\t \n\t\tif (folio_test_writeback(folio)) {\n\t\t\t \n\t\t\tif (current_is_kswapd() &&\n\t\t\t    folio_test_reclaim(folio) &&\n\t\t\t    test_bit(PGDAT_WRITEBACK, &pgdat->flags)) {\n\t\t\t\tstat->nr_immediate += nr_pages;\n\t\t\t\tgoto activate_locked;\n\n\t\t\t \n\t\t\t} else if (writeback_throttling_sane(sc) ||\n\t\t\t    !folio_test_reclaim(folio) ||\n\t\t\t    !may_enter_fs(folio, sc->gfp_mask)) {\n\t\t\t\t \n\t\t\t\tfolio_set_reclaim(folio);\n\t\t\t\tstat->nr_writeback += nr_pages;\n\t\t\t\tgoto activate_locked;\n\n\t\t\t \n\t\t\t} else {\n\t\t\t\tfolio_unlock(folio);\n\t\t\t\tfolio_wait_writeback(folio);\n\t\t\t\t \n\t\t\t\tlist_add_tail(&folio->lru, folio_list);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\tif (!ignore_references)\n\t\t\treferences = folio_check_references(folio, sc);\n\n\t\tswitch (references) {\n\t\tcase FOLIOREF_ACTIVATE:\n\t\t\tgoto activate_locked;\n\t\tcase FOLIOREF_KEEP:\n\t\t\tstat->nr_ref_keep += nr_pages;\n\t\t\tgoto keep_locked;\n\t\tcase FOLIOREF_RECLAIM:\n\t\tcase FOLIOREF_RECLAIM_CLEAN:\n\t\t\t;  \n\t\t}\n\n\t\t \n\t\tif (do_demote_pass &&\n\t\t    (thp_migration_supported() || !folio_test_large(folio))) {\n\t\t\tlist_add(&folio->lru, &demote_folios);\n\t\t\tfolio_unlock(folio);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (folio_test_anon(folio) && folio_test_swapbacked(folio)) {\n\t\t\tif (!folio_test_swapcache(folio)) {\n\t\t\t\tif (!(sc->gfp_mask & __GFP_IO))\n\t\t\t\t\tgoto keep_locked;\n\t\t\t\tif (folio_maybe_dma_pinned(folio))\n\t\t\t\t\tgoto keep_locked;\n\t\t\t\tif (folio_test_large(folio)) {\n\t\t\t\t\t \n\t\t\t\t\tif (!can_split_folio(folio, NULL))\n\t\t\t\t\t\tgoto activate_locked;\n\t\t\t\t\t \n\t\t\t\t\tif (!folio_entire_mapcount(folio) &&\n\t\t\t\t\t    split_folio_to_list(folio,\n\t\t\t\t\t\t\t\tfolio_list))\n\t\t\t\t\t\tgoto activate_locked;\n\t\t\t\t}\n\t\t\t\tif (!add_to_swap(folio)) {\n\t\t\t\t\tif (!folio_test_large(folio))\n\t\t\t\t\t\tgoto activate_locked_split;\n\t\t\t\t\t \n\t\t\t\t\tif (split_folio_to_list(folio,\n\t\t\t\t\t\t\t\tfolio_list))\n\t\t\t\t\t\tgoto activate_locked;\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\t\t\t\t\tcount_vm_event(THP_SWPOUT_FALLBACK);\n#endif\n\t\t\t\t\tif (!add_to_swap(folio))\n\t\t\t\t\t\tgoto activate_locked_split;\n\t\t\t\t}\n\t\t\t}\n\t\t} else if (folio_test_swapbacked(folio) &&\n\t\t\t   folio_test_large(folio)) {\n\t\t\t \n\t\t\tif (split_folio_to_list(folio, folio_list))\n\t\t\t\tgoto keep_locked;\n\t\t}\n\n\t\t \n\t\tif ((nr_pages > 1) && !folio_test_large(folio)) {\n\t\t\tsc->nr_scanned -= (nr_pages - 1);\n\t\t\tnr_pages = 1;\n\t\t}\n\n\t\t \n\t\tif (folio_mapped(folio)) {\n\t\t\tenum ttu_flags flags = TTU_BATCH_FLUSH;\n\t\t\tbool was_swapbacked = folio_test_swapbacked(folio);\n\n\t\t\tif (folio_test_pmd_mappable(folio))\n\t\t\t\tflags |= TTU_SPLIT_HUGE_PMD;\n\n\t\t\ttry_to_unmap(folio, flags);\n\t\t\tif (folio_mapped(folio)) {\n\t\t\t\tstat->nr_unmap_fail += nr_pages;\n\t\t\t\tif (!was_swapbacked &&\n\t\t\t\t    folio_test_swapbacked(folio))\n\t\t\t\t\tstat->nr_lazyfree_fail += nr_pages;\n\t\t\t\tgoto activate_locked;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (folio_maybe_dma_pinned(folio))\n\t\t\tgoto activate_locked;\n\n\t\tmapping = folio_mapping(folio);\n\t\tif (folio_test_dirty(folio)) {\n\t\t\t \n\t\t\tif (folio_is_file_lru(folio) &&\n\t\t\t    (!current_is_kswapd() ||\n\t\t\t     !folio_test_reclaim(folio) ||\n\t\t\t     !test_bit(PGDAT_DIRTY, &pgdat->flags))) {\n\t\t\t\t \n\t\t\t\tnode_stat_mod_folio(folio, NR_VMSCAN_IMMEDIATE,\n\t\t\t\t\t\tnr_pages);\n\t\t\t\tfolio_set_reclaim(folio);\n\n\t\t\t\tgoto activate_locked;\n\t\t\t}\n\n\t\t\tif (references == FOLIOREF_RECLAIM_CLEAN)\n\t\t\t\tgoto keep_locked;\n\t\t\tif (!may_enter_fs(folio, sc->gfp_mask))\n\t\t\t\tgoto keep_locked;\n\t\t\tif (!sc->may_writepage)\n\t\t\t\tgoto keep_locked;\n\n\t\t\t \n\t\t\ttry_to_unmap_flush_dirty();\n\t\t\tswitch (pageout(folio, mapping, &plug)) {\n\t\t\tcase PAGE_KEEP:\n\t\t\t\tgoto keep_locked;\n\t\t\tcase PAGE_ACTIVATE:\n\t\t\t\tgoto activate_locked;\n\t\t\tcase PAGE_SUCCESS:\n\t\t\t\tstat->nr_pageout += nr_pages;\n\n\t\t\t\tif (folio_test_writeback(folio))\n\t\t\t\t\tgoto keep;\n\t\t\t\tif (folio_test_dirty(folio))\n\t\t\t\t\tgoto keep;\n\n\t\t\t\t \n\t\t\t\tif (!folio_trylock(folio))\n\t\t\t\t\tgoto keep;\n\t\t\t\tif (folio_test_dirty(folio) ||\n\t\t\t\t    folio_test_writeback(folio))\n\t\t\t\t\tgoto keep_locked;\n\t\t\t\tmapping = folio_mapping(folio);\n\t\t\t\tfallthrough;\n\t\t\tcase PAGE_CLEAN:\n\t\t\t\t;  \n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (folio_needs_release(folio)) {\n\t\t\tif (!filemap_release_folio(folio, sc->gfp_mask))\n\t\t\t\tgoto activate_locked;\n\t\t\tif (!mapping && folio_ref_count(folio) == 1) {\n\t\t\t\tfolio_unlock(folio);\n\t\t\t\tif (folio_put_testzero(folio))\n\t\t\t\t\tgoto free_it;\n\t\t\t\telse {\n\t\t\t\t\t \n\t\t\t\t\tnr_reclaimed += nr_pages;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif (folio_test_anon(folio) && !folio_test_swapbacked(folio)) {\n\t\t\t \n\t\t\tif (!folio_ref_freeze(folio, 1))\n\t\t\t\tgoto keep_locked;\n\t\t\t \n\t\t\tcount_vm_events(PGLAZYFREED, nr_pages);\n\t\t\tcount_memcg_folio_events(folio, PGLAZYFREED, nr_pages);\n\t\t} else if (!mapping || !__remove_mapping(mapping, folio, true,\n\t\t\t\t\t\t\t sc->target_mem_cgroup))\n\t\t\tgoto keep_locked;\n\n\t\tfolio_unlock(folio);\nfree_it:\n\t\t \n\t\tnr_reclaimed += nr_pages;\n\n\t\t \n\t\tif (unlikely(folio_test_large(folio)))\n\t\t\tdestroy_large_folio(folio);\n\t\telse\n\t\t\tlist_add(&folio->lru, &free_folios);\n\t\tcontinue;\n\nactivate_locked_split:\n\t\t \n\t\tif (nr_pages > 1) {\n\t\t\tsc->nr_scanned -= (nr_pages - 1);\n\t\t\tnr_pages = 1;\n\t\t}\nactivate_locked:\n\t\t \n\t\tif (folio_test_swapcache(folio) &&\n\t\t    (mem_cgroup_swap_full(folio) || folio_test_mlocked(folio)))\n\t\t\tfolio_free_swap(folio);\n\t\tVM_BUG_ON_FOLIO(folio_test_active(folio), folio);\n\t\tif (!folio_test_mlocked(folio)) {\n\t\t\tint type = folio_is_file_lru(folio);\n\t\t\tfolio_set_active(folio);\n\t\t\tstat->nr_activate[type] += nr_pages;\n\t\t\tcount_memcg_folio_events(folio, PGACTIVATE, nr_pages);\n\t\t}\nkeep_locked:\n\t\tfolio_unlock(folio);\nkeep:\n\t\tlist_add(&folio->lru, &ret_folios);\n\t\tVM_BUG_ON_FOLIO(folio_test_lru(folio) ||\n\t\t\t\tfolio_test_unevictable(folio), folio);\n\t}\n\t \n\n\t \n\tnr_reclaimed += demote_folio_list(&demote_folios, pgdat);\n\t \n\tif (!list_empty(&demote_folios)) {\n\t\t \n\t\tlist_splice_init(&demote_folios, folio_list);\n\n\t\t \n\t\tif (!sc->proactive) {\n\t\t\tdo_demote_pass = false;\n\t\t\tgoto retry;\n\t\t}\n\t}\n\n\tpgactivate = stat->nr_activate[0] + stat->nr_activate[1];\n\n\tmem_cgroup_uncharge_list(&free_folios);\n\ttry_to_unmap_flush();\n\tfree_unref_page_list(&free_folios);\n\n\tlist_splice(&ret_folios, folio_list);\n\tcount_vm_events(PGACTIVATE, pgactivate);\n\n\tif (plug)\n\t\tswap_write_unplug(plug);\n\treturn nr_reclaimed;\n}\n\nunsigned int reclaim_clean_pages_from_list(struct zone *zone,\n\t\t\t\t\t   struct list_head *folio_list)\n{\n\tstruct scan_control sc = {\n\t\t.gfp_mask = GFP_KERNEL,\n\t\t.may_unmap = 1,\n\t};\n\tstruct reclaim_stat stat;\n\tunsigned int nr_reclaimed;\n\tstruct folio *folio, *next;\n\tLIST_HEAD(clean_folios);\n\tunsigned int noreclaim_flag;\n\n\tlist_for_each_entry_safe(folio, next, folio_list, lru) {\n\t\tif (!folio_test_hugetlb(folio) && folio_is_file_lru(folio) &&\n\t\t    !folio_test_dirty(folio) && !__folio_test_movable(folio) &&\n\t\t    !folio_test_unevictable(folio)) {\n\t\t\tfolio_clear_active(folio);\n\t\t\tlist_move(&folio->lru, &clean_folios);\n\t\t}\n\t}\n\n\t \n\tnoreclaim_flag = memalloc_noreclaim_save();\n\tnr_reclaimed = shrink_folio_list(&clean_folios, zone->zone_pgdat, &sc,\n\t\t\t\t\t&stat, true);\n\tmemalloc_noreclaim_restore(noreclaim_flag);\n\n\tlist_splice(&clean_folios, folio_list);\n\tmod_node_page_state(zone->zone_pgdat, NR_ISOLATED_FILE,\n\t\t\t    -(long)nr_reclaimed);\n\t \n\tmod_node_page_state(zone->zone_pgdat, NR_ISOLATED_ANON,\n\t\t\t    stat.nr_lazyfree_fail);\n\tmod_node_page_state(zone->zone_pgdat, NR_ISOLATED_FILE,\n\t\t\t    -(long)stat.nr_lazyfree_fail);\n\treturn nr_reclaimed;\n}\n\n \nstatic __always_inline void update_lru_sizes(struct lruvec *lruvec,\n\t\t\tenum lru_list lru, unsigned long *nr_zone_taken)\n{\n\tint zid;\n\n\tfor (zid = 0; zid < MAX_NR_ZONES; zid++) {\n\t\tif (!nr_zone_taken[zid])\n\t\t\tcontinue;\n\n\t\tupdate_lru_size(lruvec, lru, zid, -nr_zone_taken[zid]);\n\t}\n\n}\n\n#ifdef CONFIG_CMA\n \nstatic bool skip_cma(struct folio *folio, struct scan_control *sc)\n{\n\treturn !current_is_kswapd() &&\n\t\t\tgfp_migratetype(sc->gfp_mask) != MIGRATE_MOVABLE &&\n\t\t\tget_pageblock_migratetype(&folio->page) == MIGRATE_CMA;\n}\n#else\nstatic bool skip_cma(struct folio *folio, struct scan_control *sc)\n{\n\treturn false;\n}\n#endif\n\n \nstatic unsigned long isolate_lru_folios(unsigned long nr_to_scan,\n\t\tstruct lruvec *lruvec, struct list_head *dst,\n\t\tunsigned long *nr_scanned, struct scan_control *sc,\n\t\tenum lru_list lru)\n{\n\tstruct list_head *src = &lruvec->lists[lru];\n\tunsigned long nr_taken = 0;\n\tunsigned long nr_zone_taken[MAX_NR_ZONES] = { 0 };\n\tunsigned long nr_skipped[MAX_NR_ZONES] = { 0, };\n\tunsigned long skipped = 0;\n\tunsigned long scan, total_scan, nr_pages;\n\tLIST_HEAD(folios_skipped);\n\n\ttotal_scan = 0;\n\tscan = 0;\n\twhile (scan < nr_to_scan && !list_empty(src)) {\n\t\tstruct list_head *move_to = src;\n\t\tstruct folio *folio;\n\n\t\tfolio = lru_to_folio(src);\n\t\tprefetchw_prev_lru_folio(folio, src, flags);\n\n\t\tnr_pages = folio_nr_pages(folio);\n\t\ttotal_scan += nr_pages;\n\n\t\tif (folio_zonenum(folio) > sc->reclaim_idx ||\n\t\t\t\tskip_cma(folio, sc)) {\n\t\t\tnr_skipped[folio_zonenum(folio)] += nr_pages;\n\t\t\tmove_to = &folios_skipped;\n\t\t\tgoto move;\n\t\t}\n\n\t\t \n\t\tscan += nr_pages;\n\n\t\tif (!folio_test_lru(folio))\n\t\t\tgoto move;\n\t\tif (!sc->may_unmap && folio_mapped(folio))\n\t\t\tgoto move;\n\n\t\t \n\t\tif (unlikely(!folio_try_get(folio)))\n\t\t\tgoto move;\n\n\t\tif (!folio_test_clear_lru(folio)) {\n\t\t\t \n\t\t\tfolio_put(folio);\n\t\t\tgoto move;\n\t\t}\n\n\t\tnr_taken += nr_pages;\n\t\tnr_zone_taken[folio_zonenum(folio)] += nr_pages;\n\t\tmove_to = dst;\nmove:\n\t\tlist_move(&folio->lru, move_to);\n\t}\n\n\t \n\tif (!list_empty(&folios_skipped)) {\n\t\tint zid;\n\n\t\tlist_splice(&folios_skipped, src);\n\t\tfor (zid = 0; zid < MAX_NR_ZONES; zid++) {\n\t\t\tif (!nr_skipped[zid])\n\t\t\t\tcontinue;\n\n\t\t\t__count_zid_vm_events(PGSCAN_SKIP, zid, nr_skipped[zid]);\n\t\t\tskipped += nr_skipped[zid];\n\t\t}\n\t}\n\t*nr_scanned = total_scan;\n\ttrace_mm_vmscan_lru_isolate(sc->reclaim_idx, sc->order, nr_to_scan,\n\t\t\t\t    total_scan, skipped, nr_taken,\n\t\t\t\t    sc->may_unmap ? 0 : ISOLATE_UNMAPPED, lru);\n\tupdate_lru_sizes(lruvec, lru, nr_zone_taken);\n\treturn nr_taken;\n}\n\n \nbool folio_isolate_lru(struct folio *folio)\n{\n\tbool ret = false;\n\n\tVM_BUG_ON_FOLIO(!folio_ref_count(folio), folio);\n\n\tif (folio_test_clear_lru(folio)) {\n\t\tstruct lruvec *lruvec;\n\n\t\tfolio_get(folio);\n\t\tlruvec = folio_lruvec_lock_irq(folio);\n\t\tlruvec_del_folio(lruvec, folio);\n\t\tunlock_page_lruvec_irq(lruvec);\n\t\tret = true;\n\t}\n\n\treturn ret;\n}\n\n \nstatic int too_many_isolated(struct pglist_data *pgdat, int file,\n\t\tstruct scan_control *sc)\n{\n\tunsigned long inactive, isolated;\n\tbool too_many;\n\n\tif (current_is_kswapd())\n\t\treturn 0;\n\n\tif (!writeback_throttling_sane(sc))\n\t\treturn 0;\n\n\tif (file) {\n\t\tinactive = node_page_state(pgdat, NR_INACTIVE_FILE);\n\t\tisolated = node_page_state(pgdat, NR_ISOLATED_FILE);\n\t} else {\n\t\tinactive = node_page_state(pgdat, NR_INACTIVE_ANON);\n\t\tisolated = node_page_state(pgdat, NR_ISOLATED_ANON);\n\t}\n\n\t \n\tif (gfp_has_io_fs(sc->gfp_mask))\n\t\tinactive >>= 3;\n\n\ttoo_many = isolated > inactive;\n\n\t \n\tif (!too_many)\n\t\twake_throttle_isolated(pgdat);\n\n\treturn too_many;\n}\n\n \nstatic unsigned int move_folios_to_lru(struct lruvec *lruvec,\n\t\tstruct list_head *list)\n{\n\tint nr_pages, nr_moved = 0;\n\tLIST_HEAD(folios_to_free);\n\n\twhile (!list_empty(list)) {\n\t\tstruct folio *folio = lru_to_folio(list);\n\n\t\tVM_BUG_ON_FOLIO(folio_test_lru(folio), folio);\n\t\tlist_del(&folio->lru);\n\t\tif (unlikely(!folio_evictable(folio))) {\n\t\t\tspin_unlock_irq(&lruvec->lru_lock);\n\t\t\tfolio_putback_lru(folio);\n\t\t\tspin_lock_irq(&lruvec->lru_lock);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tfolio_set_lru(folio);\n\n\t\tif (unlikely(folio_put_testzero(folio))) {\n\t\t\t__folio_clear_lru_flags(folio);\n\n\t\t\tif (unlikely(folio_test_large(folio))) {\n\t\t\t\tspin_unlock_irq(&lruvec->lru_lock);\n\t\t\t\tdestroy_large_folio(folio);\n\t\t\t\tspin_lock_irq(&lruvec->lru_lock);\n\t\t\t} else\n\t\t\t\tlist_add(&folio->lru, &folios_to_free);\n\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tVM_BUG_ON_FOLIO(!folio_matches_lruvec(folio, lruvec), folio);\n\t\tlruvec_add_folio(lruvec, folio);\n\t\tnr_pages = folio_nr_pages(folio);\n\t\tnr_moved += nr_pages;\n\t\tif (folio_test_active(folio))\n\t\t\tworkingset_age_nonresident(lruvec, nr_pages);\n\t}\n\n\t \n\tlist_splice(&folios_to_free, list);\n\n\treturn nr_moved;\n}\n\n \nstatic int current_may_throttle(void)\n{\n\treturn !(current->flags & PF_LOCAL_THROTTLE);\n}\n\n \nstatic unsigned long shrink_inactive_list(unsigned long nr_to_scan,\n\t\tstruct lruvec *lruvec, struct scan_control *sc,\n\t\tenum lru_list lru)\n{\n\tLIST_HEAD(folio_list);\n\tunsigned long nr_scanned;\n\tunsigned int nr_reclaimed = 0;\n\tunsigned long nr_taken;\n\tstruct reclaim_stat stat;\n\tbool file = is_file_lru(lru);\n\tenum vm_event_item item;\n\tstruct pglist_data *pgdat = lruvec_pgdat(lruvec);\n\tbool stalled = false;\n\n\twhile (unlikely(too_many_isolated(pgdat, file, sc))) {\n\t\tif (stalled)\n\t\t\treturn 0;\n\n\t\t \n\t\tstalled = true;\n\t\treclaim_throttle(pgdat, VMSCAN_THROTTLE_ISOLATED);\n\n\t\t \n\t\tif (fatal_signal_pending(current))\n\t\t\treturn SWAP_CLUSTER_MAX;\n\t}\n\n\tlru_add_drain();\n\n\tspin_lock_irq(&lruvec->lru_lock);\n\n\tnr_taken = isolate_lru_folios(nr_to_scan, lruvec, &folio_list,\n\t\t\t\t     &nr_scanned, sc, lru);\n\n\t__mod_node_page_state(pgdat, NR_ISOLATED_ANON + file, nr_taken);\n\titem = PGSCAN_KSWAPD + reclaimer_offset();\n\tif (!cgroup_reclaim(sc))\n\t\t__count_vm_events(item, nr_scanned);\n\t__count_memcg_events(lruvec_memcg(lruvec), item, nr_scanned);\n\t__count_vm_events(PGSCAN_ANON + file, nr_scanned);\n\n\tspin_unlock_irq(&lruvec->lru_lock);\n\n\tif (nr_taken == 0)\n\t\treturn 0;\n\n\tnr_reclaimed = shrink_folio_list(&folio_list, pgdat, sc, &stat, false);\n\n\tspin_lock_irq(&lruvec->lru_lock);\n\tmove_folios_to_lru(lruvec, &folio_list);\n\n\t__mod_node_page_state(pgdat, NR_ISOLATED_ANON + file, -nr_taken);\n\titem = PGSTEAL_KSWAPD + reclaimer_offset();\n\tif (!cgroup_reclaim(sc))\n\t\t__count_vm_events(item, nr_reclaimed);\n\t__count_memcg_events(lruvec_memcg(lruvec), item, nr_reclaimed);\n\t__count_vm_events(PGSTEAL_ANON + file, nr_reclaimed);\n\tspin_unlock_irq(&lruvec->lru_lock);\n\n\tlru_note_cost(lruvec, file, stat.nr_pageout, nr_scanned - nr_reclaimed);\n\tmem_cgroup_uncharge_list(&folio_list);\n\tfree_unref_page_list(&folio_list);\n\n\t \n\tif (stat.nr_unqueued_dirty == nr_taken) {\n\t\twakeup_flusher_threads(WB_REASON_VMSCAN);\n\t\t \n\t\tif (!writeback_throttling_sane(sc))\n\t\t\treclaim_throttle(pgdat, VMSCAN_THROTTLE_WRITEBACK);\n\t}\n\n\tsc->nr.dirty += stat.nr_dirty;\n\tsc->nr.congested += stat.nr_congested;\n\tsc->nr.unqueued_dirty += stat.nr_unqueued_dirty;\n\tsc->nr.writeback += stat.nr_writeback;\n\tsc->nr.immediate += stat.nr_immediate;\n\tsc->nr.taken += nr_taken;\n\tif (file)\n\t\tsc->nr.file_taken += nr_taken;\n\n\ttrace_mm_vmscan_lru_shrink_inactive(pgdat->node_id,\n\t\t\tnr_scanned, nr_reclaimed, &stat, sc->priority, file);\n\treturn nr_reclaimed;\n}\n\n \nstatic void shrink_active_list(unsigned long nr_to_scan,\n\t\t\t       struct lruvec *lruvec,\n\t\t\t       struct scan_control *sc,\n\t\t\t       enum lru_list lru)\n{\n\tunsigned long nr_taken;\n\tunsigned long nr_scanned;\n\tunsigned long vm_flags;\n\tLIST_HEAD(l_hold);\t \n\tLIST_HEAD(l_active);\n\tLIST_HEAD(l_inactive);\n\tunsigned nr_deactivate, nr_activate;\n\tunsigned nr_rotated = 0;\n\tint file = is_file_lru(lru);\n\tstruct pglist_data *pgdat = lruvec_pgdat(lruvec);\n\n\tlru_add_drain();\n\n\tspin_lock_irq(&lruvec->lru_lock);\n\n\tnr_taken = isolate_lru_folios(nr_to_scan, lruvec, &l_hold,\n\t\t\t\t     &nr_scanned, sc, lru);\n\n\t__mod_node_page_state(pgdat, NR_ISOLATED_ANON + file, nr_taken);\n\n\tif (!cgroup_reclaim(sc))\n\t\t__count_vm_events(PGREFILL, nr_scanned);\n\t__count_memcg_events(lruvec_memcg(lruvec), PGREFILL, nr_scanned);\n\n\tspin_unlock_irq(&lruvec->lru_lock);\n\n\twhile (!list_empty(&l_hold)) {\n\t\tstruct folio *folio;\n\n\t\tcond_resched();\n\t\tfolio = lru_to_folio(&l_hold);\n\t\tlist_del(&folio->lru);\n\n\t\tif (unlikely(!folio_evictable(folio))) {\n\t\t\tfolio_putback_lru(folio);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (unlikely(buffer_heads_over_limit)) {\n\t\t\tif (folio_needs_release(folio) &&\n\t\t\t    folio_trylock(folio)) {\n\t\t\t\tfilemap_release_folio(folio, 0);\n\t\t\t\tfolio_unlock(folio);\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (folio_referenced(folio, 0, sc->target_mem_cgroup,\n\t\t\t\t     &vm_flags) != 0) {\n\t\t\t \n\t\t\tif ((vm_flags & VM_EXEC) && folio_is_file_lru(folio)) {\n\t\t\t\tnr_rotated += folio_nr_pages(folio);\n\t\t\t\tlist_add(&folio->lru, &l_active);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\tfolio_clear_active(folio);\t \n\t\tfolio_set_workingset(folio);\n\t\tlist_add(&folio->lru, &l_inactive);\n\t}\n\n\t \n\tspin_lock_irq(&lruvec->lru_lock);\n\n\tnr_activate = move_folios_to_lru(lruvec, &l_active);\n\tnr_deactivate = move_folios_to_lru(lruvec, &l_inactive);\n\t \n\tlist_splice(&l_inactive, &l_active);\n\n\t__count_vm_events(PGDEACTIVATE, nr_deactivate);\n\t__count_memcg_events(lruvec_memcg(lruvec), PGDEACTIVATE, nr_deactivate);\n\n\t__mod_node_page_state(pgdat, NR_ISOLATED_ANON + file, -nr_taken);\n\tspin_unlock_irq(&lruvec->lru_lock);\n\n\tif (nr_rotated)\n\t\tlru_note_cost(lruvec, file, 0, nr_rotated);\n\tmem_cgroup_uncharge_list(&l_active);\n\tfree_unref_page_list(&l_active);\n\ttrace_mm_vmscan_lru_shrink_active(pgdat->node_id, nr_taken, nr_activate,\n\t\t\tnr_deactivate, nr_rotated, sc->priority, file);\n}\n\nstatic unsigned int reclaim_folio_list(struct list_head *folio_list,\n\t\t\t\t      struct pglist_data *pgdat)\n{\n\tstruct reclaim_stat dummy_stat;\n\tunsigned int nr_reclaimed;\n\tstruct folio *folio;\n\tstruct scan_control sc = {\n\t\t.gfp_mask = GFP_KERNEL,\n\t\t.may_writepage = 1,\n\t\t.may_unmap = 1,\n\t\t.may_swap = 1,\n\t\t.no_demotion = 1,\n\t};\n\n\tnr_reclaimed = shrink_folio_list(folio_list, pgdat, &sc, &dummy_stat, false);\n\twhile (!list_empty(folio_list)) {\n\t\tfolio = lru_to_folio(folio_list);\n\t\tlist_del(&folio->lru);\n\t\tfolio_putback_lru(folio);\n\t}\n\n\treturn nr_reclaimed;\n}\n\nunsigned long reclaim_pages(struct list_head *folio_list)\n{\n\tint nid;\n\tunsigned int nr_reclaimed = 0;\n\tLIST_HEAD(node_folio_list);\n\tunsigned int noreclaim_flag;\n\n\tif (list_empty(folio_list))\n\t\treturn nr_reclaimed;\n\n\tnoreclaim_flag = memalloc_noreclaim_save();\n\n\tnid = folio_nid(lru_to_folio(folio_list));\n\tdo {\n\t\tstruct folio *folio = lru_to_folio(folio_list);\n\n\t\tif (nid == folio_nid(folio)) {\n\t\t\tfolio_clear_active(folio);\n\t\t\tlist_move(&folio->lru, &node_folio_list);\n\t\t\tcontinue;\n\t\t}\n\n\t\tnr_reclaimed += reclaim_folio_list(&node_folio_list, NODE_DATA(nid));\n\t\tnid = folio_nid(lru_to_folio(folio_list));\n\t} while (!list_empty(folio_list));\n\n\tnr_reclaimed += reclaim_folio_list(&node_folio_list, NODE_DATA(nid));\n\n\tmemalloc_noreclaim_restore(noreclaim_flag);\n\n\treturn nr_reclaimed;\n}\n\nstatic unsigned long shrink_list(enum lru_list lru, unsigned long nr_to_scan,\n\t\t\t\t struct lruvec *lruvec, struct scan_control *sc)\n{\n\tif (is_active_lru(lru)) {\n\t\tif (sc->may_deactivate & (1 << is_file_lru(lru)))\n\t\t\tshrink_active_list(nr_to_scan, lruvec, sc, lru);\n\t\telse\n\t\t\tsc->skipped_deactivate = 1;\n\t\treturn 0;\n\t}\n\n\treturn shrink_inactive_list(nr_to_scan, lruvec, sc, lru);\n}\n\n \nstatic bool inactive_is_low(struct lruvec *lruvec, enum lru_list inactive_lru)\n{\n\tenum lru_list active_lru = inactive_lru + LRU_ACTIVE;\n\tunsigned long inactive, active;\n\tunsigned long inactive_ratio;\n\tunsigned long gb;\n\n\tinactive = lruvec_page_state(lruvec, NR_LRU_BASE + inactive_lru);\n\tactive = lruvec_page_state(lruvec, NR_LRU_BASE + active_lru);\n\n\tgb = (inactive + active) >> (30 - PAGE_SHIFT);\n\tif (gb)\n\t\tinactive_ratio = int_sqrt(10 * gb);\n\telse\n\t\tinactive_ratio = 1;\n\n\treturn inactive * inactive_ratio < active;\n}\n\nenum scan_balance {\n\tSCAN_EQUAL,\n\tSCAN_FRACT,\n\tSCAN_ANON,\n\tSCAN_FILE,\n};\n\nstatic void prepare_scan_count(pg_data_t *pgdat, struct scan_control *sc)\n{\n\tunsigned long file;\n\tstruct lruvec *target_lruvec;\n\n\tif (lru_gen_enabled())\n\t\treturn;\n\n\ttarget_lruvec = mem_cgroup_lruvec(sc->target_mem_cgroup, pgdat);\n\n\t \n\tmem_cgroup_flush_stats();\n\n\t \n\tspin_lock_irq(&target_lruvec->lru_lock);\n\tsc->anon_cost = target_lruvec->anon_cost;\n\tsc->file_cost = target_lruvec->file_cost;\n\tspin_unlock_irq(&target_lruvec->lru_lock);\n\n\t \n\tif (!sc->force_deactivate) {\n\t\tunsigned long refaults;\n\n\t\t \n\t\trefaults = lruvec_page_state(target_lruvec,\n\t\t\t\tWORKINGSET_ACTIVATE_ANON);\n\t\tif (refaults != target_lruvec->refaults[WORKINGSET_ANON] ||\n\t\t\tinactive_is_low(target_lruvec, LRU_INACTIVE_ANON))\n\t\t\tsc->may_deactivate |= DEACTIVATE_ANON;\n\t\telse\n\t\t\tsc->may_deactivate &= ~DEACTIVATE_ANON;\n\n\t\trefaults = lruvec_page_state(target_lruvec,\n\t\t\t\tWORKINGSET_ACTIVATE_FILE);\n\t\tif (refaults != target_lruvec->refaults[WORKINGSET_FILE] ||\n\t\t    inactive_is_low(target_lruvec, LRU_INACTIVE_FILE))\n\t\t\tsc->may_deactivate |= DEACTIVATE_FILE;\n\t\telse\n\t\t\tsc->may_deactivate &= ~DEACTIVATE_FILE;\n\t} else\n\t\tsc->may_deactivate = DEACTIVATE_ANON | DEACTIVATE_FILE;\n\n\t \n\tfile = lruvec_page_state(target_lruvec, NR_INACTIVE_FILE);\n\tif (file >> sc->priority && !(sc->may_deactivate & DEACTIVATE_FILE))\n\t\tsc->cache_trim_mode = 1;\n\telse\n\t\tsc->cache_trim_mode = 0;\n\n\t \n\tif (!cgroup_reclaim(sc)) {\n\t\tunsigned long total_high_wmark = 0;\n\t\tunsigned long free, anon;\n\t\tint z;\n\n\t\tfree = sum_zone_node_page_state(pgdat->node_id, NR_FREE_PAGES);\n\t\tfile = node_page_state(pgdat, NR_ACTIVE_FILE) +\n\t\t\t   node_page_state(pgdat, NR_INACTIVE_FILE);\n\n\t\tfor (z = 0; z < MAX_NR_ZONES; z++) {\n\t\t\tstruct zone *zone = &pgdat->node_zones[z];\n\n\t\t\tif (!managed_zone(zone))\n\t\t\t\tcontinue;\n\n\t\t\ttotal_high_wmark += high_wmark_pages(zone);\n\t\t}\n\n\t\t \n\t\tanon = node_page_state(pgdat, NR_INACTIVE_ANON);\n\n\t\tsc->file_is_tiny =\n\t\t\tfile + free <= total_high_wmark &&\n\t\t\t!(sc->may_deactivate & DEACTIVATE_ANON) &&\n\t\t\tanon >> sc->priority;\n\t}\n}\n\n \nstatic void get_scan_count(struct lruvec *lruvec, struct scan_control *sc,\n\t\t\t   unsigned long *nr)\n{\n\tstruct pglist_data *pgdat = lruvec_pgdat(lruvec);\n\tstruct mem_cgroup *memcg = lruvec_memcg(lruvec);\n\tunsigned long anon_cost, file_cost, total_cost;\n\tint swappiness = mem_cgroup_swappiness(memcg);\n\tu64 fraction[ANON_AND_FILE];\n\tu64 denominator = 0;\t \n\tenum scan_balance scan_balance;\n\tunsigned long ap, fp;\n\tenum lru_list lru;\n\n\t \n\tif (!sc->may_swap || !can_reclaim_anon_pages(memcg, pgdat->node_id, sc)) {\n\t\tscan_balance = SCAN_FILE;\n\t\tgoto out;\n\t}\n\n\t \n\tif (cgroup_reclaim(sc) && !swappiness) {\n\t\tscan_balance = SCAN_FILE;\n\t\tgoto out;\n\t}\n\n\t \n\tif (!sc->priority && swappiness) {\n\t\tscan_balance = SCAN_EQUAL;\n\t\tgoto out;\n\t}\n\n\t \n\tif (sc->file_is_tiny) {\n\t\tscan_balance = SCAN_ANON;\n\t\tgoto out;\n\t}\n\n\t \n\tif (sc->cache_trim_mode) {\n\t\tscan_balance = SCAN_FILE;\n\t\tgoto out;\n\t}\n\n\tscan_balance = SCAN_FRACT;\n\t \n\ttotal_cost = sc->anon_cost + sc->file_cost;\n\tanon_cost = total_cost + sc->anon_cost;\n\tfile_cost = total_cost + sc->file_cost;\n\ttotal_cost = anon_cost + file_cost;\n\n\tap = swappiness * (total_cost + 1);\n\tap /= anon_cost + 1;\n\n\tfp = (200 - swappiness) * (total_cost + 1);\n\tfp /= file_cost + 1;\n\n\tfraction[0] = ap;\n\tfraction[1] = fp;\n\tdenominator = ap + fp;\nout:\n\tfor_each_evictable_lru(lru) {\n\t\tint file = is_file_lru(lru);\n\t\tunsigned long lruvec_size;\n\t\tunsigned long low, min;\n\t\tunsigned long scan;\n\n\t\tlruvec_size = lruvec_lru_size(lruvec, lru, sc->reclaim_idx);\n\t\tmem_cgroup_protection(sc->target_mem_cgroup, memcg,\n\t\t\t\t      &min, &low);\n\n\t\tif (min || low) {\n\t\t\t \n\t\t\tunsigned long cgroup_size = mem_cgroup_size(memcg);\n\t\t\tunsigned long protection;\n\n\t\t\t \n\t\t\tif (!sc->memcg_low_reclaim && low > min) {\n\t\t\t\tprotection = low;\n\t\t\t\tsc->memcg_low_skipped = 1;\n\t\t\t} else {\n\t\t\t\tprotection = min;\n\t\t\t}\n\n\t\t\t \n\t\t\tcgroup_size = max(cgroup_size, protection);\n\n\t\t\tscan = lruvec_size - lruvec_size * protection /\n\t\t\t\t(cgroup_size + 1);\n\n\t\t\t \n\t\t\tscan = max(scan, SWAP_CLUSTER_MAX);\n\t\t} else {\n\t\t\tscan = lruvec_size;\n\t\t}\n\n\t\tscan >>= sc->priority;\n\n\t\t \n\t\tif (!scan && !mem_cgroup_online(memcg))\n\t\t\tscan = min(lruvec_size, SWAP_CLUSTER_MAX);\n\n\t\tswitch (scan_balance) {\n\t\tcase SCAN_EQUAL:\n\t\t\t \n\t\t\tbreak;\n\t\tcase SCAN_FRACT:\n\t\t\t \n\t\t\tscan = mem_cgroup_online(memcg) ?\n\t\t\t       div64_u64(scan * fraction[file], denominator) :\n\t\t\t       DIV64_U64_ROUND_UP(scan * fraction[file],\n\t\t\t\t\t\t  denominator);\n\t\t\tbreak;\n\t\tcase SCAN_FILE:\n\t\tcase SCAN_ANON:\n\t\t\t \n\t\t\tif ((scan_balance == SCAN_FILE) != file)\n\t\t\t\tscan = 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t \n\t\t\tBUG();\n\t\t}\n\n\t\tnr[lru] = scan;\n\t}\n}\n\n \nstatic bool can_age_anon_pages(struct pglist_data *pgdat,\n\t\t\t       struct scan_control *sc)\n{\n\t \n\tif (total_swap_pages > 0)\n\t\treturn true;\n\n\t \n\treturn can_demote(pgdat->node_id, sc);\n}\n\n#ifdef CONFIG_LRU_GEN\n\n#ifdef CONFIG_LRU_GEN_ENABLED\nDEFINE_STATIC_KEY_ARRAY_TRUE(lru_gen_caps, NR_LRU_GEN_CAPS);\n#define get_cap(cap)\tstatic_branch_likely(&lru_gen_caps[cap])\n#else\nDEFINE_STATIC_KEY_ARRAY_FALSE(lru_gen_caps, NR_LRU_GEN_CAPS);\n#define get_cap(cap)\tstatic_branch_unlikely(&lru_gen_caps[cap])\n#endif\n\nstatic bool should_walk_mmu(void)\n{\n\treturn arch_has_hw_pte_young() && get_cap(LRU_GEN_MM_WALK);\n}\n\nstatic bool should_clear_pmd_young(void)\n{\n\treturn arch_has_hw_nonleaf_pmd_young() && get_cap(LRU_GEN_NONLEAF_YOUNG);\n}\n\n \n\n#define LRU_REFS_FLAGS\t(BIT(PG_referenced) | BIT(PG_workingset))\n\n#define DEFINE_MAX_SEQ(lruvec)\t\t\t\t\t\t\\\n\tunsigned long max_seq = READ_ONCE((lruvec)->lrugen.max_seq)\n\n#define DEFINE_MIN_SEQ(lruvec)\t\t\t\t\t\t\\\n\tunsigned long min_seq[ANON_AND_FILE] = {\t\t\t\\\n\t\tREAD_ONCE((lruvec)->lrugen.min_seq[LRU_GEN_ANON]),\t\\\n\t\tREAD_ONCE((lruvec)->lrugen.min_seq[LRU_GEN_FILE]),\t\\\n\t}\n\n#define for_each_gen_type_zone(gen, type, zone)\t\t\t\t\\\n\tfor ((gen) = 0; (gen) < MAX_NR_GENS; (gen)++)\t\t\t\\\n\t\tfor ((type) = 0; (type) < ANON_AND_FILE; (type)++)\t\\\n\t\t\tfor ((zone) = 0; (zone) < MAX_NR_ZONES; (zone)++)\n\n#define get_memcg_gen(seq)\t((seq) % MEMCG_NR_GENS)\n#define get_memcg_bin(bin)\t((bin) % MEMCG_NR_BINS)\n\nstatic struct lruvec *get_lruvec(struct mem_cgroup *memcg, int nid)\n{\n\tstruct pglist_data *pgdat = NODE_DATA(nid);\n\n#ifdef CONFIG_MEMCG\n\tif (memcg) {\n\t\tstruct lruvec *lruvec = &memcg->nodeinfo[nid]->lruvec;\n\n\t\t \n\t\tif (!lruvec->pgdat)\n\t\t\tlruvec->pgdat = pgdat;\n\n\t\treturn lruvec;\n\t}\n#endif\n\tVM_WARN_ON_ONCE(!mem_cgroup_disabled());\n\n\treturn &pgdat->__lruvec;\n}\n\nstatic int get_swappiness(struct lruvec *lruvec, struct scan_control *sc)\n{\n\tstruct mem_cgroup *memcg = lruvec_memcg(lruvec);\n\tstruct pglist_data *pgdat = lruvec_pgdat(lruvec);\n\n\tif (!sc->may_swap)\n\t\treturn 0;\n\n\tif (!can_demote(pgdat->node_id, sc) &&\n\t    mem_cgroup_get_nr_swap_pages(memcg) < MIN_LRU_BATCH)\n\t\treturn 0;\n\n\treturn mem_cgroup_swappiness(memcg);\n}\n\nstatic int get_nr_gens(struct lruvec *lruvec, int type)\n{\n\treturn lruvec->lrugen.max_seq - lruvec->lrugen.min_seq[type] + 1;\n}\n\nstatic bool __maybe_unused seq_is_valid(struct lruvec *lruvec)\n{\n\t \n\treturn get_nr_gens(lruvec, LRU_GEN_FILE) >= MIN_NR_GENS &&\n\t       get_nr_gens(lruvec, LRU_GEN_FILE) <= get_nr_gens(lruvec, LRU_GEN_ANON) &&\n\t       get_nr_gens(lruvec, LRU_GEN_ANON) <= MAX_NR_GENS;\n}\n\n \n\n \n#define BLOOM_FILTER_SHIFT\t15\n\nstatic inline int filter_gen_from_seq(unsigned long seq)\n{\n\treturn seq % NR_BLOOM_FILTERS;\n}\n\nstatic void get_item_key(void *item, int *key)\n{\n\tu32 hash = hash_ptr(item, BLOOM_FILTER_SHIFT * 2);\n\n\tBUILD_BUG_ON(BLOOM_FILTER_SHIFT * 2 > BITS_PER_TYPE(u32));\n\n\tkey[0] = hash & (BIT(BLOOM_FILTER_SHIFT) - 1);\n\tkey[1] = hash >> BLOOM_FILTER_SHIFT;\n}\n\nstatic bool test_bloom_filter(struct lruvec *lruvec, unsigned long seq, void *item)\n{\n\tint key[2];\n\tunsigned long *filter;\n\tint gen = filter_gen_from_seq(seq);\n\n\tfilter = READ_ONCE(lruvec->mm_state.filters[gen]);\n\tif (!filter)\n\t\treturn true;\n\n\tget_item_key(item, key);\n\n\treturn test_bit(key[0], filter) && test_bit(key[1], filter);\n}\n\nstatic void update_bloom_filter(struct lruvec *lruvec, unsigned long seq, void *item)\n{\n\tint key[2];\n\tunsigned long *filter;\n\tint gen = filter_gen_from_seq(seq);\n\n\tfilter = READ_ONCE(lruvec->mm_state.filters[gen]);\n\tif (!filter)\n\t\treturn;\n\n\tget_item_key(item, key);\n\n\tif (!test_bit(key[0], filter))\n\t\tset_bit(key[0], filter);\n\tif (!test_bit(key[1], filter))\n\t\tset_bit(key[1], filter);\n}\n\nstatic void reset_bloom_filter(struct lruvec *lruvec, unsigned long seq)\n{\n\tunsigned long *filter;\n\tint gen = filter_gen_from_seq(seq);\n\n\tfilter = lruvec->mm_state.filters[gen];\n\tif (filter) {\n\t\tbitmap_clear(filter, 0, BIT(BLOOM_FILTER_SHIFT));\n\t\treturn;\n\t}\n\n\tfilter = bitmap_zalloc(BIT(BLOOM_FILTER_SHIFT),\n\t\t\t       __GFP_HIGH | __GFP_NOMEMALLOC | __GFP_NOWARN);\n\tWRITE_ONCE(lruvec->mm_state.filters[gen], filter);\n}\n\n \n\nstatic struct lru_gen_mm_list *get_mm_list(struct mem_cgroup *memcg)\n{\n\tstatic struct lru_gen_mm_list mm_list = {\n\t\t.fifo = LIST_HEAD_INIT(mm_list.fifo),\n\t\t.lock = __SPIN_LOCK_UNLOCKED(mm_list.lock),\n\t};\n\n#ifdef CONFIG_MEMCG\n\tif (memcg)\n\t\treturn &memcg->mm_list;\n#endif\n\tVM_WARN_ON_ONCE(!mem_cgroup_disabled());\n\n\treturn &mm_list;\n}\n\nvoid lru_gen_add_mm(struct mm_struct *mm)\n{\n\tint nid;\n\tstruct mem_cgroup *memcg = get_mem_cgroup_from_mm(mm);\n\tstruct lru_gen_mm_list *mm_list = get_mm_list(memcg);\n\n\tVM_WARN_ON_ONCE(!list_empty(&mm->lru_gen.list));\n#ifdef CONFIG_MEMCG\n\tVM_WARN_ON_ONCE(mm->lru_gen.memcg);\n\tmm->lru_gen.memcg = memcg;\n#endif\n\tspin_lock(&mm_list->lock);\n\n\tfor_each_node_state(nid, N_MEMORY) {\n\t\tstruct lruvec *lruvec = get_lruvec(memcg, nid);\n\n\t\t \n\t\tif (lruvec->mm_state.tail == &mm_list->fifo)\n\t\t\tlruvec->mm_state.tail = &mm->lru_gen.list;\n\t}\n\n\tlist_add_tail(&mm->lru_gen.list, &mm_list->fifo);\n\n\tspin_unlock(&mm_list->lock);\n}\n\nvoid lru_gen_del_mm(struct mm_struct *mm)\n{\n\tint nid;\n\tstruct lru_gen_mm_list *mm_list;\n\tstruct mem_cgroup *memcg = NULL;\n\n\tif (list_empty(&mm->lru_gen.list))\n\t\treturn;\n\n#ifdef CONFIG_MEMCG\n\tmemcg = mm->lru_gen.memcg;\n#endif\n\tmm_list = get_mm_list(memcg);\n\n\tspin_lock(&mm_list->lock);\n\n\tfor_each_node(nid) {\n\t\tstruct lruvec *lruvec = get_lruvec(memcg, nid);\n\n\t\t \n\t\tif (lruvec->mm_state.head == &mm->lru_gen.list)\n\t\t\tlruvec->mm_state.head = lruvec->mm_state.head->prev;\n\n\t\t \n\t\tif (lruvec->mm_state.tail == &mm->lru_gen.list)\n\t\t\tlruvec->mm_state.tail = lruvec->mm_state.tail->next;\n\t}\n\n\tlist_del_init(&mm->lru_gen.list);\n\n\tspin_unlock(&mm_list->lock);\n\n#ifdef CONFIG_MEMCG\n\tmem_cgroup_put(mm->lru_gen.memcg);\n\tmm->lru_gen.memcg = NULL;\n#endif\n}\n\n#ifdef CONFIG_MEMCG\nvoid lru_gen_migrate_mm(struct mm_struct *mm)\n{\n\tstruct mem_cgroup *memcg;\n\tstruct task_struct *task = rcu_dereference_protected(mm->owner, true);\n\n\tVM_WARN_ON_ONCE(task->mm != mm);\n\tlockdep_assert_held(&task->alloc_lock);\n\n\t \n\tif (mem_cgroup_disabled())\n\t\treturn;\n\n\t \n\tif (!mm->lru_gen.memcg)\n\t\treturn;\n\n\trcu_read_lock();\n\tmemcg = mem_cgroup_from_task(task);\n\trcu_read_unlock();\n\tif (memcg == mm->lru_gen.memcg)\n\t\treturn;\n\n\tVM_WARN_ON_ONCE(list_empty(&mm->lru_gen.list));\n\n\tlru_gen_del_mm(mm);\n\tlru_gen_add_mm(mm);\n}\n#endif\n\nstatic void reset_mm_stats(struct lruvec *lruvec, struct lru_gen_mm_walk *walk, bool last)\n{\n\tint i;\n\tint hist;\n\n\tlockdep_assert_held(&get_mm_list(lruvec_memcg(lruvec))->lock);\n\n\tif (walk) {\n\t\thist = lru_hist_from_seq(walk->max_seq);\n\n\t\tfor (i = 0; i < NR_MM_STATS; i++) {\n\t\t\tWRITE_ONCE(lruvec->mm_state.stats[hist][i],\n\t\t\t\t   lruvec->mm_state.stats[hist][i] + walk->mm_stats[i]);\n\t\t\twalk->mm_stats[i] = 0;\n\t\t}\n\t}\n\n\tif (NR_HIST_GENS > 1 && last) {\n\t\thist = lru_hist_from_seq(lruvec->mm_state.seq + 1);\n\n\t\tfor (i = 0; i < NR_MM_STATS; i++)\n\t\t\tWRITE_ONCE(lruvec->mm_state.stats[hist][i], 0);\n\t}\n}\n\nstatic bool should_skip_mm(struct mm_struct *mm, struct lru_gen_mm_walk *walk)\n{\n\tint type;\n\tunsigned long size = 0;\n\tstruct pglist_data *pgdat = lruvec_pgdat(walk->lruvec);\n\tint key = pgdat->node_id % BITS_PER_TYPE(mm->lru_gen.bitmap);\n\n\tif (!walk->force_scan && !test_bit(key, &mm->lru_gen.bitmap))\n\t\treturn true;\n\n\tclear_bit(key, &mm->lru_gen.bitmap);\n\n\tfor (type = !walk->can_swap; type < ANON_AND_FILE; type++) {\n\t\tsize += type ? get_mm_counter(mm, MM_FILEPAGES) :\n\t\t\t       get_mm_counter(mm, MM_ANONPAGES) +\n\t\t\t       get_mm_counter(mm, MM_SHMEMPAGES);\n\t}\n\n\tif (size < MIN_LRU_BATCH)\n\t\treturn true;\n\n\treturn !mmget_not_zero(mm);\n}\n\nstatic bool iterate_mm_list(struct lruvec *lruvec, struct lru_gen_mm_walk *walk,\n\t\t\t    struct mm_struct **iter)\n{\n\tbool first = false;\n\tbool last = false;\n\tstruct mm_struct *mm = NULL;\n\tstruct mem_cgroup *memcg = lruvec_memcg(lruvec);\n\tstruct lru_gen_mm_list *mm_list = get_mm_list(memcg);\n\tstruct lru_gen_mm_state *mm_state = &lruvec->mm_state;\n\n\t \n\tspin_lock(&mm_list->lock);\n\n\tVM_WARN_ON_ONCE(mm_state->seq + 1 < walk->max_seq);\n\n\tif (walk->max_seq <= mm_state->seq)\n\t\tgoto done;\n\n\tif (!mm_state->head)\n\t\tmm_state->head = &mm_list->fifo;\n\n\tif (mm_state->head == &mm_list->fifo)\n\t\tfirst = true;\n\n\tdo {\n\t\tmm_state->head = mm_state->head->next;\n\t\tif (mm_state->head == &mm_list->fifo) {\n\t\t\tWRITE_ONCE(mm_state->seq, mm_state->seq + 1);\n\t\t\tlast = true;\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (!mm_state->tail || mm_state->tail == mm_state->head) {\n\t\t\tmm_state->tail = mm_state->head->next;\n\t\t\twalk->force_scan = true;\n\t\t}\n\n\t\tmm = list_entry(mm_state->head, struct mm_struct, lru_gen.list);\n\t\tif (should_skip_mm(mm, walk))\n\t\t\tmm = NULL;\n\t} while (!mm);\ndone:\n\tif (*iter || last)\n\t\treset_mm_stats(lruvec, walk, last);\n\n\tspin_unlock(&mm_list->lock);\n\n\tif (mm && first)\n\t\treset_bloom_filter(lruvec, walk->max_seq + 1);\n\n\tif (*iter)\n\t\tmmput_async(*iter);\n\n\t*iter = mm;\n\n\treturn last;\n}\n\nstatic bool iterate_mm_list_nowalk(struct lruvec *lruvec, unsigned long max_seq)\n{\n\tbool success = false;\n\tstruct mem_cgroup *memcg = lruvec_memcg(lruvec);\n\tstruct lru_gen_mm_list *mm_list = get_mm_list(memcg);\n\tstruct lru_gen_mm_state *mm_state = &lruvec->mm_state;\n\n\tspin_lock(&mm_list->lock);\n\n\tVM_WARN_ON_ONCE(mm_state->seq + 1 < max_seq);\n\n\tif (max_seq > mm_state->seq) {\n\t\tmm_state->head = NULL;\n\t\tmm_state->tail = NULL;\n\t\tWRITE_ONCE(mm_state->seq, mm_state->seq + 1);\n\t\treset_mm_stats(lruvec, NULL, true);\n\t\tsuccess = true;\n\t}\n\n\tspin_unlock(&mm_list->lock);\n\n\treturn success;\n}\n\n \n\n \nstruct ctrl_pos {\n\tunsigned long refaulted;\n\tunsigned long total;\n\tint gain;\n};\n\nstatic void read_ctrl_pos(struct lruvec *lruvec, int type, int tier, int gain,\n\t\t\t  struct ctrl_pos *pos)\n{\n\tstruct lru_gen_folio *lrugen = &lruvec->lrugen;\n\tint hist = lru_hist_from_seq(lrugen->min_seq[type]);\n\n\tpos->refaulted = lrugen->avg_refaulted[type][tier] +\n\t\t\t atomic_long_read(&lrugen->refaulted[hist][type][tier]);\n\tpos->total = lrugen->avg_total[type][tier] +\n\t\t     atomic_long_read(&lrugen->evicted[hist][type][tier]);\n\tif (tier)\n\t\tpos->total += lrugen->protected[hist][type][tier - 1];\n\tpos->gain = gain;\n}\n\nstatic void reset_ctrl_pos(struct lruvec *lruvec, int type, bool carryover)\n{\n\tint hist, tier;\n\tstruct lru_gen_folio *lrugen = &lruvec->lrugen;\n\tbool clear = carryover ? NR_HIST_GENS == 1 : NR_HIST_GENS > 1;\n\tunsigned long seq = carryover ? lrugen->min_seq[type] : lrugen->max_seq + 1;\n\n\tlockdep_assert_held(&lruvec->lru_lock);\n\n\tif (!carryover && !clear)\n\t\treturn;\n\n\thist = lru_hist_from_seq(seq);\n\n\tfor (tier = 0; tier < MAX_NR_TIERS; tier++) {\n\t\tif (carryover) {\n\t\t\tunsigned long sum;\n\n\t\t\tsum = lrugen->avg_refaulted[type][tier] +\n\t\t\t      atomic_long_read(&lrugen->refaulted[hist][type][tier]);\n\t\t\tWRITE_ONCE(lrugen->avg_refaulted[type][tier], sum / 2);\n\n\t\t\tsum = lrugen->avg_total[type][tier] +\n\t\t\t      atomic_long_read(&lrugen->evicted[hist][type][tier]);\n\t\t\tif (tier)\n\t\t\t\tsum += lrugen->protected[hist][type][tier - 1];\n\t\t\tWRITE_ONCE(lrugen->avg_total[type][tier], sum / 2);\n\t\t}\n\n\t\tif (clear) {\n\t\t\tatomic_long_set(&lrugen->refaulted[hist][type][tier], 0);\n\t\t\tatomic_long_set(&lrugen->evicted[hist][type][tier], 0);\n\t\t\tif (tier)\n\t\t\t\tWRITE_ONCE(lrugen->protected[hist][type][tier - 1], 0);\n\t\t}\n\t}\n}\n\nstatic bool positive_ctrl_err(struct ctrl_pos *sp, struct ctrl_pos *pv)\n{\n\t \n\treturn pv->refaulted < MIN_LRU_BATCH ||\n\t       pv->refaulted * (sp->total + MIN_LRU_BATCH) * sp->gain <=\n\t       (sp->refaulted + 1) * pv->total * pv->gain;\n}\n\n \n\n \nstatic int folio_update_gen(struct folio *folio, int gen)\n{\n\tunsigned long new_flags, old_flags = READ_ONCE(folio->flags);\n\n\tVM_WARN_ON_ONCE(gen >= MAX_NR_GENS);\n\tVM_WARN_ON_ONCE(!rcu_read_lock_held());\n\n\tdo {\n\t\t \n\t\tif (!(old_flags & LRU_GEN_MASK)) {\n\t\t\t \n\t\t\tnew_flags = old_flags | BIT(PG_referenced);\n\t\t\tcontinue;\n\t\t}\n\n\t\tnew_flags = old_flags & ~(LRU_GEN_MASK | LRU_REFS_MASK | LRU_REFS_FLAGS);\n\t\tnew_flags |= (gen + 1UL) << LRU_GEN_PGOFF;\n\t} while (!try_cmpxchg(&folio->flags, &old_flags, new_flags));\n\n\treturn ((old_flags & LRU_GEN_MASK) >> LRU_GEN_PGOFF) - 1;\n}\n\n \nstatic int folio_inc_gen(struct lruvec *lruvec, struct folio *folio, bool reclaiming)\n{\n\tint type = folio_is_file_lru(folio);\n\tstruct lru_gen_folio *lrugen = &lruvec->lrugen;\n\tint new_gen, old_gen = lru_gen_from_seq(lrugen->min_seq[type]);\n\tunsigned long new_flags, old_flags = READ_ONCE(folio->flags);\n\n\tVM_WARN_ON_ONCE_FOLIO(!(old_flags & LRU_GEN_MASK), folio);\n\n\tdo {\n\t\tnew_gen = ((old_flags & LRU_GEN_MASK) >> LRU_GEN_PGOFF) - 1;\n\t\t \n\t\tif (new_gen >= 0 && new_gen != old_gen)\n\t\t\treturn new_gen;\n\n\t\tnew_gen = (old_gen + 1) % MAX_NR_GENS;\n\n\t\tnew_flags = old_flags & ~(LRU_GEN_MASK | LRU_REFS_MASK | LRU_REFS_FLAGS);\n\t\tnew_flags |= (new_gen + 1UL) << LRU_GEN_PGOFF;\n\t\t \n\t\tif (reclaiming)\n\t\t\tnew_flags |= BIT(PG_reclaim);\n\t} while (!try_cmpxchg(&folio->flags, &old_flags, new_flags));\n\n\tlru_gen_update_size(lruvec, folio, old_gen, new_gen);\n\n\treturn new_gen;\n}\n\nstatic void update_batch_size(struct lru_gen_mm_walk *walk, struct folio *folio,\n\t\t\t      int old_gen, int new_gen)\n{\n\tint type = folio_is_file_lru(folio);\n\tint zone = folio_zonenum(folio);\n\tint delta = folio_nr_pages(folio);\n\n\tVM_WARN_ON_ONCE(old_gen >= MAX_NR_GENS);\n\tVM_WARN_ON_ONCE(new_gen >= MAX_NR_GENS);\n\n\twalk->batched++;\n\n\twalk->nr_pages[old_gen][type][zone] -= delta;\n\twalk->nr_pages[new_gen][type][zone] += delta;\n}\n\nstatic void reset_batch_size(struct lruvec *lruvec, struct lru_gen_mm_walk *walk)\n{\n\tint gen, type, zone;\n\tstruct lru_gen_folio *lrugen = &lruvec->lrugen;\n\n\twalk->batched = 0;\n\n\tfor_each_gen_type_zone(gen, type, zone) {\n\t\tenum lru_list lru = type * LRU_INACTIVE_FILE;\n\t\tint delta = walk->nr_pages[gen][type][zone];\n\n\t\tif (!delta)\n\t\t\tcontinue;\n\n\t\twalk->nr_pages[gen][type][zone] = 0;\n\t\tWRITE_ONCE(lrugen->nr_pages[gen][type][zone],\n\t\t\t   lrugen->nr_pages[gen][type][zone] + delta);\n\n\t\tif (lru_gen_is_active(lruvec, gen))\n\t\t\tlru += LRU_ACTIVE;\n\t\t__update_lru_size(lruvec, lru, zone, delta);\n\t}\n}\n\nstatic int should_skip_vma(unsigned long start, unsigned long end, struct mm_walk *args)\n{\n\tstruct address_space *mapping;\n\tstruct vm_area_struct *vma = args->vma;\n\tstruct lru_gen_mm_walk *walk = args->private;\n\n\tif (!vma_is_accessible(vma))\n\t\treturn true;\n\n\tif (is_vm_hugetlb_page(vma))\n\t\treturn true;\n\n\tif (!vma_has_recency(vma))\n\t\treturn true;\n\n\tif (vma->vm_flags & (VM_LOCKED | VM_SPECIAL))\n\t\treturn true;\n\n\tif (vma == get_gate_vma(vma->vm_mm))\n\t\treturn true;\n\n\tif (vma_is_anonymous(vma))\n\t\treturn !walk->can_swap;\n\n\tif (WARN_ON_ONCE(!vma->vm_file || !vma->vm_file->f_mapping))\n\t\treturn true;\n\n\tmapping = vma->vm_file->f_mapping;\n\tif (mapping_unevictable(mapping))\n\t\treturn true;\n\n\tif (shmem_mapping(mapping))\n\t\treturn !walk->can_swap;\n\n\t \n\treturn !mapping->a_ops->read_folio;\n}\n\n \nstatic bool get_next_vma(unsigned long mask, unsigned long size, struct mm_walk *args,\n\t\t\t unsigned long *vm_start, unsigned long *vm_end)\n{\n\tunsigned long start = round_up(*vm_end, size);\n\tunsigned long end = (start | ~mask) + 1;\n\tVMA_ITERATOR(vmi, args->mm, start);\n\n\tVM_WARN_ON_ONCE(mask & size);\n\tVM_WARN_ON_ONCE((start & mask) != (*vm_start & mask));\n\n\tfor_each_vma(vmi, args->vma) {\n\t\tif (end && end <= args->vma->vm_start)\n\t\t\treturn false;\n\n\t\tif (should_skip_vma(args->vma->vm_start, args->vma->vm_end, args))\n\t\t\tcontinue;\n\n\t\t*vm_start = max(start, args->vma->vm_start);\n\t\t*vm_end = min(end - 1, args->vma->vm_end - 1) + 1;\n\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic unsigned long get_pte_pfn(pte_t pte, struct vm_area_struct *vma, unsigned long addr)\n{\n\tunsigned long pfn = pte_pfn(pte);\n\n\tVM_WARN_ON_ONCE(addr < vma->vm_start || addr >= vma->vm_end);\n\n\tif (!pte_present(pte) || is_zero_pfn(pfn))\n\t\treturn -1;\n\n\tif (WARN_ON_ONCE(pte_devmap(pte) || pte_special(pte)))\n\t\treturn -1;\n\n\tif (WARN_ON_ONCE(!pfn_valid(pfn)))\n\t\treturn -1;\n\n\treturn pfn;\n}\n\n#if defined(CONFIG_TRANSPARENT_HUGEPAGE) || defined(CONFIG_ARCH_HAS_NONLEAF_PMD_YOUNG)\nstatic unsigned long get_pmd_pfn(pmd_t pmd, struct vm_area_struct *vma, unsigned long addr)\n{\n\tunsigned long pfn = pmd_pfn(pmd);\n\n\tVM_WARN_ON_ONCE(addr < vma->vm_start || addr >= vma->vm_end);\n\n\tif (!pmd_present(pmd) || is_huge_zero_pmd(pmd))\n\t\treturn -1;\n\n\tif (WARN_ON_ONCE(pmd_devmap(pmd)))\n\t\treturn -1;\n\n\tif (WARN_ON_ONCE(!pfn_valid(pfn)))\n\t\treturn -1;\n\n\treturn pfn;\n}\n#endif\n\nstatic struct folio *get_pfn_folio(unsigned long pfn, struct mem_cgroup *memcg,\n\t\t\t\t   struct pglist_data *pgdat, bool can_swap)\n{\n\tstruct folio *folio;\n\n\t \n\tif (pfn < pgdat->node_start_pfn || pfn >= pgdat_end_pfn(pgdat))\n\t\treturn NULL;\n\n\tfolio = pfn_folio(pfn);\n\tif (folio_nid(folio) != pgdat->node_id)\n\t\treturn NULL;\n\n\tif (folio_memcg_rcu(folio) != memcg)\n\t\treturn NULL;\n\n\t \n\tif (!folio_is_file_lru(folio) && !can_swap)\n\t\treturn NULL;\n\n\treturn folio;\n}\n\nstatic bool suitable_to_scan(int total, int young)\n{\n\tint n = clamp_t(int, cache_line_size() / sizeof(pte_t), 2, 8);\n\n\t \n\treturn young * n >= total;\n}\n\nstatic bool walk_pte_range(pmd_t *pmd, unsigned long start, unsigned long end,\n\t\t\t   struct mm_walk *args)\n{\n\tint i;\n\tpte_t *pte;\n\tspinlock_t *ptl;\n\tunsigned long addr;\n\tint total = 0;\n\tint young = 0;\n\tstruct lru_gen_mm_walk *walk = args->private;\n\tstruct mem_cgroup *memcg = lruvec_memcg(walk->lruvec);\n\tstruct pglist_data *pgdat = lruvec_pgdat(walk->lruvec);\n\tint old_gen, new_gen = lru_gen_from_seq(walk->max_seq);\n\n\tpte = pte_offset_map_nolock(args->mm, pmd, start & PMD_MASK, &ptl);\n\tif (!pte)\n\t\treturn false;\n\tif (!spin_trylock(ptl)) {\n\t\tpte_unmap(pte);\n\t\treturn false;\n\t}\n\n\tarch_enter_lazy_mmu_mode();\nrestart:\n\tfor (i = pte_index(start), addr = start; addr != end; i++, addr += PAGE_SIZE) {\n\t\tunsigned long pfn;\n\t\tstruct folio *folio;\n\t\tpte_t ptent = ptep_get(pte + i);\n\n\t\ttotal++;\n\t\twalk->mm_stats[MM_LEAF_TOTAL]++;\n\n\t\tpfn = get_pte_pfn(ptent, args->vma, addr);\n\t\tif (pfn == -1)\n\t\t\tcontinue;\n\n\t\tif (!pte_young(ptent)) {\n\t\t\twalk->mm_stats[MM_LEAF_OLD]++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tfolio = get_pfn_folio(pfn, memcg, pgdat, walk->can_swap);\n\t\tif (!folio)\n\t\t\tcontinue;\n\n\t\tif (!ptep_test_and_clear_young(args->vma, addr, pte + i))\n\t\t\tVM_WARN_ON_ONCE(true);\n\n\t\tyoung++;\n\t\twalk->mm_stats[MM_LEAF_YOUNG]++;\n\n\t\tif (pte_dirty(ptent) && !folio_test_dirty(folio) &&\n\t\t    !(folio_test_anon(folio) && folio_test_swapbacked(folio) &&\n\t\t      !folio_test_swapcache(folio)))\n\t\t\tfolio_mark_dirty(folio);\n\n\t\told_gen = folio_update_gen(folio, new_gen);\n\t\tif (old_gen >= 0 && old_gen != new_gen)\n\t\t\tupdate_batch_size(walk, folio, old_gen, new_gen);\n\t}\n\n\tif (i < PTRS_PER_PTE && get_next_vma(PMD_MASK, PAGE_SIZE, args, &start, &end))\n\t\tgoto restart;\n\n\tarch_leave_lazy_mmu_mode();\n\tpte_unmap_unlock(pte, ptl);\n\n\treturn suitable_to_scan(total, young);\n}\n\n#if defined(CONFIG_TRANSPARENT_HUGEPAGE) || defined(CONFIG_ARCH_HAS_NONLEAF_PMD_YOUNG)\nstatic void walk_pmd_range_locked(pud_t *pud, unsigned long addr, struct vm_area_struct *vma,\n\t\t\t\t  struct mm_walk *args, unsigned long *bitmap, unsigned long *first)\n{\n\tint i;\n\tpmd_t *pmd;\n\tspinlock_t *ptl;\n\tstruct lru_gen_mm_walk *walk = args->private;\n\tstruct mem_cgroup *memcg = lruvec_memcg(walk->lruvec);\n\tstruct pglist_data *pgdat = lruvec_pgdat(walk->lruvec);\n\tint old_gen, new_gen = lru_gen_from_seq(walk->max_seq);\n\n\tVM_WARN_ON_ONCE(pud_leaf(*pud));\n\n\t \n\tif (*first == -1) {\n\t\t*first = addr;\n\t\tbitmap_zero(bitmap, MIN_LRU_BATCH);\n\t\treturn;\n\t}\n\n\ti = addr == -1 ? 0 : pmd_index(addr) - pmd_index(*first);\n\tif (i && i <= MIN_LRU_BATCH) {\n\t\t__set_bit(i - 1, bitmap);\n\t\treturn;\n\t}\n\n\tpmd = pmd_offset(pud, *first);\n\n\tptl = pmd_lockptr(args->mm, pmd);\n\tif (!spin_trylock(ptl))\n\t\tgoto done;\n\n\tarch_enter_lazy_mmu_mode();\n\n\tdo {\n\t\tunsigned long pfn;\n\t\tstruct folio *folio;\n\n\t\t \n\t\taddr = i ? (*first & PMD_MASK) + i * PMD_SIZE : *first;\n\n\t\tpfn = get_pmd_pfn(pmd[i], vma, addr);\n\t\tif (pfn == -1)\n\t\t\tgoto next;\n\n\t\tif (!pmd_trans_huge(pmd[i])) {\n\t\t\tif (should_clear_pmd_young())\n\t\t\t\tpmdp_test_and_clear_young(vma, addr, pmd + i);\n\t\t\tgoto next;\n\t\t}\n\n\t\tfolio = get_pfn_folio(pfn, memcg, pgdat, walk->can_swap);\n\t\tif (!folio)\n\t\t\tgoto next;\n\n\t\tif (!pmdp_test_and_clear_young(vma, addr, pmd + i))\n\t\t\tgoto next;\n\n\t\twalk->mm_stats[MM_LEAF_YOUNG]++;\n\n\t\tif (pmd_dirty(pmd[i]) && !folio_test_dirty(folio) &&\n\t\t    !(folio_test_anon(folio) && folio_test_swapbacked(folio) &&\n\t\t      !folio_test_swapcache(folio)))\n\t\t\tfolio_mark_dirty(folio);\n\n\t\told_gen = folio_update_gen(folio, new_gen);\n\t\tif (old_gen >= 0 && old_gen != new_gen)\n\t\t\tupdate_batch_size(walk, folio, old_gen, new_gen);\nnext:\n\t\ti = i > MIN_LRU_BATCH ? 0 : find_next_bit(bitmap, MIN_LRU_BATCH, i) + 1;\n\t} while (i <= MIN_LRU_BATCH);\n\n\tarch_leave_lazy_mmu_mode();\n\tspin_unlock(ptl);\ndone:\n\t*first = -1;\n}\n#else\nstatic void walk_pmd_range_locked(pud_t *pud, unsigned long addr, struct vm_area_struct *vma,\n\t\t\t\t  struct mm_walk *args, unsigned long *bitmap, unsigned long *first)\n{\n}\n#endif\n\nstatic void walk_pmd_range(pud_t *pud, unsigned long start, unsigned long end,\n\t\t\t   struct mm_walk *args)\n{\n\tint i;\n\tpmd_t *pmd;\n\tunsigned long next;\n\tunsigned long addr;\n\tstruct vm_area_struct *vma;\n\tDECLARE_BITMAP(bitmap, MIN_LRU_BATCH);\n\tunsigned long first = -1;\n\tstruct lru_gen_mm_walk *walk = args->private;\n\n\tVM_WARN_ON_ONCE(pud_leaf(*pud));\n\n\t \n\tpmd = pmd_offset(pud, start & PUD_MASK);\nrestart:\n\t \n\tvma = args->vma;\n\tfor (i = pmd_index(start), addr = start; addr != end; i++, addr = next) {\n\t\tpmd_t val = pmdp_get_lockless(pmd + i);\n\n\t\tnext = pmd_addr_end(addr, end);\n\n\t\tif (!pmd_present(val) || is_huge_zero_pmd(val)) {\n\t\t\twalk->mm_stats[MM_LEAF_TOTAL]++;\n\t\t\tcontinue;\n\t\t}\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\t\tif (pmd_trans_huge(val)) {\n\t\t\tunsigned long pfn = pmd_pfn(val);\n\t\t\tstruct pglist_data *pgdat = lruvec_pgdat(walk->lruvec);\n\n\t\t\twalk->mm_stats[MM_LEAF_TOTAL]++;\n\n\t\t\tif (!pmd_young(val)) {\n\t\t\t\twalk->mm_stats[MM_LEAF_OLD]++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (pfn < pgdat->node_start_pfn || pfn >= pgdat_end_pfn(pgdat))\n\t\t\t\tcontinue;\n\n\t\t\twalk_pmd_range_locked(pud, addr, vma, args, bitmap, &first);\n\t\t\tcontinue;\n\t\t}\n#endif\n\t\twalk->mm_stats[MM_NONLEAF_TOTAL]++;\n\n\t\tif (should_clear_pmd_young()) {\n\t\t\tif (!pmd_young(val))\n\t\t\t\tcontinue;\n\n\t\t\twalk_pmd_range_locked(pud, addr, vma, args, bitmap, &first);\n\t\t}\n\n\t\tif (!walk->force_scan && !test_bloom_filter(walk->lruvec, walk->max_seq, pmd + i))\n\t\t\tcontinue;\n\n\t\twalk->mm_stats[MM_NONLEAF_FOUND]++;\n\n\t\tif (!walk_pte_range(&val, addr, next, args))\n\t\t\tcontinue;\n\n\t\twalk->mm_stats[MM_NONLEAF_ADDED]++;\n\n\t\t \n\t\tupdate_bloom_filter(walk->lruvec, walk->max_seq + 1, pmd + i);\n\t}\n\n\twalk_pmd_range_locked(pud, -1, vma, args, bitmap, &first);\n\n\tif (i < PTRS_PER_PMD && get_next_vma(PUD_MASK, PMD_SIZE, args, &start, &end))\n\t\tgoto restart;\n}\n\nstatic int walk_pud_range(p4d_t *p4d, unsigned long start, unsigned long end,\n\t\t\t  struct mm_walk *args)\n{\n\tint i;\n\tpud_t *pud;\n\tunsigned long addr;\n\tunsigned long next;\n\tstruct lru_gen_mm_walk *walk = args->private;\n\n\tVM_WARN_ON_ONCE(p4d_leaf(*p4d));\n\n\tpud = pud_offset(p4d, start & P4D_MASK);\nrestart:\n\tfor (i = pud_index(start), addr = start; addr != end; i++, addr = next) {\n\t\tpud_t val = READ_ONCE(pud[i]);\n\n\t\tnext = pud_addr_end(addr, end);\n\n\t\tif (!pud_present(val) || WARN_ON_ONCE(pud_leaf(val)))\n\t\t\tcontinue;\n\n\t\twalk_pmd_range(&val, addr, next, args);\n\n\t\tif (need_resched() || walk->batched >= MAX_LRU_BATCH) {\n\t\t\tend = (addr | ~PUD_MASK) + 1;\n\t\t\tgoto done;\n\t\t}\n\t}\n\n\tif (i < PTRS_PER_PUD && get_next_vma(P4D_MASK, PUD_SIZE, args, &start, &end))\n\t\tgoto restart;\n\n\tend = round_up(end, P4D_SIZE);\ndone:\n\tif (!end || !args->vma)\n\t\treturn 1;\n\n\twalk->next_addr = max(end, args->vma->vm_start);\n\n\treturn -EAGAIN;\n}\n\nstatic void walk_mm(struct lruvec *lruvec, struct mm_struct *mm, struct lru_gen_mm_walk *walk)\n{\n\tstatic const struct mm_walk_ops mm_walk_ops = {\n\t\t.test_walk = should_skip_vma,\n\t\t.p4d_entry = walk_pud_range,\n\t\t.walk_lock = PGWALK_RDLOCK,\n\t};\n\n\tint err;\n\tstruct mem_cgroup *memcg = lruvec_memcg(lruvec);\n\n\twalk->next_addr = FIRST_USER_ADDRESS;\n\n\tdo {\n\t\tDEFINE_MAX_SEQ(lruvec);\n\n\t\terr = -EBUSY;\n\n\t\t \n\t\tif (walk->max_seq != max_seq)\n\t\t\tbreak;\n\n\t\t \n\t\tif (!mem_cgroup_trylock_pages(memcg))\n\t\t\tbreak;\n\n\t\t \n\t\tif (mmap_read_trylock(mm)) {\n\t\t\terr = walk_page_range(mm, walk->next_addr, ULONG_MAX, &mm_walk_ops, walk);\n\n\t\t\tmmap_read_unlock(mm);\n\t\t}\n\n\t\tmem_cgroup_unlock_pages();\n\n\t\tif (walk->batched) {\n\t\t\tspin_lock_irq(&lruvec->lru_lock);\n\t\t\treset_batch_size(lruvec, walk);\n\t\t\tspin_unlock_irq(&lruvec->lru_lock);\n\t\t}\n\n\t\tcond_resched();\n\t} while (err == -EAGAIN);\n}\n\nstatic struct lru_gen_mm_walk *set_mm_walk(struct pglist_data *pgdat, bool force_alloc)\n{\n\tstruct lru_gen_mm_walk *walk = current->reclaim_state->mm_walk;\n\n\tif (pgdat && current_is_kswapd()) {\n\t\tVM_WARN_ON_ONCE(walk);\n\n\t\twalk = &pgdat->mm_walk;\n\t} else if (!walk && force_alloc) {\n\t\tVM_WARN_ON_ONCE(current_is_kswapd());\n\n\t\twalk = kzalloc(sizeof(*walk), __GFP_HIGH | __GFP_NOMEMALLOC | __GFP_NOWARN);\n\t}\n\n\tcurrent->reclaim_state->mm_walk = walk;\n\n\treturn walk;\n}\n\nstatic void clear_mm_walk(void)\n{\n\tstruct lru_gen_mm_walk *walk = current->reclaim_state->mm_walk;\n\n\tVM_WARN_ON_ONCE(walk && memchr_inv(walk->nr_pages, 0, sizeof(walk->nr_pages)));\n\tVM_WARN_ON_ONCE(walk && memchr_inv(walk->mm_stats, 0, sizeof(walk->mm_stats)));\n\n\tcurrent->reclaim_state->mm_walk = NULL;\n\n\tif (!current_is_kswapd())\n\t\tkfree(walk);\n}\n\nstatic bool inc_min_seq(struct lruvec *lruvec, int type, bool can_swap)\n{\n\tint zone;\n\tint remaining = MAX_LRU_BATCH;\n\tstruct lru_gen_folio *lrugen = &lruvec->lrugen;\n\tint new_gen, old_gen = lru_gen_from_seq(lrugen->min_seq[type]);\n\n\tif (type == LRU_GEN_ANON && !can_swap)\n\t\tgoto done;\n\n\t \n\tfor (zone = 0; zone < MAX_NR_ZONES; zone++) {\n\t\tstruct list_head *head = &lrugen->folios[old_gen][type][zone];\n\n\t\twhile (!list_empty(head)) {\n\t\t\tstruct folio *folio = lru_to_folio(head);\n\n\t\t\tVM_WARN_ON_ONCE_FOLIO(folio_test_unevictable(folio), folio);\n\t\t\tVM_WARN_ON_ONCE_FOLIO(folio_test_active(folio), folio);\n\t\t\tVM_WARN_ON_ONCE_FOLIO(folio_is_file_lru(folio) != type, folio);\n\t\t\tVM_WARN_ON_ONCE_FOLIO(folio_zonenum(folio) != zone, folio);\n\n\t\t\tnew_gen = folio_inc_gen(lruvec, folio, false);\n\t\t\tlist_move_tail(&folio->lru, &lrugen->folios[new_gen][type][zone]);\n\n\t\t\tif (!--remaining)\n\t\t\t\treturn false;\n\t\t}\n\t}\ndone:\n\treset_ctrl_pos(lruvec, type, true);\n\tWRITE_ONCE(lrugen->min_seq[type], lrugen->min_seq[type] + 1);\n\n\treturn true;\n}\n\nstatic bool try_to_inc_min_seq(struct lruvec *lruvec, bool can_swap)\n{\n\tint gen, type, zone;\n\tbool success = false;\n\tstruct lru_gen_folio *lrugen = &lruvec->lrugen;\n\tDEFINE_MIN_SEQ(lruvec);\n\n\tVM_WARN_ON_ONCE(!seq_is_valid(lruvec));\n\n\t \n\tfor (type = !can_swap; type < ANON_AND_FILE; type++) {\n\t\twhile (min_seq[type] + MIN_NR_GENS <= lrugen->max_seq) {\n\t\t\tgen = lru_gen_from_seq(min_seq[type]);\n\n\t\t\tfor (zone = 0; zone < MAX_NR_ZONES; zone++) {\n\t\t\t\tif (!list_empty(&lrugen->folios[gen][type][zone]))\n\t\t\t\t\tgoto next;\n\t\t\t}\n\n\t\t\tmin_seq[type]++;\n\t\t}\nnext:\n\t\t;\n\t}\n\n\t \n\tif (can_swap) {\n\t\tmin_seq[LRU_GEN_ANON] = min(min_seq[LRU_GEN_ANON], min_seq[LRU_GEN_FILE]);\n\t\tmin_seq[LRU_GEN_FILE] = max(min_seq[LRU_GEN_ANON], lrugen->min_seq[LRU_GEN_FILE]);\n\t}\n\n\tfor (type = !can_swap; type < ANON_AND_FILE; type++) {\n\t\tif (min_seq[type] == lrugen->min_seq[type])\n\t\t\tcontinue;\n\n\t\treset_ctrl_pos(lruvec, type, true);\n\t\tWRITE_ONCE(lrugen->min_seq[type], min_seq[type]);\n\t\tsuccess = true;\n\t}\n\n\treturn success;\n}\n\nstatic void inc_max_seq(struct lruvec *lruvec, bool can_swap, bool force_scan)\n{\n\tint prev, next;\n\tint type, zone;\n\tstruct lru_gen_folio *lrugen = &lruvec->lrugen;\nrestart:\n\tspin_lock_irq(&lruvec->lru_lock);\n\n\tVM_WARN_ON_ONCE(!seq_is_valid(lruvec));\n\n\tfor (type = ANON_AND_FILE - 1; type >= 0; type--) {\n\t\tif (get_nr_gens(lruvec, type) != MAX_NR_GENS)\n\t\t\tcontinue;\n\n\t\tVM_WARN_ON_ONCE(!force_scan && (type == LRU_GEN_FILE || can_swap));\n\n\t\tif (inc_min_seq(lruvec, type, can_swap))\n\t\t\tcontinue;\n\n\t\tspin_unlock_irq(&lruvec->lru_lock);\n\t\tcond_resched();\n\t\tgoto restart;\n\t}\n\n\t \n\tprev = lru_gen_from_seq(lrugen->max_seq - 1);\n\tnext = lru_gen_from_seq(lrugen->max_seq + 1);\n\n\tfor (type = 0; type < ANON_AND_FILE; type++) {\n\t\tfor (zone = 0; zone < MAX_NR_ZONES; zone++) {\n\t\t\tenum lru_list lru = type * LRU_INACTIVE_FILE;\n\t\t\tlong delta = lrugen->nr_pages[prev][type][zone] -\n\t\t\t\t     lrugen->nr_pages[next][type][zone];\n\n\t\t\tif (!delta)\n\t\t\t\tcontinue;\n\n\t\t\t__update_lru_size(lruvec, lru, zone, delta);\n\t\t\t__update_lru_size(lruvec, lru + LRU_ACTIVE, zone, -delta);\n\t\t}\n\t}\n\n\tfor (type = 0; type < ANON_AND_FILE; type++)\n\t\treset_ctrl_pos(lruvec, type, false);\n\n\tWRITE_ONCE(lrugen->timestamps[next], jiffies);\n\t \n\tsmp_store_release(&lrugen->max_seq, lrugen->max_seq + 1);\n\n\tspin_unlock_irq(&lruvec->lru_lock);\n}\n\nstatic bool try_to_inc_max_seq(struct lruvec *lruvec, unsigned long max_seq,\n\t\t\t       struct scan_control *sc, bool can_swap, bool force_scan)\n{\n\tbool success;\n\tstruct lru_gen_mm_walk *walk;\n\tstruct mm_struct *mm = NULL;\n\tstruct lru_gen_folio *lrugen = &lruvec->lrugen;\n\n\tVM_WARN_ON_ONCE(max_seq > READ_ONCE(lrugen->max_seq));\n\n\t \n\tif (max_seq <= READ_ONCE(lruvec->mm_state.seq)) {\n\t\tsuccess = false;\n\t\tgoto done;\n\t}\n\n\t \n\tif (!should_walk_mmu()) {\n\t\tsuccess = iterate_mm_list_nowalk(lruvec, max_seq);\n\t\tgoto done;\n\t}\n\n\twalk = set_mm_walk(NULL, true);\n\tif (!walk) {\n\t\tsuccess = iterate_mm_list_nowalk(lruvec, max_seq);\n\t\tgoto done;\n\t}\n\n\twalk->lruvec = lruvec;\n\twalk->max_seq = max_seq;\n\twalk->can_swap = can_swap;\n\twalk->force_scan = force_scan;\n\n\tdo {\n\t\tsuccess = iterate_mm_list(lruvec, walk, &mm);\n\t\tif (mm)\n\t\t\twalk_mm(lruvec, mm, walk);\n\t} while (mm);\ndone:\n\tif (success)\n\t\tinc_max_seq(lruvec, can_swap, force_scan);\n\n\treturn success;\n}\n\n \n\nstatic bool lruvec_is_sizable(struct lruvec *lruvec, struct scan_control *sc)\n{\n\tint gen, type, zone;\n\tunsigned long total = 0;\n\tbool can_swap = get_swappiness(lruvec, sc);\n\tstruct lru_gen_folio *lrugen = &lruvec->lrugen;\n\tstruct mem_cgroup *memcg = lruvec_memcg(lruvec);\n\tDEFINE_MAX_SEQ(lruvec);\n\tDEFINE_MIN_SEQ(lruvec);\n\n\tfor (type = !can_swap; type < ANON_AND_FILE; type++) {\n\t\tunsigned long seq;\n\n\t\tfor (seq = min_seq[type]; seq <= max_seq; seq++) {\n\t\t\tgen = lru_gen_from_seq(seq);\n\n\t\t\tfor (zone = 0; zone < MAX_NR_ZONES; zone++)\n\t\t\t\ttotal += max(READ_ONCE(lrugen->nr_pages[gen][type][zone]), 0L);\n\t\t}\n\t}\n\n\t \n\treturn mem_cgroup_online(memcg) ? (total >> sc->priority) : total;\n}\n\nstatic bool lruvec_is_reclaimable(struct lruvec *lruvec, struct scan_control *sc,\n\t\t\t\t  unsigned long min_ttl)\n{\n\tint gen;\n\tunsigned long birth;\n\tstruct mem_cgroup *memcg = lruvec_memcg(lruvec);\n\tDEFINE_MIN_SEQ(lruvec);\n\n\t \n\tgen = lru_gen_from_seq(min_seq[LRU_GEN_FILE]);\n\tbirth = READ_ONCE(lruvec->lrugen.timestamps[gen]);\n\n\tif (time_is_after_jiffies(birth + min_ttl))\n\t\treturn false;\n\n\tif (!lruvec_is_sizable(lruvec, sc))\n\t\treturn false;\n\n\tmem_cgroup_calculate_protection(NULL, memcg);\n\n\treturn !mem_cgroup_below_min(NULL, memcg);\n}\n\n \nstatic unsigned long lru_gen_min_ttl __read_mostly;\n\nstatic void lru_gen_age_node(struct pglist_data *pgdat, struct scan_control *sc)\n{\n\tstruct mem_cgroup *memcg;\n\tunsigned long min_ttl = READ_ONCE(lru_gen_min_ttl);\n\n\tVM_WARN_ON_ONCE(!current_is_kswapd());\n\n\t \n\tif (!min_ttl || sc->order || sc->priority == DEF_PRIORITY)\n\t\treturn;\n\n\tmemcg = mem_cgroup_iter(NULL, NULL, NULL);\n\tdo {\n\t\tstruct lruvec *lruvec = mem_cgroup_lruvec(memcg, pgdat);\n\n\t\tif (lruvec_is_reclaimable(lruvec, sc, min_ttl)) {\n\t\t\tmem_cgroup_iter_break(NULL, memcg);\n\t\t\treturn;\n\t\t}\n\n\t\tcond_resched();\n\t} while ((memcg = mem_cgroup_iter(NULL, memcg, NULL)));\n\n\t \n\tif (mutex_trylock(&oom_lock)) {\n\t\tstruct oom_control oc = {\n\t\t\t.gfp_mask = sc->gfp_mask,\n\t\t};\n\n\t\tout_of_memory(&oc);\n\n\t\tmutex_unlock(&oom_lock);\n\t}\n}\n\n \n\n \nvoid lru_gen_look_around(struct page_vma_mapped_walk *pvmw)\n{\n\tint i;\n\tunsigned long start;\n\tunsigned long end;\n\tstruct lru_gen_mm_walk *walk;\n\tint young = 0;\n\tpte_t *pte = pvmw->pte;\n\tunsigned long addr = pvmw->address;\n\tstruct vm_area_struct *vma = pvmw->vma;\n\tstruct folio *folio = pfn_folio(pvmw->pfn);\n\tbool can_swap = !folio_is_file_lru(folio);\n\tstruct mem_cgroup *memcg = folio_memcg(folio);\n\tstruct pglist_data *pgdat = folio_pgdat(folio);\n\tstruct lruvec *lruvec = mem_cgroup_lruvec(memcg, pgdat);\n\tDEFINE_MAX_SEQ(lruvec);\n\tint old_gen, new_gen = lru_gen_from_seq(max_seq);\n\n\tlockdep_assert_held(pvmw->ptl);\n\tVM_WARN_ON_ONCE_FOLIO(folio_test_lru(folio), folio);\n\n\tif (spin_is_contended(pvmw->ptl))\n\t\treturn;\n\n\t \n\tif (vma->vm_flags & VM_SPECIAL)\n\t\treturn;\n\n\t \n\twalk = current->reclaim_state ? current->reclaim_state->mm_walk : NULL;\n\n\tstart = max(addr & PMD_MASK, vma->vm_start);\n\tend = min(addr | ~PMD_MASK, vma->vm_end - 1) + 1;\n\n\tif (end - start > MIN_LRU_BATCH * PAGE_SIZE) {\n\t\tif (addr - start < MIN_LRU_BATCH * PAGE_SIZE / 2)\n\t\t\tend = start + MIN_LRU_BATCH * PAGE_SIZE;\n\t\telse if (end - addr < MIN_LRU_BATCH * PAGE_SIZE / 2)\n\t\t\tstart = end - MIN_LRU_BATCH * PAGE_SIZE;\n\t\telse {\n\t\t\tstart = addr - MIN_LRU_BATCH * PAGE_SIZE / 2;\n\t\t\tend = addr + MIN_LRU_BATCH * PAGE_SIZE / 2;\n\t\t}\n\t}\n\n\t \n\tif (!mem_cgroup_trylock_pages(memcg))\n\t\treturn;\n\n\tarch_enter_lazy_mmu_mode();\n\n\tpte -= (addr - start) / PAGE_SIZE;\n\n\tfor (i = 0, addr = start; addr != end; i++, addr += PAGE_SIZE) {\n\t\tunsigned long pfn;\n\t\tpte_t ptent = ptep_get(pte + i);\n\n\t\tpfn = get_pte_pfn(ptent, vma, addr);\n\t\tif (pfn == -1)\n\t\t\tcontinue;\n\n\t\tif (!pte_young(ptent))\n\t\t\tcontinue;\n\n\t\tfolio = get_pfn_folio(pfn, memcg, pgdat, can_swap);\n\t\tif (!folio)\n\t\t\tcontinue;\n\n\t\tif (!ptep_test_and_clear_young(vma, addr, pte + i))\n\t\t\tVM_WARN_ON_ONCE(true);\n\n\t\tyoung++;\n\n\t\tif (pte_dirty(ptent) && !folio_test_dirty(folio) &&\n\t\t    !(folio_test_anon(folio) && folio_test_swapbacked(folio) &&\n\t\t      !folio_test_swapcache(folio)))\n\t\t\tfolio_mark_dirty(folio);\n\n\t\tif (walk) {\n\t\t\told_gen = folio_update_gen(folio, new_gen);\n\t\t\tif (old_gen >= 0 && old_gen != new_gen)\n\t\t\t\tupdate_batch_size(walk, folio, old_gen, new_gen);\n\n\t\t\tcontinue;\n\t\t}\n\n\t\told_gen = folio_lru_gen(folio);\n\t\tif (old_gen < 0)\n\t\t\tfolio_set_referenced(folio);\n\t\telse if (old_gen != new_gen)\n\t\t\tfolio_activate(folio);\n\t}\n\n\tarch_leave_lazy_mmu_mode();\n\tmem_cgroup_unlock_pages();\n\n\t \n\tif (suitable_to_scan(i, young))\n\t\tupdate_bloom_filter(lruvec, max_seq, pvmw->pmd);\n}\n\n \n\n \nenum {\n\tMEMCG_LRU_NOP,\n\tMEMCG_LRU_HEAD,\n\tMEMCG_LRU_TAIL,\n\tMEMCG_LRU_OLD,\n\tMEMCG_LRU_YOUNG,\n};\n\n#ifdef CONFIG_MEMCG\n\nstatic int lru_gen_memcg_seg(struct lruvec *lruvec)\n{\n\treturn READ_ONCE(lruvec->lrugen.seg);\n}\n\nstatic void lru_gen_rotate_memcg(struct lruvec *lruvec, int op)\n{\n\tint seg;\n\tint old, new;\n\tunsigned long flags;\n\tint bin = get_random_u32_below(MEMCG_NR_BINS);\n\tstruct pglist_data *pgdat = lruvec_pgdat(lruvec);\n\n\tspin_lock_irqsave(&pgdat->memcg_lru.lock, flags);\n\n\tVM_WARN_ON_ONCE(hlist_nulls_unhashed(&lruvec->lrugen.list));\n\n\tseg = 0;\n\tnew = old = lruvec->lrugen.gen;\n\n\t \n\tif (op == MEMCG_LRU_HEAD)\n\t\tseg = MEMCG_LRU_HEAD;\n\telse if (op == MEMCG_LRU_TAIL)\n\t\tseg = MEMCG_LRU_TAIL;\n\telse if (op == MEMCG_LRU_OLD)\n\t\tnew = get_memcg_gen(pgdat->memcg_lru.seq);\n\telse if (op == MEMCG_LRU_YOUNG)\n\t\tnew = get_memcg_gen(pgdat->memcg_lru.seq + 1);\n\telse\n\t\tVM_WARN_ON_ONCE(true);\n\n\tWRITE_ONCE(lruvec->lrugen.seg, seg);\n\tWRITE_ONCE(lruvec->lrugen.gen, new);\n\n\thlist_nulls_del_rcu(&lruvec->lrugen.list);\n\n\tif (op == MEMCG_LRU_HEAD || op == MEMCG_LRU_OLD)\n\t\thlist_nulls_add_head_rcu(&lruvec->lrugen.list, &pgdat->memcg_lru.fifo[new][bin]);\n\telse\n\t\thlist_nulls_add_tail_rcu(&lruvec->lrugen.list, &pgdat->memcg_lru.fifo[new][bin]);\n\n\tpgdat->memcg_lru.nr_memcgs[old]--;\n\tpgdat->memcg_lru.nr_memcgs[new]++;\n\n\tif (!pgdat->memcg_lru.nr_memcgs[old] && old == get_memcg_gen(pgdat->memcg_lru.seq))\n\t\tWRITE_ONCE(pgdat->memcg_lru.seq, pgdat->memcg_lru.seq + 1);\n\n\tspin_unlock_irqrestore(&pgdat->memcg_lru.lock, flags);\n}\n\nvoid lru_gen_online_memcg(struct mem_cgroup *memcg)\n{\n\tint gen;\n\tint nid;\n\tint bin = get_random_u32_below(MEMCG_NR_BINS);\n\n\tfor_each_node(nid) {\n\t\tstruct pglist_data *pgdat = NODE_DATA(nid);\n\t\tstruct lruvec *lruvec = get_lruvec(memcg, nid);\n\n\t\tspin_lock_irq(&pgdat->memcg_lru.lock);\n\n\t\tVM_WARN_ON_ONCE(!hlist_nulls_unhashed(&lruvec->lrugen.list));\n\n\t\tgen = get_memcg_gen(pgdat->memcg_lru.seq);\n\n\t\tlruvec->lrugen.gen = gen;\n\n\t\thlist_nulls_add_tail_rcu(&lruvec->lrugen.list, &pgdat->memcg_lru.fifo[gen][bin]);\n\t\tpgdat->memcg_lru.nr_memcgs[gen]++;\n\n\t\tspin_unlock_irq(&pgdat->memcg_lru.lock);\n\t}\n}\n\nvoid lru_gen_offline_memcg(struct mem_cgroup *memcg)\n{\n\tint nid;\n\n\tfor_each_node(nid) {\n\t\tstruct lruvec *lruvec = get_lruvec(memcg, nid);\n\n\t\tlru_gen_rotate_memcg(lruvec, MEMCG_LRU_OLD);\n\t}\n}\n\nvoid lru_gen_release_memcg(struct mem_cgroup *memcg)\n{\n\tint gen;\n\tint nid;\n\n\tfor_each_node(nid) {\n\t\tstruct pglist_data *pgdat = NODE_DATA(nid);\n\t\tstruct lruvec *lruvec = get_lruvec(memcg, nid);\n\n\t\tspin_lock_irq(&pgdat->memcg_lru.lock);\n\n\t\tif (hlist_nulls_unhashed(&lruvec->lrugen.list))\n\t\t\tgoto unlock;\n\n\t\tgen = lruvec->lrugen.gen;\n\n\t\thlist_nulls_del_init_rcu(&lruvec->lrugen.list);\n\t\tpgdat->memcg_lru.nr_memcgs[gen]--;\n\n\t\tif (!pgdat->memcg_lru.nr_memcgs[gen] && gen == get_memcg_gen(pgdat->memcg_lru.seq))\n\t\t\tWRITE_ONCE(pgdat->memcg_lru.seq, pgdat->memcg_lru.seq + 1);\nunlock:\n\t\tspin_unlock_irq(&pgdat->memcg_lru.lock);\n\t}\n}\n\nvoid lru_gen_soft_reclaim(struct mem_cgroup *memcg, int nid)\n{\n\tstruct lruvec *lruvec = get_lruvec(memcg, nid);\n\n\t \n\tif (lru_gen_memcg_seg(lruvec) != MEMCG_LRU_HEAD)\n\t\tlru_gen_rotate_memcg(lruvec, MEMCG_LRU_HEAD);\n}\n\n#else  \n\nstatic int lru_gen_memcg_seg(struct lruvec *lruvec)\n{\n\treturn 0;\n}\n\n#endif\n\n \n\nstatic bool sort_folio(struct lruvec *lruvec, struct folio *folio, struct scan_control *sc,\n\t\t       int tier_idx)\n{\n\tbool success;\n\tint gen = folio_lru_gen(folio);\n\tint type = folio_is_file_lru(folio);\n\tint zone = folio_zonenum(folio);\n\tint delta = folio_nr_pages(folio);\n\tint refs = folio_lru_refs(folio);\n\tint tier = lru_tier_from_refs(refs);\n\tstruct lru_gen_folio *lrugen = &lruvec->lrugen;\n\n\tVM_WARN_ON_ONCE_FOLIO(gen >= MAX_NR_GENS, folio);\n\n\t \n\tif (!folio_evictable(folio)) {\n\t\tsuccess = lru_gen_del_folio(lruvec, folio, true);\n\t\tVM_WARN_ON_ONCE_FOLIO(!success, folio);\n\t\tfolio_set_unevictable(folio);\n\t\tlruvec_add_folio(lruvec, folio);\n\t\t__count_vm_events(UNEVICTABLE_PGCULLED, delta);\n\t\treturn true;\n\t}\n\n\t \n\tif (type == LRU_GEN_FILE && folio_test_anon(folio) && folio_test_dirty(folio)) {\n\t\tsuccess = lru_gen_del_folio(lruvec, folio, true);\n\t\tVM_WARN_ON_ONCE_FOLIO(!success, folio);\n\t\tfolio_set_swapbacked(folio);\n\t\tlruvec_add_folio_tail(lruvec, folio);\n\t\treturn true;\n\t}\n\n\t \n\tif (gen != lru_gen_from_seq(lrugen->min_seq[type])) {\n\t\tlist_move(&folio->lru, &lrugen->folios[gen][type][zone]);\n\t\treturn true;\n\t}\n\n\t \n\tif (tier > tier_idx || refs == BIT(LRU_REFS_WIDTH)) {\n\t\tint hist = lru_hist_from_seq(lrugen->min_seq[type]);\n\n\t\tgen = folio_inc_gen(lruvec, folio, false);\n\t\tlist_move_tail(&folio->lru, &lrugen->folios[gen][type][zone]);\n\n\t\tWRITE_ONCE(lrugen->protected[hist][type][tier - 1],\n\t\t\t   lrugen->protected[hist][type][tier - 1] + delta);\n\t\treturn true;\n\t}\n\n\t \n\tif (zone > sc->reclaim_idx || skip_cma(folio, sc)) {\n\t\tgen = folio_inc_gen(lruvec, folio, false);\n\t\tlist_move_tail(&folio->lru, &lrugen->folios[gen][type][zone]);\n\t\treturn true;\n\t}\n\n\t \n\tif (folio_test_locked(folio) || folio_test_writeback(folio) ||\n\t    (type == LRU_GEN_FILE && folio_test_dirty(folio))) {\n\t\tgen = folio_inc_gen(lruvec, folio, true);\n\t\tlist_move(&folio->lru, &lrugen->folios[gen][type][zone]);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic bool isolate_folio(struct lruvec *lruvec, struct folio *folio, struct scan_control *sc)\n{\n\tbool success;\n\n\t \n\tif (!(sc->gfp_mask & __GFP_IO) &&\n\t    (folio_test_dirty(folio) ||\n\t     (folio_test_anon(folio) && !folio_test_swapcache(folio))))\n\t\treturn false;\n\n\t \n\tif (!folio_try_get(folio))\n\t\treturn false;\n\n\t \n\tif (!folio_test_clear_lru(folio)) {\n\t\tfolio_put(folio);\n\t\treturn false;\n\t}\n\n\t \n\tif (!folio_test_referenced(folio))\n\t\tset_mask_bits(&folio->flags, LRU_REFS_MASK | LRU_REFS_FLAGS, 0);\n\n\t \n\tfolio_clear_reclaim(folio);\n\tfolio_clear_referenced(folio);\n\n\tsuccess = lru_gen_del_folio(lruvec, folio, true);\n\tVM_WARN_ON_ONCE_FOLIO(!success, folio);\n\n\treturn true;\n}\n\nstatic int scan_folios(struct lruvec *lruvec, struct scan_control *sc,\n\t\t       int type, int tier, struct list_head *list)\n{\n\tint i;\n\tint gen;\n\tenum vm_event_item item;\n\tint sorted = 0;\n\tint scanned = 0;\n\tint isolated = 0;\n\tint remaining = MAX_LRU_BATCH;\n\tstruct lru_gen_folio *lrugen = &lruvec->lrugen;\n\tstruct mem_cgroup *memcg = lruvec_memcg(lruvec);\n\n\tVM_WARN_ON_ONCE(!list_empty(list));\n\n\tif (get_nr_gens(lruvec, type) == MIN_NR_GENS)\n\t\treturn 0;\n\n\tgen = lru_gen_from_seq(lrugen->min_seq[type]);\n\n\tfor (i = MAX_NR_ZONES; i > 0; i--) {\n\t\tLIST_HEAD(moved);\n\t\tint skipped = 0;\n\t\tint zone = (sc->reclaim_idx + i) % MAX_NR_ZONES;\n\t\tstruct list_head *head = &lrugen->folios[gen][type][zone];\n\n\t\twhile (!list_empty(head)) {\n\t\t\tstruct folio *folio = lru_to_folio(head);\n\t\t\tint delta = folio_nr_pages(folio);\n\n\t\t\tVM_WARN_ON_ONCE_FOLIO(folio_test_unevictable(folio), folio);\n\t\t\tVM_WARN_ON_ONCE_FOLIO(folio_test_active(folio), folio);\n\t\t\tVM_WARN_ON_ONCE_FOLIO(folio_is_file_lru(folio) != type, folio);\n\t\t\tVM_WARN_ON_ONCE_FOLIO(folio_zonenum(folio) != zone, folio);\n\n\t\t\tscanned += delta;\n\n\t\t\tif (sort_folio(lruvec, folio, sc, tier))\n\t\t\t\tsorted += delta;\n\t\t\telse if (isolate_folio(lruvec, folio, sc)) {\n\t\t\t\tlist_add(&folio->lru, list);\n\t\t\t\tisolated += delta;\n\t\t\t} else {\n\t\t\t\tlist_move(&folio->lru, &moved);\n\t\t\t\tskipped += delta;\n\t\t\t}\n\n\t\t\tif (!--remaining || max(isolated, skipped) >= MIN_LRU_BATCH)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (skipped) {\n\t\t\tlist_splice(&moved, head);\n\t\t\t__count_zid_vm_events(PGSCAN_SKIP, zone, skipped);\n\t\t}\n\n\t\tif (!remaining || isolated >= MIN_LRU_BATCH)\n\t\t\tbreak;\n\t}\n\n\titem = PGSCAN_KSWAPD + reclaimer_offset();\n\tif (!cgroup_reclaim(sc)) {\n\t\t__count_vm_events(item, isolated);\n\t\t__count_vm_events(PGREFILL, sorted);\n\t}\n\t__count_memcg_events(memcg, item, isolated);\n\t__count_memcg_events(memcg, PGREFILL, sorted);\n\t__count_vm_events(PGSCAN_ANON + type, isolated);\n\n\t \n\treturn isolated || !remaining ? scanned : 0;\n}\n\nstatic int get_tier_idx(struct lruvec *lruvec, int type)\n{\n\tint tier;\n\tstruct ctrl_pos sp, pv;\n\n\t \n\tread_ctrl_pos(lruvec, type, 0, 1, &sp);\n\tfor (tier = 1; tier < MAX_NR_TIERS; tier++) {\n\t\tread_ctrl_pos(lruvec, type, tier, 2, &pv);\n\t\tif (!positive_ctrl_err(&sp, &pv))\n\t\t\tbreak;\n\t}\n\n\treturn tier - 1;\n}\n\nstatic int get_type_to_scan(struct lruvec *lruvec, int swappiness, int *tier_idx)\n{\n\tint type, tier;\n\tstruct ctrl_pos sp, pv;\n\tint gain[ANON_AND_FILE] = { swappiness, 200 - swappiness };\n\n\t \n\tread_ctrl_pos(lruvec, LRU_GEN_ANON, 0, gain[LRU_GEN_ANON], &sp);\n\tread_ctrl_pos(lruvec, LRU_GEN_FILE, 0, gain[LRU_GEN_FILE], &pv);\n\ttype = positive_ctrl_err(&sp, &pv);\n\n\tread_ctrl_pos(lruvec, !type, 0, gain[!type], &sp);\n\tfor (tier = 1; tier < MAX_NR_TIERS; tier++) {\n\t\tread_ctrl_pos(lruvec, type, tier, gain[type], &pv);\n\t\tif (!positive_ctrl_err(&sp, &pv))\n\t\t\tbreak;\n\t}\n\n\t*tier_idx = tier - 1;\n\n\treturn type;\n}\n\nstatic int isolate_folios(struct lruvec *lruvec, struct scan_control *sc, int swappiness,\n\t\t\t  int *type_scanned, struct list_head *list)\n{\n\tint i;\n\tint type;\n\tint scanned;\n\tint tier = -1;\n\tDEFINE_MIN_SEQ(lruvec);\n\n\t \n\tif (!swappiness)\n\t\ttype = LRU_GEN_FILE;\n\telse if (min_seq[LRU_GEN_ANON] < min_seq[LRU_GEN_FILE])\n\t\ttype = LRU_GEN_ANON;\n\telse if (swappiness == 1)\n\t\ttype = LRU_GEN_FILE;\n\telse if (swappiness == 200)\n\t\ttype = LRU_GEN_ANON;\n\telse\n\t\ttype = get_type_to_scan(lruvec, swappiness, &tier);\n\n\tfor (i = !swappiness; i < ANON_AND_FILE; i++) {\n\t\tif (tier < 0)\n\t\t\ttier = get_tier_idx(lruvec, type);\n\n\t\tscanned = scan_folios(lruvec, sc, type, tier, list);\n\t\tif (scanned)\n\t\t\tbreak;\n\n\t\ttype = !type;\n\t\ttier = -1;\n\t}\n\n\t*type_scanned = type;\n\n\treturn scanned;\n}\n\nstatic int evict_folios(struct lruvec *lruvec, struct scan_control *sc, int swappiness)\n{\n\tint type;\n\tint scanned;\n\tint reclaimed;\n\tLIST_HEAD(list);\n\tLIST_HEAD(clean);\n\tstruct folio *folio;\n\tstruct folio *next;\n\tenum vm_event_item item;\n\tstruct reclaim_stat stat;\n\tstruct lru_gen_mm_walk *walk;\n\tbool skip_retry = false;\n\tstruct mem_cgroup *memcg = lruvec_memcg(lruvec);\n\tstruct pglist_data *pgdat = lruvec_pgdat(lruvec);\n\n\tspin_lock_irq(&lruvec->lru_lock);\n\n\tscanned = isolate_folios(lruvec, sc, swappiness, &type, &list);\n\n\tscanned += try_to_inc_min_seq(lruvec, swappiness);\n\n\tif (get_nr_gens(lruvec, !swappiness) == MIN_NR_GENS)\n\t\tscanned = 0;\n\n\tspin_unlock_irq(&lruvec->lru_lock);\n\n\tif (list_empty(&list))\n\t\treturn scanned;\nretry:\n\treclaimed = shrink_folio_list(&list, pgdat, sc, &stat, false);\n\tsc->nr_reclaimed += reclaimed;\n\n\tlist_for_each_entry_safe_reverse(folio, next, &list, lru) {\n\t\tif (!folio_evictable(folio)) {\n\t\t\tlist_del(&folio->lru);\n\t\t\tfolio_putback_lru(folio);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (folio_test_reclaim(folio) &&\n\t\t    (folio_test_dirty(folio) || folio_test_writeback(folio))) {\n\t\t\t \n\t\t\tif (folio_test_workingset(folio))\n\t\t\t\tfolio_set_referenced(folio);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (skip_retry || folio_test_active(folio) || folio_test_referenced(folio) ||\n\t\t    folio_mapped(folio) || folio_test_locked(folio) ||\n\t\t    folio_test_dirty(folio) || folio_test_writeback(folio)) {\n\t\t\t \n\t\t\tset_mask_bits(&folio->flags, LRU_REFS_MASK | LRU_REFS_FLAGS,\n\t\t\t\t      BIT(PG_active));\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tlist_move(&folio->lru, &clean);\n\t\tsc->nr_scanned -= folio_nr_pages(folio);\n\t}\n\n\tspin_lock_irq(&lruvec->lru_lock);\n\n\tmove_folios_to_lru(lruvec, &list);\n\n\twalk = current->reclaim_state->mm_walk;\n\tif (walk && walk->batched)\n\t\treset_batch_size(lruvec, walk);\n\n\titem = PGSTEAL_KSWAPD + reclaimer_offset();\n\tif (!cgroup_reclaim(sc))\n\t\t__count_vm_events(item, reclaimed);\n\t__count_memcg_events(memcg, item, reclaimed);\n\t__count_vm_events(PGSTEAL_ANON + type, reclaimed);\n\n\tspin_unlock_irq(&lruvec->lru_lock);\n\n\tmem_cgroup_uncharge_list(&list);\n\tfree_unref_page_list(&list);\n\n\tINIT_LIST_HEAD(&list);\n\tlist_splice_init(&clean, &list);\n\n\tif (!list_empty(&list)) {\n\t\tskip_retry = true;\n\t\tgoto retry;\n\t}\n\n\treturn scanned;\n}\n\nstatic bool should_run_aging(struct lruvec *lruvec, unsigned long max_seq,\n\t\t\t     struct scan_control *sc, bool can_swap, unsigned long *nr_to_scan)\n{\n\tint gen, type, zone;\n\tunsigned long old = 0;\n\tunsigned long young = 0;\n\tunsigned long total = 0;\n\tstruct lru_gen_folio *lrugen = &lruvec->lrugen;\n\tstruct mem_cgroup *memcg = lruvec_memcg(lruvec);\n\tDEFINE_MIN_SEQ(lruvec);\n\n\t \n\tif (min_seq[!can_swap] + MIN_NR_GENS > max_seq) {\n\t\t*nr_to_scan = 0;\n\t\treturn true;\n\t}\n\n\tfor (type = !can_swap; type < ANON_AND_FILE; type++) {\n\t\tunsigned long seq;\n\n\t\tfor (seq = min_seq[type]; seq <= max_seq; seq++) {\n\t\t\tunsigned long size = 0;\n\n\t\t\tgen = lru_gen_from_seq(seq);\n\n\t\t\tfor (zone = 0; zone < MAX_NR_ZONES; zone++)\n\t\t\t\tsize += max(READ_ONCE(lrugen->nr_pages[gen][type][zone]), 0L);\n\n\t\t\ttotal += size;\n\t\t\tif (seq == max_seq)\n\t\t\t\tyoung += size;\n\t\t\telse if (seq + MIN_NR_GENS == max_seq)\n\t\t\t\told += size;\n\t\t}\n\t}\n\n\t \n\tif (!mem_cgroup_online(memcg)) {\n\t\t*nr_to_scan = total;\n\t\treturn false;\n\t}\n\n\t*nr_to_scan = total >> sc->priority;\n\n\t \n\tif (min_seq[!can_swap] + MIN_NR_GENS < max_seq)\n\t\treturn false;\n\n\t \n\tif (young * MIN_NR_GENS > total)\n\t\treturn true;\n\tif (old * (MIN_NR_GENS + 2) < total)\n\t\treturn true;\n\n\treturn false;\n}\n\n \nstatic long get_nr_to_scan(struct lruvec *lruvec, struct scan_control *sc, bool can_swap)\n{\n\tunsigned long nr_to_scan;\n\tstruct mem_cgroup *memcg = lruvec_memcg(lruvec);\n\tDEFINE_MAX_SEQ(lruvec);\n\n\tif (mem_cgroup_below_min(sc->target_mem_cgroup, memcg))\n\t\treturn -1;\n\n\tif (!should_run_aging(lruvec, max_seq, sc, can_swap, &nr_to_scan))\n\t\treturn nr_to_scan;\n\n\t \n\tif (sc->priority == DEF_PRIORITY)\n\t\treturn nr_to_scan;\n\n\t \n\treturn try_to_inc_max_seq(lruvec, max_seq, sc, can_swap, false) ? -1 : 0;\n}\n\nstatic bool should_abort_scan(struct lruvec *lruvec, struct scan_control *sc)\n{\n\tint i;\n\tenum zone_watermarks mark;\n\n\t \n\tif (!root_reclaim(sc))\n\t\treturn false;\n\n\tif (sc->nr_reclaimed >= max(sc->nr_to_reclaim, compact_gap(sc->order)))\n\t\treturn true;\n\n\t \n\tif (!current_is_kswapd() || sc->order)\n\t\treturn false;\n\n\tmark = sysctl_numa_balancing_mode & NUMA_BALANCING_MEMORY_TIERING ?\n\t       WMARK_PROMO : WMARK_HIGH;\n\n\tfor (i = 0; i <= sc->reclaim_idx; i++) {\n\t\tstruct zone *zone = lruvec_pgdat(lruvec)->node_zones + i;\n\t\tunsigned long size = wmark_pages(zone, mark) + MIN_LRU_BATCH;\n\n\t\tif (managed_zone(zone) && !zone_watermark_ok(zone, 0, size, sc->reclaim_idx, 0))\n\t\t\treturn false;\n\t}\n\n\t \n\treturn true;\n}\n\nstatic bool try_to_shrink_lruvec(struct lruvec *lruvec, struct scan_control *sc)\n{\n\tlong nr_to_scan;\n\tunsigned long scanned = 0;\n\tint swappiness = get_swappiness(lruvec, sc);\n\n\t \n\tif (swappiness && !(sc->gfp_mask & __GFP_IO))\n\t\tswappiness = 1;\n\n\twhile (true) {\n\t\tint delta;\n\n\t\tnr_to_scan = get_nr_to_scan(lruvec, sc, swappiness);\n\t\tif (nr_to_scan <= 0)\n\t\t\tbreak;\n\n\t\tdelta = evict_folios(lruvec, sc, swappiness);\n\t\tif (!delta)\n\t\t\tbreak;\n\n\t\tscanned += delta;\n\t\tif (scanned >= nr_to_scan)\n\t\t\tbreak;\n\n\t\tif (should_abort_scan(lruvec, sc))\n\t\t\tbreak;\n\n\t\tcond_resched();\n\t}\n\n\t \n\treturn nr_to_scan < 0;\n}\n\nstatic int shrink_one(struct lruvec *lruvec, struct scan_control *sc)\n{\n\tbool success;\n\tunsigned long scanned = sc->nr_scanned;\n\tunsigned long reclaimed = sc->nr_reclaimed;\n\tstruct mem_cgroup *memcg = lruvec_memcg(lruvec);\n\tstruct pglist_data *pgdat = lruvec_pgdat(lruvec);\n\n\tmem_cgroup_calculate_protection(NULL, memcg);\n\n\tif (mem_cgroup_below_min(NULL, memcg))\n\t\treturn MEMCG_LRU_YOUNG;\n\n\tif (mem_cgroup_below_low(NULL, memcg)) {\n\t\t \n\t\tif (lru_gen_memcg_seg(lruvec) != MEMCG_LRU_TAIL)\n\t\t\treturn MEMCG_LRU_TAIL;\n\n\t\tmemcg_memory_event(memcg, MEMCG_LOW);\n\t}\n\n\tsuccess = try_to_shrink_lruvec(lruvec, sc);\n\n\tshrink_slab(sc->gfp_mask, pgdat->node_id, memcg, sc->priority);\n\n\tif (!sc->proactive)\n\t\tvmpressure(sc->gfp_mask, memcg, false, sc->nr_scanned - scanned,\n\t\t\t   sc->nr_reclaimed - reclaimed);\n\n\tflush_reclaim_state(sc);\n\n\tif (success && mem_cgroup_online(memcg))\n\t\treturn MEMCG_LRU_YOUNG;\n\n\tif (!success && lruvec_is_sizable(lruvec, sc))\n\t\treturn 0;\n\n\t \n\treturn lru_gen_memcg_seg(lruvec) != MEMCG_LRU_TAIL ?\n\t       MEMCG_LRU_TAIL : MEMCG_LRU_YOUNG;\n}\n\n#ifdef CONFIG_MEMCG\n\nstatic void shrink_many(struct pglist_data *pgdat, struct scan_control *sc)\n{\n\tint op;\n\tint gen;\n\tint bin;\n\tint first_bin;\n\tstruct lruvec *lruvec;\n\tstruct lru_gen_folio *lrugen;\n\tstruct mem_cgroup *memcg;\n\tstruct hlist_nulls_node *pos;\n\n\tgen = get_memcg_gen(READ_ONCE(pgdat->memcg_lru.seq));\n\tbin = first_bin = get_random_u32_below(MEMCG_NR_BINS);\nrestart:\n\top = 0;\n\tmemcg = NULL;\n\n\trcu_read_lock();\n\n\thlist_nulls_for_each_entry_rcu(lrugen, pos, &pgdat->memcg_lru.fifo[gen][bin], list) {\n\t\tif (op) {\n\t\t\tlru_gen_rotate_memcg(lruvec, op);\n\t\t\top = 0;\n\t\t}\n\n\t\tmem_cgroup_put(memcg);\n\t\tmemcg = NULL;\n\n\t\tif (gen != READ_ONCE(lrugen->gen))\n\t\t\tcontinue;\n\n\t\tlruvec = container_of(lrugen, struct lruvec, lrugen);\n\t\tmemcg = lruvec_memcg(lruvec);\n\n\t\tif (!mem_cgroup_tryget(memcg)) {\n\t\t\tlru_gen_release_memcg(memcg);\n\t\t\tmemcg = NULL;\n\t\t\tcontinue;\n\t\t}\n\n\t\trcu_read_unlock();\n\n\t\top = shrink_one(lruvec, sc);\n\n\t\trcu_read_lock();\n\n\t\tif (should_abort_scan(lruvec, sc))\n\t\t\tbreak;\n\t}\n\n\trcu_read_unlock();\n\n\tif (op)\n\t\tlru_gen_rotate_memcg(lruvec, op);\n\n\tmem_cgroup_put(memcg);\n\n\tif (!is_a_nulls(pos))\n\t\treturn;\n\n\t \n\tif (gen != get_nulls_value(pos))\n\t\tgoto restart;\n\n\t \n\tbin = get_memcg_bin(bin + 1);\n\tif (bin != first_bin)\n\t\tgoto restart;\n}\n\nstatic void lru_gen_shrink_lruvec(struct lruvec *lruvec, struct scan_control *sc)\n{\n\tstruct blk_plug plug;\n\n\tVM_WARN_ON_ONCE(root_reclaim(sc));\n\tVM_WARN_ON_ONCE(!sc->may_writepage || !sc->may_unmap);\n\n\tlru_add_drain();\n\n\tblk_start_plug(&plug);\n\n\tset_mm_walk(NULL, sc->proactive);\n\n\tif (try_to_shrink_lruvec(lruvec, sc))\n\t\tlru_gen_rotate_memcg(lruvec, MEMCG_LRU_YOUNG);\n\n\tclear_mm_walk();\n\n\tblk_finish_plug(&plug);\n}\n\n#else  \n\nstatic void shrink_many(struct pglist_data *pgdat, struct scan_control *sc)\n{\n\tBUILD_BUG();\n}\n\nstatic void lru_gen_shrink_lruvec(struct lruvec *lruvec, struct scan_control *sc)\n{\n\tBUILD_BUG();\n}\n\n#endif\n\nstatic void set_initial_priority(struct pglist_data *pgdat, struct scan_control *sc)\n{\n\tint priority;\n\tunsigned long reclaimable;\n\tstruct lruvec *lruvec = mem_cgroup_lruvec(NULL, pgdat);\n\n\tif (sc->priority != DEF_PRIORITY || sc->nr_to_reclaim < MIN_LRU_BATCH)\n\t\treturn;\n\t \n\treclaimable = node_page_state(pgdat, NR_INACTIVE_FILE);\n\tif (get_swappiness(lruvec, sc))\n\t\treclaimable += node_page_state(pgdat, NR_INACTIVE_ANON);\n\n\t \n\tpriority = fls_long(reclaimable) - 1 - fls_long(sc->nr_to_reclaim - 1);\n\n\tsc->priority = clamp(priority, 0, DEF_PRIORITY);\n}\n\nstatic void lru_gen_shrink_node(struct pglist_data *pgdat, struct scan_control *sc)\n{\n\tstruct blk_plug plug;\n\tunsigned long reclaimed = sc->nr_reclaimed;\n\n\tVM_WARN_ON_ONCE(!root_reclaim(sc));\n\n\t \n\tif (!sc->may_writepage || !sc->may_unmap)\n\t\tgoto done;\n\n\tlru_add_drain();\n\n\tblk_start_plug(&plug);\n\n\tset_mm_walk(pgdat, sc->proactive);\n\n\tset_initial_priority(pgdat, sc);\n\n\tif (current_is_kswapd())\n\t\tsc->nr_reclaimed = 0;\n\n\tif (mem_cgroup_disabled())\n\t\tshrink_one(&pgdat->__lruvec, sc);\n\telse\n\t\tshrink_many(pgdat, sc);\n\n\tif (current_is_kswapd())\n\t\tsc->nr_reclaimed += reclaimed;\n\n\tclear_mm_walk();\n\n\tblk_finish_plug(&plug);\ndone:\n\t \n\tpgdat->kswapd_failures = 0;\n}\n\n \n\nstatic bool __maybe_unused state_is_valid(struct lruvec *lruvec)\n{\n\tstruct lru_gen_folio *lrugen = &lruvec->lrugen;\n\n\tif (lrugen->enabled) {\n\t\tenum lru_list lru;\n\n\t\tfor_each_evictable_lru(lru) {\n\t\t\tif (!list_empty(&lruvec->lists[lru]))\n\t\t\t\treturn false;\n\t\t}\n\t} else {\n\t\tint gen, type, zone;\n\n\t\tfor_each_gen_type_zone(gen, type, zone) {\n\t\t\tif (!list_empty(&lrugen->folios[gen][type][zone]))\n\t\t\t\treturn false;\n\t\t}\n\t}\n\n\treturn true;\n}\n\nstatic bool fill_evictable(struct lruvec *lruvec)\n{\n\tenum lru_list lru;\n\tint remaining = MAX_LRU_BATCH;\n\n\tfor_each_evictable_lru(lru) {\n\t\tint type = is_file_lru(lru);\n\t\tbool active = is_active_lru(lru);\n\t\tstruct list_head *head = &lruvec->lists[lru];\n\n\t\twhile (!list_empty(head)) {\n\t\t\tbool success;\n\t\t\tstruct folio *folio = lru_to_folio(head);\n\n\t\t\tVM_WARN_ON_ONCE_FOLIO(folio_test_unevictable(folio), folio);\n\t\t\tVM_WARN_ON_ONCE_FOLIO(folio_test_active(folio) != active, folio);\n\t\t\tVM_WARN_ON_ONCE_FOLIO(folio_is_file_lru(folio) != type, folio);\n\t\t\tVM_WARN_ON_ONCE_FOLIO(folio_lru_gen(folio) != -1, folio);\n\n\t\t\tlruvec_del_folio(lruvec, folio);\n\t\t\tsuccess = lru_gen_add_folio(lruvec, folio, false);\n\t\t\tVM_WARN_ON_ONCE(!success);\n\n\t\t\tif (!--remaining)\n\t\t\t\treturn false;\n\t\t}\n\t}\n\n\treturn true;\n}\n\nstatic bool drain_evictable(struct lruvec *lruvec)\n{\n\tint gen, type, zone;\n\tint remaining = MAX_LRU_BATCH;\n\n\tfor_each_gen_type_zone(gen, type, zone) {\n\t\tstruct list_head *head = &lruvec->lrugen.folios[gen][type][zone];\n\n\t\twhile (!list_empty(head)) {\n\t\t\tbool success;\n\t\t\tstruct folio *folio = lru_to_folio(head);\n\n\t\t\tVM_WARN_ON_ONCE_FOLIO(folio_test_unevictable(folio), folio);\n\t\t\tVM_WARN_ON_ONCE_FOLIO(folio_test_active(folio), folio);\n\t\t\tVM_WARN_ON_ONCE_FOLIO(folio_is_file_lru(folio) != type, folio);\n\t\t\tVM_WARN_ON_ONCE_FOLIO(folio_zonenum(folio) != zone, folio);\n\n\t\t\tsuccess = lru_gen_del_folio(lruvec, folio, false);\n\t\t\tVM_WARN_ON_ONCE(!success);\n\t\t\tlruvec_add_folio(lruvec, folio);\n\n\t\t\tif (!--remaining)\n\t\t\t\treturn false;\n\t\t}\n\t}\n\n\treturn true;\n}\n\nstatic void lru_gen_change_state(bool enabled)\n{\n\tstatic DEFINE_MUTEX(state_mutex);\n\n\tstruct mem_cgroup *memcg;\n\n\tcgroup_lock();\n\tcpus_read_lock();\n\tget_online_mems();\n\tmutex_lock(&state_mutex);\n\n\tif (enabled == lru_gen_enabled())\n\t\tgoto unlock;\n\n\tif (enabled)\n\t\tstatic_branch_enable_cpuslocked(&lru_gen_caps[LRU_GEN_CORE]);\n\telse\n\t\tstatic_branch_disable_cpuslocked(&lru_gen_caps[LRU_GEN_CORE]);\n\n\tmemcg = mem_cgroup_iter(NULL, NULL, NULL);\n\tdo {\n\t\tint nid;\n\n\t\tfor_each_node(nid) {\n\t\t\tstruct lruvec *lruvec = get_lruvec(memcg, nid);\n\n\t\t\tspin_lock_irq(&lruvec->lru_lock);\n\n\t\t\tVM_WARN_ON_ONCE(!seq_is_valid(lruvec));\n\t\t\tVM_WARN_ON_ONCE(!state_is_valid(lruvec));\n\n\t\t\tlruvec->lrugen.enabled = enabled;\n\n\t\t\twhile (!(enabled ? fill_evictable(lruvec) : drain_evictable(lruvec))) {\n\t\t\t\tspin_unlock_irq(&lruvec->lru_lock);\n\t\t\t\tcond_resched();\n\t\t\t\tspin_lock_irq(&lruvec->lru_lock);\n\t\t\t}\n\n\t\t\tspin_unlock_irq(&lruvec->lru_lock);\n\t\t}\n\n\t\tcond_resched();\n\t} while ((memcg = mem_cgroup_iter(NULL, memcg, NULL)));\nunlock:\n\tmutex_unlock(&state_mutex);\n\tput_online_mems();\n\tcpus_read_unlock();\n\tcgroup_unlock();\n}\n\n \n\nstatic ssize_t min_ttl_ms_show(struct kobject *kobj, struct kobj_attribute *attr, char *buf)\n{\n\treturn sysfs_emit(buf, \"%u\\n\", jiffies_to_msecs(READ_ONCE(lru_gen_min_ttl)));\n}\n\n \nstatic ssize_t min_ttl_ms_store(struct kobject *kobj, struct kobj_attribute *attr,\n\t\t\t\tconst char *buf, size_t len)\n{\n\tunsigned int msecs;\n\n\tif (kstrtouint(buf, 0, &msecs))\n\t\treturn -EINVAL;\n\n\tWRITE_ONCE(lru_gen_min_ttl, msecs_to_jiffies(msecs));\n\n\treturn len;\n}\n\nstatic struct kobj_attribute lru_gen_min_ttl_attr = __ATTR_RW(min_ttl_ms);\n\nstatic ssize_t enabled_show(struct kobject *kobj, struct kobj_attribute *attr, char *buf)\n{\n\tunsigned int caps = 0;\n\n\tif (get_cap(LRU_GEN_CORE))\n\t\tcaps |= BIT(LRU_GEN_CORE);\n\n\tif (should_walk_mmu())\n\t\tcaps |= BIT(LRU_GEN_MM_WALK);\n\n\tif (should_clear_pmd_young())\n\t\tcaps |= BIT(LRU_GEN_NONLEAF_YOUNG);\n\n\treturn sysfs_emit(buf, \"0x%04x\\n\", caps);\n}\n\n \nstatic ssize_t enabled_store(struct kobject *kobj, struct kobj_attribute *attr,\n\t\t\t     const char *buf, size_t len)\n{\n\tint i;\n\tunsigned int caps;\n\n\tif (tolower(*buf) == 'n')\n\t\tcaps = 0;\n\telse if (tolower(*buf) == 'y')\n\t\tcaps = -1;\n\telse if (kstrtouint(buf, 0, &caps))\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < NR_LRU_GEN_CAPS; i++) {\n\t\tbool enabled = caps & BIT(i);\n\n\t\tif (i == LRU_GEN_CORE)\n\t\t\tlru_gen_change_state(enabled);\n\t\telse if (enabled)\n\t\t\tstatic_branch_enable(&lru_gen_caps[i]);\n\t\telse\n\t\t\tstatic_branch_disable(&lru_gen_caps[i]);\n\t}\n\n\treturn len;\n}\n\nstatic struct kobj_attribute lru_gen_enabled_attr = __ATTR_RW(enabled);\n\nstatic struct attribute *lru_gen_attrs[] = {\n\t&lru_gen_min_ttl_attr.attr,\n\t&lru_gen_enabled_attr.attr,\n\tNULL\n};\n\nstatic const struct attribute_group lru_gen_attr_group = {\n\t.name = \"lru_gen\",\n\t.attrs = lru_gen_attrs,\n};\n\n \n\nstatic void *lru_gen_seq_start(struct seq_file *m, loff_t *pos)\n{\n\tstruct mem_cgroup *memcg;\n\tloff_t nr_to_skip = *pos;\n\n\tm->private = kvmalloc(PATH_MAX, GFP_KERNEL);\n\tif (!m->private)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmemcg = mem_cgroup_iter(NULL, NULL, NULL);\n\tdo {\n\t\tint nid;\n\n\t\tfor_each_node_state(nid, N_MEMORY) {\n\t\t\tif (!nr_to_skip--)\n\t\t\t\treturn get_lruvec(memcg, nid);\n\t\t}\n\t} while ((memcg = mem_cgroup_iter(NULL, memcg, NULL)));\n\n\treturn NULL;\n}\n\nstatic void lru_gen_seq_stop(struct seq_file *m, void *v)\n{\n\tif (!IS_ERR_OR_NULL(v))\n\t\tmem_cgroup_iter_break(NULL, lruvec_memcg(v));\n\n\tkvfree(m->private);\n\tm->private = NULL;\n}\n\nstatic void *lru_gen_seq_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tint nid = lruvec_pgdat(v)->node_id;\n\tstruct mem_cgroup *memcg = lruvec_memcg(v);\n\n\t++*pos;\n\n\tnid = next_memory_node(nid);\n\tif (nid == MAX_NUMNODES) {\n\t\tmemcg = mem_cgroup_iter(NULL, memcg, NULL);\n\t\tif (!memcg)\n\t\t\treturn NULL;\n\n\t\tnid = first_memory_node;\n\t}\n\n\treturn get_lruvec(memcg, nid);\n}\n\nstatic void lru_gen_seq_show_full(struct seq_file *m, struct lruvec *lruvec,\n\t\t\t\t  unsigned long max_seq, unsigned long *min_seq,\n\t\t\t\t  unsigned long seq)\n{\n\tint i;\n\tint type, tier;\n\tint hist = lru_hist_from_seq(seq);\n\tstruct lru_gen_folio *lrugen = &lruvec->lrugen;\n\n\tfor (tier = 0; tier < MAX_NR_TIERS; tier++) {\n\t\tseq_printf(m, \"            %10d\", tier);\n\t\tfor (type = 0; type < ANON_AND_FILE; type++) {\n\t\t\tconst char *s = \"   \";\n\t\t\tunsigned long n[3] = {};\n\n\t\t\tif (seq == max_seq) {\n\t\t\t\ts = \"RT \";\n\t\t\t\tn[0] = READ_ONCE(lrugen->avg_refaulted[type][tier]);\n\t\t\t\tn[1] = READ_ONCE(lrugen->avg_total[type][tier]);\n\t\t\t} else if (seq == min_seq[type] || NR_HIST_GENS > 1) {\n\t\t\t\ts = \"rep\";\n\t\t\t\tn[0] = atomic_long_read(&lrugen->refaulted[hist][type][tier]);\n\t\t\t\tn[1] = atomic_long_read(&lrugen->evicted[hist][type][tier]);\n\t\t\t\tif (tier)\n\t\t\t\t\tn[2] = READ_ONCE(lrugen->protected[hist][type][tier - 1]);\n\t\t\t}\n\n\t\t\tfor (i = 0; i < 3; i++)\n\t\t\t\tseq_printf(m, \" %10lu%c\", n[i], s[i]);\n\t\t}\n\t\tseq_putc(m, '\\n');\n\t}\n\n\tseq_puts(m, \"                      \");\n\tfor (i = 0; i < NR_MM_STATS; i++) {\n\t\tconst char *s = \"      \";\n\t\tunsigned long n = 0;\n\n\t\tif (seq == max_seq && NR_HIST_GENS == 1) {\n\t\t\ts = \"LOYNFA\";\n\t\t\tn = READ_ONCE(lruvec->mm_state.stats[hist][i]);\n\t\t} else if (seq != max_seq && NR_HIST_GENS > 1) {\n\t\t\ts = \"loynfa\";\n\t\t\tn = READ_ONCE(lruvec->mm_state.stats[hist][i]);\n\t\t}\n\n\t\tseq_printf(m, \" %10lu%c\", n, s[i]);\n\t}\n\tseq_putc(m, '\\n');\n}\n\n \nstatic int lru_gen_seq_show(struct seq_file *m, void *v)\n{\n\tunsigned long seq;\n\tbool full = !debugfs_real_fops(m->file)->write;\n\tstruct lruvec *lruvec = v;\n\tstruct lru_gen_folio *lrugen = &lruvec->lrugen;\n\tint nid = lruvec_pgdat(lruvec)->node_id;\n\tstruct mem_cgroup *memcg = lruvec_memcg(lruvec);\n\tDEFINE_MAX_SEQ(lruvec);\n\tDEFINE_MIN_SEQ(lruvec);\n\n\tif (nid == first_memory_node) {\n\t\tconst char *path = memcg ? m->private : \"\";\n\n#ifdef CONFIG_MEMCG\n\t\tif (memcg)\n\t\t\tcgroup_path(memcg->css.cgroup, m->private, PATH_MAX);\n#endif\n\t\tseq_printf(m, \"memcg %5hu %s\\n\", mem_cgroup_id(memcg), path);\n\t}\n\n\tseq_printf(m, \" node %5d\\n\", nid);\n\n\tif (!full)\n\t\tseq = min_seq[LRU_GEN_ANON];\n\telse if (max_seq >= MAX_NR_GENS)\n\t\tseq = max_seq - MAX_NR_GENS + 1;\n\telse\n\t\tseq = 0;\n\n\tfor (; seq <= max_seq; seq++) {\n\t\tint type, zone;\n\t\tint gen = lru_gen_from_seq(seq);\n\t\tunsigned long birth = READ_ONCE(lruvec->lrugen.timestamps[gen]);\n\n\t\tseq_printf(m, \" %10lu %10u\", seq, jiffies_to_msecs(jiffies - birth));\n\n\t\tfor (type = 0; type < ANON_AND_FILE; type++) {\n\t\t\tunsigned long size = 0;\n\t\t\tchar mark = full && seq < min_seq[type] ? 'x' : ' ';\n\n\t\t\tfor (zone = 0; zone < MAX_NR_ZONES; zone++)\n\t\t\t\tsize += max(READ_ONCE(lrugen->nr_pages[gen][type][zone]), 0L);\n\n\t\t\tseq_printf(m, \" %10lu%c\", size, mark);\n\t\t}\n\n\t\tseq_putc(m, '\\n');\n\n\t\tif (full)\n\t\t\tlru_gen_seq_show_full(m, lruvec, max_seq, min_seq, seq);\n\t}\n\n\treturn 0;\n}\n\nstatic const struct seq_operations lru_gen_seq_ops = {\n\t.start = lru_gen_seq_start,\n\t.stop = lru_gen_seq_stop,\n\t.next = lru_gen_seq_next,\n\t.show = lru_gen_seq_show,\n};\n\nstatic int run_aging(struct lruvec *lruvec, unsigned long seq, struct scan_control *sc,\n\t\t     bool can_swap, bool force_scan)\n{\n\tDEFINE_MAX_SEQ(lruvec);\n\tDEFINE_MIN_SEQ(lruvec);\n\n\tif (seq < max_seq)\n\t\treturn 0;\n\n\tif (seq > max_seq)\n\t\treturn -EINVAL;\n\n\tif (!force_scan && min_seq[!can_swap] + MAX_NR_GENS - 1 <= max_seq)\n\t\treturn -ERANGE;\n\n\ttry_to_inc_max_seq(lruvec, max_seq, sc, can_swap, force_scan);\n\n\treturn 0;\n}\n\nstatic int run_eviction(struct lruvec *lruvec, unsigned long seq, struct scan_control *sc,\n\t\t\tint swappiness, unsigned long nr_to_reclaim)\n{\n\tDEFINE_MAX_SEQ(lruvec);\n\n\tif (seq + MIN_NR_GENS > max_seq)\n\t\treturn -EINVAL;\n\n\tsc->nr_reclaimed = 0;\n\n\twhile (!signal_pending(current)) {\n\t\tDEFINE_MIN_SEQ(lruvec);\n\n\t\tif (seq < min_seq[!swappiness])\n\t\t\treturn 0;\n\n\t\tif (sc->nr_reclaimed >= nr_to_reclaim)\n\t\t\treturn 0;\n\n\t\tif (!evict_folios(lruvec, sc, swappiness))\n\t\t\treturn 0;\n\n\t\tcond_resched();\n\t}\n\n\treturn -EINTR;\n}\n\nstatic int run_cmd(char cmd, int memcg_id, int nid, unsigned long seq,\n\t\t   struct scan_control *sc, int swappiness, unsigned long opt)\n{\n\tstruct lruvec *lruvec;\n\tint err = -EINVAL;\n\tstruct mem_cgroup *memcg = NULL;\n\n\tif (nid < 0 || nid >= MAX_NUMNODES || !node_state(nid, N_MEMORY))\n\t\treturn -EINVAL;\n\n\tif (!mem_cgroup_disabled()) {\n\t\trcu_read_lock();\n\n\t\tmemcg = mem_cgroup_from_id(memcg_id);\n\t\tif (!mem_cgroup_tryget(memcg))\n\t\t\tmemcg = NULL;\n\n\t\trcu_read_unlock();\n\n\t\tif (!memcg)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (memcg_id != mem_cgroup_id(memcg))\n\t\tgoto done;\n\n\tlruvec = get_lruvec(memcg, nid);\n\n\tif (swappiness < 0)\n\t\tswappiness = get_swappiness(lruvec, sc);\n\telse if (swappiness > 200)\n\t\tgoto done;\n\n\tswitch (cmd) {\n\tcase '+':\n\t\terr = run_aging(lruvec, seq, sc, swappiness, opt);\n\t\tbreak;\n\tcase '-':\n\t\terr = run_eviction(lruvec, seq, sc, swappiness, opt);\n\t\tbreak;\n\t}\ndone:\n\tmem_cgroup_put(memcg);\n\n\treturn err;\n}\n\n \nstatic ssize_t lru_gen_seq_write(struct file *file, const char __user *src,\n\t\t\t\t size_t len, loff_t *pos)\n{\n\tvoid *buf;\n\tchar *cur, *next;\n\tunsigned int flags;\n\tstruct blk_plug plug;\n\tint err = -EINVAL;\n\tstruct scan_control sc = {\n\t\t.may_writepage = true,\n\t\t.may_unmap = true,\n\t\t.may_swap = true,\n\t\t.reclaim_idx = MAX_NR_ZONES - 1,\n\t\t.gfp_mask = GFP_KERNEL,\n\t};\n\n\tbuf = kvmalloc(len + 1, GFP_KERNEL);\n\tif (!buf)\n\t\treturn -ENOMEM;\n\n\tif (copy_from_user(buf, src, len)) {\n\t\tkvfree(buf);\n\t\treturn -EFAULT;\n\t}\n\n\tset_task_reclaim_state(current, &sc.reclaim_state);\n\tflags = memalloc_noreclaim_save();\n\tblk_start_plug(&plug);\n\tif (!set_mm_walk(NULL, true)) {\n\t\terr = -ENOMEM;\n\t\tgoto done;\n\t}\n\n\tnext = buf;\n\tnext[len] = '\\0';\n\n\twhile ((cur = strsep(&next, \",;\\n\"))) {\n\t\tint n;\n\t\tint end;\n\t\tchar cmd;\n\t\tunsigned int memcg_id;\n\t\tunsigned int nid;\n\t\tunsigned long seq;\n\t\tunsigned int swappiness = -1;\n\t\tunsigned long opt = -1;\n\n\t\tcur = skip_spaces(cur);\n\t\tif (!*cur)\n\t\t\tcontinue;\n\n\t\tn = sscanf(cur, \"%c %u %u %lu %n %u %n %lu %n\", &cmd, &memcg_id, &nid,\n\t\t\t   &seq, &end, &swappiness, &end, &opt, &end);\n\t\tif (n < 4 || cur[end]) {\n\t\t\terr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\terr = run_cmd(cmd, memcg_id, nid, seq, &sc, swappiness, opt);\n\t\tif (err)\n\t\t\tbreak;\n\t}\ndone:\n\tclear_mm_walk();\n\tblk_finish_plug(&plug);\n\tmemalloc_noreclaim_restore(flags);\n\tset_task_reclaim_state(current, NULL);\n\n\tkvfree(buf);\n\n\treturn err ? : len;\n}\n\nstatic int lru_gen_seq_open(struct inode *inode, struct file *file)\n{\n\treturn seq_open(file, &lru_gen_seq_ops);\n}\n\nstatic const struct file_operations lru_gen_rw_fops = {\n\t.open = lru_gen_seq_open,\n\t.read = seq_read,\n\t.write = lru_gen_seq_write,\n\t.llseek = seq_lseek,\n\t.release = seq_release,\n};\n\nstatic const struct file_operations lru_gen_ro_fops = {\n\t.open = lru_gen_seq_open,\n\t.read = seq_read,\n\t.llseek = seq_lseek,\n\t.release = seq_release,\n};\n\n \n\nvoid lru_gen_init_lruvec(struct lruvec *lruvec)\n{\n\tint i;\n\tint gen, type, zone;\n\tstruct lru_gen_folio *lrugen = &lruvec->lrugen;\n\n\tlrugen->max_seq = MIN_NR_GENS + 1;\n\tlrugen->enabled = lru_gen_enabled();\n\n\tfor (i = 0; i <= MIN_NR_GENS + 1; i++)\n\t\tlrugen->timestamps[i] = jiffies;\n\n\tfor_each_gen_type_zone(gen, type, zone)\n\t\tINIT_LIST_HEAD(&lrugen->folios[gen][type][zone]);\n\n\tlruvec->mm_state.seq = MIN_NR_GENS;\n}\n\n#ifdef CONFIG_MEMCG\n\nvoid lru_gen_init_pgdat(struct pglist_data *pgdat)\n{\n\tint i, j;\n\n\tspin_lock_init(&pgdat->memcg_lru.lock);\n\n\tfor (i = 0; i < MEMCG_NR_GENS; i++) {\n\t\tfor (j = 0; j < MEMCG_NR_BINS; j++)\n\t\t\tINIT_HLIST_NULLS_HEAD(&pgdat->memcg_lru.fifo[i][j], i);\n\t}\n}\n\nvoid lru_gen_init_memcg(struct mem_cgroup *memcg)\n{\n\tINIT_LIST_HEAD(&memcg->mm_list.fifo);\n\tspin_lock_init(&memcg->mm_list.lock);\n}\n\nvoid lru_gen_exit_memcg(struct mem_cgroup *memcg)\n{\n\tint i;\n\tint nid;\n\n\tVM_WARN_ON_ONCE(!list_empty(&memcg->mm_list.fifo));\n\n\tfor_each_node(nid) {\n\t\tstruct lruvec *lruvec = get_lruvec(memcg, nid);\n\n\t\tVM_WARN_ON_ONCE(memchr_inv(lruvec->lrugen.nr_pages, 0,\n\t\t\t\t\t   sizeof(lruvec->lrugen.nr_pages)));\n\n\t\tlruvec->lrugen.list.next = LIST_POISON1;\n\n\t\tfor (i = 0; i < NR_BLOOM_FILTERS; i++) {\n\t\t\tbitmap_free(lruvec->mm_state.filters[i]);\n\t\t\tlruvec->mm_state.filters[i] = NULL;\n\t\t}\n\t}\n}\n\n#endif  \n\nstatic int __init init_lru_gen(void)\n{\n\tBUILD_BUG_ON(MIN_NR_GENS + 1 >= MAX_NR_GENS);\n\tBUILD_BUG_ON(BIT(LRU_GEN_WIDTH) <= MAX_NR_GENS);\n\n\tif (sysfs_create_group(mm_kobj, &lru_gen_attr_group))\n\t\tpr_err(\"lru_gen: failed to create sysfs group\\n\");\n\n\tdebugfs_create_file(\"lru_gen\", 0644, NULL, NULL, &lru_gen_rw_fops);\n\tdebugfs_create_file(\"lru_gen_full\", 0444, NULL, NULL, &lru_gen_ro_fops);\n\n\treturn 0;\n};\nlate_initcall(init_lru_gen);\n\n#else  \n\nstatic void lru_gen_age_node(struct pglist_data *pgdat, struct scan_control *sc)\n{\n}\n\nstatic void lru_gen_shrink_lruvec(struct lruvec *lruvec, struct scan_control *sc)\n{\n}\n\nstatic void lru_gen_shrink_node(struct pglist_data *pgdat, struct scan_control *sc)\n{\n}\n\n#endif  \n\nstatic void shrink_lruvec(struct lruvec *lruvec, struct scan_control *sc)\n{\n\tunsigned long nr[NR_LRU_LISTS];\n\tunsigned long targets[NR_LRU_LISTS];\n\tunsigned long nr_to_scan;\n\tenum lru_list lru;\n\tunsigned long nr_reclaimed = 0;\n\tunsigned long nr_to_reclaim = sc->nr_to_reclaim;\n\tbool proportional_reclaim;\n\tstruct blk_plug plug;\n\n\tif (lru_gen_enabled() && !root_reclaim(sc)) {\n\t\tlru_gen_shrink_lruvec(lruvec, sc);\n\t\treturn;\n\t}\n\n\tget_scan_count(lruvec, sc, nr);\n\n\t \n\tmemcpy(targets, nr, sizeof(nr));\n\n\t \n\tproportional_reclaim = (!cgroup_reclaim(sc) && !current_is_kswapd() &&\n\t\t\t\tsc->priority == DEF_PRIORITY);\n\n\tblk_start_plug(&plug);\n\twhile (nr[LRU_INACTIVE_ANON] || nr[LRU_ACTIVE_FILE] ||\n\t\t\t\t\tnr[LRU_INACTIVE_FILE]) {\n\t\tunsigned long nr_anon, nr_file, percentage;\n\t\tunsigned long nr_scanned;\n\n\t\tfor_each_evictable_lru(lru) {\n\t\t\tif (nr[lru]) {\n\t\t\t\tnr_to_scan = min(nr[lru], SWAP_CLUSTER_MAX);\n\t\t\t\tnr[lru] -= nr_to_scan;\n\n\t\t\t\tnr_reclaimed += shrink_list(lru, nr_to_scan,\n\t\t\t\t\t\t\t    lruvec, sc);\n\t\t\t}\n\t\t}\n\n\t\tcond_resched();\n\n\t\tif (nr_reclaimed < nr_to_reclaim || proportional_reclaim)\n\t\t\tcontinue;\n\n\t\t \n\t\tnr_file = nr[LRU_INACTIVE_FILE] + nr[LRU_ACTIVE_FILE];\n\t\tnr_anon = nr[LRU_INACTIVE_ANON] + nr[LRU_ACTIVE_ANON];\n\n\t\t \n\t\tif (!nr_file || !nr_anon)\n\t\t\tbreak;\n\n\t\tif (nr_file > nr_anon) {\n\t\t\tunsigned long scan_target = targets[LRU_INACTIVE_ANON] +\n\t\t\t\t\t\ttargets[LRU_ACTIVE_ANON] + 1;\n\t\t\tlru = LRU_BASE;\n\t\t\tpercentage = nr_anon * 100 / scan_target;\n\t\t} else {\n\t\t\tunsigned long scan_target = targets[LRU_INACTIVE_FILE] +\n\t\t\t\t\t\ttargets[LRU_ACTIVE_FILE] + 1;\n\t\t\tlru = LRU_FILE;\n\t\t\tpercentage = nr_file * 100 / scan_target;\n\t\t}\n\n\t\t \n\t\tnr[lru] = 0;\n\t\tnr[lru + LRU_ACTIVE] = 0;\n\n\t\t \n\t\tlru = (lru == LRU_FILE) ? LRU_BASE : LRU_FILE;\n\t\tnr_scanned = targets[lru] - nr[lru];\n\t\tnr[lru] = targets[lru] * (100 - percentage) / 100;\n\t\tnr[lru] -= min(nr[lru], nr_scanned);\n\n\t\tlru += LRU_ACTIVE;\n\t\tnr_scanned = targets[lru] - nr[lru];\n\t\tnr[lru] = targets[lru] * (100 - percentage) / 100;\n\t\tnr[lru] -= min(nr[lru], nr_scanned);\n\t}\n\tblk_finish_plug(&plug);\n\tsc->nr_reclaimed += nr_reclaimed;\n\n\t \n\tif (can_age_anon_pages(lruvec_pgdat(lruvec), sc) &&\n\t    inactive_is_low(lruvec, LRU_INACTIVE_ANON))\n\t\tshrink_active_list(SWAP_CLUSTER_MAX, lruvec,\n\t\t\t\t   sc, LRU_ACTIVE_ANON);\n}\n\n \nstatic bool in_reclaim_compaction(struct scan_control *sc)\n{\n\tif (IS_ENABLED(CONFIG_COMPACTION) && sc->order &&\n\t\t\t(sc->order > PAGE_ALLOC_COSTLY_ORDER ||\n\t\t\t sc->priority < DEF_PRIORITY - 2))\n\t\treturn true;\n\n\treturn false;\n}\n\n \nstatic inline bool should_continue_reclaim(struct pglist_data *pgdat,\n\t\t\t\t\tunsigned long nr_reclaimed,\n\t\t\t\t\tstruct scan_control *sc)\n{\n\tunsigned long pages_for_compaction;\n\tunsigned long inactive_lru_pages;\n\tint z;\n\n\t \n\tif (!in_reclaim_compaction(sc))\n\t\treturn false;\n\n\t \n\tif (!nr_reclaimed)\n\t\treturn false;\n\n\t \n\tfor (z = 0; z <= sc->reclaim_idx; z++) {\n\t\tstruct zone *zone = &pgdat->node_zones[z];\n\t\tif (!managed_zone(zone))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (zone_watermark_ok(zone, sc->order, min_wmark_pages(zone),\n\t\t\t\t      sc->reclaim_idx, 0))\n\t\t\treturn false;\n\n\t\tif (compaction_suitable(zone, sc->order, sc->reclaim_idx))\n\t\t\treturn false;\n\t}\n\n\t \n\tpages_for_compaction = compact_gap(sc->order);\n\tinactive_lru_pages = node_page_state(pgdat, NR_INACTIVE_FILE);\n\tif (can_reclaim_anon_pages(NULL, pgdat->node_id, sc))\n\t\tinactive_lru_pages += node_page_state(pgdat, NR_INACTIVE_ANON);\n\n\treturn inactive_lru_pages > pages_for_compaction;\n}\n\nstatic void shrink_node_memcgs(pg_data_t *pgdat, struct scan_control *sc)\n{\n\tstruct mem_cgroup *target_memcg = sc->target_mem_cgroup;\n\tstruct mem_cgroup *memcg;\n\n\tmemcg = mem_cgroup_iter(target_memcg, NULL, NULL);\n\tdo {\n\t\tstruct lruvec *lruvec = mem_cgroup_lruvec(memcg, pgdat);\n\t\tunsigned long reclaimed;\n\t\tunsigned long scanned;\n\n\t\t \n\t\tcond_resched();\n\n\t\tmem_cgroup_calculate_protection(target_memcg, memcg);\n\n\t\tif (mem_cgroup_below_min(target_memcg, memcg)) {\n\t\t\t \n\t\t\tcontinue;\n\t\t} else if (mem_cgroup_below_low(target_memcg, memcg)) {\n\t\t\t \n\t\t\tif (!sc->memcg_low_reclaim) {\n\t\t\t\tsc->memcg_low_skipped = 1;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tmemcg_memory_event(memcg, MEMCG_LOW);\n\t\t}\n\n\t\treclaimed = sc->nr_reclaimed;\n\t\tscanned = sc->nr_scanned;\n\n\t\tshrink_lruvec(lruvec, sc);\n\n\t\tshrink_slab(sc->gfp_mask, pgdat->node_id, memcg,\n\t\t\t    sc->priority);\n\n\t\t \n\t\tif (!sc->proactive)\n\t\t\tvmpressure(sc->gfp_mask, memcg, false,\n\t\t\t\t   sc->nr_scanned - scanned,\n\t\t\t\t   sc->nr_reclaimed - reclaimed);\n\n\t} while ((memcg = mem_cgroup_iter(target_memcg, memcg, NULL)));\n}\n\nstatic void shrink_node(pg_data_t *pgdat, struct scan_control *sc)\n{\n\tunsigned long nr_reclaimed, nr_scanned, nr_node_reclaimed;\n\tstruct lruvec *target_lruvec;\n\tbool reclaimable = false;\n\n\tif (lru_gen_enabled() && root_reclaim(sc)) {\n\t\tlru_gen_shrink_node(pgdat, sc);\n\t\treturn;\n\t}\n\n\ttarget_lruvec = mem_cgroup_lruvec(sc->target_mem_cgroup, pgdat);\n\nagain:\n\tmemset(&sc->nr, 0, sizeof(sc->nr));\n\n\tnr_reclaimed = sc->nr_reclaimed;\n\tnr_scanned = sc->nr_scanned;\n\n\tprepare_scan_count(pgdat, sc);\n\n\tshrink_node_memcgs(pgdat, sc);\n\n\tflush_reclaim_state(sc);\n\n\tnr_node_reclaimed = sc->nr_reclaimed - nr_reclaimed;\n\n\t \n\tif (!sc->proactive)\n\t\tvmpressure(sc->gfp_mask, sc->target_mem_cgroup, true,\n\t\t\t   sc->nr_scanned - nr_scanned, nr_node_reclaimed);\n\n\tif (nr_node_reclaimed)\n\t\treclaimable = true;\n\n\tif (current_is_kswapd()) {\n\t\t \n\t\tif (sc->nr.writeback && sc->nr.writeback == sc->nr.taken)\n\t\t\tset_bit(PGDAT_WRITEBACK, &pgdat->flags);\n\n\t\t \n\t\tif (sc->nr.unqueued_dirty == sc->nr.file_taken)\n\t\t\tset_bit(PGDAT_DIRTY, &pgdat->flags);\n\n\t\t \n\t\tif (sc->nr.immediate)\n\t\t\treclaim_throttle(pgdat, VMSCAN_THROTTLE_WRITEBACK);\n\t}\n\n\t \n\tif (sc->nr.dirty && sc->nr.dirty == sc->nr.congested) {\n\t\tif (cgroup_reclaim(sc) && writeback_throttling_sane(sc))\n\t\t\tset_bit(LRUVEC_CGROUP_CONGESTED, &target_lruvec->flags);\n\n\t\tif (current_is_kswapd())\n\t\t\tset_bit(LRUVEC_NODE_CONGESTED, &target_lruvec->flags);\n\t}\n\n\t \n\tif (!current_is_kswapd() && current_may_throttle() &&\n\t    !sc->hibernation_mode &&\n\t    (test_bit(LRUVEC_CGROUP_CONGESTED, &target_lruvec->flags) ||\n\t     test_bit(LRUVEC_NODE_CONGESTED, &target_lruvec->flags)))\n\t\treclaim_throttle(pgdat, VMSCAN_THROTTLE_CONGESTED);\n\n\tif (should_continue_reclaim(pgdat, nr_node_reclaimed, sc))\n\t\tgoto again;\n\n\t \n\tif (reclaimable)\n\t\tpgdat->kswapd_failures = 0;\n}\n\n \nstatic inline bool compaction_ready(struct zone *zone, struct scan_control *sc)\n{\n\tunsigned long watermark;\n\n\t \n\tif (zone_watermark_ok(zone, sc->order, min_wmark_pages(zone),\n\t\t\t      sc->reclaim_idx, 0))\n\t\treturn true;\n\n\t \n\tif (!compaction_suitable(zone, sc->order, sc->reclaim_idx))\n\t\treturn false;\n\n\t \n\twatermark = high_wmark_pages(zone) + compact_gap(sc->order);\n\n\treturn zone_watermark_ok_safe(zone, 0, watermark, sc->reclaim_idx);\n}\n\nstatic void consider_reclaim_throttle(pg_data_t *pgdat, struct scan_control *sc)\n{\n\t \n\tif (sc->nr_reclaimed > (sc->nr_scanned >> 3)) {\n\t\twait_queue_head_t *wqh;\n\n\t\twqh = &pgdat->reclaim_wait[VMSCAN_THROTTLE_NOPROGRESS];\n\t\tif (waitqueue_active(wqh))\n\t\t\twake_up(wqh);\n\n\t\treturn;\n\t}\n\n\t \n\tif (current_is_kswapd() || cgroup_reclaim(sc))\n\t\treturn;\n\n\t \n\tif (sc->priority == 1 && !sc->nr_reclaimed)\n\t\treclaim_throttle(pgdat, VMSCAN_THROTTLE_NOPROGRESS);\n}\n\n \nstatic void shrink_zones(struct zonelist *zonelist, struct scan_control *sc)\n{\n\tstruct zoneref *z;\n\tstruct zone *zone;\n\tunsigned long nr_soft_reclaimed;\n\tunsigned long nr_soft_scanned;\n\tgfp_t orig_mask;\n\tpg_data_t *last_pgdat = NULL;\n\tpg_data_t *first_pgdat = NULL;\n\n\t \n\torig_mask = sc->gfp_mask;\n\tif (buffer_heads_over_limit) {\n\t\tsc->gfp_mask |= __GFP_HIGHMEM;\n\t\tsc->reclaim_idx = gfp_zone(sc->gfp_mask);\n\t}\n\n\tfor_each_zone_zonelist_nodemask(zone, z, zonelist,\n\t\t\t\t\tsc->reclaim_idx, sc->nodemask) {\n\t\t \n\t\tif (!cgroup_reclaim(sc)) {\n\t\t\tif (!cpuset_zone_allowed(zone,\n\t\t\t\t\t\t GFP_KERNEL | __GFP_HARDWALL))\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tif (IS_ENABLED(CONFIG_COMPACTION) &&\n\t\t\t    sc->order > PAGE_ALLOC_COSTLY_ORDER &&\n\t\t\t    compaction_ready(zone, sc)) {\n\t\t\t\tsc->compaction_ready = true;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (zone->zone_pgdat == last_pgdat)\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tnr_soft_scanned = 0;\n\t\t\tnr_soft_reclaimed = mem_cgroup_soft_limit_reclaim(zone->zone_pgdat,\n\t\t\t\t\t\tsc->order, sc->gfp_mask,\n\t\t\t\t\t\t&nr_soft_scanned);\n\t\t\tsc->nr_reclaimed += nr_soft_reclaimed;\n\t\t\tsc->nr_scanned += nr_soft_scanned;\n\t\t\t \n\t\t}\n\n\t\tif (!first_pgdat)\n\t\t\tfirst_pgdat = zone->zone_pgdat;\n\n\t\t \n\t\tif (zone->zone_pgdat == last_pgdat)\n\t\t\tcontinue;\n\t\tlast_pgdat = zone->zone_pgdat;\n\t\tshrink_node(zone->zone_pgdat, sc);\n\t}\n\n\tif (first_pgdat)\n\t\tconsider_reclaim_throttle(first_pgdat, sc);\n\n\t \n\tsc->gfp_mask = orig_mask;\n}\n\nstatic void snapshot_refaults(struct mem_cgroup *target_memcg, pg_data_t *pgdat)\n{\n\tstruct lruvec *target_lruvec;\n\tunsigned long refaults;\n\n\tif (lru_gen_enabled())\n\t\treturn;\n\n\ttarget_lruvec = mem_cgroup_lruvec(target_memcg, pgdat);\n\trefaults = lruvec_page_state(target_lruvec, WORKINGSET_ACTIVATE_ANON);\n\ttarget_lruvec->refaults[WORKINGSET_ANON] = refaults;\n\trefaults = lruvec_page_state(target_lruvec, WORKINGSET_ACTIVATE_FILE);\n\ttarget_lruvec->refaults[WORKINGSET_FILE] = refaults;\n}\n\n \nstatic unsigned long do_try_to_free_pages(struct zonelist *zonelist,\n\t\t\t\t\t  struct scan_control *sc)\n{\n\tint initial_priority = sc->priority;\n\tpg_data_t *last_pgdat;\n\tstruct zoneref *z;\n\tstruct zone *zone;\nretry:\n\tdelayacct_freepages_start();\n\n\tif (!cgroup_reclaim(sc))\n\t\t__count_zid_vm_events(ALLOCSTALL, sc->reclaim_idx, 1);\n\n\tdo {\n\t\tif (!sc->proactive)\n\t\t\tvmpressure_prio(sc->gfp_mask, sc->target_mem_cgroup,\n\t\t\t\t\tsc->priority);\n\t\tsc->nr_scanned = 0;\n\t\tshrink_zones(zonelist, sc);\n\n\t\tif (sc->nr_reclaimed >= sc->nr_to_reclaim)\n\t\t\tbreak;\n\n\t\tif (sc->compaction_ready)\n\t\t\tbreak;\n\n\t\t \n\t\tif (sc->priority < DEF_PRIORITY - 2)\n\t\t\tsc->may_writepage = 1;\n\t} while (--sc->priority >= 0);\n\n\tlast_pgdat = NULL;\n\tfor_each_zone_zonelist_nodemask(zone, z, zonelist, sc->reclaim_idx,\n\t\t\t\t\tsc->nodemask) {\n\t\tif (zone->zone_pgdat == last_pgdat)\n\t\t\tcontinue;\n\t\tlast_pgdat = zone->zone_pgdat;\n\n\t\tsnapshot_refaults(sc->target_mem_cgroup, zone->zone_pgdat);\n\n\t\tif (cgroup_reclaim(sc)) {\n\t\t\tstruct lruvec *lruvec;\n\n\t\t\tlruvec = mem_cgroup_lruvec(sc->target_mem_cgroup,\n\t\t\t\t\t\t   zone->zone_pgdat);\n\t\t\tclear_bit(LRUVEC_CGROUP_CONGESTED, &lruvec->flags);\n\t\t}\n\t}\n\n\tdelayacct_freepages_end();\n\n\tif (sc->nr_reclaimed)\n\t\treturn sc->nr_reclaimed;\n\n\t \n\tif (sc->compaction_ready)\n\t\treturn 1;\n\n\t \n\tif (sc->skipped_deactivate) {\n\t\tsc->priority = initial_priority;\n\t\tsc->force_deactivate = 1;\n\t\tsc->skipped_deactivate = 0;\n\t\tgoto retry;\n\t}\n\n\t \n\tif (sc->memcg_low_skipped) {\n\t\tsc->priority = initial_priority;\n\t\tsc->force_deactivate = 0;\n\t\tsc->memcg_low_reclaim = 1;\n\t\tsc->memcg_low_skipped = 0;\n\t\tgoto retry;\n\t}\n\n\treturn 0;\n}\n\nstatic bool allow_direct_reclaim(pg_data_t *pgdat)\n{\n\tstruct zone *zone;\n\tunsigned long pfmemalloc_reserve = 0;\n\tunsigned long free_pages = 0;\n\tint i;\n\tbool wmark_ok;\n\n\tif (pgdat->kswapd_failures >= MAX_RECLAIM_RETRIES)\n\t\treturn true;\n\n\tfor (i = 0; i <= ZONE_NORMAL; i++) {\n\t\tzone = &pgdat->node_zones[i];\n\t\tif (!managed_zone(zone))\n\t\t\tcontinue;\n\n\t\tif (!zone_reclaimable_pages(zone))\n\t\t\tcontinue;\n\n\t\tpfmemalloc_reserve += min_wmark_pages(zone);\n\t\tfree_pages += zone_page_state_snapshot(zone, NR_FREE_PAGES);\n\t}\n\n\t \n\tif (!pfmemalloc_reserve)\n\t\treturn true;\n\n\twmark_ok = free_pages > pfmemalloc_reserve / 2;\n\n\t \n\tif (!wmark_ok && waitqueue_active(&pgdat->kswapd_wait)) {\n\t\tif (READ_ONCE(pgdat->kswapd_highest_zoneidx) > ZONE_NORMAL)\n\t\t\tWRITE_ONCE(pgdat->kswapd_highest_zoneidx, ZONE_NORMAL);\n\n\t\twake_up_interruptible(&pgdat->kswapd_wait);\n\t}\n\n\treturn wmark_ok;\n}\n\n \nstatic bool throttle_direct_reclaim(gfp_t gfp_mask, struct zonelist *zonelist,\n\t\t\t\t\tnodemask_t *nodemask)\n{\n\tstruct zoneref *z;\n\tstruct zone *zone;\n\tpg_data_t *pgdat = NULL;\n\n\t \n\tif (current->flags & PF_KTHREAD)\n\t\tgoto out;\n\n\t \n\tif (fatal_signal_pending(current))\n\t\tgoto out;\n\n\t \n\tfor_each_zone_zonelist_nodemask(zone, z, zonelist,\n\t\t\t\t\tgfp_zone(gfp_mask), nodemask) {\n\t\tif (zone_idx(zone) > ZONE_NORMAL)\n\t\t\tcontinue;\n\n\t\t \n\t\tpgdat = zone->zone_pgdat;\n\t\tif (allow_direct_reclaim(pgdat))\n\t\t\tgoto out;\n\t\tbreak;\n\t}\n\n\t \n\tif (!pgdat)\n\t\tgoto out;\n\n\t \n\tcount_vm_event(PGSCAN_DIRECT_THROTTLE);\n\n\t \n\tif (!(gfp_mask & __GFP_FS))\n\t\twait_event_interruptible_timeout(pgdat->pfmemalloc_wait,\n\t\t\tallow_direct_reclaim(pgdat), HZ);\n\telse\n\t\t \n\t\twait_event_killable(zone->zone_pgdat->pfmemalloc_wait,\n\t\t\tallow_direct_reclaim(pgdat));\n\n\tif (fatal_signal_pending(current))\n\t\treturn true;\n\nout:\n\treturn false;\n}\n\nunsigned long try_to_free_pages(struct zonelist *zonelist, int order,\n\t\t\t\tgfp_t gfp_mask, nodemask_t *nodemask)\n{\n\tunsigned long nr_reclaimed;\n\tstruct scan_control sc = {\n\t\t.nr_to_reclaim = SWAP_CLUSTER_MAX,\n\t\t.gfp_mask = current_gfp_context(gfp_mask),\n\t\t.reclaim_idx = gfp_zone(gfp_mask),\n\t\t.order = order,\n\t\t.nodemask = nodemask,\n\t\t.priority = DEF_PRIORITY,\n\t\t.may_writepage = !laptop_mode,\n\t\t.may_unmap = 1,\n\t\t.may_swap = 1,\n\t};\n\n\t \n\tBUILD_BUG_ON(MAX_ORDER >= S8_MAX);\n\tBUILD_BUG_ON(DEF_PRIORITY > S8_MAX);\n\tBUILD_BUG_ON(MAX_NR_ZONES > S8_MAX);\n\n\t \n\tif (throttle_direct_reclaim(sc.gfp_mask, zonelist, nodemask))\n\t\treturn 1;\n\n\tset_task_reclaim_state(current, &sc.reclaim_state);\n\ttrace_mm_vmscan_direct_reclaim_begin(order, sc.gfp_mask);\n\n\tnr_reclaimed = do_try_to_free_pages(zonelist, &sc);\n\n\ttrace_mm_vmscan_direct_reclaim_end(nr_reclaimed);\n\tset_task_reclaim_state(current, NULL);\n\n\treturn nr_reclaimed;\n}\n\n#ifdef CONFIG_MEMCG\n\n \nunsigned long mem_cgroup_shrink_node(struct mem_cgroup *memcg,\n\t\t\t\t\t\tgfp_t gfp_mask, bool noswap,\n\t\t\t\t\t\tpg_data_t *pgdat,\n\t\t\t\t\t\tunsigned long *nr_scanned)\n{\n\tstruct lruvec *lruvec = mem_cgroup_lruvec(memcg, pgdat);\n\tstruct scan_control sc = {\n\t\t.nr_to_reclaim = SWAP_CLUSTER_MAX,\n\t\t.target_mem_cgroup = memcg,\n\t\t.may_writepage = !laptop_mode,\n\t\t.may_unmap = 1,\n\t\t.reclaim_idx = MAX_NR_ZONES - 1,\n\t\t.may_swap = !noswap,\n\t};\n\n\tWARN_ON_ONCE(!current->reclaim_state);\n\n\tsc.gfp_mask = (gfp_mask & GFP_RECLAIM_MASK) |\n\t\t\t(GFP_HIGHUSER_MOVABLE & ~GFP_RECLAIM_MASK);\n\n\ttrace_mm_vmscan_memcg_softlimit_reclaim_begin(sc.order,\n\t\t\t\t\t\t      sc.gfp_mask);\n\n\t \n\tshrink_lruvec(lruvec, &sc);\n\n\ttrace_mm_vmscan_memcg_softlimit_reclaim_end(sc.nr_reclaimed);\n\n\t*nr_scanned = sc.nr_scanned;\n\n\treturn sc.nr_reclaimed;\n}\n\nunsigned long try_to_free_mem_cgroup_pages(struct mem_cgroup *memcg,\n\t\t\t\t\t   unsigned long nr_pages,\n\t\t\t\t\t   gfp_t gfp_mask,\n\t\t\t\t\t   unsigned int reclaim_options)\n{\n\tunsigned long nr_reclaimed;\n\tunsigned int noreclaim_flag;\n\tstruct scan_control sc = {\n\t\t.nr_to_reclaim = max(nr_pages, SWAP_CLUSTER_MAX),\n\t\t.gfp_mask = (current_gfp_context(gfp_mask) & GFP_RECLAIM_MASK) |\n\t\t\t\t(GFP_HIGHUSER_MOVABLE & ~GFP_RECLAIM_MASK),\n\t\t.reclaim_idx = MAX_NR_ZONES - 1,\n\t\t.target_mem_cgroup = memcg,\n\t\t.priority = DEF_PRIORITY,\n\t\t.may_writepage = !laptop_mode,\n\t\t.may_unmap = 1,\n\t\t.may_swap = !!(reclaim_options & MEMCG_RECLAIM_MAY_SWAP),\n\t\t.proactive = !!(reclaim_options & MEMCG_RECLAIM_PROACTIVE),\n\t};\n\t \n\tstruct zonelist *zonelist = node_zonelist(numa_node_id(), sc.gfp_mask);\n\n\tset_task_reclaim_state(current, &sc.reclaim_state);\n\ttrace_mm_vmscan_memcg_reclaim_begin(0, sc.gfp_mask);\n\tnoreclaim_flag = memalloc_noreclaim_save();\n\n\tnr_reclaimed = do_try_to_free_pages(zonelist, &sc);\n\n\tmemalloc_noreclaim_restore(noreclaim_flag);\n\ttrace_mm_vmscan_memcg_reclaim_end(nr_reclaimed);\n\tset_task_reclaim_state(current, NULL);\n\n\treturn nr_reclaimed;\n}\n#endif\n\nstatic void kswapd_age_node(struct pglist_data *pgdat, struct scan_control *sc)\n{\n\tstruct mem_cgroup *memcg;\n\tstruct lruvec *lruvec;\n\n\tif (lru_gen_enabled()) {\n\t\tlru_gen_age_node(pgdat, sc);\n\t\treturn;\n\t}\n\n\tif (!can_age_anon_pages(pgdat, sc))\n\t\treturn;\n\n\tlruvec = mem_cgroup_lruvec(NULL, pgdat);\n\tif (!inactive_is_low(lruvec, LRU_INACTIVE_ANON))\n\t\treturn;\n\n\tmemcg = mem_cgroup_iter(NULL, NULL, NULL);\n\tdo {\n\t\tlruvec = mem_cgroup_lruvec(memcg, pgdat);\n\t\tshrink_active_list(SWAP_CLUSTER_MAX, lruvec,\n\t\t\t\t   sc, LRU_ACTIVE_ANON);\n\t\tmemcg = mem_cgroup_iter(NULL, memcg, NULL);\n\t} while (memcg);\n}\n\nstatic bool pgdat_watermark_boosted(pg_data_t *pgdat, int highest_zoneidx)\n{\n\tint i;\n\tstruct zone *zone;\n\n\t \n\tfor (i = highest_zoneidx; i >= 0; i--) {\n\t\tzone = pgdat->node_zones + i;\n\t\tif (!managed_zone(zone))\n\t\t\tcontinue;\n\n\t\tif (zone->watermark_boost)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nstatic bool pgdat_balanced(pg_data_t *pgdat, int order, int highest_zoneidx)\n{\n\tint i;\n\tunsigned long mark = -1;\n\tstruct zone *zone;\n\n\t \n\tfor (i = 0; i <= highest_zoneidx; i++) {\n\t\tzone = pgdat->node_zones + i;\n\n\t\tif (!managed_zone(zone))\n\t\t\tcontinue;\n\n\t\tif (sysctl_numa_balancing_mode & NUMA_BALANCING_MEMORY_TIERING)\n\t\t\tmark = wmark_pages(zone, WMARK_PROMO);\n\t\telse\n\t\t\tmark = high_wmark_pages(zone);\n\t\tif (zone_watermark_ok_safe(zone, order, mark, highest_zoneidx))\n\t\t\treturn true;\n\t}\n\n\t \n\tif (mark == -1)\n\t\treturn true;\n\n\treturn false;\n}\n\n \nstatic void clear_pgdat_congested(pg_data_t *pgdat)\n{\n\tstruct lruvec *lruvec = mem_cgroup_lruvec(NULL, pgdat);\n\n\tclear_bit(LRUVEC_NODE_CONGESTED, &lruvec->flags);\n\tclear_bit(LRUVEC_CGROUP_CONGESTED, &lruvec->flags);\n\tclear_bit(PGDAT_DIRTY, &pgdat->flags);\n\tclear_bit(PGDAT_WRITEBACK, &pgdat->flags);\n}\n\n \nstatic bool prepare_kswapd_sleep(pg_data_t *pgdat, int order,\n\t\t\t\tint highest_zoneidx)\n{\n\t \n\tif (waitqueue_active(&pgdat->pfmemalloc_wait))\n\t\twake_up_all(&pgdat->pfmemalloc_wait);\n\n\t \n\tif (pgdat->kswapd_failures >= MAX_RECLAIM_RETRIES)\n\t\treturn true;\n\n\tif (pgdat_balanced(pgdat, order, highest_zoneidx)) {\n\t\tclear_pgdat_congested(pgdat);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nstatic bool kswapd_shrink_node(pg_data_t *pgdat,\n\t\t\t       struct scan_control *sc)\n{\n\tstruct zone *zone;\n\tint z;\n\n\t \n\tsc->nr_to_reclaim = 0;\n\tfor (z = 0; z <= sc->reclaim_idx; z++) {\n\t\tzone = pgdat->node_zones + z;\n\t\tif (!managed_zone(zone))\n\t\t\tcontinue;\n\n\t\tsc->nr_to_reclaim += max(high_wmark_pages(zone), SWAP_CLUSTER_MAX);\n\t}\n\n\t \n\tshrink_node(pgdat, sc);\n\n\t \n\tif (sc->order && sc->nr_reclaimed >= compact_gap(sc->order))\n\t\tsc->order = 0;\n\n\treturn sc->nr_scanned >= sc->nr_to_reclaim;\n}\n\n \nstatic inline void\nupdate_reclaim_active(pg_data_t *pgdat, int highest_zoneidx, bool active)\n{\n\tint i;\n\tstruct zone *zone;\n\n\tfor (i = 0; i <= highest_zoneidx; i++) {\n\t\tzone = pgdat->node_zones + i;\n\n\t\tif (!managed_zone(zone))\n\t\t\tcontinue;\n\n\t\tif (active)\n\t\t\tset_bit(ZONE_RECLAIM_ACTIVE, &zone->flags);\n\t\telse\n\t\t\tclear_bit(ZONE_RECLAIM_ACTIVE, &zone->flags);\n\t}\n}\n\nstatic inline void\nset_reclaim_active(pg_data_t *pgdat, int highest_zoneidx)\n{\n\tupdate_reclaim_active(pgdat, highest_zoneidx, true);\n}\n\nstatic inline void\nclear_reclaim_active(pg_data_t *pgdat, int highest_zoneidx)\n{\n\tupdate_reclaim_active(pgdat, highest_zoneidx, false);\n}\n\n \nstatic int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n{\n\tint i;\n\tunsigned long nr_soft_reclaimed;\n\tunsigned long nr_soft_scanned;\n\tunsigned long pflags;\n\tunsigned long nr_boost_reclaim;\n\tunsigned long zone_boosts[MAX_NR_ZONES] = { 0, };\n\tbool boosted;\n\tstruct zone *zone;\n\tstruct scan_control sc = {\n\t\t.gfp_mask = GFP_KERNEL,\n\t\t.order = order,\n\t\t.may_unmap = 1,\n\t};\n\n\tset_task_reclaim_state(current, &sc.reclaim_state);\n\tpsi_memstall_enter(&pflags);\n\t__fs_reclaim_acquire(_THIS_IP_);\n\n\tcount_vm_event(PAGEOUTRUN);\n\n\t \n\tnr_boost_reclaim = 0;\n\tfor (i = 0; i <= highest_zoneidx; i++) {\n\t\tzone = pgdat->node_zones + i;\n\t\tif (!managed_zone(zone))\n\t\t\tcontinue;\n\n\t\tnr_boost_reclaim += zone->watermark_boost;\n\t\tzone_boosts[i] = zone->watermark_boost;\n\t}\n\tboosted = nr_boost_reclaim;\n\nrestart:\n\tset_reclaim_active(pgdat, highest_zoneidx);\n\tsc.priority = DEF_PRIORITY;\n\tdo {\n\t\tunsigned long nr_reclaimed = sc.nr_reclaimed;\n\t\tbool raise_priority = true;\n\t\tbool balanced;\n\t\tbool ret;\n\n\t\tsc.reclaim_idx = highest_zoneidx;\n\n\t\t \n\t\tif (buffer_heads_over_limit) {\n\t\t\tfor (i = MAX_NR_ZONES - 1; i >= 0; i--) {\n\t\t\t\tzone = pgdat->node_zones + i;\n\t\t\t\tif (!managed_zone(zone))\n\t\t\t\t\tcontinue;\n\n\t\t\t\tsc.reclaim_idx = i;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tbalanced = pgdat_balanced(pgdat, sc.order, highest_zoneidx);\n\t\tif (!balanced && nr_boost_reclaim) {\n\t\t\tnr_boost_reclaim = 0;\n\t\t\tgoto restart;\n\t\t}\n\n\t\t \n\t\tif (!nr_boost_reclaim && balanced)\n\t\t\tgoto out;\n\n\t\t \n\t\tif (nr_boost_reclaim && sc.priority == DEF_PRIORITY - 2)\n\t\t\traise_priority = false;\n\n\t\t \n\t\tsc.may_writepage = !laptop_mode && !nr_boost_reclaim;\n\t\tsc.may_swap = !nr_boost_reclaim;\n\n\t\t \n\t\tkswapd_age_node(pgdat, &sc);\n\n\t\t \n\t\tif (sc.priority < DEF_PRIORITY - 2)\n\t\t\tsc.may_writepage = 1;\n\n\t\t \n\t\tsc.nr_scanned = 0;\n\t\tnr_soft_scanned = 0;\n\t\tnr_soft_reclaimed = mem_cgroup_soft_limit_reclaim(pgdat, sc.order,\n\t\t\t\t\t\tsc.gfp_mask, &nr_soft_scanned);\n\t\tsc.nr_reclaimed += nr_soft_reclaimed;\n\n\t\t \n\t\tif (kswapd_shrink_node(pgdat, &sc))\n\t\t\traise_priority = false;\n\n\t\t \n\t\tif (waitqueue_active(&pgdat->pfmemalloc_wait) &&\n\t\t\t\tallow_direct_reclaim(pgdat))\n\t\t\twake_up_all(&pgdat->pfmemalloc_wait);\n\n\t\t \n\t\t__fs_reclaim_release(_THIS_IP_);\n\t\tret = try_to_freeze();\n\t\t__fs_reclaim_acquire(_THIS_IP_);\n\t\tif (ret || kthread_should_stop())\n\t\t\tbreak;\n\n\t\t \n\t\tnr_reclaimed = sc.nr_reclaimed - nr_reclaimed;\n\t\tnr_boost_reclaim -= min(nr_boost_reclaim, nr_reclaimed);\n\n\t\t \n\t\tif (nr_boost_reclaim && !nr_reclaimed)\n\t\t\tbreak;\n\n\t\tif (raise_priority || !nr_reclaimed)\n\t\t\tsc.priority--;\n\t} while (sc.priority >= 1);\n\n\tif (!sc.nr_reclaimed)\n\t\tpgdat->kswapd_failures++;\n\nout:\n\tclear_reclaim_active(pgdat, highest_zoneidx);\n\n\t \n\tif (boosted) {\n\t\tunsigned long flags;\n\n\t\tfor (i = 0; i <= highest_zoneidx; i++) {\n\t\t\tif (!zone_boosts[i])\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tzone = pgdat->node_zones + i;\n\t\t\tspin_lock_irqsave(&zone->lock, flags);\n\t\t\tzone->watermark_boost -= min(zone->watermark_boost, zone_boosts[i]);\n\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n\t\t}\n\n\t\t \n\t\twakeup_kcompactd(pgdat, pageblock_order, highest_zoneidx);\n\t}\n\n\tsnapshot_refaults(NULL, pgdat);\n\t__fs_reclaim_release(_THIS_IP_);\n\tpsi_memstall_leave(&pflags);\n\tset_task_reclaim_state(current, NULL);\n\n\t \n\treturn sc.order;\n}\n\n \nstatic enum zone_type kswapd_highest_zoneidx(pg_data_t *pgdat,\n\t\t\t\t\t   enum zone_type prev_highest_zoneidx)\n{\n\tenum zone_type curr_idx = READ_ONCE(pgdat->kswapd_highest_zoneidx);\n\n\treturn curr_idx == MAX_NR_ZONES ? prev_highest_zoneidx : curr_idx;\n}\n\nstatic void kswapd_try_to_sleep(pg_data_t *pgdat, int alloc_order, int reclaim_order,\n\t\t\t\tunsigned int highest_zoneidx)\n{\n\tlong remaining = 0;\n\tDEFINE_WAIT(wait);\n\n\tif (freezing(current) || kthread_should_stop())\n\t\treturn;\n\n\tprepare_to_wait(&pgdat->kswapd_wait, &wait, TASK_INTERRUPTIBLE);\n\n\t \n\tif (prepare_kswapd_sleep(pgdat, reclaim_order, highest_zoneidx)) {\n\t\t \n\t\treset_isolation_suitable(pgdat);\n\n\t\t \n\t\twakeup_kcompactd(pgdat, alloc_order, highest_zoneidx);\n\n\t\tremaining = schedule_timeout(HZ/10);\n\n\t\t \n\t\tif (remaining) {\n\t\t\tWRITE_ONCE(pgdat->kswapd_highest_zoneidx,\n\t\t\t\t\tkswapd_highest_zoneidx(pgdat,\n\t\t\t\t\t\t\thighest_zoneidx));\n\n\t\t\tif (READ_ONCE(pgdat->kswapd_order) < reclaim_order)\n\t\t\t\tWRITE_ONCE(pgdat->kswapd_order, reclaim_order);\n\t\t}\n\n\t\tfinish_wait(&pgdat->kswapd_wait, &wait);\n\t\tprepare_to_wait(&pgdat->kswapd_wait, &wait, TASK_INTERRUPTIBLE);\n\t}\n\n\t \n\tif (!remaining &&\n\t    prepare_kswapd_sleep(pgdat, reclaim_order, highest_zoneidx)) {\n\t\ttrace_mm_vmscan_kswapd_sleep(pgdat->node_id);\n\n\t\t \n\t\tset_pgdat_percpu_threshold(pgdat, calculate_normal_threshold);\n\n\t\tif (!kthread_should_stop())\n\t\t\tschedule();\n\n\t\tset_pgdat_percpu_threshold(pgdat, calculate_pressure_threshold);\n\t} else {\n\t\tif (remaining)\n\t\t\tcount_vm_event(KSWAPD_LOW_WMARK_HIT_QUICKLY);\n\t\telse\n\t\t\tcount_vm_event(KSWAPD_HIGH_WMARK_HIT_QUICKLY);\n\t}\n\tfinish_wait(&pgdat->kswapd_wait, &wait);\n}\n\n \nstatic int kswapd(void *p)\n{\n\tunsigned int alloc_order, reclaim_order;\n\tunsigned int highest_zoneidx = MAX_NR_ZONES - 1;\n\tpg_data_t *pgdat = (pg_data_t *)p;\n\tstruct task_struct *tsk = current;\n\tconst struct cpumask *cpumask = cpumask_of_node(pgdat->node_id);\n\n\tif (!cpumask_empty(cpumask))\n\t\tset_cpus_allowed_ptr(tsk, cpumask);\n\n\t \n\ttsk->flags |= PF_MEMALLOC | PF_KSWAPD;\n\tset_freezable();\n\n\tWRITE_ONCE(pgdat->kswapd_order, 0);\n\tWRITE_ONCE(pgdat->kswapd_highest_zoneidx, MAX_NR_ZONES);\n\tatomic_set(&pgdat->nr_writeback_throttled, 0);\n\tfor ( ; ; ) {\n\t\tbool ret;\n\n\t\talloc_order = reclaim_order = READ_ONCE(pgdat->kswapd_order);\n\t\thighest_zoneidx = kswapd_highest_zoneidx(pgdat,\n\t\t\t\t\t\t\thighest_zoneidx);\n\nkswapd_try_sleep:\n\t\tkswapd_try_to_sleep(pgdat, alloc_order, reclaim_order,\n\t\t\t\t\thighest_zoneidx);\n\n\t\t \n\t\talloc_order = READ_ONCE(pgdat->kswapd_order);\n\t\thighest_zoneidx = kswapd_highest_zoneidx(pgdat,\n\t\t\t\t\t\t\thighest_zoneidx);\n\t\tWRITE_ONCE(pgdat->kswapd_order, 0);\n\t\tWRITE_ONCE(pgdat->kswapd_highest_zoneidx, MAX_NR_ZONES);\n\n\t\tret = try_to_freeze();\n\t\tif (kthread_should_stop())\n\t\t\tbreak;\n\n\t\t \n\t\tif (ret)\n\t\t\tcontinue;\n\n\t\t \n\t\ttrace_mm_vmscan_kswapd_wake(pgdat->node_id, highest_zoneidx,\n\t\t\t\t\t\talloc_order);\n\t\treclaim_order = balance_pgdat(pgdat, alloc_order,\n\t\t\t\t\t\thighest_zoneidx);\n\t\tif (reclaim_order < alloc_order)\n\t\t\tgoto kswapd_try_sleep;\n\t}\n\n\ttsk->flags &= ~(PF_MEMALLOC | PF_KSWAPD);\n\n\treturn 0;\n}\n\n \nvoid wakeup_kswapd(struct zone *zone, gfp_t gfp_flags, int order,\n\t\t   enum zone_type highest_zoneidx)\n{\n\tpg_data_t *pgdat;\n\tenum zone_type curr_idx;\n\n\tif (!managed_zone(zone))\n\t\treturn;\n\n\tif (!cpuset_zone_allowed(zone, gfp_flags))\n\t\treturn;\n\n\tpgdat = zone->zone_pgdat;\n\tcurr_idx = READ_ONCE(pgdat->kswapd_highest_zoneidx);\n\n\tif (curr_idx == MAX_NR_ZONES || curr_idx < highest_zoneidx)\n\t\tWRITE_ONCE(pgdat->kswapd_highest_zoneidx, highest_zoneidx);\n\n\tif (READ_ONCE(pgdat->kswapd_order) < order)\n\t\tWRITE_ONCE(pgdat->kswapd_order, order);\n\n\tif (!waitqueue_active(&pgdat->kswapd_wait))\n\t\treturn;\n\n\t \n\tif (pgdat->kswapd_failures >= MAX_RECLAIM_RETRIES ||\n\t    (pgdat_balanced(pgdat, order, highest_zoneidx) &&\n\t     !pgdat_watermark_boosted(pgdat, highest_zoneidx))) {\n\t\t \n\t\tif (!(gfp_flags & __GFP_DIRECT_RECLAIM))\n\t\t\twakeup_kcompactd(pgdat, order, highest_zoneidx);\n\t\treturn;\n\t}\n\n\ttrace_mm_vmscan_wakeup_kswapd(pgdat->node_id, highest_zoneidx, order,\n\t\t\t\t      gfp_flags);\n\twake_up_interruptible(&pgdat->kswapd_wait);\n}\n\n#ifdef CONFIG_HIBERNATION\n \nunsigned long shrink_all_memory(unsigned long nr_to_reclaim)\n{\n\tstruct scan_control sc = {\n\t\t.nr_to_reclaim = nr_to_reclaim,\n\t\t.gfp_mask = GFP_HIGHUSER_MOVABLE,\n\t\t.reclaim_idx = MAX_NR_ZONES - 1,\n\t\t.priority = DEF_PRIORITY,\n\t\t.may_writepage = 1,\n\t\t.may_unmap = 1,\n\t\t.may_swap = 1,\n\t\t.hibernation_mode = 1,\n\t};\n\tstruct zonelist *zonelist = node_zonelist(numa_node_id(), sc.gfp_mask);\n\tunsigned long nr_reclaimed;\n\tunsigned int noreclaim_flag;\n\n\tfs_reclaim_acquire(sc.gfp_mask);\n\tnoreclaim_flag = memalloc_noreclaim_save();\n\tset_task_reclaim_state(current, &sc.reclaim_state);\n\n\tnr_reclaimed = do_try_to_free_pages(zonelist, &sc);\n\n\tset_task_reclaim_state(current, NULL);\n\tmemalloc_noreclaim_restore(noreclaim_flag);\n\tfs_reclaim_release(sc.gfp_mask);\n\n\treturn nr_reclaimed;\n}\n#endif  \n\n \nvoid __meminit kswapd_run(int nid)\n{\n\tpg_data_t *pgdat = NODE_DATA(nid);\n\n\tpgdat_kswapd_lock(pgdat);\n\tif (!pgdat->kswapd) {\n\t\tpgdat->kswapd = kthread_run(kswapd, pgdat, \"kswapd%d\", nid);\n\t\tif (IS_ERR(pgdat->kswapd)) {\n\t\t\t \n\t\t\tBUG_ON(system_state < SYSTEM_RUNNING);\n\t\t\tpr_err(\"Failed to start kswapd on node %d\\n\", nid);\n\t\t\tpgdat->kswapd = NULL;\n\t\t}\n\t}\n\tpgdat_kswapd_unlock(pgdat);\n}\n\n \nvoid __meminit kswapd_stop(int nid)\n{\n\tpg_data_t *pgdat = NODE_DATA(nid);\n\tstruct task_struct *kswapd;\n\n\tpgdat_kswapd_lock(pgdat);\n\tkswapd = pgdat->kswapd;\n\tif (kswapd) {\n\t\tkthread_stop(kswapd);\n\t\tpgdat->kswapd = NULL;\n\t}\n\tpgdat_kswapd_unlock(pgdat);\n}\n\nstatic int __init kswapd_init(void)\n{\n\tint nid;\n\n\tswap_setup();\n\tfor_each_node_state(nid, N_MEMORY)\n \t\tkswapd_run(nid);\n\treturn 0;\n}\n\nmodule_init(kswapd_init)\n\n#ifdef CONFIG_NUMA\n \nint node_reclaim_mode __read_mostly;\n\n \n#define NODE_RECLAIM_PRIORITY 4\n\n \nint sysctl_min_unmapped_ratio = 1;\n\n \nint sysctl_min_slab_ratio = 5;\n\nstatic inline unsigned long node_unmapped_file_pages(struct pglist_data *pgdat)\n{\n\tunsigned long file_mapped = node_page_state(pgdat, NR_FILE_MAPPED);\n\tunsigned long file_lru = node_page_state(pgdat, NR_INACTIVE_FILE) +\n\t\tnode_page_state(pgdat, NR_ACTIVE_FILE);\n\n\t \n\treturn (file_lru > file_mapped) ? (file_lru - file_mapped) : 0;\n}\n\n \nstatic unsigned long node_pagecache_reclaimable(struct pglist_data *pgdat)\n{\n\tunsigned long nr_pagecache_reclaimable;\n\tunsigned long delta = 0;\n\n\t \n\tif (node_reclaim_mode & RECLAIM_UNMAP)\n\t\tnr_pagecache_reclaimable = node_page_state(pgdat, NR_FILE_PAGES);\n\telse\n\t\tnr_pagecache_reclaimable = node_unmapped_file_pages(pgdat);\n\n\t \n\tif (!(node_reclaim_mode & RECLAIM_WRITE))\n\t\tdelta += node_page_state(pgdat, NR_FILE_DIRTY);\n\n\t \n\tif (unlikely(delta > nr_pagecache_reclaimable))\n\t\tdelta = nr_pagecache_reclaimable;\n\n\treturn nr_pagecache_reclaimable - delta;\n}\n\n \nstatic int __node_reclaim(struct pglist_data *pgdat, gfp_t gfp_mask, unsigned int order)\n{\n\t \n\tconst unsigned long nr_pages = 1 << order;\n\tstruct task_struct *p = current;\n\tunsigned int noreclaim_flag;\n\tstruct scan_control sc = {\n\t\t.nr_to_reclaim = max(nr_pages, SWAP_CLUSTER_MAX),\n\t\t.gfp_mask = current_gfp_context(gfp_mask),\n\t\t.order = order,\n\t\t.priority = NODE_RECLAIM_PRIORITY,\n\t\t.may_writepage = !!(node_reclaim_mode & RECLAIM_WRITE),\n\t\t.may_unmap = !!(node_reclaim_mode & RECLAIM_UNMAP),\n\t\t.may_swap = 1,\n\t\t.reclaim_idx = gfp_zone(gfp_mask),\n\t};\n\tunsigned long pflags;\n\n\ttrace_mm_vmscan_node_reclaim_begin(pgdat->node_id, order,\n\t\t\t\t\t   sc.gfp_mask);\n\n\tcond_resched();\n\tpsi_memstall_enter(&pflags);\n\tfs_reclaim_acquire(sc.gfp_mask);\n\t \n\tnoreclaim_flag = memalloc_noreclaim_save();\n\tset_task_reclaim_state(p, &sc.reclaim_state);\n\n\tif (node_pagecache_reclaimable(pgdat) > pgdat->min_unmapped_pages ||\n\t    node_page_state_pages(pgdat, NR_SLAB_RECLAIMABLE_B) > pgdat->min_slab_pages) {\n\t\t \n\t\tdo {\n\t\t\tshrink_node(pgdat, &sc);\n\t\t} while (sc.nr_reclaimed < nr_pages && --sc.priority >= 0);\n\t}\n\n\tset_task_reclaim_state(p, NULL);\n\tmemalloc_noreclaim_restore(noreclaim_flag);\n\tfs_reclaim_release(sc.gfp_mask);\n\tpsi_memstall_leave(&pflags);\n\n\ttrace_mm_vmscan_node_reclaim_end(sc.nr_reclaimed);\n\n\treturn sc.nr_reclaimed >= nr_pages;\n}\n\nint node_reclaim(struct pglist_data *pgdat, gfp_t gfp_mask, unsigned int order)\n{\n\tint ret;\n\n\t \n\tif (node_pagecache_reclaimable(pgdat) <= pgdat->min_unmapped_pages &&\n\t    node_page_state_pages(pgdat, NR_SLAB_RECLAIMABLE_B) <=\n\t    pgdat->min_slab_pages)\n\t\treturn NODE_RECLAIM_FULL;\n\n\t \n\tif (!gfpflags_allow_blocking(gfp_mask) || (current->flags & PF_MEMALLOC))\n\t\treturn NODE_RECLAIM_NOSCAN;\n\n\t \n\tif (node_state(pgdat->node_id, N_CPU) && pgdat->node_id != numa_node_id())\n\t\treturn NODE_RECLAIM_NOSCAN;\n\n\tif (test_and_set_bit(PGDAT_RECLAIM_LOCKED, &pgdat->flags))\n\t\treturn NODE_RECLAIM_NOSCAN;\n\n\tret = __node_reclaim(pgdat, gfp_mask, order);\n\tclear_bit(PGDAT_RECLAIM_LOCKED, &pgdat->flags);\n\n\tif (!ret)\n\t\tcount_vm_event(PGSCAN_ZONE_RECLAIM_FAILED);\n\n\treturn ret;\n}\n#endif\n\n \nvoid check_move_unevictable_folios(struct folio_batch *fbatch)\n{\n\tstruct lruvec *lruvec = NULL;\n\tint pgscanned = 0;\n\tint pgrescued = 0;\n\tint i;\n\n\tfor (i = 0; i < fbatch->nr; i++) {\n\t\tstruct folio *folio = fbatch->folios[i];\n\t\tint nr_pages = folio_nr_pages(folio);\n\n\t\tpgscanned += nr_pages;\n\n\t\t \n\t\tif (!folio_test_clear_lru(folio))\n\t\t\tcontinue;\n\n\t\tlruvec = folio_lruvec_relock_irq(folio, lruvec);\n\t\tif (folio_evictable(folio) && folio_test_unevictable(folio)) {\n\t\t\tlruvec_del_folio(lruvec, folio);\n\t\t\tfolio_clear_unevictable(folio);\n\t\t\tlruvec_add_folio(lruvec, folio);\n\t\t\tpgrescued += nr_pages;\n\t\t}\n\t\tfolio_set_lru(folio);\n\t}\n\n\tif (lruvec) {\n\t\t__count_vm_events(UNEVICTABLE_PGRESCUED, pgrescued);\n\t\t__count_vm_events(UNEVICTABLE_PGSCANNED, pgscanned);\n\t\tunlock_page_lruvec_irq(lruvec);\n\t} else if (pgscanned) {\n\t\tcount_vm_events(UNEVICTABLE_PGSCANNED, pgscanned);\n\t}\n}\nEXPORT_SYMBOL_GPL(check_move_unevictable_folios);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}