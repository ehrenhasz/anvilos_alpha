{
  "module_name": "slab_common.c",
  "hash_id": "3e12524a4d49e6c1f583a5a851dd6822d22a45986caf018ce0e1768f6fa3bbda",
  "original_prompt": "Ingested from linux-6.6.14/mm/slab_common.c",
  "human_readable_source": "\n \n#include <linux/slab.h>\n\n#include <linux/mm.h>\n#include <linux/poison.h>\n#include <linux/interrupt.h>\n#include <linux/memory.h>\n#include <linux/cache.h>\n#include <linux/compiler.h>\n#include <linux/kfence.h>\n#include <linux/module.h>\n#include <linux/cpu.h>\n#include <linux/uaccess.h>\n#include <linux/seq_file.h>\n#include <linux/dma-mapping.h>\n#include <linux/swiotlb.h>\n#include <linux/proc_fs.h>\n#include <linux/debugfs.h>\n#include <linux/kasan.h>\n#include <asm/cacheflush.h>\n#include <asm/tlbflush.h>\n#include <asm/page.h>\n#include <linux/memcontrol.h>\n#include <linux/stackdepot.h>\n\n#include \"internal.h\"\n#include \"slab.h\"\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/kmem.h>\n\nenum slab_state slab_state;\nLIST_HEAD(slab_caches);\nDEFINE_MUTEX(slab_mutex);\nstruct kmem_cache *kmem_cache;\n\nstatic LIST_HEAD(slab_caches_to_rcu_destroy);\nstatic void slab_caches_to_rcu_destroy_workfn(struct work_struct *work);\nstatic DECLARE_WORK(slab_caches_to_rcu_destroy_work,\n\t\t    slab_caches_to_rcu_destroy_workfn);\n\n \n#define SLAB_NEVER_MERGE (SLAB_RED_ZONE | SLAB_POISON | SLAB_STORE_USER | \\\n\t\tSLAB_TRACE | SLAB_TYPESAFE_BY_RCU | SLAB_NOLEAKTRACE | \\\n\t\tSLAB_FAILSLAB | SLAB_NO_MERGE | kasan_never_merge())\n\n#define SLAB_MERGE_SAME (SLAB_RECLAIM_ACCOUNT | SLAB_CACHE_DMA | \\\n\t\t\t SLAB_CACHE_DMA32 | SLAB_ACCOUNT)\n\n \nstatic bool slab_nomerge = !IS_ENABLED(CONFIG_SLAB_MERGE_DEFAULT);\n\nstatic int __init setup_slab_nomerge(char *str)\n{\n\tslab_nomerge = true;\n\treturn 1;\n}\n\nstatic int __init setup_slab_merge(char *str)\n{\n\tslab_nomerge = false;\n\treturn 1;\n}\n\n#ifdef CONFIG_SLUB\n__setup_param(\"slub_nomerge\", slub_nomerge, setup_slab_nomerge, 0);\n__setup_param(\"slub_merge\", slub_merge, setup_slab_merge, 0);\n#endif\n\n__setup(\"slab_nomerge\", setup_slab_nomerge);\n__setup(\"slab_merge\", setup_slab_merge);\n\n \nunsigned int kmem_cache_size(struct kmem_cache *s)\n{\n\treturn s->object_size;\n}\nEXPORT_SYMBOL(kmem_cache_size);\n\n#ifdef CONFIG_DEBUG_VM\nstatic int kmem_cache_sanity_check(const char *name, unsigned int size)\n{\n\tif (!name || in_interrupt() || size > KMALLOC_MAX_SIZE) {\n\t\tpr_err(\"kmem_cache_create(%s) integrity check failed\\n\", name);\n\t\treturn -EINVAL;\n\t}\n\n\tWARN_ON(strchr(name, ' '));\t \n\treturn 0;\n}\n#else\nstatic inline int kmem_cache_sanity_check(const char *name, unsigned int size)\n{\n\treturn 0;\n}\n#endif\n\n \nstatic unsigned int calculate_alignment(slab_flags_t flags,\n\t\tunsigned int align, unsigned int size)\n{\n\t \n\tif (flags & SLAB_HWCACHE_ALIGN) {\n\t\tunsigned int ralign;\n\n\t\tralign = cache_line_size();\n\t\twhile (size <= ralign / 2)\n\t\t\tralign /= 2;\n\t\talign = max(align, ralign);\n\t}\n\n\talign = max(align, arch_slab_minalign());\n\n\treturn ALIGN(align, sizeof(void *));\n}\n\n \nint slab_unmergeable(struct kmem_cache *s)\n{\n\tif (slab_nomerge || (s->flags & SLAB_NEVER_MERGE))\n\t\treturn 1;\n\n\tif (s->ctor)\n\t\treturn 1;\n\n#ifdef CONFIG_HARDENED_USERCOPY\n\tif (s->usersize)\n\t\treturn 1;\n#endif\n\n\t \n\tif (s->refcount < 0)\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstruct kmem_cache *find_mergeable(unsigned int size, unsigned int align,\n\t\tslab_flags_t flags, const char *name, void (*ctor)(void *))\n{\n\tstruct kmem_cache *s;\n\n\tif (slab_nomerge)\n\t\treturn NULL;\n\n\tif (ctor)\n\t\treturn NULL;\n\n\tsize = ALIGN(size, sizeof(void *));\n\talign = calculate_alignment(flags, align, size);\n\tsize = ALIGN(size, align);\n\tflags = kmem_cache_flags(size, flags, name);\n\n\tif (flags & SLAB_NEVER_MERGE)\n\t\treturn NULL;\n\n\tlist_for_each_entry_reverse(s, &slab_caches, list) {\n\t\tif (slab_unmergeable(s))\n\t\t\tcontinue;\n\n\t\tif (size > s->size)\n\t\t\tcontinue;\n\n\t\tif ((flags & SLAB_MERGE_SAME) != (s->flags & SLAB_MERGE_SAME))\n\t\t\tcontinue;\n\t\t \n\t\tif ((s->size & ~(align - 1)) != s->size)\n\t\t\tcontinue;\n\n\t\tif (s->size - size >= sizeof(void *))\n\t\t\tcontinue;\n\n\t\tif (IS_ENABLED(CONFIG_SLAB) && align &&\n\t\t\t(align > s->align || s->align % align))\n\t\t\tcontinue;\n\n\t\treturn s;\n\t}\n\treturn NULL;\n}\n\nstatic struct kmem_cache *create_cache(const char *name,\n\t\tunsigned int object_size, unsigned int align,\n\t\tslab_flags_t flags, unsigned int useroffset,\n\t\tunsigned int usersize, void (*ctor)(void *),\n\t\tstruct kmem_cache *root_cache)\n{\n\tstruct kmem_cache *s;\n\tint err;\n\n\tif (WARN_ON(useroffset + usersize > object_size))\n\t\tuseroffset = usersize = 0;\n\n\terr = -ENOMEM;\n\ts = kmem_cache_zalloc(kmem_cache, GFP_KERNEL);\n\tif (!s)\n\t\tgoto out;\n\n\ts->name = name;\n\ts->size = s->object_size = object_size;\n\ts->align = align;\n\ts->ctor = ctor;\n#ifdef CONFIG_HARDENED_USERCOPY\n\ts->useroffset = useroffset;\n\ts->usersize = usersize;\n#endif\n\n\terr = __kmem_cache_create(s, flags);\n\tif (err)\n\t\tgoto out_free_cache;\n\n\ts->refcount = 1;\n\tlist_add(&s->list, &slab_caches);\n\treturn s;\n\nout_free_cache:\n\tkmem_cache_free(kmem_cache, s);\nout:\n\treturn ERR_PTR(err);\n}\n\n \nstruct kmem_cache *\nkmem_cache_create_usercopy(const char *name,\n\t\t  unsigned int size, unsigned int align,\n\t\t  slab_flags_t flags,\n\t\t  unsigned int useroffset, unsigned int usersize,\n\t\t  void (*ctor)(void *))\n{\n\tstruct kmem_cache *s = NULL;\n\tconst char *cache_name;\n\tint err;\n\n#ifdef CONFIG_SLUB_DEBUG\n\t \n\tif (flags & SLAB_DEBUG_FLAGS)\n\t\tstatic_branch_enable(&slub_debug_enabled);\n\tif (flags & SLAB_STORE_USER)\n\t\tstack_depot_init();\n#endif\n\n\tmutex_lock(&slab_mutex);\n\n\terr = kmem_cache_sanity_check(name, size);\n\tif (err) {\n\t\tgoto out_unlock;\n\t}\n\n\t \n\tif (flags & ~SLAB_FLAGS_PERMITTED) {\n\t\terr = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\t \n\tflags &= CACHE_CREATE_MASK;\n\n\t \n\tif (!IS_ENABLED(CONFIG_HARDENED_USERCOPY) ||\n\t    WARN_ON(!usersize && useroffset) ||\n\t    WARN_ON(size < usersize || size - usersize < useroffset))\n\t\tusersize = useroffset = 0;\n\n\tif (!usersize)\n\t\ts = __kmem_cache_alias(name, size, align, flags, ctor);\n\tif (s)\n\t\tgoto out_unlock;\n\n\tcache_name = kstrdup_const(name, GFP_KERNEL);\n\tif (!cache_name) {\n\t\terr = -ENOMEM;\n\t\tgoto out_unlock;\n\t}\n\n\ts = create_cache(cache_name, size,\n\t\t\t calculate_alignment(flags, align, size),\n\t\t\t flags, useroffset, usersize, ctor, NULL);\n\tif (IS_ERR(s)) {\n\t\terr = PTR_ERR(s);\n\t\tkfree_const(cache_name);\n\t}\n\nout_unlock:\n\tmutex_unlock(&slab_mutex);\n\n\tif (err) {\n\t\tif (flags & SLAB_PANIC)\n\t\t\tpanic(\"%s: Failed to create slab '%s'. Error %d\\n\",\n\t\t\t\t__func__, name, err);\n\t\telse {\n\t\t\tpr_warn(\"%s(%s) failed with error %d\\n\",\n\t\t\t\t__func__, name, err);\n\t\t\tdump_stack();\n\t\t}\n\t\treturn NULL;\n\t}\n\treturn s;\n}\nEXPORT_SYMBOL(kmem_cache_create_usercopy);\n\n \nstruct kmem_cache *\nkmem_cache_create(const char *name, unsigned int size, unsigned int align,\n\t\tslab_flags_t flags, void (*ctor)(void *))\n{\n\treturn kmem_cache_create_usercopy(name, size, align, flags, 0, 0,\n\t\t\t\t\t  ctor);\n}\nEXPORT_SYMBOL(kmem_cache_create);\n\n#ifdef SLAB_SUPPORTS_SYSFS\n \nstatic void kmem_cache_release(struct kmem_cache *s)\n{\n\tsysfs_slab_unlink(s);\n\tsysfs_slab_release(s);\n}\n#else\nstatic void kmem_cache_release(struct kmem_cache *s)\n{\n\tslab_kmem_cache_release(s);\n}\n#endif\n\nstatic void slab_caches_to_rcu_destroy_workfn(struct work_struct *work)\n{\n\tLIST_HEAD(to_destroy);\n\tstruct kmem_cache *s, *s2;\n\n\t \n\tmutex_lock(&slab_mutex);\n\tlist_splice_init(&slab_caches_to_rcu_destroy, &to_destroy);\n\tmutex_unlock(&slab_mutex);\n\n\tif (list_empty(&to_destroy))\n\t\treturn;\n\n\trcu_barrier();\n\n\tlist_for_each_entry_safe(s, s2, &to_destroy, list) {\n\t\tdebugfs_slab_release(s);\n\t\tkfence_shutdown_cache(s);\n\t\tkmem_cache_release(s);\n\t}\n}\n\nstatic int shutdown_cache(struct kmem_cache *s)\n{\n\t \n\tkasan_cache_shutdown(s);\n\n\tif (__kmem_cache_shutdown(s) != 0)\n\t\treturn -EBUSY;\n\n\tlist_del(&s->list);\n\n\tif (s->flags & SLAB_TYPESAFE_BY_RCU) {\n\t\tlist_add_tail(&s->list, &slab_caches_to_rcu_destroy);\n\t\tschedule_work(&slab_caches_to_rcu_destroy_work);\n\t} else {\n\t\tkfence_shutdown_cache(s);\n\t\tdebugfs_slab_release(s);\n\t}\n\n\treturn 0;\n}\n\nvoid slab_kmem_cache_release(struct kmem_cache *s)\n{\n\t__kmem_cache_release(s);\n\tkfree_const(s->name);\n\tkmem_cache_free(kmem_cache, s);\n}\n\nvoid kmem_cache_destroy(struct kmem_cache *s)\n{\n\tint err = -EBUSY;\n\tbool rcu_set;\n\n\tif (unlikely(!s) || !kasan_check_byte(s))\n\t\treturn;\n\n\tcpus_read_lock();\n\tmutex_lock(&slab_mutex);\n\n\trcu_set = s->flags & SLAB_TYPESAFE_BY_RCU;\n\n\ts->refcount--;\n\tif (s->refcount)\n\t\tgoto out_unlock;\n\n\terr = shutdown_cache(s);\n\tWARN(err, \"%s %s: Slab cache still has objects when called from %pS\",\n\t     __func__, s->name, (void *)_RET_IP_);\nout_unlock:\n\tmutex_unlock(&slab_mutex);\n\tcpus_read_unlock();\n\tif (!err && !rcu_set)\n\t\tkmem_cache_release(s);\n}\nEXPORT_SYMBOL(kmem_cache_destroy);\n\n \nint kmem_cache_shrink(struct kmem_cache *cachep)\n{\n\tkasan_cache_shrink(cachep);\n\n\treturn __kmem_cache_shrink(cachep);\n}\nEXPORT_SYMBOL(kmem_cache_shrink);\n\nbool slab_is_available(void)\n{\n\treturn slab_state >= UP;\n}\n\n#ifdef CONFIG_PRINTK\n \nbool kmem_valid_obj(void *object)\n{\n\tstruct folio *folio;\n\n\t \n\tif (object < (void *)PAGE_SIZE || !virt_addr_valid(object))\n\t\treturn false;\n\tfolio = virt_to_folio(object);\n\treturn folio_test_slab(folio);\n}\nEXPORT_SYMBOL_GPL(kmem_valid_obj);\n\nstatic void kmem_obj_info(struct kmem_obj_info *kpp, void *object, struct slab *slab)\n{\n\tif (__kfence_obj_info(kpp, object, slab))\n\t\treturn;\n\t__kmem_obj_info(kpp, object, slab);\n}\n\n \nvoid kmem_dump_obj(void *object)\n{\n\tchar *cp = IS_ENABLED(CONFIG_MMU) ? \"\" : \"/vmalloc\";\n\tint i;\n\tstruct slab *slab;\n\tunsigned long ptroffset;\n\tstruct kmem_obj_info kp = { };\n\n\tif (WARN_ON_ONCE(!virt_addr_valid(object)))\n\t\treturn;\n\tslab = virt_to_slab(object);\n\tif (WARN_ON_ONCE(!slab)) {\n\t\tpr_cont(\" non-slab memory.\\n\");\n\t\treturn;\n\t}\n\tkmem_obj_info(&kp, object, slab);\n\tif (kp.kp_slab_cache)\n\t\tpr_cont(\" slab%s %s\", cp, kp.kp_slab_cache->name);\n\telse\n\t\tpr_cont(\" slab%s\", cp);\n\tif (is_kfence_address(object))\n\t\tpr_cont(\" (kfence)\");\n\tif (kp.kp_objp)\n\t\tpr_cont(\" start %px\", kp.kp_objp);\n\tif (kp.kp_data_offset)\n\t\tpr_cont(\" data offset %lu\", kp.kp_data_offset);\n\tif (kp.kp_objp) {\n\t\tptroffset = ((char *)object - (char *)kp.kp_objp) - kp.kp_data_offset;\n\t\tpr_cont(\" pointer offset %lu\", ptroffset);\n\t}\n\tif (kp.kp_slab_cache && kp.kp_slab_cache->object_size)\n\t\tpr_cont(\" size %u\", kp.kp_slab_cache->object_size);\n\tif (kp.kp_ret)\n\t\tpr_cont(\" allocated at %pS\\n\", kp.kp_ret);\n\telse\n\t\tpr_cont(\"\\n\");\n\tfor (i = 0; i < ARRAY_SIZE(kp.kp_stack); i++) {\n\t\tif (!kp.kp_stack[i])\n\t\t\tbreak;\n\t\tpr_info(\"    %pS\\n\", kp.kp_stack[i]);\n\t}\n\n\tif (kp.kp_free_stack[0])\n\t\tpr_cont(\" Free path:\\n\");\n\n\tfor (i = 0; i < ARRAY_SIZE(kp.kp_free_stack); i++) {\n\t\tif (!kp.kp_free_stack[i])\n\t\t\tbreak;\n\t\tpr_info(\"    %pS\\n\", kp.kp_free_stack[i]);\n\t}\n\n}\nEXPORT_SYMBOL_GPL(kmem_dump_obj);\n#endif\n\n \nvoid __init create_boot_cache(struct kmem_cache *s, const char *name,\n\t\tunsigned int size, slab_flags_t flags,\n\t\tunsigned int useroffset, unsigned int usersize)\n{\n\tint err;\n\tunsigned int align = ARCH_KMALLOC_MINALIGN;\n\n\ts->name = name;\n\ts->size = s->object_size = size;\n\n\t \n\tif (is_power_of_2(size))\n\t\talign = max(align, size);\n\ts->align = calculate_alignment(flags, align, size);\n\n#ifdef CONFIG_HARDENED_USERCOPY\n\ts->useroffset = useroffset;\n\ts->usersize = usersize;\n#endif\n\n\terr = __kmem_cache_create(s, flags);\n\n\tif (err)\n\t\tpanic(\"Creation of kmalloc slab %s size=%u failed. Reason %d\\n\",\n\t\t\t\t\tname, size, err);\n\n\ts->refcount = -1;\t \n}\n\nstatic struct kmem_cache *__init create_kmalloc_cache(const char *name,\n\t\t\t\t\t\t      unsigned int size,\n\t\t\t\t\t\t      slab_flags_t flags)\n{\n\tstruct kmem_cache *s = kmem_cache_zalloc(kmem_cache, GFP_NOWAIT);\n\n\tif (!s)\n\t\tpanic(\"Out of memory when creating slab %s\\n\", name);\n\n\tcreate_boot_cache(s, name, size, flags | SLAB_KMALLOC, 0, size);\n\tlist_add(&s->list, &slab_caches);\n\ts->refcount = 1;\n\treturn s;\n}\n\nstruct kmem_cache *\nkmalloc_caches[NR_KMALLOC_TYPES][KMALLOC_SHIFT_HIGH + 1] __ro_after_init =\n{  \nstatic u8 size_index[24] __ro_after_init = {\n\t3,\t \n\t4,\t \n\t5,\t \n\t5,\t \n\t6,\t \n\t6,\t \n\t6,\t \n\t6,\t \n\t1,\t \n\t1,\t \n\t1,\t \n\t1,\t \n\t7,\t \n\t7,\t \n\t7,\t \n\t7,\t \n\t2,\t \n\t2,\t \n\t2,\t \n\t2,\t \n\t2,\t \n\t2,\t \n\t2,\t \n\t2\t \n};\n\nstatic inline unsigned int size_index_elem(unsigned int bytes)\n{\n\treturn (bytes - 1) / 8;\n}\n\n \nstruct kmem_cache *kmalloc_slab(size_t size, gfp_t flags, unsigned long caller)\n{\n\tunsigned int index;\n\n\tif (size <= 192) {\n\t\tif (!size)\n\t\t\treturn ZERO_SIZE_PTR;\n\n\t\tindex = size_index[size_index_elem(size)];\n\t} else {\n\t\tif (WARN_ON_ONCE(size > KMALLOC_MAX_CACHE_SIZE))\n\t\t\treturn NULL;\n\t\tindex = fls(size - 1);\n\t}\n\n\treturn kmalloc_caches[kmalloc_type(flags, caller)][index];\n}\n\nsize_t kmalloc_size_roundup(size_t size)\n{\n\tif (size && size <= KMALLOC_MAX_CACHE_SIZE) {\n\t\t \n\t\treturn kmalloc_slab(size, GFP_KERNEL, 0)->object_size;\n\t}\n\n\t \n\tif (size && size <= KMALLOC_MAX_SIZE)\n\t\treturn PAGE_SIZE << get_order(size);\n\n\t \n\treturn size;\n\n}\nEXPORT_SYMBOL(kmalloc_size_roundup);\n\n#ifdef CONFIG_ZONE_DMA\n#define KMALLOC_DMA_NAME(sz)\t.name[KMALLOC_DMA] = \"dma-kmalloc-\" #sz,\n#else\n#define KMALLOC_DMA_NAME(sz)\n#endif\n\n#ifdef CONFIG_MEMCG_KMEM\n#define KMALLOC_CGROUP_NAME(sz)\t.name[KMALLOC_CGROUP] = \"kmalloc-cg-\" #sz,\n#else\n#define KMALLOC_CGROUP_NAME(sz)\n#endif\n\n#ifndef CONFIG_SLUB_TINY\n#define KMALLOC_RCL_NAME(sz)\t.name[KMALLOC_RECLAIM] = \"kmalloc-rcl-\" #sz,\n#else\n#define KMALLOC_RCL_NAME(sz)\n#endif\n\n#ifdef CONFIG_RANDOM_KMALLOC_CACHES\n#define __KMALLOC_RANDOM_CONCAT(a, b) a ## b\n#define KMALLOC_RANDOM_NAME(N, sz) __KMALLOC_RANDOM_CONCAT(KMA_RAND_, N)(sz)\n#define KMA_RAND_1(sz)                  .name[KMALLOC_RANDOM_START +  1] = \"kmalloc-rnd-01-\" #sz,\n#define KMA_RAND_2(sz)  KMA_RAND_1(sz)  .name[KMALLOC_RANDOM_START +  2] = \"kmalloc-rnd-02-\" #sz,\n#define KMA_RAND_3(sz)  KMA_RAND_2(sz)  .name[KMALLOC_RANDOM_START +  3] = \"kmalloc-rnd-03-\" #sz,\n#define KMA_RAND_4(sz)  KMA_RAND_3(sz)  .name[KMALLOC_RANDOM_START +  4] = \"kmalloc-rnd-04-\" #sz,\n#define KMA_RAND_5(sz)  KMA_RAND_4(sz)  .name[KMALLOC_RANDOM_START +  5] = \"kmalloc-rnd-05-\" #sz,\n#define KMA_RAND_6(sz)  KMA_RAND_5(sz)  .name[KMALLOC_RANDOM_START +  6] = \"kmalloc-rnd-06-\" #sz,\n#define KMA_RAND_7(sz)  KMA_RAND_6(sz)  .name[KMALLOC_RANDOM_START +  7] = \"kmalloc-rnd-07-\" #sz,\n#define KMA_RAND_8(sz)  KMA_RAND_7(sz)  .name[KMALLOC_RANDOM_START +  8] = \"kmalloc-rnd-08-\" #sz,\n#define KMA_RAND_9(sz)  KMA_RAND_8(sz)  .name[KMALLOC_RANDOM_START +  9] = \"kmalloc-rnd-09-\" #sz,\n#define KMA_RAND_10(sz) KMA_RAND_9(sz)  .name[KMALLOC_RANDOM_START + 10] = \"kmalloc-rnd-10-\" #sz,\n#define KMA_RAND_11(sz) KMA_RAND_10(sz) .name[KMALLOC_RANDOM_START + 11] = \"kmalloc-rnd-11-\" #sz,\n#define KMA_RAND_12(sz) KMA_RAND_11(sz) .name[KMALLOC_RANDOM_START + 12] = \"kmalloc-rnd-12-\" #sz,\n#define KMA_RAND_13(sz) KMA_RAND_12(sz) .name[KMALLOC_RANDOM_START + 13] = \"kmalloc-rnd-13-\" #sz,\n#define KMA_RAND_14(sz) KMA_RAND_13(sz) .name[KMALLOC_RANDOM_START + 14] = \"kmalloc-rnd-14-\" #sz,\n#define KMA_RAND_15(sz) KMA_RAND_14(sz) .name[KMALLOC_RANDOM_START + 15] = \"kmalloc-rnd-15-\" #sz,\n#else \n#define KMALLOC_RANDOM_NAME(N, sz)\n#endif\n\n#define INIT_KMALLOC_INFO(__size, __short_size)\t\t\t\\\n{\t\t\t\t\t\t\t\t\\\n\t.name[KMALLOC_NORMAL]  = \"kmalloc-\" #__short_size,\t\\\n\tKMALLOC_RCL_NAME(__short_size)\t\t\t\t\\\n\tKMALLOC_CGROUP_NAME(__short_size)\t\t\t\\\n\tKMALLOC_DMA_NAME(__short_size)\t\t\t\t\\\n\tKMALLOC_RANDOM_NAME(RANDOM_KMALLOC_CACHES_NR, __short_size)\t\\\n\t.size = __size,\t\t\t\t\t\t\\\n}\n\n \nconst struct kmalloc_info_struct kmalloc_info[] __initconst = {\n\tINIT_KMALLOC_INFO(0, 0),\n\tINIT_KMALLOC_INFO(96, 96),\n\tINIT_KMALLOC_INFO(192, 192),\n\tINIT_KMALLOC_INFO(8, 8),\n\tINIT_KMALLOC_INFO(16, 16),\n\tINIT_KMALLOC_INFO(32, 32),\n\tINIT_KMALLOC_INFO(64, 64),\n\tINIT_KMALLOC_INFO(128, 128),\n\tINIT_KMALLOC_INFO(256, 256),\n\tINIT_KMALLOC_INFO(512, 512),\n\tINIT_KMALLOC_INFO(1024, 1k),\n\tINIT_KMALLOC_INFO(2048, 2k),\n\tINIT_KMALLOC_INFO(4096, 4k),\n\tINIT_KMALLOC_INFO(8192, 8k),\n\tINIT_KMALLOC_INFO(16384, 16k),\n\tINIT_KMALLOC_INFO(32768, 32k),\n\tINIT_KMALLOC_INFO(65536, 64k),\n\tINIT_KMALLOC_INFO(131072, 128k),\n\tINIT_KMALLOC_INFO(262144, 256k),\n\tINIT_KMALLOC_INFO(524288, 512k),\n\tINIT_KMALLOC_INFO(1048576, 1M),\n\tINIT_KMALLOC_INFO(2097152, 2M)\n};\n\n \nvoid __init setup_kmalloc_cache_index_table(void)\n{\n\tunsigned int i;\n\n\tBUILD_BUG_ON(KMALLOC_MIN_SIZE > 256 ||\n\t\t!is_power_of_2(KMALLOC_MIN_SIZE));\n\n\tfor (i = 8; i < KMALLOC_MIN_SIZE; i += 8) {\n\t\tunsigned int elem = size_index_elem(i);\n\n\t\tif (elem >= ARRAY_SIZE(size_index))\n\t\t\tbreak;\n\t\tsize_index[elem] = KMALLOC_SHIFT_LOW;\n\t}\n\n\tif (KMALLOC_MIN_SIZE >= 64) {\n\t\t \n\t\tfor (i = 64 + 8; i <= 96; i += 8)\n\t\t\tsize_index[size_index_elem(i)] = 7;\n\n\t}\n\n\tif (KMALLOC_MIN_SIZE >= 128) {\n\t\t \n\t\tfor (i = 128 + 8; i <= 192; i += 8)\n\t\t\tsize_index[size_index_elem(i)] = 8;\n\t}\n}\n\nstatic unsigned int __kmalloc_minalign(void)\n{\n\tunsigned int minalign = dma_get_cache_alignment();\n\n\tif (IS_ENABLED(CONFIG_DMA_BOUNCE_UNALIGNED_KMALLOC) &&\n\t    is_swiotlb_allocated())\n\t\tminalign = ARCH_KMALLOC_MINALIGN;\n\n\treturn max(minalign, arch_slab_minalign());\n}\n\nvoid __init\nnew_kmalloc_cache(int idx, enum kmalloc_cache_type type, slab_flags_t flags)\n{\n\tunsigned int minalign = __kmalloc_minalign();\n\tunsigned int aligned_size = kmalloc_info[idx].size;\n\tint aligned_idx = idx;\n\n\tif ((KMALLOC_RECLAIM != KMALLOC_NORMAL) && (type == KMALLOC_RECLAIM)) {\n\t\tflags |= SLAB_RECLAIM_ACCOUNT;\n\t} else if (IS_ENABLED(CONFIG_MEMCG_KMEM) && (type == KMALLOC_CGROUP)) {\n\t\tif (mem_cgroup_kmem_disabled()) {\n\t\t\tkmalloc_caches[type][idx] = kmalloc_caches[KMALLOC_NORMAL][idx];\n\t\t\treturn;\n\t\t}\n\t\tflags |= SLAB_ACCOUNT;\n\t} else if (IS_ENABLED(CONFIG_ZONE_DMA) && (type == KMALLOC_DMA)) {\n\t\tflags |= SLAB_CACHE_DMA;\n\t}\n\n#ifdef CONFIG_RANDOM_KMALLOC_CACHES\n\tif (type >= KMALLOC_RANDOM_START && type <= KMALLOC_RANDOM_END)\n\t\tflags |= SLAB_NO_MERGE;\n#endif\n\n\t \n\tif (IS_ENABLED(CONFIG_MEMCG_KMEM) && (type == KMALLOC_NORMAL))\n\t\tflags |= SLAB_NO_MERGE;\n\n\tif (minalign > ARCH_KMALLOC_MINALIGN) {\n\t\taligned_size = ALIGN(aligned_size, minalign);\n\t\taligned_idx = __kmalloc_index(aligned_size, false);\n\t}\n\n\tif (!kmalloc_caches[type][aligned_idx])\n\t\tkmalloc_caches[type][aligned_idx] = create_kmalloc_cache(\n\t\t\t\t\tkmalloc_info[aligned_idx].name[type],\n\t\t\t\t\taligned_size, flags);\n\tif (idx != aligned_idx)\n\t\tkmalloc_caches[type][idx] = kmalloc_caches[type][aligned_idx];\n}\n\n \nvoid __init create_kmalloc_caches(slab_flags_t flags)\n{\n\tint i;\n\tenum kmalloc_cache_type type;\n\n\t \n\tfor (type = KMALLOC_NORMAL; type < NR_KMALLOC_TYPES; type++) {\n\t\tfor (i = KMALLOC_SHIFT_LOW; i <= KMALLOC_SHIFT_HIGH; i++) {\n\t\t\tif (!kmalloc_caches[type][i])\n\t\t\t\tnew_kmalloc_cache(i, type, flags);\n\n\t\t\t \n\t\t\tif (KMALLOC_MIN_SIZE <= 32 && i == 6 &&\n\t\t\t\t\t!kmalloc_caches[type][1])\n\t\t\t\tnew_kmalloc_cache(1, type, flags);\n\t\t\tif (KMALLOC_MIN_SIZE <= 64 && i == 7 &&\n\t\t\t\t\t!kmalloc_caches[type][2])\n\t\t\t\tnew_kmalloc_cache(2, type, flags);\n\t\t}\n\t}\n#ifdef CONFIG_RANDOM_KMALLOC_CACHES\n\trandom_kmalloc_seed = get_random_u64();\n#endif\n\n\t \n\tslab_state = UP;\n}\n\nvoid free_large_kmalloc(struct folio *folio, void *object)\n{\n\tunsigned int order = folio_order(folio);\n\n\tif (WARN_ON_ONCE(order == 0))\n\t\tpr_warn_once(\"object pointer: 0x%p\\n\", object);\n\n\tkmemleak_free(object);\n\tkasan_kfree_large(object);\n\tkmsan_kfree_large(object);\n\n\tmod_lruvec_page_state(folio_page(folio, 0), NR_SLAB_UNRECLAIMABLE_B,\n\t\t\t      -(PAGE_SIZE << order));\n\t__free_pages(folio_page(folio, 0), order);\n}\n\nstatic void *__kmalloc_large_node(size_t size, gfp_t flags, int node);\nstatic __always_inline\nvoid *__do_kmalloc_node(size_t size, gfp_t flags, int node, unsigned long caller)\n{\n\tstruct kmem_cache *s;\n\tvoid *ret;\n\n\tif (unlikely(size > KMALLOC_MAX_CACHE_SIZE)) {\n\t\tret = __kmalloc_large_node(size, flags, node);\n\t\ttrace_kmalloc(caller, ret, size,\n\t\t\t      PAGE_SIZE << get_order(size), flags, node);\n\t\treturn ret;\n\t}\n\n\ts = kmalloc_slab(size, flags, caller);\n\n\tif (unlikely(ZERO_OR_NULL_PTR(s)))\n\t\treturn s;\n\n\tret = __kmem_cache_alloc_node(s, flags, node, size, caller);\n\tret = kasan_kmalloc(s, ret, size, flags);\n\ttrace_kmalloc(caller, ret, size, s->size, flags, node);\n\treturn ret;\n}\n\nvoid *__kmalloc_node(size_t size, gfp_t flags, int node)\n{\n\treturn __do_kmalloc_node(size, flags, node, _RET_IP_);\n}\nEXPORT_SYMBOL(__kmalloc_node);\n\nvoid *__kmalloc(size_t size, gfp_t flags)\n{\n\treturn __do_kmalloc_node(size, flags, NUMA_NO_NODE, _RET_IP_);\n}\nEXPORT_SYMBOL(__kmalloc);\n\nvoid *__kmalloc_node_track_caller(size_t size, gfp_t flags,\n\t\t\t\t  int node, unsigned long caller)\n{\n\treturn __do_kmalloc_node(size, flags, node, caller);\n}\nEXPORT_SYMBOL(__kmalloc_node_track_caller);\n\n \nvoid kfree(const void *object)\n{\n\tstruct folio *folio;\n\tstruct slab *slab;\n\tstruct kmem_cache *s;\n\n\ttrace_kfree(_RET_IP_, object);\n\n\tif (unlikely(ZERO_OR_NULL_PTR(object)))\n\t\treturn;\n\n\tfolio = virt_to_folio(object);\n\tif (unlikely(!folio_test_slab(folio))) {\n\t\tfree_large_kmalloc(folio, (void *)object);\n\t\treturn;\n\t}\n\n\tslab = folio_slab(folio);\n\ts = slab->slab_cache;\n\t__kmem_cache_free(s, (void *)object, _RET_IP_);\n}\nEXPORT_SYMBOL(kfree);\n\n \nsize_t __ksize(const void *object)\n{\n\tstruct folio *folio;\n\n\tif (unlikely(object == ZERO_SIZE_PTR))\n\t\treturn 0;\n\n\tfolio = virt_to_folio(object);\n\n\tif (unlikely(!folio_test_slab(folio))) {\n\t\tif (WARN_ON(folio_size(folio) <= KMALLOC_MAX_CACHE_SIZE))\n\t\t\treturn 0;\n\t\tif (WARN_ON(object != folio_address(folio)))\n\t\t\treturn 0;\n\t\treturn folio_size(folio);\n\t}\n\n#ifdef CONFIG_SLUB_DEBUG\n\tskip_orig_size_check(folio_slab(folio)->slab_cache, object);\n#endif\n\n\treturn slab_ksize(folio_slab(folio)->slab_cache);\n}\n\nvoid *kmalloc_trace(struct kmem_cache *s, gfp_t gfpflags, size_t size)\n{\n\tvoid *ret = __kmem_cache_alloc_node(s, gfpflags, NUMA_NO_NODE,\n\t\t\t\t\t    size, _RET_IP_);\n\n\ttrace_kmalloc(_RET_IP_, ret, size, s->size, gfpflags, NUMA_NO_NODE);\n\n\tret = kasan_kmalloc(s, ret, size, gfpflags);\n\treturn ret;\n}\nEXPORT_SYMBOL(kmalloc_trace);\n\nvoid *kmalloc_node_trace(struct kmem_cache *s, gfp_t gfpflags,\n\t\t\t int node, size_t size)\n{\n\tvoid *ret = __kmem_cache_alloc_node(s, gfpflags, node, size, _RET_IP_);\n\n\ttrace_kmalloc(_RET_IP_, ret, size, s->size, gfpflags, node);\n\n\tret = kasan_kmalloc(s, ret, size, gfpflags);\n\treturn ret;\n}\nEXPORT_SYMBOL(kmalloc_node_trace);\n\ngfp_t kmalloc_fix_flags(gfp_t flags)\n{\n\tgfp_t invalid_mask = flags & GFP_SLAB_BUG_MASK;\n\n\tflags &= ~GFP_SLAB_BUG_MASK;\n\tpr_warn(\"Unexpected gfp: %#x (%pGg). Fixing up to gfp: %#x (%pGg). Fix your code!\\n\",\n\t\t\tinvalid_mask, &invalid_mask, flags, &flags);\n\tdump_stack();\n\n\treturn flags;\n}\n\n \n\nstatic void *__kmalloc_large_node(size_t size, gfp_t flags, int node)\n{\n\tstruct page *page;\n\tvoid *ptr = NULL;\n\tunsigned int order = get_order(size);\n\n\tif (unlikely(flags & GFP_SLAB_BUG_MASK))\n\t\tflags = kmalloc_fix_flags(flags);\n\n\tflags |= __GFP_COMP;\n\tpage = alloc_pages_node(node, flags, order);\n\tif (page) {\n\t\tptr = page_address(page);\n\t\tmod_lruvec_page_state(page, NR_SLAB_UNRECLAIMABLE_B,\n\t\t\t\t      PAGE_SIZE << order);\n\t}\n\n\tptr = kasan_kmalloc_large(ptr, size, flags);\n\t \n\tkmemleak_alloc(ptr, size, 1, flags);\n\tkmsan_kmalloc_large(ptr, size, flags);\n\n\treturn ptr;\n}\n\nvoid *kmalloc_large(size_t size, gfp_t flags)\n{\n\tvoid *ret = __kmalloc_large_node(size, flags, NUMA_NO_NODE);\n\n\ttrace_kmalloc(_RET_IP_, ret, size, PAGE_SIZE << get_order(size),\n\t\t      flags, NUMA_NO_NODE);\n\treturn ret;\n}\nEXPORT_SYMBOL(kmalloc_large);\n\nvoid *kmalloc_large_node(size_t size, gfp_t flags, int node)\n{\n\tvoid *ret = __kmalloc_large_node(size, flags, node);\n\n\ttrace_kmalloc(_RET_IP_, ret, size, PAGE_SIZE << get_order(size),\n\t\t      flags, node);\n\treturn ret;\n}\nEXPORT_SYMBOL(kmalloc_large_node);\n\n#ifdef CONFIG_SLAB_FREELIST_RANDOM\n \nstatic void freelist_randomize(unsigned int *list,\n\t\t\t       unsigned int count)\n{\n\tunsigned int rand;\n\tunsigned int i;\n\n\tfor (i = 0; i < count; i++)\n\t\tlist[i] = i;\n\n\t \n\tfor (i = count - 1; i > 0; i--) {\n\t\trand = get_random_u32_below(i + 1);\n\t\tswap(list[i], list[rand]);\n\t}\n}\n\n \nint cache_random_seq_create(struct kmem_cache *cachep, unsigned int count,\n\t\t\t\t    gfp_t gfp)\n{\n\n\tif (count < 2 || cachep->random_seq)\n\t\treturn 0;\n\n\tcachep->random_seq = kcalloc(count, sizeof(unsigned int), gfp);\n\tif (!cachep->random_seq)\n\t\treturn -ENOMEM;\n\n\tfreelist_randomize(cachep->random_seq, count);\n\treturn 0;\n}\n\n \nvoid cache_random_seq_destroy(struct kmem_cache *cachep)\n{\n\tkfree(cachep->random_seq);\n\tcachep->random_seq = NULL;\n}\n#endif  \n\n#if defined(CONFIG_SLAB) || defined(CONFIG_SLUB_DEBUG)\n#ifdef CONFIG_SLAB\n#define SLABINFO_RIGHTS (0600)\n#else\n#define SLABINFO_RIGHTS (0400)\n#endif\n\nstatic void print_slabinfo_header(struct seq_file *m)\n{\n\t \n#ifdef CONFIG_DEBUG_SLAB\n\tseq_puts(m, \"slabinfo - version: 2.1 (statistics)\\n\");\n#else\n\tseq_puts(m, \"slabinfo - version: 2.1\\n\");\n#endif\n\tseq_puts(m, \"# name            <active_objs> <num_objs> <objsize> <objperslab> <pagesperslab>\");\n\tseq_puts(m, \" : tunables <limit> <batchcount> <sharedfactor>\");\n\tseq_puts(m, \" : slabdata <active_slabs> <num_slabs> <sharedavail>\");\n#ifdef CONFIG_DEBUG_SLAB\n\tseq_puts(m, \" : globalstat <listallocs> <maxobjs> <grown> <reaped> <error> <maxfreeable> <nodeallocs> <remotefrees> <alienoverflow>\");\n\tseq_puts(m, \" : cpustat <allochit> <allocmiss> <freehit> <freemiss>\");\n#endif\n\tseq_putc(m, '\\n');\n}\n\nstatic void *slab_start(struct seq_file *m, loff_t *pos)\n{\n\tmutex_lock(&slab_mutex);\n\treturn seq_list_start(&slab_caches, *pos);\n}\n\nstatic void *slab_next(struct seq_file *m, void *p, loff_t *pos)\n{\n\treturn seq_list_next(p, &slab_caches, pos);\n}\n\nstatic void slab_stop(struct seq_file *m, void *p)\n{\n\tmutex_unlock(&slab_mutex);\n}\n\nstatic void cache_show(struct kmem_cache *s, struct seq_file *m)\n{\n\tstruct slabinfo sinfo;\n\n\tmemset(&sinfo, 0, sizeof(sinfo));\n\tget_slabinfo(s, &sinfo);\n\n\tseq_printf(m, \"%-17s %6lu %6lu %6u %4u %4d\",\n\t\t   s->name, sinfo.active_objs, sinfo.num_objs, s->size,\n\t\t   sinfo.objects_per_slab, (1 << sinfo.cache_order));\n\n\tseq_printf(m, \" : tunables %4u %4u %4u\",\n\t\t   sinfo.limit, sinfo.batchcount, sinfo.shared);\n\tseq_printf(m, \" : slabdata %6lu %6lu %6lu\",\n\t\t   sinfo.active_slabs, sinfo.num_slabs, sinfo.shared_avail);\n\tslabinfo_show_stats(m, s);\n\tseq_putc(m, '\\n');\n}\n\nstatic int slab_show(struct seq_file *m, void *p)\n{\n\tstruct kmem_cache *s = list_entry(p, struct kmem_cache, list);\n\n\tif (p == slab_caches.next)\n\t\tprint_slabinfo_header(m);\n\tcache_show(s, m);\n\treturn 0;\n}\n\nvoid dump_unreclaimable_slab(void)\n{\n\tstruct kmem_cache *s;\n\tstruct slabinfo sinfo;\n\n\t \n\tif (!mutex_trylock(&slab_mutex)) {\n\t\tpr_warn(\"excessive unreclaimable slab but cannot dump stats\\n\");\n\t\treturn;\n\t}\n\n\tpr_info(\"Unreclaimable slab info:\\n\");\n\tpr_info(\"Name                      Used          Total\\n\");\n\n\tlist_for_each_entry(s, &slab_caches, list) {\n\t\tif (s->flags & SLAB_RECLAIM_ACCOUNT)\n\t\t\tcontinue;\n\n\t\tget_slabinfo(s, &sinfo);\n\n\t\tif (sinfo.num_objs > 0)\n\t\t\tpr_info(\"%-17s %10luKB %10luKB\\n\", s->name,\n\t\t\t\t(sinfo.active_objs * s->size) / 1024,\n\t\t\t\t(sinfo.num_objs * s->size) / 1024);\n\t}\n\tmutex_unlock(&slab_mutex);\n}\n\n \nstatic const struct seq_operations slabinfo_op = {\n\t.start = slab_start,\n\t.next = slab_next,\n\t.stop = slab_stop,\n\t.show = slab_show,\n};\n\nstatic int slabinfo_open(struct inode *inode, struct file *file)\n{\n\treturn seq_open(file, &slabinfo_op);\n}\n\nstatic const struct proc_ops slabinfo_proc_ops = {\n\t.proc_flags\t= PROC_ENTRY_PERMANENT,\n\t.proc_open\t= slabinfo_open,\n\t.proc_read\t= seq_read,\n\t.proc_write\t= slabinfo_write,\n\t.proc_lseek\t= seq_lseek,\n\t.proc_release\t= seq_release,\n};\n\nstatic int __init slab_proc_init(void)\n{\n\tproc_create(\"slabinfo\", SLABINFO_RIGHTS, NULL, &slabinfo_proc_ops);\n\treturn 0;\n}\nmodule_init(slab_proc_init);\n\n#endif  \n\nstatic __always_inline __realloc_size(2) void *\n__do_krealloc(const void *p, size_t new_size, gfp_t flags)\n{\n\tvoid *ret;\n\tsize_t ks;\n\n\t \n\tif (likely(!ZERO_OR_NULL_PTR(p))) {\n\t\tif (!kasan_check_byte(p))\n\t\t\treturn NULL;\n\t\tks = ksize(p);\n\t} else\n\t\tks = 0;\n\n\t \n\tif (ks >= new_size) {\n\t\tp = kasan_krealloc((void *)p, new_size, flags);\n\t\treturn (void *)p;\n\t}\n\n\tret = kmalloc_track_caller(new_size, flags);\n\tif (ret && p) {\n\t\t \n\t\tkasan_disable_current();\n\t\tmemcpy(ret, kasan_reset_tag(p), ks);\n\t\tkasan_enable_current();\n\t}\n\n\treturn ret;\n}\n\n \nvoid *krealloc(const void *p, size_t new_size, gfp_t flags)\n{\n\tvoid *ret;\n\n\tif (unlikely(!new_size)) {\n\t\tkfree(p);\n\t\treturn ZERO_SIZE_PTR;\n\t}\n\n\tret = __do_krealloc(p, new_size, flags);\n\tif (ret && kasan_reset_tag(p) != kasan_reset_tag(ret))\n\t\tkfree(p);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(krealloc);\n\n \nvoid kfree_sensitive(const void *p)\n{\n\tsize_t ks;\n\tvoid *mem = (void *)p;\n\n\tks = ksize(mem);\n\tif (ks) {\n\t\tkasan_unpoison_range(mem, ks);\n\t\tmemzero_explicit(mem, ks);\n\t}\n\tkfree(mem);\n}\nEXPORT_SYMBOL(kfree_sensitive);\n\nsize_t ksize(const void *objp)\n{\n\t \n\tif (unlikely(ZERO_OR_NULL_PTR(objp)) || !kasan_check_byte(objp))\n\t\treturn 0;\n\n\treturn kfence_ksize(objp) ?: __ksize(objp);\n}\nEXPORT_SYMBOL(ksize);\n\n \nEXPORT_TRACEPOINT_SYMBOL(kmalloc);\nEXPORT_TRACEPOINT_SYMBOL(kmem_cache_alloc);\nEXPORT_TRACEPOINT_SYMBOL(kfree);\nEXPORT_TRACEPOINT_SYMBOL(kmem_cache_free);\n\nint should_failslab(struct kmem_cache *s, gfp_t gfpflags)\n{\n\tif (__should_failslab(s, gfpflags))\n\t\treturn -ENOMEM;\n\treturn 0;\n}\nALLOW_ERROR_INJECTION(should_failslab, ERRNO);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}