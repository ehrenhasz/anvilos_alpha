{
  "module_name": "truncate.c",
  "hash_id": "077d465b2f2065469bdd24701013d21e21eb378787ae14dd008e0444efb304db",
  "original_prompt": "Ingested from linux-6.6.14/mm/truncate.c",
  "human_readable_source": "\n \n\n#include <linux/kernel.h>\n#include <linux/backing-dev.h>\n#include <linux/dax.h>\n#include <linux/gfp.h>\n#include <linux/mm.h>\n#include <linux/swap.h>\n#include <linux/export.h>\n#include <linux/pagemap.h>\n#include <linux/highmem.h>\n#include <linux/pagevec.h>\n#include <linux/task_io_accounting_ops.h>\n#include <linux/shmem_fs.h>\n#include <linux/rmap.h>\n#include \"internal.h\"\n\n \nstatic inline void __clear_shadow_entry(struct address_space *mapping,\n\t\t\t\tpgoff_t index, void *entry)\n{\n\tXA_STATE(xas, &mapping->i_pages, index);\n\n\txas_set_update(&xas, workingset_update_node);\n\tif (xas_load(&xas) != entry)\n\t\treturn;\n\txas_store(&xas, NULL);\n}\n\nstatic void clear_shadow_entry(struct address_space *mapping, pgoff_t index,\n\t\t\t       void *entry)\n{\n\tspin_lock(&mapping->host->i_lock);\n\txa_lock_irq(&mapping->i_pages);\n\t__clear_shadow_entry(mapping, index, entry);\n\txa_unlock_irq(&mapping->i_pages);\n\tif (mapping_shrinkable(mapping))\n\t\tinode_add_lru(mapping->host);\n\tspin_unlock(&mapping->host->i_lock);\n}\n\n \nstatic void truncate_folio_batch_exceptionals(struct address_space *mapping,\n\t\t\t\tstruct folio_batch *fbatch, pgoff_t *indices)\n{\n\tint i, j;\n\tbool dax;\n\n\t \n\tif (shmem_mapping(mapping))\n\t\treturn;\n\n\tfor (j = 0; j < folio_batch_count(fbatch); j++)\n\t\tif (xa_is_value(fbatch->folios[j]))\n\t\t\tbreak;\n\n\tif (j == folio_batch_count(fbatch))\n\t\treturn;\n\n\tdax = dax_mapping(mapping);\n\tif (!dax) {\n\t\tspin_lock(&mapping->host->i_lock);\n\t\txa_lock_irq(&mapping->i_pages);\n\t}\n\n\tfor (i = j; i < folio_batch_count(fbatch); i++) {\n\t\tstruct folio *folio = fbatch->folios[i];\n\t\tpgoff_t index = indices[i];\n\n\t\tif (!xa_is_value(folio)) {\n\t\t\tfbatch->folios[j++] = folio;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (unlikely(dax)) {\n\t\t\tdax_delete_mapping_entry(mapping, index);\n\t\t\tcontinue;\n\t\t}\n\n\t\t__clear_shadow_entry(mapping, index, folio);\n\t}\n\n\tif (!dax) {\n\t\txa_unlock_irq(&mapping->i_pages);\n\t\tif (mapping_shrinkable(mapping))\n\t\t\tinode_add_lru(mapping->host);\n\t\tspin_unlock(&mapping->host->i_lock);\n\t}\n\tfbatch->nr = j;\n}\n\n \nstatic int invalidate_exceptional_entry(struct address_space *mapping,\n\t\t\t\t\tpgoff_t index, void *entry)\n{\n\t \n\tif (shmem_mapping(mapping) || dax_mapping(mapping))\n\t\treturn 1;\n\tclear_shadow_entry(mapping, index, entry);\n\treturn 1;\n}\n\n \nstatic int invalidate_exceptional_entry2(struct address_space *mapping,\n\t\t\t\t\t pgoff_t index, void *entry)\n{\n\t \n\tif (shmem_mapping(mapping))\n\t\treturn 1;\n\tif (dax_mapping(mapping))\n\t\treturn dax_invalidate_mapping_entry_sync(mapping, index);\n\tclear_shadow_entry(mapping, index, entry);\n\treturn 1;\n}\n\n \nvoid folio_invalidate(struct folio *folio, size_t offset, size_t length)\n{\n\tconst struct address_space_operations *aops = folio->mapping->a_ops;\n\n\tif (aops->invalidate_folio)\n\t\taops->invalidate_folio(folio, offset, length);\n}\nEXPORT_SYMBOL_GPL(folio_invalidate);\n\n \nstatic void truncate_cleanup_folio(struct folio *folio)\n{\n\tif (folio_mapped(folio))\n\t\tunmap_mapping_folio(folio);\n\n\tif (folio_has_private(folio))\n\t\tfolio_invalidate(folio, 0, folio_size(folio));\n\n\t \n\tfolio_cancel_dirty(folio);\n\tfolio_clear_mappedtodisk(folio);\n}\n\nint truncate_inode_folio(struct address_space *mapping, struct folio *folio)\n{\n\tif (folio->mapping != mapping)\n\t\treturn -EIO;\n\n\ttruncate_cleanup_folio(folio);\n\tfilemap_remove_folio(folio);\n\treturn 0;\n}\n\n \nbool truncate_inode_partial_folio(struct folio *folio, loff_t start, loff_t end)\n{\n\tloff_t pos = folio_pos(folio);\n\tunsigned int offset, length;\n\n\tif (pos < start)\n\t\toffset = start - pos;\n\telse\n\t\toffset = 0;\n\tlength = folio_size(folio);\n\tif (pos + length <= (u64)end)\n\t\tlength = length - offset;\n\telse\n\t\tlength = end + 1 - pos - offset;\n\n\tfolio_wait_writeback(folio);\n\tif (length == folio_size(folio)) {\n\t\ttruncate_inode_folio(folio->mapping, folio);\n\t\treturn true;\n\t}\n\n\t \n\tfolio_zero_range(folio, offset, length);\n\n\tif (folio_has_private(folio))\n\t\tfolio_invalidate(folio, offset, length);\n\tif (!folio_test_large(folio))\n\t\treturn true;\n\tif (split_folio(folio) == 0)\n\t\treturn true;\n\tif (folio_test_dirty(folio))\n\t\treturn false;\n\ttruncate_inode_folio(folio->mapping, folio);\n\treturn true;\n}\n\n \nint generic_error_remove_page(struct address_space *mapping, struct page *page)\n{\n\tVM_BUG_ON_PAGE(PageTail(page), page);\n\n\tif (!mapping)\n\t\treturn -EINVAL;\n\t \n\tif (!S_ISREG(mapping->host->i_mode))\n\t\treturn -EIO;\n\treturn truncate_inode_folio(mapping, page_folio(page));\n}\nEXPORT_SYMBOL(generic_error_remove_page);\n\nstatic long mapping_evict_folio(struct address_space *mapping,\n\t\tstruct folio *folio)\n{\n\tif (folio_test_dirty(folio) || folio_test_writeback(folio))\n\t\treturn 0;\n\t \n\tif (folio_ref_count(folio) >\n\t\t\tfolio_nr_pages(folio) + folio_has_private(folio) + 1)\n\t\treturn 0;\n\tif (!filemap_release_folio(folio, 0))\n\t\treturn 0;\n\n\treturn remove_mapping(mapping, folio);\n}\n\n \nlong invalidate_inode_page(struct page *page)\n{\n\tstruct folio *folio = page_folio(page);\n\tstruct address_space *mapping = folio_mapping(folio);\n\n\t \n\tif (!mapping)\n\t\treturn 0;\n\treturn mapping_evict_folio(mapping, folio);\n}\n\n \nvoid truncate_inode_pages_range(struct address_space *mapping,\n\t\t\t\tloff_t lstart, loff_t lend)\n{\n\tpgoff_t\t\tstart;\t\t \n\tpgoff_t\t\tend;\t\t \n\tstruct folio_batch fbatch;\n\tpgoff_t\t\tindices[PAGEVEC_SIZE];\n\tpgoff_t\t\tindex;\n\tint\t\ti;\n\tstruct folio\t*folio;\n\tbool\t\tsame_folio;\n\n\tif (mapping_empty(mapping))\n\t\treturn;\n\n\t \n\tstart = (lstart + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\tif (lend == -1)\n\t\t \n\t\tend = -1;\n\telse\n\t\tend = (lend + 1) >> PAGE_SHIFT;\n\n\tfolio_batch_init(&fbatch);\n\tindex = start;\n\twhile (index < end && find_lock_entries(mapping, &index, end - 1,\n\t\t\t&fbatch, indices)) {\n\t\ttruncate_folio_batch_exceptionals(mapping, &fbatch, indices);\n\t\tfor (i = 0; i < folio_batch_count(&fbatch); i++)\n\t\t\ttruncate_cleanup_folio(fbatch.folios[i]);\n\t\tdelete_from_page_cache_batch(mapping, &fbatch);\n\t\tfor (i = 0; i < folio_batch_count(&fbatch); i++)\n\t\t\tfolio_unlock(fbatch.folios[i]);\n\t\tfolio_batch_release(&fbatch);\n\t\tcond_resched();\n\t}\n\n\tsame_folio = (lstart >> PAGE_SHIFT) == (lend >> PAGE_SHIFT);\n\tfolio = __filemap_get_folio(mapping, lstart >> PAGE_SHIFT, FGP_LOCK, 0);\n\tif (!IS_ERR(folio)) {\n\t\tsame_folio = lend < folio_pos(folio) + folio_size(folio);\n\t\tif (!truncate_inode_partial_folio(folio, lstart, lend)) {\n\t\t\tstart = folio_next_index(folio);\n\t\t\tif (same_folio)\n\t\t\t\tend = folio->index;\n\t\t}\n\t\tfolio_unlock(folio);\n\t\tfolio_put(folio);\n\t\tfolio = NULL;\n\t}\n\n\tif (!same_folio) {\n\t\tfolio = __filemap_get_folio(mapping, lend >> PAGE_SHIFT,\n\t\t\t\t\t\tFGP_LOCK, 0);\n\t\tif (!IS_ERR(folio)) {\n\t\t\tif (!truncate_inode_partial_folio(folio, lstart, lend))\n\t\t\t\tend = folio->index;\n\t\t\tfolio_unlock(folio);\n\t\t\tfolio_put(folio);\n\t\t}\n\t}\n\n\tindex = start;\n\twhile (index < end) {\n\t\tcond_resched();\n\t\tif (!find_get_entries(mapping, &index, end - 1, &fbatch,\n\t\t\t\tindices)) {\n\t\t\t \n\t\t\tif (index == start)\n\t\t\t\tbreak;\n\t\t\t \n\t\t\tindex = start;\n\t\t\tcontinue;\n\t\t}\n\n\t\tfor (i = 0; i < folio_batch_count(&fbatch); i++) {\n\t\t\tstruct folio *folio = fbatch.folios[i];\n\n\t\t\t \n\n\t\t\tif (xa_is_value(folio))\n\t\t\t\tcontinue;\n\n\t\t\tfolio_lock(folio);\n\t\t\tVM_BUG_ON_FOLIO(!folio_contains(folio, indices[i]), folio);\n\t\t\tfolio_wait_writeback(folio);\n\t\t\ttruncate_inode_folio(mapping, folio);\n\t\t\tfolio_unlock(folio);\n\t\t}\n\t\ttruncate_folio_batch_exceptionals(mapping, &fbatch, indices);\n\t\tfolio_batch_release(&fbatch);\n\t}\n}\nEXPORT_SYMBOL(truncate_inode_pages_range);\n\n \nvoid truncate_inode_pages(struct address_space *mapping, loff_t lstart)\n{\n\ttruncate_inode_pages_range(mapping, lstart, (loff_t)-1);\n}\nEXPORT_SYMBOL(truncate_inode_pages);\n\n \nvoid truncate_inode_pages_final(struct address_space *mapping)\n{\n\t \n\tmapping_set_exiting(mapping);\n\n\tif (!mapping_empty(mapping)) {\n\t\t \n\t\txa_lock_irq(&mapping->i_pages);\n\t\txa_unlock_irq(&mapping->i_pages);\n\t}\n\n\ttruncate_inode_pages(mapping, 0);\n}\nEXPORT_SYMBOL(truncate_inode_pages_final);\n\n \nunsigned long mapping_try_invalidate(struct address_space *mapping,\n\t\tpgoff_t start, pgoff_t end, unsigned long *nr_failed)\n{\n\tpgoff_t indices[PAGEVEC_SIZE];\n\tstruct folio_batch fbatch;\n\tpgoff_t index = start;\n\tunsigned long ret;\n\tunsigned long count = 0;\n\tint i;\n\n\tfolio_batch_init(&fbatch);\n\twhile (find_lock_entries(mapping, &index, end, &fbatch, indices)) {\n\t\tfor (i = 0; i < folio_batch_count(&fbatch); i++) {\n\t\t\tstruct folio *folio = fbatch.folios[i];\n\n\t\t\t \n\n\t\t\tif (xa_is_value(folio)) {\n\t\t\t\tcount += invalidate_exceptional_entry(mapping,\n\t\t\t\t\t\t\t     indices[i], folio);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tret = mapping_evict_folio(mapping, folio);\n\t\t\tfolio_unlock(folio);\n\t\t\t \n\t\t\tif (!ret) {\n\t\t\t\tdeactivate_file_folio(folio);\n\t\t\t\t \n\t\t\t\tif (nr_failed)\n\t\t\t\t\t(*nr_failed)++;\n\t\t\t}\n\t\t\tcount += ret;\n\t\t}\n\t\tfolio_batch_remove_exceptionals(&fbatch);\n\t\tfolio_batch_release(&fbatch);\n\t\tcond_resched();\n\t}\n\treturn count;\n}\n\n \nunsigned long invalidate_mapping_pages(struct address_space *mapping,\n\t\tpgoff_t start, pgoff_t end)\n{\n\treturn mapping_try_invalidate(mapping, start, end, NULL);\n}\nEXPORT_SYMBOL(invalidate_mapping_pages);\n\n \nstatic int invalidate_complete_folio2(struct address_space *mapping,\n\t\t\t\t\tstruct folio *folio)\n{\n\tif (folio->mapping != mapping)\n\t\treturn 0;\n\n\tif (!filemap_release_folio(folio, GFP_KERNEL))\n\t\treturn 0;\n\n\tspin_lock(&mapping->host->i_lock);\n\txa_lock_irq(&mapping->i_pages);\n\tif (folio_test_dirty(folio))\n\t\tgoto failed;\n\n\tBUG_ON(folio_has_private(folio));\n\t__filemap_remove_folio(folio, NULL);\n\txa_unlock_irq(&mapping->i_pages);\n\tif (mapping_shrinkable(mapping))\n\t\tinode_add_lru(mapping->host);\n\tspin_unlock(&mapping->host->i_lock);\n\n\tfilemap_free_folio(mapping, folio);\n\treturn 1;\nfailed:\n\txa_unlock_irq(&mapping->i_pages);\n\tspin_unlock(&mapping->host->i_lock);\n\treturn 0;\n}\n\nstatic int folio_launder(struct address_space *mapping, struct folio *folio)\n{\n\tif (!folio_test_dirty(folio))\n\t\treturn 0;\n\tif (folio->mapping != mapping || mapping->a_ops->launder_folio == NULL)\n\t\treturn 0;\n\treturn mapping->a_ops->launder_folio(folio);\n}\n\n \nint invalidate_inode_pages2_range(struct address_space *mapping,\n\t\t\t\t  pgoff_t start, pgoff_t end)\n{\n\tpgoff_t indices[PAGEVEC_SIZE];\n\tstruct folio_batch fbatch;\n\tpgoff_t index;\n\tint i;\n\tint ret = 0;\n\tint ret2 = 0;\n\tint did_range_unmap = 0;\n\n\tif (mapping_empty(mapping))\n\t\treturn 0;\n\n\tfolio_batch_init(&fbatch);\n\tindex = start;\n\twhile (find_get_entries(mapping, &index, end, &fbatch, indices)) {\n\t\tfor (i = 0; i < folio_batch_count(&fbatch); i++) {\n\t\t\tstruct folio *folio = fbatch.folios[i];\n\n\t\t\t \n\n\t\t\tif (xa_is_value(folio)) {\n\t\t\t\tif (!invalidate_exceptional_entry2(mapping,\n\t\t\t\t\t\tindices[i], folio))\n\t\t\t\t\tret = -EBUSY;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (!did_range_unmap && folio_mapped(folio)) {\n\t\t\t\t \n\t\t\t\tunmap_mapping_pages(mapping, indices[i],\n\t\t\t\t\t\t(1 + end - indices[i]), false);\n\t\t\t\tdid_range_unmap = 1;\n\t\t\t}\n\n\t\t\tfolio_lock(folio);\n\t\t\tif (unlikely(folio->mapping != mapping)) {\n\t\t\t\tfolio_unlock(folio);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tVM_BUG_ON_FOLIO(!folio_contains(folio, indices[i]), folio);\n\t\t\tfolio_wait_writeback(folio);\n\n\t\t\tif (folio_mapped(folio))\n\t\t\t\tunmap_mapping_folio(folio);\n\t\t\tBUG_ON(folio_mapped(folio));\n\n\t\t\tret2 = folio_launder(mapping, folio);\n\t\t\tif (ret2 == 0) {\n\t\t\t\tif (!invalidate_complete_folio2(mapping, folio))\n\t\t\t\t\tret2 = -EBUSY;\n\t\t\t}\n\t\t\tif (ret2 < 0)\n\t\t\t\tret = ret2;\n\t\t\tfolio_unlock(folio);\n\t\t}\n\t\tfolio_batch_remove_exceptionals(&fbatch);\n\t\tfolio_batch_release(&fbatch);\n\t\tcond_resched();\n\t}\n\t \n\tif (dax_mapping(mapping)) {\n\t\tunmap_mapping_pages(mapping, start, end - start + 1, false);\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(invalidate_inode_pages2_range);\n\n \nint invalidate_inode_pages2(struct address_space *mapping)\n{\n\treturn invalidate_inode_pages2_range(mapping, 0, -1);\n}\nEXPORT_SYMBOL_GPL(invalidate_inode_pages2);\n\n \nvoid truncate_pagecache(struct inode *inode, loff_t newsize)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tloff_t holebegin = round_up(newsize, PAGE_SIZE);\n\n\t \n\tunmap_mapping_range(mapping, holebegin, 0, 1);\n\ttruncate_inode_pages(mapping, newsize);\n\tunmap_mapping_range(mapping, holebegin, 0, 1);\n}\nEXPORT_SYMBOL(truncate_pagecache);\n\n \nvoid truncate_setsize(struct inode *inode, loff_t newsize)\n{\n\tloff_t oldsize = inode->i_size;\n\n\ti_size_write(inode, newsize);\n\tif (newsize > oldsize)\n\t\tpagecache_isize_extended(inode, oldsize, newsize);\n\ttruncate_pagecache(inode, newsize);\n}\nEXPORT_SYMBOL(truncate_setsize);\n\n \nvoid pagecache_isize_extended(struct inode *inode, loff_t from, loff_t to)\n{\n\tint bsize = i_blocksize(inode);\n\tloff_t rounded_from;\n\tstruct page *page;\n\tpgoff_t index;\n\n\tWARN_ON(to > inode->i_size);\n\n\tif (from >= to || bsize == PAGE_SIZE)\n\t\treturn;\n\t \n\trounded_from = round_up(from, bsize);\n\tif (to <= rounded_from || !(rounded_from & (PAGE_SIZE - 1)))\n\t\treturn;\n\n\tindex = from >> PAGE_SHIFT;\n\tpage = find_lock_page(inode->i_mapping, index);\n\t \n\tif (!page)\n\t\treturn;\n\t \n\tif (page_mkclean(page))\n\t\tset_page_dirty(page);\n\tunlock_page(page);\n\tput_page(page);\n}\nEXPORT_SYMBOL(pagecache_isize_extended);\n\n \nvoid truncate_pagecache_range(struct inode *inode, loff_t lstart, loff_t lend)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tloff_t unmap_start = round_up(lstart, PAGE_SIZE);\n\tloff_t unmap_end = round_down(1 + lend, PAGE_SIZE) - 1;\n\t \n\n\t \n\tif ((u64)unmap_end > (u64)unmap_start)\n\t\tunmap_mapping_range(mapping, unmap_start,\n\t\t\t\t    1 + unmap_end - unmap_start, 0);\n\ttruncate_inode_pages_range(mapping, lstart, lend);\n}\nEXPORT_SYMBOL(truncate_pagecache_range);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}