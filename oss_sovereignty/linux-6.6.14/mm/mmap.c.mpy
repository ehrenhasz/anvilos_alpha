{
  "module_name": "mmap.c",
  "hash_id": "227745eef7c62d957ad87a0e9e0dd20376f0ac37a6f344371e116e49b357a138",
  "original_prompt": "Ingested from linux-6.6.14/mm/mmap.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/kernel.h>\n#include <linux/slab.h>\n#include <linux/backing-dev.h>\n#include <linux/mm.h>\n#include <linux/mm_inline.h>\n#include <linux/shm.h>\n#include <linux/mman.h>\n#include <linux/pagemap.h>\n#include <linux/swap.h>\n#include <linux/syscalls.h>\n#include <linux/capability.h>\n#include <linux/init.h>\n#include <linux/file.h>\n#include <linux/fs.h>\n#include <linux/personality.h>\n#include <linux/security.h>\n#include <linux/hugetlb.h>\n#include <linux/shmem_fs.h>\n#include <linux/profile.h>\n#include <linux/export.h>\n#include <linux/mount.h>\n#include <linux/mempolicy.h>\n#include <linux/rmap.h>\n#include <linux/mmu_notifier.h>\n#include <linux/mmdebug.h>\n#include <linux/perf_event.h>\n#include <linux/audit.h>\n#include <linux/khugepaged.h>\n#include <linux/uprobes.h>\n#include <linux/notifier.h>\n#include <linux/memory.h>\n#include <linux/printk.h>\n#include <linux/userfaultfd_k.h>\n#include <linux/moduleparam.h>\n#include <linux/pkeys.h>\n#include <linux/oom.h>\n#include <linux/sched/mm.h>\n#include <linux/ksm.h>\n\n#include <linux/uaccess.h>\n#include <asm/cacheflush.h>\n#include <asm/tlb.h>\n#include <asm/mmu_context.h>\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/mmap.h>\n\n#include \"internal.h\"\n\n#ifndef arch_mmap_check\n#define arch_mmap_check(addr, len, flags)\t(0)\n#endif\n\n#ifdef CONFIG_HAVE_ARCH_MMAP_RND_BITS\nconst int mmap_rnd_bits_min = CONFIG_ARCH_MMAP_RND_BITS_MIN;\nconst int mmap_rnd_bits_max = CONFIG_ARCH_MMAP_RND_BITS_MAX;\nint mmap_rnd_bits __read_mostly = CONFIG_ARCH_MMAP_RND_BITS;\n#endif\n#ifdef CONFIG_HAVE_ARCH_MMAP_RND_COMPAT_BITS\nconst int mmap_rnd_compat_bits_min = CONFIG_ARCH_MMAP_RND_COMPAT_BITS_MIN;\nconst int mmap_rnd_compat_bits_max = CONFIG_ARCH_MMAP_RND_COMPAT_BITS_MAX;\nint mmap_rnd_compat_bits __read_mostly = CONFIG_ARCH_MMAP_RND_COMPAT_BITS;\n#endif\n\nstatic bool ignore_rlimit_data;\ncore_param(ignore_rlimit_data, ignore_rlimit_data, bool, 0644);\n\nstatic void unmap_region(struct mm_struct *mm, struct ma_state *mas,\n\t\tstruct vm_area_struct *vma, struct vm_area_struct *prev,\n\t\tstruct vm_area_struct *next, unsigned long start,\n\t\tunsigned long end, unsigned long tree_end, bool mm_wr_locked);\n\nstatic pgprot_t vm_pgprot_modify(pgprot_t oldprot, unsigned long vm_flags)\n{\n\treturn pgprot_modify(oldprot, vm_get_page_prot(vm_flags));\n}\n\n \nvoid vma_set_page_prot(struct vm_area_struct *vma)\n{\n\tunsigned long vm_flags = vma->vm_flags;\n\tpgprot_t vm_page_prot;\n\n\tvm_page_prot = vm_pgprot_modify(vma->vm_page_prot, vm_flags);\n\tif (vma_wants_writenotify(vma, vm_page_prot)) {\n\t\tvm_flags &= ~VM_SHARED;\n\t\tvm_page_prot = vm_pgprot_modify(vm_page_prot, vm_flags);\n\t}\n\t \n\tWRITE_ONCE(vma->vm_page_prot, vm_page_prot);\n}\n\n \nstatic void __remove_shared_vm_struct(struct vm_area_struct *vma,\n\t\tstruct file *file, struct address_space *mapping)\n{\n\tif (vma->vm_flags & VM_SHARED)\n\t\tmapping_unmap_writable(mapping);\n\n\tflush_dcache_mmap_lock(mapping);\n\tvma_interval_tree_remove(vma, &mapping->i_mmap);\n\tflush_dcache_mmap_unlock(mapping);\n}\n\n \nvoid unlink_file_vma(struct vm_area_struct *vma)\n{\n\tstruct file *file = vma->vm_file;\n\n\tif (file) {\n\t\tstruct address_space *mapping = file->f_mapping;\n\t\ti_mmap_lock_write(mapping);\n\t\t__remove_shared_vm_struct(vma, file, mapping);\n\t\ti_mmap_unlock_write(mapping);\n\t}\n}\n\n \nstatic void remove_vma(struct vm_area_struct *vma, bool unreachable)\n{\n\tmight_sleep();\n\tif (vma->vm_ops && vma->vm_ops->close)\n\t\tvma->vm_ops->close(vma);\n\tif (vma->vm_file)\n\t\tfput(vma->vm_file);\n\tmpol_put(vma_policy(vma));\n\tif (unreachable)\n\t\t__vm_area_free(vma);\n\telse\n\t\tvm_area_free(vma);\n}\n\nstatic inline struct vm_area_struct *vma_prev_limit(struct vma_iterator *vmi,\n\t\t\t\t\t\t    unsigned long min)\n{\n\treturn mas_prev(&vmi->mas, min);\n}\n\n \nstatic int check_brk_limits(unsigned long addr, unsigned long len)\n{\n\tunsigned long mapped_addr;\n\n\tmapped_addr = get_unmapped_area(NULL, addr, len, 0, MAP_FIXED);\n\tif (IS_ERR_VALUE(mapped_addr))\n\t\treturn mapped_addr;\n\n\treturn mlock_future_ok(current->mm, current->mm->def_flags, len)\n\t\t? 0 : -EAGAIN;\n}\nstatic int do_brk_flags(struct vma_iterator *vmi, struct vm_area_struct *brkvma,\n\t\tunsigned long addr, unsigned long request, unsigned long flags);\nSYSCALL_DEFINE1(brk, unsigned long, brk)\n{\n\tunsigned long newbrk, oldbrk, origbrk;\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *brkvma, *next = NULL;\n\tunsigned long min_brk;\n\tbool populate = false;\n\tLIST_HEAD(uf);\n\tstruct vma_iterator vmi;\n\n\tif (mmap_write_lock_killable(mm))\n\t\treturn -EINTR;\n\n\torigbrk = mm->brk;\n\n#ifdef CONFIG_COMPAT_BRK\n\t \n\tif (current->brk_randomized)\n\t\tmin_brk = mm->start_brk;\n\telse\n\t\tmin_brk = mm->end_data;\n#else\n\tmin_brk = mm->start_brk;\n#endif\n\tif (brk < min_brk)\n\t\tgoto out;\n\n\t \n\tif (check_data_rlimit(rlimit(RLIMIT_DATA), brk, mm->start_brk,\n\t\t\t      mm->end_data, mm->start_data))\n\t\tgoto out;\n\n\tnewbrk = PAGE_ALIGN(brk);\n\toldbrk = PAGE_ALIGN(mm->brk);\n\tif (oldbrk == newbrk) {\n\t\tmm->brk = brk;\n\t\tgoto success;\n\t}\n\n\t \n\tif (brk <= mm->brk) {\n\t\t \n\t\tvma_iter_init(&vmi, mm, newbrk);\n\t\tbrkvma = vma_find(&vmi, oldbrk);\n\t\tif (!brkvma || brkvma->vm_start >= oldbrk)\n\t\t\tgoto out;  \n\t\t \n\t\tmm->brk = brk;\n\t\tif (do_vma_munmap(&vmi, brkvma, newbrk, oldbrk, &uf, true))\n\t\t\tgoto out;\n\n\t\tgoto success_unlocked;\n\t}\n\n\tif (check_brk_limits(oldbrk, newbrk - oldbrk))\n\t\tgoto out;\n\n\t \n\tvma_iter_init(&vmi, mm, oldbrk);\n\tnext = vma_find(&vmi, newbrk + PAGE_SIZE + stack_guard_gap);\n\tif (next && newbrk + PAGE_SIZE > vm_start_gap(next))\n\t\tgoto out;\n\n\tbrkvma = vma_prev_limit(&vmi, mm->start_brk);\n\t \n\tif (do_brk_flags(&vmi, brkvma, oldbrk, newbrk - oldbrk, 0) < 0)\n\t\tgoto out;\n\n\tmm->brk = brk;\n\tif (mm->def_flags & VM_LOCKED)\n\t\tpopulate = true;\n\nsuccess:\n\tmmap_write_unlock(mm);\nsuccess_unlocked:\n\tuserfaultfd_unmap_complete(mm, &uf);\n\tif (populate)\n\t\tmm_populate(oldbrk, newbrk - oldbrk);\n\treturn brk;\n\nout:\n\tmm->brk = origbrk;\n\tmmap_write_unlock(mm);\n\treturn origbrk;\n}\n\n#if defined(CONFIG_DEBUG_VM_MAPLE_TREE)\nstatic void validate_mm(struct mm_struct *mm)\n{\n\tint bug = 0;\n\tint i = 0;\n\tstruct vm_area_struct *vma;\n\tVMA_ITERATOR(vmi, mm, 0);\n\n\tmt_validate(&mm->mm_mt);\n\tfor_each_vma(vmi, vma) {\n#ifdef CONFIG_DEBUG_VM_RB\n\t\tstruct anon_vma *anon_vma = vma->anon_vma;\n\t\tstruct anon_vma_chain *avc;\n#endif\n\t\tunsigned long vmi_start, vmi_end;\n\t\tbool warn = 0;\n\n\t\tvmi_start = vma_iter_addr(&vmi);\n\t\tvmi_end = vma_iter_end(&vmi);\n\t\tif (VM_WARN_ON_ONCE_MM(vma->vm_end != vmi_end, mm))\n\t\t\twarn = 1;\n\n\t\tif (VM_WARN_ON_ONCE_MM(vma->vm_start != vmi_start, mm))\n\t\t\twarn = 1;\n\n\t\tif (warn) {\n\t\t\tpr_emerg(\"issue in %s\\n\", current->comm);\n\t\t\tdump_stack();\n\t\t\tdump_vma(vma);\n\t\t\tpr_emerg(\"tree range: %px start %lx end %lx\\n\", vma,\n\t\t\t\t vmi_start, vmi_end - 1);\n\t\t\tvma_iter_dump_tree(&vmi);\n\t\t}\n\n#ifdef CONFIG_DEBUG_VM_RB\n\t\tif (anon_vma) {\n\t\t\tanon_vma_lock_read(anon_vma);\n\t\t\tlist_for_each_entry(avc, &vma->anon_vma_chain, same_vma)\n\t\t\t\tanon_vma_interval_tree_verify(avc);\n\t\t\tanon_vma_unlock_read(anon_vma);\n\t\t}\n#endif\n\t\ti++;\n\t}\n\tif (i != mm->map_count) {\n\t\tpr_emerg(\"map_count %d vma iterator %d\\n\", mm->map_count, i);\n\t\tbug = 1;\n\t}\n\tVM_BUG_ON_MM(bug, mm);\n}\n\n#else  \n#define validate_mm(mm) do { } while (0)\n#endif  \n\n \nstatic inline void\nanon_vma_interval_tree_pre_update_vma(struct vm_area_struct *vma)\n{\n\tstruct anon_vma_chain *avc;\n\n\tlist_for_each_entry(avc, &vma->anon_vma_chain, same_vma)\n\t\tanon_vma_interval_tree_remove(avc, &avc->anon_vma->rb_root);\n}\n\nstatic inline void\nanon_vma_interval_tree_post_update_vma(struct vm_area_struct *vma)\n{\n\tstruct anon_vma_chain *avc;\n\n\tlist_for_each_entry(avc, &vma->anon_vma_chain, same_vma)\n\t\tanon_vma_interval_tree_insert(avc, &avc->anon_vma->rb_root);\n}\n\nstatic unsigned long count_vma_pages_range(struct mm_struct *mm,\n\t\tunsigned long addr, unsigned long end)\n{\n\tVMA_ITERATOR(vmi, mm, addr);\n\tstruct vm_area_struct *vma;\n\tunsigned long nr_pages = 0;\n\n\tfor_each_vma_range(vmi, vma, end) {\n\t\tunsigned long vm_start = max(addr, vma->vm_start);\n\t\tunsigned long vm_end = min(end, vma->vm_end);\n\n\t\tnr_pages += PHYS_PFN(vm_end - vm_start);\n\t}\n\n\treturn nr_pages;\n}\n\nstatic void __vma_link_file(struct vm_area_struct *vma,\n\t\t\t    struct address_space *mapping)\n{\n\tif (vma->vm_flags & VM_SHARED)\n\t\tmapping_allow_writable(mapping);\n\n\tflush_dcache_mmap_lock(mapping);\n\tvma_interval_tree_insert(vma, &mapping->i_mmap);\n\tflush_dcache_mmap_unlock(mapping);\n}\n\nstatic int vma_link(struct mm_struct *mm, struct vm_area_struct *vma)\n{\n\tVMA_ITERATOR(vmi, mm, 0);\n\tstruct address_space *mapping = NULL;\n\n\tvma_iter_config(&vmi, vma->vm_start, vma->vm_end);\n\tif (vma_iter_prealloc(&vmi, vma))\n\t\treturn -ENOMEM;\n\n\tvma_start_write(vma);\n\n\tvma_iter_store(&vmi, vma);\n\n\tif (vma->vm_file) {\n\t\tmapping = vma->vm_file->f_mapping;\n\t\ti_mmap_lock_write(mapping);\n\t\t__vma_link_file(vma, mapping);\n\t\ti_mmap_unlock_write(mapping);\n\t}\n\n\tmm->map_count++;\n\tvalidate_mm(mm);\n\treturn 0;\n}\n\n \nstatic inline void init_multi_vma_prep(struct vma_prepare *vp,\n\t\tstruct vm_area_struct *vma, struct vm_area_struct *next,\n\t\tstruct vm_area_struct *remove, struct vm_area_struct *remove2)\n{\n\tmemset(vp, 0, sizeof(struct vma_prepare));\n\tvp->vma = vma;\n\tvp->anon_vma = vma->anon_vma;\n\tvp->remove = remove;\n\tvp->remove2 = remove2;\n\tvp->adj_next = next;\n\tif (!vp->anon_vma && next)\n\t\tvp->anon_vma = next->anon_vma;\n\n\tvp->file = vma->vm_file;\n\tif (vp->file)\n\t\tvp->mapping = vma->vm_file->f_mapping;\n\n}\n\n \nstatic inline void init_vma_prep(struct vma_prepare *vp,\n\t\t\t\t struct vm_area_struct *vma)\n{\n\tinit_multi_vma_prep(vp, vma, NULL, NULL, NULL);\n}\n\n\n \nstatic inline void vma_prepare(struct vma_prepare *vp)\n{\n\tif (vp->file) {\n\t\tuprobe_munmap(vp->vma, vp->vma->vm_start, vp->vma->vm_end);\n\n\t\tif (vp->adj_next)\n\t\t\tuprobe_munmap(vp->adj_next, vp->adj_next->vm_start,\n\t\t\t\t      vp->adj_next->vm_end);\n\n\t\ti_mmap_lock_write(vp->mapping);\n\t\tif (vp->insert && vp->insert->vm_file) {\n\t\t\t \n\t\t\t__vma_link_file(vp->insert,\n\t\t\t\t\tvp->insert->vm_file->f_mapping);\n\t\t}\n\t}\n\n\tif (vp->anon_vma) {\n\t\tanon_vma_lock_write(vp->anon_vma);\n\t\tanon_vma_interval_tree_pre_update_vma(vp->vma);\n\t\tif (vp->adj_next)\n\t\t\tanon_vma_interval_tree_pre_update_vma(vp->adj_next);\n\t}\n\n\tif (vp->file) {\n\t\tflush_dcache_mmap_lock(vp->mapping);\n\t\tvma_interval_tree_remove(vp->vma, &vp->mapping->i_mmap);\n\t\tif (vp->adj_next)\n\t\t\tvma_interval_tree_remove(vp->adj_next,\n\t\t\t\t\t\t &vp->mapping->i_mmap);\n\t}\n\n}\n\n \nstatic inline void vma_complete(struct vma_prepare *vp,\n\t\t\t\tstruct vma_iterator *vmi, struct mm_struct *mm)\n{\n\tif (vp->file) {\n\t\tif (vp->adj_next)\n\t\t\tvma_interval_tree_insert(vp->adj_next,\n\t\t\t\t\t\t &vp->mapping->i_mmap);\n\t\tvma_interval_tree_insert(vp->vma, &vp->mapping->i_mmap);\n\t\tflush_dcache_mmap_unlock(vp->mapping);\n\t}\n\n\tif (vp->remove && vp->file) {\n\t\t__remove_shared_vm_struct(vp->remove, vp->file, vp->mapping);\n\t\tif (vp->remove2)\n\t\t\t__remove_shared_vm_struct(vp->remove2, vp->file,\n\t\t\t\t\t\t  vp->mapping);\n\t} else if (vp->insert) {\n\t\t \n\t\tvma_iter_store(vmi, vp->insert);\n\t\tmm->map_count++;\n\t}\n\n\tif (vp->anon_vma) {\n\t\tanon_vma_interval_tree_post_update_vma(vp->vma);\n\t\tif (vp->adj_next)\n\t\t\tanon_vma_interval_tree_post_update_vma(vp->adj_next);\n\t\tanon_vma_unlock_write(vp->anon_vma);\n\t}\n\n\tif (vp->file) {\n\t\ti_mmap_unlock_write(vp->mapping);\n\t\tuprobe_mmap(vp->vma);\n\n\t\tif (vp->adj_next)\n\t\t\tuprobe_mmap(vp->adj_next);\n\t}\n\n\tif (vp->remove) {\nagain:\n\t\tvma_mark_detached(vp->remove, true);\n\t\tif (vp->file) {\n\t\t\tuprobe_munmap(vp->remove, vp->remove->vm_start,\n\t\t\t\t      vp->remove->vm_end);\n\t\t\tfput(vp->file);\n\t\t}\n\t\tif (vp->remove->anon_vma)\n\t\t\tanon_vma_merge(vp->vma, vp->remove);\n\t\tmm->map_count--;\n\t\tmpol_put(vma_policy(vp->remove));\n\t\tif (!vp->remove2)\n\t\t\tWARN_ON_ONCE(vp->vma->vm_end < vp->remove->vm_end);\n\t\tvm_area_free(vp->remove);\n\n\t\t \n\t\tif (vp->remove2) {\n\t\t\tvp->remove = vp->remove2;\n\t\t\tvp->remove2 = NULL;\n\t\t\tgoto again;\n\t\t}\n\t}\n\tif (vp->insert && vp->file)\n\t\tuprobe_mmap(vp->insert);\n\tvalidate_mm(mm);\n}\n\n \nstatic inline int dup_anon_vma(struct vm_area_struct *dst,\n\t\tstruct vm_area_struct *src, struct vm_area_struct **dup)\n{\n\t \n\tif (src->anon_vma && !dst->anon_vma) {\n\t\tint ret;\n\n\t\tvma_assert_write_locked(dst);\n\t\tdst->anon_vma = src->anon_vma;\n\t\tret = anon_vma_clone(dst, src);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\t*dup = dst;\n\t}\n\n\treturn 0;\n}\n\n \nint vma_expand(struct vma_iterator *vmi, struct vm_area_struct *vma,\n\t       unsigned long start, unsigned long end, pgoff_t pgoff,\n\t       struct vm_area_struct *next)\n{\n\tstruct vm_area_struct *anon_dup = NULL;\n\tbool remove_next = false;\n\tstruct vma_prepare vp;\n\n\tvma_start_write(vma);\n\tif (next && (vma != next) && (end == next->vm_end)) {\n\t\tint ret;\n\n\t\tremove_next = true;\n\t\tvma_start_write(next);\n\t\tret = dup_anon_vma(vma, next, &anon_dup);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tinit_multi_vma_prep(&vp, vma, NULL, remove_next ? next : NULL, NULL);\n\t \n\tVM_WARN_ON(next && !vp.remove &&\n\t\t  next != vma && end > next->vm_start);\n\t \n\tVM_WARN_ON(vma->vm_start < start || vma->vm_end > end);\n\n\t \n\tvma_iter_config(vmi, start, end);\n\tif (vma_iter_prealloc(vmi, vma))\n\t\tgoto nomem;\n\n\tvma_prepare(&vp);\n\tvma_adjust_trans_huge(vma, start, end, 0);\n\tvma->vm_start = start;\n\tvma->vm_end = end;\n\tvma->vm_pgoff = pgoff;\n\tvma_iter_store(vmi, vma);\n\n\tvma_complete(&vp, vmi, vma->vm_mm);\n\treturn 0;\n\nnomem:\n\tif (anon_dup)\n\t\tunlink_anon_vmas(anon_dup);\n\treturn -ENOMEM;\n}\n\n \nint vma_shrink(struct vma_iterator *vmi, struct vm_area_struct *vma,\n\t       unsigned long start, unsigned long end, pgoff_t pgoff)\n{\n\tstruct vma_prepare vp;\n\n\tWARN_ON((vma->vm_start != start) && (vma->vm_end != end));\n\n\tif (vma->vm_start < start)\n\t\tvma_iter_config(vmi, vma->vm_start, start);\n\telse\n\t\tvma_iter_config(vmi, end, vma->vm_end);\n\n\tif (vma_iter_prealloc(vmi, NULL))\n\t\treturn -ENOMEM;\n\n\tvma_start_write(vma);\n\n\tinit_vma_prep(&vp, vma);\n\tvma_prepare(&vp);\n\tvma_adjust_trans_huge(vma, start, end, 0);\n\n\tvma_iter_clear(vmi);\n\tvma->vm_start = start;\n\tvma->vm_end = end;\n\tvma->vm_pgoff = pgoff;\n\tvma_complete(&vp, vmi, vma->vm_mm);\n\treturn 0;\n}\n\n \nstatic inline bool is_mergeable_vma(struct vm_area_struct *vma,\n\t\tstruct file *file, unsigned long vm_flags,\n\t\tstruct vm_userfaultfd_ctx vm_userfaultfd_ctx,\n\t\tstruct anon_vma_name *anon_name, bool may_remove_vma)\n{\n\t \n\tif ((vma->vm_flags ^ vm_flags) & ~VM_SOFTDIRTY)\n\t\treturn false;\n\tif (vma->vm_file != file)\n\t\treturn false;\n\tif (may_remove_vma && vma->vm_ops && vma->vm_ops->close)\n\t\treturn false;\n\tif (!is_mergeable_vm_userfaultfd_ctx(vma, vm_userfaultfd_ctx))\n\t\treturn false;\n\tif (!anon_vma_name_eq(anon_vma_name(vma), anon_name))\n\t\treturn false;\n\treturn true;\n}\n\nstatic inline bool is_mergeable_anon_vma(struct anon_vma *anon_vma1,\n\t\t struct anon_vma *anon_vma2, struct vm_area_struct *vma)\n{\n\t \n\tif ((!anon_vma1 || !anon_vma2) && (!vma ||\n\t\tlist_is_singular(&vma->anon_vma_chain)))\n\t\treturn true;\n\treturn anon_vma1 == anon_vma2;\n}\n\n \nstatic bool\ncan_vma_merge_before(struct vm_area_struct *vma, unsigned long vm_flags,\n\t\tstruct anon_vma *anon_vma, struct file *file,\n\t\tpgoff_t vm_pgoff, struct vm_userfaultfd_ctx vm_userfaultfd_ctx,\n\t\tstruct anon_vma_name *anon_name)\n{\n\tif (is_mergeable_vma(vma, file, vm_flags, vm_userfaultfd_ctx, anon_name, true) &&\n\t    is_mergeable_anon_vma(anon_vma, vma->anon_vma, vma)) {\n\t\tif (vma->vm_pgoff == vm_pgoff)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\n \nstatic bool\ncan_vma_merge_after(struct vm_area_struct *vma, unsigned long vm_flags,\n\t\tstruct anon_vma *anon_vma, struct file *file,\n\t\tpgoff_t vm_pgoff, struct vm_userfaultfd_ctx vm_userfaultfd_ctx,\n\t\tstruct anon_vma_name *anon_name)\n{\n\tif (is_mergeable_vma(vma, file, vm_flags, vm_userfaultfd_ctx, anon_name, false) &&\n\t    is_mergeable_anon_vma(anon_vma, vma->anon_vma, vma)) {\n\t\tpgoff_t vm_pglen;\n\t\tvm_pglen = vma_pages(vma);\n\t\tif (vma->vm_pgoff + vm_pglen == vm_pgoff)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\n \nstruct vm_area_struct *vma_merge(struct vma_iterator *vmi, struct mm_struct *mm,\n\t\t\tstruct vm_area_struct *prev, unsigned long addr,\n\t\t\tunsigned long end, unsigned long vm_flags,\n\t\t\tstruct anon_vma *anon_vma, struct file *file,\n\t\t\tpgoff_t pgoff, struct mempolicy *policy,\n\t\t\tstruct vm_userfaultfd_ctx vm_userfaultfd_ctx,\n\t\t\tstruct anon_vma_name *anon_name)\n{\n\tstruct vm_area_struct *curr, *next, *res;\n\tstruct vm_area_struct *vma, *adjust, *remove, *remove2;\n\tstruct vm_area_struct *anon_dup = NULL;\n\tstruct vma_prepare vp;\n\tpgoff_t vma_pgoff;\n\tint err = 0;\n\tbool merge_prev = false;\n\tbool merge_next = false;\n\tbool vma_expanded = false;\n\tunsigned long vma_start = addr;\n\tunsigned long vma_end = end;\n\tpgoff_t pglen = (end - addr) >> PAGE_SHIFT;\n\tlong adj_start = 0;\n\n\t \n\tif (vm_flags & VM_SPECIAL)\n\t\treturn NULL;\n\n\t \n\tcurr = find_vma_intersection(mm, prev ? prev->vm_end : 0, end);\n\n\tif (!curr ||\t\t\t \n\t    end == curr->vm_end)\t \n\t\tnext = vma_lookup(mm, end);\n\telse\n\t\tnext = NULL;\t\t \n\n\tif (prev) {\n\t\tvma_start = prev->vm_start;\n\t\tvma_pgoff = prev->vm_pgoff;\n\n\t\t \n\t\tif (addr == prev->vm_end && mpol_equal(vma_policy(prev), policy)\n\t\t    && can_vma_merge_after(prev, vm_flags, anon_vma, file,\n\t\t\t\t\t   pgoff, vm_userfaultfd_ctx, anon_name)) {\n\t\t\tmerge_prev = true;\n\t\t\tvma_prev(vmi);\n\t\t}\n\t}\n\n\t \n\tif (next && mpol_equal(policy, vma_policy(next)) &&\n\t    can_vma_merge_before(next, vm_flags, anon_vma, file, pgoff+pglen,\n\t\t\t\t vm_userfaultfd_ctx, anon_name)) {\n\t\tmerge_next = true;\n\t}\n\n\t \n\tVM_WARN_ON(prev && addr <= prev->vm_start);\n\tVM_WARN_ON(curr && (addr != curr->vm_start || end > curr->vm_end));\n\tVM_WARN_ON(addr >= end);\n\n\tif (!merge_prev && !merge_next)\n\t\treturn NULL;  \n\n\tif (merge_prev)\n\t\tvma_start_write(prev);\n\n\tres = vma = prev;\n\tremove = remove2 = adjust = NULL;\n\n\t \n\tif (merge_prev && merge_next &&\n\t    is_mergeable_anon_vma(prev->anon_vma, next->anon_vma, NULL)) {\n\t\tvma_start_write(next);\n\t\tremove = next;\t\t\t\t \n\t\tvma_end = next->vm_end;\n\t\terr = dup_anon_vma(prev, next, &anon_dup);\n\t\tif (curr) {\t\t\t\t \n\t\t\tvma_start_write(curr);\n\t\t\tremove = curr;\n\t\t\tremove2 = next;\n\t\t\tif (!next->anon_vma)\n\t\t\t\terr = dup_anon_vma(prev, curr, &anon_dup);\n\t\t}\n\t} else if (merge_prev) {\t\t\t \n\t\tif (curr) {\n\t\t\tvma_start_write(curr);\n\t\t\terr = dup_anon_vma(prev, curr, &anon_dup);\n\t\t\tif (end == curr->vm_end) {\t \n\t\t\t\tremove = curr;\n\t\t\t} else {\t\t\t \n\t\t\t\tadjust = curr;\n\t\t\t\tadj_start = (end - curr->vm_start);\n\t\t\t}\n\t\t}\n\t} else {  \n\t\tvma_start_write(next);\n\t\tres = next;\n\t\tif (prev && addr < prev->vm_end) {\t \n\t\t\tvma_start_write(prev);\n\t\t\tvma_end = addr;\n\t\t\tadjust = next;\n\t\t\tadj_start = -(prev->vm_end - addr);\n\t\t\terr = dup_anon_vma(next, prev, &anon_dup);\n\t\t} else {\n\t\t\t \n\t\t\tvma = next;\t\t\t \n\t\t\tvma_start = addr;\n\t\t\tvma_end = next->vm_end;\n\t\t\tvma_pgoff = next->vm_pgoff - pglen;\n\t\t\tif (curr) {\t\t\t \n\t\t\t\tvma_pgoff = curr->vm_pgoff;\n\t\t\t\tvma_start_write(curr);\n\t\t\t\tremove = curr;\n\t\t\t\terr = dup_anon_vma(next, curr, &anon_dup);\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tif (err)\n\t\tgoto anon_vma_fail;\n\n\tif (vma_start < vma->vm_start || vma_end > vma->vm_end)\n\t\tvma_expanded = true;\n\n\tif (vma_expanded) {\n\t\tvma_iter_config(vmi, vma_start, vma_end);\n\t} else {\n\t\tvma_iter_config(vmi, adjust->vm_start + adj_start,\n\t\t\t\tadjust->vm_end);\n\t}\n\n\tif (vma_iter_prealloc(vmi, vma))\n\t\tgoto prealloc_fail;\n\n\tinit_multi_vma_prep(&vp, vma, adjust, remove, remove2);\n\tVM_WARN_ON(vp.anon_vma && adjust && adjust->anon_vma &&\n\t\t   vp.anon_vma != adjust->anon_vma);\n\n\tvma_prepare(&vp);\n\tvma_adjust_trans_huge(vma, vma_start, vma_end, adj_start);\n\n\tvma->vm_start = vma_start;\n\tvma->vm_end = vma_end;\n\tvma->vm_pgoff = vma_pgoff;\n\n\tif (vma_expanded)\n\t\tvma_iter_store(vmi, vma);\n\n\tif (adj_start) {\n\t\tadjust->vm_start += adj_start;\n\t\tadjust->vm_pgoff += adj_start >> PAGE_SHIFT;\n\t\tif (adj_start < 0) {\n\t\t\tWARN_ON(vma_expanded);\n\t\t\tvma_iter_store(vmi, next);\n\t\t}\n\t}\n\n\tvma_complete(&vp, vmi, mm);\n\tkhugepaged_enter_vma(res, vm_flags);\n\treturn res;\n\nprealloc_fail:\n\tif (anon_dup)\n\t\tunlink_anon_vmas(anon_dup);\n\nanon_vma_fail:\n\tvma_iter_set(vmi, addr);\n\tvma_iter_load(vmi);\n\treturn NULL;\n}\n\n \nstatic int anon_vma_compatible(struct vm_area_struct *a, struct vm_area_struct *b)\n{\n\treturn a->vm_end == b->vm_start &&\n\t\tmpol_equal(vma_policy(a), vma_policy(b)) &&\n\t\ta->vm_file == b->vm_file &&\n\t\t!((a->vm_flags ^ b->vm_flags) & ~(VM_ACCESS_FLAGS | VM_SOFTDIRTY)) &&\n\t\tb->vm_pgoff == a->vm_pgoff + ((b->vm_start - a->vm_start) >> PAGE_SHIFT);\n}\n\n \nstatic struct anon_vma *reusable_anon_vma(struct vm_area_struct *old, struct vm_area_struct *a, struct vm_area_struct *b)\n{\n\tif (anon_vma_compatible(a, b)) {\n\t\tstruct anon_vma *anon_vma = READ_ONCE(old->anon_vma);\n\n\t\tif (anon_vma && list_is_singular(&old->anon_vma_chain))\n\t\t\treturn anon_vma;\n\t}\n\treturn NULL;\n}\n\n \nstruct anon_vma *find_mergeable_anon_vma(struct vm_area_struct *vma)\n{\n\tMA_STATE(mas, &vma->vm_mm->mm_mt, vma->vm_end, vma->vm_end);\n\tstruct anon_vma *anon_vma = NULL;\n\tstruct vm_area_struct *prev, *next;\n\n\t \n\tnext = mas_walk(&mas);\n\tif (next) {\n\t\tanon_vma = reusable_anon_vma(next, vma, next);\n\t\tif (anon_vma)\n\t\t\treturn anon_vma;\n\t}\n\n\tprev = mas_prev(&mas, 0);\n\tVM_BUG_ON_VMA(prev != vma, vma);\n\tprev = mas_prev(&mas, 0);\n\t \n\tif (prev)\n\t\tanon_vma = reusable_anon_vma(prev, prev, vma);\n\n\t \n\treturn anon_vma;\n}\n\n \nstatic inline unsigned long round_hint_to_min(unsigned long hint)\n{\n\thint &= PAGE_MASK;\n\tif (((void *)hint != NULL) &&\n\t    (hint < mmap_min_addr))\n\t\treturn PAGE_ALIGN(mmap_min_addr);\n\treturn hint;\n}\n\nbool mlock_future_ok(struct mm_struct *mm, unsigned long flags,\n\t\t\tunsigned long bytes)\n{\n\tunsigned long locked_pages, limit_pages;\n\n\tif (!(flags & VM_LOCKED) || capable(CAP_IPC_LOCK))\n\t\treturn true;\n\n\tlocked_pages = bytes >> PAGE_SHIFT;\n\tlocked_pages += mm->locked_vm;\n\n\tlimit_pages = rlimit(RLIMIT_MEMLOCK);\n\tlimit_pages >>= PAGE_SHIFT;\n\n\treturn locked_pages <= limit_pages;\n}\n\nstatic inline u64 file_mmap_size_max(struct file *file, struct inode *inode)\n{\n\tif (S_ISREG(inode->i_mode))\n\t\treturn MAX_LFS_FILESIZE;\n\n\tif (S_ISBLK(inode->i_mode))\n\t\treturn MAX_LFS_FILESIZE;\n\n\tif (S_ISSOCK(inode->i_mode))\n\t\treturn MAX_LFS_FILESIZE;\n\n\t \n\tif (file->f_mode & FMODE_UNSIGNED_OFFSET)\n\t\treturn 0;\n\n\t \n\treturn ULONG_MAX;\n}\n\nstatic inline bool file_mmap_ok(struct file *file, struct inode *inode,\n\t\t\t\tunsigned long pgoff, unsigned long len)\n{\n\tu64 maxsize = file_mmap_size_max(file, inode);\n\n\tif (maxsize && len > maxsize)\n\t\treturn false;\n\tmaxsize -= len;\n\tif (pgoff > maxsize >> PAGE_SHIFT)\n\t\treturn false;\n\treturn true;\n}\n\n \nunsigned long do_mmap(struct file *file, unsigned long addr,\n\t\t\tunsigned long len, unsigned long prot,\n\t\t\tunsigned long flags, vm_flags_t vm_flags,\n\t\t\tunsigned long pgoff, unsigned long *populate,\n\t\t\tstruct list_head *uf)\n{\n\tstruct mm_struct *mm = current->mm;\n\tint pkey = 0;\n\n\t*populate = 0;\n\n\tif (!len)\n\t\treturn -EINVAL;\n\n\t \n\tif ((prot & PROT_READ) && (current->personality & READ_IMPLIES_EXEC))\n\t\tif (!(file && path_noexec(&file->f_path)))\n\t\t\tprot |= PROT_EXEC;\n\n\t \n\tif (flags & MAP_FIXED_NOREPLACE)\n\t\tflags |= MAP_FIXED;\n\n\tif (!(flags & MAP_FIXED))\n\t\taddr = round_hint_to_min(addr);\n\n\t \n\tlen = PAGE_ALIGN(len);\n\tif (!len)\n\t\treturn -ENOMEM;\n\n\t \n\tif ((pgoff + (len >> PAGE_SHIFT)) < pgoff)\n\t\treturn -EOVERFLOW;\n\n\t \n\tif (mm->map_count > sysctl_max_map_count)\n\t\treturn -ENOMEM;\n\n\t \n\taddr = get_unmapped_area(file, addr, len, pgoff, flags);\n\tif (IS_ERR_VALUE(addr))\n\t\treturn addr;\n\n\tif (flags & MAP_FIXED_NOREPLACE) {\n\t\tif (find_vma_intersection(mm, addr, addr + len))\n\t\t\treturn -EEXIST;\n\t}\n\n\tif (prot == PROT_EXEC) {\n\t\tpkey = execute_only_pkey(mm);\n\t\tif (pkey < 0)\n\t\t\tpkey = 0;\n\t}\n\n\t \n\tvm_flags |= calc_vm_prot_bits(prot, pkey) | calc_vm_flag_bits(flags) |\n\t\t\tmm->def_flags | VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC;\n\n\tif (flags & MAP_LOCKED)\n\t\tif (!can_do_mlock())\n\t\t\treturn -EPERM;\n\n\tif (!mlock_future_ok(mm, vm_flags, len))\n\t\treturn -EAGAIN;\n\n\tif (file) {\n\t\tstruct inode *inode = file_inode(file);\n\t\tunsigned long flags_mask;\n\n\t\tif (!file_mmap_ok(file, inode, pgoff, len))\n\t\t\treturn -EOVERFLOW;\n\n\t\tflags_mask = LEGACY_MAP_MASK | file->f_op->mmap_supported_flags;\n\n\t\tswitch (flags & MAP_TYPE) {\n\t\tcase MAP_SHARED:\n\t\t\t \n\t\t\tflags &= LEGACY_MAP_MASK;\n\t\t\tfallthrough;\n\t\tcase MAP_SHARED_VALIDATE:\n\t\t\tif (flags & ~flags_mask)\n\t\t\t\treturn -EOPNOTSUPP;\n\t\t\tif (prot & PROT_WRITE) {\n\t\t\t\tif (!(file->f_mode & FMODE_WRITE))\n\t\t\t\t\treturn -EACCES;\n\t\t\t\tif (IS_SWAPFILE(file->f_mapping->host))\n\t\t\t\t\treturn -ETXTBSY;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (IS_APPEND(inode) && (file->f_mode & FMODE_WRITE))\n\t\t\t\treturn -EACCES;\n\n\t\t\tvm_flags |= VM_SHARED | VM_MAYSHARE;\n\t\t\tif (!(file->f_mode & FMODE_WRITE))\n\t\t\t\tvm_flags &= ~(VM_MAYWRITE | VM_SHARED);\n\t\t\tfallthrough;\n\t\tcase MAP_PRIVATE:\n\t\t\tif (!(file->f_mode & FMODE_READ))\n\t\t\t\treturn -EACCES;\n\t\t\tif (path_noexec(&file->f_path)) {\n\t\t\t\tif (vm_flags & VM_EXEC)\n\t\t\t\t\treturn -EPERM;\n\t\t\t\tvm_flags &= ~VM_MAYEXEC;\n\t\t\t}\n\n\t\t\tif (!file->f_op->mmap)\n\t\t\t\treturn -ENODEV;\n\t\t\tif (vm_flags & (VM_GROWSDOWN|VM_GROWSUP))\n\t\t\t\treturn -EINVAL;\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else {\n\t\tswitch (flags & MAP_TYPE) {\n\t\tcase MAP_SHARED:\n\t\t\tif (vm_flags & (VM_GROWSDOWN|VM_GROWSUP))\n\t\t\t\treturn -EINVAL;\n\t\t\t \n\t\t\tpgoff = 0;\n\t\t\tvm_flags |= VM_SHARED | VM_MAYSHARE;\n\t\t\tbreak;\n\t\tcase MAP_PRIVATE:\n\t\t\t \n\t\t\tpgoff = addr >> PAGE_SHIFT;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t \n\tif (flags & MAP_NORESERVE) {\n\t\t \n\t\tif (sysctl_overcommit_memory != OVERCOMMIT_NEVER)\n\t\t\tvm_flags |= VM_NORESERVE;\n\n\t\t \n\t\tif (file && is_file_hugepages(file))\n\t\t\tvm_flags |= VM_NORESERVE;\n\t}\n\n\taddr = mmap_region(file, addr, len, vm_flags, pgoff, uf);\n\tif (!IS_ERR_VALUE(addr) &&\n\t    ((vm_flags & VM_LOCKED) ||\n\t     (flags & (MAP_POPULATE | MAP_NONBLOCK)) == MAP_POPULATE))\n\t\t*populate = len;\n\treturn addr;\n}\n\nunsigned long ksys_mmap_pgoff(unsigned long addr, unsigned long len,\n\t\t\t      unsigned long prot, unsigned long flags,\n\t\t\t      unsigned long fd, unsigned long pgoff)\n{\n\tstruct file *file = NULL;\n\tunsigned long retval;\n\n\tif (!(flags & MAP_ANONYMOUS)) {\n\t\taudit_mmap_fd(fd, flags);\n\t\tfile = fget(fd);\n\t\tif (!file)\n\t\t\treturn -EBADF;\n\t\tif (is_file_hugepages(file)) {\n\t\t\tlen = ALIGN(len, huge_page_size(hstate_file(file)));\n\t\t} else if (unlikely(flags & MAP_HUGETLB)) {\n\t\t\tretval = -EINVAL;\n\t\t\tgoto out_fput;\n\t\t}\n\t} else if (flags & MAP_HUGETLB) {\n\t\tstruct hstate *hs;\n\n\t\ths = hstate_sizelog((flags >> MAP_HUGE_SHIFT) & MAP_HUGE_MASK);\n\t\tif (!hs)\n\t\t\treturn -EINVAL;\n\n\t\tlen = ALIGN(len, huge_page_size(hs));\n\t\t \n\t\tfile = hugetlb_file_setup(HUGETLB_ANON_FILE, len,\n\t\t\t\tVM_NORESERVE,\n\t\t\t\tHUGETLB_ANONHUGE_INODE,\n\t\t\t\t(flags >> MAP_HUGE_SHIFT) & MAP_HUGE_MASK);\n\t\tif (IS_ERR(file))\n\t\t\treturn PTR_ERR(file);\n\t}\n\n\tretval = vm_mmap_pgoff(file, addr, len, prot, flags, pgoff);\nout_fput:\n\tif (file)\n\t\tfput(file);\n\treturn retval;\n}\n\nSYSCALL_DEFINE6(mmap_pgoff, unsigned long, addr, unsigned long, len,\n\t\tunsigned long, prot, unsigned long, flags,\n\t\tunsigned long, fd, unsigned long, pgoff)\n{\n\treturn ksys_mmap_pgoff(addr, len, prot, flags, fd, pgoff);\n}\n\n#ifdef __ARCH_WANT_SYS_OLD_MMAP\nstruct mmap_arg_struct {\n\tunsigned long addr;\n\tunsigned long len;\n\tunsigned long prot;\n\tunsigned long flags;\n\tunsigned long fd;\n\tunsigned long offset;\n};\n\nSYSCALL_DEFINE1(old_mmap, struct mmap_arg_struct __user *, arg)\n{\n\tstruct mmap_arg_struct a;\n\n\tif (copy_from_user(&a, arg, sizeof(a)))\n\t\treturn -EFAULT;\n\tif (offset_in_page(a.offset))\n\t\treturn -EINVAL;\n\n\treturn ksys_mmap_pgoff(a.addr, a.len, a.prot, a.flags, a.fd,\n\t\t\t       a.offset >> PAGE_SHIFT);\n}\n#endif  \n\nstatic bool vm_ops_needs_writenotify(const struct vm_operations_struct *vm_ops)\n{\n\treturn vm_ops && (vm_ops->page_mkwrite || vm_ops->pfn_mkwrite);\n}\n\nstatic bool vma_is_shared_writable(struct vm_area_struct *vma)\n{\n\treturn (vma->vm_flags & (VM_WRITE | VM_SHARED)) ==\n\t\t(VM_WRITE | VM_SHARED);\n}\n\nstatic bool vma_fs_can_writeback(struct vm_area_struct *vma)\n{\n\t \n\tif (vma->vm_flags & VM_PFNMAP)\n\t\treturn false;\n\n\treturn vma->vm_file && vma->vm_file->f_mapping &&\n\t\tmapping_can_writeback(vma->vm_file->f_mapping);\n}\n\n \nbool vma_needs_dirty_tracking(struct vm_area_struct *vma)\n{\n\t \n\tif (!vma_is_shared_writable(vma))\n\t\treturn false;\n\n\t \n\tif (vm_ops_needs_writenotify(vma->vm_ops))\n\t\treturn true;\n\n\t \n\treturn vma_fs_can_writeback(vma);\n}\n\n \nint vma_wants_writenotify(struct vm_area_struct *vma, pgprot_t vm_page_prot)\n{\n\t \n\tif (!vma_is_shared_writable(vma))\n\t\treturn 0;\n\n\t \n\tif (vm_ops_needs_writenotify(vma->vm_ops))\n\t\treturn 1;\n\n\t \n\tif (pgprot_val(vm_page_prot) !=\n\t    pgprot_val(vm_pgprot_modify(vm_page_prot, vma->vm_flags)))\n\t\treturn 0;\n\n\t \n\tif (vma_soft_dirty_enabled(vma) && !is_vm_hugetlb_page(vma))\n\t\treturn 1;\n\n\t \n\tif (userfaultfd_wp(vma))\n\t\treturn 1;\n\n\t \n\treturn vma_fs_can_writeback(vma);\n}\n\n \nstatic inline int accountable_mapping(struct file *file, vm_flags_t vm_flags)\n{\n\t \n\tif (file && is_file_hugepages(file))\n\t\treturn 0;\n\n\treturn (vm_flags & (VM_NORESERVE | VM_SHARED | VM_WRITE)) == VM_WRITE;\n}\n\n \nstatic unsigned long unmapped_area(struct vm_unmapped_area_info *info)\n{\n\tunsigned long length, gap;\n\tunsigned long low_limit, high_limit;\n\tstruct vm_area_struct *tmp;\n\n\tMA_STATE(mas, &current->mm->mm_mt, 0, 0);\n\n\t \n\tlength = info->length + info->align_mask;\n\tif (length < info->length)\n\t\treturn -ENOMEM;\n\n\tlow_limit = info->low_limit;\n\tif (low_limit < mmap_min_addr)\n\t\tlow_limit = mmap_min_addr;\n\thigh_limit = info->high_limit;\nretry:\n\tif (mas_empty_area(&mas, low_limit, high_limit - 1, length))\n\t\treturn -ENOMEM;\n\n\tgap = mas.index;\n\tgap += (info->align_offset - gap) & info->align_mask;\n\ttmp = mas_next(&mas, ULONG_MAX);\n\tif (tmp && (tmp->vm_flags & VM_STARTGAP_FLAGS)) {  \n\t\tif (vm_start_gap(tmp) < gap + length - 1) {\n\t\t\tlow_limit = tmp->vm_end;\n\t\t\tmas_reset(&mas);\n\t\t\tgoto retry;\n\t\t}\n\t} else {\n\t\ttmp = mas_prev(&mas, 0);\n\t\tif (tmp && vm_end_gap(tmp) > gap) {\n\t\t\tlow_limit = vm_end_gap(tmp);\n\t\t\tmas_reset(&mas);\n\t\t\tgoto retry;\n\t\t}\n\t}\n\n\treturn gap;\n}\n\n \nstatic unsigned long unmapped_area_topdown(struct vm_unmapped_area_info *info)\n{\n\tunsigned long length, gap, gap_end;\n\tunsigned long low_limit, high_limit;\n\tstruct vm_area_struct *tmp;\n\n\tMA_STATE(mas, &current->mm->mm_mt, 0, 0);\n\t \n\tlength = info->length + info->align_mask;\n\tif (length < info->length)\n\t\treturn -ENOMEM;\n\n\tlow_limit = info->low_limit;\n\tif (low_limit < mmap_min_addr)\n\t\tlow_limit = mmap_min_addr;\n\thigh_limit = info->high_limit;\nretry:\n\tif (mas_empty_area_rev(&mas, low_limit, high_limit - 1, length))\n\t\treturn -ENOMEM;\n\n\tgap = mas.last + 1 - info->length;\n\tgap -= (gap - info->align_offset) & info->align_mask;\n\tgap_end = mas.last;\n\ttmp = mas_next(&mas, ULONG_MAX);\n\tif (tmp && (tmp->vm_flags & VM_STARTGAP_FLAGS)) {  \n\t\tif (vm_start_gap(tmp) <= gap_end) {\n\t\t\thigh_limit = vm_start_gap(tmp);\n\t\t\tmas_reset(&mas);\n\t\t\tgoto retry;\n\t\t}\n\t} else {\n\t\ttmp = mas_prev(&mas, 0);\n\t\tif (tmp && vm_end_gap(tmp) > gap) {\n\t\t\thigh_limit = tmp->vm_start;\n\t\t\tmas_reset(&mas);\n\t\t\tgoto retry;\n\t\t}\n\t}\n\n\treturn gap;\n}\n\n \nunsigned long vm_unmapped_area(struct vm_unmapped_area_info *info)\n{\n\tunsigned long addr;\n\n\tif (info->flags & VM_UNMAPPED_AREA_TOPDOWN)\n\t\taddr = unmapped_area_topdown(info);\n\telse\n\t\taddr = unmapped_area(info);\n\n\ttrace_vm_unmapped_area(addr, info);\n\treturn addr;\n}\n\n \nunsigned long\ngeneric_get_unmapped_area(struct file *filp, unsigned long addr,\n\t\t\t  unsigned long len, unsigned long pgoff,\n\t\t\t  unsigned long flags)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma, *prev;\n\tstruct vm_unmapped_area_info info;\n\tconst unsigned long mmap_end = arch_get_mmap_end(addr, len, flags);\n\n\tif (len > mmap_end - mmap_min_addr)\n\t\treturn -ENOMEM;\n\n\tif (flags & MAP_FIXED)\n\t\treturn addr;\n\n\tif (addr) {\n\t\taddr = PAGE_ALIGN(addr);\n\t\tvma = find_vma_prev(mm, addr, &prev);\n\t\tif (mmap_end - len >= addr && addr >= mmap_min_addr &&\n\t\t    (!vma || addr + len <= vm_start_gap(vma)) &&\n\t\t    (!prev || addr >= vm_end_gap(prev)))\n\t\t\treturn addr;\n\t}\n\n\tinfo.flags = 0;\n\tinfo.length = len;\n\tinfo.low_limit = mm->mmap_base;\n\tinfo.high_limit = mmap_end;\n\tinfo.align_mask = 0;\n\tinfo.align_offset = 0;\n\treturn vm_unmapped_area(&info);\n}\n\n#ifndef HAVE_ARCH_UNMAPPED_AREA\nunsigned long\narch_get_unmapped_area(struct file *filp, unsigned long addr,\n\t\t       unsigned long len, unsigned long pgoff,\n\t\t       unsigned long flags)\n{\n\treturn generic_get_unmapped_area(filp, addr, len, pgoff, flags);\n}\n#endif\n\n \nunsigned long\ngeneric_get_unmapped_area_topdown(struct file *filp, unsigned long addr,\n\t\t\t\t  unsigned long len, unsigned long pgoff,\n\t\t\t\t  unsigned long flags)\n{\n\tstruct vm_area_struct *vma, *prev;\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_unmapped_area_info info;\n\tconst unsigned long mmap_end = arch_get_mmap_end(addr, len, flags);\n\n\t \n\tif (len > mmap_end - mmap_min_addr)\n\t\treturn -ENOMEM;\n\n\tif (flags & MAP_FIXED)\n\t\treturn addr;\n\n\t \n\tif (addr) {\n\t\taddr = PAGE_ALIGN(addr);\n\t\tvma = find_vma_prev(mm, addr, &prev);\n\t\tif (mmap_end - len >= addr && addr >= mmap_min_addr &&\n\t\t\t\t(!vma || addr + len <= vm_start_gap(vma)) &&\n\t\t\t\t(!prev || addr >= vm_end_gap(prev)))\n\t\t\treturn addr;\n\t}\n\n\tinfo.flags = VM_UNMAPPED_AREA_TOPDOWN;\n\tinfo.length = len;\n\tinfo.low_limit = PAGE_SIZE;\n\tinfo.high_limit = arch_get_mmap_base(addr, mm->mmap_base);\n\tinfo.align_mask = 0;\n\tinfo.align_offset = 0;\n\taddr = vm_unmapped_area(&info);\n\n\t \n\tif (offset_in_page(addr)) {\n\t\tVM_BUG_ON(addr != -ENOMEM);\n\t\tinfo.flags = 0;\n\t\tinfo.low_limit = TASK_UNMAPPED_BASE;\n\t\tinfo.high_limit = mmap_end;\n\t\taddr = vm_unmapped_area(&info);\n\t}\n\n\treturn addr;\n}\n\n#ifndef HAVE_ARCH_UNMAPPED_AREA_TOPDOWN\nunsigned long\narch_get_unmapped_area_topdown(struct file *filp, unsigned long addr,\n\t\t\t       unsigned long len, unsigned long pgoff,\n\t\t\t       unsigned long flags)\n{\n\treturn generic_get_unmapped_area_topdown(filp, addr, len, pgoff, flags);\n}\n#endif\n\nunsigned long\nget_unmapped_area(struct file *file, unsigned long addr, unsigned long len,\n\t\tunsigned long pgoff, unsigned long flags)\n{\n\tunsigned long (*get_area)(struct file *, unsigned long,\n\t\t\t\t  unsigned long, unsigned long, unsigned long);\n\n\tunsigned long error = arch_mmap_check(addr, len, flags);\n\tif (error)\n\t\treturn error;\n\n\t \n\tif (len > TASK_SIZE)\n\t\treturn -ENOMEM;\n\n\tget_area = current->mm->get_unmapped_area;\n\tif (file) {\n\t\tif (file->f_op->get_unmapped_area)\n\t\t\tget_area = file->f_op->get_unmapped_area;\n\t} else if (flags & MAP_SHARED) {\n\t\t \n\t\tpgoff = 0;\n\t\tget_area = shmem_get_unmapped_area;\n\t}\n\n\taddr = get_area(file, addr, len, pgoff, flags);\n\tif (IS_ERR_VALUE(addr))\n\t\treturn addr;\n\n\tif (addr > TASK_SIZE - len)\n\t\treturn -ENOMEM;\n\tif (offset_in_page(addr))\n\t\treturn -EINVAL;\n\n\terror = security_mmap_addr(addr);\n\treturn error ? error : addr;\n}\n\nEXPORT_SYMBOL(get_unmapped_area);\n\n \nstruct vm_area_struct *find_vma_intersection(struct mm_struct *mm,\n\t\t\t\t\t     unsigned long start_addr,\n\t\t\t\t\t     unsigned long end_addr)\n{\n\tunsigned long index = start_addr;\n\n\tmmap_assert_locked(mm);\n\treturn mt_find(&mm->mm_mt, &index, end_addr - 1);\n}\nEXPORT_SYMBOL(find_vma_intersection);\n\n \nstruct vm_area_struct *find_vma(struct mm_struct *mm, unsigned long addr)\n{\n\tunsigned long index = addr;\n\n\tmmap_assert_locked(mm);\n\treturn mt_find(&mm->mm_mt, &index, ULONG_MAX);\n}\nEXPORT_SYMBOL(find_vma);\n\n \nstruct vm_area_struct *\nfind_vma_prev(struct mm_struct *mm, unsigned long addr,\n\t\t\tstruct vm_area_struct **pprev)\n{\n\tstruct vm_area_struct *vma;\n\tMA_STATE(mas, &mm->mm_mt, addr, addr);\n\n\tvma = mas_walk(&mas);\n\t*pprev = mas_prev(&mas, 0);\n\tif (!vma)\n\t\tvma = mas_next(&mas, ULONG_MAX);\n\treturn vma;\n}\n\n \nstatic int acct_stack_growth(struct vm_area_struct *vma,\n\t\t\t     unsigned long size, unsigned long grow)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long new_start;\n\n\t \n\tif (!may_expand_vm(mm, vma->vm_flags, grow))\n\t\treturn -ENOMEM;\n\n\t \n\tif (size > rlimit(RLIMIT_STACK))\n\t\treturn -ENOMEM;\n\n\t \n\tif (!mlock_future_ok(mm, vma->vm_flags, grow << PAGE_SHIFT))\n\t\treturn -ENOMEM;\n\n\t \n\tnew_start = (vma->vm_flags & VM_GROWSUP) ? vma->vm_start :\n\t\t\tvma->vm_end - size;\n\tif (is_hugepage_only_range(vma->vm_mm, new_start, size))\n\t\treturn -EFAULT;\n\n\t \n\tif (security_vm_enough_memory_mm(mm, grow))\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\n#if defined(CONFIG_STACK_GROWSUP) || defined(CONFIG_IA64)\n \nstatic int expand_upwards(struct vm_area_struct *vma, unsigned long address)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct vm_area_struct *next;\n\tunsigned long gap_addr;\n\tint error = 0;\n\tMA_STATE(mas, &mm->mm_mt, vma->vm_start, address);\n\n\tif (!(vma->vm_flags & VM_GROWSUP))\n\t\treturn -EFAULT;\n\n\t \n\taddress &= PAGE_MASK;\n\tif (address >= (TASK_SIZE & PAGE_MASK))\n\t\treturn -ENOMEM;\n\taddress += PAGE_SIZE;\n\n\t \n\tgap_addr = address + stack_guard_gap;\n\n\t \n\tif (gap_addr < address || gap_addr > TASK_SIZE)\n\t\tgap_addr = TASK_SIZE;\n\n\tnext = find_vma_intersection(mm, vma->vm_end, gap_addr);\n\tif (next && vma_is_accessible(next)) {\n\t\tif (!(next->vm_flags & VM_GROWSUP))\n\t\t\treturn -ENOMEM;\n\t\t \n\t}\n\n\tif (next)\n\t\tmas_prev_range(&mas, address);\n\n\t__mas_set_range(&mas, vma->vm_start, address - 1);\n\tif (mas_preallocate(&mas, vma, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\t \n\tif (unlikely(anon_vma_prepare(vma))) {\n\t\tmas_destroy(&mas);\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tvma_start_write(vma);\n\t \n\tanon_vma_lock_write(vma->anon_vma);\n\n\t \n\tif (address > vma->vm_end) {\n\t\tunsigned long size, grow;\n\n\t\tsize = address - vma->vm_start;\n\t\tgrow = (address - vma->vm_end) >> PAGE_SHIFT;\n\n\t\terror = -ENOMEM;\n\t\tif (vma->vm_pgoff + (size >> PAGE_SHIFT) >= vma->vm_pgoff) {\n\t\t\terror = acct_stack_growth(vma, size, grow);\n\t\t\tif (!error) {\n\t\t\t\t \n\t\t\t\tspin_lock(&mm->page_table_lock);\n\t\t\t\tif (vma->vm_flags & VM_LOCKED)\n\t\t\t\t\tmm->locked_vm += grow;\n\t\t\t\tvm_stat_account(mm, vma->vm_flags, grow);\n\t\t\t\tanon_vma_interval_tree_pre_update_vma(vma);\n\t\t\t\tvma->vm_end = address;\n\t\t\t\t \n\t\t\t\tmas_store_prealloc(&mas, vma);\n\t\t\t\tanon_vma_interval_tree_post_update_vma(vma);\n\t\t\t\tspin_unlock(&mm->page_table_lock);\n\n\t\t\t\tperf_event_mmap(vma);\n\t\t\t}\n\t\t}\n\t}\n\tanon_vma_unlock_write(vma->anon_vma);\n\tkhugepaged_enter_vma(vma, vma->vm_flags);\n\tmas_destroy(&mas);\n\tvalidate_mm(mm);\n\treturn error;\n}\n#endif  \n\n \nint expand_downwards(struct vm_area_struct *vma, unsigned long address)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tMA_STATE(mas, &mm->mm_mt, vma->vm_start, vma->vm_start);\n\tstruct vm_area_struct *prev;\n\tint error = 0;\n\n\tif (!(vma->vm_flags & VM_GROWSDOWN))\n\t\treturn -EFAULT;\n\n\taddress &= PAGE_MASK;\n\tif (address < mmap_min_addr || address < FIRST_USER_ADDRESS)\n\t\treturn -EPERM;\n\n\t \n\tprev = mas_prev(&mas, 0);\n\t \n\tif (prev) {\n\t\tif (!(prev->vm_flags & VM_GROWSDOWN) &&\n\t\t    vma_is_accessible(prev) &&\n\t\t    (address - prev->vm_end < stack_guard_gap))\n\t\t\treturn -ENOMEM;\n\t}\n\n\tif (prev)\n\t\tmas_next_range(&mas, vma->vm_start);\n\n\t__mas_set_range(&mas, address, vma->vm_end - 1);\n\tif (mas_preallocate(&mas, vma, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\t \n\tif (unlikely(anon_vma_prepare(vma))) {\n\t\tmas_destroy(&mas);\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tvma_start_write(vma);\n\t \n\tanon_vma_lock_write(vma->anon_vma);\n\n\t \n\tif (address < vma->vm_start) {\n\t\tunsigned long size, grow;\n\n\t\tsize = vma->vm_end - address;\n\t\tgrow = (vma->vm_start - address) >> PAGE_SHIFT;\n\n\t\terror = -ENOMEM;\n\t\tif (grow <= vma->vm_pgoff) {\n\t\t\terror = acct_stack_growth(vma, size, grow);\n\t\t\tif (!error) {\n\t\t\t\t \n\t\t\t\tspin_lock(&mm->page_table_lock);\n\t\t\t\tif (vma->vm_flags & VM_LOCKED)\n\t\t\t\t\tmm->locked_vm += grow;\n\t\t\t\tvm_stat_account(mm, vma->vm_flags, grow);\n\t\t\t\tanon_vma_interval_tree_pre_update_vma(vma);\n\t\t\t\tvma->vm_start = address;\n\t\t\t\tvma->vm_pgoff -= grow;\n\t\t\t\t \n\t\t\t\tmas_store_prealloc(&mas, vma);\n\t\t\t\tanon_vma_interval_tree_post_update_vma(vma);\n\t\t\t\tspin_unlock(&mm->page_table_lock);\n\n\t\t\t\tperf_event_mmap(vma);\n\t\t\t}\n\t\t}\n\t}\n\tanon_vma_unlock_write(vma->anon_vma);\n\tkhugepaged_enter_vma(vma, vma->vm_flags);\n\tmas_destroy(&mas);\n\tvalidate_mm(mm);\n\treturn error;\n}\n\n \nunsigned long stack_guard_gap = 256UL<<PAGE_SHIFT;\n\nstatic int __init cmdline_parse_stack_guard_gap(char *p)\n{\n\tunsigned long val;\n\tchar *endptr;\n\n\tval = simple_strtoul(p, &endptr, 10);\n\tif (!*endptr)\n\t\tstack_guard_gap = val << PAGE_SHIFT;\n\n\treturn 1;\n}\n__setup(\"stack_guard_gap=\", cmdline_parse_stack_guard_gap);\n\n#ifdef CONFIG_STACK_GROWSUP\nint expand_stack_locked(struct vm_area_struct *vma, unsigned long address)\n{\n\treturn expand_upwards(vma, address);\n}\n\nstruct vm_area_struct *find_extend_vma_locked(struct mm_struct *mm, unsigned long addr)\n{\n\tstruct vm_area_struct *vma, *prev;\n\n\taddr &= PAGE_MASK;\n\tvma = find_vma_prev(mm, addr, &prev);\n\tif (vma && (vma->vm_start <= addr))\n\t\treturn vma;\n\tif (!prev)\n\t\treturn NULL;\n\tif (expand_stack_locked(prev, addr))\n\t\treturn NULL;\n\tif (prev->vm_flags & VM_LOCKED)\n\t\tpopulate_vma_page_range(prev, addr, prev->vm_end, NULL);\n\treturn prev;\n}\n#else\nint expand_stack_locked(struct vm_area_struct *vma, unsigned long address)\n{\n\tif (unlikely(!(vma->vm_flags & VM_GROWSDOWN)))\n\t\treturn -EINVAL;\n\treturn expand_downwards(vma, address);\n}\n\nstruct vm_area_struct *find_extend_vma_locked(struct mm_struct *mm, unsigned long addr)\n{\n\tstruct vm_area_struct *vma;\n\tunsigned long start;\n\n\taddr &= PAGE_MASK;\n\tvma = find_vma(mm, addr);\n\tif (!vma)\n\t\treturn NULL;\n\tif (vma->vm_start <= addr)\n\t\treturn vma;\n\tstart = vma->vm_start;\n\tif (expand_stack_locked(vma, addr))\n\t\treturn NULL;\n\tif (vma->vm_flags & VM_LOCKED)\n\t\tpopulate_vma_page_range(vma, addr, start, NULL);\n\treturn vma;\n}\n#endif\n\n \n#ifdef CONFIG_IA64\nstatic inline bool vma_expand_ok(struct vm_area_struct *vma, unsigned long addr)\n{\n\treturn REGION_NUMBER(addr) == REGION_NUMBER(vma->vm_start) &&\n\t\tREGION_OFFSET(addr) < RGN_MAP_LIMIT;\n}\n\n \nstatic inline int vma_expand_up(struct vm_area_struct *vma, unsigned long addr)\n{\n\tif (!vma_expand_ok(vma, addr))\n\t\treturn -EFAULT;\n\tif (vma->vm_end != (addr & PAGE_MASK))\n\t\treturn -EFAULT;\n\treturn expand_upwards(vma, addr);\n}\n\nstatic inline bool vma_expand_down(struct vm_area_struct *vma, unsigned long addr)\n{\n\tif (!vma_expand_ok(vma, addr))\n\t\treturn -EFAULT;\n\treturn expand_downwards(vma, addr);\n}\n\n#elif defined(CONFIG_STACK_GROWSUP)\n\n#define vma_expand_up(vma,addr) expand_upwards(vma, addr)\n#define vma_expand_down(vma, addr) (-EFAULT)\n\n#else\n\n#define vma_expand_up(vma,addr) (-EFAULT)\n#define vma_expand_down(vma, addr) expand_downwards(vma, addr)\n\n#endif\n\n \nstruct vm_area_struct *expand_stack(struct mm_struct *mm, unsigned long addr)\n{\n\tstruct vm_area_struct *vma, *prev;\n\n\tmmap_read_unlock(mm);\n\tif (mmap_write_lock_killable(mm))\n\t\treturn NULL;\n\n\tvma = find_vma_prev(mm, addr, &prev);\n\tif (vma && vma->vm_start <= addr)\n\t\tgoto success;\n\n\tif (prev && !vma_expand_up(prev, addr)) {\n\t\tvma = prev;\n\t\tgoto success;\n\t}\n\n\tif (vma && !vma_expand_down(vma, addr))\n\t\tgoto success;\n\n\tmmap_write_unlock(mm);\n\treturn NULL;\n\nsuccess:\n\tmmap_write_downgrade(mm);\n\treturn vma;\n}\n\n \nstatic inline void remove_mt(struct mm_struct *mm, struct ma_state *mas)\n{\n\tunsigned long nr_accounted = 0;\n\tstruct vm_area_struct *vma;\n\n\t \n\tupdate_hiwater_vm(mm);\n\tmas_for_each(mas, vma, ULONG_MAX) {\n\t\tlong nrpages = vma_pages(vma);\n\n\t\tif (vma->vm_flags & VM_ACCOUNT)\n\t\t\tnr_accounted += nrpages;\n\t\tvm_stat_account(mm, vma->vm_flags, -nrpages);\n\t\tremove_vma(vma, false);\n\t}\n\tvm_unacct_memory(nr_accounted);\n}\n\n \nstatic void unmap_region(struct mm_struct *mm, struct ma_state *mas,\n\t\tstruct vm_area_struct *vma, struct vm_area_struct *prev,\n\t\tstruct vm_area_struct *next, unsigned long start,\n\t\tunsigned long end, unsigned long tree_end, bool mm_wr_locked)\n{\n\tstruct mmu_gather tlb;\n\tunsigned long mt_start = mas->index;\n\n\tlru_add_drain();\n\ttlb_gather_mmu(&tlb, mm);\n\tupdate_hiwater_rss(mm);\n\tunmap_vmas(&tlb, mas, vma, start, end, tree_end, mm_wr_locked);\n\tmas_set(mas, mt_start);\n\tfree_pgtables(&tlb, mas, vma, prev ? prev->vm_end : FIRST_USER_ADDRESS,\n\t\t\t\t next ? next->vm_start : USER_PGTABLES_CEILING,\n\t\t\t\t mm_wr_locked);\n\ttlb_finish_mmu(&tlb);\n}\n\n \nint __split_vma(struct vma_iterator *vmi, struct vm_area_struct *vma,\n\t\tunsigned long addr, int new_below)\n{\n\tstruct vma_prepare vp;\n\tstruct vm_area_struct *new;\n\tint err;\n\n\tWARN_ON(vma->vm_start >= addr);\n\tWARN_ON(vma->vm_end <= addr);\n\n\tif (vma->vm_ops && vma->vm_ops->may_split) {\n\t\terr = vma->vm_ops->may_split(vma, addr);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tnew = vm_area_dup(vma);\n\tif (!new)\n\t\treturn -ENOMEM;\n\n\tif (new_below) {\n\t\tnew->vm_end = addr;\n\t} else {\n\t\tnew->vm_start = addr;\n\t\tnew->vm_pgoff += ((addr - vma->vm_start) >> PAGE_SHIFT);\n\t}\n\n\terr = -ENOMEM;\n\tvma_iter_config(vmi, new->vm_start, new->vm_end);\n\tif (vma_iter_prealloc(vmi, new))\n\t\tgoto out_free_vma;\n\n\terr = vma_dup_policy(vma, new);\n\tif (err)\n\t\tgoto out_free_vmi;\n\n\terr = anon_vma_clone(new, vma);\n\tif (err)\n\t\tgoto out_free_mpol;\n\n\tif (new->vm_file)\n\t\tget_file(new->vm_file);\n\n\tif (new->vm_ops && new->vm_ops->open)\n\t\tnew->vm_ops->open(new);\n\n\tvma_start_write(vma);\n\tvma_start_write(new);\n\n\tinit_vma_prep(&vp, vma);\n\tvp.insert = new;\n\tvma_prepare(&vp);\n\tvma_adjust_trans_huge(vma, vma->vm_start, addr, 0);\n\n\tif (new_below) {\n\t\tvma->vm_start = addr;\n\t\tvma->vm_pgoff += (addr - new->vm_start) >> PAGE_SHIFT;\n\t} else {\n\t\tvma->vm_end = addr;\n\t}\n\n\t \n\tvma_complete(&vp, vmi, vma->vm_mm);\n\n\t \n\tif (new_below)\n\t\tvma_next(vmi);\n\treturn 0;\n\nout_free_mpol:\n\tmpol_put(vma_policy(new));\nout_free_vmi:\n\tvma_iter_free(vmi);\nout_free_vma:\n\tvm_area_free(new);\n\treturn err;\n}\n\n \nint split_vma(struct vma_iterator *vmi, struct vm_area_struct *vma,\n\t      unsigned long addr, int new_below)\n{\n\tif (vma->vm_mm->map_count >= sysctl_max_map_count)\n\t\treturn -ENOMEM;\n\n\treturn __split_vma(vmi, vma, addr, new_below);\n}\n\n \nstatic int\ndo_vmi_align_munmap(struct vma_iterator *vmi, struct vm_area_struct *vma,\n\t\t    struct mm_struct *mm, unsigned long start,\n\t\t    unsigned long end, struct list_head *uf, bool unlock)\n{\n\tstruct vm_area_struct *prev, *next = NULL;\n\tstruct maple_tree mt_detach;\n\tint count = 0;\n\tint error = -ENOMEM;\n\tunsigned long locked_vm = 0;\n\tMA_STATE(mas_detach, &mt_detach, 0, 0);\n\tmt_init_flags(&mt_detach, vmi->mas.tree->ma_flags & MT_FLAGS_LOCK_MASK);\n\tmt_on_stack(mt_detach);\n\n\t \n\n\t \n\tif (start > vma->vm_start) {\n\n\t\t \n\t\tif (end < vma->vm_end && mm->map_count >= sysctl_max_map_count)\n\t\t\tgoto map_count_exceeded;\n\n\t\terror = __split_vma(vmi, vma, start, 1);\n\t\tif (error)\n\t\t\tgoto start_split_failed;\n\t}\n\n\t \n\tnext = vma;\n\tdo {\n\t\t \n\t\tif (next->vm_end > end) {\n\t\t\terror = __split_vma(vmi, next, end, 0);\n\t\t\tif (error)\n\t\t\t\tgoto end_split_failed;\n\t\t}\n\t\tvma_start_write(next);\n\t\tmas_set(&mas_detach, count);\n\t\terror = mas_store_gfp(&mas_detach, next, GFP_KERNEL);\n\t\tif (error)\n\t\t\tgoto munmap_gather_failed;\n\t\tvma_mark_detached(next, true);\n\t\tif (next->vm_flags & VM_LOCKED)\n\t\t\tlocked_vm += vma_pages(next);\n\n\t\tcount++;\n\t\tif (unlikely(uf)) {\n\t\t\t \n\t\t\terror = userfaultfd_unmap_prep(next, start, end, uf);\n\n\t\t\tif (error)\n\t\t\t\tgoto userfaultfd_error;\n\t\t}\n#ifdef CONFIG_DEBUG_VM_MAPLE_TREE\n\t\tBUG_ON(next->vm_start < start);\n\t\tBUG_ON(next->vm_start > end);\n#endif\n\t} for_each_vma_range(*vmi, next, end);\n\n#if defined(CONFIG_DEBUG_VM_MAPLE_TREE)\n\t \n\t{\n\t\tMA_STATE(test, &mt_detach, 0, 0);\n\t\tstruct vm_area_struct *vma_mas, *vma_test;\n\t\tint test_count = 0;\n\n\t\tvma_iter_set(vmi, start);\n\t\trcu_read_lock();\n\t\tvma_test = mas_find(&test, count - 1);\n\t\tfor_each_vma_range(*vmi, vma_mas, end) {\n\t\t\tBUG_ON(vma_mas != vma_test);\n\t\t\ttest_count++;\n\t\t\tvma_test = mas_next(&test, count - 1);\n\t\t}\n\t\trcu_read_unlock();\n\t\tBUG_ON(count != test_count);\n\t}\n#endif\n\n\twhile (vma_iter_addr(vmi) > start)\n\t\tvma_iter_prev_range(vmi);\n\n\terror = vma_iter_clear_gfp(vmi, start, end, GFP_KERNEL);\n\tif (error)\n\t\tgoto clear_tree_failed;\n\n\t \n\tmm->locked_vm -= locked_vm;\n\tmm->map_count -= count;\n\tif (unlock)\n\t\tmmap_write_downgrade(mm);\n\n\tprev = vma_iter_prev_range(vmi);\n\tnext = vma_next(vmi);\n\tif (next)\n\t\tvma_iter_prev_range(vmi);\n\n\t \n\tmas_set(&mas_detach, 1);\n\tunmap_region(mm, &mas_detach, vma, prev, next, start, end, count,\n\t\t     !unlock);\n\t \n\tmas_set(&mas_detach, 0);\n\tremove_mt(mm, &mas_detach);\n\tvalidate_mm(mm);\n\tif (unlock)\n\t\tmmap_read_unlock(mm);\n\n\t__mt_destroy(&mt_detach);\n\treturn 0;\n\nclear_tree_failed:\nuserfaultfd_error:\nmunmap_gather_failed:\nend_split_failed:\n\tmas_set(&mas_detach, 0);\n\tmas_for_each(&mas_detach, next, end)\n\t\tvma_mark_detached(next, false);\n\n\t__mt_destroy(&mt_detach);\nstart_split_failed:\nmap_count_exceeded:\n\tvalidate_mm(mm);\n\treturn error;\n}\n\n \nint do_vmi_munmap(struct vma_iterator *vmi, struct mm_struct *mm,\n\t\t  unsigned long start, size_t len, struct list_head *uf,\n\t\t  bool unlock)\n{\n\tunsigned long end;\n\tstruct vm_area_struct *vma;\n\n\tif ((offset_in_page(start)) || start > TASK_SIZE || len > TASK_SIZE-start)\n\t\treturn -EINVAL;\n\n\tend = start + PAGE_ALIGN(len);\n\tif (end == start)\n\t\treturn -EINVAL;\n\n\t  \n\tarch_unmap(mm, start, end);\n\n\t \n\tvma = vma_find(vmi, end);\n\tif (!vma) {\n\t\tif (unlock)\n\t\t\tmmap_write_unlock(mm);\n\t\treturn 0;\n\t}\n\n\treturn do_vmi_align_munmap(vmi, vma, mm, start, end, uf, unlock);\n}\n\n \nint do_munmap(struct mm_struct *mm, unsigned long start, size_t len,\n\t      struct list_head *uf)\n{\n\tVMA_ITERATOR(vmi, mm, start);\n\n\treturn do_vmi_munmap(&vmi, mm, start, len, uf, false);\n}\n\nunsigned long mmap_region(struct file *file, unsigned long addr,\n\t\tunsigned long len, vm_flags_t vm_flags, unsigned long pgoff,\n\t\tstruct list_head *uf)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma = NULL;\n\tstruct vm_area_struct *next, *prev, *merge;\n\tpgoff_t pglen = len >> PAGE_SHIFT;\n\tunsigned long charged = 0;\n\tunsigned long end = addr + len;\n\tunsigned long merge_start = addr, merge_end = end;\n\tpgoff_t vm_pgoff;\n\tint error;\n\tVMA_ITERATOR(vmi, mm, addr);\n\n\t \n\tif (!may_expand_vm(mm, vm_flags, len >> PAGE_SHIFT)) {\n\t\tunsigned long nr_pages;\n\n\t\t \n\t\tnr_pages = count_vma_pages_range(mm, addr, end);\n\n\t\tif (!may_expand_vm(mm, vm_flags,\n\t\t\t\t\t(len >> PAGE_SHIFT) - nr_pages))\n\t\t\treturn -ENOMEM;\n\t}\n\n\t \n\tif (do_vmi_munmap(&vmi, mm, addr, len, uf, false))\n\t\treturn -ENOMEM;\n\n\t \n\tif (accountable_mapping(file, vm_flags)) {\n\t\tcharged = len >> PAGE_SHIFT;\n\t\tif (security_vm_enough_memory_mm(mm, charged))\n\t\t\treturn -ENOMEM;\n\t\tvm_flags |= VM_ACCOUNT;\n\t}\n\n\tnext = vma_next(&vmi);\n\tprev = vma_prev(&vmi);\n\tif (vm_flags & VM_SPECIAL) {\n\t\tif (prev)\n\t\t\tvma_iter_next_range(&vmi);\n\t\tgoto cannot_expand;\n\t}\n\n\t \n\t \n\tif (next && next->vm_start == end && !vma_policy(next) &&\n\t    can_vma_merge_before(next, vm_flags, NULL, file, pgoff+pglen,\n\t\t\t\t NULL_VM_UFFD_CTX, NULL)) {\n\t\tmerge_end = next->vm_end;\n\t\tvma = next;\n\t\tvm_pgoff = next->vm_pgoff - pglen;\n\t}\n\n\t \n\tif (prev && prev->vm_end == addr && !vma_policy(prev) &&\n\t    (vma ? can_vma_merge_after(prev, vm_flags, vma->anon_vma, file,\n\t\t\t\t       pgoff, vma->vm_userfaultfd_ctx, NULL) :\n\t\t   can_vma_merge_after(prev, vm_flags, NULL, file, pgoff,\n\t\t\t\t       NULL_VM_UFFD_CTX, NULL))) {\n\t\tmerge_start = prev->vm_start;\n\t\tvma = prev;\n\t\tvm_pgoff = prev->vm_pgoff;\n\t} else if (prev) {\n\t\tvma_iter_next_range(&vmi);\n\t}\n\n\t \n\tif (vma &&\n\t    !vma_expand(&vmi, vma, merge_start, merge_end, vm_pgoff, next)) {\n\t\tkhugepaged_enter_vma(vma, vm_flags);\n\t\tgoto expanded;\n\t}\n\n\tif (vma == prev)\n\t\tvma_iter_set(&vmi, addr);\ncannot_expand:\n\n\t \n\tvma = vm_area_alloc(mm);\n\tif (!vma) {\n\t\terror = -ENOMEM;\n\t\tgoto unacct_error;\n\t}\n\n\tvma_iter_config(&vmi, addr, end);\n\tvma->vm_start = addr;\n\tvma->vm_end = end;\n\tvm_flags_init(vma, vm_flags);\n\tvma->vm_page_prot = vm_get_page_prot(vm_flags);\n\tvma->vm_pgoff = pgoff;\n\n\tif (file) {\n\t\tif (vm_flags & VM_SHARED) {\n\t\t\terror = mapping_map_writable(file->f_mapping);\n\t\t\tif (error)\n\t\t\t\tgoto free_vma;\n\t\t}\n\n\t\tvma->vm_file = get_file(file);\n\t\terror = call_mmap(file, vma);\n\t\tif (error)\n\t\t\tgoto unmap_and_free_vma;\n\n\t\t \n\t\terror = -EINVAL;\n\t\tif (WARN_ON((addr != vma->vm_start)))\n\t\t\tgoto close_and_free_vma;\n\n\t\tvma_iter_config(&vmi, addr, end);\n\t\t \n\t\tif (unlikely(vm_flags != vma->vm_flags && prev)) {\n\t\t\tmerge = vma_merge(&vmi, mm, prev, vma->vm_start,\n\t\t\t\t    vma->vm_end, vma->vm_flags, NULL,\n\t\t\t\t    vma->vm_file, vma->vm_pgoff, NULL,\n\t\t\t\t    NULL_VM_UFFD_CTX, NULL);\n\t\t\tif (merge) {\n\t\t\t\t \n\t\t\t\tfput(vma->vm_file);\n\t\t\t\tvm_area_free(vma);\n\t\t\t\tvma = merge;\n\t\t\t\t \n\t\t\t\tvm_flags = vma->vm_flags;\n\t\t\t\tgoto unmap_writable;\n\t\t\t}\n\t\t}\n\n\t\tvm_flags = vma->vm_flags;\n\t} else if (vm_flags & VM_SHARED) {\n\t\terror = shmem_zero_setup(vma);\n\t\tif (error)\n\t\t\tgoto free_vma;\n\t} else {\n\t\tvma_set_anonymous(vma);\n\t}\n\n\tif (map_deny_write_exec(vma, vma->vm_flags)) {\n\t\terror = -EACCES;\n\t\tgoto close_and_free_vma;\n\t}\n\n\t \n\terror = -EINVAL;\n\tif (!arch_validate_flags(vma->vm_flags))\n\t\tgoto close_and_free_vma;\n\n\terror = -ENOMEM;\n\tif (vma_iter_prealloc(&vmi, vma))\n\t\tgoto close_and_free_vma;\n\n\t \n\tvma_start_write(vma);\n\tvma_iter_store(&vmi, vma);\n\tmm->map_count++;\n\tif (vma->vm_file) {\n\t\ti_mmap_lock_write(vma->vm_file->f_mapping);\n\t\tif (vma->vm_flags & VM_SHARED)\n\t\t\tmapping_allow_writable(vma->vm_file->f_mapping);\n\n\t\tflush_dcache_mmap_lock(vma->vm_file->f_mapping);\n\t\tvma_interval_tree_insert(vma, &vma->vm_file->f_mapping->i_mmap);\n\t\tflush_dcache_mmap_unlock(vma->vm_file->f_mapping);\n\t\ti_mmap_unlock_write(vma->vm_file->f_mapping);\n\t}\n\n\t \n\tkhugepaged_enter_vma(vma, vma->vm_flags);\n\n\t \nunmap_writable:\n\tif (file && vm_flags & VM_SHARED)\n\t\tmapping_unmap_writable(file->f_mapping);\n\tfile = vma->vm_file;\n\tksm_add_vma(vma);\nexpanded:\n\tperf_event_mmap(vma);\n\n\tvm_stat_account(mm, vm_flags, len >> PAGE_SHIFT);\n\tif (vm_flags & VM_LOCKED) {\n\t\tif ((vm_flags & VM_SPECIAL) || vma_is_dax(vma) ||\n\t\t\t\t\tis_vm_hugetlb_page(vma) ||\n\t\t\t\t\tvma == get_gate_vma(current->mm))\n\t\t\tvm_flags_clear(vma, VM_LOCKED_MASK);\n\t\telse\n\t\t\tmm->locked_vm += (len >> PAGE_SHIFT);\n\t}\n\n\tif (file)\n\t\tuprobe_mmap(vma);\n\n\t \n\tvm_flags_set(vma, VM_SOFTDIRTY);\n\n\tvma_set_page_prot(vma);\n\n\tvalidate_mm(mm);\n\treturn addr;\n\nclose_and_free_vma:\n\tif (file && vma->vm_ops && vma->vm_ops->close)\n\t\tvma->vm_ops->close(vma);\n\n\tif (file || vma->vm_file) {\nunmap_and_free_vma:\n\t\tfput(vma->vm_file);\n\t\tvma->vm_file = NULL;\n\n\t\tvma_iter_set(&vmi, vma->vm_end);\n\t\t \n\t\tunmap_region(mm, &vmi.mas, vma, prev, next, vma->vm_start,\n\t\t\t     vma->vm_end, vma->vm_end, true);\n\t}\n\tif (file && (vm_flags & VM_SHARED))\n\t\tmapping_unmap_writable(file->f_mapping);\nfree_vma:\n\tvm_area_free(vma);\nunacct_error:\n\tif (charged)\n\t\tvm_unacct_memory(charged);\n\tvalidate_mm(mm);\n\treturn error;\n}\n\nstatic int __vm_munmap(unsigned long start, size_t len, bool unlock)\n{\n\tint ret;\n\tstruct mm_struct *mm = current->mm;\n\tLIST_HEAD(uf);\n\tVMA_ITERATOR(vmi, mm, start);\n\n\tif (mmap_write_lock_killable(mm))\n\t\treturn -EINTR;\n\n\tret = do_vmi_munmap(&vmi, mm, start, len, &uf, unlock);\n\tif (ret || !unlock)\n\t\tmmap_write_unlock(mm);\n\n\tuserfaultfd_unmap_complete(mm, &uf);\n\treturn ret;\n}\n\nint vm_munmap(unsigned long start, size_t len)\n{\n\treturn __vm_munmap(start, len, false);\n}\nEXPORT_SYMBOL(vm_munmap);\n\nSYSCALL_DEFINE2(munmap, unsigned long, addr, size_t, len)\n{\n\taddr = untagged_addr(addr);\n\treturn __vm_munmap(addr, len, true);\n}\n\n\n \nSYSCALL_DEFINE5(remap_file_pages, unsigned long, start, unsigned long, size,\n\t\tunsigned long, prot, unsigned long, pgoff, unsigned long, flags)\n{\n\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long populate = 0;\n\tunsigned long ret = -EINVAL;\n\tstruct file *file;\n\n\tpr_warn_once(\"%s (%d) uses deprecated remap_file_pages() syscall. See Documentation/mm/remap_file_pages.rst.\\n\",\n\t\t     current->comm, current->pid);\n\n\tif (prot)\n\t\treturn ret;\n\tstart = start & PAGE_MASK;\n\tsize = size & PAGE_MASK;\n\n\tif (start + size <= start)\n\t\treturn ret;\n\n\t \n\tif (pgoff + (size >> PAGE_SHIFT) < pgoff)\n\t\treturn ret;\n\n\tif (mmap_write_lock_killable(mm))\n\t\treturn -EINTR;\n\n\tvma = vma_lookup(mm, start);\n\n\tif (!vma || !(vma->vm_flags & VM_SHARED))\n\t\tgoto out;\n\n\tif (start + size > vma->vm_end) {\n\t\tVMA_ITERATOR(vmi, mm, vma->vm_end);\n\t\tstruct vm_area_struct *next, *prev = vma;\n\n\t\tfor_each_vma_range(vmi, next, start + size) {\n\t\t\t \n\t\t\tif (next->vm_start != prev->vm_end)\n\t\t\t\tgoto out;\n\n\t\t\tif (next->vm_file != vma->vm_file)\n\t\t\t\tgoto out;\n\n\t\t\tif (next->vm_flags != vma->vm_flags)\n\t\t\t\tgoto out;\n\n\t\t\tif (start + size <= next->vm_end)\n\t\t\t\tbreak;\n\n\t\t\tprev = next;\n\t\t}\n\n\t\tif (!next)\n\t\t\tgoto out;\n\t}\n\n\tprot |= vma->vm_flags & VM_READ ? PROT_READ : 0;\n\tprot |= vma->vm_flags & VM_WRITE ? PROT_WRITE : 0;\n\tprot |= vma->vm_flags & VM_EXEC ? PROT_EXEC : 0;\n\n\tflags &= MAP_NONBLOCK;\n\tflags |= MAP_SHARED | MAP_FIXED | MAP_POPULATE;\n\tif (vma->vm_flags & VM_LOCKED)\n\t\tflags |= MAP_LOCKED;\n\n\tfile = get_file(vma->vm_file);\n\tret = do_mmap(vma->vm_file, start, size,\n\t\t\tprot, flags, 0, pgoff, &populate, NULL);\n\tfput(file);\nout:\n\tmmap_write_unlock(mm);\n\tif (populate)\n\t\tmm_populate(ret, populate);\n\tif (!IS_ERR_VALUE(ret))\n\t\tret = 0;\n\treturn ret;\n}\n\n \nint do_vma_munmap(struct vma_iterator *vmi, struct vm_area_struct *vma,\n\t\tunsigned long start, unsigned long end, struct list_head *uf,\n\t\tbool unlock)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\n\tarch_unmap(mm, start, end);\n\treturn do_vmi_align_munmap(vmi, vma, mm, start, end, uf, unlock);\n}\n\n \nstatic int do_brk_flags(struct vma_iterator *vmi, struct vm_area_struct *vma,\n\t\tunsigned long addr, unsigned long len, unsigned long flags)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vma_prepare vp;\n\n\t \n\tflags |= VM_DATA_DEFAULT_FLAGS | VM_ACCOUNT | mm->def_flags;\n\tif (!may_expand_vm(mm, flags, len >> PAGE_SHIFT))\n\t\treturn -ENOMEM;\n\n\tif (mm->map_count > sysctl_max_map_count)\n\t\treturn -ENOMEM;\n\n\tif (security_vm_enough_memory_mm(mm, len >> PAGE_SHIFT))\n\t\treturn -ENOMEM;\n\n\t \n\tif (vma && vma->vm_end == addr && !vma_policy(vma) &&\n\t    can_vma_merge_after(vma, flags, NULL, NULL,\n\t\t\t\taddr >> PAGE_SHIFT, NULL_VM_UFFD_CTX, NULL)) {\n\t\tvma_iter_config(vmi, vma->vm_start, addr + len);\n\t\tif (vma_iter_prealloc(vmi, vma))\n\t\t\tgoto unacct_fail;\n\n\t\tvma_start_write(vma);\n\n\t\tinit_vma_prep(&vp, vma);\n\t\tvma_prepare(&vp);\n\t\tvma_adjust_trans_huge(vma, vma->vm_start, addr + len, 0);\n\t\tvma->vm_end = addr + len;\n\t\tvm_flags_set(vma, VM_SOFTDIRTY);\n\t\tvma_iter_store(vmi, vma);\n\n\t\tvma_complete(&vp, vmi, mm);\n\t\tkhugepaged_enter_vma(vma, flags);\n\t\tgoto out;\n\t}\n\n\tif (vma)\n\t\tvma_iter_next_range(vmi);\n\t \n\tvma = vm_area_alloc(mm);\n\tif (!vma)\n\t\tgoto unacct_fail;\n\n\tvma_set_anonymous(vma);\n\tvma->vm_start = addr;\n\tvma->vm_end = addr + len;\n\tvma->vm_pgoff = addr >> PAGE_SHIFT;\n\tvm_flags_init(vma, flags);\n\tvma->vm_page_prot = vm_get_page_prot(flags);\n\tvma_start_write(vma);\n\tif (vma_iter_store_gfp(vmi, vma, GFP_KERNEL))\n\t\tgoto mas_store_fail;\n\n\tmm->map_count++;\n\tvalidate_mm(mm);\n\tksm_add_vma(vma);\nout:\n\tperf_event_mmap(vma);\n\tmm->total_vm += len >> PAGE_SHIFT;\n\tmm->data_vm += len >> PAGE_SHIFT;\n\tif (flags & VM_LOCKED)\n\t\tmm->locked_vm += (len >> PAGE_SHIFT);\n\tvm_flags_set(vma, VM_SOFTDIRTY);\n\treturn 0;\n\nmas_store_fail:\n\tvm_area_free(vma);\nunacct_fail:\n\tvm_unacct_memory(len >> PAGE_SHIFT);\n\treturn -ENOMEM;\n}\n\nint vm_brk_flags(unsigned long addr, unsigned long request, unsigned long flags)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma = NULL;\n\tunsigned long len;\n\tint ret;\n\tbool populate;\n\tLIST_HEAD(uf);\n\tVMA_ITERATOR(vmi, mm, addr);\n\n\tlen = PAGE_ALIGN(request);\n\tif (len < request)\n\t\treturn -ENOMEM;\n\tif (!len)\n\t\treturn 0;\n\n\t \n\tif ((flags & (~VM_EXEC)) != 0)\n\t\treturn -EINVAL;\n\n\tif (mmap_write_lock_killable(mm))\n\t\treturn -EINTR;\n\n\tret = check_brk_limits(addr, len);\n\tif (ret)\n\t\tgoto limits_failed;\n\n\tret = do_vmi_munmap(&vmi, mm, addr, len, &uf, 0);\n\tif (ret)\n\t\tgoto munmap_failed;\n\n\tvma = vma_prev(&vmi);\n\tret = do_brk_flags(&vmi, vma, addr, len, flags);\n\tpopulate = ((mm->def_flags & VM_LOCKED) != 0);\n\tmmap_write_unlock(mm);\n\tuserfaultfd_unmap_complete(mm, &uf);\n\tif (populate && !ret)\n\t\tmm_populate(addr, len);\n\treturn ret;\n\nmunmap_failed:\nlimits_failed:\n\tmmap_write_unlock(mm);\n\treturn ret;\n}\nEXPORT_SYMBOL(vm_brk_flags);\n\nint vm_brk(unsigned long addr, unsigned long len)\n{\n\treturn vm_brk_flags(addr, len, 0);\n}\nEXPORT_SYMBOL(vm_brk);\n\n \nvoid exit_mmap(struct mm_struct *mm)\n{\n\tstruct mmu_gather tlb;\n\tstruct vm_area_struct *vma;\n\tunsigned long nr_accounted = 0;\n\tMA_STATE(mas, &mm->mm_mt, 0, 0);\n\tint count = 0;\n\n\t \n\tmmu_notifier_release(mm);\n\n\tmmap_read_lock(mm);\n\tarch_exit_mmap(mm);\n\n\tvma = mas_find(&mas, ULONG_MAX);\n\tif (!vma) {\n\t\t \n\t\tmmap_read_unlock(mm);\n\t\treturn;\n\t}\n\n\tlru_add_drain();\n\tflush_cache_mm(mm);\n\ttlb_gather_mmu_fullmm(&tlb, mm);\n\t \n\t \n\tunmap_vmas(&tlb, &mas, vma, 0, ULONG_MAX, ULONG_MAX, false);\n\tmmap_read_unlock(mm);\n\n\t \n\tset_bit(MMF_OOM_SKIP, &mm->flags);\n\tmmap_write_lock(mm);\n\tmt_clear_in_rcu(&mm->mm_mt);\n\tmas_set(&mas, vma->vm_end);\n\tfree_pgtables(&tlb, &mas, vma, FIRST_USER_ADDRESS,\n\t\t      USER_PGTABLES_CEILING, true);\n\ttlb_finish_mmu(&tlb);\n\n\t \n\tmas_set(&mas, vma->vm_end);\n\tdo {\n\t\tif (vma->vm_flags & VM_ACCOUNT)\n\t\t\tnr_accounted += vma_pages(vma);\n\t\tremove_vma(vma, true);\n\t\tcount++;\n\t\tcond_resched();\n\t} while ((vma = mas_find(&mas, ULONG_MAX)) != NULL);\n\n\tBUG_ON(count != mm->map_count);\n\n\ttrace_exit_mmap(mm);\n\t__mt_destroy(&mm->mm_mt);\n\tmmap_write_unlock(mm);\n\tvm_unacct_memory(nr_accounted);\n}\n\n \nint insert_vm_struct(struct mm_struct *mm, struct vm_area_struct *vma)\n{\n\tunsigned long charged = vma_pages(vma);\n\n\n\tif (find_vma_intersection(mm, vma->vm_start, vma->vm_end))\n\t\treturn -ENOMEM;\n\n\tif ((vma->vm_flags & VM_ACCOUNT) &&\n\t     security_vm_enough_memory_mm(mm, charged))\n\t\treturn -ENOMEM;\n\n\t \n\tif (vma_is_anonymous(vma)) {\n\t\tBUG_ON(vma->anon_vma);\n\t\tvma->vm_pgoff = vma->vm_start >> PAGE_SHIFT;\n\t}\n\n\tif (vma_link(mm, vma)) {\n\t\tvm_unacct_memory(charged);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\n \nstruct vm_area_struct *copy_vma(struct vm_area_struct **vmap,\n\tunsigned long addr, unsigned long len, pgoff_t pgoff,\n\tbool *need_rmap_locks)\n{\n\tstruct vm_area_struct *vma = *vmap;\n\tunsigned long vma_start = vma->vm_start;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct vm_area_struct *new_vma, *prev;\n\tbool faulted_in_anon_vma = true;\n\tVMA_ITERATOR(vmi, mm, addr);\n\n\t \n\tif (unlikely(vma_is_anonymous(vma) && !vma->anon_vma)) {\n\t\tpgoff = addr >> PAGE_SHIFT;\n\t\tfaulted_in_anon_vma = false;\n\t}\n\n\tnew_vma = find_vma_prev(mm, addr, &prev);\n\tif (new_vma && new_vma->vm_start < addr + len)\n\t\treturn NULL;\t \n\n\tnew_vma = vma_merge(&vmi, mm, prev, addr, addr + len, vma->vm_flags,\n\t\t\t    vma->anon_vma, vma->vm_file, pgoff, vma_policy(vma),\n\t\t\t    vma->vm_userfaultfd_ctx, anon_vma_name(vma));\n\tif (new_vma) {\n\t\t \n\t\tif (unlikely(vma_start >= new_vma->vm_start &&\n\t\t\t     vma_start < new_vma->vm_end)) {\n\t\t\t \n\t\t\tVM_BUG_ON_VMA(faulted_in_anon_vma, new_vma);\n\t\t\t*vmap = vma = new_vma;\n\t\t}\n\t\t*need_rmap_locks = (new_vma->vm_pgoff <= vma->vm_pgoff);\n\t} else {\n\t\tnew_vma = vm_area_dup(vma);\n\t\tif (!new_vma)\n\t\t\tgoto out;\n\t\tnew_vma->vm_start = addr;\n\t\tnew_vma->vm_end = addr + len;\n\t\tnew_vma->vm_pgoff = pgoff;\n\t\tif (vma_dup_policy(vma, new_vma))\n\t\t\tgoto out_free_vma;\n\t\tif (anon_vma_clone(new_vma, vma))\n\t\t\tgoto out_free_mempol;\n\t\tif (new_vma->vm_file)\n\t\t\tget_file(new_vma->vm_file);\n\t\tif (new_vma->vm_ops && new_vma->vm_ops->open)\n\t\t\tnew_vma->vm_ops->open(new_vma);\n\t\tif (vma_link(mm, new_vma))\n\t\t\tgoto out_vma_link;\n\t\t*need_rmap_locks = false;\n\t}\n\treturn new_vma;\n\nout_vma_link:\n\tif (new_vma->vm_ops && new_vma->vm_ops->close)\n\t\tnew_vma->vm_ops->close(new_vma);\n\n\tif (new_vma->vm_file)\n\t\tfput(new_vma->vm_file);\n\n\tunlink_anon_vmas(new_vma);\nout_free_mempol:\n\tmpol_put(vma_policy(new_vma));\nout_free_vma:\n\tvm_area_free(new_vma);\nout:\n\treturn NULL;\n}\n\n \nbool may_expand_vm(struct mm_struct *mm, vm_flags_t flags, unsigned long npages)\n{\n\tif (mm->total_vm + npages > rlimit(RLIMIT_AS) >> PAGE_SHIFT)\n\t\treturn false;\n\n\tif (is_data_mapping(flags) &&\n\t    mm->data_vm + npages > rlimit(RLIMIT_DATA) >> PAGE_SHIFT) {\n\t\t \n\t\tif (rlimit(RLIMIT_DATA) == 0 &&\n\t\t    mm->data_vm + npages <= rlimit_max(RLIMIT_DATA) >> PAGE_SHIFT)\n\t\t\treturn true;\n\n\t\tpr_warn_once(\"%s (%d): VmData %lu exceed data ulimit %lu. Update limits%s.\\n\",\n\t\t\t     current->comm, current->pid,\n\t\t\t     (mm->data_vm + npages) << PAGE_SHIFT,\n\t\t\t     rlimit(RLIMIT_DATA),\n\t\t\t     ignore_rlimit_data ? \"\" : \" or use boot option ignore_rlimit_data\");\n\n\t\tif (!ignore_rlimit_data)\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nvoid vm_stat_account(struct mm_struct *mm, vm_flags_t flags, long npages)\n{\n\tWRITE_ONCE(mm->total_vm, READ_ONCE(mm->total_vm)+npages);\n\n\tif (is_exec_mapping(flags))\n\t\tmm->exec_vm += npages;\n\telse if (is_stack_mapping(flags))\n\t\tmm->stack_vm += npages;\n\telse if (is_data_mapping(flags))\n\t\tmm->data_vm += npages;\n}\n\nstatic vm_fault_t special_mapping_fault(struct vm_fault *vmf);\n\n \nstatic void special_mapping_close(struct vm_area_struct *vma)\n{\n}\n\nstatic const char *special_mapping_name(struct vm_area_struct *vma)\n{\n\treturn ((struct vm_special_mapping *)vma->vm_private_data)->name;\n}\n\nstatic int special_mapping_mremap(struct vm_area_struct *new_vma)\n{\n\tstruct vm_special_mapping *sm = new_vma->vm_private_data;\n\n\tif (WARN_ON_ONCE(current->mm != new_vma->vm_mm))\n\t\treturn -EFAULT;\n\n\tif (sm->mremap)\n\t\treturn sm->mremap(sm, new_vma);\n\n\treturn 0;\n}\n\nstatic int special_mapping_split(struct vm_area_struct *vma, unsigned long addr)\n{\n\t \n\treturn -EINVAL;\n}\n\nstatic const struct vm_operations_struct special_mapping_vmops = {\n\t.close = special_mapping_close,\n\t.fault = special_mapping_fault,\n\t.mremap = special_mapping_mremap,\n\t.name = special_mapping_name,\n\t \n\t.access = NULL,\n\t.may_split = special_mapping_split,\n};\n\nstatic const struct vm_operations_struct legacy_special_mapping_vmops = {\n\t.close = special_mapping_close,\n\t.fault = special_mapping_fault,\n};\n\nstatic vm_fault_t special_mapping_fault(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tpgoff_t pgoff;\n\tstruct page **pages;\n\n\tif (vma->vm_ops == &legacy_special_mapping_vmops) {\n\t\tpages = vma->vm_private_data;\n\t} else {\n\t\tstruct vm_special_mapping *sm = vma->vm_private_data;\n\n\t\tif (sm->fault)\n\t\t\treturn sm->fault(sm, vmf->vma, vmf);\n\n\t\tpages = sm->pages;\n\t}\n\n\tfor (pgoff = vmf->pgoff; pgoff && *pages; ++pages)\n\t\tpgoff--;\n\n\tif (*pages) {\n\t\tstruct page *page = *pages;\n\t\tget_page(page);\n\t\tvmf->page = page;\n\t\treturn 0;\n\t}\n\n\treturn VM_FAULT_SIGBUS;\n}\n\nstatic struct vm_area_struct *__install_special_mapping(\n\tstruct mm_struct *mm,\n\tunsigned long addr, unsigned long len,\n\tunsigned long vm_flags, void *priv,\n\tconst struct vm_operations_struct *ops)\n{\n\tint ret;\n\tstruct vm_area_struct *vma;\n\n\tvma = vm_area_alloc(mm);\n\tif (unlikely(vma == NULL))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tvma->vm_start = addr;\n\tvma->vm_end = addr + len;\n\n\tvm_flags_init(vma, (vm_flags | mm->def_flags |\n\t\t      VM_DONTEXPAND | VM_SOFTDIRTY) & ~VM_LOCKED_MASK);\n\tvma->vm_page_prot = vm_get_page_prot(vma->vm_flags);\n\n\tvma->vm_ops = ops;\n\tvma->vm_private_data = priv;\n\n\tret = insert_vm_struct(mm, vma);\n\tif (ret)\n\t\tgoto out;\n\n\tvm_stat_account(mm, vma->vm_flags, len >> PAGE_SHIFT);\n\n\tperf_event_mmap(vma);\n\n\treturn vma;\n\nout:\n\tvm_area_free(vma);\n\treturn ERR_PTR(ret);\n}\n\nbool vma_is_special_mapping(const struct vm_area_struct *vma,\n\tconst struct vm_special_mapping *sm)\n{\n\treturn vma->vm_private_data == sm &&\n\t\t(vma->vm_ops == &special_mapping_vmops ||\n\t\t vma->vm_ops == &legacy_special_mapping_vmops);\n}\n\n \nstruct vm_area_struct *_install_special_mapping(\n\tstruct mm_struct *mm,\n\tunsigned long addr, unsigned long len,\n\tunsigned long vm_flags, const struct vm_special_mapping *spec)\n{\n\treturn __install_special_mapping(mm, addr, len, vm_flags, (void *)spec,\n\t\t\t\t\t&special_mapping_vmops);\n}\n\nint install_special_mapping(struct mm_struct *mm,\n\t\t\t    unsigned long addr, unsigned long len,\n\t\t\t    unsigned long vm_flags, struct page **pages)\n{\n\tstruct vm_area_struct *vma = __install_special_mapping(\n\t\tmm, addr, len, vm_flags, (void *)pages,\n\t\t&legacy_special_mapping_vmops);\n\n\treturn PTR_ERR_OR_ZERO(vma);\n}\n\nstatic DEFINE_MUTEX(mm_all_locks_mutex);\n\nstatic void vm_lock_anon_vma(struct mm_struct *mm, struct anon_vma *anon_vma)\n{\n\tif (!test_bit(0, (unsigned long *) &anon_vma->root->rb_root.rb_root.rb_node)) {\n\t\t \n\t\tdown_write_nest_lock(&anon_vma->root->rwsem, &mm->mmap_lock);\n\t\t \n\t\tif (__test_and_set_bit(0, (unsigned long *)\n\t\t\t\t       &anon_vma->root->rb_root.rb_root.rb_node))\n\t\t\tBUG();\n\t}\n}\n\nstatic void vm_lock_mapping(struct mm_struct *mm, struct address_space *mapping)\n{\n\tif (!test_bit(AS_MM_ALL_LOCKS, &mapping->flags)) {\n\t\t \n\t\tif (test_and_set_bit(AS_MM_ALL_LOCKS, &mapping->flags))\n\t\t\tBUG();\n\t\tdown_write_nest_lock(&mapping->i_mmap_rwsem, &mm->mmap_lock);\n\t}\n}\n\n \nint mm_take_all_locks(struct mm_struct *mm)\n{\n\tstruct vm_area_struct *vma;\n\tstruct anon_vma_chain *avc;\n\tMA_STATE(mas, &mm->mm_mt, 0, 0);\n\n\tmmap_assert_write_locked(mm);\n\n\tmutex_lock(&mm_all_locks_mutex);\n\n\t \n\tmas_for_each(&mas, vma, ULONG_MAX) {\n\t\tif (signal_pending(current))\n\t\t\tgoto out_unlock;\n\t\tvma_start_write(vma);\n\t}\n\n\tmas_set(&mas, 0);\n\tmas_for_each(&mas, vma, ULONG_MAX) {\n\t\tif (signal_pending(current))\n\t\t\tgoto out_unlock;\n\t\tif (vma->vm_file && vma->vm_file->f_mapping &&\n\t\t\t\tis_vm_hugetlb_page(vma))\n\t\t\tvm_lock_mapping(mm, vma->vm_file->f_mapping);\n\t}\n\n\tmas_set(&mas, 0);\n\tmas_for_each(&mas, vma, ULONG_MAX) {\n\t\tif (signal_pending(current))\n\t\t\tgoto out_unlock;\n\t\tif (vma->vm_file && vma->vm_file->f_mapping &&\n\t\t\t\t!is_vm_hugetlb_page(vma))\n\t\t\tvm_lock_mapping(mm, vma->vm_file->f_mapping);\n\t}\n\n\tmas_set(&mas, 0);\n\tmas_for_each(&mas, vma, ULONG_MAX) {\n\t\tif (signal_pending(current))\n\t\t\tgoto out_unlock;\n\t\tif (vma->anon_vma)\n\t\t\tlist_for_each_entry(avc, &vma->anon_vma_chain, same_vma)\n\t\t\t\tvm_lock_anon_vma(mm, avc->anon_vma);\n\t}\n\n\treturn 0;\n\nout_unlock:\n\tmm_drop_all_locks(mm);\n\treturn -EINTR;\n}\n\nstatic void vm_unlock_anon_vma(struct anon_vma *anon_vma)\n{\n\tif (test_bit(0, (unsigned long *) &anon_vma->root->rb_root.rb_root.rb_node)) {\n\t\t \n\t\tif (!__test_and_clear_bit(0, (unsigned long *)\n\t\t\t\t\t  &anon_vma->root->rb_root.rb_root.rb_node))\n\t\t\tBUG();\n\t\tanon_vma_unlock_write(anon_vma);\n\t}\n}\n\nstatic void vm_unlock_mapping(struct address_space *mapping)\n{\n\tif (test_bit(AS_MM_ALL_LOCKS, &mapping->flags)) {\n\t\t \n\t\ti_mmap_unlock_write(mapping);\n\t\tif (!test_and_clear_bit(AS_MM_ALL_LOCKS,\n\t\t\t\t\t&mapping->flags))\n\t\t\tBUG();\n\t}\n}\n\n \nvoid mm_drop_all_locks(struct mm_struct *mm)\n{\n\tstruct vm_area_struct *vma;\n\tstruct anon_vma_chain *avc;\n\tMA_STATE(mas, &mm->mm_mt, 0, 0);\n\n\tmmap_assert_write_locked(mm);\n\tBUG_ON(!mutex_is_locked(&mm_all_locks_mutex));\n\n\tmas_for_each(&mas, vma, ULONG_MAX) {\n\t\tif (vma->anon_vma)\n\t\t\tlist_for_each_entry(avc, &vma->anon_vma_chain, same_vma)\n\t\t\t\tvm_unlock_anon_vma(avc->anon_vma);\n\t\tif (vma->vm_file && vma->vm_file->f_mapping)\n\t\t\tvm_unlock_mapping(vma->vm_file->f_mapping);\n\t}\n\n\tmutex_unlock(&mm_all_locks_mutex);\n}\n\n \nvoid __init mmap_init(void)\n{\n\tint ret;\n\n\tret = percpu_counter_init(&vm_committed_as, 0, GFP_KERNEL);\n\tVM_BUG_ON(ret);\n}\n\n \nstatic int init_user_reserve(void)\n{\n\tunsigned long free_kbytes;\n\n\tfree_kbytes = K(global_zone_page_state(NR_FREE_PAGES));\n\n\tsysctl_user_reserve_kbytes = min(free_kbytes / 32, 1UL << 17);\n\treturn 0;\n}\nsubsys_initcall(init_user_reserve);\n\n \nstatic int init_admin_reserve(void)\n{\n\tunsigned long free_kbytes;\n\n\tfree_kbytes = K(global_zone_page_state(NR_FREE_PAGES));\n\n\tsysctl_admin_reserve_kbytes = min(free_kbytes / 32, 1UL << 13);\n\treturn 0;\n}\nsubsys_initcall(init_admin_reserve);\n\n \nstatic int reserve_mem_notifier(struct notifier_block *nb,\n\t\t\t     unsigned long action, void *data)\n{\n\tunsigned long tmp, free_kbytes;\n\n\tswitch (action) {\n\tcase MEM_ONLINE:\n\t\t \n\t\ttmp = sysctl_user_reserve_kbytes;\n\t\tif (0 < tmp && tmp < (1UL << 17))\n\t\t\tinit_user_reserve();\n\n\t\t \n\t\ttmp = sysctl_admin_reserve_kbytes;\n\t\tif (0 < tmp && tmp < (1UL << 13))\n\t\t\tinit_admin_reserve();\n\n\t\tbreak;\n\tcase MEM_OFFLINE:\n\t\tfree_kbytes = K(global_zone_page_state(NR_FREE_PAGES));\n\n\t\tif (sysctl_user_reserve_kbytes > free_kbytes) {\n\t\t\tinit_user_reserve();\n\t\t\tpr_info(\"vm.user_reserve_kbytes reset to %lu\\n\",\n\t\t\t\tsysctl_user_reserve_kbytes);\n\t\t}\n\n\t\tif (sysctl_admin_reserve_kbytes > free_kbytes) {\n\t\t\tinit_admin_reserve();\n\t\t\tpr_info(\"vm.admin_reserve_kbytes reset to %lu\\n\",\n\t\t\t\tsysctl_admin_reserve_kbytes);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn NOTIFY_OK;\n}\n\nstatic int __meminit init_reserve_notifier(void)\n{\n\tif (hotplug_memory_notifier(reserve_mem_notifier, DEFAULT_CALLBACK_PRI))\n\t\tpr_err(\"Failed registering memory add/remove notifier for admin reserve\\n\");\n\n\treturn 0;\n}\nsubsys_initcall(init_reserve_notifier);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}