{
  "module_name": "alloc.rs",
  "hash_id": "469218c9cb3ecb5aa4bfa945c2694acb31fcaa54393e3cffe9494afa25328718",
  "original_prompt": "Ingested from linux-6.6.14/rust/alloc/alloc.rs",
  "human_readable_source": "// SPDX-License-Identifier: Apache-2.0 OR MIT\n\n//! Memory allocation APIs\n\n#![stable(feature = \"alloc_module\", since = \"1.28.0\")]\n\n#[cfg(not(test))]\nuse core::intrinsics;\nuse core::intrinsics::{min_align_of_val, size_of_val};\n\nuse core::ptr::Unique;\n#[cfg(not(test))]\nuse core::ptr::{self, NonNull};\n\n#[stable(feature = \"alloc_module\", since = \"1.28.0\")]\n#[doc(inline)]\npub use core::alloc::*;\n\n#[cfg(test)]\nmod tests;\n\nextern \"Rust\" {\n    // These are the magic symbols to call the global allocator. rustc generates\n    // them to call `__rg_alloc` etc. if there is a `#[global_allocator]` attribute\n    // (the code expanding that attribute macro generates those functions), or to call\n    // the default implementations in std (`__rdl_alloc` etc. in `library/std/src/alloc.rs`)\n    // otherwise.\n    // The rustc fork of LLVM 14 and earlier also special-cases these function names to be able to optimize them\n    // like `malloc`, `realloc`, and `free`, respectively.\n    #[rustc_allocator]\n    #[rustc_nounwind]\n    fn __rust_alloc(size: usize, align: usize) -> *mut u8;\n    #[rustc_deallocator]\n    #[rustc_nounwind]\n    fn __rust_dealloc(ptr: *mut u8, size: usize, align: usize);\n    #[rustc_reallocator]\n    #[rustc_nounwind]\n    fn __rust_realloc(ptr: *mut u8, old_size: usize, align: usize, new_size: usize) -> *mut u8;\n    #[rustc_allocator_zeroed]\n    #[rustc_nounwind]\n    fn __rust_alloc_zeroed(size: usize, align: usize) -> *mut u8;\n\n    #[cfg(not(bootstrap))]\n    static __rust_no_alloc_shim_is_unstable: u8;\n}\n\n/// The global memory allocator.\n///\n/// This type implements the [`Allocator`] trait by forwarding calls\n/// to the allocator registered with the `#[global_allocator]` attribute\n/// if there is one, or the `std` crate\u2019s default.\n///\n/// Note: while this type is unstable, the functionality it provides can be\n/// accessed through the [free functions in `alloc`](self#functions).\n#[unstable(feature = \"allocator_api\", issue = \"32838\")]\n#[derive(Copy, Clone, Default, Debug)]\n#[cfg(not(test))]\npub struct Global;\n\n#[cfg(test)]\npub use std::alloc::Global;\n\n/// Allocate memory with the global allocator.\n///\n/// This function forwards calls to the [`GlobalAlloc::alloc`] method\n/// of the allocator registered with the `#[global_allocator]` attribute\n/// if there is one, or the `std` crate\u2019s default.\n///\n/// This function is expected to be deprecated in favor of the `alloc` method\n/// of the [`Global`] type when it and the [`Allocator`] trait become stable.\n///\n/// # Safety\n///\n/// See [`GlobalAlloc::alloc`].\n///\n/// # Examples\n///\n/// ```\n/// use std::alloc::{alloc, dealloc, handle_alloc_error, Layout};\n///\n/// unsafe {\n///     let layout = Layout::new::<u16>();\n///     let ptr = alloc(layout);\n///     if ptr.is_null() {\n///         handle_alloc_error(layout);\n///     }\n///\n///     *(ptr as *mut u16) = 42;\n///     assert_eq!(*(ptr as *mut u16), 42);\n///\n///     dealloc(ptr, layout);\n/// }\n/// ```\n#[stable(feature = \"global_alloc\", since = \"1.28.0\")]\n#[must_use = \"losing the pointer will leak memory\"]\n#[inline]\npub unsafe fn alloc(layout: Layout) -> *mut u8 {\n    unsafe {\n        // Make sure we don't accidentally allow omitting the allocator shim in\n        // stable code until it is actually stabilized.\n        #[cfg(not(bootstrap))]\n        core::ptr::read_volatile(&__rust_no_alloc_shim_is_unstable);\n\n        __rust_alloc(layout.size(), layout.align())\n    }\n}\n\n/// Deallocate memory with the global allocator.\n///\n/// This function forwards calls to the [`GlobalAlloc::dealloc`] method\n/// of the allocator registered with the `#[global_allocator]` attribute\n/// if there is one, or the `std` crate\u2019s default.\n///\n/// This function is expected to be deprecated in favor of the `dealloc` method\n/// of the [`Global`] type when it and the [`Allocator`] trait become stable.\n///\n/// # Safety\n///\n/// See [`GlobalAlloc::dealloc`].\n#[stable(feature = \"global_alloc\", since = \"1.28.0\")]\n#[inline]\npub unsafe fn dealloc(ptr: *mut u8, layout: Layout) {\n    unsafe { __rust_dealloc(ptr, layout.size(), layout.align()) }\n}\n\n/// Reallocate memory with the global allocator.\n///\n/// This function forwards calls to the [`GlobalAlloc::realloc`] method\n/// of the allocator registered with the `#[global_allocator]` attribute\n/// if there is one, or the `std` crate\u2019s default.\n///\n/// This function is expected to be deprecated in favor of the `realloc` method\n/// of the [`Global`] type when it and the [`Allocator`] trait become stable.\n///\n/// # Safety\n///\n/// See [`GlobalAlloc::realloc`].\n#[stable(feature = \"global_alloc\", since = \"1.28.0\")]\n#[must_use = \"losing the pointer will leak memory\"]\n#[inline]\npub unsafe fn realloc(ptr: *mut u8, layout: Layout, new_size: usize) -> *mut u8 {\n    unsafe { __rust_realloc(ptr, layout.size(), layout.align(), new_size) }\n}\n\n/// Allocate zero-initialized memory with the global allocator.\n///\n/// This function forwards calls to the [`GlobalAlloc::alloc_zeroed`] method\n/// of the allocator registered with the `#[global_allocator]` attribute\n/// if there is one, or the `std` crate\u2019s default.\n///\n/// This function is expected to be deprecated in favor of the `alloc_zeroed` method\n/// of the [`Global`] type when it and the [`Allocator`] trait become stable.\n///\n/// # Safety\n///\n/// See [`GlobalAlloc::alloc_zeroed`].\n///\n/// # Examples\n///\n/// ```\n/// use std::alloc::{alloc_zeroed, dealloc, Layout};\n///\n/// unsafe {\n///     let layout = Layout::new::<u16>();\n///     let ptr = alloc_zeroed(layout);\n///\n///     assert_eq!(*(ptr as *mut u16), 0);\n///\n///     dealloc(ptr, layout);\n/// }\n/// ```\n#[stable(feature = \"global_alloc\", since = \"1.28.0\")]\n#[must_use = \"losing the pointer will leak memory\"]\n#[inline]\npub unsafe fn alloc_zeroed(layout: Layout) -> *mut u8 {\n    unsafe { __rust_alloc_zeroed(layout.size(), layout.align()) }\n}\n\n#[cfg(not(test))]\nimpl Global {\n    #[inline]\n    fn alloc_impl(&self, layout: Layout, zeroed: bool) -> Result<NonNull<[u8]>, AllocError> {\n        match layout.size() {\n            0 => Ok(NonNull::slice_from_raw_parts(layout.dangling(), 0)),\n            // SAFETY: `layout` is non-zero in size,\n            size => unsafe {\n                let raw_ptr = if zeroed { alloc_zeroed(layout) } else { alloc(layout) };\n                let ptr = NonNull::new(raw_ptr).ok_or(AllocError)?;\n                Ok(NonNull::slice_from_raw_parts(ptr, size))\n            },\n        }\n    }\n\n    // SAFETY: Same as `Allocator::grow`\n    #[inline]\n    unsafe fn grow_impl(\n        &self,\n        ptr: NonNull<u8>,\n        old_layout: Layout,\n        new_layout: Layout,\n        zeroed: bool,\n    ) -> Result<NonNull<[u8]>, AllocError> {\n        debug_assert!(\n            new_layout.size() >= old_layout.size(),\n            \"`new_layout.size()` must be greater than or equal to `old_layout.size()`\"\n        );\n\n        match old_layout.size() {\n            0 => self.alloc_impl(new_layout, zeroed),\n\n            // SAFETY: `new_size` is non-zero as `old_size` is greater than or equal to `new_size`\n            // as required by safety conditions. Other conditions must be upheld by the caller\n            old_size if old_layout.align() == new_layout.align() => unsafe {\n                let new_size = new_layout.size();\n\n                // `realloc` probably checks for `new_size >= old_layout.size()` or something similar.\n                intrinsics::assume(new_size >= old_layout.size());\n\n                let raw_ptr = realloc(ptr.as_ptr(), old_layout, new_size);\n                let ptr = NonNull::new(raw_ptr).ok_or(AllocError)?;\n                if zeroed {\n                    raw_ptr.add(old_size).write_bytes(0, new_size - old_size);\n                }\n                Ok(NonNull::slice_from_raw_parts(ptr, new_size))\n            },\n\n            // SAFETY: because `new_layout.size()` must be greater than or equal to `old_size`,\n            // both the old and new memory allocation are valid for reads and writes for `old_size`\n            // bytes. Also, because the old allocation wasn't yet deallocated, it cannot overlap\n            // `new_ptr`. Thus, the call to `copy_nonoverlapping` is safe. The safety contract\n            // for `dealloc` must be upheld by the caller.\n            old_size => unsafe {\n                let new_ptr = self.alloc_impl(new_layout, zeroed)?;\n                ptr::copy_nonoverlapping(ptr.as_ptr(), new_ptr.as_mut_ptr(), old_size);\n                self.deallocate(ptr, old_layout);\n                Ok(new_ptr)\n            },\n        }\n    }\n}\n\n#[unstable(feature = \"allocator_api\", issue = \"32838\")]\n#[cfg(not(test))]\nunsafe impl Allocator for Global {\n    #[inline]\n    fn allocate(&self, layout: Layout) -> Result<NonNull<[u8]>, AllocError> {\n        self.alloc_impl(layout, false)\n    }\n\n    #[inline]\n    fn allocate_zeroed(&self, layout: Layout) -> Result<NonNull<[u8]>, AllocError> {\n        self.alloc_impl(layout, true)\n    }\n\n    #[inline]\n    unsafe fn deallocate(&self, ptr: NonNull<u8>, layout: Layout) {\n        if layout.size() != 0 {\n            // SAFETY: `layout` is non-zero in size,\n            // other conditions must be upheld by the caller\n            unsafe { dealloc(ptr.as_ptr(), layout) }\n        }\n    }\n\n    #[inline]\n    unsafe fn grow(\n        &self,\n        ptr: NonNull<u8>,\n        old_layout: Layout,\n        new_layout: Layout,\n    ) -> Result<NonNull<[u8]>, AllocError> {\n        // SAFETY: all conditions must be upheld by the caller\n        unsafe { self.grow_impl(ptr, old_layout, new_layout, false) }\n    }\n\n    #[inline]\n    unsafe fn grow_zeroed(\n        &self,\n        ptr: NonNull<u8>,\n        old_layout: Layout,\n        new_layout: Layout,\n    ) -> Result<NonNull<[u8]>, AllocError> {\n        // SAFETY: all conditions must be upheld by the caller\n        unsafe { self.grow_impl(ptr, old_layout, new_layout, true) }\n    }\n\n    #[inline]\n    unsafe fn shrink(\n        &self,\n        ptr: NonNull<u8>,\n        old_layout: Layout,\n        new_layout: Layout,\n    ) -> Result<NonNull<[u8]>, AllocError> {\n        debug_assert!(\n            new_layout.size() <= old_layout.size(),\n            \"`new_layout.size()` must be smaller than or equal to `old_layout.size()`\"\n        );\n\n        match new_layout.size() {\n            // SAFETY: conditions must be upheld by the caller\n            0 => unsafe {\n                self.deallocate(ptr, old_layout);\n                Ok(NonNull::slice_from_raw_parts(new_layout.dangling(), 0))\n            },\n\n            // SAFETY: `new_size` is non-zero. Other conditions must be upheld by the caller\n            new_size if old_layout.align() == new_layout.align() => unsafe {\n                // `realloc` probably checks for `new_size <= old_layout.size()` or something similar.\n                intrinsics::assume(new_size <= old_layout.size());\n\n                let raw_ptr = realloc(ptr.as_ptr(), old_layout, new_size);\n                let ptr = NonNull::new(raw_ptr).ok_or(AllocError)?;\n                Ok(NonNull::slice_from_raw_parts(ptr, new_size))\n            },\n\n            // SAFETY: because `new_size` must be smaller than or equal to `old_layout.size()`,\n            // both the old and new memory allocation are valid for reads and writes for `new_size`\n            // bytes. Also, because the old allocation wasn't yet deallocated, it cannot overlap\n            // `new_ptr`. Thus, the call to `copy_nonoverlapping` is safe. The safety contract\n            // for `dealloc` must be upheld by the caller.\n            new_size => unsafe {\n                let new_ptr = self.allocate(new_layout)?;\n                ptr::copy_nonoverlapping(ptr.as_ptr(), new_ptr.as_mut_ptr(), new_size);\n                self.deallocate(ptr, old_layout);\n                Ok(new_ptr)\n            },\n        }\n    }\n}\n\n/// The allocator for unique pointers.\n#[cfg(all(not(no_global_oom_handling), not(test)))]\n#[lang = \"exchange_malloc\"]\n#[inline]\nunsafe fn exchange_malloc(size: usize, align: usize) -> *mut u8 {\n    let layout = unsafe { Layout::from_size_align_unchecked(size, align) };\n    match Global.allocate(layout) {\n        Ok(ptr) => ptr.as_mut_ptr(),\n        Err(_) => handle_alloc_error(layout),\n    }\n}\n\n#[cfg_attr(not(test), lang = \"box_free\")]\n#[inline]\n// This signature has to be the same as `Box`, otherwise an ICE will happen.\n// When an additional parameter to `Box` is added (like `A: Allocator`), this has to be added here as\n// well.\n// For example if `Box` is changed to  `struct Box<T: ?Sized, A: Allocator>(Unique<T>, A)`,\n// this function has to be changed to `fn box_free<T: ?Sized, A: Allocator>(Unique<T>, A)` as well.\npub(crate) unsafe fn box_free<T: ?Sized, A: Allocator>(ptr: Unique<T>, alloc: A) {\n    unsafe {\n        let size = size_of_val(ptr.as_ref());\n        let align = min_align_of_val(ptr.as_ref());\n        let layout = Layout::from_size_align_unchecked(size, align);\n        alloc.deallocate(From::from(ptr.cast()), layout)\n    }\n}\n\n// # Allocation error handler\n\n#[cfg(not(no_global_oom_handling))]\nextern \"Rust\" {\n    // This is the magic symbol to call the global alloc error handler. rustc generates\n    // it to call `__rg_oom` if there is a `#[alloc_error_handler]`, or to call the\n    // default implementations below (`__rdl_oom`) otherwise.\n    fn __rust_alloc_error_handler(size: usize, align: usize) -> !;\n}\n\n/// Abort on memory allocation error or failure.\n///\n/// Callers of memory allocation APIs wishing to abort computation\n/// in response to an allocation error are encouraged to call this function,\n/// rather than directly invoking `panic!` or similar.\n///\n/// The default behavior of this function is to print a message to standard error\n/// and abort the process.\n/// It can be replaced with [`set_alloc_error_hook`] and [`take_alloc_error_hook`].\n///\n/// [`set_alloc_error_hook`]: ../../std/alloc/fn.set_alloc_error_hook.html\n/// [`take_alloc_error_hook`]: ../../std/alloc/fn.take_alloc_error_hook.html\n#[stable(feature = \"global_alloc\", since = \"1.28.0\")]\n#[rustc_const_unstable(feature = \"const_alloc_error\", issue = \"92523\")]\n#[cfg(all(not(no_global_oom_handling), not(test)))]\n#[cold]\npub const fn handle_alloc_error(layout: Layout) -> ! {\n    const fn ct_error(_: Layout) -> ! {\n        panic!(\"allocation failed\");\n    }\n\n    fn rt_error(layout: Layout) -> ! {\n        unsafe {\n            __rust_alloc_error_handler(layout.size(), layout.align());\n        }\n    }\n\n    unsafe { core::intrinsics::const_eval_select((layout,), ct_error, rt_error) }\n}\n\n// For alloc test `std::alloc::handle_alloc_error` can be used directly.\n#[cfg(all(not(no_global_oom_handling), test))]\npub use std::alloc::handle_alloc_error;\n\n#[cfg(all(not(no_global_oom_handling), not(test)))]\n#[doc(hidden)]\n#[allow(unused_attributes)]\n#[unstable(feature = \"alloc_internals\", issue = \"none\")]\npub mod __alloc_error_handler {\n    // called via generated `__rust_alloc_error_handler` if there is no\n    // `#[alloc_error_handler]`.\n    #[rustc_std_internal_symbol]\n    pub unsafe fn __rdl_oom(size: usize, _align: usize) -> ! {\n        extern \"Rust\" {\n            // This symbol is emitted by rustc next to __rust_alloc_error_handler.\n            // Its value depends on the -Zoom={panic,abort} compiler option.\n            static __rust_alloc_error_handler_should_panic: u8;\n        }\n\n        #[allow(unused_unsafe)]\n        if unsafe { __rust_alloc_error_handler_should_panic != 0 } {\n            panic!(\"memory allocation of {size} bytes failed\")\n        } else {\n            core::panicking::panic_nounwind_fmt(format_args!(\n                \"memory allocation of {size} bytes failed\"\n            ))\n        }\n    }\n}\n\n/// Specialize clones into pre-allocated, uninitialized memory.\n/// Used by `Box::clone` and `Rc`/`Arc::make_mut`.\npub(crate) trait WriteCloneIntoRaw: Sized {\n    unsafe fn write_clone_into_raw(&self, target: *mut Self);\n}\n\nimpl<T: Clone> WriteCloneIntoRaw for T {\n    #[inline]\n    default unsafe fn write_clone_into_raw(&self, target: *mut Self) {\n        // Having allocated *first* may allow the optimizer to create\n        // the cloned value in-place, skipping the local and move.\n        unsafe { target.write(self.clone()) };\n    }\n}\n\nimpl<T: Copy> WriteCloneIntoRaw for T {\n    #[inline]\n    unsafe fn write_clone_into_raw(&self, target: *mut Self) {\n        // We can always copy in-place, without ever involving a local value.\n        unsafe { target.copy_from_nonoverlapping(self, 1) };\n    }\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}