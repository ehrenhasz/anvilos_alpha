{
  "module_name": "stats.h",
  "hash_id": "da2891e4deffaf18b8621ad1a4f1cbc90bf8e26bd3b8e8a12020c0f44618cdca",
  "original_prompt": "Ingested from linux-6.6.14/kernel/sched/stats.h",
  "human_readable_source": " \n#ifndef _KERNEL_STATS_H\n#define _KERNEL_STATS_H\n\n#ifdef CONFIG_SCHEDSTATS\n\nextern struct static_key_false sched_schedstats;\n\n \nstatic inline void\nrq_sched_info_arrive(struct rq *rq, unsigned long long delta)\n{\n\tif (rq) {\n\t\trq->rq_sched_info.run_delay += delta;\n\t\trq->rq_sched_info.pcount++;\n\t}\n}\n\n \nstatic inline void\nrq_sched_info_depart(struct rq *rq, unsigned long long delta)\n{\n\tif (rq)\n\t\trq->rq_cpu_time += delta;\n}\n\nstatic inline void\nrq_sched_info_dequeue(struct rq *rq, unsigned long long delta)\n{\n\tif (rq)\n\t\trq->rq_sched_info.run_delay += delta;\n}\n#define   schedstat_enabled()\t\tstatic_branch_unlikely(&sched_schedstats)\n#define __schedstat_inc(var)\t\tdo { var++; } while (0)\n#define   schedstat_inc(var)\t\tdo { if (schedstat_enabled()) { var++; } } while (0)\n#define __schedstat_add(var, amt)\tdo { var += (amt); } while (0)\n#define   schedstat_add(var, amt)\tdo { if (schedstat_enabled()) { var += (amt); } } while (0)\n#define __schedstat_set(var, val)\tdo { var = (val); } while (0)\n#define   schedstat_set(var, val)\tdo { if (schedstat_enabled()) { var = (val); } } while (0)\n#define   schedstat_val(var)\t\t(var)\n#define   schedstat_val_or_zero(var)\t((schedstat_enabled()) ? (var) : 0)\n\nvoid __update_stats_wait_start(struct rq *rq, struct task_struct *p,\n\t\t\t       struct sched_statistics *stats);\n\nvoid __update_stats_wait_end(struct rq *rq, struct task_struct *p,\n\t\t\t     struct sched_statistics *stats);\nvoid __update_stats_enqueue_sleeper(struct rq *rq, struct task_struct *p,\n\t\t\t\t    struct sched_statistics *stats);\n\nstatic inline void\ncheck_schedstat_required(void)\n{\n\tif (schedstat_enabled())\n\t\treturn;\n\n\t \n\tif (trace_sched_stat_wait_enabled()    ||\n\t    trace_sched_stat_sleep_enabled()   ||\n\t    trace_sched_stat_iowait_enabled()  ||\n\t    trace_sched_stat_blocked_enabled() ||\n\t    trace_sched_stat_runtime_enabled())\n\t\tprintk_deferred_once(\"Scheduler tracepoints stat_sleep, stat_iowait, stat_blocked and stat_runtime require the kernel parameter schedstats=enable or kernel.sched_schedstats=1\\n\");\n}\n\n#else  \n\nstatic inline void rq_sched_info_arrive  (struct rq *rq, unsigned long long delta) { }\nstatic inline void rq_sched_info_dequeue(struct rq *rq, unsigned long long delta) { }\nstatic inline void rq_sched_info_depart  (struct rq *rq, unsigned long long delta) { }\n# define   schedstat_enabled()\t\t0\n# define __schedstat_inc(var)\t\tdo { } while (0)\n# define   schedstat_inc(var)\t\tdo { } while (0)\n# define __schedstat_add(var, amt)\tdo { } while (0)\n# define   schedstat_add(var, amt)\tdo { } while (0)\n# define __schedstat_set(var, val)\tdo { } while (0)\n# define   schedstat_set(var, val)\tdo { } while (0)\n# define   schedstat_val(var)\t\t0\n# define   schedstat_val_or_zero(var)\t0\n\n# define __update_stats_wait_start(rq, p, stats)       do { } while (0)\n# define __update_stats_wait_end(rq, p, stats)         do { } while (0)\n# define __update_stats_enqueue_sleeper(rq, p, stats)  do { } while (0)\n# define check_schedstat_required()                    do { } while (0)\n\n#endif  \n\n#ifdef CONFIG_FAIR_GROUP_SCHED\nstruct sched_entity_stats {\n\tstruct sched_entity     se;\n\tstruct sched_statistics stats;\n} __no_randomize_layout;\n#endif\n\nstatic inline struct sched_statistics *\n__schedstats_from_se(struct sched_entity *se)\n{\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tif (!entity_is_task(se))\n\t\treturn &container_of(se, struct sched_entity_stats, se)->stats;\n#endif\n\treturn &task_of(se)->stats;\n}\n\n#ifdef CONFIG_PSI\nvoid psi_task_change(struct task_struct *task, int clear, int set);\nvoid psi_task_switch(struct task_struct *prev, struct task_struct *next,\n\t\t     bool sleep);\nvoid psi_account_irqtime(struct task_struct *task, u32 delta);\n\n \nstatic inline void psi_enqueue(struct task_struct *p, bool wakeup)\n{\n\tint clear = 0, set = TSK_RUNNING;\n\n\tif (static_branch_likely(&psi_disabled))\n\t\treturn;\n\n\tif (p->in_memstall)\n\t\tset |= TSK_MEMSTALL_RUNNING;\n\n\tif (!wakeup) {\n\t\tif (p->in_memstall)\n\t\t\tset |= TSK_MEMSTALL;\n\t} else {\n\t\tif (p->in_iowait)\n\t\t\tclear |= TSK_IOWAIT;\n\t}\n\n\tpsi_task_change(p, clear, set);\n}\n\nstatic inline void psi_dequeue(struct task_struct *p, bool sleep)\n{\n\tif (static_branch_likely(&psi_disabled))\n\t\treturn;\n\n\t \n\tif (sleep)\n\t\treturn;\n\n\tpsi_task_change(p, p->psi_flags, 0);\n}\n\nstatic inline void psi_ttwu_dequeue(struct task_struct *p)\n{\n\tif (static_branch_likely(&psi_disabled))\n\t\treturn;\n\t \n\tif (unlikely(p->psi_flags)) {\n\t\tstruct rq_flags rf;\n\t\tstruct rq *rq;\n\n\t\trq = __task_rq_lock(p, &rf);\n\t\tpsi_task_change(p, p->psi_flags, 0);\n\t\t__task_rq_unlock(rq, &rf);\n\t}\n}\n\nstatic inline void psi_sched_switch(struct task_struct *prev,\n\t\t\t\t    struct task_struct *next,\n\t\t\t\t    bool sleep)\n{\n\tif (static_branch_likely(&psi_disabled))\n\t\treturn;\n\n\tpsi_task_switch(prev, next, sleep);\n}\n\n#else  \nstatic inline void psi_enqueue(struct task_struct *p, bool wakeup) {}\nstatic inline void psi_dequeue(struct task_struct *p, bool sleep) {}\nstatic inline void psi_ttwu_dequeue(struct task_struct *p) {}\nstatic inline void psi_sched_switch(struct task_struct *prev,\n\t\t\t\t    struct task_struct *next,\n\t\t\t\t    bool sleep) {}\nstatic inline void psi_account_irqtime(struct task_struct *task, u32 delta) {}\n#endif  \n\n#ifdef CONFIG_SCHED_INFO\n \nstatic inline void sched_info_dequeue(struct rq *rq, struct task_struct *t)\n{\n\tunsigned long long delta = 0;\n\n\tif (!t->sched_info.last_queued)\n\t\treturn;\n\n\tdelta = rq_clock(rq) - t->sched_info.last_queued;\n\tt->sched_info.last_queued = 0;\n\tt->sched_info.run_delay += delta;\n\n\trq_sched_info_dequeue(rq, delta);\n}\n\n \nstatic void sched_info_arrive(struct rq *rq, struct task_struct *t)\n{\n\tunsigned long long now, delta = 0;\n\n\tif (!t->sched_info.last_queued)\n\t\treturn;\n\n\tnow = rq_clock(rq);\n\tdelta = now - t->sched_info.last_queued;\n\tt->sched_info.last_queued = 0;\n\tt->sched_info.run_delay += delta;\n\tt->sched_info.last_arrival = now;\n\tt->sched_info.pcount++;\n\n\trq_sched_info_arrive(rq, delta);\n}\n\n \nstatic inline void sched_info_enqueue(struct rq *rq, struct task_struct *t)\n{\n\tif (!t->sched_info.last_queued)\n\t\tt->sched_info.last_queued = rq_clock(rq);\n}\n\n \nstatic inline void sched_info_depart(struct rq *rq, struct task_struct *t)\n{\n\tunsigned long long delta = rq_clock(rq) - t->sched_info.last_arrival;\n\n\trq_sched_info_depart(rq, delta);\n\n\tif (task_is_running(t))\n\t\tsched_info_enqueue(rq, t);\n}\n\n \nstatic inline void\nsched_info_switch(struct rq *rq, struct task_struct *prev, struct task_struct *next)\n{\n\t \n\tif (prev != rq->idle)\n\t\tsched_info_depart(rq, prev);\n\n\tif (next != rq->idle)\n\t\tsched_info_arrive(rq, next);\n}\n\n#else  \n# define sched_info_enqueue(rq, t)\tdo { } while (0)\n# define sched_info_dequeue(rq, t)\tdo { } while (0)\n# define sched_info_switch(rq, t, next)\tdo { } while (0)\n#endif  \n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}