{
  "module_name": "cpudeadline.c",
  "hash_id": "d92283260c97a0d49b94ba84442e941e9d7ed3e25a20dd985cbf497169e29b21",
  "original_prompt": "Ingested from linux-6.6.14/kernel/sched/cpudeadline.c",
  "human_readable_source": "\n \n\nstatic inline int parent(int i)\n{\n\treturn (i - 1) >> 1;\n}\n\nstatic inline int left_child(int i)\n{\n\treturn (i << 1) + 1;\n}\n\nstatic inline int right_child(int i)\n{\n\treturn (i << 1) + 2;\n}\n\nstatic void cpudl_heapify_down(struct cpudl *cp, int idx)\n{\n\tint l, r, largest;\n\n\tint orig_cpu = cp->elements[idx].cpu;\n\tu64 orig_dl = cp->elements[idx].dl;\n\n\tif (left_child(idx) >= cp->size)\n\t\treturn;\n\n\t \n\twhile (1) {\n\t\tu64 largest_dl;\n\n\t\tl = left_child(idx);\n\t\tr = right_child(idx);\n\t\tlargest = idx;\n\t\tlargest_dl = orig_dl;\n\n\t\tif ((l < cp->size) && dl_time_before(orig_dl,\n\t\t\t\t\t\tcp->elements[l].dl)) {\n\t\t\tlargest = l;\n\t\t\tlargest_dl = cp->elements[l].dl;\n\t\t}\n\t\tif ((r < cp->size) && dl_time_before(largest_dl,\n\t\t\t\t\t\tcp->elements[r].dl))\n\t\t\tlargest = r;\n\n\t\tif (largest == idx)\n\t\t\tbreak;\n\n\t\t \n\t\tcp->elements[idx].cpu = cp->elements[largest].cpu;\n\t\tcp->elements[idx].dl = cp->elements[largest].dl;\n\t\tcp->elements[cp->elements[idx].cpu].idx = idx;\n\t\tidx = largest;\n\t}\n\t \n\tcp->elements[idx].cpu = orig_cpu;\n\tcp->elements[idx].dl = orig_dl;\n\tcp->elements[cp->elements[idx].cpu].idx = idx;\n}\n\nstatic void cpudl_heapify_up(struct cpudl *cp, int idx)\n{\n\tint p;\n\n\tint orig_cpu = cp->elements[idx].cpu;\n\tu64 orig_dl = cp->elements[idx].dl;\n\n\tif (idx == 0)\n\t\treturn;\n\n\tdo {\n\t\tp = parent(idx);\n\t\tif (dl_time_before(orig_dl, cp->elements[p].dl))\n\t\t\tbreak;\n\t\t \n\t\tcp->elements[idx].cpu = cp->elements[p].cpu;\n\t\tcp->elements[idx].dl = cp->elements[p].dl;\n\t\tcp->elements[cp->elements[idx].cpu].idx = idx;\n\t\tidx = p;\n\t} while (idx != 0);\n\t \n\tcp->elements[idx].cpu = orig_cpu;\n\tcp->elements[idx].dl = orig_dl;\n\tcp->elements[cp->elements[idx].cpu].idx = idx;\n}\n\nstatic void cpudl_heapify(struct cpudl *cp, int idx)\n{\n\tif (idx > 0 && dl_time_before(cp->elements[parent(idx)].dl,\n\t\t\t\tcp->elements[idx].dl))\n\t\tcpudl_heapify_up(cp, idx);\n\telse\n\t\tcpudl_heapify_down(cp, idx);\n}\n\nstatic inline int cpudl_maximum(struct cpudl *cp)\n{\n\treturn cp->elements[0].cpu;\n}\n\n \nint cpudl_find(struct cpudl *cp, struct task_struct *p,\n\t       struct cpumask *later_mask)\n{\n\tconst struct sched_dl_entity *dl_se = &p->dl;\n\n\tif (later_mask &&\n\t    cpumask_and(later_mask, cp->free_cpus, &p->cpus_mask)) {\n\t\tunsigned long cap, max_cap = 0;\n\t\tint cpu, max_cpu = -1;\n\n\t\tif (!sched_asym_cpucap_active())\n\t\t\treturn 1;\n\n\t\t \n\t\tfor_each_cpu(cpu, later_mask) {\n\t\t\tif (!dl_task_fits_capacity(p, cpu)) {\n\t\t\t\tcpumask_clear_cpu(cpu, later_mask);\n\n\t\t\t\tcap = capacity_orig_of(cpu);\n\n\t\t\t\tif (cap > max_cap ||\n\t\t\t\t    (cpu == task_cpu(p) && cap == max_cap)) {\n\t\t\t\t\tmax_cap = cap;\n\t\t\t\t\tmax_cpu = cpu;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif (cpumask_empty(later_mask))\n\t\t\tcpumask_set_cpu(max_cpu, later_mask);\n\n\t\treturn 1;\n\t} else {\n\t\tint best_cpu = cpudl_maximum(cp);\n\n\t\tWARN_ON(best_cpu != -1 && !cpu_present(best_cpu));\n\n\t\tif (cpumask_test_cpu(best_cpu, &p->cpus_mask) &&\n\t\t    dl_time_before(dl_se->deadline, cp->elements[0].dl)) {\n\t\t\tif (later_mask)\n\t\t\t\tcpumask_set_cpu(best_cpu, later_mask);\n\n\t\t\treturn 1;\n\t\t}\n\t}\n\treturn 0;\n}\n\n \nvoid cpudl_clear(struct cpudl *cp, int cpu)\n{\n\tint old_idx, new_cpu;\n\tunsigned long flags;\n\n\tWARN_ON(!cpu_present(cpu));\n\n\traw_spin_lock_irqsave(&cp->lock, flags);\n\n\told_idx = cp->elements[cpu].idx;\n\tif (old_idx == IDX_INVALID) {\n\t\t \n\t} else {\n\t\tnew_cpu = cp->elements[cp->size - 1].cpu;\n\t\tcp->elements[old_idx].dl = cp->elements[cp->size - 1].dl;\n\t\tcp->elements[old_idx].cpu = new_cpu;\n\t\tcp->size--;\n\t\tcp->elements[new_cpu].idx = old_idx;\n\t\tcp->elements[cpu].idx = IDX_INVALID;\n\t\tcpudl_heapify(cp, old_idx);\n\n\t\tcpumask_set_cpu(cpu, cp->free_cpus);\n\t}\n\traw_spin_unlock_irqrestore(&cp->lock, flags);\n}\n\n \nvoid cpudl_set(struct cpudl *cp, int cpu, u64 dl)\n{\n\tint old_idx;\n\tunsigned long flags;\n\n\tWARN_ON(!cpu_present(cpu));\n\n\traw_spin_lock_irqsave(&cp->lock, flags);\n\n\told_idx = cp->elements[cpu].idx;\n\tif (old_idx == IDX_INVALID) {\n\t\tint new_idx = cp->size++;\n\n\t\tcp->elements[new_idx].dl = dl;\n\t\tcp->elements[new_idx].cpu = cpu;\n\t\tcp->elements[cpu].idx = new_idx;\n\t\tcpudl_heapify_up(cp, new_idx);\n\t\tcpumask_clear_cpu(cpu, cp->free_cpus);\n\t} else {\n\t\tcp->elements[old_idx].dl = dl;\n\t\tcpudl_heapify(cp, old_idx);\n\t}\n\n\traw_spin_unlock_irqrestore(&cp->lock, flags);\n}\n\n \nvoid cpudl_set_freecpu(struct cpudl *cp, int cpu)\n{\n\tcpumask_set_cpu(cpu, cp->free_cpus);\n}\n\n \nvoid cpudl_clear_freecpu(struct cpudl *cp, int cpu)\n{\n\tcpumask_clear_cpu(cpu, cp->free_cpus);\n}\n\n \nint cpudl_init(struct cpudl *cp)\n{\n\tint i;\n\n\traw_spin_lock_init(&cp->lock);\n\tcp->size = 0;\n\n\tcp->elements = kcalloc(nr_cpu_ids,\n\t\t\t       sizeof(struct cpudl_item),\n\t\t\t       GFP_KERNEL);\n\tif (!cp->elements)\n\t\treturn -ENOMEM;\n\n\tif (!zalloc_cpumask_var(&cp->free_cpus, GFP_KERNEL)) {\n\t\tkfree(cp->elements);\n\t\treturn -ENOMEM;\n\t}\n\n\tfor_each_possible_cpu(i)\n\t\tcp->elements[i].idx = IDX_INVALID;\n\n\treturn 0;\n}\n\n \nvoid cpudl_cleanup(struct cpudl *cp)\n{\n\tfree_cpumask_var(cp->free_cpus);\n\tkfree(cp->elements);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}