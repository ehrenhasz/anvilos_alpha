{
  "module_name": "debug.c",
  "hash_id": "79e178b2149ad15ad84c3b55501c0e033da1eddb7ec6b0a1209e37e93fbc7522",
  "original_prompt": "Ingested from linux-6.6.14/kernel/sched/debug.c",
  "human_readable_source": "\n \n\n \n#define SEQ_printf(m, x...)\t\t\t\\\n do {\t\t\t\t\t\t\\\n\tif (m)\t\t\t\t\t\\\n\t\tseq_printf(m, x);\t\t\\\n\telse\t\t\t\t\t\\\n\t\tpr_cont(x);\t\t\t\\\n } while (0)\n\n \nstatic long long nsec_high(unsigned long long nsec)\n{\n\tif ((long long)nsec < 0) {\n\t\tnsec = -nsec;\n\t\tdo_div(nsec, 1000000);\n\t\treturn -nsec;\n\t}\n\tdo_div(nsec, 1000000);\n\n\treturn nsec;\n}\n\nstatic unsigned long nsec_low(unsigned long long nsec)\n{\n\tif ((long long)nsec < 0)\n\t\tnsec = -nsec;\n\n\treturn do_div(nsec, 1000000);\n}\n\n#define SPLIT_NS(x) nsec_high(x), nsec_low(x)\n\n#define SCHED_FEAT(name, enabled)\t\\\n\t#name ,\n\nstatic const char * const sched_feat_names[] = {\n#include \"features.h\"\n};\n\n#undef SCHED_FEAT\n\nstatic int sched_feat_show(struct seq_file *m, void *v)\n{\n\tint i;\n\n\tfor (i = 0; i < __SCHED_FEAT_NR; i++) {\n\t\tif (!(sysctl_sched_features & (1UL << i)))\n\t\t\tseq_puts(m, \"NO_\");\n\t\tseq_printf(m, \"%s \", sched_feat_names[i]);\n\t}\n\tseq_puts(m, \"\\n\");\n\n\treturn 0;\n}\n\n#ifdef CONFIG_JUMP_LABEL\n\n#define jump_label_key__true  STATIC_KEY_INIT_TRUE\n#define jump_label_key__false STATIC_KEY_INIT_FALSE\n\n#define SCHED_FEAT(name, enabled)\t\\\n\tjump_label_key__##enabled ,\n\nstruct static_key sched_feat_keys[__SCHED_FEAT_NR] = {\n#include \"features.h\"\n};\n\n#undef SCHED_FEAT\n\nstatic void sched_feat_disable(int i)\n{\n\tstatic_key_disable_cpuslocked(&sched_feat_keys[i]);\n}\n\nstatic void sched_feat_enable(int i)\n{\n\tstatic_key_enable_cpuslocked(&sched_feat_keys[i]);\n}\n#else\nstatic void sched_feat_disable(int i) { };\nstatic void sched_feat_enable(int i) { };\n#endif  \n\nstatic int sched_feat_set(char *cmp)\n{\n\tint i;\n\tint neg = 0;\n\n\tif (strncmp(cmp, \"NO_\", 3) == 0) {\n\t\tneg = 1;\n\t\tcmp += 3;\n\t}\n\n\ti = match_string(sched_feat_names, __SCHED_FEAT_NR, cmp);\n\tif (i < 0)\n\t\treturn i;\n\n\tif (neg) {\n\t\tsysctl_sched_features &= ~(1UL << i);\n\t\tsched_feat_disable(i);\n\t} else {\n\t\tsysctl_sched_features |= (1UL << i);\n\t\tsched_feat_enable(i);\n\t}\n\n\treturn 0;\n}\n\nstatic ssize_t\nsched_feat_write(struct file *filp, const char __user *ubuf,\n\t\tsize_t cnt, loff_t *ppos)\n{\n\tchar buf[64];\n\tchar *cmp;\n\tint ret;\n\tstruct inode *inode;\n\n\tif (cnt > 63)\n\t\tcnt = 63;\n\n\tif (copy_from_user(&buf, ubuf, cnt))\n\t\treturn -EFAULT;\n\n\tbuf[cnt] = 0;\n\tcmp = strstrip(buf);\n\n\t \n\tinode = file_inode(filp);\n\tcpus_read_lock();\n\tinode_lock(inode);\n\tret = sched_feat_set(cmp);\n\tinode_unlock(inode);\n\tcpus_read_unlock();\n\tif (ret < 0)\n\t\treturn ret;\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}\n\nstatic int sched_feat_open(struct inode *inode, struct file *filp)\n{\n\treturn single_open(filp, sched_feat_show, NULL);\n}\n\nstatic const struct file_operations sched_feat_fops = {\n\t.open\t\t= sched_feat_open,\n\t.write\t\t= sched_feat_write,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= single_release,\n};\n\n#ifdef CONFIG_SMP\n\nstatic ssize_t sched_scaling_write(struct file *filp, const char __user *ubuf,\n\t\t\t\t   size_t cnt, loff_t *ppos)\n{\n\tchar buf[16];\n\tunsigned int scaling;\n\n\tif (cnt > 15)\n\t\tcnt = 15;\n\n\tif (copy_from_user(&buf, ubuf, cnt))\n\t\treturn -EFAULT;\n\tbuf[cnt] = '\\0';\n\n\tif (kstrtouint(buf, 10, &scaling))\n\t\treturn -EINVAL;\n\n\tif (scaling >= SCHED_TUNABLESCALING_END)\n\t\treturn -EINVAL;\n\n\tsysctl_sched_tunable_scaling = scaling;\n\tif (sched_update_scaling())\n\t\treturn -EINVAL;\n\n\t*ppos += cnt;\n\treturn cnt;\n}\n\nstatic int sched_scaling_show(struct seq_file *m, void *v)\n{\n\tseq_printf(m, \"%d\\n\", sysctl_sched_tunable_scaling);\n\treturn 0;\n}\n\nstatic int sched_scaling_open(struct inode *inode, struct file *filp)\n{\n\treturn single_open(filp, sched_scaling_show, NULL);\n}\n\nstatic const struct file_operations sched_scaling_fops = {\n\t.open\t\t= sched_scaling_open,\n\t.write\t\t= sched_scaling_write,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= single_release,\n};\n\n#endif  \n\n#ifdef CONFIG_PREEMPT_DYNAMIC\n\nstatic ssize_t sched_dynamic_write(struct file *filp, const char __user *ubuf,\n\t\t\t\t   size_t cnt, loff_t *ppos)\n{\n\tchar buf[16];\n\tint mode;\n\n\tif (cnt > 15)\n\t\tcnt = 15;\n\n\tif (copy_from_user(&buf, ubuf, cnt))\n\t\treturn -EFAULT;\n\n\tbuf[cnt] = 0;\n\tmode = sched_dynamic_mode(strstrip(buf));\n\tif (mode < 0)\n\t\treturn mode;\n\n\tsched_dynamic_update(mode);\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}\n\nstatic int sched_dynamic_show(struct seq_file *m, void *v)\n{\n\tstatic const char * preempt_modes[] = {\n\t\t\"none\", \"voluntary\", \"full\"\n\t};\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(preempt_modes); i++) {\n\t\tif (preempt_dynamic_mode == i)\n\t\t\tseq_puts(m, \"(\");\n\t\tseq_puts(m, preempt_modes[i]);\n\t\tif (preempt_dynamic_mode == i)\n\t\t\tseq_puts(m, \")\");\n\n\t\tseq_puts(m, \" \");\n\t}\n\n\tseq_puts(m, \"\\n\");\n\treturn 0;\n}\n\nstatic int sched_dynamic_open(struct inode *inode, struct file *filp)\n{\n\treturn single_open(filp, sched_dynamic_show, NULL);\n}\n\nstatic const struct file_operations sched_dynamic_fops = {\n\t.open\t\t= sched_dynamic_open,\n\t.write\t\t= sched_dynamic_write,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= single_release,\n};\n\n#endif  \n\n__read_mostly bool sched_debug_verbose;\n\n#ifdef CONFIG_SMP\nstatic struct dentry           *sd_dentry;\n\n\nstatic ssize_t sched_verbose_write(struct file *filp, const char __user *ubuf,\n\t\t\t\t  size_t cnt, loff_t *ppos)\n{\n\tssize_t result;\n\tbool orig;\n\n\tcpus_read_lock();\n\tmutex_lock(&sched_domains_mutex);\n\n\torig = sched_debug_verbose;\n\tresult = debugfs_write_file_bool(filp, ubuf, cnt, ppos);\n\n\tif (sched_debug_verbose && !orig)\n\t\tupdate_sched_domain_debugfs();\n\telse if (!sched_debug_verbose && orig) {\n\t\tdebugfs_remove(sd_dentry);\n\t\tsd_dentry = NULL;\n\t}\n\n\tmutex_unlock(&sched_domains_mutex);\n\tcpus_read_unlock();\n\n\treturn result;\n}\n#else\n#define sched_verbose_write debugfs_write_file_bool\n#endif\n\nstatic const struct file_operations sched_verbose_fops = {\n\t.read =         debugfs_read_file_bool,\n\t.write =        sched_verbose_write,\n\t.open =         simple_open,\n\t.llseek =       default_llseek,\n};\n\nstatic const struct seq_operations sched_debug_sops;\n\nstatic int sched_debug_open(struct inode *inode, struct file *filp)\n{\n\treturn seq_open(filp, &sched_debug_sops);\n}\n\nstatic const struct file_operations sched_debug_fops = {\n\t.open\t\t= sched_debug_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= seq_release,\n};\n\nstatic struct dentry *debugfs_sched;\n\nstatic __init int sched_init_debug(void)\n{\n\tstruct dentry __maybe_unused *numa;\n\n\tdebugfs_sched = debugfs_create_dir(\"sched\", NULL);\n\n\tdebugfs_create_file(\"features\", 0644, debugfs_sched, NULL, &sched_feat_fops);\n\tdebugfs_create_file_unsafe(\"verbose\", 0644, debugfs_sched, &sched_debug_verbose, &sched_verbose_fops);\n#ifdef CONFIG_PREEMPT_DYNAMIC\n\tdebugfs_create_file(\"preempt\", 0644, debugfs_sched, NULL, &sched_dynamic_fops);\n#endif\n\n\tdebugfs_create_u32(\"base_slice_ns\", 0644, debugfs_sched, &sysctl_sched_base_slice);\n\n\tdebugfs_create_u32(\"latency_warn_ms\", 0644, debugfs_sched, &sysctl_resched_latency_warn_ms);\n\tdebugfs_create_u32(\"latency_warn_once\", 0644, debugfs_sched, &sysctl_resched_latency_warn_once);\n\n#ifdef CONFIG_SMP\n\tdebugfs_create_file(\"tunable_scaling\", 0644, debugfs_sched, NULL, &sched_scaling_fops);\n\tdebugfs_create_u32(\"migration_cost_ns\", 0644, debugfs_sched, &sysctl_sched_migration_cost);\n\tdebugfs_create_u32(\"nr_migrate\", 0644, debugfs_sched, &sysctl_sched_nr_migrate);\n\n\tmutex_lock(&sched_domains_mutex);\n\tupdate_sched_domain_debugfs();\n\tmutex_unlock(&sched_domains_mutex);\n#endif\n\n#ifdef CONFIG_NUMA_BALANCING\n\tnuma = debugfs_create_dir(\"numa_balancing\", debugfs_sched);\n\n\tdebugfs_create_u32(\"scan_delay_ms\", 0644, numa, &sysctl_numa_balancing_scan_delay);\n\tdebugfs_create_u32(\"scan_period_min_ms\", 0644, numa, &sysctl_numa_balancing_scan_period_min);\n\tdebugfs_create_u32(\"scan_period_max_ms\", 0644, numa, &sysctl_numa_balancing_scan_period_max);\n\tdebugfs_create_u32(\"scan_size_mb\", 0644, numa, &sysctl_numa_balancing_scan_size);\n\tdebugfs_create_u32(\"hot_threshold_ms\", 0644, numa, &sysctl_numa_balancing_hot_threshold);\n#endif\n\n\tdebugfs_create_file(\"debug\", 0444, debugfs_sched, NULL, &sched_debug_fops);\n\n\treturn 0;\n}\nlate_initcall(sched_init_debug);\n\n#ifdef CONFIG_SMP\n\nstatic cpumask_var_t\t\tsd_sysctl_cpus;\n\nstatic int sd_flags_show(struct seq_file *m, void *v)\n{\n\tunsigned long flags = *(unsigned int *)m->private;\n\tint idx;\n\n\tfor_each_set_bit(idx, &flags, __SD_FLAG_CNT) {\n\t\tseq_puts(m, sd_flag_debug[idx].name);\n\t\tseq_puts(m, \" \");\n\t}\n\tseq_puts(m, \"\\n\");\n\n\treturn 0;\n}\n\nstatic int sd_flags_open(struct inode *inode, struct file *file)\n{\n\treturn single_open(file, sd_flags_show, inode->i_private);\n}\n\nstatic const struct file_operations sd_flags_fops = {\n\t.open\t\t= sd_flags_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= single_release,\n};\n\nstatic void register_sd(struct sched_domain *sd, struct dentry *parent)\n{\n#define SDM(type, mode, member)\t\\\n\tdebugfs_create_##type(#member, mode, parent, &sd->member)\n\n\tSDM(ulong, 0644, min_interval);\n\tSDM(ulong, 0644, max_interval);\n\tSDM(u64,   0644, max_newidle_lb_cost);\n\tSDM(u32,   0644, busy_factor);\n\tSDM(u32,   0644, imbalance_pct);\n\tSDM(u32,   0644, cache_nice_tries);\n\tSDM(str,   0444, name);\n\n#undef SDM\n\n\tdebugfs_create_file(\"flags\", 0444, parent, &sd->flags, &sd_flags_fops);\n\tdebugfs_create_file(\"groups_flags\", 0444, parent, &sd->groups->flags, &sd_flags_fops);\n}\n\nvoid update_sched_domain_debugfs(void)\n{\n\tint cpu, i;\n\n\t \n\tif (!debugfs_sched)\n\t\treturn;\n\n\tif (!sched_debug_verbose)\n\t\treturn;\n\n\tif (!cpumask_available(sd_sysctl_cpus)) {\n\t\tif (!alloc_cpumask_var(&sd_sysctl_cpus, GFP_KERNEL))\n\t\t\treturn;\n\t\tcpumask_copy(sd_sysctl_cpus, cpu_possible_mask);\n\t}\n\n\tif (!sd_dentry) {\n\t\tsd_dentry = debugfs_create_dir(\"domains\", debugfs_sched);\n\n\t\t \n\t\tif (cpumask_empty(sd_sysctl_cpus))\n\t\t\tcpumask_copy(sd_sysctl_cpus, cpu_online_mask);\n\t}\n\n\tfor_each_cpu(cpu, sd_sysctl_cpus) {\n\t\tstruct sched_domain *sd;\n\t\tstruct dentry *d_cpu;\n\t\tchar buf[32];\n\n\t\tsnprintf(buf, sizeof(buf), \"cpu%d\", cpu);\n\t\tdebugfs_lookup_and_remove(buf, sd_dentry);\n\t\td_cpu = debugfs_create_dir(buf, sd_dentry);\n\n\t\ti = 0;\n\t\tfor_each_domain(cpu, sd) {\n\t\t\tstruct dentry *d_sd;\n\n\t\t\tsnprintf(buf, sizeof(buf), \"domain%d\", i);\n\t\t\td_sd = debugfs_create_dir(buf, d_cpu);\n\n\t\t\tregister_sd(sd, d_sd);\n\t\t\ti++;\n\t\t}\n\n\t\t__cpumask_clear_cpu(cpu, sd_sysctl_cpus);\n\t}\n}\n\nvoid dirty_sched_domain_sysctl(int cpu)\n{\n\tif (cpumask_available(sd_sysctl_cpus))\n\t\t__cpumask_set_cpu(cpu, sd_sysctl_cpus);\n}\n\n#endif  \n\n#ifdef CONFIG_FAIR_GROUP_SCHED\nstatic void print_cfs_group_stats(struct seq_file *m, int cpu, struct task_group *tg)\n{\n\tstruct sched_entity *se = tg->se[cpu];\n\n#define P(F)\t\tSEQ_printf(m, \"  .%-30s: %lld\\n\",\t#F, (long long)F)\n#define P_SCHEDSTAT(F)\tSEQ_printf(m, \"  .%-30s: %lld\\n\",\t\\\n\t\t#F, (long long)schedstat_val(stats->F))\n#define PN(F)\t\tSEQ_printf(m, \"  .%-30s: %lld.%06ld\\n\", #F, SPLIT_NS((long long)F))\n#define PN_SCHEDSTAT(F)\tSEQ_printf(m, \"  .%-30s: %lld.%06ld\\n\", \\\n\t\t#F, SPLIT_NS((long long)schedstat_val(stats->F)))\n\n\tif (!se)\n\t\treturn;\n\n\tPN(se->exec_start);\n\tPN(se->vruntime);\n\tPN(se->sum_exec_runtime);\n\n\tif (schedstat_enabled()) {\n\t\tstruct sched_statistics *stats;\n\t\tstats = __schedstats_from_se(se);\n\n\t\tPN_SCHEDSTAT(wait_start);\n\t\tPN_SCHEDSTAT(sleep_start);\n\t\tPN_SCHEDSTAT(block_start);\n\t\tPN_SCHEDSTAT(sleep_max);\n\t\tPN_SCHEDSTAT(block_max);\n\t\tPN_SCHEDSTAT(exec_max);\n\t\tPN_SCHEDSTAT(slice_max);\n\t\tPN_SCHEDSTAT(wait_max);\n\t\tPN_SCHEDSTAT(wait_sum);\n\t\tP_SCHEDSTAT(wait_count);\n\t}\n\n\tP(se->load.weight);\n#ifdef CONFIG_SMP\n\tP(se->avg.load_avg);\n\tP(se->avg.util_avg);\n\tP(se->avg.runnable_avg);\n#endif\n\n#undef PN_SCHEDSTAT\n#undef PN\n#undef P_SCHEDSTAT\n#undef P\n}\n#endif\n\n#ifdef CONFIG_CGROUP_SCHED\nstatic DEFINE_SPINLOCK(sched_debug_lock);\nstatic char group_path[PATH_MAX];\n\nstatic void task_group_path(struct task_group *tg, char *path, int plen)\n{\n\tif (autogroup_path(tg, path, plen))\n\t\treturn;\n\n\tcgroup_path(tg->css.cgroup, path, plen);\n}\n\n \n#define SEQ_printf_task_group_path(m, tg, fmt...)\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tif (spin_trylock(&sched_debug_lock)) {\t\t\t\t\\\n\t\ttask_group_path(tg, group_path, sizeof(group_path));\t\\\n\t\tSEQ_printf(m, fmt, group_path);\t\t\t\t\\\n\t\tspin_unlock(&sched_debug_lock);\t\t\t\t\\\n\t} else {\t\t\t\t\t\t\t\\\n\t\tchar buf[128];\t\t\t\t\t\t\\\n\t\tchar *bufend = buf + sizeof(buf) - 3;\t\t\t\\\n\t\ttask_group_path(tg, buf, bufend - buf);\t\t\t\\\n\t\tstrcpy(bufend - 1, \"...\");\t\t\t\t\\\n\t\tSEQ_printf(m, fmt, buf);\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n}\n#endif\n\nstatic void\nprint_task(struct seq_file *m, struct rq *rq, struct task_struct *p)\n{\n\tif (task_current(rq, p))\n\t\tSEQ_printf(m, \">R\");\n\telse\n\t\tSEQ_printf(m, \" %c\", task_state_to_char(p));\n\n\tSEQ_printf(m, \"%15s %5d %9Ld.%06ld %c %9Ld.%06ld %9Ld.%06ld %9Ld.%06ld %9Ld %5d \",\n\t\tp->comm, task_pid_nr(p),\n\t\tSPLIT_NS(p->se.vruntime),\n\t\tentity_eligible(cfs_rq_of(&p->se), &p->se) ? 'E' : 'N',\n\t\tSPLIT_NS(p->se.deadline),\n\t\tSPLIT_NS(p->se.slice),\n\t\tSPLIT_NS(p->se.sum_exec_runtime),\n\t\t(long long)(p->nvcsw + p->nivcsw),\n\t\tp->prio);\n\n\tSEQ_printf(m, \"%9lld.%06ld %9lld.%06ld %9lld.%06ld %9lld.%06ld\",\n\t\tSPLIT_NS(schedstat_val_or_zero(p->stats.wait_sum)),\n\t\tSPLIT_NS(p->se.sum_exec_runtime),\n\t\tSPLIT_NS(schedstat_val_or_zero(p->stats.sum_sleep_runtime)),\n\t\tSPLIT_NS(schedstat_val_or_zero(p->stats.sum_block_runtime)));\n\n#ifdef CONFIG_NUMA_BALANCING\n\tSEQ_printf(m, \" %d %d\", task_node(p), task_numa_group_id(p));\n#endif\n#ifdef CONFIG_CGROUP_SCHED\n\tSEQ_printf_task_group_path(m, task_group(p), \" %s\")\n#endif\n\n\tSEQ_printf(m, \"\\n\");\n}\n\nstatic void print_rq(struct seq_file *m, struct rq *rq, int rq_cpu)\n{\n\tstruct task_struct *g, *p;\n\n\tSEQ_printf(m, \"\\n\");\n\tSEQ_printf(m, \"runnable tasks:\\n\");\n\tSEQ_printf(m, \" S            task   PID         tree-key  switches  prio\"\n\t\t   \"     wait-time             sum-exec        sum-sleep\\n\");\n\tSEQ_printf(m, \"-------------------------------------------------------\"\n\t\t   \"------------------------------------------------------\\n\");\n\n\trcu_read_lock();\n\tfor_each_process_thread(g, p) {\n\t\tif (task_cpu(p) != rq_cpu)\n\t\t\tcontinue;\n\n\t\tprint_task(m, rq, p);\n\t}\n\trcu_read_unlock();\n}\n\nvoid print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)\n{\n\ts64 left_vruntime = -1, min_vruntime, right_vruntime = -1, spread;\n\tstruct sched_entity *last, *first;\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long flags;\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tSEQ_printf(m, \"\\n\");\n\tSEQ_printf_task_group_path(m, cfs_rq->tg, \"cfs_rq[%d]:%s\\n\", cpu);\n#else\n\tSEQ_printf(m, \"\\n\");\n\tSEQ_printf(m, \"cfs_rq[%d]:\\n\", cpu);\n#endif\n\tSEQ_printf(m, \"  .%-30s: %Ld.%06ld\\n\", \"exec_clock\",\n\t\t\tSPLIT_NS(cfs_rq->exec_clock));\n\n\traw_spin_rq_lock_irqsave(rq, flags);\n\tfirst = __pick_first_entity(cfs_rq);\n\tif (first)\n\t\tleft_vruntime = first->vruntime;\n\tlast = __pick_last_entity(cfs_rq);\n\tif (last)\n\t\tright_vruntime = last->vruntime;\n\tmin_vruntime = cfs_rq->min_vruntime;\n\traw_spin_rq_unlock_irqrestore(rq, flags);\n\n\tSEQ_printf(m, \"  .%-30s: %Ld.%06ld\\n\", \"left_vruntime\",\n\t\t\tSPLIT_NS(left_vruntime));\n\tSEQ_printf(m, \"  .%-30s: %Ld.%06ld\\n\", \"min_vruntime\",\n\t\t\tSPLIT_NS(min_vruntime));\n\tSEQ_printf(m, \"  .%-30s: %Ld.%06ld\\n\", \"avg_vruntime\",\n\t\t\tSPLIT_NS(avg_vruntime(cfs_rq)));\n\tSEQ_printf(m, \"  .%-30s: %Ld.%06ld\\n\", \"right_vruntime\",\n\t\t\tSPLIT_NS(right_vruntime));\n\tspread = right_vruntime - left_vruntime;\n\tSEQ_printf(m, \"  .%-30s: %Ld.%06ld\\n\", \"spread\", SPLIT_NS(spread));\n\tSEQ_printf(m, \"  .%-30s: %d\\n\", \"nr_spread_over\",\n\t\t\tcfs_rq->nr_spread_over);\n\tSEQ_printf(m, \"  .%-30s: %d\\n\", \"nr_running\", cfs_rq->nr_running);\n\tSEQ_printf(m, \"  .%-30s: %d\\n\", \"h_nr_running\", cfs_rq->h_nr_running);\n\tSEQ_printf(m, \"  .%-30s: %d\\n\", \"idle_nr_running\",\n\t\t\tcfs_rq->idle_nr_running);\n\tSEQ_printf(m, \"  .%-30s: %d\\n\", \"idle_h_nr_running\",\n\t\t\tcfs_rq->idle_h_nr_running);\n\tSEQ_printf(m, \"  .%-30s: %ld\\n\", \"load\", cfs_rq->load.weight);\n#ifdef CONFIG_SMP\n\tSEQ_printf(m, \"  .%-30s: %lu\\n\", \"load_avg\",\n\t\t\tcfs_rq->avg.load_avg);\n\tSEQ_printf(m, \"  .%-30s: %lu\\n\", \"runnable_avg\",\n\t\t\tcfs_rq->avg.runnable_avg);\n\tSEQ_printf(m, \"  .%-30s: %lu\\n\", \"util_avg\",\n\t\t\tcfs_rq->avg.util_avg);\n\tSEQ_printf(m, \"  .%-30s: %u\\n\", \"util_est_enqueued\",\n\t\t\tcfs_rq->avg.util_est.enqueued);\n\tSEQ_printf(m, \"  .%-30s: %ld\\n\", \"removed.load_avg\",\n\t\t\tcfs_rq->removed.load_avg);\n\tSEQ_printf(m, \"  .%-30s: %ld\\n\", \"removed.util_avg\",\n\t\t\tcfs_rq->removed.util_avg);\n\tSEQ_printf(m, \"  .%-30s: %ld\\n\", \"removed.runnable_avg\",\n\t\t\tcfs_rq->removed.runnable_avg);\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tSEQ_printf(m, \"  .%-30s: %lu\\n\", \"tg_load_avg_contrib\",\n\t\t\tcfs_rq->tg_load_avg_contrib);\n\tSEQ_printf(m, \"  .%-30s: %ld\\n\", \"tg_load_avg\",\n\t\t\tatomic_long_read(&cfs_rq->tg->load_avg));\n#endif\n#endif\n#ifdef CONFIG_CFS_BANDWIDTH\n\tSEQ_printf(m, \"  .%-30s: %d\\n\", \"throttled\",\n\t\t\tcfs_rq->throttled);\n\tSEQ_printf(m, \"  .%-30s: %d\\n\", \"throttle_count\",\n\t\t\tcfs_rq->throttle_count);\n#endif\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tprint_cfs_group_stats(m, cpu, cfs_rq->tg);\n#endif\n}\n\nvoid print_rt_rq(struct seq_file *m, int cpu, struct rt_rq *rt_rq)\n{\n#ifdef CONFIG_RT_GROUP_SCHED\n\tSEQ_printf(m, \"\\n\");\n\tSEQ_printf_task_group_path(m, rt_rq->tg, \"rt_rq[%d]:%s\\n\", cpu);\n#else\n\tSEQ_printf(m, \"\\n\");\n\tSEQ_printf(m, \"rt_rq[%d]:\\n\", cpu);\n#endif\n\n#define P(x) \\\n\tSEQ_printf(m, \"  .%-30s: %Ld\\n\", #x, (long long)(rt_rq->x))\n#define PU(x) \\\n\tSEQ_printf(m, \"  .%-30s: %lu\\n\", #x, (unsigned long)(rt_rq->x))\n#define PN(x) \\\n\tSEQ_printf(m, \"  .%-30s: %Ld.%06ld\\n\", #x, SPLIT_NS(rt_rq->x))\n\n\tPU(rt_nr_running);\n#ifdef CONFIG_SMP\n\tPU(rt_nr_migratory);\n#endif\n\tP(rt_throttled);\n\tPN(rt_time);\n\tPN(rt_runtime);\n\n#undef PN\n#undef PU\n#undef P\n}\n\nvoid print_dl_rq(struct seq_file *m, int cpu, struct dl_rq *dl_rq)\n{\n\tstruct dl_bw *dl_bw;\n\n\tSEQ_printf(m, \"\\n\");\n\tSEQ_printf(m, \"dl_rq[%d]:\\n\", cpu);\n\n#define PU(x) \\\n\tSEQ_printf(m, \"  .%-30s: %lu\\n\", #x, (unsigned long)(dl_rq->x))\n\n\tPU(dl_nr_running);\n#ifdef CONFIG_SMP\n\tPU(dl_nr_migratory);\n\tdl_bw = &cpu_rq(cpu)->rd->dl_bw;\n#else\n\tdl_bw = &dl_rq->dl_bw;\n#endif\n\tSEQ_printf(m, \"  .%-30s: %lld\\n\", \"dl_bw->bw\", dl_bw->bw);\n\tSEQ_printf(m, \"  .%-30s: %lld\\n\", \"dl_bw->total_bw\", dl_bw->total_bw);\n\n#undef PU\n}\n\nstatic void print_cpu(struct seq_file *m, int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\n#ifdef CONFIG_X86\n\t{\n\t\tunsigned int freq = cpu_khz ? : 1;\n\n\t\tSEQ_printf(m, \"cpu#%d, %u.%03u MHz\\n\",\n\t\t\t   cpu, freq / 1000, (freq % 1000));\n\t}\n#else\n\tSEQ_printf(m, \"cpu#%d\\n\", cpu);\n#endif\n\n#define P(x)\t\t\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (sizeof(rq->x) == 4)\t\t\t\t\t\t\\\n\t\tSEQ_printf(m, \"  .%-30s: %d\\n\", #x, (int)(rq->x));\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\tSEQ_printf(m, \"  .%-30s: %Ld\\n\", #x, (long long)(rq->x));\\\n} while (0)\n\n#define PN(x) \\\n\tSEQ_printf(m, \"  .%-30s: %Ld.%06ld\\n\", #x, SPLIT_NS(rq->x))\n\n\tP(nr_running);\n\tP(nr_switches);\n\tP(nr_uninterruptible);\n\tPN(next_balance);\n\tSEQ_printf(m, \"  .%-30s: %ld\\n\", \"curr->pid\", (long)(task_pid_nr(rq->curr)));\n\tPN(clock);\n\tPN(clock_task);\n#undef P\n#undef PN\n\n#ifdef CONFIG_SMP\n#define P64(n) SEQ_printf(m, \"  .%-30s: %Ld\\n\", #n, rq->n);\n\tP64(avg_idle);\n\tP64(max_idle_balance_cost);\n#undef P64\n#endif\n\n#define P(n) SEQ_printf(m, \"  .%-30s: %d\\n\", #n, schedstat_val(rq->n));\n\tif (schedstat_enabled()) {\n\t\tP(yld_count);\n\t\tP(sched_count);\n\t\tP(sched_goidle);\n\t\tP(ttwu_count);\n\t\tP(ttwu_local);\n\t}\n#undef P\n\n\tprint_cfs_stats(m, cpu);\n\tprint_rt_stats(m, cpu);\n\tprint_dl_stats(m, cpu);\n\n\tprint_rq(m, rq, cpu);\n\tSEQ_printf(m, \"\\n\");\n}\n\nstatic const char *sched_tunable_scaling_names[] = {\n\t\"none\",\n\t\"logarithmic\",\n\t\"linear\"\n};\n\nstatic void sched_debug_header(struct seq_file *m)\n{\n\tu64 ktime, sched_clk, cpu_clk;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tktime = ktime_to_ns(ktime_get());\n\tsched_clk = sched_clock();\n\tcpu_clk = local_clock();\n\tlocal_irq_restore(flags);\n\n\tSEQ_printf(m, \"Sched Debug Version: v0.11, %s %.*s\\n\",\n\t\tinit_utsname()->release,\n\t\t(int)strcspn(init_utsname()->version, \" \"),\n\t\tinit_utsname()->version);\n\n#define P(x) \\\n\tSEQ_printf(m, \"%-40s: %Ld\\n\", #x, (long long)(x))\n#define PN(x) \\\n\tSEQ_printf(m, \"%-40s: %Ld.%06ld\\n\", #x, SPLIT_NS(x))\n\tPN(ktime);\n\tPN(sched_clk);\n\tPN(cpu_clk);\n\tP(jiffies);\n#ifdef CONFIG_HAVE_UNSTABLE_SCHED_CLOCK\n\tP(sched_clock_stable());\n#endif\n#undef PN\n#undef P\n\n\tSEQ_printf(m, \"\\n\");\n\tSEQ_printf(m, \"sysctl_sched\\n\");\n\n#define P(x) \\\n\tSEQ_printf(m, \"  .%-40s: %Ld\\n\", #x, (long long)(x))\n#define PN(x) \\\n\tSEQ_printf(m, \"  .%-40s: %Ld.%06ld\\n\", #x, SPLIT_NS(x))\n\tPN(sysctl_sched_base_slice);\n\tP(sysctl_sched_child_runs_first);\n\tP(sysctl_sched_features);\n#undef PN\n#undef P\n\n\tSEQ_printf(m, \"  .%-40s: %d (%s)\\n\",\n\t\t\"sysctl_sched_tunable_scaling\",\n\t\tsysctl_sched_tunable_scaling,\n\t\tsched_tunable_scaling_names[sysctl_sched_tunable_scaling]);\n\tSEQ_printf(m, \"\\n\");\n}\n\nstatic int sched_debug_show(struct seq_file *m, void *v)\n{\n\tint cpu = (unsigned long)(v - 2);\n\n\tif (cpu != -1)\n\t\tprint_cpu(m, cpu);\n\telse\n\t\tsched_debug_header(m);\n\n\treturn 0;\n}\n\nvoid sysrq_sched_debug_show(void)\n{\n\tint cpu;\n\n\tsched_debug_header(NULL);\n\tfor_each_online_cpu(cpu) {\n\t\t \n\t\ttouch_nmi_watchdog();\n\t\ttouch_all_softlockup_watchdogs();\n\t\tprint_cpu(NULL, cpu);\n\t}\n}\n\n \nstatic void *sched_debug_start(struct seq_file *file, loff_t *offset)\n{\n\tunsigned long n = *offset;\n\n\tif (n == 0)\n\t\treturn (void *) 1;\n\n\tn--;\n\n\tif (n > 0)\n\t\tn = cpumask_next(n - 1, cpu_online_mask);\n\telse\n\t\tn = cpumask_first(cpu_online_mask);\n\n\t*offset = n + 1;\n\n\tif (n < nr_cpu_ids)\n\t\treturn (void *)(unsigned long)(n + 2);\n\n\treturn NULL;\n}\n\nstatic void *sched_debug_next(struct seq_file *file, void *data, loff_t *offset)\n{\n\t(*offset)++;\n\treturn sched_debug_start(file, offset);\n}\n\nstatic void sched_debug_stop(struct seq_file *file, void *data)\n{\n}\n\nstatic const struct seq_operations sched_debug_sops = {\n\t.start\t\t= sched_debug_start,\n\t.next\t\t= sched_debug_next,\n\t.stop\t\t= sched_debug_stop,\n\t.show\t\t= sched_debug_show,\n};\n\n#define __PS(S, F) SEQ_printf(m, \"%-45s:%21Ld\\n\", S, (long long)(F))\n#define __P(F) __PS(#F, F)\n#define   P(F) __PS(#F, p->F)\n#define   PM(F, M) __PS(#F, p->F & (M))\n#define __PSN(S, F) SEQ_printf(m, \"%-45s:%14Ld.%06ld\\n\", S, SPLIT_NS((long long)(F)))\n#define __PN(F) __PSN(#F, F)\n#define   PN(F) __PSN(#F, p->F)\n\n\n#ifdef CONFIG_NUMA_BALANCING\nvoid print_numa_stats(struct seq_file *m, int node, unsigned long tsf,\n\t\tunsigned long tpf, unsigned long gsf, unsigned long gpf)\n{\n\tSEQ_printf(m, \"numa_faults node=%d \", node);\n\tSEQ_printf(m, \"task_private=%lu task_shared=%lu \", tpf, tsf);\n\tSEQ_printf(m, \"group_private=%lu group_shared=%lu\\n\", gpf, gsf);\n}\n#endif\n\n\nstatic void sched_show_numa(struct task_struct *p, struct seq_file *m)\n{\n#ifdef CONFIG_NUMA_BALANCING\n\tif (p->mm)\n\t\tP(mm->numa_scan_seq);\n\n\tP(numa_pages_migrated);\n\tP(numa_preferred_nid);\n\tP(total_numa_faults);\n\tSEQ_printf(m, \"current_node=%d, numa_group_id=%d\\n\",\n\t\t\ttask_node(p), task_numa_group_id(p));\n\tshow_numa_stats(p, m);\n#endif\n}\n\nvoid proc_sched_show_task(struct task_struct *p, struct pid_namespace *ns,\n\t\t\t\t\t\t  struct seq_file *m)\n{\n\tunsigned long nr_switches;\n\n\tSEQ_printf(m, \"%s (%d, #threads: %d)\\n\", p->comm, task_pid_nr_ns(p, ns),\n\t\t\t\t\t\tget_nr_threads(p));\n\tSEQ_printf(m,\n\t\t\"---------------------------------------------------------\"\n\t\t\"----------\\n\");\n\n#define P_SCHEDSTAT(F)  __PS(#F, schedstat_val(p->stats.F))\n#define PN_SCHEDSTAT(F) __PSN(#F, schedstat_val(p->stats.F))\n\n\tPN(se.exec_start);\n\tPN(se.vruntime);\n\tPN(se.sum_exec_runtime);\n\n\tnr_switches = p->nvcsw + p->nivcsw;\n\n\tP(se.nr_migrations);\n\n\tif (schedstat_enabled()) {\n\t\tu64 avg_atom, avg_per_cpu;\n\n\t\tPN_SCHEDSTAT(sum_sleep_runtime);\n\t\tPN_SCHEDSTAT(sum_block_runtime);\n\t\tPN_SCHEDSTAT(wait_start);\n\t\tPN_SCHEDSTAT(sleep_start);\n\t\tPN_SCHEDSTAT(block_start);\n\t\tPN_SCHEDSTAT(sleep_max);\n\t\tPN_SCHEDSTAT(block_max);\n\t\tPN_SCHEDSTAT(exec_max);\n\t\tPN_SCHEDSTAT(slice_max);\n\t\tPN_SCHEDSTAT(wait_max);\n\t\tPN_SCHEDSTAT(wait_sum);\n\t\tP_SCHEDSTAT(wait_count);\n\t\tPN_SCHEDSTAT(iowait_sum);\n\t\tP_SCHEDSTAT(iowait_count);\n\t\tP_SCHEDSTAT(nr_migrations_cold);\n\t\tP_SCHEDSTAT(nr_failed_migrations_affine);\n\t\tP_SCHEDSTAT(nr_failed_migrations_running);\n\t\tP_SCHEDSTAT(nr_failed_migrations_hot);\n\t\tP_SCHEDSTAT(nr_forced_migrations);\n\t\tP_SCHEDSTAT(nr_wakeups);\n\t\tP_SCHEDSTAT(nr_wakeups_sync);\n\t\tP_SCHEDSTAT(nr_wakeups_migrate);\n\t\tP_SCHEDSTAT(nr_wakeups_local);\n\t\tP_SCHEDSTAT(nr_wakeups_remote);\n\t\tP_SCHEDSTAT(nr_wakeups_affine);\n\t\tP_SCHEDSTAT(nr_wakeups_affine_attempts);\n\t\tP_SCHEDSTAT(nr_wakeups_passive);\n\t\tP_SCHEDSTAT(nr_wakeups_idle);\n\n\t\tavg_atom = p->se.sum_exec_runtime;\n\t\tif (nr_switches)\n\t\t\tavg_atom = div64_ul(avg_atom, nr_switches);\n\t\telse\n\t\t\tavg_atom = -1LL;\n\n\t\tavg_per_cpu = p->se.sum_exec_runtime;\n\t\tif (p->se.nr_migrations) {\n\t\t\tavg_per_cpu = div64_u64(avg_per_cpu,\n\t\t\t\t\t\tp->se.nr_migrations);\n\t\t} else {\n\t\t\tavg_per_cpu = -1LL;\n\t\t}\n\n\t\t__PN(avg_atom);\n\t\t__PN(avg_per_cpu);\n\n#ifdef CONFIG_SCHED_CORE\n\t\tPN_SCHEDSTAT(core_forceidle_sum);\n#endif\n\t}\n\n\t__P(nr_switches);\n\t__PS(\"nr_voluntary_switches\", p->nvcsw);\n\t__PS(\"nr_involuntary_switches\", p->nivcsw);\n\n\tP(se.load.weight);\n#ifdef CONFIG_SMP\n\tP(se.avg.load_sum);\n\tP(se.avg.runnable_sum);\n\tP(se.avg.util_sum);\n\tP(se.avg.load_avg);\n\tP(se.avg.runnable_avg);\n\tP(se.avg.util_avg);\n\tP(se.avg.last_update_time);\n\tP(se.avg.util_est.ewma);\n\tPM(se.avg.util_est.enqueued, ~UTIL_AVG_UNCHANGED);\n#endif\n#ifdef CONFIG_UCLAMP_TASK\n\t__PS(\"uclamp.min\", p->uclamp_req[UCLAMP_MIN].value);\n\t__PS(\"uclamp.max\", p->uclamp_req[UCLAMP_MAX].value);\n\t__PS(\"effective uclamp.min\", uclamp_eff_value(p, UCLAMP_MIN));\n\t__PS(\"effective uclamp.max\", uclamp_eff_value(p, UCLAMP_MAX));\n#endif\n\tP(policy);\n\tP(prio);\n\tif (task_has_dl_policy(p)) {\n\t\tP(dl.runtime);\n\t\tP(dl.deadline);\n\t}\n#undef PN_SCHEDSTAT\n#undef P_SCHEDSTAT\n\n\t{\n\t\tunsigned int this_cpu = raw_smp_processor_id();\n\t\tu64 t0, t1;\n\n\t\tt0 = cpu_clock(this_cpu);\n\t\tt1 = cpu_clock(this_cpu);\n\t\t__PS(\"clock-delta\", t1-t0);\n\t}\n\n\tsched_show_numa(p, m);\n}\n\nvoid proc_sched_set_task(struct task_struct *p)\n{\n#ifdef CONFIG_SCHEDSTATS\n\tmemset(&p->stats, 0, sizeof(p->stats));\n#endif\n}\n\nvoid resched_latency_warn(int cpu, u64 latency)\n{\n\tstatic DEFINE_RATELIMIT_STATE(latency_check_ratelimit, 60 * 60 * HZ, 1);\n\n\tWARN(__ratelimit(&latency_check_ratelimit),\n\t     \"sched: CPU %d need_resched set for > %llu ns (%d ticks) \"\n\t     \"without schedule\\n\",\n\t     cpu, latency, cpu_rq(cpu)->ticks_without_resched);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}