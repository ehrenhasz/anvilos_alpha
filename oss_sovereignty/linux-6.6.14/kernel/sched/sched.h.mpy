{
  "module_name": "sched.h",
  "hash_id": "cc89ed67062d11607b73a8d6bb1be0e73c38c430d38e3fa234372f63dffacf33",
  "original_prompt": "Ingested from linux-6.6.14/kernel/sched/sched.h",
  "human_readable_source": " \n \n#ifndef _KERNEL_SCHED_SCHED_H\n#define _KERNEL_SCHED_SCHED_H\n\n#include <linux/sched/affinity.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched/cpufreq.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/rseq_api.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/smt.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/task_flags.h>\n#include <linux/sched/task.h>\n#include <linux/sched/topology.h>\n\n#include <linux/atomic.h>\n#include <linux/bitmap.h>\n#include <linux/bug.h>\n#include <linux/capability.h>\n#include <linux/cgroup_api.h>\n#include <linux/cgroup.h>\n#include <linux/context_tracking.h>\n#include <linux/cpufreq.h>\n#include <linux/cpumask_api.h>\n#include <linux/ctype.h>\n#include <linux/file.h>\n#include <linux/fs_api.h>\n#include <linux/hrtimer_api.h>\n#include <linux/interrupt.h>\n#include <linux/irq_work.h>\n#include <linux/jiffies.h>\n#include <linux/kref_api.h>\n#include <linux/kthread.h>\n#include <linux/ktime_api.h>\n#include <linux/lockdep_api.h>\n#include <linux/lockdep.h>\n#include <linux/minmax.h>\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <linux/mutex_api.h>\n#include <linux/plist.h>\n#include <linux/poll.h>\n#include <linux/proc_fs.h>\n#include <linux/profile.h>\n#include <linux/psi.h>\n#include <linux/rcupdate.h>\n#include <linux/seq_file.h>\n#include <linux/seqlock.h>\n#include <linux/softirq.h>\n#include <linux/spinlock_api.h>\n#include <linux/static_key.h>\n#include <linux/stop_machine.h>\n#include <linux/syscalls_api.h>\n#include <linux/syscalls.h>\n#include <linux/tick.h>\n#include <linux/topology.h>\n#include <linux/types.h>\n#include <linux/u64_stats_sync_api.h>\n#include <linux/uaccess.h>\n#include <linux/wait_api.h>\n#include <linux/wait_bit.h>\n#include <linux/workqueue_api.h>\n\n#include <trace/events/power.h>\n#include <trace/events/sched.h>\n\n#include \"../workqueue_internal.h\"\n\n#ifdef CONFIG_CGROUP_SCHED\n#include <linux/cgroup.h>\n#include <linux/psi.h>\n#endif\n\n#ifdef CONFIG_SCHED_DEBUG\n# include <linux/static_key.h>\n#endif\n\n#ifdef CONFIG_PARAVIRT\n# include <asm/paravirt.h>\n# include <asm/paravirt_api_clock.h>\n#endif\n\n#include \"cpupri.h\"\n#include \"cpudeadline.h\"\n\n#ifdef CONFIG_SCHED_DEBUG\n# define SCHED_WARN_ON(x)      WARN_ONCE(x, #x)\n#else\n# define SCHED_WARN_ON(x)      ({ (void)(x), 0; })\n#endif\n\nstruct rq;\nstruct cpuidle_state;\n\n \n#define TASK_ON_RQ_QUEUED\t1\n#define TASK_ON_RQ_MIGRATING\t2\n\nextern __read_mostly int scheduler_running;\n\nextern unsigned long calc_load_update;\nextern atomic_long_t calc_load_tasks;\n\nextern unsigned int sysctl_sched_child_runs_first;\n\nextern void calc_global_load_tick(struct rq *this_rq);\nextern long calc_load_fold_active(struct rq *this_rq, long adjust);\n\nextern void call_trace_sched_update_nr_running(struct rq *rq, int count);\n\nextern unsigned int sysctl_sched_rt_period;\nextern int sysctl_sched_rt_runtime;\nextern int sched_rr_timeslice;\n\n \n#define NS_TO_JIFFIES(TIME)\t((unsigned long)(TIME) / (NSEC_PER_SEC / HZ))\n\n \n#ifdef CONFIG_64BIT\n# define NICE_0_LOAD_SHIFT\t(SCHED_FIXEDPOINT_SHIFT + SCHED_FIXEDPOINT_SHIFT)\n# define scale_load(w)\t\t((w) << SCHED_FIXEDPOINT_SHIFT)\n# define scale_load_down(w) \\\n({ \\\n\tunsigned long __w = (w); \\\n\tif (__w) \\\n\t\t__w = max(2UL, __w >> SCHED_FIXEDPOINT_SHIFT); \\\n\t__w; \\\n})\n#else\n# define NICE_0_LOAD_SHIFT\t(SCHED_FIXEDPOINT_SHIFT)\n# define scale_load(w)\t\t(w)\n# define scale_load_down(w)\t(w)\n#endif\n\n \n#define NICE_0_LOAD\t\t(1L << NICE_0_LOAD_SHIFT)\n\n \n#define DL_SCALE\t\t10\n\n \n#define RUNTIME_INF\t\t((u64)~0ULL)\n\nstatic inline int idle_policy(int policy)\n{\n\treturn policy == SCHED_IDLE;\n}\nstatic inline int fair_policy(int policy)\n{\n\treturn policy == SCHED_NORMAL || policy == SCHED_BATCH;\n}\n\nstatic inline int rt_policy(int policy)\n{\n\treturn policy == SCHED_FIFO || policy == SCHED_RR;\n}\n\nstatic inline int dl_policy(int policy)\n{\n\treturn policy == SCHED_DEADLINE;\n}\nstatic inline bool valid_policy(int policy)\n{\n\treturn idle_policy(policy) || fair_policy(policy) ||\n\t\trt_policy(policy) || dl_policy(policy);\n}\n\nstatic inline int task_has_idle_policy(struct task_struct *p)\n{\n\treturn idle_policy(p->policy);\n}\n\nstatic inline int task_has_rt_policy(struct task_struct *p)\n{\n\treturn rt_policy(p->policy);\n}\n\nstatic inline int task_has_dl_policy(struct task_struct *p)\n{\n\treturn dl_policy(p->policy);\n}\n\n#define cap_scale(v, s) ((v)*(s) >> SCHED_CAPACITY_SHIFT)\n\nstatic inline void update_avg(u64 *avg, u64 sample)\n{\n\ts64 diff = sample - *avg;\n\t*avg += diff / 8;\n}\n\n \n#define shr_bound(val, shift)\t\t\t\t\t\t\t\\\n\t(val >> min_t(typeof(shift), shift, BITS_PER_TYPE(typeof(val)) - 1))\n\n \n#define SCHED_FLAG_SUGOV\t0x10000000\n\n#define SCHED_DL_FLAGS (SCHED_FLAG_RECLAIM | SCHED_FLAG_DL_OVERRUN | SCHED_FLAG_SUGOV)\n\nstatic inline bool dl_entity_is_special(const struct sched_dl_entity *dl_se)\n{\n#ifdef CONFIG_CPU_FREQ_GOV_SCHEDUTIL\n\treturn unlikely(dl_se->flags & SCHED_FLAG_SUGOV);\n#else\n\treturn false;\n#endif\n}\n\n \nstatic inline bool dl_entity_preempt(const struct sched_dl_entity *a,\n\t\t\t\t     const struct sched_dl_entity *b)\n{\n\treturn dl_entity_is_special(a) ||\n\t       dl_time_before(a->deadline, b->deadline);\n}\n\n \nstruct rt_prio_array {\n\tDECLARE_BITMAP(bitmap, MAX_RT_PRIO+1);  \n\tstruct list_head queue[MAX_RT_PRIO];\n};\n\nstruct rt_bandwidth {\n\t \n\traw_spinlock_t\t\trt_runtime_lock;\n\tktime_t\t\t\trt_period;\n\tu64\t\t\trt_runtime;\n\tstruct hrtimer\t\trt_period_timer;\n\tunsigned int\t\trt_period_active;\n};\n\nvoid __dl_clear_params(struct task_struct *p);\n\nstatic inline int dl_bandwidth_enabled(void)\n{\n\treturn sysctl_sched_rt_runtime >= 0;\n}\n\n \nstruct dl_bw {\n\traw_spinlock_t\t\tlock;\n\tu64\t\t\tbw;\n\tu64\t\t\ttotal_bw;\n};\n\nextern void init_dl_bw(struct dl_bw *dl_b);\nextern int  sched_dl_global_validate(void);\nextern void sched_dl_do_global(void);\nextern int  sched_dl_overflow(struct task_struct *p, int policy, const struct sched_attr *attr);\nextern void __setparam_dl(struct task_struct *p, const struct sched_attr *attr);\nextern void __getparam_dl(struct task_struct *p, struct sched_attr *attr);\nextern bool __checkparam_dl(const struct sched_attr *attr);\nextern bool dl_param_changed(struct task_struct *p, const struct sched_attr *attr);\nextern int  dl_cpuset_cpumask_can_shrink(const struct cpumask *cur, const struct cpumask *trial);\nextern int  dl_bw_check_overflow(int cpu);\n\n#ifdef CONFIG_CGROUP_SCHED\n\nstruct cfs_rq;\nstruct rt_rq;\n\nextern struct list_head task_groups;\n\nstruct cfs_bandwidth {\n#ifdef CONFIG_CFS_BANDWIDTH\n\traw_spinlock_t\t\tlock;\n\tktime_t\t\t\tperiod;\n\tu64\t\t\tquota;\n\tu64\t\t\truntime;\n\tu64\t\t\tburst;\n\tu64\t\t\truntime_snap;\n\ts64\t\t\thierarchical_quota;\n\n\tu8\t\t\tidle;\n\tu8\t\t\tperiod_active;\n\tu8\t\t\tslack_started;\n\tstruct hrtimer\t\tperiod_timer;\n\tstruct hrtimer\t\tslack_timer;\n\tstruct list_head\tthrottled_cfs_rq;\n\n\t \n\tint\t\t\tnr_periods;\n\tint\t\t\tnr_throttled;\n\tint\t\t\tnr_burst;\n\tu64\t\t\tthrottled_time;\n\tu64\t\t\tburst_time;\n#endif\n};\n\n \nstruct task_group {\n\tstruct cgroup_subsys_state css;\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\t \n\tstruct sched_entity\t**se;\n\t \n\tstruct cfs_rq\t\t**cfs_rq;\n\tunsigned long\t\tshares;\n\n\t \n\tint\t\t\tidle;\n\n#ifdef\tCONFIG_SMP\n\t \n\tatomic_long_t\t\tload_avg ____cacheline_aligned;\n#endif\n#endif\n\n#ifdef CONFIG_RT_GROUP_SCHED\n\tstruct sched_rt_entity\t**rt_se;\n\tstruct rt_rq\t\t**rt_rq;\n\n\tstruct rt_bandwidth\trt_bandwidth;\n#endif\n\n\tstruct rcu_head\t\trcu;\n\tstruct list_head\tlist;\n\n\tstruct task_group\t*parent;\n\tstruct list_head\tsiblings;\n\tstruct list_head\tchildren;\n\n#ifdef CONFIG_SCHED_AUTOGROUP\n\tstruct autogroup\t*autogroup;\n#endif\n\n\tstruct cfs_bandwidth\tcfs_bandwidth;\n\n#ifdef CONFIG_UCLAMP_TASK_GROUP\n\t \n\tunsigned int\t\tuclamp_pct[UCLAMP_CNT];\n\t \n\tstruct uclamp_se\tuclamp_req[UCLAMP_CNT];\n\t \n\tstruct uclamp_se\tuclamp[UCLAMP_CNT];\n#endif\n\n};\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n#define ROOT_TASK_GROUP_LOAD\tNICE_0_LOAD\n\n \n#define MIN_SHARES\t\t(1UL <<  1)\n#define MAX_SHARES\t\t(1UL << 18)\n#endif\n\ntypedef int (*tg_visitor)(struct task_group *, void *);\n\nextern int walk_tg_tree_from(struct task_group *from,\n\t\t\t     tg_visitor down, tg_visitor up, void *data);\n\n \nstatic inline int walk_tg_tree(tg_visitor down, tg_visitor up, void *data)\n{\n\treturn walk_tg_tree_from(&root_task_group, down, up, data);\n}\n\nextern int tg_nop(struct task_group *tg, void *data);\n\nextern void free_fair_sched_group(struct task_group *tg);\nextern int alloc_fair_sched_group(struct task_group *tg, struct task_group *parent);\nextern void online_fair_sched_group(struct task_group *tg);\nextern void unregister_fair_sched_group(struct task_group *tg);\nextern void init_tg_cfs_entry(struct task_group *tg, struct cfs_rq *cfs_rq,\n\t\t\tstruct sched_entity *se, int cpu,\n\t\t\tstruct sched_entity *parent);\nextern void init_cfs_bandwidth(struct cfs_bandwidth *cfs_b, struct cfs_bandwidth *parent);\n\nextern void __refill_cfs_bandwidth_runtime(struct cfs_bandwidth *cfs_b);\nextern void start_cfs_bandwidth(struct cfs_bandwidth *cfs_b);\nextern void unthrottle_cfs_rq(struct cfs_rq *cfs_rq);\nextern bool cfs_task_bw_constrained(struct task_struct *p);\n\nextern void init_tg_rt_entry(struct task_group *tg, struct rt_rq *rt_rq,\n\t\tstruct sched_rt_entity *rt_se, int cpu,\n\t\tstruct sched_rt_entity *parent);\nextern int sched_group_set_rt_runtime(struct task_group *tg, long rt_runtime_us);\nextern int sched_group_set_rt_period(struct task_group *tg, u64 rt_period_us);\nextern long sched_group_rt_runtime(struct task_group *tg);\nextern long sched_group_rt_period(struct task_group *tg);\nextern int sched_rt_can_attach(struct task_group *tg, struct task_struct *tsk);\n\nextern struct task_group *sched_create_group(struct task_group *parent);\nextern void sched_online_group(struct task_group *tg,\n\t\t\t       struct task_group *parent);\nextern void sched_destroy_group(struct task_group *tg);\nextern void sched_release_group(struct task_group *tg);\n\nextern void sched_move_task(struct task_struct *tsk);\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\nextern int sched_group_set_shares(struct task_group *tg, unsigned long shares);\n\nextern int sched_group_set_idle(struct task_group *tg, long idle);\n\n#ifdef CONFIG_SMP\nextern void set_task_rq_fair(struct sched_entity *se,\n\t\t\t     struct cfs_rq *prev, struct cfs_rq *next);\n#else  \nstatic inline void set_task_rq_fair(struct sched_entity *se,\n\t\t\t     struct cfs_rq *prev, struct cfs_rq *next) { }\n#endif  \n#endif  \n\n#else  \n\nstruct cfs_bandwidth { };\nstatic inline bool cfs_task_bw_constrained(struct task_struct *p) { return false; }\n\n#endif\t \n\nextern void unregister_rt_sched_group(struct task_group *tg);\nextern void free_rt_sched_group(struct task_group *tg);\nextern int alloc_rt_sched_group(struct task_group *tg, struct task_group *parent);\n\n \n#ifdef CONFIG_64BIT\n# define u64_u32_load_copy(var, copy)       var\n# define u64_u32_store_copy(var, copy, val) (var = val)\n#else\n# define u64_u32_load_copy(var, copy)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tu64 __val, __val_copy;\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\t__val_copy = copy;\t\t\t\t\t\\\n\t\t \t\t\t\t\t\t\t\\\n\t\tsmp_rmb();\t\t\t\t\t\t\\\n\t\t__val = var;\t\t\t\t\t\t\\\n\t} while (__val != __val_copy);\t\t\t\t\t\\\n\t__val;\t\t\t\t\t\t\t\t\\\n})\n# define u64_u32_store_copy(var, copy, val)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\ttypeof(val) __val = (val);\t\t\t\t\t\\\n\tvar = __val;\t\t\t\t\t\t\t\\\n\t \t\t\t\t\t\t\t\t\\\n\tsmp_wmb();\t\t\t\t\t\t\t\\\n\tcopy = __val;\t\t\t\t\t\t\t\\\n} while (0)\n#endif\n# define u64_u32_load(var)      u64_u32_load_copy(var, var##_copy)\n# define u64_u32_store(var, val) u64_u32_store_copy(var, var##_copy, val)\n\n \nstruct cfs_rq {\n\tstruct load_weight\tload;\n\tunsigned int\t\tnr_running;\n\tunsigned int\t\th_nr_running;       \n\tunsigned int\t\tidle_nr_running;    \n\tunsigned int\t\tidle_h_nr_running;  \n\n\ts64\t\t\tavg_vruntime;\n\tu64\t\t\tavg_load;\n\n\tu64\t\t\texec_clock;\n\tu64\t\t\tmin_vruntime;\n#ifdef CONFIG_SCHED_CORE\n\tunsigned int\t\tforceidle_seq;\n\tu64\t\t\tmin_vruntime_fi;\n#endif\n\n#ifndef CONFIG_64BIT\n\tu64\t\t\tmin_vruntime_copy;\n#endif\n\n\tstruct rb_root_cached\ttasks_timeline;\n\n\t \n\tstruct sched_entity\t*curr;\n\tstruct sched_entity\t*next;\n\n#ifdef\tCONFIG_SCHED_DEBUG\n\tunsigned int\t\tnr_spread_over;\n#endif\n\n#ifdef CONFIG_SMP\n\t \n\tstruct sched_avg\tavg;\n#ifndef CONFIG_64BIT\n\tu64\t\t\tlast_update_time_copy;\n#endif\n\tstruct {\n\t\traw_spinlock_t\tlock ____cacheline_aligned;\n\t\tint\t\tnr;\n\t\tunsigned long\tload_avg;\n\t\tunsigned long\tutil_avg;\n\t\tunsigned long\trunnable_avg;\n\t} removed;\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tunsigned long\t\ttg_load_avg_contrib;\n\tlong\t\t\tpropagate;\n\tlong\t\t\tprop_runnable_sum;\n\n\t \n\tunsigned long\t\th_load;\n\tu64\t\t\tlast_h_load_update;\n\tstruct sched_entity\t*h_load_next;\n#endif  \n#endif  \n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tstruct rq\t\t*rq;\t \n\n\t \n\tint\t\t\ton_list;\n\tstruct list_head\tleaf_cfs_rq_list;\n\tstruct task_group\t*tg;\t \n\n\t \n\tint\t\t\tidle;\n\n#ifdef CONFIG_CFS_BANDWIDTH\n\tint\t\t\truntime_enabled;\n\ts64\t\t\truntime_remaining;\n\n\tu64\t\t\tthrottled_pelt_idle;\n#ifndef CONFIG_64BIT\n\tu64                     throttled_pelt_idle_copy;\n#endif\n\tu64\t\t\tthrottled_clock;\n\tu64\t\t\tthrottled_clock_pelt;\n\tu64\t\t\tthrottled_clock_pelt_time;\n\tu64\t\t\tthrottled_clock_self;\n\tu64\t\t\tthrottled_clock_self_time;\n\tint\t\t\tthrottled;\n\tint\t\t\tthrottle_count;\n\tstruct list_head\tthrottled_list;\n#ifdef CONFIG_SMP\n\tstruct list_head\tthrottled_csd_list;\n#endif\n#endif  \n#endif  \n};\n\nstatic inline int rt_bandwidth_enabled(void)\n{\n\treturn sysctl_sched_rt_runtime >= 0;\n}\n\n \n#if defined(CONFIG_IRQ_WORK) && defined(CONFIG_SMP)\n# define HAVE_RT_PUSH_IPI\n#endif\n\n \nstruct rt_rq {\n\tstruct rt_prio_array\tactive;\n\tunsigned int\t\trt_nr_running;\n\tunsigned int\t\trr_nr_running;\n#if defined CONFIG_SMP || defined CONFIG_RT_GROUP_SCHED\n\tstruct {\n\t\tint\t\tcurr;  \n#ifdef CONFIG_SMP\n\t\tint\t\tnext;  \n#endif\n\t} highest_prio;\n#endif\n#ifdef CONFIG_SMP\n\tunsigned int\t\trt_nr_migratory;\n\tunsigned int\t\trt_nr_total;\n\tint\t\t\toverloaded;\n\tstruct plist_head\tpushable_tasks;\n\n#endif  \n\tint\t\t\trt_queued;\n\n\tint\t\t\trt_throttled;\n\tu64\t\t\trt_time;\n\tu64\t\t\trt_runtime;\n\t \n\traw_spinlock_t\t\trt_runtime_lock;\n\n#ifdef CONFIG_RT_GROUP_SCHED\n\tunsigned int\t\trt_nr_boosted;\n\n\tstruct rq\t\t*rq;\n\tstruct task_group\t*tg;\n#endif\n};\n\nstatic inline bool rt_rq_is_runnable(struct rt_rq *rt_rq)\n{\n\treturn rt_rq->rt_queued && rt_rq->rt_nr_running;\n}\n\n \nstruct dl_rq {\n\t \n\tstruct rb_root_cached\troot;\n\n\tunsigned int\t\tdl_nr_running;\n\n#ifdef CONFIG_SMP\n\t \n\tstruct {\n\t\tu64\t\tcurr;\n\t\tu64\t\tnext;\n\t} earliest_dl;\n\n\tunsigned int\t\tdl_nr_migratory;\n\tint\t\t\toverloaded;\n\n\t \n\tstruct rb_root_cached\tpushable_dl_tasks_root;\n#else\n\tstruct dl_bw\t\tdl_bw;\n#endif\n\t \n\tu64\t\t\trunning_bw;\n\n\t \n\tu64\t\t\tthis_bw;\n\tu64\t\t\textra_bw;\n\n\t \n\tu64\t\t\tmax_bw;\n\n\t \n\tu64\t\t\tbw_ratio;\n};\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n \n#define entity_is_task(se)\t(!se->my_q)\n\nstatic inline void se_update_runnable(struct sched_entity *se)\n{\n\tif (!entity_is_task(se))\n\t\tse->runnable_weight = se->my_q->h_nr_running;\n}\n\nstatic inline long se_runnable(struct sched_entity *se)\n{\n\tif (entity_is_task(se))\n\t\treturn !!se->on_rq;\n\telse\n\t\treturn se->runnable_weight;\n}\n\n#else\n#define entity_is_task(se)\t1\n\nstatic inline void se_update_runnable(struct sched_entity *se) {}\n\nstatic inline long se_runnable(struct sched_entity *se)\n{\n\treturn !!se->on_rq;\n}\n#endif\n\n#ifdef CONFIG_SMP\n \nstatic inline long se_weight(struct sched_entity *se)\n{\n\treturn scale_load_down(se->load.weight);\n}\n\n\nstatic inline bool sched_asym_prefer(int a, int b)\n{\n\treturn arch_asym_cpu_priority(a) > arch_asym_cpu_priority(b);\n}\n\nstruct perf_domain {\n\tstruct em_perf_domain *em_pd;\n\tstruct perf_domain *next;\n\tstruct rcu_head rcu;\n};\n\n \n#define SG_OVERLOAD\t\t0x1  \n#define SG_OVERUTILIZED\t\t0x2  \n\n \nstruct root_domain {\n\tatomic_t\t\trefcount;\n\tatomic_t\t\trto_count;\n\tstruct rcu_head\t\trcu;\n\tcpumask_var_t\t\tspan;\n\tcpumask_var_t\t\tonline;\n\n\t \n\tint\t\t\toverload;\n\n\t \n\tint\t\t\toverutilized;\n\n\t \n\tcpumask_var_t\t\tdlo_mask;\n\tatomic_t\t\tdlo_count;\n\tstruct dl_bw\t\tdl_bw;\n\tstruct cpudl\t\tcpudl;\n\n\t \n\tu64 visit_gen;\n\n#ifdef HAVE_RT_PUSH_IPI\n\t \n\tstruct irq_work\t\trto_push_work;\n\traw_spinlock_t\t\trto_lock;\n\t \n\tint\t\t\trto_loop;\n\tint\t\t\trto_cpu;\n\t \n\tatomic_t\t\trto_loop_next;\n\tatomic_t\t\trto_loop_start;\n#endif\n\t \n\tcpumask_var_t\t\trto_mask;\n\tstruct cpupri\t\tcpupri;\n\n\tunsigned long\t\tmax_cpu_capacity;\n\n\t \n\tstruct perf_domain __rcu *pd;\n};\n\nextern void init_defrootdomain(void);\nextern int sched_init_domains(const struct cpumask *cpu_map);\nextern void rq_attach_root(struct rq *rq, struct root_domain *rd);\nextern void sched_get_rd(struct root_domain *rd);\nextern void sched_put_rd(struct root_domain *rd);\n\n#ifdef HAVE_RT_PUSH_IPI\nextern void rto_push_irq_work_func(struct irq_work *work);\n#endif\n#endif  \n\n#ifdef CONFIG_UCLAMP_TASK\n \nstruct uclamp_bucket {\n\tunsigned long value : bits_per(SCHED_CAPACITY_SCALE);\n\tunsigned long tasks : BITS_PER_LONG - bits_per(SCHED_CAPACITY_SCALE);\n};\n\n \nstruct uclamp_rq {\n\tunsigned int value;\n\tstruct uclamp_bucket bucket[UCLAMP_BUCKETS];\n};\n\nDECLARE_STATIC_KEY_FALSE(sched_uclamp_used);\n#endif  \n\nstruct rq;\nstruct balance_callback {\n\tstruct balance_callback *next;\n\tvoid (*func)(struct rq *rq);\n};\n\n \nstruct rq {\n\t \n\traw_spinlock_t\t\t__lock;\n\n\t \n\tunsigned int\t\tnr_running;\n#ifdef CONFIG_NUMA_BALANCING\n\tunsigned int\t\tnr_numa_running;\n\tunsigned int\t\tnr_preferred_running;\n\tunsigned int\t\tnuma_migrate_on;\n#endif\n#ifdef CONFIG_NO_HZ_COMMON\n#ifdef CONFIG_SMP\n\tunsigned long\t\tlast_blocked_load_update_tick;\n\tunsigned int\t\thas_blocked_load;\n\tcall_single_data_t\tnohz_csd;\n#endif  \n\tunsigned int\t\tnohz_tick_stopped;\n\tatomic_t\t\tnohz_flags;\n#endif  \n\n#ifdef CONFIG_SMP\n\tunsigned int\t\tttwu_pending;\n#endif\n\tu64\t\t\tnr_switches;\n\n#ifdef CONFIG_UCLAMP_TASK\n\t \n\tstruct uclamp_rq\tuclamp[UCLAMP_CNT] ____cacheline_aligned;\n\tunsigned int\t\tuclamp_flags;\n#define UCLAMP_FLAG_IDLE 0x01\n#endif\n\n\tstruct cfs_rq\t\tcfs;\n\tstruct rt_rq\t\trt;\n\tstruct dl_rq\t\tdl;\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\t \n\tstruct list_head\tleaf_cfs_rq_list;\n\tstruct list_head\t*tmp_alone_branch;\n#endif  \n\n\t \n\tunsigned int\t\tnr_uninterruptible;\n\n\tstruct task_struct __rcu\t*curr;\n\tstruct task_struct\t*idle;\n\tstruct task_struct\t*stop;\n\tunsigned long\t\tnext_balance;\n\tstruct mm_struct\t*prev_mm;\n\n\tunsigned int\t\tclock_update_flags;\n\tu64\t\t\tclock;\n\t \n\tu64\t\t\tclock_task ____cacheline_aligned;\n\tu64\t\t\tclock_pelt;\n\tunsigned long\t\tlost_idle_time;\n\tu64\t\t\tclock_pelt_idle;\n\tu64\t\t\tclock_idle;\n#ifndef CONFIG_64BIT\n\tu64\t\t\tclock_pelt_idle_copy;\n\tu64\t\t\tclock_idle_copy;\n#endif\n\n\tatomic_t\t\tnr_iowait;\n\n#ifdef CONFIG_SCHED_DEBUG\n\tu64 last_seen_need_resched_ns;\n\tint ticks_without_resched;\n#endif\n\n#ifdef CONFIG_MEMBARRIER\n\tint membarrier_state;\n#endif\n\n#ifdef CONFIG_SMP\n\tstruct root_domain\t\t*rd;\n\tstruct sched_domain __rcu\t*sd;\n\n\tunsigned long\t\tcpu_capacity;\n\tunsigned long\t\tcpu_capacity_orig;\n\n\tstruct balance_callback *balance_callback;\n\n\tunsigned char\t\tnohz_idle_balance;\n\tunsigned char\t\tidle_balance;\n\n\tunsigned long\t\tmisfit_task_load;\n\n\t \n\tint\t\t\tactive_balance;\n\tint\t\t\tpush_cpu;\n\tstruct cpu_stop_work\tactive_balance_work;\n\n\t \n\tint\t\t\tcpu;\n\tint\t\t\tonline;\n\n\tstruct list_head cfs_tasks;\n\n\tstruct sched_avg\tavg_rt;\n\tstruct sched_avg\tavg_dl;\n#ifdef CONFIG_HAVE_SCHED_AVG_IRQ\n\tstruct sched_avg\tavg_irq;\n#endif\n#ifdef CONFIG_SCHED_THERMAL_PRESSURE\n\tstruct sched_avg\tavg_thermal;\n#endif\n\tu64\t\t\tidle_stamp;\n\tu64\t\t\tavg_idle;\n\n\tunsigned long\t\twake_stamp;\n\tu64\t\t\twake_avg_idle;\n\n\t \n\tu64\t\t\tmax_idle_balance_cost;\n\n#ifdef CONFIG_HOTPLUG_CPU\n\tstruct rcuwait\t\thotplug_wait;\n#endif\n#endif  \n\n#ifdef CONFIG_IRQ_TIME_ACCOUNTING\n\tu64\t\t\tprev_irq_time;\n#endif\n#ifdef CONFIG_PARAVIRT\n\tu64\t\t\tprev_steal_time;\n#endif\n#ifdef CONFIG_PARAVIRT_TIME_ACCOUNTING\n\tu64\t\t\tprev_steal_time_rq;\n#endif\n\n\t \n\tunsigned long\t\tcalc_load_update;\n\tlong\t\t\tcalc_load_active;\n\n#ifdef CONFIG_SCHED_HRTICK\n#ifdef CONFIG_SMP\n\tcall_single_data_t\thrtick_csd;\n#endif\n\tstruct hrtimer\t\thrtick_timer;\n\tktime_t \t\thrtick_time;\n#endif\n\n#ifdef CONFIG_SCHEDSTATS\n\t \n\tstruct sched_info\trq_sched_info;\n\tunsigned long long\trq_cpu_time;\n\t \n\n\t \n\tunsigned int\t\tyld_count;\n\n\t \n\tunsigned int\t\tsched_count;\n\tunsigned int\t\tsched_goidle;\n\n\t \n\tunsigned int\t\tttwu_count;\n\tunsigned int\t\tttwu_local;\n#endif\n\n#ifdef CONFIG_CPU_IDLE\n\t \n\tstruct cpuidle_state\t*idle_state;\n#endif\n\n#ifdef CONFIG_SMP\n\tunsigned int\t\tnr_pinned;\n#endif\n\tunsigned int\t\tpush_busy;\n\tstruct cpu_stop_work\tpush_work;\n\n#ifdef CONFIG_SCHED_CORE\n\t \n\tstruct rq\t\t*core;\n\tstruct task_struct\t*core_pick;\n\tunsigned int\t\tcore_enabled;\n\tunsigned int\t\tcore_sched_seq;\n\tstruct rb_root\t\tcore_tree;\n\n\t \n\tunsigned int\t\tcore_task_seq;\n\tunsigned int\t\tcore_pick_seq;\n\tunsigned long\t\tcore_cookie;\n\tunsigned int\t\tcore_forceidle_count;\n\tunsigned int\t\tcore_forceidle_seq;\n\tunsigned int\t\tcore_forceidle_occupation;\n\tu64\t\t\tcore_forceidle_start;\n#endif\n\n\t \n\tcpumask_var_t\t\tscratch_mask;\n\n#if defined(CONFIG_CFS_BANDWIDTH) && defined(CONFIG_SMP)\n\tcall_single_data_t\tcfsb_csd;\n\tstruct list_head\tcfsb_csd_list;\n#endif\n};\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\n \nstatic inline struct rq *rq_of(struct cfs_rq *cfs_rq)\n{\n\treturn cfs_rq->rq;\n}\n\n#else\n\nstatic inline struct rq *rq_of(struct cfs_rq *cfs_rq)\n{\n\treturn container_of(cfs_rq, struct rq, cfs);\n}\n#endif\n\nstatic inline int cpu_of(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\treturn rq->cpu;\n#else\n\treturn 0;\n#endif\n}\n\n#define MDF_PUSH\t0x01\n\nstatic inline bool is_migration_disabled(struct task_struct *p)\n{\n#ifdef CONFIG_SMP\n\treturn p->migration_disabled;\n#else\n\treturn false;\n#endif\n}\n\nDECLARE_PER_CPU_SHARED_ALIGNED(struct rq, runqueues);\n\n#define cpu_rq(cpu)\t\t(&per_cpu(runqueues, (cpu)))\n#define this_rq()\t\tthis_cpu_ptr(&runqueues)\n#define task_rq(p)\t\tcpu_rq(task_cpu(p))\n#define cpu_curr(cpu)\t\t(cpu_rq(cpu)->curr)\n#define raw_rq()\t\traw_cpu_ptr(&runqueues)\n\nstruct sched_group;\n#ifdef CONFIG_SCHED_CORE\nstatic inline struct cpumask *sched_group_span(struct sched_group *sg);\n\nDECLARE_STATIC_KEY_FALSE(__sched_core_enabled);\n\nstatic inline bool sched_core_enabled(struct rq *rq)\n{\n\treturn static_branch_unlikely(&__sched_core_enabled) && rq->core_enabled;\n}\n\nstatic inline bool sched_core_disabled(void)\n{\n\treturn !static_branch_unlikely(&__sched_core_enabled);\n}\n\n \nstatic inline raw_spinlock_t *rq_lockp(struct rq *rq)\n{\n\tif (sched_core_enabled(rq))\n\t\treturn &rq->core->__lock;\n\n\treturn &rq->__lock;\n}\n\nstatic inline raw_spinlock_t *__rq_lockp(struct rq *rq)\n{\n\tif (rq->core_enabled)\n\t\treturn &rq->core->__lock;\n\n\treturn &rq->__lock;\n}\n\nbool cfs_prio_less(const struct task_struct *a, const struct task_struct *b,\n\t\t\tbool fi);\nvoid task_vruntime_update(struct rq *rq, struct task_struct *p, bool in_fi);\n\n \nstatic inline bool sched_cpu_cookie_match(struct rq *rq, struct task_struct *p)\n{\n\t \n\tif (!sched_core_enabled(rq))\n\t\treturn true;\n\n\treturn rq->core->core_cookie == p->core_cookie;\n}\n\nstatic inline bool sched_core_cookie_match(struct rq *rq, struct task_struct *p)\n{\n\tbool idle_core = true;\n\tint cpu;\n\n\t \n\tif (!sched_core_enabled(rq))\n\t\treturn true;\n\n\tfor_each_cpu(cpu, cpu_smt_mask(cpu_of(rq))) {\n\t\tif (!available_idle_cpu(cpu)) {\n\t\t\tidle_core = false;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\treturn idle_core || rq->core->core_cookie == p->core_cookie;\n}\n\nstatic inline bool sched_group_cookie_match(struct rq *rq,\n\t\t\t\t\t    struct task_struct *p,\n\t\t\t\t\t    struct sched_group *group)\n{\n\tint cpu;\n\n\t \n\tif (!sched_core_enabled(rq))\n\t\treturn true;\n\n\tfor_each_cpu_and(cpu, sched_group_span(group), p->cpus_ptr) {\n\t\tif (sched_core_cookie_match(cpu_rq(cpu), p))\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic inline bool sched_core_enqueued(struct task_struct *p)\n{\n\treturn !RB_EMPTY_NODE(&p->core_node);\n}\n\nextern void sched_core_enqueue(struct rq *rq, struct task_struct *p);\nextern void sched_core_dequeue(struct rq *rq, struct task_struct *p, int flags);\n\nextern void sched_core_get(void);\nextern void sched_core_put(void);\n\n#else  \n\nstatic inline bool sched_core_enabled(struct rq *rq)\n{\n\treturn false;\n}\n\nstatic inline bool sched_core_disabled(void)\n{\n\treturn true;\n}\n\nstatic inline raw_spinlock_t *rq_lockp(struct rq *rq)\n{\n\treturn &rq->__lock;\n}\n\nstatic inline raw_spinlock_t *__rq_lockp(struct rq *rq)\n{\n\treturn &rq->__lock;\n}\n\nstatic inline bool sched_cpu_cookie_match(struct rq *rq, struct task_struct *p)\n{\n\treturn true;\n}\n\nstatic inline bool sched_core_cookie_match(struct rq *rq, struct task_struct *p)\n{\n\treturn true;\n}\n\nstatic inline bool sched_group_cookie_match(struct rq *rq,\n\t\t\t\t\t    struct task_struct *p,\n\t\t\t\t\t    struct sched_group *group)\n{\n\treturn true;\n}\n#endif  \n\nstatic inline void lockdep_assert_rq_held(struct rq *rq)\n{\n\tlockdep_assert_held(__rq_lockp(rq));\n}\n\nextern void raw_spin_rq_lock_nested(struct rq *rq, int subclass);\nextern bool raw_spin_rq_trylock(struct rq *rq);\nextern void raw_spin_rq_unlock(struct rq *rq);\n\nstatic inline void raw_spin_rq_lock(struct rq *rq)\n{\n\traw_spin_rq_lock_nested(rq, 0);\n}\n\nstatic inline void raw_spin_rq_lock_irq(struct rq *rq)\n{\n\tlocal_irq_disable();\n\traw_spin_rq_lock(rq);\n}\n\nstatic inline void raw_spin_rq_unlock_irq(struct rq *rq)\n{\n\traw_spin_rq_unlock(rq);\n\tlocal_irq_enable();\n}\n\nstatic inline unsigned long _raw_spin_rq_lock_irqsave(struct rq *rq)\n{\n\tunsigned long flags;\n\tlocal_irq_save(flags);\n\traw_spin_rq_lock(rq);\n\treturn flags;\n}\n\nstatic inline void raw_spin_rq_unlock_irqrestore(struct rq *rq, unsigned long flags)\n{\n\traw_spin_rq_unlock(rq);\n\tlocal_irq_restore(flags);\n}\n\n#define raw_spin_rq_lock_irqsave(rq, flags)\t\\\ndo {\t\t\t\t\t\t\\\n\tflags = _raw_spin_rq_lock_irqsave(rq);\t\\\n} while (0)\n\n#ifdef CONFIG_SCHED_SMT\nextern void __update_idle_core(struct rq *rq);\n\nstatic inline void update_idle_core(struct rq *rq)\n{\n\tif (static_branch_unlikely(&sched_smt_present))\n\t\t__update_idle_core(rq);\n}\n\n#else\nstatic inline void update_idle_core(struct rq *rq) { }\n#endif\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\nstatic inline struct task_struct *task_of(struct sched_entity *se)\n{\n\tSCHED_WARN_ON(!entity_is_task(se));\n\treturn container_of(se, struct task_struct, se);\n}\n\nstatic inline struct cfs_rq *task_cfs_rq(struct task_struct *p)\n{\n\treturn p->se.cfs_rq;\n}\n\n \nstatic inline struct cfs_rq *cfs_rq_of(const struct sched_entity *se)\n{\n\treturn se->cfs_rq;\n}\n\n \nstatic inline struct cfs_rq *group_cfs_rq(struct sched_entity *grp)\n{\n\treturn grp->my_q;\n}\n\n#else\n\n#define task_of(_se)\tcontainer_of(_se, struct task_struct, se)\n\nstatic inline struct cfs_rq *task_cfs_rq(const struct task_struct *p)\n{\n\treturn &task_rq(p)->cfs;\n}\n\nstatic inline struct cfs_rq *cfs_rq_of(const struct sched_entity *se)\n{\n\tconst struct task_struct *p = task_of(se);\n\tstruct rq *rq = task_rq(p);\n\n\treturn &rq->cfs;\n}\n\n \nstatic inline struct cfs_rq *group_cfs_rq(struct sched_entity *grp)\n{\n\treturn NULL;\n}\n#endif\n\nextern void update_rq_clock(struct rq *rq);\n\n \n#define RQCF_REQ_SKIP\t\t0x01\n#define RQCF_ACT_SKIP\t\t0x02\n#define RQCF_UPDATED\t\t0x04\n\nstatic inline void assert_clock_updated(struct rq *rq)\n{\n\t \n\tSCHED_WARN_ON(rq->clock_update_flags < RQCF_ACT_SKIP);\n}\n\nstatic inline u64 rq_clock(struct rq *rq)\n{\n\tlockdep_assert_rq_held(rq);\n\tassert_clock_updated(rq);\n\n\treturn rq->clock;\n}\n\nstatic inline u64 rq_clock_task(struct rq *rq)\n{\n\tlockdep_assert_rq_held(rq);\n\tassert_clock_updated(rq);\n\n\treturn rq->clock_task;\n}\n\n \nextern int sched_thermal_decay_shift;\n\nstatic inline u64 rq_clock_thermal(struct rq *rq)\n{\n\treturn rq_clock_task(rq) >> sched_thermal_decay_shift;\n}\n\nstatic inline void rq_clock_skip_update(struct rq *rq)\n{\n\tlockdep_assert_rq_held(rq);\n\trq->clock_update_flags |= RQCF_REQ_SKIP;\n}\n\n \nstatic inline void rq_clock_cancel_skipupdate(struct rq *rq)\n{\n\tlockdep_assert_rq_held(rq);\n\trq->clock_update_flags &= ~RQCF_REQ_SKIP;\n}\n\n \nstatic inline void rq_clock_start_loop_update(struct rq *rq)\n{\n\tlockdep_assert_rq_held(rq);\n\tSCHED_WARN_ON(rq->clock_update_flags & RQCF_ACT_SKIP);\n\trq->clock_update_flags |= RQCF_ACT_SKIP;\n}\n\nstatic inline void rq_clock_stop_loop_update(struct rq *rq)\n{\n\tlockdep_assert_rq_held(rq);\n\trq->clock_update_flags &= ~RQCF_ACT_SKIP;\n}\n\nstruct rq_flags {\n\tunsigned long flags;\n\tstruct pin_cookie cookie;\n#ifdef CONFIG_SCHED_DEBUG\n\t \n\tunsigned int clock_update_flags;\n#endif\n};\n\nextern struct balance_callback balance_push_callback;\n\n \nstatic inline void rq_pin_lock(struct rq *rq, struct rq_flags *rf)\n{\n\trf->cookie = lockdep_pin_lock(__rq_lockp(rq));\n\n#ifdef CONFIG_SCHED_DEBUG\n\trq->clock_update_flags &= (RQCF_REQ_SKIP|RQCF_ACT_SKIP);\n\trf->clock_update_flags = 0;\n#ifdef CONFIG_SMP\n\tSCHED_WARN_ON(rq->balance_callback && rq->balance_callback != &balance_push_callback);\n#endif\n#endif\n}\n\nstatic inline void rq_unpin_lock(struct rq *rq, struct rq_flags *rf)\n{\n#ifdef CONFIG_SCHED_DEBUG\n\tif (rq->clock_update_flags > RQCF_ACT_SKIP)\n\t\trf->clock_update_flags = RQCF_UPDATED;\n#endif\n\n\tlockdep_unpin_lock(__rq_lockp(rq), rf->cookie);\n}\n\nstatic inline void rq_repin_lock(struct rq *rq, struct rq_flags *rf)\n{\n\tlockdep_repin_lock(__rq_lockp(rq), rf->cookie);\n\n#ifdef CONFIG_SCHED_DEBUG\n\t \n\trq->clock_update_flags |= rf->clock_update_flags;\n#endif\n}\n\nstruct rq *__task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(rq->lock);\n\nstruct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(p->pi_lock)\n\t__acquires(rq->lock);\n\nstatic inline void __task_rq_unlock(struct rq *rq, struct rq_flags *rf)\n\t__releases(rq->lock)\n{\n\trq_unpin_lock(rq, rf);\n\traw_spin_rq_unlock(rq);\n}\n\nstatic inline void\ntask_rq_unlock(struct rq *rq, struct task_struct *p, struct rq_flags *rf)\n\t__releases(rq->lock)\n\t__releases(p->pi_lock)\n{\n\trq_unpin_lock(rq, rf);\n\traw_spin_rq_unlock(rq);\n\traw_spin_unlock_irqrestore(&p->pi_lock, rf->flags);\n}\n\nstatic inline void\nrq_lock_irqsave(struct rq *rq, struct rq_flags *rf)\n\t__acquires(rq->lock)\n{\n\traw_spin_rq_lock_irqsave(rq, rf->flags);\n\trq_pin_lock(rq, rf);\n}\n\nstatic inline void\nrq_lock_irq(struct rq *rq, struct rq_flags *rf)\n\t__acquires(rq->lock)\n{\n\traw_spin_rq_lock_irq(rq);\n\trq_pin_lock(rq, rf);\n}\n\nstatic inline void\nrq_lock(struct rq *rq, struct rq_flags *rf)\n\t__acquires(rq->lock)\n{\n\traw_spin_rq_lock(rq);\n\trq_pin_lock(rq, rf);\n}\n\nstatic inline void\nrq_unlock_irqrestore(struct rq *rq, struct rq_flags *rf)\n\t__releases(rq->lock)\n{\n\trq_unpin_lock(rq, rf);\n\traw_spin_rq_unlock_irqrestore(rq, rf->flags);\n}\n\nstatic inline void\nrq_unlock_irq(struct rq *rq, struct rq_flags *rf)\n\t__releases(rq->lock)\n{\n\trq_unpin_lock(rq, rf);\n\traw_spin_rq_unlock_irq(rq);\n}\n\nstatic inline void\nrq_unlock(struct rq *rq, struct rq_flags *rf)\n\t__releases(rq->lock)\n{\n\trq_unpin_lock(rq, rf);\n\traw_spin_rq_unlock(rq);\n}\n\nDEFINE_LOCK_GUARD_1(rq_lock, struct rq,\n\t\t    rq_lock(_T->lock, &_T->rf),\n\t\t    rq_unlock(_T->lock, &_T->rf),\n\t\t    struct rq_flags rf)\n\nDEFINE_LOCK_GUARD_1(rq_lock_irq, struct rq,\n\t\t    rq_lock_irq(_T->lock, &_T->rf),\n\t\t    rq_unlock_irq(_T->lock, &_T->rf),\n\t\t    struct rq_flags rf)\n\nDEFINE_LOCK_GUARD_1(rq_lock_irqsave, struct rq,\n\t\t    rq_lock_irqsave(_T->lock, &_T->rf),\n\t\t    rq_unlock_irqrestore(_T->lock, &_T->rf),\n\t\t    struct rq_flags rf)\n\nstatic inline struct rq *\nthis_rq_lock_irq(struct rq_flags *rf)\n\t__acquires(rq->lock)\n{\n\tstruct rq *rq;\n\n\tlocal_irq_disable();\n\trq = this_rq();\n\trq_lock(rq, rf);\n\treturn rq;\n}\n\n#ifdef CONFIG_NUMA\nenum numa_topology_type {\n\tNUMA_DIRECT,\n\tNUMA_GLUELESS_MESH,\n\tNUMA_BACKPLANE,\n};\nextern enum numa_topology_type sched_numa_topology_type;\nextern int sched_max_numa_distance;\nextern bool find_numa_distance(int distance);\nextern void sched_init_numa(int offline_node);\nextern void sched_update_numa(int cpu, bool online);\nextern void sched_domains_numa_masks_set(unsigned int cpu);\nextern void sched_domains_numa_masks_clear(unsigned int cpu);\nextern int sched_numa_find_closest(const struct cpumask *cpus, int cpu);\n#else\nstatic inline void sched_init_numa(int offline_node) { }\nstatic inline void sched_update_numa(int cpu, bool online) { }\nstatic inline void sched_domains_numa_masks_set(unsigned int cpu) { }\nstatic inline void sched_domains_numa_masks_clear(unsigned int cpu) { }\nstatic inline int sched_numa_find_closest(const struct cpumask *cpus, int cpu)\n{\n\treturn nr_cpu_ids;\n}\n#endif\n\n#ifdef CONFIG_NUMA_BALANCING\n \nenum numa_faults_stats {\n\tNUMA_MEM = 0,\n\tNUMA_CPU,\n\tNUMA_MEMBUF,\n\tNUMA_CPUBUF\n};\nextern void sched_setnuma(struct task_struct *p, int node);\nextern int migrate_task_to(struct task_struct *p, int cpu);\nextern int migrate_swap(struct task_struct *p, struct task_struct *t,\n\t\t\tint cpu, int scpu);\nextern void init_numa_balancing(unsigned long clone_flags, struct task_struct *p);\n#else\nstatic inline void\ninit_numa_balancing(unsigned long clone_flags, struct task_struct *p)\n{\n}\n#endif  \n\n#ifdef CONFIG_SMP\n\nstatic inline void\nqueue_balance_callback(struct rq *rq,\n\t\t       struct balance_callback *head,\n\t\t       void (*func)(struct rq *rq))\n{\n\tlockdep_assert_rq_held(rq);\n\n\t \n\tif (unlikely(head->next || rq->balance_callback == &balance_push_callback))\n\t\treturn;\n\n\thead->func = func;\n\thead->next = rq->balance_callback;\n\trq->balance_callback = head;\n}\n\n#define rcu_dereference_check_sched_domain(p) \\\n\trcu_dereference_check((p), \\\n\t\t\t      lockdep_is_held(&sched_domains_mutex))\n\n \n#define for_each_domain(cpu, __sd) \\\n\tfor (__sd = rcu_dereference_check_sched_domain(cpu_rq(cpu)->sd); \\\n\t\t\t__sd; __sd = __sd->parent)\n\n \n#define SD_FLAG(name, mflags) (name * !!((mflags) & SDF_SHARED_CHILD)) |\nstatic const unsigned int SD_SHARED_CHILD_MASK =\n#include <linux/sched/sd_flags.h>\n0;\n#undef SD_FLAG\n\n \nstatic inline struct sched_domain *highest_flag_domain(int cpu, int flag)\n{\n\tstruct sched_domain *sd, *hsd = NULL;\n\n\tfor_each_domain(cpu, sd) {\n\t\tif (sd->flags & flag) {\n\t\t\thsd = sd;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (flag & SD_SHARED_CHILD_MASK)\n\t\t\tbreak;\n\t}\n\n\treturn hsd;\n}\n\nstatic inline struct sched_domain *lowest_flag_domain(int cpu, int flag)\n{\n\tstruct sched_domain *sd;\n\n\tfor_each_domain(cpu, sd) {\n\t\tif (sd->flags & flag)\n\t\t\tbreak;\n\t}\n\n\treturn sd;\n}\n\nDECLARE_PER_CPU(struct sched_domain __rcu *, sd_llc);\nDECLARE_PER_CPU(int, sd_llc_size);\nDECLARE_PER_CPU(int, sd_llc_id);\nDECLARE_PER_CPU(struct sched_domain_shared __rcu *, sd_llc_shared);\nDECLARE_PER_CPU(struct sched_domain __rcu *, sd_numa);\nDECLARE_PER_CPU(struct sched_domain __rcu *, sd_asym_packing);\nDECLARE_PER_CPU(struct sched_domain __rcu *, sd_asym_cpucapacity);\nextern struct static_key_false sched_asym_cpucapacity;\n\nstatic __always_inline bool sched_asym_cpucap_active(void)\n{\n\treturn static_branch_unlikely(&sched_asym_cpucapacity);\n}\n\nstruct sched_group_capacity {\n\tatomic_t\t\tref;\n\t \n\tunsigned long\t\tcapacity;\n\tunsigned long\t\tmin_capacity;\t\t \n\tunsigned long\t\tmax_capacity;\t\t \n\tunsigned long\t\tnext_update;\n\tint\t\t\timbalance;\t\t \n\n#ifdef CONFIG_SCHED_DEBUG\n\tint\t\t\tid;\n#endif\n\n\tunsigned long\t\tcpumask[];\t\t \n};\n\nstruct sched_group {\n\tstruct sched_group\t*next;\t\t\t \n\tatomic_t\t\tref;\n\n\tunsigned int\t\tgroup_weight;\n\tunsigned int\t\tcores;\n\tstruct sched_group_capacity *sgc;\n\tint\t\t\tasym_prefer_cpu;\t \n\tint\t\t\tflags;\n\n\t \n\tunsigned long\t\tcpumask[];\n};\n\nstatic inline struct cpumask *sched_group_span(struct sched_group *sg)\n{\n\treturn to_cpumask(sg->cpumask);\n}\n\n \nstatic inline struct cpumask *group_balance_mask(struct sched_group *sg)\n{\n\treturn to_cpumask(sg->sgc->cpumask);\n}\n\nextern int group_balance_cpu(struct sched_group *sg);\n\n#ifdef CONFIG_SCHED_DEBUG\nvoid update_sched_domain_debugfs(void);\nvoid dirty_sched_domain_sysctl(int cpu);\n#else\nstatic inline void update_sched_domain_debugfs(void)\n{\n}\nstatic inline void dirty_sched_domain_sysctl(int cpu)\n{\n}\n#endif\n\nextern int sched_update_scaling(void);\n\nstatic inline const struct cpumask *task_user_cpus(struct task_struct *p)\n{\n\tif (!p->user_cpus_ptr)\n\t\treturn cpu_possible_mask;  \n\treturn p->user_cpus_ptr;\n}\n#endif  \n\n#include \"stats.h\"\n\n#if defined(CONFIG_SCHED_CORE) && defined(CONFIG_SCHEDSTATS)\n\nextern void __sched_core_account_forceidle(struct rq *rq);\n\nstatic inline void sched_core_account_forceidle(struct rq *rq)\n{\n\tif (schedstat_enabled())\n\t\t__sched_core_account_forceidle(rq);\n}\n\nextern void __sched_core_tick(struct rq *rq);\n\nstatic inline void sched_core_tick(struct rq *rq)\n{\n\tif (sched_core_enabled(rq) && schedstat_enabled())\n\t\t__sched_core_tick(rq);\n}\n\n#else\n\nstatic inline void sched_core_account_forceidle(struct rq *rq) {}\n\nstatic inline void sched_core_tick(struct rq *rq) {}\n\n#endif  \n\n#ifdef CONFIG_CGROUP_SCHED\n\n \nstatic inline struct task_group *task_group(struct task_struct *p)\n{\n\treturn p->sched_task_group;\n}\n\n \nstatic inline void set_task_rq(struct task_struct *p, unsigned int cpu)\n{\n#if defined(CONFIG_FAIR_GROUP_SCHED) || defined(CONFIG_RT_GROUP_SCHED)\n\tstruct task_group *tg = task_group(p);\n#endif\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tset_task_rq_fair(&p->se, p->se.cfs_rq, tg->cfs_rq[cpu]);\n\tp->se.cfs_rq = tg->cfs_rq[cpu];\n\tp->se.parent = tg->se[cpu];\n\tp->se.depth = tg->se[cpu] ? tg->se[cpu]->depth + 1 : 0;\n#endif\n\n#ifdef CONFIG_RT_GROUP_SCHED\n\tp->rt.rt_rq  = tg->rt_rq[cpu];\n\tp->rt.parent = tg->rt_se[cpu];\n#endif\n}\n\n#else  \n\nstatic inline void set_task_rq(struct task_struct *p, unsigned int cpu) { }\nstatic inline struct task_group *task_group(struct task_struct *p)\n{\n\treturn NULL;\n}\n\n#endif  \n\nstatic inline void __set_task_cpu(struct task_struct *p, unsigned int cpu)\n{\n\tset_task_rq(p, cpu);\n#ifdef CONFIG_SMP\n\t \n\tsmp_wmb();\n\tWRITE_ONCE(task_thread_info(p)->cpu, cpu);\n\tp->wake_cpu = cpu;\n#endif\n}\n\n \n#ifdef CONFIG_SCHED_DEBUG\n# define const_debug __read_mostly\n#else\n# define const_debug const\n#endif\n\n#define SCHED_FEAT(name, enabled)\t\\\n\t__SCHED_FEAT_##name ,\n\nenum {\n#include \"features.h\"\n\t__SCHED_FEAT_NR,\n};\n\n#undef SCHED_FEAT\n\n#ifdef CONFIG_SCHED_DEBUG\n\n \nextern const_debug unsigned int sysctl_sched_features;\n\n#ifdef CONFIG_JUMP_LABEL\n#define SCHED_FEAT(name, enabled)\t\t\t\t\t\\\nstatic __always_inline bool static_branch_##name(struct static_key *key) \\\n{\t\t\t\t\t\t\t\t\t\\\n\treturn static_key_##enabled(key);\t\t\t\t\\\n}\n\n#include \"features.h\"\n#undef SCHED_FEAT\n\nextern struct static_key sched_feat_keys[__SCHED_FEAT_NR];\n#define sched_feat(x) (static_branch_##x(&sched_feat_keys[__SCHED_FEAT_##x]))\n\n#else  \n\n#define sched_feat(x) (sysctl_sched_features & (1UL << __SCHED_FEAT_##x))\n\n#endif  \n\n#else  \n\n \n#define SCHED_FEAT(name, enabled)\t\\\n\t(1UL << __SCHED_FEAT_##name) * enabled |\nstatic const_debug __maybe_unused unsigned int sysctl_sched_features =\n#include \"features.h\"\n\t0;\n#undef SCHED_FEAT\n\n#define sched_feat(x) !!(sysctl_sched_features & (1UL << __SCHED_FEAT_##x))\n\n#endif  \n\nextern struct static_key_false sched_numa_balancing;\nextern struct static_key_false sched_schedstats;\n\nstatic inline u64 global_rt_period(void)\n{\n\treturn (u64)sysctl_sched_rt_period * NSEC_PER_USEC;\n}\n\nstatic inline u64 global_rt_runtime(void)\n{\n\tif (sysctl_sched_rt_runtime < 0)\n\t\treturn RUNTIME_INF;\n\n\treturn (u64)sysctl_sched_rt_runtime * NSEC_PER_USEC;\n}\n\nstatic inline int task_current(struct rq *rq, struct task_struct *p)\n{\n\treturn rq->curr == p;\n}\n\nstatic inline int task_on_cpu(struct rq *rq, struct task_struct *p)\n{\n#ifdef CONFIG_SMP\n\treturn p->on_cpu;\n#else\n\treturn task_current(rq, p);\n#endif\n}\n\nstatic inline int task_on_rq_queued(struct task_struct *p)\n{\n\treturn p->on_rq == TASK_ON_RQ_QUEUED;\n}\n\nstatic inline int task_on_rq_migrating(struct task_struct *p)\n{\n\treturn READ_ONCE(p->on_rq) == TASK_ON_RQ_MIGRATING;\n}\n\n \n#define WF_EXEC         0x02  \n#define WF_FORK         0x04  \n#define WF_TTWU         0x08  \n\n#define WF_SYNC         0x10  \n#define WF_MIGRATED     0x20  \n#define WF_CURRENT_CPU  0x40  \n\n#ifdef CONFIG_SMP\nstatic_assert(WF_EXEC == SD_BALANCE_EXEC);\nstatic_assert(WF_FORK == SD_BALANCE_FORK);\nstatic_assert(WF_TTWU == SD_BALANCE_WAKE);\n#endif\n\n \n\n#define WEIGHT_IDLEPRIO\t\t3\n#define WMULT_IDLEPRIO\t\t1431655765\n\nextern const int\t\tsched_prio_to_weight[40];\nextern const u32\t\tsched_prio_to_wmult[40];\n\n \n\n#define DEQUEUE_SLEEP\t\t0x01\n#define DEQUEUE_SAVE\t\t0x02  \n#define DEQUEUE_MOVE\t\t0x04  \n#define DEQUEUE_NOCLOCK\t\t0x08  \n\n#define ENQUEUE_WAKEUP\t\t0x01\n#define ENQUEUE_RESTORE\t\t0x02\n#define ENQUEUE_MOVE\t\t0x04\n#define ENQUEUE_NOCLOCK\t\t0x08\n\n#define ENQUEUE_HEAD\t\t0x10\n#define ENQUEUE_REPLENISH\t0x20\n#ifdef CONFIG_SMP\n#define ENQUEUE_MIGRATED\t0x40\n#else\n#define ENQUEUE_MIGRATED\t0x00\n#endif\n#define ENQUEUE_INITIAL\t\t0x80\n\n#define RETRY_TASK\t\t((void *)-1UL)\n\nstruct affinity_context {\n\tconst struct cpumask *new_mask;\n\tstruct cpumask *user_mask;\n\tunsigned int flags;\n};\n\nstruct sched_class {\n\n#ifdef CONFIG_UCLAMP_TASK\n\tint uclamp_enabled;\n#endif\n\n\tvoid (*enqueue_task) (struct rq *rq, struct task_struct *p, int flags);\n\tvoid (*dequeue_task) (struct rq *rq, struct task_struct *p, int flags);\n\tvoid (*yield_task)   (struct rq *rq);\n\tbool (*yield_to_task)(struct rq *rq, struct task_struct *p);\n\n\tvoid (*check_preempt_curr)(struct rq *rq, struct task_struct *p, int flags);\n\n\tstruct task_struct *(*pick_next_task)(struct rq *rq);\n\n\tvoid (*put_prev_task)(struct rq *rq, struct task_struct *p);\n\tvoid (*set_next_task)(struct rq *rq, struct task_struct *p, bool first);\n\n#ifdef CONFIG_SMP\n\tint (*balance)(struct rq *rq, struct task_struct *prev, struct rq_flags *rf);\n\tint  (*select_task_rq)(struct task_struct *p, int task_cpu, int flags);\n\n\tstruct task_struct * (*pick_task)(struct rq *rq);\n\n\tvoid (*migrate_task_rq)(struct task_struct *p, int new_cpu);\n\n\tvoid (*task_woken)(struct rq *this_rq, struct task_struct *task);\n\n\tvoid (*set_cpus_allowed)(struct task_struct *p, struct affinity_context *ctx);\n\n\tvoid (*rq_online)(struct rq *rq);\n\tvoid (*rq_offline)(struct rq *rq);\n\n\tstruct rq *(*find_lock_rq)(struct task_struct *p, struct rq *rq);\n#endif\n\n\tvoid (*task_tick)(struct rq *rq, struct task_struct *p, int queued);\n\tvoid (*task_fork)(struct task_struct *p);\n\tvoid (*task_dead)(struct task_struct *p);\n\n\t \n\tvoid (*switched_from)(struct rq *this_rq, struct task_struct *task);\n\tvoid (*switched_to)  (struct rq *this_rq, struct task_struct *task);\n\tvoid (*prio_changed) (struct rq *this_rq, struct task_struct *task,\n\t\t\t      int oldprio);\n\n\tunsigned int (*get_rr_interval)(struct rq *rq,\n\t\t\t\t\tstruct task_struct *task);\n\n\tvoid (*update_curr)(struct rq *rq);\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tvoid (*task_change_group)(struct task_struct *p);\n#endif\n\n#ifdef CONFIG_SCHED_CORE\n\tint (*task_is_throttled)(struct task_struct *p, int cpu);\n#endif\n};\n\nstatic inline void put_prev_task(struct rq *rq, struct task_struct *prev)\n{\n\tWARN_ON_ONCE(rq->curr != prev);\n\tprev->sched_class->put_prev_task(rq, prev);\n}\n\nstatic inline void set_next_task(struct rq *rq, struct task_struct *next)\n{\n\tnext->sched_class->set_next_task(rq, next, false);\n}\n\n\n \n#define DEFINE_SCHED_CLASS(name) \\\nconst struct sched_class name##_sched_class \\\n\t__aligned(__alignof__(struct sched_class)) \\\n\t__section(\"__\" #name \"_sched_class\")\n\n \nextern struct sched_class __sched_class_highest[];\nextern struct sched_class __sched_class_lowest[];\n\n#define for_class_range(class, _from, _to) \\\n\tfor (class = (_from); class < (_to); class++)\n\n#define for_each_class(class) \\\n\tfor_class_range(class, __sched_class_highest, __sched_class_lowest)\n\n#define sched_class_above(_a, _b)\t((_a) < (_b))\n\nextern const struct sched_class stop_sched_class;\nextern const struct sched_class dl_sched_class;\nextern const struct sched_class rt_sched_class;\nextern const struct sched_class fair_sched_class;\nextern const struct sched_class idle_sched_class;\n\nstatic inline bool sched_stop_runnable(struct rq *rq)\n{\n\treturn rq->stop && task_on_rq_queued(rq->stop);\n}\n\nstatic inline bool sched_dl_runnable(struct rq *rq)\n{\n\treturn rq->dl.dl_nr_running > 0;\n}\n\nstatic inline bool sched_rt_runnable(struct rq *rq)\n{\n\treturn rq->rt.rt_queued > 0;\n}\n\nstatic inline bool sched_fair_runnable(struct rq *rq)\n{\n\treturn rq->cfs.nr_running > 0;\n}\n\nextern struct task_struct *pick_next_task_fair(struct rq *rq, struct task_struct *prev, struct rq_flags *rf);\nextern struct task_struct *pick_next_task_idle(struct rq *rq);\n\n#define SCA_CHECK\t\t0x01\n#define SCA_MIGRATE_DISABLE\t0x02\n#define SCA_MIGRATE_ENABLE\t0x04\n#define SCA_USER\t\t0x08\n\n#ifdef CONFIG_SMP\n\nextern void update_group_capacity(struct sched_domain *sd, int cpu);\n\nextern void trigger_load_balance(struct rq *rq);\n\nextern void set_cpus_allowed_common(struct task_struct *p, struct affinity_context *ctx);\n\nstatic inline struct task_struct *get_push_task(struct rq *rq)\n{\n\tstruct task_struct *p = rq->curr;\n\n\tlockdep_assert_rq_held(rq);\n\n\tif (rq->push_busy)\n\t\treturn NULL;\n\n\tif (p->nr_cpus_allowed == 1)\n\t\treturn NULL;\n\n\tif (p->migration_disabled)\n\t\treturn NULL;\n\n\trq->push_busy = true;\n\treturn get_task_struct(p);\n}\n\nextern int push_cpu_stop(void *arg);\n\n#endif\n\n#ifdef CONFIG_CPU_IDLE\nstatic inline void idle_set_state(struct rq *rq,\n\t\t\t\t  struct cpuidle_state *idle_state)\n{\n\trq->idle_state = idle_state;\n}\n\nstatic inline struct cpuidle_state *idle_get_state(struct rq *rq)\n{\n\tSCHED_WARN_ON(!rcu_read_lock_held());\n\n\treturn rq->idle_state;\n}\n#else\nstatic inline void idle_set_state(struct rq *rq,\n\t\t\t\t  struct cpuidle_state *idle_state)\n{\n}\n\nstatic inline struct cpuidle_state *idle_get_state(struct rq *rq)\n{\n\treturn NULL;\n}\n#endif\n\nextern void schedule_idle(void);\nasmlinkage void schedule_user(void);\n\nextern void sysrq_sched_debug_show(void);\nextern void sched_init_granularity(void);\nextern void update_max_interval(void);\n\nextern void init_sched_dl_class(void);\nextern void init_sched_rt_class(void);\nextern void init_sched_fair_class(void);\n\nextern void reweight_task(struct task_struct *p, int prio);\n\nextern void resched_curr(struct rq *rq);\nextern void resched_cpu(int cpu);\n\nextern struct rt_bandwidth def_rt_bandwidth;\nextern void init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime);\nextern bool sched_rt_bandwidth_account(struct rt_rq *rt_rq);\n\nextern void init_dl_task_timer(struct sched_dl_entity *dl_se);\nextern void init_dl_inactive_task_timer(struct sched_dl_entity *dl_se);\n\n#define BW_SHIFT\t\t20\n#define BW_UNIT\t\t\t(1 << BW_SHIFT)\n#define RATIO_SHIFT\t\t8\n#define MAX_BW_BITS\t\t(64 - BW_SHIFT)\n#define MAX_BW\t\t\t((1ULL << MAX_BW_BITS) - 1)\nunsigned long to_ratio(u64 period, u64 runtime);\n\nextern void init_entity_runnable_average(struct sched_entity *se);\nextern void post_init_entity_util_avg(struct task_struct *p);\n\n#ifdef CONFIG_NO_HZ_FULL\nextern bool sched_can_stop_tick(struct rq *rq);\nextern int __init sched_tick_offload_init(void);\n\n \nstatic inline void sched_update_tick_dependency(struct rq *rq)\n{\n\tint cpu = cpu_of(rq);\n\n\tif (!tick_nohz_full_cpu(cpu))\n\t\treturn;\n\n\tif (sched_can_stop_tick(rq))\n\t\ttick_nohz_dep_clear_cpu(cpu, TICK_DEP_BIT_SCHED);\n\telse\n\t\ttick_nohz_dep_set_cpu(cpu, TICK_DEP_BIT_SCHED);\n}\n#else\nstatic inline int sched_tick_offload_init(void) { return 0; }\nstatic inline void sched_update_tick_dependency(struct rq *rq) { }\n#endif\n\nstatic inline void add_nr_running(struct rq *rq, unsigned count)\n{\n\tunsigned prev_nr = rq->nr_running;\n\n\trq->nr_running = prev_nr + count;\n\tif (trace_sched_update_nr_running_tp_enabled()) {\n\t\tcall_trace_sched_update_nr_running(rq, count);\n\t}\n\n#ifdef CONFIG_SMP\n\tif (prev_nr < 2 && rq->nr_running >= 2) {\n\t\tif (!READ_ONCE(rq->rd->overload))\n\t\t\tWRITE_ONCE(rq->rd->overload, 1);\n\t}\n#endif\n\n\tsched_update_tick_dependency(rq);\n}\n\nstatic inline void sub_nr_running(struct rq *rq, unsigned count)\n{\n\trq->nr_running -= count;\n\tif (trace_sched_update_nr_running_tp_enabled()) {\n\t\tcall_trace_sched_update_nr_running(rq, -count);\n\t}\n\n\t \n\tsched_update_tick_dependency(rq);\n}\n\nextern void activate_task(struct rq *rq, struct task_struct *p, int flags);\nextern void deactivate_task(struct rq *rq, struct task_struct *p, int flags);\n\nextern void check_preempt_curr(struct rq *rq, struct task_struct *p, int flags);\n\n#ifdef CONFIG_PREEMPT_RT\n#define SCHED_NR_MIGRATE_BREAK 8\n#else\n#define SCHED_NR_MIGRATE_BREAK 32\n#endif\n\nextern const_debug unsigned int sysctl_sched_nr_migrate;\nextern const_debug unsigned int sysctl_sched_migration_cost;\n\nextern unsigned int sysctl_sched_base_slice;\n\n#ifdef CONFIG_SCHED_DEBUG\nextern int sysctl_resched_latency_warn_ms;\nextern int sysctl_resched_latency_warn_once;\n\nextern unsigned int sysctl_sched_tunable_scaling;\n\nextern unsigned int sysctl_numa_balancing_scan_delay;\nextern unsigned int sysctl_numa_balancing_scan_period_min;\nextern unsigned int sysctl_numa_balancing_scan_period_max;\nextern unsigned int sysctl_numa_balancing_scan_size;\nextern unsigned int sysctl_numa_balancing_hot_threshold;\n#endif\n\n#ifdef CONFIG_SCHED_HRTICK\n\n \nstatic inline int hrtick_enabled(struct rq *rq)\n{\n\tif (!cpu_active(cpu_of(rq)))\n\t\treturn 0;\n\treturn hrtimer_is_hres_active(&rq->hrtick_timer);\n}\n\nstatic inline int hrtick_enabled_fair(struct rq *rq)\n{\n\tif (!sched_feat(HRTICK))\n\t\treturn 0;\n\treturn hrtick_enabled(rq);\n}\n\nstatic inline int hrtick_enabled_dl(struct rq *rq)\n{\n\tif (!sched_feat(HRTICK_DL))\n\t\treturn 0;\n\treturn hrtick_enabled(rq);\n}\n\nvoid hrtick_start(struct rq *rq, u64 delay);\n\n#else\n\nstatic inline int hrtick_enabled_fair(struct rq *rq)\n{\n\treturn 0;\n}\n\nstatic inline int hrtick_enabled_dl(struct rq *rq)\n{\n\treturn 0;\n}\n\nstatic inline int hrtick_enabled(struct rq *rq)\n{\n\treturn 0;\n}\n\n#endif  \n\n#ifndef arch_scale_freq_tick\nstatic __always_inline\nvoid arch_scale_freq_tick(void)\n{\n}\n#endif\n\n#ifndef arch_scale_freq_capacity\n \nstatic __always_inline\nunsigned long arch_scale_freq_capacity(int cpu)\n{\n\treturn SCHED_CAPACITY_SCALE;\n}\n#endif\n\n#ifdef CONFIG_SCHED_DEBUG\n \nstatic inline void double_rq_clock_clear_update(struct rq *rq1, struct rq *rq2)\n{\n\trq1->clock_update_flags &= (RQCF_REQ_SKIP|RQCF_ACT_SKIP);\n\t \n#ifdef CONFIG_SMP\n\trq2->clock_update_flags &= (RQCF_REQ_SKIP|RQCF_ACT_SKIP);\n#endif\n}\n#else\nstatic inline void double_rq_clock_clear_update(struct rq *rq1, struct rq *rq2) {}\n#endif\n\n#define DEFINE_LOCK_GUARD_2(name, type, _lock, _unlock, ...)\t\t\\\n__DEFINE_UNLOCK_GUARD(name, type, _unlock, type *lock2; __VA_ARGS__) \\\nstatic inline class_##name##_t class_##name##_constructor(type *lock, type *lock2) \\\n{ class_##name##_t _t = { .lock = lock, .lock2 = lock2 }, *_T = &_t;\t\\\n  _lock; return _t; }\n\n#ifdef CONFIG_SMP\n\nstatic inline bool rq_order_less(struct rq *rq1, struct rq *rq2)\n{\n#ifdef CONFIG_SCHED_CORE\n\t \n\tif (rq1->core->cpu < rq2->core->cpu)\n\t\treturn true;\n\tif (rq1->core->cpu > rq2->core->cpu)\n\t\treturn false;\n\n\t \n#endif\n\treturn rq1->cpu < rq2->cpu;\n}\n\nextern void double_rq_lock(struct rq *rq1, struct rq *rq2);\n\n#ifdef CONFIG_PREEMPTION\n\n \nstatic inline int _double_lock_balance(struct rq *this_rq, struct rq *busiest)\n\t__releases(this_rq->lock)\n\t__acquires(busiest->lock)\n\t__acquires(this_rq->lock)\n{\n\traw_spin_rq_unlock(this_rq);\n\tdouble_rq_lock(this_rq, busiest);\n\n\treturn 1;\n}\n\n#else\n \nstatic inline int _double_lock_balance(struct rq *this_rq, struct rq *busiest)\n\t__releases(this_rq->lock)\n\t__acquires(busiest->lock)\n\t__acquires(this_rq->lock)\n{\n\tif (__rq_lockp(this_rq) == __rq_lockp(busiest) ||\n\t    likely(raw_spin_rq_trylock(busiest))) {\n\t\tdouble_rq_clock_clear_update(this_rq, busiest);\n\t\treturn 0;\n\t}\n\n\tif (rq_order_less(this_rq, busiest)) {\n\t\traw_spin_rq_lock_nested(busiest, SINGLE_DEPTH_NESTING);\n\t\tdouble_rq_clock_clear_update(this_rq, busiest);\n\t\treturn 0;\n\t}\n\n\traw_spin_rq_unlock(this_rq);\n\tdouble_rq_lock(this_rq, busiest);\n\n\treturn 1;\n}\n\n#endif  \n\n \nstatic inline int double_lock_balance(struct rq *this_rq, struct rq *busiest)\n{\n\tlockdep_assert_irqs_disabled();\n\n\treturn _double_lock_balance(this_rq, busiest);\n}\n\nstatic inline void double_unlock_balance(struct rq *this_rq, struct rq *busiest)\n\t__releases(busiest->lock)\n{\n\tif (__rq_lockp(this_rq) != __rq_lockp(busiest))\n\t\traw_spin_rq_unlock(busiest);\n\tlock_set_subclass(&__rq_lockp(this_rq)->dep_map, 0, _RET_IP_);\n}\n\nstatic inline void double_lock(spinlock_t *l1, spinlock_t *l2)\n{\n\tif (l1 > l2)\n\t\tswap(l1, l2);\n\n\tspin_lock(l1);\n\tspin_lock_nested(l2, SINGLE_DEPTH_NESTING);\n}\n\nstatic inline void double_lock_irq(spinlock_t *l1, spinlock_t *l2)\n{\n\tif (l1 > l2)\n\t\tswap(l1, l2);\n\n\tspin_lock_irq(l1);\n\tspin_lock_nested(l2, SINGLE_DEPTH_NESTING);\n}\n\nstatic inline void double_raw_lock(raw_spinlock_t *l1, raw_spinlock_t *l2)\n{\n\tif (l1 > l2)\n\t\tswap(l1, l2);\n\n\traw_spin_lock(l1);\n\traw_spin_lock_nested(l2, SINGLE_DEPTH_NESTING);\n}\n\nstatic inline void double_raw_unlock(raw_spinlock_t *l1, raw_spinlock_t *l2)\n{\n\traw_spin_unlock(l1);\n\traw_spin_unlock(l2);\n}\n\nDEFINE_LOCK_GUARD_2(double_raw_spinlock, raw_spinlock_t,\n\t\t    double_raw_lock(_T->lock, _T->lock2),\n\t\t    double_raw_unlock(_T->lock, _T->lock2))\n\n \nstatic inline void double_rq_unlock(struct rq *rq1, struct rq *rq2)\n\t__releases(rq1->lock)\n\t__releases(rq2->lock)\n{\n\tif (__rq_lockp(rq1) != __rq_lockp(rq2))\n\t\traw_spin_rq_unlock(rq2);\n\telse\n\t\t__release(rq2->lock);\n\traw_spin_rq_unlock(rq1);\n}\n\nextern void set_rq_online (struct rq *rq);\nextern void set_rq_offline(struct rq *rq);\nextern bool sched_smp_initialized;\n\n#else  \n\n \nstatic inline void double_rq_lock(struct rq *rq1, struct rq *rq2)\n\t__acquires(rq1->lock)\n\t__acquires(rq2->lock)\n{\n\tWARN_ON_ONCE(!irqs_disabled());\n\tWARN_ON_ONCE(rq1 != rq2);\n\traw_spin_rq_lock(rq1);\n\t__acquire(rq2->lock);\t \n\tdouble_rq_clock_clear_update(rq1, rq2);\n}\n\n \nstatic inline void double_rq_unlock(struct rq *rq1, struct rq *rq2)\n\t__releases(rq1->lock)\n\t__releases(rq2->lock)\n{\n\tWARN_ON_ONCE(rq1 != rq2);\n\traw_spin_rq_unlock(rq1);\n\t__release(rq2->lock);\n}\n\n#endif\n\nDEFINE_LOCK_GUARD_2(double_rq_lock, struct rq,\n\t\t    double_rq_lock(_T->lock, _T->lock2),\n\t\t    double_rq_unlock(_T->lock, _T->lock2))\n\nextern struct sched_entity *__pick_first_entity(struct cfs_rq *cfs_rq);\nextern struct sched_entity *__pick_last_entity(struct cfs_rq *cfs_rq);\n\n#ifdef\tCONFIG_SCHED_DEBUG\nextern bool sched_debug_verbose;\n\nextern void print_cfs_stats(struct seq_file *m, int cpu);\nextern void print_rt_stats(struct seq_file *m, int cpu);\nextern void print_dl_stats(struct seq_file *m, int cpu);\nextern void print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq);\nextern void print_rt_rq(struct seq_file *m, int cpu, struct rt_rq *rt_rq);\nextern void print_dl_rq(struct seq_file *m, int cpu, struct dl_rq *dl_rq);\n\nextern void resched_latency_warn(int cpu, u64 latency);\n#ifdef CONFIG_NUMA_BALANCING\nextern void\nshow_numa_stats(struct task_struct *p, struct seq_file *m);\nextern void\nprint_numa_stats(struct seq_file *m, int node, unsigned long tsf,\n\tunsigned long tpf, unsigned long gsf, unsigned long gpf);\n#endif  \n#else\nstatic inline void resched_latency_warn(int cpu, u64 latency) {}\n#endif  \n\nextern void init_cfs_rq(struct cfs_rq *cfs_rq);\nextern void init_rt_rq(struct rt_rq *rt_rq);\nextern void init_dl_rq(struct dl_rq *dl_rq);\n\nextern void cfs_bandwidth_usage_inc(void);\nextern void cfs_bandwidth_usage_dec(void);\n\n#ifdef CONFIG_NO_HZ_COMMON\n#define NOHZ_BALANCE_KICK_BIT\t0\n#define NOHZ_STATS_KICK_BIT\t1\n#define NOHZ_NEWILB_KICK_BIT\t2\n#define NOHZ_NEXT_KICK_BIT\t3\n\n \n#define NOHZ_BALANCE_KICK\tBIT(NOHZ_BALANCE_KICK_BIT)\n \n#define NOHZ_STATS_KICK\t\tBIT(NOHZ_STATS_KICK_BIT)\n \n#define NOHZ_NEWILB_KICK\tBIT(NOHZ_NEWILB_KICK_BIT)\n \n#define NOHZ_NEXT_KICK\t\tBIT(NOHZ_NEXT_KICK_BIT)\n\n#define NOHZ_KICK_MASK\t(NOHZ_BALANCE_KICK | NOHZ_STATS_KICK | NOHZ_NEXT_KICK)\n\n#define nohz_flags(cpu)\t(&cpu_rq(cpu)->nohz_flags)\n\nextern void nohz_balance_exit_idle(struct rq *rq);\n#else\nstatic inline void nohz_balance_exit_idle(struct rq *rq) { }\n#endif\n\n#if defined(CONFIG_SMP) && defined(CONFIG_NO_HZ_COMMON)\nextern void nohz_run_idle_balance(int cpu);\n#else\nstatic inline void nohz_run_idle_balance(int cpu) { }\n#endif\n\n#ifdef CONFIG_IRQ_TIME_ACCOUNTING\nstruct irqtime {\n\tu64\t\t\ttotal;\n\tu64\t\t\ttick_delta;\n\tu64\t\t\tirq_start_time;\n\tstruct u64_stats_sync\tsync;\n};\n\nDECLARE_PER_CPU(struct irqtime, cpu_irqtime);\n\n \nstatic inline u64 irq_time_read(int cpu)\n{\n\tstruct irqtime *irqtime = &per_cpu(cpu_irqtime, cpu);\n\tunsigned int seq;\n\tu64 total;\n\n\tdo {\n\t\tseq = __u64_stats_fetch_begin(&irqtime->sync);\n\t\ttotal = irqtime->total;\n\t} while (__u64_stats_fetch_retry(&irqtime->sync, seq));\n\n\treturn total;\n}\n#endif  \n\n#ifdef CONFIG_CPU_FREQ\nDECLARE_PER_CPU(struct update_util_data __rcu *, cpufreq_update_util_data);\n\n \nstatic inline void cpufreq_update_util(struct rq *rq, unsigned int flags)\n{\n\tstruct update_util_data *data;\n\n\tdata = rcu_dereference_sched(*per_cpu_ptr(&cpufreq_update_util_data,\n\t\t\t\t\t\t  cpu_of(rq)));\n\tif (data)\n\t\tdata->func(data, rq_clock(rq), flags);\n}\n#else\nstatic inline void cpufreq_update_util(struct rq *rq, unsigned int flags) {}\n#endif  \n\n#ifdef arch_scale_freq_capacity\n# ifndef arch_scale_freq_invariant\n#  define arch_scale_freq_invariant()\ttrue\n# endif\n#else\n# define arch_scale_freq_invariant()\tfalse\n#endif\n\n#ifdef CONFIG_SMP\nstatic inline unsigned long capacity_orig_of(int cpu)\n{\n\treturn cpu_rq(cpu)->cpu_capacity_orig;\n}\n\n \nenum cpu_util_type {\n\tFREQUENCY_UTIL,\n\tENERGY_UTIL,\n};\n\nunsigned long effective_cpu_util(int cpu, unsigned long util_cfs,\n\t\t\t\t enum cpu_util_type type,\n\t\t\t\t struct task_struct *p);\n\n \nstatic inline bool dl_task_fits_capacity(struct task_struct *p, int cpu)\n{\n\tunsigned long cap = arch_scale_cpu_capacity(cpu);\n\n\treturn cap >= p->dl.dl_density >> (BW_SHIFT - SCHED_CAPACITY_SHIFT);\n}\n\nstatic inline unsigned long cpu_bw_dl(struct rq *rq)\n{\n\treturn (rq->dl.running_bw * SCHED_CAPACITY_SCALE) >> BW_SHIFT;\n}\n\nstatic inline unsigned long cpu_util_dl(struct rq *rq)\n{\n\treturn READ_ONCE(rq->avg_dl.util_avg);\n}\n\n\nextern unsigned long cpu_util_cfs(int cpu);\nextern unsigned long cpu_util_cfs_boost(int cpu);\n\nstatic inline unsigned long cpu_util_rt(struct rq *rq)\n{\n\treturn READ_ONCE(rq->avg_rt.util_avg);\n}\n#endif\n\n#ifdef CONFIG_UCLAMP_TASK\nunsigned long uclamp_eff_value(struct task_struct *p, enum uclamp_id clamp_id);\n\nstatic inline unsigned long uclamp_rq_get(struct rq *rq,\n\t\t\t\t\t  enum uclamp_id clamp_id)\n{\n\treturn READ_ONCE(rq->uclamp[clamp_id].value);\n}\n\nstatic inline void uclamp_rq_set(struct rq *rq, enum uclamp_id clamp_id,\n\t\t\t\t unsigned int value)\n{\n\tWRITE_ONCE(rq->uclamp[clamp_id].value, value);\n}\n\nstatic inline bool uclamp_rq_is_idle(struct rq *rq)\n{\n\treturn rq->uclamp_flags & UCLAMP_FLAG_IDLE;\n}\n\n \nstatic __always_inline\nunsigned long uclamp_rq_util_with(struct rq *rq, unsigned long util,\n\t\t\t\t  struct task_struct *p)\n{\n\tunsigned long min_util = 0;\n\tunsigned long max_util = 0;\n\n\tif (!static_branch_likely(&sched_uclamp_used))\n\t\treturn util;\n\n\tif (p) {\n\t\tmin_util = uclamp_eff_value(p, UCLAMP_MIN);\n\t\tmax_util = uclamp_eff_value(p, UCLAMP_MAX);\n\n\t\t \n\t\tif (uclamp_rq_is_idle(rq))\n\t\t\tgoto out;\n\t}\n\n\tmin_util = max_t(unsigned long, min_util, uclamp_rq_get(rq, UCLAMP_MIN));\n\tmax_util = max_t(unsigned long, max_util, uclamp_rq_get(rq, UCLAMP_MAX));\nout:\n\t \n\tif (unlikely(min_util >= max_util))\n\t\treturn min_util;\n\n\treturn clamp(util, min_util, max_util);\n}\n\n \nstatic inline bool uclamp_rq_is_capped(struct rq *rq)\n{\n\tunsigned long rq_util;\n\tunsigned long max_util;\n\n\tif (!static_branch_likely(&sched_uclamp_used))\n\t\treturn false;\n\n\trq_util = cpu_util_cfs(cpu_of(rq)) + cpu_util_rt(rq);\n\tmax_util = READ_ONCE(rq->uclamp[UCLAMP_MAX].value);\n\n\treturn max_util != SCHED_CAPACITY_SCALE && rq_util >= max_util;\n}\n\n \nstatic inline bool uclamp_is_used(void)\n{\n\treturn static_branch_likely(&sched_uclamp_used);\n}\n#else  \nstatic inline unsigned long uclamp_eff_value(struct task_struct *p,\n\t\t\t\t\t     enum uclamp_id clamp_id)\n{\n\tif (clamp_id == UCLAMP_MIN)\n\t\treturn 0;\n\n\treturn SCHED_CAPACITY_SCALE;\n}\n\nstatic inline\nunsigned long uclamp_rq_util_with(struct rq *rq, unsigned long util,\n\t\t\t\t  struct task_struct *p)\n{\n\treturn util;\n}\n\nstatic inline bool uclamp_rq_is_capped(struct rq *rq) { return false; }\n\nstatic inline bool uclamp_is_used(void)\n{\n\treturn false;\n}\n\nstatic inline unsigned long uclamp_rq_get(struct rq *rq,\n\t\t\t\t\t  enum uclamp_id clamp_id)\n{\n\tif (clamp_id == UCLAMP_MIN)\n\t\treturn 0;\n\n\treturn SCHED_CAPACITY_SCALE;\n}\n\nstatic inline void uclamp_rq_set(struct rq *rq, enum uclamp_id clamp_id,\n\t\t\t\t unsigned int value)\n{\n}\n\nstatic inline bool uclamp_rq_is_idle(struct rq *rq)\n{\n\treturn false;\n}\n#endif  \n\n#ifdef CONFIG_HAVE_SCHED_AVG_IRQ\nstatic inline unsigned long cpu_util_irq(struct rq *rq)\n{\n\treturn rq->avg_irq.util_avg;\n}\n\nstatic inline\nunsigned long scale_irq_capacity(unsigned long util, unsigned long irq, unsigned long max)\n{\n\tutil *= (max - irq);\n\tutil /= max;\n\n\treturn util;\n\n}\n#else\nstatic inline unsigned long cpu_util_irq(struct rq *rq)\n{\n\treturn 0;\n}\n\nstatic inline\nunsigned long scale_irq_capacity(unsigned long util, unsigned long irq, unsigned long max)\n{\n\treturn util;\n}\n#endif\n\n#if defined(CONFIG_ENERGY_MODEL) && defined(CONFIG_CPU_FREQ_GOV_SCHEDUTIL)\n\n#define perf_domain_span(pd) (to_cpumask(((pd)->em_pd->cpus)))\n\nDECLARE_STATIC_KEY_FALSE(sched_energy_present);\n\nstatic inline bool sched_energy_enabled(void)\n{\n\treturn static_branch_unlikely(&sched_energy_present);\n}\n\n#else  \n\n#define perf_domain_span(pd) NULL\nstatic inline bool sched_energy_enabled(void) { return false; }\n\n#endif  \n\n#ifdef CONFIG_MEMBARRIER\n \nstatic inline void membarrier_switch_mm(struct rq *rq,\n\t\t\t\t\tstruct mm_struct *prev_mm,\n\t\t\t\t\tstruct mm_struct *next_mm)\n{\n\tint membarrier_state;\n\n\tif (prev_mm == next_mm)\n\t\treturn;\n\n\tmembarrier_state = atomic_read(&next_mm->membarrier_state);\n\tif (READ_ONCE(rq->membarrier_state) == membarrier_state)\n\t\treturn;\n\n\tWRITE_ONCE(rq->membarrier_state, membarrier_state);\n}\n#else\nstatic inline void membarrier_switch_mm(struct rq *rq,\n\t\t\t\t\tstruct mm_struct *prev_mm,\n\t\t\t\t\tstruct mm_struct *next_mm)\n{\n}\n#endif\n\n#ifdef CONFIG_SMP\nstatic inline bool is_per_cpu_kthread(struct task_struct *p)\n{\n\tif (!(p->flags & PF_KTHREAD))\n\t\treturn false;\n\n\tif (p->nr_cpus_allowed != 1)\n\t\treturn false;\n\n\treturn true;\n}\n#endif\n\nextern void swake_up_all_locked(struct swait_queue_head *q);\nextern void __prepare_to_swait(struct swait_queue_head *q, struct swait_queue *wait);\n\nextern int try_to_wake_up(struct task_struct *tsk, unsigned int state, int wake_flags);\n\n#ifdef CONFIG_PREEMPT_DYNAMIC\nextern int preempt_dynamic_mode;\nextern int sched_dynamic_mode(const char *str);\nextern void sched_dynamic_update(int mode);\n#endif\n\nstatic inline void update_current_exec_runtime(struct task_struct *curr,\n\t\t\t\t\t\tu64 now, u64 delta_exec)\n{\n\tcurr->se.sum_exec_runtime += delta_exec;\n\taccount_group_exec_runtime(curr, delta_exec);\n\n\tcurr->se.exec_start = now;\n\tcgroup_account_cputime(curr, delta_exec);\n}\n\n#ifdef CONFIG_SCHED_MM_CID\n\n#define SCHED_MM_CID_PERIOD_NS\t(100ULL * 1000000)\t \n#define MM_CID_SCAN_DELAY\t100\t\t\t \n\nextern raw_spinlock_t cid_lock;\nextern int use_cid_lock;\n\nextern void sched_mm_cid_migrate_from(struct task_struct *t);\nextern void sched_mm_cid_migrate_to(struct rq *dst_rq, struct task_struct *t);\nextern void task_tick_mm_cid(struct rq *rq, struct task_struct *curr);\nextern void init_sched_mm_cid(struct task_struct *t);\n\nstatic inline void __mm_cid_put(struct mm_struct *mm, int cid)\n{\n\tif (cid < 0)\n\t\treturn;\n\tcpumask_clear_cpu(cid, mm_cidmask(mm));\n}\n\n \nstatic inline void mm_cid_put_lazy(struct task_struct *t)\n{\n\tstruct mm_struct *mm = t->mm;\n\tstruct mm_cid __percpu *pcpu_cid = mm->pcpu_cid;\n\tint cid;\n\n\tlockdep_assert_irqs_disabled();\n\tcid = __this_cpu_read(pcpu_cid->cid);\n\tif (!mm_cid_is_lazy_put(cid) ||\n\t    !try_cmpxchg(&this_cpu_ptr(pcpu_cid)->cid, &cid, MM_CID_UNSET))\n\t\treturn;\n\t__mm_cid_put(mm, mm_cid_clear_lazy_put(cid));\n}\n\nstatic inline int mm_cid_pcpu_unset(struct mm_struct *mm)\n{\n\tstruct mm_cid __percpu *pcpu_cid = mm->pcpu_cid;\n\tint cid, res;\n\n\tlockdep_assert_irqs_disabled();\n\tcid = __this_cpu_read(pcpu_cid->cid);\n\tfor (;;) {\n\t\tif (mm_cid_is_unset(cid))\n\t\t\treturn MM_CID_UNSET;\n\t\t \n\t\tres = cmpxchg(&this_cpu_ptr(pcpu_cid)->cid, cid, MM_CID_UNSET);\n\t\tif (res == cid)\n\t\t\tbreak;\n\t\tcid = res;\n\t}\n\treturn cid;\n}\n\nstatic inline void mm_cid_put(struct mm_struct *mm)\n{\n\tint cid;\n\n\tlockdep_assert_irqs_disabled();\n\tcid = mm_cid_pcpu_unset(mm);\n\tif (cid == MM_CID_UNSET)\n\t\treturn;\n\t__mm_cid_put(mm, mm_cid_clear_lazy_put(cid));\n}\n\nstatic inline int __mm_cid_try_get(struct mm_struct *mm)\n{\n\tstruct cpumask *cpumask;\n\tint cid;\n\n\tcpumask = mm_cidmask(mm);\n\t \n\tfor (;;) {\n\t\tcid = cpumask_first_zero(cpumask);\n\t\tif (cid < nr_cpu_ids)\n\t\t\tbreak;\n\t\tcpu_relax();\n\t}\n\tif (cpumask_test_and_set_cpu(cid, cpumask))\n\t\treturn -1;\n\treturn cid;\n}\n\n \nstatic inline void mm_cid_snapshot_time(struct rq *rq, struct mm_struct *mm)\n{\n\tstruct mm_cid *pcpu_cid = per_cpu_ptr(mm->pcpu_cid, cpu_of(rq));\n\n\tlockdep_assert_rq_held(rq);\n\tWRITE_ONCE(pcpu_cid->time, rq->clock);\n}\n\nstatic inline int __mm_cid_get(struct rq *rq, struct mm_struct *mm)\n{\n\tint cid;\n\n\t \n\tif (!READ_ONCE(use_cid_lock)) {\n\t\tcid = __mm_cid_try_get(mm);\n\t\tif (cid >= 0)\n\t\t\tgoto end;\n\t\traw_spin_lock(&cid_lock);\n\t} else {\n\t\traw_spin_lock(&cid_lock);\n\t\tcid = __mm_cid_try_get(mm);\n\t\tif (cid >= 0)\n\t\t\tgoto unlock;\n\t}\n\n\t \n\tWRITE_ONCE(use_cid_lock, 1);\n\t \n\tbarrier();\n\t \n\tdo {\n\t\tcid = __mm_cid_try_get(mm);\n\t\tcpu_relax();\n\t} while (cid < 0);\n\t \n\tbarrier();\n\tWRITE_ONCE(use_cid_lock, 0);\nunlock:\n\traw_spin_unlock(&cid_lock);\nend:\n\tmm_cid_snapshot_time(rq, mm);\n\treturn cid;\n}\n\nstatic inline int mm_cid_get(struct rq *rq, struct mm_struct *mm)\n{\n\tstruct mm_cid __percpu *pcpu_cid = mm->pcpu_cid;\n\tstruct cpumask *cpumask;\n\tint cid;\n\n\tlockdep_assert_rq_held(rq);\n\tcpumask = mm_cidmask(mm);\n\tcid = __this_cpu_read(pcpu_cid->cid);\n\tif (mm_cid_is_valid(cid)) {\n\t\tmm_cid_snapshot_time(rq, mm);\n\t\treturn cid;\n\t}\n\tif (mm_cid_is_lazy_put(cid)) {\n\t\tif (try_cmpxchg(&this_cpu_ptr(pcpu_cid)->cid, &cid, MM_CID_UNSET))\n\t\t\t__mm_cid_put(mm, mm_cid_clear_lazy_put(cid));\n\t}\n\tcid = __mm_cid_get(rq, mm);\n\t__this_cpu_write(pcpu_cid->cid, cid);\n\treturn cid;\n}\n\nstatic inline void switch_mm_cid(struct rq *rq,\n\t\t\t\t struct task_struct *prev,\n\t\t\t\t struct task_struct *next)\n{\n\t \n\tif (!next->mm) {                                \n\t\t \n\t\tif (prev->mm)                           \n\t\t\tsmp_mb__after_mmgrab();\n\t\t \n\t} else {                                        \n\t\t \n\t\tif (!prev->mm)                          \n\t\t\tsmp_mb();\n\t\t \n\t}\n\tif (prev->mm_cid_active) {\n\t\tmm_cid_snapshot_time(rq, prev->mm);\n\t\tmm_cid_put_lazy(prev);\n\t\tprev->mm_cid = -1;\n\t}\n\tif (next->mm_cid_active)\n\t\tnext->last_mm_cid = next->mm_cid = mm_cid_get(rq, next->mm);\n}\n\n#else\nstatic inline void switch_mm_cid(struct rq *rq, struct task_struct *prev, struct task_struct *next) { }\nstatic inline void sched_mm_cid_migrate_from(struct task_struct *t) { }\nstatic inline void sched_mm_cid_migrate_to(struct rq *dst_rq, struct task_struct *t) { }\nstatic inline void task_tick_mm_cid(struct rq *rq, struct task_struct *curr) { }\nstatic inline void init_sched_mm_cid(struct task_struct *t) { }\n#endif\n\nextern u64 avg_vruntime(struct cfs_rq *cfs_rq);\nextern int entity_eligible(struct cfs_rq *cfs_rq, struct sched_entity *se);\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}