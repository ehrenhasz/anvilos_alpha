{
  "module_name": "cpufreq_schedutil.c",
  "hash_id": "e20248ea72cbbff5f68bda5d7cf80651fe0ab0b28fff12da04cf1a44d849efb7",
  "original_prompt": "Ingested from linux-6.6.14/kernel/sched/cpufreq_schedutil.c",
  "human_readable_source": "\n \n\n#define IOWAIT_BOOST_MIN\t(SCHED_CAPACITY_SCALE / 8)\n\nstruct sugov_tunables {\n\tstruct gov_attr_set\tattr_set;\n\tunsigned int\t\trate_limit_us;\n};\n\nstruct sugov_policy {\n\tstruct cpufreq_policy\t*policy;\n\n\tstruct sugov_tunables\t*tunables;\n\tstruct list_head\ttunables_hook;\n\n\traw_spinlock_t\t\tupdate_lock;\n\tu64\t\t\tlast_freq_update_time;\n\ts64\t\t\tfreq_update_delay_ns;\n\tunsigned int\t\tnext_freq;\n\tunsigned int\t\tcached_raw_freq;\n\n\t \n\tstruct\t\t\tirq_work irq_work;\n\tstruct\t\t\tkthread_work work;\n\tstruct\t\t\tmutex work_lock;\n\tstruct\t\t\tkthread_worker worker;\n\tstruct task_struct\t*thread;\n\tbool\t\t\twork_in_progress;\n\n\tbool\t\t\tlimits_changed;\n\tbool\t\t\tneed_freq_update;\n};\n\nstruct sugov_cpu {\n\tstruct update_util_data\tupdate_util;\n\tstruct sugov_policy\t*sg_policy;\n\tunsigned int\t\tcpu;\n\n\tbool\t\t\tiowait_boost_pending;\n\tunsigned int\t\tiowait_boost;\n\tu64\t\t\tlast_update;\n\n\tunsigned long\t\tutil;\n\tunsigned long\t\tbw_dl;\n\n\t \n#ifdef CONFIG_NO_HZ_COMMON\n\tunsigned long\t\tsaved_idle_calls;\n#endif\n};\n\nstatic DEFINE_PER_CPU(struct sugov_cpu, sugov_cpu);\n\n \n\nstatic bool sugov_should_update_freq(struct sugov_policy *sg_policy, u64 time)\n{\n\ts64 delta_ns;\n\n\t \n\tif (!cpufreq_this_cpu_can_update(sg_policy->policy))\n\t\treturn false;\n\n\tif (unlikely(sg_policy->limits_changed)) {\n\t\tsg_policy->limits_changed = false;\n\t\tsg_policy->need_freq_update = true;\n\t\treturn true;\n\t}\n\n\tdelta_ns = time - sg_policy->last_freq_update_time;\n\n\treturn delta_ns >= sg_policy->freq_update_delay_ns;\n}\n\nstatic bool sugov_update_next_freq(struct sugov_policy *sg_policy, u64 time,\n\t\t\t\t   unsigned int next_freq)\n{\n\tif (sg_policy->need_freq_update)\n\t\tsg_policy->need_freq_update = cpufreq_driver_test_flags(CPUFREQ_NEED_UPDATE_LIMITS);\n\telse if (sg_policy->next_freq == next_freq)\n\t\treturn false;\n\n\tsg_policy->next_freq = next_freq;\n\tsg_policy->last_freq_update_time = time;\n\n\treturn true;\n}\n\nstatic void sugov_deferred_update(struct sugov_policy *sg_policy)\n{\n\tif (!sg_policy->work_in_progress) {\n\t\tsg_policy->work_in_progress = true;\n\t\tirq_work_queue(&sg_policy->irq_work);\n\t}\n}\n\n \nstatic unsigned int get_next_freq(struct sugov_policy *sg_policy,\n\t\t\t\t  unsigned long util, unsigned long max)\n{\n\tstruct cpufreq_policy *policy = sg_policy->policy;\n\tunsigned int freq = arch_scale_freq_invariant() ?\n\t\t\t\tpolicy->cpuinfo.max_freq : policy->cur;\n\n\tutil = map_util_perf(util);\n\tfreq = map_util_freq(util, freq, max);\n\n\tif (freq == sg_policy->cached_raw_freq && !sg_policy->need_freq_update)\n\t\treturn sg_policy->next_freq;\n\n\tsg_policy->cached_raw_freq = freq;\n\treturn cpufreq_driver_resolve_freq(policy, freq);\n}\n\nstatic void sugov_get_util(struct sugov_cpu *sg_cpu)\n{\n\tunsigned long util = cpu_util_cfs_boost(sg_cpu->cpu);\n\tstruct rq *rq = cpu_rq(sg_cpu->cpu);\n\n\tsg_cpu->bw_dl = cpu_bw_dl(rq);\n\tsg_cpu->util = effective_cpu_util(sg_cpu->cpu, util,\n\t\t\t\t\t  FREQUENCY_UTIL, NULL);\n}\n\n \nstatic bool sugov_iowait_reset(struct sugov_cpu *sg_cpu, u64 time,\n\t\t\t       bool set_iowait_boost)\n{\n\ts64 delta_ns = time - sg_cpu->last_update;\n\n\t \n\tif (delta_ns <= TICK_NSEC)\n\t\treturn false;\n\n\tsg_cpu->iowait_boost = set_iowait_boost ? IOWAIT_BOOST_MIN : 0;\n\tsg_cpu->iowait_boost_pending = set_iowait_boost;\n\n\treturn true;\n}\n\n \nstatic void sugov_iowait_boost(struct sugov_cpu *sg_cpu, u64 time,\n\t\t\t       unsigned int flags)\n{\n\tbool set_iowait_boost = flags & SCHED_CPUFREQ_IOWAIT;\n\n\t \n\tif (sg_cpu->iowait_boost &&\n\t    sugov_iowait_reset(sg_cpu, time, set_iowait_boost))\n\t\treturn;\n\n\t \n\tif (!set_iowait_boost)\n\t\treturn;\n\n\t \n\tif (sg_cpu->iowait_boost_pending)\n\t\treturn;\n\tsg_cpu->iowait_boost_pending = true;\n\n\t \n\tif (sg_cpu->iowait_boost) {\n\t\tsg_cpu->iowait_boost =\n\t\t\tmin_t(unsigned int, sg_cpu->iowait_boost << 1, SCHED_CAPACITY_SCALE);\n\t\treturn;\n\t}\n\n\t \n\tsg_cpu->iowait_boost = IOWAIT_BOOST_MIN;\n}\n\n \nstatic void sugov_iowait_apply(struct sugov_cpu *sg_cpu, u64 time,\n\t\t\t       unsigned long max_cap)\n{\n\tunsigned long boost;\n\n\t \n\tif (!sg_cpu->iowait_boost)\n\t\treturn;\n\n\t \n\tif (sugov_iowait_reset(sg_cpu, time, false))\n\t\treturn;\n\n\tif (!sg_cpu->iowait_boost_pending) {\n\t\t \n\t\tsg_cpu->iowait_boost >>= 1;\n\t\tif (sg_cpu->iowait_boost < IOWAIT_BOOST_MIN) {\n\t\t\tsg_cpu->iowait_boost = 0;\n\t\t\treturn;\n\t\t}\n\t}\n\n\tsg_cpu->iowait_boost_pending = false;\n\n\t \n\tboost = (sg_cpu->iowait_boost * max_cap) >> SCHED_CAPACITY_SHIFT;\n\tboost = uclamp_rq_util_with(cpu_rq(sg_cpu->cpu), boost, NULL);\n\tif (sg_cpu->util < boost)\n\t\tsg_cpu->util = boost;\n}\n\n#ifdef CONFIG_NO_HZ_COMMON\nstatic bool sugov_cpu_is_busy(struct sugov_cpu *sg_cpu)\n{\n\tunsigned long idle_calls = tick_nohz_get_idle_calls_cpu(sg_cpu->cpu);\n\tbool ret = idle_calls == sg_cpu->saved_idle_calls;\n\n\tsg_cpu->saved_idle_calls = idle_calls;\n\treturn ret;\n}\n#else\nstatic inline bool sugov_cpu_is_busy(struct sugov_cpu *sg_cpu) { return false; }\n#endif  \n\n \nstatic inline void ignore_dl_rate_limit(struct sugov_cpu *sg_cpu)\n{\n\tif (cpu_bw_dl(cpu_rq(sg_cpu->cpu)) > sg_cpu->bw_dl)\n\t\tsg_cpu->sg_policy->limits_changed = true;\n}\n\nstatic inline bool sugov_update_single_common(struct sugov_cpu *sg_cpu,\n\t\t\t\t\t      u64 time, unsigned long max_cap,\n\t\t\t\t\t      unsigned int flags)\n{\n\tsugov_iowait_boost(sg_cpu, time, flags);\n\tsg_cpu->last_update = time;\n\n\tignore_dl_rate_limit(sg_cpu);\n\n\tif (!sugov_should_update_freq(sg_cpu->sg_policy, time))\n\t\treturn false;\n\n\tsugov_get_util(sg_cpu);\n\tsugov_iowait_apply(sg_cpu, time, max_cap);\n\n\treturn true;\n}\n\nstatic void sugov_update_single_freq(struct update_util_data *hook, u64 time,\n\t\t\t\t     unsigned int flags)\n{\n\tstruct sugov_cpu *sg_cpu = container_of(hook, struct sugov_cpu, update_util);\n\tstruct sugov_policy *sg_policy = sg_cpu->sg_policy;\n\tunsigned int cached_freq = sg_policy->cached_raw_freq;\n\tunsigned long max_cap;\n\tunsigned int next_f;\n\n\tmax_cap = arch_scale_cpu_capacity(sg_cpu->cpu);\n\n\tif (!sugov_update_single_common(sg_cpu, time, max_cap, flags))\n\t\treturn;\n\n\tnext_f = get_next_freq(sg_policy, sg_cpu->util, max_cap);\n\t \n\tif (!uclamp_rq_is_capped(cpu_rq(sg_cpu->cpu)) &&\n\t    sugov_cpu_is_busy(sg_cpu) && next_f < sg_policy->next_freq &&\n\t    !sg_policy->need_freq_update) {\n\t\tnext_f = sg_policy->next_freq;\n\n\t\t \n\t\tsg_policy->cached_raw_freq = cached_freq;\n\t}\n\n\tif (!sugov_update_next_freq(sg_policy, time, next_f))\n\t\treturn;\n\n\t \n\tif (sg_policy->policy->fast_switch_enabled) {\n\t\tcpufreq_driver_fast_switch(sg_policy->policy, next_f);\n\t} else {\n\t\traw_spin_lock(&sg_policy->update_lock);\n\t\tsugov_deferred_update(sg_policy);\n\t\traw_spin_unlock(&sg_policy->update_lock);\n\t}\n}\n\nstatic void sugov_update_single_perf(struct update_util_data *hook, u64 time,\n\t\t\t\t     unsigned int flags)\n{\n\tstruct sugov_cpu *sg_cpu = container_of(hook, struct sugov_cpu, update_util);\n\tunsigned long prev_util = sg_cpu->util;\n\tunsigned long max_cap;\n\n\t \n\tif (!arch_scale_freq_invariant()) {\n\t\tsugov_update_single_freq(hook, time, flags);\n\t\treturn;\n\t}\n\n\tmax_cap = arch_scale_cpu_capacity(sg_cpu->cpu);\n\n\tif (!sugov_update_single_common(sg_cpu, time, max_cap, flags))\n\t\treturn;\n\n\t \n\tif (!uclamp_rq_is_capped(cpu_rq(sg_cpu->cpu)) &&\n\t    sugov_cpu_is_busy(sg_cpu) && sg_cpu->util < prev_util)\n\t\tsg_cpu->util = prev_util;\n\n\tcpufreq_driver_adjust_perf(sg_cpu->cpu, map_util_perf(sg_cpu->bw_dl),\n\t\t\t\t   map_util_perf(sg_cpu->util), max_cap);\n\n\tsg_cpu->sg_policy->last_freq_update_time = time;\n}\n\nstatic unsigned int sugov_next_freq_shared(struct sugov_cpu *sg_cpu, u64 time)\n{\n\tstruct sugov_policy *sg_policy = sg_cpu->sg_policy;\n\tstruct cpufreq_policy *policy = sg_policy->policy;\n\tunsigned long util = 0, max_cap;\n\tunsigned int j;\n\n\tmax_cap = arch_scale_cpu_capacity(sg_cpu->cpu);\n\n\tfor_each_cpu(j, policy->cpus) {\n\t\tstruct sugov_cpu *j_sg_cpu = &per_cpu(sugov_cpu, j);\n\n\t\tsugov_get_util(j_sg_cpu);\n\t\tsugov_iowait_apply(j_sg_cpu, time, max_cap);\n\n\t\tutil = max(j_sg_cpu->util, util);\n\t}\n\n\treturn get_next_freq(sg_policy, util, max_cap);\n}\n\nstatic void\nsugov_update_shared(struct update_util_data *hook, u64 time, unsigned int flags)\n{\n\tstruct sugov_cpu *sg_cpu = container_of(hook, struct sugov_cpu, update_util);\n\tstruct sugov_policy *sg_policy = sg_cpu->sg_policy;\n\tunsigned int next_f;\n\n\traw_spin_lock(&sg_policy->update_lock);\n\n\tsugov_iowait_boost(sg_cpu, time, flags);\n\tsg_cpu->last_update = time;\n\n\tignore_dl_rate_limit(sg_cpu);\n\n\tif (sugov_should_update_freq(sg_policy, time)) {\n\t\tnext_f = sugov_next_freq_shared(sg_cpu, time);\n\n\t\tif (!sugov_update_next_freq(sg_policy, time, next_f))\n\t\t\tgoto unlock;\n\n\t\tif (sg_policy->policy->fast_switch_enabled)\n\t\t\tcpufreq_driver_fast_switch(sg_policy->policy, next_f);\n\t\telse\n\t\t\tsugov_deferred_update(sg_policy);\n\t}\nunlock:\n\traw_spin_unlock(&sg_policy->update_lock);\n}\n\nstatic void sugov_work(struct kthread_work *work)\n{\n\tstruct sugov_policy *sg_policy = container_of(work, struct sugov_policy, work);\n\tunsigned int freq;\n\tunsigned long flags;\n\n\t \n\traw_spin_lock_irqsave(&sg_policy->update_lock, flags);\n\tfreq = sg_policy->next_freq;\n\tsg_policy->work_in_progress = false;\n\traw_spin_unlock_irqrestore(&sg_policy->update_lock, flags);\n\n\tmutex_lock(&sg_policy->work_lock);\n\t__cpufreq_driver_target(sg_policy->policy, freq, CPUFREQ_RELATION_L);\n\tmutex_unlock(&sg_policy->work_lock);\n}\n\nstatic void sugov_irq_work(struct irq_work *irq_work)\n{\n\tstruct sugov_policy *sg_policy;\n\n\tsg_policy = container_of(irq_work, struct sugov_policy, irq_work);\n\n\tkthread_queue_work(&sg_policy->worker, &sg_policy->work);\n}\n\n \n\nstatic struct sugov_tunables *global_tunables;\nstatic DEFINE_MUTEX(global_tunables_lock);\n\nstatic inline struct sugov_tunables *to_sugov_tunables(struct gov_attr_set *attr_set)\n{\n\treturn container_of(attr_set, struct sugov_tunables, attr_set);\n}\n\nstatic ssize_t rate_limit_us_show(struct gov_attr_set *attr_set, char *buf)\n{\n\tstruct sugov_tunables *tunables = to_sugov_tunables(attr_set);\n\n\treturn sprintf(buf, \"%u\\n\", tunables->rate_limit_us);\n}\n\nstatic ssize_t\nrate_limit_us_store(struct gov_attr_set *attr_set, const char *buf, size_t count)\n{\n\tstruct sugov_tunables *tunables = to_sugov_tunables(attr_set);\n\tstruct sugov_policy *sg_policy;\n\tunsigned int rate_limit_us;\n\n\tif (kstrtouint(buf, 10, &rate_limit_us))\n\t\treturn -EINVAL;\n\n\ttunables->rate_limit_us = rate_limit_us;\n\n\tlist_for_each_entry(sg_policy, &attr_set->policy_list, tunables_hook)\n\t\tsg_policy->freq_update_delay_ns = rate_limit_us * NSEC_PER_USEC;\n\n\treturn count;\n}\n\nstatic struct governor_attr rate_limit_us = __ATTR_RW(rate_limit_us);\n\nstatic struct attribute *sugov_attrs[] = {\n\t&rate_limit_us.attr,\n\tNULL\n};\nATTRIBUTE_GROUPS(sugov);\n\nstatic void sugov_tunables_free(struct kobject *kobj)\n{\n\tstruct gov_attr_set *attr_set = to_gov_attr_set(kobj);\n\n\tkfree(to_sugov_tunables(attr_set));\n}\n\nstatic const struct kobj_type sugov_tunables_ktype = {\n\t.default_groups = sugov_groups,\n\t.sysfs_ops = &governor_sysfs_ops,\n\t.release = &sugov_tunables_free,\n};\n\n \n\nstruct cpufreq_governor schedutil_gov;\n\nstatic struct sugov_policy *sugov_policy_alloc(struct cpufreq_policy *policy)\n{\n\tstruct sugov_policy *sg_policy;\n\n\tsg_policy = kzalloc(sizeof(*sg_policy), GFP_KERNEL);\n\tif (!sg_policy)\n\t\treturn NULL;\n\n\tsg_policy->policy = policy;\n\traw_spin_lock_init(&sg_policy->update_lock);\n\treturn sg_policy;\n}\n\nstatic void sugov_policy_free(struct sugov_policy *sg_policy)\n{\n\tkfree(sg_policy);\n}\n\nstatic int sugov_kthread_create(struct sugov_policy *sg_policy)\n{\n\tstruct task_struct *thread;\n\tstruct sched_attr attr = {\n\t\t.size\t\t= sizeof(struct sched_attr),\n\t\t.sched_policy\t= SCHED_DEADLINE,\n\t\t.sched_flags\t= SCHED_FLAG_SUGOV,\n\t\t.sched_nice\t= 0,\n\t\t.sched_priority\t= 0,\n\t\t \n\t\t.sched_runtime\t=  1000000,\n\t\t.sched_deadline = 10000000,\n\t\t.sched_period\t= 10000000,\n\t};\n\tstruct cpufreq_policy *policy = sg_policy->policy;\n\tint ret;\n\n\t \n\tif (policy->fast_switch_enabled)\n\t\treturn 0;\n\n\tkthread_init_work(&sg_policy->work, sugov_work);\n\tkthread_init_worker(&sg_policy->worker);\n\tthread = kthread_create(kthread_worker_fn, &sg_policy->worker,\n\t\t\t\t\"sugov:%d\",\n\t\t\t\tcpumask_first(policy->related_cpus));\n\tif (IS_ERR(thread)) {\n\t\tpr_err(\"failed to create sugov thread: %ld\\n\", PTR_ERR(thread));\n\t\treturn PTR_ERR(thread);\n\t}\n\n\tret = sched_setattr_nocheck(thread, &attr);\n\tif (ret) {\n\t\tkthread_stop(thread);\n\t\tpr_warn(\"%s: failed to set SCHED_DEADLINE\\n\", __func__);\n\t\treturn ret;\n\t}\n\n\tsg_policy->thread = thread;\n\tkthread_bind_mask(thread, policy->related_cpus);\n\tinit_irq_work(&sg_policy->irq_work, sugov_irq_work);\n\tmutex_init(&sg_policy->work_lock);\n\n\twake_up_process(thread);\n\n\treturn 0;\n}\n\nstatic void sugov_kthread_stop(struct sugov_policy *sg_policy)\n{\n\t \n\tif (sg_policy->policy->fast_switch_enabled)\n\t\treturn;\n\n\tkthread_flush_worker(&sg_policy->worker);\n\tkthread_stop(sg_policy->thread);\n\tmutex_destroy(&sg_policy->work_lock);\n}\n\nstatic struct sugov_tunables *sugov_tunables_alloc(struct sugov_policy *sg_policy)\n{\n\tstruct sugov_tunables *tunables;\n\n\ttunables = kzalloc(sizeof(*tunables), GFP_KERNEL);\n\tif (tunables) {\n\t\tgov_attr_set_init(&tunables->attr_set, &sg_policy->tunables_hook);\n\t\tif (!have_governor_per_policy())\n\t\t\tglobal_tunables = tunables;\n\t}\n\treturn tunables;\n}\n\nstatic void sugov_clear_global_tunables(void)\n{\n\tif (!have_governor_per_policy())\n\t\tglobal_tunables = NULL;\n}\n\nstatic int sugov_init(struct cpufreq_policy *policy)\n{\n\tstruct sugov_policy *sg_policy;\n\tstruct sugov_tunables *tunables;\n\tint ret = 0;\n\n\t \n\tif (policy->governor_data)\n\t\treturn -EBUSY;\n\n\tcpufreq_enable_fast_switch(policy);\n\n\tsg_policy = sugov_policy_alloc(policy);\n\tif (!sg_policy) {\n\t\tret = -ENOMEM;\n\t\tgoto disable_fast_switch;\n\t}\n\n\tret = sugov_kthread_create(sg_policy);\n\tif (ret)\n\t\tgoto free_sg_policy;\n\n\tmutex_lock(&global_tunables_lock);\n\n\tif (global_tunables) {\n\t\tif (WARN_ON(have_governor_per_policy())) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto stop_kthread;\n\t\t}\n\t\tpolicy->governor_data = sg_policy;\n\t\tsg_policy->tunables = global_tunables;\n\n\t\tgov_attr_set_get(&global_tunables->attr_set, &sg_policy->tunables_hook);\n\t\tgoto out;\n\t}\n\n\ttunables = sugov_tunables_alloc(sg_policy);\n\tif (!tunables) {\n\t\tret = -ENOMEM;\n\t\tgoto stop_kthread;\n\t}\n\n\ttunables->rate_limit_us = cpufreq_policy_transition_delay_us(policy);\n\n\tpolicy->governor_data = sg_policy;\n\tsg_policy->tunables = tunables;\n\n\tret = kobject_init_and_add(&tunables->attr_set.kobj, &sugov_tunables_ktype,\n\t\t\t\t   get_governor_parent_kobj(policy), \"%s\",\n\t\t\t\t   schedutil_gov.name);\n\tif (ret)\n\t\tgoto fail;\n\nout:\n\tmutex_unlock(&global_tunables_lock);\n\treturn 0;\n\nfail:\n\tkobject_put(&tunables->attr_set.kobj);\n\tpolicy->governor_data = NULL;\n\tsugov_clear_global_tunables();\n\nstop_kthread:\n\tsugov_kthread_stop(sg_policy);\n\tmutex_unlock(&global_tunables_lock);\n\nfree_sg_policy:\n\tsugov_policy_free(sg_policy);\n\ndisable_fast_switch:\n\tcpufreq_disable_fast_switch(policy);\n\n\tpr_err(\"initialization failed (error %d)\\n\", ret);\n\treturn ret;\n}\n\nstatic void sugov_exit(struct cpufreq_policy *policy)\n{\n\tstruct sugov_policy *sg_policy = policy->governor_data;\n\tstruct sugov_tunables *tunables = sg_policy->tunables;\n\tunsigned int count;\n\n\tmutex_lock(&global_tunables_lock);\n\n\tcount = gov_attr_set_put(&tunables->attr_set, &sg_policy->tunables_hook);\n\tpolicy->governor_data = NULL;\n\tif (!count)\n\t\tsugov_clear_global_tunables();\n\n\tmutex_unlock(&global_tunables_lock);\n\n\tsugov_kthread_stop(sg_policy);\n\tsugov_policy_free(sg_policy);\n\tcpufreq_disable_fast_switch(policy);\n}\n\nstatic int sugov_start(struct cpufreq_policy *policy)\n{\n\tstruct sugov_policy *sg_policy = policy->governor_data;\n\tvoid (*uu)(struct update_util_data *data, u64 time, unsigned int flags);\n\tunsigned int cpu;\n\n\tsg_policy->freq_update_delay_ns\t= sg_policy->tunables->rate_limit_us * NSEC_PER_USEC;\n\tsg_policy->last_freq_update_time\t= 0;\n\tsg_policy->next_freq\t\t\t= 0;\n\tsg_policy->work_in_progress\t\t= false;\n\tsg_policy->limits_changed\t\t= false;\n\tsg_policy->cached_raw_freq\t\t= 0;\n\n\tsg_policy->need_freq_update = cpufreq_driver_test_flags(CPUFREQ_NEED_UPDATE_LIMITS);\n\n\tfor_each_cpu(cpu, policy->cpus) {\n\t\tstruct sugov_cpu *sg_cpu = &per_cpu(sugov_cpu, cpu);\n\n\t\tmemset(sg_cpu, 0, sizeof(*sg_cpu));\n\t\tsg_cpu->cpu\t\t\t= cpu;\n\t\tsg_cpu->sg_policy\t\t= sg_policy;\n\t}\n\n\tif (policy_is_shared(policy))\n\t\tuu = sugov_update_shared;\n\telse if (policy->fast_switch_enabled && cpufreq_driver_has_adjust_perf())\n\t\tuu = sugov_update_single_perf;\n\telse\n\t\tuu = sugov_update_single_freq;\n\n\tfor_each_cpu(cpu, policy->cpus) {\n\t\tstruct sugov_cpu *sg_cpu = &per_cpu(sugov_cpu, cpu);\n\n\t\tcpufreq_add_update_util_hook(cpu, &sg_cpu->update_util, uu);\n\t}\n\treturn 0;\n}\n\nstatic void sugov_stop(struct cpufreq_policy *policy)\n{\n\tstruct sugov_policy *sg_policy = policy->governor_data;\n\tunsigned int cpu;\n\n\tfor_each_cpu(cpu, policy->cpus)\n\t\tcpufreq_remove_update_util_hook(cpu);\n\n\tsynchronize_rcu();\n\n\tif (!policy->fast_switch_enabled) {\n\t\tirq_work_sync(&sg_policy->irq_work);\n\t\tkthread_cancel_work_sync(&sg_policy->work);\n\t}\n}\n\nstatic void sugov_limits(struct cpufreq_policy *policy)\n{\n\tstruct sugov_policy *sg_policy = policy->governor_data;\n\n\tif (!policy->fast_switch_enabled) {\n\t\tmutex_lock(&sg_policy->work_lock);\n\t\tcpufreq_policy_apply_limits(policy);\n\t\tmutex_unlock(&sg_policy->work_lock);\n\t}\n\n\tsg_policy->limits_changed = true;\n}\n\nstruct cpufreq_governor schedutil_gov = {\n\t.name\t\t\t= \"schedutil\",\n\t.owner\t\t\t= THIS_MODULE,\n\t.flags\t\t\t= CPUFREQ_GOV_DYNAMIC_SWITCHING,\n\t.init\t\t\t= sugov_init,\n\t.exit\t\t\t= sugov_exit,\n\t.start\t\t\t= sugov_start,\n\t.stop\t\t\t= sugov_stop,\n\t.limits\t\t\t= sugov_limits,\n};\n\n#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_SCHEDUTIL\nstruct cpufreq_governor *cpufreq_default_governor(void)\n{\n\treturn &schedutil_gov;\n}\n#endif\n\ncpufreq_governor_init(schedutil_gov);\n\n#ifdef CONFIG_ENERGY_MODEL\nstatic void rebuild_sd_workfn(struct work_struct *work)\n{\n\trebuild_sched_domains_energy();\n}\nstatic DECLARE_WORK(rebuild_sd_work, rebuild_sd_workfn);\n\n \nvoid sched_cpufreq_governor_change(struct cpufreq_policy *policy,\n\t\t\t\t  struct cpufreq_governor *old_gov)\n{\n\tif (old_gov == &schedutil_gov || policy->governor == &schedutil_gov) {\n\t\t \n\t\tschedule_work(&rebuild_sd_work);\n\t}\n\n}\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}