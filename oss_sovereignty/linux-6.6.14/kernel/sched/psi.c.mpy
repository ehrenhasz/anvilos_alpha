{
  "module_name": "psi.c",
  "hash_id": "bee81db9fff955c01c54c5c943bbd946067b7c56abbb1408a6bc8712d67c3961",
  "original_prompt": "Ingested from linux-6.6.14/kernel/sched/psi.c",
  "human_readable_source": "\n \n\nstatic int psi_bug __read_mostly;\n\nDEFINE_STATIC_KEY_FALSE(psi_disabled);\nstatic DEFINE_STATIC_KEY_TRUE(psi_cgroups_enabled);\n\n#ifdef CONFIG_PSI_DEFAULT_DISABLED\nstatic bool psi_enable;\n#else\nstatic bool psi_enable = true;\n#endif\nstatic int __init setup_psi(char *str)\n{\n\treturn kstrtobool(str, &psi_enable) == 0;\n}\n__setup(\"psi=\", setup_psi);\n\n \n#define PSI_FREQ\t(2*HZ+1)\t \n#define EXP_10s\t\t1677\t\t \n#define EXP_60s\t\t1981\t\t \n#define EXP_300s\t2034\t\t \n\n \n#define WINDOW_MAX_US 10000000\t \n#define UPDATES_PER_WINDOW 10\t \n\n \nstatic u64 psi_period __read_mostly;\n\n \nstatic DEFINE_PER_CPU(struct psi_group_cpu, system_group_pcpu);\nstruct psi_group psi_system = {\n\t.pcpu = &system_group_pcpu,\n};\n\nstatic void psi_avgs_work(struct work_struct *work);\n\nstatic void poll_timer_fn(struct timer_list *t);\n\nstatic void group_init(struct psi_group *group)\n{\n\tint cpu;\n\n\tgroup->enabled = true;\n\tfor_each_possible_cpu(cpu)\n\t\tseqcount_init(&per_cpu_ptr(group->pcpu, cpu)->seq);\n\tgroup->avg_last_update = sched_clock();\n\tgroup->avg_next_update = group->avg_last_update + psi_period;\n\tmutex_init(&group->avgs_lock);\n\n\t \n\tINIT_LIST_HEAD(&group->avg_triggers);\n\tmemset(group->avg_nr_triggers, 0, sizeof(group->avg_nr_triggers));\n\tINIT_DELAYED_WORK(&group->avgs_work, psi_avgs_work);\n\n\t \n\tatomic_set(&group->rtpoll_scheduled, 0);\n\tmutex_init(&group->rtpoll_trigger_lock);\n\tINIT_LIST_HEAD(&group->rtpoll_triggers);\n\tgroup->rtpoll_min_period = U32_MAX;\n\tgroup->rtpoll_next_update = ULLONG_MAX;\n\tinit_waitqueue_head(&group->rtpoll_wait);\n\ttimer_setup(&group->rtpoll_timer, poll_timer_fn, 0);\n\trcu_assign_pointer(group->rtpoll_task, NULL);\n}\n\nvoid __init psi_init(void)\n{\n\tif (!psi_enable) {\n\t\tstatic_branch_enable(&psi_disabled);\n\t\tstatic_branch_disable(&psi_cgroups_enabled);\n\t\treturn;\n\t}\n\n\tif (!cgroup_psi_enabled())\n\t\tstatic_branch_disable(&psi_cgroups_enabled);\n\n\tpsi_period = jiffies_to_nsecs(PSI_FREQ);\n\tgroup_init(&psi_system);\n}\n\nstatic bool test_state(unsigned int *tasks, enum psi_states state, bool oncpu)\n{\n\tswitch (state) {\n\tcase PSI_IO_SOME:\n\t\treturn unlikely(tasks[NR_IOWAIT]);\n\tcase PSI_IO_FULL:\n\t\treturn unlikely(tasks[NR_IOWAIT] && !tasks[NR_RUNNING]);\n\tcase PSI_MEM_SOME:\n\t\treturn unlikely(tasks[NR_MEMSTALL]);\n\tcase PSI_MEM_FULL:\n\t\treturn unlikely(tasks[NR_MEMSTALL] &&\n\t\t\ttasks[NR_RUNNING] == tasks[NR_MEMSTALL_RUNNING]);\n\tcase PSI_CPU_SOME:\n\t\treturn unlikely(tasks[NR_RUNNING] > oncpu);\n\tcase PSI_CPU_FULL:\n\t\treturn unlikely(tasks[NR_RUNNING] && !oncpu);\n\tcase PSI_NONIDLE:\n\t\treturn tasks[NR_IOWAIT] || tasks[NR_MEMSTALL] ||\n\t\t\ttasks[NR_RUNNING];\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic void get_recent_times(struct psi_group *group, int cpu,\n\t\t\t     enum psi_aggregators aggregator, u32 *times,\n\t\t\t     u32 *pchanged_states)\n{\n\tstruct psi_group_cpu *groupc = per_cpu_ptr(group->pcpu, cpu);\n\tint current_cpu = raw_smp_processor_id();\n\tunsigned int tasks[NR_PSI_TASK_COUNTS];\n\tu64 now, state_start;\n\tenum psi_states s;\n\tunsigned int seq;\n\tu32 state_mask;\n\n\t*pchanged_states = 0;\n\n\t \n\tdo {\n\t\tseq = read_seqcount_begin(&groupc->seq);\n\t\tnow = cpu_clock(cpu);\n\t\tmemcpy(times, groupc->times, sizeof(groupc->times));\n\t\tstate_mask = groupc->state_mask;\n\t\tstate_start = groupc->state_start;\n\t\tif (cpu == current_cpu)\n\t\t\tmemcpy(tasks, groupc->tasks, sizeof(groupc->tasks));\n\t} while (read_seqcount_retry(&groupc->seq, seq));\n\n\t \n\tfor (s = 0; s < NR_PSI_STATES; s++) {\n\t\tu32 delta;\n\t\t \n\t\tif (state_mask & (1 << s))\n\t\t\ttimes[s] += now - state_start;\n\n\t\tdelta = times[s] - groupc->times_prev[aggregator][s];\n\t\tgroupc->times_prev[aggregator][s] = times[s];\n\n\t\ttimes[s] = delta;\n\t\tif (delta)\n\t\t\t*pchanged_states |= (1 << s);\n\t}\n\n\t \n\tif (current_work() == &group->avgs_work.work) {\n\t\tbool reschedule;\n\n\t\tif (cpu == current_cpu)\n\t\t\treschedule = tasks[NR_RUNNING] +\n\t\t\t\t     tasks[NR_IOWAIT] +\n\t\t\t\t     tasks[NR_MEMSTALL] > 1;\n\t\telse\n\t\t\treschedule = *pchanged_states & (1 << PSI_NONIDLE);\n\n\t\tif (reschedule)\n\t\t\t*pchanged_states |= PSI_STATE_RESCHEDULE;\n\t}\n}\n\nstatic void calc_avgs(unsigned long avg[3], int missed_periods,\n\t\t      u64 time, u64 period)\n{\n\tunsigned long pct;\n\n\t \n\tif (missed_periods) {\n\t\tavg[0] = calc_load_n(avg[0], EXP_10s, 0, missed_periods);\n\t\tavg[1] = calc_load_n(avg[1], EXP_60s, 0, missed_periods);\n\t\tavg[2] = calc_load_n(avg[2], EXP_300s, 0, missed_periods);\n\t}\n\n\t \n\tpct = div_u64(time * 100, period);\n\tpct *= FIXED_1;\n\tavg[0] = calc_load(avg[0], EXP_10s, pct);\n\tavg[1] = calc_load(avg[1], EXP_60s, pct);\n\tavg[2] = calc_load(avg[2], EXP_300s, pct);\n}\n\nstatic void collect_percpu_times(struct psi_group *group,\n\t\t\t\t enum psi_aggregators aggregator,\n\t\t\t\t u32 *pchanged_states)\n{\n\tu64 deltas[NR_PSI_STATES - 1] = { 0, };\n\tunsigned long nonidle_total = 0;\n\tu32 changed_states = 0;\n\tint cpu;\n\tint s;\n\n\t \n\tfor_each_possible_cpu(cpu) {\n\t\tu32 times[NR_PSI_STATES];\n\t\tu32 nonidle;\n\t\tu32 cpu_changed_states;\n\n\t\tget_recent_times(group, cpu, aggregator, times,\n\t\t\t\t&cpu_changed_states);\n\t\tchanged_states |= cpu_changed_states;\n\n\t\tnonidle = nsecs_to_jiffies(times[PSI_NONIDLE]);\n\t\tnonidle_total += nonidle;\n\n\t\tfor (s = 0; s < PSI_NONIDLE; s++)\n\t\t\tdeltas[s] += (u64)times[s] * nonidle;\n\t}\n\n\t \n\n\t \n\tfor (s = 0; s < NR_PSI_STATES - 1; s++)\n\t\tgroup->total[aggregator][s] +=\n\t\t\t\tdiv_u64(deltas[s], max(nonidle_total, 1UL));\n\n\tif (pchanged_states)\n\t\t*pchanged_states = changed_states;\n}\n\n \nstatic void window_reset(struct psi_window *win, u64 now, u64 value,\n\t\t\t u64 prev_growth)\n{\n\twin->start_time = now;\n\twin->start_value = value;\n\twin->prev_growth = prev_growth;\n}\n\n \nstatic u64 window_update(struct psi_window *win, u64 now, u64 value)\n{\n\tu64 elapsed;\n\tu64 growth;\n\n\telapsed = now - win->start_time;\n\tgrowth = value - win->start_value;\n\t \n\tif (elapsed > win->size)\n\t\twindow_reset(win, now, value, growth);\n\telse {\n\t\tu32 remaining;\n\n\t\tremaining = win->size - elapsed;\n\t\tgrowth += div64_u64(win->prev_growth * remaining, win->size);\n\t}\n\n\treturn growth;\n}\n\nstatic u64 update_triggers(struct psi_group *group, u64 now, bool *update_total,\n\t\t\t\t\t\t   enum psi_aggregators aggregator)\n{\n\tstruct psi_trigger *t;\n\tu64 *total = group->total[aggregator];\n\tstruct list_head *triggers;\n\tu64 *aggregator_total;\n\t*update_total = false;\n\n\tif (aggregator == PSI_AVGS) {\n\t\ttriggers = &group->avg_triggers;\n\t\taggregator_total = group->avg_total;\n\t} else {\n\t\ttriggers = &group->rtpoll_triggers;\n\t\taggregator_total = group->rtpoll_total;\n\t}\n\n\t \n\tlist_for_each_entry(t, triggers, node) {\n\t\tu64 growth;\n\t\tbool new_stall;\n\n\t\tnew_stall = aggregator_total[t->state] != total[t->state];\n\n\t\t \n\t\tif (!new_stall && !t->pending_event)\n\t\t\tcontinue;\n\t\t \n\t\tif (new_stall) {\n\t\t\t \n\t\t\t*update_total = true;\n\n\t\t\t \n\t\t\tgrowth = window_update(&t->win, now, total[t->state]);\n\t\t\tif (!t->pending_event) {\n\t\t\t\tif (growth < t->threshold)\n\t\t\t\t\tcontinue;\n\n\t\t\t\tt->pending_event = true;\n\t\t\t}\n\t\t}\n\t\t \n\t\tif (now < t->last_event_time + t->win.size)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (cmpxchg(&t->event, 0, 1) == 0) {\n\t\t\tif (t->of)\n\t\t\t\tkernfs_notify(t->of->kn);\n\t\t\telse\n\t\t\t\twake_up_interruptible(&t->event_wait);\n\t\t}\n\t\tt->last_event_time = now;\n\t\t \n\t\tt->pending_event = false;\n\t}\n\n\treturn now + group->rtpoll_min_period;\n}\n\nstatic u64 update_averages(struct psi_group *group, u64 now)\n{\n\tunsigned long missed_periods = 0;\n\tu64 expires, period;\n\tu64 avg_next_update;\n\tint s;\n\n\t \n\texpires = group->avg_next_update;\n\tif (now - expires >= psi_period)\n\t\tmissed_periods = div_u64(now - expires, psi_period);\n\n\t \n\tavg_next_update = expires + ((1 + missed_periods) * psi_period);\n\tperiod = now - (group->avg_last_update + (missed_periods * psi_period));\n\tgroup->avg_last_update = now;\n\n\tfor (s = 0; s < NR_PSI_STATES - 1; s++) {\n\t\tu32 sample;\n\n\t\tsample = group->total[PSI_AVGS][s] - group->avg_total[s];\n\t\t \n\t\tif (sample > period)\n\t\t\tsample = period;\n\t\tgroup->avg_total[s] += sample;\n\t\tcalc_avgs(group->avg[s], missed_periods, sample, period);\n\t}\n\n\treturn avg_next_update;\n}\n\nstatic void psi_avgs_work(struct work_struct *work)\n{\n\tstruct delayed_work *dwork;\n\tstruct psi_group *group;\n\tu32 changed_states;\n\tbool update_total;\n\tu64 now;\n\n\tdwork = to_delayed_work(work);\n\tgroup = container_of(dwork, struct psi_group, avgs_work);\n\n\tmutex_lock(&group->avgs_lock);\n\n\tnow = sched_clock();\n\n\tcollect_percpu_times(group, PSI_AVGS, &changed_states);\n\t \n\tif (now >= group->avg_next_update) {\n\t\tupdate_triggers(group, now, &update_total, PSI_AVGS);\n\t\tgroup->avg_next_update = update_averages(group, now);\n\t}\n\n\tif (changed_states & PSI_STATE_RESCHEDULE) {\n\t\tschedule_delayed_work(dwork, nsecs_to_jiffies(\n\t\t\t\tgroup->avg_next_update - now) + 1);\n\t}\n\n\tmutex_unlock(&group->avgs_lock);\n}\n\nstatic void init_rtpoll_triggers(struct psi_group *group, u64 now)\n{\n\tstruct psi_trigger *t;\n\n\tlist_for_each_entry(t, &group->rtpoll_triggers, node)\n\t\twindow_reset(&t->win, now,\n\t\t\t\tgroup->total[PSI_POLL][t->state], 0);\n\tmemcpy(group->rtpoll_total, group->total[PSI_POLL],\n\t\t   sizeof(group->rtpoll_total));\n\tgroup->rtpoll_next_update = now + group->rtpoll_min_period;\n}\n\n \nstatic void psi_schedule_rtpoll_work(struct psi_group *group, unsigned long delay,\n\t\t\t\t   bool force)\n{\n\tstruct task_struct *task;\n\n\t \n\tif (atomic_xchg(&group->rtpoll_scheduled, 1) && !force)\n\t\treturn;\n\n\trcu_read_lock();\n\n\ttask = rcu_dereference(group->rtpoll_task);\n\t \n\tif (likely(task))\n\t\tmod_timer(&group->rtpoll_timer, jiffies + delay);\n\telse\n\t\tatomic_set(&group->rtpoll_scheduled, 0);\n\n\trcu_read_unlock();\n}\n\nstatic void psi_rtpoll_work(struct psi_group *group)\n{\n\tbool force_reschedule = false;\n\tu32 changed_states;\n\tbool update_total;\n\tu64 now;\n\n\tmutex_lock(&group->rtpoll_trigger_lock);\n\n\tnow = sched_clock();\n\n\tif (now > group->rtpoll_until) {\n\t\t \n\t\tatomic_set(&group->rtpoll_scheduled, 0);\n\t\t \n\t\tsmp_mb();\n\t} else {\n\t\t \n\t\tforce_reschedule = true;\n\t}\n\n\n\tcollect_percpu_times(group, PSI_POLL, &changed_states);\n\n\tif (changed_states & group->rtpoll_states) {\n\t\t \n\t\tif (now > group->rtpoll_until)\n\t\t\tinit_rtpoll_triggers(group, now);\n\n\t\t \n\t\tgroup->rtpoll_until = now +\n\t\t\tgroup->rtpoll_min_period * UPDATES_PER_WINDOW;\n\t}\n\n\tif (now > group->rtpoll_until) {\n\t\tgroup->rtpoll_next_update = ULLONG_MAX;\n\t\tgoto out;\n\t}\n\n\tif (now >= group->rtpoll_next_update) {\n\t\tgroup->rtpoll_next_update = update_triggers(group, now, &update_total, PSI_POLL);\n\t\tif (update_total)\n\t\t\tmemcpy(group->rtpoll_total, group->total[PSI_POLL],\n\t\t\t\t   sizeof(group->rtpoll_total));\n\t}\n\n\tpsi_schedule_rtpoll_work(group,\n\t\tnsecs_to_jiffies(group->rtpoll_next_update - now) + 1,\n\t\tforce_reschedule);\n\nout:\n\tmutex_unlock(&group->rtpoll_trigger_lock);\n}\n\nstatic int psi_rtpoll_worker(void *data)\n{\n\tstruct psi_group *group = (struct psi_group *)data;\n\n\tsched_set_fifo_low(current);\n\n\twhile (true) {\n\t\twait_event_interruptible(group->rtpoll_wait,\n\t\t\t\tatomic_cmpxchg(&group->rtpoll_wakeup, 1, 0) ||\n\t\t\t\tkthread_should_stop());\n\t\tif (kthread_should_stop())\n\t\t\tbreak;\n\n\t\tpsi_rtpoll_work(group);\n\t}\n\treturn 0;\n}\n\nstatic void poll_timer_fn(struct timer_list *t)\n{\n\tstruct psi_group *group = from_timer(group, t, rtpoll_timer);\n\n\tatomic_set(&group->rtpoll_wakeup, 1);\n\twake_up_interruptible(&group->rtpoll_wait);\n}\n\nstatic void record_times(struct psi_group_cpu *groupc, u64 now)\n{\n\tu32 delta;\n\n\tdelta = now - groupc->state_start;\n\tgroupc->state_start = now;\n\n\tif (groupc->state_mask & (1 << PSI_IO_SOME)) {\n\t\tgroupc->times[PSI_IO_SOME] += delta;\n\t\tif (groupc->state_mask & (1 << PSI_IO_FULL))\n\t\t\tgroupc->times[PSI_IO_FULL] += delta;\n\t}\n\n\tif (groupc->state_mask & (1 << PSI_MEM_SOME)) {\n\t\tgroupc->times[PSI_MEM_SOME] += delta;\n\t\tif (groupc->state_mask & (1 << PSI_MEM_FULL))\n\t\t\tgroupc->times[PSI_MEM_FULL] += delta;\n\t}\n\n\tif (groupc->state_mask & (1 << PSI_CPU_SOME)) {\n\t\tgroupc->times[PSI_CPU_SOME] += delta;\n\t\tif (groupc->state_mask & (1 << PSI_CPU_FULL))\n\t\t\tgroupc->times[PSI_CPU_FULL] += delta;\n\t}\n\n\tif (groupc->state_mask & (1 << PSI_NONIDLE))\n\t\tgroupc->times[PSI_NONIDLE] += delta;\n}\n\nstatic void psi_group_change(struct psi_group *group, int cpu,\n\t\t\t     unsigned int clear, unsigned int set, u64 now,\n\t\t\t     bool wake_clock)\n{\n\tstruct psi_group_cpu *groupc;\n\tunsigned int t, m;\n\tenum psi_states s;\n\tu32 state_mask;\n\n\tgroupc = per_cpu_ptr(group->pcpu, cpu);\n\n\t \n\twrite_seqcount_begin(&groupc->seq);\n\n\t \n\tif (unlikely(clear & TSK_ONCPU)) {\n\t\tstate_mask = 0;\n\t\tclear &= ~TSK_ONCPU;\n\t} else if (unlikely(set & TSK_ONCPU)) {\n\t\tstate_mask = PSI_ONCPU;\n\t\tset &= ~TSK_ONCPU;\n\t} else {\n\t\tstate_mask = groupc->state_mask & PSI_ONCPU;\n\t}\n\n\t \n\tfor (t = 0, m = clear; m; m &= ~(1 << t), t++) {\n\t\tif (!(m & (1 << t)))\n\t\t\tcontinue;\n\t\tif (groupc->tasks[t]) {\n\t\t\tgroupc->tasks[t]--;\n\t\t} else if (!psi_bug) {\n\t\t\tprintk_deferred(KERN_ERR \"psi: task underflow! cpu=%d t=%d tasks=[%u %u %u %u] clear=%x set=%x\\n\",\n\t\t\t\t\tcpu, t, groupc->tasks[0],\n\t\t\t\t\tgroupc->tasks[1], groupc->tasks[2],\n\t\t\t\t\tgroupc->tasks[3], clear, set);\n\t\t\tpsi_bug = 1;\n\t\t}\n\t}\n\n\tfor (t = 0; set; set &= ~(1 << t), t++)\n\t\tif (set & (1 << t))\n\t\t\tgroupc->tasks[t]++;\n\n\tif (!group->enabled) {\n\t\t \n\t\tif (unlikely(groupc->state_mask & (1 << PSI_NONIDLE)))\n\t\t\trecord_times(groupc, now);\n\n\t\tgroupc->state_mask = state_mask;\n\n\t\twrite_seqcount_end(&groupc->seq);\n\t\treturn;\n\t}\n\n\tfor (s = 0; s < NR_PSI_STATES; s++) {\n\t\tif (test_state(groupc->tasks, s, state_mask & PSI_ONCPU))\n\t\t\tstate_mask |= (1 << s);\n\t}\n\n\t \n\tif (unlikely((state_mask & PSI_ONCPU) && cpu_curr(cpu)->in_memstall))\n\t\tstate_mask |= (1 << PSI_MEM_FULL);\n\n\trecord_times(groupc, now);\n\n\tgroupc->state_mask = state_mask;\n\n\twrite_seqcount_end(&groupc->seq);\n\n\tif (state_mask & group->rtpoll_states)\n\t\tpsi_schedule_rtpoll_work(group, 1, false);\n\n\tif (wake_clock && !delayed_work_pending(&group->avgs_work))\n\t\tschedule_delayed_work(&group->avgs_work, PSI_FREQ);\n}\n\nstatic inline struct psi_group *task_psi_group(struct task_struct *task)\n{\n#ifdef CONFIG_CGROUPS\n\tif (static_branch_likely(&psi_cgroups_enabled))\n\t\treturn cgroup_psi(task_dfl_cgroup(task));\n#endif\n\treturn &psi_system;\n}\n\nstatic void psi_flags_change(struct task_struct *task, int clear, int set)\n{\n\tif (((task->psi_flags & set) ||\n\t     (task->psi_flags & clear) != clear) &&\n\t    !psi_bug) {\n\t\tprintk_deferred(KERN_ERR \"psi: inconsistent task state! task=%d:%s cpu=%d psi_flags=%x clear=%x set=%x\\n\",\n\t\t\t\ttask->pid, task->comm, task_cpu(task),\n\t\t\t\ttask->psi_flags, clear, set);\n\t\tpsi_bug = 1;\n\t}\n\n\ttask->psi_flags &= ~clear;\n\ttask->psi_flags |= set;\n}\n\nvoid psi_task_change(struct task_struct *task, int clear, int set)\n{\n\tint cpu = task_cpu(task);\n\tstruct psi_group *group;\n\tu64 now;\n\n\tif (!task->pid)\n\t\treturn;\n\n\tpsi_flags_change(task, clear, set);\n\n\tnow = cpu_clock(cpu);\n\n\tgroup = task_psi_group(task);\n\tdo {\n\t\tpsi_group_change(group, cpu, clear, set, now, true);\n\t} while ((group = group->parent));\n}\n\nvoid psi_task_switch(struct task_struct *prev, struct task_struct *next,\n\t\t     bool sleep)\n{\n\tstruct psi_group *group, *common = NULL;\n\tint cpu = task_cpu(prev);\n\tu64 now = cpu_clock(cpu);\n\n\tif (next->pid) {\n\t\tpsi_flags_change(next, 0, TSK_ONCPU);\n\t\t \n\t\tgroup = task_psi_group(next);\n\t\tdo {\n\t\t\tif (per_cpu_ptr(group->pcpu, cpu)->state_mask &\n\t\t\t    PSI_ONCPU) {\n\t\t\t\tcommon = group;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tpsi_group_change(group, cpu, 0, TSK_ONCPU, now, true);\n\t\t} while ((group = group->parent));\n\t}\n\n\tif (prev->pid) {\n\t\tint clear = TSK_ONCPU, set = 0;\n\t\tbool wake_clock = true;\n\n\t\t \n\t\tif (sleep) {\n\t\t\tclear |= TSK_RUNNING;\n\t\t\tif (prev->in_memstall)\n\t\t\t\tclear |= TSK_MEMSTALL_RUNNING;\n\t\t\tif (prev->in_iowait)\n\t\t\t\tset |= TSK_IOWAIT;\n\n\t\t\t \n\t\t\tif (unlikely((prev->flags & PF_WQ_WORKER) &&\n\t\t\t\t     wq_worker_last_func(prev) == psi_avgs_work))\n\t\t\t\twake_clock = false;\n\t\t}\n\n\t\tpsi_flags_change(prev, clear, set);\n\n\t\tgroup = task_psi_group(prev);\n\t\tdo {\n\t\t\tif (group == common)\n\t\t\t\tbreak;\n\t\t\tpsi_group_change(group, cpu, clear, set, now, wake_clock);\n\t\t} while ((group = group->parent));\n\n\t\t \n\t\tif ((prev->psi_flags ^ next->psi_flags) & ~TSK_ONCPU) {\n\t\t\tclear &= ~TSK_ONCPU;\n\t\t\tfor (; group; group = group->parent)\n\t\t\t\tpsi_group_change(group, cpu, clear, set, now, wake_clock);\n\t\t}\n\t}\n}\n\n#ifdef CONFIG_IRQ_TIME_ACCOUNTING\nvoid psi_account_irqtime(struct task_struct *task, u32 delta)\n{\n\tint cpu = task_cpu(task);\n\tstruct psi_group *group;\n\tstruct psi_group_cpu *groupc;\n\tu64 now;\n\n\tif (!task->pid)\n\t\treturn;\n\n\tnow = cpu_clock(cpu);\n\n\tgroup = task_psi_group(task);\n\tdo {\n\t\tif (!group->enabled)\n\t\t\tcontinue;\n\n\t\tgroupc = per_cpu_ptr(group->pcpu, cpu);\n\n\t\twrite_seqcount_begin(&groupc->seq);\n\n\t\trecord_times(groupc, now);\n\t\tgroupc->times[PSI_IRQ_FULL] += delta;\n\n\t\twrite_seqcount_end(&groupc->seq);\n\n\t\tif (group->rtpoll_states & (1 << PSI_IRQ_FULL))\n\t\t\tpsi_schedule_rtpoll_work(group, 1, false);\n\t} while ((group = group->parent));\n}\n#endif\n\n \nvoid psi_memstall_enter(unsigned long *flags)\n{\n\tstruct rq_flags rf;\n\tstruct rq *rq;\n\n\tif (static_branch_likely(&psi_disabled))\n\t\treturn;\n\n\t*flags = current->in_memstall;\n\tif (*flags)\n\t\treturn;\n\t \n\trq = this_rq_lock_irq(&rf);\n\n\tcurrent->in_memstall = 1;\n\tpsi_task_change(current, 0, TSK_MEMSTALL | TSK_MEMSTALL_RUNNING);\n\n\trq_unlock_irq(rq, &rf);\n}\nEXPORT_SYMBOL_GPL(psi_memstall_enter);\n\n \nvoid psi_memstall_leave(unsigned long *flags)\n{\n\tstruct rq_flags rf;\n\tstruct rq *rq;\n\n\tif (static_branch_likely(&psi_disabled))\n\t\treturn;\n\n\tif (*flags)\n\t\treturn;\n\t \n\trq = this_rq_lock_irq(&rf);\n\n\tcurrent->in_memstall = 0;\n\tpsi_task_change(current, TSK_MEMSTALL | TSK_MEMSTALL_RUNNING, 0);\n\n\trq_unlock_irq(rq, &rf);\n}\nEXPORT_SYMBOL_GPL(psi_memstall_leave);\n\n#ifdef CONFIG_CGROUPS\nint psi_cgroup_alloc(struct cgroup *cgroup)\n{\n\tif (!static_branch_likely(&psi_cgroups_enabled))\n\t\treturn 0;\n\n\tcgroup->psi = kzalloc(sizeof(struct psi_group), GFP_KERNEL);\n\tif (!cgroup->psi)\n\t\treturn -ENOMEM;\n\n\tcgroup->psi->pcpu = alloc_percpu(struct psi_group_cpu);\n\tif (!cgroup->psi->pcpu) {\n\t\tkfree(cgroup->psi);\n\t\treturn -ENOMEM;\n\t}\n\tgroup_init(cgroup->psi);\n\tcgroup->psi->parent = cgroup_psi(cgroup_parent(cgroup));\n\treturn 0;\n}\n\nvoid psi_cgroup_free(struct cgroup *cgroup)\n{\n\tif (!static_branch_likely(&psi_cgroups_enabled))\n\t\treturn;\n\n\tcancel_delayed_work_sync(&cgroup->psi->avgs_work);\n\tfree_percpu(cgroup->psi->pcpu);\n\t \n\tWARN_ONCE(cgroup->psi->rtpoll_states, \"psi: trigger leak\\n\");\n\tkfree(cgroup->psi);\n}\n\n \nvoid cgroup_move_task(struct task_struct *task, struct css_set *to)\n{\n\tunsigned int task_flags;\n\tstruct rq_flags rf;\n\tstruct rq *rq;\n\n\tif (!static_branch_likely(&psi_cgroups_enabled)) {\n\t\t \n\t\trcu_assign_pointer(task->cgroups, to);\n\t\treturn;\n\t}\n\n\trq = task_rq_lock(task, &rf);\n\n\t \n\ttask_flags = task->psi_flags;\n\n\tif (task_flags)\n\t\tpsi_task_change(task, task_flags, 0);\n\n\t \n\trcu_assign_pointer(task->cgroups, to);\n\n\tif (task_flags)\n\t\tpsi_task_change(task, 0, task_flags);\n\n\ttask_rq_unlock(rq, task, &rf);\n}\n\nvoid psi_cgroup_restart(struct psi_group *group)\n{\n\tint cpu;\n\n\t \n\tif (!group->enabled)\n\t\treturn;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct rq *rq = cpu_rq(cpu);\n\t\tstruct rq_flags rf;\n\t\tu64 now;\n\n\t\trq_lock_irq(rq, &rf);\n\t\tnow = cpu_clock(cpu);\n\t\tpsi_group_change(group, cpu, 0, 0, now, true);\n\t\trq_unlock_irq(rq, &rf);\n\t}\n}\n#endif  \n\nint psi_show(struct seq_file *m, struct psi_group *group, enum psi_res res)\n{\n\tbool only_full = false;\n\tint full;\n\tu64 now;\n\n\tif (static_branch_likely(&psi_disabled))\n\t\treturn -EOPNOTSUPP;\n\n\t \n\tmutex_lock(&group->avgs_lock);\n\tnow = sched_clock();\n\tcollect_percpu_times(group, PSI_AVGS, NULL);\n\tif (now >= group->avg_next_update)\n\t\tgroup->avg_next_update = update_averages(group, now);\n\tmutex_unlock(&group->avgs_lock);\n\n#ifdef CONFIG_IRQ_TIME_ACCOUNTING\n\tonly_full = res == PSI_IRQ;\n#endif\n\n\tfor (full = 0; full < 2 - only_full; full++) {\n\t\tunsigned long avg[3] = { 0, };\n\t\tu64 total = 0;\n\t\tint w;\n\n\t\t \n\t\tif (!(group == &psi_system && res == PSI_CPU && full)) {\n\t\t\tfor (w = 0; w < 3; w++)\n\t\t\t\tavg[w] = group->avg[res * 2 + full][w];\n\t\t\ttotal = div_u64(group->total[PSI_AVGS][res * 2 + full],\n\t\t\t\t\tNSEC_PER_USEC);\n\t\t}\n\n\t\tseq_printf(m, \"%s avg10=%lu.%02lu avg60=%lu.%02lu avg300=%lu.%02lu total=%llu\\n\",\n\t\t\t   full || only_full ? \"full\" : \"some\",\n\t\t\t   LOAD_INT(avg[0]), LOAD_FRAC(avg[0]),\n\t\t\t   LOAD_INT(avg[1]), LOAD_FRAC(avg[1]),\n\t\t\t   LOAD_INT(avg[2]), LOAD_FRAC(avg[2]),\n\t\t\t   total);\n\t}\n\n\treturn 0;\n}\n\nstruct psi_trigger *psi_trigger_create(struct psi_group *group, char *buf,\n\t\t\t\t       enum psi_res res, struct file *file,\n\t\t\t\t       struct kernfs_open_file *of)\n{\n\tstruct psi_trigger *t;\n\tenum psi_states state;\n\tu32 threshold_us;\n\tbool privileged;\n\tu32 window_us;\n\n\tif (static_branch_likely(&psi_disabled))\n\t\treturn ERR_PTR(-EOPNOTSUPP);\n\n\t \n\tprivileged = cap_raised(file->f_cred->cap_effective, CAP_SYS_RESOURCE);\n\n\tif (sscanf(buf, \"some %u %u\", &threshold_us, &window_us) == 2)\n\t\tstate = PSI_IO_SOME + res * 2;\n\telse if (sscanf(buf, \"full %u %u\", &threshold_us, &window_us) == 2)\n\t\tstate = PSI_IO_FULL + res * 2;\n\telse\n\t\treturn ERR_PTR(-EINVAL);\n\n#ifdef CONFIG_IRQ_TIME_ACCOUNTING\n\tif (res == PSI_IRQ && --state != PSI_IRQ_FULL)\n\t\treturn ERR_PTR(-EINVAL);\n#endif\n\n\tif (state >= PSI_NONIDLE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (window_us == 0 || window_us > WINDOW_MAX_US)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t \n\tif (!privileged && window_us % 2000000)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t \n\tif (threshold_us == 0 || threshold_us > window_us)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tt = kmalloc(sizeof(*t), GFP_KERNEL);\n\tif (!t)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tt->group = group;\n\tt->state = state;\n\tt->threshold = threshold_us * NSEC_PER_USEC;\n\tt->win.size = window_us * NSEC_PER_USEC;\n\twindow_reset(&t->win, sched_clock(),\n\t\t\tgroup->total[PSI_POLL][t->state], 0);\n\n\tt->event = 0;\n\tt->last_event_time = 0;\n\tt->of = of;\n\tif (!of)\n\t\tinit_waitqueue_head(&t->event_wait);\n\tt->pending_event = false;\n\tt->aggregator = privileged ? PSI_POLL : PSI_AVGS;\n\n\tif (privileged) {\n\t\tmutex_lock(&group->rtpoll_trigger_lock);\n\n\t\tif (!rcu_access_pointer(group->rtpoll_task)) {\n\t\t\tstruct task_struct *task;\n\n\t\t\ttask = kthread_create(psi_rtpoll_worker, group, \"psimon\");\n\t\t\tif (IS_ERR(task)) {\n\t\t\t\tkfree(t);\n\t\t\t\tmutex_unlock(&group->rtpoll_trigger_lock);\n\t\t\t\treturn ERR_CAST(task);\n\t\t\t}\n\t\t\tatomic_set(&group->rtpoll_wakeup, 0);\n\t\t\twake_up_process(task);\n\t\t\trcu_assign_pointer(group->rtpoll_task, task);\n\t\t}\n\n\t\tlist_add(&t->node, &group->rtpoll_triggers);\n\t\tgroup->rtpoll_min_period = min(group->rtpoll_min_period,\n\t\t\tdiv_u64(t->win.size, UPDATES_PER_WINDOW));\n\t\tgroup->rtpoll_nr_triggers[t->state]++;\n\t\tgroup->rtpoll_states |= (1 << t->state);\n\n\t\tmutex_unlock(&group->rtpoll_trigger_lock);\n\t} else {\n\t\tmutex_lock(&group->avgs_lock);\n\n\t\tlist_add(&t->node, &group->avg_triggers);\n\t\tgroup->avg_nr_triggers[t->state]++;\n\n\t\tmutex_unlock(&group->avgs_lock);\n\t}\n\treturn t;\n}\n\nvoid psi_trigger_destroy(struct psi_trigger *t)\n{\n\tstruct psi_group *group;\n\tstruct task_struct *task_to_destroy = NULL;\n\n\t \n\tif (!t)\n\t\treturn;\n\n\tgroup = t->group;\n\t \n\tif (t->of)\n\t\tkernfs_notify(t->of->kn);\n\telse\n\t\twake_up_interruptible(&t->event_wait);\n\n\tif (t->aggregator == PSI_AVGS) {\n\t\tmutex_lock(&group->avgs_lock);\n\t\tif (!list_empty(&t->node)) {\n\t\t\tlist_del(&t->node);\n\t\t\tgroup->avg_nr_triggers[t->state]--;\n\t\t}\n\t\tmutex_unlock(&group->avgs_lock);\n\t} else {\n\t\tmutex_lock(&group->rtpoll_trigger_lock);\n\t\tif (!list_empty(&t->node)) {\n\t\t\tstruct psi_trigger *tmp;\n\t\t\tu64 period = ULLONG_MAX;\n\n\t\t\tlist_del(&t->node);\n\t\t\tgroup->rtpoll_nr_triggers[t->state]--;\n\t\t\tif (!group->rtpoll_nr_triggers[t->state])\n\t\t\t\tgroup->rtpoll_states &= ~(1 << t->state);\n\t\t\t \n\t\t\tif (group->rtpoll_min_period == div_u64(t->win.size, UPDATES_PER_WINDOW)) {\n\t\t\t\tlist_for_each_entry(tmp, &group->rtpoll_triggers, node)\n\t\t\t\t\tperiod = min(period, div_u64(tmp->win.size,\n\t\t\t\t\t\t\tUPDATES_PER_WINDOW));\n\t\t\t\tgroup->rtpoll_min_period = period;\n\t\t\t}\n\t\t\t \n\t\t\tif (group->rtpoll_states == 0) {\n\t\t\t\tgroup->rtpoll_until = 0;\n\t\t\t\ttask_to_destroy = rcu_dereference_protected(\n\t\t\t\t\t\tgroup->rtpoll_task,\n\t\t\t\t\t\tlockdep_is_held(&group->rtpoll_trigger_lock));\n\t\t\t\trcu_assign_pointer(group->rtpoll_task, NULL);\n\t\t\t\tdel_timer(&group->rtpoll_timer);\n\t\t\t}\n\t\t}\n\t\tmutex_unlock(&group->rtpoll_trigger_lock);\n\t}\n\n\t \n\tsynchronize_rcu();\n\t \n\tif (task_to_destroy) {\n\t\t \n\t\tkthread_stop(task_to_destroy);\n\t\tatomic_set(&group->rtpoll_scheduled, 0);\n\t}\n\tkfree(t);\n}\n\n__poll_t psi_trigger_poll(void **trigger_ptr,\n\t\t\t\tstruct file *file, poll_table *wait)\n{\n\t__poll_t ret = DEFAULT_POLLMASK;\n\tstruct psi_trigger *t;\n\n\tif (static_branch_likely(&psi_disabled))\n\t\treturn DEFAULT_POLLMASK | EPOLLERR | EPOLLPRI;\n\n\tt = smp_load_acquire(trigger_ptr);\n\tif (!t)\n\t\treturn DEFAULT_POLLMASK | EPOLLERR | EPOLLPRI;\n\n\tif (t->of)\n\t\tkernfs_generic_poll(t->of, wait);\n\telse\n\t\tpoll_wait(file, &t->event_wait, wait);\n\n\tif (cmpxchg(&t->event, 1, 0) == 1)\n\t\tret |= EPOLLPRI;\n\n\treturn ret;\n}\n\n#ifdef CONFIG_PROC_FS\nstatic int psi_io_show(struct seq_file *m, void *v)\n{\n\treturn psi_show(m, &psi_system, PSI_IO);\n}\n\nstatic int psi_memory_show(struct seq_file *m, void *v)\n{\n\treturn psi_show(m, &psi_system, PSI_MEM);\n}\n\nstatic int psi_cpu_show(struct seq_file *m, void *v)\n{\n\treturn psi_show(m, &psi_system, PSI_CPU);\n}\n\nstatic int psi_io_open(struct inode *inode, struct file *file)\n{\n\treturn single_open(file, psi_io_show, NULL);\n}\n\nstatic int psi_memory_open(struct inode *inode, struct file *file)\n{\n\treturn single_open(file, psi_memory_show, NULL);\n}\n\nstatic int psi_cpu_open(struct inode *inode, struct file *file)\n{\n\treturn single_open(file, psi_cpu_show, NULL);\n}\n\nstatic ssize_t psi_write(struct file *file, const char __user *user_buf,\n\t\t\t size_t nbytes, enum psi_res res)\n{\n\tchar buf[32];\n\tsize_t buf_size;\n\tstruct seq_file *seq;\n\tstruct psi_trigger *new;\n\n\tif (static_branch_likely(&psi_disabled))\n\t\treturn -EOPNOTSUPP;\n\n\tif (!nbytes)\n\t\treturn -EINVAL;\n\n\tbuf_size = min(nbytes, sizeof(buf));\n\tif (copy_from_user(buf, user_buf, buf_size))\n\t\treturn -EFAULT;\n\n\tbuf[buf_size - 1] = '\\0';\n\n\tseq = file->private_data;\n\n\t \n\tmutex_lock(&seq->lock);\n\n\t \n\tif (seq->private) {\n\t\tmutex_unlock(&seq->lock);\n\t\treturn -EBUSY;\n\t}\n\n\tnew = psi_trigger_create(&psi_system, buf, res, file, NULL);\n\tif (IS_ERR(new)) {\n\t\tmutex_unlock(&seq->lock);\n\t\treturn PTR_ERR(new);\n\t}\n\n\tsmp_store_release(&seq->private, new);\n\tmutex_unlock(&seq->lock);\n\n\treturn nbytes;\n}\n\nstatic ssize_t psi_io_write(struct file *file, const char __user *user_buf,\n\t\t\t    size_t nbytes, loff_t *ppos)\n{\n\treturn psi_write(file, user_buf, nbytes, PSI_IO);\n}\n\nstatic ssize_t psi_memory_write(struct file *file, const char __user *user_buf,\n\t\t\t\tsize_t nbytes, loff_t *ppos)\n{\n\treturn psi_write(file, user_buf, nbytes, PSI_MEM);\n}\n\nstatic ssize_t psi_cpu_write(struct file *file, const char __user *user_buf,\n\t\t\t     size_t nbytes, loff_t *ppos)\n{\n\treturn psi_write(file, user_buf, nbytes, PSI_CPU);\n}\n\nstatic __poll_t psi_fop_poll(struct file *file, poll_table *wait)\n{\n\tstruct seq_file *seq = file->private_data;\n\n\treturn psi_trigger_poll(&seq->private, file, wait);\n}\n\nstatic int psi_fop_release(struct inode *inode, struct file *file)\n{\n\tstruct seq_file *seq = file->private_data;\n\n\tpsi_trigger_destroy(seq->private);\n\treturn single_release(inode, file);\n}\n\nstatic const struct proc_ops psi_io_proc_ops = {\n\t.proc_open\t= psi_io_open,\n\t.proc_read\t= seq_read,\n\t.proc_lseek\t= seq_lseek,\n\t.proc_write\t= psi_io_write,\n\t.proc_poll\t= psi_fop_poll,\n\t.proc_release\t= psi_fop_release,\n};\n\nstatic const struct proc_ops psi_memory_proc_ops = {\n\t.proc_open\t= psi_memory_open,\n\t.proc_read\t= seq_read,\n\t.proc_lseek\t= seq_lseek,\n\t.proc_write\t= psi_memory_write,\n\t.proc_poll\t= psi_fop_poll,\n\t.proc_release\t= psi_fop_release,\n};\n\nstatic const struct proc_ops psi_cpu_proc_ops = {\n\t.proc_open\t= psi_cpu_open,\n\t.proc_read\t= seq_read,\n\t.proc_lseek\t= seq_lseek,\n\t.proc_write\t= psi_cpu_write,\n\t.proc_poll\t= psi_fop_poll,\n\t.proc_release\t= psi_fop_release,\n};\n\n#ifdef CONFIG_IRQ_TIME_ACCOUNTING\nstatic int psi_irq_show(struct seq_file *m, void *v)\n{\n\treturn psi_show(m, &psi_system, PSI_IRQ);\n}\n\nstatic int psi_irq_open(struct inode *inode, struct file *file)\n{\n\treturn single_open(file, psi_irq_show, NULL);\n}\n\nstatic ssize_t psi_irq_write(struct file *file, const char __user *user_buf,\n\t\t\t     size_t nbytes, loff_t *ppos)\n{\n\treturn psi_write(file, user_buf, nbytes, PSI_IRQ);\n}\n\nstatic const struct proc_ops psi_irq_proc_ops = {\n\t.proc_open\t= psi_irq_open,\n\t.proc_read\t= seq_read,\n\t.proc_lseek\t= seq_lseek,\n\t.proc_write\t= psi_irq_write,\n\t.proc_poll\t= psi_fop_poll,\n\t.proc_release\t= psi_fop_release,\n};\n#endif\n\nstatic int __init psi_proc_init(void)\n{\n\tif (psi_enable) {\n\t\tproc_mkdir(\"pressure\", NULL);\n\t\tproc_create(\"pressure/io\", 0666, NULL, &psi_io_proc_ops);\n\t\tproc_create(\"pressure/memory\", 0666, NULL, &psi_memory_proc_ops);\n\t\tproc_create(\"pressure/cpu\", 0666, NULL, &psi_cpu_proc_ops);\n#ifdef CONFIG_IRQ_TIME_ACCOUNTING\n\t\tproc_create(\"pressure/irq\", 0666, NULL, &psi_irq_proc_ops);\n#endif\n\t}\n\treturn 0;\n}\nmodule_init(psi_proc_init);\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}