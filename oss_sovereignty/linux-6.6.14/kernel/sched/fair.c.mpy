{
  "module_name": "fair.c",
  "hash_id": "9757861aa1e75ae9f95299eae3aceb42bc5a2afb47061f6a82a8103bfcf3773b",
  "original_prompt": "Ingested from linux-6.6.14/kernel/sched/fair.c",
  "human_readable_source": "\n \n#include <linux/energy_model.h>\n#include <linux/mmap_lock.h>\n#include <linux/hugetlb_inline.h>\n#include <linux/jiffies.h>\n#include <linux/mm_api.h>\n#include <linux/highmem.h>\n#include <linux/spinlock_api.h>\n#include <linux/cpumask_api.h>\n#include <linux/lockdep_api.h>\n#include <linux/softirq.h>\n#include <linux/refcount_api.h>\n#include <linux/topology.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/cond_resched.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/nohz.h>\n\n#include <linux/cpuidle.h>\n#include <linux/interrupt.h>\n#include <linux/memory-tiers.h>\n#include <linux/mempolicy.h>\n#include <linux/mutex_api.h>\n#include <linux/profile.h>\n#include <linux/psi.h>\n#include <linux/ratelimit.h>\n#include <linux/task_work.h>\n#include <linux/rbtree_augmented.h>\n\n#include <asm/switch_to.h>\n\n#include <linux/sched/cond_resched.h>\n\n#include \"sched.h\"\n#include \"stats.h\"\n#include \"autogroup.h\"\n\n \nunsigned int sysctl_sched_tunable_scaling = SCHED_TUNABLESCALING_LOG;\n\n \nunsigned int sysctl_sched_base_slice\t\t\t= 750000ULL;\nstatic unsigned int normalized_sysctl_sched_base_slice\t= 750000ULL;\n\n \nunsigned int sysctl_sched_child_runs_first __read_mostly;\n\nconst_debug unsigned int sysctl_sched_migration_cost\t= 500000UL;\n\nint sched_thermal_decay_shift;\nstatic int __init setup_sched_thermal_decay_shift(char *str)\n{\n\tint _shift = 0;\n\n\tif (kstrtoint(str, 0, &_shift))\n\t\tpr_warn(\"Unable to set scheduler thermal pressure decay shift parameter\\n\");\n\n\tsched_thermal_decay_shift = clamp(_shift, 0, 10);\n\treturn 1;\n}\n__setup(\"sched_thermal_decay_shift=\", setup_sched_thermal_decay_shift);\n\n#ifdef CONFIG_SMP\n \nint __weak arch_asym_cpu_priority(int cpu)\n{\n\treturn -cpu;\n}\n\n \n#define fits_capacity(cap, max)\t((cap) * 1280 < (max) * 1024)\n\n \n#define capacity_greater(cap1, cap2) ((cap1) * 1024 > (cap2) * 1078)\n#endif\n\n#ifdef CONFIG_CFS_BANDWIDTH\n \nstatic unsigned int sysctl_sched_cfs_bandwidth_slice\t\t= 5000UL;\n#endif\n\n#ifdef CONFIG_NUMA_BALANCING\n \nstatic unsigned int sysctl_numa_balancing_promote_rate_limit = 65536;\n#endif\n\n#ifdef CONFIG_SYSCTL\nstatic struct ctl_table sched_fair_sysctls[] = {\n\t{\n\t\t.procname       = \"sched_child_runs_first\",\n\t\t.data           = &sysctl_sched_child_runs_first,\n\t\t.maxlen         = sizeof(unsigned int),\n\t\t.mode           = 0644,\n\t\t.proc_handler   = proc_dointvec,\n\t},\n#ifdef CONFIG_CFS_BANDWIDTH\n\t{\n\t\t.procname       = \"sched_cfs_bandwidth_slice_us\",\n\t\t.data           = &sysctl_sched_cfs_bandwidth_slice,\n\t\t.maxlen         = sizeof(unsigned int),\n\t\t.mode           = 0644,\n\t\t.proc_handler   = proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t},\n#endif\n#ifdef CONFIG_NUMA_BALANCING\n\t{\n\t\t.procname\t= \"numa_balancing_promote_rate_limit_MBps\",\n\t\t.data\t\t= &sysctl_numa_balancing_promote_rate_limit,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t},\n#endif  \n\t{}\n};\n\nstatic int __init sched_fair_sysctl_init(void)\n{\n\tregister_sysctl_init(\"kernel\", sched_fair_sysctls);\n\treturn 0;\n}\nlate_initcall(sched_fair_sysctl_init);\n#endif\n\nstatic inline void update_load_add(struct load_weight *lw, unsigned long inc)\n{\n\tlw->weight += inc;\n\tlw->inv_weight = 0;\n}\n\nstatic inline void update_load_sub(struct load_weight *lw, unsigned long dec)\n{\n\tlw->weight -= dec;\n\tlw->inv_weight = 0;\n}\n\nstatic inline void update_load_set(struct load_weight *lw, unsigned long w)\n{\n\tlw->weight = w;\n\tlw->inv_weight = 0;\n}\n\n \nstatic unsigned int get_update_sysctl_factor(void)\n{\n\tunsigned int cpus = min_t(unsigned int, num_online_cpus(), 8);\n\tunsigned int factor;\n\n\tswitch (sysctl_sched_tunable_scaling) {\n\tcase SCHED_TUNABLESCALING_NONE:\n\t\tfactor = 1;\n\t\tbreak;\n\tcase SCHED_TUNABLESCALING_LINEAR:\n\t\tfactor = cpus;\n\t\tbreak;\n\tcase SCHED_TUNABLESCALING_LOG:\n\tdefault:\n\t\tfactor = 1 + ilog2(cpus);\n\t\tbreak;\n\t}\n\n\treturn factor;\n}\n\nstatic void update_sysctl(void)\n{\n\tunsigned int factor = get_update_sysctl_factor();\n\n#define SET_SYSCTL(name) \\\n\t(sysctl_##name = (factor) * normalized_sysctl_##name)\n\tSET_SYSCTL(sched_base_slice);\n#undef SET_SYSCTL\n}\n\nvoid __init sched_init_granularity(void)\n{\n\tupdate_sysctl();\n}\n\n#define WMULT_CONST\t(~0U)\n#define WMULT_SHIFT\t32\n\nstatic void __update_inv_weight(struct load_weight *lw)\n{\n\tunsigned long w;\n\n\tif (likely(lw->inv_weight))\n\t\treturn;\n\n\tw = scale_load_down(lw->weight);\n\n\tif (BITS_PER_LONG > 32 && unlikely(w >= WMULT_CONST))\n\t\tlw->inv_weight = 1;\n\telse if (unlikely(!w))\n\t\tlw->inv_weight = WMULT_CONST;\n\telse\n\t\tlw->inv_weight = WMULT_CONST / w;\n}\n\n \nstatic u64 __calc_delta(u64 delta_exec, unsigned long weight, struct load_weight *lw)\n{\n\tu64 fact = scale_load_down(weight);\n\tu32 fact_hi = (u32)(fact >> 32);\n\tint shift = WMULT_SHIFT;\n\tint fs;\n\n\t__update_inv_weight(lw);\n\n\tif (unlikely(fact_hi)) {\n\t\tfs = fls(fact_hi);\n\t\tshift -= fs;\n\t\tfact >>= fs;\n\t}\n\n\tfact = mul_u32_u32(fact, lw->inv_weight);\n\n\tfact_hi = (u32)(fact >> 32);\n\tif (fact_hi) {\n\t\tfs = fls(fact_hi);\n\t\tshift -= fs;\n\t\tfact >>= fs;\n\t}\n\n\treturn mul_u64_u32_shr(delta_exec, fact, shift);\n}\n\n \nstatic inline u64 calc_delta_fair(u64 delta, struct sched_entity *se)\n{\n\tif (unlikely(se->load.weight != NICE_0_LOAD))\n\t\tdelta = __calc_delta(delta, NICE_0_LOAD, &se->load);\n\n\treturn delta;\n}\n\nconst struct sched_class fair_sched_class;\n\n \n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\n \n#define for_each_sched_entity(se) \\\n\t\tfor (; se; se = se->parent)\n\nstatic inline bool list_add_leaf_cfs_rq(struct cfs_rq *cfs_rq)\n{\n\tstruct rq *rq = rq_of(cfs_rq);\n\tint cpu = cpu_of(rq);\n\n\tif (cfs_rq->on_list)\n\t\treturn rq->tmp_alone_branch == &rq->leaf_cfs_rq_list;\n\n\tcfs_rq->on_list = 1;\n\n\t \n\tif (cfs_rq->tg->parent &&\n\t    cfs_rq->tg->parent->cfs_rq[cpu]->on_list) {\n\t\t \n\t\tlist_add_tail_rcu(&cfs_rq->leaf_cfs_rq_list,\n\t\t\t&(cfs_rq->tg->parent->cfs_rq[cpu]->leaf_cfs_rq_list));\n\t\t \n\t\trq->tmp_alone_branch = &rq->leaf_cfs_rq_list;\n\t\treturn true;\n\t}\n\n\tif (!cfs_rq->tg->parent) {\n\t\t \n\t\tlist_add_tail_rcu(&cfs_rq->leaf_cfs_rq_list,\n\t\t\t&rq->leaf_cfs_rq_list);\n\t\t \n\t\trq->tmp_alone_branch = &rq->leaf_cfs_rq_list;\n\t\treturn true;\n\t}\n\n\t \n\tlist_add_rcu(&cfs_rq->leaf_cfs_rq_list, rq->tmp_alone_branch);\n\t \n\trq->tmp_alone_branch = &cfs_rq->leaf_cfs_rq_list;\n\treturn false;\n}\n\nstatic inline void list_del_leaf_cfs_rq(struct cfs_rq *cfs_rq)\n{\n\tif (cfs_rq->on_list) {\n\t\tstruct rq *rq = rq_of(cfs_rq);\n\n\t\t \n\t\tif (rq->tmp_alone_branch == &cfs_rq->leaf_cfs_rq_list)\n\t\t\trq->tmp_alone_branch = cfs_rq->leaf_cfs_rq_list.prev;\n\n\t\tlist_del_rcu(&cfs_rq->leaf_cfs_rq_list);\n\t\tcfs_rq->on_list = 0;\n\t}\n}\n\nstatic inline void assert_list_leaf_cfs_rq(struct rq *rq)\n{\n\tSCHED_WARN_ON(rq->tmp_alone_branch != &rq->leaf_cfs_rq_list);\n}\n\n \n#define for_each_leaf_cfs_rq_safe(rq, cfs_rq, pos)\t\t\t\\\n\tlist_for_each_entry_safe(cfs_rq, pos, &rq->leaf_cfs_rq_list,\t\\\n\t\t\t\t leaf_cfs_rq_list)\n\n \nstatic inline struct cfs_rq *\nis_same_group(struct sched_entity *se, struct sched_entity *pse)\n{\n\tif (se->cfs_rq == pse->cfs_rq)\n\t\treturn se->cfs_rq;\n\n\treturn NULL;\n}\n\nstatic inline struct sched_entity *parent_entity(const struct sched_entity *se)\n{\n\treturn se->parent;\n}\n\nstatic void\nfind_matching_se(struct sched_entity **se, struct sched_entity **pse)\n{\n\tint se_depth, pse_depth;\n\n\t \n\n\t \n\tse_depth = (*se)->depth;\n\tpse_depth = (*pse)->depth;\n\n\twhile (se_depth > pse_depth) {\n\t\tse_depth--;\n\t\t*se = parent_entity(*se);\n\t}\n\n\twhile (pse_depth > se_depth) {\n\t\tpse_depth--;\n\t\t*pse = parent_entity(*pse);\n\t}\n\n\twhile (!is_same_group(*se, *pse)) {\n\t\t*se = parent_entity(*se);\n\t\t*pse = parent_entity(*pse);\n\t}\n}\n\nstatic int tg_is_idle(struct task_group *tg)\n{\n\treturn tg->idle > 0;\n}\n\nstatic int cfs_rq_is_idle(struct cfs_rq *cfs_rq)\n{\n\treturn cfs_rq->idle > 0;\n}\n\nstatic int se_is_idle(struct sched_entity *se)\n{\n\tif (entity_is_task(se))\n\t\treturn task_has_idle_policy(task_of(se));\n\treturn cfs_rq_is_idle(group_cfs_rq(se));\n}\n\n#else\t \n\n#define for_each_sched_entity(se) \\\n\t\tfor (; se; se = NULL)\n\nstatic inline bool list_add_leaf_cfs_rq(struct cfs_rq *cfs_rq)\n{\n\treturn true;\n}\n\nstatic inline void list_del_leaf_cfs_rq(struct cfs_rq *cfs_rq)\n{\n}\n\nstatic inline void assert_list_leaf_cfs_rq(struct rq *rq)\n{\n}\n\n#define for_each_leaf_cfs_rq_safe(rq, cfs_rq, pos)\t\\\n\t\tfor (cfs_rq = &rq->cfs, pos = NULL; cfs_rq; cfs_rq = pos)\n\nstatic inline struct sched_entity *parent_entity(struct sched_entity *se)\n{\n\treturn NULL;\n}\n\nstatic inline void\nfind_matching_se(struct sched_entity **se, struct sched_entity **pse)\n{\n}\n\nstatic inline int tg_is_idle(struct task_group *tg)\n{\n\treturn 0;\n}\n\nstatic int cfs_rq_is_idle(struct cfs_rq *cfs_rq)\n{\n\treturn 0;\n}\n\nstatic int se_is_idle(struct sched_entity *se)\n{\n\treturn 0;\n}\n\n#endif\t \n\nstatic __always_inline\nvoid account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec);\n\n \n\nstatic inline u64 max_vruntime(u64 max_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - max_vruntime);\n\tif (delta > 0)\n\t\tmax_vruntime = vruntime;\n\n\treturn max_vruntime;\n}\n\nstatic inline u64 min_vruntime(u64 min_vruntime, u64 vruntime)\n{\n\ts64 delta = (s64)(vruntime - min_vruntime);\n\tif (delta < 0)\n\t\tmin_vruntime = vruntime;\n\n\treturn min_vruntime;\n}\n\nstatic inline bool entity_before(const struct sched_entity *a,\n\t\t\t\t const struct sched_entity *b)\n{\n\treturn (s64)(a->vruntime - b->vruntime) < 0;\n}\n\nstatic inline s64 entity_key(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\treturn (s64)(se->vruntime - cfs_rq->min_vruntime);\n}\n\n#define __node_2_se(node) \\\n\trb_entry((node), struct sched_entity, run_node)\n\n \nstatic void\navg_vruntime_add(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tunsigned long weight = scale_load_down(se->load.weight);\n\ts64 key = entity_key(cfs_rq, se);\n\n\tcfs_rq->avg_vruntime += key * weight;\n\tcfs_rq->avg_load += weight;\n}\n\nstatic void\navg_vruntime_sub(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tunsigned long weight = scale_load_down(se->load.weight);\n\ts64 key = entity_key(cfs_rq, se);\n\n\tcfs_rq->avg_vruntime -= key * weight;\n\tcfs_rq->avg_load -= weight;\n}\n\nstatic inline\nvoid avg_vruntime_update(struct cfs_rq *cfs_rq, s64 delta)\n{\n\t \n\tcfs_rq->avg_vruntime -= cfs_rq->avg_load * delta;\n}\n\n \nu64 avg_vruntime(struct cfs_rq *cfs_rq)\n{\n\tstruct sched_entity *curr = cfs_rq->curr;\n\ts64 avg = cfs_rq->avg_vruntime;\n\tlong load = cfs_rq->avg_load;\n\n\tif (curr && curr->on_rq) {\n\t\tunsigned long weight = scale_load_down(curr->load.weight);\n\n\t\tavg += entity_key(cfs_rq, curr) * weight;\n\t\tload += weight;\n\t}\n\n\tif (load) {\n\t\t \n\t\tif (avg < 0)\n\t\t\tavg -= (load - 1);\n\t\tavg = div_s64(avg, load);\n\t}\n\n\treturn cfs_rq->min_vruntime + avg;\n}\n\n \nstatic void update_entity_lag(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\ts64 lag, limit;\n\n\tSCHED_WARN_ON(!se->on_rq);\n\tlag = avg_vruntime(cfs_rq) - se->vruntime;\n\n\tlimit = calc_delta_fair(max_t(u64, 2*se->slice, TICK_NSEC), se);\n\tse->vlag = clamp(lag, -limit, limit);\n}\n\n \nint entity_eligible(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tstruct sched_entity *curr = cfs_rq->curr;\n\ts64 avg = cfs_rq->avg_vruntime;\n\tlong load = cfs_rq->avg_load;\n\n\tif (curr && curr->on_rq) {\n\t\tunsigned long weight = scale_load_down(curr->load.weight);\n\n\t\tavg += entity_key(cfs_rq, curr) * weight;\n\t\tload += weight;\n\t}\n\n\treturn avg >= entity_key(cfs_rq, se) * load;\n}\n\nstatic u64 __update_min_vruntime(struct cfs_rq *cfs_rq, u64 vruntime)\n{\n\tu64 min_vruntime = cfs_rq->min_vruntime;\n\t \n\ts64 delta = (s64)(vruntime - min_vruntime);\n\tif (delta > 0) {\n\t\tavg_vruntime_update(cfs_rq, delta);\n\t\tmin_vruntime = vruntime;\n\t}\n\treturn min_vruntime;\n}\n\nstatic void update_min_vruntime(struct cfs_rq *cfs_rq)\n{\n\tstruct sched_entity *se = __pick_first_entity(cfs_rq);\n\tstruct sched_entity *curr = cfs_rq->curr;\n\n\tu64 vruntime = cfs_rq->min_vruntime;\n\n\tif (curr) {\n\t\tif (curr->on_rq)\n\t\t\tvruntime = curr->vruntime;\n\t\telse\n\t\t\tcurr = NULL;\n\t}\n\n\tif (se) {\n\t\tif (!curr)\n\t\t\tvruntime = se->vruntime;\n\t\telse\n\t\t\tvruntime = min_vruntime(vruntime, se->vruntime);\n\t}\n\n\t \n\tu64_u32_store(cfs_rq->min_vruntime,\n\t\t      __update_min_vruntime(cfs_rq, vruntime));\n}\n\nstatic inline bool __entity_less(struct rb_node *a, const struct rb_node *b)\n{\n\treturn entity_before(__node_2_se(a), __node_2_se(b));\n}\n\n#define deadline_gt(field, lse, rse) ({ (s64)((lse)->field - (rse)->field) > 0; })\n\nstatic inline void __update_min_deadline(struct sched_entity *se, struct rb_node *node)\n{\n\tif (node) {\n\t\tstruct sched_entity *rse = __node_2_se(node);\n\t\tif (deadline_gt(min_deadline, se, rse))\n\t\t\tse->min_deadline = rse->min_deadline;\n\t}\n}\n\n \nstatic inline bool min_deadline_update(struct sched_entity *se, bool exit)\n{\n\tu64 old_min_deadline = se->min_deadline;\n\tstruct rb_node *node = &se->run_node;\n\n\tse->min_deadline = se->deadline;\n\t__update_min_deadline(se, node->rb_right);\n\t__update_min_deadline(se, node->rb_left);\n\n\treturn se->min_deadline == old_min_deadline;\n}\n\nRB_DECLARE_CALLBACKS(static, min_deadline_cb, struct sched_entity,\n\t\t     run_node, min_deadline, min_deadline_update);\n\n \nstatic void __enqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tavg_vruntime_add(cfs_rq, se);\n\tse->min_deadline = se->deadline;\n\trb_add_augmented_cached(&se->run_node, &cfs_rq->tasks_timeline,\n\t\t\t\t__entity_less, &min_deadline_cb);\n}\n\nstatic void __dequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\trb_erase_augmented_cached(&se->run_node, &cfs_rq->tasks_timeline,\n\t\t\t\t  &min_deadline_cb);\n\tavg_vruntime_sub(cfs_rq, se);\n}\n\nstruct sched_entity *__pick_first_entity(struct cfs_rq *cfs_rq)\n{\n\tstruct rb_node *left = rb_first_cached(&cfs_rq->tasks_timeline);\n\n\tif (!left)\n\t\treturn NULL;\n\n\treturn __node_2_se(left);\n}\n\n \nstatic struct sched_entity *__pick_eevdf(struct cfs_rq *cfs_rq)\n{\n\tstruct rb_node *node = cfs_rq->tasks_timeline.rb_root.rb_node;\n\tstruct sched_entity *curr = cfs_rq->curr;\n\tstruct sched_entity *best = NULL;\n\tstruct sched_entity *best_left = NULL;\n\n\tif (curr && (!curr->on_rq || !entity_eligible(cfs_rq, curr)))\n\t\tcurr = NULL;\n\tbest = curr;\n\n\t \n\tif (sched_feat(RUN_TO_PARITY) && curr && curr->vlag == curr->deadline)\n\t\treturn curr;\n\n\twhile (node) {\n\t\tstruct sched_entity *se = __node_2_se(node);\n\n\t\t \n\t\tif (!entity_eligible(cfs_rq, se)) {\n\t\t\tnode = node->rb_left;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (!best || deadline_gt(deadline, best, se))\n\t\t\tbest = se;\n\n\t\t \n\t\tif (node->rb_left) {\n\t\t\tstruct sched_entity *left = __node_2_se(node->rb_left);\n\n\t\t\tif (!best_left || deadline_gt(min_deadline, best_left, left))\n\t\t\t\tbest_left = left;\n\n\t\t\t \n\t\t\tif (left->min_deadline == se->min_deadline)\n\t\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (se->deadline == se->min_deadline)\n\t\t\tbreak;\n\n\t\t \n\t\tnode = node->rb_right;\n\t}\n\n\t \n\tif (!best_left || (s64)(best_left->min_deadline - best->deadline) > 0)\n\t\treturn best;\n\n\t \n\tnode = &best_left->run_node;\n\twhile (node) {\n\t\tstruct sched_entity *se = __node_2_se(node);\n\n\t\t \n\t\tif (se->deadline == se->min_deadline)\n\t\t\treturn se;\n\n\t\t \n\t\tif (node->rb_left &&\n\t\t    __node_2_se(node->rb_left)->min_deadline == se->min_deadline) {\n\t\t\tnode = node->rb_left;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tnode = node->rb_right;\n\t}\n\treturn NULL;\n}\n\nstatic struct sched_entity *pick_eevdf(struct cfs_rq *cfs_rq)\n{\n\tstruct sched_entity *se = __pick_eevdf(cfs_rq);\n\n\tif (!se) {\n\t\tstruct sched_entity *left = __pick_first_entity(cfs_rq);\n\t\tif (left) {\n\t\t\tpr_err(\"EEVDF scheduling fail, picking leftmost\\n\");\n\t\t\treturn left;\n\t\t}\n\t}\n\n\treturn se;\n}\n\n#ifdef CONFIG_SCHED_DEBUG\nstruct sched_entity *__pick_last_entity(struct cfs_rq *cfs_rq)\n{\n\tstruct rb_node *last = rb_last(&cfs_rq->tasks_timeline.rb_root);\n\n\tif (!last)\n\t\treturn NULL;\n\n\treturn __node_2_se(last);\n}\n\n \n#ifdef CONFIG_SMP\nint sched_update_scaling(void)\n{\n\tunsigned int factor = get_update_sysctl_factor();\n\n#define WRT_SYSCTL(name) \\\n\t(normalized_sysctl_##name = sysctl_##name / (factor))\n\tWRT_SYSCTL(sched_base_slice);\n#undef WRT_SYSCTL\n\n\treturn 0;\n}\n#endif\n#endif\n\nstatic void clear_buddies(struct cfs_rq *cfs_rq, struct sched_entity *se);\n\n \nstatic void update_deadline(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tif ((s64)(se->vruntime - se->deadline) < 0)\n\t\treturn;\n\n\t \n\tse->slice = sysctl_sched_base_slice;\n\n\t \n\tse->deadline = se->vruntime + calc_delta_fair(se->slice, se);\n\n\t \n\tif (cfs_rq->nr_running > 1) {\n\t\tresched_curr(rq_of(cfs_rq));\n\t\tclear_buddies(cfs_rq, se);\n\t}\n}\n\n#include \"pelt.h\"\n#ifdef CONFIG_SMP\n\nstatic int select_idle_sibling(struct task_struct *p, int prev_cpu, int cpu);\nstatic unsigned long task_h_load(struct task_struct *p);\nstatic unsigned long capacity_of(int cpu);\n\n \nvoid init_entity_runnable_average(struct sched_entity *se)\n{\n\tstruct sched_avg *sa = &se->avg;\n\n\tmemset(sa, 0, sizeof(*sa));\n\n\t \n\tif (entity_is_task(se))\n\t\tsa->load_avg = scale_load_down(se->load.weight);\n\n\t \n}\n\n \nvoid post_init_entity_util_avg(struct task_struct *p)\n{\n\tstruct sched_entity *se = &p->se;\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\tstruct sched_avg *sa = &se->avg;\n\tlong cpu_scale = arch_scale_cpu_capacity(cpu_of(rq_of(cfs_rq)));\n\tlong cap = (long)(cpu_scale - cfs_rq->avg.util_avg) / 2;\n\n\tif (p->sched_class != &fair_sched_class) {\n\t\t \n\t\tse->avg.last_update_time = cfs_rq_clock_pelt(cfs_rq);\n\t\treturn;\n\t}\n\n\tif (cap > 0) {\n\t\tif (cfs_rq->avg.util_avg != 0) {\n\t\t\tsa->util_avg  = cfs_rq->avg.util_avg * se->load.weight;\n\t\t\tsa->util_avg /= (cfs_rq->avg.load_avg + 1);\n\n\t\t\tif (sa->util_avg > cap)\n\t\t\t\tsa->util_avg = cap;\n\t\t} else {\n\t\t\tsa->util_avg = cap;\n\t\t}\n\t}\n\n\tsa->runnable_avg = sa->util_avg;\n}\n\n#else  \nvoid init_entity_runnable_average(struct sched_entity *se)\n{\n}\nvoid post_init_entity_util_avg(struct task_struct *p)\n{\n}\nstatic void update_tg_load_avg(struct cfs_rq *cfs_rq)\n{\n}\n#endif  \n\n \nstatic void update_curr(struct cfs_rq *cfs_rq)\n{\n\tstruct sched_entity *curr = cfs_rq->curr;\n\tu64 now = rq_clock_task(rq_of(cfs_rq));\n\tu64 delta_exec;\n\n\tif (unlikely(!curr))\n\t\treturn;\n\n\tdelta_exec = now - curr->exec_start;\n\tif (unlikely((s64)delta_exec <= 0))\n\t\treturn;\n\n\tcurr->exec_start = now;\n\n\tif (schedstat_enabled()) {\n\t\tstruct sched_statistics *stats;\n\n\t\tstats = __schedstats_from_se(curr);\n\t\t__schedstat_set(stats->exec_max,\n\t\t\t\tmax(delta_exec, stats->exec_max));\n\t}\n\n\tcurr->sum_exec_runtime += delta_exec;\n\tschedstat_add(cfs_rq->exec_clock, delta_exec);\n\n\tcurr->vruntime += calc_delta_fair(delta_exec, curr);\n\tupdate_deadline(cfs_rq, curr);\n\tupdate_min_vruntime(cfs_rq);\n\n\tif (entity_is_task(curr)) {\n\t\tstruct task_struct *curtask = task_of(curr);\n\n\t\ttrace_sched_stat_runtime(curtask, delta_exec, curr->vruntime);\n\t\tcgroup_account_cputime(curtask, delta_exec);\n\t\taccount_group_exec_runtime(curtask, delta_exec);\n\t}\n\n\taccount_cfs_rq_runtime(cfs_rq, delta_exec);\n}\n\nstatic void update_curr_fair(struct rq *rq)\n{\n\tupdate_curr(cfs_rq_of(&rq->curr->se));\n}\n\nstatic inline void\nupdate_stats_wait_start_fair(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tstruct sched_statistics *stats;\n\tstruct task_struct *p = NULL;\n\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\tstats = __schedstats_from_se(se);\n\n\tif (entity_is_task(se))\n\t\tp = task_of(se);\n\n\t__update_stats_wait_start(rq_of(cfs_rq), p, stats);\n}\n\nstatic inline void\nupdate_stats_wait_end_fair(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tstruct sched_statistics *stats;\n\tstruct task_struct *p = NULL;\n\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\tstats = __schedstats_from_se(se);\n\n\t \n\tif (unlikely(!schedstat_val(stats->wait_start)))\n\t\treturn;\n\n\tif (entity_is_task(se))\n\t\tp = task_of(se);\n\n\t__update_stats_wait_end(rq_of(cfs_rq), p, stats);\n}\n\nstatic inline void\nupdate_stats_enqueue_sleeper_fair(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tstruct sched_statistics *stats;\n\tstruct task_struct *tsk = NULL;\n\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\tstats = __schedstats_from_se(se);\n\n\tif (entity_is_task(se))\n\t\ttsk = task_of(se);\n\n\t__update_stats_enqueue_sleeper(rq_of(cfs_rq), tsk, stats);\n}\n\n \nstatic inline void\nupdate_stats_enqueue_fair(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\t \n\tif (se != cfs_rq->curr)\n\t\tupdate_stats_wait_start_fair(cfs_rq, se);\n\n\tif (flags & ENQUEUE_WAKEUP)\n\t\tupdate_stats_enqueue_sleeper_fair(cfs_rq, se);\n}\n\nstatic inline void\nupdate_stats_dequeue_fair(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\t \n\tif (se != cfs_rq->curr)\n\t\tupdate_stats_wait_end_fair(cfs_rq, se);\n\n\tif ((flags & DEQUEUE_SLEEP) && entity_is_task(se)) {\n\t\tstruct task_struct *tsk = task_of(se);\n\t\tunsigned int state;\n\n\t\t \n\t\tstate = READ_ONCE(tsk->__state);\n\t\tif (state & TASK_INTERRUPTIBLE)\n\t\t\t__schedstat_set(tsk->stats.sleep_start,\n\t\t\t\t      rq_clock(rq_of(cfs_rq)));\n\t\tif (state & TASK_UNINTERRUPTIBLE)\n\t\t\t__schedstat_set(tsk->stats.block_start,\n\t\t\t\t      rq_clock(rq_of(cfs_rq)));\n\t}\n}\n\n \nstatic inline void\nupdate_stats_curr_start(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\t \n\tse->exec_start = rq_clock_task(rq_of(cfs_rq));\n}\n\n \n\nstatic inline bool is_core_idle(int cpu)\n{\n#ifdef CONFIG_SCHED_SMT\n\tint sibling;\n\n\tfor_each_cpu(sibling, cpu_smt_mask(cpu)) {\n\t\tif (cpu == sibling)\n\t\t\tcontinue;\n\n\t\tif (!idle_cpu(sibling))\n\t\t\treturn false;\n\t}\n#endif\n\n\treturn true;\n}\n\n#ifdef CONFIG_NUMA\n#define NUMA_IMBALANCE_MIN 2\n\nstatic inline long\nadjust_numa_imbalance(int imbalance, int dst_running, int imb_numa_nr)\n{\n\t \n\tif (dst_running > imb_numa_nr)\n\t\treturn imbalance;\n\n\t \n\tif (imbalance <= NUMA_IMBALANCE_MIN)\n\t\treturn 0;\n\n\treturn imbalance;\n}\n#endif  \n\n#ifdef CONFIG_NUMA_BALANCING\n \nunsigned int sysctl_numa_balancing_scan_period_min = 1000;\nunsigned int sysctl_numa_balancing_scan_period_max = 60000;\n\n \nunsigned int sysctl_numa_balancing_scan_size = 256;\n\n \nunsigned int sysctl_numa_balancing_scan_delay = 1000;\n\n \nunsigned int sysctl_numa_balancing_hot_threshold = MSEC_PER_SEC;\n\nstruct numa_group {\n\trefcount_t refcount;\n\n\tspinlock_t lock;  \n\tint nr_tasks;\n\tpid_t gid;\n\tint active_nodes;\n\n\tstruct rcu_head rcu;\n\tunsigned long total_faults;\n\tunsigned long max_faults_cpu;\n\t \n\tunsigned long faults[];\n};\n\n \nstatic struct numa_group *deref_task_numa_group(struct task_struct *p)\n{\n\treturn rcu_dereference_check(p->numa_group, p == current ||\n\t\t(lockdep_is_held(__rq_lockp(task_rq(p))) && !READ_ONCE(p->on_cpu)));\n}\n\nstatic struct numa_group *deref_curr_numa_group(struct task_struct *p)\n{\n\treturn rcu_dereference_protected(p->numa_group, p == current);\n}\n\nstatic inline unsigned long group_faults_priv(struct numa_group *ng);\nstatic inline unsigned long group_faults_shared(struct numa_group *ng);\n\nstatic unsigned int task_nr_scan_windows(struct task_struct *p)\n{\n\tunsigned long rss = 0;\n\tunsigned long nr_scan_pages;\n\n\t \n\tnr_scan_pages = sysctl_numa_balancing_scan_size << (20 - PAGE_SHIFT);\n\trss = get_mm_rss(p->mm);\n\tif (!rss)\n\t\trss = nr_scan_pages;\n\n\trss = round_up(rss, nr_scan_pages);\n\treturn rss / nr_scan_pages;\n}\n\n \n#define MAX_SCAN_WINDOW 2560\n\nstatic unsigned int task_scan_min(struct task_struct *p)\n{\n\tunsigned int scan_size = READ_ONCE(sysctl_numa_balancing_scan_size);\n\tunsigned int scan, floor;\n\tunsigned int windows = 1;\n\n\tif (scan_size < MAX_SCAN_WINDOW)\n\t\twindows = MAX_SCAN_WINDOW / scan_size;\n\tfloor = 1000 / windows;\n\n\tscan = sysctl_numa_balancing_scan_period_min / task_nr_scan_windows(p);\n\treturn max_t(unsigned int, floor, scan);\n}\n\nstatic unsigned int task_scan_start(struct task_struct *p)\n{\n\tunsigned long smin = task_scan_min(p);\n\tunsigned long period = smin;\n\tstruct numa_group *ng;\n\n\t \n\trcu_read_lock();\n\tng = rcu_dereference(p->numa_group);\n\tif (ng) {\n\t\tunsigned long shared = group_faults_shared(ng);\n\t\tunsigned long private = group_faults_priv(ng);\n\n\t\tperiod *= refcount_read(&ng->refcount);\n\t\tperiod *= shared + 1;\n\t\tperiod /= private + shared + 1;\n\t}\n\trcu_read_unlock();\n\n\treturn max(smin, period);\n}\n\nstatic unsigned int task_scan_max(struct task_struct *p)\n{\n\tunsigned long smin = task_scan_min(p);\n\tunsigned long smax;\n\tstruct numa_group *ng;\n\n\t \n\tsmax = sysctl_numa_balancing_scan_period_max / task_nr_scan_windows(p);\n\n\t \n\tng = deref_curr_numa_group(p);\n\tif (ng) {\n\t\tunsigned long shared = group_faults_shared(ng);\n\t\tunsigned long private = group_faults_priv(ng);\n\t\tunsigned long period = smax;\n\n\t\tperiod *= refcount_read(&ng->refcount);\n\t\tperiod *= shared + 1;\n\t\tperiod /= private + shared + 1;\n\n\t\tsmax = max(smax, period);\n\t}\n\n\treturn max(smin, smax);\n}\n\nstatic void account_numa_enqueue(struct rq *rq, struct task_struct *p)\n{\n\trq->nr_numa_running += (p->numa_preferred_nid != NUMA_NO_NODE);\n\trq->nr_preferred_running += (p->numa_preferred_nid == task_node(p));\n}\n\nstatic void account_numa_dequeue(struct rq *rq, struct task_struct *p)\n{\n\trq->nr_numa_running -= (p->numa_preferred_nid != NUMA_NO_NODE);\n\trq->nr_preferred_running -= (p->numa_preferred_nid == task_node(p));\n}\n\n \n#define NR_NUMA_HINT_FAULT_TYPES 2\n\n \n#define NR_NUMA_HINT_FAULT_STATS (NR_NUMA_HINT_FAULT_TYPES * 2)\n\n \n#define NR_NUMA_HINT_FAULT_BUCKETS (NR_NUMA_HINT_FAULT_STATS * 2)\n\npid_t task_numa_group_id(struct task_struct *p)\n{\n\tstruct numa_group *ng;\n\tpid_t gid = 0;\n\n\trcu_read_lock();\n\tng = rcu_dereference(p->numa_group);\n\tif (ng)\n\t\tgid = ng->gid;\n\trcu_read_unlock();\n\n\treturn gid;\n}\n\n \nstatic inline int task_faults_idx(enum numa_faults_stats s, int nid, int priv)\n{\n\treturn NR_NUMA_HINT_FAULT_TYPES * (s * nr_node_ids + nid) + priv;\n}\n\nstatic inline unsigned long task_faults(struct task_struct *p, int nid)\n{\n\tif (!p->numa_faults)\n\t\treturn 0;\n\n\treturn p->numa_faults[task_faults_idx(NUMA_MEM, nid, 0)] +\n\t\tp->numa_faults[task_faults_idx(NUMA_MEM, nid, 1)];\n}\n\nstatic inline unsigned long group_faults(struct task_struct *p, int nid)\n{\n\tstruct numa_group *ng = deref_task_numa_group(p);\n\n\tif (!ng)\n\t\treturn 0;\n\n\treturn ng->faults[task_faults_idx(NUMA_MEM, nid, 0)] +\n\t\tng->faults[task_faults_idx(NUMA_MEM, nid, 1)];\n}\n\nstatic inline unsigned long group_faults_cpu(struct numa_group *group, int nid)\n{\n\treturn group->faults[task_faults_idx(NUMA_CPU, nid, 0)] +\n\t\tgroup->faults[task_faults_idx(NUMA_CPU, nid, 1)];\n}\n\nstatic inline unsigned long group_faults_priv(struct numa_group *ng)\n{\n\tunsigned long faults = 0;\n\tint node;\n\n\tfor_each_online_node(node) {\n\t\tfaults += ng->faults[task_faults_idx(NUMA_MEM, node, 1)];\n\t}\n\n\treturn faults;\n}\n\nstatic inline unsigned long group_faults_shared(struct numa_group *ng)\n{\n\tunsigned long faults = 0;\n\tint node;\n\n\tfor_each_online_node(node) {\n\t\tfaults += ng->faults[task_faults_idx(NUMA_MEM, node, 0)];\n\t}\n\n\treturn faults;\n}\n\n \n#define ACTIVE_NODE_FRACTION 3\n\nstatic bool numa_is_active_node(int nid, struct numa_group *ng)\n{\n\treturn group_faults_cpu(ng, nid) * ACTIVE_NODE_FRACTION > ng->max_faults_cpu;\n}\n\n \nstatic unsigned long score_nearby_nodes(struct task_struct *p, int nid,\n\t\t\t\t\tint lim_dist, bool task)\n{\n\tunsigned long score = 0;\n\tint node, max_dist;\n\n\t \n\tif (sched_numa_topology_type == NUMA_DIRECT)\n\t\treturn 0;\n\n\t \n\tmax_dist = READ_ONCE(sched_max_numa_distance);\n\t \n\tfor_each_online_node(node) {\n\t\tunsigned long faults;\n\t\tint dist = node_distance(nid, node);\n\n\t\t \n\t\tif (dist >= max_dist || node == nid)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (sched_numa_topology_type == NUMA_BACKPLANE && dist >= lim_dist)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (task)\n\t\t\tfaults = task_faults(p, node);\n\t\telse\n\t\t\tfaults = group_faults(p, node);\n\n\t\t \n\t\tif (sched_numa_topology_type == NUMA_GLUELESS_MESH) {\n\t\t\tfaults *= (max_dist - dist);\n\t\t\tfaults /= (max_dist - LOCAL_DISTANCE);\n\t\t}\n\n\t\tscore += faults;\n\t}\n\n\treturn score;\n}\n\n \nstatic inline unsigned long task_weight(struct task_struct *p, int nid,\n\t\t\t\t\tint dist)\n{\n\tunsigned long faults, total_faults;\n\n\tif (!p->numa_faults)\n\t\treturn 0;\n\n\ttotal_faults = p->total_numa_faults;\n\n\tif (!total_faults)\n\t\treturn 0;\n\n\tfaults = task_faults(p, nid);\n\tfaults += score_nearby_nodes(p, nid, dist, true);\n\n\treturn 1000 * faults / total_faults;\n}\n\nstatic inline unsigned long group_weight(struct task_struct *p, int nid,\n\t\t\t\t\t int dist)\n{\n\tstruct numa_group *ng = deref_task_numa_group(p);\n\tunsigned long faults, total_faults;\n\n\tif (!ng)\n\t\treturn 0;\n\n\ttotal_faults = ng->total_faults;\n\n\tif (!total_faults)\n\t\treturn 0;\n\n\tfaults = group_faults(p, nid);\n\tfaults += score_nearby_nodes(p, nid, dist, false);\n\n\treturn 1000 * faults / total_faults;\n}\n\n \nstatic inline bool cpupid_valid(int cpupid)\n{\n\treturn cpupid_to_cpu(cpupid) < nr_cpu_ids;\n}\n\n \nstatic bool pgdat_free_space_enough(struct pglist_data *pgdat)\n{\n\tint z;\n\tunsigned long enough_wmark;\n\n\tenough_wmark = max(1UL * 1024 * 1024 * 1024 >> PAGE_SHIFT,\n\t\t\t   pgdat->node_present_pages >> 4);\n\tfor (z = pgdat->nr_zones - 1; z >= 0; z--) {\n\t\tstruct zone *zone = pgdat->node_zones + z;\n\n\t\tif (!populated_zone(zone))\n\t\t\tcontinue;\n\n\t\tif (zone_watermark_ok(zone, 0,\n\t\t\t\t      wmark_pages(zone, WMARK_PROMO) + enough_wmark,\n\t\t\t\t      ZONE_MOVABLE, 0))\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\n \nstatic int numa_hint_fault_latency(struct page *page)\n{\n\tint last_time, time;\n\n\ttime = jiffies_to_msecs(jiffies);\n\tlast_time = xchg_page_access_time(page, time);\n\n\treturn (time - last_time) & PAGE_ACCESS_TIME_MASK;\n}\n\n \nstatic bool numa_promotion_rate_limit(struct pglist_data *pgdat,\n\t\t\t\t      unsigned long rate_limit, int nr)\n{\n\tunsigned long nr_cand;\n\tunsigned int now, start;\n\n\tnow = jiffies_to_msecs(jiffies);\n\tmod_node_page_state(pgdat, PGPROMOTE_CANDIDATE, nr);\n\tnr_cand = node_page_state(pgdat, PGPROMOTE_CANDIDATE);\n\tstart = pgdat->nbp_rl_start;\n\tif (now - start > MSEC_PER_SEC &&\n\t    cmpxchg(&pgdat->nbp_rl_start, start, now) == start)\n\t\tpgdat->nbp_rl_nr_cand = nr_cand;\n\tif (nr_cand - pgdat->nbp_rl_nr_cand >= rate_limit)\n\t\treturn true;\n\treturn false;\n}\n\n#define NUMA_MIGRATION_ADJUST_STEPS\t16\n\nstatic void numa_promotion_adjust_threshold(struct pglist_data *pgdat,\n\t\t\t\t\t    unsigned long rate_limit,\n\t\t\t\t\t    unsigned int ref_th)\n{\n\tunsigned int now, start, th_period, unit_th, th;\n\tunsigned long nr_cand, ref_cand, diff_cand;\n\n\tnow = jiffies_to_msecs(jiffies);\n\tth_period = sysctl_numa_balancing_scan_period_max;\n\tstart = pgdat->nbp_th_start;\n\tif (now - start > th_period &&\n\t    cmpxchg(&pgdat->nbp_th_start, start, now) == start) {\n\t\tref_cand = rate_limit *\n\t\t\tsysctl_numa_balancing_scan_period_max / MSEC_PER_SEC;\n\t\tnr_cand = node_page_state(pgdat, PGPROMOTE_CANDIDATE);\n\t\tdiff_cand = nr_cand - pgdat->nbp_th_nr_cand;\n\t\tunit_th = ref_th * 2 / NUMA_MIGRATION_ADJUST_STEPS;\n\t\tth = pgdat->nbp_threshold ? : ref_th;\n\t\tif (diff_cand > ref_cand * 11 / 10)\n\t\t\tth = max(th - unit_th, unit_th);\n\t\telse if (diff_cand < ref_cand * 9 / 10)\n\t\t\tth = min(th + unit_th, ref_th * 2);\n\t\tpgdat->nbp_th_nr_cand = nr_cand;\n\t\tpgdat->nbp_threshold = th;\n\t}\n}\n\nbool should_numa_migrate_memory(struct task_struct *p, struct page * page,\n\t\t\t\tint src_nid, int dst_cpu)\n{\n\tstruct numa_group *ng = deref_curr_numa_group(p);\n\tint dst_nid = cpu_to_node(dst_cpu);\n\tint last_cpupid, this_cpupid;\n\n\t \n\tif (sysctl_numa_balancing_mode & NUMA_BALANCING_MEMORY_TIERING &&\n\t    !node_is_toptier(src_nid)) {\n\t\tstruct pglist_data *pgdat;\n\t\tunsigned long rate_limit;\n\t\tunsigned int latency, th, def_th;\n\n\t\tpgdat = NODE_DATA(dst_nid);\n\t\tif (pgdat_free_space_enough(pgdat)) {\n\t\t\t \n\t\t\tpgdat->nbp_threshold = 0;\n\t\t\treturn true;\n\t\t}\n\n\t\tdef_th = sysctl_numa_balancing_hot_threshold;\n\t\trate_limit = sysctl_numa_balancing_promote_rate_limit << \\\n\t\t\t(20 - PAGE_SHIFT);\n\t\tnuma_promotion_adjust_threshold(pgdat, rate_limit, def_th);\n\n\t\tth = pgdat->nbp_threshold ? : def_th;\n\t\tlatency = numa_hint_fault_latency(page);\n\t\tif (latency >= th)\n\t\t\treturn false;\n\n\t\treturn !numa_promotion_rate_limit(pgdat, rate_limit,\n\t\t\t\t\t\t  thp_nr_pages(page));\n\t}\n\n\tthis_cpupid = cpu_pid_to_cpupid(dst_cpu, current->pid);\n\tlast_cpupid = page_cpupid_xchg_last(page, this_cpupid);\n\n\tif (!(sysctl_numa_balancing_mode & NUMA_BALANCING_MEMORY_TIERING) &&\n\t    !node_is_toptier(src_nid) && !cpupid_valid(last_cpupid))\n\t\treturn false;\n\n\t \n\tif ((p->numa_preferred_nid == NUMA_NO_NODE || p->numa_scan_seq <= 4) &&\n\t    (cpupid_pid_unset(last_cpupid) || cpupid_match_pid(p, last_cpupid)))\n\t\treturn true;\n\n\t \n\tif (!cpupid_pid_unset(last_cpupid) &&\n\t\t\t\tcpupid_to_nid(last_cpupid) != dst_nid)\n\t\treturn false;\n\n\t \n\tif (cpupid_match_pid(p, last_cpupid))\n\t\treturn true;\n\n\t \n\tif (!ng)\n\t\treturn true;\n\n\t \n\tif (group_faults_cpu(ng, dst_nid) > group_faults_cpu(ng, src_nid) *\n\t\t\t\t\tACTIVE_NODE_FRACTION)\n\t\treturn true;\n\n\t \n\treturn group_faults_cpu(ng, dst_nid) * group_faults(p, src_nid) * 3 >\n\t       group_faults_cpu(ng, src_nid) * group_faults(p, dst_nid) * 4;\n}\n\n \nenum numa_type {\n\t \n\tnode_has_spare = 0,\n\t \n\tnode_fully_busy,\n\t \n\tnode_overloaded\n};\n\n \nstruct numa_stats {\n\tunsigned long load;\n\tunsigned long runnable;\n\tunsigned long util;\n\t \n\tunsigned long compute_capacity;\n\tunsigned int nr_running;\n\tunsigned int weight;\n\tenum numa_type node_type;\n\tint idle_cpu;\n};\n\nstruct task_numa_env {\n\tstruct task_struct *p;\n\n\tint src_cpu, src_nid;\n\tint dst_cpu, dst_nid;\n\tint imb_numa_nr;\n\n\tstruct numa_stats src_stats, dst_stats;\n\n\tint imbalance_pct;\n\tint dist;\n\n\tstruct task_struct *best_task;\n\tlong best_imp;\n\tint best_cpu;\n};\n\nstatic unsigned long cpu_load(struct rq *rq);\nstatic unsigned long cpu_runnable(struct rq *rq);\n\nstatic inline enum\nnuma_type numa_classify(unsigned int imbalance_pct,\n\t\t\t struct numa_stats *ns)\n{\n\tif ((ns->nr_running > ns->weight) &&\n\t    (((ns->compute_capacity * 100) < (ns->util * imbalance_pct)) ||\n\t     ((ns->compute_capacity * imbalance_pct) < (ns->runnable * 100))))\n\t\treturn node_overloaded;\n\n\tif ((ns->nr_running < ns->weight) ||\n\t    (((ns->compute_capacity * 100) > (ns->util * imbalance_pct)) &&\n\t     ((ns->compute_capacity * imbalance_pct) > (ns->runnable * 100))))\n\t\treturn node_has_spare;\n\n\treturn node_fully_busy;\n}\n\n#ifdef CONFIG_SCHED_SMT\n \nstatic inline bool test_idle_cores(int cpu);\nstatic inline int numa_idle_core(int idle_core, int cpu)\n{\n\tif (!static_branch_likely(&sched_smt_present) ||\n\t    idle_core >= 0 || !test_idle_cores(cpu))\n\t\treturn idle_core;\n\n\t \n\tif (is_core_idle(cpu))\n\t\tidle_core = cpu;\n\n\treturn idle_core;\n}\n#else\nstatic inline int numa_idle_core(int idle_core, int cpu)\n{\n\treturn idle_core;\n}\n#endif\n\n \nstatic void update_numa_stats(struct task_numa_env *env,\n\t\t\t      struct numa_stats *ns, int nid,\n\t\t\t      bool find_idle)\n{\n\tint cpu, idle_core = -1;\n\n\tmemset(ns, 0, sizeof(*ns));\n\tns->idle_cpu = -1;\n\n\trcu_read_lock();\n\tfor_each_cpu(cpu, cpumask_of_node(nid)) {\n\t\tstruct rq *rq = cpu_rq(cpu);\n\n\t\tns->load += cpu_load(rq);\n\t\tns->runnable += cpu_runnable(rq);\n\t\tns->util += cpu_util_cfs(cpu);\n\t\tns->nr_running += rq->cfs.h_nr_running;\n\t\tns->compute_capacity += capacity_of(cpu);\n\n\t\tif (find_idle && idle_core < 0 && !rq->nr_running && idle_cpu(cpu)) {\n\t\t\tif (READ_ONCE(rq->numa_migrate_on) ||\n\t\t\t    !cpumask_test_cpu(cpu, env->p->cpus_ptr))\n\t\t\t\tcontinue;\n\n\t\t\tif (ns->idle_cpu == -1)\n\t\t\t\tns->idle_cpu = cpu;\n\n\t\t\tidle_core = numa_idle_core(idle_core, cpu);\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\tns->weight = cpumask_weight(cpumask_of_node(nid));\n\n\tns->node_type = numa_classify(env->imbalance_pct, ns);\n\n\tif (idle_core >= 0)\n\t\tns->idle_cpu = idle_core;\n}\n\nstatic void task_numa_assign(struct task_numa_env *env,\n\t\t\t     struct task_struct *p, long imp)\n{\n\tstruct rq *rq = cpu_rq(env->dst_cpu);\n\n\t \n\tif (env->best_cpu != env->dst_cpu && xchg(&rq->numa_migrate_on, 1)) {\n\t\tint cpu;\n\t\tint start = env->dst_cpu;\n\n\t\t \n\t\tfor_each_cpu_wrap(cpu, cpumask_of_node(env->dst_nid), start + 1) {\n\t\t\tif (cpu == env->best_cpu || !idle_cpu(cpu) ||\n\t\t\t    !cpumask_test_cpu(cpu, env->p->cpus_ptr)) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tenv->dst_cpu = cpu;\n\t\t\trq = cpu_rq(env->dst_cpu);\n\t\t\tif (!xchg(&rq->numa_migrate_on, 1))\n\t\t\t\tgoto assign;\n\t\t}\n\n\t\t \n\t\treturn;\n\t}\n\nassign:\n\t \n\tif (env->best_cpu != -1 && env->best_cpu != env->dst_cpu) {\n\t\trq = cpu_rq(env->best_cpu);\n\t\tWRITE_ONCE(rq->numa_migrate_on, 0);\n\t}\n\n\tif (env->best_task)\n\t\tput_task_struct(env->best_task);\n\tif (p)\n\t\tget_task_struct(p);\n\n\tenv->best_task = p;\n\tenv->best_imp = imp;\n\tenv->best_cpu = env->dst_cpu;\n}\n\nstatic bool load_too_imbalanced(long src_load, long dst_load,\n\t\t\t\tstruct task_numa_env *env)\n{\n\tlong imb, old_imb;\n\tlong orig_src_load, orig_dst_load;\n\tlong src_capacity, dst_capacity;\n\n\t \n\tsrc_capacity = env->src_stats.compute_capacity;\n\tdst_capacity = env->dst_stats.compute_capacity;\n\n\timb = abs(dst_load * src_capacity - src_load * dst_capacity);\n\n\torig_src_load = env->src_stats.load;\n\torig_dst_load = env->dst_stats.load;\n\n\told_imb = abs(orig_dst_load * src_capacity - orig_src_load * dst_capacity);\n\n\t \n\treturn (imb > old_imb);\n}\n\n \n#define SMALLIMP\t30\n\n \nstatic bool task_numa_compare(struct task_numa_env *env,\n\t\t\t      long taskimp, long groupimp, bool maymove)\n{\n\tstruct numa_group *cur_ng, *p_ng = deref_curr_numa_group(env->p);\n\tstruct rq *dst_rq = cpu_rq(env->dst_cpu);\n\tlong imp = p_ng ? groupimp : taskimp;\n\tstruct task_struct *cur;\n\tlong src_load, dst_load;\n\tint dist = env->dist;\n\tlong moveimp = imp;\n\tlong load;\n\tbool stopsearch = false;\n\n\tif (READ_ONCE(dst_rq->numa_migrate_on))\n\t\treturn false;\n\n\trcu_read_lock();\n\tcur = rcu_dereference(dst_rq->curr);\n\tif (cur && ((cur->flags & PF_EXITING) || is_idle_task(cur)))\n\t\tcur = NULL;\n\n\t \n\tif (cur == env->p) {\n\t\tstopsearch = true;\n\t\tgoto unlock;\n\t}\n\n\tif (!cur) {\n\t\tif (maymove && moveimp >= env->best_imp)\n\t\t\tgoto assign;\n\t\telse\n\t\t\tgoto unlock;\n\t}\n\n\t \n\tif (!cpumask_test_cpu(env->src_cpu, cur->cpus_ptr))\n\t\tgoto unlock;\n\n\t \n\tif (env->best_task &&\n\t    env->best_task->numa_preferred_nid == env->src_nid &&\n\t    cur->numa_preferred_nid != env->src_nid) {\n\t\tgoto unlock;\n\t}\n\n\t \n\tcur_ng = rcu_dereference(cur->numa_group);\n\tif (cur_ng == p_ng) {\n\t\t \n\t\tif (env->dst_stats.node_type == node_has_spare)\n\t\t\tgoto unlock;\n\n\t\timp = taskimp + task_weight(cur, env->src_nid, dist) -\n\t\t      task_weight(cur, env->dst_nid, dist);\n\t\t \n\t\tif (cur_ng)\n\t\t\timp -= imp / 16;\n\t} else {\n\t\t \n\t\tif (cur_ng && p_ng)\n\t\t\timp += group_weight(cur, env->src_nid, dist) -\n\t\t\t       group_weight(cur, env->dst_nid, dist);\n\t\telse\n\t\t\timp += task_weight(cur, env->src_nid, dist) -\n\t\t\t       task_weight(cur, env->dst_nid, dist);\n\t}\n\n\t \n\tif (cur->numa_preferred_nid == env->dst_nid)\n\t\timp -= imp / 16;\n\n\t \n\tif (cur->numa_preferred_nid == env->src_nid)\n\t\timp += imp / 8;\n\n\tif (maymove && moveimp > imp && moveimp > env->best_imp) {\n\t\timp = moveimp;\n\t\tcur = NULL;\n\t\tgoto assign;\n\t}\n\n\t \n\tif (env->best_task && cur->numa_preferred_nid == env->src_nid &&\n\t    env->best_task->numa_preferred_nid != env->src_nid) {\n\t\tgoto assign;\n\t}\n\n\t \n\tif (imp < SMALLIMP || imp <= env->best_imp + SMALLIMP / 2)\n\t\tgoto unlock;\n\n\t \n\tload = task_h_load(env->p) - task_h_load(cur);\n\tif (!load)\n\t\tgoto assign;\n\n\tdst_load = env->dst_stats.load + load;\n\tsrc_load = env->src_stats.load - load;\n\n\tif (load_too_imbalanced(src_load, dst_load, env))\n\t\tgoto unlock;\n\nassign:\n\t \n\tif (!cur) {\n\t\tint cpu = env->dst_stats.idle_cpu;\n\n\t\t \n\t\tif (cpu < 0)\n\t\t\tcpu = env->dst_cpu;\n\n\t\t \n\t\tif (!idle_cpu(cpu) && env->best_cpu >= 0 &&\n\t\t    idle_cpu(env->best_cpu)) {\n\t\t\tcpu = env->best_cpu;\n\t\t}\n\n\t\tenv->dst_cpu = cpu;\n\t}\n\n\ttask_numa_assign(env, cur, imp);\n\n\t \n\tif (maymove && !cur && env->best_cpu >= 0 && idle_cpu(env->best_cpu))\n\t\tstopsearch = true;\n\n\t \n\tif (!maymove && env->best_task &&\n\t    env->best_task->numa_preferred_nid == env->src_nid) {\n\t\tstopsearch = true;\n\t}\nunlock:\n\trcu_read_unlock();\n\n\treturn stopsearch;\n}\n\nstatic void task_numa_find_cpu(struct task_numa_env *env,\n\t\t\t\tlong taskimp, long groupimp)\n{\n\tbool maymove = false;\n\tint cpu;\n\n\t \n\tif (env->dst_stats.node_type == node_has_spare) {\n\t\tunsigned int imbalance;\n\t\tint src_running, dst_running;\n\n\t\t \n\t\tsrc_running = env->src_stats.nr_running - 1;\n\t\tdst_running = env->dst_stats.nr_running + 1;\n\t\timbalance = max(0, dst_running - src_running);\n\t\timbalance = adjust_numa_imbalance(imbalance, dst_running,\n\t\t\t\t\t\t  env->imb_numa_nr);\n\n\t\t \n\t\tif (!imbalance) {\n\t\t\tmaymove = true;\n\t\t\tif (env->dst_stats.idle_cpu >= 0) {\n\t\t\t\tenv->dst_cpu = env->dst_stats.idle_cpu;\n\t\t\t\ttask_numa_assign(env, NULL, 0);\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tlong src_load, dst_load, load;\n\t\t \n\t\tload = task_h_load(env->p);\n\t\tdst_load = env->dst_stats.load + load;\n\t\tsrc_load = env->src_stats.load - load;\n\t\tmaymove = !load_too_imbalanced(src_load, dst_load, env);\n\t}\n\n\tfor_each_cpu(cpu, cpumask_of_node(env->dst_nid)) {\n\t\t \n\t\tif (!cpumask_test_cpu(cpu, env->p->cpus_ptr))\n\t\t\tcontinue;\n\n\t\tenv->dst_cpu = cpu;\n\t\tif (task_numa_compare(env, taskimp, groupimp, maymove))\n\t\t\tbreak;\n\t}\n}\n\nstatic int task_numa_migrate(struct task_struct *p)\n{\n\tstruct task_numa_env env = {\n\t\t.p = p,\n\n\t\t.src_cpu = task_cpu(p),\n\t\t.src_nid = task_node(p),\n\n\t\t.imbalance_pct = 112,\n\n\t\t.best_task = NULL,\n\t\t.best_imp = 0,\n\t\t.best_cpu = -1,\n\t};\n\tunsigned long taskweight, groupweight;\n\tstruct sched_domain *sd;\n\tlong taskimp, groupimp;\n\tstruct numa_group *ng;\n\tstruct rq *best_rq;\n\tint nid, ret, dist;\n\n\t \n\trcu_read_lock();\n\tsd = rcu_dereference(per_cpu(sd_numa, env.src_cpu));\n\tif (sd) {\n\t\tenv.imbalance_pct = 100 + (sd->imbalance_pct - 100) / 2;\n\t\tenv.imb_numa_nr = sd->imb_numa_nr;\n\t}\n\trcu_read_unlock();\n\n\t \n\tif (unlikely(!sd)) {\n\t\tsched_setnuma(p, task_node(p));\n\t\treturn -EINVAL;\n\t}\n\n\tenv.dst_nid = p->numa_preferred_nid;\n\tdist = env.dist = node_distance(env.src_nid, env.dst_nid);\n\ttaskweight = task_weight(p, env.src_nid, dist);\n\tgroupweight = group_weight(p, env.src_nid, dist);\n\tupdate_numa_stats(&env, &env.src_stats, env.src_nid, false);\n\ttaskimp = task_weight(p, env.dst_nid, dist) - taskweight;\n\tgroupimp = group_weight(p, env.dst_nid, dist) - groupweight;\n\tupdate_numa_stats(&env, &env.dst_stats, env.dst_nid, true);\n\n\t \n\ttask_numa_find_cpu(&env, taskimp, groupimp);\n\n\t \n\tng = deref_curr_numa_group(p);\n\tif (env.best_cpu == -1 || (ng && ng->active_nodes > 1)) {\n\t\tfor_each_node_state(nid, N_CPU) {\n\t\t\tif (nid == env.src_nid || nid == p->numa_preferred_nid)\n\t\t\t\tcontinue;\n\n\t\t\tdist = node_distance(env.src_nid, env.dst_nid);\n\t\t\tif (sched_numa_topology_type == NUMA_BACKPLANE &&\n\t\t\t\t\t\tdist != env.dist) {\n\t\t\t\ttaskweight = task_weight(p, env.src_nid, dist);\n\t\t\t\tgroupweight = group_weight(p, env.src_nid, dist);\n\t\t\t}\n\n\t\t\t \n\t\t\ttaskimp = task_weight(p, nid, dist) - taskweight;\n\t\t\tgroupimp = group_weight(p, nid, dist) - groupweight;\n\t\t\tif (taskimp < 0 && groupimp < 0)\n\t\t\t\tcontinue;\n\n\t\t\tenv.dist = dist;\n\t\t\tenv.dst_nid = nid;\n\t\t\tupdate_numa_stats(&env, &env.dst_stats, env.dst_nid, true);\n\t\t\ttask_numa_find_cpu(&env, taskimp, groupimp);\n\t\t}\n\t}\n\n\t \n\tif (ng) {\n\t\tif (env.best_cpu == -1)\n\t\t\tnid = env.src_nid;\n\t\telse\n\t\t\tnid = cpu_to_node(env.best_cpu);\n\n\t\tif (nid != p->numa_preferred_nid)\n\t\t\tsched_setnuma(p, nid);\n\t}\n\n\t \n\tif (env.best_cpu == -1) {\n\t\ttrace_sched_stick_numa(p, env.src_cpu, NULL, -1);\n\t\treturn -EAGAIN;\n\t}\n\n\tbest_rq = cpu_rq(env.best_cpu);\n\tif (env.best_task == NULL) {\n\t\tret = migrate_task_to(p, env.best_cpu);\n\t\tWRITE_ONCE(best_rq->numa_migrate_on, 0);\n\t\tif (ret != 0)\n\t\t\ttrace_sched_stick_numa(p, env.src_cpu, NULL, env.best_cpu);\n\t\treturn ret;\n\t}\n\n\tret = migrate_swap(p, env.best_task, env.best_cpu, env.src_cpu);\n\tWRITE_ONCE(best_rq->numa_migrate_on, 0);\n\n\tif (ret != 0)\n\t\ttrace_sched_stick_numa(p, env.src_cpu, env.best_task, env.best_cpu);\n\tput_task_struct(env.best_task);\n\treturn ret;\n}\n\n \nstatic void numa_migrate_preferred(struct task_struct *p)\n{\n\tunsigned long interval = HZ;\n\n\t \n\tif (unlikely(p->numa_preferred_nid == NUMA_NO_NODE || !p->numa_faults))\n\t\treturn;\n\n\t \n\tinterval = min(interval, msecs_to_jiffies(p->numa_scan_period) / 16);\n\tp->numa_migrate_retry = jiffies + interval;\n\n\t \n\tif (task_node(p) == p->numa_preferred_nid)\n\t\treturn;\n\n\t \n\ttask_numa_migrate(p);\n}\n\n \nstatic void numa_group_count_active_nodes(struct numa_group *numa_group)\n{\n\tunsigned long faults, max_faults = 0;\n\tint nid, active_nodes = 0;\n\n\tfor_each_node_state(nid, N_CPU) {\n\t\tfaults = group_faults_cpu(numa_group, nid);\n\t\tif (faults > max_faults)\n\t\t\tmax_faults = faults;\n\t}\n\n\tfor_each_node_state(nid, N_CPU) {\n\t\tfaults = group_faults_cpu(numa_group, nid);\n\t\tif (faults * ACTIVE_NODE_FRACTION > max_faults)\n\t\t\tactive_nodes++;\n\t}\n\n\tnuma_group->max_faults_cpu = max_faults;\n\tnuma_group->active_nodes = active_nodes;\n}\n\n \n#define NUMA_PERIOD_SLOTS 10\n#define NUMA_PERIOD_THRESHOLD 7\n\n \nstatic void update_task_scan_period(struct task_struct *p,\n\t\t\tunsigned long shared, unsigned long private)\n{\n\tunsigned int period_slot;\n\tint lr_ratio, ps_ratio;\n\tint diff;\n\n\tunsigned long remote = p->numa_faults_locality[0];\n\tunsigned long local = p->numa_faults_locality[1];\n\n\t \n\tif (local + shared == 0 || p->numa_faults_locality[2]) {\n\t\tp->numa_scan_period = min(p->numa_scan_period_max,\n\t\t\tp->numa_scan_period << 1);\n\n\t\tp->mm->numa_next_scan = jiffies +\n\t\t\tmsecs_to_jiffies(p->numa_scan_period);\n\n\t\treturn;\n\t}\n\n\t \n\tperiod_slot = DIV_ROUND_UP(p->numa_scan_period, NUMA_PERIOD_SLOTS);\n\tlr_ratio = (local * NUMA_PERIOD_SLOTS) / (local + remote);\n\tps_ratio = (private * NUMA_PERIOD_SLOTS) / (private + shared);\n\n\tif (ps_ratio >= NUMA_PERIOD_THRESHOLD) {\n\t\t \n\t\tint slot = ps_ratio - NUMA_PERIOD_THRESHOLD;\n\t\tif (!slot)\n\t\t\tslot = 1;\n\t\tdiff = slot * period_slot;\n\t} else if (lr_ratio >= NUMA_PERIOD_THRESHOLD) {\n\t\t \n\t\tint slot = lr_ratio - NUMA_PERIOD_THRESHOLD;\n\t\tif (!slot)\n\t\t\tslot = 1;\n\t\tdiff = slot * period_slot;\n\t} else {\n\t\t \n\t\tint ratio = max(lr_ratio, ps_ratio);\n\t\tdiff = -(NUMA_PERIOD_THRESHOLD - ratio) * period_slot;\n\t}\n\n\tp->numa_scan_period = clamp(p->numa_scan_period + diff,\n\t\t\ttask_scan_min(p), task_scan_max(p));\n\tmemset(p->numa_faults_locality, 0, sizeof(p->numa_faults_locality));\n}\n\n \nstatic u64 numa_get_avg_runtime(struct task_struct *p, u64 *period)\n{\n\tu64 runtime, delta, now;\n\t \n\tnow = p->se.exec_start;\n\truntime = p->se.sum_exec_runtime;\n\n\tif (p->last_task_numa_placement) {\n\t\tdelta = runtime - p->last_sum_exec_runtime;\n\t\t*period = now - p->last_task_numa_placement;\n\n\t\t \n\t\tif (unlikely((s64)*period < 0))\n\t\t\t*period = 0;\n\t} else {\n\t\tdelta = p->se.avg.load_sum;\n\t\t*period = LOAD_AVG_MAX;\n\t}\n\n\tp->last_sum_exec_runtime = runtime;\n\tp->last_task_numa_placement = now;\n\n\treturn delta;\n}\n\n \nstatic int preferred_group_nid(struct task_struct *p, int nid)\n{\n\tnodemask_t nodes;\n\tint dist;\n\n\t \n\tif (sched_numa_topology_type == NUMA_DIRECT)\n\t\treturn nid;\n\n\t \n\tif (sched_numa_topology_type == NUMA_GLUELESS_MESH) {\n\t\tunsigned long score, max_score = 0;\n\t\tint node, max_node = nid;\n\n\t\tdist = sched_max_numa_distance;\n\n\t\tfor_each_node_state(node, N_CPU) {\n\t\t\tscore = group_weight(p, node, dist);\n\t\t\tif (score > max_score) {\n\t\t\t\tmax_score = score;\n\t\t\t\tmax_node = node;\n\t\t\t}\n\t\t}\n\t\treturn max_node;\n\t}\n\n\t \n\tnodes = node_states[N_CPU];\n\tfor (dist = sched_max_numa_distance; dist > LOCAL_DISTANCE; dist--) {\n\t\tunsigned long max_faults = 0;\n\t\tnodemask_t max_group = NODE_MASK_NONE;\n\t\tint a, b;\n\n\t\t \n\t\tif (!find_numa_distance(dist))\n\t\t\tcontinue;\n\n\t\tfor_each_node_mask(a, nodes) {\n\t\t\tunsigned long faults = 0;\n\t\t\tnodemask_t this_group;\n\t\t\tnodes_clear(this_group);\n\n\t\t\t \n\t\t\tfor_each_node_mask(b, nodes) {\n\t\t\t\tif (node_distance(a, b) < dist) {\n\t\t\t\t\tfaults += group_faults(p, b);\n\t\t\t\t\tnode_set(b, this_group);\n\t\t\t\t\tnode_clear(b, nodes);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t \n\t\t\tif (faults > max_faults) {\n\t\t\t\tmax_faults = faults;\n\t\t\t\tmax_group = this_group;\n\t\t\t\t \n\t\t\t\tnid = a;\n\t\t\t}\n\t\t}\n\t\t \n\t\tif (!max_faults)\n\t\t\tbreak;\n\t\tnodes = max_group;\n\t}\n\treturn nid;\n}\n\nstatic void task_numa_placement(struct task_struct *p)\n{\n\tint seq, nid, max_nid = NUMA_NO_NODE;\n\tunsigned long max_faults = 0;\n\tunsigned long fault_types[2] = { 0, 0 };\n\tunsigned long total_faults;\n\tu64 runtime, period;\n\tspinlock_t *group_lock = NULL;\n\tstruct numa_group *ng;\n\n\t \n\tseq = READ_ONCE(p->mm->numa_scan_seq);\n\tif (p->numa_scan_seq == seq)\n\t\treturn;\n\tp->numa_scan_seq = seq;\n\tp->numa_scan_period_max = task_scan_max(p);\n\n\ttotal_faults = p->numa_faults_locality[0] +\n\t\t       p->numa_faults_locality[1];\n\truntime = numa_get_avg_runtime(p, &period);\n\n\t \n\tng = deref_curr_numa_group(p);\n\tif (ng) {\n\t\tgroup_lock = &ng->lock;\n\t\tspin_lock_irq(group_lock);\n\t}\n\n\t \n\tfor_each_online_node(nid) {\n\t\t \n\t\tint mem_idx, membuf_idx, cpu_idx, cpubuf_idx;\n\t\tunsigned long faults = 0, group_faults = 0;\n\t\tint priv;\n\n\t\tfor (priv = 0; priv < NR_NUMA_HINT_FAULT_TYPES; priv++) {\n\t\t\tlong diff, f_diff, f_weight;\n\n\t\t\tmem_idx = task_faults_idx(NUMA_MEM, nid, priv);\n\t\t\tmembuf_idx = task_faults_idx(NUMA_MEMBUF, nid, priv);\n\t\t\tcpu_idx = task_faults_idx(NUMA_CPU, nid, priv);\n\t\t\tcpubuf_idx = task_faults_idx(NUMA_CPUBUF, nid, priv);\n\n\t\t\t \n\t\t\tdiff = p->numa_faults[membuf_idx] - p->numa_faults[mem_idx] / 2;\n\t\t\tfault_types[priv] += p->numa_faults[membuf_idx];\n\t\t\tp->numa_faults[membuf_idx] = 0;\n\n\t\t\t \n\t\t\tf_weight = div64_u64(runtime << 16, period + 1);\n\t\t\tf_weight = (f_weight * p->numa_faults[cpubuf_idx]) /\n\t\t\t\t   (total_faults + 1);\n\t\t\tf_diff = f_weight - p->numa_faults[cpu_idx] / 2;\n\t\t\tp->numa_faults[cpubuf_idx] = 0;\n\n\t\t\tp->numa_faults[mem_idx] += diff;\n\t\t\tp->numa_faults[cpu_idx] += f_diff;\n\t\t\tfaults += p->numa_faults[mem_idx];\n\t\t\tp->total_numa_faults += diff;\n\t\t\tif (ng) {\n\t\t\t\t \n\t\t\t\tng->faults[mem_idx] += diff;\n\t\t\t\tng->faults[cpu_idx] += f_diff;\n\t\t\t\tng->total_faults += diff;\n\t\t\t\tgroup_faults += ng->faults[mem_idx];\n\t\t\t}\n\t\t}\n\n\t\tif (!ng) {\n\t\t\tif (faults > max_faults) {\n\t\t\t\tmax_faults = faults;\n\t\t\t\tmax_nid = nid;\n\t\t\t}\n\t\t} else if (group_faults > max_faults) {\n\t\t\tmax_faults = group_faults;\n\t\t\tmax_nid = nid;\n\t\t}\n\t}\n\n\t \n\tif (max_nid != NUMA_NO_NODE && !node_state(max_nid, N_CPU)) {\n\t\tint near_nid = max_nid;\n\t\tint distance, near_distance = INT_MAX;\n\n\t\tfor_each_node_state(nid, N_CPU) {\n\t\t\tdistance = node_distance(max_nid, nid);\n\t\t\tif (distance < near_distance) {\n\t\t\t\tnear_nid = nid;\n\t\t\t\tnear_distance = distance;\n\t\t\t}\n\t\t}\n\t\tmax_nid = near_nid;\n\t}\n\n\tif (ng) {\n\t\tnuma_group_count_active_nodes(ng);\n\t\tspin_unlock_irq(group_lock);\n\t\tmax_nid = preferred_group_nid(p, max_nid);\n\t}\n\n\tif (max_faults) {\n\t\t \n\t\tif (max_nid != p->numa_preferred_nid)\n\t\t\tsched_setnuma(p, max_nid);\n\t}\n\n\tupdate_task_scan_period(p, fault_types[0], fault_types[1]);\n}\n\nstatic inline int get_numa_group(struct numa_group *grp)\n{\n\treturn refcount_inc_not_zero(&grp->refcount);\n}\n\nstatic inline void put_numa_group(struct numa_group *grp)\n{\n\tif (refcount_dec_and_test(&grp->refcount))\n\t\tkfree_rcu(grp, rcu);\n}\n\nstatic void task_numa_group(struct task_struct *p, int cpupid, int flags,\n\t\t\tint *priv)\n{\n\tstruct numa_group *grp, *my_grp;\n\tstruct task_struct *tsk;\n\tbool join = false;\n\tint cpu = cpupid_to_cpu(cpupid);\n\tint i;\n\n\tif (unlikely(!deref_curr_numa_group(p))) {\n\t\tunsigned int size = sizeof(struct numa_group) +\n\t\t\t\t    NR_NUMA_HINT_FAULT_STATS *\n\t\t\t\t    nr_node_ids * sizeof(unsigned long);\n\n\t\tgrp = kzalloc(size, GFP_KERNEL | __GFP_NOWARN);\n\t\tif (!grp)\n\t\t\treturn;\n\n\t\trefcount_set(&grp->refcount, 1);\n\t\tgrp->active_nodes = 1;\n\t\tgrp->max_faults_cpu = 0;\n\t\tspin_lock_init(&grp->lock);\n\t\tgrp->gid = p->pid;\n\n\t\tfor (i = 0; i < NR_NUMA_HINT_FAULT_STATS * nr_node_ids; i++)\n\t\t\tgrp->faults[i] = p->numa_faults[i];\n\n\t\tgrp->total_faults = p->total_numa_faults;\n\n\t\tgrp->nr_tasks++;\n\t\trcu_assign_pointer(p->numa_group, grp);\n\t}\n\n\trcu_read_lock();\n\ttsk = READ_ONCE(cpu_rq(cpu)->curr);\n\n\tif (!cpupid_match_pid(tsk, cpupid))\n\t\tgoto no_join;\n\n\tgrp = rcu_dereference(tsk->numa_group);\n\tif (!grp)\n\t\tgoto no_join;\n\n\tmy_grp = deref_curr_numa_group(p);\n\tif (grp == my_grp)\n\t\tgoto no_join;\n\n\t \n\tif (my_grp->nr_tasks > grp->nr_tasks)\n\t\tgoto no_join;\n\n\t \n\tif (my_grp->nr_tasks == grp->nr_tasks && my_grp > grp)\n\t\tgoto no_join;\n\n\t \n\tif (tsk->mm == current->mm)\n\t\tjoin = true;\n\n\t \n\tif (flags & TNF_SHARED)\n\t\tjoin = true;\n\n\t \n\t*priv = !join;\n\n\tif (join && !get_numa_group(grp))\n\t\tgoto no_join;\n\n\trcu_read_unlock();\n\n\tif (!join)\n\t\treturn;\n\n\tWARN_ON_ONCE(irqs_disabled());\n\tdouble_lock_irq(&my_grp->lock, &grp->lock);\n\n\tfor (i = 0; i < NR_NUMA_HINT_FAULT_STATS * nr_node_ids; i++) {\n\t\tmy_grp->faults[i] -= p->numa_faults[i];\n\t\tgrp->faults[i] += p->numa_faults[i];\n\t}\n\tmy_grp->total_faults -= p->total_numa_faults;\n\tgrp->total_faults += p->total_numa_faults;\n\n\tmy_grp->nr_tasks--;\n\tgrp->nr_tasks++;\n\n\tspin_unlock(&my_grp->lock);\n\tspin_unlock_irq(&grp->lock);\n\n\trcu_assign_pointer(p->numa_group, grp);\n\n\tput_numa_group(my_grp);\n\treturn;\n\nno_join:\n\trcu_read_unlock();\n\treturn;\n}\n\n \nvoid task_numa_free(struct task_struct *p, bool final)\n{\n\t \n\tstruct numa_group *grp = rcu_dereference_raw(p->numa_group);\n\tunsigned long *numa_faults = p->numa_faults;\n\tunsigned long flags;\n\tint i;\n\n\tif (!numa_faults)\n\t\treturn;\n\n\tif (grp) {\n\t\tspin_lock_irqsave(&grp->lock, flags);\n\t\tfor (i = 0; i < NR_NUMA_HINT_FAULT_STATS * nr_node_ids; i++)\n\t\t\tgrp->faults[i] -= p->numa_faults[i];\n\t\tgrp->total_faults -= p->total_numa_faults;\n\n\t\tgrp->nr_tasks--;\n\t\tspin_unlock_irqrestore(&grp->lock, flags);\n\t\tRCU_INIT_POINTER(p->numa_group, NULL);\n\t\tput_numa_group(grp);\n\t}\n\n\tif (final) {\n\t\tp->numa_faults = NULL;\n\t\tkfree(numa_faults);\n\t} else {\n\t\tp->total_numa_faults = 0;\n\t\tfor (i = 0; i < NR_NUMA_HINT_FAULT_STATS * nr_node_ids; i++)\n\t\t\tnuma_faults[i] = 0;\n\t}\n}\n\n \nvoid task_numa_fault(int last_cpupid, int mem_node, int pages, int flags)\n{\n\tstruct task_struct *p = current;\n\tbool migrated = flags & TNF_MIGRATED;\n\tint cpu_node = task_node(current);\n\tint local = !!(flags & TNF_FAULT_LOCAL);\n\tstruct numa_group *ng;\n\tint priv;\n\n\tif (!static_branch_likely(&sched_numa_balancing))\n\t\treturn;\n\n\t \n\tif (!p->mm)\n\t\treturn;\n\n\t \n\tif (!node_is_toptier(mem_node) &&\n\t    (sysctl_numa_balancing_mode & NUMA_BALANCING_MEMORY_TIERING ||\n\t     !cpupid_valid(last_cpupid)))\n\t\treturn;\n\n\t \n\tif (unlikely(!p->numa_faults)) {\n\t\tint size = sizeof(*p->numa_faults) *\n\t\t\t   NR_NUMA_HINT_FAULT_BUCKETS * nr_node_ids;\n\n\t\tp->numa_faults = kzalloc(size, GFP_KERNEL|__GFP_NOWARN);\n\t\tif (!p->numa_faults)\n\t\t\treturn;\n\n\t\tp->total_numa_faults = 0;\n\t\tmemset(p->numa_faults_locality, 0, sizeof(p->numa_faults_locality));\n\t}\n\n\t \n\tif (unlikely(last_cpupid == (-1 & LAST_CPUPID_MASK))) {\n\t\tpriv = 1;\n\t} else {\n\t\tpriv = cpupid_match_pid(p, last_cpupid);\n\t\tif (!priv && !(flags & TNF_NO_GROUP))\n\t\t\ttask_numa_group(p, last_cpupid, flags, &priv);\n\t}\n\n\t \n\tng = deref_curr_numa_group(p);\n\tif (!priv && !local && ng && ng->active_nodes > 1 &&\n\t\t\t\tnuma_is_active_node(cpu_node, ng) &&\n\t\t\t\tnuma_is_active_node(mem_node, ng))\n\t\tlocal = 1;\n\n\t \n\tif (time_after(jiffies, p->numa_migrate_retry)) {\n\t\ttask_numa_placement(p);\n\t\tnuma_migrate_preferred(p);\n\t}\n\n\tif (migrated)\n\t\tp->numa_pages_migrated += pages;\n\tif (flags & TNF_MIGRATE_FAIL)\n\t\tp->numa_faults_locality[2] += pages;\n\n\tp->numa_faults[task_faults_idx(NUMA_MEMBUF, mem_node, priv)] += pages;\n\tp->numa_faults[task_faults_idx(NUMA_CPUBUF, cpu_node, priv)] += pages;\n\tp->numa_faults_locality[local] += pages;\n}\n\nstatic void reset_ptenuma_scan(struct task_struct *p)\n{\n\t \n\tWRITE_ONCE(p->mm->numa_scan_seq, READ_ONCE(p->mm->numa_scan_seq) + 1);\n\tp->mm->numa_scan_offset = 0;\n}\n\nstatic bool vma_is_accessed(struct vm_area_struct *vma)\n{\n\tunsigned long pids;\n\t \n\tif (READ_ONCE(current->mm->numa_scan_seq) < 2)\n\t\treturn true;\n\n\tpids = vma->numab_state->access_pids[0] | vma->numab_state->access_pids[1];\n\treturn test_bit(hash_32(current->pid, ilog2(BITS_PER_LONG)), &pids);\n}\n\n#define VMA_PID_RESET_PERIOD (4 * sysctl_numa_balancing_scan_delay)\n\n \nstatic void task_numa_work(struct callback_head *work)\n{\n\tunsigned long migrate, next_scan, now = jiffies;\n\tstruct task_struct *p = current;\n\tstruct mm_struct *mm = p->mm;\n\tu64 runtime = p->se.sum_exec_runtime;\n\tstruct vm_area_struct *vma;\n\tunsigned long start, end;\n\tunsigned long nr_pte_updates = 0;\n\tlong pages, virtpages;\n\tstruct vma_iterator vmi;\n\n\tSCHED_WARN_ON(p != container_of(work, struct task_struct, numa_work));\n\n\twork->next = work;\n\t \n\tif (p->flags & PF_EXITING)\n\t\treturn;\n\n\tif (!mm->numa_next_scan) {\n\t\tmm->numa_next_scan = now +\n\t\t\tmsecs_to_jiffies(sysctl_numa_balancing_scan_delay);\n\t}\n\n\t \n\tmigrate = mm->numa_next_scan;\n\tif (time_before(now, migrate))\n\t\treturn;\n\n\tif (p->numa_scan_period == 0) {\n\t\tp->numa_scan_period_max = task_scan_max(p);\n\t\tp->numa_scan_period = task_scan_start(p);\n\t}\n\n\tnext_scan = now + msecs_to_jiffies(p->numa_scan_period);\n\tif (!try_cmpxchg(&mm->numa_next_scan, &migrate, next_scan))\n\t\treturn;\n\n\t \n\tp->node_stamp += 2 * TICK_NSEC;\n\n\tstart = mm->numa_scan_offset;\n\tpages = sysctl_numa_balancing_scan_size;\n\tpages <<= 20 - PAGE_SHIFT;  \n\tvirtpages = pages * 8;\t    \n\tif (!pages)\n\t\treturn;\n\n\n\tif (!mmap_read_trylock(mm))\n\t\treturn;\n\tvma_iter_init(&vmi, mm, start);\n\tvma = vma_next(&vmi);\n\tif (!vma) {\n\t\treset_ptenuma_scan(p);\n\t\tstart = 0;\n\t\tvma_iter_set(&vmi, start);\n\t\tvma = vma_next(&vmi);\n\t}\n\n\tdo {\n\t\tif (!vma_migratable(vma) || !vma_policy_mof(vma) ||\n\t\t\tis_vm_hugetlb_page(vma) || (vma->vm_flags & VM_MIXEDMAP)) {\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (!vma->vm_mm ||\n\t\t    (vma->vm_file && (vma->vm_flags & (VM_READ|VM_WRITE)) == (VM_READ)))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (!vma_is_accessible(vma))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (!vma->numab_state) {\n\t\t\tvma->numab_state = kzalloc(sizeof(struct vma_numab_state),\n\t\t\t\tGFP_KERNEL);\n\t\t\tif (!vma->numab_state)\n\t\t\t\tcontinue;\n\n\t\t\tvma->numab_state->next_scan = now +\n\t\t\t\tmsecs_to_jiffies(sysctl_numa_balancing_scan_delay);\n\n\t\t\t \n\t\t\tvma->numab_state->next_pid_reset =  vma->numab_state->next_scan +\n\t\t\t\tmsecs_to_jiffies(VMA_PID_RESET_PERIOD);\n\t\t}\n\n\t\t \n\t\tif (mm->numa_scan_seq && time_before(jiffies,\n\t\t\t\t\t\tvma->numab_state->next_scan))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (!vma_is_accessed(vma))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (mm->numa_scan_seq &&\n\t\t\t\ttime_after(jiffies, vma->numab_state->next_pid_reset)) {\n\t\t\tvma->numab_state->next_pid_reset = vma->numab_state->next_pid_reset +\n\t\t\t\tmsecs_to_jiffies(VMA_PID_RESET_PERIOD);\n\t\t\tvma->numab_state->access_pids[0] = READ_ONCE(vma->numab_state->access_pids[1]);\n\t\t\tvma->numab_state->access_pids[1] = 0;\n\t\t}\n\n\t\tdo {\n\t\t\tstart = max(start, vma->vm_start);\n\t\t\tend = ALIGN(start + (pages << PAGE_SHIFT), HPAGE_SIZE);\n\t\t\tend = min(end, vma->vm_end);\n\t\t\tnr_pte_updates = change_prot_numa(vma, start, end);\n\n\t\t\t \n\t\t\tif (nr_pte_updates)\n\t\t\t\tpages -= (end - start) >> PAGE_SHIFT;\n\t\t\tvirtpages -= (end - start) >> PAGE_SHIFT;\n\n\t\t\tstart = end;\n\t\t\tif (pages <= 0 || virtpages <= 0)\n\t\t\t\tgoto out;\n\n\t\t\tcond_resched();\n\t\t} while (end != vma->vm_end);\n\t} for_each_vma(vmi, vma);\n\nout:\n\t \n\tif (vma)\n\t\tmm->numa_scan_offset = start;\n\telse\n\t\treset_ptenuma_scan(p);\n\tmmap_read_unlock(mm);\n\n\t \n\tif (unlikely(p->se.sum_exec_runtime != runtime)) {\n\t\tu64 diff = p->se.sum_exec_runtime - runtime;\n\t\tp->node_stamp += 32 * diff;\n\t}\n}\n\nvoid init_numa_balancing(unsigned long clone_flags, struct task_struct *p)\n{\n\tint mm_users = 0;\n\tstruct mm_struct *mm = p->mm;\n\n\tif (mm) {\n\t\tmm_users = atomic_read(&mm->mm_users);\n\t\tif (mm_users == 1) {\n\t\t\tmm->numa_next_scan = jiffies + msecs_to_jiffies(sysctl_numa_balancing_scan_delay);\n\t\t\tmm->numa_scan_seq = 0;\n\t\t}\n\t}\n\tp->node_stamp\t\t\t= 0;\n\tp->numa_scan_seq\t\t= mm ? mm->numa_scan_seq : 0;\n\tp->numa_scan_period\t\t= sysctl_numa_balancing_scan_delay;\n\tp->numa_migrate_retry\t\t= 0;\n\t \n\tp->numa_work.next\t\t= &p->numa_work;\n\tp->numa_faults\t\t\t= NULL;\n\tp->numa_pages_migrated\t\t= 0;\n\tp->total_numa_faults\t\t= 0;\n\tRCU_INIT_POINTER(p->numa_group, NULL);\n\tp->last_task_numa_placement\t= 0;\n\tp->last_sum_exec_runtime\t= 0;\n\n\tinit_task_work(&p->numa_work, task_numa_work);\n\n\t \n\tif (!(clone_flags & CLONE_VM)) {\n\t\tp->numa_preferred_nid = NUMA_NO_NODE;\n\t\treturn;\n\t}\n\n\t \n\tif (mm) {\n\t\tunsigned int delay;\n\n\t\tdelay = min_t(unsigned int, task_scan_max(current),\n\t\t\tcurrent->numa_scan_period * mm_users * NSEC_PER_MSEC);\n\t\tdelay += 2 * TICK_NSEC;\n\t\tp->node_stamp = delay;\n\t}\n}\n\n \nstatic void task_tick_numa(struct rq *rq, struct task_struct *curr)\n{\n\tstruct callback_head *work = &curr->numa_work;\n\tu64 period, now;\n\n\t \n\tif (!curr->mm || (curr->flags & (PF_EXITING | PF_KTHREAD)) || work->next != work)\n\t\treturn;\n\n\t \n\tnow = curr->se.sum_exec_runtime;\n\tperiod = (u64)curr->numa_scan_period * NSEC_PER_MSEC;\n\n\tif (now > curr->node_stamp + period) {\n\t\tif (!curr->node_stamp)\n\t\t\tcurr->numa_scan_period = task_scan_start(curr);\n\t\tcurr->node_stamp += period;\n\n\t\tif (!time_before(jiffies, curr->mm->numa_next_scan))\n\t\t\ttask_work_add(curr, work, TWA_RESUME);\n\t}\n}\n\nstatic void update_scan_period(struct task_struct *p, int new_cpu)\n{\n\tint src_nid = cpu_to_node(task_cpu(p));\n\tint dst_nid = cpu_to_node(new_cpu);\n\n\tif (!static_branch_likely(&sched_numa_balancing))\n\t\treturn;\n\n\tif (!p->mm || !p->numa_faults || (p->flags & PF_EXITING))\n\t\treturn;\n\n\tif (src_nid == dst_nid)\n\t\treturn;\n\n\t \n\tif (p->numa_scan_seq) {\n\t\t \n\t\tif (dst_nid == p->numa_preferred_nid ||\n\t\t    (p->numa_preferred_nid != NUMA_NO_NODE &&\n\t\t\tsrc_nid != p->numa_preferred_nid))\n\t\t\treturn;\n\t}\n\n\tp->numa_scan_period = task_scan_start(p);\n}\n\n#else\nstatic void task_tick_numa(struct rq *rq, struct task_struct *curr)\n{\n}\n\nstatic inline void account_numa_enqueue(struct rq *rq, struct task_struct *p)\n{\n}\n\nstatic inline void account_numa_dequeue(struct rq *rq, struct task_struct *p)\n{\n}\n\nstatic inline void update_scan_period(struct task_struct *p, int new_cpu)\n{\n}\n\n#endif  \n\nstatic void\naccount_entity_enqueue(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tupdate_load_add(&cfs_rq->load, se->load.weight);\n#ifdef CONFIG_SMP\n\tif (entity_is_task(se)) {\n\t\tstruct rq *rq = rq_of(cfs_rq);\n\n\t\taccount_numa_enqueue(rq, task_of(se));\n\t\tlist_add(&se->group_node, &rq->cfs_tasks);\n\t}\n#endif\n\tcfs_rq->nr_running++;\n\tif (se_is_idle(se))\n\t\tcfs_rq->idle_nr_running++;\n}\n\nstatic void\naccount_entity_dequeue(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tupdate_load_sub(&cfs_rq->load, se->load.weight);\n#ifdef CONFIG_SMP\n\tif (entity_is_task(se)) {\n\t\taccount_numa_dequeue(rq_of(cfs_rq), task_of(se));\n\t\tlist_del_init(&se->group_node);\n\t}\n#endif\n\tcfs_rq->nr_running--;\n\tif (se_is_idle(se))\n\t\tcfs_rq->idle_nr_running--;\n}\n\n \n#define add_positive(_ptr, _val) do {                           \\\n\ttypeof(_ptr) ptr = (_ptr);                              \\\n\ttypeof(_val) val = (_val);                              \\\n\ttypeof(*ptr) res, var = READ_ONCE(*ptr);                \\\n\t\t\t\t\t\t\t\t\\\n\tres = var + val;                                        \\\n\t\t\t\t\t\t\t\t\\\n\tif (val < 0 && res > var)                               \\\n\t\tres = 0;                                        \\\n\t\t\t\t\t\t\t\t\\\n\tWRITE_ONCE(*ptr, res);                                  \\\n} while (0)\n\n \n#define sub_positive(_ptr, _val) do {\t\t\t\t\\\n\ttypeof(_ptr) ptr = (_ptr);\t\t\t\t\\\n\ttypeof(*ptr) val = (_val);\t\t\t\t\\\n\ttypeof(*ptr) res, var = READ_ONCE(*ptr);\t\t\\\n\tres = var - val;\t\t\t\t\t\\\n\tif (res > var)\t\t\t\t\t\t\\\n\t\tres = 0;\t\t\t\t\t\\\n\tWRITE_ONCE(*ptr, res);\t\t\t\t\t\\\n} while (0)\n\n \n#define lsub_positive(_ptr, _val) do {\t\t\t\t\\\n\ttypeof(_ptr) ptr = (_ptr);\t\t\t\t\\\n\t*ptr -= min_t(typeof(*ptr), *ptr, _val);\t\t\\\n} while (0)\n\n#ifdef CONFIG_SMP\nstatic inline void\nenqueue_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tcfs_rq->avg.load_avg += se->avg.load_avg;\n\tcfs_rq->avg.load_sum += se_weight(se) * se->avg.load_sum;\n}\n\nstatic inline void\ndequeue_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tsub_positive(&cfs_rq->avg.load_avg, se->avg.load_avg);\n\tsub_positive(&cfs_rq->avg.load_sum, se_weight(se) * se->avg.load_sum);\n\t \n\tcfs_rq->avg.load_sum = max_t(u32, cfs_rq->avg.load_sum,\n\t\t\t\t\t  cfs_rq->avg.load_avg * PELT_MIN_DIVIDER);\n}\n#else\nstatic inline void\nenqueue_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) { }\nstatic inline void\ndequeue_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) { }\n#endif\n\nstatic void reweight_eevdf(struct cfs_rq *cfs_rq, struct sched_entity *se,\n\t\t\t   unsigned long weight)\n{\n\tunsigned long old_weight = se->load.weight;\n\tu64 avruntime = avg_vruntime(cfs_rq);\n\ts64 vlag, vslice;\n\n\t \n\tif (avruntime != se->vruntime) {\n\t\tvlag = (s64)(avruntime - se->vruntime);\n\t\tvlag = div_s64(vlag * old_weight, weight);\n\t\tse->vruntime = avruntime - vlag;\n\t}\n\n\t \n\tvslice = (s64)(se->deadline - avruntime);\n\tvslice = div_s64(vslice * old_weight, weight);\n\tse->deadline = avruntime + vslice;\n}\n\nstatic void reweight_entity(struct cfs_rq *cfs_rq, struct sched_entity *se,\n\t\t\t    unsigned long weight)\n{\n\tbool curr = cfs_rq->curr == se;\n\n\tif (se->on_rq) {\n\t\t \n\t\tif (curr)\n\t\t\tupdate_curr(cfs_rq);\n\t\telse\n\t\t\t__dequeue_entity(cfs_rq, se);\n\t\tupdate_load_sub(&cfs_rq->load, se->load.weight);\n\t}\n\tdequeue_load_avg(cfs_rq, se);\n\n\tif (!se->on_rq) {\n\t\t \n\t\tse->vlag = div_s64(se->vlag * se->load.weight, weight);\n\t} else {\n\t\treweight_eevdf(cfs_rq, se, weight);\n\t}\n\n\tupdate_load_set(&se->load, weight);\n\n#ifdef CONFIG_SMP\n\tdo {\n\t\tu32 divider = get_pelt_divider(&se->avg);\n\n\t\tse->avg.load_avg = div_u64(se_weight(se) * se->avg.load_sum, divider);\n\t} while (0);\n#endif\n\n\tenqueue_load_avg(cfs_rq, se);\n\tif (se->on_rq) {\n\t\tupdate_load_add(&cfs_rq->load, se->load.weight);\n\t\tif (!curr)\n\t\t\t__enqueue_entity(cfs_rq, se);\n\n\t\t \n\t\tupdate_min_vruntime(cfs_rq);\n\t}\n}\n\nvoid reweight_task(struct task_struct *p, int prio)\n{\n\tstruct sched_entity *se = &p->se;\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\tstruct load_weight *load = &se->load;\n\tunsigned long weight = scale_load(sched_prio_to_weight[prio]);\n\n\treweight_entity(cfs_rq, se, weight);\n\tload->inv_weight = sched_prio_to_wmult[prio];\n}\n\nstatic inline int throttled_hierarchy(struct cfs_rq *cfs_rq);\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n#ifdef CONFIG_SMP\n \nstatic long calc_group_shares(struct cfs_rq *cfs_rq)\n{\n\tlong tg_weight, tg_shares, load, shares;\n\tstruct task_group *tg = cfs_rq->tg;\n\n\ttg_shares = READ_ONCE(tg->shares);\n\n\tload = max(scale_load_down(cfs_rq->load.weight), cfs_rq->avg.load_avg);\n\n\ttg_weight = atomic_long_read(&tg->load_avg);\n\n\t \n\ttg_weight -= cfs_rq->tg_load_avg_contrib;\n\ttg_weight += load;\n\n\tshares = (tg_shares * load);\n\tif (tg_weight)\n\t\tshares /= tg_weight;\n\n\t \n\treturn clamp_t(long, shares, MIN_SHARES, tg_shares);\n}\n#endif  \n\n \nstatic void update_cfs_group(struct sched_entity *se)\n{\n\tstruct cfs_rq *gcfs_rq = group_cfs_rq(se);\n\tlong shares;\n\n\tif (!gcfs_rq)\n\t\treturn;\n\n\tif (throttled_hierarchy(gcfs_rq))\n\t\treturn;\n\n#ifndef CONFIG_SMP\n\tshares = READ_ONCE(gcfs_rq->tg->shares);\n#else\n\tshares = calc_group_shares(gcfs_rq);\n#endif\n\tif (unlikely(se->load.weight != shares))\n\t\treweight_entity(cfs_rq_of(se), se, shares);\n}\n\n#else  \nstatic inline void update_cfs_group(struct sched_entity *se)\n{\n}\n#endif  \n\nstatic inline void cfs_rq_util_change(struct cfs_rq *cfs_rq, int flags)\n{\n\tstruct rq *rq = rq_of(cfs_rq);\n\n\tif (&rq->cfs == cfs_rq) {\n\t\t \n\t\tcpufreq_update_util(rq, flags);\n\t}\n}\n\n#ifdef CONFIG_SMP\nstatic inline bool load_avg_is_decayed(struct sched_avg *sa)\n{\n\tif (sa->load_sum)\n\t\treturn false;\n\n\tif (sa->util_sum)\n\t\treturn false;\n\n\tif (sa->runnable_sum)\n\t\treturn false;\n\n\t \n\tSCHED_WARN_ON(sa->load_avg ||\n\t\t      sa->util_avg ||\n\t\t      sa->runnable_avg);\n\n\treturn true;\n}\n\nstatic inline u64 cfs_rq_last_update_time(struct cfs_rq *cfs_rq)\n{\n\treturn u64_u32_load_copy(cfs_rq->avg.last_update_time,\n\t\t\t\t cfs_rq->last_update_time_copy);\n}\n#ifdef CONFIG_FAIR_GROUP_SCHED\n \nstatic inline bool child_cfs_rq_on_list(struct cfs_rq *cfs_rq)\n{\n\tstruct cfs_rq *prev_cfs_rq;\n\tstruct list_head *prev;\n\n\tif (cfs_rq->on_list) {\n\t\tprev = cfs_rq->leaf_cfs_rq_list.prev;\n\t} else {\n\t\tstruct rq *rq = rq_of(cfs_rq);\n\n\t\tprev = rq->tmp_alone_branch;\n\t}\n\n\tprev_cfs_rq = container_of(prev, struct cfs_rq, leaf_cfs_rq_list);\n\n\treturn (prev_cfs_rq->tg->parent == cfs_rq->tg);\n}\n\nstatic inline bool cfs_rq_is_decayed(struct cfs_rq *cfs_rq)\n{\n\tif (cfs_rq->load.weight)\n\t\treturn false;\n\n\tif (!load_avg_is_decayed(&cfs_rq->avg))\n\t\treturn false;\n\n\tif (child_cfs_rq_on_list(cfs_rq))\n\t\treturn false;\n\n\treturn true;\n}\n\n \nstatic inline void update_tg_load_avg(struct cfs_rq *cfs_rq)\n{\n\tlong delta = cfs_rq->avg.load_avg - cfs_rq->tg_load_avg_contrib;\n\n\t \n\tif (cfs_rq->tg == &root_task_group)\n\t\treturn;\n\n\tif (abs(delta) > cfs_rq->tg_load_avg_contrib / 64) {\n\t\tatomic_long_add(delta, &cfs_rq->tg->load_avg);\n\t\tcfs_rq->tg_load_avg_contrib = cfs_rq->avg.load_avg;\n\t}\n}\n\n \nvoid set_task_rq_fair(struct sched_entity *se,\n\t\t      struct cfs_rq *prev, struct cfs_rq *next)\n{\n\tu64 p_last_update_time;\n\tu64 n_last_update_time;\n\n\tif (!sched_feat(ATTACH_AGE_LOAD))\n\t\treturn;\n\n\t \n\tif (!(se->avg.last_update_time && prev))\n\t\treturn;\n\n\tp_last_update_time = cfs_rq_last_update_time(prev);\n\tn_last_update_time = cfs_rq_last_update_time(next);\n\n\t__update_load_avg_blocked_se(p_last_update_time, se);\n\tse->avg.last_update_time = n_last_update_time;\n}\n\n \nstatic inline void\nupdate_tg_cfs_util(struct cfs_rq *cfs_rq, struct sched_entity *se, struct cfs_rq *gcfs_rq)\n{\n\tlong delta_sum, delta_avg = gcfs_rq->avg.util_avg - se->avg.util_avg;\n\tu32 new_sum, divider;\n\n\t \n\tif (!delta_avg)\n\t\treturn;\n\n\t \n\tdivider = get_pelt_divider(&cfs_rq->avg);\n\n\n\t \n\tse->avg.util_avg = gcfs_rq->avg.util_avg;\n\tnew_sum = se->avg.util_avg * divider;\n\tdelta_sum = (long)new_sum - (long)se->avg.util_sum;\n\tse->avg.util_sum = new_sum;\n\n\t \n\tadd_positive(&cfs_rq->avg.util_avg, delta_avg);\n\tadd_positive(&cfs_rq->avg.util_sum, delta_sum);\n\n\t \n\tcfs_rq->avg.util_sum = max_t(u32, cfs_rq->avg.util_sum,\n\t\t\t\t\t  cfs_rq->avg.util_avg * PELT_MIN_DIVIDER);\n}\n\nstatic inline void\nupdate_tg_cfs_runnable(struct cfs_rq *cfs_rq, struct sched_entity *se, struct cfs_rq *gcfs_rq)\n{\n\tlong delta_sum, delta_avg = gcfs_rq->avg.runnable_avg - se->avg.runnable_avg;\n\tu32 new_sum, divider;\n\n\t \n\tif (!delta_avg)\n\t\treturn;\n\n\t \n\tdivider = get_pelt_divider(&cfs_rq->avg);\n\n\t \n\tse->avg.runnable_avg = gcfs_rq->avg.runnable_avg;\n\tnew_sum = se->avg.runnable_avg * divider;\n\tdelta_sum = (long)new_sum - (long)se->avg.runnable_sum;\n\tse->avg.runnable_sum = new_sum;\n\n\t \n\tadd_positive(&cfs_rq->avg.runnable_avg, delta_avg);\n\tadd_positive(&cfs_rq->avg.runnable_sum, delta_sum);\n\t \n\tcfs_rq->avg.runnable_sum = max_t(u32, cfs_rq->avg.runnable_sum,\n\t\t\t\t\t      cfs_rq->avg.runnable_avg * PELT_MIN_DIVIDER);\n}\n\nstatic inline void\nupdate_tg_cfs_load(struct cfs_rq *cfs_rq, struct sched_entity *se, struct cfs_rq *gcfs_rq)\n{\n\tlong delta_avg, running_sum, runnable_sum = gcfs_rq->prop_runnable_sum;\n\tunsigned long load_avg;\n\tu64 load_sum = 0;\n\ts64 delta_sum;\n\tu32 divider;\n\n\tif (!runnable_sum)\n\t\treturn;\n\n\tgcfs_rq->prop_runnable_sum = 0;\n\n\t \n\tdivider = get_pelt_divider(&cfs_rq->avg);\n\n\tif (runnable_sum >= 0) {\n\t\t \n\t\trunnable_sum += se->avg.load_sum;\n\t\trunnable_sum = min_t(long, runnable_sum, divider);\n\t} else {\n\t\t \n\t\tif (scale_load_down(gcfs_rq->load.weight)) {\n\t\t\tload_sum = div_u64(gcfs_rq->avg.load_sum,\n\t\t\t\tscale_load_down(gcfs_rq->load.weight));\n\t\t}\n\n\t\t \n\t\trunnable_sum = min(se->avg.load_sum, load_sum);\n\t}\n\n\t \n\trunning_sum = se->avg.util_sum >> SCHED_CAPACITY_SHIFT;\n\trunnable_sum = max(runnable_sum, running_sum);\n\n\tload_sum = se_weight(se) * runnable_sum;\n\tload_avg = div_u64(load_sum, divider);\n\n\tdelta_avg = load_avg - se->avg.load_avg;\n\tif (!delta_avg)\n\t\treturn;\n\n\tdelta_sum = load_sum - (s64)se_weight(se) * se->avg.load_sum;\n\n\tse->avg.load_sum = runnable_sum;\n\tse->avg.load_avg = load_avg;\n\tadd_positive(&cfs_rq->avg.load_avg, delta_avg);\n\tadd_positive(&cfs_rq->avg.load_sum, delta_sum);\n\t \n\tcfs_rq->avg.load_sum = max_t(u32, cfs_rq->avg.load_sum,\n\t\t\t\t\t  cfs_rq->avg.load_avg * PELT_MIN_DIVIDER);\n}\n\nstatic inline void add_tg_cfs_propagate(struct cfs_rq *cfs_rq, long runnable_sum)\n{\n\tcfs_rq->propagate = 1;\n\tcfs_rq->prop_runnable_sum += runnable_sum;\n}\n\n \nstatic inline int propagate_entity_load_avg(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq, *gcfs_rq;\n\n\tif (entity_is_task(se))\n\t\treturn 0;\n\n\tgcfs_rq = group_cfs_rq(se);\n\tif (!gcfs_rq->propagate)\n\t\treturn 0;\n\n\tgcfs_rq->propagate = 0;\n\n\tcfs_rq = cfs_rq_of(se);\n\n\tadd_tg_cfs_propagate(cfs_rq, gcfs_rq->prop_runnable_sum);\n\n\tupdate_tg_cfs_util(cfs_rq, se, gcfs_rq);\n\tupdate_tg_cfs_runnable(cfs_rq, se, gcfs_rq);\n\tupdate_tg_cfs_load(cfs_rq, se, gcfs_rq);\n\n\ttrace_pelt_cfs_tp(cfs_rq);\n\ttrace_pelt_se_tp(se);\n\n\treturn 1;\n}\n\n \nstatic inline bool skip_blocked_update(struct sched_entity *se)\n{\n\tstruct cfs_rq *gcfs_rq = group_cfs_rq(se);\n\n\t \n\tif (se->avg.load_avg || se->avg.util_avg)\n\t\treturn false;\n\n\t \n\tif (gcfs_rq->propagate)\n\t\treturn false;\n\n\t \n\treturn true;\n}\n\n#else  \n\nstatic inline void update_tg_load_avg(struct cfs_rq *cfs_rq) {}\n\nstatic inline int propagate_entity_load_avg(struct sched_entity *se)\n{\n\treturn 0;\n}\n\nstatic inline void add_tg_cfs_propagate(struct cfs_rq *cfs_rq, long runnable_sum) {}\n\n#endif  \n\n#ifdef CONFIG_NO_HZ_COMMON\nstatic inline void migrate_se_pelt_lag(struct sched_entity *se)\n{\n\tu64 throttled = 0, now, lut;\n\tstruct cfs_rq *cfs_rq;\n\tstruct rq *rq;\n\tbool is_idle;\n\n\tif (load_avg_is_decayed(&se->avg))\n\t\treturn;\n\n\tcfs_rq = cfs_rq_of(se);\n\trq = rq_of(cfs_rq);\n\n\trcu_read_lock();\n\tis_idle = is_idle_task(rcu_dereference(rq->curr));\n\trcu_read_unlock();\n\n\t \n\tif (!is_idle)\n\t\treturn;\n\n\t \n\n#ifdef CONFIG_CFS_BANDWIDTH\n\tthrottled = u64_u32_load(cfs_rq->throttled_pelt_idle);\n\t \n\tif (throttled == U64_MAX)\n\t\treturn;\n#endif\n\tnow = u64_u32_load(rq->clock_pelt_idle);\n\t \n\tsmp_rmb();\n\tlut = cfs_rq_last_update_time(cfs_rq);\n\n\tnow -= throttled;\n\tif (now < lut)\n\t\t \n\t\tnow = lut;\n\telse\n\t\tnow += sched_clock_cpu(cpu_of(rq)) - u64_u32_load(rq->clock_idle);\n\n\t__update_load_avg_blocked_se(now, se);\n}\n#else\nstatic void migrate_se_pelt_lag(struct sched_entity *se) {}\n#endif\n\n \nstatic inline int\nupdate_cfs_rq_load_avg(u64 now, struct cfs_rq *cfs_rq)\n{\n\tunsigned long removed_load = 0, removed_util = 0, removed_runnable = 0;\n\tstruct sched_avg *sa = &cfs_rq->avg;\n\tint decayed = 0;\n\n\tif (cfs_rq->removed.nr) {\n\t\tunsigned long r;\n\t\tu32 divider = get_pelt_divider(&cfs_rq->avg);\n\n\t\traw_spin_lock(&cfs_rq->removed.lock);\n\t\tswap(cfs_rq->removed.util_avg, removed_util);\n\t\tswap(cfs_rq->removed.load_avg, removed_load);\n\t\tswap(cfs_rq->removed.runnable_avg, removed_runnable);\n\t\tcfs_rq->removed.nr = 0;\n\t\traw_spin_unlock(&cfs_rq->removed.lock);\n\n\t\tr = removed_load;\n\t\tsub_positive(&sa->load_avg, r);\n\t\tsub_positive(&sa->load_sum, r * divider);\n\t\t \n\t\tsa->load_sum = max_t(u32, sa->load_sum, sa->load_avg * PELT_MIN_DIVIDER);\n\n\t\tr = removed_util;\n\t\tsub_positive(&sa->util_avg, r);\n\t\tsub_positive(&sa->util_sum, r * divider);\n\t\t \n\t\tsa->util_sum = max_t(u32, sa->util_sum, sa->util_avg * PELT_MIN_DIVIDER);\n\n\t\tr = removed_runnable;\n\t\tsub_positive(&sa->runnable_avg, r);\n\t\tsub_positive(&sa->runnable_sum, r * divider);\n\t\t \n\t\tsa->runnable_sum = max_t(u32, sa->runnable_sum,\n\t\t\t\t\t      sa->runnable_avg * PELT_MIN_DIVIDER);\n\n\t\t \n\t\tadd_tg_cfs_propagate(cfs_rq,\n\t\t\t-(long)(removed_runnable * divider) >> SCHED_CAPACITY_SHIFT);\n\n\t\tdecayed = 1;\n\t}\n\n\tdecayed |= __update_load_avg_cfs_rq(now, cfs_rq);\n\tu64_u32_store_copy(sa->last_update_time,\n\t\t\t   cfs_rq->last_update_time_copy,\n\t\t\t   sa->last_update_time);\n\treturn decayed;\n}\n\n \nstatic void attach_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\t \n\tu32 divider = get_pelt_divider(&cfs_rq->avg);\n\n\t \n\tse->avg.last_update_time = cfs_rq->avg.last_update_time;\n\tse->avg.period_contrib = cfs_rq->avg.period_contrib;\n\n\t \n\tse->avg.util_sum = se->avg.util_avg * divider;\n\n\tse->avg.runnable_sum = se->avg.runnable_avg * divider;\n\n\tse->avg.load_sum = se->avg.load_avg * divider;\n\tif (se_weight(se) < se->avg.load_sum)\n\t\tse->avg.load_sum = div_u64(se->avg.load_sum, se_weight(se));\n\telse\n\t\tse->avg.load_sum = 1;\n\n\tenqueue_load_avg(cfs_rq, se);\n\tcfs_rq->avg.util_avg += se->avg.util_avg;\n\tcfs_rq->avg.util_sum += se->avg.util_sum;\n\tcfs_rq->avg.runnable_avg += se->avg.runnable_avg;\n\tcfs_rq->avg.runnable_sum += se->avg.runnable_sum;\n\n\tadd_tg_cfs_propagate(cfs_rq, se->avg.load_sum);\n\n\tcfs_rq_util_change(cfs_rq, 0);\n\n\ttrace_pelt_cfs_tp(cfs_rq);\n}\n\n \nstatic void detach_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tdequeue_load_avg(cfs_rq, se);\n\tsub_positive(&cfs_rq->avg.util_avg, se->avg.util_avg);\n\tsub_positive(&cfs_rq->avg.util_sum, se->avg.util_sum);\n\t \n\tcfs_rq->avg.util_sum = max_t(u32, cfs_rq->avg.util_sum,\n\t\t\t\t\t  cfs_rq->avg.util_avg * PELT_MIN_DIVIDER);\n\n\tsub_positive(&cfs_rq->avg.runnable_avg, se->avg.runnable_avg);\n\tsub_positive(&cfs_rq->avg.runnable_sum, se->avg.runnable_sum);\n\t \n\tcfs_rq->avg.runnable_sum = max_t(u32, cfs_rq->avg.runnable_sum,\n\t\t\t\t\t      cfs_rq->avg.runnable_avg * PELT_MIN_DIVIDER);\n\n\tadd_tg_cfs_propagate(cfs_rq, -se->avg.load_sum);\n\n\tcfs_rq_util_change(cfs_rq, 0);\n\n\ttrace_pelt_cfs_tp(cfs_rq);\n}\n\n \n#define UPDATE_TG\t0x1\n#define SKIP_AGE_LOAD\t0x2\n#define DO_ATTACH\t0x4\n#define DO_DETACH\t0x8\n\n \nstatic inline void update_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\tu64 now = cfs_rq_clock_pelt(cfs_rq);\n\tint decayed;\n\n\t \n\tif (se->avg.last_update_time && !(flags & SKIP_AGE_LOAD))\n\t\t__update_load_avg_se(now, cfs_rq, se);\n\n\tdecayed  = update_cfs_rq_load_avg(now, cfs_rq);\n\tdecayed |= propagate_entity_load_avg(se);\n\n\tif (!se->avg.last_update_time && (flags & DO_ATTACH)) {\n\n\t\t \n\t\tattach_entity_load_avg(cfs_rq, se);\n\t\tupdate_tg_load_avg(cfs_rq);\n\n\t} else if (flags & DO_DETACH) {\n\t\t \n\t\tdetach_entity_load_avg(cfs_rq, se);\n\t\tupdate_tg_load_avg(cfs_rq);\n\t} else if (decayed) {\n\t\tcfs_rq_util_change(cfs_rq, 0);\n\n\t\tif (flags & UPDATE_TG)\n\t\t\tupdate_tg_load_avg(cfs_rq);\n\t}\n}\n\n \nstatic void sync_entity_load_avg(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\tu64 last_update_time;\n\n\tlast_update_time = cfs_rq_last_update_time(cfs_rq);\n\t__update_load_avg_blocked_se(last_update_time, se);\n}\n\n \nstatic void remove_entity_load_avg(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\tunsigned long flags;\n\n\t \n\n\tsync_entity_load_avg(se);\n\n\traw_spin_lock_irqsave(&cfs_rq->removed.lock, flags);\n\t++cfs_rq->removed.nr;\n\tcfs_rq->removed.util_avg\t+= se->avg.util_avg;\n\tcfs_rq->removed.load_avg\t+= se->avg.load_avg;\n\tcfs_rq->removed.runnable_avg\t+= se->avg.runnable_avg;\n\traw_spin_unlock_irqrestore(&cfs_rq->removed.lock, flags);\n}\n\nstatic inline unsigned long cfs_rq_runnable_avg(struct cfs_rq *cfs_rq)\n{\n\treturn cfs_rq->avg.runnable_avg;\n}\n\nstatic inline unsigned long cfs_rq_load_avg(struct cfs_rq *cfs_rq)\n{\n\treturn cfs_rq->avg.load_avg;\n}\n\nstatic int newidle_balance(struct rq *this_rq, struct rq_flags *rf);\n\nstatic inline unsigned long task_util(struct task_struct *p)\n{\n\treturn READ_ONCE(p->se.avg.util_avg);\n}\n\nstatic inline unsigned long _task_util_est(struct task_struct *p)\n{\n\tstruct util_est ue = READ_ONCE(p->se.avg.util_est);\n\n\treturn max(ue.ewma, (ue.enqueued & ~UTIL_AVG_UNCHANGED));\n}\n\nstatic inline unsigned long task_util_est(struct task_struct *p)\n{\n\treturn max(task_util(p), _task_util_est(p));\n}\n\nstatic inline void util_est_enqueue(struct cfs_rq *cfs_rq,\n\t\t\t\t    struct task_struct *p)\n{\n\tunsigned int enqueued;\n\n\tif (!sched_feat(UTIL_EST))\n\t\treturn;\n\n\t \n\tenqueued  = cfs_rq->avg.util_est.enqueued;\n\tenqueued += _task_util_est(p);\n\tWRITE_ONCE(cfs_rq->avg.util_est.enqueued, enqueued);\n\n\ttrace_sched_util_est_cfs_tp(cfs_rq);\n}\n\nstatic inline void util_est_dequeue(struct cfs_rq *cfs_rq,\n\t\t\t\t    struct task_struct *p)\n{\n\tunsigned int enqueued;\n\n\tif (!sched_feat(UTIL_EST))\n\t\treturn;\n\n\t \n\tenqueued  = cfs_rq->avg.util_est.enqueued;\n\tenqueued -= min_t(unsigned int, enqueued, _task_util_est(p));\n\tWRITE_ONCE(cfs_rq->avg.util_est.enqueued, enqueued);\n\n\ttrace_sched_util_est_cfs_tp(cfs_rq);\n}\n\n#define UTIL_EST_MARGIN (SCHED_CAPACITY_SCALE / 100)\n\n \nstatic inline bool within_margin(int value, int margin)\n{\n\treturn ((unsigned int)(value + margin - 1) < (2 * margin - 1));\n}\n\nstatic inline void util_est_update(struct cfs_rq *cfs_rq,\n\t\t\t\t   struct task_struct *p,\n\t\t\t\t   bool task_sleep)\n{\n\tlong last_ewma_diff, last_enqueued_diff;\n\tstruct util_est ue;\n\n\tif (!sched_feat(UTIL_EST))\n\t\treturn;\n\n\t \n\tif (!task_sleep)\n\t\treturn;\n\n\t \n\tue = p->se.avg.util_est;\n\tif (ue.enqueued & UTIL_AVG_UNCHANGED)\n\t\treturn;\n\n\tlast_enqueued_diff = ue.enqueued;\n\n\t \n\tue.enqueued = task_util(p);\n\tif (sched_feat(UTIL_EST_FASTUP)) {\n\t\tif (ue.ewma < ue.enqueued) {\n\t\t\tue.ewma = ue.enqueued;\n\t\t\tgoto done;\n\t\t}\n\t}\n\n\t \n\tlast_ewma_diff = ue.enqueued - ue.ewma;\n\tlast_enqueued_diff -= ue.enqueued;\n\tif (within_margin(last_ewma_diff, UTIL_EST_MARGIN)) {\n\t\tif (!within_margin(last_enqueued_diff, UTIL_EST_MARGIN))\n\t\t\tgoto done;\n\n\t\treturn;\n\t}\n\n\t \n\tif (task_util(p) > capacity_orig_of(cpu_of(rq_of(cfs_rq))))\n\t\treturn;\n\n\t \n\tue.ewma <<= UTIL_EST_WEIGHT_SHIFT;\n\tue.ewma  += last_ewma_diff;\n\tue.ewma >>= UTIL_EST_WEIGHT_SHIFT;\ndone:\n\tue.enqueued |= UTIL_AVG_UNCHANGED;\n\tWRITE_ONCE(p->se.avg.util_est, ue);\n\n\ttrace_sched_util_est_se_tp(&p->se);\n}\n\nstatic inline int util_fits_cpu(unsigned long util,\n\t\t\t\tunsigned long uclamp_min,\n\t\t\t\tunsigned long uclamp_max,\n\t\t\t\tint cpu)\n{\n\tunsigned long capacity_orig, capacity_orig_thermal;\n\tunsigned long capacity = capacity_of(cpu);\n\tbool fits, uclamp_max_fits;\n\n\t \n\tfits = fits_capacity(util, capacity);\n\n\tif (!uclamp_is_used())\n\t\treturn fits;\n\n\t \n\tcapacity_orig = capacity_orig_of(cpu);\n\tcapacity_orig_thermal = capacity_orig - arch_scale_thermal_pressure(cpu);\n\n\t \n\tuclamp_max_fits = (capacity_orig == SCHED_CAPACITY_SCALE) && (uclamp_max == SCHED_CAPACITY_SCALE);\n\tuclamp_max_fits = !uclamp_max_fits && (uclamp_max <= capacity_orig);\n\tfits = fits || uclamp_max_fits;\n\n\t \n\tuclamp_min = min(uclamp_min, uclamp_max);\n\tif (fits && (util < uclamp_min) && (uclamp_min > capacity_orig_thermal))\n\t\treturn -1;\n\n\treturn fits;\n}\n\nstatic inline int task_fits_cpu(struct task_struct *p, int cpu)\n{\n\tunsigned long uclamp_min = uclamp_eff_value(p, UCLAMP_MIN);\n\tunsigned long uclamp_max = uclamp_eff_value(p, UCLAMP_MAX);\n\tunsigned long util = task_util_est(p);\n\t \n\treturn (util_fits_cpu(util, uclamp_min, uclamp_max, cpu) > 0);\n}\n\nstatic inline void update_misfit_status(struct task_struct *p, struct rq *rq)\n{\n\tif (!sched_asym_cpucap_active())\n\t\treturn;\n\n\tif (!p || p->nr_cpus_allowed == 1) {\n\t\trq->misfit_task_load = 0;\n\t\treturn;\n\t}\n\n\tif (task_fits_cpu(p, cpu_of(rq))) {\n\t\trq->misfit_task_load = 0;\n\t\treturn;\n\t}\n\n\t \n\trq->misfit_task_load = max_t(unsigned long, task_h_load(p), 1);\n}\n\n#else  \n\nstatic inline bool cfs_rq_is_decayed(struct cfs_rq *cfs_rq)\n{\n\treturn !cfs_rq->nr_running;\n}\n\n#define UPDATE_TG\t0x0\n#define SKIP_AGE_LOAD\t0x0\n#define DO_ATTACH\t0x0\n#define DO_DETACH\t0x0\n\nstatic inline void update_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int not_used1)\n{\n\tcfs_rq_util_change(cfs_rq, 0);\n}\n\nstatic inline void remove_entity_load_avg(struct sched_entity *se) {}\n\nstatic inline void\nattach_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) {}\nstatic inline void\ndetach_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) {}\n\nstatic inline int newidle_balance(struct rq *rq, struct rq_flags *rf)\n{\n\treturn 0;\n}\n\nstatic inline void\nutil_est_enqueue(struct cfs_rq *cfs_rq, struct task_struct *p) {}\n\nstatic inline void\nutil_est_dequeue(struct cfs_rq *cfs_rq, struct task_struct *p) {}\n\nstatic inline void\nutil_est_update(struct cfs_rq *cfs_rq, struct task_struct *p,\n\t\tbool task_sleep) {}\nstatic inline void update_misfit_status(struct task_struct *p, struct rq *rq) {}\n\n#endif  \n\nstatic void\nplace_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\tu64 vslice, vruntime = avg_vruntime(cfs_rq);\n\ts64 lag = 0;\n\n\tse->slice = sysctl_sched_base_slice;\n\tvslice = calc_delta_fair(se->slice, se);\n\n\t \n\tif (sched_feat(PLACE_LAG) && cfs_rq->nr_running) {\n\t\tstruct sched_entity *curr = cfs_rq->curr;\n\t\tunsigned long load;\n\n\t\tlag = se->vlag;\n\n\t\t \n\t\tload = cfs_rq->avg_load;\n\t\tif (curr && curr->on_rq)\n\t\t\tload += scale_load_down(curr->load.weight);\n\n\t\tlag *= load + scale_load_down(se->load.weight);\n\t\tif (WARN_ON_ONCE(!load))\n\t\t\tload = 1;\n\t\tlag = div_s64(lag, load);\n\t}\n\n\tse->vruntime = vruntime - lag;\n\n\t \n\tif (sched_feat(PLACE_DEADLINE_INITIAL) && (flags & ENQUEUE_INITIAL))\n\t\tvslice /= 2;\n\n\t \n\tse->deadline = se->vruntime + vslice;\n}\n\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq);\nstatic inline int cfs_rq_throttled(struct cfs_rq *cfs_rq);\n\nstatic inline bool cfs_bandwidth_used(void);\n\nstatic void\nenqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\tbool curr = cfs_rq->curr == se;\n\n\t \n\tif (curr)\n\t\tplace_entity(cfs_rq, se, flags);\n\n\tupdate_curr(cfs_rq);\n\n\t \n\tupdate_load_avg(cfs_rq, se, UPDATE_TG | DO_ATTACH);\n\tse_update_runnable(se);\n\t \n\tupdate_cfs_group(se);\n\n\t \n\tif (!curr)\n\t\tplace_entity(cfs_rq, se, flags);\n\n\taccount_entity_enqueue(cfs_rq, se);\n\n\t \n\tif (flags & ENQUEUE_MIGRATED)\n\t\tse->exec_start = 0;\n\n\tcheck_schedstat_required();\n\tupdate_stats_enqueue_fair(cfs_rq, se, flags);\n\tif (!curr)\n\t\t__enqueue_entity(cfs_rq, se);\n\tse->on_rq = 1;\n\n\tif (cfs_rq->nr_running == 1) {\n\t\tcheck_enqueue_throttle(cfs_rq);\n\t\tif (!throttled_hierarchy(cfs_rq)) {\n\t\t\tlist_add_leaf_cfs_rq(cfs_rq);\n\t\t} else {\n#ifdef CONFIG_CFS_BANDWIDTH\n\t\t\tstruct rq *rq = rq_of(cfs_rq);\n\n\t\t\tif (cfs_rq_throttled(cfs_rq) && !cfs_rq->throttled_clock)\n\t\t\t\tcfs_rq->throttled_clock = rq_clock(rq);\n\t\t\tif (!cfs_rq->throttled_clock_self)\n\t\t\t\tcfs_rq->throttled_clock_self = rq_clock(rq);\n#endif\n\t\t}\n\t}\n}\n\nstatic void __clear_buddies_next(struct sched_entity *se)\n{\n\tfor_each_sched_entity(se) {\n\t\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\t\tif (cfs_rq->next != se)\n\t\t\tbreak;\n\n\t\tcfs_rq->next = NULL;\n\t}\n}\n\nstatic void clear_buddies(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tif (cfs_rq->next == se)\n\t\t__clear_buddies_next(se);\n}\n\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic void\ndequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)\n{\n\tint action = UPDATE_TG;\n\n\tif (entity_is_task(se) && task_on_rq_migrating(task_of(se)))\n\t\taction |= DO_DETACH;\n\n\t \n\tupdate_curr(cfs_rq);\n\n\t \n\tupdate_load_avg(cfs_rq, se, action);\n\tse_update_runnable(se);\n\n\tupdate_stats_dequeue_fair(cfs_rq, se, flags);\n\n\tclear_buddies(cfs_rq, se);\n\n\tupdate_entity_lag(cfs_rq, se);\n\tif (se != cfs_rq->curr)\n\t\t__dequeue_entity(cfs_rq, se);\n\tse->on_rq = 0;\n\taccount_entity_dequeue(cfs_rq, se);\n\n\t \n\treturn_cfs_rq_runtime(cfs_rq);\n\n\tupdate_cfs_group(se);\n\n\t \n\tif ((flags & (DEQUEUE_SAVE | DEQUEUE_MOVE)) != DEQUEUE_SAVE)\n\t\tupdate_min_vruntime(cfs_rq);\n\n\tif (cfs_rq->nr_running == 0)\n\t\tupdate_idle_cfs_rq_clock_pelt(cfs_rq);\n}\n\nstatic void\nset_next_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)\n{\n\tclear_buddies(cfs_rq, se);\n\n\t \n\tif (se->on_rq) {\n\t\t \n\t\tupdate_stats_wait_end_fair(cfs_rq, se);\n\t\t__dequeue_entity(cfs_rq, se);\n\t\tupdate_load_avg(cfs_rq, se, UPDATE_TG);\n\t\t \n\t\tse->vlag = se->deadline;\n\t}\n\n\tupdate_stats_curr_start(cfs_rq, se);\n\tcfs_rq->curr = se;\n\n\t \n\tif (schedstat_enabled() &&\n\t    rq_of(cfs_rq)->cfs.load.weight >= 2*se->load.weight) {\n\t\tstruct sched_statistics *stats;\n\n\t\tstats = __schedstats_from_se(se);\n\t\t__schedstat_set(stats->slice_max,\n\t\t\t\tmax((u64)stats->slice_max,\n\t\t\t\t    se->sum_exec_runtime - se->prev_sum_exec_runtime));\n\t}\n\n\tse->prev_sum_exec_runtime = se->sum_exec_runtime;\n}\n\n \nstatic struct sched_entity *\npick_next_entity(struct cfs_rq *cfs_rq, struct sched_entity *curr)\n{\n\t \n\tif (sched_feat(NEXT_BUDDY) &&\n\t    cfs_rq->next && entity_eligible(cfs_rq, cfs_rq->next))\n\t\treturn cfs_rq->next;\n\n\treturn pick_eevdf(cfs_rq);\n}\n\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);\n\nstatic void put_prev_entity(struct cfs_rq *cfs_rq, struct sched_entity *prev)\n{\n\t \n\tif (prev->on_rq)\n\t\tupdate_curr(cfs_rq);\n\n\t \n\tcheck_cfs_rq_runtime(cfs_rq);\n\n\tif (prev->on_rq) {\n\t\tupdate_stats_wait_start_fair(cfs_rq, prev);\n\t\t \n\t\t__enqueue_entity(cfs_rq, prev);\n\t\t \n\t\tupdate_load_avg(cfs_rq, prev, 0);\n\t}\n\tcfs_rq->curr = NULL;\n}\n\nstatic void\nentity_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr, int queued)\n{\n\t \n\tupdate_curr(cfs_rq);\n\n\t \n\tupdate_load_avg(cfs_rq, curr, UPDATE_TG);\n\tupdate_cfs_group(curr);\n\n#ifdef CONFIG_SCHED_HRTICK\n\t \n\tif (queued) {\n\t\tresched_curr(rq_of(cfs_rq));\n\t\treturn;\n\t}\n\t \n\tif (!sched_feat(DOUBLE_TICK) &&\n\t\t\thrtimer_active(&rq_of(cfs_rq)->hrtick_timer))\n\t\treturn;\n#endif\n}\n\n\n \n\n#ifdef CONFIG_CFS_BANDWIDTH\n\n#ifdef CONFIG_JUMP_LABEL\nstatic struct static_key __cfs_bandwidth_used;\n\nstatic inline bool cfs_bandwidth_used(void)\n{\n\treturn static_key_false(&__cfs_bandwidth_used);\n}\n\nvoid cfs_bandwidth_usage_inc(void)\n{\n\tstatic_key_slow_inc_cpuslocked(&__cfs_bandwidth_used);\n}\n\nvoid cfs_bandwidth_usage_dec(void)\n{\n\tstatic_key_slow_dec_cpuslocked(&__cfs_bandwidth_used);\n}\n#else  \nstatic bool cfs_bandwidth_used(void)\n{\n\treturn true;\n}\n\nvoid cfs_bandwidth_usage_inc(void) {}\nvoid cfs_bandwidth_usage_dec(void) {}\n#endif  \n\n \nstatic inline u64 default_cfs_period(void)\n{\n\treturn 100000000ULL;\n}\n\nstatic inline u64 sched_cfs_bandwidth_slice(void)\n{\n\treturn (u64)sysctl_sched_cfs_bandwidth_slice * NSEC_PER_USEC;\n}\n\n \nvoid __refill_cfs_bandwidth_runtime(struct cfs_bandwidth *cfs_b)\n{\n\ts64 runtime;\n\n\tif (unlikely(cfs_b->quota == RUNTIME_INF))\n\t\treturn;\n\n\tcfs_b->runtime += cfs_b->quota;\n\truntime = cfs_b->runtime_snap - cfs_b->runtime;\n\tif (runtime > 0) {\n\t\tcfs_b->burst_time += runtime;\n\t\tcfs_b->nr_burst++;\n\t}\n\n\tcfs_b->runtime = min(cfs_b->runtime, cfs_b->quota + cfs_b->burst);\n\tcfs_b->runtime_snap = cfs_b->runtime;\n}\n\nstatic inline struct cfs_bandwidth *tg_cfs_bandwidth(struct task_group *tg)\n{\n\treturn &tg->cfs_bandwidth;\n}\n\n \nstatic int __assign_cfs_rq_runtime(struct cfs_bandwidth *cfs_b,\n\t\t\t\t   struct cfs_rq *cfs_rq, u64 target_runtime)\n{\n\tu64 min_amount, amount = 0;\n\n\tlockdep_assert_held(&cfs_b->lock);\n\n\t \n\tmin_amount = target_runtime - cfs_rq->runtime_remaining;\n\n\tif (cfs_b->quota == RUNTIME_INF)\n\t\tamount = min_amount;\n\telse {\n\t\tstart_cfs_bandwidth(cfs_b);\n\n\t\tif (cfs_b->runtime > 0) {\n\t\t\tamount = min(cfs_b->runtime, min_amount);\n\t\t\tcfs_b->runtime -= amount;\n\t\t\tcfs_b->idle = 0;\n\t\t}\n\t}\n\n\tcfs_rq->runtime_remaining += amount;\n\n\treturn cfs_rq->runtime_remaining > 0;\n}\n\n \nstatic int assign_cfs_rq_runtime(struct cfs_rq *cfs_rq)\n{\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);\n\tint ret;\n\n\traw_spin_lock(&cfs_b->lock);\n\tret = __assign_cfs_rq_runtime(cfs_b, cfs_rq, sched_cfs_bandwidth_slice());\n\traw_spin_unlock(&cfs_b->lock);\n\n\treturn ret;\n}\n\nstatic void __account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec)\n{\n\t \n\tcfs_rq->runtime_remaining -= delta_exec;\n\n\tif (likely(cfs_rq->runtime_remaining > 0))\n\t\treturn;\n\n\tif (cfs_rq->throttled)\n\t\treturn;\n\t \n\tif (!assign_cfs_rq_runtime(cfs_rq) && likely(cfs_rq->curr))\n\t\tresched_curr(rq_of(cfs_rq));\n}\n\nstatic __always_inline\nvoid account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec)\n{\n\tif (!cfs_bandwidth_used() || !cfs_rq->runtime_enabled)\n\t\treturn;\n\n\t__account_cfs_rq_runtime(cfs_rq, delta_exec);\n}\n\nstatic inline int cfs_rq_throttled(struct cfs_rq *cfs_rq)\n{\n\treturn cfs_bandwidth_used() && cfs_rq->throttled;\n}\n\n \nstatic inline int throttled_hierarchy(struct cfs_rq *cfs_rq)\n{\n\treturn cfs_bandwidth_used() && cfs_rq->throttle_count;\n}\n\n \nstatic inline int throttled_lb_pair(struct task_group *tg,\n\t\t\t\t    int src_cpu, int dest_cpu)\n{\n\tstruct cfs_rq *src_cfs_rq, *dest_cfs_rq;\n\n\tsrc_cfs_rq = tg->cfs_rq[src_cpu];\n\tdest_cfs_rq = tg->cfs_rq[dest_cpu];\n\n\treturn throttled_hierarchy(src_cfs_rq) ||\n\t       throttled_hierarchy(dest_cfs_rq);\n}\n\nstatic int tg_unthrottle_up(struct task_group *tg, void *data)\n{\n\tstruct rq *rq = data;\n\tstruct cfs_rq *cfs_rq = tg->cfs_rq[cpu_of(rq)];\n\n\tcfs_rq->throttle_count--;\n\tif (!cfs_rq->throttle_count) {\n\t\tcfs_rq->throttled_clock_pelt_time += rq_clock_pelt(rq) -\n\t\t\t\t\t     cfs_rq->throttled_clock_pelt;\n\n\t\t \n\t\tif (!cfs_rq_is_decayed(cfs_rq))\n\t\t\tlist_add_leaf_cfs_rq(cfs_rq);\n\n\t\tif (cfs_rq->throttled_clock_self) {\n\t\t\tu64 delta = rq_clock(rq) - cfs_rq->throttled_clock_self;\n\n\t\t\tcfs_rq->throttled_clock_self = 0;\n\n\t\t\tif (SCHED_WARN_ON((s64)delta < 0))\n\t\t\t\tdelta = 0;\n\n\t\t\tcfs_rq->throttled_clock_self_time += delta;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int tg_throttle_down(struct task_group *tg, void *data)\n{\n\tstruct rq *rq = data;\n\tstruct cfs_rq *cfs_rq = tg->cfs_rq[cpu_of(rq)];\n\n\t \n\tif (!cfs_rq->throttle_count) {\n\t\tcfs_rq->throttled_clock_pelt = rq_clock_pelt(rq);\n\t\tlist_del_leaf_cfs_rq(cfs_rq);\n\n\t\tSCHED_WARN_ON(cfs_rq->throttled_clock_self);\n\t\tif (cfs_rq->nr_running)\n\t\t\tcfs_rq->throttled_clock_self = rq_clock(rq);\n\t}\n\tcfs_rq->throttle_count++;\n\n\treturn 0;\n}\n\nstatic bool throttle_cfs_rq(struct cfs_rq *cfs_rq)\n{\n\tstruct rq *rq = rq_of(cfs_rq);\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);\n\tstruct sched_entity *se;\n\tlong task_delta, idle_task_delta, dequeue = 1;\n\n\traw_spin_lock(&cfs_b->lock);\n\t \n\tif (__assign_cfs_rq_runtime(cfs_b, cfs_rq, 1)) {\n\t\t \n\t\tdequeue = 0;\n\t} else {\n\t\tlist_add_tail_rcu(&cfs_rq->throttled_list,\n\t\t\t\t  &cfs_b->throttled_cfs_rq);\n\t}\n\traw_spin_unlock(&cfs_b->lock);\n\n\tif (!dequeue)\n\t\treturn false;   \n\n\tse = cfs_rq->tg->se[cpu_of(rq_of(cfs_rq))];\n\n\t \n\trcu_read_lock();\n\twalk_tg_tree_from(cfs_rq->tg, tg_throttle_down, tg_nop, (void *)rq);\n\trcu_read_unlock();\n\n\ttask_delta = cfs_rq->h_nr_running;\n\tidle_task_delta = cfs_rq->idle_h_nr_running;\n\tfor_each_sched_entity(se) {\n\t\tstruct cfs_rq *qcfs_rq = cfs_rq_of(se);\n\t\t \n\t\tif (!se->on_rq)\n\t\t\tgoto done;\n\n\t\tdequeue_entity(qcfs_rq, se, DEQUEUE_SLEEP);\n\n\t\tif (cfs_rq_is_idle(group_cfs_rq(se)))\n\t\t\tidle_task_delta = cfs_rq->h_nr_running;\n\n\t\tqcfs_rq->h_nr_running -= task_delta;\n\t\tqcfs_rq->idle_h_nr_running -= idle_task_delta;\n\n\t\tif (qcfs_rq->load.weight) {\n\t\t\t \n\t\t\tse = parent_entity(se);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tfor_each_sched_entity(se) {\n\t\tstruct cfs_rq *qcfs_rq = cfs_rq_of(se);\n\t\t \n\t\tif (!se->on_rq)\n\t\t\tgoto done;\n\n\t\tupdate_load_avg(qcfs_rq, se, 0);\n\t\tse_update_runnable(se);\n\n\t\tif (cfs_rq_is_idle(group_cfs_rq(se)))\n\t\t\tidle_task_delta = cfs_rq->h_nr_running;\n\n\t\tqcfs_rq->h_nr_running -= task_delta;\n\t\tqcfs_rq->idle_h_nr_running -= idle_task_delta;\n\t}\n\n\t \n\tsub_nr_running(rq, task_delta);\n\ndone:\n\t \n\tcfs_rq->throttled = 1;\n\tSCHED_WARN_ON(cfs_rq->throttled_clock);\n\tif (cfs_rq->nr_running)\n\t\tcfs_rq->throttled_clock = rq_clock(rq);\n\treturn true;\n}\n\nvoid unthrottle_cfs_rq(struct cfs_rq *cfs_rq)\n{\n\tstruct rq *rq = rq_of(cfs_rq);\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);\n\tstruct sched_entity *se;\n\tlong task_delta, idle_task_delta;\n\n\tse = cfs_rq->tg->se[cpu_of(rq)];\n\n\tcfs_rq->throttled = 0;\n\n\tupdate_rq_clock(rq);\n\n\traw_spin_lock(&cfs_b->lock);\n\tif (cfs_rq->throttled_clock) {\n\t\tcfs_b->throttled_time += rq_clock(rq) - cfs_rq->throttled_clock;\n\t\tcfs_rq->throttled_clock = 0;\n\t}\n\tlist_del_rcu(&cfs_rq->throttled_list);\n\traw_spin_unlock(&cfs_b->lock);\n\n\t \n\twalk_tg_tree_from(cfs_rq->tg, tg_nop, tg_unthrottle_up, (void *)rq);\n\n\tif (!cfs_rq->load.weight) {\n\t\tif (!cfs_rq->on_list)\n\t\t\treturn;\n\t\t \n\t\tfor_each_sched_entity(se) {\n\t\t\tif (list_add_leaf_cfs_rq(cfs_rq_of(se)))\n\t\t\t\tbreak;\n\t\t}\n\t\tgoto unthrottle_throttle;\n\t}\n\n\ttask_delta = cfs_rq->h_nr_running;\n\tidle_task_delta = cfs_rq->idle_h_nr_running;\n\tfor_each_sched_entity(se) {\n\t\tstruct cfs_rq *qcfs_rq = cfs_rq_of(se);\n\n\t\tif (se->on_rq)\n\t\t\tbreak;\n\t\tenqueue_entity(qcfs_rq, se, ENQUEUE_WAKEUP);\n\n\t\tif (cfs_rq_is_idle(group_cfs_rq(se)))\n\t\t\tidle_task_delta = cfs_rq->h_nr_running;\n\n\t\tqcfs_rq->h_nr_running += task_delta;\n\t\tqcfs_rq->idle_h_nr_running += idle_task_delta;\n\n\t\t \n\t\tif (cfs_rq_throttled(qcfs_rq))\n\t\t\tgoto unthrottle_throttle;\n\t}\n\n\tfor_each_sched_entity(se) {\n\t\tstruct cfs_rq *qcfs_rq = cfs_rq_of(se);\n\n\t\tupdate_load_avg(qcfs_rq, se, UPDATE_TG);\n\t\tse_update_runnable(se);\n\n\t\tif (cfs_rq_is_idle(group_cfs_rq(se)))\n\t\t\tidle_task_delta = cfs_rq->h_nr_running;\n\n\t\tqcfs_rq->h_nr_running += task_delta;\n\t\tqcfs_rq->idle_h_nr_running += idle_task_delta;\n\n\t\t \n\t\tif (cfs_rq_throttled(qcfs_rq))\n\t\t\tgoto unthrottle_throttle;\n\t}\n\n\t \n\tadd_nr_running(rq, task_delta);\n\nunthrottle_throttle:\n\tassert_list_leaf_cfs_rq(rq);\n\n\t \n\tif (rq->curr == rq->idle && rq->cfs.nr_running)\n\t\tresched_curr(rq);\n}\n\n#ifdef CONFIG_SMP\nstatic void __cfsb_csd_unthrottle(void *arg)\n{\n\tstruct cfs_rq *cursor, *tmp;\n\tstruct rq *rq = arg;\n\tstruct rq_flags rf;\n\n\trq_lock(rq, &rf);\n\n\t \n\tupdate_rq_clock(rq);\n\trq_clock_start_loop_update(rq);\n\n\t \n\trcu_read_lock();\n\n\tlist_for_each_entry_safe(cursor, tmp, &rq->cfsb_csd_list,\n\t\t\t\t throttled_csd_list) {\n\t\tlist_del_init(&cursor->throttled_csd_list);\n\n\t\tif (cfs_rq_throttled(cursor))\n\t\t\tunthrottle_cfs_rq(cursor);\n\t}\n\n\trcu_read_unlock();\n\n\trq_clock_stop_loop_update(rq);\n\trq_unlock(rq, &rf);\n}\n\nstatic inline void __unthrottle_cfs_rq_async(struct cfs_rq *cfs_rq)\n{\n\tstruct rq *rq = rq_of(cfs_rq);\n\tbool first;\n\n\tif (rq == this_rq()) {\n\t\tunthrottle_cfs_rq(cfs_rq);\n\t\treturn;\n\t}\n\n\t \n\tif (SCHED_WARN_ON(!list_empty(&cfs_rq->throttled_csd_list)))\n\t\treturn;\n\n\tfirst = list_empty(&rq->cfsb_csd_list);\n\tlist_add_tail(&cfs_rq->throttled_csd_list, &rq->cfsb_csd_list);\n\tif (first)\n\t\tsmp_call_function_single_async(cpu_of(rq), &rq->cfsb_csd);\n}\n#else\nstatic inline void __unthrottle_cfs_rq_async(struct cfs_rq *cfs_rq)\n{\n\tunthrottle_cfs_rq(cfs_rq);\n}\n#endif\n\nstatic void unthrottle_cfs_rq_async(struct cfs_rq *cfs_rq)\n{\n\tlockdep_assert_rq_held(rq_of(cfs_rq));\n\n\tif (SCHED_WARN_ON(!cfs_rq_throttled(cfs_rq) ||\n\t    cfs_rq->runtime_remaining <= 0))\n\t\treturn;\n\n\t__unthrottle_cfs_rq_async(cfs_rq);\n}\n\nstatic bool distribute_cfs_runtime(struct cfs_bandwidth *cfs_b)\n{\n\tstruct cfs_rq *local_unthrottle = NULL;\n\tint this_cpu = smp_processor_id();\n\tu64 runtime, remaining = 1;\n\tbool throttled = false;\n\tstruct cfs_rq *cfs_rq;\n\tstruct rq_flags rf;\n\tstruct rq *rq;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(cfs_rq, &cfs_b->throttled_cfs_rq,\n\t\t\t\tthrottled_list) {\n\t\trq = rq_of(cfs_rq);\n\n\t\tif (!remaining) {\n\t\t\tthrottled = true;\n\t\t\tbreak;\n\t\t}\n\n\t\trq_lock_irqsave(rq, &rf);\n\t\tif (!cfs_rq_throttled(cfs_rq))\n\t\t\tgoto next;\n\n#ifdef CONFIG_SMP\n\t\t \n\t\tif (!list_empty(&cfs_rq->throttled_csd_list))\n\t\t\tgoto next;\n#endif\n\n\t\t \n\t\tSCHED_WARN_ON(cfs_rq->runtime_remaining > 0);\n\n\t\traw_spin_lock(&cfs_b->lock);\n\t\truntime = -cfs_rq->runtime_remaining + 1;\n\t\tif (runtime > cfs_b->runtime)\n\t\t\truntime = cfs_b->runtime;\n\t\tcfs_b->runtime -= runtime;\n\t\tremaining = cfs_b->runtime;\n\t\traw_spin_unlock(&cfs_b->lock);\n\n\t\tcfs_rq->runtime_remaining += runtime;\n\n\t\t \n\t\tif (cfs_rq->runtime_remaining > 0) {\n\t\t\tif (cpu_of(rq) != this_cpu ||\n\t\t\t    SCHED_WARN_ON(local_unthrottle))\n\t\t\t\tunthrottle_cfs_rq_async(cfs_rq);\n\t\t\telse\n\t\t\t\tlocal_unthrottle = cfs_rq;\n\t\t} else {\n\t\t\tthrottled = true;\n\t\t}\n\nnext:\n\t\trq_unlock_irqrestore(rq, &rf);\n\t}\n\trcu_read_unlock();\n\n\tif (local_unthrottle) {\n\t\trq = cpu_rq(this_cpu);\n\t\trq_lock_irqsave(rq, &rf);\n\t\tif (cfs_rq_throttled(local_unthrottle))\n\t\t\tunthrottle_cfs_rq(local_unthrottle);\n\t\trq_unlock_irqrestore(rq, &rf);\n\t}\n\n\treturn throttled;\n}\n\n \nstatic int do_sched_cfs_period_timer(struct cfs_bandwidth *cfs_b, int overrun, unsigned long flags)\n{\n\tint throttled;\n\n\t \n\tif (cfs_b->quota == RUNTIME_INF)\n\t\tgoto out_deactivate;\n\n\tthrottled = !list_empty(&cfs_b->throttled_cfs_rq);\n\tcfs_b->nr_periods += overrun;\n\n\t \n\t__refill_cfs_bandwidth_runtime(cfs_b);\n\n\t \n\tif (cfs_b->idle && !throttled)\n\t\tgoto out_deactivate;\n\n\tif (!throttled) {\n\t\t \n\t\tcfs_b->idle = 1;\n\t\treturn 0;\n\t}\n\n\t \n\tcfs_b->nr_throttled += overrun;\n\n\t \n\twhile (throttled && cfs_b->runtime > 0) {\n\t\traw_spin_unlock_irqrestore(&cfs_b->lock, flags);\n\t\t \n\t\tthrottled = distribute_cfs_runtime(cfs_b);\n\t\traw_spin_lock_irqsave(&cfs_b->lock, flags);\n\t}\n\n\t \n\tcfs_b->idle = 0;\n\n\treturn 0;\n\nout_deactivate:\n\treturn 1;\n}\n\n \nstatic const u64 min_cfs_rq_runtime = 1 * NSEC_PER_MSEC;\n \nstatic const u64 min_bandwidth_expiration = 2 * NSEC_PER_MSEC;\n \nstatic const u64 cfs_bandwidth_slack_period = 5 * NSEC_PER_MSEC;\n\n \nstatic int runtime_refresh_within(struct cfs_bandwidth *cfs_b, u64 min_expire)\n{\n\tstruct hrtimer *refresh_timer = &cfs_b->period_timer;\n\ts64 remaining;\n\n\t \n\tif (hrtimer_callback_running(refresh_timer))\n\t\treturn 1;\n\n\t \n\tremaining = ktime_to_ns(hrtimer_expires_remaining(refresh_timer));\n\tif (remaining < (s64)min_expire)\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic void start_cfs_slack_bandwidth(struct cfs_bandwidth *cfs_b)\n{\n\tu64 min_left = cfs_bandwidth_slack_period + min_bandwidth_expiration;\n\n\t \n\tif (runtime_refresh_within(cfs_b, min_left))\n\t\treturn;\n\n\t \n\tif (cfs_b->slack_started)\n\t\treturn;\n\tcfs_b->slack_started = true;\n\n\thrtimer_start(&cfs_b->slack_timer,\n\t\t\tns_to_ktime(cfs_bandwidth_slack_period),\n\t\t\tHRTIMER_MODE_REL);\n}\n\n \nstatic void __return_cfs_rq_runtime(struct cfs_rq *cfs_rq)\n{\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);\n\ts64 slack_runtime = cfs_rq->runtime_remaining - min_cfs_rq_runtime;\n\n\tif (slack_runtime <= 0)\n\t\treturn;\n\n\traw_spin_lock(&cfs_b->lock);\n\tif (cfs_b->quota != RUNTIME_INF) {\n\t\tcfs_b->runtime += slack_runtime;\n\n\t\t \n\t\tif (cfs_b->runtime > sched_cfs_bandwidth_slice() &&\n\t\t    !list_empty(&cfs_b->throttled_cfs_rq))\n\t\t\tstart_cfs_slack_bandwidth(cfs_b);\n\t}\n\traw_spin_unlock(&cfs_b->lock);\n\n\t \n\tcfs_rq->runtime_remaining -= slack_runtime;\n}\n\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq)\n{\n\tif (!cfs_bandwidth_used())\n\t\treturn;\n\n\tif (!cfs_rq->runtime_enabled || cfs_rq->nr_running)\n\t\treturn;\n\n\t__return_cfs_rq_runtime(cfs_rq);\n}\n\n \nstatic void do_sched_cfs_slack_timer(struct cfs_bandwidth *cfs_b)\n{\n\tu64 runtime = 0, slice = sched_cfs_bandwidth_slice();\n\tunsigned long flags;\n\n\t \n\traw_spin_lock_irqsave(&cfs_b->lock, flags);\n\tcfs_b->slack_started = false;\n\n\tif (runtime_refresh_within(cfs_b, min_bandwidth_expiration)) {\n\t\traw_spin_unlock_irqrestore(&cfs_b->lock, flags);\n\t\treturn;\n\t}\n\n\tif (cfs_b->quota != RUNTIME_INF && cfs_b->runtime > slice)\n\t\truntime = cfs_b->runtime;\n\n\traw_spin_unlock_irqrestore(&cfs_b->lock, flags);\n\n\tif (!runtime)\n\t\treturn;\n\n\tdistribute_cfs_runtime(cfs_b);\n}\n\n \nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq)\n{\n\tif (!cfs_bandwidth_used())\n\t\treturn;\n\n\t \n\tif (!cfs_rq->runtime_enabled || cfs_rq->curr)\n\t\treturn;\n\n\t \n\tif (cfs_rq_throttled(cfs_rq))\n\t\treturn;\n\n\t \n\taccount_cfs_rq_runtime(cfs_rq, 0);\n\tif (cfs_rq->runtime_remaining <= 0)\n\t\tthrottle_cfs_rq(cfs_rq);\n}\n\nstatic void sync_throttle(struct task_group *tg, int cpu)\n{\n\tstruct cfs_rq *pcfs_rq, *cfs_rq;\n\n\tif (!cfs_bandwidth_used())\n\t\treturn;\n\n\tif (!tg->parent)\n\t\treturn;\n\n\tcfs_rq = tg->cfs_rq[cpu];\n\tpcfs_rq = tg->parent->cfs_rq[cpu];\n\n\tcfs_rq->throttle_count = pcfs_rq->throttle_count;\n\tcfs_rq->throttled_clock_pelt = rq_clock_pelt(cpu_rq(cpu));\n}\n\n \nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq)\n{\n\tif (!cfs_bandwidth_used())\n\t\treturn false;\n\n\tif (likely(!cfs_rq->runtime_enabled || cfs_rq->runtime_remaining > 0))\n\t\treturn false;\n\n\t \n\tif (cfs_rq_throttled(cfs_rq))\n\t\treturn true;\n\n\treturn throttle_cfs_rq(cfs_rq);\n}\n\nstatic enum hrtimer_restart sched_cfs_slack_timer(struct hrtimer *timer)\n{\n\tstruct cfs_bandwidth *cfs_b =\n\t\tcontainer_of(timer, struct cfs_bandwidth, slack_timer);\n\n\tdo_sched_cfs_slack_timer(cfs_b);\n\n\treturn HRTIMER_NORESTART;\n}\n\nextern const u64 max_cfs_quota_period;\n\nstatic enum hrtimer_restart sched_cfs_period_timer(struct hrtimer *timer)\n{\n\tstruct cfs_bandwidth *cfs_b =\n\t\tcontainer_of(timer, struct cfs_bandwidth, period_timer);\n\tunsigned long flags;\n\tint overrun;\n\tint idle = 0;\n\tint count = 0;\n\n\traw_spin_lock_irqsave(&cfs_b->lock, flags);\n\tfor (;;) {\n\t\toverrun = hrtimer_forward_now(timer, cfs_b->period);\n\t\tif (!overrun)\n\t\t\tbreak;\n\n\t\tidle = do_sched_cfs_period_timer(cfs_b, overrun, flags);\n\n\t\tif (++count > 3) {\n\t\t\tu64 new, old = ktime_to_ns(cfs_b->period);\n\n\t\t\t \n\t\t\tnew = old * 2;\n\t\t\tif (new < max_cfs_quota_period) {\n\t\t\t\tcfs_b->period = ns_to_ktime(new);\n\t\t\t\tcfs_b->quota *= 2;\n\t\t\t\tcfs_b->burst *= 2;\n\n\t\t\t\tpr_warn_ratelimited(\n\t\"cfs_period_timer[cpu%d]: period too short, scaling up (new cfs_period_us = %lld, cfs_quota_us = %lld)\\n\",\n\t\t\t\t\tsmp_processor_id(),\n\t\t\t\t\tdiv_u64(new, NSEC_PER_USEC),\n\t\t\t\t\tdiv_u64(cfs_b->quota, NSEC_PER_USEC));\n\t\t\t} else {\n\t\t\t\tpr_warn_ratelimited(\n\t\"cfs_period_timer[cpu%d]: period too short, but cannot scale up without losing precision (cfs_period_us = %lld, cfs_quota_us = %lld)\\n\",\n\t\t\t\t\tsmp_processor_id(),\n\t\t\t\t\tdiv_u64(old, NSEC_PER_USEC),\n\t\t\t\t\tdiv_u64(cfs_b->quota, NSEC_PER_USEC));\n\t\t\t}\n\n\t\t\t \n\t\t\tcount = 0;\n\t\t}\n\t}\n\tif (idle)\n\t\tcfs_b->period_active = 0;\n\traw_spin_unlock_irqrestore(&cfs_b->lock, flags);\n\n\treturn idle ? HRTIMER_NORESTART : HRTIMER_RESTART;\n}\n\nvoid init_cfs_bandwidth(struct cfs_bandwidth *cfs_b, struct cfs_bandwidth *parent)\n{\n\traw_spin_lock_init(&cfs_b->lock);\n\tcfs_b->runtime = 0;\n\tcfs_b->quota = RUNTIME_INF;\n\tcfs_b->period = ns_to_ktime(default_cfs_period());\n\tcfs_b->burst = 0;\n\tcfs_b->hierarchical_quota = parent ? parent->hierarchical_quota : RUNTIME_INF;\n\n\tINIT_LIST_HEAD(&cfs_b->throttled_cfs_rq);\n\thrtimer_init(&cfs_b->period_timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS_PINNED);\n\tcfs_b->period_timer.function = sched_cfs_period_timer;\n\n\t \n\thrtimer_set_expires(&cfs_b->period_timer,\n\t\t\t    get_random_u32_below(cfs_b->period));\n\thrtimer_init(&cfs_b->slack_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);\n\tcfs_b->slack_timer.function = sched_cfs_slack_timer;\n\tcfs_b->slack_started = false;\n}\n\nstatic void init_cfs_rq_runtime(struct cfs_rq *cfs_rq)\n{\n\tcfs_rq->runtime_enabled = 0;\n\tINIT_LIST_HEAD(&cfs_rq->throttled_list);\n#ifdef CONFIG_SMP\n\tINIT_LIST_HEAD(&cfs_rq->throttled_csd_list);\n#endif\n}\n\nvoid start_cfs_bandwidth(struct cfs_bandwidth *cfs_b)\n{\n\tlockdep_assert_held(&cfs_b->lock);\n\n\tif (cfs_b->period_active)\n\t\treturn;\n\n\tcfs_b->period_active = 1;\n\thrtimer_forward_now(&cfs_b->period_timer, cfs_b->period);\n\thrtimer_start_expires(&cfs_b->period_timer, HRTIMER_MODE_ABS_PINNED);\n}\n\nstatic void destroy_cfs_bandwidth(struct cfs_bandwidth *cfs_b)\n{\n\tint __maybe_unused i;\n\n\t \n\tif (!cfs_b->throttled_cfs_rq.next)\n\t\treturn;\n\n\thrtimer_cancel(&cfs_b->period_timer);\n\thrtimer_cancel(&cfs_b->slack_timer);\n\n\t \n#ifdef CONFIG_SMP\n\tfor_each_possible_cpu(i) {\n\t\tstruct rq *rq = cpu_rq(i);\n\t\tunsigned long flags;\n\n\t\tif (list_empty(&rq->cfsb_csd_list))\n\t\t\tcontinue;\n\n\t\tlocal_irq_save(flags);\n\t\t__cfsb_csd_unthrottle(rq);\n\t\tlocal_irq_restore(flags);\n\t}\n#endif\n}\n\n \n\n \nstatic void __maybe_unused update_runtime_enabled(struct rq *rq)\n{\n\tstruct task_group *tg;\n\n\tlockdep_assert_rq_held(rq);\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(tg, &task_groups, list) {\n\t\tstruct cfs_bandwidth *cfs_b = &tg->cfs_bandwidth;\n\t\tstruct cfs_rq *cfs_rq = tg->cfs_rq[cpu_of(rq)];\n\n\t\traw_spin_lock(&cfs_b->lock);\n\t\tcfs_rq->runtime_enabled = cfs_b->quota != RUNTIME_INF;\n\t\traw_spin_unlock(&cfs_b->lock);\n\t}\n\trcu_read_unlock();\n}\n\n \nstatic void __maybe_unused unthrottle_offline_cfs_rqs(struct rq *rq)\n{\n\tstruct task_group *tg;\n\n\tlockdep_assert_rq_held(rq);\n\n\t \n\trq_clock_start_loop_update(rq);\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(tg, &task_groups, list) {\n\t\tstruct cfs_rq *cfs_rq = tg->cfs_rq[cpu_of(rq)];\n\n\t\tif (!cfs_rq->runtime_enabled)\n\t\t\tcontinue;\n\n\t\t \n\t\tcfs_rq->runtime_remaining = 1;\n\t\t \n\t\tcfs_rq->runtime_enabled = 0;\n\n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tunthrottle_cfs_rq(cfs_rq);\n\t}\n\trcu_read_unlock();\n\n\trq_clock_stop_loop_update(rq);\n}\n\nbool cfs_task_bw_constrained(struct task_struct *p)\n{\n\tstruct cfs_rq *cfs_rq = task_cfs_rq(p);\n\n\tif (!cfs_bandwidth_used())\n\t\treturn false;\n\n\tif (cfs_rq->runtime_enabled ||\n\t    tg_cfs_bandwidth(cfs_rq->tg)->hierarchical_quota != RUNTIME_INF)\n\t\treturn true;\n\n\treturn false;\n}\n\n#ifdef CONFIG_NO_HZ_FULL\n \nstatic void sched_fair_update_stop_tick(struct rq *rq, struct task_struct *p)\n{\n\tint cpu = cpu_of(rq);\n\n\tif (!sched_feat(HZ_BW) || !cfs_bandwidth_used())\n\t\treturn;\n\n\tif (!tick_nohz_full_cpu(cpu))\n\t\treturn;\n\n\tif (rq->nr_running != 1)\n\t\treturn;\n\n\t \n\tif (cfs_task_bw_constrained(p))\n\t\ttick_nohz_dep_set_cpu(cpu, TICK_DEP_BIT_SCHED);\n}\n#endif\n\n#else  \n\nstatic inline bool cfs_bandwidth_used(void)\n{\n\treturn false;\n}\n\nstatic void account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec) {}\nstatic bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq) { return false; }\nstatic void check_enqueue_throttle(struct cfs_rq *cfs_rq) {}\nstatic inline void sync_throttle(struct task_group *tg, int cpu) {}\nstatic __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq) {}\n\nstatic inline int cfs_rq_throttled(struct cfs_rq *cfs_rq)\n{\n\treturn 0;\n}\n\nstatic inline int throttled_hierarchy(struct cfs_rq *cfs_rq)\n{\n\treturn 0;\n}\n\nstatic inline int throttled_lb_pair(struct task_group *tg,\n\t\t\t\t    int src_cpu, int dest_cpu)\n{\n\treturn 0;\n}\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\nvoid init_cfs_bandwidth(struct cfs_bandwidth *cfs_b, struct cfs_bandwidth *parent) {}\nstatic void init_cfs_rq_runtime(struct cfs_rq *cfs_rq) {}\n#endif\n\nstatic inline struct cfs_bandwidth *tg_cfs_bandwidth(struct task_group *tg)\n{\n\treturn NULL;\n}\nstatic inline void destroy_cfs_bandwidth(struct cfs_bandwidth *cfs_b) {}\nstatic inline void update_runtime_enabled(struct rq *rq) {}\nstatic inline void unthrottle_offline_cfs_rqs(struct rq *rq) {}\n#ifdef CONFIG_CGROUP_SCHED\nbool cfs_task_bw_constrained(struct task_struct *p)\n{\n\treturn false;\n}\n#endif\n#endif  \n\n#if !defined(CONFIG_CFS_BANDWIDTH) || !defined(CONFIG_NO_HZ_FULL)\nstatic inline void sched_fair_update_stop_tick(struct rq *rq, struct task_struct *p) {}\n#endif\n\n \n\n#ifdef CONFIG_SCHED_HRTICK\nstatic void hrtick_start_fair(struct rq *rq, struct task_struct *p)\n{\n\tstruct sched_entity *se = &p->se;\n\n\tSCHED_WARN_ON(task_rq(p) != rq);\n\n\tif (rq->cfs.h_nr_running > 1) {\n\t\tu64 ran = se->sum_exec_runtime - se->prev_sum_exec_runtime;\n\t\tu64 slice = se->slice;\n\t\ts64 delta = slice - ran;\n\n\t\tif (delta < 0) {\n\t\t\tif (task_current(rq, p))\n\t\t\t\tresched_curr(rq);\n\t\t\treturn;\n\t\t}\n\t\thrtick_start(rq, delta);\n\t}\n}\n\n \nstatic void hrtick_update(struct rq *rq)\n{\n\tstruct task_struct *curr = rq->curr;\n\n\tif (!hrtick_enabled_fair(rq) || curr->sched_class != &fair_sched_class)\n\t\treturn;\n\n\thrtick_start_fair(rq, curr);\n}\n#else  \nstatic inline void\nhrtick_start_fair(struct rq *rq, struct task_struct *p)\n{\n}\n\nstatic inline void hrtick_update(struct rq *rq)\n{\n}\n#endif\n\n#ifdef CONFIG_SMP\nstatic inline bool cpu_overutilized(int cpu)\n{\n\tunsigned long rq_util_min = uclamp_rq_get(cpu_rq(cpu), UCLAMP_MIN);\n\tunsigned long rq_util_max = uclamp_rq_get(cpu_rq(cpu), UCLAMP_MAX);\n\n\t \n\treturn !util_fits_cpu(cpu_util_cfs(cpu), rq_util_min, rq_util_max, cpu);\n}\n\nstatic inline void update_overutilized_status(struct rq *rq)\n{\n\tif (!READ_ONCE(rq->rd->overutilized) && cpu_overutilized(rq->cpu)) {\n\t\tWRITE_ONCE(rq->rd->overutilized, SG_OVERUTILIZED);\n\t\ttrace_sched_overutilized_tp(rq->rd, SG_OVERUTILIZED);\n\t}\n}\n#else\nstatic inline void update_overutilized_status(struct rq *rq) { }\n#endif\n\n \nstatic int sched_idle_rq(struct rq *rq)\n{\n\treturn unlikely(rq->nr_running == rq->cfs.idle_h_nr_running &&\n\t\t\trq->nr_running);\n}\n\n#ifdef CONFIG_SMP\nstatic int sched_idle_cpu(int cpu)\n{\n\treturn sched_idle_rq(cpu_rq(cpu));\n}\n#endif\n\n \nstatic void\nenqueue_task_fair(struct rq *rq, struct task_struct *p, int flags)\n{\n\tstruct cfs_rq *cfs_rq;\n\tstruct sched_entity *se = &p->se;\n\tint idle_h_nr_running = task_has_idle_policy(p);\n\tint task_new = !(flags & ENQUEUE_WAKEUP);\n\n\t \n\tutil_est_enqueue(&rq->cfs, p);\n\n\t \n\tif (p->in_iowait)\n\t\tcpufreq_update_util(rq, SCHED_CPUFREQ_IOWAIT);\n\n\tfor_each_sched_entity(se) {\n\t\tif (se->on_rq)\n\t\t\tbreak;\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tenqueue_entity(cfs_rq, se, flags);\n\n\t\tcfs_rq->h_nr_running++;\n\t\tcfs_rq->idle_h_nr_running += idle_h_nr_running;\n\n\t\tif (cfs_rq_is_idle(cfs_rq))\n\t\t\tidle_h_nr_running = 1;\n\n\t\t \n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tgoto enqueue_throttle;\n\n\t\tflags = ENQUEUE_WAKEUP;\n\t}\n\n\tfor_each_sched_entity(se) {\n\t\tcfs_rq = cfs_rq_of(se);\n\n\t\tupdate_load_avg(cfs_rq, se, UPDATE_TG);\n\t\tse_update_runnable(se);\n\t\tupdate_cfs_group(se);\n\n\t\tcfs_rq->h_nr_running++;\n\t\tcfs_rq->idle_h_nr_running += idle_h_nr_running;\n\n\t\tif (cfs_rq_is_idle(cfs_rq))\n\t\t\tidle_h_nr_running = 1;\n\n\t\t \n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tgoto enqueue_throttle;\n\t}\n\n\t \n\tadd_nr_running(rq, 1);\n\n\t \n\tif (!task_new)\n\t\tupdate_overutilized_status(rq);\n\nenqueue_throttle:\n\tassert_list_leaf_cfs_rq(rq);\n\n\thrtick_update(rq);\n}\n\nstatic void set_next_buddy(struct sched_entity *se);\n\n \nstatic void dequeue_task_fair(struct rq *rq, struct task_struct *p, int flags)\n{\n\tstruct cfs_rq *cfs_rq;\n\tstruct sched_entity *se = &p->se;\n\tint task_sleep = flags & DEQUEUE_SLEEP;\n\tint idle_h_nr_running = task_has_idle_policy(p);\n\tbool was_sched_idle = sched_idle_rq(rq);\n\n\tutil_est_dequeue(&rq->cfs, p);\n\n\tfor_each_sched_entity(se) {\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tdequeue_entity(cfs_rq, se, flags);\n\n\t\tcfs_rq->h_nr_running--;\n\t\tcfs_rq->idle_h_nr_running -= idle_h_nr_running;\n\n\t\tif (cfs_rq_is_idle(cfs_rq))\n\t\t\tidle_h_nr_running = 1;\n\n\t\t \n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tgoto dequeue_throttle;\n\n\t\t \n\t\tif (cfs_rq->load.weight) {\n\t\t\t \n\t\t\tse = parent_entity(se);\n\t\t\t \n\t\t\tif (task_sleep && se && !throttled_hierarchy(cfs_rq))\n\t\t\t\tset_next_buddy(se);\n\t\t\tbreak;\n\t\t}\n\t\tflags |= DEQUEUE_SLEEP;\n\t}\n\n\tfor_each_sched_entity(se) {\n\t\tcfs_rq = cfs_rq_of(se);\n\n\t\tupdate_load_avg(cfs_rq, se, UPDATE_TG);\n\t\tse_update_runnable(se);\n\t\tupdate_cfs_group(se);\n\n\t\tcfs_rq->h_nr_running--;\n\t\tcfs_rq->idle_h_nr_running -= idle_h_nr_running;\n\n\t\tif (cfs_rq_is_idle(cfs_rq))\n\t\t\tidle_h_nr_running = 1;\n\n\t\t \n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tgoto dequeue_throttle;\n\n\t}\n\n\t \n\tsub_nr_running(rq, 1);\n\n\t \n\tif (unlikely(!was_sched_idle && sched_idle_rq(rq)))\n\t\trq->next_balance = jiffies;\n\ndequeue_throttle:\n\tutil_est_update(&rq->cfs, p, task_sleep);\n\thrtick_update(rq);\n}\n\n#ifdef CONFIG_SMP\n\n \nstatic DEFINE_PER_CPU(cpumask_var_t, load_balance_mask);\nstatic DEFINE_PER_CPU(cpumask_var_t, select_rq_mask);\nstatic DEFINE_PER_CPU(cpumask_var_t, should_we_balance_tmpmask);\n\n#ifdef CONFIG_NO_HZ_COMMON\n\nstatic struct {\n\tcpumask_var_t idle_cpus_mask;\n\tatomic_t nr_cpus;\n\tint has_blocked;\t\t \n\tint needs_update;\t\t \n\tunsigned long next_balance;      \n\tunsigned long next_blocked;\t \n} nohz ____cacheline_aligned;\n\n#endif  \n\nstatic unsigned long cpu_load(struct rq *rq)\n{\n\treturn cfs_rq_load_avg(&rq->cfs);\n}\n\n \nstatic unsigned long cpu_load_without(struct rq *rq, struct task_struct *p)\n{\n\tstruct cfs_rq *cfs_rq;\n\tunsigned int load;\n\n\t \n\tif (cpu_of(rq) != task_cpu(p) || !READ_ONCE(p->se.avg.last_update_time))\n\t\treturn cpu_load(rq);\n\n\tcfs_rq = &rq->cfs;\n\tload = READ_ONCE(cfs_rq->avg.load_avg);\n\n\t \n\tlsub_positive(&load, task_h_load(p));\n\n\treturn load;\n}\n\nstatic unsigned long cpu_runnable(struct rq *rq)\n{\n\treturn cfs_rq_runnable_avg(&rq->cfs);\n}\n\nstatic unsigned long cpu_runnable_without(struct rq *rq, struct task_struct *p)\n{\n\tstruct cfs_rq *cfs_rq;\n\tunsigned int runnable;\n\n\t \n\tif (cpu_of(rq) != task_cpu(p) || !READ_ONCE(p->se.avg.last_update_time))\n\t\treturn cpu_runnable(rq);\n\n\tcfs_rq = &rq->cfs;\n\trunnable = READ_ONCE(cfs_rq->avg.runnable_avg);\n\n\t \n\tlsub_positive(&runnable, p->se.avg.runnable_avg);\n\n\treturn runnable;\n}\n\nstatic unsigned long capacity_of(int cpu)\n{\n\treturn cpu_rq(cpu)->cpu_capacity;\n}\n\nstatic void record_wakee(struct task_struct *p)\n{\n\t \n\tif (time_after(jiffies, current->wakee_flip_decay_ts + HZ)) {\n\t\tcurrent->wakee_flips >>= 1;\n\t\tcurrent->wakee_flip_decay_ts = jiffies;\n\t}\n\n\tif (current->last_wakee != p) {\n\t\tcurrent->last_wakee = p;\n\t\tcurrent->wakee_flips++;\n\t}\n}\n\n \nstatic int wake_wide(struct task_struct *p)\n{\n\tunsigned int master = current->wakee_flips;\n\tunsigned int slave = p->wakee_flips;\n\tint factor = __this_cpu_read(sd_llc_size);\n\n\tif (master < slave)\n\t\tswap(master, slave);\n\tif (slave < factor || master < slave * factor)\n\t\treturn 0;\n\treturn 1;\n}\n\n \nstatic int\nwake_affine_idle(int this_cpu, int prev_cpu, int sync)\n{\n\t \n\tif (available_idle_cpu(this_cpu) && cpus_share_cache(this_cpu, prev_cpu))\n\t\treturn available_idle_cpu(prev_cpu) ? prev_cpu : this_cpu;\n\n\tif (sync && cpu_rq(this_cpu)->nr_running == 1)\n\t\treturn this_cpu;\n\n\tif (available_idle_cpu(prev_cpu))\n\t\treturn prev_cpu;\n\n\treturn nr_cpumask_bits;\n}\n\nstatic int\nwake_affine_weight(struct sched_domain *sd, struct task_struct *p,\n\t\t   int this_cpu, int prev_cpu, int sync)\n{\n\ts64 this_eff_load, prev_eff_load;\n\tunsigned long task_load;\n\n\tthis_eff_load = cpu_load(cpu_rq(this_cpu));\n\n\tif (sync) {\n\t\tunsigned long current_load = task_h_load(current);\n\n\t\tif (current_load > this_eff_load)\n\t\t\treturn this_cpu;\n\n\t\tthis_eff_load -= current_load;\n\t}\n\n\ttask_load = task_h_load(p);\n\n\tthis_eff_load += task_load;\n\tif (sched_feat(WA_BIAS))\n\t\tthis_eff_load *= 100;\n\tthis_eff_load *= capacity_of(prev_cpu);\n\n\tprev_eff_load = cpu_load(cpu_rq(prev_cpu));\n\tprev_eff_load -= task_load;\n\tif (sched_feat(WA_BIAS))\n\t\tprev_eff_load *= 100 + (sd->imbalance_pct - 100) / 2;\n\tprev_eff_load *= capacity_of(this_cpu);\n\n\t \n\tif (sync)\n\t\tprev_eff_load += 1;\n\n\treturn this_eff_load < prev_eff_load ? this_cpu : nr_cpumask_bits;\n}\n\nstatic int wake_affine(struct sched_domain *sd, struct task_struct *p,\n\t\t       int this_cpu, int prev_cpu, int sync)\n{\n\tint target = nr_cpumask_bits;\n\n\tif (sched_feat(WA_IDLE))\n\t\ttarget = wake_affine_idle(this_cpu, prev_cpu, sync);\n\n\tif (sched_feat(WA_WEIGHT) && target == nr_cpumask_bits)\n\t\ttarget = wake_affine_weight(sd, p, this_cpu, prev_cpu, sync);\n\n\tschedstat_inc(p->stats.nr_wakeups_affine_attempts);\n\tif (target != this_cpu)\n\t\treturn prev_cpu;\n\n\tschedstat_inc(sd->ttwu_move_affine);\n\tschedstat_inc(p->stats.nr_wakeups_affine);\n\treturn target;\n}\n\nstatic struct sched_group *\nfind_idlest_group(struct sched_domain *sd, struct task_struct *p, int this_cpu);\n\n \nstatic int\nfind_idlest_group_cpu(struct sched_group *group, struct task_struct *p, int this_cpu)\n{\n\tunsigned long load, min_load = ULONG_MAX;\n\tunsigned int min_exit_latency = UINT_MAX;\n\tu64 latest_idle_timestamp = 0;\n\tint least_loaded_cpu = this_cpu;\n\tint shallowest_idle_cpu = -1;\n\tint i;\n\n\t \n\tif (group->group_weight == 1)\n\t\treturn cpumask_first(sched_group_span(group));\n\n\t \n\tfor_each_cpu_and(i, sched_group_span(group), p->cpus_ptr) {\n\t\tstruct rq *rq = cpu_rq(i);\n\n\t\tif (!sched_core_cookie_match(rq, p))\n\t\t\tcontinue;\n\n\t\tif (sched_idle_cpu(i))\n\t\t\treturn i;\n\n\t\tif (available_idle_cpu(i)) {\n\t\t\tstruct cpuidle_state *idle = idle_get_state(rq);\n\t\t\tif (idle && idle->exit_latency < min_exit_latency) {\n\t\t\t\t \n\t\t\t\tmin_exit_latency = idle->exit_latency;\n\t\t\t\tlatest_idle_timestamp = rq->idle_stamp;\n\t\t\t\tshallowest_idle_cpu = i;\n\t\t\t} else if ((!idle || idle->exit_latency == min_exit_latency) &&\n\t\t\t\t   rq->idle_stamp > latest_idle_timestamp) {\n\t\t\t\t \n\t\t\t\tlatest_idle_timestamp = rq->idle_stamp;\n\t\t\t\tshallowest_idle_cpu = i;\n\t\t\t}\n\t\t} else if (shallowest_idle_cpu == -1) {\n\t\t\tload = cpu_load(cpu_rq(i));\n\t\t\tif (load < min_load) {\n\t\t\t\tmin_load = load;\n\t\t\t\tleast_loaded_cpu = i;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn shallowest_idle_cpu != -1 ? shallowest_idle_cpu : least_loaded_cpu;\n}\n\nstatic inline int find_idlest_cpu(struct sched_domain *sd, struct task_struct *p,\n\t\t\t\t  int cpu, int prev_cpu, int sd_flag)\n{\n\tint new_cpu = cpu;\n\n\tif (!cpumask_intersects(sched_domain_span(sd), p->cpus_ptr))\n\t\treturn prev_cpu;\n\n\t \n\tif (!(sd_flag & SD_BALANCE_FORK))\n\t\tsync_entity_load_avg(&p->se);\n\n\twhile (sd) {\n\t\tstruct sched_group *group;\n\t\tstruct sched_domain *tmp;\n\t\tint weight;\n\n\t\tif (!(sd->flags & sd_flag)) {\n\t\t\tsd = sd->child;\n\t\t\tcontinue;\n\t\t}\n\n\t\tgroup = find_idlest_group(sd, p, cpu);\n\t\tif (!group) {\n\t\t\tsd = sd->child;\n\t\t\tcontinue;\n\t\t}\n\n\t\tnew_cpu = find_idlest_group_cpu(group, p, cpu);\n\t\tif (new_cpu == cpu) {\n\t\t\t \n\t\t\tsd = sd->child;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tcpu = new_cpu;\n\t\tweight = sd->span_weight;\n\t\tsd = NULL;\n\t\tfor_each_domain(cpu, tmp) {\n\t\t\tif (weight <= tmp->span_weight)\n\t\t\t\tbreak;\n\t\t\tif (tmp->flags & sd_flag)\n\t\t\t\tsd = tmp;\n\t\t}\n\t}\n\n\treturn new_cpu;\n}\n\nstatic inline int __select_idle_cpu(int cpu, struct task_struct *p)\n{\n\tif ((available_idle_cpu(cpu) || sched_idle_cpu(cpu)) &&\n\t    sched_cpu_cookie_match(cpu_rq(cpu), p))\n\t\treturn cpu;\n\n\treturn -1;\n}\n\n#ifdef CONFIG_SCHED_SMT\nDEFINE_STATIC_KEY_FALSE(sched_smt_present);\nEXPORT_SYMBOL_GPL(sched_smt_present);\n\nstatic inline void set_idle_cores(int cpu, int val)\n{\n\tstruct sched_domain_shared *sds;\n\n\tsds = rcu_dereference(per_cpu(sd_llc_shared, cpu));\n\tif (sds)\n\t\tWRITE_ONCE(sds->has_idle_cores, val);\n}\n\nstatic inline bool test_idle_cores(int cpu)\n{\n\tstruct sched_domain_shared *sds;\n\n\tsds = rcu_dereference(per_cpu(sd_llc_shared, cpu));\n\tif (sds)\n\t\treturn READ_ONCE(sds->has_idle_cores);\n\n\treturn false;\n}\n\n \nvoid __update_idle_core(struct rq *rq)\n{\n\tint core = cpu_of(rq);\n\tint cpu;\n\n\trcu_read_lock();\n\tif (test_idle_cores(core))\n\t\tgoto unlock;\n\n\tfor_each_cpu(cpu, cpu_smt_mask(core)) {\n\t\tif (cpu == core)\n\t\t\tcontinue;\n\n\t\tif (!available_idle_cpu(cpu))\n\t\t\tgoto unlock;\n\t}\n\n\tset_idle_cores(core, 1);\nunlock:\n\trcu_read_unlock();\n}\n\n \nstatic int select_idle_core(struct task_struct *p, int core, struct cpumask *cpus, int *idle_cpu)\n{\n\tbool idle = true;\n\tint cpu;\n\n\tfor_each_cpu(cpu, cpu_smt_mask(core)) {\n\t\tif (!available_idle_cpu(cpu)) {\n\t\t\tidle = false;\n\t\t\tif (*idle_cpu == -1) {\n\t\t\t\tif (sched_idle_cpu(cpu) && cpumask_test_cpu(cpu, p->cpus_ptr)) {\n\t\t\t\t\t*idle_cpu = cpu;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\tif (*idle_cpu == -1 && cpumask_test_cpu(cpu, p->cpus_ptr))\n\t\t\t*idle_cpu = cpu;\n\t}\n\n\tif (idle)\n\t\treturn core;\n\n\tcpumask_andnot(cpus, cpus, cpu_smt_mask(core));\n\treturn -1;\n}\n\n \nstatic int select_idle_smt(struct task_struct *p, int target)\n{\n\tint cpu;\n\n\tfor_each_cpu_and(cpu, cpu_smt_mask(target), p->cpus_ptr) {\n\t\tif (cpu == target)\n\t\t\tcontinue;\n\t\tif (available_idle_cpu(cpu) || sched_idle_cpu(cpu))\n\t\t\treturn cpu;\n\t}\n\n\treturn -1;\n}\n\n#else  \n\nstatic inline void set_idle_cores(int cpu, int val)\n{\n}\n\nstatic inline bool test_idle_cores(int cpu)\n{\n\treturn false;\n}\n\nstatic inline int select_idle_core(struct task_struct *p, int core, struct cpumask *cpus, int *idle_cpu)\n{\n\treturn __select_idle_cpu(core, p);\n}\n\nstatic inline int select_idle_smt(struct task_struct *p, int target)\n{\n\treturn -1;\n}\n\n#endif  \n\n \nstatic int select_idle_cpu(struct task_struct *p, struct sched_domain *sd, bool has_idle_core, int target)\n{\n\tstruct cpumask *cpus = this_cpu_cpumask_var_ptr(select_rq_mask);\n\tint i, cpu, idle_cpu = -1, nr = INT_MAX;\n\tstruct sched_domain_shared *sd_share;\n\tstruct rq *this_rq = this_rq();\n\tint this = smp_processor_id();\n\tstruct sched_domain *this_sd = NULL;\n\tu64 time = 0;\n\n\tcpumask_and(cpus, sched_domain_span(sd), p->cpus_ptr);\n\n\tif (sched_feat(SIS_PROP) && !has_idle_core) {\n\t\tu64 avg_cost, avg_idle, span_avg;\n\t\tunsigned long now = jiffies;\n\n\t\tthis_sd = rcu_dereference(*this_cpu_ptr(&sd_llc));\n\t\tif (!this_sd)\n\t\t\treturn -1;\n\n\t\t \n\t\tif (unlikely(this_rq->wake_stamp < now)) {\n\t\t\twhile (this_rq->wake_stamp < now && this_rq->wake_avg_idle) {\n\t\t\t\tthis_rq->wake_stamp++;\n\t\t\t\tthis_rq->wake_avg_idle >>= 1;\n\t\t\t}\n\t\t}\n\n\t\tavg_idle = this_rq->wake_avg_idle;\n\t\tavg_cost = this_sd->avg_scan_cost + 1;\n\n\t\tspan_avg = sd->span_weight * avg_idle;\n\t\tif (span_avg > 4*avg_cost)\n\t\t\tnr = div_u64(span_avg, avg_cost);\n\t\telse\n\t\t\tnr = 4;\n\n\t\ttime = cpu_clock(this);\n\t}\n\n\tif (sched_feat(SIS_UTIL)) {\n\t\tsd_share = rcu_dereference(per_cpu(sd_llc_shared, target));\n\t\tif (sd_share) {\n\t\t\t \n\t\t\tnr = READ_ONCE(sd_share->nr_idle_scan) + 1;\n\t\t\t \n\t\t\tif (nr == 1)\n\t\t\t\treturn -1;\n\t\t}\n\t}\n\n\tfor_each_cpu_wrap(cpu, cpus, target + 1) {\n\t\tif (has_idle_core) {\n\t\t\ti = select_idle_core(p, cpu, cpus, &idle_cpu);\n\t\t\tif ((unsigned int)i < nr_cpumask_bits)\n\t\t\t\treturn i;\n\n\t\t} else {\n\t\t\tif (!--nr)\n\t\t\t\treturn -1;\n\t\t\tidle_cpu = __select_idle_cpu(cpu, p);\n\t\t\tif ((unsigned int)idle_cpu < nr_cpumask_bits)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (has_idle_core)\n\t\tset_idle_cores(target, false);\n\n\tif (sched_feat(SIS_PROP) && this_sd && !has_idle_core) {\n\t\ttime = cpu_clock(this) - time;\n\n\t\t \n\t\tthis_rq->wake_avg_idle -= min(this_rq->wake_avg_idle, time);\n\n\t\tupdate_avg(&this_sd->avg_scan_cost, time);\n\t}\n\n\treturn idle_cpu;\n}\n\n \nstatic int\nselect_idle_capacity(struct task_struct *p, struct sched_domain *sd, int target)\n{\n\tunsigned long task_util, util_min, util_max, best_cap = 0;\n\tint fits, best_fits = 0;\n\tint cpu, best_cpu = -1;\n\tstruct cpumask *cpus;\n\n\tcpus = this_cpu_cpumask_var_ptr(select_rq_mask);\n\tcpumask_and(cpus, sched_domain_span(sd), p->cpus_ptr);\n\n\ttask_util = task_util_est(p);\n\tutil_min = uclamp_eff_value(p, UCLAMP_MIN);\n\tutil_max = uclamp_eff_value(p, UCLAMP_MAX);\n\n\tfor_each_cpu_wrap(cpu, cpus, target) {\n\t\tunsigned long cpu_cap = capacity_of(cpu);\n\n\t\tif (!available_idle_cpu(cpu) && !sched_idle_cpu(cpu))\n\t\t\tcontinue;\n\n\t\tfits = util_fits_cpu(task_util, util_min, util_max, cpu);\n\n\t\t \n\t\tif (fits > 0)\n\t\t\treturn cpu;\n\t\t \n\t\telse if (fits < 0)\n\t\t\tcpu_cap = capacity_orig_of(cpu) - thermal_load_avg(cpu_rq(cpu));\n\n\t\t \n\t\tif ((fits < best_fits) ||\n\t\t    ((fits == best_fits) && (cpu_cap > best_cap))) {\n\t\t\tbest_cap = cpu_cap;\n\t\t\tbest_cpu = cpu;\n\t\t\tbest_fits = fits;\n\t\t}\n\t}\n\n\treturn best_cpu;\n}\n\nstatic inline bool asym_fits_cpu(unsigned long util,\n\t\t\t\t unsigned long util_min,\n\t\t\t\t unsigned long util_max,\n\t\t\t\t int cpu)\n{\n\tif (sched_asym_cpucap_active())\n\t\t \n\t\treturn (util_fits_cpu(util, util_min, util_max, cpu) > 0);\n\n\treturn true;\n}\n\n \nstatic int select_idle_sibling(struct task_struct *p, int prev, int target)\n{\n\tbool has_idle_core = false;\n\tstruct sched_domain *sd;\n\tunsigned long task_util, util_min, util_max;\n\tint i, recent_used_cpu;\n\n\t \n\tif (sched_asym_cpucap_active()) {\n\t\tsync_entity_load_avg(&p->se);\n\t\ttask_util = task_util_est(p);\n\t\tutil_min = uclamp_eff_value(p, UCLAMP_MIN);\n\t\tutil_max = uclamp_eff_value(p, UCLAMP_MAX);\n\t}\n\n\t \n\tlockdep_assert_irqs_disabled();\n\n\tif ((available_idle_cpu(target) || sched_idle_cpu(target)) &&\n\t    asym_fits_cpu(task_util, util_min, util_max, target))\n\t\treturn target;\n\n\t \n\tif (prev != target && cpus_share_cache(prev, target) &&\n\t    (available_idle_cpu(prev) || sched_idle_cpu(prev)) &&\n\t    asym_fits_cpu(task_util, util_min, util_max, prev))\n\t\treturn prev;\n\n\t \n\tif (is_per_cpu_kthread(current) &&\n\t    in_task() &&\n\t    prev == smp_processor_id() &&\n\t    this_rq()->nr_running <= 1 &&\n\t    asym_fits_cpu(task_util, util_min, util_max, prev)) {\n\t\treturn prev;\n\t}\n\n\t \n\trecent_used_cpu = p->recent_used_cpu;\n\tp->recent_used_cpu = prev;\n\tif (recent_used_cpu != prev &&\n\t    recent_used_cpu != target &&\n\t    cpus_share_cache(recent_used_cpu, target) &&\n\t    (available_idle_cpu(recent_used_cpu) || sched_idle_cpu(recent_used_cpu)) &&\n\t    cpumask_test_cpu(recent_used_cpu, p->cpus_ptr) &&\n\t    asym_fits_cpu(task_util, util_min, util_max, recent_used_cpu)) {\n\t\treturn recent_used_cpu;\n\t}\n\n\t \n\tif (sched_asym_cpucap_active()) {\n\t\tsd = rcu_dereference(per_cpu(sd_asym_cpucapacity, target));\n\t\t \n\t\tif (sd) {\n\t\t\ti = select_idle_capacity(p, sd, target);\n\t\t\treturn ((unsigned)i < nr_cpumask_bits) ? i : target;\n\t\t}\n\t}\n\n\tsd = rcu_dereference(per_cpu(sd_llc, target));\n\tif (!sd)\n\t\treturn target;\n\n\tif (sched_smt_active()) {\n\t\thas_idle_core = test_idle_cores(target);\n\n\t\tif (!has_idle_core && cpus_share_cache(prev, target)) {\n\t\t\ti = select_idle_smt(p, prev);\n\t\t\tif ((unsigned int)i < nr_cpumask_bits)\n\t\t\t\treturn i;\n\t\t}\n\t}\n\n\ti = select_idle_cpu(p, sd, has_idle_core, target);\n\tif ((unsigned)i < nr_cpumask_bits)\n\t\treturn i;\n\n\treturn target;\n}\n\n \nstatic unsigned long\ncpu_util(int cpu, struct task_struct *p, int dst_cpu, int boost)\n{\n\tstruct cfs_rq *cfs_rq = &cpu_rq(cpu)->cfs;\n\tunsigned long util = READ_ONCE(cfs_rq->avg.util_avg);\n\tunsigned long runnable;\n\n\tif (boost) {\n\t\trunnable = READ_ONCE(cfs_rq->avg.runnable_avg);\n\t\tutil = max(util, runnable);\n\t}\n\n\t \n\tif (p && task_cpu(p) == cpu && dst_cpu != cpu)\n\t\tlsub_positive(&util, task_util(p));\n\telse if (p && task_cpu(p) != cpu && dst_cpu == cpu)\n\t\tutil += task_util(p);\n\n\tif (sched_feat(UTIL_EST)) {\n\t\tunsigned long util_est;\n\n\t\tutil_est = READ_ONCE(cfs_rq->avg.util_est.enqueued);\n\n\t\t \n\t\tif (dst_cpu == cpu)\n\t\t\tutil_est += _task_util_est(p);\n\t\telse if (p && unlikely(task_on_rq_queued(p) || current == p))\n\t\t\tlsub_positive(&util_est, _task_util_est(p));\n\n\t\tutil = max(util, util_est);\n\t}\n\n\treturn min(util, capacity_orig_of(cpu));\n}\n\nunsigned long cpu_util_cfs(int cpu)\n{\n\treturn cpu_util(cpu, NULL, -1, 0);\n}\n\nunsigned long cpu_util_cfs_boost(int cpu)\n{\n\treturn cpu_util(cpu, NULL, -1, 1);\n}\n\n \nstatic unsigned long cpu_util_without(int cpu, struct task_struct *p)\n{\n\t \n\tif (cpu != task_cpu(p) || !READ_ONCE(p->se.avg.last_update_time))\n\t\tp = NULL;\n\n\treturn cpu_util(cpu, p, -1, 0);\n}\n\n \nstruct energy_env {\n\tunsigned long task_busy_time;\n\tunsigned long pd_busy_time;\n\tunsigned long cpu_cap;\n\tunsigned long pd_cap;\n};\n\n \nstatic inline void eenv_task_busy_time(struct energy_env *eenv,\n\t\t\t\t       struct task_struct *p, int prev_cpu)\n{\n\tunsigned long busy_time, max_cap = arch_scale_cpu_capacity(prev_cpu);\n\tunsigned long irq = cpu_util_irq(cpu_rq(prev_cpu));\n\n\tif (unlikely(irq >= max_cap))\n\t\tbusy_time = max_cap;\n\telse\n\t\tbusy_time = scale_irq_capacity(task_util_est(p), irq, max_cap);\n\n\teenv->task_busy_time = busy_time;\n}\n\n \nstatic inline void eenv_pd_busy_time(struct energy_env *eenv,\n\t\t\t\t     struct cpumask *pd_cpus,\n\t\t\t\t     struct task_struct *p)\n{\n\tunsigned long busy_time = 0;\n\tint cpu;\n\n\tfor_each_cpu(cpu, pd_cpus) {\n\t\tunsigned long util = cpu_util(cpu, p, -1, 0);\n\n\t\tbusy_time += effective_cpu_util(cpu, util, ENERGY_UTIL, NULL);\n\t}\n\n\teenv->pd_busy_time = min(eenv->pd_cap, busy_time);\n}\n\n \nstatic inline unsigned long\neenv_pd_max_util(struct energy_env *eenv, struct cpumask *pd_cpus,\n\t\t struct task_struct *p, int dst_cpu)\n{\n\tunsigned long max_util = 0;\n\tint cpu;\n\n\tfor_each_cpu(cpu, pd_cpus) {\n\t\tstruct task_struct *tsk = (cpu == dst_cpu) ? p : NULL;\n\t\tunsigned long util = cpu_util(cpu, p, dst_cpu, 1);\n\t\tunsigned long eff_util;\n\n\t\t \n\t\teff_util = effective_cpu_util(cpu, util, FREQUENCY_UTIL, tsk);\n\t\tmax_util = max(max_util, eff_util);\n\t}\n\n\treturn min(max_util, eenv->cpu_cap);\n}\n\n \nstatic inline unsigned long\ncompute_energy(struct energy_env *eenv, struct perf_domain *pd,\n\t       struct cpumask *pd_cpus, struct task_struct *p, int dst_cpu)\n{\n\tunsigned long max_util = eenv_pd_max_util(eenv, pd_cpus, p, dst_cpu);\n\tunsigned long busy_time = eenv->pd_busy_time;\n\n\tif (dst_cpu >= 0)\n\t\tbusy_time = min(eenv->pd_cap, busy_time + eenv->task_busy_time);\n\n\treturn em_cpu_energy(pd->em_pd, max_util, busy_time, eenv->cpu_cap);\n}\n\n \nstatic int find_energy_efficient_cpu(struct task_struct *p, int prev_cpu)\n{\n\tstruct cpumask *cpus = this_cpu_cpumask_var_ptr(select_rq_mask);\n\tunsigned long prev_delta = ULONG_MAX, best_delta = ULONG_MAX;\n\tunsigned long p_util_min = uclamp_is_used() ? uclamp_eff_value(p, UCLAMP_MIN) : 0;\n\tunsigned long p_util_max = uclamp_is_used() ? uclamp_eff_value(p, UCLAMP_MAX) : 1024;\n\tstruct root_domain *rd = this_rq()->rd;\n\tint cpu, best_energy_cpu, target = -1;\n\tint prev_fits = -1, best_fits = -1;\n\tunsigned long best_thermal_cap = 0;\n\tunsigned long prev_thermal_cap = 0;\n\tstruct sched_domain *sd;\n\tstruct perf_domain *pd;\n\tstruct energy_env eenv;\n\n\trcu_read_lock();\n\tpd = rcu_dereference(rd->pd);\n\tif (!pd || READ_ONCE(rd->overutilized))\n\t\tgoto unlock;\n\n\t \n\tsd = rcu_dereference(*this_cpu_ptr(&sd_asym_cpucapacity));\n\twhile (sd && !cpumask_test_cpu(prev_cpu, sched_domain_span(sd)))\n\t\tsd = sd->parent;\n\tif (!sd)\n\t\tgoto unlock;\n\n\ttarget = prev_cpu;\n\n\tsync_entity_load_avg(&p->se);\n\tif (!task_util_est(p) && p_util_min == 0)\n\t\tgoto unlock;\n\n\teenv_task_busy_time(&eenv, p, prev_cpu);\n\n\tfor (; pd; pd = pd->next) {\n\t\tunsigned long util_min = p_util_min, util_max = p_util_max;\n\t\tunsigned long cpu_cap, cpu_thermal_cap, util;\n\t\tlong prev_spare_cap = -1, max_spare_cap = -1;\n\t\tunsigned long rq_util_min, rq_util_max;\n\t\tunsigned long cur_delta, base_energy;\n\t\tint max_spare_cap_cpu = -1;\n\t\tint fits, max_fits = -1;\n\n\t\tcpumask_and(cpus, perf_domain_span(pd), cpu_online_mask);\n\n\t\tif (cpumask_empty(cpus))\n\t\t\tcontinue;\n\n\t\t \n\t\tcpu = cpumask_first(cpus);\n\t\tcpu_thermal_cap = arch_scale_cpu_capacity(cpu);\n\t\tcpu_thermal_cap -= arch_scale_thermal_pressure(cpu);\n\n\t\teenv.cpu_cap = cpu_thermal_cap;\n\t\teenv.pd_cap = 0;\n\n\t\tfor_each_cpu(cpu, cpus) {\n\t\t\tstruct rq *rq = cpu_rq(cpu);\n\n\t\t\teenv.pd_cap += cpu_thermal_cap;\n\n\t\t\tif (!cpumask_test_cpu(cpu, sched_domain_span(sd)))\n\t\t\t\tcontinue;\n\n\t\t\tif (!cpumask_test_cpu(cpu, p->cpus_ptr))\n\t\t\t\tcontinue;\n\n\t\t\tutil = cpu_util(cpu, p, cpu, 0);\n\t\t\tcpu_cap = capacity_of(cpu);\n\n\t\t\t \n\t\t\tif (uclamp_is_used() && !uclamp_rq_is_idle(rq)) {\n\t\t\t\t \n\t\t\t\trq_util_min = uclamp_rq_get(rq, UCLAMP_MIN);\n\t\t\t\trq_util_max = uclamp_rq_get(rq, UCLAMP_MAX);\n\n\t\t\t\tutil_min = max(rq_util_min, p_util_min);\n\t\t\t\tutil_max = max(rq_util_max, p_util_max);\n\t\t\t}\n\n\t\t\tfits = util_fits_cpu(util, util_min, util_max, cpu);\n\t\t\tif (!fits)\n\t\t\t\tcontinue;\n\n\t\t\tlsub_positive(&cpu_cap, util);\n\n\t\t\tif (cpu == prev_cpu) {\n\t\t\t\t \n\t\t\t\tprev_spare_cap = cpu_cap;\n\t\t\t\tprev_fits = fits;\n\t\t\t} else if ((fits > max_fits) ||\n\t\t\t\t   ((fits == max_fits) && ((long)cpu_cap > max_spare_cap))) {\n\t\t\t\t \n\t\t\t\tmax_spare_cap = cpu_cap;\n\t\t\t\tmax_spare_cap_cpu = cpu;\n\t\t\t\tmax_fits = fits;\n\t\t\t}\n\t\t}\n\n\t\tif (max_spare_cap_cpu < 0 && prev_spare_cap < 0)\n\t\t\tcontinue;\n\n\t\teenv_pd_busy_time(&eenv, cpus, p);\n\t\t \n\t\tbase_energy = compute_energy(&eenv, pd, cpus, p, -1);\n\n\t\t \n\t\tif (prev_spare_cap > -1) {\n\t\t\tprev_delta = compute_energy(&eenv, pd, cpus, p,\n\t\t\t\t\t\t    prev_cpu);\n\t\t\t \n\t\t\tif (prev_delta < base_energy)\n\t\t\t\tgoto unlock;\n\t\t\tprev_delta -= base_energy;\n\t\t\tprev_thermal_cap = cpu_thermal_cap;\n\t\t\tbest_delta = min(best_delta, prev_delta);\n\t\t}\n\n\t\t \n\t\tif (max_spare_cap_cpu >= 0 && max_spare_cap > prev_spare_cap) {\n\t\t\t \n\t\t\tif (max_fits < best_fits)\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tif ((max_fits < 0) &&\n\t\t\t    (cpu_thermal_cap <= best_thermal_cap))\n\t\t\t\tcontinue;\n\n\t\t\tcur_delta = compute_energy(&eenv, pd, cpus, p,\n\t\t\t\t\t\t   max_spare_cap_cpu);\n\t\t\t \n\t\t\tif (cur_delta < base_energy)\n\t\t\t\tgoto unlock;\n\t\t\tcur_delta -= base_energy;\n\n\t\t\t \n\t\t\tif ((max_fits > 0) && (best_fits > 0) &&\n\t\t\t    (cur_delta >= best_delta))\n\t\t\t\tcontinue;\n\n\t\t\tbest_delta = cur_delta;\n\t\t\tbest_energy_cpu = max_spare_cap_cpu;\n\t\t\tbest_fits = max_fits;\n\t\t\tbest_thermal_cap = cpu_thermal_cap;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\tif ((best_fits > prev_fits) ||\n\t    ((best_fits > 0) && (best_delta < prev_delta)) ||\n\t    ((best_fits < 0) && (best_thermal_cap > prev_thermal_cap)))\n\t\ttarget = best_energy_cpu;\n\n\treturn target;\n\nunlock:\n\trcu_read_unlock();\n\n\treturn target;\n}\n\n \nstatic int\nselect_task_rq_fair(struct task_struct *p, int prev_cpu, int wake_flags)\n{\n\tint sync = (wake_flags & WF_SYNC) && !(current->flags & PF_EXITING);\n\tstruct sched_domain *tmp, *sd = NULL;\n\tint cpu = smp_processor_id();\n\tint new_cpu = prev_cpu;\n\tint want_affine = 0;\n\t \n\tint sd_flag = wake_flags & 0xF;\n\n\t \n\tlockdep_assert_held(&p->pi_lock);\n\tif (wake_flags & WF_TTWU) {\n\t\trecord_wakee(p);\n\n\t\tif ((wake_flags & WF_CURRENT_CPU) &&\n\t\t    cpumask_test_cpu(cpu, p->cpus_ptr))\n\t\t\treturn cpu;\n\n\t\tif (sched_energy_enabled()) {\n\t\t\tnew_cpu = find_energy_efficient_cpu(p, prev_cpu);\n\t\t\tif (new_cpu >= 0)\n\t\t\t\treturn new_cpu;\n\t\t\tnew_cpu = prev_cpu;\n\t\t}\n\n\t\twant_affine = !wake_wide(p) && cpumask_test_cpu(cpu, p->cpus_ptr);\n\t}\n\n\trcu_read_lock();\n\tfor_each_domain(cpu, tmp) {\n\t\t \n\t\tif (want_affine && (tmp->flags & SD_WAKE_AFFINE) &&\n\t\t    cpumask_test_cpu(prev_cpu, sched_domain_span(tmp))) {\n\t\t\tif (cpu != prev_cpu)\n\t\t\t\tnew_cpu = wake_affine(tmp, p, cpu, prev_cpu, sync);\n\n\t\t\tsd = NULL;  \n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (tmp->flags & sd_flag)\n\t\t\tsd = tmp;\n\t\telse if (!want_affine)\n\t\t\tbreak;\n\t}\n\n\tif (unlikely(sd)) {\n\t\t \n\t\tnew_cpu = find_idlest_cpu(sd, p, cpu, prev_cpu, sd_flag);\n\t} else if (wake_flags & WF_TTWU) {  \n\t\t \n\t\tnew_cpu = select_idle_sibling(p, prev_cpu, new_cpu);\n\t}\n\trcu_read_unlock();\n\n\treturn new_cpu;\n}\n\n \nstatic void migrate_task_rq_fair(struct task_struct *p, int new_cpu)\n{\n\tstruct sched_entity *se = &p->se;\n\n\tif (!task_on_rq_migrating(p)) {\n\t\tremove_entity_load_avg(se);\n\n\t\t \n\t\tmigrate_se_pelt_lag(se);\n\t}\n\n\t \n\tse->avg.last_update_time = 0;\n\n\tupdate_scan_period(p, new_cpu);\n}\n\nstatic void task_dead_fair(struct task_struct *p)\n{\n\tremove_entity_load_avg(&p->se);\n}\n\nstatic int\nbalance_fair(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)\n{\n\tif (rq->nr_running)\n\t\treturn 1;\n\n\treturn newidle_balance(rq, rf) != 0;\n}\n#endif  \n\nstatic void set_next_buddy(struct sched_entity *se)\n{\n\tfor_each_sched_entity(se) {\n\t\tif (SCHED_WARN_ON(!se->on_rq))\n\t\t\treturn;\n\t\tif (se_is_idle(se))\n\t\t\treturn;\n\t\tcfs_rq_of(se)->next = se;\n\t}\n}\n\n \nstatic void check_preempt_wakeup(struct rq *rq, struct task_struct *p, int wake_flags)\n{\n\tstruct task_struct *curr = rq->curr;\n\tstruct sched_entity *se = &curr->se, *pse = &p->se;\n\tstruct cfs_rq *cfs_rq = task_cfs_rq(curr);\n\tint next_buddy_marked = 0;\n\tint cse_is_idle, pse_is_idle;\n\n\tif (unlikely(se == pse))\n\t\treturn;\n\n\t \n\tif (unlikely(throttled_hierarchy(cfs_rq_of(pse))))\n\t\treturn;\n\n\tif (sched_feat(NEXT_BUDDY) && !(wake_flags & WF_FORK)) {\n\t\tset_next_buddy(pse);\n\t\tnext_buddy_marked = 1;\n\t}\n\n\t \n\tif (test_tsk_need_resched(curr))\n\t\treturn;\n\n\t \n\tif (unlikely(task_has_idle_policy(curr)) &&\n\t    likely(!task_has_idle_policy(p)))\n\t\tgoto preempt;\n\n\t \n\tif (unlikely(p->policy != SCHED_NORMAL) || !sched_feat(WAKEUP_PREEMPTION))\n\t\treturn;\n\n\tfind_matching_se(&se, &pse);\n\tWARN_ON_ONCE(!pse);\n\n\tcse_is_idle = se_is_idle(se);\n\tpse_is_idle = se_is_idle(pse);\n\n\t \n\tif (cse_is_idle && !pse_is_idle)\n\t\tgoto preempt;\n\tif (cse_is_idle != pse_is_idle)\n\t\treturn;\n\n\tcfs_rq = cfs_rq_of(se);\n\tupdate_curr(cfs_rq);\n\n\t \n\tif (pick_eevdf(cfs_rq) == pse)\n\t\tgoto preempt;\n\n\treturn;\n\npreempt:\n\tresched_curr(rq);\n}\n\n#ifdef CONFIG_SMP\nstatic struct task_struct *pick_task_fair(struct rq *rq)\n{\n\tstruct sched_entity *se;\n\tstruct cfs_rq *cfs_rq;\n\nagain:\n\tcfs_rq = &rq->cfs;\n\tif (!cfs_rq->nr_running)\n\t\treturn NULL;\n\n\tdo {\n\t\tstruct sched_entity *curr = cfs_rq->curr;\n\n\t\t \n\t\tif (curr) {\n\t\t\tif (curr->on_rq)\n\t\t\t\tupdate_curr(cfs_rq);\n\t\t\telse\n\t\t\t\tcurr = NULL;\n\n\t\t\tif (unlikely(check_cfs_rq_runtime(cfs_rq)))\n\t\t\t\tgoto again;\n\t\t}\n\n\t\tse = pick_next_entity(cfs_rq, curr);\n\t\tcfs_rq = group_cfs_rq(se);\n\t} while (cfs_rq);\n\n\treturn task_of(se);\n}\n#endif\n\nstruct task_struct *\npick_next_task_fair(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)\n{\n\tstruct cfs_rq *cfs_rq = &rq->cfs;\n\tstruct sched_entity *se;\n\tstruct task_struct *p;\n\tint new_tasks;\n\nagain:\n\tif (!sched_fair_runnable(rq))\n\t\tgoto idle;\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tif (!prev || prev->sched_class != &fair_sched_class)\n\t\tgoto simple;\n\n\t \n\n\tdo {\n\t\tstruct sched_entity *curr = cfs_rq->curr;\n\n\t\t \n\t\tif (curr) {\n\t\t\tif (curr->on_rq)\n\t\t\t\tupdate_curr(cfs_rq);\n\t\t\telse\n\t\t\t\tcurr = NULL;\n\n\t\t\t \n\t\t\tif (unlikely(check_cfs_rq_runtime(cfs_rq))) {\n\t\t\t\tcfs_rq = &rq->cfs;\n\n\t\t\t\tif (!cfs_rq->nr_running)\n\t\t\t\t\tgoto idle;\n\n\t\t\t\tgoto simple;\n\t\t\t}\n\t\t}\n\n\t\tse = pick_next_entity(cfs_rq, curr);\n\t\tcfs_rq = group_cfs_rq(se);\n\t} while (cfs_rq);\n\n\tp = task_of(se);\n\n\t \n\tif (prev != p) {\n\t\tstruct sched_entity *pse = &prev->se;\n\n\t\twhile (!(cfs_rq = is_same_group(se, pse))) {\n\t\t\tint se_depth = se->depth;\n\t\t\tint pse_depth = pse->depth;\n\n\t\t\tif (se_depth <= pse_depth) {\n\t\t\t\tput_prev_entity(cfs_rq_of(pse), pse);\n\t\t\t\tpse = parent_entity(pse);\n\t\t\t}\n\t\t\tif (se_depth >= pse_depth) {\n\t\t\t\tset_next_entity(cfs_rq_of(se), se);\n\t\t\t\tse = parent_entity(se);\n\t\t\t}\n\t\t}\n\n\t\tput_prev_entity(cfs_rq, pse);\n\t\tset_next_entity(cfs_rq, se);\n\t}\n\n\tgoto done;\nsimple:\n#endif\n\tif (prev)\n\t\tput_prev_task(rq, prev);\n\n\tdo {\n\t\tse = pick_next_entity(cfs_rq, NULL);\n\t\tset_next_entity(cfs_rq, se);\n\t\tcfs_rq = group_cfs_rq(se);\n\t} while (cfs_rq);\n\n\tp = task_of(se);\n\ndone: __maybe_unused;\n#ifdef CONFIG_SMP\n\t \n\tlist_move(&p->se.group_node, &rq->cfs_tasks);\n#endif\n\n\tif (hrtick_enabled_fair(rq))\n\t\thrtick_start_fair(rq, p);\n\n\tupdate_misfit_status(p, rq);\n\tsched_fair_update_stop_tick(rq, p);\n\n\treturn p;\n\nidle:\n\tif (!rf)\n\t\treturn NULL;\n\n\tnew_tasks = newidle_balance(rq, rf);\n\n\t \n\tif (new_tasks < 0)\n\t\treturn RETRY_TASK;\n\n\tif (new_tasks > 0)\n\t\tgoto again;\n\n\t \n\tupdate_idle_rq_clock_pelt(rq);\n\n\treturn NULL;\n}\n\nstatic struct task_struct *__pick_next_task_fair(struct rq *rq)\n{\n\treturn pick_next_task_fair(rq, NULL, NULL);\n}\n\n \nstatic void put_prev_task_fair(struct rq *rq, struct task_struct *prev)\n{\n\tstruct sched_entity *se = &prev->se;\n\tstruct cfs_rq *cfs_rq;\n\n\tfor_each_sched_entity(se) {\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tput_prev_entity(cfs_rq, se);\n\t}\n}\n\n \nstatic void yield_task_fair(struct rq *rq)\n{\n\tstruct task_struct *curr = rq->curr;\n\tstruct cfs_rq *cfs_rq = task_cfs_rq(curr);\n\tstruct sched_entity *se = &curr->se;\n\n\t \n\tif (unlikely(rq->nr_running == 1))\n\t\treturn;\n\n\tclear_buddies(cfs_rq, se);\n\n\tupdate_rq_clock(rq);\n\t \n\tupdate_curr(cfs_rq);\n\t \n\trq_clock_skip_update(rq);\n\n\tse->deadline += calc_delta_fair(se->slice, se);\n}\n\nstatic bool yield_to_task_fair(struct rq *rq, struct task_struct *p)\n{\n\tstruct sched_entity *se = &p->se;\n\n\t \n\tif (!se->on_rq || throttled_hierarchy(cfs_rq_of(se)))\n\t\treturn false;\n\n\t \n\tset_next_buddy(se);\n\n\tyield_task_fair(rq);\n\n\treturn true;\n}\n\n#ifdef CONFIG_SMP\n \n\nstatic unsigned long __read_mostly max_load_balance_interval = HZ/10;\n\nenum fbq_type { regular, remote, all };\n\n \nenum group_type {\n\t \n\tgroup_has_spare = 0,\n\t \n\tgroup_fully_busy,\n\t \n\tgroup_misfit_task,\n\t \n\tgroup_smt_balance,\n\t \n\tgroup_asym_packing,\n\t \n\tgroup_imbalanced,\n\t \n\tgroup_overloaded\n};\n\nenum migration_type {\n\tmigrate_load = 0,\n\tmigrate_util,\n\tmigrate_task,\n\tmigrate_misfit\n};\n\n#define LBF_ALL_PINNED\t0x01\n#define LBF_NEED_BREAK\t0x02\n#define LBF_DST_PINNED  0x04\n#define LBF_SOME_PINNED\t0x08\n#define LBF_ACTIVE_LB\t0x10\n\nstruct lb_env {\n\tstruct sched_domain\t*sd;\n\n\tstruct rq\t\t*src_rq;\n\tint\t\t\tsrc_cpu;\n\n\tint\t\t\tdst_cpu;\n\tstruct rq\t\t*dst_rq;\n\n\tstruct cpumask\t\t*dst_grpmask;\n\tint\t\t\tnew_dst_cpu;\n\tenum cpu_idle_type\tidle;\n\tlong\t\t\timbalance;\n\t \n\tstruct cpumask\t\t*cpus;\n\n\tunsigned int\t\tflags;\n\n\tunsigned int\t\tloop;\n\tunsigned int\t\tloop_break;\n\tunsigned int\t\tloop_max;\n\n\tenum fbq_type\t\tfbq_type;\n\tenum migration_type\tmigration_type;\n\tstruct list_head\ttasks;\n};\n\n \nstatic int task_hot(struct task_struct *p, struct lb_env *env)\n{\n\ts64 delta;\n\n\tlockdep_assert_rq_held(env->src_rq);\n\n\tif (p->sched_class != &fair_sched_class)\n\t\treturn 0;\n\n\tif (unlikely(task_has_idle_policy(p)))\n\t\treturn 0;\n\n\t \n\tif (env->sd->flags & SD_SHARE_CPUCAPACITY)\n\t\treturn 0;\n\n\t \n\tif (sched_feat(CACHE_HOT_BUDDY) && env->dst_rq->nr_running &&\n\t    (&p->se == cfs_rq_of(&p->se)->next))\n\t\treturn 1;\n\n\tif (sysctl_sched_migration_cost == -1)\n\t\treturn 1;\n\n\t \n\tif (!sched_core_cookie_match(cpu_rq(env->dst_cpu), p))\n\t\treturn 1;\n\n\tif (sysctl_sched_migration_cost == 0)\n\t\treturn 0;\n\n\tdelta = rq_clock_task(env->src_rq) - p->se.exec_start;\n\n\treturn delta < (s64)sysctl_sched_migration_cost;\n}\n\n#ifdef CONFIG_NUMA_BALANCING\n \nstatic int migrate_degrades_locality(struct task_struct *p, struct lb_env *env)\n{\n\tstruct numa_group *numa_group = rcu_dereference(p->numa_group);\n\tunsigned long src_weight, dst_weight;\n\tint src_nid, dst_nid, dist;\n\n\tif (!static_branch_likely(&sched_numa_balancing))\n\t\treturn -1;\n\n\tif (!p->numa_faults || !(env->sd->flags & SD_NUMA))\n\t\treturn -1;\n\n\tsrc_nid = cpu_to_node(env->src_cpu);\n\tdst_nid = cpu_to_node(env->dst_cpu);\n\n\tif (src_nid == dst_nid)\n\t\treturn -1;\n\n\t \n\tif (src_nid == p->numa_preferred_nid) {\n\t\tif (env->src_rq->nr_running > env->src_rq->nr_preferred_running)\n\t\t\treturn 1;\n\t\telse\n\t\t\treturn -1;\n\t}\n\n\t \n\tif (dst_nid == p->numa_preferred_nid)\n\t\treturn 0;\n\n\t \n\tif (env->idle == CPU_IDLE)\n\t\treturn -1;\n\n\tdist = node_distance(src_nid, dst_nid);\n\tif (numa_group) {\n\t\tsrc_weight = group_weight(p, src_nid, dist);\n\t\tdst_weight = group_weight(p, dst_nid, dist);\n\t} else {\n\t\tsrc_weight = task_weight(p, src_nid, dist);\n\t\tdst_weight = task_weight(p, dst_nid, dist);\n\t}\n\n\treturn dst_weight < src_weight;\n}\n\n#else\nstatic inline int migrate_degrades_locality(struct task_struct *p,\n\t\t\t\t\t     struct lb_env *env)\n{\n\treturn -1;\n}\n#endif\n\n \nstatic\nint can_migrate_task(struct task_struct *p, struct lb_env *env)\n{\n\tint tsk_cache_hot;\n\n\tlockdep_assert_rq_held(env->src_rq);\n\n\t \n\tif (throttled_lb_pair(task_group(p), env->src_cpu, env->dst_cpu))\n\t\treturn 0;\n\n\t \n\tif (kthread_is_per_cpu(p))\n\t\treturn 0;\n\n\tif (!cpumask_test_cpu(env->dst_cpu, p->cpus_ptr)) {\n\t\tint cpu;\n\n\t\tschedstat_inc(p->stats.nr_failed_migrations_affine);\n\n\t\tenv->flags |= LBF_SOME_PINNED;\n\n\t\t \n\t\tif (env->idle == CPU_NEWLY_IDLE ||\n\t\t    env->flags & (LBF_DST_PINNED | LBF_ACTIVE_LB))\n\t\t\treturn 0;\n\n\t\t \n\t\tfor_each_cpu_and(cpu, env->dst_grpmask, env->cpus) {\n\t\t\tif (cpumask_test_cpu(cpu, p->cpus_ptr)) {\n\t\t\t\tenv->flags |= LBF_DST_PINNED;\n\t\t\t\tenv->new_dst_cpu = cpu;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\treturn 0;\n\t}\n\n\t \n\tenv->flags &= ~LBF_ALL_PINNED;\n\n\tif (task_on_cpu(env->src_rq, p)) {\n\t\tschedstat_inc(p->stats.nr_failed_migrations_running);\n\t\treturn 0;\n\t}\n\n\t \n\tif (env->flags & LBF_ACTIVE_LB)\n\t\treturn 1;\n\n\ttsk_cache_hot = migrate_degrades_locality(p, env);\n\tif (tsk_cache_hot == -1)\n\t\ttsk_cache_hot = task_hot(p, env);\n\n\tif (tsk_cache_hot <= 0 ||\n\t    env->sd->nr_balance_failed > env->sd->cache_nice_tries) {\n\t\tif (tsk_cache_hot == 1) {\n\t\t\tschedstat_inc(env->sd->lb_hot_gained[env->idle]);\n\t\t\tschedstat_inc(p->stats.nr_forced_migrations);\n\t\t}\n\t\treturn 1;\n\t}\n\n\tschedstat_inc(p->stats.nr_failed_migrations_hot);\n\treturn 0;\n}\n\n \nstatic void detach_task(struct task_struct *p, struct lb_env *env)\n{\n\tlockdep_assert_rq_held(env->src_rq);\n\n\tdeactivate_task(env->src_rq, p, DEQUEUE_NOCLOCK);\n\tset_task_cpu(p, env->dst_cpu);\n}\n\n \nstatic struct task_struct *detach_one_task(struct lb_env *env)\n{\n\tstruct task_struct *p;\n\n\tlockdep_assert_rq_held(env->src_rq);\n\n\tlist_for_each_entry_reverse(p,\n\t\t\t&env->src_rq->cfs_tasks, se.group_node) {\n\t\tif (!can_migrate_task(p, env))\n\t\t\tcontinue;\n\n\t\tdetach_task(p, env);\n\n\t\t \n\t\tschedstat_inc(env->sd->lb_gained[env->idle]);\n\t\treturn p;\n\t}\n\treturn NULL;\n}\n\n \nstatic int detach_tasks(struct lb_env *env)\n{\n\tstruct list_head *tasks = &env->src_rq->cfs_tasks;\n\tunsigned long util, load;\n\tstruct task_struct *p;\n\tint detached = 0;\n\n\tlockdep_assert_rq_held(env->src_rq);\n\n\t \n\tif (env->src_rq->nr_running <= 1) {\n\t\tenv->flags &= ~LBF_ALL_PINNED;\n\t\treturn 0;\n\t}\n\n\tif (env->imbalance <= 0)\n\t\treturn 0;\n\n\twhile (!list_empty(tasks)) {\n\t\t \n\t\tif (env->idle != CPU_NOT_IDLE && env->src_rq->nr_running <= 1)\n\t\t\tbreak;\n\n\t\tenv->loop++;\n\t\t \n\t\tif (env->loop > env->loop_max &&\n\t\t    !(env->flags & LBF_ALL_PINNED))\n\t\t\tbreak;\n\n\t\t \n\t\tif (env->loop > env->loop_break) {\n\t\t\tenv->loop_break += SCHED_NR_MIGRATE_BREAK;\n\t\t\tenv->flags |= LBF_NEED_BREAK;\n\t\t\tbreak;\n\t\t}\n\n\t\tp = list_last_entry(tasks, struct task_struct, se.group_node);\n\n\t\tif (!can_migrate_task(p, env))\n\t\t\tgoto next;\n\n\t\tswitch (env->migration_type) {\n\t\tcase migrate_load:\n\t\t\t \n\t\t\tload = max_t(unsigned long, task_h_load(p), 1);\n\n\t\t\tif (sched_feat(LB_MIN) &&\n\t\t\t    load < 16 && !env->sd->nr_balance_failed)\n\t\t\t\tgoto next;\n\n\t\t\t \n\t\t\tif (shr_bound(load, env->sd->nr_balance_failed) > env->imbalance)\n\t\t\t\tgoto next;\n\n\t\t\tenv->imbalance -= load;\n\t\t\tbreak;\n\n\t\tcase migrate_util:\n\t\t\tutil = task_util_est(p);\n\n\t\t\tif (util > env->imbalance)\n\t\t\t\tgoto next;\n\n\t\t\tenv->imbalance -= util;\n\t\t\tbreak;\n\n\t\tcase migrate_task:\n\t\t\tenv->imbalance--;\n\t\t\tbreak;\n\n\t\tcase migrate_misfit:\n\t\t\t \n\t\t\tif (task_fits_cpu(p, env->src_cpu))\n\t\t\t\tgoto next;\n\n\t\t\tenv->imbalance = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tdetach_task(p, env);\n\t\tlist_add(&p->se.group_node, &env->tasks);\n\n\t\tdetached++;\n\n#ifdef CONFIG_PREEMPTION\n\t\t \n\t\tif (env->idle == CPU_NEWLY_IDLE)\n\t\t\tbreak;\n#endif\n\n\t\t \n\t\tif (env->imbalance <= 0)\n\t\t\tbreak;\n\n\t\tcontinue;\nnext:\n\t\tlist_move(&p->se.group_node, tasks);\n\t}\n\n\t \n\tschedstat_add(env->sd->lb_gained[env->idle], detached);\n\n\treturn detached;\n}\n\n \nstatic void attach_task(struct rq *rq, struct task_struct *p)\n{\n\tlockdep_assert_rq_held(rq);\n\n\tWARN_ON_ONCE(task_rq(p) != rq);\n\tactivate_task(rq, p, ENQUEUE_NOCLOCK);\n\tcheck_preempt_curr(rq, p, 0);\n}\n\n \nstatic void attach_one_task(struct rq *rq, struct task_struct *p)\n{\n\tstruct rq_flags rf;\n\n\trq_lock(rq, &rf);\n\tupdate_rq_clock(rq);\n\tattach_task(rq, p);\n\trq_unlock(rq, &rf);\n}\n\n \nstatic void attach_tasks(struct lb_env *env)\n{\n\tstruct list_head *tasks = &env->tasks;\n\tstruct task_struct *p;\n\tstruct rq_flags rf;\n\n\trq_lock(env->dst_rq, &rf);\n\tupdate_rq_clock(env->dst_rq);\n\n\twhile (!list_empty(tasks)) {\n\t\tp = list_first_entry(tasks, struct task_struct, se.group_node);\n\t\tlist_del_init(&p->se.group_node);\n\n\t\tattach_task(env->dst_rq, p);\n\t}\n\n\trq_unlock(env->dst_rq, &rf);\n}\n\n#ifdef CONFIG_NO_HZ_COMMON\nstatic inline bool cfs_rq_has_blocked(struct cfs_rq *cfs_rq)\n{\n\tif (cfs_rq->avg.load_avg)\n\t\treturn true;\n\n\tif (cfs_rq->avg.util_avg)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic inline bool others_have_blocked(struct rq *rq)\n{\n\tif (READ_ONCE(rq->avg_rt.util_avg))\n\t\treturn true;\n\n\tif (READ_ONCE(rq->avg_dl.util_avg))\n\t\treturn true;\n\n\tif (thermal_load_avg(rq))\n\t\treturn true;\n\n#ifdef CONFIG_HAVE_SCHED_AVG_IRQ\n\tif (READ_ONCE(rq->avg_irq.util_avg))\n\t\treturn true;\n#endif\n\n\treturn false;\n}\n\nstatic inline void update_blocked_load_tick(struct rq *rq)\n{\n\tWRITE_ONCE(rq->last_blocked_load_update_tick, jiffies);\n}\n\nstatic inline void update_blocked_load_status(struct rq *rq, bool has_blocked)\n{\n\tif (!has_blocked)\n\t\trq->has_blocked_load = 0;\n}\n#else\nstatic inline bool cfs_rq_has_blocked(struct cfs_rq *cfs_rq) { return false; }\nstatic inline bool others_have_blocked(struct rq *rq) { return false; }\nstatic inline void update_blocked_load_tick(struct rq *rq) {}\nstatic inline void update_blocked_load_status(struct rq *rq, bool has_blocked) {}\n#endif\n\nstatic bool __update_blocked_others(struct rq *rq, bool *done)\n{\n\tconst struct sched_class *curr_class;\n\tu64 now = rq_clock_pelt(rq);\n\tunsigned long thermal_pressure;\n\tbool decayed;\n\n\t \n\tcurr_class = rq->curr->sched_class;\n\n\tthermal_pressure = arch_scale_thermal_pressure(cpu_of(rq));\n\n\tdecayed = update_rt_rq_load_avg(now, rq, curr_class == &rt_sched_class) |\n\t\t  update_dl_rq_load_avg(now, rq, curr_class == &dl_sched_class) |\n\t\t  update_thermal_load_avg(rq_clock_thermal(rq), rq, thermal_pressure) |\n\t\t  update_irq_load_avg(rq, 0);\n\n\tif (others_have_blocked(rq))\n\t\t*done = false;\n\n\treturn decayed;\n}\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\nstatic bool __update_blocked_fair(struct rq *rq, bool *done)\n{\n\tstruct cfs_rq *cfs_rq, *pos;\n\tbool decayed = false;\n\tint cpu = cpu_of(rq);\n\n\t \n\tfor_each_leaf_cfs_rq_safe(rq, cfs_rq, pos) {\n\t\tstruct sched_entity *se;\n\n\t\tif (update_cfs_rq_load_avg(cfs_rq_clock_pelt(cfs_rq), cfs_rq)) {\n\t\t\tupdate_tg_load_avg(cfs_rq);\n\n\t\t\tif (cfs_rq->nr_running == 0)\n\t\t\t\tupdate_idle_cfs_rq_clock_pelt(cfs_rq);\n\n\t\t\tif (cfs_rq == &rq->cfs)\n\t\t\t\tdecayed = true;\n\t\t}\n\n\t\t \n\t\tse = cfs_rq->tg->se[cpu];\n\t\tif (se && !skip_blocked_update(se))\n\t\t\tupdate_load_avg(cfs_rq_of(se), se, UPDATE_TG);\n\n\t\t \n\t\tif (cfs_rq_is_decayed(cfs_rq))\n\t\t\tlist_del_leaf_cfs_rq(cfs_rq);\n\n\t\t \n\t\tif (cfs_rq_has_blocked(cfs_rq))\n\t\t\t*done = false;\n\t}\n\n\treturn decayed;\n}\n\n \nstatic void update_cfs_rq_h_load(struct cfs_rq *cfs_rq)\n{\n\tstruct rq *rq = rq_of(cfs_rq);\n\tstruct sched_entity *se = cfs_rq->tg->se[cpu_of(rq)];\n\tunsigned long now = jiffies;\n\tunsigned long load;\n\n\tif (cfs_rq->last_h_load_update == now)\n\t\treturn;\n\n\tWRITE_ONCE(cfs_rq->h_load_next, NULL);\n\tfor_each_sched_entity(se) {\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tWRITE_ONCE(cfs_rq->h_load_next, se);\n\t\tif (cfs_rq->last_h_load_update == now)\n\t\t\tbreak;\n\t}\n\n\tif (!se) {\n\t\tcfs_rq->h_load = cfs_rq_load_avg(cfs_rq);\n\t\tcfs_rq->last_h_load_update = now;\n\t}\n\n\twhile ((se = READ_ONCE(cfs_rq->h_load_next)) != NULL) {\n\t\tload = cfs_rq->h_load;\n\t\tload = div64_ul(load * se->avg.load_avg,\n\t\t\tcfs_rq_load_avg(cfs_rq) + 1);\n\t\tcfs_rq = group_cfs_rq(se);\n\t\tcfs_rq->h_load = load;\n\t\tcfs_rq->last_h_load_update = now;\n\t}\n}\n\nstatic unsigned long task_h_load(struct task_struct *p)\n{\n\tstruct cfs_rq *cfs_rq = task_cfs_rq(p);\n\n\tupdate_cfs_rq_h_load(cfs_rq);\n\treturn div64_ul(p->se.avg.load_avg * cfs_rq->h_load,\n\t\t\tcfs_rq_load_avg(cfs_rq) + 1);\n}\n#else\nstatic bool __update_blocked_fair(struct rq *rq, bool *done)\n{\n\tstruct cfs_rq *cfs_rq = &rq->cfs;\n\tbool decayed;\n\n\tdecayed = update_cfs_rq_load_avg(cfs_rq_clock_pelt(cfs_rq), cfs_rq);\n\tif (cfs_rq_has_blocked(cfs_rq))\n\t\t*done = false;\n\n\treturn decayed;\n}\n\nstatic unsigned long task_h_load(struct task_struct *p)\n{\n\treturn p->se.avg.load_avg;\n}\n#endif\n\nstatic void update_blocked_averages(int cpu)\n{\n\tbool decayed = false, done = true;\n\tstruct rq *rq = cpu_rq(cpu);\n\tstruct rq_flags rf;\n\n\trq_lock_irqsave(rq, &rf);\n\tupdate_blocked_load_tick(rq);\n\tupdate_rq_clock(rq);\n\n\tdecayed |= __update_blocked_others(rq, &done);\n\tdecayed |= __update_blocked_fair(rq, &done);\n\n\tupdate_blocked_load_status(rq, !done);\n\tif (decayed)\n\t\tcpufreq_update_util(rq, 0);\n\trq_unlock_irqrestore(rq, &rf);\n}\n\n \n\n \nstruct sg_lb_stats {\n\tunsigned long avg_load;  \n\tunsigned long group_load;  \n\tunsigned long group_capacity;\n\tunsigned long group_util;  \n\tunsigned long group_runnable;  \n\tunsigned int sum_nr_running;  \n\tunsigned int sum_h_nr_running;  \n\tunsigned int idle_cpus;\n\tunsigned int group_weight;\n\tenum group_type group_type;\n\tunsigned int group_asym_packing;  \n\tunsigned int group_smt_balance;   \n\tunsigned long group_misfit_task_load;  \n#ifdef CONFIG_NUMA_BALANCING\n\tunsigned int nr_numa_running;\n\tunsigned int nr_preferred_running;\n#endif\n};\n\n \nstruct sd_lb_stats {\n\tstruct sched_group *busiest;\t \n\tstruct sched_group *local;\t \n\tunsigned long total_load;\t \n\tunsigned long total_capacity;\t \n\tunsigned long avg_load;\t \n\tunsigned int prefer_sibling;  \n\n\tstruct sg_lb_stats busiest_stat; \n\tstruct sg_lb_stats local_stat;\t \n};\n\nstatic inline void init_sd_lb_stats(struct sd_lb_stats *sds)\n{\n\t \n\t*sds = (struct sd_lb_stats){\n\t\t.busiest = NULL,\n\t\t.local = NULL,\n\t\t.total_load = 0UL,\n\t\t.total_capacity = 0UL,\n\t\t.busiest_stat = {\n\t\t\t.idle_cpus = UINT_MAX,\n\t\t\t.group_type = group_has_spare,\n\t\t},\n\t};\n}\n\nstatic unsigned long scale_rt_capacity(int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long max = arch_scale_cpu_capacity(cpu);\n\tunsigned long used, free;\n\tunsigned long irq;\n\n\tirq = cpu_util_irq(rq);\n\n\tif (unlikely(irq >= max))\n\t\treturn 1;\n\n\t \n\tused = READ_ONCE(rq->avg_rt.util_avg);\n\tused += READ_ONCE(rq->avg_dl.util_avg);\n\tused += thermal_load_avg(rq);\n\n\tif (unlikely(used >= max))\n\t\treturn 1;\n\n\tfree = max - used;\n\n\treturn scale_irq_capacity(free, irq, max);\n}\n\nstatic void update_cpu_capacity(struct sched_domain *sd, int cpu)\n{\n\tunsigned long capacity = scale_rt_capacity(cpu);\n\tstruct sched_group *sdg = sd->groups;\n\n\tcpu_rq(cpu)->cpu_capacity_orig = arch_scale_cpu_capacity(cpu);\n\n\tif (!capacity)\n\t\tcapacity = 1;\n\n\tcpu_rq(cpu)->cpu_capacity = capacity;\n\ttrace_sched_cpu_capacity_tp(cpu_rq(cpu));\n\n\tsdg->sgc->capacity = capacity;\n\tsdg->sgc->min_capacity = capacity;\n\tsdg->sgc->max_capacity = capacity;\n}\n\nvoid update_group_capacity(struct sched_domain *sd, int cpu)\n{\n\tstruct sched_domain *child = sd->child;\n\tstruct sched_group *group, *sdg = sd->groups;\n\tunsigned long capacity, min_capacity, max_capacity;\n\tunsigned long interval;\n\n\tinterval = msecs_to_jiffies(sd->balance_interval);\n\tinterval = clamp(interval, 1UL, max_load_balance_interval);\n\tsdg->sgc->next_update = jiffies + interval;\n\n\tif (!child) {\n\t\tupdate_cpu_capacity(sd, cpu);\n\t\treturn;\n\t}\n\n\tcapacity = 0;\n\tmin_capacity = ULONG_MAX;\n\tmax_capacity = 0;\n\n\tif (child->flags & SD_OVERLAP) {\n\t\t \n\n\t\tfor_each_cpu(cpu, sched_group_span(sdg)) {\n\t\t\tunsigned long cpu_cap = capacity_of(cpu);\n\n\t\t\tcapacity += cpu_cap;\n\t\t\tmin_capacity = min(cpu_cap, min_capacity);\n\t\t\tmax_capacity = max(cpu_cap, max_capacity);\n\t\t}\n\t} else  {\n\t\t \n\n\t\tgroup = child->groups;\n\t\tdo {\n\t\t\tstruct sched_group_capacity *sgc = group->sgc;\n\n\t\t\tcapacity += sgc->capacity;\n\t\t\tmin_capacity = min(sgc->min_capacity, min_capacity);\n\t\t\tmax_capacity = max(sgc->max_capacity, max_capacity);\n\t\t\tgroup = group->next;\n\t\t} while (group != child->groups);\n\t}\n\n\tsdg->sgc->capacity = capacity;\n\tsdg->sgc->min_capacity = min_capacity;\n\tsdg->sgc->max_capacity = max_capacity;\n}\n\n \nstatic inline int\ncheck_cpu_capacity(struct rq *rq, struct sched_domain *sd)\n{\n\treturn ((rq->cpu_capacity * sd->imbalance_pct) <\n\t\t\t\t(rq->cpu_capacity_orig * 100));\n}\n\n \nstatic inline int check_misfit_status(struct rq *rq, struct sched_domain *sd)\n{\n\treturn rq->misfit_task_load &&\n\t\t(rq->cpu_capacity_orig < rq->rd->max_cpu_capacity ||\n\t\t check_cpu_capacity(rq, sd));\n}\n\n \n\nstatic inline int sg_imbalanced(struct sched_group *group)\n{\n\treturn group->sgc->imbalance;\n}\n\n \nstatic inline bool\ngroup_has_capacity(unsigned int imbalance_pct, struct sg_lb_stats *sgs)\n{\n\tif (sgs->sum_nr_running < sgs->group_weight)\n\t\treturn true;\n\n\tif ((sgs->group_capacity * imbalance_pct) <\n\t\t\t(sgs->group_runnable * 100))\n\t\treturn false;\n\n\tif ((sgs->group_capacity * 100) >\n\t\t\t(sgs->group_util * imbalance_pct))\n\t\treturn true;\n\n\treturn false;\n}\n\n \nstatic inline bool\ngroup_is_overloaded(unsigned int imbalance_pct, struct sg_lb_stats *sgs)\n{\n\tif (sgs->sum_nr_running <= sgs->group_weight)\n\t\treturn false;\n\n\tif ((sgs->group_capacity * 100) <\n\t\t\t(sgs->group_util * imbalance_pct))\n\t\treturn true;\n\n\tif ((sgs->group_capacity * imbalance_pct) <\n\t\t\t(sgs->group_runnable * 100))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic inline enum\ngroup_type group_classify(unsigned int imbalance_pct,\n\t\t\t  struct sched_group *group,\n\t\t\t  struct sg_lb_stats *sgs)\n{\n\tif (group_is_overloaded(imbalance_pct, sgs))\n\t\treturn group_overloaded;\n\n\tif (sg_imbalanced(group))\n\t\treturn group_imbalanced;\n\n\tif (sgs->group_asym_packing)\n\t\treturn group_asym_packing;\n\n\tif (sgs->group_smt_balance)\n\t\treturn group_smt_balance;\n\n\tif (sgs->group_misfit_task_load)\n\t\treturn group_misfit_task;\n\n\tif (!group_has_capacity(imbalance_pct, sgs))\n\t\treturn group_fully_busy;\n\n\treturn group_has_spare;\n}\n\n \nstatic bool sched_use_asym_prio(struct sched_domain *sd, int cpu)\n{\n\tif (!sched_smt_active())\n\t\treturn true;\n\n\treturn sd->flags & SD_SHARE_CPUCAPACITY || is_core_idle(cpu);\n}\n\n \nstatic inline bool\nsched_asym(struct lb_env *env, struct sd_lb_stats *sds,  struct sg_lb_stats *sgs,\n\t   struct sched_group *group)\n{\n\t \n\tif (!sched_use_asym_prio(env->sd, env->dst_cpu))\n\t\treturn false;\n\n\t \n\tif (group->flags & SD_SHARE_CPUCAPACITY) {\n\t\tif (sgs->group_weight - sgs->idle_cpus != 1)\n\t\t\treturn false;\n\t}\n\n\treturn sched_asym_prefer(env->dst_cpu, group->asym_prefer_cpu);\n}\n\n \nstatic inline bool smt_vs_nonsmt_groups(struct sched_group *sg1,\n\t\t\t\t    struct sched_group *sg2)\n{\n\tif (!sg1 || !sg2)\n\t\treturn false;\n\n\treturn (sg1->flags & SD_SHARE_CPUCAPACITY) !=\n\t\t(sg2->flags & SD_SHARE_CPUCAPACITY);\n}\n\nstatic inline bool smt_balance(struct lb_env *env, struct sg_lb_stats *sgs,\n\t\t\t       struct sched_group *group)\n{\n\tif (env->idle == CPU_NOT_IDLE)\n\t\treturn false;\n\n\t \n\tif (group->flags & SD_SHARE_CPUCAPACITY &&\n\t    sgs->sum_h_nr_running > 1)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic inline long sibling_imbalance(struct lb_env *env,\n\t\t\t\t    struct sd_lb_stats *sds,\n\t\t\t\t    struct sg_lb_stats *busiest,\n\t\t\t\t    struct sg_lb_stats *local)\n{\n\tint ncores_busiest, ncores_local;\n\tlong imbalance;\n\n\tif (env->idle == CPU_NOT_IDLE || !busiest->sum_nr_running)\n\t\treturn 0;\n\n\tncores_busiest = sds->busiest->cores;\n\tncores_local = sds->local->cores;\n\n\tif (ncores_busiest == ncores_local) {\n\t\timbalance = busiest->sum_nr_running;\n\t\tlsub_positive(&imbalance, local->sum_nr_running);\n\t\treturn imbalance;\n\t}\n\n\t \n\timbalance = ncores_local * busiest->sum_nr_running;\n\tlsub_positive(&imbalance, ncores_busiest * local->sum_nr_running);\n\t \n\timbalance = 2 * imbalance + ncores_local + ncores_busiest;\n\timbalance /= ncores_local + ncores_busiest;\n\n\t \n\tif (imbalance <= 1 && local->sum_nr_running == 0 &&\n\t    busiest->sum_nr_running > 1)\n\t\timbalance = 2;\n\n\treturn imbalance;\n}\n\nstatic inline bool\nsched_reduced_capacity(struct rq *rq, struct sched_domain *sd)\n{\n\t \n\tif (rq->cfs.h_nr_running != 1)\n\t\treturn false;\n\n\treturn check_cpu_capacity(rq, sd);\n}\n\n \nstatic inline void update_sg_lb_stats(struct lb_env *env,\n\t\t\t\t      struct sd_lb_stats *sds,\n\t\t\t\t      struct sched_group *group,\n\t\t\t\t      struct sg_lb_stats *sgs,\n\t\t\t\t      int *sg_status)\n{\n\tint i, nr_running, local_group;\n\n\tmemset(sgs, 0, sizeof(*sgs));\n\n\tlocal_group = group == sds->local;\n\n\tfor_each_cpu_and(i, sched_group_span(group), env->cpus) {\n\t\tstruct rq *rq = cpu_rq(i);\n\t\tunsigned long load = cpu_load(rq);\n\n\t\tsgs->group_load += load;\n\t\tsgs->group_util += cpu_util_cfs(i);\n\t\tsgs->group_runnable += cpu_runnable(rq);\n\t\tsgs->sum_h_nr_running += rq->cfs.h_nr_running;\n\n\t\tnr_running = rq->nr_running;\n\t\tsgs->sum_nr_running += nr_running;\n\n\t\tif (nr_running > 1)\n\t\t\t*sg_status |= SG_OVERLOAD;\n\n\t\tif (cpu_overutilized(i))\n\t\t\t*sg_status |= SG_OVERUTILIZED;\n\n#ifdef CONFIG_NUMA_BALANCING\n\t\tsgs->nr_numa_running += rq->nr_numa_running;\n\t\tsgs->nr_preferred_running += rq->nr_preferred_running;\n#endif\n\t\t \n\t\tif (!nr_running && idle_cpu(i)) {\n\t\t\tsgs->idle_cpus++;\n\t\t\t \n\t\t\tcontinue;\n\t\t}\n\n\t\tif (local_group)\n\t\t\tcontinue;\n\n\t\tif (env->sd->flags & SD_ASYM_CPUCAPACITY) {\n\t\t\t \n\t\t\tif (sgs->group_misfit_task_load < rq->misfit_task_load) {\n\t\t\t\tsgs->group_misfit_task_load = rq->misfit_task_load;\n\t\t\t\t*sg_status |= SG_OVERLOAD;\n\t\t\t}\n\t\t} else if ((env->idle != CPU_NOT_IDLE) &&\n\t\t\t   sched_reduced_capacity(rq, env->sd)) {\n\t\t\t \n\t\t\tif (sgs->group_misfit_task_load < load)\n\t\t\t\tsgs->group_misfit_task_load = load;\n\t\t}\n\t}\n\n\tsgs->group_capacity = group->sgc->capacity;\n\n\tsgs->group_weight = group->group_weight;\n\n\t \n\tif (!local_group && env->sd->flags & SD_ASYM_PACKING &&\n\t    env->idle != CPU_NOT_IDLE && sgs->sum_h_nr_running &&\n\t    sched_asym(env, sds, sgs, group)) {\n\t\tsgs->group_asym_packing = 1;\n\t}\n\n\t \n\tif (!local_group && smt_balance(env, sgs, group))\n\t\tsgs->group_smt_balance = 1;\n\n\tsgs->group_type = group_classify(env->sd->imbalance_pct, group, sgs);\n\n\t \n\tif (sgs->group_type == group_overloaded)\n\t\tsgs->avg_load = (sgs->group_load * SCHED_CAPACITY_SCALE) /\n\t\t\t\tsgs->group_capacity;\n}\n\n \nstatic bool update_sd_pick_busiest(struct lb_env *env,\n\t\t\t\t   struct sd_lb_stats *sds,\n\t\t\t\t   struct sched_group *sg,\n\t\t\t\t   struct sg_lb_stats *sgs)\n{\n\tstruct sg_lb_stats *busiest = &sds->busiest_stat;\n\n\t \n\tif (!sgs->sum_h_nr_running)\n\t\treturn false;\n\n\t \n\tif ((env->sd->flags & SD_ASYM_CPUCAPACITY) &&\n\t    (sgs->group_type == group_misfit_task) &&\n\t    (!capacity_greater(capacity_of(env->dst_cpu), sg->sgc->max_capacity) ||\n\t     sds->local_stat.group_type != group_has_spare))\n\t\treturn false;\n\n\tif (sgs->group_type > busiest->group_type)\n\t\treturn true;\n\n\tif (sgs->group_type < busiest->group_type)\n\t\treturn false;\n\n\t \n\n\tswitch (sgs->group_type) {\n\tcase group_overloaded:\n\t\t \n\t\tif (sgs->avg_load <= busiest->avg_load)\n\t\t\treturn false;\n\t\tbreak;\n\n\tcase group_imbalanced:\n\t\t \n\t\treturn false;\n\n\tcase group_asym_packing:\n\t\t \n\t\tif (sched_asym_prefer(sg->asym_prefer_cpu, sds->busiest->asym_prefer_cpu))\n\t\t\treturn false;\n\t\tbreak;\n\n\tcase group_misfit_task:\n\t\t \n\t\tif (sgs->group_misfit_task_load < busiest->group_misfit_task_load)\n\t\t\treturn false;\n\t\tbreak;\n\n\tcase group_smt_balance:\n\t\t \n\t\tif (sgs->idle_cpus != 0 || busiest->idle_cpus != 0)\n\t\t\tgoto has_spare;\n\n\t\tfallthrough;\n\n\tcase group_fully_busy:\n\t\t \n\n\t\tif (sgs->avg_load < busiest->avg_load)\n\t\t\treturn false;\n\n\t\tif (sgs->avg_load == busiest->avg_load) {\n\t\t\t \n\t\t\tif (sds->busiest->flags & SD_SHARE_CPUCAPACITY)\n\t\t\t\treturn false;\n\t\t}\n\n\t\tbreak;\n\n\tcase group_has_spare:\n\t\t \n\t\tif (smt_vs_nonsmt_groups(sds->busiest, sg)) {\n\t\t\tif (sg->flags & SD_SHARE_CPUCAPACITY && sgs->sum_h_nr_running <= 1)\n\t\t\t\treturn false;\n\t\t\telse\n\t\t\t\treturn true;\n\t\t}\nhas_spare:\n\n\t\t \n\t\tif (sgs->idle_cpus > busiest->idle_cpus)\n\t\t\treturn false;\n\t\telse if ((sgs->idle_cpus == busiest->idle_cpus) &&\n\t\t\t (sgs->sum_nr_running <= busiest->sum_nr_running))\n\t\t\treturn false;\n\n\t\tbreak;\n\t}\n\n\t \n\tif ((env->sd->flags & SD_ASYM_CPUCAPACITY) &&\n\t    (sgs->group_type <= group_fully_busy) &&\n\t    (capacity_greater(sg->sgc->min_capacity, capacity_of(env->dst_cpu))))\n\t\treturn false;\n\n\treturn true;\n}\n\n#ifdef CONFIG_NUMA_BALANCING\nstatic inline enum fbq_type fbq_classify_group(struct sg_lb_stats *sgs)\n{\n\tif (sgs->sum_h_nr_running > sgs->nr_numa_running)\n\t\treturn regular;\n\tif (sgs->sum_h_nr_running > sgs->nr_preferred_running)\n\t\treturn remote;\n\treturn all;\n}\n\nstatic inline enum fbq_type fbq_classify_rq(struct rq *rq)\n{\n\tif (rq->nr_running > rq->nr_numa_running)\n\t\treturn regular;\n\tif (rq->nr_running > rq->nr_preferred_running)\n\t\treturn remote;\n\treturn all;\n}\n#else\nstatic inline enum fbq_type fbq_classify_group(struct sg_lb_stats *sgs)\n{\n\treturn all;\n}\n\nstatic inline enum fbq_type fbq_classify_rq(struct rq *rq)\n{\n\treturn regular;\n}\n#endif  \n\n\nstruct sg_lb_stats;\n\n \n\nstatic unsigned int task_running_on_cpu(int cpu, struct task_struct *p)\n{\n\t \n\tif (cpu != task_cpu(p) || !READ_ONCE(p->se.avg.last_update_time))\n\t\treturn 0;\n\n\tif (task_on_rq_queued(p))\n\t\treturn 1;\n\n\treturn 0;\n}\n\n \nstatic int idle_cpu_without(int cpu, struct task_struct *p)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\n\tif (rq->curr != rq->idle && rq->curr != p)\n\t\treturn 0;\n\n\t \n\n#ifdef CONFIG_SMP\n\tif (rq->ttwu_pending)\n\t\treturn 0;\n#endif\n\n\treturn 1;\n}\n\n \nstatic inline void update_sg_wakeup_stats(struct sched_domain *sd,\n\t\t\t\t\t  struct sched_group *group,\n\t\t\t\t\t  struct sg_lb_stats *sgs,\n\t\t\t\t\t  struct task_struct *p)\n{\n\tint i, nr_running;\n\n\tmemset(sgs, 0, sizeof(*sgs));\n\n\t \n\tif (sd->flags & SD_ASYM_CPUCAPACITY)\n\t\tsgs->group_misfit_task_load = 1;\n\n\tfor_each_cpu(i, sched_group_span(group)) {\n\t\tstruct rq *rq = cpu_rq(i);\n\t\tunsigned int local;\n\n\t\tsgs->group_load += cpu_load_without(rq, p);\n\t\tsgs->group_util += cpu_util_without(i, p);\n\t\tsgs->group_runnable += cpu_runnable_without(rq, p);\n\t\tlocal = task_running_on_cpu(i, p);\n\t\tsgs->sum_h_nr_running += rq->cfs.h_nr_running - local;\n\n\t\tnr_running = rq->nr_running - local;\n\t\tsgs->sum_nr_running += nr_running;\n\n\t\t \n\t\tif (!nr_running && idle_cpu_without(i, p))\n\t\t\tsgs->idle_cpus++;\n\n\t\t \n\t\tif (sd->flags & SD_ASYM_CPUCAPACITY &&\n\t\t    sgs->group_misfit_task_load &&\n\t\t    task_fits_cpu(p, i))\n\t\t\tsgs->group_misfit_task_load = 0;\n\n\t}\n\n\tsgs->group_capacity = group->sgc->capacity;\n\n\tsgs->group_weight = group->group_weight;\n\n\tsgs->group_type = group_classify(sd->imbalance_pct, group, sgs);\n\n\t \n\tif (sgs->group_type == group_fully_busy ||\n\t\tsgs->group_type == group_overloaded)\n\t\tsgs->avg_load = (sgs->group_load * SCHED_CAPACITY_SCALE) /\n\t\t\t\tsgs->group_capacity;\n}\n\nstatic bool update_pick_idlest(struct sched_group *idlest,\n\t\t\t       struct sg_lb_stats *idlest_sgs,\n\t\t\t       struct sched_group *group,\n\t\t\t       struct sg_lb_stats *sgs)\n{\n\tif (sgs->group_type < idlest_sgs->group_type)\n\t\treturn true;\n\n\tif (sgs->group_type > idlest_sgs->group_type)\n\t\treturn false;\n\n\t \n\n\tswitch (sgs->group_type) {\n\tcase group_overloaded:\n\tcase group_fully_busy:\n\t\t \n\t\tif (idlest_sgs->avg_load <= sgs->avg_load)\n\t\t\treturn false;\n\t\tbreak;\n\n\tcase group_imbalanced:\n\tcase group_asym_packing:\n\tcase group_smt_balance:\n\t\t \n\t\treturn false;\n\n\tcase group_misfit_task:\n\t\t \n\t\tif (idlest->sgc->max_capacity >= group->sgc->max_capacity)\n\t\t\treturn false;\n\t\tbreak;\n\n\tcase group_has_spare:\n\t\t \n\t\tif (idlest_sgs->idle_cpus > sgs->idle_cpus)\n\t\t\treturn false;\n\n\t\t \n\t\tif (idlest_sgs->idle_cpus == sgs->idle_cpus &&\n\t\t\tidlest_sgs->group_util <= sgs->group_util)\n\t\t\treturn false;\n\n\t\tbreak;\n\t}\n\n\treturn true;\n}\n\n \nstatic struct sched_group *\nfind_idlest_group(struct sched_domain *sd, struct task_struct *p, int this_cpu)\n{\n\tstruct sched_group *idlest = NULL, *local = NULL, *group = sd->groups;\n\tstruct sg_lb_stats local_sgs, tmp_sgs;\n\tstruct sg_lb_stats *sgs;\n\tunsigned long imbalance;\n\tstruct sg_lb_stats idlest_sgs = {\n\t\t\t.avg_load = UINT_MAX,\n\t\t\t.group_type = group_overloaded,\n\t};\n\n\tdo {\n\t\tint local_group;\n\n\t\t \n\t\tif (!cpumask_intersects(sched_group_span(group),\n\t\t\t\t\tp->cpus_ptr))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (!sched_group_cookie_match(cpu_rq(this_cpu), p, group))\n\t\t\tcontinue;\n\n\t\tlocal_group = cpumask_test_cpu(this_cpu,\n\t\t\t\t\t       sched_group_span(group));\n\n\t\tif (local_group) {\n\t\t\tsgs = &local_sgs;\n\t\t\tlocal = group;\n\t\t} else {\n\t\t\tsgs = &tmp_sgs;\n\t\t}\n\n\t\tupdate_sg_wakeup_stats(sd, group, sgs, p);\n\n\t\tif (!local_group && update_pick_idlest(idlest, &idlest_sgs, group, sgs)) {\n\t\t\tidlest = group;\n\t\t\tidlest_sgs = *sgs;\n\t\t}\n\n\t} while (group = group->next, group != sd->groups);\n\n\n\t \n\tif (!idlest)\n\t\treturn NULL;\n\n\t \n\tif (!local)\n\t\treturn idlest;\n\n\t \n\tif (local_sgs.group_type < idlest_sgs.group_type)\n\t\treturn NULL;\n\n\t \n\tif (local_sgs.group_type > idlest_sgs.group_type)\n\t\treturn idlest;\n\n\tswitch (local_sgs.group_type) {\n\tcase group_overloaded:\n\tcase group_fully_busy:\n\n\t\t \n\t\timbalance = scale_load_down(NICE_0_LOAD) *\n\t\t\t\t(sd->imbalance_pct-100) / 100;\n\n\t\t \n\n\t\tif ((sd->flags & SD_NUMA) &&\n\t\t    ((idlest_sgs.avg_load + imbalance) >= local_sgs.avg_load))\n\t\t\treturn NULL;\n\n\t\t \n\t\tif (idlest_sgs.avg_load >= (local_sgs.avg_load + imbalance))\n\t\t\treturn NULL;\n\n\t\tif (100 * local_sgs.avg_load <= sd->imbalance_pct * idlest_sgs.avg_load)\n\t\t\treturn NULL;\n\t\tbreak;\n\n\tcase group_imbalanced:\n\tcase group_asym_packing:\n\tcase group_smt_balance:\n\t\t \n\t\treturn NULL;\n\n\tcase group_misfit_task:\n\t\t \n\t\tif (local->sgc->max_capacity >= idlest->sgc->max_capacity)\n\t\t\treturn NULL;\n\t\tbreak;\n\n\tcase group_has_spare:\n#ifdef CONFIG_NUMA\n\t\tif (sd->flags & SD_NUMA) {\n\t\t\tint imb_numa_nr = sd->imb_numa_nr;\n#ifdef CONFIG_NUMA_BALANCING\n\t\t\tint idlest_cpu;\n\t\t\t \n\t\t\tif (cpu_to_node(this_cpu) == p->numa_preferred_nid)\n\t\t\t\treturn NULL;\n\n\t\t\tidlest_cpu = cpumask_first(sched_group_span(idlest));\n\t\t\tif (cpu_to_node(idlest_cpu) == p->numa_preferred_nid)\n\t\t\t\treturn idlest;\n#endif  \n\t\t\t \n\t\t\tif (p->nr_cpus_allowed != NR_CPUS) {\n\t\t\t\tstruct cpumask *cpus = this_cpu_cpumask_var_ptr(select_rq_mask);\n\n\t\t\t\tcpumask_and(cpus, sched_group_span(local), p->cpus_ptr);\n\t\t\t\timb_numa_nr = min(cpumask_weight(cpus), sd->imb_numa_nr);\n\t\t\t}\n\n\t\t\timbalance = abs(local_sgs.idle_cpus - idlest_sgs.idle_cpus);\n\t\t\tif (!adjust_numa_imbalance(imbalance,\n\t\t\t\t\t\t   local_sgs.sum_nr_running + 1,\n\t\t\t\t\t\t   imb_numa_nr)) {\n\t\t\t\treturn NULL;\n\t\t\t}\n\t\t}\n#endif  \n\n\t\t \n\t\tif (local_sgs.idle_cpus >= idlest_sgs.idle_cpus)\n\t\t\treturn NULL;\n\t\tbreak;\n\t}\n\n\treturn idlest;\n}\n\nstatic void update_idle_cpu_scan(struct lb_env *env,\n\t\t\t\t unsigned long sum_util)\n{\n\tstruct sched_domain_shared *sd_share;\n\tint llc_weight, pct;\n\tu64 x, y, tmp;\n\t \n\tif (!sched_feat(SIS_UTIL) || env->idle == CPU_NEWLY_IDLE)\n\t\treturn;\n\n\tllc_weight = per_cpu(sd_llc_size, env->dst_cpu);\n\tif (env->sd->span_weight != llc_weight)\n\t\treturn;\n\n\tsd_share = rcu_dereference(per_cpu(sd_llc_shared, env->dst_cpu));\n\tif (!sd_share)\n\t\treturn;\n\n\t \n\t \n\tx = sum_util;\n\tdo_div(x, llc_weight);\n\n\t \n\tpct = env->sd->imbalance_pct;\n\ttmp = x * x * pct * pct;\n\tdo_div(tmp, 10000 * SCHED_CAPACITY_SCALE);\n\ttmp = min_t(long, tmp, SCHED_CAPACITY_SCALE);\n\ty = SCHED_CAPACITY_SCALE - tmp;\n\n\t \n\ty *= llc_weight;\n\tdo_div(y, SCHED_CAPACITY_SCALE);\n\tif ((int)y != sd_share->nr_idle_scan)\n\t\tWRITE_ONCE(sd_share->nr_idle_scan, (int)y);\n}\n\n \n\nstatic inline void update_sd_lb_stats(struct lb_env *env, struct sd_lb_stats *sds)\n{\n\tstruct sched_group *sg = env->sd->groups;\n\tstruct sg_lb_stats *local = &sds->local_stat;\n\tstruct sg_lb_stats tmp_sgs;\n\tunsigned long sum_util = 0;\n\tint sg_status = 0;\n\n\tdo {\n\t\tstruct sg_lb_stats *sgs = &tmp_sgs;\n\t\tint local_group;\n\n\t\tlocal_group = cpumask_test_cpu(env->dst_cpu, sched_group_span(sg));\n\t\tif (local_group) {\n\t\t\tsds->local = sg;\n\t\t\tsgs = local;\n\n\t\t\tif (env->idle != CPU_NEWLY_IDLE ||\n\t\t\t    time_after_eq(jiffies, sg->sgc->next_update))\n\t\t\t\tupdate_group_capacity(env->sd, env->dst_cpu);\n\t\t}\n\n\t\tupdate_sg_lb_stats(env, sds, sg, sgs, &sg_status);\n\n\t\tif (local_group)\n\t\t\tgoto next_group;\n\n\n\t\tif (update_sd_pick_busiest(env, sds, sg, sgs)) {\n\t\t\tsds->busiest = sg;\n\t\t\tsds->busiest_stat = *sgs;\n\t\t}\n\nnext_group:\n\t\t \n\t\tsds->total_load += sgs->group_load;\n\t\tsds->total_capacity += sgs->group_capacity;\n\n\t\tsum_util += sgs->group_util;\n\t\tsg = sg->next;\n\t} while (sg != env->sd->groups);\n\n\t \n\tif (sds->busiest)\n\t\tsds->prefer_sibling = !!(sds->busiest->flags & SD_PREFER_SIBLING);\n\n\n\tif (env->sd->flags & SD_NUMA)\n\t\tenv->fbq_type = fbq_classify_group(&sds->busiest_stat);\n\n\tif (!env->sd->parent) {\n\t\tstruct root_domain *rd = env->dst_rq->rd;\n\n\t\t \n\t\tWRITE_ONCE(rd->overload, sg_status & SG_OVERLOAD);\n\n\t\t \n\t\tWRITE_ONCE(rd->overutilized, sg_status & SG_OVERUTILIZED);\n\t\ttrace_sched_overutilized_tp(rd, sg_status & SG_OVERUTILIZED);\n\t} else if (sg_status & SG_OVERUTILIZED) {\n\t\tstruct root_domain *rd = env->dst_rq->rd;\n\n\t\tWRITE_ONCE(rd->overutilized, SG_OVERUTILIZED);\n\t\ttrace_sched_overutilized_tp(rd, SG_OVERUTILIZED);\n\t}\n\n\tupdate_idle_cpu_scan(env, sum_util);\n}\n\n \nstatic inline void calculate_imbalance(struct lb_env *env, struct sd_lb_stats *sds)\n{\n\tstruct sg_lb_stats *local, *busiest;\n\n\tlocal = &sds->local_stat;\n\tbusiest = &sds->busiest_stat;\n\n\tif (busiest->group_type == group_misfit_task) {\n\t\tif (env->sd->flags & SD_ASYM_CPUCAPACITY) {\n\t\t\t \n\t\t\tenv->migration_type = migrate_misfit;\n\t\t\tenv->imbalance = 1;\n\t\t} else {\n\t\t\t \n\t\t\tenv->migration_type = migrate_load;\n\t\t\tenv->imbalance = busiest->group_misfit_task_load;\n\t\t}\n\t\treturn;\n\t}\n\n\tif (busiest->group_type == group_asym_packing) {\n\t\t \n\t\tenv->migration_type = migrate_task;\n\t\tenv->imbalance = busiest->sum_h_nr_running;\n\t\treturn;\n\t}\n\n\tif (busiest->group_type == group_smt_balance) {\n\t\t \n\t\tenv->migration_type = migrate_task;\n\t\tenv->imbalance = 1;\n\t\treturn;\n\t}\n\n\tif (busiest->group_type == group_imbalanced) {\n\t\t \n\t\tenv->migration_type = migrate_task;\n\t\tenv->imbalance = 1;\n\t\treturn;\n\t}\n\n\t \n\tif (local->group_type == group_has_spare) {\n\t\tif ((busiest->group_type > group_fully_busy) &&\n\t\t    !(env->sd->flags & SD_SHARE_PKG_RESOURCES)) {\n\t\t\t \n\t\t\tenv->migration_type = migrate_util;\n\t\t\tenv->imbalance = max(local->group_capacity, local->group_util) -\n\t\t\t\t\t local->group_util;\n\n\t\t\t \n\t\t\tif (env->idle != CPU_NOT_IDLE && env->imbalance == 0) {\n\t\t\t\tenv->migration_type = migrate_task;\n\t\t\t\tenv->imbalance = 1;\n\t\t\t}\n\n\t\t\treturn;\n\t\t}\n\n\t\tif (busiest->group_weight == 1 || sds->prefer_sibling) {\n\t\t\t \n\t\t\tenv->migration_type = migrate_task;\n\t\t\tenv->imbalance = sibling_imbalance(env, sds, busiest, local);\n\t\t} else {\n\n\t\t\t \n\t\t\tenv->migration_type = migrate_task;\n\t\t\tenv->imbalance = max_t(long, 0,\n\t\t\t\t\t       (local->idle_cpus - busiest->idle_cpus));\n\t\t}\n\n#ifdef CONFIG_NUMA\n\t\t \n\t\tif (env->sd->flags & SD_NUMA) {\n\t\t\tenv->imbalance = adjust_numa_imbalance(env->imbalance,\n\t\t\t\t\t\t\t       local->sum_nr_running + 1,\n\t\t\t\t\t\t\t       env->sd->imb_numa_nr);\n\t\t}\n#endif\n\n\t\t \n\t\tenv->imbalance >>= 1;\n\n\t\treturn;\n\t}\n\n\t \n\tif (local->group_type < group_overloaded) {\n\t\t \n\n\t\tlocal->avg_load = (local->group_load * SCHED_CAPACITY_SCALE) /\n\t\t\t\t  local->group_capacity;\n\n\t\t \n\t\tif (local->avg_load >= busiest->avg_load) {\n\t\t\tenv->imbalance = 0;\n\t\t\treturn;\n\t\t}\n\n\t\tsds->avg_load = (sds->total_load * SCHED_CAPACITY_SCALE) /\n\t\t\t\tsds->total_capacity;\n\n\t\t \n\t\tif (local->avg_load >= sds->avg_load) {\n\t\t\tenv->imbalance = 0;\n\t\t\treturn;\n\t\t}\n\n\t}\n\n\t \n\tenv->migration_type = migrate_load;\n\tenv->imbalance = min(\n\t\t(busiest->avg_load - sds->avg_load) * busiest->group_capacity,\n\t\t(sds->avg_load - local->avg_load) * local->group_capacity\n\t) / SCHED_CAPACITY_SCALE;\n}\n\n \n\n \n\n \nstatic struct sched_group *find_busiest_group(struct lb_env *env)\n{\n\tstruct sg_lb_stats *local, *busiest;\n\tstruct sd_lb_stats sds;\n\n\tinit_sd_lb_stats(&sds);\n\n\t \n\tupdate_sd_lb_stats(env, &sds);\n\n\t \n\tif (!sds.busiest)\n\t\tgoto out_balanced;\n\n\tbusiest = &sds.busiest_stat;\n\n\t \n\tif (busiest->group_type == group_misfit_task)\n\t\tgoto force_balance;\n\n\tif (sched_energy_enabled()) {\n\t\tstruct root_domain *rd = env->dst_rq->rd;\n\n\t\tif (rcu_dereference(rd->pd) && !READ_ONCE(rd->overutilized))\n\t\t\tgoto out_balanced;\n\t}\n\n\t \n\tif (busiest->group_type == group_asym_packing)\n\t\tgoto force_balance;\n\n\t \n\tif (busiest->group_type == group_imbalanced)\n\t\tgoto force_balance;\n\n\tlocal = &sds.local_stat;\n\t \n\tif (local->group_type > busiest->group_type)\n\t\tgoto out_balanced;\n\n\t \n\tif (local->group_type == group_overloaded) {\n\t\t \n\t\tif (local->avg_load >= busiest->avg_load)\n\t\t\tgoto out_balanced;\n\n\t\t \n\t\tsds.avg_load = (sds.total_load * SCHED_CAPACITY_SCALE) /\n\t\t\t\tsds.total_capacity;\n\n\t\t \n\t\tif (local->avg_load >= sds.avg_load)\n\t\t\tgoto out_balanced;\n\n\t\t \n\t\tif (100 * busiest->avg_load <=\n\t\t\t\tenv->sd->imbalance_pct * local->avg_load)\n\t\t\tgoto out_balanced;\n\t}\n\n\t \n\tif (sds.prefer_sibling && local->group_type == group_has_spare &&\n\t    sibling_imbalance(env, &sds, busiest, local) > 1)\n\t\tgoto force_balance;\n\n\tif (busiest->group_type != group_overloaded) {\n\t\tif (env->idle == CPU_NOT_IDLE) {\n\t\t\t \n\t\t\tgoto out_balanced;\n\t\t}\n\n\t\tif (busiest->group_type == group_smt_balance &&\n\t\t    smt_vs_nonsmt_groups(sds.local, sds.busiest)) {\n\t\t\t \n\t\t\tgoto force_balance;\n\t\t}\n\n\t\tif (busiest->group_weight > 1 &&\n\t\t    local->idle_cpus <= (busiest->idle_cpus + 1)) {\n\t\t\t \n\t\t\tgoto out_balanced;\n\t\t}\n\n\t\tif (busiest->sum_h_nr_running == 1) {\n\t\t\t \n\t\t\tgoto out_balanced;\n\t\t}\n\t}\n\nforce_balance:\n\t \n\tcalculate_imbalance(env, &sds);\n\treturn env->imbalance ? sds.busiest : NULL;\n\nout_balanced:\n\tenv->imbalance = 0;\n\treturn NULL;\n}\n\n \nstatic struct rq *find_busiest_queue(struct lb_env *env,\n\t\t\t\t     struct sched_group *group)\n{\n\tstruct rq *busiest = NULL, *rq;\n\tunsigned long busiest_util = 0, busiest_load = 0, busiest_capacity = 1;\n\tunsigned int busiest_nr = 0;\n\tint i;\n\n\tfor_each_cpu_and(i, sched_group_span(group), env->cpus) {\n\t\tunsigned long capacity, load, util;\n\t\tunsigned int nr_running;\n\t\tenum fbq_type rt;\n\n\t\trq = cpu_rq(i);\n\t\trt = fbq_classify_rq(rq);\n\n\t\t \n\t\tif (rt > env->fbq_type)\n\t\t\tcontinue;\n\n\t\tnr_running = rq->cfs.h_nr_running;\n\t\tif (!nr_running)\n\t\t\tcontinue;\n\n\t\tcapacity = capacity_of(i);\n\n\t\t \n\t\tif (env->sd->flags & SD_ASYM_CPUCAPACITY &&\n\t\t    !capacity_greater(capacity_of(env->dst_cpu), capacity) &&\n\t\t    nr_running == 1)\n\t\t\tcontinue;\n\n\t\t \n\t\tif ((env->sd->flags & SD_ASYM_PACKING) &&\n\t\t    sched_use_asym_prio(env->sd, i) &&\n\t\t    sched_asym_prefer(i, env->dst_cpu) &&\n\t\t    nr_running == 1)\n\t\t\tcontinue;\n\n\t\tswitch (env->migration_type) {\n\t\tcase migrate_load:\n\t\t\t \n\t\t\tload = cpu_load(rq);\n\n\t\t\tif (nr_running == 1 && load > env->imbalance &&\n\t\t\t    !check_cpu_capacity(rq, env->sd))\n\t\t\t\tbreak;\n\n\t\t\t \n\t\t\tif (load * busiest_capacity > busiest_load * capacity) {\n\t\t\t\tbusiest_load = load;\n\t\t\t\tbusiest_capacity = capacity;\n\t\t\t\tbusiest = rq;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase migrate_util:\n\t\t\tutil = cpu_util_cfs_boost(i);\n\n\t\t\t \n\t\t\tif (nr_running <= 1)\n\t\t\t\tcontinue;\n\n\t\t\tif (busiest_util < util) {\n\t\t\t\tbusiest_util = util;\n\t\t\t\tbusiest = rq;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase migrate_task:\n\t\t\tif (busiest_nr < nr_running) {\n\t\t\t\tbusiest_nr = nr_running;\n\t\t\t\tbusiest = rq;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase migrate_misfit:\n\t\t\t \n\t\t\tif (rq->misfit_task_load > busiest_load) {\n\t\t\t\tbusiest_load = rq->misfit_task_load;\n\t\t\t\tbusiest = rq;\n\t\t\t}\n\n\t\t\tbreak;\n\n\t\t}\n\t}\n\n\treturn busiest;\n}\n\n \n#define MAX_PINNED_INTERVAL\t512\n\nstatic inline bool\nasym_active_balance(struct lb_env *env)\n{\n\t \n\treturn env->idle != CPU_NOT_IDLE && (env->sd->flags & SD_ASYM_PACKING) &&\n\t       sched_use_asym_prio(env->sd, env->dst_cpu) &&\n\t       (sched_asym_prefer(env->dst_cpu, env->src_cpu) ||\n\t\t!sched_use_asym_prio(env->sd, env->src_cpu));\n}\n\nstatic inline bool\nimbalanced_active_balance(struct lb_env *env)\n{\n\tstruct sched_domain *sd = env->sd;\n\n\t \n\tif ((env->migration_type == migrate_task) &&\n\t    (sd->nr_balance_failed > sd->cache_nice_tries+2))\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic int need_active_balance(struct lb_env *env)\n{\n\tstruct sched_domain *sd = env->sd;\n\n\tif (asym_active_balance(env))\n\t\treturn 1;\n\n\tif (imbalanced_active_balance(env))\n\t\treturn 1;\n\n\t \n\tif ((env->idle != CPU_NOT_IDLE) &&\n\t    (env->src_rq->cfs.h_nr_running == 1)) {\n\t\tif ((check_cpu_capacity(env->src_rq, sd)) &&\n\t\t    (capacity_of(env->src_cpu)*sd->imbalance_pct < capacity_of(env->dst_cpu)*100))\n\t\t\treturn 1;\n\t}\n\n\tif (env->migration_type == migrate_misfit)\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic int active_load_balance_cpu_stop(void *data);\n\nstatic int should_we_balance(struct lb_env *env)\n{\n\tstruct cpumask *swb_cpus = this_cpu_cpumask_var_ptr(should_we_balance_tmpmask);\n\tstruct sched_group *sg = env->sd->groups;\n\tint cpu, idle_smt = -1;\n\n\t \n\tif (!cpumask_test_cpu(env->dst_cpu, env->cpus))\n\t\treturn 0;\n\n\t \n\tif (env->idle == CPU_NEWLY_IDLE) {\n\t\tif (env->dst_rq->nr_running > 0 || env->dst_rq->ttwu_pending)\n\t\t\treturn 0;\n\t\treturn 1;\n\t}\n\n\tcpumask_copy(swb_cpus, group_balance_mask(sg));\n\t \n\tfor_each_cpu_and(cpu, swb_cpus, env->cpus) {\n\t\tif (!idle_cpu(cpu))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (!(env->sd->flags & SD_SHARE_CPUCAPACITY) && !is_core_idle(cpu)) {\n\t\t\tif (idle_smt == -1)\n\t\t\t\tidle_smt = cpu;\n\t\t\t \n#ifdef CONFIG_SCHED_SMT\n\t\t\tcpumask_andnot(swb_cpus, swb_cpus, cpu_smt_mask(cpu));\n#endif\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\treturn cpu == env->dst_cpu;\n\t}\n\n\t \n\tif (idle_smt != -1)\n\t\treturn idle_smt == env->dst_cpu;\n\n\t \n\treturn group_balance_cpu(sg) == env->dst_cpu;\n}\n\n \nstatic int load_balance(int this_cpu, struct rq *this_rq,\n\t\t\tstruct sched_domain *sd, enum cpu_idle_type idle,\n\t\t\tint *continue_balancing)\n{\n\tint ld_moved, cur_ld_moved, active_balance = 0;\n\tstruct sched_domain *sd_parent = sd->parent;\n\tstruct sched_group *group;\n\tstruct rq *busiest;\n\tstruct rq_flags rf;\n\tstruct cpumask *cpus = this_cpu_cpumask_var_ptr(load_balance_mask);\n\tstruct lb_env env = {\n\t\t.sd\t\t= sd,\n\t\t.dst_cpu\t= this_cpu,\n\t\t.dst_rq\t\t= this_rq,\n\t\t.dst_grpmask    = group_balance_mask(sd->groups),\n\t\t.idle\t\t= idle,\n\t\t.loop_break\t= SCHED_NR_MIGRATE_BREAK,\n\t\t.cpus\t\t= cpus,\n\t\t.fbq_type\t= all,\n\t\t.tasks\t\t= LIST_HEAD_INIT(env.tasks),\n\t};\n\n\tcpumask_and(cpus, sched_domain_span(sd), cpu_active_mask);\n\n\tschedstat_inc(sd->lb_count[idle]);\n\nredo:\n\tif (!should_we_balance(&env)) {\n\t\t*continue_balancing = 0;\n\t\tgoto out_balanced;\n\t}\n\n\tgroup = find_busiest_group(&env);\n\tif (!group) {\n\t\tschedstat_inc(sd->lb_nobusyg[idle]);\n\t\tgoto out_balanced;\n\t}\n\n\tbusiest = find_busiest_queue(&env, group);\n\tif (!busiest) {\n\t\tschedstat_inc(sd->lb_nobusyq[idle]);\n\t\tgoto out_balanced;\n\t}\n\n\tWARN_ON_ONCE(busiest == env.dst_rq);\n\n\tschedstat_add(sd->lb_imbalance[idle], env.imbalance);\n\n\tenv.src_cpu = busiest->cpu;\n\tenv.src_rq = busiest;\n\n\tld_moved = 0;\n\t \n\tenv.flags |= LBF_ALL_PINNED;\n\tif (busiest->nr_running > 1) {\n\t\t \n\t\tenv.loop_max  = min(sysctl_sched_nr_migrate, busiest->nr_running);\n\nmore_balance:\n\t\trq_lock_irqsave(busiest, &rf);\n\t\tupdate_rq_clock(busiest);\n\n\t\t \n\t\tcur_ld_moved = detach_tasks(&env);\n\n\t\t \n\n\t\trq_unlock(busiest, &rf);\n\n\t\tif (cur_ld_moved) {\n\t\t\tattach_tasks(&env);\n\t\t\tld_moved += cur_ld_moved;\n\t\t}\n\n\t\tlocal_irq_restore(rf.flags);\n\n\t\tif (env.flags & LBF_NEED_BREAK) {\n\t\t\tenv.flags &= ~LBF_NEED_BREAK;\n\t\t\t \n\t\t\tif (env.loop < busiest->nr_running)\n\t\t\t\tgoto more_balance;\n\t\t}\n\n\t\t \n\t\tif ((env.flags & LBF_DST_PINNED) && env.imbalance > 0) {\n\n\t\t\t \n\t\t\t__cpumask_clear_cpu(env.dst_cpu, env.cpus);\n\n\t\t\tenv.dst_rq\t = cpu_rq(env.new_dst_cpu);\n\t\t\tenv.dst_cpu\t = env.new_dst_cpu;\n\t\t\tenv.flags\t&= ~LBF_DST_PINNED;\n\t\t\tenv.loop\t = 0;\n\t\t\tenv.loop_break\t = SCHED_NR_MIGRATE_BREAK;\n\n\t\t\t \n\t\t\tgoto more_balance;\n\t\t}\n\n\t\t \n\t\tif (sd_parent) {\n\t\t\tint *group_imbalance = &sd_parent->groups->sgc->imbalance;\n\n\t\t\tif ((env.flags & LBF_SOME_PINNED) && env.imbalance > 0)\n\t\t\t\t*group_imbalance = 1;\n\t\t}\n\n\t\t \n\t\tif (unlikely(env.flags & LBF_ALL_PINNED)) {\n\t\t\t__cpumask_clear_cpu(cpu_of(busiest), cpus);\n\t\t\t \n\t\t\tif (!cpumask_subset(cpus, env.dst_grpmask)) {\n\t\t\t\tenv.loop = 0;\n\t\t\t\tenv.loop_break = SCHED_NR_MIGRATE_BREAK;\n\t\t\t\tgoto redo;\n\t\t\t}\n\t\t\tgoto out_all_pinned;\n\t\t}\n\t}\n\n\tif (!ld_moved) {\n\t\tschedstat_inc(sd->lb_failed[idle]);\n\t\t \n\t\tif (idle != CPU_NEWLY_IDLE)\n\t\t\tsd->nr_balance_failed++;\n\n\t\tif (need_active_balance(&env)) {\n\t\t\tunsigned long flags;\n\n\t\t\traw_spin_rq_lock_irqsave(busiest, flags);\n\n\t\t\t \n\t\t\tif (!cpumask_test_cpu(this_cpu, busiest->curr->cpus_ptr)) {\n\t\t\t\traw_spin_rq_unlock_irqrestore(busiest, flags);\n\t\t\t\tgoto out_one_pinned;\n\t\t\t}\n\n\t\t\t \n\t\t\tenv.flags &= ~LBF_ALL_PINNED;\n\n\t\t\t \n\t\t\tif (!busiest->active_balance) {\n\t\t\t\tbusiest->active_balance = 1;\n\t\t\t\tbusiest->push_cpu = this_cpu;\n\t\t\t\tactive_balance = 1;\n\t\t\t}\n\n\t\t\tpreempt_disable();\n\t\t\traw_spin_rq_unlock_irqrestore(busiest, flags);\n\t\t\tif (active_balance) {\n\t\t\t\tstop_one_cpu_nowait(cpu_of(busiest),\n\t\t\t\t\tactive_load_balance_cpu_stop, busiest,\n\t\t\t\t\t&busiest->active_balance_work);\n\t\t\t}\n\t\t\tpreempt_enable();\n\t\t}\n\t} else {\n\t\tsd->nr_balance_failed = 0;\n\t}\n\n\tif (likely(!active_balance) || need_active_balance(&env)) {\n\t\t \n\t\tsd->balance_interval = sd->min_interval;\n\t}\n\n\tgoto out;\n\nout_balanced:\n\t \n\tif (sd_parent && !(env.flags & LBF_ALL_PINNED)) {\n\t\tint *group_imbalance = &sd_parent->groups->sgc->imbalance;\n\n\t\tif (*group_imbalance)\n\t\t\t*group_imbalance = 0;\n\t}\n\nout_all_pinned:\n\t \n\tschedstat_inc(sd->lb_balanced[idle]);\n\n\tsd->nr_balance_failed = 0;\n\nout_one_pinned:\n\tld_moved = 0;\n\n\t \n\tif (env.idle == CPU_NEWLY_IDLE)\n\t\tgoto out;\n\n\t \n\tif ((env.flags & LBF_ALL_PINNED &&\n\t     sd->balance_interval < MAX_PINNED_INTERVAL) ||\n\t    sd->balance_interval < sd->max_interval)\n\t\tsd->balance_interval *= 2;\nout:\n\treturn ld_moved;\n}\n\nstatic inline unsigned long\nget_sd_balance_interval(struct sched_domain *sd, int cpu_busy)\n{\n\tunsigned long interval = sd->balance_interval;\n\n\tif (cpu_busy)\n\t\tinterval *= sd->busy_factor;\n\n\t \n\tinterval = msecs_to_jiffies(interval);\n\n\t \n\tif (cpu_busy)\n\t\tinterval -= 1;\n\n\tinterval = clamp(interval, 1UL, max_load_balance_interval);\n\n\treturn interval;\n}\n\nstatic inline void\nupdate_next_balance(struct sched_domain *sd, unsigned long *next_balance)\n{\n\tunsigned long interval, next;\n\n\t \n\tinterval = get_sd_balance_interval(sd, 0);\n\tnext = sd->last_balance + interval;\n\n\tif (time_after(*next_balance, next))\n\t\t*next_balance = next;\n}\n\n \nstatic int active_load_balance_cpu_stop(void *data)\n{\n\tstruct rq *busiest_rq = data;\n\tint busiest_cpu = cpu_of(busiest_rq);\n\tint target_cpu = busiest_rq->push_cpu;\n\tstruct rq *target_rq = cpu_rq(target_cpu);\n\tstruct sched_domain *sd;\n\tstruct task_struct *p = NULL;\n\tstruct rq_flags rf;\n\n\trq_lock_irq(busiest_rq, &rf);\n\t \n\tif (!cpu_active(busiest_cpu) || !cpu_active(target_cpu))\n\t\tgoto out_unlock;\n\n\t \n\tif (unlikely(busiest_cpu != smp_processor_id() ||\n\t\t     !busiest_rq->active_balance))\n\t\tgoto out_unlock;\n\n\t \n\tif (busiest_rq->nr_running <= 1)\n\t\tgoto out_unlock;\n\n\t \n\tWARN_ON_ONCE(busiest_rq == target_rq);\n\n\t \n\trcu_read_lock();\n\tfor_each_domain(target_cpu, sd) {\n\t\tif (cpumask_test_cpu(busiest_cpu, sched_domain_span(sd)))\n\t\t\tbreak;\n\t}\n\n\tif (likely(sd)) {\n\t\tstruct lb_env env = {\n\t\t\t.sd\t\t= sd,\n\t\t\t.dst_cpu\t= target_cpu,\n\t\t\t.dst_rq\t\t= target_rq,\n\t\t\t.src_cpu\t= busiest_rq->cpu,\n\t\t\t.src_rq\t\t= busiest_rq,\n\t\t\t.idle\t\t= CPU_IDLE,\n\t\t\t.flags\t\t= LBF_ACTIVE_LB,\n\t\t};\n\n\t\tschedstat_inc(sd->alb_count);\n\t\tupdate_rq_clock(busiest_rq);\n\n\t\tp = detach_one_task(&env);\n\t\tif (p) {\n\t\t\tschedstat_inc(sd->alb_pushed);\n\t\t\t \n\t\t\tsd->nr_balance_failed = 0;\n\t\t} else {\n\t\t\tschedstat_inc(sd->alb_failed);\n\t\t}\n\t}\n\trcu_read_unlock();\nout_unlock:\n\tbusiest_rq->active_balance = 0;\n\trq_unlock(busiest_rq, &rf);\n\n\tif (p)\n\t\tattach_one_task(target_rq, p);\n\n\tlocal_irq_enable();\n\n\treturn 0;\n}\n\nstatic DEFINE_SPINLOCK(balancing);\n\n \nvoid update_max_interval(void)\n{\n\tmax_load_balance_interval = HZ*num_online_cpus()/10;\n}\n\nstatic inline bool update_newidle_cost(struct sched_domain *sd, u64 cost)\n{\n\tif (cost > sd->max_newidle_lb_cost) {\n\t\t \n\t\tsd->max_newidle_lb_cost = cost;\n\t\tsd->last_decay_max_lb_cost = jiffies;\n\t} else if (time_after(jiffies, sd->last_decay_max_lb_cost + HZ)) {\n\t\t \n\t\tsd->max_newidle_lb_cost = (sd->max_newidle_lb_cost * 253) / 256;\n\t\tsd->last_decay_max_lb_cost = jiffies;\n\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nstatic void rebalance_domains(struct rq *rq, enum cpu_idle_type idle)\n{\n\tint continue_balancing = 1;\n\tint cpu = rq->cpu;\n\tint busy = idle != CPU_IDLE && !sched_idle_cpu(cpu);\n\tunsigned long interval;\n\tstruct sched_domain *sd;\n\t \n\tunsigned long next_balance = jiffies + 60*HZ;\n\tint update_next_balance = 0;\n\tint need_serialize, need_decay = 0;\n\tu64 max_cost = 0;\n\n\trcu_read_lock();\n\tfor_each_domain(cpu, sd) {\n\t\t \n\t\tneed_decay = update_newidle_cost(sd, 0);\n\t\tmax_cost += sd->max_newidle_lb_cost;\n\n\t\t \n\t\tif (!continue_balancing) {\n\t\t\tif (need_decay)\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\t}\n\n\t\tinterval = get_sd_balance_interval(sd, busy);\n\n\t\tneed_serialize = sd->flags & SD_SERIALIZE;\n\t\tif (need_serialize) {\n\t\t\tif (!spin_trylock(&balancing))\n\t\t\t\tgoto out;\n\t\t}\n\n\t\tif (time_after_eq(jiffies, sd->last_balance + interval)) {\n\t\t\tif (load_balance(cpu, rq, sd, idle, &continue_balancing)) {\n\t\t\t\t \n\t\t\t\tidle = idle_cpu(cpu) ? CPU_IDLE : CPU_NOT_IDLE;\n\t\t\t\tbusy = idle != CPU_IDLE && !sched_idle_cpu(cpu);\n\t\t\t}\n\t\t\tsd->last_balance = jiffies;\n\t\t\tinterval = get_sd_balance_interval(sd, busy);\n\t\t}\n\t\tif (need_serialize)\n\t\t\tspin_unlock(&balancing);\nout:\n\t\tif (time_after(next_balance, sd->last_balance + interval)) {\n\t\t\tnext_balance = sd->last_balance + interval;\n\t\t\tupdate_next_balance = 1;\n\t\t}\n\t}\n\tif (need_decay) {\n\t\t \n\t\trq->max_idle_balance_cost =\n\t\t\tmax((u64)sysctl_sched_migration_cost, max_cost);\n\t}\n\trcu_read_unlock();\n\n\t \n\tif (likely(update_next_balance))\n\t\trq->next_balance = next_balance;\n\n}\n\nstatic inline int on_null_domain(struct rq *rq)\n{\n\treturn unlikely(!rcu_dereference_sched(rq->sd));\n}\n\n#ifdef CONFIG_NO_HZ_COMMON\n \n\nstatic inline int find_new_ilb(void)\n{\n\tint ilb;\n\tconst struct cpumask *hk_mask;\n\n\thk_mask = housekeeping_cpumask(HK_TYPE_MISC);\n\n\tfor_each_cpu_and(ilb, nohz.idle_cpus_mask, hk_mask) {\n\n\t\tif (ilb == smp_processor_id())\n\t\t\tcontinue;\n\n\t\tif (idle_cpu(ilb))\n\t\t\treturn ilb;\n\t}\n\n\treturn nr_cpu_ids;\n}\n\n \nstatic void kick_ilb(unsigned int flags)\n{\n\tint ilb_cpu;\n\n\t \n\tif (flags & NOHZ_BALANCE_KICK)\n\t\tnohz.next_balance = jiffies+1;\n\n\tilb_cpu = find_new_ilb();\n\n\tif (ilb_cpu >= nr_cpu_ids)\n\t\treturn;\n\n\t \n\tflags = atomic_fetch_or(flags, nohz_flags(ilb_cpu));\n\tif (flags & NOHZ_KICK_MASK)\n\t\treturn;\n\n\t \n\tsmp_call_function_single_async(ilb_cpu, &cpu_rq(ilb_cpu)->nohz_csd);\n}\n\n \nstatic void nohz_balancer_kick(struct rq *rq)\n{\n\tunsigned long now = jiffies;\n\tstruct sched_domain_shared *sds;\n\tstruct sched_domain *sd;\n\tint nr_busy, i, cpu = rq->cpu;\n\tunsigned int flags = 0;\n\n\tif (unlikely(rq->idle_balance))\n\t\treturn;\n\n\t \n\tnohz_balance_exit_idle(rq);\n\n\t \n\tif (likely(!atomic_read(&nohz.nr_cpus)))\n\t\treturn;\n\n\tif (READ_ONCE(nohz.has_blocked) &&\n\t    time_after(now, READ_ONCE(nohz.next_blocked)))\n\t\tflags = NOHZ_STATS_KICK;\n\n\tif (time_before(now, nohz.next_balance))\n\t\tgoto out;\n\n\tif (rq->nr_running >= 2) {\n\t\tflags = NOHZ_STATS_KICK | NOHZ_BALANCE_KICK;\n\t\tgoto out;\n\t}\n\n\trcu_read_lock();\n\n\tsd = rcu_dereference(rq->sd);\n\tif (sd) {\n\t\t \n\t\tif (rq->cfs.h_nr_running >= 1 && check_cpu_capacity(rq, sd)) {\n\t\t\tflags = NOHZ_STATS_KICK | NOHZ_BALANCE_KICK;\n\t\t\tgoto unlock;\n\t\t}\n\t}\n\n\tsd = rcu_dereference(per_cpu(sd_asym_packing, cpu));\n\tif (sd) {\n\t\t \n\t\tfor_each_cpu_and(i, sched_domain_span(sd), nohz.idle_cpus_mask) {\n\t\t\tif (sched_use_asym_prio(sd, i) &&\n\t\t\t    sched_asym_prefer(i, cpu)) {\n\t\t\t\tflags = NOHZ_STATS_KICK | NOHZ_BALANCE_KICK;\n\t\t\t\tgoto unlock;\n\t\t\t}\n\t\t}\n\t}\n\n\tsd = rcu_dereference(per_cpu(sd_asym_cpucapacity, cpu));\n\tif (sd) {\n\t\t \n\t\tif (check_misfit_status(rq, sd)) {\n\t\t\tflags = NOHZ_STATS_KICK | NOHZ_BALANCE_KICK;\n\t\t\tgoto unlock;\n\t\t}\n\n\t\t \n\t\tgoto unlock;\n\t}\n\n\tsds = rcu_dereference(per_cpu(sd_llc_shared, cpu));\n\tif (sds) {\n\t\t \n\t\tnr_busy = atomic_read(&sds->nr_busy_cpus);\n\t\tif (nr_busy > 1) {\n\t\t\tflags = NOHZ_STATS_KICK | NOHZ_BALANCE_KICK;\n\t\t\tgoto unlock;\n\t\t}\n\t}\nunlock:\n\trcu_read_unlock();\nout:\n\tif (READ_ONCE(nohz.needs_update))\n\t\tflags |= NOHZ_NEXT_KICK;\n\n\tif (flags)\n\t\tkick_ilb(flags);\n}\n\nstatic void set_cpu_sd_state_busy(int cpu)\n{\n\tstruct sched_domain *sd;\n\n\trcu_read_lock();\n\tsd = rcu_dereference(per_cpu(sd_llc, cpu));\n\n\tif (!sd || !sd->nohz_idle)\n\t\tgoto unlock;\n\tsd->nohz_idle = 0;\n\n\tatomic_inc(&sd->shared->nr_busy_cpus);\nunlock:\n\trcu_read_unlock();\n}\n\nvoid nohz_balance_exit_idle(struct rq *rq)\n{\n\tSCHED_WARN_ON(rq != this_rq());\n\n\tif (likely(!rq->nohz_tick_stopped))\n\t\treturn;\n\n\trq->nohz_tick_stopped = 0;\n\tcpumask_clear_cpu(rq->cpu, nohz.idle_cpus_mask);\n\tatomic_dec(&nohz.nr_cpus);\n\n\tset_cpu_sd_state_busy(rq->cpu);\n}\n\nstatic void set_cpu_sd_state_idle(int cpu)\n{\n\tstruct sched_domain *sd;\n\n\trcu_read_lock();\n\tsd = rcu_dereference(per_cpu(sd_llc, cpu));\n\n\tif (!sd || sd->nohz_idle)\n\t\tgoto unlock;\n\tsd->nohz_idle = 1;\n\n\tatomic_dec(&sd->shared->nr_busy_cpus);\nunlock:\n\trcu_read_unlock();\n}\n\n \nvoid nohz_balance_enter_idle(int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\n\tSCHED_WARN_ON(cpu != smp_processor_id());\n\n\t \n\tif (!cpu_active(cpu))\n\t\treturn;\n\n\t \n\tif (!housekeeping_cpu(cpu, HK_TYPE_SCHED))\n\t\treturn;\n\n\t \n\trq->has_blocked_load = 1;\n\n\t \n\tif (rq->nohz_tick_stopped)\n\t\tgoto out;\n\n\t \n\tif (on_null_domain(rq))\n\t\treturn;\n\n\trq->nohz_tick_stopped = 1;\n\n\tcpumask_set_cpu(cpu, nohz.idle_cpus_mask);\n\tatomic_inc(&nohz.nr_cpus);\n\n\t \n\tsmp_mb__after_atomic();\n\n\tset_cpu_sd_state_idle(cpu);\n\n\tWRITE_ONCE(nohz.needs_update, 1);\nout:\n\t \n\tWRITE_ONCE(nohz.has_blocked, 1);\n}\n\nstatic bool update_nohz_stats(struct rq *rq)\n{\n\tunsigned int cpu = rq->cpu;\n\n\tif (!rq->has_blocked_load)\n\t\treturn false;\n\n\tif (!cpumask_test_cpu(cpu, nohz.idle_cpus_mask))\n\t\treturn false;\n\n\tif (!time_after(jiffies, READ_ONCE(rq->last_blocked_load_update_tick)))\n\t\treturn true;\n\n\tupdate_blocked_averages(cpu);\n\n\treturn rq->has_blocked_load;\n}\n\n \nstatic void _nohz_idle_balance(struct rq *this_rq, unsigned int flags)\n{\n\t \n\tunsigned long now = jiffies;\n\tunsigned long next_balance = now + 60*HZ;\n\tbool has_blocked_load = false;\n\tint update_next_balance = 0;\n\tint this_cpu = this_rq->cpu;\n\tint balance_cpu;\n\tstruct rq *rq;\n\n\tSCHED_WARN_ON((flags & NOHZ_KICK_MASK) == NOHZ_BALANCE_KICK);\n\n\t \n\tif (flags & NOHZ_STATS_KICK)\n\t\tWRITE_ONCE(nohz.has_blocked, 0);\n\tif (flags & NOHZ_NEXT_KICK)\n\t\tWRITE_ONCE(nohz.needs_update, 0);\n\n\t \n\tsmp_mb();\n\n\t \n\tfor_each_cpu_wrap(balance_cpu,  nohz.idle_cpus_mask, this_cpu+1) {\n\t\tif (!idle_cpu(balance_cpu))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (need_resched()) {\n\t\t\tif (flags & NOHZ_STATS_KICK)\n\t\t\t\thas_blocked_load = true;\n\t\t\tif (flags & NOHZ_NEXT_KICK)\n\t\t\t\tWRITE_ONCE(nohz.needs_update, 1);\n\t\t\tgoto abort;\n\t\t}\n\n\t\trq = cpu_rq(balance_cpu);\n\n\t\tif (flags & NOHZ_STATS_KICK)\n\t\t\thas_blocked_load |= update_nohz_stats(rq);\n\n\t\t \n\t\tif (time_after_eq(jiffies, rq->next_balance)) {\n\t\t\tstruct rq_flags rf;\n\n\t\t\trq_lock_irqsave(rq, &rf);\n\t\t\tupdate_rq_clock(rq);\n\t\t\trq_unlock_irqrestore(rq, &rf);\n\n\t\t\tif (flags & NOHZ_BALANCE_KICK)\n\t\t\t\trebalance_domains(rq, CPU_IDLE);\n\t\t}\n\n\t\tif (time_after(next_balance, rq->next_balance)) {\n\t\t\tnext_balance = rq->next_balance;\n\t\t\tupdate_next_balance = 1;\n\t\t}\n\t}\n\n\t \n\tif (likely(update_next_balance))\n\t\tnohz.next_balance = next_balance;\n\n\tif (flags & NOHZ_STATS_KICK)\n\t\tWRITE_ONCE(nohz.next_blocked,\n\t\t\t   now + msecs_to_jiffies(LOAD_AVG_PERIOD));\n\nabort:\n\t \n\tif (has_blocked_load)\n\t\tWRITE_ONCE(nohz.has_blocked, 1);\n}\n\n \nstatic bool nohz_idle_balance(struct rq *this_rq, enum cpu_idle_type idle)\n{\n\tunsigned int flags = this_rq->nohz_idle_balance;\n\n\tif (!flags)\n\t\treturn false;\n\n\tthis_rq->nohz_idle_balance = 0;\n\n\tif (idle != CPU_IDLE)\n\t\treturn false;\n\n\t_nohz_idle_balance(this_rq, flags);\n\n\treturn true;\n}\n\n \nvoid nohz_run_idle_balance(int cpu)\n{\n\tunsigned int flags;\n\n\tflags = atomic_fetch_andnot(NOHZ_NEWILB_KICK, nohz_flags(cpu));\n\n\t \n\tif ((flags == NOHZ_NEWILB_KICK) && !need_resched())\n\t\t_nohz_idle_balance(cpu_rq(cpu), NOHZ_STATS_KICK);\n}\n\nstatic void nohz_newidle_balance(struct rq *this_rq)\n{\n\tint this_cpu = this_rq->cpu;\n\n\t \n\tif (!housekeeping_cpu(this_cpu, HK_TYPE_SCHED))\n\t\treturn;\n\n\t \n\tif (this_rq->avg_idle < sysctl_sched_migration_cost)\n\t\treturn;\n\n\t \n\tif (!READ_ONCE(nohz.has_blocked) ||\n\t    time_before(jiffies, READ_ONCE(nohz.next_blocked)))\n\t\treturn;\n\n\t \n\tatomic_or(NOHZ_NEWILB_KICK, nohz_flags(this_cpu));\n}\n\n#else  \nstatic inline void nohz_balancer_kick(struct rq *rq) { }\n\nstatic inline bool nohz_idle_balance(struct rq *this_rq, enum cpu_idle_type idle)\n{\n\treturn false;\n}\n\nstatic inline void nohz_newidle_balance(struct rq *this_rq) { }\n#endif  \n\n \nstatic int newidle_balance(struct rq *this_rq, struct rq_flags *rf)\n{\n\tunsigned long next_balance = jiffies + HZ;\n\tint this_cpu = this_rq->cpu;\n\tu64 t0, t1, curr_cost = 0;\n\tstruct sched_domain *sd;\n\tint pulled_task = 0;\n\n\tupdate_misfit_status(NULL, this_rq);\n\n\t \n\tif (this_rq->ttwu_pending)\n\t\treturn 0;\n\n\t \n\tthis_rq->idle_stamp = rq_clock(this_rq);\n\n\t \n\tif (!cpu_active(this_cpu))\n\t\treturn 0;\n\n\t \n\trq_unpin_lock(this_rq, rf);\n\n\trcu_read_lock();\n\tsd = rcu_dereference_check_sched_domain(this_rq->sd);\n\n\tif (!READ_ONCE(this_rq->rd->overload) ||\n\t    (sd && this_rq->avg_idle < sd->max_newidle_lb_cost)) {\n\n\t\tif (sd)\n\t\t\tupdate_next_balance(sd, &next_balance);\n\t\trcu_read_unlock();\n\n\t\tgoto out;\n\t}\n\trcu_read_unlock();\n\n\traw_spin_rq_unlock(this_rq);\n\n\tt0 = sched_clock_cpu(this_cpu);\n\tupdate_blocked_averages(this_cpu);\n\n\trcu_read_lock();\n\tfor_each_domain(this_cpu, sd) {\n\t\tint continue_balancing = 1;\n\t\tu64 domain_cost;\n\n\t\tupdate_next_balance(sd, &next_balance);\n\n\t\tif (this_rq->avg_idle < curr_cost + sd->max_newidle_lb_cost)\n\t\t\tbreak;\n\n\t\tif (sd->flags & SD_BALANCE_NEWIDLE) {\n\n\t\t\tpulled_task = load_balance(this_cpu, this_rq,\n\t\t\t\t\t\t   sd, CPU_NEWLY_IDLE,\n\t\t\t\t\t\t   &continue_balancing);\n\n\t\t\tt1 = sched_clock_cpu(this_cpu);\n\t\t\tdomain_cost = t1 - t0;\n\t\t\tupdate_newidle_cost(sd, domain_cost);\n\n\t\t\tcurr_cost += domain_cost;\n\t\t\tt0 = t1;\n\t\t}\n\n\t\t \n\t\tif (pulled_task || this_rq->nr_running > 0 ||\n\t\t    this_rq->ttwu_pending)\n\t\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\traw_spin_rq_lock(this_rq);\n\n\tif (curr_cost > this_rq->max_idle_balance_cost)\n\t\tthis_rq->max_idle_balance_cost = curr_cost;\n\n\t \n\tif (this_rq->cfs.h_nr_running && !pulled_task)\n\t\tpulled_task = 1;\n\n\t \n\tif (this_rq->nr_running != this_rq->cfs.h_nr_running)\n\t\tpulled_task = -1;\n\nout:\n\t \n\tif (time_after(this_rq->next_balance, next_balance))\n\t\tthis_rq->next_balance = next_balance;\n\n\tif (pulled_task)\n\t\tthis_rq->idle_stamp = 0;\n\telse\n\t\tnohz_newidle_balance(this_rq);\n\n\trq_repin_lock(this_rq, rf);\n\n\treturn pulled_task;\n}\n\n \nstatic __latent_entropy void run_rebalance_domains(struct softirq_action *h)\n{\n\tstruct rq *this_rq = this_rq();\n\tenum cpu_idle_type idle = this_rq->idle_balance ?\n\t\t\t\t\t\tCPU_IDLE : CPU_NOT_IDLE;\n\n\t \n\tif (nohz_idle_balance(this_rq, idle))\n\t\treturn;\n\n\t \n\tupdate_blocked_averages(this_rq->cpu);\n\trebalance_domains(this_rq, idle);\n}\n\n \nvoid trigger_load_balance(struct rq *rq)\n{\n\t \n\tif (unlikely(on_null_domain(rq) || !cpu_active(cpu_of(rq))))\n\t\treturn;\n\n\tif (time_after_eq(jiffies, rq->next_balance))\n\t\traise_softirq(SCHED_SOFTIRQ);\n\n\tnohz_balancer_kick(rq);\n}\n\nstatic void rq_online_fair(struct rq *rq)\n{\n\tupdate_sysctl();\n\n\tupdate_runtime_enabled(rq);\n}\n\nstatic void rq_offline_fair(struct rq *rq)\n{\n\tupdate_sysctl();\n\n\t \n\tunthrottle_offline_cfs_rqs(rq);\n}\n\n#endif  \n\n#ifdef CONFIG_SCHED_CORE\nstatic inline bool\n__entity_slice_used(struct sched_entity *se, int min_nr_tasks)\n{\n\tu64 rtime = se->sum_exec_runtime - se->prev_sum_exec_runtime;\n\tu64 slice = se->slice;\n\n\treturn (rtime * min_nr_tasks > slice);\n}\n\n#define MIN_NR_TASKS_DURING_FORCEIDLE\t2\nstatic inline void task_tick_core(struct rq *rq, struct task_struct *curr)\n{\n\tif (!sched_core_enabled(rq))\n\t\treturn;\n\n\t \n\tif (rq->core->core_forceidle_count && rq->cfs.nr_running == 1 &&\n\t    __entity_slice_used(&curr->se, MIN_NR_TASKS_DURING_FORCEIDLE))\n\t\tresched_curr(rq);\n}\n\n \nstatic void se_fi_update(const struct sched_entity *se, unsigned int fi_seq,\n\t\t\t bool forceidle)\n{\n\tfor_each_sched_entity(se) {\n\t\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\n\t\tif (forceidle) {\n\t\t\tif (cfs_rq->forceidle_seq == fi_seq)\n\t\t\t\tbreak;\n\t\t\tcfs_rq->forceidle_seq = fi_seq;\n\t\t}\n\n\t\tcfs_rq->min_vruntime_fi = cfs_rq->min_vruntime;\n\t}\n}\n\nvoid task_vruntime_update(struct rq *rq, struct task_struct *p, bool in_fi)\n{\n\tstruct sched_entity *se = &p->se;\n\n\tif (p->sched_class != &fair_sched_class)\n\t\treturn;\n\n\tse_fi_update(se, rq->core->core_forceidle_seq, in_fi);\n}\n\nbool cfs_prio_less(const struct task_struct *a, const struct task_struct *b,\n\t\t\tbool in_fi)\n{\n\tstruct rq *rq = task_rq(a);\n\tconst struct sched_entity *sea = &a->se;\n\tconst struct sched_entity *seb = &b->se;\n\tstruct cfs_rq *cfs_rqa;\n\tstruct cfs_rq *cfs_rqb;\n\ts64 delta;\n\n\tSCHED_WARN_ON(task_rq(b)->core != rq->core);\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\t \n\twhile (sea->cfs_rq->tg != seb->cfs_rq->tg) {\n\t\tint sea_depth = sea->depth;\n\t\tint seb_depth = seb->depth;\n\n\t\tif (sea_depth >= seb_depth)\n\t\t\tsea = parent_entity(sea);\n\t\tif (sea_depth <= seb_depth)\n\t\t\tseb = parent_entity(seb);\n\t}\n\n\tse_fi_update(sea, rq->core->core_forceidle_seq, in_fi);\n\tse_fi_update(seb, rq->core->core_forceidle_seq, in_fi);\n\n\tcfs_rqa = sea->cfs_rq;\n\tcfs_rqb = seb->cfs_rq;\n#else\n\tcfs_rqa = &task_rq(a)->cfs;\n\tcfs_rqb = &task_rq(b)->cfs;\n#endif\n\n\t \n\tdelta = (s64)(sea->vruntime - seb->vruntime) +\n\t\t(s64)(cfs_rqb->min_vruntime_fi - cfs_rqa->min_vruntime_fi);\n\n\treturn delta > 0;\n}\n\nstatic int task_is_throttled_fair(struct task_struct *p, int cpu)\n{\n\tstruct cfs_rq *cfs_rq;\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tcfs_rq = task_group(p)->cfs_rq[cpu];\n#else\n\tcfs_rq = &cpu_rq(cpu)->cfs;\n#endif\n\treturn throttled_hierarchy(cfs_rq);\n}\n#else\nstatic inline void task_tick_core(struct rq *rq, struct task_struct *curr) {}\n#endif\n\n \nstatic void task_tick_fair(struct rq *rq, struct task_struct *curr, int queued)\n{\n\tstruct cfs_rq *cfs_rq;\n\tstruct sched_entity *se = &curr->se;\n\n\tfor_each_sched_entity(se) {\n\t\tcfs_rq = cfs_rq_of(se);\n\t\tentity_tick(cfs_rq, se, queued);\n\t}\n\n\tif (static_branch_unlikely(&sched_numa_balancing))\n\t\ttask_tick_numa(rq, curr);\n\n\tupdate_misfit_status(curr, rq);\n\tupdate_overutilized_status(task_rq(curr));\n\n\ttask_tick_core(rq, curr);\n}\n\n \nstatic void task_fork_fair(struct task_struct *p)\n{\n\tstruct sched_entity *se = &p->se, *curr;\n\tstruct cfs_rq *cfs_rq;\n\tstruct rq *rq = this_rq();\n\tstruct rq_flags rf;\n\n\trq_lock(rq, &rf);\n\tupdate_rq_clock(rq);\n\n\tcfs_rq = task_cfs_rq(current);\n\tcurr = cfs_rq->curr;\n\tif (curr)\n\t\tupdate_curr(cfs_rq);\n\tplace_entity(cfs_rq, se, ENQUEUE_INITIAL);\n\trq_unlock(rq, &rf);\n}\n\n \nstatic void\nprio_changed_fair(struct rq *rq, struct task_struct *p, int oldprio)\n{\n\tif (!task_on_rq_queued(p))\n\t\treturn;\n\n\tif (rq->cfs.nr_running == 1)\n\t\treturn;\n\n\t \n\tif (task_current(rq, p)) {\n\t\tif (p->prio > oldprio)\n\t\t\tresched_curr(rq);\n\t} else\n\t\tcheck_preempt_curr(rq, p, 0);\n}\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n \nstatic void propagate_entity_cfs_rq(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\n\tif (cfs_rq_throttled(cfs_rq))\n\t\treturn;\n\n\tif (!throttled_hierarchy(cfs_rq))\n\t\tlist_add_leaf_cfs_rq(cfs_rq);\n\n\t \n\tse = se->parent;\n\n\tfor_each_sched_entity(se) {\n\t\tcfs_rq = cfs_rq_of(se);\n\n\t\tupdate_load_avg(cfs_rq, se, UPDATE_TG);\n\n\t\tif (cfs_rq_throttled(cfs_rq))\n\t\t\tbreak;\n\n\t\tif (!throttled_hierarchy(cfs_rq))\n\t\t\tlist_add_leaf_cfs_rq(cfs_rq);\n\t}\n}\n#else\nstatic void propagate_entity_cfs_rq(struct sched_entity *se) { }\n#endif\n\nstatic void detach_entity_cfs_rq(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\n#ifdef CONFIG_SMP\n\t \n\tif (!se->avg.last_update_time)\n\t\treturn;\n#endif\n\n\t \n\tupdate_load_avg(cfs_rq, se, 0);\n\tdetach_entity_load_avg(cfs_rq, se);\n\tupdate_tg_load_avg(cfs_rq);\n\tpropagate_entity_cfs_rq(se);\n}\n\nstatic void attach_entity_cfs_rq(struct sched_entity *se)\n{\n\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\n\t \n\tupdate_load_avg(cfs_rq, se, sched_feat(ATTACH_AGE_LOAD) ? 0 : SKIP_AGE_LOAD);\n\tattach_entity_load_avg(cfs_rq, se);\n\tupdate_tg_load_avg(cfs_rq);\n\tpropagate_entity_cfs_rq(se);\n}\n\nstatic void detach_task_cfs_rq(struct task_struct *p)\n{\n\tstruct sched_entity *se = &p->se;\n\n\tdetach_entity_cfs_rq(se);\n}\n\nstatic void attach_task_cfs_rq(struct task_struct *p)\n{\n\tstruct sched_entity *se = &p->se;\n\n\tattach_entity_cfs_rq(se);\n}\n\nstatic void switched_from_fair(struct rq *rq, struct task_struct *p)\n{\n\tdetach_task_cfs_rq(p);\n}\n\nstatic void switched_to_fair(struct rq *rq, struct task_struct *p)\n{\n\tattach_task_cfs_rq(p);\n\n\tif (task_on_rq_queued(p)) {\n\t\t \n\t\tif (task_current(rq, p))\n\t\t\tresched_curr(rq);\n\t\telse\n\t\t\tcheck_preempt_curr(rq, p, 0);\n\t}\n}\n\n \nstatic void set_next_task_fair(struct rq *rq, struct task_struct *p, bool first)\n{\n\tstruct sched_entity *se = &p->se;\n\n#ifdef CONFIG_SMP\n\tif (task_on_rq_queued(p)) {\n\t\t \n\t\tlist_move(&se->group_node, &rq->cfs_tasks);\n\t}\n#endif\n\n\tfor_each_sched_entity(se) {\n\t\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\n\t\tset_next_entity(cfs_rq, se);\n\t\t \n\t\taccount_cfs_rq_runtime(cfs_rq, 0);\n\t}\n}\n\nvoid init_cfs_rq(struct cfs_rq *cfs_rq)\n{\n\tcfs_rq->tasks_timeline = RB_ROOT_CACHED;\n\tu64_u32_store(cfs_rq->min_vruntime, (u64)(-(1LL << 20)));\n#ifdef CONFIG_SMP\n\traw_spin_lock_init(&cfs_rq->removed.lock);\n#endif\n}\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\nstatic void task_change_group_fair(struct task_struct *p)\n{\n\t \n\tif (READ_ONCE(p->__state) == TASK_NEW)\n\t\treturn;\n\n\tdetach_task_cfs_rq(p);\n\n#ifdef CONFIG_SMP\n\t \n\tp->se.avg.last_update_time = 0;\n#endif\n\tset_task_rq(p, task_cpu(p));\n\tattach_task_cfs_rq(p);\n}\n\nvoid free_fair_sched_group(struct task_group *tg)\n{\n\tint i;\n\n\tfor_each_possible_cpu(i) {\n\t\tif (tg->cfs_rq)\n\t\t\tkfree(tg->cfs_rq[i]);\n\t\tif (tg->se)\n\t\t\tkfree(tg->se[i]);\n\t}\n\n\tkfree(tg->cfs_rq);\n\tkfree(tg->se);\n}\n\nint alloc_fair_sched_group(struct task_group *tg, struct task_group *parent)\n{\n\tstruct sched_entity *se;\n\tstruct cfs_rq *cfs_rq;\n\tint i;\n\n\ttg->cfs_rq = kcalloc(nr_cpu_ids, sizeof(cfs_rq), GFP_KERNEL);\n\tif (!tg->cfs_rq)\n\t\tgoto err;\n\ttg->se = kcalloc(nr_cpu_ids, sizeof(se), GFP_KERNEL);\n\tif (!tg->se)\n\t\tgoto err;\n\n\ttg->shares = NICE_0_LOAD;\n\n\tinit_cfs_bandwidth(tg_cfs_bandwidth(tg), tg_cfs_bandwidth(parent));\n\n\tfor_each_possible_cpu(i) {\n\t\tcfs_rq = kzalloc_node(sizeof(struct cfs_rq),\n\t\t\t\t      GFP_KERNEL, cpu_to_node(i));\n\t\tif (!cfs_rq)\n\t\t\tgoto err;\n\n\t\tse = kzalloc_node(sizeof(struct sched_entity_stats),\n\t\t\t\t  GFP_KERNEL, cpu_to_node(i));\n\t\tif (!se)\n\t\t\tgoto err_free_rq;\n\n\t\tinit_cfs_rq(cfs_rq);\n\t\tinit_tg_cfs_entry(tg, cfs_rq, se, i, parent->se[i]);\n\t\tinit_entity_runnable_average(se);\n\t}\n\n\treturn 1;\n\nerr_free_rq:\n\tkfree(cfs_rq);\nerr:\n\treturn 0;\n}\n\nvoid online_fair_sched_group(struct task_group *tg)\n{\n\tstruct sched_entity *se;\n\tstruct rq_flags rf;\n\tstruct rq *rq;\n\tint i;\n\n\tfor_each_possible_cpu(i) {\n\t\trq = cpu_rq(i);\n\t\tse = tg->se[i];\n\t\trq_lock_irq(rq, &rf);\n\t\tupdate_rq_clock(rq);\n\t\tattach_entity_cfs_rq(se);\n\t\tsync_throttle(tg, i);\n\t\trq_unlock_irq(rq, &rf);\n\t}\n}\n\nvoid unregister_fair_sched_group(struct task_group *tg)\n{\n\tunsigned long flags;\n\tstruct rq *rq;\n\tint cpu;\n\n\tdestroy_cfs_bandwidth(tg_cfs_bandwidth(tg));\n\n\tfor_each_possible_cpu(cpu) {\n\t\tif (tg->se[cpu])\n\t\t\tremove_entity_load_avg(tg->se[cpu]);\n\n\t\t \n\t\tif (!tg->cfs_rq[cpu]->on_list)\n\t\t\tcontinue;\n\n\t\trq = cpu_rq(cpu);\n\n\t\traw_spin_rq_lock_irqsave(rq, flags);\n\t\tlist_del_leaf_cfs_rq(tg->cfs_rq[cpu]);\n\t\traw_spin_rq_unlock_irqrestore(rq, flags);\n\t}\n}\n\nvoid init_tg_cfs_entry(struct task_group *tg, struct cfs_rq *cfs_rq,\n\t\t\tstruct sched_entity *se, int cpu,\n\t\t\tstruct sched_entity *parent)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\n\tcfs_rq->tg = tg;\n\tcfs_rq->rq = rq;\n\tinit_cfs_rq_runtime(cfs_rq);\n\n\ttg->cfs_rq[cpu] = cfs_rq;\n\ttg->se[cpu] = se;\n\n\t \n\tif (!se)\n\t\treturn;\n\n\tif (!parent) {\n\t\tse->cfs_rq = &rq->cfs;\n\t\tse->depth = 0;\n\t} else {\n\t\tse->cfs_rq = parent->my_q;\n\t\tse->depth = parent->depth + 1;\n\t}\n\n\tse->my_q = cfs_rq;\n\t \n\tupdate_load_set(&se->load, NICE_0_LOAD);\n\tse->parent = parent;\n}\n\nstatic DEFINE_MUTEX(shares_mutex);\n\nstatic int __sched_group_set_shares(struct task_group *tg, unsigned long shares)\n{\n\tint i;\n\n\tlockdep_assert_held(&shares_mutex);\n\n\t \n\tif (!tg->se[0])\n\t\treturn -EINVAL;\n\n\tshares = clamp(shares, scale_load(MIN_SHARES), scale_load(MAX_SHARES));\n\n\tif (tg->shares == shares)\n\t\treturn 0;\n\n\ttg->shares = shares;\n\tfor_each_possible_cpu(i) {\n\t\tstruct rq *rq = cpu_rq(i);\n\t\tstruct sched_entity *se = tg->se[i];\n\t\tstruct rq_flags rf;\n\n\t\t \n\t\trq_lock_irqsave(rq, &rf);\n\t\tupdate_rq_clock(rq);\n\t\tfor_each_sched_entity(se) {\n\t\t\tupdate_load_avg(cfs_rq_of(se), se, UPDATE_TG);\n\t\t\tupdate_cfs_group(se);\n\t\t}\n\t\trq_unlock_irqrestore(rq, &rf);\n\t}\n\n\treturn 0;\n}\n\nint sched_group_set_shares(struct task_group *tg, unsigned long shares)\n{\n\tint ret;\n\n\tmutex_lock(&shares_mutex);\n\tif (tg_is_idle(tg))\n\t\tret = -EINVAL;\n\telse\n\t\tret = __sched_group_set_shares(tg, shares);\n\tmutex_unlock(&shares_mutex);\n\n\treturn ret;\n}\n\nint sched_group_set_idle(struct task_group *tg, long idle)\n{\n\tint i;\n\n\tif (tg == &root_task_group)\n\t\treturn -EINVAL;\n\n\tif (idle < 0 || idle > 1)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&shares_mutex);\n\n\tif (tg->idle == idle) {\n\t\tmutex_unlock(&shares_mutex);\n\t\treturn 0;\n\t}\n\n\ttg->idle = idle;\n\n\tfor_each_possible_cpu(i) {\n\t\tstruct rq *rq = cpu_rq(i);\n\t\tstruct sched_entity *se = tg->se[i];\n\t\tstruct cfs_rq *parent_cfs_rq, *grp_cfs_rq = tg->cfs_rq[i];\n\t\tbool was_idle = cfs_rq_is_idle(grp_cfs_rq);\n\t\tlong idle_task_delta;\n\t\tstruct rq_flags rf;\n\n\t\trq_lock_irqsave(rq, &rf);\n\n\t\tgrp_cfs_rq->idle = idle;\n\t\tif (WARN_ON_ONCE(was_idle == cfs_rq_is_idle(grp_cfs_rq)))\n\t\t\tgoto next_cpu;\n\n\t\tif (se->on_rq) {\n\t\t\tparent_cfs_rq = cfs_rq_of(se);\n\t\t\tif (cfs_rq_is_idle(grp_cfs_rq))\n\t\t\t\tparent_cfs_rq->idle_nr_running++;\n\t\t\telse\n\t\t\t\tparent_cfs_rq->idle_nr_running--;\n\t\t}\n\n\t\tidle_task_delta = grp_cfs_rq->h_nr_running -\n\t\t\t\t  grp_cfs_rq->idle_h_nr_running;\n\t\tif (!cfs_rq_is_idle(grp_cfs_rq))\n\t\t\tidle_task_delta *= -1;\n\n\t\tfor_each_sched_entity(se) {\n\t\t\tstruct cfs_rq *cfs_rq = cfs_rq_of(se);\n\n\t\t\tif (!se->on_rq)\n\t\t\t\tbreak;\n\n\t\t\tcfs_rq->idle_h_nr_running += idle_task_delta;\n\n\t\t\t \n\t\t\tif (cfs_rq_is_idle(cfs_rq))\n\t\t\t\tbreak;\n\t\t}\n\nnext_cpu:\n\t\trq_unlock_irqrestore(rq, &rf);\n\t}\n\n\t \n\tif (tg_is_idle(tg))\n\t\t__sched_group_set_shares(tg, scale_load(WEIGHT_IDLEPRIO));\n\telse\n\t\t__sched_group_set_shares(tg, NICE_0_LOAD);\n\n\tmutex_unlock(&shares_mutex);\n\treturn 0;\n}\n\n#else  \n\nvoid free_fair_sched_group(struct task_group *tg) { }\n\nint alloc_fair_sched_group(struct task_group *tg, struct task_group *parent)\n{\n\treturn 1;\n}\n\nvoid online_fair_sched_group(struct task_group *tg) { }\n\nvoid unregister_fair_sched_group(struct task_group *tg) { }\n\n#endif  \n\n\nstatic unsigned int get_rr_interval_fair(struct rq *rq, struct task_struct *task)\n{\n\tstruct sched_entity *se = &task->se;\n\tunsigned int rr_interval = 0;\n\n\t \n\tif (rq->cfs.load.weight)\n\t\trr_interval = NS_TO_JIFFIES(se->slice);\n\n\treturn rr_interval;\n}\n\n \nDEFINE_SCHED_CLASS(fair) = {\n\n\t.enqueue_task\t\t= enqueue_task_fair,\n\t.dequeue_task\t\t= dequeue_task_fair,\n\t.yield_task\t\t= yield_task_fair,\n\t.yield_to_task\t\t= yield_to_task_fair,\n\n\t.check_preempt_curr\t= check_preempt_wakeup,\n\n\t.pick_next_task\t\t= __pick_next_task_fair,\n\t.put_prev_task\t\t= put_prev_task_fair,\n\t.set_next_task          = set_next_task_fair,\n\n#ifdef CONFIG_SMP\n\t.balance\t\t= balance_fair,\n\t.pick_task\t\t= pick_task_fair,\n\t.select_task_rq\t\t= select_task_rq_fair,\n\t.migrate_task_rq\t= migrate_task_rq_fair,\n\n\t.rq_online\t\t= rq_online_fair,\n\t.rq_offline\t\t= rq_offline_fair,\n\n\t.task_dead\t\t= task_dead_fair,\n\t.set_cpus_allowed\t= set_cpus_allowed_common,\n#endif\n\n\t.task_tick\t\t= task_tick_fair,\n\t.task_fork\t\t= task_fork_fair,\n\n\t.prio_changed\t\t= prio_changed_fair,\n\t.switched_from\t\t= switched_from_fair,\n\t.switched_to\t\t= switched_to_fair,\n\n\t.get_rr_interval\t= get_rr_interval_fair,\n\n\t.update_curr\t\t= update_curr_fair,\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\t.task_change_group\t= task_change_group_fair,\n#endif\n\n#ifdef CONFIG_SCHED_CORE\n\t.task_is_throttled\t= task_is_throttled_fair,\n#endif\n\n#ifdef CONFIG_UCLAMP_TASK\n\t.uclamp_enabled\t\t= 1,\n#endif\n};\n\n#ifdef CONFIG_SCHED_DEBUG\nvoid print_cfs_stats(struct seq_file *m, int cpu)\n{\n\tstruct cfs_rq *cfs_rq, *pos;\n\n\trcu_read_lock();\n\tfor_each_leaf_cfs_rq_safe(cpu_rq(cpu), cfs_rq, pos)\n\t\tprint_cfs_rq(m, cpu, cfs_rq);\n\trcu_read_unlock();\n}\n\n#ifdef CONFIG_NUMA_BALANCING\nvoid show_numa_stats(struct task_struct *p, struct seq_file *m)\n{\n\tint node;\n\tunsigned long tsf = 0, tpf = 0, gsf = 0, gpf = 0;\n\tstruct numa_group *ng;\n\n\trcu_read_lock();\n\tng = rcu_dereference(p->numa_group);\n\tfor_each_online_node(node) {\n\t\tif (p->numa_faults) {\n\t\t\ttsf = p->numa_faults[task_faults_idx(NUMA_MEM, node, 0)];\n\t\t\ttpf = p->numa_faults[task_faults_idx(NUMA_MEM, node, 1)];\n\t\t}\n\t\tif (ng) {\n\t\t\tgsf = ng->faults[task_faults_idx(NUMA_MEM, node, 0)],\n\t\t\tgpf = ng->faults[task_faults_idx(NUMA_MEM, node, 1)];\n\t\t}\n\t\tprint_numa_stats(m, node, tsf, tpf, gsf, gpf);\n\t}\n\trcu_read_unlock();\n}\n#endif  \n#endif  \n\n__init void init_sched_fair_class(void)\n{\n#ifdef CONFIG_SMP\n\tint i;\n\n\tfor_each_possible_cpu(i) {\n\t\tzalloc_cpumask_var_node(&per_cpu(load_balance_mask, i), GFP_KERNEL, cpu_to_node(i));\n\t\tzalloc_cpumask_var_node(&per_cpu(select_rq_mask,    i), GFP_KERNEL, cpu_to_node(i));\n\t\tzalloc_cpumask_var_node(&per_cpu(should_we_balance_tmpmask, i),\n\t\t\t\t\tGFP_KERNEL, cpu_to_node(i));\n\n#ifdef CONFIG_CFS_BANDWIDTH\n\t\tINIT_CSD(&cpu_rq(i)->cfsb_csd, __cfsb_csd_unthrottle, cpu_rq(i));\n\t\tINIT_LIST_HEAD(&cpu_rq(i)->cfsb_csd_list);\n#endif\n\t}\n\n\topen_softirq(SCHED_SOFTIRQ, run_rebalance_domains);\n\n#ifdef CONFIG_NO_HZ_COMMON\n\tnohz.next_balance = jiffies;\n\tnohz.next_blocked = jiffies;\n\tzalloc_cpumask_var(&nohz.idle_cpus_mask, GFP_NOWAIT);\n#endif\n#endif  \n\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}