{
  "module_name": "core.c",
  "hash_id": "15d1edf51a8c85f03ad3fe286fb52342c19846b36aa8641b0d69fe6a753672e5",
  "original_prompt": "Ingested from linux-6.6.14/kernel/sched/core.c",
  "human_readable_source": "\n \n#include <linux/highmem.h>\n#include <linux/hrtimer_api.h>\n#include <linux/ktime_api.h>\n#include <linux/sched/signal.h>\n#include <linux/syscalls_api.h>\n#include <linux/debug_locks.h>\n#include <linux/prefetch.h>\n#include <linux/capability.h>\n#include <linux/pgtable_api.h>\n#include <linux/wait_bit.h>\n#include <linux/jiffies.h>\n#include <linux/spinlock_api.h>\n#include <linux/cpumask_api.h>\n#include <linux/lockdep_api.h>\n#include <linux/hardirq.h>\n#include <linux/softirq.h>\n#include <linux/refcount_api.h>\n#include <linux/topology.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/cond_resched.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/init.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/rseq_api.h>\n#include <linux/sched/rt.h>\n\n#include <linux/blkdev.h>\n#include <linux/context_tracking.h>\n#include <linux/cpuset.h>\n#include <linux/delayacct.h>\n#include <linux/init_task.h>\n#include <linux/interrupt.h>\n#include <linux/ioprio.h>\n#include <linux/kallsyms.h>\n#include <linux/kcov.h>\n#include <linux/kprobes.h>\n#include <linux/llist_api.h>\n#include <linux/mmu_context.h>\n#include <linux/mmzone.h>\n#include <linux/mutex_api.h>\n#include <linux/nmi.h>\n#include <linux/nospec.h>\n#include <linux/perf_event_api.h>\n#include <linux/profile.h>\n#include <linux/psi.h>\n#include <linux/rcuwait_api.h>\n#include <linux/sched/wake_q.h>\n#include <linux/scs.h>\n#include <linux/slab.h>\n#include <linux/syscalls.h>\n#include <linux/vtime.h>\n#include <linux/wait_api.h>\n#include <linux/workqueue_api.h>\n\n#ifdef CONFIG_PREEMPT_DYNAMIC\n# ifdef CONFIG_GENERIC_ENTRY\n#  include <linux/entry-common.h>\n# endif\n#endif\n\n#include <uapi/linux/sched/types.h>\n\n#include <asm/irq_regs.h>\n#include <asm/switch_to.h>\n#include <asm/tlb.h>\n\n#define CREATE_TRACE_POINTS\n#include <linux/sched/rseq_api.h>\n#include <trace/events/sched.h>\n#include <trace/events/ipi.h>\n#undef CREATE_TRACE_POINTS\n\n#include \"sched.h\"\n#include \"stats.h\"\n#include \"autogroup.h\"\n\n#include \"autogroup.h\"\n#include \"pelt.h\"\n#include \"smp.h\"\n#include \"stats.h\"\n\n#include \"../workqueue_internal.h\"\n#include \"../../io_uring/io-wq.h\"\n#include \"../smpboot.h\"\n\nEXPORT_TRACEPOINT_SYMBOL_GPL(ipi_send_cpu);\nEXPORT_TRACEPOINT_SYMBOL_GPL(ipi_send_cpumask);\n\n \nEXPORT_TRACEPOINT_SYMBOL_GPL(pelt_cfs_tp);\nEXPORT_TRACEPOINT_SYMBOL_GPL(pelt_rt_tp);\nEXPORT_TRACEPOINT_SYMBOL_GPL(pelt_dl_tp);\nEXPORT_TRACEPOINT_SYMBOL_GPL(pelt_irq_tp);\nEXPORT_TRACEPOINT_SYMBOL_GPL(pelt_se_tp);\nEXPORT_TRACEPOINT_SYMBOL_GPL(pelt_thermal_tp);\nEXPORT_TRACEPOINT_SYMBOL_GPL(sched_cpu_capacity_tp);\nEXPORT_TRACEPOINT_SYMBOL_GPL(sched_overutilized_tp);\nEXPORT_TRACEPOINT_SYMBOL_GPL(sched_util_est_cfs_tp);\nEXPORT_TRACEPOINT_SYMBOL_GPL(sched_util_est_se_tp);\nEXPORT_TRACEPOINT_SYMBOL_GPL(sched_update_nr_running_tp);\n\nDEFINE_PER_CPU_SHARED_ALIGNED(struct rq, runqueues);\n\n#ifdef CONFIG_SCHED_DEBUG\n \n#define SCHED_FEAT(name, enabled)\t\\\n\t(1UL << __SCHED_FEAT_##name) * enabled |\nconst_debug unsigned int sysctl_sched_features =\n#include \"features.h\"\n\t0;\n#undef SCHED_FEAT\n\n \n__read_mostly int sysctl_resched_latency_warn_ms = 100;\n__read_mostly int sysctl_resched_latency_warn_once = 1;\n#endif  \n\n \nconst_debug unsigned int sysctl_sched_nr_migrate = SCHED_NR_MIGRATE_BREAK;\n\n__read_mostly int scheduler_running;\n\n#ifdef CONFIG_SCHED_CORE\n\nDEFINE_STATIC_KEY_FALSE(__sched_core_enabled);\n\n \nstatic inline int __task_prio(const struct task_struct *p)\n{\n\tif (p->sched_class == &stop_sched_class)  \n\t\treturn -2;\n\n\tif (rt_prio(p->prio))  \n\t\treturn p->prio;  \n\n\tif (p->sched_class == &idle_sched_class)\n\t\treturn MAX_RT_PRIO + NICE_WIDTH;  \n\n\treturn MAX_RT_PRIO + MAX_NICE;  \n}\n\n \n\n \nstatic inline bool prio_less(const struct task_struct *a,\n\t\t\t     const struct task_struct *b, bool in_fi)\n{\n\n\tint pa = __task_prio(a), pb = __task_prio(b);\n\n\tif (-pa < -pb)\n\t\treturn true;\n\n\tif (-pb < -pa)\n\t\treturn false;\n\n\tif (pa == -1)  \n\t\treturn !dl_time_before(a->dl.deadline, b->dl.deadline);\n\n\tif (pa == MAX_RT_PRIO + MAX_NICE)\t \n\t\treturn cfs_prio_less(a, b, in_fi);\n\n\treturn false;\n}\n\nstatic inline bool __sched_core_less(const struct task_struct *a,\n\t\t\t\t     const struct task_struct *b)\n{\n\tif (a->core_cookie < b->core_cookie)\n\t\treturn true;\n\n\tif (a->core_cookie > b->core_cookie)\n\t\treturn false;\n\n\t \n\tif (prio_less(b, a, !!task_rq(a)->core->core_forceidle_count))\n\t\treturn true;\n\n\treturn false;\n}\n\n#define __node_2_sc(node) rb_entry((node), struct task_struct, core_node)\n\nstatic inline bool rb_sched_core_less(struct rb_node *a, const struct rb_node *b)\n{\n\treturn __sched_core_less(__node_2_sc(a), __node_2_sc(b));\n}\n\nstatic inline int rb_sched_core_cmp(const void *key, const struct rb_node *node)\n{\n\tconst struct task_struct *p = __node_2_sc(node);\n\tunsigned long cookie = (unsigned long)key;\n\n\tif (cookie < p->core_cookie)\n\t\treturn -1;\n\n\tif (cookie > p->core_cookie)\n\t\treturn 1;\n\n\treturn 0;\n}\n\nvoid sched_core_enqueue(struct rq *rq, struct task_struct *p)\n{\n\trq->core->core_task_seq++;\n\n\tif (!p->core_cookie)\n\t\treturn;\n\n\trb_add(&p->core_node, &rq->core_tree, rb_sched_core_less);\n}\n\nvoid sched_core_dequeue(struct rq *rq, struct task_struct *p, int flags)\n{\n\trq->core->core_task_seq++;\n\n\tif (sched_core_enqueued(p)) {\n\t\trb_erase(&p->core_node, &rq->core_tree);\n\t\tRB_CLEAR_NODE(&p->core_node);\n\t}\n\n\t \n\tif (!(flags & DEQUEUE_SAVE) && rq->nr_running == 1 &&\n\t    rq->core->core_forceidle_count && rq->curr == rq->idle)\n\t\tresched_curr(rq);\n}\n\nstatic int sched_task_is_throttled(struct task_struct *p, int cpu)\n{\n\tif (p->sched_class->task_is_throttled)\n\t\treturn p->sched_class->task_is_throttled(p, cpu);\n\n\treturn 0;\n}\n\nstatic struct task_struct *sched_core_next(struct task_struct *p, unsigned long cookie)\n{\n\tstruct rb_node *node = &p->core_node;\n\tint cpu = task_cpu(p);\n\n\tdo {\n\t\tnode = rb_next(node);\n\t\tif (!node)\n\t\t\treturn NULL;\n\n\t\tp = __node_2_sc(node);\n\t\tif (p->core_cookie != cookie)\n\t\t\treturn NULL;\n\n\t} while (sched_task_is_throttled(p, cpu));\n\n\treturn p;\n}\n\n \nstatic struct task_struct *sched_core_find(struct rq *rq, unsigned long cookie)\n{\n\tstruct task_struct *p;\n\tstruct rb_node *node;\n\n\tnode = rb_find_first((void *)cookie, &rq->core_tree, rb_sched_core_cmp);\n\tif (!node)\n\t\treturn NULL;\n\n\tp = __node_2_sc(node);\n\tif (!sched_task_is_throttled(p, rq->cpu))\n\t\treturn p;\n\n\treturn sched_core_next(p, cookie);\n}\n\n \n\nstatic DEFINE_MUTEX(sched_core_mutex);\nstatic atomic_t sched_core_count;\nstatic struct cpumask sched_core_mask;\n\nstatic void sched_core_lock(int cpu, unsigned long *flags)\n{\n\tconst struct cpumask *smt_mask = cpu_smt_mask(cpu);\n\tint t, i = 0;\n\n\tlocal_irq_save(*flags);\n\tfor_each_cpu(t, smt_mask)\n\t\traw_spin_lock_nested(&cpu_rq(t)->__lock, i++);\n}\n\nstatic void sched_core_unlock(int cpu, unsigned long *flags)\n{\n\tconst struct cpumask *smt_mask = cpu_smt_mask(cpu);\n\tint t;\n\n\tfor_each_cpu(t, smt_mask)\n\t\traw_spin_unlock(&cpu_rq(t)->__lock);\n\tlocal_irq_restore(*flags);\n}\n\nstatic void __sched_core_flip(bool enabled)\n{\n\tunsigned long flags;\n\tint cpu, t;\n\n\tcpus_read_lock();\n\n\t \n\tcpumask_copy(&sched_core_mask, cpu_online_mask);\n\tfor_each_cpu(cpu, &sched_core_mask) {\n\t\tconst struct cpumask *smt_mask = cpu_smt_mask(cpu);\n\n\t\tsched_core_lock(cpu, &flags);\n\n\t\tfor_each_cpu(t, smt_mask)\n\t\t\tcpu_rq(t)->core_enabled = enabled;\n\n\t\tcpu_rq(cpu)->core->core_forceidle_start = 0;\n\n\t\tsched_core_unlock(cpu, &flags);\n\n\t\tcpumask_andnot(&sched_core_mask, &sched_core_mask, smt_mask);\n\t}\n\n\t \n\tfor_each_cpu_andnot(cpu, cpu_possible_mask, cpu_online_mask)\n\t\tcpu_rq(cpu)->core_enabled = enabled;\n\n\tcpus_read_unlock();\n}\n\nstatic void sched_core_assert_empty(void)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu)\n\t\tWARN_ON_ONCE(!RB_EMPTY_ROOT(&cpu_rq(cpu)->core_tree));\n}\n\nstatic void __sched_core_enable(void)\n{\n\tstatic_branch_enable(&__sched_core_enabled);\n\t \n\tsynchronize_rcu();\n\t__sched_core_flip(true);\n\tsched_core_assert_empty();\n}\n\nstatic void __sched_core_disable(void)\n{\n\tsched_core_assert_empty();\n\t__sched_core_flip(false);\n\tstatic_branch_disable(&__sched_core_enabled);\n}\n\nvoid sched_core_get(void)\n{\n\tif (atomic_inc_not_zero(&sched_core_count))\n\t\treturn;\n\n\tmutex_lock(&sched_core_mutex);\n\tif (!atomic_read(&sched_core_count))\n\t\t__sched_core_enable();\n\n\tsmp_mb__before_atomic();\n\tatomic_inc(&sched_core_count);\n\tmutex_unlock(&sched_core_mutex);\n}\n\nstatic void __sched_core_put(struct work_struct *work)\n{\n\tif (atomic_dec_and_mutex_lock(&sched_core_count, &sched_core_mutex)) {\n\t\t__sched_core_disable();\n\t\tmutex_unlock(&sched_core_mutex);\n\t}\n}\n\nvoid sched_core_put(void)\n{\n\tstatic DECLARE_WORK(_work, __sched_core_put);\n\n\t \n\tif (!atomic_add_unless(&sched_core_count, -1, 1))\n\t\tschedule_work(&_work);\n}\n\n#else  \n\nstatic inline void sched_core_enqueue(struct rq *rq, struct task_struct *p) { }\nstatic inline void\nsched_core_dequeue(struct rq *rq, struct task_struct *p, int flags) { }\n\n#endif  \n\n \n\nvoid raw_spin_rq_lock_nested(struct rq *rq, int subclass)\n{\n\traw_spinlock_t *lock;\n\n\t \n\tpreempt_disable();\n\tif (sched_core_disabled()) {\n\t\traw_spin_lock_nested(&rq->__lock, subclass);\n\t\t \n\t\tpreempt_enable_no_resched();\n\t\treturn;\n\t}\n\n\tfor (;;) {\n\t\tlock = __rq_lockp(rq);\n\t\traw_spin_lock_nested(lock, subclass);\n\t\tif (likely(lock == __rq_lockp(rq))) {\n\t\t\t \n\t\t\tpreempt_enable_no_resched();\n\t\t\treturn;\n\t\t}\n\t\traw_spin_unlock(lock);\n\t}\n}\n\nbool raw_spin_rq_trylock(struct rq *rq)\n{\n\traw_spinlock_t *lock;\n\tbool ret;\n\n\t \n\tpreempt_disable();\n\tif (sched_core_disabled()) {\n\t\tret = raw_spin_trylock(&rq->__lock);\n\t\tpreempt_enable();\n\t\treturn ret;\n\t}\n\n\tfor (;;) {\n\t\tlock = __rq_lockp(rq);\n\t\tret = raw_spin_trylock(lock);\n\t\tif (!ret || (likely(lock == __rq_lockp(rq)))) {\n\t\t\tpreempt_enable();\n\t\t\treturn ret;\n\t\t}\n\t\traw_spin_unlock(lock);\n\t}\n}\n\nvoid raw_spin_rq_unlock(struct rq *rq)\n{\n\traw_spin_unlock(rq_lockp(rq));\n}\n\n#ifdef CONFIG_SMP\n \nvoid double_rq_lock(struct rq *rq1, struct rq *rq2)\n{\n\tlockdep_assert_irqs_disabled();\n\n\tif (rq_order_less(rq2, rq1))\n\t\tswap(rq1, rq2);\n\n\traw_spin_rq_lock(rq1);\n\tif (__rq_lockp(rq1) != __rq_lockp(rq2))\n\t\traw_spin_rq_lock_nested(rq2, SINGLE_DEPTH_NESTING);\n\n\tdouble_rq_clock_clear_update(rq1, rq2);\n}\n#endif\n\n \nstruct rq *__task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(rq->lock)\n{\n\tstruct rq *rq;\n\n\tlockdep_assert_held(&p->pi_lock);\n\n\tfor (;;) {\n\t\trq = task_rq(p);\n\t\traw_spin_rq_lock(rq);\n\t\tif (likely(rq == task_rq(p) && !task_on_rq_migrating(p))) {\n\t\t\trq_pin_lock(rq, rf);\n\t\t\treturn rq;\n\t\t}\n\t\traw_spin_rq_unlock(rq);\n\n\t\twhile (unlikely(task_on_rq_migrating(p)))\n\t\t\tcpu_relax();\n\t}\n}\n\n \nstruct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)\n\t__acquires(p->pi_lock)\n\t__acquires(rq->lock)\n{\n\tstruct rq *rq;\n\n\tfor (;;) {\n\t\traw_spin_lock_irqsave(&p->pi_lock, rf->flags);\n\t\trq = task_rq(p);\n\t\traw_spin_rq_lock(rq);\n\t\t \n\t\tif (likely(rq == task_rq(p) && !task_on_rq_migrating(p))) {\n\t\t\trq_pin_lock(rq, rf);\n\t\t\treturn rq;\n\t\t}\n\t\traw_spin_rq_unlock(rq);\n\t\traw_spin_unlock_irqrestore(&p->pi_lock, rf->flags);\n\n\t\twhile (unlikely(task_on_rq_migrating(p)))\n\t\t\tcpu_relax();\n\t}\n}\n\n \n\nstatic void update_rq_clock_task(struct rq *rq, s64 delta)\n{\n \n\ts64 __maybe_unused steal = 0, irq_delta = 0;\n\n#ifdef CONFIG_IRQ_TIME_ACCOUNTING\n\tirq_delta = irq_time_read(cpu_of(rq)) - rq->prev_irq_time;\n\n\t \n\tif (irq_delta > delta)\n\t\tirq_delta = delta;\n\n\trq->prev_irq_time += irq_delta;\n\tdelta -= irq_delta;\n\tpsi_account_irqtime(rq->curr, irq_delta);\n\tdelayacct_irq(rq->curr, irq_delta);\n#endif\n#ifdef CONFIG_PARAVIRT_TIME_ACCOUNTING\n\tif (static_key_false((&paravirt_steal_rq_enabled))) {\n\t\tsteal = paravirt_steal_clock(cpu_of(rq));\n\t\tsteal -= rq->prev_steal_time_rq;\n\n\t\tif (unlikely(steal > delta))\n\t\t\tsteal = delta;\n\n\t\trq->prev_steal_time_rq += steal;\n\t\tdelta -= steal;\n\t}\n#endif\n\n\trq->clock_task += delta;\n\n#ifdef CONFIG_HAVE_SCHED_AVG_IRQ\n\tif ((irq_delta + steal) && sched_feat(NONTASK_CAPACITY))\n\t\tupdate_irq_load_avg(rq, irq_delta + steal);\n#endif\n\tupdate_rq_clock_pelt(rq, delta);\n}\n\nvoid update_rq_clock(struct rq *rq)\n{\n\ts64 delta;\n\n\tlockdep_assert_rq_held(rq);\n\n\tif (rq->clock_update_flags & RQCF_ACT_SKIP)\n\t\treturn;\n\n#ifdef CONFIG_SCHED_DEBUG\n\tif (sched_feat(WARN_DOUBLE_CLOCK))\n\t\tSCHED_WARN_ON(rq->clock_update_flags & RQCF_UPDATED);\n\trq->clock_update_flags |= RQCF_UPDATED;\n#endif\n\n\tdelta = sched_clock_cpu(cpu_of(rq)) - rq->clock;\n\tif (delta < 0)\n\t\treturn;\n\trq->clock += delta;\n\tupdate_rq_clock_task(rq, delta);\n}\n\n#ifdef CONFIG_SCHED_HRTICK\n \n\nstatic void hrtick_clear(struct rq *rq)\n{\n\tif (hrtimer_active(&rq->hrtick_timer))\n\t\thrtimer_cancel(&rq->hrtick_timer);\n}\n\n \nstatic enum hrtimer_restart hrtick(struct hrtimer *timer)\n{\n\tstruct rq *rq = container_of(timer, struct rq, hrtick_timer);\n\tstruct rq_flags rf;\n\n\tWARN_ON_ONCE(cpu_of(rq) != smp_processor_id());\n\n\trq_lock(rq, &rf);\n\tupdate_rq_clock(rq);\n\trq->curr->sched_class->task_tick(rq, rq->curr, 1);\n\trq_unlock(rq, &rf);\n\n\treturn HRTIMER_NORESTART;\n}\n\n#ifdef CONFIG_SMP\n\nstatic void __hrtick_restart(struct rq *rq)\n{\n\tstruct hrtimer *timer = &rq->hrtick_timer;\n\tktime_t time = rq->hrtick_time;\n\n\thrtimer_start(timer, time, HRTIMER_MODE_ABS_PINNED_HARD);\n}\n\n \nstatic void __hrtick_start(void *arg)\n{\n\tstruct rq *rq = arg;\n\tstruct rq_flags rf;\n\n\trq_lock(rq, &rf);\n\t__hrtick_restart(rq);\n\trq_unlock(rq, &rf);\n}\n\n \nvoid hrtick_start(struct rq *rq, u64 delay)\n{\n\tstruct hrtimer *timer = &rq->hrtick_timer;\n\ts64 delta;\n\n\t \n\tdelta = max_t(s64, delay, 10000LL);\n\trq->hrtick_time = ktime_add_ns(timer->base->get_time(), delta);\n\n\tif (rq == this_rq())\n\t\t__hrtick_restart(rq);\n\telse\n\t\tsmp_call_function_single_async(cpu_of(rq), &rq->hrtick_csd);\n}\n\n#else\n \nvoid hrtick_start(struct rq *rq, u64 delay)\n{\n\t \n\tdelay = max_t(u64, delay, 10000LL);\n\thrtimer_start(&rq->hrtick_timer, ns_to_ktime(delay),\n\t\t      HRTIMER_MODE_REL_PINNED_HARD);\n}\n\n#endif  \n\nstatic void hrtick_rq_init(struct rq *rq)\n{\n#ifdef CONFIG_SMP\n\tINIT_CSD(&rq->hrtick_csd, __hrtick_start, rq);\n#endif\n\thrtimer_init(&rq->hrtick_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL_HARD);\n\trq->hrtick_timer.function = hrtick;\n}\n#else\t \nstatic inline void hrtick_clear(struct rq *rq)\n{\n}\n\nstatic inline void hrtick_rq_init(struct rq *rq)\n{\n}\n#endif\t \n\n \n#define fetch_or(ptr, mask)\t\t\t\t\t\t\\\n\t({\t\t\t\t\t\t\t\t\\\n\t\ttypeof(ptr) _ptr = (ptr);\t\t\t\t\\\n\t\ttypeof(mask) _mask = (mask);\t\t\t\t\\\n\t\ttypeof(*_ptr) _val = *_ptr;\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\tdo {\t\t\t\t\t\t\t\\\n\t\t} while (!try_cmpxchg(_ptr, &_val, _val | _mask));\t\\\n\t_val;\t\t\t\t\t\t\t\t\\\n})\n\n#if defined(CONFIG_SMP) && defined(TIF_POLLING_NRFLAG)\n \nstatic inline bool set_nr_and_not_polling(struct task_struct *p)\n{\n\tstruct thread_info *ti = task_thread_info(p);\n\treturn !(fetch_or(&ti->flags, _TIF_NEED_RESCHED) & _TIF_POLLING_NRFLAG);\n}\n\n \nstatic bool set_nr_if_polling(struct task_struct *p)\n{\n\tstruct thread_info *ti = task_thread_info(p);\n\ttypeof(ti->flags) val = READ_ONCE(ti->flags);\n\n\tfor (;;) {\n\t\tif (!(val & _TIF_POLLING_NRFLAG))\n\t\t\treturn false;\n\t\tif (val & _TIF_NEED_RESCHED)\n\t\t\treturn true;\n\t\tif (try_cmpxchg(&ti->flags, &val, val | _TIF_NEED_RESCHED))\n\t\t\tbreak;\n\t}\n\treturn true;\n}\n\n#else\nstatic inline bool set_nr_and_not_polling(struct task_struct *p)\n{\n\tset_tsk_need_resched(p);\n\treturn true;\n}\n\n#ifdef CONFIG_SMP\nstatic inline bool set_nr_if_polling(struct task_struct *p)\n{\n\treturn false;\n}\n#endif\n#endif\n\nstatic bool __wake_q_add(struct wake_q_head *head, struct task_struct *task)\n{\n\tstruct wake_q_node *node = &task->wake_q;\n\n\t \n\tsmp_mb__before_atomic();\n\tif (unlikely(cmpxchg_relaxed(&node->next, NULL, WAKE_Q_TAIL)))\n\t\treturn false;\n\n\t \n\t*head->lastp = node;\n\thead->lastp = &node->next;\n\treturn true;\n}\n\n \nvoid wake_q_add(struct wake_q_head *head, struct task_struct *task)\n{\n\tif (__wake_q_add(head, task))\n\t\tget_task_struct(task);\n}\n\n \nvoid wake_q_add_safe(struct wake_q_head *head, struct task_struct *task)\n{\n\tif (!__wake_q_add(head, task))\n\t\tput_task_struct(task);\n}\n\nvoid wake_up_q(struct wake_q_head *head)\n{\n\tstruct wake_q_node *node = head->first;\n\n\twhile (node != WAKE_Q_TAIL) {\n\t\tstruct task_struct *task;\n\n\t\ttask = container_of(node, struct task_struct, wake_q);\n\t\t \n\t\tnode = node->next;\n\t\ttask->wake_q.next = NULL;\n\n\t\t \n\t\twake_up_process(task);\n\t\tput_task_struct(task);\n\t}\n}\n\n \nvoid resched_curr(struct rq *rq)\n{\n\tstruct task_struct *curr = rq->curr;\n\tint cpu;\n\n\tlockdep_assert_rq_held(rq);\n\n\tif (test_tsk_need_resched(curr))\n\t\treturn;\n\n\tcpu = cpu_of(rq);\n\n\tif (cpu == smp_processor_id()) {\n\t\tset_tsk_need_resched(curr);\n\t\tset_preempt_need_resched();\n\t\treturn;\n\t}\n\n\tif (set_nr_and_not_polling(curr))\n\t\tsmp_send_reschedule(cpu);\n\telse\n\t\ttrace_sched_wake_idle_without_ipi(cpu);\n}\n\nvoid resched_cpu(int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long flags;\n\n\traw_spin_rq_lock_irqsave(rq, flags);\n\tif (cpu_online(cpu) || cpu == smp_processor_id())\n\t\tresched_curr(rq);\n\traw_spin_rq_unlock_irqrestore(rq, flags);\n}\n\n#ifdef CONFIG_SMP\n#ifdef CONFIG_NO_HZ_COMMON\n \nint get_nohz_timer_target(void)\n{\n\tint i, cpu = smp_processor_id(), default_cpu = -1;\n\tstruct sched_domain *sd;\n\tconst struct cpumask *hk_mask;\n\n\tif (housekeeping_cpu(cpu, HK_TYPE_TIMER)) {\n\t\tif (!idle_cpu(cpu))\n\t\t\treturn cpu;\n\t\tdefault_cpu = cpu;\n\t}\n\n\thk_mask = housekeeping_cpumask(HK_TYPE_TIMER);\n\n\tguard(rcu)();\n\n\tfor_each_domain(cpu, sd) {\n\t\tfor_each_cpu_and(i, sched_domain_span(sd), hk_mask) {\n\t\t\tif (cpu == i)\n\t\t\t\tcontinue;\n\n\t\t\tif (!idle_cpu(i))\n\t\t\t\treturn i;\n\t\t}\n\t}\n\n\tif (default_cpu == -1)\n\t\tdefault_cpu = housekeeping_any_cpu(HK_TYPE_TIMER);\n\n\treturn default_cpu;\n}\n\n \nstatic void wake_up_idle_cpu(int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\n\tif (cpu == smp_processor_id())\n\t\treturn;\n\n\tif (set_nr_and_not_polling(rq->idle))\n\t\tsmp_send_reschedule(cpu);\n\telse\n\t\ttrace_sched_wake_idle_without_ipi(cpu);\n}\n\nstatic bool wake_up_full_nohz_cpu(int cpu)\n{\n\t \n\tif (cpu_is_offline(cpu))\n\t\treturn true;   \n\tif (tick_nohz_full_cpu(cpu)) {\n\t\tif (cpu != smp_processor_id() ||\n\t\t    tick_nohz_tick_stopped())\n\t\t\ttick_nohz_full_kick_cpu(cpu);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nvoid wake_up_nohz_cpu(int cpu)\n{\n\tif (!wake_up_full_nohz_cpu(cpu))\n\t\twake_up_idle_cpu(cpu);\n}\n\nstatic void nohz_csd_func(void *info)\n{\n\tstruct rq *rq = info;\n\tint cpu = cpu_of(rq);\n\tunsigned int flags;\n\n\t \n\tflags = atomic_fetch_andnot(NOHZ_KICK_MASK | NOHZ_NEWILB_KICK, nohz_flags(cpu));\n\tWARN_ON(!(flags & NOHZ_KICK_MASK));\n\n\trq->idle_balance = idle_cpu(cpu);\n\tif (rq->idle_balance && !need_resched()) {\n\t\trq->nohz_idle_balance = flags;\n\t\traise_softirq_irqoff(SCHED_SOFTIRQ);\n\t}\n}\n\n#endif  \n\n#ifdef CONFIG_NO_HZ_FULL\nstatic inline bool __need_bw_check(struct rq *rq, struct task_struct *p)\n{\n\tif (rq->nr_running != 1)\n\t\treturn false;\n\n\tif (p->sched_class != &fair_sched_class)\n\t\treturn false;\n\n\tif (!task_on_rq_queued(p))\n\t\treturn false;\n\n\treturn true;\n}\n\nbool sched_can_stop_tick(struct rq *rq)\n{\n\tint fifo_nr_running;\n\n\t \n\tif (rq->dl.dl_nr_running)\n\t\treturn false;\n\n\t \n\tif (rq->rt.rr_nr_running) {\n\t\tif (rq->rt.rr_nr_running == 1)\n\t\t\treturn true;\n\t\telse\n\t\t\treturn false;\n\t}\n\n\t \n\tfifo_nr_running = rq->rt.rt_nr_running - rq->rt.rr_nr_running;\n\tif (fifo_nr_running)\n\t\treturn true;\n\n\t \n\tif (rq->nr_running > 1)\n\t\treturn false;\n\n\t \n\tif (sched_feat(HZ_BW) && __need_bw_check(rq, rq->curr)) {\n\t\tif (cfs_task_bw_constrained(rq->curr))\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n#endif  \n#endif  \n\n#if defined(CONFIG_RT_GROUP_SCHED) || (defined(CONFIG_FAIR_GROUP_SCHED) && \\\n\t\t\t(defined(CONFIG_SMP) || defined(CONFIG_CFS_BANDWIDTH)))\n \nint walk_tg_tree_from(struct task_group *from,\n\t\t\t     tg_visitor down, tg_visitor up, void *data)\n{\n\tstruct task_group *parent, *child;\n\tint ret;\n\n\tparent = from;\n\ndown:\n\tret = (*down)(parent, data);\n\tif (ret)\n\t\tgoto out;\n\tlist_for_each_entry_rcu(child, &parent->children, siblings) {\n\t\tparent = child;\n\t\tgoto down;\n\nup:\n\t\tcontinue;\n\t}\n\tret = (*up)(parent, data);\n\tif (ret || parent == from)\n\t\tgoto out;\n\n\tchild = parent;\n\tparent = parent->parent;\n\tif (parent)\n\t\tgoto up;\nout:\n\treturn ret;\n}\n\nint tg_nop(struct task_group *tg, void *data)\n{\n\treturn 0;\n}\n#endif\n\nstatic void set_load_weight(struct task_struct *p, bool update_load)\n{\n\tint prio = p->static_prio - MAX_RT_PRIO;\n\tstruct load_weight *load = &p->se.load;\n\n\t \n\tif (task_has_idle_policy(p)) {\n\t\tload->weight = scale_load(WEIGHT_IDLEPRIO);\n\t\tload->inv_weight = WMULT_IDLEPRIO;\n\t\treturn;\n\t}\n\n\t \n\tif (update_load && p->sched_class == &fair_sched_class) {\n\t\treweight_task(p, prio);\n\t} else {\n\t\tload->weight = scale_load(sched_prio_to_weight[prio]);\n\t\tload->inv_weight = sched_prio_to_wmult[prio];\n\t}\n}\n\n#ifdef CONFIG_UCLAMP_TASK\n \nstatic DEFINE_MUTEX(uclamp_mutex);\n\n \nstatic unsigned int __maybe_unused sysctl_sched_uclamp_util_min = SCHED_CAPACITY_SCALE;\n\n \nstatic unsigned int __maybe_unused sysctl_sched_uclamp_util_max = SCHED_CAPACITY_SCALE;\n\n \nstatic unsigned int sysctl_sched_uclamp_util_min_rt_default = SCHED_CAPACITY_SCALE;\n\n \nstatic struct uclamp_se uclamp_default[UCLAMP_CNT];\n\n \nDEFINE_STATIC_KEY_FALSE(sched_uclamp_used);\n\n \n#define UCLAMP_BUCKET_DELTA DIV_ROUND_CLOSEST(SCHED_CAPACITY_SCALE, UCLAMP_BUCKETS)\n\n#define for_each_clamp_id(clamp_id) \\\n\tfor ((clamp_id) = 0; (clamp_id) < UCLAMP_CNT; (clamp_id)++)\n\nstatic inline unsigned int uclamp_bucket_id(unsigned int clamp_value)\n{\n\treturn min_t(unsigned int, clamp_value / UCLAMP_BUCKET_DELTA, UCLAMP_BUCKETS - 1);\n}\n\nstatic inline unsigned int uclamp_none(enum uclamp_id clamp_id)\n{\n\tif (clamp_id == UCLAMP_MIN)\n\t\treturn 0;\n\treturn SCHED_CAPACITY_SCALE;\n}\n\nstatic inline void uclamp_se_set(struct uclamp_se *uc_se,\n\t\t\t\t unsigned int value, bool user_defined)\n{\n\tuc_se->value = value;\n\tuc_se->bucket_id = uclamp_bucket_id(value);\n\tuc_se->user_defined = user_defined;\n}\n\nstatic inline unsigned int\nuclamp_idle_value(struct rq *rq, enum uclamp_id clamp_id,\n\t\t  unsigned int clamp_value)\n{\n\t \n\tif (clamp_id == UCLAMP_MAX) {\n\t\trq->uclamp_flags |= UCLAMP_FLAG_IDLE;\n\t\treturn clamp_value;\n\t}\n\n\treturn uclamp_none(UCLAMP_MIN);\n}\n\nstatic inline void uclamp_idle_reset(struct rq *rq, enum uclamp_id clamp_id,\n\t\t\t\t     unsigned int clamp_value)\n{\n\t \n\tif (!(rq->uclamp_flags & UCLAMP_FLAG_IDLE))\n\t\treturn;\n\n\tuclamp_rq_set(rq, clamp_id, clamp_value);\n}\n\nstatic inline\nunsigned int uclamp_rq_max_value(struct rq *rq, enum uclamp_id clamp_id,\n\t\t\t\t   unsigned int clamp_value)\n{\n\tstruct uclamp_bucket *bucket = rq->uclamp[clamp_id].bucket;\n\tint bucket_id = UCLAMP_BUCKETS - 1;\n\n\t \n\tfor ( ; bucket_id >= 0; bucket_id--) {\n\t\tif (!bucket[bucket_id].tasks)\n\t\t\tcontinue;\n\t\treturn bucket[bucket_id].value;\n\t}\n\n\t \n\treturn uclamp_idle_value(rq, clamp_id, clamp_value);\n}\n\nstatic void __uclamp_update_util_min_rt_default(struct task_struct *p)\n{\n\tunsigned int default_util_min;\n\tstruct uclamp_se *uc_se;\n\n\tlockdep_assert_held(&p->pi_lock);\n\n\tuc_se = &p->uclamp_req[UCLAMP_MIN];\n\n\t \n\tif (uc_se->user_defined)\n\t\treturn;\n\n\tdefault_util_min = sysctl_sched_uclamp_util_min_rt_default;\n\tuclamp_se_set(uc_se, default_util_min, false);\n}\n\nstatic void uclamp_update_util_min_rt_default(struct task_struct *p)\n{\n\tstruct rq_flags rf;\n\tstruct rq *rq;\n\n\tif (!rt_task(p))\n\t\treturn;\n\n\t \n\trq = task_rq_lock(p, &rf);\n\t__uclamp_update_util_min_rt_default(p);\n\ttask_rq_unlock(rq, p, &rf);\n}\n\nstatic inline struct uclamp_se\nuclamp_tg_restrict(struct task_struct *p, enum uclamp_id clamp_id)\n{\n\t \n\tstruct uclamp_se uc_req = p->uclamp_req[clamp_id];\n#ifdef CONFIG_UCLAMP_TASK_GROUP\n\tunsigned int tg_min, tg_max, value;\n\n\t \n\tif (task_group_is_autogroup(task_group(p)))\n\t\treturn uc_req;\n\tif (task_group(p) == &root_task_group)\n\t\treturn uc_req;\n\n\ttg_min = task_group(p)->uclamp[UCLAMP_MIN].value;\n\ttg_max = task_group(p)->uclamp[UCLAMP_MAX].value;\n\tvalue = uc_req.value;\n\tvalue = clamp(value, tg_min, tg_max);\n\tuclamp_se_set(&uc_req, value, false);\n#endif\n\n\treturn uc_req;\n}\n\n \nstatic inline struct uclamp_se\nuclamp_eff_get(struct task_struct *p, enum uclamp_id clamp_id)\n{\n\tstruct uclamp_se uc_req = uclamp_tg_restrict(p, clamp_id);\n\tstruct uclamp_se uc_max = uclamp_default[clamp_id];\n\n\t \n\tif (unlikely(uc_req.value > uc_max.value))\n\t\treturn uc_max;\n\n\treturn uc_req;\n}\n\nunsigned long uclamp_eff_value(struct task_struct *p, enum uclamp_id clamp_id)\n{\n\tstruct uclamp_se uc_eff;\n\n\t \n\tif (p->uclamp[clamp_id].active)\n\t\treturn (unsigned long)p->uclamp[clamp_id].value;\n\n\tuc_eff = uclamp_eff_get(p, clamp_id);\n\n\treturn (unsigned long)uc_eff.value;\n}\n\n \nstatic inline void uclamp_rq_inc_id(struct rq *rq, struct task_struct *p,\n\t\t\t\t    enum uclamp_id clamp_id)\n{\n\tstruct uclamp_rq *uc_rq = &rq->uclamp[clamp_id];\n\tstruct uclamp_se *uc_se = &p->uclamp[clamp_id];\n\tstruct uclamp_bucket *bucket;\n\n\tlockdep_assert_rq_held(rq);\n\n\t \n\tp->uclamp[clamp_id] = uclamp_eff_get(p, clamp_id);\n\n\tbucket = &uc_rq->bucket[uc_se->bucket_id];\n\tbucket->tasks++;\n\tuc_se->active = true;\n\n\tuclamp_idle_reset(rq, clamp_id, uc_se->value);\n\n\t \n\tif (bucket->tasks == 1 || uc_se->value > bucket->value)\n\t\tbucket->value = uc_se->value;\n\n\tif (uc_se->value > uclamp_rq_get(rq, clamp_id))\n\t\tuclamp_rq_set(rq, clamp_id, uc_se->value);\n}\n\n \nstatic inline void uclamp_rq_dec_id(struct rq *rq, struct task_struct *p,\n\t\t\t\t    enum uclamp_id clamp_id)\n{\n\tstruct uclamp_rq *uc_rq = &rq->uclamp[clamp_id];\n\tstruct uclamp_se *uc_se = &p->uclamp[clamp_id];\n\tstruct uclamp_bucket *bucket;\n\tunsigned int bkt_clamp;\n\tunsigned int rq_clamp;\n\n\tlockdep_assert_rq_held(rq);\n\n\t \n\tif (unlikely(!uc_se->active))\n\t\treturn;\n\n\tbucket = &uc_rq->bucket[uc_se->bucket_id];\n\n\tSCHED_WARN_ON(!bucket->tasks);\n\tif (likely(bucket->tasks))\n\t\tbucket->tasks--;\n\n\tuc_se->active = false;\n\n\t \n\tif (likely(bucket->tasks))\n\t\treturn;\n\n\trq_clamp = uclamp_rq_get(rq, clamp_id);\n\t \n\tSCHED_WARN_ON(bucket->value > rq_clamp);\n\tif (bucket->value >= rq_clamp) {\n\t\tbkt_clamp = uclamp_rq_max_value(rq, clamp_id, uc_se->value);\n\t\tuclamp_rq_set(rq, clamp_id, bkt_clamp);\n\t}\n}\n\nstatic inline void uclamp_rq_inc(struct rq *rq, struct task_struct *p)\n{\n\tenum uclamp_id clamp_id;\n\n\t \n\tif (!static_branch_unlikely(&sched_uclamp_used))\n\t\treturn;\n\n\tif (unlikely(!p->sched_class->uclamp_enabled))\n\t\treturn;\n\n\tfor_each_clamp_id(clamp_id)\n\t\tuclamp_rq_inc_id(rq, p, clamp_id);\n\n\t \n\tif (rq->uclamp_flags & UCLAMP_FLAG_IDLE)\n\t\trq->uclamp_flags &= ~UCLAMP_FLAG_IDLE;\n}\n\nstatic inline void uclamp_rq_dec(struct rq *rq, struct task_struct *p)\n{\n\tenum uclamp_id clamp_id;\n\n\t \n\tif (!static_branch_unlikely(&sched_uclamp_used))\n\t\treturn;\n\n\tif (unlikely(!p->sched_class->uclamp_enabled))\n\t\treturn;\n\n\tfor_each_clamp_id(clamp_id)\n\t\tuclamp_rq_dec_id(rq, p, clamp_id);\n}\n\nstatic inline void uclamp_rq_reinc_id(struct rq *rq, struct task_struct *p,\n\t\t\t\t      enum uclamp_id clamp_id)\n{\n\tif (!p->uclamp[clamp_id].active)\n\t\treturn;\n\n\tuclamp_rq_dec_id(rq, p, clamp_id);\n\tuclamp_rq_inc_id(rq, p, clamp_id);\n\n\t \n\tif (clamp_id == UCLAMP_MAX && (rq->uclamp_flags & UCLAMP_FLAG_IDLE))\n\t\trq->uclamp_flags &= ~UCLAMP_FLAG_IDLE;\n}\n\nstatic inline void\nuclamp_update_active(struct task_struct *p)\n{\n\tenum uclamp_id clamp_id;\n\tstruct rq_flags rf;\n\tstruct rq *rq;\n\n\t \n\trq = task_rq_lock(p, &rf);\n\n\t \n\tfor_each_clamp_id(clamp_id)\n\t\tuclamp_rq_reinc_id(rq, p, clamp_id);\n\n\ttask_rq_unlock(rq, p, &rf);\n}\n\n#ifdef CONFIG_UCLAMP_TASK_GROUP\nstatic inline void\nuclamp_update_active_tasks(struct cgroup_subsys_state *css)\n{\n\tstruct css_task_iter it;\n\tstruct task_struct *p;\n\n\tcss_task_iter_start(css, 0, &it);\n\twhile ((p = css_task_iter_next(&it)))\n\t\tuclamp_update_active(p);\n\tcss_task_iter_end(&it);\n}\n\nstatic void cpu_util_update_eff(struct cgroup_subsys_state *css);\n#endif\n\n#ifdef CONFIG_SYSCTL\n#ifdef CONFIG_UCLAMP_TASK\n#ifdef CONFIG_UCLAMP_TASK_GROUP\nstatic void uclamp_update_root_tg(void)\n{\n\tstruct task_group *tg = &root_task_group;\n\n\tuclamp_se_set(&tg->uclamp_req[UCLAMP_MIN],\n\t\t      sysctl_sched_uclamp_util_min, false);\n\tuclamp_se_set(&tg->uclamp_req[UCLAMP_MAX],\n\t\t      sysctl_sched_uclamp_util_max, false);\n\n\trcu_read_lock();\n\tcpu_util_update_eff(&root_task_group.css);\n\trcu_read_unlock();\n}\n#else\nstatic void uclamp_update_root_tg(void) { }\n#endif\n\nstatic void uclamp_sync_util_min_rt_default(void)\n{\n\tstruct task_struct *g, *p;\n\n\t \n\tread_lock(&tasklist_lock);\n\tsmp_mb__after_spinlock();\n\tread_unlock(&tasklist_lock);\n\n\trcu_read_lock();\n\tfor_each_process_thread(g, p)\n\t\tuclamp_update_util_min_rt_default(p);\n\trcu_read_unlock();\n}\n\nstatic int sysctl_sched_uclamp_handler(struct ctl_table *table, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tbool update_root_tg = false;\n\tint old_min, old_max, old_min_rt;\n\tint result;\n\n\tguard(mutex)(&uclamp_mutex);\n\n\told_min = sysctl_sched_uclamp_util_min;\n\told_max = sysctl_sched_uclamp_util_max;\n\told_min_rt = sysctl_sched_uclamp_util_min_rt_default;\n\n\tresult = proc_dointvec(table, write, buffer, lenp, ppos);\n\tif (result)\n\t\tgoto undo;\n\tif (!write)\n\t\treturn 0;\n\n\tif (sysctl_sched_uclamp_util_min > sysctl_sched_uclamp_util_max ||\n\t    sysctl_sched_uclamp_util_max > SCHED_CAPACITY_SCALE\t||\n\t    sysctl_sched_uclamp_util_min_rt_default > SCHED_CAPACITY_SCALE) {\n\n\t\tresult = -EINVAL;\n\t\tgoto undo;\n\t}\n\n\tif (old_min != sysctl_sched_uclamp_util_min) {\n\t\tuclamp_se_set(&uclamp_default[UCLAMP_MIN],\n\t\t\t      sysctl_sched_uclamp_util_min, false);\n\t\tupdate_root_tg = true;\n\t}\n\tif (old_max != sysctl_sched_uclamp_util_max) {\n\t\tuclamp_se_set(&uclamp_default[UCLAMP_MAX],\n\t\t\t      sysctl_sched_uclamp_util_max, false);\n\t\tupdate_root_tg = true;\n\t}\n\n\tif (update_root_tg) {\n\t\tstatic_branch_enable(&sched_uclamp_used);\n\t\tuclamp_update_root_tg();\n\t}\n\n\tif (old_min_rt != sysctl_sched_uclamp_util_min_rt_default) {\n\t\tstatic_branch_enable(&sched_uclamp_used);\n\t\tuclamp_sync_util_min_rt_default();\n\t}\n\n\t \n\treturn 0;\n\nundo:\n\tsysctl_sched_uclamp_util_min = old_min;\n\tsysctl_sched_uclamp_util_max = old_max;\n\tsysctl_sched_uclamp_util_min_rt_default = old_min_rt;\n\treturn result;\n}\n#endif\n#endif\n\nstatic int uclamp_validate(struct task_struct *p,\n\t\t\t   const struct sched_attr *attr)\n{\n\tint util_min = p->uclamp_req[UCLAMP_MIN].value;\n\tint util_max = p->uclamp_req[UCLAMP_MAX].value;\n\n\tif (attr->sched_flags & SCHED_FLAG_UTIL_CLAMP_MIN) {\n\t\tutil_min = attr->sched_util_min;\n\n\t\tif (util_min + 1 > SCHED_CAPACITY_SCALE + 1)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (attr->sched_flags & SCHED_FLAG_UTIL_CLAMP_MAX) {\n\t\tutil_max = attr->sched_util_max;\n\n\t\tif (util_max + 1 > SCHED_CAPACITY_SCALE + 1)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (util_min != -1 && util_max != -1 && util_min > util_max)\n\t\treturn -EINVAL;\n\n\t \n\tstatic_branch_enable(&sched_uclamp_used);\n\n\treturn 0;\n}\n\nstatic bool uclamp_reset(const struct sched_attr *attr,\n\t\t\t enum uclamp_id clamp_id,\n\t\t\t struct uclamp_se *uc_se)\n{\n\t \n\tif (likely(!(attr->sched_flags & SCHED_FLAG_UTIL_CLAMP)) &&\n\t    !uc_se->user_defined)\n\t\treturn true;\n\n\t \n\tif (clamp_id == UCLAMP_MIN &&\n\t    attr->sched_flags & SCHED_FLAG_UTIL_CLAMP_MIN &&\n\t    attr->sched_util_min == -1) {\n\t\treturn true;\n\t}\n\n\tif (clamp_id == UCLAMP_MAX &&\n\t    attr->sched_flags & SCHED_FLAG_UTIL_CLAMP_MAX &&\n\t    attr->sched_util_max == -1) {\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic void __setscheduler_uclamp(struct task_struct *p,\n\t\t\t\t  const struct sched_attr *attr)\n{\n\tenum uclamp_id clamp_id;\n\n\tfor_each_clamp_id(clamp_id) {\n\t\tstruct uclamp_se *uc_se = &p->uclamp_req[clamp_id];\n\t\tunsigned int value;\n\n\t\tif (!uclamp_reset(attr, clamp_id, uc_se))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (unlikely(rt_task(p) && clamp_id == UCLAMP_MIN))\n\t\t\tvalue = sysctl_sched_uclamp_util_min_rt_default;\n\t\telse\n\t\t\tvalue = uclamp_none(clamp_id);\n\n\t\tuclamp_se_set(uc_se, value, false);\n\n\t}\n\n\tif (likely(!(attr->sched_flags & SCHED_FLAG_UTIL_CLAMP)))\n\t\treturn;\n\n\tif (attr->sched_flags & SCHED_FLAG_UTIL_CLAMP_MIN &&\n\t    attr->sched_util_min != -1) {\n\t\tuclamp_se_set(&p->uclamp_req[UCLAMP_MIN],\n\t\t\t      attr->sched_util_min, true);\n\t}\n\n\tif (attr->sched_flags & SCHED_FLAG_UTIL_CLAMP_MAX &&\n\t    attr->sched_util_max != -1) {\n\t\tuclamp_se_set(&p->uclamp_req[UCLAMP_MAX],\n\t\t\t      attr->sched_util_max, true);\n\t}\n}\n\nstatic void uclamp_fork(struct task_struct *p)\n{\n\tenum uclamp_id clamp_id;\n\n\t \n\tfor_each_clamp_id(clamp_id)\n\t\tp->uclamp[clamp_id].active = false;\n\n\tif (likely(!p->sched_reset_on_fork))\n\t\treturn;\n\n\tfor_each_clamp_id(clamp_id) {\n\t\tuclamp_se_set(&p->uclamp_req[clamp_id],\n\t\t\t      uclamp_none(clamp_id), false);\n\t}\n}\n\nstatic void uclamp_post_fork(struct task_struct *p)\n{\n\tuclamp_update_util_min_rt_default(p);\n}\n\nstatic void __init init_uclamp_rq(struct rq *rq)\n{\n\tenum uclamp_id clamp_id;\n\tstruct uclamp_rq *uc_rq = rq->uclamp;\n\n\tfor_each_clamp_id(clamp_id) {\n\t\tuc_rq[clamp_id] = (struct uclamp_rq) {\n\t\t\t.value = uclamp_none(clamp_id)\n\t\t};\n\t}\n\n\trq->uclamp_flags = UCLAMP_FLAG_IDLE;\n}\n\nstatic void __init init_uclamp(void)\n{\n\tstruct uclamp_se uc_max = {};\n\tenum uclamp_id clamp_id;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu)\n\t\tinit_uclamp_rq(cpu_rq(cpu));\n\n\tfor_each_clamp_id(clamp_id) {\n\t\tuclamp_se_set(&init_task.uclamp_req[clamp_id],\n\t\t\t      uclamp_none(clamp_id), false);\n\t}\n\n\t \n\tuclamp_se_set(&uc_max, uclamp_none(UCLAMP_MAX), false);\n\tfor_each_clamp_id(clamp_id) {\n\t\tuclamp_default[clamp_id] = uc_max;\n#ifdef CONFIG_UCLAMP_TASK_GROUP\n\t\troot_task_group.uclamp_req[clamp_id] = uc_max;\n\t\troot_task_group.uclamp[clamp_id] = uc_max;\n#endif\n\t}\n}\n\n#else  \nstatic inline void uclamp_rq_inc(struct rq *rq, struct task_struct *p) { }\nstatic inline void uclamp_rq_dec(struct rq *rq, struct task_struct *p) { }\nstatic inline int uclamp_validate(struct task_struct *p,\n\t\t\t\t  const struct sched_attr *attr)\n{\n\treturn -EOPNOTSUPP;\n}\nstatic void __setscheduler_uclamp(struct task_struct *p,\n\t\t\t\t  const struct sched_attr *attr) { }\nstatic inline void uclamp_fork(struct task_struct *p) { }\nstatic inline void uclamp_post_fork(struct task_struct *p) { }\nstatic inline void init_uclamp(void) { }\n#endif  \n\nbool sched_task_on_rq(struct task_struct *p)\n{\n\treturn task_on_rq_queued(p);\n}\n\nunsigned long get_wchan(struct task_struct *p)\n{\n\tunsigned long ip = 0;\n\tunsigned int state;\n\n\tif (!p || p == current)\n\t\treturn 0;\n\n\t \n\traw_spin_lock_irq(&p->pi_lock);\n\tstate = READ_ONCE(p->__state);\n\tsmp_rmb();  \n\tif (state != TASK_RUNNING && state != TASK_WAKING && !p->on_rq)\n\t\tip = __get_wchan(p);\n\traw_spin_unlock_irq(&p->pi_lock);\n\n\treturn ip;\n}\n\nstatic inline void enqueue_task(struct rq *rq, struct task_struct *p, int flags)\n{\n\tif (!(flags & ENQUEUE_NOCLOCK))\n\t\tupdate_rq_clock(rq);\n\n\tif (!(flags & ENQUEUE_RESTORE)) {\n\t\tsched_info_enqueue(rq, p);\n\t\tpsi_enqueue(p, (flags & ENQUEUE_WAKEUP) && !(flags & ENQUEUE_MIGRATED));\n\t}\n\n\tuclamp_rq_inc(rq, p);\n\tp->sched_class->enqueue_task(rq, p, flags);\n\n\tif (sched_core_enabled(rq))\n\t\tsched_core_enqueue(rq, p);\n}\n\nstatic inline void dequeue_task(struct rq *rq, struct task_struct *p, int flags)\n{\n\tif (sched_core_enabled(rq))\n\t\tsched_core_dequeue(rq, p, flags);\n\n\tif (!(flags & DEQUEUE_NOCLOCK))\n\t\tupdate_rq_clock(rq);\n\n\tif (!(flags & DEQUEUE_SAVE)) {\n\t\tsched_info_dequeue(rq, p);\n\t\tpsi_dequeue(p, flags & DEQUEUE_SLEEP);\n\t}\n\n\tuclamp_rq_dec(rq, p);\n\tp->sched_class->dequeue_task(rq, p, flags);\n}\n\nvoid activate_task(struct rq *rq, struct task_struct *p, int flags)\n{\n\tif (task_on_rq_migrating(p))\n\t\tflags |= ENQUEUE_MIGRATED;\n\tif (flags & ENQUEUE_MIGRATED)\n\t\tsched_mm_cid_migrate_to(rq, p);\n\n\tenqueue_task(rq, p, flags);\n\n\tp->on_rq = TASK_ON_RQ_QUEUED;\n}\n\nvoid deactivate_task(struct rq *rq, struct task_struct *p, int flags)\n{\n\tp->on_rq = (flags & DEQUEUE_SLEEP) ? 0 : TASK_ON_RQ_MIGRATING;\n\n\tdequeue_task(rq, p, flags);\n}\n\nstatic inline int __normal_prio(int policy, int rt_prio, int nice)\n{\n\tint prio;\n\n\tif (dl_policy(policy))\n\t\tprio = MAX_DL_PRIO - 1;\n\telse if (rt_policy(policy))\n\t\tprio = MAX_RT_PRIO - 1 - rt_prio;\n\telse\n\t\tprio = NICE_TO_PRIO(nice);\n\n\treturn prio;\n}\n\n \nstatic inline int normal_prio(struct task_struct *p)\n{\n\treturn __normal_prio(p->policy, p->rt_priority, PRIO_TO_NICE(p->static_prio));\n}\n\n \nstatic int effective_prio(struct task_struct *p)\n{\n\tp->normal_prio = normal_prio(p);\n\t \n\tif (!rt_prio(p->prio))\n\t\treturn p->normal_prio;\n\treturn p->prio;\n}\n\n \ninline int task_curr(const struct task_struct *p)\n{\n\treturn cpu_curr(task_cpu(p)) == p;\n}\n\n \nstatic inline void check_class_changed(struct rq *rq, struct task_struct *p,\n\t\t\t\t       const struct sched_class *prev_class,\n\t\t\t\t       int oldprio)\n{\n\tif (prev_class != p->sched_class) {\n\t\tif (prev_class->switched_from)\n\t\t\tprev_class->switched_from(rq, p);\n\n\t\tp->sched_class->switched_to(rq, p);\n\t} else if (oldprio != p->prio || dl_task(p))\n\t\tp->sched_class->prio_changed(rq, p, oldprio);\n}\n\nvoid check_preempt_curr(struct rq *rq, struct task_struct *p, int flags)\n{\n\tif (p->sched_class == rq->curr->sched_class)\n\t\trq->curr->sched_class->check_preempt_curr(rq, p, flags);\n\telse if (sched_class_above(p->sched_class, rq->curr->sched_class))\n\t\tresched_curr(rq);\n\n\t \n\tif (task_on_rq_queued(rq->curr) && test_tsk_need_resched(rq->curr))\n\t\trq_clock_skip_update(rq);\n}\n\nstatic __always_inline\nint __task_state_match(struct task_struct *p, unsigned int state)\n{\n\tif (READ_ONCE(p->__state) & state)\n\t\treturn 1;\n\n#ifdef CONFIG_PREEMPT_RT\n\tif (READ_ONCE(p->saved_state) & state)\n\t\treturn -1;\n#endif\n\treturn 0;\n}\n\nstatic __always_inline\nint task_state_match(struct task_struct *p, unsigned int state)\n{\n#ifdef CONFIG_PREEMPT_RT\n\tint match;\n\n\t \n\traw_spin_lock_irq(&p->pi_lock);\n\tmatch = __task_state_match(p, state);\n\traw_spin_unlock_irq(&p->pi_lock);\n\n\treturn match;\n#else\n\treturn __task_state_match(p, state);\n#endif\n}\n\n \nunsigned long wait_task_inactive(struct task_struct *p, unsigned int match_state)\n{\n\tint running, queued, match;\n\tstruct rq_flags rf;\n\tunsigned long ncsw;\n\tstruct rq *rq;\n\n\tfor (;;) {\n\t\t \n\t\trq = task_rq(p);\n\n\t\t \n\t\twhile (task_on_cpu(rq, p)) {\n\t\t\tif (!task_state_match(p, match_state))\n\t\t\t\treturn 0;\n\t\t\tcpu_relax();\n\t\t}\n\n\t\t \n\t\trq = task_rq_lock(p, &rf);\n\t\ttrace_sched_wait_task(p);\n\t\trunning = task_on_cpu(rq, p);\n\t\tqueued = task_on_rq_queued(p);\n\t\tncsw = 0;\n\t\tif ((match = __task_state_match(p, match_state))) {\n\t\t\t \n\t\t\tif (match < 0)\n\t\t\t\tqueued = 1;\n\t\t\tncsw = p->nvcsw | LONG_MIN;  \n\t\t}\n\t\ttask_rq_unlock(rq, p, &rf);\n\n\t\t \n\t\tif (unlikely(!ncsw))\n\t\t\tbreak;\n\n\t\t \n\t\tif (unlikely(running)) {\n\t\t\tcpu_relax();\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (unlikely(queued)) {\n\t\t\tktime_t to = NSEC_PER_SEC / HZ;\n\n\t\t\tset_current_state(TASK_UNINTERRUPTIBLE);\n\t\t\tschedule_hrtimeout(&to, HRTIMER_MODE_REL_HARD);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tbreak;\n\t}\n\n\treturn ncsw;\n}\n\n#ifdef CONFIG_SMP\n\nstatic void\n__do_set_cpus_allowed(struct task_struct *p, struct affinity_context *ctx);\n\nstatic int __set_cpus_allowed_ptr(struct task_struct *p,\n\t\t\t\t  struct affinity_context *ctx);\n\nstatic void migrate_disable_switch(struct rq *rq, struct task_struct *p)\n{\n\tstruct affinity_context ac = {\n\t\t.new_mask  = cpumask_of(rq->cpu),\n\t\t.flags     = SCA_MIGRATE_DISABLE,\n\t};\n\n\tif (likely(!p->migration_disabled))\n\t\treturn;\n\n\tif (p->cpus_ptr != &p->cpus_mask)\n\t\treturn;\n\n\t \n\t__do_set_cpus_allowed(p, &ac);\n}\n\nvoid migrate_disable(void)\n{\n\tstruct task_struct *p = current;\n\n\tif (p->migration_disabled) {\n\t\tp->migration_disabled++;\n\t\treturn;\n\t}\n\n\tpreempt_disable();\n\tthis_rq()->nr_pinned++;\n\tp->migration_disabled = 1;\n\tpreempt_enable();\n}\nEXPORT_SYMBOL_GPL(migrate_disable);\n\nvoid migrate_enable(void)\n{\n\tstruct task_struct *p = current;\n\tstruct affinity_context ac = {\n\t\t.new_mask  = &p->cpus_mask,\n\t\t.flags     = SCA_MIGRATE_ENABLE,\n\t};\n\n\tif (p->migration_disabled > 1) {\n\t\tp->migration_disabled--;\n\t\treturn;\n\t}\n\n\tif (WARN_ON_ONCE(!p->migration_disabled))\n\t\treturn;\n\n\t \n\tpreempt_disable();\n\tif (p->cpus_ptr != &p->cpus_mask)\n\t\t__set_cpus_allowed_ptr(p, &ac);\n\t \n\tbarrier();\n\tp->migration_disabled = 0;\n\tthis_rq()->nr_pinned--;\n\tpreempt_enable();\n}\nEXPORT_SYMBOL_GPL(migrate_enable);\n\nstatic inline bool rq_has_pinned_tasks(struct rq *rq)\n{\n\treturn rq->nr_pinned;\n}\n\n \nstatic inline bool is_cpu_allowed(struct task_struct *p, int cpu)\n{\n\t \n\tif (!cpumask_test_cpu(cpu, p->cpus_ptr))\n\t\treturn false;\n\n\t \n\tif (is_migration_disabled(p))\n\t\treturn cpu_online(cpu);\n\n\t \n\tif (!(p->flags & PF_KTHREAD))\n\t\treturn cpu_active(cpu) && task_cpu_possible(cpu, p);\n\n\t \n\tif (kthread_is_per_cpu(p))\n\t\treturn cpu_online(cpu);\n\n\t \n\tif (cpu_dying(cpu))\n\t\treturn false;\n\n\t \n\treturn cpu_online(cpu);\n}\n\n \n\n \nstatic struct rq *move_queued_task(struct rq *rq, struct rq_flags *rf,\n\t\t\t\t   struct task_struct *p, int new_cpu)\n{\n\tlockdep_assert_rq_held(rq);\n\n\tdeactivate_task(rq, p, DEQUEUE_NOCLOCK);\n\tset_task_cpu(p, new_cpu);\n\trq_unlock(rq, rf);\n\n\trq = cpu_rq(new_cpu);\n\n\trq_lock(rq, rf);\n\tWARN_ON_ONCE(task_cpu(p) != new_cpu);\n\tactivate_task(rq, p, 0);\n\tcheck_preempt_curr(rq, p, 0);\n\n\treturn rq;\n}\n\nstruct migration_arg {\n\tstruct task_struct\t\t*task;\n\tint\t\t\t\tdest_cpu;\n\tstruct set_affinity_pending\t*pending;\n};\n\n \nstruct set_affinity_pending {\n\trefcount_t\t\trefs;\n\tunsigned int\t\tstop_pending;\n\tstruct completion\tdone;\n\tstruct cpu_stop_work\tstop_work;\n\tstruct migration_arg\targ;\n};\n\n \nstatic struct rq *__migrate_task(struct rq *rq, struct rq_flags *rf,\n\t\t\t\t struct task_struct *p, int dest_cpu)\n{\n\t \n\tif (!is_cpu_allowed(p, dest_cpu))\n\t\treturn rq;\n\n\trq = move_queued_task(rq, rf, p, dest_cpu);\n\n\treturn rq;\n}\n\n \nstatic int migration_cpu_stop(void *data)\n{\n\tstruct migration_arg *arg = data;\n\tstruct set_affinity_pending *pending = arg->pending;\n\tstruct task_struct *p = arg->task;\n\tstruct rq *rq = this_rq();\n\tbool complete = false;\n\tstruct rq_flags rf;\n\n\t \n\tlocal_irq_save(rf.flags);\n\t \n\tflush_smp_call_function_queue();\n\n\traw_spin_lock(&p->pi_lock);\n\trq_lock(rq, &rf);\n\n\t \n\tWARN_ON_ONCE(pending && pending != p->migration_pending);\n\n\t \n\tif (task_rq(p) == rq) {\n\t\tif (is_migration_disabled(p))\n\t\t\tgoto out;\n\n\t\tif (pending) {\n\t\t\tp->migration_pending = NULL;\n\t\t\tcomplete = true;\n\n\t\t\tif (cpumask_test_cpu(task_cpu(p), &p->cpus_mask))\n\t\t\t\tgoto out;\n\t\t}\n\n\t\tif (task_on_rq_queued(p)) {\n\t\t\tupdate_rq_clock(rq);\n\t\t\trq = __migrate_task(rq, &rf, p, arg->dest_cpu);\n\t\t} else {\n\t\t\tp->wake_cpu = arg->dest_cpu;\n\t\t}\n\n\t\t \n\n\t} else if (pending) {\n\t\t \n\n\t\t \n\t\tif (cpumask_test_cpu(task_cpu(p), p->cpus_ptr)) {\n\t\t\tp->migration_pending = NULL;\n\t\t\tcomplete = true;\n\t\t\tgoto out;\n\t\t}\n\n\t\t \n\t\tWARN_ON_ONCE(!pending->stop_pending);\n\t\tpreempt_disable();\n\t\ttask_rq_unlock(rq, p, &rf);\n\t\tstop_one_cpu_nowait(task_cpu(p), migration_cpu_stop,\n\t\t\t\t    &pending->arg, &pending->stop_work);\n\t\tpreempt_enable();\n\t\treturn 0;\n\t}\nout:\n\tif (pending)\n\t\tpending->stop_pending = false;\n\ttask_rq_unlock(rq, p, &rf);\n\n\tif (complete)\n\t\tcomplete_all(&pending->done);\n\n\treturn 0;\n}\n\nint push_cpu_stop(void *arg)\n{\n\tstruct rq *lowest_rq = NULL, *rq = this_rq();\n\tstruct task_struct *p = arg;\n\n\traw_spin_lock_irq(&p->pi_lock);\n\traw_spin_rq_lock(rq);\n\n\tif (task_rq(p) != rq)\n\t\tgoto out_unlock;\n\n\tif (is_migration_disabled(p)) {\n\t\tp->migration_flags |= MDF_PUSH;\n\t\tgoto out_unlock;\n\t}\n\n\tp->migration_flags &= ~MDF_PUSH;\n\n\tif (p->sched_class->find_lock_rq)\n\t\tlowest_rq = p->sched_class->find_lock_rq(p, rq);\n\n\tif (!lowest_rq)\n\t\tgoto out_unlock;\n\n\t\n\tif (task_rq(p) == rq) {\n\t\tdeactivate_task(rq, p, 0);\n\t\tset_task_cpu(p, lowest_rq->cpu);\n\t\tactivate_task(lowest_rq, p, 0);\n\t\tresched_curr(lowest_rq);\n\t}\n\n\tdouble_unlock_balance(rq, lowest_rq);\n\nout_unlock:\n\trq->push_busy = false;\n\traw_spin_rq_unlock(rq);\n\traw_spin_unlock_irq(&p->pi_lock);\n\n\tput_task_struct(p);\n\treturn 0;\n}\n\n \nvoid set_cpus_allowed_common(struct task_struct *p, struct affinity_context *ctx)\n{\n\tif (ctx->flags & (SCA_MIGRATE_ENABLE | SCA_MIGRATE_DISABLE)) {\n\t\tp->cpus_ptr = ctx->new_mask;\n\t\treturn;\n\t}\n\n\tcpumask_copy(&p->cpus_mask, ctx->new_mask);\n\tp->nr_cpus_allowed = cpumask_weight(ctx->new_mask);\n\n\t \n\tif (ctx->flags & SCA_USER)\n\t\tswap(p->user_cpus_ptr, ctx->user_mask);\n}\n\nstatic void\n__do_set_cpus_allowed(struct task_struct *p, struct affinity_context *ctx)\n{\n\tstruct rq *rq = task_rq(p);\n\tbool queued, running;\n\n\t \n\tif (ctx->flags & SCA_MIGRATE_DISABLE)\n\t\tSCHED_WARN_ON(!p->on_cpu);\n\telse\n\t\tlockdep_assert_held(&p->pi_lock);\n\n\tqueued = task_on_rq_queued(p);\n\trunning = task_current(rq, p);\n\n\tif (queued) {\n\t\t \n\t\tlockdep_assert_rq_held(rq);\n\t\tdequeue_task(rq, p, DEQUEUE_SAVE | DEQUEUE_NOCLOCK);\n\t}\n\tif (running)\n\t\tput_prev_task(rq, p);\n\n\tp->sched_class->set_cpus_allowed(p, ctx);\n\n\tif (queued)\n\t\tenqueue_task(rq, p, ENQUEUE_RESTORE | ENQUEUE_NOCLOCK);\n\tif (running)\n\t\tset_next_task(rq, p);\n}\n\n \nvoid do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask)\n{\n\tstruct affinity_context ac = {\n\t\t.new_mask  = new_mask,\n\t\t.user_mask = NULL,\n\t\t.flags     = SCA_USER,\t \n\t};\n\tunion cpumask_rcuhead {\n\t\tcpumask_t cpumask;\n\t\tstruct rcu_head rcu;\n\t};\n\n\t__do_set_cpus_allowed(p, &ac);\n\n\t \n\tkfree_rcu((union cpumask_rcuhead *)ac.user_mask, rcu);\n}\n\nstatic cpumask_t *alloc_user_cpus_ptr(int node)\n{\n\t \n\tint size = max_t(int, cpumask_size(), sizeof(struct rcu_head));\n\n\treturn kmalloc_node(size, GFP_KERNEL, node);\n}\n\nint dup_user_cpus_ptr(struct task_struct *dst, struct task_struct *src,\n\t\t      int node)\n{\n\tcpumask_t *user_mask;\n\tunsigned long flags;\n\n\t \n\tdst->user_cpus_ptr = NULL;\n\n\t \n\tif (data_race(!src->user_cpus_ptr))\n\t\treturn 0;\n\n\tuser_mask = alloc_user_cpus_ptr(node);\n\tif (!user_mask)\n\t\treturn -ENOMEM;\n\n\t \n\traw_spin_lock_irqsave(&src->pi_lock, flags);\n\tif (src->user_cpus_ptr) {\n\t\tswap(dst->user_cpus_ptr, user_mask);\n\t\tcpumask_copy(dst->user_cpus_ptr, src->user_cpus_ptr);\n\t}\n\traw_spin_unlock_irqrestore(&src->pi_lock, flags);\n\n\tif (unlikely(user_mask))\n\t\tkfree(user_mask);\n\n\treturn 0;\n}\n\nstatic inline struct cpumask *clear_user_cpus_ptr(struct task_struct *p)\n{\n\tstruct cpumask *user_mask = NULL;\n\n\tswap(p->user_cpus_ptr, user_mask);\n\n\treturn user_mask;\n}\n\nvoid release_user_cpus_ptr(struct task_struct *p)\n{\n\tkfree(clear_user_cpus_ptr(p));\n}\n\n \nstatic int affine_move_task(struct rq *rq, struct task_struct *p, struct rq_flags *rf,\n\t\t\t    int dest_cpu, unsigned int flags)\n\t__releases(rq->lock)\n\t__releases(p->pi_lock)\n{\n\tstruct set_affinity_pending my_pending = { }, *pending = NULL;\n\tbool stop_pending, complete = false;\n\n\t \n\tif (cpumask_test_cpu(task_cpu(p), &p->cpus_mask)) {\n\t\tstruct task_struct *push_task = NULL;\n\n\t\tif ((flags & SCA_MIGRATE_ENABLE) &&\n\t\t    (p->migration_flags & MDF_PUSH) && !rq->push_busy) {\n\t\t\trq->push_busy = true;\n\t\t\tpush_task = get_task_struct(p);\n\t\t}\n\n\t\t \n\t\tpending = p->migration_pending;\n\t\tif (pending && !pending->stop_pending) {\n\t\t\tp->migration_pending = NULL;\n\t\t\tcomplete = true;\n\t\t}\n\n\t\tpreempt_disable();\n\t\ttask_rq_unlock(rq, p, rf);\n\t\tif (push_task) {\n\t\t\tstop_one_cpu_nowait(rq->cpu, push_cpu_stop,\n\t\t\t\t\t    p, &rq->push_work);\n\t\t}\n\t\tpreempt_enable();\n\n\t\tif (complete)\n\t\t\tcomplete_all(&pending->done);\n\n\t\treturn 0;\n\t}\n\n\tif (!(flags & SCA_MIGRATE_ENABLE)) {\n\t\t \n\t\tif (!p->migration_pending) {\n\t\t\t \n\t\t\trefcount_set(&my_pending.refs, 1);\n\t\t\tinit_completion(&my_pending.done);\n\t\t\tmy_pending.arg = (struct migration_arg) {\n\t\t\t\t.task = p,\n\t\t\t\t.dest_cpu = dest_cpu,\n\t\t\t\t.pending = &my_pending,\n\t\t\t};\n\n\t\t\tp->migration_pending = &my_pending;\n\t\t} else {\n\t\t\tpending = p->migration_pending;\n\t\t\trefcount_inc(&pending->refs);\n\t\t\t \n\t\t\tpending->arg.dest_cpu = dest_cpu;\n\t\t}\n\t}\n\tpending = p->migration_pending;\n\t \n\tif (WARN_ON_ONCE(!pending)) {\n\t\ttask_rq_unlock(rq, p, rf);\n\t\treturn -EINVAL;\n\t}\n\n\tif (task_on_cpu(rq, p) || READ_ONCE(p->__state) == TASK_WAKING) {\n\t\t \n\t\tstop_pending = pending->stop_pending;\n\t\tif (!stop_pending)\n\t\t\tpending->stop_pending = true;\n\n\t\tif (flags & SCA_MIGRATE_ENABLE)\n\t\t\tp->migration_flags &= ~MDF_PUSH;\n\n\t\tpreempt_disable();\n\t\ttask_rq_unlock(rq, p, rf);\n\t\tif (!stop_pending) {\n\t\t\tstop_one_cpu_nowait(cpu_of(rq), migration_cpu_stop,\n\t\t\t\t\t    &pending->arg, &pending->stop_work);\n\t\t}\n\t\tpreempt_enable();\n\n\t\tif (flags & SCA_MIGRATE_ENABLE)\n\t\t\treturn 0;\n\t} else {\n\n\t\tif (!is_migration_disabled(p)) {\n\t\t\tif (task_on_rq_queued(p))\n\t\t\t\trq = move_queued_task(rq, rf, p, dest_cpu);\n\n\t\t\tif (!pending->stop_pending) {\n\t\t\t\tp->migration_pending = NULL;\n\t\t\t\tcomplete = true;\n\t\t\t}\n\t\t}\n\t\ttask_rq_unlock(rq, p, rf);\n\n\t\tif (complete)\n\t\t\tcomplete_all(&pending->done);\n\t}\n\n\twait_for_completion(&pending->done);\n\n\tif (refcount_dec_and_test(&pending->refs))\n\t\twake_up_var(&pending->refs);  \n\n\t \n\twait_var_event(&my_pending.refs, !refcount_read(&my_pending.refs));\n\n\t \n\tWARN_ON_ONCE(my_pending.stop_pending);\n\n\treturn 0;\n}\n\n \nstatic int __set_cpus_allowed_ptr_locked(struct task_struct *p,\n\t\t\t\t\t struct affinity_context *ctx,\n\t\t\t\t\t struct rq *rq,\n\t\t\t\t\t struct rq_flags *rf)\n\t__releases(rq->lock)\n\t__releases(p->pi_lock)\n{\n\tconst struct cpumask *cpu_allowed_mask = task_cpu_possible_mask(p);\n\tconst struct cpumask *cpu_valid_mask = cpu_active_mask;\n\tbool kthread = p->flags & PF_KTHREAD;\n\tunsigned int dest_cpu;\n\tint ret = 0;\n\n\tupdate_rq_clock(rq);\n\n\tif (kthread || is_migration_disabled(p)) {\n\t\t \n\t\tcpu_valid_mask = cpu_online_mask;\n\t}\n\n\tif (!kthread && !cpumask_subset(ctx->new_mask, cpu_allowed_mask)) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t \n\tif ((ctx->flags & SCA_CHECK) && (p->flags & PF_NO_SETAFFINITY)) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (!(ctx->flags & SCA_MIGRATE_ENABLE)) {\n\t\tif (cpumask_equal(&p->cpus_mask, ctx->new_mask)) {\n\t\t\tif (ctx->flags & SCA_USER)\n\t\t\t\tswap(p->user_cpus_ptr, ctx->user_mask);\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (WARN_ON_ONCE(p == current &&\n\t\t\t\t is_migration_disabled(p) &&\n\t\t\t\t !cpumask_test_cpu(task_cpu(p), ctx->new_mask))) {\n\t\t\tret = -EBUSY;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t \n\tdest_cpu = cpumask_any_and_distribute(cpu_valid_mask, ctx->new_mask);\n\tif (dest_cpu >= nr_cpu_ids) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t__do_set_cpus_allowed(p, ctx);\n\n\treturn affine_move_task(rq, p, rf, dest_cpu, ctx->flags);\n\nout:\n\ttask_rq_unlock(rq, p, rf);\n\n\treturn ret;\n}\n\n \nstatic int __set_cpus_allowed_ptr(struct task_struct *p,\n\t\t\t\t  struct affinity_context *ctx)\n{\n\tstruct rq_flags rf;\n\tstruct rq *rq;\n\n\trq = task_rq_lock(p, &rf);\n\t \n\tif (p->user_cpus_ptr &&\n\t    !(ctx->flags & (SCA_USER | SCA_MIGRATE_ENABLE | SCA_MIGRATE_DISABLE)) &&\n\t    cpumask_and(rq->scratch_mask, ctx->new_mask, p->user_cpus_ptr))\n\t\tctx->new_mask = rq->scratch_mask;\n\n\treturn __set_cpus_allowed_ptr_locked(p, ctx, rq, &rf);\n}\n\nint set_cpus_allowed_ptr(struct task_struct *p, const struct cpumask *new_mask)\n{\n\tstruct affinity_context ac = {\n\t\t.new_mask  = new_mask,\n\t\t.flags     = 0,\n\t};\n\n\treturn __set_cpus_allowed_ptr(p, &ac);\n}\nEXPORT_SYMBOL_GPL(set_cpus_allowed_ptr);\n\n \nstatic int restrict_cpus_allowed_ptr(struct task_struct *p,\n\t\t\t\t     struct cpumask *new_mask,\n\t\t\t\t     const struct cpumask *subset_mask)\n{\n\tstruct affinity_context ac = {\n\t\t.new_mask  = new_mask,\n\t\t.flags     = 0,\n\t};\n\tstruct rq_flags rf;\n\tstruct rq *rq;\n\tint err;\n\n\trq = task_rq_lock(p, &rf);\n\n\t \n\tif (task_has_dl_policy(p) && dl_bandwidth_enabled()) {\n\t\terr = -EPERM;\n\t\tgoto err_unlock;\n\t}\n\n\tif (!cpumask_and(new_mask, task_user_cpus(p), subset_mask)) {\n\t\terr = -EINVAL;\n\t\tgoto err_unlock;\n\t}\n\n\treturn __set_cpus_allowed_ptr_locked(p, &ac, rq, &rf);\n\nerr_unlock:\n\ttask_rq_unlock(rq, p, &rf);\n\treturn err;\n}\n\n \nvoid force_compatible_cpus_allowed_ptr(struct task_struct *p)\n{\n\tcpumask_var_t new_mask;\n\tconst struct cpumask *override_mask = task_cpu_possible_mask(p);\n\n\talloc_cpumask_var(&new_mask, GFP_KERNEL);\n\n\t \n\tcpus_read_lock();\n\tif (!cpumask_available(new_mask))\n\t\tgoto out_set_mask;\n\n\tif (!restrict_cpus_allowed_ptr(p, new_mask, override_mask))\n\t\tgoto out_free_mask;\n\n\t \n\tcpuset_cpus_allowed(p, new_mask);\n\toverride_mask = new_mask;\n\nout_set_mask:\n\tif (printk_ratelimit()) {\n\t\tprintk_deferred(\"Overriding affinity for process %d (%s) to CPUs %*pbl\\n\",\n\t\t\t\ttask_pid_nr(p), p->comm,\n\t\t\t\tcpumask_pr_args(override_mask));\n\t}\n\n\tWARN_ON(set_cpus_allowed_ptr(p, override_mask));\nout_free_mask:\n\tcpus_read_unlock();\n\tfree_cpumask_var(new_mask);\n}\n\nstatic int\n__sched_setaffinity(struct task_struct *p, struct affinity_context *ctx);\n\n \nvoid relax_compatible_cpus_allowed_ptr(struct task_struct *p)\n{\n\tstruct affinity_context ac = {\n\t\t.new_mask  = task_user_cpus(p),\n\t\t.flags     = 0,\n\t};\n\tint ret;\n\n\t \n\tret = __sched_setaffinity(p, &ac);\n\tWARN_ON_ONCE(ret);\n}\n\nvoid set_task_cpu(struct task_struct *p, unsigned int new_cpu)\n{\n#ifdef CONFIG_SCHED_DEBUG\n\tunsigned int state = READ_ONCE(p->__state);\n\n\t \n\tWARN_ON_ONCE(state != TASK_RUNNING && state != TASK_WAKING && !p->on_rq);\n\n\t \n\tWARN_ON_ONCE(state == TASK_RUNNING &&\n\t\t     p->sched_class == &fair_sched_class &&\n\t\t     (p->on_rq && !task_on_rq_migrating(p)));\n\n#ifdef CONFIG_LOCKDEP\n\t \n\tWARN_ON_ONCE(debug_locks && !(lockdep_is_held(&p->pi_lock) ||\n\t\t\t\t      lockdep_is_held(__rq_lockp(task_rq(p)))));\n#endif\n\t \n\tWARN_ON_ONCE(!cpu_online(new_cpu));\n\n\tWARN_ON_ONCE(is_migration_disabled(p));\n#endif\n\n\ttrace_sched_migrate_task(p, new_cpu);\n\n\tif (task_cpu(p) != new_cpu) {\n\t\tif (p->sched_class->migrate_task_rq)\n\t\t\tp->sched_class->migrate_task_rq(p, new_cpu);\n\t\tp->se.nr_migrations++;\n\t\trseq_migrate(p);\n\t\tsched_mm_cid_migrate_from(p);\n\t\tperf_event_task_migrate(p);\n\t}\n\n\t__set_task_cpu(p, new_cpu);\n}\n\n#ifdef CONFIG_NUMA_BALANCING\nstatic void __migrate_swap_task(struct task_struct *p, int cpu)\n{\n\tif (task_on_rq_queued(p)) {\n\t\tstruct rq *src_rq, *dst_rq;\n\t\tstruct rq_flags srf, drf;\n\n\t\tsrc_rq = task_rq(p);\n\t\tdst_rq = cpu_rq(cpu);\n\n\t\trq_pin_lock(src_rq, &srf);\n\t\trq_pin_lock(dst_rq, &drf);\n\n\t\tdeactivate_task(src_rq, p, 0);\n\t\tset_task_cpu(p, cpu);\n\t\tactivate_task(dst_rq, p, 0);\n\t\tcheck_preempt_curr(dst_rq, p, 0);\n\n\t\trq_unpin_lock(dst_rq, &drf);\n\t\trq_unpin_lock(src_rq, &srf);\n\n\t} else {\n\t\t \n\t\tp->wake_cpu = cpu;\n\t}\n}\n\nstruct migration_swap_arg {\n\tstruct task_struct *src_task, *dst_task;\n\tint src_cpu, dst_cpu;\n};\n\nstatic int migrate_swap_stop(void *data)\n{\n\tstruct migration_swap_arg *arg = data;\n\tstruct rq *src_rq, *dst_rq;\n\n\tif (!cpu_active(arg->src_cpu) || !cpu_active(arg->dst_cpu))\n\t\treturn -EAGAIN;\n\n\tsrc_rq = cpu_rq(arg->src_cpu);\n\tdst_rq = cpu_rq(arg->dst_cpu);\n\n\tguard(double_raw_spinlock)(&arg->src_task->pi_lock, &arg->dst_task->pi_lock);\n\tguard(double_rq_lock)(src_rq, dst_rq);\n\n\tif (task_cpu(arg->dst_task) != arg->dst_cpu)\n\t\treturn -EAGAIN;\n\n\tif (task_cpu(arg->src_task) != arg->src_cpu)\n\t\treturn -EAGAIN;\n\n\tif (!cpumask_test_cpu(arg->dst_cpu, arg->src_task->cpus_ptr))\n\t\treturn -EAGAIN;\n\n\tif (!cpumask_test_cpu(arg->src_cpu, arg->dst_task->cpus_ptr))\n\t\treturn -EAGAIN;\n\n\t__migrate_swap_task(arg->src_task, arg->dst_cpu);\n\t__migrate_swap_task(arg->dst_task, arg->src_cpu);\n\n\treturn 0;\n}\n\n \nint migrate_swap(struct task_struct *cur, struct task_struct *p,\n\t\tint target_cpu, int curr_cpu)\n{\n\tstruct migration_swap_arg arg;\n\tint ret = -EINVAL;\n\n\targ = (struct migration_swap_arg){\n\t\t.src_task = cur,\n\t\t.src_cpu = curr_cpu,\n\t\t.dst_task = p,\n\t\t.dst_cpu = target_cpu,\n\t};\n\n\tif (arg.src_cpu == arg.dst_cpu)\n\t\tgoto out;\n\n\t \n\tif (!cpu_active(arg.src_cpu) || !cpu_active(arg.dst_cpu))\n\t\tgoto out;\n\n\tif (!cpumask_test_cpu(arg.dst_cpu, arg.src_task->cpus_ptr))\n\t\tgoto out;\n\n\tif (!cpumask_test_cpu(arg.src_cpu, arg.dst_task->cpus_ptr))\n\t\tgoto out;\n\n\ttrace_sched_swap_numa(cur, arg.src_cpu, p, arg.dst_cpu);\n\tret = stop_two_cpus(arg.dst_cpu, arg.src_cpu, migrate_swap_stop, &arg);\n\nout:\n\treturn ret;\n}\n#endif  \n\n \nvoid kick_process(struct task_struct *p)\n{\n\tint cpu;\n\n\tpreempt_disable();\n\tcpu = task_cpu(p);\n\tif ((cpu != smp_processor_id()) && task_curr(p))\n\t\tsmp_send_reschedule(cpu);\n\tpreempt_enable();\n}\nEXPORT_SYMBOL_GPL(kick_process);\n\n \nstatic int select_fallback_rq(int cpu, struct task_struct *p)\n{\n\tint nid = cpu_to_node(cpu);\n\tconst struct cpumask *nodemask = NULL;\n\tenum { cpuset, possible, fail } state = cpuset;\n\tint dest_cpu;\n\n\t \n\tif (nid != -1) {\n\t\tnodemask = cpumask_of_node(nid);\n\n\t\t \n\t\tfor_each_cpu(dest_cpu, nodemask) {\n\t\t\tif (is_cpu_allowed(p, dest_cpu))\n\t\t\t\treturn dest_cpu;\n\t\t}\n\t}\n\n\tfor (;;) {\n\t\t \n\t\tfor_each_cpu(dest_cpu, p->cpus_ptr) {\n\t\t\tif (!is_cpu_allowed(p, dest_cpu))\n\t\t\t\tcontinue;\n\n\t\t\tgoto out;\n\t\t}\n\n\t\t \n\t\tswitch (state) {\n\t\tcase cpuset:\n\t\t\tif (cpuset_cpus_allowed_fallback(p)) {\n\t\t\t\tstate = possible;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tfallthrough;\n\t\tcase possible:\n\t\t\t \n\t\t\tdo_set_cpus_allowed(p, task_cpu_possible_mask(p));\n\t\t\tstate = fail;\n\t\t\tbreak;\n\t\tcase fail:\n\t\t\tBUG();\n\t\t\tbreak;\n\t\t}\n\t}\n\nout:\n\tif (state != cpuset) {\n\t\t \n\t\tif (p->mm && printk_ratelimit()) {\n\t\t\tprintk_deferred(\"process %d (%s) no longer affine to cpu%d\\n\",\n\t\t\t\t\ttask_pid_nr(p), p->comm, cpu);\n\t\t}\n\t}\n\n\treturn dest_cpu;\n}\n\n \nstatic inline\nint select_task_rq(struct task_struct *p, int cpu, int wake_flags)\n{\n\tlockdep_assert_held(&p->pi_lock);\n\n\tif (p->nr_cpus_allowed > 1 && !is_migration_disabled(p))\n\t\tcpu = p->sched_class->select_task_rq(p, cpu, wake_flags);\n\telse\n\t\tcpu = cpumask_any(p->cpus_ptr);\n\n\t \n\tif (unlikely(!is_cpu_allowed(p, cpu)))\n\t\tcpu = select_fallback_rq(task_cpu(p), p);\n\n\treturn cpu;\n}\n\nvoid sched_set_stop_task(int cpu, struct task_struct *stop)\n{\n\tstatic struct lock_class_key stop_pi_lock;\n\tstruct sched_param param = { .sched_priority = MAX_RT_PRIO - 1 };\n\tstruct task_struct *old_stop = cpu_rq(cpu)->stop;\n\n\tif (stop) {\n\t\t \n\t\tsched_setscheduler_nocheck(stop, SCHED_FIFO, &param);\n\n\t\tstop->sched_class = &stop_sched_class;\n\n\t\t \n\t\tlockdep_set_class(&stop->pi_lock, &stop_pi_lock);\n\t}\n\n\tcpu_rq(cpu)->stop = stop;\n\n\tif (old_stop) {\n\t\t \n\t\told_stop->sched_class = &rt_sched_class;\n\t}\n}\n\n#else  \n\nstatic inline int __set_cpus_allowed_ptr(struct task_struct *p,\n\t\t\t\t\t struct affinity_context *ctx)\n{\n\treturn set_cpus_allowed_ptr(p, ctx->new_mask);\n}\n\nstatic inline void migrate_disable_switch(struct rq *rq, struct task_struct *p) { }\n\nstatic inline bool rq_has_pinned_tasks(struct rq *rq)\n{\n\treturn false;\n}\n\nstatic inline cpumask_t *alloc_user_cpus_ptr(int node)\n{\n\treturn NULL;\n}\n\n#endif  \n\nstatic void\nttwu_stat(struct task_struct *p, int cpu, int wake_flags)\n{\n\tstruct rq *rq;\n\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\trq = this_rq();\n\n#ifdef CONFIG_SMP\n\tif (cpu == rq->cpu) {\n\t\t__schedstat_inc(rq->ttwu_local);\n\t\t__schedstat_inc(p->stats.nr_wakeups_local);\n\t} else {\n\t\tstruct sched_domain *sd;\n\n\t\t__schedstat_inc(p->stats.nr_wakeups_remote);\n\n\t\tguard(rcu)();\n\t\tfor_each_domain(rq->cpu, sd) {\n\t\t\tif (cpumask_test_cpu(cpu, sched_domain_span(sd))) {\n\t\t\t\t__schedstat_inc(sd->ttwu_wake_remote);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (wake_flags & WF_MIGRATED)\n\t\t__schedstat_inc(p->stats.nr_wakeups_migrate);\n#endif  \n\n\t__schedstat_inc(rq->ttwu_count);\n\t__schedstat_inc(p->stats.nr_wakeups);\n\n\tif (wake_flags & WF_SYNC)\n\t\t__schedstat_inc(p->stats.nr_wakeups_sync);\n}\n\n \nstatic inline void ttwu_do_wakeup(struct task_struct *p)\n{\n\tWRITE_ONCE(p->__state, TASK_RUNNING);\n\ttrace_sched_wakeup(p);\n}\n\nstatic void\nttwu_do_activate(struct rq *rq, struct task_struct *p, int wake_flags,\n\t\t struct rq_flags *rf)\n{\n\tint en_flags = ENQUEUE_WAKEUP | ENQUEUE_NOCLOCK;\n\n\tlockdep_assert_rq_held(rq);\n\n\tif (p->sched_contributes_to_load)\n\t\trq->nr_uninterruptible--;\n\n#ifdef CONFIG_SMP\n\tif (wake_flags & WF_MIGRATED)\n\t\ten_flags |= ENQUEUE_MIGRATED;\n\telse\n#endif\n\tif (p->in_iowait) {\n\t\tdelayacct_blkio_end(p);\n\t\tatomic_dec(&task_rq(p)->nr_iowait);\n\t}\n\n\tactivate_task(rq, p, en_flags);\n\tcheck_preempt_curr(rq, p, wake_flags);\n\n\tttwu_do_wakeup(p);\n\n#ifdef CONFIG_SMP\n\tif (p->sched_class->task_woken) {\n\t\t \n\t\trq_unpin_lock(rq, rf);\n\t\tp->sched_class->task_woken(rq, p);\n\t\trq_repin_lock(rq, rf);\n\t}\n\n\tif (rq->idle_stamp) {\n\t\tu64 delta = rq_clock(rq) - rq->idle_stamp;\n\t\tu64 max = 2*rq->max_idle_balance_cost;\n\n\t\tupdate_avg(&rq->avg_idle, delta);\n\n\t\tif (rq->avg_idle > max)\n\t\t\trq->avg_idle = max;\n\n\t\trq->wake_stamp = jiffies;\n\t\trq->wake_avg_idle = rq->avg_idle / 2;\n\n\t\trq->idle_stamp = 0;\n\t}\n#endif\n}\n\n \nstatic int ttwu_runnable(struct task_struct *p, int wake_flags)\n{\n\tstruct rq_flags rf;\n\tstruct rq *rq;\n\tint ret = 0;\n\n\trq = __task_rq_lock(p, &rf);\n\tif (task_on_rq_queued(p)) {\n\t\tif (!task_on_cpu(rq, p)) {\n\t\t\t \n\t\t\tupdate_rq_clock(rq);\n\t\t\tcheck_preempt_curr(rq, p, wake_flags);\n\t\t}\n\t\tttwu_do_wakeup(p);\n\t\tret = 1;\n\t}\n\t__task_rq_unlock(rq, &rf);\n\n\treturn ret;\n}\n\n#ifdef CONFIG_SMP\nvoid sched_ttwu_pending(void *arg)\n{\n\tstruct llist_node *llist = arg;\n\tstruct rq *rq = this_rq();\n\tstruct task_struct *p, *t;\n\tstruct rq_flags rf;\n\n\tif (!llist)\n\t\treturn;\n\n\trq_lock_irqsave(rq, &rf);\n\tupdate_rq_clock(rq);\n\n\tllist_for_each_entry_safe(p, t, llist, wake_entry.llist) {\n\t\tif (WARN_ON_ONCE(p->on_cpu))\n\t\t\tsmp_cond_load_acquire(&p->on_cpu, !VAL);\n\n\t\tif (WARN_ON_ONCE(task_cpu(p) != cpu_of(rq)))\n\t\t\tset_task_cpu(p, cpu_of(rq));\n\n\t\tttwu_do_activate(rq, p, p->sched_remote_wakeup ? WF_MIGRATED : 0, &rf);\n\t}\n\n\t \n\tWRITE_ONCE(rq->ttwu_pending, 0);\n\trq_unlock_irqrestore(rq, &rf);\n}\n\n \nbool call_function_single_prep_ipi(int cpu)\n{\n\tif (set_nr_if_polling(cpu_rq(cpu)->idle)) {\n\t\ttrace_sched_wake_idle_without_ipi(cpu);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\n \nstatic void __ttwu_queue_wakelist(struct task_struct *p, int cpu, int wake_flags)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\n\tp->sched_remote_wakeup = !!(wake_flags & WF_MIGRATED);\n\n\tWRITE_ONCE(rq->ttwu_pending, 1);\n\t__smp_call_single_queue(cpu, &p->wake_entry.llist);\n}\n\nvoid wake_up_if_idle(int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\n\tguard(rcu)();\n\tif (is_idle_task(rcu_dereference(rq->curr))) {\n\t\tguard(rq_lock_irqsave)(rq);\n\t\tif (is_idle_task(rq->curr))\n\t\t\tresched_curr(rq);\n\t}\n}\n\nbool cpus_share_cache(int this_cpu, int that_cpu)\n{\n\tif (this_cpu == that_cpu)\n\t\treturn true;\n\n\treturn per_cpu(sd_llc_id, this_cpu) == per_cpu(sd_llc_id, that_cpu);\n}\n\nstatic inline bool ttwu_queue_cond(struct task_struct *p, int cpu)\n{\n\t \n\tif (!cpu_active(cpu))\n\t\treturn false;\n\n\t \n\tif (!cpumask_test_cpu(cpu, p->cpus_ptr))\n\t\treturn false;\n\n\t \n\tif (!cpus_share_cache(smp_processor_id(), cpu))\n\t\treturn true;\n\n\tif (cpu == smp_processor_id())\n\t\treturn false;\n\n\t \n\tif (!cpu_rq(cpu)->nr_running)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic bool ttwu_queue_wakelist(struct task_struct *p, int cpu, int wake_flags)\n{\n\tif (sched_feat(TTWU_QUEUE) && ttwu_queue_cond(p, cpu)) {\n\t\tsched_clock_cpu(cpu);  \n\t\t__ttwu_queue_wakelist(p, cpu, wake_flags);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n#else  \n\nstatic inline bool ttwu_queue_wakelist(struct task_struct *p, int cpu, int wake_flags)\n{\n\treturn false;\n}\n\n#endif  \n\nstatic void ttwu_queue(struct task_struct *p, int cpu, int wake_flags)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tstruct rq_flags rf;\n\n\tif (ttwu_queue_wakelist(p, cpu, wake_flags))\n\t\treturn;\n\n\trq_lock(rq, &rf);\n\tupdate_rq_clock(rq);\n\tttwu_do_activate(rq, p, wake_flags, &rf);\n\trq_unlock(rq, &rf);\n}\n\n \nstatic __always_inline\nbool ttwu_state_match(struct task_struct *p, unsigned int state, int *success)\n{\n\tint match;\n\n\tif (IS_ENABLED(CONFIG_DEBUG_PREEMPT)) {\n\t\tWARN_ON_ONCE((state & TASK_RTLOCK_WAIT) &&\n\t\t\t     state != TASK_RTLOCK_WAIT);\n\t}\n\n\t*success = !!(match = __task_state_match(p, state));\n\n#ifdef CONFIG_PREEMPT_RT\n\t \n\tif (match < 0)\n\t\tp->saved_state = TASK_RUNNING;\n#endif\n\treturn match > 0;\n}\n\n \n\n \nint try_to_wake_up(struct task_struct *p, unsigned int state, int wake_flags)\n{\n\tguard(preempt)();\n\tint cpu, success = 0;\n\n\tif (p == current) {\n\t\t \n\t\tif (!ttwu_state_match(p, state, &success))\n\t\t\tgoto out;\n\n\t\ttrace_sched_waking(p);\n\t\tttwu_do_wakeup(p);\n\t\tgoto out;\n\t}\n\n\t \n\tscoped_guard (raw_spinlock_irqsave, &p->pi_lock) {\n\t\tsmp_mb__after_spinlock();\n\t\tif (!ttwu_state_match(p, state, &success))\n\t\t\tbreak;\n\n\t\ttrace_sched_waking(p);\n\n\t\t \n\t\tsmp_rmb();\n\t\tif (READ_ONCE(p->on_rq) && ttwu_runnable(p, wake_flags))\n\t\t\tbreak;\n\n#ifdef CONFIG_SMP\n\t\t \n\t\tsmp_acquire__after_ctrl_dep();\n\n\t\t \n\t\tWRITE_ONCE(p->__state, TASK_WAKING);\n\n\t\t \n\t\tif (smp_load_acquire(&p->on_cpu) &&\n\t\t    ttwu_queue_wakelist(p, task_cpu(p), wake_flags))\n\t\t\tbreak;\n\n\t\t \n\t\tsmp_cond_load_acquire(&p->on_cpu, !VAL);\n\n\t\tcpu = select_task_rq(p, p->wake_cpu, wake_flags | WF_TTWU);\n\t\tif (task_cpu(p) != cpu) {\n\t\t\tif (p->in_iowait) {\n\t\t\t\tdelayacct_blkio_end(p);\n\t\t\t\tatomic_dec(&task_rq(p)->nr_iowait);\n\t\t\t}\n\n\t\t\twake_flags |= WF_MIGRATED;\n\t\t\tpsi_ttwu_dequeue(p);\n\t\t\tset_task_cpu(p, cpu);\n\t\t}\n#else\n\t\tcpu = task_cpu(p);\n#endif  \n\n\t\tttwu_queue(p, cpu, wake_flags);\n\t}\nout:\n\tif (success)\n\t\tttwu_stat(p, task_cpu(p), wake_flags);\n\n\treturn success;\n}\n\nstatic bool __task_needs_rq_lock(struct task_struct *p)\n{\n\tunsigned int state = READ_ONCE(p->__state);\n\n\t \n\tif (state == TASK_RUNNING || state == TASK_WAKING)\n\t\treturn true;\n\n\t \n\tsmp_rmb();\n\tif (p->on_rq)\n\t\treturn true;\n\n#ifdef CONFIG_SMP\n\t \n\tsmp_rmb();\n\tsmp_cond_load_acquire(&p->on_cpu, !VAL);\n#endif\n\n\treturn false;\n}\n\n \nint task_call_func(struct task_struct *p, task_call_f func, void *arg)\n{\n\tstruct rq *rq = NULL;\n\tstruct rq_flags rf;\n\tint ret;\n\n\traw_spin_lock_irqsave(&p->pi_lock, rf.flags);\n\n\tif (__task_needs_rq_lock(p))\n\t\trq = __task_rq_lock(p, &rf);\n\n\t \n\tret = func(p, arg);\n\n\tif (rq)\n\t\trq_unlock(rq, &rf);\n\n\traw_spin_unlock_irqrestore(&p->pi_lock, rf.flags);\n\treturn ret;\n}\n\n \nstruct task_struct *cpu_curr_snapshot(int cpu)\n{\n\tstruct task_struct *t;\n\n\tsmp_mb();  \n\tt = rcu_dereference(cpu_curr(cpu));\n\tsmp_mb();  \n\treturn t;\n}\n\n \nint wake_up_process(struct task_struct *p)\n{\n\treturn try_to_wake_up(p, TASK_NORMAL, 0);\n}\nEXPORT_SYMBOL(wake_up_process);\n\nint wake_up_state(struct task_struct *p, unsigned int state)\n{\n\treturn try_to_wake_up(p, state, 0);\n}\n\n \nstatic void __sched_fork(unsigned long clone_flags, struct task_struct *p)\n{\n\tp->on_rq\t\t\t= 0;\n\n\tp->se.on_rq\t\t\t= 0;\n\tp->se.exec_start\t\t= 0;\n\tp->se.sum_exec_runtime\t\t= 0;\n\tp->se.prev_sum_exec_runtime\t= 0;\n\tp->se.nr_migrations\t\t= 0;\n\tp->se.vruntime\t\t\t= 0;\n\tp->se.vlag\t\t\t= 0;\n\tp->se.slice\t\t\t= sysctl_sched_base_slice;\n\tINIT_LIST_HEAD(&p->se.group_node);\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tp->se.cfs_rq\t\t\t= NULL;\n#endif\n\n#ifdef CONFIG_SCHEDSTATS\n\t \n\tmemset(&p->stats, 0, sizeof(p->stats));\n#endif\n\n\tRB_CLEAR_NODE(&p->dl.rb_node);\n\tinit_dl_task_timer(&p->dl);\n\tinit_dl_inactive_task_timer(&p->dl);\n\t__dl_clear_params(p);\n\n\tINIT_LIST_HEAD(&p->rt.run_list);\n\tp->rt.timeout\t\t= 0;\n\tp->rt.time_slice\t= sched_rr_timeslice;\n\tp->rt.on_rq\t\t= 0;\n\tp->rt.on_list\t\t= 0;\n\n#ifdef CONFIG_PREEMPT_NOTIFIERS\n\tINIT_HLIST_HEAD(&p->preempt_notifiers);\n#endif\n\n#ifdef CONFIG_COMPACTION\n\tp->capture_control = NULL;\n#endif\n\tinit_numa_balancing(clone_flags, p);\n#ifdef CONFIG_SMP\n\tp->wake_entry.u_flags = CSD_TYPE_TTWU;\n\tp->migration_pending = NULL;\n#endif\n\tinit_sched_mm_cid(p);\n}\n\nDEFINE_STATIC_KEY_FALSE(sched_numa_balancing);\n\n#ifdef CONFIG_NUMA_BALANCING\n\nint sysctl_numa_balancing_mode;\n\nstatic void __set_numabalancing_state(bool enabled)\n{\n\tif (enabled)\n\t\tstatic_branch_enable(&sched_numa_balancing);\n\telse\n\t\tstatic_branch_disable(&sched_numa_balancing);\n}\n\nvoid set_numabalancing_state(bool enabled)\n{\n\tif (enabled)\n\t\tsysctl_numa_balancing_mode = NUMA_BALANCING_NORMAL;\n\telse\n\t\tsysctl_numa_balancing_mode = NUMA_BALANCING_DISABLED;\n\t__set_numabalancing_state(enabled);\n}\n\n#ifdef CONFIG_PROC_SYSCTL\nstatic void reset_memory_tiering(void)\n{\n\tstruct pglist_data *pgdat;\n\n\tfor_each_online_pgdat(pgdat) {\n\t\tpgdat->nbp_threshold = 0;\n\t\tpgdat->nbp_th_nr_cand = node_page_state(pgdat, PGPROMOTE_CANDIDATE);\n\t\tpgdat->nbp_th_start = jiffies_to_msecs(jiffies);\n\t}\n}\n\nstatic int sysctl_numa_balancing(struct ctl_table *table, int write,\n\t\t\t  void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct ctl_table t;\n\tint err;\n\tint state = sysctl_numa_balancing_mode;\n\n\tif (write && !capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tt = *table;\n\tt.data = &state;\n\terr = proc_dointvec_minmax(&t, write, buffer, lenp, ppos);\n\tif (err < 0)\n\t\treturn err;\n\tif (write) {\n\t\tif (!(sysctl_numa_balancing_mode & NUMA_BALANCING_MEMORY_TIERING) &&\n\t\t    (state & NUMA_BALANCING_MEMORY_TIERING))\n\t\t\treset_memory_tiering();\n\t\tsysctl_numa_balancing_mode = state;\n\t\t__set_numabalancing_state(state);\n\t}\n\treturn err;\n}\n#endif\n#endif\n\n#ifdef CONFIG_SCHEDSTATS\n\nDEFINE_STATIC_KEY_FALSE(sched_schedstats);\n\nstatic void set_schedstats(bool enabled)\n{\n\tif (enabled)\n\t\tstatic_branch_enable(&sched_schedstats);\n\telse\n\t\tstatic_branch_disable(&sched_schedstats);\n}\n\nvoid force_schedstat_enabled(void)\n{\n\tif (!schedstat_enabled()) {\n\t\tpr_info(\"kernel profiling enabled schedstats, disable via kernel.sched_schedstats.\\n\");\n\t\tstatic_branch_enable(&sched_schedstats);\n\t}\n}\n\nstatic int __init setup_schedstats(char *str)\n{\n\tint ret = 0;\n\tif (!str)\n\t\tgoto out;\n\n\tif (!strcmp(str, \"enable\")) {\n\t\tset_schedstats(true);\n\t\tret = 1;\n\t} else if (!strcmp(str, \"disable\")) {\n\t\tset_schedstats(false);\n\t\tret = 1;\n\t}\nout:\n\tif (!ret)\n\t\tpr_warn(\"Unable to parse schedstats=\\n\");\n\n\treturn ret;\n}\n__setup(\"schedstats=\", setup_schedstats);\n\n#ifdef CONFIG_PROC_SYSCTL\nstatic int sysctl_schedstats(struct ctl_table *table, int write, void *buffer,\n\t\tsize_t *lenp, loff_t *ppos)\n{\n\tstruct ctl_table t;\n\tint err;\n\tint state = static_branch_likely(&sched_schedstats);\n\n\tif (write && !capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tt = *table;\n\tt.data = &state;\n\terr = proc_dointvec_minmax(&t, write, buffer, lenp, ppos);\n\tif (err < 0)\n\t\treturn err;\n\tif (write)\n\t\tset_schedstats(state);\n\treturn err;\n}\n#endif  \n#endif  \n\n#ifdef CONFIG_SYSCTL\nstatic struct ctl_table sched_core_sysctls[] = {\n#ifdef CONFIG_SCHEDSTATS\n\t{\n\t\t.procname       = \"sched_schedstats\",\n\t\t.data           = NULL,\n\t\t.maxlen         = sizeof(unsigned int),\n\t\t.mode           = 0644,\n\t\t.proc_handler   = sysctl_schedstats,\n\t\t.extra1         = SYSCTL_ZERO,\n\t\t.extra2         = SYSCTL_ONE,\n\t},\n#endif  \n#ifdef CONFIG_UCLAMP_TASK\n\t{\n\t\t.procname       = \"sched_util_clamp_min\",\n\t\t.data           = &sysctl_sched_uclamp_util_min,\n\t\t.maxlen         = sizeof(unsigned int),\n\t\t.mode           = 0644,\n\t\t.proc_handler   = sysctl_sched_uclamp_handler,\n\t},\n\t{\n\t\t.procname       = \"sched_util_clamp_max\",\n\t\t.data           = &sysctl_sched_uclamp_util_max,\n\t\t.maxlen         = sizeof(unsigned int),\n\t\t.mode           = 0644,\n\t\t.proc_handler   = sysctl_sched_uclamp_handler,\n\t},\n\t{\n\t\t.procname       = \"sched_util_clamp_min_rt_default\",\n\t\t.data           = &sysctl_sched_uclamp_util_min_rt_default,\n\t\t.maxlen         = sizeof(unsigned int),\n\t\t.mode           = 0644,\n\t\t.proc_handler   = sysctl_sched_uclamp_handler,\n\t},\n#endif  \n#ifdef CONFIG_NUMA_BALANCING\n\t{\n\t\t.procname\t= \"numa_balancing\",\n\t\t.data\t\t= NULL,  \n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= sysctl_numa_balancing,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_FOUR,\n\t},\n#endif  \n\t{}\n};\nstatic int __init sched_core_sysctl_init(void)\n{\n\tregister_sysctl_init(\"kernel\", sched_core_sysctls);\n\treturn 0;\n}\nlate_initcall(sched_core_sysctl_init);\n#endif  \n\n \nint sched_fork(unsigned long clone_flags, struct task_struct *p)\n{\n\t__sched_fork(clone_flags, p);\n\t \n\tp->__state = TASK_NEW;\n\n\t \n\tp->prio = current->normal_prio;\n\n\tuclamp_fork(p);\n\n\t \n\tif (unlikely(p->sched_reset_on_fork)) {\n\t\tif (task_has_dl_policy(p) || task_has_rt_policy(p)) {\n\t\t\tp->policy = SCHED_NORMAL;\n\t\t\tp->static_prio = NICE_TO_PRIO(0);\n\t\t\tp->rt_priority = 0;\n\t\t} else if (PRIO_TO_NICE(p->static_prio) < 0)\n\t\t\tp->static_prio = NICE_TO_PRIO(0);\n\n\t\tp->prio = p->normal_prio = p->static_prio;\n\t\tset_load_weight(p, false);\n\n\t\t \n\t\tp->sched_reset_on_fork = 0;\n\t}\n\n\tif (dl_prio(p->prio))\n\t\treturn -EAGAIN;\n\telse if (rt_prio(p->prio))\n\t\tp->sched_class = &rt_sched_class;\n\telse\n\t\tp->sched_class = &fair_sched_class;\n\n\tinit_entity_runnable_average(&p->se);\n\n\n#ifdef CONFIG_SCHED_INFO\n\tif (likely(sched_info_on()))\n\t\tmemset(&p->sched_info, 0, sizeof(p->sched_info));\n#endif\n#if defined(CONFIG_SMP)\n\tp->on_cpu = 0;\n#endif\n\tinit_task_preempt_count(p);\n#ifdef CONFIG_SMP\n\tplist_node_init(&p->pushable_tasks, MAX_PRIO);\n\tRB_CLEAR_NODE(&p->pushable_dl_tasks);\n#endif\n\treturn 0;\n}\n\nvoid sched_cgroup_fork(struct task_struct *p, struct kernel_clone_args *kargs)\n{\n\tunsigned long flags;\n\n\t \n\traw_spin_lock_irqsave(&p->pi_lock, flags);\n#ifdef CONFIG_CGROUP_SCHED\n\tif (1) {\n\t\tstruct task_group *tg;\n\t\ttg = container_of(kargs->cset->subsys[cpu_cgrp_id],\n\t\t\t\t  struct task_group, css);\n\t\ttg = autogroup_task_group(p, tg);\n\t\tp->sched_task_group = tg;\n\t}\n#endif\n\trseq_migrate(p);\n\t \n\t__set_task_cpu(p, smp_processor_id());\n\tif (p->sched_class->task_fork)\n\t\tp->sched_class->task_fork(p);\n\traw_spin_unlock_irqrestore(&p->pi_lock, flags);\n}\n\nvoid sched_post_fork(struct task_struct *p)\n{\n\tuclamp_post_fork(p);\n}\n\nunsigned long to_ratio(u64 period, u64 runtime)\n{\n\tif (runtime == RUNTIME_INF)\n\t\treturn BW_UNIT;\n\n\t \n\tif (period == 0)\n\t\treturn 0;\n\n\treturn div64_u64(runtime << BW_SHIFT, period);\n}\n\n \nvoid wake_up_new_task(struct task_struct *p)\n{\n\tstruct rq_flags rf;\n\tstruct rq *rq;\n\n\traw_spin_lock_irqsave(&p->pi_lock, rf.flags);\n\tWRITE_ONCE(p->__state, TASK_RUNNING);\n#ifdef CONFIG_SMP\n\t \n\tp->recent_used_cpu = task_cpu(p);\n\trseq_migrate(p);\n\t__set_task_cpu(p, select_task_rq(p, task_cpu(p), WF_FORK));\n#endif\n\trq = __task_rq_lock(p, &rf);\n\tupdate_rq_clock(rq);\n\tpost_init_entity_util_avg(p);\n\n\tactivate_task(rq, p, ENQUEUE_NOCLOCK);\n\ttrace_sched_wakeup_new(p);\n\tcheck_preempt_curr(rq, p, WF_FORK);\n#ifdef CONFIG_SMP\n\tif (p->sched_class->task_woken) {\n\t\t \n\t\trq_unpin_lock(rq, &rf);\n\t\tp->sched_class->task_woken(rq, p);\n\t\trq_repin_lock(rq, &rf);\n\t}\n#endif\n\ttask_rq_unlock(rq, p, &rf);\n}\n\n#ifdef CONFIG_PREEMPT_NOTIFIERS\n\nstatic DEFINE_STATIC_KEY_FALSE(preempt_notifier_key);\n\nvoid preempt_notifier_inc(void)\n{\n\tstatic_branch_inc(&preempt_notifier_key);\n}\nEXPORT_SYMBOL_GPL(preempt_notifier_inc);\n\nvoid preempt_notifier_dec(void)\n{\n\tstatic_branch_dec(&preempt_notifier_key);\n}\nEXPORT_SYMBOL_GPL(preempt_notifier_dec);\n\n \nvoid preempt_notifier_register(struct preempt_notifier *notifier)\n{\n\tif (!static_branch_unlikely(&preempt_notifier_key))\n\t\tWARN(1, \"registering preempt_notifier while notifiers disabled\\n\");\n\n\thlist_add_head(&notifier->link, &current->preempt_notifiers);\n}\nEXPORT_SYMBOL_GPL(preempt_notifier_register);\n\n \nvoid preempt_notifier_unregister(struct preempt_notifier *notifier)\n{\n\thlist_del(&notifier->link);\n}\nEXPORT_SYMBOL_GPL(preempt_notifier_unregister);\n\nstatic void __fire_sched_in_preempt_notifiers(struct task_struct *curr)\n{\n\tstruct preempt_notifier *notifier;\n\n\thlist_for_each_entry(notifier, &curr->preempt_notifiers, link)\n\t\tnotifier->ops->sched_in(notifier, raw_smp_processor_id());\n}\n\nstatic __always_inline void fire_sched_in_preempt_notifiers(struct task_struct *curr)\n{\n\tif (static_branch_unlikely(&preempt_notifier_key))\n\t\t__fire_sched_in_preempt_notifiers(curr);\n}\n\nstatic void\n__fire_sched_out_preempt_notifiers(struct task_struct *curr,\n\t\t\t\t   struct task_struct *next)\n{\n\tstruct preempt_notifier *notifier;\n\n\thlist_for_each_entry(notifier, &curr->preempt_notifiers, link)\n\t\tnotifier->ops->sched_out(notifier, next);\n}\n\nstatic __always_inline void\nfire_sched_out_preempt_notifiers(struct task_struct *curr,\n\t\t\t\t struct task_struct *next)\n{\n\tif (static_branch_unlikely(&preempt_notifier_key))\n\t\t__fire_sched_out_preempt_notifiers(curr, next);\n}\n\n#else  \n\nstatic inline void fire_sched_in_preempt_notifiers(struct task_struct *curr)\n{\n}\n\nstatic inline void\nfire_sched_out_preempt_notifiers(struct task_struct *curr,\n\t\t\t\t struct task_struct *next)\n{\n}\n\n#endif  \n\nstatic inline void prepare_task(struct task_struct *next)\n{\n#ifdef CONFIG_SMP\n\t \n\tWRITE_ONCE(next->on_cpu, 1);\n#endif\n}\n\nstatic inline void finish_task(struct task_struct *prev)\n{\n#ifdef CONFIG_SMP\n\t \n\tsmp_store_release(&prev->on_cpu, 0);\n#endif\n}\n\n#ifdef CONFIG_SMP\n\nstatic void do_balance_callbacks(struct rq *rq, struct balance_callback *head)\n{\n\tvoid (*func)(struct rq *rq);\n\tstruct balance_callback *next;\n\n\tlockdep_assert_rq_held(rq);\n\n\twhile (head) {\n\t\tfunc = (void (*)(struct rq *))head->func;\n\t\tnext = head->next;\n\t\thead->next = NULL;\n\t\thead = next;\n\n\t\tfunc(rq);\n\t}\n}\n\nstatic void balance_push(struct rq *rq);\n\n \nstruct balance_callback balance_push_callback = {\n\t.next = NULL,\n\t.func = balance_push,\n};\n\nstatic inline struct balance_callback *\n__splice_balance_callbacks(struct rq *rq, bool split)\n{\n\tstruct balance_callback *head = rq->balance_callback;\n\n\tif (likely(!head))\n\t\treturn NULL;\n\n\tlockdep_assert_rq_held(rq);\n\t \n\tif (split && head == &balance_push_callback)\n\t\thead = NULL;\n\telse\n\t\trq->balance_callback = NULL;\n\n\treturn head;\n}\n\nstatic inline struct balance_callback *splice_balance_callbacks(struct rq *rq)\n{\n\treturn __splice_balance_callbacks(rq, true);\n}\n\nstatic void __balance_callbacks(struct rq *rq)\n{\n\tdo_balance_callbacks(rq, __splice_balance_callbacks(rq, false));\n}\n\nstatic inline void balance_callbacks(struct rq *rq, struct balance_callback *head)\n{\n\tunsigned long flags;\n\n\tif (unlikely(head)) {\n\t\traw_spin_rq_lock_irqsave(rq, flags);\n\t\tdo_balance_callbacks(rq, head);\n\t\traw_spin_rq_unlock_irqrestore(rq, flags);\n\t}\n}\n\n#else\n\nstatic inline void __balance_callbacks(struct rq *rq)\n{\n}\n\nstatic inline struct balance_callback *splice_balance_callbacks(struct rq *rq)\n{\n\treturn NULL;\n}\n\nstatic inline void balance_callbacks(struct rq *rq, struct balance_callback *head)\n{\n}\n\n#endif\n\nstatic inline void\nprepare_lock_switch(struct rq *rq, struct task_struct *next, struct rq_flags *rf)\n{\n\t \n\trq_unpin_lock(rq, rf);\n\tspin_release(&__rq_lockp(rq)->dep_map, _THIS_IP_);\n#ifdef CONFIG_DEBUG_SPINLOCK\n\t \n\trq_lockp(rq)->owner = next;\n#endif\n}\n\nstatic inline void finish_lock_switch(struct rq *rq)\n{\n\t \n\tspin_acquire(&__rq_lockp(rq)->dep_map, 0, 0, _THIS_IP_);\n\t__balance_callbacks(rq);\n\traw_spin_rq_unlock_irq(rq);\n}\n\n \n\n#ifndef prepare_arch_switch\n# define prepare_arch_switch(next)\tdo { } while (0)\n#endif\n\n#ifndef finish_arch_post_lock_switch\n# define finish_arch_post_lock_switch()\tdo { } while (0)\n#endif\n\nstatic inline void kmap_local_sched_out(void)\n{\n#ifdef CONFIG_KMAP_LOCAL\n\tif (unlikely(current->kmap_ctrl.idx))\n\t\t__kmap_local_sched_out();\n#endif\n}\n\nstatic inline void kmap_local_sched_in(void)\n{\n#ifdef CONFIG_KMAP_LOCAL\n\tif (unlikely(current->kmap_ctrl.idx))\n\t\t__kmap_local_sched_in();\n#endif\n}\n\n \nstatic inline void\nprepare_task_switch(struct rq *rq, struct task_struct *prev,\n\t\t    struct task_struct *next)\n{\n\tkcov_prepare_switch(prev);\n\tsched_info_switch(rq, prev, next);\n\tperf_event_task_sched_out(prev, next);\n\trseq_preempt(prev);\n\tfire_sched_out_preempt_notifiers(prev, next);\n\tkmap_local_sched_out();\n\tprepare_task(next);\n\tprepare_arch_switch(next);\n}\n\n \nstatic struct rq *finish_task_switch(struct task_struct *prev)\n\t__releases(rq->lock)\n{\n\tstruct rq *rq = this_rq();\n\tstruct mm_struct *mm = rq->prev_mm;\n\tunsigned int prev_state;\n\n\t \n\tif (WARN_ONCE(preempt_count() != 2*PREEMPT_DISABLE_OFFSET,\n\t\t      \"corrupted preempt_count: %s/%d/0x%x\\n\",\n\t\t      current->comm, current->pid, preempt_count()))\n\t\tpreempt_count_set(FORK_PREEMPT_COUNT);\n\n\trq->prev_mm = NULL;\n\n\t \n\tprev_state = READ_ONCE(prev->__state);\n\tvtime_task_switch(prev);\n\tperf_event_task_sched_in(prev, current);\n\tfinish_task(prev);\n\ttick_nohz_task_switch();\n\tfinish_lock_switch(rq);\n\tfinish_arch_post_lock_switch();\n\tkcov_finish_switch(current);\n\t \n\tkmap_local_sched_in();\n\n\tfire_sched_in_preempt_notifiers(current);\n\t \n\tif (mm) {\n\t\tmembarrier_mm_sync_core_before_usermode(mm);\n\t\tmmdrop_lazy_tlb_sched(mm);\n\t}\n\n\tif (unlikely(prev_state == TASK_DEAD)) {\n\t\tif (prev->sched_class->task_dead)\n\t\t\tprev->sched_class->task_dead(prev);\n\n\t\t \n\t\tput_task_stack(prev);\n\n\t\tput_task_struct_rcu_user(prev);\n\t}\n\n\treturn rq;\n}\n\n \nasmlinkage __visible void schedule_tail(struct task_struct *prev)\n\t__releases(rq->lock)\n{\n\t \n\n\tfinish_task_switch(prev);\n\tpreempt_enable();\n\n\tif (current->set_child_tid)\n\t\tput_user(task_pid_vnr(current), current->set_child_tid);\n\n\tcalculate_sigpending();\n}\n\n \nstatic __always_inline struct rq *\ncontext_switch(struct rq *rq, struct task_struct *prev,\n\t       struct task_struct *next, struct rq_flags *rf)\n{\n\tprepare_task_switch(rq, prev, next);\n\n\t \n\tarch_start_context_switch(prev);\n\n\t \n\tif (!next->mm) {                                 \n\t\tenter_lazy_tlb(prev->active_mm, next);\n\n\t\tnext->active_mm = prev->active_mm;\n\t\tif (prev->mm)                            \n\t\t\tmmgrab_lazy_tlb(prev->active_mm);\n\t\telse\n\t\t\tprev->active_mm = NULL;\n\t} else {                                         \n\t\tmembarrier_switch_mm(rq, prev->active_mm, next->mm);\n\t\t \n\t\tswitch_mm_irqs_off(prev->active_mm, next->mm, next);\n\t\tlru_gen_use_mm(next->mm);\n\n\t\tif (!prev->mm) {                        \n\t\t\t \n\t\t\trq->prev_mm = prev->active_mm;\n\t\t\tprev->active_mm = NULL;\n\t\t}\n\t}\n\n\t \n\tswitch_mm_cid(rq, prev, next);\n\n\tprepare_lock_switch(rq, next, rf);\n\n\t \n\tswitch_to(prev, next, prev);\n\tbarrier();\n\n\treturn finish_task_switch(prev);\n}\n\n \nunsigned int nr_running(void)\n{\n\tunsigned int i, sum = 0;\n\n\tfor_each_online_cpu(i)\n\t\tsum += cpu_rq(i)->nr_running;\n\n\treturn sum;\n}\n\n \nbool single_task_running(void)\n{\n\treturn raw_rq()->nr_running == 1;\n}\nEXPORT_SYMBOL(single_task_running);\n\nunsigned long long nr_context_switches_cpu(int cpu)\n{\n\treturn cpu_rq(cpu)->nr_switches;\n}\n\nunsigned long long nr_context_switches(void)\n{\n\tint i;\n\tunsigned long long sum = 0;\n\n\tfor_each_possible_cpu(i)\n\t\tsum += cpu_rq(i)->nr_switches;\n\n\treturn sum;\n}\n\n \n\nunsigned int nr_iowait_cpu(int cpu)\n{\n\treturn atomic_read(&cpu_rq(cpu)->nr_iowait);\n}\n\n \n\nunsigned int nr_iowait(void)\n{\n\tunsigned int i, sum = 0;\n\n\tfor_each_possible_cpu(i)\n\t\tsum += nr_iowait_cpu(i);\n\n\treturn sum;\n}\n\n#ifdef CONFIG_SMP\n\n \nvoid sched_exec(void)\n{\n\tstruct task_struct *p = current;\n\tstruct migration_arg arg;\n\tint dest_cpu;\n\n\tscoped_guard (raw_spinlock_irqsave, &p->pi_lock) {\n\t\tdest_cpu = p->sched_class->select_task_rq(p, task_cpu(p), WF_EXEC);\n\t\tif (dest_cpu == smp_processor_id())\n\t\t\treturn;\n\n\t\tif (unlikely(!cpu_active(dest_cpu)))\n\t\t\treturn;\n\n\t\targ = (struct migration_arg){ p, dest_cpu };\n\t}\n\tstop_one_cpu(task_cpu(p), migration_cpu_stop, &arg);\n}\n\n#endif\n\nDEFINE_PER_CPU(struct kernel_stat, kstat);\nDEFINE_PER_CPU(struct kernel_cpustat, kernel_cpustat);\n\nEXPORT_PER_CPU_SYMBOL(kstat);\nEXPORT_PER_CPU_SYMBOL(kernel_cpustat);\n\n \nstatic inline void prefetch_curr_exec_start(struct task_struct *p)\n{\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tstruct sched_entity *curr = (&p->se)->cfs_rq->curr;\n#else\n\tstruct sched_entity *curr = (&task_rq(p)->cfs)->curr;\n#endif\n\tprefetch(curr);\n\tprefetch(&curr->exec_start);\n}\n\n \nunsigned long long task_sched_runtime(struct task_struct *p)\n{\n\tstruct rq_flags rf;\n\tstruct rq *rq;\n\tu64 ns;\n\n#if defined(CONFIG_64BIT) && defined(CONFIG_SMP)\n\t \n\tif (!p->on_cpu || !task_on_rq_queued(p))\n\t\treturn p->se.sum_exec_runtime;\n#endif\n\n\trq = task_rq_lock(p, &rf);\n\t \n\tif (task_current(rq, p) && task_on_rq_queued(p)) {\n\t\tprefetch_curr_exec_start(p);\n\t\tupdate_rq_clock(rq);\n\t\tp->sched_class->update_curr(rq);\n\t}\n\tns = p->se.sum_exec_runtime;\n\ttask_rq_unlock(rq, p, &rf);\n\n\treturn ns;\n}\n\n#ifdef CONFIG_SCHED_DEBUG\nstatic u64 cpu_resched_latency(struct rq *rq)\n{\n\tint latency_warn_ms = READ_ONCE(sysctl_resched_latency_warn_ms);\n\tu64 resched_latency, now = rq_clock(rq);\n\tstatic bool warned_once;\n\n\tif (sysctl_resched_latency_warn_once && warned_once)\n\t\treturn 0;\n\n\tif (!need_resched() || !latency_warn_ms)\n\t\treturn 0;\n\n\tif (system_state == SYSTEM_BOOTING)\n\t\treturn 0;\n\n\tif (!rq->last_seen_need_resched_ns) {\n\t\trq->last_seen_need_resched_ns = now;\n\t\trq->ticks_without_resched = 0;\n\t\treturn 0;\n\t}\n\n\trq->ticks_without_resched++;\n\tresched_latency = now - rq->last_seen_need_resched_ns;\n\tif (resched_latency <= latency_warn_ms * NSEC_PER_MSEC)\n\t\treturn 0;\n\n\twarned_once = true;\n\n\treturn resched_latency;\n}\n\nstatic int __init setup_resched_latency_warn_ms(char *str)\n{\n\tlong val;\n\n\tif ((kstrtol(str, 0, &val))) {\n\t\tpr_warn(\"Unable to set resched_latency_warn_ms\\n\");\n\t\treturn 1;\n\t}\n\n\tsysctl_resched_latency_warn_ms = val;\n\treturn 1;\n}\n__setup(\"resched_latency_warn_ms=\", setup_resched_latency_warn_ms);\n#else\nstatic inline u64 cpu_resched_latency(struct rq *rq) { return 0; }\n#endif  \n\n \nvoid scheduler_tick(void)\n{\n\tint cpu = smp_processor_id();\n\tstruct rq *rq = cpu_rq(cpu);\n\tstruct task_struct *curr = rq->curr;\n\tstruct rq_flags rf;\n\tunsigned long thermal_pressure;\n\tu64 resched_latency;\n\n\tif (housekeeping_cpu(cpu, HK_TYPE_TICK))\n\t\tarch_scale_freq_tick();\n\n\tsched_clock_tick();\n\n\trq_lock(rq, &rf);\n\n\tupdate_rq_clock(rq);\n\tthermal_pressure = arch_scale_thermal_pressure(cpu_of(rq));\n\tupdate_thermal_load_avg(rq_clock_thermal(rq), rq, thermal_pressure);\n\tcurr->sched_class->task_tick(rq, curr, 0);\n\tif (sched_feat(LATENCY_WARN))\n\t\tresched_latency = cpu_resched_latency(rq);\n\tcalc_global_load_tick(rq);\n\tsched_core_tick(rq);\n\ttask_tick_mm_cid(rq, curr);\n\n\trq_unlock(rq, &rf);\n\n\tif (sched_feat(LATENCY_WARN) && resched_latency)\n\t\tresched_latency_warn(cpu, resched_latency);\n\n\tperf_event_task_tick();\n\n\tif (curr->flags & PF_WQ_WORKER)\n\t\twq_worker_tick(curr);\n\n#ifdef CONFIG_SMP\n\trq->idle_balance = idle_cpu(cpu);\n\ttrigger_load_balance(rq);\n#endif\n}\n\n#ifdef CONFIG_NO_HZ_FULL\n\nstruct tick_work {\n\tint\t\t\tcpu;\n\tatomic_t\t\tstate;\n\tstruct delayed_work\twork;\n};\n \n#define TICK_SCHED_REMOTE_OFFLINE\t0\n#define TICK_SCHED_REMOTE_OFFLINING\t1\n#define TICK_SCHED_REMOTE_RUNNING\t2\n\n \n\nstatic struct tick_work __percpu *tick_work_cpu;\n\nstatic void sched_tick_remote(struct work_struct *work)\n{\n\tstruct delayed_work *dwork = to_delayed_work(work);\n\tstruct tick_work *twork = container_of(dwork, struct tick_work, work);\n\tint cpu = twork->cpu;\n\tstruct rq *rq = cpu_rq(cpu);\n\tint os;\n\n\t \n\tif (tick_nohz_tick_stopped_cpu(cpu)) {\n\t\tguard(rq_lock_irq)(rq);\n\t\tstruct task_struct *curr = rq->curr;\n\n\t\tif (cpu_online(cpu)) {\n\t\t\tupdate_rq_clock(rq);\n\n\t\t\tif (!is_idle_task(curr)) {\n\t\t\t\t \n\t\t\t\tu64 delta = rq_clock_task(rq) - curr->se.exec_start;\n\t\t\t\tWARN_ON_ONCE(delta > (u64)NSEC_PER_SEC * 3);\n\t\t\t}\n\t\t\tcurr->sched_class->task_tick(rq, curr, 0);\n\n\t\t\tcalc_load_nohz_remote(rq);\n\t\t}\n\t}\n\n\t \n\tos = atomic_fetch_add_unless(&twork->state, -1, TICK_SCHED_REMOTE_RUNNING);\n\tWARN_ON_ONCE(os == TICK_SCHED_REMOTE_OFFLINE);\n\tif (os == TICK_SCHED_REMOTE_RUNNING)\n\t\tqueue_delayed_work(system_unbound_wq, dwork, HZ);\n}\n\nstatic void sched_tick_start(int cpu)\n{\n\tint os;\n\tstruct tick_work *twork;\n\n\tif (housekeeping_cpu(cpu, HK_TYPE_TICK))\n\t\treturn;\n\n\tWARN_ON_ONCE(!tick_work_cpu);\n\n\ttwork = per_cpu_ptr(tick_work_cpu, cpu);\n\tos = atomic_xchg(&twork->state, TICK_SCHED_REMOTE_RUNNING);\n\tWARN_ON_ONCE(os == TICK_SCHED_REMOTE_RUNNING);\n\tif (os == TICK_SCHED_REMOTE_OFFLINE) {\n\t\ttwork->cpu = cpu;\n\t\tINIT_DELAYED_WORK(&twork->work, sched_tick_remote);\n\t\tqueue_delayed_work(system_unbound_wq, &twork->work, HZ);\n\t}\n}\n\n#ifdef CONFIG_HOTPLUG_CPU\nstatic void sched_tick_stop(int cpu)\n{\n\tstruct tick_work *twork;\n\tint os;\n\n\tif (housekeeping_cpu(cpu, HK_TYPE_TICK))\n\t\treturn;\n\n\tWARN_ON_ONCE(!tick_work_cpu);\n\n\ttwork = per_cpu_ptr(tick_work_cpu, cpu);\n\t \n\tos = atomic_xchg(&twork->state, TICK_SCHED_REMOTE_OFFLINING);\n\tWARN_ON_ONCE(os != TICK_SCHED_REMOTE_RUNNING);\n\t \n}\n#endif  \n\nint __init sched_tick_offload_init(void)\n{\n\ttick_work_cpu = alloc_percpu(struct tick_work);\n\tBUG_ON(!tick_work_cpu);\n\treturn 0;\n}\n\n#else  \nstatic inline void sched_tick_start(int cpu) { }\nstatic inline void sched_tick_stop(int cpu) { }\n#endif\n\n#if defined(CONFIG_PREEMPTION) && (defined(CONFIG_DEBUG_PREEMPT) || \\\n\t\t\t\tdefined(CONFIG_TRACE_PREEMPT_TOGGLE))\n \nstatic inline void preempt_latency_start(int val)\n{\n\tif (preempt_count() == val) {\n\t\tunsigned long ip = get_lock_parent_ip();\n#ifdef CONFIG_DEBUG_PREEMPT\n\t\tcurrent->preempt_disable_ip = ip;\n#endif\n\t\ttrace_preempt_off(CALLER_ADDR0, ip);\n\t}\n}\n\nvoid preempt_count_add(int val)\n{\n#ifdef CONFIG_DEBUG_PREEMPT\n\t \n\tif (DEBUG_LOCKS_WARN_ON((preempt_count() < 0)))\n\t\treturn;\n#endif\n\t__preempt_count_add(val);\n#ifdef CONFIG_DEBUG_PREEMPT\n\t \n\tDEBUG_LOCKS_WARN_ON((preempt_count() & PREEMPT_MASK) >=\n\t\t\t\tPREEMPT_MASK - 10);\n#endif\n\tpreempt_latency_start(val);\n}\nEXPORT_SYMBOL(preempt_count_add);\nNOKPROBE_SYMBOL(preempt_count_add);\n\n \nstatic inline void preempt_latency_stop(int val)\n{\n\tif (preempt_count() == val)\n\t\ttrace_preempt_on(CALLER_ADDR0, get_lock_parent_ip());\n}\n\nvoid preempt_count_sub(int val)\n{\n#ifdef CONFIG_DEBUG_PREEMPT\n\t \n\tif (DEBUG_LOCKS_WARN_ON(val > preempt_count()))\n\t\treturn;\n\t \n\tif (DEBUG_LOCKS_WARN_ON((val < PREEMPT_MASK) &&\n\t\t\t!(preempt_count() & PREEMPT_MASK)))\n\t\treturn;\n#endif\n\n\tpreempt_latency_stop(val);\n\t__preempt_count_sub(val);\n}\nEXPORT_SYMBOL(preempt_count_sub);\nNOKPROBE_SYMBOL(preempt_count_sub);\n\n#else\nstatic inline void preempt_latency_start(int val) { }\nstatic inline void preempt_latency_stop(int val) { }\n#endif\n\nstatic inline unsigned long get_preempt_disable_ip(struct task_struct *p)\n{\n#ifdef CONFIG_DEBUG_PREEMPT\n\treturn p->preempt_disable_ip;\n#else\n\treturn 0;\n#endif\n}\n\n \nstatic noinline void __schedule_bug(struct task_struct *prev)\n{\n\t \n\tunsigned long preempt_disable_ip = get_preempt_disable_ip(current);\n\n\tif (oops_in_progress)\n\t\treturn;\n\n\tprintk(KERN_ERR \"BUG: scheduling while atomic: %s/%d/0x%08x\\n\",\n\t\tprev->comm, prev->pid, preempt_count());\n\n\tdebug_show_held_locks(prev);\n\tprint_modules();\n\tif (irqs_disabled())\n\t\tprint_irqtrace_events(prev);\n\tif (IS_ENABLED(CONFIG_DEBUG_PREEMPT)\n\t    && in_atomic_preempt_off()) {\n\t\tpr_err(\"Preemption disabled at:\");\n\t\tprint_ip_sym(KERN_ERR, preempt_disable_ip);\n\t}\n\tcheck_panic_on_warn(\"scheduling while atomic\");\n\n\tdump_stack();\n\tadd_taint(TAINT_WARN, LOCKDEP_STILL_OK);\n}\n\n \nstatic inline void schedule_debug(struct task_struct *prev, bool preempt)\n{\n#ifdef CONFIG_SCHED_STACK_END_CHECK\n\tif (task_stack_end_corrupted(prev))\n\t\tpanic(\"corrupted stack end detected inside scheduler\\n\");\n\n\tif (task_scs_end_corrupted(prev))\n\t\tpanic(\"corrupted shadow stack detected inside scheduler\\n\");\n#endif\n\n#ifdef CONFIG_DEBUG_ATOMIC_SLEEP\n\tif (!preempt && READ_ONCE(prev->__state) && prev->non_block_count) {\n\t\tprintk(KERN_ERR \"BUG: scheduling in a non-blocking section: %s/%d/%i\\n\",\n\t\t\tprev->comm, prev->pid, prev->non_block_count);\n\t\tdump_stack();\n\t\tadd_taint(TAINT_WARN, LOCKDEP_STILL_OK);\n\t}\n#endif\n\n\tif (unlikely(in_atomic_preempt_off())) {\n\t\t__schedule_bug(prev);\n\t\tpreempt_count_set(PREEMPT_DISABLED);\n\t}\n\trcu_sleep_check();\n\tSCHED_WARN_ON(ct_state() == CONTEXT_USER);\n\n\tprofile_hit(SCHED_PROFILING, __builtin_return_address(0));\n\n\tschedstat_inc(this_rq()->sched_count);\n}\n\nstatic void put_prev_task_balance(struct rq *rq, struct task_struct *prev,\n\t\t\t\t  struct rq_flags *rf)\n{\n#ifdef CONFIG_SMP\n\tconst struct sched_class *class;\n\t \n\tfor_class_range(class, prev->sched_class, &idle_sched_class) {\n\t\tif (class->balance(rq, prev, rf))\n\t\t\tbreak;\n\t}\n#endif\n\n\tput_prev_task(rq, prev);\n}\n\n \nstatic inline struct task_struct *\n__pick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)\n{\n\tconst struct sched_class *class;\n\tstruct task_struct *p;\n\n\t \n\tif (likely(!sched_class_above(prev->sched_class, &fair_sched_class) &&\n\t\t   rq->nr_running == rq->cfs.h_nr_running)) {\n\n\t\tp = pick_next_task_fair(rq, prev, rf);\n\t\tif (unlikely(p == RETRY_TASK))\n\t\t\tgoto restart;\n\n\t\t \n\t\tif (!p) {\n\t\t\tput_prev_task(rq, prev);\n\t\t\tp = pick_next_task_idle(rq);\n\t\t}\n\n\t\treturn p;\n\t}\n\nrestart:\n\tput_prev_task_balance(rq, prev, rf);\n\n\tfor_each_class(class) {\n\t\tp = class->pick_next_task(rq);\n\t\tif (p)\n\t\t\treturn p;\n\t}\n\n\tBUG();  \n}\n\n#ifdef CONFIG_SCHED_CORE\nstatic inline bool is_task_rq_idle(struct task_struct *t)\n{\n\treturn (task_rq(t)->idle == t);\n}\n\nstatic inline bool cookie_equals(struct task_struct *a, unsigned long cookie)\n{\n\treturn is_task_rq_idle(a) || (a->core_cookie == cookie);\n}\n\nstatic inline bool cookie_match(struct task_struct *a, struct task_struct *b)\n{\n\tif (is_task_rq_idle(a) || is_task_rq_idle(b))\n\t\treturn true;\n\n\treturn a->core_cookie == b->core_cookie;\n}\n\nstatic inline struct task_struct *pick_task(struct rq *rq)\n{\n\tconst struct sched_class *class;\n\tstruct task_struct *p;\n\n\tfor_each_class(class) {\n\t\tp = class->pick_task(rq);\n\t\tif (p)\n\t\t\treturn p;\n\t}\n\n\tBUG();  \n}\n\nextern void task_vruntime_update(struct rq *rq, struct task_struct *p, bool in_fi);\n\nstatic void queue_core_balance(struct rq *rq);\n\nstatic struct task_struct *\npick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)\n{\n\tstruct task_struct *next, *p, *max = NULL;\n\tconst struct cpumask *smt_mask;\n\tbool fi_before = false;\n\tbool core_clock_updated = (rq == rq->core);\n\tunsigned long cookie;\n\tint i, cpu, occ = 0;\n\tstruct rq *rq_i;\n\tbool need_sync;\n\n\tif (!sched_core_enabled(rq))\n\t\treturn __pick_next_task(rq, prev, rf);\n\n\tcpu = cpu_of(rq);\n\n\t \n\tif (cpu_is_offline(cpu)) {\n\t\t \n\t\trq->core_pick = NULL;\n\t\treturn __pick_next_task(rq, prev, rf);\n\t}\n\n\t \n\tif (rq->core->core_pick_seq == rq->core->core_task_seq &&\n\t    rq->core->core_pick_seq != rq->core_sched_seq &&\n\t    rq->core_pick) {\n\t\tWRITE_ONCE(rq->core_sched_seq, rq->core->core_pick_seq);\n\n\t\tnext = rq->core_pick;\n\t\tif (next != prev) {\n\t\t\tput_prev_task(rq, prev);\n\t\t\tset_next_task(rq, next);\n\t\t}\n\n\t\trq->core_pick = NULL;\n\t\tgoto out;\n\t}\n\n\tput_prev_task_balance(rq, prev, rf);\n\n\tsmt_mask = cpu_smt_mask(cpu);\n\tneed_sync = !!rq->core->core_cookie;\n\n\t \n\trq->core->core_cookie = 0UL;\n\tif (rq->core->core_forceidle_count) {\n\t\tif (!core_clock_updated) {\n\t\t\tupdate_rq_clock(rq->core);\n\t\t\tcore_clock_updated = true;\n\t\t}\n\t\tsched_core_account_forceidle(rq);\n\t\t \n\t\trq->core->core_forceidle_start = 0;\n\t\trq->core->core_forceidle_count = 0;\n\t\trq->core->core_forceidle_occupation = 0;\n\t\tneed_sync = true;\n\t\tfi_before = true;\n\t}\n\n\t \n\trq->core->core_task_seq++;\n\n\t \n\tif (!need_sync) {\n\t\tnext = pick_task(rq);\n\t\tif (!next->core_cookie) {\n\t\t\trq->core_pick = NULL;\n\t\t\t \n\t\t\tWARN_ON_ONCE(fi_before);\n\t\t\ttask_vruntime_update(rq, next, false);\n\t\t\tgoto out_set_next;\n\t\t}\n\t}\n\n\t \n\tfor_each_cpu_wrap(i, smt_mask, cpu) {\n\t\trq_i = cpu_rq(i);\n\n\t\t \n\t\tif (i != cpu && (rq_i != rq->core || !core_clock_updated))\n\t\t\tupdate_rq_clock(rq_i);\n\n\t\tp = rq_i->core_pick = pick_task(rq_i);\n\t\tif (!max || prio_less(max, p, fi_before))\n\t\t\tmax = p;\n\t}\n\n\tcookie = rq->core->core_cookie = max->core_cookie;\n\n\t \n\tfor_each_cpu(i, smt_mask) {\n\t\trq_i = cpu_rq(i);\n\t\tp = rq_i->core_pick;\n\n\t\tif (!cookie_equals(p, cookie)) {\n\t\t\tp = NULL;\n\t\t\tif (cookie)\n\t\t\t\tp = sched_core_find(rq_i, cookie);\n\t\t\tif (!p)\n\t\t\t\tp = idle_sched_class.pick_task(rq_i);\n\t\t}\n\n\t\trq_i->core_pick = p;\n\n\t\tif (p == rq_i->idle) {\n\t\t\tif (rq_i->nr_running) {\n\t\t\t\trq->core->core_forceidle_count++;\n\t\t\t\tif (!fi_before)\n\t\t\t\t\trq->core->core_forceidle_seq++;\n\t\t\t}\n\t\t} else {\n\t\t\tocc++;\n\t\t}\n\t}\n\n\tif (schedstat_enabled() && rq->core->core_forceidle_count) {\n\t\trq->core->core_forceidle_start = rq_clock(rq->core);\n\t\trq->core->core_forceidle_occupation = occ;\n\t}\n\n\trq->core->core_pick_seq = rq->core->core_task_seq;\n\tnext = rq->core_pick;\n\trq->core_sched_seq = rq->core->core_pick_seq;\n\n\t \n\tWARN_ON_ONCE(!next);\n\n\t \n\tfor_each_cpu(i, smt_mask) {\n\t\trq_i = cpu_rq(i);\n\n\t\t \n\t\tif (!rq_i->core_pick)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (!(fi_before && rq->core->core_forceidle_count))\n\t\t\ttask_vruntime_update(rq_i, rq_i->core_pick, !!rq->core->core_forceidle_count);\n\n\t\trq_i->core_pick->core_occupation = occ;\n\n\t\tif (i == cpu) {\n\t\t\trq_i->core_pick = NULL;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tWARN_ON_ONCE(!cookie_match(next, rq_i->core_pick));\n\n\t\tif (rq_i->curr == rq_i->core_pick) {\n\t\t\trq_i->core_pick = NULL;\n\t\t\tcontinue;\n\t\t}\n\n\t\tresched_curr(rq_i);\n\t}\n\nout_set_next:\n\tset_next_task(rq, next);\nout:\n\tif (rq->core->core_forceidle_count && next == rq->idle)\n\t\tqueue_core_balance(rq);\n\n\treturn next;\n}\n\nstatic bool try_steal_cookie(int this, int that)\n{\n\tstruct rq *dst = cpu_rq(this), *src = cpu_rq(that);\n\tstruct task_struct *p;\n\tunsigned long cookie;\n\tbool success = false;\n\n\tguard(irq)();\n\tguard(double_rq_lock)(dst, src);\n\n\tcookie = dst->core->core_cookie;\n\tif (!cookie)\n\t\treturn false;\n\n\tif (dst->curr != dst->idle)\n\t\treturn false;\n\n\tp = sched_core_find(src, cookie);\n\tif (!p)\n\t\treturn false;\n\n\tdo {\n\t\tif (p == src->core_pick || p == src->curr)\n\t\t\tgoto next;\n\n\t\tif (!is_cpu_allowed(p, this))\n\t\t\tgoto next;\n\n\t\tif (p->core_occupation > dst->idle->core_occupation)\n\t\t\tgoto next;\n\t\t \n\t\tif (sched_task_is_throttled(p, this))\n\t\t\tgoto next;\n\n\t\tdeactivate_task(src, p, 0);\n\t\tset_task_cpu(p, this);\n\t\tactivate_task(dst, p, 0);\n\n\t\tresched_curr(dst);\n\n\t\tsuccess = true;\n\t\tbreak;\n\nnext:\n\t\tp = sched_core_next(p, cookie);\n\t} while (p);\n\n\treturn success;\n}\n\nstatic bool steal_cookie_task(int cpu, struct sched_domain *sd)\n{\n\tint i;\n\n\tfor_each_cpu_wrap(i, sched_domain_span(sd), cpu + 1) {\n\t\tif (i == cpu)\n\t\t\tcontinue;\n\n\t\tif (need_resched())\n\t\t\tbreak;\n\n\t\tif (try_steal_cookie(cpu, i))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic void sched_core_balance(struct rq *rq)\n{\n\tstruct sched_domain *sd;\n\tint cpu = cpu_of(rq);\n\n\tpreempt_disable();\n\trcu_read_lock();\n\traw_spin_rq_unlock_irq(rq);\n\tfor_each_domain(cpu, sd) {\n\t\tif (need_resched())\n\t\t\tbreak;\n\n\t\tif (steal_cookie_task(cpu, sd))\n\t\t\tbreak;\n\t}\n\traw_spin_rq_lock_irq(rq);\n\trcu_read_unlock();\n\tpreempt_enable();\n}\n\nstatic DEFINE_PER_CPU(struct balance_callback, core_balance_head);\n\nstatic void queue_core_balance(struct rq *rq)\n{\n\tif (!sched_core_enabled(rq))\n\t\treturn;\n\n\tif (!rq->core->core_cookie)\n\t\treturn;\n\n\tif (!rq->nr_running)  \n\t\treturn;\n\n\tqueue_balance_callback(rq, &per_cpu(core_balance_head, rq->cpu), sched_core_balance);\n}\n\nDEFINE_LOCK_GUARD_1(core_lock, int,\n\t\t    sched_core_lock(*_T->lock, &_T->flags),\n\t\t    sched_core_unlock(*_T->lock, &_T->flags),\n\t\t    unsigned long flags)\n\nstatic void sched_core_cpu_starting(unsigned int cpu)\n{\n\tconst struct cpumask *smt_mask = cpu_smt_mask(cpu);\n\tstruct rq *rq = cpu_rq(cpu), *core_rq = NULL;\n\tint t;\n\n\tguard(core_lock)(&cpu);\n\n\tWARN_ON_ONCE(rq->core != rq);\n\n\t \n\tif (cpumask_weight(smt_mask) == 1)\n\t\treturn;\n\n\t \n\tfor_each_cpu(t, smt_mask) {\n\t\tif (t == cpu)\n\t\t\tcontinue;\n\t\trq = cpu_rq(t);\n\t\tif (rq->core == rq) {\n\t\t\tcore_rq = rq;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (WARN_ON_ONCE(!core_rq))  \n\t\treturn;\n\n\t \n\tfor_each_cpu(t, smt_mask) {\n\t\trq = cpu_rq(t);\n\n\t\tif (t == cpu)\n\t\t\trq->core = core_rq;\n\n\t\tWARN_ON_ONCE(rq->core != core_rq);\n\t}\n}\n\nstatic void sched_core_cpu_deactivate(unsigned int cpu)\n{\n\tconst struct cpumask *smt_mask = cpu_smt_mask(cpu);\n\tstruct rq *rq = cpu_rq(cpu), *core_rq = NULL;\n\tint t;\n\n\tguard(core_lock)(&cpu);\n\n\t \n\tif (cpumask_weight(smt_mask) == 1) {\n\t\tWARN_ON_ONCE(rq->core != rq);\n\t\treturn;\n\t}\n\n\t \n\tif (rq->core != rq)\n\t\treturn;\n\n\t \n\tfor_each_cpu(t, smt_mask) {\n\t\tif (t == cpu)\n\t\t\tcontinue;\n\t\tcore_rq = cpu_rq(t);\n\t\tbreak;\n\t}\n\n\tif (WARN_ON_ONCE(!core_rq))  \n\t\treturn;\n\n\t \n\tcore_rq->core_task_seq             = rq->core_task_seq;\n\tcore_rq->core_pick_seq             = rq->core_pick_seq;\n\tcore_rq->core_cookie               = rq->core_cookie;\n\tcore_rq->core_forceidle_count      = rq->core_forceidle_count;\n\tcore_rq->core_forceidle_seq        = rq->core_forceidle_seq;\n\tcore_rq->core_forceidle_occupation = rq->core_forceidle_occupation;\n\n\t \n\tcore_rq->core_forceidle_start = 0;\n\n\t \n\tfor_each_cpu(t, smt_mask) {\n\t\trq = cpu_rq(t);\n\t\trq->core = core_rq;\n\t}\n}\n\nstatic inline void sched_core_cpu_dying(unsigned int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\n\tif (rq->core != rq)\n\t\trq->core = rq;\n}\n\n#else  \n\nstatic inline void sched_core_cpu_starting(unsigned int cpu) {}\nstatic inline void sched_core_cpu_deactivate(unsigned int cpu) {}\nstatic inline void sched_core_cpu_dying(unsigned int cpu) {}\n\nstatic struct task_struct *\npick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)\n{\n\treturn __pick_next_task(rq, prev, rf);\n}\n\n#endif  \n\n \n#define SM_NONE\t\t\t0x0\n#define SM_PREEMPT\t\t0x1\n#define SM_RTLOCK_WAIT\t\t0x2\n\n#ifndef CONFIG_PREEMPT_RT\n# define SM_MASK_PREEMPT\t(~0U)\n#else\n# define SM_MASK_PREEMPT\tSM_PREEMPT\n#endif\n\n \nstatic void __sched notrace __schedule(unsigned int sched_mode)\n{\n\tstruct task_struct *prev, *next;\n\tunsigned long *switch_count;\n\tunsigned long prev_state;\n\tstruct rq_flags rf;\n\tstruct rq *rq;\n\tint cpu;\n\n\tcpu = smp_processor_id();\n\trq = cpu_rq(cpu);\n\tprev = rq->curr;\n\n\tschedule_debug(prev, !!sched_mode);\n\n\tif (sched_feat(HRTICK) || sched_feat(HRTICK_DL))\n\t\thrtick_clear(rq);\n\n\tlocal_irq_disable();\n\trcu_note_context_switch(!!sched_mode);\n\n\t \n\trq_lock(rq, &rf);\n\tsmp_mb__after_spinlock();\n\n\t \n\trq->clock_update_flags <<= 1;\n\tupdate_rq_clock(rq);\n\trq->clock_update_flags = RQCF_UPDATED;\n\n\tswitch_count = &prev->nivcsw;\n\n\t \n\tprev_state = READ_ONCE(prev->__state);\n\tif (!(sched_mode & SM_MASK_PREEMPT) && prev_state) {\n\t\tif (signal_pending_state(prev_state, prev)) {\n\t\t\tWRITE_ONCE(prev->__state, TASK_RUNNING);\n\t\t} else {\n\t\t\tprev->sched_contributes_to_load =\n\t\t\t\t(prev_state & TASK_UNINTERRUPTIBLE) &&\n\t\t\t\t!(prev_state & TASK_NOLOAD) &&\n\t\t\t\t!(prev_state & TASK_FROZEN);\n\n\t\t\tif (prev->sched_contributes_to_load)\n\t\t\t\trq->nr_uninterruptible++;\n\n\t\t\t \n\t\t\tdeactivate_task(rq, prev, DEQUEUE_SLEEP | DEQUEUE_NOCLOCK);\n\n\t\t\tif (prev->in_iowait) {\n\t\t\t\tatomic_inc(&rq->nr_iowait);\n\t\t\t\tdelayacct_blkio_start();\n\t\t\t}\n\t\t}\n\t\tswitch_count = &prev->nvcsw;\n\t}\n\n\tnext = pick_next_task(rq, prev, &rf);\n\tclear_tsk_need_resched(prev);\n\tclear_preempt_need_resched();\n#ifdef CONFIG_SCHED_DEBUG\n\trq->last_seen_need_resched_ns = 0;\n#endif\n\n\tif (likely(prev != next)) {\n\t\trq->nr_switches++;\n\t\t \n\t\tRCU_INIT_POINTER(rq->curr, next);\n\t\t \n\t\t++*switch_count;\n\n\t\tmigrate_disable_switch(rq, prev);\n\t\tpsi_sched_switch(prev, next, !task_on_rq_queued(prev));\n\n\t\ttrace_sched_switch(sched_mode & SM_MASK_PREEMPT, prev, next, prev_state);\n\n\t\t \n\t\trq = context_switch(rq, prev, next, &rf);\n\t} else {\n\t\trq_unpin_lock(rq, &rf);\n\t\t__balance_callbacks(rq);\n\t\traw_spin_rq_unlock_irq(rq);\n\t}\n}\n\nvoid __noreturn do_task_dead(void)\n{\n\t \n\tset_special_state(TASK_DEAD);\n\n\t \n\tcurrent->flags |= PF_NOFREEZE;\n\n\t__schedule(SM_NONE);\n\tBUG();\n\n\t \n\tfor (;;)\n\t\tcpu_relax();\n}\n\nstatic inline void sched_submit_work(struct task_struct *tsk)\n{\n\tunsigned int task_flags;\n\n\tif (task_is_running(tsk))\n\t\treturn;\n\n\ttask_flags = tsk->flags;\n\t \n\tif (task_flags & (PF_WQ_WORKER | PF_IO_WORKER)) {\n\t\tif (task_flags & PF_WQ_WORKER)\n\t\t\twq_worker_sleeping(tsk);\n\t\telse\n\t\t\tio_wq_worker_sleeping(tsk);\n\t}\n\n\t \n\tSCHED_WARN_ON(current->__state & TASK_RTLOCK_WAIT);\n\n\t \n\tblk_flush_plug(tsk->plug, true);\n}\n\nstatic void sched_update_worker(struct task_struct *tsk)\n{\n\tif (tsk->flags & (PF_WQ_WORKER | PF_IO_WORKER)) {\n\t\tif (tsk->flags & PF_WQ_WORKER)\n\t\t\twq_worker_running(tsk);\n\t\telse\n\t\t\tio_wq_worker_running(tsk);\n\t}\n}\n\nasmlinkage __visible void __sched schedule(void)\n{\n\tstruct task_struct *tsk = current;\n\n\tsched_submit_work(tsk);\n\tdo {\n\t\tpreempt_disable();\n\t\t__schedule(SM_NONE);\n\t\tsched_preempt_enable_no_resched();\n\t} while (need_resched());\n\tsched_update_worker(tsk);\n}\nEXPORT_SYMBOL(schedule);\n\n \nvoid __sched schedule_idle(void)\n{\n\t \n\tWARN_ON_ONCE(current->__state);\n\tdo {\n\t\t__schedule(SM_NONE);\n\t} while (need_resched());\n}\n\n#if defined(CONFIG_CONTEXT_TRACKING_USER) && !defined(CONFIG_HAVE_CONTEXT_TRACKING_USER_OFFSTACK)\nasmlinkage __visible void __sched schedule_user(void)\n{\n\t \n\tenum ctx_state prev_state = exception_enter();\n\tschedule();\n\texception_exit(prev_state);\n}\n#endif\n\n \nvoid __sched schedule_preempt_disabled(void)\n{\n\tsched_preempt_enable_no_resched();\n\tschedule();\n\tpreempt_disable();\n}\n\n#ifdef CONFIG_PREEMPT_RT\nvoid __sched notrace schedule_rtlock(void)\n{\n\tdo {\n\t\tpreempt_disable();\n\t\t__schedule(SM_RTLOCK_WAIT);\n\t\tsched_preempt_enable_no_resched();\n\t} while (need_resched());\n}\nNOKPROBE_SYMBOL(schedule_rtlock);\n#endif\n\nstatic void __sched notrace preempt_schedule_common(void)\n{\n\tdo {\n\t\t \n\t\tpreempt_disable_notrace();\n\t\tpreempt_latency_start(1);\n\t\t__schedule(SM_PREEMPT);\n\t\tpreempt_latency_stop(1);\n\t\tpreempt_enable_no_resched_notrace();\n\n\t\t \n\t} while (need_resched());\n}\n\n#ifdef CONFIG_PREEMPTION\n \nasmlinkage __visible void __sched notrace preempt_schedule(void)\n{\n\t \n\tif (likely(!preemptible()))\n\t\treturn;\n\tpreempt_schedule_common();\n}\nNOKPROBE_SYMBOL(preempt_schedule);\nEXPORT_SYMBOL(preempt_schedule);\n\n#ifdef CONFIG_PREEMPT_DYNAMIC\n#if defined(CONFIG_HAVE_PREEMPT_DYNAMIC_CALL)\n#ifndef preempt_schedule_dynamic_enabled\n#define preempt_schedule_dynamic_enabled\tpreempt_schedule\n#define preempt_schedule_dynamic_disabled\tNULL\n#endif\nDEFINE_STATIC_CALL(preempt_schedule, preempt_schedule_dynamic_enabled);\nEXPORT_STATIC_CALL_TRAMP(preempt_schedule);\n#elif defined(CONFIG_HAVE_PREEMPT_DYNAMIC_KEY)\nstatic DEFINE_STATIC_KEY_TRUE(sk_dynamic_preempt_schedule);\nvoid __sched notrace dynamic_preempt_schedule(void)\n{\n\tif (!static_branch_unlikely(&sk_dynamic_preempt_schedule))\n\t\treturn;\n\tpreempt_schedule();\n}\nNOKPROBE_SYMBOL(dynamic_preempt_schedule);\nEXPORT_SYMBOL(dynamic_preempt_schedule);\n#endif\n#endif\n\n \nasmlinkage __visible void __sched notrace preempt_schedule_notrace(void)\n{\n\tenum ctx_state prev_ctx;\n\n\tif (likely(!preemptible()))\n\t\treturn;\n\n\tdo {\n\t\t \n\t\tpreempt_disable_notrace();\n\t\tpreempt_latency_start(1);\n\t\t \n\t\tprev_ctx = exception_enter();\n\t\t__schedule(SM_PREEMPT);\n\t\texception_exit(prev_ctx);\n\n\t\tpreempt_latency_stop(1);\n\t\tpreempt_enable_no_resched_notrace();\n\t} while (need_resched());\n}\nEXPORT_SYMBOL_GPL(preempt_schedule_notrace);\n\n#ifdef CONFIG_PREEMPT_DYNAMIC\n#if defined(CONFIG_HAVE_PREEMPT_DYNAMIC_CALL)\n#ifndef preempt_schedule_notrace_dynamic_enabled\n#define preempt_schedule_notrace_dynamic_enabled\tpreempt_schedule_notrace\n#define preempt_schedule_notrace_dynamic_disabled\tNULL\n#endif\nDEFINE_STATIC_CALL(preempt_schedule_notrace, preempt_schedule_notrace_dynamic_enabled);\nEXPORT_STATIC_CALL_TRAMP(preempt_schedule_notrace);\n#elif defined(CONFIG_HAVE_PREEMPT_DYNAMIC_KEY)\nstatic DEFINE_STATIC_KEY_TRUE(sk_dynamic_preempt_schedule_notrace);\nvoid __sched notrace dynamic_preempt_schedule_notrace(void)\n{\n\tif (!static_branch_unlikely(&sk_dynamic_preempt_schedule_notrace))\n\t\treturn;\n\tpreempt_schedule_notrace();\n}\nNOKPROBE_SYMBOL(dynamic_preempt_schedule_notrace);\nEXPORT_SYMBOL(dynamic_preempt_schedule_notrace);\n#endif\n#endif\n\n#endif  \n\n \nasmlinkage __visible void __sched preempt_schedule_irq(void)\n{\n\tenum ctx_state prev_state;\n\n\t \n\tBUG_ON(preempt_count() || !irqs_disabled());\n\n\tprev_state = exception_enter();\n\n\tdo {\n\t\tpreempt_disable();\n\t\tlocal_irq_enable();\n\t\t__schedule(SM_PREEMPT);\n\t\tlocal_irq_disable();\n\t\tsched_preempt_enable_no_resched();\n\t} while (need_resched());\n\n\texception_exit(prev_state);\n}\n\nint default_wake_function(wait_queue_entry_t *curr, unsigned mode, int wake_flags,\n\t\t\t  void *key)\n{\n\tWARN_ON_ONCE(IS_ENABLED(CONFIG_SCHED_DEBUG) && wake_flags & ~(WF_SYNC|WF_CURRENT_CPU));\n\treturn try_to_wake_up(curr->private, mode, wake_flags);\n}\nEXPORT_SYMBOL(default_wake_function);\n\nstatic void __setscheduler_prio(struct task_struct *p, int prio)\n{\n\tif (dl_prio(prio))\n\t\tp->sched_class = &dl_sched_class;\n\telse if (rt_prio(prio))\n\t\tp->sched_class = &rt_sched_class;\n\telse\n\t\tp->sched_class = &fair_sched_class;\n\n\tp->prio = prio;\n}\n\n#ifdef CONFIG_RT_MUTEXES\n\nstatic inline int __rt_effective_prio(struct task_struct *pi_task, int prio)\n{\n\tif (pi_task)\n\t\tprio = min(prio, pi_task->prio);\n\n\treturn prio;\n}\n\nstatic inline int rt_effective_prio(struct task_struct *p, int prio)\n{\n\tstruct task_struct *pi_task = rt_mutex_get_top_task(p);\n\n\treturn __rt_effective_prio(pi_task, prio);\n}\n\n \nvoid rt_mutex_setprio(struct task_struct *p, struct task_struct *pi_task)\n{\n\tint prio, oldprio, queued, running, queue_flag =\n\t\tDEQUEUE_SAVE | DEQUEUE_MOVE | DEQUEUE_NOCLOCK;\n\tconst struct sched_class *prev_class;\n\tstruct rq_flags rf;\n\tstruct rq *rq;\n\n\t \n\tprio = __rt_effective_prio(pi_task, p->normal_prio);\n\n\t \n\tif (p->pi_top_task == pi_task && prio == p->prio && !dl_prio(prio))\n\t\treturn;\n\n\trq = __task_rq_lock(p, &rf);\n\tupdate_rq_clock(rq);\n\t \n\tp->pi_top_task = pi_task;\n\n\t \n\tif (prio == p->prio && !dl_prio(prio))\n\t\tgoto out_unlock;\n\n\t \n\tif (unlikely(p == rq->idle)) {\n\t\tWARN_ON(p != rq->curr);\n\t\tWARN_ON(p->pi_blocked_on);\n\t\tgoto out_unlock;\n\t}\n\n\ttrace_sched_pi_setprio(p, pi_task);\n\toldprio = p->prio;\n\n\tif (oldprio == prio)\n\t\tqueue_flag &= ~DEQUEUE_MOVE;\n\n\tprev_class = p->sched_class;\n\tqueued = task_on_rq_queued(p);\n\trunning = task_current(rq, p);\n\tif (queued)\n\t\tdequeue_task(rq, p, queue_flag);\n\tif (running)\n\t\tput_prev_task(rq, p);\n\n\t \n\tif (dl_prio(prio)) {\n\t\tif (!dl_prio(p->normal_prio) ||\n\t\t    (pi_task && dl_prio(pi_task->prio) &&\n\t\t     dl_entity_preempt(&pi_task->dl, &p->dl))) {\n\t\t\tp->dl.pi_se = pi_task->dl.pi_se;\n\t\t\tqueue_flag |= ENQUEUE_REPLENISH;\n\t\t} else {\n\t\t\tp->dl.pi_se = &p->dl;\n\t\t}\n\t} else if (rt_prio(prio)) {\n\t\tif (dl_prio(oldprio))\n\t\t\tp->dl.pi_se = &p->dl;\n\t\tif (oldprio < prio)\n\t\t\tqueue_flag |= ENQUEUE_HEAD;\n\t} else {\n\t\tif (dl_prio(oldprio))\n\t\t\tp->dl.pi_se = &p->dl;\n\t\tif (rt_prio(oldprio))\n\t\t\tp->rt.timeout = 0;\n\t}\n\n\t__setscheduler_prio(p, prio);\n\n\tif (queued)\n\t\tenqueue_task(rq, p, queue_flag);\n\tif (running)\n\t\tset_next_task(rq, p);\n\n\tcheck_class_changed(rq, p, prev_class, oldprio);\nout_unlock:\n\t \n\tpreempt_disable();\n\n\trq_unpin_lock(rq, &rf);\n\t__balance_callbacks(rq);\n\traw_spin_rq_unlock(rq);\n\n\tpreempt_enable();\n}\n#else\nstatic inline int rt_effective_prio(struct task_struct *p, int prio)\n{\n\treturn prio;\n}\n#endif\n\nvoid set_user_nice(struct task_struct *p, long nice)\n{\n\tbool queued, running;\n\tint old_prio;\n\tstruct rq_flags rf;\n\tstruct rq *rq;\n\n\tif (task_nice(p) == nice || nice < MIN_NICE || nice > MAX_NICE)\n\t\treturn;\n\t \n\trq = task_rq_lock(p, &rf);\n\tupdate_rq_clock(rq);\n\n\t \n\tif (task_has_dl_policy(p) || task_has_rt_policy(p)) {\n\t\tp->static_prio = NICE_TO_PRIO(nice);\n\t\tgoto out_unlock;\n\t}\n\tqueued = task_on_rq_queued(p);\n\trunning = task_current(rq, p);\n\tif (queued)\n\t\tdequeue_task(rq, p, DEQUEUE_SAVE | DEQUEUE_NOCLOCK);\n\tif (running)\n\t\tput_prev_task(rq, p);\n\n\tp->static_prio = NICE_TO_PRIO(nice);\n\tset_load_weight(p, true);\n\told_prio = p->prio;\n\tp->prio = effective_prio(p);\n\n\tif (queued)\n\t\tenqueue_task(rq, p, ENQUEUE_RESTORE | ENQUEUE_NOCLOCK);\n\tif (running)\n\t\tset_next_task(rq, p);\n\n\t \n\tp->sched_class->prio_changed(rq, p, old_prio);\n\nout_unlock:\n\ttask_rq_unlock(rq, p, &rf);\n}\nEXPORT_SYMBOL(set_user_nice);\n\n \nstatic bool is_nice_reduction(const struct task_struct *p, const int nice)\n{\n\t \n\tint nice_rlim = nice_to_rlimit(nice);\n\n\treturn (nice_rlim <= task_rlimit(p, RLIMIT_NICE));\n}\n\n \nint can_nice(const struct task_struct *p, const int nice)\n{\n\treturn is_nice_reduction(p, nice) || capable(CAP_SYS_NICE);\n}\n\n#ifdef __ARCH_WANT_SYS_NICE\n\n \nSYSCALL_DEFINE1(nice, int, increment)\n{\n\tlong nice, retval;\n\n\t \n\tincrement = clamp(increment, -NICE_WIDTH, NICE_WIDTH);\n\tnice = task_nice(current) + increment;\n\n\tnice = clamp_val(nice, MIN_NICE, MAX_NICE);\n\tif (increment < 0 && !can_nice(current, nice))\n\t\treturn -EPERM;\n\n\tretval = security_task_setnice(current, nice);\n\tif (retval)\n\t\treturn retval;\n\n\tset_user_nice(current, nice);\n\treturn 0;\n}\n\n#endif\n\n \nint task_prio(const struct task_struct *p)\n{\n\treturn p->prio - MAX_RT_PRIO;\n}\n\n \nint idle_cpu(int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\n\tif (rq->curr != rq->idle)\n\t\treturn 0;\n\n\tif (rq->nr_running)\n\t\treturn 0;\n\n#ifdef CONFIG_SMP\n\tif (rq->ttwu_pending)\n\t\treturn 0;\n#endif\n\n\treturn 1;\n}\n\n \nint available_idle_cpu(int cpu)\n{\n\tif (!idle_cpu(cpu))\n\t\treturn 0;\n\n\tif (vcpu_is_preempted(cpu))\n\t\treturn 0;\n\n\treturn 1;\n}\n\n \nstruct task_struct *idle_task(int cpu)\n{\n\treturn cpu_rq(cpu)->idle;\n}\n\n#ifdef CONFIG_SCHED_CORE\nint sched_core_idle_cpu(int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\n\tif (sched_core_enabled(rq) && rq->curr == rq->idle)\n\t\treturn 1;\n\n\treturn idle_cpu(cpu);\n}\n\n#endif\n\n#ifdef CONFIG_SMP\n \nunsigned long effective_cpu_util(int cpu, unsigned long util_cfs,\n\t\t\t\t enum cpu_util_type type,\n\t\t\t\t struct task_struct *p)\n{\n\tunsigned long dl_util, util, irq, max;\n\tstruct rq *rq = cpu_rq(cpu);\n\n\tmax = arch_scale_cpu_capacity(cpu);\n\n\tif (!uclamp_is_used() &&\n\t    type == FREQUENCY_UTIL && rt_rq_is_runnable(&rq->rt)) {\n\t\treturn max;\n\t}\n\n\t \n\tirq = cpu_util_irq(rq);\n\tif (unlikely(irq >= max))\n\t\treturn max;\n\n\t \n\tutil = util_cfs + cpu_util_rt(rq);\n\tif (type == FREQUENCY_UTIL)\n\t\tutil = uclamp_rq_util_with(rq, util, p);\n\n\tdl_util = cpu_util_dl(rq);\n\n\t \n\tif (util + dl_util >= max)\n\t\treturn max;\n\n\t \n\tif (type == ENERGY_UTIL)\n\t\tutil += dl_util;\n\n\t \n\tutil = scale_irq_capacity(util, irq, max);\n\tutil += irq;\n\n\t \n\tif (type == FREQUENCY_UTIL)\n\t\tutil += cpu_bw_dl(rq);\n\n\treturn min(max, util);\n}\n\nunsigned long sched_cpu_util(int cpu)\n{\n\treturn effective_cpu_util(cpu, cpu_util_cfs(cpu), ENERGY_UTIL, NULL);\n}\n#endif  \n\n \nstatic struct task_struct *find_process_by_pid(pid_t pid)\n{\n\treturn pid ? find_task_by_vpid(pid) : current;\n}\n\n \n#define SETPARAM_POLICY\t-1\n\nstatic void __setscheduler_params(struct task_struct *p,\n\t\tconst struct sched_attr *attr)\n{\n\tint policy = attr->sched_policy;\n\n\tif (policy == SETPARAM_POLICY)\n\t\tpolicy = p->policy;\n\n\tp->policy = policy;\n\n\tif (dl_policy(policy))\n\t\t__setparam_dl(p, attr);\n\telse if (fair_policy(policy))\n\t\tp->static_prio = NICE_TO_PRIO(attr->sched_nice);\n\n\t \n\tp->rt_priority = attr->sched_priority;\n\tp->normal_prio = normal_prio(p);\n\tset_load_weight(p, true);\n}\n\n \nstatic bool check_same_owner(struct task_struct *p)\n{\n\tconst struct cred *cred = current_cred(), *pcred;\n\tbool match;\n\n\trcu_read_lock();\n\tpcred = __task_cred(p);\n\tmatch = (uid_eq(cred->euid, pcred->euid) ||\n\t\t uid_eq(cred->euid, pcred->uid));\n\trcu_read_unlock();\n\treturn match;\n}\n\n \nstatic int user_check_sched_setscheduler(struct task_struct *p,\n\t\t\t\t\t const struct sched_attr *attr,\n\t\t\t\t\t int policy, int reset_on_fork)\n{\n\tif (fair_policy(policy)) {\n\t\tif (attr->sched_nice < task_nice(p) &&\n\t\t    !is_nice_reduction(p, attr->sched_nice))\n\t\t\tgoto req_priv;\n\t}\n\n\tif (rt_policy(policy)) {\n\t\tunsigned long rlim_rtprio = task_rlimit(p, RLIMIT_RTPRIO);\n\n\t\t \n\t\tif (policy != p->policy && !rlim_rtprio)\n\t\t\tgoto req_priv;\n\n\t\t \n\t\tif (attr->sched_priority > p->rt_priority &&\n\t\t    attr->sched_priority > rlim_rtprio)\n\t\t\tgoto req_priv;\n\t}\n\n\t \n\tif (dl_policy(policy))\n\t\tgoto req_priv;\n\n\t \n\tif (task_has_idle_policy(p) && !idle_policy(policy)) {\n\t\tif (!is_nice_reduction(p, task_nice(p)))\n\t\t\tgoto req_priv;\n\t}\n\n\t \n\tif (!check_same_owner(p))\n\t\tgoto req_priv;\n\n\t \n\tif (p->sched_reset_on_fork && !reset_on_fork)\n\t\tgoto req_priv;\n\n\treturn 0;\n\nreq_priv:\n\tif (!capable(CAP_SYS_NICE))\n\t\treturn -EPERM;\n\n\treturn 0;\n}\n\nstatic int __sched_setscheduler(struct task_struct *p,\n\t\t\t\tconst struct sched_attr *attr,\n\t\t\t\tbool user, bool pi)\n{\n\tint oldpolicy = -1, policy = attr->sched_policy;\n\tint retval, oldprio, newprio, queued, running;\n\tconst struct sched_class *prev_class;\n\tstruct balance_callback *head;\n\tstruct rq_flags rf;\n\tint reset_on_fork;\n\tint queue_flags = DEQUEUE_SAVE | DEQUEUE_MOVE | DEQUEUE_NOCLOCK;\n\tstruct rq *rq;\n\tbool cpuset_locked = false;\n\n\t \n\tBUG_ON(pi && in_interrupt());\nrecheck:\n\t \n\tif (policy < 0) {\n\t\treset_on_fork = p->sched_reset_on_fork;\n\t\tpolicy = oldpolicy = p->policy;\n\t} else {\n\t\treset_on_fork = !!(attr->sched_flags & SCHED_FLAG_RESET_ON_FORK);\n\n\t\tif (!valid_policy(policy))\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (attr->sched_flags & ~(SCHED_FLAG_ALL | SCHED_FLAG_SUGOV))\n\t\treturn -EINVAL;\n\n\t \n\tif (attr->sched_priority > MAX_RT_PRIO-1)\n\t\treturn -EINVAL;\n\tif ((dl_policy(policy) && !__checkparam_dl(attr)) ||\n\t    (rt_policy(policy) != (attr->sched_priority != 0)))\n\t\treturn -EINVAL;\n\n\tif (user) {\n\t\tretval = user_check_sched_setscheduler(p, attr, policy, reset_on_fork);\n\t\tif (retval)\n\t\t\treturn retval;\n\n\t\tif (attr->sched_flags & SCHED_FLAG_SUGOV)\n\t\t\treturn -EINVAL;\n\n\t\tretval = security_task_setscheduler(p);\n\t\tif (retval)\n\t\t\treturn retval;\n\t}\n\n\t \n\tif (attr->sched_flags & SCHED_FLAG_UTIL_CLAMP) {\n\t\tretval = uclamp_validate(p, attr);\n\t\tif (retval)\n\t\t\treturn retval;\n\t}\n\n\t \n\tif (dl_policy(policy) || dl_policy(p->policy)) {\n\t\tcpuset_locked = true;\n\t\tcpuset_lock();\n\t}\n\n\t \n\trq = task_rq_lock(p, &rf);\n\tupdate_rq_clock(rq);\n\n\t \n\tif (p == rq->stop) {\n\t\tretval = -EINVAL;\n\t\tgoto unlock;\n\t}\n\n\t \n\tif (unlikely(policy == p->policy)) {\n\t\tif (fair_policy(policy) && attr->sched_nice != task_nice(p))\n\t\t\tgoto change;\n\t\tif (rt_policy(policy) && attr->sched_priority != p->rt_priority)\n\t\t\tgoto change;\n\t\tif (dl_policy(policy) && dl_param_changed(p, attr))\n\t\t\tgoto change;\n\t\tif (attr->sched_flags & SCHED_FLAG_UTIL_CLAMP)\n\t\t\tgoto change;\n\n\t\tp->sched_reset_on_fork = reset_on_fork;\n\t\tretval = 0;\n\t\tgoto unlock;\n\t}\nchange:\n\n\tif (user) {\n#ifdef CONFIG_RT_GROUP_SCHED\n\t\t \n\t\tif (rt_bandwidth_enabled() && rt_policy(policy) &&\n\t\t\t\ttask_group(p)->rt_bandwidth.rt_runtime == 0 &&\n\t\t\t\t!task_group_is_autogroup(task_group(p))) {\n\t\t\tretval = -EPERM;\n\t\t\tgoto unlock;\n\t\t}\n#endif\n#ifdef CONFIG_SMP\n\t\tif (dl_bandwidth_enabled() && dl_policy(policy) &&\n\t\t\t\t!(attr->sched_flags & SCHED_FLAG_SUGOV)) {\n\t\t\tcpumask_t *span = rq->rd->span;\n\n\t\t\t \n\t\t\tif (!cpumask_subset(span, p->cpus_ptr) ||\n\t\t\t    rq->rd->dl_bw.bw == 0) {\n\t\t\t\tretval = -EPERM;\n\t\t\t\tgoto unlock;\n\t\t\t}\n\t\t}\n#endif\n\t}\n\n\t \n\tif (unlikely(oldpolicy != -1 && oldpolicy != p->policy)) {\n\t\tpolicy = oldpolicy = -1;\n\t\ttask_rq_unlock(rq, p, &rf);\n\t\tif (cpuset_locked)\n\t\t\tcpuset_unlock();\n\t\tgoto recheck;\n\t}\n\n\t \n\tif ((dl_policy(policy) || dl_task(p)) && sched_dl_overflow(p, policy, attr)) {\n\t\tretval = -EBUSY;\n\t\tgoto unlock;\n\t}\n\n\tp->sched_reset_on_fork = reset_on_fork;\n\toldprio = p->prio;\n\n\tnewprio = __normal_prio(policy, attr->sched_priority, attr->sched_nice);\n\tif (pi) {\n\t\t \n\t\tnewprio = rt_effective_prio(p, newprio);\n\t\tif (newprio == oldprio)\n\t\t\tqueue_flags &= ~DEQUEUE_MOVE;\n\t}\n\n\tqueued = task_on_rq_queued(p);\n\trunning = task_current(rq, p);\n\tif (queued)\n\t\tdequeue_task(rq, p, queue_flags);\n\tif (running)\n\t\tput_prev_task(rq, p);\n\n\tprev_class = p->sched_class;\n\n\tif (!(attr->sched_flags & SCHED_FLAG_KEEP_PARAMS)) {\n\t\t__setscheduler_params(p, attr);\n\t\t__setscheduler_prio(p, newprio);\n\t}\n\t__setscheduler_uclamp(p, attr);\n\n\tif (queued) {\n\t\t \n\t\tif (oldprio < p->prio)\n\t\t\tqueue_flags |= ENQUEUE_HEAD;\n\n\t\tenqueue_task(rq, p, queue_flags);\n\t}\n\tif (running)\n\t\tset_next_task(rq, p);\n\n\tcheck_class_changed(rq, p, prev_class, oldprio);\n\n\t \n\tpreempt_disable();\n\thead = splice_balance_callbacks(rq);\n\ttask_rq_unlock(rq, p, &rf);\n\n\tif (pi) {\n\t\tif (cpuset_locked)\n\t\t\tcpuset_unlock();\n\t\trt_mutex_adjust_pi(p);\n\t}\n\n\t \n\tbalance_callbacks(rq, head);\n\tpreempt_enable();\n\n\treturn 0;\n\nunlock:\n\ttask_rq_unlock(rq, p, &rf);\n\tif (cpuset_locked)\n\t\tcpuset_unlock();\n\treturn retval;\n}\n\nstatic int _sched_setscheduler(struct task_struct *p, int policy,\n\t\t\t       const struct sched_param *param, bool check)\n{\n\tstruct sched_attr attr = {\n\t\t.sched_policy   = policy,\n\t\t.sched_priority = param->sched_priority,\n\t\t.sched_nice\t= PRIO_TO_NICE(p->static_prio),\n\t};\n\n\t \n\tif ((policy != SETPARAM_POLICY) && (policy & SCHED_RESET_ON_FORK)) {\n\t\tattr.sched_flags |= SCHED_FLAG_RESET_ON_FORK;\n\t\tpolicy &= ~SCHED_RESET_ON_FORK;\n\t\tattr.sched_policy = policy;\n\t}\n\n\treturn __sched_setscheduler(p, &attr, check, true);\n}\n \nint sched_setscheduler(struct task_struct *p, int policy,\n\t\t       const struct sched_param *param)\n{\n\treturn _sched_setscheduler(p, policy, param, true);\n}\n\nint sched_setattr(struct task_struct *p, const struct sched_attr *attr)\n{\n\treturn __sched_setscheduler(p, attr, true, true);\n}\n\nint sched_setattr_nocheck(struct task_struct *p, const struct sched_attr *attr)\n{\n\treturn __sched_setscheduler(p, attr, false, true);\n}\nEXPORT_SYMBOL_GPL(sched_setattr_nocheck);\n\n \nint sched_setscheduler_nocheck(struct task_struct *p, int policy,\n\t\t\t       const struct sched_param *param)\n{\n\treturn _sched_setscheduler(p, policy, param, false);\n}\n\n \nvoid sched_set_fifo(struct task_struct *p)\n{\n\tstruct sched_param sp = { .sched_priority = MAX_RT_PRIO / 2 };\n\tWARN_ON_ONCE(sched_setscheduler_nocheck(p, SCHED_FIFO, &sp) != 0);\n}\nEXPORT_SYMBOL_GPL(sched_set_fifo);\n\n \nvoid sched_set_fifo_low(struct task_struct *p)\n{\n\tstruct sched_param sp = { .sched_priority = 1 };\n\tWARN_ON_ONCE(sched_setscheduler_nocheck(p, SCHED_FIFO, &sp) != 0);\n}\nEXPORT_SYMBOL_GPL(sched_set_fifo_low);\n\nvoid sched_set_normal(struct task_struct *p, int nice)\n{\n\tstruct sched_attr attr = {\n\t\t.sched_policy = SCHED_NORMAL,\n\t\t.sched_nice = nice,\n\t};\n\tWARN_ON_ONCE(sched_setattr_nocheck(p, &attr) != 0);\n}\nEXPORT_SYMBOL_GPL(sched_set_normal);\n\nstatic int\ndo_sched_setscheduler(pid_t pid, int policy, struct sched_param __user *param)\n{\n\tstruct sched_param lparam;\n\tstruct task_struct *p;\n\tint retval;\n\n\tif (!param || pid < 0)\n\t\treturn -EINVAL;\n\tif (copy_from_user(&lparam, param, sizeof(struct sched_param)))\n\t\treturn -EFAULT;\n\n\trcu_read_lock();\n\tretval = -ESRCH;\n\tp = find_process_by_pid(pid);\n\tif (likely(p))\n\t\tget_task_struct(p);\n\trcu_read_unlock();\n\n\tif (likely(p)) {\n\t\tretval = sched_setscheduler(p, policy, &lparam);\n\t\tput_task_struct(p);\n\t}\n\n\treturn retval;\n}\n\n \nstatic int sched_copy_attr(struct sched_attr __user *uattr, struct sched_attr *attr)\n{\n\tu32 size;\n\tint ret;\n\n\t \n\tmemset(attr, 0, sizeof(*attr));\n\n\tret = get_user(size, &uattr->size);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tif (!size)\n\t\tsize = SCHED_ATTR_SIZE_VER0;\n\tif (size < SCHED_ATTR_SIZE_VER0 || size > PAGE_SIZE)\n\t\tgoto err_size;\n\n\tret = copy_struct_from_user(attr, sizeof(*attr), uattr, size);\n\tif (ret) {\n\t\tif (ret == -E2BIG)\n\t\t\tgoto err_size;\n\t\treturn ret;\n\t}\n\n\tif ((attr->sched_flags & SCHED_FLAG_UTIL_CLAMP) &&\n\t    size < SCHED_ATTR_SIZE_VER1)\n\t\treturn -EINVAL;\n\n\t \n\tattr->sched_nice = clamp(attr->sched_nice, MIN_NICE, MAX_NICE);\n\n\treturn 0;\n\nerr_size:\n\tput_user(sizeof(*attr), &uattr->size);\n\treturn -E2BIG;\n}\n\nstatic void get_params(struct task_struct *p, struct sched_attr *attr)\n{\n\tif (task_has_dl_policy(p))\n\t\t__getparam_dl(p, attr);\n\telse if (task_has_rt_policy(p))\n\t\tattr->sched_priority = p->rt_priority;\n\telse\n\t\tattr->sched_nice = task_nice(p);\n}\n\n \nSYSCALL_DEFINE3(sched_setscheduler, pid_t, pid, int, policy, struct sched_param __user *, param)\n{\n\tif (policy < 0)\n\t\treturn -EINVAL;\n\n\treturn do_sched_setscheduler(pid, policy, param);\n}\n\n \nSYSCALL_DEFINE2(sched_setparam, pid_t, pid, struct sched_param __user *, param)\n{\n\treturn do_sched_setscheduler(pid, SETPARAM_POLICY, param);\n}\n\n \nSYSCALL_DEFINE3(sched_setattr, pid_t, pid, struct sched_attr __user *, uattr,\n\t\t\t       unsigned int, flags)\n{\n\tstruct sched_attr attr;\n\tstruct task_struct *p;\n\tint retval;\n\n\tif (!uattr || pid < 0 || flags)\n\t\treturn -EINVAL;\n\n\tretval = sched_copy_attr(uattr, &attr);\n\tif (retval)\n\t\treturn retval;\n\n\tif ((int)attr.sched_policy < 0)\n\t\treturn -EINVAL;\n\tif (attr.sched_flags & SCHED_FLAG_KEEP_POLICY)\n\t\tattr.sched_policy = SETPARAM_POLICY;\n\n\trcu_read_lock();\n\tretval = -ESRCH;\n\tp = find_process_by_pid(pid);\n\tif (likely(p))\n\t\tget_task_struct(p);\n\trcu_read_unlock();\n\n\tif (likely(p)) {\n\t\tif (attr.sched_flags & SCHED_FLAG_KEEP_PARAMS)\n\t\t\tget_params(p, &attr);\n\t\tretval = sched_setattr(p, &attr);\n\t\tput_task_struct(p);\n\t}\n\n\treturn retval;\n}\n\n \nSYSCALL_DEFINE1(sched_getscheduler, pid_t, pid)\n{\n\tstruct task_struct *p;\n\tint retval;\n\n\tif (pid < 0)\n\t\treturn -EINVAL;\n\n\tretval = -ESRCH;\n\trcu_read_lock();\n\tp = find_process_by_pid(pid);\n\tif (p) {\n\t\tretval = security_task_getscheduler(p);\n\t\tif (!retval)\n\t\t\tretval = p->policy\n\t\t\t\t| (p->sched_reset_on_fork ? SCHED_RESET_ON_FORK : 0);\n\t}\n\trcu_read_unlock();\n\treturn retval;\n}\n\n \nSYSCALL_DEFINE2(sched_getparam, pid_t, pid, struct sched_param __user *, param)\n{\n\tstruct sched_param lp = { .sched_priority = 0 };\n\tstruct task_struct *p;\n\tint retval;\n\n\tif (!param || pid < 0)\n\t\treturn -EINVAL;\n\n\trcu_read_lock();\n\tp = find_process_by_pid(pid);\n\tretval = -ESRCH;\n\tif (!p)\n\t\tgoto out_unlock;\n\n\tretval = security_task_getscheduler(p);\n\tif (retval)\n\t\tgoto out_unlock;\n\n\tif (task_has_rt_policy(p))\n\t\tlp.sched_priority = p->rt_priority;\n\trcu_read_unlock();\n\n\t \n\tretval = copy_to_user(param, &lp, sizeof(*param)) ? -EFAULT : 0;\n\n\treturn retval;\n\nout_unlock:\n\trcu_read_unlock();\n\treturn retval;\n}\n\n \nstatic int\nsched_attr_copy_to_user(struct sched_attr __user *uattr,\n\t\t\tstruct sched_attr *kattr,\n\t\t\tunsigned int usize)\n{\n\tunsigned int ksize = sizeof(*kattr);\n\n\tif (!access_ok(uattr, usize))\n\t\treturn -EFAULT;\n\n\t \n\tkattr->size = min(usize, ksize);\n\n\tif (copy_to_user(uattr, kattr, kattr->size))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\n \nSYSCALL_DEFINE4(sched_getattr, pid_t, pid, struct sched_attr __user *, uattr,\n\t\tunsigned int, usize, unsigned int, flags)\n{\n\tstruct sched_attr kattr = { };\n\tstruct task_struct *p;\n\tint retval;\n\n\tif (!uattr || pid < 0 || usize > PAGE_SIZE ||\n\t    usize < SCHED_ATTR_SIZE_VER0 || flags)\n\t\treturn -EINVAL;\n\n\trcu_read_lock();\n\tp = find_process_by_pid(pid);\n\tretval = -ESRCH;\n\tif (!p)\n\t\tgoto out_unlock;\n\n\tretval = security_task_getscheduler(p);\n\tif (retval)\n\t\tgoto out_unlock;\n\n\tkattr.sched_policy = p->policy;\n\tif (p->sched_reset_on_fork)\n\t\tkattr.sched_flags |= SCHED_FLAG_RESET_ON_FORK;\n\tget_params(p, &kattr);\n\tkattr.sched_flags &= SCHED_FLAG_ALL;\n\n#ifdef CONFIG_UCLAMP_TASK\n\t \n\tkattr.sched_util_min = p->uclamp_req[UCLAMP_MIN].value;\n\tkattr.sched_util_max = p->uclamp_req[UCLAMP_MAX].value;\n#endif\n\n\trcu_read_unlock();\n\n\treturn sched_attr_copy_to_user(uattr, &kattr, usize);\n\nout_unlock:\n\trcu_read_unlock();\n\treturn retval;\n}\n\n#ifdef CONFIG_SMP\nint dl_task_check_affinity(struct task_struct *p, const struct cpumask *mask)\n{\n\tint ret = 0;\n\n\t \n\tif (!task_has_dl_policy(p) || !dl_bandwidth_enabled())\n\t\treturn 0;\n\n\t \n\trcu_read_lock();\n\tif (!cpumask_subset(task_rq(p)->rd->span, mask))\n\t\tret = -EBUSY;\n\trcu_read_unlock();\n\treturn ret;\n}\n#endif\n\nstatic int\n__sched_setaffinity(struct task_struct *p, struct affinity_context *ctx)\n{\n\tint retval;\n\tcpumask_var_t cpus_allowed, new_mask;\n\n\tif (!alloc_cpumask_var(&cpus_allowed, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\tif (!alloc_cpumask_var(&new_mask, GFP_KERNEL)) {\n\t\tretval = -ENOMEM;\n\t\tgoto out_free_cpus_allowed;\n\t}\n\n\tcpuset_cpus_allowed(p, cpus_allowed);\n\tcpumask_and(new_mask, ctx->new_mask, cpus_allowed);\n\n\tctx->new_mask = new_mask;\n\tctx->flags |= SCA_CHECK;\n\n\tretval = dl_task_check_affinity(p, new_mask);\n\tif (retval)\n\t\tgoto out_free_new_mask;\n\n\tretval = __set_cpus_allowed_ptr(p, ctx);\n\tif (retval)\n\t\tgoto out_free_new_mask;\n\n\tcpuset_cpus_allowed(p, cpus_allowed);\n\tif (!cpumask_subset(new_mask, cpus_allowed)) {\n\t\t \n\t\tcpumask_copy(new_mask, cpus_allowed);\n\n\t\t \n\t\tif (unlikely((ctx->flags & SCA_USER) && ctx->user_mask)) {\n\t\t\tbool empty = !cpumask_and(new_mask, new_mask,\n\t\t\t\t\t\t  ctx->user_mask);\n\n\t\t\tif (WARN_ON_ONCE(empty))\n\t\t\t\tcpumask_copy(new_mask, cpus_allowed);\n\t\t}\n\t\t__set_cpus_allowed_ptr(p, ctx);\n\t\tretval = -EINVAL;\n\t}\n\nout_free_new_mask:\n\tfree_cpumask_var(new_mask);\nout_free_cpus_allowed:\n\tfree_cpumask_var(cpus_allowed);\n\treturn retval;\n}\n\nlong sched_setaffinity(pid_t pid, const struct cpumask *in_mask)\n{\n\tstruct affinity_context ac;\n\tstruct cpumask *user_mask;\n\tstruct task_struct *p;\n\tint retval;\n\n\trcu_read_lock();\n\n\tp = find_process_by_pid(pid);\n\tif (!p) {\n\t\trcu_read_unlock();\n\t\treturn -ESRCH;\n\t}\n\n\t \n\tget_task_struct(p);\n\trcu_read_unlock();\n\n\tif (p->flags & PF_NO_SETAFFINITY) {\n\t\tretval = -EINVAL;\n\t\tgoto out_put_task;\n\t}\n\n\tif (!check_same_owner(p)) {\n\t\trcu_read_lock();\n\t\tif (!ns_capable(__task_cred(p)->user_ns, CAP_SYS_NICE)) {\n\t\t\trcu_read_unlock();\n\t\t\tretval = -EPERM;\n\t\t\tgoto out_put_task;\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\n\tretval = security_task_setscheduler(p);\n\tif (retval)\n\t\tgoto out_put_task;\n\n\t \n\tuser_mask = alloc_user_cpus_ptr(NUMA_NO_NODE);\n\tif (user_mask) {\n\t\tcpumask_copy(user_mask, in_mask);\n\t} else if (IS_ENABLED(CONFIG_SMP)) {\n\t\tretval = -ENOMEM;\n\t\tgoto out_put_task;\n\t}\n\n\tac = (struct affinity_context){\n\t\t.new_mask  = in_mask,\n\t\t.user_mask = user_mask,\n\t\t.flags     = SCA_USER,\n\t};\n\n\tretval = __sched_setaffinity(p, &ac);\n\tkfree(ac.user_mask);\n\nout_put_task:\n\tput_task_struct(p);\n\treturn retval;\n}\n\nstatic int get_user_cpu_mask(unsigned long __user *user_mask_ptr, unsigned len,\n\t\t\t     struct cpumask *new_mask)\n{\n\tif (len < cpumask_size())\n\t\tcpumask_clear(new_mask);\n\telse if (len > cpumask_size())\n\t\tlen = cpumask_size();\n\n\treturn copy_from_user(new_mask, user_mask_ptr, len) ? -EFAULT : 0;\n}\n\n \nSYSCALL_DEFINE3(sched_setaffinity, pid_t, pid, unsigned int, len,\n\t\tunsigned long __user *, user_mask_ptr)\n{\n\tcpumask_var_t new_mask;\n\tint retval;\n\n\tif (!alloc_cpumask_var(&new_mask, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\tretval = get_user_cpu_mask(user_mask_ptr, len, new_mask);\n\tif (retval == 0)\n\t\tretval = sched_setaffinity(pid, new_mask);\n\tfree_cpumask_var(new_mask);\n\treturn retval;\n}\n\nlong sched_getaffinity(pid_t pid, struct cpumask *mask)\n{\n\tstruct task_struct *p;\n\tunsigned long flags;\n\tint retval;\n\n\trcu_read_lock();\n\n\tretval = -ESRCH;\n\tp = find_process_by_pid(pid);\n\tif (!p)\n\t\tgoto out_unlock;\n\n\tretval = security_task_getscheduler(p);\n\tif (retval)\n\t\tgoto out_unlock;\n\n\traw_spin_lock_irqsave(&p->pi_lock, flags);\n\tcpumask_and(mask, &p->cpus_mask, cpu_active_mask);\n\traw_spin_unlock_irqrestore(&p->pi_lock, flags);\n\nout_unlock:\n\trcu_read_unlock();\n\n\treturn retval;\n}\n\n \nSYSCALL_DEFINE3(sched_getaffinity, pid_t, pid, unsigned int, len,\n\t\tunsigned long __user *, user_mask_ptr)\n{\n\tint ret;\n\tcpumask_var_t mask;\n\n\tif ((len * BITS_PER_BYTE) < nr_cpu_ids)\n\t\treturn -EINVAL;\n\tif (len & (sizeof(unsigned long)-1))\n\t\treturn -EINVAL;\n\n\tif (!zalloc_cpumask_var(&mask, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\tret = sched_getaffinity(pid, mask);\n\tif (ret == 0) {\n\t\tunsigned int retlen = min(len, cpumask_size());\n\n\t\tif (copy_to_user(user_mask_ptr, cpumask_bits(mask), retlen))\n\t\t\tret = -EFAULT;\n\t\telse\n\t\t\tret = retlen;\n\t}\n\tfree_cpumask_var(mask);\n\n\treturn ret;\n}\n\nstatic void do_sched_yield(void)\n{\n\tstruct rq_flags rf;\n\tstruct rq *rq;\n\n\trq = this_rq_lock_irq(&rf);\n\n\tschedstat_inc(rq->yld_count);\n\tcurrent->sched_class->yield_task(rq);\n\n\tpreempt_disable();\n\trq_unlock_irq(rq, &rf);\n\tsched_preempt_enable_no_resched();\n\n\tschedule();\n}\n\n \nSYSCALL_DEFINE0(sched_yield)\n{\n\tdo_sched_yield();\n\treturn 0;\n}\n\n#if !defined(CONFIG_PREEMPTION) || defined(CONFIG_PREEMPT_DYNAMIC)\nint __sched __cond_resched(void)\n{\n\tif (should_resched(0)) {\n\t\tpreempt_schedule_common();\n\t\treturn 1;\n\t}\n\t \n#ifndef CONFIG_PREEMPT_RCU\n\trcu_all_qs();\n#endif\n\treturn 0;\n}\nEXPORT_SYMBOL(__cond_resched);\n#endif\n\n#ifdef CONFIG_PREEMPT_DYNAMIC\n#if defined(CONFIG_HAVE_PREEMPT_DYNAMIC_CALL)\n#define cond_resched_dynamic_enabled\t__cond_resched\n#define cond_resched_dynamic_disabled\t((void *)&__static_call_return0)\nDEFINE_STATIC_CALL_RET0(cond_resched, __cond_resched);\nEXPORT_STATIC_CALL_TRAMP(cond_resched);\n\n#define might_resched_dynamic_enabled\t__cond_resched\n#define might_resched_dynamic_disabled\t((void *)&__static_call_return0)\nDEFINE_STATIC_CALL_RET0(might_resched, __cond_resched);\nEXPORT_STATIC_CALL_TRAMP(might_resched);\n#elif defined(CONFIG_HAVE_PREEMPT_DYNAMIC_KEY)\nstatic DEFINE_STATIC_KEY_FALSE(sk_dynamic_cond_resched);\nint __sched dynamic_cond_resched(void)\n{\n\tklp_sched_try_switch();\n\tif (!static_branch_unlikely(&sk_dynamic_cond_resched))\n\t\treturn 0;\n\treturn __cond_resched();\n}\nEXPORT_SYMBOL(dynamic_cond_resched);\n\nstatic DEFINE_STATIC_KEY_FALSE(sk_dynamic_might_resched);\nint __sched dynamic_might_resched(void)\n{\n\tif (!static_branch_unlikely(&sk_dynamic_might_resched))\n\t\treturn 0;\n\treturn __cond_resched();\n}\nEXPORT_SYMBOL(dynamic_might_resched);\n#endif\n#endif\n\n \nint __cond_resched_lock(spinlock_t *lock)\n{\n\tint resched = should_resched(PREEMPT_LOCK_OFFSET);\n\tint ret = 0;\n\n\tlockdep_assert_held(lock);\n\n\tif (spin_needbreak(lock) || resched) {\n\t\tspin_unlock(lock);\n\t\tif (!_cond_resched())\n\t\t\tcpu_relax();\n\t\tret = 1;\n\t\tspin_lock(lock);\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL(__cond_resched_lock);\n\nint __cond_resched_rwlock_read(rwlock_t *lock)\n{\n\tint resched = should_resched(PREEMPT_LOCK_OFFSET);\n\tint ret = 0;\n\n\tlockdep_assert_held_read(lock);\n\n\tif (rwlock_needbreak(lock) || resched) {\n\t\tread_unlock(lock);\n\t\tif (!_cond_resched())\n\t\t\tcpu_relax();\n\t\tret = 1;\n\t\tread_lock(lock);\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL(__cond_resched_rwlock_read);\n\nint __cond_resched_rwlock_write(rwlock_t *lock)\n{\n\tint resched = should_resched(PREEMPT_LOCK_OFFSET);\n\tint ret = 0;\n\n\tlockdep_assert_held_write(lock);\n\n\tif (rwlock_needbreak(lock) || resched) {\n\t\twrite_unlock(lock);\n\t\tif (!_cond_resched())\n\t\t\tcpu_relax();\n\t\tret = 1;\n\t\twrite_lock(lock);\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL(__cond_resched_rwlock_write);\n\n#ifdef CONFIG_PREEMPT_DYNAMIC\n\n#ifdef CONFIG_GENERIC_ENTRY\n#include <linux/entry-common.h>\n#endif\n\n \n\nenum {\n\tpreempt_dynamic_undefined = -1,\n\tpreempt_dynamic_none,\n\tpreempt_dynamic_voluntary,\n\tpreempt_dynamic_full,\n};\n\nint preempt_dynamic_mode = preempt_dynamic_undefined;\n\nint sched_dynamic_mode(const char *str)\n{\n\tif (!strcmp(str, \"none\"))\n\t\treturn preempt_dynamic_none;\n\n\tif (!strcmp(str, \"voluntary\"))\n\t\treturn preempt_dynamic_voluntary;\n\n\tif (!strcmp(str, \"full\"))\n\t\treturn preempt_dynamic_full;\n\n\treturn -EINVAL;\n}\n\n#if defined(CONFIG_HAVE_PREEMPT_DYNAMIC_CALL)\n#define preempt_dynamic_enable(f)\tstatic_call_update(f, f##_dynamic_enabled)\n#define preempt_dynamic_disable(f)\tstatic_call_update(f, f##_dynamic_disabled)\n#elif defined(CONFIG_HAVE_PREEMPT_DYNAMIC_KEY)\n#define preempt_dynamic_enable(f)\tstatic_key_enable(&sk_dynamic_##f.key)\n#define preempt_dynamic_disable(f)\tstatic_key_disable(&sk_dynamic_##f.key)\n#else\n#error \"Unsupported PREEMPT_DYNAMIC mechanism\"\n#endif\n\nstatic DEFINE_MUTEX(sched_dynamic_mutex);\nstatic bool klp_override;\n\nstatic void __sched_dynamic_update(int mode)\n{\n\t \n\tif (!klp_override)\n\t\tpreempt_dynamic_enable(cond_resched);\n\tpreempt_dynamic_enable(might_resched);\n\tpreempt_dynamic_enable(preempt_schedule);\n\tpreempt_dynamic_enable(preempt_schedule_notrace);\n\tpreempt_dynamic_enable(irqentry_exit_cond_resched);\n\n\tswitch (mode) {\n\tcase preempt_dynamic_none:\n\t\tif (!klp_override)\n\t\t\tpreempt_dynamic_enable(cond_resched);\n\t\tpreempt_dynamic_disable(might_resched);\n\t\tpreempt_dynamic_disable(preempt_schedule);\n\t\tpreempt_dynamic_disable(preempt_schedule_notrace);\n\t\tpreempt_dynamic_disable(irqentry_exit_cond_resched);\n\t\tif (mode != preempt_dynamic_mode)\n\t\t\tpr_info(\"Dynamic Preempt: none\\n\");\n\t\tbreak;\n\n\tcase preempt_dynamic_voluntary:\n\t\tif (!klp_override)\n\t\t\tpreempt_dynamic_enable(cond_resched);\n\t\tpreempt_dynamic_enable(might_resched);\n\t\tpreempt_dynamic_disable(preempt_schedule);\n\t\tpreempt_dynamic_disable(preempt_schedule_notrace);\n\t\tpreempt_dynamic_disable(irqentry_exit_cond_resched);\n\t\tif (mode != preempt_dynamic_mode)\n\t\t\tpr_info(\"Dynamic Preempt: voluntary\\n\");\n\t\tbreak;\n\n\tcase preempt_dynamic_full:\n\t\tif (!klp_override)\n\t\t\tpreempt_dynamic_disable(cond_resched);\n\t\tpreempt_dynamic_disable(might_resched);\n\t\tpreempt_dynamic_enable(preempt_schedule);\n\t\tpreempt_dynamic_enable(preempt_schedule_notrace);\n\t\tpreempt_dynamic_enable(irqentry_exit_cond_resched);\n\t\tif (mode != preempt_dynamic_mode)\n\t\t\tpr_info(\"Dynamic Preempt: full\\n\");\n\t\tbreak;\n\t}\n\n\tpreempt_dynamic_mode = mode;\n}\n\nvoid sched_dynamic_update(int mode)\n{\n\tmutex_lock(&sched_dynamic_mutex);\n\t__sched_dynamic_update(mode);\n\tmutex_unlock(&sched_dynamic_mutex);\n}\n\n#ifdef CONFIG_HAVE_PREEMPT_DYNAMIC_CALL\n\nstatic int klp_cond_resched(void)\n{\n\t__klp_sched_try_switch();\n\treturn __cond_resched();\n}\n\nvoid sched_dynamic_klp_enable(void)\n{\n\tmutex_lock(&sched_dynamic_mutex);\n\n\tklp_override = true;\n\tstatic_call_update(cond_resched, klp_cond_resched);\n\n\tmutex_unlock(&sched_dynamic_mutex);\n}\n\nvoid sched_dynamic_klp_disable(void)\n{\n\tmutex_lock(&sched_dynamic_mutex);\n\n\tklp_override = false;\n\t__sched_dynamic_update(preempt_dynamic_mode);\n\n\tmutex_unlock(&sched_dynamic_mutex);\n}\n\n#endif  \n\nstatic int __init setup_preempt_mode(char *str)\n{\n\tint mode = sched_dynamic_mode(str);\n\tif (mode < 0) {\n\t\tpr_warn(\"Dynamic Preempt: unsupported mode: %s\\n\", str);\n\t\treturn 0;\n\t}\n\n\tsched_dynamic_update(mode);\n\treturn 1;\n}\n__setup(\"preempt=\", setup_preempt_mode);\n\nstatic void __init preempt_dynamic_init(void)\n{\n\tif (preempt_dynamic_mode == preempt_dynamic_undefined) {\n\t\tif (IS_ENABLED(CONFIG_PREEMPT_NONE)) {\n\t\t\tsched_dynamic_update(preempt_dynamic_none);\n\t\t} else if (IS_ENABLED(CONFIG_PREEMPT_VOLUNTARY)) {\n\t\t\tsched_dynamic_update(preempt_dynamic_voluntary);\n\t\t} else {\n\t\t\t \n\t\t\tWARN_ON_ONCE(!IS_ENABLED(CONFIG_PREEMPT));\n\t\t\tpreempt_dynamic_mode = preempt_dynamic_full;\n\t\t\tpr_info(\"Dynamic Preempt: full\\n\");\n\t\t}\n\t}\n}\n\n#define PREEMPT_MODEL_ACCESSOR(mode) \\\n\tbool preempt_model_##mode(void)\t\t\t\t\t\t \\\n\t{\t\t\t\t\t\t\t\t\t \\\n\t\tWARN_ON_ONCE(preempt_dynamic_mode == preempt_dynamic_undefined); \\\n\t\treturn preempt_dynamic_mode == preempt_dynamic_##mode;\t\t \\\n\t}\t\t\t\t\t\t\t\t\t \\\n\tEXPORT_SYMBOL_GPL(preempt_model_##mode)\n\nPREEMPT_MODEL_ACCESSOR(none);\nPREEMPT_MODEL_ACCESSOR(voluntary);\nPREEMPT_MODEL_ACCESSOR(full);\n\n#else  \n\nstatic inline void preempt_dynamic_init(void) { }\n\n#endif  \n\n \nvoid __sched yield(void)\n{\n\tset_current_state(TASK_RUNNING);\n\tdo_sched_yield();\n}\nEXPORT_SYMBOL(yield);\n\n \nint __sched yield_to(struct task_struct *p, bool preempt)\n{\n\tstruct task_struct *curr = current;\n\tstruct rq *rq, *p_rq;\n\tunsigned long flags;\n\tint yielded = 0;\n\n\tlocal_irq_save(flags);\n\trq = this_rq();\n\nagain:\n\tp_rq = task_rq(p);\n\t \n\tif (rq->nr_running == 1 && p_rq->nr_running == 1) {\n\t\tyielded = -ESRCH;\n\t\tgoto out_irq;\n\t}\n\n\tdouble_rq_lock(rq, p_rq);\n\tif (task_rq(p) != p_rq) {\n\t\tdouble_rq_unlock(rq, p_rq);\n\t\tgoto again;\n\t}\n\n\tif (!curr->sched_class->yield_to_task)\n\t\tgoto out_unlock;\n\n\tif (curr->sched_class != p->sched_class)\n\t\tgoto out_unlock;\n\n\tif (task_on_cpu(p_rq, p) || !task_is_running(p))\n\t\tgoto out_unlock;\n\n\tyielded = curr->sched_class->yield_to_task(rq, p);\n\tif (yielded) {\n\t\tschedstat_inc(rq->yld_count);\n\t\t \n\t\tif (preempt && rq != p_rq)\n\t\t\tresched_curr(p_rq);\n\t}\n\nout_unlock:\n\tdouble_rq_unlock(rq, p_rq);\nout_irq:\n\tlocal_irq_restore(flags);\n\n\tif (yielded > 0)\n\t\tschedule();\n\n\treturn yielded;\n}\nEXPORT_SYMBOL_GPL(yield_to);\n\nint io_schedule_prepare(void)\n{\n\tint old_iowait = current->in_iowait;\n\n\tcurrent->in_iowait = 1;\n\tblk_flush_plug(current->plug, true);\n\treturn old_iowait;\n}\n\nvoid io_schedule_finish(int token)\n{\n\tcurrent->in_iowait = token;\n}\n\n \nlong __sched io_schedule_timeout(long timeout)\n{\n\tint token;\n\tlong ret;\n\n\ttoken = io_schedule_prepare();\n\tret = schedule_timeout(timeout);\n\tio_schedule_finish(token);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(io_schedule_timeout);\n\nvoid __sched io_schedule(void)\n{\n\tint token;\n\n\ttoken = io_schedule_prepare();\n\tschedule();\n\tio_schedule_finish(token);\n}\nEXPORT_SYMBOL(io_schedule);\n\n \nSYSCALL_DEFINE1(sched_get_priority_max, int, policy)\n{\n\tint ret = -EINVAL;\n\n\tswitch (policy) {\n\tcase SCHED_FIFO:\n\tcase SCHED_RR:\n\t\tret = MAX_RT_PRIO-1;\n\t\tbreak;\n\tcase SCHED_DEADLINE:\n\tcase SCHED_NORMAL:\n\tcase SCHED_BATCH:\n\tcase SCHED_IDLE:\n\t\tret = 0;\n\t\tbreak;\n\t}\n\treturn ret;\n}\n\n \nSYSCALL_DEFINE1(sched_get_priority_min, int, policy)\n{\n\tint ret = -EINVAL;\n\n\tswitch (policy) {\n\tcase SCHED_FIFO:\n\tcase SCHED_RR:\n\t\tret = 1;\n\t\tbreak;\n\tcase SCHED_DEADLINE:\n\tcase SCHED_NORMAL:\n\tcase SCHED_BATCH:\n\tcase SCHED_IDLE:\n\t\tret = 0;\n\t}\n\treturn ret;\n}\n\nstatic int sched_rr_get_interval(pid_t pid, struct timespec64 *t)\n{\n\tstruct task_struct *p;\n\tunsigned int time_slice;\n\tstruct rq_flags rf;\n\tstruct rq *rq;\n\tint retval;\n\n\tif (pid < 0)\n\t\treturn -EINVAL;\n\n\tretval = -ESRCH;\n\trcu_read_lock();\n\tp = find_process_by_pid(pid);\n\tif (!p)\n\t\tgoto out_unlock;\n\n\tretval = security_task_getscheduler(p);\n\tif (retval)\n\t\tgoto out_unlock;\n\n\trq = task_rq_lock(p, &rf);\n\ttime_slice = 0;\n\tif (p->sched_class->get_rr_interval)\n\t\ttime_slice = p->sched_class->get_rr_interval(rq, p);\n\ttask_rq_unlock(rq, p, &rf);\n\n\trcu_read_unlock();\n\tjiffies_to_timespec64(time_slice, t);\n\treturn 0;\n\nout_unlock:\n\trcu_read_unlock();\n\treturn retval;\n}\n\n \nSYSCALL_DEFINE2(sched_rr_get_interval, pid_t, pid,\n\t\tstruct __kernel_timespec __user *, interval)\n{\n\tstruct timespec64 t;\n\tint retval = sched_rr_get_interval(pid, &t);\n\n\tif (retval == 0)\n\t\tretval = put_timespec64(&t, interval);\n\n\treturn retval;\n}\n\n#ifdef CONFIG_COMPAT_32BIT_TIME\nSYSCALL_DEFINE2(sched_rr_get_interval_time32, pid_t, pid,\n\t\tstruct old_timespec32 __user *, interval)\n{\n\tstruct timespec64 t;\n\tint retval = sched_rr_get_interval(pid, &t);\n\n\tif (retval == 0)\n\t\tretval = put_old_timespec32(&t, interval);\n\treturn retval;\n}\n#endif\n\nvoid sched_show_task(struct task_struct *p)\n{\n\tunsigned long free = 0;\n\tint ppid;\n\n\tif (!try_get_task_stack(p))\n\t\treturn;\n\n\tpr_info(\"task:%-15.15s state:%c\", p->comm, task_state_to_char(p));\n\n\tif (task_is_running(p))\n\t\tpr_cont(\"  running task    \");\n#ifdef CONFIG_DEBUG_STACK_USAGE\n\tfree = stack_not_used(p);\n#endif\n\tppid = 0;\n\trcu_read_lock();\n\tif (pid_alive(p))\n\t\tppid = task_pid_nr(rcu_dereference(p->real_parent));\n\trcu_read_unlock();\n\tpr_cont(\" stack:%-5lu pid:%-5d ppid:%-6d flags:0x%08lx\\n\",\n\t\tfree, task_pid_nr(p), ppid,\n\t\tread_task_thread_flags(p));\n\n\tprint_worker_info(KERN_INFO, p);\n\tprint_stop_info(KERN_INFO, p);\n\tshow_stack(p, NULL, KERN_INFO);\n\tput_task_stack(p);\n}\nEXPORT_SYMBOL_GPL(sched_show_task);\n\nstatic inline bool\nstate_filter_match(unsigned long state_filter, struct task_struct *p)\n{\n\tunsigned int state = READ_ONCE(p->__state);\n\n\t \n\tif (!state_filter)\n\t\treturn true;\n\n\t \n\tif (!(state & state_filter))\n\t\treturn false;\n\n\t \n\tif (state_filter == TASK_UNINTERRUPTIBLE && (state & TASK_NOLOAD))\n\t\treturn false;\n\n\treturn true;\n}\n\n\nvoid show_state_filter(unsigned int state_filter)\n{\n\tstruct task_struct *g, *p;\n\n\trcu_read_lock();\n\tfor_each_process_thread(g, p) {\n\t\t \n\t\ttouch_nmi_watchdog();\n\t\ttouch_all_softlockup_watchdogs();\n\t\tif (state_filter_match(state_filter, p))\n\t\t\tsched_show_task(p);\n\t}\n\n#ifdef CONFIG_SCHED_DEBUG\n\tif (!state_filter)\n\t\tsysrq_sched_debug_show();\n#endif\n\trcu_read_unlock();\n\t \n\tif (!state_filter)\n\t\tdebug_show_all_locks();\n}\n\n \nvoid __init init_idle(struct task_struct *idle, int cpu)\n{\n#ifdef CONFIG_SMP\n\tstruct affinity_context ac = (struct affinity_context) {\n\t\t.new_mask  = cpumask_of(cpu),\n\t\t.flags     = 0,\n\t};\n#endif\n\tstruct rq *rq = cpu_rq(cpu);\n\tunsigned long flags;\n\n\t__sched_fork(0, idle);\n\n\traw_spin_lock_irqsave(&idle->pi_lock, flags);\n\traw_spin_rq_lock(rq);\n\n\tidle->__state = TASK_RUNNING;\n\tidle->se.exec_start = sched_clock();\n\t \n\tidle->flags |= PF_KTHREAD | PF_NO_SETAFFINITY;\n\tkthread_set_per_cpu(idle, cpu);\n\n#ifdef CONFIG_SMP\n\t \n\tset_cpus_allowed_common(idle, &ac);\n#endif\n\t \n\trcu_read_lock();\n\t__set_task_cpu(idle, cpu);\n\trcu_read_unlock();\n\n\trq->idle = idle;\n\trcu_assign_pointer(rq->curr, idle);\n\tidle->on_rq = TASK_ON_RQ_QUEUED;\n#ifdef CONFIG_SMP\n\tidle->on_cpu = 1;\n#endif\n\traw_spin_rq_unlock(rq);\n\traw_spin_unlock_irqrestore(&idle->pi_lock, flags);\n\n\t \n\tinit_idle_preempt_count(idle, cpu);\n\n\t \n\tidle->sched_class = &idle_sched_class;\n\tftrace_graph_init_idle_task(idle, cpu);\n\tvtime_init_idle(idle, cpu);\n#ifdef CONFIG_SMP\n\tsprintf(idle->comm, \"%s/%d\", INIT_TASK_COMM, cpu);\n#endif\n}\n\n#ifdef CONFIG_SMP\n\nint cpuset_cpumask_can_shrink(const struct cpumask *cur,\n\t\t\t      const struct cpumask *trial)\n{\n\tint ret = 1;\n\n\tif (cpumask_empty(cur))\n\t\treturn ret;\n\n\tret = dl_cpuset_cpumask_can_shrink(cur, trial);\n\n\treturn ret;\n}\n\nint task_can_attach(struct task_struct *p)\n{\n\tint ret = 0;\n\n\t \n\tif (p->flags & PF_NO_SETAFFINITY)\n\t\tret = -EINVAL;\n\n\treturn ret;\n}\n\nbool sched_smp_initialized __read_mostly;\n\n#ifdef CONFIG_NUMA_BALANCING\n \nint migrate_task_to(struct task_struct *p, int target_cpu)\n{\n\tstruct migration_arg arg = { p, target_cpu };\n\tint curr_cpu = task_cpu(p);\n\n\tif (curr_cpu == target_cpu)\n\t\treturn 0;\n\n\tif (!cpumask_test_cpu(target_cpu, p->cpus_ptr))\n\t\treturn -EINVAL;\n\n\t \n\n\ttrace_sched_move_numa(p, curr_cpu, target_cpu);\n\treturn stop_one_cpu(curr_cpu, migration_cpu_stop, &arg);\n}\n\n \nvoid sched_setnuma(struct task_struct *p, int nid)\n{\n\tbool queued, running;\n\tstruct rq_flags rf;\n\tstruct rq *rq;\n\n\trq = task_rq_lock(p, &rf);\n\tqueued = task_on_rq_queued(p);\n\trunning = task_current(rq, p);\n\n\tif (queued)\n\t\tdequeue_task(rq, p, DEQUEUE_SAVE);\n\tif (running)\n\t\tput_prev_task(rq, p);\n\n\tp->numa_preferred_nid = nid;\n\n\tif (queued)\n\t\tenqueue_task(rq, p, ENQUEUE_RESTORE | ENQUEUE_NOCLOCK);\n\tif (running)\n\t\tset_next_task(rq, p);\n\ttask_rq_unlock(rq, p, &rf);\n}\n#endif  \n\n#ifdef CONFIG_HOTPLUG_CPU\n \nvoid idle_task_exit(void)\n{\n\tstruct mm_struct *mm = current->active_mm;\n\n\tBUG_ON(cpu_online(smp_processor_id()));\n\tBUG_ON(current != this_rq()->idle);\n\n\tif (mm != &init_mm) {\n\t\tswitch_mm(mm, &init_mm, current);\n\t\tfinish_arch_post_lock_switch();\n\t}\n\n\t \n}\n\nstatic int __balance_push_cpu_stop(void *arg)\n{\n\tstruct task_struct *p = arg;\n\tstruct rq *rq = this_rq();\n\tstruct rq_flags rf;\n\tint cpu;\n\n\traw_spin_lock_irq(&p->pi_lock);\n\trq_lock(rq, &rf);\n\n\tupdate_rq_clock(rq);\n\n\tif (task_rq(p) == rq && task_on_rq_queued(p)) {\n\t\tcpu = select_fallback_rq(rq->cpu, p);\n\t\trq = __migrate_task(rq, &rf, p, cpu);\n\t}\n\n\trq_unlock(rq, &rf);\n\traw_spin_unlock_irq(&p->pi_lock);\n\n\tput_task_struct(p);\n\n\treturn 0;\n}\n\nstatic DEFINE_PER_CPU(struct cpu_stop_work, push_work);\n\n \nstatic void balance_push(struct rq *rq)\n{\n\tstruct task_struct *push_task = rq->curr;\n\n\tlockdep_assert_rq_held(rq);\n\n\t \n\trq->balance_callback = &balance_push_callback;\n\n\t \n\tif (!cpu_dying(rq->cpu) || rq != this_rq())\n\t\treturn;\n\n\t \n\tif (kthread_is_per_cpu(push_task) ||\n\t    is_migration_disabled(push_task)) {\n\n\t\t \n\t\tif (!rq->nr_running && !rq_has_pinned_tasks(rq) &&\n\t\t    rcuwait_active(&rq->hotplug_wait)) {\n\t\t\traw_spin_rq_unlock(rq);\n\t\t\trcuwait_wake_up(&rq->hotplug_wait);\n\t\t\traw_spin_rq_lock(rq);\n\t\t}\n\t\treturn;\n\t}\n\n\tget_task_struct(push_task);\n\t \n\tpreempt_disable();\n\traw_spin_rq_unlock(rq);\n\tstop_one_cpu_nowait(rq->cpu, __balance_push_cpu_stop, push_task,\n\t\t\t    this_cpu_ptr(&push_work));\n\tpreempt_enable();\n\t \n\traw_spin_rq_lock(rq);\n}\n\nstatic void balance_push_set(int cpu, bool on)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tstruct rq_flags rf;\n\n\trq_lock_irqsave(rq, &rf);\n\tif (on) {\n\t\tWARN_ON_ONCE(rq->balance_callback);\n\t\trq->balance_callback = &balance_push_callback;\n\t} else if (rq->balance_callback == &balance_push_callback) {\n\t\trq->balance_callback = NULL;\n\t}\n\trq_unlock_irqrestore(rq, &rf);\n}\n\n \nstatic void balance_hotplug_wait(void)\n{\n\tstruct rq *rq = this_rq();\n\n\trcuwait_wait_event(&rq->hotplug_wait,\n\t\t\t   rq->nr_running == 1 && !rq_has_pinned_tasks(rq),\n\t\t\t   TASK_UNINTERRUPTIBLE);\n}\n\n#else\n\nstatic inline void balance_push(struct rq *rq)\n{\n}\n\nstatic inline void balance_push_set(int cpu, bool on)\n{\n}\n\nstatic inline void balance_hotplug_wait(void)\n{\n}\n\n#endif  \n\nvoid set_rq_online(struct rq *rq)\n{\n\tif (!rq->online) {\n\t\tconst struct sched_class *class;\n\n\t\tcpumask_set_cpu(rq->cpu, rq->rd->online);\n\t\trq->online = 1;\n\n\t\tfor_each_class(class) {\n\t\t\tif (class->rq_online)\n\t\t\t\tclass->rq_online(rq);\n\t\t}\n\t}\n}\n\nvoid set_rq_offline(struct rq *rq)\n{\n\tif (rq->online) {\n\t\tconst struct sched_class *class;\n\n\t\tupdate_rq_clock(rq);\n\t\tfor_each_class(class) {\n\t\t\tif (class->rq_offline)\n\t\t\t\tclass->rq_offline(rq);\n\t\t}\n\n\t\tcpumask_clear_cpu(rq->cpu, rq->rd->online);\n\t\trq->online = 0;\n\t}\n}\n\n \nstatic int num_cpus_frozen;\n\n \nstatic void cpuset_cpu_active(void)\n{\n\tif (cpuhp_tasks_frozen) {\n\t\t \n\t\tpartition_sched_domains(1, NULL, NULL);\n\t\tif (--num_cpus_frozen)\n\t\t\treturn;\n\t\t \n\t\tcpuset_force_rebuild();\n\t}\n\tcpuset_update_active_cpus();\n}\n\nstatic int cpuset_cpu_inactive(unsigned int cpu)\n{\n\tif (!cpuhp_tasks_frozen) {\n\t\tint ret = dl_bw_check_overflow(cpu);\n\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tcpuset_update_active_cpus();\n\t} else {\n\t\tnum_cpus_frozen++;\n\t\tpartition_sched_domains(1, NULL, NULL);\n\t}\n\treturn 0;\n}\n\nint sched_cpu_activate(unsigned int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tstruct rq_flags rf;\n\n\t \n\tbalance_push_set(cpu, false);\n\n#ifdef CONFIG_SCHED_SMT\n\t \n\tif (cpumask_weight(cpu_smt_mask(cpu)) == 2)\n\t\tstatic_branch_inc_cpuslocked(&sched_smt_present);\n#endif\n\tset_cpu_active(cpu, true);\n\n\tif (sched_smp_initialized) {\n\t\tsched_update_numa(cpu, true);\n\t\tsched_domains_numa_masks_set(cpu);\n\t\tcpuset_cpu_active();\n\t}\n\n\t \n\trq_lock_irqsave(rq, &rf);\n\tif (rq->rd) {\n\t\tBUG_ON(!cpumask_test_cpu(cpu, rq->rd->span));\n\t\tset_rq_online(rq);\n\t}\n\trq_unlock_irqrestore(rq, &rf);\n\n\treturn 0;\n}\n\nint sched_cpu_deactivate(unsigned int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tstruct rq_flags rf;\n\tint ret;\n\n\t \n\tnohz_balance_exit_idle(rq);\n\n\tset_cpu_active(cpu, false);\n\n\t \n\tbalance_push_set(cpu, true);\n\n\t \n\tsynchronize_rcu();\n\n\trq_lock_irqsave(rq, &rf);\n\tif (rq->rd) {\n\t\tBUG_ON(!cpumask_test_cpu(cpu, rq->rd->span));\n\t\tset_rq_offline(rq);\n\t}\n\trq_unlock_irqrestore(rq, &rf);\n\n#ifdef CONFIG_SCHED_SMT\n\t \n\tif (cpumask_weight(cpu_smt_mask(cpu)) == 2)\n\t\tstatic_branch_dec_cpuslocked(&sched_smt_present);\n\n\tsched_core_cpu_deactivate(cpu);\n#endif\n\n\tif (!sched_smp_initialized)\n\t\treturn 0;\n\n\tsched_update_numa(cpu, false);\n\tret = cpuset_cpu_inactive(cpu);\n\tif (ret) {\n\t\tbalance_push_set(cpu, false);\n\t\tset_cpu_active(cpu, true);\n\t\tsched_update_numa(cpu, true);\n\t\treturn ret;\n\t}\n\tsched_domains_numa_masks_clear(cpu);\n\treturn 0;\n}\n\nstatic void sched_rq_cpu_starting(unsigned int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\n\trq->calc_load_update = calc_load_update;\n\tupdate_max_interval();\n}\n\nint sched_cpu_starting(unsigned int cpu)\n{\n\tsched_core_cpu_starting(cpu);\n\tsched_rq_cpu_starting(cpu);\n\tsched_tick_start(cpu);\n\treturn 0;\n}\n\n#ifdef CONFIG_HOTPLUG_CPU\n\n \nint sched_cpu_wait_empty(unsigned int cpu)\n{\n\tbalance_hotplug_wait();\n\treturn 0;\n}\n\n \nstatic void calc_load_migrate(struct rq *rq)\n{\n\tlong delta = calc_load_fold_active(rq, 1);\n\n\tif (delta)\n\t\tatomic_long_add(delta, &calc_load_tasks);\n}\n\nstatic void dump_rq_tasks(struct rq *rq, const char *loglvl)\n{\n\tstruct task_struct *g, *p;\n\tint cpu = cpu_of(rq);\n\n\tlockdep_assert_rq_held(rq);\n\n\tprintk(\"%sCPU%d enqueued tasks (%u total):\\n\", loglvl, cpu, rq->nr_running);\n\tfor_each_process_thread(g, p) {\n\t\tif (task_cpu(p) != cpu)\n\t\t\tcontinue;\n\n\t\tif (!task_on_rq_queued(p))\n\t\t\tcontinue;\n\n\t\tprintk(\"%s\\tpid: %d, name: %s\\n\", loglvl, p->pid, p->comm);\n\t}\n}\n\nint sched_cpu_dying(unsigned int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tstruct rq_flags rf;\n\n\t \n\tsched_tick_stop(cpu);\n\n\trq_lock_irqsave(rq, &rf);\n\tif (rq->nr_running != 1 || rq_has_pinned_tasks(rq)) {\n\t\tWARN(true, \"Dying CPU not properly vacated!\");\n\t\tdump_rq_tasks(rq, KERN_WARNING);\n\t}\n\trq_unlock_irqrestore(rq, &rf);\n\n\tcalc_load_migrate(rq);\n\tupdate_max_interval();\n\thrtick_clear(rq);\n\tsched_core_cpu_dying(cpu);\n\treturn 0;\n}\n#endif\n\nvoid __init sched_init_smp(void)\n{\n\tsched_init_numa(NUMA_NO_NODE);\n\n\t \n\tmutex_lock(&sched_domains_mutex);\n\tsched_init_domains(cpu_active_mask);\n\tmutex_unlock(&sched_domains_mutex);\n\n\t \n\tif (set_cpus_allowed_ptr(current, housekeeping_cpumask(HK_TYPE_DOMAIN)) < 0)\n\t\tBUG();\n\tcurrent->flags &= ~PF_NO_SETAFFINITY;\n\tsched_init_granularity();\n\n\tinit_sched_rt_class();\n\tinit_sched_dl_class();\n\n\tsched_smp_initialized = true;\n}\n\nstatic int __init migration_init(void)\n{\n\tsched_cpu_starting(smp_processor_id());\n\treturn 0;\n}\nearly_initcall(migration_init);\n\n#else\nvoid __init sched_init_smp(void)\n{\n\tsched_init_granularity();\n}\n#endif  \n\nint in_sched_functions(unsigned long addr)\n{\n\treturn in_lock_functions(addr) ||\n\t\t(addr >= (unsigned long)__sched_text_start\n\t\t&& addr < (unsigned long)__sched_text_end);\n}\n\n#ifdef CONFIG_CGROUP_SCHED\n \nstruct task_group root_task_group;\nLIST_HEAD(task_groups);\n\n \nstatic struct kmem_cache *task_group_cache __read_mostly;\n#endif\n\nvoid __init sched_init(void)\n{\n\tunsigned long ptr = 0;\n\tint i;\n\n\t \n\tBUG_ON(&idle_sched_class != &fair_sched_class + 1 ||\n\t       &fair_sched_class != &rt_sched_class + 1 ||\n\t       &rt_sched_class   != &dl_sched_class + 1);\n#ifdef CONFIG_SMP\n\tBUG_ON(&dl_sched_class != &stop_sched_class + 1);\n#endif\n\n\twait_bit_init();\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tptr += 2 * nr_cpu_ids * sizeof(void **);\n#endif\n#ifdef CONFIG_RT_GROUP_SCHED\n\tptr += 2 * nr_cpu_ids * sizeof(void **);\n#endif\n\tif (ptr) {\n\t\tptr = (unsigned long)kzalloc(ptr, GFP_NOWAIT);\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\t\troot_task_group.se = (struct sched_entity **)ptr;\n\t\tptr += nr_cpu_ids * sizeof(void **);\n\n\t\troot_task_group.cfs_rq = (struct cfs_rq **)ptr;\n\t\tptr += nr_cpu_ids * sizeof(void **);\n\n\t\troot_task_group.shares = ROOT_TASK_GROUP_LOAD;\n\t\tinit_cfs_bandwidth(&root_task_group.cfs_bandwidth, NULL);\n#endif  \n#ifdef CONFIG_RT_GROUP_SCHED\n\t\troot_task_group.rt_se = (struct sched_rt_entity **)ptr;\n\t\tptr += nr_cpu_ids * sizeof(void **);\n\n\t\troot_task_group.rt_rq = (struct rt_rq **)ptr;\n\t\tptr += nr_cpu_ids * sizeof(void **);\n\n#endif  \n\t}\n\n\tinit_rt_bandwidth(&def_rt_bandwidth, global_rt_period(), global_rt_runtime());\n\n#ifdef CONFIG_SMP\n\tinit_defrootdomain();\n#endif\n\n#ifdef CONFIG_RT_GROUP_SCHED\n\tinit_rt_bandwidth(&root_task_group.rt_bandwidth,\n\t\t\tglobal_rt_period(), global_rt_runtime());\n#endif  \n\n#ifdef CONFIG_CGROUP_SCHED\n\ttask_group_cache = KMEM_CACHE(task_group, 0);\n\n\tlist_add(&root_task_group.list, &task_groups);\n\tINIT_LIST_HEAD(&root_task_group.children);\n\tINIT_LIST_HEAD(&root_task_group.siblings);\n\tautogroup_init(&init_task);\n#endif  \n\n\tfor_each_possible_cpu(i) {\n\t\tstruct rq *rq;\n\n\t\trq = cpu_rq(i);\n\t\traw_spin_lock_init(&rq->__lock);\n\t\trq->nr_running = 0;\n\t\trq->calc_load_active = 0;\n\t\trq->calc_load_update = jiffies + LOAD_FREQ;\n\t\tinit_cfs_rq(&rq->cfs);\n\t\tinit_rt_rq(&rq->rt);\n\t\tinit_dl_rq(&rq->dl);\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\t\tINIT_LIST_HEAD(&rq->leaf_cfs_rq_list);\n\t\trq->tmp_alone_branch = &rq->leaf_cfs_rq_list;\n\t\t \n\t\tinit_tg_cfs_entry(&root_task_group, &rq->cfs, NULL, i, NULL);\n#endif  \n\n\t\trq->rt.rt_runtime = def_rt_bandwidth.rt_runtime;\n#ifdef CONFIG_RT_GROUP_SCHED\n\t\tinit_tg_rt_entry(&root_task_group, &rq->rt, NULL, i, NULL);\n#endif\n#ifdef CONFIG_SMP\n\t\trq->sd = NULL;\n\t\trq->rd = NULL;\n\t\trq->cpu_capacity = rq->cpu_capacity_orig = SCHED_CAPACITY_SCALE;\n\t\trq->balance_callback = &balance_push_callback;\n\t\trq->active_balance = 0;\n\t\trq->next_balance = jiffies;\n\t\trq->push_cpu = 0;\n\t\trq->cpu = i;\n\t\trq->online = 0;\n\t\trq->idle_stamp = 0;\n\t\trq->avg_idle = 2*sysctl_sched_migration_cost;\n\t\trq->wake_stamp = jiffies;\n\t\trq->wake_avg_idle = rq->avg_idle;\n\t\trq->max_idle_balance_cost = sysctl_sched_migration_cost;\n\n\t\tINIT_LIST_HEAD(&rq->cfs_tasks);\n\n\t\trq_attach_root(rq, &def_root_domain);\n#ifdef CONFIG_NO_HZ_COMMON\n\t\trq->last_blocked_load_update_tick = jiffies;\n\t\tatomic_set(&rq->nohz_flags, 0);\n\n\t\tINIT_CSD(&rq->nohz_csd, nohz_csd_func, rq);\n#endif\n#ifdef CONFIG_HOTPLUG_CPU\n\t\trcuwait_init(&rq->hotplug_wait);\n#endif\n#endif  \n\t\thrtick_rq_init(rq);\n\t\tatomic_set(&rq->nr_iowait, 0);\n\n#ifdef CONFIG_SCHED_CORE\n\t\trq->core = rq;\n\t\trq->core_pick = NULL;\n\t\trq->core_enabled = 0;\n\t\trq->core_tree = RB_ROOT;\n\t\trq->core_forceidle_count = 0;\n\t\trq->core_forceidle_occupation = 0;\n\t\trq->core_forceidle_start = 0;\n\n\t\trq->core_cookie = 0UL;\n#endif\n\t\tzalloc_cpumask_var_node(&rq->scratch_mask, GFP_KERNEL, cpu_to_node(i));\n\t}\n\n\tset_load_weight(&init_task, false);\n\n\t \n\tmmgrab_lazy_tlb(&init_mm);\n\tenter_lazy_tlb(&init_mm, current);\n\n\t \n\tWARN_ON(!set_kthread_struct(current));\n\n\t \n\tinit_idle(current, smp_processor_id());\n\n\tcalc_load_update = jiffies + LOAD_FREQ;\n\n#ifdef CONFIG_SMP\n\tidle_thread_set_boot_cpu();\n\tbalance_push_set(smp_processor_id(), false);\n#endif\n\tinit_sched_fair_class();\n\n\tpsi_init();\n\n\tinit_uclamp();\n\n\tpreempt_dynamic_init();\n\n\tscheduler_running = 1;\n}\n\n#ifdef CONFIG_DEBUG_ATOMIC_SLEEP\n\nvoid __might_sleep(const char *file, int line)\n{\n\tunsigned int state = get_current_state();\n\t \n\tWARN_ONCE(state != TASK_RUNNING && current->task_state_change,\n\t\t\t\"do not call blocking ops when !TASK_RUNNING; \"\n\t\t\t\"state=%x set at [<%p>] %pS\\n\", state,\n\t\t\t(void *)current->task_state_change,\n\t\t\t(void *)current->task_state_change);\n\n\t__might_resched(file, line, 0);\n}\nEXPORT_SYMBOL(__might_sleep);\n\nstatic void print_preempt_disable_ip(int preempt_offset, unsigned long ip)\n{\n\tif (!IS_ENABLED(CONFIG_DEBUG_PREEMPT))\n\t\treturn;\n\n\tif (preempt_count() == preempt_offset)\n\t\treturn;\n\n\tpr_err(\"Preemption disabled at:\");\n\tprint_ip_sym(KERN_ERR, ip);\n}\n\nstatic inline bool resched_offsets_ok(unsigned int offsets)\n{\n\tunsigned int nested = preempt_count();\n\n\tnested += rcu_preempt_depth() << MIGHT_RESCHED_RCU_SHIFT;\n\n\treturn nested == offsets;\n}\n\nvoid __might_resched(const char *file, int line, unsigned int offsets)\n{\n\t \n\tstatic unsigned long prev_jiffy;\n\n\tunsigned long preempt_disable_ip;\n\n\t \n\trcu_sleep_check();\n\n\tif ((resched_offsets_ok(offsets) && !irqs_disabled() &&\n\t     !is_idle_task(current) && !current->non_block_count) ||\n\t    system_state == SYSTEM_BOOTING || system_state > SYSTEM_RUNNING ||\n\t    oops_in_progress)\n\t\treturn;\n\n\tif (time_before(jiffies, prev_jiffy + HZ) && prev_jiffy)\n\t\treturn;\n\tprev_jiffy = jiffies;\n\n\t \n\tpreempt_disable_ip = get_preempt_disable_ip(current);\n\n\tpr_err(\"BUG: sleeping function called from invalid context at %s:%d\\n\",\n\t       file, line);\n\tpr_err(\"in_atomic(): %d, irqs_disabled(): %d, non_block: %d, pid: %d, name: %s\\n\",\n\t       in_atomic(), irqs_disabled(), current->non_block_count,\n\t       current->pid, current->comm);\n\tpr_err(\"preempt_count: %x, expected: %x\\n\", preempt_count(),\n\t       offsets & MIGHT_RESCHED_PREEMPT_MASK);\n\n\tif (IS_ENABLED(CONFIG_PREEMPT_RCU)) {\n\t\tpr_err(\"RCU nest depth: %d, expected: %u\\n\",\n\t\t       rcu_preempt_depth(), offsets >> MIGHT_RESCHED_RCU_SHIFT);\n\t}\n\n\tif (task_stack_end_corrupted(current))\n\t\tpr_emerg(\"Thread overran stack, or stack corrupted\\n\");\n\n\tdebug_show_held_locks(current);\n\tif (irqs_disabled())\n\t\tprint_irqtrace_events(current);\n\n\tprint_preempt_disable_ip(offsets & MIGHT_RESCHED_PREEMPT_MASK,\n\t\t\t\t preempt_disable_ip);\n\n\tdump_stack();\n\tadd_taint(TAINT_WARN, LOCKDEP_STILL_OK);\n}\nEXPORT_SYMBOL(__might_resched);\n\nvoid __cant_sleep(const char *file, int line, int preempt_offset)\n{\n\tstatic unsigned long prev_jiffy;\n\n\tif (irqs_disabled())\n\t\treturn;\n\n\tif (!IS_ENABLED(CONFIG_PREEMPT_COUNT))\n\t\treturn;\n\n\tif (preempt_count() > preempt_offset)\n\t\treturn;\n\n\tif (time_before(jiffies, prev_jiffy + HZ) && prev_jiffy)\n\t\treturn;\n\tprev_jiffy = jiffies;\n\n\tprintk(KERN_ERR \"BUG: assuming atomic context at %s:%d\\n\", file, line);\n\tprintk(KERN_ERR \"in_atomic(): %d, irqs_disabled(): %d, pid: %d, name: %s\\n\",\n\t\t\tin_atomic(), irqs_disabled(),\n\t\t\tcurrent->pid, current->comm);\n\n\tdebug_show_held_locks(current);\n\tdump_stack();\n\tadd_taint(TAINT_WARN, LOCKDEP_STILL_OK);\n}\nEXPORT_SYMBOL_GPL(__cant_sleep);\n\n#ifdef CONFIG_SMP\nvoid __cant_migrate(const char *file, int line)\n{\n\tstatic unsigned long prev_jiffy;\n\n\tif (irqs_disabled())\n\t\treturn;\n\n\tif (is_migration_disabled(current))\n\t\treturn;\n\n\tif (!IS_ENABLED(CONFIG_PREEMPT_COUNT))\n\t\treturn;\n\n\tif (preempt_count() > 0)\n\t\treturn;\n\n\tif (time_before(jiffies, prev_jiffy + HZ) && prev_jiffy)\n\t\treturn;\n\tprev_jiffy = jiffies;\n\n\tpr_err(\"BUG: assuming non migratable context at %s:%d\\n\", file, line);\n\tpr_err(\"in_atomic(): %d, irqs_disabled(): %d, migration_disabled() %u pid: %d, name: %s\\n\",\n\t       in_atomic(), irqs_disabled(), is_migration_disabled(current),\n\t       current->pid, current->comm);\n\n\tdebug_show_held_locks(current);\n\tdump_stack();\n\tadd_taint(TAINT_WARN, LOCKDEP_STILL_OK);\n}\nEXPORT_SYMBOL_GPL(__cant_migrate);\n#endif\n#endif\n\n#ifdef CONFIG_MAGIC_SYSRQ\nvoid normalize_rt_tasks(void)\n{\n\tstruct task_struct *g, *p;\n\tstruct sched_attr attr = {\n\t\t.sched_policy = SCHED_NORMAL,\n\t};\n\n\tread_lock(&tasklist_lock);\n\tfor_each_process_thread(g, p) {\n\t\t \n\t\tif (p->flags & PF_KTHREAD)\n\t\t\tcontinue;\n\n\t\tp->se.exec_start = 0;\n\t\tschedstat_set(p->stats.wait_start,  0);\n\t\tschedstat_set(p->stats.sleep_start, 0);\n\t\tschedstat_set(p->stats.block_start, 0);\n\n\t\tif (!dl_task(p) && !rt_task(p)) {\n\t\t\t \n\t\t\tif (task_nice(p) < 0)\n\t\t\t\tset_user_nice(p, 0);\n\t\t\tcontinue;\n\t\t}\n\n\t\t__sched_setscheduler(p, &attr, false, false);\n\t}\n\tread_unlock(&tasklist_lock);\n}\n\n#endif  \n\n#if defined(CONFIG_IA64) || defined(CONFIG_KGDB_KDB)\n \n\n \nstruct task_struct *curr_task(int cpu)\n{\n\treturn cpu_curr(cpu);\n}\n\n#endif  \n\n#ifdef CONFIG_IA64\n \nvoid ia64_set_curr_task(int cpu, struct task_struct *p)\n{\n\tcpu_curr(cpu) = p;\n}\n\n#endif\n\n#ifdef CONFIG_CGROUP_SCHED\n \nstatic DEFINE_SPINLOCK(task_group_lock);\n\nstatic inline void alloc_uclamp_sched_group(struct task_group *tg,\n\t\t\t\t\t    struct task_group *parent)\n{\n#ifdef CONFIG_UCLAMP_TASK_GROUP\n\tenum uclamp_id clamp_id;\n\n\tfor_each_clamp_id(clamp_id) {\n\t\tuclamp_se_set(&tg->uclamp_req[clamp_id],\n\t\t\t      uclamp_none(clamp_id), false);\n\t\ttg->uclamp[clamp_id] = parent->uclamp[clamp_id];\n\t}\n#endif\n}\n\nstatic void sched_free_group(struct task_group *tg)\n{\n\tfree_fair_sched_group(tg);\n\tfree_rt_sched_group(tg);\n\tautogroup_free(tg);\n\tkmem_cache_free(task_group_cache, tg);\n}\n\nstatic void sched_free_group_rcu(struct rcu_head *rcu)\n{\n\tsched_free_group(container_of(rcu, struct task_group, rcu));\n}\n\nstatic void sched_unregister_group(struct task_group *tg)\n{\n\tunregister_fair_sched_group(tg);\n\tunregister_rt_sched_group(tg);\n\t \n\tcall_rcu(&tg->rcu, sched_free_group_rcu);\n}\n\n \nstruct task_group *sched_create_group(struct task_group *parent)\n{\n\tstruct task_group *tg;\n\n\ttg = kmem_cache_alloc(task_group_cache, GFP_KERNEL | __GFP_ZERO);\n\tif (!tg)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (!alloc_fair_sched_group(tg, parent))\n\t\tgoto err;\n\n\tif (!alloc_rt_sched_group(tg, parent))\n\t\tgoto err;\n\n\talloc_uclamp_sched_group(tg, parent);\n\n\treturn tg;\n\nerr:\n\tsched_free_group(tg);\n\treturn ERR_PTR(-ENOMEM);\n}\n\nvoid sched_online_group(struct task_group *tg, struct task_group *parent)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&task_group_lock, flags);\n\tlist_add_rcu(&tg->list, &task_groups);\n\n\t \n\tWARN_ON(!parent);\n\n\ttg->parent = parent;\n\tINIT_LIST_HEAD(&tg->children);\n\tlist_add_rcu(&tg->siblings, &parent->children);\n\tspin_unlock_irqrestore(&task_group_lock, flags);\n\n\tonline_fair_sched_group(tg);\n}\n\n \nstatic void sched_unregister_group_rcu(struct rcu_head *rhp)\n{\n\t \n\tsched_unregister_group(container_of(rhp, struct task_group, rcu));\n}\n\nvoid sched_destroy_group(struct task_group *tg)\n{\n\t \n\tcall_rcu(&tg->rcu, sched_unregister_group_rcu);\n}\n\nvoid sched_release_group(struct task_group *tg)\n{\n\tunsigned long flags;\n\n\t \n\tspin_lock_irqsave(&task_group_lock, flags);\n\tlist_del_rcu(&tg->list);\n\tlist_del_rcu(&tg->siblings);\n\tspin_unlock_irqrestore(&task_group_lock, flags);\n}\n\nstatic struct task_group *sched_get_task_group(struct task_struct *tsk)\n{\n\tstruct task_group *tg;\n\n\t \n\ttg = container_of(task_css_check(tsk, cpu_cgrp_id, true),\n\t\t\t  struct task_group, css);\n\ttg = autogroup_task_group(tsk, tg);\n\n\treturn tg;\n}\n\nstatic void sched_change_group(struct task_struct *tsk, struct task_group *group)\n{\n\ttsk->sched_task_group = group;\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tif (tsk->sched_class->task_change_group)\n\t\ttsk->sched_class->task_change_group(tsk);\n\telse\n#endif\n\t\tset_task_rq(tsk, task_cpu(tsk));\n}\n\n \nvoid sched_move_task(struct task_struct *tsk)\n{\n\tint queued, running, queue_flags =\n\t\tDEQUEUE_SAVE | DEQUEUE_MOVE | DEQUEUE_NOCLOCK;\n\tstruct task_group *group;\n\tstruct rq_flags rf;\n\tstruct rq *rq;\n\n\trq = task_rq_lock(tsk, &rf);\n\t \n\tgroup = sched_get_task_group(tsk);\n\tif (group == tsk->sched_task_group)\n\t\tgoto unlock;\n\n\tupdate_rq_clock(rq);\n\n\trunning = task_current(rq, tsk);\n\tqueued = task_on_rq_queued(tsk);\n\n\tif (queued)\n\t\tdequeue_task(rq, tsk, queue_flags);\n\tif (running)\n\t\tput_prev_task(rq, tsk);\n\n\tsched_change_group(tsk, group);\n\n\tif (queued)\n\t\tenqueue_task(rq, tsk, queue_flags);\n\tif (running) {\n\t\tset_next_task(rq, tsk);\n\t\t \n\t\tresched_curr(rq);\n\t}\n\nunlock:\n\ttask_rq_unlock(rq, tsk, &rf);\n}\n\nstatic inline struct task_group *css_tg(struct cgroup_subsys_state *css)\n{\n\treturn css ? container_of(css, struct task_group, css) : NULL;\n}\n\nstatic struct cgroup_subsys_state *\ncpu_cgroup_css_alloc(struct cgroup_subsys_state *parent_css)\n{\n\tstruct task_group *parent = css_tg(parent_css);\n\tstruct task_group *tg;\n\n\tif (!parent) {\n\t\t \n\t\treturn &root_task_group.css;\n\t}\n\n\ttg = sched_create_group(parent);\n\tif (IS_ERR(tg))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\treturn &tg->css;\n}\n\n \nstatic int cpu_cgroup_css_online(struct cgroup_subsys_state *css)\n{\n\tstruct task_group *tg = css_tg(css);\n\tstruct task_group *parent = css_tg(css->parent);\n\n\tif (parent)\n\t\tsched_online_group(tg, parent);\n\n#ifdef CONFIG_UCLAMP_TASK_GROUP\n\t \n\tmutex_lock(&uclamp_mutex);\n\trcu_read_lock();\n\tcpu_util_update_eff(css);\n\trcu_read_unlock();\n\tmutex_unlock(&uclamp_mutex);\n#endif\n\n\treturn 0;\n}\n\nstatic void cpu_cgroup_css_released(struct cgroup_subsys_state *css)\n{\n\tstruct task_group *tg = css_tg(css);\n\n\tsched_release_group(tg);\n}\n\nstatic void cpu_cgroup_css_free(struct cgroup_subsys_state *css)\n{\n\tstruct task_group *tg = css_tg(css);\n\n\t \n\tsched_unregister_group(tg);\n}\n\n#ifdef CONFIG_RT_GROUP_SCHED\nstatic int cpu_cgroup_can_attach(struct cgroup_taskset *tset)\n{\n\tstruct task_struct *task;\n\tstruct cgroup_subsys_state *css;\n\n\tcgroup_taskset_for_each(task, css, tset) {\n\t\tif (!sched_rt_can_attach(css_tg(css), task))\n\t\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n#endif\n\nstatic void cpu_cgroup_attach(struct cgroup_taskset *tset)\n{\n\tstruct task_struct *task;\n\tstruct cgroup_subsys_state *css;\n\n\tcgroup_taskset_for_each(task, css, tset)\n\t\tsched_move_task(task);\n}\n\n#ifdef CONFIG_UCLAMP_TASK_GROUP\nstatic void cpu_util_update_eff(struct cgroup_subsys_state *css)\n{\n\tstruct cgroup_subsys_state *top_css = css;\n\tstruct uclamp_se *uc_parent = NULL;\n\tstruct uclamp_se *uc_se = NULL;\n\tunsigned int eff[UCLAMP_CNT];\n\tenum uclamp_id clamp_id;\n\tunsigned int clamps;\n\n\tlockdep_assert_held(&uclamp_mutex);\n\tSCHED_WARN_ON(!rcu_read_lock_held());\n\n\tcss_for_each_descendant_pre(css, top_css) {\n\t\tuc_parent = css_tg(css)->parent\n\t\t\t? css_tg(css)->parent->uclamp : NULL;\n\n\t\tfor_each_clamp_id(clamp_id) {\n\t\t\t \n\t\t\teff[clamp_id] = css_tg(css)->uclamp_req[clamp_id].value;\n\t\t\t \n\t\t\tif (uc_parent &&\n\t\t\t    eff[clamp_id] > uc_parent[clamp_id].value) {\n\t\t\t\teff[clamp_id] = uc_parent[clamp_id].value;\n\t\t\t}\n\t\t}\n\t\t \n\t\teff[UCLAMP_MIN] = min(eff[UCLAMP_MIN], eff[UCLAMP_MAX]);\n\n\t\t \n\t\tclamps = 0x0;\n\t\tuc_se = css_tg(css)->uclamp;\n\t\tfor_each_clamp_id(clamp_id) {\n\t\t\tif (eff[clamp_id] == uc_se[clamp_id].value)\n\t\t\t\tcontinue;\n\t\t\tuc_se[clamp_id].value = eff[clamp_id];\n\t\t\tuc_se[clamp_id].bucket_id = uclamp_bucket_id(eff[clamp_id]);\n\t\t\tclamps |= (0x1 << clamp_id);\n\t\t}\n\t\tif (!clamps) {\n\t\t\tcss = css_rightmost_descendant(css);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tuclamp_update_active_tasks(css);\n\t}\n}\n\n \n#define _POW10(exp) ((unsigned int)1e##exp)\n#define POW10(exp) _POW10(exp)\n\nstruct uclamp_request {\n#define UCLAMP_PERCENT_SHIFT\t2\n#define UCLAMP_PERCENT_SCALE\t(100 * POW10(UCLAMP_PERCENT_SHIFT))\n\ts64 percent;\n\tu64 util;\n\tint ret;\n};\n\nstatic inline struct uclamp_request\ncapacity_from_percent(char *buf)\n{\n\tstruct uclamp_request req = {\n\t\t.percent = UCLAMP_PERCENT_SCALE,\n\t\t.util = SCHED_CAPACITY_SCALE,\n\t\t.ret = 0,\n\t};\n\n\tbuf = strim(buf);\n\tif (strcmp(buf, \"max\")) {\n\t\treq.ret = cgroup_parse_float(buf, UCLAMP_PERCENT_SHIFT,\n\t\t\t\t\t     &req.percent);\n\t\tif (req.ret)\n\t\t\treturn req;\n\t\tif ((u64)req.percent > UCLAMP_PERCENT_SCALE) {\n\t\t\treq.ret = -ERANGE;\n\t\t\treturn req;\n\t\t}\n\n\t\treq.util = req.percent << SCHED_CAPACITY_SHIFT;\n\t\treq.util = DIV_ROUND_CLOSEST_ULL(req.util, UCLAMP_PERCENT_SCALE);\n\t}\n\n\treturn req;\n}\n\nstatic ssize_t cpu_uclamp_write(struct kernfs_open_file *of, char *buf,\n\t\t\t\tsize_t nbytes, loff_t off,\n\t\t\t\tenum uclamp_id clamp_id)\n{\n\tstruct uclamp_request req;\n\tstruct task_group *tg;\n\n\treq = capacity_from_percent(buf);\n\tif (req.ret)\n\t\treturn req.ret;\n\n\tstatic_branch_enable(&sched_uclamp_used);\n\n\tmutex_lock(&uclamp_mutex);\n\trcu_read_lock();\n\n\ttg = css_tg(of_css(of));\n\tif (tg->uclamp_req[clamp_id].value != req.util)\n\t\tuclamp_se_set(&tg->uclamp_req[clamp_id], req.util, false);\n\n\t \n\ttg->uclamp_pct[clamp_id] = req.percent;\n\n\t \n\tcpu_util_update_eff(of_css(of));\n\n\trcu_read_unlock();\n\tmutex_unlock(&uclamp_mutex);\n\n\treturn nbytes;\n}\n\nstatic ssize_t cpu_uclamp_min_write(struct kernfs_open_file *of,\n\t\t\t\t    char *buf, size_t nbytes,\n\t\t\t\t    loff_t off)\n{\n\treturn cpu_uclamp_write(of, buf, nbytes, off, UCLAMP_MIN);\n}\n\nstatic ssize_t cpu_uclamp_max_write(struct kernfs_open_file *of,\n\t\t\t\t    char *buf, size_t nbytes,\n\t\t\t\t    loff_t off)\n{\n\treturn cpu_uclamp_write(of, buf, nbytes, off, UCLAMP_MAX);\n}\n\nstatic inline void cpu_uclamp_print(struct seq_file *sf,\n\t\t\t\t    enum uclamp_id clamp_id)\n{\n\tstruct task_group *tg;\n\tu64 util_clamp;\n\tu64 percent;\n\tu32 rem;\n\n\trcu_read_lock();\n\ttg = css_tg(seq_css(sf));\n\tutil_clamp = tg->uclamp_req[clamp_id].value;\n\trcu_read_unlock();\n\n\tif (util_clamp == SCHED_CAPACITY_SCALE) {\n\t\tseq_puts(sf, \"max\\n\");\n\t\treturn;\n\t}\n\n\tpercent = tg->uclamp_pct[clamp_id];\n\tpercent = div_u64_rem(percent, POW10(UCLAMP_PERCENT_SHIFT), &rem);\n\tseq_printf(sf, \"%llu.%0*u\\n\", percent, UCLAMP_PERCENT_SHIFT, rem);\n}\n\nstatic int cpu_uclamp_min_show(struct seq_file *sf, void *v)\n{\n\tcpu_uclamp_print(sf, UCLAMP_MIN);\n\treturn 0;\n}\n\nstatic int cpu_uclamp_max_show(struct seq_file *sf, void *v)\n{\n\tcpu_uclamp_print(sf, UCLAMP_MAX);\n\treturn 0;\n}\n#endif  \n\n#ifdef CONFIG_FAIR_GROUP_SCHED\nstatic int cpu_shares_write_u64(struct cgroup_subsys_state *css,\n\t\t\t\tstruct cftype *cftype, u64 shareval)\n{\n\tif (shareval > scale_load_down(ULONG_MAX))\n\t\tshareval = MAX_SHARES;\n\treturn sched_group_set_shares(css_tg(css), scale_load(shareval));\n}\n\nstatic u64 cpu_shares_read_u64(struct cgroup_subsys_state *css,\n\t\t\t       struct cftype *cft)\n{\n\tstruct task_group *tg = css_tg(css);\n\n\treturn (u64) scale_load_down(tg->shares);\n}\n\n#ifdef CONFIG_CFS_BANDWIDTH\nstatic DEFINE_MUTEX(cfs_constraints_mutex);\n\nconst u64 max_cfs_quota_period = 1 * NSEC_PER_SEC;  \nstatic const u64 min_cfs_quota_period = 1 * NSEC_PER_MSEC;  \n \nstatic const u64 max_cfs_runtime = MAX_BW * NSEC_PER_USEC;\n\nstatic int __cfs_schedulable(struct task_group *tg, u64 period, u64 runtime);\n\nstatic int tg_set_cfs_bandwidth(struct task_group *tg, u64 period, u64 quota,\n\t\t\t\tu64 burst)\n{\n\tint i, ret = 0, runtime_enabled, runtime_was_enabled;\n\tstruct cfs_bandwidth *cfs_b = &tg->cfs_bandwidth;\n\n\tif (tg == &root_task_group)\n\t\treturn -EINVAL;\n\n\t \n\tif (quota < min_cfs_quota_period || period < min_cfs_quota_period)\n\t\treturn -EINVAL;\n\n\t \n\tif (period > max_cfs_quota_period)\n\t\treturn -EINVAL;\n\n\t \n\tif (quota != RUNTIME_INF && quota > max_cfs_runtime)\n\t\treturn -EINVAL;\n\n\tif (quota != RUNTIME_INF && (burst > quota ||\n\t\t\t\t     burst + quota > max_cfs_runtime))\n\t\treturn -EINVAL;\n\n\t \n\tcpus_read_lock();\n\tmutex_lock(&cfs_constraints_mutex);\n\tret = __cfs_schedulable(tg, period, quota);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\truntime_enabled = quota != RUNTIME_INF;\n\truntime_was_enabled = cfs_b->quota != RUNTIME_INF;\n\t \n\tif (runtime_enabled && !runtime_was_enabled)\n\t\tcfs_bandwidth_usage_inc();\n\traw_spin_lock_irq(&cfs_b->lock);\n\tcfs_b->period = ns_to_ktime(period);\n\tcfs_b->quota = quota;\n\tcfs_b->burst = burst;\n\n\t__refill_cfs_bandwidth_runtime(cfs_b);\n\n\t \n\tif (runtime_enabled)\n\t\tstart_cfs_bandwidth(cfs_b);\n\n\traw_spin_unlock_irq(&cfs_b->lock);\n\n\tfor_each_online_cpu(i) {\n\t\tstruct cfs_rq *cfs_rq = tg->cfs_rq[i];\n\t\tstruct rq *rq = cfs_rq->rq;\n\t\tstruct rq_flags rf;\n\n\t\trq_lock_irq(rq, &rf);\n\t\tcfs_rq->runtime_enabled = runtime_enabled;\n\t\tcfs_rq->runtime_remaining = 0;\n\n\t\tif (cfs_rq->throttled)\n\t\t\tunthrottle_cfs_rq(cfs_rq);\n\t\trq_unlock_irq(rq, &rf);\n\t}\n\tif (runtime_was_enabled && !runtime_enabled)\n\t\tcfs_bandwidth_usage_dec();\nout_unlock:\n\tmutex_unlock(&cfs_constraints_mutex);\n\tcpus_read_unlock();\n\n\treturn ret;\n}\n\nstatic int tg_set_cfs_quota(struct task_group *tg, long cfs_quota_us)\n{\n\tu64 quota, period, burst;\n\n\tperiod = ktime_to_ns(tg->cfs_bandwidth.period);\n\tburst = tg->cfs_bandwidth.burst;\n\tif (cfs_quota_us < 0)\n\t\tquota = RUNTIME_INF;\n\telse if ((u64)cfs_quota_us <= U64_MAX / NSEC_PER_USEC)\n\t\tquota = (u64)cfs_quota_us * NSEC_PER_USEC;\n\telse\n\t\treturn -EINVAL;\n\n\treturn tg_set_cfs_bandwidth(tg, period, quota, burst);\n}\n\nstatic long tg_get_cfs_quota(struct task_group *tg)\n{\n\tu64 quota_us;\n\n\tif (tg->cfs_bandwidth.quota == RUNTIME_INF)\n\t\treturn -1;\n\n\tquota_us = tg->cfs_bandwidth.quota;\n\tdo_div(quota_us, NSEC_PER_USEC);\n\n\treturn quota_us;\n}\n\nstatic int tg_set_cfs_period(struct task_group *tg, long cfs_period_us)\n{\n\tu64 quota, period, burst;\n\n\tif ((u64)cfs_period_us > U64_MAX / NSEC_PER_USEC)\n\t\treturn -EINVAL;\n\n\tperiod = (u64)cfs_period_us * NSEC_PER_USEC;\n\tquota = tg->cfs_bandwidth.quota;\n\tburst = tg->cfs_bandwidth.burst;\n\n\treturn tg_set_cfs_bandwidth(tg, period, quota, burst);\n}\n\nstatic long tg_get_cfs_period(struct task_group *tg)\n{\n\tu64 cfs_period_us;\n\n\tcfs_period_us = ktime_to_ns(tg->cfs_bandwidth.period);\n\tdo_div(cfs_period_us, NSEC_PER_USEC);\n\n\treturn cfs_period_us;\n}\n\nstatic int tg_set_cfs_burst(struct task_group *tg, long cfs_burst_us)\n{\n\tu64 quota, period, burst;\n\n\tif ((u64)cfs_burst_us > U64_MAX / NSEC_PER_USEC)\n\t\treturn -EINVAL;\n\n\tburst = (u64)cfs_burst_us * NSEC_PER_USEC;\n\tperiod = ktime_to_ns(tg->cfs_bandwidth.period);\n\tquota = tg->cfs_bandwidth.quota;\n\n\treturn tg_set_cfs_bandwidth(tg, period, quota, burst);\n}\n\nstatic long tg_get_cfs_burst(struct task_group *tg)\n{\n\tu64 burst_us;\n\n\tburst_us = tg->cfs_bandwidth.burst;\n\tdo_div(burst_us, NSEC_PER_USEC);\n\n\treturn burst_us;\n}\n\nstatic s64 cpu_cfs_quota_read_s64(struct cgroup_subsys_state *css,\n\t\t\t\t  struct cftype *cft)\n{\n\treturn tg_get_cfs_quota(css_tg(css));\n}\n\nstatic int cpu_cfs_quota_write_s64(struct cgroup_subsys_state *css,\n\t\t\t\t   struct cftype *cftype, s64 cfs_quota_us)\n{\n\treturn tg_set_cfs_quota(css_tg(css), cfs_quota_us);\n}\n\nstatic u64 cpu_cfs_period_read_u64(struct cgroup_subsys_state *css,\n\t\t\t\t   struct cftype *cft)\n{\n\treturn tg_get_cfs_period(css_tg(css));\n}\n\nstatic int cpu_cfs_period_write_u64(struct cgroup_subsys_state *css,\n\t\t\t\t    struct cftype *cftype, u64 cfs_period_us)\n{\n\treturn tg_set_cfs_period(css_tg(css), cfs_period_us);\n}\n\nstatic u64 cpu_cfs_burst_read_u64(struct cgroup_subsys_state *css,\n\t\t\t\t  struct cftype *cft)\n{\n\treturn tg_get_cfs_burst(css_tg(css));\n}\n\nstatic int cpu_cfs_burst_write_u64(struct cgroup_subsys_state *css,\n\t\t\t\t   struct cftype *cftype, u64 cfs_burst_us)\n{\n\treturn tg_set_cfs_burst(css_tg(css), cfs_burst_us);\n}\n\nstruct cfs_schedulable_data {\n\tstruct task_group *tg;\n\tu64 period, quota;\n};\n\n \nstatic u64 normalize_cfs_quota(struct task_group *tg,\n\t\t\t       struct cfs_schedulable_data *d)\n{\n\tu64 quota, period;\n\n\tif (tg == d->tg) {\n\t\tperiod = d->period;\n\t\tquota = d->quota;\n\t} else {\n\t\tperiod = tg_get_cfs_period(tg);\n\t\tquota = tg_get_cfs_quota(tg);\n\t}\n\n\t \n\tif (quota == RUNTIME_INF || quota == -1)\n\t\treturn RUNTIME_INF;\n\n\treturn to_ratio(period, quota);\n}\n\nstatic int tg_cfs_schedulable_down(struct task_group *tg, void *data)\n{\n\tstruct cfs_schedulable_data *d = data;\n\tstruct cfs_bandwidth *cfs_b = &tg->cfs_bandwidth;\n\ts64 quota = 0, parent_quota = -1;\n\n\tif (!tg->parent) {\n\t\tquota = RUNTIME_INF;\n\t} else {\n\t\tstruct cfs_bandwidth *parent_b = &tg->parent->cfs_bandwidth;\n\n\t\tquota = normalize_cfs_quota(tg, d);\n\t\tparent_quota = parent_b->hierarchical_quota;\n\n\t\t \n\t\tif (cgroup_subsys_on_dfl(cpu_cgrp_subsys)) {\n\t\t\tif (quota == RUNTIME_INF)\n\t\t\t\tquota = parent_quota;\n\t\t\telse if (parent_quota != RUNTIME_INF)\n\t\t\t\tquota = min(quota, parent_quota);\n\t\t} else {\n\t\t\tif (quota == RUNTIME_INF)\n\t\t\t\tquota = parent_quota;\n\t\t\telse if (parent_quota != RUNTIME_INF && quota > parent_quota)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\tcfs_b->hierarchical_quota = quota;\n\n\treturn 0;\n}\n\nstatic int __cfs_schedulable(struct task_group *tg, u64 period, u64 quota)\n{\n\tint ret;\n\tstruct cfs_schedulable_data data = {\n\t\t.tg = tg,\n\t\t.period = period,\n\t\t.quota = quota,\n\t};\n\n\tif (quota != RUNTIME_INF) {\n\t\tdo_div(data.period, NSEC_PER_USEC);\n\t\tdo_div(data.quota, NSEC_PER_USEC);\n\t}\n\n\trcu_read_lock();\n\tret = walk_tg_tree(tg_cfs_schedulable_down, tg_nop, &data);\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\nstatic int cpu_cfs_stat_show(struct seq_file *sf, void *v)\n{\n\tstruct task_group *tg = css_tg(seq_css(sf));\n\tstruct cfs_bandwidth *cfs_b = &tg->cfs_bandwidth;\n\n\tseq_printf(sf, \"nr_periods %d\\n\", cfs_b->nr_periods);\n\tseq_printf(sf, \"nr_throttled %d\\n\", cfs_b->nr_throttled);\n\tseq_printf(sf, \"throttled_time %llu\\n\", cfs_b->throttled_time);\n\n\tif (schedstat_enabled() && tg != &root_task_group) {\n\t\tstruct sched_statistics *stats;\n\t\tu64 ws = 0;\n\t\tint i;\n\n\t\tfor_each_possible_cpu(i) {\n\t\t\tstats = __schedstats_from_se(tg->se[i]);\n\t\t\tws += schedstat_val(stats->wait_sum);\n\t\t}\n\n\t\tseq_printf(sf, \"wait_sum %llu\\n\", ws);\n\t}\n\n\tseq_printf(sf, \"nr_bursts %d\\n\", cfs_b->nr_burst);\n\tseq_printf(sf, \"burst_time %llu\\n\", cfs_b->burst_time);\n\n\treturn 0;\n}\n\nstatic u64 throttled_time_self(struct task_group *tg)\n{\n\tint i;\n\tu64 total = 0;\n\n\tfor_each_possible_cpu(i) {\n\t\ttotal += READ_ONCE(tg->cfs_rq[i]->throttled_clock_self_time);\n\t}\n\n\treturn total;\n}\n\nstatic int cpu_cfs_local_stat_show(struct seq_file *sf, void *v)\n{\n\tstruct task_group *tg = css_tg(seq_css(sf));\n\n\tseq_printf(sf, \"throttled_time %llu\\n\", throttled_time_self(tg));\n\n\treturn 0;\n}\n#endif  \n#endif  \n\n#ifdef CONFIG_RT_GROUP_SCHED\nstatic int cpu_rt_runtime_write(struct cgroup_subsys_state *css,\n\t\t\t\tstruct cftype *cft, s64 val)\n{\n\treturn sched_group_set_rt_runtime(css_tg(css), val);\n}\n\nstatic s64 cpu_rt_runtime_read(struct cgroup_subsys_state *css,\n\t\t\t       struct cftype *cft)\n{\n\treturn sched_group_rt_runtime(css_tg(css));\n}\n\nstatic int cpu_rt_period_write_uint(struct cgroup_subsys_state *css,\n\t\t\t\t    struct cftype *cftype, u64 rt_period_us)\n{\n\treturn sched_group_set_rt_period(css_tg(css), rt_period_us);\n}\n\nstatic u64 cpu_rt_period_read_uint(struct cgroup_subsys_state *css,\n\t\t\t\t   struct cftype *cft)\n{\n\treturn sched_group_rt_period(css_tg(css));\n}\n#endif  \n\n#ifdef CONFIG_FAIR_GROUP_SCHED\nstatic s64 cpu_idle_read_s64(struct cgroup_subsys_state *css,\n\t\t\t       struct cftype *cft)\n{\n\treturn css_tg(css)->idle;\n}\n\nstatic int cpu_idle_write_s64(struct cgroup_subsys_state *css,\n\t\t\t\tstruct cftype *cft, s64 idle)\n{\n\treturn sched_group_set_idle(css_tg(css), idle);\n}\n#endif\n\nstatic struct cftype cpu_legacy_files[] = {\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\t{\n\t\t.name = \"shares\",\n\t\t.read_u64 = cpu_shares_read_u64,\n\t\t.write_u64 = cpu_shares_write_u64,\n\t},\n\t{\n\t\t.name = \"idle\",\n\t\t.read_s64 = cpu_idle_read_s64,\n\t\t.write_s64 = cpu_idle_write_s64,\n\t},\n#endif\n#ifdef CONFIG_CFS_BANDWIDTH\n\t{\n\t\t.name = \"cfs_quota_us\",\n\t\t.read_s64 = cpu_cfs_quota_read_s64,\n\t\t.write_s64 = cpu_cfs_quota_write_s64,\n\t},\n\t{\n\t\t.name = \"cfs_period_us\",\n\t\t.read_u64 = cpu_cfs_period_read_u64,\n\t\t.write_u64 = cpu_cfs_period_write_u64,\n\t},\n\t{\n\t\t.name = \"cfs_burst_us\",\n\t\t.read_u64 = cpu_cfs_burst_read_u64,\n\t\t.write_u64 = cpu_cfs_burst_write_u64,\n\t},\n\t{\n\t\t.name = \"stat\",\n\t\t.seq_show = cpu_cfs_stat_show,\n\t},\n\t{\n\t\t.name = \"stat.local\",\n\t\t.seq_show = cpu_cfs_local_stat_show,\n\t},\n#endif\n#ifdef CONFIG_RT_GROUP_SCHED\n\t{\n\t\t.name = \"rt_runtime_us\",\n\t\t.read_s64 = cpu_rt_runtime_read,\n\t\t.write_s64 = cpu_rt_runtime_write,\n\t},\n\t{\n\t\t.name = \"rt_period_us\",\n\t\t.read_u64 = cpu_rt_period_read_uint,\n\t\t.write_u64 = cpu_rt_period_write_uint,\n\t},\n#endif\n#ifdef CONFIG_UCLAMP_TASK_GROUP\n\t{\n\t\t.name = \"uclamp.min\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.seq_show = cpu_uclamp_min_show,\n\t\t.write = cpu_uclamp_min_write,\n\t},\n\t{\n\t\t.name = \"uclamp.max\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.seq_show = cpu_uclamp_max_show,\n\t\t.write = cpu_uclamp_max_write,\n\t},\n#endif\n\t{ }\t \n};\n\nstatic int cpu_extra_stat_show(struct seq_file *sf,\n\t\t\t       struct cgroup_subsys_state *css)\n{\n#ifdef CONFIG_CFS_BANDWIDTH\n\t{\n\t\tstruct task_group *tg = css_tg(css);\n\t\tstruct cfs_bandwidth *cfs_b = &tg->cfs_bandwidth;\n\t\tu64 throttled_usec, burst_usec;\n\n\t\tthrottled_usec = cfs_b->throttled_time;\n\t\tdo_div(throttled_usec, NSEC_PER_USEC);\n\t\tburst_usec = cfs_b->burst_time;\n\t\tdo_div(burst_usec, NSEC_PER_USEC);\n\n\t\tseq_printf(sf, \"nr_periods %d\\n\"\n\t\t\t   \"nr_throttled %d\\n\"\n\t\t\t   \"throttled_usec %llu\\n\"\n\t\t\t   \"nr_bursts %d\\n\"\n\t\t\t   \"burst_usec %llu\\n\",\n\t\t\t   cfs_b->nr_periods, cfs_b->nr_throttled,\n\t\t\t   throttled_usec, cfs_b->nr_burst, burst_usec);\n\t}\n#endif\n\treturn 0;\n}\n\nstatic int cpu_local_stat_show(struct seq_file *sf,\n\t\t\t       struct cgroup_subsys_state *css)\n{\n#ifdef CONFIG_CFS_BANDWIDTH\n\t{\n\t\tstruct task_group *tg = css_tg(css);\n\t\tu64 throttled_self_usec;\n\n\t\tthrottled_self_usec = throttled_time_self(tg);\n\t\tdo_div(throttled_self_usec, NSEC_PER_USEC);\n\n\t\tseq_printf(sf, \"throttled_usec %llu\\n\",\n\t\t\t   throttled_self_usec);\n\t}\n#endif\n\treturn 0;\n}\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\nstatic u64 cpu_weight_read_u64(struct cgroup_subsys_state *css,\n\t\t\t       struct cftype *cft)\n{\n\tstruct task_group *tg = css_tg(css);\n\tu64 weight = scale_load_down(tg->shares);\n\n\treturn DIV_ROUND_CLOSEST_ULL(weight * CGROUP_WEIGHT_DFL, 1024);\n}\n\nstatic int cpu_weight_write_u64(struct cgroup_subsys_state *css,\n\t\t\t\tstruct cftype *cft, u64 weight)\n{\n\t \n\tif (weight < CGROUP_WEIGHT_MIN || weight > CGROUP_WEIGHT_MAX)\n\t\treturn -ERANGE;\n\n\tweight = DIV_ROUND_CLOSEST_ULL(weight * 1024, CGROUP_WEIGHT_DFL);\n\n\treturn sched_group_set_shares(css_tg(css), scale_load(weight));\n}\n\nstatic s64 cpu_weight_nice_read_s64(struct cgroup_subsys_state *css,\n\t\t\t\t    struct cftype *cft)\n{\n\tunsigned long weight = scale_load_down(css_tg(css)->shares);\n\tint last_delta = INT_MAX;\n\tint prio, delta;\n\n\t \n\tfor (prio = 0; prio < ARRAY_SIZE(sched_prio_to_weight); prio++) {\n\t\tdelta = abs(sched_prio_to_weight[prio] - weight);\n\t\tif (delta >= last_delta)\n\t\t\tbreak;\n\t\tlast_delta = delta;\n\t}\n\n\treturn PRIO_TO_NICE(prio - 1 + MAX_RT_PRIO);\n}\n\nstatic int cpu_weight_nice_write_s64(struct cgroup_subsys_state *css,\n\t\t\t\t     struct cftype *cft, s64 nice)\n{\n\tunsigned long weight;\n\tint idx;\n\n\tif (nice < MIN_NICE || nice > MAX_NICE)\n\t\treturn -ERANGE;\n\n\tidx = NICE_TO_PRIO(nice) - MAX_RT_PRIO;\n\tidx = array_index_nospec(idx, 40);\n\tweight = sched_prio_to_weight[idx];\n\n\treturn sched_group_set_shares(css_tg(css), scale_load(weight));\n}\n#endif\n\nstatic void __maybe_unused cpu_period_quota_print(struct seq_file *sf,\n\t\t\t\t\t\t  long period, long quota)\n{\n\tif (quota < 0)\n\t\tseq_puts(sf, \"max\");\n\telse\n\t\tseq_printf(sf, \"%ld\", quota);\n\n\tseq_printf(sf, \" %ld\\n\", period);\n}\n\n \nstatic int __maybe_unused cpu_period_quota_parse(char *buf,\n\t\t\t\t\t\t u64 *periodp, u64 *quotap)\n{\n\tchar tok[21];\t \n\n\tif (sscanf(buf, \"%20s %llu\", tok, periodp) < 1)\n\t\treturn -EINVAL;\n\n\t*periodp *= NSEC_PER_USEC;\n\n\tif (sscanf(tok, \"%llu\", quotap))\n\t\t*quotap *= NSEC_PER_USEC;\n\telse if (!strcmp(tok, \"max\"))\n\t\t*quotap = RUNTIME_INF;\n\telse\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\n#ifdef CONFIG_CFS_BANDWIDTH\nstatic int cpu_max_show(struct seq_file *sf, void *v)\n{\n\tstruct task_group *tg = css_tg(seq_css(sf));\n\n\tcpu_period_quota_print(sf, tg_get_cfs_period(tg), tg_get_cfs_quota(tg));\n\treturn 0;\n}\n\nstatic ssize_t cpu_max_write(struct kernfs_open_file *of,\n\t\t\t     char *buf, size_t nbytes, loff_t off)\n{\n\tstruct task_group *tg = css_tg(of_css(of));\n\tu64 period = tg_get_cfs_period(tg);\n\tu64 burst = tg_get_cfs_burst(tg);\n\tu64 quota;\n\tint ret;\n\n\tret = cpu_period_quota_parse(buf, &period, &quota);\n\tif (!ret)\n\t\tret = tg_set_cfs_bandwidth(tg, period, quota, burst);\n\treturn ret ?: nbytes;\n}\n#endif\n\nstatic struct cftype cpu_files[] = {\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\t{\n\t\t.name = \"weight\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.read_u64 = cpu_weight_read_u64,\n\t\t.write_u64 = cpu_weight_write_u64,\n\t},\n\t{\n\t\t.name = \"weight.nice\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.read_s64 = cpu_weight_nice_read_s64,\n\t\t.write_s64 = cpu_weight_nice_write_s64,\n\t},\n\t{\n\t\t.name = \"idle\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.read_s64 = cpu_idle_read_s64,\n\t\t.write_s64 = cpu_idle_write_s64,\n\t},\n#endif\n#ifdef CONFIG_CFS_BANDWIDTH\n\t{\n\t\t.name = \"max\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.seq_show = cpu_max_show,\n\t\t.write = cpu_max_write,\n\t},\n\t{\n\t\t.name = \"max.burst\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.read_u64 = cpu_cfs_burst_read_u64,\n\t\t.write_u64 = cpu_cfs_burst_write_u64,\n\t},\n#endif\n#ifdef CONFIG_UCLAMP_TASK_GROUP\n\t{\n\t\t.name = \"uclamp.min\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.seq_show = cpu_uclamp_min_show,\n\t\t.write = cpu_uclamp_min_write,\n\t},\n\t{\n\t\t.name = \"uclamp.max\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.seq_show = cpu_uclamp_max_show,\n\t\t.write = cpu_uclamp_max_write,\n\t},\n#endif\n\t{ }\t \n};\n\nstruct cgroup_subsys cpu_cgrp_subsys = {\n\t.css_alloc\t= cpu_cgroup_css_alloc,\n\t.css_online\t= cpu_cgroup_css_online,\n\t.css_released\t= cpu_cgroup_css_released,\n\t.css_free\t= cpu_cgroup_css_free,\n\t.css_extra_stat_show = cpu_extra_stat_show,\n\t.css_local_stat_show = cpu_local_stat_show,\n#ifdef CONFIG_RT_GROUP_SCHED\n\t.can_attach\t= cpu_cgroup_can_attach,\n#endif\n\t.attach\t\t= cpu_cgroup_attach,\n\t.legacy_cftypes\t= cpu_legacy_files,\n\t.dfl_cftypes\t= cpu_files,\n\t.early_init\t= true,\n\t.threaded\t= true,\n};\n\n#endif\t \n\nvoid dump_cpu_task(int cpu)\n{\n\tif (cpu == smp_processor_id() && in_hardirq()) {\n\t\tstruct pt_regs *regs;\n\n\t\tregs = get_irq_regs();\n\t\tif (regs) {\n\t\t\tshow_regs(regs);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tif (trigger_single_cpu_backtrace(cpu))\n\t\treturn;\n\n\tpr_info(\"Task dump for CPU %d:\\n\", cpu);\n\tsched_show_task(cpu_curr(cpu));\n}\n\n \nconst int sched_prio_to_weight[40] = {\n       88761,     71755,     56483,     46273,     36291,\n       29154,     23254,     18705,     14949,     11916,\n        9548,      7620,      6100,      4904,      3906,\n        3121,      2501,      1991,      1586,      1277,\n        1024,       820,       655,       526,       423,\n         335,       272,       215,       172,       137,\n         110,        87,        70,        56,        45,\n          36,        29,        23,        18,        15,\n};\n\n \nconst u32 sched_prio_to_wmult[40] = {\n       48388,     59856,     76040,     92818,    118348,\n      147320,    184698,    229616,    287308,    360437,\n      449829,    563644,    704093,    875809,   1099582,\n     1376151,   1717300,   2157191,   2708050,   3363326,\n     4194304,   5237765,   6557202,   8165337,  10153587,\n    12820798,  15790321,  19976592,  24970740,  31350126,\n    39045157,  49367440,  61356676,  76695844,  95443717,\n   119304647, 148102320, 186737708, 238609294, 286331153,\n};\n\nvoid call_trace_sched_update_nr_running(struct rq *rq, int count)\n{\n        trace_sched_update_nr_running_tp(rq, count);\n}\n\n#ifdef CONFIG_SCHED_MM_CID\n\n \nDEFINE_RAW_SPINLOCK(cid_lock);\n\n \nint use_cid_lock;\n\n \n\nvoid sched_mm_cid_migrate_from(struct task_struct *t)\n{\n\tt->migrate_from_cpu = task_cpu(t);\n}\n\nstatic\nint __sched_mm_cid_migrate_from_fetch_cid(struct rq *src_rq,\n\t\t\t\t\t  struct task_struct *t,\n\t\t\t\t\t  struct mm_cid *src_pcpu_cid)\n{\n\tstruct mm_struct *mm = t->mm;\n\tstruct task_struct *src_task;\n\tint src_cid, last_mm_cid;\n\n\tif (!mm)\n\t\treturn -1;\n\n\tlast_mm_cid = t->last_mm_cid;\n\t \n\tif (last_mm_cid == -1)\n\t\treturn -1;\n\tsrc_cid = READ_ONCE(src_pcpu_cid->cid);\n\tif (!mm_cid_is_valid(src_cid) || last_mm_cid != src_cid)\n\t\treturn -1;\n\n\t \n\trcu_read_lock();\n\tsrc_task = rcu_dereference(src_rq->curr);\n\tif (READ_ONCE(src_task->mm_cid_active) && src_task->mm == mm) {\n\t\trcu_read_unlock();\n\t\tt->last_mm_cid = -1;\n\t\treturn -1;\n\t}\n\trcu_read_unlock();\n\n\treturn src_cid;\n}\n\nstatic\nint __sched_mm_cid_migrate_from_try_steal_cid(struct rq *src_rq,\n\t\t\t\t\t      struct task_struct *t,\n\t\t\t\t\t      struct mm_cid *src_pcpu_cid,\n\t\t\t\t\t      int src_cid)\n{\n\tstruct task_struct *src_task;\n\tstruct mm_struct *mm = t->mm;\n\tint lazy_cid;\n\n\tif (src_cid == -1)\n\t\treturn -1;\n\n\t \n\tlazy_cid = mm_cid_set_lazy_put(src_cid);\n\tif (!try_cmpxchg(&src_pcpu_cid->cid, &src_cid, lazy_cid))\n\t\treturn -1;\n\n\t \n\n\t \n\trcu_read_lock();\n\tsrc_task = rcu_dereference(src_rq->curr);\n\tif (READ_ONCE(src_task->mm_cid_active) && src_task->mm == mm) {\n\t\trcu_read_unlock();\n\t\t \n\t\tt->last_mm_cid = -1;\n\t\treturn -1;\n\t}\n\trcu_read_unlock();\n\n\t \n\tif (!try_cmpxchg(&src_pcpu_cid->cid, &lazy_cid, MM_CID_UNSET))\n\t\treturn -1;\n\treturn src_cid;\n}\n\n \nvoid sched_mm_cid_migrate_to(struct rq *dst_rq, struct task_struct *t)\n{\n\tstruct mm_cid *src_pcpu_cid, *dst_pcpu_cid;\n\tstruct mm_struct *mm = t->mm;\n\tint src_cid, dst_cid, src_cpu;\n\tstruct rq *src_rq;\n\n\tlockdep_assert_rq_held(dst_rq);\n\n\tif (!mm)\n\t\treturn;\n\tsrc_cpu = t->migrate_from_cpu;\n\tif (src_cpu == -1) {\n\t\tt->last_mm_cid = -1;\n\t\treturn;\n\t}\n\t \n\tdst_pcpu_cid = per_cpu_ptr(mm->pcpu_cid, cpu_of(dst_rq));\n\tdst_cid = READ_ONCE(dst_pcpu_cid->cid);\n\tif (!mm_cid_is_unset(dst_cid) &&\n\t    atomic_read(&mm->mm_users) >= t->nr_cpus_allowed)\n\t\treturn;\n\tsrc_pcpu_cid = per_cpu_ptr(mm->pcpu_cid, src_cpu);\n\tsrc_rq = cpu_rq(src_cpu);\n\tsrc_cid = __sched_mm_cid_migrate_from_fetch_cid(src_rq, t, src_pcpu_cid);\n\tif (src_cid == -1)\n\t\treturn;\n\tsrc_cid = __sched_mm_cid_migrate_from_try_steal_cid(src_rq, t, src_pcpu_cid,\n\t\t\t\t\t\t\t    src_cid);\n\tif (src_cid == -1)\n\t\treturn;\n\tif (!mm_cid_is_unset(dst_cid)) {\n\t\t__mm_cid_put(mm, src_cid);\n\t\treturn;\n\t}\n\t \n\tmm_cid_snapshot_time(dst_rq, mm);\n\tWRITE_ONCE(dst_pcpu_cid->cid, src_cid);\n}\n\nstatic void sched_mm_cid_remote_clear(struct mm_struct *mm, struct mm_cid *pcpu_cid,\n\t\t\t\t      int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tstruct task_struct *t;\n\tunsigned long flags;\n\tint cid, lazy_cid;\n\n\tcid = READ_ONCE(pcpu_cid->cid);\n\tif (!mm_cid_is_valid(cid))\n\t\treturn;\n\n\t \n\tlazy_cid = mm_cid_set_lazy_put(cid);\n\tif (!try_cmpxchg(&pcpu_cid->cid, &cid, lazy_cid))\n\t\treturn;\n\n\t \n\n\t \n\trcu_read_lock();\n\tt = rcu_dereference(rq->curr);\n\tif (READ_ONCE(t->mm_cid_active) && t->mm == mm) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\trcu_read_unlock();\n\n\t \n\tlocal_irq_save(flags);\n\tif (try_cmpxchg(&pcpu_cid->cid, &lazy_cid, MM_CID_UNSET))\n\t\t__mm_cid_put(mm, cid);\n\tlocal_irq_restore(flags);\n}\n\nstatic void sched_mm_cid_remote_clear_old(struct mm_struct *mm, int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tstruct mm_cid *pcpu_cid;\n\tstruct task_struct *curr;\n\tu64 rq_clock;\n\n\t \n\trq_clock = READ_ONCE(rq->clock);\n\tpcpu_cid = per_cpu_ptr(mm->pcpu_cid, cpu);\n\n\t \n\trcu_read_lock();\n\tcurr = rcu_dereference(rq->curr);\n\tif (READ_ONCE(curr->mm_cid_active) && curr->mm == mm) {\n\t\tWRITE_ONCE(pcpu_cid->time, rq_clock);\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\trcu_read_unlock();\n\n\tif (rq_clock < pcpu_cid->time + SCHED_MM_CID_PERIOD_NS)\n\t\treturn;\n\tsched_mm_cid_remote_clear(mm, pcpu_cid, cpu);\n}\n\nstatic void sched_mm_cid_remote_clear_weight(struct mm_struct *mm, int cpu,\n\t\t\t\t\t     int weight)\n{\n\tstruct mm_cid *pcpu_cid;\n\tint cid;\n\n\tpcpu_cid = per_cpu_ptr(mm->pcpu_cid, cpu);\n\tcid = READ_ONCE(pcpu_cid->cid);\n\tif (!mm_cid_is_valid(cid) || cid < weight)\n\t\treturn;\n\tsched_mm_cid_remote_clear(mm, pcpu_cid, cpu);\n}\n\nstatic void task_mm_cid_work(struct callback_head *work)\n{\n\tunsigned long now = jiffies, old_scan, next_scan;\n\tstruct task_struct *t = current;\n\tstruct cpumask *cidmask;\n\tstruct mm_struct *mm;\n\tint weight, cpu;\n\n\tSCHED_WARN_ON(t != container_of(work, struct task_struct, cid_work));\n\n\twork->next = work;\t \n\tif (t->flags & PF_EXITING)\n\t\treturn;\n\tmm = t->mm;\n\tif (!mm)\n\t\treturn;\n\told_scan = READ_ONCE(mm->mm_cid_next_scan);\n\tnext_scan = now + msecs_to_jiffies(MM_CID_SCAN_DELAY);\n\tif (!old_scan) {\n\t\tunsigned long res;\n\n\t\tres = cmpxchg(&mm->mm_cid_next_scan, old_scan, next_scan);\n\t\tif (res != old_scan)\n\t\t\told_scan = res;\n\t\telse\n\t\t\told_scan = next_scan;\n\t}\n\tif (time_before(now, old_scan))\n\t\treturn;\n\tif (!try_cmpxchg(&mm->mm_cid_next_scan, &old_scan, next_scan))\n\t\treturn;\n\tcidmask = mm_cidmask(mm);\n\t \n\tfor_each_possible_cpu(cpu)\n\t\tsched_mm_cid_remote_clear_old(mm, cpu);\n\tweight = cpumask_weight(cidmask);\n\t \n\tfor_each_possible_cpu(cpu)\n\t\tsched_mm_cid_remote_clear_weight(mm, cpu, weight);\n}\n\nvoid init_sched_mm_cid(struct task_struct *t)\n{\n\tstruct mm_struct *mm = t->mm;\n\tint mm_users = 0;\n\n\tif (mm) {\n\t\tmm_users = atomic_read(&mm->mm_users);\n\t\tif (mm_users == 1)\n\t\t\tmm->mm_cid_next_scan = jiffies + msecs_to_jiffies(MM_CID_SCAN_DELAY);\n\t}\n\tt->cid_work.next = &t->cid_work;\t \n\tinit_task_work(&t->cid_work, task_mm_cid_work);\n}\n\nvoid task_tick_mm_cid(struct rq *rq, struct task_struct *curr)\n{\n\tstruct callback_head *work = &curr->cid_work;\n\tunsigned long now = jiffies;\n\n\tif (!curr->mm || (curr->flags & (PF_EXITING | PF_KTHREAD)) ||\n\t    work->next != work)\n\t\treturn;\n\tif (time_before(now, READ_ONCE(curr->mm->mm_cid_next_scan)))\n\t\treturn;\n\ttask_work_add(curr, work, TWA_RESUME);\n}\n\nvoid sched_mm_cid_exit_signals(struct task_struct *t)\n{\n\tstruct mm_struct *mm = t->mm;\n\tstruct rq_flags rf;\n\tstruct rq *rq;\n\n\tif (!mm)\n\t\treturn;\n\n\tpreempt_disable();\n\trq = this_rq();\n\trq_lock_irqsave(rq, &rf);\n\tpreempt_enable_no_resched();\t \n\tWRITE_ONCE(t->mm_cid_active, 0);\n\t \n\tsmp_mb();\n\tmm_cid_put(mm);\n\tt->last_mm_cid = t->mm_cid = -1;\n\trq_unlock_irqrestore(rq, &rf);\n}\n\nvoid sched_mm_cid_before_execve(struct task_struct *t)\n{\n\tstruct mm_struct *mm = t->mm;\n\tstruct rq_flags rf;\n\tstruct rq *rq;\n\n\tif (!mm)\n\t\treturn;\n\n\tpreempt_disable();\n\trq = this_rq();\n\trq_lock_irqsave(rq, &rf);\n\tpreempt_enable_no_resched();\t \n\tWRITE_ONCE(t->mm_cid_active, 0);\n\t \n\tsmp_mb();\n\tmm_cid_put(mm);\n\tt->last_mm_cid = t->mm_cid = -1;\n\trq_unlock_irqrestore(rq, &rf);\n}\n\nvoid sched_mm_cid_after_execve(struct task_struct *t)\n{\n\tstruct mm_struct *mm = t->mm;\n\tstruct rq_flags rf;\n\tstruct rq *rq;\n\n\tif (!mm)\n\t\treturn;\n\n\tpreempt_disable();\n\trq = this_rq();\n\trq_lock_irqsave(rq, &rf);\n\tpreempt_enable_no_resched();\t \n\tWRITE_ONCE(t->mm_cid_active, 1);\n\t \n\tsmp_mb();\n\tt->last_mm_cid = t->mm_cid = mm_cid_get(rq, mm);\n\trq_unlock_irqrestore(rq, &rf);\n\trseq_set_notify_resume(t);\n}\n\nvoid sched_mm_cid_fork(struct task_struct *t)\n{\n\tWARN_ON_ONCE(!t->mm || t->mm_cid != -1);\n\tt->mm_cid_active = 1;\n}\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}