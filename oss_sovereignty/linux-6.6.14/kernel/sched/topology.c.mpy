{
  "module_name": "topology.c",
  "hash_id": "35fd3dd4902811ed987e796ff4108f594f18f158bdc9897fdcae90cb5f083f8f",
  "original_prompt": "Ingested from linux-6.6.14/kernel/sched/topology.c",
  "human_readable_source": "\n \n\n#include <linux/bsearch.h>\n\nDEFINE_MUTEX(sched_domains_mutex);\n\n \nstatic cpumask_var_t sched_domains_tmpmask;\nstatic cpumask_var_t sched_domains_tmpmask2;\n\n#ifdef CONFIG_SCHED_DEBUG\n\nstatic int __init sched_debug_setup(char *str)\n{\n\tsched_debug_verbose = true;\n\n\treturn 0;\n}\nearly_param(\"sched_verbose\", sched_debug_setup);\n\nstatic inline bool sched_debug(void)\n{\n\treturn sched_debug_verbose;\n}\n\n#define SD_FLAG(_name, mflags) [__##_name] = { .meta_flags = mflags, .name = #_name },\nconst struct sd_flag_debug sd_flag_debug[] = {\n#include <linux/sched/sd_flags.h>\n};\n#undef SD_FLAG\n\nstatic int sched_domain_debug_one(struct sched_domain *sd, int cpu, int level,\n\t\t\t\t  struct cpumask *groupmask)\n{\n\tstruct sched_group *group = sd->groups;\n\tunsigned long flags = sd->flags;\n\tunsigned int idx;\n\n\tcpumask_clear(groupmask);\n\n\tprintk(KERN_DEBUG \"%*s domain-%d: \", level, \"\", level);\n\tprintk(KERN_CONT \"span=%*pbl level=%s\\n\",\n\t       cpumask_pr_args(sched_domain_span(sd)), sd->name);\n\n\tif (!cpumask_test_cpu(cpu, sched_domain_span(sd))) {\n\t\tprintk(KERN_ERR \"ERROR: domain->span does not contain CPU%d\\n\", cpu);\n\t}\n\tif (group && !cpumask_test_cpu(cpu, sched_group_span(group))) {\n\t\tprintk(KERN_ERR \"ERROR: domain->groups does not contain CPU%d\\n\", cpu);\n\t}\n\n\tfor_each_set_bit(idx, &flags, __SD_FLAG_CNT) {\n\t\tunsigned int flag = BIT(idx);\n\t\tunsigned int meta_flags = sd_flag_debug[idx].meta_flags;\n\n\t\tif ((meta_flags & SDF_SHARED_CHILD) && sd->child &&\n\t\t    !(sd->child->flags & flag))\n\t\t\tprintk(KERN_ERR \"ERROR: flag %s set here but not in child\\n\",\n\t\t\t       sd_flag_debug[idx].name);\n\n\t\tif ((meta_flags & SDF_SHARED_PARENT) && sd->parent &&\n\t\t    !(sd->parent->flags & flag))\n\t\t\tprintk(KERN_ERR \"ERROR: flag %s set here but not in parent\\n\",\n\t\t\t       sd_flag_debug[idx].name);\n\t}\n\n\tprintk(KERN_DEBUG \"%*s groups:\", level + 1, \"\");\n\tdo {\n\t\tif (!group) {\n\t\t\tprintk(\"\\n\");\n\t\t\tprintk(KERN_ERR \"ERROR: group is NULL\\n\");\n\t\t\tbreak;\n\t\t}\n\n\t\tif (cpumask_empty(sched_group_span(group))) {\n\t\t\tprintk(KERN_CONT \"\\n\");\n\t\t\tprintk(KERN_ERR \"ERROR: empty group\\n\");\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!(sd->flags & SD_OVERLAP) &&\n\t\t    cpumask_intersects(groupmask, sched_group_span(group))) {\n\t\t\tprintk(KERN_CONT \"\\n\");\n\t\t\tprintk(KERN_ERR \"ERROR: repeated CPUs\\n\");\n\t\t\tbreak;\n\t\t}\n\n\t\tcpumask_or(groupmask, groupmask, sched_group_span(group));\n\n\t\tprintk(KERN_CONT \" %d:{ span=%*pbl\",\n\t\t\t\tgroup->sgc->id,\n\t\t\t\tcpumask_pr_args(sched_group_span(group)));\n\n\t\tif ((sd->flags & SD_OVERLAP) &&\n\t\t    !cpumask_equal(group_balance_mask(group), sched_group_span(group))) {\n\t\t\tprintk(KERN_CONT \" mask=%*pbl\",\n\t\t\t\tcpumask_pr_args(group_balance_mask(group)));\n\t\t}\n\n\t\tif (group->sgc->capacity != SCHED_CAPACITY_SCALE)\n\t\t\tprintk(KERN_CONT \" cap=%lu\", group->sgc->capacity);\n\n\t\tif (group == sd->groups && sd->child &&\n\t\t    !cpumask_equal(sched_domain_span(sd->child),\n\t\t\t\t   sched_group_span(group))) {\n\t\t\tprintk(KERN_ERR \"ERROR: domain->groups does not match domain->child\\n\");\n\t\t}\n\n\t\tprintk(KERN_CONT \" }\");\n\n\t\tgroup = group->next;\n\n\t\tif (group != sd->groups)\n\t\t\tprintk(KERN_CONT \",\");\n\n\t} while (group != sd->groups);\n\tprintk(KERN_CONT \"\\n\");\n\n\tif (!cpumask_equal(sched_domain_span(sd), groupmask))\n\t\tprintk(KERN_ERR \"ERROR: groups don't span domain->span\\n\");\n\n\tif (sd->parent &&\n\t    !cpumask_subset(groupmask, sched_domain_span(sd->parent)))\n\t\tprintk(KERN_ERR \"ERROR: parent span is not a superset of domain->span\\n\");\n\treturn 0;\n}\n\nstatic void sched_domain_debug(struct sched_domain *sd, int cpu)\n{\n\tint level = 0;\n\n\tif (!sched_debug_verbose)\n\t\treturn;\n\n\tif (!sd) {\n\t\tprintk(KERN_DEBUG \"CPU%d attaching NULL sched-domain.\\n\", cpu);\n\t\treturn;\n\t}\n\n\tprintk(KERN_DEBUG \"CPU%d attaching sched-domain(s):\\n\", cpu);\n\n\tfor (;;) {\n\t\tif (sched_domain_debug_one(sd, cpu, level, sched_domains_tmpmask))\n\t\t\tbreak;\n\t\tlevel++;\n\t\tsd = sd->parent;\n\t\tif (!sd)\n\t\t\tbreak;\n\t}\n}\n#else  \n\n# define sched_debug_verbose 0\n# define sched_domain_debug(sd, cpu) do { } while (0)\nstatic inline bool sched_debug(void)\n{\n\treturn false;\n}\n#endif  \n\n \n#define SD_FLAG(name, mflags) (name * !!((mflags) & SDF_NEEDS_GROUPS)) |\nstatic const unsigned int SD_DEGENERATE_GROUPS_MASK =\n#include <linux/sched/sd_flags.h>\n0;\n#undef SD_FLAG\n\nstatic int sd_degenerate(struct sched_domain *sd)\n{\n\tif (cpumask_weight(sched_domain_span(sd)) == 1)\n\t\treturn 1;\n\n\t \n\tif ((sd->flags & SD_DEGENERATE_GROUPS_MASK) &&\n\t    (sd->groups != sd->groups->next))\n\t\treturn 0;\n\n\t \n\tif (sd->flags & (SD_WAKE_AFFINE))\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic int\nsd_parent_degenerate(struct sched_domain *sd, struct sched_domain *parent)\n{\n\tunsigned long cflags = sd->flags, pflags = parent->flags;\n\n\tif (sd_degenerate(parent))\n\t\treturn 1;\n\n\tif (!cpumask_equal(sched_domain_span(sd), sched_domain_span(parent)))\n\t\treturn 0;\n\n\t \n\tif (parent->groups == parent->groups->next)\n\t\tpflags &= ~SD_DEGENERATE_GROUPS_MASK;\n\n\tif (~cflags & pflags)\n\t\treturn 0;\n\n\treturn 1;\n}\n\n#if defined(CONFIG_ENERGY_MODEL) && defined(CONFIG_CPU_FREQ_GOV_SCHEDUTIL)\nDEFINE_STATIC_KEY_FALSE(sched_energy_present);\nstatic unsigned int sysctl_sched_energy_aware = 1;\nstatic DEFINE_MUTEX(sched_energy_mutex);\nstatic bool sched_energy_update;\n\nvoid rebuild_sched_domains_energy(void)\n{\n\tmutex_lock(&sched_energy_mutex);\n\tsched_energy_update = true;\n\trebuild_sched_domains();\n\tsched_energy_update = false;\n\tmutex_unlock(&sched_energy_mutex);\n}\n\n#ifdef CONFIG_PROC_SYSCTL\nstatic int sched_energy_aware_handler(struct ctl_table *table, int write,\n\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tint ret, state;\n\n\tif (write && !capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);\n\tif (!ret && write) {\n\t\tstate = static_branch_unlikely(&sched_energy_present);\n\t\tif (state != sysctl_sched_energy_aware)\n\t\t\trebuild_sched_domains_energy();\n\t}\n\n\treturn ret;\n}\n\nstatic struct ctl_table sched_energy_aware_sysctls[] = {\n\t{\n\t\t.procname       = \"sched_energy_aware\",\n\t\t.data           = &sysctl_sched_energy_aware,\n\t\t.maxlen         = sizeof(unsigned int),\n\t\t.mode           = 0644,\n\t\t.proc_handler   = sched_energy_aware_handler,\n\t\t.extra1         = SYSCTL_ZERO,\n\t\t.extra2         = SYSCTL_ONE,\n\t},\n\t{}\n};\n\nstatic int __init sched_energy_aware_sysctl_init(void)\n{\n\tregister_sysctl_init(\"kernel\", sched_energy_aware_sysctls);\n\treturn 0;\n}\n\nlate_initcall(sched_energy_aware_sysctl_init);\n#endif\n\nstatic void free_pd(struct perf_domain *pd)\n{\n\tstruct perf_domain *tmp;\n\n\twhile (pd) {\n\t\ttmp = pd->next;\n\t\tkfree(pd);\n\t\tpd = tmp;\n\t}\n}\n\nstatic struct perf_domain *find_pd(struct perf_domain *pd, int cpu)\n{\n\twhile (pd) {\n\t\tif (cpumask_test_cpu(cpu, perf_domain_span(pd)))\n\t\t\treturn pd;\n\t\tpd = pd->next;\n\t}\n\n\treturn NULL;\n}\n\nstatic struct perf_domain *pd_init(int cpu)\n{\n\tstruct em_perf_domain *obj = em_cpu_get(cpu);\n\tstruct perf_domain *pd;\n\n\tif (!obj) {\n\t\tif (sched_debug())\n\t\t\tpr_info(\"%s: no EM found for CPU%d\\n\", __func__, cpu);\n\t\treturn NULL;\n\t}\n\n\tpd = kzalloc(sizeof(*pd), GFP_KERNEL);\n\tif (!pd)\n\t\treturn NULL;\n\tpd->em_pd = obj;\n\n\treturn pd;\n}\n\nstatic void perf_domain_debug(const struct cpumask *cpu_map,\n\t\t\t\t\t\tstruct perf_domain *pd)\n{\n\tif (!sched_debug() || !pd)\n\t\treturn;\n\n\tprintk(KERN_DEBUG \"root_domain %*pbl:\", cpumask_pr_args(cpu_map));\n\n\twhile (pd) {\n\t\tprintk(KERN_CONT \" pd%d:{ cpus=%*pbl nr_pstate=%d }\",\n\t\t\t\tcpumask_first(perf_domain_span(pd)),\n\t\t\t\tcpumask_pr_args(perf_domain_span(pd)),\n\t\t\t\tem_pd_nr_perf_states(pd->em_pd));\n\t\tpd = pd->next;\n\t}\n\n\tprintk(KERN_CONT \"\\n\");\n}\n\nstatic void destroy_perf_domain_rcu(struct rcu_head *rp)\n{\n\tstruct perf_domain *pd;\n\n\tpd = container_of(rp, struct perf_domain, rcu);\n\tfree_pd(pd);\n}\n\nstatic void sched_energy_set(bool has_eas)\n{\n\tif (!has_eas && static_branch_unlikely(&sched_energy_present)) {\n\t\tif (sched_debug())\n\t\t\tpr_info(\"%s: stopping EAS\\n\", __func__);\n\t\tstatic_branch_disable_cpuslocked(&sched_energy_present);\n\t} else if (has_eas && !static_branch_unlikely(&sched_energy_present)) {\n\t\tif (sched_debug())\n\t\t\tpr_info(\"%s: starting EAS\\n\", __func__);\n\t\tstatic_branch_enable_cpuslocked(&sched_energy_present);\n\t}\n}\n\n \n#define EM_MAX_COMPLEXITY 2048\n\nextern struct cpufreq_governor schedutil_gov;\nstatic bool build_perf_domains(const struct cpumask *cpu_map)\n{\n\tint i, nr_pd = 0, nr_ps = 0, nr_cpus = cpumask_weight(cpu_map);\n\tstruct perf_domain *pd = NULL, *tmp;\n\tint cpu = cpumask_first(cpu_map);\n\tstruct root_domain *rd = cpu_rq(cpu)->rd;\n\tstruct cpufreq_policy *policy;\n\tstruct cpufreq_governor *gov;\n\n\tif (!sysctl_sched_energy_aware)\n\t\tgoto free;\n\n\t \n\tif (!per_cpu(sd_asym_cpucapacity, cpu)) {\n\t\tif (sched_debug()) {\n\t\t\tpr_info(\"rd %*pbl: CPUs do not have asymmetric capacities\\n\",\n\t\t\t\t\tcpumask_pr_args(cpu_map));\n\t\t}\n\t\tgoto free;\n\t}\n\n\t \n\tif (sched_smt_active()) {\n\t\tpr_warn(\"rd %*pbl: Disabling EAS, SMT is not supported\\n\",\n\t\t\tcpumask_pr_args(cpu_map));\n\t\tgoto free;\n\t}\n\n\tif (!arch_scale_freq_invariant()) {\n\t\tif (sched_debug()) {\n\t\t\tpr_warn(\"rd %*pbl: Disabling EAS: frequency-invariant load tracking not yet supported\",\n\t\t\t\tcpumask_pr_args(cpu_map));\n\t\t}\n\t\tgoto free;\n\t}\n\n\tfor_each_cpu(i, cpu_map) {\n\t\t \n\t\tif (find_pd(pd, i))\n\t\t\tcontinue;\n\n\t\t \n\t\tpolicy = cpufreq_cpu_get(i);\n\t\tif (!policy)\n\t\t\tgoto free;\n\t\tgov = policy->governor;\n\t\tcpufreq_cpu_put(policy);\n\t\tif (gov != &schedutil_gov) {\n\t\t\tif (rd->pd)\n\t\t\t\tpr_warn(\"rd %*pbl: Disabling EAS, schedutil is mandatory\\n\",\n\t\t\t\t\t\tcpumask_pr_args(cpu_map));\n\t\t\tgoto free;\n\t\t}\n\n\t\t \n\t\ttmp = pd_init(i);\n\t\tif (!tmp)\n\t\t\tgoto free;\n\t\ttmp->next = pd;\n\t\tpd = tmp;\n\n\t\t \n\t\tnr_pd++;\n\t\tnr_ps += em_pd_nr_perf_states(pd->em_pd);\n\t}\n\n\t \n\tif (nr_pd * (nr_ps + nr_cpus) > EM_MAX_COMPLEXITY) {\n\t\tWARN(1, \"rd %*pbl: Failed to start EAS, EM complexity is too high\\n\",\n\t\t\t\t\t\tcpumask_pr_args(cpu_map));\n\t\tgoto free;\n\t}\n\n\tperf_domain_debug(cpu_map, pd);\n\n\t \n\ttmp = rd->pd;\n\trcu_assign_pointer(rd->pd, pd);\n\tif (tmp)\n\t\tcall_rcu(&tmp->rcu, destroy_perf_domain_rcu);\n\n\treturn !!pd;\n\nfree:\n\tfree_pd(pd);\n\ttmp = rd->pd;\n\trcu_assign_pointer(rd->pd, NULL);\n\tif (tmp)\n\t\tcall_rcu(&tmp->rcu, destroy_perf_domain_rcu);\n\n\treturn false;\n}\n#else\nstatic void free_pd(struct perf_domain *pd) { }\n#endif  \n\nstatic void free_rootdomain(struct rcu_head *rcu)\n{\n\tstruct root_domain *rd = container_of(rcu, struct root_domain, rcu);\n\n\tcpupri_cleanup(&rd->cpupri);\n\tcpudl_cleanup(&rd->cpudl);\n\tfree_cpumask_var(rd->dlo_mask);\n\tfree_cpumask_var(rd->rto_mask);\n\tfree_cpumask_var(rd->online);\n\tfree_cpumask_var(rd->span);\n\tfree_pd(rd->pd);\n\tkfree(rd);\n}\n\nvoid rq_attach_root(struct rq *rq, struct root_domain *rd)\n{\n\tstruct root_domain *old_rd = NULL;\n\tstruct rq_flags rf;\n\n\trq_lock_irqsave(rq, &rf);\n\n\tif (rq->rd) {\n\t\told_rd = rq->rd;\n\n\t\tif (cpumask_test_cpu(rq->cpu, old_rd->online))\n\t\t\tset_rq_offline(rq);\n\n\t\tcpumask_clear_cpu(rq->cpu, old_rd->span);\n\n\t\t \n\t\tif (!atomic_dec_and_test(&old_rd->refcount))\n\t\t\told_rd = NULL;\n\t}\n\n\tatomic_inc(&rd->refcount);\n\trq->rd = rd;\n\n\tcpumask_set_cpu(rq->cpu, rd->span);\n\tif (cpumask_test_cpu(rq->cpu, cpu_active_mask))\n\t\tset_rq_online(rq);\n\n\trq_unlock_irqrestore(rq, &rf);\n\n\tif (old_rd)\n\t\tcall_rcu(&old_rd->rcu, free_rootdomain);\n}\n\nvoid sched_get_rd(struct root_domain *rd)\n{\n\tatomic_inc(&rd->refcount);\n}\n\nvoid sched_put_rd(struct root_domain *rd)\n{\n\tif (!atomic_dec_and_test(&rd->refcount))\n\t\treturn;\n\n\tcall_rcu(&rd->rcu, free_rootdomain);\n}\n\nstatic int init_rootdomain(struct root_domain *rd)\n{\n\tif (!zalloc_cpumask_var(&rd->span, GFP_KERNEL))\n\t\tgoto out;\n\tif (!zalloc_cpumask_var(&rd->online, GFP_KERNEL))\n\t\tgoto free_span;\n\tif (!zalloc_cpumask_var(&rd->dlo_mask, GFP_KERNEL))\n\t\tgoto free_online;\n\tif (!zalloc_cpumask_var(&rd->rto_mask, GFP_KERNEL))\n\t\tgoto free_dlo_mask;\n\n#ifdef HAVE_RT_PUSH_IPI\n\trd->rto_cpu = -1;\n\traw_spin_lock_init(&rd->rto_lock);\n\trd->rto_push_work = IRQ_WORK_INIT_HARD(rto_push_irq_work_func);\n#endif\n\n\trd->visit_gen = 0;\n\tinit_dl_bw(&rd->dl_bw);\n\tif (cpudl_init(&rd->cpudl) != 0)\n\t\tgoto free_rto_mask;\n\n\tif (cpupri_init(&rd->cpupri) != 0)\n\t\tgoto free_cpudl;\n\treturn 0;\n\nfree_cpudl:\n\tcpudl_cleanup(&rd->cpudl);\nfree_rto_mask:\n\tfree_cpumask_var(rd->rto_mask);\nfree_dlo_mask:\n\tfree_cpumask_var(rd->dlo_mask);\nfree_online:\n\tfree_cpumask_var(rd->online);\nfree_span:\n\tfree_cpumask_var(rd->span);\nout:\n\treturn -ENOMEM;\n}\n\n \nstruct root_domain def_root_domain;\n\nvoid __init init_defrootdomain(void)\n{\n\tinit_rootdomain(&def_root_domain);\n\n\tatomic_set(&def_root_domain.refcount, 1);\n}\n\nstatic struct root_domain *alloc_rootdomain(void)\n{\n\tstruct root_domain *rd;\n\n\trd = kzalloc(sizeof(*rd), GFP_KERNEL);\n\tif (!rd)\n\t\treturn NULL;\n\n\tif (init_rootdomain(rd) != 0) {\n\t\tkfree(rd);\n\t\treturn NULL;\n\t}\n\n\treturn rd;\n}\n\nstatic void free_sched_groups(struct sched_group *sg, int free_sgc)\n{\n\tstruct sched_group *tmp, *first;\n\n\tif (!sg)\n\t\treturn;\n\n\tfirst = sg;\n\tdo {\n\t\ttmp = sg->next;\n\n\t\tif (free_sgc && atomic_dec_and_test(&sg->sgc->ref))\n\t\t\tkfree(sg->sgc);\n\n\t\tif (atomic_dec_and_test(&sg->ref))\n\t\t\tkfree(sg);\n\t\tsg = tmp;\n\t} while (sg != first);\n}\n\nstatic void destroy_sched_domain(struct sched_domain *sd)\n{\n\t \n\tfree_sched_groups(sd->groups, 1);\n\n\tif (sd->shared && atomic_dec_and_test(&sd->shared->ref))\n\t\tkfree(sd->shared);\n\tkfree(sd);\n}\n\nstatic void destroy_sched_domains_rcu(struct rcu_head *rcu)\n{\n\tstruct sched_domain *sd = container_of(rcu, struct sched_domain, rcu);\n\n\twhile (sd) {\n\t\tstruct sched_domain *parent = sd->parent;\n\t\tdestroy_sched_domain(sd);\n\t\tsd = parent;\n\t}\n}\n\nstatic void destroy_sched_domains(struct sched_domain *sd)\n{\n\tif (sd)\n\t\tcall_rcu(&sd->rcu, destroy_sched_domains_rcu);\n}\n\n \nDEFINE_PER_CPU(struct sched_domain __rcu *, sd_llc);\nDEFINE_PER_CPU(int, sd_llc_size);\nDEFINE_PER_CPU(int, sd_llc_id);\nDEFINE_PER_CPU(struct sched_domain_shared __rcu *, sd_llc_shared);\nDEFINE_PER_CPU(struct sched_domain __rcu *, sd_numa);\nDEFINE_PER_CPU(struct sched_domain __rcu *, sd_asym_packing);\nDEFINE_PER_CPU(struct sched_domain __rcu *, sd_asym_cpucapacity);\nDEFINE_STATIC_KEY_FALSE(sched_asym_cpucapacity);\n\nstatic void update_top_cache_domain(int cpu)\n{\n\tstruct sched_domain_shared *sds = NULL;\n\tstruct sched_domain *sd;\n\tint id = cpu;\n\tint size = 1;\n\n\tsd = highest_flag_domain(cpu, SD_SHARE_PKG_RESOURCES);\n\tif (sd) {\n\t\tid = cpumask_first(sched_domain_span(sd));\n\t\tsize = cpumask_weight(sched_domain_span(sd));\n\t\tsds = sd->shared;\n\t}\n\n\trcu_assign_pointer(per_cpu(sd_llc, cpu), sd);\n\tper_cpu(sd_llc_size, cpu) = size;\n\tper_cpu(sd_llc_id, cpu) = id;\n\trcu_assign_pointer(per_cpu(sd_llc_shared, cpu), sds);\n\n\tsd = lowest_flag_domain(cpu, SD_NUMA);\n\trcu_assign_pointer(per_cpu(sd_numa, cpu), sd);\n\n\tsd = highest_flag_domain(cpu, SD_ASYM_PACKING);\n\trcu_assign_pointer(per_cpu(sd_asym_packing, cpu), sd);\n\n\tsd = lowest_flag_domain(cpu, SD_ASYM_CPUCAPACITY_FULL);\n\trcu_assign_pointer(per_cpu(sd_asym_cpucapacity, cpu), sd);\n}\n\n \nstatic void\ncpu_attach_domain(struct sched_domain *sd, struct root_domain *rd, int cpu)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\tstruct sched_domain *tmp;\n\n\t \n\tfor (tmp = sd; tmp; ) {\n\t\tstruct sched_domain *parent = tmp->parent;\n\t\tif (!parent)\n\t\t\tbreak;\n\n\t\tif (sd_parent_degenerate(tmp, parent)) {\n\t\t\ttmp->parent = parent->parent;\n\n\t\t\tif (parent->parent) {\n\t\t\t\tparent->parent->child = tmp;\n\t\t\t\tparent->parent->groups->flags = tmp->flags;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (parent->flags & SD_PREFER_SIBLING)\n\t\t\t\ttmp->flags |= SD_PREFER_SIBLING;\n\t\t\tdestroy_sched_domain(parent);\n\t\t} else\n\t\t\ttmp = tmp->parent;\n\t}\n\n\tif (sd && sd_degenerate(sd)) {\n\t\ttmp = sd;\n\t\tsd = sd->parent;\n\t\tdestroy_sched_domain(tmp);\n\t\tif (sd) {\n\t\t\tstruct sched_group *sg = sd->groups;\n\n\t\t\t \n\t\t\tdo {\n\t\t\t\tsg->flags = 0;\n\t\t\t} while (sg != sd->groups);\n\n\t\t\tsd->child = NULL;\n\t\t}\n\t}\n\n\tsched_domain_debug(sd, cpu);\n\n\trq_attach_root(rq, rd);\n\ttmp = rq->sd;\n\trcu_assign_pointer(rq->sd, sd);\n\tdirty_sched_domain_sysctl(cpu);\n\tdestroy_sched_domains(tmp);\n\n\tupdate_top_cache_domain(cpu);\n}\n\nstruct s_data {\n\tstruct sched_domain * __percpu *sd;\n\tstruct root_domain\t*rd;\n};\n\nenum s_alloc {\n\tsa_rootdomain,\n\tsa_sd,\n\tsa_sd_storage,\n\tsa_none,\n};\n\n \nint group_balance_cpu(struct sched_group *sg)\n{\n\treturn cpumask_first(group_balance_mask(sg));\n}\n\n\n \n\n\n \nstatic void\nbuild_balance_mask(struct sched_domain *sd, struct sched_group *sg, struct cpumask *mask)\n{\n\tconst struct cpumask *sg_span = sched_group_span(sg);\n\tstruct sd_data *sdd = sd->private;\n\tstruct sched_domain *sibling;\n\tint i;\n\n\tcpumask_clear(mask);\n\n\tfor_each_cpu(i, sg_span) {\n\t\tsibling = *per_cpu_ptr(sdd->sd, i);\n\n\t\t \n\t\tif (!sibling->child)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (!cpumask_equal(sg_span, sched_domain_span(sibling->child)))\n\t\t\tcontinue;\n\n\t\tcpumask_set_cpu(i, mask);\n\t}\n\n\t \n\tWARN_ON_ONCE(cpumask_empty(mask));\n}\n\n \nstatic struct sched_group *\nbuild_group_from_child_sched_domain(struct sched_domain *sd, int cpu)\n{\n\tstruct sched_group *sg;\n\tstruct cpumask *sg_span;\n\n\tsg = kzalloc_node(sizeof(struct sched_group) + cpumask_size(),\n\t\t\tGFP_KERNEL, cpu_to_node(cpu));\n\n\tif (!sg)\n\t\treturn NULL;\n\n\tsg_span = sched_group_span(sg);\n\tif (sd->child) {\n\t\tcpumask_copy(sg_span, sched_domain_span(sd->child));\n\t\tsg->flags = sd->child->flags;\n\t} else {\n\t\tcpumask_copy(sg_span, sched_domain_span(sd));\n\t}\n\n\tatomic_inc(&sg->ref);\n\treturn sg;\n}\n\nstatic void init_overlap_sched_group(struct sched_domain *sd,\n\t\t\t\t     struct sched_group *sg)\n{\n\tstruct cpumask *mask = sched_domains_tmpmask2;\n\tstruct sd_data *sdd = sd->private;\n\tstruct cpumask *sg_span;\n\tint cpu;\n\n\tbuild_balance_mask(sd, sg, mask);\n\tcpu = cpumask_first(mask);\n\n\tsg->sgc = *per_cpu_ptr(sdd->sgc, cpu);\n\tif (atomic_inc_return(&sg->sgc->ref) == 1)\n\t\tcpumask_copy(group_balance_mask(sg), mask);\n\telse\n\t\tWARN_ON_ONCE(!cpumask_equal(group_balance_mask(sg), mask));\n\n\t \n\tsg_span = sched_group_span(sg);\n\tsg->sgc->capacity = SCHED_CAPACITY_SCALE * cpumask_weight(sg_span);\n\tsg->sgc->min_capacity = SCHED_CAPACITY_SCALE;\n\tsg->sgc->max_capacity = SCHED_CAPACITY_SCALE;\n}\n\nstatic struct sched_domain *\nfind_descended_sibling(struct sched_domain *sd, struct sched_domain *sibling)\n{\n\t \n\twhile (sibling->child &&\n\t       !cpumask_subset(sched_domain_span(sibling->child),\n\t\t\t       sched_domain_span(sd)))\n\t\tsibling = sibling->child;\n\n\t \n\twhile (sibling->child &&\n\t       cpumask_equal(sched_domain_span(sibling->child),\n\t\t\t     sched_domain_span(sibling)))\n\t\tsibling = sibling->child;\n\n\treturn sibling;\n}\n\nstatic int\nbuild_overlap_sched_groups(struct sched_domain *sd, int cpu)\n{\n\tstruct sched_group *first = NULL, *last = NULL, *sg;\n\tconst struct cpumask *span = sched_domain_span(sd);\n\tstruct cpumask *covered = sched_domains_tmpmask;\n\tstruct sd_data *sdd = sd->private;\n\tstruct sched_domain *sibling;\n\tint i;\n\n\tcpumask_clear(covered);\n\n\tfor_each_cpu_wrap(i, span, cpu) {\n\t\tstruct cpumask *sg_span;\n\n\t\tif (cpumask_test_cpu(i, covered))\n\t\t\tcontinue;\n\n\t\tsibling = *per_cpu_ptr(sdd->sd, i);\n\n\t\t \n\t\tif (!cpumask_test_cpu(i, sched_domain_span(sibling)))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (sibling->child &&\n\t\t    !cpumask_subset(sched_domain_span(sibling->child), span))\n\t\t\tsibling = find_descended_sibling(sd, sibling);\n\n\t\tsg = build_group_from_child_sched_domain(sibling, cpu);\n\t\tif (!sg)\n\t\t\tgoto fail;\n\n\t\tsg_span = sched_group_span(sg);\n\t\tcpumask_or(covered, covered, sg_span);\n\n\t\tinit_overlap_sched_group(sibling, sg);\n\n\t\tif (!first)\n\t\t\tfirst = sg;\n\t\tif (last)\n\t\t\tlast->next = sg;\n\t\tlast = sg;\n\t\tlast->next = first;\n\t}\n\tsd->groups = first;\n\n\treturn 0;\n\nfail:\n\tfree_sched_groups(first, 0);\n\n\treturn -ENOMEM;\n}\n\n\n \n\nstatic struct sched_group *get_group(int cpu, struct sd_data *sdd)\n{\n\tstruct sched_domain *sd = *per_cpu_ptr(sdd->sd, cpu);\n\tstruct sched_domain *child = sd->child;\n\tstruct sched_group *sg;\n\tbool already_visited;\n\n\tif (child)\n\t\tcpu = cpumask_first(sched_domain_span(child));\n\n\tsg = *per_cpu_ptr(sdd->sg, cpu);\n\tsg->sgc = *per_cpu_ptr(sdd->sgc, cpu);\n\n\t \n\talready_visited = atomic_inc_return(&sg->ref) > 1;\n\t \n\tWARN_ON(already_visited != (atomic_inc_return(&sg->sgc->ref) > 1));\n\n\t \n\tif (already_visited)\n\t\treturn sg;\n\n\tif (child) {\n\t\tcpumask_copy(sched_group_span(sg), sched_domain_span(child));\n\t\tcpumask_copy(group_balance_mask(sg), sched_group_span(sg));\n\t\tsg->flags = child->flags;\n\t} else {\n\t\tcpumask_set_cpu(cpu, sched_group_span(sg));\n\t\tcpumask_set_cpu(cpu, group_balance_mask(sg));\n\t}\n\n\tsg->sgc->capacity = SCHED_CAPACITY_SCALE * cpumask_weight(sched_group_span(sg));\n\tsg->sgc->min_capacity = SCHED_CAPACITY_SCALE;\n\tsg->sgc->max_capacity = SCHED_CAPACITY_SCALE;\n\n\treturn sg;\n}\n\n \nstatic int\nbuild_sched_groups(struct sched_domain *sd, int cpu)\n{\n\tstruct sched_group *first = NULL, *last = NULL;\n\tstruct sd_data *sdd = sd->private;\n\tconst struct cpumask *span = sched_domain_span(sd);\n\tstruct cpumask *covered;\n\tint i;\n\n\tlockdep_assert_held(&sched_domains_mutex);\n\tcovered = sched_domains_tmpmask;\n\n\tcpumask_clear(covered);\n\n\tfor_each_cpu_wrap(i, span, cpu) {\n\t\tstruct sched_group *sg;\n\n\t\tif (cpumask_test_cpu(i, covered))\n\t\t\tcontinue;\n\n\t\tsg = get_group(i, sdd);\n\n\t\tcpumask_or(covered, covered, sched_group_span(sg));\n\n\t\tif (!first)\n\t\t\tfirst = sg;\n\t\tif (last)\n\t\t\tlast->next = sg;\n\t\tlast = sg;\n\t}\n\tlast->next = first;\n\tsd->groups = first;\n\n\treturn 0;\n}\n\n \nstatic void init_sched_groups_capacity(int cpu, struct sched_domain *sd)\n{\n\tstruct sched_group *sg = sd->groups;\n\tstruct cpumask *mask = sched_domains_tmpmask2;\n\n\tWARN_ON(!sg);\n\n\tdo {\n\t\tint cpu, cores = 0, max_cpu = -1;\n\n\t\tsg->group_weight = cpumask_weight(sched_group_span(sg));\n\n\t\tcpumask_copy(mask, sched_group_span(sg));\n\t\tfor_each_cpu(cpu, mask) {\n\t\t\tcores++;\n#ifdef CONFIG_SCHED_SMT\n\t\t\tcpumask_andnot(mask, mask, cpu_smt_mask(cpu));\n#endif\n\t\t}\n\t\tsg->cores = cores;\n\n\t\tif (!(sd->flags & SD_ASYM_PACKING))\n\t\t\tgoto next;\n\n\t\tfor_each_cpu(cpu, sched_group_span(sg)) {\n\t\t\tif (max_cpu < 0)\n\t\t\t\tmax_cpu = cpu;\n\t\t\telse if (sched_asym_prefer(cpu, max_cpu))\n\t\t\t\tmax_cpu = cpu;\n\t\t}\n\t\tsg->asym_prefer_cpu = max_cpu;\n\nnext:\n\t\tsg = sg->next;\n\t} while (sg != sd->groups);\n\n\tif (cpu != group_balance_cpu(sg))\n\t\treturn;\n\n\tupdate_group_capacity(sd, cpu);\n}\n\n \nstruct asym_cap_data {\n\tstruct list_head link;\n\tunsigned long capacity;\n\tunsigned long cpus[];\n};\n\n \nstatic LIST_HEAD(asym_cap_list);\n\n#define cpu_capacity_span(asym_data) to_cpumask((asym_data)->cpus)\n\n \nstatic inline int\nasym_cpu_capacity_classify(const struct cpumask *sd_span,\n\t\t\t   const struct cpumask *cpu_map)\n{\n\tstruct asym_cap_data *entry;\n\tint count = 0, miss = 0;\n\n\t \n\tlist_for_each_entry(entry, &asym_cap_list, link) {\n\t\tif (cpumask_intersects(sd_span, cpu_capacity_span(entry)))\n\t\t\t++count;\n\t\telse if (cpumask_intersects(cpu_map, cpu_capacity_span(entry)))\n\t\t\t++miss;\n\t}\n\n\tWARN_ON_ONCE(!count && !list_empty(&asym_cap_list));\n\n\t \n\tif (count < 2)\n\t\treturn 0;\n\t \n\tif (miss)\n\t\treturn SD_ASYM_CPUCAPACITY;\n\n\t \n\treturn SD_ASYM_CPUCAPACITY | SD_ASYM_CPUCAPACITY_FULL;\n\n}\n\nstatic inline void asym_cpu_capacity_update_data(int cpu)\n{\n\tunsigned long capacity = arch_scale_cpu_capacity(cpu);\n\tstruct asym_cap_data *entry = NULL;\n\n\tlist_for_each_entry(entry, &asym_cap_list, link) {\n\t\tif (capacity == entry->capacity)\n\t\t\tgoto done;\n\t}\n\n\tentry = kzalloc(sizeof(*entry) + cpumask_size(), GFP_KERNEL);\n\tif (WARN_ONCE(!entry, \"Failed to allocate memory for asymmetry data\\n\"))\n\t\treturn;\n\tentry->capacity = capacity;\n\tlist_add(&entry->link, &asym_cap_list);\ndone:\n\t__cpumask_set_cpu(cpu, cpu_capacity_span(entry));\n}\n\n \nstatic void asym_cpu_capacity_scan(void)\n{\n\tstruct asym_cap_data *entry, *next;\n\tint cpu;\n\n\tlist_for_each_entry(entry, &asym_cap_list, link)\n\t\tcpumask_clear(cpu_capacity_span(entry));\n\n\tfor_each_cpu_and(cpu, cpu_possible_mask, housekeeping_cpumask(HK_TYPE_DOMAIN))\n\t\tasym_cpu_capacity_update_data(cpu);\n\n\tlist_for_each_entry_safe(entry, next, &asym_cap_list, link) {\n\t\tif (cpumask_empty(cpu_capacity_span(entry))) {\n\t\t\tlist_del(&entry->link);\n\t\t\tkfree(entry);\n\t\t}\n\t}\n\n\t \n\tif (list_is_singular(&asym_cap_list)) {\n\t\tentry = list_first_entry(&asym_cap_list, typeof(*entry), link);\n\t\tlist_del(&entry->link);\n\t\tkfree(entry);\n\t}\n}\n\n \n\nstatic int default_relax_domain_level = -1;\nint sched_domain_level_max;\n\nstatic int __init setup_relax_domain_level(char *str)\n{\n\tif (kstrtoint(str, 0, &default_relax_domain_level))\n\t\tpr_warn(\"Unable to set relax_domain_level\\n\");\n\n\treturn 1;\n}\n__setup(\"relax_domain_level=\", setup_relax_domain_level);\n\nstatic void set_domain_attribute(struct sched_domain *sd,\n\t\t\t\t struct sched_domain_attr *attr)\n{\n\tint request;\n\n\tif (!attr || attr->relax_domain_level < 0) {\n\t\tif (default_relax_domain_level < 0)\n\t\t\treturn;\n\t\trequest = default_relax_domain_level;\n\t} else\n\t\trequest = attr->relax_domain_level;\n\n\tif (sd->level > request) {\n\t\t \n\t\tsd->flags &= ~(SD_BALANCE_WAKE|SD_BALANCE_NEWIDLE);\n\t}\n}\n\nstatic void __sdt_free(const struct cpumask *cpu_map);\nstatic int __sdt_alloc(const struct cpumask *cpu_map);\n\nstatic void __free_domain_allocs(struct s_data *d, enum s_alloc what,\n\t\t\t\t const struct cpumask *cpu_map)\n{\n\tswitch (what) {\n\tcase sa_rootdomain:\n\t\tif (!atomic_read(&d->rd->refcount))\n\t\t\tfree_rootdomain(&d->rd->rcu);\n\t\tfallthrough;\n\tcase sa_sd:\n\t\tfree_percpu(d->sd);\n\t\tfallthrough;\n\tcase sa_sd_storage:\n\t\t__sdt_free(cpu_map);\n\t\tfallthrough;\n\tcase sa_none:\n\t\tbreak;\n\t}\n}\n\nstatic enum s_alloc\n__visit_domain_allocation_hell(struct s_data *d, const struct cpumask *cpu_map)\n{\n\tmemset(d, 0, sizeof(*d));\n\n\tif (__sdt_alloc(cpu_map))\n\t\treturn sa_sd_storage;\n\td->sd = alloc_percpu(struct sched_domain *);\n\tif (!d->sd)\n\t\treturn sa_sd_storage;\n\td->rd = alloc_rootdomain();\n\tif (!d->rd)\n\t\treturn sa_sd;\n\n\treturn sa_rootdomain;\n}\n\n \nstatic void claim_allocations(int cpu, struct sched_domain *sd)\n{\n\tstruct sd_data *sdd = sd->private;\n\n\tWARN_ON_ONCE(*per_cpu_ptr(sdd->sd, cpu) != sd);\n\t*per_cpu_ptr(sdd->sd, cpu) = NULL;\n\n\tif (atomic_read(&(*per_cpu_ptr(sdd->sds, cpu))->ref))\n\t\t*per_cpu_ptr(sdd->sds, cpu) = NULL;\n\n\tif (atomic_read(&(*per_cpu_ptr(sdd->sg, cpu))->ref))\n\t\t*per_cpu_ptr(sdd->sg, cpu) = NULL;\n\n\tif (atomic_read(&(*per_cpu_ptr(sdd->sgc, cpu))->ref))\n\t\t*per_cpu_ptr(sdd->sgc, cpu) = NULL;\n}\n\n#ifdef CONFIG_NUMA\nenum numa_topology_type sched_numa_topology_type;\n\nstatic int\t\t\tsched_domains_numa_levels;\nstatic int\t\t\tsched_domains_curr_level;\n\nint\t\t\t\tsched_max_numa_distance;\nstatic int\t\t\t*sched_domains_numa_distance;\nstatic struct cpumask\t\t***sched_domains_numa_masks;\n#endif\n\n \n#define TOPOLOGY_SD_FLAGS\t\t\\\n\t(SD_SHARE_CPUCAPACITY\t|\t\\\n\t SD_SHARE_PKG_RESOURCES |\t\\\n\t SD_NUMA\t\t|\t\\\n\t SD_ASYM_PACKING)\n\nstatic struct sched_domain *\nsd_init(struct sched_domain_topology_level *tl,\n\tconst struct cpumask *cpu_map,\n\tstruct sched_domain *child, int cpu)\n{\n\tstruct sd_data *sdd = &tl->data;\n\tstruct sched_domain *sd = *per_cpu_ptr(sdd->sd, cpu);\n\tint sd_id, sd_weight, sd_flags = 0;\n\tstruct cpumask *sd_span;\n\n#ifdef CONFIG_NUMA\n\t \n\tsched_domains_curr_level = tl->numa_level;\n#endif\n\n\tsd_weight = cpumask_weight(tl->mask(cpu));\n\n\tif (tl->sd_flags)\n\t\tsd_flags = (*tl->sd_flags)();\n\tif (WARN_ONCE(sd_flags & ~TOPOLOGY_SD_FLAGS,\n\t\t\t\"wrong sd_flags in topology description\\n\"))\n\t\tsd_flags &= TOPOLOGY_SD_FLAGS;\n\n\t*sd = (struct sched_domain){\n\t\t.min_interval\t\t= sd_weight,\n\t\t.max_interval\t\t= 2*sd_weight,\n\t\t.busy_factor\t\t= 16,\n\t\t.imbalance_pct\t\t= 117,\n\n\t\t.cache_nice_tries\t= 0,\n\n\t\t.flags\t\t\t= 1*SD_BALANCE_NEWIDLE\n\t\t\t\t\t| 1*SD_BALANCE_EXEC\n\t\t\t\t\t| 1*SD_BALANCE_FORK\n\t\t\t\t\t| 0*SD_BALANCE_WAKE\n\t\t\t\t\t| 1*SD_WAKE_AFFINE\n\t\t\t\t\t| 0*SD_SHARE_CPUCAPACITY\n\t\t\t\t\t| 0*SD_SHARE_PKG_RESOURCES\n\t\t\t\t\t| 0*SD_SERIALIZE\n\t\t\t\t\t| 1*SD_PREFER_SIBLING\n\t\t\t\t\t| 0*SD_NUMA\n\t\t\t\t\t| sd_flags\n\t\t\t\t\t,\n\n\t\t.last_balance\t\t= jiffies,\n\t\t.balance_interval\t= sd_weight,\n\t\t.max_newidle_lb_cost\t= 0,\n\t\t.last_decay_max_lb_cost\t= jiffies,\n\t\t.child\t\t\t= child,\n#ifdef CONFIG_SCHED_DEBUG\n\t\t.name\t\t\t= tl->name,\n#endif\n\t};\n\n\tsd_span = sched_domain_span(sd);\n\tcpumask_and(sd_span, cpu_map, tl->mask(cpu));\n\tsd_id = cpumask_first(sd_span);\n\n\tsd->flags |= asym_cpu_capacity_classify(sd_span, cpu_map);\n\n\tWARN_ONCE((sd->flags & (SD_SHARE_CPUCAPACITY | SD_ASYM_CPUCAPACITY)) ==\n\t\t  (SD_SHARE_CPUCAPACITY | SD_ASYM_CPUCAPACITY),\n\t\t  \"CPU capacity asymmetry not supported on SMT\\n\");\n\n\t \n\t \n\tif ((sd->flags & SD_ASYM_CPUCAPACITY) && sd->child)\n\t\tsd->child->flags &= ~SD_PREFER_SIBLING;\n\n\tif (sd->flags & SD_SHARE_CPUCAPACITY) {\n\t\tsd->imbalance_pct = 110;\n\n\t} else if (sd->flags & SD_SHARE_PKG_RESOURCES) {\n\t\tsd->imbalance_pct = 117;\n\t\tsd->cache_nice_tries = 1;\n\n#ifdef CONFIG_NUMA\n\t} else if (sd->flags & SD_NUMA) {\n\t\tsd->cache_nice_tries = 2;\n\n\t\tsd->flags &= ~SD_PREFER_SIBLING;\n\t\tsd->flags |= SD_SERIALIZE;\n\t\tif (sched_domains_numa_distance[tl->numa_level] > node_reclaim_distance) {\n\t\t\tsd->flags &= ~(SD_BALANCE_EXEC |\n\t\t\t\t       SD_BALANCE_FORK |\n\t\t\t\t       SD_WAKE_AFFINE);\n\t\t}\n\n#endif\n\t} else {\n\t\tsd->cache_nice_tries = 1;\n\t}\n\n\t \n\tif (sd->flags & SD_SHARE_PKG_RESOURCES) {\n\t\tsd->shared = *per_cpu_ptr(sdd->sds, sd_id);\n\t\tatomic_inc(&sd->shared->ref);\n\t\tatomic_set(&sd->shared->nr_busy_cpus, sd_weight);\n\t}\n\n\tsd->private = sdd;\n\n\treturn sd;\n}\n\n \nstatic struct sched_domain_topology_level default_topology[] = {\n#ifdef CONFIG_SCHED_SMT\n\t{ cpu_smt_mask, cpu_smt_flags, SD_INIT_NAME(SMT) },\n#endif\n\n#ifdef CONFIG_SCHED_CLUSTER\n\t{ cpu_clustergroup_mask, cpu_cluster_flags, SD_INIT_NAME(CLS) },\n#endif\n\n#ifdef CONFIG_SCHED_MC\n\t{ cpu_coregroup_mask, cpu_core_flags, SD_INIT_NAME(MC) },\n#endif\n\t{ cpu_cpu_mask, SD_INIT_NAME(DIE) },\n\t{ NULL, },\n};\n\nstatic struct sched_domain_topology_level *sched_domain_topology =\n\tdefault_topology;\nstatic struct sched_domain_topology_level *sched_domain_topology_saved;\n\n#define for_each_sd_topology(tl)\t\t\t\\\n\tfor (tl = sched_domain_topology; tl->mask; tl++)\n\nvoid __init set_sched_topology(struct sched_domain_topology_level *tl)\n{\n\tif (WARN_ON_ONCE(sched_smp_initialized))\n\t\treturn;\n\n\tsched_domain_topology = tl;\n\tsched_domain_topology_saved = NULL;\n}\n\n#ifdef CONFIG_NUMA\n\nstatic const struct cpumask *sd_numa_mask(int cpu)\n{\n\treturn sched_domains_numa_masks[sched_domains_curr_level][cpu_to_node(cpu)];\n}\n\nstatic void sched_numa_warn(const char *str)\n{\n\tstatic int done = false;\n\tint i,j;\n\n\tif (done)\n\t\treturn;\n\n\tdone = true;\n\n\tprintk(KERN_WARNING \"ERROR: %s\\n\\n\", str);\n\n\tfor (i = 0; i < nr_node_ids; i++) {\n\t\tprintk(KERN_WARNING \"  \");\n\t\tfor (j = 0; j < nr_node_ids; j++) {\n\t\t\tif (!node_state(i, N_CPU) || !node_state(j, N_CPU))\n\t\t\t\tprintk(KERN_CONT \"(%02d) \", node_distance(i,j));\n\t\t\telse\n\t\t\t\tprintk(KERN_CONT \" %02d  \", node_distance(i,j));\n\t\t}\n\t\tprintk(KERN_CONT \"\\n\");\n\t}\n\tprintk(KERN_WARNING \"\\n\");\n}\n\nbool find_numa_distance(int distance)\n{\n\tbool found = false;\n\tint i, *distances;\n\n\tif (distance == node_distance(0, 0))\n\t\treturn true;\n\n\trcu_read_lock();\n\tdistances = rcu_dereference(sched_domains_numa_distance);\n\tif (!distances)\n\t\tgoto unlock;\n\tfor (i = 0; i < sched_domains_numa_levels; i++) {\n\t\tif (distances[i] == distance) {\n\t\t\tfound = true;\n\t\t\tbreak;\n\t\t}\n\t}\nunlock:\n\trcu_read_unlock();\n\n\treturn found;\n}\n\n#define for_each_cpu_node_but(n, nbut)\t\t\\\n\tfor_each_node_state(n, N_CPU)\t\t\\\n\t\tif (n == nbut)\t\t\t\\\n\t\t\tcontinue;\t\t\\\n\t\telse\n\n \nstatic void init_numa_topology_type(int offline_node)\n{\n\tint a, b, c, n;\n\n\tn = sched_max_numa_distance;\n\n\tif (sched_domains_numa_levels <= 2) {\n\t\tsched_numa_topology_type = NUMA_DIRECT;\n\t\treturn;\n\t}\n\n\tfor_each_cpu_node_but(a, offline_node) {\n\t\tfor_each_cpu_node_but(b, offline_node) {\n\t\t\t \n\t\t\tif (node_distance(a, b) < n)\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tfor_each_cpu_node_but(c, offline_node) {\n\t\t\t\tif (node_distance(a, c) < n &&\n\t\t\t\t    node_distance(b, c) < n) {\n\t\t\t\t\tsched_numa_topology_type =\n\t\t\t\t\t\t\tNUMA_GLUELESS_MESH;\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tsched_numa_topology_type = NUMA_BACKPLANE;\n\t\t\treturn;\n\t\t}\n\t}\n\n\tpr_err(\"Failed to find a NUMA topology type, defaulting to DIRECT\\n\");\n\tsched_numa_topology_type = NUMA_DIRECT;\n}\n\n\n#define NR_DISTANCE_VALUES (1 << DISTANCE_BITS)\n\nvoid sched_init_numa(int offline_node)\n{\n\tstruct sched_domain_topology_level *tl;\n\tunsigned long *distance_map;\n\tint nr_levels = 0;\n\tint i, j;\n\tint *distances;\n\tstruct cpumask ***masks;\n\n\t \n\tdistance_map = bitmap_alloc(NR_DISTANCE_VALUES, GFP_KERNEL);\n\tif (!distance_map)\n\t\treturn;\n\n\tbitmap_zero(distance_map, NR_DISTANCE_VALUES);\n\tfor_each_cpu_node_but(i, offline_node) {\n\t\tfor_each_cpu_node_but(j, offline_node) {\n\t\t\tint distance = node_distance(i, j);\n\n\t\t\tif (distance < LOCAL_DISTANCE || distance >= NR_DISTANCE_VALUES) {\n\t\t\t\tsched_numa_warn(\"Invalid distance value range\");\n\t\t\t\tbitmap_free(distance_map);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tbitmap_set(distance_map, distance, 1);\n\t\t}\n\t}\n\t \n\tnr_levels = bitmap_weight(distance_map, NR_DISTANCE_VALUES);\n\n\tdistances = kcalloc(nr_levels, sizeof(int), GFP_KERNEL);\n\tif (!distances) {\n\t\tbitmap_free(distance_map);\n\t\treturn;\n\t}\n\n\tfor (i = 0, j = 0; i < nr_levels; i++, j++) {\n\t\tj = find_next_bit(distance_map, NR_DISTANCE_VALUES, j);\n\t\tdistances[i] = j;\n\t}\n\trcu_assign_pointer(sched_domains_numa_distance, distances);\n\n\tbitmap_free(distance_map);\n\n\t \n\n\t \n\tsched_domains_numa_levels = 0;\n\n\tmasks = kzalloc(sizeof(void *) * nr_levels, GFP_KERNEL);\n\tif (!masks)\n\t\treturn;\n\n\t \n\tfor (i = 0; i < nr_levels; i++) {\n\t\tmasks[i] = kzalloc(nr_node_ids * sizeof(void *), GFP_KERNEL);\n\t\tif (!masks[i])\n\t\t\treturn;\n\n\t\tfor_each_cpu_node_but(j, offline_node) {\n\t\t\tstruct cpumask *mask = kzalloc(cpumask_size(), GFP_KERNEL);\n\t\t\tint k;\n\n\t\t\tif (!mask)\n\t\t\t\treturn;\n\n\t\t\tmasks[i][j] = mask;\n\n\t\t\tfor_each_cpu_node_but(k, offline_node) {\n\t\t\t\tif (sched_debug() && (node_distance(j, k) != node_distance(k, j)))\n\t\t\t\t\tsched_numa_warn(\"Node-distance not symmetric\");\n\n\t\t\t\tif (node_distance(j, k) > sched_domains_numa_distance[i])\n\t\t\t\t\tcontinue;\n\n\t\t\t\tcpumask_or(mask, mask, cpumask_of_node(k));\n\t\t\t}\n\t\t}\n\t}\n\trcu_assign_pointer(sched_domains_numa_masks, masks);\n\n\t \n\tfor (i = 0; sched_domain_topology[i].mask; i++);\n\n\ttl = kzalloc((i + nr_levels + 1) *\n\t\t\tsizeof(struct sched_domain_topology_level), GFP_KERNEL);\n\tif (!tl)\n\t\treturn;\n\n\t \n\tfor (i = 0; sched_domain_topology[i].mask; i++)\n\t\ttl[i] = sched_domain_topology[i];\n\n\t \n\ttl[i++] = (struct sched_domain_topology_level){\n\t\t.mask = sd_numa_mask,\n\t\t.numa_level = 0,\n\t\tSD_INIT_NAME(NODE)\n\t};\n\n\t \n\tfor (j = 1; j < nr_levels; i++, j++) {\n\t\ttl[i] = (struct sched_domain_topology_level){\n\t\t\t.mask = sd_numa_mask,\n\t\t\t.sd_flags = cpu_numa_flags,\n\t\t\t.flags = SDTL_OVERLAP,\n\t\t\t.numa_level = j,\n\t\t\tSD_INIT_NAME(NUMA)\n\t\t};\n\t}\n\n\tsched_domain_topology_saved = sched_domain_topology;\n\tsched_domain_topology = tl;\n\n\tsched_domains_numa_levels = nr_levels;\n\tWRITE_ONCE(sched_max_numa_distance, sched_domains_numa_distance[nr_levels - 1]);\n\n\tinit_numa_topology_type(offline_node);\n}\n\n\nstatic void sched_reset_numa(void)\n{\n\tint nr_levels, *distances;\n\tstruct cpumask ***masks;\n\n\tnr_levels = sched_domains_numa_levels;\n\tsched_domains_numa_levels = 0;\n\tsched_max_numa_distance = 0;\n\tsched_numa_topology_type = NUMA_DIRECT;\n\tdistances = sched_domains_numa_distance;\n\trcu_assign_pointer(sched_domains_numa_distance, NULL);\n\tmasks = sched_domains_numa_masks;\n\trcu_assign_pointer(sched_domains_numa_masks, NULL);\n\tif (distances || masks) {\n\t\tint i, j;\n\n\t\tsynchronize_rcu();\n\t\tkfree(distances);\n\t\tfor (i = 0; i < nr_levels && masks; i++) {\n\t\t\tif (!masks[i])\n\t\t\t\tcontinue;\n\t\t\tfor_each_node(j)\n\t\t\t\tkfree(masks[i][j]);\n\t\t\tkfree(masks[i]);\n\t\t}\n\t\tkfree(masks);\n\t}\n\tif (sched_domain_topology_saved) {\n\t\tkfree(sched_domain_topology);\n\t\tsched_domain_topology = sched_domain_topology_saved;\n\t\tsched_domain_topology_saved = NULL;\n\t}\n}\n\n \nvoid sched_update_numa(int cpu, bool online)\n{\n\tint node;\n\n\tnode = cpu_to_node(cpu);\n\t \n\tif (cpumask_weight(cpumask_of_node(node)) != 1)\n\t\treturn;\n\n\tsched_reset_numa();\n\tsched_init_numa(online ? NUMA_NO_NODE : node);\n}\n\nvoid sched_domains_numa_masks_set(unsigned int cpu)\n{\n\tint node = cpu_to_node(cpu);\n\tint i, j;\n\n\tfor (i = 0; i < sched_domains_numa_levels; i++) {\n\t\tfor (j = 0; j < nr_node_ids; j++) {\n\t\t\tif (!node_state(j, N_CPU))\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tif (node_distance(j, node) <= sched_domains_numa_distance[i])\n\t\t\t\tcpumask_set_cpu(cpu, sched_domains_numa_masks[i][j]);\n\t\t}\n\t}\n}\n\nvoid sched_domains_numa_masks_clear(unsigned int cpu)\n{\n\tint i, j;\n\n\tfor (i = 0; i < sched_domains_numa_levels; i++) {\n\t\tfor (j = 0; j < nr_node_ids; j++) {\n\t\t\tif (sched_domains_numa_masks[i][j])\n\t\t\t\tcpumask_clear_cpu(cpu, sched_domains_numa_masks[i][j]);\n\t\t}\n\t}\n}\n\n \nint sched_numa_find_closest(const struct cpumask *cpus, int cpu)\n{\n\tint i, j = cpu_to_node(cpu), found = nr_cpu_ids;\n\tstruct cpumask ***masks;\n\n\trcu_read_lock();\n\tmasks = rcu_dereference(sched_domains_numa_masks);\n\tif (!masks)\n\t\tgoto unlock;\n\tfor (i = 0; i < sched_domains_numa_levels; i++) {\n\t\tif (!masks[i][j])\n\t\t\tbreak;\n\t\tcpu = cpumask_any_and(cpus, masks[i][j]);\n\t\tif (cpu < nr_cpu_ids) {\n\t\t\tfound = cpu;\n\t\t\tbreak;\n\t\t}\n\t}\nunlock:\n\trcu_read_unlock();\n\n\treturn found;\n}\n\nstruct __cmp_key {\n\tconst struct cpumask *cpus;\n\tstruct cpumask ***masks;\n\tint node;\n\tint cpu;\n\tint w;\n};\n\nstatic int hop_cmp(const void *a, const void *b)\n{\n\tstruct cpumask **prev_hop, **cur_hop = *(struct cpumask ***)b;\n\tstruct __cmp_key *k = (struct __cmp_key *)a;\n\n\tif (cpumask_weight_and(k->cpus, cur_hop[k->node]) <= k->cpu)\n\t\treturn 1;\n\n\tif (b == k->masks) {\n\t\tk->w = 0;\n\t\treturn 0;\n\t}\n\n\tprev_hop = *((struct cpumask ***)b - 1);\n\tk->w = cpumask_weight_and(k->cpus, prev_hop[k->node]);\n\tif (k->w <= k->cpu)\n\t\treturn 0;\n\n\treturn -1;\n}\n\n \nint sched_numa_find_nth_cpu(const struct cpumask *cpus, int cpu, int node)\n{\n\tstruct __cmp_key k = { .cpus = cpus, .cpu = cpu };\n\tstruct cpumask ***hop_masks;\n\tint hop, ret = nr_cpu_ids;\n\n\trcu_read_lock();\n\n\t \n\tnode = numa_nearest_node(node, N_CPU);\n\tk.node = node;\n\n\tk.masks = rcu_dereference(sched_domains_numa_masks);\n\tif (!k.masks)\n\t\tgoto unlock;\n\n\thop_masks = bsearch(&k, k.masks, sched_domains_numa_levels, sizeof(k.masks[0]), hop_cmp);\n\thop = hop_masks\t- k.masks;\n\n\tret = hop ?\n\t\tcpumask_nth_and_andnot(cpu - k.w, cpus, k.masks[hop][node], k.masks[hop-1][node]) :\n\t\tcpumask_nth_and(cpu, cpus, k.masks[0][node]);\nunlock:\n\trcu_read_unlock();\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(sched_numa_find_nth_cpu);\n\n \nconst struct cpumask *sched_numa_hop_mask(unsigned int node, unsigned int hops)\n{\n\tstruct cpumask ***masks;\n\n\tif (node >= nr_node_ids || hops >= sched_domains_numa_levels)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tmasks = rcu_dereference(sched_domains_numa_masks);\n\tif (!masks)\n\t\treturn ERR_PTR(-EBUSY);\n\n\treturn masks[hops][node];\n}\nEXPORT_SYMBOL_GPL(sched_numa_hop_mask);\n\n#endif  \n\nstatic int __sdt_alloc(const struct cpumask *cpu_map)\n{\n\tstruct sched_domain_topology_level *tl;\n\tint j;\n\n\tfor_each_sd_topology(tl) {\n\t\tstruct sd_data *sdd = &tl->data;\n\n\t\tsdd->sd = alloc_percpu(struct sched_domain *);\n\t\tif (!sdd->sd)\n\t\t\treturn -ENOMEM;\n\n\t\tsdd->sds = alloc_percpu(struct sched_domain_shared *);\n\t\tif (!sdd->sds)\n\t\t\treturn -ENOMEM;\n\n\t\tsdd->sg = alloc_percpu(struct sched_group *);\n\t\tif (!sdd->sg)\n\t\t\treturn -ENOMEM;\n\n\t\tsdd->sgc = alloc_percpu(struct sched_group_capacity *);\n\t\tif (!sdd->sgc)\n\t\t\treturn -ENOMEM;\n\n\t\tfor_each_cpu(j, cpu_map) {\n\t\t\tstruct sched_domain *sd;\n\t\t\tstruct sched_domain_shared *sds;\n\t\t\tstruct sched_group *sg;\n\t\t\tstruct sched_group_capacity *sgc;\n\n\t\t\tsd = kzalloc_node(sizeof(struct sched_domain) + cpumask_size(),\n\t\t\t\t\tGFP_KERNEL, cpu_to_node(j));\n\t\t\tif (!sd)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\t*per_cpu_ptr(sdd->sd, j) = sd;\n\n\t\t\tsds = kzalloc_node(sizeof(struct sched_domain_shared),\n\t\t\t\t\tGFP_KERNEL, cpu_to_node(j));\n\t\t\tif (!sds)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\t*per_cpu_ptr(sdd->sds, j) = sds;\n\n\t\t\tsg = kzalloc_node(sizeof(struct sched_group) + cpumask_size(),\n\t\t\t\t\tGFP_KERNEL, cpu_to_node(j));\n\t\t\tif (!sg)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tsg->next = sg;\n\n\t\t\t*per_cpu_ptr(sdd->sg, j) = sg;\n\n\t\t\tsgc = kzalloc_node(sizeof(struct sched_group_capacity) + cpumask_size(),\n\t\t\t\t\tGFP_KERNEL, cpu_to_node(j));\n\t\t\tif (!sgc)\n\t\t\t\treturn -ENOMEM;\n\n#ifdef CONFIG_SCHED_DEBUG\n\t\t\tsgc->id = j;\n#endif\n\n\t\t\t*per_cpu_ptr(sdd->sgc, j) = sgc;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void __sdt_free(const struct cpumask *cpu_map)\n{\n\tstruct sched_domain_topology_level *tl;\n\tint j;\n\n\tfor_each_sd_topology(tl) {\n\t\tstruct sd_data *sdd = &tl->data;\n\n\t\tfor_each_cpu(j, cpu_map) {\n\t\t\tstruct sched_domain *sd;\n\n\t\t\tif (sdd->sd) {\n\t\t\t\tsd = *per_cpu_ptr(sdd->sd, j);\n\t\t\t\tif (sd && (sd->flags & SD_OVERLAP))\n\t\t\t\t\tfree_sched_groups(sd->groups, 0);\n\t\t\t\tkfree(*per_cpu_ptr(sdd->sd, j));\n\t\t\t}\n\n\t\t\tif (sdd->sds)\n\t\t\t\tkfree(*per_cpu_ptr(sdd->sds, j));\n\t\t\tif (sdd->sg)\n\t\t\t\tkfree(*per_cpu_ptr(sdd->sg, j));\n\t\t\tif (sdd->sgc)\n\t\t\t\tkfree(*per_cpu_ptr(sdd->sgc, j));\n\t\t}\n\t\tfree_percpu(sdd->sd);\n\t\tsdd->sd = NULL;\n\t\tfree_percpu(sdd->sds);\n\t\tsdd->sds = NULL;\n\t\tfree_percpu(sdd->sg);\n\t\tsdd->sg = NULL;\n\t\tfree_percpu(sdd->sgc);\n\t\tsdd->sgc = NULL;\n\t}\n}\n\nstatic struct sched_domain *build_sched_domain(struct sched_domain_topology_level *tl,\n\t\tconst struct cpumask *cpu_map, struct sched_domain_attr *attr,\n\t\tstruct sched_domain *child, int cpu)\n{\n\tstruct sched_domain *sd = sd_init(tl, cpu_map, child, cpu);\n\n\tif (child) {\n\t\tsd->level = child->level + 1;\n\t\tsched_domain_level_max = max(sched_domain_level_max, sd->level);\n\t\tchild->parent = sd;\n\n\t\tif (!cpumask_subset(sched_domain_span(child),\n\t\t\t\t    sched_domain_span(sd))) {\n\t\t\tpr_err(\"BUG: arch topology borken\\n\");\n#ifdef CONFIG_SCHED_DEBUG\n\t\t\tpr_err(\"     the %s domain not a subset of the %s domain\\n\",\n\t\t\t\t\tchild->name, sd->name);\n#endif\n\t\t\t \n\t\t\tcpumask_or(sched_domain_span(sd),\n\t\t\t\t   sched_domain_span(sd),\n\t\t\t\t   sched_domain_span(child));\n\t\t}\n\n\t}\n\tset_domain_attribute(sd, attr);\n\n\treturn sd;\n}\n\n \nstatic bool topology_span_sane(struct sched_domain_topology_level *tl,\n\t\t\t      const struct cpumask *cpu_map, int cpu)\n{\n\tint i;\n\n\t \n\tif (tl->flags & SDTL_OVERLAP)\n\t\treturn true;\n\n\t \n\tfor_each_cpu(i, cpu_map) {\n\t\tif (i == cpu)\n\t\t\tcontinue;\n\t\t \n\t\tif (!cpumask_equal(tl->mask(cpu), tl->mask(i)) &&\n\t\t    cpumask_intersects(tl->mask(cpu), tl->mask(i)))\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\n \nstatic int\nbuild_sched_domains(const struct cpumask *cpu_map, struct sched_domain_attr *attr)\n{\n\tenum s_alloc alloc_state = sa_none;\n\tstruct sched_domain *sd;\n\tstruct s_data d;\n\tstruct rq *rq = NULL;\n\tint i, ret = -ENOMEM;\n\tbool has_asym = false;\n\n\tif (WARN_ON(cpumask_empty(cpu_map)))\n\t\tgoto error;\n\n\talloc_state = __visit_domain_allocation_hell(&d, cpu_map);\n\tif (alloc_state != sa_rootdomain)\n\t\tgoto error;\n\n\t \n\tfor_each_cpu(i, cpu_map) {\n\t\tstruct sched_domain_topology_level *tl;\n\n\t\tsd = NULL;\n\t\tfor_each_sd_topology(tl) {\n\n\t\t\tif (WARN_ON(!topology_span_sane(tl, cpu_map, i)))\n\t\t\t\tgoto error;\n\n\t\t\tsd = build_sched_domain(tl, cpu_map, attr, sd, i);\n\n\t\t\thas_asym |= sd->flags & SD_ASYM_CPUCAPACITY;\n\n\t\t\tif (tl == sched_domain_topology)\n\t\t\t\t*per_cpu_ptr(d.sd, i) = sd;\n\t\t\tif (tl->flags & SDTL_OVERLAP)\n\t\t\t\tsd->flags |= SD_OVERLAP;\n\t\t\tif (cpumask_equal(cpu_map, sched_domain_span(sd)))\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\tfor_each_cpu(i, cpu_map) {\n\t\tfor (sd = *per_cpu_ptr(d.sd, i); sd; sd = sd->parent) {\n\t\t\tsd->span_weight = cpumask_weight(sched_domain_span(sd));\n\t\t\tif (sd->flags & SD_OVERLAP) {\n\t\t\t\tif (build_overlap_sched_groups(sd, i))\n\t\t\t\t\tgoto error;\n\t\t\t} else {\n\t\t\t\tif (build_sched_groups(sd, i))\n\t\t\t\t\tgoto error;\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tfor_each_cpu(i, cpu_map) {\n\t\tunsigned int imb = 0;\n\t\tunsigned int imb_span = 1;\n\n\t\tfor (sd = *per_cpu_ptr(d.sd, i); sd; sd = sd->parent) {\n\t\t\tstruct sched_domain *child = sd->child;\n\n\t\t\tif (!(sd->flags & SD_SHARE_PKG_RESOURCES) && child &&\n\t\t\t    (child->flags & SD_SHARE_PKG_RESOURCES)) {\n\t\t\t\tstruct sched_domain __rcu *top_p;\n\t\t\t\tunsigned int nr_llcs;\n\n\t\t\t\t \n\t\t\t\tnr_llcs = sd->span_weight / child->span_weight;\n\t\t\t\tif (nr_llcs == 1)\n\t\t\t\t\timb = sd->span_weight >> 3;\n\t\t\t\telse\n\t\t\t\t\timb = nr_llcs;\n\t\t\t\timb = max(1U, imb);\n\t\t\t\tsd->imb_numa_nr = imb;\n\n\t\t\t\t \n\t\t\t\ttop_p = sd->parent;\n\t\t\t\twhile (top_p && !(top_p->flags & SD_NUMA)) {\n\t\t\t\t\ttop_p = top_p->parent;\n\t\t\t\t}\n\t\t\t\timb_span = top_p ? top_p->span_weight : sd->span_weight;\n\t\t\t} else {\n\t\t\t\tint factor = max(1U, (sd->span_weight / imb_span));\n\n\t\t\t\tsd->imb_numa_nr = imb * factor;\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tfor (i = nr_cpumask_bits-1; i >= 0; i--) {\n\t\tif (!cpumask_test_cpu(i, cpu_map))\n\t\t\tcontinue;\n\n\t\tfor (sd = *per_cpu_ptr(d.sd, i); sd; sd = sd->parent) {\n\t\t\tclaim_allocations(i, sd);\n\t\t\tinit_sched_groups_capacity(i, sd);\n\t\t}\n\t}\n\n\t \n\trcu_read_lock();\n\tfor_each_cpu(i, cpu_map) {\n\t\trq = cpu_rq(i);\n\t\tsd = *per_cpu_ptr(d.sd, i);\n\n\t\t \n\t\tif (rq->cpu_capacity_orig > READ_ONCE(d.rd->max_cpu_capacity))\n\t\t\tWRITE_ONCE(d.rd->max_cpu_capacity, rq->cpu_capacity_orig);\n\n\t\tcpu_attach_domain(sd, d.rd, i);\n\t}\n\trcu_read_unlock();\n\n\tif (has_asym)\n\t\tstatic_branch_inc_cpuslocked(&sched_asym_cpucapacity);\n\n\tif (rq && sched_debug_verbose) {\n\t\tpr_info(\"root domain span: %*pbl (max cpu_capacity = %lu)\\n\",\n\t\t\tcpumask_pr_args(cpu_map), rq->rd->max_cpu_capacity);\n\t}\n\n\tret = 0;\nerror:\n\t__free_domain_allocs(&d, alloc_state, cpu_map);\n\n\treturn ret;\n}\n\n \nstatic cpumask_var_t\t\t\t*doms_cur;\n\n \nstatic int\t\t\t\tndoms_cur;\n\n \nstatic struct sched_domain_attr\t\t*dattr_cur;\n\n \nstatic cpumask_var_t\t\t\tfallback_doms;\n\n \nint __weak arch_update_cpu_topology(void)\n{\n\treturn 0;\n}\n\ncpumask_var_t *alloc_sched_domains(unsigned int ndoms)\n{\n\tint i;\n\tcpumask_var_t *doms;\n\n\tdoms = kmalloc_array(ndoms, sizeof(*doms), GFP_KERNEL);\n\tif (!doms)\n\t\treturn NULL;\n\tfor (i = 0; i < ndoms; i++) {\n\t\tif (!alloc_cpumask_var(&doms[i], GFP_KERNEL)) {\n\t\t\tfree_sched_domains(doms, i);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\treturn doms;\n}\n\nvoid free_sched_domains(cpumask_var_t doms[], unsigned int ndoms)\n{\n\tunsigned int i;\n\tfor (i = 0; i < ndoms; i++)\n\t\tfree_cpumask_var(doms[i]);\n\tkfree(doms);\n}\n\n \nint __init sched_init_domains(const struct cpumask *cpu_map)\n{\n\tint err;\n\n\tzalloc_cpumask_var(&sched_domains_tmpmask, GFP_KERNEL);\n\tzalloc_cpumask_var(&sched_domains_tmpmask2, GFP_KERNEL);\n\tzalloc_cpumask_var(&fallback_doms, GFP_KERNEL);\n\n\tarch_update_cpu_topology();\n\tasym_cpu_capacity_scan();\n\tndoms_cur = 1;\n\tdoms_cur = alloc_sched_domains(ndoms_cur);\n\tif (!doms_cur)\n\t\tdoms_cur = &fallback_doms;\n\tcpumask_and(doms_cur[0], cpu_map, housekeeping_cpumask(HK_TYPE_DOMAIN));\n\terr = build_sched_domains(doms_cur[0], NULL);\n\n\treturn err;\n}\n\n \nstatic void detach_destroy_domains(const struct cpumask *cpu_map)\n{\n\tunsigned int cpu = cpumask_any(cpu_map);\n\tint i;\n\n\tif (rcu_access_pointer(per_cpu(sd_asym_cpucapacity, cpu)))\n\t\tstatic_branch_dec_cpuslocked(&sched_asym_cpucapacity);\n\n\trcu_read_lock();\n\tfor_each_cpu(i, cpu_map)\n\t\tcpu_attach_domain(NULL, &def_root_domain, i);\n\trcu_read_unlock();\n}\n\n \nstatic int dattrs_equal(struct sched_domain_attr *cur, int idx_cur,\n\t\t\tstruct sched_domain_attr *new, int idx_new)\n{\n\tstruct sched_domain_attr tmp;\n\n\t \n\tif (!new && !cur)\n\t\treturn 1;\n\n\ttmp = SD_ATTR_INIT;\n\n\treturn !memcmp(cur ? (cur + idx_cur) : &tmp,\n\t\t\tnew ? (new + idx_new) : &tmp,\n\t\t\tsizeof(struct sched_domain_attr));\n}\n\n \nvoid partition_sched_domains_locked(int ndoms_new, cpumask_var_t doms_new[],\n\t\t\t\t    struct sched_domain_attr *dattr_new)\n{\n\tbool __maybe_unused has_eas = false;\n\tint i, j, n;\n\tint new_topology;\n\n\tlockdep_assert_held(&sched_domains_mutex);\n\n\t \n\tnew_topology = arch_update_cpu_topology();\n\t \n\tif (new_topology)\n\t\tasym_cpu_capacity_scan();\n\n\tif (!doms_new) {\n\t\tWARN_ON_ONCE(dattr_new);\n\t\tn = 0;\n\t\tdoms_new = alloc_sched_domains(1);\n\t\tif (doms_new) {\n\t\t\tn = 1;\n\t\t\tcpumask_and(doms_new[0], cpu_active_mask,\n\t\t\t\t    housekeeping_cpumask(HK_TYPE_DOMAIN));\n\t\t}\n\t} else {\n\t\tn = ndoms_new;\n\t}\n\n\t \n\tfor (i = 0; i < ndoms_cur; i++) {\n\t\tfor (j = 0; j < n && !new_topology; j++) {\n\t\t\tif (cpumask_equal(doms_cur[i], doms_new[j]) &&\n\t\t\t    dattrs_equal(dattr_cur, i, dattr_new, j)) {\n\t\t\t\tstruct root_domain *rd;\n\n\t\t\t\t \n\t\t\t\trd = cpu_rq(cpumask_any(doms_cur[i]))->rd;\n\t\t\t\tdl_clear_root_domain(rd);\n\t\t\t\tgoto match1;\n\t\t\t}\n\t\t}\n\t\t \n\t\tdetach_destroy_domains(doms_cur[i]);\nmatch1:\n\t\t;\n\t}\n\n\tn = ndoms_cur;\n\tif (!doms_new) {\n\t\tn = 0;\n\t\tdoms_new = &fallback_doms;\n\t\tcpumask_and(doms_new[0], cpu_active_mask,\n\t\t\t    housekeeping_cpumask(HK_TYPE_DOMAIN));\n\t}\n\n\t \n\tfor (i = 0; i < ndoms_new; i++) {\n\t\tfor (j = 0; j < n && !new_topology; j++) {\n\t\t\tif (cpumask_equal(doms_new[i], doms_cur[j]) &&\n\t\t\t    dattrs_equal(dattr_new, i, dattr_cur, j))\n\t\t\t\tgoto match2;\n\t\t}\n\t\t \n\t\tbuild_sched_domains(doms_new[i], dattr_new ? dattr_new + i : NULL);\nmatch2:\n\t\t;\n\t}\n\n#if defined(CONFIG_ENERGY_MODEL) && defined(CONFIG_CPU_FREQ_GOV_SCHEDUTIL)\n\t \n\tfor (i = 0; i < ndoms_new; i++) {\n\t\tfor (j = 0; j < n && !sched_energy_update; j++) {\n\t\t\tif (cpumask_equal(doms_new[i], doms_cur[j]) &&\n\t\t\t    cpu_rq(cpumask_first(doms_cur[j]))->rd->pd) {\n\t\t\t\thas_eas = true;\n\t\t\t\tgoto match3;\n\t\t\t}\n\t\t}\n\t\t \n\t\thas_eas |= build_perf_domains(doms_new[i]);\nmatch3:\n\t\t;\n\t}\n\tsched_energy_set(has_eas);\n#endif\n\n\t \n\tif (doms_cur != &fallback_doms)\n\t\tfree_sched_domains(doms_cur, ndoms_cur);\n\n\tkfree(dattr_cur);\n\tdoms_cur = doms_new;\n\tdattr_cur = dattr_new;\n\tndoms_cur = ndoms_new;\n\n\tupdate_sched_domain_debugfs();\n}\n\n \nvoid partition_sched_domains(int ndoms_new, cpumask_var_t doms_new[],\n\t\t\t     struct sched_domain_attr *dattr_new)\n{\n\tmutex_lock(&sched_domains_mutex);\n\tpartition_sched_domains_locked(ndoms_new, doms_new, dattr_new);\n\tmutex_unlock(&sched_domains_mutex);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}