{
  "module_name": "clock.c",
  "hash_id": "a060c36f39faa1e8a30a2c1d6807322454b7a01daf4a1058d72595eb5e7576ce",
  "original_prompt": "Ingested from linux-6.6.14/kernel/sched/clock.c",
  "human_readable_source": "\n \n\n \nnotrace unsigned long long __weak sched_clock(void)\n{\n\treturn (unsigned long long)(jiffies - INITIAL_JIFFIES)\n\t\t\t\t\t* (NSEC_PER_SEC / HZ);\n}\nEXPORT_SYMBOL_GPL(sched_clock);\n\nstatic DEFINE_STATIC_KEY_FALSE(sched_clock_running);\n\n#ifdef CONFIG_HAVE_UNSTABLE_SCHED_CLOCK\n \nstatic DEFINE_STATIC_KEY_FALSE(__sched_clock_stable);\nstatic int __sched_clock_stable_early = 1;\n\n \n__read_mostly u64 __sched_clock_offset;\nstatic __read_mostly u64 __gtod_offset;\n\nstruct sched_clock_data {\n\tu64\t\t\ttick_raw;\n\tu64\t\t\ttick_gtod;\n\tu64\t\t\tclock;\n};\n\nstatic DEFINE_PER_CPU_SHARED_ALIGNED(struct sched_clock_data, sched_clock_data);\n\nstatic __always_inline struct sched_clock_data *this_scd(void)\n{\n\treturn this_cpu_ptr(&sched_clock_data);\n}\n\nnotrace static inline struct sched_clock_data *cpu_sdc(int cpu)\n{\n\treturn &per_cpu(sched_clock_data, cpu);\n}\n\nnotrace int sched_clock_stable(void)\n{\n\treturn static_branch_likely(&__sched_clock_stable);\n}\n\nnotrace static void __scd_stamp(struct sched_clock_data *scd)\n{\n\tscd->tick_gtod = ktime_get_ns();\n\tscd->tick_raw = sched_clock();\n}\n\nnotrace static void __set_sched_clock_stable(void)\n{\n\tstruct sched_clock_data *scd;\n\n\t \n\tlocal_irq_disable();\n\tscd = this_scd();\n\t \n\t__sched_clock_offset = (scd->tick_gtod + __gtod_offset) - (scd->tick_raw);\n\tlocal_irq_enable();\n\n\tprintk(KERN_INFO \"sched_clock: Marking stable (%lld, %lld)->(%lld, %lld)\\n\",\n\t\t\tscd->tick_gtod, __gtod_offset,\n\t\t\tscd->tick_raw,  __sched_clock_offset);\n\n\tstatic_branch_enable(&__sched_clock_stable);\n\ttick_dep_clear(TICK_DEP_BIT_CLOCK_UNSTABLE);\n}\n\n \nnotrace static void __sched_clock_work(struct work_struct *work)\n{\n\tstruct sched_clock_data *scd;\n\tint cpu;\n\n\t \n\tpreempt_disable();\n\tscd = this_scd();\n\t__scd_stamp(scd);\n\tscd->clock = scd->tick_gtod + __gtod_offset;\n\tpreempt_enable();\n\n\t \n\tfor_each_possible_cpu(cpu)\n\t\tper_cpu(sched_clock_data, cpu) = *scd;\n\n\tprintk(KERN_WARNING \"TSC found unstable after boot, most likely due to broken BIOS. Use 'tsc=unstable'.\\n\");\n\tprintk(KERN_INFO \"sched_clock: Marking unstable (%lld, %lld)<-(%lld, %lld)\\n\",\n\t\t\tscd->tick_gtod, __gtod_offset,\n\t\t\tscd->tick_raw,  __sched_clock_offset);\n\n\tstatic_branch_disable(&__sched_clock_stable);\n}\n\nstatic DECLARE_WORK(sched_clock_work, __sched_clock_work);\n\nnotrace static void __clear_sched_clock_stable(void)\n{\n\tif (!sched_clock_stable())\n\t\treturn;\n\n\ttick_dep_set(TICK_DEP_BIT_CLOCK_UNSTABLE);\n\tschedule_work(&sched_clock_work);\n}\n\nnotrace void clear_sched_clock_stable(void)\n{\n\t__sched_clock_stable_early = 0;\n\n\tsmp_mb();  \n\n\tif (static_key_count(&sched_clock_running.key) == 2)\n\t\t__clear_sched_clock_stable();\n}\n\nnotrace static void __sched_clock_gtod_offset(void)\n{\n\tstruct sched_clock_data *scd = this_scd();\n\n\t__scd_stamp(scd);\n\t__gtod_offset = (scd->tick_raw + __sched_clock_offset) - scd->tick_gtod;\n}\n\nvoid __init sched_clock_init(void)\n{\n\t \n\tlocal_irq_disable();\n\t__sched_clock_gtod_offset();\n\tlocal_irq_enable();\n\n\tstatic_branch_inc(&sched_clock_running);\n}\n \nstatic int __init sched_clock_init_late(void)\n{\n\tstatic_branch_inc(&sched_clock_running);\n\t \n\tsmp_mb();  \n\n\tif (__sched_clock_stable_early)\n\t\t__set_sched_clock_stable();\n\n\treturn 0;\n}\nlate_initcall(sched_clock_init_late);\n\n \n\nstatic __always_inline u64 wrap_min(u64 x, u64 y)\n{\n\treturn (s64)(x - y) < 0 ? x : y;\n}\n\nstatic __always_inline u64 wrap_max(u64 x, u64 y)\n{\n\treturn (s64)(x - y) > 0 ? x : y;\n}\n\n \nstatic __always_inline u64 sched_clock_local(struct sched_clock_data *scd)\n{\n\tu64 now, clock, old_clock, min_clock, max_clock, gtod;\n\ts64 delta;\n\nagain:\n\tnow = sched_clock_noinstr();\n\tdelta = now - scd->tick_raw;\n\tif (unlikely(delta < 0))\n\t\tdelta = 0;\n\n\told_clock = scd->clock;\n\n\t \n\n\tgtod = scd->tick_gtod + __gtod_offset;\n\tclock = gtod + delta;\n\tmin_clock = wrap_max(gtod, old_clock);\n\tmax_clock = wrap_max(old_clock, gtod + TICK_NSEC);\n\n\tclock = wrap_max(clock, min_clock);\n\tclock = wrap_min(clock, max_clock);\n\n\tif (!raw_try_cmpxchg64(&scd->clock, &old_clock, clock))\n\t\tgoto again;\n\n\treturn clock;\n}\n\nnoinstr u64 local_clock_noinstr(void)\n{\n\tu64 clock;\n\n\tif (static_branch_likely(&__sched_clock_stable))\n\t\treturn sched_clock_noinstr() + __sched_clock_offset;\n\n\tif (!static_branch_likely(&sched_clock_running))\n\t\treturn sched_clock_noinstr();\n\n\tclock = sched_clock_local(this_scd());\n\n\treturn clock;\n}\n\nu64 local_clock(void)\n{\n\tu64 now;\n\tpreempt_disable_notrace();\n\tnow = local_clock_noinstr();\n\tpreempt_enable_notrace();\n\treturn now;\n}\nEXPORT_SYMBOL_GPL(local_clock);\n\nstatic notrace u64 sched_clock_remote(struct sched_clock_data *scd)\n{\n\tstruct sched_clock_data *my_scd = this_scd();\n\tu64 this_clock, remote_clock;\n\tu64 *ptr, old_val, val;\n\n#if BITS_PER_LONG != 64\nagain:\n\t \n\tthis_clock = sched_clock_local(my_scd);\n\t \n\tremote_clock = cmpxchg64(&scd->clock, 0, 0);\n#else\n\t \n\tsched_clock_local(my_scd);\nagain:\n\tthis_clock = my_scd->clock;\n\tremote_clock = scd->clock;\n#endif\n\n\t \n\tif (likely((s64)(remote_clock - this_clock) < 0)) {\n\t\tptr = &scd->clock;\n\t\told_val = remote_clock;\n\t\tval = this_clock;\n\t} else {\n\t\t \n\t\tptr = &my_scd->clock;\n\t\told_val = this_clock;\n\t\tval = remote_clock;\n\t}\n\n\tif (!try_cmpxchg64(ptr, &old_val, val))\n\t\tgoto again;\n\n\treturn val;\n}\n\n \nnotrace u64 sched_clock_cpu(int cpu)\n{\n\tstruct sched_clock_data *scd;\n\tu64 clock;\n\n\tif (sched_clock_stable())\n\t\treturn sched_clock() + __sched_clock_offset;\n\n\tif (!static_branch_likely(&sched_clock_running))\n\t\treturn sched_clock();\n\n\tpreempt_disable_notrace();\n\tscd = cpu_sdc(cpu);\n\n\tif (cpu != smp_processor_id())\n\t\tclock = sched_clock_remote(scd);\n\telse\n\t\tclock = sched_clock_local(scd);\n\tpreempt_enable_notrace();\n\n\treturn clock;\n}\nEXPORT_SYMBOL_GPL(sched_clock_cpu);\n\nnotrace void sched_clock_tick(void)\n{\n\tstruct sched_clock_data *scd;\n\n\tif (sched_clock_stable())\n\t\treturn;\n\n\tif (!static_branch_likely(&sched_clock_running))\n\t\treturn;\n\n\tlockdep_assert_irqs_disabled();\n\n\tscd = this_scd();\n\t__scd_stamp(scd);\n\tsched_clock_local(scd);\n}\n\nnotrace void sched_clock_tick_stable(void)\n{\n\tif (!sched_clock_stable())\n\t\treturn;\n\n\t \n\tlocal_irq_disable();\n\t__sched_clock_gtod_offset();\n\tlocal_irq_enable();\n}\n\n \nnotrace void sched_clock_idle_sleep_event(void)\n{\n\tsched_clock_cpu(smp_processor_id());\n}\nEXPORT_SYMBOL_GPL(sched_clock_idle_sleep_event);\n\n \nnotrace void sched_clock_idle_wakeup_event(void)\n{\n\tunsigned long flags;\n\n\tif (sched_clock_stable())\n\t\treturn;\n\n\tif (unlikely(timekeeping_suspended))\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\tsched_clock_tick();\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL_GPL(sched_clock_idle_wakeup_event);\n\n#else  \n\nvoid __init sched_clock_init(void)\n{\n\tstatic_branch_inc(&sched_clock_running);\n\tlocal_irq_disable();\n\tgeneric_sched_clock_init();\n\tlocal_irq_enable();\n}\n\nnotrace u64 sched_clock_cpu(int cpu)\n{\n\tif (!static_branch_likely(&sched_clock_running))\n\t\treturn 0;\n\n\treturn sched_clock();\n}\n\n#endif  \n\n \nnotrace u64 __weak running_clock(void)\n{\n\treturn local_clock();\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}