{
  "module_name": "idle.c",
  "hash_id": "66cc2240fe303771a2462f0e1b269a582beeebaf724833d9fd36dd3c967d11d2",
  "original_prompt": "Ingested from linux-6.6.14/kernel/sched/idle.c",
  "human_readable_source": "\n \n\n \nextern char __cpuidle_text_start[], __cpuidle_text_end[];\n\n \nvoid sched_idle_set_state(struct cpuidle_state *idle_state)\n{\n\tidle_set_state(this_rq(), idle_state);\n}\n\nstatic int __read_mostly cpu_idle_force_poll;\n\nvoid cpu_idle_poll_ctrl(bool enable)\n{\n\tif (enable) {\n\t\tcpu_idle_force_poll++;\n\t} else {\n\t\tcpu_idle_force_poll--;\n\t\tWARN_ON_ONCE(cpu_idle_force_poll < 0);\n\t}\n}\n\n#ifdef CONFIG_GENERIC_IDLE_POLL_SETUP\nstatic int __init cpu_idle_poll_setup(char *__unused)\n{\n\tcpu_idle_force_poll = 1;\n\n\treturn 1;\n}\n__setup(\"nohlt\", cpu_idle_poll_setup);\n\nstatic int __init cpu_idle_nopoll_setup(char *__unused)\n{\n\tcpu_idle_force_poll = 0;\n\n\treturn 1;\n}\n__setup(\"hlt\", cpu_idle_nopoll_setup);\n#endif\n\nstatic noinline int __cpuidle cpu_idle_poll(void)\n{\n\tinstrumentation_begin();\n\ttrace_cpu_idle(0, smp_processor_id());\n\tstop_critical_timings();\n\tct_cpuidle_enter();\n\n\traw_local_irq_enable();\n\twhile (!tif_need_resched() &&\n\t       (cpu_idle_force_poll || tick_check_broadcast_expired()))\n\t\tcpu_relax();\n\traw_local_irq_disable();\n\n\tct_cpuidle_exit();\n\tstart_critical_timings();\n\ttrace_cpu_idle(PWR_EVENT_EXIT, smp_processor_id());\n\tlocal_irq_enable();\n\tinstrumentation_end();\n\n\treturn 1;\n}\n\n \nvoid __weak arch_cpu_idle_prepare(void) { }\nvoid __weak arch_cpu_idle_enter(void) { }\nvoid __weak arch_cpu_idle_exit(void) { }\nvoid __weak __noreturn arch_cpu_idle_dead(void) { while (1); }\nvoid __weak arch_cpu_idle(void)\n{\n\tcpu_idle_force_poll = 1;\n}\n\n \nvoid __cpuidle default_idle_call(void)\n{\n\tinstrumentation_begin();\n\tif (!current_clr_polling_and_test()) {\n\t\ttrace_cpu_idle(1, smp_processor_id());\n\t\tstop_critical_timings();\n\n\t\tct_cpuidle_enter();\n\t\tarch_cpu_idle();\n\t\tct_cpuidle_exit();\n\n\t\tstart_critical_timings();\n\t\ttrace_cpu_idle(PWR_EVENT_EXIT, smp_processor_id());\n\t}\n\tlocal_irq_enable();\n\tinstrumentation_end();\n}\n\nstatic int call_cpuidle_s2idle(struct cpuidle_driver *drv,\n\t\t\t       struct cpuidle_device *dev)\n{\n\tif (current_clr_polling_and_test())\n\t\treturn -EBUSY;\n\n\treturn cpuidle_enter_s2idle(drv, dev);\n}\n\nstatic int call_cpuidle(struct cpuidle_driver *drv, struct cpuidle_device *dev,\n\t\t      int next_state)\n{\n\t \n\tif (current_clr_polling_and_test()) {\n\t\tdev->last_residency_ns = 0;\n\t\tlocal_irq_enable();\n\t\treturn -EBUSY;\n\t}\n\n\t \n\treturn cpuidle_enter(drv, dev, next_state);\n}\n\n \nstatic void cpuidle_idle_call(void)\n{\n\tstruct cpuidle_device *dev = cpuidle_get_device();\n\tstruct cpuidle_driver *drv = cpuidle_get_cpu_driver(dev);\n\tint next_state, entered_state;\n\n\t \n\tif (need_resched()) {\n\t\tlocal_irq_enable();\n\t\treturn;\n\t}\n\n\t \n\n\tif (cpuidle_not_available(drv, dev)) {\n\t\ttick_nohz_idle_stop_tick();\n\n\t\tdefault_idle_call();\n\t\tgoto exit_idle;\n\t}\n\n\t \n\n\tif (idle_should_enter_s2idle() || dev->forced_idle_latency_limit_ns) {\n\t\tu64 max_latency_ns;\n\n\t\tif (idle_should_enter_s2idle()) {\n\n\t\t\tentered_state = call_cpuidle_s2idle(drv, dev);\n\t\t\tif (entered_state > 0)\n\t\t\t\tgoto exit_idle;\n\n\t\t\tmax_latency_ns = U64_MAX;\n\t\t} else {\n\t\t\tmax_latency_ns = dev->forced_idle_latency_limit_ns;\n\t\t}\n\n\t\ttick_nohz_idle_stop_tick();\n\n\t\tnext_state = cpuidle_find_deepest_state(drv, dev, max_latency_ns);\n\t\tcall_cpuidle(drv, dev, next_state);\n\t} else {\n\t\tbool stop_tick = true;\n\n\t\t \n\t\tnext_state = cpuidle_select(drv, dev, &stop_tick);\n\n\t\tif (stop_tick || tick_nohz_tick_stopped())\n\t\t\ttick_nohz_idle_stop_tick();\n\t\telse\n\t\t\ttick_nohz_idle_retain_tick();\n\n\t\tentered_state = call_cpuidle(drv, dev, next_state);\n\t\t \n\t\tcpuidle_reflect(dev, entered_state);\n\t}\n\nexit_idle:\n\t__current_set_polling();\n\n\t \n\tif (WARN_ON_ONCE(irqs_disabled()))\n\t\tlocal_irq_enable();\n}\n\n \nstatic void do_idle(void)\n{\n\tint cpu = smp_processor_id();\n\n\t \n\tnohz_run_idle_balance(cpu);\n\n\t \n\n\t__current_set_polling();\n\ttick_nohz_idle_enter();\n\n\twhile (!need_resched()) {\n\t\trmb();\n\n\t\tlocal_irq_disable();\n\n\t\tif (cpu_is_offline(cpu)) {\n\t\t\ttick_nohz_idle_stop_tick();\n\t\t\tcpuhp_report_idle_dead();\n\t\t\tarch_cpu_idle_dead();\n\t\t}\n\n\t\tarch_cpu_idle_enter();\n\t\trcu_nocb_flush_deferred_wakeup();\n\n\t\t \n\t\tif (cpu_idle_force_poll || tick_check_broadcast_expired()) {\n\t\t\ttick_nohz_idle_restart_tick();\n\t\t\tcpu_idle_poll();\n\t\t} else {\n\t\t\tcpuidle_idle_call();\n\t\t}\n\t\tarch_cpu_idle_exit();\n\t}\n\n\t \n\tpreempt_set_need_resched();\n\ttick_nohz_idle_exit();\n\t__current_clr_polling();\n\n\t \n\tsmp_mb__after_atomic();\n\n\t \n\tflush_smp_call_function_queue();\n\tschedule_idle();\n\n\tif (unlikely(klp_patch_pending(current)))\n\t\tklp_update_patch_state(current);\n}\n\nbool cpu_in_idle(unsigned long pc)\n{\n\treturn pc >= (unsigned long)__cpuidle_text_start &&\n\t\tpc < (unsigned long)__cpuidle_text_end;\n}\n\nstruct idle_timer {\n\tstruct hrtimer timer;\n\tint done;\n};\n\nstatic enum hrtimer_restart idle_inject_timer_fn(struct hrtimer *timer)\n{\n\tstruct idle_timer *it = container_of(timer, struct idle_timer, timer);\n\n\tWRITE_ONCE(it->done, 1);\n\tset_tsk_need_resched(current);\n\n\treturn HRTIMER_NORESTART;\n}\n\nvoid play_idle_precise(u64 duration_ns, u64 latency_ns)\n{\n\tstruct idle_timer it;\n\n\t \n\tWARN_ON_ONCE(current->policy != SCHED_FIFO);\n\tWARN_ON_ONCE(current->nr_cpus_allowed != 1);\n\tWARN_ON_ONCE(!(current->flags & PF_KTHREAD));\n\tWARN_ON_ONCE(!(current->flags & PF_NO_SETAFFINITY));\n\tWARN_ON_ONCE(!duration_ns);\n\tWARN_ON_ONCE(current->mm);\n\n\trcu_sleep_check();\n\tpreempt_disable();\n\tcurrent->flags |= PF_IDLE;\n\tcpuidle_use_deepest_state(latency_ns);\n\n\tit.done = 0;\n\thrtimer_init_on_stack(&it.timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL_HARD);\n\tit.timer.function = idle_inject_timer_fn;\n\thrtimer_start(&it.timer, ns_to_ktime(duration_ns),\n\t\t      HRTIMER_MODE_REL_PINNED_HARD);\n\n\twhile (!READ_ONCE(it.done))\n\t\tdo_idle();\n\n\tcpuidle_use_deepest_state(0);\n\tcurrent->flags &= ~PF_IDLE;\n\n\tpreempt_fold_need_resched();\n\tpreempt_enable();\n}\nEXPORT_SYMBOL_GPL(play_idle_precise);\n\nvoid cpu_startup_entry(enum cpuhp_state state)\n{\n\tcurrent->flags |= PF_IDLE;\n\tarch_cpu_idle_prepare();\n\tcpuhp_online_idle(state);\n\twhile (1)\n\t\tdo_idle();\n}\n\n \n\n#ifdef CONFIG_SMP\nstatic int\nselect_task_rq_idle(struct task_struct *p, int cpu, int flags)\n{\n\treturn task_cpu(p);  \n}\n\nstatic int\nbalance_idle(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)\n{\n\treturn WARN_ON_ONCE(1);\n}\n#endif\n\n \nstatic void check_preempt_curr_idle(struct rq *rq, struct task_struct *p, int flags)\n{\n\tresched_curr(rq);\n}\n\nstatic void put_prev_task_idle(struct rq *rq, struct task_struct *prev)\n{\n}\n\nstatic void set_next_task_idle(struct rq *rq, struct task_struct *next, bool first)\n{\n\tupdate_idle_core(rq);\n\tschedstat_inc(rq->sched_goidle);\n}\n\n#ifdef CONFIG_SMP\nstatic struct task_struct *pick_task_idle(struct rq *rq)\n{\n\treturn rq->idle;\n}\n#endif\n\nstruct task_struct *pick_next_task_idle(struct rq *rq)\n{\n\tstruct task_struct *next = rq->idle;\n\n\tset_next_task_idle(rq, next, true);\n\n\treturn next;\n}\n\n \nstatic void\ndequeue_task_idle(struct rq *rq, struct task_struct *p, int flags)\n{\n\traw_spin_rq_unlock_irq(rq);\n\tprintk(KERN_ERR \"bad: scheduling from the idle thread!\\n\");\n\tdump_stack();\n\traw_spin_rq_lock_irq(rq);\n}\n\n \nstatic void task_tick_idle(struct rq *rq, struct task_struct *curr, int queued)\n{\n}\n\nstatic void switched_to_idle(struct rq *rq, struct task_struct *p)\n{\n\tBUG();\n}\n\nstatic void\nprio_changed_idle(struct rq *rq, struct task_struct *p, int oldprio)\n{\n\tBUG();\n}\n\nstatic void update_curr_idle(struct rq *rq)\n{\n}\n\n \nDEFINE_SCHED_CLASS(idle) = {\n\n\t \n\n\t \n\t.dequeue_task\t\t= dequeue_task_idle,\n\n\t.check_preempt_curr\t= check_preempt_curr_idle,\n\n\t.pick_next_task\t\t= pick_next_task_idle,\n\t.put_prev_task\t\t= put_prev_task_idle,\n\t.set_next_task          = set_next_task_idle,\n\n#ifdef CONFIG_SMP\n\t.balance\t\t= balance_idle,\n\t.pick_task\t\t= pick_task_idle,\n\t.select_task_rq\t\t= select_task_rq_idle,\n\t.set_cpus_allowed\t= set_cpus_allowed_common,\n#endif\n\n\t.task_tick\t\t= task_tick_idle,\n\n\t.prio_changed\t\t= prio_changed_idle,\n\t.switched_to\t\t= switched_to_idle,\n\t.update_curr\t\t= update_curr_idle,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}