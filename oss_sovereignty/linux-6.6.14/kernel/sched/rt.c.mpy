{
  "module_name": "rt.c",
  "hash_id": "1fcc51ac1f1408b934f4c577900ced2f6ca68fc72fad57af4fe67178378d84b0",
  "original_prompt": "Ingested from linux-6.6.14/kernel/sched/rt.c",
  "human_readable_source": "\n \n\nint sched_rr_timeslice = RR_TIMESLICE;\n \nstatic const u64 max_rt_runtime = MAX_BW;\n\nstatic int do_sched_rt_period_timer(struct rt_bandwidth *rt_b, int overrun);\n\nstruct rt_bandwidth def_rt_bandwidth;\n\n \nunsigned int sysctl_sched_rt_period = 1000000;\n\n \nint sysctl_sched_rt_runtime = 950000;\n\n#ifdef CONFIG_SYSCTL\nstatic int sysctl_sched_rr_timeslice = (MSEC_PER_SEC * RR_TIMESLICE) / HZ;\nstatic int sched_rt_handler(struct ctl_table *table, int write, void *buffer,\n\t\tsize_t *lenp, loff_t *ppos);\nstatic int sched_rr_handler(struct ctl_table *table, int write, void *buffer,\n\t\tsize_t *lenp, loff_t *ppos);\nstatic struct ctl_table sched_rt_sysctls[] = {\n\t{\n\t\t.procname       = \"sched_rt_period_us\",\n\t\t.data           = &sysctl_sched_rt_period,\n\t\t.maxlen         = sizeof(unsigned int),\n\t\t.mode           = 0644,\n\t\t.proc_handler   = sched_rt_handler,\n\t},\n\t{\n\t\t.procname       = \"sched_rt_runtime_us\",\n\t\t.data           = &sysctl_sched_rt_runtime,\n\t\t.maxlen         = sizeof(int),\n\t\t.mode           = 0644,\n\t\t.proc_handler   = sched_rt_handler,\n\t},\n\t{\n\t\t.procname       = \"sched_rr_timeslice_ms\",\n\t\t.data           = &sysctl_sched_rr_timeslice,\n\t\t.maxlen         = sizeof(int),\n\t\t.mode           = 0644,\n\t\t.proc_handler   = sched_rr_handler,\n\t},\n\t{}\n};\n\nstatic int __init sched_rt_sysctl_init(void)\n{\n\tregister_sysctl_init(\"kernel\", sched_rt_sysctls);\n\treturn 0;\n}\nlate_initcall(sched_rt_sysctl_init);\n#endif\n\nstatic enum hrtimer_restart sched_rt_period_timer(struct hrtimer *timer)\n{\n\tstruct rt_bandwidth *rt_b =\n\t\tcontainer_of(timer, struct rt_bandwidth, rt_period_timer);\n\tint idle = 0;\n\tint overrun;\n\n\traw_spin_lock(&rt_b->rt_runtime_lock);\n\tfor (;;) {\n\t\toverrun = hrtimer_forward_now(timer, rt_b->rt_period);\n\t\tif (!overrun)\n\t\t\tbreak;\n\n\t\traw_spin_unlock(&rt_b->rt_runtime_lock);\n\t\tidle = do_sched_rt_period_timer(rt_b, overrun);\n\t\traw_spin_lock(&rt_b->rt_runtime_lock);\n\t}\n\tif (idle)\n\t\trt_b->rt_period_active = 0;\n\traw_spin_unlock(&rt_b->rt_runtime_lock);\n\n\treturn idle ? HRTIMER_NORESTART : HRTIMER_RESTART;\n}\n\nvoid init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime)\n{\n\trt_b->rt_period = ns_to_ktime(period);\n\trt_b->rt_runtime = runtime;\n\n\traw_spin_lock_init(&rt_b->rt_runtime_lock);\n\n\thrtimer_init(&rt_b->rt_period_timer, CLOCK_MONOTONIC,\n\t\t     HRTIMER_MODE_REL_HARD);\n\trt_b->rt_period_timer.function = sched_rt_period_timer;\n}\n\nstatic inline void do_start_rt_bandwidth(struct rt_bandwidth *rt_b)\n{\n\traw_spin_lock(&rt_b->rt_runtime_lock);\n\tif (!rt_b->rt_period_active) {\n\t\trt_b->rt_period_active = 1;\n\t\t \n\t\thrtimer_forward_now(&rt_b->rt_period_timer, ns_to_ktime(0));\n\t\thrtimer_start_expires(&rt_b->rt_period_timer,\n\t\t\t\t      HRTIMER_MODE_ABS_PINNED_HARD);\n\t}\n\traw_spin_unlock(&rt_b->rt_runtime_lock);\n}\n\nstatic void start_rt_bandwidth(struct rt_bandwidth *rt_b)\n{\n\tif (!rt_bandwidth_enabled() || rt_b->rt_runtime == RUNTIME_INF)\n\t\treturn;\n\n\tdo_start_rt_bandwidth(rt_b);\n}\n\nvoid init_rt_rq(struct rt_rq *rt_rq)\n{\n\tstruct rt_prio_array *array;\n\tint i;\n\n\tarray = &rt_rq->active;\n\tfor (i = 0; i < MAX_RT_PRIO; i++) {\n\t\tINIT_LIST_HEAD(array->queue + i);\n\t\t__clear_bit(i, array->bitmap);\n\t}\n\t \n\t__set_bit(MAX_RT_PRIO, array->bitmap);\n\n#if defined CONFIG_SMP\n\trt_rq->highest_prio.curr = MAX_RT_PRIO-1;\n\trt_rq->highest_prio.next = MAX_RT_PRIO-1;\n\trt_rq->rt_nr_migratory = 0;\n\trt_rq->overloaded = 0;\n\tplist_head_init(&rt_rq->pushable_tasks);\n#endif  \n\t \n\trt_rq->rt_queued = 0;\n\n\trt_rq->rt_time = 0;\n\trt_rq->rt_throttled = 0;\n\trt_rq->rt_runtime = 0;\n\traw_spin_lock_init(&rt_rq->rt_runtime_lock);\n}\n\n#ifdef CONFIG_RT_GROUP_SCHED\nstatic void destroy_rt_bandwidth(struct rt_bandwidth *rt_b)\n{\n\thrtimer_cancel(&rt_b->rt_period_timer);\n}\n\n#define rt_entity_is_task(rt_se) (!(rt_se)->my_q)\n\nstatic inline struct task_struct *rt_task_of(struct sched_rt_entity *rt_se)\n{\n#ifdef CONFIG_SCHED_DEBUG\n\tWARN_ON_ONCE(!rt_entity_is_task(rt_se));\n#endif\n\treturn container_of(rt_se, struct task_struct, rt);\n}\n\nstatic inline struct rq *rq_of_rt_rq(struct rt_rq *rt_rq)\n{\n\treturn rt_rq->rq;\n}\n\nstatic inline struct rt_rq *rt_rq_of_se(struct sched_rt_entity *rt_se)\n{\n\treturn rt_se->rt_rq;\n}\n\nstatic inline struct rq *rq_of_rt_se(struct sched_rt_entity *rt_se)\n{\n\tstruct rt_rq *rt_rq = rt_se->rt_rq;\n\n\treturn rt_rq->rq;\n}\n\nvoid unregister_rt_sched_group(struct task_group *tg)\n{\n\tif (tg->rt_se)\n\t\tdestroy_rt_bandwidth(&tg->rt_bandwidth);\n\n}\n\nvoid free_rt_sched_group(struct task_group *tg)\n{\n\tint i;\n\n\tfor_each_possible_cpu(i) {\n\t\tif (tg->rt_rq)\n\t\t\tkfree(tg->rt_rq[i]);\n\t\tif (tg->rt_se)\n\t\t\tkfree(tg->rt_se[i]);\n\t}\n\n\tkfree(tg->rt_rq);\n\tkfree(tg->rt_se);\n}\n\nvoid init_tg_rt_entry(struct task_group *tg, struct rt_rq *rt_rq,\n\t\tstruct sched_rt_entity *rt_se, int cpu,\n\t\tstruct sched_rt_entity *parent)\n{\n\tstruct rq *rq = cpu_rq(cpu);\n\n\trt_rq->highest_prio.curr = MAX_RT_PRIO-1;\n\trt_rq->rt_nr_boosted = 0;\n\trt_rq->rq = rq;\n\trt_rq->tg = tg;\n\n\ttg->rt_rq[cpu] = rt_rq;\n\ttg->rt_se[cpu] = rt_se;\n\n\tif (!rt_se)\n\t\treturn;\n\n\tif (!parent)\n\t\trt_se->rt_rq = &rq->rt;\n\telse\n\t\trt_se->rt_rq = parent->my_q;\n\n\trt_se->my_q = rt_rq;\n\trt_se->parent = parent;\n\tINIT_LIST_HEAD(&rt_se->run_list);\n}\n\nint alloc_rt_sched_group(struct task_group *tg, struct task_group *parent)\n{\n\tstruct rt_rq *rt_rq;\n\tstruct sched_rt_entity *rt_se;\n\tint i;\n\n\ttg->rt_rq = kcalloc(nr_cpu_ids, sizeof(rt_rq), GFP_KERNEL);\n\tif (!tg->rt_rq)\n\t\tgoto err;\n\ttg->rt_se = kcalloc(nr_cpu_ids, sizeof(rt_se), GFP_KERNEL);\n\tif (!tg->rt_se)\n\t\tgoto err;\n\n\tinit_rt_bandwidth(&tg->rt_bandwidth,\n\t\t\tktime_to_ns(def_rt_bandwidth.rt_period), 0);\n\n\tfor_each_possible_cpu(i) {\n\t\trt_rq = kzalloc_node(sizeof(struct rt_rq),\n\t\t\t\t     GFP_KERNEL, cpu_to_node(i));\n\t\tif (!rt_rq)\n\t\t\tgoto err;\n\n\t\trt_se = kzalloc_node(sizeof(struct sched_rt_entity),\n\t\t\t\t     GFP_KERNEL, cpu_to_node(i));\n\t\tif (!rt_se)\n\t\t\tgoto err_free_rq;\n\n\t\tinit_rt_rq(rt_rq);\n\t\trt_rq->rt_runtime = tg->rt_bandwidth.rt_runtime;\n\t\tinit_tg_rt_entry(tg, rt_rq, rt_se, i, parent->rt_se[i]);\n\t}\n\n\treturn 1;\n\nerr_free_rq:\n\tkfree(rt_rq);\nerr:\n\treturn 0;\n}\n\n#else  \n\n#define rt_entity_is_task(rt_se) (1)\n\nstatic inline struct task_struct *rt_task_of(struct sched_rt_entity *rt_se)\n{\n\treturn container_of(rt_se, struct task_struct, rt);\n}\n\nstatic inline struct rq *rq_of_rt_rq(struct rt_rq *rt_rq)\n{\n\treturn container_of(rt_rq, struct rq, rt);\n}\n\nstatic inline struct rq *rq_of_rt_se(struct sched_rt_entity *rt_se)\n{\n\tstruct task_struct *p = rt_task_of(rt_se);\n\n\treturn task_rq(p);\n}\n\nstatic inline struct rt_rq *rt_rq_of_se(struct sched_rt_entity *rt_se)\n{\n\tstruct rq *rq = rq_of_rt_se(rt_se);\n\n\treturn &rq->rt;\n}\n\nvoid unregister_rt_sched_group(struct task_group *tg) { }\n\nvoid free_rt_sched_group(struct task_group *tg) { }\n\nint alloc_rt_sched_group(struct task_group *tg, struct task_group *parent)\n{\n\treturn 1;\n}\n#endif  \n\n#ifdef CONFIG_SMP\n\nstatic inline bool need_pull_rt_task(struct rq *rq, struct task_struct *prev)\n{\n\t \n\treturn rq->online && rq->rt.highest_prio.curr > prev->prio;\n}\n\nstatic inline int rt_overloaded(struct rq *rq)\n{\n\treturn atomic_read(&rq->rd->rto_count);\n}\n\nstatic inline void rt_set_overload(struct rq *rq)\n{\n\tif (!rq->online)\n\t\treturn;\n\n\tcpumask_set_cpu(rq->cpu, rq->rd->rto_mask);\n\t \n\tsmp_wmb();\n\tatomic_inc(&rq->rd->rto_count);\n}\n\nstatic inline void rt_clear_overload(struct rq *rq)\n{\n\tif (!rq->online)\n\t\treturn;\n\n\t \n\tatomic_dec(&rq->rd->rto_count);\n\tcpumask_clear_cpu(rq->cpu, rq->rd->rto_mask);\n}\n\nstatic void update_rt_migration(struct rt_rq *rt_rq)\n{\n\tif (rt_rq->rt_nr_migratory && rt_rq->rt_nr_total > 1) {\n\t\tif (!rt_rq->overloaded) {\n\t\t\trt_set_overload(rq_of_rt_rq(rt_rq));\n\t\t\trt_rq->overloaded = 1;\n\t\t}\n\t} else if (rt_rq->overloaded) {\n\t\trt_clear_overload(rq_of_rt_rq(rt_rq));\n\t\trt_rq->overloaded = 0;\n\t}\n}\n\nstatic void inc_rt_migration(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)\n{\n\tstruct task_struct *p;\n\n\tif (!rt_entity_is_task(rt_se))\n\t\treturn;\n\n\tp = rt_task_of(rt_se);\n\trt_rq = &rq_of_rt_rq(rt_rq)->rt;\n\n\trt_rq->rt_nr_total++;\n\tif (p->nr_cpus_allowed > 1)\n\t\trt_rq->rt_nr_migratory++;\n\n\tupdate_rt_migration(rt_rq);\n}\n\nstatic void dec_rt_migration(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)\n{\n\tstruct task_struct *p;\n\n\tif (!rt_entity_is_task(rt_se))\n\t\treturn;\n\n\tp = rt_task_of(rt_se);\n\trt_rq = &rq_of_rt_rq(rt_rq)->rt;\n\n\trt_rq->rt_nr_total--;\n\tif (p->nr_cpus_allowed > 1)\n\t\trt_rq->rt_nr_migratory--;\n\n\tupdate_rt_migration(rt_rq);\n}\n\nstatic inline int has_pushable_tasks(struct rq *rq)\n{\n\treturn !plist_head_empty(&rq->rt.pushable_tasks);\n}\n\nstatic DEFINE_PER_CPU(struct balance_callback, rt_push_head);\nstatic DEFINE_PER_CPU(struct balance_callback, rt_pull_head);\n\nstatic void push_rt_tasks(struct rq *);\nstatic void pull_rt_task(struct rq *);\n\nstatic inline void rt_queue_push_tasks(struct rq *rq)\n{\n\tif (!has_pushable_tasks(rq))\n\t\treturn;\n\n\tqueue_balance_callback(rq, &per_cpu(rt_push_head, rq->cpu), push_rt_tasks);\n}\n\nstatic inline void rt_queue_pull_task(struct rq *rq)\n{\n\tqueue_balance_callback(rq, &per_cpu(rt_pull_head, rq->cpu), pull_rt_task);\n}\n\nstatic void enqueue_pushable_task(struct rq *rq, struct task_struct *p)\n{\n\tplist_del(&p->pushable_tasks, &rq->rt.pushable_tasks);\n\tplist_node_init(&p->pushable_tasks, p->prio);\n\tplist_add(&p->pushable_tasks, &rq->rt.pushable_tasks);\n\n\t \n\tif (p->prio < rq->rt.highest_prio.next)\n\t\trq->rt.highest_prio.next = p->prio;\n}\n\nstatic void dequeue_pushable_task(struct rq *rq, struct task_struct *p)\n{\n\tplist_del(&p->pushable_tasks, &rq->rt.pushable_tasks);\n\n\t \n\tif (has_pushable_tasks(rq)) {\n\t\tp = plist_first_entry(&rq->rt.pushable_tasks,\n\t\t\t\t      struct task_struct, pushable_tasks);\n\t\trq->rt.highest_prio.next = p->prio;\n\t} else {\n\t\trq->rt.highest_prio.next = MAX_RT_PRIO-1;\n\t}\n}\n\n#else\n\nstatic inline void enqueue_pushable_task(struct rq *rq, struct task_struct *p)\n{\n}\n\nstatic inline void dequeue_pushable_task(struct rq *rq, struct task_struct *p)\n{\n}\n\nstatic inline\nvoid inc_rt_migration(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)\n{\n}\n\nstatic inline\nvoid dec_rt_migration(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)\n{\n}\n\nstatic inline void rt_queue_push_tasks(struct rq *rq)\n{\n}\n#endif  \n\nstatic void enqueue_top_rt_rq(struct rt_rq *rt_rq);\nstatic void dequeue_top_rt_rq(struct rt_rq *rt_rq, unsigned int count);\n\nstatic inline int on_rt_rq(struct sched_rt_entity *rt_se)\n{\n\treturn rt_se->on_rq;\n}\n\n#ifdef CONFIG_UCLAMP_TASK\n \nstatic inline bool rt_task_fits_capacity(struct task_struct *p, int cpu)\n{\n\tunsigned int min_cap;\n\tunsigned int max_cap;\n\tunsigned int cpu_cap;\n\n\t \n\tif (!sched_asym_cpucap_active())\n\t\treturn true;\n\n\tmin_cap = uclamp_eff_value(p, UCLAMP_MIN);\n\tmax_cap = uclamp_eff_value(p, UCLAMP_MAX);\n\n\tcpu_cap = capacity_orig_of(cpu);\n\n\treturn cpu_cap >= min(min_cap, max_cap);\n}\n#else\nstatic inline bool rt_task_fits_capacity(struct task_struct *p, int cpu)\n{\n\treturn true;\n}\n#endif\n\n#ifdef CONFIG_RT_GROUP_SCHED\n\nstatic inline u64 sched_rt_runtime(struct rt_rq *rt_rq)\n{\n\tif (!rt_rq->tg)\n\t\treturn RUNTIME_INF;\n\n\treturn rt_rq->rt_runtime;\n}\n\nstatic inline u64 sched_rt_period(struct rt_rq *rt_rq)\n{\n\treturn ktime_to_ns(rt_rq->tg->rt_bandwidth.rt_period);\n}\n\ntypedef struct task_group *rt_rq_iter_t;\n\nstatic inline struct task_group *next_task_group(struct task_group *tg)\n{\n\tdo {\n\t\ttg = list_entry_rcu(tg->list.next,\n\t\t\ttypeof(struct task_group), list);\n\t} while (&tg->list != &task_groups && task_group_is_autogroup(tg));\n\n\tif (&tg->list == &task_groups)\n\t\ttg = NULL;\n\n\treturn tg;\n}\n\n#define for_each_rt_rq(rt_rq, iter, rq)\t\t\t\t\t\\\n\tfor (iter = container_of(&task_groups, typeof(*iter), list);\t\\\n\t\t(iter = next_task_group(iter)) &&\t\t\t\\\n\t\t(rt_rq = iter->rt_rq[cpu_of(rq)]);)\n\n#define for_each_sched_rt_entity(rt_se) \\\n\tfor (; rt_se; rt_se = rt_se->parent)\n\nstatic inline struct rt_rq *group_rt_rq(struct sched_rt_entity *rt_se)\n{\n\treturn rt_se->my_q;\n}\n\nstatic void enqueue_rt_entity(struct sched_rt_entity *rt_se, unsigned int flags);\nstatic void dequeue_rt_entity(struct sched_rt_entity *rt_se, unsigned int flags);\n\nstatic void sched_rt_rq_enqueue(struct rt_rq *rt_rq)\n{\n\tstruct task_struct *curr = rq_of_rt_rq(rt_rq)->curr;\n\tstruct rq *rq = rq_of_rt_rq(rt_rq);\n\tstruct sched_rt_entity *rt_se;\n\n\tint cpu = cpu_of(rq);\n\n\trt_se = rt_rq->tg->rt_se[cpu];\n\n\tif (rt_rq->rt_nr_running) {\n\t\tif (!rt_se)\n\t\t\tenqueue_top_rt_rq(rt_rq);\n\t\telse if (!on_rt_rq(rt_se))\n\t\t\tenqueue_rt_entity(rt_se, 0);\n\n\t\tif (rt_rq->highest_prio.curr < curr->prio)\n\t\t\tresched_curr(rq);\n\t}\n}\n\nstatic void sched_rt_rq_dequeue(struct rt_rq *rt_rq)\n{\n\tstruct sched_rt_entity *rt_se;\n\tint cpu = cpu_of(rq_of_rt_rq(rt_rq));\n\n\trt_se = rt_rq->tg->rt_se[cpu];\n\n\tif (!rt_se) {\n\t\tdequeue_top_rt_rq(rt_rq, rt_rq->rt_nr_running);\n\t\t \n\t\tcpufreq_update_util(rq_of_rt_rq(rt_rq), 0);\n\t}\n\telse if (on_rt_rq(rt_se))\n\t\tdequeue_rt_entity(rt_se, 0);\n}\n\nstatic inline int rt_rq_throttled(struct rt_rq *rt_rq)\n{\n\treturn rt_rq->rt_throttled && !rt_rq->rt_nr_boosted;\n}\n\nstatic int rt_se_boosted(struct sched_rt_entity *rt_se)\n{\n\tstruct rt_rq *rt_rq = group_rt_rq(rt_se);\n\tstruct task_struct *p;\n\n\tif (rt_rq)\n\t\treturn !!rt_rq->rt_nr_boosted;\n\n\tp = rt_task_of(rt_se);\n\treturn p->prio != p->normal_prio;\n}\n\n#ifdef CONFIG_SMP\nstatic inline const struct cpumask *sched_rt_period_mask(void)\n{\n\treturn this_rq()->rd->span;\n}\n#else\nstatic inline const struct cpumask *sched_rt_period_mask(void)\n{\n\treturn cpu_online_mask;\n}\n#endif\n\nstatic inline\nstruct rt_rq *sched_rt_period_rt_rq(struct rt_bandwidth *rt_b, int cpu)\n{\n\treturn container_of(rt_b, struct task_group, rt_bandwidth)->rt_rq[cpu];\n}\n\nstatic inline struct rt_bandwidth *sched_rt_bandwidth(struct rt_rq *rt_rq)\n{\n\treturn &rt_rq->tg->rt_bandwidth;\n}\n\n#else  \n\nstatic inline u64 sched_rt_runtime(struct rt_rq *rt_rq)\n{\n\treturn rt_rq->rt_runtime;\n}\n\nstatic inline u64 sched_rt_period(struct rt_rq *rt_rq)\n{\n\treturn ktime_to_ns(def_rt_bandwidth.rt_period);\n}\n\ntypedef struct rt_rq *rt_rq_iter_t;\n\n#define for_each_rt_rq(rt_rq, iter, rq) \\\n\tfor ((void) iter, rt_rq = &rq->rt; rt_rq; rt_rq = NULL)\n\n#define for_each_sched_rt_entity(rt_se) \\\n\tfor (; rt_se; rt_se = NULL)\n\nstatic inline struct rt_rq *group_rt_rq(struct sched_rt_entity *rt_se)\n{\n\treturn NULL;\n}\n\nstatic inline void sched_rt_rq_enqueue(struct rt_rq *rt_rq)\n{\n\tstruct rq *rq = rq_of_rt_rq(rt_rq);\n\n\tif (!rt_rq->rt_nr_running)\n\t\treturn;\n\n\tenqueue_top_rt_rq(rt_rq);\n\tresched_curr(rq);\n}\n\nstatic inline void sched_rt_rq_dequeue(struct rt_rq *rt_rq)\n{\n\tdequeue_top_rt_rq(rt_rq, rt_rq->rt_nr_running);\n}\n\nstatic inline int rt_rq_throttled(struct rt_rq *rt_rq)\n{\n\treturn rt_rq->rt_throttled;\n}\n\nstatic inline const struct cpumask *sched_rt_period_mask(void)\n{\n\treturn cpu_online_mask;\n}\n\nstatic inline\nstruct rt_rq *sched_rt_period_rt_rq(struct rt_bandwidth *rt_b, int cpu)\n{\n\treturn &cpu_rq(cpu)->rt;\n}\n\nstatic inline struct rt_bandwidth *sched_rt_bandwidth(struct rt_rq *rt_rq)\n{\n\treturn &def_rt_bandwidth;\n}\n\n#endif  \n\nbool sched_rt_bandwidth_account(struct rt_rq *rt_rq)\n{\n\tstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);\n\n\treturn (hrtimer_active(&rt_b->rt_period_timer) ||\n\t\trt_rq->rt_time < rt_b->rt_runtime);\n}\n\n#ifdef CONFIG_SMP\n \nstatic void do_balance_runtime(struct rt_rq *rt_rq)\n{\n\tstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);\n\tstruct root_domain *rd = rq_of_rt_rq(rt_rq)->rd;\n\tint i, weight;\n\tu64 rt_period;\n\n\tweight = cpumask_weight(rd->span);\n\n\traw_spin_lock(&rt_b->rt_runtime_lock);\n\trt_period = ktime_to_ns(rt_b->rt_period);\n\tfor_each_cpu(i, rd->span) {\n\t\tstruct rt_rq *iter = sched_rt_period_rt_rq(rt_b, i);\n\t\ts64 diff;\n\n\t\tif (iter == rt_rq)\n\t\t\tcontinue;\n\n\t\traw_spin_lock(&iter->rt_runtime_lock);\n\t\t \n\t\tif (iter->rt_runtime == RUNTIME_INF)\n\t\t\tgoto next;\n\n\t\t \n\t\tdiff = iter->rt_runtime - iter->rt_time;\n\t\tif (diff > 0) {\n\t\t\tdiff = div_u64((u64)diff, weight);\n\t\t\tif (rt_rq->rt_runtime + diff > rt_period)\n\t\t\t\tdiff = rt_period - rt_rq->rt_runtime;\n\t\t\titer->rt_runtime -= diff;\n\t\t\trt_rq->rt_runtime += diff;\n\t\t\tif (rt_rq->rt_runtime == rt_period) {\n\t\t\t\traw_spin_unlock(&iter->rt_runtime_lock);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\nnext:\n\t\traw_spin_unlock(&iter->rt_runtime_lock);\n\t}\n\traw_spin_unlock(&rt_b->rt_runtime_lock);\n}\n\n \nstatic void __disable_runtime(struct rq *rq)\n{\n\tstruct root_domain *rd = rq->rd;\n\trt_rq_iter_t iter;\n\tstruct rt_rq *rt_rq;\n\n\tif (unlikely(!scheduler_running))\n\t\treturn;\n\n\tfor_each_rt_rq(rt_rq, iter, rq) {\n\t\tstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);\n\t\ts64 want;\n\t\tint i;\n\n\t\traw_spin_lock(&rt_b->rt_runtime_lock);\n\t\traw_spin_lock(&rt_rq->rt_runtime_lock);\n\t\t \n\t\tif (rt_rq->rt_runtime == RUNTIME_INF ||\n\t\t\t\trt_rq->rt_runtime == rt_b->rt_runtime)\n\t\t\tgoto balanced;\n\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);\n\n\t\t \n\t\twant = rt_b->rt_runtime - rt_rq->rt_runtime;\n\n\t\t \n\t\tfor_each_cpu(i, rd->span) {\n\t\t\tstruct rt_rq *iter = sched_rt_period_rt_rq(rt_b, i);\n\t\t\ts64 diff;\n\n\t\t\t \n\t\t\tif (iter == rt_rq || iter->rt_runtime == RUNTIME_INF)\n\t\t\t\tcontinue;\n\n\t\t\traw_spin_lock(&iter->rt_runtime_lock);\n\t\t\tif (want > 0) {\n\t\t\t\tdiff = min_t(s64, iter->rt_runtime, want);\n\t\t\t\titer->rt_runtime -= diff;\n\t\t\t\twant -= diff;\n\t\t\t} else {\n\t\t\t\titer->rt_runtime -= want;\n\t\t\t\twant -= want;\n\t\t\t}\n\t\t\traw_spin_unlock(&iter->rt_runtime_lock);\n\n\t\t\tif (!want)\n\t\t\t\tbreak;\n\t\t}\n\n\t\traw_spin_lock(&rt_rq->rt_runtime_lock);\n\t\t \n\t\tWARN_ON_ONCE(want);\nbalanced:\n\t\t \n\t\trt_rq->rt_runtime = RUNTIME_INF;\n\t\trt_rq->rt_throttled = 0;\n\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);\n\t\traw_spin_unlock(&rt_b->rt_runtime_lock);\n\n\t\t \n\t\tsched_rt_rq_enqueue(rt_rq);\n\t}\n}\n\nstatic void __enable_runtime(struct rq *rq)\n{\n\trt_rq_iter_t iter;\n\tstruct rt_rq *rt_rq;\n\n\tif (unlikely(!scheduler_running))\n\t\treturn;\n\n\t \n\tfor_each_rt_rq(rt_rq, iter, rq) {\n\t\tstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);\n\n\t\traw_spin_lock(&rt_b->rt_runtime_lock);\n\t\traw_spin_lock(&rt_rq->rt_runtime_lock);\n\t\trt_rq->rt_runtime = rt_b->rt_runtime;\n\t\trt_rq->rt_time = 0;\n\t\trt_rq->rt_throttled = 0;\n\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);\n\t\traw_spin_unlock(&rt_b->rt_runtime_lock);\n\t}\n}\n\nstatic void balance_runtime(struct rt_rq *rt_rq)\n{\n\tif (!sched_feat(RT_RUNTIME_SHARE))\n\t\treturn;\n\n\tif (rt_rq->rt_time > rt_rq->rt_runtime) {\n\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);\n\t\tdo_balance_runtime(rt_rq);\n\t\traw_spin_lock(&rt_rq->rt_runtime_lock);\n\t}\n}\n#else  \nstatic inline void balance_runtime(struct rt_rq *rt_rq) {}\n#endif  \n\nstatic int do_sched_rt_period_timer(struct rt_bandwidth *rt_b, int overrun)\n{\n\tint i, idle = 1, throttled = 0;\n\tconst struct cpumask *span;\n\n\tspan = sched_rt_period_mask();\n#ifdef CONFIG_RT_GROUP_SCHED\n\t \n\tif (rt_b == &root_task_group.rt_bandwidth)\n\t\tspan = cpu_online_mask;\n#endif\n\tfor_each_cpu(i, span) {\n\t\tint enqueue = 0;\n\t\tstruct rt_rq *rt_rq = sched_rt_period_rt_rq(rt_b, i);\n\t\tstruct rq *rq = rq_of_rt_rq(rt_rq);\n\t\tstruct rq_flags rf;\n\t\tint skip;\n\n\t\t \n\t\traw_spin_lock(&rt_rq->rt_runtime_lock);\n\t\tif (!sched_feat(RT_RUNTIME_SHARE) && rt_rq->rt_runtime != RUNTIME_INF)\n\t\t\trt_rq->rt_runtime = rt_b->rt_runtime;\n\t\tskip = !rt_rq->rt_time && !rt_rq->rt_nr_running;\n\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);\n\t\tif (skip)\n\t\t\tcontinue;\n\n\t\trq_lock(rq, &rf);\n\t\tupdate_rq_clock(rq);\n\n\t\tif (rt_rq->rt_time) {\n\t\t\tu64 runtime;\n\n\t\t\traw_spin_lock(&rt_rq->rt_runtime_lock);\n\t\t\tif (rt_rq->rt_throttled)\n\t\t\t\tbalance_runtime(rt_rq);\n\t\t\truntime = rt_rq->rt_runtime;\n\t\t\trt_rq->rt_time -= min(rt_rq->rt_time, overrun*runtime);\n\t\t\tif (rt_rq->rt_throttled && rt_rq->rt_time < runtime) {\n\t\t\t\trt_rq->rt_throttled = 0;\n\t\t\t\tenqueue = 1;\n\n\t\t\t\t \n\t\t\t\tif (rt_rq->rt_nr_running && rq->curr == rq->idle)\n\t\t\t\t\trq_clock_cancel_skipupdate(rq);\n\t\t\t}\n\t\t\tif (rt_rq->rt_time || rt_rq->rt_nr_running)\n\t\t\t\tidle = 0;\n\t\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);\n\t\t} else if (rt_rq->rt_nr_running) {\n\t\t\tidle = 0;\n\t\t\tif (!rt_rq_throttled(rt_rq))\n\t\t\t\tenqueue = 1;\n\t\t}\n\t\tif (rt_rq->rt_throttled)\n\t\t\tthrottled = 1;\n\n\t\tif (enqueue)\n\t\t\tsched_rt_rq_enqueue(rt_rq);\n\t\trq_unlock(rq, &rf);\n\t}\n\n\tif (!throttled && (!rt_bandwidth_enabled() || rt_b->rt_runtime == RUNTIME_INF))\n\t\treturn 1;\n\n\treturn idle;\n}\n\nstatic inline int rt_se_prio(struct sched_rt_entity *rt_se)\n{\n#ifdef CONFIG_RT_GROUP_SCHED\n\tstruct rt_rq *rt_rq = group_rt_rq(rt_se);\n\n\tif (rt_rq)\n\t\treturn rt_rq->highest_prio.curr;\n#endif\n\n\treturn rt_task_of(rt_se)->prio;\n}\n\nstatic int sched_rt_runtime_exceeded(struct rt_rq *rt_rq)\n{\n\tu64 runtime = sched_rt_runtime(rt_rq);\n\n\tif (rt_rq->rt_throttled)\n\t\treturn rt_rq_throttled(rt_rq);\n\n\tif (runtime >= sched_rt_period(rt_rq))\n\t\treturn 0;\n\n\tbalance_runtime(rt_rq);\n\truntime = sched_rt_runtime(rt_rq);\n\tif (runtime == RUNTIME_INF)\n\t\treturn 0;\n\n\tif (rt_rq->rt_time > runtime) {\n\t\tstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);\n\n\t\t \n\t\tif (likely(rt_b->rt_runtime)) {\n\t\t\trt_rq->rt_throttled = 1;\n\t\t\tprintk_deferred_once(\"sched: RT throttling activated\\n\");\n\t\t} else {\n\t\t\t \n\t\t\trt_rq->rt_time = 0;\n\t\t}\n\n\t\tif (rt_rq_throttled(rt_rq)) {\n\t\t\tsched_rt_rq_dequeue(rt_rq);\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic void update_curr_rt(struct rq *rq)\n{\n\tstruct task_struct *curr = rq->curr;\n\tstruct sched_rt_entity *rt_se = &curr->rt;\n\tu64 delta_exec;\n\tu64 now;\n\n\tif (curr->sched_class != &rt_sched_class)\n\t\treturn;\n\n\tnow = rq_clock_task(rq);\n\tdelta_exec = now - curr->se.exec_start;\n\tif (unlikely((s64)delta_exec <= 0))\n\t\treturn;\n\n\tschedstat_set(curr->stats.exec_max,\n\t\t      max(curr->stats.exec_max, delta_exec));\n\n\ttrace_sched_stat_runtime(curr, delta_exec, 0);\n\n\tupdate_current_exec_runtime(curr, now, delta_exec);\n\n\tif (!rt_bandwidth_enabled())\n\t\treturn;\n\n\tfor_each_sched_rt_entity(rt_se) {\n\t\tstruct rt_rq *rt_rq = rt_rq_of_se(rt_se);\n\t\tint exceeded;\n\n\t\tif (sched_rt_runtime(rt_rq) != RUNTIME_INF) {\n\t\t\traw_spin_lock(&rt_rq->rt_runtime_lock);\n\t\t\trt_rq->rt_time += delta_exec;\n\t\t\texceeded = sched_rt_runtime_exceeded(rt_rq);\n\t\t\tif (exceeded)\n\t\t\t\tresched_curr(rq);\n\t\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);\n\t\t\tif (exceeded)\n\t\t\t\tdo_start_rt_bandwidth(sched_rt_bandwidth(rt_rq));\n\t\t}\n\t}\n}\n\nstatic void\ndequeue_top_rt_rq(struct rt_rq *rt_rq, unsigned int count)\n{\n\tstruct rq *rq = rq_of_rt_rq(rt_rq);\n\n\tBUG_ON(&rq->rt != rt_rq);\n\n\tif (!rt_rq->rt_queued)\n\t\treturn;\n\n\tBUG_ON(!rq->nr_running);\n\n\tsub_nr_running(rq, count);\n\trt_rq->rt_queued = 0;\n\n}\n\nstatic void\nenqueue_top_rt_rq(struct rt_rq *rt_rq)\n{\n\tstruct rq *rq = rq_of_rt_rq(rt_rq);\n\n\tBUG_ON(&rq->rt != rt_rq);\n\n\tif (rt_rq->rt_queued)\n\t\treturn;\n\n\tif (rt_rq_throttled(rt_rq))\n\t\treturn;\n\n\tif (rt_rq->rt_nr_running) {\n\t\tadd_nr_running(rq, rt_rq->rt_nr_running);\n\t\trt_rq->rt_queued = 1;\n\t}\n\n\t \n\tcpufreq_update_util(rq, 0);\n}\n\n#if defined CONFIG_SMP\n\nstatic void\ninc_rt_prio_smp(struct rt_rq *rt_rq, int prio, int prev_prio)\n{\n\tstruct rq *rq = rq_of_rt_rq(rt_rq);\n\n#ifdef CONFIG_RT_GROUP_SCHED\n\t \n\tif (&rq->rt != rt_rq)\n\t\treturn;\n#endif\n\tif (rq->online && prio < prev_prio)\n\t\tcpupri_set(&rq->rd->cpupri, rq->cpu, prio);\n}\n\nstatic void\ndec_rt_prio_smp(struct rt_rq *rt_rq, int prio, int prev_prio)\n{\n\tstruct rq *rq = rq_of_rt_rq(rt_rq);\n\n#ifdef CONFIG_RT_GROUP_SCHED\n\t \n\tif (&rq->rt != rt_rq)\n\t\treturn;\n#endif\n\tif (rq->online && rt_rq->highest_prio.curr != prev_prio)\n\t\tcpupri_set(&rq->rd->cpupri, rq->cpu, rt_rq->highest_prio.curr);\n}\n\n#else  \n\nstatic inline\nvoid inc_rt_prio_smp(struct rt_rq *rt_rq, int prio, int prev_prio) {}\nstatic inline\nvoid dec_rt_prio_smp(struct rt_rq *rt_rq, int prio, int prev_prio) {}\n\n#endif  \n\n#if defined CONFIG_SMP || defined CONFIG_RT_GROUP_SCHED\nstatic void\ninc_rt_prio(struct rt_rq *rt_rq, int prio)\n{\n\tint prev_prio = rt_rq->highest_prio.curr;\n\n\tif (prio < prev_prio)\n\t\trt_rq->highest_prio.curr = prio;\n\n\tinc_rt_prio_smp(rt_rq, prio, prev_prio);\n}\n\nstatic void\ndec_rt_prio(struct rt_rq *rt_rq, int prio)\n{\n\tint prev_prio = rt_rq->highest_prio.curr;\n\n\tif (rt_rq->rt_nr_running) {\n\n\t\tWARN_ON(prio < prev_prio);\n\n\t\t \n\t\tif (prio == prev_prio) {\n\t\t\tstruct rt_prio_array *array = &rt_rq->active;\n\n\t\t\trt_rq->highest_prio.curr =\n\t\t\t\tsched_find_first_bit(array->bitmap);\n\t\t}\n\n\t} else {\n\t\trt_rq->highest_prio.curr = MAX_RT_PRIO-1;\n\t}\n\n\tdec_rt_prio_smp(rt_rq, prio, prev_prio);\n}\n\n#else\n\nstatic inline void inc_rt_prio(struct rt_rq *rt_rq, int prio) {}\nstatic inline void dec_rt_prio(struct rt_rq *rt_rq, int prio) {}\n\n#endif  \n\n#ifdef CONFIG_RT_GROUP_SCHED\n\nstatic void\ninc_rt_group(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)\n{\n\tif (rt_se_boosted(rt_se))\n\t\trt_rq->rt_nr_boosted++;\n\n\tif (rt_rq->tg)\n\t\tstart_rt_bandwidth(&rt_rq->tg->rt_bandwidth);\n}\n\nstatic void\ndec_rt_group(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)\n{\n\tif (rt_se_boosted(rt_se))\n\t\trt_rq->rt_nr_boosted--;\n\n\tWARN_ON(!rt_rq->rt_nr_running && rt_rq->rt_nr_boosted);\n}\n\n#else  \n\nstatic void\ninc_rt_group(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)\n{\n\tstart_rt_bandwidth(&def_rt_bandwidth);\n}\n\nstatic inline\nvoid dec_rt_group(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq) {}\n\n#endif  \n\nstatic inline\nunsigned int rt_se_nr_running(struct sched_rt_entity *rt_se)\n{\n\tstruct rt_rq *group_rq = group_rt_rq(rt_se);\n\n\tif (group_rq)\n\t\treturn group_rq->rt_nr_running;\n\telse\n\t\treturn 1;\n}\n\nstatic inline\nunsigned int rt_se_rr_nr_running(struct sched_rt_entity *rt_se)\n{\n\tstruct rt_rq *group_rq = group_rt_rq(rt_se);\n\tstruct task_struct *tsk;\n\n\tif (group_rq)\n\t\treturn group_rq->rr_nr_running;\n\n\ttsk = rt_task_of(rt_se);\n\n\treturn (tsk->policy == SCHED_RR) ? 1 : 0;\n}\n\nstatic inline\nvoid inc_rt_tasks(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)\n{\n\tint prio = rt_se_prio(rt_se);\n\n\tWARN_ON(!rt_prio(prio));\n\trt_rq->rt_nr_running += rt_se_nr_running(rt_se);\n\trt_rq->rr_nr_running += rt_se_rr_nr_running(rt_se);\n\n\tinc_rt_prio(rt_rq, prio);\n\tinc_rt_migration(rt_se, rt_rq);\n\tinc_rt_group(rt_se, rt_rq);\n}\n\nstatic inline\nvoid dec_rt_tasks(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)\n{\n\tWARN_ON(!rt_prio(rt_se_prio(rt_se)));\n\tWARN_ON(!rt_rq->rt_nr_running);\n\trt_rq->rt_nr_running -= rt_se_nr_running(rt_se);\n\trt_rq->rr_nr_running -= rt_se_rr_nr_running(rt_se);\n\n\tdec_rt_prio(rt_rq, rt_se_prio(rt_se));\n\tdec_rt_migration(rt_se, rt_rq);\n\tdec_rt_group(rt_se, rt_rq);\n}\n\n \nstatic inline bool move_entity(unsigned int flags)\n{\n\tif ((flags & (DEQUEUE_SAVE | DEQUEUE_MOVE)) == DEQUEUE_SAVE)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic void __delist_rt_entity(struct sched_rt_entity *rt_se, struct rt_prio_array *array)\n{\n\tlist_del_init(&rt_se->run_list);\n\n\tif (list_empty(array->queue + rt_se_prio(rt_se)))\n\t\t__clear_bit(rt_se_prio(rt_se), array->bitmap);\n\n\trt_se->on_list = 0;\n}\n\nstatic inline struct sched_statistics *\n__schedstats_from_rt_se(struct sched_rt_entity *rt_se)\n{\n#ifdef CONFIG_RT_GROUP_SCHED\n\t \n\tif (!rt_entity_is_task(rt_se))\n\t\treturn NULL;\n#endif\n\n\treturn &rt_task_of(rt_se)->stats;\n}\n\nstatic inline void\nupdate_stats_wait_start_rt(struct rt_rq *rt_rq, struct sched_rt_entity *rt_se)\n{\n\tstruct sched_statistics *stats;\n\tstruct task_struct *p = NULL;\n\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\tif (rt_entity_is_task(rt_se))\n\t\tp = rt_task_of(rt_se);\n\n\tstats = __schedstats_from_rt_se(rt_se);\n\tif (!stats)\n\t\treturn;\n\n\t__update_stats_wait_start(rq_of_rt_rq(rt_rq), p, stats);\n}\n\nstatic inline void\nupdate_stats_enqueue_sleeper_rt(struct rt_rq *rt_rq, struct sched_rt_entity *rt_se)\n{\n\tstruct sched_statistics *stats;\n\tstruct task_struct *p = NULL;\n\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\tif (rt_entity_is_task(rt_se))\n\t\tp = rt_task_of(rt_se);\n\n\tstats = __schedstats_from_rt_se(rt_se);\n\tif (!stats)\n\t\treturn;\n\n\t__update_stats_enqueue_sleeper(rq_of_rt_rq(rt_rq), p, stats);\n}\n\nstatic inline void\nupdate_stats_enqueue_rt(struct rt_rq *rt_rq, struct sched_rt_entity *rt_se,\n\t\t\tint flags)\n{\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\tif (flags & ENQUEUE_WAKEUP)\n\t\tupdate_stats_enqueue_sleeper_rt(rt_rq, rt_se);\n}\n\nstatic inline void\nupdate_stats_wait_end_rt(struct rt_rq *rt_rq, struct sched_rt_entity *rt_se)\n{\n\tstruct sched_statistics *stats;\n\tstruct task_struct *p = NULL;\n\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\tif (rt_entity_is_task(rt_se))\n\t\tp = rt_task_of(rt_se);\n\n\tstats = __schedstats_from_rt_se(rt_se);\n\tif (!stats)\n\t\treturn;\n\n\t__update_stats_wait_end(rq_of_rt_rq(rt_rq), p, stats);\n}\n\nstatic inline void\nupdate_stats_dequeue_rt(struct rt_rq *rt_rq, struct sched_rt_entity *rt_se,\n\t\t\tint flags)\n{\n\tstruct task_struct *p = NULL;\n\n\tif (!schedstat_enabled())\n\t\treturn;\n\n\tif (rt_entity_is_task(rt_se))\n\t\tp = rt_task_of(rt_se);\n\n\tif ((flags & DEQUEUE_SLEEP) && p) {\n\t\tunsigned int state;\n\n\t\tstate = READ_ONCE(p->__state);\n\t\tif (state & TASK_INTERRUPTIBLE)\n\t\t\t__schedstat_set(p->stats.sleep_start,\n\t\t\t\t\trq_clock(rq_of_rt_rq(rt_rq)));\n\n\t\tif (state & TASK_UNINTERRUPTIBLE)\n\t\t\t__schedstat_set(p->stats.block_start,\n\t\t\t\t\trq_clock(rq_of_rt_rq(rt_rq)));\n\t}\n}\n\nstatic void __enqueue_rt_entity(struct sched_rt_entity *rt_se, unsigned int flags)\n{\n\tstruct rt_rq *rt_rq = rt_rq_of_se(rt_se);\n\tstruct rt_prio_array *array = &rt_rq->active;\n\tstruct rt_rq *group_rq = group_rt_rq(rt_se);\n\tstruct list_head *queue = array->queue + rt_se_prio(rt_se);\n\n\t \n\tif (group_rq && (rt_rq_throttled(group_rq) || !group_rq->rt_nr_running)) {\n\t\tif (rt_se->on_list)\n\t\t\t__delist_rt_entity(rt_se, array);\n\t\treturn;\n\t}\n\n\tif (move_entity(flags)) {\n\t\tWARN_ON_ONCE(rt_se->on_list);\n\t\tif (flags & ENQUEUE_HEAD)\n\t\t\tlist_add(&rt_se->run_list, queue);\n\t\telse\n\t\t\tlist_add_tail(&rt_se->run_list, queue);\n\n\t\t__set_bit(rt_se_prio(rt_se), array->bitmap);\n\t\trt_se->on_list = 1;\n\t}\n\trt_se->on_rq = 1;\n\n\tinc_rt_tasks(rt_se, rt_rq);\n}\n\nstatic void __dequeue_rt_entity(struct sched_rt_entity *rt_se, unsigned int flags)\n{\n\tstruct rt_rq *rt_rq = rt_rq_of_se(rt_se);\n\tstruct rt_prio_array *array = &rt_rq->active;\n\n\tif (move_entity(flags)) {\n\t\tWARN_ON_ONCE(!rt_se->on_list);\n\t\t__delist_rt_entity(rt_se, array);\n\t}\n\trt_se->on_rq = 0;\n\n\tdec_rt_tasks(rt_se, rt_rq);\n}\n\n \nstatic void dequeue_rt_stack(struct sched_rt_entity *rt_se, unsigned int flags)\n{\n\tstruct sched_rt_entity *back = NULL;\n\tunsigned int rt_nr_running;\n\n\tfor_each_sched_rt_entity(rt_se) {\n\t\trt_se->back = back;\n\t\tback = rt_se;\n\t}\n\n\trt_nr_running = rt_rq_of_se(back)->rt_nr_running;\n\n\tfor (rt_se = back; rt_se; rt_se = rt_se->back) {\n\t\tif (on_rt_rq(rt_se))\n\t\t\t__dequeue_rt_entity(rt_se, flags);\n\t}\n\n\tdequeue_top_rt_rq(rt_rq_of_se(back), rt_nr_running);\n}\n\nstatic void enqueue_rt_entity(struct sched_rt_entity *rt_se, unsigned int flags)\n{\n\tstruct rq *rq = rq_of_rt_se(rt_se);\n\n\tupdate_stats_enqueue_rt(rt_rq_of_se(rt_se), rt_se, flags);\n\n\tdequeue_rt_stack(rt_se, flags);\n\tfor_each_sched_rt_entity(rt_se)\n\t\t__enqueue_rt_entity(rt_se, flags);\n\tenqueue_top_rt_rq(&rq->rt);\n}\n\nstatic void dequeue_rt_entity(struct sched_rt_entity *rt_se, unsigned int flags)\n{\n\tstruct rq *rq = rq_of_rt_se(rt_se);\n\n\tupdate_stats_dequeue_rt(rt_rq_of_se(rt_se), rt_se, flags);\n\n\tdequeue_rt_stack(rt_se, flags);\n\n\tfor_each_sched_rt_entity(rt_se) {\n\t\tstruct rt_rq *rt_rq = group_rt_rq(rt_se);\n\n\t\tif (rt_rq && rt_rq->rt_nr_running)\n\t\t\t__enqueue_rt_entity(rt_se, flags);\n\t}\n\tenqueue_top_rt_rq(&rq->rt);\n}\n\n \nstatic void\nenqueue_task_rt(struct rq *rq, struct task_struct *p, int flags)\n{\n\tstruct sched_rt_entity *rt_se = &p->rt;\n\n\tif (flags & ENQUEUE_WAKEUP)\n\t\trt_se->timeout = 0;\n\n\tcheck_schedstat_required();\n\tupdate_stats_wait_start_rt(rt_rq_of_se(rt_se), rt_se);\n\n\tenqueue_rt_entity(rt_se, flags);\n\n\tif (!task_current(rq, p) && p->nr_cpus_allowed > 1)\n\t\tenqueue_pushable_task(rq, p);\n}\n\nstatic void dequeue_task_rt(struct rq *rq, struct task_struct *p, int flags)\n{\n\tstruct sched_rt_entity *rt_se = &p->rt;\n\n\tupdate_curr_rt(rq);\n\tdequeue_rt_entity(rt_se, flags);\n\n\tdequeue_pushable_task(rq, p);\n}\n\n \nstatic void\nrequeue_rt_entity(struct rt_rq *rt_rq, struct sched_rt_entity *rt_se, int head)\n{\n\tif (on_rt_rq(rt_se)) {\n\t\tstruct rt_prio_array *array = &rt_rq->active;\n\t\tstruct list_head *queue = array->queue + rt_se_prio(rt_se);\n\n\t\tif (head)\n\t\t\tlist_move(&rt_se->run_list, queue);\n\t\telse\n\t\t\tlist_move_tail(&rt_se->run_list, queue);\n\t}\n}\n\nstatic void requeue_task_rt(struct rq *rq, struct task_struct *p, int head)\n{\n\tstruct sched_rt_entity *rt_se = &p->rt;\n\tstruct rt_rq *rt_rq;\n\n\tfor_each_sched_rt_entity(rt_se) {\n\t\trt_rq = rt_rq_of_se(rt_se);\n\t\trequeue_rt_entity(rt_rq, rt_se, head);\n\t}\n}\n\nstatic void yield_task_rt(struct rq *rq)\n{\n\trequeue_task_rt(rq, rq->curr, 0);\n}\n\n#ifdef CONFIG_SMP\nstatic int find_lowest_rq(struct task_struct *task);\n\nstatic int\nselect_task_rq_rt(struct task_struct *p, int cpu, int flags)\n{\n\tstruct task_struct *curr;\n\tstruct rq *rq;\n\tbool test;\n\n\t \n\tif (!(flags & (WF_TTWU | WF_FORK)))\n\t\tgoto out;\n\n\trq = cpu_rq(cpu);\n\n\trcu_read_lock();\n\tcurr = READ_ONCE(rq->curr);  \n\n\t \n\ttest = curr &&\n\t       unlikely(rt_task(curr)) &&\n\t       (curr->nr_cpus_allowed < 2 || curr->prio <= p->prio);\n\n\tif (test || !rt_task_fits_capacity(p, cpu)) {\n\t\tint target = find_lowest_rq(p);\n\n\t\t \n\t\tif (!test && target != -1 && !rt_task_fits_capacity(p, target))\n\t\t\tgoto out_unlock;\n\n\t\t \n\t\tif (target != -1 &&\n\t\t    p->prio < cpu_rq(target)->rt.highest_prio.curr)\n\t\t\tcpu = target;\n\t}\n\nout_unlock:\n\trcu_read_unlock();\n\nout:\n\treturn cpu;\n}\n\nstatic void check_preempt_equal_prio(struct rq *rq, struct task_struct *p)\n{\n\t \n\tif (rq->curr->nr_cpus_allowed == 1 ||\n\t    !cpupri_find(&rq->rd->cpupri, rq->curr, NULL))\n\t\treturn;\n\n\t \n\tif (p->nr_cpus_allowed != 1 &&\n\t    cpupri_find(&rq->rd->cpupri, p, NULL))\n\t\treturn;\n\n\t \n\trequeue_task_rt(rq, p, 1);\n\tresched_curr(rq);\n}\n\nstatic int balance_rt(struct rq *rq, struct task_struct *p, struct rq_flags *rf)\n{\n\tif (!on_rt_rq(&p->rt) && need_pull_rt_task(rq, p)) {\n\t\t \n\t\trq_unpin_lock(rq, rf);\n\t\tpull_rt_task(rq);\n\t\trq_repin_lock(rq, rf);\n\t}\n\n\treturn sched_stop_runnable(rq) || sched_dl_runnable(rq) || sched_rt_runnable(rq);\n}\n#endif  \n\n \nstatic void check_preempt_curr_rt(struct rq *rq, struct task_struct *p, int flags)\n{\n\tif (p->prio < rq->curr->prio) {\n\t\tresched_curr(rq);\n\t\treturn;\n\t}\n\n#ifdef CONFIG_SMP\n\t \n\tif (p->prio == rq->curr->prio && !test_tsk_need_resched(rq->curr))\n\t\tcheck_preempt_equal_prio(rq, p);\n#endif\n}\n\nstatic inline void set_next_task_rt(struct rq *rq, struct task_struct *p, bool first)\n{\n\tstruct sched_rt_entity *rt_se = &p->rt;\n\tstruct rt_rq *rt_rq = &rq->rt;\n\n\tp->se.exec_start = rq_clock_task(rq);\n\tif (on_rt_rq(&p->rt))\n\t\tupdate_stats_wait_end_rt(rt_rq, rt_se);\n\n\t \n\tdequeue_pushable_task(rq, p);\n\n\tif (!first)\n\t\treturn;\n\n\t \n\tif (rq->curr->sched_class != &rt_sched_class)\n\t\tupdate_rt_rq_load_avg(rq_clock_pelt(rq), rq, 0);\n\n\trt_queue_push_tasks(rq);\n}\n\nstatic struct sched_rt_entity *pick_next_rt_entity(struct rt_rq *rt_rq)\n{\n\tstruct rt_prio_array *array = &rt_rq->active;\n\tstruct sched_rt_entity *next = NULL;\n\tstruct list_head *queue;\n\tint idx;\n\n\tidx = sched_find_first_bit(array->bitmap);\n\tBUG_ON(idx >= MAX_RT_PRIO);\n\n\tqueue = array->queue + idx;\n\tif (SCHED_WARN_ON(list_empty(queue)))\n\t\treturn NULL;\n\tnext = list_entry(queue->next, struct sched_rt_entity, run_list);\n\n\treturn next;\n}\n\nstatic struct task_struct *_pick_next_task_rt(struct rq *rq)\n{\n\tstruct sched_rt_entity *rt_se;\n\tstruct rt_rq *rt_rq  = &rq->rt;\n\n\tdo {\n\t\trt_se = pick_next_rt_entity(rt_rq);\n\t\tif (unlikely(!rt_se))\n\t\t\treturn NULL;\n\t\trt_rq = group_rt_rq(rt_se);\n\t} while (rt_rq);\n\n\treturn rt_task_of(rt_se);\n}\n\nstatic struct task_struct *pick_task_rt(struct rq *rq)\n{\n\tstruct task_struct *p;\n\n\tif (!sched_rt_runnable(rq))\n\t\treturn NULL;\n\n\tp = _pick_next_task_rt(rq);\n\n\treturn p;\n}\n\nstatic struct task_struct *pick_next_task_rt(struct rq *rq)\n{\n\tstruct task_struct *p = pick_task_rt(rq);\n\n\tif (p)\n\t\tset_next_task_rt(rq, p, true);\n\n\treturn p;\n}\n\nstatic void put_prev_task_rt(struct rq *rq, struct task_struct *p)\n{\n\tstruct sched_rt_entity *rt_se = &p->rt;\n\tstruct rt_rq *rt_rq = &rq->rt;\n\n\tif (on_rt_rq(&p->rt))\n\t\tupdate_stats_wait_start_rt(rt_rq, rt_se);\n\n\tupdate_curr_rt(rq);\n\n\tupdate_rt_rq_load_avg(rq_clock_pelt(rq), rq, 1);\n\n\t \n\tif (on_rt_rq(&p->rt) && p->nr_cpus_allowed > 1)\n\t\tenqueue_pushable_task(rq, p);\n}\n\n#ifdef CONFIG_SMP\n\n \n#define RT_MAX_TRIES 3\n\nstatic int pick_rt_task(struct rq *rq, struct task_struct *p, int cpu)\n{\n\tif (!task_on_cpu(rq, p) &&\n\t    cpumask_test_cpu(cpu, &p->cpus_mask))\n\t\treturn 1;\n\n\treturn 0;\n}\n\n \nstatic struct task_struct *pick_highest_pushable_task(struct rq *rq, int cpu)\n{\n\tstruct plist_head *head = &rq->rt.pushable_tasks;\n\tstruct task_struct *p;\n\n\tif (!has_pushable_tasks(rq))\n\t\treturn NULL;\n\n\tplist_for_each_entry(p, head, pushable_tasks) {\n\t\tif (pick_rt_task(rq, p, cpu))\n\t\t\treturn p;\n\t}\n\n\treturn NULL;\n}\n\nstatic DEFINE_PER_CPU(cpumask_var_t, local_cpu_mask);\n\nstatic int find_lowest_rq(struct task_struct *task)\n{\n\tstruct sched_domain *sd;\n\tstruct cpumask *lowest_mask = this_cpu_cpumask_var_ptr(local_cpu_mask);\n\tint this_cpu = smp_processor_id();\n\tint cpu      = task_cpu(task);\n\tint ret;\n\n\t \n\tif (unlikely(!lowest_mask))\n\t\treturn -1;\n\n\tif (task->nr_cpus_allowed == 1)\n\t\treturn -1;  \n\n\t \n\tif (sched_asym_cpucap_active()) {\n\n\t\tret = cpupri_find_fitness(&task_rq(task)->rd->cpupri,\n\t\t\t\t\t  task, lowest_mask,\n\t\t\t\t\t  rt_task_fits_capacity);\n\t} else {\n\n\t\tret = cpupri_find(&task_rq(task)->rd->cpupri,\n\t\t\t\t  task, lowest_mask);\n\t}\n\n\tif (!ret)\n\t\treturn -1;  \n\n\t \n\tif (cpumask_test_cpu(cpu, lowest_mask))\n\t\treturn cpu;\n\n\t \n\tif (!cpumask_test_cpu(this_cpu, lowest_mask))\n\t\tthis_cpu = -1;  \n\n\trcu_read_lock();\n\tfor_each_domain(cpu, sd) {\n\t\tif (sd->flags & SD_WAKE_AFFINE) {\n\t\t\tint best_cpu;\n\n\t\t\t \n\t\t\tif (this_cpu != -1 &&\n\t\t\t    cpumask_test_cpu(this_cpu, sched_domain_span(sd))) {\n\t\t\t\trcu_read_unlock();\n\t\t\t\treturn this_cpu;\n\t\t\t}\n\n\t\t\tbest_cpu = cpumask_any_and_distribute(lowest_mask,\n\t\t\t\t\t\t\t      sched_domain_span(sd));\n\t\t\tif (best_cpu < nr_cpu_ids) {\n\t\t\t\trcu_read_unlock();\n\t\t\t\treturn best_cpu;\n\t\t\t}\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\t \n\tif (this_cpu != -1)\n\t\treturn this_cpu;\n\n\tcpu = cpumask_any_distribute(lowest_mask);\n\tif (cpu < nr_cpu_ids)\n\t\treturn cpu;\n\n\treturn -1;\n}\n\n \nstatic struct rq *find_lock_lowest_rq(struct task_struct *task, struct rq *rq)\n{\n\tstruct rq *lowest_rq = NULL;\n\tint tries;\n\tint cpu;\n\n\tfor (tries = 0; tries < RT_MAX_TRIES; tries++) {\n\t\tcpu = find_lowest_rq(task);\n\n\t\tif ((cpu == -1) || (cpu == rq->cpu))\n\t\t\tbreak;\n\n\t\tlowest_rq = cpu_rq(cpu);\n\n\t\tif (lowest_rq->rt.highest_prio.curr <= task->prio) {\n\t\t\t \n\t\t\tlowest_rq = NULL;\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (double_lock_balance(rq, lowest_rq)) {\n\t\t\t \n\t\t\tif (unlikely(task_rq(task) != rq ||\n\t\t\t\t     !cpumask_test_cpu(lowest_rq->cpu, &task->cpus_mask) ||\n\t\t\t\t     task_on_cpu(rq, task) ||\n\t\t\t\t     !rt_task(task) ||\n\t\t\t\t     is_migration_disabled(task) ||\n\t\t\t\t     !task_on_rq_queued(task))) {\n\n\t\t\t\tdouble_unlock_balance(rq, lowest_rq);\n\t\t\t\tlowest_rq = NULL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (lowest_rq->rt.highest_prio.curr > task->prio)\n\t\t\tbreak;\n\n\t\t \n\t\tdouble_unlock_balance(rq, lowest_rq);\n\t\tlowest_rq = NULL;\n\t}\n\n\treturn lowest_rq;\n}\n\nstatic struct task_struct *pick_next_pushable_task(struct rq *rq)\n{\n\tstruct task_struct *p;\n\n\tif (!has_pushable_tasks(rq))\n\t\treturn NULL;\n\n\tp = plist_first_entry(&rq->rt.pushable_tasks,\n\t\t\t      struct task_struct, pushable_tasks);\n\n\tBUG_ON(rq->cpu != task_cpu(p));\n\tBUG_ON(task_current(rq, p));\n\tBUG_ON(p->nr_cpus_allowed <= 1);\n\n\tBUG_ON(!task_on_rq_queued(p));\n\tBUG_ON(!rt_task(p));\n\n\treturn p;\n}\n\n \nstatic int push_rt_task(struct rq *rq, bool pull)\n{\n\tstruct task_struct *next_task;\n\tstruct rq *lowest_rq;\n\tint ret = 0;\n\n\tif (!rq->rt.overloaded)\n\t\treturn 0;\n\n\tnext_task = pick_next_pushable_task(rq);\n\tif (!next_task)\n\t\treturn 0;\n\nretry:\n\t \n\tif (unlikely(next_task->prio < rq->curr->prio)) {\n\t\tresched_curr(rq);\n\t\treturn 0;\n\t}\n\n\tif (is_migration_disabled(next_task)) {\n\t\tstruct task_struct *push_task = NULL;\n\t\tint cpu;\n\n\t\tif (!pull || rq->push_busy)\n\t\t\treturn 0;\n\n\t\t \n\t\tif (rq->curr->sched_class != &rt_sched_class)\n\t\t\treturn 0;\n\n\t\tcpu = find_lowest_rq(rq->curr);\n\t\tif (cpu == -1 || cpu == rq->cpu)\n\t\t\treturn 0;\n\n\t\t \n\t\tpush_task = get_push_task(rq);\n\t\tif (push_task) {\n\t\t\tpreempt_disable();\n\t\t\traw_spin_rq_unlock(rq);\n\t\t\tstop_one_cpu_nowait(rq->cpu, push_cpu_stop,\n\t\t\t\t\t    push_task, &rq->push_work);\n\t\t\tpreempt_enable();\n\t\t\traw_spin_rq_lock(rq);\n\t\t}\n\n\t\treturn 0;\n\t}\n\n\tif (WARN_ON(next_task == rq->curr))\n\t\treturn 0;\n\n\t \n\tget_task_struct(next_task);\n\n\t \n\tlowest_rq = find_lock_lowest_rq(next_task, rq);\n\tif (!lowest_rq) {\n\t\tstruct task_struct *task;\n\t\t \n\t\ttask = pick_next_pushable_task(rq);\n\t\tif (task == next_task) {\n\t\t\t \n\t\t\tgoto out;\n\t\t}\n\n\t\tif (!task)\n\t\t\t \n\t\t\tgoto out;\n\n\t\t \n\t\tput_task_struct(next_task);\n\t\tnext_task = task;\n\t\tgoto retry;\n\t}\n\n\tdeactivate_task(rq, next_task, 0);\n\tset_task_cpu(next_task, lowest_rq->cpu);\n\tactivate_task(lowest_rq, next_task, 0);\n\tresched_curr(lowest_rq);\n\tret = 1;\n\n\tdouble_unlock_balance(rq, lowest_rq);\nout:\n\tput_task_struct(next_task);\n\n\treturn ret;\n}\n\nstatic void push_rt_tasks(struct rq *rq)\n{\n\t \n\twhile (push_rt_task(rq, false))\n\t\t;\n}\n\n#ifdef HAVE_RT_PUSH_IPI\n\n \nstatic int rto_next_cpu(struct root_domain *rd)\n{\n\tint next;\n\tint cpu;\n\n\t \n\tfor (;;) {\n\n\t\t \n\t\tcpu = cpumask_next(rd->rto_cpu, rd->rto_mask);\n\n\t\trd->rto_cpu = cpu;\n\n\t\tif (cpu < nr_cpu_ids)\n\t\t\treturn cpu;\n\n\t\trd->rto_cpu = -1;\n\n\t\t \n\t\tnext = atomic_read_acquire(&rd->rto_loop_next);\n\n\t\tif (rd->rto_loop == next)\n\t\t\tbreak;\n\n\t\trd->rto_loop = next;\n\t}\n\n\treturn -1;\n}\n\nstatic inline bool rto_start_trylock(atomic_t *v)\n{\n\treturn !atomic_cmpxchg_acquire(v, 0, 1);\n}\n\nstatic inline void rto_start_unlock(atomic_t *v)\n{\n\tatomic_set_release(v, 0);\n}\n\nstatic void tell_cpu_to_push(struct rq *rq)\n{\n\tint cpu = -1;\n\n\t \n\tatomic_inc(&rq->rd->rto_loop_next);\n\n\t \n\tif (!rto_start_trylock(&rq->rd->rto_loop_start))\n\t\treturn;\n\n\traw_spin_lock(&rq->rd->rto_lock);\n\n\t \n\tif (rq->rd->rto_cpu < 0)\n\t\tcpu = rto_next_cpu(rq->rd);\n\n\traw_spin_unlock(&rq->rd->rto_lock);\n\n\trto_start_unlock(&rq->rd->rto_loop_start);\n\n\tif (cpu >= 0) {\n\t\t \n\t\tsched_get_rd(rq->rd);\n\t\tirq_work_queue_on(&rq->rd->rto_push_work, cpu);\n\t}\n}\n\n \nvoid rto_push_irq_work_func(struct irq_work *work)\n{\n\tstruct root_domain *rd =\n\t\tcontainer_of(work, struct root_domain, rto_push_work);\n\tstruct rq *rq;\n\tint cpu;\n\n\trq = this_rq();\n\n\t \n\tif (has_pushable_tasks(rq)) {\n\t\traw_spin_rq_lock(rq);\n\t\twhile (push_rt_task(rq, true))\n\t\t\t;\n\t\traw_spin_rq_unlock(rq);\n\t}\n\n\traw_spin_lock(&rd->rto_lock);\n\n\t \n\tcpu = rto_next_cpu(rd);\n\n\traw_spin_unlock(&rd->rto_lock);\n\n\tif (cpu < 0) {\n\t\tsched_put_rd(rd);\n\t\treturn;\n\t}\n\n\t \n\tirq_work_queue_on(&rd->rto_push_work, cpu);\n}\n#endif  \n\nstatic void pull_rt_task(struct rq *this_rq)\n{\n\tint this_cpu = this_rq->cpu, cpu;\n\tbool resched = false;\n\tstruct task_struct *p, *push_task;\n\tstruct rq *src_rq;\n\tint rt_overload_count = rt_overloaded(this_rq);\n\n\tif (likely(!rt_overload_count))\n\t\treturn;\n\n\t \n\tsmp_rmb();\n\n\t \n\tif (rt_overload_count == 1 &&\n\t    cpumask_test_cpu(this_rq->cpu, this_rq->rd->rto_mask))\n\t\treturn;\n\n#ifdef HAVE_RT_PUSH_IPI\n\tif (sched_feat(RT_PUSH_IPI)) {\n\t\ttell_cpu_to_push(this_rq);\n\t\treturn;\n\t}\n#endif\n\n\tfor_each_cpu(cpu, this_rq->rd->rto_mask) {\n\t\tif (this_cpu == cpu)\n\t\t\tcontinue;\n\n\t\tsrc_rq = cpu_rq(cpu);\n\n\t\t \n\t\tif (src_rq->rt.highest_prio.next >=\n\t\t    this_rq->rt.highest_prio.curr)\n\t\t\tcontinue;\n\n\t\t \n\t\tpush_task = NULL;\n\t\tdouble_lock_balance(this_rq, src_rq);\n\n\t\t \n\t\tp = pick_highest_pushable_task(src_rq, this_cpu);\n\n\t\t \n\t\tif (p && (p->prio < this_rq->rt.highest_prio.curr)) {\n\t\t\tWARN_ON(p == src_rq->curr);\n\t\t\tWARN_ON(!task_on_rq_queued(p));\n\n\t\t\t \n\t\t\tif (p->prio < src_rq->curr->prio)\n\t\t\t\tgoto skip;\n\n\t\t\tif (is_migration_disabled(p)) {\n\t\t\t\tpush_task = get_push_task(src_rq);\n\t\t\t} else {\n\t\t\t\tdeactivate_task(src_rq, p, 0);\n\t\t\t\tset_task_cpu(p, this_cpu);\n\t\t\t\tactivate_task(this_rq, p, 0);\n\t\t\t\tresched = true;\n\t\t\t}\n\t\t\t \n\t\t}\nskip:\n\t\tdouble_unlock_balance(this_rq, src_rq);\n\n\t\tif (push_task) {\n\t\t\tpreempt_disable();\n\t\t\traw_spin_rq_unlock(this_rq);\n\t\t\tstop_one_cpu_nowait(src_rq->cpu, push_cpu_stop,\n\t\t\t\t\t    push_task, &src_rq->push_work);\n\t\t\tpreempt_enable();\n\t\t\traw_spin_rq_lock(this_rq);\n\t\t}\n\t}\n\n\tif (resched)\n\t\tresched_curr(this_rq);\n}\n\n \nstatic void task_woken_rt(struct rq *rq, struct task_struct *p)\n{\n\tbool need_to_push = !task_on_cpu(rq, p) &&\n\t\t\t    !test_tsk_need_resched(rq->curr) &&\n\t\t\t    p->nr_cpus_allowed > 1 &&\n\t\t\t    (dl_task(rq->curr) || rt_task(rq->curr)) &&\n\t\t\t    (rq->curr->nr_cpus_allowed < 2 ||\n\t\t\t     rq->curr->prio <= p->prio);\n\n\tif (need_to_push)\n\t\tpush_rt_tasks(rq);\n}\n\n \nstatic void rq_online_rt(struct rq *rq)\n{\n\tif (rq->rt.overloaded)\n\t\trt_set_overload(rq);\n\n\t__enable_runtime(rq);\n\n\tcpupri_set(&rq->rd->cpupri, rq->cpu, rq->rt.highest_prio.curr);\n}\n\n \nstatic void rq_offline_rt(struct rq *rq)\n{\n\tif (rq->rt.overloaded)\n\t\trt_clear_overload(rq);\n\n\t__disable_runtime(rq);\n\n\tcpupri_set(&rq->rd->cpupri, rq->cpu, CPUPRI_INVALID);\n}\n\n \nstatic void switched_from_rt(struct rq *rq, struct task_struct *p)\n{\n\t \n\tif (!task_on_rq_queued(p) || rq->rt.rt_nr_running)\n\t\treturn;\n\n\trt_queue_pull_task(rq);\n}\n\nvoid __init init_sched_rt_class(void)\n{\n\tunsigned int i;\n\n\tfor_each_possible_cpu(i) {\n\t\tzalloc_cpumask_var_node(&per_cpu(local_cpu_mask, i),\n\t\t\t\t\tGFP_KERNEL, cpu_to_node(i));\n\t}\n}\n#endif  \n\n \nstatic void switched_to_rt(struct rq *rq, struct task_struct *p)\n{\n\t \n\tif (task_current(rq, p)) {\n\t\tupdate_rt_rq_load_avg(rq_clock_pelt(rq), rq, 0);\n\t\treturn;\n\t}\n\n\t \n\tif (task_on_rq_queued(p)) {\n#ifdef CONFIG_SMP\n\t\tif (p->nr_cpus_allowed > 1 && rq->rt.overloaded)\n\t\t\trt_queue_push_tasks(rq);\n#endif  \n\t\tif (p->prio < rq->curr->prio && cpu_online(cpu_of(rq)))\n\t\t\tresched_curr(rq);\n\t}\n}\n\n \nstatic void\nprio_changed_rt(struct rq *rq, struct task_struct *p, int oldprio)\n{\n\tif (!task_on_rq_queued(p))\n\t\treturn;\n\n\tif (task_current(rq, p)) {\n#ifdef CONFIG_SMP\n\t\t \n\t\tif (oldprio < p->prio)\n\t\t\trt_queue_pull_task(rq);\n\n\t\t \n\t\tif (p->prio > rq->rt.highest_prio.curr)\n\t\t\tresched_curr(rq);\n#else\n\t\t \n\t\tif (oldprio < p->prio)\n\t\t\tresched_curr(rq);\n#endif  \n\t} else {\n\t\t \n\t\tif (p->prio < rq->curr->prio)\n\t\t\tresched_curr(rq);\n\t}\n}\n\n#ifdef CONFIG_POSIX_TIMERS\nstatic void watchdog(struct rq *rq, struct task_struct *p)\n{\n\tunsigned long soft, hard;\n\n\t \n\tsoft = task_rlimit(p, RLIMIT_RTTIME);\n\thard = task_rlimit_max(p, RLIMIT_RTTIME);\n\n\tif (soft != RLIM_INFINITY) {\n\t\tunsigned long next;\n\n\t\tif (p->rt.watchdog_stamp != jiffies) {\n\t\t\tp->rt.timeout++;\n\t\t\tp->rt.watchdog_stamp = jiffies;\n\t\t}\n\n\t\tnext = DIV_ROUND_UP(min(soft, hard), USEC_PER_SEC/HZ);\n\t\tif (p->rt.timeout > next) {\n\t\t\tposix_cputimers_rt_watchdog(&p->posix_cputimers,\n\t\t\t\t\t\t    p->se.sum_exec_runtime);\n\t\t}\n\t}\n}\n#else\nstatic inline void watchdog(struct rq *rq, struct task_struct *p) { }\n#endif\n\n \nstatic void task_tick_rt(struct rq *rq, struct task_struct *p, int queued)\n{\n\tstruct sched_rt_entity *rt_se = &p->rt;\n\n\tupdate_curr_rt(rq);\n\tupdate_rt_rq_load_avg(rq_clock_pelt(rq), rq, 1);\n\n\twatchdog(rq, p);\n\n\t \n\tif (p->policy != SCHED_RR)\n\t\treturn;\n\n\tif (--p->rt.time_slice)\n\t\treturn;\n\n\tp->rt.time_slice = sched_rr_timeslice;\n\n\t \n\tfor_each_sched_rt_entity(rt_se) {\n\t\tif (rt_se->run_list.prev != rt_se->run_list.next) {\n\t\t\trequeue_task_rt(rq, p, 0);\n\t\t\tresched_curr(rq);\n\t\t\treturn;\n\t\t}\n\t}\n}\n\nstatic unsigned int get_rr_interval_rt(struct rq *rq, struct task_struct *task)\n{\n\t \n\tif (task->policy == SCHED_RR)\n\t\treturn sched_rr_timeslice;\n\telse\n\t\treturn 0;\n}\n\n#ifdef CONFIG_SCHED_CORE\nstatic int task_is_throttled_rt(struct task_struct *p, int cpu)\n{\n\tstruct rt_rq *rt_rq;\n\n#ifdef CONFIG_RT_GROUP_SCHED\n\trt_rq = task_group(p)->rt_rq[cpu];\n#else\n\trt_rq = &cpu_rq(cpu)->rt;\n#endif\n\n\treturn rt_rq_throttled(rt_rq);\n}\n#endif\n\nDEFINE_SCHED_CLASS(rt) = {\n\n\t.enqueue_task\t\t= enqueue_task_rt,\n\t.dequeue_task\t\t= dequeue_task_rt,\n\t.yield_task\t\t= yield_task_rt,\n\n\t.check_preempt_curr\t= check_preempt_curr_rt,\n\n\t.pick_next_task\t\t= pick_next_task_rt,\n\t.put_prev_task\t\t= put_prev_task_rt,\n\t.set_next_task          = set_next_task_rt,\n\n#ifdef CONFIG_SMP\n\t.balance\t\t= balance_rt,\n\t.pick_task\t\t= pick_task_rt,\n\t.select_task_rq\t\t= select_task_rq_rt,\n\t.set_cpus_allowed       = set_cpus_allowed_common,\n\t.rq_online              = rq_online_rt,\n\t.rq_offline             = rq_offline_rt,\n\t.task_woken\t\t= task_woken_rt,\n\t.switched_from\t\t= switched_from_rt,\n\t.find_lock_rq\t\t= find_lock_lowest_rq,\n#endif\n\n\t.task_tick\t\t= task_tick_rt,\n\n\t.get_rr_interval\t= get_rr_interval_rt,\n\n\t.prio_changed\t\t= prio_changed_rt,\n\t.switched_to\t\t= switched_to_rt,\n\n\t.update_curr\t\t= update_curr_rt,\n\n#ifdef CONFIG_SCHED_CORE\n\t.task_is_throttled\t= task_is_throttled_rt,\n#endif\n\n#ifdef CONFIG_UCLAMP_TASK\n\t.uclamp_enabled\t\t= 1,\n#endif\n};\n\n#ifdef CONFIG_RT_GROUP_SCHED\n \nstatic DEFINE_MUTEX(rt_constraints_mutex);\n\nstatic inline int tg_has_rt_tasks(struct task_group *tg)\n{\n\tstruct task_struct *task;\n\tstruct css_task_iter it;\n\tint ret = 0;\n\n\t \n\tif (task_group_is_autogroup(tg))\n\t\treturn 0;\n\n\tcss_task_iter_start(&tg->css, 0, &it);\n\twhile (!ret && (task = css_task_iter_next(&it)))\n\t\tret |= rt_task(task);\n\tcss_task_iter_end(&it);\n\n\treturn ret;\n}\n\nstruct rt_schedulable_data {\n\tstruct task_group *tg;\n\tu64 rt_period;\n\tu64 rt_runtime;\n};\n\nstatic int tg_rt_schedulable(struct task_group *tg, void *data)\n{\n\tstruct rt_schedulable_data *d = data;\n\tstruct task_group *child;\n\tunsigned long total, sum = 0;\n\tu64 period, runtime;\n\n\tperiod = ktime_to_ns(tg->rt_bandwidth.rt_period);\n\truntime = tg->rt_bandwidth.rt_runtime;\n\n\tif (tg == d->tg) {\n\t\tperiod = d->rt_period;\n\t\truntime = d->rt_runtime;\n\t}\n\n\t \n\tif (runtime > period && runtime != RUNTIME_INF)\n\t\treturn -EINVAL;\n\n\t \n\tif (rt_bandwidth_enabled() && !runtime &&\n\t    tg->rt_bandwidth.rt_runtime && tg_has_rt_tasks(tg))\n\t\treturn -EBUSY;\n\n\ttotal = to_ratio(period, runtime);\n\n\t \n\tif (total > to_ratio(global_rt_period(), global_rt_runtime()))\n\t\treturn -EINVAL;\n\n\t \n\tlist_for_each_entry_rcu(child, &tg->children, siblings) {\n\t\tperiod = ktime_to_ns(child->rt_bandwidth.rt_period);\n\t\truntime = child->rt_bandwidth.rt_runtime;\n\n\t\tif (child == d->tg) {\n\t\t\tperiod = d->rt_period;\n\t\t\truntime = d->rt_runtime;\n\t\t}\n\n\t\tsum += to_ratio(period, runtime);\n\t}\n\n\tif (sum > total)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int __rt_schedulable(struct task_group *tg, u64 period, u64 runtime)\n{\n\tint ret;\n\n\tstruct rt_schedulable_data data = {\n\t\t.tg = tg,\n\t\t.rt_period = period,\n\t\t.rt_runtime = runtime,\n\t};\n\n\trcu_read_lock();\n\tret = walk_tg_tree(tg_rt_schedulable, tg_nop, &data);\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\nstatic int tg_set_rt_bandwidth(struct task_group *tg,\n\t\tu64 rt_period, u64 rt_runtime)\n{\n\tint i, err = 0;\n\n\t \n\tif (tg == &root_task_group && rt_runtime == 0)\n\t\treturn -EINVAL;\n\n\t \n\tif (rt_period == 0)\n\t\treturn -EINVAL;\n\n\t \n\tif (rt_runtime != RUNTIME_INF && rt_runtime > max_rt_runtime)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&rt_constraints_mutex);\n\terr = __rt_schedulable(tg, rt_period, rt_runtime);\n\tif (err)\n\t\tgoto unlock;\n\n\traw_spin_lock_irq(&tg->rt_bandwidth.rt_runtime_lock);\n\ttg->rt_bandwidth.rt_period = ns_to_ktime(rt_period);\n\ttg->rt_bandwidth.rt_runtime = rt_runtime;\n\n\tfor_each_possible_cpu(i) {\n\t\tstruct rt_rq *rt_rq = tg->rt_rq[i];\n\n\t\traw_spin_lock(&rt_rq->rt_runtime_lock);\n\t\trt_rq->rt_runtime = rt_runtime;\n\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);\n\t}\n\traw_spin_unlock_irq(&tg->rt_bandwidth.rt_runtime_lock);\nunlock:\n\tmutex_unlock(&rt_constraints_mutex);\n\n\treturn err;\n}\n\nint sched_group_set_rt_runtime(struct task_group *tg, long rt_runtime_us)\n{\n\tu64 rt_runtime, rt_period;\n\n\trt_period = ktime_to_ns(tg->rt_bandwidth.rt_period);\n\trt_runtime = (u64)rt_runtime_us * NSEC_PER_USEC;\n\tif (rt_runtime_us < 0)\n\t\trt_runtime = RUNTIME_INF;\n\telse if ((u64)rt_runtime_us > U64_MAX / NSEC_PER_USEC)\n\t\treturn -EINVAL;\n\n\treturn tg_set_rt_bandwidth(tg, rt_period, rt_runtime);\n}\n\nlong sched_group_rt_runtime(struct task_group *tg)\n{\n\tu64 rt_runtime_us;\n\n\tif (tg->rt_bandwidth.rt_runtime == RUNTIME_INF)\n\t\treturn -1;\n\n\trt_runtime_us = tg->rt_bandwidth.rt_runtime;\n\tdo_div(rt_runtime_us, NSEC_PER_USEC);\n\treturn rt_runtime_us;\n}\n\nint sched_group_set_rt_period(struct task_group *tg, u64 rt_period_us)\n{\n\tu64 rt_runtime, rt_period;\n\n\tif (rt_period_us > U64_MAX / NSEC_PER_USEC)\n\t\treturn -EINVAL;\n\n\trt_period = rt_period_us * NSEC_PER_USEC;\n\trt_runtime = tg->rt_bandwidth.rt_runtime;\n\n\treturn tg_set_rt_bandwidth(tg, rt_period, rt_runtime);\n}\n\nlong sched_group_rt_period(struct task_group *tg)\n{\n\tu64 rt_period_us;\n\n\trt_period_us = ktime_to_ns(tg->rt_bandwidth.rt_period);\n\tdo_div(rt_period_us, NSEC_PER_USEC);\n\treturn rt_period_us;\n}\n\n#ifdef CONFIG_SYSCTL\nstatic int sched_rt_global_constraints(void)\n{\n\tint ret = 0;\n\n\tmutex_lock(&rt_constraints_mutex);\n\tret = __rt_schedulable(NULL, 0, 0);\n\tmutex_unlock(&rt_constraints_mutex);\n\n\treturn ret;\n}\n#endif  \n\nint sched_rt_can_attach(struct task_group *tg, struct task_struct *tsk)\n{\n\t \n\tif (rt_task(tsk) && tg->rt_bandwidth.rt_runtime == 0)\n\t\treturn 0;\n\n\treturn 1;\n}\n\n#else  \n\n#ifdef CONFIG_SYSCTL\nstatic int sched_rt_global_constraints(void)\n{\n\tunsigned long flags;\n\tint i;\n\n\traw_spin_lock_irqsave(&def_rt_bandwidth.rt_runtime_lock, flags);\n\tfor_each_possible_cpu(i) {\n\t\tstruct rt_rq *rt_rq = &cpu_rq(i)->rt;\n\n\t\traw_spin_lock(&rt_rq->rt_runtime_lock);\n\t\trt_rq->rt_runtime = global_rt_runtime();\n\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);\n\t}\n\traw_spin_unlock_irqrestore(&def_rt_bandwidth.rt_runtime_lock, flags);\n\n\treturn 0;\n}\n#endif  \n#endif  \n\n#ifdef CONFIG_SYSCTL\nstatic int sched_rt_global_validate(void)\n{\n\tif (sysctl_sched_rt_period <= 0)\n\t\treturn -EINVAL;\n\n\tif ((sysctl_sched_rt_runtime != RUNTIME_INF) &&\n\t\t((sysctl_sched_rt_runtime > sysctl_sched_rt_period) ||\n\t\t ((u64)sysctl_sched_rt_runtime *\n\t\t\tNSEC_PER_USEC > max_rt_runtime)))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic void sched_rt_do_global(void)\n{\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&def_rt_bandwidth.rt_runtime_lock, flags);\n\tdef_rt_bandwidth.rt_runtime = global_rt_runtime();\n\tdef_rt_bandwidth.rt_period = ns_to_ktime(global_rt_period());\n\traw_spin_unlock_irqrestore(&def_rt_bandwidth.rt_runtime_lock, flags);\n}\n\nstatic int sched_rt_handler(struct ctl_table *table, int write, void *buffer,\n\t\tsize_t *lenp, loff_t *ppos)\n{\n\tint old_period, old_runtime;\n\tstatic DEFINE_MUTEX(mutex);\n\tint ret;\n\n\tmutex_lock(&mutex);\n\told_period = sysctl_sched_rt_period;\n\told_runtime = sysctl_sched_rt_runtime;\n\n\tret = proc_dointvec(table, write, buffer, lenp, ppos);\n\n\tif (!ret && write) {\n\t\tret = sched_rt_global_validate();\n\t\tif (ret)\n\t\t\tgoto undo;\n\n\t\tret = sched_dl_global_validate();\n\t\tif (ret)\n\t\t\tgoto undo;\n\n\t\tret = sched_rt_global_constraints();\n\t\tif (ret)\n\t\t\tgoto undo;\n\n\t\tsched_rt_do_global();\n\t\tsched_dl_do_global();\n\t}\n\tif (0) {\nundo:\n\t\tsysctl_sched_rt_period = old_period;\n\t\tsysctl_sched_rt_runtime = old_runtime;\n\t}\n\tmutex_unlock(&mutex);\n\n\treturn ret;\n}\n\nstatic int sched_rr_handler(struct ctl_table *table, int write, void *buffer,\n\t\tsize_t *lenp, loff_t *ppos)\n{\n\tint ret;\n\tstatic DEFINE_MUTEX(mutex);\n\n\tmutex_lock(&mutex);\n\tret = proc_dointvec(table, write, buffer, lenp, ppos);\n\t \n\tif (!ret && write) {\n\t\tsched_rr_timeslice =\n\t\t\tsysctl_sched_rr_timeslice <= 0 ? RR_TIMESLICE :\n\t\t\tmsecs_to_jiffies(sysctl_sched_rr_timeslice);\n\n\t\tif (sysctl_sched_rr_timeslice <= 0)\n\t\t\tsysctl_sched_rr_timeslice = jiffies_to_msecs(RR_TIMESLICE);\n\t}\n\tmutex_unlock(&mutex);\n\n\treturn ret;\n}\n#endif  \n\n#ifdef CONFIG_SCHED_DEBUG\nvoid print_rt_stats(struct seq_file *m, int cpu)\n{\n\trt_rq_iter_t iter;\n\tstruct rt_rq *rt_rq;\n\n\trcu_read_lock();\n\tfor_each_rt_rq(rt_rq, iter, cpu_rq(cpu))\n\t\tprint_rt_rq(m, cpu, rt_rq);\n\trcu_read_unlock();\n}\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}