{
  "module_name": "cputime.c",
  "hash_id": "c7ca3bf746260aee5fc07920487f512150650f4030d56a33566b2c3852010ea1",
  "original_prompt": "Ingested from linux-6.6.14/kernel/sched/cputime.c",
  "human_readable_source": "\n \n\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE\n #include <asm/cputime.h>\n#endif\n\n#ifdef CONFIG_IRQ_TIME_ACCOUNTING\n\n \nDEFINE_PER_CPU(struct irqtime, cpu_irqtime);\n\nstatic int sched_clock_irqtime;\n\nvoid enable_sched_clock_irqtime(void)\n{\n\tsched_clock_irqtime = 1;\n}\n\nvoid disable_sched_clock_irqtime(void)\n{\n\tsched_clock_irqtime = 0;\n}\n\nstatic void irqtime_account_delta(struct irqtime *irqtime, u64 delta,\n\t\t\t\t  enum cpu_usage_stat idx)\n{\n\tu64 *cpustat = kcpustat_this_cpu->cpustat;\n\n\tu64_stats_update_begin(&irqtime->sync);\n\tcpustat[idx] += delta;\n\tirqtime->total += delta;\n\tirqtime->tick_delta += delta;\n\tu64_stats_update_end(&irqtime->sync);\n}\n\n \nvoid irqtime_account_irq(struct task_struct *curr, unsigned int offset)\n{\n\tstruct irqtime *irqtime = this_cpu_ptr(&cpu_irqtime);\n\tunsigned int pc;\n\ts64 delta;\n\tint cpu;\n\n\tif (!sched_clock_irqtime)\n\t\treturn;\n\n\tcpu = smp_processor_id();\n\tdelta = sched_clock_cpu(cpu) - irqtime->irq_start_time;\n\tirqtime->irq_start_time += delta;\n\tpc = irq_count() - offset;\n\n\t \n\tif (pc & HARDIRQ_MASK)\n\t\tirqtime_account_delta(irqtime, delta, CPUTIME_IRQ);\n\telse if ((pc & SOFTIRQ_OFFSET) && curr != this_cpu_ksoftirqd())\n\t\tirqtime_account_delta(irqtime, delta, CPUTIME_SOFTIRQ);\n}\n\nstatic u64 irqtime_tick_accounted(u64 maxtime)\n{\n\tstruct irqtime *irqtime = this_cpu_ptr(&cpu_irqtime);\n\tu64 delta;\n\n\tdelta = min(irqtime->tick_delta, maxtime);\n\tirqtime->tick_delta -= delta;\n\n\treturn delta;\n}\n\n#else  \n\n#define sched_clock_irqtime\t(0)\n\nstatic u64 irqtime_tick_accounted(u64 dummy)\n{\n\treturn 0;\n}\n\n#endif  \n\nstatic inline void task_group_account_field(struct task_struct *p, int index,\n\t\t\t\t\t    u64 tmp)\n{\n\t \n\t__this_cpu_add(kernel_cpustat.cpustat[index], tmp);\n\n\tcgroup_account_cputime_field(p, index, tmp);\n}\n\n \nvoid account_user_time(struct task_struct *p, u64 cputime)\n{\n\tint index;\n\n\t \n\tp->utime += cputime;\n\taccount_group_user_time(p, cputime);\n\n\tindex = (task_nice(p) > 0) ? CPUTIME_NICE : CPUTIME_USER;\n\n\t \n\ttask_group_account_field(p, index, cputime);\n\n\t \n\tacct_account_cputime(p);\n}\n\n \nvoid account_guest_time(struct task_struct *p, u64 cputime)\n{\n\tu64 *cpustat = kcpustat_this_cpu->cpustat;\n\n\t \n\tp->utime += cputime;\n\taccount_group_user_time(p, cputime);\n\tp->gtime += cputime;\n\n\t \n\tif (task_nice(p) > 0) {\n\t\ttask_group_account_field(p, CPUTIME_NICE, cputime);\n\t\tcpustat[CPUTIME_GUEST_NICE] += cputime;\n\t} else {\n\t\ttask_group_account_field(p, CPUTIME_USER, cputime);\n\t\tcpustat[CPUTIME_GUEST] += cputime;\n\t}\n}\n\n \nvoid account_system_index_time(struct task_struct *p,\n\t\t\t       u64 cputime, enum cpu_usage_stat index)\n{\n\t \n\tp->stime += cputime;\n\taccount_group_system_time(p, cputime);\n\n\t \n\ttask_group_account_field(p, index, cputime);\n\n\t \n\tacct_account_cputime(p);\n}\n\n \nvoid account_system_time(struct task_struct *p, int hardirq_offset, u64 cputime)\n{\n\tint index;\n\n\tif ((p->flags & PF_VCPU) && (irq_count() - hardirq_offset == 0)) {\n\t\taccount_guest_time(p, cputime);\n\t\treturn;\n\t}\n\n\tif (hardirq_count() - hardirq_offset)\n\t\tindex = CPUTIME_IRQ;\n\telse if (in_serving_softirq())\n\t\tindex = CPUTIME_SOFTIRQ;\n\telse\n\t\tindex = CPUTIME_SYSTEM;\n\n\taccount_system_index_time(p, cputime, index);\n}\n\n \nvoid account_steal_time(u64 cputime)\n{\n\tu64 *cpustat = kcpustat_this_cpu->cpustat;\n\n\tcpustat[CPUTIME_STEAL] += cputime;\n}\n\n \nvoid account_idle_time(u64 cputime)\n{\n\tu64 *cpustat = kcpustat_this_cpu->cpustat;\n\tstruct rq *rq = this_rq();\n\n\tif (atomic_read(&rq->nr_iowait) > 0)\n\t\tcpustat[CPUTIME_IOWAIT] += cputime;\n\telse\n\t\tcpustat[CPUTIME_IDLE] += cputime;\n}\n\n\n#ifdef CONFIG_SCHED_CORE\n \nvoid __account_forceidle_time(struct task_struct *p, u64 delta)\n{\n\t__schedstat_add(p->stats.core_forceidle_sum, delta);\n\n\ttask_group_account_field(p, CPUTIME_FORCEIDLE, delta);\n}\n#endif\n\n \nstatic __always_inline u64 steal_account_process_time(u64 maxtime)\n{\n#ifdef CONFIG_PARAVIRT\n\tif (static_key_false(&paravirt_steal_enabled)) {\n\t\tu64 steal;\n\n\t\tsteal = paravirt_steal_clock(smp_processor_id());\n\t\tsteal -= this_rq()->prev_steal_time;\n\t\tsteal = min(steal, maxtime);\n\t\taccount_steal_time(steal);\n\t\tthis_rq()->prev_steal_time += steal;\n\n\t\treturn steal;\n\t}\n#endif\n\treturn 0;\n}\n\n \nstatic inline u64 account_other_time(u64 max)\n{\n\tu64 accounted;\n\n\tlockdep_assert_irqs_disabled();\n\n\taccounted = steal_account_process_time(max);\n\n\tif (accounted < max)\n\t\taccounted += irqtime_tick_accounted(max - accounted);\n\n\treturn accounted;\n}\n\n#ifdef CONFIG_64BIT\nstatic inline u64 read_sum_exec_runtime(struct task_struct *t)\n{\n\treturn t->se.sum_exec_runtime;\n}\n#else\nstatic u64 read_sum_exec_runtime(struct task_struct *t)\n{\n\tu64 ns;\n\tstruct rq_flags rf;\n\tstruct rq *rq;\n\n\trq = task_rq_lock(t, &rf);\n\tns = t->se.sum_exec_runtime;\n\ttask_rq_unlock(rq, t, &rf);\n\n\treturn ns;\n}\n#endif\n\n \nvoid thread_group_cputime(struct task_struct *tsk, struct task_cputime *times)\n{\n\tstruct signal_struct *sig = tsk->signal;\n\tu64 utime, stime;\n\tstruct task_struct *t;\n\tunsigned int seq, nextseq;\n\tunsigned long flags;\n\n\t \n\tif (same_thread_group(current, tsk))\n\t\t(void) task_sched_runtime(current);\n\n\trcu_read_lock();\n\t \n\tnextseq = 0;\n\tdo {\n\t\tseq = nextseq;\n\t\tflags = read_seqbegin_or_lock_irqsave(&sig->stats_lock, &seq);\n\t\ttimes->utime = sig->utime;\n\t\ttimes->stime = sig->stime;\n\t\ttimes->sum_exec_runtime = sig->sum_sched_runtime;\n\n\t\tfor_each_thread(tsk, t) {\n\t\t\ttask_cputime(t, &utime, &stime);\n\t\t\ttimes->utime += utime;\n\t\t\ttimes->stime += stime;\n\t\t\ttimes->sum_exec_runtime += read_sum_exec_runtime(t);\n\t\t}\n\t\t \n\t\tnextseq = 1;\n\t} while (need_seqretry(&sig->stats_lock, seq));\n\tdone_seqretry_irqrestore(&sig->stats_lock, seq, flags);\n\trcu_read_unlock();\n}\n\n#ifdef CONFIG_IRQ_TIME_ACCOUNTING\n \nstatic void irqtime_account_process_tick(struct task_struct *p, int user_tick,\n\t\t\t\t\t int ticks)\n{\n\tu64 other, cputime = TICK_NSEC * ticks;\n\n\t \n\tother = account_other_time(ULONG_MAX);\n\tif (other >= cputime)\n\t\treturn;\n\n\tcputime -= other;\n\n\tif (this_cpu_ksoftirqd() == p) {\n\t\t \n\t\taccount_system_index_time(p, cputime, CPUTIME_SOFTIRQ);\n\t} else if (user_tick) {\n\t\taccount_user_time(p, cputime);\n\t} else if (p == this_rq()->idle) {\n\t\taccount_idle_time(cputime);\n\t} else if (p->flags & PF_VCPU) {  \n\t\taccount_guest_time(p, cputime);\n\t} else {\n\t\taccount_system_index_time(p, cputime, CPUTIME_SYSTEM);\n\t}\n}\n\nstatic void irqtime_account_idle_ticks(int ticks)\n{\n\tirqtime_account_process_tick(current, 0, ticks);\n}\n#else  \nstatic inline void irqtime_account_idle_ticks(int ticks) { }\nstatic inline void irqtime_account_process_tick(struct task_struct *p, int user_tick,\n\t\t\t\t\t\tint nr_ticks) { }\n#endif  \n\n \n#ifdef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE\n\n# ifndef __ARCH_HAS_VTIME_TASK_SWITCH\nvoid vtime_task_switch(struct task_struct *prev)\n{\n\tif (is_idle_task(prev))\n\t\tvtime_account_idle(prev);\n\telse\n\t\tvtime_account_kernel(prev);\n\n\tvtime_flush(prev);\n\tarch_vtime_task_switch(prev);\n}\n# endif\n\nvoid vtime_account_irq(struct task_struct *tsk, unsigned int offset)\n{\n\tunsigned int pc = irq_count() - offset;\n\n\tif (pc & HARDIRQ_OFFSET) {\n\t\tvtime_account_hardirq(tsk);\n\t} else if (pc & SOFTIRQ_OFFSET) {\n\t\tvtime_account_softirq(tsk);\n\t} else if (!IS_ENABLED(CONFIG_HAVE_VIRT_CPU_ACCOUNTING_IDLE) &&\n\t\t   is_idle_task(tsk)) {\n\t\tvtime_account_idle(tsk);\n\t} else {\n\t\tvtime_account_kernel(tsk);\n\t}\n}\n\nvoid cputime_adjust(struct task_cputime *curr, struct prev_cputime *prev,\n\t\t    u64 *ut, u64 *st)\n{\n\t*ut = curr->utime;\n\t*st = curr->stime;\n}\n\nvoid task_cputime_adjusted(struct task_struct *p, u64 *ut, u64 *st)\n{\n\t*ut = p->utime;\n\t*st = p->stime;\n}\nEXPORT_SYMBOL_GPL(task_cputime_adjusted);\n\nvoid thread_group_cputime_adjusted(struct task_struct *p, u64 *ut, u64 *st)\n{\n\tstruct task_cputime cputime;\n\n\tthread_group_cputime(p, &cputime);\n\n\t*ut = cputime.utime;\n\t*st = cputime.stime;\n}\n\n#else  \n\n \nvoid account_process_tick(struct task_struct *p, int user_tick)\n{\n\tu64 cputime, steal;\n\n\tif (vtime_accounting_enabled_this_cpu())\n\t\treturn;\n\n\tif (sched_clock_irqtime) {\n\t\tirqtime_account_process_tick(p, user_tick, 1);\n\t\treturn;\n\t}\n\n\tcputime = TICK_NSEC;\n\tsteal = steal_account_process_time(ULONG_MAX);\n\n\tif (steal >= cputime)\n\t\treturn;\n\n\tcputime -= steal;\n\n\tif (user_tick)\n\t\taccount_user_time(p, cputime);\n\telse if ((p != this_rq()->idle) || (irq_count() != HARDIRQ_OFFSET))\n\t\taccount_system_time(p, HARDIRQ_OFFSET, cputime);\n\telse\n\t\taccount_idle_time(cputime);\n}\n\n \nvoid account_idle_ticks(unsigned long ticks)\n{\n\tu64 cputime, steal;\n\n\tif (sched_clock_irqtime) {\n\t\tirqtime_account_idle_ticks(ticks);\n\t\treturn;\n\t}\n\n\tcputime = ticks * TICK_NSEC;\n\tsteal = steal_account_process_time(ULONG_MAX);\n\n\tif (steal >= cputime)\n\t\treturn;\n\n\tcputime -= steal;\n\taccount_idle_time(cputime);\n}\n\n \nvoid cputime_adjust(struct task_cputime *curr, struct prev_cputime *prev,\n\t\t    u64 *ut, u64 *st)\n{\n\tu64 rtime, stime, utime;\n\tunsigned long flags;\n\n\t \n\traw_spin_lock_irqsave(&prev->lock, flags);\n\trtime = curr->sum_exec_runtime;\n\n\t \n\tif (prev->stime + prev->utime >= rtime)\n\t\tgoto out;\n\n\tstime = curr->stime;\n\tutime = curr->utime;\n\n\t \n\tif (stime == 0) {\n\t\tutime = rtime;\n\t\tgoto update;\n\t}\n\n\tif (utime == 0) {\n\t\tstime = rtime;\n\t\tgoto update;\n\t}\n\n\tstime = mul_u64_u64_div_u64(stime, rtime, stime + utime);\n\nupdate:\n\t \n\tif (stime < prev->stime)\n\t\tstime = prev->stime;\n\tutime = rtime - stime;\n\n\t \n\tif (utime < prev->utime) {\n\t\tutime = prev->utime;\n\t\tstime = rtime - utime;\n\t}\n\n\tprev->stime = stime;\n\tprev->utime = utime;\nout:\n\t*ut = prev->utime;\n\t*st = prev->stime;\n\traw_spin_unlock_irqrestore(&prev->lock, flags);\n}\n\nvoid task_cputime_adjusted(struct task_struct *p, u64 *ut, u64 *st)\n{\n\tstruct task_cputime cputime = {\n\t\t.sum_exec_runtime = p->se.sum_exec_runtime,\n\t};\n\n\tif (task_cputime(p, &cputime.utime, &cputime.stime))\n\t\tcputime.sum_exec_runtime = task_sched_runtime(p);\n\tcputime_adjust(&cputime, &p->prev_cputime, ut, st);\n}\nEXPORT_SYMBOL_GPL(task_cputime_adjusted);\n\nvoid thread_group_cputime_adjusted(struct task_struct *p, u64 *ut, u64 *st)\n{\n\tstruct task_cputime cputime;\n\n\tthread_group_cputime(p, &cputime);\n\tcputime_adjust(&cputime, &p->signal->prev_cputime, ut, st);\n}\n#endif  \n\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN\nstatic u64 vtime_delta(struct vtime *vtime)\n{\n\tunsigned long long clock;\n\n\tclock = sched_clock();\n\tif (clock < vtime->starttime)\n\t\treturn 0;\n\n\treturn clock - vtime->starttime;\n}\n\nstatic u64 get_vtime_delta(struct vtime *vtime)\n{\n\tu64 delta = vtime_delta(vtime);\n\tu64 other;\n\n\t \n\tother = account_other_time(delta);\n\tWARN_ON_ONCE(vtime->state == VTIME_INACTIVE);\n\tvtime->starttime += delta;\n\n\treturn delta - other;\n}\n\nstatic void vtime_account_system(struct task_struct *tsk,\n\t\t\t\t struct vtime *vtime)\n{\n\tvtime->stime += get_vtime_delta(vtime);\n\tif (vtime->stime >= TICK_NSEC) {\n\t\taccount_system_time(tsk, irq_count(), vtime->stime);\n\t\tvtime->stime = 0;\n\t}\n}\n\nstatic void vtime_account_guest(struct task_struct *tsk,\n\t\t\t\tstruct vtime *vtime)\n{\n\tvtime->gtime += get_vtime_delta(vtime);\n\tif (vtime->gtime >= TICK_NSEC) {\n\t\taccount_guest_time(tsk, vtime->gtime);\n\t\tvtime->gtime = 0;\n\t}\n}\n\nstatic void __vtime_account_kernel(struct task_struct *tsk,\n\t\t\t\t   struct vtime *vtime)\n{\n\t \n\tif (vtime->state == VTIME_GUEST)\n\t\tvtime_account_guest(tsk, vtime);\n\telse\n\t\tvtime_account_system(tsk, vtime);\n}\n\nvoid vtime_account_kernel(struct task_struct *tsk)\n{\n\tstruct vtime *vtime = &tsk->vtime;\n\n\tif (!vtime_delta(vtime))\n\t\treturn;\n\n\twrite_seqcount_begin(&vtime->seqcount);\n\t__vtime_account_kernel(tsk, vtime);\n\twrite_seqcount_end(&vtime->seqcount);\n}\n\nvoid vtime_user_enter(struct task_struct *tsk)\n{\n\tstruct vtime *vtime = &tsk->vtime;\n\n\twrite_seqcount_begin(&vtime->seqcount);\n\tvtime_account_system(tsk, vtime);\n\tvtime->state = VTIME_USER;\n\twrite_seqcount_end(&vtime->seqcount);\n}\n\nvoid vtime_user_exit(struct task_struct *tsk)\n{\n\tstruct vtime *vtime = &tsk->vtime;\n\n\twrite_seqcount_begin(&vtime->seqcount);\n\tvtime->utime += get_vtime_delta(vtime);\n\tif (vtime->utime >= TICK_NSEC) {\n\t\taccount_user_time(tsk, vtime->utime);\n\t\tvtime->utime = 0;\n\t}\n\tvtime->state = VTIME_SYS;\n\twrite_seqcount_end(&vtime->seqcount);\n}\n\nvoid vtime_guest_enter(struct task_struct *tsk)\n{\n\tstruct vtime *vtime = &tsk->vtime;\n\t \n\twrite_seqcount_begin(&vtime->seqcount);\n\tvtime_account_system(tsk, vtime);\n\ttsk->flags |= PF_VCPU;\n\tvtime->state = VTIME_GUEST;\n\twrite_seqcount_end(&vtime->seqcount);\n}\nEXPORT_SYMBOL_GPL(vtime_guest_enter);\n\nvoid vtime_guest_exit(struct task_struct *tsk)\n{\n\tstruct vtime *vtime = &tsk->vtime;\n\n\twrite_seqcount_begin(&vtime->seqcount);\n\tvtime_account_guest(tsk, vtime);\n\ttsk->flags &= ~PF_VCPU;\n\tvtime->state = VTIME_SYS;\n\twrite_seqcount_end(&vtime->seqcount);\n}\nEXPORT_SYMBOL_GPL(vtime_guest_exit);\n\nvoid vtime_account_idle(struct task_struct *tsk)\n{\n\taccount_idle_time(get_vtime_delta(&tsk->vtime));\n}\n\nvoid vtime_task_switch_generic(struct task_struct *prev)\n{\n\tstruct vtime *vtime = &prev->vtime;\n\n\twrite_seqcount_begin(&vtime->seqcount);\n\tif (vtime->state == VTIME_IDLE)\n\t\tvtime_account_idle(prev);\n\telse\n\t\t__vtime_account_kernel(prev, vtime);\n\tvtime->state = VTIME_INACTIVE;\n\tvtime->cpu = -1;\n\twrite_seqcount_end(&vtime->seqcount);\n\n\tvtime = &current->vtime;\n\n\twrite_seqcount_begin(&vtime->seqcount);\n\tif (is_idle_task(current))\n\t\tvtime->state = VTIME_IDLE;\n\telse if (current->flags & PF_VCPU)\n\t\tvtime->state = VTIME_GUEST;\n\telse\n\t\tvtime->state = VTIME_SYS;\n\tvtime->starttime = sched_clock();\n\tvtime->cpu = smp_processor_id();\n\twrite_seqcount_end(&vtime->seqcount);\n}\n\nvoid vtime_init_idle(struct task_struct *t, int cpu)\n{\n\tstruct vtime *vtime = &t->vtime;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\twrite_seqcount_begin(&vtime->seqcount);\n\tvtime->state = VTIME_IDLE;\n\tvtime->starttime = sched_clock();\n\tvtime->cpu = cpu;\n\twrite_seqcount_end(&vtime->seqcount);\n\tlocal_irq_restore(flags);\n}\n\nu64 task_gtime(struct task_struct *t)\n{\n\tstruct vtime *vtime = &t->vtime;\n\tunsigned int seq;\n\tu64 gtime;\n\n\tif (!vtime_accounting_enabled())\n\t\treturn t->gtime;\n\n\tdo {\n\t\tseq = read_seqcount_begin(&vtime->seqcount);\n\n\t\tgtime = t->gtime;\n\t\tif (vtime->state == VTIME_GUEST)\n\t\t\tgtime += vtime->gtime + vtime_delta(vtime);\n\n\t} while (read_seqcount_retry(&vtime->seqcount, seq));\n\n\treturn gtime;\n}\n\n \nbool task_cputime(struct task_struct *t, u64 *utime, u64 *stime)\n{\n\tstruct vtime *vtime = &t->vtime;\n\tunsigned int seq;\n\tu64 delta;\n\tint ret;\n\n\tif (!vtime_accounting_enabled()) {\n\t\t*utime = t->utime;\n\t\t*stime = t->stime;\n\t\treturn false;\n\t}\n\n\tdo {\n\t\tret = false;\n\t\tseq = read_seqcount_begin(&vtime->seqcount);\n\n\t\t*utime = t->utime;\n\t\t*stime = t->stime;\n\n\t\t \n\t\tif (vtime->state < VTIME_SYS)\n\t\t\tcontinue;\n\n\t\tret = true;\n\t\tdelta = vtime_delta(vtime);\n\n\t\t \n\t\tif (vtime->state == VTIME_SYS)\n\t\t\t*stime += vtime->stime + delta;\n\t\telse\n\t\t\t*utime += vtime->utime + delta;\n\t} while (read_seqcount_retry(&vtime->seqcount, seq));\n\n\treturn ret;\n}\n\nstatic int vtime_state_fetch(struct vtime *vtime, int cpu)\n{\n\tint state = READ_ONCE(vtime->state);\n\n\t \n\tif (vtime->cpu != cpu && vtime->cpu != -1)\n\t\treturn -EAGAIN;\n\n\t \n\tif (state == VTIME_INACTIVE)\n\t\treturn -EAGAIN;\n\n\treturn state;\n}\n\nstatic u64 kcpustat_user_vtime(struct vtime *vtime)\n{\n\tif (vtime->state == VTIME_USER)\n\t\treturn vtime->utime + vtime_delta(vtime);\n\telse if (vtime->state == VTIME_GUEST)\n\t\treturn vtime->gtime + vtime_delta(vtime);\n\treturn 0;\n}\n\nstatic int kcpustat_field_vtime(u64 *cpustat,\n\t\t\t\tstruct task_struct *tsk,\n\t\t\t\tenum cpu_usage_stat usage,\n\t\t\t\tint cpu, u64 *val)\n{\n\tstruct vtime *vtime = &tsk->vtime;\n\tunsigned int seq;\n\n\tdo {\n\t\tint state;\n\n\t\tseq = read_seqcount_begin(&vtime->seqcount);\n\n\t\tstate = vtime_state_fetch(vtime, cpu);\n\t\tif (state < 0)\n\t\t\treturn state;\n\n\t\t*val = cpustat[usage];\n\n\t\t \n\t\tswitch (usage) {\n\t\tcase CPUTIME_SYSTEM:\n\t\t\tif (state == VTIME_SYS)\n\t\t\t\t*val += vtime->stime + vtime_delta(vtime);\n\t\t\tbreak;\n\t\tcase CPUTIME_USER:\n\t\t\tif (task_nice(tsk) <= 0)\n\t\t\t\t*val += kcpustat_user_vtime(vtime);\n\t\t\tbreak;\n\t\tcase CPUTIME_NICE:\n\t\t\tif (task_nice(tsk) > 0)\n\t\t\t\t*val += kcpustat_user_vtime(vtime);\n\t\t\tbreak;\n\t\tcase CPUTIME_GUEST:\n\t\t\tif (state == VTIME_GUEST && task_nice(tsk) <= 0)\n\t\t\t\t*val += vtime->gtime + vtime_delta(vtime);\n\t\t\tbreak;\n\t\tcase CPUTIME_GUEST_NICE:\n\t\t\tif (state == VTIME_GUEST && task_nice(tsk) > 0)\n\t\t\t\t*val += vtime->gtime + vtime_delta(vtime);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t} while (read_seqcount_retry(&vtime->seqcount, seq));\n\n\treturn 0;\n}\n\nu64 kcpustat_field(struct kernel_cpustat *kcpustat,\n\t\t   enum cpu_usage_stat usage, int cpu)\n{\n\tu64 *cpustat = kcpustat->cpustat;\n\tu64 val = cpustat[usage];\n\tstruct rq *rq;\n\tint err;\n\n\tif (!vtime_accounting_enabled_cpu(cpu))\n\t\treturn val;\n\n\trq = cpu_rq(cpu);\n\n\tfor (;;) {\n\t\tstruct task_struct *curr;\n\n\t\trcu_read_lock();\n\t\tcurr = rcu_dereference(rq->curr);\n\t\tif (WARN_ON_ONCE(!curr)) {\n\t\t\trcu_read_unlock();\n\t\t\treturn cpustat[usage];\n\t\t}\n\n\t\terr = kcpustat_field_vtime(cpustat, curr, usage, cpu, &val);\n\t\trcu_read_unlock();\n\n\t\tif (!err)\n\t\t\treturn val;\n\n\t\tcpu_relax();\n\t}\n}\nEXPORT_SYMBOL_GPL(kcpustat_field);\n\nstatic int kcpustat_cpu_fetch_vtime(struct kernel_cpustat *dst,\n\t\t\t\t    const struct kernel_cpustat *src,\n\t\t\t\t    struct task_struct *tsk, int cpu)\n{\n\tstruct vtime *vtime = &tsk->vtime;\n\tunsigned int seq;\n\n\tdo {\n\t\tu64 *cpustat;\n\t\tu64 delta;\n\t\tint state;\n\n\t\tseq = read_seqcount_begin(&vtime->seqcount);\n\n\t\tstate = vtime_state_fetch(vtime, cpu);\n\t\tif (state < 0)\n\t\t\treturn state;\n\n\t\t*dst = *src;\n\t\tcpustat = dst->cpustat;\n\n\t\t \n\t\tif (state < VTIME_SYS)\n\t\t\tcontinue;\n\n\t\tdelta = vtime_delta(vtime);\n\n\t\t \n\t\tif (state == VTIME_SYS) {\n\t\t\tcpustat[CPUTIME_SYSTEM] += vtime->stime + delta;\n\t\t} else if (state == VTIME_USER) {\n\t\t\tif (task_nice(tsk) > 0)\n\t\t\t\tcpustat[CPUTIME_NICE] += vtime->utime + delta;\n\t\t\telse\n\t\t\t\tcpustat[CPUTIME_USER] += vtime->utime + delta;\n\t\t} else {\n\t\t\tWARN_ON_ONCE(state != VTIME_GUEST);\n\t\t\tif (task_nice(tsk) > 0) {\n\t\t\t\tcpustat[CPUTIME_GUEST_NICE] += vtime->gtime + delta;\n\t\t\t\tcpustat[CPUTIME_NICE] += vtime->gtime + delta;\n\t\t\t} else {\n\t\t\t\tcpustat[CPUTIME_GUEST] += vtime->gtime + delta;\n\t\t\t\tcpustat[CPUTIME_USER] += vtime->gtime + delta;\n\t\t\t}\n\t\t}\n\t} while (read_seqcount_retry(&vtime->seqcount, seq));\n\n\treturn 0;\n}\n\nvoid kcpustat_cpu_fetch(struct kernel_cpustat *dst, int cpu)\n{\n\tconst struct kernel_cpustat *src = &kcpustat_cpu(cpu);\n\tstruct rq *rq;\n\tint err;\n\n\tif (!vtime_accounting_enabled_cpu(cpu)) {\n\t\t*dst = *src;\n\t\treturn;\n\t}\n\n\trq = cpu_rq(cpu);\n\n\tfor (;;) {\n\t\tstruct task_struct *curr;\n\n\t\trcu_read_lock();\n\t\tcurr = rcu_dereference(rq->curr);\n\t\tif (WARN_ON_ONCE(!curr)) {\n\t\t\trcu_read_unlock();\n\t\t\t*dst = *src;\n\t\t\treturn;\n\t\t}\n\n\t\terr = kcpustat_cpu_fetch_vtime(dst, src, curr, cpu);\n\t\trcu_read_unlock();\n\n\t\tif (!err)\n\t\t\treturn;\n\n\t\tcpu_relax();\n\t}\n}\nEXPORT_SYMBOL_GPL(kcpustat_cpu_fetch);\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}