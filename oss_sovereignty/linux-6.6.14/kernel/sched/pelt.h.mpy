{
  "module_name": "pelt.h",
  "hash_id": "9565f75b065deea264e2b9bb109dacffce915604374458b6ffc32cfeb3e162c2",
  "original_prompt": "Ingested from linux-6.6.14/kernel/sched/pelt.h",
  "human_readable_source": "#ifdef CONFIG_SMP\n#include \"sched-pelt.h\"\n\nint __update_load_avg_blocked_se(u64 now, struct sched_entity *se);\nint __update_load_avg_se(u64 now, struct cfs_rq *cfs_rq, struct sched_entity *se);\nint __update_load_avg_cfs_rq(u64 now, struct cfs_rq *cfs_rq);\nint update_rt_rq_load_avg(u64 now, struct rq *rq, int running);\nint update_dl_rq_load_avg(u64 now, struct rq *rq, int running);\n\n#ifdef CONFIG_SCHED_THERMAL_PRESSURE\nint update_thermal_load_avg(u64 now, struct rq *rq, u64 capacity);\n\nstatic inline u64 thermal_load_avg(struct rq *rq)\n{\n\treturn READ_ONCE(rq->avg_thermal.load_avg);\n}\n#else\nstatic inline int\nupdate_thermal_load_avg(u64 now, struct rq *rq, u64 capacity)\n{\n\treturn 0;\n}\n\nstatic inline u64 thermal_load_avg(struct rq *rq)\n{\n\treturn 0;\n}\n#endif\n\n#ifdef CONFIG_HAVE_SCHED_AVG_IRQ\nint update_irq_load_avg(struct rq *rq, u64 running);\n#else\nstatic inline int\nupdate_irq_load_avg(struct rq *rq, u64 running)\n{\n\treturn 0;\n}\n#endif\n\n#define PELT_MIN_DIVIDER\t(LOAD_AVG_MAX - 1024)\n\nstatic inline u32 get_pelt_divider(struct sched_avg *avg)\n{\n\treturn PELT_MIN_DIVIDER + avg->period_contrib;\n}\n\nstatic inline void cfs_se_util_change(struct sched_avg *avg)\n{\n\tunsigned int enqueued;\n\n\tif (!sched_feat(UTIL_EST))\n\t\treturn;\n\n\t \n\tenqueued = avg->util_est.enqueued;\n\tif (!(enqueued & UTIL_AVG_UNCHANGED))\n\t\treturn;\n\n\t \n\tenqueued &= ~UTIL_AVG_UNCHANGED;\n\tWRITE_ONCE(avg->util_est.enqueued, enqueued);\n}\n\nstatic inline u64 rq_clock_pelt(struct rq *rq)\n{\n\tlockdep_assert_rq_held(rq);\n\tassert_clock_updated(rq);\n\n\treturn rq->clock_pelt - rq->lost_idle_time;\n}\n\n \nstatic inline void _update_idle_rq_clock_pelt(struct rq *rq)\n{\n\trq->clock_pelt  = rq_clock_task(rq);\n\n\tu64_u32_store(rq->clock_idle, rq_clock(rq));\n\t \n\tsmp_wmb();\n\tu64_u32_store(rq->clock_pelt_idle, rq_clock_pelt(rq));\n}\n\n \nstatic inline void update_rq_clock_pelt(struct rq *rq, s64 delta)\n{\n\tif (unlikely(is_idle_task(rq->curr))) {\n\t\t_update_idle_rq_clock_pelt(rq);\n\t\treturn;\n\t}\n\n\t \n\n\t \n\tdelta = cap_scale(delta, arch_scale_cpu_capacity(cpu_of(rq)));\n\tdelta = cap_scale(delta, arch_scale_freq_capacity(cpu_of(rq)));\n\n\trq->clock_pelt += delta;\n}\n\n \nstatic inline void update_idle_rq_clock_pelt(struct rq *rq)\n{\n\tu32 divider = ((LOAD_AVG_MAX - 1024) << SCHED_CAPACITY_SHIFT) - LOAD_AVG_MAX;\n\tu32 util_sum = rq->cfs.avg.util_sum;\n\tutil_sum += rq->avg_rt.util_sum;\n\tutil_sum += rq->avg_dl.util_sum;\n\n\t \n\tif (util_sum >= divider)\n\t\trq->lost_idle_time += rq_clock_task(rq) - rq->clock_pelt;\n\n\t_update_idle_rq_clock_pelt(rq);\n}\n\n#ifdef CONFIG_CFS_BANDWIDTH\nstatic inline void update_idle_cfs_rq_clock_pelt(struct cfs_rq *cfs_rq)\n{\n\tu64 throttled;\n\n\tif (unlikely(cfs_rq->throttle_count))\n\t\tthrottled = U64_MAX;\n\telse\n\t\tthrottled = cfs_rq->throttled_clock_pelt_time;\n\n\tu64_u32_store(cfs_rq->throttled_pelt_idle, throttled);\n}\n\n \nstatic inline u64 cfs_rq_clock_pelt(struct cfs_rq *cfs_rq)\n{\n\tif (unlikely(cfs_rq->throttle_count))\n\t\treturn cfs_rq->throttled_clock_pelt - cfs_rq->throttled_clock_pelt_time;\n\n\treturn rq_clock_pelt(rq_of(cfs_rq)) - cfs_rq->throttled_clock_pelt_time;\n}\n#else\nstatic inline void update_idle_cfs_rq_clock_pelt(struct cfs_rq *cfs_rq) { }\nstatic inline u64 cfs_rq_clock_pelt(struct cfs_rq *cfs_rq)\n{\n\treturn rq_clock_pelt(rq_of(cfs_rq));\n}\n#endif\n\n#else\n\nstatic inline int\nupdate_cfs_rq_load_avg(u64 now, struct cfs_rq *cfs_rq)\n{\n\treturn 0;\n}\n\nstatic inline int\nupdate_rt_rq_load_avg(u64 now, struct rq *rq, int running)\n{\n\treturn 0;\n}\n\nstatic inline int\nupdate_dl_rq_load_avg(u64 now, struct rq *rq, int running)\n{\n\treturn 0;\n}\n\nstatic inline int\nupdate_thermal_load_avg(u64 now, struct rq *rq, u64 capacity)\n{\n\treturn 0;\n}\n\nstatic inline u64 thermal_load_avg(struct rq *rq)\n{\n\treturn 0;\n}\n\nstatic inline int\nupdate_irq_load_avg(struct rq *rq, u64 running)\n{\n\treturn 0;\n}\n\nstatic inline u64 rq_clock_pelt(struct rq *rq)\n{\n\treturn rq_clock_task(rq);\n}\n\nstatic inline void\nupdate_rq_clock_pelt(struct rq *rq, s64 delta) { }\n\nstatic inline void\nupdate_idle_rq_clock_pelt(struct rq *rq) { }\n\nstatic inline void update_idle_cfs_rq_clock_pelt(struct cfs_rq *cfs_rq) { }\n#endif\n\n\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}