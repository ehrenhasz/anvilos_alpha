{
  "module_name": "kexec_core.c",
  "hash_id": "737555438f207e5e856f3a0fc6095c9c805e4543dcd11004b3d67fb92249d8d8",
  "original_prompt": "Ingested from linux-6.6.14/kernel/kexec_core.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/btf.h>\n#include <linux/capability.h>\n#include <linux/mm.h>\n#include <linux/file.h>\n#include <linux/slab.h>\n#include <linux/fs.h>\n#include <linux/kexec.h>\n#include <linux/mutex.h>\n#include <linux/list.h>\n#include <linux/highmem.h>\n#include <linux/syscalls.h>\n#include <linux/reboot.h>\n#include <linux/ioport.h>\n#include <linux/hardirq.h>\n#include <linux/elf.h>\n#include <linux/elfcore.h>\n#include <linux/utsname.h>\n#include <linux/numa.h>\n#include <linux/suspend.h>\n#include <linux/device.h>\n#include <linux/freezer.h>\n#include <linux/panic_notifier.h>\n#include <linux/pm.h>\n#include <linux/cpu.h>\n#include <linux/uaccess.h>\n#include <linux/io.h>\n#include <linux/console.h>\n#include <linux/vmalloc.h>\n#include <linux/swap.h>\n#include <linux/syscore_ops.h>\n#include <linux/compiler.h>\n#include <linux/hugetlb.h>\n#include <linux/objtool.h>\n#include <linux/kmsg_dump.h>\n\n#include <asm/page.h>\n#include <asm/sections.h>\n\n#include <crypto/hash.h>\n#include \"kexec_internal.h\"\n\natomic_t __kexec_lock = ATOMIC_INIT(0);\n\n \nbool kexec_in_progress = false;\n\n\n \nstruct resource crashk_res = {\n\t.name  = \"Crash kernel\",\n\t.start = 0,\n\t.end   = 0,\n\t.flags = IORESOURCE_BUSY | IORESOURCE_SYSTEM_RAM,\n\t.desc  = IORES_DESC_CRASH_KERNEL\n};\nstruct resource crashk_low_res = {\n\t.name  = \"Crash kernel\",\n\t.start = 0,\n\t.end   = 0,\n\t.flags = IORESOURCE_BUSY | IORESOURCE_SYSTEM_RAM,\n\t.desc  = IORES_DESC_CRASH_KERNEL\n};\n\nint kexec_should_crash(struct task_struct *p)\n{\n\t \n\tif (crash_kexec_post_notifiers)\n\t\treturn 0;\n\t \n\tif (in_interrupt() || !p->pid || is_global_init(p) || panic_on_oops)\n\t\treturn 1;\n\treturn 0;\n}\n\nint kexec_crash_loaded(void)\n{\n\treturn !!kexec_crash_image;\n}\nEXPORT_SYMBOL_GPL(kexec_crash_loaded);\n\n \n\n \n#define KIMAGE_NO_DEST (-1UL)\n#define PAGE_COUNT(x) (((x) + PAGE_SIZE - 1) >> PAGE_SHIFT)\n\nstatic struct page *kimage_alloc_page(struct kimage *image,\n\t\t\t\t       gfp_t gfp_mask,\n\t\t\t\t       unsigned long dest);\n\nint sanity_check_segment_list(struct kimage *image)\n{\n\tint i;\n\tunsigned long nr_segments = image->nr_segments;\n\tunsigned long total_pages = 0;\n\tunsigned long nr_pages = totalram_pages();\n\n\t \n\tfor (i = 0; i < nr_segments; i++) {\n\t\tunsigned long mstart, mend;\n\n\t\tmstart = image->segment[i].mem;\n\t\tmend   = mstart + image->segment[i].memsz;\n\t\tif (mstart > mend)\n\t\t\treturn -EADDRNOTAVAIL;\n\t\tif ((mstart & ~PAGE_MASK) || (mend & ~PAGE_MASK))\n\t\t\treturn -EADDRNOTAVAIL;\n\t\tif (mend >= KEXEC_DESTINATION_MEMORY_LIMIT)\n\t\t\treturn -EADDRNOTAVAIL;\n\t}\n\n\t \n\tfor (i = 0; i < nr_segments; i++) {\n\t\tunsigned long mstart, mend;\n\t\tunsigned long j;\n\n\t\tmstart = image->segment[i].mem;\n\t\tmend   = mstart + image->segment[i].memsz;\n\t\tfor (j = 0; j < i; j++) {\n\t\t\tunsigned long pstart, pend;\n\n\t\t\tpstart = image->segment[j].mem;\n\t\t\tpend   = pstart + image->segment[j].memsz;\n\t\t\t \n\t\t\tif ((mend > pstart) && (mstart < pend))\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t \n\tfor (i = 0; i < nr_segments; i++) {\n\t\tif (image->segment[i].bufsz > image->segment[i].memsz)\n\t\t\treturn -EINVAL;\n\t}\n\n\t \n\tfor (i = 0; i < nr_segments; i++) {\n\t\tif (PAGE_COUNT(image->segment[i].memsz) > nr_pages / 2)\n\t\t\treturn -EINVAL;\n\n\t\ttotal_pages += PAGE_COUNT(image->segment[i].memsz);\n\t}\n\n\tif (total_pages > nr_pages / 2)\n\t\treturn -EINVAL;\n\n\t \n\n\tif (image->type == KEXEC_TYPE_CRASH) {\n\t\tfor (i = 0; i < nr_segments; i++) {\n\t\t\tunsigned long mstart, mend;\n\n\t\t\tmstart = image->segment[i].mem;\n\t\t\tmend = mstart + image->segment[i].memsz - 1;\n\t\t\t \n\t\t\tif ((mstart < phys_to_boot_phys(crashk_res.start)) ||\n\t\t\t    (mend > phys_to_boot_phys(crashk_res.end)))\n\t\t\t\treturn -EADDRNOTAVAIL;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstruct kimage *do_kimage_alloc_init(void)\n{\n\tstruct kimage *image;\n\n\t \n\timage = kzalloc(sizeof(*image), GFP_KERNEL);\n\tif (!image)\n\t\treturn NULL;\n\n\timage->head = 0;\n\timage->entry = &image->head;\n\timage->last_entry = &image->head;\n\timage->control_page = ~0;  \n\timage->type = KEXEC_TYPE_DEFAULT;\n\n\t \n\tINIT_LIST_HEAD(&image->control_pages);\n\n\t \n\tINIT_LIST_HEAD(&image->dest_pages);\n\n\t \n\tINIT_LIST_HEAD(&image->unusable_pages);\n\n#ifdef CONFIG_CRASH_HOTPLUG\n\timage->hp_action = KEXEC_CRASH_HP_NONE;\n\timage->elfcorehdr_index = -1;\n\timage->elfcorehdr_updated = false;\n#endif\n\n\treturn image;\n}\n\nint kimage_is_destination_range(struct kimage *image,\n\t\t\t\t\tunsigned long start,\n\t\t\t\t\tunsigned long end)\n{\n\tunsigned long i;\n\n\tfor (i = 0; i < image->nr_segments; i++) {\n\t\tunsigned long mstart, mend;\n\n\t\tmstart = image->segment[i].mem;\n\t\tmend = mstart + image->segment[i].memsz;\n\t\tif ((end > mstart) && (start < mend))\n\t\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic struct page *kimage_alloc_pages(gfp_t gfp_mask, unsigned int order)\n{\n\tstruct page *pages;\n\n\tif (fatal_signal_pending(current))\n\t\treturn NULL;\n\tpages = alloc_pages(gfp_mask & ~__GFP_ZERO, order);\n\tif (pages) {\n\t\tunsigned int count, i;\n\n\t\tpages->mapping = NULL;\n\t\tset_page_private(pages, order);\n\t\tcount = 1 << order;\n\t\tfor (i = 0; i < count; i++)\n\t\t\tSetPageReserved(pages + i);\n\n\t\tarch_kexec_post_alloc_pages(page_address(pages), count,\n\t\t\t\t\t    gfp_mask);\n\n\t\tif (gfp_mask & __GFP_ZERO)\n\t\t\tfor (i = 0; i < count; i++)\n\t\t\t\tclear_highpage(pages + i);\n\t}\n\n\treturn pages;\n}\n\nstatic void kimage_free_pages(struct page *page)\n{\n\tunsigned int order, count, i;\n\n\torder = page_private(page);\n\tcount = 1 << order;\n\n\tarch_kexec_pre_free_pages(page_address(page), count);\n\n\tfor (i = 0; i < count; i++)\n\t\tClearPageReserved(page + i);\n\t__free_pages(page, order);\n}\n\nvoid kimage_free_page_list(struct list_head *list)\n{\n\tstruct page *page, *next;\n\n\tlist_for_each_entry_safe(page, next, list, lru) {\n\t\tlist_del(&page->lru);\n\t\tkimage_free_pages(page);\n\t}\n}\n\nstatic struct page *kimage_alloc_normal_control_pages(struct kimage *image,\n\t\t\t\t\t\t\tunsigned int order)\n{\n\t \n\tstruct list_head extra_pages;\n\tstruct page *pages;\n\tunsigned int count;\n\n\tcount = 1 << order;\n\tINIT_LIST_HEAD(&extra_pages);\n\n\t \n\tdo {\n\t\tunsigned long pfn, epfn, addr, eaddr;\n\n\t\tpages = kimage_alloc_pages(KEXEC_CONTROL_MEMORY_GFP, order);\n\t\tif (!pages)\n\t\t\tbreak;\n\t\tpfn   = page_to_boot_pfn(pages);\n\t\tepfn  = pfn + count;\n\t\taddr  = pfn << PAGE_SHIFT;\n\t\teaddr = epfn << PAGE_SHIFT;\n\t\tif ((epfn >= (KEXEC_CONTROL_MEMORY_LIMIT >> PAGE_SHIFT)) ||\n\t\t\t      kimage_is_destination_range(image, addr, eaddr)) {\n\t\t\tlist_add(&pages->lru, &extra_pages);\n\t\t\tpages = NULL;\n\t\t}\n\t} while (!pages);\n\n\tif (pages) {\n\t\t \n\t\tlist_add(&pages->lru, &image->control_pages);\n\n\t\t \n\t}\n\t \n\tkimage_free_page_list(&extra_pages);\n\n\treturn pages;\n}\n\nstatic struct page *kimage_alloc_crash_control_pages(struct kimage *image,\n\t\t\t\t\t\t      unsigned int order)\n{\n\t \n\tunsigned long hole_start, hole_end, size;\n\tstruct page *pages;\n\n\tpages = NULL;\n\tsize = (1 << order) << PAGE_SHIFT;\n\thole_start = (image->control_page + (size - 1)) & ~(size - 1);\n\thole_end   = hole_start + size - 1;\n\twhile (hole_end <= crashk_res.end) {\n\t\tunsigned long i;\n\n\t\tcond_resched();\n\n\t\tif (hole_end > KEXEC_CRASH_CONTROL_MEMORY_LIMIT)\n\t\t\tbreak;\n\t\t \n\t\tfor (i = 0; i < image->nr_segments; i++) {\n\t\t\tunsigned long mstart, mend;\n\n\t\t\tmstart = image->segment[i].mem;\n\t\t\tmend   = mstart + image->segment[i].memsz - 1;\n\t\t\tif ((hole_end >= mstart) && (hole_start <= mend)) {\n\t\t\t\t \n\t\t\t\thole_start = (mend + (size - 1)) & ~(size - 1);\n\t\t\t\thole_end   = hole_start + size - 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\t \n\t\tif (i == image->nr_segments) {\n\t\t\tpages = pfn_to_page(hole_start >> PAGE_SHIFT);\n\t\t\timage->control_page = hole_end;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\tif (pages)\n\t\tarch_kexec_post_alloc_pages(page_address(pages), 1 << order, 0);\n\n\treturn pages;\n}\n\n\nstruct page *kimage_alloc_control_pages(struct kimage *image,\n\t\t\t\t\t unsigned int order)\n{\n\tstruct page *pages = NULL;\n\n\tswitch (image->type) {\n\tcase KEXEC_TYPE_DEFAULT:\n\t\tpages = kimage_alloc_normal_control_pages(image, order);\n\t\tbreak;\n\tcase KEXEC_TYPE_CRASH:\n\t\tpages = kimage_alloc_crash_control_pages(image, order);\n\t\tbreak;\n\t}\n\n\treturn pages;\n}\n\nint kimage_crash_copy_vmcoreinfo(struct kimage *image)\n{\n\tstruct page *vmcoreinfo_page;\n\tvoid *safecopy;\n\n\tif (image->type != KEXEC_TYPE_CRASH)\n\t\treturn 0;\n\n\t \n\tvmcoreinfo_page = kimage_alloc_control_pages(image, 0);\n\tif (!vmcoreinfo_page) {\n\t\tpr_warn(\"Could not allocate vmcoreinfo buffer\\n\");\n\t\treturn -ENOMEM;\n\t}\n\tsafecopy = vmap(&vmcoreinfo_page, 1, VM_MAP, PAGE_KERNEL);\n\tif (!safecopy) {\n\t\tpr_warn(\"Could not vmap vmcoreinfo buffer\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\timage->vmcoreinfo_data_copy = safecopy;\n\tcrash_update_vmcoreinfo_safecopy(safecopy);\n\n\treturn 0;\n}\n\nstatic int kimage_add_entry(struct kimage *image, kimage_entry_t entry)\n{\n\tif (*image->entry != 0)\n\t\timage->entry++;\n\n\tif (image->entry == image->last_entry) {\n\t\tkimage_entry_t *ind_page;\n\t\tstruct page *page;\n\n\t\tpage = kimage_alloc_page(image, GFP_KERNEL, KIMAGE_NO_DEST);\n\t\tif (!page)\n\t\t\treturn -ENOMEM;\n\n\t\tind_page = page_address(page);\n\t\t*image->entry = virt_to_boot_phys(ind_page) | IND_INDIRECTION;\n\t\timage->entry = ind_page;\n\t\timage->last_entry = ind_page +\n\t\t\t\t      ((PAGE_SIZE/sizeof(kimage_entry_t)) - 1);\n\t}\n\t*image->entry = entry;\n\timage->entry++;\n\t*image->entry = 0;\n\n\treturn 0;\n}\n\nstatic int kimage_set_destination(struct kimage *image,\n\t\t\t\t   unsigned long destination)\n{\n\tdestination &= PAGE_MASK;\n\n\treturn kimage_add_entry(image, destination | IND_DESTINATION);\n}\n\n\nstatic int kimage_add_page(struct kimage *image, unsigned long page)\n{\n\tpage &= PAGE_MASK;\n\n\treturn kimage_add_entry(image, page | IND_SOURCE);\n}\n\n\nstatic void kimage_free_extra_pages(struct kimage *image)\n{\n\t \n\tkimage_free_page_list(&image->dest_pages);\n\n\t \n\tkimage_free_page_list(&image->unusable_pages);\n\n}\n\nvoid kimage_terminate(struct kimage *image)\n{\n\tif (*image->entry != 0)\n\t\timage->entry++;\n\n\t*image->entry = IND_DONE;\n}\n\n#define for_each_kimage_entry(image, ptr, entry) \\\n\tfor (ptr = &image->head; (entry = *ptr) && !(entry & IND_DONE); \\\n\t\tptr = (entry & IND_INDIRECTION) ? \\\n\t\t\tboot_phys_to_virt((entry & PAGE_MASK)) : ptr + 1)\n\nstatic void kimage_free_entry(kimage_entry_t entry)\n{\n\tstruct page *page;\n\n\tpage = boot_pfn_to_page(entry >> PAGE_SHIFT);\n\tkimage_free_pages(page);\n}\n\nvoid kimage_free(struct kimage *image)\n{\n\tkimage_entry_t *ptr, entry;\n\tkimage_entry_t ind = 0;\n\n\tif (!image)\n\t\treturn;\n\n\tif (image->vmcoreinfo_data_copy) {\n\t\tcrash_update_vmcoreinfo_safecopy(NULL);\n\t\tvunmap(image->vmcoreinfo_data_copy);\n\t}\n\n\tkimage_free_extra_pages(image);\n\tfor_each_kimage_entry(image, ptr, entry) {\n\t\tif (entry & IND_INDIRECTION) {\n\t\t\t \n\t\t\tif (ind & IND_INDIRECTION)\n\t\t\t\tkimage_free_entry(ind);\n\t\t\t \n\t\t\tind = entry;\n\t\t} else if (entry & IND_SOURCE)\n\t\t\tkimage_free_entry(entry);\n\t}\n\t \n\tif (ind & IND_INDIRECTION)\n\t\tkimage_free_entry(ind);\n\n\t \n\tmachine_kexec_cleanup(image);\n\n\t \n\tkimage_free_page_list(&image->control_pages);\n\n\t \n\tif (image->file_mode)\n\t\tkimage_file_post_load_cleanup(image);\n\n\tkfree(image);\n}\n\nstatic kimage_entry_t *kimage_dst_used(struct kimage *image,\n\t\t\t\t\tunsigned long page)\n{\n\tkimage_entry_t *ptr, entry;\n\tunsigned long destination = 0;\n\n\tfor_each_kimage_entry(image, ptr, entry) {\n\t\tif (entry & IND_DESTINATION)\n\t\t\tdestination = entry & PAGE_MASK;\n\t\telse if (entry & IND_SOURCE) {\n\t\t\tif (page == destination)\n\t\t\t\treturn ptr;\n\t\t\tdestination += PAGE_SIZE;\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\nstatic struct page *kimage_alloc_page(struct kimage *image,\n\t\t\t\t\tgfp_t gfp_mask,\n\t\t\t\t\tunsigned long destination)\n{\n\t \n\tstruct page *page;\n\tunsigned long addr;\n\n\t \n\tlist_for_each_entry(page, &image->dest_pages, lru) {\n\t\taddr = page_to_boot_pfn(page) << PAGE_SHIFT;\n\t\tif (addr == destination) {\n\t\t\tlist_del(&page->lru);\n\t\t\treturn page;\n\t\t}\n\t}\n\tpage = NULL;\n\twhile (1) {\n\t\tkimage_entry_t *old;\n\n\t\t \n\t\tpage = kimage_alloc_pages(gfp_mask, 0);\n\t\tif (!page)\n\t\t\treturn NULL;\n\t\t \n\t\tif (page_to_boot_pfn(page) >\n\t\t\t\t(KEXEC_SOURCE_MEMORY_LIMIT >> PAGE_SHIFT)) {\n\t\t\tlist_add(&page->lru, &image->unusable_pages);\n\t\t\tcontinue;\n\t\t}\n\t\taddr = page_to_boot_pfn(page) << PAGE_SHIFT;\n\n\t\t \n\t\tif (addr == destination)\n\t\t\tbreak;\n\n\t\t \n\t\tif (!kimage_is_destination_range(image, addr,\n\t\t\t\t\t\t  addr + PAGE_SIZE))\n\t\t\tbreak;\n\n\t\t \n\t\told = kimage_dst_used(image, addr);\n\t\tif (old) {\n\t\t\t \n\t\t\tunsigned long old_addr;\n\t\t\tstruct page *old_page;\n\n\t\t\told_addr = *old & PAGE_MASK;\n\t\t\told_page = boot_pfn_to_page(old_addr >> PAGE_SHIFT);\n\t\t\tcopy_highpage(page, old_page);\n\t\t\t*old = addr | (*old & ~PAGE_MASK);\n\n\t\t\t \n\t\t\tif (!(gfp_mask & __GFP_HIGHMEM) &&\n\t\t\t    PageHighMem(old_page)) {\n\t\t\t\tkimage_free_pages(old_page);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tpage = old_page;\n\t\t\tbreak;\n\t\t}\n\t\t \n\t\tlist_add(&page->lru, &image->dest_pages);\n\t}\n\n\treturn page;\n}\n\nstatic int kimage_load_normal_segment(struct kimage *image,\n\t\t\t\t\t struct kexec_segment *segment)\n{\n\tunsigned long maddr;\n\tsize_t ubytes, mbytes;\n\tint result;\n\tunsigned char __user *buf = NULL;\n\tunsigned char *kbuf = NULL;\n\n\tif (image->file_mode)\n\t\tkbuf = segment->kbuf;\n\telse\n\t\tbuf = segment->buf;\n\tubytes = segment->bufsz;\n\tmbytes = segment->memsz;\n\tmaddr = segment->mem;\n\n\tresult = kimage_set_destination(image, maddr);\n\tif (result < 0)\n\t\tgoto out;\n\n\twhile (mbytes) {\n\t\tstruct page *page;\n\t\tchar *ptr;\n\t\tsize_t uchunk, mchunk;\n\n\t\tpage = kimage_alloc_page(image, GFP_HIGHUSER, maddr);\n\t\tif (!page) {\n\t\t\tresult  = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tresult = kimage_add_page(image, page_to_boot_pfn(page)\n\t\t\t\t\t\t\t\t<< PAGE_SHIFT);\n\t\tif (result < 0)\n\t\t\tgoto out;\n\n\t\tptr = kmap_local_page(page);\n\t\t \n\t\tclear_page(ptr);\n\t\tptr += maddr & ~PAGE_MASK;\n\t\tmchunk = min_t(size_t, mbytes,\n\t\t\t\tPAGE_SIZE - (maddr & ~PAGE_MASK));\n\t\tuchunk = min(ubytes, mchunk);\n\n\t\t \n\t\tif (image->file_mode)\n\t\t\tmemcpy(ptr, kbuf, uchunk);\n\t\telse\n\t\t\tresult = copy_from_user(ptr, buf, uchunk);\n\t\tkunmap_local(ptr);\n\t\tif (result) {\n\t\t\tresult = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t\tubytes -= uchunk;\n\t\tmaddr  += mchunk;\n\t\tif (image->file_mode)\n\t\t\tkbuf += mchunk;\n\t\telse\n\t\t\tbuf += mchunk;\n\t\tmbytes -= mchunk;\n\n\t\tcond_resched();\n\t}\nout:\n\treturn result;\n}\n\nstatic int kimage_load_crash_segment(struct kimage *image,\n\t\t\t\t\tstruct kexec_segment *segment)\n{\n\t \n\tunsigned long maddr;\n\tsize_t ubytes, mbytes;\n\tint result;\n\tunsigned char __user *buf = NULL;\n\tunsigned char *kbuf = NULL;\n\n\tresult = 0;\n\tif (image->file_mode)\n\t\tkbuf = segment->kbuf;\n\telse\n\t\tbuf = segment->buf;\n\tubytes = segment->bufsz;\n\tmbytes = segment->memsz;\n\tmaddr = segment->mem;\n\twhile (mbytes) {\n\t\tstruct page *page;\n\t\tchar *ptr;\n\t\tsize_t uchunk, mchunk;\n\n\t\tpage = boot_pfn_to_page(maddr >> PAGE_SHIFT);\n\t\tif (!page) {\n\t\t\tresult  = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tarch_kexec_post_alloc_pages(page_address(page), 1, 0);\n\t\tptr = kmap_local_page(page);\n\t\tptr += maddr & ~PAGE_MASK;\n\t\tmchunk = min_t(size_t, mbytes,\n\t\t\t\tPAGE_SIZE - (maddr & ~PAGE_MASK));\n\t\tuchunk = min(ubytes, mchunk);\n\t\tif (mchunk > uchunk) {\n\t\t\t \n\t\t\tmemset(ptr + uchunk, 0, mchunk - uchunk);\n\t\t}\n\n\t\t \n\t\tif (image->file_mode)\n\t\t\tmemcpy(ptr, kbuf, uchunk);\n\t\telse\n\t\t\tresult = copy_from_user(ptr, buf, uchunk);\n\t\tkexec_flush_icache_page(page);\n\t\tkunmap_local(ptr);\n\t\tarch_kexec_pre_free_pages(page_address(page), 1);\n\t\tif (result) {\n\t\t\tresult = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t\tubytes -= uchunk;\n\t\tmaddr  += mchunk;\n\t\tif (image->file_mode)\n\t\t\tkbuf += mchunk;\n\t\telse\n\t\t\tbuf += mchunk;\n\t\tmbytes -= mchunk;\n\n\t\tcond_resched();\n\t}\nout:\n\treturn result;\n}\n\nint kimage_load_segment(struct kimage *image,\n\t\t\t\tstruct kexec_segment *segment)\n{\n\tint result = -ENOMEM;\n\n\tswitch (image->type) {\n\tcase KEXEC_TYPE_DEFAULT:\n\t\tresult = kimage_load_normal_segment(image, segment);\n\t\tbreak;\n\tcase KEXEC_TYPE_CRASH:\n\t\tresult = kimage_load_crash_segment(image, segment);\n\t\tbreak;\n\t}\n\n\treturn result;\n}\n\nstruct kexec_load_limit {\n\t \n\tstruct mutex mutex;\n\tint limit;\n};\n\nstatic struct kexec_load_limit load_limit_reboot = {\n\t.mutex = __MUTEX_INITIALIZER(load_limit_reboot.mutex),\n\t.limit = -1,\n};\n\nstatic struct kexec_load_limit load_limit_panic = {\n\t.mutex = __MUTEX_INITIALIZER(load_limit_panic.mutex),\n\t.limit = -1,\n};\n\nstruct kimage *kexec_image;\nstruct kimage *kexec_crash_image;\nstatic int kexec_load_disabled;\n\n#ifdef CONFIG_SYSCTL\nstatic int kexec_limit_handler(struct ctl_table *table, int write,\n\t\t\t       void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct kexec_load_limit *limit = table->data;\n\tint val;\n\tstruct ctl_table tmp = {\n\t\t.data = &val,\n\t\t.maxlen = sizeof(val),\n\t\t.mode = table->mode,\n\t};\n\tint ret;\n\n\tif (write) {\n\t\tret = proc_dointvec(&tmp, write, buffer, lenp, ppos);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tif (val < 0)\n\t\t\treturn -EINVAL;\n\n\t\tmutex_lock(&limit->mutex);\n\t\tif (limit->limit != -1 && val >= limit->limit)\n\t\t\tret = -EINVAL;\n\t\telse\n\t\t\tlimit->limit = val;\n\t\tmutex_unlock(&limit->mutex);\n\n\t\treturn ret;\n\t}\n\n\tmutex_lock(&limit->mutex);\n\tval = limit->limit;\n\tmutex_unlock(&limit->mutex);\n\n\treturn proc_dointvec(&tmp, write, buffer, lenp, ppos);\n}\n\nstatic struct ctl_table kexec_core_sysctls[] = {\n\t{\n\t\t.procname\t= \"kexec_load_disabled\",\n\t\t.data\t\t= &kexec_load_disabled,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t \n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_ONE,\n\t},\n\t{\n\t\t.procname\t= \"kexec_load_limit_panic\",\n\t\t.data\t\t= &load_limit_panic,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= kexec_limit_handler,\n\t},\n\t{\n\t\t.procname\t= \"kexec_load_limit_reboot\",\n\t\t.data\t\t= &load_limit_reboot,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= kexec_limit_handler,\n\t},\n\t{ }\n};\n\nstatic int __init kexec_core_sysctl_init(void)\n{\n\tregister_sysctl_init(\"kernel\", kexec_core_sysctls);\n\treturn 0;\n}\nlate_initcall(kexec_core_sysctl_init);\n#endif\n\nbool kexec_load_permitted(int kexec_image_type)\n{\n\tstruct kexec_load_limit *limit;\n\n\t \n\tif (!capable(CAP_SYS_BOOT) || kexec_load_disabled)\n\t\treturn false;\n\n\t \n\tlimit = (kexec_image_type == KEXEC_TYPE_CRASH) ?\n\t\t&load_limit_panic : &load_limit_reboot;\n\tmutex_lock(&limit->mutex);\n\tif (!limit->limit) {\n\t\tmutex_unlock(&limit->mutex);\n\t\treturn false;\n\t}\n\tif (limit->limit != -1)\n\t\tlimit->limit--;\n\tmutex_unlock(&limit->mutex);\n\n\treturn true;\n}\n\n \nvoid __noclone __crash_kexec(struct pt_regs *regs)\n{\n\t \n\tif (kexec_trylock()) {\n\t\tif (kexec_crash_image) {\n\t\t\tstruct pt_regs fixed_regs;\n\n\t\t\tcrash_setup_regs(&fixed_regs, regs);\n\t\t\tcrash_save_vmcoreinfo();\n\t\t\tmachine_crash_shutdown(&fixed_regs);\n\t\t\tmachine_kexec(kexec_crash_image);\n\t\t}\n\t\tkexec_unlock();\n\t}\n}\nSTACK_FRAME_NON_STANDARD(__crash_kexec);\n\n__bpf_kfunc void crash_kexec(struct pt_regs *regs)\n{\n\tint old_cpu, this_cpu;\n\n\t \n\tthis_cpu = raw_smp_processor_id();\n\told_cpu = atomic_cmpxchg(&panic_cpu, PANIC_CPU_INVALID, this_cpu);\n\tif (old_cpu == PANIC_CPU_INVALID) {\n\t\t \n\t\t__crash_kexec(regs);\n\n\t\t \n\t\tatomic_set(&panic_cpu, PANIC_CPU_INVALID);\n\t}\n}\n\nstatic inline resource_size_t crash_resource_size(const struct resource *res)\n{\n\treturn !res->end ? 0 : resource_size(res);\n}\n\nssize_t crash_get_memory_size(void)\n{\n\tssize_t size = 0;\n\n\tif (!kexec_trylock())\n\t\treturn -EBUSY;\n\n\tsize += crash_resource_size(&crashk_res);\n\tsize += crash_resource_size(&crashk_low_res);\n\n\tkexec_unlock();\n\treturn size;\n}\n\nstatic int __crash_shrink_memory(struct resource *old_res,\n\t\t\t\t unsigned long new_size)\n{\n\tstruct resource *ram_res;\n\n\tram_res = kzalloc(sizeof(*ram_res), GFP_KERNEL);\n\tif (!ram_res)\n\t\treturn -ENOMEM;\n\n\tram_res->start = old_res->start + new_size;\n\tram_res->end   = old_res->end;\n\tram_res->flags = IORESOURCE_BUSY | IORESOURCE_SYSTEM_RAM;\n\tram_res->name  = \"System RAM\";\n\n\tif (!new_size) {\n\t\trelease_resource(old_res);\n\t\told_res->start = 0;\n\t\told_res->end   = 0;\n\t} else {\n\t\tcrashk_res.end = ram_res->start - 1;\n\t}\n\n\tcrash_free_reserved_phys_range(ram_res->start, ram_res->end);\n\tinsert_resource(&iomem_resource, ram_res);\n\n\treturn 0;\n}\n\nint crash_shrink_memory(unsigned long new_size)\n{\n\tint ret = 0;\n\tunsigned long old_size, low_size;\n\n\tif (!kexec_trylock())\n\t\treturn -EBUSY;\n\n\tif (kexec_crash_image) {\n\t\tret = -ENOENT;\n\t\tgoto unlock;\n\t}\n\n\tlow_size = crash_resource_size(&crashk_low_res);\n\told_size = crash_resource_size(&crashk_res) + low_size;\n\tnew_size = roundup(new_size, KEXEC_CRASH_MEM_ALIGN);\n\tif (new_size >= old_size) {\n\t\tret = (new_size == old_size) ? 0 : -EINVAL;\n\t\tgoto unlock;\n\t}\n\n\t \n\tif (low_size > new_size) {\n\t\tret = __crash_shrink_memory(&crashk_res, 0);\n\t\tif (ret)\n\t\t\tgoto unlock;\n\n\t\tret = __crash_shrink_memory(&crashk_low_res, new_size);\n\t} else {\n\t\tret = __crash_shrink_memory(&crashk_res, new_size - low_size);\n\t}\n\n\t \n\tif (!crashk_res.end && crashk_low_res.end) {\n\t\tcrashk_res.start = crashk_low_res.start;\n\t\tcrashk_res.end   = crashk_low_res.end;\n\t\trelease_resource(&crashk_low_res);\n\t\tcrashk_low_res.start = 0;\n\t\tcrashk_low_res.end   = 0;\n\t\tinsert_resource(&iomem_resource, &crashk_res);\n\t}\n\nunlock:\n\tkexec_unlock();\n\treturn ret;\n}\n\nvoid crash_save_cpu(struct pt_regs *regs, int cpu)\n{\n\tstruct elf_prstatus prstatus;\n\tu32 *buf;\n\n\tif ((cpu < 0) || (cpu >= nr_cpu_ids))\n\t\treturn;\n\n\t \n\tbuf = (u32 *)per_cpu_ptr(crash_notes, cpu);\n\tif (!buf)\n\t\treturn;\n\tmemset(&prstatus, 0, sizeof(prstatus));\n\tprstatus.common.pr_pid = current->pid;\n\telf_core_copy_regs(&prstatus.pr_reg, regs);\n\tbuf = append_elf_note(buf, KEXEC_CORE_NOTE_NAME, NT_PRSTATUS,\n\t\t\t      &prstatus, sizeof(prstatus));\n\tfinal_note(buf);\n}\n\n \nint kernel_kexec(void)\n{\n\tint error = 0;\n\n\tif (!kexec_trylock())\n\t\treturn -EBUSY;\n\tif (!kexec_image) {\n\t\terror = -EINVAL;\n\t\tgoto Unlock;\n\t}\n\n#ifdef CONFIG_KEXEC_JUMP\n\tif (kexec_image->preserve_context) {\n\t\tpm_prepare_console();\n\t\terror = freeze_processes();\n\t\tif (error) {\n\t\t\terror = -EBUSY;\n\t\t\tgoto Restore_console;\n\t\t}\n\t\tsuspend_console();\n\t\terror = dpm_suspend_start(PMSG_FREEZE);\n\t\tif (error)\n\t\t\tgoto Resume_console;\n\t\t \n\t\terror = dpm_suspend_end(PMSG_FREEZE);\n\t\tif (error)\n\t\t\tgoto Resume_devices;\n\t\terror = suspend_disable_secondary_cpus();\n\t\tif (error)\n\t\t\tgoto Enable_cpus;\n\t\tlocal_irq_disable();\n\t\terror = syscore_suspend();\n\t\tif (error)\n\t\t\tgoto Enable_irqs;\n\t} else\n#endif\n\t{\n\t\tkexec_in_progress = true;\n\t\tkernel_restart_prepare(\"kexec reboot\");\n\t\tmigrate_to_reboot_cpu();\n\n\t\t \n\t\tcpu_hotplug_enable();\n\t\tpr_notice(\"Starting new kernel\\n\");\n\t\tmachine_shutdown();\n\t}\n\n\tkmsg_dump(KMSG_DUMP_SHUTDOWN);\n\tmachine_kexec(kexec_image);\n\n#ifdef CONFIG_KEXEC_JUMP\n\tif (kexec_image->preserve_context) {\n\t\tsyscore_resume();\n Enable_irqs:\n\t\tlocal_irq_enable();\n Enable_cpus:\n\t\tsuspend_enable_secondary_cpus();\n\t\tdpm_resume_start(PMSG_RESTORE);\n Resume_devices:\n\t\tdpm_resume_end(PMSG_RESTORE);\n Resume_console:\n\t\tresume_console();\n\t\tthaw_processes();\n Restore_console:\n\t\tpm_restore_console();\n\t}\n#endif\n\n Unlock:\n\tkexec_unlock();\n\treturn error;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}