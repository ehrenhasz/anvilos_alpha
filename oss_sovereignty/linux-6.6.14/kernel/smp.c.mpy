{
  "module_name": "smp.c",
  "hash_id": "a4b38a23227665491b66c81c37b35358c73d91b91ab7d8cd435335820adabe6b",
  "original_prompt": "Ingested from linux-6.6.14/kernel/smp.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/irq_work.h>\n#include <linux/rcupdate.h>\n#include <linux/rculist.h>\n#include <linux/kernel.h>\n#include <linux/export.h>\n#include <linux/percpu.h>\n#include <linux/init.h>\n#include <linux/interrupt.h>\n#include <linux/gfp.h>\n#include <linux/smp.h>\n#include <linux/cpu.h>\n#include <linux/sched.h>\n#include <linux/sched/idle.h>\n#include <linux/hypervisor.h>\n#include <linux/sched/clock.h>\n#include <linux/nmi.h>\n#include <linux/sched/debug.h>\n#include <linux/jump_label.h>\n\n#include <trace/events/ipi.h>\n#define CREATE_TRACE_POINTS\n#include <trace/events/csd.h>\n#undef CREATE_TRACE_POINTS\n\n#include \"smpboot.h\"\n#include \"sched/smp.h\"\n\n#define CSD_TYPE(_csd)\t((_csd)->node.u_flags & CSD_FLAG_TYPE_MASK)\n\nstruct call_function_data {\n\tcall_single_data_t\t__percpu *csd;\n\tcpumask_var_t\t\tcpumask;\n\tcpumask_var_t\t\tcpumask_ipi;\n};\n\nstatic DEFINE_PER_CPU_ALIGNED(struct call_function_data, cfd_data);\n\nstatic DEFINE_PER_CPU_SHARED_ALIGNED(struct llist_head, call_single_queue);\n\nstatic DEFINE_PER_CPU(atomic_t, trigger_backtrace) = ATOMIC_INIT(1);\n\nstatic void __flush_smp_call_function_queue(bool warn_cpu_offline);\n\nint smpcfd_prepare_cpu(unsigned int cpu)\n{\n\tstruct call_function_data *cfd = &per_cpu(cfd_data, cpu);\n\n\tif (!zalloc_cpumask_var_node(&cfd->cpumask, GFP_KERNEL,\n\t\t\t\t     cpu_to_node(cpu)))\n\t\treturn -ENOMEM;\n\tif (!zalloc_cpumask_var_node(&cfd->cpumask_ipi, GFP_KERNEL,\n\t\t\t\t     cpu_to_node(cpu))) {\n\t\tfree_cpumask_var(cfd->cpumask);\n\t\treturn -ENOMEM;\n\t}\n\tcfd->csd = alloc_percpu(call_single_data_t);\n\tif (!cfd->csd) {\n\t\tfree_cpumask_var(cfd->cpumask);\n\t\tfree_cpumask_var(cfd->cpumask_ipi);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nint smpcfd_dead_cpu(unsigned int cpu)\n{\n\tstruct call_function_data *cfd = &per_cpu(cfd_data, cpu);\n\n\tfree_cpumask_var(cfd->cpumask);\n\tfree_cpumask_var(cfd->cpumask_ipi);\n\tfree_percpu(cfd->csd);\n\treturn 0;\n}\n\nint smpcfd_dying_cpu(unsigned int cpu)\n{\n\t \n\t__flush_smp_call_function_queue(false);\n\tirq_work_run();\n\treturn 0;\n}\n\nvoid __init call_function_init(void)\n{\n\tint i;\n\n\tfor_each_possible_cpu(i)\n\t\tinit_llist_head(&per_cpu(call_single_queue, i));\n\n\tsmpcfd_prepare_cpu(smp_processor_id());\n}\n\nstatic __always_inline void\nsend_call_function_single_ipi(int cpu)\n{\n\tif (call_function_single_prep_ipi(cpu)) {\n\t\ttrace_ipi_send_cpu(cpu, _RET_IP_,\n\t\t\t\t   generic_smp_call_function_single_interrupt);\n\t\tarch_send_call_function_single_ipi(cpu);\n\t}\n}\n\nstatic __always_inline void\nsend_call_function_ipi_mask(struct cpumask *mask)\n{\n\ttrace_ipi_send_cpumask(mask, _RET_IP_,\n\t\t\t       generic_smp_call_function_single_interrupt);\n\tarch_send_call_function_ipi_mask(mask);\n}\n\nstatic __always_inline void\ncsd_do_func(smp_call_func_t func, void *info, struct __call_single_data *csd)\n{\n\ttrace_csd_function_entry(func, csd);\n\tfunc(info);\n\ttrace_csd_function_exit(func, csd);\n}\n\n#ifdef CONFIG_CSD_LOCK_WAIT_DEBUG\n\nstatic DEFINE_STATIC_KEY_MAYBE(CONFIG_CSD_LOCK_WAIT_DEBUG_DEFAULT, csdlock_debug_enabled);\n\n \nstatic int __init csdlock_debug(char *str)\n{\n\tint ret;\n\tunsigned int val = 0;\n\n\tret = get_option(&str, &val);\n\tif (ret) {\n\t\tif (val)\n\t\t\tstatic_branch_enable(&csdlock_debug_enabled);\n\t\telse\n\t\t\tstatic_branch_disable(&csdlock_debug_enabled);\n\t}\n\n\treturn 1;\n}\n__setup(\"csdlock_debug=\", csdlock_debug);\n\nstatic DEFINE_PER_CPU(call_single_data_t *, cur_csd);\nstatic DEFINE_PER_CPU(smp_call_func_t, cur_csd_func);\nstatic DEFINE_PER_CPU(void *, cur_csd_info);\n\nstatic ulong csd_lock_timeout = 5000;   \nmodule_param(csd_lock_timeout, ulong, 0444);\nstatic int panic_on_ipistall;   \nmodule_param(panic_on_ipistall, int, 0444);\n\nstatic atomic_t csd_bug_count = ATOMIC_INIT(0);\n\n \nstatic void __csd_lock_record(struct __call_single_data *csd)\n{\n\tif (!csd) {\n\t\tsmp_mb();  \n\t\t__this_cpu_write(cur_csd, NULL);\n\t\treturn;\n\t}\n\t__this_cpu_write(cur_csd_func, csd->func);\n\t__this_cpu_write(cur_csd_info, csd->info);\n\tsmp_wmb();  \n\t__this_cpu_write(cur_csd, csd);\n\tsmp_mb();  \n\t\t   \n}\n\nstatic __always_inline void csd_lock_record(struct __call_single_data *csd)\n{\n\tif (static_branch_unlikely(&csdlock_debug_enabled))\n\t\t__csd_lock_record(csd);\n}\n\nstatic int csd_lock_wait_getcpu(struct __call_single_data *csd)\n{\n\tunsigned int csd_type;\n\n\tcsd_type = CSD_TYPE(csd);\n\tif (csd_type == CSD_TYPE_ASYNC || csd_type == CSD_TYPE_SYNC)\n\t\treturn csd->node.dst;  \n\treturn -1;\n}\n\n \nstatic bool csd_lock_wait_toolong(struct __call_single_data *csd, u64 ts0, u64 *ts1, int *bug_id)\n{\n\tint cpu = -1;\n\tint cpux;\n\tbool firsttime;\n\tu64 ts2, ts_delta;\n\tcall_single_data_t *cpu_cur_csd;\n\tunsigned int flags = READ_ONCE(csd->node.u_flags);\n\tunsigned long long csd_lock_timeout_ns = csd_lock_timeout * NSEC_PER_MSEC;\n\n\tif (!(flags & CSD_FLAG_LOCK)) {\n\t\tif (!unlikely(*bug_id))\n\t\t\treturn true;\n\t\tcpu = csd_lock_wait_getcpu(csd);\n\t\tpr_alert(\"csd: CSD lock (#%d) got unstuck on CPU#%02d, CPU#%02d released the lock.\\n\",\n\t\t\t *bug_id, raw_smp_processor_id(), cpu);\n\t\treturn true;\n\t}\n\n\tts2 = sched_clock();\n\t \n\tts_delta = ts2 - *ts1;\n\tif (likely(ts_delta <= csd_lock_timeout_ns || csd_lock_timeout_ns == 0))\n\t\treturn false;\n\n\tfirsttime = !*bug_id;\n\tif (firsttime)\n\t\t*bug_id = atomic_inc_return(&csd_bug_count);\n\tcpu = csd_lock_wait_getcpu(csd);\n\tif (WARN_ONCE(cpu < 0 || cpu >= nr_cpu_ids, \"%s: cpu = %d\\n\", __func__, cpu))\n\t\tcpux = 0;\n\telse\n\t\tcpux = cpu;\n\tcpu_cur_csd = smp_load_acquire(&per_cpu(cur_csd, cpux));  \n\t \n\tts_delta = ts2 - ts0;\n\tpr_alert(\"csd: %s non-responsive CSD lock (#%d) on CPU#%d, waiting %llu ns for CPU#%02d %pS(%ps).\\n\",\n\t\t firsttime ? \"Detected\" : \"Continued\", *bug_id, raw_smp_processor_id(), ts_delta,\n\t\t cpu, csd->func, csd->info);\n\t \n\tBUG_ON(panic_on_ipistall > 0 && (s64)ts_delta > ((s64)panic_on_ipistall * NSEC_PER_MSEC));\n\tif (cpu_cur_csd && csd != cpu_cur_csd) {\n\t\tpr_alert(\"\\tcsd: CSD lock (#%d) handling prior %pS(%ps) request.\\n\",\n\t\t\t *bug_id, READ_ONCE(per_cpu(cur_csd_func, cpux)),\n\t\t\t READ_ONCE(per_cpu(cur_csd_info, cpux)));\n\t} else {\n\t\tpr_alert(\"\\tcsd: CSD lock (#%d) %s.\\n\",\n\t\t\t *bug_id, !cpu_cur_csd ? \"unresponsive\" : \"handling this request\");\n\t}\n\tif (cpu >= 0) {\n\t\tif (atomic_cmpxchg_acquire(&per_cpu(trigger_backtrace, cpu), 1, 0))\n\t\t\tdump_cpu_task(cpu);\n\t\tif (!cpu_cur_csd) {\n\t\t\tpr_alert(\"csd: Re-sending CSD lock (#%d) IPI from CPU#%02d to CPU#%02d\\n\", *bug_id, raw_smp_processor_id(), cpu);\n\t\t\tarch_send_call_function_single_ipi(cpu);\n\t\t}\n\t}\n\tif (firsttime)\n\t\tdump_stack();\n\t*ts1 = ts2;\n\n\treturn false;\n}\n\n \nstatic void __csd_lock_wait(struct __call_single_data *csd)\n{\n\tint bug_id = 0;\n\tu64 ts0, ts1;\n\n\tts1 = ts0 = sched_clock();\n\tfor (;;) {\n\t\tif (csd_lock_wait_toolong(csd, ts0, &ts1, &bug_id))\n\t\t\tbreak;\n\t\tcpu_relax();\n\t}\n\tsmp_acquire__after_ctrl_dep();\n}\n\nstatic __always_inline void csd_lock_wait(struct __call_single_data *csd)\n{\n\tif (static_branch_unlikely(&csdlock_debug_enabled)) {\n\t\t__csd_lock_wait(csd);\n\t\treturn;\n\t}\n\n\tsmp_cond_load_acquire(&csd->node.u_flags, !(VAL & CSD_FLAG_LOCK));\n}\n#else\nstatic void csd_lock_record(struct __call_single_data *csd)\n{\n}\n\nstatic __always_inline void csd_lock_wait(struct __call_single_data *csd)\n{\n\tsmp_cond_load_acquire(&csd->node.u_flags, !(VAL & CSD_FLAG_LOCK));\n}\n#endif\n\nstatic __always_inline void csd_lock(struct __call_single_data *csd)\n{\n\tcsd_lock_wait(csd);\n\tcsd->node.u_flags |= CSD_FLAG_LOCK;\n\n\t \n\tsmp_wmb();\n}\n\nstatic __always_inline void csd_unlock(struct __call_single_data *csd)\n{\n\tWARN_ON(!(csd->node.u_flags & CSD_FLAG_LOCK));\n\n\t \n\tsmp_store_release(&csd->node.u_flags, 0);\n}\n\nstatic DEFINE_PER_CPU_SHARED_ALIGNED(call_single_data_t, csd_data);\n\nvoid __smp_call_single_queue(int cpu, struct llist_node *node)\n{\n\t \n\tif (trace_csd_queue_cpu_enabled()) {\n\t\tcall_single_data_t *csd;\n\t\tsmp_call_func_t func;\n\n\t\tcsd = container_of(node, call_single_data_t, node.llist);\n\t\tfunc = CSD_TYPE(csd) == CSD_TYPE_TTWU ?\n\t\t\tsched_ttwu_pending : csd->func;\n\n\t\ttrace_csd_queue_cpu(cpu, _RET_IP_, func, csd);\n\t}\n\n\t \n\tif (llist_add(node, &per_cpu(call_single_queue, cpu)))\n\t\tsend_call_function_single_ipi(cpu);\n}\n\n \nstatic int generic_exec_single(int cpu, struct __call_single_data *csd)\n{\n\tif (cpu == smp_processor_id()) {\n\t\tsmp_call_func_t func = csd->func;\n\t\tvoid *info = csd->info;\n\t\tunsigned long flags;\n\n\t\t \n\t\tcsd_lock_record(csd);\n\t\tcsd_unlock(csd);\n\t\tlocal_irq_save(flags);\n\t\tcsd_do_func(func, info, NULL);\n\t\tcsd_lock_record(NULL);\n\t\tlocal_irq_restore(flags);\n\t\treturn 0;\n\t}\n\n\tif ((unsigned)cpu >= nr_cpu_ids || !cpu_online(cpu)) {\n\t\tcsd_unlock(csd);\n\t\treturn -ENXIO;\n\t}\n\n\t__smp_call_single_queue(cpu, &csd->node.llist);\n\n\treturn 0;\n}\n\n \nvoid generic_smp_call_function_single_interrupt(void)\n{\n\t__flush_smp_call_function_queue(true);\n}\n\n \nstatic void __flush_smp_call_function_queue(bool warn_cpu_offline)\n{\n\tcall_single_data_t *csd, *csd_next;\n\tstruct llist_node *entry, *prev;\n\tstruct llist_head *head;\n\tstatic bool warned;\n\tatomic_t *tbt;\n\n\tlockdep_assert_irqs_disabled();\n\n\t \n\ttbt = this_cpu_ptr(&trigger_backtrace);\n\tatomic_set_release(tbt, 1);\n\n\thead = this_cpu_ptr(&call_single_queue);\n\tentry = llist_del_all(head);\n\tentry = llist_reverse_order(entry);\n\n\t \n\tif (unlikely(warn_cpu_offline && !cpu_online(smp_processor_id()) &&\n\t\t     !warned && entry != NULL)) {\n\t\twarned = true;\n\t\tWARN(1, \"IPI on offline CPU %d\\n\", smp_processor_id());\n\n\t\t \n\t\tllist_for_each_entry(csd, entry, node.llist) {\n\t\t\tswitch (CSD_TYPE(csd)) {\n\t\t\tcase CSD_TYPE_ASYNC:\n\t\t\tcase CSD_TYPE_SYNC:\n\t\t\tcase CSD_TYPE_IRQ_WORK:\n\t\t\t\tpr_warn(\"IPI callback %pS sent to offline CPU\\n\",\n\t\t\t\t\tcsd->func);\n\t\t\t\tbreak;\n\n\t\t\tcase CSD_TYPE_TTWU:\n\t\t\t\tpr_warn(\"IPI task-wakeup sent to offline CPU\\n\");\n\t\t\t\tbreak;\n\n\t\t\tdefault:\n\t\t\t\tpr_warn(\"IPI callback, unknown type %d, sent to offline CPU\\n\",\n\t\t\t\t\tCSD_TYPE(csd));\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tprev = NULL;\n\tllist_for_each_entry_safe(csd, csd_next, entry, node.llist) {\n\t\t \n\t\tif (CSD_TYPE(csd) == CSD_TYPE_SYNC) {\n\t\t\tsmp_call_func_t func = csd->func;\n\t\t\tvoid *info = csd->info;\n\n\t\t\tif (prev) {\n\t\t\t\tprev->next = &csd_next->node.llist;\n\t\t\t} else {\n\t\t\t\tentry = &csd_next->node.llist;\n\t\t\t}\n\n\t\t\tcsd_lock_record(csd);\n\t\t\tcsd_do_func(func, info, csd);\n\t\t\tcsd_unlock(csd);\n\t\t\tcsd_lock_record(NULL);\n\t\t} else {\n\t\t\tprev = &csd->node.llist;\n\t\t}\n\t}\n\n\tif (!entry)\n\t\treturn;\n\n\t \n\tprev = NULL;\n\tllist_for_each_entry_safe(csd, csd_next, entry, node.llist) {\n\t\tint type = CSD_TYPE(csd);\n\n\t\tif (type != CSD_TYPE_TTWU) {\n\t\t\tif (prev) {\n\t\t\t\tprev->next = &csd_next->node.llist;\n\t\t\t} else {\n\t\t\t\tentry = &csd_next->node.llist;\n\t\t\t}\n\n\t\t\tif (type == CSD_TYPE_ASYNC) {\n\t\t\t\tsmp_call_func_t func = csd->func;\n\t\t\t\tvoid *info = csd->info;\n\n\t\t\t\tcsd_lock_record(csd);\n\t\t\t\tcsd_unlock(csd);\n\t\t\t\tcsd_do_func(func, info, csd);\n\t\t\t\tcsd_lock_record(NULL);\n\t\t\t} else if (type == CSD_TYPE_IRQ_WORK) {\n\t\t\t\tirq_work_single(csd);\n\t\t\t}\n\n\t\t} else {\n\t\t\tprev = &csd->node.llist;\n\t\t}\n\t}\n\n\t \n\tif (entry) {\n\t\tcsd = llist_entry(entry, typeof(*csd), node.llist);\n\t\tcsd_do_func(sched_ttwu_pending, entry, csd);\n\t}\n}\n\n\n \nvoid flush_smp_call_function_queue(void)\n{\n\tunsigned int was_pending;\n\tunsigned long flags;\n\n\tif (llist_empty(this_cpu_ptr(&call_single_queue)))\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\t \n\twas_pending = local_softirq_pending();\n\t__flush_smp_call_function_queue(true);\n\tif (local_softirq_pending())\n\t\tdo_softirq_post_smp_call_flush(was_pending);\n\n\tlocal_irq_restore(flags);\n}\n\n \nint smp_call_function_single(int cpu, smp_call_func_t func, void *info,\n\t\t\t     int wait)\n{\n\tcall_single_data_t *csd;\n\tcall_single_data_t csd_stack = {\n\t\t.node = { .u_flags = CSD_FLAG_LOCK | CSD_TYPE_SYNC, },\n\t};\n\tint this_cpu;\n\tint err;\n\n\t \n\tthis_cpu = get_cpu();\n\n\t \n\tWARN_ON_ONCE(cpu_online(this_cpu) && irqs_disabled()\n\t\t     && !oops_in_progress);\n\n\t \n\tWARN_ON_ONCE(!in_task());\n\n\tcsd = &csd_stack;\n\tif (!wait) {\n\t\tcsd = this_cpu_ptr(&csd_data);\n\t\tcsd_lock(csd);\n\t}\n\n\tcsd->func = func;\n\tcsd->info = info;\n#ifdef CONFIG_CSD_LOCK_WAIT_DEBUG\n\tcsd->node.src = smp_processor_id();\n\tcsd->node.dst = cpu;\n#endif\n\n\terr = generic_exec_single(cpu, csd);\n\n\tif (wait)\n\t\tcsd_lock_wait(csd);\n\n\tput_cpu();\n\n\treturn err;\n}\nEXPORT_SYMBOL(smp_call_function_single);\n\n \nint smp_call_function_single_async(int cpu, struct __call_single_data *csd)\n{\n\tint err = 0;\n\n\tpreempt_disable();\n\n\tif (csd->node.u_flags & CSD_FLAG_LOCK) {\n\t\terr = -EBUSY;\n\t\tgoto out;\n\t}\n\n\tcsd->node.u_flags = CSD_FLAG_LOCK;\n\tsmp_wmb();\n\n\terr = generic_exec_single(cpu, csd);\n\nout:\n\tpreempt_enable();\n\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(smp_call_function_single_async);\n\n \nint smp_call_function_any(const struct cpumask *mask,\n\t\t\t  smp_call_func_t func, void *info, int wait)\n{\n\tunsigned int cpu;\n\tconst struct cpumask *nodemask;\n\tint ret;\n\n\t \n\tcpu = get_cpu();\n\tif (cpumask_test_cpu(cpu, mask))\n\t\tgoto call;\n\n\t \n\tnodemask = cpumask_of_node(cpu_to_node(cpu));\n\tfor (cpu = cpumask_first_and(nodemask, mask); cpu < nr_cpu_ids;\n\t     cpu = cpumask_next_and(cpu, nodemask, mask)) {\n\t\tif (cpu_online(cpu))\n\t\t\tgoto call;\n\t}\n\n\t \n\tcpu = cpumask_any_and(mask, cpu_online_mask);\ncall:\n\tret = smp_call_function_single(cpu, func, info, wait);\n\tput_cpu();\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(smp_call_function_any);\n\n \n#define SCF_WAIT\t(1U << 0)\n#define SCF_RUN_LOCAL\t(1U << 1)\n\nstatic void smp_call_function_many_cond(const struct cpumask *mask,\n\t\t\t\t\tsmp_call_func_t func, void *info,\n\t\t\t\t\tunsigned int scf_flags,\n\t\t\t\t\tsmp_cond_func_t cond_func)\n{\n\tint cpu, last_cpu, this_cpu = smp_processor_id();\n\tstruct call_function_data *cfd;\n\tbool wait = scf_flags & SCF_WAIT;\n\tint nr_cpus = 0;\n\tbool run_remote = false;\n\tbool run_local = false;\n\n\tlockdep_assert_preemption_disabled();\n\n\t \n\tif (cpu_online(this_cpu) && !oops_in_progress &&\n\t    !early_boot_irqs_disabled)\n\t\tlockdep_assert_irqs_enabled();\n\n\t \n\tWARN_ON_ONCE(!in_task());\n\n\t \n\tif ((scf_flags & SCF_RUN_LOCAL) && cpumask_test_cpu(this_cpu, mask))\n\t\trun_local = true;\n\n\t \n\tcpu = cpumask_first_and(mask, cpu_online_mask);\n\tif (cpu == this_cpu)\n\t\tcpu = cpumask_next_and(cpu, mask, cpu_online_mask);\n\tif (cpu < nr_cpu_ids)\n\t\trun_remote = true;\n\n\tif (run_remote) {\n\t\tcfd = this_cpu_ptr(&cfd_data);\n\t\tcpumask_and(cfd->cpumask, mask, cpu_online_mask);\n\t\t__cpumask_clear_cpu(this_cpu, cfd->cpumask);\n\n\t\tcpumask_clear(cfd->cpumask_ipi);\n\t\tfor_each_cpu(cpu, cfd->cpumask) {\n\t\t\tcall_single_data_t *csd = per_cpu_ptr(cfd->csd, cpu);\n\n\t\t\tif (cond_func && !cond_func(cpu, info)) {\n\t\t\t\t__cpumask_clear_cpu(cpu, cfd->cpumask);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tcsd_lock(csd);\n\t\t\tif (wait)\n\t\t\t\tcsd->node.u_flags |= CSD_TYPE_SYNC;\n\t\t\tcsd->func = func;\n\t\t\tcsd->info = info;\n#ifdef CONFIG_CSD_LOCK_WAIT_DEBUG\n\t\t\tcsd->node.src = smp_processor_id();\n\t\t\tcsd->node.dst = cpu;\n#endif\n\t\t\ttrace_csd_queue_cpu(cpu, _RET_IP_, func, csd);\n\n\t\t\tif (llist_add(&csd->node.llist, &per_cpu(call_single_queue, cpu))) {\n\t\t\t\t__cpumask_set_cpu(cpu, cfd->cpumask_ipi);\n\t\t\t\tnr_cpus++;\n\t\t\t\tlast_cpu = cpu;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (nr_cpus == 1)\n\t\t\tsend_call_function_single_ipi(last_cpu);\n\t\telse if (likely(nr_cpus > 1))\n\t\t\tsend_call_function_ipi_mask(cfd->cpumask_ipi);\n\t}\n\n\tif (run_local && (!cond_func || cond_func(this_cpu, info))) {\n\t\tunsigned long flags;\n\n\t\tlocal_irq_save(flags);\n\t\tcsd_do_func(func, info, NULL);\n\t\tlocal_irq_restore(flags);\n\t}\n\n\tif (run_remote && wait) {\n\t\tfor_each_cpu(cpu, cfd->cpumask) {\n\t\t\tcall_single_data_t *csd;\n\n\t\t\tcsd = per_cpu_ptr(cfd->csd, cpu);\n\t\t\tcsd_lock_wait(csd);\n\t\t}\n\t}\n}\n\n \nvoid smp_call_function_many(const struct cpumask *mask,\n\t\t\t    smp_call_func_t func, void *info, bool wait)\n{\n\tsmp_call_function_many_cond(mask, func, info, wait * SCF_WAIT, NULL);\n}\nEXPORT_SYMBOL(smp_call_function_many);\n\n \nvoid smp_call_function(smp_call_func_t func, void *info, int wait)\n{\n\tpreempt_disable();\n\tsmp_call_function_many(cpu_online_mask, func, info, wait);\n\tpreempt_enable();\n}\nEXPORT_SYMBOL(smp_call_function);\n\n \nunsigned int setup_max_cpus = NR_CPUS;\nEXPORT_SYMBOL(setup_max_cpus);\n\n\n \n\nvoid __weak __init arch_disable_smp_support(void) { }\n\nstatic int __init nosmp(char *str)\n{\n\tsetup_max_cpus = 0;\n\tarch_disable_smp_support();\n\n\treturn 0;\n}\n\nearly_param(\"nosmp\", nosmp);\n\n \nstatic int __init nrcpus(char *str)\n{\n\tint nr_cpus;\n\n\tif (get_option(&str, &nr_cpus) && nr_cpus > 0 && nr_cpus < nr_cpu_ids)\n\t\tset_nr_cpu_ids(nr_cpus);\n\n\treturn 0;\n}\n\nearly_param(\"nr_cpus\", nrcpus);\n\nstatic int __init maxcpus(char *str)\n{\n\tget_option(&str, &setup_max_cpus);\n\tif (setup_max_cpus == 0)\n\t\tarch_disable_smp_support();\n\n\treturn 0;\n}\n\nearly_param(\"maxcpus\", maxcpus);\n\n#if (NR_CPUS > 1) && !defined(CONFIG_FORCE_NR_CPUS)\n \nunsigned int nr_cpu_ids __read_mostly = NR_CPUS;\nEXPORT_SYMBOL(nr_cpu_ids);\n#endif\n\n \nvoid __init setup_nr_cpu_ids(void)\n{\n\tset_nr_cpu_ids(find_last_bit(cpumask_bits(cpu_possible_mask), NR_CPUS) + 1);\n}\n\n \nvoid __init smp_init(void)\n{\n\tint num_nodes, num_cpus;\n\n\tidle_threads_init();\n\tcpuhp_threads_init();\n\n\tpr_info(\"Bringing up secondary CPUs ...\\n\");\n\n\tbringup_nonboot_cpus(setup_max_cpus);\n\n\tnum_nodes = num_online_nodes();\n\tnum_cpus  = num_online_cpus();\n\tpr_info(\"Brought up %d node%s, %d CPU%s\\n\",\n\t\tnum_nodes, (num_nodes > 1 ? \"s\" : \"\"),\n\t\tnum_cpus,  (num_cpus  > 1 ? \"s\" : \"\"));\n\n\t \n\tsmp_cpus_done(setup_max_cpus);\n}\n\n \nvoid on_each_cpu_cond_mask(smp_cond_func_t cond_func, smp_call_func_t func,\n\t\t\t   void *info, bool wait, const struct cpumask *mask)\n{\n\tunsigned int scf_flags = SCF_RUN_LOCAL;\n\n\tif (wait)\n\t\tscf_flags |= SCF_WAIT;\n\n\tpreempt_disable();\n\tsmp_call_function_many_cond(mask, func, info, scf_flags, cond_func);\n\tpreempt_enable();\n}\nEXPORT_SYMBOL(on_each_cpu_cond_mask);\n\nstatic void do_nothing(void *unused)\n{\n}\n\n \nvoid kick_all_cpus_sync(void)\n{\n\t \n\tsmp_mb();\n\tsmp_call_function(do_nothing, NULL, 1);\n}\nEXPORT_SYMBOL_GPL(kick_all_cpus_sync);\n\n \nvoid wake_up_all_idle_cpus(void)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tpreempt_disable();\n\t\tif (cpu != smp_processor_id() && cpu_online(cpu))\n\t\t\twake_up_if_idle(cpu);\n\t\tpreempt_enable();\n\t}\n}\nEXPORT_SYMBOL_GPL(wake_up_all_idle_cpus);\n\n \nstruct smp_call_on_cpu_struct {\n\tstruct work_struct\twork;\n\tstruct completion\tdone;\n\tint\t\t\t(*func)(void *);\n\tvoid\t\t\t*data;\n\tint\t\t\tret;\n\tint\t\t\tcpu;\n};\n\nstatic void smp_call_on_cpu_callback(struct work_struct *work)\n{\n\tstruct smp_call_on_cpu_struct *sscs;\n\n\tsscs = container_of(work, struct smp_call_on_cpu_struct, work);\n\tif (sscs->cpu >= 0)\n\t\thypervisor_pin_vcpu(sscs->cpu);\n\tsscs->ret = sscs->func(sscs->data);\n\tif (sscs->cpu >= 0)\n\t\thypervisor_pin_vcpu(-1);\n\n\tcomplete(&sscs->done);\n}\n\nint smp_call_on_cpu(unsigned int cpu, int (*func)(void *), void *par, bool phys)\n{\n\tstruct smp_call_on_cpu_struct sscs = {\n\t\t.done = COMPLETION_INITIALIZER_ONSTACK(sscs.done),\n\t\t.func = func,\n\t\t.data = par,\n\t\t.cpu  = phys ? cpu : -1,\n\t};\n\n\tINIT_WORK_ONSTACK(&sscs.work, smp_call_on_cpu_callback);\n\n\tif (cpu >= nr_cpu_ids || !cpu_online(cpu))\n\t\treturn -ENXIO;\n\n\tqueue_work_on(cpu, system_wq, &sscs.work);\n\twait_for_completion(&sscs.done);\n\n\treturn sscs.ret;\n}\nEXPORT_SYMBOL_GPL(smp_call_on_cpu);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}