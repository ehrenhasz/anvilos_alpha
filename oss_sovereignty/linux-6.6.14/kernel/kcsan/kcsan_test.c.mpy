{
  "module_name": "kcsan_test.c",
  "hash_id": "7189113962a36577f4f071bdf9043dd2fbf6a9d6fb8ff135ce11902cd1fab752",
  "original_prompt": "Ingested from linux-6.6.14/kernel/kcsan/kcsan_test.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) \"kcsan_test: \" fmt\n\n#include <kunit/test.h>\n#include <linux/atomic.h>\n#include <linux/bitops.h>\n#include <linux/jiffies.h>\n#include <linux/kcsan-checks.h>\n#include <linux/kernel.h>\n#include <linux/mutex.h>\n#include <linux/sched.h>\n#include <linux/seqlock.h>\n#include <linux/spinlock.h>\n#include <linux/string.h>\n#include <linux/timer.h>\n#include <linux/torture.h>\n#include <linux/tracepoint.h>\n#include <linux/types.h>\n#include <trace/events/printk.h>\n\n#define KCSAN_TEST_REQUIRES(test, cond) do {\t\t\t\\\n\tif (!(cond))\t\t\t\t\t\t\\\n\t\tkunit_skip((test), \"Test requires: \" #cond);\t\\\n} while (0)\n\n#ifdef CONFIG_CC_HAS_TSAN_COMPOUND_READ_BEFORE_WRITE\n#define __KCSAN_ACCESS_RW(alt) (KCSAN_ACCESS_COMPOUND | KCSAN_ACCESS_WRITE)\n#else\n#define __KCSAN_ACCESS_RW(alt) (alt)\n#endif\n\n \nstatic void (*access_kernels[2])(void);\n\nstatic struct task_struct **threads;  \nstatic unsigned long end_time;        \n\n \nstatic struct {\n\tspinlock_t lock;\n\tint nlines;\n\tchar lines[3][512];\n} observed = {\n\t.lock = __SPIN_LOCK_UNLOCKED(observed.lock),\n};\n\n \nstatic __no_kcsan inline void\nbegin_test_checks(void (*func1)(void), void (*func2)(void))\n{\n\tkcsan_disable_current();\n\n\t \n\tend_time = jiffies + msecs_to_jiffies(CONFIG_KCSAN_REPORT_ONCE_IN_MS + 500);\n\n\t \n\tsmp_store_release(&access_kernels[0], func1);\n\tsmp_store_release(&access_kernels[1], func2);\n}\n\n \nstatic __no_kcsan inline bool\nend_test_checks(bool stop)\n{\n\tif (!stop && time_before(jiffies, end_time)) {\n\t\t \n\t\tmight_sleep();\n\t\treturn false;\n\t}\n\n\tkcsan_enable_current();\n\treturn true;\n}\n\n \n__no_kcsan\nstatic void probe_console(void *ignore, const char *buf, size_t len)\n{\n\tunsigned long flags;\n\tint nlines;\n\n\t \n\n\tspin_lock_irqsave(&observed.lock, flags);\n\tnlines = observed.nlines;\n\n\tif (strnstr(buf, \"BUG: KCSAN: \", len) && strnstr(buf, \"test_\", len)) {\n\t\t \n\t\tstrscpy(observed.lines[0], buf, min(len + 1, sizeof(observed.lines[0])));\n\t\tnlines = 1;\n\t} else if ((nlines == 1 || nlines == 2) && strnstr(buf, \"bytes by\", len)) {\n\t\tstrscpy(observed.lines[nlines++], buf, min(len + 1, sizeof(observed.lines[0])));\n\n\t\tif (strnstr(buf, \"race at unknown origin\", len)) {\n\t\t\tif (WARN_ON(nlines != 2))\n\t\t\t\tgoto out;\n\n\t\t\t \n\t\t\tstrcpy(observed.lines[nlines++], \"<none>\");\n\t\t}\n\t}\n\nout:\n\tWRITE_ONCE(observed.nlines, nlines);  \n\tspin_unlock_irqrestore(&observed.lock, flags);\n}\n\n \n__no_kcsan\nstatic bool report_available(void)\n{\n\treturn READ_ONCE(observed.nlines) == ARRAY_SIZE(observed.lines);\n}\n\n \nstruct expect_report {\n\t \n\tstruct {\n\t\tvoid *fn;     \n\t\tvoid *addr;   \n\t\tsize_t size;  \n\t\tint type;     \n\t} access[2];\n};\n\n \n__no_kcsan\nstatic bool __report_matches(const struct expect_report *r)\n{\n\tconst bool is_assert = (r->access[0].type | r->access[1].type) & KCSAN_ACCESS_ASSERT;\n\tbool ret = false;\n\tunsigned long flags;\n\ttypeof(*observed.lines) *expect;\n\tconst char *end;\n\tchar *cur;\n\tint i;\n\n\t \n\tif (!report_available())\n\t\treturn false;\n\n\texpect = kmalloc(sizeof(observed.lines), GFP_KERNEL);\n\tif (WARN_ON(!expect))\n\t\treturn false;\n\n\t \n\n\t \n\tcur = expect[0];\n\tend = &expect[0][sizeof(expect[0]) - 1];\n\tcur += scnprintf(cur, end - cur, \"BUG: KCSAN: %s in \",\n\t\t\t is_assert ? \"assert: race\" : \"data-race\");\n\tif (r->access[1].fn) {\n\t\tchar tmp[2][64];\n\t\tint cmp;\n\n\t\t \n\t\tscnprintf(tmp[0], sizeof(tmp[0]), \"%pS\", r->access[0].fn);\n\t\tscnprintf(tmp[1], sizeof(tmp[1]), \"%pS\", r->access[1].fn);\n\t\tcmp = strcmp(tmp[0], tmp[1]);\n\t\tcur += scnprintf(cur, end - cur, \"%ps / %ps\",\n\t\t\t\t cmp < 0 ? r->access[0].fn : r->access[1].fn,\n\t\t\t\t cmp < 0 ? r->access[1].fn : r->access[0].fn);\n\t} else {\n\t\tscnprintf(cur, end - cur, \"%pS\", r->access[0].fn);\n\t\t \n\t\tcur = strchr(expect[0], '+');\n\t\tif (cur)\n\t\t\t*cur = '\\0';\n\t}\n\n\t \n\tcur = expect[1];\n\tend = &expect[1][sizeof(expect[1]) - 1];\n\tif (!r->access[1].fn)\n\t\tcur += scnprintf(cur, end - cur, \"race at unknown origin, with \");\n\n\t \n\tfor (i = 0; i < 2; ++i) {\n\t\tconst int ty = r->access[i].type;\n\t\tconst char *const access_type =\n\t\t\t(ty & KCSAN_ACCESS_ASSERT) ?\n\t\t\t\t      ((ty & KCSAN_ACCESS_WRITE) ?\n\t\t\t\t\t       \"assert no accesses\" :\n\t\t\t\t\t       \"assert no writes\") :\n\t\t\t\t      ((ty & KCSAN_ACCESS_WRITE) ?\n\t\t\t\t\t       ((ty & KCSAN_ACCESS_COMPOUND) ?\n\t\t\t\t\t\t\t\"read-write\" :\n\t\t\t\t\t\t\t\"write\") :\n\t\t\t\t\t       \"read\");\n\t\tconst bool is_atomic = (ty & KCSAN_ACCESS_ATOMIC);\n\t\tconst bool is_scoped = (ty & KCSAN_ACCESS_SCOPED);\n\t\tconst char *const access_type_aux =\n\t\t\t\t(is_atomic && is_scoped)\t? \" (marked, reordered)\"\n\t\t\t\t: (is_atomic\t\t\t? \" (marked)\"\n\t\t\t\t   : (is_scoped\t\t\t? \" (reordered)\" : \"\"));\n\n\t\tif (i == 1) {\n\t\t\t \n\t\t\tcur = expect[2];\n\t\t\tend = &expect[2][sizeof(expect[2]) - 1];\n\n\t\t\tif (!r->access[1].fn) {\n\t\t\t\t \n\t\t\t\tstrcpy(cur, \"<none>\");\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tcur += scnprintf(cur, end - cur, \"%s%s to \", access_type,\n\t\t\t\t access_type_aux);\n\n\t\tif (r->access[i].addr)  \n\t\t\tcur += scnprintf(cur, end - cur, \"0x%px of %zu bytes\",\n\t\t\t\t\t r->access[i].addr, r->access[i].size);\n\t}\n\n\tspin_lock_irqsave(&observed.lock, flags);\n\tif (!report_available())\n\t\tgoto out;  \n\n\t \n\tret = strstr(observed.lines[0], expect[0]) &&\n\t       \n\t      ((strstr(observed.lines[1], expect[1]) &&\n\t\tstrstr(observed.lines[2], expect[2])) ||\n\t       (strstr(observed.lines[1], expect[2]) &&\n\t\tstrstr(observed.lines[2], expect[1])));\nout:\n\tspin_unlock_irqrestore(&observed.lock, flags);\n\tkfree(expect);\n\treturn ret;\n}\n\nstatic __always_inline const struct expect_report *\n__report_set_scoped(struct expect_report *r, int accesses)\n{\n\tBUILD_BUG_ON(accesses > 3);\n\n\tif (accesses & 1)\n\t\tr->access[0].type |= KCSAN_ACCESS_SCOPED;\n\telse\n\t\tr->access[0].type &= ~KCSAN_ACCESS_SCOPED;\n\n\tif (accesses & 2)\n\t\tr->access[1].type |= KCSAN_ACCESS_SCOPED;\n\telse\n\t\tr->access[1].type &= ~KCSAN_ACCESS_SCOPED;\n\n\treturn r;\n}\n\n__no_kcsan\nstatic bool report_matches_any_reordered(struct expect_report *r)\n{\n\treturn __report_matches(__report_set_scoped(r, 0)) ||\n\t       __report_matches(__report_set_scoped(r, 1)) ||\n\t       __report_matches(__report_set_scoped(r, 2)) ||\n\t       __report_matches(__report_set_scoped(r, 3));\n}\n\n#ifdef CONFIG_KCSAN_WEAK_MEMORY\n \n#define report_matches report_matches_any_reordered\n#else\n#define report_matches __report_matches\n#endif\n\n \n\nstatic long test_sink;\nstatic long test_var;\n \nstatic long test_array[3 * PAGE_SIZE / sizeof(long)];\nstatic struct {\n\tlong val[8];\n} test_struct;\nstatic DEFINE_SEQLOCK(test_seqlock);\nstatic DEFINE_SPINLOCK(test_spinlock);\nstatic DEFINE_MUTEX(test_mutex);\n\n \n__no_kcsan\nstatic noinline void sink_value(long v) { WRITE_ONCE(test_sink, v); }\n\n \nstatic noinline void test_delay(int iter)\n{\n\twhile (iter--)\n\t\tsink_value(READ_ONCE(test_sink));\n}\n\nstatic noinline void test_kernel_read(void) { sink_value(test_var); }\n\nstatic noinline void test_kernel_write(void)\n{\n\ttest_var = READ_ONCE_NOCHECK(test_sink) + 1;\n}\n\nstatic noinline void test_kernel_write_nochange(void) { test_var = 42; }\n\n \nstatic noinline void test_kernel_write_nochange_rcu(void) { test_var = 42; }\n\nstatic noinline void test_kernel_read_atomic(void)\n{\n\tsink_value(READ_ONCE(test_var));\n}\n\nstatic noinline void test_kernel_write_atomic(void)\n{\n\tWRITE_ONCE(test_var, READ_ONCE_NOCHECK(test_sink) + 1);\n}\n\nstatic noinline void test_kernel_atomic_rmw(void)\n{\n\t \n\t__atomic_fetch_add(&test_var, 1, __ATOMIC_RELAXED);\n}\n\n__no_kcsan\nstatic noinline void test_kernel_write_uninstrumented(void) { test_var++; }\n\nstatic noinline void test_kernel_data_race(void) { data_race(test_var++); }\n\nstatic noinline void test_kernel_assert_writer(void)\n{\n\tASSERT_EXCLUSIVE_WRITER(test_var);\n}\n\nstatic noinline void test_kernel_assert_access(void)\n{\n\tASSERT_EXCLUSIVE_ACCESS(test_var);\n}\n\n#define TEST_CHANGE_BITS 0xff00ff00\n\nstatic noinline void test_kernel_change_bits(void)\n{\n\tif (IS_ENABLED(CONFIG_KCSAN_IGNORE_ATOMICS)) {\n\t\t \n\t\tkcsan_nestable_atomic_begin();\n\t\ttest_var ^= TEST_CHANGE_BITS;\n\t\tkcsan_nestable_atomic_end();\n\t} else\n\t\tWRITE_ONCE(test_var, READ_ONCE(test_var) ^ TEST_CHANGE_BITS);\n}\n\nstatic noinline void test_kernel_assert_bits_change(void)\n{\n\tASSERT_EXCLUSIVE_BITS(test_var, TEST_CHANGE_BITS);\n}\n\nstatic noinline void test_kernel_assert_bits_nochange(void)\n{\n\tASSERT_EXCLUSIVE_BITS(test_var, ~TEST_CHANGE_BITS);\n}\n\n \nstatic noinline void test_enter_scope(void)\n{\n\tint x = 0;\n\n\t \n\tREAD_ONCE(test_sink);\n\tkcsan_check_read(&x, sizeof(x));\n}\n\nstatic noinline void test_kernel_assert_writer_scoped(void)\n{\n\tASSERT_EXCLUSIVE_WRITER_SCOPED(test_var);\n\ttest_enter_scope();\n}\n\nstatic noinline void test_kernel_assert_access_scoped(void)\n{\n\tASSERT_EXCLUSIVE_ACCESS_SCOPED(test_var);\n\ttest_enter_scope();\n}\n\nstatic noinline void test_kernel_rmw_array(void)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(test_array); ++i)\n\t\ttest_array[i]++;\n}\n\nstatic noinline void test_kernel_write_struct(void)\n{\n\tkcsan_check_write(&test_struct, sizeof(test_struct));\n\tkcsan_disable_current();\n\ttest_struct.val[3]++;  \n\tkcsan_enable_current();\n}\n\nstatic noinline void test_kernel_write_struct_part(void)\n{\n\ttest_struct.val[3] = 42;\n}\n\nstatic noinline void test_kernel_read_struct_zero_size(void)\n{\n\tkcsan_check_read(&test_struct.val[3], 0);\n}\n\nstatic noinline void test_kernel_jiffies_reader(void)\n{\n\tsink_value((long)jiffies);\n}\n\nstatic noinline void test_kernel_seqlock_reader(void)\n{\n\tunsigned int seq;\n\n\tdo {\n\t\tseq = read_seqbegin(&test_seqlock);\n\t\tsink_value(test_var);\n\t} while (read_seqretry(&test_seqlock, seq));\n}\n\nstatic noinline void test_kernel_seqlock_writer(void)\n{\n\tunsigned long flags;\n\n\twrite_seqlock_irqsave(&test_seqlock, flags);\n\ttest_var++;\n\twrite_sequnlock_irqrestore(&test_seqlock, flags);\n}\n\nstatic noinline void test_kernel_atomic_builtins(void)\n{\n\t \n\t__atomic_load_n(&test_var, __ATOMIC_RELAXED);\n}\n\nstatic noinline void test_kernel_xor_1bit(void)\n{\n\t \n\tkcsan_nestable_atomic_begin();\n\ttest_var ^= 0x10000;\n\tkcsan_nestable_atomic_end();\n}\n\n#define TEST_KERNEL_LOCKED(name, acquire, release)\t\t\\\n\tstatic noinline void test_kernel_##name(void)\t\t\\\n\t{\t\t\t\t\t\t\t\\\n\t\tlong *flag = &test_struct.val[0];\t\t\\\n\t\tlong v = 0;\t\t\t\t\t\\\n\t\tif (!(acquire))\t\t\t\t\t\\\n\t\t\treturn;\t\t\t\t\t\\\n\t\twhile (v++ < 100) {\t\t\t\t\\\n\t\t\ttest_var++;\t\t\t\t\\\n\t\t\tbarrier();\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\\\n\t\trelease;\t\t\t\t\t\\\n\t\ttest_delay(10);\t\t\t\t\t\\\n\t}\n\nTEST_KERNEL_LOCKED(with_memorder,\n\t\t   cmpxchg_acquire(flag, 0, 1) == 0,\n\t\t   smp_store_release(flag, 0));\nTEST_KERNEL_LOCKED(wrong_memorder,\n\t\t   cmpxchg_relaxed(flag, 0, 1) == 0,\n\t\t   WRITE_ONCE(*flag, 0));\nTEST_KERNEL_LOCKED(atomic_builtin_with_memorder,\n\t\t   __atomic_compare_exchange_n(flag, &v, 1, 0, __ATOMIC_ACQUIRE, __ATOMIC_RELAXED),\n\t\t   __atomic_store_n(flag, 0, __ATOMIC_RELEASE));\nTEST_KERNEL_LOCKED(atomic_builtin_wrong_memorder,\n\t\t   __atomic_compare_exchange_n(flag, &v, 1, 0, __ATOMIC_RELAXED, __ATOMIC_RELAXED),\n\t\t   __atomic_store_n(flag, 0, __ATOMIC_RELAXED));\n\n \n\n \nstatic void test_barrier_nothreads(struct kunit *test)\n{\n#ifdef CONFIG_KCSAN_WEAK_MEMORY\n\tstruct kcsan_scoped_access *reorder_access = &current->kcsan_ctx.reorder_access;\n#else\n\tstruct kcsan_scoped_access *reorder_access = NULL;\n#endif\n\tarch_spinlock_t arch_spinlock = __ARCH_SPIN_LOCK_UNLOCKED;\n\tatomic_t dummy;\n\n\tKCSAN_TEST_REQUIRES(test, reorder_access != NULL);\n\tKCSAN_TEST_REQUIRES(test, IS_ENABLED(CONFIG_SMP));\n\n#define __KCSAN_EXPECT_BARRIER(access_type, barrier, order_before, name)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\t\t\t\\\n\t\treorder_access->type = (access_type) | KCSAN_ACCESS_SCOPED;\t\t\t\\\n\t\treorder_access->size = sizeof(test_var);\t\t\t\t\t\\\n\t\tbarrier;\t\t\t\t\t\t\t\t\t\\\n\t\tKUNIT_EXPECT_EQ_MSG(test, reorder_access->size,\t\t\t\t\t\\\n\t\t\t\t    order_before ? 0 : sizeof(test_var),\t\t\t\\\n\t\t\t\t    \"improperly instrumented type=(\" #access_type \"): \" name);\t\\\n\t} while (0)\n#define KCSAN_EXPECT_READ_BARRIER(b, o)  __KCSAN_EXPECT_BARRIER(0, b, o, #b)\n#define KCSAN_EXPECT_WRITE_BARRIER(b, o) __KCSAN_EXPECT_BARRIER(KCSAN_ACCESS_WRITE, b, o, #b)\n#define KCSAN_EXPECT_RW_BARRIER(b, o)    __KCSAN_EXPECT_BARRIER(KCSAN_ACCESS_COMPOUND | KCSAN_ACCESS_WRITE, b, o, #b)\n\n\t \n\tspin_lock(&test_spinlock);\n\tspin_unlock(&test_spinlock);\n\tmutex_lock(&test_mutex);\n\tmutex_unlock(&test_mutex);\n\n\t \n\ttest_var = 0;\n\twhile (test_var++ < 1000000 && reorder_access->size != sizeof(test_var))\n\t\t__kcsan_check_read(&test_var, sizeof(test_var));\n\tKUNIT_ASSERT_EQ(test, reorder_access->size, sizeof(test_var));\n\n\tkcsan_nestable_atomic_begin();  \n\n\tKCSAN_EXPECT_READ_BARRIER(mb(), true);\n\tKCSAN_EXPECT_READ_BARRIER(wmb(), false);\n\tKCSAN_EXPECT_READ_BARRIER(rmb(), true);\n\tKCSAN_EXPECT_READ_BARRIER(smp_mb(), true);\n\tKCSAN_EXPECT_READ_BARRIER(smp_wmb(), false);\n\tKCSAN_EXPECT_READ_BARRIER(smp_rmb(), true);\n\tKCSAN_EXPECT_READ_BARRIER(dma_wmb(), false);\n\tKCSAN_EXPECT_READ_BARRIER(dma_rmb(), true);\n\tKCSAN_EXPECT_READ_BARRIER(smp_mb__before_atomic(), true);\n\tKCSAN_EXPECT_READ_BARRIER(smp_mb__after_atomic(), true);\n\tKCSAN_EXPECT_READ_BARRIER(smp_mb__after_spinlock(), true);\n\tKCSAN_EXPECT_READ_BARRIER(smp_store_mb(test_var, 0), true);\n\tKCSAN_EXPECT_READ_BARRIER(smp_load_acquire(&test_var), false);\n\tKCSAN_EXPECT_READ_BARRIER(smp_store_release(&test_var, 0), true);\n\tKCSAN_EXPECT_READ_BARRIER(xchg(&test_var, 0), true);\n\tKCSAN_EXPECT_READ_BARRIER(xchg_release(&test_var, 0), true);\n\tKCSAN_EXPECT_READ_BARRIER(xchg_relaxed(&test_var, 0), false);\n\tKCSAN_EXPECT_READ_BARRIER(cmpxchg(&test_var, 0,  0), true);\n\tKCSAN_EXPECT_READ_BARRIER(cmpxchg_release(&test_var, 0,  0), true);\n\tKCSAN_EXPECT_READ_BARRIER(cmpxchg_relaxed(&test_var, 0,  0), false);\n\tKCSAN_EXPECT_READ_BARRIER(atomic_read(&dummy), false);\n\tKCSAN_EXPECT_READ_BARRIER(atomic_read_acquire(&dummy), false);\n\tKCSAN_EXPECT_READ_BARRIER(atomic_set(&dummy, 0), false);\n\tKCSAN_EXPECT_READ_BARRIER(atomic_set_release(&dummy, 0), true);\n\tKCSAN_EXPECT_READ_BARRIER(atomic_add(1, &dummy), false);\n\tKCSAN_EXPECT_READ_BARRIER(atomic_add_return(1, &dummy), true);\n\tKCSAN_EXPECT_READ_BARRIER(atomic_add_return_acquire(1, &dummy), false);\n\tKCSAN_EXPECT_READ_BARRIER(atomic_add_return_release(1, &dummy), true);\n\tKCSAN_EXPECT_READ_BARRIER(atomic_add_return_relaxed(1, &dummy), false);\n\tKCSAN_EXPECT_READ_BARRIER(atomic_fetch_add(1, &dummy), true);\n\tKCSAN_EXPECT_READ_BARRIER(atomic_fetch_add_acquire(1, &dummy), false);\n\tKCSAN_EXPECT_READ_BARRIER(atomic_fetch_add_release(1, &dummy), true);\n\tKCSAN_EXPECT_READ_BARRIER(atomic_fetch_add_relaxed(1, &dummy), false);\n\tKCSAN_EXPECT_READ_BARRIER(test_and_set_bit(0, &test_var), true);\n\tKCSAN_EXPECT_READ_BARRIER(test_and_clear_bit(0, &test_var), true);\n\tKCSAN_EXPECT_READ_BARRIER(test_and_change_bit(0, &test_var), true);\n\tKCSAN_EXPECT_READ_BARRIER(clear_bit_unlock(0, &test_var), true);\n\tKCSAN_EXPECT_READ_BARRIER(__clear_bit_unlock(0, &test_var), true);\n\tKCSAN_EXPECT_READ_BARRIER(arch_spin_lock(&arch_spinlock), false);\n\tKCSAN_EXPECT_READ_BARRIER(arch_spin_unlock(&arch_spinlock), true);\n\tKCSAN_EXPECT_READ_BARRIER(spin_lock(&test_spinlock), false);\n\tKCSAN_EXPECT_READ_BARRIER(spin_unlock(&test_spinlock), true);\n\tKCSAN_EXPECT_READ_BARRIER(mutex_lock(&test_mutex), false);\n\tKCSAN_EXPECT_READ_BARRIER(mutex_unlock(&test_mutex), true);\n\n\tKCSAN_EXPECT_WRITE_BARRIER(mb(), true);\n\tKCSAN_EXPECT_WRITE_BARRIER(wmb(), true);\n\tKCSAN_EXPECT_WRITE_BARRIER(rmb(), false);\n\tKCSAN_EXPECT_WRITE_BARRIER(smp_mb(), true);\n\tKCSAN_EXPECT_WRITE_BARRIER(smp_wmb(), true);\n\tKCSAN_EXPECT_WRITE_BARRIER(smp_rmb(), false);\n\tKCSAN_EXPECT_WRITE_BARRIER(dma_wmb(), true);\n\tKCSAN_EXPECT_WRITE_BARRIER(dma_rmb(), false);\n\tKCSAN_EXPECT_WRITE_BARRIER(smp_mb__before_atomic(), true);\n\tKCSAN_EXPECT_WRITE_BARRIER(smp_mb__after_atomic(), true);\n\tKCSAN_EXPECT_WRITE_BARRIER(smp_mb__after_spinlock(), true);\n\tKCSAN_EXPECT_WRITE_BARRIER(smp_store_mb(test_var, 0), true);\n\tKCSAN_EXPECT_WRITE_BARRIER(smp_load_acquire(&test_var), false);\n\tKCSAN_EXPECT_WRITE_BARRIER(smp_store_release(&test_var, 0), true);\n\tKCSAN_EXPECT_WRITE_BARRIER(xchg(&test_var, 0), true);\n\tKCSAN_EXPECT_WRITE_BARRIER(xchg_release(&test_var, 0), true);\n\tKCSAN_EXPECT_WRITE_BARRIER(xchg_relaxed(&test_var, 0), false);\n\tKCSAN_EXPECT_WRITE_BARRIER(cmpxchg(&test_var, 0,  0), true);\n\tKCSAN_EXPECT_WRITE_BARRIER(cmpxchg_release(&test_var, 0,  0), true);\n\tKCSAN_EXPECT_WRITE_BARRIER(cmpxchg_relaxed(&test_var, 0,  0), false);\n\tKCSAN_EXPECT_WRITE_BARRIER(atomic_read(&dummy), false);\n\tKCSAN_EXPECT_WRITE_BARRIER(atomic_read_acquire(&dummy), false);\n\tKCSAN_EXPECT_WRITE_BARRIER(atomic_set(&dummy, 0), false);\n\tKCSAN_EXPECT_WRITE_BARRIER(atomic_set_release(&dummy, 0), true);\n\tKCSAN_EXPECT_WRITE_BARRIER(atomic_add(1, &dummy), false);\n\tKCSAN_EXPECT_WRITE_BARRIER(atomic_add_return(1, &dummy), true);\n\tKCSAN_EXPECT_WRITE_BARRIER(atomic_add_return_acquire(1, &dummy), false);\n\tKCSAN_EXPECT_WRITE_BARRIER(atomic_add_return_release(1, &dummy), true);\n\tKCSAN_EXPECT_WRITE_BARRIER(atomic_add_return_relaxed(1, &dummy), false);\n\tKCSAN_EXPECT_WRITE_BARRIER(atomic_fetch_add(1, &dummy), true);\n\tKCSAN_EXPECT_WRITE_BARRIER(atomic_fetch_add_acquire(1, &dummy), false);\n\tKCSAN_EXPECT_WRITE_BARRIER(atomic_fetch_add_release(1, &dummy), true);\n\tKCSAN_EXPECT_WRITE_BARRIER(atomic_fetch_add_relaxed(1, &dummy), false);\n\tKCSAN_EXPECT_WRITE_BARRIER(test_and_set_bit(0, &test_var), true);\n\tKCSAN_EXPECT_WRITE_BARRIER(test_and_clear_bit(0, &test_var), true);\n\tKCSAN_EXPECT_WRITE_BARRIER(test_and_change_bit(0, &test_var), true);\n\tKCSAN_EXPECT_WRITE_BARRIER(clear_bit_unlock(0, &test_var), true);\n\tKCSAN_EXPECT_WRITE_BARRIER(__clear_bit_unlock(0, &test_var), true);\n\tKCSAN_EXPECT_WRITE_BARRIER(arch_spin_lock(&arch_spinlock), false);\n\tKCSAN_EXPECT_WRITE_BARRIER(arch_spin_unlock(&arch_spinlock), true);\n\tKCSAN_EXPECT_WRITE_BARRIER(spin_lock(&test_spinlock), false);\n\tKCSAN_EXPECT_WRITE_BARRIER(spin_unlock(&test_spinlock), true);\n\tKCSAN_EXPECT_WRITE_BARRIER(mutex_lock(&test_mutex), false);\n\tKCSAN_EXPECT_WRITE_BARRIER(mutex_unlock(&test_mutex), true);\n\n\tKCSAN_EXPECT_RW_BARRIER(mb(), true);\n\tKCSAN_EXPECT_RW_BARRIER(wmb(), true);\n\tKCSAN_EXPECT_RW_BARRIER(rmb(), true);\n\tKCSAN_EXPECT_RW_BARRIER(smp_mb(), true);\n\tKCSAN_EXPECT_RW_BARRIER(smp_wmb(), true);\n\tKCSAN_EXPECT_RW_BARRIER(smp_rmb(), true);\n\tKCSAN_EXPECT_RW_BARRIER(dma_wmb(), true);\n\tKCSAN_EXPECT_RW_BARRIER(dma_rmb(), true);\n\tKCSAN_EXPECT_RW_BARRIER(smp_mb__before_atomic(), true);\n\tKCSAN_EXPECT_RW_BARRIER(smp_mb__after_atomic(), true);\n\tKCSAN_EXPECT_RW_BARRIER(smp_mb__after_spinlock(), true);\n\tKCSAN_EXPECT_RW_BARRIER(smp_store_mb(test_var, 0), true);\n\tKCSAN_EXPECT_RW_BARRIER(smp_load_acquire(&test_var), false);\n\tKCSAN_EXPECT_RW_BARRIER(smp_store_release(&test_var, 0), true);\n\tKCSAN_EXPECT_RW_BARRIER(xchg(&test_var, 0), true);\n\tKCSAN_EXPECT_RW_BARRIER(xchg_release(&test_var, 0), true);\n\tKCSAN_EXPECT_RW_BARRIER(xchg_relaxed(&test_var, 0), false);\n\tKCSAN_EXPECT_RW_BARRIER(cmpxchg(&test_var, 0,  0), true);\n\tKCSAN_EXPECT_RW_BARRIER(cmpxchg_release(&test_var, 0,  0), true);\n\tKCSAN_EXPECT_RW_BARRIER(cmpxchg_relaxed(&test_var, 0,  0), false);\n\tKCSAN_EXPECT_RW_BARRIER(atomic_read(&dummy), false);\n\tKCSAN_EXPECT_RW_BARRIER(atomic_read_acquire(&dummy), false);\n\tKCSAN_EXPECT_RW_BARRIER(atomic_set(&dummy, 0), false);\n\tKCSAN_EXPECT_RW_BARRIER(atomic_set_release(&dummy, 0), true);\n\tKCSAN_EXPECT_RW_BARRIER(atomic_add(1, &dummy), false);\n\tKCSAN_EXPECT_RW_BARRIER(atomic_add_return(1, &dummy), true);\n\tKCSAN_EXPECT_RW_BARRIER(atomic_add_return_acquire(1, &dummy), false);\n\tKCSAN_EXPECT_RW_BARRIER(atomic_add_return_release(1, &dummy), true);\n\tKCSAN_EXPECT_RW_BARRIER(atomic_add_return_relaxed(1, &dummy), false);\n\tKCSAN_EXPECT_RW_BARRIER(atomic_fetch_add(1, &dummy), true);\n\tKCSAN_EXPECT_RW_BARRIER(atomic_fetch_add_acquire(1, &dummy), false);\n\tKCSAN_EXPECT_RW_BARRIER(atomic_fetch_add_release(1, &dummy), true);\n\tKCSAN_EXPECT_RW_BARRIER(atomic_fetch_add_relaxed(1, &dummy), false);\n\tKCSAN_EXPECT_RW_BARRIER(test_and_set_bit(0, &test_var), true);\n\tKCSAN_EXPECT_RW_BARRIER(test_and_clear_bit(0, &test_var), true);\n\tKCSAN_EXPECT_RW_BARRIER(test_and_change_bit(0, &test_var), true);\n\tKCSAN_EXPECT_RW_BARRIER(clear_bit_unlock(0, &test_var), true);\n\tKCSAN_EXPECT_RW_BARRIER(__clear_bit_unlock(0, &test_var), true);\n\tKCSAN_EXPECT_RW_BARRIER(arch_spin_lock(&arch_spinlock), false);\n\tKCSAN_EXPECT_RW_BARRIER(arch_spin_unlock(&arch_spinlock), true);\n\tKCSAN_EXPECT_RW_BARRIER(spin_lock(&test_spinlock), false);\n\tKCSAN_EXPECT_RW_BARRIER(spin_unlock(&test_spinlock), true);\n\tKCSAN_EXPECT_RW_BARRIER(mutex_lock(&test_mutex), false);\n\tKCSAN_EXPECT_RW_BARRIER(mutex_unlock(&test_mutex), true);\n\n#ifdef clear_bit_unlock_is_negative_byte\n\tKCSAN_EXPECT_READ_BARRIER(clear_bit_unlock_is_negative_byte(0, &test_var), true);\n\tKCSAN_EXPECT_WRITE_BARRIER(clear_bit_unlock_is_negative_byte(0, &test_var), true);\n\tKCSAN_EXPECT_RW_BARRIER(clear_bit_unlock_is_negative_byte(0, &test_var), true);\n#endif\n\tkcsan_nestable_atomic_end();\n}\n\n \n__no_kcsan\nstatic void test_basic(struct kunit *test)\n{\n\tstruct expect_report expect = {\n\t\t.access = {\n\t\t\t{ test_kernel_write, &test_var, sizeof(test_var), KCSAN_ACCESS_WRITE },\n\t\t\t{ test_kernel_read, &test_var, sizeof(test_var), 0 },\n\t\t},\n\t};\n\tstruct expect_report never = {\n\t\t.access = {\n\t\t\t{ test_kernel_read, &test_var, sizeof(test_var), 0 },\n\t\t\t{ test_kernel_read, &test_var, sizeof(test_var), 0 },\n\t\t},\n\t};\n\tbool match_expect = false;\n\tbool match_never = false;\n\n\tbegin_test_checks(test_kernel_write, test_kernel_read);\n\tdo {\n\t\tmatch_expect |= report_matches(&expect);\n\t\tmatch_never = report_matches(&never);\n\t} while (!end_test_checks(match_never));\n\tKUNIT_EXPECT_TRUE(test, match_expect);\n\tKUNIT_EXPECT_FALSE(test, match_never);\n}\n\n \n__no_kcsan\nstatic void test_concurrent_races(struct kunit *test)\n{\n\tstruct expect_report expect = {\n\t\t.access = {\n\t\t\t \n\t\t\t{ test_kernel_rmw_array, NULL, 0, __KCSAN_ACCESS_RW(KCSAN_ACCESS_WRITE) },\n\t\t\t{ test_kernel_rmw_array, NULL, 0, __KCSAN_ACCESS_RW(0) },\n\t\t},\n\t};\n\tstruct expect_report never = {\n\t\t.access = {\n\t\t\t{ test_kernel_rmw_array, NULL, 0, 0 },\n\t\t\t{ test_kernel_rmw_array, NULL, 0, 0 },\n\t\t},\n\t};\n\tbool match_expect = false;\n\tbool match_never = false;\n\n\tbegin_test_checks(test_kernel_rmw_array, test_kernel_rmw_array);\n\tdo {\n\t\tmatch_expect |= report_matches(&expect);\n\t\tmatch_never |= report_matches(&never);\n\t} while (!end_test_checks(false));\n\tKUNIT_EXPECT_TRUE(test, match_expect);  \n\tKUNIT_EXPECT_FALSE(test, match_never);\n}\n\n \n__no_kcsan\nstatic void test_novalue_change(struct kunit *test)\n{\n\tstruct expect_report expect_rw = {\n\t\t.access = {\n\t\t\t{ test_kernel_write_nochange, &test_var, sizeof(test_var), KCSAN_ACCESS_WRITE },\n\t\t\t{ test_kernel_read, &test_var, sizeof(test_var), 0 },\n\t\t},\n\t};\n\tstruct expect_report expect_ww = {\n\t\t.access = {\n\t\t\t{ test_kernel_write_nochange, &test_var, sizeof(test_var), KCSAN_ACCESS_WRITE },\n\t\t\t{ test_kernel_write_nochange, &test_var, sizeof(test_var), KCSAN_ACCESS_WRITE },\n\t\t},\n\t};\n\tbool match_expect = false;\n\n\ttest_kernel_write_nochange();  \n\tbegin_test_checks(test_kernel_write_nochange, test_kernel_read);\n\tdo {\n\t\tmatch_expect = report_matches(&expect_rw) || report_matches(&expect_ww);\n\t} while (!end_test_checks(match_expect));\n\tif (IS_ENABLED(CONFIG_KCSAN_REPORT_VALUE_CHANGE_ONLY))\n\t\tKUNIT_EXPECT_FALSE(test, match_expect);\n\telse\n\t\tKUNIT_EXPECT_TRUE(test, match_expect);\n}\n\n \n__no_kcsan\nstatic void test_novalue_change_exception(struct kunit *test)\n{\n\tstruct expect_report expect_rw = {\n\t\t.access = {\n\t\t\t{ test_kernel_write_nochange_rcu, &test_var, sizeof(test_var), KCSAN_ACCESS_WRITE },\n\t\t\t{ test_kernel_read, &test_var, sizeof(test_var), 0 },\n\t\t},\n\t};\n\tstruct expect_report expect_ww = {\n\t\t.access = {\n\t\t\t{ test_kernel_write_nochange_rcu, &test_var, sizeof(test_var), KCSAN_ACCESS_WRITE },\n\t\t\t{ test_kernel_write_nochange_rcu, &test_var, sizeof(test_var), KCSAN_ACCESS_WRITE },\n\t\t},\n\t};\n\tbool match_expect = false;\n\n\ttest_kernel_write_nochange_rcu();  \n\tbegin_test_checks(test_kernel_write_nochange_rcu, test_kernel_read);\n\tdo {\n\t\tmatch_expect = report_matches(&expect_rw) || report_matches(&expect_ww);\n\t} while (!end_test_checks(match_expect));\n\tKUNIT_EXPECT_TRUE(test, match_expect);\n}\n\n \n__no_kcsan\nstatic void test_unknown_origin(struct kunit *test)\n{\n\tstruct expect_report expect = {\n\t\t.access = {\n\t\t\t{ test_kernel_read, &test_var, sizeof(test_var), 0 },\n\t\t\t{ NULL },\n\t\t},\n\t};\n\tbool match_expect = false;\n\n\tbegin_test_checks(test_kernel_write_uninstrumented, test_kernel_read);\n\tdo {\n\t\tmatch_expect = report_matches(&expect);\n\t} while (!end_test_checks(match_expect));\n\tif (IS_ENABLED(CONFIG_KCSAN_REPORT_RACE_UNKNOWN_ORIGIN))\n\t\tKUNIT_EXPECT_TRUE(test, match_expect);\n\telse\n\t\tKUNIT_EXPECT_FALSE(test, match_expect);\n}\n\n \n__no_kcsan\nstatic void test_write_write_assume_atomic(struct kunit *test)\n{\n\tstruct expect_report expect = {\n\t\t.access = {\n\t\t\t{ test_kernel_write, &test_var, sizeof(test_var), KCSAN_ACCESS_WRITE },\n\t\t\t{ test_kernel_write, &test_var, sizeof(test_var), KCSAN_ACCESS_WRITE },\n\t\t},\n\t};\n\tbool match_expect = false;\n\n\tbegin_test_checks(test_kernel_write, test_kernel_write);\n\tdo {\n\t\tsink_value(READ_ONCE(test_var));  \n\t\tmatch_expect = report_matches(&expect);\n\t} while (!end_test_checks(match_expect));\n\tif (IS_ENABLED(CONFIG_KCSAN_ASSUME_PLAIN_WRITES_ATOMIC))\n\t\tKUNIT_EXPECT_FALSE(test, match_expect);\n\telse\n\t\tKUNIT_EXPECT_TRUE(test, match_expect);\n}\n\n \n__no_kcsan\nstatic void test_write_write_struct(struct kunit *test)\n{\n\tstruct expect_report expect = {\n\t\t.access = {\n\t\t\t{ test_kernel_write_struct, &test_struct, sizeof(test_struct), KCSAN_ACCESS_WRITE },\n\t\t\t{ test_kernel_write_struct, &test_struct, sizeof(test_struct), KCSAN_ACCESS_WRITE },\n\t\t},\n\t};\n\tbool match_expect = false;\n\n\tbegin_test_checks(test_kernel_write_struct, test_kernel_write_struct);\n\tdo {\n\t\tmatch_expect = report_matches(&expect);\n\t} while (!end_test_checks(match_expect));\n\tKUNIT_EXPECT_TRUE(test, match_expect);\n}\n\n \n__no_kcsan\nstatic void test_write_write_struct_part(struct kunit *test)\n{\n\tstruct expect_report expect = {\n\t\t.access = {\n\t\t\t{ test_kernel_write_struct, &test_struct, sizeof(test_struct), KCSAN_ACCESS_WRITE },\n\t\t\t{ test_kernel_write_struct_part, &test_struct.val[3], sizeof(test_struct.val[3]), KCSAN_ACCESS_WRITE },\n\t\t},\n\t};\n\tbool match_expect = false;\n\n\tbegin_test_checks(test_kernel_write_struct, test_kernel_write_struct_part);\n\tdo {\n\t\tmatch_expect = report_matches(&expect);\n\t} while (!end_test_checks(match_expect));\n\tKUNIT_EXPECT_TRUE(test, match_expect);\n}\n\n \n__no_kcsan\nstatic void test_read_atomic_write_atomic(struct kunit *test)\n{\n\tbool match_never = false;\n\n\tbegin_test_checks(test_kernel_read_atomic, test_kernel_write_atomic);\n\tdo {\n\t\tmatch_never = report_available();\n\t} while (!end_test_checks(match_never));\n\tKUNIT_EXPECT_FALSE(test, match_never);\n}\n\n \n__no_kcsan\nstatic void test_read_plain_atomic_write(struct kunit *test)\n{\n\tstruct expect_report expect = {\n\t\t.access = {\n\t\t\t{ test_kernel_read, &test_var, sizeof(test_var), 0 },\n\t\t\t{ test_kernel_write_atomic, &test_var, sizeof(test_var), KCSAN_ACCESS_WRITE | KCSAN_ACCESS_ATOMIC },\n\t\t},\n\t};\n\tbool match_expect = false;\n\n\tKCSAN_TEST_REQUIRES(test, !IS_ENABLED(CONFIG_KCSAN_IGNORE_ATOMICS));\n\n\tbegin_test_checks(test_kernel_read, test_kernel_write_atomic);\n\tdo {\n\t\tmatch_expect = report_matches(&expect);\n\t} while (!end_test_checks(match_expect));\n\tKUNIT_EXPECT_TRUE(test, match_expect);\n}\n\n \n__no_kcsan\nstatic void test_read_plain_atomic_rmw(struct kunit *test)\n{\n\tstruct expect_report expect = {\n\t\t.access = {\n\t\t\t{ test_kernel_read, &test_var, sizeof(test_var), 0 },\n\t\t\t{ test_kernel_atomic_rmw, &test_var, sizeof(test_var),\n\t\t\t\tKCSAN_ACCESS_COMPOUND | KCSAN_ACCESS_WRITE | KCSAN_ACCESS_ATOMIC },\n\t\t},\n\t};\n\tbool match_expect = false;\n\n\tKCSAN_TEST_REQUIRES(test, !IS_ENABLED(CONFIG_KCSAN_IGNORE_ATOMICS));\n\n\tbegin_test_checks(test_kernel_read, test_kernel_atomic_rmw);\n\tdo {\n\t\tmatch_expect = report_matches(&expect);\n\t} while (!end_test_checks(match_expect));\n\tKUNIT_EXPECT_TRUE(test, match_expect);\n}\n\n \n__no_kcsan\nstatic void test_zero_size_access(struct kunit *test)\n{\n\tstruct expect_report expect = {\n\t\t.access = {\n\t\t\t{ test_kernel_write_struct, &test_struct, sizeof(test_struct), KCSAN_ACCESS_WRITE },\n\t\t\t{ test_kernel_write_struct, &test_struct, sizeof(test_struct), KCSAN_ACCESS_WRITE },\n\t\t},\n\t};\n\tstruct expect_report never = {\n\t\t.access = {\n\t\t\t{ test_kernel_write_struct, &test_struct, sizeof(test_struct), KCSAN_ACCESS_WRITE },\n\t\t\t{ test_kernel_read_struct_zero_size, &test_struct.val[3], 0, 0 },\n\t\t},\n\t};\n\tbool match_expect = false;\n\tbool match_never = false;\n\n\tbegin_test_checks(test_kernel_write_struct, test_kernel_read_struct_zero_size);\n\tdo {\n\t\tmatch_expect |= report_matches(&expect);\n\t\tmatch_never = report_matches(&never);\n\t} while (!end_test_checks(match_never));\n\tKUNIT_EXPECT_TRUE(test, match_expect);  \n\tKUNIT_EXPECT_FALSE(test, match_never);\n}\n\n \n__no_kcsan\nstatic void test_data_race(struct kunit *test)\n{\n\tbool match_never = false;\n\n\tbegin_test_checks(test_kernel_data_race, test_kernel_data_race);\n\tdo {\n\t\tmatch_never = report_available();\n\t} while (!end_test_checks(match_never));\n\tKUNIT_EXPECT_FALSE(test, match_never);\n}\n\n__no_kcsan\nstatic void test_assert_exclusive_writer(struct kunit *test)\n{\n\tstruct expect_report expect = {\n\t\t.access = {\n\t\t\t{ test_kernel_assert_writer, &test_var, sizeof(test_var), KCSAN_ACCESS_ASSERT },\n\t\t\t{ test_kernel_write_nochange, &test_var, sizeof(test_var), KCSAN_ACCESS_WRITE },\n\t\t},\n\t};\n\tbool match_expect = false;\n\n\tbegin_test_checks(test_kernel_assert_writer, test_kernel_write_nochange);\n\tdo {\n\t\tmatch_expect = report_matches(&expect);\n\t} while (!end_test_checks(match_expect));\n\tKUNIT_EXPECT_TRUE(test, match_expect);\n}\n\n__no_kcsan\nstatic void test_assert_exclusive_access(struct kunit *test)\n{\n\tstruct expect_report expect = {\n\t\t.access = {\n\t\t\t{ test_kernel_assert_access, &test_var, sizeof(test_var), KCSAN_ACCESS_ASSERT | KCSAN_ACCESS_WRITE },\n\t\t\t{ test_kernel_read, &test_var, sizeof(test_var), 0 },\n\t\t},\n\t};\n\tbool match_expect = false;\n\n\tbegin_test_checks(test_kernel_assert_access, test_kernel_read);\n\tdo {\n\t\tmatch_expect = report_matches(&expect);\n\t} while (!end_test_checks(match_expect));\n\tKUNIT_EXPECT_TRUE(test, match_expect);\n}\n\n__no_kcsan\nstatic void test_assert_exclusive_access_writer(struct kunit *test)\n{\n\tstruct expect_report expect_access_writer = {\n\t\t.access = {\n\t\t\t{ test_kernel_assert_access, &test_var, sizeof(test_var), KCSAN_ACCESS_ASSERT | KCSAN_ACCESS_WRITE },\n\t\t\t{ test_kernel_assert_writer, &test_var, sizeof(test_var), KCSAN_ACCESS_ASSERT },\n\t\t},\n\t};\n\tstruct expect_report expect_access_access = {\n\t\t.access = {\n\t\t\t{ test_kernel_assert_access, &test_var, sizeof(test_var), KCSAN_ACCESS_ASSERT | KCSAN_ACCESS_WRITE },\n\t\t\t{ test_kernel_assert_access, &test_var, sizeof(test_var), KCSAN_ACCESS_ASSERT | KCSAN_ACCESS_WRITE },\n\t\t},\n\t};\n\tstruct expect_report never = {\n\t\t.access = {\n\t\t\t{ test_kernel_assert_writer, &test_var, sizeof(test_var), KCSAN_ACCESS_ASSERT },\n\t\t\t{ test_kernel_assert_writer, &test_var, sizeof(test_var), KCSAN_ACCESS_ASSERT },\n\t\t},\n\t};\n\tbool match_expect_access_writer = false;\n\tbool match_expect_access_access = false;\n\tbool match_never = false;\n\n\tbegin_test_checks(test_kernel_assert_access, test_kernel_assert_writer);\n\tdo {\n\t\tmatch_expect_access_writer |= report_matches(&expect_access_writer);\n\t\tmatch_expect_access_access |= report_matches(&expect_access_access);\n\t\tmatch_never |= report_matches(&never);\n\t} while (!end_test_checks(match_never));\n\tKUNIT_EXPECT_TRUE(test, match_expect_access_writer);\n\tKUNIT_EXPECT_TRUE(test, match_expect_access_access);\n\tKUNIT_EXPECT_FALSE(test, match_never);\n}\n\n__no_kcsan\nstatic void test_assert_exclusive_bits_change(struct kunit *test)\n{\n\tstruct expect_report expect = {\n\t\t.access = {\n\t\t\t{ test_kernel_assert_bits_change, &test_var, sizeof(test_var), KCSAN_ACCESS_ASSERT },\n\t\t\t{ test_kernel_change_bits, &test_var, sizeof(test_var),\n\t\t\t\tKCSAN_ACCESS_WRITE | (IS_ENABLED(CONFIG_KCSAN_IGNORE_ATOMICS) ? 0 : KCSAN_ACCESS_ATOMIC) },\n\t\t},\n\t};\n\tbool match_expect = false;\n\n\tbegin_test_checks(test_kernel_assert_bits_change, test_kernel_change_bits);\n\tdo {\n\t\tmatch_expect = report_matches(&expect);\n\t} while (!end_test_checks(match_expect));\n\tKUNIT_EXPECT_TRUE(test, match_expect);\n}\n\n__no_kcsan\nstatic void test_assert_exclusive_bits_nochange(struct kunit *test)\n{\n\tbool match_never = false;\n\n\tbegin_test_checks(test_kernel_assert_bits_nochange, test_kernel_change_bits);\n\tdo {\n\t\tmatch_never = report_available();\n\t} while (!end_test_checks(match_never));\n\tKUNIT_EXPECT_FALSE(test, match_never);\n}\n\n__no_kcsan\nstatic void test_assert_exclusive_writer_scoped(struct kunit *test)\n{\n\tstruct expect_report expect_start = {\n\t\t.access = {\n\t\t\t{ test_kernel_assert_writer_scoped, &test_var, sizeof(test_var), KCSAN_ACCESS_ASSERT | KCSAN_ACCESS_SCOPED },\n\t\t\t{ test_kernel_write_nochange, &test_var, sizeof(test_var), KCSAN_ACCESS_WRITE },\n\t\t},\n\t};\n\tstruct expect_report expect_inscope = {\n\t\t.access = {\n\t\t\t{ test_enter_scope, &test_var, sizeof(test_var), KCSAN_ACCESS_ASSERT | KCSAN_ACCESS_SCOPED },\n\t\t\t{ test_kernel_write_nochange, &test_var, sizeof(test_var), KCSAN_ACCESS_WRITE },\n\t\t},\n\t};\n\tbool match_expect_start = false;\n\tbool match_expect_inscope = false;\n\n\tbegin_test_checks(test_kernel_assert_writer_scoped, test_kernel_write_nochange);\n\tdo {\n\t\tmatch_expect_start |= report_matches(&expect_start);\n\t\tmatch_expect_inscope |= report_matches(&expect_inscope);\n\t} while (!end_test_checks(match_expect_inscope));\n\tKUNIT_EXPECT_TRUE(test, match_expect_start);\n\tKUNIT_EXPECT_FALSE(test, match_expect_inscope);\n}\n\n__no_kcsan\nstatic void test_assert_exclusive_access_scoped(struct kunit *test)\n{\n\tstruct expect_report expect_start1 = {\n\t\t.access = {\n\t\t\t{ test_kernel_assert_access_scoped, &test_var, sizeof(test_var), KCSAN_ACCESS_ASSERT | KCSAN_ACCESS_WRITE | KCSAN_ACCESS_SCOPED },\n\t\t\t{ test_kernel_read, &test_var, sizeof(test_var), 0 },\n\t\t},\n\t};\n\tstruct expect_report expect_start2 = {\n\t\t.access = { expect_start1.access[0], expect_start1.access[0] },\n\t};\n\tstruct expect_report expect_inscope = {\n\t\t.access = {\n\t\t\t{ test_enter_scope, &test_var, sizeof(test_var), KCSAN_ACCESS_ASSERT | KCSAN_ACCESS_WRITE | KCSAN_ACCESS_SCOPED },\n\t\t\t{ test_kernel_read, &test_var, sizeof(test_var), 0 },\n\t\t},\n\t};\n\tbool match_expect_start = false;\n\tbool match_expect_inscope = false;\n\n\tbegin_test_checks(test_kernel_assert_access_scoped, test_kernel_read);\n\tend_time += msecs_to_jiffies(1000);  \n\tdo {\n\t\tmatch_expect_start |= report_matches(&expect_start1) || report_matches(&expect_start2);\n\t\tmatch_expect_inscope |= report_matches(&expect_inscope);\n\t} while (!end_test_checks(match_expect_inscope));\n\tKUNIT_EXPECT_TRUE(test, match_expect_start);\n\tKUNIT_EXPECT_FALSE(test, match_expect_inscope);\n}\n\n \n__no_kcsan\nstatic void test_jiffies_noreport(struct kunit *test)\n{\n\tbool match_never = false;\n\n\tbegin_test_checks(test_kernel_jiffies_reader, test_kernel_jiffies_reader);\n\tdo {\n\t\tmatch_never = report_available();\n\t} while (!end_test_checks(match_never));\n\tKUNIT_EXPECT_FALSE(test, match_never);\n}\n\n \n__no_kcsan\nstatic void test_seqlock_noreport(struct kunit *test)\n{\n\tbool match_never = false;\n\n\tbegin_test_checks(test_kernel_seqlock_reader, test_kernel_seqlock_writer);\n\tdo {\n\t\tmatch_never = report_available();\n\t} while (!end_test_checks(match_never));\n\tKUNIT_EXPECT_FALSE(test, match_never);\n}\n\n \nstatic void test_atomic_builtins(struct kunit *test)\n{\n\tbool match_never = false;\n\n\tbegin_test_checks(test_kernel_atomic_builtins, test_kernel_atomic_builtins);\n\tdo {\n\t\tlong tmp;\n\n\t\tkcsan_enable_current();\n\n\t\t__atomic_store_n(&test_var, 42L, __ATOMIC_RELAXED);\n\t\tKUNIT_EXPECT_EQ(test, 42L, __atomic_load_n(&test_var, __ATOMIC_RELAXED));\n\n\t\tKUNIT_EXPECT_EQ(test, 42L, __atomic_exchange_n(&test_var, 20, __ATOMIC_RELAXED));\n\t\tKUNIT_EXPECT_EQ(test, 20L, test_var);\n\n\t\ttmp = 20L;\n\t\tKUNIT_EXPECT_TRUE(test, __atomic_compare_exchange_n(&test_var, &tmp, 30L,\n\t\t\t\t\t\t\t\t    0, __ATOMIC_RELAXED,\n\t\t\t\t\t\t\t\t    __ATOMIC_RELAXED));\n\t\tKUNIT_EXPECT_EQ(test, tmp, 20L);\n\t\tKUNIT_EXPECT_EQ(test, test_var, 30L);\n\t\tKUNIT_EXPECT_FALSE(test, __atomic_compare_exchange_n(&test_var, &tmp, 40L,\n\t\t\t\t\t\t\t\t     1, __ATOMIC_RELAXED,\n\t\t\t\t\t\t\t\t     __ATOMIC_RELAXED));\n\t\tKUNIT_EXPECT_EQ(test, tmp, 30L);\n\t\tKUNIT_EXPECT_EQ(test, test_var, 30L);\n\n\t\tKUNIT_EXPECT_EQ(test, 30L, __atomic_fetch_add(&test_var, 1, __ATOMIC_RELAXED));\n\t\tKUNIT_EXPECT_EQ(test, 31L, __atomic_fetch_sub(&test_var, 1, __ATOMIC_RELAXED));\n\t\tKUNIT_EXPECT_EQ(test, 30L, __atomic_fetch_and(&test_var, 0xf, __ATOMIC_RELAXED));\n\t\tKUNIT_EXPECT_EQ(test, 14L, __atomic_fetch_xor(&test_var, 0xf, __ATOMIC_RELAXED));\n\t\tKUNIT_EXPECT_EQ(test, 1L, __atomic_fetch_or(&test_var, 0xf0, __ATOMIC_RELAXED));\n\t\tKUNIT_EXPECT_EQ(test, 241L, __atomic_fetch_nand(&test_var, 0xf, __ATOMIC_RELAXED));\n\t\tKUNIT_EXPECT_EQ(test, -2L, test_var);\n\n\t\t__atomic_thread_fence(__ATOMIC_SEQ_CST);\n\t\t__atomic_signal_fence(__ATOMIC_SEQ_CST);\n\n\t\tkcsan_disable_current();\n\n\t\tmatch_never = report_available();\n\t} while (!end_test_checks(match_never));\n\tKUNIT_EXPECT_FALSE(test, match_never);\n}\n\n__no_kcsan\nstatic void test_1bit_value_change(struct kunit *test)\n{\n\tstruct expect_report expect = {\n\t\t.access = {\n\t\t\t{ test_kernel_read, &test_var, sizeof(test_var), 0 },\n\t\t\t{ test_kernel_xor_1bit, &test_var, sizeof(test_var), __KCSAN_ACCESS_RW(KCSAN_ACCESS_WRITE) },\n\t\t},\n\t};\n\tbool match = false;\n\n\tbegin_test_checks(test_kernel_read, test_kernel_xor_1bit);\n\tdo {\n\t\tmatch = IS_ENABLED(CONFIG_KCSAN_PERMISSIVE)\n\t\t\t\t? report_available()\n\t\t\t\t: report_matches(&expect);\n\t} while (!end_test_checks(match));\n\tif (IS_ENABLED(CONFIG_KCSAN_PERMISSIVE))\n\t\tKUNIT_EXPECT_FALSE(test, match);\n\telse\n\t\tKUNIT_EXPECT_TRUE(test, match);\n}\n\n__no_kcsan\nstatic void test_correct_barrier(struct kunit *test)\n{\n\tstruct expect_report expect = {\n\t\t.access = {\n\t\t\t{ test_kernel_with_memorder, &test_var, sizeof(test_var), __KCSAN_ACCESS_RW(KCSAN_ACCESS_WRITE) },\n\t\t\t{ test_kernel_with_memorder, &test_var, sizeof(test_var), __KCSAN_ACCESS_RW(0) },\n\t\t},\n\t};\n\tbool match_expect = false;\n\n\ttest_struct.val[0] = 0;  \n\tbegin_test_checks(test_kernel_with_memorder, test_kernel_with_memorder);\n\tdo {\n\t\tmatch_expect = report_matches_any_reordered(&expect);\n\t} while (!end_test_checks(match_expect));\n\tKUNIT_EXPECT_FALSE(test, match_expect);\n}\n\n__no_kcsan\nstatic void test_missing_barrier(struct kunit *test)\n{\n\tstruct expect_report expect = {\n\t\t.access = {\n\t\t\t{ test_kernel_wrong_memorder, &test_var, sizeof(test_var), __KCSAN_ACCESS_RW(KCSAN_ACCESS_WRITE) },\n\t\t\t{ test_kernel_wrong_memorder, &test_var, sizeof(test_var), __KCSAN_ACCESS_RW(0) },\n\t\t},\n\t};\n\tbool match_expect = false;\n\n\ttest_struct.val[0] = 0;  \n\tbegin_test_checks(test_kernel_wrong_memorder, test_kernel_wrong_memorder);\n\tdo {\n\t\tmatch_expect = report_matches_any_reordered(&expect);\n\t} while (!end_test_checks(match_expect));\n\tif (IS_ENABLED(CONFIG_KCSAN_WEAK_MEMORY))\n\t\tKUNIT_EXPECT_TRUE(test, match_expect);\n\telse\n\t\tKUNIT_EXPECT_FALSE(test, match_expect);\n}\n\n__no_kcsan\nstatic void test_atomic_builtins_correct_barrier(struct kunit *test)\n{\n\tstruct expect_report expect = {\n\t\t.access = {\n\t\t\t{ test_kernel_atomic_builtin_with_memorder, &test_var, sizeof(test_var), __KCSAN_ACCESS_RW(KCSAN_ACCESS_WRITE) },\n\t\t\t{ test_kernel_atomic_builtin_with_memorder, &test_var, sizeof(test_var), __KCSAN_ACCESS_RW(0) },\n\t\t},\n\t};\n\tbool match_expect = false;\n\n\ttest_struct.val[0] = 0;  \n\tbegin_test_checks(test_kernel_atomic_builtin_with_memorder,\n\t\t\t  test_kernel_atomic_builtin_with_memorder);\n\tdo {\n\t\tmatch_expect = report_matches_any_reordered(&expect);\n\t} while (!end_test_checks(match_expect));\n\tKUNIT_EXPECT_FALSE(test, match_expect);\n}\n\n__no_kcsan\nstatic void test_atomic_builtins_missing_barrier(struct kunit *test)\n{\n\tstruct expect_report expect = {\n\t\t.access = {\n\t\t\t{ test_kernel_atomic_builtin_wrong_memorder, &test_var, sizeof(test_var), __KCSAN_ACCESS_RW(KCSAN_ACCESS_WRITE) },\n\t\t\t{ test_kernel_atomic_builtin_wrong_memorder, &test_var, sizeof(test_var), __KCSAN_ACCESS_RW(0) },\n\t\t},\n\t};\n\tbool match_expect = false;\n\n\ttest_struct.val[0] = 0;  \n\tbegin_test_checks(test_kernel_atomic_builtin_wrong_memorder,\n\t\t\t  test_kernel_atomic_builtin_wrong_memorder);\n\tdo {\n\t\tmatch_expect = report_matches_any_reordered(&expect);\n\t} while (!end_test_checks(match_expect));\n\tif (IS_ENABLED(CONFIG_KCSAN_WEAK_MEMORY))\n\t\tKUNIT_EXPECT_TRUE(test, match_expect);\n\telse\n\t\tKUNIT_EXPECT_FALSE(test, match_expect);\n}\n\n \nstatic const void *nthreads_gen_params(const void *prev, char *desc)\n{\n\tlong nthreads = (long)prev;\n\n\tif (nthreads < 0 || nthreads >= 32)\n\t\tnthreads = 0;  \n\telse if (!nthreads)\n\t\tnthreads = 2;  \n\telse if (nthreads < 5)\n\t\tnthreads++;\n\telse if (nthreads == 5)\n\t\tnthreads = 8;\n\telse\n\t\tnthreads *= 2;\n\n\tif (!preempt_model_preemptible() ||\n\t    !IS_ENABLED(CONFIG_KCSAN_INTERRUPT_WATCHER)) {\n\t\t \n\t\tconst long min_unused_cpus = preempt_model_none() ? 2 : 0;\n\t\tconst long min_required_cpus = 2 + min_unused_cpus;\n\n\t\tif (num_online_cpus() < min_required_cpus) {\n\t\t\tpr_err_once(\"Too few online CPUs (%u < %ld) for test\\n\",\n\t\t\t\t    num_online_cpus(), min_required_cpus);\n\t\t\tnthreads = 0;\n\t\t} else if (nthreads >= num_online_cpus() - min_unused_cpus) {\n\t\t\t \n\t\t\tnthreads = -(num_online_cpus() - min_unused_cpus);\n\t\t\tpr_warn_once(\"Limiting number of threads to %ld (only %d online CPUs)\\n\",\n\t\t\t\t     -nthreads, num_online_cpus());\n\t\t}\n\t}\n\n\tsnprintf(desc, KUNIT_PARAM_DESC_SIZE, \"threads=%ld\", abs(nthreads));\n\treturn (void *)nthreads;\n}\n\n#define KCSAN_KUNIT_CASE(test_name) KUNIT_CASE_PARAM(test_name, nthreads_gen_params)\nstatic struct kunit_case kcsan_test_cases[] = {\n\tKUNIT_CASE(test_barrier_nothreads),\n\tKCSAN_KUNIT_CASE(test_basic),\n\tKCSAN_KUNIT_CASE(test_concurrent_races),\n\tKCSAN_KUNIT_CASE(test_novalue_change),\n\tKCSAN_KUNIT_CASE(test_novalue_change_exception),\n\tKCSAN_KUNIT_CASE(test_unknown_origin),\n\tKCSAN_KUNIT_CASE(test_write_write_assume_atomic),\n\tKCSAN_KUNIT_CASE(test_write_write_struct),\n\tKCSAN_KUNIT_CASE(test_write_write_struct_part),\n\tKCSAN_KUNIT_CASE(test_read_atomic_write_atomic),\n\tKCSAN_KUNIT_CASE(test_read_plain_atomic_write),\n\tKCSAN_KUNIT_CASE(test_read_plain_atomic_rmw),\n\tKCSAN_KUNIT_CASE(test_zero_size_access),\n\tKCSAN_KUNIT_CASE(test_data_race),\n\tKCSAN_KUNIT_CASE(test_assert_exclusive_writer),\n\tKCSAN_KUNIT_CASE(test_assert_exclusive_access),\n\tKCSAN_KUNIT_CASE(test_assert_exclusive_access_writer),\n\tKCSAN_KUNIT_CASE(test_assert_exclusive_bits_change),\n\tKCSAN_KUNIT_CASE(test_assert_exclusive_bits_nochange),\n\tKCSAN_KUNIT_CASE(test_assert_exclusive_writer_scoped),\n\tKCSAN_KUNIT_CASE(test_assert_exclusive_access_scoped),\n\tKCSAN_KUNIT_CASE(test_jiffies_noreport),\n\tKCSAN_KUNIT_CASE(test_seqlock_noreport),\n\tKCSAN_KUNIT_CASE(test_atomic_builtins),\n\tKCSAN_KUNIT_CASE(test_1bit_value_change),\n\tKCSAN_KUNIT_CASE(test_correct_barrier),\n\tKCSAN_KUNIT_CASE(test_missing_barrier),\n\tKCSAN_KUNIT_CASE(test_atomic_builtins_correct_barrier),\n\tKCSAN_KUNIT_CASE(test_atomic_builtins_missing_barrier),\n\t{},\n};\n\n \n\n \n__no_kcsan\nstatic void access_thread_timer(struct timer_list *timer)\n{\n\tstatic atomic_t cnt = ATOMIC_INIT(0);\n\tunsigned int idx;\n\tvoid (*func)(void);\n\n\tidx = (unsigned int)atomic_inc_return(&cnt) % ARRAY_SIZE(access_kernels);\n\t \n\tfunc = smp_load_acquire(&access_kernels[idx]);\n\tif (func)\n\t\tfunc();\n}\n\n \n__no_kcsan\nstatic int access_thread(void *arg)\n{\n\tstruct timer_list timer;\n\tunsigned int cnt = 0;\n\tunsigned int idx;\n\tvoid (*func)(void);\n\n\ttimer_setup_on_stack(&timer, access_thread_timer, 0);\n\tdo {\n\t\tmight_sleep();\n\n\t\tif (!timer_pending(&timer))\n\t\t\tmod_timer(&timer, jiffies + 1);\n\t\telse {\n\t\t\t \n\t\t\tidx = cnt++ % ARRAY_SIZE(access_kernels);\n\t\t\t \n\t\t\tfunc = smp_load_acquire(&access_kernels[idx]);\n\t\t\tif (func)\n\t\t\t\tfunc();\n\t\t}\n\t} while (!torture_must_stop());\n\tdel_timer_sync(&timer);\n\tdestroy_timer_on_stack(&timer);\n\n\ttorture_kthread_stopping(\"access_thread\");\n\treturn 0;\n}\n\n__no_kcsan\nstatic int test_init(struct kunit *test)\n{\n\tunsigned long flags;\n\tint nthreads;\n\tint i;\n\n\tspin_lock_irqsave(&observed.lock, flags);\n\tfor (i = 0; i < ARRAY_SIZE(observed.lines); ++i)\n\t\tobserved.lines[i][0] = '\\0';\n\tobserved.nlines = 0;\n\tspin_unlock_irqrestore(&observed.lock, flags);\n\n\tif (strstr(test->name, \"nothreads\"))\n\t\treturn 0;\n\n\tif (!torture_init_begin((char *)test->name, 1))\n\t\treturn -EBUSY;\n\n\tif (WARN_ON(threads))\n\t\tgoto err;\n\n\tfor (i = 0; i < ARRAY_SIZE(access_kernels); ++i) {\n\t\tif (WARN_ON(access_kernels[i]))\n\t\t\tgoto err;\n\t}\n\n\tnthreads = abs((long)test->param_value);\n\tif (WARN_ON(!nthreads))\n\t\tgoto err;\n\n\tthreads = kcalloc(nthreads + 1, sizeof(struct task_struct *), GFP_KERNEL);\n\tif (WARN_ON(!threads))\n\t\tgoto err;\n\n\tthreads[nthreads] = NULL;\n\tfor (i = 0; i < nthreads; ++i) {\n\t\tif (torture_create_kthread(access_thread, NULL, threads[i]))\n\t\t\tgoto err;\n\t}\n\n\ttorture_init_end();\n\n\treturn 0;\n\nerr:\n\tkfree(threads);\n\tthreads = NULL;\n\ttorture_init_end();\n\treturn -EINVAL;\n}\n\n__no_kcsan\nstatic void test_exit(struct kunit *test)\n{\n\tstruct task_struct **stop_thread;\n\tint i;\n\n\tif (strstr(test->name, \"nothreads\"))\n\t\treturn;\n\n\tif (torture_cleanup_begin())\n\t\treturn;\n\n\tfor (i = 0; i < ARRAY_SIZE(access_kernels); ++i)\n\t\tWRITE_ONCE(access_kernels[i], NULL);\n\n\tif (threads) {\n\t\tfor (stop_thread = threads; *stop_thread; stop_thread++)\n\t\t\ttorture_stop_kthread(reader_thread, *stop_thread);\n\n\t\tkfree(threads);\n\t\tthreads = NULL;\n\t}\n\n\ttorture_cleanup_end();\n}\n\n__no_kcsan\nstatic void register_tracepoints(void)\n{\n\tregister_trace_console(probe_console, NULL);\n}\n\n__no_kcsan\nstatic void unregister_tracepoints(void)\n{\n\tunregister_trace_console(probe_console, NULL);\n}\n\nstatic int kcsan_suite_init(struct kunit_suite *suite)\n{\n\tregister_tracepoints();\n\treturn 0;\n}\n\nstatic void kcsan_suite_exit(struct kunit_suite *suite)\n{\n\tunregister_tracepoints();\n\ttracepoint_synchronize_unregister();\n}\n\nstatic struct kunit_suite kcsan_test_suite = {\n\t.name = \"kcsan\",\n\t.test_cases = kcsan_test_cases,\n\t.init = test_init,\n\t.exit = test_exit,\n\t.suite_init = kcsan_suite_init,\n\t.suite_exit = kcsan_suite_exit,\n};\n\nkunit_test_suites(&kcsan_test_suite);\n\nMODULE_LICENSE(\"GPL v2\");\nMODULE_AUTHOR(\"Marco Elver <elver@google.com>\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}