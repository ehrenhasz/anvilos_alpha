{
  "module_name": "core.c",
  "hash_id": "a7605ea67da08bed39699004837cd1d52af1037db60785a9c3014dd8c18dcbbe",
  "original_prompt": "Ingested from linux-6.6.14/kernel/kcsan/core.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) \"kcsan: \" fmt\n\n#include <linux/atomic.h>\n#include <linux/bug.h>\n#include <linux/delay.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/list.h>\n#include <linux/minmax.h>\n#include <linux/moduleparam.h>\n#include <linux/percpu.h>\n#include <linux/preempt.h>\n#include <linux/sched.h>\n#include <linux/string.h>\n#include <linux/uaccess.h>\n\n#include \"encoding.h\"\n#include \"kcsan.h\"\n#include \"permissive.h\"\n\nstatic bool kcsan_early_enable = IS_ENABLED(CONFIG_KCSAN_EARLY_ENABLE);\nunsigned int kcsan_udelay_task = CONFIG_KCSAN_UDELAY_TASK;\nunsigned int kcsan_udelay_interrupt = CONFIG_KCSAN_UDELAY_INTERRUPT;\nstatic long kcsan_skip_watch = CONFIG_KCSAN_SKIP_WATCH;\nstatic bool kcsan_interrupt_watcher = IS_ENABLED(CONFIG_KCSAN_INTERRUPT_WATCHER);\n\n#ifdef MODULE_PARAM_PREFIX\n#undef MODULE_PARAM_PREFIX\n#endif\n#define MODULE_PARAM_PREFIX \"kcsan.\"\nmodule_param_named(early_enable, kcsan_early_enable, bool, 0);\nmodule_param_named(udelay_task, kcsan_udelay_task, uint, 0644);\nmodule_param_named(udelay_interrupt, kcsan_udelay_interrupt, uint, 0644);\nmodule_param_named(skip_watch, kcsan_skip_watch, long, 0644);\nmodule_param_named(interrupt_watcher, kcsan_interrupt_watcher, bool, 0444);\n\n#ifdef CONFIG_KCSAN_WEAK_MEMORY\nstatic bool kcsan_weak_memory = true;\nmodule_param_named(weak_memory, kcsan_weak_memory, bool, 0644);\n#else\n#define kcsan_weak_memory false\n#endif\n\nbool kcsan_enabled;\n\n \nstatic DEFINE_PER_CPU(struct kcsan_ctx, kcsan_cpu_ctx) = {\n\t.scoped_accesses\t= {LIST_POISON1, NULL},\n};\n\n \n#define SLOT_IDX(slot, i) (slot + ((i + KCSAN_CHECK_ADJACENT) % NUM_SLOTS))\n\n \n#define SLOT_IDX_FAST(slot, i) (slot + i)\n\n \nstatic atomic_long_t watchpoints[CONFIG_KCSAN_NUM_WATCHPOINTS + NUM_SLOTS-1];\n\n \nstatic DEFINE_PER_CPU(long, kcsan_skip);\n\n \nstatic DEFINE_PER_CPU(u32, kcsan_rand_state);\n\nstatic __always_inline atomic_long_t *find_watchpoint(unsigned long addr,\n\t\t\t\t\t\t      size_t size,\n\t\t\t\t\t\t      bool expect_write,\n\t\t\t\t\t\t      long *encoded_watchpoint)\n{\n\tconst int slot = watchpoint_slot(addr);\n\tconst unsigned long addr_masked = addr & WATCHPOINT_ADDR_MASK;\n\tatomic_long_t *watchpoint;\n\tunsigned long wp_addr_masked;\n\tsize_t wp_size;\n\tbool is_write;\n\tint i;\n\n\tBUILD_BUG_ON(CONFIG_KCSAN_NUM_WATCHPOINTS < NUM_SLOTS);\n\n\tfor (i = 0; i < NUM_SLOTS; ++i) {\n\t\twatchpoint = &watchpoints[SLOT_IDX_FAST(slot, i)];\n\t\t*encoded_watchpoint = atomic_long_read(watchpoint);\n\t\tif (!decode_watchpoint(*encoded_watchpoint, &wp_addr_masked,\n\t\t\t\t       &wp_size, &is_write))\n\t\t\tcontinue;\n\n\t\tif (expect_write && !is_write)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (matching_access(wp_addr_masked, wp_size, addr_masked, size))\n\t\t\treturn watchpoint;\n\t}\n\n\treturn NULL;\n}\n\nstatic inline atomic_long_t *\ninsert_watchpoint(unsigned long addr, size_t size, bool is_write)\n{\n\tconst int slot = watchpoint_slot(addr);\n\tconst long encoded_watchpoint = encode_watchpoint(addr, size, is_write);\n\tatomic_long_t *watchpoint;\n\tint i;\n\n\t \n\tBUILD_BUG_ON(SLOT_IDX(0, 0) != KCSAN_CHECK_ADJACENT);\n\tBUILD_BUG_ON(SLOT_IDX(0, KCSAN_CHECK_ADJACENT+1) != 0);\n\tBUILD_BUG_ON(SLOT_IDX(CONFIG_KCSAN_NUM_WATCHPOINTS-1, KCSAN_CHECK_ADJACENT) != ARRAY_SIZE(watchpoints)-1);\n\tBUILD_BUG_ON(SLOT_IDX(CONFIG_KCSAN_NUM_WATCHPOINTS-1, KCSAN_CHECK_ADJACENT+1) != ARRAY_SIZE(watchpoints) - NUM_SLOTS);\n\n\tfor (i = 0; i < NUM_SLOTS; ++i) {\n\t\tlong expect_val = INVALID_WATCHPOINT;\n\n\t\t \n\t\twatchpoint = &watchpoints[SLOT_IDX(slot, i)];\n\t\tif (atomic_long_try_cmpxchg_relaxed(watchpoint, &expect_val, encoded_watchpoint))\n\t\t\treturn watchpoint;\n\t}\n\n\treturn NULL;\n}\n\n \nstatic __always_inline bool\ntry_consume_watchpoint(atomic_long_t *watchpoint, long encoded_watchpoint)\n{\n\treturn atomic_long_try_cmpxchg_relaxed(watchpoint, &encoded_watchpoint, CONSUMED_WATCHPOINT);\n}\n\n \nstatic inline bool consume_watchpoint(atomic_long_t *watchpoint)\n{\n\treturn atomic_long_xchg_relaxed(watchpoint, CONSUMED_WATCHPOINT) != CONSUMED_WATCHPOINT;\n}\n\n \nstatic inline void remove_watchpoint(atomic_long_t *watchpoint)\n{\n\tatomic_long_set(watchpoint, INVALID_WATCHPOINT);\n}\n\nstatic __always_inline struct kcsan_ctx *get_ctx(void)\n{\n\t \n\treturn in_task() ? &current->kcsan_ctx : raw_cpu_ptr(&kcsan_cpu_ctx);\n}\n\nstatic __always_inline void\ncheck_access(const volatile void *ptr, size_t size, int type, unsigned long ip);\n\n \nstatic noinline void kcsan_check_scoped_accesses(void)\n{\n\tstruct kcsan_ctx *ctx = get_ctx();\n\tstruct kcsan_scoped_access *scoped_access;\n\n\tif (ctx->disable_scoped)\n\t\treturn;\n\n\tctx->disable_scoped++;\n\tlist_for_each_entry(scoped_access, &ctx->scoped_accesses, list) {\n\t\tcheck_access(scoped_access->ptr, scoped_access->size,\n\t\t\t     scoped_access->type, scoped_access->ip);\n\t}\n\tctx->disable_scoped--;\n}\n\n \nstatic __always_inline bool\nis_atomic(struct kcsan_ctx *ctx, const volatile void *ptr, size_t size, int type)\n{\n\tif (type & KCSAN_ACCESS_ATOMIC)\n\t\treturn true;\n\n\t \n\tif (type & KCSAN_ACCESS_ASSERT)\n\t\treturn false;\n\n\tif (IS_ENABLED(CONFIG_KCSAN_ASSUME_PLAIN_WRITES_ATOMIC) &&\n\t    (type & KCSAN_ACCESS_WRITE) && size <= sizeof(long) &&\n\t    !(type & KCSAN_ACCESS_COMPOUND) && IS_ALIGNED((unsigned long)ptr, size))\n\t\treturn true;  \n\n\tif (ctx->atomic_next > 0) {\n\t\t \n\t\tif ((hardirq_count() >> HARDIRQ_SHIFT) < 2)\n\t\t\t--ctx->atomic_next;  \n\t\treturn true;\n\t}\n\n\treturn ctx->atomic_nest_count > 0 || ctx->in_flat_atomic;\n}\n\nstatic __always_inline bool\nshould_watch(struct kcsan_ctx *ctx, const volatile void *ptr, size_t size, int type)\n{\n\t \n\tif (is_atomic(ctx, ptr, size, type))\n\t\treturn false;\n\n\tif (this_cpu_dec_return(kcsan_skip) >= 0)\n\t\treturn false;\n\n\t \n\n\t \n\treturn true;\n}\n\n \nstatic u32 kcsan_prandom_u32_max(u32 ep_ro)\n{\n\tu32 state = this_cpu_read(kcsan_rand_state);\n\n\tstate = 1664525 * state + 1013904223;\n\tthis_cpu_write(kcsan_rand_state, state);\n\n\treturn state % ep_ro;\n}\n\nstatic inline void reset_kcsan_skip(void)\n{\n\tlong skip_count = kcsan_skip_watch -\n\t\t\t  (IS_ENABLED(CONFIG_KCSAN_SKIP_WATCH_RANDOMIZE) ?\n\t\t\t\t   kcsan_prandom_u32_max(kcsan_skip_watch) :\n\t\t\t\t   0);\n\tthis_cpu_write(kcsan_skip, skip_count);\n}\n\nstatic __always_inline bool kcsan_is_enabled(struct kcsan_ctx *ctx)\n{\n\treturn READ_ONCE(kcsan_enabled) && !ctx->disable_count;\n}\n\n \nstatic void delay_access(int type)\n{\n\tunsigned int delay = in_task() ? kcsan_udelay_task : kcsan_udelay_interrupt;\n\t \n\tunsigned int skew_delay_order =\n\t\t(type & (KCSAN_ACCESS_COMPOUND | KCSAN_ACCESS_ASSERT)) ? 1 : 0;\n\n\tdelay -= IS_ENABLED(CONFIG_KCSAN_DELAY_RANDOMIZE) ?\n\t\t\t       kcsan_prandom_u32_max(delay >> skew_delay_order) :\n\t\t\t       0;\n\tudelay(delay);\n}\n\n \nstatic __always_inline u64 read_instrumented_memory(const volatile void *ptr, size_t size)\n{\n\t \n\tswitch (size) {\n\tcase 1:  return *(const volatile u8 *)ptr;\n\tcase 2:  return *(const volatile u16 *)ptr;\n\tcase 4:  return *(const volatile u32 *)ptr;\n\tcase 8:  return *(const volatile u64 *)ptr;\n\tdefault: return 0;  \n\t}\n}\n\nvoid kcsan_save_irqtrace(struct task_struct *task)\n{\n#ifdef CONFIG_TRACE_IRQFLAGS\n\ttask->kcsan_save_irqtrace = task->irqtrace;\n#endif\n}\n\nvoid kcsan_restore_irqtrace(struct task_struct *task)\n{\n#ifdef CONFIG_TRACE_IRQFLAGS\n\ttask->irqtrace = task->kcsan_save_irqtrace;\n#endif\n}\n\nstatic __always_inline int get_kcsan_stack_depth(void)\n{\n#ifdef CONFIG_KCSAN_WEAK_MEMORY\n\treturn current->kcsan_stack_depth;\n#else\n\tBUILD_BUG();\n\treturn 0;\n#endif\n}\n\nstatic __always_inline void add_kcsan_stack_depth(int val)\n{\n#ifdef CONFIG_KCSAN_WEAK_MEMORY\n\tcurrent->kcsan_stack_depth += val;\n#else\n\tBUILD_BUG();\n#endif\n}\n\nstatic __always_inline struct kcsan_scoped_access *get_reorder_access(struct kcsan_ctx *ctx)\n{\n#ifdef CONFIG_KCSAN_WEAK_MEMORY\n\treturn ctx->disable_scoped ? NULL : &ctx->reorder_access;\n#else\n\treturn NULL;\n#endif\n}\n\nstatic __always_inline bool\nfind_reorder_access(struct kcsan_ctx *ctx, const volatile void *ptr, size_t size,\n\t\t    int type, unsigned long ip)\n{\n\tstruct kcsan_scoped_access *reorder_access = get_reorder_access(ctx);\n\n\tif (!reorder_access)\n\t\treturn false;\n\n\t \n\treturn reorder_access->ptr == ptr && reorder_access->size == size &&\n\t       reorder_access->type == type && reorder_access->ip == ip;\n}\n\nstatic inline void\nset_reorder_access(struct kcsan_ctx *ctx, const volatile void *ptr, size_t size,\n\t\t   int type, unsigned long ip)\n{\n\tstruct kcsan_scoped_access *reorder_access = get_reorder_access(ctx);\n\n\tif (!reorder_access || !kcsan_weak_memory)\n\t\treturn;\n\n\t \n\tctx->disable_scoped++;\n\tbarrier();\n\treorder_access->ptr\t\t= ptr;\n\treorder_access->size\t\t= size;\n\treorder_access->type\t\t= type | KCSAN_ACCESS_SCOPED;\n\treorder_access->ip\t\t= ip;\n\treorder_access->stack_depth\t= get_kcsan_stack_depth();\n\tbarrier();\n\tctx->disable_scoped--;\n}\n\n \n\nstatic noinline void kcsan_found_watchpoint(const volatile void *ptr,\n\t\t\t\t\t    size_t size,\n\t\t\t\t\t    int type,\n\t\t\t\t\t    unsigned long ip,\n\t\t\t\t\t    atomic_long_t *watchpoint,\n\t\t\t\t\t    long encoded_watchpoint)\n{\n\tconst bool is_assert = (type & KCSAN_ACCESS_ASSERT) != 0;\n\tstruct kcsan_ctx *ctx = get_ctx();\n\tunsigned long flags;\n\tbool consumed;\n\n\t \n\n\tif (!kcsan_is_enabled(ctx))\n\t\treturn;\n\n\t \n\tif (ctx->access_mask && !find_reorder_access(ctx, ptr, size, type, ip))\n\t\treturn;\n\n\t \n\tif (!is_assert && kcsan_ignore_address(ptr))\n\t\treturn;\n\n\t \n\tconsumed = try_consume_watchpoint(watchpoint, encoded_watchpoint);\n\n\t \n\tflags = user_access_save();\n\n\tif (consumed) {\n\t\tkcsan_save_irqtrace(current);\n\t\tkcsan_report_set_info(ptr, size, type, ip, watchpoint - watchpoints);\n\t\tkcsan_restore_irqtrace(current);\n\t} else {\n\t\t \n\t\tatomic_long_inc(&kcsan_counters[KCSAN_COUNTER_REPORT_RACES]);\n\t}\n\n\tif (is_assert)\n\t\tatomic_long_inc(&kcsan_counters[KCSAN_COUNTER_ASSERT_FAILURES]);\n\telse\n\t\tatomic_long_inc(&kcsan_counters[KCSAN_COUNTER_DATA_RACES]);\n\n\tuser_access_restore(flags);\n}\n\nstatic noinline void\nkcsan_setup_watchpoint(const volatile void *ptr, size_t size, int type, unsigned long ip)\n{\n\tconst bool is_write = (type & KCSAN_ACCESS_WRITE) != 0;\n\tconst bool is_assert = (type & KCSAN_ACCESS_ASSERT) != 0;\n\tatomic_long_t *watchpoint;\n\tu64 old, new, diff;\n\tenum kcsan_value_change value_change = KCSAN_VALUE_CHANGE_MAYBE;\n\tbool interrupt_watcher = kcsan_interrupt_watcher;\n\tunsigned long ua_flags = user_access_save();\n\tstruct kcsan_ctx *ctx = get_ctx();\n\tunsigned long access_mask = ctx->access_mask;\n\tunsigned long irq_flags = 0;\n\tbool is_reorder_access;\n\n\t \n\treset_kcsan_skip();\n\n\tif (!kcsan_is_enabled(ctx))\n\t\tgoto out;\n\n\t \n\tif (!is_assert && kcsan_ignore_address(ptr))\n\t\tgoto out;\n\n\tif (!check_encodable((unsigned long)ptr, size)) {\n\t\tatomic_long_inc(&kcsan_counters[KCSAN_COUNTER_UNENCODABLE_ACCESSES]);\n\t\tgoto out;\n\t}\n\n\t \n\tis_reorder_access = find_reorder_access(ctx, ptr, size, type, ip);\n\tif (is_reorder_access)\n\t\tinterrupt_watcher = false;\n\t \n\tctx->disable_scoped++;\n\n\t \n\tkcsan_save_irqtrace(current);\n\tif (!interrupt_watcher)\n\t\tlocal_irq_save(irq_flags);\n\n\twatchpoint = insert_watchpoint((unsigned long)ptr, size, is_write);\n\tif (watchpoint == NULL) {\n\t\t \n\t\tatomic_long_inc(&kcsan_counters[KCSAN_COUNTER_NO_CAPACITY]);\n\t\tgoto out_unlock;\n\t}\n\n\tatomic_long_inc(&kcsan_counters[KCSAN_COUNTER_SETUP_WATCHPOINTS]);\n\tatomic_long_inc(&kcsan_counters[KCSAN_COUNTER_USED_WATCHPOINTS]);\n\n\t \n\told = is_reorder_access ? 0 : read_instrumented_memory(ptr, size);\n\n\t \n\tdelay_access(type);\n\n\t \n\tif (!is_reorder_access) {\n\t\tnew = read_instrumented_memory(ptr, size);\n\t} else {\n\t\t \n\t\tnew = 0;\n\t\taccess_mask = 0;\n\t}\n\n\tdiff = old ^ new;\n\tif (access_mask)\n\t\tdiff &= access_mask;\n\n\t \n\tif (diff && !kcsan_ignore_data_race(size, type, old, new, diff))\n\t\tvalue_change = KCSAN_VALUE_CHANGE_TRUE;\n\n\t \n\tif (!consume_watchpoint(watchpoint)) {\n\t\t \n\t\tif (value_change == KCSAN_VALUE_CHANGE_MAYBE) {\n\t\t\tif (access_mask != 0) {\n\t\t\t\t \n\t\t\t\tvalue_change = KCSAN_VALUE_CHANGE_FALSE;\n\t\t\t} else if (size > 8 || is_assert) {\n\t\t\t\t \n\t\t\t\tvalue_change = KCSAN_VALUE_CHANGE_TRUE;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (is_assert && value_change == KCSAN_VALUE_CHANGE_TRUE)\n\t\t\tatomic_long_inc(&kcsan_counters[KCSAN_COUNTER_ASSERT_FAILURES]);\n\n\t\tkcsan_report_known_origin(ptr, size, type, ip,\n\t\t\t\t\t  value_change, watchpoint - watchpoints,\n\t\t\t\t\t  old, new, access_mask);\n\t} else if (value_change == KCSAN_VALUE_CHANGE_TRUE) {\n\t\t \n\n\t\tatomic_long_inc(&kcsan_counters[KCSAN_COUNTER_RACES_UNKNOWN_ORIGIN]);\n\t\tif (is_assert)\n\t\t\tatomic_long_inc(&kcsan_counters[KCSAN_COUNTER_ASSERT_FAILURES]);\n\n\t\tif (IS_ENABLED(CONFIG_KCSAN_REPORT_RACE_UNKNOWN_ORIGIN) || is_assert) {\n\t\t\tkcsan_report_unknown_origin(ptr, size, type, ip,\n\t\t\t\t\t\t    old, new, access_mask);\n\t\t}\n\t}\n\n\t \n\tremove_watchpoint(watchpoint);\n\tatomic_long_dec(&kcsan_counters[KCSAN_COUNTER_USED_WATCHPOINTS]);\n\nout_unlock:\n\tif (!interrupt_watcher)\n\t\tlocal_irq_restore(irq_flags);\n\tkcsan_restore_irqtrace(current);\n\tctx->disable_scoped--;\n\n\t \n\tif (!access_mask && !is_assert)\n\t\tset_reorder_access(ctx, ptr, size, type, ip);\nout:\n\tuser_access_restore(ua_flags);\n}\n\nstatic __always_inline void\ncheck_access(const volatile void *ptr, size_t size, int type, unsigned long ip)\n{\n\tatomic_long_t *watchpoint;\n\tlong encoded_watchpoint;\n\n\t \n\tif (unlikely(size == 0))\n\t\treturn;\n\nagain:\n\t \n\twatchpoint = find_watchpoint((unsigned long)ptr, size,\n\t\t\t\t     !(type & KCSAN_ACCESS_WRITE),\n\t\t\t\t     &encoded_watchpoint);\n\t \n\n\tif (unlikely(watchpoint != NULL))\n\t\tkcsan_found_watchpoint(ptr, size, type, ip, watchpoint, encoded_watchpoint);\n\telse {\n\t\tstruct kcsan_ctx *ctx = get_ctx();  \n\n\t\tif (unlikely(should_watch(ctx, ptr, size, type))) {\n\t\t\tkcsan_setup_watchpoint(ptr, size, type, ip);\n\t\t\treturn;\n\t\t}\n\n\t\tif (!(type & KCSAN_ACCESS_SCOPED)) {\n\t\t\tstruct kcsan_scoped_access *reorder_access = get_reorder_access(ctx);\n\n\t\t\tif (reorder_access) {\n\t\t\t\t \n\t\t\t\tptr = reorder_access->ptr;\n\t\t\t\ttype = reorder_access->type;\n\t\t\t\tip = reorder_access->ip;\n\t\t\t\t \n\t\t\t\tbarrier();\n\t\t\t\tsize = READ_ONCE(reorder_access->size);\n\t\t\t\tif (size)\n\t\t\t\t\tgoto again;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (unlikely(ctx->scoped_accesses.prev))\n\t\t\tkcsan_check_scoped_accesses();\n\t}\n}\n\n \n\nvoid __init kcsan_init(void)\n{\n\tint cpu;\n\n\tBUG_ON(!in_task());\n\n\tfor_each_possible_cpu(cpu)\n\t\tper_cpu(kcsan_rand_state, cpu) = (u32)get_cycles();\n\n\t \n\tif (kcsan_early_enable) {\n\t\tpr_info(\"enabled early\\n\");\n\t\tWRITE_ONCE(kcsan_enabled, true);\n\t}\n\n\tif (IS_ENABLED(CONFIG_KCSAN_REPORT_VALUE_CHANGE_ONLY) ||\n\t    IS_ENABLED(CONFIG_KCSAN_ASSUME_PLAIN_WRITES_ATOMIC) ||\n\t    IS_ENABLED(CONFIG_KCSAN_PERMISSIVE) ||\n\t    IS_ENABLED(CONFIG_KCSAN_IGNORE_ATOMICS)) {\n\t\tpr_warn(\"non-strict mode configured - use CONFIG_KCSAN_STRICT=y to see all data races\\n\");\n\t} else {\n\t\tpr_info(\"strict mode configured\\n\");\n\t}\n}\n\n \n\nvoid kcsan_disable_current(void)\n{\n\t++get_ctx()->disable_count;\n}\nEXPORT_SYMBOL(kcsan_disable_current);\n\nvoid kcsan_enable_current(void)\n{\n\tif (get_ctx()->disable_count-- == 0) {\n\t\t \n\t\tkcsan_disable_current();  \n\t\tkcsan_disable_current();  \n\t\tWARN(1, \"Unbalanced %s()\", __func__);\n\t\tkcsan_enable_current();\n\t}\n}\nEXPORT_SYMBOL(kcsan_enable_current);\n\nvoid kcsan_enable_current_nowarn(void)\n{\n\tif (get_ctx()->disable_count-- == 0)\n\t\tkcsan_disable_current();\n}\nEXPORT_SYMBOL(kcsan_enable_current_nowarn);\n\nvoid kcsan_nestable_atomic_begin(void)\n{\n\t \n\n\t++get_ctx()->atomic_nest_count;\n}\nEXPORT_SYMBOL(kcsan_nestable_atomic_begin);\n\nvoid kcsan_nestable_atomic_end(void)\n{\n\tif (get_ctx()->atomic_nest_count-- == 0) {\n\t\t \n\t\tkcsan_nestable_atomic_begin();  \n\t\tkcsan_disable_current();  \n\t\tWARN(1, \"Unbalanced %s()\", __func__);\n\t\tkcsan_enable_current();\n\t}\n}\nEXPORT_SYMBOL(kcsan_nestable_atomic_end);\n\nvoid kcsan_flat_atomic_begin(void)\n{\n\tget_ctx()->in_flat_atomic = true;\n}\nEXPORT_SYMBOL(kcsan_flat_atomic_begin);\n\nvoid kcsan_flat_atomic_end(void)\n{\n\tget_ctx()->in_flat_atomic = false;\n}\nEXPORT_SYMBOL(kcsan_flat_atomic_end);\n\nvoid kcsan_atomic_next(int n)\n{\n\tget_ctx()->atomic_next = n;\n}\nEXPORT_SYMBOL(kcsan_atomic_next);\n\nvoid kcsan_set_access_mask(unsigned long mask)\n{\n\tget_ctx()->access_mask = mask;\n}\nEXPORT_SYMBOL(kcsan_set_access_mask);\n\nstruct kcsan_scoped_access *\nkcsan_begin_scoped_access(const volatile void *ptr, size_t size, int type,\n\t\t\t  struct kcsan_scoped_access *sa)\n{\n\tstruct kcsan_ctx *ctx = get_ctx();\n\n\tcheck_access(ptr, size, type, _RET_IP_);\n\n\tctx->disable_count++;  \n\n\tINIT_LIST_HEAD(&sa->list);\n\tsa->ptr = ptr;\n\tsa->size = size;\n\tsa->type = type;\n\tsa->ip = _RET_IP_;\n\n\tif (!ctx->scoped_accesses.prev)  \n\t\tINIT_LIST_HEAD(&ctx->scoped_accesses);\n\tlist_add(&sa->list, &ctx->scoped_accesses);\n\n\tctx->disable_count--;\n\treturn sa;\n}\nEXPORT_SYMBOL(kcsan_begin_scoped_access);\n\nvoid kcsan_end_scoped_access(struct kcsan_scoped_access *sa)\n{\n\tstruct kcsan_ctx *ctx = get_ctx();\n\n\tif (WARN(!ctx->scoped_accesses.prev, \"Unbalanced %s()?\", __func__))\n\t\treturn;\n\n\tctx->disable_count++;  \n\n\tlist_del(&sa->list);\n\tif (list_empty(&ctx->scoped_accesses))\n\t\t \n\t\tctx->scoped_accesses.prev = NULL;\n\n\tctx->disable_count--;\n\n\tcheck_access(sa->ptr, sa->size, sa->type, sa->ip);\n}\nEXPORT_SYMBOL(kcsan_end_scoped_access);\n\nvoid __kcsan_check_access(const volatile void *ptr, size_t size, int type)\n{\n\tcheck_access(ptr, size, type, _RET_IP_);\n}\nEXPORT_SYMBOL(__kcsan_check_access);\n\n#define DEFINE_MEMORY_BARRIER(name, order_before_cond)\t\t\t\t\\\n\tvoid __kcsan_##name(void)\t\t\t\t\t\t\\\n\t{\t\t\t\t\t\t\t\t\t\\\n\t\tstruct kcsan_scoped_access *sa = get_reorder_access(get_ctx());\t\\\n\t\tif (!sa)\t\t\t\t\t\t\t\\\n\t\t\treturn;\t\t\t\t\t\t\t\\\n\t\tif (order_before_cond)\t\t\t\t\t\t\\\n\t\t\tsa->size = 0;\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\t\\\n\tEXPORT_SYMBOL(__kcsan_##name)\n\nDEFINE_MEMORY_BARRIER(mb, true);\nDEFINE_MEMORY_BARRIER(wmb, sa->type & (KCSAN_ACCESS_WRITE | KCSAN_ACCESS_COMPOUND));\nDEFINE_MEMORY_BARRIER(rmb, !(sa->type & KCSAN_ACCESS_WRITE) || (sa->type & KCSAN_ACCESS_COMPOUND));\nDEFINE_MEMORY_BARRIER(release, true);\n\n \n\n#define DEFINE_TSAN_READ_WRITE(size)                                           \\\n\tvoid __tsan_read##size(void *ptr);                                     \\\n\tvoid __tsan_read##size(void *ptr)                                      \\\n\t{                                                                      \\\n\t\tcheck_access(ptr, size, 0, _RET_IP_);                          \\\n\t}                                                                      \\\n\tEXPORT_SYMBOL(__tsan_read##size);                                      \\\n\tvoid __tsan_unaligned_read##size(void *ptr)                            \\\n\t\t__alias(__tsan_read##size);                                    \\\n\tEXPORT_SYMBOL(__tsan_unaligned_read##size);                            \\\n\tvoid __tsan_write##size(void *ptr);                                    \\\n\tvoid __tsan_write##size(void *ptr)                                     \\\n\t{                                                                      \\\n\t\tcheck_access(ptr, size, KCSAN_ACCESS_WRITE, _RET_IP_);         \\\n\t}                                                                      \\\n\tEXPORT_SYMBOL(__tsan_write##size);                                     \\\n\tvoid __tsan_unaligned_write##size(void *ptr)                           \\\n\t\t__alias(__tsan_write##size);                                   \\\n\tEXPORT_SYMBOL(__tsan_unaligned_write##size);                           \\\n\tvoid __tsan_read_write##size(void *ptr);                               \\\n\tvoid __tsan_read_write##size(void *ptr)                                \\\n\t{                                                                      \\\n\t\tcheck_access(ptr, size,                                        \\\n\t\t\t     KCSAN_ACCESS_COMPOUND | KCSAN_ACCESS_WRITE,       \\\n\t\t\t     _RET_IP_);                                        \\\n\t}                                                                      \\\n\tEXPORT_SYMBOL(__tsan_read_write##size);                                \\\n\tvoid __tsan_unaligned_read_write##size(void *ptr)                      \\\n\t\t__alias(__tsan_read_write##size);                              \\\n\tEXPORT_SYMBOL(__tsan_unaligned_read_write##size)\n\nDEFINE_TSAN_READ_WRITE(1);\nDEFINE_TSAN_READ_WRITE(2);\nDEFINE_TSAN_READ_WRITE(4);\nDEFINE_TSAN_READ_WRITE(8);\nDEFINE_TSAN_READ_WRITE(16);\n\nvoid __tsan_read_range(void *ptr, size_t size);\nvoid __tsan_read_range(void *ptr, size_t size)\n{\n\tcheck_access(ptr, size, 0, _RET_IP_);\n}\nEXPORT_SYMBOL(__tsan_read_range);\n\nvoid __tsan_write_range(void *ptr, size_t size);\nvoid __tsan_write_range(void *ptr, size_t size)\n{\n\tcheck_access(ptr, size, KCSAN_ACCESS_WRITE, _RET_IP_);\n}\nEXPORT_SYMBOL(__tsan_write_range);\n\n \n#define DEFINE_TSAN_VOLATILE_READ_WRITE(size)                                  \\\n\tvoid __tsan_volatile_read##size(void *ptr);                            \\\n\tvoid __tsan_volatile_read##size(void *ptr)                             \\\n\t{                                                                      \\\n\t\tconst bool is_atomic = size <= sizeof(long long) &&            \\\n\t\t\t\t       IS_ALIGNED((unsigned long)ptr, size);   \\\n\t\tif (IS_ENABLED(CONFIG_KCSAN_IGNORE_ATOMICS) && is_atomic)      \\\n\t\t\treturn;                                                \\\n\t\tcheck_access(ptr, size, is_atomic ? KCSAN_ACCESS_ATOMIC : 0,   \\\n\t\t\t     _RET_IP_);                                        \\\n\t}                                                                      \\\n\tEXPORT_SYMBOL(__tsan_volatile_read##size);                             \\\n\tvoid __tsan_unaligned_volatile_read##size(void *ptr)                   \\\n\t\t__alias(__tsan_volatile_read##size);                           \\\n\tEXPORT_SYMBOL(__tsan_unaligned_volatile_read##size);                   \\\n\tvoid __tsan_volatile_write##size(void *ptr);                           \\\n\tvoid __tsan_volatile_write##size(void *ptr)                            \\\n\t{                                                                      \\\n\t\tconst bool is_atomic = size <= sizeof(long long) &&            \\\n\t\t\t\t       IS_ALIGNED((unsigned long)ptr, size);   \\\n\t\tif (IS_ENABLED(CONFIG_KCSAN_IGNORE_ATOMICS) && is_atomic)      \\\n\t\t\treturn;                                                \\\n\t\tcheck_access(ptr, size,                                        \\\n\t\t\t     KCSAN_ACCESS_WRITE |                              \\\n\t\t\t\t     (is_atomic ? KCSAN_ACCESS_ATOMIC : 0),    \\\n\t\t\t     _RET_IP_);                                        \\\n\t}                                                                      \\\n\tEXPORT_SYMBOL(__tsan_volatile_write##size);                            \\\n\tvoid __tsan_unaligned_volatile_write##size(void *ptr)                  \\\n\t\t__alias(__tsan_volatile_write##size);                          \\\n\tEXPORT_SYMBOL(__tsan_unaligned_volatile_write##size)\n\nDEFINE_TSAN_VOLATILE_READ_WRITE(1);\nDEFINE_TSAN_VOLATILE_READ_WRITE(2);\nDEFINE_TSAN_VOLATILE_READ_WRITE(4);\nDEFINE_TSAN_VOLATILE_READ_WRITE(8);\nDEFINE_TSAN_VOLATILE_READ_WRITE(16);\n\n \nvoid __tsan_func_entry(void *call_pc);\nnoinline void __tsan_func_entry(void *call_pc)\n{\n\tif (!IS_ENABLED(CONFIG_KCSAN_WEAK_MEMORY))\n\t\treturn;\n\n\tadd_kcsan_stack_depth(1);\n}\nEXPORT_SYMBOL(__tsan_func_entry);\n\nvoid __tsan_func_exit(void);\nnoinline void __tsan_func_exit(void)\n{\n\tstruct kcsan_scoped_access *reorder_access;\n\n\tif (!IS_ENABLED(CONFIG_KCSAN_WEAK_MEMORY))\n\t\treturn;\n\n\treorder_access = get_reorder_access(get_ctx());\n\tif (!reorder_access)\n\t\tgoto out;\n\n\tif (get_kcsan_stack_depth() <= reorder_access->stack_depth) {\n\t\t \n\t\tcheck_access(reorder_access->ptr, reorder_access->size,\n\t\t\t     reorder_access->type, reorder_access->ip);\n\t\treorder_access->size = 0;\n\t\treorder_access->stack_depth = INT_MIN;\n\t}\nout:\n\tadd_kcsan_stack_depth(-1);\n}\nEXPORT_SYMBOL(__tsan_func_exit);\n\nvoid __tsan_init(void);\nvoid __tsan_init(void)\n{\n}\nEXPORT_SYMBOL(__tsan_init);\n\n \n\nstatic __always_inline void kcsan_atomic_builtin_memorder(int memorder)\n{\n\tif (memorder == __ATOMIC_RELEASE ||\n\t    memorder == __ATOMIC_SEQ_CST ||\n\t    memorder == __ATOMIC_ACQ_REL)\n\t\t__kcsan_release();\n}\n\n#define DEFINE_TSAN_ATOMIC_LOAD_STORE(bits)                                                        \\\n\tu##bits __tsan_atomic##bits##_load(const u##bits *ptr, int memorder);                      \\\n\tu##bits __tsan_atomic##bits##_load(const u##bits *ptr, int memorder)                       \\\n\t{                                                                                          \\\n\t\tkcsan_atomic_builtin_memorder(memorder);                                           \\\n\t\tif (!IS_ENABLED(CONFIG_KCSAN_IGNORE_ATOMICS)) {                                    \\\n\t\t\tcheck_access(ptr, bits / BITS_PER_BYTE, KCSAN_ACCESS_ATOMIC, _RET_IP_);    \\\n\t\t}                                                                                  \\\n\t\treturn __atomic_load_n(ptr, memorder);                                             \\\n\t}                                                                                          \\\n\tEXPORT_SYMBOL(__tsan_atomic##bits##_load);                                                 \\\n\tvoid __tsan_atomic##bits##_store(u##bits *ptr, u##bits v, int memorder);                   \\\n\tvoid __tsan_atomic##bits##_store(u##bits *ptr, u##bits v, int memorder)                    \\\n\t{                                                                                          \\\n\t\tkcsan_atomic_builtin_memorder(memorder);                                           \\\n\t\tif (!IS_ENABLED(CONFIG_KCSAN_IGNORE_ATOMICS)) {                                    \\\n\t\t\tcheck_access(ptr, bits / BITS_PER_BYTE,                                    \\\n\t\t\t\t     KCSAN_ACCESS_WRITE | KCSAN_ACCESS_ATOMIC, _RET_IP_);          \\\n\t\t}                                                                                  \\\n\t\t__atomic_store_n(ptr, v, memorder);                                                \\\n\t}                                                                                          \\\n\tEXPORT_SYMBOL(__tsan_atomic##bits##_store)\n\n#define DEFINE_TSAN_ATOMIC_RMW(op, bits, suffix)                                                   \\\n\tu##bits __tsan_atomic##bits##_##op(u##bits *ptr, u##bits v, int memorder);                 \\\n\tu##bits __tsan_atomic##bits##_##op(u##bits *ptr, u##bits v, int memorder)                  \\\n\t{                                                                                          \\\n\t\tkcsan_atomic_builtin_memorder(memorder);                                           \\\n\t\tif (!IS_ENABLED(CONFIG_KCSAN_IGNORE_ATOMICS)) {                                    \\\n\t\t\tcheck_access(ptr, bits / BITS_PER_BYTE,                                    \\\n\t\t\t\t     KCSAN_ACCESS_COMPOUND | KCSAN_ACCESS_WRITE |                  \\\n\t\t\t\t\t     KCSAN_ACCESS_ATOMIC, _RET_IP_);                       \\\n\t\t}                                                                                  \\\n\t\treturn __atomic_##op##suffix(ptr, v, memorder);                                    \\\n\t}                                                                                          \\\n\tEXPORT_SYMBOL(__tsan_atomic##bits##_##op)\n\n \n#define DEFINE_TSAN_ATOMIC_CMPXCHG(bits, strength, weak)                                           \\\n\tint __tsan_atomic##bits##_compare_exchange_##strength(u##bits *ptr, u##bits *exp,          \\\n\t\t\t\t\t\t\t      u##bits val, int mo, int fail_mo);   \\\n\tint __tsan_atomic##bits##_compare_exchange_##strength(u##bits *ptr, u##bits *exp,          \\\n\t\t\t\t\t\t\t      u##bits val, int mo, int fail_mo)    \\\n\t{                                                                                          \\\n\t\tkcsan_atomic_builtin_memorder(mo);                                                 \\\n\t\tif (!IS_ENABLED(CONFIG_KCSAN_IGNORE_ATOMICS)) {                                    \\\n\t\t\tcheck_access(ptr, bits / BITS_PER_BYTE,                                    \\\n\t\t\t\t     KCSAN_ACCESS_COMPOUND | KCSAN_ACCESS_WRITE |                  \\\n\t\t\t\t\t     KCSAN_ACCESS_ATOMIC, _RET_IP_);                       \\\n\t\t}                                                                                  \\\n\t\treturn __atomic_compare_exchange_n(ptr, exp, val, weak, mo, fail_mo);              \\\n\t}                                                                                          \\\n\tEXPORT_SYMBOL(__tsan_atomic##bits##_compare_exchange_##strength)\n\n#define DEFINE_TSAN_ATOMIC_CMPXCHG_VAL(bits)                                                       \\\n\tu##bits __tsan_atomic##bits##_compare_exchange_val(u##bits *ptr, u##bits exp, u##bits val, \\\n\t\t\t\t\t\t\t   int mo, int fail_mo);                   \\\n\tu##bits __tsan_atomic##bits##_compare_exchange_val(u##bits *ptr, u##bits exp, u##bits val, \\\n\t\t\t\t\t\t\t   int mo, int fail_mo)                    \\\n\t{                                                                                          \\\n\t\tkcsan_atomic_builtin_memorder(mo);                                                 \\\n\t\tif (!IS_ENABLED(CONFIG_KCSAN_IGNORE_ATOMICS)) {                                    \\\n\t\t\tcheck_access(ptr, bits / BITS_PER_BYTE,                                    \\\n\t\t\t\t     KCSAN_ACCESS_COMPOUND | KCSAN_ACCESS_WRITE |                  \\\n\t\t\t\t\t     KCSAN_ACCESS_ATOMIC, _RET_IP_);                       \\\n\t\t}                                                                                  \\\n\t\t__atomic_compare_exchange_n(ptr, &exp, val, 0, mo, fail_mo);                       \\\n\t\treturn exp;                                                                        \\\n\t}                                                                                          \\\n\tEXPORT_SYMBOL(__tsan_atomic##bits##_compare_exchange_val)\n\n#define DEFINE_TSAN_ATOMIC_OPS(bits)                                                               \\\n\tDEFINE_TSAN_ATOMIC_LOAD_STORE(bits);                                                       \\\n\tDEFINE_TSAN_ATOMIC_RMW(exchange, bits, _n);                                                \\\n\tDEFINE_TSAN_ATOMIC_RMW(fetch_add, bits, );                                                 \\\n\tDEFINE_TSAN_ATOMIC_RMW(fetch_sub, bits, );                                                 \\\n\tDEFINE_TSAN_ATOMIC_RMW(fetch_and, bits, );                                                 \\\n\tDEFINE_TSAN_ATOMIC_RMW(fetch_or, bits, );                                                  \\\n\tDEFINE_TSAN_ATOMIC_RMW(fetch_xor, bits, );                                                 \\\n\tDEFINE_TSAN_ATOMIC_RMW(fetch_nand, bits, );                                                \\\n\tDEFINE_TSAN_ATOMIC_CMPXCHG(bits, strong, 0);                                               \\\n\tDEFINE_TSAN_ATOMIC_CMPXCHG(bits, weak, 1);                                                 \\\n\tDEFINE_TSAN_ATOMIC_CMPXCHG_VAL(bits)\n\nDEFINE_TSAN_ATOMIC_OPS(8);\nDEFINE_TSAN_ATOMIC_OPS(16);\nDEFINE_TSAN_ATOMIC_OPS(32);\n#ifdef CONFIG_64BIT\nDEFINE_TSAN_ATOMIC_OPS(64);\n#endif\n\nvoid __tsan_atomic_thread_fence(int memorder);\nvoid __tsan_atomic_thread_fence(int memorder)\n{\n\tkcsan_atomic_builtin_memorder(memorder);\n\t__atomic_thread_fence(memorder);\n}\nEXPORT_SYMBOL(__tsan_atomic_thread_fence);\n\n \nvoid __tsan_atomic_signal_fence(int memorder);\nnoinline void __tsan_atomic_signal_fence(int memorder)\n{\n\tswitch (memorder) {\n\tcase __KCSAN_BARRIER_TO_SIGNAL_FENCE_mb:\n\t\t__kcsan_mb();\n\t\tbreak;\n\tcase __KCSAN_BARRIER_TO_SIGNAL_FENCE_wmb:\n\t\t__kcsan_wmb();\n\t\tbreak;\n\tcase __KCSAN_BARRIER_TO_SIGNAL_FENCE_rmb:\n\t\t__kcsan_rmb();\n\t\tbreak;\n\tcase __KCSAN_BARRIER_TO_SIGNAL_FENCE_release:\n\t\t__kcsan_release();\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\nEXPORT_SYMBOL(__tsan_atomic_signal_fence);\n\n#ifdef __HAVE_ARCH_MEMSET\nvoid *__tsan_memset(void *s, int c, size_t count);\nnoinline void *__tsan_memset(void *s, int c, size_t count)\n{\n\t \n\tsize_t check_len = min_t(size_t, count, MAX_ENCODABLE_SIZE);\n\n\tcheck_access(s, check_len, KCSAN_ACCESS_WRITE, _RET_IP_);\n\treturn memset(s, c, count);\n}\n#else\nvoid *__tsan_memset(void *s, int c, size_t count) __alias(memset);\n#endif\nEXPORT_SYMBOL(__tsan_memset);\n\n#ifdef __HAVE_ARCH_MEMMOVE\nvoid *__tsan_memmove(void *dst, const void *src, size_t len);\nnoinline void *__tsan_memmove(void *dst, const void *src, size_t len)\n{\n\tsize_t check_len = min_t(size_t, len, MAX_ENCODABLE_SIZE);\n\n\tcheck_access(dst, check_len, KCSAN_ACCESS_WRITE, _RET_IP_);\n\tcheck_access(src, check_len, 0, _RET_IP_);\n\treturn memmove(dst, src, len);\n}\n#else\nvoid *__tsan_memmove(void *dst, const void *src, size_t len) __alias(memmove);\n#endif\nEXPORT_SYMBOL(__tsan_memmove);\n\n#ifdef __HAVE_ARCH_MEMCPY\nvoid *__tsan_memcpy(void *dst, const void *src, size_t len);\nnoinline void *__tsan_memcpy(void *dst, const void *src, size_t len)\n{\n\tsize_t check_len = min_t(size_t, len, MAX_ENCODABLE_SIZE);\n\n\tcheck_access(dst, check_len, KCSAN_ACCESS_WRITE, _RET_IP_);\n\tcheck_access(src, check_len, 0, _RET_IP_);\n\treturn memcpy(dst, src, len);\n}\n#else\nvoid *__tsan_memcpy(void *dst, const void *src, size_t len) __alias(memcpy);\n#endif\nEXPORT_SYMBOL(__tsan_memcpy);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}