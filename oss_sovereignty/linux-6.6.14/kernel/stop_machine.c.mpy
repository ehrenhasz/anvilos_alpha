{
  "module_name": "stop_machine.c",
  "hash_id": "3347655044cac20a18b2cb18beeff3f9334d2d46d5eded3fc09364213e8bff80",
  "original_prompt": "Ingested from linux-6.6.14/kernel/stop_machine.c",
  "human_readable_source": "\n \n#include <linux/compiler.h>\n#include <linux/completion.h>\n#include <linux/cpu.h>\n#include <linux/init.h>\n#include <linux/kthread.h>\n#include <linux/export.h>\n#include <linux/percpu.h>\n#include <linux/sched.h>\n#include <linux/stop_machine.h>\n#include <linux/interrupt.h>\n#include <linux/kallsyms.h>\n#include <linux/smpboot.h>\n#include <linux/atomic.h>\n#include <linux/nmi.h>\n#include <linux/sched/wake_q.h>\n\n \nstruct cpu_stop_done {\n\tatomic_t\t\tnr_todo;\t \n\tint\t\t\tret;\t\t \n\tstruct completion\tcompletion;\t \n};\n\n \nstruct cpu_stopper {\n\tstruct task_struct\t*thread;\n\n\traw_spinlock_t\t\tlock;\n\tbool\t\t\tenabled;\t \n\tstruct list_head\tworks;\t\t \n\n\tstruct cpu_stop_work\tstop_work;\t \n\tunsigned long\t\tcaller;\n\tcpu_stop_fn_t\t\tfn;\n};\n\nstatic DEFINE_PER_CPU(struct cpu_stopper, cpu_stopper);\nstatic bool stop_machine_initialized = false;\n\nvoid print_stop_info(const char *log_lvl, struct task_struct *task)\n{\n\t \n\tstruct cpu_stopper *stopper = per_cpu_ptr(&cpu_stopper, task_cpu(task));\n\n\tif (task != stopper->thread)\n\t\treturn;\n\n\tprintk(\"%sStopper: %pS <- %pS\\n\", log_lvl, stopper->fn, (void *)stopper->caller);\n}\n\n \nstatic DEFINE_MUTEX(stop_cpus_mutex);\nstatic bool stop_cpus_in_progress;\n\nstatic void cpu_stop_init_done(struct cpu_stop_done *done, unsigned int nr_todo)\n{\n\tmemset(done, 0, sizeof(*done));\n\tatomic_set(&done->nr_todo, nr_todo);\n\tinit_completion(&done->completion);\n}\n\n \nstatic void cpu_stop_signal_done(struct cpu_stop_done *done)\n{\n\tif (atomic_dec_and_test(&done->nr_todo))\n\t\tcomplete(&done->completion);\n}\n\nstatic void __cpu_stop_queue_work(struct cpu_stopper *stopper,\n\t\t\t\t\tstruct cpu_stop_work *work,\n\t\t\t\t\tstruct wake_q_head *wakeq)\n{\n\tlist_add_tail(&work->list, &stopper->works);\n\twake_q_add(wakeq, stopper->thread);\n}\n\n \nstatic bool cpu_stop_queue_work(unsigned int cpu, struct cpu_stop_work *work)\n{\n\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\n\tDEFINE_WAKE_Q(wakeq);\n\tunsigned long flags;\n\tbool enabled;\n\n\tpreempt_disable();\n\traw_spin_lock_irqsave(&stopper->lock, flags);\n\tenabled = stopper->enabled;\n\tif (enabled)\n\t\t__cpu_stop_queue_work(stopper, work, &wakeq);\n\telse if (work->done)\n\t\tcpu_stop_signal_done(work->done);\n\traw_spin_unlock_irqrestore(&stopper->lock, flags);\n\n\twake_up_q(&wakeq);\n\tpreempt_enable();\n\n\treturn enabled;\n}\n\n \nint stop_one_cpu(unsigned int cpu, cpu_stop_fn_t fn, void *arg)\n{\n\tstruct cpu_stop_done done;\n\tstruct cpu_stop_work work = { .fn = fn, .arg = arg, .done = &done, .caller = _RET_IP_ };\n\n\tcpu_stop_init_done(&done, 1);\n\tif (!cpu_stop_queue_work(cpu, &work))\n\t\treturn -ENOENT;\n\t \n\tcond_resched();\n\twait_for_completion(&done.completion);\n\treturn done.ret;\n}\n\n \nenum multi_stop_state {\n\t \n\tMULTI_STOP_NONE,\n\t \n\tMULTI_STOP_PREPARE,\n\t \n\tMULTI_STOP_DISABLE_IRQ,\n\t \n\tMULTI_STOP_RUN,\n\t \n\tMULTI_STOP_EXIT,\n};\n\nstruct multi_stop_data {\n\tcpu_stop_fn_t\t\tfn;\n\tvoid\t\t\t*data;\n\t \n\tunsigned int\t\tnum_threads;\n\tconst struct cpumask\t*active_cpus;\n\n\tenum multi_stop_state\tstate;\n\tatomic_t\t\tthread_ack;\n};\n\nstatic void set_state(struct multi_stop_data *msdata,\n\t\t      enum multi_stop_state newstate)\n{\n\t \n\tatomic_set(&msdata->thread_ack, msdata->num_threads);\n\tsmp_wmb();\n\tWRITE_ONCE(msdata->state, newstate);\n}\n\n \nstatic void ack_state(struct multi_stop_data *msdata)\n{\n\tif (atomic_dec_and_test(&msdata->thread_ack))\n\t\tset_state(msdata, msdata->state + 1);\n}\n\nnotrace void __weak stop_machine_yield(const struct cpumask *cpumask)\n{\n\tcpu_relax();\n}\n\n \nstatic int multi_cpu_stop(void *data)\n{\n\tstruct multi_stop_data *msdata = data;\n\tenum multi_stop_state newstate, curstate = MULTI_STOP_NONE;\n\tint cpu = smp_processor_id(), err = 0;\n\tconst struct cpumask *cpumask;\n\tunsigned long flags;\n\tbool is_active;\n\n\t \n\tlocal_save_flags(flags);\n\n\tif (!msdata->active_cpus) {\n\t\tcpumask = cpu_online_mask;\n\t\tis_active = cpu == cpumask_first(cpumask);\n\t} else {\n\t\tcpumask = msdata->active_cpus;\n\t\tis_active = cpumask_test_cpu(cpu, cpumask);\n\t}\n\n\t \n\tdo {\n\t\t \n\t\tstop_machine_yield(cpumask);\n\t\tnewstate = READ_ONCE(msdata->state);\n\t\tif (newstate != curstate) {\n\t\t\tcurstate = newstate;\n\t\t\tswitch (curstate) {\n\t\t\tcase MULTI_STOP_DISABLE_IRQ:\n\t\t\t\tlocal_irq_disable();\n\t\t\t\thard_irq_disable();\n\t\t\t\tbreak;\n\t\t\tcase MULTI_STOP_RUN:\n\t\t\t\tif (is_active)\n\t\t\t\t\terr = msdata->fn(msdata->data);\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tack_state(msdata);\n\t\t} else if (curstate > MULTI_STOP_PREPARE) {\n\t\t\t \n\t\t\ttouch_nmi_watchdog();\n\t\t}\n\t\trcu_momentary_dyntick_idle();\n\t} while (curstate != MULTI_STOP_EXIT);\n\n\tlocal_irq_restore(flags);\n\treturn err;\n}\n\nstatic int cpu_stop_queue_two_works(int cpu1, struct cpu_stop_work *work1,\n\t\t\t\t    int cpu2, struct cpu_stop_work *work2)\n{\n\tstruct cpu_stopper *stopper1 = per_cpu_ptr(&cpu_stopper, cpu1);\n\tstruct cpu_stopper *stopper2 = per_cpu_ptr(&cpu_stopper, cpu2);\n\tDEFINE_WAKE_Q(wakeq);\n\tint err;\n\nretry:\n\t \n\tpreempt_disable();\n\traw_spin_lock_irq(&stopper1->lock);\n\traw_spin_lock_nested(&stopper2->lock, SINGLE_DEPTH_NESTING);\n\n\tif (!stopper1->enabled || !stopper2->enabled) {\n\t\terr = -ENOENT;\n\t\tgoto unlock;\n\t}\n\n\t \n\tif (unlikely(stop_cpus_in_progress)) {\n\t\terr = -EDEADLK;\n\t\tgoto unlock;\n\t}\n\n\terr = 0;\n\t__cpu_stop_queue_work(stopper1, work1, &wakeq);\n\t__cpu_stop_queue_work(stopper2, work2, &wakeq);\n\nunlock:\n\traw_spin_unlock(&stopper2->lock);\n\traw_spin_unlock_irq(&stopper1->lock);\n\n\tif (unlikely(err == -EDEADLK)) {\n\t\tpreempt_enable();\n\n\t\twhile (stop_cpus_in_progress)\n\t\t\tcpu_relax();\n\n\t\tgoto retry;\n\t}\n\n\twake_up_q(&wakeq);\n\tpreempt_enable();\n\n\treturn err;\n}\n \nint stop_two_cpus(unsigned int cpu1, unsigned int cpu2, cpu_stop_fn_t fn, void *arg)\n{\n\tstruct cpu_stop_done done;\n\tstruct cpu_stop_work work1, work2;\n\tstruct multi_stop_data msdata;\n\n\tmsdata = (struct multi_stop_data){\n\t\t.fn = fn,\n\t\t.data = arg,\n\t\t.num_threads = 2,\n\t\t.active_cpus = cpumask_of(cpu1),\n\t};\n\n\twork1 = work2 = (struct cpu_stop_work){\n\t\t.fn = multi_cpu_stop,\n\t\t.arg = &msdata,\n\t\t.done = &done,\n\t\t.caller = _RET_IP_,\n\t};\n\n\tcpu_stop_init_done(&done, 2);\n\tset_state(&msdata, MULTI_STOP_PREPARE);\n\n\tif (cpu1 > cpu2)\n\t\tswap(cpu1, cpu2);\n\tif (cpu_stop_queue_two_works(cpu1, &work1, cpu2, &work2))\n\t\treturn -ENOENT;\n\n\twait_for_completion(&done.completion);\n\treturn done.ret;\n}\n\n \nbool stop_one_cpu_nowait(unsigned int cpu, cpu_stop_fn_t fn, void *arg,\n\t\t\tstruct cpu_stop_work *work_buf)\n{\n\t*work_buf = (struct cpu_stop_work){ .fn = fn, .arg = arg, .caller = _RET_IP_, };\n\treturn cpu_stop_queue_work(cpu, work_buf);\n}\n\nstatic bool queue_stop_cpus_work(const struct cpumask *cpumask,\n\t\t\t\t cpu_stop_fn_t fn, void *arg,\n\t\t\t\t struct cpu_stop_done *done)\n{\n\tstruct cpu_stop_work *work;\n\tunsigned int cpu;\n\tbool queued = false;\n\n\t \n\tpreempt_disable();\n\tstop_cpus_in_progress = true;\n\tbarrier();\n\tfor_each_cpu(cpu, cpumask) {\n\t\twork = &per_cpu(cpu_stopper.stop_work, cpu);\n\t\twork->fn = fn;\n\t\twork->arg = arg;\n\t\twork->done = done;\n\t\twork->caller = _RET_IP_;\n\t\tif (cpu_stop_queue_work(cpu, work))\n\t\t\tqueued = true;\n\t}\n\tbarrier();\n\tstop_cpus_in_progress = false;\n\tpreempt_enable();\n\n\treturn queued;\n}\n\nstatic int __stop_cpus(const struct cpumask *cpumask,\n\t\t       cpu_stop_fn_t fn, void *arg)\n{\n\tstruct cpu_stop_done done;\n\n\tcpu_stop_init_done(&done, cpumask_weight(cpumask));\n\tif (!queue_stop_cpus_work(cpumask, fn, arg, &done))\n\t\treturn -ENOENT;\n\twait_for_completion(&done.completion);\n\treturn done.ret;\n}\n\n \nstatic int stop_cpus(const struct cpumask *cpumask, cpu_stop_fn_t fn, void *arg)\n{\n\tint ret;\n\n\t \n\tmutex_lock(&stop_cpus_mutex);\n\tret = __stop_cpus(cpumask, fn, arg);\n\tmutex_unlock(&stop_cpus_mutex);\n\treturn ret;\n}\n\nstatic int cpu_stop_should_run(unsigned int cpu)\n{\n\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\n\tunsigned long flags;\n\tint run;\n\n\traw_spin_lock_irqsave(&stopper->lock, flags);\n\trun = !list_empty(&stopper->works);\n\traw_spin_unlock_irqrestore(&stopper->lock, flags);\n\treturn run;\n}\n\nstatic void cpu_stopper_thread(unsigned int cpu)\n{\n\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\n\tstruct cpu_stop_work *work;\n\nrepeat:\n\twork = NULL;\n\traw_spin_lock_irq(&stopper->lock);\n\tif (!list_empty(&stopper->works)) {\n\t\twork = list_first_entry(&stopper->works,\n\t\t\t\t\tstruct cpu_stop_work, list);\n\t\tlist_del_init(&work->list);\n\t}\n\traw_spin_unlock_irq(&stopper->lock);\n\n\tif (work) {\n\t\tcpu_stop_fn_t fn = work->fn;\n\t\tvoid *arg = work->arg;\n\t\tstruct cpu_stop_done *done = work->done;\n\t\tint ret;\n\n\t\t \n\t\tstopper->caller = work->caller;\n\t\tstopper->fn = fn;\n\t\tpreempt_count_inc();\n\t\tret = fn(arg);\n\t\tif (done) {\n\t\t\tif (ret)\n\t\t\t\tdone->ret = ret;\n\t\t\tcpu_stop_signal_done(done);\n\t\t}\n\t\tpreempt_count_dec();\n\t\tstopper->fn = NULL;\n\t\tstopper->caller = 0;\n\t\tWARN_ONCE(preempt_count(),\n\t\t\t  \"cpu_stop: %ps(%p) leaked preempt count\\n\", fn, arg);\n\t\tgoto repeat;\n\t}\n}\n\nvoid stop_machine_park(int cpu)\n{\n\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\n\t \n\tstopper->enabled = false;\n\tkthread_park(stopper->thread);\n}\n\nstatic void cpu_stop_create(unsigned int cpu)\n{\n\tsched_set_stop_task(cpu, per_cpu(cpu_stopper.thread, cpu));\n}\n\nstatic void cpu_stop_park(unsigned int cpu)\n{\n\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\n\n\tWARN_ON(!list_empty(&stopper->works));\n}\n\nvoid stop_machine_unpark(int cpu)\n{\n\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\n\n\tstopper->enabled = true;\n\tkthread_unpark(stopper->thread);\n}\n\nstatic struct smp_hotplug_thread cpu_stop_threads = {\n\t.store\t\t\t= &cpu_stopper.thread,\n\t.thread_should_run\t= cpu_stop_should_run,\n\t.thread_fn\t\t= cpu_stopper_thread,\n\t.thread_comm\t\t= \"migration/%u\",\n\t.create\t\t\t= cpu_stop_create,\n\t.park\t\t\t= cpu_stop_park,\n\t.selfparking\t\t= true,\n};\n\nstatic int __init cpu_stop_init(void)\n{\n\tunsigned int cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\n\n\t\traw_spin_lock_init(&stopper->lock);\n\t\tINIT_LIST_HEAD(&stopper->works);\n\t}\n\n\tBUG_ON(smpboot_register_percpu_thread(&cpu_stop_threads));\n\tstop_machine_unpark(raw_smp_processor_id());\n\tstop_machine_initialized = true;\n\treturn 0;\n}\nearly_initcall(cpu_stop_init);\n\nint stop_machine_cpuslocked(cpu_stop_fn_t fn, void *data,\n\t\t\t    const struct cpumask *cpus)\n{\n\tstruct multi_stop_data msdata = {\n\t\t.fn = fn,\n\t\t.data = data,\n\t\t.num_threads = num_online_cpus(),\n\t\t.active_cpus = cpus,\n\t};\n\n\tlockdep_assert_cpus_held();\n\n\tif (!stop_machine_initialized) {\n\t\t \n\t\tunsigned long flags;\n\t\tint ret;\n\n\t\tWARN_ON_ONCE(msdata.num_threads != 1);\n\n\t\tlocal_irq_save(flags);\n\t\thard_irq_disable();\n\t\tret = (*fn)(data);\n\t\tlocal_irq_restore(flags);\n\n\t\treturn ret;\n\t}\n\n\t \n\tset_state(&msdata, MULTI_STOP_PREPARE);\n\treturn stop_cpus(cpu_online_mask, multi_cpu_stop, &msdata);\n}\n\nint stop_machine(cpu_stop_fn_t fn, void *data, const struct cpumask *cpus)\n{\n\tint ret;\n\n\t \n\tcpus_read_lock();\n\tret = stop_machine_cpuslocked(fn, data, cpus);\n\tcpus_read_unlock();\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(stop_machine);\n\n#ifdef CONFIG_SCHED_SMT\nint stop_core_cpuslocked(unsigned int cpu, cpu_stop_fn_t fn, void *data)\n{\n\tconst struct cpumask *smt_mask = cpu_smt_mask(cpu);\n\n\tstruct multi_stop_data msdata = {\n\t\t.fn = fn,\n\t\t.data = data,\n\t\t.num_threads = cpumask_weight(smt_mask),\n\t\t.active_cpus = smt_mask,\n\t};\n\n\tlockdep_assert_cpus_held();\n\n\t \n\tset_state(&msdata, MULTI_STOP_PREPARE);\n\treturn stop_cpus(smt_mask, multi_cpu_stop, &msdata);\n}\nEXPORT_SYMBOL_GPL(stop_core_cpuslocked);\n#endif\n\n \nint stop_machine_from_inactive_cpu(cpu_stop_fn_t fn, void *data,\n\t\t\t\t  const struct cpumask *cpus)\n{\n\tstruct multi_stop_data msdata = { .fn = fn, .data = data,\n\t\t\t\t\t    .active_cpus = cpus };\n\tstruct cpu_stop_done done;\n\tint ret;\n\n\t \n\tBUG_ON(cpu_active(raw_smp_processor_id()));\n\tmsdata.num_threads = num_active_cpus() + 1;\t \n\n\t \n\twhile (!mutex_trylock(&stop_cpus_mutex))\n\t\tcpu_relax();\n\n\t \n\tset_state(&msdata, MULTI_STOP_PREPARE);\n\tcpu_stop_init_done(&done, num_active_cpus());\n\tqueue_stop_cpus_work(cpu_active_mask, multi_cpu_stop, &msdata,\n\t\t\t     &done);\n\tret = multi_cpu_stop(&msdata);\n\n\t \n\twhile (!completion_done(&done.completion))\n\t\tcpu_relax();\n\n\tmutex_unlock(&stop_cpus_mutex);\n\treturn ret ?: done.ret;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}