{
  "module_name": "cpu.c",
  "hash_id": "9fda7b04103e7075596df3a1dfbda6a60aac3391d2ba0b240b2c57a32729d9a4",
  "original_prompt": "Ingested from linux-6.6.14/kernel/cpu.c",
  "human_readable_source": " \n#include <linux/sched/mm.h>\n#include <linux/proc_fs.h>\n#include <linux/smp.h>\n#include <linux/init.h>\n#include <linux/notifier.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/task.h>\n#include <linux/sched/smt.h>\n#include <linux/unistd.h>\n#include <linux/cpu.h>\n#include <linux/oom.h>\n#include <linux/rcupdate.h>\n#include <linux/delay.h>\n#include <linux/export.h>\n#include <linux/bug.h>\n#include <linux/kthread.h>\n#include <linux/stop_machine.h>\n#include <linux/mutex.h>\n#include <linux/gfp.h>\n#include <linux/suspend.h>\n#include <linux/lockdep.h>\n#include <linux/tick.h>\n#include <linux/irq.h>\n#include <linux/nmi.h>\n#include <linux/smpboot.h>\n#include <linux/relay.h>\n#include <linux/slab.h>\n#include <linux/scs.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/cpuset.h>\n#include <linux/random.h>\n#include <linux/cc_platform.h>\n\n#include <trace/events/power.h>\n#define CREATE_TRACE_POINTS\n#include <trace/events/cpuhp.h>\n\n#include \"smpboot.h\"\n\n \nstruct cpuhp_cpu_state {\n\tenum cpuhp_state\tstate;\n\tenum cpuhp_state\ttarget;\n\tenum cpuhp_state\tfail;\n#ifdef CONFIG_SMP\n\tstruct task_struct\t*thread;\n\tbool\t\t\tshould_run;\n\tbool\t\t\trollback;\n\tbool\t\t\tsingle;\n\tbool\t\t\tbringup;\n\tstruct hlist_node\t*node;\n\tstruct hlist_node\t*last;\n\tenum cpuhp_state\tcb_state;\n\tint\t\t\tresult;\n\tatomic_t\t\tap_sync_state;\n\tstruct completion\tdone_up;\n\tstruct completion\tdone_down;\n#endif\n};\n\nstatic DEFINE_PER_CPU(struct cpuhp_cpu_state, cpuhp_state) = {\n\t.fail = CPUHP_INVALID,\n};\n\n#ifdef CONFIG_SMP\ncpumask_t cpus_booted_once_mask;\n#endif\n\n#if defined(CONFIG_LOCKDEP) && defined(CONFIG_SMP)\nstatic struct lockdep_map cpuhp_state_up_map =\n\tSTATIC_LOCKDEP_MAP_INIT(\"cpuhp_state-up\", &cpuhp_state_up_map);\nstatic struct lockdep_map cpuhp_state_down_map =\n\tSTATIC_LOCKDEP_MAP_INIT(\"cpuhp_state-down\", &cpuhp_state_down_map);\n\n\nstatic inline void cpuhp_lock_acquire(bool bringup)\n{\n\tlock_map_acquire(bringup ? &cpuhp_state_up_map : &cpuhp_state_down_map);\n}\n\nstatic inline void cpuhp_lock_release(bool bringup)\n{\n\tlock_map_release(bringup ? &cpuhp_state_up_map : &cpuhp_state_down_map);\n}\n#else\n\nstatic inline void cpuhp_lock_acquire(bool bringup) { }\nstatic inline void cpuhp_lock_release(bool bringup) { }\n\n#endif\n\n \nstruct cpuhp_step {\n\tconst char\t\t*name;\n\tunion {\n\t\tint\t\t(*single)(unsigned int cpu);\n\t\tint\t\t(*multi)(unsigned int cpu,\n\t\t\t\t\t struct hlist_node *node);\n\t} startup;\n\tunion {\n\t\tint\t\t(*single)(unsigned int cpu);\n\t\tint\t\t(*multi)(unsigned int cpu,\n\t\t\t\t\t struct hlist_node *node);\n\t} teardown;\n\t \n\tstruct hlist_head\tlist;\n\t \n\tbool\t\t\tcant_stop;\n\tbool\t\t\tmulti_instance;\n};\n\nstatic DEFINE_MUTEX(cpuhp_state_mutex);\nstatic struct cpuhp_step cpuhp_hp_states[];\n\nstatic struct cpuhp_step *cpuhp_get_step(enum cpuhp_state state)\n{\n\treturn cpuhp_hp_states + state;\n}\n\nstatic bool cpuhp_step_empty(bool bringup, struct cpuhp_step *step)\n{\n\treturn bringup ? !step->startup.single : !step->teardown.single;\n}\n\n \nstatic int cpuhp_invoke_callback(unsigned int cpu, enum cpuhp_state state,\n\t\t\t\t bool bringup, struct hlist_node *node,\n\t\t\t\t struct hlist_node **lastp)\n{\n\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\n\tstruct cpuhp_step *step = cpuhp_get_step(state);\n\tint (*cbm)(unsigned int cpu, struct hlist_node *node);\n\tint (*cb)(unsigned int cpu);\n\tint ret, cnt;\n\n\tif (st->fail == state) {\n\t\tst->fail = CPUHP_INVALID;\n\t\treturn -EAGAIN;\n\t}\n\n\tif (cpuhp_step_empty(bringup, step)) {\n\t\tWARN_ON_ONCE(1);\n\t\treturn 0;\n\t}\n\n\tif (!step->multi_instance) {\n\t\tWARN_ON_ONCE(lastp && *lastp);\n\t\tcb = bringup ? step->startup.single : step->teardown.single;\n\n\t\ttrace_cpuhp_enter(cpu, st->target, state, cb);\n\t\tret = cb(cpu);\n\t\ttrace_cpuhp_exit(cpu, st->state, state, ret);\n\t\treturn ret;\n\t}\n\tcbm = bringup ? step->startup.multi : step->teardown.multi;\n\n\t \n\tif (node) {\n\t\tWARN_ON_ONCE(lastp && *lastp);\n\t\ttrace_cpuhp_multi_enter(cpu, st->target, state, cbm, node);\n\t\tret = cbm(cpu, node);\n\t\ttrace_cpuhp_exit(cpu, st->state, state, ret);\n\t\treturn ret;\n\t}\n\n\t \n\tcnt = 0;\n\thlist_for_each(node, &step->list) {\n\t\tif (lastp && node == *lastp)\n\t\t\tbreak;\n\n\t\ttrace_cpuhp_multi_enter(cpu, st->target, state, cbm, node);\n\t\tret = cbm(cpu, node);\n\t\ttrace_cpuhp_exit(cpu, st->state, state, ret);\n\t\tif (ret) {\n\t\t\tif (!lastp)\n\t\t\t\tgoto err;\n\n\t\t\t*lastp = node;\n\t\t\treturn ret;\n\t\t}\n\t\tcnt++;\n\t}\n\tif (lastp)\n\t\t*lastp = NULL;\n\treturn 0;\nerr:\n\t \n\tcbm = !bringup ? step->startup.multi : step->teardown.multi;\n\tif (!cbm)\n\t\treturn ret;\n\n\thlist_for_each(node, &step->list) {\n\t\tif (!cnt--)\n\t\t\tbreak;\n\n\t\ttrace_cpuhp_multi_enter(cpu, st->target, state, cbm, node);\n\t\tret = cbm(cpu, node);\n\t\ttrace_cpuhp_exit(cpu, st->state, state, ret);\n\t\t \n\t\tWARN_ON_ONCE(ret);\n\t}\n\treturn ret;\n}\n\n#ifdef CONFIG_SMP\nstatic bool cpuhp_is_ap_state(enum cpuhp_state state)\n{\n\t \n\treturn state > CPUHP_BRINGUP_CPU && state != CPUHP_TEARDOWN_CPU;\n}\n\nstatic inline void wait_for_ap_thread(struct cpuhp_cpu_state *st, bool bringup)\n{\n\tstruct completion *done = bringup ? &st->done_up : &st->done_down;\n\twait_for_completion(done);\n}\n\nstatic inline void complete_ap_thread(struct cpuhp_cpu_state *st, bool bringup)\n{\n\tstruct completion *done = bringup ? &st->done_up : &st->done_down;\n\tcomplete(done);\n}\n\n \nstatic bool cpuhp_is_atomic_state(enum cpuhp_state state)\n{\n\treturn CPUHP_AP_IDLE_DEAD <= state && state < CPUHP_AP_ONLINE;\n}\n\n \nenum cpuhp_sync_state {\n\tSYNC_STATE_DEAD,\n\tSYNC_STATE_KICKED,\n\tSYNC_STATE_SHOULD_DIE,\n\tSYNC_STATE_ALIVE,\n\tSYNC_STATE_SHOULD_ONLINE,\n\tSYNC_STATE_ONLINE,\n};\n\n#ifdef CONFIG_HOTPLUG_CORE_SYNC\n \nstatic inline void cpuhp_ap_update_sync_state(enum cpuhp_sync_state state)\n{\n\tatomic_t *st = this_cpu_ptr(&cpuhp_state.ap_sync_state);\n\n\t(void)atomic_xchg(st, state);\n}\n\nvoid __weak arch_cpuhp_sync_state_poll(void) { cpu_relax(); }\n\nstatic bool cpuhp_wait_for_sync_state(unsigned int cpu, enum cpuhp_sync_state state,\n\t\t\t\t      enum cpuhp_sync_state next_state)\n{\n\tatomic_t *st = per_cpu_ptr(&cpuhp_state.ap_sync_state, cpu);\n\tktime_t now, end, start = ktime_get();\n\tint sync;\n\n\tend = start + 10ULL * NSEC_PER_SEC;\n\n\tsync = atomic_read(st);\n\twhile (1) {\n\t\tif (sync == state) {\n\t\t\tif (!atomic_try_cmpxchg(st, &sync, next_state))\n\t\t\t\tcontinue;\n\t\t\treturn true;\n\t\t}\n\n\t\tnow = ktime_get();\n\t\tif (now > end) {\n\t\t\t \n\t\t\treturn false;\n\t\t} else if (now - start < NSEC_PER_MSEC) {\n\t\t\t \n\t\t\tarch_cpuhp_sync_state_poll();\n\t\t} else {\n\t\t\tusleep_range_state(USEC_PER_MSEC, 2 * USEC_PER_MSEC, TASK_UNINTERRUPTIBLE);\n\t\t}\n\t\tsync = atomic_read(st);\n\t}\n\treturn true;\n}\n#else   \nstatic inline void cpuhp_ap_update_sync_state(enum cpuhp_sync_state state) { }\n#endif  \n\n#ifdef CONFIG_HOTPLUG_CORE_SYNC_DEAD\n \nvoid cpuhp_ap_report_dead(void)\n{\n\tcpuhp_ap_update_sync_state(SYNC_STATE_DEAD);\n}\n\nvoid __weak arch_cpuhp_cleanup_dead_cpu(unsigned int cpu) { }\n\n \nstatic void cpuhp_bp_sync_dead(unsigned int cpu)\n{\n\tatomic_t *st = per_cpu_ptr(&cpuhp_state.ap_sync_state, cpu);\n\tint sync = atomic_read(st);\n\n\tdo {\n\t\t \n\t\tif (sync == SYNC_STATE_DEAD)\n\t\t\tbreak;\n\t} while (!atomic_try_cmpxchg(st, &sync, SYNC_STATE_SHOULD_DIE));\n\n\tif (cpuhp_wait_for_sync_state(cpu, SYNC_STATE_DEAD, SYNC_STATE_DEAD)) {\n\t\t \n\t\tarch_cpuhp_cleanup_dead_cpu(cpu);\n\t\treturn;\n\t}\n\n\t \n\tpr_err(\"CPU%u failed to report dead state\\n\", cpu);\n}\n#else  \nstatic inline void cpuhp_bp_sync_dead(unsigned int cpu) { }\n#endif  \n\n#ifdef CONFIG_HOTPLUG_CORE_SYNC_FULL\n \nvoid cpuhp_ap_sync_alive(void)\n{\n\tatomic_t *st = this_cpu_ptr(&cpuhp_state.ap_sync_state);\n\n\tcpuhp_ap_update_sync_state(SYNC_STATE_ALIVE);\n\n\t \n\twhile (atomic_read(st) != SYNC_STATE_SHOULD_ONLINE)\n\t\tcpu_relax();\n}\n\nstatic bool cpuhp_can_boot_ap(unsigned int cpu)\n{\n\tatomic_t *st = per_cpu_ptr(&cpuhp_state.ap_sync_state, cpu);\n\tint sync = atomic_read(st);\n\nagain:\n\tswitch (sync) {\n\tcase SYNC_STATE_DEAD:\n\t\t \n\t\tbreak;\n\tcase SYNC_STATE_KICKED:\n\t\t \n\t\tbreak;\n\tcase SYNC_STATE_ALIVE:\n\t\t \n\t\tbreak;\n\tdefault:\n\t\t \n\t\treturn false;\n\t}\n\n\t \n\tif (!atomic_try_cmpxchg(st, &sync, SYNC_STATE_KICKED))\n\t\tgoto again;\n\n\treturn true;\n}\n\nvoid __weak arch_cpuhp_cleanup_kick_cpu(unsigned int cpu) { }\n\n \nstatic int cpuhp_bp_sync_alive(unsigned int cpu)\n{\n\tint ret = 0;\n\n\tif (!IS_ENABLED(CONFIG_HOTPLUG_CORE_SYNC_FULL))\n\t\treturn 0;\n\n\tif (!cpuhp_wait_for_sync_state(cpu, SYNC_STATE_ALIVE, SYNC_STATE_SHOULD_ONLINE)) {\n\t\tpr_err(\"CPU%u failed to report alive state\\n\", cpu);\n\t\tret = -EIO;\n\t}\n\n\t \n\tarch_cpuhp_cleanup_kick_cpu(cpu);\n\treturn ret;\n}\n#else  \nstatic inline int cpuhp_bp_sync_alive(unsigned int cpu) { return 0; }\nstatic inline bool cpuhp_can_boot_ap(unsigned int cpu) { return true; }\n#endif  \n\n \nstatic DEFINE_MUTEX(cpu_add_remove_lock);\nbool cpuhp_tasks_frozen;\nEXPORT_SYMBOL_GPL(cpuhp_tasks_frozen);\n\n \nvoid cpu_maps_update_begin(void)\n{\n\tmutex_lock(&cpu_add_remove_lock);\n}\n\nvoid cpu_maps_update_done(void)\n{\n\tmutex_unlock(&cpu_add_remove_lock);\n}\n\n \nstatic int cpu_hotplug_disabled;\n\n#ifdef CONFIG_HOTPLUG_CPU\n\nDEFINE_STATIC_PERCPU_RWSEM(cpu_hotplug_lock);\n\nvoid cpus_read_lock(void)\n{\n\tpercpu_down_read(&cpu_hotplug_lock);\n}\nEXPORT_SYMBOL_GPL(cpus_read_lock);\n\nint cpus_read_trylock(void)\n{\n\treturn percpu_down_read_trylock(&cpu_hotplug_lock);\n}\nEXPORT_SYMBOL_GPL(cpus_read_trylock);\n\nvoid cpus_read_unlock(void)\n{\n\tpercpu_up_read(&cpu_hotplug_lock);\n}\nEXPORT_SYMBOL_GPL(cpus_read_unlock);\n\nvoid cpus_write_lock(void)\n{\n\tpercpu_down_write(&cpu_hotplug_lock);\n}\n\nvoid cpus_write_unlock(void)\n{\n\tpercpu_up_write(&cpu_hotplug_lock);\n}\n\nvoid lockdep_assert_cpus_held(void)\n{\n\t \n\tif (system_state < SYSTEM_RUNNING)\n\t\treturn;\n\n\tpercpu_rwsem_assert_held(&cpu_hotplug_lock);\n}\n\n#ifdef CONFIG_LOCKDEP\nint lockdep_is_cpus_held(void)\n{\n\treturn percpu_rwsem_is_held(&cpu_hotplug_lock);\n}\n#endif\n\nstatic void lockdep_acquire_cpus_lock(void)\n{\n\trwsem_acquire(&cpu_hotplug_lock.dep_map, 0, 0, _THIS_IP_);\n}\n\nstatic void lockdep_release_cpus_lock(void)\n{\n\trwsem_release(&cpu_hotplug_lock.dep_map, _THIS_IP_);\n}\n\n \nvoid cpu_hotplug_disable(void)\n{\n\tcpu_maps_update_begin();\n\tcpu_hotplug_disabled++;\n\tcpu_maps_update_done();\n}\nEXPORT_SYMBOL_GPL(cpu_hotplug_disable);\n\nstatic void __cpu_hotplug_enable(void)\n{\n\tif (WARN_ONCE(!cpu_hotplug_disabled, \"Unbalanced cpu hotplug enable\\n\"))\n\t\treturn;\n\tcpu_hotplug_disabled--;\n}\n\nvoid cpu_hotplug_enable(void)\n{\n\tcpu_maps_update_begin();\n\t__cpu_hotplug_enable();\n\tcpu_maps_update_done();\n}\nEXPORT_SYMBOL_GPL(cpu_hotplug_enable);\n\n#else\n\nstatic void lockdep_acquire_cpus_lock(void)\n{\n}\n\nstatic void lockdep_release_cpus_lock(void)\n{\n}\n\n#endif\t \n\n \nvoid __weak arch_smt_update(void) { }\n\n#ifdef CONFIG_HOTPLUG_SMT\n\nenum cpuhp_smt_control cpu_smt_control __read_mostly = CPU_SMT_ENABLED;\nstatic unsigned int cpu_smt_max_threads __ro_after_init;\nunsigned int cpu_smt_num_threads __read_mostly = UINT_MAX;\n\nvoid __init cpu_smt_disable(bool force)\n{\n\tif (!cpu_smt_possible())\n\t\treturn;\n\n\tif (force) {\n\t\tpr_info(\"SMT: Force disabled\\n\");\n\t\tcpu_smt_control = CPU_SMT_FORCE_DISABLED;\n\t} else {\n\t\tpr_info(\"SMT: disabled\\n\");\n\t\tcpu_smt_control = CPU_SMT_DISABLED;\n\t}\n\tcpu_smt_num_threads = 1;\n}\n\n \nvoid __init cpu_smt_set_num_threads(unsigned int num_threads,\n\t\t\t\t    unsigned int max_threads)\n{\n\tWARN_ON(!num_threads || (num_threads > max_threads));\n\n\tif (max_threads == 1)\n\t\tcpu_smt_control = CPU_SMT_NOT_SUPPORTED;\n\n\tcpu_smt_max_threads = max_threads;\n\n\t \n\tif (cpu_smt_control != CPU_SMT_ENABLED)\n\t\tcpu_smt_num_threads = 1;\n\telse if (num_threads < cpu_smt_num_threads)\n\t\tcpu_smt_num_threads = num_threads;\n}\n\nstatic int __init smt_cmdline_disable(char *str)\n{\n\tcpu_smt_disable(str && !strcmp(str, \"force\"));\n\treturn 0;\n}\nearly_param(\"nosmt\", smt_cmdline_disable);\n\n \nstatic inline bool cpu_smt_thread_allowed(unsigned int cpu)\n{\n#ifdef CONFIG_SMT_NUM_THREADS_DYNAMIC\n\treturn topology_smt_thread_allowed(cpu);\n#else\n\treturn true;\n#endif\n}\n\nstatic inline bool cpu_bootable(unsigned int cpu)\n{\n\tif (cpu_smt_control == CPU_SMT_ENABLED && cpu_smt_thread_allowed(cpu))\n\t\treturn true;\n\n\t \n\tif (cpu_smt_control == CPU_SMT_NOT_IMPLEMENTED)\n\t\treturn true;\n\n\t \n\tif (cpu_smt_control == CPU_SMT_NOT_SUPPORTED)\n\t\treturn true;\n\n\tif (topology_is_primary_thread(cpu))\n\t\treturn true;\n\n\t \n\treturn !cpumask_test_cpu(cpu, &cpus_booted_once_mask);\n}\n\n \nbool cpu_smt_possible(void)\n{\n\treturn cpu_smt_control != CPU_SMT_FORCE_DISABLED &&\n\t\tcpu_smt_control != CPU_SMT_NOT_SUPPORTED;\n}\nEXPORT_SYMBOL_GPL(cpu_smt_possible);\n\n#else\nstatic inline bool cpu_bootable(unsigned int cpu) { return true; }\n#endif\n\nstatic inline enum cpuhp_state\ncpuhp_set_state(int cpu, struct cpuhp_cpu_state *st, enum cpuhp_state target)\n{\n\tenum cpuhp_state prev_state = st->state;\n\tbool bringup = st->state < target;\n\n\tst->rollback = false;\n\tst->last = NULL;\n\n\tst->target = target;\n\tst->single = false;\n\tst->bringup = bringup;\n\tif (cpu_dying(cpu) != !bringup)\n\t\tset_cpu_dying(cpu, !bringup);\n\n\treturn prev_state;\n}\n\nstatic inline void\ncpuhp_reset_state(int cpu, struct cpuhp_cpu_state *st,\n\t\t  enum cpuhp_state prev_state)\n{\n\tbool bringup = !st->bringup;\n\n\tst->target = prev_state;\n\n\t \n\tif (st->rollback)\n\t\treturn;\n\n\tst->rollback = true;\n\n\t \n\tif (!st->last) {\n\t\tif (st->bringup)\n\t\t\tst->state--;\n\t\telse\n\t\t\tst->state++;\n\t}\n\n\tst->bringup = bringup;\n\tif (cpu_dying(cpu) != !bringup)\n\t\tset_cpu_dying(cpu, !bringup);\n}\n\n \nstatic void __cpuhp_kick_ap(struct cpuhp_cpu_state *st)\n{\n\tif (!st->single && st->state == st->target)\n\t\treturn;\n\n\tst->result = 0;\n\t \n\tsmp_mb();\n\tst->should_run = true;\n\twake_up_process(st->thread);\n\twait_for_ap_thread(st, st->bringup);\n}\n\nstatic int cpuhp_kick_ap(int cpu, struct cpuhp_cpu_state *st,\n\t\t\t enum cpuhp_state target)\n{\n\tenum cpuhp_state prev_state;\n\tint ret;\n\n\tprev_state = cpuhp_set_state(cpu, st, target);\n\t__cpuhp_kick_ap(st);\n\tif ((ret = st->result)) {\n\t\tcpuhp_reset_state(cpu, st, prev_state);\n\t\t__cpuhp_kick_ap(st);\n\t}\n\n\treturn ret;\n}\n\nstatic int bringup_wait_for_ap_online(unsigned int cpu)\n{\n\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\n\n\t \n\twait_for_ap_thread(st, true);\n\tif (WARN_ON_ONCE((!cpu_online(cpu))))\n\t\treturn -ECANCELED;\n\n\t \n\tkthread_unpark(st->thread);\n\n\t \n\tif (!cpu_bootable(cpu))\n\t\treturn -ECANCELED;\n\treturn 0;\n}\n\n#ifdef CONFIG_HOTPLUG_SPLIT_STARTUP\nstatic int cpuhp_kick_ap_alive(unsigned int cpu)\n{\n\tif (!cpuhp_can_boot_ap(cpu))\n\t\treturn -EAGAIN;\n\n\treturn arch_cpuhp_kick_ap_alive(cpu, idle_thread_get(cpu));\n}\n\nstatic int cpuhp_bringup_ap(unsigned int cpu)\n{\n\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\n\tint ret;\n\n\t \n\tirq_lock_sparse();\n\n\tret = cpuhp_bp_sync_alive(cpu);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tret = bringup_wait_for_ap_online(cpu);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tirq_unlock_sparse();\n\n\tif (st->target <= CPUHP_AP_ONLINE_IDLE)\n\t\treturn 0;\n\n\treturn cpuhp_kick_ap(cpu, st, st->target);\n\nout_unlock:\n\tirq_unlock_sparse();\n\treturn ret;\n}\n#else\nstatic int bringup_cpu(unsigned int cpu)\n{\n\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\n\tstruct task_struct *idle = idle_thread_get(cpu);\n\tint ret;\n\n\tif (!cpuhp_can_boot_ap(cpu))\n\t\treturn -EAGAIN;\n\n\t \n\tirq_lock_sparse();\n\n\tret = __cpu_up(cpu, idle);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tret = cpuhp_bp_sync_alive(cpu);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tret = bringup_wait_for_ap_online(cpu);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tirq_unlock_sparse();\n\n\tif (st->target <= CPUHP_AP_ONLINE_IDLE)\n\t\treturn 0;\n\n\treturn cpuhp_kick_ap(cpu, st, st->target);\n\nout_unlock:\n\tirq_unlock_sparse();\n\treturn ret;\n}\n#endif\n\nstatic int finish_cpu(unsigned int cpu)\n{\n\tstruct task_struct *idle = idle_thread_get(cpu);\n\tstruct mm_struct *mm = idle->active_mm;\n\n\t \n\tif (mm != &init_mm)\n\t\tidle->active_mm = &init_mm;\n\tmmdrop_lazy_tlb(mm);\n\treturn 0;\n}\n\n \n\n \nstatic bool cpuhp_next_state(bool bringup,\n\t\t\t     enum cpuhp_state *state_to_run,\n\t\t\t     struct cpuhp_cpu_state *st,\n\t\t\t     enum cpuhp_state target)\n{\n\tdo {\n\t\tif (bringup) {\n\t\t\tif (st->state >= target)\n\t\t\t\treturn false;\n\n\t\t\t*state_to_run = ++st->state;\n\t\t} else {\n\t\t\tif (st->state <= target)\n\t\t\t\treturn false;\n\n\t\t\t*state_to_run = st->state--;\n\t\t}\n\n\t\tif (!cpuhp_step_empty(bringup, cpuhp_get_step(*state_to_run)))\n\t\t\tbreak;\n\t} while (true);\n\n\treturn true;\n}\n\nstatic int __cpuhp_invoke_callback_range(bool bringup,\n\t\t\t\t\t unsigned int cpu,\n\t\t\t\t\t struct cpuhp_cpu_state *st,\n\t\t\t\t\t enum cpuhp_state target,\n\t\t\t\t\t bool nofail)\n{\n\tenum cpuhp_state state;\n\tint ret = 0;\n\n\twhile (cpuhp_next_state(bringup, &state, st, target)) {\n\t\tint err;\n\n\t\terr = cpuhp_invoke_callback(cpu, state, bringup, NULL, NULL);\n\t\tif (!err)\n\t\t\tcontinue;\n\n\t\tif (nofail) {\n\t\t\tpr_warn(\"CPU %u %s state %s (%d) failed (%d)\\n\",\n\t\t\t\tcpu, bringup ? \"UP\" : \"DOWN\",\n\t\t\t\tcpuhp_get_step(st->state)->name,\n\t\t\t\tst->state, err);\n\t\t\tret = -1;\n\t\t} else {\n\t\t\tret = err;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstatic inline int cpuhp_invoke_callback_range(bool bringup,\n\t\t\t\t\t      unsigned int cpu,\n\t\t\t\t\t      struct cpuhp_cpu_state *st,\n\t\t\t\t\t      enum cpuhp_state target)\n{\n\treturn __cpuhp_invoke_callback_range(bringup, cpu, st, target, false);\n}\n\nstatic inline void cpuhp_invoke_callback_range_nofail(bool bringup,\n\t\t\t\t\t\t      unsigned int cpu,\n\t\t\t\t\t\t      struct cpuhp_cpu_state *st,\n\t\t\t\t\t\t      enum cpuhp_state target)\n{\n\t__cpuhp_invoke_callback_range(bringup, cpu, st, target, true);\n}\n\nstatic inline bool can_rollback_cpu(struct cpuhp_cpu_state *st)\n{\n\tif (IS_ENABLED(CONFIG_HOTPLUG_CPU))\n\t\treturn true;\n\t \n\treturn st->state <= CPUHP_BRINGUP_CPU;\n}\n\nstatic int cpuhp_up_callbacks(unsigned int cpu, struct cpuhp_cpu_state *st,\n\t\t\t      enum cpuhp_state target)\n{\n\tenum cpuhp_state prev_state = st->state;\n\tint ret = 0;\n\n\tret = cpuhp_invoke_callback_range(true, cpu, st, target);\n\tif (ret) {\n\t\tpr_debug(\"CPU UP failed (%d) CPU %u state %s (%d)\\n\",\n\t\t\t ret, cpu, cpuhp_get_step(st->state)->name,\n\t\t\t st->state);\n\n\t\tcpuhp_reset_state(cpu, st, prev_state);\n\t\tif (can_rollback_cpu(st))\n\t\t\tWARN_ON(cpuhp_invoke_callback_range(false, cpu, st,\n\t\t\t\t\t\t\t    prev_state));\n\t}\n\treturn ret;\n}\n\n \nstatic int cpuhp_should_run(unsigned int cpu)\n{\n\tstruct cpuhp_cpu_state *st = this_cpu_ptr(&cpuhp_state);\n\n\treturn st->should_run;\n}\n\n \nstatic void cpuhp_thread_fun(unsigned int cpu)\n{\n\tstruct cpuhp_cpu_state *st = this_cpu_ptr(&cpuhp_state);\n\tbool bringup = st->bringup;\n\tenum cpuhp_state state;\n\n\tif (WARN_ON_ONCE(!st->should_run))\n\t\treturn;\n\n\t \n\tsmp_mb();\n\n\t \n\tlockdep_acquire_cpus_lock();\n\tcpuhp_lock_acquire(bringup);\n\n\tif (st->single) {\n\t\tstate = st->cb_state;\n\t\tst->should_run = false;\n\t} else {\n\t\tst->should_run = cpuhp_next_state(bringup, &state, st, st->target);\n\t\tif (!st->should_run)\n\t\t\tgoto end;\n\t}\n\n\tWARN_ON_ONCE(!cpuhp_is_ap_state(state));\n\n\tif (cpuhp_is_atomic_state(state)) {\n\t\tlocal_irq_disable();\n\t\tst->result = cpuhp_invoke_callback(cpu, state, bringup, st->node, &st->last);\n\t\tlocal_irq_enable();\n\n\t\t \n\t\tWARN_ON_ONCE(st->result);\n\t} else {\n\t\tst->result = cpuhp_invoke_callback(cpu, state, bringup, st->node, &st->last);\n\t}\n\n\tif (st->result) {\n\t\t \n\t\tWARN_ON_ONCE(st->rollback);\n\t\tst->should_run = false;\n\t}\n\nend:\n\tcpuhp_lock_release(bringup);\n\tlockdep_release_cpus_lock();\n\n\tif (!st->should_run)\n\t\tcomplete_ap_thread(st, bringup);\n}\n\n \nstatic int\ncpuhp_invoke_ap_callback(int cpu, enum cpuhp_state state, bool bringup,\n\t\t\t struct hlist_node *node)\n{\n\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\n\tint ret;\n\n\tif (!cpu_online(cpu))\n\t\treturn 0;\n\n\tcpuhp_lock_acquire(false);\n\tcpuhp_lock_release(false);\n\n\tcpuhp_lock_acquire(true);\n\tcpuhp_lock_release(true);\n\n\t \n\tif (!st->thread)\n\t\treturn cpuhp_invoke_callback(cpu, state, bringup, node, NULL);\n\n\tst->rollback = false;\n\tst->last = NULL;\n\n\tst->node = node;\n\tst->bringup = bringup;\n\tst->cb_state = state;\n\tst->single = true;\n\n\t__cpuhp_kick_ap(st);\n\n\t \n\tif ((ret = st->result) && st->last) {\n\t\tst->rollback = true;\n\t\tst->bringup = !bringup;\n\n\t\t__cpuhp_kick_ap(st);\n\t}\n\n\t \n\tst->node = st->last = NULL;\n\treturn ret;\n}\n\nstatic int cpuhp_kick_ap_work(unsigned int cpu)\n{\n\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\n\tenum cpuhp_state prev_state = st->state;\n\tint ret;\n\n\tcpuhp_lock_acquire(false);\n\tcpuhp_lock_release(false);\n\n\tcpuhp_lock_acquire(true);\n\tcpuhp_lock_release(true);\n\n\ttrace_cpuhp_enter(cpu, st->target, prev_state, cpuhp_kick_ap_work);\n\tret = cpuhp_kick_ap(cpu, st, st->target);\n\ttrace_cpuhp_exit(cpu, st->state, prev_state, ret);\n\n\treturn ret;\n}\n\nstatic struct smp_hotplug_thread cpuhp_threads = {\n\t.store\t\t\t= &cpuhp_state.thread,\n\t.thread_should_run\t= cpuhp_should_run,\n\t.thread_fn\t\t= cpuhp_thread_fun,\n\t.thread_comm\t\t= \"cpuhp/%u\",\n\t.selfparking\t\t= true,\n};\n\nstatic __init void cpuhp_init_state(void)\n{\n\tstruct cpuhp_cpu_state *st;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tst = per_cpu_ptr(&cpuhp_state, cpu);\n\t\tinit_completion(&st->done_up);\n\t\tinit_completion(&st->done_down);\n\t}\n}\n\nvoid __init cpuhp_threads_init(void)\n{\n\tcpuhp_init_state();\n\tBUG_ON(smpboot_register_percpu_thread(&cpuhp_threads));\n\tkthread_unpark(this_cpu_read(cpuhp_state.thread));\n}\n\n \nstatic void cpu_up_down_serialize_trainwrecks(bool tasks_frozen)\n{\n\t \n\tif (!tasks_frozen)\n\t\tcpuset_wait_for_hotplug();\n}\n\n#ifdef CONFIG_HOTPLUG_CPU\n#ifndef arch_clear_mm_cpumask_cpu\n#define arch_clear_mm_cpumask_cpu(cpu, mm) cpumask_clear_cpu(cpu, mm_cpumask(mm))\n#endif\n\n \nvoid clear_tasks_mm_cpumask(int cpu)\n{\n\tstruct task_struct *p;\n\n\t \n\tWARN_ON(cpu_online(cpu));\n\trcu_read_lock();\n\tfor_each_process(p) {\n\t\tstruct task_struct *t;\n\n\t\t \n\t\tt = find_lock_task_mm(p);\n\t\tif (!t)\n\t\t\tcontinue;\n\t\tarch_clear_mm_cpumask_cpu(cpu, t->mm);\n\t\ttask_unlock(t);\n\t}\n\trcu_read_unlock();\n}\n\n \nstatic int take_cpu_down(void *_param)\n{\n\tstruct cpuhp_cpu_state *st = this_cpu_ptr(&cpuhp_state);\n\tenum cpuhp_state target = max((int)st->target, CPUHP_AP_OFFLINE);\n\tint err, cpu = smp_processor_id();\n\n\t \n\terr = __cpu_disable();\n\tif (err < 0)\n\t\treturn err;\n\n\t \n\tWARN_ON(st->state != (CPUHP_TEARDOWN_CPU - 1));\n\n\t \n\tcpuhp_invoke_callback_range_nofail(false, cpu, st, target);\n\n\t \n\ttick_handover_do_timer();\n\t \n\ttick_offline_cpu(cpu);\n\t \n\tstop_machine_park(cpu);\n\treturn 0;\n}\n\nstatic int takedown_cpu(unsigned int cpu)\n{\n\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\n\tint err;\n\n\t \n\tkthread_park(st->thread);\n\n\t \n\tirq_lock_sparse();\n\n\t \n\terr = stop_machine_cpuslocked(take_cpu_down, NULL, cpumask_of(cpu));\n\tif (err) {\n\t\t \n\t\tirq_unlock_sparse();\n\t\t \n\t\tkthread_unpark(st->thread);\n\t\treturn err;\n\t}\n\tBUG_ON(cpu_online(cpu));\n\n\t \n\twait_for_ap_thread(st, false);\n\tBUG_ON(st->state != CPUHP_AP_IDLE_DEAD);\n\n\t \n\tirq_unlock_sparse();\n\n\thotplug_cpu__broadcast_tick_pull(cpu);\n\t \n\t__cpu_die(cpu);\n\n\tcpuhp_bp_sync_dead(cpu);\n\n\ttick_cleanup_dead_cpu(cpu);\n\trcutree_migrate_callbacks(cpu);\n\treturn 0;\n}\n\nstatic void cpuhp_complete_idle_dead(void *arg)\n{\n\tstruct cpuhp_cpu_state *st = arg;\n\n\tcomplete_ap_thread(st, false);\n}\n\nvoid cpuhp_report_idle_dead(void)\n{\n\tstruct cpuhp_cpu_state *st = this_cpu_ptr(&cpuhp_state);\n\n\tBUG_ON(st->state != CPUHP_AP_OFFLINE);\n\trcu_report_dead(smp_processor_id());\n\tst->state = CPUHP_AP_IDLE_DEAD;\n\t \n\tsmp_call_function_single(cpumask_first(cpu_online_mask),\n\t\t\t\t cpuhp_complete_idle_dead, st, 0);\n}\n\nstatic int cpuhp_down_callbacks(unsigned int cpu, struct cpuhp_cpu_state *st,\n\t\t\t\tenum cpuhp_state target)\n{\n\tenum cpuhp_state prev_state = st->state;\n\tint ret = 0;\n\n\tret = cpuhp_invoke_callback_range(false, cpu, st, target);\n\tif (ret) {\n\t\tpr_debug(\"CPU DOWN failed (%d) CPU %u state %s (%d)\\n\",\n\t\t\t ret, cpu, cpuhp_get_step(st->state)->name,\n\t\t\t st->state);\n\n\t\tcpuhp_reset_state(cpu, st, prev_state);\n\n\t\tif (st->state < prev_state)\n\t\t\tWARN_ON(cpuhp_invoke_callback_range(true, cpu, st,\n\t\t\t\t\t\t\t    prev_state));\n\t}\n\n\treturn ret;\n}\n\n \nstatic int __ref _cpu_down(unsigned int cpu, int tasks_frozen,\n\t\t\t   enum cpuhp_state target)\n{\n\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\n\tint prev_state, ret = 0;\n\n\tif (num_online_cpus() == 1)\n\t\treturn -EBUSY;\n\n\tif (!cpu_present(cpu))\n\t\treturn -EINVAL;\n\n\tcpus_write_lock();\n\n\tcpuhp_tasks_frozen = tasks_frozen;\n\n\tprev_state = cpuhp_set_state(cpu, st, target);\n\t \n\tif (st->state > CPUHP_TEARDOWN_CPU) {\n\t\tst->target = max((int)target, CPUHP_TEARDOWN_CPU);\n\t\tret = cpuhp_kick_ap_work(cpu);\n\t\t \n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\t \n\t\tif (st->state > CPUHP_TEARDOWN_CPU)\n\t\t\tgoto out;\n\n\t\tst->target = target;\n\t}\n\t \n\tret = cpuhp_down_callbacks(cpu, st, target);\n\tif (ret && st->state < prev_state) {\n\t\tif (st->state == CPUHP_TEARDOWN_CPU) {\n\t\t\tcpuhp_reset_state(cpu, st, prev_state);\n\t\t\t__cpuhp_kick_ap(st);\n\t\t} else {\n\t\t\tWARN(1, \"DEAD callback error for CPU%d\", cpu);\n\t\t}\n\t}\n\nout:\n\tcpus_write_unlock();\n\t \n\tlockup_detector_cleanup();\n\tarch_smt_update();\n\tcpu_up_down_serialize_trainwrecks(tasks_frozen);\n\treturn ret;\n}\n\nstruct cpu_down_work {\n\tunsigned int\t\tcpu;\n\tenum cpuhp_state\ttarget;\n};\n\nstatic long __cpu_down_maps_locked(void *arg)\n{\n\tstruct cpu_down_work *work = arg;\n\n\treturn _cpu_down(work->cpu, 0, work->target);\n}\n\nstatic int cpu_down_maps_locked(unsigned int cpu, enum cpuhp_state target)\n{\n\tstruct cpu_down_work work = { .cpu = cpu, .target = target, };\n\n\t \n\tif (cc_platform_has(CC_ATTR_HOTPLUG_DISABLED))\n\t\treturn -EOPNOTSUPP;\n\tif (cpu_hotplug_disabled)\n\t\treturn -EBUSY;\n\n\t \n\tfor_each_cpu_and(cpu, cpu_online_mask, housekeeping_cpumask(HK_TYPE_DOMAIN)) {\n\t\tif (cpu != work.cpu)\n\t\t\treturn work_on_cpu(cpu, __cpu_down_maps_locked, &work);\n\t}\n\treturn -EBUSY;\n}\n\nstatic int cpu_down(unsigned int cpu, enum cpuhp_state target)\n{\n\tint err;\n\n\tcpu_maps_update_begin();\n\terr = cpu_down_maps_locked(cpu, target);\n\tcpu_maps_update_done();\n\treturn err;\n}\n\n \nint cpu_device_down(struct device *dev)\n{\n\treturn cpu_down(dev->id, CPUHP_OFFLINE);\n}\n\nint remove_cpu(unsigned int cpu)\n{\n\tint ret;\n\n\tlock_device_hotplug();\n\tret = device_offline(get_cpu_device(cpu));\n\tunlock_device_hotplug();\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(remove_cpu);\n\nvoid smp_shutdown_nonboot_cpus(unsigned int primary_cpu)\n{\n\tunsigned int cpu;\n\tint error;\n\n\tcpu_maps_update_begin();\n\n\t \n\tif (!cpu_online(primary_cpu))\n\t\tprimary_cpu = cpumask_first(cpu_online_mask);\n\n\tfor_each_online_cpu(cpu) {\n\t\tif (cpu == primary_cpu)\n\t\t\tcontinue;\n\n\t\terror = cpu_down_maps_locked(cpu, CPUHP_OFFLINE);\n\t\tif (error) {\n\t\t\tpr_err(\"Failed to offline CPU%d - error=%d\",\n\t\t\t\tcpu, error);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\tBUG_ON(num_online_cpus() > 1);\n\n\t \n\tcpu_hotplug_disabled++;\n\n\tcpu_maps_update_done();\n}\n\n#else\n#define takedown_cpu\t\tNULL\n#endif  \n\n \nvoid notify_cpu_starting(unsigned int cpu)\n{\n\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\n\tenum cpuhp_state target = min((int)st->target, CPUHP_AP_ONLINE);\n\n\trcu_cpu_starting(cpu);\t \n\tcpumask_set_cpu(cpu, &cpus_booted_once_mask);\n\n\t \n\tcpuhp_invoke_callback_range_nofail(true, cpu, st, target);\n}\n\n \nvoid cpuhp_online_idle(enum cpuhp_state state)\n{\n\tstruct cpuhp_cpu_state *st = this_cpu_ptr(&cpuhp_state);\n\n\t \n\tif (state != CPUHP_AP_ONLINE_IDLE)\n\t\treturn;\n\n\tcpuhp_ap_update_sync_state(SYNC_STATE_ONLINE);\n\n\t \n\tstop_machine_unpark(smp_processor_id());\n\n\tst->state = CPUHP_AP_ONLINE_IDLE;\n\tcomplete_ap_thread(st, true);\n}\n\n \nstatic int _cpu_up(unsigned int cpu, int tasks_frozen, enum cpuhp_state target)\n{\n\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\n\tstruct task_struct *idle;\n\tint ret = 0;\n\n\tcpus_write_lock();\n\n\tif (!cpu_present(cpu)) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t \n\tif (st->state >= target)\n\t\tgoto out;\n\n\tif (st->state == CPUHP_OFFLINE) {\n\t\t \n\t\tidle = idle_thread_get(cpu);\n\t\tif (IS_ERR(idle)) {\n\t\t\tret = PTR_ERR(idle);\n\t\t\tgoto out;\n\t\t}\n\n\t\t \n\t\tscs_task_reset(idle);\n\t\tkasan_unpoison_task_stack(idle);\n\t}\n\n\tcpuhp_tasks_frozen = tasks_frozen;\n\n\tcpuhp_set_state(cpu, st, target);\n\t \n\tif (st->state > CPUHP_BRINGUP_CPU) {\n\t\tret = cpuhp_kick_ap_work(cpu);\n\t\t \n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\t \n\ttarget = min((int)target, CPUHP_BRINGUP_CPU);\n\tret = cpuhp_up_callbacks(cpu, st, target);\nout:\n\tcpus_write_unlock();\n\tarch_smt_update();\n\tcpu_up_down_serialize_trainwrecks(tasks_frozen);\n\treturn ret;\n}\n\nstatic int cpu_up(unsigned int cpu, enum cpuhp_state target)\n{\n\tint err = 0;\n\n\tif (!cpu_possible(cpu)) {\n\t\tpr_err(\"can't online cpu %d because it is not configured as may-hotadd at boot time\\n\",\n\t\t       cpu);\n#if defined(CONFIG_IA64)\n\t\tpr_err(\"please check additional_cpus= boot parameter\\n\");\n#endif\n\t\treturn -EINVAL;\n\t}\n\n\terr = try_online_node(cpu_to_node(cpu));\n\tif (err)\n\t\treturn err;\n\n\tcpu_maps_update_begin();\n\n\tif (cpu_hotplug_disabled) {\n\t\terr = -EBUSY;\n\t\tgoto out;\n\t}\n\tif (!cpu_bootable(cpu)) {\n\t\terr = -EPERM;\n\t\tgoto out;\n\t}\n\n\terr = _cpu_up(cpu, 0, target);\nout:\n\tcpu_maps_update_done();\n\treturn err;\n}\n\n \nint cpu_device_up(struct device *dev)\n{\n\treturn cpu_up(dev->id, CPUHP_ONLINE);\n}\n\nint add_cpu(unsigned int cpu)\n{\n\tint ret;\n\n\tlock_device_hotplug();\n\tret = device_online(get_cpu_device(cpu));\n\tunlock_device_hotplug();\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(add_cpu);\n\n \nint bringup_hibernate_cpu(unsigned int sleep_cpu)\n{\n\tint ret;\n\n\tif (!cpu_online(sleep_cpu)) {\n\t\tpr_info(\"Hibernated on a CPU that is offline! Bringing CPU up.\\n\");\n\t\tret = cpu_up(sleep_cpu, CPUHP_ONLINE);\n\t\tif (ret) {\n\t\t\tpr_err(\"Failed to bring hibernate-CPU up!\\n\");\n\t\t\treturn ret;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic void __init cpuhp_bringup_mask(const struct cpumask *mask, unsigned int ncpus,\n\t\t\t\t      enum cpuhp_state target)\n{\n\tunsigned int cpu;\n\n\tfor_each_cpu(cpu, mask) {\n\t\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\n\n\t\tif (cpu_up(cpu, target) && can_rollback_cpu(st)) {\n\t\t\t \n\t\t\tWARN_ON(cpuhp_invoke_callback_range(false, cpu, st, CPUHP_OFFLINE));\n\t\t}\n\n\t\tif (!--ncpus)\n\t\t\tbreak;\n\t}\n}\n\n#ifdef CONFIG_HOTPLUG_PARALLEL\nstatic bool __cpuhp_parallel_bringup __ro_after_init = true;\n\nstatic int __init parallel_bringup_parse_param(char *arg)\n{\n\treturn kstrtobool(arg, &__cpuhp_parallel_bringup);\n}\nearly_param(\"cpuhp.parallel\", parallel_bringup_parse_param);\n\nstatic inline bool cpuhp_smt_aware(void)\n{\n\treturn cpu_smt_max_threads > 1;\n}\n\nstatic inline const struct cpumask *cpuhp_get_primary_thread_mask(void)\n{\n\treturn cpu_primary_thread_mask;\n}\n\n \nstatic bool __init cpuhp_bringup_cpus_parallel(unsigned int ncpus)\n{\n\tconst struct cpumask *mask = cpu_present_mask;\n\n\tif (__cpuhp_parallel_bringup)\n\t\t__cpuhp_parallel_bringup = arch_cpuhp_init_parallel_bringup();\n\tif (!__cpuhp_parallel_bringup)\n\t\treturn false;\n\n\tif (cpuhp_smt_aware()) {\n\t\tconst struct cpumask *pmask = cpuhp_get_primary_thread_mask();\n\t\tstatic struct cpumask tmp_mask __initdata;\n\n\t\t \n\t\tcpumask_and(&tmp_mask, mask, pmask);\n\t\tcpuhp_bringup_mask(&tmp_mask, ncpus, CPUHP_BP_KICK_AP);\n\t\tcpuhp_bringup_mask(&tmp_mask, ncpus, CPUHP_ONLINE);\n\t\t \n\t\tncpus -= num_online_cpus();\n\t\tif (!ncpus)\n\t\t\treturn true;\n\t\t \n\t\tcpumask_andnot(&tmp_mask, mask, pmask);\n\t\tmask = &tmp_mask;\n\t}\n\n\t \n\tcpuhp_bringup_mask(mask, ncpus, CPUHP_BP_KICK_AP);\n\tcpuhp_bringup_mask(mask, ncpus, CPUHP_ONLINE);\n\treturn true;\n}\n#else\nstatic inline bool cpuhp_bringup_cpus_parallel(unsigned int ncpus) { return false; }\n#endif  \n\nvoid __init bringup_nonboot_cpus(unsigned int setup_max_cpus)\n{\n\t \n\tif (cpuhp_bringup_cpus_parallel(setup_max_cpus))\n\t\treturn;\n\n\t \n\tcpuhp_bringup_mask(cpu_present_mask, setup_max_cpus, CPUHP_ONLINE);\n}\n\n#ifdef CONFIG_PM_SLEEP_SMP\nstatic cpumask_var_t frozen_cpus;\n\nint freeze_secondary_cpus(int primary)\n{\n\tint cpu, error = 0;\n\n\tcpu_maps_update_begin();\n\tif (primary == -1) {\n\t\tprimary = cpumask_first(cpu_online_mask);\n\t\tif (!housekeeping_cpu(primary, HK_TYPE_TIMER))\n\t\t\tprimary = housekeeping_any_cpu(HK_TYPE_TIMER);\n\t} else {\n\t\tif (!cpu_online(primary))\n\t\t\tprimary = cpumask_first(cpu_online_mask);\n\t}\n\n\t \n\tcpumask_clear(frozen_cpus);\n\n\tpr_info(\"Disabling non-boot CPUs ...\\n\");\n\tfor_each_online_cpu(cpu) {\n\t\tif (cpu == primary)\n\t\t\tcontinue;\n\n\t\tif (pm_wakeup_pending()) {\n\t\t\tpr_info(\"Wakeup pending. Abort CPU freeze\\n\");\n\t\t\terror = -EBUSY;\n\t\t\tbreak;\n\t\t}\n\n\t\ttrace_suspend_resume(TPS(\"CPU_OFF\"), cpu, true);\n\t\terror = _cpu_down(cpu, 1, CPUHP_OFFLINE);\n\t\ttrace_suspend_resume(TPS(\"CPU_OFF\"), cpu, false);\n\t\tif (!error)\n\t\t\tcpumask_set_cpu(cpu, frozen_cpus);\n\t\telse {\n\t\t\tpr_err(\"Error taking CPU%d down: %d\\n\", cpu, error);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!error)\n\t\tBUG_ON(num_online_cpus() > 1);\n\telse\n\t\tpr_err(\"Non-boot CPUs are not disabled\\n\");\n\n\t \n\tcpu_hotplug_disabled++;\n\n\tcpu_maps_update_done();\n\treturn error;\n}\n\nvoid __weak arch_thaw_secondary_cpus_begin(void)\n{\n}\n\nvoid __weak arch_thaw_secondary_cpus_end(void)\n{\n}\n\nvoid thaw_secondary_cpus(void)\n{\n\tint cpu, error;\n\n\t \n\tcpu_maps_update_begin();\n\t__cpu_hotplug_enable();\n\tif (cpumask_empty(frozen_cpus))\n\t\tgoto out;\n\n\tpr_info(\"Enabling non-boot CPUs ...\\n\");\n\n\tarch_thaw_secondary_cpus_begin();\n\n\tfor_each_cpu(cpu, frozen_cpus) {\n\t\ttrace_suspend_resume(TPS(\"CPU_ON\"), cpu, true);\n\t\terror = _cpu_up(cpu, 1, CPUHP_ONLINE);\n\t\ttrace_suspend_resume(TPS(\"CPU_ON\"), cpu, false);\n\t\tif (!error) {\n\t\t\tpr_info(\"CPU%d is up\\n\", cpu);\n\t\t\tcontinue;\n\t\t}\n\t\tpr_warn(\"Error taking CPU%d up: %d\\n\", cpu, error);\n\t}\n\n\tarch_thaw_secondary_cpus_end();\n\n\tcpumask_clear(frozen_cpus);\nout:\n\tcpu_maps_update_done();\n}\n\nstatic int __init alloc_frozen_cpus(void)\n{\n\tif (!alloc_cpumask_var(&frozen_cpus, GFP_KERNEL|__GFP_ZERO))\n\t\treturn -ENOMEM;\n\treturn 0;\n}\ncore_initcall(alloc_frozen_cpus);\n\n \nstatic int\ncpu_hotplug_pm_callback(struct notifier_block *nb,\n\t\t\tunsigned long action, void *ptr)\n{\n\tswitch (action) {\n\n\tcase PM_SUSPEND_PREPARE:\n\tcase PM_HIBERNATION_PREPARE:\n\t\tcpu_hotplug_disable();\n\t\tbreak;\n\n\tcase PM_POST_SUSPEND:\n\tcase PM_POST_HIBERNATION:\n\t\tcpu_hotplug_enable();\n\t\tbreak;\n\n\tdefault:\n\t\treturn NOTIFY_DONE;\n\t}\n\n\treturn NOTIFY_OK;\n}\n\n\nstatic int __init cpu_hotplug_pm_sync_init(void)\n{\n\t \n\tpm_notifier(cpu_hotplug_pm_callback, 0);\n\treturn 0;\n}\ncore_initcall(cpu_hotplug_pm_sync_init);\n\n#endif  \n\nint __boot_cpu_id;\n\n#endif  \n\n \nstatic struct cpuhp_step cpuhp_hp_states[] = {\n\t[CPUHP_OFFLINE] = {\n\t\t.name\t\t\t= \"offline\",\n\t\t.startup.single\t\t= NULL,\n\t\t.teardown.single\t= NULL,\n\t},\n#ifdef CONFIG_SMP\n\t[CPUHP_CREATE_THREADS]= {\n\t\t.name\t\t\t= \"threads:prepare\",\n\t\t.startup.single\t\t= smpboot_create_threads,\n\t\t.teardown.single\t= NULL,\n\t\t.cant_stop\t\t= true,\n\t},\n\t[CPUHP_PERF_PREPARE] = {\n\t\t.name\t\t\t= \"perf:prepare\",\n\t\t.startup.single\t\t= perf_event_init_cpu,\n\t\t.teardown.single\t= perf_event_exit_cpu,\n\t},\n\t[CPUHP_RANDOM_PREPARE] = {\n\t\t.name\t\t\t= \"random:prepare\",\n\t\t.startup.single\t\t= random_prepare_cpu,\n\t\t.teardown.single\t= NULL,\n\t},\n\t[CPUHP_WORKQUEUE_PREP] = {\n\t\t.name\t\t\t= \"workqueue:prepare\",\n\t\t.startup.single\t\t= workqueue_prepare_cpu,\n\t\t.teardown.single\t= NULL,\n\t},\n\t[CPUHP_HRTIMERS_PREPARE] = {\n\t\t.name\t\t\t= \"hrtimers:prepare\",\n\t\t.startup.single\t\t= hrtimers_prepare_cpu,\n\t\t.teardown.single\t= NULL,\n\t},\n\t[CPUHP_SMPCFD_PREPARE] = {\n\t\t.name\t\t\t= \"smpcfd:prepare\",\n\t\t.startup.single\t\t= smpcfd_prepare_cpu,\n\t\t.teardown.single\t= smpcfd_dead_cpu,\n\t},\n\t[CPUHP_RELAY_PREPARE] = {\n\t\t.name\t\t\t= \"relay:prepare\",\n\t\t.startup.single\t\t= relay_prepare_cpu,\n\t\t.teardown.single\t= NULL,\n\t},\n\t[CPUHP_SLAB_PREPARE] = {\n\t\t.name\t\t\t= \"slab:prepare\",\n\t\t.startup.single\t\t= slab_prepare_cpu,\n\t\t.teardown.single\t= slab_dead_cpu,\n\t},\n\t[CPUHP_RCUTREE_PREP] = {\n\t\t.name\t\t\t= \"RCU/tree:prepare\",\n\t\t.startup.single\t\t= rcutree_prepare_cpu,\n\t\t.teardown.single\t= rcutree_dead_cpu,\n\t},\n\t \n\t[CPUHP_TIMERS_PREPARE] = {\n\t\t.name\t\t\t= \"timers:prepare\",\n\t\t.startup.single\t\t= timers_prepare_cpu,\n\t\t.teardown.single\t= timers_dead_cpu,\n\t},\n\n#ifdef CONFIG_HOTPLUG_SPLIT_STARTUP\n\t \n\t[CPUHP_BP_KICK_AP] = {\n\t\t.name\t\t\t= \"cpu:kick_ap\",\n\t\t.startup.single\t\t= cpuhp_kick_ap_alive,\n\t},\n\n\t \n\t[CPUHP_BRINGUP_CPU] = {\n\t\t.name\t\t\t= \"cpu:bringup\",\n\t\t.startup.single\t\t= cpuhp_bringup_ap,\n\t\t.teardown.single\t= finish_cpu,\n\t\t.cant_stop\t\t= true,\n\t},\n#else\n\t \n\t[CPUHP_BRINGUP_CPU] = {\n\t\t.name\t\t\t= \"cpu:bringup\",\n\t\t.startup.single\t\t= bringup_cpu,\n\t\t.teardown.single\t= finish_cpu,\n\t\t.cant_stop\t\t= true,\n\t},\n#endif\n\t \n\t[CPUHP_AP_IDLE_DEAD] = {\n\t\t.name\t\t\t= \"idle:dead\",\n\t},\n\t \n\t[CPUHP_AP_OFFLINE] = {\n\t\t.name\t\t\t= \"ap:offline\",\n\t\t.cant_stop\t\t= true,\n\t},\n\t \n\t[CPUHP_AP_SCHED_STARTING] = {\n\t\t.name\t\t\t= \"sched:starting\",\n\t\t.startup.single\t\t= sched_cpu_starting,\n\t\t.teardown.single\t= sched_cpu_dying,\n\t},\n\t[CPUHP_AP_RCUTREE_DYING] = {\n\t\t.name\t\t\t= \"RCU/tree:dying\",\n\t\t.startup.single\t\t= NULL,\n\t\t.teardown.single\t= rcutree_dying_cpu,\n\t},\n\t[CPUHP_AP_SMPCFD_DYING] = {\n\t\t.name\t\t\t= \"smpcfd:dying\",\n\t\t.startup.single\t\t= NULL,\n\t\t.teardown.single\t= smpcfd_dying_cpu,\n\t},\n\t[CPUHP_AP_HRTIMERS_DYING] = {\n\t\t.name\t\t\t= \"hrtimers:dying\",\n\t\t.startup.single\t\t= NULL,\n\t\t.teardown.single\t= hrtimers_cpu_dying,\n\t},\n\n\t \n\t[CPUHP_AP_ONLINE] = {\n\t\t.name\t\t\t= \"ap:online\",\n\t},\n\t \n\t[CPUHP_TEARDOWN_CPU] = {\n\t\t.name\t\t\t= \"cpu:teardown\",\n\t\t.startup.single\t\t= NULL,\n\t\t.teardown.single\t= takedown_cpu,\n\t\t.cant_stop\t\t= true,\n\t},\n\n\t[CPUHP_AP_SCHED_WAIT_EMPTY] = {\n\t\t.name\t\t\t= \"sched:waitempty\",\n\t\t.startup.single\t\t= NULL,\n\t\t.teardown.single\t= sched_cpu_wait_empty,\n\t},\n\n\t \n\t[CPUHP_AP_SMPBOOT_THREADS] = {\n\t\t.name\t\t\t= \"smpboot/threads:online\",\n\t\t.startup.single\t\t= smpboot_unpark_threads,\n\t\t.teardown.single\t= smpboot_park_threads,\n\t},\n\t[CPUHP_AP_IRQ_AFFINITY_ONLINE] = {\n\t\t.name\t\t\t= \"irq/affinity:online\",\n\t\t.startup.single\t\t= irq_affinity_online_cpu,\n\t\t.teardown.single\t= NULL,\n\t},\n\t[CPUHP_AP_PERF_ONLINE] = {\n\t\t.name\t\t\t= \"perf:online\",\n\t\t.startup.single\t\t= perf_event_init_cpu,\n\t\t.teardown.single\t= perf_event_exit_cpu,\n\t},\n\t[CPUHP_AP_WATCHDOG_ONLINE] = {\n\t\t.name\t\t\t= \"lockup_detector:online\",\n\t\t.startup.single\t\t= lockup_detector_online_cpu,\n\t\t.teardown.single\t= lockup_detector_offline_cpu,\n\t},\n\t[CPUHP_AP_WORKQUEUE_ONLINE] = {\n\t\t.name\t\t\t= \"workqueue:online\",\n\t\t.startup.single\t\t= workqueue_online_cpu,\n\t\t.teardown.single\t= workqueue_offline_cpu,\n\t},\n\t[CPUHP_AP_RANDOM_ONLINE] = {\n\t\t.name\t\t\t= \"random:online\",\n\t\t.startup.single\t\t= random_online_cpu,\n\t\t.teardown.single\t= NULL,\n\t},\n\t[CPUHP_AP_RCUTREE_ONLINE] = {\n\t\t.name\t\t\t= \"RCU/tree:online\",\n\t\t.startup.single\t\t= rcutree_online_cpu,\n\t\t.teardown.single\t= rcutree_offline_cpu,\n\t},\n#endif\n\t \n\n#ifdef CONFIG_SMP\n\t \n\t[CPUHP_AP_ACTIVE] = {\n\t\t.name\t\t\t= \"sched:active\",\n\t\t.startup.single\t\t= sched_cpu_activate,\n\t\t.teardown.single\t= sched_cpu_deactivate,\n\t},\n#endif\n\n\t \n\t[CPUHP_ONLINE] = {\n\t\t.name\t\t\t= \"online\",\n\t\t.startup.single\t\t= NULL,\n\t\t.teardown.single\t= NULL,\n\t},\n};\n\n \nstatic int cpuhp_cb_check(enum cpuhp_state state)\n{\n\tif (state <= CPUHP_OFFLINE || state >= CPUHP_ONLINE)\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\n \nstatic int cpuhp_reserve_state(enum cpuhp_state state)\n{\n\tenum cpuhp_state i, end;\n\tstruct cpuhp_step *step;\n\n\tswitch (state) {\n\tcase CPUHP_AP_ONLINE_DYN:\n\t\tstep = cpuhp_hp_states + CPUHP_AP_ONLINE_DYN;\n\t\tend = CPUHP_AP_ONLINE_DYN_END;\n\t\tbreak;\n\tcase CPUHP_BP_PREPARE_DYN:\n\t\tstep = cpuhp_hp_states + CPUHP_BP_PREPARE_DYN;\n\t\tend = CPUHP_BP_PREPARE_DYN_END;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tfor (i = state; i <= end; i++, step++) {\n\t\tif (!step->name)\n\t\t\treturn i;\n\t}\n\tWARN(1, \"No more dynamic states available for CPU hotplug\\n\");\n\treturn -ENOSPC;\n}\n\nstatic int cpuhp_store_callbacks(enum cpuhp_state state, const char *name,\n\t\t\t\t int (*startup)(unsigned int cpu),\n\t\t\t\t int (*teardown)(unsigned int cpu),\n\t\t\t\t bool multi_instance)\n{\n\t \n\tstruct cpuhp_step *sp;\n\tint ret = 0;\n\n\t \n\tif (name && (state == CPUHP_AP_ONLINE_DYN ||\n\t\t     state == CPUHP_BP_PREPARE_DYN)) {\n\t\tret = cpuhp_reserve_state(state);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\tstate = ret;\n\t}\n\tsp = cpuhp_get_step(state);\n\tif (name && sp->name)\n\t\treturn -EBUSY;\n\n\tsp->startup.single = startup;\n\tsp->teardown.single = teardown;\n\tsp->name = name;\n\tsp->multi_instance = multi_instance;\n\tINIT_HLIST_HEAD(&sp->list);\n\treturn ret;\n}\n\nstatic void *cpuhp_get_teardown_cb(enum cpuhp_state state)\n{\n\treturn cpuhp_get_step(state)->teardown.single;\n}\n\n \nstatic int cpuhp_issue_call(int cpu, enum cpuhp_state state, bool bringup,\n\t\t\t    struct hlist_node *node)\n{\n\tstruct cpuhp_step *sp = cpuhp_get_step(state);\n\tint ret;\n\n\t \n\tif (cpuhp_step_empty(bringup, sp))\n\t\treturn 0;\n\t \n#ifdef CONFIG_SMP\n\tif (cpuhp_is_ap_state(state))\n\t\tret = cpuhp_invoke_ap_callback(cpu, state, bringup, node);\n\telse\n\t\tret = cpuhp_invoke_callback(cpu, state, bringup, node, NULL);\n#else\n\tret = cpuhp_invoke_callback(cpu, state, bringup, node, NULL);\n#endif\n\tBUG_ON(ret && !bringup);\n\treturn ret;\n}\n\n \nstatic void cpuhp_rollback_install(int failedcpu, enum cpuhp_state state,\n\t\t\t\t   struct hlist_node *node)\n{\n\tint cpu;\n\n\t \n\tfor_each_present_cpu(cpu) {\n\t\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\n\t\tint cpustate = st->state;\n\n\t\tif (cpu >= failedcpu)\n\t\t\tbreak;\n\n\t\t \n\t\tif (cpustate >= state)\n\t\t\tcpuhp_issue_call(cpu, state, false, node);\n\t}\n}\n\nint __cpuhp_state_add_instance_cpuslocked(enum cpuhp_state state,\n\t\t\t\t\t  struct hlist_node *node,\n\t\t\t\t\t  bool invoke)\n{\n\tstruct cpuhp_step *sp;\n\tint cpu;\n\tint ret;\n\n\tlockdep_assert_cpus_held();\n\n\tsp = cpuhp_get_step(state);\n\tif (sp->multi_instance == false)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&cpuhp_state_mutex);\n\n\tif (!invoke || !sp->startup.multi)\n\t\tgoto add_node;\n\n\t \n\tfor_each_present_cpu(cpu) {\n\t\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\n\t\tint cpustate = st->state;\n\n\t\tif (cpustate < state)\n\t\t\tcontinue;\n\n\t\tret = cpuhp_issue_call(cpu, state, true, node);\n\t\tif (ret) {\n\t\t\tif (sp->teardown.multi)\n\t\t\t\tcpuhp_rollback_install(cpu, state, node);\n\t\t\tgoto unlock;\n\t\t}\n\t}\nadd_node:\n\tret = 0;\n\thlist_add_head(node, &sp->list);\nunlock:\n\tmutex_unlock(&cpuhp_state_mutex);\n\treturn ret;\n}\n\nint __cpuhp_state_add_instance(enum cpuhp_state state, struct hlist_node *node,\n\t\t\t       bool invoke)\n{\n\tint ret;\n\n\tcpus_read_lock();\n\tret = __cpuhp_state_add_instance_cpuslocked(state, node, invoke);\n\tcpus_read_unlock();\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(__cpuhp_state_add_instance);\n\n \nint __cpuhp_setup_state_cpuslocked(enum cpuhp_state state,\n\t\t\t\t   const char *name, bool invoke,\n\t\t\t\t   int (*startup)(unsigned int cpu),\n\t\t\t\t   int (*teardown)(unsigned int cpu),\n\t\t\t\t   bool multi_instance)\n{\n\tint cpu, ret = 0;\n\tbool dynstate;\n\n\tlockdep_assert_cpus_held();\n\n\tif (cpuhp_cb_check(state) || !name)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&cpuhp_state_mutex);\n\n\tret = cpuhp_store_callbacks(state, name, startup, teardown,\n\t\t\t\t    multi_instance);\n\n\tdynstate = state == CPUHP_AP_ONLINE_DYN;\n\tif (ret > 0 && dynstate) {\n\t\tstate = ret;\n\t\tret = 0;\n\t}\n\n\tif (ret || !invoke || !startup)\n\t\tgoto out;\n\n\t \n\tfor_each_present_cpu(cpu) {\n\t\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\n\t\tint cpustate = st->state;\n\n\t\tif (cpustate < state)\n\t\t\tcontinue;\n\n\t\tret = cpuhp_issue_call(cpu, state, true, NULL);\n\t\tif (ret) {\n\t\t\tif (teardown)\n\t\t\t\tcpuhp_rollback_install(cpu, state, NULL);\n\t\t\tcpuhp_store_callbacks(state, NULL, NULL, NULL, false);\n\t\t\tgoto out;\n\t\t}\n\t}\nout:\n\tmutex_unlock(&cpuhp_state_mutex);\n\t \n\tif (!ret && dynstate)\n\t\treturn state;\n\treturn ret;\n}\nEXPORT_SYMBOL(__cpuhp_setup_state_cpuslocked);\n\nint __cpuhp_setup_state(enum cpuhp_state state,\n\t\t\tconst char *name, bool invoke,\n\t\t\tint (*startup)(unsigned int cpu),\n\t\t\tint (*teardown)(unsigned int cpu),\n\t\t\tbool multi_instance)\n{\n\tint ret;\n\n\tcpus_read_lock();\n\tret = __cpuhp_setup_state_cpuslocked(state, name, invoke, startup,\n\t\t\t\t\t     teardown, multi_instance);\n\tcpus_read_unlock();\n\treturn ret;\n}\nEXPORT_SYMBOL(__cpuhp_setup_state);\n\nint __cpuhp_state_remove_instance(enum cpuhp_state state,\n\t\t\t\t  struct hlist_node *node, bool invoke)\n{\n\tstruct cpuhp_step *sp = cpuhp_get_step(state);\n\tint cpu;\n\n\tBUG_ON(cpuhp_cb_check(state));\n\n\tif (!sp->multi_instance)\n\t\treturn -EINVAL;\n\n\tcpus_read_lock();\n\tmutex_lock(&cpuhp_state_mutex);\n\n\tif (!invoke || !cpuhp_get_teardown_cb(state))\n\t\tgoto remove;\n\t \n\tfor_each_present_cpu(cpu) {\n\t\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\n\t\tint cpustate = st->state;\n\n\t\tif (cpustate >= state)\n\t\t\tcpuhp_issue_call(cpu, state, false, node);\n\t}\n\nremove:\n\thlist_del(node);\n\tmutex_unlock(&cpuhp_state_mutex);\n\tcpus_read_unlock();\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(__cpuhp_state_remove_instance);\n\n \nvoid __cpuhp_remove_state_cpuslocked(enum cpuhp_state state, bool invoke)\n{\n\tstruct cpuhp_step *sp = cpuhp_get_step(state);\n\tint cpu;\n\n\tBUG_ON(cpuhp_cb_check(state));\n\n\tlockdep_assert_cpus_held();\n\n\tmutex_lock(&cpuhp_state_mutex);\n\tif (sp->multi_instance) {\n\t\tWARN(!hlist_empty(&sp->list),\n\t\t     \"Error: Removing state %d which has instances left.\\n\",\n\t\t     state);\n\t\tgoto remove;\n\t}\n\n\tif (!invoke || !cpuhp_get_teardown_cb(state))\n\t\tgoto remove;\n\n\t \n\tfor_each_present_cpu(cpu) {\n\t\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\n\t\tint cpustate = st->state;\n\n\t\tif (cpustate >= state)\n\t\t\tcpuhp_issue_call(cpu, state, false, NULL);\n\t}\nremove:\n\tcpuhp_store_callbacks(state, NULL, NULL, NULL, false);\n\tmutex_unlock(&cpuhp_state_mutex);\n}\nEXPORT_SYMBOL(__cpuhp_remove_state_cpuslocked);\n\nvoid __cpuhp_remove_state(enum cpuhp_state state, bool invoke)\n{\n\tcpus_read_lock();\n\t__cpuhp_remove_state_cpuslocked(state, invoke);\n\tcpus_read_unlock();\n}\nEXPORT_SYMBOL(__cpuhp_remove_state);\n\n#ifdef CONFIG_HOTPLUG_SMT\nstatic void cpuhp_offline_cpu_device(unsigned int cpu)\n{\n\tstruct device *dev = get_cpu_device(cpu);\n\n\tdev->offline = true;\n\t \n\tkobject_uevent(&dev->kobj, KOBJ_OFFLINE);\n}\n\nstatic void cpuhp_online_cpu_device(unsigned int cpu)\n{\n\tstruct device *dev = get_cpu_device(cpu);\n\n\tdev->offline = false;\n\t \n\tkobject_uevent(&dev->kobj, KOBJ_ONLINE);\n}\n\nint cpuhp_smt_disable(enum cpuhp_smt_control ctrlval)\n{\n\tint cpu, ret = 0;\n\n\tcpu_maps_update_begin();\n\tfor_each_online_cpu(cpu) {\n\t\tif (topology_is_primary_thread(cpu))\n\t\t\tcontinue;\n\t\t \n\t\tif (ctrlval == CPU_SMT_ENABLED && cpu_smt_thread_allowed(cpu))\n\t\t\tcontinue;\n\t\tret = cpu_down_maps_locked(cpu, CPUHP_OFFLINE);\n\t\tif (ret)\n\t\t\tbreak;\n\t\t \n\t\tcpuhp_offline_cpu_device(cpu);\n\t}\n\tif (!ret)\n\t\tcpu_smt_control = ctrlval;\n\tcpu_maps_update_done();\n\treturn ret;\n}\n\nint cpuhp_smt_enable(void)\n{\n\tint cpu, ret = 0;\n\n\tcpu_maps_update_begin();\n\tcpu_smt_control = CPU_SMT_ENABLED;\n\tfor_each_present_cpu(cpu) {\n\t\t \n\t\tif (cpu_online(cpu) || !node_online(cpu_to_node(cpu)))\n\t\t\tcontinue;\n\t\tif (!cpu_smt_thread_allowed(cpu))\n\t\t\tcontinue;\n\t\tret = _cpu_up(cpu, 0, CPUHP_ONLINE);\n\t\tif (ret)\n\t\t\tbreak;\n\t\t \n\t\tcpuhp_online_cpu_device(cpu);\n\t}\n\tcpu_maps_update_done();\n\treturn ret;\n}\n#endif\n\n#if defined(CONFIG_SYSFS) && defined(CONFIG_HOTPLUG_CPU)\nstatic ssize_t state_show(struct device *dev,\n\t\t\t  struct device_attribute *attr, char *buf)\n{\n\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, dev->id);\n\n\treturn sprintf(buf, \"%d\\n\", st->state);\n}\nstatic DEVICE_ATTR_RO(state);\n\nstatic ssize_t target_store(struct device *dev, struct device_attribute *attr,\n\t\t\t    const char *buf, size_t count)\n{\n\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, dev->id);\n\tstruct cpuhp_step *sp;\n\tint target, ret;\n\n\tret = kstrtoint(buf, 10, &target);\n\tif (ret)\n\t\treturn ret;\n\n#ifdef CONFIG_CPU_HOTPLUG_STATE_CONTROL\n\tif (target < CPUHP_OFFLINE || target > CPUHP_ONLINE)\n\t\treturn -EINVAL;\n#else\n\tif (target != CPUHP_OFFLINE && target != CPUHP_ONLINE)\n\t\treturn -EINVAL;\n#endif\n\n\tret = lock_device_hotplug_sysfs();\n\tif (ret)\n\t\treturn ret;\n\n\tmutex_lock(&cpuhp_state_mutex);\n\tsp = cpuhp_get_step(target);\n\tret = !sp->name || sp->cant_stop ? -EINVAL : 0;\n\tmutex_unlock(&cpuhp_state_mutex);\n\tif (ret)\n\t\tgoto out;\n\n\tif (st->state < target)\n\t\tret = cpu_up(dev->id, target);\n\telse if (st->state > target)\n\t\tret = cpu_down(dev->id, target);\n\telse if (WARN_ON(st->target != target))\n\t\tst->target = target;\nout:\n\tunlock_device_hotplug();\n\treturn ret ? ret : count;\n}\n\nstatic ssize_t target_show(struct device *dev,\n\t\t\t   struct device_attribute *attr, char *buf)\n{\n\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, dev->id);\n\n\treturn sprintf(buf, \"%d\\n\", st->target);\n}\nstatic DEVICE_ATTR_RW(target);\n\nstatic ssize_t fail_store(struct device *dev, struct device_attribute *attr,\n\t\t\t  const char *buf, size_t count)\n{\n\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, dev->id);\n\tstruct cpuhp_step *sp;\n\tint fail, ret;\n\n\tret = kstrtoint(buf, 10, &fail);\n\tif (ret)\n\t\treturn ret;\n\n\tif (fail == CPUHP_INVALID) {\n\t\tst->fail = fail;\n\t\treturn count;\n\t}\n\n\tif (fail < CPUHP_OFFLINE || fail > CPUHP_ONLINE)\n\t\treturn -EINVAL;\n\n\t \n\tif (cpuhp_is_atomic_state(fail))\n\t\treturn -EINVAL;\n\n\t \n\tif (fail <= CPUHP_BRINGUP_CPU && st->state > CPUHP_BRINGUP_CPU)\n\t\treturn -EINVAL;\n\n\t \n\tmutex_lock(&cpuhp_state_mutex);\n\tsp = cpuhp_get_step(fail);\n\tif (!sp->startup.single && !sp->teardown.single)\n\t\tret = -EINVAL;\n\tmutex_unlock(&cpuhp_state_mutex);\n\tif (ret)\n\t\treturn ret;\n\n\tst->fail = fail;\n\n\treturn count;\n}\n\nstatic ssize_t fail_show(struct device *dev,\n\t\t\t struct device_attribute *attr, char *buf)\n{\n\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, dev->id);\n\n\treturn sprintf(buf, \"%d\\n\", st->fail);\n}\n\nstatic DEVICE_ATTR_RW(fail);\n\nstatic struct attribute *cpuhp_cpu_attrs[] = {\n\t&dev_attr_state.attr,\n\t&dev_attr_target.attr,\n\t&dev_attr_fail.attr,\n\tNULL\n};\n\nstatic const struct attribute_group cpuhp_cpu_attr_group = {\n\t.attrs = cpuhp_cpu_attrs,\n\t.name = \"hotplug\",\n\tNULL\n};\n\nstatic ssize_t states_show(struct device *dev,\n\t\t\t\t struct device_attribute *attr, char *buf)\n{\n\tssize_t cur, res = 0;\n\tint i;\n\n\tmutex_lock(&cpuhp_state_mutex);\n\tfor (i = CPUHP_OFFLINE; i <= CPUHP_ONLINE; i++) {\n\t\tstruct cpuhp_step *sp = cpuhp_get_step(i);\n\n\t\tif (sp->name) {\n\t\t\tcur = sprintf(buf, \"%3d: %s\\n\", i, sp->name);\n\t\t\tbuf += cur;\n\t\t\tres += cur;\n\t\t}\n\t}\n\tmutex_unlock(&cpuhp_state_mutex);\n\treturn res;\n}\nstatic DEVICE_ATTR_RO(states);\n\nstatic struct attribute *cpuhp_cpu_root_attrs[] = {\n\t&dev_attr_states.attr,\n\tNULL\n};\n\nstatic const struct attribute_group cpuhp_cpu_root_attr_group = {\n\t.attrs = cpuhp_cpu_root_attrs,\n\t.name = \"hotplug\",\n\tNULL\n};\n\n#ifdef CONFIG_HOTPLUG_SMT\n\nstatic bool cpu_smt_num_threads_valid(unsigned int threads)\n{\n\tif (IS_ENABLED(CONFIG_SMT_NUM_THREADS_DYNAMIC))\n\t\treturn threads >= 1 && threads <= cpu_smt_max_threads;\n\treturn threads == 1 || threads == cpu_smt_max_threads;\n}\n\nstatic ssize_t\n__store_smt_control(struct device *dev, struct device_attribute *attr,\n\t\t    const char *buf, size_t count)\n{\n\tint ctrlval, ret, num_threads, orig_threads;\n\tbool force_off;\n\n\tif (cpu_smt_control == CPU_SMT_FORCE_DISABLED)\n\t\treturn -EPERM;\n\n\tif (cpu_smt_control == CPU_SMT_NOT_SUPPORTED)\n\t\treturn -ENODEV;\n\n\tif (sysfs_streq(buf, \"on\")) {\n\t\tctrlval = CPU_SMT_ENABLED;\n\t\tnum_threads = cpu_smt_max_threads;\n\t} else if (sysfs_streq(buf, \"off\")) {\n\t\tctrlval = CPU_SMT_DISABLED;\n\t\tnum_threads = 1;\n\t} else if (sysfs_streq(buf, \"forceoff\")) {\n\t\tctrlval = CPU_SMT_FORCE_DISABLED;\n\t\tnum_threads = 1;\n\t} else if (kstrtoint(buf, 10, &num_threads) == 0) {\n\t\tif (num_threads == 1)\n\t\t\tctrlval = CPU_SMT_DISABLED;\n\t\telse if (cpu_smt_num_threads_valid(num_threads))\n\t\t\tctrlval = CPU_SMT_ENABLED;\n\t\telse\n\t\t\treturn -EINVAL;\n\t} else {\n\t\treturn -EINVAL;\n\t}\n\n\tret = lock_device_hotplug_sysfs();\n\tif (ret)\n\t\treturn ret;\n\n\torig_threads = cpu_smt_num_threads;\n\tcpu_smt_num_threads = num_threads;\n\n\tforce_off = ctrlval != cpu_smt_control && ctrlval == CPU_SMT_FORCE_DISABLED;\n\n\tif (num_threads > orig_threads)\n\t\tret = cpuhp_smt_enable();\n\telse if (num_threads < orig_threads || force_off)\n\t\tret = cpuhp_smt_disable(ctrlval);\n\n\tunlock_device_hotplug();\n\treturn ret ? ret : count;\n}\n\n#else  \nstatic ssize_t\n__store_smt_control(struct device *dev, struct device_attribute *attr,\n\t\t    const char *buf, size_t count)\n{\n\treturn -ENODEV;\n}\n#endif  \n\nstatic const char *smt_states[] = {\n\t[CPU_SMT_ENABLED]\t\t= \"on\",\n\t[CPU_SMT_DISABLED]\t\t= \"off\",\n\t[CPU_SMT_FORCE_DISABLED]\t= \"forceoff\",\n\t[CPU_SMT_NOT_SUPPORTED]\t\t= \"notsupported\",\n\t[CPU_SMT_NOT_IMPLEMENTED]\t= \"notimplemented\",\n};\n\nstatic ssize_t control_show(struct device *dev,\n\t\t\t    struct device_attribute *attr, char *buf)\n{\n\tconst char *state = smt_states[cpu_smt_control];\n\n#ifdef CONFIG_HOTPLUG_SMT\n\t \n\tif (cpu_smt_control == CPU_SMT_ENABLED &&\n\t    cpu_smt_num_threads != cpu_smt_max_threads)\n\t\treturn sysfs_emit(buf, \"%d\\n\", cpu_smt_num_threads);\n#endif\n\n\treturn snprintf(buf, PAGE_SIZE - 2, \"%s\\n\", state);\n}\n\nstatic ssize_t control_store(struct device *dev, struct device_attribute *attr,\n\t\t\t     const char *buf, size_t count)\n{\n\treturn __store_smt_control(dev, attr, buf, count);\n}\nstatic DEVICE_ATTR_RW(control);\n\nstatic ssize_t active_show(struct device *dev,\n\t\t\t   struct device_attribute *attr, char *buf)\n{\n\treturn snprintf(buf, PAGE_SIZE - 2, \"%d\\n\", sched_smt_active());\n}\nstatic DEVICE_ATTR_RO(active);\n\nstatic struct attribute *cpuhp_smt_attrs[] = {\n\t&dev_attr_control.attr,\n\t&dev_attr_active.attr,\n\tNULL\n};\n\nstatic const struct attribute_group cpuhp_smt_attr_group = {\n\t.attrs = cpuhp_smt_attrs,\n\t.name = \"smt\",\n\tNULL\n};\n\nstatic int __init cpu_smt_sysfs_init(void)\n{\n\tstruct device *dev_root;\n\tint ret = -ENODEV;\n\n\tdev_root = bus_get_dev_root(&cpu_subsys);\n\tif (dev_root) {\n\t\tret = sysfs_create_group(&dev_root->kobj, &cpuhp_smt_attr_group);\n\t\tput_device(dev_root);\n\t}\n\treturn ret;\n}\n\nstatic int __init cpuhp_sysfs_init(void)\n{\n\tstruct device *dev_root;\n\tint cpu, ret;\n\n\tret = cpu_smt_sysfs_init();\n\tif (ret)\n\t\treturn ret;\n\n\tdev_root = bus_get_dev_root(&cpu_subsys);\n\tif (dev_root) {\n\t\tret = sysfs_create_group(&dev_root->kobj, &cpuhp_cpu_root_attr_group);\n\t\tput_device(dev_root);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct device *dev = get_cpu_device(cpu);\n\n\t\tif (!dev)\n\t\t\tcontinue;\n\t\tret = sysfs_create_group(&dev->kobj, &cpuhp_cpu_attr_group);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\treturn 0;\n}\ndevice_initcall(cpuhp_sysfs_init);\n#endif  \n\n \n\n \n#define MASK_DECLARE_1(x)\t[x+1][0] = (1UL << (x))\n#define MASK_DECLARE_2(x)\tMASK_DECLARE_1(x), MASK_DECLARE_1(x+1)\n#define MASK_DECLARE_4(x)\tMASK_DECLARE_2(x), MASK_DECLARE_2(x+2)\n#define MASK_DECLARE_8(x)\tMASK_DECLARE_4(x), MASK_DECLARE_4(x+4)\n\nconst unsigned long cpu_bit_bitmap[BITS_PER_LONG+1][BITS_TO_LONGS(NR_CPUS)] = {\n\n\tMASK_DECLARE_8(0),\tMASK_DECLARE_8(8),\n\tMASK_DECLARE_8(16),\tMASK_DECLARE_8(24),\n#if BITS_PER_LONG > 32\n\tMASK_DECLARE_8(32),\tMASK_DECLARE_8(40),\n\tMASK_DECLARE_8(48),\tMASK_DECLARE_8(56),\n#endif\n};\nEXPORT_SYMBOL_GPL(cpu_bit_bitmap);\n\nconst DECLARE_BITMAP(cpu_all_bits, NR_CPUS) = CPU_BITS_ALL;\nEXPORT_SYMBOL(cpu_all_bits);\n\n#ifdef CONFIG_INIT_ALL_POSSIBLE\nstruct cpumask __cpu_possible_mask __read_mostly\n\t= {CPU_BITS_ALL};\n#else\nstruct cpumask __cpu_possible_mask __read_mostly;\n#endif\nEXPORT_SYMBOL(__cpu_possible_mask);\n\nstruct cpumask __cpu_online_mask __read_mostly;\nEXPORT_SYMBOL(__cpu_online_mask);\n\nstruct cpumask __cpu_present_mask __read_mostly;\nEXPORT_SYMBOL(__cpu_present_mask);\n\nstruct cpumask __cpu_active_mask __read_mostly;\nEXPORT_SYMBOL(__cpu_active_mask);\n\nstruct cpumask __cpu_dying_mask __read_mostly;\nEXPORT_SYMBOL(__cpu_dying_mask);\n\natomic_t __num_online_cpus __read_mostly;\nEXPORT_SYMBOL(__num_online_cpus);\n\nvoid init_cpu_present(const struct cpumask *src)\n{\n\tcpumask_copy(&__cpu_present_mask, src);\n}\n\nvoid init_cpu_possible(const struct cpumask *src)\n{\n\tcpumask_copy(&__cpu_possible_mask, src);\n}\n\nvoid init_cpu_online(const struct cpumask *src)\n{\n\tcpumask_copy(&__cpu_online_mask, src);\n}\n\nvoid set_cpu_online(unsigned int cpu, bool online)\n{\n\t \n\tif (online) {\n\t\tif (!cpumask_test_and_set_cpu(cpu, &__cpu_online_mask))\n\t\t\tatomic_inc(&__num_online_cpus);\n\t} else {\n\t\tif (cpumask_test_and_clear_cpu(cpu, &__cpu_online_mask))\n\t\t\tatomic_dec(&__num_online_cpus);\n\t}\n}\n\n \nvoid __init boot_cpu_init(void)\n{\n\tint cpu = smp_processor_id();\n\n\t \n\tset_cpu_online(cpu, true);\n\tset_cpu_active(cpu, true);\n\tset_cpu_present(cpu, true);\n\tset_cpu_possible(cpu, true);\n\n#ifdef CONFIG_SMP\n\t__boot_cpu_id = cpu;\n#endif\n}\n\n \nvoid __init boot_cpu_hotplug_init(void)\n{\n#ifdef CONFIG_SMP\n\tcpumask_set_cpu(smp_processor_id(), &cpus_booted_once_mask);\n\tatomic_set(this_cpu_ptr(&cpuhp_state.ap_sync_state), SYNC_STATE_ONLINE);\n#endif\n\tthis_cpu_write(cpuhp_state.state, CPUHP_ONLINE);\n\tthis_cpu_write(cpuhp_state.target, CPUHP_ONLINE);\n}\n\n \nenum cpu_mitigations {\n\tCPU_MITIGATIONS_OFF,\n\tCPU_MITIGATIONS_AUTO,\n\tCPU_MITIGATIONS_AUTO_NOSMT,\n};\n\nstatic enum cpu_mitigations cpu_mitigations __ro_after_init =\n\tCPU_MITIGATIONS_AUTO;\n\nstatic int __init mitigations_parse_cmdline(char *arg)\n{\n\tif (!strcmp(arg, \"off\"))\n\t\tcpu_mitigations = CPU_MITIGATIONS_OFF;\n\telse if (!strcmp(arg, \"auto\"))\n\t\tcpu_mitigations = CPU_MITIGATIONS_AUTO;\n\telse if (!strcmp(arg, \"auto,nosmt\"))\n\t\tcpu_mitigations = CPU_MITIGATIONS_AUTO_NOSMT;\n\telse\n\t\tpr_crit(\"Unsupported mitigations=%s, system may still be vulnerable\\n\",\n\t\t\targ);\n\n\treturn 0;\n}\nearly_param(\"mitigations\", mitigations_parse_cmdline);\n\n \nbool cpu_mitigations_off(void)\n{\n\treturn cpu_mitigations == CPU_MITIGATIONS_OFF;\n}\nEXPORT_SYMBOL_GPL(cpu_mitigations_off);\n\n \nbool cpu_mitigations_auto_nosmt(void)\n{\n\treturn cpu_mitigations == CPU_MITIGATIONS_AUTO_NOSMT;\n}\nEXPORT_SYMBOL_GPL(cpu_mitigations_auto_nosmt);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}