{
  "module_name": "kthread.c",
  "hash_id": "1e077bc61f647eed6df5cf02acda1d78837e2708522fd3877ed92727eefbe3e2",
  "original_prompt": "Ingested from linux-6.6.14/kernel/kthread.c",
  "human_readable_source": "\n \n#include <uapi/linux/sched/types.h>\n#include <linux/mm.h>\n#include <linux/mmu_context.h>\n#include <linux/sched.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/task.h>\n#include <linux/kthread.h>\n#include <linux/completion.h>\n#include <linux/err.h>\n#include <linux/cgroup.h>\n#include <linux/cpuset.h>\n#include <linux/unistd.h>\n#include <linux/file.h>\n#include <linux/export.h>\n#include <linux/mutex.h>\n#include <linux/slab.h>\n#include <linux/freezer.h>\n#include <linux/ptrace.h>\n#include <linux/uaccess.h>\n#include <linux/numa.h>\n#include <linux/sched/isolation.h>\n#include <trace/events/sched.h>\n\n\nstatic DEFINE_SPINLOCK(kthread_create_lock);\nstatic LIST_HEAD(kthread_create_list);\nstruct task_struct *kthreadd_task;\n\nstruct kthread_create_info\n{\n\t \n\tchar *full_name;\n\tint (*threadfn)(void *data);\n\tvoid *data;\n\tint node;\n\n\t \n\tstruct task_struct *result;\n\tstruct completion *done;\n\n\tstruct list_head list;\n};\n\nstruct kthread {\n\tunsigned long flags;\n\tunsigned int cpu;\n\tint result;\n\tint (*threadfn)(void *);\n\tvoid *data;\n\tstruct completion parked;\n\tstruct completion exited;\n#ifdef CONFIG_BLK_CGROUP\n\tstruct cgroup_subsys_state *blkcg_css;\n#endif\n\t \n\tchar *full_name;\n};\n\nenum KTHREAD_BITS {\n\tKTHREAD_IS_PER_CPU = 0,\n\tKTHREAD_SHOULD_STOP,\n\tKTHREAD_SHOULD_PARK,\n};\n\nstatic inline struct kthread *to_kthread(struct task_struct *k)\n{\n\tWARN_ON(!(k->flags & PF_KTHREAD));\n\treturn k->worker_private;\n}\n\n \nstatic inline struct kthread *__to_kthread(struct task_struct *p)\n{\n\tvoid *kthread = p->worker_private;\n\tif (kthread && !(p->flags & PF_KTHREAD))\n\t\tkthread = NULL;\n\treturn kthread;\n}\n\nvoid get_kthread_comm(char *buf, size_t buf_size, struct task_struct *tsk)\n{\n\tstruct kthread *kthread = to_kthread(tsk);\n\n\tif (!kthread || !kthread->full_name) {\n\t\t__get_task_comm(buf, buf_size, tsk);\n\t\treturn;\n\t}\n\n\tstrscpy_pad(buf, kthread->full_name, buf_size);\n}\n\nbool set_kthread_struct(struct task_struct *p)\n{\n\tstruct kthread *kthread;\n\n\tif (WARN_ON_ONCE(to_kthread(p)))\n\t\treturn false;\n\n\tkthread = kzalloc(sizeof(*kthread), GFP_KERNEL);\n\tif (!kthread)\n\t\treturn false;\n\n\tinit_completion(&kthread->exited);\n\tinit_completion(&kthread->parked);\n\tp->vfork_done = &kthread->exited;\n\n\tp->worker_private = kthread;\n\treturn true;\n}\n\nvoid free_kthread_struct(struct task_struct *k)\n{\n\tstruct kthread *kthread;\n\n\t \n\tkthread = to_kthread(k);\n\tif (!kthread)\n\t\treturn;\n\n#ifdef CONFIG_BLK_CGROUP\n\tWARN_ON_ONCE(kthread->blkcg_css);\n#endif\n\tk->worker_private = NULL;\n\tkfree(kthread->full_name);\n\tkfree(kthread);\n}\n\n \nbool kthread_should_stop(void)\n{\n\treturn test_bit(KTHREAD_SHOULD_STOP, &to_kthread(current)->flags);\n}\nEXPORT_SYMBOL(kthread_should_stop);\n\nstatic bool __kthread_should_park(struct task_struct *k)\n{\n\treturn test_bit(KTHREAD_SHOULD_PARK, &to_kthread(k)->flags);\n}\n\n \nbool kthread_should_park(void)\n{\n\treturn __kthread_should_park(current);\n}\nEXPORT_SYMBOL_GPL(kthread_should_park);\n\nbool kthread_should_stop_or_park(void)\n{\n\tstruct kthread *kthread = __to_kthread(current);\n\n\tif (!kthread)\n\t\treturn false;\n\n\treturn kthread->flags & (BIT(KTHREAD_SHOULD_STOP) | BIT(KTHREAD_SHOULD_PARK));\n}\n\n \nbool kthread_freezable_should_stop(bool *was_frozen)\n{\n\tbool frozen = false;\n\n\tmight_sleep();\n\n\tif (unlikely(freezing(current)))\n\t\tfrozen = __refrigerator(true);\n\n\tif (was_frozen)\n\t\t*was_frozen = frozen;\n\n\treturn kthread_should_stop();\n}\nEXPORT_SYMBOL_GPL(kthread_freezable_should_stop);\n\n \nvoid *kthread_func(struct task_struct *task)\n{\n\tstruct kthread *kthread = __to_kthread(task);\n\tif (kthread)\n\t\treturn kthread->threadfn;\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(kthread_func);\n\n \nvoid *kthread_data(struct task_struct *task)\n{\n\treturn to_kthread(task)->data;\n}\nEXPORT_SYMBOL_GPL(kthread_data);\n\n \nvoid *kthread_probe_data(struct task_struct *task)\n{\n\tstruct kthread *kthread = __to_kthread(task);\n\tvoid *data = NULL;\n\n\tif (kthread)\n\t\tcopy_from_kernel_nofault(&data, &kthread->data, sizeof(data));\n\treturn data;\n}\n\nstatic void __kthread_parkme(struct kthread *self)\n{\n\tfor (;;) {\n\t\t \n\t\tset_special_state(TASK_PARKED);\n\t\tif (!test_bit(KTHREAD_SHOULD_PARK, &self->flags))\n\t\t\tbreak;\n\n\t\t \n\t\tpreempt_disable();\n\t\tcomplete(&self->parked);\n\t\tschedule_preempt_disabled();\n\t\tpreempt_enable();\n\t}\n\t__set_current_state(TASK_RUNNING);\n}\n\nvoid kthread_parkme(void)\n{\n\t__kthread_parkme(to_kthread(current));\n}\nEXPORT_SYMBOL_GPL(kthread_parkme);\n\n \nvoid __noreturn kthread_exit(long result)\n{\n\tstruct kthread *kthread = to_kthread(current);\n\tkthread->result = result;\n\tdo_exit(0);\n}\n\n \nvoid __noreturn kthread_complete_and_exit(struct completion *comp, long code)\n{\n\tif (comp)\n\t\tcomplete(comp);\n\n\tkthread_exit(code);\n}\nEXPORT_SYMBOL(kthread_complete_and_exit);\n\nstatic int kthread(void *_create)\n{\n\tstatic const struct sched_param param = { .sched_priority = 0 };\n\t \n\tstruct kthread_create_info *create = _create;\n\tint (*threadfn)(void *data) = create->threadfn;\n\tvoid *data = create->data;\n\tstruct completion *done;\n\tstruct kthread *self;\n\tint ret;\n\n\tself = to_kthread(current);\n\n\t \n\tdone = xchg(&create->done, NULL);\n\tif (!done) {\n\t\tkfree(create->full_name);\n\t\tkfree(create);\n\t\tkthread_exit(-EINTR);\n\t}\n\n\tself->full_name = create->full_name;\n\tself->threadfn = threadfn;\n\tself->data = data;\n\n\t \n\tsched_setscheduler_nocheck(current, SCHED_NORMAL, &param);\n\tset_cpus_allowed_ptr(current, housekeeping_cpumask(HK_TYPE_KTHREAD));\n\n\t \n\t__set_current_state(TASK_UNINTERRUPTIBLE);\n\tcreate->result = current;\n\t \n\tpreempt_disable();\n\tcomplete(done);\n\tschedule_preempt_disabled();\n\tpreempt_enable();\n\n\tret = -EINTR;\n\tif (!test_bit(KTHREAD_SHOULD_STOP, &self->flags)) {\n\t\tcgroup_kthread_ready();\n\t\t__kthread_parkme(self);\n\t\tret = threadfn(data);\n\t}\n\tkthread_exit(ret);\n}\n\n \nint tsk_fork_get_node(struct task_struct *tsk)\n{\n#ifdef CONFIG_NUMA\n\tif (tsk == kthreadd_task)\n\t\treturn tsk->pref_node_fork;\n#endif\n\treturn NUMA_NO_NODE;\n}\n\nstatic void create_kthread(struct kthread_create_info *create)\n{\n\tint pid;\n\n#ifdef CONFIG_NUMA\n\tcurrent->pref_node_fork = create->node;\n#endif\n\t \n\tpid = kernel_thread(kthread, create, create->full_name,\n\t\t\t    CLONE_FS | CLONE_FILES | SIGCHLD);\n\tif (pid < 0) {\n\t\t \n\t\tstruct completion *done = xchg(&create->done, NULL);\n\n\t\tkfree(create->full_name);\n\t\tif (!done) {\n\t\t\tkfree(create);\n\t\t\treturn;\n\t\t}\n\t\tcreate->result = ERR_PTR(pid);\n\t\tcomplete(done);\n\t}\n}\n\nstatic __printf(4, 0)\nstruct task_struct *__kthread_create_on_node(int (*threadfn)(void *data),\n\t\t\t\t\t\t    void *data, int node,\n\t\t\t\t\t\t    const char namefmt[],\n\t\t\t\t\t\t    va_list args)\n{\n\tDECLARE_COMPLETION_ONSTACK(done);\n\tstruct task_struct *task;\n\tstruct kthread_create_info *create = kmalloc(sizeof(*create),\n\t\t\t\t\t\t     GFP_KERNEL);\n\n\tif (!create)\n\t\treturn ERR_PTR(-ENOMEM);\n\tcreate->threadfn = threadfn;\n\tcreate->data = data;\n\tcreate->node = node;\n\tcreate->done = &done;\n\tcreate->full_name = kvasprintf(GFP_KERNEL, namefmt, args);\n\tif (!create->full_name) {\n\t\ttask = ERR_PTR(-ENOMEM);\n\t\tgoto free_create;\n\t}\n\n\tspin_lock(&kthread_create_lock);\n\tlist_add_tail(&create->list, &kthread_create_list);\n\tspin_unlock(&kthread_create_lock);\n\n\twake_up_process(kthreadd_task);\n\t \n\tif (unlikely(wait_for_completion_killable(&done))) {\n\t\t \n\t\tif (xchg(&create->done, NULL))\n\t\t\treturn ERR_PTR(-EINTR);\n\t\t \n\t\twait_for_completion(&done);\n\t}\n\ttask = create->result;\nfree_create:\n\tkfree(create);\n\treturn task;\n}\n\n \nstruct task_struct *kthread_create_on_node(int (*threadfn)(void *data),\n\t\t\t\t\t   void *data, int node,\n\t\t\t\t\t   const char namefmt[],\n\t\t\t\t\t   ...)\n{\n\tstruct task_struct *task;\n\tva_list args;\n\n\tva_start(args, namefmt);\n\ttask = __kthread_create_on_node(threadfn, data, node, namefmt, args);\n\tva_end(args);\n\n\treturn task;\n}\nEXPORT_SYMBOL(kthread_create_on_node);\n\nstatic void __kthread_bind_mask(struct task_struct *p, const struct cpumask *mask, unsigned int state)\n{\n\tunsigned long flags;\n\n\tif (!wait_task_inactive(p, state)) {\n\t\tWARN_ON(1);\n\t\treturn;\n\t}\n\n\t \n\traw_spin_lock_irqsave(&p->pi_lock, flags);\n\tdo_set_cpus_allowed(p, mask);\n\tp->flags |= PF_NO_SETAFFINITY;\n\traw_spin_unlock_irqrestore(&p->pi_lock, flags);\n}\n\nstatic void __kthread_bind(struct task_struct *p, unsigned int cpu, unsigned int state)\n{\n\t__kthread_bind_mask(p, cpumask_of(cpu), state);\n}\n\nvoid kthread_bind_mask(struct task_struct *p, const struct cpumask *mask)\n{\n\t__kthread_bind_mask(p, mask, TASK_UNINTERRUPTIBLE);\n}\n\n \nvoid kthread_bind(struct task_struct *p, unsigned int cpu)\n{\n\t__kthread_bind(p, cpu, TASK_UNINTERRUPTIBLE);\n}\nEXPORT_SYMBOL(kthread_bind);\n\n \nstruct task_struct *kthread_create_on_cpu(int (*threadfn)(void *data),\n\t\t\t\t\t  void *data, unsigned int cpu,\n\t\t\t\t\t  const char *namefmt)\n{\n\tstruct task_struct *p;\n\n\tp = kthread_create_on_node(threadfn, data, cpu_to_node(cpu), namefmt,\n\t\t\t\t   cpu);\n\tif (IS_ERR(p))\n\t\treturn p;\n\tkthread_bind(p, cpu);\n\t \n\tto_kthread(p)->cpu = cpu;\n\treturn p;\n}\nEXPORT_SYMBOL(kthread_create_on_cpu);\n\nvoid kthread_set_per_cpu(struct task_struct *k, int cpu)\n{\n\tstruct kthread *kthread = to_kthread(k);\n\tif (!kthread)\n\t\treturn;\n\n\tWARN_ON_ONCE(!(k->flags & PF_NO_SETAFFINITY));\n\n\tif (cpu < 0) {\n\t\tclear_bit(KTHREAD_IS_PER_CPU, &kthread->flags);\n\t\treturn;\n\t}\n\n\tkthread->cpu = cpu;\n\tset_bit(KTHREAD_IS_PER_CPU, &kthread->flags);\n}\n\nbool kthread_is_per_cpu(struct task_struct *p)\n{\n\tstruct kthread *kthread = __to_kthread(p);\n\tif (!kthread)\n\t\treturn false;\n\n\treturn test_bit(KTHREAD_IS_PER_CPU, &kthread->flags);\n}\n\n \nvoid kthread_unpark(struct task_struct *k)\n{\n\tstruct kthread *kthread = to_kthread(k);\n\n\t \n\tif (test_bit(KTHREAD_IS_PER_CPU, &kthread->flags))\n\t\t__kthread_bind(k, kthread->cpu, TASK_PARKED);\n\n\tclear_bit(KTHREAD_SHOULD_PARK, &kthread->flags);\n\t \n\twake_up_state(k, TASK_PARKED);\n}\nEXPORT_SYMBOL_GPL(kthread_unpark);\n\n \nint kthread_park(struct task_struct *k)\n{\n\tstruct kthread *kthread = to_kthread(k);\n\n\tif (WARN_ON(k->flags & PF_EXITING))\n\t\treturn -ENOSYS;\n\n\tif (WARN_ON_ONCE(test_bit(KTHREAD_SHOULD_PARK, &kthread->flags)))\n\t\treturn -EBUSY;\n\n\tset_bit(KTHREAD_SHOULD_PARK, &kthread->flags);\n\tif (k != current) {\n\t\twake_up_process(k);\n\t\t \n\t\twait_for_completion(&kthread->parked);\n\t\t \n\t\tWARN_ON_ONCE(!wait_task_inactive(k, TASK_PARKED));\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kthread_park);\n\n \nint kthread_stop(struct task_struct *k)\n{\n\tstruct kthread *kthread;\n\tint ret;\n\n\ttrace_sched_kthread_stop(k);\n\n\tget_task_struct(k);\n\tkthread = to_kthread(k);\n\tset_bit(KTHREAD_SHOULD_STOP, &kthread->flags);\n\tkthread_unpark(k);\n\tset_tsk_thread_flag(k, TIF_NOTIFY_SIGNAL);\n\twake_up_process(k);\n\twait_for_completion(&kthread->exited);\n\tret = kthread->result;\n\tput_task_struct(k);\n\n\ttrace_sched_kthread_stop_ret(ret);\n\treturn ret;\n}\nEXPORT_SYMBOL(kthread_stop);\n\nint kthreadd(void *unused)\n{\n\tstruct task_struct *tsk = current;\n\n\t \n\tset_task_comm(tsk, \"kthreadd\");\n\tignore_signals(tsk);\n\tset_cpus_allowed_ptr(tsk, housekeeping_cpumask(HK_TYPE_KTHREAD));\n\tset_mems_allowed(node_states[N_MEMORY]);\n\n\tcurrent->flags |= PF_NOFREEZE;\n\tcgroup_init_kthreadd();\n\n\tfor (;;) {\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\tif (list_empty(&kthread_create_list))\n\t\t\tschedule();\n\t\t__set_current_state(TASK_RUNNING);\n\n\t\tspin_lock(&kthread_create_lock);\n\t\twhile (!list_empty(&kthread_create_list)) {\n\t\t\tstruct kthread_create_info *create;\n\n\t\t\tcreate = list_entry(kthread_create_list.next,\n\t\t\t\t\t    struct kthread_create_info, list);\n\t\t\tlist_del_init(&create->list);\n\t\t\tspin_unlock(&kthread_create_lock);\n\n\t\t\tcreate_kthread(create);\n\n\t\t\tspin_lock(&kthread_create_lock);\n\t\t}\n\t\tspin_unlock(&kthread_create_lock);\n\t}\n\n\treturn 0;\n}\n\nvoid __kthread_init_worker(struct kthread_worker *worker,\n\t\t\t\tconst char *name,\n\t\t\t\tstruct lock_class_key *key)\n{\n\tmemset(worker, 0, sizeof(struct kthread_worker));\n\traw_spin_lock_init(&worker->lock);\n\tlockdep_set_class_and_name(&worker->lock, key, name);\n\tINIT_LIST_HEAD(&worker->work_list);\n\tINIT_LIST_HEAD(&worker->delayed_work_list);\n}\nEXPORT_SYMBOL_GPL(__kthread_init_worker);\n\n \nint kthread_worker_fn(void *worker_ptr)\n{\n\tstruct kthread_worker *worker = worker_ptr;\n\tstruct kthread_work *work;\n\n\t \n\tWARN_ON(worker->task && worker->task != current);\n\tworker->task = current;\n\n\tif (worker->flags & KTW_FREEZABLE)\n\t\tset_freezable();\n\nrepeat:\n\tset_current_state(TASK_INTERRUPTIBLE);\t \n\n\tif (kthread_should_stop()) {\n\t\t__set_current_state(TASK_RUNNING);\n\t\traw_spin_lock_irq(&worker->lock);\n\t\tworker->task = NULL;\n\t\traw_spin_unlock_irq(&worker->lock);\n\t\treturn 0;\n\t}\n\n\twork = NULL;\n\traw_spin_lock_irq(&worker->lock);\n\tif (!list_empty(&worker->work_list)) {\n\t\twork = list_first_entry(&worker->work_list,\n\t\t\t\t\tstruct kthread_work, node);\n\t\tlist_del_init(&work->node);\n\t}\n\tworker->current_work = work;\n\traw_spin_unlock_irq(&worker->lock);\n\n\tif (work) {\n\t\tkthread_work_func_t func = work->func;\n\t\t__set_current_state(TASK_RUNNING);\n\t\ttrace_sched_kthread_work_execute_start(work);\n\t\twork->func(work);\n\t\t \n\t\ttrace_sched_kthread_work_execute_end(work, func);\n\t} else if (!freezing(current))\n\t\tschedule();\n\n\ttry_to_freeze();\n\tcond_resched();\n\tgoto repeat;\n}\nEXPORT_SYMBOL_GPL(kthread_worker_fn);\n\nstatic __printf(3, 0) struct kthread_worker *\n__kthread_create_worker(int cpu, unsigned int flags,\n\t\t\tconst char namefmt[], va_list args)\n{\n\tstruct kthread_worker *worker;\n\tstruct task_struct *task;\n\tint node = NUMA_NO_NODE;\n\n\tworker = kzalloc(sizeof(*worker), GFP_KERNEL);\n\tif (!worker)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tkthread_init_worker(worker);\n\n\tif (cpu >= 0)\n\t\tnode = cpu_to_node(cpu);\n\n\ttask = __kthread_create_on_node(kthread_worker_fn, worker,\n\t\t\t\t\t\tnode, namefmt, args);\n\tif (IS_ERR(task))\n\t\tgoto fail_task;\n\n\tif (cpu >= 0)\n\t\tkthread_bind(task, cpu);\n\n\tworker->flags = flags;\n\tworker->task = task;\n\twake_up_process(task);\n\treturn worker;\n\nfail_task:\n\tkfree(worker);\n\treturn ERR_CAST(task);\n}\n\n \nstruct kthread_worker *\nkthread_create_worker(unsigned int flags, const char namefmt[], ...)\n{\n\tstruct kthread_worker *worker;\n\tva_list args;\n\n\tva_start(args, namefmt);\n\tworker = __kthread_create_worker(-1, flags, namefmt, args);\n\tva_end(args);\n\n\treturn worker;\n}\nEXPORT_SYMBOL(kthread_create_worker);\n\n \nstruct kthread_worker *\nkthread_create_worker_on_cpu(int cpu, unsigned int flags,\n\t\t\t     const char namefmt[], ...)\n{\n\tstruct kthread_worker *worker;\n\tva_list args;\n\n\tva_start(args, namefmt);\n\tworker = __kthread_create_worker(cpu, flags, namefmt, args);\n\tva_end(args);\n\n\treturn worker;\n}\nEXPORT_SYMBOL(kthread_create_worker_on_cpu);\n\n \nstatic inline bool queuing_blocked(struct kthread_worker *worker,\n\t\t\t\t   struct kthread_work *work)\n{\n\tlockdep_assert_held(&worker->lock);\n\n\treturn !list_empty(&work->node) || work->canceling;\n}\n\nstatic void kthread_insert_work_sanity_check(struct kthread_worker *worker,\n\t\t\t\t\t     struct kthread_work *work)\n{\n\tlockdep_assert_held(&worker->lock);\n\tWARN_ON_ONCE(!list_empty(&work->node));\n\t \n\tWARN_ON_ONCE(work->worker && work->worker != worker);\n}\n\n \nstatic void kthread_insert_work(struct kthread_worker *worker,\n\t\t\t\tstruct kthread_work *work,\n\t\t\t\tstruct list_head *pos)\n{\n\tkthread_insert_work_sanity_check(worker, work);\n\n\ttrace_sched_kthread_work_queue_work(worker, work);\n\n\tlist_add_tail(&work->node, pos);\n\twork->worker = worker;\n\tif (!worker->current_work && likely(worker->task))\n\t\twake_up_process(worker->task);\n}\n\n \nbool kthread_queue_work(struct kthread_worker *worker,\n\t\t\tstruct kthread_work *work)\n{\n\tbool ret = false;\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&worker->lock, flags);\n\tif (!queuing_blocked(worker, work)) {\n\t\tkthread_insert_work(worker, work, &worker->work_list);\n\t\tret = true;\n\t}\n\traw_spin_unlock_irqrestore(&worker->lock, flags);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(kthread_queue_work);\n\n \nvoid kthread_delayed_work_timer_fn(struct timer_list *t)\n{\n\tstruct kthread_delayed_work *dwork = from_timer(dwork, t, timer);\n\tstruct kthread_work *work = &dwork->work;\n\tstruct kthread_worker *worker = work->worker;\n\tunsigned long flags;\n\n\t \n\tif (WARN_ON_ONCE(!worker))\n\t\treturn;\n\n\traw_spin_lock_irqsave(&worker->lock, flags);\n\t \n\tWARN_ON_ONCE(work->worker != worker);\n\n\t \n\tWARN_ON_ONCE(list_empty(&work->node));\n\tlist_del_init(&work->node);\n\tif (!work->canceling)\n\t\tkthread_insert_work(worker, work, &worker->work_list);\n\n\traw_spin_unlock_irqrestore(&worker->lock, flags);\n}\nEXPORT_SYMBOL(kthread_delayed_work_timer_fn);\n\nstatic void __kthread_queue_delayed_work(struct kthread_worker *worker,\n\t\t\t\t\t struct kthread_delayed_work *dwork,\n\t\t\t\t\t unsigned long delay)\n{\n\tstruct timer_list *timer = &dwork->timer;\n\tstruct kthread_work *work = &dwork->work;\n\n\tWARN_ON_ONCE(timer->function != kthread_delayed_work_timer_fn);\n\n\t \n\tif (!delay) {\n\t\tkthread_insert_work(worker, work, &worker->work_list);\n\t\treturn;\n\t}\n\n\t \n\tkthread_insert_work_sanity_check(worker, work);\n\n\tlist_add(&work->node, &worker->delayed_work_list);\n\twork->worker = worker;\n\ttimer->expires = jiffies + delay;\n\tadd_timer(timer);\n}\n\n \nbool kthread_queue_delayed_work(struct kthread_worker *worker,\n\t\t\t\tstruct kthread_delayed_work *dwork,\n\t\t\t\tunsigned long delay)\n{\n\tstruct kthread_work *work = &dwork->work;\n\tunsigned long flags;\n\tbool ret = false;\n\n\traw_spin_lock_irqsave(&worker->lock, flags);\n\n\tif (!queuing_blocked(worker, work)) {\n\t\t__kthread_queue_delayed_work(worker, dwork, delay);\n\t\tret = true;\n\t}\n\n\traw_spin_unlock_irqrestore(&worker->lock, flags);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(kthread_queue_delayed_work);\n\nstruct kthread_flush_work {\n\tstruct kthread_work\twork;\n\tstruct completion\tdone;\n};\n\nstatic void kthread_flush_work_fn(struct kthread_work *work)\n{\n\tstruct kthread_flush_work *fwork =\n\t\tcontainer_of(work, struct kthread_flush_work, work);\n\tcomplete(&fwork->done);\n}\n\n \nvoid kthread_flush_work(struct kthread_work *work)\n{\n\tstruct kthread_flush_work fwork = {\n\t\tKTHREAD_WORK_INIT(fwork.work, kthread_flush_work_fn),\n\t\tCOMPLETION_INITIALIZER_ONSTACK(fwork.done),\n\t};\n\tstruct kthread_worker *worker;\n\tbool noop = false;\n\n\tworker = work->worker;\n\tif (!worker)\n\t\treturn;\n\n\traw_spin_lock_irq(&worker->lock);\n\t \n\tWARN_ON_ONCE(work->worker != worker);\n\n\tif (!list_empty(&work->node))\n\t\tkthread_insert_work(worker, &fwork.work, work->node.next);\n\telse if (worker->current_work == work)\n\t\tkthread_insert_work(worker, &fwork.work,\n\t\t\t\t    worker->work_list.next);\n\telse\n\t\tnoop = true;\n\n\traw_spin_unlock_irq(&worker->lock);\n\n\tif (!noop)\n\t\twait_for_completion(&fwork.done);\n}\nEXPORT_SYMBOL_GPL(kthread_flush_work);\n\n \nstatic void kthread_cancel_delayed_work_timer(struct kthread_work *work,\n\t\t\t\t\t      unsigned long *flags)\n{\n\tstruct kthread_delayed_work *dwork =\n\t\tcontainer_of(work, struct kthread_delayed_work, work);\n\tstruct kthread_worker *worker = work->worker;\n\n\t \n\twork->canceling++;\n\traw_spin_unlock_irqrestore(&worker->lock, *flags);\n\tdel_timer_sync(&dwork->timer);\n\traw_spin_lock_irqsave(&worker->lock, *flags);\n\twork->canceling--;\n}\n\n \nstatic bool __kthread_cancel_work(struct kthread_work *work)\n{\n\t \n\tif (!list_empty(&work->node)) {\n\t\tlist_del_init(&work->node);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nbool kthread_mod_delayed_work(struct kthread_worker *worker,\n\t\t\t      struct kthread_delayed_work *dwork,\n\t\t\t      unsigned long delay)\n{\n\tstruct kthread_work *work = &dwork->work;\n\tunsigned long flags;\n\tint ret;\n\n\traw_spin_lock_irqsave(&worker->lock, flags);\n\n\t \n\tif (!work->worker) {\n\t\tret = false;\n\t\tgoto fast_queue;\n\t}\n\n\t \n\tWARN_ON_ONCE(work->worker != worker);\n\n\t \n\tkthread_cancel_delayed_work_timer(work, &flags);\n\tif (work->canceling) {\n\t\t \n\t\tret = true;\n\t\tgoto out;\n\t}\n\tret = __kthread_cancel_work(work);\n\nfast_queue:\n\t__kthread_queue_delayed_work(worker, dwork, delay);\nout:\n\traw_spin_unlock_irqrestore(&worker->lock, flags);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(kthread_mod_delayed_work);\n\nstatic bool __kthread_cancel_work_sync(struct kthread_work *work, bool is_dwork)\n{\n\tstruct kthread_worker *worker = work->worker;\n\tunsigned long flags;\n\tint ret = false;\n\n\tif (!worker)\n\t\tgoto out;\n\n\traw_spin_lock_irqsave(&worker->lock, flags);\n\t \n\tWARN_ON_ONCE(work->worker != worker);\n\n\tif (is_dwork)\n\t\tkthread_cancel_delayed_work_timer(work, &flags);\n\n\tret = __kthread_cancel_work(work);\n\n\tif (worker->current_work != work)\n\t\tgoto out_fast;\n\n\t \n\twork->canceling++;\n\traw_spin_unlock_irqrestore(&worker->lock, flags);\n\tkthread_flush_work(work);\n\traw_spin_lock_irqsave(&worker->lock, flags);\n\twork->canceling--;\n\nout_fast:\n\traw_spin_unlock_irqrestore(&worker->lock, flags);\nout:\n\treturn ret;\n}\n\n \nbool kthread_cancel_work_sync(struct kthread_work *work)\n{\n\treturn __kthread_cancel_work_sync(work, false);\n}\nEXPORT_SYMBOL_GPL(kthread_cancel_work_sync);\n\n \nbool kthread_cancel_delayed_work_sync(struct kthread_delayed_work *dwork)\n{\n\treturn __kthread_cancel_work_sync(&dwork->work, true);\n}\nEXPORT_SYMBOL_GPL(kthread_cancel_delayed_work_sync);\n\n \nvoid kthread_flush_worker(struct kthread_worker *worker)\n{\n\tstruct kthread_flush_work fwork = {\n\t\tKTHREAD_WORK_INIT(fwork.work, kthread_flush_work_fn),\n\t\tCOMPLETION_INITIALIZER_ONSTACK(fwork.done),\n\t};\n\n\tkthread_queue_work(worker, &fwork.work);\n\twait_for_completion(&fwork.done);\n}\nEXPORT_SYMBOL_GPL(kthread_flush_worker);\n\n \nvoid kthread_destroy_worker(struct kthread_worker *worker)\n{\n\tstruct task_struct *task;\n\n\ttask = worker->task;\n\tif (WARN_ON(!task))\n\t\treturn;\n\n\tkthread_flush_worker(worker);\n\tkthread_stop(task);\n\tWARN_ON(!list_empty(&worker->delayed_work_list));\n\tWARN_ON(!list_empty(&worker->work_list));\n\tkfree(worker);\n}\nEXPORT_SYMBOL(kthread_destroy_worker);\n\n \nvoid kthread_use_mm(struct mm_struct *mm)\n{\n\tstruct mm_struct *active_mm;\n\tstruct task_struct *tsk = current;\n\n\tWARN_ON_ONCE(!(tsk->flags & PF_KTHREAD));\n\tWARN_ON_ONCE(tsk->mm);\n\n\t \n\tmmgrab(mm);\n\n\ttask_lock(tsk);\n\t \n\tlocal_irq_disable();\n\tactive_mm = tsk->active_mm;\n\ttsk->active_mm = mm;\n\ttsk->mm = mm;\n\tmembarrier_update_current_mm(mm);\n\tswitch_mm_irqs_off(active_mm, mm, tsk);\n\tlocal_irq_enable();\n\ttask_unlock(tsk);\n#ifdef finish_arch_post_lock_switch\n\tfinish_arch_post_lock_switch();\n#endif\n\n\t \n\tmmdrop_lazy_tlb(active_mm);\n}\nEXPORT_SYMBOL_GPL(kthread_use_mm);\n\n \nvoid kthread_unuse_mm(struct mm_struct *mm)\n{\n\tstruct task_struct *tsk = current;\n\n\tWARN_ON_ONCE(!(tsk->flags & PF_KTHREAD));\n\tWARN_ON_ONCE(!tsk->mm);\n\n\ttask_lock(tsk);\n\t \n\tsmp_mb__after_spinlock();\n\tsync_mm_rss(mm);\n\tlocal_irq_disable();\n\ttsk->mm = NULL;\n\tmembarrier_update_current_mm(NULL);\n\tmmgrab_lazy_tlb(mm);\n\t \n\tenter_lazy_tlb(mm, tsk);\n\tlocal_irq_enable();\n\ttask_unlock(tsk);\n\n\tmmdrop(mm);\n}\nEXPORT_SYMBOL_GPL(kthread_unuse_mm);\n\n#ifdef CONFIG_BLK_CGROUP\n \nvoid kthread_associate_blkcg(struct cgroup_subsys_state *css)\n{\n\tstruct kthread *kthread;\n\n\tif (!(current->flags & PF_KTHREAD))\n\t\treturn;\n\tkthread = to_kthread(current);\n\tif (!kthread)\n\t\treturn;\n\n\tif (kthread->blkcg_css) {\n\t\tcss_put(kthread->blkcg_css);\n\t\tkthread->blkcg_css = NULL;\n\t}\n\tif (css) {\n\t\tcss_get(css);\n\t\tkthread->blkcg_css = css;\n\t}\n}\nEXPORT_SYMBOL(kthread_associate_blkcg);\n\n \nstruct cgroup_subsys_state *kthread_blkcg(void)\n{\n\tstruct kthread *kthread;\n\n\tif (current->flags & PF_KTHREAD) {\n\t\tkthread = to_kthread(current);\n\t\tif (kthread)\n\t\t\treturn kthread->blkcg_css;\n\t}\n\treturn NULL;\n}\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}