{
  "module_name": "exit.c",
  "hash_id": "8c12c76adbbf96b221ca35a1a7a1497431bf8e60ec73605d56aef8a6dd2cbd4b",
  "original_prompt": "Ingested from linux-6.6.14/kernel/exit.c",
  "human_readable_source": "\n \n\n#include <linux/mm.h>\n#include <linux/slab.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/task.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/cputime.h>\n#include <linux/interrupt.h>\n#include <linux/module.h>\n#include <linux/capability.h>\n#include <linux/completion.h>\n#include <linux/personality.h>\n#include <linux/tty.h>\n#include <linux/iocontext.h>\n#include <linux/key.h>\n#include <linux/cpu.h>\n#include <linux/acct.h>\n#include <linux/tsacct_kern.h>\n#include <linux/file.h>\n#include <linux/fdtable.h>\n#include <linux/freezer.h>\n#include <linux/binfmts.h>\n#include <linux/nsproxy.h>\n#include <linux/pid_namespace.h>\n#include <linux/ptrace.h>\n#include <linux/profile.h>\n#include <linux/mount.h>\n#include <linux/proc_fs.h>\n#include <linux/kthread.h>\n#include <linux/mempolicy.h>\n#include <linux/taskstats_kern.h>\n#include <linux/delayacct.h>\n#include <linux/cgroup.h>\n#include <linux/syscalls.h>\n#include <linux/signal.h>\n#include <linux/posix-timers.h>\n#include <linux/cn_proc.h>\n#include <linux/mutex.h>\n#include <linux/futex.h>\n#include <linux/pipe_fs_i.h>\n#include <linux/audit.h>  \n#include <linux/resource.h>\n#include <linux/task_io_accounting_ops.h>\n#include <linux/blkdev.h>\n#include <linux/task_work.h>\n#include <linux/fs_struct.h>\n#include <linux/init_task.h>\n#include <linux/perf_event.h>\n#include <trace/events/sched.h>\n#include <linux/hw_breakpoint.h>\n#include <linux/oom.h>\n#include <linux/writeback.h>\n#include <linux/shm.h>\n#include <linux/kcov.h>\n#include <linux/kmsan.h>\n#include <linux/random.h>\n#include <linux/rcuwait.h>\n#include <linux/compat.h>\n#include <linux/io_uring.h>\n#include <linux/kprobes.h>\n#include <linux/rethook.h>\n#include <linux/sysfs.h>\n#include <linux/user_events.h>\n\n#include <linux/uaccess.h>\n#include <asm/unistd.h>\n#include <asm/mmu_context.h>\n\n \nstatic unsigned int oops_limit = 10000;\n\n#ifdef CONFIG_SYSCTL\nstatic struct ctl_table kern_exit_table[] = {\n\t{\n\t\t.procname       = \"oops_limit\",\n\t\t.data           = &oops_limit,\n\t\t.maxlen         = sizeof(oops_limit),\n\t\t.mode           = 0644,\n\t\t.proc_handler   = proc_douintvec,\n\t},\n\t{ }\n};\n\nstatic __init int kernel_exit_sysctls_init(void)\n{\n\tregister_sysctl_init(\"kernel\", kern_exit_table);\n\treturn 0;\n}\nlate_initcall(kernel_exit_sysctls_init);\n#endif\n\nstatic atomic_t oops_count = ATOMIC_INIT(0);\n\n#ifdef CONFIG_SYSFS\nstatic ssize_t oops_count_show(struct kobject *kobj, struct kobj_attribute *attr,\n\t\t\t       char *page)\n{\n\treturn sysfs_emit(page, \"%d\\n\", atomic_read(&oops_count));\n}\n\nstatic struct kobj_attribute oops_count_attr = __ATTR_RO(oops_count);\n\nstatic __init int kernel_exit_sysfs_init(void)\n{\n\tsysfs_add_file_to_group(kernel_kobj, &oops_count_attr.attr, NULL);\n\treturn 0;\n}\nlate_initcall(kernel_exit_sysfs_init);\n#endif\n\nstatic void __unhash_process(struct task_struct *p, bool group_dead)\n{\n\tnr_threads--;\n\tdetach_pid(p, PIDTYPE_PID);\n\tif (group_dead) {\n\t\tdetach_pid(p, PIDTYPE_TGID);\n\t\tdetach_pid(p, PIDTYPE_PGID);\n\t\tdetach_pid(p, PIDTYPE_SID);\n\n\t\tlist_del_rcu(&p->tasks);\n\t\tlist_del_init(&p->sibling);\n\t\t__this_cpu_dec(process_counts);\n\t}\n\tlist_del_rcu(&p->thread_group);\n\tlist_del_rcu(&p->thread_node);\n}\n\n \nstatic void __exit_signal(struct task_struct *tsk)\n{\n\tstruct signal_struct *sig = tsk->signal;\n\tbool group_dead = thread_group_leader(tsk);\n\tstruct sighand_struct *sighand;\n\tstruct tty_struct *tty;\n\tu64 utime, stime;\n\n\tsighand = rcu_dereference_check(tsk->sighand,\n\t\t\t\t\tlockdep_tasklist_lock_is_held());\n\tspin_lock(&sighand->siglock);\n\n#ifdef CONFIG_POSIX_TIMERS\n\tposix_cpu_timers_exit(tsk);\n\tif (group_dead)\n\t\tposix_cpu_timers_exit_group(tsk);\n#endif\n\n\tif (group_dead) {\n\t\ttty = sig->tty;\n\t\tsig->tty = NULL;\n\t} else {\n\t\t \n\t\tif (sig->notify_count > 0 && !--sig->notify_count)\n\t\t\twake_up_process(sig->group_exec_task);\n\n\t\tif (tsk == sig->curr_target)\n\t\t\tsig->curr_target = next_thread(tsk);\n\t}\n\n\tadd_device_randomness((const void*) &tsk->se.sum_exec_runtime,\n\t\t\t      sizeof(unsigned long long));\n\n\t \n\ttask_cputime(tsk, &utime, &stime);\n\twrite_seqlock(&sig->stats_lock);\n\tsig->utime += utime;\n\tsig->stime += stime;\n\tsig->gtime += task_gtime(tsk);\n\tsig->min_flt += tsk->min_flt;\n\tsig->maj_flt += tsk->maj_flt;\n\tsig->nvcsw += tsk->nvcsw;\n\tsig->nivcsw += tsk->nivcsw;\n\tsig->inblock += task_io_get_inblock(tsk);\n\tsig->oublock += task_io_get_oublock(tsk);\n\ttask_io_accounting_add(&sig->ioac, &tsk->ioac);\n\tsig->sum_sched_runtime += tsk->se.sum_exec_runtime;\n\tsig->nr_threads--;\n\t__unhash_process(tsk, group_dead);\n\twrite_sequnlock(&sig->stats_lock);\n\n\t \n\tflush_sigqueue(&tsk->pending);\n\ttsk->sighand = NULL;\n\tspin_unlock(&sighand->siglock);\n\n\t__cleanup_sighand(sighand);\n\tclear_tsk_thread_flag(tsk, TIF_SIGPENDING);\n\tif (group_dead) {\n\t\tflush_sigqueue(&sig->shared_pending);\n\t\ttty_kref_put(tty);\n\t}\n}\n\nstatic void delayed_put_task_struct(struct rcu_head *rhp)\n{\n\tstruct task_struct *tsk = container_of(rhp, struct task_struct, rcu);\n\n\tkprobe_flush_task(tsk);\n\trethook_flush_task(tsk);\n\tperf_event_delayed_put(tsk);\n\ttrace_sched_process_free(tsk);\n\tput_task_struct(tsk);\n}\n\nvoid put_task_struct_rcu_user(struct task_struct *task)\n{\n\tif (refcount_dec_and_test(&task->rcu_users))\n\t\tcall_rcu(&task->rcu, delayed_put_task_struct);\n}\n\nvoid __weak release_thread(struct task_struct *dead_task)\n{\n}\n\nvoid release_task(struct task_struct *p)\n{\n\tstruct task_struct *leader;\n\tstruct pid *thread_pid;\n\tint zap_leader;\nrepeat:\n\t \n\trcu_read_lock();\n\tdec_rlimit_ucounts(task_ucounts(p), UCOUNT_RLIMIT_NPROC, 1);\n\trcu_read_unlock();\n\n\tcgroup_release(p);\n\n\twrite_lock_irq(&tasklist_lock);\n\tptrace_release_task(p);\n\tthread_pid = get_pid(p->thread_pid);\n\t__exit_signal(p);\n\n\t \n\tzap_leader = 0;\n\tleader = p->group_leader;\n\tif (leader != p && thread_group_empty(leader)\n\t\t\t&& leader->exit_state == EXIT_ZOMBIE) {\n\t\t \n\t\tzap_leader = do_notify_parent(leader, leader->exit_signal);\n\t\tif (zap_leader)\n\t\t\tleader->exit_state = EXIT_DEAD;\n\t}\n\n\twrite_unlock_irq(&tasklist_lock);\n\tseccomp_filter_release(p);\n\tproc_flush_pid(thread_pid);\n\tput_pid(thread_pid);\n\trelease_thread(p);\n\tput_task_struct_rcu_user(p);\n\n\tp = leader;\n\tif (unlikely(zap_leader))\n\t\tgoto repeat;\n}\n\nint rcuwait_wake_up(struct rcuwait *w)\n{\n\tint ret = 0;\n\tstruct task_struct *task;\n\n\trcu_read_lock();\n\n\t \n\tsmp_mb();  \n\n\ttask = rcu_dereference(w->task);\n\tif (task)\n\t\tret = wake_up_process(task);\n\trcu_read_unlock();\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(rcuwait_wake_up);\n\n \nstatic int will_become_orphaned_pgrp(struct pid *pgrp,\n\t\t\t\t\tstruct task_struct *ignored_task)\n{\n\tstruct task_struct *p;\n\n\tdo_each_pid_task(pgrp, PIDTYPE_PGID, p) {\n\t\tif ((p == ignored_task) ||\n\t\t    (p->exit_state && thread_group_empty(p)) ||\n\t\t    is_global_init(p->real_parent))\n\t\t\tcontinue;\n\n\t\tif (task_pgrp(p->real_parent) != pgrp &&\n\t\t    task_session(p->real_parent) == task_session(p))\n\t\t\treturn 0;\n\t} while_each_pid_task(pgrp, PIDTYPE_PGID, p);\n\n\treturn 1;\n}\n\nint is_current_pgrp_orphaned(void)\n{\n\tint retval;\n\n\tread_lock(&tasklist_lock);\n\tretval = will_become_orphaned_pgrp(task_pgrp(current), NULL);\n\tread_unlock(&tasklist_lock);\n\n\treturn retval;\n}\n\nstatic bool has_stopped_jobs(struct pid *pgrp)\n{\n\tstruct task_struct *p;\n\n\tdo_each_pid_task(pgrp, PIDTYPE_PGID, p) {\n\t\tif (p->signal->flags & SIGNAL_STOP_STOPPED)\n\t\t\treturn true;\n\t} while_each_pid_task(pgrp, PIDTYPE_PGID, p);\n\n\treturn false;\n}\n\n \nstatic void\nkill_orphaned_pgrp(struct task_struct *tsk, struct task_struct *parent)\n{\n\tstruct pid *pgrp = task_pgrp(tsk);\n\tstruct task_struct *ignored_task = tsk;\n\n\tif (!parent)\n\t\t \n\t\tparent = tsk->real_parent;\n\telse\n\t\t \n\t\tignored_task = NULL;\n\n\tif (task_pgrp(parent) != pgrp &&\n\t    task_session(parent) == task_session(tsk) &&\n\t    will_become_orphaned_pgrp(pgrp, ignored_task) &&\n\t    has_stopped_jobs(pgrp)) {\n\t\t__kill_pgrp_info(SIGHUP, SEND_SIG_PRIV, pgrp);\n\t\t__kill_pgrp_info(SIGCONT, SEND_SIG_PRIV, pgrp);\n\t}\n}\n\nstatic void coredump_task_exit(struct task_struct *tsk)\n{\n\tstruct core_state *core_state;\n\n\t \n\tspin_lock_irq(&tsk->sighand->siglock);\n\ttsk->flags |= PF_POSTCOREDUMP;\n\tcore_state = tsk->signal->core_state;\n\tspin_unlock_irq(&tsk->sighand->siglock);\n\n\t \n\tif (core_state &&\n\t    ((tsk->flags & (PF_IO_WORKER | PF_USER_WORKER)) != PF_USER_WORKER)) {\n\t\tstruct core_thread self;\n\n\t\tself.task = current;\n\t\tif (self.task->flags & PF_SIGNALED)\n\t\t\tself.next = xchg(&core_state->dumper.next, &self);\n\t\telse\n\t\t\tself.task = NULL;\n\t\t \n\t\tif (atomic_dec_and_test(&core_state->nr_threads))\n\t\t\tcomplete(&core_state->startup);\n\n\t\tfor (;;) {\n\t\t\tset_current_state(TASK_UNINTERRUPTIBLE|TASK_FREEZABLE);\n\t\t\tif (!self.task)  \n\t\t\t\tbreak;\n\t\t\tschedule();\n\t\t}\n\t\t__set_current_state(TASK_RUNNING);\n\t}\n}\n\n#ifdef CONFIG_MEMCG\n \nvoid mm_update_next_owner(struct mm_struct *mm)\n{\n\tstruct task_struct *c, *g, *p = current;\n\nretry:\n\t \n\tif (mm->owner != p)\n\t\treturn;\n\t \n\tif (atomic_read(&mm->mm_users) <= 1) {\n\t\tWRITE_ONCE(mm->owner, NULL);\n\t\treturn;\n\t}\n\n\tread_lock(&tasklist_lock);\n\t \n\tlist_for_each_entry(c, &p->children, sibling) {\n\t\tif (c->mm == mm)\n\t\t\tgoto assign_new_owner;\n\t}\n\n\t \n\tlist_for_each_entry(c, &p->real_parent->children, sibling) {\n\t\tif (c->mm == mm)\n\t\t\tgoto assign_new_owner;\n\t}\n\n\t \n\tfor_each_process(g) {\n\t\tif (g->flags & PF_KTHREAD)\n\t\t\tcontinue;\n\t\tfor_each_thread(g, c) {\n\t\t\tif (c->mm == mm)\n\t\t\t\tgoto assign_new_owner;\n\t\t\tif (c->mm)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tread_unlock(&tasklist_lock);\n\t \n\tWRITE_ONCE(mm->owner, NULL);\n\treturn;\n\nassign_new_owner:\n\tBUG_ON(c == p);\n\tget_task_struct(c);\n\t \n\ttask_lock(c);\n\t \n\tread_unlock(&tasklist_lock);\n\tif (c->mm != mm) {\n\t\ttask_unlock(c);\n\t\tput_task_struct(c);\n\t\tgoto retry;\n\t}\n\tWRITE_ONCE(mm->owner, c);\n\tlru_gen_migrate_mm(mm);\n\ttask_unlock(c);\n\tput_task_struct(c);\n}\n#endif  \n\n \nstatic void exit_mm(void)\n{\n\tstruct mm_struct *mm = current->mm;\n\n\texit_mm_release(current, mm);\n\tif (!mm)\n\t\treturn;\n\tsync_mm_rss(mm);\n\tmmap_read_lock(mm);\n\tmmgrab_lazy_tlb(mm);\n\tBUG_ON(mm != current->active_mm);\n\t \n\ttask_lock(current);\n\t \n\tsmp_mb__after_spinlock();\n\tlocal_irq_disable();\n\tcurrent->mm = NULL;\n\tmembarrier_update_current_mm(NULL);\n\tenter_lazy_tlb(mm, current);\n\tlocal_irq_enable();\n\ttask_unlock(current);\n\tmmap_read_unlock(mm);\n\tmm_update_next_owner(mm);\n\tmmput(mm);\n\tif (test_thread_flag(TIF_MEMDIE))\n\t\texit_oom_victim();\n}\n\nstatic struct task_struct *find_alive_thread(struct task_struct *p)\n{\n\tstruct task_struct *t;\n\n\tfor_each_thread(p, t) {\n\t\tif (!(t->flags & PF_EXITING))\n\t\t\treturn t;\n\t}\n\treturn NULL;\n}\n\nstatic struct task_struct *find_child_reaper(struct task_struct *father,\n\t\t\t\t\t\tstruct list_head *dead)\n\t__releases(&tasklist_lock)\n\t__acquires(&tasklist_lock)\n{\n\tstruct pid_namespace *pid_ns = task_active_pid_ns(father);\n\tstruct task_struct *reaper = pid_ns->child_reaper;\n\tstruct task_struct *p, *n;\n\n\tif (likely(reaper != father))\n\t\treturn reaper;\n\n\treaper = find_alive_thread(father);\n\tif (reaper) {\n\t\tpid_ns->child_reaper = reaper;\n\t\treturn reaper;\n\t}\n\n\twrite_unlock_irq(&tasklist_lock);\n\n\tlist_for_each_entry_safe(p, n, dead, ptrace_entry) {\n\t\tlist_del_init(&p->ptrace_entry);\n\t\trelease_task(p);\n\t}\n\n\tzap_pid_ns_processes(pid_ns);\n\twrite_lock_irq(&tasklist_lock);\n\n\treturn father;\n}\n\n \nstatic struct task_struct *find_new_reaper(struct task_struct *father,\n\t\t\t\t\t   struct task_struct *child_reaper)\n{\n\tstruct task_struct *thread, *reaper;\n\n\tthread = find_alive_thread(father);\n\tif (thread)\n\t\treturn thread;\n\n\tif (father->signal->has_child_subreaper) {\n\t\tunsigned int ns_level = task_pid(father)->level;\n\t\t \n\t\tfor (reaper = father->real_parent;\n\t\t     task_pid(reaper)->level == ns_level;\n\t\t     reaper = reaper->real_parent) {\n\t\t\tif (reaper == &init_task)\n\t\t\t\tbreak;\n\t\t\tif (!reaper->signal->is_child_subreaper)\n\t\t\t\tcontinue;\n\t\t\tthread = find_alive_thread(reaper);\n\t\t\tif (thread)\n\t\t\t\treturn thread;\n\t\t}\n\t}\n\n\treturn child_reaper;\n}\n\n \nstatic void reparent_leader(struct task_struct *father, struct task_struct *p,\n\t\t\t\tstruct list_head *dead)\n{\n\tif (unlikely(p->exit_state == EXIT_DEAD))\n\t\treturn;\n\n\t \n\tp->exit_signal = SIGCHLD;\n\n\t \n\tif (!p->ptrace &&\n\t    p->exit_state == EXIT_ZOMBIE && thread_group_empty(p)) {\n\t\tif (do_notify_parent(p, p->exit_signal)) {\n\t\t\tp->exit_state = EXIT_DEAD;\n\t\t\tlist_add(&p->ptrace_entry, dead);\n\t\t}\n\t}\n\n\tkill_orphaned_pgrp(p, father);\n}\n\n \nstatic void forget_original_parent(struct task_struct *father,\n\t\t\t\t\tstruct list_head *dead)\n{\n\tstruct task_struct *p, *t, *reaper;\n\n\tif (unlikely(!list_empty(&father->ptraced)))\n\t\texit_ptrace(father, dead);\n\n\t \n\treaper = find_child_reaper(father, dead);\n\tif (list_empty(&father->children))\n\t\treturn;\n\n\treaper = find_new_reaper(father, reaper);\n\tlist_for_each_entry(p, &father->children, sibling) {\n\t\tfor_each_thread(p, t) {\n\t\t\tRCU_INIT_POINTER(t->real_parent, reaper);\n\t\t\tBUG_ON((!t->ptrace) != (rcu_access_pointer(t->parent) == father));\n\t\t\tif (likely(!t->ptrace))\n\t\t\t\tt->parent = t->real_parent;\n\t\t\tif (t->pdeath_signal)\n\t\t\t\tgroup_send_sig_info(t->pdeath_signal,\n\t\t\t\t\t\t    SEND_SIG_NOINFO, t,\n\t\t\t\t\t\t    PIDTYPE_TGID);\n\t\t}\n\t\t \n\t\tif (!same_thread_group(reaper, father))\n\t\t\treparent_leader(father, p, dead);\n\t}\n\tlist_splice_tail_init(&father->children, &reaper->children);\n}\n\n \nstatic void exit_notify(struct task_struct *tsk, int group_dead)\n{\n\tbool autoreap;\n\tstruct task_struct *p, *n;\n\tLIST_HEAD(dead);\n\n\twrite_lock_irq(&tasklist_lock);\n\tforget_original_parent(tsk, &dead);\n\n\tif (group_dead)\n\t\tkill_orphaned_pgrp(tsk->group_leader, NULL);\n\n\ttsk->exit_state = EXIT_ZOMBIE;\n\tif (unlikely(tsk->ptrace)) {\n\t\tint sig = thread_group_leader(tsk) &&\n\t\t\t\tthread_group_empty(tsk) &&\n\t\t\t\t!ptrace_reparented(tsk) ?\n\t\t\ttsk->exit_signal : SIGCHLD;\n\t\tautoreap = do_notify_parent(tsk, sig);\n\t} else if (thread_group_leader(tsk)) {\n\t\tautoreap = thread_group_empty(tsk) &&\n\t\t\tdo_notify_parent(tsk, tsk->exit_signal);\n\t} else {\n\t\tautoreap = true;\n\t}\n\n\tif (autoreap) {\n\t\ttsk->exit_state = EXIT_DEAD;\n\t\tlist_add(&tsk->ptrace_entry, &dead);\n\t}\n\n\t \n\tif (unlikely(tsk->signal->notify_count < 0))\n\t\twake_up_process(tsk->signal->group_exec_task);\n\twrite_unlock_irq(&tasklist_lock);\n\n\tlist_for_each_entry_safe(p, n, &dead, ptrace_entry) {\n\t\tlist_del_init(&p->ptrace_entry);\n\t\trelease_task(p);\n\t}\n}\n\n#ifdef CONFIG_DEBUG_STACK_USAGE\nstatic void check_stack_usage(void)\n{\n\tstatic DEFINE_SPINLOCK(low_water_lock);\n\tstatic int lowest_to_date = THREAD_SIZE;\n\tunsigned long free;\n\n\tfree = stack_not_used(current);\n\n\tif (free >= lowest_to_date)\n\t\treturn;\n\n\tspin_lock(&low_water_lock);\n\tif (free < lowest_to_date) {\n\t\tpr_info(\"%s (%d) used greatest stack depth: %lu bytes left\\n\",\n\t\t\tcurrent->comm, task_pid_nr(current), free);\n\t\tlowest_to_date = free;\n\t}\n\tspin_unlock(&low_water_lock);\n}\n#else\nstatic inline void check_stack_usage(void) {}\n#endif\n\nstatic void synchronize_group_exit(struct task_struct *tsk, long code)\n{\n\tstruct sighand_struct *sighand = tsk->sighand;\n\tstruct signal_struct *signal = tsk->signal;\n\n\tspin_lock_irq(&sighand->siglock);\n\tsignal->quick_threads--;\n\tif ((signal->quick_threads == 0) &&\n\t    !(signal->flags & SIGNAL_GROUP_EXIT)) {\n\t\tsignal->flags = SIGNAL_GROUP_EXIT;\n\t\tsignal->group_exit_code = code;\n\t\tsignal->group_stop_count = 0;\n\t}\n\tspin_unlock_irq(&sighand->siglock);\n}\n\nvoid __noreturn do_exit(long code)\n{\n\tstruct task_struct *tsk = current;\n\tint group_dead;\n\n\tWARN_ON(irqs_disabled());\n\n\tsynchronize_group_exit(tsk, code);\n\n\tWARN_ON(tsk->plug);\n\n\tkcov_task_exit(tsk);\n\tkmsan_task_exit(tsk);\n\n\tcoredump_task_exit(tsk);\n\tptrace_event(PTRACE_EVENT_EXIT, code);\n\tuser_events_exit(tsk);\n\n\tio_uring_files_cancel();\n\texit_signals(tsk);   \n\n\t \n\tif (tsk->mm)\n\t\tsync_mm_rss(tsk->mm);\n\tacct_update_integrals(tsk);\n\tgroup_dead = atomic_dec_and_test(&tsk->signal->live);\n\tif (group_dead) {\n\t\t \n\t\tif (unlikely(is_global_init(tsk)))\n\t\t\tpanic(\"Attempted to kill init! exitcode=0x%08x\\n\",\n\t\t\t\ttsk->signal->group_exit_code ?: (int)code);\n\n#ifdef CONFIG_POSIX_TIMERS\n\t\thrtimer_cancel(&tsk->signal->real_timer);\n\t\texit_itimers(tsk);\n#endif\n\t\tif (tsk->mm)\n\t\t\tsetmax_mm_hiwater_rss(&tsk->signal->maxrss, tsk->mm);\n\t}\n\tacct_collect(code, group_dead);\n\tif (group_dead)\n\t\ttty_audit_exit();\n\taudit_free(tsk);\n\n\ttsk->exit_code = code;\n\ttaskstats_exit(tsk, group_dead);\n\n\texit_mm();\n\n\tif (group_dead)\n\t\tacct_process();\n\ttrace_sched_process_exit(tsk);\n\n\texit_sem(tsk);\n\texit_shm(tsk);\n\texit_files(tsk);\n\texit_fs(tsk);\n\tif (group_dead)\n\t\tdisassociate_ctty(1);\n\texit_task_namespaces(tsk);\n\texit_task_work(tsk);\n\texit_thread(tsk);\n\n\t \n\tperf_event_exit_task(tsk);\n\n\tsched_autogroup_exit_task(tsk);\n\tcgroup_exit(tsk);\n\n\t \n\tflush_ptrace_hw_breakpoint(tsk);\n\n\texit_tasks_rcu_start();\n\texit_notify(tsk, group_dead);\n\tproc_exit_connector(tsk);\n\tmpol_put_task_policy(tsk);\n#ifdef CONFIG_FUTEX\n\tif (unlikely(current->pi_state_cache))\n\t\tkfree(current->pi_state_cache);\n#endif\n\t \n\tdebug_check_no_locks_held();\n\n\tif (tsk->io_context)\n\t\texit_io_context(tsk);\n\n\tif (tsk->splice_pipe)\n\t\tfree_pipe_info(tsk->splice_pipe);\n\n\tif (tsk->task_frag.page)\n\t\tput_page(tsk->task_frag.page);\n\n\texit_task_stack_account(tsk);\n\n\tcheck_stack_usage();\n\tpreempt_disable();\n\tif (tsk->nr_dirtied)\n\t\t__this_cpu_add(dirty_throttle_leaks, tsk->nr_dirtied);\n\texit_rcu();\n\texit_tasks_rcu_finish();\n\n\tlockdep_free_task(tsk);\n\tdo_task_dead();\n}\n\nvoid __noreturn make_task_dead(int signr)\n{\n\t \n\tstruct task_struct *tsk = current;\n\tunsigned int limit;\n\n\tif (unlikely(in_interrupt()))\n\t\tpanic(\"Aiee, killing interrupt handler!\");\n\tif (unlikely(!tsk->pid))\n\t\tpanic(\"Attempted to kill the idle task!\");\n\n\tif (unlikely(irqs_disabled())) {\n\t\tpr_info(\"note: %s[%d] exited with irqs disabled\\n\",\n\t\t\tcurrent->comm, task_pid_nr(current));\n\t\tlocal_irq_enable();\n\t}\n\tif (unlikely(in_atomic())) {\n\t\tpr_info(\"note: %s[%d] exited with preempt_count %d\\n\",\n\t\t\tcurrent->comm, task_pid_nr(current),\n\t\t\tpreempt_count());\n\t\tpreempt_count_set(PREEMPT_ENABLED);\n\t}\n\n\t \n\tlimit = READ_ONCE(oops_limit);\n\tif (atomic_inc_return(&oops_count) >= limit && limit)\n\t\tpanic(\"Oopsed too often (kernel.oops_limit is %d)\", limit);\n\n\t \n\tif (unlikely(tsk->flags & PF_EXITING)) {\n\t\tpr_alert(\"Fixing recursive fault but reboot is needed!\\n\");\n\t\tfutex_exit_recursive(tsk);\n\t\ttsk->exit_state = EXIT_DEAD;\n\t\trefcount_inc(&tsk->rcu_users);\n\t\tdo_task_dead();\n\t}\n\n\tdo_exit(signr);\n}\n\nSYSCALL_DEFINE1(exit, int, error_code)\n{\n\tdo_exit((error_code&0xff)<<8);\n}\n\n \nvoid __noreturn\ndo_group_exit(int exit_code)\n{\n\tstruct signal_struct *sig = current->signal;\n\n\tif (sig->flags & SIGNAL_GROUP_EXIT)\n\t\texit_code = sig->group_exit_code;\n\telse if (sig->group_exec_task)\n\t\texit_code = 0;\n\telse {\n\t\tstruct sighand_struct *const sighand = current->sighand;\n\n\t\tspin_lock_irq(&sighand->siglock);\n\t\tif (sig->flags & SIGNAL_GROUP_EXIT)\n\t\t\t \n\t\t\texit_code = sig->group_exit_code;\n\t\telse if (sig->group_exec_task)\n\t\t\texit_code = 0;\n\t\telse {\n\t\t\tsig->group_exit_code = exit_code;\n\t\t\tsig->flags = SIGNAL_GROUP_EXIT;\n\t\t\tzap_other_threads(current);\n\t\t}\n\t\tspin_unlock_irq(&sighand->siglock);\n\t}\n\n\tdo_exit(exit_code);\n\t \n}\n\n \nSYSCALL_DEFINE1(exit_group, int, error_code)\n{\n\tdo_group_exit((error_code & 0xff) << 8);\n\t \n\treturn 0;\n}\n\nstruct waitid_info {\n\tpid_t pid;\n\tuid_t uid;\n\tint status;\n\tint cause;\n};\n\nstruct wait_opts {\n\tenum pid_type\t\two_type;\n\tint\t\t\two_flags;\n\tstruct pid\t\t*wo_pid;\n\n\tstruct waitid_info\t*wo_info;\n\tint\t\t\two_stat;\n\tstruct rusage\t\t*wo_rusage;\n\n\twait_queue_entry_t\t\tchild_wait;\n\tint\t\t\tnotask_error;\n};\n\nstatic int eligible_pid(struct wait_opts *wo, struct task_struct *p)\n{\n\treturn\two->wo_type == PIDTYPE_MAX ||\n\t\ttask_pid_type(p, wo->wo_type) == wo->wo_pid;\n}\n\nstatic int\neligible_child(struct wait_opts *wo, bool ptrace, struct task_struct *p)\n{\n\tif (!eligible_pid(wo, p))\n\t\treturn 0;\n\n\t \n\tif (ptrace || (wo->wo_flags & __WALL))\n\t\treturn 1;\n\n\t \n\tif ((p->exit_signal != SIGCHLD) ^ !!(wo->wo_flags & __WCLONE))\n\t\treturn 0;\n\n\treturn 1;\n}\n\n \nstatic int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)\n{\n\tint state, status;\n\tpid_t pid = task_pid_vnr(p);\n\tuid_t uid = from_kuid_munged(current_user_ns(), task_uid(p));\n\tstruct waitid_info *infop;\n\n\tif (!likely(wo->wo_flags & WEXITED))\n\t\treturn 0;\n\n\tif (unlikely(wo->wo_flags & WNOWAIT)) {\n\t\tstatus = (p->signal->flags & SIGNAL_GROUP_EXIT)\n\t\t\t? p->signal->group_exit_code : p->exit_code;\n\t\tget_task_struct(p);\n\t\tread_unlock(&tasklist_lock);\n\t\tsched_annotate_sleep();\n\t\tif (wo->wo_rusage)\n\t\t\tgetrusage(p, RUSAGE_BOTH, wo->wo_rusage);\n\t\tput_task_struct(p);\n\t\tgoto out_info;\n\t}\n\t \n\tstate = (ptrace_reparented(p) && thread_group_leader(p)) ?\n\t\tEXIT_TRACE : EXIT_DEAD;\n\tif (cmpxchg(&p->exit_state, EXIT_ZOMBIE, state) != EXIT_ZOMBIE)\n\t\treturn 0;\n\t \n\tread_unlock(&tasklist_lock);\n\tsched_annotate_sleep();\n\n\t \n\tif (state == EXIT_DEAD && thread_group_leader(p)) {\n\t\tstruct signal_struct *sig = p->signal;\n\t\tstruct signal_struct *psig = current->signal;\n\t\tunsigned long maxrss;\n\t\tu64 tgutime, tgstime;\n\n\t\t \n\t\tthread_group_cputime_adjusted(p, &tgutime, &tgstime);\n\t\tspin_lock_irq(&current->sighand->siglock);\n\t\twrite_seqlock(&psig->stats_lock);\n\t\tpsig->cutime += tgutime + sig->cutime;\n\t\tpsig->cstime += tgstime + sig->cstime;\n\t\tpsig->cgtime += task_gtime(p) + sig->gtime + sig->cgtime;\n\t\tpsig->cmin_flt +=\n\t\t\tp->min_flt + sig->min_flt + sig->cmin_flt;\n\t\tpsig->cmaj_flt +=\n\t\t\tp->maj_flt + sig->maj_flt + sig->cmaj_flt;\n\t\tpsig->cnvcsw +=\n\t\t\tp->nvcsw + sig->nvcsw + sig->cnvcsw;\n\t\tpsig->cnivcsw +=\n\t\t\tp->nivcsw + sig->nivcsw + sig->cnivcsw;\n\t\tpsig->cinblock +=\n\t\t\ttask_io_get_inblock(p) +\n\t\t\tsig->inblock + sig->cinblock;\n\t\tpsig->coublock +=\n\t\t\ttask_io_get_oublock(p) +\n\t\t\tsig->oublock + sig->coublock;\n\t\tmaxrss = max(sig->maxrss, sig->cmaxrss);\n\t\tif (psig->cmaxrss < maxrss)\n\t\t\tpsig->cmaxrss = maxrss;\n\t\ttask_io_accounting_add(&psig->ioac, &p->ioac);\n\t\ttask_io_accounting_add(&psig->ioac, &sig->ioac);\n\t\twrite_sequnlock(&psig->stats_lock);\n\t\tspin_unlock_irq(&current->sighand->siglock);\n\t}\n\n\tif (wo->wo_rusage)\n\t\tgetrusage(p, RUSAGE_BOTH, wo->wo_rusage);\n\tstatus = (p->signal->flags & SIGNAL_GROUP_EXIT)\n\t\t? p->signal->group_exit_code : p->exit_code;\n\two->wo_stat = status;\n\n\tif (state == EXIT_TRACE) {\n\t\twrite_lock_irq(&tasklist_lock);\n\t\t \n\t\tptrace_unlink(p);\n\n\t\t \n\t\tstate = EXIT_ZOMBIE;\n\t\tif (do_notify_parent(p, p->exit_signal))\n\t\t\tstate = EXIT_DEAD;\n\t\tp->exit_state = state;\n\t\twrite_unlock_irq(&tasklist_lock);\n\t}\n\tif (state == EXIT_DEAD)\n\t\trelease_task(p);\n\nout_info:\n\tinfop = wo->wo_info;\n\tif (infop) {\n\t\tif ((status & 0x7f) == 0) {\n\t\t\tinfop->cause = CLD_EXITED;\n\t\t\tinfop->status = status >> 8;\n\t\t} else {\n\t\t\tinfop->cause = (status & 0x80) ? CLD_DUMPED : CLD_KILLED;\n\t\t\tinfop->status = status & 0x7f;\n\t\t}\n\t\tinfop->pid = pid;\n\t\tinfop->uid = uid;\n\t}\n\n\treturn pid;\n}\n\nstatic int *task_stopped_code(struct task_struct *p, bool ptrace)\n{\n\tif (ptrace) {\n\t\tif (task_is_traced(p) && !(p->jobctl & JOBCTL_LISTENING))\n\t\t\treturn &p->exit_code;\n\t} else {\n\t\tif (p->signal->flags & SIGNAL_STOP_STOPPED)\n\t\t\treturn &p->signal->group_exit_code;\n\t}\n\treturn NULL;\n}\n\n \nstatic int wait_task_stopped(struct wait_opts *wo,\n\t\t\t\tint ptrace, struct task_struct *p)\n{\n\tstruct waitid_info *infop;\n\tint exit_code, *p_code, why;\n\tuid_t uid = 0;  \n\tpid_t pid;\n\n\t \n\tif (!ptrace && !(wo->wo_flags & WUNTRACED))\n\t\treturn 0;\n\n\tif (!task_stopped_code(p, ptrace))\n\t\treturn 0;\n\n\texit_code = 0;\n\tspin_lock_irq(&p->sighand->siglock);\n\n\tp_code = task_stopped_code(p, ptrace);\n\tif (unlikely(!p_code))\n\t\tgoto unlock_sig;\n\n\texit_code = *p_code;\n\tif (!exit_code)\n\t\tgoto unlock_sig;\n\n\tif (!unlikely(wo->wo_flags & WNOWAIT))\n\t\t*p_code = 0;\n\n\tuid = from_kuid_munged(current_user_ns(), task_uid(p));\nunlock_sig:\n\tspin_unlock_irq(&p->sighand->siglock);\n\tif (!exit_code)\n\t\treturn 0;\n\n\t \n\tget_task_struct(p);\n\tpid = task_pid_vnr(p);\n\twhy = ptrace ? CLD_TRAPPED : CLD_STOPPED;\n\tread_unlock(&tasklist_lock);\n\tsched_annotate_sleep();\n\tif (wo->wo_rusage)\n\t\tgetrusage(p, RUSAGE_BOTH, wo->wo_rusage);\n\tput_task_struct(p);\n\n\tif (likely(!(wo->wo_flags & WNOWAIT)))\n\t\two->wo_stat = (exit_code << 8) | 0x7f;\n\n\tinfop = wo->wo_info;\n\tif (infop) {\n\t\tinfop->cause = why;\n\t\tinfop->status = exit_code;\n\t\tinfop->pid = pid;\n\t\tinfop->uid = uid;\n\t}\n\treturn pid;\n}\n\n \nstatic int wait_task_continued(struct wait_opts *wo, struct task_struct *p)\n{\n\tstruct waitid_info *infop;\n\tpid_t pid;\n\tuid_t uid;\n\n\tif (!unlikely(wo->wo_flags & WCONTINUED))\n\t\treturn 0;\n\n\tif (!(p->signal->flags & SIGNAL_STOP_CONTINUED))\n\t\treturn 0;\n\n\tspin_lock_irq(&p->sighand->siglock);\n\t \n\tif (!(p->signal->flags & SIGNAL_STOP_CONTINUED)) {\n\t\tspin_unlock_irq(&p->sighand->siglock);\n\t\treturn 0;\n\t}\n\tif (!unlikely(wo->wo_flags & WNOWAIT))\n\t\tp->signal->flags &= ~SIGNAL_STOP_CONTINUED;\n\tuid = from_kuid_munged(current_user_ns(), task_uid(p));\n\tspin_unlock_irq(&p->sighand->siglock);\n\n\tpid = task_pid_vnr(p);\n\tget_task_struct(p);\n\tread_unlock(&tasklist_lock);\n\tsched_annotate_sleep();\n\tif (wo->wo_rusage)\n\t\tgetrusage(p, RUSAGE_BOTH, wo->wo_rusage);\n\tput_task_struct(p);\n\n\tinfop = wo->wo_info;\n\tif (!infop) {\n\t\two->wo_stat = 0xffff;\n\t} else {\n\t\tinfop->cause = CLD_CONTINUED;\n\t\tinfop->pid = pid;\n\t\tinfop->uid = uid;\n\t\tinfop->status = SIGCONT;\n\t}\n\treturn pid;\n}\n\n \nstatic int wait_consider_task(struct wait_opts *wo, int ptrace,\n\t\t\t\tstruct task_struct *p)\n{\n\t \n\tint exit_state = READ_ONCE(p->exit_state);\n\tint ret;\n\n\tif (unlikely(exit_state == EXIT_DEAD))\n\t\treturn 0;\n\n\tret = eligible_child(wo, ptrace, p);\n\tif (!ret)\n\t\treturn ret;\n\n\tif (unlikely(exit_state == EXIT_TRACE)) {\n\t\t \n\t\tif (likely(!ptrace))\n\t\t\two->notask_error = 0;\n\t\treturn 0;\n\t}\n\n\tif (likely(!ptrace) && unlikely(p->ptrace)) {\n\t\t \n\t\tif (!ptrace_reparented(p))\n\t\t\tptrace = 1;\n\t}\n\n\t \n\tif (exit_state == EXIT_ZOMBIE) {\n\t\t \n\t\tif (!delay_group_leader(p)) {\n\t\t\t \n\t\t\tif (unlikely(ptrace) || likely(!p->ptrace))\n\t\t\t\treturn wait_task_zombie(wo, p);\n\t\t}\n\n\t\t \n\t\tif (likely(!ptrace) || (wo->wo_flags & (WCONTINUED | WEXITED)))\n\t\t\two->notask_error = 0;\n\t} else {\n\t\t \n\t\two->notask_error = 0;\n\t}\n\n\t \n\tret = wait_task_stopped(wo, ptrace, p);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\treturn wait_task_continued(wo, p);\n}\n\n \nstatic int do_wait_thread(struct wait_opts *wo, struct task_struct *tsk)\n{\n\tstruct task_struct *p;\n\n\tlist_for_each_entry(p, &tsk->children, sibling) {\n\t\tint ret = wait_consider_task(wo, 0, p);\n\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int ptrace_do_wait(struct wait_opts *wo, struct task_struct *tsk)\n{\n\tstruct task_struct *p;\n\n\tlist_for_each_entry(p, &tsk->ptraced, ptrace_entry) {\n\t\tint ret = wait_consider_task(wo, 1, p);\n\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int child_wait_callback(wait_queue_entry_t *wait, unsigned mode,\n\t\t\t\tint sync, void *key)\n{\n\tstruct wait_opts *wo = container_of(wait, struct wait_opts,\n\t\t\t\t\t\tchild_wait);\n\tstruct task_struct *p = key;\n\n\tif (!eligible_pid(wo, p))\n\t\treturn 0;\n\n\tif ((wo->wo_flags & __WNOTHREAD) && wait->private != p->parent)\n\t\treturn 0;\n\n\treturn default_wake_function(wait, mode, sync, key);\n}\n\nvoid __wake_up_parent(struct task_struct *p, struct task_struct *parent)\n{\n\t__wake_up_sync_key(&parent->signal->wait_chldexit,\n\t\t\t   TASK_INTERRUPTIBLE, p);\n}\n\nstatic bool is_effectively_child(struct wait_opts *wo, bool ptrace,\n\t\t\t\t struct task_struct *target)\n{\n\tstruct task_struct *parent =\n\t\t!ptrace ? target->real_parent : target->parent;\n\n\treturn current == parent || (!(wo->wo_flags & __WNOTHREAD) &&\n\t\t\t\t     same_thread_group(current, parent));\n}\n\n \nstatic int do_wait_pid(struct wait_opts *wo)\n{\n\tbool ptrace;\n\tstruct task_struct *target;\n\tint retval;\n\n\tptrace = false;\n\ttarget = pid_task(wo->wo_pid, PIDTYPE_TGID);\n\tif (target && is_effectively_child(wo, ptrace, target)) {\n\t\tretval = wait_consider_task(wo, ptrace, target);\n\t\tif (retval)\n\t\t\treturn retval;\n\t}\n\n\tptrace = true;\n\ttarget = pid_task(wo->wo_pid, PIDTYPE_PID);\n\tif (target && target->ptrace &&\n\t    is_effectively_child(wo, ptrace, target)) {\n\t\tretval = wait_consider_task(wo, ptrace, target);\n\t\tif (retval)\n\t\t\treturn retval;\n\t}\n\n\treturn 0;\n}\n\nstatic long do_wait(struct wait_opts *wo)\n{\n\tint retval;\n\n\ttrace_sched_process_wait(wo->wo_pid);\n\n\tinit_waitqueue_func_entry(&wo->child_wait, child_wait_callback);\n\two->child_wait.private = current;\n\tadd_wait_queue(&current->signal->wait_chldexit, &wo->child_wait);\nrepeat:\n\t \n\two->notask_error = -ECHILD;\n\tif ((wo->wo_type < PIDTYPE_MAX) &&\n\t   (!wo->wo_pid || !pid_has_task(wo->wo_pid, wo->wo_type)))\n\t\tgoto notask;\n\n\tset_current_state(TASK_INTERRUPTIBLE);\n\tread_lock(&tasklist_lock);\n\n\tif (wo->wo_type == PIDTYPE_PID) {\n\t\tretval = do_wait_pid(wo);\n\t\tif (retval)\n\t\t\tgoto end;\n\t} else {\n\t\tstruct task_struct *tsk = current;\n\n\t\tdo {\n\t\t\tretval = do_wait_thread(wo, tsk);\n\t\t\tif (retval)\n\t\t\t\tgoto end;\n\n\t\t\tretval = ptrace_do_wait(wo, tsk);\n\t\t\tif (retval)\n\t\t\t\tgoto end;\n\n\t\t\tif (wo->wo_flags & __WNOTHREAD)\n\t\t\t\tbreak;\n\t\t} while_each_thread(current, tsk);\n\t}\n\tread_unlock(&tasklist_lock);\n\nnotask:\n\tretval = wo->notask_error;\n\tif (!retval && !(wo->wo_flags & WNOHANG)) {\n\t\tretval = -ERESTARTSYS;\n\t\tif (!signal_pending(current)) {\n\t\t\tschedule();\n\t\t\tgoto repeat;\n\t\t}\n\t}\nend:\n\t__set_current_state(TASK_RUNNING);\n\tremove_wait_queue(&current->signal->wait_chldexit, &wo->child_wait);\n\treturn retval;\n}\n\nstatic long kernel_waitid(int which, pid_t upid, struct waitid_info *infop,\n\t\t\t  int options, struct rusage *ru)\n{\n\tstruct wait_opts wo;\n\tstruct pid *pid = NULL;\n\tenum pid_type type;\n\tlong ret;\n\tunsigned int f_flags = 0;\n\n\tif (options & ~(WNOHANG|WNOWAIT|WEXITED|WSTOPPED|WCONTINUED|\n\t\t\t__WNOTHREAD|__WCLONE|__WALL))\n\t\treturn -EINVAL;\n\tif (!(options & (WEXITED|WSTOPPED|WCONTINUED)))\n\t\treturn -EINVAL;\n\n\tswitch (which) {\n\tcase P_ALL:\n\t\ttype = PIDTYPE_MAX;\n\t\tbreak;\n\tcase P_PID:\n\t\ttype = PIDTYPE_PID;\n\t\tif (upid <= 0)\n\t\t\treturn -EINVAL;\n\n\t\tpid = find_get_pid(upid);\n\t\tbreak;\n\tcase P_PGID:\n\t\ttype = PIDTYPE_PGID;\n\t\tif (upid < 0)\n\t\t\treturn -EINVAL;\n\n\t\tif (upid)\n\t\t\tpid = find_get_pid(upid);\n\t\telse\n\t\t\tpid = get_task_pid(current, PIDTYPE_PGID);\n\t\tbreak;\n\tcase P_PIDFD:\n\t\ttype = PIDTYPE_PID;\n\t\tif (upid < 0)\n\t\t\treturn -EINVAL;\n\n\t\tpid = pidfd_get_pid(upid, &f_flags);\n\t\tif (IS_ERR(pid))\n\t\t\treturn PTR_ERR(pid);\n\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\two.wo_type\t= type;\n\two.wo_pid\t= pid;\n\two.wo_flags\t= options;\n\two.wo_info\t= infop;\n\two.wo_rusage\t= ru;\n\tif (f_flags & O_NONBLOCK)\n\t\two.wo_flags |= WNOHANG;\n\n\tret = do_wait(&wo);\n\tif (!ret && !(options & WNOHANG) && (f_flags & O_NONBLOCK))\n\t\tret = -EAGAIN;\n\n\tput_pid(pid);\n\treturn ret;\n}\n\nSYSCALL_DEFINE5(waitid, int, which, pid_t, upid, struct siginfo __user *,\n\t\tinfop, int, options, struct rusage __user *, ru)\n{\n\tstruct rusage r;\n\tstruct waitid_info info = {.status = 0};\n\tlong err = kernel_waitid(which, upid, &info, options, ru ? &r : NULL);\n\tint signo = 0;\n\n\tif (err > 0) {\n\t\tsigno = SIGCHLD;\n\t\terr = 0;\n\t\tif (ru && copy_to_user(ru, &r, sizeof(struct rusage)))\n\t\t\treturn -EFAULT;\n\t}\n\tif (!infop)\n\t\treturn err;\n\n\tif (!user_write_access_begin(infop, sizeof(*infop)))\n\t\treturn -EFAULT;\n\n\tunsafe_put_user(signo, &infop->si_signo, Efault);\n\tunsafe_put_user(0, &infop->si_errno, Efault);\n\tunsafe_put_user(info.cause, &infop->si_code, Efault);\n\tunsafe_put_user(info.pid, &infop->si_pid, Efault);\n\tunsafe_put_user(info.uid, &infop->si_uid, Efault);\n\tunsafe_put_user(info.status, &infop->si_status, Efault);\n\tuser_write_access_end();\n\treturn err;\nEfault:\n\tuser_write_access_end();\n\treturn -EFAULT;\n}\n\nlong kernel_wait4(pid_t upid, int __user *stat_addr, int options,\n\t\t  struct rusage *ru)\n{\n\tstruct wait_opts wo;\n\tstruct pid *pid = NULL;\n\tenum pid_type type;\n\tlong ret;\n\n\tif (options & ~(WNOHANG|WUNTRACED|WCONTINUED|\n\t\t\t__WNOTHREAD|__WCLONE|__WALL))\n\t\treturn -EINVAL;\n\n\t \n\tif (upid == INT_MIN)\n\t\treturn -ESRCH;\n\n\tif (upid == -1)\n\t\ttype = PIDTYPE_MAX;\n\telse if (upid < 0) {\n\t\ttype = PIDTYPE_PGID;\n\t\tpid = find_get_pid(-upid);\n\t} else if (upid == 0) {\n\t\ttype = PIDTYPE_PGID;\n\t\tpid = get_task_pid(current, PIDTYPE_PGID);\n\t} else   {\n\t\ttype = PIDTYPE_PID;\n\t\tpid = find_get_pid(upid);\n\t}\n\n\two.wo_type\t= type;\n\two.wo_pid\t= pid;\n\two.wo_flags\t= options | WEXITED;\n\two.wo_info\t= NULL;\n\two.wo_stat\t= 0;\n\two.wo_rusage\t= ru;\n\tret = do_wait(&wo);\n\tput_pid(pid);\n\tif (ret > 0 && stat_addr && put_user(wo.wo_stat, stat_addr))\n\t\tret = -EFAULT;\n\n\treturn ret;\n}\n\nint kernel_wait(pid_t pid, int *stat)\n{\n\tstruct wait_opts wo = {\n\t\t.wo_type\t= PIDTYPE_PID,\n\t\t.wo_pid\t\t= find_get_pid(pid),\n\t\t.wo_flags\t= WEXITED,\n\t};\n\tint ret;\n\n\tret = do_wait(&wo);\n\tif (ret > 0 && wo.wo_stat)\n\t\t*stat = wo.wo_stat;\n\tput_pid(wo.wo_pid);\n\treturn ret;\n}\n\nSYSCALL_DEFINE4(wait4, pid_t, upid, int __user *, stat_addr,\n\t\tint, options, struct rusage __user *, ru)\n{\n\tstruct rusage r;\n\tlong err = kernel_wait4(upid, stat_addr, options, ru ? &r : NULL);\n\n\tif (err > 0) {\n\t\tif (ru && copy_to_user(ru, &r, sizeof(struct rusage)))\n\t\t\treturn -EFAULT;\n\t}\n\treturn err;\n}\n\n#ifdef __ARCH_WANT_SYS_WAITPID\n\n \nSYSCALL_DEFINE3(waitpid, pid_t, pid, int __user *, stat_addr, int, options)\n{\n\treturn kernel_wait4(pid, stat_addr, options, NULL);\n}\n\n#endif\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE4(wait4,\n\tcompat_pid_t, pid,\n\tcompat_uint_t __user *, stat_addr,\n\tint, options,\n\tstruct compat_rusage __user *, ru)\n{\n\tstruct rusage r;\n\tlong err = kernel_wait4(pid, stat_addr, options, ru ? &r : NULL);\n\tif (err > 0) {\n\t\tif (ru && put_compat_rusage(&r, ru))\n\t\t\treturn -EFAULT;\n\t}\n\treturn err;\n}\n\nCOMPAT_SYSCALL_DEFINE5(waitid,\n\t\tint, which, compat_pid_t, pid,\n\t\tstruct compat_siginfo __user *, infop, int, options,\n\t\tstruct compat_rusage __user *, uru)\n{\n\tstruct rusage ru;\n\tstruct waitid_info info = {.status = 0};\n\tlong err = kernel_waitid(which, pid, &info, options, uru ? &ru : NULL);\n\tint signo = 0;\n\tif (err > 0) {\n\t\tsigno = SIGCHLD;\n\t\terr = 0;\n\t\tif (uru) {\n\t\t\t \n\t\t\tif (COMPAT_USE_64BIT_TIME)\n\t\t\t\terr = copy_to_user(uru, &ru, sizeof(ru));\n\t\t\telse\n\t\t\t\terr = put_compat_rusage(&ru, uru);\n\t\t\tif (err)\n\t\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\n\tif (!infop)\n\t\treturn err;\n\n\tif (!user_write_access_begin(infop, sizeof(*infop)))\n\t\treturn -EFAULT;\n\n\tunsafe_put_user(signo, &infop->si_signo, Efault);\n\tunsafe_put_user(0, &infop->si_errno, Efault);\n\tunsafe_put_user(info.cause, &infop->si_code, Efault);\n\tunsafe_put_user(info.pid, &infop->si_pid, Efault);\n\tunsafe_put_user(info.uid, &infop->si_uid, Efault);\n\tunsafe_put_user(info.status, &infop->si_status, Efault);\n\tuser_write_access_end();\n\treturn err;\nEfault:\n\tuser_write_access_end();\n\treturn -EFAULT;\n}\n#endif\n\n \nbool thread_group_exited(struct pid *pid)\n{\n\tstruct task_struct *task;\n\tbool exited;\n\n\trcu_read_lock();\n\ttask = pid_task(pid, PIDTYPE_PID);\n\texited = !task ||\n\t\t(READ_ONCE(task->exit_state) && thread_group_empty(task));\n\trcu_read_unlock();\n\n\treturn exited;\n}\nEXPORT_SYMBOL(thread_group_exited);\n\n \n__weak __function_aligned void abort(void)\n{\n\tBUG();\n\n\t \n\tpanic(\"Oops failed to kill thread\");\n}\nEXPORT_SYMBOL(abort);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}