{
  "module_name": "watchdog_perf.c",
  "hash_id": "ed7908480879071e8943e92cd2dc03b1fcb9fb83ade0c719c43acc0fba8e3090",
  "original_prompt": "Ingested from linux-6.6.14/kernel/watchdog_perf.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) \"NMI watchdog: \" fmt\n\n#include <linux/nmi.h>\n#include <linux/atomic.h>\n#include <linux/module.h>\n#include <linux/sched/debug.h>\n\n#include <asm/irq_regs.h>\n#include <linux/perf_event.h>\n\nstatic DEFINE_PER_CPU(struct perf_event *, watchdog_ev);\nstatic DEFINE_PER_CPU(struct perf_event *, dead_event);\nstatic struct cpumask dead_events_mask;\n\nstatic atomic_t watchdog_cpus = ATOMIC_INIT(0);\n\n#ifdef CONFIG_HARDLOCKUP_CHECK_TIMESTAMP\nstatic DEFINE_PER_CPU(ktime_t, last_timestamp);\nstatic DEFINE_PER_CPU(unsigned int, nmi_rearmed);\nstatic ktime_t watchdog_hrtimer_sample_threshold __read_mostly;\n\nvoid watchdog_update_hrtimer_threshold(u64 period)\n{\n\t \n\twatchdog_hrtimer_sample_threshold = period * 2;\n}\n\nstatic bool watchdog_check_timestamp(void)\n{\n\tktime_t delta, now = ktime_get_mono_fast_ns();\n\n\tdelta = now - __this_cpu_read(last_timestamp);\n\tif (delta < watchdog_hrtimer_sample_threshold) {\n\t\t \n\t\tif (__this_cpu_inc_return(nmi_rearmed) < 10)\n\t\t\treturn false;\n\t}\n\t__this_cpu_write(nmi_rearmed, 0);\n\t__this_cpu_write(last_timestamp, now);\n\treturn true;\n}\n#else\nstatic inline bool watchdog_check_timestamp(void)\n{\n\treturn true;\n}\n#endif\n\nstatic struct perf_event_attr wd_hw_attr = {\n\t.type\t\t= PERF_TYPE_HARDWARE,\n\t.config\t\t= PERF_COUNT_HW_CPU_CYCLES,\n\t.size\t\t= sizeof(struct perf_event_attr),\n\t.pinned\t\t= 1,\n\t.disabled\t= 1,\n};\n\n \nstatic void watchdog_overflow_callback(struct perf_event *event,\n\t\t\t\t       struct perf_sample_data *data,\n\t\t\t\t       struct pt_regs *regs)\n{\n\t \n\tevent->hw.interrupts = 0;\n\n\tif (!watchdog_check_timestamp())\n\t\treturn;\n\n\twatchdog_hardlockup_check(smp_processor_id(), regs);\n}\n\nstatic int hardlockup_detector_event_create(void)\n{\n\tunsigned int cpu;\n\tstruct perf_event_attr *wd_attr;\n\tstruct perf_event *evt;\n\n\t \n\tWARN_ON(!is_percpu_thread());\n\tcpu = raw_smp_processor_id();\n\twd_attr = &wd_hw_attr;\n\twd_attr->sample_period = hw_nmi_get_sample_period(watchdog_thresh);\n\n\t \n\tevt = perf_event_create_kernel_counter(wd_attr, cpu, NULL,\n\t\t\t\t\t       watchdog_overflow_callback, NULL);\n\tif (IS_ERR(evt)) {\n\t\tpr_debug(\"Perf event create on CPU %d failed with %ld\\n\", cpu,\n\t\t\t PTR_ERR(evt));\n\t\treturn PTR_ERR(evt);\n\t}\n\tthis_cpu_write(watchdog_ev, evt);\n\treturn 0;\n}\n\n \nvoid watchdog_hardlockup_enable(unsigned int cpu)\n{\n\tWARN_ON_ONCE(cpu != smp_processor_id());\n\n\tif (hardlockup_detector_event_create())\n\t\treturn;\n\n\t \n\tif (!atomic_fetch_inc(&watchdog_cpus))\n\t\tpr_info(\"Enabled. Permanently consumes one hw-PMU counter.\\n\");\n\n\tperf_event_enable(this_cpu_read(watchdog_ev));\n}\n\n \nvoid watchdog_hardlockup_disable(unsigned int cpu)\n{\n\tstruct perf_event *event = this_cpu_read(watchdog_ev);\n\n\tWARN_ON_ONCE(cpu != smp_processor_id());\n\n\tif (event) {\n\t\tperf_event_disable(event);\n\t\tthis_cpu_write(watchdog_ev, NULL);\n\t\tthis_cpu_write(dead_event, event);\n\t\tcpumask_set_cpu(smp_processor_id(), &dead_events_mask);\n\t\tatomic_dec(&watchdog_cpus);\n\t}\n}\n\n \nvoid hardlockup_detector_perf_cleanup(void)\n{\n\tint cpu;\n\n\tfor_each_cpu(cpu, &dead_events_mask) {\n\t\tstruct perf_event *event = per_cpu(dead_event, cpu);\n\n\t\t \n\t\tif (event)\n\t\t\tperf_event_release_kernel(event);\n\t\tper_cpu(dead_event, cpu) = NULL;\n\t}\n\tcpumask_clear(&dead_events_mask);\n}\n\n \nvoid __init hardlockup_detector_perf_stop(void)\n{\n\tint cpu;\n\n\tlockdep_assert_cpus_held();\n\n\tfor_each_online_cpu(cpu) {\n\t\tstruct perf_event *event = per_cpu(watchdog_ev, cpu);\n\n\t\tif (event)\n\t\t\tperf_event_disable(event);\n\t}\n}\n\n \nvoid __init hardlockup_detector_perf_restart(void)\n{\n\tint cpu;\n\n\tlockdep_assert_cpus_held();\n\n\tif (!(watchdog_enabled & WATCHDOG_HARDLOCKUP_ENABLED))\n\t\treturn;\n\n\tfor_each_online_cpu(cpu) {\n\t\tstruct perf_event *event = per_cpu(watchdog_ev, cpu);\n\n\t\tif (event)\n\t\t\tperf_event_enable(event);\n\t}\n}\n\nbool __weak __init arch_perf_nmi_is_available(void)\n{\n\treturn true;\n}\n\n \nint __init watchdog_hardlockup_probe(void)\n{\n\tint ret;\n\n\tif (!arch_perf_nmi_is_available())\n\t\treturn -ENODEV;\n\n\tret = hardlockup_detector_event_create();\n\n\tif (ret) {\n\t\tpr_info(\"Perf NMI watchdog permanently disabled\\n\");\n\t} else {\n\t\tperf_event_release_kernel(this_cpu_read(watchdog_ev));\n\t\tthis_cpu_write(watchdog_ev, NULL);\n\t}\n\treturn ret;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}