{
  "module_name": "irq_work.c",
  "hash_id": "46f0e5512d088d670440e9abb59cf528e8de20778ab1c56eafb431f1c97c6c6b",
  "original_prompt": "Ingested from linux-6.6.14/kernel/irq_work.c",
  "human_readable_source": "\n \n\n#include <linux/bug.h>\n#include <linux/kernel.h>\n#include <linux/export.h>\n#include <linux/irq_work.h>\n#include <linux/percpu.h>\n#include <linux/hardirq.h>\n#include <linux/irqflags.h>\n#include <linux/sched.h>\n#include <linux/tick.h>\n#include <linux/cpu.h>\n#include <linux/notifier.h>\n#include <linux/smp.h>\n#include <linux/smpboot.h>\n#include <asm/processor.h>\n#include <linux/kasan.h>\n\n#include <trace/events/ipi.h>\n\nstatic DEFINE_PER_CPU(struct llist_head, raised_list);\nstatic DEFINE_PER_CPU(struct llist_head, lazy_list);\nstatic DEFINE_PER_CPU(struct task_struct *, irq_workd);\n\nstatic void wake_irq_workd(void)\n{\n\tstruct task_struct *tsk = __this_cpu_read(irq_workd);\n\n\tif (!llist_empty(this_cpu_ptr(&lazy_list)) && tsk)\n\t\twake_up_process(tsk);\n}\n\n#ifdef CONFIG_SMP\nstatic void irq_work_wake(struct irq_work *entry)\n{\n\twake_irq_workd();\n}\n\nstatic DEFINE_PER_CPU(struct irq_work, irq_work_wakeup) =\n\tIRQ_WORK_INIT_HARD(irq_work_wake);\n#endif\n\nstatic int irq_workd_should_run(unsigned int cpu)\n{\n\treturn !llist_empty(this_cpu_ptr(&lazy_list));\n}\n\n \nstatic bool irq_work_claim(struct irq_work *work)\n{\n\tint oflags;\n\n\toflags = atomic_fetch_or(IRQ_WORK_CLAIMED | CSD_TYPE_IRQ_WORK, &work->node.a_flags);\n\t \n\tif (oflags & IRQ_WORK_PENDING)\n\t\treturn false;\n\treturn true;\n}\n\nvoid __weak arch_irq_work_raise(void)\n{\n\t \n}\n\nstatic __always_inline void irq_work_raise(struct irq_work *work)\n{\n\tif (trace_ipi_send_cpu_enabled() && arch_irq_work_has_interrupt())\n\t\ttrace_ipi_send_cpu(smp_processor_id(), _RET_IP_, work->func);\n\n\tarch_irq_work_raise();\n}\n\n \nstatic void __irq_work_queue_local(struct irq_work *work)\n{\n\tstruct llist_head *list;\n\tbool rt_lazy_work = false;\n\tbool lazy_work = false;\n\tint work_flags;\n\n\twork_flags = atomic_read(&work->node.a_flags);\n\tif (work_flags & IRQ_WORK_LAZY)\n\t\tlazy_work = true;\n\telse if (IS_ENABLED(CONFIG_PREEMPT_RT) &&\n\t\t !(work_flags & IRQ_WORK_HARD_IRQ))\n\t\trt_lazy_work = true;\n\n\tif (lazy_work || rt_lazy_work)\n\t\tlist = this_cpu_ptr(&lazy_list);\n\telse\n\t\tlist = this_cpu_ptr(&raised_list);\n\n\tif (!llist_add(&work->node.llist, list))\n\t\treturn;\n\n\t \n\tif (!lazy_work || tick_nohz_tick_stopped())\n\t\tirq_work_raise(work);\n}\n\n \nbool irq_work_queue(struct irq_work *work)\n{\n\t \n\tif (!irq_work_claim(work))\n\t\treturn false;\n\n\t \n\tpreempt_disable();\n\t__irq_work_queue_local(work);\n\tpreempt_enable();\n\n\treturn true;\n}\nEXPORT_SYMBOL_GPL(irq_work_queue);\n\n \nbool irq_work_queue_on(struct irq_work *work, int cpu)\n{\n#ifndef CONFIG_SMP\n\treturn irq_work_queue(work);\n\n#else  \n\t \n\tWARN_ON_ONCE(cpu_is_offline(cpu));\n\n\t \n\tif (!irq_work_claim(work))\n\t\treturn false;\n\n\tkasan_record_aux_stack_noalloc(work);\n\n\tpreempt_disable();\n\tif (cpu != smp_processor_id()) {\n\t\t \n\t\tWARN_ON_ONCE(in_nmi());\n\n\t\t \n\t\tif (IS_ENABLED(CONFIG_PREEMPT_RT) &&\n\t\t    !(atomic_read(&work->node.a_flags) & IRQ_WORK_HARD_IRQ)) {\n\n\t\t\tif (!llist_add(&work->node.llist, &per_cpu(lazy_list, cpu)))\n\t\t\t\tgoto out;\n\n\t\t\twork = &per_cpu(irq_work_wakeup, cpu);\n\t\t\tif (!irq_work_claim(work))\n\t\t\t\tgoto out;\n\t\t}\n\n\t\t__smp_call_single_queue(cpu, &work->node.llist);\n\t} else {\n\t\t__irq_work_queue_local(work);\n\t}\nout:\n\tpreempt_enable();\n\n\treturn true;\n#endif  \n}\n\nbool irq_work_needs_cpu(void)\n{\n\tstruct llist_head *raised, *lazy;\n\n\traised = this_cpu_ptr(&raised_list);\n\tlazy = this_cpu_ptr(&lazy_list);\n\n\tif (llist_empty(raised) || arch_irq_work_has_interrupt())\n\t\tif (llist_empty(lazy))\n\t\t\treturn false;\n\n\t \n\tWARN_ON_ONCE(cpu_is_offline(smp_processor_id()));\n\n\treturn true;\n}\n\nvoid irq_work_single(void *arg)\n{\n\tstruct irq_work *work = arg;\n\tint flags;\n\n\t \n\tflags = atomic_read(&work->node.a_flags);\n\tflags &= ~IRQ_WORK_PENDING;\n\tatomic_set(&work->node.a_flags, flags);\n\n\t \n\tsmp_mb();\n\n\tlockdep_irq_work_enter(flags);\n\twork->func(work);\n\tlockdep_irq_work_exit(flags);\n\n\t \n\t(void)atomic_cmpxchg(&work->node.a_flags, flags, flags & ~IRQ_WORK_BUSY);\n\n\tif ((IS_ENABLED(CONFIG_PREEMPT_RT) && !irq_work_is_hard(work)) ||\n\t    !arch_irq_work_has_interrupt())\n\t\trcuwait_wake_up(&work->irqwait);\n}\n\nstatic void irq_work_run_list(struct llist_head *list)\n{\n\tstruct irq_work *work, *tmp;\n\tstruct llist_node *llnode;\n\n\t \n\tBUG_ON(!irqs_disabled() && !IS_ENABLED(CONFIG_PREEMPT_RT));\n\n\tif (llist_empty(list))\n\t\treturn;\n\n\tllnode = llist_del_all(list);\n\tllist_for_each_entry_safe(work, tmp, llnode, node.llist)\n\t\tirq_work_single(work);\n}\n\n \nvoid irq_work_run(void)\n{\n\tirq_work_run_list(this_cpu_ptr(&raised_list));\n\tif (!IS_ENABLED(CONFIG_PREEMPT_RT))\n\t\tirq_work_run_list(this_cpu_ptr(&lazy_list));\n\telse\n\t\twake_irq_workd();\n}\nEXPORT_SYMBOL_GPL(irq_work_run);\n\nvoid irq_work_tick(void)\n{\n\tstruct llist_head *raised = this_cpu_ptr(&raised_list);\n\n\tif (!llist_empty(raised) && !arch_irq_work_has_interrupt())\n\t\tirq_work_run_list(raised);\n\n\tif (!IS_ENABLED(CONFIG_PREEMPT_RT))\n\t\tirq_work_run_list(this_cpu_ptr(&lazy_list));\n\telse\n\t\twake_irq_workd();\n}\n\n \nvoid irq_work_sync(struct irq_work *work)\n{\n\tlockdep_assert_irqs_enabled();\n\tmight_sleep();\n\n\tif ((IS_ENABLED(CONFIG_PREEMPT_RT) && !irq_work_is_hard(work)) ||\n\t    !arch_irq_work_has_interrupt()) {\n\t\trcuwait_wait_event(&work->irqwait, !irq_work_is_busy(work),\n\t\t\t\t   TASK_UNINTERRUPTIBLE);\n\t\treturn;\n\t}\n\n\twhile (irq_work_is_busy(work))\n\t\tcpu_relax();\n}\nEXPORT_SYMBOL_GPL(irq_work_sync);\n\nstatic void run_irq_workd(unsigned int cpu)\n{\n\tirq_work_run_list(this_cpu_ptr(&lazy_list));\n}\n\nstatic void irq_workd_setup(unsigned int cpu)\n{\n\tsched_set_fifo_low(current);\n}\n\nstatic struct smp_hotplug_thread irqwork_threads = {\n\t.store                  = &irq_workd,\n\t.setup\t\t\t= irq_workd_setup,\n\t.thread_should_run      = irq_workd_should_run,\n\t.thread_fn              = run_irq_workd,\n\t.thread_comm            = \"irq_work/%u\",\n};\n\nstatic __init int irq_work_init_threads(void)\n{\n\tif (IS_ENABLED(CONFIG_PREEMPT_RT))\n\t\tBUG_ON(smpboot_register_percpu_thread(&irqwork_threads));\n\treturn 0;\n}\nearly_initcall(irq_work_init_threads);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}