{
  "module_name": "callchain.c",
  "hash_id": "d04ad7688991d55b6ca7678e7b10b37ae51939126d7cb32f042a872b3dfeee12",
  "original_prompt": "Ingested from linux-6.6.14/kernel/events/callchain.c",
  "human_readable_source": "\n \n\n#include <linux/perf_event.h>\n#include <linux/slab.h>\n#include <linux/sched/task_stack.h>\n\n#include \"internal.h\"\n\nstruct callchain_cpus_entries {\n\tstruct rcu_head\t\t\trcu_head;\n\tstruct perf_callchain_entry\t*cpu_entries[];\n};\n\nint sysctl_perf_event_max_stack __read_mostly = PERF_MAX_STACK_DEPTH;\nint sysctl_perf_event_max_contexts_per_stack __read_mostly = PERF_MAX_CONTEXTS_PER_STACK;\n\nstatic inline size_t perf_callchain_entry__sizeof(void)\n{\n\treturn (sizeof(struct perf_callchain_entry) +\n\t\tsizeof(__u64) * (sysctl_perf_event_max_stack +\n\t\t\t\t sysctl_perf_event_max_contexts_per_stack));\n}\n\nstatic DEFINE_PER_CPU(int, callchain_recursion[PERF_NR_CONTEXTS]);\nstatic atomic_t nr_callchain_events;\nstatic DEFINE_MUTEX(callchain_mutex);\nstatic struct callchain_cpus_entries *callchain_cpus_entries;\n\n\n__weak void perf_callchain_kernel(struct perf_callchain_entry_ctx *entry,\n\t\t\t\t  struct pt_regs *regs)\n{\n}\n\n__weak void perf_callchain_user(struct perf_callchain_entry_ctx *entry,\n\t\t\t\tstruct pt_regs *regs)\n{\n}\n\nstatic void release_callchain_buffers_rcu(struct rcu_head *head)\n{\n\tstruct callchain_cpus_entries *entries;\n\tint cpu;\n\n\tentries = container_of(head, struct callchain_cpus_entries, rcu_head);\n\n\tfor_each_possible_cpu(cpu)\n\t\tkfree(entries->cpu_entries[cpu]);\n\n\tkfree(entries);\n}\n\nstatic void release_callchain_buffers(void)\n{\n\tstruct callchain_cpus_entries *entries;\n\n\tentries = callchain_cpus_entries;\n\tRCU_INIT_POINTER(callchain_cpus_entries, NULL);\n\tcall_rcu(&entries->rcu_head, release_callchain_buffers_rcu);\n}\n\nstatic int alloc_callchain_buffers(void)\n{\n\tint cpu;\n\tint size;\n\tstruct callchain_cpus_entries *entries;\n\n\t \n\tsize = offsetof(struct callchain_cpus_entries, cpu_entries[nr_cpu_ids]);\n\n\tentries = kzalloc(size, GFP_KERNEL);\n\tif (!entries)\n\t\treturn -ENOMEM;\n\n\tsize = perf_callchain_entry__sizeof() * PERF_NR_CONTEXTS;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tentries->cpu_entries[cpu] = kmalloc_node(size, GFP_KERNEL,\n\t\t\t\t\t\t\t cpu_to_node(cpu));\n\t\tif (!entries->cpu_entries[cpu])\n\t\t\tgoto fail;\n\t}\n\n\trcu_assign_pointer(callchain_cpus_entries, entries);\n\n\treturn 0;\n\nfail:\n\tfor_each_possible_cpu(cpu)\n\t\tkfree(entries->cpu_entries[cpu]);\n\tkfree(entries);\n\n\treturn -ENOMEM;\n}\n\nint get_callchain_buffers(int event_max_stack)\n{\n\tint err = 0;\n\tint count;\n\n\tmutex_lock(&callchain_mutex);\n\n\tcount = atomic_inc_return(&nr_callchain_events);\n\tif (WARN_ON_ONCE(count < 1)) {\n\t\terr = -EINVAL;\n\t\tgoto exit;\n\t}\n\n\t \n\tif (event_max_stack > sysctl_perf_event_max_stack) {\n\t\terr = -EOVERFLOW;\n\t\tgoto exit;\n\t}\n\n\tif (count == 1)\n\t\terr = alloc_callchain_buffers();\nexit:\n\tif (err)\n\t\tatomic_dec(&nr_callchain_events);\n\n\tmutex_unlock(&callchain_mutex);\n\n\treturn err;\n}\n\nvoid put_callchain_buffers(void)\n{\n\tif (atomic_dec_and_mutex_lock(&nr_callchain_events, &callchain_mutex)) {\n\t\trelease_callchain_buffers();\n\t\tmutex_unlock(&callchain_mutex);\n\t}\n}\n\nstruct perf_callchain_entry *get_callchain_entry(int *rctx)\n{\n\tint cpu;\n\tstruct callchain_cpus_entries *entries;\n\n\t*rctx = get_recursion_context(this_cpu_ptr(callchain_recursion));\n\tif (*rctx == -1)\n\t\treturn NULL;\n\n\tentries = rcu_dereference(callchain_cpus_entries);\n\tif (!entries) {\n\t\tput_recursion_context(this_cpu_ptr(callchain_recursion), *rctx);\n\t\treturn NULL;\n\t}\n\n\tcpu = smp_processor_id();\n\n\treturn (((void *)entries->cpu_entries[cpu]) +\n\t\t(*rctx * perf_callchain_entry__sizeof()));\n}\n\nvoid\nput_callchain_entry(int rctx)\n{\n\tput_recursion_context(this_cpu_ptr(callchain_recursion), rctx);\n}\n\nstruct perf_callchain_entry *\nget_perf_callchain(struct pt_regs *regs, u32 init_nr, bool kernel, bool user,\n\t\t   u32 max_stack, bool crosstask, bool add_mark)\n{\n\tstruct perf_callchain_entry *entry;\n\tstruct perf_callchain_entry_ctx ctx;\n\tint rctx;\n\n\tentry = get_callchain_entry(&rctx);\n\tif (!entry)\n\t\treturn NULL;\n\n\tctx.entry     = entry;\n\tctx.max_stack = max_stack;\n\tctx.nr\t      = entry->nr = init_nr;\n\tctx.contexts       = 0;\n\tctx.contexts_maxed = false;\n\n\tif (kernel && !user_mode(regs)) {\n\t\tif (add_mark)\n\t\t\tperf_callchain_store_context(&ctx, PERF_CONTEXT_KERNEL);\n\t\tperf_callchain_kernel(&ctx, regs);\n\t}\n\n\tif (user) {\n\t\tif (!user_mode(regs)) {\n\t\t\tif  (current->mm)\n\t\t\t\tregs = task_pt_regs(current);\n\t\t\telse\n\t\t\t\tregs = NULL;\n\t\t}\n\n\t\tif (regs) {\n\t\t\tif (crosstask)\n\t\t\t\tgoto exit_put;\n\n\t\t\tif (add_mark)\n\t\t\t\tperf_callchain_store_context(&ctx, PERF_CONTEXT_USER);\n\n\t\t\tperf_callchain_user(&ctx, regs);\n\t\t}\n\t}\n\nexit_put:\n\tput_callchain_entry(rctx);\n\n\treturn entry;\n}\n\n \nint perf_event_max_stack_handler(struct ctl_table *table, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tint *value = table->data;\n\tint new_value = *value, ret;\n\tstruct ctl_table new_table = *table;\n\n\tnew_table.data = &new_value;\n\tret = proc_dointvec_minmax(&new_table, write, buffer, lenp, ppos);\n\tif (ret || !write)\n\t\treturn ret;\n\n\tmutex_lock(&callchain_mutex);\n\tif (atomic_read(&nr_callchain_events))\n\t\tret = -EBUSY;\n\telse\n\t\t*value = new_value;\n\n\tmutex_unlock(&callchain_mutex);\n\n\treturn ret;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}