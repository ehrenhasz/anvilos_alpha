{
  "module_name": "ring_buffer.c",
  "hash_id": "72723674aec2d55ed26fff917e06291f1ee3c36b4cde14d41a6e32d9cd1dc37d",
  "original_prompt": "Ingested from linux-6.6.14/kernel/events/ring_buffer.c",
  "human_readable_source": "\n \n\n#include <linux/perf_event.h>\n#include <linux/vmalloc.h>\n#include <linux/slab.h>\n#include <linux/circ_buf.h>\n#include <linux/poll.h>\n#include <linux/nospec.h>\n\n#include \"internal.h\"\n\nstatic void perf_output_wakeup(struct perf_output_handle *handle)\n{\n\tatomic_set(&handle->rb->poll, EPOLLIN);\n\n\thandle->event->pending_wakeup = 1;\n\tirq_work_queue(&handle->event->pending_irq);\n}\n\n \nstatic void perf_output_get_handle(struct perf_output_handle *handle)\n{\n\tstruct perf_buffer *rb = handle->rb;\n\n\tpreempt_disable();\n\n\t \n\t(*(volatile unsigned int *)&rb->nest)++;\n\thandle->wakeup = local_read(&rb->wakeup);\n}\n\nstatic void perf_output_put_handle(struct perf_output_handle *handle)\n{\n\tstruct perf_buffer *rb = handle->rb;\n\tunsigned long head;\n\tunsigned int nest;\n\n\t \n\tnest = READ_ONCE(rb->nest);\n\tif (nest > 1) {\n\t\tWRITE_ONCE(rb->nest, nest - 1);\n\t\tgoto out;\n\t}\n\nagain:\n\t \n\tbarrier();\n\thead = local_read(&rb->head);\n\n\t \n\n\t \n\tsmp_wmb();  \n\tWRITE_ONCE(rb->user_page->data_head, head);\n\n\t \n\tbarrier();\n\tWRITE_ONCE(rb->nest, 0);\n\n\t \n\tbarrier();\n\tif (unlikely(head != local_read(&rb->head))) {\n\t\tWRITE_ONCE(rb->nest, 1);\n\t\tgoto again;\n\t}\n\n\tif (handle->wakeup != local_read(&rb->wakeup))\n\t\tperf_output_wakeup(handle);\n\nout:\n\tpreempt_enable();\n}\n\nstatic __always_inline bool\nring_buffer_has_space(unsigned long head, unsigned long tail,\n\t\t      unsigned long data_size, unsigned int size,\n\t\t      bool backward)\n{\n\tif (!backward)\n\t\treturn CIRC_SPACE(head, tail, data_size) >= size;\n\telse\n\t\treturn CIRC_SPACE(tail, head, data_size) >= size;\n}\n\nstatic __always_inline int\n__perf_output_begin(struct perf_output_handle *handle,\n\t\t    struct perf_sample_data *data,\n\t\t    struct perf_event *event, unsigned int size,\n\t\t    bool backward)\n{\n\tstruct perf_buffer *rb;\n\tunsigned long tail, offset, head;\n\tint have_lost, page_shift;\n\tstruct {\n\t\tstruct perf_event_header header;\n\t\tu64\t\t\t id;\n\t\tu64\t\t\t lost;\n\t} lost_event;\n\n\trcu_read_lock();\n\t \n\tif (event->parent)\n\t\tevent = event->parent;\n\n\trb = rcu_dereference(event->rb);\n\tif (unlikely(!rb))\n\t\tgoto out;\n\n\tif (unlikely(rb->paused)) {\n\t\tif (rb->nr_pages) {\n\t\t\tlocal_inc(&rb->lost);\n\t\t\tatomic64_inc(&event->lost_samples);\n\t\t}\n\t\tgoto out;\n\t}\n\n\thandle->rb    = rb;\n\thandle->event = event;\n\n\thave_lost = local_read(&rb->lost);\n\tif (unlikely(have_lost)) {\n\t\tsize += sizeof(lost_event);\n\t\tif (event->attr.sample_id_all)\n\t\t\tsize += event->id_header_size;\n\t}\n\n\tperf_output_get_handle(handle);\n\n\toffset = local_read(&rb->head);\n\tdo {\n\t\thead = offset;\n\t\ttail = READ_ONCE(rb->user_page->data_tail);\n\t\tif (!rb->overwrite) {\n\t\t\tif (unlikely(!ring_buffer_has_space(head, tail,\n\t\t\t\t\t\t\t    perf_data_size(rb),\n\t\t\t\t\t\t\t    size, backward)))\n\t\t\t\tgoto fail;\n\t\t}\n\n\t\t \n\n\t\tif (!backward)\n\t\t\thead += size;\n\t\telse\n\t\t\thead -= size;\n\t} while (!local_try_cmpxchg(&rb->head, &offset, head));\n\n\tif (backward) {\n\t\toffset = head;\n\t\thead = (u64)(-head);\n\t}\n\n\t \n\n\tif (unlikely(head - local_read(&rb->wakeup) > rb->watermark))\n\t\tlocal_add(rb->watermark, &rb->wakeup);\n\n\tpage_shift = PAGE_SHIFT + page_order(rb);\n\n\thandle->page = (offset >> page_shift) & (rb->nr_pages - 1);\n\toffset &= (1UL << page_shift) - 1;\n\thandle->addr = rb->data_pages[handle->page] + offset;\n\thandle->size = (1UL << page_shift) - offset;\n\n\tif (unlikely(have_lost)) {\n\t\tlost_event.header.size = sizeof(lost_event);\n\t\tlost_event.header.type = PERF_RECORD_LOST;\n\t\tlost_event.header.misc = 0;\n\t\tlost_event.id          = event->id;\n\t\tlost_event.lost        = local_xchg(&rb->lost, 0);\n\n\t\t \n\t\tperf_event_header__init_id(&lost_event.header, data, event);\n\t\tperf_output_put(handle, lost_event);\n\t\tperf_event__output_id_sample(event, handle, data);\n\t}\n\n\treturn 0;\n\nfail:\n\tlocal_inc(&rb->lost);\n\tatomic64_inc(&event->lost_samples);\n\tperf_output_put_handle(handle);\nout:\n\trcu_read_unlock();\n\n\treturn -ENOSPC;\n}\n\nint perf_output_begin_forward(struct perf_output_handle *handle,\n\t\t\t      struct perf_sample_data *data,\n\t\t\t      struct perf_event *event, unsigned int size)\n{\n\treturn __perf_output_begin(handle, data, event, size, false);\n}\n\nint perf_output_begin_backward(struct perf_output_handle *handle,\n\t\t\t       struct perf_sample_data *data,\n\t\t\t       struct perf_event *event, unsigned int size)\n{\n\treturn __perf_output_begin(handle, data, event, size, true);\n}\n\nint perf_output_begin(struct perf_output_handle *handle,\n\t\t      struct perf_sample_data *data,\n\t\t      struct perf_event *event, unsigned int size)\n{\n\n\treturn __perf_output_begin(handle, data, event, size,\n\t\t\t\t   unlikely(is_write_backward(event)));\n}\n\nunsigned int perf_output_copy(struct perf_output_handle *handle,\n\t\t      const void *buf, unsigned int len)\n{\n\treturn __output_copy(handle, buf, len);\n}\n\nunsigned int perf_output_skip(struct perf_output_handle *handle,\n\t\t\t      unsigned int len)\n{\n\treturn __output_skip(handle, NULL, len);\n}\n\nvoid perf_output_end(struct perf_output_handle *handle)\n{\n\tperf_output_put_handle(handle);\n\trcu_read_unlock();\n}\n\nstatic void\nring_buffer_init(struct perf_buffer *rb, long watermark, int flags)\n{\n\tlong max_size = perf_data_size(rb);\n\n\tif (watermark)\n\t\trb->watermark = min(max_size, watermark);\n\n\tif (!rb->watermark)\n\t\trb->watermark = max_size / 2;\n\n\tif (flags & RING_BUFFER_WRITABLE)\n\t\trb->overwrite = 0;\n\telse\n\t\trb->overwrite = 1;\n\n\trefcount_set(&rb->refcount, 1);\n\n\tINIT_LIST_HEAD(&rb->event_list);\n\tspin_lock_init(&rb->event_lock);\n\n\t \n\tif (!rb->nr_pages)\n\t\trb->paused = 1;\n}\n\nvoid perf_aux_output_flag(struct perf_output_handle *handle, u64 flags)\n{\n\t \n\tif (WARN_ON_ONCE(flags & PERF_AUX_FLAG_OVERWRITE))\n\t\treturn;\n\n\thandle->aux_flags |= flags;\n}\nEXPORT_SYMBOL_GPL(perf_aux_output_flag);\n\n \nvoid *perf_aux_output_begin(struct perf_output_handle *handle,\n\t\t\t    struct perf_event *event)\n{\n\tstruct perf_event *output_event = event;\n\tunsigned long aux_head, aux_tail;\n\tstruct perf_buffer *rb;\n\tunsigned int nest;\n\n\tif (output_event->parent)\n\t\toutput_event = output_event->parent;\n\n\t \n\trb = ring_buffer_get(output_event);\n\tif (!rb)\n\t\treturn NULL;\n\n\tif (!rb_has_aux(rb))\n\t\tgoto err;\n\n\t \n\tif (!atomic_read(&rb->aux_mmap_count))\n\t\tgoto err;\n\n\tif (!refcount_inc_not_zero(&rb->aux_refcount))\n\t\tgoto err;\n\n\tnest = READ_ONCE(rb->aux_nest);\n\t \n\tif (WARN_ON_ONCE(nest))\n\t\tgoto err_put;\n\n\tWRITE_ONCE(rb->aux_nest, nest + 1);\n\n\taux_head = rb->aux_head;\n\n\thandle->rb = rb;\n\thandle->event = event;\n\thandle->head = aux_head;\n\thandle->size = 0;\n\thandle->aux_flags = 0;\n\n\t \n\tif (!rb->aux_overwrite) {\n\t\taux_tail = READ_ONCE(rb->user_page->aux_tail);\n\t\thandle->wakeup = rb->aux_wakeup + rb->aux_watermark;\n\t\tif (aux_head - aux_tail < perf_aux_size(rb))\n\t\t\thandle->size = CIRC_SPACE(aux_head, aux_tail, perf_aux_size(rb));\n\n\t\t \n\t\tif (!handle->size) {  \n\t\t\tevent->pending_disable = smp_processor_id();\n\t\t\tperf_output_wakeup(handle);\n\t\t\tWRITE_ONCE(rb->aux_nest, 0);\n\t\t\tgoto err_put;\n\t\t}\n\t}\n\n\treturn handle->rb->aux_priv;\n\nerr_put:\n\t \n\trb_free_aux(rb);\n\nerr:\n\tring_buffer_put(rb);\n\thandle->event = NULL;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(perf_aux_output_begin);\n\nstatic __always_inline bool rb_need_aux_wakeup(struct perf_buffer *rb)\n{\n\tif (rb->aux_overwrite)\n\t\treturn false;\n\n\tif (rb->aux_head - rb->aux_wakeup >= rb->aux_watermark) {\n\t\trb->aux_wakeup = rounddown(rb->aux_head, rb->aux_watermark);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nvoid perf_aux_output_end(struct perf_output_handle *handle, unsigned long size)\n{\n\tbool wakeup = !!(handle->aux_flags & PERF_AUX_FLAG_TRUNCATED);\n\tstruct perf_buffer *rb = handle->rb;\n\tunsigned long aux_head;\n\n\t \n\tif (rb->aux_overwrite) {\n\t\thandle->aux_flags |= PERF_AUX_FLAG_OVERWRITE;\n\n\t\taux_head = handle->head;\n\t\trb->aux_head = aux_head;\n\t} else {\n\t\thandle->aux_flags &= ~PERF_AUX_FLAG_OVERWRITE;\n\n\t\taux_head = rb->aux_head;\n\t\trb->aux_head += size;\n\t}\n\n\t \n\tif (size || (handle->aux_flags & ~(u64)PERF_AUX_FLAG_OVERWRITE))\n\t\tperf_event_aux_event(handle->event, aux_head, size,\n\t\t\t\t     handle->aux_flags);\n\n\tWRITE_ONCE(rb->user_page->aux_head, rb->aux_head);\n\tif (rb_need_aux_wakeup(rb))\n\t\twakeup = true;\n\n\tif (wakeup) {\n\t\tif (handle->aux_flags & PERF_AUX_FLAG_TRUNCATED)\n\t\t\thandle->event->pending_disable = smp_processor_id();\n\t\tperf_output_wakeup(handle);\n\t}\n\n\thandle->event = NULL;\n\n\tWRITE_ONCE(rb->aux_nest, 0);\n\t \n\trb_free_aux(rb);\n\tring_buffer_put(rb);\n}\nEXPORT_SYMBOL_GPL(perf_aux_output_end);\n\n \nint perf_aux_output_skip(struct perf_output_handle *handle, unsigned long size)\n{\n\tstruct perf_buffer *rb = handle->rb;\n\n\tif (size > handle->size)\n\t\treturn -ENOSPC;\n\n\trb->aux_head += size;\n\n\tWRITE_ONCE(rb->user_page->aux_head, rb->aux_head);\n\tif (rb_need_aux_wakeup(rb)) {\n\t\tperf_output_wakeup(handle);\n\t\thandle->wakeup = rb->aux_wakeup + rb->aux_watermark;\n\t}\n\n\thandle->head = rb->aux_head;\n\thandle->size -= size;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(perf_aux_output_skip);\n\nvoid *perf_get_aux(struct perf_output_handle *handle)\n{\n\t \n\tif (!handle->event)\n\t\treturn NULL;\n\n\treturn handle->rb->aux_priv;\n}\nEXPORT_SYMBOL_GPL(perf_get_aux);\n\n \nlong perf_output_copy_aux(struct perf_output_handle *aux_handle,\n\t\t\t  struct perf_output_handle *handle,\n\t\t\t  unsigned long from, unsigned long to)\n{\n\tstruct perf_buffer *rb = aux_handle->rb;\n\tunsigned long tocopy, remainder, len = 0;\n\tvoid *addr;\n\n\tfrom &= (rb->aux_nr_pages << PAGE_SHIFT) - 1;\n\tto &= (rb->aux_nr_pages << PAGE_SHIFT) - 1;\n\n\tdo {\n\t\ttocopy = PAGE_SIZE - offset_in_page(from);\n\t\tif (to > from)\n\t\t\ttocopy = min(tocopy, to - from);\n\t\tif (!tocopy)\n\t\t\tbreak;\n\n\t\taddr = rb->aux_pages[from >> PAGE_SHIFT];\n\t\taddr += offset_in_page(from);\n\n\t\tremainder = perf_output_copy(handle, addr, tocopy);\n\t\tif (remainder)\n\t\t\treturn -EFAULT;\n\n\t\tlen += tocopy;\n\t\tfrom += tocopy;\n\t\tfrom &= (rb->aux_nr_pages << PAGE_SHIFT) - 1;\n\t} while (to != from);\n\n\treturn len;\n}\n\n#define PERF_AUX_GFP\t(GFP_KERNEL | __GFP_ZERO | __GFP_NOWARN | __GFP_NORETRY)\n\nstatic struct page *rb_alloc_aux_page(int node, int order)\n{\n\tstruct page *page;\n\n\tif (order > MAX_ORDER)\n\t\torder = MAX_ORDER;\n\n\tdo {\n\t\tpage = alloc_pages_node(node, PERF_AUX_GFP, order);\n\t} while (!page && order--);\n\n\tif (page && order) {\n\t\t \n\t\tsplit_page(page, order);\n\t\tSetPagePrivate(page);\n\t\tset_page_private(page, order);\n\t}\n\n\treturn page;\n}\n\nstatic void rb_free_aux_page(struct perf_buffer *rb, int idx)\n{\n\tstruct page *page = virt_to_page(rb->aux_pages[idx]);\n\n\tClearPagePrivate(page);\n\tpage->mapping = NULL;\n\t__free_page(page);\n}\n\nstatic void __rb_free_aux(struct perf_buffer *rb)\n{\n\tint pg;\n\n\t \n\tWARN_ON_ONCE(in_atomic());\n\n\tif (rb->aux_priv) {\n\t\trb->free_aux(rb->aux_priv);\n\t\trb->free_aux = NULL;\n\t\trb->aux_priv = NULL;\n\t}\n\n\tif (rb->aux_nr_pages) {\n\t\tfor (pg = 0; pg < rb->aux_nr_pages; pg++)\n\t\t\trb_free_aux_page(rb, pg);\n\n\t\tkfree(rb->aux_pages);\n\t\trb->aux_nr_pages = 0;\n\t}\n}\n\nint rb_alloc_aux(struct perf_buffer *rb, struct perf_event *event,\n\t\t pgoff_t pgoff, int nr_pages, long watermark, int flags)\n{\n\tbool overwrite = !(flags & RING_BUFFER_WRITABLE);\n\tint node = (event->cpu == -1) ? -1 : cpu_to_node(event->cpu);\n\tint ret = -ENOMEM, max_order;\n\n\tif (!has_aux(event))\n\t\treturn -EOPNOTSUPP;\n\n\tif (!overwrite) {\n\t\t \n\t\tif (!watermark)\n\t\t\twatermark = nr_pages << (PAGE_SHIFT - 1);\n\n\t\t \n\t\tmax_order = get_order(watermark);\n\t} else {\n\t\t \n\t\tmax_order = ilog2(nr_pages);\n\t\twatermark = 0;\n\t}\n\n\t \n\tif (get_order((unsigned long)nr_pages * sizeof(void *)) > MAX_ORDER)\n\t\treturn -ENOMEM;\n\trb->aux_pages = kcalloc_node(nr_pages, sizeof(void *), GFP_KERNEL,\n\t\t\t\t     node);\n\tif (!rb->aux_pages)\n\t\treturn -ENOMEM;\n\n\trb->free_aux = event->pmu->free_aux;\n\tfor (rb->aux_nr_pages = 0; rb->aux_nr_pages < nr_pages;) {\n\t\tstruct page *page;\n\t\tint last, order;\n\n\t\torder = min(max_order, ilog2(nr_pages - rb->aux_nr_pages));\n\t\tpage = rb_alloc_aux_page(node, order);\n\t\tif (!page)\n\t\t\tgoto out;\n\n\t\tfor (last = rb->aux_nr_pages + (1 << page_private(page));\n\t\t     last > rb->aux_nr_pages; rb->aux_nr_pages++)\n\t\t\trb->aux_pages[rb->aux_nr_pages] = page_address(page++);\n\t}\n\n\t \n\tif ((event->pmu->capabilities & PERF_PMU_CAP_AUX_NO_SG) &&\n\t    overwrite) {\n\t\tstruct page *page = virt_to_page(rb->aux_pages[0]);\n\n\t\tif (page_private(page) != max_order)\n\t\t\tgoto out;\n\t}\n\n\trb->aux_priv = event->pmu->setup_aux(event, rb->aux_pages, nr_pages,\n\t\t\t\t\t     overwrite);\n\tif (!rb->aux_priv)\n\t\tgoto out;\n\n\tret = 0;\n\n\t \n\trefcount_set(&rb->aux_refcount, 1);\n\n\trb->aux_overwrite = overwrite;\n\trb->aux_watermark = watermark;\n\nout:\n\tif (!ret)\n\t\trb->aux_pgoff = pgoff;\n\telse\n\t\t__rb_free_aux(rb);\n\n\treturn ret;\n}\n\nvoid rb_free_aux(struct perf_buffer *rb)\n{\n\tif (refcount_dec_and_test(&rb->aux_refcount))\n\t\t__rb_free_aux(rb);\n}\n\n#ifndef CONFIG_PERF_USE_VMALLOC\n\n \n\nstatic struct page *\n__perf_mmap_to_page(struct perf_buffer *rb, unsigned long pgoff)\n{\n\tif (pgoff > rb->nr_pages)\n\t\treturn NULL;\n\n\tif (pgoff == 0)\n\t\treturn virt_to_page(rb->user_page);\n\n\treturn virt_to_page(rb->data_pages[pgoff - 1]);\n}\n\nstatic void *perf_mmap_alloc_page(int cpu)\n{\n\tstruct page *page;\n\tint node;\n\n\tnode = (cpu == -1) ? cpu : cpu_to_node(cpu);\n\tpage = alloc_pages_node(node, GFP_KERNEL | __GFP_ZERO, 0);\n\tif (!page)\n\t\treturn NULL;\n\n\treturn page_address(page);\n}\n\nstatic void perf_mmap_free_page(void *addr)\n{\n\tstruct page *page = virt_to_page(addr);\n\n\tpage->mapping = NULL;\n\t__free_page(page);\n}\n\nstruct perf_buffer *rb_alloc(int nr_pages, long watermark, int cpu, int flags)\n{\n\tstruct perf_buffer *rb;\n\tunsigned long size;\n\tint i, node;\n\n\tsize = sizeof(struct perf_buffer);\n\tsize += nr_pages * sizeof(void *);\n\n\tif (order_base_2(size) > PAGE_SHIFT+MAX_ORDER)\n\t\tgoto fail;\n\n\tnode = (cpu == -1) ? cpu : cpu_to_node(cpu);\n\trb = kzalloc_node(size, GFP_KERNEL, node);\n\tif (!rb)\n\t\tgoto fail;\n\n\trb->user_page = perf_mmap_alloc_page(cpu);\n\tif (!rb->user_page)\n\t\tgoto fail_user_page;\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\trb->data_pages[i] = perf_mmap_alloc_page(cpu);\n\t\tif (!rb->data_pages[i])\n\t\t\tgoto fail_data_pages;\n\t}\n\n\trb->nr_pages = nr_pages;\n\n\tring_buffer_init(rb, watermark, flags);\n\n\treturn rb;\n\nfail_data_pages:\n\tfor (i--; i >= 0; i--)\n\t\tperf_mmap_free_page(rb->data_pages[i]);\n\n\tperf_mmap_free_page(rb->user_page);\n\nfail_user_page:\n\tkfree(rb);\n\nfail:\n\treturn NULL;\n}\n\nvoid rb_free(struct perf_buffer *rb)\n{\n\tint i;\n\n\tperf_mmap_free_page(rb->user_page);\n\tfor (i = 0; i < rb->nr_pages; i++)\n\t\tperf_mmap_free_page(rb->data_pages[i]);\n\tkfree(rb);\n}\n\n#else\nstatic struct page *\n__perf_mmap_to_page(struct perf_buffer *rb, unsigned long pgoff)\n{\n\t \n\tif (pgoff > data_page_nr(rb))\n\t\treturn NULL;\n\n\treturn vmalloc_to_page((void *)rb->user_page + pgoff * PAGE_SIZE);\n}\n\nstatic void perf_mmap_unmark_page(void *addr)\n{\n\tstruct page *page = vmalloc_to_page(addr);\n\n\tpage->mapping = NULL;\n}\n\nstatic void rb_free_work(struct work_struct *work)\n{\n\tstruct perf_buffer *rb;\n\tvoid *base;\n\tint i, nr;\n\n\trb = container_of(work, struct perf_buffer, work);\n\tnr = data_page_nr(rb);\n\n\tbase = rb->user_page;\n\t \n\tfor (i = 0; i <= nr; i++)\n\t\tperf_mmap_unmark_page(base + (i * PAGE_SIZE));\n\n\tvfree(base);\n\tkfree(rb);\n}\n\nvoid rb_free(struct perf_buffer *rb)\n{\n\tschedule_work(&rb->work);\n}\n\nstruct perf_buffer *rb_alloc(int nr_pages, long watermark, int cpu, int flags)\n{\n\tstruct perf_buffer *rb;\n\tunsigned long size;\n\tvoid *all_buf;\n\tint node;\n\n\tsize = sizeof(struct perf_buffer);\n\tsize += sizeof(void *);\n\n\tnode = (cpu == -1) ? cpu : cpu_to_node(cpu);\n\trb = kzalloc_node(size, GFP_KERNEL, node);\n\tif (!rb)\n\t\tgoto fail;\n\n\tINIT_WORK(&rb->work, rb_free_work);\n\n\tall_buf = vmalloc_user((nr_pages + 1) * PAGE_SIZE);\n\tif (!all_buf)\n\t\tgoto fail_all_buf;\n\n\trb->user_page = all_buf;\n\trb->data_pages[0] = all_buf + PAGE_SIZE;\n\tif (nr_pages) {\n\t\trb->nr_pages = 1;\n\t\trb->page_order = ilog2(nr_pages);\n\t}\n\n\tring_buffer_init(rb, watermark, flags);\n\n\treturn rb;\n\nfail_all_buf:\n\tkfree(rb);\n\nfail:\n\treturn NULL;\n}\n\n#endif\n\nstruct page *\nperf_mmap_to_page(struct perf_buffer *rb, unsigned long pgoff)\n{\n\tif (rb->aux_nr_pages) {\n\t\t \n\t\tif (pgoff > rb->aux_pgoff + rb->aux_nr_pages)\n\t\t\treturn NULL;\n\n\t\t \n\t\tif (pgoff >= rb->aux_pgoff) {\n\t\t\tint aux_pgoff = array_index_nospec(pgoff - rb->aux_pgoff, rb->aux_nr_pages);\n\t\t\treturn virt_to_page(rb->aux_pages[aux_pgoff]);\n\t\t}\n\t}\n\n\treturn __perf_mmap_to_page(rb, pgoff);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}