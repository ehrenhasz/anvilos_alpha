{
  "module_name": "core.c",
  "hash_id": "e803775ef3e03b914846acfc53eb285217630fe1ad5d491de3caf93a4a6ad154",
  "original_prompt": "Ingested from linux-6.6.14/kernel/events/core.c",
  "human_readable_source": "\n \n\n#include <linux/fs.h>\n#include <linux/mm.h>\n#include <linux/cpu.h>\n#include <linux/smp.h>\n#include <linux/idr.h>\n#include <linux/file.h>\n#include <linux/poll.h>\n#include <linux/slab.h>\n#include <linux/hash.h>\n#include <linux/tick.h>\n#include <linux/sysfs.h>\n#include <linux/dcache.h>\n#include <linux/percpu.h>\n#include <linux/ptrace.h>\n#include <linux/reboot.h>\n#include <linux/vmstat.h>\n#include <linux/device.h>\n#include <linux/export.h>\n#include <linux/vmalloc.h>\n#include <linux/hardirq.h>\n#include <linux/hugetlb.h>\n#include <linux/rculist.h>\n#include <linux/uaccess.h>\n#include <linux/syscalls.h>\n#include <linux/anon_inodes.h>\n#include <linux/kernel_stat.h>\n#include <linux/cgroup.h>\n#include <linux/perf_event.h>\n#include <linux/trace_events.h>\n#include <linux/hw_breakpoint.h>\n#include <linux/mm_types.h>\n#include <linux/module.h>\n#include <linux/mman.h>\n#include <linux/compat.h>\n#include <linux/bpf.h>\n#include <linux/filter.h>\n#include <linux/namei.h>\n#include <linux/parser.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/mm.h>\n#include <linux/proc_ns.h>\n#include <linux/mount.h>\n#include <linux/min_heap.h>\n#include <linux/highmem.h>\n#include <linux/pgtable.h>\n#include <linux/buildid.h>\n#include <linux/task_work.h>\n\n#include \"internal.h\"\n\n#include <asm/irq_regs.h>\n\ntypedef int (*remote_function_f)(void *);\n\nstruct remote_function_call {\n\tstruct task_struct\t*p;\n\tremote_function_f\tfunc;\n\tvoid\t\t\t*info;\n\tint\t\t\tret;\n};\n\nstatic void remote_function(void *data)\n{\n\tstruct remote_function_call *tfc = data;\n\tstruct task_struct *p = tfc->p;\n\n\tif (p) {\n\t\t \n\t\tif (task_cpu(p) != smp_processor_id())\n\t\t\treturn;\n\n\t\t \n\n\t\ttfc->ret = -ESRCH;  \n\t\tif (p != current)\n\t\t\treturn;\n\t}\n\n\ttfc->ret = tfc->func(tfc->info);\n}\n\n \nstatic int\ntask_function_call(struct task_struct *p, remote_function_f func, void *info)\n{\n\tstruct remote_function_call data = {\n\t\t.p\t= p,\n\t\t.func\t= func,\n\t\t.info\t= info,\n\t\t.ret\t= -EAGAIN,\n\t};\n\tint ret;\n\n\tfor (;;) {\n\t\tret = smp_call_function_single(task_cpu(p), remote_function,\n\t\t\t\t\t       &data, 1);\n\t\tif (!ret)\n\t\t\tret = data.ret;\n\n\t\tif (ret != -EAGAIN)\n\t\t\tbreak;\n\n\t\tcond_resched();\n\t}\n\n\treturn ret;\n}\n\n \nstatic int cpu_function_call(int cpu, remote_function_f func, void *info)\n{\n\tstruct remote_function_call data = {\n\t\t.p\t= NULL,\n\t\t.func\t= func,\n\t\t.info\t= info,\n\t\t.ret\t= -ENXIO,  \n\t};\n\n\tsmp_call_function_single(cpu, remote_function, &data, 1);\n\n\treturn data.ret;\n}\n\nstatic void perf_ctx_lock(struct perf_cpu_context *cpuctx,\n\t\t\t  struct perf_event_context *ctx)\n{\n\traw_spin_lock(&cpuctx->ctx.lock);\n\tif (ctx)\n\t\traw_spin_lock(&ctx->lock);\n}\n\nstatic void perf_ctx_unlock(struct perf_cpu_context *cpuctx,\n\t\t\t    struct perf_event_context *ctx)\n{\n\tif (ctx)\n\t\traw_spin_unlock(&ctx->lock);\n\traw_spin_unlock(&cpuctx->ctx.lock);\n}\n\n#define TASK_TOMBSTONE ((void *)-1L)\n\nstatic bool is_kernel_event(struct perf_event *event)\n{\n\treturn READ_ONCE(event->owner) == TASK_TOMBSTONE;\n}\n\nstatic DEFINE_PER_CPU(struct perf_cpu_context, perf_cpu_context);\n\nstruct perf_event_context *perf_cpu_task_ctx(void)\n{\n\tlockdep_assert_irqs_disabled();\n\treturn this_cpu_ptr(&perf_cpu_context)->task_ctx;\n}\n\n \n\ntypedef void (*event_f)(struct perf_event *, struct perf_cpu_context *,\n\t\t\tstruct perf_event_context *, void *);\n\nstruct event_function_struct {\n\tstruct perf_event *event;\n\tevent_f func;\n\tvoid *data;\n};\n\nstatic int event_function(void *info)\n{\n\tstruct event_function_struct *efs = info;\n\tstruct perf_event *event = efs->event;\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct perf_cpu_context *cpuctx = this_cpu_ptr(&perf_cpu_context);\n\tstruct perf_event_context *task_ctx = cpuctx->task_ctx;\n\tint ret = 0;\n\n\tlockdep_assert_irqs_disabled();\n\n\tperf_ctx_lock(cpuctx, task_ctx);\n\t \n\tif (ctx->task) {\n\t\tif (ctx->task != current) {\n\t\t\tret = -ESRCH;\n\t\t\tgoto unlock;\n\t\t}\n\n\t\t \n\t\tWARN_ON_ONCE(!ctx->is_active);\n\t\t \n\t\tWARN_ON_ONCE(task_ctx != ctx);\n\t} else {\n\t\tWARN_ON_ONCE(&cpuctx->ctx != ctx);\n\t}\n\n\tefs->func(event, cpuctx, ctx, efs->data);\nunlock:\n\tperf_ctx_unlock(cpuctx, task_ctx);\n\n\treturn ret;\n}\n\nstatic void event_function_call(struct perf_event *event, event_f func, void *data)\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct task_struct *task = READ_ONCE(ctx->task);  \n\tstruct event_function_struct efs = {\n\t\t.event = event,\n\t\t.func = func,\n\t\t.data = data,\n\t};\n\n\tif (!event->parent) {\n\t\t \n\t\tlockdep_assert_held(&ctx->mutex);\n\t}\n\n\tif (!task) {\n\t\tcpu_function_call(event->cpu, event_function, &efs);\n\t\treturn;\n\t}\n\n\tif (task == TASK_TOMBSTONE)\n\t\treturn;\n\nagain:\n\tif (!task_function_call(task, event_function, &efs))\n\t\treturn;\n\n\traw_spin_lock_irq(&ctx->lock);\n\t \n\ttask = ctx->task;\n\tif (task == TASK_TOMBSTONE) {\n\t\traw_spin_unlock_irq(&ctx->lock);\n\t\treturn;\n\t}\n\tif (ctx->is_active) {\n\t\traw_spin_unlock_irq(&ctx->lock);\n\t\tgoto again;\n\t}\n\tfunc(event, NULL, ctx, data);\n\traw_spin_unlock_irq(&ctx->lock);\n}\n\n \nstatic void event_function_local(struct perf_event *event, event_f func, void *data)\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct perf_cpu_context *cpuctx = this_cpu_ptr(&perf_cpu_context);\n\tstruct task_struct *task = READ_ONCE(ctx->task);\n\tstruct perf_event_context *task_ctx = NULL;\n\n\tlockdep_assert_irqs_disabled();\n\n\tif (task) {\n\t\tif (task == TASK_TOMBSTONE)\n\t\t\treturn;\n\n\t\ttask_ctx = ctx;\n\t}\n\n\tperf_ctx_lock(cpuctx, task_ctx);\n\n\ttask = ctx->task;\n\tif (task == TASK_TOMBSTONE)\n\t\tgoto unlock;\n\n\tif (task) {\n\t\t \n\t\tif (ctx->is_active) {\n\t\t\tif (WARN_ON_ONCE(task != current))\n\t\t\t\tgoto unlock;\n\n\t\t\tif (WARN_ON_ONCE(cpuctx->task_ctx != ctx))\n\t\t\t\tgoto unlock;\n\t\t}\n\t} else {\n\t\tWARN_ON_ONCE(&cpuctx->ctx != ctx);\n\t}\n\n\tfunc(event, cpuctx, ctx, data);\nunlock:\n\tperf_ctx_unlock(cpuctx, task_ctx);\n}\n\n#define PERF_FLAG_ALL (PERF_FLAG_FD_NO_GROUP |\\\n\t\t       PERF_FLAG_FD_OUTPUT  |\\\n\t\t       PERF_FLAG_PID_CGROUP |\\\n\t\t       PERF_FLAG_FD_CLOEXEC)\n\n \n#define PERF_SAMPLE_BRANCH_PERM_PLM \\\n\t(PERF_SAMPLE_BRANCH_KERNEL |\\\n\t PERF_SAMPLE_BRANCH_HV)\n\nenum event_type_t {\n\tEVENT_FLEXIBLE = 0x1,\n\tEVENT_PINNED = 0x2,\n\tEVENT_TIME = 0x4,\n\t \n\tEVENT_CPU = 0x8,\n\tEVENT_CGROUP = 0x10,\n\tEVENT_ALL = EVENT_FLEXIBLE | EVENT_PINNED,\n};\n\n \n\nstatic void perf_sched_delayed(struct work_struct *work);\nDEFINE_STATIC_KEY_FALSE(perf_sched_events);\nstatic DECLARE_DELAYED_WORK(perf_sched_work, perf_sched_delayed);\nstatic DEFINE_MUTEX(perf_sched_mutex);\nstatic atomic_t perf_sched_count;\n\nstatic DEFINE_PER_CPU(struct pmu_event_list, pmu_sb_events);\n\nstatic atomic_t nr_mmap_events __read_mostly;\nstatic atomic_t nr_comm_events __read_mostly;\nstatic atomic_t nr_namespaces_events __read_mostly;\nstatic atomic_t nr_task_events __read_mostly;\nstatic atomic_t nr_freq_events __read_mostly;\nstatic atomic_t nr_switch_events __read_mostly;\nstatic atomic_t nr_ksymbol_events __read_mostly;\nstatic atomic_t nr_bpf_events __read_mostly;\nstatic atomic_t nr_cgroup_events __read_mostly;\nstatic atomic_t nr_text_poke_events __read_mostly;\nstatic atomic_t nr_build_id_events __read_mostly;\n\nstatic LIST_HEAD(pmus);\nstatic DEFINE_MUTEX(pmus_lock);\nstatic struct srcu_struct pmus_srcu;\nstatic cpumask_var_t perf_online_mask;\nstatic struct kmem_cache *perf_event_cache;\n\n \nint sysctl_perf_event_paranoid __read_mostly = 2;\n\n \nint sysctl_perf_event_mlock __read_mostly = 512 + (PAGE_SIZE / 1024);  \n\n \n#define DEFAULT_MAX_SAMPLE_RATE\t\t100000\n#define DEFAULT_SAMPLE_PERIOD_NS\t(NSEC_PER_SEC / DEFAULT_MAX_SAMPLE_RATE)\n#define DEFAULT_CPU_TIME_MAX_PERCENT\t25\n\nint sysctl_perf_event_sample_rate __read_mostly\t= DEFAULT_MAX_SAMPLE_RATE;\n\nstatic int max_samples_per_tick __read_mostly\t= DIV_ROUND_UP(DEFAULT_MAX_SAMPLE_RATE, HZ);\nstatic int perf_sample_period_ns __read_mostly\t= DEFAULT_SAMPLE_PERIOD_NS;\n\nstatic int perf_sample_allowed_ns __read_mostly =\n\tDEFAULT_SAMPLE_PERIOD_NS * DEFAULT_CPU_TIME_MAX_PERCENT / 100;\n\nstatic void update_perf_cpu_limits(void)\n{\n\tu64 tmp = perf_sample_period_ns;\n\n\ttmp *= sysctl_perf_cpu_time_max_percent;\n\ttmp = div_u64(tmp, 100);\n\tif (!tmp)\n\t\ttmp = 1;\n\n\tWRITE_ONCE(perf_sample_allowed_ns, tmp);\n}\n\nstatic bool perf_rotate_context(struct perf_cpu_pmu_context *cpc);\n\nint perf_proc_update_handler(struct ctl_table *table, int write,\n\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tint ret;\n\tint perf_cpu = sysctl_perf_cpu_time_max_percent;\n\t \n\tif (write && (perf_cpu == 100 || perf_cpu == 0))\n\t\treturn -EINVAL;\n\n\tret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);\n\tif (ret || !write)\n\t\treturn ret;\n\n\tmax_samples_per_tick = DIV_ROUND_UP(sysctl_perf_event_sample_rate, HZ);\n\tperf_sample_period_ns = NSEC_PER_SEC / sysctl_perf_event_sample_rate;\n\tupdate_perf_cpu_limits();\n\n\treturn 0;\n}\n\nint sysctl_perf_cpu_time_max_percent __read_mostly = DEFAULT_CPU_TIME_MAX_PERCENT;\n\nint perf_cpu_time_max_percent_handler(struct ctl_table *table, int write,\n\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tint ret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);\n\n\tif (ret || !write)\n\t\treturn ret;\n\n\tif (sysctl_perf_cpu_time_max_percent == 100 ||\n\t    sysctl_perf_cpu_time_max_percent == 0) {\n\t\tprintk(KERN_WARNING\n\t\t       \"perf: Dynamic interrupt throttling disabled, can hang your system!\\n\");\n\t\tWRITE_ONCE(perf_sample_allowed_ns, 0);\n\t} else {\n\t\tupdate_perf_cpu_limits();\n\t}\n\n\treturn 0;\n}\n\n \n#define NR_ACCUMULATED_SAMPLES 128\nstatic DEFINE_PER_CPU(u64, running_sample_length);\n\nstatic u64 __report_avg;\nstatic u64 __report_allowed;\n\nstatic void perf_duration_warn(struct irq_work *w)\n{\n\tprintk_ratelimited(KERN_INFO\n\t\t\"perf: interrupt took too long (%lld > %lld), lowering \"\n\t\t\"kernel.perf_event_max_sample_rate to %d\\n\",\n\t\t__report_avg, __report_allowed,\n\t\tsysctl_perf_event_sample_rate);\n}\n\nstatic DEFINE_IRQ_WORK(perf_duration_work, perf_duration_warn);\n\nvoid perf_sample_event_took(u64 sample_len_ns)\n{\n\tu64 max_len = READ_ONCE(perf_sample_allowed_ns);\n\tu64 running_len;\n\tu64 avg_len;\n\tu32 max;\n\n\tif (max_len == 0)\n\t\treturn;\n\n\t \n\trunning_len = __this_cpu_read(running_sample_length);\n\trunning_len -= running_len/NR_ACCUMULATED_SAMPLES;\n\trunning_len += sample_len_ns;\n\t__this_cpu_write(running_sample_length, running_len);\n\n\t \n\tavg_len = running_len/NR_ACCUMULATED_SAMPLES;\n\tif (avg_len <= max_len)\n\t\treturn;\n\n\t__report_avg = avg_len;\n\t__report_allowed = max_len;\n\n\t \n\tavg_len += avg_len / 4;\n\tmax = (TICK_NSEC / 100) * sysctl_perf_cpu_time_max_percent;\n\tif (avg_len < max)\n\t\tmax /= (u32)avg_len;\n\telse\n\t\tmax = 1;\n\n\tWRITE_ONCE(perf_sample_allowed_ns, avg_len);\n\tWRITE_ONCE(max_samples_per_tick, max);\n\n\tsysctl_perf_event_sample_rate = max * HZ;\n\tperf_sample_period_ns = NSEC_PER_SEC / sysctl_perf_event_sample_rate;\n\n\tif (!irq_work_queue(&perf_duration_work)) {\n\t\tearly_printk(\"perf: interrupt took too long (%lld > %lld), lowering \"\n\t\t\t     \"kernel.perf_event_max_sample_rate to %d\\n\",\n\t\t\t     __report_avg, __report_allowed,\n\t\t\t     sysctl_perf_event_sample_rate);\n\t}\n}\n\nstatic atomic64_t perf_event_id;\n\nstatic void update_context_time(struct perf_event_context *ctx);\nstatic u64 perf_event_time(struct perf_event *event);\n\nvoid __weak perf_event_print_debug(void)\t{ }\n\nstatic inline u64 perf_clock(void)\n{\n\treturn local_clock();\n}\n\nstatic inline u64 perf_event_clock(struct perf_event *event)\n{\n\treturn event->clock();\n}\n\n \n\nstatic __always_inline enum perf_event_state\n__perf_effective_state(struct perf_event *event)\n{\n\tstruct perf_event *leader = event->group_leader;\n\n\tif (leader->state <= PERF_EVENT_STATE_OFF)\n\t\treturn leader->state;\n\n\treturn event->state;\n}\n\nstatic __always_inline void\n__perf_update_times(struct perf_event *event, u64 now, u64 *enabled, u64 *running)\n{\n\tenum perf_event_state state = __perf_effective_state(event);\n\tu64 delta = now - event->tstamp;\n\n\t*enabled = event->total_time_enabled;\n\tif (state >= PERF_EVENT_STATE_INACTIVE)\n\t\t*enabled += delta;\n\n\t*running = event->total_time_running;\n\tif (state >= PERF_EVENT_STATE_ACTIVE)\n\t\t*running += delta;\n}\n\nstatic void perf_event_update_time(struct perf_event *event)\n{\n\tu64 now = perf_event_time(event);\n\n\t__perf_update_times(event, now, &event->total_time_enabled,\n\t\t\t\t\t&event->total_time_running);\n\tevent->tstamp = now;\n}\n\nstatic void perf_event_update_sibling_time(struct perf_event *leader)\n{\n\tstruct perf_event *sibling;\n\n\tfor_each_sibling_event(sibling, leader)\n\t\tperf_event_update_time(sibling);\n}\n\nstatic void\nperf_event_set_state(struct perf_event *event, enum perf_event_state state)\n{\n\tif (event->state == state)\n\t\treturn;\n\n\tperf_event_update_time(event);\n\t \n\tif ((event->state < 0) ^ (state < 0))\n\t\tperf_event_update_sibling_time(event);\n\n\tWRITE_ONCE(event->state, state);\n}\n\n \n\n#define __store_release(ptr, val)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tbarrier();\t\t\t\t\t\t\t\\\n\tWRITE_ONCE(*(ptr), (val));\t\t\t\t\t\\\n} while (0)\n\n#define __load_acquire(ptr)\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__unqual_scalar_typeof(*(ptr)) ___p = READ_ONCE(*(ptr));\t\\\n\tbarrier();\t\t\t\t\t\t\t\\\n\t___p;\t\t\t\t\t\t\t\t\\\n})\n\nstatic void perf_ctx_disable(struct perf_event_context *ctx, bool cgroup)\n{\n\tstruct perf_event_pmu_context *pmu_ctx;\n\n\tlist_for_each_entry(pmu_ctx, &ctx->pmu_ctx_list, pmu_ctx_entry) {\n\t\tif (cgroup && !pmu_ctx->nr_cgroups)\n\t\t\tcontinue;\n\t\tperf_pmu_disable(pmu_ctx->pmu);\n\t}\n}\n\nstatic void perf_ctx_enable(struct perf_event_context *ctx, bool cgroup)\n{\n\tstruct perf_event_pmu_context *pmu_ctx;\n\n\tlist_for_each_entry(pmu_ctx, &ctx->pmu_ctx_list, pmu_ctx_entry) {\n\t\tif (cgroup && !pmu_ctx->nr_cgroups)\n\t\t\tcontinue;\n\t\tperf_pmu_enable(pmu_ctx->pmu);\n\t}\n}\n\nstatic void ctx_sched_out(struct perf_event_context *ctx, enum event_type_t event_type);\nstatic void ctx_sched_in(struct perf_event_context *ctx, enum event_type_t event_type);\n\n#ifdef CONFIG_CGROUP_PERF\n\nstatic inline bool\nperf_cgroup_match(struct perf_event *event)\n{\n\tstruct perf_cpu_context *cpuctx = this_cpu_ptr(&perf_cpu_context);\n\n\t \n\tif (!event->cgrp)\n\t\treturn true;\n\n\t \n\tif (!cpuctx->cgrp)\n\t\treturn false;\n\n\t \n\treturn cgroup_is_descendant(cpuctx->cgrp->css.cgroup,\n\t\t\t\t    event->cgrp->css.cgroup);\n}\n\nstatic inline void perf_detach_cgroup(struct perf_event *event)\n{\n\tcss_put(&event->cgrp->css);\n\tevent->cgrp = NULL;\n}\n\nstatic inline int is_cgroup_event(struct perf_event *event)\n{\n\treturn event->cgrp != NULL;\n}\n\nstatic inline u64 perf_cgroup_event_time(struct perf_event *event)\n{\n\tstruct perf_cgroup_info *t;\n\n\tt = per_cpu_ptr(event->cgrp->info, event->cpu);\n\treturn t->time;\n}\n\nstatic inline u64 perf_cgroup_event_time_now(struct perf_event *event, u64 now)\n{\n\tstruct perf_cgroup_info *t;\n\n\tt = per_cpu_ptr(event->cgrp->info, event->cpu);\n\tif (!__load_acquire(&t->active))\n\t\treturn t->time;\n\tnow += READ_ONCE(t->timeoffset);\n\treturn now;\n}\n\nstatic inline void __update_cgrp_time(struct perf_cgroup_info *info, u64 now, bool adv)\n{\n\tif (adv)\n\t\tinfo->time += now - info->timestamp;\n\tinfo->timestamp = now;\n\t \n\tWRITE_ONCE(info->timeoffset, info->time - info->timestamp);\n}\n\nstatic inline void update_cgrp_time_from_cpuctx(struct perf_cpu_context *cpuctx, bool final)\n{\n\tstruct perf_cgroup *cgrp = cpuctx->cgrp;\n\tstruct cgroup_subsys_state *css;\n\tstruct perf_cgroup_info *info;\n\n\tif (cgrp) {\n\t\tu64 now = perf_clock();\n\n\t\tfor (css = &cgrp->css; css; css = css->parent) {\n\t\t\tcgrp = container_of(css, struct perf_cgroup, css);\n\t\t\tinfo = this_cpu_ptr(cgrp->info);\n\n\t\t\t__update_cgrp_time(info, now, true);\n\t\t\tif (final)\n\t\t\t\t__store_release(&info->active, 0);\n\t\t}\n\t}\n}\n\nstatic inline void update_cgrp_time_from_event(struct perf_event *event)\n{\n\tstruct perf_cgroup_info *info;\n\n\t \n\tif (!is_cgroup_event(event))\n\t\treturn;\n\n\tinfo = this_cpu_ptr(event->cgrp->info);\n\t \n\tif (info->active)\n\t\t__update_cgrp_time(info, perf_clock(), true);\n}\n\nstatic inline void\nperf_cgroup_set_timestamp(struct perf_cpu_context *cpuctx)\n{\n\tstruct perf_event_context *ctx = &cpuctx->ctx;\n\tstruct perf_cgroup *cgrp = cpuctx->cgrp;\n\tstruct perf_cgroup_info *info;\n\tstruct cgroup_subsys_state *css;\n\n\t \n\tif (!cgrp)\n\t\treturn;\n\n\tWARN_ON_ONCE(!ctx->nr_cgroups);\n\n\tfor (css = &cgrp->css; css; css = css->parent) {\n\t\tcgrp = container_of(css, struct perf_cgroup, css);\n\t\tinfo = this_cpu_ptr(cgrp->info);\n\t\t__update_cgrp_time(info, ctx->timestamp, false);\n\t\t__store_release(&info->active, 1);\n\t}\n}\n\n \nstatic void perf_cgroup_switch(struct task_struct *task)\n{\n\tstruct perf_cpu_context *cpuctx = this_cpu_ptr(&perf_cpu_context);\n\tstruct perf_cgroup *cgrp;\n\n\t \n\tif (READ_ONCE(cpuctx->cgrp) == NULL)\n\t\treturn;\n\n\tWARN_ON_ONCE(cpuctx->ctx.nr_cgroups == 0);\n\n\tcgrp = perf_cgroup_from_task(task, NULL);\n\tif (READ_ONCE(cpuctx->cgrp) == cgrp)\n\t\treturn;\n\n\tperf_ctx_lock(cpuctx, cpuctx->task_ctx);\n\tperf_ctx_disable(&cpuctx->ctx, true);\n\n\tctx_sched_out(&cpuctx->ctx, EVENT_ALL|EVENT_CGROUP);\n\t \n\tcpuctx->cgrp = cgrp;\n\t \n\tctx_sched_in(&cpuctx->ctx, EVENT_ALL|EVENT_CGROUP);\n\n\tperf_ctx_enable(&cpuctx->ctx, true);\n\tperf_ctx_unlock(cpuctx, cpuctx->task_ctx);\n}\n\nstatic int perf_cgroup_ensure_storage(struct perf_event *event,\n\t\t\t\tstruct cgroup_subsys_state *css)\n{\n\tstruct perf_cpu_context *cpuctx;\n\tstruct perf_event **storage;\n\tint cpu, heap_size, ret = 0;\n\n\t \n\tfor (heap_size = 1; css; css = css->parent)\n\t\theap_size++;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tcpuctx = per_cpu_ptr(&perf_cpu_context, cpu);\n\t\tif (heap_size <= cpuctx->heap_size)\n\t\t\tcontinue;\n\n\t\tstorage = kmalloc_node(heap_size * sizeof(struct perf_event *),\n\t\t\t\t       GFP_KERNEL, cpu_to_node(cpu));\n\t\tif (!storage) {\n\t\t\tret = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\n\t\traw_spin_lock_irq(&cpuctx->ctx.lock);\n\t\tif (cpuctx->heap_size < heap_size) {\n\t\t\tswap(cpuctx->heap, storage);\n\t\t\tif (storage == cpuctx->heap_default)\n\t\t\t\tstorage = NULL;\n\t\t\tcpuctx->heap_size = heap_size;\n\t\t}\n\t\traw_spin_unlock_irq(&cpuctx->ctx.lock);\n\n\t\tkfree(storage);\n\t}\n\n\treturn ret;\n}\n\nstatic inline int perf_cgroup_connect(int fd, struct perf_event *event,\n\t\t\t\t      struct perf_event_attr *attr,\n\t\t\t\t      struct perf_event *group_leader)\n{\n\tstruct perf_cgroup *cgrp;\n\tstruct cgroup_subsys_state *css;\n\tstruct fd f = fdget(fd);\n\tint ret = 0;\n\n\tif (!f.file)\n\t\treturn -EBADF;\n\n\tcss = css_tryget_online_from_dir(f.file->f_path.dentry,\n\t\t\t\t\t &perf_event_cgrp_subsys);\n\tif (IS_ERR(css)) {\n\t\tret = PTR_ERR(css);\n\t\tgoto out;\n\t}\n\n\tret = perf_cgroup_ensure_storage(event, css);\n\tif (ret)\n\t\tgoto out;\n\n\tcgrp = container_of(css, struct perf_cgroup, css);\n\tevent->cgrp = cgrp;\n\n\t \n\tif (group_leader && group_leader->cgrp != cgrp) {\n\t\tperf_detach_cgroup(event);\n\t\tret = -EINVAL;\n\t}\nout:\n\tfdput(f);\n\treturn ret;\n}\n\nstatic inline void\nperf_cgroup_event_enable(struct perf_event *event, struct perf_event_context *ctx)\n{\n\tstruct perf_cpu_context *cpuctx;\n\n\tif (!is_cgroup_event(event))\n\t\treturn;\n\n\tevent->pmu_ctx->nr_cgroups++;\n\n\t \n\tcpuctx = container_of(ctx, struct perf_cpu_context, ctx);\n\n\tif (ctx->nr_cgroups++)\n\t\treturn;\n\n\tcpuctx->cgrp = perf_cgroup_from_task(current, ctx);\n}\n\nstatic inline void\nperf_cgroup_event_disable(struct perf_event *event, struct perf_event_context *ctx)\n{\n\tstruct perf_cpu_context *cpuctx;\n\n\tif (!is_cgroup_event(event))\n\t\treturn;\n\n\tevent->pmu_ctx->nr_cgroups--;\n\n\t \n\tcpuctx = container_of(ctx, struct perf_cpu_context, ctx);\n\n\tif (--ctx->nr_cgroups)\n\t\treturn;\n\n\tcpuctx->cgrp = NULL;\n}\n\n#else  \n\nstatic inline bool\nperf_cgroup_match(struct perf_event *event)\n{\n\treturn true;\n}\n\nstatic inline void perf_detach_cgroup(struct perf_event *event)\n{}\n\nstatic inline int is_cgroup_event(struct perf_event *event)\n{\n\treturn 0;\n}\n\nstatic inline void update_cgrp_time_from_event(struct perf_event *event)\n{\n}\n\nstatic inline void update_cgrp_time_from_cpuctx(struct perf_cpu_context *cpuctx,\n\t\t\t\t\t\tbool final)\n{\n}\n\nstatic inline int perf_cgroup_connect(pid_t pid, struct perf_event *event,\n\t\t\t\t      struct perf_event_attr *attr,\n\t\t\t\t      struct perf_event *group_leader)\n{\n\treturn -EINVAL;\n}\n\nstatic inline void\nperf_cgroup_set_timestamp(struct perf_cpu_context *cpuctx)\n{\n}\n\nstatic inline u64 perf_cgroup_event_time(struct perf_event *event)\n{\n\treturn 0;\n}\n\nstatic inline u64 perf_cgroup_event_time_now(struct perf_event *event, u64 now)\n{\n\treturn 0;\n}\n\nstatic inline void\nperf_cgroup_event_enable(struct perf_event *event, struct perf_event_context *ctx)\n{\n}\n\nstatic inline void\nperf_cgroup_event_disable(struct perf_event *event, struct perf_event_context *ctx)\n{\n}\n\nstatic void perf_cgroup_switch(struct task_struct *task)\n{\n}\n#endif\n\n \n#define PERF_CPU_HRTIMER (1000 / HZ)\n \nstatic enum hrtimer_restart perf_mux_hrtimer_handler(struct hrtimer *hr)\n{\n\tstruct perf_cpu_pmu_context *cpc;\n\tbool rotations;\n\n\tlockdep_assert_irqs_disabled();\n\n\tcpc = container_of(hr, struct perf_cpu_pmu_context, hrtimer);\n\trotations = perf_rotate_context(cpc);\n\n\traw_spin_lock(&cpc->hrtimer_lock);\n\tif (rotations)\n\t\thrtimer_forward_now(hr, cpc->hrtimer_interval);\n\telse\n\t\tcpc->hrtimer_active = 0;\n\traw_spin_unlock(&cpc->hrtimer_lock);\n\n\treturn rotations ? HRTIMER_RESTART : HRTIMER_NORESTART;\n}\n\nstatic void __perf_mux_hrtimer_init(struct perf_cpu_pmu_context *cpc, int cpu)\n{\n\tstruct hrtimer *timer = &cpc->hrtimer;\n\tstruct pmu *pmu = cpc->epc.pmu;\n\tu64 interval;\n\n\t \n\tinterval = pmu->hrtimer_interval_ms;\n\tif (interval < 1)\n\t\tinterval = pmu->hrtimer_interval_ms = PERF_CPU_HRTIMER;\n\n\tcpc->hrtimer_interval = ns_to_ktime(NSEC_PER_MSEC * interval);\n\n\traw_spin_lock_init(&cpc->hrtimer_lock);\n\thrtimer_init(timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS_PINNED_HARD);\n\ttimer->function = perf_mux_hrtimer_handler;\n}\n\nstatic int perf_mux_hrtimer_restart(struct perf_cpu_pmu_context *cpc)\n{\n\tstruct hrtimer *timer = &cpc->hrtimer;\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&cpc->hrtimer_lock, flags);\n\tif (!cpc->hrtimer_active) {\n\t\tcpc->hrtimer_active = 1;\n\t\thrtimer_forward_now(timer, cpc->hrtimer_interval);\n\t\thrtimer_start_expires(timer, HRTIMER_MODE_ABS_PINNED_HARD);\n\t}\n\traw_spin_unlock_irqrestore(&cpc->hrtimer_lock, flags);\n\n\treturn 0;\n}\n\nstatic int perf_mux_hrtimer_restart_ipi(void *arg)\n{\n\treturn perf_mux_hrtimer_restart(arg);\n}\n\nvoid perf_pmu_disable(struct pmu *pmu)\n{\n\tint *count = this_cpu_ptr(pmu->pmu_disable_count);\n\tif (!(*count)++)\n\t\tpmu->pmu_disable(pmu);\n}\n\nvoid perf_pmu_enable(struct pmu *pmu)\n{\n\tint *count = this_cpu_ptr(pmu->pmu_disable_count);\n\tif (!--(*count))\n\t\tpmu->pmu_enable(pmu);\n}\n\nstatic void perf_assert_pmu_disabled(struct pmu *pmu)\n{\n\tWARN_ON_ONCE(*this_cpu_ptr(pmu->pmu_disable_count) == 0);\n}\n\nstatic void get_ctx(struct perf_event_context *ctx)\n{\n\trefcount_inc(&ctx->refcount);\n}\n\nstatic void *alloc_task_ctx_data(struct pmu *pmu)\n{\n\tif (pmu->task_ctx_cache)\n\t\treturn kmem_cache_zalloc(pmu->task_ctx_cache, GFP_KERNEL);\n\n\treturn NULL;\n}\n\nstatic void free_task_ctx_data(struct pmu *pmu, void *task_ctx_data)\n{\n\tif (pmu->task_ctx_cache && task_ctx_data)\n\t\tkmem_cache_free(pmu->task_ctx_cache, task_ctx_data);\n}\n\nstatic void free_ctx(struct rcu_head *head)\n{\n\tstruct perf_event_context *ctx;\n\n\tctx = container_of(head, struct perf_event_context, rcu_head);\n\tkfree(ctx);\n}\n\nstatic void put_ctx(struct perf_event_context *ctx)\n{\n\tif (refcount_dec_and_test(&ctx->refcount)) {\n\t\tif (ctx->parent_ctx)\n\t\t\tput_ctx(ctx->parent_ctx);\n\t\tif (ctx->task && ctx->task != TASK_TOMBSTONE)\n\t\t\tput_task_struct(ctx->task);\n\t\tcall_rcu(&ctx->rcu_head, free_ctx);\n\t}\n}\n\n \nstatic struct perf_event_context *\nperf_event_ctx_lock_nested(struct perf_event *event, int nesting)\n{\n\tstruct perf_event_context *ctx;\n\nagain:\n\trcu_read_lock();\n\tctx = READ_ONCE(event->ctx);\n\tif (!refcount_inc_not_zero(&ctx->refcount)) {\n\t\trcu_read_unlock();\n\t\tgoto again;\n\t}\n\trcu_read_unlock();\n\n\tmutex_lock_nested(&ctx->mutex, nesting);\n\tif (event->ctx != ctx) {\n\t\tmutex_unlock(&ctx->mutex);\n\t\tput_ctx(ctx);\n\t\tgoto again;\n\t}\n\n\treturn ctx;\n}\n\nstatic inline struct perf_event_context *\nperf_event_ctx_lock(struct perf_event *event)\n{\n\treturn perf_event_ctx_lock_nested(event, 0);\n}\n\nstatic void perf_event_ctx_unlock(struct perf_event *event,\n\t\t\t\t  struct perf_event_context *ctx)\n{\n\tmutex_unlock(&ctx->mutex);\n\tput_ctx(ctx);\n}\n\n \nstatic __must_check struct perf_event_context *\nunclone_ctx(struct perf_event_context *ctx)\n{\n\tstruct perf_event_context *parent_ctx = ctx->parent_ctx;\n\n\tlockdep_assert_held(&ctx->lock);\n\n\tif (parent_ctx)\n\t\tctx->parent_ctx = NULL;\n\tctx->generation++;\n\n\treturn parent_ctx;\n}\n\nstatic u32 perf_event_pid_type(struct perf_event *event, struct task_struct *p,\n\t\t\t\tenum pid_type type)\n{\n\tu32 nr;\n\t \n\tif (event->parent)\n\t\tevent = event->parent;\n\n\tnr = __task_pid_nr_ns(p, type, event->ns);\n\t \n\tif (!nr && !pid_alive(p))\n\t\tnr = -1;\n\treturn nr;\n}\n\nstatic u32 perf_event_pid(struct perf_event *event, struct task_struct *p)\n{\n\treturn perf_event_pid_type(event, p, PIDTYPE_TGID);\n}\n\nstatic u32 perf_event_tid(struct perf_event *event, struct task_struct *p)\n{\n\treturn perf_event_pid_type(event, p, PIDTYPE_PID);\n}\n\n \nstatic u64 primary_event_id(struct perf_event *event)\n{\n\tu64 id = event->id;\n\n\tif (event->parent)\n\t\tid = event->parent->id;\n\n\treturn id;\n}\n\n \nstatic struct perf_event_context *\nperf_lock_task_context(struct task_struct *task, unsigned long *flags)\n{\n\tstruct perf_event_context *ctx;\n\nretry:\n\t \n\tlocal_irq_save(*flags);\n\trcu_read_lock();\n\tctx = rcu_dereference(task->perf_event_ctxp);\n\tif (ctx) {\n\t\t \n\t\traw_spin_lock(&ctx->lock);\n\t\tif (ctx != rcu_dereference(task->perf_event_ctxp)) {\n\t\t\traw_spin_unlock(&ctx->lock);\n\t\t\trcu_read_unlock();\n\t\t\tlocal_irq_restore(*flags);\n\t\t\tgoto retry;\n\t\t}\n\n\t\tif (ctx->task == TASK_TOMBSTONE ||\n\t\t    !refcount_inc_not_zero(&ctx->refcount)) {\n\t\t\traw_spin_unlock(&ctx->lock);\n\t\t\tctx = NULL;\n\t\t} else {\n\t\t\tWARN_ON_ONCE(ctx->task != task);\n\t\t}\n\t}\n\trcu_read_unlock();\n\tif (!ctx)\n\t\tlocal_irq_restore(*flags);\n\treturn ctx;\n}\n\n \nstatic struct perf_event_context *\nperf_pin_task_context(struct task_struct *task)\n{\n\tstruct perf_event_context *ctx;\n\tunsigned long flags;\n\n\tctx = perf_lock_task_context(task, &flags);\n\tif (ctx) {\n\t\t++ctx->pin_count;\n\t\traw_spin_unlock_irqrestore(&ctx->lock, flags);\n\t}\n\treturn ctx;\n}\n\nstatic void perf_unpin_context(struct perf_event_context *ctx)\n{\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&ctx->lock, flags);\n\t--ctx->pin_count;\n\traw_spin_unlock_irqrestore(&ctx->lock, flags);\n}\n\n \nstatic void __update_context_time(struct perf_event_context *ctx, bool adv)\n{\n\tu64 now = perf_clock();\n\n\tlockdep_assert_held(&ctx->lock);\n\n\tif (adv)\n\t\tctx->time += now - ctx->timestamp;\n\tctx->timestamp = now;\n\n\t \n\tWRITE_ONCE(ctx->timeoffset, ctx->time - ctx->timestamp);\n}\n\nstatic void update_context_time(struct perf_event_context *ctx)\n{\n\t__update_context_time(ctx, true);\n}\n\nstatic u64 perf_event_time(struct perf_event *event)\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\n\tif (unlikely(!ctx))\n\t\treturn 0;\n\n\tif (is_cgroup_event(event))\n\t\treturn perf_cgroup_event_time(event);\n\n\treturn ctx->time;\n}\n\nstatic u64 perf_event_time_now(struct perf_event *event, u64 now)\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\n\tif (unlikely(!ctx))\n\t\treturn 0;\n\n\tif (is_cgroup_event(event))\n\t\treturn perf_cgroup_event_time_now(event, now);\n\n\tif (!(__load_acquire(&ctx->is_active) & EVENT_TIME))\n\t\treturn ctx->time;\n\n\tnow += READ_ONCE(ctx->timeoffset);\n\treturn now;\n}\n\nstatic enum event_type_t get_event_type(struct perf_event *event)\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tenum event_type_t event_type;\n\n\tlockdep_assert_held(&ctx->lock);\n\n\t \n\tif (event->group_leader != event)\n\t\tevent = event->group_leader;\n\n\tevent_type = event->attr.pinned ? EVENT_PINNED : EVENT_FLEXIBLE;\n\tif (!ctx->task)\n\t\tevent_type |= EVENT_CPU;\n\n\treturn event_type;\n}\n\n \nstatic void init_event_group(struct perf_event *event)\n{\n\tRB_CLEAR_NODE(&event->group_node);\n\tevent->group_index = 0;\n}\n\n \nstatic struct perf_event_groups *\nget_event_groups(struct perf_event *event, struct perf_event_context *ctx)\n{\n\tif (event->attr.pinned)\n\t\treturn &ctx->pinned_groups;\n\telse\n\t\treturn &ctx->flexible_groups;\n}\n\n \nstatic void perf_event_groups_init(struct perf_event_groups *groups)\n{\n\tgroups->tree = RB_ROOT;\n\tgroups->index = 0;\n}\n\nstatic inline struct cgroup *event_cgroup(const struct perf_event *event)\n{\n\tstruct cgroup *cgroup = NULL;\n\n#ifdef CONFIG_CGROUP_PERF\n\tif (event->cgrp)\n\t\tcgroup = event->cgrp->css.cgroup;\n#endif\n\n\treturn cgroup;\n}\n\n \nstatic __always_inline int\nperf_event_groups_cmp(const int left_cpu, const struct pmu *left_pmu,\n\t\t      const struct cgroup *left_cgroup, const u64 left_group_index,\n\t\t      const struct perf_event *right)\n{\n\tif (left_cpu < right->cpu)\n\t\treturn -1;\n\tif (left_cpu > right->cpu)\n\t\treturn 1;\n\n\tif (left_pmu) {\n\t\tif (left_pmu < right->pmu_ctx->pmu)\n\t\t\treturn -1;\n\t\tif (left_pmu > right->pmu_ctx->pmu)\n\t\t\treturn 1;\n\t}\n\n#ifdef CONFIG_CGROUP_PERF\n\t{\n\t\tconst struct cgroup *right_cgroup = event_cgroup(right);\n\n\t\tif (left_cgroup != right_cgroup) {\n\t\t\tif (!left_cgroup) {\n\t\t\t\t \n\t\t\t\treturn -1;\n\t\t\t}\n\t\t\tif (!right_cgroup) {\n\t\t\t\t \n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\t \n\t\t\tif (cgroup_id(left_cgroup) < cgroup_id(right_cgroup))\n\t\t\t\treturn -1;\n\n\t\t\treturn 1;\n\t\t}\n\t}\n#endif\n\n\tif (left_group_index < right->group_index)\n\t\treturn -1;\n\tif (left_group_index > right->group_index)\n\t\treturn 1;\n\n\treturn 0;\n}\n\n#define __node_2_pe(node) \\\n\trb_entry((node), struct perf_event, group_node)\n\nstatic inline bool __group_less(struct rb_node *a, const struct rb_node *b)\n{\n\tstruct perf_event *e = __node_2_pe(a);\n\treturn perf_event_groups_cmp(e->cpu, e->pmu_ctx->pmu, event_cgroup(e),\n\t\t\t\t     e->group_index, __node_2_pe(b)) < 0;\n}\n\nstruct __group_key {\n\tint cpu;\n\tstruct pmu *pmu;\n\tstruct cgroup *cgroup;\n};\n\nstatic inline int __group_cmp(const void *key, const struct rb_node *node)\n{\n\tconst struct __group_key *a = key;\n\tconst struct perf_event *b = __node_2_pe(node);\n\n\t \n\treturn perf_event_groups_cmp(a->cpu, a->pmu, a->cgroup, b->group_index, b);\n}\n\nstatic inline int\n__group_cmp_ignore_cgroup(const void *key, const struct rb_node *node)\n{\n\tconst struct __group_key *a = key;\n\tconst struct perf_event *b = __node_2_pe(node);\n\n\t \n\treturn perf_event_groups_cmp(a->cpu, a->pmu, event_cgroup(b),\n\t\t\t\t     b->group_index, b);\n}\n\n \nstatic void\nperf_event_groups_insert(struct perf_event_groups *groups,\n\t\t\t struct perf_event *event)\n{\n\tevent->group_index = ++groups->index;\n\n\trb_add(&event->group_node, &groups->tree, __group_less);\n}\n\n \nstatic void\nadd_event_to_groups(struct perf_event *event, struct perf_event_context *ctx)\n{\n\tstruct perf_event_groups *groups;\n\n\tgroups = get_event_groups(event, ctx);\n\tperf_event_groups_insert(groups, event);\n}\n\n \nstatic void\nperf_event_groups_delete(struct perf_event_groups *groups,\n\t\t\t struct perf_event *event)\n{\n\tWARN_ON_ONCE(RB_EMPTY_NODE(&event->group_node) ||\n\t\t     RB_EMPTY_ROOT(&groups->tree));\n\n\trb_erase(&event->group_node, &groups->tree);\n\tinit_event_group(event);\n}\n\n \nstatic void\ndel_event_from_groups(struct perf_event *event, struct perf_event_context *ctx)\n{\n\tstruct perf_event_groups *groups;\n\n\tgroups = get_event_groups(event, ctx);\n\tperf_event_groups_delete(groups, event);\n}\n\n \nstatic struct perf_event *\nperf_event_groups_first(struct perf_event_groups *groups, int cpu,\n\t\t\tstruct pmu *pmu, struct cgroup *cgrp)\n{\n\tstruct __group_key key = {\n\t\t.cpu = cpu,\n\t\t.pmu = pmu,\n\t\t.cgroup = cgrp,\n\t};\n\tstruct rb_node *node;\n\n\tnode = rb_find_first(&key, &groups->tree, __group_cmp);\n\tif (node)\n\t\treturn __node_2_pe(node);\n\n\treturn NULL;\n}\n\nstatic struct perf_event *\nperf_event_groups_next(struct perf_event *event, struct pmu *pmu)\n{\n\tstruct __group_key key = {\n\t\t.cpu = event->cpu,\n\t\t.pmu = pmu,\n\t\t.cgroup = event_cgroup(event),\n\t};\n\tstruct rb_node *next;\n\n\tnext = rb_next_match(&key, &event->group_node, __group_cmp);\n\tif (next)\n\t\treturn __node_2_pe(next);\n\n\treturn NULL;\n}\n\n#define perf_event_groups_for_cpu_pmu(event, groups, cpu, pmu)\t\t\\\n\tfor (event = perf_event_groups_first(groups, cpu, pmu, NULL);\t\\\n\t     event; event = perf_event_groups_next(event, pmu))\n\n \n#define perf_event_groups_for_each(event, groups)\t\t\t\\\n\tfor (event = rb_entry_safe(rb_first(&((groups)->tree)),\t\t\\\n\t\t\t\ttypeof(*event), group_node); event;\t\\\n\t\tevent = rb_entry_safe(rb_next(&event->group_node),\t\\\n\t\t\t\ttypeof(*event), group_node))\n\n \nstatic void\nlist_add_event(struct perf_event *event, struct perf_event_context *ctx)\n{\n\tlockdep_assert_held(&ctx->lock);\n\n\tWARN_ON_ONCE(event->attach_state & PERF_ATTACH_CONTEXT);\n\tevent->attach_state |= PERF_ATTACH_CONTEXT;\n\n\tevent->tstamp = perf_event_time(event);\n\n\t \n\tif (event->group_leader == event) {\n\t\tevent->group_caps = event->event_caps;\n\t\tadd_event_to_groups(event, ctx);\n\t}\n\n\tlist_add_rcu(&event->event_entry, &ctx->event_list);\n\tctx->nr_events++;\n\tif (event->hw.flags & PERF_EVENT_FLAG_USER_READ_CNT)\n\t\tctx->nr_user++;\n\tif (event->attr.inherit_stat)\n\t\tctx->nr_stat++;\n\n\tif (event->state > PERF_EVENT_STATE_OFF)\n\t\tperf_cgroup_event_enable(event, ctx);\n\n\tctx->generation++;\n\tevent->pmu_ctx->nr_events++;\n}\n\n \nstatic inline void perf_event__state_init(struct perf_event *event)\n{\n\tevent->state = event->attr.disabled ? PERF_EVENT_STATE_OFF :\n\t\t\t\t\t      PERF_EVENT_STATE_INACTIVE;\n}\n\nstatic int __perf_event_read_size(u64 read_format, int nr_siblings)\n{\n\tint entry = sizeof(u64);  \n\tint size = 0;\n\tint nr = 1;\n\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_ENABLED)\n\t\tsize += sizeof(u64);\n\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_RUNNING)\n\t\tsize += sizeof(u64);\n\n\tif (read_format & PERF_FORMAT_ID)\n\t\tentry += sizeof(u64);\n\n\tif (read_format & PERF_FORMAT_LOST)\n\t\tentry += sizeof(u64);\n\n\tif (read_format & PERF_FORMAT_GROUP) {\n\t\tnr += nr_siblings;\n\t\tsize += sizeof(u64);\n\t}\n\n\t \n\treturn size + nr * entry;\n}\n\nstatic void __perf_event_header_size(struct perf_event *event, u64 sample_type)\n{\n\tstruct perf_sample_data *data;\n\tu16 size = 0;\n\n\tif (sample_type & PERF_SAMPLE_IP)\n\t\tsize += sizeof(data->ip);\n\n\tif (sample_type & PERF_SAMPLE_ADDR)\n\t\tsize += sizeof(data->addr);\n\n\tif (sample_type & PERF_SAMPLE_PERIOD)\n\t\tsize += sizeof(data->period);\n\n\tif (sample_type & PERF_SAMPLE_WEIGHT_TYPE)\n\t\tsize += sizeof(data->weight.full);\n\n\tif (sample_type & PERF_SAMPLE_READ)\n\t\tsize += event->read_size;\n\n\tif (sample_type & PERF_SAMPLE_DATA_SRC)\n\t\tsize += sizeof(data->data_src.val);\n\n\tif (sample_type & PERF_SAMPLE_TRANSACTION)\n\t\tsize += sizeof(data->txn);\n\n\tif (sample_type & PERF_SAMPLE_PHYS_ADDR)\n\t\tsize += sizeof(data->phys_addr);\n\n\tif (sample_type & PERF_SAMPLE_CGROUP)\n\t\tsize += sizeof(data->cgroup);\n\n\tif (sample_type & PERF_SAMPLE_DATA_PAGE_SIZE)\n\t\tsize += sizeof(data->data_page_size);\n\n\tif (sample_type & PERF_SAMPLE_CODE_PAGE_SIZE)\n\t\tsize += sizeof(data->code_page_size);\n\n\tevent->header_size = size;\n}\n\n \nstatic void perf_event__header_size(struct perf_event *event)\n{\n\tevent->read_size =\n\t\t__perf_event_read_size(event->attr.read_format,\n\t\t\t\t       event->group_leader->nr_siblings);\n\t__perf_event_header_size(event, event->attr.sample_type);\n}\n\nstatic void perf_event__id_header_size(struct perf_event *event)\n{\n\tstruct perf_sample_data *data;\n\tu64 sample_type = event->attr.sample_type;\n\tu16 size = 0;\n\n\tif (sample_type & PERF_SAMPLE_TID)\n\t\tsize += sizeof(data->tid_entry);\n\n\tif (sample_type & PERF_SAMPLE_TIME)\n\t\tsize += sizeof(data->time);\n\n\tif (sample_type & PERF_SAMPLE_IDENTIFIER)\n\t\tsize += sizeof(data->id);\n\n\tif (sample_type & PERF_SAMPLE_ID)\n\t\tsize += sizeof(data->id);\n\n\tif (sample_type & PERF_SAMPLE_STREAM_ID)\n\t\tsize += sizeof(data->stream_id);\n\n\tif (sample_type & PERF_SAMPLE_CPU)\n\t\tsize += sizeof(data->cpu_entry);\n\n\tevent->id_header_size = size;\n}\n\n \nstatic bool perf_event_validate_size(struct perf_event *event)\n{\n\tstruct perf_event *sibling, *group_leader = event->group_leader;\n\n\tif (__perf_event_read_size(event->attr.read_format,\n\t\t\t\t   group_leader->nr_siblings + 1) > 16*1024)\n\t\treturn false;\n\n\tif (__perf_event_read_size(group_leader->attr.read_format,\n\t\t\t\t   group_leader->nr_siblings + 1) > 16*1024)\n\t\treturn false;\n\n\t \n\tif (event == group_leader)\n\t\treturn true;\n\n\tfor_each_sibling_event(sibling, group_leader) {\n\t\tif (__perf_event_read_size(sibling->attr.read_format,\n\t\t\t\t\t   group_leader->nr_siblings + 1) > 16*1024)\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic void perf_group_attach(struct perf_event *event)\n{\n\tstruct perf_event *group_leader = event->group_leader, *pos;\n\n\tlockdep_assert_held(&event->ctx->lock);\n\n\t \n\tif (event->attach_state & PERF_ATTACH_GROUP)\n\t\treturn;\n\n\tevent->attach_state |= PERF_ATTACH_GROUP;\n\n\tif (group_leader == event)\n\t\treturn;\n\n\tWARN_ON_ONCE(group_leader->ctx != event->ctx);\n\n\tgroup_leader->group_caps &= event->event_caps;\n\n\tlist_add_tail(&event->sibling_list, &group_leader->sibling_list);\n\tgroup_leader->nr_siblings++;\n\tgroup_leader->group_generation++;\n\n\tperf_event__header_size(group_leader);\n\n\tfor_each_sibling_event(pos, group_leader)\n\t\tperf_event__header_size(pos);\n}\n\n \nstatic void\nlist_del_event(struct perf_event *event, struct perf_event_context *ctx)\n{\n\tWARN_ON_ONCE(event->ctx != ctx);\n\tlockdep_assert_held(&ctx->lock);\n\n\t \n\tif (!(event->attach_state & PERF_ATTACH_CONTEXT))\n\t\treturn;\n\n\tevent->attach_state &= ~PERF_ATTACH_CONTEXT;\n\n\tctx->nr_events--;\n\tif (event->hw.flags & PERF_EVENT_FLAG_USER_READ_CNT)\n\t\tctx->nr_user--;\n\tif (event->attr.inherit_stat)\n\t\tctx->nr_stat--;\n\n\tlist_del_rcu(&event->event_entry);\n\n\tif (event->group_leader == event)\n\t\tdel_event_from_groups(event, ctx);\n\n\t \n\tif (event->state > PERF_EVENT_STATE_OFF) {\n\t\tperf_cgroup_event_disable(event, ctx);\n\t\tperf_event_set_state(event, PERF_EVENT_STATE_OFF);\n\t}\n\n\tctx->generation++;\n\tevent->pmu_ctx->nr_events--;\n}\n\nstatic int\nperf_aux_output_match(struct perf_event *event, struct perf_event *aux_event)\n{\n\tif (!has_aux(aux_event))\n\t\treturn 0;\n\n\tif (!event->pmu->aux_output_match)\n\t\treturn 0;\n\n\treturn event->pmu->aux_output_match(aux_event);\n}\n\nstatic void put_event(struct perf_event *event);\nstatic void event_sched_out(struct perf_event *event,\n\t\t\t    struct perf_event_context *ctx);\n\nstatic void perf_put_aux_event(struct perf_event *event)\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct perf_event *iter;\n\n\t \n\tif (event->aux_event) {\n\t\titer = event->aux_event;\n\t\tevent->aux_event = NULL;\n\t\tput_event(iter);\n\t\treturn;\n\t}\n\n\t \n\tfor_each_sibling_event(iter, event->group_leader) {\n\t\tif (iter->aux_event != event)\n\t\t\tcontinue;\n\n\t\titer->aux_event = NULL;\n\t\tput_event(event);\n\n\t\t \n\t\tevent_sched_out(iter, ctx);\n\t\tperf_event_set_state(event, PERF_EVENT_STATE_ERROR);\n\t}\n}\n\nstatic bool perf_need_aux_event(struct perf_event *event)\n{\n\treturn !!event->attr.aux_output || !!event->attr.aux_sample_size;\n}\n\nstatic int perf_get_aux_event(struct perf_event *event,\n\t\t\t      struct perf_event *group_leader)\n{\n\t \n\tif (!group_leader)\n\t\treturn 0;\n\n\t \n\tif (event->attr.aux_output && event->attr.aux_sample_size)\n\t\treturn 0;\n\n\tif (event->attr.aux_output &&\n\t    !perf_aux_output_match(event, group_leader))\n\t\treturn 0;\n\n\tif (event->attr.aux_sample_size && !group_leader->pmu->snapshot_aux)\n\t\treturn 0;\n\n\tif (!atomic_long_inc_not_zero(&group_leader->refcount))\n\t\treturn 0;\n\n\t \n\tevent->aux_event = group_leader;\n\n\treturn 1;\n}\n\nstatic inline struct list_head *get_event_list(struct perf_event *event)\n{\n\treturn event->attr.pinned ? &event->pmu_ctx->pinned_active :\n\t\t\t\t    &event->pmu_ctx->flexible_active;\n}\n\n \nstatic inline void perf_remove_sibling_event(struct perf_event *event)\n{\n\tevent_sched_out(event, event->ctx);\n\tperf_event_set_state(event, PERF_EVENT_STATE_ERROR);\n}\n\nstatic void perf_group_detach(struct perf_event *event)\n{\n\tstruct perf_event *leader = event->group_leader;\n\tstruct perf_event *sibling, *tmp;\n\tstruct perf_event_context *ctx = event->ctx;\n\n\tlockdep_assert_held(&ctx->lock);\n\n\t \n\tif (!(event->attach_state & PERF_ATTACH_GROUP))\n\t\treturn;\n\n\tevent->attach_state &= ~PERF_ATTACH_GROUP;\n\n\tperf_put_aux_event(event);\n\n\t \n\tif (leader != event) {\n\t\tlist_del_init(&event->sibling_list);\n\t\tevent->group_leader->nr_siblings--;\n\t\tevent->group_leader->group_generation++;\n\t\tgoto out;\n\t}\n\n\t \n\tlist_for_each_entry_safe(sibling, tmp, &event->sibling_list, sibling_list) {\n\n\t\tif (sibling->event_caps & PERF_EV_CAP_SIBLING)\n\t\t\tperf_remove_sibling_event(sibling);\n\n\t\tsibling->group_leader = sibling;\n\t\tlist_del_init(&sibling->sibling_list);\n\n\t\t \n\t\tsibling->group_caps = event->group_caps;\n\n\t\tif (sibling->attach_state & PERF_ATTACH_CONTEXT) {\n\t\t\tadd_event_to_groups(sibling, event->ctx);\n\n\t\t\tif (sibling->state == PERF_EVENT_STATE_ACTIVE)\n\t\t\t\tlist_add_tail(&sibling->active_list, get_event_list(sibling));\n\t\t}\n\n\t\tWARN_ON_ONCE(sibling->ctx != event->ctx);\n\t}\n\nout:\n\tfor_each_sibling_event(tmp, leader)\n\t\tperf_event__header_size(tmp);\n\n\tperf_event__header_size(leader);\n}\n\nstatic void sync_child_event(struct perf_event *child_event);\n\nstatic void perf_child_detach(struct perf_event *event)\n{\n\tstruct perf_event *parent_event = event->parent;\n\n\tif (!(event->attach_state & PERF_ATTACH_CHILD))\n\t\treturn;\n\n\tevent->attach_state &= ~PERF_ATTACH_CHILD;\n\n\tif (WARN_ON_ONCE(!parent_event))\n\t\treturn;\n\n\tlockdep_assert_held(&parent_event->child_mutex);\n\n\tsync_child_event(event);\n\tlist_del_init(&event->child_list);\n}\n\nstatic bool is_orphaned_event(struct perf_event *event)\n{\n\treturn event->state == PERF_EVENT_STATE_DEAD;\n}\n\nstatic inline int\nevent_filter_match(struct perf_event *event)\n{\n\treturn (event->cpu == -1 || event->cpu == smp_processor_id()) &&\n\t       perf_cgroup_match(event);\n}\n\nstatic void\nevent_sched_out(struct perf_event *event, struct perf_event_context *ctx)\n{\n\tstruct perf_event_pmu_context *epc = event->pmu_ctx;\n\tstruct perf_cpu_pmu_context *cpc = this_cpu_ptr(epc->pmu->cpu_pmu_context);\n\tenum perf_event_state state = PERF_EVENT_STATE_INACTIVE;\n\n\t\n\n\tWARN_ON_ONCE(event->ctx != ctx);\n\tlockdep_assert_held(&ctx->lock);\n\n\tif (event->state != PERF_EVENT_STATE_ACTIVE)\n\t\treturn;\n\n\t \n\tlist_del_init(&event->active_list);\n\n\tperf_pmu_disable(event->pmu);\n\n\tevent->pmu->del(event, 0);\n\tevent->oncpu = -1;\n\n\tif (event->pending_disable) {\n\t\tevent->pending_disable = 0;\n\t\tperf_cgroup_event_disable(event, ctx);\n\t\tstate = PERF_EVENT_STATE_OFF;\n\t}\n\n\tif (event->pending_sigtrap) {\n\t\tbool dec = true;\n\n\t\tevent->pending_sigtrap = 0;\n\t\tif (state != PERF_EVENT_STATE_OFF &&\n\t\t    !event->pending_work) {\n\t\t\tevent->pending_work = 1;\n\t\t\tdec = false;\n\t\t\tWARN_ON_ONCE(!atomic_long_inc_not_zero(&event->refcount));\n\t\t\ttask_work_add(current, &event->pending_task, TWA_RESUME);\n\t\t}\n\t\tif (dec)\n\t\t\tlocal_dec(&event->ctx->nr_pending);\n\t}\n\n\tperf_event_set_state(event, state);\n\n\tif (!is_software_event(event))\n\t\tcpc->active_oncpu--;\n\tif (event->attr.freq && event->attr.sample_freq)\n\t\tctx->nr_freq--;\n\tif (event->attr.exclusive || !cpc->active_oncpu)\n\t\tcpc->exclusive = 0;\n\n\tperf_pmu_enable(event->pmu);\n}\n\nstatic void\ngroup_sched_out(struct perf_event *group_event, struct perf_event_context *ctx)\n{\n\tstruct perf_event *event;\n\n\tif (group_event->state != PERF_EVENT_STATE_ACTIVE)\n\t\treturn;\n\n\tperf_assert_pmu_disabled(group_event->pmu_ctx->pmu);\n\n\tevent_sched_out(group_event, ctx);\n\n\t \n\tfor_each_sibling_event(event, group_event)\n\t\tevent_sched_out(event, ctx);\n}\n\n#define DETACH_GROUP\t0x01UL\n#define DETACH_CHILD\t0x02UL\n#define DETACH_DEAD\t0x04UL\n\n \nstatic void\n__perf_remove_from_context(struct perf_event *event,\n\t\t\t   struct perf_cpu_context *cpuctx,\n\t\t\t   struct perf_event_context *ctx,\n\t\t\t   void *info)\n{\n\tstruct perf_event_pmu_context *pmu_ctx = event->pmu_ctx;\n\tunsigned long flags = (unsigned long)info;\n\n\tif (ctx->is_active & EVENT_TIME) {\n\t\tupdate_context_time(ctx);\n\t\tupdate_cgrp_time_from_cpuctx(cpuctx, false);\n\t}\n\n\t \n\tif (flags & DETACH_DEAD)\n\t\tevent->pending_disable = 1;\n\tevent_sched_out(event, ctx);\n\tif (flags & DETACH_GROUP)\n\t\tperf_group_detach(event);\n\tif (flags & DETACH_CHILD)\n\t\tperf_child_detach(event);\n\tlist_del_event(event, ctx);\n\tif (flags & DETACH_DEAD)\n\t\tevent->state = PERF_EVENT_STATE_DEAD;\n\n\tif (!pmu_ctx->nr_events) {\n\t\tpmu_ctx->rotate_necessary = 0;\n\n\t\tif (ctx->task && ctx->is_active) {\n\t\t\tstruct perf_cpu_pmu_context *cpc;\n\n\t\t\tcpc = this_cpu_ptr(pmu_ctx->pmu->cpu_pmu_context);\n\t\t\tWARN_ON_ONCE(cpc->task_epc && cpc->task_epc != pmu_ctx);\n\t\t\tcpc->task_epc = NULL;\n\t\t}\n\t}\n\n\tif (!ctx->nr_events && ctx->is_active) {\n\t\tif (ctx == &cpuctx->ctx)\n\t\t\tupdate_cgrp_time_from_cpuctx(cpuctx, true);\n\n\t\tctx->is_active = 0;\n\t\tif (ctx->task) {\n\t\t\tWARN_ON_ONCE(cpuctx->task_ctx != ctx);\n\t\t\tcpuctx->task_ctx = NULL;\n\t\t}\n\t}\n}\n\n \nstatic void perf_remove_from_context(struct perf_event *event, unsigned long flags)\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\n\tlockdep_assert_held(&ctx->mutex);\n\n\t \n\traw_spin_lock_irq(&ctx->lock);\n\tif (!ctx->is_active) {\n\t\t__perf_remove_from_context(event, this_cpu_ptr(&perf_cpu_context),\n\t\t\t\t\t   ctx, (void *)flags);\n\t\traw_spin_unlock_irq(&ctx->lock);\n\t\treturn;\n\t}\n\traw_spin_unlock_irq(&ctx->lock);\n\n\tevent_function_call(event, __perf_remove_from_context, (void *)flags);\n}\n\n \nstatic void __perf_event_disable(struct perf_event *event,\n\t\t\t\t struct perf_cpu_context *cpuctx,\n\t\t\t\t struct perf_event_context *ctx,\n\t\t\t\t void *info)\n{\n\tif (event->state < PERF_EVENT_STATE_INACTIVE)\n\t\treturn;\n\n\tif (ctx->is_active & EVENT_TIME) {\n\t\tupdate_context_time(ctx);\n\t\tupdate_cgrp_time_from_event(event);\n\t}\n\n\tperf_pmu_disable(event->pmu_ctx->pmu);\n\n\tif (event == event->group_leader)\n\t\tgroup_sched_out(event, ctx);\n\telse\n\t\tevent_sched_out(event, ctx);\n\n\tperf_event_set_state(event, PERF_EVENT_STATE_OFF);\n\tperf_cgroup_event_disable(event, ctx);\n\n\tperf_pmu_enable(event->pmu_ctx->pmu);\n}\n\n \nstatic void _perf_event_disable(struct perf_event *event)\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\n\traw_spin_lock_irq(&ctx->lock);\n\tif (event->state <= PERF_EVENT_STATE_OFF) {\n\t\traw_spin_unlock_irq(&ctx->lock);\n\t\treturn;\n\t}\n\traw_spin_unlock_irq(&ctx->lock);\n\n\tevent_function_call(event, __perf_event_disable, NULL);\n}\n\nvoid perf_event_disable_local(struct perf_event *event)\n{\n\tevent_function_local(event, __perf_event_disable, NULL);\n}\n\n \nvoid perf_event_disable(struct perf_event *event)\n{\n\tstruct perf_event_context *ctx;\n\n\tctx = perf_event_ctx_lock(event);\n\t_perf_event_disable(event);\n\tperf_event_ctx_unlock(event, ctx);\n}\nEXPORT_SYMBOL_GPL(perf_event_disable);\n\nvoid perf_event_disable_inatomic(struct perf_event *event)\n{\n\tevent->pending_disable = 1;\n\tirq_work_queue(&event->pending_irq);\n}\n\n#define MAX_INTERRUPTS (~0ULL)\n\nstatic void perf_log_throttle(struct perf_event *event, int enable);\nstatic void perf_log_itrace_start(struct perf_event *event);\n\nstatic int\nevent_sched_in(struct perf_event *event, struct perf_event_context *ctx)\n{\n\tstruct perf_event_pmu_context *epc = event->pmu_ctx;\n\tstruct perf_cpu_pmu_context *cpc = this_cpu_ptr(epc->pmu->cpu_pmu_context);\n\tint ret = 0;\n\n\tWARN_ON_ONCE(event->ctx != ctx);\n\n\tlockdep_assert_held(&ctx->lock);\n\n\tif (event->state <= PERF_EVENT_STATE_OFF)\n\t\treturn 0;\n\n\tWRITE_ONCE(event->oncpu, smp_processor_id());\n\t \n\tsmp_wmb();\n\tperf_event_set_state(event, PERF_EVENT_STATE_ACTIVE);\n\n\t \n\tif (unlikely(event->hw.interrupts == MAX_INTERRUPTS)) {\n\t\tperf_log_throttle(event, 1);\n\t\tevent->hw.interrupts = 0;\n\t}\n\n\tperf_pmu_disable(event->pmu);\n\n\tperf_log_itrace_start(event);\n\n\tif (event->pmu->add(event, PERF_EF_START)) {\n\t\tperf_event_set_state(event, PERF_EVENT_STATE_INACTIVE);\n\t\tevent->oncpu = -1;\n\t\tret = -EAGAIN;\n\t\tgoto out;\n\t}\n\n\tif (!is_software_event(event))\n\t\tcpc->active_oncpu++;\n\tif (event->attr.freq && event->attr.sample_freq)\n\t\tctx->nr_freq++;\n\n\tif (event->attr.exclusive)\n\t\tcpc->exclusive = 1;\n\nout:\n\tperf_pmu_enable(event->pmu);\n\n\treturn ret;\n}\n\nstatic int\ngroup_sched_in(struct perf_event *group_event, struct perf_event_context *ctx)\n{\n\tstruct perf_event *event, *partial_group = NULL;\n\tstruct pmu *pmu = group_event->pmu_ctx->pmu;\n\n\tif (group_event->state == PERF_EVENT_STATE_OFF)\n\t\treturn 0;\n\n\tpmu->start_txn(pmu, PERF_PMU_TXN_ADD);\n\n\tif (event_sched_in(group_event, ctx))\n\t\tgoto error;\n\n\t \n\tfor_each_sibling_event(event, group_event) {\n\t\tif (event_sched_in(event, ctx)) {\n\t\t\tpartial_group = event;\n\t\t\tgoto group_error;\n\t\t}\n\t}\n\n\tif (!pmu->commit_txn(pmu))\n\t\treturn 0;\n\ngroup_error:\n\t \n\tfor_each_sibling_event(event, group_event) {\n\t\tif (event == partial_group)\n\t\t\tbreak;\n\n\t\tevent_sched_out(event, ctx);\n\t}\n\tevent_sched_out(group_event, ctx);\n\nerror:\n\tpmu->cancel_txn(pmu);\n\treturn -EAGAIN;\n}\n\n \nstatic int group_can_go_on(struct perf_event *event, int can_add_hw)\n{\n\tstruct perf_event_pmu_context *epc = event->pmu_ctx;\n\tstruct perf_cpu_pmu_context *cpc = this_cpu_ptr(epc->pmu->cpu_pmu_context);\n\n\t \n\tif (event->group_caps & PERF_EV_CAP_SOFTWARE)\n\t\treturn 1;\n\t \n\tif (cpc->exclusive)\n\t\treturn 0;\n\t \n\tif (event->attr.exclusive && !list_empty(get_event_list(event)))\n\t\treturn 0;\n\t \n\treturn can_add_hw;\n}\n\nstatic void add_event_to_ctx(struct perf_event *event,\n\t\t\t       struct perf_event_context *ctx)\n{\n\tlist_add_event(event, ctx);\n\tperf_group_attach(event);\n}\n\nstatic void task_ctx_sched_out(struct perf_event_context *ctx,\n\t\t\t\tenum event_type_t event_type)\n{\n\tstruct perf_cpu_context *cpuctx = this_cpu_ptr(&perf_cpu_context);\n\n\tif (!cpuctx->task_ctx)\n\t\treturn;\n\n\tif (WARN_ON_ONCE(ctx != cpuctx->task_ctx))\n\t\treturn;\n\n\tctx_sched_out(ctx, event_type);\n}\n\nstatic void perf_event_sched_in(struct perf_cpu_context *cpuctx,\n\t\t\t\tstruct perf_event_context *ctx)\n{\n\tctx_sched_in(&cpuctx->ctx, EVENT_PINNED);\n\tif (ctx)\n\t\t ctx_sched_in(ctx, EVENT_PINNED);\n\tctx_sched_in(&cpuctx->ctx, EVENT_FLEXIBLE);\n\tif (ctx)\n\t\t ctx_sched_in(ctx, EVENT_FLEXIBLE);\n}\n\n \n \nstatic void ctx_resched(struct perf_cpu_context *cpuctx,\n\t\t\tstruct perf_event_context *task_ctx,\n\t\t\tenum event_type_t event_type)\n{\n\tbool cpu_event = !!(event_type & EVENT_CPU);\n\n\t \n\tif (event_type & EVENT_PINNED)\n\t\tevent_type |= EVENT_FLEXIBLE;\n\n\tevent_type &= EVENT_ALL;\n\n\tperf_ctx_disable(&cpuctx->ctx, false);\n\tif (task_ctx) {\n\t\tperf_ctx_disable(task_ctx, false);\n\t\ttask_ctx_sched_out(task_ctx, event_type);\n\t}\n\n\t \n\tif (cpu_event)\n\t\tctx_sched_out(&cpuctx->ctx, event_type);\n\telse if (event_type & EVENT_PINNED)\n\t\tctx_sched_out(&cpuctx->ctx, EVENT_FLEXIBLE);\n\n\tperf_event_sched_in(cpuctx, task_ctx);\n\n\tperf_ctx_enable(&cpuctx->ctx, false);\n\tif (task_ctx)\n\t\tperf_ctx_enable(task_ctx, false);\n}\n\nvoid perf_pmu_resched(struct pmu *pmu)\n{\n\tstruct perf_cpu_context *cpuctx = this_cpu_ptr(&perf_cpu_context);\n\tstruct perf_event_context *task_ctx = cpuctx->task_ctx;\n\n\tperf_ctx_lock(cpuctx, task_ctx);\n\tctx_resched(cpuctx, task_ctx, EVENT_ALL|EVENT_CPU);\n\tperf_ctx_unlock(cpuctx, task_ctx);\n}\n\n \nstatic int  __perf_install_in_context(void *info)\n{\n\tstruct perf_event *event = info;\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct perf_cpu_context *cpuctx = this_cpu_ptr(&perf_cpu_context);\n\tstruct perf_event_context *task_ctx = cpuctx->task_ctx;\n\tbool reprogram = true;\n\tint ret = 0;\n\n\traw_spin_lock(&cpuctx->ctx.lock);\n\tif (ctx->task) {\n\t\traw_spin_lock(&ctx->lock);\n\t\ttask_ctx = ctx;\n\n\t\treprogram = (ctx->task == current);\n\n\t\t \n\t\tif (task_curr(ctx->task) && !reprogram) {\n\t\t\tret = -ESRCH;\n\t\t\tgoto unlock;\n\t\t}\n\n\t\tWARN_ON_ONCE(reprogram && cpuctx->task_ctx && cpuctx->task_ctx != ctx);\n\t} else if (task_ctx) {\n\t\traw_spin_lock(&task_ctx->lock);\n\t}\n\n#ifdef CONFIG_CGROUP_PERF\n\tif (event->state > PERF_EVENT_STATE_OFF && is_cgroup_event(event)) {\n\t\t \n\t\tstruct perf_cgroup *cgrp = perf_cgroup_from_task(current, ctx);\n\t\treprogram = cgroup_is_descendant(cgrp->css.cgroup,\n\t\t\t\t\tevent->cgrp->css.cgroup);\n\t}\n#endif\n\n\tif (reprogram) {\n\t\tctx_sched_out(ctx, EVENT_TIME);\n\t\tadd_event_to_ctx(event, ctx);\n\t\tctx_resched(cpuctx, task_ctx, get_event_type(event));\n\t} else {\n\t\tadd_event_to_ctx(event, ctx);\n\t}\n\nunlock:\n\tperf_ctx_unlock(cpuctx, task_ctx);\n\n\treturn ret;\n}\n\nstatic bool exclusive_event_installable(struct perf_event *event,\n\t\t\t\t\tstruct perf_event_context *ctx);\n\n \nstatic void\nperf_install_in_context(struct perf_event_context *ctx,\n\t\t\tstruct perf_event *event,\n\t\t\tint cpu)\n{\n\tstruct task_struct *task = READ_ONCE(ctx->task);\n\n\tlockdep_assert_held(&ctx->mutex);\n\n\tWARN_ON_ONCE(!exclusive_event_installable(event, ctx));\n\n\tif (event->cpu != -1)\n\t\tWARN_ON_ONCE(event->cpu != cpu);\n\n\t \n\tsmp_store_release(&event->ctx, ctx);\n\n\t \n\tif (__perf_effective_state(event) == PERF_EVENT_STATE_OFF &&\n\t    ctx->nr_events && !is_cgroup_event(event)) {\n\t\traw_spin_lock_irq(&ctx->lock);\n\t\tif (ctx->task == TASK_TOMBSTONE) {\n\t\t\traw_spin_unlock_irq(&ctx->lock);\n\t\t\treturn;\n\t\t}\n\t\tadd_event_to_ctx(event, ctx);\n\t\traw_spin_unlock_irq(&ctx->lock);\n\t\treturn;\n\t}\n\n\tif (!task) {\n\t\tcpu_function_call(cpu, __perf_install_in_context, event);\n\t\treturn;\n\t}\n\n\t \n\tif (WARN_ON_ONCE(task == TASK_TOMBSTONE))\n\t\treturn;\n\n\t \n\n\t \n\tsmp_mb();\nagain:\n\tif (!task_function_call(task, __perf_install_in_context, event))\n\t\treturn;\n\n\traw_spin_lock_irq(&ctx->lock);\n\ttask = ctx->task;\n\tif (WARN_ON_ONCE(task == TASK_TOMBSTONE)) {\n\t\t \n\t\traw_spin_unlock_irq(&ctx->lock);\n\t\treturn;\n\t}\n\t \n\tif (task_curr(task)) {\n\t\traw_spin_unlock_irq(&ctx->lock);\n\t\tgoto again;\n\t}\n\tadd_event_to_ctx(event, ctx);\n\traw_spin_unlock_irq(&ctx->lock);\n}\n\n \nstatic void __perf_event_enable(struct perf_event *event,\n\t\t\t\tstruct perf_cpu_context *cpuctx,\n\t\t\t\tstruct perf_event_context *ctx,\n\t\t\t\tvoid *info)\n{\n\tstruct perf_event *leader = event->group_leader;\n\tstruct perf_event_context *task_ctx;\n\n\tif (event->state >= PERF_EVENT_STATE_INACTIVE ||\n\t    event->state <= PERF_EVENT_STATE_ERROR)\n\t\treturn;\n\n\tif (ctx->is_active)\n\t\tctx_sched_out(ctx, EVENT_TIME);\n\n\tperf_event_set_state(event, PERF_EVENT_STATE_INACTIVE);\n\tperf_cgroup_event_enable(event, ctx);\n\n\tif (!ctx->is_active)\n\t\treturn;\n\n\tif (!event_filter_match(event)) {\n\t\tctx_sched_in(ctx, EVENT_TIME);\n\t\treturn;\n\t}\n\n\t \n\tif (leader != event && leader->state != PERF_EVENT_STATE_ACTIVE) {\n\t\tctx_sched_in(ctx, EVENT_TIME);\n\t\treturn;\n\t}\n\n\ttask_ctx = cpuctx->task_ctx;\n\tif (ctx->task)\n\t\tWARN_ON_ONCE(task_ctx != ctx);\n\n\tctx_resched(cpuctx, task_ctx, get_event_type(event));\n}\n\n \nstatic void _perf_event_enable(struct perf_event *event)\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\n\traw_spin_lock_irq(&ctx->lock);\n\tif (event->state >= PERF_EVENT_STATE_INACTIVE ||\n\t    event->state <  PERF_EVENT_STATE_ERROR) {\nout:\n\t\traw_spin_unlock_irq(&ctx->lock);\n\t\treturn;\n\t}\n\n\t \n\tif (event->state == PERF_EVENT_STATE_ERROR) {\n\t\t \n\t\tif (event->event_caps & PERF_EV_CAP_SIBLING &&\n\t\t    event->group_leader == event)\n\t\t\tgoto out;\n\n\t\tevent->state = PERF_EVENT_STATE_OFF;\n\t}\n\traw_spin_unlock_irq(&ctx->lock);\n\n\tevent_function_call(event, __perf_event_enable, NULL);\n}\n\n \nvoid perf_event_enable(struct perf_event *event)\n{\n\tstruct perf_event_context *ctx;\n\n\tctx = perf_event_ctx_lock(event);\n\t_perf_event_enable(event);\n\tperf_event_ctx_unlock(event, ctx);\n}\nEXPORT_SYMBOL_GPL(perf_event_enable);\n\nstruct stop_event_data {\n\tstruct perf_event\t*event;\n\tunsigned int\t\trestart;\n};\n\nstatic int __perf_event_stop(void *info)\n{\n\tstruct stop_event_data *sd = info;\n\tstruct perf_event *event = sd->event;\n\n\t \n\tif (READ_ONCE(event->state) != PERF_EVENT_STATE_ACTIVE)\n\t\treturn 0;\n\n\t \n\tsmp_rmb();\n\n\t \n\tif (READ_ONCE(event->oncpu) != smp_processor_id())\n\t\treturn -EAGAIN;\n\n\tevent->pmu->stop(event, PERF_EF_UPDATE);\n\n\t \n\tif (sd->restart)\n\t\tevent->pmu->start(event, 0);\n\n\treturn 0;\n}\n\nstatic int perf_event_stop(struct perf_event *event, int restart)\n{\n\tstruct stop_event_data sd = {\n\t\t.event\t\t= event,\n\t\t.restart\t= restart,\n\t};\n\tint ret = 0;\n\n\tdo {\n\t\tif (READ_ONCE(event->state) != PERF_EVENT_STATE_ACTIVE)\n\t\t\treturn 0;\n\n\t\t \n\t\tsmp_rmb();\n\n\t\t \n\t\tret = cpu_function_call(READ_ONCE(event->oncpu),\n\t\t\t\t\t__perf_event_stop, &sd);\n\t} while (ret == -EAGAIN);\n\n\treturn ret;\n}\n\n \nvoid perf_event_addr_filters_sync(struct perf_event *event)\n{\n\tstruct perf_addr_filters_head *ifh = perf_event_addr_filters(event);\n\n\tif (!has_addr_filter(event))\n\t\treturn;\n\n\traw_spin_lock(&ifh->lock);\n\tif (event->addr_filters_gen != event->hw.addr_filters_gen) {\n\t\tevent->pmu->addr_filters_sync(event);\n\t\tevent->hw.addr_filters_gen = event->addr_filters_gen;\n\t}\n\traw_spin_unlock(&ifh->lock);\n}\nEXPORT_SYMBOL_GPL(perf_event_addr_filters_sync);\n\nstatic int _perf_event_refresh(struct perf_event *event, int refresh)\n{\n\t \n\tif (event->attr.inherit || !is_sampling_event(event))\n\t\treturn -EINVAL;\n\n\tatomic_add(refresh, &event->event_limit);\n\t_perf_event_enable(event);\n\n\treturn 0;\n}\n\n \nint perf_event_refresh(struct perf_event *event, int refresh)\n{\n\tstruct perf_event_context *ctx;\n\tint ret;\n\n\tctx = perf_event_ctx_lock(event);\n\tret = _perf_event_refresh(event, refresh);\n\tperf_event_ctx_unlock(event, ctx);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(perf_event_refresh);\n\nstatic int perf_event_modify_breakpoint(struct perf_event *bp,\n\t\t\t\t\t struct perf_event_attr *attr)\n{\n\tint err;\n\n\t_perf_event_disable(bp);\n\n\terr = modify_user_hw_breakpoint_check(bp, attr, true);\n\n\tif (!bp->attr.disabled)\n\t\t_perf_event_enable(bp);\n\n\treturn err;\n}\n\n \nstatic void perf_event_modify_copy_attr(struct perf_event_attr *to,\n\t\t\t\t\tconst struct perf_event_attr *from)\n{\n\tto->sig_data = from->sig_data;\n}\n\nstatic int perf_event_modify_attr(struct perf_event *event,\n\t\t\t\t  struct perf_event_attr *attr)\n{\n\tint (*func)(struct perf_event *, struct perf_event_attr *);\n\tstruct perf_event *child;\n\tint err;\n\n\tif (event->attr.type != attr->type)\n\t\treturn -EINVAL;\n\n\tswitch (event->attr.type) {\n\tcase PERF_TYPE_BREAKPOINT:\n\t\tfunc = perf_event_modify_breakpoint;\n\t\tbreak;\n\tdefault:\n\t\t \n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tWARN_ON_ONCE(event->ctx->parent_ctx);\n\n\tmutex_lock(&event->child_mutex);\n\t \n\tperf_event_modify_copy_attr(&event->attr, attr);\n\terr = func(event, attr);\n\tif (err)\n\t\tgoto out;\n\tlist_for_each_entry(child, &event->child_list, child_list) {\n\t\tperf_event_modify_copy_attr(&child->attr, attr);\n\t\terr = func(child, attr);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\nout:\n\tmutex_unlock(&event->child_mutex);\n\treturn err;\n}\n\nstatic void __pmu_ctx_sched_out(struct perf_event_pmu_context *pmu_ctx,\n\t\t\t\tenum event_type_t event_type)\n{\n\tstruct perf_event_context *ctx = pmu_ctx->ctx;\n\tstruct perf_event *event, *tmp;\n\tstruct pmu *pmu = pmu_ctx->pmu;\n\n\tif (ctx->task && !ctx->is_active) {\n\t\tstruct perf_cpu_pmu_context *cpc;\n\n\t\tcpc = this_cpu_ptr(pmu->cpu_pmu_context);\n\t\tWARN_ON_ONCE(cpc->task_epc && cpc->task_epc != pmu_ctx);\n\t\tcpc->task_epc = NULL;\n\t}\n\n\tif (!event_type)\n\t\treturn;\n\n\tperf_pmu_disable(pmu);\n\tif (event_type & EVENT_PINNED) {\n\t\tlist_for_each_entry_safe(event, tmp,\n\t\t\t\t\t &pmu_ctx->pinned_active,\n\t\t\t\t\t active_list)\n\t\t\tgroup_sched_out(event, ctx);\n\t}\n\n\tif (event_type & EVENT_FLEXIBLE) {\n\t\tlist_for_each_entry_safe(event, tmp,\n\t\t\t\t\t &pmu_ctx->flexible_active,\n\t\t\t\t\t active_list)\n\t\t\tgroup_sched_out(event, ctx);\n\t\t \n\t\tpmu_ctx->rotate_necessary = 0;\n\t}\n\tperf_pmu_enable(pmu);\n}\n\nstatic void\nctx_sched_out(struct perf_event_context *ctx, enum event_type_t event_type)\n{\n\tstruct perf_cpu_context *cpuctx = this_cpu_ptr(&perf_cpu_context);\n\tstruct perf_event_pmu_context *pmu_ctx;\n\tint is_active = ctx->is_active;\n\tbool cgroup = event_type & EVENT_CGROUP;\n\n\tevent_type &= ~EVENT_CGROUP;\n\n\tlockdep_assert_held(&ctx->lock);\n\n\tif (likely(!ctx->nr_events)) {\n\t\t \n\t\tWARN_ON_ONCE(ctx->is_active);\n\t\tif (ctx->task)\n\t\t\tWARN_ON_ONCE(cpuctx->task_ctx);\n\t\treturn;\n\t}\n\n\t \n\tif (is_active & EVENT_TIME) {\n\t\t \n\t\tupdate_context_time(ctx);\n\t\tupdate_cgrp_time_from_cpuctx(cpuctx, ctx == &cpuctx->ctx);\n\t\t \n\t\tbarrier();\n\t}\n\n\tctx->is_active &= ~event_type;\n\tif (!(ctx->is_active & EVENT_ALL))\n\t\tctx->is_active = 0;\n\n\tif (ctx->task) {\n\t\tWARN_ON_ONCE(cpuctx->task_ctx != ctx);\n\t\tif (!ctx->is_active)\n\t\t\tcpuctx->task_ctx = NULL;\n\t}\n\n\tis_active ^= ctx->is_active;  \n\n\tlist_for_each_entry(pmu_ctx, &ctx->pmu_ctx_list, pmu_ctx_entry) {\n\t\tif (cgroup && !pmu_ctx->nr_cgroups)\n\t\t\tcontinue;\n\t\t__pmu_ctx_sched_out(pmu_ctx, is_active);\n\t}\n}\n\n \nstatic int context_equiv(struct perf_event_context *ctx1,\n\t\t\t struct perf_event_context *ctx2)\n{\n\tlockdep_assert_held(&ctx1->lock);\n\tlockdep_assert_held(&ctx2->lock);\n\n\t \n\tif (ctx1->pin_count || ctx2->pin_count)\n\t\treturn 0;\n\n\t \n\tif (ctx1 == ctx2->parent_ctx && ctx1->generation == ctx2->parent_gen)\n\t\treturn 1;\n\n\t \n\tif (ctx1->parent_ctx == ctx2 && ctx1->parent_gen == ctx2->generation)\n\t\treturn 1;\n\n\t \n\tif (ctx1->parent_ctx && ctx1->parent_ctx == ctx2->parent_ctx &&\n\t\t\tctx1->parent_gen == ctx2->parent_gen)\n\t\treturn 1;\n\n\t \n\treturn 0;\n}\n\nstatic void __perf_event_sync_stat(struct perf_event *event,\n\t\t\t\t     struct perf_event *next_event)\n{\n\tu64 value;\n\n\tif (!event->attr.inherit_stat)\n\t\treturn;\n\n\t \n\tif (event->state == PERF_EVENT_STATE_ACTIVE)\n\t\tevent->pmu->read(event);\n\n\tperf_event_update_time(event);\n\n\t \n\tvalue = local64_read(&next_event->count);\n\tvalue = local64_xchg(&event->count, value);\n\tlocal64_set(&next_event->count, value);\n\n\tswap(event->total_time_enabled, next_event->total_time_enabled);\n\tswap(event->total_time_running, next_event->total_time_running);\n\n\t \n\tperf_event_update_userpage(event);\n\tperf_event_update_userpage(next_event);\n}\n\nstatic void perf_event_sync_stat(struct perf_event_context *ctx,\n\t\t\t\t   struct perf_event_context *next_ctx)\n{\n\tstruct perf_event *event, *next_event;\n\n\tif (!ctx->nr_stat)\n\t\treturn;\n\n\tupdate_context_time(ctx);\n\n\tevent = list_first_entry(&ctx->event_list,\n\t\t\t\t   struct perf_event, event_entry);\n\n\tnext_event = list_first_entry(&next_ctx->event_list,\n\t\t\t\t\tstruct perf_event, event_entry);\n\n\twhile (&event->event_entry != &ctx->event_list &&\n\t       &next_event->event_entry != &next_ctx->event_list) {\n\n\t\t__perf_event_sync_stat(event, next_event);\n\n\t\tevent = list_next_entry(event, event_entry);\n\t\tnext_event = list_next_entry(next_event, event_entry);\n\t}\n}\n\n#define double_list_for_each_entry(pos1, pos2, head1, head2, member)\t\\\n\tfor (pos1 = list_first_entry(head1, typeof(*pos1), member),\t\\\n\t     pos2 = list_first_entry(head2, typeof(*pos2), member);\t\\\n\t     !list_entry_is_head(pos1, head1, member) &&\t\t\\\n\t     !list_entry_is_head(pos2, head2, member);\t\t\t\\\n\t     pos1 = list_next_entry(pos1, member),\t\t\t\\\n\t     pos2 = list_next_entry(pos2, member))\n\nstatic void perf_event_swap_task_ctx_data(struct perf_event_context *prev_ctx,\n\t\t\t\t\t  struct perf_event_context *next_ctx)\n{\n\tstruct perf_event_pmu_context *prev_epc, *next_epc;\n\n\tif (!prev_ctx->nr_task_data)\n\t\treturn;\n\n\tdouble_list_for_each_entry(prev_epc, next_epc,\n\t\t\t\t   &prev_ctx->pmu_ctx_list, &next_ctx->pmu_ctx_list,\n\t\t\t\t   pmu_ctx_entry) {\n\n\t\tif (WARN_ON_ONCE(prev_epc->pmu != next_epc->pmu))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (prev_epc->pmu->swap_task_ctx)\n\t\t\tprev_epc->pmu->swap_task_ctx(prev_epc, next_epc);\n\t\telse\n\t\t\tswap(prev_epc->task_ctx_data, next_epc->task_ctx_data);\n\t}\n}\n\nstatic void perf_ctx_sched_task_cb(struct perf_event_context *ctx, bool sched_in)\n{\n\tstruct perf_event_pmu_context *pmu_ctx;\n\tstruct perf_cpu_pmu_context *cpc;\n\n\tlist_for_each_entry(pmu_ctx, &ctx->pmu_ctx_list, pmu_ctx_entry) {\n\t\tcpc = this_cpu_ptr(pmu_ctx->pmu->cpu_pmu_context);\n\n\t\tif (cpc->sched_cb_usage && pmu_ctx->pmu->sched_task)\n\t\t\tpmu_ctx->pmu->sched_task(pmu_ctx, sched_in);\n\t}\n}\n\nstatic void\nperf_event_context_sched_out(struct task_struct *task, struct task_struct *next)\n{\n\tstruct perf_event_context *ctx = task->perf_event_ctxp;\n\tstruct perf_event_context *next_ctx;\n\tstruct perf_event_context *parent, *next_parent;\n\tint do_switch = 1;\n\n\tif (likely(!ctx))\n\t\treturn;\n\n\trcu_read_lock();\n\tnext_ctx = rcu_dereference(next->perf_event_ctxp);\n\tif (!next_ctx)\n\t\tgoto unlock;\n\n\tparent = rcu_dereference(ctx->parent_ctx);\n\tnext_parent = rcu_dereference(next_ctx->parent_ctx);\n\n\t \n\tif (!parent && !next_parent)\n\t\tgoto unlock;\n\n\tif (next_parent == ctx || next_ctx == parent || next_parent == parent) {\n\t\t \n\t\traw_spin_lock(&ctx->lock);\n\t\traw_spin_lock_nested(&next_ctx->lock, SINGLE_DEPTH_NESTING);\n\t\tif (context_equiv(ctx, next_ctx)) {\n\n\t\t\tperf_ctx_disable(ctx, false);\n\n\t\t\t \n\t\t\tif (local_read(&ctx->nr_pending) ||\n\t\t\t    local_read(&next_ctx->nr_pending)) {\n\t\t\t\t \n\t\t\t\traw_spin_unlock(&next_ctx->lock);\n\t\t\t\trcu_read_unlock();\n\t\t\t\tgoto inside_switch;\n\t\t\t}\n\n\t\t\tWRITE_ONCE(ctx->task, next);\n\t\t\tWRITE_ONCE(next_ctx->task, task);\n\n\t\t\tperf_ctx_sched_task_cb(ctx, false);\n\t\t\tperf_event_swap_task_ctx_data(ctx, next_ctx);\n\n\t\t\tperf_ctx_enable(ctx, false);\n\n\t\t\t \n\t\t\tRCU_INIT_POINTER(task->perf_event_ctxp, next_ctx);\n\t\t\tRCU_INIT_POINTER(next->perf_event_ctxp, ctx);\n\n\t\t\tdo_switch = 0;\n\n\t\t\tperf_event_sync_stat(ctx, next_ctx);\n\t\t}\n\t\traw_spin_unlock(&next_ctx->lock);\n\t\traw_spin_unlock(&ctx->lock);\n\t}\nunlock:\n\trcu_read_unlock();\n\n\tif (do_switch) {\n\t\traw_spin_lock(&ctx->lock);\n\t\tperf_ctx_disable(ctx, false);\n\ninside_switch:\n\t\tperf_ctx_sched_task_cb(ctx, false);\n\t\ttask_ctx_sched_out(ctx, EVENT_ALL);\n\n\t\tperf_ctx_enable(ctx, false);\n\t\traw_spin_unlock(&ctx->lock);\n\t}\n}\n\nstatic DEFINE_PER_CPU(struct list_head, sched_cb_list);\nstatic DEFINE_PER_CPU(int, perf_sched_cb_usages);\n\nvoid perf_sched_cb_dec(struct pmu *pmu)\n{\n\tstruct perf_cpu_pmu_context *cpc = this_cpu_ptr(pmu->cpu_pmu_context);\n\n\tthis_cpu_dec(perf_sched_cb_usages);\n\tbarrier();\n\n\tif (!--cpc->sched_cb_usage)\n\t\tlist_del(&cpc->sched_cb_entry);\n}\n\n\nvoid perf_sched_cb_inc(struct pmu *pmu)\n{\n\tstruct perf_cpu_pmu_context *cpc = this_cpu_ptr(pmu->cpu_pmu_context);\n\n\tif (!cpc->sched_cb_usage++)\n\t\tlist_add(&cpc->sched_cb_entry, this_cpu_ptr(&sched_cb_list));\n\n\tbarrier();\n\tthis_cpu_inc(perf_sched_cb_usages);\n}\n\n \nstatic void __perf_pmu_sched_task(struct perf_cpu_pmu_context *cpc, bool sched_in)\n{\n\tstruct perf_cpu_context *cpuctx = this_cpu_ptr(&perf_cpu_context);\n\tstruct pmu *pmu;\n\n\tpmu = cpc->epc.pmu;\n\n\t \n\tif (WARN_ON_ONCE(!pmu->sched_task))\n\t\treturn;\n\n\tperf_ctx_lock(cpuctx, cpuctx->task_ctx);\n\tperf_pmu_disable(pmu);\n\n\tpmu->sched_task(cpc->task_epc, sched_in);\n\n\tperf_pmu_enable(pmu);\n\tperf_ctx_unlock(cpuctx, cpuctx->task_ctx);\n}\n\nstatic void perf_pmu_sched_task(struct task_struct *prev,\n\t\t\t\tstruct task_struct *next,\n\t\t\t\tbool sched_in)\n{\n\tstruct perf_cpu_context *cpuctx = this_cpu_ptr(&perf_cpu_context);\n\tstruct perf_cpu_pmu_context *cpc;\n\n\t \n\tif (prev == next || cpuctx->task_ctx)\n\t\treturn;\n\n\tlist_for_each_entry(cpc, this_cpu_ptr(&sched_cb_list), sched_cb_entry)\n\t\t__perf_pmu_sched_task(cpc, sched_in);\n}\n\nstatic void perf_event_switch(struct task_struct *task,\n\t\t\t      struct task_struct *next_prev, bool sched_in);\n\n \nvoid __perf_event_task_sched_out(struct task_struct *task,\n\t\t\t\t struct task_struct *next)\n{\n\tif (__this_cpu_read(perf_sched_cb_usages))\n\t\tperf_pmu_sched_task(task, next, false);\n\n\tif (atomic_read(&nr_switch_events))\n\t\tperf_event_switch(task, next, false);\n\n\tperf_event_context_sched_out(task, next);\n\n\t \n\tperf_cgroup_switch(next);\n}\n\nstatic bool perf_less_group_idx(const void *l, const void *r)\n{\n\tconst struct perf_event *le = *(const struct perf_event **)l;\n\tconst struct perf_event *re = *(const struct perf_event **)r;\n\n\treturn le->group_index < re->group_index;\n}\n\nstatic void swap_ptr(void *l, void *r)\n{\n\tvoid **lp = l, **rp = r;\n\n\tswap(*lp, *rp);\n}\n\nstatic const struct min_heap_callbacks perf_min_heap = {\n\t.elem_size = sizeof(struct perf_event *),\n\t.less = perf_less_group_idx,\n\t.swp = swap_ptr,\n};\n\nstatic void __heap_add(struct min_heap *heap, struct perf_event *event)\n{\n\tstruct perf_event **itrs = heap->data;\n\n\tif (event) {\n\t\titrs[heap->nr] = event;\n\t\theap->nr++;\n\t}\n}\n\nstatic void __link_epc(struct perf_event_pmu_context *pmu_ctx)\n{\n\tstruct perf_cpu_pmu_context *cpc;\n\n\tif (!pmu_ctx->ctx->task)\n\t\treturn;\n\n\tcpc = this_cpu_ptr(pmu_ctx->pmu->cpu_pmu_context);\n\tWARN_ON_ONCE(cpc->task_epc && cpc->task_epc != pmu_ctx);\n\tcpc->task_epc = pmu_ctx;\n}\n\nstatic noinline int visit_groups_merge(struct perf_event_context *ctx,\n\t\t\t\tstruct perf_event_groups *groups, int cpu,\n\t\t\t\tstruct pmu *pmu,\n\t\t\t\tint (*func)(struct perf_event *, void *),\n\t\t\t\tvoid *data)\n{\n#ifdef CONFIG_CGROUP_PERF\n\tstruct cgroup_subsys_state *css = NULL;\n#endif\n\tstruct perf_cpu_context *cpuctx = NULL;\n\t \n\tstruct perf_event *itrs[2];\n\tstruct min_heap event_heap;\n\tstruct perf_event **evt;\n\tint ret;\n\n\tif (pmu->filter && pmu->filter(pmu, cpu))\n\t\treturn 0;\n\n\tif (!ctx->task) {\n\t\tcpuctx = this_cpu_ptr(&perf_cpu_context);\n\t\tevent_heap = (struct min_heap){\n\t\t\t.data = cpuctx->heap,\n\t\t\t.nr = 0,\n\t\t\t.size = cpuctx->heap_size,\n\t\t};\n\n\t\tlockdep_assert_held(&cpuctx->ctx.lock);\n\n#ifdef CONFIG_CGROUP_PERF\n\t\tif (cpuctx->cgrp)\n\t\t\tcss = &cpuctx->cgrp->css;\n#endif\n\t} else {\n\t\tevent_heap = (struct min_heap){\n\t\t\t.data = itrs,\n\t\t\t.nr = 0,\n\t\t\t.size = ARRAY_SIZE(itrs),\n\t\t};\n\t\t \n\t\t__heap_add(&event_heap, perf_event_groups_first(groups, -1, pmu, NULL));\n\t}\n\tevt = event_heap.data;\n\n\t__heap_add(&event_heap, perf_event_groups_first(groups, cpu, pmu, NULL));\n\n#ifdef CONFIG_CGROUP_PERF\n\tfor (; css; css = css->parent)\n\t\t__heap_add(&event_heap, perf_event_groups_first(groups, cpu, pmu, css->cgroup));\n#endif\n\n\tif (event_heap.nr) {\n\t\t__link_epc((*evt)->pmu_ctx);\n\t\tperf_assert_pmu_disabled((*evt)->pmu_ctx->pmu);\n\t}\n\n\tmin_heapify_all(&event_heap, &perf_min_heap);\n\n\twhile (event_heap.nr) {\n\t\tret = func(*evt, data);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\t*evt = perf_event_groups_next(*evt, pmu);\n\t\tif (*evt)\n\t\t\tmin_heapify(&event_heap, 0, &perf_min_heap);\n\t\telse\n\t\t\tmin_heap_pop(&event_heap, &perf_min_heap);\n\t}\n\n\treturn 0;\n}\n\n \nstatic inline bool event_update_userpage(struct perf_event *event)\n{\n\tif (likely(!atomic_read(&event->mmap_count)))\n\t\treturn false;\n\n\tperf_event_update_time(event);\n\tperf_event_update_userpage(event);\n\n\treturn true;\n}\n\nstatic inline void group_update_userpage(struct perf_event *group_event)\n{\n\tstruct perf_event *event;\n\n\tif (!event_update_userpage(group_event))\n\t\treturn;\n\n\tfor_each_sibling_event(event, group_event)\n\t\tevent_update_userpage(event);\n}\n\nstatic int merge_sched_in(struct perf_event *event, void *data)\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tint *can_add_hw = data;\n\n\tif (event->state <= PERF_EVENT_STATE_OFF)\n\t\treturn 0;\n\n\tif (!event_filter_match(event))\n\t\treturn 0;\n\n\tif (group_can_go_on(event, *can_add_hw)) {\n\t\tif (!group_sched_in(event, ctx))\n\t\t\tlist_add_tail(&event->active_list, get_event_list(event));\n\t}\n\n\tif (event->state == PERF_EVENT_STATE_INACTIVE) {\n\t\t*can_add_hw = 0;\n\t\tif (event->attr.pinned) {\n\t\t\tperf_cgroup_event_disable(event, ctx);\n\t\t\tperf_event_set_state(event, PERF_EVENT_STATE_ERROR);\n\t\t} else {\n\t\t\tstruct perf_cpu_pmu_context *cpc;\n\n\t\t\tevent->pmu_ctx->rotate_necessary = 1;\n\t\t\tcpc = this_cpu_ptr(event->pmu_ctx->pmu->cpu_pmu_context);\n\t\t\tperf_mux_hrtimer_restart(cpc);\n\t\t\tgroup_update_userpage(event);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void pmu_groups_sched_in(struct perf_event_context *ctx,\n\t\t\t\tstruct perf_event_groups *groups,\n\t\t\t\tstruct pmu *pmu)\n{\n\tint can_add_hw = 1;\n\tvisit_groups_merge(ctx, groups, smp_processor_id(), pmu,\n\t\t\t   merge_sched_in, &can_add_hw);\n}\n\nstatic void ctx_groups_sched_in(struct perf_event_context *ctx,\n\t\t\t\tstruct perf_event_groups *groups,\n\t\t\t\tbool cgroup)\n{\n\tstruct perf_event_pmu_context *pmu_ctx;\n\n\tlist_for_each_entry(pmu_ctx, &ctx->pmu_ctx_list, pmu_ctx_entry) {\n\t\tif (cgroup && !pmu_ctx->nr_cgroups)\n\t\t\tcontinue;\n\t\tpmu_groups_sched_in(ctx, groups, pmu_ctx->pmu);\n\t}\n}\n\nstatic void __pmu_ctx_sched_in(struct perf_event_context *ctx,\n\t\t\t       struct pmu *pmu)\n{\n\tpmu_groups_sched_in(ctx, &ctx->flexible_groups, pmu);\n}\n\nstatic void\nctx_sched_in(struct perf_event_context *ctx, enum event_type_t event_type)\n{\n\tstruct perf_cpu_context *cpuctx = this_cpu_ptr(&perf_cpu_context);\n\tint is_active = ctx->is_active;\n\tbool cgroup = event_type & EVENT_CGROUP;\n\n\tevent_type &= ~EVENT_CGROUP;\n\n\tlockdep_assert_held(&ctx->lock);\n\n\tif (likely(!ctx->nr_events))\n\t\treturn;\n\n\tif (!(is_active & EVENT_TIME)) {\n\t\t \n\t\t__update_context_time(ctx, false);\n\t\tperf_cgroup_set_timestamp(cpuctx);\n\t\t \n\t\tbarrier();\n\t}\n\n\tctx->is_active |= (event_type | EVENT_TIME);\n\tif (ctx->task) {\n\t\tif (!is_active)\n\t\t\tcpuctx->task_ctx = ctx;\n\t\telse\n\t\t\tWARN_ON_ONCE(cpuctx->task_ctx != ctx);\n\t}\n\n\tis_active ^= ctx->is_active;  \n\n\t \n\tif (is_active & EVENT_PINNED)\n\t\tctx_groups_sched_in(ctx, &ctx->pinned_groups, cgroup);\n\n\t \n\tif (is_active & EVENT_FLEXIBLE)\n\t\tctx_groups_sched_in(ctx, &ctx->flexible_groups, cgroup);\n}\n\nstatic void perf_event_context_sched_in(struct task_struct *task)\n{\n\tstruct perf_cpu_context *cpuctx = this_cpu_ptr(&perf_cpu_context);\n\tstruct perf_event_context *ctx;\n\n\trcu_read_lock();\n\tctx = rcu_dereference(task->perf_event_ctxp);\n\tif (!ctx)\n\t\tgoto rcu_unlock;\n\n\tif (cpuctx->task_ctx == ctx) {\n\t\tperf_ctx_lock(cpuctx, ctx);\n\t\tperf_ctx_disable(ctx, false);\n\n\t\tperf_ctx_sched_task_cb(ctx, true);\n\n\t\tperf_ctx_enable(ctx, false);\n\t\tperf_ctx_unlock(cpuctx, ctx);\n\t\tgoto rcu_unlock;\n\t}\n\n\tperf_ctx_lock(cpuctx, ctx);\n\t \n\tif (!ctx->nr_events)\n\t\tgoto unlock;\n\n\tperf_ctx_disable(ctx, false);\n\t \n\tif (!RB_EMPTY_ROOT(&ctx->pinned_groups.tree)) {\n\t\tperf_ctx_disable(&cpuctx->ctx, false);\n\t\tctx_sched_out(&cpuctx->ctx, EVENT_FLEXIBLE);\n\t}\n\n\tperf_event_sched_in(cpuctx, ctx);\n\n\tperf_ctx_sched_task_cb(cpuctx->task_ctx, true);\n\n\tif (!RB_EMPTY_ROOT(&ctx->pinned_groups.tree))\n\t\tperf_ctx_enable(&cpuctx->ctx, false);\n\n\tperf_ctx_enable(ctx, false);\n\nunlock:\n\tperf_ctx_unlock(cpuctx, ctx);\nrcu_unlock:\n\trcu_read_unlock();\n}\n\n \nvoid __perf_event_task_sched_in(struct task_struct *prev,\n\t\t\t\tstruct task_struct *task)\n{\n\tperf_event_context_sched_in(task);\n\n\tif (atomic_read(&nr_switch_events))\n\t\tperf_event_switch(task, prev, true);\n\n\tif (__this_cpu_read(perf_sched_cb_usages))\n\t\tperf_pmu_sched_task(prev, task, true);\n}\n\nstatic u64 perf_calculate_period(struct perf_event *event, u64 nsec, u64 count)\n{\n\tu64 frequency = event->attr.sample_freq;\n\tu64 sec = NSEC_PER_SEC;\n\tu64 divisor, dividend;\n\n\tint count_fls, nsec_fls, frequency_fls, sec_fls;\n\n\tcount_fls = fls64(count);\n\tnsec_fls = fls64(nsec);\n\tfrequency_fls = fls64(frequency);\n\tsec_fls = 30;\n\n\t \n\n\t \n#define REDUCE_FLS(a, b)\t\t\\\ndo {\t\t\t\t\t\\\n\tif (a##_fls > b##_fls) {\t\\\n\t\ta >>= 1;\t\t\\\n\t\ta##_fls--;\t\t\\\n\t} else {\t\t\t\\\n\t\tb >>= 1;\t\t\\\n\t\tb##_fls--;\t\t\\\n\t}\t\t\t\t\\\n} while (0)\n\n\t \n\twhile (count_fls + sec_fls > 64 && nsec_fls + frequency_fls > 64) {\n\t\tREDUCE_FLS(nsec, frequency);\n\t\tREDUCE_FLS(sec, count);\n\t}\n\n\tif (count_fls + sec_fls > 64) {\n\t\tdivisor = nsec * frequency;\n\n\t\twhile (count_fls + sec_fls > 64) {\n\t\t\tREDUCE_FLS(count, sec);\n\t\t\tdivisor >>= 1;\n\t\t}\n\n\t\tdividend = count * sec;\n\t} else {\n\t\tdividend = count * sec;\n\n\t\twhile (nsec_fls + frequency_fls > 64) {\n\t\t\tREDUCE_FLS(nsec, frequency);\n\t\t\tdividend >>= 1;\n\t\t}\n\n\t\tdivisor = nsec * frequency;\n\t}\n\n\tif (!divisor)\n\t\treturn dividend;\n\n\treturn div64_u64(dividend, divisor);\n}\n\nstatic DEFINE_PER_CPU(int, perf_throttled_count);\nstatic DEFINE_PER_CPU(u64, perf_throttled_seq);\n\nstatic void perf_adjust_period(struct perf_event *event, u64 nsec, u64 count, bool disable)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\ts64 period, sample_period;\n\ts64 delta;\n\n\tperiod = perf_calculate_period(event, nsec, count);\n\n\tdelta = (s64)(period - hwc->sample_period);\n\tdelta = (delta + 7) / 8;  \n\n\tsample_period = hwc->sample_period + delta;\n\n\tif (!sample_period)\n\t\tsample_period = 1;\n\n\thwc->sample_period = sample_period;\n\n\tif (local64_read(&hwc->period_left) > 8*sample_period) {\n\t\tif (disable)\n\t\t\tevent->pmu->stop(event, PERF_EF_UPDATE);\n\n\t\tlocal64_set(&hwc->period_left, 0);\n\n\t\tif (disable)\n\t\t\tevent->pmu->start(event, PERF_EF_RELOAD);\n\t}\n}\n\n \nstatic void\nperf_adjust_freq_unthr_context(struct perf_event_context *ctx, bool unthrottle)\n{\n\tstruct perf_event *event;\n\tstruct hw_perf_event *hwc;\n\tu64 now, period = TICK_NSEC;\n\ts64 delta;\n\n\t \n\tif (!(ctx->nr_freq || unthrottle))\n\t\treturn;\n\n\traw_spin_lock(&ctx->lock);\n\n\tlist_for_each_entry_rcu(event, &ctx->event_list, event_entry) {\n\t\tif (event->state != PERF_EVENT_STATE_ACTIVE)\n\t\t\tcontinue;\n\n\t\t\n\t\tif (!event_filter_match(event))\n\t\t\tcontinue;\n\n\t\tperf_pmu_disable(event->pmu);\n\n\t\thwc = &event->hw;\n\n\t\tif (hwc->interrupts == MAX_INTERRUPTS) {\n\t\t\thwc->interrupts = 0;\n\t\t\tperf_log_throttle(event, 1);\n\t\t\tevent->pmu->start(event, 0);\n\t\t}\n\n\t\tif (!event->attr.freq || !event->attr.sample_freq)\n\t\t\tgoto next;\n\n\t\t \n\t\tevent->pmu->stop(event, PERF_EF_UPDATE);\n\n\t\tnow = local64_read(&event->count);\n\t\tdelta = now - hwc->freq_count_stamp;\n\t\thwc->freq_count_stamp = now;\n\n\t\t \n\t\tif (delta > 0)\n\t\t\tperf_adjust_period(event, period, delta, false);\n\n\t\tevent->pmu->start(event, delta > 0 ? PERF_EF_RELOAD : 0);\n\tnext:\n\t\tperf_pmu_enable(event->pmu);\n\t}\n\n\traw_spin_unlock(&ctx->lock);\n}\n\n \nstatic void rotate_ctx(struct perf_event_context *ctx, struct perf_event *event)\n{\n\t \n\tif (ctx->rotate_disable)\n\t\treturn;\n\n\tperf_event_groups_delete(&ctx->flexible_groups, event);\n\tperf_event_groups_insert(&ctx->flexible_groups, event);\n}\n\n \nstatic inline struct perf_event *\nctx_event_to_rotate(struct perf_event_pmu_context *pmu_ctx)\n{\n\tstruct perf_event *event;\n\tstruct rb_node *node;\n\tstruct rb_root *tree;\n\tstruct __group_key key = {\n\t\t.pmu = pmu_ctx->pmu,\n\t};\n\n\t \n\tevent = list_first_entry_or_null(&pmu_ctx->flexible_active,\n\t\t\t\t\t struct perf_event, active_list);\n\tif (event)\n\t\tgoto out;\n\n\t \n\ttree = &pmu_ctx->ctx->flexible_groups.tree;\n\n\tif (!pmu_ctx->ctx->task) {\n\t\tkey.cpu = smp_processor_id();\n\n\t\tnode = rb_find_first(&key, tree, __group_cmp_ignore_cgroup);\n\t\tif (node)\n\t\t\tevent = __node_2_pe(node);\n\t\tgoto out;\n\t}\n\n\tkey.cpu = -1;\n\tnode = rb_find_first(&key, tree, __group_cmp_ignore_cgroup);\n\tif (node) {\n\t\tevent = __node_2_pe(node);\n\t\tgoto out;\n\t}\n\n\tkey.cpu = smp_processor_id();\n\tnode = rb_find_first(&key, tree, __group_cmp_ignore_cgroup);\n\tif (node)\n\t\tevent = __node_2_pe(node);\n\nout:\n\t \n\tpmu_ctx->rotate_necessary = 0;\n\n\treturn event;\n}\n\nstatic bool perf_rotate_context(struct perf_cpu_pmu_context *cpc)\n{\n\tstruct perf_cpu_context *cpuctx = this_cpu_ptr(&perf_cpu_context);\n\tstruct perf_event_pmu_context *cpu_epc, *task_epc = NULL;\n\tstruct perf_event *cpu_event = NULL, *task_event = NULL;\n\tint cpu_rotate, task_rotate;\n\tstruct pmu *pmu;\n\n\t \n\n\tcpu_epc = &cpc->epc;\n\tpmu = cpu_epc->pmu;\n\ttask_epc = cpc->task_epc;\n\n\tcpu_rotate = cpu_epc->rotate_necessary;\n\ttask_rotate = task_epc ? task_epc->rotate_necessary : 0;\n\n\tif (!(cpu_rotate || task_rotate))\n\t\treturn false;\n\n\tperf_ctx_lock(cpuctx, cpuctx->task_ctx);\n\tperf_pmu_disable(pmu);\n\n\tif (task_rotate)\n\t\ttask_event = ctx_event_to_rotate(task_epc);\n\tif (cpu_rotate)\n\t\tcpu_event = ctx_event_to_rotate(cpu_epc);\n\n\t \n\tif (task_event || (task_epc && cpu_event)) {\n\t\tupdate_context_time(task_epc->ctx);\n\t\t__pmu_ctx_sched_out(task_epc, EVENT_FLEXIBLE);\n\t}\n\n\tif (cpu_event) {\n\t\tupdate_context_time(&cpuctx->ctx);\n\t\t__pmu_ctx_sched_out(cpu_epc, EVENT_FLEXIBLE);\n\t\trotate_ctx(&cpuctx->ctx, cpu_event);\n\t\t__pmu_ctx_sched_in(&cpuctx->ctx, pmu);\n\t}\n\n\tif (task_event)\n\t\trotate_ctx(task_epc->ctx, task_event);\n\n\tif (task_event || (task_epc && cpu_event))\n\t\t__pmu_ctx_sched_in(task_epc->ctx, pmu);\n\n\tperf_pmu_enable(pmu);\n\tperf_ctx_unlock(cpuctx, cpuctx->task_ctx);\n\n\treturn true;\n}\n\nvoid perf_event_task_tick(void)\n{\n\tstruct perf_cpu_context *cpuctx = this_cpu_ptr(&perf_cpu_context);\n\tstruct perf_event_context *ctx;\n\tint throttled;\n\n\tlockdep_assert_irqs_disabled();\n\n\t__this_cpu_inc(perf_throttled_seq);\n\tthrottled = __this_cpu_xchg(perf_throttled_count, 0);\n\ttick_dep_clear_cpu(smp_processor_id(), TICK_DEP_BIT_PERF_EVENTS);\n\n\tperf_adjust_freq_unthr_context(&cpuctx->ctx, !!throttled);\n\n\trcu_read_lock();\n\tctx = rcu_dereference(current->perf_event_ctxp);\n\tif (ctx)\n\t\tperf_adjust_freq_unthr_context(ctx, !!throttled);\n\trcu_read_unlock();\n}\n\nstatic int event_enable_on_exec(struct perf_event *event,\n\t\t\t\tstruct perf_event_context *ctx)\n{\n\tif (!event->attr.enable_on_exec)\n\t\treturn 0;\n\n\tevent->attr.enable_on_exec = 0;\n\tif (event->state >= PERF_EVENT_STATE_INACTIVE)\n\t\treturn 0;\n\n\tperf_event_set_state(event, PERF_EVENT_STATE_INACTIVE);\n\n\treturn 1;\n}\n\n \nstatic void perf_event_enable_on_exec(struct perf_event_context *ctx)\n{\n\tstruct perf_event_context *clone_ctx = NULL;\n\tenum event_type_t event_type = 0;\n\tstruct perf_cpu_context *cpuctx;\n\tstruct perf_event *event;\n\tunsigned long flags;\n\tint enabled = 0;\n\n\tlocal_irq_save(flags);\n\tif (WARN_ON_ONCE(current->perf_event_ctxp != ctx))\n\t\tgoto out;\n\n\tif (!ctx->nr_events)\n\t\tgoto out;\n\n\tcpuctx = this_cpu_ptr(&perf_cpu_context);\n\tperf_ctx_lock(cpuctx, ctx);\n\tctx_sched_out(ctx, EVENT_TIME);\n\n\tlist_for_each_entry(event, &ctx->event_list, event_entry) {\n\t\tenabled |= event_enable_on_exec(event, ctx);\n\t\tevent_type |= get_event_type(event);\n\t}\n\n\t \n\tif (enabled) {\n\t\tclone_ctx = unclone_ctx(ctx);\n\t\tctx_resched(cpuctx, ctx, event_type);\n\t} else {\n\t\tctx_sched_in(ctx, EVENT_TIME);\n\t}\n\tperf_ctx_unlock(cpuctx, ctx);\n\nout:\n\tlocal_irq_restore(flags);\n\n\tif (clone_ctx)\n\t\tput_ctx(clone_ctx);\n}\n\nstatic void perf_remove_from_owner(struct perf_event *event);\nstatic void perf_event_exit_event(struct perf_event *event,\n\t\t\t\t  struct perf_event_context *ctx);\n\n \nstatic void perf_event_remove_on_exec(struct perf_event_context *ctx)\n{\n\tstruct perf_event_context *clone_ctx = NULL;\n\tstruct perf_event *event, *next;\n\tunsigned long flags;\n\tbool modified = false;\n\n\tmutex_lock(&ctx->mutex);\n\n\tif (WARN_ON_ONCE(ctx->task != current))\n\t\tgoto unlock;\n\n\tlist_for_each_entry_safe(event, next, &ctx->event_list, event_entry) {\n\t\tif (!event->attr.remove_on_exec)\n\t\t\tcontinue;\n\n\t\tif (!is_kernel_event(event))\n\t\t\tperf_remove_from_owner(event);\n\n\t\tmodified = true;\n\n\t\tperf_event_exit_event(event, ctx);\n\t}\n\n\traw_spin_lock_irqsave(&ctx->lock, flags);\n\tif (modified)\n\t\tclone_ctx = unclone_ctx(ctx);\n\traw_spin_unlock_irqrestore(&ctx->lock, flags);\n\nunlock:\n\tmutex_unlock(&ctx->mutex);\n\n\tif (clone_ctx)\n\t\tput_ctx(clone_ctx);\n}\n\nstruct perf_read_data {\n\tstruct perf_event *event;\n\tbool group;\n\tint ret;\n};\n\nstatic int __perf_event_read_cpu(struct perf_event *event, int event_cpu)\n{\n\tu16 local_pkg, event_pkg;\n\n\tif (event->group_caps & PERF_EV_CAP_READ_ACTIVE_PKG) {\n\t\tint local_cpu = smp_processor_id();\n\n\t\tevent_pkg = topology_physical_package_id(event_cpu);\n\t\tlocal_pkg = topology_physical_package_id(local_cpu);\n\n\t\tif (event_pkg == local_pkg)\n\t\t\treturn local_cpu;\n\t}\n\n\treturn event_cpu;\n}\n\n \nstatic void __perf_event_read(void *info)\n{\n\tstruct perf_read_data *data = info;\n\tstruct perf_event *sub, *event = data->event;\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct perf_cpu_context *cpuctx = this_cpu_ptr(&perf_cpu_context);\n\tstruct pmu *pmu = event->pmu;\n\n\t \n\tif (ctx->task && cpuctx->task_ctx != ctx)\n\t\treturn;\n\n\traw_spin_lock(&ctx->lock);\n\tif (ctx->is_active & EVENT_TIME) {\n\t\tupdate_context_time(ctx);\n\t\tupdate_cgrp_time_from_event(event);\n\t}\n\n\tperf_event_update_time(event);\n\tif (data->group)\n\t\tperf_event_update_sibling_time(event);\n\n\tif (event->state != PERF_EVENT_STATE_ACTIVE)\n\t\tgoto unlock;\n\n\tif (!data->group) {\n\t\tpmu->read(event);\n\t\tdata->ret = 0;\n\t\tgoto unlock;\n\t}\n\n\tpmu->start_txn(pmu, PERF_PMU_TXN_READ);\n\n\tpmu->read(event);\n\n\tfor_each_sibling_event(sub, event) {\n\t\tif (sub->state == PERF_EVENT_STATE_ACTIVE) {\n\t\t\t \n\t\t\tsub->pmu->read(sub);\n\t\t}\n\t}\n\n\tdata->ret = pmu->commit_txn(pmu);\n\nunlock:\n\traw_spin_unlock(&ctx->lock);\n}\n\nstatic inline u64 perf_event_count(struct perf_event *event)\n{\n\treturn local64_read(&event->count) + atomic64_read(&event->child_count);\n}\n\nstatic void calc_timer_values(struct perf_event *event,\n\t\t\t\tu64 *now,\n\t\t\t\tu64 *enabled,\n\t\t\t\tu64 *running)\n{\n\tu64 ctx_time;\n\n\t*now = perf_clock();\n\tctx_time = perf_event_time_now(event, *now);\n\t__perf_update_times(event, ctx_time, enabled, running);\n}\n\n \nint perf_event_read_local(struct perf_event *event, u64 *value,\n\t\t\t  u64 *enabled, u64 *running)\n{\n\tunsigned long flags;\n\tint ret = 0;\n\n\t \n\tlocal_irq_save(flags);\n\n\t \n\tif (event->attr.inherit) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\t \n\tif ((event->attach_state & PERF_ATTACH_TASK) &&\n\t    event->hw.target != current) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t \n\tif (!(event->attach_state & PERF_ATTACH_TASK) &&\n\t    event->cpu != smp_processor_id()) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t \n\tif (event->attr.pinned && event->oncpu != smp_processor_id()) {\n\t\tret = -EBUSY;\n\t\tgoto out;\n\t}\n\n\t \n\tif (event->oncpu == smp_processor_id())\n\t\tevent->pmu->read(event);\n\n\t*value = local64_read(&event->count);\n\tif (enabled || running) {\n\t\tu64 __enabled, __running, __now;\n\n\t\tcalc_timer_values(event, &__now, &__enabled, &__running);\n\t\tif (enabled)\n\t\t\t*enabled = __enabled;\n\t\tif (running)\n\t\t\t*running = __running;\n\t}\nout:\n\tlocal_irq_restore(flags);\n\n\treturn ret;\n}\n\nstatic int perf_event_read(struct perf_event *event, bool group)\n{\n\tenum perf_event_state state = READ_ONCE(event->state);\n\tint event_cpu, ret = 0;\n\n\t \nagain:\n\tif (state == PERF_EVENT_STATE_ACTIVE) {\n\t\tstruct perf_read_data data;\n\n\t\t \n\t\tsmp_rmb();\n\n\t\tevent_cpu = READ_ONCE(event->oncpu);\n\t\tif ((unsigned)event_cpu >= nr_cpu_ids)\n\t\t\treturn 0;\n\n\t\tdata = (struct perf_read_data){\n\t\t\t.event = event,\n\t\t\t.group = group,\n\t\t\t.ret = 0,\n\t\t};\n\n\t\tpreempt_disable();\n\t\tevent_cpu = __perf_event_read_cpu(event, event_cpu);\n\n\t\t \n\t\t(void)smp_call_function_single(event_cpu, __perf_event_read, &data, 1);\n\t\tpreempt_enable();\n\t\tret = data.ret;\n\n\t} else if (state == PERF_EVENT_STATE_INACTIVE) {\n\t\tstruct perf_event_context *ctx = event->ctx;\n\t\tunsigned long flags;\n\n\t\traw_spin_lock_irqsave(&ctx->lock, flags);\n\t\tstate = event->state;\n\t\tif (state != PERF_EVENT_STATE_INACTIVE) {\n\t\t\traw_spin_unlock_irqrestore(&ctx->lock, flags);\n\t\t\tgoto again;\n\t\t}\n\n\t\t \n\t\tif (ctx->is_active & EVENT_TIME) {\n\t\t\tupdate_context_time(ctx);\n\t\t\tupdate_cgrp_time_from_event(event);\n\t\t}\n\n\t\tperf_event_update_time(event);\n\t\tif (group)\n\t\t\tperf_event_update_sibling_time(event);\n\t\traw_spin_unlock_irqrestore(&ctx->lock, flags);\n\t}\n\n\treturn ret;\n}\n\n \nstatic void __perf_event_init_context(struct perf_event_context *ctx)\n{\n\traw_spin_lock_init(&ctx->lock);\n\tmutex_init(&ctx->mutex);\n\tINIT_LIST_HEAD(&ctx->pmu_ctx_list);\n\tperf_event_groups_init(&ctx->pinned_groups);\n\tperf_event_groups_init(&ctx->flexible_groups);\n\tINIT_LIST_HEAD(&ctx->event_list);\n\trefcount_set(&ctx->refcount, 1);\n}\n\nstatic void\n__perf_init_event_pmu_context(struct perf_event_pmu_context *epc, struct pmu *pmu)\n{\n\tepc->pmu = pmu;\n\tINIT_LIST_HEAD(&epc->pmu_ctx_entry);\n\tINIT_LIST_HEAD(&epc->pinned_active);\n\tINIT_LIST_HEAD(&epc->flexible_active);\n\tatomic_set(&epc->refcount, 1);\n}\n\nstatic struct perf_event_context *\nalloc_perf_context(struct task_struct *task)\n{\n\tstruct perf_event_context *ctx;\n\n\tctx = kzalloc(sizeof(struct perf_event_context), GFP_KERNEL);\n\tif (!ctx)\n\t\treturn NULL;\n\n\t__perf_event_init_context(ctx);\n\tif (task)\n\t\tctx->task = get_task_struct(task);\n\n\treturn ctx;\n}\n\nstatic struct task_struct *\nfind_lively_task_by_vpid(pid_t vpid)\n{\n\tstruct task_struct *task;\n\n\trcu_read_lock();\n\tif (!vpid)\n\t\ttask = current;\n\telse\n\t\ttask = find_task_by_vpid(vpid);\n\tif (task)\n\t\tget_task_struct(task);\n\trcu_read_unlock();\n\n\tif (!task)\n\t\treturn ERR_PTR(-ESRCH);\n\n\treturn task;\n}\n\n \nstatic struct perf_event_context *\nfind_get_context(struct task_struct *task, struct perf_event *event)\n{\n\tstruct perf_event_context *ctx, *clone_ctx = NULL;\n\tstruct perf_cpu_context *cpuctx;\n\tunsigned long flags;\n\tint err;\n\n\tif (!task) {\n\t\t \n\t\terr = perf_allow_cpu(&event->attr);\n\t\tif (err)\n\t\t\treturn ERR_PTR(err);\n\n\t\tcpuctx = per_cpu_ptr(&perf_cpu_context, event->cpu);\n\t\tctx = &cpuctx->ctx;\n\t\tget_ctx(ctx);\n\t\traw_spin_lock_irqsave(&ctx->lock, flags);\n\t\t++ctx->pin_count;\n\t\traw_spin_unlock_irqrestore(&ctx->lock, flags);\n\n\t\treturn ctx;\n\t}\n\n\terr = -EINVAL;\nretry:\n\tctx = perf_lock_task_context(task, &flags);\n\tif (ctx) {\n\t\tclone_ctx = unclone_ctx(ctx);\n\t\t++ctx->pin_count;\n\n\t\traw_spin_unlock_irqrestore(&ctx->lock, flags);\n\n\t\tif (clone_ctx)\n\t\t\tput_ctx(clone_ctx);\n\t} else {\n\t\tctx = alloc_perf_context(task);\n\t\terr = -ENOMEM;\n\t\tif (!ctx)\n\t\t\tgoto errout;\n\n\t\terr = 0;\n\t\tmutex_lock(&task->perf_event_mutex);\n\t\t \n\t\tif (task->flags & PF_EXITING)\n\t\t\terr = -ESRCH;\n\t\telse if (task->perf_event_ctxp)\n\t\t\terr = -EAGAIN;\n\t\telse {\n\t\t\tget_ctx(ctx);\n\t\t\t++ctx->pin_count;\n\t\t\trcu_assign_pointer(task->perf_event_ctxp, ctx);\n\t\t}\n\t\tmutex_unlock(&task->perf_event_mutex);\n\n\t\tif (unlikely(err)) {\n\t\t\tput_ctx(ctx);\n\n\t\t\tif (err == -EAGAIN)\n\t\t\t\tgoto retry;\n\t\t\tgoto errout;\n\t\t}\n\t}\n\n\treturn ctx;\n\nerrout:\n\treturn ERR_PTR(err);\n}\n\nstatic struct perf_event_pmu_context *\nfind_get_pmu_context(struct pmu *pmu, struct perf_event_context *ctx,\n\t\t     struct perf_event *event)\n{\n\tstruct perf_event_pmu_context *new = NULL, *epc;\n\tvoid *task_ctx_data = NULL;\n\n\tif (!ctx->task) {\n\t\t \n\t\tstruct perf_cpu_pmu_context *cpc;\n\n\t\tcpc = per_cpu_ptr(pmu->cpu_pmu_context, event->cpu);\n\t\tepc = &cpc->epc;\n\t\traw_spin_lock_irq(&ctx->lock);\n\t\tif (!epc->ctx) {\n\t\t\tatomic_set(&epc->refcount, 1);\n\t\t\tepc->embedded = 1;\n\t\t\tlist_add(&epc->pmu_ctx_entry, &ctx->pmu_ctx_list);\n\t\t\tepc->ctx = ctx;\n\t\t} else {\n\t\t\tWARN_ON_ONCE(epc->ctx != ctx);\n\t\t\tatomic_inc(&epc->refcount);\n\t\t}\n\t\traw_spin_unlock_irq(&ctx->lock);\n\t\treturn epc;\n\t}\n\n\tnew = kzalloc(sizeof(*epc), GFP_KERNEL);\n\tif (!new)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (event->attach_state & PERF_ATTACH_TASK_DATA) {\n\t\ttask_ctx_data = alloc_task_ctx_data(pmu);\n\t\tif (!task_ctx_data) {\n\t\t\tkfree(new);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t}\n\n\t__perf_init_event_pmu_context(new, pmu);\n\n\t \n\n\traw_spin_lock_irq(&ctx->lock);\n\tlist_for_each_entry(epc, &ctx->pmu_ctx_list, pmu_ctx_entry) {\n\t\tif (epc->pmu == pmu) {\n\t\t\tWARN_ON_ONCE(epc->ctx != ctx);\n\t\t\tatomic_inc(&epc->refcount);\n\t\t\tgoto found_epc;\n\t\t}\n\t}\n\n\tepc = new;\n\tnew = NULL;\n\n\tlist_add(&epc->pmu_ctx_entry, &ctx->pmu_ctx_list);\n\tepc->ctx = ctx;\n\nfound_epc:\n\tif (task_ctx_data && !epc->task_ctx_data) {\n\t\tepc->task_ctx_data = task_ctx_data;\n\t\ttask_ctx_data = NULL;\n\t\tctx->nr_task_data++;\n\t}\n\traw_spin_unlock_irq(&ctx->lock);\n\n\tfree_task_ctx_data(pmu, task_ctx_data);\n\tkfree(new);\n\n\treturn epc;\n}\n\nstatic void get_pmu_ctx(struct perf_event_pmu_context *epc)\n{\n\tWARN_ON_ONCE(!atomic_inc_not_zero(&epc->refcount));\n}\n\nstatic void free_epc_rcu(struct rcu_head *head)\n{\n\tstruct perf_event_pmu_context *epc = container_of(head, typeof(*epc), rcu_head);\n\n\tkfree(epc->task_ctx_data);\n\tkfree(epc);\n}\n\nstatic void put_pmu_ctx(struct perf_event_pmu_context *epc)\n{\n\tstruct perf_event_context *ctx = epc->ctx;\n\tunsigned long flags;\n\n\t \n\tif (!atomic_dec_and_raw_lock_irqsave(&epc->refcount, &ctx->lock, flags))\n\t\treturn;\n\n\tWARN_ON_ONCE(list_empty(&epc->pmu_ctx_entry));\n\n\tlist_del_init(&epc->pmu_ctx_entry);\n\tepc->ctx = NULL;\n\n\tWARN_ON_ONCE(!list_empty(&epc->pinned_active));\n\tWARN_ON_ONCE(!list_empty(&epc->flexible_active));\n\n\traw_spin_unlock_irqrestore(&ctx->lock, flags);\n\n\tif (epc->embedded)\n\t\treturn;\n\n\tcall_rcu(&epc->rcu_head, free_epc_rcu);\n}\n\nstatic void perf_event_free_filter(struct perf_event *event);\n\nstatic void free_event_rcu(struct rcu_head *head)\n{\n\tstruct perf_event *event = container_of(head, typeof(*event), rcu_head);\n\n\tif (event->ns)\n\t\tput_pid_ns(event->ns);\n\tperf_event_free_filter(event);\n\tkmem_cache_free(perf_event_cache, event);\n}\n\nstatic void ring_buffer_attach(struct perf_event *event,\n\t\t\t       struct perf_buffer *rb);\n\nstatic void detach_sb_event(struct perf_event *event)\n{\n\tstruct pmu_event_list *pel = per_cpu_ptr(&pmu_sb_events, event->cpu);\n\n\traw_spin_lock(&pel->lock);\n\tlist_del_rcu(&event->sb_list);\n\traw_spin_unlock(&pel->lock);\n}\n\nstatic bool is_sb_event(struct perf_event *event)\n{\n\tstruct perf_event_attr *attr = &event->attr;\n\n\tif (event->parent)\n\t\treturn false;\n\n\tif (event->attach_state & PERF_ATTACH_TASK)\n\t\treturn false;\n\n\tif (attr->mmap || attr->mmap_data || attr->mmap2 ||\n\t    attr->comm || attr->comm_exec ||\n\t    attr->task || attr->ksymbol ||\n\t    attr->context_switch || attr->text_poke ||\n\t    attr->bpf_event)\n\t\treturn true;\n\treturn false;\n}\n\nstatic void unaccount_pmu_sb_event(struct perf_event *event)\n{\n\tif (is_sb_event(event))\n\t\tdetach_sb_event(event);\n}\n\n#ifdef CONFIG_NO_HZ_FULL\nstatic DEFINE_SPINLOCK(nr_freq_lock);\n#endif\n\nstatic void unaccount_freq_event_nohz(void)\n{\n#ifdef CONFIG_NO_HZ_FULL\n\tspin_lock(&nr_freq_lock);\n\tif (atomic_dec_and_test(&nr_freq_events))\n\t\ttick_nohz_dep_clear(TICK_DEP_BIT_PERF_EVENTS);\n\tspin_unlock(&nr_freq_lock);\n#endif\n}\n\nstatic void unaccount_freq_event(void)\n{\n\tif (tick_nohz_full_enabled())\n\t\tunaccount_freq_event_nohz();\n\telse\n\t\tatomic_dec(&nr_freq_events);\n}\n\nstatic void unaccount_event(struct perf_event *event)\n{\n\tbool dec = false;\n\n\tif (event->parent)\n\t\treturn;\n\n\tif (event->attach_state & (PERF_ATTACH_TASK | PERF_ATTACH_SCHED_CB))\n\t\tdec = true;\n\tif (event->attr.mmap || event->attr.mmap_data)\n\t\tatomic_dec(&nr_mmap_events);\n\tif (event->attr.build_id)\n\t\tatomic_dec(&nr_build_id_events);\n\tif (event->attr.comm)\n\t\tatomic_dec(&nr_comm_events);\n\tif (event->attr.namespaces)\n\t\tatomic_dec(&nr_namespaces_events);\n\tif (event->attr.cgroup)\n\t\tatomic_dec(&nr_cgroup_events);\n\tif (event->attr.task)\n\t\tatomic_dec(&nr_task_events);\n\tif (event->attr.freq)\n\t\tunaccount_freq_event();\n\tif (event->attr.context_switch) {\n\t\tdec = true;\n\t\tatomic_dec(&nr_switch_events);\n\t}\n\tif (is_cgroup_event(event))\n\t\tdec = true;\n\tif (has_branch_stack(event))\n\t\tdec = true;\n\tif (event->attr.ksymbol)\n\t\tatomic_dec(&nr_ksymbol_events);\n\tif (event->attr.bpf_event)\n\t\tatomic_dec(&nr_bpf_events);\n\tif (event->attr.text_poke)\n\t\tatomic_dec(&nr_text_poke_events);\n\n\tif (dec) {\n\t\tif (!atomic_add_unless(&perf_sched_count, -1, 1))\n\t\t\tschedule_delayed_work(&perf_sched_work, HZ);\n\t}\n\n\tunaccount_pmu_sb_event(event);\n}\n\nstatic void perf_sched_delayed(struct work_struct *work)\n{\n\tmutex_lock(&perf_sched_mutex);\n\tif (atomic_dec_and_test(&perf_sched_count))\n\t\tstatic_branch_disable(&perf_sched_events);\n\tmutex_unlock(&perf_sched_mutex);\n}\n\n \nstatic int exclusive_event_init(struct perf_event *event)\n{\n\tstruct pmu *pmu = event->pmu;\n\n\tif (!is_exclusive_pmu(pmu))\n\t\treturn 0;\n\n\t \n\tif (event->attach_state & PERF_ATTACH_TASK) {\n\t\tif (!atomic_inc_unless_negative(&pmu->exclusive_cnt))\n\t\t\treturn -EBUSY;\n\t} else {\n\t\tif (!atomic_dec_unless_positive(&pmu->exclusive_cnt))\n\t\t\treturn -EBUSY;\n\t}\n\n\treturn 0;\n}\n\nstatic void exclusive_event_destroy(struct perf_event *event)\n{\n\tstruct pmu *pmu = event->pmu;\n\n\tif (!is_exclusive_pmu(pmu))\n\t\treturn;\n\n\t \n\tif (event->attach_state & PERF_ATTACH_TASK)\n\t\tatomic_dec(&pmu->exclusive_cnt);\n\telse\n\t\tatomic_inc(&pmu->exclusive_cnt);\n}\n\nstatic bool exclusive_event_match(struct perf_event *e1, struct perf_event *e2)\n{\n\tif ((e1->pmu == e2->pmu) &&\n\t    (e1->cpu == e2->cpu ||\n\t     e1->cpu == -1 ||\n\t     e2->cpu == -1))\n\t\treturn true;\n\treturn false;\n}\n\nstatic bool exclusive_event_installable(struct perf_event *event,\n\t\t\t\t\tstruct perf_event_context *ctx)\n{\n\tstruct perf_event *iter_event;\n\tstruct pmu *pmu = event->pmu;\n\n\tlockdep_assert_held(&ctx->mutex);\n\n\tif (!is_exclusive_pmu(pmu))\n\t\treturn true;\n\n\tlist_for_each_entry(iter_event, &ctx->event_list, event_entry) {\n\t\tif (exclusive_event_match(iter_event, event))\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic void perf_addr_filters_splice(struct perf_event *event,\n\t\t\t\t       struct list_head *head);\n\nstatic void _free_event(struct perf_event *event)\n{\n\tirq_work_sync(&event->pending_irq);\n\n\tunaccount_event(event);\n\n\tsecurity_perf_event_free(event);\n\n\tif (event->rb) {\n\t\t \n\t\tmutex_lock(&event->mmap_mutex);\n\t\tring_buffer_attach(event, NULL);\n\t\tmutex_unlock(&event->mmap_mutex);\n\t}\n\n\tif (is_cgroup_event(event))\n\t\tperf_detach_cgroup(event);\n\n\tif (!event->parent) {\n\t\tif (event->attr.sample_type & PERF_SAMPLE_CALLCHAIN)\n\t\t\tput_callchain_buffers();\n\t}\n\n\tperf_event_free_bpf_prog(event);\n\tperf_addr_filters_splice(event, NULL);\n\tkfree(event->addr_filter_ranges);\n\n\tif (event->destroy)\n\t\tevent->destroy(event);\n\n\t \n\tif (event->hw.target)\n\t\tput_task_struct(event->hw.target);\n\n\tif (event->pmu_ctx)\n\t\tput_pmu_ctx(event->pmu_ctx);\n\n\t \n\tif (event->ctx)\n\t\tput_ctx(event->ctx);\n\n\texclusive_event_destroy(event);\n\tmodule_put(event->pmu->module);\n\n\tcall_rcu(&event->rcu_head, free_event_rcu);\n}\n\n \nstatic void free_event(struct perf_event *event)\n{\n\tif (WARN(atomic_long_cmpxchg(&event->refcount, 1, 0) != 1,\n\t\t\t\t\"unexpected event refcount: %ld; ptr=%p\\n\",\n\t\t\t\tatomic_long_read(&event->refcount), event)) {\n\t\t \n\t\treturn;\n\t}\n\n\t_free_event(event);\n}\n\n \nstatic void perf_remove_from_owner(struct perf_event *event)\n{\n\tstruct task_struct *owner;\n\n\trcu_read_lock();\n\t \n\towner = READ_ONCE(event->owner);\n\tif (owner) {\n\t\t \n\t\tget_task_struct(owner);\n\t}\n\trcu_read_unlock();\n\n\tif (owner) {\n\t\t \n\t\tmutex_lock_nested(&owner->perf_event_mutex, SINGLE_DEPTH_NESTING);\n\n\t\t \n\t\tif (event->owner) {\n\t\t\tlist_del_init(&event->owner_entry);\n\t\t\tsmp_store_release(&event->owner, NULL);\n\t\t}\n\t\tmutex_unlock(&owner->perf_event_mutex);\n\t\tput_task_struct(owner);\n\t}\n}\n\nstatic void put_event(struct perf_event *event)\n{\n\tif (!atomic_long_dec_and_test(&event->refcount))\n\t\treturn;\n\n\t_free_event(event);\n}\n\n \nint perf_event_release_kernel(struct perf_event *event)\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct perf_event *child, *tmp;\n\tLIST_HEAD(free_list);\n\n\t \n\tif (!ctx) {\n\t\tWARN_ON_ONCE(event->attach_state &\n\t\t\t\t(PERF_ATTACH_CONTEXT|PERF_ATTACH_GROUP));\n\t\tgoto no_ctx;\n\t}\n\n\tif (!is_kernel_event(event))\n\t\tperf_remove_from_owner(event);\n\n\tctx = perf_event_ctx_lock(event);\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\n\t \n\tperf_remove_from_context(event, DETACH_GROUP|DETACH_DEAD);\n\n\tperf_event_ctx_unlock(event, ctx);\n\nagain:\n\tmutex_lock(&event->child_mutex);\n\tlist_for_each_entry(child, &event->child_list, child_list) {\n\n\t\t \n\t\tctx = READ_ONCE(child->ctx);\n\t\t \n\t\tget_ctx(ctx);\n\n\t\t \n\t\tmutex_unlock(&event->child_mutex);\n\t\tmutex_lock(&ctx->mutex);\n\t\tmutex_lock(&event->child_mutex);\n\n\t\t \n\t\ttmp = list_first_entry_or_null(&event->child_list,\n\t\t\t\t\t       struct perf_event, child_list);\n\t\tif (tmp == child) {\n\t\t\tperf_remove_from_context(child, DETACH_GROUP);\n\t\t\tlist_move(&child->child_list, &free_list);\n\t\t\t \n\t\t\tput_event(event);\n\t\t}\n\n\t\tmutex_unlock(&event->child_mutex);\n\t\tmutex_unlock(&ctx->mutex);\n\t\tput_ctx(ctx);\n\t\tgoto again;\n\t}\n\tmutex_unlock(&event->child_mutex);\n\n\tlist_for_each_entry_safe(child, tmp, &free_list, child_list) {\n\t\tvoid *var = &child->ctx->refcount;\n\n\t\tlist_del(&child->child_list);\n\t\tfree_event(child);\n\n\t\t \n\t\tsmp_mb();  \n\t\twake_up_var(var);\n\t}\n\nno_ctx:\n\tput_event(event);  \n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(perf_event_release_kernel);\n\n \nstatic int perf_release(struct inode *inode, struct file *file)\n{\n\tperf_event_release_kernel(file->private_data);\n\treturn 0;\n}\n\nstatic u64 __perf_event_read_value(struct perf_event *event, u64 *enabled, u64 *running)\n{\n\tstruct perf_event *child;\n\tu64 total = 0;\n\n\t*enabled = 0;\n\t*running = 0;\n\n\tmutex_lock(&event->child_mutex);\n\n\t(void)perf_event_read(event, false);\n\ttotal += perf_event_count(event);\n\n\t*enabled += event->total_time_enabled +\n\t\t\tatomic64_read(&event->child_total_time_enabled);\n\t*running += event->total_time_running +\n\t\t\tatomic64_read(&event->child_total_time_running);\n\n\tlist_for_each_entry(child, &event->child_list, child_list) {\n\t\t(void)perf_event_read(child, false);\n\t\ttotal += perf_event_count(child);\n\t\t*enabled += child->total_time_enabled;\n\t\t*running += child->total_time_running;\n\t}\n\tmutex_unlock(&event->child_mutex);\n\n\treturn total;\n}\n\nu64 perf_event_read_value(struct perf_event *event, u64 *enabled, u64 *running)\n{\n\tstruct perf_event_context *ctx;\n\tu64 count;\n\n\tctx = perf_event_ctx_lock(event);\n\tcount = __perf_event_read_value(event, enabled, running);\n\tperf_event_ctx_unlock(event, ctx);\n\n\treturn count;\n}\nEXPORT_SYMBOL_GPL(perf_event_read_value);\n\nstatic int __perf_read_group_add(struct perf_event *leader,\n\t\t\t\t\tu64 read_format, u64 *values)\n{\n\tstruct perf_event_context *ctx = leader->ctx;\n\tstruct perf_event *sub, *parent;\n\tunsigned long flags;\n\tint n = 1;  \n\tint ret;\n\n\tret = perf_event_read(leader, true);\n\tif (ret)\n\t\treturn ret;\n\n\traw_spin_lock_irqsave(&ctx->lock, flags);\n\t \n\tparent = leader->parent;\n\tif (parent &&\n\t    (parent->group_generation != leader->group_generation ||\n\t     parent->nr_siblings != leader->nr_siblings)) {\n\t\tret = -ECHILD;\n\t\tgoto unlock;\n\t}\n\n\t \n\tif (read_format & PERF_FORMAT_TOTAL_TIME_ENABLED) {\n\t\tvalues[n++] += leader->total_time_enabled +\n\t\t\tatomic64_read(&leader->child_total_time_enabled);\n\t}\n\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_RUNNING) {\n\t\tvalues[n++] += leader->total_time_running +\n\t\t\tatomic64_read(&leader->child_total_time_running);\n\t}\n\n\t \n\tvalues[n++] += perf_event_count(leader);\n\tif (read_format & PERF_FORMAT_ID)\n\t\tvalues[n++] = primary_event_id(leader);\n\tif (read_format & PERF_FORMAT_LOST)\n\t\tvalues[n++] = atomic64_read(&leader->lost_samples);\n\n\tfor_each_sibling_event(sub, leader) {\n\t\tvalues[n++] += perf_event_count(sub);\n\t\tif (read_format & PERF_FORMAT_ID)\n\t\t\tvalues[n++] = primary_event_id(sub);\n\t\tif (read_format & PERF_FORMAT_LOST)\n\t\t\tvalues[n++] = atomic64_read(&sub->lost_samples);\n\t}\n\nunlock:\n\traw_spin_unlock_irqrestore(&ctx->lock, flags);\n\treturn ret;\n}\n\nstatic int perf_read_group(struct perf_event *event,\n\t\t\t\t   u64 read_format, char __user *buf)\n{\n\tstruct perf_event *leader = event->group_leader, *child;\n\tstruct perf_event_context *ctx = leader->ctx;\n\tint ret;\n\tu64 *values;\n\n\tlockdep_assert_held(&ctx->mutex);\n\n\tvalues = kzalloc(event->read_size, GFP_KERNEL);\n\tif (!values)\n\t\treturn -ENOMEM;\n\n\tvalues[0] = 1 + leader->nr_siblings;\n\n\tmutex_lock(&leader->child_mutex);\n\n\tret = __perf_read_group_add(leader, read_format, values);\n\tif (ret)\n\t\tgoto unlock;\n\n\tlist_for_each_entry(child, &leader->child_list, child_list) {\n\t\tret = __perf_read_group_add(child, read_format, values);\n\t\tif (ret)\n\t\t\tgoto unlock;\n\t}\n\n\tmutex_unlock(&leader->child_mutex);\n\n\tret = event->read_size;\n\tif (copy_to_user(buf, values, event->read_size))\n\t\tret = -EFAULT;\n\tgoto out;\n\nunlock:\n\tmutex_unlock(&leader->child_mutex);\nout:\n\tkfree(values);\n\treturn ret;\n}\n\nstatic int perf_read_one(struct perf_event *event,\n\t\t\t\t u64 read_format, char __user *buf)\n{\n\tu64 enabled, running;\n\tu64 values[5];\n\tint n = 0;\n\n\tvalues[n++] = __perf_event_read_value(event, &enabled, &running);\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_ENABLED)\n\t\tvalues[n++] = enabled;\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_RUNNING)\n\t\tvalues[n++] = running;\n\tif (read_format & PERF_FORMAT_ID)\n\t\tvalues[n++] = primary_event_id(event);\n\tif (read_format & PERF_FORMAT_LOST)\n\t\tvalues[n++] = atomic64_read(&event->lost_samples);\n\n\tif (copy_to_user(buf, values, n * sizeof(u64)))\n\t\treturn -EFAULT;\n\n\treturn n * sizeof(u64);\n}\n\nstatic bool is_event_hup(struct perf_event *event)\n{\n\tbool no_children;\n\n\tif (event->state > PERF_EVENT_STATE_EXIT)\n\t\treturn false;\n\n\tmutex_lock(&event->child_mutex);\n\tno_children = list_empty(&event->child_list);\n\tmutex_unlock(&event->child_mutex);\n\treturn no_children;\n}\n\n \nstatic ssize_t\n__perf_read(struct perf_event *event, char __user *buf, size_t count)\n{\n\tu64 read_format = event->attr.read_format;\n\tint ret;\n\n\t \n\tif (event->state == PERF_EVENT_STATE_ERROR)\n\t\treturn 0;\n\n\tif (count < event->read_size)\n\t\treturn -ENOSPC;\n\n\tWARN_ON_ONCE(event->ctx->parent_ctx);\n\tif (read_format & PERF_FORMAT_GROUP)\n\t\tret = perf_read_group(event, read_format, buf);\n\telse\n\t\tret = perf_read_one(event, read_format, buf);\n\n\treturn ret;\n}\n\nstatic ssize_t\nperf_read(struct file *file, char __user *buf, size_t count, loff_t *ppos)\n{\n\tstruct perf_event *event = file->private_data;\n\tstruct perf_event_context *ctx;\n\tint ret;\n\n\tret = security_perf_event_read(event);\n\tif (ret)\n\t\treturn ret;\n\n\tctx = perf_event_ctx_lock(event);\n\tret = __perf_read(event, buf, count);\n\tperf_event_ctx_unlock(event, ctx);\n\n\treturn ret;\n}\n\nstatic __poll_t perf_poll(struct file *file, poll_table *wait)\n{\n\tstruct perf_event *event = file->private_data;\n\tstruct perf_buffer *rb;\n\t__poll_t events = EPOLLHUP;\n\n\tpoll_wait(file, &event->waitq, wait);\n\n\tif (is_event_hup(event))\n\t\treturn events;\n\n\t \n\tmutex_lock(&event->mmap_mutex);\n\trb = event->rb;\n\tif (rb)\n\t\tevents = atomic_xchg(&rb->poll, 0);\n\tmutex_unlock(&event->mmap_mutex);\n\treturn events;\n}\n\nstatic void _perf_event_reset(struct perf_event *event)\n{\n\t(void)perf_event_read(event, false);\n\tlocal64_set(&event->count, 0);\n\tperf_event_update_userpage(event);\n}\n\n \nu64 perf_event_pause(struct perf_event *event, bool reset)\n{\n\tstruct perf_event_context *ctx;\n\tu64 count;\n\n\tctx = perf_event_ctx_lock(event);\n\tWARN_ON_ONCE(event->attr.inherit);\n\t_perf_event_disable(event);\n\tcount = local64_read(&event->count);\n\tif (reset)\n\t\tlocal64_set(&event->count, 0);\n\tperf_event_ctx_unlock(event, ctx);\n\n\treturn count;\n}\nEXPORT_SYMBOL_GPL(perf_event_pause);\n\n \nstatic void perf_event_for_each_child(struct perf_event *event,\n\t\t\t\t\tvoid (*func)(struct perf_event *))\n{\n\tstruct perf_event *child;\n\n\tWARN_ON_ONCE(event->ctx->parent_ctx);\n\n\tmutex_lock(&event->child_mutex);\n\tfunc(event);\n\tlist_for_each_entry(child, &event->child_list, child_list)\n\t\tfunc(child);\n\tmutex_unlock(&event->child_mutex);\n}\n\nstatic void perf_event_for_each(struct perf_event *event,\n\t\t\t\t  void (*func)(struct perf_event *))\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct perf_event *sibling;\n\n\tlockdep_assert_held(&ctx->mutex);\n\n\tevent = event->group_leader;\n\n\tperf_event_for_each_child(event, func);\n\tfor_each_sibling_event(sibling, event)\n\t\tperf_event_for_each_child(sibling, func);\n}\n\nstatic void __perf_event_period(struct perf_event *event,\n\t\t\t\tstruct perf_cpu_context *cpuctx,\n\t\t\t\tstruct perf_event_context *ctx,\n\t\t\t\tvoid *info)\n{\n\tu64 value = *((u64 *)info);\n\tbool active;\n\n\tif (event->attr.freq) {\n\t\tevent->attr.sample_freq = value;\n\t} else {\n\t\tevent->attr.sample_period = value;\n\t\tevent->hw.sample_period = value;\n\t}\n\n\tactive = (event->state == PERF_EVENT_STATE_ACTIVE);\n\tif (active) {\n\t\tperf_pmu_disable(event->pmu);\n\t\t \n\t\tif (event->hw.interrupts == MAX_INTERRUPTS) {\n\t\t\tevent->hw.interrupts = 0;\n\t\t\tperf_log_throttle(event, 1);\n\t\t}\n\t\tevent->pmu->stop(event, PERF_EF_UPDATE);\n\t}\n\n\tlocal64_set(&event->hw.period_left, 0);\n\n\tif (active) {\n\t\tevent->pmu->start(event, PERF_EF_RELOAD);\n\t\tperf_pmu_enable(event->pmu);\n\t}\n}\n\nstatic int perf_event_check_period(struct perf_event *event, u64 value)\n{\n\treturn event->pmu->check_period(event, value);\n}\n\nstatic int _perf_event_period(struct perf_event *event, u64 value)\n{\n\tif (!is_sampling_event(event))\n\t\treturn -EINVAL;\n\n\tif (!value)\n\t\treturn -EINVAL;\n\n\tif (event->attr.freq && value > sysctl_perf_event_sample_rate)\n\t\treturn -EINVAL;\n\n\tif (perf_event_check_period(event, value))\n\t\treturn -EINVAL;\n\n\tif (!event->attr.freq && (value & (1ULL << 63)))\n\t\treturn -EINVAL;\n\n\tevent_function_call(event, __perf_event_period, &value);\n\n\treturn 0;\n}\n\nint perf_event_period(struct perf_event *event, u64 value)\n{\n\tstruct perf_event_context *ctx;\n\tint ret;\n\n\tctx = perf_event_ctx_lock(event);\n\tret = _perf_event_period(event, value);\n\tperf_event_ctx_unlock(event, ctx);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(perf_event_period);\n\nstatic const struct file_operations perf_fops;\n\nstatic inline int perf_fget_light(int fd, struct fd *p)\n{\n\tstruct fd f = fdget(fd);\n\tif (!f.file)\n\t\treturn -EBADF;\n\n\tif (f.file->f_op != &perf_fops) {\n\t\tfdput(f);\n\t\treturn -EBADF;\n\t}\n\t*p = f;\n\treturn 0;\n}\n\nstatic int perf_event_set_output(struct perf_event *event,\n\t\t\t\t struct perf_event *output_event);\nstatic int perf_event_set_filter(struct perf_event *event, void __user *arg);\nstatic int perf_copy_attr(struct perf_event_attr __user *uattr,\n\t\t\t  struct perf_event_attr *attr);\n\nstatic long _perf_ioctl(struct perf_event *event, unsigned int cmd, unsigned long arg)\n{\n\tvoid (*func)(struct perf_event *);\n\tu32 flags = arg;\n\n\tswitch (cmd) {\n\tcase PERF_EVENT_IOC_ENABLE:\n\t\tfunc = _perf_event_enable;\n\t\tbreak;\n\tcase PERF_EVENT_IOC_DISABLE:\n\t\tfunc = _perf_event_disable;\n\t\tbreak;\n\tcase PERF_EVENT_IOC_RESET:\n\t\tfunc = _perf_event_reset;\n\t\tbreak;\n\n\tcase PERF_EVENT_IOC_REFRESH:\n\t\treturn _perf_event_refresh(event, arg);\n\n\tcase PERF_EVENT_IOC_PERIOD:\n\t{\n\t\tu64 value;\n\n\t\tif (copy_from_user(&value, (u64 __user *)arg, sizeof(value)))\n\t\t\treturn -EFAULT;\n\n\t\treturn _perf_event_period(event, value);\n\t}\n\tcase PERF_EVENT_IOC_ID:\n\t{\n\t\tu64 id = primary_event_id(event);\n\n\t\tif (copy_to_user((void __user *)arg, &id, sizeof(id)))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\t}\n\n\tcase PERF_EVENT_IOC_SET_OUTPUT:\n\t{\n\t\tint ret;\n\t\tif (arg != -1) {\n\t\t\tstruct perf_event *output_event;\n\t\t\tstruct fd output;\n\t\t\tret = perf_fget_light(arg, &output);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t\toutput_event = output.file->private_data;\n\t\t\tret = perf_event_set_output(event, output_event);\n\t\t\tfdput(output);\n\t\t} else {\n\t\t\tret = perf_event_set_output(event, NULL);\n\t\t}\n\t\treturn ret;\n\t}\n\n\tcase PERF_EVENT_IOC_SET_FILTER:\n\t\treturn perf_event_set_filter(event, (void __user *)arg);\n\n\tcase PERF_EVENT_IOC_SET_BPF:\n\t{\n\t\tstruct bpf_prog *prog;\n\t\tint err;\n\n\t\tprog = bpf_prog_get(arg);\n\t\tif (IS_ERR(prog))\n\t\t\treturn PTR_ERR(prog);\n\n\t\terr = perf_event_set_bpf_prog(event, prog, 0);\n\t\tif (err) {\n\t\t\tbpf_prog_put(prog);\n\t\t\treturn err;\n\t\t}\n\n\t\treturn 0;\n\t}\n\n\tcase PERF_EVENT_IOC_PAUSE_OUTPUT: {\n\t\tstruct perf_buffer *rb;\n\n\t\trcu_read_lock();\n\t\trb = rcu_dereference(event->rb);\n\t\tif (!rb || !rb->nr_pages) {\n\t\t\trcu_read_unlock();\n\t\t\treturn -EINVAL;\n\t\t}\n\t\trb_toggle_paused(rb, !!arg);\n\t\trcu_read_unlock();\n\t\treturn 0;\n\t}\n\n\tcase PERF_EVENT_IOC_QUERY_BPF:\n\t\treturn perf_event_query_prog_array(event, (void __user *)arg);\n\n\tcase PERF_EVENT_IOC_MODIFY_ATTRIBUTES: {\n\t\tstruct perf_event_attr new_attr;\n\t\tint err = perf_copy_attr((struct perf_event_attr __user *)arg,\n\t\t\t\t\t &new_attr);\n\n\t\tif (err)\n\t\t\treturn err;\n\n\t\treturn perf_event_modify_attr(event,  &new_attr);\n\t}\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\tif (flags & PERF_IOC_FLAG_GROUP)\n\t\tperf_event_for_each(event, func);\n\telse\n\t\tperf_event_for_each_child(event, func);\n\n\treturn 0;\n}\n\nstatic long perf_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tstruct perf_event *event = file->private_data;\n\tstruct perf_event_context *ctx;\n\tlong ret;\n\n\t \n\tret = security_perf_event_write(event);\n\tif (ret)\n\t\treturn ret;\n\n\tctx = perf_event_ctx_lock(event);\n\tret = _perf_ioctl(event, cmd, arg);\n\tperf_event_ctx_unlock(event, ctx);\n\n\treturn ret;\n}\n\n#ifdef CONFIG_COMPAT\nstatic long perf_compat_ioctl(struct file *file, unsigned int cmd,\n\t\t\t\tunsigned long arg)\n{\n\tswitch (_IOC_NR(cmd)) {\n\tcase _IOC_NR(PERF_EVENT_IOC_SET_FILTER):\n\tcase _IOC_NR(PERF_EVENT_IOC_ID):\n\tcase _IOC_NR(PERF_EVENT_IOC_QUERY_BPF):\n\tcase _IOC_NR(PERF_EVENT_IOC_MODIFY_ATTRIBUTES):\n\t\t \n\t\tif (_IOC_SIZE(cmd) == sizeof(compat_uptr_t)) {\n\t\t\tcmd &= ~IOCSIZE_MASK;\n\t\t\tcmd |= sizeof(void *) << IOCSIZE_SHIFT;\n\t\t}\n\t\tbreak;\n\t}\n\treturn perf_ioctl(file, cmd, arg);\n}\n#else\n# define perf_compat_ioctl NULL\n#endif\n\nint perf_event_task_enable(void)\n{\n\tstruct perf_event_context *ctx;\n\tstruct perf_event *event;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_for_each_entry(event, &current->perf_event_list, owner_entry) {\n\t\tctx = perf_event_ctx_lock(event);\n\t\tperf_event_for_each_child(event, _perf_event_enable);\n\t\tperf_event_ctx_unlock(event, ctx);\n\t}\n\tmutex_unlock(&current->perf_event_mutex);\n\n\treturn 0;\n}\n\nint perf_event_task_disable(void)\n{\n\tstruct perf_event_context *ctx;\n\tstruct perf_event *event;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_for_each_entry(event, &current->perf_event_list, owner_entry) {\n\t\tctx = perf_event_ctx_lock(event);\n\t\tperf_event_for_each_child(event, _perf_event_disable);\n\t\tperf_event_ctx_unlock(event, ctx);\n\t}\n\tmutex_unlock(&current->perf_event_mutex);\n\n\treturn 0;\n}\n\nstatic int perf_event_index(struct perf_event *event)\n{\n\tif (event->hw.state & PERF_HES_STOPPED)\n\t\treturn 0;\n\n\tif (event->state != PERF_EVENT_STATE_ACTIVE)\n\t\treturn 0;\n\n\treturn event->pmu->event_idx(event);\n}\n\nstatic void perf_event_init_userpage(struct perf_event *event)\n{\n\tstruct perf_event_mmap_page *userpg;\n\tstruct perf_buffer *rb;\n\n\trcu_read_lock();\n\trb = rcu_dereference(event->rb);\n\tif (!rb)\n\t\tgoto unlock;\n\n\tuserpg = rb->user_page;\n\n\t \n\tuserpg->cap_bit0_is_deprecated = 1;\n\tuserpg->size = offsetof(struct perf_event_mmap_page, __reserved);\n\tuserpg->data_offset = PAGE_SIZE;\n\tuserpg->data_size = perf_data_size(rb);\n\nunlock:\n\trcu_read_unlock();\n}\n\nvoid __weak arch_perf_update_userpage(\n\tstruct perf_event *event, struct perf_event_mmap_page *userpg, u64 now)\n{\n}\n\n \nvoid perf_event_update_userpage(struct perf_event *event)\n{\n\tstruct perf_event_mmap_page *userpg;\n\tstruct perf_buffer *rb;\n\tu64 enabled, running, now;\n\n\trcu_read_lock();\n\trb = rcu_dereference(event->rb);\n\tif (!rb)\n\t\tgoto unlock;\n\n\t \n\tcalc_timer_values(event, &now, &enabled, &running);\n\n\tuserpg = rb->user_page;\n\t \n\tpreempt_disable();\n\t++userpg->lock;\n\tbarrier();\n\tuserpg->index = perf_event_index(event);\n\tuserpg->offset = perf_event_count(event);\n\tif (userpg->index)\n\t\tuserpg->offset -= local64_read(&event->hw.prev_count);\n\n\tuserpg->time_enabled = enabled +\n\t\t\tatomic64_read(&event->child_total_time_enabled);\n\n\tuserpg->time_running = running +\n\t\t\tatomic64_read(&event->child_total_time_running);\n\n\tarch_perf_update_userpage(event, userpg, now);\n\n\tbarrier();\n\t++userpg->lock;\n\tpreempt_enable();\nunlock:\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL_GPL(perf_event_update_userpage);\n\nstatic vm_fault_t perf_mmap_fault(struct vm_fault *vmf)\n{\n\tstruct perf_event *event = vmf->vma->vm_file->private_data;\n\tstruct perf_buffer *rb;\n\tvm_fault_t ret = VM_FAULT_SIGBUS;\n\n\tif (vmf->flags & FAULT_FLAG_MKWRITE) {\n\t\tif (vmf->pgoff == 0)\n\t\t\tret = 0;\n\t\treturn ret;\n\t}\n\n\trcu_read_lock();\n\trb = rcu_dereference(event->rb);\n\tif (!rb)\n\t\tgoto unlock;\n\n\tif (vmf->pgoff && (vmf->flags & FAULT_FLAG_WRITE))\n\t\tgoto unlock;\n\n\tvmf->page = perf_mmap_to_page(rb, vmf->pgoff);\n\tif (!vmf->page)\n\t\tgoto unlock;\n\n\tget_page(vmf->page);\n\tvmf->page->mapping = vmf->vma->vm_file->f_mapping;\n\tvmf->page->index   = vmf->pgoff;\n\n\tret = 0;\nunlock:\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\nstatic void ring_buffer_attach(struct perf_event *event,\n\t\t\t       struct perf_buffer *rb)\n{\n\tstruct perf_buffer *old_rb = NULL;\n\tunsigned long flags;\n\n\tWARN_ON_ONCE(event->parent);\n\n\tif (event->rb) {\n\t\t \n\t\tWARN_ON_ONCE(event->rcu_pending);\n\n\t\told_rb = event->rb;\n\t\tspin_lock_irqsave(&old_rb->event_lock, flags);\n\t\tlist_del_rcu(&event->rb_entry);\n\t\tspin_unlock_irqrestore(&old_rb->event_lock, flags);\n\n\t\tevent->rcu_batches = get_state_synchronize_rcu();\n\t\tevent->rcu_pending = 1;\n\t}\n\n\tif (rb) {\n\t\tif (event->rcu_pending) {\n\t\t\tcond_synchronize_rcu(event->rcu_batches);\n\t\t\tevent->rcu_pending = 0;\n\t\t}\n\n\t\tspin_lock_irqsave(&rb->event_lock, flags);\n\t\tlist_add_rcu(&event->rb_entry, &rb->event_list);\n\t\tspin_unlock_irqrestore(&rb->event_lock, flags);\n\t}\n\n\t \n\tif (has_aux(event))\n\t\tperf_event_stop(event, 0);\n\n\trcu_assign_pointer(event->rb, rb);\n\n\tif (old_rb) {\n\t\tring_buffer_put(old_rb);\n\t\t \n\t\twake_up_all(&event->waitq);\n\t}\n}\n\nstatic void ring_buffer_wakeup(struct perf_event *event)\n{\n\tstruct perf_buffer *rb;\n\n\tif (event->parent)\n\t\tevent = event->parent;\n\n\trcu_read_lock();\n\trb = rcu_dereference(event->rb);\n\tif (rb) {\n\t\tlist_for_each_entry_rcu(event, &rb->event_list, rb_entry)\n\t\t\twake_up_all(&event->waitq);\n\t}\n\trcu_read_unlock();\n}\n\nstruct perf_buffer *ring_buffer_get(struct perf_event *event)\n{\n\tstruct perf_buffer *rb;\n\n\tif (event->parent)\n\t\tevent = event->parent;\n\n\trcu_read_lock();\n\trb = rcu_dereference(event->rb);\n\tif (rb) {\n\t\tif (!refcount_inc_not_zero(&rb->refcount))\n\t\t\trb = NULL;\n\t}\n\trcu_read_unlock();\n\n\treturn rb;\n}\n\nvoid ring_buffer_put(struct perf_buffer *rb)\n{\n\tif (!refcount_dec_and_test(&rb->refcount))\n\t\treturn;\n\n\tWARN_ON_ONCE(!list_empty(&rb->event_list));\n\n\tcall_rcu(&rb->rcu_head, rb_free_rcu);\n}\n\nstatic void perf_mmap_open(struct vm_area_struct *vma)\n{\n\tstruct perf_event *event = vma->vm_file->private_data;\n\n\tatomic_inc(&event->mmap_count);\n\tatomic_inc(&event->rb->mmap_count);\n\n\tif (vma->vm_pgoff)\n\t\tatomic_inc(&event->rb->aux_mmap_count);\n\n\tif (event->pmu->event_mapped)\n\t\tevent->pmu->event_mapped(event, vma->vm_mm);\n}\n\nstatic void perf_pmu_output_stop(struct perf_event *event);\n\n \nstatic void perf_mmap_close(struct vm_area_struct *vma)\n{\n\tstruct perf_event *event = vma->vm_file->private_data;\n\tstruct perf_buffer *rb = ring_buffer_get(event);\n\tstruct user_struct *mmap_user = rb->mmap_user;\n\tint mmap_locked = rb->mmap_locked;\n\tunsigned long size = perf_data_size(rb);\n\tbool detach_rest = false;\n\n\tif (event->pmu->event_unmapped)\n\t\tevent->pmu->event_unmapped(event, vma->vm_mm);\n\n\t \n\tif (rb_has_aux(rb) && vma->vm_pgoff == rb->aux_pgoff &&\n\t    atomic_dec_and_mutex_lock(&rb->aux_mmap_count, &event->mmap_mutex)) {\n\t\t \n\t\tperf_pmu_output_stop(event);\n\n\t\t \n\t\tatomic_long_sub(rb->aux_nr_pages - rb->aux_mmap_locked, &mmap_user->locked_vm);\n\t\tatomic64_sub(rb->aux_mmap_locked, &vma->vm_mm->pinned_vm);\n\n\t\t \n\t\trb_free_aux(rb);\n\t\tWARN_ON_ONCE(refcount_read(&rb->aux_refcount));\n\n\t\tmutex_unlock(&event->mmap_mutex);\n\t}\n\n\tif (atomic_dec_and_test(&rb->mmap_count))\n\t\tdetach_rest = true;\n\n\tif (!atomic_dec_and_mutex_lock(&event->mmap_count, &event->mmap_mutex))\n\t\tgoto out_put;\n\n\tring_buffer_attach(event, NULL);\n\tmutex_unlock(&event->mmap_mutex);\n\n\t \n\tif (!detach_rest)\n\t\tgoto out_put;\n\n\t \nagain:\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(event, &rb->event_list, rb_entry) {\n\t\tif (!atomic_long_inc_not_zero(&event->refcount)) {\n\t\t\t \n\t\t\tcontinue;\n\t\t}\n\t\trcu_read_unlock();\n\n\t\tmutex_lock(&event->mmap_mutex);\n\t\t \n\t\tif (event->rb == rb)\n\t\t\tring_buffer_attach(event, NULL);\n\n\t\tmutex_unlock(&event->mmap_mutex);\n\t\tput_event(event);\n\n\t\t \n\t\tgoto again;\n\t}\n\trcu_read_unlock();\n\n\t \n\n\tatomic_long_sub((size >> PAGE_SHIFT) + 1 - mmap_locked,\n\t\t\t&mmap_user->locked_vm);\n\tatomic64_sub(mmap_locked, &vma->vm_mm->pinned_vm);\n\tfree_uid(mmap_user);\n\nout_put:\n\tring_buffer_put(rb);  \n}\n\nstatic const struct vm_operations_struct perf_mmap_vmops = {\n\t.open\t\t= perf_mmap_open,\n\t.close\t\t= perf_mmap_close,  \n\t.fault\t\t= perf_mmap_fault,\n\t.page_mkwrite\t= perf_mmap_fault,\n};\n\nstatic int perf_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\tstruct perf_event *event = file->private_data;\n\tunsigned long user_locked, user_lock_limit;\n\tstruct user_struct *user = current_user();\n\tstruct perf_buffer *rb = NULL;\n\tunsigned long locked, lock_limit;\n\tunsigned long vma_size;\n\tunsigned long nr_pages;\n\tlong user_extra = 0, extra = 0;\n\tint ret = 0, flags = 0;\n\n\t \n\tif (event->cpu == -1 && event->attr.inherit)\n\t\treturn -EINVAL;\n\n\tif (!(vma->vm_flags & VM_SHARED))\n\t\treturn -EINVAL;\n\n\tret = security_perf_event_read(event);\n\tif (ret)\n\t\treturn ret;\n\n\tvma_size = vma->vm_end - vma->vm_start;\n\n\tif (vma->vm_pgoff == 0) {\n\t\tnr_pages = (vma_size / PAGE_SIZE) - 1;\n\t} else {\n\t\t \n\t\tu64 aux_offset, aux_size;\n\n\t\tif (!event->rb)\n\t\t\treturn -EINVAL;\n\n\t\tnr_pages = vma_size / PAGE_SIZE;\n\n\t\tmutex_lock(&event->mmap_mutex);\n\t\tret = -EINVAL;\n\n\t\trb = event->rb;\n\t\tif (!rb)\n\t\t\tgoto aux_unlock;\n\n\t\taux_offset = READ_ONCE(rb->user_page->aux_offset);\n\t\taux_size = READ_ONCE(rb->user_page->aux_size);\n\n\t\tif (aux_offset < perf_data_size(rb) + PAGE_SIZE)\n\t\t\tgoto aux_unlock;\n\n\t\tif (aux_offset != vma->vm_pgoff << PAGE_SHIFT)\n\t\t\tgoto aux_unlock;\n\n\t\t \n\t\tif (rb_has_aux(rb) && rb->aux_pgoff != vma->vm_pgoff)\n\t\t\tgoto aux_unlock;\n\n\t\tif (aux_size != vma_size || aux_size != nr_pages * PAGE_SIZE)\n\t\t\tgoto aux_unlock;\n\n\t\t \n\t\tif (rb_has_aux(rb) && rb->aux_nr_pages != nr_pages)\n\t\t\tgoto aux_unlock;\n\n\t\tif (!is_power_of_2(nr_pages))\n\t\t\tgoto aux_unlock;\n\n\t\tif (!atomic_inc_not_zero(&rb->mmap_count))\n\t\t\tgoto aux_unlock;\n\n\t\tif (rb_has_aux(rb)) {\n\t\t\tatomic_inc(&rb->aux_mmap_count);\n\t\t\tret = 0;\n\t\t\tgoto unlock;\n\t\t}\n\n\t\tatomic_set(&rb->aux_mmap_count, 1);\n\t\tuser_extra = nr_pages;\n\n\t\tgoto accounting;\n\t}\n\n\t \n\tif (nr_pages != 0 && !is_power_of_2(nr_pages))\n\t\treturn -EINVAL;\n\n\tif (vma_size != PAGE_SIZE * (1 + nr_pages))\n\t\treturn -EINVAL;\n\n\tWARN_ON_ONCE(event->ctx->parent_ctx);\nagain:\n\tmutex_lock(&event->mmap_mutex);\n\tif (event->rb) {\n\t\tif (data_page_nr(event->rb) != nr_pages) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto unlock;\n\t\t}\n\n\t\tif (!atomic_inc_not_zero(&event->rb->mmap_count)) {\n\t\t\t \n\t\t\tring_buffer_attach(event, NULL);\n\t\t\tmutex_unlock(&event->mmap_mutex);\n\t\t\tgoto again;\n\t\t}\n\n\t\tgoto unlock;\n\t}\n\n\tuser_extra = nr_pages + 1;\n\naccounting:\n\tuser_lock_limit = sysctl_perf_event_mlock >> (PAGE_SHIFT - 10);\n\n\t \n\tuser_lock_limit *= num_online_cpus();\n\n\tuser_locked = atomic_long_read(&user->locked_vm);\n\n\t \n\tif (user_locked > user_lock_limit)\n\t\tuser_locked = user_lock_limit;\n\tuser_locked += user_extra;\n\n\tif (user_locked > user_lock_limit) {\n\t\t \n\t\textra = user_locked - user_lock_limit;\n\t\tuser_extra -= extra;\n\t}\n\n\tlock_limit = rlimit(RLIMIT_MEMLOCK);\n\tlock_limit >>= PAGE_SHIFT;\n\tlocked = atomic64_read(&vma->vm_mm->pinned_vm) + extra;\n\n\tif ((locked > lock_limit) && perf_is_paranoid() &&\n\t\t!capable(CAP_IPC_LOCK)) {\n\t\tret = -EPERM;\n\t\tgoto unlock;\n\t}\n\n\tWARN_ON(!rb && event->rb);\n\n\tif (vma->vm_flags & VM_WRITE)\n\t\tflags |= RING_BUFFER_WRITABLE;\n\n\tif (!rb) {\n\t\trb = rb_alloc(nr_pages,\n\t\t\t      event->attr.watermark ? event->attr.wakeup_watermark : 0,\n\t\t\t      event->cpu, flags);\n\n\t\tif (!rb) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto unlock;\n\t\t}\n\n\t\tatomic_set(&rb->mmap_count, 1);\n\t\trb->mmap_user = get_current_user();\n\t\trb->mmap_locked = extra;\n\n\t\tring_buffer_attach(event, rb);\n\n\t\tperf_event_update_time(event);\n\t\tperf_event_init_userpage(event);\n\t\tperf_event_update_userpage(event);\n\t} else {\n\t\tret = rb_alloc_aux(rb, event, vma->vm_pgoff, nr_pages,\n\t\t\t\t   event->attr.aux_watermark, flags);\n\t\tif (!ret)\n\t\t\trb->aux_mmap_locked = extra;\n\t}\n\nunlock:\n\tif (!ret) {\n\t\tatomic_long_add(user_extra, &user->locked_vm);\n\t\tatomic64_add(extra, &vma->vm_mm->pinned_vm);\n\n\t\tatomic_inc(&event->mmap_count);\n\t} else if (rb) {\n\t\tatomic_dec(&rb->mmap_count);\n\t}\naux_unlock:\n\tmutex_unlock(&event->mmap_mutex);\n\n\t \n\tvm_flags_set(vma, VM_DONTCOPY | VM_DONTEXPAND | VM_DONTDUMP);\n\tvma->vm_ops = &perf_mmap_vmops;\n\n\tif (event->pmu->event_mapped)\n\t\tevent->pmu->event_mapped(event, vma->vm_mm);\n\n\treturn ret;\n}\n\nstatic int perf_fasync(int fd, struct file *filp, int on)\n{\n\tstruct inode *inode = file_inode(filp);\n\tstruct perf_event *event = filp->private_data;\n\tint retval;\n\n\tinode_lock(inode);\n\tretval = fasync_helper(fd, filp, on, &event->fasync);\n\tinode_unlock(inode);\n\n\tif (retval < 0)\n\t\treturn retval;\n\n\treturn 0;\n}\n\nstatic const struct file_operations perf_fops = {\n\t.llseek\t\t\t= no_llseek,\n\t.release\t\t= perf_release,\n\t.read\t\t\t= perf_read,\n\t.poll\t\t\t= perf_poll,\n\t.unlocked_ioctl\t\t= perf_ioctl,\n\t.compat_ioctl\t\t= perf_compat_ioctl,\n\t.mmap\t\t\t= perf_mmap,\n\t.fasync\t\t\t= perf_fasync,\n};\n\n \n\nstatic inline struct fasync_struct **perf_event_fasync(struct perf_event *event)\n{\n\t \n\tif (event->parent)\n\t\tevent = event->parent;\n\treturn &event->fasync;\n}\n\nvoid perf_event_wakeup(struct perf_event *event)\n{\n\tring_buffer_wakeup(event);\n\n\tif (event->pending_kill) {\n\t\tkill_fasync(perf_event_fasync(event), SIGIO, event->pending_kill);\n\t\tevent->pending_kill = 0;\n\t}\n}\n\nstatic void perf_sigtrap(struct perf_event *event)\n{\n\t \n\tif (WARN_ON_ONCE(event->ctx->task != current))\n\t\treturn;\n\n\t \n\tif (current->flags & PF_EXITING)\n\t\treturn;\n\n\tsend_sig_perf((void __user *)event->pending_addr,\n\t\t      event->orig_type, event->attr.sig_data);\n}\n\n \nstatic void __perf_pending_irq(struct perf_event *event)\n{\n\tint cpu = READ_ONCE(event->oncpu);\n\n\t \n\tif (cpu < 0)\n\t\treturn;\n\n\t \n\tif (cpu == smp_processor_id()) {\n\t\tif (event->pending_sigtrap) {\n\t\t\tevent->pending_sigtrap = 0;\n\t\t\tperf_sigtrap(event);\n\t\t\tlocal_dec(&event->ctx->nr_pending);\n\t\t}\n\t\tif (event->pending_disable) {\n\t\t\tevent->pending_disable = 0;\n\t\t\tperf_event_disable_local(event);\n\t\t}\n\t\treturn;\n\t}\n\n\t \n\tirq_work_queue_on(&event->pending_irq, cpu);\n}\n\nstatic void perf_pending_irq(struct irq_work *entry)\n{\n\tstruct perf_event *event = container_of(entry, struct perf_event, pending_irq);\n\tint rctx;\n\n\t \n\trctx = perf_swevent_get_recursion_context();\n\n\t \n\tif (event->pending_wakeup) {\n\t\tevent->pending_wakeup = 0;\n\t\tperf_event_wakeup(event);\n\t}\n\n\t__perf_pending_irq(event);\n\n\tif (rctx >= 0)\n\t\tperf_swevent_put_recursion_context(rctx);\n}\n\nstatic void perf_pending_task(struct callback_head *head)\n{\n\tstruct perf_event *event = container_of(head, struct perf_event, pending_task);\n\tint rctx;\n\n\t \n\tpreempt_disable_notrace();\n\trctx = perf_swevent_get_recursion_context();\n\n\tif (event->pending_work) {\n\t\tevent->pending_work = 0;\n\t\tperf_sigtrap(event);\n\t\tlocal_dec(&event->ctx->nr_pending);\n\t}\n\n\tif (rctx >= 0)\n\t\tperf_swevent_put_recursion_context(rctx);\n\tpreempt_enable_notrace();\n\n\tput_event(event);\n}\n\n#ifdef CONFIG_GUEST_PERF_EVENTS\nstruct perf_guest_info_callbacks __rcu *perf_guest_cbs;\n\nDEFINE_STATIC_CALL_RET0(__perf_guest_state, *perf_guest_cbs->state);\nDEFINE_STATIC_CALL_RET0(__perf_guest_get_ip, *perf_guest_cbs->get_ip);\nDEFINE_STATIC_CALL_RET0(__perf_guest_handle_intel_pt_intr, *perf_guest_cbs->handle_intel_pt_intr);\n\nvoid perf_register_guest_info_callbacks(struct perf_guest_info_callbacks *cbs)\n{\n\tif (WARN_ON_ONCE(rcu_access_pointer(perf_guest_cbs)))\n\t\treturn;\n\n\trcu_assign_pointer(perf_guest_cbs, cbs);\n\tstatic_call_update(__perf_guest_state, cbs->state);\n\tstatic_call_update(__perf_guest_get_ip, cbs->get_ip);\n\n\t \n\tif (cbs->handle_intel_pt_intr)\n\t\tstatic_call_update(__perf_guest_handle_intel_pt_intr,\n\t\t\t\t   cbs->handle_intel_pt_intr);\n}\nEXPORT_SYMBOL_GPL(perf_register_guest_info_callbacks);\n\nvoid perf_unregister_guest_info_callbacks(struct perf_guest_info_callbacks *cbs)\n{\n\tif (WARN_ON_ONCE(rcu_access_pointer(perf_guest_cbs) != cbs))\n\t\treturn;\n\n\trcu_assign_pointer(perf_guest_cbs, NULL);\n\tstatic_call_update(__perf_guest_state, (void *)&__static_call_return0);\n\tstatic_call_update(__perf_guest_get_ip, (void *)&__static_call_return0);\n\tstatic_call_update(__perf_guest_handle_intel_pt_intr,\n\t\t\t   (void *)&__static_call_return0);\n\tsynchronize_rcu();\n}\nEXPORT_SYMBOL_GPL(perf_unregister_guest_info_callbacks);\n#endif\n\nstatic void\nperf_output_sample_regs(struct perf_output_handle *handle,\n\t\t\tstruct pt_regs *regs, u64 mask)\n{\n\tint bit;\n\tDECLARE_BITMAP(_mask, 64);\n\n\tbitmap_from_u64(_mask, mask);\n\tfor_each_set_bit(bit, _mask, sizeof(mask) * BITS_PER_BYTE) {\n\t\tu64 val;\n\n\t\tval = perf_reg_value(regs, bit);\n\t\tperf_output_put(handle, val);\n\t}\n}\n\nstatic void perf_sample_regs_user(struct perf_regs *regs_user,\n\t\t\t\t  struct pt_regs *regs)\n{\n\tif (user_mode(regs)) {\n\t\tregs_user->abi = perf_reg_abi(current);\n\t\tregs_user->regs = regs;\n\t} else if (!(current->flags & PF_KTHREAD)) {\n\t\tperf_get_regs_user(regs_user, regs);\n\t} else {\n\t\tregs_user->abi = PERF_SAMPLE_REGS_ABI_NONE;\n\t\tregs_user->regs = NULL;\n\t}\n}\n\nstatic void perf_sample_regs_intr(struct perf_regs *regs_intr,\n\t\t\t\t  struct pt_regs *regs)\n{\n\tregs_intr->regs = regs;\n\tregs_intr->abi  = perf_reg_abi(current);\n}\n\n\n \nstatic u64 perf_ustack_task_size(struct pt_regs *regs)\n{\n\tunsigned long addr = perf_user_stack_pointer(regs);\n\n\tif (!addr || addr >= TASK_SIZE)\n\t\treturn 0;\n\n\treturn TASK_SIZE - addr;\n}\n\nstatic u16\nperf_sample_ustack_size(u16 stack_size, u16 header_size,\n\t\t\tstruct pt_regs *regs)\n{\n\tu64 task_size;\n\n\t \n\tif (!regs)\n\t\treturn 0;\n\n\t \n\n\ttask_size  = min((u64) USHRT_MAX, perf_ustack_task_size(regs));\n\tstack_size = min(stack_size, (u16) task_size);\n\n\t \n\theader_size += 2 * sizeof(u64);\n\n\t \n\tif ((u16) (header_size + stack_size) < header_size) {\n\t\t \n\t\tstack_size = USHRT_MAX - header_size - sizeof(u64);\n\t\tstack_size = round_up(stack_size, sizeof(u64));\n\t}\n\n\treturn stack_size;\n}\n\nstatic void\nperf_output_sample_ustack(struct perf_output_handle *handle, u64 dump_size,\n\t\t\t  struct pt_regs *regs)\n{\n\t \n\tif (!regs) {\n\t\tu64 size = 0;\n\t\tperf_output_put(handle, size);\n\t} else {\n\t\tunsigned long sp;\n\t\tunsigned int rem;\n\t\tu64 dyn_size;\n\n\t\t \n\n\t\t \n\t\tperf_output_put(handle, dump_size);\n\n\t\t \n\t\tsp = perf_user_stack_pointer(regs);\n\t\trem = __output_copy_user(handle, (void *) sp, dump_size);\n\t\tdyn_size = dump_size - rem;\n\n\t\tperf_output_skip(handle, rem);\n\n\t\t \n\t\tperf_output_put(handle, dyn_size);\n\t}\n}\n\nstatic unsigned long perf_prepare_sample_aux(struct perf_event *event,\n\t\t\t\t\t  struct perf_sample_data *data,\n\t\t\t\t\t  size_t size)\n{\n\tstruct perf_event *sampler = event->aux_event;\n\tstruct perf_buffer *rb;\n\n\tdata->aux_size = 0;\n\n\tif (!sampler)\n\t\tgoto out;\n\n\tif (WARN_ON_ONCE(READ_ONCE(sampler->state) != PERF_EVENT_STATE_ACTIVE))\n\t\tgoto out;\n\n\tif (WARN_ON_ONCE(READ_ONCE(sampler->oncpu) != smp_processor_id()))\n\t\tgoto out;\n\n\trb = ring_buffer_get(sampler);\n\tif (!rb)\n\t\tgoto out;\n\n\t \n\tif (READ_ONCE(rb->aux_in_sampling)) {\n\t\tdata->aux_size = 0;\n\t} else {\n\t\tsize = min_t(size_t, size, perf_aux_size(rb));\n\t\tdata->aux_size = ALIGN(size, sizeof(u64));\n\t}\n\tring_buffer_put(rb);\n\nout:\n\treturn data->aux_size;\n}\n\nstatic long perf_pmu_snapshot_aux(struct perf_buffer *rb,\n                                 struct perf_event *event,\n                                 struct perf_output_handle *handle,\n                                 unsigned long size)\n{\n\tunsigned long flags;\n\tlong ret;\n\n\t \n\tlocal_irq_save(flags);\n\t \n\tWRITE_ONCE(rb->aux_in_sampling, 1);\n\tbarrier();\n\n\tret = event->pmu->snapshot_aux(event, handle, size);\n\n\tbarrier();\n\tWRITE_ONCE(rb->aux_in_sampling, 0);\n\tlocal_irq_restore(flags);\n\n\treturn ret;\n}\n\nstatic void perf_aux_sample_output(struct perf_event *event,\n\t\t\t\t   struct perf_output_handle *handle,\n\t\t\t\t   struct perf_sample_data *data)\n{\n\tstruct perf_event *sampler = event->aux_event;\n\tstruct perf_buffer *rb;\n\tunsigned long pad;\n\tlong size;\n\n\tif (WARN_ON_ONCE(!sampler || !data->aux_size))\n\t\treturn;\n\n\trb = ring_buffer_get(sampler);\n\tif (!rb)\n\t\treturn;\n\n\tsize = perf_pmu_snapshot_aux(rb, sampler, handle, data->aux_size);\n\n\t \n\tif (WARN_ON_ONCE(size < 0))\n\t\tgoto out_put;\n\n\t \n\tpad = data->aux_size - size;\n\tif (WARN_ON_ONCE(pad >= sizeof(u64)))\n\t\tpad = 8;\n\n\tif (pad) {\n\t\tu64 zero = 0;\n\t\tperf_output_copy(handle, &zero, pad);\n\t}\n\nout_put:\n\tring_buffer_put(rb);\n}\n\n \n#define PERF_SAMPLE_ID_ALL  (PERF_SAMPLE_TID | PERF_SAMPLE_TIME |\t\\\n\t\t\t     PERF_SAMPLE_ID | PERF_SAMPLE_STREAM_ID |\t\\\n\t\t\t     PERF_SAMPLE_CPU | PERF_SAMPLE_IDENTIFIER)\n\nstatic void __perf_event_header__init_id(struct perf_sample_data *data,\n\t\t\t\t\t struct perf_event *event,\n\t\t\t\t\t u64 sample_type)\n{\n\tdata->type = event->attr.sample_type;\n\tdata->sample_flags |= data->type & PERF_SAMPLE_ID_ALL;\n\n\tif (sample_type & PERF_SAMPLE_TID) {\n\t\t \n\t\tdata->tid_entry.pid = perf_event_pid(event, current);\n\t\tdata->tid_entry.tid = perf_event_tid(event, current);\n\t}\n\n\tif (sample_type & PERF_SAMPLE_TIME)\n\t\tdata->time = perf_event_clock(event);\n\n\tif (sample_type & (PERF_SAMPLE_ID | PERF_SAMPLE_IDENTIFIER))\n\t\tdata->id = primary_event_id(event);\n\n\tif (sample_type & PERF_SAMPLE_STREAM_ID)\n\t\tdata->stream_id = event->id;\n\n\tif (sample_type & PERF_SAMPLE_CPU) {\n\t\tdata->cpu_entry.cpu\t = raw_smp_processor_id();\n\t\tdata->cpu_entry.reserved = 0;\n\t}\n}\n\nvoid perf_event_header__init_id(struct perf_event_header *header,\n\t\t\t\tstruct perf_sample_data *data,\n\t\t\t\tstruct perf_event *event)\n{\n\tif (event->attr.sample_id_all) {\n\t\theader->size += event->id_header_size;\n\t\t__perf_event_header__init_id(data, event, event->attr.sample_type);\n\t}\n}\n\nstatic void __perf_event__output_id_sample(struct perf_output_handle *handle,\n\t\t\t\t\t   struct perf_sample_data *data)\n{\n\tu64 sample_type = data->type;\n\n\tif (sample_type & PERF_SAMPLE_TID)\n\t\tperf_output_put(handle, data->tid_entry);\n\n\tif (sample_type & PERF_SAMPLE_TIME)\n\t\tperf_output_put(handle, data->time);\n\n\tif (sample_type & PERF_SAMPLE_ID)\n\t\tperf_output_put(handle, data->id);\n\n\tif (sample_type & PERF_SAMPLE_STREAM_ID)\n\t\tperf_output_put(handle, data->stream_id);\n\n\tif (sample_type & PERF_SAMPLE_CPU)\n\t\tperf_output_put(handle, data->cpu_entry);\n\n\tif (sample_type & PERF_SAMPLE_IDENTIFIER)\n\t\tperf_output_put(handle, data->id);\n}\n\nvoid perf_event__output_id_sample(struct perf_event *event,\n\t\t\t\t  struct perf_output_handle *handle,\n\t\t\t\t  struct perf_sample_data *sample)\n{\n\tif (event->attr.sample_id_all)\n\t\t__perf_event__output_id_sample(handle, sample);\n}\n\nstatic void perf_output_read_one(struct perf_output_handle *handle,\n\t\t\t\t struct perf_event *event,\n\t\t\t\t u64 enabled, u64 running)\n{\n\tu64 read_format = event->attr.read_format;\n\tu64 values[5];\n\tint n = 0;\n\n\tvalues[n++] = perf_event_count(event);\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_ENABLED) {\n\t\tvalues[n++] = enabled +\n\t\t\tatomic64_read(&event->child_total_time_enabled);\n\t}\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_RUNNING) {\n\t\tvalues[n++] = running +\n\t\t\tatomic64_read(&event->child_total_time_running);\n\t}\n\tif (read_format & PERF_FORMAT_ID)\n\t\tvalues[n++] = primary_event_id(event);\n\tif (read_format & PERF_FORMAT_LOST)\n\t\tvalues[n++] = atomic64_read(&event->lost_samples);\n\n\t__output_copy(handle, values, n * sizeof(u64));\n}\n\nstatic void perf_output_read_group(struct perf_output_handle *handle,\n\t\t\t    struct perf_event *event,\n\t\t\t    u64 enabled, u64 running)\n{\n\tstruct perf_event *leader = event->group_leader, *sub;\n\tu64 read_format = event->attr.read_format;\n\tunsigned long flags;\n\tu64 values[6];\n\tint n = 0;\n\n\t \n\tlocal_irq_save(flags);\n\n\tvalues[n++] = 1 + leader->nr_siblings;\n\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_ENABLED)\n\t\tvalues[n++] = enabled;\n\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_RUNNING)\n\t\tvalues[n++] = running;\n\n\tif ((leader != event) &&\n\t    (leader->state == PERF_EVENT_STATE_ACTIVE))\n\t\tleader->pmu->read(leader);\n\n\tvalues[n++] = perf_event_count(leader);\n\tif (read_format & PERF_FORMAT_ID)\n\t\tvalues[n++] = primary_event_id(leader);\n\tif (read_format & PERF_FORMAT_LOST)\n\t\tvalues[n++] = atomic64_read(&leader->lost_samples);\n\n\t__output_copy(handle, values, n * sizeof(u64));\n\n\tfor_each_sibling_event(sub, leader) {\n\t\tn = 0;\n\n\t\tif ((sub != event) &&\n\t\t    (sub->state == PERF_EVENT_STATE_ACTIVE))\n\t\t\tsub->pmu->read(sub);\n\n\t\tvalues[n++] = perf_event_count(sub);\n\t\tif (read_format & PERF_FORMAT_ID)\n\t\t\tvalues[n++] = primary_event_id(sub);\n\t\tif (read_format & PERF_FORMAT_LOST)\n\t\t\tvalues[n++] = atomic64_read(&sub->lost_samples);\n\n\t\t__output_copy(handle, values, n * sizeof(u64));\n\t}\n\n\tlocal_irq_restore(flags);\n}\n\n#define PERF_FORMAT_TOTAL_TIMES (PERF_FORMAT_TOTAL_TIME_ENABLED|\\\n\t\t\t\t PERF_FORMAT_TOTAL_TIME_RUNNING)\n\n \nstatic void perf_output_read(struct perf_output_handle *handle,\n\t\t\t     struct perf_event *event)\n{\n\tu64 enabled = 0, running = 0, now;\n\tu64 read_format = event->attr.read_format;\n\n\t \n\tif (read_format & PERF_FORMAT_TOTAL_TIMES)\n\t\tcalc_timer_values(event, &now, &enabled, &running);\n\n\tif (event->attr.read_format & PERF_FORMAT_GROUP)\n\t\tperf_output_read_group(handle, event, enabled, running);\n\telse\n\t\tperf_output_read_one(handle, event, enabled, running);\n}\n\nvoid perf_output_sample(struct perf_output_handle *handle,\n\t\t\tstruct perf_event_header *header,\n\t\t\tstruct perf_sample_data *data,\n\t\t\tstruct perf_event *event)\n{\n\tu64 sample_type = data->type;\n\n\tperf_output_put(handle, *header);\n\n\tif (sample_type & PERF_SAMPLE_IDENTIFIER)\n\t\tperf_output_put(handle, data->id);\n\n\tif (sample_type & PERF_SAMPLE_IP)\n\t\tperf_output_put(handle, data->ip);\n\n\tif (sample_type & PERF_SAMPLE_TID)\n\t\tperf_output_put(handle, data->tid_entry);\n\n\tif (sample_type & PERF_SAMPLE_TIME)\n\t\tperf_output_put(handle, data->time);\n\n\tif (sample_type & PERF_SAMPLE_ADDR)\n\t\tperf_output_put(handle, data->addr);\n\n\tif (sample_type & PERF_SAMPLE_ID)\n\t\tperf_output_put(handle, data->id);\n\n\tif (sample_type & PERF_SAMPLE_STREAM_ID)\n\t\tperf_output_put(handle, data->stream_id);\n\n\tif (sample_type & PERF_SAMPLE_CPU)\n\t\tperf_output_put(handle, data->cpu_entry);\n\n\tif (sample_type & PERF_SAMPLE_PERIOD)\n\t\tperf_output_put(handle, data->period);\n\n\tif (sample_type & PERF_SAMPLE_READ)\n\t\tperf_output_read(handle, event);\n\n\tif (sample_type & PERF_SAMPLE_CALLCHAIN) {\n\t\tint size = 1;\n\n\t\tsize += data->callchain->nr;\n\t\tsize *= sizeof(u64);\n\t\t__output_copy(handle, data->callchain, size);\n\t}\n\n\tif (sample_type & PERF_SAMPLE_RAW) {\n\t\tstruct perf_raw_record *raw = data->raw;\n\n\t\tif (raw) {\n\t\t\tstruct perf_raw_frag *frag = &raw->frag;\n\n\t\t\tperf_output_put(handle, raw->size);\n\t\t\tdo {\n\t\t\t\tif (frag->copy) {\n\t\t\t\t\t__output_custom(handle, frag->copy,\n\t\t\t\t\t\t\tfrag->data, frag->size);\n\t\t\t\t} else {\n\t\t\t\t\t__output_copy(handle, frag->data,\n\t\t\t\t\t\t      frag->size);\n\t\t\t\t}\n\t\t\t\tif (perf_raw_frag_last(frag))\n\t\t\t\t\tbreak;\n\t\t\t\tfrag = frag->next;\n\t\t\t} while (1);\n\t\t\tif (frag->pad)\n\t\t\t\t__output_skip(handle, NULL, frag->pad);\n\t\t} else {\n\t\t\tstruct {\n\t\t\t\tu32\tsize;\n\t\t\t\tu32\tdata;\n\t\t\t} raw = {\n\t\t\t\t.size = sizeof(u32),\n\t\t\t\t.data = 0,\n\t\t\t};\n\t\t\tperf_output_put(handle, raw);\n\t\t}\n\t}\n\n\tif (sample_type & PERF_SAMPLE_BRANCH_STACK) {\n\t\tif (data->br_stack) {\n\t\t\tsize_t size;\n\n\t\t\tsize = data->br_stack->nr\n\t\t\t     * sizeof(struct perf_branch_entry);\n\n\t\t\tperf_output_put(handle, data->br_stack->nr);\n\t\t\tif (branch_sample_hw_index(event))\n\t\t\t\tperf_output_put(handle, data->br_stack->hw_idx);\n\t\t\tperf_output_copy(handle, data->br_stack->entries, size);\n\t\t} else {\n\t\t\t \n\t\t\tu64 nr = 0;\n\t\t\tperf_output_put(handle, nr);\n\t\t}\n\t}\n\n\tif (sample_type & PERF_SAMPLE_REGS_USER) {\n\t\tu64 abi = data->regs_user.abi;\n\n\t\t \n\t\tperf_output_put(handle, abi);\n\n\t\tif (abi) {\n\t\t\tu64 mask = event->attr.sample_regs_user;\n\t\t\tperf_output_sample_regs(handle,\n\t\t\t\t\t\tdata->regs_user.regs,\n\t\t\t\t\t\tmask);\n\t\t}\n\t}\n\n\tif (sample_type & PERF_SAMPLE_STACK_USER) {\n\t\tperf_output_sample_ustack(handle,\n\t\t\t\t\t  data->stack_user_size,\n\t\t\t\t\t  data->regs_user.regs);\n\t}\n\n\tif (sample_type & PERF_SAMPLE_WEIGHT_TYPE)\n\t\tperf_output_put(handle, data->weight.full);\n\n\tif (sample_type & PERF_SAMPLE_DATA_SRC)\n\t\tperf_output_put(handle, data->data_src.val);\n\n\tif (sample_type & PERF_SAMPLE_TRANSACTION)\n\t\tperf_output_put(handle, data->txn);\n\n\tif (sample_type & PERF_SAMPLE_REGS_INTR) {\n\t\tu64 abi = data->regs_intr.abi;\n\t\t \n\t\tperf_output_put(handle, abi);\n\n\t\tif (abi) {\n\t\t\tu64 mask = event->attr.sample_regs_intr;\n\n\t\t\tperf_output_sample_regs(handle,\n\t\t\t\t\t\tdata->regs_intr.regs,\n\t\t\t\t\t\tmask);\n\t\t}\n\t}\n\n\tif (sample_type & PERF_SAMPLE_PHYS_ADDR)\n\t\tperf_output_put(handle, data->phys_addr);\n\n\tif (sample_type & PERF_SAMPLE_CGROUP)\n\t\tperf_output_put(handle, data->cgroup);\n\n\tif (sample_type & PERF_SAMPLE_DATA_PAGE_SIZE)\n\t\tperf_output_put(handle, data->data_page_size);\n\n\tif (sample_type & PERF_SAMPLE_CODE_PAGE_SIZE)\n\t\tperf_output_put(handle, data->code_page_size);\n\n\tif (sample_type & PERF_SAMPLE_AUX) {\n\t\tperf_output_put(handle, data->aux_size);\n\n\t\tif (data->aux_size)\n\t\t\tperf_aux_sample_output(event, handle, data);\n\t}\n\n\tif (!event->attr.watermark) {\n\t\tint wakeup_events = event->attr.wakeup_events;\n\n\t\tif (wakeup_events) {\n\t\t\tstruct perf_buffer *rb = handle->rb;\n\t\t\tint events = local_inc_return(&rb->events);\n\n\t\t\tif (events >= wakeup_events) {\n\t\t\t\tlocal_sub(wakeup_events, &rb->events);\n\t\t\t\tlocal_inc(&rb->wakeup);\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic u64 perf_virt_to_phys(u64 virt)\n{\n\tu64 phys_addr = 0;\n\n\tif (!virt)\n\t\treturn 0;\n\n\tif (virt >= TASK_SIZE) {\n\t\t \n\t\tif (virt_addr_valid((void *)(uintptr_t)virt) &&\n\t\t    !(virt >= VMALLOC_START && virt < VMALLOC_END))\n\t\t\tphys_addr = (u64)virt_to_phys((void *)(uintptr_t)virt);\n\t} else {\n\t\t \n\t\tif (current->mm != NULL) {\n\t\t\tstruct page *p;\n\n\t\t\tpagefault_disable();\n\t\t\tif (get_user_page_fast_only(virt, 0, &p)) {\n\t\t\t\tphys_addr = page_to_phys(p) + virt % PAGE_SIZE;\n\t\t\t\tput_page(p);\n\t\t\t}\n\t\t\tpagefault_enable();\n\t\t}\n\t}\n\n\treturn phys_addr;\n}\n\n \nstatic u64 perf_get_pgtable_size(struct mm_struct *mm, unsigned long addr)\n{\n\tu64 size = 0;\n\n#ifdef CONFIG_HAVE_FAST_GUP\n\tpgd_t *pgdp, pgd;\n\tp4d_t *p4dp, p4d;\n\tpud_t *pudp, pud;\n\tpmd_t *pmdp, pmd;\n\tpte_t *ptep, pte;\n\n\tpgdp = pgd_offset(mm, addr);\n\tpgd = READ_ONCE(*pgdp);\n\tif (pgd_none(pgd))\n\t\treturn 0;\n\n\tif (pgd_leaf(pgd))\n\t\treturn pgd_leaf_size(pgd);\n\n\tp4dp = p4d_offset_lockless(pgdp, pgd, addr);\n\tp4d = READ_ONCE(*p4dp);\n\tif (!p4d_present(p4d))\n\t\treturn 0;\n\n\tif (p4d_leaf(p4d))\n\t\treturn p4d_leaf_size(p4d);\n\n\tpudp = pud_offset_lockless(p4dp, p4d, addr);\n\tpud = READ_ONCE(*pudp);\n\tif (!pud_present(pud))\n\t\treturn 0;\n\n\tif (pud_leaf(pud))\n\t\treturn pud_leaf_size(pud);\n\n\tpmdp = pmd_offset_lockless(pudp, pud, addr);\nagain:\n\tpmd = pmdp_get_lockless(pmdp);\n\tif (!pmd_present(pmd))\n\t\treturn 0;\n\n\tif (pmd_leaf(pmd))\n\t\treturn pmd_leaf_size(pmd);\n\n\tptep = pte_offset_map(&pmd, addr);\n\tif (!ptep)\n\t\tgoto again;\n\n\tpte = ptep_get_lockless(ptep);\n\tif (pte_present(pte))\n\t\tsize = pte_leaf_size(pte);\n\tpte_unmap(ptep);\n#endif  \n\n\treturn size;\n}\n\nstatic u64 perf_get_page_size(unsigned long addr)\n{\n\tstruct mm_struct *mm;\n\tunsigned long flags;\n\tu64 size;\n\n\tif (!addr)\n\t\treturn 0;\n\n\t \n\tlocal_irq_save(flags);\n\n\tmm = current->mm;\n\tif (!mm) {\n\t\t \n\t\tmm = &init_mm;\n\t}\n\n\tsize = perf_get_pgtable_size(mm, addr);\n\n\tlocal_irq_restore(flags);\n\n\treturn size;\n}\n\nstatic struct perf_callchain_entry __empty_callchain = { .nr = 0, };\n\nstruct perf_callchain_entry *\nperf_callchain(struct perf_event *event, struct pt_regs *regs)\n{\n\tbool kernel = !event->attr.exclude_callchain_kernel;\n\tbool user   = !event->attr.exclude_callchain_user;\n\t \n\tbool crosstask = event->ctx->task && event->ctx->task != current;\n\tconst u32 max_stack = event->attr.sample_max_stack;\n\tstruct perf_callchain_entry *callchain;\n\n\tif (!kernel && !user)\n\t\treturn &__empty_callchain;\n\n\tcallchain = get_perf_callchain(regs, 0, kernel, user,\n\t\t\t\t       max_stack, crosstask, true);\n\treturn callchain ?: &__empty_callchain;\n}\n\nstatic __always_inline u64 __cond_set(u64 flags, u64 s, u64 d)\n{\n\treturn d * !!(flags & s);\n}\n\nvoid perf_prepare_sample(struct perf_sample_data *data,\n\t\t\t struct perf_event *event,\n\t\t\t struct pt_regs *regs)\n{\n\tu64 sample_type = event->attr.sample_type;\n\tu64 filtered_sample_type;\n\n\t \n\tfiltered_sample_type = sample_type;\n\tfiltered_sample_type |= __cond_set(sample_type, PERF_SAMPLE_CODE_PAGE_SIZE,\n\t\t\t\t\t   PERF_SAMPLE_IP);\n\tfiltered_sample_type |= __cond_set(sample_type, PERF_SAMPLE_DATA_PAGE_SIZE |\n\t\t\t\t\t   PERF_SAMPLE_PHYS_ADDR, PERF_SAMPLE_ADDR);\n\tfiltered_sample_type |= __cond_set(sample_type, PERF_SAMPLE_STACK_USER,\n\t\t\t\t\t   PERF_SAMPLE_REGS_USER);\n\tfiltered_sample_type &= ~data->sample_flags;\n\n\tif (filtered_sample_type == 0) {\n\t\t \n\t\tdata->type = event->attr.sample_type;\n\t\treturn;\n\t}\n\n\t__perf_event_header__init_id(data, event, filtered_sample_type);\n\n\tif (filtered_sample_type & PERF_SAMPLE_IP) {\n\t\tdata->ip = perf_instruction_pointer(regs);\n\t\tdata->sample_flags |= PERF_SAMPLE_IP;\n\t}\n\n\tif (filtered_sample_type & PERF_SAMPLE_CALLCHAIN)\n\t\tperf_sample_save_callchain(data, event, regs);\n\n\tif (filtered_sample_type & PERF_SAMPLE_RAW) {\n\t\tdata->raw = NULL;\n\t\tdata->dyn_size += sizeof(u64);\n\t\tdata->sample_flags |= PERF_SAMPLE_RAW;\n\t}\n\n\tif (filtered_sample_type & PERF_SAMPLE_BRANCH_STACK) {\n\t\tdata->br_stack = NULL;\n\t\tdata->dyn_size += sizeof(u64);\n\t\tdata->sample_flags |= PERF_SAMPLE_BRANCH_STACK;\n\t}\n\n\tif (filtered_sample_type & PERF_SAMPLE_REGS_USER)\n\t\tperf_sample_regs_user(&data->regs_user, regs);\n\n\t \n\tif ((sample_type & ~data->sample_flags) & PERF_SAMPLE_REGS_USER) {\n\t\t \n\t\tint size = sizeof(u64);\n\n\t\tif (data->regs_user.regs) {\n\t\t\tu64 mask = event->attr.sample_regs_user;\n\t\t\tsize += hweight64(mask) * sizeof(u64);\n\t\t}\n\n\t\tdata->dyn_size += size;\n\t\tdata->sample_flags |= PERF_SAMPLE_REGS_USER;\n\t}\n\n\tif (filtered_sample_type & PERF_SAMPLE_STACK_USER) {\n\t\t \n\t\tu16 stack_size = event->attr.sample_stack_user;\n\t\tu16 header_size = perf_sample_data_size(data, event);\n\t\tu16 size = sizeof(u64);\n\n\t\tstack_size = perf_sample_ustack_size(stack_size, header_size,\n\t\t\t\t\t\t     data->regs_user.regs);\n\n\t\t \n\t\tif (stack_size)\n\t\t\tsize += sizeof(u64) + stack_size;\n\n\t\tdata->stack_user_size = stack_size;\n\t\tdata->dyn_size += size;\n\t\tdata->sample_flags |= PERF_SAMPLE_STACK_USER;\n\t}\n\n\tif (filtered_sample_type & PERF_SAMPLE_WEIGHT_TYPE) {\n\t\tdata->weight.full = 0;\n\t\tdata->sample_flags |= PERF_SAMPLE_WEIGHT_TYPE;\n\t}\n\n\tif (filtered_sample_type & PERF_SAMPLE_DATA_SRC) {\n\t\tdata->data_src.val = PERF_MEM_NA;\n\t\tdata->sample_flags |= PERF_SAMPLE_DATA_SRC;\n\t}\n\n\tif (filtered_sample_type & PERF_SAMPLE_TRANSACTION) {\n\t\tdata->txn = 0;\n\t\tdata->sample_flags |= PERF_SAMPLE_TRANSACTION;\n\t}\n\n\tif (filtered_sample_type & PERF_SAMPLE_ADDR) {\n\t\tdata->addr = 0;\n\t\tdata->sample_flags |= PERF_SAMPLE_ADDR;\n\t}\n\n\tif (filtered_sample_type & PERF_SAMPLE_REGS_INTR) {\n\t\t \n\t\tint size = sizeof(u64);\n\n\t\tperf_sample_regs_intr(&data->regs_intr, regs);\n\n\t\tif (data->regs_intr.regs) {\n\t\t\tu64 mask = event->attr.sample_regs_intr;\n\n\t\t\tsize += hweight64(mask) * sizeof(u64);\n\t\t}\n\n\t\tdata->dyn_size += size;\n\t\tdata->sample_flags |= PERF_SAMPLE_REGS_INTR;\n\t}\n\n\tif (filtered_sample_type & PERF_SAMPLE_PHYS_ADDR) {\n\t\tdata->phys_addr = perf_virt_to_phys(data->addr);\n\t\tdata->sample_flags |= PERF_SAMPLE_PHYS_ADDR;\n\t}\n\n#ifdef CONFIG_CGROUP_PERF\n\tif (filtered_sample_type & PERF_SAMPLE_CGROUP) {\n\t\tstruct cgroup *cgrp;\n\n\t\t \n\t\tcgrp = task_css_check(current, perf_event_cgrp_id, 1)->cgroup;\n\t\tdata->cgroup = cgroup_id(cgrp);\n\t\tdata->sample_flags |= PERF_SAMPLE_CGROUP;\n\t}\n#endif\n\n\t \n\tif (filtered_sample_type & PERF_SAMPLE_DATA_PAGE_SIZE) {\n\t\tdata->data_page_size = perf_get_page_size(data->addr);\n\t\tdata->sample_flags |= PERF_SAMPLE_DATA_PAGE_SIZE;\n\t}\n\n\tif (filtered_sample_type & PERF_SAMPLE_CODE_PAGE_SIZE) {\n\t\tdata->code_page_size = perf_get_page_size(data->ip);\n\t\tdata->sample_flags |= PERF_SAMPLE_CODE_PAGE_SIZE;\n\t}\n\n\tif (filtered_sample_type & PERF_SAMPLE_AUX) {\n\t\tu64 size;\n\t\tu16 header_size = perf_sample_data_size(data, event);\n\n\t\theader_size += sizeof(u64);  \n\n\t\t \n\t\tsize = min_t(size_t, U16_MAX - header_size,\n\t\t\t     event->attr.aux_sample_size);\n\t\tsize = rounddown(size, 8);\n\t\tsize = perf_prepare_sample_aux(event, data, size);\n\n\t\tWARN_ON_ONCE(size + header_size > U16_MAX);\n\t\tdata->dyn_size += size + sizeof(u64);  \n\t\tdata->sample_flags |= PERF_SAMPLE_AUX;\n\t}\n}\n\nvoid perf_prepare_header(struct perf_event_header *header,\n\t\t\t struct perf_sample_data *data,\n\t\t\t struct perf_event *event,\n\t\t\t struct pt_regs *regs)\n{\n\theader->type = PERF_RECORD_SAMPLE;\n\theader->size = perf_sample_data_size(data, event);\n\theader->misc = perf_misc_flags(regs);\n\n\t \n\tWARN_ON_ONCE(header->size & 7);\n}\n\nstatic __always_inline int\n__perf_event_output(struct perf_event *event,\n\t\t    struct perf_sample_data *data,\n\t\t    struct pt_regs *regs,\n\t\t    int (*output_begin)(struct perf_output_handle *,\n\t\t\t\t\tstruct perf_sample_data *,\n\t\t\t\t\tstruct perf_event *,\n\t\t\t\t\tunsigned int))\n{\n\tstruct perf_output_handle handle;\n\tstruct perf_event_header header;\n\tint err;\n\n\t \n\trcu_read_lock();\n\n\tperf_prepare_sample(data, event, regs);\n\tperf_prepare_header(&header, data, event, regs);\n\n\terr = output_begin(&handle, data, event, header.size);\n\tif (err)\n\t\tgoto exit;\n\n\tperf_output_sample(&handle, &header, data, event);\n\n\tperf_output_end(&handle);\n\nexit:\n\trcu_read_unlock();\n\treturn err;\n}\n\nvoid\nperf_event_output_forward(struct perf_event *event,\n\t\t\t struct perf_sample_data *data,\n\t\t\t struct pt_regs *regs)\n{\n\t__perf_event_output(event, data, regs, perf_output_begin_forward);\n}\n\nvoid\nperf_event_output_backward(struct perf_event *event,\n\t\t\t   struct perf_sample_data *data,\n\t\t\t   struct pt_regs *regs)\n{\n\t__perf_event_output(event, data, regs, perf_output_begin_backward);\n}\n\nint\nperf_event_output(struct perf_event *event,\n\t\t  struct perf_sample_data *data,\n\t\t  struct pt_regs *regs)\n{\n\treturn __perf_event_output(event, data, regs, perf_output_begin);\n}\n\n \n\nstruct perf_read_event {\n\tstruct perf_event_header\theader;\n\n\tu32\t\t\t\tpid;\n\tu32\t\t\t\ttid;\n};\n\nstatic void\nperf_event_read_event(struct perf_event *event,\n\t\t\tstruct task_struct *task)\n{\n\tstruct perf_output_handle handle;\n\tstruct perf_sample_data sample;\n\tstruct perf_read_event read_event = {\n\t\t.header = {\n\t\t\t.type = PERF_RECORD_READ,\n\t\t\t.misc = 0,\n\t\t\t.size = sizeof(read_event) + event->read_size,\n\t\t},\n\t\t.pid = perf_event_pid(event, task),\n\t\t.tid = perf_event_tid(event, task),\n\t};\n\tint ret;\n\n\tperf_event_header__init_id(&read_event.header, &sample, event);\n\tret = perf_output_begin(&handle, &sample, event, read_event.header.size);\n\tif (ret)\n\t\treturn;\n\n\tperf_output_put(&handle, read_event);\n\tperf_output_read(&handle, event);\n\tperf_event__output_id_sample(event, &handle, &sample);\n\n\tperf_output_end(&handle);\n}\n\ntypedef void (perf_iterate_f)(struct perf_event *event, void *data);\n\nstatic void\nperf_iterate_ctx(struct perf_event_context *ctx,\n\t\t   perf_iterate_f output,\n\t\t   void *data, bool all)\n{\n\tstruct perf_event *event;\n\n\tlist_for_each_entry_rcu(event, &ctx->event_list, event_entry) {\n\t\tif (!all) {\n\t\t\tif (event->state < PERF_EVENT_STATE_INACTIVE)\n\t\t\t\tcontinue;\n\t\t\tif (!event_filter_match(event))\n\t\t\t\tcontinue;\n\t\t}\n\n\t\toutput(event, data);\n\t}\n}\n\nstatic void perf_iterate_sb_cpu(perf_iterate_f output, void *data)\n{\n\tstruct pmu_event_list *pel = this_cpu_ptr(&pmu_sb_events);\n\tstruct perf_event *event;\n\n\tlist_for_each_entry_rcu(event, &pel->list, sb_list) {\n\t\t \n\t\tif (!smp_load_acquire(&event->ctx))\n\t\t\tcontinue;\n\n\t\tif (event->state < PERF_EVENT_STATE_INACTIVE)\n\t\t\tcontinue;\n\t\tif (!event_filter_match(event))\n\t\t\tcontinue;\n\t\toutput(event, data);\n\t}\n}\n\n \nstatic void\nperf_iterate_sb(perf_iterate_f output, void *data,\n\t       struct perf_event_context *task_ctx)\n{\n\tstruct perf_event_context *ctx;\n\n\trcu_read_lock();\n\tpreempt_disable();\n\n\t \n\tif (task_ctx) {\n\t\tperf_iterate_ctx(task_ctx, output, data, false);\n\t\tgoto done;\n\t}\n\n\tperf_iterate_sb_cpu(output, data);\n\n\tctx = rcu_dereference(current->perf_event_ctxp);\n\tif (ctx)\n\t\tperf_iterate_ctx(ctx, output, data, false);\ndone:\n\tpreempt_enable();\n\trcu_read_unlock();\n}\n\n \nstatic void perf_event_addr_filters_exec(struct perf_event *event, void *data)\n{\n\tstruct perf_addr_filters_head *ifh = perf_event_addr_filters(event);\n\tstruct perf_addr_filter *filter;\n\tunsigned int restart = 0, count = 0;\n\tunsigned long flags;\n\n\tif (!has_addr_filter(event))\n\t\treturn;\n\n\traw_spin_lock_irqsave(&ifh->lock, flags);\n\tlist_for_each_entry(filter, &ifh->list, entry) {\n\t\tif (filter->path.dentry) {\n\t\t\tevent->addr_filter_ranges[count].start = 0;\n\t\t\tevent->addr_filter_ranges[count].size = 0;\n\t\t\trestart++;\n\t\t}\n\n\t\tcount++;\n\t}\n\n\tif (restart)\n\t\tevent->addr_filters_gen++;\n\traw_spin_unlock_irqrestore(&ifh->lock, flags);\n\n\tif (restart)\n\t\tperf_event_stop(event, 1);\n}\n\nvoid perf_event_exec(void)\n{\n\tstruct perf_event_context *ctx;\n\n\tctx = perf_pin_task_context(current);\n\tif (!ctx)\n\t\treturn;\n\n\tperf_event_enable_on_exec(ctx);\n\tperf_event_remove_on_exec(ctx);\n\tperf_iterate_ctx(ctx, perf_event_addr_filters_exec, NULL, true);\n\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\n}\n\nstruct remote_output {\n\tstruct perf_buffer\t*rb;\n\tint\t\t\terr;\n};\n\nstatic void __perf_event_output_stop(struct perf_event *event, void *data)\n{\n\tstruct perf_event *parent = event->parent;\n\tstruct remote_output *ro = data;\n\tstruct perf_buffer *rb = ro->rb;\n\tstruct stop_event_data sd = {\n\t\t.event\t= event,\n\t};\n\n\tif (!has_aux(event))\n\t\treturn;\n\n\tif (!parent)\n\t\tparent = event;\n\n\t \n\tif (rcu_dereference(parent->rb) == rb)\n\t\tro->err = __perf_event_stop(&sd);\n}\n\nstatic int __perf_pmu_output_stop(void *info)\n{\n\tstruct perf_event *event = info;\n\tstruct perf_cpu_context *cpuctx = this_cpu_ptr(&perf_cpu_context);\n\tstruct remote_output ro = {\n\t\t.rb\t= event->rb,\n\t};\n\n\trcu_read_lock();\n\tperf_iterate_ctx(&cpuctx->ctx, __perf_event_output_stop, &ro, false);\n\tif (cpuctx->task_ctx)\n\t\tperf_iterate_ctx(cpuctx->task_ctx, __perf_event_output_stop,\n\t\t\t\t   &ro, false);\n\trcu_read_unlock();\n\n\treturn ro.err;\n}\n\nstatic void perf_pmu_output_stop(struct perf_event *event)\n{\n\tstruct perf_event *iter;\n\tint err, cpu;\n\nrestart:\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(iter, &event->rb->event_list, rb_entry) {\n\t\t \n\t\tcpu = iter->cpu;\n\t\tif (cpu == -1)\n\t\t\tcpu = READ_ONCE(iter->oncpu);\n\n\t\tif (cpu == -1)\n\t\t\tcontinue;\n\n\t\terr = cpu_function_call(cpu, __perf_pmu_output_stop, event);\n\t\tif (err == -EAGAIN) {\n\t\t\trcu_read_unlock();\n\t\t\tgoto restart;\n\t\t}\n\t}\n\trcu_read_unlock();\n}\n\n \n\nstruct perf_task_event {\n\tstruct task_struct\t\t*task;\n\tstruct perf_event_context\t*task_ctx;\n\n\tstruct {\n\t\tstruct perf_event_header\theader;\n\n\t\tu32\t\t\t\tpid;\n\t\tu32\t\t\t\tppid;\n\t\tu32\t\t\t\ttid;\n\t\tu32\t\t\t\tptid;\n\t\tu64\t\t\t\ttime;\n\t} event_id;\n};\n\nstatic int perf_event_task_match(struct perf_event *event)\n{\n\treturn event->attr.comm  || event->attr.mmap ||\n\t       event->attr.mmap2 || event->attr.mmap_data ||\n\t       event->attr.task;\n}\n\nstatic void perf_event_task_output(struct perf_event *event,\n\t\t\t\t   void *data)\n{\n\tstruct perf_task_event *task_event = data;\n\tstruct perf_output_handle handle;\n\tstruct perf_sample_data\tsample;\n\tstruct task_struct *task = task_event->task;\n\tint ret, size = task_event->event_id.header.size;\n\n\tif (!perf_event_task_match(event))\n\t\treturn;\n\n\tperf_event_header__init_id(&task_event->event_id.header, &sample, event);\n\n\tret = perf_output_begin(&handle, &sample, event,\n\t\t\t\ttask_event->event_id.header.size);\n\tif (ret)\n\t\tgoto out;\n\n\ttask_event->event_id.pid = perf_event_pid(event, task);\n\ttask_event->event_id.tid = perf_event_tid(event, task);\n\n\tif (task_event->event_id.header.type == PERF_RECORD_EXIT) {\n\t\ttask_event->event_id.ppid = perf_event_pid(event,\n\t\t\t\t\t\t\ttask->real_parent);\n\t\ttask_event->event_id.ptid = perf_event_pid(event,\n\t\t\t\t\t\t\ttask->real_parent);\n\t} else {   \n\t\ttask_event->event_id.ppid = perf_event_pid(event, current);\n\t\ttask_event->event_id.ptid = perf_event_tid(event, current);\n\t}\n\n\ttask_event->event_id.time = perf_event_clock(event);\n\n\tperf_output_put(&handle, task_event->event_id);\n\n\tperf_event__output_id_sample(event, &handle, &sample);\n\n\tperf_output_end(&handle);\nout:\n\ttask_event->event_id.header.size = size;\n}\n\nstatic void perf_event_task(struct task_struct *task,\n\t\t\t      struct perf_event_context *task_ctx,\n\t\t\t      int new)\n{\n\tstruct perf_task_event task_event;\n\n\tif (!atomic_read(&nr_comm_events) &&\n\t    !atomic_read(&nr_mmap_events) &&\n\t    !atomic_read(&nr_task_events))\n\t\treturn;\n\n\ttask_event = (struct perf_task_event){\n\t\t.task\t  = task,\n\t\t.task_ctx = task_ctx,\n\t\t.event_id    = {\n\t\t\t.header = {\n\t\t\t\t.type = new ? PERF_RECORD_FORK : PERF_RECORD_EXIT,\n\t\t\t\t.misc = 0,\n\t\t\t\t.size = sizeof(task_event.event_id),\n\t\t\t},\n\t\t\t \n\t\t\t \n\t\t\t \n\t\t\t \n\t\t\t \n\t\t},\n\t};\n\n\tperf_iterate_sb(perf_event_task_output,\n\t\t       &task_event,\n\t\t       task_ctx);\n}\n\nvoid perf_event_fork(struct task_struct *task)\n{\n\tperf_event_task(task, NULL, 1);\n\tperf_event_namespaces(task);\n}\n\n \n\nstruct perf_comm_event {\n\tstruct task_struct\t*task;\n\tchar\t\t\t*comm;\n\tint\t\t\tcomm_size;\n\n\tstruct {\n\t\tstruct perf_event_header\theader;\n\n\t\tu32\t\t\t\tpid;\n\t\tu32\t\t\t\ttid;\n\t} event_id;\n};\n\nstatic int perf_event_comm_match(struct perf_event *event)\n{\n\treturn event->attr.comm;\n}\n\nstatic void perf_event_comm_output(struct perf_event *event,\n\t\t\t\t   void *data)\n{\n\tstruct perf_comm_event *comm_event = data;\n\tstruct perf_output_handle handle;\n\tstruct perf_sample_data sample;\n\tint size = comm_event->event_id.header.size;\n\tint ret;\n\n\tif (!perf_event_comm_match(event))\n\t\treturn;\n\n\tperf_event_header__init_id(&comm_event->event_id.header, &sample, event);\n\tret = perf_output_begin(&handle, &sample, event,\n\t\t\t\tcomm_event->event_id.header.size);\n\n\tif (ret)\n\t\tgoto out;\n\n\tcomm_event->event_id.pid = perf_event_pid(event, comm_event->task);\n\tcomm_event->event_id.tid = perf_event_tid(event, comm_event->task);\n\n\tperf_output_put(&handle, comm_event->event_id);\n\t__output_copy(&handle, comm_event->comm,\n\t\t\t\t   comm_event->comm_size);\n\n\tperf_event__output_id_sample(event, &handle, &sample);\n\n\tperf_output_end(&handle);\nout:\n\tcomm_event->event_id.header.size = size;\n}\n\nstatic void perf_event_comm_event(struct perf_comm_event *comm_event)\n{\n\tchar comm[TASK_COMM_LEN];\n\tunsigned int size;\n\n\tmemset(comm, 0, sizeof(comm));\n\tstrscpy(comm, comm_event->task->comm, sizeof(comm));\n\tsize = ALIGN(strlen(comm)+1, sizeof(u64));\n\n\tcomm_event->comm = comm;\n\tcomm_event->comm_size = size;\n\n\tcomm_event->event_id.header.size = sizeof(comm_event->event_id) + size;\n\n\tperf_iterate_sb(perf_event_comm_output,\n\t\t       comm_event,\n\t\t       NULL);\n}\n\nvoid perf_event_comm(struct task_struct *task, bool exec)\n{\n\tstruct perf_comm_event comm_event;\n\n\tif (!atomic_read(&nr_comm_events))\n\t\treturn;\n\n\tcomm_event = (struct perf_comm_event){\n\t\t.task\t= task,\n\t\t \n\t\t \n\t\t.event_id  = {\n\t\t\t.header = {\n\t\t\t\t.type = PERF_RECORD_COMM,\n\t\t\t\t.misc = exec ? PERF_RECORD_MISC_COMM_EXEC : 0,\n\t\t\t\t \n\t\t\t},\n\t\t\t \n\t\t\t \n\t\t},\n\t};\n\n\tperf_event_comm_event(&comm_event);\n}\n\n \n\nstruct perf_namespaces_event {\n\tstruct task_struct\t\t*task;\n\n\tstruct {\n\t\tstruct perf_event_header\theader;\n\n\t\tu32\t\t\t\tpid;\n\t\tu32\t\t\t\ttid;\n\t\tu64\t\t\t\tnr_namespaces;\n\t\tstruct perf_ns_link_info\tlink_info[NR_NAMESPACES];\n\t} event_id;\n};\n\nstatic int perf_event_namespaces_match(struct perf_event *event)\n{\n\treturn event->attr.namespaces;\n}\n\nstatic void perf_event_namespaces_output(struct perf_event *event,\n\t\t\t\t\t void *data)\n{\n\tstruct perf_namespaces_event *namespaces_event = data;\n\tstruct perf_output_handle handle;\n\tstruct perf_sample_data sample;\n\tu16 header_size = namespaces_event->event_id.header.size;\n\tint ret;\n\n\tif (!perf_event_namespaces_match(event))\n\t\treturn;\n\n\tperf_event_header__init_id(&namespaces_event->event_id.header,\n\t\t\t\t   &sample, event);\n\tret = perf_output_begin(&handle, &sample, event,\n\t\t\t\tnamespaces_event->event_id.header.size);\n\tif (ret)\n\t\tgoto out;\n\n\tnamespaces_event->event_id.pid = perf_event_pid(event,\n\t\t\t\t\t\t\tnamespaces_event->task);\n\tnamespaces_event->event_id.tid = perf_event_tid(event,\n\t\t\t\t\t\t\tnamespaces_event->task);\n\n\tperf_output_put(&handle, namespaces_event->event_id);\n\n\tperf_event__output_id_sample(event, &handle, &sample);\n\n\tperf_output_end(&handle);\nout:\n\tnamespaces_event->event_id.header.size = header_size;\n}\n\nstatic void perf_fill_ns_link_info(struct perf_ns_link_info *ns_link_info,\n\t\t\t\t   struct task_struct *task,\n\t\t\t\t   const struct proc_ns_operations *ns_ops)\n{\n\tstruct path ns_path;\n\tstruct inode *ns_inode;\n\tint error;\n\n\terror = ns_get_path(&ns_path, task, ns_ops);\n\tif (!error) {\n\t\tns_inode = ns_path.dentry->d_inode;\n\t\tns_link_info->dev = new_encode_dev(ns_inode->i_sb->s_dev);\n\t\tns_link_info->ino = ns_inode->i_ino;\n\t\tpath_put(&ns_path);\n\t}\n}\n\nvoid perf_event_namespaces(struct task_struct *task)\n{\n\tstruct perf_namespaces_event namespaces_event;\n\tstruct perf_ns_link_info *ns_link_info;\n\n\tif (!atomic_read(&nr_namespaces_events))\n\t\treturn;\n\n\tnamespaces_event = (struct perf_namespaces_event){\n\t\t.task\t= task,\n\t\t.event_id  = {\n\t\t\t.header = {\n\t\t\t\t.type = PERF_RECORD_NAMESPACES,\n\t\t\t\t.misc = 0,\n\t\t\t\t.size = sizeof(namespaces_event.event_id),\n\t\t\t},\n\t\t\t \n\t\t\t \n\t\t\t.nr_namespaces = NR_NAMESPACES,\n\t\t\t \n\t\t},\n\t};\n\n\tns_link_info = namespaces_event.event_id.link_info;\n\n\tperf_fill_ns_link_info(&ns_link_info[MNT_NS_INDEX],\n\t\t\t       task, &mntns_operations);\n\n#ifdef CONFIG_USER_NS\n\tperf_fill_ns_link_info(&ns_link_info[USER_NS_INDEX],\n\t\t\t       task, &userns_operations);\n#endif\n#ifdef CONFIG_NET_NS\n\tperf_fill_ns_link_info(&ns_link_info[NET_NS_INDEX],\n\t\t\t       task, &netns_operations);\n#endif\n#ifdef CONFIG_UTS_NS\n\tperf_fill_ns_link_info(&ns_link_info[UTS_NS_INDEX],\n\t\t\t       task, &utsns_operations);\n#endif\n#ifdef CONFIG_IPC_NS\n\tperf_fill_ns_link_info(&ns_link_info[IPC_NS_INDEX],\n\t\t\t       task, &ipcns_operations);\n#endif\n#ifdef CONFIG_PID_NS\n\tperf_fill_ns_link_info(&ns_link_info[PID_NS_INDEX],\n\t\t\t       task, &pidns_operations);\n#endif\n#ifdef CONFIG_CGROUPS\n\tperf_fill_ns_link_info(&ns_link_info[CGROUP_NS_INDEX],\n\t\t\t       task, &cgroupns_operations);\n#endif\n\n\tperf_iterate_sb(perf_event_namespaces_output,\n\t\t\t&namespaces_event,\n\t\t\tNULL);\n}\n\n \n#ifdef CONFIG_CGROUP_PERF\n\nstruct perf_cgroup_event {\n\tchar\t\t\t\t*path;\n\tint\t\t\t\tpath_size;\n\tstruct {\n\t\tstruct perf_event_header\theader;\n\t\tu64\t\t\t\tid;\n\t\tchar\t\t\t\tpath[];\n\t} event_id;\n};\n\nstatic int perf_event_cgroup_match(struct perf_event *event)\n{\n\treturn event->attr.cgroup;\n}\n\nstatic void perf_event_cgroup_output(struct perf_event *event, void *data)\n{\n\tstruct perf_cgroup_event *cgroup_event = data;\n\tstruct perf_output_handle handle;\n\tstruct perf_sample_data sample;\n\tu16 header_size = cgroup_event->event_id.header.size;\n\tint ret;\n\n\tif (!perf_event_cgroup_match(event))\n\t\treturn;\n\n\tperf_event_header__init_id(&cgroup_event->event_id.header,\n\t\t\t\t   &sample, event);\n\tret = perf_output_begin(&handle, &sample, event,\n\t\t\t\tcgroup_event->event_id.header.size);\n\tif (ret)\n\t\tgoto out;\n\n\tperf_output_put(&handle, cgroup_event->event_id);\n\t__output_copy(&handle, cgroup_event->path, cgroup_event->path_size);\n\n\tperf_event__output_id_sample(event, &handle, &sample);\n\n\tperf_output_end(&handle);\nout:\n\tcgroup_event->event_id.header.size = header_size;\n}\n\nstatic void perf_event_cgroup(struct cgroup *cgrp)\n{\n\tstruct perf_cgroup_event cgroup_event;\n\tchar path_enomem[16] = \"//enomem\";\n\tchar *pathname;\n\tsize_t size;\n\n\tif (!atomic_read(&nr_cgroup_events))\n\t\treturn;\n\n\tcgroup_event = (struct perf_cgroup_event){\n\t\t.event_id  = {\n\t\t\t.header = {\n\t\t\t\t.type = PERF_RECORD_CGROUP,\n\t\t\t\t.misc = 0,\n\t\t\t\t.size = sizeof(cgroup_event.event_id),\n\t\t\t},\n\t\t\t.id = cgroup_id(cgrp),\n\t\t},\n\t};\n\n\tpathname = kmalloc(PATH_MAX, GFP_KERNEL);\n\tif (pathname == NULL) {\n\t\tcgroup_event.path = path_enomem;\n\t} else {\n\t\t \n\t\tcgroup_path(cgrp, pathname, PATH_MAX - sizeof(u64));\n\t\tcgroup_event.path = pathname;\n\t}\n\n\t \n\tsize = strlen(cgroup_event.path) + 1;\n\twhile (!IS_ALIGNED(size, sizeof(u64)))\n\t\tcgroup_event.path[size++] = '\\0';\n\n\tcgroup_event.event_id.header.size += size;\n\tcgroup_event.path_size = size;\n\n\tperf_iterate_sb(perf_event_cgroup_output,\n\t\t\t&cgroup_event,\n\t\t\tNULL);\n\n\tkfree(pathname);\n}\n\n#endif\n\n \n\nstruct perf_mmap_event {\n\tstruct vm_area_struct\t*vma;\n\n\tconst char\t\t*file_name;\n\tint\t\t\tfile_size;\n\tint\t\t\tmaj, min;\n\tu64\t\t\tino;\n\tu64\t\t\tino_generation;\n\tu32\t\t\tprot, flags;\n\tu8\t\t\tbuild_id[BUILD_ID_SIZE_MAX];\n\tu32\t\t\tbuild_id_size;\n\n\tstruct {\n\t\tstruct perf_event_header\theader;\n\n\t\tu32\t\t\t\tpid;\n\t\tu32\t\t\t\ttid;\n\t\tu64\t\t\t\tstart;\n\t\tu64\t\t\t\tlen;\n\t\tu64\t\t\t\tpgoff;\n\t} event_id;\n};\n\nstatic int perf_event_mmap_match(struct perf_event *event,\n\t\t\t\t void *data)\n{\n\tstruct perf_mmap_event *mmap_event = data;\n\tstruct vm_area_struct *vma = mmap_event->vma;\n\tint executable = vma->vm_flags & VM_EXEC;\n\n\treturn (!executable && event->attr.mmap_data) ||\n\t       (executable && (event->attr.mmap || event->attr.mmap2));\n}\n\nstatic void perf_event_mmap_output(struct perf_event *event,\n\t\t\t\t   void *data)\n{\n\tstruct perf_mmap_event *mmap_event = data;\n\tstruct perf_output_handle handle;\n\tstruct perf_sample_data sample;\n\tint size = mmap_event->event_id.header.size;\n\tu32 type = mmap_event->event_id.header.type;\n\tbool use_build_id;\n\tint ret;\n\n\tif (!perf_event_mmap_match(event, data))\n\t\treturn;\n\n\tif (event->attr.mmap2) {\n\t\tmmap_event->event_id.header.type = PERF_RECORD_MMAP2;\n\t\tmmap_event->event_id.header.size += sizeof(mmap_event->maj);\n\t\tmmap_event->event_id.header.size += sizeof(mmap_event->min);\n\t\tmmap_event->event_id.header.size += sizeof(mmap_event->ino);\n\t\tmmap_event->event_id.header.size += sizeof(mmap_event->ino_generation);\n\t\tmmap_event->event_id.header.size += sizeof(mmap_event->prot);\n\t\tmmap_event->event_id.header.size += sizeof(mmap_event->flags);\n\t}\n\n\tperf_event_header__init_id(&mmap_event->event_id.header, &sample, event);\n\tret = perf_output_begin(&handle, &sample, event,\n\t\t\t\tmmap_event->event_id.header.size);\n\tif (ret)\n\t\tgoto out;\n\n\tmmap_event->event_id.pid = perf_event_pid(event, current);\n\tmmap_event->event_id.tid = perf_event_tid(event, current);\n\n\tuse_build_id = event->attr.build_id && mmap_event->build_id_size;\n\n\tif (event->attr.mmap2 && use_build_id)\n\t\tmmap_event->event_id.header.misc |= PERF_RECORD_MISC_MMAP_BUILD_ID;\n\n\tperf_output_put(&handle, mmap_event->event_id);\n\n\tif (event->attr.mmap2) {\n\t\tif (use_build_id) {\n\t\t\tu8 size[4] = { (u8) mmap_event->build_id_size, 0, 0, 0 };\n\n\t\t\t__output_copy(&handle, size, 4);\n\t\t\t__output_copy(&handle, mmap_event->build_id, BUILD_ID_SIZE_MAX);\n\t\t} else {\n\t\t\tperf_output_put(&handle, mmap_event->maj);\n\t\t\tperf_output_put(&handle, mmap_event->min);\n\t\t\tperf_output_put(&handle, mmap_event->ino);\n\t\t\tperf_output_put(&handle, mmap_event->ino_generation);\n\t\t}\n\t\tperf_output_put(&handle, mmap_event->prot);\n\t\tperf_output_put(&handle, mmap_event->flags);\n\t}\n\n\t__output_copy(&handle, mmap_event->file_name,\n\t\t\t\t   mmap_event->file_size);\n\n\tperf_event__output_id_sample(event, &handle, &sample);\n\n\tperf_output_end(&handle);\nout:\n\tmmap_event->event_id.header.size = size;\n\tmmap_event->event_id.header.type = type;\n}\n\nstatic void perf_event_mmap_event(struct perf_mmap_event *mmap_event)\n{\n\tstruct vm_area_struct *vma = mmap_event->vma;\n\tstruct file *file = vma->vm_file;\n\tint maj = 0, min = 0;\n\tu64 ino = 0, gen = 0;\n\tu32 prot = 0, flags = 0;\n\tunsigned int size;\n\tchar tmp[16];\n\tchar *buf = NULL;\n\tchar *name = NULL;\n\n\tif (vma->vm_flags & VM_READ)\n\t\tprot |= PROT_READ;\n\tif (vma->vm_flags & VM_WRITE)\n\t\tprot |= PROT_WRITE;\n\tif (vma->vm_flags & VM_EXEC)\n\t\tprot |= PROT_EXEC;\n\n\tif (vma->vm_flags & VM_MAYSHARE)\n\t\tflags = MAP_SHARED;\n\telse\n\t\tflags = MAP_PRIVATE;\n\n\tif (vma->vm_flags & VM_LOCKED)\n\t\tflags |= MAP_LOCKED;\n\tif (is_vm_hugetlb_page(vma))\n\t\tflags |= MAP_HUGETLB;\n\n\tif (file) {\n\t\tstruct inode *inode;\n\t\tdev_t dev;\n\n\t\tbuf = kmalloc(PATH_MAX, GFP_KERNEL);\n\t\tif (!buf) {\n\t\t\tname = \"//enomem\";\n\t\t\tgoto cpy_name;\n\t\t}\n\t\t \n\t\tname = file_path(file, buf, PATH_MAX - sizeof(u64));\n\t\tif (IS_ERR(name)) {\n\t\t\tname = \"//toolong\";\n\t\t\tgoto cpy_name;\n\t\t}\n\t\tinode = file_inode(vma->vm_file);\n\t\tdev = inode->i_sb->s_dev;\n\t\tino = inode->i_ino;\n\t\tgen = inode->i_generation;\n\t\tmaj = MAJOR(dev);\n\t\tmin = MINOR(dev);\n\n\t\tgoto got_name;\n\t} else {\n\t\tif (vma->vm_ops && vma->vm_ops->name)\n\t\t\tname = (char *) vma->vm_ops->name(vma);\n\t\tif (!name)\n\t\t\tname = (char *)arch_vma_name(vma);\n\t\tif (!name) {\n\t\t\tif (vma_is_initial_heap(vma))\n\t\t\t\tname = \"[heap]\";\n\t\t\telse if (vma_is_initial_stack(vma))\n\t\t\t\tname = \"[stack]\";\n\t\t\telse\n\t\t\t\tname = \"//anon\";\n\t\t}\n\t}\n\ncpy_name:\n\tstrscpy(tmp, name, sizeof(tmp));\n\tname = tmp;\ngot_name:\n\t \n\tsize = strlen(name)+1;\n\twhile (!IS_ALIGNED(size, sizeof(u64)))\n\t\tname[size++] = '\\0';\n\n\tmmap_event->file_name = name;\n\tmmap_event->file_size = size;\n\tmmap_event->maj = maj;\n\tmmap_event->min = min;\n\tmmap_event->ino = ino;\n\tmmap_event->ino_generation = gen;\n\tmmap_event->prot = prot;\n\tmmap_event->flags = flags;\n\n\tif (!(vma->vm_flags & VM_EXEC))\n\t\tmmap_event->event_id.header.misc |= PERF_RECORD_MISC_MMAP_DATA;\n\n\tmmap_event->event_id.header.size = sizeof(mmap_event->event_id) + size;\n\n\tif (atomic_read(&nr_build_id_events))\n\t\tbuild_id_parse(vma, mmap_event->build_id, &mmap_event->build_id_size);\n\n\tperf_iterate_sb(perf_event_mmap_output,\n\t\t       mmap_event,\n\t\t       NULL);\n\n\tkfree(buf);\n}\n\n \nstatic bool perf_addr_filter_match(struct perf_addr_filter *filter,\n\t\t\t\t     struct file *file, unsigned long offset,\n\t\t\t\t     unsigned long size)\n{\n\t \n\tif (!filter->path.dentry)\n\t\treturn false;\n\n\tif (d_inode(filter->path.dentry) != file_inode(file))\n\t\treturn false;\n\n\tif (filter->offset > offset + size)\n\t\treturn false;\n\n\tif (filter->offset + filter->size < offset)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic bool perf_addr_filter_vma_adjust(struct perf_addr_filter *filter,\n\t\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\t\tstruct perf_addr_filter_range *fr)\n{\n\tunsigned long vma_size = vma->vm_end - vma->vm_start;\n\tunsigned long off = vma->vm_pgoff << PAGE_SHIFT;\n\tstruct file *file = vma->vm_file;\n\n\tif (!perf_addr_filter_match(filter, file, off, vma_size))\n\t\treturn false;\n\n\tif (filter->offset < off) {\n\t\tfr->start = vma->vm_start;\n\t\tfr->size = min(vma_size, filter->size - (off - filter->offset));\n\t} else {\n\t\tfr->start = vma->vm_start + filter->offset - off;\n\t\tfr->size = min(vma->vm_end - fr->start, filter->size);\n\t}\n\n\treturn true;\n}\n\nstatic void __perf_addr_filters_adjust(struct perf_event *event, void *data)\n{\n\tstruct perf_addr_filters_head *ifh = perf_event_addr_filters(event);\n\tstruct vm_area_struct *vma = data;\n\tstruct perf_addr_filter *filter;\n\tunsigned int restart = 0, count = 0;\n\tunsigned long flags;\n\n\tif (!has_addr_filter(event))\n\t\treturn;\n\n\tif (!vma->vm_file)\n\t\treturn;\n\n\traw_spin_lock_irqsave(&ifh->lock, flags);\n\tlist_for_each_entry(filter, &ifh->list, entry) {\n\t\tif (perf_addr_filter_vma_adjust(filter, vma,\n\t\t\t\t\t\t&event->addr_filter_ranges[count]))\n\t\t\trestart++;\n\n\t\tcount++;\n\t}\n\n\tif (restart)\n\t\tevent->addr_filters_gen++;\n\traw_spin_unlock_irqrestore(&ifh->lock, flags);\n\n\tif (restart)\n\t\tperf_event_stop(event, 1);\n}\n\n \nstatic void perf_addr_filters_adjust(struct vm_area_struct *vma)\n{\n\tstruct perf_event_context *ctx;\n\n\t \n\tif (!(vma->vm_flags & VM_EXEC))\n\t\treturn;\n\n\trcu_read_lock();\n\tctx = rcu_dereference(current->perf_event_ctxp);\n\tif (ctx)\n\t\tperf_iterate_ctx(ctx, __perf_addr_filters_adjust, vma, true);\n\trcu_read_unlock();\n}\n\nvoid perf_event_mmap(struct vm_area_struct *vma)\n{\n\tstruct perf_mmap_event mmap_event;\n\n\tif (!atomic_read(&nr_mmap_events))\n\t\treturn;\n\n\tmmap_event = (struct perf_mmap_event){\n\t\t.vma\t= vma,\n\t\t \n\t\t \n\t\t.event_id  = {\n\t\t\t.header = {\n\t\t\t\t.type = PERF_RECORD_MMAP,\n\t\t\t\t.misc = PERF_RECORD_MISC_USER,\n\t\t\t\t \n\t\t\t},\n\t\t\t \n\t\t\t \n\t\t\t.start  = vma->vm_start,\n\t\t\t.len    = vma->vm_end - vma->vm_start,\n\t\t\t.pgoff  = (u64)vma->vm_pgoff << PAGE_SHIFT,\n\t\t},\n\t\t \n\t\t \n\t\t \n\t\t \n\t\t \n\t\t \n\t};\n\n\tperf_addr_filters_adjust(vma);\n\tperf_event_mmap_event(&mmap_event);\n}\n\nvoid perf_event_aux_event(struct perf_event *event, unsigned long head,\n\t\t\t  unsigned long size, u64 flags)\n{\n\tstruct perf_output_handle handle;\n\tstruct perf_sample_data sample;\n\tstruct perf_aux_event {\n\t\tstruct perf_event_header\theader;\n\t\tu64\t\t\t\toffset;\n\t\tu64\t\t\t\tsize;\n\t\tu64\t\t\t\tflags;\n\t} rec = {\n\t\t.header = {\n\t\t\t.type = PERF_RECORD_AUX,\n\t\t\t.misc = 0,\n\t\t\t.size = sizeof(rec),\n\t\t},\n\t\t.offset\t\t= head,\n\t\t.size\t\t= size,\n\t\t.flags\t\t= flags,\n\t};\n\tint ret;\n\n\tperf_event_header__init_id(&rec.header, &sample, event);\n\tret = perf_output_begin(&handle, &sample, event, rec.header.size);\n\n\tif (ret)\n\t\treturn;\n\n\tperf_output_put(&handle, rec);\n\tperf_event__output_id_sample(event, &handle, &sample);\n\n\tperf_output_end(&handle);\n}\n\n \nvoid perf_log_lost_samples(struct perf_event *event, u64 lost)\n{\n\tstruct perf_output_handle handle;\n\tstruct perf_sample_data sample;\n\tint ret;\n\n\tstruct {\n\t\tstruct perf_event_header\theader;\n\t\tu64\t\t\t\tlost;\n\t} lost_samples_event = {\n\t\t.header = {\n\t\t\t.type = PERF_RECORD_LOST_SAMPLES,\n\t\t\t.misc = 0,\n\t\t\t.size = sizeof(lost_samples_event),\n\t\t},\n\t\t.lost\t\t= lost,\n\t};\n\n\tperf_event_header__init_id(&lost_samples_event.header, &sample, event);\n\n\tret = perf_output_begin(&handle, &sample, event,\n\t\t\t\tlost_samples_event.header.size);\n\tif (ret)\n\t\treturn;\n\n\tperf_output_put(&handle, lost_samples_event);\n\tperf_event__output_id_sample(event, &handle, &sample);\n\tperf_output_end(&handle);\n}\n\n \n\nstruct perf_switch_event {\n\tstruct task_struct\t*task;\n\tstruct task_struct\t*next_prev;\n\n\tstruct {\n\t\tstruct perf_event_header\theader;\n\t\tu32\t\t\t\tnext_prev_pid;\n\t\tu32\t\t\t\tnext_prev_tid;\n\t} event_id;\n};\n\nstatic int perf_event_switch_match(struct perf_event *event)\n{\n\treturn event->attr.context_switch;\n}\n\nstatic void perf_event_switch_output(struct perf_event *event, void *data)\n{\n\tstruct perf_switch_event *se = data;\n\tstruct perf_output_handle handle;\n\tstruct perf_sample_data sample;\n\tint ret;\n\n\tif (!perf_event_switch_match(event))\n\t\treturn;\n\n\t \n\tif (event->ctx->task) {\n\t\tse->event_id.header.type = PERF_RECORD_SWITCH;\n\t\tse->event_id.header.size = sizeof(se->event_id.header);\n\t} else {\n\t\tse->event_id.header.type = PERF_RECORD_SWITCH_CPU_WIDE;\n\t\tse->event_id.header.size = sizeof(se->event_id);\n\t\tse->event_id.next_prev_pid =\n\t\t\t\t\tperf_event_pid(event, se->next_prev);\n\t\tse->event_id.next_prev_tid =\n\t\t\t\t\tperf_event_tid(event, se->next_prev);\n\t}\n\n\tperf_event_header__init_id(&se->event_id.header, &sample, event);\n\n\tret = perf_output_begin(&handle, &sample, event, se->event_id.header.size);\n\tif (ret)\n\t\treturn;\n\n\tif (event->ctx->task)\n\t\tperf_output_put(&handle, se->event_id.header);\n\telse\n\t\tperf_output_put(&handle, se->event_id);\n\n\tperf_event__output_id_sample(event, &handle, &sample);\n\n\tperf_output_end(&handle);\n}\n\nstatic void perf_event_switch(struct task_struct *task,\n\t\t\t      struct task_struct *next_prev, bool sched_in)\n{\n\tstruct perf_switch_event switch_event;\n\n\t \n\n\tswitch_event = (struct perf_switch_event){\n\t\t.task\t\t= task,\n\t\t.next_prev\t= next_prev,\n\t\t.event_id\t= {\n\t\t\t.header = {\n\t\t\t\t \n\t\t\t\t.misc = sched_in ? 0 : PERF_RECORD_MISC_SWITCH_OUT,\n\t\t\t\t \n\t\t\t},\n\t\t\t \n\t\t\t \n\t\t},\n\t};\n\n\tif (!sched_in && task->on_rq) {\n\t\tswitch_event.event_id.header.misc |=\n\t\t\t\tPERF_RECORD_MISC_SWITCH_OUT_PREEMPT;\n\t}\n\n\tperf_iterate_sb(perf_event_switch_output, &switch_event, NULL);\n}\n\n \n\nstatic void perf_log_throttle(struct perf_event *event, int enable)\n{\n\tstruct perf_output_handle handle;\n\tstruct perf_sample_data sample;\n\tint ret;\n\n\tstruct {\n\t\tstruct perf_event_header\theader;\n\t\tu64\t\t\t\ttime;\n\t\tu64\t\t\t\tid;\n\t\tu64\t\t\t\tstream_id;\n\t} throttle_event = {\n\t\t.header = {\n\t\t\t.type = PERF_RECORD_THROTTLE,\n\t\t\t.misc = 0,\n\t\t\t.size = sizeof(throttle_event),\n\t\t},\n\t\t.time\t\t= perf_event_clock(event),\n\t\t.id\t\t= primary_event_id(event),\n\t\t.stream_id\t= event->id,\n\t};\n\n\tif (enable)\n\t\tthrottle_event.header.type = PERF_RECORD_UNTHROTTLE;\n\n\tperf_event_header__init_id(&throttle_event.header, &sample, event);\n\n\tret = perf_output_begin(&handle, &sample, event,\n\t\t\t\tthrottle_event.header.size);\n\tif (ret)\n\t\treturn;\n\n\tperf_output_put(&handle, throttle_event);\n\tperf_event__output_id_sample(event, &handle, &sample);\n\tperf_output_end(&handle);\n}\n\n \n\nstruct perf_ksymbol_event {\n\tconst char\t*name;\n\tint\t\tname_len;\n\tstruct {\n\t\tstruct perf_event_header        header;\n\t\tu64\t\t\t\taddr;\n\t\tu32\t\t\t\tlen;\n\t\tu16\t\t\t\tksym_type;\n\t\tu16\t\t\t\tflags;\n\t} event_id;\n};\n\nstatic int perf_event_ksymbol_match(struct perf_event *event)\n{\n\treturn event->attr.ksymbol;\n}\n\nstatic void perf_event_ksymbol_output(struct perf_event *event, void *data)\n{\n\tstruct perf_ksymbol_event *ksymbol_event = data;\n\tstruct perf_output_handle handle;\n\tstruct perf_sample_data sample;\n\tint ret;\n\n\tif (!perf_event_ksymbol_match(event))\n\t\treturn;\n\n\tperf_event_header__init_id(&ksymbol_event->event_id.header,\n\t\t\t\t   &sample, event);\n\tret = perf_output_begin(&handle, &sample, event,\n\t\t\t\tksymbol_event->event_id.header.size);\n\tif (ret)\n\t\treturn;\n\n\tperf_output_put(&handle, ksymbol_event->event_id);\n\t__output_copy(&handle, ksymbol_event->name, ksymbol_event->name_len);\n\tperf_event__output_id_sample(event, &handle, &sample);\n\n\tperf_output_end(&handle);\n}\n\nvoid perf_event_ksymbol(u16 ksym_type, u64 addr, u32 len, bool unregister,\n\t\t\tconst char *sym)\n{\n\tstruct perf_ksymbol_event ksymbol_event;\n\tchar name[KSYM_NAME_LEN];\n\tu16 flags = 0;\n\tint name_len;\n\n\tif (!atomic_read(&nr_ksymbol_events))\n\t\treturn;\n\n\tif (ksym_type >= PERF_RECORD_KSYMBOL_TYPE_MAX ||\n\t    ksym_type == PERF_RECORD_KSYMBOL_TYPE_UNKNOWN)\n\t\tgoto err;\n\n\tstrscpy(name, sym, KSYM_NAME_LEN);\n\tname_len = strlen(name) + 1;\n\twhile (!IS_ALIGNED(name_len, sizeof(u64)))\n\t\tname[name_len++] = '\\0';\n\tBUILD_BUG_ON(KSYM_NAME_LEN % sizeof(u64));\n\n\tif (unregister)\n\t\tflags |= PERF_RECORD_KSYMBOL_FLAGS_UNREGISTER;\n\n\tksymbol_event = (struct perf_ksymbol_event){\n\t\t.name = name,\n\t\t.name_len = name_len,\n\t\t.event_id = {\n\t\t\t.header = {\n\t\t\t\t.type = PERF_RECORD_KSYMBOL,\n\t\t\t\t.size = sizeof(ksymbol_event.event_id) +\n\t\t\t\t\tname_len,\n\t\t\t},\n\t\t\t.addr = addr,\n\t\t\t.len = len,\n\t\t\t.ksym_type = ksym_type,\n\t\t\t.flags = flags,\n\t\t},\n\t};\n\n\tperf_iterate_sb(perf_event_ksymbol_output, &ksymbol_event, NULL);\n\treturn;\nerr:\n\tWARN_ONCE(1, \"%s: Invalid KSYMBOL type 0x%x\\n\", __func__, ksym_type);\n}\n\n \n\nstruct perf_bpf_event {\n\tstruct bpf_prog\t*prog;\n\tstruct {\n\t\tstruct perf_event_header        header;\n\t\tu16\t\t\t\ttype;\n\t\tu16\t\t\t\tflags;\n\t\tu32\t\t\t\tid;\n\t\tu8\t\t\t\ttag[BPF_TAG_SIZE];\n\t} event_id;\n};\n\nstatic int perf_event_bpf_match(struct perf_event *event)\n{\n\treturn event->attr.bpf_event;\n}\n\nstatic void perf_event_bpf_output(struct perf_event *event, void *data)\n{\n\tstruct perf_bpf_event *bpf_event = data;\n\tstruct perf_output_handle handle;\n\tstruct perf_sample_data sample;\n\tint ret;\n\n\tif (!perf_event_bpf_match(event))\n\t\treturn;\n\n\tperf_event_header__init_id(&bpf_event->event_id.header,\n\t\t\t\t   &sample, event);\n\tret = perf_output_begin(&handle, &sample, event,\n\t\t\t\tbpf_event->event_id.header.size);\n\tif (ret)\n\t\treturn;\n\n\tperf_output_put(&handle, bpf_event->event_id);\n\tperf_event__output_id_sample(event, &handle, &sample);\n\n\tperf_output_end(&handle);\n}\n\nstatic void perf_event_bpf_emit_ksymbols(struct bpf_prog *prog,\n\t\t\t\t\t enum perf_bpf_event_type type)\n{\n\tbool unregister = type == PERF_BPF_EVENT_PROG_UNLOAD;\n\tint i;\n\n\tif (prog->aux->func_cnt == 0) {\n\t\tperf_event_ksymbol(PERF_RECORD_KSYMBOL_TYPE_BPF,\n\t\t\t\t   (u64)(unsigned long)prog->bpf_func,\n\t\t\t\t   prog->jited_len, unregister,\n\t\t\t\t   prog->aux->ksym.name);\n\t} else {\n\t\tfor (i = 0; i < prog->aux->func_cnt; i++) {\n\t\t\tstruct bpf_prog *subprog = prog->aux->func[i];\n\n\t\t\tperf_event_ksymbol(\n\t\t\t\tPERF_RECORD_KSYMBOL_TYPE_BPF,\n\t\t\t\t(u64)(unsigned long)subprog->bpf_func,\n\t\t\t\tsubprog->jited_len, unregister,\n\t\t\t\tsubprog->aux->ksym.name);\n\t\t}\n\t}\n}\n\nvoid perf_event_bpf_event(struct bpf_prog *prog,\n\t\t\t  enum perf_bpf_event_type type,\n\t\t\t  u16 flags)\n{\n\tstruct perf_bpf_event bpf_event;\n\n\tif (type <= PERF_BPF_EVENT_UNKNOWN ||\n\t    type >= PERF_BPF_EVENT_MAX)\n\t\treturn;\n\n\tswitch (type) {\n\tcase PERF_BPF_EVENT_PROG_LOAD:\n\tcase PERF_BPF_EVENT_PROG_UNLOAD:\n\t\tif (atomic_read(&nr_ksymbol_events))\n\t\t\tperf_event_bpf_emit_ksymbols(prog, type);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (!atomic_read(&nr_bpf_events))\n\t\treturn;\n\n\tbpf_event = (struct perf_bpf_event){\n\t\t.prog = prog,\n\t\t.event_id = {\n\t\t\t.header = {\n\t\t\t\t.type = PERF_RECORD_BPF_EVENT,\n\t\t\t\t.size = sizeof(bpf_event.event_id),\n\t\t\t},\n\t\t\t.type = type,\n\t\t\t.flags = flags,\n\t\t\t.id = prog->aux->id,\n\t\t},\n\t};\n\n\tBUILD_BUG_ON(BPF_TAG_SIZE % sizeof(u64));\n\n\tmemcpy(bpf_event.event_id.tag, prog->tag, BPF_TAG_SIZE);\n\tperf_iterate_sb(perf_event_bpf_output, &bpf_event, NULL);\n}\n\nstruct perf_text_poke_event {\n\tconst void\t\t*old_bytes;\n\tconst void\t\t*new_bytes;\n\tsize_t\t\t\tpad;\n\tu16\t\t\told_len;\n\tu16\t\t\tnew_len;\n\n\tstruct {\n\t\tstruct perf_event_header\theader;\n\n\t\tu64\t\t\t\taddr;\n\t} event_id;\n};\n\nstatic int perf_event_text_poke_match(struct perf_event *event)\n{\n\treturn event->attr.text_poke;\n}\n\nstatic void perf_event_text_poke_output(struct perf_event *event, void *data)\n{\n\tstruct perf_text_poke_event *text_poke_event = data;\n\tstruct perf_output_handle handle;\n\tstruct perf_sample_data sample;\n\tu64 padding = 0;\n\tint ret;\n\n\tif (!perf_event_text_poke_match(event))\n\t\treturn;\n\n\tperf_event_header__init_id(&text_poke_event->event_id.header, &sample, event);\n\n\tret = perf_output_begin(&handle, &sample, event,\n\t\t\t\ttext_poke_event->event_id.header.size);\n\tif (ret)\n\t\treturn;\n\n\tperf_output_put(&handle, text_poke_event->event_id);\n\tperf_output_put(&handle, text_poke_event->old_len);\n\tperf_output_put(&handle, text_poke_event->new_len);\n\n\t__output_copy(&handle, text_poke_event->old_bytes, text_poke_event->old_len);\n\t__output_copy(&handle, text_poke_event->new_bytes, text_poke_event->new_len);\n\n\tif (text_poke_event->pad)\n\t\t__output_copy(&handle, &padding, text_poke_event->pad);\n\n\tperf_event__output_id_sample(event, &handle, &sample);\n\n\tperf_output_end(&handle);\n}\n\nvoid perf_event_text_poke(const void *addr, const void *old_bytes,\n\t\t\t  size_t old_len, const void *new_bytes, size_t new_len)\n{\n\tstruct perf_text_poke_event text_poke_event;\n\tsize_t tot, pad;\n\n\tif (!atomic_read(&nr_text_poke_events))\n\t\treturn;\n\n\ttot  = sizeof(text_poke_event.old_len) + old_len;\n\ttot += sizeof(text_poke_event.new_len) + new_len;\n\tpad  = ALIGN(tot, sizeof(u64)) - tot;\n\n\ttext_poke_event = (struct perf_text_poke_event){\n\t\t.old_bytes    = old_bytes,\n\t\t.new_bytes    = new_bytes,\n\t\t.pad          = pad,\n\t\t.old_len      = old_len,\n\t\t.new_len      = new_len,\n\t\t.event_id  = {\n\t\t\t.header = {\n\t\t\t\t.type = PERF_RECORD_TEXT_POKE,\n\t\t\t\t.misc = PERF_RECORD_MISC_KERNEL,\n\t\t\t\t.size = sizeof(text_poke_event.event_id) + tot + pad,\n\t\t\t},\n\t\t\t.addr = (unsigned long)addr,\n\t\t},\n\t};\n\n\tperf_iterate_sb(perf_event_text_poke_output, &text_poke_event, NULL);\n}\n\nvoid perf_event_itrace_started(struct perf_event *event)\n{\n\tevent->attach_state |= PERF_ATTACH_ITRACE;\n}\n\nstatic void perf_log_itrace_start(struct perf_event *event)\n{\n\tstruct perf_output_handle handle;\n\tstruct perf_sample_data sample;\n\tstruct perf_aux_event {\n\t\tstruct perf_event_header        header;\n\t\tu32\t\t\t\tpid;\n\t\tu32\t\t\t\ttid;\n\t} rec;\n\tint ret;\n\n\tif (event->parent)\n\t\tevent = event->parent;\n\n\tif (!(event->pmu->capabilities & PERF_PMU_CAP_ITRACE) ||\n\t    event->attach_state & PERF_ATTACH_ITRACE)\n\t\treturn;\n\n\trec.header.type\t= PERF_RECORD_ITRACE_START;\n\trec.header.misc\t= 0;\n\trec.header.size\t= sizeof(rec);\n\trec.pid\t= perf_event_pid(event, current);\n\trec.tid\t= perf_event_tid(event, current);\n\n\tperf_event_header__init_id(&rec.header, &sample, event);\n\tret = perf_output_begin(&handle, &sample, event, rec.header.size);\n\n\tif (ret)\n\t\treturn;\n\n\tperf_output_put(&handle, rec);\n\tperf_event__output_id_sample(event, &handle, &sample);\n\n\tperf_output_end(&handle);\n}\n\nvoid perf_report_aux_output_id(struct perf_event *event, u64 hw_id)\n{\n\tstruct perf_output_handle handle;\n\tstruct perf_sample_data sample;\n\tstruct perf_aux_event {\n\t\tstruct perf_event_header        header;\n\t\tu64\t\t\t\thw_id;\n\t} rec;\n\tint ret;\n\n\tif (event->parent)\n\t\tevent = event->parent;\n\n\trec.header.type\t= PERF_RECORD_AUX_OUTPUT_HW_ID;\n\trec.header.misc\t= 0;\n\trec.header.size\t= sizeof(rec);\n\trec.hw_id\t= hw_id;\n\n\tperf_event_header__init_id(&rec.header, &sample, event);\n\tret = perf_output_begin(&handle, &sample, event, rec.header.size);\n\n\tif (ret)\n\t\treturn;\n\n\tperf_output_put(&handle, rec);\n\tperf_event__output_id_sample(event, &handle, &sample);\n\n\tperf_output_end(&handle);\n}\nEXPORT_SYMBOL_GPL(perf_report_aux_output_id);\n\nstatic int\n__perf_event_account_interrupt(struct perf_event *event, int throttle)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint ret = 0;\n\tu64 seq;\n\n\tseq = __this_cpu_read(perf_throttled_seq);\n\tif (seq != hwc->interrupts_seq) {\n\t\thwc->interrupts_seq = seq;\n\t\thwc->interrupts = 1;\n\t} else {\n\t\thwc->interrupts++;\n\t\tif (unlikely(throttle &&\n\t\t\t     hwc->interrupts > max_samples_per_tick)) {\n\t\t\t__this_cpu_inc(perf_throttled_count);\n\t\t\ttick_dep_set_cpu(smp_processor_id(), TICK_DEP_BIT_PERF_EVENTS);\n\t\t\thwc->interrupts = MAX_INTERRUPTS;\n\t\t\tperf_log_throttle(event, 0);\n\t\t\tret = 1;\n\t\t}\n\t}\n\n\tif (event->attr.freq) {\n\t\tu64 now = perf_clock();\n\t\ts64 delta = now - hwc->freq_time_stamp;\n\n\t\thwc->freq_time_stamp = now;\n\n\t\tif (delta > 0 && delta < 2*TICK_NSEC)\n\t\t\tperf_adjust_period(event, delta, hwc->last_period, true);\n\t}\n\n\treturn ret;\n}\n\nint perf_event_account_interrupt(struct perf_event *event)\n{\n\treturn __perf_event_account_interrupt(event, 1);\n}\n\nstatic inline bool sample_is_allowed(struct perf_event *event, struct pt_regs *regs)\n{\n\t \n\tif (event->attr.exclude_kernel && !user_mode(regs))\n\t\treturn false;\n\n\treturn true;\n}\n\n \n\nstatic int __perf_event_overflow(struct perf_event *event,\n\t\t\t\t int throttle, struct perf_sample_data *data,\n\t\t\t\t struct pt_regs *regs)\n{\n\tint events = atomic_read(&event->event_limit);\n\tint ret = 0;\n\n\t \n\tif (unlikely(!is_sampling_event(event)))\n\t\treturn 0;\n\n\tret = __perf_event_account_interrupt(event, throttle);\n\n\t \n\n\tevent->pending_kill = POLL_IN;\n\tif (events && atomic_dec_and_test(&event->event_limit)) {\n\t\tret = 1;\n\t\tevent->pending_kill = POLL_HUP;\n\t\tperf_event_disable_inatomic(event);\n\t}\n\n\tif (event->attr.sigtrap) {\n\t\t \n\t\tbool valid_sample = sample_is_allowed(event, regs);\n\t\tunsigned int pending_id = 1;\n\n\t\tif (regs)\n\t\t\tpending_id = hash32_ptr((void *)instruction_pointer(regs)) ?: 1;\n\t\tif (!event->pending_sigtrap) {\n\t\t\tevent->pending_sigtrap = pending_id;\n\t\t\tlocal_inc(&event->ctx->nr_pending);\n\t\t} else if (event->attr.exclude_kernel && valid_sample) {\n\t\t\t \n\t\t\tWARN_ON_ONCE(event->pending_sigtrap != pending_id);\n\t\t}\n\n\t\tevent->pending_addr = 0;\n\t\tif (valid_sample && (data->sample_flags & PERF_SAMPLE_ADDR))\n\t\t\tevent->pending_addr = data->addr;\n\t\tirq_work_queue(&event->pending_irq);\n\t}\n\n\tREAD_ONCE(event->overflow_handler)(event, data, regs);\n\n\tif (*perf_event_fasync(event) && event->pending_kill) {\n\t\tevent->pending_wakeup = 1;\n\t\tirq_work_queue(&event->pending_irq);\n\t}\n\n\treturn ret;\n}\n\nint perf_event_overflow(struct perf_event *event,\n\t\t\tstruct perf_sample_data *data,\n\t\t\tstruct pt_regs *regs)\n{\n\treturn __perf_event_overflow(event, 1, data, regs);\n}\n\n \n\nstruct swevent_htable {\n\tstruct swevent_hlist\t\t*swevent_hlist;\n\tstruct mutex\t\t\thlist_mutex;\n\tint\t\t\t\thlist_refcount;\n\n\t \n\tint\t\t\t\trecursion[PERF_NR_CONTEXTS];\n};\n\nstatic DEFINE_PER_CPU(struct swevent_htable, swevent_htable);\n\n \n\nu64 perf_swevent_set_period(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tu64 period = hwc->last_period;\n\tu64 nr, offset;\n\ts64 old, val;\n\n\thwc->last_period = hwc->sample_period;\n\n\told = local64_read(&hwc->period_left);\n\tdo {\n\t\tval = old;\n\t\tif (val < 0)\n\t\t\treturn 0;\n\n\t\tnr = div64_u64(period + val, period);\n\t\toffset = nr * period;\n\t\tval -= offset;\n\t} while (!local64_try_cmpxchg(&hwc->period_left, &old, val));\n\n\treturn nr;\n}\n\nstatic void perf_swevent_overflow(struct perf_event *event, u64 overflow,\n\t\t\t\t    struct perf_sample_data *data,\n\t\t\t\t    struct pt_regs *regs)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint throttle = 0;\n\n\tif (!overflow)\n\t\toverflow = perf_swevent_set_period(event);\n\n\tif (hwc->interrupts == MAX_INTERRUPTS)\n\t\treturn;\n\n\tfor (; overflow; overflow--) {\n\t\tif (__perf_event_overflow(event, throttle,\n\t\t\t\t\t    data, regs)) {\n\t\t\t \n\t\t\tbreak;\n\t\t}\n\t\tthrottle = 1;\n\t}\n}\n\nstatic void perf_swevent_event(struct perf_event *event, u64 nr,\n\t\t\t       struct perf_sample_data *data,\n\t\t\t       struct pt_regs *regs)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tlocal64_add(nr, &event->count);\n\n\tif (!regs)\n\t\treturn;\n\n\tif (!is_sampling_event(event))\n\t\treturn;\n\n\tif ((event->attr.sample_type & PERF_SAMPLE_PERIOD) && !event->attr.freq) {\n\t\tdata->period = nr;\n\t\treturn perf_swevent_overflow(event, 1, data, regs);\n\t} else\n\t\tdata->period = event->hw.last_period;\n\n\tif (nr == 1 && hwc->sample_period == 1 && !event->attr.freq)\n\t\treturn perf_swevent_overflow(event, 1, data, regs);\n\n\tif (local64_add_negative(nr, &hwc->period_left))\n\t\treturn;\n\n\tperf_swevent_overflow(event, 0, data, regs);\n}\n\nstatic int perf_exclude_event(struct perf_event *event,\n\t\t\t      struct pt_regs *regs)\n{\n\tif (event->hw.state & PERF_HES_STOPPED)\n\t\treturn 1;\n\n\tif (regs) {\n\t\tif (event->attr.exclude_user && user_mode(regs))\n\t\t\treturn 1;\n\n\t\tif (event->attr.exclude_kernel && !user_mode(regs))\n\t\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic int perf_swevent_match(struct perf_event *event,\n\t\t\t\tenum perf_type_id type,\n\t\t\t\tu32 event_id,\n\t\t\t\tstruct perf_sample_data *data,\n\t\t\t\tstruct pt_regs *regs)\n{\n\tif (event->attr.type != type)\n\t\treturn 0;\n\n\tif (event->attr.config != event_id)\n\t\treturn 0;\n\n\tif (perf_exclude_event(event, regs))\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic inline u64 swevent_hash(u64 type, u32 event_id)\n{\n\tu64 val = event_id | (type << 32);\n\n\treturn hash_64(val, SWEVENT_HLIST_BITS);\n}\n\nstatic inline struct hlist_head *\n__find_swevent_head(struct swevent_hlist *hlist, u64 type, u32 event_id)\n{\n\tu64 hash = swevent_hash(type, event_id);\n\n\treturn &hlist->heads[hash];\n}\n\n \nstatic inline struct hlist_head *\nfind_swevent_head_rcu(struct swevent_htable *swhash, u64 type, u32 event_id)\n{\n\tstruct swevent_hlist *hlist;\n\n\thlist = rcu_dereference(swhash->swevent_hlist);\n\tif (!hlist)\n\t\treturn NULL;\n\n\treturn __find_swevent_head(hlist, type, event_id);\n}\n\n \nstatic inline struct hlist_head *\nfind_swevent_head(struct swevent_htable *swhash, struct perf_event *event)\n{\n\tstruct swevent_hlist *hlist;\n\tu32 event_id = event->attr.config;\n\tu64 type = event->attr.type;\n\n\t \n\thlist = rcu_dereference_protected(swhash->swevent_hlist,\n\t\t\t\t\t  lockdep_is_held(&event->ctx->lock));\n\tif (!hlist)\n\t\treturn NULL;\n\n\treturn __find_swevent_head(hlist, type, event_id);\n}\n\nstatic void do_perf_sw_event(enum perf_type_id type, u32 event_id,\n\t\t\t\t    u64 nr,\n\t\t\t\t    struct perf_sample_data *data,\n\t\t\t\t    struct pt_regs *regs)\n{\n\tstruct swevent_htable *swhash = this_cpu_ptr(&swevent_htable);\n\tstruct perf_event *event;\n\tstruct hlist_head *head;\n\n\trcu_read_lock();\n\thead = find_swevent_head_rcu(swhash, type, event_id);\n\tif (!head)\n\t\tgoto end;\n\n\thlist_for_each_entry_rcu(event, head, hlist_entry) {\n\t\tif (perf_swevent_match(event, type, event_id, data, regs))\n\t\t\tperf_swevent_event(event, nr, data, regs);\n\t}\nend:\n\trcu_read_unlock();\n}\n\nDEFINE_PER_CPU(struct pt_regs, __perf_regs[4]);\n\nint perf_swevent_get_recursion_context(void)\n{\n\tstruct swevent_htable *swhash = this_cpu_ptr(&swevent_htable);\n\n\treturn get_recursion_context(swhash->recursion);\n}\nEXPORT_SYMBOL_GPL(perf_swevent_get_recursion_context);\n\nvoid perf_swevent_put_recursion_context(int rctx)\n{\n\tstruct swevent_htable *swhash = this_cpu_ptr(&swevent_htable);\n\n\tput_recursion_context(swhash->recursion, rctx);\n}\n\nvoid ___perf_sw_event(u32 event_id, u64 nr, struct pt_regs *regs, u64 addr)\n{\n\tstruct perf_sample_data data;\n\n\tif (WARN_ON_ONCE(!regs))\n\t\treturn;\n\n\tperf_sample_data_init(&data, addr, 0);\n\tdo_perf_sw_event(PERF_TYPE_SOFTWARE, event_id, nr, &data, regs);\n}\n\nvoid __perf_sw_event(u32 event_id, u64 nr, struct pt_regs *regs, u64 addr)\n{\n\tint rctx;\n\n\tpreempt_disable_notrace();\n\trctx = perf_swevent_get_recursion_context();\n\tif (unlikely(rctx < 0))\n\t\tgoto fail;\n\n\t___perf_sw_event(event_id, nr, regs, addr);\n\n\tperf_swevent_put_recursion_context(rctx);\nfail:\n\tpreempt_enable_notrace();\n}\n\nstatic void perf_swevent_read(struct perf_event *event)\n{\n}\n\nstatic int perf_swevent_add(struct perf_event *event, int flags)\n{\n\tstruct swevent_htable *swhash = this_cpu_ptr(&swevent_htable);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct hlist_head *head;\n\n\tif (is_sampling_event(event)) {\n\t\thwc->last_period = hwc->sample_period;\n\t\tperf_swevent_set_period(event);\n\t}\n\n\thwc->state = !(flags & PERF_EF_START);\n\n\thead = find_swevent_head(swhash, event);\n\tif (WARN_ON_ONCE(!head))\n\t\treturn -EINVAL;\n\n\thlist_add_head_rcu(&event->hlist_entry, head);\n\tperf_event_update_userpage(event);\n\n\treturn 0;\n}\n\nstatic void perf_swevent_del(struct perf_event *event, int flags)\n{\n\thlist_del_rcu(&event->hlist_entry);\n}\n\nstatic void perf_swevent_start(struct perf_event *event, int flags)\n{\n\tevent->hw.state = 0;\n}\n\nstatic void perf_swevent_stop(struct perf_event *event, int flags)\n{\n\tevent->hw.state = PERF_HES_STOPPED;\n}\n\n \nstatic inline struct swevent_hlist *\nswevent_hlist_deref(struct swevent_htable *swhash)\n{\n\treturn rcu_dereference_protected(swhash->swevent_hlist,\n\t\t\t\t\t lockdep_is_held(&swhash->hlist_mutex));\n}\n\nstatic void swevent_hlist_release(struct swevent_htable *swhash)\n{\n\tstruct swevent_hlist *hlist = swevent_hlist_deref(swhash);\n\n\tif (!hlist)\n\t\treturn;\n\n\tRCU_INIT_POINTER(swhash->swevent_hlist, NULL);\n\tkfree_rcu(hlist, rcu_head);\n}\n\nstatic void swevent_hlist_put_cpu(int cpu)\n{\n\tstruct swevent_htable *swhash = &per_cpu(swevent_htable, cpu);\n\n\tmutex_lock(&swhash->hlist_mutex);\n\n\tif (!--swhash->hlist_refcount)\n\t\tswevent_hlist_release(swhash);\n\n\tmutex_unlock(&swhash->hlist_mutex);\n}\n\nstatic void swevent_hlist_put(void)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu)\n\t\tswevent_hlist_put_cpu(cpu);\n}\n\nstatic int swevent_hlist_get_cpu(int cpu)\n{\n\tstruct swevent_htable *swhash = &per_cpu(swevent_htable, cpu);\n\tint err = 0;\n\n\tmutex_lock(&swhash->hlist_mutex);\n\tif (!swevent_hlist_deref(swhash) &&\n\t    cpumask_test_cpu(cpu, perf_online_mask)) {\n\t\tstruct swevent_hlist *hlist;\n\n\t\thlist = kzalloc(sizeof(*hlist), GFP_KERNEL);\n\t\tif (!hlist) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto exit;\n\t\t}\n\t\trcu_assign_pointer(swhash->swevent_hlist, hlist);\n\t}\n\tswhash->hlist_refcount++;\nexit:\n\tmutex_unlock(&swhash->hlist_mutex);\n\n\treturn err;\n}\n\nstatic int swevent_hlist_get(void)\n{\n\tint err, cpu, failed_cpu;\n\n\tmutex_lock(&pmus_lock);\n\tfor_each_possible_cpu(cpu) {\n\t\terr = swevent_hlist_get_cpu(cpu);\n\t\tif (err) {\n\t\t\tfailed_cpu = cpu;\n\t\t\tgoto fail;\n\t\t}\n\t}\n\tmutex_unlock(&pmus_lock);\n\treturn 0;\nfail:\n\tfor_each_possible_cpu(cpu) {\n\t\tif (cpu == failed_cpu)\n\t\t\tbreak;\n\t\tswevent_hlist_put_cpu(cpu);\n\t}\n\tmutex_unlock(&pmus_lock);\n\treturn err;\n}\n\nstruct static_key perf_swevent_enabled[PERF_COUNT_SW_MAX];\n\nstatic void sw_perf_event_destroy(struct perf_event *event)\n{\n\tu64 event_id = event->attr.config;\n\n\tWARN_ON(event->parent);\n\n\tstatic_key_slow_dec(&perf_swevent_enabled[event_id]);\n\tswevent_hlist_put();\n}\n\nstatic struct pmu perf_cpu_clock;  \nstatic struct pmu perf_task_clock;\n\nstatic int perf_swevent_init(struct perf_event *event)\n{\n\tu64 event_id = event->attr.config;\n\n\tif (event->attr.type != PERF_TYPE_SOFTWARE)\n\t\treturn -ENOENT;\n\n\t \n\tif (has_branch_stack(event))\n\t\treturn -EOPNOTSUPP;\n\n\tswitch (event_id) {\n\tcase PERF_COUNT_SW_CPU_CLOCK:\n\t\tevent->attr.type = perf_cpu_clock.type;\n\t\treturn -ENOENT;\n\tcase PERF_COUNT_SW_TASK_CLOCK:\n\t\tevent->attr.type = perf_task_clock.type;\n\t\treturn -ENOENT;\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (event_id >= PERF_COUNT_SW_MAX)\n\t\treturn -ENOENT;\n\n\tif (!event->parent) {\n\t\tint err;\n\n\t\terr = swevent_hlist_get();\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tstatic_key_slow_inc(&perf_swevent_enabled[event_id]);\n\t\tevent->destroy = sw_perf_event_destroy;\n\t}\n\n\treturn 0;\n}\n\nstatic struct pmu perf_swevent = {\n\t.task_ctx_nr\t= perf_sw_context,\n\n\t.capabilities\t= PERF_PMU_CAP_NO_NMI,\n\n\t.event_init\t= perf_swevent_init,\n\t.add\t\t= perf_swevent_add,\n\t.del\t\t= perf_swevent_del,\n\t.start\t\t= perf_swevent_start,\n\t.stop\t\t= perf_swevent_stop,\n\t.read\t\t= perf_swevent_read,\n};\n\n#ifdef CONFIG_EVENT_TRACING\n\nstatic void tp_perf_event_destroy(struct perf_event *event)\n{\n\tperf_trace_destroy(event);\n}\n\nstatic int perf_tp_event_init(struct perf_event *event)\n{\n\tint err;\n\n\tif (event->attr.type != PERF_TYPE_TRACEPOINT)\n\t\treturn -ENOENT;\n\n\t \n\tif (has_branch_stack(event))\n\t\treturn -EOPNOTSUPP;\n\n\terr = perf_trace_init(event);\n\tif (err)\n\t\treturn err;\n\n\tevent->destroy = tp_perf_event_destroy;\n\n\treturn 0;\n}\n\nstatic struct pmu perf_tracepoint = {\n\t.task_ctx_nr\t= perf_sw_context,\n\n\t.event_init\t= perf_tp_event_init,\n\t.add\t\t= perf_trace_add,\n\t.del\t\t= perf_trace_del,\n\t.start\t\t= perf_swevent_start,\n\t.stop\t\t= perf_swevent_stop,\n\t.read\t\t= perf_swevent_read,\n};\n\nstatic int perf_tp_filter_match(struct perf_event *event,\n\t\t\t\tstruct perf_sample_data *data)\n{\n\tvoid *record = data->raw->frag.data;\n\n\t \n\tif (event->parent)\n\t\tevent = event->parent;\n\n\tif (likely(!event->filter) || filter_match_preds(event->filter, record))\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic int perf_tp_event_match(struct perf_event *event,\n\t\t\t\tstruct perf_sample_data *data,\n\t\t\t\tstruct pt_regs *regs)\n{\n\tif (event->hw.state & PERF_HES_STOPPED)\n\t\treturn 0;\n\t \n\tif (event->attr.exclude_kernel && !user_mode(regs))\n\t\treturn 0;\n\n\tif (!perf_tp_filter_match(event, data))\n\t\treturn 0;\n\n\treturn 1;\n}\n\nvoid perf_trace_run_bpf_submit(void *raw_data, int size, int rctx,\n\t\t\t       struct trace_event_call *call, u64 count,\n\t\t\t       struct pt_regs *regs, struct hlist_head *head,\n\t\t\t       struct task_struct *task)\n{\n\tif (bpf_prog_array_valid(call)) {\n\t\t*(struct pt_regs **)raw_data = regs;\n\t\tif (!trace_call_bpf(call, raw_data) || hlist_empty(head)) {\n\t\t\tperf_swevent_put_recursion_context(rctx);\n\t\t\treturn;\n\t\t}\n\t}\n\tperf_tp_event(call->event.type, count, raw_data, size, regs, head,\n\t\t      rctx, task);\n}\nEXPORT_SYMBOL_GPL(perf_trace_run_bpf_submit);\n\nstatic void __perf_tp_event_target_task(u64 count, void *record,\n\t\t\t\t\tstruct pt_regs *regs,\n\t\t\t\t\tstruct perf_sample_data *data,\n\t\t\t\t\tstruct perf_event *event)\n{\n\tstruct trace_entry *entry = record;\n\n\tif (event->attr.config != entry->type)\n\t\treturn;\n\t \n\tif (event->attr.sigtrap)\n\t\treturn;\n\tif (perf_tp_event_match(event, data, regs))\n\t\tperf_swevent_event(event, count, data, regs);\n}\n\nstatic void perf_tp_event_target_task(u64 count, void *record,\n\t\t\t\t      struct pt_regs *regs,\n\t\t\t\t      struct perf_sample_data *data,\n\t\t\t\t      struct perf_event_context *ctx)\n{\n\tunsigned int cpu = smp_processor_id();\n\tstruct pmu *pmu = &perf_tracepoint;\n\tstruct perf_event *event, *sibling;\n\n\tperf_event_groups_for_cpu_pmu(event, &ctx->pinned_groups, cpu, pmu) {\n\t\t__perf_tp_event_target_task(count, record, regs, data, event);\n\t\tfor_each_sibling_event(sibling, event)\n\t\t\t__perf_tp_event_target_task(count, record, regs, data, sibling);\n\t}\n\n\tperf_event_groups_for_cpu_pmu(event, &ctx->flexible_groups, cpu, pmu) {\n\t\t__perf_tp_event_target_task(count, record, regs, data, event);\n\t\tfor_each_sibling_event(sibling, event)\n\t\t\t__perf_tp_event_target_task(count, record, regs, data, sibling);\n\t}\n}\n\nvoid perf_tp_event(u16 event_type, u64 count, void *record, int entry_size,\n\t\t   struct pt_regs *regs, struct hlist_head *head, int rctx,\n\t\t   struct task_struct *task)\n{\n\tstruct perf_sample_data data;\n\tstruct perf_event *event;\n\n\tstruct perf_raw_record raw = {\n\t\t.frag = {\n\t\t\t.size = entry_size,\n\t\t\t.data = record,\n\t\t},\n\t};\n\n\tperf_sample_data_init(&data, 0, 0);\n\tperf_sample_save_raw_data(&data, &raw);\n\n\tperf_trace_buf_update(record, event_type);\n\n\thlist_for_each_entry_rcu(event, head, hlist_entry) {\n\t\tif (perf_tp_event_match(event, &data, regs)) {\n\t\t\tperf_swevent_event(event, count, &data, regs);\n\n\t\t\t \n\t\t\tperf_sample_data_init(&data, 0, 0);\n\t\t\tperf_sample_save_raw_data(&data, &raw);\n\t\t}\n\t}\n\n\t \n\tif (task && task != current) {\n\t\tstruct perf_event_context *ctx;\n\n\t\trcu_read_lock();\n\t\tctx = rcu_dereference(task->perf_event_ctxp);\n\t\tif (!ctx)\n\t\t\tgoto unlock;\n\n\t\traw_spin_lock(&ctx->lock);\n\t\tperf_tp_event_target_task(count, record, regs, &data, ctx);\n\t\traw_spin_unlock(&ctx->lock);\nunlock:\n\t\trcu_read_unlock();\n\t}\n\n\tperf_swevent_put_recursion_context(rctx);\n}\nEXPORT_SYMBOL_GPL(perf_tp_event);\n\n#if defined(CONFIG_KPROBE_EVENTS) || defined(CONFIG_UPROBE_EVENTS)\n \nenum perf_probe_config {\n\tPERF_PROBE_CONFIG_IS_RETPROBE = 1U << 0,   \n\tPERF_UPROBE_REF_CTR_OFFSET_BITS = 32,\n\tPERF_UPROBE_REF_CTR_OFFSET_SHIFT = 64 - PERF_UPROBE_REF_CTR_OFFSET_BITS,\n};\n\nPMU_FORMAT_ATTR(retprobe, \"config:0\");\n#endif\n\n#ifdef CONFIG_KPROBE_EVENTS\nstatic struct attribute *kprobe_attrs[] = {\n\t&format_attr_retprobe.attr,\n\tNULL,\n};\n\nstatic struct attribute_group kprobe_format_group = {\n\t.name = \"format\",\n\t.attrs = kprobe_attrs,\n};\n\nstatic const struct attribute_group *kprobe_attr_groups[] = {\n\t&kprobe_format_group,\n\tNULL,\n};\n\nstatic int perf_kprobe_event_init(struct perf_event *event);\nstatic struct pmu perf_kprobe = {\n\t.task_ctx_nr\t= perf_sw_context,\n\t.event_init\t= perf_kprobe_event_init,\n\t.add\t\t= perf_trace_add,\n\t.del\t\t= perf_trace_del,\n\t.start\t\t= perf_swevent_start,\n\t.stop\t\t= perf_swevent_stop,\n\t.read\t\t= perf_swevent_read,\n\t.attr_groups\t= kprobe_attr_groups,\n};\n\nstatic int perf_kprobe_event_init(struct perf_event *event)\n{\n\tint err;\n\tbool is_retprobe;\n\n\tif (event->attr.type != perf_kprobe.type)\n\t\treturn -ENOENT;\n\n\tif (!perfmon_capable())\n\t\treturn -EACCES;\n\n\t \n\tif (has_branch_stack(event))\n\t\treturn -EOPNOTSUPP;\n\n\tis_retprobe = event->attr.config & PERF_PROBE_CONFIG_IS_RETPROBE;\n\terr = perf_kprobe_init(event, is_retprobe);\n\tif (err)\n\t\treturn err;\n\n\tevent->destroy = perf_kprobe_destroy;\n\n\treturn 0;\n}\n#endif  \n\n#ifdef CONFIG_UPROBE_EVENTS\nPMU_FORMAT_ATTR(ref_ctr_offset, \"config:32-63\");\n\nstatic struct attribute *uprobe_attrs[] = {\n\t&format_attr_retprobe.attr,\n\t&format_attr_ref_ctr_offset.attr,\n\tNULL,\n};\n\nstatic struct attribute_group uprobe_format_group = {\n\t.name = \"format\",\n\t.attrs = uprobe_attrs,\n};\n\nstatic const struct attribute_group *uprobe_attr_groups[] = {\n\t&uprobe_format_group,\n\tNULL,\n};\n\nstatic int perf_uprobe_event_init(struct perf_event *event);\nstatic struct pmu perf_uprobe = {\n\t.task_ctx_nr\t= perf_sw_context,\n\t.event_init\t= perf_uprobe_event_init,\n\t.add\t\t= perf_trace_add,\n\t.del\t\t= perf_trace_del,\n\t.start\t\t= perf_swevent_start,\n\t.stop\t\t= perf_swevent_stop,\n\t.read\t\t= perf_swevent_read,\n\t.attr_groups\t= uprobe_attr_groups,\n};\n\nstatic int perf_uprobe_event_init(struct perf_event *event)\n{\n\tint err;\n\tunsigned long ref_ctr_offset;\n\tbool is_retprobe;\n\n\tif (event->attr.type != perf_uprobe.type)\n\t\treturn -ENOENT;\n\n\tif (!perfmon_capable())\n\t\treturn -EACCES;\n\n\t \n\tif (has_branch_stack(event))\n\t\treturn -EOPNOTSUPP;\n\n\tis_retprobe = event->attr.config & PERF_PROBE_CONFIG_IS_RETPROBE;\n\tref_ctr_offset = event->attr.config >> PERF_UPROBE_REF_CTR_OFFSET_SHIFT;\n\terr = perf_uprobe_init(event, ref_ctr_offset, is_retprobe);\n\tif (err)\n\t\treturn err;\n\n\tevent->destroy = perf_uprobe_destroy;\n\n\treturn 0;\n}\n#endif  \n\nstatic inline void perf_tp_register(void)\n{\n\tperf_pmu_register(&perf_tracepoint, \"tracepoint\", PERF_TYPE_TRACEPOINT);\n#ifdef CONFIG_KPROBE_EVENTS\n\tperf_pmu_register(&perf_kprobe, \"kprobe\", -1);\n#endif\n#ifdef CONFIG_UPROBE_EVENTS\n\tperf_pmu_register(&perf_uprobe, \"uprobe\", -1);\n#endif\n}\n\nstatic void perf_event_free_filter(struct perf_event *event)\n{\n\tftrace_profile_free_filter(event);\n}\n\n#ifdef CONFIG_BPF_SYSCALL\nstatic void bpf_overflow_handler(struct perf_event *event,\n\t\t\t\t struct perf_sample_data *data,\n\t\t\t\t struct pt_regs *regs)\n{\n\tstruct bpf_perf_event_data_kern ctx = {\n\t\t.data = data,\n\t\t.event = event,\n\t};\n\tstruct bpf_prog *prog;\n\tint ret = 0;\n\n\tctx.regs = perf_arch_bpf_user_pt_regs(regs);\n\tif (unlikely(__this_cpu_inc_return(bpf_prog_active) != 1))\n\t\tgoto out;\n\trcu_read_lock();\n\tprog = READ_ONCE(event->prog);\n\tif (prog) {\n\t\tperf_prepare_sample(data, event, regs);\n\t\tret = bpf_prog_run(prog, &ctx);\n\t}\n\trcu_read_unlock();\nout:\n\t__this_cpu_dec(bpf_prog_active);\n\tif (!ret)\n\t\treturn;\n\n\tevent->orig_overflow_handler(event, data, regs);\n}\n\nstatic int perf_event_set_bpf_handler(struct perf_event *event,\n\t\t\t\t      struct bpf_prog *prog,\n\t\t\t\t      u64 bpf_cookie)\n{\n\tif (event->overflow_handler_context)\n\t\t \n\t\treturn -EINVAL;\n\n\tif (event->prog)\n\t\treturn -EEXIST;\n\n\tif (prog->type != BPF_PROG_TYPE_PERF_EVENT)\n\t\treturn -EINVAL;\n\n\tif (event->attr.precise_ip &&\n\t    prog->call_get_stack &&\n\t    (!(event->attr.sample_type & PERF_SAMPLE_CALLCHAIN) ||\n\t     event->attr.exclude_callchain_kernel ||\n\t     event->attr.exclude_callchain_user)) {\n\t\t \n\t\treturn -EPROTO;\n\t}\n\n\tevent->prog = prog;\n\tevent->bpf_cookie = bpf_cookie;\n\tevent->orig_overflow_handler = READ_ONCE(event->overflow_handler);\n\tWRITE_ONCE(event->overflow_handler, bpf_overflow_handler);\n\treturn 0;\n}\n\nstatic void perf_event_free_bpf_handler(struct perf_event *event)\n{\n\tstruct bpf_prog *prog = event->prog;\n\n\tif (!prog)\n\t\treturn;\n\n\tWRITE_ONCE(event->overflow_handler, event->orig_overflow_handler);\n\tevent->prog = NULL;\n\tbpf_prog_put(prog);\n}\n#else\nstatic int perf_event_set_bpf_handler(struct perf_event *event,\n\t\t\t\t      struct bpf_prog *prog,\n\t\t\t\t      u64 bpf_cookie)\n{\n\treturn -EOPNOTSUPP;\n}\nstatic void perf_event_free_bpf_handler(struct perf_event *event)\n{\n}\n#endif\n\n \nstatic inline bool perf_event_is_tracing(struct perf_event *event)\n{\n\tif (event->pmu == &perf_tracepoint)\n\t\treturn true;\n#ifdef CONFIG_KPROBE_EVENTS\n\tif (event->pmu == &perf_kprobe)\n\t\treturn true;\n#endif\n#ifdef CONFIG_UPROBE_EVENTS\n\tif (event->pmu == &perf_uprobe)\n\t\treturn true;\n#endif\n\treturn false;\n}\n\nint perf_event_set_bpf_prog(struct perf_event *event, struct bpf_prog *prog,\n\t\t\t    u64 bpf_cookie)\n{\n\tbool is_kprobe, is_uprobe, is_tracepoint, is_syscall_tp;\n\n\tif (!perf_event_is_tracing(event))\n\t\treturn perf_event_set_bpf_handler(event, prog, bpf_cookie);\n\n\tis_kprobe = event->tp_event->flags & TRACE_EVENT_FL_KPROBE;\n\tis_uprobe = event->tp_event->flags & TRACE_EVENT_FL_UPROBE;\n\tis_tracepoint = event->tp_event->flags & TRACE_EVENT_FL_TRACEPOINT;\n\tis_syscall_tp = is_syscall_trace_event(event->tp_event);\n\tif (!is_kprobe && !is_uprobe && !is_tracepoint && !is_syscall_tp)\n\t\t \n\t\treturn -EINVAL;\n\n\tif (((is_kprobe || is_uprobe) && prog->type != BPF_PROG_TYPE_KPROBE) ||\n\t    (is_tracepoint && prog->type != BPF_PROG_TYPE_TRACEPOINT) ||\n\t    (is_syscall_tp && prog->type != BPF_PROG_TYPE_TRACEPOINT))\n\t\treturn -EINVAL;\n\n\tif (prog->type == BPF_PROG_TYPE_KPROBE && prog->aux->sleepable && !is_uprobe)\n\t\t \n\t\treturn -EINVAL;\n\n\t \n\tif (prog->kprobe_override && !is_kprobe)\n\t\treturn -EINVAL;\n\n\tif (is_tracepoint || is_syscall_tp) {\n\t\tint off = trace_event_get_offsets(event->tp_event);\n\n\t\tif (prog->aux->max_ctx_offset > off)\n\t\t\treturn -EACCES;\n\t}\n\n\treturn perf_event_attach_bpf_prog(event, prog, bpf_cookie);\n}\n\nvoid perf_event_free_bpf_prog(struct perf_event *event)\n{\n\tif (!perf_event_is_tracing(event)) {\n\t\tperf_event_free_bpf_handler(event);\n\t\treturn;\n\t}\n\tperf_event_detach_bpf_prog(event);\n}\n\n#else\n\nstatic inline void perf_tp_register(void)\n{\n}\n\nstatic void perf_event_free_filter(struct perf_event *event)\n{\n}\n\nint perf_event_set_bpf_prog(struct perf_event *event, struct bpf_prog *prog,\n\t\t\t    u64 bpf_cookie)\n{\n\treturn -ENOENT;\n}\n\nvoid perf_event_free_bpf_prog(struct perf_event *event)\n{\n}\n#endif  \n\n#ifdef CONFIG_HAVE_HW_BREAKPOINT\nvoid perf_bp_event(struct perf_event *bp, void *data)\n{\n\tstruct perf_sample_data sample;\n\tstruct pt_regs *regs = data;\n\n\tperf_sample_data_init(&sample, bp->attr.bp_addr, 0);\n\n\tif (!bp->hw.state && !perf_exclude_event(bp, regs))\n\t\tperf_swevent_event(bp, 1, &sample, regs);\n}\n#endif\n\n \nstatic struct perf_addr_filter *\nperf_addr_filter_new(struct perf_event *event, struct list_head *filters)\n{\n\tint node = cpu_to_node(event->cpu == -1 ? 0 : event->cpu);\n\tstruct perf_addr_filter *filter;\n\n\tfilter = kzalloc_node(sizeof(*filter), GFP_KERNEL, node);\n\tif (!filter)\n\t\treturn NULL;\n\n\tINIT_LIST_HEAD(&filter->entry);\n\tlist_add_tail(&filter->entry, filters);\n\n\treturn filter;\n}\n\nstatic void free_filters_list(struct list_head *filters)\n{\n\tstruct perf_addr_filter *filter, *iter;\n\n\tlist_for_each_entry_safe(filter, iter, filters, entry) {\n\t\tpath_put(&filter->path);\n\t\tlist_del(&filter->entry);\n\t\tkfree(filter);\n\t}\n}\n\n \nstatic void perf_addr_filters_splice(struct perf_event *event,\n\t\t\t\t     struct list_head *head)\n{\n\tunsigned long flags;\n\tLIST_HEAD(list);\n\n\tif (!has_addr_filter(event))\n\t\treturn;\n\n\t \n\tif (event->parent)\n\t\treturn;\n\n\traw_spin_lock_irqsave(&event->addr_filters.lock, flags);\n\n\tlist_splice_init(&event->addr_filters.list, &list);\n\tif (head)\n\t\tlist_splice(head, &event->addr_filters.list);\n\n\traw_spin_unlock_irqrestore(&event->addr_filters.lock, flags);\n\n\tfree_filters_list(&list);\n}\n\n \nstatic void perf_addr_filter_apply(struct perf_addr_filter *filter,\n\t\t\t\t   struct mm_struct *mm,\n\t\t\t\t   struct perf_addr_filter_range *fr)\n{\n\tstruct vm_area_struct *vma;\n\tVMA_ITERATOR(vmi, mm, 0);\n\n\tfor_each_vma(vmi, vma) {\n\t\tif (!vma->vm_file)\n\t\t\tcontinue;\n\n\t\tif (perf_addr_filter_vma_adjust(filter, vma, fr))\n\t\t\treturn;\n\t}\n}\n\n \nstatic void perf_event_addr_filters_apply(struct perf_event *event)\n{\n\tstruct perf_addr_filters_head *ifh = perf_event_addr_filters(event);\n\tstruct task_struct *task = READ_ONCE(event->ctx->task);\n\tstruct perf_addr_filter *filter;\n\tstruct mm_struct *mm = NULL;\n\tunsigned int count = 0;\n\tunsigned long flags;\n\n\t \n\tif (task == TASK_TOMBSTONE)\n\t\treturn;\n\n\tif (ifh->nr_file_filters) {\n\t\tmm = get_task_mm(task);\n\t\tif (!mm)\n\t\t\tgoto restart;\n\n\t\tmmap_read_lock(mm);\n\t}\n\n\traw_spin_lock_irqsave(&ifh->lock, flags);\n\tlist_for_each_entry(filter, &ifh->list, entry) {\n\t\tif (filter->path.dentry) {\n\t\t\t \n\t\t\tevent->addr_filter_ranges[count].start = 0;\n\t\t\tevent->addr_filter_ranges[count].size = 0;\n\n\t\t\tperf_addr_filter_apply(filter, mm, &event->addr_filter_ranges[count]);\n\t\t} else {\n\t\t\tevent->addr_filter_ranges[count].start = filter->offset;\n\t\t\tevent->addr_filter_ranges[count].size  = filter->size;\n\t\t}\n\n\t\tcount++;\n\t}\n\n\tevent->addr_filters_gen++;\n\traw_spin_unlock_irqrestore(&ifh->lock, flags);\n\n\tif (ifh->nr_file_filters) {\n\t\tmmap_read_unlock(mm);\n\n\t\tmmput(mm);\n\t}\n\nrestart:\n\tperf_event_stop(event, 1);\n}\n\n \nenum {\n\tIF_ACT_NONE = -1,\n\tIF_ACT_FILTER,\n\tIF_ACT_START,\n\tIF_ACT_STOP,\n\tIF_SRC_FILE,\n\tIF_SRC_KERNEL,\n\tIF_SRC_FILEADDR,\n\tIF_SRC_KERNELADDR,\n};\n\nenum {\n\tIF_STATE_ACTION = 0,\n\tIF_STATE_SOURCE,\n\tIF_STATE_END,\n};\n\nstatic const match_table_t if_tokens = {\n\t{ IF_ACT_FILTER,\t\"filter\" },\n\t{ IF_ACT_START,\t\t\"start\" },\n\t{ IF_ACT_STOP,\t\t\"stop\" },\n\t{ IF_SRC_FILE,\t\t\"%u/%u@%s\" },\n\t{ IF_SRC_KERNEL,\t\"%u/%u\" },\n\t{ IF_SRC_FILEADDR,\t\"%u@%s\" },\n\t{ IF_SRC_KERNELADDR,\t\"%u\" },\n\t{ IF_ACT_NONE,\t\tNULL },\n};\n\n \nstatic int\nperf_event_parse_addr_filter(struct perf_event *event, char *fstr,\n\t\t\t     struct list_head *filters)\n{\n\tstruct perf_addr_filter *filter = NULL;\n\tchar *start, *orig, *filename = NULL;\n\tsubstring_t args[MAX_OPT_ARGS];\n\tint state = IF_STATE_ACTION, token;\n\tunsigned int kernel = 0;\n\tint ret = -EINVAL;\n\n\torig = fstr = kstrdup(fstr, GFP_KERNEL);\n\tif (!fstr)\n\t\treturn -ENOMEM;\n\n\twhile ((start = strsep(&fstr, \" ,\\n\")) != NULL) {\n\t\tstatic const enum perf_addr_filter_action_t actions[] = {\n\t\t\t[IF_ACT_FILTER]\t= PERF_ADDR_FILTER_ACTION_FILTER,\n\t\t\t[IF_ACT_START]\t= PERF_ADDR_FILTER_ACTION_START,\n\t\t\t[IF_ACT_STOP]\t= PERF_ADDR_FILTER_ACTION_STOP,\n\t\t};\n\t\tret = -EINVAL;\n\n\t\tif (!*start)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (state == IF_STATE_ACTION) {\n\t\t\tfilter = perf_addr_filter_new(event, filters);\n\t\t\tif (!filter)\n\t\t\t\tgoto fail;\n\t\t}\n\n\t\ttoken = match_token(start, if_tokens, args);\n\t\tswitch (token) {\n\t\tcase IF_ACT_FILTER:\n\t\tcase IF_ACT_START:\n\t\tcase IF_ACT_STOP:\n\t\t\tif (state != IF_STATE_ACTION)\n\t\t\t\tgoto fail;\n\n\t\t\tfilter->action = actions[token];\n\t\t\tstate = IF_STATE_SOURCE;\n\t\t\tbreak;\n\n\t\tcase IF_SRC_KERNELADDR:\n\t\tcase IF_SRC_KERNEL:\n\t\t\tkernel = 1;\n\t\t\tfallthrough;\n\n\t\tcase IF_SRC_FILEADDR:\n\t\tcase IF_SRC_FILE:\n\t\t\tif (state != IF_STATE_SOURCE)\n\t\t\t\tgoto fail;\n\n\t\t\t*args[0].to = 0;\n\t\t\tret = kstrtoul(args[0].from, 0, &filter->offset);\n\t\t\tif (ret)\n\t\t\t\tgoto fail;\n\n\t\t\tif (token == IF_SRC_KERNEL || token == IF_SRC_FILE) {\n\t\t\t\t*args[1].to = 0;\n\t\t\t\tret = kstrtoul(args[1].from, 0, &filter->size);\n\t\t\t\tif (ret)\n\t\t\t\t\tgoto fail;\n\t\t\t}\n\n\t\t\tif (token == IF_SRC_FILE || token == IF_SRC_FILEADDR) {\n\t\t\t\tint fpos = token == IF_SRC_FILE ? 2 : 1;\n\n\t\t\t\tkfree(filename);\n\t\t\t\tfilename = match_strdup(&args[fpos]);\n\t\t\t\tif (!filename) {\n\t\t\t\t\tret = -ENOMEM;\n\t\t\t\t\tgoto fail;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tstate = IF_STATE_END;\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tgoto fail;\n\t\t}\n\n\t\t \n\t\tif (state == IF_STATE_END) {\n\t\t\tret = -EINVAL;\n\n\t\t\t \n\t\t\tif (filter->action == PERF_ADDR_FILTER_ACTION_FILTER &&\n\t\t\t    !filter->size)\n\t\t\t\tgoto fail;\n\n\t\t\tif (!kernel) {\n\t\t\t\tif (!filename)\n\t\t\t\t\tgoto fail;\n\n\t\t\t\t \n\t\t\t\tret = -EOPNOTSUPP;\n\t\t\t\tif (!event->ctx->task)\n\t\t\t\t\tgoto fail;\n\n\t\t\t\t \n\t\t\t\tret = kern_path(filename, LOOKUP_FOLLOW,\n\t\t\t\t\t\t&filter->path);\n\t\t\t\tif (ret)\n\t\t\t\t\tgoto fail;\n\n\t\t\t\tret = -EINVAL;\n\t\t\t\tif (!filter->path.dentry ||\n\t\t\t\t    !S_ISREG(d_inode(filter->path.dentry)\n\t\t\t\t\t     ->i_mode))\n\t\t\t\t\tgoto fail;\n\n\t\t\t\tevent->addr_filters.nr_file_filters++;\n\t\t\t}\n\n\t\t\t \n\t\t\tkfree(filename);\n\t\t\tfilename = NULL;\n\t\t\tstate = IF_STATE_ACTION;\n\t\t\tfilter = NULL;\n\t\t\tkernel = 0;\n\t\t}\n\t}\n\n\tif (state != IF_STATE_ACTION)\n\t\tgoto fail;\n\n\tkfree(filename);\n\tkfree(orig);\n\n\treturn 0;\n\nfail:\n\tkfree(filename);\n\tfree_filters_list(filters);\n\tkfree(orig);\n\n\treturn ret;\n}\n\nstatic int\nperf_event_set_addr_filter(struct perf_event *event, char *filter_str)\n{\n\tLIST_HEAD(filters);\n\tint ret;\n\n\t \n\tlockdep_assert_held(&event->ctx->mutex);\n\n\tif (WARN_ON_ONCE(event->parent))\n\t\treturn -EINVAL;\n\n\tret = perf_event_parse_addr_filter(event, filter_str, &filters);\n\tif (ret)\n\t\tgoto fail_clear_files;\n\n\tret = event->pmu->addr_filters_validate(&filters);\n\tif (ret)\n\t\tgoto fail_free_filters;\n\n\t \n\tperf_addr_filters_splice(event, &filters);\n\n\t \n\tperf_event_for_each_child(event, perf_event_addr_filters_apply);\n\n\treturn ret;\n\nfail_free_filters:\n\tfree_filters_list(&filters);\n\nfail_clear_files:\n\tevent->addr_filters.nr_file_filters = 0;\n\n\treturn ret;\n}\n\nstatic int perf_event_set_filter(struct perf_event *event, void __user *arg)\n{\n\tint ret = -EINVAL;\n\tchar *filter_str;\n\n\tfilter_str = strndup_user(arg, PAGE_SIZE);\n\tif (IS_ERR(filter_str))\n\t\treturn PTR_ERR(filter_str);\n\n#ifdef CONFIG_EVENT_TRACING\n\tif (perf_event_is_tracing(event)) {\n\t\tstruct perf_event_context *ctx = event->ctx;\n\n\t\t \n\t\tmutex_unlock(&ctx->mutex);\n\t\tret = ftrace_profile_set_filter(event, event->attr.config, filter_str);\n\t\tmutex_lock(&ctx->mutex);\n\t} else\n#endif\n\tif (has_addr_filter(event))\n\t\tret = perf_event_set_addr_filter(event, filter_str);\n\n\tkfree(filter_str);\n\treturn ret;\n}\n\n \n\nstatic enum hrtimer_restart perf_swevent_hrtimer(struct hrtimer *hrtimer)\n{\n\tenum hrtimer_restart ret = HRTIMER_RESTART;\n\tstruct perf_sample_data data;\n\tstruct pt_regs *regs;\n\tstruct perf_event *event;\n\tu64 period;\n\n\tevent = container_of(hrtimer, struct perf_event, hw.hrtimer);\n\n\tif (event->state != PERF_EVENT_STATE_ACTIVE)\n\t\treturn HRTIMER_NORESTART;\n\n\tevent->pmu->read(event);\n\n\tperf_sample_data_init(&data, 0, event->hw.last_period);\n\tregs = get_irq_regs();\n\n\tif (regs && !perf_exclude_event(event, regs)) {\n\t\tif (!(event->attr.exclude_idle && is_idle_task(current)))\n\t\t\tif (__perf_event_overflow(event, 1, &data, regs))\n\t\t\t\tret = HRTIMER_NORESTART;\n\t}\n\n\tperiod = max_t(u64, 10000, event->hw.sample_period);\n\thrtimer_forward_now(hrtimer, ns_to_ktime(period));\n\n\treturn ret;\n}\n\nstatic void perf_swevent_start_hrtimer(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\ts64 period;\n\n\tif (!is_sampling_event(event))\n\t\treturn;\n\n\tperiod = local64_read(&hwc->period_left);\n\tif (period) {\n\t\tif (period < 0)\n\t\t\tperiod = 10000;\n\n\t\tlocal64_set(&hwc->period_left, 0);\n\t} else {\n\t\tperiod = max_t(u64, 10000, hwc->sample_period);\n\t}\n\thrtimer_start(&hwc->hrtimer, ns_to_ktime(period),\n\t\t      HRTIMER_MODE_REL_PINNED_HARD);\n}\n\nstatic void perf_swevent_cancel_hrtimer(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tif (is_sampling_event(event)) {\n\t\tktime_t remaining = hrtimer_get_remaining(&hwc->hrtimer);\n\t\tlocal64_set(&hwc->period_left, ktime_to_ns(remaining));\n\n\t\thrtimer_cancel(&hwc->hrtimer);\n\t}\n}\n\nstatic void perf_swevent_init_hrtimer(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tif (!is_sampling_event(event))\n\t\treturn;\n\n\thrtimer_init(&hwc->hrtimer, CLOCK_MONOTONIC, HRTIMER_MODE_REL_HARD);\n\thwc->hrtimer.function = perf_swevent_hrtimer;\n\n\t \n\tif (event->attr.freq) {\n\t\tlong freq = event->attr.sample_freq;\n\n\t\tevent->attr.sample_period = NSEC_PER_SEC / freq;\n\t\thwc->sample_period = event->attr.sample_period;\n\t\tlocal64_set(&hwc->period_left, hwc->sample_period);\n\t\thwc->last_period = hwc->sample_period;\n\t\tevent->attr.freq = 0;\n\t}\n}\n\n \n\nstatic void cpu_clock_event_update(struct perf_event *event)\n{\n\ts64 prev;\n\tu64 now;\n\n\tnow = local_clock();\n\tprev = local64_xchg(&event->hw.prev_count, now);\n\tlocal64_add(now - prev, &event->count);\n}\n\nstatic void cpu_clock_event_start(struct perf_event *event, int flags)\n{\n\tlocal64_set(&event->hw.prev_count, local_clock());\n\tperf_swevent_start_hrtimer(event);\n}\n\nstatic void cpu_clock_event_stop(struct perf_event *event, int flags)\n{\n\tperf_swevent_cancel_hrtimer(event);\n\tcpu_clock_event_update(event);\n}\n\nstatic int cpu_clock_event_add(struct perf_event *event, int flags)\n{\n\tif (flags & PERF_EF_START)\n\t\tcpu_clock_event_start(event, flags);\n\tperf_event_update_userpage(event);\n\n\treturn 0;\n}\n\nstatic void cpu_clock_event_del(struct perf_event *event, int flags)\n{\n\tcpu_clock_event_stop(event, flags);\n}\n\nstatic void cpu_clock_event_read(struct perf_event *event)\n{\n\tcpu_clock_event_update(event);\n}\n\nstatic int cpu_clock_event_init(struct perf_event *event)\n{\n\tif (event->attr.type != perf_cpu_clock.type)\n\t\treturn -ENOENT;\n\n\tif (event->attr.config != PERF_COUNT_SW_CPU_CLOCK)\n\t\treturn -ENOENT;\n\n\t \n\tif (has_branch_stack(event))\n\t\treturn -EOPNOTSUPP;\n\n\tperf_swevent_init_hrtimer(event);\n\n\treturn 0;\n}\n\nstatic struct pmu perf_cpu_clock = {\n\t.task_ctx_nr\t= perf_sw_context,\n\n\t.capabilities\t= PERF_PMU_CAP_NO_NMI,\n\t.dev\t\t= PMU_NULL_DEV,\n\n\t.event_init\t= cpu_clock_event_init,\n\t.add\t\t= cpu_clock_event_add,\n\t.del\t\t= cpu_clock_event_del,\n\t.start\t\t= cpu_clock_event_start,\n\t.stop\t\t= cpu_clock_event_stop,\n\t.read\t\t= cpu_clock_event_read,\n};\n\n \n\nstatic void task_clock_event_update(struct perf_event *event, u64 now)\n{\n\tu64 prev;\n\ts64 delta;\n\n\tprev = local64_xchg(&event->hw.prev_count, now);\n\tdelta = now - prev;\n\tlocal64_add(delta, &event->count);\n}\n\nstatic void task_clock_event_start(struct perf_event *event, int flags)\n{\n\tlocal64_set(&event->hw.prev_count, event->ctx->time);\n\tperf_swevent_start_hrtimer(event);\n}\n\nstatic void task_clock_event_stop(struct perf_event *event, int flags)\n{\n\tperf_swevent_cancel_hrtimer(event);\n\ttask_clock_event_update(event, event->ctx->time);\n}\n\nstatic int task_clock_event_add(struct perf_event *event, int flags)\n{\n\tif (flags & PERF_EF_START)\n\t\ttask_clock_event_start(event, flags);\n\tperf_event_update_userpage(event);\n\n\treturn 0;\n}\n\nstatic void task_clock_event_del(struct perf_event *event, int flags)\n{\n\ttask_clock_event_stop(event, PERF_EF_UPDATE);\n}\n\nstatic void task_clock_event_read(struct perf_event *event)\n{\n\tu64 now = perf_clock();\n\tu64 delta = now - event->ctx->timestamp;\n\tu64 time = event->ctx->time + delta;\n\n\ttask_clock_event_update(event, time);\n}\n\nstatic int task_clock_event_init(struct perf_event *event)\n{\n\tif (event->attr.type != perf_task_clock.type)\n\t\treturn -ENOENT;\n\n\tif (event->attr.config != PERF_COUNT_SW_TASK_CLOCK)\n\t\treturn -ENOENT;\n\n\t \n\tif (has_branch_stack(event))\n\t\treturn -EOPNOTSUPP;\n\n\tperf_swevent_init_hrtimer(event);\n\n\treturn 0;\n}\n\nstatic struct pmu perf_task_clock = {\n\t.task_ctx_nr\t= perf_sw_context,\n\n\t.capabilities\t= PERF_PMU_CAP_NO_NMI,\n\t.dev\t\t= PMU_NULL_DEV,\n\n\t.event_init\t= task_clock_event_init,\n\t.add\t\t= task_clock_event_add,\n\t.del\t\t= task_clock_event_del,\n\t.start\t\t= task_clock_event_start,\n\t.stop\t\t= task_clock_event_stop,\n\t.read\t\t= task_clock_event_read,\n};\n\nstatic void perf_pmu_nop_void(struct pmu *pmu)\n{\n}\n\nstatic void perf_pmu_nop_txn(struct pmu *pmu, unsigned int flags)\n{\n}\n\nstatic int perf_pmu_nop_int(struct pmu *pmu)\n{\n\treturn 0;\n}\n\nstatic int perf_event_nop_int(struct perf_event *event, u64 value)\n{\n\treturn 0;\n}\n\nstatic DEFINE_PER_CPU(unsigned int, nop_txn_flags);\n\nstatic void perf_pmu_start_txn(struct pmu *pmu, unsigned int flags)\n{\n\t__this_cpu_write(nop_txn_flags, flags);\n\n\tif (flags & ~PERF_PMU_TXN_ADD)\n\t\treturn;\n\n\tperf_pmu_disable(pmu);\n}\n\nstatic int perf_pmu_commit_txn(struct pmu *pmu)\n{\n\tunsigned int flags = __this_cpu_read(nop_txn_flags);\n\n\t__this_cpu_write(nop_txn_flags, 0);\n\n\tif (flags & ~PERF_PMU_TXN_ADD)\n\t\treturn 0;\n\n\tperf_pmu_enable(pmu);\n\treturn 0;\n}\n\nstatic void perf_pmu_cancel_txn(struct pmu *pmu)\n{\n\tunsigned int flags =  __this_cpu_read(nop_txn_flags);\n\n\t__this_cpu_write(nop_txn_flags, 0);\n\n\tif (flags & ~PERF_PMU_TXN_ADD)\n\t\treturn;\n\n\tperf_pmu_enable(pmu);\n}\n\nstatic int perf_event_idx_default(struct perf_event *event)\n{\n\treturn 0;\n}\n\nstatic void free_pmu_context(struct pmu *pmu)\n{\n\tfree_percpu(pmu->cpu_pmu_context);\n}\n\n \nstatic ssize_t nr_addr_filters_show(struct device *dev,\n\t\t\t\t    struct device_attribute *attr,\n\t\t\t\t    char *page)\n{\n\tstruct pmu *pmu = dev_get_drvdata(dev);\n\n\treturn scnprintf(page, PAGE_SIZE - 1, \"%d\\n\", pmu->nr_addr_filters);\n}\nDEVICE_ATTR_RO(nr_addr_filters);\n\nstatic struct idr pmu_idr;\n\nstatic ssize_t\ntype_show(struct device *dev, struct device_attribute *attr, char *page)\n{\n\tstruct pmu *pmu = dev_get_drvdata(dev);\n\n\treturn scnprintf(page, PAGE_SIZE - 1, \"%d\\n\", pmu->type);\n}\nstatic DEVICE_ATTR_RO(type);\n\nstatic ssize_t\nperf_event_mux_interval_ms_show(struct device *dev,\n\t\t\t\tstruct device_attribute *attr,\n\t\t\t\tchar *page)\n{\n\tstruct pmu *pmu = dev_get_drvdata(dev);\n\n\treturn scnprintf(page, PAGE_SIZE - 1, \"%d\\n\", pmu->hrtimer_interval_ms);\n}\n\nstatic DEFINE_MUTEX(mux_interval_mutex);\n\nstatic ssize_t\nperf_event_mux_interval_ms_store(struct device *dev,\n\t\t\t\t struct device_attribute *attr,\n\t\t\t\t const char *buf, size_t count)\n{\n\tstruct pmu *pmu = dev_get_drvdata(dev);\n\tint timer, cpu, ret;\n\n\tret = kstrtoint(buf, 0, &timer);\n\tif (ret)\n\t\treturn ret;\n\n\tif (timer < 1)\n\t\treturn -EINVAL;\n\n\t \n\tif (timer == pmu->hrtimer_interval_ms)\n\t\treturn count;\n\n\tmutex_lock(&mux_interval_mutex);\n\tpmu->hrtimer_interval_ms = timer;\n\n\t \n\tcpus_read_lock();\n\tfor_each_online_cpu(cpu) {\n\t\tstruct perf_cpu_pmu_context *cpc;\n\t\tcpc = per_cpu_ptr(pmu->cpu_pmu_context, cpu);\n\t\tcpc->hrtimer_interval = ns_to_ktime(NSEC_PER_MSEC * timer);\n\n\t\tcpu_function_call(cpu, perf_mux_hrtimer_restart_ipi, cpc);\n\t}\n\tcpus_read_unlock();\n\tmutex_unlock(&mux_interval_mutex);\n\n\treturn count;\n}\nstatic DEVICE_ATTR_RW(perf_event_mux_interval_ms);\n\nstatic struct attribute *pmu_dev_attrs[] = {\n\t&dev_attr_type.attr,\n\t&dev_attr_perf_event_mux_interval_ms.attr,\n\tNULL,\n};\nATTRIBUTE_GROUPS(pmu_dev);\n\nstatic int pmu_bus_running;\nstatic struct bus_type pmu_bus = {\n\t.name\t\t= \"event_source\",\n\t.dev_groups\t= pmu_dev_groups,\n};\n\nstatic void pmu_dev_release(struct device *dev)\n{\n\tkfree(dev);\n}\n\nstatic int pmu_dev_alloc(struct pmu *pmu)\n{\n\tint ret = -ENOMEM;\n\n\tpmu->dev = kzalloc(sizeof(struct device), GFP_KERNEL);\n\tif (!pmu->dev)\n\t\tgoto out;\n\n\tpmu->dev->groups = pmu->attr_groups;\n\tdevice_initialize(pmu->dev);\n\n\tdev_set_drvdata(pmu->dev, pmu);\n\tpmu->dev->bus = &pmu_bus;\n\tpmu->dev->parent = pmu->parent;\n\tpmu->dev->release = pmu_dev_release;\n\n\tret = dev_set_name(pmu->dev, \"%s\", pmu->name);\n\tif (ret)\n\t\tgoto free_dev;\n\n\tret = device_add(pmu->dev);\n\tif (ret)\n\t\tgoto free_dev;\n\n\t \n\tif (pmu->nr_addr_filters)\n\t\tret = device_create_file(pmu->dev, &dev_attr_nr_addr_filters);\n\n\tif (ret)\n\t\tgoto del_dev;\n\n\tif (pmu->attr_update)\n\t\tret = sysfs_update_groups(&pmu->dev->kobj, pmu->attr_update);\n\n\tif (ret)\n\t\tgoto del_dev;\n\nout:\n\treturn ret;\n\ndel_dev:\n\tdevice_del(pmu->dev);\n\nfree_dev:\n\tput_device(pmu->dev);\n\tgoto out;\n}\n\nstatic struct lock_class_key cpuctx_mutex;\nstatic struct lock_class_key cpuctx_lock;\n\nint perf_pmu_register(struct pmu *pmu, const char *name, int type)\n{\n\tint cpu, ret, max = PERF_TYPE_MAX;\n\n\tmutex_lock(&pmus_lock);\n\tret = -ENOMEM;\n\tpmu->pmu_disable_count = alloc_percpu(int);\n\tif (!pmu->pmu_disable_count)\n\t\tgoto unlock;\n\n\tpmu->type = -1;\n\tif (WARN_ONCE(!name, \"Can not register anonymous pmu.\\n\")) {\n\t\tret = -EINVAL;\n\t\tgoto free_pdc;\n\t}\n\n\tpmu->name = name;\n\n\tif (type >= 0)\n\t\tmax = type;\n\n\tret = idr_alloc(&pmu_idr, pmu, max, 0, GFP_KERNEL);\n\tif (ret < 0)\n\t\tgoto free_pdc;\n\n\tWARN_ON(type >= 0 && ret != type);\n\n\ttype = ret;\n\tpmu->type = type;\n\n\tif (pmu_bus_running && !pmu->dev) {\n\t\tret = pmu_dev_alloc(pmu);\n\t\tif (ret)\n\t\t\tgoto free_idr;\n\t}\n\n\tret = -ENOMEM;\n\tpmu->cpu_pmu_context = alloc_percpu(struct perf_cpu_pmu_context);\n\tif (!pmu->cpu_pmu_context)\n\t\tgoto free_dev;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct perf_cpu_pmu_context *cpc;\n\n\t\tcpc = per_cpu_ptr(pmu->cpu_pmu_context, cpu);\n\t\t__perf_init_event_pmu_context(&cpc->epc, pmu);\n\t\t__perf_mux_hrtimer_init(cpc, cpu);\n\t}\n\n\tif (!pmu->start_txn) {\n\t\tif (pmu->pmu_enable) {\n\t\t\t \n\t\t\tpmu->start_txn  = perf_pmu_start_txn;\n\t\t\tpmu->commit_txn = perf_pmu_commit_txn;\n\t\t\tpmu->cancel_txn = perf_pmu_cancel_txn;\n\t\t} else {\n\t\t\tpmu->start_txn  = perf_pmu_nop_txn;\n\t\t\tpmu->commit_txn = perf_pmu_nop_int;\n\t\t\tpmu->cancel_txn = perf_pmu_nop_void;\n\t\t}\n\t}\n\n\tif (!pmu->pmu_enable) {\n\t\tpmu->pmu_enable  = perf_pmu_nop_void;\n\t\tpmu->pmu_disable = perf_pmu_nop_void;\n\t}\n\n\tif (!pmu->check_period)\n\t\tpmu->check_period = perf_event_nop_int;\n\n\tif (!pmu->event_idx)\n\t\tpmu->event_idx = perf_event_idx_default;\n\n\tlist_add_rcu(&pmu->entry, &pmus);\n\tatomic_set(&pmu->exclusive_cnt, 0);\n\tret = 0;\nunlock:\n\tmutex_unlock(&pmus_lock);\n\n\treturn ret;\n\nfree_dev:\n\tif (pmu->dev && pmu->dev != PMU_NULL_DEV) {\n\t\tdevice_del(pmu->dev);\n\t\tput_device(pmu->dev);\n\t}\n\nfree_idr:\n\tidr_remove(&pmu_idr, pmu->type);\n\nfree_pdc:\n\tfree_percpu(pmu->pmu_disable_count);\n\tgoto unlock;\n}\nEXPORT_SYMBOL_GPL(perf_pmu_register);\n\nvoid perf_pmu_unregister(struct pmu *pmu)\n{\n\tmutex_lock(&pmus_lock);\n\tlist_del_rcu(&pmu->entry);\n\n\t \n\tsynchronize_srcu(&pmus_srcu);\n\tsynchronize_rcu();\n\n\tfree_percpu(pmu->pmu_disable_count);\n\tidr_remove(&pmu_idr, pmu->type);\n\tif (pmu_bus_running && pmu->dev && pmu->dev != PMU_NULL_DEV) {\n\t\tif (pmu->nr_addr_filters)\n\t\t\tdevice_remove_file(pmu->dev, &dev_attr_nr_addr_filters);\n\t\tdevice_del(pmu->dev);\n\t\tput_device(pmu->dev);\n\t}\n\tfree_pmu_context(pmu);\n\tmutex_unlock(&pmus_lock);\n}\nEXPORT_SYMBOL_GPL(perf_pmu_unregister);\n\nstatic inline bool has_extended_regs(struct perf_event *event)\n{\n\treturn (event->attr.sample_regs_user & PERF_REG_EXTENDED_MASK) ||\n\t       (event->attr.sample_regs_intr & PERF_REG_EXTENDED_MASK);\n}\n\nstatic int perf_try_init_event(struct pmu *pmu, struct perf_event *event)\n{\n\tstruct perf_event_context *ctx = NULL;\n\tint ret;\n\n\tif (!try_module_get(pmu->module))\n\t\treturn -ENODEV;\n\n\t \n\tif (event->group_leader != event && pmu->task_ctx_nr != perf_sw_context) {\n\t\t \n\t\tctx = perf_event_ctx_lock_nested(event->group_leader,\n\t\t\t\t\t\t SINGLE_DEPTH_NESTING);\n\t\tBUG_ON(!ctx);\n\t}\n\n\tevent->pmu = pmu;\n\tret = pmu->event_init(event);\n\n\tif (ctx)\n\t\tperf_event_ctx_unlock(event->group_leader, ctx);\n\n\tif (!ret) {\n\t\tif (!(pmu->capabilities & PERF_PMU_CAP_EXTENDED_REGS) &&\n\t\t    has_extended_regs(event))\n\t\t\tret = -EOPNOTSUPP;\n\n\t\tif (pmu->capabilities & PERF_PMU_CAP_NO_EXCLUDE &&\n\t\t    event_has_any_exclude_flag(event))\n\t\t\tret = -EINVAL;\n\n\t\tif (ret && event->destroy)\n\t\t\tevent->destroy(event);\n\t}\n\n\tif (ret)\n\t\tmodule_put(pmu->module);\n\n\treturn ret;\n}\n\nstatic struct pmu *perf_init_event(struct perf_event *event)\n{\n\tbool extended_type = false;\n\tint idx, type, ret;\n\tstruct pmu *pmu;\n\n\tidx = srcu_read_lock(&pmus_srcu);\n\n\t \n\tevent->orig_type = event->attr.type;\n\n\t \n\tif (event->parent && event->parent->pmu) {\n\t\tpmu = event->parent->pmu;\n\t\tret = perf_try_init_event(pmu, event);\n\t\tif (!ret)\n\t\t\tgoto unlock;\n\t}\n\n\t \n\ttype = event->attr.type;\n\tif (type == PERF_TYPE_HARDWARE || type == PERF_TYPE_HW_CACHE) {\n\t\ttype = event->attr.config >> PERF_PMU_TYPE_SHIFT;\n\t\tif (!type) {\n\t\t\ttype = PERF_TYPE_RAW;\n\t\t} else {\n\t\t\textended_type = true;\n\t\t\tevent->attr.config &= PERF_HW_EVENT_MASK;\n\t\t}\n\t}\n\nagain:\n\trcu_read_lock();\n\tpmu = idr_find(&pmu_idr, type);\n\trcu_read_unlock();\n\tif (pmu) {\n\t\tif (event->attr.type != type && type != PERF_TYPE_RAW &&\n\t\t    !(pmu->capabilities & PERF_PMU_CAP_EXTENDED_HW_TYPE))\n\t\t\tgoto fail;\n\n\t\tret = perf_try_init_event(pmu, event);\n\t\tif (ret == -ENOENT && event->attr.type != type && !extended_type) {\n\t\t\ttype = event->attr.type;\n\t\t\tgoto again;\n\t\t}\n\n\t\tif (ret)\n\t\t\tpmu = ERR_PTR(ret);\n\n\t\tgoto unlock;\n\t}\n\n\tlist_for_each_entry_rcu(pmu, &pmus, entry, lockdep_is_held(&pmus_srcu)) {\n\t\tret = perf_try_init_event(pmu, event);\n\t\tif (!ret)\n\t\t\tgoto unlock;\n\n\t\tif (ret != -ENOENT) {\n\t\t\tpmu = ERR_PTR(ret);\n\t\t\tgoto unlock;\n\t\t}\n\t}\nfail:\n\tpmu = ERR_PTR(-ENOENT);\nunlock:\n\tsrcu_read_unlock(&pmus_srcu, idx);\n\n\treturn pmu;\n}\n\nstatic void attach_sb_event(struct perf_event *event)\n{\n\tstruct pmu_event_list *pel = per_cpu_ptr(&pmu_sb_events, event->cpu);\n\n\traw_spin_lock(&pel->lock);\n\tlist_add_rcu(&event->sb_list, &pel->list);\n\traw_spin_unlock(&pel->lock);\n}\n\n \nstatic void account_pmu_sb_event(struct perf_event *event)\n{\n\tif (is_sb_event(event))\n\t\tattach_sb_event(event);\n}\n\n \nstatic void account_freq_event_nohz(void)\n{\n#ifdef CONFIG_NO_HZ_FULL\n\t \n\tspin_lock(&nr_freq_lock);\n\tif (atomic_inc_return(&nr_freq_events) == 1)\n\t\ttick_nohz_dep_set(TICK_DEP_BIT_PERF_EVENTS);\n\tspin_unlock(&nr_freq_lock);\n#endif\n}\n\nstatic void account_freq_event(void)\n{\n\tif (tick_nohz_full_enabled())\n\t\taccount_freq_event_nohz();\n\telse\n\t\tatomic_inc(&nr_freq_events);\n}\n\n\nstatic void account_event(struct perf_event *event)\n{\n\tbool inc = false;\n\n\tif (event->parent)\n\t\treturn;\n\n\tif (event->attach_state & (PERF_ATTACH_TASK | PERF_ATTACH_SCHED_CB))\n\t\tinc = true;\n\tif (event->attr.mmap || event->attr.mmap_data)\n\t\tatomic_inc(&nr_mmap_events);\n\tif (event->attr.build_id)\n\t\tatomic_inc(&nr_build_id_events);\n\tif (event->attr.comm)\n\t\tatomic_inc(&nr_comm_events);\n\tif (event->attr.namespaces)\n\t\tatomic_inc(&nr_namespaces_events);\n\tif (event->attr.cgroup)\n\t\tatomic_inc(&nr_cgroup_events);\n\tif (event->attr.task)\n\t\tatomic_inc(&nr_task_events);\n\tif (event->attr.freq)\n\t\taccount_freq_event();\n\tif (event->attr.context_switch) {\n\t\tatomic_inc(&nr_switch_events);\n\t\tinc = true;\n\t}\n\tif (has_branch_stack(event))\n\t\tinc = true;\n\tif (is_cgroup_event(event))\n\t\tinc = true;\n\tif (event->attr.ksymbol)\n\t\tatomic_inc(&nr_ksymbol_events);\n\tif (event->attr.bpf_event)\n\t\tatomic_inc(&nr_bpf_events);\n\tif (event->attr.text_poke)\n\t\tatomic_inc(&nr_text_poke_events);\n\n\tif (inc) {\n\t\t \n\t\tif (atomic_inc_not_zero(&perf_sched_count))\n\t\t\tgoto enabled;\n\n\t\tmutex_lock(&perf_sched_mutex);\n\t\tif (!atomic_read(&perf_sched_count)) {\n\t\t\tstatic_branch_enable(&perf_sched_events);\n\t\t\t \n\t\t\tsynchronize_rcu();\n\t\t}\n\t\t \n\t\tatomic_inc(&perf_sched_count);\n\t\tmutex_unlock(&perf_sched_mutex);\n\t}\nenabled:\n\n\taccount_pmu_sb_event(event);\n}\n\n \nstatic struct perf_event *\nperf_event_alloc(struct perf_event_attr *attr, int cpu,\n\t\t struct task_struct *task,\n\t\t struct perf_event *group_leader,\n\t\t struct perf_event *parent_event,\n\t\t perf_overflow_handler_t overflow_handler,\n\t\t void *context, int cgroup_fd)\n{\n\tstruct pmu *pmu;\n\tstruct perf_event *event;\n\tstruct hw_perf_event *hwc;\n\tlong err = -EINVAL;\n\tint node;\n\n\tif ((unsigned)cpu >= nr_cpu_ids) {\n\t\tif (!task || cpu != -1)\n\t\t\treturn ERR_PTR(-EINVAL);\n\t}\n\tif (attr->sigtrap && !task) {\n\t\t \n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tnode = (cpu >= 0) ? cpu_to_node(cpu) : -1;\n\tevent = kmem_cache_alloc_node(perf_event_cache, GFP_KERNEL | __GFP_ZERO,\n\t\t\t\t      node);\n\tif (!event)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\t \n\tif (!group_leader)\n\t\tgroup_leader = event;\n\n\tmutex_init(&event->child_mutex);\n\tINIT_LIST_HEAD(&event->child_list);\n\n\tINIT_LIST_HEAD(&event->event_entry);\n\tINIT_LIST_HEAD(&event->sibling_list);\n\tINIT_LIST_HEAD(&event->active_list);\n\tinit_event_group(event);\n\tINIT_LIST_HEAD(&event->rb_entry);\n\tINIT_LIST_HEAD(&event->active_entry);\n\tINIT_LIST_HEAD(&event->addr_filters.list);\n\tINIT_HLIST_NODE(&event->hlist_entry);\n\n\n\tinit_waitqueue_head(&event->waitq);\n\tinit_irq_work(&event->pending_irq, perf_pending_irq);\n\tinit_task_work(&event->pending_task, perf_pending_task);\n\n\tmutex_init(&event->mmap_mutex);\n\traw_spin_lock_init(&event->addr_filters.lock);\n\n\tatomic_long_set(&event->refcount, 1);\n\tevent->cpu\t\t= cpu;\n\tevent->attr\t\t= *attr;\n\tevent->group_leader\t= group_leader;\n\tevent->pmu\t\t= NULL;\n\tevent->oncpu\t\t= -1;\n\n\tevent->parent\t\t= parent_event;\n\n\tevent->ns\t\t= get_pid_ns(task_active_pid_ns(current));\n\tevent->id\t\t= atomic64_inc_return(&perf_event_id);\n\n\tevent->state\t\t= PERF_EVENT_STATE_INACTIVE;\n\n\tif (parent_event)\n\t\tevent->event_caps = parent_event->event_caps;\n\n\tif (task) {\n\t\tevent->attach_state = PERF_ATTACH_TASK;\n\t\t \n\t\tevent->hw.target = get_task_struct(task);\n\t}\n\n\tevent->clock = &local_clock;\n\tif (parent_event)\n\t\tevent->clock = parent_event->clock;\n\n\tif (!overflow_handler && parent_event) {\n\t\toverflow_handler = parent_event->overflow_handler;\n\t\tcontext = parent_event->overflow_handler_context;\n#if defined(CONFIG_BPF_SYSCALL) && defined(CONFIG_EVENT_TRACING)\n\t\tif (overflow_handler == bpf_overflow_handler) {\n\t\t\tstruct bpf_prog *prog = parent_event->prog;\n\n\t\t\tbpf_prog_inc(prog);\n\t\t\tevent->prog = prog;\n\t\t\tevent->orig_overflow_handler =\n\t\t\t\tparent_event->orig_overflow_handler;\n\t\t}\n#endif\n\t}\n\n\tif (overflow_handler) {\n\t\tevent->overflow_handler\t= overflow_handler;\n\t\tevent->overflow_handler_context = context;\n\t} else if (is_write_backward(event)){\n\t\tevent->overflow_handler = perf_event_output_backward;\n\t\tevent->overflow_handler_context = NULL;\n\t} else {\n\t\tevent->overflow_handler = perf_event_output_forward;\n\t\tevent->overflow_handler_context = NULL;\n\t}\n\n\tperf_event__state_init(event);\n\n\tpmu = NULL;\n\n\thwc = &event->hw;\n\thwc->sample_period = attr->sample_period;\n\tif (attr->freq && attr->sample_freq)\n\t\thwc->sample_period = 1;\n\thwc->last_period = hwc->sample_period;\n\n\tlocal64_set(&hwc->period_left, hwc->sample_period);\n\n\t \n\tif (attr->inherit && (attr->sample_type & PERF_SAMPLE_READ))\n\t\tgoto err_ns;\n\n\tif (!has_branch_stack(event))\n\t\tevent->attr.branch_sample_type = 0;\n\n\tpmu = perf_init_event(event);\n\tif (IS_ERR(pmu)) {\n\t\terr = PTR_ERR(pmu);\n\t\tgoto err_ns;\n\t}\n\n\t \n\tif (pmu->task_ctx_nr == perf_invalid_context && (task || cgroup_fd != -1)) {\n\t\terr = -EINVAL;\n\t\tgoto err_pmu;\n\t}\n\n\tif (event->attr.aux_output &&\n\t    !(pmu->capabilities & PERF_PMU_CAP_AUX_OUTPUT)) {\n\t\terr = -EOPNOTSUPP;\n\t\tgoto err_pmu;\n\t}\n\n\tif (cgroup_fd != -1) {\n\t\terr = perf_cgroup_connect(cgroup_fd, event, attr, group_leader);\n\t\tif (err)\n\t\t\tgoto err_pmu;\n\t}\n\n\terr = exclusive_event_init(event);\n\tif (err)\n\t\tgoto err_pmu;\n\n\tif (has_addr_filter(event)) {\n\t\tevent->addr_filter_ranges = kcalloc(pmu->nr_addr_filters,\n\t\t\t\t\t\t    sizeof(struct perf_addr_filter_range),\n\t\t\t\t\t\t    GFP_KERNEL);\n\t\tif (!event->addr_filter_ranges) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_per_task;\n\t\t}\n\n\t\t \n\t\tif (event->parent) {\n\t\t\tstruct perf_addr_filters_head *ifh = perf_event_addr_filters(event);\n\n\t\t\traw_spin_lock_irq(&ifh->lock);\n\t\t\tmemcpy(event->addr_filter_ranges,\n\t\t\t       event->parent->addr_filter_ranges,\n\t\t\t       pmu->nr_addr_filters * sizeof(struct perf_addr_filter_range));\n\t\t\traw_spin_unlock_irq(&ifh->lock);\n\t\t}\n\n\t\t \n\t\tevent->addr_filters_gen = 1;\n\t}\n\n\tif (!event->parent) {\n\t\tif (event->attr.sample_type & PERF_SAMPLE_CALLCHAIN) {\n\t\t\terr = get_callchain_buffers(attr->sample_max_stack);\n\t\t\tif (err)\n\t\t\t\tgoto err_addr_filters;\n\t\t}\n\t}\n\n\terr = security_perf_event_alloc(event);\n\tif (err)\n\t\tgoto err_callchain_buffer;\n\n\t \n\taccount_event(event);\n\n\treturn event;\n\nerr_callchain_buffer:\n\tif (!event->parent) {\n\t\tif (event->attr.sample_type & PERF_SAMPLE_CALLCHAIN)\n\t\t\tput_callchain_buffers();\n\t}\nerr_addr_filters:\n\tkfree(event->addr_filter_ranges);\n\nerr_per_task:\n\texclusive_event_destroy(event);\n\nerr_pmu:\n\tif (is_cgroup_event(event))\n\t\tperf_detach_cgroup(event);\n\tif (event->destroy)\n\t\tevent->destroy(event);\n\tmodule_put(pmu->module);\nerr_ns:\n\tif (event->hw.target)\n\t\tput_task_struct(event->hw.target);\n\tcall_rcu(&event->rcu_head, free_event_rcu);\n\n\treturn ERR_PTR(err);\n}\n\nstatic int perf_copy_attr(struct perf_event_attr __user *uattr,\n\t\t\t  struct perf_event_attr *attr)\n{\n\tu32 size;\n\tint ret;\n\n\t \n\tmemset(attr, 0, sizeof(*attr));\n\n\tret = get_user(size, &uattr->size);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tif (!size)\n\t\tsize = PERF_ATTR_SIZE_VER0;\n\tif (size < PERF_ATTR_SIZE_VER0 || size > PAGE_SIZE)\n\t\tgoto err_size;\n\n\tret = copy_struct_from_user(attr, sizeof(*attr), uattr, size);\n\tif (ret) {\n\t\tif (ret == -E2BIG)\n\t\t\tgoto err_size;\n\t\treturn ret;\n\t}\n\n\tattr->size = size;\n\n\tif (attr->__reserved_1 || attr->__reserved_2 || attr->__reserved_3)\n\t\treturn -EINVAL;\n\n\tif (attr->sample_type & ~(PERF_SAMPLE_MAX-1))\n\t\treturn -EINVAL;\n\n\tif (attr->read_format & ~(PERF_FORMAT_MAX-1))\n\t\treturn -EINVAL;\n\n\tif (attr->sample_type & PERF_SAMPLE_BRANCH_STACK) {\n\t\tu64 mask = attr->branch_sample_type;\n\n\t\t \n\t\tif (mask & ~(PERF_SAMPLE_BRANCH_MAX-1))\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tif (!(mask & ~PERF_SAMPLE_BRANCH_PLM_ALL))\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tif (!(mask & PERF_SAMPLE_BRANCH_PLM_ALL)) {\n\n\t\t\t \n\t\t\tif (!attr->exclude_kernel)\n\t\t\t\tmask |= PERF_SAMPLE_BRANCH_KERNEL;\n\n\t\t\tif (!attr->exclude_user)\n\t\t\t\tmask |= PERF_SAMPLE_BRANCH_USER;\n\n\t\t\tif (!attr->exclude_hv)\n\t\t\t\tmask |= PERF_SAMPLE_BRANCH_HV;\n\t\t\t \n\t\t\tattr->branch_sample_type = mask;\n\t\t}\n\t\t \n\t\tif (mask & PERF_SAMPLE_BRANCH_PERM_PLM) {\n\t\t\tret = perf_allow_kernel(attr);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t}\n\n\tif (attr->sample_type & PERF_SAMPLE_REGS_USER) {\n\t\tret = perf_reg_validate(attr->sample_regs_user);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (attr->sample_type & PERF_SAMPLE_STACK_USER) {\n\t\tif (!arch_perf_have_user_stack_dump())\n\t\t\treturn -ENOSYS;\n\n\t\t \n\t\tif (attr->sample_stack_user >= USHRT_MAX)\n\t\t\treturn -EINVAL;\n\t\telse if (!IS_ALIGNED(attr->sample_stack_user, sizeof(u64)))\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (!attr->sample_max_stack)\n\t\tattr->sample_max_stack = sysctl_perf_event_max_stack;\n\n\tif (attr->sample_type & PERF_SAMPLE_REGS_INTR)\n\t\tret = perf_reg_validate(attr->sample_regs_intr);\n\n#ifndef CONFIG_CGROUP_PERF\n\tif (attr->sample_type & PERF_SAMPLE_CGROUP)\n\t\treturn -EINVAL;\n#endif\n\tif ((attr->sample_type & PERF_SAMPLE_WEIGHT) &&\n\t    (attr->sample_type & PERF_SAMPLE_WEIGHT_STRUCT))\n\t\treturn -EINVAL;\n\n\tif (!attr->inherit && attr->inherit_thread)\n\t\treturn -EINVAL;\n\n\tif (attr->remove_on_exec && attr->enable_on_exec)\n\t\treturn -EINVAL;\n\n\tif (attr->sigtrap && !attr->remove_on_exec)\n\t\treturn -EINVAL;\n\nout:\n\treturn ret;\n\nerr_size:\n\tput_user(sizeof(*attr), &uattr->size);\n\tret = -E2BIG;\n\tgoto out;\n}\n\nstatic void mutex_lock_double(struct mutex *a, struct mutex *b)\n{\n\tif (b < a)\n\t\tswap(a, b);\n\n\tmutex_lock(a);\n\tmutex_lock_nested(b, SINGLE_DEPTH_NESTING);\n}\n\nstatic int\nperf_event_set_output(struct perf_event *event, struct perf_event *output_event)\n{\n\tstruct perf_buffer *rb = NULL;\n\tint ret = -EINVAL;\n\n\tif (!output_event) {\n\t\tmutex_lock(&event->mmap_mutex);\n\t\tgoto set;\n\t}\n\n\t \n\tif (event == output_event)\n\t\tgoto out;\n\n\t \n\tif (output_event->cpu != event->cpu)\n\t\tgoto out;\n\n\t \n\tif (output_event->cpu == -1 && output_event->hw.target != event->hw.target)\n\t\tgoto out;\n\n\t \n\tif (output_event->clock != event->clock)\n\t\tgoto out;\n\n\t \n\tif (is_write_backward(output_event) != is_write_backward(event))\n\t\tgoto out;\n\n\t \n\tif (has_aux(event) && has_aux(output_event) &&\n\t    event->pmu != output_event->pmu)\n\t\tgoto out;\n\n\t \n\tmutex_lock_double(&event->mmap_mutex, &output_event->mmap_mutex);\nset:\n\t \n\tif (atomic_read(&event->mmap_count))\n\t\tgoto unlock;\n\n\tif (output_event) {\n\t\t \n\t\trb = ring_buffer_get(output_event);\n\t\tif (!rb)\n\t\t\tgoto unlock;\n\n\t\t \n\t\tif (!atomic_read(&rb->mmap_count)) {\n\t\t\tring_buffer_put(rb);\n\t\t\tgoto unlock;\n\t\t}\n\t}\n\n\tring_buffer_attach(event, rb);\n\n\tret = 0;\nunlock:\n\tmutex_unlock(&event->mmap_mutex);\n\tif (output_event)\n\t\tmutex_unlock(&output_event->mmap_mutex);\n\nout:\n\treturn ret;\n}\n\nstatic int perf_event_set_clock(struct perf_event *event, clockid_t clk_id)\n{\n\tbool nmi_safe = false;\n\n\tswitch (clk_id) {\n\tcase CLOCK_MONOTONIC:\n\t\tevent->clock = &ktime_get_mono_fast_ns;\n\t\tnmi_safe = true;\n\t\tbreak;\n\n\tcase CLOCK_MONOTONIC_RAW:\n\t\tevent->clock = &ktime_get_raw_fast_ns;\n\t\tnmi_safe = true;\n\t\tbreak;\n\n\tcase CLOCK_REALTIME:\n\t\tevent->clock = &ktime_get_real_ns;\n\t\tbreak;\n\n\tcase CLOCK_BOOTTIME:\n\t\tevent->clock = &ktime_get_boottime_ns;\n\t\tbreak;\n\n\tcase CLOCK_TAI:\n\t\tevent->clock = &ktime_get_clocktai_ns;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif (!nmi_safe && !(event->pmu->capabilities & PERF_PMU_CAP_NO_NMI))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic bool\nperf_check_permission(struct perf_event_attr *attr, struct task_struct *task)\n{\n\tunsigned int ptrace_mode = PTRACE_MODE_READ_REALCREDS;\n\tbool is_capable = perfmon_capable();\n\n\tif (attr->sigtrap) {\n\t\t \n\t\trcu_read_lock();\n\t\tis_capable &= ns_capable(__task_cred(task)->user_ns, CAP_KILL);\n\t\trcu_read_unlock();\n\n\t\t \n\t\tptrace_mode = PTRACE_MODE_ATTACH_REALCREDS;\n\t}\n\n\t \n\treturn is_capable || ptrace_may_access(task, ptrace_mode);\n}\n\n \nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event_pmu_context *pmu_ctx;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx;\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\tint cgroup_fd = -1;\n\n\t \n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\t \n\terr = security_perf_event_open(&attr, PERF_SECURITY_OPEN);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\terr = perf_allow_kernel(&attr);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (attr.namespaces) {\n\t\tif (!perfmon_capable())\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t \n\tif ((attr.sample_type & PERF_SAMPLE_PHYS_ADDR)) {\n\t\terr = perf_allow_kernel(&attr);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\t \n\tif (attr.sample_type & PERF_SAMPLE_REGS_INTR) {\n\t\terr = security_locked_down(LOCKDOWN_PERF);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\t \n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP)\n\t\tcgroup_fd = pid;\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL, cgroup_fd);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_task;\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\t \n\tpmu = event->pmu;\n\n\tif (attr.use_clockid) {\n\t\terr = perf_event_set_clock(event, attr.clockid);\n\t\tif (err)\n\t\t\tgoto err_alloc;\n\t}\n\n\tif (pmu->task_ctx_nr == perf_sw_context)\n\t\tevent->event_caps |= PERF_EV_CAP_SOFTWARE;\n\n\tif (task) {\n\t\terr = down_read_interruptible(&task->signal->exec_update_lock);\n\t\tif (err)\n\t\t\tgoto err_alloc;\n\n\t\t \n\t\terr = -EACCES;\n\t\tif (!perf_check_permission(&attr, task))\n\t\t\tgoto err_cred;\n\t}\n\n\t \n\tctx = find_get_context(task, event);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_cred;\n\t}\n\n\tmutex_lock(&ctx->mutex);\n\n\tif (ctx->task == TASK_TOMBSTONE) {\n\t\terr = -ESRCH;\n\t\tgoto err_locked;\n\t}\n\n\tif (!task) {\n\t\t \n\t\tstruct perf_cpu_context *cpuctx = per_cpu_ptr(&perf_cpu_context, event->cpu);\n\n\t\tif (!cpuctx->online) {\n\t\t\terr = -ENODEV;\n\t\t\tgoto err_locked;\n\t\t}\n\t}\n\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t \n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_locked;\n\n\t\t \n\t\tif (group_leader->clock != event->clock)\n\t\t\tgoto err_locked;\n\n\t\t \n\t\tif (group_leader->cpu != event->cpu)\n\t\t\tgoto err_locked;\n\n\t\t \n\t\tif (group_leader->ctx != ctx)\n\t\t\tgoto err_locked;\n\n\t\t \n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_locked;\n\n\t\tif (is_software_event(event) &&\n\t\t    !in_software_context(group_leader)) {\n\t\t\t \n\t\t\tpmu = group_leader->pmu_ctx->pmu;\n\t\t} else if (!is_software_event(event)) {\n\t\t\tif (is_software_event(group_leader) &&\n\t\t\t    (group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {\n\t\t\t\t \n\t\t\t\tmove_group = 1;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (!in_software_context(group_leader) &&\n\t\t\t    group_leader->pmu_ctx->pmu != pmu)\n\t\t\t\tgoto err_locked;\n\t\t}\n\t}\n\n\t \n\tpmu_ctx = find_get_pmu_context(pmu, ctx, event);\n\tif (IS_ERR(pmu_ctx)) {\n\t\terr = PTR_ERR(pmu_ctx);\n\t\tgoto err_locked;\n\t}\n\tevent->pmu_ctx = pmu_ctx;\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tif (!perf_event_validate_size(event)) {\n\t\terr = -E2BIG;\n\t\tgoto err_context;\n\t}\n\n\tif (perf_need_aux_event(event) && !perf_get_aux_event(event, group_leader)) {\n\t\terr = -EINVAL;\n\t\tgoto err_context;\n\t}\n\n\t \n\tif (!exclusive_event_installable(event, ctx)) {\n\t\terr = -EBUSY;\n\t\tgoto err_context;\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event, f_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tevent_file = NULL;\n\t\tgoto err_context;\n\t}\n\n\t \n\n\tif (move_group) {\n\t\tperf_remove_from_context(group_leader, 0);\n\t\tput_pmu_ctx(group_leader->pmu_ctx);\n\n\t\tfor_each_sibling_event(sibling, group_leader) {\n\t\t\tperf_remove_from_context(sibling, 0);\n\t\t\tput_pmu_ctx(sibling->pmu_ctx);\n\t\t}\n\n\t\t \n\t\tfor_each_sibling_event(sibling, group_leader) {\n\t\t\tsibling->pmu_ctx = pmu_ctx;\n\t\t\tget_pmu_ctx(pmu_ctx);\n\t\t\tperf_event__state_init(sibling);\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t}\n\n\t\t \n\t\tgroup_leader->pmu_ctx = pmu_ctx;\n\t\tget_pmu_ctx(pmu_ctx);\n\t\tperf_event__state_init(group_leader);\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t}\n\n\t \n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\tevent->owner = current;\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\n\tmutex_unlock(&ctx->mutex);\n\n\tif (task) {\n\t\tup_read(&task->signal->exec_update_lock);\n\t\tput_task_struct(task);\n\t}\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t \n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tput_pmu_ctx(event->pmu_ctx);\n\tevent->pmu_ctx = NULL;  \nerr_locked:\n\tmutex_unlock(&ctx->mutex);\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_cred:\n\tif (task)\n\t\tup_read(&task->signal->exec_update_lock);\nerr_alloc:\n\tfree_event(event);\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}\n\n \nstruct perf_event *\nperf_event_create_kernel_counter(struct perf_event_attr *attr, int cpu,\n\t\t\t\t struct task_struct *task,\n\t\t\t\t perf_overflow_handler_t overflow_handler,\n\t\t\t\t void *context)\n{\n\tstruct perf_event_pmu_context *pmu_ctx;\n\tstruct perf_event_context *ctx;\n\tstruct perf_event *event;\n\tstruct pmu *pmu;\n\tint err;\n\n\t \n\tif (attr->aux_output)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tevent = perf_event_alloc(attr, cpu, task, NULL, NULL,\n\t\t\t\t overflow_handler, context, -1);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err;\n\t}\n\n\t \n\tevent->owner = TASK_TOMBSTONE;\n\tpmu = event->pmu;\n\n\tif (pmu->task_ctx_nr == perf_sw_context)\n\t\tevent->event_caps |= PERF_EV_CAP_SOFTWARE;\n\n\t \n\tctx = find_get_context(task, event);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\tmutex_lock(&ctx->mutex);\n\tif (ctx->task == TASK_TOMBSTONE) {\n\t\terr = -ESRCH;\n\t\tgoto err_unlock;\n\t}\n\n\tpmu_ctx = find_get_pmu_context(pmu, ctx, event);\n\tif (IS_ERR(pmu_ctx)) {\n\t\terr = PTR_ERR(pmu_ctx);\n\t\tgoto err_unlock;\n\t}\n\tevent->pmu_ctx = pmu_ctx;\n\n\tif (!task) {\n\t\t \n\t\tstruct perf_cpu_context *cpuctx =\n\t\t\tcontainer_of(ctx, struct perf_cpu_context, ctx);\n\t\tif (!cpuctx->online) {\n\t\t\terr = -ENODEV;\n\t\t\tgoto err_pmu_ctx;\n\t\t}\n\t}\n\n\tif (!exclusive_event_installable(event, ctx)) {\n\t\terr = -EBUSY;\n\t\tgoto err_pmu_ctx;\n\t}\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\tmutex_unlock(&ctx->mutex);\n\n\treturn event;\n\nerr_pmu_ctx:\n\tput_pmu_ctx(pmu_ctx);\n\tevent->pmu_ctx = NULL;  \nerr_unlock:\n\tmutex_unlock(&ctx->mutex);\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr:\n\treturn ERR_PTR(err);\n}\nEXPORT_SYMBOL_GPL(perf_event_create_kernel_counter);\n\nstatic void __perf_pmu_remove(struct perf_event_context *ctx,\n\t\t\t      int cpu, struct pmu *pmu,\n\t\t\t      struct perf_event_groups *groups,\n\t\t\t      struct list_head *events)\n{\n\tstruct perf_event *event, *sibling;\n\n\tperf_event_groups_for_cpu_pmu(event, groups, cpu, pmu) {\n\t\tperf_remove_from_context(event, 0);\n\t\tput_pmu_ctx(event->pmu_ctx);\n\t\tlist_add(&event->migrate_entry, events);\n\n\t\tfor_each_sibling_event(sibling, event) {\n\t\t\tperf_remove_from_context(sibling, 0);\n\t\t\tput_pmu_ctx(sibling->pmu_ctx);\n\t\t\tlist_add(&sibling->migrate_entry, events);\n\t\t}\n\t}\n}\n\nstatic void __perf_pmu_install_event(struct pmu *pmu,\n\t\t\t\t     struct perf_event_context *ctx,\n\t\t\t\t     int cpu, struct perf_event *event)\n{\n\tstruct perf_event_pmu_context *epc;\n\tstruct perf_event_context *old_ctx = event->ctx;\n\n\tget_ctx(ctx);  \n\n\tevent->cpu = cpu;\n\tepc = find_get_pmu_context(pmu, ctx, event);\n\tevent->pmu_ctx = epc;\n\n\tif (event->state >= PERF_EVENT_STATE_OFF)\n\t\tevent->state = PERF_EVENT_STATE_INACTIVE;\n\tperf_install_in_context(ctx, event, cpu);\n\n\t \n\tput_ctx(old_ctx);\n}\n\nstatic void __perf_pmu_install(struct perf_event_context *ctx,\n\t\t\t       int cpu, struct pmu *pmu, struct list_head *events)\n{\n\tstruct perf_event *event, *tmp;\n\n\t \n\tlist_for_each_entry_safe(event, tmp, events, migrate_entry) {\n\t\tif (event->group_leader == event)\n\t\t\tcontinue;\n\n\t\tlist_del(&event->migrate_entry);\n\t\t__perf_pmu_install_event(pmu, ctx, cpu, event);\n\t}\n\n\t \n\tlist_for_each_entry_safe(event, tmp, events, migrate_entry) {\n\t\tlist_del(&event->migrate_entry);\n\t\t__perf_pmu_install_event(pmu, ctx, cpu, event);\n\t}\n}\n\nvoid perf_pmu_migrate_context(struct pmu *pmu, int src_cpu, int dst_cpu)\n{\n\tstruct perf_event_context *src_ctx, *dst_ctx;\n\tLIST_HEAD(events);\n\n\t \n\tsrc_ctx = &per_cpu_ptr(&perf_cpu_context, src_cpu)->ctx;\n\tdst_ctx = &per_cpu_ptr(&perf_cpu_context, dst_cpu)->ctx;\n\n\t \n\tmutex_lock_double(&src_ctx->mutex, &dst_ctx->mutex);\n\n\t__perf_pmu_remove(src_ctx, src_cpu, pmu, &src_ctx->pinned_groups, &events);\n\t__perf_pmu_remove(src_ctx, src_cpu, pmu, &src_ctx->flexible_groups, &events);\n\n\tif (!list_empty(&events)) {\n\t\t \n\t\tsynchronize_rcu();\n\n\t\t__perf_pmu_install(dst_ctx, dst_cpu, pmu, &events);\n\t}\n\n\tmutex_unlock(&dst_ctx->mutex);\n\tmutex_unlock(&src_ctx->mutex);\n}\nEXPORT_SYMBOL_GPL(perf_pmu_migrate_context);\n\nstatic void sync_child_event(struct perf_event *child_event)\n{\n\tstruct perf_event *parent_event = child_event->parent;\n\tu64 child_val;\n\n\tif (child_event->attr.inherit_stat) {\n\t\tstruct task_struct *task = child_event->ctx->task;\n\n\t\tif (task && task != TASK_TOMBSTONE)\n\t\t\tperf_event_read_event(child_event, task);\n\t}\n\n\tchild_val = perf_event_count(child_event);\n\n\t \n\tatomic64_add(child_val, &parent_event->child_count);\n\tatomic64_add(child_event->total_time_enabled,\n\t\t     &parent_event->child_total_time_enabled);\n\tatomic64_add(child_event->total_time_running,\n\t\t     &parent_event->child_total_time_running);\n}\n\nstatic void\nperf_event_exit_event(struct perf_event *event, struct perf_event_context *ctx)\n{\n\tstruct perf_event *parent_event = event->parent;\n\tunsigned long detach_flags = 0;\n\n\tif (parent_event) {\n\t\t \n\t\tdetach_flags = DETACH_GROUP | DETACH_CHILD;\n\t\tmutex_lock(&parent_event->child_mutex);\n\t}\n\n\tperf_remove_from_context(event, detach_flags);\n\n\traw_spin_lock_irq(&ctx->lock);\n\tif (event->state > PERF_EVENT_STATE_EXIT)\n\t\tperf_event_set_state(event, PERF_EVENT_STATE_EXIT);\n\traw_spin_unlock_irq(&ctx->lock);\n\n\t \n\tif (parent_event) {\n\t\tmutex_unlock(&parent_event->child_mutex);\n\t\t \n\t\tperf_event_wakeup(parent_event);\n\t\tfree_event(event);\n\t\tput_event(parent_event);\n\t\treturn;\n\t}\n\n\t \n\tperf_event_wakeup(event);\n}\n\nstatic void perf_event_exit_task_context(struct task_struct *child)\n{\n\tstruct perf_event_context *child_ctx, *clone_ctx = NULL;\n\tstruct perf_event *child_event, *next;\n\n\tWARN_ON_ONCE(child != current);\n\n\tchild_ctx = perf_pin_task_context(child);\n\tif (!child_ctx)\n\t\treturn;\n\n\t \n\tmutex_lock(&child_ctx->mutex);\n\n\t \n\traw_spin_lock_irq(&child_ctx->lock);\n\ttask_ctx_sched_out(child_ctx, EVENT_ALL);\n\n\t \n\tRCU_INIT_POINTER(child->perf_event_ctxp, NULL);\n\tput_ctx(child_ctx);  \n\tWRITE_ONCE(child_ctx->task, TASK_TOMBSTONE);\n\tput_task_struct(current);  \n\n\tclone_ctx = unclone_ctx(child_ctx);\n\traw_spin_unlock_irq(&child_ctx->lock);\n\n\tif (clone_ctx)\n\t\tput_ctx(clone_ctx);\n\n\t \n\tperf_event_task(child, child_ctx, 0);\n\n\tlist_for_each_entry_safe(child_event, next, &child_ctx->event_list, event_entry)\n\t\tperf_event_exit_event(child_event, child_ctx);\n\n\tmutex_unlock(&child_ctx->mutex);\n\n\tput_ctx(child_ctx);\n}\n\n \nvoid perf_event_exit_task(struct task_struct *child)\n{\n\tstruct perf_event *event, *tmp;\n\n\tmutex_lock(&child->perf_event_mutex);\n\tlist_for_each_entry_safe(event, tmp, &child->perf_event_list,\n\t\t\t\t owner_entry) {\n\t\tlist_del_init(&event->owner_entry);\n\n\t\t \n\t\tsmp_store_release(&event->owner, NULL);\n\t}\n\tmutex_unlock(&child->perf_event_mutex);\n\n\tperf_event_exit_task_context(child);\n\n\t \n\tperf_event_task(child, NULL, 0);\n}\n\nstatic void perf_free_event(struct perf_event *event,\n\t\t\t    struct perf_event_context *ctx)\n{\n\tstruct perf_event *parent = event->parent;\n\n\tif (WARN_ON_ONCE(!parent))\n\t\treturn;\n\n\tmutex_lock(&parent->child_mutex);\n\tlist_del_init(&event->child_list);\n\tmutex_unlock(&parent->child_mutex);\n\n\tput_event(parent);\n\n\traw_spin_lock_irq(&ctx->lock);\n\tperf_group_detach(event);\n\tlist_del_event(event, ctx);\n\traw_spin_unlock_irq(&ctx->lock);\n\tfree_event(event);\n}\n\n \nvoid perf_event_free_task(struct task_struct *task)\n{\n\tstruct perf_event_context *ctx;\n\tstruct perf_event *event, *tmp;\n\n\tctx = rcu_access_pointer(task->perf_event_ctxp);\n\tif (!ctx)\n\t\treturn;\n\n\tmutex_lock(&ctx->mutex);\n\traw_spin_lock_irq(&ctx->lock);\n\t \n\tRCU_INIT_POINTER(task->perf_event_ctxp, NULL);\n\tWRITE_ONCE(ctx->task, TASK_TOMBSTONE);\n\tput_task_struct(task);  \n\traw_spin_unlock_irq(&ctx->lock);\n\n\n\tlist_for_each_entry_safe(event, tmp, &ctx->event_list, event_entry)\n\t\tperf_free_event(event, ctx);\n\n\tmutex_unlock(&ctx->mutex);\n\n\t \n\twait_var_event(&ctx->refcount, refcount_read(&ctx->refcount) == 1);\n\tput_ctx(ctx);  \n}\n\nvoid perf_event_delayed_put(struct task_struct *task)\n{\n\tWARN_ON_ONCE(task->perf_event_ctxp);\n}\n\nstruct file *perf_event_get(unsigned int fd)\n{\n\tstruct file *file = fget(fd);\n\tif (!file)\n\t\treturn ERR_PTR(-EBADF);\n\n\tif (file->f_op != &perf_fops) {\n\t\tfput(file);\n\t\treturn ERR_PTR(-EBADF);\n\t}\n\n\treturn file;\n}\n\nconst struct perf_event *perf_get_event(struct file *file)\n{\n\tif (file->f_op != &perf_fops)\n\t\treturn ERR_PTR(-EINVAL);\n\n\treturn file->private_data;\n}\n\nconst struct perf_event_attr *perf_event_attrs(struct perf_event *event)\n{\n\tif (!event)\n\t\treturn ERR_PTR(-EINVAL);\n\n\treturn &event->attr;\n}\n\n \nstatic struct perf_event *\ninherit_event(struct perf_event *parent_event,\n\t      struct task_struct *parent,\n\t      struct perf_event_context *parent_ctx,\n\t      struct task_struct *child,\n\t      struct perf_event *group_leader,\n\t      struct perf_event_context *child_ctx)\n{\n\tenum perf_event_state parent_state = parent_event->state;\n\tstruct perf_event_pmu_context *pmu_ctx;\n\tstruct perf_event *child_event;\n\tunsigned long flags;\n\n\t \n\tif (parent_event->parent)\n\t\tparent_event = parent_event->parent;\n\n\tchild_event = perf_event_alloc(&parent_event->attr,\n\t\t\t\t\t   parent_event->cpu,\n\t\t\t\t\t   child,\n\t\t\t\t\t   group_leader, parent_event,\n\t\t\t\t\t   NULL, NULL, -1);\n\tif (IS_ERR(child_event))\n\t\treturn child_event;\n\n\tpmu_ctx = find_get_pmu_context(child_event->pmu, child_ctx, child_event);\n\tif (IS_ERR(pmu_ctx)) {\n\t\tfree_event(child_event);\n\t\treturn ERR_CAST(pmu_ctx);\n\t}\n\tchild_event->pmu_ctx = pmu_ctx;\n\n\t \n\tmutex_lock(&parent_event->child_mutex);\n\tif (is_orphaned_event(parent_event) ||\n\t    !atomic_long_inc_not_zero(&parent_event->refcount)) {\n\t\tmutex_unlock(&parent_event->child_mutex);\n\t\t \n\t\tfree_event(child_event);\n\t\treturn NULL;\n\t}\n\n\tget_ctx(child_ctx);\n\n\t \n\tif (parent_state >= PERF_EVENT_STATE_INACTIVE)\n\t\tchild_event->state = PERF_EVENT_STATE_INACTIVE;\n\telse\n\t\tchild_event->state = PERF_EVENT_STATE_OFF;\n\n\tif (parent_event->attr.freq) {\n\t\tu64 sample_period = parent_event->hw.sample_period;\n\t\tstruct hw_perf_event *hwc = &child_event->hw;\n\n\t\thwc->sample_period = sample_period;\n\t\thwc->last_period   = sample_period;\n\n\t\tlocal64_set(&hwc->period_left, sample_period);\n\t}\n\n\tchild_event->ctx = child_ctx;\n\tchild_event->overflow_handler = parent_event->overflow_handler;\n\tchild_event->overflow_handler_context\n\t\t= parent_event->overflow_handler_context;\n\n\t \n\tperf_event__header_size(child_event);\n\tperf_event__id_header_size(child_event);\n\n\t \n\traw_spin_lock_irqsave(&child_ctx->lock, flags);\n\tadd_event_to_ctx(child_event, child_ctx);\n\tchild_event->attach_state |= PERF_ATTACH_CHILD;\n\traw_spin_unlock_irqrestore(&child_ctx->lock, flags);\n\n\t \n\tlist_add_tail(&child_event->child_list, &parent_event->child_list);\n\tmutex_unlock(&parent_event->child_mutex);\n\n\treturn child_event;\n}\n\n \nstatic int inherit_group(struct perf_event *parent_event,\n\t      struct task_struct *parent,\n\t      struct perf_event_context *parent_ctx,\n\t      struct task_struct *child,\n\t      struct perf_event_context *child_ctx)\n{\n\tstruct perf_event *leader;\n\tstruct perf_event *sub;\n\tstruct perf_event *child_ctr;\n\n\tleader = inherit_event(parent_event, parent, parent_ctx,\n\t\t\t\t child, NULL, child_ctx);\n\tif (IS_ERR(leader))\n\t\treturn PTR_ERR(leader);\n\t \n\tfor_each_sibling_event(sub, parent_event) {\n\t\tchild_ctr = inherit_event(sub, parent, parent_ctx,\n\t\t\t\t\t    child, leader, child_ctx);\n\t\tif (IS_ERR(child_ctr))\n\t\t\treturn PTR_ERR(child_ctr);\n\n\t\tif (sub->aux_event == parent_event && child_ctr &&\n\t\t    !perf_get_aux_event(child_ctr, leader))\n\t\t\treturn -EINVAL;\n\t}\n\tif (leader)\n\t\tleader->group_generation = parent_event->group_generation;\n\treturn 0;\n}\n\n \nstatic int\ninherit_task_group(struct perf_event *event, struct task_struct *parent,\n\t\t   struct perf_event_context *parent_ctx,\n\t\t   struct task_struct *child,\n\t\t   u64 clone_flags, int *inherited_all)\n{\n\tstruct perf_event_context *child_ctx;\n\tint ret;\n\n\tif (!event->attr.inherit ||\n\t    (event->attr.inherit_thread && !(clone_flags & CLONE_THREAD)) ||\n\t     \n\t    (event->attr.sigtrap && (clone_flags & CLONE_CLEAR_SIGHAND))) {\n\t\t*inherited_all = 0;\n\t\treturn 0;\n\t}\n\n\tchild_ctx = child->perf_event_ctxp;\n\tif (!child_ctx) {\n\t\t \n\t\tchild_ctx = alloc_perf_context(child);\n\t\tif (!child_ctx)\n\t\t\treturn -ENOMEM;\n\n\t\tchild->perf_event_ctxp = child_ctx;\n\t}\n\n\tret = inherit_group(event, parent, parent_ctx, child, child_ctx);\n\tif (ret)\n\t\t*inherited_all = 0;\n\n\treturn ret;\n}\n\n \nstatic int perf_event_init_context(struct task_struct *child, u64 clone_flags)\n{\n\tstruct perf_event_context *child_ctx, *parent_ctx;\n\tstruct perf_event_context *cloned_ctx;\n\tstruct perf_event *event;\n\tstruct task_struct *parent = current;\n\tint inherited_all = 1;\n\tunsigned long flags;\n\tint ret = 0;\n\n\tif (likely(!parent->perf_event_ctxp))\n\t\treturn 0;\n\n\t \n\tparent_ctx = perf_pin_task_context(parent);\n\tif (!parent_ctx)\n\t\treturn 0;\n\n\t \n\n\t \n\tmutex_lock(&parent_ctx->mutex);\n\n\t \n\tperf_event_groups_for_each(event, &parent_ctx->pinned_groups) {\n\t\tret = inherit_task_group(event, parent, parent_ctx,\n\t\t\t\t\t child, clone_flags, &inherited_all);\n\t\tif (ret)\n\t\t\tgoto out_unlock;\n\t}\n\n\t \n\traw_spin_lock_irqsave(&parent_ctx->lock, flags);\n\tparent_ctx->rotate_disable = 1;\n\traw_spin_unlock_irqrestore(&parent_ctx->lock, flags);\n\n\tperf_event_groups_for_each(event, &parent_ctx->flexible_groups) {\n\t\tret = inherit_task_group(event, parent, parent_ctx,\n\t\t\t\t\t child, clone_flags, &inherited_all);\n\t\tif (ret)\n\t\t\tgoto out_unlock;\n\t}\n\n\traw_spin_lock_irqsave(&parent_ctx->lock, flags);\n\tparent_ctx->rotate_disable = 0;\n\n\tchild_ctx = child->perf_event_ctxp;\n\n\tif (child_ctx && inherited_all) {\n\t\t \n\t\tcloned_ctx = parent_ctx->parent_ctx;\n\t\tif (cloned_ctx) {\n\t\t\tchild_ctx->parent_ctx = cloned_ctx;\n\t\t\tchild_ctx->parent_gen = parent_ctx->parent_gen;\n\t\t} else {\n\t\t\tchild_ctx->parent_ctx = parent_ctx;\n\t\t\tchild_ctx->parent_gen = parent_ctx->generation;\n\t\t}\n\t\tget_ctx(child_ctx->parent_ctx);\n\t}\n\n\traw_spin_unlock_irqrestore(&parent_ctx->lock, flags);\nout_unlock:\n\tmutex_unlock(&parent_ctx->mutex);\n\n\tperf_unpin_context(parent_ctx);\n\tput_ctx(parent_ctx);\n\n\treturn ret;\n}\n\n \nint perf_event_init_task(struct task_struct *child, u64 clone_flags)\n{\n\tint ret;\n\n\tchild->perf_event_ctxp = NULL;\n\tmutex_init(&child->perf_event_mutex);\n\tINIT_LIST_HEAD(&child->perf_event_list);\n\n\tret = perf_event_init_context(child, clone_flags);\n\tif (ret) {\n\t\tperf_event_free_task(child);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic void __init perf_event_init_all_cpus(void)\n{\n\tstruct swevent_htable *swhash;\n\tstruct perf_cpu_context *cpuctx;\n\tint cpu;\n\n\tzalloc_cpumask_var(&perf_online_mask, GFP_KERNEL);\n\n\tfor_each_possible_cpu(cpu) {\n\t\tswhash = &per_cpu(swevent_htable, cpu);\n\t\tmutex_init(&swhash->hlist_mutex);\n\n\t\tINIT_LIST_HEAD(&per_cpu(pmu_sb_events.list, cpu));\n\t\traw_spin_lock_init(&per_cpu(pmu_sb_events.lock, cpu));\n\n\t\tINIT_LIST_HEAD(&per_cpu(sched_cb_list, cpu));\n\n\t\tcpuctx = per_cpu_ptr(&perf_cpu_context, cpu);\n\t\t__perf_event_init_context(&cpuctx->ctx);\n\t\tlockdep_set_class(&cpuctx->ctx.mutex, &cpuctx_mutex);\n\t\tlockdep_set_class(&cpuctx->ctx.lock, &cpuctx_lock);\n\t\tcpuctx->online = cpumask_test_cpu(cpu, perf_online_mask);\n\t\tcpuctx->heap_size = ARRAY_SIZE(cpuctx->heap_default);\n\t\tcpuctx->heap = cpuctx->heap_default;\n\t}\n}\n\nstatic void perf_swevent_init_cpu(unsigned int cpu)\n{\n\tstruct swevent_htable *swhash = &per_cpu(swevent_htable, cpu);\n\n\tmutex_lock(&swhash->hlist_mutex);\n\tif (swhash->hlist_refcount > 0 && !swevent_hlist_deref(swhash)) {\n\t\tstruct swevent_hlist *hlist;\n\n\t\thlist = kzalloc_node(sizeof(*hlist), GFP_KERNEL, cpu_to_node(cpu));\n\t\tWARN_ON(!hlist);\n\t\trcu_assign_pointer(swhash->swevent_hlist, hlist);\n\t}\n\tmutex_unlock(&swhash->hlist_mutex);\n}\n\n#if defined CONFIG_HOTPLUG_CPU || defined CONFIG_KEXEC_CORE\nstatic void __perf_event_exit_context(void *__info)\n{\n\tstruct perf_cpu_context *cpuctx = this_cpu_ptr(&perf_cpu_context);\n\tstruct perf_event_context *ctx = __info;\n\tstruct perf_event *event;\n\n\traw_spin_lock(&ctx->lock);\n\tctx_sched_out(ctx, EVENT_TIME);\n\tlist_for_each_entry(event, &ctx->event_list, event_entry)\n\t\t__perf_remove_from_context(event, cpuctx, ctx, (void *)DETACH_GROUP);\n\traw_spin_unlock(&ctx->lock);\n}\n\nstatic void perf_event_exit_cpu_context(int cpu)\n{\n\tstruct perf_cpu_context *cpuctx;\n\tstruct perf_event_context *ctx;\n\n\t\n\tmutex_lock(&pmus_lock);\n\tcpuctx = per_cpu_ptr(&perf_cpu_context, cpu);\n\tctx = &cpuctx->ctx;\n\n\tmutex_lock(&ctx->mutex);\n\tsmp_call_function_single(cpu, __perf_event_exit_context, ctx, 1);\n\tcpuctx->online = 0;\n\tmutex_unlock(&ctx->mutex);\n\tcpumask_clear_cpu(cpu, perf_online_mask);\n\tmutex_unlock(&pmus_lock);\n}\n#else\n\nstatic void perf_event_exit_cpu_context(int cpu) { }\n\n#endif\n\nint perf_event_init_cpu(unsigned int cpu)\n{\n\tstruct perf_cpu_context *cpuctx;\n\tstruct perf_event_context *ctx;\n\n\tperf_swevent_init_cpu(cpu);\n\n\tmutex_lock(&pmus_lock);\n\tcpumask_set_cpu(cpu, perf_online_mask);\n\tcpuctx = per_cpu_ptr(&perf_cpu_context, cpu);\n\tctx = &cpuctx->ctx;\n\n\tmutex_lock(&ctx->mutex);\n\tcpuctx->online = 1;\n\tmutex_unlock(&ctx->mutex);\n\tmutex_unlock(&pmus_lock);\n\n\treturn 0;\n}\n\nint perf_event_exit_cpu(unsigned int cpu)\n{\n\tperf_event_exit_cpu_context(cpu);\n\treturn 0;\n}\n\nstatic int\nperf_reboot(struct notifier_block *notifier, unsigned long val, void *v)\n{\n\tint cpu;\n\n\tfor_each_online_cpu(cpu)\n\t\tperf_event_exit_cpu(cpu);\n\n\treturn NOTIFY_OK;\n}\n\n \nstatic struct notifier_block perf_reboot_notifier = {\n\t.notifier_call = perf_reboot,\n\t.priority = INT_MIN,\n};\n\nvoid __init perf_event_init(void)\n{\n\tint ret;\n\n\tidr_init(&pmu_idr);\n\n\tperf_event_init_all_cpus();\n\tinit_srcu_struct(&pmus_srcu);\n\tperf_pmu_register(&perf_swevent, \"software\", PERF_TYPE_SOFTWARE);\n\tperf_pmu_register(&perf_cpu_clock, \"cpu_clock\", -1);\n\tperf_pmu_register(&perf_task_clock, \"task_clock\", -1);\n\tperf_tp_register();\n\tperf_event_init_cpu(smp_processor_id());\n\tregister_reboot_notifier(&perf_reboot_notifier);\n\n\tret = init_hw_breakpoint();\n\tWARN(ret, \"hw_breakpoint initialization failed with: %d\", ret);\n\n\tperf_event_cache = KMEM_CACHE(perf_event, SLAB_PANIC);\n\n\t \n\tBUILD_BUG_ON((offsetof(struct perf_event_mmap_page, data_head))\n\t\t     != 1024);\n}\n\nssize_t perf_event_sysfs_show(struct device *dev, struct device_attribute *attr,\n\t\t\t      char *page)\n{\n\tstruct perf_pmu_events_attr *pmu_attr =\n\t\tcontainer_of(attr, struct perf_pmu_events_attr, attr);\n\n\tif (pmu_attr->event_str)\n\t\treturn sprintf(page, \"%s\\n\", pmu_attr->event_str);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(perf_event_sysfs_show);\n\nstatic int __init perf_event_sysfs_init(void)\n{\n\tstruct pmu *pmu;\n\tint ret;\n\n\tmutex_lock(&pmus_lock);\n\n\tret = bus_register(&pmu_bus);\n\tif (ret)\n\t\tgoto unlock;\n\n\tlist_for_each_entry(pmu, &pmus, entry) {\n\t\tif (pmu->dev)\n\t\t\tcontinue;\n\n\t\tret = pmu_dev_alloc(pmu);\n\t\tWARN(ret, \"Failed to register pmu: %s, reason %d\\n\", pmu->name, ret);\n\t}\n\tpmu_bus_running = 1;\n\tret = 0;\n\nunlock:\n\tmutex_unlock(&pmus_lock);\n\n\treturn ret;\n}\ndevice_initcall(perf_event_sysfs_init);\n\n#ifdef CONFIG_CGROUP_PERF\nstatic struct cgroup_subsys_state *\nperf_cgroup_css_alloc(struct cgroup_subsys_state *parent_css)\n{\n\tstruct perf_cgroup *jc;\n\n\tjc = kzalloc(sizeof(*jc), GFP_KERNEL);\n\tif (!jc)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tjc->info = alloc_percpu(struct perf_cgroup_info);\n\tif (!jc->info) {\n\t\tkfree(jc);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\treturn &jc->css;\n}\n\nstatic void perf_cgroup_css_free(struct cgroup_subsys_state *css)\n{\n\tstruct perf_cgroup *jc = container_of(css, struct perf_cgroup, css);\n\n\tfree_percpu(jc->info);\n\tkfree(jc);\n}\n\nstatic int perf_cgroup_css_online(struct cgroup_subsys_state *css)\n{\n\tperf_event_cgroup(css->cgroup);\n\treturn 0;\n}\n\nstatic int __perf_cgroup_move(void *info)\n{\n\tstruct task_struct *task = info;\n\n\tpreempt_disable();\n\tperf_cgroup_switch(task);\n\tpreempt_enable();\n\n\treturn 0;\n}\n\nstatic void perf_cgroup_attach(struct cgroup_taskset *tset)\n{\n\tstruct task_struct *task;\n\tstruct cgroup_subsys_state *css;\n\n\tcgroup_taskset_for_each(task, css, tset)\n\t\ttask_function_call(task, __perf_cgroup_move, task);\n}\n\nstruct cgroup_subsys perf_event_cgrp_subsys = {\n\t.css_alloc\t= perf_cgroup_css_alloc,\n\t.css_free\t= perf_cgroup_css_free,\n\t.css_online\t= perf_cgroup_css_online,\n\t.attach\t\t= perf_cgroup_attach,\n\t \n\t.implicit_on_dfl = true,\n\t.threaded\t= true,\n};\n#endif  \n\nDEFINE_STATIC_CALL_RET0(perf_snapshot_branch_stack, perf_snapshot_branch_stack_t);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}