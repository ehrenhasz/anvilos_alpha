{
  "module_name": "hw_breakpoint.c",
  "hash_id": "8ea878722b9d881f1cc46ae93449bf56a0ebfd70ceb22452d9eb32ced32377ce",
  "original_prompt": "Ingested from linux-6.6.14/kernel/events/hw_breakpoint.c",
  "human_readable_source": "\n \n\n \n\n#include <linux/hw_breakpoint.h>\n\n#include <linux/atomic.h>\n#include <linux/bug.h>\n#include <linux/cpu.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/irqflags.h>\n#include <linux/kdebug.h>\n#include <linux/kernel.h>\n#include <linux/mutex.h>\n#include <linux/notifier.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/percpu.h>\n#include <linux/rhashtable.h>\n#include <linux/sched.h>\n#include <linux/slab.h>\n\n \nstruct bp_slots_histogram {\n#ifdef hw_breakpoint_slots\n\tatomic_t count[hw_breakpoint_slots(0)];\n#else\n\tatomic_t *count;\n#endif\n};\n\n \nstruct bp_cpuinfo {\n\t \n\tunsigned int\t\t\tcpu_pinned;\n\t \n\tstruct bp_slots_histogram\ttsk_pinned;\n};\n\nstatic DEFINE_PER_CPU(struct bp_cpuinfo, bp_cpuinfo[TYPE_MAX]);\n\nstatic struct bp_cpuinfo *get_bp_info(int cpu, enum bp_type_idx type)\n{\n\treturn per_cpu_ptr(bp_cpuinfo + type, cpu);\n}\n\n \nstatic struct bp_slots_histogram cpu_pinned[TYPE_MAX];\n \nstatic struct bp_slots_histogram tsk_pinned_all[TYPE_MAX];\n\n \nstatic struct rhltable task_bps_ht;\nstatic const struct rhashtable_params task_bps_ht_params = {\n\t.head_offset = offsetof(struct hw_perf_event, bp_list),\n\t.key_offset = offsetof(struct hw_perf_event, target),\n\t.key_len = sizeof_field(struct hw_perf_event, target),\n\t.automatic_shrinking = true,\n};\n\nstatic bool constraints_initialized __ro_after_init;\n\n \nDEFINE_STATIC_PERCPU_RWSEM(bp_cpuinfo_sem);\n\n \nstatic inline struct mutex *get_task_bps_mutex(struct perf_event *bp)\n{\n\tstruct task_struct *tsk = bp->hw.target;\n\n\treturn tsk ? &tsk->perf_event_mutex : NULL;\n}\n\nstatic struct mutex *bp_constraints_lock(struct perf_event *bp)\n{\n\tstruct mutex *tsk_mtx = get_task_bps_mutex(bp);\n\n\tif (tsk_mtx) {\n\t\t \n\t\tmutex_lock_nested(tsk_mtx, SINGLE_DEPTH_NESTING);\n\t\tpercpu_down_read(&bp_cpuinfo_sem);\n\t} else {\n\t\tpercpu_down_write(&bp_cpuinfo_sem);\n\t}\n\n\treturn tsk_mtx;\n}\n\nstatic void bp_constraints_unlock(struct mutex *tsk_mtx)\n{\n\tif (tsk_mtx) {\n\t\tpercpu_up_read(&bp_cpuinfo_sem);\n\t\tmutex_unlock(tsk_mtx);\n\t} else {\n\t\tpercpu_up_write(&bp_cpuinfo_sem);\n\t}\n}\n\nstatic bool bp_constraints_is_locked(struct perf_event *bp)\n{\n\tstruct mutex *tsk_mtx = get_task_bps_mutex(bp);\n\n\treturn percpu_is_write_locked(&bp_cpuinfo_sem) ||\n\t       (tsk_mtx ? mutex_is_locked(tsk_mtx) :\n\t\t\t  percpu_is_read_locked(&bp_cpuinfo_sem));\n}\n\nstatic inline void assert_bp_constraints_lock_held(struct perf_event *bp)\n{\n\tstruct mutex *tsk_mtx = get_task_bps_mutex(bp);\n\n\tif (tsk_mtx)\n\t\tlockdep_assert_held(tsk_mtx);\n\tlockdep_assert_held(&bp_cpuinfo_sem);\n}\n\n#ifdef hw_breakpoint_slots\n \nstatic_assert(hw_breakpoint_slots(TYPE_INST) == hw_breakpoint_slots(TYPE_DATA));\nstatic inline int hw_breakpoint_slots_cached(int type)\t{ return hw_breakpoint_slots(type); }\nstatic inline int init_breakpoint_slots(void)\t\t{ return 0; }\n#else\n \nstatic int __nr_bp_slots[TYPE_MAX] __ro_after_init;\n\nstatic inline int hw_breakpoint_slots_cached(int type)\n{\n\treturn __nr_bp_slots[type];\n}\n\nstatic __init bool\nbp_slots_histogram_alloc(struct bp_slots_histogram *hist, enum bp_type_idx type)\n{\n\thist->count = kcalloc(hw_breakpoint_slots_cached(type), sizeof(*hist->count), GFP_KERNEL);\n\treturn hist->count;\n}\n\nstatic __init void bp_slots_histogram_free(struct bp_slots_histogram *hist)\n{\n\tkfree(hist->count);\n}\n\nstatic __init int init_breakpoint_slots(void)\n{\n\tint i, cpu, err_cpu;\n\n\tfor (i = 0; i < TYPE_MAX; i++)\n\t\t__nr_bp_slots[i] = hw_breakpoint_slots(i);\n\n\tfor_each_possible_cpu(cpu) {\n\t\tfor (i = 0; i < TYPE_MAX; i++) {\n\t\t\tstruct bp_cpuinfo *info = get_bp_info(cpu, i);\n\n\t\t\tif (!bp_slots_histogram_alloc(&info->tsk_pinned, i))\n\t\t\t\tgoto err;\n\t\t}\n\t}\n\tfor (i = 0; i < TYPE_MAX; i++) {\n\t\tif (!bp_slots_histogram_alloc(&cpu_pinned[i], i))\n\t\t\tgoto err;\n\t\tif (!bp_slots_histogram_alloc(&tsk_pinned_all[i], i))\n\t\t\tgoto err;\n\t}\n\n\treturn 0;\nerr:\n\tfor_each_possible_cpu(err_cpu) {\n\t\tfor (i = 0; i < TYPE_MAX; i++)\n\t\t\tbp_slots_histogram_free(&get_bp_info(err_cpu, i)->tsk_pinned);\n\t\tif (err_cpu == cpu)\n\t\t\tbreak;\n\t}\n\tfor (i = 0; i < TYPE_MAX; i++) {\n\t\tbp_slots_histogram_free(&cpu_pinned[i]);\n\t\tbp_slots_histogram_free(&tsk_pinned_all[i]);\n\t}\n\n\treturn -ENOMEM;\n}\n#endif\n\nstatic inline void\nbp_slots_histogram_add(struct bp_slots_histogram *hist, int old, int val)\n{\n\tconst int old_idx = old - 1;\n\tconst int new_idx = old_idx + val;\n\n\tif (old_idx >= 0)\n\t\tWARN_ON(atomic_dec_return_relaxed(&hist->count[old_idx]) < 0);\n\tif (new_idx >= 0)\n\t\tWARN_ON(atomic_inc_return_relaxed(&hist->count[new_idx]) < 0);\n}\n\nstatic int\nbp_slots_histogram_max(struct bp_slots_histogram *hist, enum bp_type_idx type)\n{\n\tfor (int i = hw_breakpoint_slots_cached(type) - 1; i >= 0; i--) {\n\t\tconst int count = atomic_read(&hist->count[i]);\n\n\t\t \n\t\tASSERT_EXCLUSIVE_WRITER(hist->count[i]);\n\t\tif (count > 0)\n\t\t\treturn i + 1;\n\t\tWARN(count < 0, \"inconsistent breakpoint slots histogram\");\n\t}\n\n\treturn 0;\n}\n\nstatic int\nbp_slots_histogram_max_merge(struct bp_slots_histogram *hist1, struct bp_slots_histogram *hist2,\n\t\t\t     enum bp_type_idx type)\n{\n\tfor (int i = hw_breakpoint_slots_cached(type) - 1; i >= 0; i--) {\n\t\tconst int count1 = atomic_read(&hist1->count[i]);\n\t\tconst int count2 = atomic_read(&hist2->count[i]);\n\n\t\t \n\t\tASSERT_EXCLUSIVE_WRITER(hist1->count[i]);\n\t\tASSERT_EXCLUSIVE_WRITER(hist2->count[i]);\n\t\tif (count1 + count2 > 0)\n\t\t\treturn i + 1;\n\t\tWARN(count1 < 0, \"inconsistent breakpoint slots histogram\");\n\t\tWARN(count2 < 0, \"inconsistent breakpoint slots histogram\");\n\t}\n\n\treturn 0;\n}\n\n#ifndef hw_breakpoint_weight\nstatic inline int hw_breakpoint_weight(struct perf_event *bp)\n{\n\treturn 1;\n}\n#endif\n\nstatic inline enum bp_type_idx find_slot_idx(u64 bp_type)\n{\n\tif (bp_type & HW_BREAKPOINT_RW)\n\t\treturn TYPE_DATA;\n\n\treturn TYPE_INST;\n}\n\n \nstatic unsigned int max_task_bp_pinned(int cpu, enum bp_type_idx type)\n{\n\tstruct bp_slots_histogram *tsk_pinned = &get_bp_info(cpu, type)->tsk_pinned;\n\n\t \n\tlockdep_assert_held_write(&bp_cpuinfo_sem);\n\treturn bp_slots_histogram_max_merge(tsk_pinned, &tsk_pinned_all[type], type);\n}\n\n \nstatic int task_bp_pinned(int cpu, struct perf_event *bp, enum bp_type_idx type)\n{\n\tstruct rhlist_head *head, *pos;\n\tstruct perf_event *iter;\n\tint count = 0;\n\n\t \n\tassert_bp_constraints_lock_held(bp);\n\n\trcu_read_lock();\n\thead = rhltable_lookup(&task_bps_ht, &bp->hw.target, task_bps_ht_params);\n\tif (!head)\n\t\tgoto out;\n\n\trhl_for_each_entry_rcu(iter, pos, head, hw.bp_list) {\n\t\tif (find_slot_idx(iter->attr.bp_type) != type)\n\t\t\tcontinue;\n\n\t\tif (iter->cpu >= 0) {\n\t\t\tif (cpu == -1) {\n\t\t\t\tcount = -1;\n\t\t\t\tgoto out;\n\t\t\t} else if (cpu != iter->cpu)\n\t\t\t\tcontinue;\n\t\t}\n\n\t\tcount += hw_breakpoint_weight(iter);\n\t}\n\nout:\n\trcu_read_unlock();\n\treturn count;\n}\n\nstatic const struct cpumask *cpumask_of_bp(struct perf_event *bp)\n{\n\tif (bp->cpu >= 0)\n\t\treturn cpumask_of(bp->cpu);\n\treturn cpu_possible_mask;\n}\n\n \nstatic int\nmax_bp_pinned_slots(struct perf_event *bp, enum bp_type_idx type)\n{\n\tconst struct cpumask *cpumask = cpumask_of_bp(bp);\n\tint pinned_slots = 0;\n\tint cpu;\n\n\tif (bp->hw.target && bp->cpu < 0) {\n\t\tint max_pinned = task_bp_pinned(-1, bp, type);\n\n\t\tif (max_pinned >= 0) {\n\t\t\t \n\t\t\tmax_pinned += bp_slots_histogram_max(&cpu_pinned[type], type);\n\t\t\treturn max_pinned;\n\t\t}\n\t}\n\n\tfor_each_cpu(cpu, cpumask) {\n\t\tstruct bp_cpuinfo *info = get_bp_info(cpu, type);\n\t\tint nr;\n\n\t\tnr = info->cpu_pinned;\n\t\tif (!bp->hw.target)\n\t\t\tnr += max_task_bp_pinned(cpu, type);\n\t\telse\n\t\t\tnr += task_bp_pinned(cpu, bp, type);\n\n\t\tpinned_slots = max(nr, pinned_slots);\n\t}\n\n\treturn pinned_slots;\n}\n\n \nstatic int\ntoggle_bp_slot(struct perf_event *bp, bool enable, enum bp_type_idx type, int weight)\n{\n\tint cpu, next_tsk_pinned;\n\n\tif (!enable)\n\t\tweight = -weight;\n\n\tif (!bp->hw.target) {\n\t\t \n\t\tstruct bp_cpuinfo *info = get_bp_info(bp->cpu, type);\n\n\t\tlockdep_assert_held_write(&bp_cpuinfo_sem);\n\t\tbp_slots_histogram_add(&cpu_pinned[type], info->cpu_pinned, weight);\n\t\tinfo->cpu_pinned += weight;\n\t\treturn 0;\n\t}\n\n\t \n\tlockdep_assert_held_read(&bp_cpuinfo_sem);\n\n\t \n\n\tif (!enable) {\n\t\t \n\t\tint ret = rhltable_remove(&task_bps_ht, &bp->hw.bp_list, task_bps_ht_params);\n\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\t \n\tnext_tsk_pinned = task_bp_pinned(-1, bp, type);\n\n\tif (next_tsk_pinned >= 0) {\n\t\tif (bp->cpu < 0) {  \n\t\t\tif (!enable)\n\t\t\t\tnext_tsk_pinned += hw_breakpoint_weight(bp);\n\t\t\tbp_slots_histogram_add(&tsk_pinned_all[type], next_tsk_pinned, weight);\n\t\t} else if (enable) {  \n\t\t\t \n\t\t\tfor_each_possible_cpu(cpu) {\n\t\t\t\tbp_slots_histogram_add(&get_bp_info(cpu, type)->tsk_pinned,\n\t\t\t\t\t\t       0, next_tsk_pinned);\n\t\t\t}\n\t\t\t \n\t\t\tbp_slots_histogram_add(&get_bp_info(bp->cpu, type)->tsk_pinned,\n\t\t\t\t\t       next_tsk_pinned, weight);\n\t\t\t \n\t\t\tbp_slots_histogram_add(&tsk_pinned_all[type], next_tsk_pinned,\n\t\t\t\t\t       -next_tsk_pinned);\n\t\t} else {  \n\t\t\t \n\t\t\tbp_slots_histogram_add(&get_bp_info(bp->cpu, type)->tsk_pinned,\n\t\t\t\t\t       next_tsk_pinned + hw_breakpoint_weight(bp), weight);\n\t\t\t \n\t\t\tfor_each_possible_cpu(cpu) {\n\t\t\t\tbp_slots_histogram_add(&get_bp_info(cpu, type)->tsk_pinned,\n\t\t\t\t\t\t       next_tsk_pinned, -next_tsk_pinned);\n\t\t\t}\n\t\t\t \n\t\t\tbp_slots_histogram_add(&tsk_pinned_all[type], 0, next_tsk_pinned);\n\t\t}\n\t} else {  \n\t\tconst struct cpumask *cpumask = cpumask_of_bp(bp);\n\n\t\tfor_each_cpu(cpu, cpumask) {\n\t\t\tnext_tsk_pinned = task_bp_pinned(cpu, bp, type);\n\t\t\tif (!enable)\n\t\t\t\tnext_tsk_pinned += hw_breakpoint_weight(bp);\n\t\t\tbp_slots_histogram_add(&get_bp_info(cpu, type)->tsk_pinned,\n\t\t\t\t\t       next_tsk_pinned, weight);\n\t\t}\n\t}\n\n\t \n\tassert_bp_constraints_lock_held(bp);\n\n\tif (enable)\n\t\treturn rhltable_insert(&task_bps_ht, &bp->hw.bp_list, task_bps_ht_params);\n\n\treturn 0;\n}\n\n \nstatic int __reserve_bp_slot(struct perf_event *bp, u64 bp_type)\n{\n\tenum bp_type_idx type;\n\tint max_pinned_slots;\n\tint weight;\n\n\t \n\tif (!constraints_initialized)\n\t\treturn -ENOMEM;\n\n\t \n\tif (bp_type == HW_BREAKPOINT_EMPTY ||\n\t    bp_type == HW_BREAKPOINT_INVALID)\n\t\treturn -EINVAL;\n\n\ttype = find_slot_idx(bp_type);\n\tweight = hw_breakpoint_weight(bp);\n\n\t \n\tmax_pinned_slots = max_bp_pinned_slots(bp, type) + weight;\n\tif (max_pinned_slots > hw_breakpoint_slots_cached(type))\n\t\treturn -ENOSPC;\n\n\treturn toggle_bp_slot(bp, true, type, weight);\n}\n\nint reserve_bp_slot(struct perf_event *bp)\n{\n\tstruct mutex *mtx = bp_constraints_lock(bp);\n\tint ret = __reserve_bp_slot(bp, bp->attr.bp_type);\n\n\tbp_constraints_unlock(mtx);\n\treturn ret;\n}\n\nstatic void __release_bp_slot(struct perf_event *bp, u64 bp_type)\n{\n\tenum bp_type_idx type;\n\tint weight;\n\n\ttype = find_slot_idx(bp_type);\n\tweight = hw_breakpoint_weight(bp);\n\tWARN_ON(toggle_bp_slot(bp, false, type, weight));\n}\n\nvoid release_bp_slot(struct perf_event *bp)\n{\n\tstruct mutex *mtx = bp_constraints_lock(bp);\n\n\t__release_bp_slot(bp, bp->attr.bp_type);\n\tbp_constraints_unlock(mtx);\n}\n\nstatic int __modify_bp_slot(struct perf_event *bp, u64 old_type, u64 new_type)\n{\n\tint err;\n\n\t__release_bp_slot(bp, old_type);\n\n\terr = __reserve_bp_slot(bp, new_type);\n\tif (err) {\n\t\t \n\t\tWARN_ON(__reserve_bp_slot(bp, old_type));\n\t}\n\n\treturn err;\n}\n\nstatic int modify_bp_slot(struct perf_event *bp, u64 old_type, u64 new_type)\n{\n\tstruct mutex *mtx = bp_constraints_lock(bp);\n\tint ret = __modify_bp_slot(bp, old_type, new_type);\n\n\tbp_constraints_unlock(mtx);\n\treturn ret;\n}\n\n \nint dbg_reserve_bp_slot(struct perf_event *bp)\n{\n\tint ret;\n\n\tif (bp_constraints_is_locked(bp))\n\t\treturn -1;\n\n\t \n\tlockdep_off();\n\tret = __reserve_bp_slot(bp, bp->attr.bp_type);\n\tlockdep_on();\n\n\treturn ret;\n}\n\nint dbg_release_bp_slot(struct perf_event *bp)\n{\n\tif (bp_constraints_is_locked(bp))\n\t\treturn -1;\n\n\t \n\tlockdep_off();\n\t__release_bp_slot(bp, bp->attr.bp_type);\n\tlockdep_on();\n\n\treturn 0;\n}\n\nstatic int hw_breakpoint_parse(struct perf_event *bp,\n\t\t\t       const struct perf_event_attr *attr,\n\t\t\t       struct arch_hw_breakpoint *hw)\n{\n\tint err;\n\n\terr = hw_breakpoint_arch_parse(bp, attr, hw);\n\tif (err)\n\t\treturn err;\n\n\tif (arch_check_bp_in_kernelspace(hw)) {\n\t\tif (attr->exclude_kernel)\n\t\t\treturn -EINVAL;\n\t\t \n\t\tif (!capable(CAP_SYS_ADMIN))\n\t\t\treturn -EPERM;\n\t}\n\n\treturn 0;\n}\n\nint register_perf_hw_breakpoint(struct perf_event *bp)\n{\n\tstruct arch_hw_breakpoint hw = { };\n\tint err;\n\n\terr = reserve_bp_slot(bp);\n\tif (err)\n\t\treturn err;\n\n\terr = hw_breakpoint_parse(bp, &bp->attr, &hw);\n\tif (err) {\n\t\trelease_bp_slot(bp);\n\t\treturn err;\n\t}\n\n\tbp->hw.info = hw;\n\n\treturn 0;\n}\n\n \nstruct perf_event *\nregister_user_hw_breakpoint(struct perf_event_attr *attr,\n\t\t\t    perf_overflow_handler_t triggered,\n\t\t\t    void *context,\n\t\t\t    struct task_struct *tsk)\n{\n\treturn perf_event_create_kernel_counter(attr, -1, tsk, triggered,\n\t\t\t\t\t\tcontext);\n}\nEXPORT_SYMBOL_GPL(register_user_hw_breakpoint);\n\nstatic void hw_breakpoint_copy_attr(struct perf_event_attr *to,\n\t\t\t\t    struct perf_event_attr *from)\n{\n\tto->bp_addr = from->bp_addr;\n\tto->bp_type = from->bp_type;\n\tto->bp_len  = from->bp_len;\n\tto->disabled = from->disabled;\n}\n\nint\nmodify_user_hw_breakpoint_check(struct perf_event *bp, struct perf_event_attr *attr,\n\t\t\t        bool check)\n{\n\tstruct arch_hw_breakpoint hw = { };\n\tint err;\n\n\terr = hw_breakpoint_parse(bp, attr, &hw);\n\tif (err)\n\t\treturn err;\n\n\tif (check) {\n\t\tstruct perf_event_attr old_attr;\n\n\t\told_attr = bp->attr;\n\t\thw_breakpoint_copy_attr(&old_attr, attr);\n\t\tif (memcmp(&old_attr, attr, sizeof(*attr)))\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (bp->attr.bp_type != attr->bp_type) {\n\t\terr = modify_bp_slot(bp, bp->attr.bp_type, attr->bp_type);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\thw_breakpoint_copy_attr(&bp->attr, attr);\n\tbp->hw.info = hw;\n\n\treturn 0;\n}\n\n \nint modify_user_hw_breakpoint(struct perf_event *bp, struct perf_event_attr *attr)\n{\n\tint err;\n\n\t \n\tif (irqs_disabled() && bp->ctx && bp->ctx->task == current)\n\t\tperf_event_disable_local(bp);\n\telse\n\t\tperf_event_disable(bp);\n\n\terr = modify_user_hw_breakpoint_check(bp, attr, false);\n\n\tif (!bp->attr.disabled)\n\t\tperf_event_enable(bp);\n\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(modify_user_hw_breakpoint);\n\n \nvoid unregister_hw_breakpoint(struct perf_event *bp)\n{\n\tif (!bp)\n\t\treturn;\n\tperf_event_release_kernel(bp);\n}\nEXPORT_SYMBOL_GPL(unregister_hw_breakpoint);\n\n \nstruct perf_event * __percpu *\nregister_wide_hw_breakpoint(struct perf_event_attr *attr,\n\t\t\t    perf_overflow_handler_t triggered,\n\t\t\t    void *context)\n{\n\tstruct perf_event * __percpu *cpu_events, *bp;\n\tlong err = 0;\n\tint cpu;\n\n\tcpu_events = alloc_percpu(typeof(*cpu_events));\n\tif (!cpu_events)\n\t\treturn (void __percpu __force *)ERR_PTR(-ENOMEM);\n\n\tcpus_read_lock();\n\tfor_each_online_cpu(cpu) {\n\t\tbp = perf_event_create_kernel_counter(attr, cpu, NULL,\n\t\t\t\t\t\t      triggered, context);\n\t\tif (IS_ERR(bp)) {\n\t\t\terr = PTR_ERR(bp);\n\t\t\tbreak;\n\t\t}\n\n\t\tper_cpu(*cpu_events, cpu) = bp;\n\t}\n\tcpus_read_unlock();\n\n\tif (likely(!err))\n\t\treturn cpu_events;\n\n\tunregister_wide_hw_breakpoint(cpu_events);\n\treturn (void __percpu __force *)ERR_PTR(err);\n}\nEXPORT_SYMBOL_GPL(register_wide_hw_breakpoint);\n\n \nvoid unregister_wide_hw_breakpoint(struct perf_event * __percpu *cpu_events)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu)\n\t\tunregister_hw_breakpoint(per_cpu(*cpu_events, cpu));\n\n\tfree_percpu(cpu_events);\n}\nEXPORT_SYMBOL_GPL(unregister_wide_hw_breakpoint);\n\n \nbool hw_breakpoint_is_used(void)\n{\n\tint cpu;\n\n\tif (!constraints_initialized)\n\t\treturn false;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tfor (int type = 0; type < TYPE_MAX; ++type) {\n\t\t\tstruct bp_cpuinfo *info = get_bp_info(cpu, type);\n\n\t\t\tif (info->cpu_pinned)\n\t\t\t\treturn true;\n\n\t\t\tfor (int slot = 0; slot < hw_breakpoint_slots_cached(type); ++slot) {\n\t\t\t\tif (atomic_read(&info->tsk_pinned.count[slot]))\n\t\t\t\t\treturn true;\n\t\t\t}\n\t\t}\n\t}\n\n\tfor (int type = 0; type < TYPE_MAX; ++type) {\n\t\tfor (int slot = 0; slot < hw_breakpoint_slots_cached(type); ++slot) {\n\t\t\t \n\t\t\tif (WARN_ON(atomic_read(&cpu_pinned[type].count[slot])))\n\t\t\t\treturn true;\n\n\t\t\tif (atomic_read(&tsk_pinned_all[type].count[slot]))\n\t\t\t\treturn true;\n\t\t}\n\t}\n\n\treturn false;\n}\n\nstatic struct notifier_block hw_breakpoint_exceptions_nb = {\n\t.notifier_call = hw_breakpoint_exceptions_notify,\n\t \n\t.priority = 0x7fffffff\n};\n\nstatic void bp_perf_event_destroy(struct perf_event *event)\n{\n\trelease_bp_slot(event);\n}\n\nstatic int hw_breakpoint_event_init(struct perf_event *bp)\n{\n\tint err;\n\n\tif (bp->attr.type != PERF_TYPE_BREAKPOINT)\n\t\treturn -ENOENT;\n\n\t \n\tif (has_branch_stack(bp))\n\t\treturn -EOPNOTSUPP;\n\n\terr = register_perf_hw_breakpoint(bp);\n\tif (err)\n\t\treturn err;\n\n\tbp->destroy = bp_perf_event_destroy;\n\n\treturn 0;\n}\n\nstatic int hw_breakpoint_add(struct perf_event *bp, int flags)\n{\n\tif (!(flags & PERF_EF_START))\n\t\tbp->hw.state = PERF_HES_STOPPED;\n\n\tif (is_sampling_event(bp)) {\n\t\tbp->hw.last_period = bp->hw.sample_period;\n\t\tperf_swevent_set_period(bp);\n\t}\n\n\treturn arch_install_hw_breakpoint(bp);\n}\n\nstatic void hw_breakpoint_del(struct perf_event *bp, int flags)\n{\n\tarch_uninstall_hw_breakpoint(bp);\n}\n\nstatic void hw_breakpoint_start(struct perf_event *bp, int flags)\n{\n\tbp->hw.state = 0;\n}\n\nstatic void hw_breakpoint_stop(struct perf_event *bp, int flags)\n{\n\tbp->hw.state = PERF_HES_STOPPED;\n}\n\nstatic struct pmu perf_breakpoint = {\n\t.task_ctx_nr\t= perf_sw_context,  \n\n\t.event_init\t= hw_breakpoint_event_init,\n\t.add\t\t= hw_breakpoint_add,\n\t.del\t\t= hw_breakpoint_del,\n\t.start\t\t= hw_breakpoint_start,\n\t.stop\t\t= hw_breakpoint_stop,\n\t.read\t\t= hw_breakpoint_pmu_read,\n};\n\nint __init init_hw_breakpoint(void)\n{\n\tint ret;\n\n\tret = rhltable_init(&task_bps_ht, &task_bps_ht_params);\n\tif (ret)\n\t\treturn ret;\n\n\tret = init_breakpoint_slots();\n\tif (ret)\n\t\treturn ret;\n\n\tconstraints_initialized = true;\n\n\tperf_pmu_register(&perf_breakpoint, \"breakpoint\", PERF_TYPE_BREAKPOINT);\n\n\treturn register_die_notifier(&hw_breakpoint_exceptions_nb);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}