{
  "module_name": "uprobes.c",
  "hash_id": "9eb516ccf57a3e25c5a9cebfe101b675d80bd6e8d2aefd68476b322c18527b64",
  "original_prompt": "Ingested from linux-6.6.14/kernel/events/uprobes.c",
  "human_readable_source": "\n \n\n#include <linux/kernel.h>\n#include <linux/highmem.h>\n#include <linux/pagemap.h>\t \n#include <linux/slab.h>\n#include <linux/sched.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/coredump.h>\n#include <linux/export.h>\n#include <linux/rmap.h>\t\t \n#include <linux/mmu_notifier.h>\t \n#include <linux/swap.h>\t\t \n#include <linux/ptrace.h>\t \n#include <linux/kdebug.h>\t \n#include <linux/percpu-rwsem.h>\n#include <linux/task_work.h>\n#include <linux/shmem_fs.h>\n#include <linux/khugepaged.h>\n\n#include <linux/uprobes.h>\n\n#define UINSNS_PER_PAGE\t\t\t(PAGE_SIZE/UPROBE_XOL_SLOT_BYTES)\n#define MAX_UPROBE_XOL_SLOTS\t\tUINSNS_PER_PAGE\n\nstatic struct rb_root uprobes_tree = RB_ROOT;\n \n#define no_uprobe_events()\tRB_EMPTY_ROOT(&uprobes_tree)\n\nstatic DEFINE_SPINLOCK(uprobes_treelock);\t \n\n#define UPROBES_HASH_SZ\t13\n \nstatic struct mutex uprobes_mmap_mutex[UPROBES_HASH_SZ];\n#define uprobes_mmap_hash(v)\t(&uprobes_mmap_mutex[((unsigned long)(v)) % UPROBES_HASH_SZ])\n\nDEFINE_STATIC_PERCPU_RWSEM(dup_mmap_sem);\n\n \n#define UPROBE_COPY_INSN\t0\n\nstruct uprobe {\n\tstruct rb_node\t\trb_node;\t \n\trefcount_t\t\tref;\n\tstruct rw_semaphore\tregister_rwsem;\n\tstruct rw_semaphore\tconsumer_rwsem;\n\tstruct list_head\tpending_list;\n\tstruct uprobe_consumer\t*consumers;\n\tstruct inode\t\t*inode;\t\t \n\tloff_t\t\t\toffset;\n\tloff_t\t\t\tref_ctr_offset;\n\tunsigned long\t\tflags;\n\n\t \n\tstruct arch_uprobe\tarch;\n};\n\nstruct delayed_uprobe {\n\tstruct list_head list;\n\tstruct uprobe *uprobe;\n\tstruct mm_struct *mm;\n};\n\nstatic DEFINE_MUTEX(delayed_uprobe_lock);\nstatic LIST_HEAD(delayed_uprobe_list);\n\n \nstruct xol_area {\n\twait_queue_head_t \t\twq;\t\t \n\tatomic_t \t\t\tslot_count;\t \n\tunsigned long \t\t\t*bitmap;\t \n\n\tstruct vm_special_mapping\txol_mapping;\n\tstruct page \t\t\t*pages[2];\n\t \n\tunsigned long \t\t\tvaddr;\t\t \n};\n\n \nstatic bool valid_vma(struct vm_area_struct *vma, bool is_register)\n{\n\tvm_flags_t flags = VM_HUGETLB | VM_MAYEXEC | VM_MAYSHARE;\n\n\tif (is_register)\n\t\tflags |= VM_WRITE;\n\n\treturn vma->vm_file && (vma->vm_flags & flags) == VM_MAYEXEC;\n}\n\nstatic unsigned long offset_to_vaddr(struct vm_area_struct *vma, loff_t offset)\n{\n\treturn vma->vm_start + offset - ((loff_t)vma->vm_pgoff << PAGE_SHIFT);\n}\n\nstatic loff_t vaddr_to_offset(struct vm_area_struct *vma, unsigned long vaddr)\n{\n\treturn ((loff_t)vma->vm_pgoff << PAGE_SHIFT) + (vaddr - vma->vm_start);\n}\n\n \nstatic int __replace_page(struct vm_area_struct *vma, unsigned long addr,\n\t\t\t\tstruct page *old_page, struct page *new_page)\n{\n\tstruct folio *old_folio = page_folio(old_page);\n\tstruct folio *new_folio;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tDEFINE_FOLIO_VMA_WALK(pvmw, old_folio, vma, addr, 0);\n\tint err;\n\tstruct mmu_notifier_range range;\n\n\tmmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, mm, addr,\n\t\t\t\taddr + PAGE_SIZE);\n\n\tif (new_page) {\n\t\tnew_folio = page_folio(new_page);\n\t\terr = mem_cgroup_charge(new_folio, vma->vm_mm, GFP_KERNEL);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\t \n\tfolio_lock(old_folio);\n\n\tmmu_notifier_invalidate_range_start(&range);\n\terr = -EAGAIN;\n\tif (!page_vma_mapped_walk(&pvmw))\n\t\tgoto unlock;\n\tVM_BUG_ON_PAGE(addr != pvmw.address, old_page);\n\n\tif (new_page) {\n\t\tfolio_get(new_folio);\n\t\tpage_add_new_anon_rmap(new_page, vma, addr);\n\t\tfolio_add_lru_vma(new_folio, vma);\n\t} else\n\t\t \n\t\tdec_mm_counter(mm, MM_ANONPAGES);\n\n\tif (!folio_test_anon(old_folio)) {\n\t\tdec_mm_counter(mm, mm_counter_file(old_page));\n\t\tinc_mm_counter(mm, MM_ANONPAGES);\n\t}\n\n\tflush_cache_page(vma, addr, pte_pfn(ptep_get(pvmw.pte)));\n\tptep_clear_flush(vma, addr, pvmw.pte);\n\tif (new_page)\n\t\tset_pte_at_notify(mm, addr, pvmw.pte,\n\t\t\t\t  mk_pte(new_page, vma->vm_page_prot));\n\n\tpage_remove_rmap(old_page, vma, false);\n\tif (!folio_mapped(old_folio))\n\t\tfolio_free_swap(old_folio);\n\tpage_vma_mapped_walk_done(&pvmw);\n\tfolio_put(old_folio);\n\n\terr = 0;\n unlock:\n\tmmu_notifier_invalidate_range_end(&range);\n\tfolio_unlock(old_folio);\n\treturn err;\n}\n\n \nbool __weak is_swbp_insn(uprobe_opcode_t *insn)\n{\n\treturn *insn == UPROBE_SWBP_INSN;\n}\n\n \nbool __weak is_trap_insn(uprobe_opcode_t *insn)\n{\n\treturn is_swbp_insn(insn);\n}\n\nstatic void copy_from_page(struct page *page, unsigned long vaddr, void *dst, int len)\n{\n\tvoid *kaddr = kmap_atomic(page);\n\tmemcpy(dst, kaddr + (vaddr & ~PAGE_MASK), len);\n\tkunmap_atomic(kaddr);\n}\n\nstatic void copy_to_page(struct page *page, unsigned long vaddr, const void *src, int len)\n{\n\tvoid *kaddr = kmap_atomic(page);\n\tmemcpy(kaddr + (vaddr & ~PAGE_MASK), src, len);\n\tkunmap_atomic(kaddr);\n}\n\nstatic int verify_opcode(struct page *page, unsigned long vaddr, uprobe_opcode_t *new_opcode)\n{\n\tuprobe_opcode_t old_opcode;\n\tbool is_swbp;\n\n\t \n\tcopy_from_page(page, vaddr, &old_opcode, UPROBE_SWBP_INSN_SIZE);\n\tis_swbp = is_swbp_insn(&old_opcode);\n\n\tif (is_swbp_insn(new_opcode)) {\n\t\tif (is_swbp)\t\t \n\t\t\treturn 0;\n\t} else {\n\t\tif (!is_swbp)\t\t \n\t\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\nstatic struct delayed_uprobe *\ndelayed_uprobe_check(struct uprobe *uprobe, struct mm_struct *mm)\n{\n\tstruct delayed_uprobe *du;\n\n\tlist_for_each_entry(du, &delayed_uprobe_list, list)\n\t\tif (du->uprobe == uprobe && du->mm == mm)\n\t\t\treturn du;\n\treturn NULL;\n}\n\nstatic int delayed_uprobe_add(struct uprobe *uprobe, struct mm_struct *mm)\n{\n\tstruct delayed_uprobe *du;\n\n\tif (delayed_uprobe_check(uprobe, mm))\n\t\treturn 0;\n\n\tdu  = kzalloc(sizeof(*du), GFP_KERNEL);\n\tif (!du)\n\t\treturn -ENOMEM;\n\n\tdu->uprobe = uprobe;\n\tdu->mm = mm;\n\tlist_add(&du->list, &delayed_uprobe_list);\n\treturn 0;\n}\n\nstatic void delayed_uprobe_delete(struct delayed_uprobe *du)\n{\n\tif (WARN_ON(!du))\n\t\treturn;\n\tlist_del(&du->list);\n\tkfree(du);\n}\n\nstatic void delayed_uprobe_remove(struct uprobe *uprobe, struct mm_struct *mm)\n{\n\tstruct list_head *pos, *q;\n\tstruct delayed_uprobe *du;\n\n\tif (!uprobe && !mm)\n\t\treturn;\n\n\tlist_for_each_safe(pos, q, &delayed_uprobe_list) {\n\t\tdu = list_entry(pos, struct delayed_uprobe, list);\n\n\t\tif (uprobe && du->uprobe != uprobe)\n\t\t\tcontinue;\n\t\tif (mm && du->mm != mm)\n\t\t\tcontinue;\n\n\t\tdelayed_uprobe_delete(du);\n\t}\n}\n\nstatic bool valid_ref_ctr_vma(struct uprobe *uprobe,\n\t\t\t      struct vm_area_struct *vma)\n{\n\tunsigned long vaddr = offset_to_vaddr(vma, uprobe->ref_ctr_offset);\n\n\treturn uprobe->ref_ctr_offset &&\n\t\tvma->vm_file &&\n\t\tfile_inode(vma->vm_file) == uprobe->inode &&\n\t\t(vma->vm_flags & (VM_WRITE|VM_SHARED)) == VM_WRITE &&\n\t\tvma->vm_start <= vaddr &&\n\t\tvma->vm_end > vaddr;\n}\n\nstatic struct vm_area_struct *\nfind_ref_ctr_vma(struct uprobe *uprobe, struct mm_struct *mm)\n{\n\tVMA_ITERATOR(vmi, mm, 0);\n\tstruct vm_area_struct *tmp;\n\n\tfor_each_vma(vmi, tmp)\n\t\tif (valid_ref_ctr_vma(uprobe, tmp))\n\t\t\treturn tmp;\n\n\treturn NULL;\n}\n\nstatic int\n__update_ref_ctr(struct mm_struct *mm, unsigned long vaddr, short d)\n{\n\tvoid *kaddr;\n\tstruct page *page;\n\tint ret;\n\tshort *ptr;\n\n\tif (!vaddr || !d)\n\t\treturn -EINVAL;\n\n\tret = get_user_pages_remote(mm, vaddr, 1,\n\t\t\t\t    FOLL_WRITE, &page, NULL);\n\tif (unlikely(ret <= 0)) {\n\t\t \n\t\treturn ret == 0 ? -EBUSY : ret;\n\t}\n\n\tkaddr = kmap_atomic(page);\n\tptr = kaddr + (vaddr & ~PAGE_MASK);\n\n\tif (unlikely(*ptr + d < 0)) {\n\t\tpr_warn(\"ref_ctr going negative. vaddr: 0x%lx, \"\n\t\t\t\"curr val: %d, delta: %d\\n\", vaddr, *ptr, d);\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t*ptr += d;\n\tret = 0;\nout:\n\tkunmap_atomic(kaddr);\n\tput_page(page);\n\treturn ret;\n}\n\nstatic void update_ref_ctr_warn(struct uprobe *uprobe,\n\t\t\t\tstruct mm_struct *mm, short d)\n{\n\tpr_warn(\"ref_ctr %s failed for inode: 0x%lx offset: \"\n\t\t\"0x%llx ref_ctr_offset: 0x%llx of mm: 0x%pK\\n\",\n\t\td > 0 ? \"increment\" : \"decrement\", uprobe->inode->i_ino,\n\t\t(unsigned long long) uprobe->offset,\n\t\t(unsigned long long) uprobe->ref_ctr_offset, mm);\n}\n\nstatic int update_ref_ctr(struct uprobe *uprobe, struct mm_struct *mm,\n\t\t\t  short d)\n{\n\tstruct vm_area_struct *rc_vma;\n\tunsigned long rc_vaddr;\n\tint ret = 0;\n\n\trc_vma = find_ref_ctr_vma(uprobe, mm);\n\n\tif (rc_vma) {\n\t\trc_vaddr = offset_to_vaddr(rc_vma, uprobe->ref_ctr_offset);\n\t\tret = __update_ref_ctr(mm, rc_vaddr, d);\n\t\tif (ret)\n\t\t\tupdate_ref_ctr_warn(uprobe, mm, d);\n\n\t\tif (d > 0)\n\t\t\treturn ret;\n\t}\n\n\tmutex_lock(&delayed_uprobe_lock);\n\tif (d > 0)\n\t\tret = delayed_uprobe_add(uprobe, mm);\n\telse\n\t\tdelayed_uprobe_remove(uprobe, mm);\n\tmutex_unlock(&delayed_uprobe_lock);\n\n\treturn ret;\n}\n\n \nint uprobe_write_opcode(struct arch_uprobe *auprobe, struct mm_struct *mm,\n\t\t\tunsigned long vaddr, uprobe_opcode_t opcode)\n{\n\tstruct uprobe *uprobe;\n\tstruct page *old_page, *new_page;\n\tstruct vm_area_struct *vma;\n\tint ret, is_register, ref_ctr_updated = 0;\n\tbool orig_page_huge = false;\n\tunsigned int gup_flags = FOLL_FORCE;\n\n\tis_register = is_swbp_insn(&opcode);\n\tuprobe = container_of(auprobe, struct uprobe, arch);\n\nretry:\n\tif (is_register)\n\t\tgup_flags |= FOLL_SPLIT_PMD;\n\t \n\told_page = get_user_page_vma_remote(mm, vaddr, gup_flags, &vma);\n\tif (IS_ERR_OR_NULL(old_page))\n\t\treturn old_page ? PTR_ERR(old_page) : 0;\n\n\tret = verify_opcode(old_page, vaddr, &opcode);\n\tif (ret <= 0)\n\t\tgoto put_old;\n\n\tif (WARN(!is_register && PageCompound(old_page),\n\t\t \"uprobe unregister should never work on compound page\\n\")) {\n\t\tret = -EINVAL;\n\t\tgoto put_old;\n\t}\n\n\t \n\tif (!ref_ctr_updated && uprobe->ref_ctr_offset) {\n\t\tret = update_ref_ctr(uprobe, mm, is_register ? 1 : -1);\n\t\tif (ret)\n\t\t\tgoto put_old;\n\n\t\tref_ctr_updated = 1;\n\t}\n\n\tret = 0;\n\tif (!is_register && !PageAnon(old_page))\n\t\tgoto put_old;\n\n\tret = anon_vma_prepare(vma);\n\tif (ret)\n\t\tgoto put_old;\n\n\tret = -ENOMEM;\n\tnew_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma, vaddr);\n\tif (!new_page)\n\t\tgoto put_old;\n\n\t__SetPageUptodate(new_page);\n\tcopy_highpage(new_page, old_page);\n\tcopy_to_page(new_page, vaddr, &opcode, UPROBE_SWBP_INSN_SIZE);\n\n\tif (!is_register) {\n\t\tstruct page *orig_page;\n\t\tpgoff_t index;\n\n\t\tVM_BUG_ON_PAGE(!PageAnon(old_page), old_page);\n\n\t\tindex = vaddr_to_offset(vma, vaddr & PAGE_MASK) >> PAGE_SHIFT;\n\t\torig_page = find_get_page(vma->vm_file->f_inode->i_mapping,\n\t\t\t\t\t  index);\n\n\t\tif (orig_page) {\n\t\t\tif (PageUptodate(orig_page) &&\n\t\t\t    pages_identical(new_page, orig_page)) {\n\t\t\t\t \n\t\t\t\tput_page(new_page);\n\t\t\t\tnew_page = NULL;\n\n\t\t\t\tif (PageCompound(orig_page))\n\t\t\t\t\torig_page_huge = true;\n\t\t\t}\n\t\t\tput_page(orig_page);\n\t\t}\n\t}\n\n\tret = __replace_page(vma, vaddr, old_page, new_page);\n\tif (new_page)\n\t\tput_page(new_page);\nput_old:\n\tput_page(old_page);\n\n\tif (unlikely(ret == -EAGAIN))\n\t\tgoto retry;\n\n\t \n\tif (ret && is_register && ref_ctr_updated)\n\t\tupdate_ref_ctr(uprobe, mm, -1);\n\n\t \n\tif (!ret && orig_page_huge)\n\t\tcollapse_pte_mapped_thp(mm, vaddr, false);\n\n\treturn ret;\n}\n\n \nint __weak set_swbp(struct arch_uprobe *auprobe, struct mm_struct *mm, unsigned long vaddr)\n{\n\treturn uprobe_write_opcode(auprobe, mm, vaddr, UPROBE_SWBP_INSN);\n}\n\n \nint __weak\nset_orig_insn(struct arch_uprobe *auprobe, struct mm_struct *mm, unsigned long vaddr)\n{\n\treturn uprobe_write_opcode(auprobe, mm, vaddr,\n\t\t\t*(uprobe_opcode_t *)&auprobe->insn);\n}\n\nstatic struct uprobe *get_uprobe(struct uprobe *uprobe)\n{\n\trefcount_inc(&uprobe->ref);\n\treturn uprobe;\n}\n\nstatic void put_uprobe(struct uprobe *uprobe)\n{\n\tif (refcount_dec_and_test(&uprobe->ref)) {\n\t\t \n\t\tmutex_lock(&delayed_uprobe_lock);\n\t\tdelayed_uprobe_remove(uprobe, NULL);\n\t\tmutex_unlock(&delayed_uprobe_lock);\n\t\tkfree(uprobe);\n\t}\n}\n\nstatic __always_inline\nint uprobe_cmp(const struct inode *l_inode, const loff_t l_offset,\n\t       const struct uprobe *r)\n{\n\tif (l_inode < r->inode)\n\t\treturn -1;\n\n\tif (l_inode > r->inode)\n\t\treturn 1;\n\n\tif (l_offset < r->offset)\n\t\treturn -1;\n\n\tif (l_offset > r->offset)\n\t\treturn 1;\n\n\treturn 0;\n}\n\n#define __node_2_uprobe(node) \\\n\trb_entry((node), struct uprobe, rb_node)\n\nstruct __uprobe_key {\n\tstruct inode *inode;\n\tloff_t offset;\n};\n\nstatic inline int __uprobe_cmp_key(const void *key, const struct rb_node *b)\n{\n\tconst struct __uprobe_key *a = key;\n\treturn uprobe_cmp(a->inode, a->offset, __node_2_uprobe(b));\n}\n\nstatic inline int __uprobe_cmp(struct rb_node *a, const struct rb_node *b)\n{\n\tstruct uprobe *u = __node_2_uprobe(a);\n\treturn uprobe_cmp(u->inode, u->offset, __node_2_uprobe(b));\n}\n\nstatic struct uprobe *__find_uprobe(struct inode *inode, loff_t offset)\n{\n\tstruct __uprobe_key key = {\n\t\t.inode = inode,\n\t\t.offset = offset,\n\t};\n\tstruct rb_node *node = rb_find(&key, &uprobes_tree, __uprobe_cmp_key);\n\n\tif (node)\n\t\treturn get_uprobe(__node_2_uprobe(node));\n\n\treturn NULL;\n}\n\n \nstatic struct uprobe *find_uprobe(struct inode *inode, loff_t offset)\n{\n\tstruct uprobe *uprobe;\n\n\tspin_lock(&uprobes_treelock);\n\tuprobe = __find_uprobe(inode, offset);\n\tspin_unlock(&uprobes_treelock);\n\n\treturn uprobe;\n}\n\nstatic struct uprobe *__insert_uprobe(struct uprobe *uprobe)\n{\n\tstruct rb_node *node;\n\n\tnode = rb_find_add(&uprobe->rb_node, &uprobes_tree, __uprobe_cmp);\n\tif (node)\n\t\treturn get_uprobe(__node_2_uprobe(node));\n\n\t \n\trefcount_set(&uprobe->ref, 2);\n\treturn NULL;\n}\n\n \nstatic struct uprobe *insert_uprobe(struct uprobe *uprobe)\n{\n\tstruct uprobe *u;\n\n\tspin_lock(&uprobes_treelock);\n\tu = __insert_uprobe(uprobe);\n\tspin_unlock(&uprobes_treelock);\n\n\treturn u;\n}\n\nstatic void\nref_ctr_mismatch_warn(struct uprobe *cur_uprobe, struct uprobe *uprobe)\n{\n\tpr_warn(\"ref_ctr_offset mismatch. inode: 0x%lx offset: 0x%llx \"\n\t\t\"ref_ctr_offset(old): 0x%llx ref_ctr_offset(new): 0x%llx\\n\",\n\t\tuprobe->inode->i_ino, (unsigned long long) uprobe->offset,\n\t\t(unsigned long long) cur_uprobe->ref_ctr_offset,\n\t\t(unsigned long long) uprobe->ref_ctr_offset);\n}\n\nstatic struct uprobe *alloc_uprobe(struct inode *inode, loff_t offset,\n\t\t\t\t   loff_t ref_ctr_offset)\n{\n\tstruct uprobe *uprobe, *cur_uprobe;\n\n\tuprobe = kzalloc(sizeof(struct uprobe), GFP_KERNEL);\n\tif (!uprobe)\n\t\treturn NULL;\n\n\tuprobe->inode = inode;\n\tuprobe->offset = offset;\n\tuprobe->ref_ctr_offset = ref_ctr_offset;\n\tinit_rwsem(&uprobe->register_rwsem);\n\tinit_rwsem(&uprobe->consumer_rwsem);\n\n\t \n\tcur_uprobe = insert_uprobe(uprobe);\n\t \n\tif (cur_uprobe) {\n\t\tif (cur_uprobe->ref_ctr_offset != uprobe->ref_ctr_offset) {\n\t\t\tref_ctr_mismatch_warn(cur_uprobe, uprobe);\n\t\t\tput_uprobe(cur_uprobe);\n\t\t\tkfree(uprobe);\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\t}\n\t\tkfree(uprobe);\n\t\tuprobe = cur_uprobe;\n\t}\n\n\treturn uprobe;\n}\n\nstatic void consumer_add(struct uprobe *uprobe, struct uprobe_consumer *uc)\n{\n\tdown_write(&uprobe->consumer_rwsem);\n\tuc->next = uprobe->consumers;\n\tuprobe->consumers = uc;\n\tup_write(&uprobe->consumer_rwsem);\n}\n\n \nstatic bool consumer_del(struct uprobe *uprobe, struct uprobe_consumer *uc)\n{\n\tstruct uprobe_consumer **con;\n\tbool ret = false;\n\n\tdown_write(&uprobe->consumer_rwsem);\n\tfor (con = &uprobe->consumers; *con; con = &(*con)->next) {\n\t\tif (*con == uc) {\n\t\t\t*con = uc->next;\n\t\t\tret = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\tup_write(&uprobe->consumer_rwsem);\n\n\treturn ret;\n}\n\nstatic int __copy_insn(struct address_space *mapping, struct file *filp,\n\t\t\tvoid *insn, int nbytes, loff_t offset)\n{\n\tstruct page *page;\n\t \n\tif (mapping->a_ops->read_folio)\n\t\tpage = read_mapping_page(mapping, offset >> PAGE_SHIFT, filp);\n\telse\n\t\tpage = shmem_read_mapping_page(mapping, offset >> PAGE_SHIFT);\n\tif (IS_ERR(page))\n\t\treturn PTR_ERR(page);\n\n\tcopy_from_page(page, offset, insn, nbytes);\n\tput_page(page);\n\n\treturn 0;\n}\n\nstatic int copy_insn(struct uprobe *uprobe, struct file *filp)\n{\n\tstruct address_space *mapping = uprobe->inode->i_mapping;\n\tloff_t offs = uprobe->offset;\n\tvoid *insn = &uprobe->arch.insn;\n\tint size = sizeof(uprobe->arch.insn);\n\tint len, err = -EIO;\n\n\t \n\tdo {\n\t\tif (offs >= i_size_read(uprobe->inode))\n\t\t\tbreak;\n\n\t\tlen = min_t(int, size, PAGE_SIZE - (offs & ~PAGE_MASK));\n\t\terr = __copy_insn(mapping, filp, insn, len, offs);\n\t\tif (err)\n\t\t\tbreak;\n\n\t\tinsn += len;\n\t\toffs += len;\n\t\tsize -= len;\n\t} while (size);\n\n\treturn err;\n}\n\nstatic int prepare_uprobe(struct uprobe *uprobe, struct file *file,\n\t\t\t\tstruct mm_struct *mm, unsigned long vaddr)\n{\n\tint ret = 0;\n\n\tif (test_bit(UPROBE_COPY_INSN, &uprobe->flags))\n\t\treturn ret;\n\n\t \n\tdown_write(&uprobe->consumer_rwsem);\n\tif (test_bit(UPROBE_COPY_INSN, &uprobe->flags))\n\t\tgoto out;\n\n\tret = copy_insn(uprobe, file);\n\tif (ret)\n\t\tgoto out;\n\n\tret = -ENOTSUPP;\n\tif (is_trap_insn((uprobe_opcode_t *)&uprobe->arch.insn))\n\t\tgoto out;\n\n\tret = arch_uprobe_analyze_insn(&uprobe->arch, mm, vaddr);\n\tif (ret)\n\t\tgoto out;\n\n\tsmp_wmb();  \n\tset_bit(UPROBE_COPY_INSN, &uprobe->flags);\n\n out:\n\tup_write(&uprobe->consumer_rwsem);\n\n\treturn ret;\n}\n\nstatic inline bool consumer_filter(struct uprobe_consumer *uc,\n\t\t\t\t   enum uprobe_filter_ctx ctx, struct mm_struct *mm)\n{\n\treturn !uc->filter || uc->filter(uc, ctx, mm);\n}\n\nstatic bool filter_chain(struct uprobe *uprobe,\n\t\t\t enum uprobe_filter_ctx ctx, struct mm_struct *mm)\n{\n\tstruct uprobe_consumer *uc;\n\tbool ret = false;\n\n\tdown_read(&uprobe->consumer_rwsem);\n\tfor (uc = uprobe->consumers; uc; uc = uc->next) {\n\t\tret = consumer_filter(uc, ctx, mm);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\tup_read(&uprobe->consumer_rwsem);\n\n\treturn ret;\n}\n\nstatic int\ninstall_breakpoint(struct uprobe *uprobe, struct mm_struct *mm,\n\t\t\tstruct vm_area_struct *vma, unsigned long vaddr)\n{\n\tbool first_uprobe;\n\tint ret;\n\n\tret = prepare_uprobe(uprobe, vma->vm_file, mm, vaddr);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tfirst_uprobe = !test_bit(MMF_HAS_UPROBES, &mm->flags);\n\tif (first_uprobe)\n\t\tset_bit(MMF_HAS_UPROBES, &mm->flags);\n\n\tret = set_swbp(&uprobe->arch, mm, vaddr);\n\tif (!ret)\n\t\tclear_bit(MMF_RECALC_UPROBES, &mm->flags);\n\telse if (first_uprobe)\n\t\tclear_bit(MMF_HAS_UPROBES, &mm->flags);\n\n\treturn ret;\n}\n\nstatic int\nremove_breakpoint(struct uprobe *uprobe, struct mm_struct *mm, unsigned long vaddr)\n{\n\tset_bit(MMF_RECALC_UPROBES, &mm->flags);\n\treturn set_orig_insn(&uprobe->arch, mm, vaddr);\n}\n\nstatic inline bool uprobe_is_active(struct uprobe *uprobe)\n{\n\treturn !RB_EMPTY_NODE(&uprobe->rb_node);\n}\n \nstatic void delete_uprobe(struct uprobe *uprobe)\n{\n\tif (WARN_ON(!uprobe_is_active(uprobe)))\n\t\treturn;\n\n\tspin_lock(&uprobes_treelock);\n\trb_erase(&uprobe->rb_node, &uprobes_tree);\n\tspin_unlock(&uprobes_treelock);\n\tRB_CLEAR_NODE(&uprobe->rb_node);  \n\tput_uprobe(uprobe);\n}\n\nstruct map_info {\n\tstruct map_info *next;\n\tstruct mm_struct *mm;\n\tunsigned long vaddr;\n};\n\nstatic inline struct map_info *free_map_info(struct map_info *info)\n{\n\tstruct map_info *next = info->next;\n\tkfree(info);\n\treturn next;\n}\n\nstatic struct map_info *\nbuild_map_info(struct address_space *mapping, loff_t offset, bool is_register)\n{\n\tunsigned long pgoff = offset >> PAGE_SHIFT;\n\tstruct vm_area_struct *vma;\n\tstruct map_info *curr = NULL;\n\tstruct map_info *prev = NULL;\n\tstruct map_info *info;\n\tint more = 0;\n\n again:\n\ti_mmap_lock_read(mapping);\n\tvma_interval_tree_foreach(vma, &mapping->i_mmap, pgoff, pgoff) {\n\t\tif (!valid_vma(vma, is_register))\n\t\t\tcontinue;\n\n\t\tif (!prev && !more) {\n\t\t\t \n\t\t\tprev = kmalloc(sizeof(struct map_info),\n\t\t\t\t\tGFP_NOWAIT | __GFP_NOMEMALLOC | __GFP_NOWARN);\n\t\t\tif (prev)\n\t\t\t\tprev->next = NULL;\n\t\t}\n\t\tif (!prev) {\n\t\t\tmore++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!mmget_not_zero(vma->vm_mm))\n\t\t\tcontinue;\n\n\t\tinfo = prev;\n\t\tprev = prev->next;\n\t\tinfo->next = curr;\n\t\tcurr = info;\n\n\t\tinfo->mm = vma->vm_mm;\n\t\tinfo->vaddr = offset_to_vaddr(vma, offset);\n\t}\n\ti_mmap_unlock_read(mapping);\n\n\tif (!more)\n\t\tgoto out;\n\n\tprev = curr;\n\twhile (curr) {\n\t\tmmput(curr->mm);\n\t\tcurr = curr->next;\n\t}\n\n\tdo {\n\t\tinfo = kmalloc(sizeof(struct map_info), GFP_KERNEL);\n\t\tif (!info) {\n\t\t\tcurr = ERR_PTR(-ENOMEM);\n\t\t\tgoto out;\n\t\t}\n\t\tinfo->next = prev;\n\t\tprev = info;\n\t} while (--more);\n\n\tgoto again;\n out:\n\twhile (prev)\n\t\tprev = free_map_info(prev);\n\treturn curr;\n}\n\nstatic int\nregister_for_each_vma(struct uprobe *uprobe, struct uprobe_consumer *new)\n{\n\tbool is_register = !!new;\n\tstruct map_info *info;\n\tint err = 0;\n\n\tpercpu_down_write(&dup_mmap_sem);\n\tinfo = build_map_info(uprobe->inode->i_mapping,\n\t\t\t\t\tuprobe->offset, is_register);\n\tif (IS_ERR(info)) {\n\t\terr = PTR_ERR(info);\n\t\tgoto out;\n\t}\n\n\twhile (info) {\n\t\tstruct mm_struct *mm = info->mm;\n\t\tstruct vm_area_struct *vma;\n\n\t\tif (err && is_register)\n\t\t\tgoto free;\n\n\t\tmmap_write_lock(mm);\n\t\tvma = find_vma(mm, info->vaddr);\n\t\tif (!vma || !valid_vma(vma, is_register) ||\n\t\t    file_inode(vma->vm_file) != uprobe->inode)\n\t\t\tgoto unlock;\n\n\t\tif (vma->vm_start > info->vaddr ||\n\t\t    vaddr_to_offset(vma, info->vaddr) != uprobe->offset)\n\t\t\tgoto unlock;\n\n\t\tif (is_register) {\n\t\t\t \n\t\t\tif (consumer_filter(new,\n\t\t\t\t\tUPROBE_FILTER_REGISTER, mm))\n\t\t\t\terr = install_breakpoint(uprobe, mm, vma, info->vaddr);\n\t\t} else if (test_bit(MMF_HAS_UPROBES, &mm->flags)) {\n\t\t\tif (!filter_chain(uprobe,\n\t\t\t\t\tUPROBE_FILTER_UNREGISTER, mm))\n\t\t\t\terr |= remove_breakpoint(uprobe, mm, info->vaddr);\n\t\t}\n\n unlock:\n\t\tmmap_write_unlock(mm);\n free:\n\t\tmmput(mm);\n\t\tinfo = free_map_info(info);\n\t}\n out:\n\tpercpu_up_write(&dup_mmap_sem);\n\treturn err;\n}\n\nstatic void\n__uprobe_unregister(struct uprobe *uprobe, struct uprobe_consumer *uc)\n{\n\tint err;\n\n\tif (WARN_ON(!consumer_del(uprobe, uc)))\n\t\treturn;\n\n\terr = register_for_each_vma(uprobe, NULL);\n\t \n\tif (!uprobe->consumers && !err)\n\t\tdelete_uprobe(uprobe);\n}\n\n \nvoid uprobe_unregister(struct inode *inode, loff_t offset, struct uprobe_consumer *uc)\n{\n\tstruct uprobe *uprobe;\n\n\tuprobe = find_uprobe(inode, offset);\n\tif (WARN_ON(!uprobe))\n\t\treturn;\n\n\tdown_write(&uprobe->register_rwsem);\n\t__uprobe_unregister(uprobe, uc);\n\tup_write(&uprobe->register_rwsem);\n\tput_uprobe(uprobe);\n}\nEXPORT_SYMBOL_GPL(uprobe_unregister);\n\n \nstatic int __uprobe_register(struct inode *inode, loff_t offset,\n\t\t\t     loff_t ref_ctr_offset, struct uprobe_consumer *uc)\n{\n\tstruct uprobe *uprobe;\n\tint ret;\n\n\t \n\tif (!uc->handler && !uc->ret_handler)\n\t\treturn -EINVAL;\n\n\t \n\tif (!inode->i_mapping->a_ops->read_folio &&\n\t    !shmem_mapping(inode->i_mapping))\n\t\treturn -EIO;\n\t \n\tif (offset > i_size_read(inode))\n\t\treturn -EINVAL;\n\n\t \n\tif (!IS_ALIGNED(offset, UPROBE_SWBP_INSN_SIZE))\n\t\treturn -EINVAL;\n\tif (!IS_ALIGNED(ref_ctr_offset, sizeof(short)))\n\t\treturn -EINVAL;\n\n retry:\n\tuprobe = alloc_uprobe(inode, offset, ref_ctr_offset);\n\tif (!uprobe)\n\t\treturn -ENOMEM;\n\tif (IS_ERR(uprobe))\n\t\treturn PTR_ERR(uprobe);\n\n\t \n\tdown_write(&uprobe->register_rwsem);\n\tret = -EAGAIN;\n\tif (likely(uprobe_is_active(uprobe))) {\n\t\tconsumer_add(uprobe, uc);\n\t\tret = register_for_each_vma(uprobe, uc);\n\t\tif (ret)\n\t\t\t__uprobe_unregister(uprobe, uc);\n\t}\n\tup_write(&uprobe->register_rwsem);\n\tput_uprobe(uprobe);\n\n\tif (unlikely(ret == -EAGAIN))\n\t\tgoto retry;\n\treturn ret;\n}\n\nint uprobe_register(struct inode *inode, loff_t offset,\n\t\t    struct uprobe_consumer *uc)\n{\n\treturn __uprobe_register(inode, offset, 0, uc);\n}\nEXPORT_SYMBOL_GPL(uprobe_register);\n\nint uprobe_register_refctr(struct inode *inode, loff_t offset,\n\t\t\t   loff_t ref_ctr_offset, struct uprobe_consumer *uc)\n{\n\treturn __uprobe_register(inode, offset, ref_ctr_offset, uc);\n}\nEXPORT_SYMBOL_GPL(uprobe_register_refctr);\n\n \nint uprobe_apply(struct inode *inode, loff_t offset,\n\t\t\tstruct uprobe_consumer *uc, bool add)\n{\n\tstruct uprobe *uprobe;\n\tstruct uprobe_consumer *con;\n\tint ret = -ENOENT;\n\n\tuprobe = find_uprobe(inode, offset);\n\tif (WARN_ON(!uprobe))\n\t\treturn ret;\n\n\tdown_write(&uprobe->register_rwsem);\n\tfor (con = uprobe->consumers; con && con != uc ; con = con->next)\n\t\t;\n\tif (con)\n\t\tret = register_for_each_vma(uprobe, add ? uc : NULL);\n\tup_write(&uprobe->register_rwsem);\n\tput_uprobe(uprobe);\n\n\treturn ret;\n}\n\nstatic int unapply_uprobe(struct uprobe *uprobe, struct mm_struct *mm)\n{\n\tVMA_ITERATOR(vmi, mm, 0);\n\tstruct vm_area_struct *vma;\n\tint err = 0;\n\n\tmmap_read_lock(mm);\n\tfor_each_vma(vmi, vma) {\n\t\tunsigned long vaddr;\n\t\tloff_t offset;\n\n\t\tif (!valid_vma(vma, false) ||\n\t\t    file_inode(vma->vm_file) != uprobe->inode)\n\t\t\tcontinue;\n\n\t\toffset = (loff_t)vma->vm_pgoff << PAGE_SHIFT;\n\t\tif (uprobe->offset <  offset ||\n\t\t    uprobe->offset >= offset + vma->vm_end - vma->vm_start)\n\t\t\tcontinue;\n\n\t\tvaddr = offset_to_vaddr(vma, uprobe->offset);\n\t\terr |= remove_breakpoint(uprobe, mm, vaddr);\n\t}\n\tmmap_read_unlock(mm);\n\n\treturn err;\n}\n\nstatic struct rb_node *\nfind_node_in_range(struct inode *inode, loff_t min, loff_t max)\n{\n\tstruct rb_node *n = uprobes_tree.rb_node;\n\n\twhile (n) {\n\t\tstruct uprobe *u = rb_entry(n, struct uprobe, rb_node);\n\n\t\tif (inode < u->inode) {\n\t\t\tn = n->rb_left;\n\t\t} else if (inode > u->inode) {\n\t\t\tn = n->rb_right;\n\t\t} else {\n\t\t\tif (max < u->offset)\n\t\t\t\tn = n->rb_left;\n\t\t\telse if (min > u->offset)\n\t\t\t\tn = n->rb_right;\n\t\t\telse\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn n;\n}\n\n \nstatic void build_probe_list(struct inode *inode,\n\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\tunsigned long start, unsigned long end,\n\t\t\t\tstruct list_head *head)\n{\n\tloff_t min, max;\n\tstruct rb_node *n, *t;\n\tstruct uprobe *u;\n\n\tINIT_LIST_HEAD(head);\n\tmin = vaddr_to_offset(vma, start);\n\tmax = min + (end - start) - 1;\n\n\tspin_lock(&uprobes_treelock);\n\tn = find_node_in_range(inode, min, max);\n\tif (n) {\n\t\tfor (t = n; t; t = rb_prev(t)) {\n\t\t\tu = rb_entry(t, struct uprobe, rb_node);\n\t\t\tif (u->inode != inode || u->offset < min)\n\t\t\t\tbreak;\n\t\t\tlist_add(&u->pending_list, head);\n\t\t\tget_uprobe(u);\n\t\t}\n\t\tfor (t = n; (t = rb_next(t)); ) {\n\t\t\tu = rb_entry(t, struct uprobe, rb_node);\n\t\t\tif (u->inode != inode || u->offset > max)\n\t\t\t\tbreak;\n\t\t\tlist_add(&u->pending_list, head);\n\t\t\tget_uprobe(u);\n\t\t}\n\t}\n\tspin_unlock(&uprobes_treelock);\n}\n\n \nstatic int delayed_ref_ctr_inc(struct vm_area_struct *vma)\n{\n\tstruct list_head *pos, *q;\n\tstruct delayed_uprobe *du;\n\tunsigned long vaddr;\n\tint ret = 0, err = 0;\n\n\tmutex_lock(&delayed_uprobe_lock);\n\tlist_for_each_safe(pos, q, &delayed_uprobe_list) {\n\t\tdu = list_entry(pos, struct delayed_uprobe, list);\n\n\t\tif (du->mm != vma->vm_mm ||\n\t\t    !valid_ref_ctr_vma(du->uprobe, vma))\n\t\t\tcontinue;\n\n\t\tvaddr = offset_to_vaddr(vma, du->uprobe->ref_ctr_offset);\n\t\tret = __update_ref_ctr(vma->vm_mm, vaddr, 1);\n\t\tif (ret) {\n\t\t\tupdate_ref_ctr_warn(du->uprobe, vma->vm_mm, 1);\n\t\t\tif (!err)\n\t\t\t\terr = ret;\n\t\t}\n\t\tdelayed_uprobe_delete(du);\n\t}\n\tmutex_unlock(&delayed_uprobe_lock);\n\treturn err;\n}\n\n \nint uprobe_mmap(struct vm_area_struct *vma)\n{\n\tstruct list_head tmp_list;\n\tstruct uprobe *uprobe, *u;\n\tstruct inode *inode;\n\n\tif (no_uprobe_events())\n\t\treturn 0;\n\n\tif (vma->vm_file &&\n\t    (vma->vm_flags & (VM_WRITE|VM_SHARED)) == VM_WRITE &&\n\t    test_bit(MMF_HAS_UPROBES, &vma->vm_mm->flags))\n\t\tdelayed_ref_ctr_inc(vma);\n\n\tif (!valid_vma(vma, true))\n\t\treturn 0;\n\n\tinode = file_inode(vma->vm_file);\n\tif (!inode)\n\t\treturn 0;\n\n\tmutex_lock(uprobes_mmap_hash(inode));\n\tbuild_probe_list(inode, vma, vma->vm_start, vma->vm_end, &tmp_list);\n\t \n\tlist_for_each_entry_safe(uprobe, u, &tmp_list, pending_list) {\n\t\tif (!fatal_signal_pending(current) &&\n\t\t    filter_chain(uprobe, UPROBE_FILTER_MMAP, vma->vm_mm)) {\n\t\t\tunsigned long vaddr = offset_to_vaddr(vma, uprobe->offset);\n\t\t\tinstall_breakpoint(uprobe, vma->vm_mm, vma, vaddr);\n\t\t}\n\t\tput_uprobe(uprobe);\n\t}\n\tmutex_unlock(uprobes_mmap_hash(inode));\n\n\treturn 0;\n}\n\nstatic bool\nvma_has_uprobes(struct vm_area_struct *vma, unsigned long start, unsigned long end)\n{\n\tloff_t min, max;\n\tstruct inode *inode;\n\tstruct rb_node *n;\n\n\tinode = file_inode(vma->vm_file);\n\n\tmin = vaddr_to_offset(vma, start);\n\tmax = min + (end - start) - 1;\n\n\tspin_lock(&uprobes_treelock);\n\tn = find_node_in_range(inode, min, max);\n\tspin_unlock(&uprobes_treelock);\n\n\treturn !!n;\n}\n\n \nvoid uprobe_munmap(struct vm_area_struct *vma, unsigned long start, unsigned long end)\n{\n\tif (no_uprobe_events() || !valid_vma(vma, false))\n\t\treturn;\n\n\tif (!atomic_read(&vma->vm_mm->mm_users))  \n\t\treturn;\n\n\tif (!test_bit(MMF_HAS_UPROBES, &vma->vm_mm->flags) ||\n\t     test_bit(MMF_RECALC_UPROBES, &vma->vm_mm->flags))\n\t\treturn;\n\n\tif (vma_has_uprobes(vma, start, end))\n\t\tset_bit(MMF_RECALC_UPROBES, &vma->vm_mm->flags);\n}\n\n \nstatic int xol_add_vma(struct mm_struct *mm, struct xol_area *area)\n{\n\tstruct vm_area_struct *vma;\n\tint ret;\n\n\tif (mmap_write_lock_killable(mm))\n\t\treturn -EINTR;\n\n\tif (mm->uprobes_state.xol_area) {\n\t\tret = -EALREADY;\n\t\tgoto fail;\n\t}\n\n\tif (!area->vaddr) {\n\t\t \n\t\tarea->vaddr = get_unmapped_area(NULL, TASK_SIZE - PAGE_SIZE,\n\t\t\t\t\t\tPAGE_SIZE, 0, 0);\n\t\tif (IS_ERR_VALUE(area->vaddr)) {\n\t\t\tret = area->vaddr;\n\t\t\tgoto fail;\n\t\t}\n\t}\n\n\tvma = _install_special_mapping(mm, area->vaddr, PAGE_SIZE,\n\t\t\t\tVM_EXEC|VM_MAYEXEC|VM_DONTCOPY|VM_IO,\n\t\t\t\t&area->xol_mapping);\n\tif (IS_ERR(vma)) {\n\t\tret = PTR_ERR(vma);\n\t\tgoto fail;\n\t}\n\n\tret = 0;\n\t \n\tsmp_store_release(&mm->uprobes_state.xol_area, area);  \n fail:\n\tmmap_write_unlock(mm);\n\n\treturn ret;\n}\n\nstatic struct xol_area *__create_xol_area(unsigned long vaddr)\n{\n\tstruct mm_struct *mm = current->mm;\n\tuprobe_opcode_t insn = UPROBE_SWBP_INSN;\n\tstruct xol_area *area;\n\n\tarea = kmalloc(sizeof(*area), GFP_KERNEL);\n\tif (unlikely(!area))\n\t\tgoto out;\n\n\tarea->bitmap = kcalloc(BITS_TO_LONGS(UINSNS_PER_PAGE), sizeof(long),\n\t\t\t       GFP_KERNEL);\n\tif (!area->bitmap)\n\t\tgoto free_area;\n\n\tarea->xol_mapping.name = \"[uprobes]\";\n\tarea->xol_mapping.fault = NULL;\n\tarea->xol_mapping.pages = area->pages;\n\tarea->pages[0] = alloc_page(GFP_HIGHUSER);\n\tif (!area->pages[0])\n\t\tgoto free_bitmap;\n\tarea->pages[1] = NULL;\n\n\tarea->vaddr = vaddr;\n\tinit_waitqueue_head(&area->wq);\n\t \n\tset_bit(0, area->bitmap);\n\tatomic_set(&area->slot_count, 1);\n\tarch_uprobe_copy_ixol(area->pages[0], 0, &insn, UPROBE_SWBP_INSN_SIZE);\n\n\tif (!xol_add_vma(mm, area))\n\t\treturn area;\n\n\t__free_page(area->pages[0]);\n free_bitmap:\n\tkfree(area->bitmap);\n free_area:\n\tkfree(area);\n out:\n\treturn NULL;\n}\n\n \nstatic struct xol_area *get_xol_area(void)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct xol_area *area;\n\n\tif (!mm->uprobes_state.xol_area)\n\t\t__create_xol_area(0);\n\n\t \n\tarea = READ_ONCE(mm->uprobes_state.xol_area);  \n\treturn area;\n}\n\n \nvoid uprobe_clear_state(struct mm_struct *mm)\n{\n\tstruct xol_area *area = mm->uprobes_state.xol_area;\n\n\tmutex_lock(&delayed_uprobe_lock);\n\tdelayed_uprobe_remove(NULL, mm);\n\tmutex_unlock(&delayed_uprobe_lock);\n\n\tif (!area)\n\t\treturn;\n\n\tput_page(area->pages[0]);\n\tkfree(area->bitmap);\n\tkfree(area);\n}\n\nvoid uprobe_start_dup_mmap(void)\n{\n\tpercpu_down_read(&dup_mmap_sem);\n}\n\nvoid uprobe_end_dup_mmap(void)\n{\n\tpercpu_up_read(&dup_mmap_sem);\n}\n\nvoid uprobe_dup_mmap(struct mm_struct *oldmm, struct mm_struct *newmm)\n{\n\tif (test_bit(MMF_HAS_UPROBES, &oldmm->flags)) {\n\t\tset_bit(MMF_HAS_UPROBES, &newmm->flags);\n\t\t \n\t\tset_bit(MMF_RECALC_UPROBES, &newmm->flags);\n\t}\n}\n\n \nstatic unsigned long xol_take_insn_slot(struct xol_area *area)\n{\n\tunsigned long slot_addr;\n\tint slot_nr;\n\n\tdo {\n\t\tslot_nr = find_first_zero_bit(area->bitmap, UINSNS_PER_PAGE);\n\t\tif (slot_nr < UINSNS_PER_PAGE) {\n\t\t\tif (!test_and_set_bit(slot_nr, area->bitmap))\n\t\t\t\tbreak;\n\n\t\t\tslot_nr = UINSNS_PER_PAGE;\n\t\t\tcontinue;\n\t\t}\n\t\twait_event(area->wq, (atomic_read(&area->slot_count) < UINSNS_PER_PAGE));\n\t} while (slot_nr >= UINSNS_PER_PAGE);\n\n\tslot_addr = area->vaddr + (slot_nr * UPROBE_XOL_SLOT_BYTES);\n\tatomic_inc(&area->slot_count);\n\n\treturn slot_addr;\n}\n\n \nstatic unsigned long xol_get_insn_slot(struct uprobe *uprobe)\n{\n\tstruct xol_area *area;\n\tunsigned long xol_vaddr;\n\n\tarea = get_xol_area();\n\tif (!area)\n\t\treturn 0;\n\n\txol_vaddr = xol_take_insn_slot(area);\n\tif (unlikely(!xol_vaddr))\n\t\treturn 0;\n\n\tarch_uprobe_copy_ixol(area->pages[0], xol_vaddr,\n\t\t\t      &uprobe->arch.ixol, sizeof(uprobe->arch.ixol));\n\n\treturn xol_vaddr;\n}\n\n \nstatic void xol_free_insn_slot(struct task_struct *tsk)\n{\n\tstruct xol_area *area;\n\tunsigned long vma_end;\n\tunsigned long slot_addr;\n\n\tif (!tsk->mm || !tsk->mm->uprobes_state.xol_area || !tsk->utask)\n\t\treturn;\n\n\tslot_addr = tsk->utask->xol_vaddr;\n\tif (unlikely(!slot_addr))\n\t\treturn;\n\n\tarea = tsk->mm->uprobes_state.xol_area;\n\tvma_end = area->vaddr + PAGE_SIZE;\n\tif (area->vaddr <= slot_addr && slot_addr < vma_end) {\n\t\tunsigned long offset;\n\t\tint slot_nr;\n\n\t\toffset = slot_addr - area->vaddr;\n\t\tslot_nr = offset / UPROBE_XOL_SLOT_BYTES;\n\t\tif (slot_nr >= UINSNS_PER_PAGE)\n\t\t\treturn;\n\n\t\tclear_bit(slot_nr, area->bitmap);\n\t\tatomic_dec(&area->slot_count);\n\t\tsmp_mb__after_atomic();  \n\t\tif (waitqueue_active(&area->wq))\n\t\t\twake_up(&area->wq);\n\n\t\ttsk->utask->xol_vaddr = 0;\n\t}\n}\n\nvoid __weak arch_uprobe_copy_ixol(struct page *page, unsigned long vaddr,\n\t\t\t\t  void *src, unsigned long len)\n{\n\t \n\tcopy_to_page(page, vaddr, src, len);\n\n\t \n\tflush_dcache_page(page);\n}\n\n \nunsigned long __weak uprobe_get_swbp_addr(struct pt_regs *regs)\n{\n\treturn instruction_pointer(regs) - UPROBE_SWBP_INSN_SIZE;\n}\n\nunsigned long uprobe_get_trap_addr(struct pt_regs *regs)\n{\n\tstruct uprobe_task *utask = current->utask;\n\n\tif (unlikely(utask && utask->active_uprobe))\n\t\treturn utask->vaddr;\n\n\treturn instruction_pointer(regs);\n}\n\nstatic struct return_instance *free_ret_instance(struct return_instance *ri)\n{\n\tstruct return_instance *next = ri->next;\n\tput_uprobe(ri->uprobe);\n\tkfree(ri);\n\treturn next;\n}\n\n \nvoid uprobe_free_utask(struct task_struct *t)\n{\n\tstruct uprobe_task *utask = t->utask;\n\tstruct return_instance *ri;\n\n\tif (!utask)\n\t\treturn;\n\n\tif (utask->active_uprobe)\n\t\tput_uprobe(utask->active_uprobe);\n\n\tri = utask->return_instances;\n\twhile (ri)\n\t\tri = free_ret_instance(ri);\n\n\txol_free_insn_slot(t);\n\tkfree(utask);\n\tt->utask = NULL;\n}\n\n \nstatic struct uprobe_task *get_utask(void)\n{\n\tif (!current->utask)\n\t\tcurrent->utask = kzalloc(sizeof(struct uprobe_task), GFP_KERNEL);\n\treturn current->utask;\n}\n\nstatic int dup_utask(struct task_struct *t, struct uprobe_task *o_utask)\n{\n\tstruct uprobe_task *n_utask;\n\tstruct return_instance **p, *o, *n;\n\n\tn_utask = kzalloc(sizeof(struct uprobe_task), GFP_KERNEL);\n\tif (!n_utask)\n\t\treturn -ENOMEM;\n\tt->utask = n_utask;\n\n\tp = &n_utask->return_instances;\n\tfor (o = o_utask->return_instances; o; o = o->next) {\n\t\tn = kmalloc(sizeof(struct return_instance), GFP_KERNEL);\n\t\tif (!n)\n\t\t\treturn -ENOMEM;\n\n\t\t*n = *o;\n\t\tget_uprobe(n->uprobe);\n\t\tn->next = NULL;\n\n\t\t*p = n;\n\t\tp = &n->next;\n\t\tn_utask->depth++;\n\t}\n\n\treturn 0;\n}\n\nstatic void uprobe_warn(struct task_struct *t, const char *msg)\n{\n\tpr_warn(\"uprobe: %s:%d failed to %s\\n\",\n\t\t\tcurrent->comm, current->pid, msg);\n}\n\nstatic void dup_xol_work(struct callback_head *work)\n{\n\tif (current->flags & PF_EXITING)\n\t\treturn;\n\n\tif (!__create_xol_area(current->utask->dup_xol_addr) &&\n\t\t\t!fatal_signal_pending(current))\n\t\tuprobe_warn(current, \"dup xol area\");\n}\n\n \nvoid uprobe_copy_process(struct task_struct *t, unsigned long flags)\n{\n\tstruct uprobe_task *utask = current->utask;\n\tstruct mm_struct *mm = current->mm;\n\tstruct xol_area *area;\n\n\tt->utask = NULL;\n\n\tif (!utask || !utask->return_instances)\n\t\treturn;\n\n\tif (mm == t->mm && !(flags & CLONE_VFORK))\n\t\treturn;\n\n\tif (dup_utask(t, utask))\n\t\treturn uprobe_warn(t, \"dup ret instances\");\n\n\t \n\tarea = mm->uprobes_state.xol_area;\n\tif (!area)\n\t\treturn uprobe_warn(t, \"dup xol area\");\n\n\tif (mm == t->mm)\n\t\treturn;\n\n\tt->utask->dup_xol_addr = area->vaddr;\n\tinit_task_work(&t->utask->dup_xol_work, dup_xol_work);\n\ttask_work_add(t, &t->utask->dup_xol_work, TWA_RESUME);\n}\n\n \nstatic unsigned long get_trampoline_vaddr(void)\n{\n\tstruct xol_area *area;\n\tunsigned long trampoline_vaddr = -1;\n\n\t \n\tarea = READ_ONCE(current->mm->uprobes_state.xol_area);  \n\tif (area)\n\t\ttrampoline_vaddr = area->vaddr;\n\n\treturn trampoline_vaddr;\n}\n\nstatic void cleanup_return_instances(struct uprobe_task *utask, bool chained,\n\t\t\t\t\tstruct pt_regs *regs)\n{\n\tstruct return_instance *ri = utask->return_instances;\n\tenum rp_check ctx = chained ? RP_CHECK_CHAIN_CALL : RP_CHECK_CALL;\n\n\twhile (ri && !arch_uretprobe_is_alive(ri, ctx, regs)) {\n\t\tri = free_ret_instance(ri);\n\t\tutask->depth--;\n\t}\n\tutask->return_instances = ri;\n}\n\nstatic void prepare_uretprobe(struct uprobe *uprobe, struct pt_regs *regs)\n{\n\tstruct return_instance *ri;\n\tstruct uprobe_task *utask;\n\tunsigned long orig_ret_vaddr, trampoline_vaddr;\n\tbool chained;\n\n\tif (!get_xol_area())\n\t\treturn;\n\n\tutask = get_utask();\n\tif (!utask)\n\t\treturn;\n\n\tif (utask->depth >= MAX_URETPROBE_DEPTH) {\n\t\tprintk_ratelimited(KERN_INFO \"uprobe: omit uretprobe due to\"\n\t\t\t\t\" nestedness limit pid/tgid=%d/%d\\n\",\n\t\t\t\tcurrent->pid, current->tgid);\n\t\treturn;\n\t}\n\n\tri = kmalloc(sizeof(struct return_instance), GFP_KERNEL);\n\tif (!ri)\n\t\treturn;\n\n\ttrampoline_vaddr = get_trampoline_vaddr();\n\torig_ret_vaddr = arch_uretprobe_hijack_return_addr(trampoline_vaddr, regs);\n\tif (orig_ret_vaddr == -1)\n\t\tgoto fail;\n\n\t \n\tchained = (orig_ret_vaddr == trampoline_vaddr);\n\tcleanup_return_instances(utask, chained, regs);\n\n\t \n\tif (chained) {\n\t\tif (!utask->return_instances) {\n\t\t\t \n\t\t\tuprobe_warn(current, \"handle tail call\");\n\t\t\tgoto fail;\n\t\t}\n\t\torig_ret_vaddr = utask->return_instances->orig_ret_vaddr;\n\t}\n\n\tri->uprobe = get_uprobe(uprobe);\n\tri->func = instruction_pointer(regs);\n\tri->stack = user_stack_pointer(regs);\n\tri->orig_ret_vaddr = orig_ret_vaddr;\n\tri->chained = chained;\n\n\tutask->depth++;\n\tri->next = utask->return_instances;\n\tutask->return_instances = ri;\n\n\treturn;\n fail:\n\tkfree(ri);\n}\n\n \nstatic int\npre_ssout(struct uprobe *uprobe, struct pt_regs *regs, unsigned long bp_vaddr)\n{\n\tstruct uprobe_task *utask;\n\tunsigned long xol_vaddr;\n\tint err;\n\n\tutask = get_utask();\n\tif (!utask)\n\t\treturn -ENOMEM;\n\n\txol_vaddr = xol_get_insn_slot(uprobe);\n\tif (!xol_vaddr)\n\t\treturn -ENOMEM;\n\n\tutask->xol_vaddr = xol_vaddr;\n\tutask->vaddr = bp_vaddr;\n\n\terr = arch_uprobe_pre_xol(&uprobe->arch, regs);\n\tif (unlikely(err)) {\n\t\txol_free_insn_slot(current);\n\t\treturn err;\n\t}\n\n\tutask->active_uprobe = uprobe;\n\tutask->state = UTASK_SSTEP;\n\treturn 0;\n}\n\n \nbool uprobe_deny_signal(void)\n{\n\tstruct task_struct *t = current;\n\tstruct uprobe_task *utask = t->utask;\n\n\tif (likely(!utask || !utask->active_uprobe))\n\t\treturn false;\n\n\tWARN_ON_ONCE(utask->state != UTASK_SSTEP);\n\n\tif (task_sigpending(t)) {\n\t\tspin_lock_irq(&t->sighand->siglock);\n\t\tclear_tsk_thread_flag(t, TIF_SIGPENDING);\n\t\tspin_unlock_irq(&t->sighand->siglock);\n\n\t\tif (__fatal_signal_pending(t) || arch_uprobe_xol_was_trapped(t)) {\n\t\t\tutask->state = UTASK_SSTEP_TRAPPED;\n\t\t\tset_tsk_thread_flag(t, TIF_UPROBE);\n\t\t}\n\t}\n\n\treturn true;\n}\n\nstatic void mmf_recalc_uprobes(struct mm_struct *mm)\n{\n\tVMA_ITERATOR(vmi, mm, 0);\n\tstruct vm_area_struct *vma;\n\n\tfor_each_vma(vmi, vma) {\n\t\tif (!valid_vma(vma, false))\n\t\t\tcontinue;\n\t\t \n\t\tif (vma_has_uprobes(vma, vma->vm_start, vma->vm_end))\n\t\t\treturn;\n\t}\n\n\tclear_bit(MMF_HAS_UPROBES, &mm->flags);\n}\n\nstatic int is_trap_at_addr(struct mm_struct *mm, unsigned long vaddr)\n{\n\tstruct page *page;\n\tuprobe_opcode_t opcode;\n\tint result;\n\n\tif (WARN_ON_ONCE(!IS_ALIGNED(vaddr, UPROBE_SWBP_INSN_SIZE)))\n\t\treturn -EINVAL;\n\n\tpagefault_disable();\n\tresult = __get_user(opcode, (uprobe_opcode_t __user *)vaddr);\n\tpagefault_enable();\n\n\tif (likely(result == 0))\n\t\tgoto out;\n\n\t \n\tresult = get_user_pages_remote(mm, vaddr, 1, FOLL_FORCE, &page, NULL);\n\tif (result < 0)\n\t\treturn result;\n\n\tcopy_from_page(page, vaddr, &opcode, UPROBE_SWBP_INSN_SIZE);\n\tput_page(page);\n out:\n\t \n\treturn is_trap_insn(&opcode);\n}\n\nstatic struct uprobe *find_active_uprobe(unsigned long bp_vaddr, int *is_swbp)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct uprobe *uprobe = NULL;\n\tstruct vm_area_struct *vma;\n\n\tmmap_read_lock(mm);\n\tvma = vma_lookup(mm, bp_vaddr);\n\tif (vma) {\n\t\tif (valid_vma(vma, false)) {\n\t\t\tstruct inode *inode = file_inode(vma->vm_file);\n\t\t\tloff_t offset = vaddr_to_offset(vma, bp_vaddr);\n\n\t\t\tuprobe = find_uprobe(inode, offset);\n\t\t}\n\n\t\tif (!uprobe)\n\t\t\t*is_swbp = is_trap_at_addr(mm, bp_vaddr);\n\t} else {\n\t\t*is_swbp = -EFAULT;\n\t}\n\n\tif (!uprobe && test_and_clear_bit(MMF_RECALC_UPROBES, &mm->flags))\n\t\tmmf_recalc_uprobes(mm);\n\tmmap_read_unlock(mm);\n\n\treturn uprobe;\n}\n\nstatic void handler_chain(struct uprobe *uprobe, struct pt_regs *regs)\n{\n\tstruct uprobe_consumer *uc;\n\tint remove = UPROBE_HANDLER_REMOVE;\n\tbool need_prep = false;  \n\n\tdown_read(&uprobe->register_rwsem);\n\tfor (uc = uprobe->consumers; uc; uc = uc->next) {\n\t\tint rc = 0;\n\n\t\tif (uc->handler) {\n\t\t\trc = uc->handler(uc, regs);\n\t\t\tWARN(rc & ~UPROBE_HANDLER_MASK,\n\t\t\t\t\"bad rc=0x%x from %ps()\\n\", rc, uc->handler);\n\t\t}\n\n\t\tif (uc->ret_handler)\n\t\t\tneed_prep = true;\n\n\t\tremove &= rc;\n\t}\n\n\tif (need_prep && !remove)\n\t\tprepare_uretprobe(uprobe, regs);  \n\n\tif (remove && uprobe->consumers) {\n\t\tWARN_ON(!uprobe_is_active(uprobe));\n\t\tunapply_uprobe(uprobe, current->mm);\n\t}\n\tup_read(&uprobe->register_rwsem);\n}\n\nstatic void\nhandle_uretprobe_chain(struct return_instance *ri, struct pt_regs *regs)\n{\n\tstruct uprobe *uprobe = ri->uprobe;\n\tstruct uprobe_consumer *uc;\n\n\tdown_read(&uprobe->register_rwsem);\n\tfor (uc = uprobe->consumers; uc; uc = uc->next) {\n\t\tif (uc->ret_handler)\n\t\t\tuc->ret_handler(uc, ri->func, regs);\n\t}\n\tup_read(&uprobe->register_rwsem);\n}\n\nstatic struct return_instance *find_next_ret_chain(struct return_instance *ri)\n{\n\tbool chained;\n\n\tdo {\n\t\tchained = ri->chained;\n\t\tri = ri->next;\t \n\t} while (chained);\n\n\treturn ri;\n}\n\nstatic void handle_trampoline(struct pt_regs *regs)\n{\n\tstruct uprobe_task *utask;\n\tstruct return_instance *ri, *next;\n\tbool valid;\n\n\tutask = current->utask;\n\tif (!utask)\n\t\tgoto sigill;\n\n\tri = utask->return_instances;\n\tif (!ri)\n\t\tgoto sigill;\n\n\tdo {\n\t\t \n\t\tnext = find_next_ret_chain(ri);\n\t\tvalid = !next || arch_uretprobe_is_alive(next, RP_CHECK_RET, regs);\n\n\t\tinstruction_pointer_set(regs, ri->orig_ret_vaddr);\n\t\tdo {\n\t\t\tif (valid)\n\t\t\t\thandle_uretprobe_chain(ri, regs);\n\t\t\tri = free_ret_instance(ri);\n\t\t\tutask->depth--;\n\t\t} while (ri != next);\n\t} while (!valid);\n\n\tutask->return_instances = ri;\n\treturn;\n\n sigill:\n\tuprobe_warn(current, \"handle uretprobe, sending SIGILL.\");\n\tforce_sig(SIGILL);\n\n}\n\nbool __weak arch_uprobe_ignore(struct arch_uprobe *aup, struct pt_regs *regs)\n{\n\treturn false;\n}\n\nbool __weak arch_uretprobe_is_alive(struct return_instance *ret, enum rp_check ctx,\n\t\t\t\t\tstruct pt_regs *regs)\n{\n\treturn true;\n}\n\n \nstatic void handle_swbp(struct pt_regs *regs)\n{\n\tstruct uprobe *uprobe;\n\tunsigned long bp_vaddr;\n\tint is_swbp;\n\n\tbp_vaddr = uprobe_get_swbp_addr(regs);\n\tif (bp_vaddr == get_trampoline_vaddr())\n\t\treturn handle_trampoline(regs);\n\n\tuprobe = find_active_uprobe(bp_vaddr, &is_swbp);\n\tif (!uprobe) {\n\t\tif (is_swbp > 0) {\n\t\t\t \n\t\t\tforce_sig(SIGTRAP);\n\t\t} else {\n\t\t\t \n\t\t\tinstruction_pointer_set(regs, bp_vaddr);\n\t\t}\n\t\treturn;\n\t}\n\n\t \n\tinstruction_pointer_set(regs, bp_vaddr);\n\n\t \n\tif (unlikely(!test_bit(UPROBE_COPY_INSN, &uprobe->flags)))\n\t\tgoto out;\n\n\t \n\tsmp_rmb();\n\n\t \n\tif (!get_utask())\n\t\tgoto out;\n\n\tif (arch_uprobe_ignore(&uprobe->arch, regs))\n\t\tgoto out;\n\n\thandler_chain(uprobe, regs);\n\n\tif (arch_uprobe_skip_sstep(&uprobe->arch, regs))\n\t\tgoto out;\n\n\tif (!pre_ssout(uprobe, regs, bp_vaddr))\n\t\treturn;\n\n\t \nout:\n\tput_uprobe(uprobe);\n}\n\n \nstatic void handle_singlestep(struct uprobe_task *utask, struct pt_regs *regs)\n{\n\tstruct uprobe *uprobe;\n\tint err = 0;\n\n\tuprobe = utask->active_uprobe;\n\tif (utask->state == UTASK_SSTEP_ACK)\n\t\terr = arch_uprobe_post_xol(&uprobe->arch, regs);\n\telse if (utask->state == UTASK_SSTEP_TRAPPED)\n\t\tarch_uprobe_abort_xol(&uprobe->arch, regs);\n\telse\n\t\tWARN_ON_ONCE(1);\n\n\tput_uprobe(uprobe);\n\tutask->active_uprobe = NULL;\n\tutask->state = UTASK_RUNNING;\n\txol_free_insn_slot(current);\n\n\tspin_lock_irq(&current->sighand->siglock);\n\trecalc_sigpending();  \n\tspin_unlock_irq(&current->sighand->siglock);\n\n\tif (unlikely(err)) {\n\t\tuprobe_warn(current, \"execute the probed insn, sending SIGILL.\");\n\t\tforce_sig(SIGILL);\n\t}\n}\n\n \nvoid uprobe_notify_resume(struct pt_regs *regs)\n{\n\tstruct uprobe_task *utask;\n\n\tclear_thread_flag(TIF_UPROBE);\n\n\tutask = current->utask;\n\tif (utask && utask->active_uprobe)\n\t\thandle_singlestep(utask, regs);\n\telse\n\t\thandle_swbp(regs);\n}\n\n \nint uprobe_pre_sstep_notifier(struct pt_regs *regs)\n{\n\tif (!current->mm)\n\t\treturn 0;\n\n\tif (!test_bit(MMF_HAS_UPROBES, &current->mm->flags) &&\n\t    (!current->utask || !current->utask->return_instances))\n\t\treturn 0;\n\n\tset_thread_flag(TIF_UPROBE);\n\treturn 1;\n}\n\n \nint uprobe_post_sstep_notifier(struct pt_regs *regs)\n{\n\tstruct uprobe_task *utask = current->utask;\n\n\tif (!current->mm || !utask || !utask->active_uprobe)\n\t\t \n\t\treturn 0;\n\n\tutask->state = UTASK_SSTEP_ACK;\n\tset_thread_flag(TIF_UPROBE);\n\treturn 1;\n}\n\nstatic struct notifier_block uprobe_exception_nb = {\n\t.notifier_call\t\t= arch_uprobe_exception_notify,\n\t.priority\t\t= INT_MAX-1,\t \n};\n\nvoid __init uprobes_init(void)\n{\n\tint i;\n\n\tfor (i = 0; i < UPROBES_HASH_SZ; i++)\n\t\tmutex_init(&uprobes_mmap_mutex[i]);\n\n\tBUG_ON(register_die_notifier(&uprobe_exception_nb));\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}