{
  "module_name": "timekeeping.c",
  "hash_id": "ffbcc92e3919daa40762ad6cd63513e61ff2856dc461a4b58afb9a0f14a03a9a",
  "original_prompt": "Ingested from linux-6.6.14/kernel/time/timekeeping.c",
  "human_readable_source": "\n \n#include <linux/timekeeper_internal.h>\n#include <linux/module.h>\n#include <linux/interrupt.h>\n#include <linux/percpu.h>\n#include <linux/init.h>\n#include <linux/mm.h>\n#include <linux/nmi.h>\n#include <linux/sched.h>\n#include <linux/sched/loadavg.h>\n#include <linux/sched/clock.h>\n#include <linux/syscore_ops.h>\n#include <linux/clocksource.h>\n#include <linux/jiffies.h>\n#include <linux/time.h>\n#include <linux/timex.h>\n#include <linux/tick.h>\n#include <linux/stop_machine.h>\n#include <linux/pvclock_gtod.h>\n#include <linux/compiler.h>\n#include <linux/audit.h>\n#include <linux/random.h>\n\n#include \"tick-internal.h\"\n#include \"ntp_internal.h\"\n#include \"timekeeping_internal.h\"\n\n#define TK_CLEAR_NTP\t\t(1 << 0)\n#define TK_MIRROR\t\t(1 << 1)\n#define TK_CLOCK_WAS_SET\t(1 << 2)\n\nenum timekeeping_adv_mode {\n\t \n\tTK_ADV_TICK,\n\n\t \n\tTK_ADV_FREQ\n};\n\nDEFINE_RAW_SPINLOCK(timekeeper_lock);\n\n \nstatic struct {\n\tseqcount_raw_spinlock_t\tseq;\n\tstruct timekeeper\ttimekeeper;\n} tk_core ____cacheline_aligned = {\n\t.seq = SEQCNT_RAW_SPINLOCK_ZERO(tk_core.seq, &timekeeper_lock),\n};\n\nstatic struct timekeeper shadow_timekeeper;\n\n \nint __read_mostly timekeeping_suspended;\n\n \nstruct tk_fast {\n\tseqcount_latch_t\tseq;\n\tstruct tk_read_base\tbase[2];\n};\n\n \nstatic u64 cycles_at_suspend;\n\nstatic u64 dummy_clock_read(struct clocksource *cs)\n{\n\tif (timekeeping_suspended)\n\t\treturn cycles_at_suspend;\n\treturn local_clock();\n}\n\nstatic struct clocksource dummy_clock = {\n\t.read = dummy_clock_read,\n};\n\n \n#define FAST_TK_INIT\t\t\t\t\t\t\\\n\t{\t\t\t\t\t\t\t\\\n\t\t.clock\t\t= &dummy_clock,\t\t\t\\\n\t\t.mask\t\t= CLOCKSOURCE_MASK(64),\t\t\\\n\t\t.mult\t\t= 1,\t\t\t\t\\\n\t\t.shift\t\t= 0,\t\t\t\t\\\n\t}\n\nstatic struct tk_fast tk_fast_mono ____cacheline_aligned = {\n\t.seq     = SEQCNT_LATCH_ZERO(tk_fast_mono.seq),\n\t.base[0] = FAST_TK_INIT,\n\t.base[1] = FAST_TK_INIT,\n};\n\nstatic struct tk_fast tk_fast_raw  ____cacheline_aligned = {\n\t.seq     = SEQCNT_LATCH_ZERO(tk_fast_raw.seq),\n\t.base[0] = FAST_TK_INIT,\n\t.base[1] = FAST_TK_INIT,\n};\n\nstatic inline void tk_normalize_xtime(struct timekeeper *tk)\n{\n\twhile (tk->tkr_mono.xtime_nsec >= ((u64)NSEC_PER_SEC << tk->tkr_mono.shift)) {\n\t\ttk->tkr_mono.xtime_nsec -= (u64)NSEC_PER_SEC << tk->tkr_mono.shift;\n\t\ttk->xtime_sec++;\n\t}\n\twhile (tk->tkr_raw.xtime_nsec >= ((u64)NSEC_PER_SEC << tk->tkr_raw.shift)) {\n\t\ttk->tkr_raw.xtime_nsec -= (u64)NSEC_PER_SEC << tk->tkr_raw.shift;\n\t\ttk->raw_sec++;\n\t}\n}\n\nstatic inline struct timespec64 tk_xtime(const struct timekeeper *tk)\n{\n\tstruct timespec64 ts;\n\n\tts.tv_sec = tk->xtime_sec;\n\tts.tv_nsec = (long)(tk->tkr_mono.xtime_nsec >> tk->tkr_mono.shift);\n\treturn ts;\n}\n\nstatic void tk_set_xtime(struct timekeeper *tk, const struct timespec64 *ts)\n{\n\ttk->xtime_sec = ts->tv_sec;\n\ttk->tkr_mono.xtime_nsec = (u64)ts->tv_nsec << tk->tkr_mono.shift;\n}\n\nstatic void tk_xtime_add(struct timekeeper *tk, const struct timespec64 *ts)\n{\n\ttk->xtime_sec += ts->tv_sec;\n\ttk->tkr_mono.xtime_nsec += (u64)ts->tv_nsec << tk->tkr_mono.shift;\n\ttk_normalize_xtime(tk);\n}\n\nstatic void tk_set_wall_to_mono(struct timekeeper *tk, struct timespec64 wtm)\n{\n\tstruct timespec64 tmp;\n\n\t \n\tset_normalized_timespec64(&tmp, -tk->wall_to_monotonic.tv_sec,\n\t\t\t\t\t-tk->wall_to_monotonic.tv_nsec);\n\tWARN_ON_ONCE(tk->offs_real != timespec64_to_ktime(tmp));\n\ttk->wall_to_monotonic = wtm;\n\tset_normalized_timespec64(&tmp, -wtm.tv_sec, -wtm.tv_nsec);\n\ttk->offs_real = timespec64_to_ktime(tmp);\n\ttk->offs_tai = ktime_add(tk->offs_real, ktime_set(tk->tai_offset, 0));\n}\n\nstatic inline void tk_update_sleep_time(struct timekeeper *tk, ktime_t delta)\n{\n\ttk->offs_boot = ktime_add(tk->offs_boot, delta);\n\t \n\ttk->monotonic_to_boot = ktime_to_timespec64(tk->offs_boot);\n}\n\n \nstatic inline u64 tk_clock_read(const struct tk_read_base *tkr)\n{\n\tstruct clocksource *clock = READ_ONCE(tkr->clock);\n\n\treturn clock->read(clock);\n}\n\n#ifdef CONFIG_DEBUG_TIMEKEEPING\n#define WARNING_FREQ (HZ*300)  \n\nstatic void timekeeping_check_update(struct timekeeper *tk, u64 offset)\n{\n\n\tu64 max_cycles = tk->tkr_mono.clock->max_cycles;\n\tconst char *name = tk->tkr_mono.clock->name;\n\n\tif (offset > max_cycles) {\n\t\tprintk_deferred(\"WARNING: timekeeping: Cycle offset (%lld) is larger than allowed by the '%s' clock's max_cycles value (%lld): time overflow danger\\n\",\n\t\t\t\toffset, name, max_cycles);\n\t\tprintk_deferred(\"         timekeeping: Your kernel is sick, but tries to cope by capping time updates\\n\");\n\t} else {\n\t\tif (offset > (max_cycles >> 1)) {\n\t\t\tprintk_deferred(\"INFO: timekeeping: Cycle offset (%lld) is larger than the '%s' clock's 50%% safety margin (%lld)\\n\",\n\t\t\t\t\toffset, name, max_cycles >> 1);\n\t\t\tprintk_deferred(\"      timekeeping: Your kernel is still fine, but is feeling a bit nervous\\n\");\n\t\t}\n\t}\n\n\tif (tk->underflow_seen) {\n\t\tif (jiffies - tk->last_warning > WARNING_FREQ) {\n\t\t\tprintk_deferred(\"WARNING: Underflow in clocksource '%s' observed, time update ignored.\\n\", name);\n\t\t\tprintk_deferred(\"         Please report this, consider using a different clocksource, if possible.\\n\");\n\t\t\tprintk_deferred(\"         Your kernel is probably still fine.\\n\");\n\t\t\ttk->last_warning = jiffies;\n\t\t}\n\t\ttk->underflow_seen = 0;\n\t}\n\n\tif (tk->overflow_seen) {\n\t\tif (jiffies - tk->last_warning > WARNING_FREQ) {\n\t\t\tprintk_deferred(\"WARNING: Overflow in clocksource '%s' observed, time update capped.\\n\", name);\n\t\t\tprintk_deferred(\"         Please report this, consider using a different clocksource, if possible.\\n\");\n\t\t\tprintk_deferred(\"         Your kernel is probably still fine.\\n\");\n\t\t\ttk->last_warning = jiffies;\n\t\t}\n\t\ttk->overflow_seen = 0;\n\t}\n}\n\nstatic inline u64 timekeeping_get_delta(const struct tk_read_base *tkr)\n{\n\tstruct timekeeper *tk = &tk_core.timekeeper;\n\tu64 now, last, mask, max, delta;\n\tunsigned int seq;\n\n\t \n\tdo {\n\t\tseq = read_seqcount_begin(&tk_core.seq);\n\t\tnow = tk_clock_read(tkr);\n\t\tlast = tkr->cycle_last;\n\t\tmask = tkr->mask;\n\t\tmax = tkr->clock->max_cycles;\n\t} while (read_seqcount_retry(&tk_core.seq, seq));\n\n\tdelta = clocksource_delta(now, last, mask);\n\n\t \n\tif (unlikely((~delta & mask) < (mask >> 3))) {\n\t\ttk->underflow_seen = 1;\n\t\tdelta = 0;\n\t}\n\n\t \n\tif (unlikely(delta > max)) {\n\t\ttk->overflow_seen = 1;\n\t\tdelta = tkr->clock->max_cycles;\n\t}\n\n\treturn delta;\n}\n#else\nstatic inline void timekeeping_check_update(struct timekeeper *tk, u64 offset)\n{\n}\nstatic inline u64 timekeeping_get_delta(const struct tk_read_base *tkr)\n{\n\tu64 cycle_now, delta;\n\n\t \n\tcycle_now = tk_clock_read(tkr);\n\n\t \n\tdelta = clocksource_delta(cycle_now, tkr->cycle_last, tkr->mask);\n\n\treturn delta;\n}\n#endif\n\n \nstatic void tk_setup_internals(struct timekeeper *tk, struct clocksource *clock)\n{\n\tu64 interval;\n\tu64 tmp, ntpinterval;\n\tstruct clocksource *old_clock;\n\n\t++tk->cs_was_changed_seq;\n\told_clock = tk->tkr_mono.clock;\n\ttk->tkr_mono.clock = clock;\n\ttk->tkr_mono.mask = clock->mask;\n\ttk->tkr_mono.cycle_last = tk_clock_read(&tk->tkr_mono);\n\n\ttk->tkr_raw.clock = clock;\n\ttk->tkr_raw.mask = clock->mask;\n\ttk->tkr_raw.cycle_last = tk->tkr_mono.cycle_last;\n\n\t \n\ttmp = NTP_INTERVAL_LENGTH;\n\ttmp <<= clock->shift;\n\tntpinterval = tmp;\n\ttmp += clock->mult/2;\n\tdo_div(tmp, clock->mult);\n\tif (tmp == 0)\n\t\ttmp = 1;\n\n\tinterval = (u64) tmp;\n\ttk->cycle_interval = interval;\n\n\t \n\ttk->xtime_interval = interval * clock->mult;\n\ttk->xtime_remainder = ntpinterval - tk->xtime_interval;\n\ttk->raw_interval = interval * clock->mult;\n\n\t  \n\tif (old_clock) {\n\t\tint shift_change = clock->shift - old_clock->shift;\n\t\tif (shift_change < 0) {\n\t\t\ttk->tkr_mono.xtime_nsec >>= -shift_change;\n\t\t\ttk->tkr_raw.xtime_nsec >>= -shift_change;\n\t\t} else {\n\t\t\ttk->tkr_mono.xtime_nsec <<= shift_change;\n\t\t\ttk->tkr_raw.xtime_nsec <<= shift_change;\n\t\t}\n\t}\n\n\ttk->tkr_mono.shift = clock->shift;\n\ttk->tkr_raw.shift = clock->shift;\n\n\ttk->ntp_error = 0;\n\ttk->ntp_error_shift = NTP_SCALE_SHIFT - clock->shift;\n\ttk->ntp_tick = ntpinterval << tk->ntp_error_shift;\n\n\t \n\ttk->tkr_mono.mult = clock->mult;\n\ttk->tkr_raw.mult = clock->mult;\n\ttk->ntp_err_mult = 0;\n\ttk->skip_second_overflow = 0;\n}\n\n \n\nstatic inline u64 timekeeping_delta_to_ns(const struct tk_read_base *tkr, u64 delta)\n{\n\tu64 nsec;\n\n\tnsec = delta * tkr->mult + tkr->xtime_nsec;\n\tnsec >>= tkr->shift;\n\n\treturn nsec;\n}\n\nstatic inline u64 timekeeping_get_ns(const struct tk_read_base *tkr)\n{\n\tu64 delta;\n\n\tdelta = timekeeping_get_delta(tkr);\n\treturn timekeeping_delta_to_ns(tkr, delta);\n}\n\nstatic inline u64 timekeeping_cycles_to_ns(const struct tk_read_base *tkr, u64 cycles)\n{\n\tu64 delta;\n\n\t \n\tdelta = clocksource_delta(cycles, tkr->cycle_last, tkr->mask);\n\treturn timekeeping_delta_to_ns(tkr, delta);\n}\n\n \nstatic void update_fast_timekeeper(const struct tk_read_base *tkr,\n\t\t\t\t   struct tk_fast *tkf)\n{\n\tstruct tk_read_base *base = tkf->base;\n\n\t \n\traw_write_seqcount_latch(&tkf->seq);\n\n\t \n\tmemcpy(base, tkr, sizeof(*base));\n\n\t \n\traw_write_seqcount_latch(&tkf->seq);\n\n\t \n\tmemcpy(base + 1, base, sizeof(*base));\n}\n\nstatic __always_inline u64 fast_tk_get_delta_ns(struct tk_read_base *tkr)\n{\n\tu64 delta, cycles = tk_clock_read(tkr);\n\n\tdelta = clocksource_delta(cycles, tkr->cycle_last, tkr->mask);\n\treturn timekeeping_delta_to_ns(tkr, delta);\n}\n\nstatic __always_inline u64 __ktime_get_fast_ns(struct tk_fast *tkf)\n{\n\tstruct tk_read_base *tkr;\n\tunsigned int seq;\n\tu64 now;\n\n\tdo {\n\t\tseq = raw_read_seqcount_latch(&tkf->seq);\n\t\ttkr = tkf->base + (seq & 0x01);\n\t\tnow = ktime_to_ns(tkr->base);\n\t\tnow += fast_tk_get_delta_ns(tkr);\n\t} while (raw_read_seqcount_latch_retry(&tkf->seq, seq));\n\n\treturn now;\n}\n\n \nu64 notrace ktime_get_mono_fast_ns(void)\n{\n\treturn __ktime_get_fast_ns(&tk_fast_mono);\n}\nEXPORT_SYMBOL_GPL(ktime_get_mono_fast_ns);\n\n \nu64 notrace ktime_get_raw_fast_ns(void)\n{\n\treturn __ktime_get_fast_ns(&tk_fast_raw);\n}\nEXPORT_SYMBOL_GPL(ktime_get_raw_fast_ns);\n\n \nu64 notrace ktime_get_boot_fast_ns(void)\n{\n\tstruct timekeeper *tk = &tk_core.timekeeper;\n\n\treturn (ktime_get_mono_fast_ns() + ktime_to_ns(data_race(tk->offs_boot)));\n}\nEXPORT_SYMBOL_GPL(ktime_get_boot_fast_ns);\n\n \nu64 notrace ktime_get_tai_fast_ns(void)\n{\n\tstruct timekeeper *tk = &tk_core.timekeeper;\n\n\treturn (ktime_get_mono_fast_ns() + ktime_to_ns(data_race(tk->offs_tai)));\n}\nEXPORT_SYMBOL_GPL(ktime_get_tai_fast_ns);\n\nstatic __always_inline u64 __ktime_get_real_fast(struct tk_fast *tkf, u64 *mono)\n{\n\tstruct tk_read_base *tkr;\n\tu64 basem, baser, delta;\n\tunsigned int seq;\n\n\tdo {\n\t\tseq = raw_read_seqcount_latch(&tkf->seq);\n\t\ttkr = tkf->base + (seq & 0x01);\n\t\tbasem = ktime_to_ns(tkr->base);\n\t\tbaser = ktime_to_ns(tkr->base_real);\n\t\tdelta = fast_tk_get_delta_ns(tkr);\n\t} while (raw_read_seqcount_latch_retry(&tkf->seq, seq));\n\n\tif (mono)\n\t\t*mono = basem + delta;\n\treturn baser + delta;\n}\n\n \nu64 ktime_get_real_fast_ns(void)\n{\n\treturn __ktime_get_real_fast(&tk_fast_mono, NULL);\n}\nEXPORT_SYMBOL_GPL(ktime_get_real_fast_ns);\n\n \nvoid ktime_get_fast_timestamps(struct ktime_timestamps *snapshot)\n{\n\tstruct timekeeper *tk = &tk_core.timekeeper;\n\n\tsnapshot->real = __ktime_get_real_fast(&tk_fast_mono, &snapshot->mono);\n\tsnapshot->boot = snapshot->mono + ktime_to_ns(data_race(tk->offs_boot));\n}\n\n \nstatic void halt_fast_timekeeper(const struct timekeeper *tk)\n{\n\tstatic struct tk_read_base tkr_dummy;\n\tconst struct tk_read_base *tkr = &tk->tkr_mono;\n\n\tmemcpy(&tkr_dummy, tkr, sizeof(tkr_dummy));\n\tcycles_at_suspend = tk_clock_read(tkr);\n\ttkr_dummy.clock = &dummy_clock;\n\ttkr_dummy.base_real = tkr->base + tk->offs_real;\n\tupdate_fast_timekeeper(&tkr_dummy, &tk_fast_mono);\n\n\ttkr = &tk->tkr_raw;\n\tmemcpy(&tkr_dummy, tkr, sizeof(tkr_dummy));\n\ttkr_dummy.clock = &dummy_clock;\n\tupdate_fast_timekeeper(&tkr_dummy, &tk_fast_raw);\n}\n\nstatic RAW_NOTIFIER_HEAD(pvclock_gtod_chain);\n\nstatic void update_pvclock_gtod(struct timekeeper *tk, bool was_set)\n{\n\traw_notifier_call_chain(&pvclock_gtod_chain, was_set, tk);\n}\n\n \nint pvclock_gtod_register_notifier(struct notifier_block *nb)\n{\n\tstruct timekeeper *tk = &tk_core.timekeeper;\n\tunsigned long flags;\n\tint ret;\n\n\traw_spin_lock_irqsave(&timekeeper_lock, flags);\n\tret = raw_notifier_chain_register(&pvclock_gtod_chain, nb);\n\tupdate_pvclock_gtod(tk, true);\n\traw_spin_unlock_irqrestore(&timekeeper_lock, flags);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(pvclock_gtod_register_notifier);\n\n \nint pvclock_gtod_unregister_notifier(struct notifier_block *nb)\n{\n\tunsigned long flags;\n\tint ret;\n\n\traw_spin_lock_irqsave(&timekeeper_lock, flags);\n\tret = raw_notifier_chain_unregister(&pvclock_gtod_chain, nb);\n\traw_spin_unlock_irqrestore(&timekeeper_lock, flags);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(pvclock_gtod_unregister_notifier);\n\n \nstatic inline void tk_update_leap_state(struct timekeeper *tk)\n{\n\ttk->next_leap_ktime = ntp_get_next_leap();\n\tif (tk->next_leap_ktime != KTIME_MAX)\n\t\t \n\t\ttk->next_leap_ktime = ktime_sub(tk->next_leap_ktime, tk->offs_real);\n}\n\n \nstatic inline void tk_update_ktime_data(struct timekeeper *tk)\n{\n\tu64 seconds;\n\tu32 nsec;\n\n\t \n\tseconds = (u64)(tk->xtime_sec + tk->wall_to_monotonic.tv_sec);\n\tnsec = (u32) tk->wall_to_monotonic.tv_nsec;\n\ttk->tkr_mono.base = ns_to_ktime(seconds * NSEC_PER_SEC + nsec);\n\n\t \n\tnsec += (u32)(tk->tkr_mono.xtime_nsec >> tk->tkr_mono.shift);\n\tif (nsec >= NSEC_PER_SEC)\n\t\tseconds++;\n\ttk->ktime_sec = seconds;\n\n\t \n\ttk->tkr_raw.base = ns_to_ktime(tk->raw_sec * NSEC_PER_SEC);\n}\n\n \nstatic void timekeeping_update(struct timekeeper *tk, unsigned int action)\n{\n\tif (action & TK_CLEAR_NTP) {\n\t\ttk->ntp_error = 0;\n\t\tntp_clear();\n\t}\n\n\ttk_update_leap_state(tk);\n\ttk_update_ktime_data(tk);\n\n\tupdate_vsyscall(tk);\n\tupdate_pvclock_gtod(tk, action & TK_CLOCK_WAS_SET);\n\n\ttk->tkr_mono.base_real = tk->tkr_mono.base + tk->offs_real;\n\tupdate_fast_timekeeper(&tk->tkr_mono, &tk_fast_mono);\n\tupdate_fast_timekeeper(&tk->tkr_raw,  &tk_fast_raw);\n\n\tif (action & TK_CLOCK_WAS_SET)\n\t\ttk->clock_was_set_seq++;\n\t \n\tif (action & TK_MIRROR)\n\t\tmemcpy(&shadow_timekeeper, &tk_core.timekeeper,\n\t\t       sizeof(tk_core.timekeeper));\n}\n\n \nstatic void timekeeping_forward_now(struct timekeeper *tk)\n{\n\tu64 cycle_now, delta;\n\n\tcycle_now = tk_clock_read(&tk->tkr_mono);\n\tdelta = clocksource_delta(cycle_now, tk->tkr_mono.cycle_last, tk->tkr_mono.mask);\n\ttk->tkr_mono.cycle_last = cycle_now;\n\ttk->tkr_raw.cycle_last  = cycle_now;\n\n\ttk->tkr_mono.xtime_nsec += delta * tk->tkr_mono.mult;\n\ttk->tkr_raw.xtime_nsec += delta * tk->tkr_raw.mult;\n\n\ttk_normalize_xtime(tk);\n}\n\n \nvoid ktime_get_real_ts64(struct timespec64 *ts)\n{\n\tstruct timekeeper *tk = &tk_core.timekeeper;\n\tunsigned int seq;\n\tu64 nsecs;\n\n\tWARN_ON(timekeeping_suspended);\n\n\tdo {\n\t\tseq = read_seqcount_begin(&tk_core.seq);\n\n\t\tts->tv_sec = tk->xtime_sec;\n\t\tnsecs = timekeeping_get_ns(&tk->tkr_mono);\n\n\t} while (read_seqcount_retry(&tk_core.seq, seq));\n\n\tts->tv_nsec = 0;\n\ttimespec64_add_ns(ts, nsecs);\n}\nEXPORT_SYMBOL(ktime_get_real_ts64);\n\nktime_t ktime_get(void)\n{\n\tstruct timekeeper *tk = &tk_core.timekeeper;\n\tunsigned int seq;\n\tktime_t base;\n\tu64 nsecs;\n\n\tWARN_ON(timekeeping_suspended);\n\n\tdo {\n\t\tseq = read_seqcount_begin(&tk_core.seq);\n\t\tbase = tk->tkr_mono.base;\n\t\tnsecs = timekeeping_get_ns(&tk->tkr_mono);\n\n\t} while (read_seqcount_retry(&tk_core.seq, seq));\n\n\treturn ktime_add_ns(base, nsecs);\n}\nEXPORT_SYMBOL_GPL(ktime_get);\n\nu32 ktime_get_resolution_ns(void)\n{\n\tstruct timekeeper *tk = &tk_core.timekeeper;\n\tunsigned int seq;\n\tu32 nsecs;\n\n\tWARN_ON(timekeeping_suspended);\n\n\tdo {\n\t\tseq = read_seqcount_begin(&tk_core.seq);\n\t\tnsecs = tk->tkr_mono.mult >> tk->tkr_mono.shift;\n\t} while (read_seqcount_retry(&tk_core.seq, seq));\n\n\treturn nsecs;\n}\nEXPORT_SYMBOL_GPL(ktime_get_resolution_ns);\n\nstatic ktime_t *offsets[TK_OFFS_MAX] = {\n\t[TK_OFFS_REAL]\t= &tk_core.timekeeper.offs_real,\n\t[TK_OFFS_BOOT]\t= &tk_core.timekeeper.offs_boot,\n\t[TK_OFFS_TAI]\t= &tk_core.timekeeper.offs_tai,\n};\n\nktime_t ktime_get_with_offset(enum tk_offsets offs)\n{\n\tstruct timekeeper *tk = &tk_core.timekeeper;\n\tunsigned int seq;\n\tktime_t base, *offset = offsets[offs];\n\tu64 nsecs;\n\n\tWARN_ON(timekeeping_suspended);\n\n\tdo {\n\t\tseq = read_seqcount_begin(&tk_core.seq);\n\t\tbase = ktime_add(tk->tkr_mono.base, *offset);\n\t\tnsecs = timekeeping_get_ns(&tk->tkr_mono);\n\n\t} while (read_seqcount_retry(&tk_core.seq, seq));\n\n\treturn ktime_add_ns(base, nsecs);\n\n}\nEXPORT_SYMBOL_GPL(ktime_get_with_offset);\n\nktime_t ktime_get_coarse_with_offset(enum tk_offsets offs)\n{\n\tstruct timekeeper *tk = &tk_core.timekeeper;\n\tunsigned int seq;\n\tktime_t base, *offset = offsets[offs];\n\tu64 nsecs;\n\n\tWARN_ON(timekeeping_suspended);\n\n\tdo {\n\t\tseq = read_seqcount_begin(&tk_core.seq);\n\t\tbase = ktime_add(tk->tkr_mono.base, *offset);\n\t\tnsecs = tk->tkr_mono.xtime_nsec >> tk->tkr_mono.shift;\n\n\t} while (read_seqcount_retry(&tk_core.seq, seq));\n\n\treturn ktime_add_ns(base, nsecs);\n}\nEXPORT_SYMBOL_GPL(ktime_get_coarse_with_offset);\n\n \nktime_t ktime_mono_to_any(ktime_t tmono, enum tk_offsets offs)\n{\n\tktime_t *offset = offsets[offs];\n\tunsigned int seq;\n\tktime_t tconv;\n\n\tdo {\n\t\tseq = read_seqcount_begin(&tk_core.seq);\n\t\ttconv = ktime_add(tmono, *offset);\n\t} while (read_seqcount_retry(&tk_core.seq, seq));\n\n\treturn tconv;\n}\nEXPORT_SYMBOL_GPL(ktime_mono_to_any);\n\n \nktime_t ktime_get_raw(void)\n{\n\tstruct timekeeper *tk = &tk_core.timekeeper;\n\tunsigned int seq;\n\tktime_t base;\n\tu64 nsecs;\n\n\tdo {\n\t\tseq = read_seqcount_begin(&tk_core.seq);\n\t\tbase = tk->tkr_raw.base;\n\t\tnsecs = timekeeping_get_ns(&tk->tkr_raw);\n\n\t} while (read_seqcount_retry(&tk_core.seq, seq));\n\n\treturn ktime_add_ns(base, nsecs);\n}\nEXPORT_SYMBOL_GPL(ktime_get_raw);\n\n \nvoid ktime_get_ts64(struct timespec64 *ts)\n{\n\tstruct timekeeper *tk = &tk_core.timekeeper;\n\tstruct timespec64 tomono;\n\tunsigned int seq;\n\tu64 nsec;\n\n\tWARN_ON(timekeeping_suspended);\n\n\tdo {\n\t\tseq = read_seqcount_begin(&tk_core.seq);\n\t\tts->tv_sec = tk->xtime_sec;\n\t\tnsec = timekeeping_get_ns(&tk->tkr_mono);\n\t\ttomono = tk->wall_to_monotonic;\n\n\t} while (read_seqcount_retry(&tk_core.seq, seq));\n\n\tts->tv_sec += tomono.tv_sec;\n\tts->tv_nsec = 0;\n\ttimespec64_add_ns(ts, nsec + tomono.tv_nsec);\n}\nEXPORT_SYMBOL_GPL(ktime_get_ts64);\n\n \ntime64_t ktime_get_seconds(void)\n{\n\tstruct timekeeper *tk = &tk_core.timekeeper;\n\n\tWARN_ON(timekeeping_suspended);\n\treturn tk->ktime_sec;\n}\nEXPORT_SYMBOL_GPL(ktime_get_seconds);\n\n \ntime64_t ktime_get_real_seconds(void)\n{\n\tstruct timekeeper *tk = &tk_core.timekeeper;\n\ttime64_t seconds;\n\tunsigned int seq;\n\n\tif (IS_ENABLED(CONFIG_64BIT))\n\t\treturn tk->xtime_sec;\n\n\tdo {\n\t\tseq = read_seqcount_begin(&tk_core.seq);\n\t\tseconds = tk->xtime_sec;\n\n\t} while (read_seqcount_retry(&tk_core.seq, seq));\n\n\treturn seconds;\n}\nEXPORT_SYMBOL_GPL(ktime_get_real_seconds);\n\n \nnoinstr time64_t __ktime_get_real_seconds(void)\n{\n\tstruct timekeeper *tk = &tk_core.timekeeper;\n\n\treturn tk->xtime_sec;\n}\n\n \nvoid ktime_get_snapshot(struct system_time_snapshot *systime_snapshot)\n{\n\tstruct timekeeper *tk = &tk_core.timekeeper;\n\tunsigned int seq;\n\tktime_t base_raw;\n\tktime_t base_real;\n\tu64 nsec_raw;\n\tu64 nsec_real;\n\tu64 now;\n\n\tWARN_ON_ONCE(timekeeping_suspended);\n\n\tdo {\n\t\tseq = read_seqcount_begin(&tk_core.seq);\n\t\tnow = tk_clock_read(&tk->tkr_mono);\n\t\tsystime_snapshot->cs_id = tk->tkr_mono.clock->id;\n\t\tsystime_snapshot->cs_was_changed_seq = tk->cs_was_changed_seq;\n\t\tsystime_snapshot->clock_was_set_seq = tk->clock_was_set_seq;\n\t\tbase_real = ktime_add(tk->tkr_mono.base,\n\t\t\t\t      tk_core.timekeeper.offs_real);\n\t\tbase_raw = tk->tkr_raw.base;\n\t\tnsec_real = timekeeping_cycles_to_ns(&tk->tkr_mono, now);\n\t\tnsec_raw  = timekeeping_cycles_to_ns(&tk->tkr_raw, now);\n\t} while (read_seqcount_retry(&tk_core.seq, seq));\n\n\tsystime_snapshot->cycles = now;\n\tsystime_snapshot->real = ktime_add_ns(base_real, nsec_real);\n\tsystime_snapshot->raw = ktime_add_ns(base_raw, nsec_raw);\n}\nEXPORT_SYMBOL_GPL(ktime_get_snapshot);\n\n \nstatic int scale64_check_overflow(u64 mult, u64 div, u64 *base)\n{\n\tu64 tmp, rem;\n\n\ttmp = div64_u64_rem(*base, div, &rem);\n\n\tif (((int)sizeof(u64)*8 - fls64(mult) < fls64(tmp)) ||\n\t    ((int)sizeof(u64)*8 - fls64(mult) < fls64(rem)))\n\t\treturn -EOVERFLOW;\n\ttmp *= mult;\n\n\trem = div64_u64(rem * mult, div);\n\t*base = tmp + rem;\n\treturn 0;\n}\n\n \nstatic int adjust_historical_crosststamp(struct system_time_snapshot *history,\n\t\t\t\t\t u64 partial_history_cycles,\n\t\t\t\t\t u64 total_history_cycles,\n\t\t\t\t\t bool discontinuity,\n\t\t\t\t\t struct system_device_crosststamp *ts)\n{\n\tstruct timekeeper *tk = &tk_core.timekeeper;\n\tu64 corr_raw, corr_real;\n\tbool interp_forward;\n\tint ret;\n\n\tif (total_history_cycles == 0 || partial_history_cycles == 0)\n\t\treturn 0;\n\n\t \n\tinterp_forward = partial_history_cycles > total_history_cycles / 2;\n\tpartial_history_cycles = interp_forward ?\n\t\ttotal_history_cycles - partial_history_cycles :\n\t\tpartial_history_cycles;\n\n\t \n\tcorr_raw = (u64)ktime_to_ns(\n\t\tktime_sub(ts->sys_monoraw, history->raw));\n\tret = scale64_check_overflow(partial_history_cycles,\n\t\t\t\t     total_history_cycles, &corr_raw);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tif (discontinuity) {\n\t\tcorr_real = mul_u64_u32_div\n\t\t\t(corr_raw, tk->tkr_mono.mult, tk->tkr_raw.mult);\n\t} else {\n\t\tcorr_real = (u64)ktime_to_ns(\n\t\t\tktime_sub(ts->sys_realtime, history->real));\n\t\tret = scale64_check_overflow(partial_history_cycles,\n\t\t\t\t\t     total_history_cycles, &corr_real);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t \n\tif (interp_forward) {\n\t\tts->sys_monoraw = ktime_add_ns(history->raw, corr_raw);\n\t\tts->sys_realtime = ktime_add_ns(history->real, corr_real);\n\t} else {\n\t\tts->sys_monoraw = ktime_sub_ns(ts->sys_monoraw, corr_raw);\n\t\tts->sys_realtime = ktime_sub_ns(ts->sys_realtime, corr_real);\n\t}\n\n\treturn 0;\n}\n\n \nstatic bool cycle_between(u64 before, u64 test, u64 after)\n{\n\tif (test > before && test < after)\n\t\treturn true;\n\tif (test < before && before > after)\n\t\treturn true;\n\treturn false;\n}\n\n \nint get_device_system_crosststamp(int (*get_time_fn)\n\t\t\t\t  (ktime_t *device_time,\n\t\t\t\t   struct system_counterval_t *sys_counterval,\n\t\t\t\t   void *ctx),\n\t\t\t\t  void *ctx,\n\t\t\t\t  struct system_time_snapshot *history_begin,\n\t\t\t\t  struct system_device_crosststamp *xtstamp)\n{\n\tstruct system_counterval_t system_counterval;\n\tstruct timekeeper *tk = &tk_core.timekeeper;\n\tu64 cycles, now, interval_start;\n\tunsigned int clock_was_set_seq = 0;\n\tktime_t base_real, base_raw;\n\tu64 nsec_real, nsec_raw;\n\tu8 cs_was_changed_seq;\n\tunsigned int seq;\n\tbool do_interp;\n\tint ret;\n\n\tdo {\n\t\tseq = read_seqcount_begin(&tk_core.seq);\n\t\t \n\t\tret = get_time_fn(&xtstamp->device, &system_counterval, ctx);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\t \n\t\tif (tk->tkr_mono.clock != system_counterval.cs)\n\t\t\treturn -ENODEV;\n\t\tcycles = system_counterval.cycles;\n\n\t\t \n\t\tnow = tk_clock_read(&tk->tkr_mono);\n\t\tinterval_start = tk->tkr_mono.cycle_last;\n\t\tif (!cycle_between(interval_start, cycles, now)) {\n\t\t\tclock_was_set_seq = tk->clock_was_set_seq;\n\t\t\tcs_was_changed_seq = tk->cs_was_changed_seq;\n\t\t\tcycles = interval_start;\n\t\t\tdo_interp = true;\n\t\t} else {\n\t\t\tdo_interp = false;\n\t\t}\n\n\t\tbase_real = ktime_add(tk->tkr_mono.base,\n\t\t\t\t      tk_core.timekeeper.offs_real);\n\t\tbase_raw = tk->tkr_raw.base;\n\n\t\tnsec_real = timekeeping_cycles_to_ns(&tk->tkr_mono,\n\t\t\t\t\t\t     system_counterval.cycles);\n\t\tnsec_raw = timekeeping_cycles_to_ns(&tk->tkr_raw,\n\t\t\t\t\t\t    system_counterval.cycles);\n\t} while (read_seqcount_retry(&tk_core.seq, seq));\n\n\txtstamp->sys_realtime = ktime_add_ns(base_real, nsec_real);\n\txtstamp->sys_monoraw = ktime_add_ns(base_raw, nsec_raw);\n\n\t \n\tif (do_interp) {\n\t\tu64 partial_history_cycles, total_history_cycles;\n\t\tbool discontinuity;\n\n\t\t \n\t\tif (!history_begin ||\n\t\t    !cycle_between(history_begin->cycles,\n\t\t\t\t   system_counterval.cycles, cycles) ||\n\t\t    history_begin->cs_was_changed_seq != cs_was_changed_seq)\n\t\t\treturn -EINVAL;\n\t\tpartial_history_cycles = cycles - system_counterval.cycles;\n\t\ttotal_history_cycles = cycles - history_begin->cycles;\n\t\tdiscontinuity =\n\t\t\thistory_begin->clock_was_set_seq != clock_was_set_seq;\n\n\t\tret = adjust_historical_crosststamp(history_begin,\n\t\t\t\t\t\t    partial_history_cycles,\n\t\t\t\t\t\t    total_history_cycles,\n\t\t\t\t\t\t    discontinuity, xtstamp);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(get_device_system_crosststamp);\n\n \nint do_settimeofday64(const struct timespec64 *ts)\n{\n\tstruct timekeeper *tk = &tk_core.timekeeper;\n\tstruct timespec64 ts_delta, xt;\n\tunsigned long flags;\n\tint ret = 0;\n\n\tif (!timespec64_valid_settod(ts))\n\t\treturn -EINVAL;\n\n\traw_spin_lock_irqsave(&timekeeper_lock, flags);\n\twrite_seqcount_begin(&tk_core.seq);\n\n\ttimekeeping_forward_now(tk);\n\n\txt = tk_xtime(tk);\n\tts_delta = timespec64_sub(*ts, xt);\n\n\tif (timespec64_compare(&tk->wall_to_monotonic, &ts_delta) > 0) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\ttk_set_wall_to_mono(tk, timespec64_sub(tk->wall_to_monotonic, ts_delta));\n\n\ttk_set_xtime(tk, ts);\nout:\n\ttimekeeping_update(tk, TK_CLEAR_NTP | TK_MIRROR | TK_CLOCK_WAS_SET);\n\n\twrite_seqcount_end(&tk_core.seq);\n\traw_spin_unlock_irqrestore(&timekeeper_lock, flags);\n\n\t \n\tclock_was_set(CLOCK_SET_WALL);\n\n\tif (!ret) {\n\t\taudit_tk_injoffset(ts_delta);\n\t\tadd_device_randomness(ts, sizeof(*ts));\n\t}\n\n\treturn ret;\n}\nEXPORT_SYMBOL(do_settimeofday64);\n\n \nstatic int timekeeping_inject_offset(const struct timespec64 *ts)\n{\n\tstruct timekeeper *tk = &tk_core.timekeeper;\n\tunsigned long flags;\n\tstruct timespec64 tmp;\n\tint ret = 0;\n\n\tif (ts->tv_nsec < 0 || ts->tv_nsec >= NSEC_PER_SEC)\n\t\treturn -EINVAL;\n\n\traw_spin_lock_irqsave(&timekeeper_lock, flags);\n\twrite_seqcount_begin(&tk_core.seq);\n\n\ttimekeeping_forward_now(tk);\n\n\t \n\ttmp = timespec64_add(tk_xtime(tk), *ts);\n\tif (timespec64_compare(&tk->wall_to_monotonic, ts) > 0 ||\n\t    !timespec64_valid_settod(&tmp)) {\n\t\tret = -EINVAL;\n\t\tgoto error;\n\t}\n\n\ttk_xtime_add(tk, ts);\n\ttk_set_wall_to_mono(tk, timespec64_sub(tk->wall_to_monotonic, *ts));\n\nerror:  \n\ttimekeeping_update(tk, TK_CLEAR_NTP | TK_MIRROR | TK_CLOCK_WAS_SET);\n\n\twrite_seqcount_end(&tk_core.seq);\n\traw_spin_unlock_irqrestore(&timekeeper_lock, flags);\n\n\t \n\tclock_was_set(CLOCK_SET_WALL);\n\n\treturn ret;\n}\n\n \nint persistent_clock_is_local;\n\n \nvoid timekeeping_warp_clock(void)\n{\n\tif (sys_tz.tz_minuteswest != 0) {\n\t\tstruct timespec64 adjust;\n\n\t\tpersistent_clock_is_local = 1;\n\t\tadjust.tv_sec = sys_tz.tz_minuteswest * 60;\n\t\tadjust.tv_nsec = 0;\n\t\ttimekeeping_inject_offset(&adjust);\n\t}\n}\n\n \nstatic void __timekeeping_set_tai_offset(struct timekeeper *tk, s32 tai_offset)\n{\n\ttk->tai_offset = tai_offset;\n\ttk->offs_tai = ktime_add(tk->offs_real, ktime_set(tai_offset, 0));\n}\n\n \nstatic int change_clocksource(void *data)\n{\n\tstruct timekeeper *tk = &tk_core.timekeeper;\n\tstruct clocksource *new, *old = NULL;\n\tunsigned long flags;\n\tbool change = false;\n\n\tnew = (struct clocksource *) data;\n\n\t \n\tif (try_module_get(new->owner)) {\n\t\tif (!new->enable || new->enable(new) == 0)\n\t\t\tchange = true;\n\t\telse\n\t\t\tmodule_put(new->owner);\n\t}\n\n\traw_spin_lock_irqsave(&timekeeper_lock, flags);\n\twrite_seqcount_begin(&tk_core.seq);\n\n\ttimekeeping_forward_now(tk);\n\n\tif (change) {\n\t\told = tk->tkr_mono.clock;\n\t\ttk_setup_internals(tk, new);\n\t}\n\n\ttimekeeping_update(tk, TK_CLEAR_NTP | TK_MIRROR | TK_CLOCK_WAS_SET);\n\n\twrite_seqcount_end(&tk_core.seq);\n\traw_spin_unlock_irqrestore(&timekeeper_lock, flags);\n\n\tif (old) {\n\t\tif (old->disable)\n\t\t\told->disable(old);\n\n\t\tmodule_put(old->owner);\n\t}\n\n\treturn 0;\n}\n\n \nint timekeeping_notify(struct clocksource *clock)\n{\n\tstruct timekeeper *tk = &tk_core.timekeeper;\n\n\tif (tk->tkr_mono.clock == clock)\n\t\treturn 0;\n\tstop_machine(change_clocksource, clock, NULL);\n\ttick_clock_notify();\n\treturn tk->tkr_mono.clock == clock ? 0 : -1;\n}\n\n \nvoid ktime_get_raw_ts64(struct timespec64 *ts)\n{\n\tstruct timekeeper *tk = &tk_core.timekeeper;\n\tunsigned int seq;\n\tu64 nsecs;\n\n\tdo {\n\t\tseq = read_seqcount_begin(&tk_core.seq);\n\t\tts->tv_sec = tk->raw_sec;\n\t\tnsecs = timekeeping_get_ns(&tk->tkr_raw);\n\n\t} while (read_seqcount_retry(&tk_core.seq, seq));\n\n\tts->tv_nsec = 0;\n\ttimespec64_add_ns(ts, nsecs);\n}\nEXPORT_SYMBOL(ktime_get_raw_ts64);\n\n\n \nint timekeeping_valid_for_hres(void)\n{\n\tstruct timekeeper *tk = &tk_core.timekeeper;\n\tunsigned int seq;\n\tint ret;\n\n\tdo {\n\t\tseq = read_seqcount_begin(&tk_core.seq);\n\n\t\tret = tk->tkr_mono.clock->flags & CLOCK_SOURCE_VALID_FOR_HRES;\n\n\t} while (read_seqcount_retry(&tk_core.seq, seq));\n\n\treturn ret;\n}\n\n \nu64 timekeeping_max_deferment(void)\n{\n\tstruct timekeeper *tk = &tk_core.timekeeper;\n\tunsigned int seq;\n\tu64 ret;\n\n\tdo {\n\t\tseq = read_seqcount_begin(&tk_core.seq);\n\n\t\tret = tk->tkr_mono.clock->max_idle_ns;\n\n\t} while (read_seqcount_retry(&tk_core.seq, seq));\n\n\treturn ret;\n}\n\n \nvoid __weak read_persistent_clock64(struct timespec64 *ts)\n{\n\tts->tv_sec = 0;\n\tts->tv_nsec = 0;\n}\n\n \nvoid __weak __init\nread_persistent_wall_and_boot_offset(struct timespec64 *wall_time,\n\t\t\t\t     struct timespec64 *boot_offset)\n{\n\tread_persistent_clock64(wall_time);\n\t*boot_offset = ns_to_timespec64(local_clock());\n}\n\n \nstatic bool suspend_timing_needed;\n\n \nstatic bool persistent_clock_exists;\n\n \nvoid __init timekeeping_init(void)\n{\n\tstruct timespec64 wall_time, boot_offset, wall_to_mono;\n\tstruct timekeeper *tk = &tk_core.timekeeper;\n\tstruct clocksource *clock;\n\tunsigned long flags;\n\n\tread_persistent_wall_and_boot_offset(&wall_time, &boot_offset);\n\tif (timespec64_valid_settod(&wall_time) &&\n\t    timespec64_to_ns(&wall_time) > 0) {\n\t\tpersistent_clock_exists = true;\n\t} else if (timespec64_to_ns(&wall_time) != 0) {\n\t\tpr_warn(\"Persistent clock returned invalid value\");\n\t\twall_time = (struct timespec64){0};\n\t}\n\n\tif (timespec64_compare(&wall_time, &boot_offset) < 0)\n\t\tboot_offset = (struct timespec64){0};\n\n\t \n\twall_to_mono = timespec64_sub(boot_offset, wall_time);\n\n\traw_spin_lock_irqsave(&timekeeper_lock, flags);\n\twrite_seqcount_begin(&tk_core.seq);\n\tntp_init();\n\n\tclock = clocksource_default_clock();\n\tif (clock->enable)\n\t\tclock->enable(clock);\n\ttk_setup_internals(tk, clock);\n\n\ttk_set_xtime(tk, &wall_time);\n\ttk->raw_sec = 0;\n\n\ttk_set_wall_to_mono(tk, wall_to_mono);\n\n\ttimekeeping_update(tk, TK_MIRROR | TK_CLOCK_WAS_SET);\n\n\twrite_seqcount_end(&tk_core.seq);\n\traw_spin_unlock_irqrestore(&timekeeper_lock, flags);\n}\n\n \nstatic struct timespec64 timekeeping_suspend_time;\n\n \nstatic void __timekeeping_inject_sleeptime(struct timekeeper *tk,\n\t\t\t\t\t   const struct timespec64 *delta)\n{\n\tif (!timespec64_valid_strict(delta)) {\n\t\tprintk_deferred(KERN_WARNING\n\t\t\t\t\"__timekeeping_inject_sleeptime: Invalid \"\n\t\t\t\t\"sleep delta value!\\n\");\n\t\treturn;\n\t}\n\ttk_xtime_add(tk, delta);\n\ttk_set_wall_to_mono(tk, timespec64_sub(tk->wall_to_monotonic, *delta));\n\ttk_update_sleep_time(tk, timespec64_to_ktime(*delta));\n\ttk_debug_account_sleep_time(delta);\n}\n\n#if defined(CONFIG_PM_SLEEP) && defined(CONFIG_RTC_HCTOSYS_DEVICE)\n \nbool timekeeping_rtc_skipresume(void)\n{\n\treturn !suspend_timing_needed;\n}\n\n \nbool timekeeping_rtc_skipsuspend(void)\n{\n\treturn persistent_clock_exists;\n}\n\n \nvoid timekeeping_inject_sleeptime64(const struct timespec64 *delta)\n{\n\tstruct timekeeper *tk = &tk_core.timekeeper;\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&timekeeper_lock, flags);\n\twrite_seqcount_begin(&tk_core.seq);\n\n\tsuspend_timing_needed = false;\n\n\ttimekeeping_forward_now(tk);\n\n\t__timekeeping_inject_sleeptime(tk, delta);\n\n\ttimekeeping_update(tk, TK_CLEAR_NTP | TK_MIRROR | TK_CLOCK_WAS_SET);\n\n\twrite_seqcount_end(&tk_core.seq);\n\traw_spin_unlock_irqrestore(&timekeeper_lock, flags);\n\n\t \n\tclock_was_set(CLOCK_SET_WALL | CLOCK_SET_BOOT);\n}\n#endif\n\n \nvoid timekeeping_resume(void)\n{\n\tstruct timekeeper *tk = &tk_core.timekeeper;\n\tstruct clocksource *clock = tk->tkr_mono.clock;\n\tunsigned long flags;\n\tstruct timespec64 ts_new, ts_delta;\n\tu64 cycle_now, nsec;\n\tbool inject_sleeptime = false;\n\n\tread_persistent_clock64(&ts_new);\n\n\tclockevents_resume();\n\tclocksource_resume();\n\n\traw_spin_lock_irqsave(&timekeeper_lock, flags);\n\twrite_seqcount_begin(&tk_core.seq);\n\n\t \n\tcycle_now = tk_clock_read(&tk->tkr_mono);\n\tnsec = clocksource_stop_suspend_timing(clock, cycle_now);\n\tif (nsec > 0) {\n\t\tts_delta = ns_to_timespec64(nsec);\n\t\tinject_sleeptime = true;\n\t} else if (timespec64_compare(&ts_new, &timekeeping_suspend_time) > 0) {\n\t\tts_delta = timespec64_sub(ts_new, timekeeping_suspend_time);\n\t\tinject_sleeptime = true;\n\t}\n\n\tif (inject_sleeptime) {\n\t\tsuspend_timing_needed = false;\n\t\t__timekeeping_inject_sleeptime(tk, &ts_delta);\n\t}\n\n\t \n\ttk->tkr_mono.cycle_last = cycle_now;\n\ttk->tkr_raw.cycle_last  = cycle_now;\n\n\ttk->ntp_error = 0;\n\ttimekeeping_suspended = 0;\n\ttimekeeping_update(tk, TK_MIRROR | TK_CLOCK_WAS_SET);\n\twrite_seqcount_end(&tk_core.seq);\n\traw_spin_unlock_irqrestore(&timekeeper_lock, flags);\n\n\ttouch_softlockup_watchdog();\n\n\t \n\ttick_resume();\n\t \n\ttimerfd_resume();\n}\n\nint timekeeping_suspend(void)\n{\n\tstruct timekeeper *tk = &tk_core.timekeeper;\n\tunsigned long flags;\n\tstruct timespec64\t\tdelta, delta_delta;\n\tstatic struct timespec64\told_delta;\n\tstruct clocksource *curr_clock;\n\tu64 cycle_now;\n\n\tread_persistent_clock64(&timekeeping_suspend_time);\n\n\t \n\tif (timekeeping_suspend_time.tv_sec || timekeeping_suspend_time.tv_nsec)\n\t\tpersistent_clock_exists = true;\n\n\tsuspend_timing_needed = true;\n\n\traw_spin_lock_irqsave(&timekeeper_lock, flags);\n\twrite_seqcount_begin(&tk_core.seq);\n\ttimekeeping_forward_now(tk);\n\ttimekeeping_suspended = 1;\n\n\t \n\tcurr_clock = tk->tkr_mono.clock;\n\tcycle_now = tk->tkr_mono.cycle_last;\n\tclocksource_start_suspend_timing(curr_clock, cycle_now);\n\n\tif (persistent_clock_exists) {\n\t\t \n\t\tdelta = timespec64_sub(tk_xtime(tk), timekeeping_suspend_time);\n\t\tdelta_delta = timespec64_sub(delta, old_delta);\n\t\tif (abs(delta_delta.tv_sec) >= 2) {\n\t\t\t \n\t\t\told_delta = delta;\n\t\t} else {\n\t\t\t \n\t\t\ttimekeeping_suspend_time =\n\t\t\t\ttimespec64_add(timekeeping_suspend_time, delta_delta);\n\t\t}\n\t}\n\n\ttimekeeping_update(tk, TK_MIRROR);\n\thalt_fast_timekeeper(tk);\n\twrite_seqcount_end(&tk_core.seq);\n\traw_spin_unlock_irqrestore(&timekeeper_lock, flags);\n\n\ttick_suspend();\n\tclocksource_suspend();\n\tclockevents_suspend();\n\n\treturn 0;\n}\n\n \nstatic struct syscore_ops timekeeping_syscore_ops = {\n\t.resume\t\t= timekeeping_resume,\n\t.suspend\t= timekeeping_suspend,\n};\n\nstatic int __init timekeeping_init_ops(void)\n{\n\tregister_syscore_ops(&timekeeping_syscore_ops);\n\treturn 0;\n}\ndevice_initcall(timekeeping_init_ops);\n\n \nstatic __always_inline void timekeeping_apply_adjustment(struct timekeeper *tk,\n\t\t\t\t\t\t\t s64 offset,\n\t\t\t\t\t\t\t s32 mult_adj)\n{\n\ts64 interval = tk->cycle_interval;\n\n\tif (mult_adj == 0) {\n\t\treturn;\n\t} else if (mult_adj == -1) {\n\t\tinterval = -interval;\n\t\toffset = -offset;\n\t} else if (mult_adj != 1) {\n\t\tinterval *= mult_adj;\n\t\toffset *= mult_adj;\n\t}\n\n\t \n\tif ((mult_adj > 0) && (tk->tkr_mono.mult + mult_adj < mult_adj)) {\n\t\t \n\t\tWARN_ON_ONCE(1);\n\t\treturn;\n\t}\n\n\ttk->tkr_mono.mult += mult_adj;\n\ttk->xtime_interval += interval;\n\ttk->tkr_mono.xtime_nsec -= offset;\n}\n\n \nstatic void timekeeping_adjust(struct timekeeper *tk, s64 offset)\n{\n\tu32 mult;\n\n\t \n\tif (likely(tk->ntp_tick == ntp_tick_length())) {\n\t\tmult = tk->tkr_mono.mult - tk->ntp_err_mult;\n\t} else {\n\t\ttk->ntp_tick = ntp_tick_length();\n\t\tmult = div64_u64((tk->ntp_tick >> tk->ntp_error_shift) -\n\t\t\t\t tk->xtime_remainder, tk->cycle_interval);\n\t}\n\n\t \n\ttk->ntp_err_mult = tk->ntp_error > 0 ? 1 : 0;\n\tmult += tk->ntp_err_mult;\n\n\ttimekeeping_apply_adjustment(tk, offset, mult - tk->tkr_mono.mult);\n\n\tif (unlikely(tk->tkr_mono.clock->maxadj &&\n\t\t(abs(tk->tkr_mono.mult - tk->tkr_mono.clock->mult)\n\t\t\t> tk->tkr_mono.clock->maxadj))) {\n\t\tprintk_once(KERN_WARNING\n\t\t\t\"Adjusting %s more than 11%% (%ld vs %ld)\\n\",\n\t\t\ttk->tkr_mono.clock->name, (long)tk->tkr_mono.mult,\n\t\t\t(long)tk->tkr_mono.clock->mult + tk->tkr_mono.clock->maxadj);\n\t}\n\n\t \n\tif (unlikely((s64)tk->tkr_mono.xtime_nsec < 0)) {\n\t\ttk->tkr_mono.xtime_nsec += (u64)NSEC_PER_SEC <<\n\t\t\t\t\t\t\ttk->tkr_mono.shift;\n\t\ttk->xtime_sec--;\n\t\ttk->skip_second_overflow = 1;\n\t}\n}\n\n \nstatic inline unsigned int accumulate_nsecs_to_secs(struct timekeeper *tk)\n{\n\tu64 nsecps = (u64)NSEC_PER_SEC << tk->tkr_mono.shift;\n\tunsigned int clock_set = 0;\n\n\twhile (tk->tkr_mono.xtime_nsec >= nsecps) {\n\t\tint leap;\n\n\t\ttk->tkr_mono.xtime_nsec -= nsecps;\n\t\ttk->xtime_sec++;\n\n\t\t \n\t\tif (unlikely(tk->skip_second_overflow)) {\n\t\t\ttk->skip_second_overflow = 0;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tleap = second_overflow(tk->xtime_sec);\n\t\tif (unlikely(leap)) {\n\t\t\tstruct timespec64 ts;\n\n\t\t\ttk->xtime_sec += leap;\n\n\t\t\tts.tv_sec = leap;\n\t\t\tts.tv_nsec = 0;\n\t\t\ttk_set_wall_to_mono(tk,\n\t\t\t\ttimespec64_sub(tk->wall_to_monotonic, ts));\n\n\t\t\t__timekeeping_set_tai_offset(tk, tk->tai_offset - leap);\n\n\t\t\tclock_set = TK_CLOCK_WAS_SET;\n\t\t}\n\t}\n\treturn clock_set;\n}\n\n \nstatic u64 logarithmic_accumulation(struct timekeeper *tk, u64 offset,\n\t\t\t\t    u32 shift, unsigned int *clock_set)\n{\n\tu64 interval = tk->cycle_interval << shift;\n\tu64 snsec_per_sec;\n\n\t \n\tif (offset < interval)\n\t\treturn offset;\n\n\t \n\toffset -= interval;\n\ttk->tkr_mono.cycle_last += interval;\n\ttk->tkr_raw.cycle_last  += interval;\n\n\ttk->tkr_mono.xtime_nsec += tk->xtime_interval << shift;\n\t*clock_set |= accumulate_nsecs_to_secs(tk);\n\n\t \n\ttk->tkr_raw.xtime_nsec += tk->raw_interval << shift;\n\tsnsec_per_sec = (u64)NSEC_PER_SEC << tk->tkr_raw.shift;\n\twhile (tk->tkr_raw.xtime_nsec >= snsec_per_sec) {\n\t\ttk->tkr_raw.xtime_nsec -= snsec_per_sec;\n\t\ttk->raw_sec++;\n\t}\n\n\t \n\ttk->ntp_error += tk->ntp_tick << shift;\n\ttk->ntp_error -= (tk->xtime_interval + tk->xtime_remainder) <<\n\t\t\t\t\t\t(tk->ntp_error_shift + shift);\n\n\treturn offset;\n}\n\n \nstatic bool timekeeping_advance(enum timekeeping_adv_mode mode)\n{\n\tstruct timekeeper *real_tk = &tk_core.timekeeper;\n\tstruct timekeeper *tk = &shadow_timekeeper;\n\tu64 offset;\n\tint shift = 0, maxshift;\n\tunsigned int clock_set = 0;\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&timekeeper_lock, flags);\n\n\t \n\tif (unlikely(timekeeping_suspended))\n\t\tgoto out;\n\n\toffset = clocksource_delta(tk_clock_read(&tk->tkr_mono),\n\t\t\t\t   tk->tkr_mono.cycle_last, tk->tkr_mono.mask);\n\n\t \n\tif (offset < real_tk->cycle_interval && mode == TK_ADV_TICK)\n\t\tgoto out;\n\n\t \n\ttimekeeping_check_update(tk, offset);\n\n\t \n\tshift = ilog2(offset) - ilog2(tk->cycle_interval);\n\tshift = max(0, shift);\n\t \n\tmaxshift = (64 - (ilog2(ntp_tick_length())+1)) - 1;\n\tshift = min(shift, maxshift);\n\twhile (offset >= tk->cycle_interval) {\n\t\toffset = logarithmic_accumulation(tk, offset, shift,\n\t\t\t\t\t\t\t&clock_set);\n\t\tif (offset < tk->cycle_interval<<shift)\n\t\t\tshift--;\n\t}\n\n\t \n\ttimekeeping_adjust(tk, offset);\n\n\t \n\tclock_set |= accumulate_nsecs_to_secs(tk);\n\n\twrite_seqcount_begin(&tk_core.seq);\n\t \n\ttimekeeping_update(tk, clock_set);\n\tmemcpy(real_tk, tk, sizeof(*tk));\n\t \n\twrite_seqcount_end(&tk_core.seq);\nout:\n\traw_spin_unlock_irqrestore(&timekeeper_lock, flags);\n\n\treturn !!clock_set;\n}\n\n \nvoid update_wall_time(void)\n{\n\tif (timekeeping_advance(TK_ADV_TICK))\n\t\tclock_was_set_delayed();\n}\n\n \nvoid getboottime64(struct timespec64 *ts)\n{\n\tstruct timekeeper *tk = &tk_core.timekeeper;\n\tktime_t t = ktime_sub(tk->offs_real, tk->offs_boot);\n\n\t*ts = ktime_to_timespec64(t);\n}\nEXPORT_SYMBOL_GPL(getboottime64);\n\nvoid ktime_get_coarse_real_ts64(struct timespec64 *ts)\n{\n\tstruct timekeeper *tk = &tk_core.timekeeper;\n\tunsigned int seq;\n\n\tdo {\n\t\tseq = read_seqcount_begin(&tk_core.seq);\n\n\t\t*ts = tk_xtime(tk);\n\t} while (read_seqcount_retry(&tk_core.seq, seq));\n}\nEXPORT_SYMBOL(ktime_get_coarse_real_ts64);\n\nvoid ktime_get_coarse_ts64(struct timespec64 *ts)\n{\n\tstruct timekeeper *tk = &tk_core.timekeeper;\n\tstruct timespec64 now, mono;\n\tunsigned int seq;\n\n\tdo {\n\t\tseq = read_seqcount_begin(&tk_core.seq);\n\n\t\tnow = tk_xtime(tk);\n\t\tmono = tk->wall_to_monotonic;\n\t} while (read_seqcount_retry(&tk_core.seq, seq));\n\n\tset_normalized_timespec64(ts, now.tv_sec + mono.tv_sec,\n\t\t\t\tnow.tv_nsec + mono.tv_nsec);\n}\nEXPORT_SYMBOL(ktime_get_coarse_ts64);\n\n \nvoid do_timer(unsigned long ticks)\n{\n\tjiffies_64 += ticks;\n\tcalc_global_load();\n}\n\n \nktime_t ktime_get_update_offsets_now(unsigned int *cwsseq, ktime_t *offs_real,\n\t\t\t\t     ktime_t *offs_boot, ktime_t *offs_tai)\n{\n\tstruct timekeeper *tk = &tk_core.timekeeper;\n\tunsigned int seq;\n\tktime_t base;\n\tu64 nsecs;\n\n\tdo {\n\t\tseq = read_seqcount_begin(&tk_core.seq);\n\n\t\tbase = tk->tkr_mono.base;\n\t\tnsecs = timekeeping_get_ns(&tk->tkr_mono);\n\t\tbase = ktime_add_ns(base, nsecs);\n\n\t\tif (*cwsseq != tk->clock_was_set_seq) {\n\t\t\t*cwsseq = tk->clock_was_set_seq;\n\t\t\t*offs_real = tk->offs_real;\n\t\t\t*offs_boot = tk->offs_boot;\n\t\t\t*offs_tai = tk->offs_tai;\n\t\t}\n\n\t\t \n\t\tif (unlikely(base >= tk->next_leap_ktime))\n\t\t\t*offs_real = ktime_sub(tk->offs_real, ktime_set(1, 0));\n\n\t} while (read_seqcount_retry(&tk_core.seq, seq));\n\n\treturn base;\n}\n\n \nstatic int timekeeping_validate_timex(const struct __kernel_timex *txc)\n{\n\tif (txc->modes & ADJ_ADJTIME) {\n\t\t \n\t\tif (!(txc->modes & ADJ_OFFSET_SINGLESHOT))\n\t\t\treturn -EINVAL;\n\t\tif (!(txc->modes & ADJ_OFFSET_READONLY) &&\n\t\t    !capable(CAP_SYS_TIME))\n\t\t\treturn -EPERM;\n\t} else {\n\t\t \n\t\tif (txc->modes && !capable(CAP_SYS_TIME))\n\t\t\treturn -EPERM;\n\t\t \n\t\tif (txc->modes & ADJ_TICK &&\n\t\t    (txc->tick <  900000/USER_HZ ||\n\t\t     txc->tick > 1100000/USER_HZ))\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (txc->modes & ADJ_SETOFFSET) {\n\t\t \n\t\tif (!capable(CAP_SYS_TIME))\n\t\t\treturn -EPERM;\n\n\t\t \n\t\tif (txc->time.tv_usec < 0)\n\t\t\treturn -EINVAL;\n\n\t\tif (txc->modes & ADJ_NANO) {\n\t\t\tif (txc->time.tv_usec >= NSEC_PER_SEC)\n\t\t\t\treturn -EINVAL;\n\t\t} else {\n\t\t\tif (txc->time.tv_usec >= USEC_PER_SEC)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t \n\tif ((txc->modes & ADJ_FREQUENCY) && (BITS_PER_LONG == 64)) {\n\t\tif (LLONG_MIN / PPM_SCALE > txc->freq)\n\t\t\treturn -EINVAL;\n\t\tif (LLONG_MAX / PPM_SCALE < txc->freq)\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\n \nunsigned long random_get_entropy_fallback(void)\n{\n\tstruct tk_read_base *tkr = &tk_core.timekeeper.tkr_mono;\n\tstruct clocksource *clock = READ_ONCE(tkr->clock);\n\n\tif (unlikely(timekeeping_suspended || !clock))\n\t\treturn 0;\n\treturn clock->read(clock);\n}\nEXPORT_SYMBOL_GPL(random_get_entropy_fallback);\n\n \nint do_adjtimex(struct __kernel_timex *txc)\n{\n\tstruct timekeeper *tk = &tk_core.timekeeper;\n\tstruct audit_ntp_data ad;\n\tbool clock_set = false;\n\tstruct timespec64 ts;\n\tunsigned long flags;\n\ts32 orig_tai, tai;\n\tint ret;\n\n\t \n\tret = timekeeping_validate_timex(txc);\n\tif (ret)\n\t\treturn ret;\n\tadd_device_randomness(txc, sizeof(*txc));\n\n\tif (txc->modes & ADJ_SETOFFSET) {\n\t\tstruct timespec64 delta;\n\t\tdelta.tv_sec  = txc->time.tv_sec;\n\t\tdelta.tv_nsec = txc->time.tv_usec;\n\t\tif (!(txc->modes & ADJ_NANO))\n\t\t\tdelta.tv_nsec *= 1000;\n\t\tret = timekeeping_inject_offset(&delta);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\taudit_tk_injoffset(delta);\n\t}\n\n\taudit_ntp_init(&ad);\n\n\tktime_get_real_ts64(&ts);\n\tadd_device_randomness(&ts, sizeof(ts));\n\n\traw_spin_lock_irqsave(&timekeeper_lock, flags);\n\twrite_seqcount_begin(&tk_core.seq);\n\n\torig_tai = tai = tk->tai_offset;\n\tret = __do_adjtimex(txc, &ts, &tai, &ad);\n\n\tif (tai != orig_tai) {\n\t\t__timekeeping_set_tai_offset(tk, tai);\n\t\ttimekeeping_update(tk, TK_MIRROR | TK_CLOCK_WAS_SET);\n\t\tclock_set = true;\n\t}\n\ttk_update_leap_state(tk);\n\n\twrite_seqcount_end(&tk_core.seq);\n\traw_spin_unlock_irqrestore(&timekeeper_lock, flags);\n\n\taudit_ntp_log(&ad);\n\n\t \n\tif (txc->modes & (ADJ_FREQUENCY | ADJ_TICK))\n\t\tclock_set |= timekeeping_advance(TK_ADV_FREQ);\n\n\tif (clock_set)\n\t\tclock_was_set(CLOCK_REALTIME);\n\n\tntp_notify_cmos_timer();\n\n\treturn ret;\n}\n\n#ifdef CONFIG_NTP_PPS\n \nvoid hardpps(const struct timespec64 *phase_ts, const struct timespec64 *raw_ts)\n{\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&timekeeper_lock, flags);\n\twrite_seqcount_begin(&tk_core.seq);\n\n\t__hardpps(phase_ts, raw_ts);\n\n\twrite_seqcount_end(&tk_core.seq);\n\traw_spin_unlock_irqrestore(&timekeeper_lock, flags);\n}\nEXPORT_SYMBOL(hardpps);\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}