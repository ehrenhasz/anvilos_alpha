{
  "module_name": "hrtimer.c",
  "hash_id": "a9288bb4399f9745f8417bc041f96d025468581c29b67c1560dde6e4421205c0",
  "original_prompt": "Ingested from linux-6.6.14/kernel/time/hrtimer.c",
  "human_readable_source": "\n \n\n#include <linux/cpu.h>\n#include <linux/export.h>\n#include <linux/percpu.h>\n#include <linux/hrtimer.h>\n#include <linux/notifier.h>\n#include <linux/syscalls.h>\n#include <linux/interrupt.h>\n#include <linux/tick.h>\n#include <linux/err.h>\n#include <linux/debugobjects.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/debug.h>\n#include <linux/timer.h>\n#include <linux/freezer.h>\n#include <linux/compat.h>\n\n#include <linux/uaccess.h>\n\n#include <trace/events/timer.h>\n\n#include \"tick-internal.h\"\n\n \n#define MASK_SHIFT\t\t(HRTIMER_BASE_MONOTONIC_SOFT)\n#define HRTIMER_ACTIVE_HARD\t((1U << MASK_SHIFT) - 1)\n#define HRTIMER_ACTIVE_SOFT\t(HRTIMER_ACTIVE_HARD << MASK_SHIFT)\n#define HRTIMER_ACTIVE_ALL\t(HRTIMER_ACTIVE_SOFT | HRTIMER_ACTIVE_HARD)\n\n \nDEFINE_PER_CPU(struct hrtimer_cpu_base, hrtimer_bases) =\n{\n\t.lock = __RAW_SPIN_LOCK_UNLOCKED(hrtimer_bases.lock),\n\t.clock_base =\n\t{\n\t\t{\n\t\t\t.index = HRTIMER_BASE_MONOTONIC,\n\t\t\t.clockid = CLOCK_MONOTONIC,\n\t\t\t.get_time = &ktime_get,\n\t\t},\n\t\t{\n\t\t\t.index = HRTIMER_BASE_REALTIME,\n\t\t\t.clockid = CLOCK_REALTIME,\n\t\t\t.get_time = &ktime_get_real,\n\t\t},\n\t\t{\n\t\t\t.index = HRTIMER_BASE_BOOTTIME,\n\t\t\t.clockid = CLOCK_BOOTTIME,\n\t\t\t.get_time = &ktime_get_boottime,\n\t\t},\n\t\t{\n\t\t\t.index = HRTIMER_BASE_TAI,\n\t\t\t.clockid = CLOCK_TAI,\n\t\t\t.get_time = &ktime_get_clocktai,\n\t\t},\n\t\t{\n\t\t\t.index = HRTIMER_BASE_MONOTONIC_SOFT,\n\t\t\t.clockid = CLOCK_MONOTONIC,\n\t\t\t.get_time = &ktime_get,\n\t\t},\n\t\t{\n\t\t\t.index = HRTIMER_BASE_REALTIME_SOFT,\n\t\t\t.clockid = CLOCK_REALTIME,\n\t\t\t.get_time = &ktime_get_real,\n\t\t},\n\t\t{\n\t\t\t.index = HRTIMER_BASE_BOOTTIME_SOFT,\n\t\t\t.clockid = CLOCK_BOOTTIME,\n\t\t\t.get_time = &ktime_get_boottime,\n\t\t},\n\t\t{\n\t\t\t.index = HRTIMER_BASE_TAI_SOFT,\n\t\t\t.clockid = CLOCK_TAI,\n\t\t\t.get_time = &ktime_get_clocktai,\n\t\t},\n\t}\n};\n\nstatic const int hrtimer_clock_to_base_table[MAX_CLOCKS] = {\n\t \n\t[0 ... MAX_CLOCKS - 1]\t= HRTIMER_MAX_CLOCK_BASES,\n\n\t[CLOCK_REALTIME]\t= HRTIMER_BASE_REALTIME,\n\t[CLOCK_MONOTONIC]\t= HRTIMER_BASE_MONOTONIC,\n\t[CLOCK_BOOTTIME]\t= HRTIMER_BASE_BOOTTIME,\n\t[CLOCK_TAI]\t\t= HRTIMER_BASE_TAI,\n};\n\n \n#ifdef CONFIG_SMP\n\n \nstatic struct hrtimer_cpu_base migration_cpu_base = {\n\t.clock_base = { {\n\t\t.cpu_base = &migration_cpu_base,\n\t\t.seq      = SEQCNT_RAW_SPINLOCK_ZERO(migration_cpu_base.seq,\n\t\t\t\t\t\t     &migration_cpu_base.lock),\n\t}, },\n};\n\n#define migration_base\tmigration_cpu_base.clock_base[0]\n\nstatic inline bool is_migration_base(struct hrtimer_clock_base *base)\n{\n\treturn base == &migration_base;\n}\n\n \nstatic\nstruct hrtimer_clock_base *lock_hrtimer_base(const struct hrtimer *timer,\n\t\t\t\t\t     unsigned long *flags)\n\t__acquires(&timer->base->lock)\n{\n\tstruct hrtimer_clock_base *base;\n\n\tfor (;;) {\n\t\tbase = READ_ONCE(timer->base);\n\t\tif (likely(base != &migration_base)) {\n\t\t\traw_spin_lock_irqsave(&base->cpu_base->lock, *flags);\n\t\t\tif (likely(base == timer->base))\n\t\t\t\treturn base;\n\t\t\t \n\t\t\traw_spin_unlock_irqrestore(&base->cpu_base->lock, *flags);\n\t\t}\n\t\tcpu_relax();\n\t}\n}\n\n \nstatic int\nhrtimer_check_target(struct hrtimer *timer, struct hrtimer_clock_base *new_base)\n{\n\tktime_t expires;\n\n\texpires = ktime_sub(hrtimer_get_expires(timer), new_base->offset);\n\treturn expires < new_base->cpu_base->expires_next;\n}\n\nstatic inline\nstruct hrtimer_cpu_base *get_target_base(struct hrtimer_cpu_base *base,\n\t\t\t\t\t int pinned)\n{\n#if defined(CONFIG_SMP) && defined(CONFIG_NO_HZ_COMMON)\n\tif (static_branch_likely(&timers_migration_enabled) && !pinned)\n\t\treturn &per_cpu(hrtimer_bases, get_nohz_timer_target());\n#endif\n\treturn base;\n}\n\n \nstatic inline struct hrtimer_clock_base *\nswitch_hrtimer_base(struct hrtimer *timer, struct hrtimer_clock_base *base,\n\t\t    int pinned)\n{\n\tstruct hrtimer_cpu_base *new_cpu_base, *this_cpu_base;\n\tstruct hrtimer_clock_base *new_base;\n\tint basenum = base->index;\n\n\tthis_cpu_base = this_cpu_ptr(&hrtimer_bases);\n\tnew_cpu_base = get_target_base(this_cpu_base, pinned);\nagain:\n\tnew_base = &new_cpu_base->clock_base[basenum];\n\n\tif (base != new_base) {\n\t\t \n\t\tif (unlikely(hrtimer_callback_running(timer)))\n\t\t\treturn base;\n\n\t\t \n\t\tWRITE_ONCE(timer->base, &migration_base);\n\t\traw_spin_unlock(&base->cpu_base->lock);\n\t\traw_spin_lock(&new_base->cpu_base->lock);\n\n\t\tif (new_cpu_base != this_cpu_base &&\n\t\t    hrtimer_check_target(timer, new_base)) {\n\t\t\traw_spin_unlock(&new_base->cpu_base->lock);\n\t\t\traw_spin_lock(&base->cpu_base->lock);\n\t\t\tnew_cpu_base = this_cpu_base;\n\t\t\tWRITE_ONCE(timer->base, base);\n\t\t\tgoto again;\n\t\t}\n\t\tWRITE_ONCE(timer->base, new_base);\n\t} else {\n\t\tif (new_cpu_base != this_cpu_base &&\n\t\t    hrtimer_check_target(timer, new_base)) {\n\t\t\tnew_cpu_base = this_cpu_base;\n\t\t\tgoto again;\n\t\t}\n\t}\n\treturn new_base;\n}\n\n#else  \n\nstatic inline bool is_migration_base(struct hrtimer_clock_base *base)\n{\n\treturn false;\n}\n\nstatic inline struct hrtimer_clock_base *\nlock_hrtimer_base(const struct hrtimer *timer, unsigned long *flags)\n\t__acquires(&timer->base->cpu_base->lock)\n{\n\tstruct hrtimer_clock_base *base = timer->base;\n\n\traw_spin_lock_irqsave(&base->cpu_base->lock, *flags);\n\n\treturn base;\n}\n\n# define switch_hrtimer_base(t, b, p)\t(b)\n\n#endif\t \n\n \n#if BITS_PER_LONG < 64\n \ns64 __ktime_divns(const ktime_t kt, s64 div)\n{\n\tint sft = 0;\n\ts64 dclc;\n\tu64 tmp;\n\n\tdclc = ktime_to_ns(kt);\n\ttmp = dclc < 0 ? -dclc : dclc;\n\n\t \n\twhile (div >> 32) {\n\t\tsft++;\n\t\tdiv >>= 1;\n\t}\n\ttmp >>= sft;\n\tdo_div(tmp, (u32) div);\n\treturn dclc < 0 ? -tmp : tmp;\n}\nEXPORT_SYMBOL_GPL(__ktime_divns);\n#endif  \n\n \nktime_t ktime_add_safe(const ktime_t lhs, const ktime_t rhs)\n{\n\tktime_t res = ktime_add_unsafe(lhs, rhs);\n\n\t \n\tif (res < 0 || res < lhs || res < rhs)\n\t\tres = ktime_set(KTIME_SEC_MAX, 0);\n\n\treturn res;\n}\n\nEXPORT_SYMBOL_GPL(ktime_add_safe);\n\n#ifdef CONFIG_DEBUG_OBJECTS_TIMERS\n\nstatic const struct debug_obj_descr hrtimer_debug_descr;\n\nstatic void *hrtimer_debug_hint(void *addr)\n{\n\treturn ((struct hrtimer *) addr)->function;\n}\n\n \nstatic bool hrtimer_fixup_init(void *addr, enum debug_obj_state state)\n{\n\tstruct hrtimer *timer = addr;\n\n\tswitch (state) {\n\tcase ODEBUG_STATE_ACTIVE:\n\t\thrtimer_cancel(timer);\n\t\tdebug_object_init(timer, &hrtimer_debug_descr);\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n \nstatic bool hrtimer_fixup_activate(void *addr, enum debug_obj_state state)\n{\n\tswitch (state) {\n\tcase ODEBUG_STATE_ACTIVE:\n\t\tWARN_ON(1);\n\t\tfallthrough;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n \nstatic bool hrtimer_fixup_free(void *addr, enum debug_obj_state state)\n{\n\tstruct hrtimer *timer = addr;\n\n\tswitch (state) {\n\tcase ODEBUG_STATE_ACTIVE:\n\t\thrtimer_cancel(timer);\n\t\tdebug_object_free(timer, &hrtimer_debug_descr);\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic const struct debug_obj_descr hrtimer_debug_descr = {\n\t.name\t\t= \"hrtimer\",\n\t.debug_hint\t= hrtimer_debug_hint,\n\t.fixup_init\t= hrtimer_fixup_init,\n\t.fixup_activate\t= hrtimer_fixup_activate,\n\t.fixup_free\t= hrtimer_fixup_free,\n};\n\nstatic inline void debug_hrtimer_init(struct hrtimer *timer)\n{\n\tdebug_object_init(timer, &hrtimer_debug_descr);\n}\n\nstatic inline void debug_hrtimer_activate(struct hrtimer *timer,\n\t\t\t\t\t  enum hrtimer_mode mode)\n{\n\tdebug_object_activate(timer, &hrtimer_debug_descr);\n}\n\nstatic inline void debug_hrtimer_deactivate(struct hrtimer *timer)\n{\n\tdebug_object_deactivate(timer, &hrtimer_debug_descr);\n}\n\nstatic void __hrtimer_init(struct hrtimer *timer, clockid_t clock_id,\n\t\t\t   enum hrtimer_mode mode);\n\nvoid hrtimer_init_on_stack(struct hrtimer *timer, clockid_t clock_id,\n\t\t\t   enum hrtimer_mode mode)\n{\n\tdebug_object_init_on_stack(timer, &hrtimer_debug_descr);\n\t__hrtimer_init(timer, clock_id, mode);\n}\nEXPORT_SYMBOL_GPL(hrtimer_init_on_stack);\n\nstatic void __hrtimer_init_sleeper(struct hrtimer_sleeper *sl,\n\t\t\t\t   clockid_t clock_id, enum hrtimer_mode mode);\n\nvoid hrtimer_init_sleeper_on_stack(struct hrtimer_sleeper *sl,\n\t\t\t\t   clockid_t clock_id, enum hrtimer_mode mode)\n{\n\tdebug_object_init_on_stack(&sl->timer, &hrtimer_debug_descr);\n\t__hrtimer_init_sleeper(sl, clock_id, mode);\n}\nEXPORT_SYMBOL_GPL(hrtimer_init_sleeper_on_stack);\n\nvoid destroy_hrtimer_on_stack(struct hrtimer *timer)\n{\n\tdebug_object_free(timer, &hrtimer_debug_descr);\n}\nEXPORT_SYMBOL_GPL(destroy_hrtimer_on_stack);\n\n#else\n\nstatic inline void debug_hrtimer_init(struct hrtimer *timer) { }\nstatic inline void debug_hrtimer_activate(struct hrtimer *timer,\n\t\t\t\t\t  enum hrtimer_mode mode) { }\nstatic inline void debug_hrtimer_deactivate(struct hrtimer *timer) { }\n#endif\n\nstatic inline void\ndebug_init(struct hrtimer *timer, clockid_t clockid,\n\t   enum hrtimer_mode mode)\n{\n\tdebug_hrtimer_init(timer);\n\ttrace_hrtimer_init(timer, clockid, mode);\n}\n\nstatic inline void debug_activate(struct hrtimer *timer,\n\t\t\t\t  enum hrtimer_mode mode)\n{\n\tdebug_hrtimer_activate(timer, mode);\n\ttrace_hrtimer_start(timer, mode);\n}\n\nstatic inline void debug_deactivate(struct hrtimer *timer)\n{\n\tdebug_hrtimer_deactivate(timer);\n\ttrace_hrtimer_cancel(timer);\n}\n\nstatic struct hrtimer_clock_base *\n__next_base(struct hrtimer_cpu_base *cpu_base, unsigned int *active)\n{\n\tunsigned int idx;\n\n\tif (!*active)\n\t\treturn NULL;\n\n\tidx = __ffs(*active);\n\t*active &= ~(1U << idx);\n\n\treturn &cpu_base->clock_base[idx];\n}\n\n#define for_each_active_base(base, cpu_base, active)\t\\\n\twhile ((base = __next_base((cpu_base), &(active))))\n\nstatic ktime_t __hrtimer_next_event_base(struct hrtimer_cpu_base *cpu_base,\n\t\t\t\t\t const struct hrtimer *exclude,\n\t\t\t\t\t unsigned int active,\n\t\t\t\t\t ktime_t expires_next)\n{\n\tstruct hrtimer_clock_base *base;\n\tktime_t expires;\n\n\tfor_each_active_base(base, cpu_base, active) {\n\t\tstruct timerqueue_node *next;\n\t\tstruct hrtimer *timer;\n\n\t\tnext = timerqueue_getnext(&base->active);\n\t\ttimer = container_of(next, struct hrtimer, node);\n\t\tif (timer == exclude) {\n\t\t\t \n\t\t\tnext = timerqueue_iterate_next(next);\n\t\t\tif (!next)\n\t\t\t\tcontinue;\n\n\t\t\ttimer = container_of(next, struct hrtimer, node);\n\t\t}\n\t\texpires = ktime_sub(hrtimer_get_expires(timer), base->offset);\n\t\tif (expires < expires_next) {\n\t\t\texpires_next = expires;\n\n\t\t\t \n\t\t\tif (exclude)\n\t\t\t\tcontinue;\n\n\t\t\tif (timer->is_soft)\n\t\t\t\tcpu_base->softirq_next_timer = timer;\n\t\t\telse\n\t\t\t\tcpu_base->next_timer = timer;\n\t\t}\n\t}\n\t \n\tif (expires_next < 0)\n\t\texpires_next = 0;\n\treturn expires_next;\n}\n\n \nstatic ktime_t\n__hrtimer_get_next_event(struct hrtimer_cpu_base *cpu_base, unsigned int active_mask)\n{\n\tunsigned int active;\n\tstruct hrtimer *next_timer = NULL;\n\tktime_t expires_next = KTIME_MAX;\n\n\tif (!cpu_base->softirq_activated && (active_mask & HRTIMER_ACTIVE_SOFT)) {\n\t\tactive = cpu_base->active_bases & HRTIMER_ACTIVE_SOFT;\n\t\tcpu_base->softirq_next_timer = NULL;\n\t\texpires_next = __hrtimer_next_event_base(cpu_base, NULL,\n\t\t\t\t\t\t\t active, KTIME_MAX);\n\n\t\tnext_timer = cpu_base->softirq_next_timer;\n\t}\n\n\tif (active_mask & HRTIMER_ACTIVE_HARD) {\n\t\tactive = cpu_base->active_bases & HRTIMER_ACTIVE_HARD;\n\t\tcpu_base->next_timer = next_timer;\n\t\texpires_next = __hrtimer_next_event_base(cpu_base, NULL, active,\n\t\t\t\t\t\t\t expires_next);\n\t}\n\n\treturn expires_next;\n}\n\nstatic ktime_t hrtimer_update_next_event(struct hrtimer_cpu_base *cpu_base)\n{\n\tktime_t expires_next, soft = KTIME_MAX;\n\n\t \n\tif (!cpu_base->softirq_activated) {\n\t\tsoft = __hrtimer_get_next_event(cpu_base, HRTIMER_ACTIVE_SOFT);\n\t\t \n\t\tcpu_base->softirq_expires_next = soft;\n\t}\n\n\texpires_next = __hrtimer_get_next_event(cpu_base, HRTIMER_ACTIVE_HARD);\n\t \n\tif (expires_next > soft) {\n\t\tcpu_base->next_timer = cpu_base->softirq_next_timer;\n\t\texpires_next = soft;\n\t}\n\n\treturn expires_next;\n}\n\nstatic inline ktime_t hrtimer_update_base(struct hrtimer_cpu_base *base)\n{\n\tktime_t *offs_real = &base->clock_base[HRTIMER_BASE_REALTIME].offset;\n\tktime_t *offs_boot = &base->clock_base[HRTIMER_BASE_BOOTTIME].offset;\n\tktime_t *offs_tai = &base->clock_base[HRTIMER_BASE_TAI].offset;\n\n\tktime_t now = ktime_get_update_offsets_now(&base->clock_was_set_seq,\n\t\t\t\t\t    offs_real, offs_boot, offs_tai);\n\n\tbase->clock_base[HRTIMER_BASE_REALTIME_SOFT].offset = *offs_real;\n\tbase->clock_base[HRTIMER_BASE_BOOTTIME_SOFT].offset = *offs_boot;\n\tbase->clock_base[HRTIMER_BASE_TAI_SOFT].offset = *offs_tai;\n\n\treturn now;\n}\n\n \nstatic inline int __hrtimer_hres_active(struct hrtimer_cpu_base *cpu_base)\n{\n\treturn IS_ENABLED(CONFIG_HIGH_RES_TIMERS) ?\n\t\tcpu_base->hres_active : 0;\n}\n\nstatic inline int hrtimer_hres_active(void)\n{\n\treturn __hrtimer_hres_active(this_cpu_ptr(&hrtimer_bases));\n}\n\nstatic void __hrtimer_reprogram(struct hrtimer_cpu_base *cpu_base,\n\t\t\t\tstruct hrtimer *next_timer,\n\t\t\t\tktime_t expires_next)\n{\n\tcpu_base->expires_next = expires_next;\n\n\t \n\tif (!__hrtimer_hres_active(cpu_base) || cpu_base->hang_detected)\n\t\treturn;\n\n\ttick_program_event(expires_next, 1);\n}\n\n \nstatic void\nhrtimer_force_reprogram(struct hrtimer_cpu_base *cpu_base, int skip_equal)\n{\n\tktime_t expires_next;\n\n\texpires_next = hrtimer_update_next_event(cpu_base);\n\n\tif (skip_equal && expires_next == cpu_base->expires_next)\n\t\treturn;\n\n\t__hrtimer_reprogram(cpu_base, cpu_base->next_timer, expires_next);\n}\n\n \n#ifdef CONFIG_HIGH_RES_TIMERS\n\n \nstatic bool hrtimer_hres_enabled __read_mostly  = true;\nunsigned int hrtimer_resolution __read_mostly = LOW_RES_NSEC;\nEXPORT_SYMBOL_GPL(hrtimer_resolution);\n\n \nstatic int __init setup_hrtimer_hres(char *str)\n{\n\treturn (kstrtobool(str, &hrtimer_hres_enabled) == 0);\n}\n\n__setup(\"highres=\", setup_hrtimer_hres);\n\n \nstatic inline int hrtimer_is_hres_enabled(void)\n{\n\treturn hrtimer_hres_enabled;\n}\n\nstatic void retrigger_next_event(void *arg);\n\n \nstatic void hrtimer_switch_to_hres(void)\n{\n\tstruct hrtimer_cpu_base *base = this_cpu_ptr(&hrtimer_bases);\n\n\tif (tick_init_highres()) {\n\t\tpr_warn(\"Could not switch to high resolution mode on CPU %u\\n\",\n\t\t\tbase->cpu);\n\t\treturn;\n\t}\n\tbase->hres_active = 1;\n\thrtimer_resolution = HIGH_RES_NSEC;\n\n\ttick_setup_sched_timer();\n\t \n\tretrigger_next_event(NULL);\n}\n\n#else\n\nstatic inline int hrtimer_is_hres_enabled(void) { return 0; }\nstatic inline void hrtimer_switch_to_hres(void) { }\n\n#endif  \n \nstatic void retrigger_next_event(void *arg)\n{\n\tstruct hrtimer_cpu_base *base = this_cpu_ptr(&hrtimer_bases);\n\n\t \n\tif (!__hrtimer_hres_active(base) && !tick_nohz_active)\n\t\treturn;\n\n\traw_spin_lock(&base->lock);\n\thrtimer_update_base(base);\n\tif (__hrtimer_hres_active(base))\n\t\thrtimer_force_reprogram(base, 0);\n\telse\n\t\thrtimer_update_next_event(base);\n\traw_spin_unlock(&base->lock);\n}\n\n \nstatic void hrtimer_reprogram(struct hrtimer *timer, bool reprogram)\n{\n\tstruct hrtimer_cpu_base *cpu_base = this_cpu_ptr(&hrtimer_bases);\n\tstruct hrtimer_clock_base *base = timer->base;\n\tktime_t expires = ktime_sub(hrtimer_get_expires(timer), base->offset);\n\n\tWARN_ON_ONCE(hrtimer_get_expires_tv64(timer) < 0);\n\n\t \n\tif (expires < 0)\n\t\texpires = 0;\n\n\tif (timer->is_soft) {\n\t\t \n\t\tstruct hrtimer_cpu_base *timer_cpu_base = base->cpu_base;\n\n\t\tif (timer_cpu_base->softirq_activated)\n\t\t\treturn;\n\n\t\tif (!ktime_before(expires, timer_cpu_base->softirq_expires_next))\n\t\t\treturn;\n\n\t\ttimer_cpu_base->softirq_next_timer = timer;\n\t\ttimer_cpu_base->softirq_expires_next = expires;\n\n\t\tif (!ktime_before(expires, timer_cpu_base->expires_next) ||\n\t\t    !reprogram)\n\t\t\treturn;\n\t}\n\n\t \n\tif (base->cpu_base != cpu_base)\n\t\treturn;\n\n\tif (expires >= cpu_base->expires_next)\n\t\treturn;\n\n\t \n\tif (cpu_base->in_hrtirq)\n\t\treturn;\n\n\tcpu_base->next_timer = timer;\n\n\t__hrtimer_reprogram(cpu_base, timer, expires);\n}\n\nstatic bool update_needs_ipi(struct hrtimer_cpu_base *cpu_base,\n\t\t\t     unsigned int active)\n{\n\tstruct hrtimer_clock_base *base;\n\tunsigned int seq;\n\tktime_t expires;\n\n\t \n\tseq = cpu_base->clock_was_set_seq;\n\thrtimer_update_base(cpu_base);\n\n\t \n\tif (seq == cpu_base->clock_was_set_seq)\n\t\treturn false;\n\n\t \n\tif (cpu_base->in_hrtirq)\n\t\treturn false;\n\n\t \n\tactive &= cpu_base->active_bases;\n\n\tfor_each_active_base(base, cpu_base, active) {\n\t\tstruct timerqueue_node *next;\n\n\t\tnext = timerqueue_getnext(&base->active);\n\t\texpires = ktime_sub(next->expires, base->offset);\n\t\tif (expires < cpu_base->expires_next)\n\t\t\treturn true;\n\n\t\t \n\t\tif (base->clockid < HRTIMER_BASE_MONOTONIC_SOFT)\n\t\t\tcontinue;\n\t\tif (cpu_base->softirq_activated)\n\t\t\tcontinue;\n\t\tif (expires < cpu_base->softirq_expires_next)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\n \nvoid clock_was_set(unsigned int bases)\n{\n\tstruct hrtimer_cpu_base *cpu_base = raw_cpu_ptr(&hrtimer_bases);\n\tcpumask_var_t mask;\n\tint cpu;\n\n\tif (!__hrtimer_hres_active(cpu_base) && !tick_nohz_active)\n\t\tgoto out_timerfd;\n\n\tif (!zalloc_cpumask_var(&mask, GFP_KERNEL)) {\n\t\ton_each_cpu(retrigger_next_event, NULL, 1);\n\t\tgoto out_timerfd;\n\t}\n\n\t \n\tcpus_read_lock();\n\tfor_each_online_cpu(cpu) {\n\t\tunsigned long flags;\n\n\t\tcpu_base = &per_cpu(hrtimer_bases, cpu);\n\t\traw_spin_lock_irqsave(&cpu_base->lock, flags);\n\n\t\tif (update_needs_ipi(cpu_base, bases))\n\t\t\tcpumask_set_cpu(cpu, mask);\n\n\t\traw_spin_unlock_irqrestore(&cpu_base->lock, flags);\n\t}\n\n\tpreempt_disable();\n\tsmp_call_function_many(mask, retrigger_next_event, NULL, 1);\n\tpreempt_enable();\n\tcpus_read_unlock();\n\tfree_cpumask_var(mask);\n\nout_timerfd:\n\ttimerfd_clock_was_set();\n}\n\nstatic void clock_was_set_work(struct work_struct *work)\n{\n\tclock_was_set(CLOCK_SET_WALL);\n}\n\nstatic DECLARE_WORK(hrtimer_work, clock_was_set_work);\n\n \nvoid clock_was_set_delayed(void)\n{\n\tschedule_work(&hrtimer_work);\n}\n\n \nvoid hrtimers_resume_local(void)\n{\n\tlockdep_assert_irqs_disabled();\n\t \n\tretrigger_next_event(NULL);\n}\n\n \nstatic inline\nvoid unlock_hrtimer_base(const struct hrtimer *timer, unsigned long *flags)\n\t__releases(&timer->base->cpu_base->lock)\n{\n\traw_spin_unlock_irqrestore(&timer->base->cpu_base->lock, *flags);\n}\n\n \nu64 hrtimer_forward(struct hrtimer *timer, ktime_t now, ktime_t interval)\n{\n\tu64 orun = 1;\n\tktime_t delta;\n\n\tdelta = ktime_sub(now, hrtimer_get_expires(timer));\n\n\tif (delta < 0)\n\t\treturn 0;\n\n\tif (WARN_ON(timer->state & HRTIMER_STATE_ENQUEUED))\n\t\treturn 0;\n\n\tif (interval < hrtimer_resolution)\n\t\tinterval = hrtimer_resolution;\n\n\tif (unlikely(delta >= interval)) {\n\t\ts64 incr = ktime_to_ns(interval);\n\n\t\torun = ktime_divns(delta, incr);\n\t\thrtimer_add_expires_ns(timer, incr * orun);\n\t\tif (hrtimer_get_expires_tv64(timer) > now)\n\t\t\treturn orun;\n\t\t \n\t\torun++;\n\t}\n\thrtimer_add_expires(timer, interval);\n\n\treturn orun;\n}\nEXPORT_SYMBOL_GPL(hrtimer_forward);\n\n \nstatic int enqueue_hrtimer(struct hrtimer *timer,\n\t\t\t   struct hrtimer_clock_base *base,\n\t\t\t   enum hrtimer_mode mode)\n{\n\tdebug_activate(timer, mode);\n\n\tbase->cpu_base->active_bases |= 1 << base->index;\n\n\t \n\tWRITE_ONCE(timer->state, HRTIMER_STATE_ENQUEUED);\n\n\treturn timerqueue_add(&base->active, &timer->node);\n}\n\n \nstatic void __remove_hrtimer(struct hrtimer *timer,\n\t\t\t     struct hrtimer_clock_base *base,\n\t\t\t     u8 newstate, int reprogram)\n{\n\tstruct hrtimer_cpu_base *cpu_base = base->cpu_base;\n\tu8 state = timer->state;\n\n\t \n\tWRITE_ONCE(timer->state, newstate);\n\tif (!(state & HRTIMER_STATE_ENQUEUED))\n\t\treturn;\n\n\tif (!timerqueue_del(&base->active, &timer->node))\n\t\tcpu_base->active_bases &= ~(1 << base->index);\n\n\t \n\tif (reprogram && timer == cpu_base->next_timer)\n\t\thrtimer_force_reprogram(cpu_base, 1);\n}\n\n \nstatic inline int\nremove_hrtimer(struct hrtimer *timer, struct hrtimer_clock_base *base,\n\t       bool restart, bool keep_local)\n{\n\tu8 state = timer->state;\n\n\tif (state & HRTIMER_STATE_ENQUEUED) {\n\t\tbool reprogram;\n\n\t\t \n\t\tdebug_deactivate(timer);\n\t\treprogram = base->cpu_base == this_cpu_ptr(&hrtimer_bases);\n\n\t\t \n\t\tif (!restart)\n\t\t\tstate = HRTIMER_STATE_INACTIVE;\n\t\telse\n\t\t\treprogram &= !keep_local;\n\n\t\t__remove_hrtimer(timer, base, state, reprogram);\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic inline ktime_t hrtimer_update_lowres(struct hrtimer *timer, ktime_t tim,\n\t\t\t\t\t    const enum hrtimer_mode mode)\n{\n#ifdef CONFIG_TIME_LOW_RES\n\t \n\ttimer->is_rel = mode & HRTIMER_MODE_REL;\n\tif (timer->is_rel)\n\t\ttim = ktime_add_safe(tim, hrtimer_resolution);\n#endif\n\treturn tim;\n}\n\nstatic void\nhrtimer_update_softirq_timer(struct hrtimer_cpu_base *cpu_base, bool reprogram)\n{\n\tktime_t expires;\n\n\t \n\texpires = __hrtimer_get_next_event(cpu_base, HRTIMER_ACTIVE_SOFT);\n\n\t \n\tif (expires == KTIME_MAX)\n\t\treturn;\n\n\t \n\thrtimer_reprogram(cpu_base->softirq_next_timer, reprogram);\n}\n\nstatic int __hrtimer_start_range_ns(struct hrtimer *timer, ktime_t tim,\n\t\t\t\t    u64 delta_ns, const enum hrtimer_mode mode,\n\t\t\t\t    struct hrtimer_clock_base *base)\n{\n\tstruct hrtimer_clock_base *new_base;\n\tbool force_local, first;\n\n\t \n\tforce_local = base->cpu_base == this_cpu_ptr(&hrtimer_bases);\n\tforce_local &= base->cpu_base->next_timer == timer;\n\n\t \n\tremove_hrtimer(timer, base, true, force_local);\n\n\tif (mode & HRTIMER_MODE_REL)\n\t\ttim = ktime_add_safe(tim, base->get_time());\n\n\ttim = hrtimer_update_lowres(timer, tim, mode);\n\n\thrtimer_set_expires_range_ns(timer, tim, delta_ns);\n\n\t \n\tif (!force_local) {\n\t\tnew_base = switch_hrtimer_base(timer, base,\n\t\t\t\t\t       mode & HRTIMER_MODE_PINNED);\n\t} else {\n\t\tnew_base = base;\n\t}\n\n\tfirst = enqueue_hrtimer(timer, new_base, mode);\n\tif (!force_local)\n\t\treturn first;\n\n\t \n\thrtimer_force_reprogram(new_base->cpu_base, 1);\n\treturn 0;\n}\n\n \nvoid hrtimer_start_range_ns(struct hrtimer *timer, ktime_t tim,\n\t\t\t    u64 delta_ns, const enum hrtimer_mode mode)\n{\n\tstruct hrtimer_clock_base *base;\n\tunsigned long flags;\n\n\t \n\tif (!IS_ENABLED(CONFIG_PREEMPT_RT))\n\t\tWARN_ON_ONCE(!(mode & HRTIMER_MODE_SOFT) ^ !timer->is_soft);\n\telse\n\t\tWARN_ON_ONCE(!(mode & HRTIMER_MODE_HARD) ^ !timer->is_hard);\n\n\tbase = lock_hrtimer_base(timer, &flags);\n\n\tif (__hrtimer_start_range_ns(timer, tim, delta_ns, mode, base))\n\t\thrtimer_reprogram(timer, true);\n\n\tunlock_hrtimer_base(timer, &flags);\n}\nEXPORT_SYMBOL_GPL(hrtimer_start_range_ns);\n\n \nint hrtimer_try_to_cancel(struct hrtimer *timer)\n{\n\tstruct hrtimer_clock_base *base;\n\tunsigned long flags;\n\tint ret = -1;\n\n\t \n\tif (!hrtimer_active(timer))\n\t\treturn 0;\n\n\tbase = lock_hrtimer_base(timer, &flags);\n\n\tif (!hrtimer_callback_running(timer))\n\t\tret = remove_hrtimer(timer, base, false, false);\n\n\tunlock_hrtimer_base(timer, &flags);\n\n\treturn ret;\n\n}\nEXPORT_SYMBOL_GPL(hrtimer_try_to_cancel);\n\n#ifdef CONFIG_PREEMPT_RT\nstatic void hrtimer_cpu_base_init_expiry_lock(struct hrtimer_cpu_base *base)\n{\n\tspin_lock_init(&base->softirq_expiry_lock);\n}\n\nstatic void hrtimer_cpu_base_lock_expiry(struct hrtimer_cpu_base *base)\n{\n\tspin_lock(&base->softirq_expiry_lock);\n}\n\nstatic void hrtimer_cpu_base_unlock_expiry(struct hrtimer_cpu_base *base)\n{\n\tspin_unlock(&base->softirq_expiry_lock);\n}\n\n \nstatic void hrtimer_sync_wait_running(struct hrtimer_cpu_base *cpu_base,\n\t\t\t\t      unsigned long flags)\n{\n\tif (atomic_read(&cpu_base->timer_waiters)) {\n\t\traw_spin_unlock_irqrestore(&cpu_base->lock, flags);\n\t\tspin_unlock(&cpu_base->softirq_expiry_lock);\n\t\tspin_lock(&cpu_base->softirq_expiry_lock);\n\t\traw_spin_lock_irq(&cpu_base->lock);\n\t}\n}\n\n \nvoid hrtimer_cancel_wait_running(const struct hrtimer *timer)\n{\n\t \n\tstruct hrtimer_clock_base *base = READ_ONCE(timer->base);\n\n\t \n\tif (!timer->is_soft || is_migration_base(base)) {\n\t\tcpu_relax();\n\t\treturn;\n\t}\n\n\t \n\tatomic_inc(&base->cpu_base->timer_waiters);\n\tspin_lock_bh(&base->cpu_base->softirq_expiry_lock);\n\tatomic_dec(&base->cpu_base->timer_waiters);\n\tspin_unlock_bh(&base->cpu_base->softirq_expiry_lock);\n}\n#else\nstatic inline void\nhrtimer_cpu_base_init_expiry_lock(struct hrtimer_cpu_base *base) { }\nstatic inline void\nhrtimer_cpu_base_lock_expiry(struct hrtimer_cpu_base *base) { }\nstatic inline void\nhrtimer_cpu_base_unlock_expiry(struct hrtimer_cpu_base *base) { }\nstatic inline void hrtimer_sync_wait_running(struct hrtimer_cpu_base *base,\n\t\t\t\t\t     unsigned long flags) { }\n#endif\n\n \nint hrtimer_cancel(struct hrtimer *timer)\n{\n\tint ret;\n\n\tdo {\n\t\tret = hrtimer_try_to_cancel(timer);\n\n\t\tif (ret < 0)\n\t\t\thrtimer_cancel_wait_running(timer);\n\t} while (ret < 0);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(hrtimer_cancel);\n\n \nktime_t __hrtimer_get_remaining(const struct hrtimer *timer, bool adjust)\n{\n\tunsigned long flags;\n\tktime_t rem;\n\n\tlock_hrtimer_base(timer, &flags);\n\tif (IS_ENABLED(CONFIG_TIME_LOW_RES) && adjust)\n\t\trem = hrtimer_expires_remaining_adjusted(timer);\n\telse\n\t\trem = hrtimer_expires_remaining(timer);\n\tunlock_hrtimer_base(timer, &flags);\n\n\treturn rem;\n}\nEXPORT_SYMBOL_GPL(__hrtimer_get_remaining);\n\n#ifdef CONFIG_NO_HZ_COMMON\n \nu64 hrtimer_get_next_event(void)\n{\n\tstruct hrtimer_cpu_base *cpu_base = this_cpu_ptr(&hrtimer_bases);\n\tu64 expires = KTIME_MAX;\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&cpu_base->lock, flags);\n\n\tif (!__hrtimer_hres_active(cpu_base))\n\t\texpires = __hrtimer_get_next_event(cpu_base, HRTIMER_ACTIVE_ALL);\n\n\traw_spin_unlock_irqrestore(&cpu_base->lock, flags);\n\n\treturn expires;\n}\n\n \nu64 hrtimer_next_event_without(const struct hrtimer *exclude)\n{\n\tstruct hrtimer_cpu_base *cpu_base = this_cpu_ptr(&hrtimer_bases);\n\tu64 expires = KTIME_MAX;\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&cpu_base->lock, flags);\n\n\tif (__hrtimer_hres_active(cpu_base)) {\n\t\tunsigned int active;\n\n\t\tif (!cpu_base->softirq_activated) {\n\t\t\tactive = cpu_base->active_bases & HRTIMER_ACTIVE_SOFT;\n\t\t\texpires = __hrtimer_next_event_base(cpu_base, exclude,\n\t\t\t\t\t\t\t    active, KTIME_MAX);\n\t\t}\n\t\tactive = cpu_base->active_bases & HRTIMER_ACTIVE_HARD;\n\t\texpires = __hrtimer_next_event_base(cpu_base, exclude, active,\n\t\t\t\t\t\t    expires);\n\t}\n\n\traw_spin_unlock_irqrestore(&cpu_base->lock, flags);\n\n\treturn expires;\n}\n#endif\n\nstatic inline int hrtimer_clockid_to_base(clockid_t clock_id)\n{\n\tif (likely(clock_id < MAX_CLOCKS)) {\n\t\tint base = hrtimer_clock_to_base_table[clock_id];\n\n\t\tif (likely(base != HRTIMER_MAX_CLOCK_BASES))\n\t\t\treturn base;\n\t}\n\tWARN(1, \"Invalid clockid %d. Using MONOTONIC\\n\", clock_id);\n\treturn HRTIMER_BASE_MONOTONIC;\n}\n\nstatic void __hrtimer_init(struct hrtimer *timer, clockid_t clock_id,\n\t\t\t   enum hrtimer_mode mode)\n{\n\tbool softtimer = !!(mode & HRTIMER_MODE_SOFT);\n\tstruct hrtimer_cpu_base *cpu_base;\n\tint base;\n\n\t \n\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && !(mode & HRTIMER_MODE_HARD))\n\t\tsofttimer = true;\n\n\tmemset(timer, 0, sizeof(struct hrtimer));\n\n\tcpu_base = raw_cpu_ptr(&hrtimer_bases);\n\n\t \n\tif (clock_id == CLOCK_REALTIME && mode & HRTIMER_MODE_REL)\n\t\tclock_id = CLOCK_MONOTONIC;\n\n\tbase = softtimer ? HRTIMER_MAX_CLOCK_BASES / 2 : 0;\n\tbase += hrtimer_clockid_to_base(clock_id);\n\ttimer->is_soft = softtimer;\n\ttimer->is_hard = !!(mode & HRTIMER_MODE_HARD);\n\ttimer->base = &cpu_base->clock_base[base];\n\ttimerqueue_init(&timer->node);\n}\n\n \nvoid hrtimer_init(struct hrtimer *timer, clockid_t clock_id,\n\t\t  enum hrtimer_mode mode)\n{\n\tdebug_init(timer, clock_id, mode);\n\t__hrtimer_init(timer, clock_id, mode);\n}\nEXPORT_SYMBOL_GPL(hrtimer_init);\n\n \nbool hrtimer_active(const struct hrtimer *timer)\n{\n\tstruct hrtimer_clock_base *base;\n\tunsigned int seq;\n\n\tdo {\n\t\tbase = READ_ONCE(timer->base);\n\t\tseq = raw_read_seqcount_begin(&base->seq);\n\n\t\tif (timer->state != HRTIMER_STATE_INACTIVE ||\n\t\t    base->running == timer)\n\t\t\treturn true;\n\n\t} while (read_seqcount_retry(&base->seq, seq) ||\n\t\t base != READ_ONCE(timer->base));\n\n\treturn false;\n}\nEXPORT_SYMBOL_GPL(hrtimer_active);\n\n \n\nstatic void __run_hrtimer(struct hrtimer_cpu_base *cpu_base,\n\t\t\t  struct hrtimer_clock_base *base,\n\t\t\t  struct hrtimer *timer, ktime_t *now,\n\t\t\t  unsigned long flags) __must_hold(&cpu_base->lock)\n{\n\tenum hrtimer_restart (*fn)(struct hrtimer *);\n\tbool expires_in_hardirq;\n\tint restart;\n\n\tlockdep_assert_held(&cpu_base->lock);\n\n\tdebug_deactivate(timer);\n\tbase->running = timer;\n\n\t \n\traw_write_seqcount_barrier(&base->seq);\n\n\t__remove_hrtimer(timer, base, HRTIMER_STATE_INACTIVE, 0);\n\tfn = timer->function;\n\n\t \n\tif (IS_ENABLED(CONFIG_TIME_LOW_RES))\n\t\ttimer->is_rel = false;\n\n\t \n\traw_spin_unlock_irqrestore(&cpu_base->lock, flags);\n\ttrace_hrtimer_expire_entry(timer, now);\n\texpires_in_hardirq = lockdep_hrtimer_enter(timer);\n\n\trestart = fn(timer);\n\n\tlockdep_hrtimer_exit(expires_in_hardirq);\n\ttrace_hrtimer_expire_exit(timer);\n\traw_spin_lock_irq(&cpu_base->lock);\n\n\t \n\tif (restart != HRTIMER_NORESTART &&\n\t    !(timer->state & HRTIMER_STATE_ENQUEUED))\n\t\tenqueue_hrtimer(timer, base, HRTIMER_MODE_ABS);\n\n\t \n\traw_write_seqcount_barrier(&base->seq);\n\n\tWARN_ON_ONCE(base->running != timer);\n\tbase->running = NULL;\n}\n\nstatic void __hrtimer_run_queues(struct hrtimer_cpu_base *cpu_base, ktime_t now,\n\t\t\t\t unsigned long flags, unsigned int active_mask)\n{\n\tstruct hrtimer_clock_base *base;\n\tunsigned int active = cpu_base->active_bases & active_mask;\n\n\tfor_each_active_base(base, cpu_base, active) {\n\t\tstruct timerqueue_node *node;\n\t\tktime_t basenow;\n\n\t\tbasenow = ktime_add(now, base->offset);\n\n\t\twhile ((node = timerqueue_getnext(&base->active))) {\n\t\t\tstruct hrtimer *timer;\n\n\t\t\ttimer = container_of(node, struct hrtimer, node);\n\n\t\t\t \n\t\t\tif (basenow < hrtimer_get_softexpires_tv64(timer))\n\t\t\t\tbreak;\n\n\t\t\t__run_hrtimer(cpu_base, base, timer, &basenow, flags);\n\t\t\tif (active_mask == HRTIMER_ACTIVE_SOFT)\n\t\t\t\thrtimer_sync_wait_running(cpu_base, flags);\n\t\t}\n\t}\n}\n\nstatic __latent_entropy void hrtimer_run_softirq(struct softirq_action *h)\n{\n\tstruct hrtimer_cpu_base *cpu_base = this_cpu_ptr(&hrtimer_bases);\n\tunsigned long flags;\n\tktime_t now;\n\n\thrtimer_cpu_base_lock_expiry(cpu_base);\n\traw_spin_lock_irqsave(&cpu_base->lock, flags);\n\n\tnow = hrtimer_update_base(cpu_base);\n\t__hrtimer_run_queues(cpu_base, now, flags, HRTIMER_ACTIVE_SOFT);\n\n\tcpu_base->softirq_activated = 0;\n\thrtimer_update_softirq_timer(cpu_base, true);\n\n\traw_spin_unlock_irqrestore(&cpu_base->lock, flags);\n\thrtimer_cpu_base_unlock_expiry(cpu_base);\n}\n\n#ifdef CONFIG_HIGH_RES_TIMERS\n\n \nvoid hrtimer_interrupt(struct clock_event_device *dev)\n{\n\tstruct hrtimer_cpu_base *cpu_base = this_cpu_ptr(&hrtimer_bases);\n\tktime_t expires_next, now, entry_time, delta;\n\tunsigned long flags;\n\tint retries = 0;\n\n\tBUG_ON(!cpu_base->hres_active);\n\tcpu_base->nr_events++;\n\tdev->next_event = KTIME_MAX;\n\n\traw_spin_lock_irqsave(&cpu_base->lock, flags);\n\tentry_time = now = hrtimer_update_base(cpu_base);\nretry:\n\tcpu_base->in_hrtirq = 1;\n\t \n\tcpu_base->expires_next = KTIME_MAX;\n\n\tif (!ktime_before(now, cpu_base->softirq_expires_next)) {\n\t\tcpu_base->softirq_expires_next = KTIME_MAX;\n\t\tcpu_base->softirq_activated = 1;\n\t\traise_softirq_irqoff(HRTIMER_SOFTIRQ);\n\t}\n\n\t__hrtimer_run_queues(cpu_base, now, flags, HRTIMER_ACTIVE_HARD);\n\n\t \n\texpires_next = hrtimer_update_next_event(cpu_base);\n\t \n\tcpu_base->expires_next = expires_next;\n\tcpu_base->in_hrtirq = 0;\n\traw_spin_unlock_irqrestore(&cpu_base->lock, flags);\n\n\t \n\tif (!tick_program_event(expires_next, 0)) {\n\t\tcpu_base->hang_detected = 0;\n\t\treturn;\n\t}\n\n\t \n\traw_spin_lock_irqsave(&cpu_base->lock, flags);\n\tnow = hrtimer_update_base(cpu_base);\n\tcpu_base->nr_retries++;\n\tif (++retries < 3)\n\t\tgoto retry;\n\t \n\tcpu_base->nr_hangs++;\n\tcpu_base->hang_detected = 1;\n\traw_spin_unlock_irqrestore(&cpu_base->lock, flags);\n\n\tdelta = ktime_sub(now, entry_time);\n\tif ((unsigned int)delta > cpu_base->max_hang_time)\n\t\tcpu_base->max_hang_time = (unsigned int) delta;\n\t \n\tif (delta > 100 * NSEC_PER_MSEC)\n\t\texpires_next = ktime_add_ns(now, 100 * NSEC_PER_MSEC);\n\telse\n\t\texpires_next = ktime_add(now, delta);\n\ttick_program_event(expires_next, 1);\n\tpr_warn_once(\"hrtimer: interrupt took %llu ns\\n\", ktime_to_ns(delta));\n}\n\n \nstatic inline void __hrtimer_peek_ahead_timers(void)\n{\n\tstruct tick_device *td;\n\n\tif (!hrtimer_hres_active())\n\t\treturn;\n\n\ttd = this_cpu_ptr(&tick_cpu_device);\n\tif (td && td->evtdev)\n\t\thrtimer_interrupt(td->evtdev);\n}\n\n#else  \n\nstatic inline void __hrtimer_peek_ahead_timers(void) { }\n\n#endif\t \n\n \nvoid hrtimer_run_queues(void)\n{\n\tstruct hrtimer_cpu_base *cpu_base = this_cpu_ptr(&hrtimer_bases);\n\tunsigned long flags;\n\tktime_t now;\n\n\tif (__hrtimer_hres_active(cpu_base))\n\t\treturn;\n\n\t \n\tif (tick_check_oneshot_change(!hrtimer_is_hres_enabled())) {\n\t\thrtimer_switch_to_hres();\n\t\treturn;\n\t}\n\n\traw_spin_lock_irqsave(&cpu_base->lock, flags);\n\tnow = hrtimer_update_base(cpu_base);\n\n\tif (!ktime_before(now, cpu_base->softirq_expires_next)) {\n\t\tcpu_base->softirq_expires_next = KTIME_MAX;\n\t\tcpu_base->softirq_activated = 1;\n\t\traise_softirq_irqoff(HRTIMER_SOFTIRQ);\n\t}\n\n\t__hrtimer_run_queues(cpu_base, now, flags, HRTIMER_ACTIVE_HARD);\n\traw_spin_unlock_irqrestore(&cpu_base->lock, flags);\n}\n\n \nstatic enum hrtimer_restart hrtimer_wakeup(struct hrtimer *timer)\n{\n\tstruct hrtimer_sleeper *t =\n\t\tcontainer_of(timer, struct hrtimer_sleeper, timer);\n\tstruct task_struct *task = t->task;\n\n\tt->task = NULL;\n\tif (task)\n\t\twake_up_process(task);\n\n\treturn HRTIMER_NORESTART;\n}\n\n \nvoid hrtimer_sleeper_start_expires(struct hrtimer_sleeper *sl,\n\t\t\t\t   enum hrtimer_mode mode)\n{\n\t \n\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && sl->timer.is_hard)\n\t\tmode |= HRTIMER_MODE_HARD;\n\n\thrtimer_start_expires(&sl->timer, mode);\n}\nEXPORT_SYMBOL_GPL(hrtimer_sleeper_start_expires);\n\nstatic void __hrtimer_init_sleeper(struct hrtimer_sleeper *sl,\n\t\t\t\t   clockid_t clock_id, enum hrtimer_mode mode)\n{\n\t \n\tif (IS_ENABLED(CONFIG_PREEMPT_RT)) {\n\t\tif (task_is_realtime(current) && !(mode & HRTIMER_MODE_SOFT))\n\t\t\tmode |= HRTIMER_MODE_HARD;\n\t}\n\n\t__hrtimer_init(&sl->timer, clock_id, mode);\n\tsl->timer.function = hrtimer_wakeup;\n\tsl->task = current;\n}\n\n \nvoid hrtimer_init_sleeper(struct hrtimer_sleeper *sl, clockid_t clock_id,\n\t\t\t  enum hrtimer_mode mode)\n{\n\tdebug_init(&sl->timer, clock_id, mode);\n\t__hrtimer_init_sleeper(sl, clock_id, mode);\n\n}\nEXPORT_SYMBOL_GPL(hrtimer_init_sleeper);\n\nint nanosleep_copyout(struct restart_block *restart, struct timespec64 *ts)\n{\n\tswitch(restart->nanosleep.type) {\n#ifdef CONFIG_COMPAT_32BIT_TIME\n\tcase TT_COMPAT:\n\t\tif (put_old_timespec32(ts, restart->nanosleep.compat_rmtp))\n\t\t\treturn -EFAULT;\n\t\tbreak;\n#endif\n\tcase TT_NATIVE:\n\t\tif (put_timespec64(ts, restart->nanosleep.rmtp))\n\t\t\treturn -EFAULT;\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\treturn -ERESTART_RESTARTBLOCK;\n}\n\nstatic int __sched do_nanosleep(struct hrtimer_sleeper *t, enum hrtimer_mode mode)\n{\n\tstruct restart_block *restart;\n\n\tdo {\n\t\tset_current_state(TASK_INTERRUPTIBLE|TASK_FREEZABLE);\n\t\thrtimer_sleeper_start_expires(t, mode);\n\n\t\tif (likely(t->task))\n\t\t\tschedule();\n\n\t\thrtimer_cancel(&t->timer);\n\t\tmode = HRTIMER_MODE_ABS;\n\n\t} while (t->task && !signal_pending(current));\n\n\t__set_current_state(TASK_RUNNING);\n\n\tif (!t->task)\n\t\treturn 0;\n\n\trestart = &current->restart_block;\n\tif (restart->nanosleep.type != TT_NONE) {\n\t\tktime_t rem = hrtimer_expires_remaining(&t->timer);\n\t\tstruct timespec64 rmt;\n\n\t\tif (rem <= 0)\n\t\t\treturn 0;\n\t\trmt = ktime_to_timespec64(rem);\n\n\t\treturn nanosleep_copyout(restart, &rmt);\n\t}\n\treturn -ERESTART_RESTARTBLOCK;\n}\n\nstatic long __sched hrtimer_nanosleep_restart(struct restart_block *restart)\n{\n\tstruct hrtimer_sleeper t;\n\tint ret;\n\n\thrtimer_init_sleeper_on_stack(&t, restart->nanosleep.clockid,\n\t\t\t\t      HRTIMER_MODE_ABS);\n\thrtimer_set_expires_tv64(&t.timer, restart->nanosleep.expires);\n\tret = do_nanosleep(&t, HRTIMER_MODE_ABS);\n\tdestroy_hrtimer_on_stack(&t.timer);\n\treturn ret;\n}\n\nlong hrtimer_nanosleep(ktime_t rqtp, const enum hrtimer_mode mode,\n\t\t       const clockid_t clockid)\n{\n\tstruct restart_block *restart;\n\tstruct hrtimer_sleeper t;\n\tint ret = 0;\n\tu64 slack;\n\n\tslack = current->timer_slack_ns;\n\tif (rt_task(current))\n\t\tslack = 0;\n\n\thrtimer_init_sleeper_on_stack(&t, clockid, mode);\n\thrtimer_set_expires_range_ns(&t.timer, rqtp, slack);\n\tret = do_nanosleep(&t, mode);\n\tif (ret != -ERESTART_RESTARTBLOCK)\n\t\tgoto out;\n\n\t \n\tif (mode == HRTIMER_MODE_ABS) {\n\t\tret = -ERESTARTNOHAND;\n\t\tgoto out;\n\t}\n\n\trestart = &current->restart_block;\n\trestart->nanosleep.clockid = t.timer.base->clockid;\n\trestart->nanosleep.expires = hrtimer_get_expires_tv64(&t.timer);\n\tset_restart_fn(restart, hrtimer_nanosleep_restart);\nout:\n\tdestroy_hrtimer_on_stack(&t.timer);\n\treturn ret;\n}\n\n#ifdef CONFIG_64BIT\n\nSYSCALL_DEFINE2(nanosleep, struct __kernel_timespec __user *, rqtp,\n\t\tstruct __kernel_timespec __user *, rmtp)\n{\n\tstruct timespec64 tu;\n\n\tif (get_timespec64(&tu, rqtp))\n\t\treturn -EFAULT;\n\n\tif (!timespec64_valid(&tu))\n\t\treturn -EINVAL;\n\n\tcurrent->restart_block.fn = do_no_restart_syscall;\n\tcurrent->restart_block.nanosleep.type = rmtp ? TT_NATIVE : TT_NONE;\n\tcurrent->restart_block.nanosleep.rmtp = rmtp;\n\treturn hrtimer_nanosleep(timespec64_to_ktime(tu), HRTIMER_MODE_REL,\n\t\t\t\t CLOCK_MONOTONIC);\n}\n\n#endif\n\n#ifdef CONFIG_COMPAT_32BIT_TIME\n\nSYSCALL_DEFINE2(nanosleep_time32, struct old_timespec32 __user *, rqtp,\n\t\t       struct old_timespec32 __user *, rmtp)\n{\n\tstruct timespec64 tu;\n\n\tif (get_old_timespec32(&tu, rqtp))\n\t\treturn -EFAULT;\n\n\tif (!timespec64_valid(&tu))\n\t\treturn -EINVAL;\n\n\tcurrent->restart_block.fn = do_no_restart_syscall;\n\tcurrent->restart_block.nanosleep.type = rmtp ? TT_COMPAT : TT_NONE;\n\tcurrent->restart_block.nanosleep.compat_rmtp = rmtp;\n\treturn hrtimer_nanosleep(timespec64_to_ktime(tu), HRTIMER_MODE_REL,\n\t\t\t\t CLOCK_MONOTONIC);\n}\n#endif\n\n \nint hrtimers_prepare_cpu(unsigned int cpu)\n{\n\tstruct hrtimer_cpu_base *cpu_base = &per_cpu(hrtimer_bases, cpu);\n\tint i;\n\n\tfor (i = 0; i < HRTIMER_MAX_CLOCK_BASES; i++) {\n\t\tstruct hrtimer_clock_base *clock_b = &cpu_base->clock_base[i];\n\n\t\tclock_b->cpu_base = cpu_base;\n\t\tseqcount_raw_spinlock_init(&clock_b->seq, &cpu_base->lock);\n\t\ttimerqueue_init_head(&clock_b->active);\n\t}\n\n\tcpu_base->cpu = cpu;\n\tcpu_base->active_bases = 0;\n\tcpu_base->hres_active = 0;\n\tcpu_base->hang_detected = 0;\n\tcpu_base->next_timer = NULL;\n\tcpu_base->softirq_next_timer = NULL;\n\tcpu_base->expires_next = KTIME_MAX;\n\tcpu_base->softirq_expires_next = KTIME_MAX;\n\thrtimer_cpu_base_init_expiry_lock(cpu_base);\n\treturn 0;\n}\n\n#ifdef CONFIG_HOTPLUG_CPU\n\nstatic void migrate_hrtimer_list(struct hrtimer_clock_base *old_base,\n\t\t\t\tstruct hrtimer_clock_base *new_base)\n{\n\tstruct hrtimer *timer;\n\tstruct timerqueue_node *node;\n\n\twhile ((node = timerqueue_getnext(&old_base->active))) {\n\t\ttimer = container_of(node, struct hrtimer, node);\n\t\tBUG_ON(hrtimer_callback_running(timer));\n\t\tdebug_deactivate(timer);\n\n\t\t \n\t\t__remove_hrtimer(timer, old_base, HRTIMER_STATE_ENQUEUED, 0);\n\t\ttimer->base = new_base;\n\t\t \n\t\tenqueue_hrtimer(timer, new_base, HRTIMER_MODE_ABS);\n\t}\n}\n\nint hrtimers_cpu_dying(unsigned int dying_cpu)\n{\n\tstruct hrtimer_cpu_base *old_base, *new_base;\n\tint i, ncpu = cpumask_first(cpu_active_mask);\n\n\ttick_cancel_sched_timer(dying_cpu);\n\n\told_base = this_cpu_ptr(&hrtimer_bases);\n\tnew_base = &per_cpu(hrtimer_bases, ncpu);\n\n\t \n\traw_spin_lock(&old_base->lock);\n\traw_spin_lock_nested(&new_base->lock, SINGLE_DEPTH_NESTING);\n\n\tfor (i = 0; i < HRTIMER_MAX_CLOCK_BASES; i++) {\n\t\tmigrate_hrtimer_list(&old_base->clock_base[i],\n\t\t\t\t     &new_base->clock_base[i]);\n\t}\n\n\t \n\t__hrtimer_get_next_event(new_base, HRTIMER_ACTIVE_SOFT);\n\t \n\tsmp_call_function_single(ncpu, retrigger_next_event, NULL, 0);\n\n\traw_spin_unlock(&new_base->lock);\n\traw_spin_unlock(&old_base->lock);\n\n\treturn 0;\n}\n\n#endif  \n\nvoid __init hrtimers_init(void)\n{\n\thrtimers_prepare_cpu(smp_processor_id());\n\topen_softirq(HRTIMER_SOFTIRQ, hrtimer_run_softirq);\n}\n\n \nint __sched\nschedule_hrtimeout_range_clock(ktime_t *expires, u64 delta,\n\t\t\t       const enum hrtimer_mode mode, clockid_t clock_id)\n{\n\tstruct hrtimer_sleeper t;\n\n\t \n\tif (expires && *expires == 0) {\n\t\t__set_current_state(TASK_RUNNING);\n\t\treturn 0;\n\t}\n\n\t \n\tif (!expires) {\n\t\tschedule();\n\t\treturn -EINTR;\n\t}\n\n\t \n\tif (rt_task(current))\n\t\tdelta = 0;\n\n\thrtimer_init_sleeper_on_stack(&t, clock_id, mode);\n\thrtimer_set_expires_range_ns(&t.timer, *expires, delta);\n\thrtimer_sleeper_start_expires(&t, mode);\n\n\tif (likely(t.task))\n\t\tschedule();\n\n\thrtimer_cancel(&t.timer);\n\tdestroy_hrtimer_on_stack(&t.timer);\n\n\t__set_current_state(TASK_RUNNING);\n\n\treturn !t.task ? 0 : -EINTR;\n}\nEXPORT_SYMBOL_GPL(schedule_hrtimeout_range_clock);\n\n \nint __sched schedule_hrtimeout_range(ktime_t *expires, u64 delta,\n\t\t\t\t     const enum hrtimer_mode mode)\n{\n\treturn schedule_hrtimeout_range_clock(expires, delta, mode,\n\t\t\t\t\t      CLOCK_MONOTONIC);\n}\nEXPORT_SYMBOL_GPL(schedule_hrtimeout_range);\n\n \nint __sched schedule_hrtimeout(ktime_t *expires,\n\t\t\t       const enum hrtimer_mode mode)\n{\n\treturn schedule_hrtimeout_range(expires, 0, mode);\n}\nEXPORT_SYMBOL_GPL(schedule_hrtimeout);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}