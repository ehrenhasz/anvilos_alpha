{
  "module_name": "timer.c",
  "hash_id": "03e74c6f664332e32599e37aa826e4c2502b12a62a44f5588a59ccaef3f4ec04",
  "original_prompt": "Ingested from linux-6.6.14/kernel/time/timer.c",
  "human_readable_source": "\n \n\n#include <linux/kernel_stat.h>\n#include <linux/export.h>\n#include <linux/interrupt.h>\n#include <linux/percpu.h>\n#include <linux/init.h>\n#include <linux/mm.h>\n#include <linux/swap.h>\n#include <linux/pid_namespace.h>\n#include <linux/notifier.h>\n#include <linux/thread_info.h>\n#include <linux/time.h>\n#include <linux/jiffies.h>\n#include <linux/posix-timers.h>\n#include <linux/cpu.h>\n#include <linux/syscalls.h>\n#include <linux/delay.h>\n#include <linux/tick.h>\n#include <linux/kallsyms.h>\n#include <linux/irq_work.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/debug.h>\n#include <linux/slab.h>\n#include <linux/compat.h>\n#include <linux/random.h>\n#include <linux/sysctl.h>\n\n#include <linux/uaccess.h>\n#include <asm/unistd.h>\n#include <asm/div64.h>\n#include <asm/timex.h>\n#include <asm/io.h>\n\n#include \"tick-internal.h\"\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/timer.h>\n\n__visible u64 jiffies_64 __cacheline_aligned_in_smp = INITIAL_JIFFIES;\n\nEXPORT_SYMBOL(jiffies_64);\n\n \n\n \n#define LVL_CLK_SHIFT\t3\n#define LVL_CLK_DIV\t(1UL << LVL_CLK_SHIFT)\n#define LVL_CLK_MASK\t(LVL_CLK_DIV - 1)\n#define LVL_SHIFT(n)\t((n) * LVL_CLK_SHIFT)\n#define LVL_GRAN(n)\t(1UL << LVL_SHIFT(n))\n\n \n#define LVL_START(n)\t((LVL_SIZE - 1) << (((n) - 1) * LVL_CLK_SHIFT))\n\n \n#define LVL_BITS\t6\n#define LVL_SIZE\t(1UL << LVL_BITS)\n#define LVL_MASK\t(LVL_SIZE - 1)\n#define LVL_OFFS(n)\t((n) * LVL_SIZE)\n\n \n#if HZ > 100\n# define LVL_DEPTH\t9\n# else\n# define LVL_DEPTH\t8\n#endif\n\n \n#define WHEEL_TIMEOUT_CUTOFF\t(LVL_START(LVL_DEPTH))\n#define WHEEL_TIMEOUT_MAX\t(WHEEL_TIMEOUT_CUTOFF - LVL_GRAN(LVL_DEPTH - 1))\n\n \n#define WHEEL_SIZE\t(LVL_SIZE * LVL_DEPTH)\n\n#ifdef CONFIG_NO_HZ_COMMON\n# define NR_BASES\t2\n# define BASE_STD\t0\n# define BASE_DEF\t1\n#else\n# define NR_BASES\t1\n# define BASE_STD\t0\n# define BASE_DEF\t0\n#endif\n\nstruct timer_base {\n\traw_spinlock_t\t\tlock;\n\tstruct timer_list\t*running_timer;\n#ifdef CONFIG_PREEMPT_RT\n\tspinlock_t\t\texpiry_lock;\n\tatomic_t\t\ttimer_waiters;\n#endif\n\tunsigned long\t\tclk;\n\tunsigned long\t\tnext_expiry;\n\tunsigned int\t\tcpu;\n\tbool\t\t\tnext_expiry_recalc;\n\tbool\t\t\tis_idle;\n\tbool\t\t\ttimers_pending;\n\tDECLARE_BITMAP(pending_map, WHEEL_SIZE);\n\tstruct hlist_head\tvectors[WHEEL_SIZE];\n} ____cacheline_aligned;\n\nstatic DEFINE_PER_CPU(struct timer_base, timer_bases[NR_BASES]);\n\n#ifdef CONFIG_NO_HZ_COMMON\n\nstatic DEFINE_STATIC_KEY_FALSE(timers_nohz_active);\nstatic DEFINE_MUTEX(timer_keys_mutex);\n\nstatic void timer_update_keys(struct work_struct *work);\nstatic DECLARE_WORK(timer_update_work, timer_update_keys);\n\n#ifdef CONFIG_SMP\nstatic unsigned int sysctl_timer_migration = 1;\n\nDEFINE_STATIC_KEY_FALSE(timers_migration_enabled);\n\nstatic void timers_update_migration(void)\n{\n\tif (sysctl_timer_migration && tick_nohz_active)\n\t\tstatic_branch_enable(&timers_migration_enabled);\n\telse\n\t\tstatic_branch_disable(&timers_migration_enabled);\n}\n\n#ifdef CONFIG_SYSCTL\nstatic int timer_migration_handler(struct ctl_table *table, int write,\n\t\t\t    void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tint ret;\n\n\tmutex_lock(&timer_keys_mutex);\n\tret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);\n\tif (!ret && write)\n\t\ttimers_update_migration();\n\tmutex_unlock(&timer_keys_mutex);\n\treturn ret;\n}\n\nstatic struct ctl_table timer_sysctl[] = {\n\t{\n\t\t.procname\t= \"timer_migration\",\n\t\t.data\t\t= &sysctl_timer_migration,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= timer_migration_handler,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_ONE,\n\t},\n\t{}\n};\n\nstatic int __init timer_sysctl_init(void)\n{\n\tregister_sysctl(\"kernel\", timer_sysctl);\n\treturn 0;\n}\ndevice_initcall(timer_sysctl_init);\n#endif  \n#else  \nstatic inline void timers_update_migration(void) { }\n#endif  \n\nstatic void timer_update_keys(struct work_struct *work)\n{\n\tmutex_lock(&timer_keys_mutex);\n\ttimers_update_migration();\n\tstatic_branch_enable(&timers_nohz_active);\n\tmutex_unlock(&timer_keys_mutex);\n}\n\nvoid timers_update_nohz(void)\n{\n\tschedule_work(&timer_update_work);\n}\n\nstatic inline bool is_timers_nohz_active(void)\n{\n\treturn static_branch_unlikely(&timers_nohz_active);\n}\n#else\nstatic inline bool is_timers_nohz_active(void) { return false; }\n#endif  \n\nstatic unsigned long round_jiffies_common(unsigned long j, int cpu,\n\t\tbool force_up)\n{\n\tint rem;\n\tunsigned long original = j;\n\n\t \n\tj += cpu * 3;\n\n\trem = j % HZ;\n\n\t \n\tif (rem < HZ/4 && !force_up)  \n\t\tj = j - rem;\n\telse  \n\t\tj = j - rem + HZ;\n\n\t \n\tj -= cpu * 3;\n\n\t \n\treturn time_is_after_jiffies(j) ? j : original;\n}\n\n \nunsigned long __round_jiffies(unsigned long j, int cpu)\n{\n\treturn round_jiffies_common(j, cpu, false);\n}\nEXPORT_SYMBOL_GPL(__round_jiffies);\n\n \nunsigned long __round_jiffies_relative(unsigned long j, int cpu)\n{\n\tunsigned long j0 = jiffies;\n\n\t \n\treturn round_jiffies_common(j + j0, cpu, false) - j0;\n}\nEXPORT_SYMBOL_GPL(__round_jiffies_relative);\n\n \nunsigned long round_jiffies(unsigned long j)\n{\n\treturn round_jiffies_common(j, raw_smp_processor_id(), false);\n}\nEXPORT_SYMBOL_GPL(round_jiffies);\n\n \nunsigned long round_jiffies_relative(unsigned long j)\n{\n\treturn __round_jiffies_relative(j, raw_smp_processor_id());\n}\nEXPORT_SYMBOL_GPL(round_jiffies_relative);\n\n \nunsigned long __round_jiffies_up(unsigned long j, int cpu)\n{\n\treturn round_jiffies_common(j, cpu, true);\n}\nEXPORT_SYMBOL_GPL(__round_jiffies_up);\n\n \nunsigned long __round_jiffies_up_relative(unsigned long j, int cpu)\n{\n\tunsigned long j0 = jiffies;\n\n\t \n\treturn round_jiffies_common(j + j0, cpu, true) - j0;\n}\nEXPORT_SYMBOL_GPL(__round_jiffies_up_relative);\n\n \nunsigned long round_jiffies_up(unsigned long j)\n{\n\treturn round_jiffies_common(j, raw_smp_processor_id(), true);\n}\nEXPORT_SYMBOL_GPL(round_jiffies_up);\n\n \nunsigned long round_jiffies_up_relative(unsigned long j)\n{\n\treturn __round_jiffies_up_relative(j, raw_smp_processor_id());\n}\nEXPORT_SYMBOL_GPL(round_jiffies_up_relative);\n\n\nstatic inline unsigned int timer_get_idx(struct timer_list *timer)\n{\n\treturn (timer->flags & TIMER_ARRAYMASK) >> TIMER_ARRAYSHIFT;\n}\n\nstatic inline void timer_set_idx(struct timer_list *timer, unsigned int idx)\n{\n\ttimer->flags = (timer->flags & ~TIMER_ARRAYMASK) |\n\t\t\tidx << TIMER_ARRAYSHIFT;\n}\n\n \nstatic inline unsigned calc_index(unsigned long expires, unsigned lvl,\n\t\t\t\t  unsigned long *bucket_expiry)\n{\n\n\t \n\texpires = (expires >> LVL_SHIFT(lvl)) + 1;\n\t*bucket_expiry = expires << LVL_SHIFT(lvl);\n\treturn LVL_OFFS(lvl) + (expires & LVL_MASK);\n}\n\nstatic int calc_wheel_index(unsigned long expires, unsigned long clk,\n\t\t\t    unsigned long *bucket_expiry)\n{\n\tunsigned long delta = expires - clk;\n\tunsigned int idx;\n\n\tif (delta < LVL_START(1)) {\n\t\tidx = calc_index(expires, 0, bucket_expiry);\n\t} else if (delta < LVL_START(2)) {\n\t\tidx = calc_index(expires, 1, bucket_expiry);\n\t} else if (delta < LVL_START(3)) {\n\t\tidx = calc_index(expires, 2, bucket_expiry);\n\t} else if (delta < LVL_START(4)) {\n\t\tidx = calc_index(expires, 3, bucket_expiry);\n\t} else if (delta < LVL_START(5)) {\n\t\tidx = calc_index(expires, 4, bucket_expiry);\n\t} else if (delta < LVL_START(6)) {\n\t\tidx = calc_index(expires, 5, bucket_expiry);\n\t} else if (delta < LVL_START(7)) {\n\t\tidx = calc_index(expires, 6, bucket_expiry);\n\t} else if (LVL_DEPTH > 8 && delta < LVL_START(8)) {\n\t\tidx = calc_index(expires, 7, bucket_expiry);\n\t} else if ((long) delta < 0) {\n\t\tidx = clk & LVL_MASK;\n\t\t*bucket_expiry = clk;\n\t} else {\n\t\t \n\t\tif (delta >= WHEEL_TIMEOUT_CUTOFF)\n\t\t\texpires = clk + WHEEL_TIMEOUT_MAX;\n\n\t\tidx = calc_index(expires, LVL_DEPTH - 1, bucket_expiry);\n\t}\n\treturn idx;\n}\n\nstatic void\ntrigger_dyntick_cpu(struct timer_base *base, struct timer_list *timer)\n{\n\tif (!is_timers_nohz_active())\n\t\treturn;\n\n\t \n\tif (timer->flags & TIMER_DEFERRABLE) {\n\t\tif (tick_nohz_full_cpu(base->cpu))\n\t\t\twake_up_nohz_cpu(base->cpu);\n\t\treturn;\n\t}\n\n\t \n\tif (base->is_idle)\n\t\twake_up_nohz_cpu(base->cpu);\n}\n\n \nstatic void enqueue_timer(struct timer_base *base, struct timer_list *timer,\n\t\t\t  unsigned int idx, unsigned long bucket_expiry)\n{\n\n\thlist_add_head(&timer->entry, base->vectors + idx);\n\t__set_bit(idx, base->pending_map);\n\ttimer_set_idx(timer, idx);\n\n\ttrace_timer_start(timer, timer->expires, timer->flags);\n\n\t \n\tif (time_before(bucket_expiry, base->next_expiry)) {\n\t\t \n\t\tbase->next_expiry = bucket_expiry;\n\t\tbase->timers_pending = true;\n\t\tbase->next_expiry_recalc = false;\n\t\ttrigger_dyntick_cpu(base, timer);\n\t}\n}\n\nstatic void internal_add_timer(struct timer_base *base, struct timer_list *timer)\n{\n\tunsigned long bucket_expiry;\n\tunsigned int idx;\n\n\tidx = calc_wheel_index(timer->expires, base->clk, &bucket_expiry);\n\tenqueue_timer(base, timer, idx, bucket_expiry);\n}\n\n#ifdef CONFIG_DEBUG_OBJECTS_TIMERS\n\nstatic const struct debug_obj_descr timer_debug_descr;\n\nstruct timer_hint {\n\tvoid\t(*function)(struct timer_list *t);\n\tlong\toffset;\n};\n\n#define TIMER_HINT(fn, container, timr, hintfn)\t\t\t\\\n\t{\t\t\t\t\t\t\t\\\n\t\t.function = fn,\t\t\t\t\t\\\n\t\t.offset\t  = offsetof(container, hintfn) -\t\\\n\t\t\t    offsetof(container, timr)\t\t\\\n\t}\n\nstatic const struct timer_hint timer_hints[] = {\n\tTIMER_HINT(delayed_work_timer_fn,\n\t\t   struct delayed_work, timer, work.func),\n\tTIMER_HINT(kthread_delayed_work_timer_fn,\n\t\t   struct kthread_delayed_work, timer, work.func),\n};\n\nstatic void *timer_debug_hint(void *addr)\n{\n\tstruct timer_list *timer = addr;\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(timer_hints); i++) {\n\t\tif (timer_hints[i].function == timer->function) {\n\t\t\tvoid (**fn)(void) = addr + timer_hints[i].offset;\n\n\t\t\treturn *fn;\n\t\t}\n\t}\n\n\treturn timer->function;\n}\n\nstatic bool timer_is_static_object(void *addr)\n{\n\tstruct timer_list *timer = addr;\n\n\treturn (timer->entry.pprev == NULL &&\n\t\ttimer->entry.next == TIMER_ENTRY_STATIC);\n}\n\n \nstatic bool timer_fixup_init(void *addr, enum debug_obj_state state)\n{\n\tstruct timer_list *timer = addr;\n\n\tswitch (state) {\n\tcase ODEBUG_STATE_ACTIVE:\n\t\tdel_timer_sync(timer);\n\t\tdebug_object_init(timer, &timer_debug_descr);\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n \nstatic void stub_timer(struct timer_list *unused)\n{\n\tWARN_ON(1);\n}\n\n \nstatic bool timer_fixup_activate(void *addr, enum debug_obj_state state)\n{\n\tstruct timer_list *timer = addr;\n\n\tswitch (state) {\n\tcase ODEBUG_STATE_NOTAVAILABLE:\n\t\ttimer_setup(timer, stub_timer, 0);\n\t\treturn true;\n\n\tcase ODEBUG_STATE_ACTIVE:\n\t\tWARN_ON(1);\n\t\tfallthrough;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n \nstatic bool timer_fixup_free(void *addr, enum debug_obj_state state)\n{\n\tstruct timer_list *timer = addr;\n\n\tswitch (state) {\n\tcase ODEBUG_STATE_ACTIVE:\n\t\tdel_timer_sync(timer);\n\t\tdebug_object_free(timer, &timer_debug_descr);\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n \nstatic bool timer_fixup_assert_init(void *addr, enum debug_obj_state state)\n{\n\tstruct timer_list *timer = addr;\n\n\tswitch (state) {\n\tcase ODEBUG_STATE_NOTAVAILABLE:\n\t\ttimer_setup(timer, stub_timer, 0);\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic const struct debug_obj_descr timer_debug_descr = {\n\t.name\t\t\t= \"timer_list\",\n\t.debug_hint\t\t= timer_debug_hint,\n\t.is_static_object\t= timer_is_static_object,\n\t.fixup_init\t\t= timer_fixup_init,\n\t.fixup_activate\t\t= timer_fixup_activate,\n\t.fixup_free\t\t= timer_fixup_free,\n\t.fixup_assert_init\t= timer_fixup_assert_init,\n};\n\nstatic inline void debug_timer_init(struct timer_list *timer)\n{\n\tdebug_object_init(timer, &timer_debug_descr);\n}\n\nstatic inline void debug_timer_activate(struct timer_list *timer)\n{\n\tdebug_object_activate(timer, &timer_debug_descr);\n}\n\nstatic inline void debug_timer_deactivate(struct timer_list *timer)\n{\n\tdebug_object_deactivate(timer, &timer_debug_descr);\n}\n\nstatic inline void debug_timer_assert_init(struct timer_list *timer)\n{\n\tdebug_object_assert_init(timer, &timer_debug_descr);\n}\n\nstatic void do_init_timer(struct timer_list *timer,\n\t\t\t  void (*func)(struct timer_list *),\n\t\t\t  unsigned int flags,\n\t\t\t  const char *name, struct lock_class_key *key);\n\nvoid init_timer_on_stack_key(struct timer_list *timer,\n\t\t\t     void (*func)(struct timer_list *),\n\t\t\t     unsigned int flags,\n\t\t\t     const char *name, struct lock_class_key *key)\n{\n\tdebug_object_init_on_stack(timer, &timer_debug_descr);\n\tdo_init_timer(timer, func, flags, name, key);\n}\nEXPORT_SYMBOL_GPL(init_timer_on_stack_key);\n\nvoid destroy_timer_on_stack(struct timer_list *timer)\n{\n\tdebug_object_free(timer, &timer_debug_descr);\n}\nEXPORT_SYMBOL_GPL(destroy_timer_on_stack);\n\n#else\nstatic inline void debug_timer_init(struct timer_list *timer) { }\nstatic inline void debug_timer_activate(struct timer_list *timer) { }\nstatic inline void debug_timer_deactivate(struct timer_list *timer) { }\nstatic inline void debug_timer_assert_init(struct timer_list *timer) { }\n#endif\n\nstatic inline void debug_init(struct timer_list *timer)\n{\n\tdebug_timer_init(timer);\n\ttrace_timer_init(timer);\n}\n\nstatic inline void debug_deactivate(struct timer_list *timer)\n{\n\tdebug_timer_deactivate(timer);\n\ttrace_timer_cancel(timer);\n}\n\nstatic inline void debug_assert_init(struct timer_list *timer)\n{\n\tdebug_timer_assert_init(timer);\n}\n\nstatic void do_init_timer(struct timer_list *timer,\n\t\t\t  void (*func)(struct timer_list *),\n\t\t\t  unsigned int flags,\n\t\t\t  const char *name, struct lock_class_key *key)\n{\n\ttimer->entry.pprev = NULL;\n\ttimer->function = func;\n\tif (WARN_ON_ONCE(flags & ~TIMER_INIT_FLAGS))\n\t\tflags &= TIMER_INIT_FLAGS;\n\ttimer->flags = flags | raw_smp_processor_id();\n\tlockdep_init_map(&timer->lockdep_map, name, key, 0);\n}\n\n \nvoid init_timer_key(struct timer_list *timer,\n\t\t    void (*func)(struct timer_list *), unsigned int flags,\n\t\t    const char *name, struct lock_class_key *key)\n{\n\tdebug_init(timer);\n\tdo_init_timer(timer, func, flags, name, key);\n}\nEXPORT_SYMBOL(init_timer_key);\n\nstatic inline void detach_timer(struct timer_list *timer, bool clear_pending)\n{\n\tstruct hlist_node *entry = &timer->entry;\n\n\tdebug_deactivate(timer);\n\n\t__hlist_del(entry);\n\tif (clear_pending)\n\t\tentry->pprev = NULL;\n\tentry->next = LIST_POISON2;\n}\n\nstatic int detach_if_pending(struct timer_list *timer, struct timer_base *base,\n\t\t\t     bool clear_pending)\n{\n\tunsigned idx = timer_get_idx(timer);\n\n\tif (!timer_pending(timer))\n\t\treturn 0;\n\n\tif (hlist_is_singular_node(&timer->entry, base->vectors + idx)) {\n\t\t__clear_bit(idx, base->pending_map);\n\t\tbase->next_expiry_recalc = true;\n\t}\n\n\tdetach_timer(timer, clear_pending);\n\treturn 1;\n}\n\nstatic inline struct timer_base *get_timer_cpu_base(u32 tflags, u32 cpu)\n{\n\tstruct timer_base *base = per_cpu_ptr(&timer_bases[BASE_STD], cpu);\n\n\t \n\tif (IS_ENABLED(CONFIG_NO_HZ_COMMON) && (tflags & TIMER_DEFERRABLE))\n\t\tbase = per_cpu_ptr(&timer_bases[BASE_DEF], cpu);\n\treturn base;\n}\n\nstatic inline struct timer_base *get_timer_this_cpu_base(u32 tflags)\n{\n\tstruct timer_base *base = this_cpu_ptr(&timer_bases[BASE_STD]);\n\n\t \n\tif (IS_ENABLED(CONFIG_NO_HZ_COMMON) && (tflags & TIMER_DEFERRABLE))\n\t\tbase = this_cpu_ptr(&timer_bases[BASE_DEF]);\n\treturn base;\n}\n\nstatic inline struct timer_base *get_timer_base(u32 tflags)\n{\n\treturn get_timer_cpu_base(tflags, tflags & TIMER_CPUMASK);\n}\n\nstatic inline struct timer_base *\nget_target_base(struct timer_base *base, unsigned tflags)\n{\n#if defined(CONFIG_SMP) && defined(CONFIG_NO_HZ_COMMON)\n\tif (static_branch_likely(&timers_migration_enabled) &&\n\t    !(tflags & TIMER_PINNED))\n\t\treturn get_timer_cpu_base(tflags, get_nohz_timer_target());\n#endif\n\treturn get_timer_this_cpu_base(tflags);\n}\n\nstatic inline void forward_timer_base(struct timer_base *base)\n{\n\tunsigned long jnow = READ_ONCE(jiffies);\n\n\t \n\tif ((long)(jnow - base->clk) < 1)\n\t\treturn;\n\n\t \n\tif (time_after(base->next_expiry, jnow)) {\n\t\tbase->clk = jnow;\n\t} else {\n\t\tif (WARN_ON_ONCE(time_before(base->next_expiry, base->clk)))\n\t\t\treturn;\n\t\tbase->clk = base->next_expiry;\n\t}\n}\n\n\n \nstatic struct timer_base *lock_timer_base(struct timer_list *timer,\n\t\t\t\t\t  unsigned long *flags)\n\t__acquires(timer->base->lock)\n{\n\tfor (;;) {\n\t\tstruct timer_base *base;\n\t\tu32 tf;\n\n\t\t \n\t\ttf = READ_ONCE(timer->flags);\n\n\t\tif (!(tf & TIMER_MIGRATING)) {\n\t\t\tbase = get_timer_base(tf);\n\t\t\traw_spin_lock_irqsave(&base->lock, *flags);\n\t\t\tif (timer->flags == tf)\n\t\t\t\treturn base;\n\t\t\traw_spin_unlock_irqrestore(&base->lock, *flags);\n\t\t}\n\t\tcpu_relax();\n\t}\n}\n\n#define MOD_TIMER_PENDING_ONLY\t\t0x01\n#define MOD_TIMER_REDUCE\t\t0x02\n#define MOD_TIMER_NOTPENDING\t\t0x04\n\nstatic inline int\n__mod_timer(struct timer_list *timer, unsigned long expires, unsigned int options)\n{\n\tunsigned long clk = 0, flags, bucket_expiry;\n\tstruct timer_base *base, *new_base;\n\tunsigned int idx = UINT_MAX;\n\tint ret = 0;\n\n\tdebug_assert_init(timer);\n\n\t \n\tif (!(options & MOD_TIMER_NOTPENDING) && timer_pending(timer)) {\n\t\t \n\t\tlong diff = timer->expires - expires;\n\n\t\tif (!diff)\n\t\t\treturn 1;\n\t\tif (options & MOD_TIMER_REDUCE && diff <= 0)\n\t\t\treturn 1;\n\n\t\t \n\t\tbase = lock_timer_base(timer, &flags);\n\t\t \n\t\tif (!timer->function)\n\t\t\tgoto out_unlock;\n\n\t\tforward_timer_base(base);\n\n\t\tif (timer_pending(timer) && (options & MOD_TIMER_REDUCE) &&\n\t\t    time_before_eq(timer->expires, expires)) {\n\t\t\tret = 1;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tclk = base->clk;\n\t\tidx = calc_wheel_index(expires, clk, &bucket_expiry);\n\n\t\t \n\t\tif (idx == timer_get_idx(timer)) {\n\t\t\tif (!(options & MOD_TIMER_REDUCE))\n\t\t\t\ttimer->expires = expires;\n\t\t\telse if (time_after(timer->expires, expires))\n\t\t\t\ttimer->expires = expires;\n\t\t\tret = 1;\n\t\t\tgoto out_unlock;\n\t\t}\n\t} else {\n\t\tbase = lock_timer_base(timer, &flags);\n\t\t \n\t\tif (!timer->function)\n\t\t\tgoto out_unlock;\n\n\t\tforward_timer_base(base);\n\t}\n\n\tret = detach_if_pending(timer, base, false);\n\tif (!ret && (options & MOD_TIMER_PENDING_ONLY))\n\t\tgoto out_unlock;\n\n\tnew_base = get_target_base(base, timer->flags);\n\n\tif (base != new_base) {\n\t\t \n\t\tif (likely(base->running_timer != timer)) {\n\t\t\t \n\t\t\ttimer->flags |= TIMER_MIGRATING;\n\n\t\t\traw_spin_unlock(&base->lock);\n\t\t\tbase = new_base;\n\t\t\traw_spin_lock(&base->lock);\n\t\t\tWRITE_ONCE(timer->flags,\n\t\t\t\t   (timer->flags & ~TIMER_BASEMASK) | base->cpu);\n\t\t\tforward_timer_base(base);\n\t\t}\n\t}\n\n\tdebug_timer_activate(timer);\n\n\ttimer->expires = expires;\n\t \n\tif (idx != UINT_MAX && clk == base->clk)\n\t\tenqueue_timer(base, timer, idx, bucket_expiry);\n\telse\n\t\tinternal_add_timer(base, timer);\n\nout_unlock:\n\traw_spin_unlock_irqrestore(&base->lock, flags);\n\n\treturn ret;\n}\n\n \nint mod_timer_pending(struct timer_list *timer, unsigned long expires)\n{\n\treturn __mod_timer(timer, expires, MOD_TIMER_PENDING_ONLY);\n}\nEXPORT_SYMBOL(mod_timer_pending);\n\n \nint mod_timer(struct timer_list *timer, unsigned long expires)\n{\n\treturn __mod_timer(timer, expires, 0);\n}\nEXPORT_SYMBOL(mod_timer);\n\n \nint timer_reduce(struct timer_list *timer, unsigned long expires)\n{\n\treturn __mod_timer(timer, expires, MOD_TIMER_REDUCE);\n}\nEXPORT_SYMBOL(timer_reduce);\n\n \nvoid add_timer(struct timer_list *timer)\n{\n\tif (WARN_ON_ONCE(timer_pending(timer)))\n\t\treturn;\n\t__mod_timer(timer, timer->expires, MOD_TIMER_NOTPENDING);\n}\nEXPORT_SYMBOL(add_timer);\n\n \nvoid add_timer_on(struct timer_list *timer, int cpu)\n{\n\tstruct timer_base *new_base, *base;\n\tunsigned long flags;\n\n\tdebug_assert_init(timer);\n\n\tif (WARN_ON_ONCE(timer_pending(timer)))\n\t\treturn;\n\n\tnew_base = get_timer_cpu_base(timer->flags, cpu);\n\n\t \n\tbase = lock_timer_base(timer, &flags);\n\t \n\tif (!timer->function)\n\t\tgoto out_unlock;\n\n\tif (base != new_base) {\n\t\ttimer->flags |= TIMER_MIGRATING;\n\n\t\traw_spin_unlock(&base->lock);\n\t\tbase = new_base;\n\t\traw_spin_lock(&base->lock);\n\t\tWRITE_ONCE(timer->flags,\n\t\t\t   (timer->flags & ~TIMER_BASEMASK) | cpu);\n\t}\n\tforward_timer_base(base);\n\n\tdebug_timer_activate(timer);\n\tinternal_add_timer(base, timer);\nout_unlock:\n\traw_spin_unlock_irqrestore(&base->lock, flags);\n}\nEXPORT_SYMBOL_GPL(add_timer_on);\n\n \nstatic int __timer_delete(struct timer_list *timer, bool shutdown)\n{\n\tstruct timer_base *base;\n\tunsigned long flags;\n\tint ret = 0;\n\n\tdebug_assert_init(timer);\n\n\t \n\tif (timer_pending(timer) || shutdown) {\n\t\tbase = lock_timer_base(timer, &flags);\n\t\tret = detach_if_pending(timer, base, true);\n\t\tif (shutdown)\n\t\t\ttimer->function = NULL;\n\t\traw_spin_unlock_irqrestore(&base->lock, flags);\n\t}\n\n\treturn ret;\n}\n\n \nint timer_delete(struct timer_list *timer)\n{\n\treturn __timer_delete(timer, false);\n}\nEXPORT_SYMBOL(timer_delete);\n\n \nint timer_shutdown(struct timer_list *timer)\n{\n\treturn __timer_delete(timer, true);\n}\nEXPORT_SYMBOL_GPL(timer_shutdown);\n\n \nstatic int __try_to_del_timer_sync(struct timer_list *timer, bool shutdown)\n{\n\tstruct timer_base *base;\n\tunsigned long flags;\n\tint ret = -1;\n\n\tdebug_assert_init(timer);\n\n\tbase = lock_timer_base(timer, &flags);\n\n\tif (base->running_timer != timer)\n\t\tret = detach_if_pending(timer, base, true);\n\tif (shutdown)\n\t\ttimer->function = NULL;\n\n\traw_spin_unlock_irqrestore(&base->lock, flags);\n\n\treturn ret;\n}\n\n \nint try_to_del_timer_sync(struct timer_list *timer)\n{\n\treturn __try_to_del_timer_sync(timer, false);\n}\nEXPORT_SYMBOL(try_to_del_timer_sync);\n\n#ifdef CONFIG_PREEMPT_RT\nstatic __init void timer_base_init_expiry_lock(struct timer_base *base)\n{\n\tspin_lock_init(&base->expiry_lock);\n}\n\nstatic inline void timer_base_lock_expiry(struct timer_base *base)\n{\n\tspin_lock(&base->expiry_lock);\n}\n\nstatic inline void timer_base_unlock_expiry(struct timer_base *base)\n{\n\tspin_unlock(&base->expiry_lock);\n}\n\n \nstatic void timer_sync_wait_running(struct timer_base *base)\n{\n\tif (atomic_read(&base->timer_waiters)) {\n\t\traw_spin_unlock_irq(&base->lock);\n\t\tspin_unlock(&base->expiry_lock);\n\t\tspin_lock(&base->expiry_lock);\n\t\traw_spin_lock_irq(&base->lock);\n\t}\n}\n\n \nstatic void del_timer_wait_running(struct timer_list *timer)\n{\n\tu32 tf;\n\n\ttf = READ_ONCE(timer->flags);\n\tif (!(tf & (TIMER_MIGRATING | TIMER_IRQSAFE))) {\n\t\tstruct timer_base *base = get_timer_base(tf);\n\n\t\t \n\t\tatomic_inc(&base->timer_waiters);\n\t\tspin_lock_bh(&base->expiry_lock);\n\t\tatomic_dec(&base->timer_waiters);\n\t\tspin_unlock_bh(&base->expiry_lock);\n\t}\n}\n#else\nstatic inline void timer_base_init_expiry_lock(struct timer_base *base) { }\nstatic inline void timer_base_lock_expiry(struct timer_base *base) { }\nstatic inline void timer_base_unlock_expiry(struct timer_base *base) { }\nstatic inline void timer_sync_wait_running(struct timer_base *base) { }\nstatic inline void del_timer_wait_running(struct timer_list *timer) { }\n#endif\n\n \nstatic int __timer_delete_sync(struct timer_list *timer, bool shutdown)\n{\n\tint ret;\n\n#ifdef CONFIG_LOCKDEP\n\tunsigned long flags;\n\n\t \n\tlocal_irq_save(flags);\n\tlock_map_acquire(&timer->lockdep_map);\n\tlock_map_release(&timer->lockdep_map);\n\tlocal_irq_restore(flags);\n#endif\n\t \n\tWARN_ON(in_hardirq() && !(timer->flags & TIMER_IRQSAFE));\n\n\t \n\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && !(timer->flags & TIMER_IRQSAFE))\n\t\tlockdep_assert_preemption_enabled();\n\n\tdo {\n\t\tret = __try_to_del_timer_sync(timer, shutdown);\n\n\t\tif (unlikely(ret < 0)) {\n\t\t\tdel_timer_wait_running(timer);\n\t\t\tcpu_relax();\n\t\t}\n\t} while (ret < 0);\n\n\treturn ret;\n}\n\n \nint timer_delete_sync(struct timer_list *timer)\n{\n\treturn __timer_delete_sync(timer, false);\n}\nEXPORT_SYMBOL(timer_delete_sync);\n\n \nint timer_shutdown_sync(struct timer_list *timer)\n{\n\treturn __timer_delete_sync(timer, true);\n}\nEXPORT_SYMBOL_GPL(timer_shutdown_sync);\n\nstatic void call_timer_fn(struct timer_list *timer,\n\t\t\t  void (*fn)(struct timer_list *),\n\t\t\t  unsigned long baseclk)\n{\n\tint count = preempt_count();\n\n#ifdef CONFIG_LOCKDEP\n\t \n\tstruct lockdep_map lockdep_map;\n\n\tlockdep_copy_map(&lockdep_map, &timer->lockdep_map);\n#endif\n\t \n\tlock_map_acquire(&lockdep_map);\n\n\ttrace_timer_expire_entry(timer, baseclk);\n\tfn(timer);\n\ttrace_timer_expire_exit(timer);\n\n\tlock_map_release(&lockdep_map);\n\n\tif (count != preempt_count()) {\n\t\tWARN_ONCE(1, \"timer: %pS preempt leak: %08x -> %08x\\n\",\n\t\t\t  fn, count, preempt_count());\n\t\t \n\t\tpreempt_count_set(count);\n\t}\n}\n\nstatic void expire_timers(struct timer_base *base, struct hlist_head *head)\n{\n\t \n\tunsigned long baseclk = base->clk - 1;\n\n\twhile (!hlist_empty(head)) {\n\t\tstruct timer_list *timer;\n\t\tvoid (*fn)(struct timer_list *);\n\n\t\ttimer = hlist_entry(head->first, struct timer_list, entry);\n\n\t\tbase->running_timer = timer;\n\t\tdetach_timer(timer, true);\n\n\t\tfn = timer->function;\n\n\t\tif (WARN_ON_ONCE(!fn)) {\n\t\t\t \n\t\t\tbase->running_timer = NULL;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (timer->flags & TIMER_IRQSAFE) {\n\t\t\traw_spin_unlock(&base->lock);\n\t\t\tcall_timer_fn(timer, fn, baseclk);\n\t\t\traw_spin_lock(&base->lock);\n\t\t\tbase->running_timer = NULL;\n\t\t} else {\n\t\t\traw_spin_unlock_irq(&base->lock);\n\t\t\tcall_timer_fn(timer, fn, baseclk);\n\t\t\traw_spin_lock_irq(&base->lock);\n\t\t\tbase->running_timer = NULL;\n\t\t\ttimer_sync_wait_running(base);\n\t\t}\n\t}\n}\n\nstatic int collect_expired_timers(struct timer_base *base,\n\t\t\t\t  struct hlist_head *heads)\n{\n\tunsigned long clk = base->clk = base->next_expiry;\n\tstruct hlist_head *vec;\n\tint i, levels = 0;\n\tunsigned int idx;\n\n\tfor (i = 0; i < LVL_DEPTH; i++) {\n\t\tidx = (clk & LVL_MASK) + i * LVL_SIZE;\n\n\t\tif (__test_and_clear_bit(idx, base->pending_map)) {\n\t\t\tvec = base->vectors + idx;\n\t\t\thlist_move_list(vec, heads++);\n\t\t\tlevels++;\n\t\t}\n\t\t \n\t\tif (clk & LVL_CLK_MASK)\n\t\t\tbreak;\n\t\t \n\t\tclk >>= LVL_CLK_SHIFT;\n\t}\n\treturn levels;\n}\n\n \nstatic int next_pending_bucket(struct timer_base *base, unsigned offset,\n\t\t\t       unsigned clk)\n{\n\tunsigned pos, start = offset + clk;\n\tunsigned end = offset + LVL_SIZE;\n\n\tpos = find_next_bit(base->pending_map, end, start);\n\tif (pos < end)\n\t\treturn pos - start;\n\n\tpos = find_next_bit(base->pending_map, start, offset);\n\treturn pos < start ? pos + LVL_SIZE - start : -1;\n}\n\n \nstatic unsigned long __next_timer_interrupt(struct timer_base *base)\n{\n\tunsigned long clk, next, adj;\n\tunsigned lvl, offset = 0;\n\n\tnext = base->clk + NEXT_TIMER_MAX_DELTA;\n\tclk = base->clk;\n\tfor (lvl = 0; lvl < LVL_DEPTH; lvl++, offset += LVL_SIZE) {\n\t\tint pos = next_pending_bucket(base, offset, clk & LVL_MASK);\n\t\tunsigned long lvl_clk = clk & LVL_CLK_MASK;\n\n\t\tif (pos >= 0) {\n\t\t\tunsigned long tmp = clk + (unsigned long) pos;\n\n\t\t\ttmp <<= LVL_SHIFT(lvl);\n\t\t\tif (time_before(tmp, next))\n\t\t\t\tnext = tmp;\n\n\t\t\t \n\t\t\tif (pos <= ((LVL_CLK_DIV - lvl_clk) & LVL_CLK_MASK))\n\t\t\t\tbreak;\n\t\t}\n\t\t \n\t\tadj = lvl_clk ? 1 : 0;\n\t\tclk >>= LVL_CLK_SHIFT;\n\t\tclk += adj;\n\t}\n\n\tbase->next_expiry_recalc = false;\n\tbase->timers_pending = !(next == base->clk + NEXT_TIMER_MAX_DELTA);\n\n\treturn next;\n}\n\n#ifdef CONFIG_NO_HZ_COMMON\n \nstatic u64 cmp_next_hrtimer_event(u64 basem, u64 expires)\n{\n\tu64 nextevt = hrtimer_get_next_event();\n\n\t \n\tif (expires <= nextevt)\n\t\treturn expires;\n\n\t \n\tif (nextevt <= basem)\n\t\treturn basem;\n\n\t \n\treturn DIV_ROUND_UP_ULL(nextevt, TICK_NSEC) * TICK_NSEC;\n}\n\n \nu64 get_next_timer_interrupt(unsigned long basej, u64 basem)\n{\n\tstruct timer_base *base = this_cpu_ptr(&timer_bases[BASE_STD]);\n\tu64 expires = KTIME_MAX;\n\tunsigned long nextevt;\n\n\t \n\tif (cpu_is_offline(smp_processor_id()))\n\t\treturn expires;\n\n\traw_spin_lock(&base->lock);\n\tif (base->next_expiry_recalc)\n\t\tbase->next_expiry = __next_timer_interrupt(base);\n\tnextevt = base->next_expiry;\n\n\t \n\tif (time_after(basej, base->clk)) {\n\t\tif (time_after(nextevt, basej))\n\t\t\tbase->clk = basej;\n\t\telse if (time_after(nextevt, base->clk))\n\t\t\tbase->clk = nextevt;\n\t}\n\n\tif (time_before_eq(nextevt, basej)) {\n\t\texpires = basem;\n\t\tbase->is_idle = false;\n\t} else {\n\t\tif (base->timers_pending)\n\t\t\texpires = basem + (u64)(nextevt - basej) * TICK_NSEC;\n\t\t \n\t\tif ((expires - basem) > TICK_NSEC)\n\t\t\tbase->is_idle = true;\n\t}\n\traw_spin_unlock(&base->lock);\n\n\treturn cmp_next_hrtimer_event(basem, expires);\n}\n\n \nvoid timer_clear_idle(void)\n{\n\tstruct timer_base *base = this_cpu_ptr(&timer_bases[BASE_STD]);\n\n\t \n\tbase->is_idle = false;\n}\n#endif\n\n \nstatic inline void __run_timers(struct timer_base *base)\n{\n\tstruct hlist_head heads[LVL_DEPTH];\n\tint levels;\n\n\tif (time_before(jiffies, base->next_expiry))\n\t\treturn;\n\n\ttimer_base_lock_expiry(base);\n\traw_spin_lock_irq(&base->lock);\n\n\twhile (time_after_eq(jiffies, base->clk) &&\n\t       time_after_eq(jiffies, base->next_expiry)) {\n\t\tlevels = collect_expired_timers(base, heads);\n\t\t \n\t\tWARN_ON_ONCE(!levels && !base->next_expiry_recalc\n\t\t\t     && base->timers_pending);\n\t\tbase->clk++;\n\t\tbase->next_expiry = __next_timer_interrupt(base);\n\n\t\twhile (levels--)\n\t\t\texpire_timers(base, heads + levels);\n\t}\n\traw_spin_unlock_irq(&base->lock);\n\ttimer_base_unlock_expiry(base);\n}\n\n \nstatic __latent_entropy void run_timer_softirq(struct softirq_action *h)\n{\n\tstruct timer_base *base = this_cpu_ptr(&timer_bases[BASE_STD]);\n\n\t__run_timers(base);\n\tif (IS_ENABLED(CONFIG_NO_HZ_COMMON))\n\t\t__run_timers(this_cpu_ptr(&timer_bases[BASE_DEF]));\n}\n\n \nstatic void run_local_timers(void)\n{\n\tstruct timer_base *base = this_cpu_ptr(&timer_bases[BASE_STD]);\n\n\thrtimer_run_queues();\n\t \n\tif (time_before(jiffies, base->next_expiry)) {\n\t\tif (!IS_ENABLED(CONFIG_NO_HZ_COMMON))\n\t\t\treturn;\n\t\t \n\t\tbase++;\n\t\tif (time_before(jiffies, base->next_expiry))\n\t\t\treturn;\n\t}\n\traise_softirq(TIMER_SOFTIRQ);\n}\n\n \nvoid update_process_times(int user_tick)\n{\n\tstruct task_struct *p = current;\n\n\t \n\taccount_process_tick(p, user_tick);\n\trun_local_timers();\n\trcu_sched_clock_irq(user_tick);\n#ifdef CONFIG_IRQ_WORK\n\tif (in_irq())\n\t\tirq_work_tick();\n#endif\n\tscheduler_tick();\n\tif (IS_ENABLED(CONFIG_POSIX_TIMERS))\n\t\trun_posix_cpu_timers();\n}\n\n \nstruct process_timer {\n\tstruct timer_list timer;\n\tstruct task_struct *task;\n};\n\nstatic void process_timeout(struct timer_list *t)\n{\n\tstruct process_timer *timeout = from_timer(timeout, t, timer);\n\n\twake_up_process(timeout->task);\n}\n\n \nsigned long __sched schedule_timeout(signed long timeout)\n{\n\tstruct process_timer timer;\n\tunsigned long expire;\n\n\tswitch (timeout)\n\t{\n\tcase MAX_SCHEDULE_TIMEOUT:\n\t\t \n\t\tschedule();\n\t\tgoto out;\n\tdefault:\n\t\t \n\t\tif (timeout < 0) {\n\t\t\tprintk(KERN_ERR \"schedule_timeout: wrong timeout \"\n\t\t\t\t\"value %lx\\n\", timeout);\n\t\t\tdump_stack();\n\t\t\t__set_current_state(TASK_RUNNING);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\texpire = timeout + jiffies;\n\n\ttimer.task = current;\n\ttimer_setup_on_stack(&timer.timer, process_timeout, 0);\n\t__mod_timer(&timer.timer, expire, MOD_TIMER_NOTPENDING);\n\tschedule();\n\tdel_timer_sync(&timer.timer);\n\n\t \n\tdestroy_timer_on_stack(&timer.timer);\n\n\ttimeout = expire - jiffies;\n\n out:\n\treturn timeout < 0 ? 0 : timeout;\n}\nEXPORT_SYMBOL(schedule_timeout);\n\n \nsigned long __sched schedule_timeout_interruptible(signed long timeout)\n{\n\t__set_current_state(TASK_INTERRUPTIBLE);\n\treturn schedule_timeout(timeout);\n}\nEXPORT_SYMBOL(schedule_timeout_interruptible);\n\nsigned long __sched schedule_timeout_killable(signed long timeout)\n{\n\t__set_current_state(TASK_KILLABLE);\n\treturn schedule_timeout(timeout);\n}\nEXPORT_SYMBOL(schedule_timeout_killable);\n\nsigned long __sched schedule_timeout_uninterruptible(signed long timeout)\n{\n\t__set_current_state(TASK_UNINTERRUPTIBLE);\n\treturn schedule_timeout(timeout);\n}\nEXPORT_SYMBOL(schedule_timeout_uninterruptible);\n\n \nsigned long __sched schedule_timeout_idle(signed long timeout)\n{\n\t__set_current_state(TASK_IDLE);\n\treturn schedule_timeout(timeout);\n}\nEXPORT_SYMBOL(schedule_timeout_idle);\n\n#ifdef CONFIG_HOTPLUG_CPU\nstatic void migrate_timer_list(struct timer_base *new_base, struct hlist_head *head)\n{\n\tstruct timer_list *timer;\n\tint cpu = new_base->cpu;\n\n\twhile (!hlist_empty(head)) {\n\t\ttimer = hlist_entry(head->first, struct timer_list, entry);\n\t\tdetach_timer(timer, false);\n\t\ttimer->flags = (timer->flags & ~TIMER_BASEMASK) | cpu;\n\t\tinternal_add_timer(new_base, timer);\n\t}\n}\n\nint timers_prepare_cpu(unsigned int cpu)\n{\n\tstruct timer_base *base;\n\tint b;\n\n\tfor (b = 0; b < NR_BASES; b++) {\n\t\tbase = per_cpu_ptr(&timer_bases[b], cpu);\n\t\tbase->clk = jiffies;\n\t\tbase->next_expiry = base->clk + NEXT_TIMER_MAX_DELTA;\n\t\tbase->next_expiry_recalc = false;\n\t\tbase->timers_pending = false;\n\t\tbase->is_idle = false;\n\t}\n\treturn 0;\n}\n\nint timers_dead_cpu(unsigned int cpu)\n{\n\tstruct timer_base *old_base;\n\tstruct timer_base *new_base;\n\tint b, i;\n\n\tfor (b = 0; b < NR_BASES; b++) {\n\t\told_base = per_cpu_ptr(&timer_bases[b], cpu);\n\t\tnew_base = get_cpu_ptr(&timer_bases[b]);\n\t\t \n\t\traw_spin_lock_irq(&new_base->lock);\n\t\traw_spin_lock_nested(&old_base->lock, SINGLE_DEPTH_NESTING);\n\n\t\t \n\t\tforward_timer_base(new_base);\n\n\t\tWARN_ON_ONCE(old_base->running_timer);\n\t\told_base->running_timer = NULL;\n\n\t\tfor (i = 0; i < WHEEL_SIZE; i++)\n\t\t\tmigrate_timer_list(new_base, old_base->vectors + i);\n\n\t\traw_spin_unlock(&old_base->lock);\n\t\traw_spin_unlock_irq(&new_base->lock);\n\t\tput_cpu_ptr(&timer_bases);\n\t}\n\treturn 0;\n}\n\n#endif  \n\nstatic void __init init_timer_cpu(int cpu)\n{\n\tstruct timer_base *base;\n\tint i;\n\n\tfor (i = 0; i < NR_BASES; i++) {\n\t\tbase = per_cpu_ptr(&timer_bases[i], cpu);\n\t\tbase->cpu = cpu;\n\t\traw_spin_lock_init(&base->lock);\n\t\tbase->clk = jiffies;\n\t\tbase->next_expiry = base->clk + NEXT_TIMER_MAX_DELTA;\n\t\ttimer_base_init_expiry_lock(base);\n\t}\n}\n\nstatic void __init init_timer_cpus(void)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu)\n\t\tinit_timer_cpu(cpu);\n}\n\nvoid __init init_timers(void)\n{\n\tinit_timer_cpus();\n\tposix_cputimers_init_work();\n\topen_softirq(TIMER_SOFTIRQ, run_timer_softirq);\n}\n\n \nvoid msleep(unsigned int msecs)\n{\n\tunsigned long timeout = msecs_to_jiffies(msecs) + 1;\n\n\twhile (timeout)\n\t\ttimeout = schedule_timeout_uninterruptible(timeout);\n}\n\nEXPORT_SYMBOL(msleep);\n\n \nunsigned long msleep_interruptible(unsigned int msecs)\n{\n\tunsigned long timeout = msecs_to_jiffies(msecs) + 1;\n\n\twhile (timeout && !signal_pending(current))\n\t\ttimeout = schedule_timeout_interruptible(timeout);\n\treturn jiffies_to_msecs(timeout);\n}\n\nEXPORT_SYMBOL(msleep_interruptible);\n\n \nvoid __sched usleep_range_state(unsigned long min, unsigned long max,\n\t\t\t\tunsigned int state)\n{\n\tktime_t exp = ktime_add_us(ktime_get(), min);\n\tu64 delta = (u64)(max - min) * NSEC_PER_USEC;\n\n\tfor (;;) {\n\t\t__set_current_state(state);\n\t\t \n\t\tif (!schedule_hrtimeout_range(&exp, delta, HRTIMER_MODE_ABS))\n\t\t\tbreak;\n\t}\n}\nEXPORT_SYMBOL(usleep_range_state);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}