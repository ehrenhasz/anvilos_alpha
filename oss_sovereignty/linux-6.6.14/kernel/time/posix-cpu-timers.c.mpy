{
  "module_name": "posix-cpu-timers.c",
  "hash_id": "9cc32253cefb9e408164f21f5b3d47db39cc405aa49b324d21a5c4e3571db2ac",
  "original_prompt": "Ingested from linux-6.6.14/kernel/time/posix-cpu-timers.c",
  "human_readable_source": "\n \n\n#include <linux/sched/signal.h>\n#include <linux/sched/cputime.h>\n#include <linux/posix-timers.h>\n#include <linux/errno.h>\n#include <linux/math64.h>\n#include <linux/uaccess.h>\n#include <linux/kernel_stat.h>\n#include <trace/events/timer.h>\n#include <linux/tick.h>\n#include <linux/workqueue.h>\n#include <linux/compat.h>\n#include <linux/sched/deadline.h>\n#include <linux/task_work.h>\n\n#include \"posix-timers.h\"\n\nstatic void posix_cpu_timer_rearm(struct k_itimer *timer);\n\nvoid posix_cputimers_group_init(struct posix_cputimers *pct, u64 cpu_limit)\n{\n\tposix_cputimers_init(pct);\n\tif (cpu_limit != RLIM_INFINITY) {\n\t\tpct->bases[CPUCLOCK_PROF].nextevt = cpu_limit * NSEC_PER_SEC;\n\t\tpct->timers_active = true;\n\t}\n}\n\n \nint update_rlimit_cpu(struct task_struct *task, unsigned long rlim_new)\n{\n\tu64 nsecs = rlim_new * NSEC_PER_SEC;\n\tunsigned long irq_fl;\n\n\tif (!lock_task_sighand(task, &irq_fl))\n\t\treturn -ESRCH;\n\tset_process_cpu_timer(task, CPUCLOCK_PROF, &nsecs, NULL);\n\tunlock_task_sighand(task, &irq_fl);\n\treturn 0;\n}\n\n \nstatic struct pid *pid_for_clock(const clockid_t clock, bool gettime)\n{\n\tconst bool thread = !!CPUCLOCK_PERTHREAD(clock);\n\tconst pid_t upid = CPUCLOCK_PID(clock);\n\tstruct pid *pid;\n\n\tif (CPUCLOCK_WHICH(clock) >= CPUCLOCK_MAX)\n\t\treturn NULL;\n\n\t \n\tif (upid == 0)\n\t\treturn thread ? task_pid(current) : task_tgid(current);\n\n\tpid = find_vpid(upid);\n\tif (!pid)\n\t\treturn NULL;\n\n\tif (thread) {\n\t\tstruct task_struct *tsk = pid_task(pid, PIDTYPE_PID);\n\t\treturn (tsk && same_thread_group(tsk, current)) ? pid : NULL;\n\t}\n\n\t \n\tif (gettime && (pid == task_pid(current)))\n\t\treturn task_tgid(current);\n\n\t \n\treturn pid_has_task(pid, PIDTYPE_TGID) ? pid : NULL;\n}\n\nstatic inline int validate_clock_permissions(const clockid_t clock)\n{\n\tint ret;\n\n\trcu_read_lock();\n\tret = pid_for_clock(clock, false) ? 0 : -EINVAL;\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\nstatic inline enum pid_type clock_pid_type(const clockid_t clock)\n{\n\treturn CPUCLOCK_PERTHREAD(clock) ? PIDTYPE_PID : PIDTYPE_TGID;\n}\n\nstatic inline struct task_struct *cpu_timer_task_rcu(struct k_itimer *timer)\n{\n\treturn pid_task(timer->it.cpu.pid, clock_pid_type(timer->it_clock));\n}\n\n \nstatic u64 bump_cpu_timer(struct k_itimer *timer, u64 now)\n{\n\tu64 delta, incr, expires = timer->it.cpu.node.expires;\n\tint i;\n\n\tif (!timer->it_interval)\n\t\treturn expires;\n\n\tif (now < expires)\n\t\treturn expires;\n\n\tincr = timer->it_interval;\n\tdelta = now + incr - expires;\n\n\t \n\tfor (i = 0; incr < delta - incr; i++)\n\t\tincr = incr << 1;\n\n\tfor (; i >= 0; incr >>= 1, i--) {\n\t\tif (delta < incr)\n\t\t\tcontinue;\n\n\t\ttimer->it.cpu.node.expires += incr;\n\t\ttimer->it_overrun += 1LL << i;\n\t\tdelta -= incr;\n\t}\n\treturn timer->it.cpu.node.expires;\n}\n\n \nstatic inline bool expiry_cache_is_inactive(const struct posix_cputimers *pct)\n{\n\treturn !(~pct->bases[CPUCLOCK_PROF].nextevt |\n\t\t ~pct->bases[CPUCLOCK_VIRT].nextevt |\n\t\t ~pct->bases[CPUCLOCK_SCHED].nextevt);\n}\n\nstatic int\nposix_cpu_clock_getres(const clockid_t which_clock, struct timespec64 *tp)\n{\n\tint error = validate_clock_permissions(which_clock);\n\n\tif (!error) {\n\t\ttp->tv_sec = 0;\n\t\ttp->tv_nsec = ((NSEC_PER_SEC + HZ - 1) / HZ);\n\t\tif (CPUCLOCK_WHICH(which_clock) == CPUCLOCK_SCHED) {\n\t\t\t \n\t\t\ttp->tv_nsec = 1;\n\t\t}\n\t}\n\treturn error;\n}\n\nstatic int\nposix_cpu_clock_set(const clockid_t clock, const struct timespec64 *tp)\n{\n\tint error = validate_clock_permissions(clock);\n\n\t \n\treturn error ? : -EPERM;\n}\n\n \nstatic u64 cpu_clock_sample(const clockid_t clkid, struct task_struct *p)\n{\n\tu64 utime, stime;\n\n\tif (clkid == CPUCLOCK_SCHED)\n\t\treturn task_sched_runtime(p);\n\n\ttask_cputime(p, &utime, &stime);\n\n\tswitch (clkid) {\n\tcase CPUCLOCK_PROF:\n\t\treturn utime + stime;\n\tcase CPUCLOCK_VIRT:\n\t\treturn utime;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t}\n\treturn 0;\n}\n\nstatic inline void store_samples(u64 *samples, u64 stime, u64 utime, u64 rtime)\n{\n\tsamples[CPUCLOCK_PROF] = stime + utime;\n\tsamples[CPUCLOCK_VIRT] = utime;\n\tsamples[CPUCLOCK_SCHED] = rtime;\n}\n\nstatic void task_sample_cputime(struct task_struct *p, u64 *samples)\n{\n\tu64 stime, utime;\n\n\ttask_cputime(p, &utime, &stime);\n\tstore_samples(samples, stime, utime, p->se.sum_exec_runtime);\n}\n\nstatic void proc_sample_cputime_atomic(struct task_cputime_atomic *at,\n\t\t\t\t       u64 *samples)\n{\n\tu64 stime, utime, rtime;\n\n\tutime = atomic64_read(&at->utime);\n\tstime = atomic64_read(&at->stime);\n\trtime = atomic64_read(&at->sum_exec_runtime);\n\tstore_samples(samples, stime, utime, rtime);\n}\n\n \nstatic inline void __update_gt_cputime(atomic64_t *cputime, u64 sum_cputime)\n{\n\tu64 curr_cputime = atomic64_read(cputime);\n\n\tdo {\n\t\tif (sum_cputime <= curr_cputime)\n\t\t\treturn;\n\t} while (!atomic64_try_cmpxchg(cputime, &curr_cputime, sum_cputime));\n}\n\nstatic void update_gt_cputime(struct task_cputime_atomic *cputime_atomic,\n\t\t\t      struct task_cputime *sum)\n{\n\t__update_gt_cputime(&cputime_atomic->utime, sum->utime);\n\t__update_gt_cputime(&cputime_atomic->stime, sum->stime);\n\t__update_gt_cputime(&cputime_atomic->sum_exec_runtime, sum->sum_exec_runtime);\n}\n\n \nvoid thread_group_sample_cputime(struct task_struct *tsk, u64 *samples)\n{\n\tstruct thread_group_cputimer *cputimer = &tsk->signal->cputimer;\n\tstruct posix_cputimers *pct = &tsk->signal->posix_cputimers;\n\n\tWARN_ON_ONCE(!pct->timers_active);\n\n\tproc_sample_cputime_atomic(&cputimer->cputime_atomic, samples);\n}\n\n \nstatic void thread_group_start_cputime(struct task_struct *tsk, u64 *samples)\n{\n\tstruct thread_group_cputimer *cputimer = &tsk->signal->cputimer;\n\tstruct posix_cputimers *pct = &tsk->signal->posix_cputimers;\n\n\tlockdep_assert_task_sighand_held(tsk);\n\n\t \n\tif (!READ_ONCE(pct->timers_active)) {\n\t\tstruct task_cputime sum;\n\n\t\t \n\t\tthread_group_cputime(tsk, &sum);\n\t\tupdate_gt_cputime(&cputimer->cputime_atomic, &sum);\n\n\t\t \n\t\tWRITE_ONCE(pct->timers_active, true);\n\t}\n\tproc_sample_cputime_atomic(&cputimer->cputime_atomic, samples);\n}\n\nstatic void __thread_group_cputime(struct task_struct *tsk, u64 *samples)\n{\n\tstruct task_cputime ct;\n\n\tthread_group_cputime(tsk, &ct);\n\tstore_samples(samples, ct.stime, ct.utime, ct.sum_exec_runtime);\n}\n\n \nstatic u64 cpu_clock_sample_group(const clockid_t clkid, struct task_struct *p,\n\t\t\t\t  bool start)\n{\n\tstruct thread_group_cputimer *cputimer = &p->signal->cputimer;\n\tstruct posix_cputimers *pct = &p->signal->posix_cputimers;\n\tu64 samples[CPUCLOCK_MAX];\n\n\tif (!READ_ONCE(pct->timers_active)) {\n\t\tif (start)\n\t\t\tthread_group_start_cputime(p, samples);\n\t\telse\n\t\t\t__thread_group_cputime(p, samples);\n\t} else {\n\t\tproc_sample_cputime_atomic(&cputimer->cputime_atomic, samples);\n\t}\n\n\treturn samples[clkid];\n}\n\nstatic int posix_cpu_clock_get(const clockid_t clock, struct timespec64 *tp)\n{\n\tconst clockid_t clkid = CPUCLOCK_WHICH(clock);\n\tstruct task_struct *tsk;\n\tu64 t;\n\n\trcu_read_lock();\n\ttsk = pid_task(pid_for_clock(clock, true), clock_pid_type(clock));\n\tif (!tsk) {\n\t\trcu_read_unlock();\n\t\treturn -EINVAL;\n\t}\n\n\tif (CPUCLOCK_PERTHREAD(clock))\n\t\tt = cpu_clock_sample(clkid, tsk);\n\telse\n\t\tt = cpu_clock_sample_group(clkid, tsk, false);\n\trcu_read_unlock();\n\n\t*tp = ns_to_timespec64(t);\n\treturn 0;\n}\n\n \nstatic int posix_cpu_timer_create(struct k_itimer *new_timer)\n{\n\tstatic struct lock_class_key posix_cpu_timers_key;\n\tstruct pid *pid;\n\n\trcu_read_lock();\n\tpid = pid_for_clock(new_timer->it_clock, false);\n\tif (!pid) {\n\t\trcu_read_unlock();\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (IS_ENABLED(CONFIG_POSIX_CPU_TIMERS_TASK_WORK))\n\t\tlockdep_set_class(&new_timer->it_lock, &posix_cpu_timers_key);\n\n\tnew_timer->kclock = &clock_posix_cpu;\n\ttimerqueue_init(&new_timer->it.cpu.node);\n\tnew_timer->it.cpu.pid = get_pid(pid);\n\trcu_read_unlock();\n\treturn 0;\n}\n\nstatic struct posix_cputimer_base *timer_base(struct k_itimer *timer,\n\t\t\t\t\t      struct task_struct *tsk)\n{\n\tint clkidx = CPUCLOCK_WHICH(timer->it_clock);\n\n\tif (CPUCLOCK_PERTHREAD(timer->it_clock))\n\t\treturn tsk->posix_cputimers.bases + clkidx;\n\telse\n\t\treturn tsk->signal->posix_cputimers.bases + clkidx;\n}\n\n \nstatic void trigger_base_recalc_expires(struct k_itimer *timer,\n\t\t\t\t\tstruct task_struct *tsk)\n{\n\tstruct posix_cputimer_base *base = timer_base(timer, tsk);\n\n\tbase->nextevt = 0;\n}\n\n \nstatic void disarm_timer(struct k_itimer *timer, struct task_struct *p)\n{\n\tstruct cpu_timer *ctmr = &timer->it.cpu;\n\tstruct posix_cputimer_base *base;\n\n\tif (!cpu_timer_dequeue(ctmr))\n\t\treturn;\n\n\tbase = timer_base(timer, p);\n\tif (cpu_timer_getexpires(ctmr) == base->nextevt)\n\t\ttrigger_base_recalc_expires(timer, p);\n}\n\n\n \nstatic int posix_cpu_timer_del(struct k_itimer *timer)\n{\n\tstruct cpu_timer *ctmr = &timer->it.cpu;\n\tstruct sighand_struct *sighand;\n\tstruct task_struct *p;\n\tunsigned long flags;\n\tint ret = 0;\n\n\trcu_read_lock();\n\tp = cpu_timer_task_rcu(timer);\n\tif (!p)\n\t\tgoto out;\n\n\t \n\tsighand = lock_task_sighand(p, &flags);\n\tif (unlikely(sighand == NULL)) {\n\t\t \n\t\tWARN_ON_ONCE(ctmr->head || timerqueue_node_queued(&ctmr->node));\n\t} else {\n\t\tif (timer->it.cpu.firing)\n\t\t\tret = TIMER_RETRY;\n\t\telse\n\t\t\tdisarm_timer(timer, p);\n\n\t\tunlock_task_sighand(p, &flags);\n\t}\n\nout:\n\trcu_read_unlock();\n\tif (!ret)\n\t\tput_pid(ctmr->pid);\n\n\treturn ret;\n}\n\nstatic void cleanup_timerqueue(struct timerqueue_head *head)\n{\n\tstruct timerqueue_node *node;\n\tstruct cpu_timer *ctmr;\n\n\twhile ((node = timerqueue_getnext(head))) {\n\t\ttimerqueue_del(head, node);\n\t\tctmr = container_of(node, struct cpu_timer, node);\n\t\tctmr->head = NULL;\n\t}\n}\n\n \nstatic void cleanup_timers(struct posix_cputimers *pct)\n{\n\tcleanup_timerqueue(&pct->bases[CPUCLOCK_PROF].tqhead);\n\tcleanup_timerqueue(&pct->bases[CPUCLOCK_VIRT].tqhead);\n\tcleanup_timerqueue(&pct->bases[CPUCLOCK_SCHED].tqhead);\n}\n\n \nvoid posix_cpu_timers_exit(struct task_struct *tsk)\n{\n\tcleanup_timers(&tsk->posix_cputimers);\n}\nvoid posix_cpu_timers_exit_group(struct task_struct *tsk)\n{\n\tcleanup_timers(&tsk->signal->posix_cputimers);\n}\n\n \nstatic void arm_timer(struct k_itimer *timer, struct task_struct *p)\n{\n\tstruct posix_cputimer_base *base = timer_base(timer, p);\n\tstruct cpu_timer *ctmr = &timer->it.cpu;\n\tu64 newexp = cpu_timer_getexpires(ctmr);\n\n\tif (!cpu_timer_enqueue(&base->tqhead, ctmr))\n\t\treturn;\n\n\t \n\tif (newexp < base->nextevt)\n\t\tbase->nextevt = newexp;\n\n\tif (CPUCLOCK_PERTHREAD(timer->it_clock))\n\t\ttick_dep_set_task(p, TICK_DEP_BIT_POSIX_TIMER);\n\telse\n\t\ttick_dep_set_signal(p, TICK_DEP_BIT_POSIX_TIMER);\n}\n\n \nstatic void cpu_timer_fire(struct k_itimer *timer)\n{\n\tstruct cpu_timer *ctmr = &timer->it.cpu;\n\n\tif ((timer->it_sigev_notify & ~SIGEV_THREAD_ID) == SIGEV_NONE) {\n\t\t \n\t\tcpu_timer_setexpires(ctmr, 0);\n\t} else if (unlikely(timer->sigq == NULL)) {\n\t\t \n\t\twake_up_process(timer->it_process);\n\t\tcpu_timer_setexpires(ctmr, 0);\n\t} else if (!timer->it_interval) {\n\t\t \n\t\tposix_timer_event(timer, 0);\n\t\tcpu_timer_setexpires(ctmr, 0);\n\t} else if (posix_timer_event(timer, ++timer->it_requeue_pending)) {\n\t\t \n\t\tposix_cpu_timer_rearm(timer);\n\t\t++timer->it_requeue_pending;\n\t}\n}\n\n \nstatic int posix_cpu_timer_set(struct k_itimer *timer, int timer_flags,\n\t\t\t       struct itimerspec64 *new, struct itimerspec64 *old)\n{\n\tclockid_t clkid = CPUCLOCK_WHICH(timer->it_clock);\n\tu64 old_expires, new_expires, old_incr, val;\n\tstruct cpu_timer *ctmr = &timer->it.cpu;\n\tstruct sighand_struct *sighand;\n\tstruct task_struct *p;\n\tunsigned long flags;\n\tint ret = 0;\n\n\trcu_read_lock();\n\tp = cpu_timer_task_rcu(timer);\n\tif (!p) {\n\t\t \n\t\trcu_read_unlock();\n\t\treturn -ESRCH;\n\t}\n\n\t \n\tnew_expires = ktime_to_ns(timespec64_to_ktime(new->it_value));\n\n\t \n\tsighand = lock_task_sighand(p, &flags);\n\t \n\tif (unlikely(sighand == NULL)) {\n\t\trcu_read_unlock();\n\t\treturn -ESRCH;\n\t}\n\n\t \n\told_incr = timer->it_interval;\n\told_expires = cpu_timer_getexpires(ctmr);\n\n\tif (unlikely(timer->it.cpu.firing)) {\n\t\ttimer->it.cpu.firing = -1;\n\t\tret = TIMER_RETRY;\n\t} else {\n\t\tcpu_timer_dequeue(ctmr);\n\t}\n\n\t \n\tif (CPUCLOCK_PERTHREAD(timer->it_clock))\n\t\tval = cpu_clock_sample(clkid, p);\n\telse\n\t\tval = cpu_clock_sample_group(clkid, p, true);\n\n\tif (old) {\n\t\tif (old_expires == 0) {\n\t\t\told->it_value.tv_sec = 0;\n\t\t\told->it_value.tv_nsec = 0;\n\t\t} else {\n\t\t\t \n\t\t\tu64 exp = bump_cpu_timer(timer, val);\n\n\t\t\tif (val < exp) {\n\t\t\t\told_expires = exp - val;\n\t\t\t\told->it_value = ns_to_timespec64(old_expires);\n\t\t\t} else {\n\t\t\t\told->it_value.tv_nsec = 1;\n\t\t\t\told->it_value.tv_sec = 0;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (unlikely(ret)) {\n\t\t \n\t\tunlock_task_sighand(p, &flags);\n\t\tgoto out;\n\t}\n\n\tif (new_expires != 0 && !(timer_flags & TIMER_ABSTIME)) {\n\t\tnew_expires += val;\n\t}\n\n\t \n\tcpu_timer_setexpires(ctmr, new_expires);\n\tif (new_expires != 0 && val < new_expires) {\n\t\tarm_timer(timer, p);\n\t}\n\n\tunlock_task_sighand(p, &flags);\n\t \n\ttimer->it_interval = timespec64_to_ktime(new->it_interval);\n\n\t \n\ttimer->it_requeue_pending = (timer->it_requeue_pending + 2) &\n\t\t~REQUEUE_PENDING;\n\ttimer->it_overrun_last = 0;\n\ttimer->it_overrun = -1;\n\n\tif (val >= new_expires) {\n\t\tif (new_expires != 0) {\n\t\t\t \n\t\t\tcpu_timer_fire(timer);\n\t\t}\n\n\t\t \n\t\tsighand = lock_task_sighand(p, &flags);\n\t\tif (!sighand)\n\t\t\tgoto out;\n\n\t\tif (!cpu_timer_queued(ctmr))\n\t\t\ttrigger_base_recalc_expires(timer, p);\n\n\t\tunlock_task_sighand(p, &flags);\n\t}\n out:\n\trcu_read_unlock();\n\tif (old)\n\t\told->it_interval = ns_to_timespec64(old_incr);\n\n\treturn ret;\n}\n\nstatic void posix_cpu_timer_get(struct k_itimer *timer, struct itimerspec64 *itp)\n{\n\tclockid_t clkid = CPUCLOCK_WHICH(timer->it_clock);\n\tstruct cpu_timer *ctmr = &timer->it.cpu;\n\tu64 now, expires = cpu_timer_getexpires(ctmr);\n\tstruct task_struct *p;\n\n\trcu_read_lock();\n\tp = cpu_timer_task_rcu(timer);\n\tif (!p)\n\t\tgoto out;\n\n\t \n\titp->it_interval = ktime_to_timespec64(timer->it_interval);\n\n\tif (!expires)\n\t\tgoto out;\n\n\t \n\tif (CPUCLOCK_PERTHREAD(timer->it_clock))\n\t\tnow = cpu_clock_sample(clkid, p);\n\telse\n\t\tnow = cpu_clock_sample_group(clkid, p, false);\n\n\tif (now < expires) {\n\t\titp->it_value = ns_to_timespec64(expires - now);\n\t} else {\n\t\t \n\t\titp->it_value.tv_nsec = 1;\n\t\titp->it_value.tv_sec = 0;\n\t}\nout:\n\trcu_read_unlock();\n}\n\n#define MAX_COLLECTED\t20\n\nstatic u64 collect_timerqueue(struct timerqueue_head *head,\n\t\t\t      struct list_head *firing, u64 now)\n{\n\tstruct timerqueue_node *next;\n\tint i = 0;\n\n\twhile ((next = timerqueue_getnext(head))) {\n\t\tstruct cpu_timer *ctmr;\n\t\tu64 expires;\n\n\t\tctmr = container_of(next, struct cpu_timer, node);\n\t\texpires = cpu_timer_getexpires(ctmr);\n\t\t \n\t\tif (++i == MAX_COLLECTED || now < expires)\n\t\t\treturn expires;\n\n\t\tctmr->firing = 1;\n\t\t \n\t\trcu_assign_pointer(ctmr->handling, current);\n\t\tcpu_timer_dequeue(ctmr);\n\t\tlist_add_tail(&ctmr->elist, firing);\n\t}\n\n\treturn U64_MAX;\n}\n\nstatic void collect_posix_cputimers(struct posix_cputimers *pct, u64 *samples,\n\t\t\t\t    struct list_head *firing)\n{\n\tstruct posix_cputimer_base *base = pct->bases;\n\tint i;\n\n\tfor (i = 0; i < CPUCLOCK_MAX; i++, base++) {\n\t\tbase->nextevt = collect_timerqueue(&base->tqhead, firing,\n\t\t\t\t\t\t    samples[i]);\n\t}\n}\n\nstatic inline void check_dl_overrun(struct task_struct *tsk)\n{\n\tif (tsk->dl.dl_overrun) {\n\t\ttsk->dl.dl_overrun = 0;\n\t\tsend_signal_locked(SIGXCPU, SEND_SIG_PRIV, tsk, PIDTYPE_TGID);\n\t}\n}\n\nstatic bool check_rlimit(u64 time, u64 limit, int signo, bool rt, bool hard)\n{\n\tif (time < limit)\n\t\treturn false;\n\n\tif (print_fatal_signals) {\n\t\tpr_info(\"%s Watchdog Timeout (%s): %s[%d]\\n\",\n\t\t\trt ? \"RT\" : \"CPU\", hard ? \"hard\" : \"soft\",\n\t\t\tcurrent->comm, task_pid_nr(current));\n\t}\n\tsend_signal_locked(signo, SEND_SIG_PRIV, current, PIDTYPE_TGID);\n\treturn true;\n}\n\n \nstatic void check_thread_timers(struct task_struct *tsk,\n\t\t\t\tstruct list_head *firing)\n{\n\tstruct posix_cputimers *pct = &tsk->posix_cputimers;\n\tu64 samples[CPUCLOCK_MAX];\n\tunsigned long soft;\n\n\tif (dl_task(tsk))\n\t\tcheck_dl_overrun(tsk);\n\n\tif (expiry_cache_is_inactive(pct))\n\t\treturn;\n\n\ttask_sample_cputime(tsk, samples);\n\tcollect_posix_cputimers(pct, samples, firing);\n\n\t \n\tsoft = task_rlimit(tsk, RLIMIT_RTTIME);\n\tif (soft != RLIM_INFINITY) {\n\t\t \n\t\tunsigned long rttime = tsk->rt.timeout * (USEC_PER_SEC / HZ);\n\t\tunsigned long hard = task_rlimit_max(tsk, RLIMIT_RTTIME);\n\n\t\t \n\t\tif (hard != RLIM_INFINITY &&\n\t\t    check_rlimit(rttime, hard, SIGKILL, true, true))\n\t\t\treturn;\n\n\t\t \n\t\tif (check_rlimit(rttime, soft, SIGXCPU, true, false)) {\n\t\t\tsoft += USEC_PER_SEC;\n\t\t\ttsk->signal->rlim[RLIMIT_RTTIME].rlim_cur = soft;\n\t\t}\n\t}\n\n\tif (expiry_cache_is_inactive(pct))\n\t\ttick_dep_clear_task(tsk, TICK_DEP_BIT_POSIX_TIMER);\n}\n\nstatic inline void stop_process_timers(struct signal_struct *sig)\n{\n\tstruct posix_cputimers *pct = &sig->posix_cputimers;\n\n\t \n\tWRITE_ONCE(pct->timers_active, false);\n\ttick_dep_clear_signal(sig, TICK_DEP_BIT_POSIX_TIMER);\n}\n\nstatic void check_cpu_itimer(struct task_struct *tsk, struct cpu_itimer *it,\n\t\t\t     u64 *expires, u64 cur_time, int signo)\n{\n\tif (!it->expires)\n\t\treturn;\n\n\tif (cur_time >= it->expires) {\n\t\tif (it->incr)\n\t\t\tit->expires += it->incr;\n\t\telse\n\t\t\tit->expires = 0;\n\n\t\ttrace_itimer_expire(signo == SIGPROF ?\n\t\t\t\t    ITIMER_PROF : ITIMER_VIRTUAL,\n\t\t\t\t    task_tgid(tsk), cur_time);\n\t\tsend_signal_locked(signo, SEND_SIG_PRIV, tsk, PIDTYPE_TGID);\n\t}\n\n\tif (it->expires && it->expires < *expires)\n\t\t*expires = it->expires;\n}\n\n \nstatic void check_process_timers(struct task_struct *tsk,\n\t\t\t\t struct list_head *firing)\n{\n\tstruct signal_struct *const sig = tsk->signal;\n\tstruct posix_cputimers *pct = &sig->posix_cputimers;\n\tu64 samples[CPUCLOCK_MAX];\n\tunsigned long soft;\n\n\t \n\tif (!READ_ONCE(pct->timers_active) || pct->expiry_active)\n\t\treturn;\n\n\t \n\tpct->expiry_active = true;\n\n\t \n\tproc_sample_cputime_atomic(&sig->cputimer.cputime_atomic, samples);\n\tcollect_posix_cputimers(pct, samples, firing);\n\n\t \n\tcheck_cpu_itimer(tsk, &sig->it[CPUCLOCK_PROF],\n\t\t\t &pct->bases[CPUCLOCK_PROF].nextevt,\n\t\t\t samples[CPUCLOCK_PROF], SIGPROF);\n\tcheck_cpu_itimer(tsk, &sig->it[CPUCLOCK_VIRT],\n\t\t\t &pct->bases[CPUCLOCK_VIRT].nextevt,\n\t\t\t samples[CPUCLOCK_VIRT], SIGVTALRM);\n\n\tsoft = task_rlimit(tsk, RLIMIT_CPU);\n\tif (soft != RLIM_INFINITY) {\n\t\t \n\t\tunsigned long hard = task_rlimit_max(tsk, RLIMIT_CPU);\n\t\tu64 ptime = samples[CPUCLOCK_PROF];\n\t\tu64 softns = (u64)soft * NSEC_PER_SEC;\n\t\tu64 hardns = (u64)hard * NSEC_PER_SEC;\n\n\t\t \n\t\tif (hard != RLIM_INFINITY &&\n\t\t    check_rlimit(ptime, hardns, SIGKILL, false, true))\n\t\t\treturn;\n\n\t\t \n\t\tif (check_rlimit(ptime, softns, SIGXCPU, false, false)) {\n\t\t\tsig->rlim[RLIMIT_CPU].rlim_cur = soft + 1;\n\t\t\tsoftns += NSEC_PER_SEC;\n\t\t}\n\n\t\t \n\t\tif (softns < pct->bases[CPUCLOCK_PROF].nextevt)\n\t\t\tpct->bases[CPUCLOCK_PROF].nextevt = softns;\n\t}\n\n\tif (expiry_cache_is_inactive(pct))\n\t\tstop_process_timers(sig);\n\n\tpct->expiry_active = false;\n}\n\n \nstatic void posix_cpu_timer_rearm(struct k_itimer *timer)\n{\n\tclockid_t clkid = CPUCLOCK_WHICH(timer->it_clock);\n\tstruct task_struct *p;\n\tstruct sighand_struct *sighand;\n\tunsigned long flags;\n\tu64 now;\n\n\trcu_read_lock();\n\tp = cpu_timer_task_rcu(timer);\n\tif (!p)\n\t\tgoto out;\n\n\t \n\tsighand = lock_task_sighand(p, &flags);\n\tif (unlikely(sighand == NULL))\n\t\tgoto out;\n\n\t \n\tif (CPUCLOCK_PERTHREAD(timer->it_clock))\n\t\tnow = cpu_clock_sample(clkid, p);\n\telse\n\t\tnow = cpu_clock_sample_group(clkid, p, true);\n\n\tbump_cpu_timer(timer, now);\n\n\t \n\tarm_timer(timer, p);\n\tunlock_task_sighand(p, &flags);\nout:\n\trcu_read_unlock();\n}\n\n \nstatic inline bool\ntask_cputimers_expired(const u64 *samples, struct posix_cputimers *pct)\n{\n\tint i;\n\n\tfor (i = 0; i < CPUCLOCK_MAX; i++) {\n\t\tif (samples[i] >= pct->bases[i].nextevt)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\n \nstatic inline bool fastpath_timer_check(struct task_struct *tsk)\n{\n\tstruct posix_cputimers *pct = &tsk->posix_cputimers;\n\tstruct signal_struct *sig;\n\n\tif (!expiry_cache_is_inactive(pct)) {\n\t\tu64 samples[CPUCLOCK_MAX];\n\n\t\ttask_sample_cputime(tsk, samples);\n\t\tif (task_cputimers_expired(samples, pct))\n\t\t\treturn true;\n\t}\n\n\tsig = tsk->signal;\n\tpct = &sig->posix_cputimers;\n\t \n\tif (READ_ONCE(pct->timers_active) && !READ_ONCE(pct->expiry_active)) {\n\t\tu64 samples[CPUCLOCK_MAX];\n\n\t\tproc_sample_cputime_atomic(&sig->cputimer.cputime_atomic,\n\t\t\t\t\t   samples);\n\n\t\tif (task_cputimers_expired(samples, pct))\n\t\t\treturn true;\n\t}\n\n\tif (dl_task(tsk) && tsk->dl.dl_overrun)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic void handle_posix_cpu_timers(struct task_struct *tsk);\n\n#ifdef CONFIG_POSIX_CPU_TIMERS_TASK_WORK\nstatic void posix_cpu_timers_work(struct callback_head *work)\n{\n\tstruct posix_cputimers_work *cw = container_of(work, typeof(*cw), work);\n\n\tmutex_lock(&cw->mutex);\n\thandle_posix_cpu_timers(current);\n\tmutex_unlock(&cw->mutex);\n}\n\n \nstatic void posix_cpu_timer_wait_running(struct k_itimer *timr)\n{\n\tstruct task_struct *tsk = rcu_dereference(timr->it.cpu.handling);\n\n\t \n\tif (!tsk)\n\t\treturn;\n\n\t \n\tget_task_struct(tsk);\n\t \n\trcu_read_unlock();\n\t \n\tmutex_lock(&tsk->posix_cputimers_work.mutex);\n\t \n\tmutex_unlock(&tsk->posix_cputimers_work.mutex);\n\t \n\tput_task_struct(tsk);\n\t \n\trcu_read_lock();\n}\n\nstatic void posix_cpu_timer_wait_running_nsleep(struct k_itimer *timr)\n{\n\t \n\trcu_read_lock();\n\tspin_unlock_irq(&timr->it_lock);\n\tposix_cpu_timer_wait_running(timr);\n\trcu_read_unlock();\n\t \n\tspin_lock_irq(&timr->it_lock);\n}\n\n \nvoid clear_posix_cputimers_work(struct task_struct *p)\n{\n\t \n\tmemset(&p->posix_cputimers_work.work, 0,\n\t       sizeof(p->posix_cputimers_work.work));\n\tinit_task_work(&p->posix_cputimers_work.work,\n\t\t       posix_cpu_timers_work);\n\tmutex_init(&p->posix_cputimers_work.mutex);\n\tp->posix_cputimers_work.scheduled = false;\n}\n\n \nvoid __init posix_cputimers_init_work(void)\n{\n\tclear_posix_cputimers_work(current);\n}\n\n \nstatic inline bool posix_cpu_timers_work_scheduled(struct task_struct *tsk)\n{\n\treturn tsk->posix_cputimers_work.scheduled;\n}\n\nstatic inline void __run_posix_cpu_timers(struct task_struct *tsk)\n{\n\tif (WARN_ON_ONCE(tsk->posix_cputimers_work.scheduled))\n\t\treturn;\n\n\t \n\ttsk->posix_cputimers_work.scheduled = true;\n\ttask_work_add(tsk, &tsk->posix_cputimers_work.work, TWA_RESUME);\n}\n\nstatic inline bool posix_cpu_timers_enable_work(struct task_struct *tsk,\n\t\t\t\t\t\tunsigned long start)\n{\n\tbool ret = true;\n\n\t \n\tif (!IS_ENABLED(CONFIG_PREEMPT_RT)) {\n\t\ttsk->posix_cputimers_work.scheduled = false;\n\t\treturn true;\n\t}\n\n\t \n\tlocal_irq_disable();\n\tif (start != jiffies && fastpath_timer_check(tsk))\n\t\tret = false;\n\telse\n\t\ttsk->posix_cputimers_work.scheduled = false;\n\tlocal_irq_enable();\n\n\treturn ret;\n}\n#else  \nstatic inline void __run_posix_cpu_timers(struct task_struct *tsk)\n{\n\tlockdep_posixtimer_enter();\n\thandle_posix_cpu_timers(tsk);\n\tlockdep_posixtimer_exit();\n}\n\nstatic void posix_cpu_timer_wait_running(struct k_itimer *timr)\n{\n\tcpu_relax();\n}\n\nstatic void posix_cpu_timer_wait_running_nsleep(struct k_itimer *timr)\n{\n\tspin_unlock_irq(&timr->it_lock);\n\tcpu_relax();\n\tspin_lock_irq(&timr->it_lock);\n}\n\nstatic inline bool posix_cpu_timers_work_scheduled(struct task_struct *tsk)\n{\n\treturn false;\n}\n\nstatic inline bool posix_cpu_timers_enable_work(struct task_struct *tsk,\n\t\t\t\t\t\tunsigned long start)\n{\n\treturn true;\n}\n#endif  \n\nstatic void handle_posix_cpu_timers(struct task_struct *tsk)\n{\n\tstruct k_itimer *timer, *next;\n\tunsigned long flags, start;\n\tLIST_HEAD(firing);\n\n\tif (!lock_task_sighand(tsk, &flags))\n\t\treturn;\n\n\tdo {\n\t\t \n\t\tstart = READ_ONCE(jiffies);\n\t\tbarrier();\n\n\t\t \n\t\tcheck_thread_timers(tsk, &firing);\n\n\t\tcheck_process_timers(tsk, &firing);\n\n\t\t \n\t} while (!posix_cpu_timers_enable_work(tsk, start));\n\n\t \n\tunlock_task_sighand(tsk, &flags);\n\n\t \n\tlist_for_each_entry_safe(timer, next, &firing, it.cpu.elist) {\n\t\tint cpu_firing;\n\n\t\t \n\t\tspin_lock(&timer->it_lock);\n\t\tlist_del_init(&timer->it.cpu.elist);\n\t\tcpu_firing = timer->it.cpu.firing;\n\t\ttimer->it.cpu.firing = 0;\n\t\t \n\t\tif (likely(cpu_firing >= 0))\n\t\t\tcpu_timer_fire(timer);\n\t\t \n\t\trcu_assign_pointer(timer->it.cpu.handling, NULL);\n\t\tspin_unlock(&timer->it_lock);\n\t}\n}\n\n \nvoid run_posix_cpu_timers(void)\n{\n\tstruct task_struct *tsk = current;\n\n\tlockdep_assert_irqs_disabled();\n\n\t \n\tif (posix_cpu_timers_work_scheduled(tsk))\n\t\treturn;\n\n\t \n\tif (!fastpath_timer_check(tsk))\n\t\treturn;\n\n\t__run_posix_cpu_timers(tsk);\n}\n\n \nvoid set_process_cpu_timer(struct task_struct *tsk, unsigned int clkid,\n\t\t\t   u64 *newval, u64 *oldval)\n{\n\tu64 now, *nextevt;\n\n\tif (WARN_ON_ONCE(clkid >= CPUCLOCK_SCHED))\n\t\treturn;\n\n\tnextevt = &tsk->signal->posix_cputimers.bases[clkid].nextevt;\n\tnow = cpu_clock_sample_group(clkid, tsk, true);\n\n\tif (oldval) {\n\t\t \n\t\tif (*oldval) {\n\t\t\tif (*oldval <= now) {\n\t\t\t\t \n\t\t\t\t*oldval = TICK_NSEC;\n\t\t\t} else {\n\t\t\t\t*oldval -= now;\n\t\t\t}\n\t\t}\n\n\t\tif (*newval)\n\t\t\t*newval += now;\n\t}\n\n\t \n\tif (*newval < *nextevt)\n\t\t*nextevt = *newval;\n\n\ttick_dep_set_signal(tsk, TICK_DEP_BIT_POSIX_TIMER);\n}\n\nstatic int do_cpu_nanosleep(const clockid_t which_clock, int flags,\n\t\t\t    const struct timespec64 *rqtp)\n{\n\tstruct itimerspec64 it;\n\tstruct k_itimer timer;\n\tu64 expires;\n\tint error;\n\n\t \n\tmemset(&timer, 0, sizeof timer);\n\tspin_lock_init(&timer.it_lock);\n\ttimer.it_clock = which_clock;\n\ttimer.it_overrun = -1;\n\terror = posix_cpu_timer_create(&timer);\n\ttimer.it_process = current;\n\n\tif (!error) {\n\t\tstatic struct itimerspec64 zero_it;\n\t\tstruct restart_block *restart;\n\n\t\tmemset(&it, 0, sizeof(it));\n\t\tit.it_value = *rqtp;\n\n\t\tspin_lock_irq(&timer.it_lock);\n\t\terror = posix_cpu_timer_set(&timer, flags, &it, NULL);\n\t\tif (error) {\n\t\t\tspin_unlock_irq(&timer.it_lock);\n\t\t\treturn error;\n\t\t}\n\n\t\twhile (!signal_pending(current)) {\n\t\t\tif (!cpu_timer_getexpires(&timer.it.cpu)) {\n\t\t\t\t \n\t\t\t\tposix_cpu_timer_del(&timer);\n\t\t\t\tspin_unlock_irq(&timer.it_lock);\n\t\t\t\treturn 0;\n\t\t\t}\n\n\t\t\t \n\t\t\t__set_current_state(TASK_INTERRUPTIBLE);\n\t\t\tspin_unlock_irq(&timer.it_lock);\n\t\t\tschedule();\n\t\t\tspin_lock_irq(&timer.it_lock);\n\t\t}\n\n\t\t \n\t\texpires = cpu_timer_getexpires(&timer.it.cpu);\n\t\terror = posix_cpu_timer_set(&timer, 0, &zero_it, &it);\n\t\tif (!error) {\n\t\t\t \n\t\t\tposix_cpu_timer_del(&timer);\n\t\t} else {\n\t\t\twhile (error == TIMER_RETRY) {\n\t\t\t\tposix_cpu_timer_wait_running_nsleep(&timer);\n\t\t\t\terror = posix_cpu_timer_del(&timer);\n\t\t\t}\n\t\t}\n\n\t\tspin_unlock_irq(&timer.it_lock);\n\n\t\tif ((it.it_value.tv_sec | it.it_value.tv_nsec) == 0) {\n\t\t\t \n\t\t\treturn 0;\n\t\t}\n\n\t\terror = -ERESTART_RESTARTBLOCK;\n\t\t \n\t\trestart = &current->restart_block;\n\t\trestart->nanosleep.expires = expires;\n\t\tif (restart->nanosleep.type != TT_NONE)\n\t\t\terror = nanosleep_copyout(restart, &it.it_value);\n\t}\n\n\treturn error;\n}\n\nstatic long posix_cpu_nsleep_restart(struct restart_block *restart_block);\n\nstatic int posix_cpu_nsleep(const clockid_t which_clock, int flags,\n\t\t\t    const struct timespec64 *rqtp)\n{\n\tstruct restart_block *restart_block = &current->restart_block;\n\tint error;\n\n\t \n\tif (CPUCLOCK_PERTHREAD(which_clock) &&\n\t    (CPUCLOCK_PID(which_clock) == 0 ||\n\t     CPUCLOCK_PID(which_clock) == task_pid_vnr(current)))\n\t\treturn -EINVAL;\n\n\terror = do_cpu_nanosleep(which_clock, flags, rqtp);\n\n\tif (error == -ERESTART_RESTARTBLOCK) {\n\n\t\tif (flags & TIMER_ABSTIME)\n\t\t\treturn -ERESTARTNOHAND;\n\n\t\trestart_block->nanosleep.clockid = which_clock;\n\t\tset_restart_fn(restart_block, posix_cpu_nsleep_restart);\n\t}\n\treturn error;\n}\n\nstatic long posix_cpu_nsleep_restart(struct restart_block *restart_block)\n{\n\tclockid_t which_clock = restart_block->nanosleep.clockid;\n\tstruct timespec64 t;\n\n\tt = ns_to_timespec64(restart_block->nanosleep.expires);\n\n\treturn do_cpu_nanosleep(which_clock, TIMER_ABSTIME, &t);\n}\n\n#define PROCESS_CLOCK\tmake_process_cpuclock(0, CPUCLOCK_SCHED)\n#define THREAD_CLOCK\tmake_thread_cpuclock(0, CPUCLOCK_SCHED)\n\nstatic int process_cpu_clock_getres(const clockid_t which_clock,\n\t\t\t\t    struct timespec64 *tp)\n{\n\treturn posix_cpu_clock_getres(PROCESS_CLOCK, tp);\n}\nstatic int process_cpu_clock_get(const clockid_t which_clock,\n\t\t\t\t struct timespec64 *tp)\n{\n\treturn posix_cpu_clock_get(PROCESS_CLOCK, tp);\n}\nstatic int process_cpu_timer_create(struct k_itimer *timer)\n{\n\ttimer->it_clock = PROCESS_CLOCK;\n\treturn posix_cpu_timer_create(timer);\n}\nstatic int process_cpu_nsleep(const clockid_t which_clock, int flags,\n\t\t\t      const struct timespec64 *rqtp)\n{\n\treturn posix_cpu_nsleep(PROCESS_CLOCK, flags, rqtp);\n}\nstatic int thread_cpu_clock_getres(const clockid_t which_clock,\n\t\t\t\t   struct timespec64 *tp)\n{\n\treturn posix_cpu_clock_getres(THREAD_CLOCK, tp);\n}\nstatic int thread_cpu_clock_get(const clockid_t which_clock,\n\t\t\t\tstruct timespec64 *tp)\n{\n\treturn posix_cpu_clock_get(THREAD_CLOCK, tp);\n}\nstatic int thread_cpu_timer_create(struct k_itimer *timer)\n{\n\ttimer->it_clock = THREAD_CLOCK;\n\treturn posix_cpu_timer_create(timer);\n}\n\nconst struct k_clock clock_posix_cpu = {\n\t.clock_getres\t\t= posix_cpu_clock_getres,\n\t.clock_set\t\t= posix_cpu_clock_set,\n\t.clock_get_timespec\t= posix_cpu_clock_get,\n\t.timer_create\t\t= posix_cpu_timer_create,\n\t.nsleep\t\t\t= posix_cpu_nsleep,\n\t.timer_set\t\t= posix_cpu_timer_set,\n\t.timer_del\t\t= posix_cpu_timer_del,\n\t.timer_get\t\t= posix_cpu_timer_get,\n\t.timer_rearm\t\t= posix_cpu_timer_rearm,\n\t.timer_wait_running\t= posix_cpu_timer_wait_running,\n};\n\nconst struct k_clock clock_process = {\n\t.clock_getres\t\t= process_cpu_clock_getres,\n\t.clock_get_timespec\t= process_cpu_clock_get,\n\t.timer_create\t\t= process_cpu_timer_create,\n\t.nsleep\t\t\t= process_cpu_nsleep,\n};\n\nconst struct k_clock clock_thread = {\n\t.clock_getres\t\t= thread_cpu_clock_getres,\n\t.clock_get_timespec\t= thread_cpu_clock_get,\n\t.timer_create\t\t= thread_cpu_timer_create,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}