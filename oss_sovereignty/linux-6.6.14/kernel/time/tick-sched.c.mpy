{
  "module_name": "tick-sched.c",
  "hash_id": "9c614f2b249c672cedf793199b0f23ac811bcf50455fe0299f5c217990b6291d",
  "original_prompt": "Ingested from linux-6.6.14/kernel/time/tick-sched.c",
  "human_readable_source": "\n \n#include <linux/cpu.h>\n#include <linux/err.h>\n#include <linux/hrtimer.h>\n#include <linux/interrupt.h>\n#include <linux/kernel_stat.h>\n#include <linux/percpu.h>\n#include <linux/nmi.h>\n#include <linux/profile.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/nohz.h>\n#include <linux/sched/loadavg.h>\n#include <linux/module.h>\n#include <linux/irq_work.h>\n#include <linux/posix-timers.h>\n#include <linux/context_tracking.h>\n#include <linux/mm.h>\n\n#include <asm/irq_regs.h>\n\n#include \"tick-internal.h\"\n\n#include <trace/events/timer.h>\n\n \nstatic DEFINE_PER_CPU(struct tick_sched, tick_cpu_sched);\n\nstruct tick_sched *tick_get_tick_sched(int cpu)\n{\n\treturn &per_cpu(tick_cpu_sched, cpu);\n}\n\n#if defined(CONFIG_NO_HZ_COMMON) || defined(CONFIG_HIGH_RES_TIMERS)\n \nstatic ktime_t last_jiffies_update;\n\n \nstatic void tick_do_update_jiffies64(ktime_t now)\n{\n\tunsigned long ticks = 1;\n\tktime_t delta, nextp;\n\n\t \n\tif (IS_ENABLED(CONFIG_64BIT)) {\n\t\tif (ktime_before(now, smp_load_acquire(&tick_next_period)))\n\t\t\treturn;\n\t} else {\n\t\tunsigned int seq;\n\n\t\t \n\t\tdo {\n\t\t\tseq = read_seqcount_begin(&jiffies_seq);\n\t\t\tnextp = tick_next_period;\n\t\t} while (read_seqcount_retry(&jiffies_seq, seq));\n\n\t\tif (ktime_before(now, nextp))\n\t\t\treturn;\n\t}\n\n\t \n\traw_spin_lock(&jiffies_lock);\n\t \n\tif (ktime_before(now, tick_next_period)) {\n\t\traw_spin_unlock(&jiffies_lock);\n\t\treturn;\n\t}\n\n\twrite_seqcount_begin(&jiffies_seq);\n\n\tdelta = ktime_sub(now, tick_next_period);\n\tif (unlikely(delta >= TICK_NSEC)) {\n\t\t \n\t\ts64 incr = TICK_NSEC;\n\n\t\tticks += ktime_divns(delta, incr);\n\n\t\tlast_jiffies_update = ktime_add_ns(last_jiffies_update,\n\t\t\t\t\t\t   incr * ticks);\n\t} else {\n\t\tlast_jiffies_update = ktime_add_ns(last_jiffies_update,\n\t\t\t\t\t\t   TICK_NSEC);\n\t}\n\n\t \n\tjiffies_64 += ticks;\n\n\t \n\tnextp = ktime_add_ns(last_jiffies_update, TICK_NSEC);\n\n\tif (IS_ENABLED(CONFIG_64BIT)) {\n\t\t \n\t\tsmp_store_release(&tick_next_period, nextp);\n\t} else {\n\t\t \n\t\ttick_next_period = nextp;\n\t}\n\n\t \n\twrite_seqcount_end(&jiffies_seq);\n\n\tcalc_global_load();\n\n\traw_spin_unlock(&jiffies_lock);\n\tupdate_wall_time();\n}\n\n \nstatic ktime_t tick_init_jiffy_update(void)\n{\n\tktime_t period;\n\n\traw_spin_lock(&jiffies_lock);\n\twrite_seqcount_begin(&jiffies_seq);\n\t \n\tif (last_jiffies_update == 0) {\n\t\tu32 rem;\n\n\t\t \n\t\tdiv_u64_rem(tick_next_period, TICK_NSEC, &rem);\n\t\tif (rem)\n\t\t\ttick_next_period += TICK_NSEC - rem;\n\n\t\tlast_jiffies_update = tick_next_period;\n\t}\n\tperiod = last_jiffies_update;\n\twrite_seqcount_end(&jiffies_seq);\n\traw_spin_unlock(&jiffies_lock);\n\treturn period;\n}\n\n#define MAX_STALLED_JIFFIES 5\n\nstatic void tick_sched_do_timer(struct tick_sched *ts, ktime_t now)\n{\n\tint cpu = smp_processor_id();\n\n#ifdef CONFIG_NO_HZ_COMMON\n\t \n\tif (unlikely(tick_do_timer_cpu == TICK_DO_TIMER_NONE)) {\n#ifdef CONFIG_NO_HZ_FULL\n\t\tWARN_ON_ONCE(tick_nohz_full_running);\n#endif\n\t\ttick_do_timer_cpu = cpu;\n\t}\n#endif\n\n\t \n\tif (tick_do_timer_cpu == cpu)\n\t\ttick_do_update_jiffies64(now);\n\n\t \n\tif (ts->last_tick_jiffies != jiffies) {\n\t\tts->stalled_jiffies = 0;\n\t\tts->last_tick_jiffies = READ_ONCE(jiffies);\n\t} else {\n\t\tif (++ts->stalled_jiffies == MAX_STALLED_JIFFIES) {\n\t\t\ttick_do_update_jiffies64(now);\n\t\t\tts->stalled_jiffies = 0;\n\t\t\tts->last_tick_jiffies = READ_ONCE(jiffies);\n\t\t}\n\t}\n\n\tif (ts->inidle)\n\t\tts->got_idle_tick = 1;\n}\n\nstatic void tick_sched_handle(struct tick_sched *ts, struct pt_regs *regs)\n{\n#ifdef CONFIG_NO_HZ_COMMON\n\t \n\tif (ts->tick_stopped) {\n\t\ttouch_softlockup_watchdog_sched();\n\t\tif (is_idle_task(current))\n\t\t\tts->idle_jiffies++;\n\t\t \n\t\tts->next_tick = 0;\n\t}\n#endif\n\tupdate_process_times(user_mode(regs));\n\tprofile_tick(CPU_PROFILING);\n}\n#endif\n\n#ifdef CONFIG_NO_HZ_FULL\ncpumask_var_t tick_nohz_full_mask;\nEXPORT_SYMBOL_GPL(tick_nohz_full_mask);\nbool tick_nohz_full_running;\nEXPORT_SYMBOL_GPL(tick_nohz_full_running);\nstatic atomic_t tick_dep_mask;\n\nstatic bool check_tick_dependency(atomic_t *dep)\n{\n\tint val = atomic_read(dep);\n\n\tif (val & TICK_DEP_MASK_POSIX_TIMER) {\n\t\ttrace_tick_stop(0, TICK_DEP_MASK_POSIX_TIMER);\n\t\treturn true;\n\t}\n\n\tif (val & TICK_DEP_MASK_PERF_EVENTS) {\n\t\ttrace_tick_stop(0, TICK_DEP_MASK_PERF_EVENTS);\n\t\treturn true;\n\t}\n\n\tif (val & TICK_DEP_MASK_SCHED) {\n\t\ttrace_tick_stop(0, TICK_DEP_MASK_SCHED);\n\t\treturn true;\n\t}\n\n\tif (val & TICK_DEP_MASK_CLOCK_UNSTABLE) {\n\t\ttrace_tick_stop(0, TICK_DEP_MASK_CLOCK_UNSTABLE);\n\t\treturn true;\n\t}\n\n\tif (val & TICK_DEP_MASK_RCU) {\n\t\ttrace_tick_stop(0, TICK_DEP_MASK_RCU);\n\t\treturn true;\n\t}\n\n\tif (val & TICK_DEP_MASK_RCU_EXP) {\n\t\ttrace_tick_stop(0, TICK_DEP_MASK_RCU_EXP);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic bool can_stop_full_tick(int cpu, struct tick_sched *ts)\n{\n\tlockdep_assert_irqs_disabled();\n\n\tif (unlikely(!cpu_online(cpu)))\n\t\treturn false;\n\n\tif (check_tick_dependency(&tick_dep_mask))\n\t\treturn false;\n\n\tif (check_tick_dependency(&ts->tick_dep_mask))\n\t\treturn false;\n\n\tif (check_tick_dependency(&current->tick_dep_mask))\n\t\treturn false;\n\n\tif (check_tick_dependency(&current->signal->tick_dep_mask))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic void nohz_full_kick_func(struct irq_work *work)\n{\n\t \n}\n\nstatic DEFINE_PER_CPU(struct irq_work, nohz_full_kick_work) =\n\tIRQ_WORK_INIT_HARD(nohz_full_kick_func);\n\n \nstatic void tick_nohz_full_kick(void)\n{\n\tif (!tick_nohz_full_cpu(smp_processor_id()))\n\t\treturn;\n\n\tirq_work_queue(this_cpu_ptr(&nohz_full_kick_work));\n}\n\n \nvoid tick_nohz_full_kick_cpu(int cpu)\n{\n\tif (!tick_nohz_full_cpu(cpu))\n\t\treturn;\n\n\tirq_work_queue_on(&per_cpu(nohz_full_kick_work, cpu), cpu);\n}\n\nstatic void tick_nohz_kick_task(struct task_struct *tsk)\n{\n\tint cpu;\n\n\t \n\tif (!sched_task_on_rq(tsk))\n\t\treturn;\n\n\t \n\tcpu = task_cpu(tsk);\n\n\tpreempt_disable();\n\tif (cpu_online(cpu))\n\t\ttick_nohz_full_kick_cpu(cpu);\n\tpreempt_enable();\n}\n\n \nstatic void tick_nohz_full_kick_all(void)\n{\n\tint cpu;\n\n\tif (!tick_nohz_full_running)\n\t\treturn;\n\n\tpreempt_disable();\n\tfor_each_cpu_and(cpu, tick_nohz_full_mask, cpu_online_mask)\n\t\ttick_nohz_full_kick_cpu(cpu);\n\tpreempt_enable();\n}\n\nstatic void tick_nohz_dep_set_all(atomic_t *dep,\n\t\t\t\t  enum tick_dep_bits bit)\n{\n\tint prev;\n\n\tprev = atomic_fetch_or(BIT(bit), dep);\n\tif (!prev)\n\t\ttick_nohz_full_kick_all();\n}\n\n \nvoid tick_nohz_dep_set(enum tick_dep_bits bit)\n{\n\ttick_nohz_dep_set_all(&tick_dep_mask, bit);\n}\n\nvoid tick_nohz_dep_clear(enum tick_dep_bits bit)\n{\n\tatomic_andnot(BIT(bit), &tick_dep_mask);\n}\n\n \nvoid tick_nohz_dep_set_cpu(int cpu, enum tick_dep_bits bit)\n{\n\tint prev;\n\tstruct tick_sched *ts;\n\n\tts = per_cpu_ptr(&tick_cpu_sched, cpu);\n\n\tprev = atomic_fetch_or(BIT(bit), &ts->tick_dep_mask);\n\tif (!prev) {\n\t\tpreempt_disable();\n\t\t \n\t\tif (cpu == smp_processor_id()) {\n\t\t\ttick_nohz_full_kick();\n\t\t} else {\n\t\t\t \n\t\t\tif (!WARN_ON_ONCE(in_nmi()))\n\t\t\t\ttick_nohz_full_kick_cpu(cpu);\n\t\t}\n\t\tpreempt_enable();\n\t}\n}\nEXPORT_SYMBOL_GPL(tick_nohz_dep_set_cpu);\n\nvoid tick_nohz_dep_clear_cpu(int cpu, enum tick_dep_bits bit)\n{\n\tstruct tick_sched *ts = per_cpu_ptr(&tick_cpu_sched, cpu);\n\n\tatomic_andnot(BIT(bit), &ts->tick_dep_mask);\n}\nEXPORT_SYMBOL_GPL(tick_nohz_dep_clear_cpu);\n\n \nvoid tick_nohz_dep_set_task(struct task_struct *tsk, enum tick_dep_bits bit)\n{\n\tif (!atomic_fetch_or(BIT(bit), &tsk->tick_dep_mask))\n\t\ttick_nohz_kick_task(tsk);\n}\nEXPORT_SYMBOL_GPL(tick_nohz_dep_set_task);\n\nvoid tick_nohz_dep_clear_task(struct task_struct *tsk, enum tick_dep_bits bit)\n{\n\tatomic_andnot(BIT(bit), &tsk->tick_dep_mask);\n}\nEXPORT_SYMBOL_GPL(tick_nohz_dep_clear_task);\n\n \nvoid tick_nohz_dep_set_signal(struct task_struct *tsk,\n\t\t\t      enum tick_dep_bits bit)\n{\n\tint prev;\n\tstruct signal_struct *sig = tsk->signal;\n\n\tprev = atomic_fetch_or(BIT(bit), &sig->tick_dep_mask);\n\tif (!prev) {\n\t\tstruct task_struct *t;\n\n\t\tlockdep_assert_held(&tsk->sighand->siglock);\n\t\t__for_each_thread(sig, t)\n\t\t\ttick_nohz_kick_task(t);\n\t}\n}\n\nvoid tick_nohz_dep_clear_signal(struct signal_struct *sig, enum tick_dep_bits bit)\n{\n\tatomic_andnot(BIT(bit), &sig->tick_dep_mask);\n}\n\n \nvoid __tick_nohz_task_switch(void)\n{\n\tstruct tick_sched *ts;\n\n\tif (!tick_nohz_full_cpu(smp_processor_id()))\n\t\treturn;\n\n\tts = this_cpu_ptr(&tick_cpu_sched);\n\n\tif (ts->tick_stopped) {\n\t\tif (atomic_read(&current->tick_dep_mask) ||\n\t\t    atomic_read(&current->signal->tick_dep_mask))\n\t\t\ttick_nohz_full_kick();\n\t}\n}\n\n \nvoid __init tick_nohz_full_setup(cpumask_var_t cpumask)\n{\n\talloc_bootmem_cpumask_var(&tick_nohz_full_mask);\n\tcpumask_copy(tick_nohz_full_mask, cpumask);\n\ttick_nohz_full_running = true;\n}\n\nbool tick_nohz_cpu_hotpluggable(unsigned int cpu)\n{\n\t \n\tif (tick_nohz_full_running && tick_do_timer_cpu == cpu)\n\t\treturn false;\n\treturn true;\n}\n\nstatic int tick_nohz_cpu_down(unsigned int cpu)\n{\n\treturn tick_nohz_cpu_hotpluggable(cpu) ? 0 : -EBUSY;\n}\n\nvoid __init tick_nohz_init(void)\n{\n\tint cpu, ret;\n\n\tif (!tick_nohz_full_running)\n\t\treturn;\n\n\t \n\tif (!arch_irq_work_has_interrupt()) {\n\t\tpr_warn(\"NO_HZ: Can't run full dynticks because arch doesn't support irq work self-IPIs\\n\");\n\t\tcpumask_clear(tick_nohz_full_mask);\n\t\ttick_nohz_full_running = false;\n\t\treturn;\n\t}\n\n\tif (IS_ENABLED(CONFIG_PM_SLEEP_SMP) &&\n\t\t\t!IS_ENABLED(CONFIG_PM_SLEEP_SMP_NONZERO_CPU)) {\n\t\tcpu = smp_processor_id();\n\n\t\tif (cpumask_test_cpu(cpu, tick_nohz_full_mask)) {\n\t\t\tpr_warn(\"NO_HZ: Clearing %d from nohz_full range \"\n\t\t\t\t\"for timekeeping\\n\", cpu);\n\t\t\tcpumask_clear_cpu(cpu, tick_nohz_full_mask);\n\t\t}\n\t}\n\n\tfor_each_cpu(cpu, tick_nohz_full_mask)\n\t\tct_cpu_track_user(cpu);\n\n\tret = cpuhp_setup_state_nocalls(CPUHP_AP_ONLINE_DYN,\n\t\t\t\t\t\"kernel/nohz:predown\", NULL,\n\t\t\t\t\ttick_nohz_cpu_down);\n\tWARN_ON(ret < 0);\n\tpr_info(\"NO_HZ: Full dynticks CPUs: %*pbl.\\n\",\n\t\tcpumask_pr_args(tick_nohz_full_mask));\n}\n#endif\n\n \n#ifdef CONFIG_NO_HZ_COMMON\n \nbool tick_nohz_enabled __read_mostly  = true;\nunsigned long tick_nohz_active  __read_mostly;\n \nstatic int __init setup_tick_nohz(char *str)\n{\n\treturn (kstrtobool(str, &tick_nohz_enabled) == 0);\n}\n\n__setup(\"nohz=\", setup_tick_nohz);\n\nbool tick_nohz_tick_stopped(void)\n{\n\tstruct tick_sched *ts = this_cpu_ptr(&tick_cpu_sched);\n\n\treturn ts->tick_stopped;\n}\n\nbool tick_nohz_tick_stopped_cpu(int cpu)\n{\n\tstruct tick_sched *ts = per_cpu_ptr(&tick_cpu_sched, cpu);\n\n\treturn ts->tick_stopped;\n}\n\n \nstatic void tick_nohz_update_jiffies(ktime_t now)\n{\n\tunsigned long flags;\n\n\t__this_cpu_write(tick_cpu_sched.idle_waketime, now);\n\n\tlocal_irq_save(flags);\n\ttick_do_update_jiffies64(now);\n\tlocal_irq_restore(flags);\n\n\ttouch_softlockup_watchdog_sched();\n}\n\nstatic void tick_nohz_stop_idle(struct tick_sched *ts, ktime_t now)\n{\n\tktime_t delta;\n\n\tif (WARN_ON_ONCE(!ts->idle_active))\n\t\treturn;\n\n\tdelta = ktime_sub(now, ts->idle_entrytime);\n\n\twrite_seqcount_begin(&ts->idle_sleeptime_seq);\n\tif (nr_iowait_cpu(smp_processor_id()) > 0)\n\t\tts->iowait_sleeptime = ktime_add(ts->iowait_sleeptime, delta);\n\telse\n\t\tts->idle_sleeptime = ktime_add(ts->idle_sleeptime, delta);\n\n\tts->idle_entrytime = now;\n\tts->idle_active = 0;\n\twrite_seqcount_end(&ts->idle_sleeptime_seq);\n\n\tsched_clock_idle_wakeup_event();\n}\n\nstatic void tick_nohz_start_idle(struct tick_sched *ts)\n{\n\twrite_seqcount_begin(&ts->idle_sleeptime_seq);\n\tts->idle_entrytime = ktime_get();\n\tts->idle_active = 1;\n\twrite_seqcount_end(&ts->idle_sleeptime_seq);\n\n\tsched_clock_idle_sleep_event();\n}\n\nstatic u64 get_cpu_sleep_time_us(struct tick_sched *ts, ktime_t *sleeptime,\n\t\t\t\t bool compute_delta, u64 *last_update_time)\n{\n\tktime_t now, idle;\n\tunsigned int seq;\n\n\tif (!tick_nohz_active)\n\t\treturn -1;\n\n\tnow = ktime_get();\n\tif (last_update_time)\n\t\t*last_update_time = ktime_to_us(now);\n\n\tdo {\n\t\tseq = read_seqcount_begin(&ts->idle_sleeptime_seq);\n\n\t\tif (ts->idle_active && compute_delta) {\n\t\t\tktime_t delta = ktime_sub(now, ts->idle_entrytime);\n\n\t\t\tidle = ktime_add(*sleeptime, delta);\n\t\t} else {\n\t\t\tidle = *sleeptime;\n\t\t}\n\t} while (read_seqcount_retry(&ts->idle_sleeptime_seq, seq));\n\n\treturn ktime_to_us(idle);\n\n}\n\n \nu64 get_cpu_idle_time_us(int cpu, u64 *last_update_time)\n{\n\tstruct tick_sched *ts = &per_cpu(tick_cpu_sched, cpu);\n\n\treturn get_cpu_sleep_time_us(ts, &ts->idle_sleeptime,\n\t\t\t\t     !nr_iowait_cpu(cpu), last_update_time);\n}\nEXPORT_SYMBOL_GPL(get_cpu_idle_time_us);\n\n \nu64 get_cpu_iowait_time_us(int cpu, u64 *last_update_time)\n{\n\tstruct tick_sched *ts = &per_cpu(tick_cpu_sched, cpu);\n\n\treturn get_cpu_sleep_time_us(ts, &ts->iowait_sleeptime,\n\t\t\t\t     nr_iowait_cpu(cpu), last_update_time);\n}\nEXPORT_SYMBOL_GPL(get_cpu_iowait_time_us);\n\nstatic void tick_nohz_restart(struct tick_sched *ts, ktime_t now)\n{\n\thrtimer_cancel(&ts->sched_timer);\n\thrtimer_set_expires(&ts->sched_timer, ts->last_tick);\n\n\t \n\thrtimer_forward(&ts->sched_timer, now, TICK_NSEC);\n\n\tif (ts->nohz_mode == NOHZ_MODE_HIGHRES) {\n\t\thrtimer_start_expires(&ts->sched_timer,\n\t\t\t\t      HRTIMER_MODE_ABS_PINNED_HARD);\n\t} else {\n\t\ttick_program_event(hrtimer_get_expires(&ts->sched_timer), 1);\n\t}\n\n\t \n\tts->next_tick = 0;\n}\n\nstatic inline bool local_timer_softirq_pending(void)\n{\n\treturn local_softirq_pending() & BIT(TIMER_SOFTIRQ);\n}\n\nstatic ktime_t tick_nohz_next_event(struct tick_sched *ts, int cpu)\n{\n\tu64 basemono, next_tick, delta, expires;\n\tunsigned long basejiff;\n\tunsigned int seq;\n\n\t \n\tdo {\n\t\tseq = read_seqcount_begin(&jiffies_seq);\n\t\tbasemono = last_jiffies_update;\n\t\tbasejiff = jiffies;\n\t} while (read_seqcount_retry(&jiffies_seq, seq));\n\tts->last_jiffies = basejiff;\n\tts->timer_expires_base = basemono;\n\n\t \n\tif (rcu_needs_cpu() || arch_needs_cpu() ||\n\t    irq_work_needs_cpu() || local_timer_softirq_pending()) {\n\t\tnext_tick = basemono + TICK_NSEC;\n\t} else {\n\t\t \n\t\tnext_tick = get_next_timer_interrupt(basejiff, basemono);\n\t\tts->next_timer = next_tick;\n\t}\n\n\t \n\tdelta = next_tick - basemono;\n\tif (delta <= (u64)TICK_NSEC) {\n\t\t \n\t\ttimer_clear_idle();\n\t\t \n\t\tif (!ts->tick_stopped) {\n\t\t\tts->timer_expires = 0;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t \n\tdelta = timekeeping_max_deferment();\n\tif (cpu != tick_do_timer_cpu &&\n\t    (tick_do_timer_cpu != TICK_DO_TIMER_NONE || !ts->do_timer_last))\n\t\tdelta = KTIME_MAX;\n\n\t \n\tif (delta < (KTIME_MAX - basemono))\n\t\texpires = basemono + delta;\n\telse\n\t\texpires = KTIME_MAX;\n\n\tts->timer_expires = min_t(u64, expires, next_tick);\n\nout:\n\treturn ts->timer_expires;\n}\n\nstatic void tick_nohz_stop_tick(struct tick_sched *ts, int cpu)\n{\n\tstruct clock_event_device *dev = __this_cpu_read(tick_cpu_device.evtdev);\n\tu64 basemono = ts->timer_expires_base;\n\tu64 expires = ts->timer_expires;\n\tktime_t tick = expires;\n\n\t \n\tts->timer_expires_base = 0;\n\n\t \n\tif (cpu == tick_do_timer_cpu) {\n\t\ttick_do_timer_cpu = TICK_DO_TIMER_NONE;\n\t\tts->do_timer_last = 1;\n\t} else if (tick_do_timer_cpu != TICK_DO_TIMER_NONE) {\n\t\tts->do_timer_last = 0;\n\t}\n\n\t \n\tif (ts->tick_stopped && (expires == ts->next_tick)) {\n\t\t \n\t\tif (tick == KTIME_MAX || ts->next_tick == hrtimer_get_expires(&ts->sched_timer))\n\t\t\treturn;\n\n\t\tWARN_ON_ONCE(1);\n\t\tprintk_once(\"basemono: %llu ts->next_tick: %llu dev->next_event: %llu timer->active: %d timer->expires: %llu\\n\",\n\t\t\t    basemono, ts->next_tick, dev->next_event,\n\t\t\t    hrtimer_active(&ts->sched_timer), hrtimer_get_expires(&ts->sched_timer));\n\t}\n\n\t \n\tif (!ts->tick_stopped) {\n\t\tcalc_load_nohz_start();\n\t\tquiet_vmstat();\n\n\t\tts->last_tick = hrtimer_get_expires(&ts->sched_timer);\n\t\tts->tick_stopped = 1;\n\t\ttrace_tick_stop(1, TICK_DEP_MASK_NONE);\n\t}\n\n\tts->next_tick = tick;\n\n\t \n\tif (unlikely(expires == KTIME_MAX)) {\n\t\tif (ts->nohz_mode == NOHZ_MODE_HIGHRES)\n\t\t\thrtimer_cancel(&ts->sched_timer);\n\t\telse\n\t\t\ttick_program_event(KTIME_MAX, 1);\n\t\treturn;\n\t}\n\n\tif (ts->nohz_mode == NOHZ_MODE_HIGHRES) {\n\t\thrtimer_start(&ts->sched_timer, tick,\n\t\t\t      HRTIMER_MODE_ABS_PINNED_HARD);\n\t} else {\n\t\thrtimer_set_expires(&ts->sched_timer, tick);\n\t\ttick_program_event(tick, 1);\n\t}\n}\n\nstatic void tick_nohz_retain_tick(struct tick_sched *ts)\n{\n\tts->timer_expires_base = 0;\n}\n\n#ifdef CONFIG_NO_HZ_FULL\nstatic void tick_nohz_stop_sched_tick(struct tick_sched *ts, int cpu)\n{\n\tif (tick_nohz_next_event(ts, cpu))\n\t\ttick_nohz_stop_tick(ts, cpu);\n\telse\n\t\ttick_nohz_retain_tick(ts);\n}\n#endif  \n\nstatic void tick_nohz_restart_sched_tick(struct tick_sched *ts, ktime_t now)\n{\n\t \n\ttick_do_update_jiffies64(now);\n\n\t \n\ttimer_clear_idle();\n\n\tcalc_load_nohz_stop();\n\ttouch_softlockup_watchdog_sched();\n\t \n\tts->tick_stopped  = 0;\n\ttick_nohz_restart(ts, now);\n}\n\nstatic void __tick_nohz_full_update_tick(struct tick_sched *ts,\n\t\t\t\t\t ktime_t now)\n{\n#ifdef CONFIG_NO_HZ_FULL\n\tint cpu = smp_processor_id();\n\n\tif (can_stop_full_tick(cpu, ts))\n\t\ttick_nohz_stop_sched_tick(ts, cpu);\n\telse if (ts->tick_stopped)\n\t\ttick_nohz_restart_sched_tick(ts, now);\n#endif\n}\n\nstatic void tick_nohz_full_update_tick(struct tick_sched *ts)\n{\n\tif (!tick_nohz_full_cpu(smp_processor_id()))\n\t\treturn;\n\n\tif (!ts->tick_stopped && ts->nohz_mode == NOHZ_MODE_INACTIVE)\n\t\treturn;\n\n\t__tick_nohz_full_update_tick(ts, ktime_get());\n}\n\n \nstatic bool report_idle_softirq(void)\n{\n\tstatic int ratelimit;\n\tunsigned int pending = local_softirq_pending();\n\n\tif (likely(!pending))\n\t\treturn false;\n\n\t \n\tif (!cpu_active(smp_processor_id())) {\n\t\tpending &= ~SOFTIRQ_HOTPLUG_SAFE_MASK;\n\t\tif (!pending)\n\t\t\treturn false;\n\t}\n\n\tif (ratelimit >= 10)\n\t\treturn false;\n\n\t \n\tif (local_bh_blocked())\n\t\treturn false;\n\n\tpr_warn(\"NOHZ tick-stop error: local softirq work is pending, handler #%02x!!!\\n\",\n\t\tpending);\n\tratelimit++;\n\n\treturn true;\n}\n\nstatic bool can_stop_idle_tick(int cpu, struct tick_sched *ts)\n{\n\t \n\tif (unlikely(!cpu_online(cpu))) {\n\t\tif (cpu == tick_do_timer_cpu)\n\t\t\ttick_do_timer_cpu = TICK_DO_TIMER_NONE;\n\t\t \n\t\tts->next_tick = 0;\n\t\treturn false;\n\t}\n\n\tif (unlikely(ts->nohz_mode == NOHZ_MODE_INACTIVE))\n\t\treturn false;\n\n\tif (need_resched())\n\t\treturn false;\n\n\tif (unlikely(report_idle_softirq()))\n\t\treturn false;\n\n\tif (tick_nohz_full_enabled()) {\n\t\t \n\t\tif (tick_do_timer_cpu == cpu)\n\t\t\treturn false;\n\n\t\t \n\t\tif (WARN_ON_ONCE(tick_do_timer_cpu == TICK_DO_TIMER_NONE))\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\n \nvoid tick_nohz_idle_stop_tick(void)\n{\n\tstruct tick_sched *ts = this_cpu_ptr(&tick_cpu_sched);\n\tint cpu = smp_processor_id();\n\tktime_t expires;\n\n\t \n\tif (ts->timer_expires_base)\n\t\texpires = ts->timer_expires;\n\telse if (can_stop_idle_tick(cpu, ts))\n\t\texpires = tick_nohz_next_event(ts, cpu);\n\telse\n\t\treturn;\n\n\tts->idle_calls++;\n\n\tif (expires > 0LL) {\n\t\tint was_stopped = ts->tick_stopped;\n\n\t\ttick_nohz_stop_tick(ts, cpu);\n\n\t\tts->idle_sleeps++;\n\t\tts->idle_expires = expires;\n\n\t\tif (!was_stopped && ts->tick_stopped) {\n\t\t\tts->idle_jiffies = ts->last_jiffies;\n\t\t\tnohz_balance_enter_idle(cpu);\n\t\t}\n\t} else {\n\t\ttick_nohz_retain_tick(ts);\n\t}\n}\n\nvoid tick_nohz_idle_retain_tick(void)\n{\n\ttick_nohz_retain_tick(this_cpu_ptr(&tick_cpu_sched));\n\t \n\ttimer_clear_idle();\n}\n\n \nvoid tick_nohz_idle_enter(void)\n{\n\tstruct tick_sched *ts;\n\n\tlockdep_assert_irqs_enabled();\n\n\tlocal_irq_disable();\n\n\tts = this_cpu_ptr(&tick_cpu_sched);\n\n\tWARN_ON_ONCE(ts->timer_expires_base);\n\n\tts->inidle = 1;\n\ttick_nohz_start_idle(ts);\n\n\tlocal_irq_enable();\n}\n\n \nvoid tick_nohz_irq_exit(void)\n{\n\tstruct tick_sched *ts = this_cpu_ptr(&tick_cpu_sched);\n\n\tif (ts->inidle)\n\t\ttick_nohz_start_idle(ts);\n\telse\n\t\ttick_nohz_full_update_tick(ts);\n}\n\n \nbool tick_nohz_idle_got_tick(void)\n{\n\tstruct tick_sched *ts = this_cpu_ptr(&tick_cpu_sched);\n\n\tif (ts->got_idle_tick) {\n\t\tts->got_idle_tick = 0;\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n \nktime_t tick_nohz_get_next_hrtimer(void)\n{\n\treturn __this_cpu_read(tick_cpu_device.evtdev)->next_event;\n}\n\n \nktime_t tick_nohz_get_sleep_length(ktime_t *delta_next)\n{\n\tstruct clock_event_device *dev = __this_cpu_read(tick_cpu_device.evtdev);\n\tstruct tick_sched *ts = this_cpu_ptr(&tick_cpu_sched);\n\tint cpu = smp_processor_id();\n\t \n\tktime_t now = ts->idle_entrytime;\n\tktime_t next_event;\n\n\tWARN_ON_ONCE(!ts->inidle);\n\n\t*delta_next = ktime_sub(dev->next_event, now);\n\n\tif (!can_stop_idle_tick(cpu, ts))\n\t\treturn *delta_next;\n\n\tnext_event = tick_nohz_next_event(ts, cpu);\n\tif (!next_event)\n\t\treturn *delta_next;\n\n\t \n\tnext_event = min_t(u64, next_event,\n\t\t\t   hrtimer_next_event_without(&ts->sched_timer));\n\n\treturn ktime_sub(next_event, now);\n}\n\n \nunsigned long tick_nohz_get_idle_calls_cpu(int cpu)\n{\n\tstruct tick_sched *ts = tick_get_tick_sched(cpu);\n\n\treturn ts->idle_calls;\n}\n\n \nunsigned long tick_nohz_get_idle_calls(void)\n{\n\tstruct tick_sched *ts = this_cpu_ptr(&tick_cpu_sched);\n\n\treturn ts->idle_calls;\n}\n\nstatic void tick_nohz_account_idle_time(struct tick_sched *ts,\n\t\t\t\t\tktime_t now)\n{\n\tunsigned long ticks;\n\n\tts->idle_exittime = now;\n\n\tif (vtime_accounting_enabled_this_cpu())\n\t\treturn;\n\t \n\tticks = jiffies - ts->idle_jiffies;\n\t \n\tif (ticks && ticks < LONG_MAX)\n\t\taccount_idle_ticks(ticks);\n}\n\nvoid tick_nohz_idle_restart_tick(void)\n{\n\tstruct tick_sched *ts = this_cpu_ptr(&tick_cpu_sched);\n\n\tif (ts->tick_stopped) {\n\t\tktime_t now = ktime_get();\n\t\ttick_nohz_restart_sched_tick(ts, now);\n\t\ttick_nohz_account_idle_time(ts, now);\n\t}\n}\n\nstatic void tick_nohz_idle_update_tick(struct tick_sched *ts, ktime_t now)\n{\n\tif (tick_nohz_full_cpu(smp_processor_id()))\n\t\t__tick_nohz_full_update_tick(ts, now);\n\telse\n\t\ttick_nohz_restart_sched_tick(ts, now);\n\n\ttick_nohz_account_idle_time(ts, now);\n}\n\n \nvoid tick_nohz_idle_exit(void)\n{\n\tstruct tick_sched *ts = this_cpu_ptr(&tick_cpu_sched);\n\tbool idle_active, tick_stopped;\n\tktime_t now;\n\n\tlocal_irq_disable();\n\n\tWARN_ON_ONCE(!ts->inidle);\n\tWARN_ON_ONCE(ts->timer_expires_base);\n\n\tts->inidle = 0;\n\tidle_active = ts->idle_active;\n\ttick_stopped = ts->tick_stopped;\n\n\tif (idle_active || tick_stopped)\n\t\tnow = ktime_get();\n\n\tif (idle_active)\n\t\ttick_nohz_stop_idle(ts, now);\n\n\tif (tick_stopped)\n\t\ttick_nohz_idle_update_tick(ts, now);\n\n\tlocal_irq_enable();\n}\n\n \nstatic void tick_nohz_handler(struct clock_event_device *dev)\n{\n\tstruct tick_sched *ts = this_cpu_ptr(&tick_cpu_sched);\n\tstruct pt_regs *regs = get_irq_regs();\n\tktime_t now = ktime_get();\n\n\tdev->next_event = KTIME_MAX;\n\n\ttick_sched_do_timer(ts, now);\n\ttick_sched_handle(ts, regs);\n\n\tif (unlikely(ts->tick_stopped)) {\n\t\t \n\t\ttick_program_event(KTIME_MAX, 1);\n\t\treturn;\n\t}\n\n\thrtimer_forward(&ts->sched_timer, now, TICK_NSEC);\n\ttick_program_event(hrtimer_get_expires(&ts->sched_timer), 1);\n}\n\nstatic inline void tick_nohz_activate(struct tick_sched *ts, int mode)\n{\n\tif (!tick_nohz_enabled)\n\t\treturn;\n\tts->nohz_mode = mode;\n\t \n\tif (!test_and_set_bit(0, &tick_nohz_active))\n\t\ttimers_update_nohz();\n}\n\n \nstatic void tick_nohz_switch_to_nohz(void)\n{\n\tstruct tick_sched *ts = this_cpu_ptr(&tick_cpu_sched);\n\tktime_t next;\n\n\tif (!tick_nohz_enabled)\n\t\treturn;\n\n\tif (tick_switch_to_oneshot(tick_nohz_handler))\n\t\treturn;\n\n\t \n\thrtimer_init(&ts->sched_timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS_HARD);\n\t \n\tnext = tick_init_jiffy_update();\n\n\thrtimer_set_expires(&ts->sched_timer, next);\n\thrtimer_forward_now(&ts->sched_timer, TICK_NSEC);\n\ttick_program_event(hrtimer_get_expires(&ts->sched_timer), 1);\n\ttick_nohz_activate(ts, NOHZ_MODE_LOWRES);\n}\n\nstatic inline void tick_nohz_irq_enter(void)\n{\n\tstruct tick_sched *ts = this_cpu_ptr(&tick_cpu_sched);\n\tktime_t now;\n\n\tif (!ts->idle_active && !ts->tick_stopped)\n\t\treturn;\n\tnow = ktime_get();\n\tif (ts->idle_active)\n\t\ttick_nohz_stop_idle(ts, now);\n\t \n\tif (ts->tick_stopped)\n\t\ttick_nohz_update_jiffies(now);\n}\n\n#else\n\nstatic inline void tick_nohz_switch_to_nohz(void) { }\nstatic inline void tick_nohz_irq_enter(void) { }\nstatic inline void tick_nohz_activate(struct tick_sched *ts, int mode) { }\n\n#endif  \n\n \nvoid tick_irq_enter(void)\n{\n\ttick_check_oneshot_broadcast_this_cpu();\n\ttick_nohz_irq_enter();\n}\n\n \n#ifdef CONFIG_HIGH_RES_TIMERS\n \nstatic enum hrtimer_restart tick_sched_timer(struct hrtimer *timer)\n{\n\tstruct tick_sched *ts =\n\t\tcontainer_of(timer, struct tick_sched, sched_timer);\n\tstruct pt_regs *regs = get_irq_regs();\n\tktime_t now = ktime_get();\n\n\ttick_sched_do_timer(ts, now);\n\n\t \n\tif (regs)\n\t\ttick_sched_handle(ts, regs);\n\telse\n\t\tts->next_tick = 0;\n\n\t \n\tif (unlikely(ts->tick_stopped))\n\t\treturn HRTIMER_NORESTART;\n\n\thrtimer_forward(timer, now, TICK_NSEC);\n\n\treturn HRTIMER_RESTART;\n}\n\nstatic int sched_skew_tick;\n\nstatic int __init skew_tick(char *str)\n{\n\tget_option(&str, &sched_skew_tick);\n\n\treturn 0;\n}\nearly_param(\"skew_tick\", skew_tick);\n\n \nvoid tick_setup_sched_timer(void)\n{\n\tstruct tick_sched *ts = this_cpu_ptr(&tick_cpu_sched);\n\tktime_t now = ktime_get();\n\n\t \n\thrtimer_init(&ts->sched_timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS_HARD);\n\tts->sched_timer.function = tick_sched_timer;\n\n\t \n\thrtimer_set_expires(&ts->sched_timer, tick_init_jiffy_update());\n\n\t \n\tif (sched_skew_tick) {\n\t\tu64 offset = TICK_NSEC >> 1;\n\t\tdo_div(offset, num_possible_cpus());\n\t\toffset *= smp_processor_id();\n\t\thrtimer_add_expires_ns(&ts->sched_timer, offset);\n\t}\n\n\thrtimer_forward(&ts->sched_timer, now, TICK_NSEC);\n\thrtimer_start_expires(&ts->sched_timer, HRTIMER_MODE_ABS_PINNED_HARD);\n\ttick_nohz_activate(ts, NOHZ_MODE_HIGHRES);\n}\n#endif  \n\n#if defined CONFIG_NO_HZ_COMMON || defined CONFIG_HIGH_RES_TIMERS\nvoid tick_cancel_sched_timer(int cpu)\n{\n\tstruct tick_sched *ts = &per_cpu(tick_cpu_sched, cpu);\n\tktime_t idle_sleeptime, iowait_sleeptime;\n\n# ifdef CONFIG_HIGH_RES_TIMERS\n\tif (ts->sched_timer.base)\n\t\thrtimer_cancel(&ts->sched_timer);\n# endif\n\n\tidle_sleeptime = ts->idle_sleeptime;\n\tiowait_sleeptime = ts->iowait_sleeptime;\n\tmemset(ts, 0, sizeof(*ts));\n\tts->idle_sleeptime = idle_sleeptime;\n\tts->iowait_sleeptime = iowait_sleeptime;\n}\n#endif\n\n \nvoid tick_clock_notify(void)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu)\n\t\tset_bit(0, &per_cpu(tick_cpu_sched, cpu).check_clocks);\n}\n\n \nvoid tick_oneshot_notify(void)\n{\n\tstruct tick_sched *ts = this_cpu_ptr(&tick_cpu_sched);\n\n\tset_bit(0, &ts->check_clocks);\n}\n\n \nint tick_check_oneshot_change(int allow_nohz)\n{\n\tstruct tick_sched *ts = this_cpu_ptr(&tick_cpu_sched);\n\n\tif (!test_and_clear_bit(0, &ts->check_clocks))\n\t\treturn 0;\n\n\tif (ts->nohz_mode != NOHZ_MODE_INACTIVE)\n\t\treturn 0;\n\n\tif (!timekeeping_valid_for_hres() || !tick_is_oneshot_available())\n\t\treturn 0;\n\n\tif (!allow_nohz)\n\t\treturn 1;\n\n\ttick_nohz_switch_to_nohz();\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}