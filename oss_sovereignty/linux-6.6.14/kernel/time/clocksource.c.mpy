{
  "module_name": "clocksource.c",
  "hash_id": "8c96e194879474780486e4e6afc54558a88ff1b7f7bc428664869c105e8b5a6d",
  "original_prompt": "Ingested from linux-6.6.14/kernel/time/clocksource.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/device.h>\n#include <linux/clocksource.h>\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/sched.h>  \n#include <linux/tick.h>\n#include <linux/kthread.h>\n#include <linux/prandom.h>\n#include <linux/cpu.h>\n\n#include \"tick-internal.h\"\n#include \"timekeeping_internal.h\"\n\n \nvoid\nclocks_calc_mult_shift(u32 *mult, u32 *shift, u32 from, u32 to, u32 maxsec)\n{\n\tu64 tmp;\n\tu32 sft, sftacc= 32;\n\n\t \n\ttmp = ((u64)maxsec * from) >> 32;\n\twhile (tmp) {\n\t\ttmp >>=1;\n\t\tsftacc--;\n\t}\n\n\t \n\tfor (sft = 32; sft > 0; sft--) {\n\t\ttmp = (u64) to << sft;\n\t\ttmp += from / 2;\n\t\tdo_div(tmp, from);\n\t\tif ((tmp >> sftacc) == 0)\n\t\t\tbreak;\n\t}\n\t*mult = tmp;\n\t*shift = sft;\n}\nEXPORT_SYMBOL_GPL(clocks_calc_mult_shift);\n\n \nstatic struct clocksource *curr_clocksource;\nstatic struct clocksource *suspend_clocksource;\nstatic LIST_HEAD(clocksource_list);\nstatic DEFINE_MUTEX(clocksource_mutex);\nstatic char override_name[CS_NAME_LEN];\nstatic int finished_booting;\nstatic u64 suspend_start;\n\n \n#define WATCHDOG_INTERVAL (HZ >> 1)\n\n \n#define WATCHDOG_THRESHOLD (NSEC_PER_SEC >> 5)\n\n \n#ifdef CONFIG_CLOCKSOURCE_WATCHDOG_MAX_SKEW_US\n#define MAX_SKEW_USEC\tCONFIG_CLOCKSOURCE_WATCHDOG_MAX_SKEW_US\n#else\n#define MAX_SKEW_USEC\t(125 * WATCHDOG_INTERVAL / HZ)\n#endif\n\n#define WATCHDOG_MAX_SKEW (MAX_SKEW_USEC * NSEC_PER_USEC)\n\n#ifdef CONFIG_CLOCKSOURCE_WATCHDOG\nstatic void clocksource_watchdog_work(struct work_struct *work);\nstatic void clocksource_select(void);\n\nstatic LIST_HEAD(watchdog_list);\nstatic struct clocksource *watchdog;\nstatic struct timer_list watchdog_timer;\nstatic DECLARE_WORK(watchdog_work, clocksource_watchdog_work);\nstatic DEFINE_SPINLOCK(watchdog_lock);\nstatic int watchdog_running;\nstatic atomic_t watchdog_reset_pending;\n\nstatic inline void clocksource_watchdog_lock(unsigned long *flags)\n{\n\tspin_lock_irqsave(&watchdog_lock, *flags);\n}\n\nstatic inline void clocksource_watchdog_unlock(unsigned long *flags)\n{\n\tspin_unlock_irqrestore(&watchdog_lock, *flags);\n}\n\nstatic int clocksource_watchdog_kthread(void *data);\nstatic void __clocksource_change_rating(struct clocksource *cs, int rating);\n\nstatic void clocksource_watchdog_work(struct work_struct *work)\n{\n\t \n\tkthread_run(clocksource_watchdog_kthread, NULL, \"kwatchdog\");\n}\n\nstatic void __clocksource_unstable(struct clocksource *cs)\n{\n\tcs->flags &= ~(CLOCK_SOURCE_VALID_FOR_HRES | CLOCK_SOURCE_WATCHDOG);\n\tcs->flags |= CLOCK_SOURCE_UNSTABLE;\n\n\t \n\tif (list_empty(&cs->list)) {\n\t\tcs->rating = 0;\n\t\treturn;\n\t}\n\n\tif (cs->mark_unstable)\n\t\tcs->mark_unstable(cs);\n\n\t \n\tif (finished_booting)\n\t\tschedule_work(&watchdog_work);\n}\n\n \nvoid clocksource_mark_unstable(struct clocksource *cs)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&watchdog_lock, flags);\n\tif (!(cs->flags & CLOCK_SOURCE_UNSTABLE)) {\n\t\tif (!list_empty(&cs->list) && list_empty(&cs->wd_list))\n\t\t\tlist_add(&cs->wd_list, &watchdog_list);\n\t\t__clocksource_unstable(cs);\n\t}\n\tspin_unlock_irqrestore(&watchdog_lock, flags);\n}\n\nulong max_cswd_read_retries = 2;\nmodule_param(max_cswd_read_retries, ulong, 0644);\nEXPORT_SYMBOL_GPL(max_cswd_read_retries);\nstatic int verify_n_cpus = 8;\nmodule_param(verify_n_cpus, int, 0644);\n\nenum wd_read_status {\n\tWD_READ_SUCCESS,\n\tWD_READ_UNSTABLE,\n\tWD_READ_SKIP\n};\n\nstatic enum wd_read_status cs_watchdog_read(struct clocksource *cs, u64 *csnow, u64 *wdnow)\n{\n\tunsigned int nretries;\n\tu64 wd_end, wd_end2, wd_delta;\n\tint64_t wd_delay, wd_seq_delay;\n\n\tfor (nretries = 0; nretries <= max_cswd_read_retries; nretries++) {\n\t\tlocal_irq_disable();\n\t\t*wdnow = watchdog->read(watchdog);\n\t\t*csnow = cs->read(cs);\n\t\twd_end = watchdog->read(watchdog);\n\t\twd_end2 = watchdog->read(watchdog);\n\t\tlocal_irq_enable();\n\n\t\twd_delta = clocksource_delta(wd_end, *wdnow, watchdog->mask);\n\t\twd_delay = clocksource_cyc2ns(wd_delta, watchdog->mult,\n\t\t\t\t\t      watchdog->shift);\n\t\tif (wd_delay <= WATCHDOG_MAX_SKEW) {\n\t\t\tif (nretries > 1 || nretries >= max_cswd_read_retries) {\n\t\t\t\tpr_warn(\"timekeeping watchdog on CPU%d: %s retried %d times before success\\n\",\n\t\t\t\t\tsmp_processor_id(), watchdog->name, nretries);\n\t\t\t}\n\t\t\treturn WD_READ_SUCCESS;\n\t\t}\n\n\t\t \n\t\twd_delta = clocksource_delta(wd_end2, wd_end, watchdog->mask);\n\t\twd_seq_delay = clocksource_cyc2ns(wd_delta, watchdog->mult, watchdog->shift);\n\t\tif (wd_seq_delay > WATCHDOG_MAX_SKEW/2)\n\t\t\tgoto skip_test;\n\t}\n\n\tpr_warn(\"timekeeping watchdog on CPU%d: wd-%s-wd excessive read-back delay of %lldns vs. limit of %ldns, wd-wd read-back delay only %lldns, attempt %d, marking %s unstable\\n\",\n\t\tsmp_processor_id(), cs->name, wd_delay, WATCHDOG_MAX_SKEW, wd_seq_delay, nretries, cs->name);\n\treturn WD_READ_UNSTABLE;\n\nskip_test:\n\tpr_info(\"timekeeping watchdog on CPU%d: %s wd-wd read-back delay of %lldns\\n\",\n\t\tsmp_processor_id(), watchdog->name, wd_seq_delay);\n\tpr_info(\"wd-%s-wd read-back delay of %lldns, clock-skew test skipped!\\n\",\n\t\tcs->name, wd_delay);\n\treturn WD_READ_SKIP;\n}\n\nstatic u64 csnow_mid;\nstatic cpumask_t cpus_ahead;\nstatic cpumask_t cpus_behind;\nstatic cpumask_t cpus_chosen;\n\nstatic void clocksource_verify_choose_cpus(void)\n{\n\tint cpu, i, n = verify_n_cpus;\n\n\tif (n < 0) {\n\t\t \n\t\tcpumask_copy(&cpus_chosen, cpu_online_mask);\n\t\tcpumask_clear_cpu(smp_processor_id(), &cpus_chosen);\n\t\treturn;\n\t}\n\n\t \n\tcpumask_clear(&cpus_chosen);\n\tif (n == 0 || num_online_cpus() <= 1)\n\t\treturn;\n\n\t \n\tcpu = cpumask_first(cpu_online_mask);\n\tif (cpu == smp_processor_id())\n\t\tcpu = cpumask_next(cpu, cpu_online_mask);\n\tif (WARN_ON_ONCE(cpu >= nr_cpu_ids))\n\t\treturn;\n\tcpumask_set_cpu(cpu, &cpus_chosen);\n\n\t \n\tif (n > nr_cpu_ids)\n\t\tn = nr_cpu_ids;\n\n\t \n\tfor (i = 1; i < n; i++) {\n\t\tcpu = get_random_u32_below(nr_cpu_ids);\n\t\tcpu = cpumask_next(cpu - 1, cpu_online_mask);\n\t\tif (cpu >= nr_cpu_ids)\n\t\t\tcpu = cpumask_first(cpu_online_mask);\n\t\tif (!WARN_ON_ONCE(cpu >= nr_cpu_ids))\n\t\t\tcpumask_set_cpu(cpu, &cpus_chosen);\n\t}\n\n\t \n\tcpumask_clear_cpu(smp_processor_id(), &cpus_chosen);\n}\n\nstatic void clocksource_verify_one_cpu(void *csin)\n{\n\tstruct clocksource *cs = (struct clocksource *)csin;\n\n\tcsnow_mid = cs->read(cs);\n}\n\nvoid clocksource_verify_percpu(struct clocksource *cs)\n{\n\tint64_t cs_nsec, cs_nsec_max = 0, cs_nsec_min = LLONG_MAX;\n\tu64 csnow_begin, csnow_end;\n\tint cpu, testcpu;\n\ts64 delta;\n\n\tif (verify_n_cpus == 0)\n\t\treturn;\n\tcpumask_clear(&cpus_ahead);\n\tcpumask_clear(&cpus_behind);\n\tcpus_read_lock();\n\tpreempt_disable();\n\tclocksource_verify_choose_cpus();\n\tif (cpumask_empty(&cpus_chosen)) {\n\t\tpreempt_enable();\n\t\tcpus_read_unlock();\n\t\tpr_warn(\"Not enough CPUs to check clocksource '%s'.\\n\", cs->name);\n\t\treturn;\n\t}\n\ttestcpu = smp_processor_id();\n\tpr_warn(\"Checking clocksource %s synchronization from CPU %d to CPUs %*pbl.\\n\", cs->name, testcpu, cpumask_pr_args(&cpus_chosen));\n\tfor_each_cpu(cpu, &cpus_chosen) {\n\t\tif (cpu == testcpu)\n\t\t\tcontinue;\n\t\tcsnow_begin = cs->read(cs);\n\t\tsmp_call_function_single(cpu, clocksource_verify_one_cpu, cs, 1);\n\t\tcsnow_end = cs->read(cs);\n\t\tdelta = (s64)((csnow_mid - csnow_begin) & cs->mask);\n\t\tif (delta < 0)\n\t\t\tcpumask_set_cpu(cpu, &cpus_behind);\n\t\tdelta = (csnow_end - csnow_mid) & cs->mask;\n\t\tif (delta < 0)\n\t\t\tcpumask_set_cpu(cpu, &cpus_ahead);\n\t\tdelta = clocksource_delta(csnow_end, csnow_begin, cs->mask);\n\t\tcs_nsec = clocksource_cyc2ns(delta, cs->mult, cs->shift);\n\t\tif (cs_nsec > cs_nsec_max)\n\t\t\tcs_nsec_max = cs_nsec;\n\t\tif (cs_nsec < cs_nsec_min)\n\t\t\tcs_nsec_min = cs_nsec;\n\t}\n\tpreempt_enable();\n\tcpus_read_unlock();\n\tif (!cpumask_empty(&cpus_ahead))\n\t\tpr_warn(\"        CPUs %*pbl ahead of CPU %d for clocksource %s.\\n\",\n\t\t\tcpumask_pr_args(&cpus_ahead), testcpu, cs->name);\n\tif (!cpumask_empty(&cpus_behind))\n\t\tpr_warn(\"        CPUs %*pbl behind CPU %d for clocksource %s.\\n\",\n\t\t\tcpumask_pr_args(&cpus_behind), testcpu, cs->name);\n\tif (!cpumask_empty(&cpus_ahead) || !cpumask_empty(&cpus_behind))\n\t\tpr_warn(\"        CPU %d check durations %lldns - %lldns for clocksource %s.\\n\",\n\t\t\ttestcpu, cs_nsec_min, cs_nsec_max, cs->name);\n}\nEXPORT_SYMBOL_GPL(clocksource_verify_percpu);\n\nstatic inline void clocksource_reset_watchdog(void)\n{\n\tstruct clocksource *cs;\n\n\tlist_for_each_entry(cs, &watchdog_list, wd_list)\n\t\tcs->flags &= ~CLOCK_SOURCE_WATCHDOG;\n}\n\n\nstatic void clocksource_watchdog(struct timer_list *unused)\n{\n\tu64 csnow, wdnow, cslast, wdlast, delta;\n\tint next_cpu, reset_pending;\n\tint64_t wd_nsec, cs_nsec;\n\tstruct clocksource *cs;\n\tenum wd_read_status read_ret;\n\tunsigned long extra_wait = 0;\n\tu32 md;\n\n\tspin_lock(&watchdog_lock);\n\tif (!watchdog_running)\n\t\tgoto out;\n\n\treset_pending = atomic_read(&watchdog_reset_pending);\n\n\tlist_for_each_entry(cs, &watchdog_list, wd_list) {\n\n\t\t \n\t\tif (cs->flags & CLOCK_SOURCE_UNSTABLE) {\n\t\t\tif (finished_booting)\n\t\t\t\tschedule_work(&watchdog_work);\n\t\t\tcontinue;\n\t\t}\n\n\t\tread_ret = cs_watchdog_read(cs, &csnow, &wdnow);\n\n\t\tif (read_ret == WD_READ_UNSTABLE) {\n\t\t\t \n\t\t\t__clocksource_unstable(cs);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (read_ret == WD_READ_SKIP) {\n\t\t\t \n\t\t\tclocksource_reset_watchdog();\n\t\t\textra_wait = HZ * 300;\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (!(cs->flags & CLOCK_SOURCE_WATCHDOG) ||\n\t\t    atomic_read(&watchdog_reset_pending)) {\n\t\t\tcs->flags |= CLOCK_SOURCE_WATCHDOG;\n\t\t\tcs->wd_last = wdnow;\n\t\t\tcs->cs_last = csnow;\n\t\t\tcontinue;\n\t\t}\n\n\t\tdelta = clocksource_delta(wdnow, cs->wd_last, watchdog->mask);\n\t\twd_nsec = clocksource_cyc2ns(delta, watchdog->mult,\n\t\t\t\t\t     watchdog->shift);\n\n\t\tdelta = clocksource_delta(csnow, cs->cs_last, cs->mask);\n\t\tcs_nsec = clocksource_cyc2ns(delta, cs->mult, cs->shift);\n\t\twdlast = cs->wd_last;  \n\t\tcslast = cs->cs_last;\n\t\tcs->cs_last = csnow;\n\t\tcs->wd_last = wdnow;\n\n\t\tif (atomic_read(&watchdog_reset_pending))\n\t\t\tcontinue;\n\n\t\t \n\t\tmd = cs->uncertainty_margin + watchdog->uncertainty_margin;\n\t\tif (abs(cs_nsec - wd_nsec) > md) {\n\t\t\ts64 cs_wd_msec;\n\t\t\ts64 wd_msec;\n\t\t\tu32 wd_rem;\n\n\t\t\tpr_warn(\"timekeeping watchdog on CPU%d: Marking clocksource '%s' as unstable because the skew is too large:\\n\",\n\t\t\t\tsmp_processor_id(), cs->name);\n\t\t\tpr_warn(\"                      '%s' wd_nsec: %lld wd_now: %llx wd_last: %llx mask: %llx\\n\",\n\t\t\t\twatchdog->name, wd_nsec, wdnow, wdlast, watchdog->mask);\n\t\t\tpr_warn(\"                      '%s' cs_nsec: %lld cs_now: %llx cs_last: %llx mask: %llx\\n\",\n\t\t\t\tcs->name, cs_nsec, csnow, cslast, cs->mask);\n\t\t\tcs_wd_msec = div_s64_rem(cs_nsec - wd_nsec, 1000 * 1000, &wd_rem);\n\t\t\twd_msec = div_s64_rem(wd_nsec, 1000 * 1000, &wd_rem);\n\t\t\tpr_warn(\"                      Clocksource '%s' skewed %lld ns (%lld ms) over watchdog '%s' interval of %lld ns (%lld ms)\\n\",\n\t\t\t\tcs->name, cs_nsec - wd_nsec, cs_wd_msec, watchdog->name, wd_nsec, wd_msec);\n\t\t\tif (curr_clocksource == cs)\n\t\t\t\tpr_warn(\"                      '%s' is current clocksource.\\n\", cs->name);\n\t\t\telse if (curr_clocksource)\n\t\t\t\tpr_warn(\"                      '%s' (not '%s') is current clocksource.\\n\", curr_clocksource->name, cs->name);\n\t\t\telse\n\t\t\t\tpr_warn(\"                      No current clocksource.\\n\");\n\t\t\t__clocksource_unstable(cs);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (cs == curr_clocksource && cs->tick_stable)\n\t\t\tcs->tick_stable(cs);\n\n\t\tif (!(cs->flags & CLOCK_SOURCE_VALID_FOR_HRES) &&\n\t\t    (cs->flags & CLOCK_SOURCE_IS_CONTINUOUS) &&\n\t\t    (watchdog->flags & CLOCK_SOURCE_IS_CONTINUOUS)) {\n\t\t\t \n\t\t\tcs->flags |= CLOCK_SOURCE_VALID_FOR_HRES;\n\n\t\t\t \n\t\t\tif (!finished_booting)\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tif (cs != curr_clocksource) {\n\t\t\t\tcs->flags |= CLOCK_SOURCE_RESELECT;\n\t\t\t\tschedule_work(&watchdog_work);\n\t\t\t} else {\n\t\t\t\ttick_clock_notify();\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tif (reset_pending)\n\t\tatomic_dec(&watchdog_reset_pending);\n\n\t \n\tnext_cpu = cpumask_next(raw_smp_processor_id(), cpu_online_mask);\n\tif (next_cpu >= nr_cpu_ids)\n\t\tnext_cpu = cpumask_first(cpu_online_mask);\n\n\t \n\tif (!timer_pending(&watchdog_timer)) {\n\t\twatchdog_timer.expires += WATCHDOG_INTERVAL + extra_wait;\n\t\tadd_timer_on(&watchdog_timer, next_cpu);\n\t}\nout:\n\tspin_unlock(&watchdog_lock);\n}\n\nstatic inline void clocksource_start_watchdog(void)\n{\n\tif (watchdog_running || !watchdog || list_empty(&watchdog_list))\n\t\treturn;\n\ttimer_setup(&watchdog_timer, clocksource_watchdog, 0);\n\twatchdog_timer.expires = jiffies + WATCHDOG_INTERVAL;\n\tadd_timer_on(&watchdog_timer, cpumask_first(cpu_online_mask));\n\twatchdog_running = 1;\n}\n\nstatic inline void clocksource_stop_watchdog(void)\n{\n\tif (!watchdog_running || (watchdog && !list_empty(&watchdog_list)))\n\t\treturn;\n\tdel_timer(&watchdog_timer);\n\twatchdog_running = 0;\n}\n\nstatic void clocksource_resume_watchdog(void)\n{\n\tatomic_inc(&watchdog_reset_pending);\n}\n\nstatic void clocksource_enqueue_watchdog(struct clocksource *cs)\n{\n\tINIT_LIST_HEAD(&cs->wd_list);\n\n\tif (cs->flags & CLOCK_SOURCE_MUST_VERIFY) {\n\t\t \n\t\tlist_add(&cs->wd_list, &watchdog_list);\n\t\tcs->flags &= ~CLOCK_SOURCE_WATCHDOG;\n\t} else {\n\t\t \n\t\tif (cs->flags & CLOCK_SOURCE_IS_CONTINUOUS)\n\t\t\tcs->flags |= CLOCK_SOURCE_VALID_FOR_HRES;\n\t}\n}\n\nstatic void clocksource_select_watchdog(bool fallback)\n{\n\tstruct clocksource *cs, *old_wd;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&watchdog_lock, flags);\n\t \n\told_wd = watchdog;\n\tif (fallback)\n\t\twatchdog = NULL;\n\n\tlist_for_each_entry(cs, &clocksource_list, list) {\n\t\t \n\t\tif (cs->flags & CLOCK_SOURCE_MUST_VERIFY)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (fallback && cs == old_wd)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (!watchdog || cs->rating > watchdog->rating)\n\t\t\twatchdog = cs;\n\t}\n\t \n\tif (!watchdog)\n\t\twatchdog = old_wd;\n\n\t \n\tif (watchdog != old_wd)\n\t\tclocksource_reset_watchdog();\n\n\t \n\tclocksource_start_watchdog();\n\tspin_unlock_irqrestore(&watchdog_lock, flags);\n}\n\nstatic void clocksource_dequeue_watchdog(struct clocksource *cs)\n{\n\tif (cs != watchdog) {\n\t\tif (cs->flags & CLOCK_SOURCE_MUST_VERIFY) {\n\t\t\t \n\t\t\tlist_del_init(&cs->wd_list);\n\t\t\t \n\t\t\tclocksource_stop_watchdog();\n\t\t}\n\t}\n}\n\nstatic int __clocksource_watchdog_kthread(void)\n{\n\tstruct clocksource *cs, *tmp;\n\tunsigned long flags;\n\tint select = 0;\n\n\t \n\tif (curr_clocksource &&\n\t    curr_clocksource->flags & CLOCK_SOURCE_UNSTABLE &&\n\t    curr_clocksource->flags & CLOCK_SOURCE_VERIFY_PERCPU)\n\t\tclocksource_verify_percpu(curr_clocksource);\n\n\tspin_lock_irqsave(&watchdog_lock, flags);\n\tlist_for_each_entry_safe(cs, tmp, &watchdog_list, wd_list) {\n\t\tif (cs->flags & CLOCK_SOURCE_UNSTABLE) {\n\t\t\tlist_del_init(&cs->wd_list);\n\t\t\t__clocksource_change_rating(cs, 0);\n\t\t\tselect = 1;\n\t\t}\n\t\tif (cs->flags & CLOCK_SOURCE_RESELECT) {\n\t\t\tcs->flags &= ~CLOCK_SOURCE_RESELECT;\n\t\t\tselect = 1;\n\t\t}\n\t}\n\t \n\tclocksource_stop_watchdog();\n\tspin_unlock_irqrestore(&watchdog_lock, flags);\n\n\treturn select;\n}\n\nstatic int clocksource_watchdog_kthread(void *data)\n{\n\tmutex_lock(&clocksource_mutex);\n\tif (__clocksource_watchdog_kthread())\n\t\tclocksource_select();\n\tmutex_unlock(&clocksource_mutex);\n\treturn 0;\n}\n\nstatic bool clocksource_is_watchdog(struct clocksource *cs)\n{\n\treturn cs == watchdog;\n}\n\n#else  \n\nstatic void clocksource_enqueue_watchdog(struct clocksource *cs)\n{\n\tif (cs->flags & CLOCK_SOURCE_IS_CONTINUOUS)\n\t\tcs->flags |= CLOCK_SOURCE_VALID_FOR_HRES;\n}\n\nstatic void clocksource_select_watchdog(bool fallback) { }\nstatic inline void clocksource_dequeue_watchdog(struct clocksource *cs) { }\nstatic inline void clocksource_resume_watchdog(void) { }\nstatic inline int __clocksource_watchdog_kthread(void) { return 0; }\nstatic bool clocksource_is_watchdog(struct clocksource *cs) { return false; }\nvoid clocksource_mark_unstable(struct clocksource *cs) { }\n\nstatic inline void clocksource_watchdog_lock(unsigned long *flags) { }\nstatic inline void clocksource_watchdog_unlock(unsigned long *flags) { }\n\n#endif  \n\nstatic bool clocksource_is_suspend(struct clocksource *cs)\n{\n\treturn cs == suspend_clocksource;\n}\n\nstatic void __clocksource_suspend_select(struct clocksource *cs)\n{\n\t \n\tif (!(cs->flags & CLOCK_SOURCE_SUSPEND_NONSTOP))\n\t\treturn;\n\n\t \n\tif (cs->suspend || cs->resume) {\n\t\tpr_warn(\"Nonstop clocksource %s should not supply suspend/resume interfaces\\n\",\n\t\t\tcs->name);\n\t}\n\n\t \n\tif (!suspend_clocksource || cs->rating > suspend_clocksource->rating)\n\t\tsuspend_clocksource = cs;\n}\n\n \nstatic void clocksource_suspend_select(bool fallback)\n{\n\tstruct clocksource *cs, *old_suspend;\n\n\told_suspend = suspend_clocksource;\n\tif (fallback)\n\t\tsuspend_clocksource = NULL;\n\n\tlist_for_each_entry(cs, &clocksource_list, list) {\n\t\t \n\t\tif (fallback && cs == old_suspend)\n\t\t\tcontinue;\n\n\t\t__clocksource_suspend_select(cs);\n\t}\n}\n\n \nvoid clocksource_start_suspend_timing(struct clocksource *cs, u64 start_cycles)\n{\n\tif (!suspend_clocksource)\n\t\treturn;\n\n\t \n\tif (clocksource_is_suspend(cs)) {\n\t\tsuspend_start = start_cycles;\n\t\treturn;\n\t}\n\n\tif (suspend_clocksource->enable &&\n\t    suspend_clocksource->enable(suspend_clocksource)) {\n\t\tpr_warn_once(\"Failed to enable the non-suspend-able clocksource.\\n\");\n\t\treturn;\n\t}\n\n\tsuspend_start = suspend_clocksource->read(suspend_clocksource);\n}\n\n \nu64 clocksource_stop_suspend_timing(struct clocksource *cs, u64 cycle_now)\n{\n\tu64 now, delta, nsec = 0;\n\n\tif (!suspend_clocksource)\n\t\treturn 0;\n\n\t \n\tif (clocksource_is_suspend(cs))\n\t\tnow = cycle_now;\n\telse\n\t\tnow = suspend_clocksource->read(suspend_clocksource);\n\n\tif (now > suspend_start) {\n\t\tdelta = clocksource_delta(now, suspend_start,\n\t\t\t\t\t  suspend_clocksource->mask);\n\t\tnsec = mul_u64_u32_shr(delta, suspend_clocksource->mult,\n\t\t\t\t       suspend_clocksource->shift);\n\t}\n\n\t \n\tif (!clocksource_is_suspend(cs) && suspend_clocksource->disable)\n\t\tsuspend_clocksource->disable(suspend_clocksource);\n\n\treturn nsec;\n}\n\n \nvoid clocksource_suspend(void)\n{\n\tstruct clocksource *cs;\n\n\tlist_for_each_entry_reverse(cs, &clocksource_list, list)\n\t\tif (cs->suspend)\n\t\t\tcs->suspend(cs);\n}\n\n \nvoid clocksource_resume(void)\n{\n\tstruct clocksource *cs;\n\n\tlist_for_each_entry(cs, &clocksource_list, list)\n\t\tif (cs->resume)\n\t\t\tcs->resume(cs);\n\n\tclocksource_resume_watchdog();\n}\n\n \nvoid clocksource_touch_watchdog(void)\n{\n\tclocksource_resume_watchdog();\n}\n\n \nstatic u32 clocksource_max_adjustment(struct clocksource *cs)\n{\n\tu64 ret;\n\t \n\tret = (u64)cs->mult * 11;\n\tdo_div(ret,100);\n\treturn (u32)ret;\n}\n\n \nu64 clocks_calc_max_nsecs(u32 mult, u32 shift, u32 maxadj, u64 mask, u64 *max_cyc)\n{\n\tu64 max_nsecs, max_cycles;\n\n\t \n\tmax_cycles = ULLONG_MAX;\n\tdo_div(max_cycles, mult+maxadj);\n\n\t \n\tmax_cycles = min(max_cycles, mask);\n\tmax_nsecs = clocksource_cyc2ns(max_cycles, mult - maxadj, shift);\n\n\t \n\tif (max_cyc)\n\t\t*max_cyc = max_cycles;\n\n\t \n\tmax_nsecs >>= 1;\n\n\treturn max_nsecs;\n}\n\n \nstatic inline void clocksource_update_max_deferment(struct clocksource *cs)\n{\n\tcs->max_idle_ns = clocks_calc_max_nsecs(cs->mult, cs->shift,\n\t\t\t\t\t\tcs->maxadj, cs->mask,\n\t\t\t\t\t\t&cs->max_cycles);\n}\n\nstatic struct clocksource *clocksource_find_best(bool oneshot, bool skipcur)\n{\n\tstruct clocksource *cs;\n\n\tif (!finished_booting || list_empty(&clocksource_list))\n\t\treturn NULL;\n\n\t \n\tlist_for_each_entry(cs, &clocksource_list, list) {\n\t\tif (skipcur && cs == curr_clocksource)\n\t\t\tcontinue;\n\t\tif (oneshot && !(cs->flags & CLOCK_SOURCE_VALID_FOR_HRES))\n\t\t\tcontinue;\n\t\treturn cs;\n\t}\n\treturn NULL;\n}\n\nstatic void __clocksource_select(bool skipcur)\n{\n\tbool oneshot = tick_oneshot_mode_active();\n\tstruct clocksource *best, *cs;\n\n\t \n\tbest = clocksource_find_best(oneshot, skipcur);\n\tif (!best)\n\t\treturn;\n\n\tif (!strlen(override_name))\n\t\tgoto found;\n\n\t \n\tlist_for_each_entry(cs, &clocksource_list, list) {\n\t\tif (skipcur && cs == curr_clocksource)\n\t\t\tcontinue;\n\t\tif (strcmp(cs->name, override_name) != 0)\n\t\t\tcontinue;\n\t\t \n\t\tif (!(cs->flags & CLOCK_SOURCE_VALID_FOR_HRES) && oneshot) {\n\t\t\t \n\t\t\tif (cs->flags & CLOCK_SOURCE_UNSTABLE) {\n\t\t\t\tpr_warn(\"Override clocksource %s is unstable and not HRT compatible - cannot switch while in HRT/NOHZ mode\\n\",\n\t\t\t\t\tcs->name);\n\t\t\t\toverride_name[0] = 0;\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tpr_info(\"Override clocksource %s is not currently HRT compatible - deferring\\n\",\n\t\t\t\t\tcs->name);\n\t\t\t}\n\t\t} else\n\t\t\t \n\t\t\tbest = cs;\n\t\tbreak;\n\t}\n\nfound:\n\tif (curr_clocksource != best && !timekeeping_notify(best)) {\n\t\tpr_info(\"Switched to clocksource %s\\n\", best->name);\n\t\tcurr_clocksource = best;\n\t}\n}\n\n \nstatic void clocksource_select(void)\n{\n\t__clocksource_select(false);\n}\n\nstatic void clocksource_select_fallback(void)\n{\n\t__clocksource_select(true);\n}\n\n \nstatic int __init clocksource_done_booting(void)\n{\n\tmutex_lock(&clocksource_mutex);\n\tcurr_clocksource = clocksource_default_clock();\n\tfinished_booting = 1;\n\t \n\t__clocksource_watchdog_kthread();\n\tclocksource_select();\n\tmutex_unlock(&clocksource_mutex);\n\treturn 0;\n}\nfs_initcall(clocksource_done_booting);\n\n \nstatic void clocksource_enqueue(struct clocksource *cs)\n{\n\tstruct list_head *entry = &clocksource_list;\n\tstruct clocksource *tmp;\n\n\tlist_for_each_entry(tmp, &clocksource_list, list) {\n\t\t \n\t\tif (tmp->rating < cs->rating)\n\t\t\tbreak;\n\t\tentry = &tmp->list;\n\t}\n\tlist_add(&cs->list, entry);\n}\n\n \nvoid __clocksource_update_freq_scale(struct clocksource *cs, u32 scale, u32 freq)\n{\n\tu64 sec;\n\n\t \n\tif (freq) {\n\t\t \n\t\tsec = cs->mask;\n\t\tdo_div(sec, freq);\n\t\tdo_div(sec, scale);\n\t\tif (!sec)\n\t\t\tsec = 1;\n\t\telse if (sec > 600 && cs->mask > UINT_MAX)\n\t\t\tsec = 600;\n\n\t\tclocks_calc_mult_shift(&cs->mult, &cs->shift, freq,\n\t\t\t\t       NSEC_PER_SEC / scale, sec * scale);\n\t}\n\n\t \n\tif (scale && freq && !cs->uncertainty_margin) {\n\t\tcs->uncertainty_margin = NSEC_PER_SEC / (scale * freq);\n\t\tif (cs->uncertainty_margin < 2 * WATCHDOG_MAX_SKEW)\n\t\t\tcs->uncertainty_margin = 2 * WATCHDOG_MAX_SKEW;\n\t} else if (!cs->uncertainty_margin) {\n\t\tcs->uncertainty_margin = WATCHDOG_THRESHOLD;\n\t}\n\tWARN_ON_ONCE(cs->uncertainty_margin < 2 * WATCHDOG_MAX_SKEW);\n\n\t \n\tcs->maxadj = clocksource_max_adjustment(cs);\n\twhile (freq && ((cs->mult + cs->maxadj < cs->mult)\n\t\t|| (cs->mult - cs->maxadj > cs->mult))) {\n\t\tcs->mult >>= 1;\n\t\tcs->shift--;\n\t\tcs->maxadj = clocksource_max_adjustment(cs);\n\t}\n\n\t \n\tWARN_ONCE(cs->mult + cs->maxadj < cs->mult,\n\t\t\"timekeeping: Clocksource %s might overflow on 11%% adjustment\\n\",\n\t\tcs->name);\n\n\tclocksource_update_max_deferment(cs);\n\n\tpr_info(\"%s: mask: 0x%llx max_cycles: 0x%llx, max_idle_ns: %lld ns\\n\",\n\t\tcs->name, cs->mask, cs->max_cycles, cs->max_idle_ns);\n}\nEXPORT_SYMBOL_GPL(__clocksource_update_freq_scale);\n\n \nint __clocksource_register_scale(struct clocksource *cs, u32 scale, u32 freq)\n{\n\tunsigned long flags;\n\n\tclocksource_arch_init(cs);\n\n\tif (WARN_ON_ONCE((unsigned int)cs->id >= CSID_MAX))\n\t\tcs->id = CSID_GENERIC;\n\tif (cs->vdso_clock_mode < 0 ||\n\t    cs->vdso_clock_mode >= VDSO_CLOCKMODE_MAX) {\n\t\tpr_warn(\"clocksource %s registered with invalid VDSO mode %d. Disabling VDSO support.\\n\",\n\t\t\tcs->name, cs->vdso_clock_mode);\n\t\tcs->vdso_clock_mode = VDSO_CLOCKMODE_NONE;\n\t}\n\n\t \n\t__clocksource_update_freq_scale(cs, scale, freq);\n\n\t \n\tmutex_lock(&clocksource_mutex);\n\n\tclocksource_watchdog_lock(&flags);\n\tclocksource_enqueue(cs);\n\tclocksource_enqueue_watchdog(cs);\n\tclocksource_watchdog_unlock(&flags);\n\n\tclocksource_select();\n\tclocksource_select_watchdog(false);\n\t__clocksource_suspend_select(cs);\n\tmutex_unlock(&clocksource_mutex);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(__clocksource_register_scale);\n\nstatic void __clocksource_change_rating(struct clocksource *cs, int rating)\n{\n\tlist_del(&cs->list);\n\tcs->rating = rating;\n\tclocksource_enqueue(cs);\n}\n\n \nvoid clocksource_change_rating(struct clocksource *cs, int rating)\n{\n\tunsigned long flags;\n\n\tmutex_lock(&clocksource_mutex);\n\tclocksource_watchdog_lock(&flags);\n\t__clocksource_change_rating(cs, rating);\n\tclocksource_watchdog_unlock(&flags);\n\n\tclocksource_select();\n\tclocksource_select_watchdog(false);\n\tclocksource_suspend_select(false);\n\tmutex_unlock(&clocksource_mutex);\n}\nEXPORT_SYMBOL(clocksource_change_rating);\n\n \nstatic int clocksource_unbind(struct clocksource *cs)\n{\n\tunsigned long flags;\n\n\tif (clocksource_is_watchdog(cs)) {\n\t\t \n\t\tclocksource_select_watchdog(true);\n\t\tif (clocksource_is_watchdog(cs))\n\t\t\treturn -EBUSY;\n\t}\n\n\tif (cs == curr_clocksource) {\n\t\t \n\t\tclocksource_select_fallback();\n\t\tif (curr_clocksource == cs)\n\t\t\treturn -EBUSY;\n\t}\n\n\tif (clocksource_is_suspend(cs)) {\n\t\t \n\t\tclocksource_suspend_select(true);\n\t}\n\n\tclocksource_watchdog_lock(&flags);\n\tclocksource_dequeue_watchdog(cs);\n\tlist_del_init(&cs->list);\n\tclocksource_watchdog_unlock(&flags);\n\n\treturn 0;\n}\n\n \nint clocksource_unregister(struct clocksource *cs)\n{\n\tint ret = 0;\n\n\tmutex_lock(&clocksource_mutex);\n\tif (!list_empty(&cs->list))\n\t\tret = clocksource_unbind(cs);\n\tmutex_unlock(&clocksource_mutex);\n\treturn ret;\n}\nEXPORT_SYMBOL(clocksource_unregister);\n\n#ifdef CONFIG_SYSFS\n \nstatic ssize_t current_clocksource_show(struct device *dev,\n\t\t\t\t\tstruct device_attribute *attr,\n\t\t\t\t\tchar *buf)\n{\n\tssize_t count = 0;\n\n\tmutex_lock(&clocksource_mutex);\n\tcount = snprintf(buf, PAGE_SIZE, \"%s\\n\", curr_clocksource->name);\n\tmutex_unlock(&clocksource_mutex);\n\n\treturn count;\n}\n\nssize_t sysfs_get_uname(const char *buf, char *dst, size_t cnt)\n{\n\tsize_t ret = cnt;\n\n\t \n\tif (!cnt || cnt >= CS_NAME_LEN)\n\t\treturn -EINVAL;\n\n\t \n\tif (buf[cnt-1] == '\\n')\n\t\tcnt--;\n\tif (cnt > 0)\n\t\tmemcpy(dst, buf, cnt);\n\tdst[cnt] = 0;\n\treturn ret;\n}\n\n \nstatic ssize_t current_clocksource_store(struct device *dev,\n\t\t\t\t\t struct device_attribute *attr,\n\t\t\t\t\t const char *buf, size_t count)\n{\n\tssize_t ret;\n\n\tmutex_lock(&clocksource_mutex);\n\n\tret = sysfs_get_uname(buf, override_name, count);\n\tif (ret >= 0)\n\t\tclocksource_select();\n\n\tmutex_unlock(&clocksource_mutex);\n\n\treturn ret;\n}\nstatic DEVICE_ATTR_RW(current_clocksource);\n\n \nstatic ssize_t unbind_clocksource_store(struct device *dev,\n\t\t\t\t\tstruct device_attribute *attr,\n\t\t\t\t\tconst char *buf, size_t count)\n{\n\tstruct clocksource *cs;\n\tchar name[CS_NAME_LEN];\n\tssize_t ret;\n\n\tret = sysfs_get_uname(buf, name, count);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tret = -ENODEV;\n\tmutex_lock(&clocksource_mutex);\n\tlist_for_each_entry(cs, &clocksource_list, list) {\n\t\tif (strcmp(cs->name, name))\n\t\t\tcontinue;\n\t\tret = clocksource_unbind(cs);\n\t\tbreak;\n\t}\n\tmutex_unlock(&clocksource_mutex);\n\n\treturn ret ? ret : count;\n}\nstatic DEVICE_ATTR_WO(unbind_clocksource);\n\n \nstatic ssize_t available_clocksource_show(struct device *dev,\n\t\t\t\t\t  struct device_attribute *attr,\n\t\t\t\t\t  char *buf)\n{\n\tstruct clocksource *src;\n\tssize_t count = 0;\n\n\tmutex_lock(&clocksource_mutex);\n\tlist_for_each_entry(src, &clocksource_list, list) {\n\t\t \n\t\tif (!tick_oneshot_mode_active() ||\n\t\t    (src->flags & CLOCK_SOURCE_VALID_FOR_HRES))\n\t\t\tcount += snprintf(buf + count,\n\t\t\t\t  max((ssize_t)PAGE_SIZE - count, (ssize_t)0),\n\t\t\t\t  \"%s \", src->name);\n\t}\n\tmutex_unlock(&clocksource_mutex);\n\n\tcount += snprintf(buf + count,\n\t\t\t  max((ssize_t)PAGE_SIZE - count, (ssize_t)0), \"\\n\");\n\n\treturn count;\n}\nstatic DEVICE_ATTR_RO(available_clocksource);\n\nstatic struct attribute *clocksource_attrs[] = {\n\t&dev_attr_current_clocksource.attr,\n\t&dev_attr_unbind_clocksource.attr,\n\t&dev_attr_available_clocksource.attr,\n\tNULL\n};\nATTRIBUTE_GROUPS(clocksource);\n\nstatic struct bus_type clocksource_subsys = {\n\t.name = \"clocksource\",\n\t.dev_name = \"clocksource\",\n};\n\nstatic struct device device_clocksource = {\n\t.id\t= 0,\n\t.bus\t= &clocksource_subsys,\n\t.groups\t= clocksource_groups,\n};\n\nstatic int __init init_clocksource_sysfs(void)\n{\n\tint error = subsys_system_register(&clocksource_subsys, NULL);\n\n\tif (!error)\n\t\terror = device_register(&device_clocksource);\n\n\treturn error;\n}\n\ndevice_initcall(init_clocksource_sysfs);\n#endif  \n\n \nstatic int __init boot_override_clocksource(char* str)\n{\n\tmutex_lock(&clocksource_mutex);\n\tif (str)\n\t\tstrscpy(override_name, str, sizeof(override_name));\n\tmutex_unlock(&clocksource_mutex);\n\treturn 1;\n}\n\n__setup(\"clocksource=\", boot_override_clocksource);\n\n \nstatic int __init boot_override_clock(char* str)\n{\n\tif (!strcmp(str, \"pmtmr\")) {\n\t\tpr_warn(\"clock=pmtmr is deprecated - use clocksource=acpi_pm\\n\");\n\t\treturn boot_override_clocksource(\"acpi_pm\");\n\t}\n\tpr_warn(\"clock= boot option is deprecated - use clocksource=xyz\\n\");\n\treturn boot_override_clocksource(str);\n}\n\n__setup(\"clock=\", boot_override_clock);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}