{
  "module_name": "kprobes.c",
  "hash_id": "2d32a85a70849c6f47c395231f3f8d44d89a06904a7bcb0daebd4fb22369783b",
  "original_prompt": "Ingested from linux-6.6.14/kernel/kprobes.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) \"kprobes: \" fmt\n\n#include <linux/kprobes.h>\n#include <linux/hash.h>\n#include <linux/init.h>\n#include <linux/slab.h>\n#include <linux/stddef.h>\n#include <linux/export.h>\n#include <linux/moduleloader.h>\n#include <linux/kallsyms.h>\n#include <linux/freezer.h>\n#include <linux/seq_file.h>\n#include <linux/debugfs.h>\n#include <linux/sysctl.h>\n#include <linux/kdebug.h>\n#include <linux/memory.h>\n#include <linux/ftrace.h>\n#include <linux/cpu.h>\n#include <linux/jump_label.h>\n#include <linux/static_call.h>\n#include <linux/perf_event.h>\n\n#include <asm/sections.h>\n#include <asm/cacheflush.h>\n#include <asm/errno.h>\n#include <linux/uaccess.h>\n\n#define KPROBE_HASH_BITS 6\n#define KPROBE_TABLE_SIZE (1 << KPROBE_HASH_BITS)\n\n#if !defined(CONFIG_OPTPROBES) || !defined(CONFIG_SYSCTL)\n#define kprobe_sysctls_init() do { } while (0)\n#endif\n\nstatic int kprobes_initialized;\n \nstatic struct hlist_head kprobe_table[KPROBE_TABLE_SIZE];\n\n \nstatic bool kprobes_all_disarmed;\n\n \nstatic DEFINE_MUTEX(kprobe_mutex);\nstatic DEFINE_PER_CPU(struct kprobe *, kprobe_instance);\n\nkprobe_opcode_t * __weak kprobe_lookup_name(const char *name,\n\t\t\t\t\tunsigned int __unused)\n{\n\treturn ((kprobe_opcode_t *)(kallsyms_lookup_name(name)));\n}\n\n \nstatic LIST_HEAD(kprobe_blacklist);\n\n#ifdef __ARCH_WANT_KPROBES_INSN_SLOT\n \nstruct kprobe_insn_page {\n\tstruct list_head list;\n\tkprobe_opcode_t *insns;\t\t \n\tstruct kprobe_insn_cache *cache;\n\tint nused;\n\tint ngarbage;\n\tchar slot_used[];\n};\n\n#define KPROBE_INSN_PAGE_SIZE(slots)\t\t\t\\\n\t(offsetof(struct kprobe_insn_page, slot_used) +\t\\\n\t (sizeof(char) * (slots)))\n\nstatic int slots_per_page(struct kprobe_insn_cache *c)\n{\n\treturn PAGE_SIZE/(c->insn_size * sizeof(kprobe_opcode_t));\n}\n\nenum kprobe_slot_state {\n\tSLOT_CLEAN = 0,\n\tSLOT_DIRTY = 1,\n\tSLOT_USED = 2,\n};\n\nvoid __weak *alloc_insn_page(void)\n{\n\t \n\treturn module_alloc(PAGE_SIZE);\n}\n\nstatic void free_insn_page(void *page)\n{\n\tmodule_memfree(page);\n}\n\nstruct kprobe_insn_cache kprobe_insn_slots = {\n\t.mutex = __MUTEX_INITIALIZER(kprobe_insn_slots.mutex),\n\t.alloc = alloc_insn_page,\n\t.free = free_insn_page,\n\t.sym = KPROBE_INSN_PAGE_SYM,\n\t.pages = LIST_HEAD_INIT(kprobe_insn_slots.pages),\n\t.insn_size = MAX_INSN_SIZE,\n\t.nr_garbage = 0,\n};\nstatic int collect_garbage_slots(struct kprobe_insn_cache *c);\n\n \nkprobe_opcode_t *__get_insn_slot(struct kprobe_insn_cache *c)\n{\n\tstruct kprobe_insn_page *kip;\n\tkprobe_opcode_t *slot = NULL;\n\n\t \n\tmutex_lock(&c->mutex);\n retry:\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(kip, &c->pages, list) {\n\t\tif (kip->nused < slots_per_page(c)) {\n\t\t\tint i;\n\n\t\t\tfor (i = 0; i < slots_per_page(c); i++) {\n\t\t\t\tif (kip->slot_used[i] == SLOT_CLEAN) {\n\t\t\t\t\tkip->slot_used[i] = SLOT_USED;\n\t\t\t\t\tkip->nused++;\n\t\t\t\t\tslot = kip->insns + (i * c->insn_size);\n\t\t\t\t\trcu_read_unlock();\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t}\n\t\t\t \n\t\t\tkip->nused = slots_per_page(c);\n\t\t\tWARN_ON(1);\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\t \n\tif (c->nr_garbage && collect_garbage_slots(c) == 0)\n\t\tgoto retry;\n\n\t \n\tkip = kmalloc(KPROBE_INSN_PAGE_SIZE(slots_per_page(c)), GFP_KERNEL);\n\tif (!kip)\n\t\tgoto out;\n\n\tkip->insns = c->alloc();\n\tif (!kip->insns) {\n\t\tkfree(kip);\n\t\tgoto out;\n\t}\n\tINIT_LIST_HEAD(&kip->list);\n\tmemset(kip->slot_used, SLOT_CLEAN, slots_per_page(c));\n\tkip->slot_used[0] = SLOT_USED;\n\tkip->nused = 1;\n\tkip->ngarbage = 0;\n\tkip->cache = c;\n\tlist_add_rcu(&kip->list, &c->pages);\n\tslot = kip->insns;\n\n\t \n\tperf_event_ksymbol(PERF_RECORD_KSYMBOL_TYPE_OOL, (unsigned long)kip->insns,\n\t\t\t   PAGE_SIZE, false, c->sym);\nout:\n\tmutex_unlock(&c->mutex);\n\treturn slot;\n}\n\n \nstatic bool collect_one_slot(struct kprobe_insn_page *kip, int idx)\n{\n\tkip->slot_used[idx] = SLOT_CLEAN;\n\tkip->nused--;\n\tif (kip->nused == 0) {\n\t\t \n\t\tif (!list_is_singular(&kip->list)) {\n\t\t\t \n\t\t\tperf_event_ksymbol(PERF_RECORD_KSYMBOL_TYPE_OOL,\n\t\t\t\t\t   (unsigned long)kip->insns, PAGE_SIZE, true,\n\t\t\t\t\t   kip->cache->sym);\n\t\t\tlist_del_rcu(&kip->list);\n\t\t\tsynchronize_rcu();\n\t\t\tkip->cache->free(kip->insns);\n\t\t\tkfree(kip);\n\t\t}\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic int collect_garbage_slots(struct kprobe_insn_cache *c)\n{\n\tstruct kprobe_insn_page *kip, *next;\n\n\t \n\tsynchronize_rcu();\n\n\tlist_for_each_entry_safe(kip, next, &c->pages, list) {\n\t\tint i;\n\n\t\tif (kip->ngarbage == 0)\n\t\t\tcontinue;\n\t\tkip->ngarbage = 0;\t \n\t\tfor (i = 0; i < slots_per_page(c); i++) {\n\t\t\tif (kip->slot_used[i] == SLOT_DIRTY && collect_one_slot(kip, i))\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tc->nr_garbage = 0;\n\treturn 0;\n}\n\nvoid __free_insn_slot(struct kprobe_insn_cache *c,\n\t\t      kprobe_opcode_t *slot, int dirty)\n{\n\tstruct kprobe_insn_page *kip;\n\tlong idx;\n\n\tmutex_lock(&c->mutex);\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(kip, &c->pages, list) {\n\t\tidx = ((long)slot - (long)kip->insns) /\n\t\t\t(c->insn_size * sizeof(kprobe_opcode_t));\n\t\tif (idx >= 0 && idx < slots_per_page(c))\n\t\t\tgoto out;\n\t}\n\t \n\tWARN_ON(1);\n\tkip = NULL;\nout:\n\trcu_read_unlock();\n\t \n\tif (kip) {\n\t\t \n\t\tWARN_ON(kip->slot_used[idx] != SLOT_USED);\n\t\tif (dirty) {\n\t\t\tkip->slot_used[idx] = SLOT_DIRTY;\n\t\t\tkip->ngarbage++;\n\t\t\tif (++c->nr_garbage > slots_per_page(c))\n\t\t\t\tcollect_garbage_slots(c);\n\t\t} else {\n\t\t\tcollect_one_slot(kip, idx);\n\t\t}\n\t}\n\tmutex_unlock(&c->mutex);\n}\n\n \nbool __is_insn_slot_addr(struct kprobe_insn_cache *c, unsigned long addr)\n{\n\tstruct kprobe_insn_page *kip;\n\tbool ret = false;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(kip, &c->pages, list) {\n\t\tif (addr >= (unsigned long)kip->insns &&\n\t\t    addr < (unsigned long)kip->insns + PAGE_SIZE) {\n\t\t\tret = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\nint kprobe_cache_get_kallsym(struct kprobe_insn_cache *c, unsigned int *symnum,\n\t\t\t     unsigned long *value, char *type, char *sym)\n{\n\tstruct kprobe_insn_page *kip;\n\tint ret = -ERANGE;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(kip, &c->pages, list) {\n\t\tif ((*symnum)--)\n\t\t\tcontinue;\n\t\tstrscpy(sym, c->sym, KSYM_NAME_LEN);\n\t\t*type = 't';\n\t\t*value = (unsigned long)kip->insns;\n\t\tret = 0;\n\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\n#ifdef CONFIG_OPTPROBES\nvoid __weak *alloc_optinsn_page(void)\n{\n\treturn alloc_insn_page();\n}\n\nvoid __weak free_optinsn_page(void *page)\n{\n\tfree_insn_page(page);\n}\n\n \nstruct kprobe_insn_cache kprobe_optinsn_slots = {\n\t.mutex = __MUTEX_INITIALIZER(kprobe_optinsn_slots.mutex),\n\t.alloc = alloc_optinsn_page,\n\t.free = free_optinsn_page,\n\t.sym = KPROBE_OPTINSN_PAGE_SYM,\n\t.pages = LIST_HEAD_INIT(kprobe_optinsn_slots.pages),\n\t \n\t.nr_garbage = 0,\n};\n#endif\n#endif\n\n \nstatic inline void set_kprobe_instance(struct kprobe *kp)\n{\n\t__this_cpu_write(kprobe_instance, kp);\n}\n\nstatic inline void reset_kprobe_instance(void)\n{\n\t__this_cpu_write(kprobe_instance, NULL);\n}\n\n \nstruct kprobe *get_kprobe(void *addr)\n{\n\tstruct hlist_head *head;\n\tstruct kprobe *p;\n\n\thead = &kprobe_table[hash_ptr(addr, KPROBE_HASH_BITS)];\n\thlist_for_each_entry_rcu(p, head, hlist,\n\t\t\t\t lockdep_is_held(&kprobe_mutex)) {\n\t\tif (p->addr == addr)\n\t\t\treturn p;\n\t}\n\n\treturn NULL;\n}\nNOKPROBE_SYMBOL(get_kprobe);\n\nstatic int aggr_pre_handler(struct kprobe *p, struct pt_regs *regs);\n\n \nstatic inline bool kprobe_aggrprobe(struct kprobe *p)\n{\n\treturn p->pre_handler == aggr_pre_handler;\n}\n\n \nstatic inline bool kprobe_unused(struct kprobe *p)\n{\n\treturn kprobe_aggrprobe(p) && kprobe_disabled(p) &&\n\t       list_empty(&p->list);\n}\n\n \nstatic inline void copy_kprobe(struct kprobe *ap, struct kprobe *p)\n{\n\tmemcpy(&p->opcode, &ap->opcode, sizeof(kprobe_opcode_t));\n\tmemcpy(&p->ainsn, &ap->ainsn, sizeof(struct arch_specific_insn));\n}\n\n#ifdef CONFIG_OPTPROBES\n \nstatic bool kprobes_allow_optimization;\n\n \nvoid opt_pre_handler(struct kprobe *p, struct pt_regs *regs)\n{\n\tstruct kprobe *kp;\n\n\tlist_for_each_entry_rcu(kp, &p->list, list) {\n\t\tif (kp->pre_handler && likely(!kprobe_disabled(kp))) {\n\t\t\tset_kprobe_instance(kp);\n\t\t\tkp->pre_handler(kp, regs);\n\t\t}\n\t\treset_kprobe_instance();\n\t}\n}\nNOKPROBE_SYMBOL(opt_pre_handler);\n\n \nstatic void free_aggr_kprobe(struct kprobe *p)\n{\n\tstruct optimized_kprobe *op;\n\n\top = container_of(p, struct optimized_kprobe, kp);\n\tarch_remove_optimized_kprobe(op);\n\tarch_remove_kprobe(p);\n\tkfree(op);\n}\n\n \nstatic inline int kprobe_optready(struct kprobe *p)\n{\n\tstruct optimized_kprobe *op;\n\n\tif (kprobe_aggrprobe(p)) {\n\t\top = container_of(p, struct optimized_kprobe, kp);\n\t\treturn arch_prepared_optinsn(&op->optinsn);\n\t}\n\n\treturn 0;\n}\n\n \nbool kprobe_disarmed(struct kprobe *p)\n{\n\tstruct optimized_kprobe *op;\n\n\t \n\tif (!kprobe_aggrprobe(p))\n\t\treturn kprobe_disabled(p);\n\n\top = container_of(p, struct optimized_kprobe, kp);\n\n\treturn kprobe_disabled(p) && list_empty(&op->list);\n}\n\n \nstatic bool kprobe_queued(struct kprobe *p)\n{\n\tstruct optimized_kprobe *op;\n\n\tif (kprobe_aggrprobe(p)) {\n\t\top = container_of(p, struct optimized_kprobe, kp);\n\t\tif (!list_empty(&op->list))\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\n \nstatic struct kprobe *get_optimized_kprobe(kprobe_opcode_t *addr)\n{\n\tint i;\n\tstruct kprobe *p = NULL;\n\tstruct optimized_kprobe *op;\n\n\t \n\tfor (i = 1; !p && i < MAX_OPTIMIZED_LENGTH / sizeof(kprobe_opcode_t); i++)\n\t\tp = get_kprobe(addr - i);\n\n\tif (p && kprobe_optready(p)) {\n\t\top = container_of(p, struct optimized_kprobe, kp);\n\t\tif (arch_within_optimized_kprobe(op, addr))\n\t\t\treturn p;\n\t}\n\n\treturn NULL;\n}\n\n \nstatic LIST_HEAD(optimizing_list);\nstatic LIST_HEAD(unoptimizing_list);\nstatic LIST_HEAD(freeing_list);\n\nstatic void kprobe_optimizer(struct work_struct *work);\nstatic DECLARE_DELAYED_WORK(optimizing_work, kprobe_optimizer);\n#define OPTIMIZE_DELAY 5\n\n \nstatic void do_optimize_kprobes(void)\n{\n\tlockdep_assert_held(&text_mutex);\n\t \n\tlockdep_assert_cpus_held();\n\n\t \n\tif (kprobes_all_disarmed || !kprobes_allow_optimization ||\n\t    list_empty(&optimizing_list))\n\t\treturn;\n\n\tarch_optimize_kprobes(&optimizing_list);\n}\n\n \nstatic void do_unoptimize_kprobes(void)\n{\n\tstruct optimized_kprobe *op, *tmp;\n\n\tlockdep_assert_held(&text_mutex);\n\t \n\tlockdep_assert_cpus_held();\n\n\tif (!list_empty(&unoptimizing_list))\n\t\tarch_unoptimize_kprobes(&unoptimizing_list, &freeing_list);\n\n\t \n\tlist_for_each_entry_safe(op, tmp, &freeing_list, list) {\n\t\t \n\t\top->kp.flags &= ~KPROBE_FLAG_OPTIMIZED;\n\t\t \n\t\tif (kprobe_disabled(&op->kp) && !kprobe_gone(&op->kp))\n\t\t\tarch_disarm_kprobe(&op->kp);\n\t\tif (kprobe_unused(&op->kp)) {\n\t\t\t \n\t\t\thlist_del_rcu(&op->kp.hlist);\n\t\t} else\n\t\t\tlist_del_init(&op->list);\n\t}\n}\n\n \nstatic void do_free_cleaned_kprobes(void)\n{\n\tstruct optimized_kprobe *op, *tmp;\n\n\tlist_for_each_entry_safe(op, tmp, &freeing_list, list) {\n\t\tlist_del_init(&op->list);\n\t\tif (WARN_ON_ONCE(!kprobe_unused(&op->kp))) {\n\t\t\t \n\t\t\tcontinue;\n\t\t}\n\t\tfree_aggr_kprobe(&op->kp);\n\t}\n}\n\n \nstatic void kick_kprobe_optimizer(void)\n{\n\tschedule_delayed_work(&optimizing_work, OPTIMIZE_DELAY);\n}\n\n \nstatic void kprobe_optimizer(struct work_struct *work)\n{\n\tmutex_lock(&kprobe_mutex);\n\tcpus_read_lock();\n\tmutex_lock(&text_mutex);\n\n\t \n\tdo_unoptimize_kprobes();\n\n\t \n\tsynchronize_rcu_tasks();\n\n\t \n\tdo_optimize_kprobes();\n\n\t \n\tdo_free_cleaned_kprobes();\n\n\tmutex_unlock(&text_mutex);\n\tcpus_read_unlock();\n\n\t \n\tif (!list_empty(&optimizing_list) || !list_empty(&unoptimizing_list))\n\t\tkick_kprobe_optimizer();\n\n\tmutex_unlock(&kprobe_mutex);\n}\n\n \nvoid wait_for_kprobe_optimizer(void)\n{\n\tmutex_lock(&kprobe_mutex);\n\n\twhile (!list_empty(&optimizing_list) || !list_empty(&unoptimizing_list)) {\n\t\tmutex_unlock(&kprobe_mutex);\n\n\t\t \n\t\tflush_delayed_work(&optimizing_work);\n\t\t \n\t\tcpu_relax();\n\n\t\tmutex_lock(&kprobe_mutex);\n\t}\n\n\tmutex_unlock(&kprobe_mutex);\n}\n\nbool optprobe_queued_unopt(struct optimized_kprobe *op)\n{\n\tstruct optimized_kprobe *_op;\n\n\tlist_for_each_entry(_op, &unoptimizing_list, list) {\n\t\tif (op == _op)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nstatic void optimize_kprobe(struct kprobe *p)\n{\n\tstruct optimized_kprobe *op;\n\n\t \n\tif (!kprobe_optready(p) || !kprobes_allow_optimization ||\n\t    (kprobe_disabled(p) || kprobes_all_disarmed))\n\t\treturn;\n\n\t \n\tif (p->post_handler)\n\t\treturn;\n\n\top = container_of(p, struct optimized_kprobe, kp);\n\n\t \n\tif (arch_check_optimized_kprobe(op) < 0)\n\t\treturn;\n\n\t \n\tif (op->kp.flags & KPROBE_FLAG_OPTIMIZED) {\n\t\tif (optprobe_queued_unopt(op)) {\n\t\t\t \n\t\t\tlist_del_init(&op->list);\n\t\t}\n\t\treturn;\n\t}\n\top->kp.flags |= KPROBE_FLAG_OPTIMIZED;\n\n\t \n\tif (WARN_ON_ONCE(!list_empty(&op->list)))\n\t\treturn;\n\n\tlist_add(&op->list, &optimizing_list);\n\tkick_kprobe_optimizer();\n}\n\n \nstatic void force_unoptimize_kprobe(struct optimized_kprobe *op)\n{\n\tlockdep_assert_cpus_held();\n\tarch_unoptimize_kprobe(op);\n\top->kp.flags &= ~KPROBE_FLAG_OPTIMIZED;\n}\n\n \nstatic void unoptimize_kprobe(struct kprobe *p, bool force)\n{\n\tstruct optimized_kprobe *op;\n\n\tif (!kprobe_aggrprobe(p) || kprobe_disarmed(p))\n\t\treturn;  \n\n\top = container_of(p, struct optimized_kprobe, kp);\n\tif (!kprobe_optimized(p))\n\t\treturn;\n\n\tif (!list_empty(&op->list)) {\n\t\tif (optprobe_queued_unopt(op)) {\n\t\t\t \n\t\t\tif (force) {\n\t\t\t\t \n\t\t\t\tforce_unoptimize_kprobe(op);\n\t\t\t\tlist_move(&op->list, &freeing_list);\n\t\t\t}\n\t\t} else {\n\t\t\t \n\t\t\tlist_del_init(&op->list);\n\t\t\top->kp.flags &= ~KPROBE_FLAG_OPTIMIZED;\n\t\t}\n\t\treturn;\n\t}\n\n\t \n\tif (force) {\n\t\t \n\t\tforce_unoptimize_kprobe(op);\n\t} else {\n\t\tlist_add(&op->list, &unoptimizing_list);\n\t\tkick_kprobe_optimizer();\n\t}\n}\n\n \nstatic int reuse_unused_kprobe(struct kprobe *ap)\n{\n\tstruct optimized_kprobe *op;\n\n\t \n\top = container_of(ap, struct optimized_kprobe, kp);\n\tWARN_ON_ONCE(list_empty(&op->list));\n\t \n\tap->flags &= ~KPROBE_FLAG_DISABLED;\n\t \n\tif (!kprobe_optready(ap))\n\t\treturn -EINVAL;\n\n\toptimize_kprobe(ap);\n\treturn 0;\n}\n\n \nstatic void kill_optimized_kprobe(struct kprobe *p)\n{\n\tstruct optimized_kprobe *op;\n\n\top = container_of(p, struct optimized_kprobe, kp);\n\tif (!list_empty(&op->list))\n\t\t \n\t\tlist_del_init(&op->list);\n\top->kp.flags &= ~KPROBE_FLAG_OPTIMIZED;\n\n\tif (kprobe_unused(p)) {\n\t\t \n\t\tif (optprobe_queued_unopt(op))\n\t\t\tlist_move(&op->list, &freeing_list);\n\t}\n\n\t \n\tarch_remove_optimized_kprobe(op);\n}\n\nstatic inline\nvoid __prepare_optimized_kprobe(struct optimized_kprobe *op, struct kprobe *p)\n{\n\tif (!kprobe_ftrace(p))\n\t\tarch_prepare_optimized_kprobe(op, p);\n}\n\n \nstatic void prepare_optimized_kprobe(struct kprobe *p)\n{\n\tstruct optimized_kprobe *op;\n\n\top = container_of(p, struct optimized_kprobe, kp);\n\t__prepare_optimized_kprobe(op, p);\n}\n\n \nstatic struct kprobe *alloc_aggr_kprobe(struct kprobe *p)\n{\n\tstruct optimized_kprobe *op;\n\n\top = kzalloc(sizeof(struct optimized_kprobe), GFP_KERNEL);\n\tif (!op)\n\t\treturn NULL;\n\n\tINIT_LIST_HEAD(&op->list);\n\top->kp.addr = p->addr;\n\t__prepare_optimized_kprobe(op, p);\n\n\treturn &op->kp;\n}\n\nstatic void init_aggr_kprobe(struct kprobe *ap, struct kprobe *p);\n\n \nstatic void try_to_optimize_kprobe(struct kprobe *p)\n{\n\tstruct kprobe *ap;\n\tstruct optimized_kprobe *op;\n\n\t \n\tif (kprobe_ftrace(p))\n\t\treturn;\n\n\t \n\tcpus_read_lock();\n\tjump_label_lock();\n\tmutex_lock(&text_mutex);\n\n\tap = alloc_aggr_kprobe(p);\n\tif (!ap)\n\t\tgoto out;\n\n\top = container_of(ap, struct optimized_kprobe, kp);\n\tif (!arch_prepared_optinsn(&op->optinsn)) {\n\t\t \n\t\tarch_remove_optimized_kprobe(op);\n\t\tkfree(op);\n\t\tgoto out;\n\t}\n\n\tinit_aggr_kprobe(ap, p);\n\toptimize_kprobe(ap);\t \n\nout:\n\tmutex_unlock(&text_mutex);\n\tjump_label_unlock();\n\tcpus_read_unlock();\n}\n\nstatic void optimize_all_kprobes(void)\n{\n\tstruct hlist_head *head;\n\tstruct kprobe *p;\n\tunsigned int i;\n\n\tmutex_lock(&kprobe_mutex);\n\t \n\tif (kprobes_allow_optimization)\n\t\tgoto out;\n\n\tcpus_read_lock();\n\tkprobes_allow_optimization = true;\n\tfor (i = 0; i < KPROBE_TABLE_SIZE; i++) {\n\t\thead = &kprobe_table[i];\n\t\thlist_for_each_entry(p, head, hlist)\n\t\t\tif (!kprobe_disabled(p))\n\t\t\t\toptimize_kprobe(p);\n\t}\n\tcpus_read_unlock();\n\tpr_info(\"kprobe jump-optimization is enabled. All kprobes are optimized if possible.\\n\");\nout:\n\tmutex_unlock(&kprobe_mutex);\n}\n\n#ifdef CONFIG_SYSCTL\nstatic void unoptimize_all_kprobes(void)\n{\n\tstruct hlist_head *head;\n\tstruct kprobe *p;\n\tunsigned int i;\n\n\tmutex_lock(&kprobe_mutex);\n\t \n\tif (!kprobes_allow_optimization) {\n\t\tmutex_unlock(&kprobe_mutex);\n\t\treturn;\n\t}\n\n\tcpus_read_lock();\n\tkprobes_allow_optimization = false;\n\tfor (i = 0; i < KPROBE_TABLE_SIZE; i++) {\n\t\thead = &kprobe_table[i];\n\t\thlist_for_each_entry(p, head, hlist) {\n\t\t\tif (!kprobe_disabled(p))\n\t\t\t\tunoptimize_kprobe(p, false);\n\t\t}\n\t}\n\tcpus_read_unlock();\n\tmutex_unlock(&kprobe_mutex);\n\n\t \n\twait_for_kprobe_optimizer();\n\tpr_info(\"kprobe jump-optimization is disabled. All kprobes are based on software breakpoint.\\n\");\n}\n\nstatic DEFINE_MUTEX(kprobe_sysctl_mutex);\nstatic int sysctl_kprobes_optimization;\nstatic int proc_kprobes_optimization_handler(struct ctl_table *table,\n\t\t\t\t\t     int write, void *buffer,\n\t\t\t\t\t     size_t *length, loff_t *ppos)\n{\n\tint ret;\n\n\tmutex_lock(&kprobe_sysctl_mutex);\n\tsysctl_kprobes_optimization = kprobes_allow_optimization ? 1 : 0;\n\tret = proc_dointvec_minmax(table, write, buffer, length, ppos);\n\n\tif (sysctl_kprobes_optimization)\n\t\toptimize_all_kprobes();\n\telse\n\t\tunoptimize_all_kprobes();\n\tmutex_unlock(&kprobe_sysctl_mutex);\n\n\treturn ret;\n}\n\nstatic struct ctl_table kprobe_sysctls[] = {\n\t{\n\t\t.procname\t= \"kprobes-optimization\",\n\t\t.data\t\t= &sysctl_kprobes_optimization,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_kprobes_optimization_handler,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_ONE,\n\t},\n\t{}\n};\n\nstatic void __init kprobe_sysctls_init(void)\n{\n\tregister_sysctl_init(\"debug\", kprobe_sysctls);\n}\n#endif  \n\n \nstatic void __arm_kprobe(struct kprobe *p)\n{\n\tstruct kprobe *_p;\n\n\tlockdep_assert_held(&text_mutex);\n\n\t \n\t_p = get_optimized_kprobe(p->addr);\n\tif (unlikely(_p))\n\t\t \n\t\tunoptimize_kprobe(_p, true);\n\n\tarch_arm_kprobe(p);\n\toptimize_kprobe(p);\t \n}\n\n \nstatic void __disarm_kprobe(struct kprobe *p, bool reopt)\n{\n\tstruct kprobe *_p;\n\n\tlockdep_assert_held(&text_mutex);\n\n\t \n\tunoptimize_kprobe(p, kprobes_all_disarmed);\n\n\tif (!kprobe_queued(p)) {\n\t\tarch_disarm_kprobe(p);\n\t\t \n\t\t_p = get_optimized_kprobe(p->addr);\n\t\tif (unlikely(_p) && reopt)\n\t\t\toptimize_kprobe(_p);\n\t}\n\t \n}\n\n#else  \n\n#define optimize_kprobe(p)\t\t\tdo {} while (0)\n#define unoptimize_kprobe(p, f)\t\t\tdo {} while (0)\n#define kill_optimized_kprobe(p)\t\tdo {} while (0)\n#define prepare_optimized_kprobe(p)\t\tdo {} while (0)\n#define try_to_optimize_kprobe(p)\t\tdo {} while (0)\n#define __arm_kprobe(p)\t\t\t\tarch_arm_kprobe(p)\n#define __disarm_kprobe(p, o)\t\t\tarch_disarm_kprobe(p)\n#define kprobe_disarmed(p)\t\t\tkprobe_disabled(p)\n#define wait_for_kprobe_optimizer()\t\tdo {} while (0)\n\nstatic int reuse_unused_kprobe(struct kprobe *ap)\n{\n\t \n\tWARN_ON_ONCE(1);\n\treturn -EINVAL;\n}\n\nstatic void free_aggr_kprobe(struct kprobe *p)\n{\n\tarch_remove_kprobe(p);\n\tkfree(p);\n}\n\nstatic struct kprobe *alloc_aggr_kprobe(struct kprobe *p)\n{\n\treturn kzalloc(sizeof(struct kprobe), GFP_KERNEL);\n}\n#endif  \n\n#ifdef CONFIG_KPROBES_ON_FTRACE\nstatic struct ftrace_ops kprobe_ftrace_ops __read_mostly = {\n\t.func = kprobe_ftrace_handler,\n\t.flags = FTRACE_OPS_FL_SAVE_REGS,\n};\n\nstatic struct ftrace_ops kprobe_ipmodify_ops __read_mostly = {\n\t.func = kprobe_ftrace_handler,\n\t.flags = FTRACE_OPS_FL_SAVE_REGS | FTRACE_OPS_FL_IPMODIFY,\n};\n\nstatic int kprobe_ipmodify_enabled;\nstatic int kprobe_ftrace_enabled;\n\nstatic int __arm_kprobe_ftrace(struct kprobe *p, struct ftrace_ops *ops,\n\t\t\t       int *cnt)\n{\n\tint ret;\n\n\tlockdep_assert_held(&kprobe_mutex);\n\n\tret = ftrace_set_filter_ip(ops, (unsigned long)p->addr, 0, 0);\n\tif (WARN_ONCE(ret < 0, \"Failed to arm kprobe-ftrace at %pS (error %d)\\n\", p->addr, ret))\n\t\treturn ret;\n\n\tif (*cnt == 0) {\n\t\tret = register_ftrace_function(ops);\n\t\tif (WARN(ret < 0, \"Failed to register kprobe-ftrace (error %d)\\n\", ret))\n\t\t\tgoto err_ftrace;\n\t}\n\n\t(*cnt)++;\n\treturn ret;\n\nerr_ftrace:\n\t \n\tftrace_set_filter_ip(ops, (unsigned long)p->addr, 1, 0);\n\treturn ret;\n}\n\nstatic int arm_kprobe_ftrace(struct kprobe *p)\n{\n\tbool ipmodify = (p->post_handler != NULL);\n\n\treturn __arm_kprobe_ftrace(p,\n\t\tipmodify ? &kprobe_ipmodify_ops : &kprobe_ftrace_ops,\n\t\tipmodify ? &kprobe_ipmodify_enabled : &kprobe_ftrace_enabled);\n}\n\nstatic int __disarm_kprobe_ftrace(struct kprobe *p, struct ftrace_ops *ops,\n\t\t\t\t  int *cnt)\n{\n\tint ret;\n\n\tlockdep_assert_held(&kprobe_mutex);\n\n\tif (*cnt == 1) {\n\t\tret = unregister_ftrace_function(ops);\n\t\tif (WARN(ret < 0, \"Failed to unregister kprobe-ftrace (error %d)\\n\", ret))\n\t\t\treturn ret;\n\t}\n\n\t(*cnt)--;\n\n\tret = ftrace_set_filter_ip(ops, (unsigned long)p->addr, 1, 0);\n\tWARN_ONCE(ret < 0, \"Failed to disarm kprobe-ftrace at %pS (error %d)\\n\",\n\t\t  p->addr, ret);\n\treturn ret;\n}\n\nstatic int disarm_kprobe_ftrace(struct kprobe *p)\n{\n\tbool ipmodify = (p->post_handler != NULL);\n\n\treturn __disarm_kprobe_ftrace(p,\n\t\tipmodify ? &kprobe_ipmodify_ops : &kprobe_ftrace_ops,\n\t\tipmodify ? &kprobe_ipmodify_enabled : &kprobe_ftrace_enabled);\n}\n#else\t \nstatic inline int arm_kprobe_ftrace(struct kprobe *p)\n{\n\treturn -ENODEV;\n}\n\nstatic inline int disarm_kprobe_ftrace(struct kprobe *p)\n{\n\treturn -ENODEV;\n}\n#endif\n\nstatic int prepare_kprobe(struct kprobe *p)\n{\n\t \n\tif (kprobe_ftrace(p))\n\t\treturn arch_prepare_kprobe_ftrace(p);\n\n\treturn arch_prepare_kprobe(p);\n}\n\nstatic int arm_kprobe(struct kprobe *kp)\n{\n\tif (unlikely(kprobe_ftrace(kp)))\n\t\treturn arm_kprobe_ftrace(kp);\n\n\tcpus_read_lock();\n\tmutex_lock(&text_mutex);\n\t__arm_kprobe(kp);\n\tmutex_unlock(&text_mutex);\n\tcpus_read_unlock();\n\n\treturn 0;\n}\n\nstatic int disarm_kprobe(struct kprobe *kp, bool reopt)\n{\n\tif (unlikely(kprobe_ftrace(kp)))\n\t\treturn disarm_kprobe_ftrace(kp);\n\n\tcpus_read_lock();\n\tmutex_lock(&text_mutex);\n\t__disarm_kprobe(kp, reopt);\n\tmutex_unlock(&text_mutex);\n\tcpus_read_unlock();\n\n\treturn 0;\n}\n\n \nstatic int aggr_pre_handler(struct kprobe *p, struct pt_regs *regs)\n{\n\tstruct kprobe *kp;\n\n\tlist_for_each_entry_rcu(kp, &p->list, list) {\n\t\tif (kp->pre_handler && likely(!kprobe_disabled(kp))) {\n\t\t\tset_kprobe_instance(kp);\n\t\t\tif (kp->pre_handler(kp, regs))\n\t\t\t\treturn 1;\n\t\t}\n\t\treset_kprobe_instance();\n\t}\n\treturn 0;\n}\nNOKPROBE_SYMBOL(aggr_pre_handler);\n\nstatic void aggr_post_handler(struct kprobe *p, struct pt_regs *regs,\n\t\t\t      unsigned long flags)\n{\n\tstruct kprobe *kp;\n\n\tlist_for_each_entry_rcu(kp, &p->list, list) {\n\t\tif (kp->post_handler && likely(!kprobe_disabled(kp))) {\n\t\t\tset_kprobe_instance(kp);\n\t\t\tkp->post_handler(kp, regs, flags);\n\t\t\treset_kprobe_instance();\n\t\t}\n\t}\n}\nNOKPROBE_SYMBOL(aggr_post_handler);\n\n \nvoid kprobes_inc_nmissed_count(struct kprobe *p)\n{\n\tstruct kprobe *kp;\n\n\tif (!kprobe_aggrprobe(p)) {\n\t\tp->nmissed++;\n\t} else {\n\t\tlist_for_each_entry_rcu(kp, &p->list, list)\n\t\t\tkp->nmissed++;\n\t}\n}\nNOKPROBE_SYMBOL(kprobes_inc_nmissed_count);\n\nstatic struct kprobe kprobe_busy = {\n\t.addr = (void *) get_kprobe,\n};\n\nvoid kprobe_busy_begin(void)\n{\n\tstruct kprobe_ctlblk *kcb;\n\n\tpreempt_disable();\n\t__this_cpu_write(current_kprobe, &kprobe_busy);\n\tkcb = get_kprobe_ctlblk();\n\tkcb->kprobe_status = KPROBE_HIT_ACTIVE;\n}\n\nvoid kprobe_busy_end(void)\n{\n\t__this_cpu_write(current_kprobe, NULL);\n\tpreempt_enable();\n}\n\n \nstatic int add_new_kprobe(struct kprobe *ap, struct kprobe *p)\n{\n\tif (p->post_handler)\n\t\tunoptimize_kprobe(ap, true);\t \n\n\tlist_add_rcu(&p->list, &ap->list);\n\tif (p->post_handler && !ap->post_handler)\n\t\tap->post_handler = aggr_post_handler;\n\n\treturn 0;\n}\n\n \nstatic void init_aggr_kprobe(struct kprobe *ap, struct kprobe *p)\n{\n\t \n\tcopy_kprobe(p, ap);\n\tflush_insn_slot(ap);\n\tap->addr = p->addr;\n\tap->flags = p->flags & ~KPROBE_FLAG_OPTIMIZED;\n\tap->pre_handler = aggr_pre_handler;\n\t \n\tif (p->post_handler && !kprobe_gone(p))\n\t\tap->post_handler = aggr_post_handler;\n\n\tINIT_LIST_HEAD(&ap->list);\n\tINIT_HLIST_NODE(&ap->hlist);\n\n\tlist_add_rcu(&p->list, &ap->list);\n\thlist_replace_rcu(&p->hlist, &ap->hlist);\n}\n\n \nstatic int register_aggr_kprobe(struct kprobe *orig_p, struct kprobe *p)\n{\n\tint ret = 0;\n\tstruct kprobe *ap = orig_p;\n\n\tcpus_read_lock();\n\n\t \n\tjump_label_lock();\n\tmutex_lock(&text_mutex);\n\n\tif (!kprobe_aggrprobe(orig_p)) {\n\t\t \n\t\tap = alloc_aggr_kprobe(orig_p);\n\t\tif (!ap) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tinit_aggr_kprobe(ap, orig_p);\n\t} else if (kprobe_unused(ap)) {\n\t\t \n\t\tret = reuse_unused_kprobe(ap);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tif (kprobe_gone(ap)) {\n\t\t \n\t\tret = arch_prepare_kprobe(ap);\n\t\tif (ret)\n\t\t\t \n\t\t\tgoto out;\n\n\t\t \n\t\tprepare_optimized_kprobe(ap);\n\n\t\t \n\t\tap->flags = (ap->flags & ~KPROBE_FLAG_GONE)\n\t\t\t    | KPROBE_FLAG_DISABLED;\n\t}\n\n\t \n\tcopy_kprobe(ap, p);\n\tret = add_new_kprobe(ap, p);\n\nout:\n\tmutex_unlock(&text_mutex);\n\tjump_label_unlock();\n\tcpus_read_unlock();\n\n\tif (ret == 0 && kprobe_disabled(ap) && !kprobe_disabled(p)) {\n\t\tap->flags &= ~KPROBE_FLAG_DISABLED;\n\t\tif (!kprobes_all_disarmed) {\n\t\t\t \n\t\t\tret = arm_kprobe(ap);\n\t\t\tif (ret) {\n\t\t\t\tap->flags |= KPROBE_FLAG_DISABLED;\n\t\t\t\tlist_del_rcu(&p->list);\n\t\t\t\tsynchronize_rcu();\n\t\t\t}\n\t\t}\n\t}\n\treturn ret;\n}\n\nbool __weak arch_within_kprobe_blacklist(unsigned long addr)\n{\n\t \n\treturn addr >= (unsigned long)__kprobes_text_start &&\n\t       addr < (unsigned long)__kprobes_text_end;\n}\n\nstatic bool __within_kprobe_blacklist(unsigned long addr)\n{\n\tstruct kprobe_blacklist_entry *ent;\n\n\tif (arch_within_kprobe_blacklist(addr))\n\t\treturn true;\n\t \n\tlist_for_each_entry(ent, &kprobe_blacklist, list) {\n\t\tif (addr >= ent->start_addr && addr < ent->end_addr)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nbool within_kprobe_blacklist(unsigned long addr)\n{\n\tchar symname[KSYM_NAME_LEN], *p;\n\n\tif (__within_kprobe_blacklist(addr))\n\t\treturn true;\n\n\t \n\tif (!lookup_symbol_name(addr, symname)) {\n\t\tp = strchr(symname, '.');\n\t\tif (!p)\n\t\t\treturn false;\n\t\t*p = '\\0';\n\t\taddr = (unsigned long)kprobe_lookup_name(symname, 0);\n\t\tif (addr)\n\t\t\treturn __within_kprobe_blacklist(addr);\n\t}\n\treturn false;\n}\n\n \nkprobe_opcode_t *__weak arch_adjust_kprobe_addr(unsigned long addr,\n\t\t\t\t\t\tunsigned long offset,\n\t\t\t\t\t\tbool *on_func_entry)\n{\n\t*on_func_entry = !offset;\n\treturn (kprobe_opcode_t *)(addr + offset);\n}\n\n \nstatic kprobe_opcode_t *\n_kprobe_addr(kprobe_opcode_t *addr, const char *symbol_name,\n\t     unsigned long offset, bool *on_func_entry)\n{\n\tif ((symbol_name && addr) || (!symbol_name && !addr))\n\t\tgoto invalid;\n\n\tif (symbol_name) {\n\t\t \n\t\taddr = kprobe_lookup_name(symbol_name, offset);\n\t\tif (!addr)\n\t\t\treturn ERR_PTR(-ENOENT);\n\t}\n\n\t \n\taddr = (void *)addr + offset;\n\tif (!kallsyms_lookup_size_offset((unsigned long)addr, NULL, &offset))\n\t\treturn ERR_PTR(-ENOENT);\n\taddr = (void *)addr - offset;\n\n\t \n\taddr = arch_adjust_kprobe_addr((unsigned long)addr, offset, on_func_entry);\n\tif (addr)\n\t\treturn addr;\n\ninvalid:\n\treturn ERR_PTR(-EINVAL);\n}\n\nstatic kprobe_opcode_t *kprobe_addr(struct kprobe *p)\n{\n\tbool on_func_entry;\n\treturn _kprobe_addr(p->addr, p->symbol_name, p->offset, &on_func_entry);\n}\n\n \nstatic struct kprobe *__get_valid_kprobe(struct kprobe *p)\n{\n\tstruct kprobe *ap, *list_p;\n\n\tlockdep_assert_held(&kprobe_mutex);\n\n\tap = get_kprobe(p->addr);\n\tif (unlikely(!ap))\n\t\treturn NULL;\n\n\tif (p != ap) {\n\t\tlist_for_each_entry(list_p, &ap->list, list)\n\t\t\tif (list_p == p)\n\t\t\t \n\t\t\t\tgoto valid;\n\t\treturn NULL;\n\t}\nvalid:\n\treturn ap;\n}\n\n \nstatic inline int warn_kprobe_rereg(struct kprobe *p)\n{\n\tint ret = 0;\n\n\tmutex_lock(&kprobe_mutex);\n\tif (WARN_ON_ONCE(__get_valid_kprobe(p)))\n\t\tret = -EINVAL;\n\tmutex_unlock(&kprobe_mutex);\n\n\treturn ret;\n}\n\nstatic int check_ftrace_location(struct kprobe *p)\n{\n\tunsigned long addr = (unsigned long)p->addr;\n\n\tif (ftrace_location(addr) == addr) {\n#ifdef CONFIG_KPROBES_ON_FTRACE\n\t\tp->flags |= KPROBE_FLAG_FTRACE;\n#else\t \n\t\treturn -EINVAL;\n#endif\n\t}\n\treturn 0;\n}\n\nstatic bool is_cfi_preamble_symbol(unsigned long addr)\n{\n\tchar symbuf[KSYM_NAME_LEN];\n\n\tif (lookup_symbol_name(addr, symbuf))\n\t\treturn false;\n\n\treturn str_has_prefix(\"__cfi_\", symbuf) ||\n\t\tstr_has_prefix(\"__pfx_\", symbuf);\n}\n\nstatic int check_kprobe_address_safe(struct kprobe *p,\n\t\t\t\t     struct module **probed_mod)\n{\n\tint ret;\n\n\tret = check_ftrace_location(p);\n\tif (ret)\n\t\treturn ret;\n\tjump_label_lock();\n\tpreempt_disable();\n\n\t \n\tif (!(core_kernel_text((unsigned long) p->addr) ||\n\t    is_module_text_address((unsigned long) p->addr)) ||\n\t    in_gate_area_no_mm((unsigned long) p->addr) ||\n\t    within_kprobe_blacklist((unsigned long) p->addr) ||\n\t    jump_label_text_reserved(p->addr, p->addr) ||\n\t    static_call_text_reserved(p->addr, p->addr) ||\n\t    find_bug((unsigned long)p->addr) ||\n\t    is_cfi_preamble_symbol((unsigned long)p->addr)) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t \n\t*probed_mod = __module_text_address((unsigned long) p->addr);\n\tif (*probed_mod) {\n\t\t \n\t\tif (unlikely(!try_module_get(*probed_mod))) {\n\t\t\tret = -ENOENT;\n\t\t\tgoto out;\n\t\t}\n\n\t\t \n\t\tif (within_module_init((unsigned long)p->addr, *probed_mod) &&\n\t\t    (*probed_mod)->state != MODULE_STATE_COMING) {\n\t\t\tmodule_put(*probed_mod);\n\t\t\t*probed_mod = NULL;\n\t\t\tret = -ENOENT;\n\t\t}\n\t}\nout:\n\tpreempt_enable();\n\tjump_label_unlock();\n\n\treturn ret;\n}\n\nint register_kprobe(struct kprobe *p)\n{\n\tint ret;\n\tstruct kprobe *old_p;\n\tstruct module *probed_mod;\n\tkprobe_opcode_t *addr;\n\tbool on_func_entry;\n\n\t \n\taddr = _kprobe_addr(p->addr, p->symbol_name, p->offset, &on_func_entry);\n\tif (IS_ERR(addr))\n\t\treturn PTR_ERR(addr);\n\tp->addr = addr;\n\n\tret = warn_kprobe_rereg(p);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tp->flags &= KPROBE_FLAG_DISABLED;\n\tp->nmissed = 0;\n\tINIT_LIST_HEAD(&p->list);\n\n\tret = check_kprobe_address_safe(p, &probed_mod);\n\tif (ret)\n\t\treturn ret;\n\n\tmutex_lock(&kprobe_mutex);\n\n\tif (on_func_entry)\n\t\tp->flags |= KPROBE_FLAG_ON_FUNC_ENTRY;\n\n\told_p = get_kprobe(p->addr);\n\tif (old_p) {\n\t\t \n\t\tret = register_aggr_kprobe(old_p, p);\n\t\tgoto out;\n\t}\n\n\tcpus_read_lock();\n\t \n\tmutex_lock(&text_mutex);\n\tret = prepare_kprobe(p);\n\tmutex_unlock(&text_mutex);\n\tcpus_read_unlock();\n\tif (ret)\n\t\tgoto out;\n\n\tINIT_HLIST_NODE(&p->hlist);\n\thlist_add_head_rcu(&p->hlist,\n\t\t       &kprobe_table[hash_ptr(p->addr, KPROBE_HASH_BITS)]);\n\n\tif (!kprobes_all_disarmed && !kprobe_disabled(p)) {\n\t\tret = arm_kprobe(p);\n\t\tif (ret) {\n\t\t\thlist_del_rcu(&p->hlist);\n\t\t\tsynchronize_rcu();\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t \n\ttry_to_optimize_kprobe(p);\nout:\n\tmutex_unlock(&kprobe_mutex);\n\n\tif (probed_mod)\n\t\tmodule_put(probed_mod);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(register_kprobe);\n\n \nstatic bool aggr_kprobe_disabled(struct kprobe *ap)\n{\n\tstruct kprobe *kp;\n\n\tlockdep_assert_held(&kprobe_mutex);\n\n\tlist_for_each_entry(kp, &ap->list, list)\n\t\tif (!kprobe_disabled(kp))\n\t\t\t \n\t\t\treturn false;\n\n\treturn true;\n}\n\nstatic struct kprobe *__disable_kprobe(struct kprobe *p)\n{\n\tstruct kprobe *orig_p;\n\tint ret;\n\n\tlockdep_assert_held(&kprobe_mutex);\n\n\t \n\torig_p = __get_valid_kprobe(p);\n\tif (unlikely(orig_p == NULL))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (!kprobe_disabled(p)) {\n\t\t \n\t\tif (p != orig_p)\n\t\t\tp->flags |= KPROBE_FLAG_DISABLED;\n\n\t\t \n\t\tif (p == orig_p || aggr_kprobe_disabled(orig_p)) {\n\t\t\t \n\t\t\tif (!kprobes_all_disarmed && !kprobe_disabled(orig_p)) {\n\t\t\t\tret = disarm_kprobe(orig_p, true);\n\t\t\t\tif (ret) {\n\t\t\t\t\tp->flags &= ~KPROBE_FLAG_DISABLED;\n\t\t\t\t\treturn ERR_PTR(ret);\n\t\t\t\t}\n\t\t\t}\n\t\t\torig_p->flags |= KPROBE_FLAG_DISABLED;\n\t\t}\n\t}\n\n\treturn orig_p;\n}\n\n \nstatic int __unregister_kprobe_top(struct kprobe *p)\n{\n\tstruct kprobe *ap, *list_p;\n\n\t \n\tap = __disable_kprobe(p);\n\tif (IS_ERR(ap))\n\t\treturn PTR_ERR(ap);\n\n\tif (ap == p)\n\t\t \n\t\tgoto disarmed;\n\n\t \n\tWARN_ON(!kprobe_aggrprobe(ap));\n\n\tif (list_is_singular(&ap->list) && kprobe_disarmed(ap))\n\t\t \n\t\tgoto disarmed;\n\telse {\n\t\t \n\t\tif (p->post_handler && !kprobe_gone(p)) {\n\t\t\tlist_for_each_entry(list_p, &ap->list, list) {\n\t\t\t\tif ((list_p != p) && (list_p->post_handler))\n\t\t\t\t\tgoto noclean;\n\t\t\t}\n\t\t\t \n\t\t\tif (!kprobe_ftrace(ap))\n\t\t\t\tap->post_handler = NULL;\n\t\t}\nnoclean:\n\t\t \n\t\tlist_del_rcu(&p->list);\n\t\tif (!kprobe_disabled(ap) && !kprobes_all_disarmed)\n\t\t\t \n\t\t\toptimize_kprobe(ap);\n\t}\n\treturn 0;\n\ndisarmed:\n\thlist_del_rcu(&ap->hlist);\n\treturn 0;\n}\n\nstatic void __unregister_kprobe_bottom(struct kprobe *p)\n{\n\tstruct kprobe *ap;\n\n\tif (list_empty(&p->list))\n\t\t \n\t\tarch_remove_kprobe(p);\n\telse if (list_is_singular(&p->list)) {\n\t\t \n\t\tap = list_entry(p->list.next, struct kprobe, list);\n\t\tlist_del(&p->list);\n\t\tfree_aggr_kprobe(ap);\n\t}\n\t \n}\n\nint register_kprobes(struct kprobe **kps, int num)\n{\n\tint i, ret = 0;\n\n\tif (num <= 0)\n\t\treturn -EINVAL;\n\tfor (i = 0; i < num; i++) {\n\t\tret = register_kprobe(kps[i]);\n\t\tif (ret < 0) {\n\t\t\tif (i > 0)\n\t\t\t\tunregister_kprobes(kps, i);\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(register_kprobes);\n\nvoid unregister_kprobe(struct kprobe *p)\n{\n\tunregister_kprobes(&p, 1);\n}\nEXPORT_SYMBOL_GPL(unregister_kprobe);\n\nvoid unregister_kprobes(struct kprobe **kps, int num)\n{\n\tint i;\n\n\tif (num <= 0)\n\t\treturn;\n\tmutex_lock(&kprobe_mutex);\n\tfor (i = 0; i < num; i++)\n\t\tif (__unregister_kprobe_top(kps[i]) < 0)\n\t\t\tkps[i]->addr = NULL;\n\tmutex_unlock(&kprobe_mutex);\n\n\tsynchronize_rcu();\n\tfor (i = 0; i < num; i++)\n\t\tif (kps[i]->addr)\n\t\t\t__unregister_kprobe_bottom(kps[i]);\n}\nEXPORT_SYMBOL_GPL(unregister_kprobes);\n\nint __weak kprobe_exceptions_notify(struct notifier_block *self,\n\t\t\t\t\tunsigned long val, void *data)\n{\n\treturn NOTIFY_DONE;\n}\nNOKPROBE_SYMBOL(kprobe_exceptions_notify);\n\nstatic struct notifier_block kprobe_exceptions_nb = {\n\t.notifier_call = kprobe_exceptions_notify,\n\t.priority = 0x7fffffff  \n};\n\n#ifdef CONFIG_KRETPROBES\n\n#if !defined(CONFIG_KRETPROBE_ON_RETHOOK)\nstatic void free_rp_inst_rcu(struct rcu_head *head)\n{\n\tstruct kretprobe_instance *ri = container_of(head, struct kretprobe_instance, rcu);\n\n\tif (refcount_dec_and_test(&ri->rph->ref))\n\t\tkfree(ri->rph);\n\tkfree(ri);\n}\nNOKPROBE_SYMBOL(free_rp_inst_rcu);\n\nstatic void recycle_rp_inst(struct kretprobe_instance *ri)\n{\n\tstruct kretprobe *rp = get_kretprobe(ri);\n\n\tif (likely(rp))\n\t\tfreelist_add(&ri->freelist, &rp->freelist);\n\telse\n\t\tcall_rcu(&ri->rcu, free_rp_inst_rcu);\n}\nNOKPROBE_SYMBOL(recycle_rp_inst);\n\n \nvoid kprobe_flush_task(struct task_struct *tk)\n{\n\tstruct kretprobe_instance *ri;\n\tstruct llist_node *node;\n\n\t \n\tif (unlikely(!kprobes_initialized))\n\t\treturn;\n\n\tkprobe_busy_begin();\n\n\tnode = __llist_del_all(&tk->kretprobe_instances);\n\twhile (node) {\n\t\tri = container_of(node, struct kretprobe_instance, llist);\n\t\tnode = node->next;\n\n\t\trecycle_rp_inst(ri);\n\t}\n\n\tkprobe_busy_end();\n}\nNOKPROBE_SYMBOL(kprobe_flush_task);\n\nstatic inline void free_rp_inst(struct kretprobe *rp)\n{\n\tstruct kretprobe_instance *ri;\n\tstruct freelist_node *node;\n\tint count = 0;\n\n\tnode = rp->freelist.head;\n\twhile (node) {\n\t\tri = container_of(node, struct kretprobe_instance, freelist);\n\t\tnode = node->next;\n\n\t\tkfree(ri);\n\t\tcount++;\n\t}\n\n\tif (refcount_sub_and_test(count, &rp->rph->ref)) {\n\t\tkfree(rp->rph);\n\t\trp->rph = NULL;\n\t}\n}\n\n \nstatic kprobe_opcode_t *__kretprobe_find_ret_addr(struct task_struct *tsk,\n\t\t\t\t\t\t  struct llist_node **cur)\n{\n\tstruct kretprobe_instance *ri = NULL;\n\tstruct llist_node *node = *cur;\n\n\tif (!node)\n\t\tnode = tsk->kretprobe_instances.first;\n\telse\n\t\tnode = node->next;\n\n\twhile (node) {\n\t\tri = container_of(node, struct kretprobe_instance, llist);\n\t\tif (ri->ret_addr != kretprobe_trampoline_addr()) {\n\t\t\t*cur = node;\n\t\t\treturn ri->ret_addr;\n\t\t}\n\t\tnode = node->next;\n\t}\n\treturn NULL;\n}\nNOKPROBE_SYMBOL(__kretprobe_find_ret_addr);\n\n \nunsigned long kretprobe_find_ret_addr(struct task_struct *tsk, void *fp,\n\t\t\t\t      struct llist_node **cur)\n{\n\tstruct kretprobe_instance *ri = NULL;\n\tkprobe_opcode_t *ret;\n\n\tif (WARN_ON_ONCE(!cur))\n\t\treturn 0;\n\n\tdo {\n\t\tret = __kretprobe_find_ret_addr(tsk, cur);\n\t\tif (!ret)\n\t\t\tbreak;\n\t\tri = container_of(*cur, struct kretprobe_instance, llist);\n\t} while (ri->fp != fp);\n\n\treturn (unsigned long)ret;\n}\nNOKPROBE_SYMBOL(kretprobe_find_ret_addr);\n\nvoid __weak arch_kretprobe_fixup_return(struct pt_regs *regs,\n\t\t\t\t\tkprobe_opcode_t *correct_ret_addr)\n{\n\t \n}\n\nunsigned long __kretprobe_trampoline_handler(struct pt_regs *regs,\n\t\t\t\t\t     void *frame_pointer)\n{\n\tstruct kretprobe_instance *ri = NULL;\n\tstruct llist_node *first, *node = NULL;\n\tkprobe_opcode_t *correct_ret_addr;\n\tstruct kretprobe *rp;\n\n\t \n\tcorrect_ret_addr = __kretprobe_find_ret_addr(current, &node);\n\tif (!correct_ret_addr) {\n\t\tpr_err(\"kretprobe: Return address not found, not execute handler. Maybe there is a bug in the kernel.\\n\");\n\t\tBUG_ON(1);\n\t}\n\n\t \n\tinstruction_pointer_set(regs, (unsigned long)correct_ret_addr);\n\n\t \n\tfirst = current->kretprobe_instances.first;\n\twhile (first) {\n\t\tri = container_of(first, struct kretprobe_instance, llist);\n\n\t\tif (WARN_ON_ONCE(ri->fp != frame_pointer))\n\t\t\tbreak;\n\n\t\trp = get_kretprobe(ri);\n\t\tif (rp && rp->handler) {\n\t\t\tstruct kprobe *prev = kprobe_running();\n\n\t\t\t__this_cpu_write(current_kprobe, &rp->kp);\n\t\t\tri->ret_addr = correct_ret_addr;\n\t\t\trp->handler(ri, regs);\n\t\t\t__this_cpu_write(current_kprobe, prev);\n\t\t}\n\t\tif (first == node)\n\t\t\tbreak;\n\n\t\tfirst = first->next;\n\t}\n\n\tarch_kretprobe_fixup_return(regs, correct_ret_addr);\n\n\t \n\tfirst = current->kretprobe_instances.first;\n\tcurrent->kretprobe_instances.first = node->next;\n\tnode->next = NULL;\n\n\t \n\twhile (first) {\n\t\tri = container_of(first, struct kretprobe_instance, llist);\n\t\tfirst = first->next;\n\n\t\trecycle_rp_inst(ri);\n\t}\n\n\treturn (unsigned long)correct_ret_addr;\n}\nNOKPROBE_SYMBOL(__kretprobe_trampoline_handler)\n\n \nstatic int pre_handler_kretprobe(struct kprobe *p, struct pt_regs *regs)\n{\n\tstruct kretprobe *rp = container_of(p, struct kretprobe, kp);\n\tstruct kretprobe_instance *ri;\n\tstruct freelist_node *fn;\n\n\tfn = freelist_try_get(&rp->freelist);\n\tif (!fn) {\n\t\trp->nmissed++;\n\t\treturn 0;\n\t}\n\n\tri = container_of(fn, struct kretprobe_instance, freelist);\n\n\tif (rp->entry_handler && rp->entry_handler(ri, regs)) {\n\t\tfreelist_add(&ri->freelist, &rp->freelist);\n\t\treturn 0;\n\t}\n\n\tarch_prepare_kretprobe(ri, regs);\n\n\t__llist_add(&ri->llist, &current->kretprobe_instances);\n\n\treturn 0;\n}\nNOKPROBE_SYMBOL(pre_handler_kretprobe);\n#else  \n \nstatic int pre_handler_kretprobe(struct kprobe *p, struct pt_regs *regs)\n{\n\tstruct kretprobe *rp = container_of(p, struct kretprobe, kp);\n\tstruct kretprobe_instance *ri;\n\tstruct rethook_node *rhn;\n\n\trhn = rethook_try_get(rp->rh);\n\tif (!rhn) {\n\t\trp->nmissed++;\n\t\treturn 0;\n\t}\n\n\tri = container_of(rhn, struct kretprobe_instance, node);\n\n\tif (rp->entry_handler && rp->entry_handler(ri, regs))\n\t\trethook_recycle(rhn);\n\telse\n\t\trethook_hook(rhn, regs, kprobe_ftrace(p));\n\n\treturn 0;\n}\nNOKPROBE_SYMBOL(pre_handler_kretprobe);\n\nstatic void kretprobe_rethook_handler(struct rethook_node *rh, void *data,\n\t\t\t\t      unsigned long ret_addr,\n\t\t\t\t      struct pt_regs *regs)\n{\n\tstruct kretprobe *rp = (struct kretprobe *)data;\n\tstruct kretprobe_instance *ri;\n\tstruct kprobe_ctlblk *kcb;\n\n\t \n\tif (WARN_ON_ONCE(!data) || !rp->handler)\n\t\treturn;\n\n\t__this_cpu_write(current_kprobe, &rp->kp);\n\tkcb = get_kprobe_ctlblk();\n\tkcb->kprobe_status = KPROBE_HIT_ACTIVE;\n\n\tri = container_of(rh, struct kretprobe_instance, node);\n\trp->handler(ri, regs);\n\n\t__this_cpu_write(current_kprobe, NULL);\n}\nNOKPROBE_SYMBOL(kretprobe_rethook_handler);\n\n#endif  \n\n \nint kprobe_on_func_entry(kprobe_opcode_t *addr, const char *sym, unsigned long offset)\n{\n\tbool on_func_entry;\n\tkprobe_opcode_t *kp_addr = _kprobe_addr(addr, sym, offset, &on_func_entry);\n\n\tif (IS_ERR(kp_addr))\n\t\treturn PTR_ERR(kp_addr);\n\n\tif (!on_func_entry)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nint register_kretprobe(struct kretprobe *rp)\n{\n\tint ret;\n\tstruct kretprobe_instance *inst;\n\tint i;\n\tvoid *addr;\n\n\tret = kprobe_on_func_entry(rp->kp.addr, rp->kp.symbol_name, rp->kp.offset);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tif (rp->kp.addr && warn_kprobe_rereg(&rp->kp))\n\t\treturn -EINVAL;\n\n\tif (kretprobe_blacklist_size) {\n\t\taddr = kprobe_addr(&rp->kp);\n\t\tif (IS_ERR(addr))\n\t\t\treturn PTR_ERR(addr);\n\n\t\tfor (i = 0; kretprobe_blacklist[i].name != NULL; i++) {\n\t\t\tif (kretprobe_blacklist[i].addr == addr)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tif (rp->data_size > KRETPROBE_MAX_DATA_SIZE)\n\t\treturn -E2BIG;\n\n\trp->kp.pre_handler = pre_handler_kretprobe;\n\trp->kp.post_handler = NULL;\n\n\t \n\tif (rp->maxactive <= 0)\n\t\trp->maxactive = max_t(unsigned int, 10, 2*num_possible_cpus());\n\n#ifdef CONFIG_KRETPROBE_ON_RETHOOK\n\trp->rh = rethook_alloc((void *)rp, kretprobe_rethook_handler);\n\tif (!rp->rh)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < rp->maxactive; i++) {\n\t\tinst = kzalloc(struct_size(inst, data, rp->data_size), GFP_KERNEL);\n\t\tif (inst == NULL) {\n\t\t\trethook_free(rp->rh);\n\t\t\trp->rh = NULL;\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\trethook_add_node(rp->rh, &inst->node);\n\t}\n\trp->nmissed = 0;\n\t \n\tret = register_kprobe(&rp->kp);\n\tif (ret != 0) {\n\t\trethook_free(rp->rh);\n\t\trp->rh = NULL;\n\t}\n#else\t \n\trp->freelist.head = NULL;\n\trp->rph = kzalloc(sizeof(struct kretprobe_holder), GFP_KERNEL);\n\tif (!rp->rph)\n\t\treturn -ENOMEM;\n\n\trcu_assign_pointer(rp->rph->rp, rp);\n\tfor (i = 0; i < rp->maxactive; i++) {\n\t\tinst = kzalloc(struct_size(inst, data, rp->data_size), GFP_KERNEL);\n\t\tif (inst == NULL) {\n\t\t\trefcount_set(&rp->rph->ref, i);\n\t\t\tfree_rp_inst(rp);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tinst->rph = rp->rph;\n\t\tfreelist_add(&inst->freelist, &rp->freelist);\n\t}\n\trefcount_set(&rp->rph->ref, i);\n\n\trp->nmissed = 0;\n\t \n\tret = register_kprobe(&rp->kp);\n\tif (ret != 0)\n\t\tfree_rp_inst(rp);\n#endif\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(register_kretprobe);\n\nint register_kretprobes(struct kretprobe **rps, int num)\n{\n\tint ret = 0, i;\n\n\tif (num <= 0)\n\t\treturn -EINVAL;\n\tfor (i = 0; i < num; i++) {\n\t\tret = register_kretprobe(rps[i]);\n\t\tif (ret < 0) {\n\t\t\tif (i > 0)\n\t\t\t\tunregister_kretprobes(rps, i);\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(register_kretprobes);\n\nvoid unregister_kretprobe(struct kretprobe *rp)\n{\n\tunregister_kretprobes(&rp, 1);\n}\nEXPORT_SYMBOL_GPL(unregister_kretprobe);\n\nvoid unregister_kretprobes(struct kretprobe **rps, int num)\n{\n\tint i;\n\n\tif (num <= 0)\n\t\treturn;\n\tmutex_lock(&kprobe_mutex);\n\tfor (i = 0; i < num; i++) {\n\t\tif (__unregister_kprobe_top(&rps[i]->kp) < 0)\n\t\t\trps[i]->kp.addr = NULL;\n#ifdef CONFIG_KRETPROBE_ON_RETHOOK\n\t\trethook_free(rps[i]->rh);\n#else\n\t\trcu_assign_pointer(rps[i]->rph->rp, NULL);\n#endif\n\t}\n\tmutex_unlock(&kprobe_mutex);\n\n\tsynchronize_rcu();\n\tfor (i = 0; i < num; i++) {\n\t\tif (rps[i]->kp.addr) {\n\t\t\t__unregister_kprobe_bottom(&rps[i]->kp);\n#ifndef CONFIG_KRETPROBE_ON_RETHOOK\n\t\t\tfree_rp_inst(rps[i]);\n#endif\n\t\t}\n\t}\n}\nEXPORT_SYMBOL_GPL(unregister_kretprobes);\n\n#else  \nint register_kretprobe(struct kretprobe *rp)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL_GPL(register_kretprobe);\n\nint register_kretprobes(struct kretprobe **rps, int num)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL_GPL(register_kretprobes);\n\nvoid unregister_kretprobe(struct kretprobe *rp)\n{\n}\nEXPORT_SYMBOL_GPL(unregister_kretprobe);\n\nvoid unregister_kretprobes(struct kretprobe **rps, int num)\n{\n}\nEXPORT_SYMBOL_GPL(unregister_kretprobes);\n\nstatic int pre_handler_kretprobe(struct kprobe *p, struct pt_regs *regs)\n{\n\treturn 0;\n}\nNOKPROBE_SYMBOL(pre_handler_kretprobe);\n\n#endif  \n\n \nstatic void kill_kprobe(struct kprobe *p)\n{\n\tstruct kprobe *kp;\n\n\tlockdep_assert_held(&kprobe_mutex);\n\n\t \n\tif (kprobe_ftrace(p) && !kprobe_disabled(p) && !kprobes_all_disarmed)\n\t\tdisarm_kprobe_ftrace(p);\n\n\tp->flags |= KPROBE_FLAG_GONE;\n\tif (kprobe_aggrprobe(p)) {\n\t\t \n\t\tlist_for_each_entry(kp, &p->list, list)\n\t\t\tkp->flags |= KPROBE_FLAG_GONE;\n\t\tp->post_handler = NULL;\n\t\tkill_optimized_kprobe(p);\n\t}\n\t \n\tarch_remove_kprobe(p);\n}\n\n \nint disable_kprobe(struct kprobe *kp)\n{\n\tint ret = 0;\n\tstruct kprobe *p;\n\n\tmutex_lock(&kprobe_mutex);\n\n\t \n\tp = __disable_kprobe(kp);\n\tif (IS_ERR(p))\n\t\tret = PTR_ERR(p);\n\n\tmutex_unlock(&kprobe_mutex);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(disable_kprobe);\n\n \nint enable_kprobe(struct kprobe *kp)\n{\n\tint ret = 0;\n\tstruct kprobe *p;\n\n\tmutex_lock(&kprobe_mutex);\n\n\t \n\tp = __get_valid_kprobe(kp);\n\tif (unlikely(p == NULL)) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (kprobe_gone(kp)) {\n\t\t \n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (p != kp)\n\t\tkp->flags &= ~KPROBE_FLAG_DISABLED;\n\n\tif (!kprobes_all_disarmed && kprobe_disabled(p)) {\n\t\tp->flags &= ~KPROBE_FLAG_DISABLED;\n\t\tret = arm_kprobe(p);\n\t\tif (ret) {\n\t\t\tp->flags |= KPROBE_FLAG_DISABLED;\n\t\t\tif (p != kp)\n\t\t\t\tkp->flags |= KPROBE_FLAG_DISABLED;\n\t\t}\n\t}\nout:\n\tmutex_unlock(&kprobe_mutex);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(enable_kprobe);\n\n \nvoid dump_kprobe(struct kprobe *kp)\n{\n\tpr_err(\"Dump kprobe:\\n.symbol_name = %s, .offset = %x, .addr = %pS\\n\",\n\t       kp->symbol_name, kp->offset, kp->addr);\n}\nNOKPROBE_SYMBOL(dump_kprobe);\n\nint kprobe_add_ksym_blacklist(unsigned long entry)\n{\n\tstruct kprobe_blacklist_entry *ent;\n\tunsigned long offset = 0, size = 0;\n\n\tif (!kernel_text_address(entry) ||\n\t    !kallsyms_lookup_size_offset(entry, &size, &offset))\n\t\treturn -EINVAL;\n\n\tent = kmalloc(sizeof(*ent), GFP_KERNEL);\n\tif (!ent)\n\t\treturn -ENOMEM;\n\tent->start_addr = entry;\n\tent->end_addr = entry + size;\n\tINIT_LIST_HEAD(&ent->list);\n\tlist_add_tail(&ent->list, &kprobe_blacklist);\n\n\treturn (int)size;\n}\n\n \nint kprobe_add_area_blacklist(unsigned long start, unsigned long end)\n{\n\tunsigned long entry;\n\tint ret = 0;\n\n\tfor (entry = start; entry < end; entry += ret) {\n\t\tret = kprobe_add_ksym_blacklist(entry);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\tif (ret == 0)\t \n\t\t\tret = 1;\n\t}\n\treturn 0;\n}\n\n \nstatic void kprobe_remove_area_blacklist(unsigned long start, unsigned long end)\n{\n\tstruct kprobe_blacklist_entry *ent, *n;\n\n\tlist_for_each_entry_safe(ent, n, &kprobe_blacklist, list) {\n\t\tif (ent->start_addr < start || ent->start_addr >= end)\n\t\t\tcontinue;\n\t\tlist_del(&ent->list);\n\t\tkfree(ent);\n\t}\n}\n\nstatic void kprobe_remove_ksym_blacklist(unsigned long entry)\n{\n\tkprobe_remove_area_blacklist(entry, entry + 1);\n}\n\nint __weak arch_kprobe_get_kallsym(unsigned int *symnum, unsigned long *value,\n\t\t\t\t   char *type, char *sym)\n{\n\treturn -ERANGE;\n}\n\nint kprobe_get_kallsym(unsigned int symnum, unsigned long *value, char *type,\n\t\t       char *sym)\n{\n#ifdef __ARCH_WANT_KPROBES_INSN_SLOT\n\tif (!kprobe_cache_get_kallsym(&kprobe_insn_slots, &symnum, value, type, sym))\n\t\treturn 0;\n#ifdef CONFIG_OPTPROBES\n\tif (!kprobe_cache_get_kallsym(&kprobe_optinsn_slots, &symnum, value, type, sym))\n\t\treturn 0;\n#endif\n#endif\n\tif (!arch_kprobe_get_kallsym(&symnum, value, type, sym))\n\t\treturn 0;\n\treturn -ERANGE;\n}\n\nint __init __weak arch_populate_kprobe_blacklist(void)\n{\n\treturn 0;\n}\n\n \nstatic int __init populate_kprobe_blacklist(unsigned long *start,\n\t\t\t\t\t     unsigned long *end)\n{\n\tunsigned long entry;\n\tunsigned long *iter;\n\tint ret;\n\n\tfor (iter = start; iter < end; iter++) {\n\t\tentry = (unsigned long)dereference_symbol_descriptor((void *)*iter);\n\t\tret = kprobe_add_ksym_blacklist(entry);\n\t\tif (ret == -EINVAL)\n\t\t\tcontinue;\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\t \n\tret = kprobe_add_area_blacklist((unsigned long)__kprobes_text_start,\n\t\t\t\t\t(unsigned long)__kprobes_text_end);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tret = kprobe_add_area_blacklist((unsigned long)__noinstr_text_start,\n\t\t\t\t\t(unsigned long)__noinstr_text_end);\n\n\treturn ret ? : arch_populate_kprobe_blacklist();\n}\n\nstatic void add_module_kprobe_blacklist(struct module *mod)\n{\n\tunsigned long start, end;\n\tint i;\n\n\tif (mod->kprobe_blacklist) {\n\t\tfor (i = 0; i < mod->num_kprobe_blacklist; i++)\n\t\t\tkprobe_add_ksym_blacklist(mod->kprobe_blacklist[i]);\n\t}\n\n\tstart = (unsigned long)mod->kprobes_text_start;\n\tif (start) {\n\t\tend = start + mod->kprobes_text_size;\n\t\tkprobe_add_area_blacklist(start, end);\n\t}\n\n\tstart = (unsigned long)mod->noinstr_text_start;\n\tif (start) {\n\t\tend = start + mod->noinstr_text_size;\n\t\tkprobe_add_area_blacklist(start, end);\n\t}\n}\n\nstatic void remove_module_kprobe_blacklist(struct module *mod)\n{\n\tunsigned long start, end;\n\tint i;\n\n\tif (mod->kprobe_blacklist) {\n\t\tfor (i = 0; i < mod->num_kprobe_blacklist; i++)\n\t\t\tkprobe_remove_ksym_blacklist(mod->kprobe_blacklist[i]);\n\t}\n\n\tstart = (unsigned long)mod->kprobes_text_start;\n\tif (start) {\n\t\tend = start + mod->kprobes_text_size;\n\t\tkprobe_remove_area_blacklist(start, end);\n\t}\n\n\tstart = (unsigned long)mod->noinstr_text_start;\n\tif (start) {\n\t\tend = start + mod->noinstr_text_size;\n\t\tkprobe_remove_area_blacklist(start, end);\n\t}\n}\n\n \nstatic int kprobes_module_callback(struct notifier_block *nb,\n\t\t\t\t   unsigned long val, void *data)\n{\n\tstruct module *mod = data;\n\tstruct hlist_head *head;\n\tstruct kprobe *p;\n\tunsigned int i;\n\tint checkcore = (val == MODULE_STATE_GOING);\n\n\tif (val == MODULE_STATE_COMING) {\n\t\tmutex_lock(&kprobe_mutex);\n\t\tadd_module_kprobe_blacklist(mod);\n\t\tmutex_unlock(&kprobe_mutex);\n\t}\n\tif (val != MODULE_STATE_GOING && val != MODULE_STATE_LIVE)\n\t\treturn NOTIFY_DONE;\n\n\t \n\tmutex_lock(&kprobe_mutex);\n\tfor (i = 0; i < KPROBE_TABLE_SIZE; i++) {\n\t\thead = &kprobe_table[i];\n\t\thlist_for_each_entry(p, head, hlist)\n\t\t\tif (within_module_init((unsigned long)p->addr, mod) ||\n\t\t\t    (checkcore &&\n\t\t\t     within_module_core((unsigned long)p->addr, mod))) {\n\t\t\t\t \n\t\t\t\tkill_kprobe(p);\n\t\t\t}\n\t}\n\tif (val == MODULE_STATE_GOING)\n\t\tremove_module_kprobe_blacklist(mod);\n\tmutex_unlock(&kprobe_mutex);\n\treturn NOTIFY_DONE;\n}\n\nstatic struct notifier_block kprobe_module_nb = {\n\t.notifier_call = kprobes_module_callback,\n\t.priority = 0\n};\n\nvoid kprobe_free_init_mem(void)\n{\n\tvoid *start = (void *)(&__init_begin);\n\tvoid *end = (void *)(&__init_end);\n\tstruct hlist_head *head;\n\tstruct kprobe *p;\n\tint i;\n\n\tmutex_lock(&kprobe_mutex);\n\n\t \n\tfor (i = 0; i < KPROBE_TABLE_SIZE; i++) {\n\t\thead = &kprobe_table[i];\n\t\thlist_for_each_entry(p, head, hlist) {\n\t\t\tif (start <= (void *)p->addr && (void *)p->addr < end)\n\t\t\t\tkill_kprobe(p);\n\t\t}\n\t}\n\n\tmutex_unlock(&kprobe_mutex);\n}\n\nstatic int __init init_kprobes(void)\n{\n\tint i, err;\n\n\t \n\t \n\tfor (i = 0; i < KPROBE_TABLE_SIZE; i++)\n\t\tINIT_HLIST_HEAD(&kprobe_table[i]);\n\n\terr = populate_kprobe_blacklist(__start_kprobe_blacklist,\n\t\t\t\t\t__stop_kprobe_blacklist);\n\tif (err)\n\t\tpr_err(\"Failed to populate blacklist (error %d), kprobes not restricted, be careful using them!\\n\", err);\n\n\tif (kretprobe_blacklist_size) {\n\t\t \n\t\tfor (i = 0; kretprobe_blacklist[i].name != NULL; i++) {\n\t\t\tkretprobe_blacklist[i].addr =\n\t\t\t\tkprobe_lookup_name(kretprobe_blacklist[i].name, 0);\n\t\t\tif (!kretprobe_blacklist[i].addr)\n\t\t\t\tpr_err(\"Failed to lookup symbol '%s' for kretprobe blacklist. Maybe the target function is removed or renamed.\\n\",\n\t\t\t\t       kretprobe_blacklist[i].name);\n\t\t}\n\t}\n\n\t \n\tkprobes_all_disarmed = false;\n\n#if defined(CONFIG_OPTPROBES) && defined(__ARCH_WANT_KPROBES_INSN_SLOT)\n\t \n\tkprobe_optinsn_slots.insn_size = MAX_OPTINSN_SIZE;\n#endif\n\n\terr = arch_init_kprobes();\n\tif (!err)\n\t\terr = register_die_notifier(&kprobe_exceptions_nb);\n\tif (!err)\n\t\terr = register_module_notifier(&kprobe_module_nb);\n\n\tkprobes_initialized = (err == 0);\n\tkprobe_sysctls_init();\n\treturn err;\n}\nearly_initcall(init_kprobes);\n\n#if defined(CONFIG_OPTPROBES)\nstatic int __init init_optprobes(void)\n{\n\t \n\toptimize_all_kprobes();\n\n\treturn 0;\n}\nsubsys_initcall(init_optprobes);\n#endif\n\n#ifdef CONFIG_DEBUG_FS\nstatic void report_probe(struct seq_file *pi, struct kprobe *p,\n\t\tconst char *sym, int offset, char *modname, struct kprobe *pp)\n{\n\tchar *kprobe_type;\n\tvoid *addr = p->addr;\n\n\tif (p->pre_handler == pre_handler_kretprobe)\n\t\tkprobe_type = \"r\";\n\telse\n\t\tkprobe_type = \"k\";\n\n\tif (!kallsyms_show_value(pi->file->f_cred))\n\t\taddr = NULL;\n\n\tif (sym)\n\t\tseq_printf(pi, \"%px  %s  %s+0x%x  %s \",\n\t\t\taddr, kprobe_type, sym, offset,\n\t\t\t(modname ? modname : \" \"));\n\telse\t \n\t\tseq_printf(pi, \"%px  %s  %pS \",\n\t\t\taddr, kprobe_type, p->addr);\n\n\tif (!pp)\n\t\tpp = p;\n\tseq_printf(pi, \"%s%s%s%s\\n\",\n\t\t(kprobe_gone(p) ? \"[GONE]\" : \"\"),\n\t\t((kprobe_disabled(p) && !kprobe_gone(p)) ?  \"[DISABLED]\" : \"\"),\n\t\t(kprobe_optimized(pp) ? \"[OPTIMIZED]\" : \"\"),\n\t\t(kprobe_ftrace(pp) ? \"[FTRACE]\" : \"\"));\n}\n\nstatic void *kprobe_seq_start(struct seq_file *f, loff_t *pos)\n{\n\treturn (*pos < KPROBE_TABLE_SIZE) ? pos : NULL;\n}\n\nstatic void *kprobe_seq_next(struct seq_file *f, void *v, loff_t *pos)\n{\n\t(*pos)++;\n\tif (*pos >= KPROBE_TABLE_SIZE)\n\t\treturn NULL;\n\treturn pos;\n}\n\nstatic void kprobe_seq_stop(struct seq_file *f, void *v)\n{\n\t \n}\n\nstatic int show_kprobe_addr(struct seq_file *pi, void *v)\n{\n\tstruct hlist_head *head;\n\tstruct kprobe *p, *kp;\n\tconst char *sym = NULL;\n\tunsigned int i = *(loff_t *) v;\n\tunsigned long offset = 0;\n\tchar *modname, namebuf[KSYM_NAME_LEN];\n\n\thead = &kprobe_table[i];\n\tpreempt_disable();\n\thlist_for_each_entry_rcu(p, head, hlist) {\n\t\tsym = kallsyms_lookup((unsigned long)p->addr, NULL,\n\t\t\t\t\t&offset, &modname, namebuf);\n\t\tif (kprobe_aggrprobe(p)) {\n\t\t\tlist_for_each_entry_rcu(kp, &p->list, list)\n\t\t\t\treport_probe(pi, kp, sym, offset, modname, p);\n\t\t} else\n\t\t\treport_probe(pi, p, sym, offset, modname, NULL);\n\t}\n\tpreempt_enable();\n\treturn 0;\n}\n\nstatic const struct seq_operations kprobes_sops = {\n\t.start = kprobe_seq_start,\n\t.next  = kprobe_seq_next,\n\t.stop  = kprobe_seq_stop,\n\t.show  = show_kprobe_addr\n};\n\nDEFINE_SEQ_ATTRIBUTE(kprobes);\n\n \nstatic void *kprobe_blacklist_seq_start(struct seq_file *m, loff_t *pos)\n{\n\tmutex_lock(&kprobe_mutex);\n\treturn seq_list_start(&kprobe_blacklist, *pos);\n}\n\nstatic void *kprobe_blacklist_seq_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\treturn seq_list_next(v, &kprobe_blacklist, pos);\n}\n\nstatic int kprobe_blacklist_seq_show(struct seq_file *m, void *v)\n{\n\tstruct kprobe_blacklist_entry *ent =\n\t\tlist_entry(v, struct kprobe_blacklist_entry, list);\n\n\t \n\tif (!kallsyms_show_value(m->file->f_cred))\n\t\tseq_printf(m, \"0x%px-0x%px\\t%ps\\n\", NULL, NULL,\n\t\t\t   (void *)ent->start_addr);\n\telse\n\t\tseq_printf(m, \"0x%px-0x%px\\t%ps\\n\", (void *)ent->start_addr,\n\t\t\t   (void *)ent->end_addr, (void *)ent->start_addr);\n\treturn 0;\n}\n\nstatic void kprobe_blacklist_seq_stop(struct seq_file *f, void *v)\n{\n\tmutex_unlock(&kprobe_mutex);\n}\n\nstatic const struct seq_operations kprobe_blacklist_sops = {\n\t.start = kprobe_blacklist_seq_start,\n\t.next  = kprobe_blacklist_seq_next,\n\t.stop  = kprobe_blacklist_seq_stop,\n\t.show  = kprobe_blacklist_seq_show,\n};\nDEFINE_SEQ_ATTRIBUTE(kprobe_blacklist);\n\nstatic int arm_all_kprobes(void)\n{\n\tstruct hlist_head *head;\n\tstruct kprobe *p;\n\tunsigned int i, total = 0, errors = 0;\n\tint err, ret = 0;\n\n\tmutex_lock(&kprobe_mutex);\n\n\t \n\tif (!kprobes_all_disarmed)\n\t\tgoto already_enabled;\n\n\t \n\tkprobes_all_disarmed = false;\n\t \n\tfor (i = 0; i < KPROBE_TABLE_SIZE; i++) {\n\t\thead = &kprobe_table[i];\n\t\t \n\t\thlist_for_each_entry(p, head, hlist) {\n\t\t\tif (!kprobe_disabled(p)) {\n\t\t\t\terr = arm_kprobe(p);\n\t\t\t\tif (err)  {\n\t\t\t\t\terrors++;\n\t\t\t\t\tret = err;\n\t\t\t\t}\n\t\t\t\ttotal++;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (errors)\n\t\tpr_warn(\"Kprobes globally enabled, but failed to enable %d out of %d probes. Please check which kprobes are kept disabled via debugfs.\\n\",\n\t\t\terrors, total);\n\telse\n\t\tpr_info(\"Kprobes globally enabled\\n\");\n\nalready_enabled:\n\tmutex_unlock(&kprobe_mutex);\n\treturn ret;\n}\n\nstatic int disarm_all_kprobes(void)\n{\n\tstruct hlist_head *head;\n\tstruct kprobe *p;\n\tunsigned int i, total = 0, errors = 0;\n\tint err, ret = 0;\n\n\tmutex_lock(&kprobe_mutex);\n\n\t \n\tif (kprobes_all_disarmed) {\n\t\tmutex_unlock(&kprobe_mutex);\n\t\treturn 0;\n\t}\n\n\tkprobes_all_disarmed = true;\n\n\tfor (i = 0; i < KPROBE_TABLE_SIZE; i++) {\n\t\thead = &kprobe_table[i];\n\t\t \n\t\thlist_for_each_entry(p, head, hlist) {\n\t\t\tif (!arch_trampoline_kprobe(p) && !kprobe_disabled(p)) {\n\t\t\t\terr = disarm_kprobe(p, false);\n\t\t\t\tif (err) {\n\t\t\t\t\terrors++;\n\t\t\t\t\tret = err;\n\t\t\t\t}\n\t\t\t\ttotal++;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (errors)\n\t\tpr_warn(\"Kprobes globally disabled, but failed to disable %d out of %d probes. Please check which kprobes are kept enabled via debugfs.\\n\",\n\t\t\terrors, total);\n\telse\n\t\tpr_info(\"Kprobes globally disabled\\n\");\n\n\tmutex_unlock(&kprobe_mutex);\n\n\t \n\twait_for_kprobe_optimizer();\n\n\treturn ret;\n}\n\n \nstatic ssize_t read_enabled_file_bool(struct file *file,\n\t       char __user *user_buf, size_t count, loff_t *ppos)\n{\n\tchar buf[3];\n\n\tif (!kprobes_all_disarmed)\n\t\tbuf[0] = '1';\n\telse\n\t\tbuf[0] = '0';\n\tbuf[1] = '\\n';\n\tbuf[2] = 0x00;\n\treturn simple_read_from_buffer(user_buf, count, ppos, buf, 2);\n}\n\nstatic ssize_t write_enabled_file_bool(struct file *file,\n\t       const char __user *user_buf, size_t count, loff_t *ppos)\n{\n\tbool enable;\n\tint ret;\n\n\tret = kstrtobool_from_user(user_buf, count, &enable);\n\tif (ret)\n\t\treturn ret;\n\n\tret = enable ? arm_all_kprobes() : disarm_all_kprobes();\n\tif (ret)\n\t\treturn ret;\n\n\treturn count;\n}\n\nstatic const struct file_operations fops_kp = {\n\t.read =         read_enabled_file_bool,\n\t.write =        write_enabled_file_bool,\n\t.llseek =\tdefault_llseek,\n};\n\nstatic int __init debugfs_kprobe_init(void)\n{\n\tstruct dentry *dir;\n\n\tdir = debugfs_create_dir(\"kprobes\", NULL);\n\n\tdebugfs_create_file(\"list\", 0400, dir, NULL, &kprobes_fops);\n\n\tdebugfs_create_file(\"enabled\", 0600, dir, NULL, &fops_kp);\n\n\tdebugfs_create_file(\"blacklist\", 0400, dir, NULL,\n\t\t\t    &kprobe_blacklist_fops);\n\n\treturn 0;\n}\n\nlate_initcall(debugfs_kprobe_init);\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}