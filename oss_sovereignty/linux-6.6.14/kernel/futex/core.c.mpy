{
  "module_name": "core.c",
  "hash_id": "5fb5663aa1aaca4f08b97996d52f41aef6e5e7a0a96041ea4cfe108c32def02f",
  "original_prompt": "Ingested from linux-6.6.14/kernel/futex/core.c",
  "human_readable_source": "\n \n#include <linux/compat.h>\n#include <linux/jhash.h>\n#include <linux/pagemap.h>\n#include <linux/memblock.h>\n#include <linux/fault-inject.h>\n#include <linux/slab.h>\n\n#include \"futex.h\"\n#include \"../locking/rtmutex_common.h\"\n\n \nstatic struct {\n\tstruct futex_hash_bucket *queues;\n\tunsigned long            hashsize;\n} __futex_data __read_mostly __aligned(2*sizeof(long));\n#define futex_queues   (__futex_data.queues)\n#define futex_hashsize (__futex_data.hashsize)\n\n\n \n#ifdef CONFIG_FAIL_FUTEX\n\nstatic struct {\n\tstruct fault_attr attr;\n\n\tbool ignore_private;\n} fail_futex = {\n\t.attr = FAULT_ATTR_INITIALIZER,\n\t.ignore_private = false,\n};\n\nstatic int __init setup_fail_futex(char *str)\n{\n\treturn setup_fault_attr(&fail_futex.attr, str);\n}\n__setup(\"fail_futex=\", setup_fail_futex);\n\nbool should_fail_futex(bool fshared)\n{\n\tif (fail_futex.ignore_private && !fshared)\n\t\treturn false;\n\n\treturn should_fail(&fail_futex.attr, 1);\n}\n\n#ifdef CONFIG_FAULT_INJECTION_DEBUG_FS\n\nstatic int __init fail_futex_debugfs(void)\n{\n\tumode_t mode = S_IFREG | S_IRUSR | S_IWUSR;\n\tstruct dentry *dir;\n\n\tdir = fault_create_debugfs_attr(\"fail_futex\", NULL,\n\t\t\t\t\t&fail_futex.attr);\n\tif (IS_ERR(dir))\n\t\treturn PTR_ERR(dir);\n\n\tdebugfs_create_bool(\"ignore-private\", mode, dir,\n\t\t\t    &fail_futex.ignore_private);\n\treturn 0;\n}\n\nlate_initcall(fail_futex_debugfs);\n\n#endif  \n\n#endif  \n\n \nstruct futex_hash_bucket *futex_hash(union futex_key *key)\n{\n\tu32 hash = jhash2((u32 *)key, offsetof(typeof(*key), both.offset) / 4,\n\t\t\t  key->both.offset);\n\n\treturn &futex_queues[hash & (futex_hashsize - 1)];\n}\n\n\n \nstruct hrtimer_sleeper *\nfutex_setup_timer(ktime_t *time, struct hrtimer_sleeper *timeout,\n\t\t  int flags, u64 range_ns)\n{\n\tif (!time)\n\t\treturn NULL;\n\n\thrtimer_init_sleeper_on_stack(timeout, (flags & FLAGS_CLOCKRT) ?\n\t\t\t\t      CLOCK_REALTIME : CLOCK_MONOTONIC,\n\t\t\t\t      HRTIMER_MODE_ABS);\n\t \n\thrtimer_set_expires_range_ns(&timeout->timer, *time, range_ns);\n\n\treturn timeout;\n}\n\n \nstatic u64 get_inode_sequence_number(struct inode *inode)\n{\n\tstatic atomic64_t i_seq;\n\tu64 old;\n\n\t \n\told = atomic64_read(&inode->i_sequence);\n\tif (likely(old))\n\t\treturn old;\n\n\tfor (;;) {\n\t\tu64 new = atomic64_add_return(1, &i_seq);\n\t\tif (WARN_ON_ONCE(!new))\n\t\t\tcontinue;\n\n\t\told = atomic64_cmpxchg_relaxed(&inode->i_sequence, 0, new);\n\t\tif (old)\n\t\t\treturn old;\n\t\treturn new;\n\t}\n}\n\n \nint get_futex_key(u32 __user *uaddr, bool fshared, union futex_key *key,\n\t\t  enum futex_access rw)\n{\n\tunsigned long address = (unsigned long)uaddr;\n\tstruct mm_struct *mm = current->mm;\n\tstruct page *page, *tail;\n\tstruct address_space *mapping;\n\tint err, ro = 0;\n\n\t \n\tkey->both.offset = address % PAGE_SIZE;\n\tif (unlikely((address % sizeof(u32)) != 0))\n\t\treturn -EINVAL;\n\taddress -= key->both.offset;\n\n\tif (unlikely(!access_ok(uaddr, sizeof(u32))))\n\t\treturn -EFAULT;\n\n\tif (unlikely(should_fail_futex(fshared)))\n\t\treturn -EFAULT;\n\n\t \n\tif (!fshared) {\n\t\t \n\t\tif (IS_ENABLED(CONFIG_MMU))\n\t\t\tkey->private.mm = mm;\n\t\telse\n\t\t\tkey->private.mm = NULL;\n\n\t\tkey->private.address = address;\n\t\treturn 0;\n\t}\n\nagain:\n\t \n\tif (unlikely(should_fail_futex(true)))\n\t\treturn -EFAULT;\n\n\terr = get_user_pages_fast(address, 1, FOLL_WRITE, &page);\n\t \n\tif (err == -EFAULT && rw == FUTEX_READ) {\n\t\terr = get_user_pages_fast(address, 1, 0, &page);\n\t\tro = 1;\n\t}\n\tif (err < 0)\n\t\treturn err;\n\telse\n\t\terr = 0;\n\n\t \n\ttail = page;\n\tpage = compound_head(page);\n\tmapping = READ_ONCE(page->mapping);\n\n\t \n\tif (unlikely(!mapping)) {\n\t\tint shmem_swizzled;\n\n\t\t \n\t\tlock_page(page);\n\t\tshmem_swizzled = PageSwapCache(page) || page->mapping;\n\t\tunlock_page(page);\n\t\tput_page(page);\n\n\t\tif (shmem_swizzled)\n\t\t\tgoto again;\n\n\t\treturn -EFAULT;\n\t}\n\n\t \n\tif (PageAnon(page)) {\n\t\t \n\t\tif (unlikely(should_fail_futex(true)) || ro) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\tkey->both.offset |= FUT_OFF_MMSHARED;  \n\t\tkey->private.mm = mm;\n\t\tkey->private.address = address;\n\n\t} else {\n\t\tstruct inode *inode;\n\n\t\t \n\t\trcu_read_lock();\n\n\t\tif (READ_ONCE(page->mapping) != mapping) {\n\t\t\trcu_read_unlock();\n\t\t\tput_page(page);\n\n\t\t\tgoto again;\n\t\t}\n\n\t\tinode = READ_ONCE(mapping->host);\n\t\tif (!inode) {\n\t\t\trcu_read_unlock();\n\t\t\tput_page(page);\n\n\t\t\tgoto again;\n\t\t}\n\n\t\tkey->both.offset |= FUT_OFF_INODE;  \n\t\tkey->shared.i_seq = get_inode_sequence_number(inode);\n\t\tkey->shared.pgoff = page_to_pgoff(tail);\n\t\trcu_read_unlock();\n\t}\n\nout:\n\tput_page(page);\n\treturn err;\n}\n\n \nint fault_in_user_writeable(u32 __user *uaddr)\n{\n\tstruct mm_struct *mm = current->mm;\n\tint ret;\n\n\tmmap_read_lock(mm);\n\tret = fixup_user_fault(mm, (unsigned long)uaddr,\n\t\t\t       FAULT_FLAG_WRITE, NULL);\n\tmmap_read_unlock(mm);\n\n\treturn ret < 0 ? ret : 0;\n}\n\n \nstruct futex_q *futex_top_waiter(struct futex_hash_bucket *hb, union futex_key *key)\n{\n\tstruct futex_q *this;\n\n\tplist_for_each_entry(this, &hb->chain, list) {\n\t\tif (futex_match(&this->key, key))\n\t\t\treturn this;\n\t}\n\treturn NULL;\n}\n\nint futex_cmpxchg_value_locked(u32 *curval, u32 __user *uaddr, u32 uval, u32 newval)\n{\n\tint ret;\n\n\tpagefault_disable();\n\tret = futex_atomic_cmpxchg_inatomic(curval, uaddr, uval, newval);\n\tpagefault_enable();\n\n\treturn ret;\n}\n\nint futex_get_value_locked(u32 *dest, u32 __user *from)\n{\n\tint ret;\n\n\tpagefault_disable();\n\tret = __get_user(*dest, from);\n\tpagefault_enable();\n\n\treturn ret ? -EFAULT : 0;\n}\n\n \nvoid wait_for_owner_exiting(int ret, struct task_struct *exiting)\n{\n\tif (ret != -EBUSY) {\n\t\tWARN_ON_ONCE(exiting);\n\t\treturn;\n\t}\n\n\tif (WARN_ON_ONCE(ret == -EBUSY && !exiting))\n\t\treturn;\n\n\tmutex_lock(&exiting->futex_exit_mutex);\n\t \n\tmutex_unlock(&exiting->futex_exit_mutex);\n\n\tput_task_struct(exiting);\n}\n\n \nvoid __futex_unqueue(struct futex_q *q)\n{\n\tstruct futex_hash_bucket *hb;\n\n\tif (WARN_ON_SMP(!q->lock_ptr) || WARN_ON(plist_node_empty(&q->list)))\n\t\treturn;\n\tlockdep_assert_held(q->lock_ptr);\n\n\thb = container_of(q->lock_ptr, struct futex_hash_bucket, lock);\n\tplist_del(&q->list, &hb->chain);\n\tfutex_hb_waiters_dec(hb);\n}\n\n \nstruct futex_hash_bucket *futex_q_lock(struct futex_q *q)\n\t__acquires(&hb->lock)\n{\n\tstruct futex_hash_bucket *hb;\n\n\thb = futex_hash(&q->key);\n\n\t \n\tfutex_hb_waiters_inc(hb);  \n\n\tq->lock_ptr = &hb->lock;\n\n\tspin_lock(&hb->lock);\n\treturn hb;\n}\n\nvoid futex_q_unlock(struct futex_hash_bucket *hb)\n\t__releases(&hb->lock)\n{\n\tspin_unlock(&hb->lock);\n\tfutex_hb_waiters_dec(hb);\n}\n\nvoid __futex_queue(struct futex_q *q, struct futex_hash_bucket *hb)\n{\n\tint prio;\n\n\t \n\tprio = min(current->normal_prio, MAX_RT_PRIO);\n\n\tplist_node_init(&q->list, prio);\n\tplist_add(&q->list, &hb->chain);\n\tq->task = current;\n}\n\n \nint futex_unqueue(struct futex_q *q)\n{\n\tspinlock_t *lock_ptr;\n\tint ret = 0;\n\n\t \nretry:\n\t \n\tlock_ptr = READ_ONCE(q->lock_ptr);\n\tif (lock_ptr != NULL) {\n\t\tspin_lock(lock_ptr);\n\t\t \n\t\tif (unlikely(lock_ptr != q->lock_ptr)) {\n\t\t\tspin_unlock(lock_ptr);\n\t\t\tgoto retry;\n\t\t}\n\t\t__futex_unqueue(q);\n\n\t\tBUG_ON(q->pi_state);\n\n\t\tspin_unlock(lock_ptr);\n\t\tret = 1;\n\t}\n\n\treturn ret;\n}\n\n \nvoid futex_unqueue_pi(struct futex_q *q)\n{\n\t__futex_unqueue(q);\n\n\tBUG_ON(!q->pi_state);\n\tput_pi_state(q->pi_state);\n\tq->pi_state = NULL;\n}\n\n \n#define HANDLE_DEATH_PENDING\ttrue\n#define HANDLE_DEATH_LIST\tfalse\n\n \nstatic int handle_futex_death(u32 __user *uaddr, struct task_struct *curr,\n\t\t\t      bool pi, bool pending_op)\n{\n\tu32 uval, nval, mval;\n\tpid_t owner;\n\tint err;\n\n\t \n\tif ((((unsigned long)uaddr) % sizeof(*uaddr)) != 0)\n\t\treturn -1;\n\nretry:\n\tif (get_user(uval, uaddr))\n\t\treturn -1;\n\n\t \n\towner = uval & FUTEX_TID_MASK;\n\n\tif (pending_op && !pi && !owner) {\n\t\tfutex_wake(uaddr, 1, 1, FUTEX_BITSET_MATCH_ANY);\n\t\treturn 0;\n\t}\n\n\tif (owner != task_pid_vnr(curr))\n\t\treturn 0;\n\n\t \n\tmval = (uval & FUTEX_WAITERS) | FUTEX_OWNER_DIED;\n\n\t \n\tif ((err = futex_cmpxchg_value_locked(&nval, uaddr, uval, mval))) {\n\t\tswitch (err) {\n\t\tcase -EFAULT:\n\t\t\tif (fault_in_user_writeable(uaddr))\n\t\t\t\treturn -1;\n\t\t\tgoto retry;\n\n\t\tcase -EAGAIN:\n\t\t\tcond_resched();\n\t\t\tgoto retry;\n\n\t\tdefault:\n\t\t\tWARN_ON_ONCE(1);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tif (nval != uval)\n\t\tgoto retry;\n\n\t \n\tif (!pi && (uval & FUTEX_WAITERS))\n\t\tfutex_wake(uaddr, 1, 1, FUTEX_BITSET_MATCH_ANY);\n\n\treturn 0;\n}\n\n \nstatic inline int fetch_robust_entry(struct robust_list __user **entry,\n\t\t\t\t     struct robust_list __user * __user *head,\n\t\t\t\t     unsigned int *pi)\n{\n\tunsigned long uentry;\n\n\tif (get_user(uentry, (unsigned long __user *)head))\n\t\treturn -EFAULT;\n\n\t*entry = (void __user *)(uentry & ~1UL);\n\t*pi = uentry & 1;\n\n\treturn 0;\n}\n\n \nstatic void exit_robust_list(struct task_struct *curr)\n{\n\tstruct robust_list_head __user *head = curr->robust_list;\n\tstruct robust_list __user *entry, *next_entry, *pending;\n\tunsigned int limit = ROBUST_LIST_LIMIT, pi, pip;\n\tunsigned int next_pi;\n\tunsigned long futex_offset;\n\tint rc;\n\n\t \n\tif (fetch_robust_entry(&entry, &head->list.next, &pi))\n\t\treturn;\n\t \n\tif (get_user(futex_offset, &head->futex_offset))\n\t\treturn;\n\t \n\tif (fetch_robust_entry(&pending, &head->list_op_pending, &pip))\n\t\treturn;\n\n\tnext_entry = NULL;\t \n\twhile (entry != &head->list) {\n\t\t \n\t\trc = fetch_robust_entry(&next_entry, &entry->next, &next_pi);\n\t\t \n\t\tif (entry != pending) {\n\t\t\tif (handle_futex_death((void __user *)entry + futex_offset,\n\t\t\t\t\t\tcurr, pi, HANDLE_DEATH_LIST))\n\t\t\t\treturn;\n\t\t}\n\t\tif (rc)\n\t\t\treturn;\n\t\tentry = next_entry;\n\t\tpi = next_pi;\n\t\t \n\t\tif (!--limit)\n\t\t\tbreak;\n\n\t\tcond_resched();\n\t}\n\n\tif (pending) {\n\t\thandle_futex_death((void __user *)pending + futex_offset,\n\t\t\t\t   curr, pip, HANDLE_DEATH_PENDING);\n\t}\n}\n\n#ifdef CONFIG_COMPAT\nstatic void __user *futex_uaddr(struct robust_list __user *entry,\n\t\t\t\tcompat_long_t futex_offset)\n{\n\tcompat_uptr_t base = ptr_to_compat(entry);\n\tvoid __user *uaddr = compat_ptr(base + futex_offset);\n\n\treturn uaddr;\n}\n\n \nstatic inline int\ncompat_fetch_robust_entry(compat_uptr_t *uentry, struct robust_list __user **entry,\n\t\t   compat_uptr_t __user *head, unsigned int *pi)\n{\n\tif (get_user(*uentry, head))\n\t\treturn -EFAULT;\n\n\t*entry = compat_ptr((*uentry) & ~1);\n\t*pi = (unsigned int)(*uentry) & 1;\n\n\treturn 0;\n}\n\n \nstatic void compat_exit_robust_list(struct task_struct *curr)\n{\n\tstruct compat_robust_list_head __user *head = curr->compat_robust_list;\n\tstruct robust_list __user *entry, *next_entry, *pending;\n\tunsigned int limit = ROBUST_LIST_LIMIT, pi, pip;\n\tunsigned int next_pi;\n\tcompat_uptr_t uentry, next_uentry, upending;\n\tcompat_long_t futex_offset;\n\tint rc;\n\n\t \n\tif (compat_fetch_robust_entry(&uentry, &entry, &head->list.next, &pi))\n\t\treturn;\n\t \n\tif (get_user(futex_offset, &head->futex_offset))\n\t\treturn;\n\t \n\tif (compat_fetch_robust_entry(&upending, &pending,\n\t\t\t       &head->list_op_pending, &pip))\n\t\treturn;\n\n\tnext_entry = NULL;\t \n\twhile (entry != (struct robust_list __user *) &head->list) {\n\t\t \n\t\trc = compat_fetch_robust_entry(&next_uentry, &next_entry,\n\t\t\t(compat_uptr_t __user *)&entry->next, &next_pi);\n\t\t \n\t\tif (entry != pending) {\n\t\t\tvoid __user *uaddr = futex_uaddr(entry, futex_offset);\n\n\t\t\tif (handle_futex_death(uaddr, curr, pi,\n\t\t\t\t\t       HANDLE_DEATH_LIST))\n\t\t\t\treturn;\n\t\t}\n\t\tif (rc)\n\t\t\treturn;\n\t\tuentry = next_uentry;\n\t\tentry = next_entry;\n\t\tpi = next_pi;\n\t\t \n\t\tif (!--limit)\n\t\t\tbreak;\n\n\t\tcond_resched();\n\t}\n\tif (pending) {\n\t\tvoid __user *uaddr = futex_uaddr(pending, futex_offset);\n\n\t\thandle_futex_death(uaddr, curr, pip, HANDLE_DEATH_PENDING);\n\t}\n}\n#endif\n\n#ifdef CONFIG_FUTEX_PI\n\n \nstatic void exit_pi_state_list(struct task_struct *curr)\n{\n\tstruct list_head *next, *head = &curr->pi_state_list;\n\tstruct futex_pi_state *pi_state;\n\tstruct futex_hash_bucket *hb;\n\tunion futex_key key = FUTEX_KEY_INIT;\n\n\t \n\traw_spin_lock_irq(&curr->pi_lock);\n\twhile (!list_empty(head)) {\n\t\tnext = head->next;\n\t\tpi_state = list_entry(next, struct futex_pi_state, list);\n\t\tkey = pi_state->key;\n\t\thb = futex_hash(&key);\n\n\t\t \n\t\tif (!refcount_inc_not_zero(&pi_state->refcount)) {\n\t\t\traw_spin_unlock_irq(&curr->pi_lock);\n\t\t\tcpu_relax();\n\t\t\traw_spin_lock_irq(&curr->pi_lock);\n\t\t\tcontinue;\n\t\t}\n\t\traw_spin_unlock_irq(&curr->pi_lock);\n\n\t\tspin_lock(&hb->lock);\n\t\traw_spin_lock_irq(&pi_state->pi_mutex.wait_lock);\n\t\traw_spin_lock(&curr->pi_lock);\n\t\t \n\t\tif (head->next != next) {\n\t\t\t \n\t\t\traw_spin_unlock(&pi_state->pi_mutex.wait_lock);\n\t\t\tspin_unlock(&hb->lock);\n\t\t\tput_pi_state(pi_state);\n\t\t\tcontinue;\n\t\t}\n\n\t\tWARN_ON(pi_state->owner != curr);\n\t\tWARN_ON(list_empty(&pi_state->list));\n\t\tlist_del_init(&pi_state->list);\n\t\tpi_state->owner = NULL;\n\n\t\traw_spin_unlock(&curr->pi_lock);\n\t\traw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);\n\t\tspin_unlock(&hb->lock);\n\n\t\trt_mutex_futex_unlock(&pi_state->pi_mutex);\n\t\tput_pi_state(pi_state);\n\n\t\traw_spin_lock_irq(&curr->pi_lock);\n\t}\n\traw_spin_unlock_irq(&curr->pi_lock);\n}\n#else\nstatic inline void exit_pi_state_list(struct task_struct *curr) { }\n#endif\n\nstatic void futex_cleanup(struct task_struct *tsk)\n{\n\tif (unlikely(tsk->robust_list)) {\n\t\texit_robust_list(tsk);\n\t\ttsk->robust_list = NULL;\n\t}\n\n#ifdef CONFIG_COMPAT\n\tif (unlikely(tsk->compat_robust_list)) {\n\t\tcompat_exit_robust_list(tsk);\n\t\ttsk->compat_robust_list = NULL;\n\t}\n#endif\n\n\tif (unlikely(!list_empty(&tsk->pi_state_list)))\n\t\texit_pi_state_list(tsk);\n}\n\n \nvoid futex_exit_recursive(struct task_struct *tsk)\n{\n\t \n\tif (tsk->futex_state == FUTEX_STATE_EXITING)\n\t\tmutex_unlock(&tsk->futex_exit_mutex);\n\ttsk->futex_state = FUTEX_STATE_DEAD;\n}\n\nstatic void futex_cleanup_begin(struct task_struct *tsk)\n{\n\t \n\tmutex_lock(&tsk->futex_exit_mutex);\n\n\t \n\traw_spin_lock_irq(&tsk->pi_lock);\n\ttsk->futex_state = FUTEX_STATE_EXITING;\n\traw_spin_unlock_irq(&tsk->pi_lock);\n}\n\nstatic void futex_cleanup_end(struct task_struct *tsk, int state)\n{\n\t \n\ttsk->futex_state = state;\n\t \n\tmutex_unlock(&tsk->futex_exit_mutex);\n}\n\nvoid futex_exec_release(struct task_struct *tsk)\n{\n\t \n\tfutex_cleanup_begin(tsk);\n\tfutex_cleanup(tsk);\n\t \n\tfutex_cleanup_end(tsk, FUTEX_STATE_OK);\n}\n\nvoid futex_exit_release(struct task_struct *tsk)\n{\n\tfutex_cleanup_begin(tsk);\n\tfutex_cleanup(tsk);\n\tfutex_cleanup_end(tsk, FUTEX_STATE_DEAD);\n}\n\nstatic int __init futex_init(void)\n{\n\tunsigned int futex_shift;\n\tunsigned long i;\n\n#if CONFIG_BASE_SMALL\n\tfutex_hashsize = 16;\n#else\n\tfutex_hashsize = roundup_pow_of_two(256 * num_possible_cpus());\n#endif\n\n\tfutex_queues = alloc_large_system_hash(\"futex\", sizeof(*futex_queues),\n\t\t\t\t\t       futex_hashsize, 0, 0,\n\t\t\t\t\t       &futex_shift, NULL,\n\t\t\t\t\t       futex_hashsize, futex_hashsize);\n\tfutex_hashsize = 1UL << futex_shift;\n\n\tfor (i = 0; i < futex_hashsize; i++) {\n\t\tatomic_set(&futex_queues[i].waiters, 0);\n\t\tplist_head_init(&futex_queues[i].chain);\n\t\tspin_lock_init(&futex_queues[i].lock);\n\t}\n\n\treturn 0;\n}\ncore_initcall(futex_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}