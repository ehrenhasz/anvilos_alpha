{
  "module_name": "lockdep_internals.h",
  "hash_id": "0a26e7d728cd0fd159426c25d7c5b4eec8e80f1b3822fb58ba78a1fe34ed38be",
  "original_prompt": "Ingested from linux-6.6.14/kernel/locking/lockdep_internals.h",
  "human_readable_source": " \n \n\n \nenum lock_usage_bit {\n#define LOCKDEP_STATE(__STATE)\t\t\\\n\tLOCK_USED_IN_##__STATE,\t\t\\\n\tLOCK_USED_IN_##__STATE##_READ,\t\\\n\tLOCK_ENABLED_##__STATE,\t\t\\\n\tLOCK_ENABLED_##__STATE##_READ,\n#include \"lockdep_states.h\"\n#undef LOCKDEP_STATE\n\tLOCK_USED,\n\tLOCK_USED_READ,\n\tLOCK_USAGE_STATES,\n};\n\n \nstatic_assert(LOCK_TRACE_STATES == LOCK_USAGE_STATES);\n\n#define LOCK_USAGE_READ_MASK 1\n#define LOCK_USAGE_DIR_MASK  2\n#define LOCK_USAGE_STATE_MASK (~(LOCK_USAGE_READ_MASK | LOCK_USAGE_DIR_MASK))\n\n \n#define __LOCKF(__STATE)\tLOCKF_##__STATE = (1 << LOCK_##__STATE),\n\nenum {\n#define LOCKDEP_STATE(__STATE)\t\t\t\t\t\t\\\n\t__LOCKF(USED_IN_##__STATE)\t\t\t\t\t\\\n\t__LOCKF(USED_IN_##__STATE##_READ)\t\t\t\t\\\n\t__LOCKF(ENABLED_##__STATE)\t\t\t\t\t\\\n\t__LOCKF(ENABLED_##__STATE##_READ)\n#include \"lockdep_states.h\"\n#undef LOCKDEP_STATE\n\t__LOCKF(USED)\n\t__LOCKF(USED_READ)\n};\n\n#define LOCKDEP_STATE(__STATE)\tLOCKF_ENABLED_##__STATE |\nstatic const unsigned long LOCKF_ENABLED_IRQ =\n#include \"lockdep_states.h\"\n\t0;\n#undef LOCKDEP_STATE\n\n#define LOCKDEP_STATE(__STATE)\tLOCKF_USED_IN_##__STATE |\nstatic const unsigned long LOCKF_USED_IN_IRQ =\n#include \"lockdep_states.h\"\n\t0;\n#undef LOCKDEP_STATE\n\n#define LOCKDEP_STATE(__STATE)\tLOCKF_ENABLED_##__STATE##_READ |\nstatic const unsigned long LOCKF_ENABLED_IRQ_READ =\n#include \"lockdep_states.h\"\n\t0;\n#undef LOCKDEP_STATE\n\n#define LOCKDEP_STATE(__STATE)\tLOCKF_USED_IN_##__STATE##_READ |\nstatic const unsigned long LOCKF_USED_IN_IRQ_READ =\n#include \"lockdep_states.h\"\n\t0;\n#undef LOCKDEP_STATE\n\n#define LOCKF_ENABLED_IRQ_ALL (LOCKF_ENABLED_IRQ | LOCKF_ENABLED_IRQ_READ)\n#define LOCKF_USED_IN_IRQ_ALL (LOCKF_USED_IN_IRQ | LOCKF_USED_IN_IRQ_READ)\n\n#define LOCKF_IRQ (LOCKF_ENABLED_IRQ | LOCKF_USED_IN_IRQ)\n#define LOCKF_IRQ_READ (LOCKF_ENABLED_IRQ_READ | LOCKF_USED_IN_IRQ_READ)\n\n \n#ifdef CONFIG_LOCKDEP_SMALL\n \n#define MAX_LOCKDEP_ENTRIES\t16384UL\n#define MAX_LOCKDEP_CHAINS_BITS\t15\n#define MAX_STACK_TRACE_ENTRIES\t262144UL\n#define STACK_TRACE_HASH_SIZE\t8192\n#else\n#define MAX_LOCKDEP_ENTRIES\t(1UL << CONFIG_LOCKDEP_BITS)\n\n#define MAX_LOCKDEP_CHAINS_BITS\tCONFIG_LOCKDEP_CHAINS_BITS\n\n \n#define MAX_STACK_TRACE_ENTRIES\t(1UL << CONFIG_LOCKDEP_STACK_TRACE_BITS)\n#define STACK_TRACE_HASH_SIZE\t(1 << CONFIG_LOCKDEP_STACK_TRACE_HASH_BITS)\n#endif\n\n \n#define LOCK_CHAIN_SOFTIRQ_CONTEXT\t(1 << 0)\n#define LOCK_CHAIN_HARDIRQ_CONTEXT\t(1 << 1)\n\n#define MAX_LOCKDEP_CHAINS\t(1UL << MAX_LOCKDEP_CHAINS_BITS)\n\n#define MAX_LOCKDEP_CHAIN_HLOCKS (MAX_LOCKDEP_CHAINS*5)\n\nextern struct lock_chain lock_chains[];\n\n#define LOCK_USAGE_CHARS (2*XXX_LOCK_USAGE_STATES + 1)\n\nextern void get_usage_chars(struct lock_class *class,\n\t\t\t    char usage[LOCK_USAGE_CHARS]);\n\nextern const char *__get_key_name(const struct lockdep_subclass_key *key,\n\t\t\t\t  char *str);\n\nstruct lock_class *lock_chain_get_class(struct lock_chain *chain, int i);\n\nextern unsigned long nr_lock_classes;\nextern unsigned long nr_zapped_classes;\nextern unsigned long nr_zapped_lock_chains;\nextern unsigned long nr_list_entries;\nlong lockdep_next_lockchain(long i);\nunsigned long lock_chain_count(void);\nextern unsigned long nr_stack_trace_entries;\n\nextern unsigned int nr_hardirq_chains;\nextern unsigned int nr_softirq_chains;\nextern unsigned int nr_process_chains;\nextern unsigned int nr_free_chain_hlocks;\nextern unsigned int nr_lost_chain_hlocks;\nextern unsigned int nr_large_chain_blocks;\n\nextern unsigned int max_lockdep_depth;\nextern unsigned int max_bfs_queue_depth;\nextern unsigned long max_lock_class_idx;\n\nextern struct lock_class lock_classes[MAX_LOCKDEP_KEYS];\nextern unsigned long lock_classes_in_use[];\n\n#ifdef CONFIG_PROVE_LOCKING\nextern unsigned long lockdep_count_forward_deps(struct lock_class *);\nextern unsigned long lockdep_count_backward_deps(struct lock_class *);\n#ifdef CONFIG_TRACE_IRQFLAGS\nu64 lockdep_stack_trace_count(void);\nu64 lockdep_stack_hash_count(void);\n#endif\n#else\nstatic inline unsigned long\nlockdep_count_forward_deps(struct lock_class *class)\n{\n\treturn 0;\n}\nstatic inline unsigned long\nlockdep_count_backward_deps(struct lock_class *class)\n{\n\treturn 0;\n}\n#endif\n\n#ifdef CONFIG_DEBUG_LOCKDEP\n\n#include <asm/local.h>\n \nstruct lockdep_stats {\n\tunsigned long  chain_lookup_hits;\n\tunsigned int   chain_lookup_misses;\n\tunsigned long  hardirqs_on_events;\n\tunsigned long  hardirqs_off_events;\n\tunsigned long  redundant_hardirqs_on;\n\tunsigned long  redundant_hardirqs_off;\n\tunsigned long  softirqs_on_events;\n\tunsigned long  softirqs_off_events;\n\tunsigned long  redundant_softirqs_on;\n\tunsigned long  redundant_softirqs_off;\n\tint            nr_unused_locks;\n\tunsigned int   nr_redundant_checks;\n\tunsigned int   nr_redundant;\n\tunsigned int   nr_cyclic_checks;\n\tunsigned int   nr_find_usage_forwards_checks;\n\tunsigned int   nr_find_usage_backwards_checks;\n\n\t \n\tunsigned long lock_class_ops[MAX_LOCKDEP_KEYS];\n};\n\nDECLARE_PER_CPU(struct lockdep_stats, lockdep_stats);\n\n#define __debug_atomic_inc(ptr)\t\t\t\t\t\\\n\tthis_cpu_inc(lockdep_stats.ptr);\n\n#define debug_atomic_inc(ptr)\t\t\t{\t\t\\\n\tWARN_ON_ONCE(!irqs_disabled());\t\t\t\t\\\n\t__this_cpu_inc(lockdep_stats.ptr);\t\t\t\\\n}\n\n#define debug_atomic_dec(ptr)\t\t\t{\t\t\\\n\tWARN_ON_ONCE(!irqs_disabled());\t\t\t\t\\\n\t__this_cpu_dec(lockdep_stats.ptr);\t\t\t\\\n}\n\n#define debug_atomic_read(ptr)\t\t({\t\t\t\t\\\n\tstruct lockdep_stats *__cpu_lockdep_stats;\t\t\t\\\n\tunsigned long long __total = 0;\t\t\t\t\t\\\n\tint __cpu;\t\t\t\t\t\t\t\\\n\tfor_each_possible_cpu(__cpu) {\t\t\t\t\t\\\n\t\t__cpu_lockdep_stats = &per_cpu(lockdep_stats, __cpu);\t\\\n\t\t__total += __cpu_lockdep_stats->ptr;\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\t__total;\t\t\t\t\t\t\t\\\n})\n\nstatic inline void debug_class_ops_inc(struct lock_class *class)\n{\n\tint idx;\n\n\tidx = class - lock_classes;\n\t__debug_atomic_inc(lock_class_ops[idx]);\n}\n\nstatic inline unsigned long debug_class_ops_read(struct lock_class *class)\n{\n\tint idx, cpu;\n\tunsigned long ops = 0;\n\n\tidx = class - lock_classes;\n\tfor_each_possible_cpu(cpu)\n\t\tops += per_cpu(lockdep_stats.lock_class_ops[idx], cpu);\n\treturn ops;\n}\n\n#else\n# define __debug_atomic_inc(ptr)\tdo { } while (0)\n# define debug_atomic_inc(ptr)\t\tdo { } while (0)\n# define debug_atomic_dec(ptr)\t\tdo { } while (0)\n# define debug_atomic_read(ptr)\t\t0\n# define debug_class_ops_inc(ptr)\tdo { } while (0)\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}