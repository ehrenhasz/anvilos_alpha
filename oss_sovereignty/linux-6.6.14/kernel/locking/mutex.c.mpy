{
  "module_name": "mutex.c",
  "hash_id": "4f29d3440caab3ce8ed95fe6fda62d5f765c39de9104219193404a8ff3d7a7ae",
  "original_prompt": "Ingested from linux-6.6.14/kernel/locking/mutex.c",
  "human_readable_source": "\n \n#include <linux/mutex.h>\n#include <linux/ww_mutex.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/debug.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n#include <linux/interrupt.h>\n#include <linux/debug_locks.h>\n#include <linux/osq_lock.h>\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/lock.h>\n\n#ifndef CONFIG_PREEMPT_RT\n#include \"mutex.h\"\n\n#ifdef CONFIG_DEBUG_MUTEXES\n# define MUTEX_WARN_ON(cond) DEBUG_LOCKS_WARN_ON(cond)\n#else\n# define MUTEX_WARN_ON(cond)\n#endif\n\nvoid\n__mutex_init(struct mutex *lock, const char *name, struct lock_class_key *key)\n{\n\tatomic_long_set(&lock->owner, 0);\n\traw_spin_lock_init(&lock->wait_lock);\n\tINIT_LIST_HEAD(&lock->wait_list);\n#ifdef CONFIG_MUTEX_SPIN_ON_OWNER\n\tosq_lock_init(&lock->osq);\n#endif\n\n\tdebug_mutex_init(lock, name, key);\n}\nEXPORT_SYMBOL(__mutex_init);\n\n \n#define MUTEX_FLAG_WAITERS\t0x01\n#define MUTEX_FLAG_HANDOFF\t0x02\n#define MUTEX_FLAG_PICKUP\t0x04\n\n#define MUTEX_FLAGS\t\t0x07\n\n \nstatic inline struct task_struct *__mutex_owner(struct mutex *lock)\n{\n\treturn (struct task_struct *)(atomic_long_read(&lock->owner) & ~MUTEX_FLAGS);\n}\n\nstatic inline struct task_struct *__owner_task(unsigned long owner)\n{\n\treturn (struct task_struct *)(owner & ~MUTEX_FLAGS);\n}\n\nbool mutex_is_locked(struct mutex *lock)\n{\n\treturn __mutex_owner(lock) != NULL;\n}\nEXPORT_SYMBOL(mutex_is_locked);\n\nstatic inline unsigned long __owner_flags(unsigned long owner)\n{\n\treturn owner & MUTEX_FLAGS;\n}\n\n \nstatic inline struct task_struct *__mutex_trylock_common(struct mutex *lock, bool handoff)\n{\n\tunsigned long owner, curr = (unsigned long)current;\n\n\towner = atomic_long_read(&lock->owner);\n\tfor (;;) {  \n\t\tunsigned long flags = __owner_flags(owner);\n\t\tunsigned long task = owner & ~MUTEX_FLAGS;\n\n\t\tif (task) {\n\t\t\tif (flags & MUTEX_FLAG_PICKUP) {\n\t\t\t\tif (task != curr)\n\t\t\t\t\tbreak;\n\t\t\t\tflags &= ~MUTEX_FLAG_PICKUP;\n\t\t\t} else if (handoff) {\n\t\t\t\tif (flags & MUTEX_FLAG_HANDOFF)\n\t\t\t\t\tbreak;\n\t\t\t\tflags |= MUTEX_FLAG_HANDOFF;\n\t\t\t} else {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} else {\n\t\t\tMUTEX_WARN_ON(flags & (MUTEX_FLAG_HANDOFF | MUTEX_FLAG_PICKUP));\n\t\t\ttask = curr;\n\t\t}\n\n\t\tif (atomic_long_try_cmpxchg_acquire(&lock->owner, &owner, task | flags)) {\n\t\t\tif (task == curr)\n\t\t\t\treturn NULL;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn __owner_task(owner);\n}\n\n \nstatic inline bool __mutex_trylock_or_handoff(struct mutex *lock, bool handoff)\n{\n\treturn !__mutex_trylock_common(lock, handoff);\n}\n\n \nstatic inline bool __mutex_trylock(struct mutex *lock)\n{\n\treturn !__mutex_trylock_common(lock, false);\n}\n\n#ifndef CONFIG_DEBUG_LOCK_ALLOC\n \n\n \nstatic __always_inline bool __mutex_trylock_fast(struct mutex *lock)\n{\n\tunsigned long curr = (unsigned long)current;\n\tunsigned long zero = 0UL;\n\n\tif (atomic_long_try_cmpxchg_acquire(&lock->owner, &zero, curr))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic __always_inline bool __mutex_unlock_fast(struct mutex *lock)\n{\n\tunsigned long curr = (unsigned long)current;\n\n\treturn atomic_long_try_cmpxchg_release(&lock->owner, &curr, 0UL);\n}\n#endif\n\nstatic inline void __mutex_set_flag(struct mutex *lock, unsigned long flag)\n{\n\tatomic_long_or(flag, &lock->owner);\n}\n\nstatic inline void __mutex_clear_flag(struct mutex *lock, unsigned long flag)\n{\n\tatomic_long_andnot(flag, &lock->owner);\n}\n\nstatic inline bool __mutex_waiter_is_first(struct mutex *lock, struct mutex_waiter *waiter)\n{\n\treturn list_first_entry(&lock->wait_list, struct mutex_waiter, list) == waiter;\n}\n\n \nstatic void\n__mutex_add_waiter(struct mutex *lock, struct mutex_waiter *waiter,\n\t\t   struct list_head *list)\n{\n\tdebug_mutex_add_waiter(lock, waiter, current);\n\n\tlist_add_tail(&waiter->list, list);\n\tif (__mutex_waiter_is_first(lock, waiter))\n\t\t__mutex_set_flag(lock, MUTEX_FLAG_WAITERS);\n}\n\nstatic void\n__mutex_remove_waiter(struct mutex *lock, struct mutex_waiter *waiter)\n{\n\tlist_del(&waiter->list);\n\tif (likely(list_empty(&lock->wait_list)))\n\t\t__mutex_clear_flag(lock, MUTEX_FLAGS);\n\n\tdebug_mutex_remove_waiter(lock, waiter, current);\n}\n\n \nstatic void __mutex_handoff(struct mutex *lock, struct task_struct *task)\n{\n\tunsigned long owner = atomic_long_read(&lock->owner);\n\n\tfor (;;) {\n\t\tunsigned long new;\n\n\t\tMUTEX_WARN_ON(__owner_task(owner) != current);\n\t\tMUTEX_WARN_ON(owner & MUTEX_FLAG_PICKUP);\n\n\t\tnew = (owner & MUTEX_FLAG_WAITERS);\n\t\tnew |= (unsigned long)task;\n\t\tif (task)\n\t\t\tnew |= MUTEX_FLAG_PICKUP;\n\n\t\tif (atomic_long_try_cmpxchg_release(&lock->owner, &owner, new))\n\t\t\tbreak;\n\t}\n}\n\n#ifndef CONFIG_DEBUG_LOCK_ALLOC\n \nstatic void __sched __mutex_lock_slowpath(struct mutex *lock);\n\n \nvoid __sched mutex_lock(struct mutex *lock)\n{\n\tmight_sleep();\n\n\tif (!__mutex_trylock_fast(lock))\n\t\t__mutex_lock_slowpath(lock);\n}\nEXPORT_SYMBOL(mutex_lock);\n#endif\n\n#include \"ww_mutex.h\"\n\n#ifdef CONFIG_MUTEX_SPIN_ON_OWNER\n\n \nstatic inline struct task_struct *__mutex_trylock_or_owner(struct mutex *lock)\n{\n\treturn __mutex_trylock_common(lock, false);\n}\n\nstatic inline\nbool ww_mutex_spin_on_owner(struct mutex *lock, struct ww_acquire_ctx *ww_ctx,\n\t\t\t    struct mutex_waiter *waiter)\n{\n\tstruct ww_mutex *ww;\n\n\tww = container_of(lock, struct ww_mutex, base);\n\n\t \n\tif (ww_ctx->acquired > 0 && READ_ONCE(ww->ctx))\n\t\treturn false;\n\n\t \n\tif (!waiter && (atomic_long_read(&lock->owner) & MUTEX_FLAG_WAITERS))\n\t\treturn false;\n\n\t \n\tif (waiter && !__mutex_waiter_is_first(lock, waiter))\n\t\treturn false;\n\n\treturn true;\n}\n\n \nstatic noinline\nbool mutex_spin_on_owner(struct mutex *lock, struct task_struct *owner,\n\t\t\t struct ww_acquire_ctx *ww_ctx, struct mutex_waiter *waiter)\n{\n\tbool ret = true;\n\n\tlockdep_assert_preemption_disabled();\n\n\twhile (__mutex_owner(lock) == owner) {\n\t\t \n\t\tbarrier();\n\n\t\t \n\t\tif (!owner_on_cpu(owner) || need_resched()) {\n\t\t\tret = false;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (ww_ctx && !ww_mutex_spin_on_owner(lock, ww_ctx, waiter)) {\n\t\t\tret = false;\n\t\t\tbreak;\n\t\t}\n\n\t\tcpu_relax();\n\t}\n\n\treturn ret;\n}\n\n \nstatic inline int mutex_can_spin_on_owner(struct mutex *lock)\n{\n\tstruct task_struct *owner;\n\tint retval = 1;\n\n\tlockdep_assert_preemption_disabled();\n\n\tif (need_resched())\n\t\treturn 0;\n\n\t \n\towner = __mutex_owner(lock);\n\tif (owner)\n\t\tretval = owner_on_cpu(owner);\n\n\t \n\treturn retval;\n}\n\n \nstatic __always_inline bool\nmutex_optimistic_spin(struct mutex *lock, struct ww_acquire_ctx *ww_ctx,\n\t\t      struct mutex_waiter *waiter)\n{\n\tif (!waiter) {\n\t\t \n\t\tif (!mutex_can_spin_on_owner(lock))\n\t\t\tgoto fail;\n\n\t\t \n\t\tif (!osq_lock(&lock->osq))\n\t\t\tgoto fail;\n\t}\n\n\tfor (;;) {\n\t\tstruct task_struct *owner;\n\n\t\t \n\t\towner = __mutex_trylock_or_owner(lock);\n\t\tif (!owner)\n\t\t\tbreak;\n\n\t\t \n\t\tif (!mutex_spin_on_owner(lock, owner, ww_ctx, waiter))\n\t\t\tgoto fail_unlock;\n\n\t\t \n\t\tcpu_relax();\n\t}\n\n\tif (!waiter)\n\t\tosq_unlock(&lock->osq);\n\n\treturn true;\n\n\nfail_unlock:\n\tif (!waiter)\n\t\tosq_unlock(&lock->osq);\n\nfail:\n\t \n\tif (need_resched()) {\n\t\t \n\t\t__set_current_state(TASK_RUNNING);\n\t\tschedule_preempt_disabled();\n\t}\n\n\treturn false;\n}\n#else\nstatic __always_inline bool\nmutex_optimistic_spin(struct mutex *lock, struct ww_acquire_ctx *ww_ctx,\n\t\t      struct mutex_waiter *waiter)\n{\n\treturn false;\n}\n#endif\n\nstatic noinline void __sched __mutex_unlock_slowpath(struct mutex *lock, unsigned long ip);\n\n \nvoid __sched mutex_unlock(struct mutex *lock)\n{\n#ifndef CONFIG_DEBUG_LOCK_ALLOC\n\tif (__mutex_unlock_fast(lock))\n\t\treturn;\n#endif\n\t__mutex_unlock_slowpath(lock, _RET_IP_);\n}\nEXPORT_SYMBOL(mutex_unlock);\n\n \nvoid __sched ww_mutex_unlock(struct ww_mutex *lock)\n{\n\t__ww_mutex_unlock(lock);\n\tmutex_unlock(&lock->base);\n}\nEXPORT_SYMBOL(ww_mutex_unlock);\n\n \nstatic __always_inline int __sched\n__mutex_lock_common(struct mutex *lock, unsigned int state, unsigned int subclass,\n\t\t    struct lockdep_map *nest_lock, unsigned long ip,\n\t\t    struct ww_acquire_ctx *ww_ctx, const bool use_ww_ctx)\n{\n\tstruct mutex_waiter waiter;\n\tstruct ww_mutex *ww;\n\tint ret;\n\n\tif (!use_ww_ctx)\n\t\tww_ctx = NULL;\n\n\tmight_sleep();\n\n\tMUTEX_WARN_ON(lock->magic != lock);\n\n\tww = container_of(lock, struct ww_mutex, base);\n\tif (ww_ctx) {\n\t\tif (unlikely(ww_ctx == READ_ONCE(ww->ctx)))\n\t\t\treturn -EALREADY;\n\n\t\t \n\t\tif (ww_ctx->acquired == 0)\n\t\t\tww_ctx->wounded = 0;\n\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n\t\tnest_lock = &ww_ctx->dep_map;\n#endif\n\t}\n\n\tpreempt_disable();\n\tmutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, ip);\n\n\ttrace_contention_begin(lock, LCB_F_MUTEX | LCB_F_SPIN);\n\tif (__mutex_trylock(lock) ||\n\t    mutex_optimistic_spin(lock, ww_ctx, NULL)) {\n\t\t \n\t\tlock_acquired(&lock->dep_map, ip);\n\t\tif (ww_ctx)\n\t\t\tww_mutex_set_context_fastpath(ww, ww_ctx);\n\t\ttrace_contention_end(lock, 0);\n\t\tpreempt_enable();\n\t\treturn 0;\n\t}\n\n\traw_spin_lock(&lock->wait_lock);\n\t \n\tif (__mutex_trylock(lock)) {\n\t\tif (ww_ctx)\n\t\t\t__ww_mutex_check_waiters(lock, ww_ctx);\n\n\t\tgoto skip_wait;\n\t}\n\n\tdebug_mutex_lock_common(lock, &waiter);\n\twaiter.task = current;\n\tif (use_ww_ctx)\n\t\twaiter.ww_ctx = ww_ctx;\n\n\tlock_contended(&lock->dep_map, ip);\n\n\tif (!use_ww_ctx) {\n\t\t \n\t\t__mutex_add_waiter(lock, &waiter, &lock->wait_list);\n\t} else {\n\t\t \n\t\tret = __ww_mutex_add_waiter(&waiter, lock, ww_ctx);\n\t\tif (ret)\n\t\t\tgoto err_early_kill;\n\t}\n\n\tset_current_state(state);\n\ttrace_contention_begin(lock, LCB_F_MUTEX);\n\tfor (;;) {\n\t\tbool first;\n\n\t\t \n\t\tif (__mutex_trylock(lock))\n\t\t\tgoto acquired;\n\n\t\t \n\t\tif (signal_pending_state(state, current)) {\n\t\t\tret = -EINTR;\n\t\t\tgoto err;\n\t\t}\n\n\t\tif (ww_ctx) {\n\t\t\tret = __ww_mutex_check_kill(lock, &waiter, ww_ctx);\n\t\t\tif (ret)\n\t\t\t\tgoto err;\n\t\t}\n\n\t\traw_spin_unlock(&lock->wait_lock);\n\t\tschedule_preempt_disabled();\n\n\t\tfirst = __mutex_waiter_is_first(lock, &waiter);\n\n\t\tset_current_state(state);\n\t\t \n\t\tif (__mutex_trylock_or_handoff(lock, first))\n\t\t\tbreak;\n\n\t\tif (first) {\n\t\t\ttrace_contention_begin(lock, LCB_F_MUTEX | LCB_F_SPIN);\n\t\t\tif (mutex_optimistic_spin(lock, ww_ctx, &waiter))\n\t\t\t\tbreak;\n\t\t\ttrace_contention_begin(lock, LCB_F_MUTEX);\n\t\t}\n\n\t\traw_spin_lock(&lock->wait_lock);\n\t}\n\traw_spin_lock(&lock->wait_lock);\nacquired:\n\t__set_current_state(TASK_RUNNING);\n\n\tif (ww_ctx) {\n\t\t \n\t\tif (!ww_ctx->is_wait_die &&\n\t\t    !__mutex_waiter_is_first(lock, &waiter))\n\t\t\t__ww_mutex_check_waiters(lock, ww_ctx);\n\t}\n\n\t__mutex_remove_waiter(lock, &waiter);\n\n\tdebug_mutex_free_waiter(&waiter);\n\nskip_wait:\n\t \n\tlock_acquired(&lock->dep_map, ip);\n\ttrace_contention_end(lock, 0);\n\n\tif (ww_ctx)\n\t\tww_mutex_lock_acquired(ww, ww_ctx);\n\n\traw_spin_unlock(&lock->wait_lock);\n\tpreempt_enable();\n\treturn 0;\n\nerr:\n\t__set_current_state(TASK_RUNNING);\n\t__mutex_remove_waiter(lock, &waiter);\nerr_early_kill:\n\ttrace_contention_end(lock, ret);\n\traw_spin_unlock(&lock->wait_lock);\n\tdebug_mutex_free_waiter(&waiter);\n\tmutex_release(&lock->dep_map, ip);\n\tpreempt_enable();\n\treturn ret;\n}\n\nstatic int __sched\n__mutex_lock(struct mutex *lock, unsigned int state, unsigned int subclass,\n\t     struct lockdep_map *nest_lock, unsigned long ip)\n{\n\treturn __mutex_lock_common(lock, state, subclass, nest_lock, ip, NULL, false);\n}\n\nstatic int __sched\n__ww_mutex_lock(struct mutex *lock, unsigned int state, unsigned int subclass,\n\t\tunsigned long ip, struct ww_acquire_ctx *ww_ctx)\n{\n\treturn __mutex_lock_common(lock, state, subclass, NULL, ip, ww_ctx, true);\n}\n\n \nint ww_mutex_trylock(struct ww_mutex *ww, struct ww_acquire_ctx *ww_ctx)\n{\n\tif (!ww_ctx)\n\t\treturn mutex_trylock(&ww->base);\n\n\tMUTEX_WARN_ON(ww->base.magic != &ww->base);\n\n\t \n\tif (ww_ctx->acquired == 0)\n\t\tww_ctx->wounded = 0;\n\n\tif (__mutex_trylock(&ww->base)) {\n\t\tww_mutex_set_context_fastpath(ww, ww_ctx);\n\t\tmutex_acquire_nest(&ww->base.dep_map, 0, 1, &ww_ctx->dep_map, _RET_IP_);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(ww_mutex_trylock);\n\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\nvoid __sched\nmutex_lock_nested(struct mutex *lock, unsigned int subclass)\n{\n\t__mutex_lock(lock, TASK_UNINTERRUPTIBLE, subclass, NULL, _RET_IP_);\n}\n\nEXPORT_SYMBOL_GPL(mutex_lock_nested);\n\nvoid __sched\n_mutex_lock_nest_lock(struct mutex *lock, struct lockdep_map *nest)\n{\n\t__mutex_lock(lock, TASK_UNINTERRUPTIBLE, 0, nest, _RET_IP_);\n}\nEXPORT_SYMBOL_GPL(_mutex_lock_nest_lock);\n\nint __sched\nmutex_lock_killable_nested(struct mutex *lock, unsigned int subclass)\n{\n\treturn __mutex_lock(lock, TASK_KILLABLE, subclass, NULL, _RET_IP_);\n}\nEXPORT_SYMBOL_GPL(mutex_lock_killable_nested);\n\nint __sched\nmutex_lock_interruptible_nested(struct mutex *lock, unsigned int subclass)\n{\n\treturn __mutex_lock(lock, TASK_INTERRUPTIBLE, subclass, NULL, _RET_IP_);\n}\nEXPORT_SYMBOL_GPL(mutex_lock_interruptible_nested);\n\nvoid __sched\nmutex_lock_io_nested(struct mutex *lock, unsigned int subclass)\n{\n\tint token;\n\n\tmight_sleep();\n\n\ttoken = io_schedule_prepare();\n\t__mutex_lock_common(lock, TASK_UNINTERRUPTIBLE,\n\t\t\t    subclass, NULL, _RET_IP_, NULL, 0);\n\tio_schedule_finish(token);\n}\nEXPORT_SYMBOL_GPL(mutex_lock_io_nested);\n\nstatic inline int\nww_mutex_deadlock_injection(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)\n{\n#ifdef CONFIG_DEBUG_WW_MUTEX_SLOWPATH\n\tunsigned tmp;\n\n\tif (ctx->deadlock_inject_countdown-- == 0) {\n\t\ttmp = ctx->deadlock_inject_interval;\n\t\tif (tmp > UINT_MAX/4)\n\t\t\ttmp = UINT_MAX;\n\t\telse\n\t\t\ttmp = tmp*2 + tmp + tmp/2;\n\n\t\tctx->deadlock_inject_interval = tmp;\n\t\tctx->deadlock_inject_countdown = tmp;\n\t\tctx->contending_lock = lock;\n\n\t\tww_mutex_unlock(lock);\n\n\t\treturn -EDEADLK;\n\t}\n#endif\n\n\treturn 0;\n}\n\nint __sched\nww_mutex_lock(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)\n{\n\tint ret;\n\n\tmight_sleep();\n\tret =  __ww_mutex_lock(&lock->base, TASK_UNINTERRUPTIBLE,\n\t\t\t       0, _RET_IP_, ctx);\n\tif (!ret && ctx && ctx->acquired > 1)\n\t\treturn ww_mutex_deadlock_injection(lock, ctx);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ww_mutex_lock);\n\nint __sched\nww_mutex_lock_interruptible(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)\n{\n\tint ret;\n\n\tmight_sleep();\n\tret = __ww_mutex_lock(&lock->base, TASK_INTERRUPTIBLE,\n\t\t\t      0, _RET_IP_, ctx);\n\n\tif (!ret && ctx && ctx->acquired > 1)\n\t\treturn ww_mutex_deadlock_injection(lock, ctx);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ww_mutex_lock_interruptible);\n\n#endif\n\n \nstatic noinline void __sched __mutex_unlock_slowpath(struct mutex *lock, unsigned long ip)\n{\n\tstruct task_struct *next = NULL;\n\tDEFINE_WAKE_Q(wake_q);\n\tunsigned long owner;\n\n\tmutex_release(&lock->dep_map, ip);\n\n\t \n\towner = atomic_long_read(&lock->owner);\n\tfor (;;) {\n\t\tMUTEX_WARN_ON(__owner_task(owner) != current);\n\t\tMUTEX_WARN_ON(owner & MUTEX_FLAG_PICKUP);\n\n\t\tif (owner & MUTEX_FLAG_HANDOFF)\n\t\t\tbreak;\n\n\t\tif (atomic_long_try_cmpxchg_release(&lock->owner, &owner, __owner_flags(owner))) {\n\t\t\tif (owner & MUTEX_FLAG_WAITERS)\n\t\t\t\tbreak;\n\n\t\t\treturn;\n\t\t}\n\t}\n\n\traw_spin_lock(&lock->wait_lock);\n\tdebug_mutex_unlock(lock);\n\tif (!list_empty(&lock->wait_list)) {\n\t\t \n\t\tstruct mutex_waiter *waiter =\n\t\t\tlist_first_entry(&lock->wait_list,\n\t\t\t\t\t struct mutex_waiter, list);\n\n\t\tnext = waiter->task;\n\n\t\tdebug_mutex_wake_waiter(lock, waiter);\n\t\twake_q_add(&wake_q, next);\n\t}\n\n\tif (owner & MUTEX_FLAG_HANDOFF)\n\t\t__mutex_handoff(lock, next);\n\n\traw_spin_unlock(&lock->wait_lock);\n\n\twake_up_q(&wake_q);\n}\n\n#ifndef CONFIG_DEBUG_LOCK_ALLOC\n \nstatic noinline int __sched\n__mutex_lock_killable_slowpath(struct mutex *lock);\n\nstatic noinline int __sched\n__mutex_lock_interruptible_slowpath(struct mutex *lock);\n\n \nint __sched mutex_lock_interruptible(struct mutex *lock)\n{\n\tmight_sleep();\n\n\tif (__mutex_trylock_fast(lock))\n\t\treturn 0;\n\n\treturn __mutex_lock_interruptible_slowpath(lock);\n}\n\nEXPORT_SYMBOL(mutex_lock_interruptible);\n\n \nint __sched mutex_lock_killable(struct mutex *lock)\n{\n\tmight_sleep();\n\n\tif (__mutex_trylock_fast(lock))\n\t\treturn 0;\n\n\treturn __mutex_lock_killable_slowpath(lock);\n}\nEXPORT_SYMBOL(mutex_lock_killable);\n\n \nvoid __sched mutex_lock_io(struct mutex *lock)\n{\n\tint token;\n\n\ttoken = io_schedule_prepare();\n\tmutex_lock(lock);\n\tio_schedule_finish(token);\n}\nEXPORT_SYMBOL_GPL(mutex_lock_io);\n\nstatic noinline void __sched\n__mutex_lock_slowpath(struct mutex *lock)\n{\n\t__mutex_lock(lock, TASK_UNINTERRUPTIBLE, 0, NULL, _RET_IP_);\n}\n\nstatic noinline int __sched\n__mutex_lock_killable_slowpath(struct mutex *lock)\n{\n\treturn __mutex_lock(lock, TASK_KILLABLE, 0, NULL, _RET_IP_);\n}\n\nstatic noinline int __sched\n__mutex_lock_interruptible_slowpath(struct mutex *lock)\n{\n\treturn __mutex_lock(lock, TASK_INTERRUPTIBLE, 0, NULL, _RET_IP_);\n}\n\nstatic noinline int __sched\n__ww_mutex_lock_slowpath(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)\n{\n\treturn __ww_mutex_lock(&lock->base, TASK_UNINTERRUPTIBLE, 0,\n\t\t\t       _RET_IP_, ctx);\n}\n\nstatic noinline int __sched\n__ww_mutex_lock_interruptible_slowpath(struct ww_mutex *lock,\n\t\t\t\t\t    struct ww_acquire_ctx *ctx)\n{\n\treturn __ww_mutex_lock(&lock->base, TASK_INTERRUPTIBLE, 0,\n\t\t\t       _RET_IP_, ctx);\n}\n\n#endif\n\n \nint __sched mutex_trylock(struct mutex *lock)\n{\n\tbool locked;\n\n\tMUTEX_WARN_ON(lock->magic != lock);\n\n\tlocked = __mutex_trylock(lock);\n\tif (locked)\n\t\tmutex_acquire(&lock->dep_map, 0, 1, _RET_IP_);\n\n\treturn locked;\n}\nEXPORT_SYMBOL(mutex_trylock);\n\n#ifndef CONFIG_DEBUG_LOCK_ALLOC\nint __sched\nww_mutex_lock(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)\n{\n\tmight_sleep();\n\n\tif (__mutex_trylock_fast(&lock->base)) {\n\t\tif (ctx)\n\t\t\tww_mutex_set_context_fastpath(lock, ctx);\n\t\treturn 0;\n\t}\n\n\treturn __ww_mutex_lock_slowpath(lock, ctx);\n}\nEXPORT_SYMBOL(ww_mutex_lock);\n\nint __sched\nww_mutex_lock_interruptible(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)\n{\n\tmight_sleep();\n\n\tif (__mutex_trylock_fast(&lock->base)) {\n\t\tif (ctx)\n\t\t\tww_mutex_set_context_fastpath(lock, ctx);\n\t\treturn 0;\n\t}\n\n\treturn __ww_mutex_lock_interruptible_slowpath(lock, ctx);\n}\nEXPORT_SYMBOL(ww_mutex_lock_interruptible);\n\n#endif  \n#endif  \n\n \nint atomic_dec_and_mutex_lock(atomic_t *cnt, struct mutex *lock)\n{\n\t \n\tif (atomic_add_unless(cnt, -1, 1))\n\t\treturn 0;\n\t \n\tmutex_lock(lock);\n\tif (!atomic_dec_and_test(cnt)) {\n\t\t \n\t\tmutex_unlock(lock);\n\t\treturn 0;\n\t}\n\t \n\treturn 1;\n}\nEXPORT_SYMBOL(atomic_dec_and_mutex_lock);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}