{
  "module_name": "rtmutex.c",
  "hash_id": "3c244e3ddbe10093d3abcd470febab60be65c9281154c13598f54c71b6c2b8b9",
  "original_prompt": "Ingested from linux-6.6.14/kernel/locking/rtmutex.c",
  "human_readable_source": "\n \n#include <linux/sched.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/wake_q.h>\n#include <linux/ww_mutex.h>\n\n#include <trace/events/lock.h>\n\n#include \"rtmutex_common.h\"\n\n#ifndef WW_RT\n# define build_ww_mutex()\t(false)\n# define ww_container_of(rtm)\tNULL\n\nstatic inline int __ww_mutex_add_waiter(struct rt_mutex_waiter *waiter,\n\t\t\t\t\tstruct rt_mutex *lock,\n\t\t\t\t\tstruct ww_acquire_ctx *ww_ctx)\n{\n\treturn 0;\n}\n\nstatic inline void __ww_mutex_check_waiters(struct rt_mutex *lock,\n\t\t\t\t\t    struct ww_acquire_ctx *ww_ctx)\n{\n}\n\nstatic inline void ww_mutex_lock_acquired(struct ww_mutex *lock,\n\t\t\t\t\t  struct ww_acquire_ctx *ww_ctx)\n{\n}\n\nstatic inline int __ww_mutex_check_kill(struct rt_mutex *lock,\n\t\t\t\t\tstruct rt_mutex_waiter *waiter,\n\t\t\t\t\tstruct ww_acquire_ctx *ww_ctx)\n{\n\treturn 0;\n}\n\n#else\n# define build_ww_mutex()\t(true)\n# define ww_container_of(rtm)\tcontainer_of(rtm, struct ww_mutex, base)\n# include \"ww_mutex.h\"\n#endif\n\n \n\nstatic __always_inline struct task_struct *\nrt_mutex_owner_encode(struct rt_mutex_base *lock, struct task_struct *owner)\n{\n\tunsigned long val = (unsigned long)owner;\n\n\tif (rt_mutex_has_waiters(lock))\n\t\tval |= RT_MUTEX_HAS_WAITERS;\n\n\treturn (struct task_struct *)val;\n}\n\nstatic __always_inline void\nrt_mutex_set_owner(struct rt_mutex_base *lock, struct task_struct *owner)\n{\n\t \n\txchg_acquire(&lock->owner, rt_mutex_owner_encode(lock, owner));\n}\n\nstatic __always_inline void rt_mutex_clear_owner(struct rt_mutex_base *lock)\n{\n\t \n\tWRITE_ONCE(lock->owner, rt_mutex_owner_encode(lock, NULL));\n}\n\nstatic __always_inline void clear_rt_mutex_waiters(struct rt_mutex_base *lock)\n{\n\tlock->owner = (struct task_struct *)\n\t\t\t((unsigned long)lock->owner & ~RT_MUTEX_HAS_WAITERS);\n}\n\nstatic __always_inline void\nfixup_rt_mutex_waiters(struct rt_mutex_base *lock, bool acquire_lock)\n{\n\tunsigned long owner, *p = (unsigned long *) &lock->owner;\n\n\tif (rt_mutex_has_waiters(lock))\n\t\treturn;\n\n\t \n\towner = READ_ONCE(*p);\n\tif (owner & RT_MUTEX_HAS_WAITERS) {\n\t\t \n\t\tif (acquire_lock)\n\t\t\txchg_acquire(p, owner & ~RT_MUTEX_HAS_WAITERS);\n\t\telse\n\t\t\tWRITE_ONCE(*p, owner & ~RT_MUTEX_HAS_WAITERS);\n\t}\n}\n\n \n#ifndef CONFIG_DEBUG_RT_MUTEXES\nstatic __always_inline bool rt_mutex_cmpxchg_acquire(struct rt_mutex_base *lock,\n\t\t\t\t\t\t     struct task_struct *old,\n\t\t\t\t\t\t     struct task_struct *new)\n{\n\treturn try_cmpxchg_acquire(&lock->owner, &old, new);\n}\n\nstatic __always_inline bool rt_mutex_cmpxchg_release(struct rt_mutex_base *lock,\n\t\t\t\t\t\t     struct task_struct *old,\n\t\t\t\t\t\t     struct task_struct *new)\n{\n\treturn try_cmpxchg_release(&lock->owner, &old, new);\n}\n\n \nstatic __always_inline void mark_rt_mutex_waiters(struct rt_mutex_base *lock)\n{\n\tunsigned long owner, *p = (unsigned long *) &lock->owner;\n\n\tdo {\n\t\towner = *p;\n\t} while (cmpxchg_relaxed(p, owner,\n\t\t\t\t owner | RT_MUTEX_HAS_WAITERS) != owner);\n\n\t \n\tsmp_mb__after_atomic();\n}\n\n \nstatic __always_inline bool unlock_rt_mutex_safe(struct rt_mutex_base *lock,\n\t\t\t\t\t\t unsigned long flags)\n\t__releases(lock->wait_lock)\n{\n\tstruct task_struct *owner = rt_mutex_owner(lock);\n\n\tclear_rt_mutex_waiters(lock);\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\t \n\treturn rt_mutex_cmpxchg_release(lock, owner, NULL);\n}\n\n#else\nstatic __always_inline bool rt_mutex_cmpxchg_acquire(struct rt_mutex_base *lock,\n\t\t\t\t\t\t     struct task_struct *old,\n\t\t\t\t\t\t     struct task_struct *new)\n{\n\treturn false;\n\n}\n\nstatic __always_inline bool rt_mutex_cmpxchg_release(struct rt_mutex_base *lock,\n\t\t\t\t\t\t     struct task_struct *old,\n\t\t\t\t\t\t     struct task_struct *new)\n{\n\treturn false;\n}\n\nstatic __always_inline void mark_rt_mutex_waiters(struct rt_mutex_base *lock)\n{\n\tlock->owner = (struct task_struct *)\n\t\t\t((unsigned long)lock->owner | RT_MUTEX_HAS_WAITERS);\n}\n\n \nstatic __always_inline bool unlock_rt_mutex_safe(struct rt_mutex_base *lock,\n\t\t\t\t\t\t unsigned long flags)\n\t__releases(lock->wait_lock)\n{\n\tlock->owner = NULL;\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\treturn true;\n}\n#endif\n\nstatic __always_inline int __waiter_prio(struct task_struct *task)\n{\n\tint prio = task->prio;\n\n\tif (!rt_prio(prio))\n\t\treturn DEFAULT_PRIO;\n\n\treturn prio;\n}\n\n \nstatic __always_inline void\nwaiter_update_prio(struct rt_mutex_waiter *waiter, struct task_struct *task)\n{\n\tlockdep_assert_held(&waiter->lock->wait_lock);\n\tlockdep_assert(RB_EMPTY_NODE(&waiter->tree.entry));\n\n\twaiter->tree.prio = __waiter_prio(task);\n\twaiter->tree.deadline = task->dl.deadline;\n}\n\n \nstatic __always_inline void\nwaiter_clone_prio(struct rt_mutex_waiter *waiter, struct task_struct *task)\n{\n\tlockdep_assert_held(&waiter->lock->wait_lock);\n\tlockdep_assert_held(&task->pi_lock);\n\tlockdep_assert(RB_EMPTY_NODE(&waiter->pi_tree.entry));\n\n\twaiter->pi_tree.prio = waiter->tree.prio;\n\twaiter->pi_tree.deadline = waiter->tree.deadline;\n}\n\n \n#define task_to_waiter_node(p)\t\\\n\t&(struct rt_waiter_node){ .prio = __waiter_prio(p), .deadline = (p)->dl.deadline }\n#define task_to_waiter(p)\t\\\n\t&(struct rt_mutex_waiter){ .tree = *task_to_waiter_node(p) }\n\nstatic __always_inline int rt_waiter_node_less(struct rt_waiter_node *left,\n\t\t\t\t\t       struct rt_waiter_node *right)\n{\n\tif (left->prio < right->prio)\n\t\treturn 1;\n\n\t \n\tif (dl_prio(left->prio))\n\t\treturn dl_time_before(left->deadline, right->deadline);\n\n\treturn 0;\n}\n\nstatic __always_inline int rt_waiter_node_equal(struct rt_waiter_node *left,\n\t\t\t\t\t\t struct rt_waiter_node *right)\n{\n\tif (left->prio != right->prio)\n\t\treturn 0;\n\n\t \n\tif (dl_prio(left->prio))\n\t\treturn left->deadline == right->deadline;\n\n\treturn 1;\n}\n\nstatic inline bool rt_mutex_steal(struct rt_mutex_waiter *waiter,\n\t\t\t\t  struct rt_mutex_waiter *top_waiter)\n{\n\tif (rt_waiter_node_less(&waiter->tree, &top_waiter->tree))\n\t\treturn true;\n\n#ifdef RT_MUTEX_BUILD_SPINLOCKS\n\t \n\tif (rt_prio(waiter->tree.prio) || dl_prio(waiter->tree.prio))\n\t\treturn false;\n\n\treturn rt_waiter_node_equal(&waiter->tree, &top_waiter->tree);\n#else\n\treturn false;\n#endif\n}\n\n#define __node_2_waiter(node) \\\n\trb_entry((node), struct rt_mutex_waiter, tree.entry)\n\nstatic __always_inline bool __waiter_less(struct rb_node *a, const struct rb_node *b)\n{\n\tstruct rt_mutex_waiter *aw = __node_2_waiter(a);\n\tstruct rt_mutex_waiter *bw = __node_2_waiter(b);\n\n\tif (rt_waiter_node_less(&aw->tree, &bw->tree))\n\t\treturn 1;\n\n\tif (!build_ww_mutex())\n\t\treturn 0;\n\n\tif (rt_waiter_node_less(&bw->tree, &aw->tree))\n\t\treturn 0;\n\n\t \n\tif (aw->ww_ctx) {\n\t\tif (!bw->ww_ctx)\n\t\t\treturn 1;\n\n\t\treturn (signed long)(aw->ww_ctx->stamp -\n\t\t\t\t     bw->ww_ctx->stamp) < 0;\n\t}\n\n\treturn 0;\n}\n\nstatic __always_inline void\nrt_mutex_enqueue(struct rt_mutex_base *lock, struct rt_mutex_waiter *waiter)\n{\n\tlockdep_assert_held(&lock->wait_lock);\n\n\trb_add_cached(&waiter->tree.entry, &lock->waiters, __waiter_less);\n}\n\nstatic __always_inline void\nrt_mutex_dequeue(struct rt_mutex_base *lock, struct rt_mutex_waiter *waiter)\n{\n\tlockdep_assert_held(&lock->wait_lock);\n\n\tif (RB_EMPTY_NODE(&waiter->tree.entry))\n\t\treturn;\n\n\trb_erase_cached(&waiter->tree.entry, &lock->waiters);\n\tRB_CLEAR_NODE(&waiter->tree.entry);\n}\n\n#define __node_2_rt_node(node) \\\n\trb_entry((node), struct rt_waiter_node, entry)\n\nstatic __always_inline bool __pi_waiter_less(struct rb_node *a, const struct rb_node *b)\n{\n\treturn rt_waiter_node_less(__node_2_rt_node(a), __node_2_rt_node(b));\n}\n\nstatic __always_inline void\nrt_mutex_enqueue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\tlockdep_assert_held(&task->pi_lock);\n\n\trb_add_cached(&waiter->pi_tree.entry, &task->pi_waiters, __pi_waiter_less);\n}\n\nstatic __always_inline void\nrt_mutex_dequeue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\n{\n\tlockdep_assert_held(&task->pi_lock);\n\n\tif (RB_EMPTY_NODE(&waiter->pi_tree.entry))\n\t\treturn;\n\n\trb_erase_cached(&waiter->pi_tree.entry, &task->pi_waiters);\n\tRB_CLEAR_NODE(&waiter->pi_tree.entry);\n}\n\nstatic __always_inline void rt_mutex_adjust_prio(struct rt_mutex_base *lock,\n\t\t\t\t\t\t struct task_struct *p)\n{\n\tstruct task_struct *pi_task = NULL;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\tlockdep_assert(rt_mutex_owner(lock) == p);\n\tlockdep_assert_held(&p->pi_lock);\n\n\tif (task_has_pi_waiters(p))\n\t\tpi_task = task_top_pi_waiter(p)->task;\n\n\trt_mutex_setprio(p, pi_task);\n}\n\n \nstatic __always_inline void rt_mutex_wake_q_add_task(struct rt_wake_q_head *wqh,\n\t\t\t\t\t\t     struct task_struct *task,\n\t\t\t\t\t\t     unsigned int wake_state)\n{\n\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && wake_state == TASK_RTLOCK_WAIT) {\n\t\tif (IS_ENABLED(CONFIG_PROVE_LOCKING))\n\t\t\tWARN_ON_ONCE(wqh->rtlock_task);\n\t\tget_task_struct(task);\n\t\twqh->rtlock_task = task;\n\t} else {\n\t\twake_q_add(&wqh->head, task);\n\t}\n}\n\nstatic __always_inline void rt_mutex_wake_q_add(struct rt_wake_q_head *wqh,\n\t\t\t\t\t\tstruct rt_mutex_waiter *w)\n{\n\trt_mutex_wake_q_add_task(wqh, w->task, w->wake_state);\n}\n\nstatic __always_inline void rt_mutex_wake_up_q(struct rt_wake_q_head *wqh)\n{\n\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && wqh->rtlock_task) {\n\t\twake_up_state(wqh->rtlock_task, TASK_RTLOCK_WAIT);\n\t\tput_task_struct(wqh->rtlock_task);\n\t\twqh->rtlock_task = NULL;\n\t}\n\n\tif (!wake_q_empty(&wqh->head))\n\t\twake_up_q(&wqh->head);\n\n\t \n\tpreempt_enable();\n}\n\n \nstatic __always_inline bool\nrt_mutex_cond_detect_deadlock(struct rt_mutex_waiter *waiter,\n\t\t\t      enum rtmutex_chainwalk chwalk)\n{\n\tif (IS_ENABLED(CONFIG_DEBUG_RT_MUTEXES))\n\t\treturn waiter != NULL;\n\treturn chwalk == RT_MUTEX_FULL_CHAINWALK;\n}\n\nstatic __always_inline struct rt_mutex_base *task_blocked_on_lock(struct task_struct *p)\n{\n\treturn p->pi_blocked_on ? p->pi_blocked_on->lock : NULL;\n}\n\n \nstatic int __sched rt_mutex_adjust_prio_chain(struct task_struct *task,\n\t\t\t\t\t      enum rtmutex_chainwalk chwalk,\n\t\t\t\t\t      struct rt_mutex_base *orig_lock,\n\t\t\t\t\t      struct rt_mutex_base *next_lock,\n\t\t\t\t\t      struct rt_mutex_waiter *orig_waiter,\n\t\t\t\t\t      struct task_struct *top_task)\n{\n\tstruct rt_mutex_waiter *waiter, *top_waiter = orig_waiter;\n\tstruct rt_mutex_waiter *prerequeue_top_waiter;\n\tint ret = 0, depth = 0;\n\tstruct rt_mutex_base *lock;\n\tbool detect_deadlock;\n\tbool requeue = true;\n\n\tdetect_deadlock = rt_mutex_cond_detect_deadlock(orig_waiter, chwalk);\n\n\t \n again:\n\t \n\tif (++depth > max_lock_depth) {\n\t\tstatic int prev_max;\n\n\t\t \n\t\tif (prev_max != max_lock_depth) {\n\t\t\tprev_max = max_lock_depth;\n\t\t\tprintk(KERN_WARNING \"Maximum lock depth %d reached \"\n\t\t\t       \"task: %s (%d)\\n\", max_lock_depth,\n\t\t\t       top_task->comm, task_pid_nr(top_task));\n\t\t}\n\t\tput_task_struct(task);\n\n\t\treturn -EDEADLK;\n\t}\n\n\t \n retry:\n\t \n\traw_spin_lock_irq(&task->pi_lock);\n\n\t \n\twaiter = task->pi_blocked_on;\n\n\t \n\n\t \n\tif (!waiter)\n\t\tgoto out_unlock_pi;\n\n\t \n\tif (orig_waiter && !rt_mutex_owner(orig_lock))\n\t\tgoto out_unlock_pi;\n\n\t \n\tif (next_lock != waiter->lock)\n\t\tgoto out_unlock_pi;\n\n\t \n\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && waiter->ww_ctx && detect_deadlock)\n\t\tdetect_deadlock = false;\n\n\t \n\tif (top_waiter) {\n\t\tif (!task_has_pi_waiters(task))\n\t\t\tgoto out_unlock_pi;\n\t\t \n\t\tif (top_waiter != task_top_pi_waiter(task)) {\n\t\t\tif (!detect_deadlock)\n\t\t\t\tgoto out_unlock_pi;\n\t\t\telse\n\t\t\t\trequeue = false;\n\t\t}\n\t}\n\n\t \n\tif (rt_waiter_node_equal(&waiter->tree, task_to_waiter_node(task))) {\n\t\tif (!detect_deadlock)\n\t\t\tgoto out_unlock_pi;\n\t\telse\n\t\t\trequeue = false;\n\t}\n\n\t \n\tlock = waiter->lock;\n\t \n\tif (!raw_spin_trylock(&lock->wait_lock)) {\n\t\traw_spin_unlock_irq(&task->pi_lock);\n\t\tcpu_relax();\n\t\tgoto retry;\n\t}\n\n\t \n\tif (lock == orig_lock || rt_mutex_owner(lock) == top_task) {\n\t\tret = -EDEADLK;\n\n\t\t \n\t\tif (IS_ENABLED(CONFIG_PREEMPT_RT) && orig_waiter && orig_waiter->ww_ctx)\n\t\t\tret = 0;\n\n\t\traw_spin_unlock(&lock->wait_lock);\n\t\tgoto out_unlock_pi;\n\t}\n\n\t \n\tif (!requeue) {\n\t\t \n\t\traw_spin_unlock(&task->pi_lock);\n\t\tput_task_struct(task);\n\n\t\t \n\t\tif (!rt_mutex_owner(lock)) {\n\t\t\traw_spin_unlock_irq(&lock->wait_lock);\n\t\t\treturn 0;\n\t\t}\n\n\t\t \n\t\ttask = get_task_struct(rt_mutex_owner(lock));\n\t\traw_spin_lock(&task->pi_lock);\n\n\t\t \n\t\tnext_lock = task_blocked_on_lock(task);\n\t\t \n\t\ttop_waiter = rt_mutex_top_waiter(lock);\n\n\t\t \n\t\traw_spin_unlock(&task->pi_lock);\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t\t \n\t\tif (!next_lock)\n\t\t\tgoto out_put_task;\n\t\tgoto again;\n\t}\n\n\t \n\tprerequeue_top_waiter = rt_mutex_top_waiter(lock);\n\n\t \n\trt_mutex_dequeue(lock, waiter);\n\n\t \n\twaiter_update_prio(waiter, task);\n\n\trt_mutex_enqueue(lock, waiter);\n\n\t \n\traw_spin_unlock(&task->pi_lock);\n\tput_task_struct(task);\n\n\t \n\tif (!rt_mutex_owner(lock)) {\n\t\t \n\t\ttop_waiter = rt_mutex_top_waiter(lock);\n\t\tif (prerequeue_top_waiter != top_waiter)\n\t\t\twake_up_state(top_waiter->task, top_waiter->wake_state);\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\t\treturn 0;\n\t}\n\n\t \n\ttask = get_task_struct(rt_mutex_owner(lock));\n\traw_spin_lock(&task->pi_lock);\n\n\t \n\tif (waiter == rt_mutex_top_waiter(lock)) {\n\t\t \n\t\trt_mutex_dequeue_pi(task, prerequeue_top_waiter);\n\t\twaiter_clone_prio(waiter, task);\n\t\trt_mutex_enqueue_pi(task, waiter);\n\t\trt_mutex_adjust_prio(lock, task);\n\n\t} else if (prerequeue_top_waiter == waiter) {\n\t\t \n\t\trt_mutex_dequeue_pi(task, waiter);\n\t\twaiter = rt_mutex_top_waiter(lock);\n\t\twaiter_clone_prio(waiter, task);\n\t\trt_mutex_enqueue_pi(task, waiter);\n\t\trt_mutex_adjust_prio(lock, task);\n\t} else {\n\t\t \n\t}\n\n\t \n\tnext_lock = task_blocked_on_lock(task);\n\t \n\ttop_waiter = rt_mutex_top_waiter(lock);\n\n\t \n\traw_spin_unlock(&task->pi_lock);\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t \n\tif (!next_lock)\n\t\tgoto out_put_task;\n\n\t \n\tif (!detect_deadlock && waiter != top_waiter)\n\t\tgoto out_put_task;\n\n\tgoto again;\n\n out_unlock_pi:\n\traw_spin_unlock_irq(&task->pi_lock);\n out_put_task:\n\tput_task_struct(task);\n\n\treturn ret;\n}\n\n \nstatic int __sched\ntry_to_take_rt_mutex(struct rt_mutex_base *lock, struct task_struct *task,\n\t\t     struct rt_mutex_waiter *waiter)\n{\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t \n\tmark_rt_mutex_waiters(lock);\n\n\t \n\tif (rt_mutex_owner(lock))\n\t\treturn 0;\n\n\t \n\tif (waiter) {\n\t\tstruct rt_mutex_waiter *top_waiter = rt_mutex_top_waiter(lock);\n\n\t\t \n\t\tif (waiter == top_waiter || rt_mutex_steal(waiter, top_waiter)) {\n\t\t\t \n\t\t\trt_mutex_dequeue(lock, waiter);\n\t\t} else {\n\t\t\treturn 0;\n\t\t}\n\t} else {\n\t\t \n\t\tif (rt_mutex_has_waiters(lock)) {\n\t\t\t \n\t\t\tif (!rt_mutex_steal(task_to_waiter(task),\n\t\t\t\t\t    rt_mutex_top_waiter(lock)))\n\t\t\t\treturn 0;\n\n\t\t\t \n\t\t} else {\n\t\t\t \n\t\t\tgoto takeit;\n\t\t}\n\t}\n\n\t \n\traw_spin_lock(&task->pi_lock);\n\ttask->pi_blocked_on = NULL;\n\t \n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(task, rt_mutex_top_waiter(lock));\n\traw_spin_unlock(&task->pi_lock);\n\ntakeit:\n\t \n\trt_mutex_set_owner(lock, task);\n\n\treturn 1;\n}\n\n \nstatic int __sched task_blocks_on_rt_mutex(struct rt_mutex_base *lock,\n\t\t\t\t\t   struct rt_mutex_waiter *waiter,\n\t\t\t\t\t   struct task_struct *task,\n\t\t\t\t\t   struct ww_acquire_ctx *ww_ctx,\n\t\t\t\t\t   enum rtmutex_chainwalk chwalk)\n{\n\tstruct task_struct *owner = rt_mutex_owner(lock);\n\tstruct rt_mutex_waiter *top_waiter = waiter;\n\tstruct rt_mutex_base *next_lock;\n\tint chain_walk = 0, res;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t \n\tif (owner == task && !(build_ww_mutex() && ww_ctx))\n\t\treturn -EDEADLK;\n\n\traw_spin_lock(&task->pi_lock);\n\twaiter->task = task;\n\twaiter->lock = lock;\n\twaiter_update_prio(waiter, task);\n\twaiter_clone_prio(waiter, task);\n\n\t \n\tif (rt_mutex_has_waiters(lock))\n\t\ttop_waiter = rt_mutex_top_waiter(lock);\n\trt_mutex_enqueue(lock, waiter);\n\n\ttask->pi_blocked_on = waiter;\n\n\traw_spin_unlock(&task->pi_lock);\n\n\tif (build_ww_mutex() && ww_ctx) {\n\t\tstruct rt_mutex *rtm;\n\n\t\t \n\t\trtm = container_of(lock, struct rt_mutex, rtmutex);\n\t\tres = __ww_mutex_add_waiter(waiter, rtm, ww_ctx);\n\t\tif (res) {\n\t\t\traw_spin_lock(&task->pi_lock);\n\t\t\trt_mutex_dequeue(lock, waiter);\n\t\t\ttask->pi_blocked_on = NULL;\n\t\t\traw_spin_unlock(&task->pi_lock);\n\t\t\treturn res;\n\t\t}\n\t}\n\n\tif (!owner)\n\t\treturn 0;\n\n\traw_spin_lock(&owner->pi_lock);\n\tif (waiter == rt_mutex_top_waiter(lock)) {\n\t\trt_mutex_dequeue_pi(owner, top_waiter);\n\t\trt_mutex_enqueue_pi(owner, waiter);\n\n\t\trt_mutex_adjust_prio(lock, owner);\n\t\tif (owner->pi_blocked_on)\n\t\t\tchain_walk = 1;\n\t} else if (rt_mutex_cond_detect_deadlock(waiter, chwalk)) {\n\t\tchain_walk = 1;\n\t}\n\n\t \n\tnext_lock = task_blocked_on_lock(owner);\n\n\traw_spin_unlock(&owner->pi_lock);\n\t \n\tif (!chain_walk || !next_lock)\n\t\treturn 0;\n\n\t \n\tget_task_struct(owner);\n\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\tres = rt_mutex_adjust_prio_chain(owner, chwalk, lock,\n\t\t\t\t\t next_lock, waiter, task);\n\n\traw_spin_lock_irq(&lock->wait_lock);\n\n\treturn res;\n}\n\n \nstatic void __sched mark_wakeup_next_waiter(struct rt_wake_q_head *wqh,\n\t\t\t\t\t    struct rt_mutex_base *lock)\n{\n\tstruct rt_mutex_waiter *waiter;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\traw_spin_lock(&current->pi_lock);\n\n\twaiter = rt_mutex_top_waiter(lock);\n\n\t \n\trt_mutex_dequeue_pi(current, waiter);\n\trt_mutex_adjust_prio(lock, current);\n\n\t \n\tlock->owner = (void *) RT_MUTEX_HAS_WAITERS;\n\n\t \n\tpreempt_disable();\n\trt_mutex_wake_q_add(wqh, waiter);\n\traw_spin_unlock(&current->pi_lock);\n}\n\nstatic int __sched __rt_mutex_slowtrylock(struct rt_mutex_base *lock)\n{\n\tint ret = try_to_take_rt_mutex(lock, current, NULL);\n\n\t \n\tfixup_rt_mutex_waiters(lock, true);\n\n\treturn ret;\n}\n\n \nstatic int __sched rt_mutex_slowtrylock(struct rt_mutex_base *lock)\n{\n\tunsigned long flags;\n\tint ret;\n\n\t \n\tif (rt_mutex_owner(lock))\n\t\treturn 0;\n\n\t \n\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\n\tret = __rt_mutex_slowtrylock(lock);\n\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\n\treturn ret;\n}\n\nstatic __always_inline int __rt_mutex_trylock(struct rt_mutex_base *lock)\n{\n\tif (likely(rt_mutex_cmpxchg_acquire(lock, NULL, current)))\n\t\treturn 1;\n\n\treturn rt_mutex_slowtrylock(lock);\n}\n\n \nstatic void __sched rt_mutex_slowunlock(struct rt_mutex_base *lock)\n{\n\tDEFINE_RT_WAKE_Q(wqh);\n\tunsigned long flags;\n\n\t \n\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\n\tdebug_rt_mutex_unlock(lock);\n\n\t \n\twhile (!rt_mutex_has_waiters(lock)) {\n\t\t \n\t\tif (unlock_rt_mutex_safe(lock, flags) == true)\n\t\t\treturn;\n\t\t \n\t\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\t}\n\n\t \n\tmark_wakeup_next_waiter(&wqh, lock);\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\n\trt_mutex_wake_up_q(&wqh);\n}\n\nstatic __always_inline void __rt_mutex_unlock(struct rt_mutex_base *lock)\n{\n\tif (likely(rt_mutex_cmpxchg_release(lock, current, NULL)))\n\t\treturn;\n\n\trt_mutex_slowunlock(lock);\n}\n\n#ifdef CONFIG_SMP\nstatic bool rtmutex_spin_on_owner(struct rt_mutex_base *lock,\n\t\t\t\t  struct rt_mutex_waiter *waiter,\n\t\t\t\t  struct task_struct *owner)\n{\n\tbool res = true;\n\n\trcu_read_lock();\n\tfor (;;) {\n\t\t \n\t\tif (owner != rt_mutex_owner(lock))\n\t\t\tbreak;\n\t\t \n\t\tbarrier();\n\t\t \n\t\tif (!owner_on_cpu(owner) || need_resched() ||\n\t\t    !rt_mutex_waiter_is_top_waiter(lock, waiter)) {\n\t\t\tres = false;\n\t\t\tbreak;\n\t\t}\n\t\tcpu_relax();\n\t}\n\trcu_read_unlock();\n\treturn res;\n}\n#else\nstatic bool rtmutex_spin_on_owner(struct rt_mutex_base *lock,\n\t\t\t\t  struct rt_mutex_waiter *waiter,\n\t\t\t\t  struct task_struct *owner)\n{\n\treturn false;\n}\n#endif\n\n#ifdef RT_MUTEX_BUILD_MUTEX\n \n\n \nstatic void __sched remove_waiter(struct rt_mutex_base *lock,\n\t\t\t\t  struct rt_mutex_waiter *waiter)\n{\n\tbool is_top_waiter = (waiter == rt_mutex_top_waiter(lock));\n\tstruct task_struct *owner = rt_mutex_owner(lock);\n\tstruct rt_mutex_base *next_lock;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\traw_spin_lock(&current->pi_lock);\n\trt_mutex_dequeue(lock, waiter);\n\tcurrent->pi_blocked_on = NULL;\n\traw_spin_unlock(&current->pi_lock);\n\n\t \n\tif (!owner || !is_top_waiter)\n\t\treturn;\n\n\traw_spin_lock(&owner->pi_lock);\n\n\trt_mutex_dequeue_pi(owner, waiter);\n\n\tif (rt_mutex_has_waiters(lock))\n\t\trt_mutex_enqueue_pi(owner, rt_mutex_top_waiter(lock));\n\n\trt_mutex_adjust_prio(lock, owner);\n\n\t \n\tnext_lock = task_blocked_on_lock(owner);\n\n\traw_spin_unlock(&owner->pi_lock);\n\n\t \n\tif (!next_lock)\n\t\treturn;\n\n\t \n\tget_task_struct(owner);\n\n\traw_spin_unlock_irq(&lock->wait_lock);\n\n\trt_mutex_adjust_prio_chain(owner, RT_MUTEX_MIN_CHAINWALK, lock,\n\t\t\t\t   next_lock, NULL, current);\n\n\traw_spin_lock_irq(&lock->wait_lock);\n}\n\n \nstatic int __sched rt_mutex_slowlock_block(struct rt_mutex_base *lock,\n\t\t\t\t\t   struct ww_acquire_ctx *ww_ctx,\n\t\t\t\t\t   unsigned int state,\n\t\t\t\t\t   struct hrtimer_sleeper *timeout,\n\t\t\t\t\t   struct rt_mutex_waiter *waiter)\n{\n\tstruct rt_mutex *rtm = container_of(lock, struct rt_mutex, rtmutex);\n\tstruct task_struct *owner;\n\tint ret = 0;\n\n\tfor (;;) {\n\t\t \n\t\tif (try_to_take_rt_mutex(lock, current, waiter))\n\t\t\tbreak;\n\n\t\tif (timeout && !timeout->task) {\n\t\t\tret = -ETIMEDOUT;\n\t\t\tbreak;\n\t\t}\n\t\tif (signal_pending_state(state, current)) {\n\t\t\tret = -EINTR;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (build_ww_mutex() && ww_ctx) {\n\t\t\tret = __ww_mutex_check_kill(rtm, waiter, ww_ctx);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (waiter == rt_mutex_top_waiter(lock))\n\t\t\towner = rt_mutex_owner(lock);\n\t\telse\n\t\t\towner = NULL;\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t\tif (!owner || !rtmutex_spin_on_owner(lock, waiter, owner))\n\t\t\tschedule();\n\n\t\traw_spin_lock_irq(&lock->wait_lock);\n\t\tset_current_state(state);\n\t}\n\n\t__set_current_state(TASK_RUNNING);\n\treturn ret;\n}\n\nstatic void __sched rt_mutex_handle_deadlock(int res, int detect_deadlock,\n\t\t\t\t\t     struct rt_mutex_waiter *w)\n{\n\t \n\tif (res != -EDEADLOCK || detect_deadlock)\n\t\treturn;\n\n\tif (build_ww_mutex() && w->ww_ctx)\n\t\treturn;\n\n\t \n\tWARN(1, \"rtmutex deadlock detected\\n\");\n\twhile (1) {\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\tschedule();\n\t}\n}\n\n \nstatic int __sched __rt_mutex_slowlock(struct rt_mutex_base *lock,\n\t\t\t\t       struct ww_acquire_ctx *ww_ctx,\n\t\t\t\t       unsigned int state,\n\t\t\t\t       enum rtmutex_chainwalk chwalk,\n\t\t\t\t       struct rt_mutex_waiter *waiter)\n{\n\tstruct rt_mutex *rtm = container_of(lock, struct rt_mutex, rtmutex);\n\tstruct ww_mutex *ww = ww_container_of(rtm);\n\tint ret;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\t \n\tif (try_to_take_rt_mutex(lock, current, NULL)) {\n\t\tif (build_ww_mutex() && ww_ctx) {\n\t\t\t__ww_mutex_check_waiters(rtm, ww_ctx);\n\t\t\tww_mutex_lock_acquired(ww, ww_ctx);\n\t\t}\n\t\treturn 0;\n\t}\n\n\tset_current_state(state);\n\n\ttrace_contention_begin(lock, LCB_F_RT);\n\n\tret = task_blocks_on_rt_mutex(lock, waiter, current, ww_ctx, chwalk);\n\tif (likely(!ret))\n\t\tret = rt_mutex_slowlock_block(lock, ww_ctx, state, NULL, waiter);\n\n\tif (likely(!ret)) {\n\t\t \n\t\tif (build_ww_mutex() && ww_ctx) {\n\t\t\tif (!ww_ctx->is_wait_die)\n\t\t\t\t__ww_mutex_check_waiters(rtm, ww_ctx);\n\t\t\tww_mutex_lock_acquired(ww, ww_ctx);\n\t\t}\n\t} else {\n\t\t__set_current_state(TASK_RUNNING);\n\t\tremove_waiter(lock, waiter);\n\t\trt_mutex_handle_deadlock(ret, chwalk, waiter);\n\t}\n\n\t \n\tfixup_rt_mutex_waiters(lock, true);\n\n\ttrace_contention_end(lock, ret);\n\n\treturn ret;\n}\n\nstatic inline int __rt_mutex_slowlock_locked(struct rt_mutex_base *lock,\n\t\t\t\t\t     struct ww_acquire_ctx *ww_ctx,\n\t\t\t\t\t     unsigned int state)\n{\n\tstruct rt_mutex_waiter waiter;\n\tint ret;\n\n\trt_mutex_init_waiter(&waiter);\n\twaiter.ww_ctx = ww_ctx;\n\n\tret = __rt_mutex_slowlock(lock, ww_ctx, state, RT_MUTEX_MIN_CHAINWALK,\n\t\t\t\t  &waiter);\n\n\tdebug_rt_mutex_free_waiter(&waiter);\n\treturn ret;\n}\n\n \nstatic int __sched rt_mutex_slowlock(struct rt_mutex_base *lock,\n\t\t\t\t     struct ww_acquire_ctx *ww_ctx,\n\t\t\t\t     unsigned int state)\n{\n\tunsigned long flags;\n\tint ret;\n\n\t \n\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\tret = __rt_mutex_slowlock_locked(lock, ww_ctx, state);\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n\n\treturn ret;\n}\n\nstatic __always_inline int __rt_mutex_lock(struct rt_mutex_base *lock,\n\t\t\t\t\t   unsigned int state)\n{\n\tif (likely(rt_mutex_cmpxchg_acquire(lock, NULL, current)))\n\t\treturn 0;\n\n\treturn rt_mutex_slowlock(lock, NULL, state);\n}\n#endif  \n\n#ifdef RT_MUTEX_BUILD_SPINLOCKS\n \n\n \nstatic void __sched rtlock_slowlock_locked(struct rt_mutex_base *lock)\n{\n\tstruct rt_mutex_waiter waiter;\n\tstruct task_struct *owner;\n\n\tlockdep_assert_held(&lock->wait_lock);\n\n\tif (try_to_take_rt_mutex(lock, current, NULL))\n\t\treturn;\n\n\trt_mutex_init_rtlock_waiter(&waiter);\n\n\t \n\tcurrent_save_and_set_rtlock_wait_state();\n\n\ttrace_contention_begin(lock, LCB_F_RT);\n\n\ttask_blocks_on_rt_mutex(lock, &waiter, current, NULL, RT_MUTEX_MIN_CHAINWALK);\n\n\tfor (;;) {\n\t\t \n\t\tif (try_to_take_rt_mutex(lock, current, &waiter))\n\t\t\tbreak;\n\n\t\tif (&waiter == rt_mutex_top_waiter(lock))\n\t\t\towner = rt_mutex_owner(lock);\n\t\telse\n\t\t\towner = NULL;\n\t\traw_spin_unlock_irq(&lock->wait_lock);\n\n\t\tif (!owner || !rtmutex_spin_on_owner(lock, &waiter, owner))\n\t\t\tschedule_rtlock();\n\n\t\traw_spin_lock_irq(&lock->wait_lock);\n\t\tset_current_state(TASK_RTLOCK_WAIT);\n\t}\n\n\t \n\tcurrent_restore_rtlock_saved_state();\n\n\t \n\tfixup_rt_mutex_waiters(lock, true);\n\tdebug_rt_mutex_free_waiter(&waiter);\n\n\ttrace_contention_end(lock, 0);\n}\n\nstatic __always_inline void __sched rtlock_slowlock(struct rt_mutex_base *lock)\n{\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&lock->wait_lock, flags);\n\trtlock_slowlock_locked(lock);\n\traw_spin_unlock_irqrestore(&lock->wait_lock, flags);\n}\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}