{
  "module_name": "rwsem.c",
  "hash_id": "21838884d0146b4428876f4e26ac43a169fcff7d703712af03f610ee0a8d2063",
  "original_prompt": "Ingested from linux-6.6.14/kernel/locking/rwsem.c",
  "human_readable_source": "\n \n\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/sched.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/task.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/wake_q.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/clock.h>\n#include <linux/export.h>\n#include <linux/rwsem.h>\n#include <linux/atomic.h>\n#include <trace/events/lock.h>\n\n#ifndef CONFIG_PREEMPT_RT\n#include \"lock_events.h\"\n\n \n#define RWSEM_READER_OWNED\t(1UL << 0)\n#define RWSEM_NONSPINNABLE\t(1UL << 1)\n#define RWSEM_OWNER_FLAGS_MASK\t(RWSEM_READER_OWNED | RWSEM_NONSPINNABLE)\n\n#ifdef CONFIG_DEBUG_RWSEMS\n# define DEBUG_RWSEMS_WARN_ON(c, sem)\tdo {\t\t\t\\\n\tif (!debug_locks_silent &&\t\t\t\t\\\n\t    WARN_ONCE(c, \"DEBUG_RWSEMS_WARN_ON(%s): count = 0x%lx, magic = 0x%lx, owner = 0x%lx, curr 0x%lx, list %sempty\\n\",\\\n\t\t#c, atomic_long_read(&(sem)->count),\t\t\\\n\t\t(unsigned long) sem->magic,\t\t\t\\\n\t\tatomic_long_read(&(sem)->owner), (long)current,\t\\\n\t\tlist_empty(&(sem)->wait_list) ? \"\" : \"not \"))\t\\\n\t\t\tdebug_locks_off();\t\t\t\\\n\t} while (0)\n#else\n# define DEBUG_RWSEMS_WARN_ON(c, sem)\n#endif\n\n \n#define RWSEM_WRITER_LOCKED\t(1UL << 0)\n#define RWSEM_FLAG_WAITERS\t(1UL << 1)\n#define RWSEM_FLAG_HANDOFF\t(1UL << 2)\n#define RWSEM_FLAG_READFAIL\t(1UL << (BITS_PER_LONG - 1))\n\n#define RWSEM_READER_SHIFT\t8\n#define RWSEM_READER_BIAS\t(1UL << RWSEM_READER_SHIFT)\n#define RWSEM_READER_MASK\t(~(RWSEM_READER_BIAS - 1))\n#define RWSEM_WRITER_MASK\tRWSEM_WRITER_LOCKED\n#define RWSEM_LOCK_MASK\t\t(RWSEM_WRITER_MASK|RWSEM_READER_MASK)\n#define RWSEM_READ_FAILED_MASK\t(RWSEM_WRITER_MASK|RWSEM_FLAG_WAITERS|\\\n\t\t\t\t RWSEM_FLAG_HANDOFF|RWSEM_FLAG_READFAIL)\n\n \nstatic inline void rwsem_set_owner(struct rw_semaphore *sem)\n{\n\tlockdep_assert_preemption_disabled();\n\tatomic_long_set(&sem->owner, (long)current);\n}\n\nstatic inline void rwsem_clear_owner(struct rw_semaphore *sem)\n{\n\tlockdep_assert_preemption_disabled();\n\tatomic_long_set(&sem->owner, 0);\n}\n\n \nstatic inline bool rwsem_test_oflags(struct rw_semaphore *sem, long flags)\n{\n\treturn atomic_long_read(&sem->owner) & flags;\n}\n\n \nstatic inline void __rwsem_set_reader_owned(struct rw_semaphore *sem,\n\t\t\t\t\t    struct task_struct *owner)\n{\n\tunsigned long val = (unsigned long)owner | RWSEM_READER_OWNED |\n\t\t(atomic_long_read(&sem->owner) & RWSEM_NONSPINNABLE);\n\n\tatomic_long_set(&sem->owner, val);\n}\n\nstatic inline void rwsem_set_reader_owned(struct rw_semaphore *sem)\n{\n\t__rwsem_set_reader_owned(sem, current);\n}\n\n \nstatic inline bool is_rwsem_reader_owned(struct rw_semaphore *sem)\n{\n#ifdef CONFIG_DEBUG_RWSEMS\n\t \n\tlong count = atomic_long_read(&sem->count);\n\n\tif (count & RWSEM_WRITER_MASK)\n\t\treturn false;\n#endif\n\treturn rwsem_test_oflags(sem, RWSEM_READER_OWNED);\n}\n\n#ifdef CONFIG_DEBUG_RWSEMS\n \nstatic inline void rwsem_clear_reader_owned(struct rw_semaphore *sem)\n{\n\tunsigned long val = atomic_long_read(&sem->owner);\n\n\twhile ((val & ~RWSEM_OWNER_FLAGS_MASK) == (unsigned long)current) {\n\t\tif (atomic_long_try_cmpxchg(&sem->owner, &val,\n\t\t\t\t\t    val & RWSEM_OWNER_FLAGS_MASK))\n\t\t\treturn;\n\t}\n}\n#else\nstatic inline void rwsem_clear_reader_owned(struct rw_semaphore *sem)\n{\n}\n#endif\n\n \nstatic inline void rwsem_set_nonspinnable(struct rw_semaphore *sem)\n{\n\tunsigned long owner = atomic_long_read(&sem->owner);\n\n\tdo {\n\t\tif (!(owner & RWSEM_READER_OWNED))\n\t\t\tbreak;\n\t\tif (owner & RWSEM_NONSPINNABLE)\n\t\t\tbreak;\n\t} while (!atomic_long_try_cmpxchg(&sem->owner, &owner,\n\t\t\t\t\t  owner | RWSEM_NONSPINNABLE));\n}\n\nstatic inline bool rwsem_read_trylock(struct rw_semaphore *sem, long *cntp)\n{\n\t*cntp = atomic_long_add_return_acquire(RWSEM_READER_BIAS, &sem->count);\n\n\tif (WARN_ON_ONCE(*cntp < 0))\n\t\trwsem_set_nonspinnable(sem);\n\n\tif (!(*cntp & RWSEM_READ_FAILED_MASK)) {\n\t\trwsem_set_reader_owned(sem);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic inline bool rwsem_write_trylock(struct rw_semaphore *sem)\n{\n\tlong tmp = RWSEM_UNLOCKED_VALUE;\n\n\tif (atomic_long_try_cmpxchg_acquire(&sem->count, &tmp, RWSEM_WRITER_LOCKED)) {\n\t\trwsem_set_owner(sem);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nstatic inline struct task_struct *rwsem_owner(struct rw_semaphore *sem)\n{\n\treturn (struct task_struct *)\n\t\t(atomic_long_read(&sem->owner) & ~RWSEM_OWNER_FLAGS_MASK);\n}\n\n \nstatic inline struct task_struct *\nrwsem_owner_flags(struct rw_semaphore *sem, unsigned long *pflags)\n{\n\tunsigned long owner = atomic_long_read(&sem->owner);\n\n\t*pflags = owner & RWSEM_OWNER_FLAGS_MASK;\n\treturn (struct task_struct *)(owner & ~RWSEM_OWNER_FLAGS_MASK);\n}\n\n \n\n \nvoid __init_rwsem(struct rw_semaphore *sem, const char *name,\n\t\t  struct lock_class_key *key)\n{\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n\t \n\tdebug_check_no_locks_freed((void *)sem, sizeof(*sem));\n\tlockdep_init_map_wait(&sem->dep_map, name, key, 0, LD_WAIT_SLEEP);\n#endif\n#ifdef CONFIG_DEBUG_RWSEMS\n\tsem->magic = sem;\n#endif\n\tatomic_long_set(&sem->count, RWSEM_UNLOCKED_VALUE);\n\traw_spin_lock_init(&sem->wait_lock);\n\tINIT_LIST_HEAD(&sem->wait_list);\n\tatomic_long_set(&sem->owner, 0L);\n#ifdef CONFIG_RWSEM_SPIN_ON_OWNER\n\tosq_lock_init(&sem->osq);\n#endif\n}\nEXPORT_SYMBOL(__init_rwsem);\n\nenum rwsem_waiter_type {\n\tRWSEM_WAITING_FOR_WRITE,\n\tRWSEM_WAITING_FOR_READ\n};\n\nstruct rwsem_waiter {\n\tstruct list_head list;\n\tstruct task_struct *task;\n\tenum rwsem_waiter_type type;\n\tunsigned long timeout;\n\tbool handoff_set;\n};\n#define rwsem_first_waiter(sem) \\\n\tlist_first_entry(&sem->wait_list, struct rwsem_waiter, list)\n\nenum rwsem_wake_type {\n\tRWSEM_WAKE_ANY,\t\t \n\tRWSEM_WAKE_READERS,\t \n\tRWSEM_WAKE_READ_OWNED\t \n};\n\n \n#define RWSEM_WAIT_TIMEOUT\tDIV_ROUND_UP(HZ, 250)\n\n \n#define MAX_READERS_WAKEUP\t0x100\n\nstatic inline void\nrwsem_add_waiter(struct rw_semaphore *sem, struct rwsem_waiter *waiter)\n{\n\tlockdep_assert_held(&sem->wait_lock);\n\tlist_add_tail(&waiter->list, &sem->wait_list);\n\t \n}\n\n \nstatic inline bool\nrwsem_del_waiter(struct rw_semaphore *sem, struct rwsem_waiter *waiter)\n{\n\tlockdep_assert_held(&sem->wait_lock);\n\tlist_del(&waiter->list);\n\tif (likely(!list_empty(&sem->wait_list)))\n\t\treturn true;\n\n\tatomic_long_andnot(RWSEM_FLAG_HANDOFF | RWSEM_FLAG_WAITERS, &sem->count);\n\treturn false;\n}\n\n \nstatic void rwsem_mark_wake(struct rw_semaphore *sem,\n\t\t\t    enum rwsem_wake_type wake_type,\n\t\t\t    struct wake_q_head *wake_q)\n{\n\tstruct rwsem_waiter *waiter, *tmp;\n\tlong oldcount, woken = 0, adjustment = 0;\n\tstruct list_head wlist;\n\n\tlockdep_assert_held(&sem->wait_lock);\n\n\t \n\twaiter = rwsem_first_waiter(sem);\n\n\tif (waiter->type == RWSEM_WAITING_FOR_WRITE) {\n\t\tif (wake_type == RWSEM_WAKE_ANY) {\n\t\t\t \n\t\t\twake_q_add(wake_q, waiter->task);\n\t\t\tlockevent_inc(rwsem_wake_writer);\n\t\t}\n\n\t\treturn;\n\t}\n\n\t \n\tif (unlikely(atomic_long_read(&sem->count) < 0))\n\t\treturn;\n\n\t \n\tif (wake_type != RWSEM_WAKE_READ_OWNED) {\n\t\tstruct task_struct *owner;\n\n\t\tadjustment = RWSEM_READER_BIAS;\n\t\toldcount = atomic_long_fetch_add(adjustment, &sem->count);\n\t\tif (unlikely(oldcount & RWSEM_WRITER_MASK)) {\n\t\t\t \n\t\t\tif (time_after(jiffies, waiter->timeout)) {\n\t\t\t\tif (!(oldcount & RWSEM_FLAG_HANDOFF)) {\n\t\t\t\t\tadjustment -= RWSEM_FLAG_HANDOFF;\n\t\t\t\t\tlockevent_inc(rwsem_rlock_handoff);\n\t\t\t\t}\n\t\t\t\twaiter->handoff_set = true;\n\t\t\t}\n\n\t\t\tatomic_long_add(-adjustment, &sem->count);\n\t\t\treturn;\n\t\t}\n\t\t \n\t\towner = waiter->task;\n\t\t__rwsem_set_reader_owned(sem, owner);\n\t}\n\n\t \n\tINIT_LIST_HEAD(&wlist);\n\tlist_for_each_entry_safe(waiter, tmp, &sem->wait_list, list) {\n\t\tif (waiter->type == RWSEM_WAITING_FOR_WRITE)\n\t\t\tcontinue;\n\n\t\twoken++;\n\t\tlist_move_tail(&waiter->list, &wlist);\n\n\t\t \n\t\tif (unlikely(woken >= MAX_READERS_WAKEUP))\n\t\t\tbreak;\n\t}\n\n\tadjustment = woken * RWSEM_READER_BIAS - adjustment;\n\tlockevent_cond_inc(rwsem_wake_reader, woken);\n\n\toldcount = atomic_long_read(&sem->count);\n\tif (list_empty(&sem->wait_list)) {\n\t\t \n\t\tadjustment -= RWSEM_FLAG_WAITERS;\n\t\tif (oldcount & RWSEM_FLAG_HANDOFF)\n\t\t\tadjustment -= RWSEM_FLAG_HANDOFF;\n\t} else if (woken) {\n\t\t \n\t\tif (oldcount & RWSEM_FLAG_HANDOFF)\n\t\t\tadjustment -= RWSEM_FLAG_HANDOFF;\n\t}\n\n\tif (adjustment)\n\t\tatomic_long_add(adjustment, &sem->count);\n\n\t \n\tlist_for_each_entry_safe(waiter, tmp, &wlist, list) {\n\t\tstruct task_struct *tsk;\n\n\t\ttsk = waiter->task;\n\t\tget_task_struct(tsk);\n\n\t\t \n\t\tsmp_store_release(&waiter->task, NULL);\n\t\t \n\t\twake_q_add_safe(wake_q, tsk);\n\t}\n}\n\n \nstatic inline void\nrwsem_del_wake_waiter(struct rw_semaphore *sem, struct rwsem_waiter *waiter,\n\t\t      struct wake_q_head *wake_q)\n\t\t      __releases(&sem->wait_lock)\n{\n\tbool first = rwsem_first_waiter(sem) == waiter;\n\n\twake_q_init(wake_q);\n\n\t \n\tif (rwsem_del_waiter(sem, waiter) && first)\n\t\trwsem_mark_wake(sem, RWSEM_WAKE_ANY, wake_q);\n\traw_spin_unlock_irq(&sem->wait_lock);\n\tif (!wake_q_empty(wake_q))\n\t\twake_up_q(wake_q);\n}\n\n \nstatic inline bool rwsem_try_write_lock(struct rw_semaphore *sem,\n\t\t\t\t\tstruct rwsem_waiter *waiter)\n{\n\tstruct rwsem_waiter *first = rwsem_first_waiter(sem);\n\tlong count, new;\n\n\tlockdep_assert_held(&sem->wait_lock);\n\n\tcount = atomic_long_read(&sem->count);\n\tdo {\n\t\tbool has_handoff = !!(count & RWSEM_FLAG_HANDOFF);\n\n\t\tif (has_handoff) {\n\t\t\t \n\t\t\tif (first->handoff_set && (waiter != first))\n\t\t\t\treturn false;\n\t\t}\n\n\t\tnew = count;\n\n\t\tif (count & RWSEM_LOCK_MASK) {\n\t\t\t \n\t\t\tif (has_handoff || (!rt_task(waiter->task) &&\n\t\t\t\t\t    !time_after(jiffies, waiter->timeout)))\n\t\t\t\treturn false;\n\n\t\t\tnew |= RWSEM_FLAG_HANDOFF;\n\t\t} else {\n\t\t\tnew |= RWSEM_WRITER_LOCKED;\n\t\t\tnew &= ~RWSEM_FLAG_HANDOFF;\n\n\t\t\tif (list_is_singular(&sem->wait_list))\n\t\t\t\tnew &= ~RWSEM_FLAG_WAITERS;\n\t\t}\n\t} while (!atomic_long_try_cmpxchg_acquire(&sem->count, &count, new));\n\n\t \n\tif (new & RWSEM_FLAG_HANDOFF) {\n\t\tfirst->handoff_set = true;\n\t\tlockevent_inc(rwsem_wlock_handoff);\n\t\treturn false;\n\t}\n\n\t \n\tlist_del(&waiter->list);\n\trwsem_set_owner(sem);\n\treturn true;\n}\n\n \nenum owner_state {\n\tOWNER_NULL\t\t= 1 << 0,\n\tOWNER_WRITER\t\t= 1 << 1,\n\tOWNER_READER\t\t= 1 << 2,\n\tOWNER_NONSPINNABLE\t= 1 << 3,\n};\n\n#ifdef CONFIG_RWSEM_SPIN_ON_OWNER\n \nstatic inline bool rwsem_try_write_lock_unqueued(struct rw_semaphore *sem)\n{\n\tlong count = atomic_long_read(&sem->count);\n\n\twhile (!(count & (RWSEM_LOCK_MASK|RWSEM_FLAG_HANDOFF))) {\n\t\tif (atomic_long_try_cmpxchg_acquire(&sem->count, &count,\n\t\t\t\t\tcount | RWSEM_WRITER_LOCKED)) {\n\t\t\trwsem_set_owner(sem);\n\t\t\tlockevent_inc(rwsem_opt_lock);\n\t\t\treturn true;\n\t\t}\n\t}\n\treturn false;\n}\n\nstatic inline bool rwsem_can_spin_on_owner(struct rw_semaphore *sem)\n{\n\tstruct task_struct *owner;\n\tunsigned long flags;\n\tbool ret = true;\n\n\tif (need_resched()) {\n\t\tlockevent_inc(rwsem_opt_fail);\n\t\treturn false;\n\t}\n\n\t \n\towner = rwsem_owner_flags(sem, &flags);\n\t \n\tif ((flags & RWSEM_NONSPINNABLE) ||\n\t    (owner && !(flags & RWSEM_READER_OWNED) && !owner_on_cpu(owner)))\n\t\tret = false;\n\n\tlockevent_cond_inc(rwsem_opt_fail, !ret);\n\treturn ret;\n}\n\n#define OWNER_SPINNABLE\t\t(OWNER_NULL | OWNER_WRITER | OWNER_READER)\n\nstatic inline enum owner_state\nrwsem_owner_state(struct task_struct *owner, unsigned long flags)\n{\n\tif (flags & RWSEM_NONSPINNABLE)\n\t\treturn OWNER_NONSPINNABLE;\n\n\tif (flags & RWSEM_READER_OWNED)\n\t\treturn OWNER_READER;\n\n\treturn owner ? OWNER_WRITER : OWNER_NULL;\n}\n\nstatic noinline enum owner_state\nrwsem_spin_on_owner(struct rw_semaphore *sem)\n{\n\tstruct task_struct *new, *owner;\n\tunsigned long flags, new_flags;\n\tenum owner_state state;\n\n\tlockdep_assert_preemption_disabled();\n\n\towner = rwsem_owner_flags(sem, &flags);\n\tstate = rwsem_owner_state(owner, flags);\n\tif (state != OWNER_WRITER)\n\t\treturn state;\n\n\tfor (;;) {\n\t\t \n\t\tnew = rwsem_owner_flags(sem, &new_flags);\n\t\tif ((new != owner) || (new_flags != flags)) {\n\t\t\tstate = rwsem_owner_state(new, new_flags);\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tbarrier();\n\n\t\tif (need_resched() || !owner_on_cpu(owner)) {\n\t\t\tstate = OWNER_NONSPINNABLE;\n\t\t\tbreak;\n\t\t}\n\n\t\tcpu_relax();\n\t}\n\n\treturn state;\n}\n\n \nstatic inline u64 rwsem_rspin_threshold(struct rw_semaphore *sem)\n{\n\tlong count = atomic_long_read(&sem->count);\n\tint readers = count >> RWSEM_READER_SHIFT;\n\tu64 delta;\n\n\tif (readers > 30)\n\t\treaders = 30;\n\tdelta = (20 + readers) * NSEC_PER_USEC / 2;\n\n\treturn sched_clock() + delta;\n}\n\nstatic bool rwsem_optimistic_spin(struct rw_semaphore *sem)\n{\n\tbool taken = false;\n\tint prev_owner_state = OWNER_NULL;\n\tint loop = 0;\n\tu64 rspin_threshold = 0;\n\n\t \n\tif (!osq_lock(&sem->osq))\n\t\tgoto done;\n\n\t \n\tfor (;;) {\n\t\tenum owner_state owner_state;\n\n\t\towner_state = rwsem_spin_on_owner(sem);\n\t\tif (!(owner_state & OWNER_SPINNABLE))\n\t\t\tbreak;\n\n\t\t \n\t\ttaken = rwsem_try_write_lock_unqueued(sem);\n\n\t\tif (taken)\n\t\t\tbreak;\n\n\t\t \n\t\tif (owner_state == OWNER_READER) {\n\t\t\t \n\t\t\tif (prev_owner_state != OWNER_READER) {\n\t\t\t\tif (rwsem_test_oflags(sem, RWSEM_NONSPINNABLE))\n\t\t\t\t\tbreak;\n\t\t\t\trspin_threshold = rwsem_rspin_threshold(sem);\n\t\t\t\tloop = 0;\n\t\t\t}\n\n\t\t\t \n\t\t\telse if (!(++loop & 0xf) && (sched_clock() > rspin_threshold)) {\n\t\t\t\trwsem_set_nonspinnable(sem);\n\t\t\t\tlockevent_inc(rwsem_opt_nospin);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (owner_state != OWNER_WRITER) {\n\t\t\tif (need_resched())\n\t\t\t\tbreak;\n\t\t\tif (rt_task(current) &&\n\t\t\t   (prev_owner_state != OWNER_WRITER))\n\t\t\t\tbreak;\n\t\t}\n\t\tprev_owner_state = owner_state;\n\n\t\t \n\t\tcpu_relax();\n\t}\n\tosq_unlock(&sem->osq);\ndone:\n\tlockevent_cond_inc(rwsem_opt_fail, !taken);\n\treturn taken;\n}\n\n \nstatic inline void clear_nonspinnable(struct rw_semaphore *sem)\n{\n\tif (unlikely(rwsem_test_oflags(sem, RWSEM_NONSPINNABLE)))\n\t\tatomic_long_andnot(RWSEM_NONSPINNABLE, &sem->owner);\n}\n\n#else\nstatic inline bool rwsem_can_spin_on_owner(struct rw_semaphore *sem)\n{\n\treturn false;\n}\n\nstatic inline bool rwsem_optimistic_spin(struct rw_semaphore *sem)\n{\n\treturn false;\n}\n\nstatic inline void clear_nonspinnable(struct rw_semaphore *sem) { }\n\nstatic inline enum owner_state\nrwsem_spin_on_owner(struct rw_semaphore *sem)\n{\n\treturn OWNER_NONSPINNABLE;\n}\n#endif\n\n \nstatic inline void rwsem_cond_wake_waiter(struct rw_semaphore *sem, long count,\n\t\t\t\t\t  struct wake_q_head *wake_q)\n{\n\tenum rwsem_wake_type wake_type;\n\n\tif (count & RWSEM_WRITER_MASK)\n\t\treturn;\n\n\tif (count & RWSEM_READER_MASK) {\n\t\twake_type = RWSEM_WAKE_READERS;\n\t} else {\n\t\twake_type = RWSEM_WAKE_ANY;\n\t\tclear_nonspinnable(sem);\n\t}\n\trwsem_mark_wake(sem, wake_type, wake_q);\n}\n\n \nstatic struct rw_semaphore __sched *\nrwsem_down_read_slowpath(struct rw_semaphore *sem, long count, unsigned int state)\n{\n\tlong adjustment = -RWSEM_READER_BIAS;\n\tlong rcnt = (count >> RWSEM_READER_SHIFT);\n\tstruct rwsem_waiter waiter;\n\tDEFINE_WAKE_Q(wake_q);\n\n\t \n\tif ((atomic_long_read(&sem->owner) & RWSEM_READER_OWNED) &&\n\t    (rcnt > 1) && !(count & RWSEM_WRITER_LOCKED))\n\t\tgoto queue;\n\n\t \n\tif (!(count & (RWSEM_WRITER_LOCKED | RWSEM_FLAG_HANDOFF))) {\n\t\trwsem_set_reader_owned(sem);\n\t\tlockevent_inc(rwsem_rlock_steal);\n\n\t\t \n\t\tif ((rcnt == 1) && (count & RWSEM_FLAG_WAITERS)) {\n\t\t\traw_spin_lock_irq(&sem->wait_lock);\n\t\t\tif (!list_empty(&sem->wait_list))\n\t\t\t\trwsem_mark_wake(sem, RWSEM_WAKE_READ_OWNED,\n\t\t\t\t\t\t&wake_q);\n\t\t\traw_spin_unlock_irq(&sem->wait_lock);\n\t\t\twake_up_q(&wake_q);\n\t\t}\n\t\treturn sem;\n\t}\n\nqueue:\n\twaiter.task = current;\n\twaiter.type = RWSEM_WAITING_FOR_READ;\n\twaiter.timeout = jiffies + RWSEM_WAIT_TIMEOUT;\n\twaiter.handoff_set = false;\n\n\traw_spin_lock_irq(&sem->wait_lock);\n\tif (list_empty(&sem->wait_list)) {\n\t\t \n\t\tif (!(atomic_long_read(&sem->count) & RWSEM_WRITER_MASK)) {\n\t\t\t \n\t\t\tsmp_acquire__after_ctrl_dep();\n\t\t\traw_spin_unlock_irq(&sem->wait_lock);\n\t\t\trwsem_set_reader_owned(sem);\n\t\t\tlockevent_inc(rwsem_rlock_fast);\n\t\t\treturn sem;\n\t\t}\n\t\tadjustment += RWSEM_FLAG_WAITERS;\n\t}\n\trwsem_add_waiter(sem, &waiter);\n\n\t \n\tcount = atomic_long_add_return(adjustment, &sem->count);\n\n\trwsem_cond_wake_waiter(sem, count, &wake_q);\n\traw_spin_unlock_irq(&sem->wait_lock);\n\n\tif (!wake_q_empty(&wake_q))\n\t\twake_up_q(&wake_q);\n\n\ttrace_contention_begin(sem, LCB_F_READ);\n\n\t \n\tfor (;;) {\n\t\tset_current_state(state);\n\t\tif (!smp_load_acquire(&waiter.task)) {\n\t\t\t \n\t\t\tbreak;\n\t\t}\n\t\tif (signal_pending_state(state, current)) {\n\t\t\traw_spin_lock_irq(&sem->wait_lock);\n\t\t\tif (waiter.task)\n\t\t\t\tgoto out_nolock;\n\t\t\traw_spin_unlock_irq(&sem->wait_lock);\n\t\t\t \n\t\t\tbreak;\n\t\t}\n\t\tschedule_preempt_disabled();\n\t\tlockevent_inc(rwsem_sleep_reader);\n\t}\n\n\t__set_current_state(TASK_RUNNING);\n\tlockevent_inc(rwsem_rlock);\n\ttrace_contention_end(sem, 0);\n\treturn sem;\n\nout_nolock:\n\trwsem_del_wake_waiter(sem, &waiter, &wake_q);\n\t__set_current_state(TASK_RUNNING);\n\tlockevent_inc(rwsem_rlock_fail);\n\ttrace_contention_end(sem, -EINTR);\n\treturn ERR_PTR(-EINTR);\n}\n\n \nstatic struct rw_semaphore __sched *\nrwsem_down_write_slowpath(struct rw_semaphore *sem, int state)\n{\n\tstruct rwsem_waiter waiter;\n\tDEFINE_WAKE_Q(wake_q);\n\n\t \n\tif (rwsem_can_spin_on_owner(sem) && rwsem_optimistic_spin(sem)) {\n\t\t \n\t\treturn sem;\n\t}\n\n\t \n\twaiter.task = current;\n\twaiter.type = RWSEM_WAITING_FOR_WRITE;\n\twaiter.timeout = jiffies + RWSEM_WAIT_TIMEOUT;\n\twaiter.handoff_set = false;\n\n\traw_spin_lock_irq(&sem->wait_lock);\n\trwsem_add_waiter(sem, &waiter);\n\n\t \n\tif (rwsem_first_waiter(sem) != &waiter) {\n\t\trwsem_cond_wake_waiter(sem, atomic_long_read(&sem->count),\n\t\t\t\t       &wake_q);\n\t\tif (!wake_q_empty(&wake_q)) {\n\t\t\t \n\t\t\traw_spin_unlock_irq(&sem->wait_lock);\n\t\t\twake_up_q(&wake_q);\n\t\t\traw_spin_lock_irq(&sem->wait_lock);\n\t\t}\n\t} else {\n\t\tatomic_long_or(RWSEM_FLAG_WAITERS, &sem->count);\n\t}\n\n\t \n\tset_current_state(state);\n\ttrace_contention_begin(sem, LCB_F_WRITE);\n\n\tfor (;;) {\n\t\tif (rwsem_try_write_lock(sem, &waiter)) {\n\t\t\t \n\t\t\tbreak;\n\t\t}\n\n\t\traw_spin_unlock_irq(&sem->wait_lock);\n\n\t\tif (signal_pending_state(state, current))\n\t\t\tgoto out_nolock;\n\n\t\t \n\t\tif (waiter.handoff_set) {\n\t\t\tenum owner_state owner_state;\n\n\t\t\towner_state = rwsem_spin_on_owner(sem);\n\t\t\tif (owner_state == OWNER_NULL)\n\t\t\t\tgoto trylock_again;\n\t\t}\n\n\t\tschedule_preempt_disabled();\n\t\tlockevent_inc(rwsem_sleep_writer);\n\t\tset_current_state(state);\ntrylock_again:\n\t\traw_spin_lock_irq(&sem->wait_lock);\n\t}\n\t__set_current_state(TASK_RUNNING);\n\traw_spin_unlock_irq(&sem->wait_lock);\n\tlockevent_inc(rwsem_wlock);\n\ttrace_contention_end(sem, 0);\n\treturn sem;\n\nout_nolock:\n\t__set_current_state(TASK_RUNNING);\n\traw_spin_lock_irq(&sem->wait_lock);\n\trwsem_del_wake_waiter(sem, &waiter, &wake_q);\n\tlockevent_inc(rwsem_wlock_fail);\n\ttrace_contention_end(sem, -EINTR);\n\treturn ERR_PTR(-EINTR);\n}\n\n \nstatic struct rw_semaphore *rwsem_wake(struct rw_semaphore *sem)\n{\n\tunsigned long flags;\n\tDEFINE_WAKE_Q(wake_q);\n\n\traw_spin_lock_irqsave(&sem->wait_lock, flags);\n\n\tif (!list_empty(&sem->wait_list))\n\t\trwsem_mark_wake(sem, RWSEM_WAKE_ANY, &wake_q);\n\n\traw_spin_unlock_irqrestore(&sem->wait_lock, flags);\n\twake_up_q(&wake_q);\n\n\treturn sem;\n}\n\n \nstatic struct rw_semaphore *rwsem_downgrade_wake(struct rw_semaphore *sem)\n{\n\tunsigned long flags;\n\tDEFINE_WAKE_Q(wake_q);\n\n\traw_spin_lock_irqsave(&sem->wait_lock, flags);\n\n\tif (!list_empty(&sem->wait_list))\n\t\trwsem_mark_wake(sem, RWSEM_WAKE_READ_OWNED, &wake_q);\n\n\traw_spin_unlock_irqrestore(&sem->wait_lock, flags);\n\twake_up_q(&wake_q);\n\n\treturn sem;\n}\n\n \nstatic __always_inline int __down_read_common(struct rw_semaphore *sem, int state)\n{\n\tint ret = 0;\n\tlong count;\n\n\tpreempt_disable();\n\tif (!rwsem_read_trylock(sem, &count)) {\n\t\tif (IS_ERR(rwsem_down_read_slowpath(sem, count, state))) {\n\t\t\tret = -EINTR;\n\t\t\tgoto out;\n\t\t}\n\t\tDEBUG_RWSEMS_WARN_ON(!is_rwsem_reader_owned(sem), sem);\n\t}\nout:\n\tpreempt_enable();\n\treturn ret;\n}\n\nstatic __always_inline void __down_read(struct rw_semaphore *sem)\n{\n\t__down_read_common(sem, TASK_UNINTERRUPTIBLE);\n}\n\nstatic __always_inline int __down_read_interruptible(struct rw_semaphore *sem)\n{\n\treturn __down_read_common(sem, TASK_INTERRUPTIBLE);\n}\n\nstatic __always_inline int __down_read_killable(struct rw_semaphore *sem)\n{\n\treturn __down_read_common(sem, TASK_KILLABLE);\n}\n\nstatic inline int __down_read_trylock(struct rw_semaphore *sem)\n{\n\tint ret = 0;\n\tlong tmp;\n\n\tDEBUG_RWSEMS_WARN_ON(sem->magic != sem, sem);\n\n\tpreempt_disable();\n\ttmp = atomic_long_read(&sem->count);\n\twhile (!(tmp & RWSEM_READ_FAILED_MASK)) {\n\t\tif (atomic_long_try_cmpxchg_acquire(&sem->count, &tmp,\n\t\t\t\t\t\t    tmp + RWSEM_READER_BIAS)) {\n\t\t\trwsem_set_reader_owned(sem);\n\t\t\tret = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\tpreempt_enable();\n\treturn ret;\n}\n\n \nstatic inline int __down_write_common(struct rw_semaphore *sem, int state)\n{\n\tint ret = 0;\n\n\tpreempt_disable();\n\tif (unlikely(!rwsem_write_trylock(sem))) {\n\t\tif (IS_ERR(rwsem_down_write_slowpath(sem, state)))\n\t\t\tret = -EINTR;\n\t}\n\tpreempt_enable();\n\treturn ret;\n}\n\nstatic inline void __down_write(struct rw_semaphore *sem)\n{\n\t__down_write_common(sem, TASK_UNINTERRUPTIBLE);\n}\n\nstatic inline int __down_write_killable(struct rw_semaphore *sem)\n{\n\treturn __down_write_common(sem, TASK_KILLABLE);\n}\n\nstatic inline int __down_write_trylock(struct rw_semaphore *sem)\n{\n\tint ret;\n\n\tpreempt_disable();\n\tDEBUG_RWSEMS_WARN_ON(sem->magic != sem, sem);\n\tret = rwsem_write_trylock(sem);\n\tpreempt_enable();\n\n\treturn ret;\n}\n\n \nstatic inline void __up_read(struct rw_semaphore *sem)\n{\n\tlong tmp;\n\n\tDEBUG_RWSEMS_WARN_ON(sem->magic != sem, sem);\n\tDEBUG_RWSEMS_WARN_ON(!is_rwsem_reader_owned(sem), sem);\n\n\tpreempt_disable();\n\trwsem_clear_reader_owned(sem);\n\ttmp = atomic_long_add_return_release(-RWSEM_READER_BIAS, &sem->count);\n\tDEBUG_RWSEMS_WARN_ON(tmp < 0, sem);\n\tif (unlikely((tmp & (RWSEM_LOCK_MASK|RWSEM_FLAG_WAITERS)) ==\n\t\t      RWSEM_FLAG_WAITERS)) {\n\t\tclear_nonspinnable(sem);\n\t\trwsem_wake(sem);\n\t}\n\tpreempt_enable();\n}\n\n \nstatic inline void __up_write(struct rw_semaphore *sem)\n{\n\tlong tmp;\n\n\tDEBUG_RWSEMS_WARN_ON(sem->magic != sem, sem);\n\t \n\tDEBUG_RWSEMS_WARN_ON((rwsem_owner(sem) != current) &&\n\t\t\t    !rwsem_test_oflags(sem, RWSEM_NONSPINNABLE), sem);\n\n\tpreempt_disable();\n\trwsem_clear_owner(sem);\n\ttmp = atomic_long_fetch_add_release(-RWSEM_WRITER_LOCKED, &sem->count);\n\tif (unlikely(tmp & RWSEM_FLAG_WAITERS))\n\t\trwsem_wake(sem);\n\tpreempt_enable();\n}\n\n \nstatic inline void __downgrade_write(struct rw_semaphore *sem)\n{\n\tlong tmp;\n\n\t \n\tDEBUG_RWSEMS_WARN_ON(rwsem_owner(sem) != current, sem);\n\tpreempt_disable();\n\ttmp = atomic_long_fetch_add_release(\n\t\t-RWSEM_WRITER_LOCKED+RWSEM_READER_BIAS, &sem->count);\n\trwsem_set_reader_owned(sem);\n\tif (tmp & RWSEM_FLAG_WAITERS)\n\t\trwsem_downgrade_wake(sem);\n\tpreempt_enable();\n}\n\n#else  \n\n#define RT_MUTEX_BUILD_MUTEX\n#include \"rtmutex.c\"\n\n#define rwbase_set_and_save_current_state(state)\t\\\n\tset_current_state(state)\n\n#define rwbase_restore_current_state()\t\t\t\\\n\t__set_current_state(TASK_RUNNING)\n\n#define rwbase_rtmutex_lock_state(rtm, state)\t\t\\\n\t__rt_mutex_lock(rtm, state)\n\n#define rwbase_rtmutex_slowlock_locked(rtm, state)\t\\\n\t__rt_mutex_slowlock_locked(rtm, NULL, state)\n\n#define rwbase_rtmutex_unlock(rtm)\t\t\t\\\n\t__rt_mutex_unlock(rtm)\n\n#define rwbase_rtmutex_trylock(rtm)\t\t\t\\\n\t__rt_mutex_trylock(rtm)\n\n#define rwbase_signal_pending_state(state, current)\t\\\n\tsignal_pending_state(state, current)\n\n#define rwbase_schedule()\t\t\t\t\\\n\tschedule()\n\n#include \"rwbase_rt.c\"\n\nvoid __init_rwsem(struct rw_semaphore *sem, const char *name,\n\t\t  struct lock_class_key *key)\n{\n\tinit_rwbase_rt(&(sem)->rwbase);\n\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n\tdebug_check_no_locks_freed((void *)sem, sizeof(*sem));\n\tlockdep_init_map_wait(&sem->dep_map, name, key, 0, LD_WAIT_SLEEP);\n#endif\n}\nEXPORT_SYMBOL(__init_rwsem);\n\nstatic inline void __down_read(struct rw_semaphore *sem)\n{\n\trwbase_read_lock(&sem->rwbase, TASK_UNINTERRUPTIBLE);\n}\n\nstatic inline int __down_read_interruptible(struct rw_semaphore *sem)\n{\n\treturn rwbase_read_lock(&sem->rwbase, TASK_INTERRUPTIBLE);\n}\n\nstatic inline int __down_read_killable(struct rw_semaphore *sem)\n{\n\treturn rwbase_read_lock(&sem->rwbase, TASK_KILLABLE);\n}\n\nstatic inline int __down_read_trylock(struct rw_semaphore *sem)\n{\n\treturn rwbase_read_trylock(&sem->rwbase);\n}\n\nstatic inline void __up_read(struct rw_semaphore *sem)\n{\n\trwbase_read_unlock(&sem->rwbase, TASK_NORMAL);\n}\n\nstatic inline void __sched __down_write(struct rw_semaphore *sem)\n{\n\trwbase_write_lock(&sem->rwbase, TASK_UNINTERRUPTIBLE);\n}\n\nstatic inline int __sched __down_write_killable(struct rw_semaphore *sem)\n{\n\treturn rwbase_write_lock(&sem->rwbase, TASK_KILLABLE);\n}\n\nstatic inline int __down_write_trylock(struct rw_semaphore *sem)\n{\n\treturn rwbase_write_trylock(&sem->rwbase);\n}\n\nstatic inline void __up_write(struct rw_semaphore *sem)\n{\n\trwbase_write_unlock(&sem->rwbase);\n}\n\nstatic inline void __downgrade_write(struct rw_semaphore *sem)\n{\n\trwbase_write_downgrade(&sem->rwbase);\n}\n\n \n#define DEBUG_RWSEMS_WARN_ON(c, sem)\n\nstatic inline void __rwsem_set_reader_owned(struct rw_semaphore *sem,\n\t\t\t\t\t    struct task_struct *owner)\n{\n}\n\nstatic inline bool is_rwsem_reader_owned(struct rw_semaphore *sem)\n{\n\tint count = atomic_read(&sem->rwbase.readers);\n\n\treturn count < 0 && count != READER_BIAS;\n}\n\n#endif  \n\n \nvoid __sched down_read(struct rw_semaphore *sem)\n{\n\tmight_sleep();\n\trwsem_acquire_read(&sem->dep_map, 0, 0, _RET_IP_);\n\n\tLOCK_CONTENDED(sem, __down_read_trylock, __down_read);\n}\nEXPORT_SYMBOL(down_read);\n\nint __sched down_read_interruptible(struct rw_semaphore *sem)\n{\n\tmight_sleep();\n\trwsem_acquire_read(&sem->dep_map, 0, 0, _RET_IP_);\n\n\tif (LOCK_CONTENDED_RETURN(sem, __down_read_trylock, __down_read_interruptible)) {\n\t\trwsem_release(&sem->dep_map, _RET_IP_);\n\t\treturn -EINTR;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(down_read_interruptible);\n\nint __sched down_read_killable(struct rw_semaphore *sem)\n{\n\tmight_sleep();\n\trwsem_acquire_read(&sem->dep_map, 0, 0, _RET_IP_);\n\n\tif (LOCK_CONTENDED_RETURN(sem, __down_read_trylock, __down_read_killable)) {\n\t\trwsem_release(&sem->dep_map, _RET_IP_);\n\t\treturn -EINTR;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(down_read_killable);\n\n \nint down_read_trylock(struct rw_semaphore *sem)\n{\n\tint ret = __down_read_trylock(sem);\n\n\tif (ret == 1)\n\t\trwsem_acquire_read(&sem->dep_map, 0, 1, _RET_IP_);\n\treturn ret;\n}\nEXPORT_SYMBOL(down_read_trylock);\n\n \nvoid __sched down_write(struct rw_semaphore *sem)\n{\n\tmight_sleep();\n\trwsem_acquire(&sem->dep_map, 0, 0, _RET_IP_);\n\tLOCK_CONTENDED(sem, __down_write_trylock, __down_write);\n}\nEXPORT_SYMBOL(down_write);\n\n \nint __sched down_write_killable(struct rw_semaphore *sem)\n{\n\tmight_sleep();\n\trwsem_acquire(&sem->dep_map, 0, 0, _RET_IP_);\n\n\tif (LOCK_CONTENDED_RETURN(sem, __down_write_trylock,\n\t\t\t\t  __down_write_killable)) {\n\t\trwsem_release(&sem->dep_map, _RET_IP_);\n\t\treturn -EINTR;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(down_write_killable);\n\n \nint down_write_trylock(struct rw_semaphore *sem)\n{\n\tint ret = __down_write_trylock(sem);\n\n\tif (ret == 1)\n\t\trwsem_acquire(&sem->dep_map, 0, 1, _RET_IP_);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(down_write_trylock);\n\n \nvoid up_read(struct rw_semaphore *sem)\n{\n\trwsem_release(&sem->dep_map, _RET_IP_);\n\t__up_read(sem);\n}\nEXPORT_SYMBOL(up_read);\n\n \nvoid up_write(struct rw_semaphore *sem)\n{\n\trwsem_release(&sem->dep_map, _RET_IP_);\n\t__up_write(sem);\n}\nEXPORT_SYMBOL(up_write);\n\n \nvoid downgrade_write(struct rw_semaphore *sem)\n{\n\tlock_downgrade(&sem->dep_map, _RET_IP_);\n\t__downgrade_write(sem);\n}\nEXPORT_SYMBOL(downgrade_write);\n\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n\nvoid down_read_nested(struct rw_semaphore *sem, int subclass)\n{\n\tmight_sleep();\n\trwsem_acquire_read(&sem->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED(sem, __down_read_trylock, __down_read);\n}\nEXPORT_SYMBOL(down_read_nested);\n\nint down_read_killable_nested(struct rw_semaphore *sem, int subclass)\n{\n\tmight_sleep();\n\trwsem_acquire_read(&sem->dep_map, subclass, 0, _RET_IP_);\n\n\tif (LOCK_CONTENDED_RETURN(sem, __down_read_trylock, __down_read_killable)) {\n\t\trwsem_release(&sem->dep_map, _RET_IP_);\n\t\treturn -EINTR;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(down_read_killable_nested);\n\nvoid _down_write_nest_lock(struct rw_semaphore *sem, struct lockdep_map *nest)\n{\n\tmight_sleep();\n\trwsem_acquire_nest(&sem->dep_map, 0, 0, nest, _RET_IP_);\n\tLOCK_CONTENDED(sem, __down_write_trylock, __down_write);\n}\nEXPORT_SYMBOL(_down_write_nest_lock);\n\nvoid down_read_non_owner(struct rw_semaphore *sem)\n{\n\tmight_sleep();\n\t__down_read(sem);\n\t \n\t__rwsem_set_reader_owned(sem, NULL);\n}\nEXPORT_SYMBOL(down_read_non_owner);\n\nvoid down_write_nested(struct rw_semaphore *sem, int subclass)\n{\n\tmight_sleep();\n\trwsem_acquire(&sem->dep_map, subclass, 0, _RET_IP_);\n\tLOCK_CONTENDED(sem, __down_write_trylock, __down_write);\n}\nEXPORT_SYMBOL(down_write_nested);\n\nint __sched down_write_killable_nested(struct rw_semaphore *sem, int subclass)\n{\n\tmight_sleep();\n\trwsem_acquire(&sem->dep_map, subclass, 0, _RET_IP_);\n\n\tif (LOCK_CONTENDED_RETURN(sem, __down_write_trylock,\n\t\t\t\t  __down_write_killable)) {\n\t\trwsem_release(&sem->dep_map, _RET_IP_);\n\t\treturn -EINTR;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(down_write_killable_nested);\n\nvoid up_read_non_owner(struct rw_semaphore *sem)\n{\n\tDEBUG_RWSEMS_WARN_ON(!is_rwsem_reader_owned(sem), sem);\n\t__up_read(sem);\n}\nEXPORT_SYMBOL(up_read_non_owner);\n\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}