{
  "module_name": "lockdep.c",
  "hash_id": "3537c34beae670561d43247c37f1ef4d7f94b2cb0141fba53e5a98f75fee91da",
  "original_prompt": "Ingested from linux-6.6.14/kernel/locking/lockdep.c",
  "human_readable_source": "\n \n#define DISABLE_BRANCH_PROFILING\n#include <linux/mutex.h>\n#include <linux/sched.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/task.h>\n#include <linux/sched/mm.h>\n#include <linux/delay.h>\n#include <linux/module.h>\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <linux/spinlock.h>\n#include <linux/kallsyms.h>\n#include <linux/interrupt.h>\n#include <linux/stacktrace.h>\n#include <linux/debug_locks.h>\n#include <linux/irqflags.h>\n#include <linux/utsname.h>\n#include <linux/hash.h>\n#include <linux/ftrace.h>\n#include <linux/stringify.h>\n#include <linux/bitmap.h>\n#include <linux/bitops.h>\n#include <linux/gfp.h>\n#include <linux/random.h>\n#include <linux/jhash.h>\n#include <linux/nmi.h>\n#include <linux/rcupdate.h>\n#include <linux/kprobes.h>\n#include <linux/lockdep.h>\n#include <linux/context_tracking.h>\n\n#include <asm/sections.h>\n\n#include \"lockdep_internals.h\"\n\n#include <trace/events/lock.h>\n\n#ifdef CONFIG_PROVE_LOCKING\nstatic int prove_locking = 1;\nmodule_param(prove_locking, int, 0644);\n#else\n#define prove_locking 0\n#endif\n\n#ifdef CONFIG_LOCK_STAT\nstatic int lock_stat = 1;\nmodule_param(lock_stat, int, 0644);\n#else\n#define lock_stat 0\n#endif\n\n#ifdef CONFIG_SYSCTL\nstatic struct ctl_table kern_lockdep_table[] = {\n#ifdef CONFIG_PROVE_LOCKING\n\t{\n\t\t.procname       = \"prove_locking\",\n\t\t.data           = &prove_locking,\n\t\t.maxlen         = sizeof(int),\n\t\t.mode           = 0644,\n\t\t.proc_handler   = proc_dointvec,\n\t},\n#endif  \n#ifdef CONFIG_LOCK_STAT\n\t{\n\t\t.procname       = \"lock_stat\",\n\t\t.data           = &lock_stat,\n\t\t.maxlen         = sizeof(int),\n\t\t.mode           = 0644,\n\t\t.proc_handler   = proc_dointvec,\n\t},\n#endif  \n\t{ }\n};\n\nstatic __init int kernel_lockdep_sysctls_init(void)\n{\n\tregister_sysctl_init(\"kernel\", kern_lockdep_table);\n\treturn 0;\n}\nlate_initcall(kernel_lockdep_sysctls_init);\n#endif  \n\nDEFINE_PER_CPU(unsigned int, lockdep_recursion);\nEXPORT_PER_CPU_SYMBOL_GPL(lockdep_recursion);\n\nstatic __always_inline bool lockdep_enabled(void)\n{\n\tif (!debug_locks)\n\t\treturn false;\n\n\tif (this_cpu_read(lockdep_recursion))\n\t\treturn false;\n\n\tif (current->lockdep_recursion)\n\t\treturn false;\n\n\treturn true;\n}\n\n \nstatic arch_spinlock_t __lock = (arch_spinlock_t)__ARCH_SPIN_LOCK_UNLOCKED;\nstatic struct task_struct *__owner;\n\nstatic inline void lockdep_lock(void)\n{\n\tDEBUG_LOCKS_WARN_ON(!irqs_disabled());\n\n\t__this_cpu_inc(lockdep_recursion);\n\tarch_spin_lock(&__lock);\n\t__owner = current;\n}\n\nstatic inline void lockdep_unlock(void)\n{\n\tDEBUG_LOCKS_WARN_ON(!irqs_disabled());\n\n\tif (debug_locks && DEBUG_LOCKS_WARN_ON(__owner != current))\n\t\treturn;\n\n\t__owner = NULL;\n\tarch_spin_unlock(&__lock);\n\t__this_cpu_dec(lockdep_recursion);\n}\n\nstatic inline bool lockdep_assert_locked(void)\n{\n\treturn DEBUG_LOCKS_WARN_ON(__owner != current);\n}\n\nstatic struct task_struct *lockdep_selftest_task_struct;\n\n\nstatic int graph_lock(void)\n{\n\tlockdep_lock();\n\t \n\tif (!debug_locks) {\n\t\tlockdep_unlock();\n\t\treturn 0;\n\t}\n\treturn 1;\n}\n\nstatic inline void graph_unlock(void)\n{\n\tlockdep_unlock();\n}\n\n \nstatic inline int debug_locks_off_graph_unlock(void)\n{\n\tint ret = debug_locks_off();\n\n\tlockdep_unlock();\n\n\treturn ret;\n}\n\nunsigned long nr_list_entries;\nstatic struct lock_list list_entries[MAX_LOCKDEP_ENTRIES];\nstatic DECLARE_BITMAP(list_entries_in_use, MAX_LOCKDEP_ENTRIES);\n\n \n#define KEYHASH_BITS\t\t(MAX_LOCKDEP_KEYS_BITS - 1)\n#define KEYHASH_SIZE\t\t(1UL << KEYHASH_BITS)\nstatic struct hlist_head lock_keys_hash[KEYHASH_SIZE];\nunsigned long nr_lock_classes;\nunsigned long nr_zapped_classes;\nunsigned long max_lock_class_idx;\nstruct lock_class lock_classes[MAX_LOCKDEP_KEYS];\nDECLARE_BITMAP(lock_classes_in_use, MAX_LOCKDEP_KEYS);\n\nstatic inline struct lock_class *hlock_class(struct held_lock *hlock)\n{\n\tunsigned int class_idx = hlock->class_idx;\n\n\t \n\tbarrier();\n\n\tif (!test_bit(class_idx, lock_classes_in_use)) {\n\t\t \n\t\tDEBUG_LOCKS_WARN_ON(1);\n\t\treturn NULL;\n\t}\n\n\t \n\treturn lock_classes + class_idx;\n}\n\n#ifdef CONFIG_LOCK_STAT\nstatic DEFINE_PER_CPU(struct lock_class_stats[MAX_LOCKDEP_KEYS], cpu_lock_stats);\n\nstatic inline u64 lockstat_clock(void)\n{\n\treturn local_clock();\n}\n\nstatic int lock_point(unsigned long points[], unsigned long ip)\n{\n\tint i;\n\n\tfor (i = 0; i < LOCKSTAT_POINTS; i++) {\n\t\tif (points[i] == 0) {\n\t\t\tpoints[i] = ip;\n\t\t\tbreak;\n\t\t}\n\t\tif (points[i] == ip)\n\t\t\tbreak;\n\t}\n\n\treturn i;\n}\n\nstatic void lock_time_inc(struct lock_time *lt, u64 time)\n{\n\tif (time > lt->max)\n\t\tlt->max = time;\n\n\tif (time < lt->min || !lt->nr)\n\t\tlt->min = time;\n\n\tlt->total += time;\n\tlt->nr++;\n}\n\nstatic inline void lock_time_add(struct lock_time *src, struct lock_time *dst)\n{\n\tif (!src->nr)\n\t\treturn;\n\n\tif (src->max > dst->max)\n\t\tdst->max = src->max;\n\n\tif (src->min < dst->min || !dst->nr)\n\t\tdst->min = src->min;\n\n\tdst->total += src->total;\n\tdst->nr += src->nr;\n}\n\nstruct lock_class_stats lock_stats(struct lock_class *class)\n{\n\tstruct lock_class_stats stats;\n\tint cpu, i;\n\n\tmemset(&stats, 0, sizeof(struct lock_class_stats));\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct lock_class_stats *pcs =\n\t\t\t&per_cpu(cpu_lock_stats, cpu)[class - lock_classes];\n\n\t\tfor (i = 0; i < ARRAY_SIZE(stats.contention_point); i++)\n\t\t\tstats.contention_point[i] += pcs->contention_point[i];\n\n\t\tfor (i = 0; i < ARRAY_SIZE(stats.contending_point); i++)\n\t\t\tstats.contending_point[i] += pcs->contending_point[i];\n\n\t\tlock_time_add(&pcs->read_waittime, &stats.read_waittime);\n\t\tlock_time_add(&pcs->write_waittime, &stats.write_waittime);\n\n\t\tlock_time_add(&pcs->read_holdtime, &stats.read_holdtime);\n\t\tlock_time_add(&pcs->write_holdtime, &stats.write_holdtime);\n\n\t\tfor (i = 0; i < ARRAY_SIZE(stats.bounces); i++)\n\t\t\tstats.bounces[i] += pcs->bounces[i];\n\t}\n\n\treturn stats;\n}\n\nvoid clear_lock_stats(struct lock_class *class)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct lock_class_stats *cpu_stats =\n\t\t\t&per_cpu(cpu_lock_stats, cpu)[class - lock_classes];\n\n\t\tmemset(cpu_stats, 0, sizeof(struct lock_class_stats));\n\t}\n\tmemset(class->contention_point, 0, sizeof(class->contention_point));\n\tmemset(class->contending_point, 0, sizeof(class->contending_point));\n}\n\nstatic struct lock_class_stats *get_lock_stats(struct lock_class *class)\n{\n\treturn &this_cpu_ptr(cpu_lock_stats)[class - lock_classes];\n}\n\nstatic void lock_release_holdtime(struct held_lock *hlock)\n{\n\tstruct lock_class_stats *stats;\n\tu64 holdtime;\n\n\tif (!lock_stat)\n\t\treturn;\n\n\tholdtime = lockstat_clock() - hlock->holdtime_stamp;\n\n\tstats = get_lock_stats(hlock_class(hlock));\n\tif (hlock->read)\n\t\tlock_time_inc(&stats->read_holdtime, holdtime);\n\telse\n\t\tlock_time_inc(&stats->write_holdtime, holdtime);\n}\n#else\nstatic inline void lock_release_holdtime(struct held_lock *hlock)\n{\n}\n#endif\n\n \nstatic LIST_HEAD(all_lock_classes);\nstatic LIST_HEAD(free_lock_classes);\n\n \nstruct pending_free {\n\tstruct list_head zapped;\n\tDECLARE_BITMAP(lock_chains_being_freed, MAX_LOCKDEP_CHAINS);\n};\n\n \nstatic struct delayed_free {\n\tstruct rcu_head\t\trcu_head;\n\tint\t\t\tindex;\n\tint\t\t\tscheduled;\n\tstruct pending_free\tpf[2];\n} delayed_free;\n\n \n#define CLASSHASH_BITS\t\t(MAX_LOCKDEP_KEYS_BITS - 1)\n#define CLASSHASH_SIZE\t\t(1UL << CLASSHASH_BITS)\n#define __classhashfn(key)\thash_long((unsigned long)key, CLASSHASH_BITS)\n#define classhashentry(key)\t(classhash_table + __classhashfn((key)))\n\nstatic struct hlist_head classhash_table[CLASSHASH_SIZE];\n\n \n#define CHAINHASH_BITS\t\t(MAX_LOCKDEP_CHAINS_BITS-1)\n#define CHAINHASH_SIZE\t\t(1UL << CHAINHASH_BITS)\n#define __chainhashfn(chain)\thash_long(chain, CHAINHASH_BITS)\n#define chainhashentry(chain)\t(chainhash_table + __chainhashfn((chain)))\n\nstatic struct hlist_head chainhash_table[CHAINHASH_SIZE];\n\n \nstatic inline u16 hlock_id(struct held_lock *hlock)\n{\n\tBUILD_BUG_ON(MAX_LOCKDEP_KEYS_BITS + 2 > 16);\n\n\treturn (hlock->class_idx | (hlock->read << MAX_LOCKDEP_KEYS_BITS));\n}\n\nstatic inline unsigned int chain_hlock_class_idx(u16 hlock_id)\n{\n\treturn hlock_id & (MAX_LOCKDEP_KEYS - 1);\n}\n\n \nstatic inline u64 iterate_chain_key(u64 key, u32 idx)\n{\n\tu32 k0 = key, k1 = key >> 32;\n\n\t__jhash_mix(idx, k0, k1);  \n\n\treturn k0 | (u64)k1 << 32;\n}\n\nvoid lockdep_init_task(struct task_struct *task)\n{\n\ttask->lockdep_depth = 0;  \n\ttask->curr_chain_key = INITIAL_CHAIN_KEY;\n\ttask->lockdep_recursion = 0;\n}\n\nstatic __always_inline void lockdep_recursion_inc(void)\n{\n\t__this_cpu_inc(lockdep_recursion);\n}\n\nstatic __always_inline void lockdep_recursion_finish(void)\n{\n\tif (WARN_ON_ONCE(__this_cpu_dec_return(lockdep_recursion)))\n\t\t__this_cpu_write(lockdep_recursion, 0);\n}\n\nvoid lockdep_set_selftest_task(struct task_struct *task)\n{\n\tlockdep_selftest_task_struct = task;\n}\n\n \n\n#define VERBOSE\t\t\t0\n#define VERY_VERBOSE\t\t0\n\n#if VERBOSE\n# define HARDIRQ_VERBOSE\t1\n# define SOFTIRQ_VERBOSE\t1\n#else\n# define HARDIRQ_VERBOSE\t0\n# define SOFTIRQ_VERBOSE\t0\n#endif\n\n#if VERBOSE || HARDIRQ_VERBOSE || SOFTIRQ_VERBOSE\n \nstatic int class_filter(struct lock_class *class)\n{\n#if 0\n\t \n\tif (class->name_version == 1 &&\n\t\t\t!strcmp(class->name, \"lockname\"))\n\t\treturn 1;\n\tif (class->name_version == 1 &&\n\t\t\t!strcmp(class->name, \"&struct->lockfield\"))\n\t\treturn 1;\n#endif\n\t \n\treturn 0;\n}\n#endif\n\nstatic int verbose(struct lock_class *class)\n{\n#if VERBOSE\n\treturn class_filter(class);\n#endif\n\treturn 0;\n}\n\nstatic void print_lockdep_off(const char *bug_msg)\n{\n\tprintk(KERN_DEBUG \"%s\\n\", bug_msg);\n\tprintk(KERN_DEBUG \"turning off the locking correctness validator.\\n\");\n#ifdef CONFIG_LOCK_STAT\n\tprintk(KERN_DEBUG \"Please attach the output of /proc/lock_stat to the bug report\\n\");\n#endif\n}\n\nunsigned long nr_stack_trace_entries;\n\n#ifdef CONFIG_PROVE_LOCKING\n \nstruct lock_trace {\n\tstruct hlist_node\thash_entry;\n\tu32\t\t\thash;\n\tu32\t\t\tnr_entries;\n\tunsigned long\t\tentries[] __aligned(sizeof(unsigned long));\n};\n#define LOCK_TRACE_SIZE_IN_LONGS\t\t\t\t\\\n\t(sizeof(struct lock_trace) / sizeof(unsigned long))\n \nstatic unsigned long stack_trace[MAX_STACK_TRACE_ENTRIES];\nstatic struct hlist_head stack_trace_hash[STACK_TRACE_HASH_SIZE];\n\nstatic bool traces_identical(struct lock_trace *t1, struct lock_trace *t2)\n{\n\treturn t1->hash == t2->hash && t1->nr_entries == t2->nr_entries &&\n\t\tmemcmp(t1->entries, t2->entries,\n\t\t       t1->nr_entries * sizeof(t1->entries[0])) == 0;\n}\n\nstatic struct lock_trace *save_trace(void)\n{\n\tstruct lock_trace *trace, *t2;\n\tstruct hlist_head *hash_head;\n\tu32 hash;\n\tint max_entries;\n\n\tBUILD_BUG_ON_NOT_POWER_OF_2(STACK_TRACE_HASH_SIZE);\n\tBUILD_BUG_ON(LOCK_TRACE_SIZE_IN_LONGS >= MAX_STACK_TRACE_ENTRIES);\n\n\ttrace = (struct lock_trace *)(stack_trace + nr_stack_trace_entries);\n\tmax_entries = MAX_STACK_TRACE_ENTRIES - nr_stack_trace_entries -\n\t\tLOCK_TRACE_SIZE_IN_LONGS;\n\n\tif (max_entries <= 0) {\n\t\tif (!debug_locks_off_graph_unlock())\n\t\t\treturn NULL;\n\n\t\tprint_lockdep_off(\"BUG: MAX_STACK_TRACE_ENTRIES too low!\");\n\t\tdump_stack();\n\n\t\treturn NULL;\n\t}\n\ttrace->nr_entries = stack_trace_save(trace->entries, max_entries, 3);\n\n\thash = jhash(trace->entries, trace->nr_entries *\n\t\t     sizeof(trace->entries[0]), 0);\n\ttrace->hash = hash;\n\thash_head = stack_trace_hash + (hash & (STACK_TRACE_HASH_SIZE - 1));\n\thlist_for_each_entry(t2, hash_head, hash_entry) {\n\t\tif (traces_identical(trace, t2))\n\t\t\treturn t2;\n\t}\n\tnr_stack_trace_entries += LOCK_TRACE_SIZE_IN_LONGS + trace->nr_entries;\n\thlist_add_head(&trace->hash_entry, hash_head);\n\n\treturn trace;\n}\n\n \nu64 lockdep_stack_trace_count(void)\n{\n\tstruct lock_trace *trace;\n\tu64 c = 0;\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(stack_trace_hash); i++) {\n\t\thlist_for_each_entry(trace, &stack_trace_hash[i], hash_entry) {\n\t\t\tc++;\n\t\t}\n\t}\n\n\treturn c;\n}\n\n \nu64 lockdep_stack_hash_count(void)\n{\n\tu64 c = 0;\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(stack_trace_hash); i++)\n\t\tif (!hlist_empty(&stack_trace_hash[i]))\n\t\t\tc++;\n\n\treturn c;\n}\n#endif\n\nunsigned int nr_hardirq_chains;\nunsigned int nr_softirq_chains;\nunsigned int nr_process_chains;\nunsigned int max_lockdep_depth;\n\n#ifdef CONFIG_DEBUG_LOCKDEP\n \nDEFINE_PER_CPU(struct lockdep_stats, lockdep_stats);\n#endif\n\n#ifdef CONFIG_PROVE_LOCKING\n \n\n#define __USAGE(__STATE)\t\t\t\t\t\t\\\n\t[LOCK_USED_IN_##__STATE] = \"IN-\"__stringify(__STATE)\"-W\",\t\\\n\t[LOCK_ENABLED_##__STATE] = __stringify(__STATE)\"-ON-W\",\t\t\\\n\t[LOCK_USED_IN_##__STATE##_READ] = \"IN-\"__stringify(__STATE)\"-R\",\\\n\t[LOCK_ENABLED_##__STATE##_READ] = __stringify(__STATE)\"-ON-R\",\n\nstatic const char *usage_str[] =\n{\n#define LOCKDEP_STATE(__STATE) __USAGE(__STATE)\n#include \"lockdep_states.h\"\n#undef LOCKDEP_STATE\n\t[LOCK_USED] = \"INITIAL USE\",\n\t[LOCK_USED_READ] = \"INITIAL READ USE\",\n\t \n\t[LOCK_USAGE_STATES] = \"IN-NMI\",\n};\n#endif\n\nconst char *__get_key_name(const struct lockdep_subclass_key *key, char *str)\n{\n\treturn kallsyms_lookup((unsigned long)key, NULL, NULL, NULL, str);\n}\n\nstatic inline unsigned long lock_flag(enum lock_usage_bit bit)\n{\n\treturn 1UL << bit;\n}\n\nstatic char get_usage_char(struct lock_class *class, enum lock_usage_bit bit)\n{\n\t \n\tchar c = '.';\n\n\t \n\tif (class->usage_mask & lock_flag(bit + LOCK_USAGE_DIR_MASK)) {\n\t\tc = '+';\n\t\tif (class->usage_mask & lock_flag(bit))\n\t\t\tc = '?';\n\t} else if (class->usage_mask & lock_flag(bit))\n\t\tc = '-';\n\n\treturn c;\n}\n\nvoid get_usage_chars(struct lock_class *class, char usage[LOCK_USAGE_CHARS])\n{\n\tint i = 0;\n\n#define LOCKDEP_STATE(__STATE) \t\t\t\t\t\t\\\n\tusage[i++] = get_usage_char(class, LOCK_USED_IN_##__STATE);\t\\\n\tusage[i++] = get_usage_char(class, LOCK_USED_IN_##__STATE##_READ);\n#include \"lockdep_states.h\"\n#undef LOCKDEP_STATE\n\n\tusage[i] = '\\0';\n}\n\nstatic void __print_lock_name(struct held_lock *hlock, struct lock_class *class)\n{\n\tchar str[KSYM_NAME_LEN];\n\tconst char *name;\n\n\tname = class->name;\n\tif (!name) {\n\t\tname = __get_key_name(class->key, str);\n\t\tprintk(KERN_CONT \"%s\", name);\n\t} else {\n\t\tprintk(KERN_CONT \"%s\", name);\n\t\tif (class->name_version > 1)\n\t\t\tprintk(KERN_CONT \"#%d\", class->name_version);\n\t\tif (class->subclass)\n\t\t\tprintk(KERN_CONT \"/%d\", class->subclass);\n\t\tif (hlock && class->print_fn)\n\t\t\tclass->print_fn(hlock->instance);\n\t}\n}\n\nstatic void print_lock_name(struct held_lock *hlock, struct lock_class *class)\n{\n\tchar usage[LOCK_USAGE_CHARS];\n\n\tget_usage_chars(class, usage);\n\n\tprintk(KERN_CONT \" (\");\n\t__print_lock_name(hlock, class);\n\tprintk(KERN_CONT \"){%s}-{%d:%d}\", usage,\n\t\t\tclass->wait_type_outer ?: class->wait_type_inner,\n\t\t\tclass->wait_type_inner);\n}\n\nstatic void print_lockdep_cache(struct lockdep_map *lock)\n{\n\tconst char *name;\n\tchar str[KSYM_NAME_LEN];\n\n\tname = lock->name;\n\tif (!name)\n\t\tname = __get_key_name(lock->key->subkeys, str);\n\n\tprintk(KERN_CONT \"%s\", name);\n}\n\nstatic void print_lock(struct held_lock *hlock)\n{\n\t \n\tstruct lock_class *lock = hlock_class(hlock);\n\n\tif (!lock) {\n\t\tprintk(KERN_CONT \"<RELEASED>\\n\");\n\t\treturn;\n\t}\n\n\tprintk(KERN_CONT \"%px\", hlock->instance);\n\tprint_lock_name(hlock, lock);\n\tprintk(KERN_CONT \", at: %pS\\n\", (void *)hlock->acquire_ip);\n}\n\nstatic void lockdep_print_held_locks(struct task_struct *p)\n{\n\tint i, depth = READ_ONCE(p->lockdep_depth);\n\n\tif (!depth)\n\t\tprintk(\"no locks held by %s/%d.\\n\", p->comm, task_pid_nr(p));\n\telse\n\t\tprintk(\"%d lock%s held by %s/%d:\\n\", depth,\n\t\t       depth > 1 ? \"s\" : \"\", p->comm, task_pid_nr(p));\n\t \n\tif (p != current && task_is_running(p))\n\t\treturn;\n\tfor (i = 0; i < depth; i++) {\n\t\tprintk(\" #%d: \", i);\n\t\tprint_lock(p->held_locks + i);\n\t}\n}\n\nstatic void print_kernel_ident(void)\n{\n\tprintk(\"%s %.*s %s\\n\", init_utsname()->release,\n\t\t(int)strcspn(init_utsname()->version, \" \"),\n\t\tinit_utsname()->version,\n\t\tprint_tainted());\n}\n\nstatic int very_verbose(struct lock_class *class)\n{\n#if VERY_VERBOSE\n\treturn class_filter(class);\n#endif\n\treturn 0;\n}\n\n \n#ifdef __KERNEL__\nstatic int static_obj(const void *obj)\n{\n\tunsigned long addr = (unsigned long) obj;\n\n\tif (is_kernel_core_data(addr))\n\t\treturn 1;\n\n\t \n\tif (is_kernel_rodata(addr))\n\t\treturn 1;\n\n\t \n\tif (system_state < SYSTEM_FREEING_INITMEM &&\n\t\tinit_section_contains((void *)addr, 1))\n\t\treturn 1;\n\n\t \n\tif (is_kernel_percpu_address(addr))\n\t\treturn 1;\n\n\t \n\treturn is_module_address(addr) || is_module_percpu_address(addr);\n}\n#endif\n\n \nstatic int count_matching_names(struct lock_class *new_class)\n{\n\tstruct lock_class *class;\n\tint count = 0;\n\n\tif (!new_class->name)\n\t\treturn 0;\n\n\tlist_for_each_entry(class, &all_lock_classes, lock_entry) {\n\t\tif (new_class->key - new_class->subclass == class->key)\n\t\t\treturn class->name_version;\n\t\tif (class->name && !strcmp(class->name, new_class->name))\n\t\t\tcount = max(count, class->name_version);\n\t}\n\n\treturn count + 1;\n}\n\n \nstatic noinstr struct lock_class *\nlook_up_lock_class(const struct lockdep_map *lock, unsigned int subclass)\n{\n\tstruct lockdep_subclass_key *key;\n\tstruct hlist_head *hash_head;\n\tstruct lock_class *class;\n\n\tif (unlikely(subclass >= MAX_LOCKDEP_SUBCLASSES)) {\n\t\tinstrumentation_begin();\n\t\tdebug_locks_off();\n\t\tprintk(KERN_ERR\n\t\t\t\"BUG: looking up invalid subclass: %u\\n\", subclass);\n\t\tprintk(KERN_ERR\n\t\t\t\"turning off the locking correctness validator.\\n\");\n\t\tdump_stack();\n\t\tinstrumentation_end();\n\t\treturn NULL;\n\t}\n\n\t \n\tif (unlikely(!lock->key))\n\t\treturn NULL;\n\n\t \n\tBUILD_BUG_ON(sizeof(struct lock_class_key) >\n\t\t\tsizeof(struct lockdep_map));\n\n\tkey = lock->key->subkeys + subclass;\n\n\thash_head = classhashentry(key);\n\n\t \n\tif (DEBUG_LOCKS_WARN_ON(!irqs_disabled()))\n\t\treturn NULL;\n\n\thlist_for_each_entry_rcu_notrace(class, hash_head, hash_entry) {\n\t\tif (class->key == key) {\n\t\t\t \n\t\t\tWARN_ONCE(class->name != lock->name &&\n\t\t\t\t  lock->key != &__lockdep_no_validate__,\n\t\t\t\t  \"Looking for class \\\"%s\\\" with key %ps, but found a different class \\\"%s\\\" with the same key\\n\",\n\t\t\t\t  lock->name, lock->key, class->name);\n\t\t\treturn class;\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\n \nstatic bool assign_lock_key(struct lockdep_map *lock)\n{\n\tunsigned long can_addr, addr = (unsigned long)lock;\n\n#ifdef __KERNEL__\n\t \n\tBUILD_BUG_ON(sizeof(struct lock_class_key) > sizeof(raw_spinlock_t));\n#endif\n\n\tif (__is_kernel_percpu_address(addr, &can_addr))\n\t\tlock->key = (void *)can_addr;\n\telse if (__is_module_percpu_address(addr, &can_addr))\n\t\tlock->key = (void *)can_addr;\n\telse if (static_obj(lock))\n\t\tlock->key = (void *)lock;\n\telse {\n\t\t \n\t\tdebug_locks_off();\n\t\tpr_err(\"INFO: trying to register non-static key.\\n\");\n\t\tpr_err(\"The code is fine but needs lockdep annotation, or maybe\\n\");\n\t\tpr_err(\"you didn't initialize this object before use?\\n\");\n\t\tpr_err(\"turning off the locking correctness validator.\\n\");\n\t\tdump_stack();\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\n#ifdef CONFIG_DEBUG_LOCKDEP\n\n \nstatic bool in_list(struct list_head *e, struct list_head *h)\n{\n\tstruct list_head *f;\n\n\tlist_for_each(f, h) {\n\t\tif (e == f)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nstatic bool in_any_class_list(struct list_head *e)\n{\n\tstruct lock_class *class;\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(lock_classes); i++) {\n\t\tclass = &lock_classes[i];\n\t\tif (in_list(e, &class->locks_after) ||\n\t\t    in_list(e, &class->locks_before))\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic bool class_lock_list_valid(struct lock_class *c, struct list_head *h)\n{\n\tstruct lock_list *e;\n\n\tlist_for_each_entry(e, h, entry) {\n\t\tif (e->links_to != c) {\n\t\t\tprintk(KERN_INFO \"class %s: mismatch for lock entry %ld; class %s <> %s\",\n\t\t\t       c->name ? : \"(?)\",\n\t\t\t       (unsigned long)(e - list_entries),\n\t\t\t       e->links_to && e->links_to->name ?\n\t\t\t       e->links_to->name : \"(?)\",\n\t\t\t       e->class && e->class->name ? e->class->name :\n\t\t\t       \"(?)\");\n\t\t\treturn false;\n\t\t}\n\t}\n\treturn true;\n}\n\n#ifdef CONFIG_PROVE_LOCKING\nstatic u16 chain_hlocks[MAX_LOCKDEP_CHAIN_HLOCKS];\n#endif\n\nstatic bool check_lock_chain_key(struct lock_chain *chain)\n{\n#ifdef CONFIG_PROVE_LOCKING\n\tu64 chain_key = INITIAL_CHAIN_KEY;\n\tint i;\n\n\tfor (i = chain->base; i < chain->base + chain->depth; i++)\n\t\tchain_key = iterate_chain_key(chain_key, chain_hlocks[i]);\n\t \n\tif (chain->chain_key != chain_key) {\n\t\tprintk(KERN_INFO \"chain %lld: key %#llx <> %#llx\\n\",\n\t\t       (unsigned long long)(chain - lock_chains),\n\t\t       (unsigned long long)chain->chain_key,\n\t\t       (unsigned long long)chain_key);\n\t\treturn false;\n\t}\n#endif\n\treturn true;\n}\n\nstatic bool in_any_zapped_class_list(struct lock_class *class)\n{\n\tstruct pending_free *pf;\n\tint i;\n\n\tfor (i = 0, pf = delayed_free.pf; i < ARRAY_SIZE(delayed_free.pf); i++, pf++) {\n\t\tif (in_list(&class->lock_entry, &pf->zapped))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic bool __check_data_structures(void)\n{\n\tstruct lock_class *class;\n\tstruct lock_chain *chain;\n\tstruct hlist_head *head;\n\tstruct lock_list *e;\n\tint i;\n\n\t \n\tfor (i = 0; i < ARRAY_SIZE(lock_classes); i++) {\n\t\tclass = &lock_classes[i];\n\t\tif (!in_list(&class->lock_entry, &all_lock_classes) &&\n\t\t    !in_list(&class->lock_entry, &free_lock_classes) &&\n\t\t    !in_any_zapped_class_list(class)) {\n\t\t\tprintk(KERN_INFO \"class %px/%s is not in any class list\\n\",\n\t\t\t       class, class->name ? : \"(?)\");\n\t\t\treturn false;\n\t\t}\n\t}\n\n\t \n\tfor (i = 0; i < ARRAY_SIZE(lock_classes); i++) {\n\t\tclass = &lock_classes[i];\n\t\tif (!class_lock_list_valid(class, &class->locks_before))\n\t\t\treturn false;\n\t\tif (!class_lock_list_valid(class, &class->locks_after))\n\t\t\treturn false;\n\t}\n\n\t \n\tfor (i = 0; i < ARRAY_SIZE(chainhash_table); i++) {\n\t\thead = chainhash_table + i;\n\t\thlist_for_each_entry_rcu(chain, head, entry) {\n\t\t\tif (!check_lock_chain_key(chain))\n\t\t\t\treturn false;\n\t\t}\n\t}\n\n\t \n\tfor_each_set_bit(i, list_entries_in_use, ARRAY_SIZE(list_entries)) {\n\t\te = list_entries + i;\n\t\tif (!in_any_class_list(&e->entry)) {\n\t\t\tprintk(KERN_INFO \"list entry %d is not in any class list; class %s <> %s\\n\",\n\t\t\t       (unsigned int)(e - list_entries),\n\t\t\t       e->class->name ? : \"(?)\",\n\t\t\t       e->links_to->name ? : \"(?)\");\n\t\t\treturn false;\n\t\t}\n\t}\n\n\t \n\tfor_each_clear_bit(i, list_entries_in_use, ARRAY_SIZE(list_entries)) {\n\t\te = list_entries + i;\n\t\tif (in_any_class_list(&e->entry)) {\n\t\t\tprintk(KERN_INFO \"list entry %d occurs in a class list; class %s <> %s\\n\",\n\t\t\t       (unsigned int)(e - list_entries),\n\t\t\t       e->class && e->class->name ? e->class->name :\n\t\t\t       \"(?)\",\n\t\t\t       e->links_to && e->links_to->name ?\n\t\t\t       e->links_to->name : \"(?)\");\n\t\t\treturn false;\n\t\t}\n\t}\n\n\treturn true;\n}\n\nint check_consistency = 0;\nmodule_param(check_consistency, int, 0644);\n\nstatic void check_data_structures(void)\n{\n\tstatic bool once = false;\n\n\tif (check_consistency && !once) {\n\t\tif (!__check_data_structures()) {\n\t\t\tonce = true;\n\t\t\tWARN_ON(once);\n\t\t}\n\t}\n}\n\n#else  \n\nstatic inline void check_data_structures(void) { }\n\n#endif  \n\nstatic void init_chain_block_buckets(void);\n\n \nstatic void init_data_structures_once(void)\n{\n\tstatic bool __read_mostly ds_initialized, rcu_head_initialized;\n\tint i;\n\n\tif (likely(rcu_head_initialized))\n\t\treturn;\n\n\tif (system_state >= SYSTEM_SCHEDULING) {\n\t\tinit_rcu_head(&delayed_free.rcu_head);\n\t\trcu_head_initialized = true;\n\t}\n\n\tif (ds_initialized)\n\t\treturn;\n\n\tds_initialized = true;\n\n\tINIT_LIST_HEAD(&delayed_free.pf[0].zapped);\n\tINIT_LIST_HEAD(&delayed_free.pf[1].zapped);\n\n\tfor (i = 0; i < ARRAY_SIZE(lock_classes); i++) {\n\t\tlist_add_tail(&lock_classes[i].lock_entry, &free_lock_classes);\n\t\tINIT_LIST_HEAD(&lock_classes[i].locks_after);\n\t\tINIT_LIST_HEAD(&lock_classes[i].locks_before);\n\t}\n\tinit_chain_block_buckets();\n}\n\nstatic inline struct hlist_head *keyhashentry(const struct lock_class_key *key)\n{\n\tunsigned long hash = hash_long((uintptr_t)key, KEYHASH_BITS);\n\n\treturn lock_keys_hash + hash;\n}\n\n \nvoid lockdep_register_key(struct lock_class_key *key)\n{\n\tstruct hlist_head *hash_head;\n\tstruct lock_class_key *k;\n\tunsigned long flags;\n\n\tif (WARN_ON_ONCE(static_obj(key)))\n\t\treturn;\n\thash_head = keyhashentry(key);\n\n\traw_local_irq_save(flags);\n\tif (!graph_lock())\n\t\tgoto restore_irqs;\n\thlist_for_each_entry_rcu(k, hash_head, hash_entry) {\n\t\tif (WARN_ON_ONCE(k == key))\n\t\t\tgoto out_unlock;\n\t}\n\thlist_add_head_rcu(&key->hash_entry, hash_head);\nout_unlock:\n\tgraph_unlock();\nrestore_irqs:\n\traw_local_irq_restore(flags);\n}\nEXPORT_SYMBOL_GPL(lockdep_register_key);\n\n \nstatic bool is_dynamic_key(const struct lock_class_key *key)\n{\n\tstruct hlist_head *hash_head;\n\tstruct lock_class_key *k;\n\tbool found = false;\n\n\tif (WARN_ON_ONCE(static_obj(key)))\n\t\treturn false;\n\n\t \n\tif (!debug_locks)\n\t\treturn true;\n\n\thash_head = keyhashentry(key);\n\n\trcu_read_lock();\n\thlist_for_each_entry_rcu(k, hash_head, hash_entry) {\n\t\tif (k == key) {\n\t\t\tfound = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\treturn found;\n}\n\n \nstatic struct lock_class *\nregister_lock_class(struct lockdep_map *lock, unsigned int subclass, int force)\n{\n\tstruct lockdep_subclass_key *key;\n\tstruct hlist_head *hash_head;\n\tstruct lock_class *class;\n\tint idx;\n\n\tDEBUG_LOCKS_WARN_ON(!irqs_disabled());\n\n\tclass = look_up_lock_class(lock, subclass);\n\tif (likely(class))\n\t\tgoto out_set_class_cache;\n\n\tif (!lock->key) {\n\t\tif (!assign_lock_key(lock))\n\t\t\treturn NULL;\n\t} else if (!static_obj(lock->key) && !is_dynamic_key(lock->key)) {\n\t\treturn NULL;\n\t}\n\n\tkey = lock->key->subkeys + subclass;\n\thash_head = classhashentry(key);\n\n\tif (!graph_lock()) {\n\t\treturn NULL;\n\t}\n\t \n\thlist_for_each_entry_rcu(class, hash_head, hash_entry) {\n\t\tif (class->key == key)\n\t\t\tgoto out_unlock_set;\n\t}\n\n\tinit_data_structures_once();\n\n\t \n\tclass = list_first_entry_or_null(&free_lock_classes, typeof(*class),\n\t\t\t\t\t lock_entry);\n\tif (!class) {\n\t\tif (!debug_locks_off_graph_unlock()) {\n\t\t\treturn NULL;\n\t\t}\n\n\t\tprint_lockdep_off(\"BUG: MAX_LOCKDEP_KEYS too low!\");\n\t\tdump_stack();\n\t\treturn NULL;\n\t}\n\tnr_lock_classes++;\n\t__set_bit(class - lock_classes, lock_classes_in_use);\n\tdebug_atomic_inc(nr_unused_locks);\n\tclass->key = key;\n\tclass->name = lock->name;\n\tclass->subclass = subclass;\n\tWARN_ON_ONCE(!list_empty(&class->locks_before));\n\tWARN_ON_ONCE(!list_empty(&class->locks_after));\n\tclass->name_version = count_matching_names(class);\n\tclass->wait_type_inner = lock->wait_type_inner;\n\tclass->wait_type_outer = lock->wait_type_outer;\n\tclass->lock_type = lock->lock_type;\n\t \n\thlist_add_head_rcu(&class->hash_entry, hash_head);\n\t \n\tlist_move_tail(&class->lock_entry, &all_lock_classes);\n\tidx = class - lock_classes;\n\tif (idx > max_lock_class_idx)\n\t\tmax_lock_class_idx = idx;\n\n\tif (verbose(class)) {\n\t\tgraph_unlock();\n\n\t\tprintk(\"\\nnew class %px: %s\", class->key, class->name);\n\t\tif (class->name_version > 1)\n\t\t\tprintk(KERN_CONT \"#%d\", class->name_version);\n\t\tprintk(KERN_CONT \"\\n\");\n\t\tdump_stack();\n\n\t\tif (!graph_lock()) {\n\t\t\treturn NULL;\n\t\t}\n\t}\nout_unlock_set:\n\tgraph_unlock();\n\nout_set_class_cache:\n\tif (!subclass || force)\n\t\tlock->class_cache[0] = class;\n\telse if (subclass < NR_LOCKDEP_CACHING_CLASSES)\n\t\tlock->class_cache[subclass] = class;\n\n\t \n\tif (DEBUG_LOCKS_WARN_ON(class->subclass != subclass))\n\t\treturn NULL;\n\n\treturn class;\n}\n\n#ifdef CONFIG_PROVE_LOCKING\n \nstatic struct lock_list *alloc_list_entry(void)\n{\n\tint idx = find_first_zero_bit(list_entries_in_use,\n\t\t\t\t      ARRAY_SIZE(list_entries));\n\n\tif (idx >= ARRAY_SIZE(list_entries)) {\n\t\tif (!debug_locks_off_graph_unlock())\n\t\t\treturn NULL;\n\n\t\tprint_lockdep_off(\"BUG: MAX_LOCKDEP_ENTRIES too low!\");\n\t\tdump_stack();\n\t\treturn NULL;\n\t}\n\tnr_list_entries++;\n\t__set_bit(idx, list_entries_in_use);\n\treturn list_entries + idx;\n}\n\n \nstatic int add_lock_to_list(struct lock_class *this,\n\t\t\t    struct lock_class *links_to, struct list_head *head,\n\t\t\t    u16 distance, u8 dep,\n\t\t\t    const struct lock_trace *trace)\n{\n\tstruct lock_list *entry;\n\t \n\tentry = alloc_list_entry();\n\tif (!entry)\n\t\treturn 0;\n\n\tentry->class = this;\n\tentry->links_to = links_to;\n\tentry->dep = dep;\n\tentry->distance = distance;\n\tentry->trace = trace;\n\t \n\tlist_add_tail_rcu(&entry->entry, head);\n\n\treturn 1;\n}\n\n \n#define MAX_CIRCULAR_QUEUE_SIZE\t\t(1UL << CONFIG_LOCKDEP_CIRCULAR_QUEUE_BITS)\n#define CQ_MASK\t\t\t\t(MAX_CIRCULAR_QUEUE_SIZE-1)\n\n \nstruct circular_queue {\n\tstruct lock_list *element[MAX_CIRCULAR_QUEUE_SIZE];\n\tunsigned int  front, rear;\n};\n\nstatic struct circular_queue lock_cq;\n\nunsigned int max_bfs_queue_depth;\n\nstatic unsigned int lockdep_dependency_gen_id;\n\nstatic inline void __cq_init(struct circular_queue *cq)\n{\n\tcq->front = cq->rear = 0;\n\tlockdep_dependency_gen_id++;\n}\n\nstatic inline int __cq_empty(struct circular_queue *cq)\n{\n\treturn (cq->front == cq->rear);\n}\n\nstatic inline int __cq_full(struct circular_queue *cq)\n{\n\treturn ((cq->rear + 1) & CQ_MASK) == cq->front;\n}\n\nstatic inline int __cq_enqueue(struct circular_queue *cq, struct lock_list *elem)\n{\n\tif (__cq_full(cq))\n\t\treturn -1;\n\n\tcq->element[cq->rear] = elem;\n\tcq->rear = (cq->rear + 1) & CQ_MASK;\n\treturn 0;\n}\n\n \nstatic inline struct lock_list * __cq_dequeue(struct circular_queue *cq)\n{\n\tstruct lock_list * lock;\n\n\tif (__cq_empty(cq))\n\t\treturn NULL;\n\n\tlock = cq->element[cq->front];\n\tcq->front = (cq->front + 1) & CQ_MASK;\n\n\treturn lock;\n}\n\nstatic inline unsigned int  __cq_get_elem_count(struct circular_queue *cq)\n{\n\treturn (cq->rear - cq->front) & CQ_MASK;\n}\n\nstatic inline void mark_lock_accessed(struct lock_list *lock)\n{\n\tlock->class->dep_gen_id = lockdep_dependency_gen_id;\n}\n\nstatic inline void visit_lock_entry(struct lock_list *lock,\n\t\t\t\t    struct lock_list *parent)\n{\n\tlock->parent = parent;\n}\n\nstatic inline unsigned long lock_accessed(struct lock_list *lock)\n{\n\treturn lock->class->dep_gen_id == lockdep_dependency_gen_id;\n}\n\nstatic inline struct lock_list *get_lock_parent(struct lock_list *child)\n{\n\treturn child->parent;\n}\n\nstatic inline int get_lock_depth(struct lock_list *child)\n{\n\tint depth = 0;\n\tstruct lock_list *parent;\n\n\twhile ((parent = get_lock_parent(child))) {\n\t\tchild = parent;\n\t\tdepth++;\n\t}\n\treturn depth;\n}\n\n \nstatic inline struct list_head *get_dep_list(struct lock_list *lock, int offset)\n{\n\tvoid *lock_class = lock->class;\n\n\treturn lock_class + offset;\n}\n \nenum bfs_result {\n\tBFS_EINVALIDNODE = -2,\n\tBFS_EQUEUEFULL = -1,\n\tBFS_RMATCH = 0,\n\tBFS_RNOMATCH = 1,\n};\n\n \nstatic inline bool bfs_error(enum bfs_result res)\n{\n\treturn res < 0;\n}\n\n \n#define DEP_SR_BIT (0 + (0 << 1))  \n#define DEP_ER_BIT (1 + (0 << 1))  \n#define DEP_SN_BIT (0 + (1 << 1))  \n#define DEP_EN_BIT (1 + (1 << 1))  \n\n#define DEP_SR_MASK (1U << (DEP_SR_BIT))\n#define DEP_ER_MASK (1U << (DEP_ER_BIT))\n#define DEP_SN_MASK (1U << (DEP_SN_BIT))\n#define DEP_EN_MASK (1U << (DEP_EN_BIT))\n\nstatic inline unsigned int\n__calc_dep_bit(struct held_lock *prev, struct held_lock *next)\n{\n\treturn (prev->read == 0) + ((next->read != 2) << 1);\n}\n\nstatic inline u8 calc_dep(struct held_lock *prev, struct held_lock *next)\n{\n\treturn 1U << __calc_dep_bit(prev, next);\n}\n\n \nstatic inline unsigned int\n__calc_dep_bitb(struct held_lock *prev, struct held_lock *next)\n{\n\treturn (next->read != 2) + ((prev->read == 0) << 1);\n}\n\nstatic inline u8 calc_depb(struct held_lock *prev, struct held_lock *next)\n{\n\treturn 1U << __calc_dep_bitb(prev, next);\n}\n\n \nstatic inline void __bfs_init_root(struct lock_list *lock,\n\t\t\t\t   struct lock_class *class)\n{\n\tlock->class = class;\n\tlock->parent = NULL;\n\tlock->only_xr = 0;\n}\n\n \nstatic inline void bfs_init_root(struct lock_list *lock,\n\t\t\t\t struct held_lock *hlock)\n{\n\t__bfs_init_root(lock, hlock_class(hlock));\n\tlock->only_xr = (hlock->read == 2);\n}\n\n \nstatic inline void bfs_init_rootb(struct lock_list *lock,\n\t\t\t\t  struct held_lock *hlock)\n{\n\t__bfs_init_root(lock, hlock_class(hlock));\n\tlock->only_xr = (hlock->read != 0);\n}\n\nstatic inline struct lock_list *__bfs_next(struct lock_list *lock, int offset)\n{\n\tif (!lock || !lock->parent)\n\t\treturn NULL;\n\n\treturn list_next_or_null_rcu(get_dep_list(lock->parent, offset),\n\t\t\t\t     &lock->entry, struct lock_list, entry);\n}\n\n \nstatic enum bfs_result __bfs(struct lock_list *source_entry,\n\t\t\t     void *data,\n\t\t\t     bool (*match)(struct lock_list *entry, void *data),\n\t\t\t     bool (*skip)(struct lock_list *entry, void *data),\n\t\t\t     struct lock_list **target_entry,\n\t\t\t     int offset)\n{\n\tstruct circular_queue *cq = &lock_cq;\n\tstruct lock_list *lock = NULL;\n\tstruct lock_list *entry;\n\tstruct list_head *head;\n\tunsigned int cq_depth;\n\tbool first;\n\n\tlockdep_assert_locked();\n\n\t__cq_init(cq);\n\t__cq_enqueue(cq, source_entry);\n\n\twhile ((lock = __bfs_next(lock, offset)) || (lock = __cq_dequeue(cq))) {\n\t\tif (!lock->class)\n\t\t\treturn BFS_EINVALIDNODE;\n\n\t\t \n\t\tif (lock_accessed(lock))\n\t\t\tcontinue;\n\t\telse\n\t\t\tmark_lock_accessed(lock);\n\n\t\t \n\t\tif (lock->parent) {  \n\t\t\tu8 dep = lock->dep;\n\t\t\tbool prev_only_xr = lock->parent->only_xr;\n\n\t\t\t \n\t\t\tif (prev_only_xr)\n\t\t\t\tdep &= ~(DEP_SR_MASK | DEP_SN_MASK);\n\n\t\t\t \n\t\t\tif (!dep)\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tlock->only_xr = !(dep & (DEP_SN_MASK | DEP_EN_MASK));\n\t\t}\n\n\t\t \n\t\tif (skip && skip(lock, data))\n\t\t\tcontinue;\n\n\t\tif (match(lock, data)) {\n\t\t\t*target_entry = lock;\n\t\t\treturn BFS_RMATCH;\n\t\t}\n\n\t\t \n\t\tfirst = true;\n\t\thead = get_dep_list(lock, offset);\n\t\tlist_for_each_entry_rcu(entry, head, entry) {\n\t\t\tvisit_lock_entry(entry, lock);\n\n\t\t\t \n\t\t\tif (!first)\n\t\t\t\tcontinue;\n\n\t\t\tfirst = false;\n\n\t\t\tif (__cq_enqueue(cq, entry))\n\t\t\t\treturn BFS_EQUEUEFULL;\n\n\t\t\tcq_depth = __cq_get_elem_count(cq);\n\t\t\tif (max_bfs_queue_depth < cq_depth)\n\t\t\t\tmax_bfs_queue_depth = cq_depth;\n\t\t}\n\t}\n\n\treturn BFS_RNOMATCH;\n}\n\nstatic inline enum bfs_result\n__bfs_forwards(struct lock_list *src_entry,\n\t       void *data,\n\t       bool (*match)(struct lock_list *entry, void *data),\n\t       bool (*skip)(struct lock_list *entry, void *data),\n\t       struct lock_list **target_entry)\n{\n\treturn __bfs(src_entry, data, match, skip, target_entry,\n\t\t     offsetof(struct lock_class, locks_after));\n\n}\n\nstatic inline enum bfs_result\n__bfs_backwards(struct lock_list *src_entry,\n\t\tvoid *data,\n\t\tbool (*match)(struct lock_list *entry, void *data),\n\t       bool (*skip)(struct lock_list *entry, void *data),\n\t\tstruct lock_list **target_entry)\n{\n\treturn __bfs(src_entry, data, match, skip, target_entry,\n\t\t     offsetof(struct lock_class, locks_before));\n\n}\n\nstatic void print_lock_trace(const struct lock_trace *trace,\n\t\t\t     unsigned int spaces)\n{\n\tstack_trace_print(trace->entries, trace->nr_entries, spaces);\n}\n\n \nstatic noinline void\nprint_circular_bug_entry(struct lock_list *target, int depth)\n{\n\tif (debug_locks_silent)\n\t\treturn;\n\tprintk(\"\\n-> #%u\", depth);\n\tprint_lock_name(NULL, target->class);\n\tprintk(KERN_CONT \":\\n\");\n\tprint_lock_trace(target->trace, 6);\n}\n\nstatic void\nprint_circular_lock_scenario(struct held_lock *src,\n\t\t\t     struct held_lock *tgt,\n\t\t\t     struct lock_list *prt)\n{\n\tstruct lock_class *source = hlock_class(src);\n\tstruct lock_class *target = hlock_class(tgt);\n\tstruct lock_class *parent = prt->class;\n\tint src_read = src->read;\n\tint tgt_read = tgt->read;\n\n\t \n\tif (parent != source) {\n\t\tprintk(\"Chain exists of:\\n  \");\n\t\t__print_lock_name(src, source);\n\t\tprintk(KERN_CONT \" --> \");\n\t\t__print_lock_name(NULL, parent);\n\t\tprintk(KERN_CONT \" --> \");\n\t\t__print_lock_name(tgt, target);\n\t\tprintk(KERN_CONT \"\\n\\n\");\n\t}\n\n\tprintk(\" Possible unsafe locking scenario:\\n\\n\");\n\tprintk(\"       CPU0                    CPU1\\n\");\n\tprintk(\"       ----                    ----\\n\");\n\tif (tgt_read != 0)\n\t\tprintk(\"  rlock(\");\n\telse\n\t\tprintk(\"  lock(\");\n\t__print_lock_name(tgt, target);\n\tprintk(KERN_CONT \");\\n\");\n\tprintk(\"                               lock(\");\n\t__print_lock_name(NULL, parent);\n\tprintk(KERN_CONT \");\\n\");\n\tprintk(\"                               lock(\");\n\t__print_lock_name(tgt, target);\n\tprintk(KERN_CONT \");\\n\");\n\tif (src_read != 0)\n\t\tprintk(\"  rlock(\");\n\telse if (src->sync)\n\t\tprintk(\"  sync(\");\n\telse\n\t\tprintk(\"  lock(\");\n\t__print_lock_name(src, source);\n\tprintk(KERN_CONT \");\\n\");\n\tprintk(\"\\n *** DEADLOCK ***\\n\\n\");\n}\n\n \nstatic noinline void\nprint_circular_bug_header(struct lock_list *entry, unsigned int depth,\n\t\t\tstruct held_lock *check_src,\n\t\t\tstruct held_lock *check_tgt)\n{\n\tstruct task_struct *curr = current;\n\n\tif (debug_locks_silent)\n\t\treturn;\n\n\tpr_warn(\"\\n\");\n\tpr_warn(\"======================================================\\n\");\n\tpr_warn(\"WARNING: possible circular locking dependency detected\\n\");\n\tprint_kernel_ident();\n\tpr_warn(\"------------------------------------------------------\\n\");\n\tpr_warn(\"%s/%d is trying to acquire lock:\\n\",\n\t\tcurr->comm, task_pid_nr(curr));\n\tprint_lock(check_src);\n\n\tpr_warn(\"\\nbut task is already holding lock:\\n\");\n\n\tprint_lock(check_tgt);\n\tpr_warn(\"\\nwhich lock already depends on the new lock.\\n\\n\");\n\tpr_warn(\"\\nthe existing dependency chain (in reverse order) is:\\n\");\n\n\tprint_circular_bug_entry(entry, depth);\n}\n\n \nstatic inline bool hlock_equal(struct lock_list *entry, void *data)\n{\n\tstruct held_lock *hlock = (struct held_lock *)data;\n\n\treturn hlock_class(hlock) == entry->class &&  \n\t       (hlock->read == 2 ||   \n\t\t!entry->only_xr);  \n}\n\n \nstatic inline bool hlock_conflict(struct lock_list *entry, void *data)\n{\n\tstruct held_lock *hlock = (struct held_lock *)data;\n\n\treturn hlock_class(hlock) == entry->class &&  \n\t       (hlock->read == 0 ||  \n\t\t!entry->only_xr);  \n}\n\nstatic noinline void print_circular_bug(struct lock_list *this,\n\t\t\t\tstruct lock_list *target,\n\t\t\t\tstruct held_lock *check_src,\n\t\t\t\tstruct held_lock *check_tgt)\n{\n\tstruct task_struct *curr = current;\n\tstruct lock_list *parent;\n\tstruct lock_list *first_parent;\n\tint depth;\n\n\tif (!debug_locks_off_graph_unlock() || debug_locks_silent)\n\t\treturn;\n\n\tthis->trace = save_trace();\n\tif (!this->trace)\n\t\treturn;\n\n\tdepth = get_lock_depth(target);\n\n\tprint_circular_bug_header(target, depth, check_src, check_tgt);\n\n\tparent = get_lock_parent(target);\n\tfirst_parent = parent;\n\n\twhile (parent) {\n\t\tprint_circular_bug_entry(parent, --depth);\n\t\tparent = get_lock_parent(parent);\n\t}\n\n\tprintk(\"\\nother info that might help us debug this:\\n\\n\");\n\tprint_circular_lock_scenario(check_src, check_tgt,\n\t\t\t\t     first_parent);\n\n\tlockdep_print_held_locks(curr);\n\n\tprintk(\"\\nstack backtrace:\\n\");\n\tdump_stack();\n}\n\nstatic noinline void print_bfs_bug(int ret)\n{\n\tif (!debug_locks_off_graph_unlock())\n\t\treturn;\n\n\t \n\tWARN(1, \"lockdep bfs error:%d\\n\", ret);\n}\n\nstatic bool noop_count(struct lock_list *entry, void *data)\n{\n\t(*(unsigned long *)data)++;\n\treturn false;\n}\n\nstatic unsigned long __lockdep_count_forward_deps(struct lock_list *this)\n{\n\tunsigned long  count = 0;\n\tstruct lock_list *target_entry;\n\n\t__bfs_forwards(this, (void *)&count, noop_count, NULL, &target_entry);\n\n\treturn count;\n}\nunsigned long lockdep_count_forward_deps(struct lock_class *class)\n{\n\tunsigned long ret, flags;\n\tstruct lock_list this;\n\n\t__bfs_init_root(&this, class);\n\n\traw_local_irq_save(flags);\n\tlockdep_lock();\n\tret = __lockdep_count_forward_deps(&this);\n\tlockdep_unlock();\n\traw_local_irq_restore(flags);\n\n\treturn ret;\n}\n\nstatic unsigned long __lockdep_count_backward_deps(struct lock_list *this)\n{\n\tunsigned long  count = 0;\n\tstruct lock_list *target_entry;\n\n\t__bfs_backwards(this, (void *)&count, noop_count, NULL, &target_entry);\n\n\treturn count;\n}\n\nunsigned long lockdep_count_backward_deps(struct lock_class *class)\n{\n\tunsigned long ret, flags;\n\tstruct lock_list this;\n\n\t__bfs_init_root(&this, class);\n\n\traw_local_irq_save(flags);\n\tlockdep_lock();\n\tret = __lockdep_count_backward_deps(&this);\n\tlockdep_unlock();\n\traw_local_irq_restore(flags);\n\n\treturn ret;\n}\n\n \nstatic noinline enum bfs_result\ncheck_path(struct held_lock *target, struct lock_list *src_entry,\n\t   bool (*match)(struct lock_list *entry, void *data),\n\t   bool (*skip)(struct lock_list *entry, void *data),\n\t   struct lock_list **target_entry)\n{\n\tenum bfs_result ret;\n\n\tret = __bfs_forwards(src_entry, target, match, skip, target_entry);\n\n\tif (unlikely(bfs_error(ret)))\n\t\tprint_bfs_bug(ret);\n\n\treturn ret;\n}\n\nstatic void print_deadlock_bug(struct task_struct *, struct held_lock *, struct held_lock *);\n\n \nstatic noinline enum bfs_result\ncheck_noncircular(struct held_lock *src, struct held_lock *target,\n\t\t  struct lock_trace **const trace)\n{\n\tenum bfs_result ret;\n\tstruct lock_list *target_entry;\n\tstruct lock_list src_entry;\n\n\tbfs_init_root(&src_entry, src);\n\n\tdebug_atomic_inc(nr_cyclic_checks);\n\n\tret = check_path(target, &src_entry, hlock_conflict, NULL, &target_entry);\n\n\tif (unlikely(ret == BFS_RMATCH)) {\n\t\tif (!*trace) {\n\t\t\t \n\t\t\t*trace = save_trace();\n\t\t}\n\n\t\tif (src->class_idx == target->class_idx)\n\t\t\tprint_deadlock_bug(current, src, target);\n\t\telse\n\t\t\tprint_circular_bug(&src_entry, target_entry, src, target);\n\t}\n\n\treturn ret;\n}\n\n#ifdef CONFIG_TRACE_IRQFLAGS\n\n \n\n \nstatic inline bool usage_accumulate(struct lock_list *entry, void *mask)\n{\n\tif (!entry->only_xr)\n\t\t*(unsigned long *)mask |= entry->class->usage_mask;\n\telse  \n\t\t*(unsigned long *)mask |= (entry->class->usage_mask & LOCKF_IRQ);\n\n\treturn false;\n}\n\n \nstatic inline bool usage_match(struct lock_list *entry, void *mask)\n{\n\tif (!entry->only_xr)\n\t\treturn !!(entry->class->usage_mask & *(unsigned long *)mask);\n\telse  \n\t\treturn !!((entry->class->usage_mask & LOCKF_IRQ) & *(unsigned long *)mask);\n}\n\nstatic inline bool usage_skip(struct lock_list *entry, void *mask)\n{\n\tif (entry->class->lock_type == LD_LOCK_NORMAL)\n\t\treturn false;\n\n\t \n\tif (entry->class->lock_type == LD_LOCK_PERCPU &&\n\t    DEBUG_LOCKS_WARN_ON(entry->class->wait_type_inner < LD_WAIT_CONFIG))\n\t\treturn false;\n\n\t \n\n\treturn true;\n}\n\n \nstatic enum bfs_result\nfind_usage_forwards(struct lock_list *root, unsigned long usage_mask,\n\t\t\tstruct lock_list **target_entry)\n{\n\tenum bfs_result result;\n\n\tdebug_atomic_inc(nr_find_usage_forwards_checks);\n\n\tresult = __bfs_forwards(root, &usage_mask, usage_match, usage_skip, target_entry);\n\n\treturn result;\n}\n\n \nstatic enum bfs_result\nfind_usage_backwards(struct lock_list *root, unsigned long usage_mask,\n\t\t\tstruct lock_list **target_entry)\n{\n\tenum bfs_result result;\n\n\tdebug_atomic_inc(nr_find_usage_backwards_checks);\n\n\tresult = __bfs_backwards(root, &usage_mask, usage_match, usage_skip, target_entry);\n\n\treturn result;\n}\n\nstatic void print_lock_class_header(struct lock_class *class, int depth)\n{\n\tint bit;\n\n\tprintk(\"%*s->\", depth, \"\");\n\tprint_lock_name(NULL, class);\n#ifdef CONFIG_DEBUG_LOCKDEP\n\tprintk(KERN_CONT \" ops: %lu\", debug_class_ops_read(class));\n#endif\n\tprintk(KERN_CONT \" {\\n\");\n\n\tfor (bit = 0; bit < LOCK_TRACE_STATES; bit++) {\n\t\tif (class->usage_mask & (1 << bit)) {\n\t\t\tint len = depth;\n\n\t\t\tlen += printk(\"%*s   %s\", depth, \"\", usage_str[bit]);\n\t\t\tlen += printk(KERN_CONT \" at:\\n\");\n\t\t\tprint_lock_trace(class->usage_traces[bit], len);\n\t\t}\n\t}\n\tprintk(\"%*s }\\n\", depth, \"\");\n\n\tprintk(\"%*s ... key      at: [<%px>] %pS\\n\",\n\t\tdepth, \"\", class->key, class->key);\n}\n\n \n\n \nstatic void __used\nprint_shortest_lock_dependencies(struct lock_list *leaf,\n\t\t\t\t struct lock_list *root)\n{\n\tstruct lock_list *entry = leaf;\n\tint depth;\n\n\t \n\tdepth = get_lock_depth(leaf);\n\n\tdo {\n\t\tprint_lock_class_header(entry->class, depth);\n\t\tprintk(\"%*s ... acquired at:\\n\", depth, \"\");\n\t\tprint_lock_trace(entry->trace, 2);\n\t\tprintk(\"\\n\");\n\n\t\tif (depth == 0 && (entry != root)) {\n\t\t\tprintk(\"lockdep:%s bad path found in chain graph\\n\", __func__);\n\t\t\tbreak;\n\t\t}\n\n\t\tentry = get_lock_parent(entry);\n\t\tdepth--;\n\t} while (entry && (depth >= 0));\n}\n\n \nstatic void __used\nprint_shortest_lock_dependencies_backwards(struct lock_list *leaf,\n\t\t\t\t\t   struct lock_list *root)\n{\n\tstruct lock_list *entry = leaf;\n\tconst struct lock_trace *trace = NULL;\n\tint depth;\n\n\t \n\tdepth = get_lock_depth(leaf);\n\n\tdo {\n\t\tprint_lock_class_header(entry->class, depth);\n\t\tif (trace) {\n\t\t\tprintk(\"%*s ... acquired at:\\n\", depth, \"\");\n\t\t\tprint_lock_trace(trace, 2);\n\t\t\tprintk(\"\\n\");\n\t\t}\n\n\t\t \n\t\ttrace = entry->trace;\n\n\t\tif (depth == 0 && (entry != root)) {\n\t\t\tprintk(\"lockdep:%s bad path found in chain graph\\n\", __func__);\n\t\t\tbreak;\n\t\t}\n\n\t\tentry = get_lock_parent(entry);\n\t\tdepth--;\n\t} while (entry && (depth >= 0));\n}\n\nstatic void\nprint_irq_lock_scenario(struct lock_list *safe_entry,\n\t\t\tstruct lock_list *unsafe_entry,\n\t\t\tstruct lock_class *prev_class,\n\t\t\tstruct lock_class *next_class)\n{\n\tstruct lock_class *safe_class = safe_entry->class;\n\tstruct lock_class *unsafe_class = unsafe_entry->class;\n\tstruct lock_class *middle_class = prev_class;\n\n\tif (middle_class == safe_class)\n\t\tmiddle_class = next_class;\n\n\t \n\tif (middle_class != unsafe_class) {\n\t\tprintk(\"Chain exists of:\\n  \");\n\t\t__print_lock_name(NULL, safe_class);\n\t\tprintk(KERN_CONT \" --> \");\n\t\t__print_lock_name(NULL, middle_class);\n\t\tprintk(KERN_CONT \" --> \");\n\t\t__print_lock_name(NULL, unsafe_class);\n\t\tprintk(KERN_CONT \"\\n\\n\");\n\t}\n\n\tprintk(\" Possible interrupt unsafe locking scenario:\\n\\n\");\n\tprintk(\"       CPU0                    CPU1\\n\");\n\tprintk(\"       ----                    ----\\n\");\n\tprintk(\"  lock(\");\n\t__print_lock_name(NULL, unsafe_class);\n\tprintk(KERN_CONT \");\\n\");\n\tprintk(\"                               local_irq_disable();\\n\");\n\tprintk(\"                               lock(\");\n\t__print_lock_name(NULL, safe_class);\n\tprintk(KERN_CONT \");\\n\");\n\tprintk(\"                               lock(\");\n\t__print_lock_name(NULL, middle_class);\n\tprintk(KERN_CONT \");\\n\");\n\tprintk(\"  <Interrupt>\\n\");\n\tprintk(\"    lock(\");\n\t__print_lock_name(NULL, safe_class);\n\tprintk(KERN_CONT \");\\n\");\n\tprintk(\"\\n *** DEADLOCK ***\\n\\n\");\n}\n\nstatic void\nprint_bad_irq_dependency(struct task_struct *curr,\n\t\t\t struct lock_list *prev_root,\n\t\t\t struct lock_list *next_root,\n\t\t\t struct lock_list *backwards_entry,\n\t\t\t struct lock_list *forwards_entry,\n\t\t\t struct held_lock *prev,\n\t\t\t struct held_lock *next,\n\t\t\t enum lock_usage_bit bit1,\n\t\t\t enum lock_usage_bit bit2,\n\t\t\t const char *irqclass)\n{\n\tif (!debug_locks_off_graph_unlock() || debug_locks_silent)\n\t\treturn;\n\n\tpr_warn(\"\\n\");\n\tpr_warn(\"=====================================================\\n\");\n\tpr_warn(\"WARNING: %s-safe -> %s-unsafe lock order detected\\n\",\n\t\tirqclass, irqclass);\n\tprint_kernel_ident();\n\tpr_warn(\"-----------------------------------------------------\\n\");\n\tpr_warn(\"%s/%d [HC%u[%lu]:SC%u[%lu]:HE%u:SE%u] is trying to acquire:\\n\",\n\t\tcurr->comm, task_pid_nr(curr),\n\t\tlockdep_hardirq_context(), hardirq_count() >> HARDIRQ_SHIFT,\n\t\tcurr->softirq_context, softirq_count() >> SOFTIRQ_SHIFT,\n\t\tlockdep_hardirqs_enabled(),\n\t\tcurr->softirqs_enabled);\n\tprint_lock(next);\n\n\tpr_warn(\"\\nand this task is already holding:\\n\");\n\tprint_lock(prev);\n\tpr_warn(\"which would create a new lock dependency:\\n\");\n\tprint_lock_name(prev, hlock_class(prev));\n\tpr_cont(\" ->\");\n\tprint_lock_name(next, hlock_class(next));\n\tpr_cont(\"\\n\");\n\n\tpr_warn(\"\\nbut this new dependency connects a %s-irq-safe lock:\\n\",\n\t\tirqclass);\n\tprint_lock_name(NULL, backwards_entry->class);\n\tpr_warn(\"\\n... which became %s-irq-safe at:\\n\", irqclass);\n\n\tprint_lock_trace(backwards_entry->class->usage_traces[bit1], 1);\n\n\tpr_warn(\"\\nto a %s-irq-unsafe lock:\\n\", irqclass);\n\tprint_lock_name(NULL, forwards_entry->class);\n\tpr_warn(\"\\n... which became %s-irq-unsafe at:\\n\", irqclass);\n\tpr_warn(\"...\");\n\n\tprint_lock_trace(forwards_entry->class->usage_traces[bit2], 1);\n\n\tpr_warn(\"\\nother info that might help us debug this:\\n\\n\");\n\tprint_irq_lock_scenario(backwards_entry, forwards_entry,\n\t\t\t\thlock_class(prev), hlock_class(next));\n\n\tlockdep_print_held_locks(curr);\n\n\tpr_warn(\"\\nthe dependencies between %s-irq-safe lock and the holding lock:\\n\", irqclass);\n\tprint_shortest_lock_dependencies_backwards(backwards_entry, prev_root);\n\n\tpr_warn(\"\\nthe dependencies between the lock to be acquired\");\n\tpr_warn(\" and %s-irq-unsafe lock:\\n\", irqclass);\n\tnext_root->trace = save_trace();\n\tif (!next_root->trace)\n\t\treturn;\n\tprint_shortest_lock_dependencies(forwards_entry, next_root);\n\n\tpr_warn(\"\\nstack backtrace:\\n\");\n\tdump_stack();\n}\n\nstatic const char *state_names[] = {\n#define LOCKDEP_STATE(__STATE) \\\n\t__stringify(__STATE),\n#include \"lockdep_states.h\"\n#undef LOCKDEP_STATE\n};\n\nstatic const char *state_rnames[] = {\n#define LOCKDEP_STATE(__STATE) \\\n\t__stringify(__STATE)\"-READ\",\n#include \"lockdep_states.h\"\n#undef LOCKDEP_STATE\n};\n\nstatic inline const char *state_name(enum lock_usage_bit bit)\n{\n\tif (bit & LOCK_USAGE_READ_MASK)\n\t\treturn state_rnames[bit >> LOCK_USAGE_DIR_MASK];\n\telse\n\t\treturn state_names[bit >> LOCK_USAGE_DIR_MASK];\n}\n\n \nstatic int exclusive_bit(int new_bit)\n{\n\tint state = new_bit & LOCK_USAGE_STATE_MASK;\n\tint dir = new_bit & LOCK_USAGE_DIR_MASK;\n\n\t \n\treturn state | (dir ^ LOCK_USAGE_DIR_MASK);\n}\n\n \nstatic unsigned long invert_dir_mask(unsigned long mask)\n{\n\tunsigned long excl = 0;\n\n\t \n\texcl |= (mask & LOCKF_ENABLED_IRQ_ALL) >> LOCK_USAGE_DIR_MASK;\n\texcl |= (mask & LOCKF_USED_IN_IRQ_ALL) << LOCK_USAGE_DIR_MASK;\n\n\treturn excl;\n}\n\n \nstatic unsigned long exclusive_mask(unsigned long mask)\n{\n\tunsigned long excl = invert_dir_mask(mask);\n\n\texcl |= (excl & LOCKF_IRQ_READ) >> LOCK_USAGE_READ_MASK;\n\texcl |= (excl & LOCKF_IRQ) << LOCK_USAGE_READ_MASK;\n\n\treturn excl;\n}\n\n \nstatic unsigned long original_mask(unsigned long mask)\n{\n\tunsigned long excl = invert_dir_mask(mask);\n\n\t \n\texcl |= (excl & LOCKF_IRQ_READ) >> LOCK_USAGE_READ_MASK;\n\texcl |= (excl & LOCKF_IRQ) << LOCK_USAGE_READ_MASK;\n\n\treturn excl;\n}\n\n \nstatic int find_exclusive_match(unsigned long mask,\n\t\t\t\tunsigned long excl_mask,\n\t\t\t\tenum lock_usage_bit *bitp,\n\t\t\t\tenum lock_usage_bit *excl_bitp)\n{\n\tint bit, excl, excl_read;\n\n\tfor_each_set_bit(bit, &mask, LOCK_USED) {\n\t\t \n\t\texcl = exclusive_bit(bit);\n\t\texcl_read = excl | LOCK_USAGE_READ_MASK;\n\t\tif (excl_mask & lock_flag(excl)) {\n\t\t\t*bitp = bit;\n\t\t\t*excl_bitp = excl;\n\t\t\treturn 0;\n\t\t} else if (excl_mask & lock_flag(excl_read)) {\n\t\t\t*bitp = bit;\n\t\t\t*excl_bitp = excl_read;\n\t\t\treturn 0;\n\t\t}\n\t}\n\treturn -1;\n}\n\n \nstatic int check_irq_usage(struct task_struct *curr, struct held_lock *prev,\n\t\t\t   struct held_lock *next)\n{\n\tunsigned long usage_mask = 0, forward_mask, backward_mask;\n\tenum lock_usage_bit forward_bit = 0, backward_bit = 0;\n\tstruct lock_list *target_entry1;\n\tstruct lock_list *target_entry;\n\tstruct lock_list this, that;\n\tenum bfs_result ret;\n\n\t \n\tbfs_init_rootb(&this, prev);\n\n\tret = __bfs_backwards(&this, &usage_mask, usage_accumulate, usage_skip, NULL);\n\tif (bfs_error(ret)) {\n\t\tprint_bfs_bug(ret);\n\t\treturn 0;\n\t}\n\n\tusage_mask &= LOCKF_USED_IN_IRQ_ALL;\n\tif (!usage_mask)\n\t\treturn 1;\n\n\t \n\tforward_mask = exclusive_mask(usage_mask);\n\n\tbfs_init_root(&that, next);\n\n\tret = find_usage_forwards(&that, forward_mask, &target_entry1);\n\tif (bfs_error(ret)) {\n\t\tprint_bfs_bug(ret);\n\t\treturn 0;\n\t}\n\tif (ret == BFS_RNOMATCH)\n\t\treturn 1;\n\n\t \n\tbackward_mask = original_mask(target_entry1->class->usage_mask & LOCKF_ENABLED_IRQ_ALL);\n\n\tret = find_usage_backwards(&this, backward_mask, &target_entry);\n\tif (bfs_error(ret)) {\n\t\tprint_bfs_bug(ret);\n\t\treturn 0;\n\t}\n\tif (DEBUG_LOCKS_WARN_ON(ret == BFS_RNOMATCH))\n\t\treturn 1;\n\n\t \n\tret = find_exclusive_match(target_entry->class->usage_mask,\n\t\t\t\t   target_entry1->class->usage_mask,\n\t\t\t\t   &backward_bit, &forward_bit);\n\tif (DEBUG_LOCKS_WARN_ON(ret == -1))\n\t\treturn 1;\n\n\tprint_bad_irq_dependency(curr, &this, &that,\n\t\t\t\t target_entry, target_entry1,\n\t\t\t\t prev, next,\n\t\t\t\t backward_bit, forward_bit,\n\t\t\t\t state_name(backward_bit));\n\n\treturn 0;\n}\n\n#else\n\nstatic inline int check_irq_usage(struct task_struct *curr,\n\t\t\t\t  struct held_lock *prev, struct held_lock *next)\n{\n\treturn 1;\n}\n\nstatic inline bool usage_skip(struct lock_list *entry, void *mask)\n{\n\treturn false;\n}\n\n#endif  \n\n#ifdef CONFIG_LOCKDEP_SMALL\n \nstatic noinline enum bfs_result\ncheck_redundant(struct held_lock *src, struct held_lock *target)\n{\n\tenum bfs_result ret;\n\tstruct lock_list *target_entry;\n\tstruct lock_list src_entry;\n\n\tbfs_init_root(&src_entry, src);\n\t \n\tsrc_entry.only_xr = src->read == 0;\n\n\tdebug_atomic_inc(nr_redundant_checks);\n\n\t \n\tret = check_path(target, &src_entry, hlock_equal, usage_skip, &target_entry);\n\n\tif (ret == BFS_RMATCH)\n\t\tdebug_atomic_inc(nr_redundant);\n\n\treturn ret;\n}\n\n#else\n\nstatic inline enum bfs_result\ncheck_redundant(struct held_lock *src, struct held_lock *target)\n{\n\treturn BFS_RNOMATCH;\n}\n\n#endif\n\nstatic void inc_chains(int irq_context)\n{\n\tif (irq_context & LOCK_CHAIN_HARDIRQ_CONTEXT)\n\t\tnr_hardirq_chains++;\n\telse if (irq_context & LOCK_CHAIN_SOFTIRQ_CONTEXT)\n\t\tnr_softirq_chains++;\n\telse\n\t\tnr_process_chains++;\n}\n\nstatic void dec_chains(int irq_context)\n{\n\tif (irq_context & LOCK_CHAIN_HARDIRQ_CONTEXT)\n\t\tnr_hardirq_chains--;\n\telse if (irq_context & LOCK_CHAIN_SOFTIRQ_CONTEXT)\n\t\tnr_softirq_chains--;\n\telse\n\t\tnr_process_chains--;\n}\n\nstatic void\nprint_deadlock_scenario(struct held_lock *nxt, struct held_lock *prv)\n{\n\tstruct lock_class *next = hlock_class(nxt);\n\tstruct lock_class *prev = hlock_class(prv);\n\n\tprintk(\" Possible unsafe locking scenario:\\n\\n\");\n\tprintk(\"       CPU0\\n\");\n\tprintk(\"       ----\\n\");\n\tprintk(\"  lock(\");\n\t__print_lock_name(prv, prev);\n\tprintk(KERN_CONT \");\\n\");\n\tprintk(\"  lock(\");\n\t__print_lock_name(nxt, next);\n\tprintk(KERN_CONT \");\\n\");\n\tprintk(\"\\n *** DEADLOCK ***\\n\\n\");\n\tprintk(\" May be due to missing lock nesting notation\\n\\n\");\n}\n\nstatic void\nprint_deadlock_bug(struct task_struct *curr, struct held_lock *prev,\n\t\t   struct held_lock *next)\n{\n\tstruct lock_class *class = hlock_class(prev);\n\n\tif (!debug_locks_off_graph_unlock() || debug_locks_silent)\n\t\treturn;\n\n\tpr_warn(\"\\n\");\n\tpr_warn(\"============================================\\n\");\n\tpr_warn(\"WARNING: possible recursive locking detected\\n\");\n\tprint_kernel_ident();\n\tpr_warn(\"--------------------------------------------\\n\");\n\tpr_warn(\"%s/%d is trying to acquire lock:\\n\",\n\t\tcurr->comm, task_pid_nr(curr));\n\tprint_lock(next);\n\tpr_warn(\"\\nbut task is already holding lock:\\n\");\n\tprint_lock(prev);\n\n\tif (class->cmp_fn) {\n\t\tpr_warn(\"and the lock comparison function returns %i:\\n\",\n\t\t\tclass->cmp_fn(prev->instance, next->instance));\n\t}\n\n\tpr_warn(\"\\nother info that might help us debug this:\\n\");\n\tprint_deadlock_scenario(next, prev);\n\tlockdep_print_held_locks(curr);\n\n\tpr_warn(\"\\nstack backtrace:\\n\");\n\tdump_stack();\n}\n\n \nstatic int\ncheck_deadlock(struct task_struct *curr, struct held_lock *next)\n{\n\tstruct lock_class *class;\n\tstruct held_lock *prev;\n\tstruct held_lock *nest = NULL;\n\tint i;\n\n\tfor (i = 0; i < curr->lockdep_depth; i++) {\n\t\tprev = curr->held_locks + i;\n\n\t\tif (prev->instance == next->nest_lock)\n\t\t\tnest = prev;\n\n\t\tif (hlock_class(prev) != hlock_class(next))\n\t\t\tcontinue;\n\n\t\t \n\t\tif ((next->read == 2) && prev->read)\n\t\t\tcontinue;\n\n\t\tclass = hlock_class(prev);\n\n\t\tif (class->cmp_fn &&\n\t\t    class->cmp_fn(prev->instance, next->instance) < 0)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (nest)\n\t\t\treturn 2;\n\n\t\tprint_deadlock_bug(curr, prev, next);\n\t\treturn 0;\n\t}\n\treturn 1;\n}\n\n \nstatic int\ncheck_prev_add(struct task_struct *curr, struct held_lock *prev,\n\t       struct held_lock *next, u16 distance,\n\t       struct lock_trace **const trace)\n{\n\tstruct lock_list *entry;\n\tenum bfs_result ret;\n\n\tif (!hlock_class(prev)->key || !hlock_class(next)->key) {\n\t\t \n\t\tWARN_ONCE(!debug_locks_silent && !hlock_class(prev)->key,\n\t\t\t  \"Detected use-after-free of lock class %px/%s\\n\",\n\t\t\t  hlock_class(prev),\n\t\t\t  hlock_class(prev)->name);\n\t\tWARN_ONCE(!debug_locks_silent && !hlock_class(next)->key,\n\t\t\t  \"Detected use-after-free of lock class %px/%s\\n\",\n\t\t\t  hlock_class(next),\n\t\t\t  hlock_class(next)->name);\n\t\treturn 2;\n\t}\n\n\tif (prev->class_idx == next->class_idx) {\n\t\tstruct lock_class *class = hlock_class(prev);\n\n\t\tif (class->cmp_fn &&\n\t\t    class->cmp_fn(prev->instance, next->instance) < 0)\n\t\t\treturn 2;\n\t}\n\n\t \n\tret = check_noncircular(next, prev, trace);\n\tif (unlikely(bfs_error(ret) || ret == BFS_RMATCH))\n\t\treturn 0;\n\n\tif (!check_irq_usage(curr, prev, next))\n\t\treturn 0;\n\n\t \n\tlist_for_each_entry(entry, &hlock_class(prev)->locks_after, entry) {\n\t\tif (entry->class == hlock_class(next)) {\n\t\t\tif (distance == 1)\n\t\t\t\tentry->distance = 1;\n\t\t\tentry->dep |= calc_dep(prev, next);\n\n\t\t\t \n\t\t\tlist_for_each_entry(entry, &hlock_class(next)->locks_before, entry) {\n\t\t\t\tif (entry->class == hlock_class(prev)) {\n\t\t\t\t\tif (distance == 1)\n\t\t\t\t\t\tentry->distance = 1;\n\t\t\t\t\tentry->dep |= calc_depb(prev, next);\n\t\t\t\t\treturn 1;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t \n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t \n\tret = check_redundant(prev, next);\n\tif (bfs_error(ret))\n\t\treturn 0;\n\telse if (ret == BFS_RMATCH)\n\t\treturn 2;\n\n\tif (!*trace) {\n\t\t*trace = save_trace();\n\t\tif (!*trace)\n\t\t\treturn 0;\n\t}\n\n\t \n\tret = add_lock_to_list(hlock_class(next), hlock_class(prev),\n\t\t\t       &hlock_class(prev)->locks_after, distance,\n\t\t\t       calc_dep(prev, next), *trace);\n\n\tif (!ret)\n\t\treturn 0;\n\n\tret = add_lock_to_list(hlock_class(prev), hlock_class(next),\n\t\t\t       &hlock_class(next)->locks_before, distance,\n\t\t\t       calc_depb(prev, next), *trace);\n\tif (!ret)\n\t\treturn 0;\n\n\treturn 2;\n}\n\n \nstatic int\ncheck_prevs_add(struct task_struct *curr, struct held_lock *next)\n{\n\tstruct lock_trace *trace = NULL;\n\tint depth = curr->lockdep_depth;\n\tstruct held_lock *hlock;\n\n\t \n\tif (!depth)\n\t\tgoto out_bug;\n\t \n\tif (curr->held_locks[depth].irq_context !=\n\t\t\tcurr->held_locks[depth-1].irq_context)\n\t\tgoto out_bug;\n\n\tfor (;;) {\n\t\tu16 distance = curr->lockdep_depth - depth + 1;\n\t\thlock = curr->held_locks + depth - 1;\n\n\t\tif (hlock->check) {\n\t\t\tint ret = check_prev_add(curr, hlock, next, distance, &trace);\n\t\t\tif (!ret)\n\t\t\t\treturn 0;\n\n\t\t\t \n\t\t\tif (!hlock->trylock)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tdepth--;\n\t\t \n\t\tif (!depth)\n\t\t\tbreak;\n\t\t \n\t\tif (curr->held_locks[depth].irq_context !=\n\t\t\t\tcurr->held_locks[depth-1].irq_context)\n\t\t\tbreak;\n\t}\n\treturn 1;\nout_bug:\n\tif (!debug_locks_off_graph_unlock())\n\t\treturn 0;\n\n\t \n\tWARN_ON(1);\n\n\treturn 0;\n}\n\nstruct lock_chain lock_chains[MAX_LOCKDEP_CHAINS];\nstatic DECLARE_BITMAP(lock_chains_in_use, MAX_LOCKDEP_CHAINS);\nstatic u16 chain_hlocks[MAX_LOCKDEP_CHAIN_HLOCKS];\nunsigned long nr_zapped_lock_chains;\nunsigned int nr_free_chain_hlocks;\t \nunsigned int nr_lost_chain_hlocks;\t \nunsigned int nr_large_chain_blocks;\t \n\n \n#define MAX_CHAIN_BUCKETS\t16\n#define CHAIN_BLK_FLAG\t\t(1U << 15)\n#define CHAIN_BLK_LIST_END\t0xFFFFU\n\nstatic int chain_block_buckets[MAX_CHAIN_BUCKETS];\n\nstatic inline int size_to_bucket(int size)\n{\n\tif (size > MAX_CHAIN_BUCKETS)\n\t\treturn 0;\n\n\treturn size - 1;\n}\n\n \n#define for_each_chain_block(bucket, prev, curr)\t\t\\\n\tfor ((prev) = -1, (curr) = chain_block_buckets[bucket];\t\\\n\t     (curr) >= 0;\t\t\t\t\t\\\n\t     (prev) = (curr), (curr) = chain_block_next(curr))\n\n \nstatic inline int chain_block_next(int offset)\n{\n\tint next = chain_hlocks[offset];\n\n\tWARN_ON_ONCE(!(next & CHAIN_BLK_FLAG));\n\n\tif (next == CHAIN_BLK_LIST_END)\n\t\treturn -1;\n\n\tnext &= ~CHAIN_BLK_FLAG;\n\tnext <<= 16;\n\tnext |= chain_hlocks[offset + 1];\n\n\treturn next;\n}\n\n \nstatic inline int chain_block_size(int offset)\n{\n\treturn (chain_hlocks[offset + 2] << 16) | chain_hlocks[offset + 3];\n}\n\nstatic inline void init_chain_block(int offset, int next, int bucket, int size)\n{\n\tchain_hlocks[offset] = (next >> 16) | CHAIN_BLK_FLAG;\n\tchain_hlocks[offset + 1] = (u16)next;\n\n\tif (size && !bucket) {\n\t\tchain_hlocks[offset + 2] = size >> 16;\n\t\tchain_hlocks[offset + 3] = (u16)size;\n\t}\n}\n\nstatic inline void add_chain_block(int offset, int size)\n{\n\tint bucket = size_to_bucket(size);\n\tint next = chain_block_buckets[bucket];\n\tint prev, curr;\n\n\tif (unlikely(size < 2)) {\n\t\t \n\t\tif (size)\n\t\t\tnr_lost_chain_hlocks++;\n\t\treturn;\n\t}\n\n\tnr_free_chain_hlocks += size;\n\tif (!bucket) {\n\t\tnr_large_chain_blocks++;\n\n\t\t \n\t\tfor_each_chain_block(0, prev, curr) {\n\t\t\tif (size >= chain_block_size(curr))\n\t\t\t\tbreak;\n\t\t}\n\t\tinit_chain_block(offset, curr, 0, size);\n\t\tif (prev < 0)\n\t\t\tchain_block_buckets[0] = offset;\n\t\telse\n\t\t\tinit_chain_block(prev, offset, 0, 0);\n\t\treturn;\n\t}\n\t \n\tinit_chain_block(offset, next, bucket, size);\n\tchain_block_buckets[bucket] = offset;\n}\n\n \nstatic inline void del_chain_block(int bucket, int size, int next)\n{\n\tnr_free_chain_hlocks -= size;\n\tchain_block_buckets[bucket] = next;\n\n\tif (!bucket)\n\t\tnr_large_chain_blocks--;\n}\n\nstatic void init_chain_block_buckets(void)\n{\n\tint i;\n\n\tfor (i = 0; i < MAX_CHAIN_BUCKETS; i++)\n\t\tchain_block_buckets[i] = -1;\n\n\tadd_chain_block(0, ARRAY_SIZE(chain_hlocks));\n}\n\n \nstatic int alloc_chain_hlocks(int req)\n{\n\tint bucket, curr, size;\n\n\t \n\tBUILD_BUG_ON((MAX_LOCKDEP_KEYS-1) & CHAIN_BLK_FLAG);\n\n\tinit_data_structures_once();\n\n\tif (nr_free_chain_hlocks < req)\n\t\treturn -1;\n\n\t \n\treq = max(req, 2);\n\tbucket = size_to_bucket(req);\n\tcurr = chain_block_buckets[bucket];\n\n\tif (bucket) {\n\t\tif (curr >= 0) {\n\t\t\tdel_chain_block(bucket, req, chain_block_next(curr));\n\t\t\treturn curr;\n\t\t}\n\t\t \n\t\tcurr = chain_block_buckets[0];\n\t}\n\n\t \n\tif (curr >= 0) {\n\t\tsize = chain_block_size(curr);\n\t\tif (likely(size >= req)) {\n\t\t\tdel_chain_block(0, size, chain_block_next(curr));\n\t\t\tif (size > req)\n\t\t\t\tadd_chain_block(curr + req, size - req);\n\t\t\treturn curr;\n\t\t}\n\t}\n\n\t \n\tfor (size = MAX_CHAIN_BUCKETS; size > req; size--) {\n\t\tbucket = size_to_bucket(size);\n\t\tcurr = chain_block_buckets[bucket];\n\t\tif (curr < 0)\n\t\t\tcontinue;\n\n\t\tdel_chain_block(bucket, size, chain_block_next(curr));\n\t\tadd_chain_block(curr + req, size - req);\n\t\treturn curr;\n\t}\n\n\treturn -1;\n}\n\nstatic inline void free_chain_hlocks(int base, int size)\n{\n\tadd_chain_block(base, max(size, 2));\n}\n\nstruct lock_class *lock_chain_get_class(struct lock_chain *chain, int i)\n{\n\tu16 chain_hlock = chain_hlocks[chain->base + i];\n\tunsigned int class_idx = chain_hlock_class_idx(chain_hlock);\n\n\treturn lock_classes + class_idx;\n}\n\n \nstatic inline int get_first_held_lock(struct task_struct *curr,\n\t\t\t\t\tstruct held_lock *hlock)\n{\n\tint i;\n\tstruct held_lock *hlock_curr;\n\n\tfor (i = curr->lockdep_depth - 1; i >= 0; i--) {\n\t\thlock_curr = curr->held_locks + i;\n\t\tif (hlock_curr->irq_context != hlock->irq_context)\n\t\t\tbreak;\n\n\t}\n\n\treturn ++i;\n}\n\n#ifdef CONFIG_DEBUG_LOCKDEP\n \nstatic u64 print_chain_key_iteration(u16 hlock_id, u64 chain_key)\n{\n\tu64 new_chain_key = iterate_chain_key(chain_key, hlock_id);\n\n\tprintk(\" hlock_id:%d -> chain_key:%016Lx\",\n\t\t(unsigned int)hlock_id,\n\t\t(unsigned long long)new_chain_key);\n\treturn new_chain_key;\n}\n\nstatic void\nprint_chain_keys_held_locks(struct task_struct *curr, struct held_lock *hlock_next)\n{\n\tstruct held_lock *hlock;\n\tu64 chain_key = INITIAL_CHAIN_KEY;\n\tint depth = curr->lockdep_depth;\n\tint i = get_first_held_lock(curr, hlock_next);\n\n\tprintk(\"depth: %u (irq_context %u)\\n\", depth - i + 1,\n\t\thlock_next->irq_context);\n\tfor (; i < depth; i++) {\n\t\thlock = curr->held_locks + i;\n\t\tchain_key = print_chain_key_iteration(hlock_id(hlock), chain_key);\n\n\t\tprint_lock(hlock);\n\t}\n\n\tprint_chain_key_iteration(hlock_id(hlock_next), chain_key);\n\tprint_lock(hlock_next);\n}\n\nstatic void print_chain_keys_chain(struct lock_chain *chain)\n{\n\tint i;\n\tu64 chain_key = INITIAL_CHAIN_KEY;\n\tu16 hlock_id;\n\n\tprintk(\"depth: %u\\n\", chain->depth);\n\tfor (i = 0; i < chain->depth; i++) {\n\t\thlock_id = chain_hlocks[chain->base + i];\n\t\tchain_key = print_chain_key_iteration(hlock_id, chain_key);\n\n\t\tprint_lock_name(NULL, lock_classes + chain_hlock_class_idx(hlock_id));\n\t\tprintk(\"\\n\");\n\t}\n}\n\nstatic void print_collision(struct task_struct *curr,\n\t\t\tstruct held_lock *hlock_next,\n\t\t\tstruct lock_chain *chain)\n{\n\tpr_warn(\"\\n\");\n\tpr_warn(\"============================\\n\");\n\tpr_warn(\"WARNING: chain_key collision\\n\");\n\tprint_kernel_ident();\n\tpr_warn(\"----------------------------\\n\");\n\tpr_warn(\"%s/%d: \", current->comm, task_pid_nr(current));\n\tpr_warn(\"Hash chain already cached but the contents don't match!\\n\");\n\n\tpr_warn(\"Held locks:\");\n\tprint_chain_keys_held_locks(curr, hlock_next);\n\n\tpr_warn(\"Locks in cached chain:\");\n\tprint_chain_keys_chain(chain);\n\n\tpr_warn(\"\\nstack backtrace:\\n\");\n\tdump_stack();\n}\n#endif\n\n \nstatic int check_no_collision(struct task_struct *curr,\n\t\t\tstruct held_lock *hlock,\n\t\t\tstruct lock_chain *chain)\n{\n#ifdef CONFIG_DEBUG_LOCKDEP\n\tint i, j, id;\n\n\ti = get_first_held_lock(curr, hlock);\n\n\tif (DEBUG_LOCKS_WARN_ON(chain->depth != curr->lockdep_depth - (i - 1))) {\n\t\tprint_collision(curr, hlock, chain);\n\t\treturn 0;\n\t}\n\n\tfor (j = 0; j < chain->depth - 1; j++, i++) {\n\t\tid = hlock_id(&curr->held_locks[i]);\n\n\t\tif (DEBUG_LOCKS_WARN_ON(chain_hlocks[chain->base + j] != id)) {\n\t\t\tprint_collision(curr, hlock, chain);\n\t\t\treturn 0;\n\t\t}\n\t}\n#endif\n\treturn 1;\n}\n\n \nlong lockdep_next_lockchain(long i)\n{\n\ti = find_next_bit(lock_chains_in_use, ARRAY_SIZE(lock_chains), i + 1);\n\treturn i < ARRAY_SIZE(lock_chains) ? i : -2;\n}\n\nunsigned long lock_chain_count(void)\n{\n\treturn bitmap_weight(lock_chains_in_use, ARRAY_SIZE(lock_chains));\n}\n\n \nstatic struct lock_chain *alloc_lock_chain(void)\n{\n\tint idx = find_first_zero_bit(lock_chains_in_use,\n\t\t\t\t      ARRAY_SIZE(lock_chains));\n\n\tif (unlikely(idx >= ARRAY_SIZE(lock_chains)))\n\t\treturn NULL;\n\t__set_bit(idx, lock_chains_in_use);\n\treturn lock_chains + idx;\n}\n\n \nstatic inline int add_chain_cache(struct task_struct *curr,\n\t\t\t\t  struct held_lock *hlock,\n\t\t\t\t  u64 chain_key)\n{\n\tstruct hlist_head *hash_head = chainhashentry(chain_key);\n\tstruct lock_chain *chain;\n\tint i, j;\n\n\t \n\tif (lockdep_assert_locked())\n\t\treturn 0;\n\n\tchain = alloc_lock_chain();\n\tif (!chain) {\n\t\tif (!debug_locks_off_graph_unlock())\n\t\t\treturn 0;\n\n\t\tprint_lockdep_off(\"BUG: MAX_LOCKDEP_CHAINS too low!\");\n\t\tdump_stack();\n\t\treturn 0;\n\t}\n\tchain->chain_key = chain_key;\n\tchain->irq_context = hlock->irq_context;\n\ti = get_first_held_lock(curr, hlock);\n\tchain->depth = curr->lockdep_depth + 1 - i;\n\n\tBUILD_BUG_ON((1UL << 24) <= ARRAY_SIZE(chain_hlocks));\n\tBUILD_BUG_ON((1UL << 6)  <= ARRAY_SIZE(curr->held_locks));\n\tBUILD_BUG_ON((1UL << 8*sizeof(chain_hlocks[0])) <= ARRAY_SIZE(lock_classes));\n\n\tj = alloc_chain_hlocks(chain->depth);\n\tif (j < 0) {\n\t\tif (!debug_locks_off_graph_unlock())\n\t\t\treturn 0;\n\n\t\tprint_lockdep_off(\"BUG: MAX_LOCKDEP_CHAIN_HLOCKS too low!\");\n\t\tdump_stack();\n\t\treturn 0;\n\t}\n\n\tchain->base = j;\n\tfor (j = 0; j < chain->depth - 1; j++, i++) {\n\t\tint lock_id = hlock_id(curr->held_locks + i);\n\n\t\tchain_hlocks[chain->base + j] = lock_id;\n\t}\n\tchain_hlocks[chain->base + j] = hlock_id(hlock);\n\thlist_add_head_rcu(&chain->entry, hash_head);\n\tdebug_atomic_inc(chain_lookup_misses);\n\tinc_chains(chain->irq_context);\n\n\treturn 1;\n}\n\n \nstatic inline struct lock_chain *lookup_chain_cache(u64 chain_key)\n{\n\tstruct hlist_head *hash_head = chainhashentry(chain_key);\n\tstruct lock_chain *chain;\n\n\thlist_for_each_entry_rcu(chain, hash_head, entry) {\n\t\tif (READ_ONCE(chain->chain_key) == chain_key) {\n\t\t\tdebug_atomic_inc(chain_lookup_hits);\n\t\t\treturn chain;\n\t\t}\n\t}\n\treturn NULL;\n}\n\n \nstatic inline int lookup_chain_cache_add(struct task_struct *curr,\n\t\t\t\t\t struct held_lock *hlock,\n\t\t\t\t\t u64 chain_key)\n{\n\tstruct lock_class *class = hlock_class(hlock);\n\tstruct lock_chain *chain = lookup_chain_cache(chain_key);\n\n\tif (chain) {\ncache_hit:\n\t\tif (!check_no_collision(curr, hlock, chain))\n\t\t\treturn 0;\n\n\t\tif (very_verbose(class)) {\n\t\t\tprintk(\"\\nhash chain already cached, key: \"\n\t\t\t\t\t\"%016Lx tail class: [%px] %s\\n\",\n\t\t\t\t\t(unsigned long long)chain_key,\n\t\t\t\t\tclass->key, class->name);\n\t\t}\n\n\t\treturn 0;\n\t}\n\n\tif (very_verbose(class)) {\n\t\tprintk(\"\\nnew hash chain, key: %016Lx tail class: [%px] %s\\n\",\n\t\t\t(unsigned long long)chain_key, class->key, class->name);\n\t}\n\n\tif (!graph_lock())\n\t\treturn 0;\n\n\t \n\tchain = lookup_chain_cache(chain_key);\n\tif (chain) {\n\t\tgraph_unlock();\n\t\tgoto cache_hit;\n\t}\n\n\tif (!add_chain_cache(curr, hlock, chain_key))\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic int validate_chain(struct task_struct *curr,\n\t\t\t  struct held_lock *hlock,\n\t\t\t  int chain_head, u64 chain_key)\n{\n\t \n\tif (!hlock->trylock && hlock->check &&\n\t    lookup_chain_cache_add(curr, hlock, chain_key)) {\n\t\t \n\t\t \n\t\tint ret = check_deadlock(curr, hlock);\n\n\t\tif (!ret)\n\t\t\treturn 0;\n\t\t \n\t\tif (!chain_head && ret != 2) {\n\t\t\tif (!check_prevs_add(curr, hlock))\n\t\t\t\treturn 0;\n\t\t}\n\n\t\tgraph_unlock();\n\t} else {\n\t\t \n\t\tif (unlikely(!debug_locks))\n\t\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n#else\nstatic inline int validate_chain(struct task_struct *curr,\n\t\t\t\t struct held_lock *hlock,\n\t\t\t\t int chain_head, u64 chain_key)\n{\n\treturn 1;\n}\n\nstatic void init_chain_block_buckets(void)\t{ }\n#endif  \n\n \nstatic void check_chain_key(struct task_struct *curr)\n{\n#ifdef CONFIG_DEBUG_LOCKDEP\n\tstruct held_lock *hlock, *prev_hlock = NULL;\n\tunsigned int i;\n\tu64 chain_key = INITIAL_CHAIN_KEY;\n\n\tfor (i = 0; i < curr->lockdep_depth; i++) {\n\t\thlock = curr->held_locks + i;\n\t\tif (chain_key != hlock->prev_chain_key) {\n\t\t\tdebug_locks_off();\n\t\t\t \n\t\t\tWARN(1, \"hm#1, depth: %u [%u], %016Lx != %016Lx\\n\",\n\t\t\t\tcurr->lockdep_depth, i,\n\t\t\t\t(unsigned long long)chain_key,\n\t\t\t\t(unsigned long long)hlock->prev_chain_key);\n\t\t\treturn;\n\t\t}\n\n\t\t \n\t\tif (DEBUG_LOCKS_WARN_ON(!test_bit(hlock->class_idx, lock_classes_in_use)))\n\t\t\treturn;\n\n\t\tif (prev_hlock && (prev_hlock->irq_context !=\n\t\t\t\t\t\t\thlock->irq_context))\n\t\t\tchain_key = INITIAL_CHAIN_KEY;\n\t\tchain_key = iterate_chain_key(chain_key, hlock_id(hlock));\n\t\tprev_hlock = hlock;\n\t}\n\tif (chain_key != curr->curr_chain_key) {\n\t\tdebug_locks_off();\n\t\t \n\t\tWARN(1, \"hm#2, depth: %u [%u], %016Lx != %016Lx\\n\",\n\t\t\tcurr->lockdep_depth, i,\n\t\t\t(unsigned long long)chain_key,\n\t\t\t(unsigned long long)curr->curr_chain_key);\n\t}\n#endif\n}\n\n#ifdef CONFIG_PROVE_LOCKING\nstatic int mark_lock(struct task_struct *curr, struct held_lock *this,\n\t\t     enum lock_usage_bit new_bit);\n\nstatic void print_usage_bug_scenario(struct held_lock *lock)\n{\n\tstruct lock_class *class = hlock_class(lock);\n\n\tprintk(\" Possible unsafe locking scenario:\\n\\n\");\n\tprintk(\"       CPU0\\n\");\n\tprintk(\"       ----\\n\");\n\tprintk(\"  lock(\");\n\t__print_lock_name(lock, class);\n\tprintk(KERN_CONT \");\\n\");\n\tprintk(\"  <Interrupt>\\n\");\n\tprintk(\"    lock(\");\n\t__print_lock_name(lock, class);\n\tprintk(KERN_CONT \");\\n\");\n\tprintk(\"\\n *** DEADLOCK ***\\n\\n\");\n}\n\nstatic void\nprint_usage_bug(struct task_struct *curr, struct held_lock *this,\n\t\tenum lock_usage_bit prev_bit, enum lock_usage_bit new_bit)\n{\n\tif (!debug_locks_off() || debug_locks_silent)\n\t\treturn;\n\n\tpr_warn(\"\\n\");\n\tpr_warn(\"================================\\n\");\n\tpr_warn(\"WARNING: inconsistent lock state\\n\");\n\tprint_kernel_ident();\n\tpr_warn(\"--------------------------------\\n\");\n\n\tpr_warn(\"inconsistent {%s} -> {%s} usage.\\n\",\n\t\tusage_str[prev_bit], usage_str[new_bit]);\n\n\tpr_warn(\"%s/%d [HC%u[%lu]:SC%u[%lu]:HE%u:SE%u] takes:\\n\",\n\t\tcurr->comm, task_pid_nr(curr),\n\t\tlockdep_hardirq_context(), hardirq_count() >> HARDIRQ_SHIFT,\n\t\tlockdep_softirq_context(curr), softirq_count() >> SOFTIRQ_SHIFT,\n\t\tlockdep_hardirqs_enabled(),\n\t\tlockdep_softirqs_enabled(curr));\n\tprint_lock(this);\n\n\tpr_warn(\"{%s} state was registered at:\\n\", usage_str[prev_bit]);\n\tprint_lock_trace(hlock_class(this)->usage_traces[prev_bit], 1);\n\n\tprint_irqtrace_events(curr);\n\tpr_warn(\"\\nother info that might help us debug this:\\n\");\n\tprint_usage_bug_scenario(this);\n\n\tlockdep_print_held_locks(curr);\n\n\tpr_warn(\"\\nstack backtrace:\\n\");\n\tdump_stack();\n}\n\n \nstatic inline int\nvalid_state(struct task_struct *curr, struct held_lock *this,\n\t    enum lock_usage_bit new_bit, enum lock_usage_bit bad_bit)\n{\n\tif (unlikely(hlock_class(this)->usage_mask & (1 << bad_bit))) {\n\t\tgraph_unlock();\n\t\tprint_usage_bug(curr, this, bad_bit, new_bit);\n\t\treturn 0;\n\t}\n\treturn 1;\n}\n\n\n \nstatic void\nprint_irq_inversion_bug(struct task_struct *curr,\n\t\t\tstruct lock_list *root, struct lock_list *other,\n\t\t\tstruct held_lock *this, int forwards,\n\t\t\tconst char *irqclass)\n{\n\tstruct lock_list *entry = other;\n\tstruct lock_list *middle = NULL;\n\tint depth;\n\n\tif (!debug_locks_off_graph_unlock() || debug_locks_silent)\n\t\treturn;\n\n\tpr_warn(\"\\n\");\n\tpr_warn(\"========================================================\\n\");\n\tpr_warn(\"WARNING: possible irq lock inversion dependency detected\\n\");\n\tprint_kernel_ident();\n\tpr_warn(\"--------------------------------------------------------\\n\");\n\tpr_warn(\"%s/%d just changed the state of lock:\\n\",\n\t\tcurr->comm, task_pid_nr(curr));\n\tprint_lock(this);\n\tif (forwards)\n\t\tpr_warn(\"but this lock took another, %s-unsafe lock in the past:\\n\", irqclass);\n\telse\n\t\tpr_warn(\"but this lock was taken by another, %s-safe lock in the past:\\n\", irqclass);\n\tprint_lock_name(NULL, other->class);\n\tpr_warn(\"\\n\\nand interrupts could create inverse lock ordering between them.\\n\\n\");\n\n\tpr_warn(\"\\nother info that might help us debug this:\\n\");\n\n\t \n\tdepth = get_lock_depth(other);\n\tdo {\n\t\tif (depth == 0 && (entry != root)) {\n\t\t\tpr_warn(\"lockdep:%s bad path found in chain graph\\n\", __func__);\n\t\t\tbreak;\n\t\t}\n\t\tmiddle = entry;\n\t\tentry = get_lock_parent(entry);\n\t\tdepth--;\n\t} while (entry && entry != root && (depth >= 0));\n\tif (forwards)\n\t\tprint_irq_lock_scenario(root, other,\n\t\t\tmiddle ? middle->class : root->class, other->class);\n\telse\n\t\tprint_irq_lock_scenario(other, root,\n\t\t\tmiddle ? middle->class : other->class, root->class);\n\n\tlockdep_print_held_locks(curr);\n\n\tpr_warn(\"\\nthe shortest dependencies between 2nd lock and 1st lock:\\n\");\n\troot->trace = save_trace();\n\tif (!root->trace)\n\t\treturn;\n\tprint_shortest_lock_dependencies(other, root);\n\n\tpr_warn(\"\\nstack backtrace:\\n\");\n\tdump_stack();\n}\n\n \nstatic int\ncheck_usage_forwards(struct task_struct *curr, struct held_lock *this,\n\t\t     enum lock_usage_bit bit)\n{\n\tenum bfs_result ret;\n\tstruct lock_list root;\n\tstruct lock_list *target_entry;\n\tenum lock_usage_bit read_bit = bit + LOCK_USAGE_READ_MASK;\n\tunsigned usage_mask = lock_flag(bit) | lock_flag(read_bit);\n\n\tbfs_init_root(&root, this);\n\tret = find_usage_forwards(&root, usage_mask, &target_entry);\n\tif (bfs_error(ret)) {\n\t\tprint_bfs_bug(ret);\n\t\treturn 0;\n\t}\n\tif (ret == BFS_RNOMATCH)\n\t\treturn 1;\n\n\t \n\tif (target_entry->class->usage_mask & lock_flag(bit)) {\n\t\tprint_irq_inversion_bug(curr, &root, target_entry,\n\t\t\t\t\tthis, 1, state_name(bit));\n\t} else {\n\t\tprint_irq_inversion_bug(curr, &root, target_entry,\n\t\t\t\t\tthis, 1, state_name(read_bit));\n\t}\n\n\treturn 0;\n}\n\n \nstatic int\ncheck_usage_backwards(struct task_struct *curr, struct held_lock *this,\n\t\t      enum lock_usage_bit bit)\n{\n\tenum bfs_result ret;\n\tstruct lock_list root;\n\tstruct lock_list *target_entry;\n\tenum lock_usage_bit read_bit = bit + LOCK_USAGE_READ_MASK;\n\tunsigned usage_mask = lock_flag(bit) | lock_flag(read_bit);\n\n\tbfs_init_rootb(&root, this);\n\tret = find_usage_backwards(&root, usage_mask, &target_entry);\n\tif (bfs_error(ret)) {\n\t\tprint_bfs_bug(ret);\n\t\treturn 0;\n\t}\n\tif (ret == BFS_RNOMATCH)\n\t\treturn 1;\n\n\t \n\tif (target_entry->class->usage_mask & lock_flag(bit)) {\n\t\tprint_irq_inversion_bug(curr, &root, target_entry,\n\t\t\t\t\tthis, 0, state_name(bit));\n\t} else {\n\t\tprint_irq_inversion_bug(curr, &root, target_entry,\n\t\t\t\t\tthis, 0, state_name(read_bit));\n\t}\n\n\treturn 0;\n}\n\nvoid print_irqtrace_events(struct task_struct *curr)\n{\n\tconst struct irqtrace_events *trace = &curr->irqtrace;\n\n\tprintk(\"irq event stamp: %u\\n\", trace->irq_events);\n\tprintk(\"hardirqs last  enabled at (%u): [<%px>] %pS\\n\",\n\t\ttrace->hardirq_enable_event, (void *)trace->hardirq_enable_ip,\n\t\t(void *)trace->hardirq_enable_ip);\n\tprintk(\"hardirqs last disabled at (%u): [<%px>] %pS\\n\",\n\t\ttrace->hardirq_disable_event, (void *)trace->hardirq_disable_ip,\n\t\t(void *)trace->hardirq_disable_ip);\n\tprintk(\"softirqs last  enabled at (%u): [<%px>] %pS\\n\",\n\t\ttrace->softirq_enable_event, (void *)trace->softirq_enable_ip,\n\t\t(void *)trace->softirq_enable_ip);\n\tprintk(\"softirqs last disabled at (%u): [<%px>] %pS\\n\",\n\t\ttrace->softirq_disable_event, (void *)trace->softirq_disable_ip,\n\t\t(void *)trace->softirq_disable_ip);\n}\n\nstatic int HARDIRQ_verbose(struct lock_class *class)\n{\n#if HARDIRQ_VERBOSE\n\treturn class_filter(class);\n#endif\n\treturn 0;\n}\n\nstatic int SOFTIRQ_verbose(struct lock_class *class)\n{\n#if SOFTIRQ_VERBOSE\n\treturn class_filter(class);\n#endif\n\treturn 0;\n}\n\nstatic int (*state_verbose_f[])(struct lock_class *class) = {\n#define LOCKDEP_STATE(__STATE) \\\n\t__STATE##_verbose,\n#include \"lockdep_states.h\"\n#undef LOCKDEP_STATE\n};\n\nstatic inline int state_verbose(enum lock_usage_bit bit,\n\t\t\t\tstruct lock_class *class)\n{\n\treturn state_verbose_f[bit >> LOCK_USAGE_DIR_MASK](class);\n}\n\ntypedef int (*check_usage_f)(struct task_struct *, struct held_lock *,\n\t\t\t     enum lock_usage_bit bit, const char *name);\n\nstatic int\nmark_lock_irq(struct task_struct *curr, struct held_lock *this,\n\t\tenum lock_usage_bit new_bit)\n{\n\tint excl_bit = exclusive_bit(new_bit);\n\tint read = new_bit & LOCK_USAGE_READ_MASK;\n\tint dir = new_bit & LOCK_USAGE_DIR_MASK;\n\n\t \n\tif (!valid_state(curr, this, new_bit, excl_bit))\n\t\treturn 0;\n\n\t \n\tif (!read && !valid_state(curr, this, new_bit,\n\t\t\t\t  excl_bit + LOCK_USAGE_READ_MASK))\n\t\treturn 0;\n\n\n\t \n\tif (dir) {\n\t\t \n\t\tif (!check_usage_backwards(curr, this, excl_bit))\n\t\t\treturn 0;\n\t} else {\n\t\t \n\t\tif (!check_usage_forwards(curr, this, excl_bit))\n\t\t\treturn 0;\n\t}\n\n\tif (state_verbose(new_bit, hlock_class(this)))\n\t\treturn 2;\n\n\treturn 1;\n}\n\n \nstatic int\nmark_held_locks(struct task_struct *curr, enum lock_usage_bit base_bit)\n{\n\tstruct held_lock *hlock;\n\tint i;\n\n\tfor (i = 0; i < curr->lockdep_depth; i++) {\n\t\tenum lock_usage_bit hlock_bit = base_bit;\n\t\thlock = curr->held_locks + i;\n\n\t\tif (hlock->read)\n\t\t\thlock_bit += LOCK_USAGE_READ_MASK;\n\n\t\tBUG_ON(hlock_bit >= LOCK_USAGE_STATES);\n\n\t\tif (!hlock->check)\n\t\t\tcontinue;\n\n\t\tif (!mark_lock(curr, hlock, hlock_bit))\n\t\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\n \nstatic void __trace_hardirqs_on_caller(void)\n{\n\tstruct task_struct *curr = current;\n\n\t \n\tif (!mark_held_locks(curr, LOCK_ENABLED_HARDIRQ))\n\t\treturn;\n\t \n\tif (curr->softirqs_enabled)\n\t\tmark_held_locks(curr, LOCK_ENABLED_SOFTIRQ);\n}\n\n \nvoid lockdep_hardirqs_on_prepare(void)\n{\n\tif (unlikely(!debug_locks))\n\t\treturn;\n\n\t \n\tif (unlikely(in_nmi()))\n\t\treturn;\n\n\tif (unlikely(this_cpu_read(lockdep_recursion)))\n\t\treturn;\n\n\tif (unlikely(lockdep_hardirqs_enabled())) {\n\t\t \n\t\t__debug_atomic_inc(redundant_hardirqs_on);\n\t\treturn;\n\t}\n\n\t \n\tif (DEBUG_LOCKS_WARN_ON(!irqs_disabled()))\n\t\treturn;\n\n\t \n\tif (DEBUG_LOCKS_WARN_ON(early_boot_irqs_disabled))\n\t\treturn;\n\n\t \n\tif (DEBUG_LOCKS_WARN_ON(lockdep_hardirq_context()))\n\t\treturn;\n\n\tcurrent->hardirq_chain_key = current->curr_chain_key;\n\n\tlockdep_recursion_inc();\n\t__trace_hardirqs_on_caller();\n\tlockdep_recursion_finish();\n}\nEXPORT_SYMBOL_GPL(lockdep_hardirqs_on_prepare);\n\nvoid noinstr lockdep_hardirqs_on(unsigned long ip)\n{\n\tstruct irqtrace_events *trace = &current->irqtrace;\n\n\tif (unlikely(!debug_locks))\n\t\treturn;\n\n\t \n\tif (unlikely(in_nmi())) {\n\t\tif (!IS_ENABLED(CONFIG_TRACE_IRQFLAGS_NMI))\n\t\t\treturn;\n\n\t\t \n\t\tgoto skip_checks;\n\t}\n\n\tif (unlikely(this_cpu_read(lockdep_recursion)))\n\t\treturn;\n\n\tif (lockdep_hardirqs_enabled()) {\n\t\t \n\t\t__debug_atomic_inc(redundant_hardirqs_on);\n\t\treturn;\n\t}\n\n\t \n\tif (DEBUG_LOCKS_WARN_ON(!irqs_disabled()))\n\t\treturn;\n\n\t \n\tDEBUG_LOCKS_WARN_ON(current->hardirq_chain_key !=\n\t\t\t    current->curr_chain_key);\n\nskip_checks:\n\t \n\t__this_cpu_write(hardirqs_enabled, 1);\n\ttrace->hardirq_enable_ip = ip;\n\ttrace->hardirq_enable_event = ++trace->irq_events;\n\tdebug_atomic_inc(hardirqs_on_events);\n}\nEXPORT_SYMBOL_GPL(lockdep_hardirqs_on);\n\n \nvoid noinstr lockdep_hardirqs_off(unsigned long ip)\n{\n\tif (unlikely(!debug_locks))\n\t\treturn;\n\n\t \n\tif (in_nmi()) {\n\t\tif (!IS_ENABLED(CONFIG_TRACE_IRQFLAGS_NMI))\n\t\t\treturn;\n\t} else if (__this_cpu_read(lockdep_recursion))\n\t\treturn;\n\n\t \n\tif (DEBUG_LOCKS_WARN_ON(!irqs_disabled()))\n\t\treturn;\n\n\tif (lockdep_hardirqs_enabled()) {\n\t\tstruct irqtrace_events *trace = &current->irqtrace;\n\n\t\t \n\t\t__this_cpu_write(hardirqs_enabled, 0);\n\t\ttrace->hardirq_disable_ip = ip;\n\t\ttrace->hardirq_disable_event = ++trace->irq_events;\n\t\tdebug_atomic_inc(hardirqs_off_events);\n\t} else {\n\t\tdebug_atomic_inc(redundant_hardirqs_off);\n\t}\n}\nEXPORT_SYMBOL_GPL(lockdep_hardirqs_off);\n\n \nvoid lockdep_softirqs_on(unsigned long ip)\n{\n\tstruct irqtrace_events *trace = &current->irqtrace;\n\n\tif (unlikely(!lockdep_enabled()))\n\t\treturn;\n\n\t \n\tif (DEBUG_LOCKS_WARN_ON(!irqs_disabled()))\n\t\treturn;\n\n\tif (current->softirqs_enabled) {\n\t\tdebug_atomic_inc(redundant_softirqs_on);\n\t\treturn;\n\t}\n\n\tlockdep_recursion_inc();\n\t \n\tcurrent->softirqs_enabled = 1;\n\ttrace->softirq_enable_ip = ip;\n\ttrace->softirq_enable_event = ++trace->irq_events;\n\tdebug_atomic_inc(softirqs_on_events);\n\t \n\tif (lockdep_hardirqs_enabled())\n\t\tmark_held_locks(current, LOCK_ENABLED_SOFTIRQ);\n\tlockdep_recursion_finish();\n}\n\n \nvoid lockdep_softirqs_off(unsigned long ip)\n{\n\tif (unlikely(!lockdep_enabled()))\n\t\treturn;\n\n\t \n\tif (DEBUG_LOCKS_WARN_ON(!irqs_disabled()))\n\t\treturn;\n\n\tif (current->softirqs_enabled) {\n\t\tstruct irqtrace_events *trace = &current->irqtrace;\n\n\t\t \n\t\tcurrent->softirqs_enabled = 0;\n\t\ttrace->softirq_disable_ip = ip;\n\t\ttrace->softirq_disable_event = ++trace->irq_events;\n\t\tdebug_atomic_inc(softirqs_off_events);\n\t\t \n\t\tDEBUG_LOCKS_WARN_ON(!softirq_count());\n\t} else\n\t\tdebug_atomic_inc(redundant_softirqs_off);\n}\n\nstatic int\nmark_usage(struct task_struct *curr, struct held_lock *hlock, int check)\n{\n\tif (!check)\n\t\tgoto lock_used;\n\n\t \n\tif (!hlock->trylock) {\n\t\tif (hlock->read) {\n\t\t\tif (lockdep_hardirq_context())\n\t\t\t\tif (!mark_lock(curr, hlock,\n\t\t\t\t\t\tLOCK_USED_IN_HARDIRQ_READ))\n\t\t\t\t\treturn 0;\n\t\t\tif (curr->softirq_context)\n\t\t\t\tif (!mark_lock(curr, hlock,\n\t\t\t\t\t\tLOCK_USED_IN_SOFTIRQ_READ))\n\t\t\t\t\treturn 0;\n\t\t} else {\n\t\t\tif (lockdep_hardirq_context())\n\t\t\t\tif (!mark_lock(curr, hlock, LOCK_USED_IN_HARDIRQ))\n\t\t\t\t\treturn 0;\n\t\t\tif (curr->softirq_context)\n\t\t\t\tif (!mark_lock(curr, hlock, LOCK_USED_IN_SOFTIRQ))\n\t\t\t\t\treturn 0;\n\t\t}\n\t}\n\n\t \n\tif (!hlock->hardirqs_off && !hlock->sync) {\n\t\tif (hlock->read) {\n\t\t\tif (!mark_lock(curr, hlock,\n\t\t\t\t\tLOCK_ENABLED_HARDIRQ_READ))\n\t\t\t\treturn 0;\n\t\t\tif (curr->softirqs_enabled)\n\t\t\t\tif (!mark_lock(curr, hlock,\n\t\t\t\t\t\tLOCK_ENABLED_SOFTIRQ_READ))\n\t\t\t\t\treturn 0;\n\t\t} else {\n\t\t\tif (!mark_lock(curr, hlock,\n\t\t\t\t\tLOCK_ENABLED_HARDIRQ))\n\t\t\t\treturn 0;\n\t\t\tif (curr->softirqs_enabled)\n\t\t\t\tif (!mark_lock(curr, hlock,\n\t\t\t\t\t\tLOCK_ENABLED_SOFTIRQ))\n\t\t\t\t\treturn 0;\n\t\t}\n\t}\n\nlock_used:\n\t \n\tif (!mark_lock(curr, hlock, LOCK_USED))\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic inline unsigned int task_irq_context(struct task_struct *task)\n{\n\treturn LOCK_CHAIN_HARDIRQ_CONTEXT * !!lockdep_hardirq_context() +\n\t       LOCK_CHAIN_SOFTIRQ_CONTEXT * !!task->softirq_context;\n}\n\nstatic int separate_irq_context(struct task_struct *curr,\n\t\tstruct held_lock *hlock)\n{\n\tunsigned int depth = curr->lockdep_depth;\n\n\t \n\tif (depth) {\n\t\tstruct held_lock *prev_hlock;\n\n\t\tprev_hlock = curr->held_locks + depth-1;\n\t\t \n\t\tif (prev_hlock->irq_context != hlock->irq_context)\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}\n\n \nstatic int mark_lock(struct task_struct *curr, struct held_lock *this,\n\t\t\t     enum lock_usage_bit new_bit)\n{\n\tunsigned int new_mask, ret = 1;\n\n\tif (new_bit >= LOCK_USAGE_STATES) {\n\t\tDEBUG_LOCKS_WARN_ON(1);\n\t\treturn 0;\n\t}\n\n\tif (new_bit == LOCK_USED && this->read)\n\t\tnew_bit = LOCK_USED_READ;\n\n\tnew_mask = 1 << new_bit;\n\n\t \n\tif (likely(hlock_class(this)->usage_mask & new_mask))\n\t\treturn 1;\n\n\tif (!graph_lock())\n\t\treturn 0;\n\t \n\tif (unlikely(hlock_class(this)->usage_mask & new_mask))\n\t\tgoto unlock;\n\n\tif (!hlock_class(this)->usage_mask)\n\t\tdebug_atomic_dec(nr_unused_locks);\n\n\thlock_class(this)->usage_mask |= new_mask;\n\n\tif (new_bit < LOCK_TRACE_STATES) {\n\t\tif (!(hlock_class(this)->usage_traces[new_bit] = save_trace()))\n\t\t\treturn 0;\n\t}\n\n\tif (new_bit < LOCK_USED) {\n\t\tret = mark_lock_irq(curr, this, new_bit);\n\t\tif (!ret)\n\t\t\treturn 0;\n\t}\n\nunlock:\n\tgraph_unlock();\n\n\t \n\tif (ret == 2) {\n\t\tprintk(\"\\nmarked lock as {%s}:\\n\", usage_str[new_bit]);\n\t\tprint_lock(this);\n\t\tprint_irqtrace_events(curr);\n\t\tdump_stack();\n\t}\n\n\treturn ret;\n}\n\nstatic inline short task_wait_context(struct task_struct *curr)\n{\n\t \n\tif (lockdep_hardirq_context()) {\n\t\t \n\t\tif (curr->hardirq_threaded || curr->irq_config)\n\t\t\treturn LD_WAIT_CONFIG;\n\n\t\treturn LD_WAIT_SPIN;\n\t} else if (curr->softirq_context) {\n\t\t \n\t\treturn LD_WAIT_CONFIG;\n\t}\n\n\treturn LD_WAIT_MAX;\n}\n\nstatic int\nprint_lock_invalid_wait_context(struct task_struct *curr,\n\t\t\t\tstruct held_lock *hlock)\n{\n\tshort curr_inner;\n\n\tif (!debug_locks_off())\n\t\treturn 0;\n\tif (debug_locks_silent)\n\t\treturn 0;\n\n\tpr_warn(\"\\n\");\n\tpr_warn(\"=============================\\n\");\n\tpr_warn(\"[ BUG: Invalid wait context ]\\n\");\n\tprint_kernel_ident();\n\tpr_warn(\"-----------------------------\\n\");\n\n\tpr_warn(\"%s/%d is trying to lock:\\n\", curr->comm, task_pid_nr(curr));\n\tprint_lock(hlock);\n\n\tpr_warn(\"other info that might help us debug this:\\n\");\n\n\tcurr_inner = task_wait_context(curr);\n\tpr_warn(\"context-{%d:%d}\\n\", curr_inner, curr_inner);\n\n\tlockdep_print_held_locks(curr);\n\n\tpr_warn(\"stack backtrace:\\n\");\n\tdump_stack();\n\n\treturn 0;\n}\n\n \nstatic int check_wait_context(struct task_struct *curr, struct held_lock *next)\n{\n\tu8 next_inner = hlock_class(next)->wait_type_inner;\n\tu8 next_outer = hlock_class(next)->wait_type_outer;\n\tu8 curr_inner;\n\tint depth;\n\n\tif (!next_inner || next->trylock)\n\t\treturn 0;\n\n\tif (!next_outer)\n\t\tnext_outer = next_inner;\n\n\t \n\tfor (depth = curr->lockdep_depth - 1; depth >= 0; depth--) {\n\t\tstruct held_lock *prev = curr->held_locks + depth;\n\t\tif (prev->irq_context != next->irq_context)\n\t\t\tbreak;\n\t}\n\tdepth++;\n\n\tcurr_inner = task_wait_context(curr);\n\n\tfor (; depth < curr->lockdep_depth; depth++) {\n\t\tstruct held_lock *prev = curr->held_locks + depth;\n\t\tstruct lock_class *class = hlock_class(prev);\n\t\tu8 prev_inner = class->wait_type_inner;\n\n\t\tif (prev_inner) {\n\t\t\t \n\t\t\tcurr_inner = min(curr_inner, prev_inner);\n\n\t\t\t \n\t\t\tif (unlikely(class->lock_type == LD_LOCK_WAIT_OVERRIDE))\n\t\t\t\tcurr_inner = prev_inner;\n\t\t}\n\t}\n\n\tif (next_outer > curr_inner)\n\t\treturn print_lock_invalid_wait_context(curr, next);\n\n\treturn 0;\n}\n\n#else  \n\nstatic inline int\nmark_usage(struct task_struct *curr, struct held_lock *hlock, int check)\n{\n\treturn 1;\n}\n\nstatic inline unsigned int task_irq_context(struct task_struct *task)\n{\n\treturn 0;\n}\n\nstatic inline int separate_irq_context(struct task_struct *curr,\n\t\tstruct held_lock *hlock)\n{\n\treturn 0;\n}\n\nstatic inline int check_wait_context(struct task_struct *curr,\n\t\t\t\t     struct held_lock *next)\n{\n\treturn 0;\n}\n\n#endif  \n\n \nvoid lockdep_init_map_type(struct lockdep_map *lock, const char *name,\n\t\t\t    struct lock_class_key *key, int subclass,\n\t\t\t    u8 inner, u8 outer, u8 lock_type)\n{\n\tint i;\n\n\tfor (i = 0; i < NR_LOCKDEP_CACHING_CLASSES; i++)\n\t\tlock->class_cache[i] = NULL;\n\n#ifdef CONFIG_LOCK_STAT\n\tlock->cpu = raw_smp_processor_id();\n#endif\n\n\t \n\tif (DEBUG_LOCKS_WARN_ON(!name)) {\n\t\tlock->name = \"NULL\";\n\t\treturn;\n\t}\n\n\tlock->name = name;\n\n\tlock->wait_type_outer = outer;\n\tlock->wait_type_inner = inner;\n\tlock->lock_type = lock_type;\n\n\t \n\tif (DEBUG_LOCKS_WARN_ON(!key))\n\t\treturn;\n\t \n\tif (!static_obj(key) && !is_dynamic_key(key)) {\n\t\tif (debug_locks)\n\t\t\tprintk(KERN_ERR \"BUG: key %px has not been registered!\\n\", key);\n\t\tDEBUG_LOCKS_WARN_ON(1);\n\t\treturn;\n\t}\n\tlock->key = key;\n\n\tif (unlikely(!debug_locks))\n\t\treturn;\n\n\tif (subclass) {\n\t\tunsigned long flags;\n\n\t\tif (DEBUG_LOCKS_WARN_ON(!lockdep_enabled()))\n\t\t\treturn;\n\n\t\traw_local_irq_save(flags);\n\t\tlockdep_recursion_inc();\n\t\tregister_lock_class(lock, subclass, 1);\n\t\tlockdep_recursion_finish();\n\t\traw_local_irq_restore(flags);\n\t}\n}\nEXPORT_SYMBOL_GPL(lockdep_init_map_type);\n\nstruct lock_class_key __lockdep_no_validate__;\nEXPORT_SYMBOL_GPL(__lockdep_no_validate__);\n\n#ifdef CONFIG_PROVE_LOCKING\nvoid lockdep_set_lock_cmp_fn(struct lockdep_map *lock, lock_cmp_fn cmp_fn,\n\t\t\t     lock_print_fn print_fn)\n{\n\tstruct lock_class *class = lock->class_cache[0];\n\tunsigned long flags;\n\n\traw_local_irq_save(flags);\n\tlockdep_recursion_inc();\n\n\tif (!class)\n\t\tclass = register_lock_class(lock, 0, 0);\n\n\tif (class) {\n\t\tWARN_ON(class->cmp_fn\t&& class->cmp_fn != cmp_fn);\n\t\tWARN_ON(class->print_fn && class->print_fn != print_fn);\n\n\t\tclass->cmp_fn\t= cmp_fn;\n\t\tclass->print_fn = print_fn;\n\t}\n\n\tlockdep_recursion_finish();\n\traw_local_irq_restore(flags);\n}\nEXPORT_SYMBOL_GPL(lockdep_set_lock_cmp_fn);\n#endif\n\nstatic void\nprint_lock_nested_lock_not_held(struct task_struct *curr,\n\t\t\t\tstruct held_lock *hlock)\n{\n\tif (!debug_locks_off())\n\t\treturn;\n\tif (debug_locks_silent)\n\t\treturn;\n\n\tpr_warn(\"\\n\");\n\tpr_warn(\"==================================\\n\");\n\tpr_warn(\"WARNING: Nested lock was not taken\\n\");\n\tprint_kernel_ident();\n\tpr_warn(\"----------------------------------\\n\");\n\n\tpr_warn(\"%s/%d is trying to lock:\\n\", curr->comm, task_pid_nr(curr));\n\tprint_lock(hlock);\n\n\tpr_warn(\"\\nbut this task is not holding:\\n\");\n\tpr_warn(\"%s\\n\", hlock->nest_lock->name);\n\n\tpr_warn(\"\\nstack backtrace:\\n\");\n\tdump_stack();\n\n\tpr_warn(\"\\nother info that might help us debug this:\\n\");\n\tlockdep_print_held_locks(curr);\n\n\tpr_warn(\"\\nstack backtrace:\\n\");\n\tdump_stack();\n}\n\nstatic int __lock_is_held(const struct lockdep_map *lock, int read);\n\n \nstatic int __lock_acquire(struct lockdep_map *lock, unsigned int subclass,\n\t\t\t  int trylock, int read, int check, int hardirqs_off,\n\t\t\t  struct lockdep_map *nest_lock, unsigned long ip,\n\t\t\t  int references, int pin_count, int sync)\n{\n\tstruct task_struct *curr = current;\n\tstruct lock_class *class = NULL;\n\tstruct held_lock *hlock;\n\tunsigned int depth;\n\tint chain_head = 0;\n\tint class_idx;\n\tu64 chain_key;\n\n\tif (unlikely(!debug_locks))\n\t\treturn 0;\n\n\tif (!prove_locking || lock->key == &__lockdep_no_validate__)\n\t\tcheck = 0;\n\n\tif (subclass < NR_LOCKDEP_CACHING_CLASSES)\n\t\tclass = lock->class_cache[subclass];\n\t \n\tif (unlikely(!class)) {\n\t\tclass = register_lock_class(lock, subclass, 0);\n\t\tif (!class)\n\t\t\treturn 0;\n\t}\n\n\tdebug_class_ops_inc(class);\n\n\tif (very_verbose(class)) {\n\t\tprintk(\"\\nacquire class [%px] %s\", class->key, class->name);\n\t\tif (class->name_version > 1)\n\t\t\tprintk(KERN_CONT \"#%d\", class->name_version);\n\t\tprintk(KERN_CONT \"\\n\");\n\t\tdump_stack();\n\t}\n\n\t \n\tdepth = curr->lockdep_depth;\n\t \n\tif (DEBUG_LOCKS_WARN_ON(depth >= MAX_LOCK_DEPTH))\n\t\treturn 0;\n\n\tclass_idx = class - lock_classes;\n\n\tif (depth && !sync) {\n\t\t \n\t\thlock = curr->held_locks + depth - 1;\n\t\tif (hlock->class_idx == class_idx && nest_lock) {\n\t\t\tif (!references)\n\t\t\t\treferences++;\n\n\t\t\tif (!hlock->references)\n\t\t\t\thlock->references++;\n\n\t\t\thlock->references += references;\n\n\t\t\t \n\t\t\tif (DEBUG_LOCKS_WARN_ON(hlock->references < references))\n\t\t\t\treturn 0;\n\n\t\t\treturn 2;\n\t\t}\n\t}\n\n\thlock = curr->held_locks + depth;\n\t \n\tif (DEBUG_LOCKS_WARN_ON(!class))\n\t\treturn 0;\n\thlock->class_idx = class_idx;\n\thlock->acquire_ip = ip;\n\thlock->instance = lock;\n\thlock->nest_lock = nest_lock;\n\thlock->irq_context = task_irq_context(curr);\n\thlock->trylock = trylock;\n\thlock->read = read;\n\thlock->check = check;\n\thlock->sync = !!sync;\n\thlock->hardirqs_off = !!hardirqs_off;\n\thlock->references = references;\n#ifdef CONFIG_LOCK_STAT\n\thlock->waittime_stamp = 0;\n\thlock->holdtime_stamp = lockstat_clock();\n#endif\n\thlock->pin_count = pin_count;\n\n\tif (check_wait_context(curr, hlock))\n\t\treturn 0;\n\n\t \n\tif (!mark_usage(curr, hlock, check))\n\t\treturn 0;\n\n\t \n\t \n\tif (DEBUG_LOCKS_WARN_ON(!test_bit(class_idx, lock_classes_in_use)))\n\t\treturn 0;\n\n\tchain_key = curr->curr_chain_key;\n\tif (!depth) {\n\t\t \n\t\tif (DEBUG_LOCKS_WARN_ON(chain_key != INITIAL_CHAIN_KEY))\n\t\t\treturn 0;\n\t\tchain_head = 1;\n\t}\n\n\thlock->prev_chain_key = chain_key;\n\tif (separate_irq_context(curr, hlock)) {\n\t\tchain_key = INITIAL_CHAIN_KEY;\n\t\tchain_head = 1;\n\t}\n\tchain_key = iterate_chain_key(chain_key, hlock_id(hlock));\n\n\tif (nest_lock && !__lock_is_held(nest_lock, -1)) {\n\t\tprint_lock_nested_lock_not_held(curr, hlock);\n\t\treturn 0;\n\t}\n\n\tif (!debug_locks_silent) {\n\t\tWARN_ON_ONCE(depth && !hlock_class(hlock - 1)->key);\n\t\tWARN_ON_ONCE(!hlock_class(hlock)->key);\n\t}\n\n\tif (!validate_chain(curr, hlock, chain_head, chain_key))\n\t\treturn 0;\n\n\t \n\tif (hlock->sync)\n\t\treturn 1;\n\n\tcurr->curr_chain_key = chain_key;\n\tcurr->lockdep_depth++;\n\tcheck_chain_key(curr);\n#ifdef CONFIG_DEBUG_LOCKDEP\n\tif (unlikely(!debug_locks))\n\t\treturn 0;\n#endif\n\tif (unlikely(curr->lockdep_depth >= MAX_LOCK_DEPTH)) {\n\t\tdebug_locks_off();\n\t\tprint_lockdep_off(\"BUG: MAX_LOCK_DEPTH too low!\");\n\t\tprintk(KERN_DEBUG \"depth: %i  max: %lu!\\n\",\n\t\t       curr->lockdep_depth, MAX_LOCK_DEPTH);\n\n\t\tlockdep_print_held_locks(current);\n\t\tdebug_show_all_locks();\n\t\tdump_stack();\n\n\t\treturn 0;\n\t}\n\n\tif (unlikely(curr->lockdep_depth > max_lockdep_depth))\n\t\tmax_lockdep_depth = curr->lockdep_depth;\n\n\treturn 1;\n}\n\nstatic void print_unlock_imbalance_bug(struct task_struct *curr,\n\t\t\t\t       struct lockdep_map *lock,\n\t\t\t\t       unsigned long ip)\n{\n\tif (!debug_locks_off())\n\t\treturn;\n\tif (debug_locks_silent)\n\t\treturn;\n\n\tpr_warn(\"\\n\");\n\tpr_warn(\"=====================================\\n\");\n\tpr_warn(\"WARNING: bad unlock balance detected!\\n\");\n\tprint_kernel_ident();\n\tpr_warn(\"-------------------------------------\\n\");\n\tpr_warn(\"%s/%d is trying to release lock (\",\n\t\tcurr->comm, task_pid_nr(curr));\n\tprint_lockdep_cache(lock);\n\tpr_cont(\") at:\\n\");\n\tprint_ip_sym(KERN_WARNING, ip);\n\tpr_warn(\"but there are no more locks to release!\\n\");\n\tpr_warn(\"\\nother info that might help us debug this:\\n\");\n\tlockdep_print_held_locks(curr);\n\n\tpr_warn(\"\\nstack backtrace:\\n\");\n\tdump_stack();\n}\n\nstatic noinstr int match_held_lock(const struct held_lock *hlock,\n\t\t\t\t   const struct lockdep_map *lock)\n{\n\tif (hlock->instance == lock)\n\t\treturn 1;\n\n\tif (hlock->references) {\n\t\tconst struct lock_class *class = lock->class_cache[0];\n\n\t\tif (!class)\n\t\t\tclass = look_up_lock_class(lock, 0);\n\n\t\t \n\t\tif (!class)\n\t\t\treturn 0;\n\n\t\t \n\t\tif (DEBUG_LOCKS_WARN_ON(!hlock->nest_lock))\n\t\t\treturn 0;\n\n\t\tif (hlock->class_idx == class - lock_classes)\n\t\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\n \nstatic struct held_lock *find_held_lock(struct task_struct *curr,\n\t\t\t\t\tstruct lockdep_map *lock,\n\t\t\t\t\tunsigned int depth, int *idx)\n{\n\tstruct held_lock *ret, *hlock, *prev_hlock;\n\tint i;\n\n\ti = depth - 1;\n\thlock = curr->held_locks + i;\n\tret = hlock;\n\tif (match_held_lock(hlock, lock))\n\t\tgoto out;\n\n\tret = NULL;\n\tfor (i--, prev_hlock = hlock--;\n\t     i >= 0;\n\t     i--, prev_hlock = hlock--) {\n\t\t \n\t\tif (prev_hlock->irq_context != hlock->irq_context) {\n\t\t\tret = NULL;\n\t\t\tbreak;\n\t\t}\n\t\tif (match_held_lock(hlock, lock)) {\n\t\t\tret = hlock;\n\t\t\tbreak;\n\t\t}\n\t}\n\nout:\n\t*idx = i;\n\treturn ret;\n}\n\nstatic int reacquire_held_locks(struct task_struct *curr, unsigned int depth,\n\t\t\t\tint idx, unsigned int *merged)\n{\n\tstruct held_lock *hlock;\n\tint first_idx = idx;\n\n\tif (DEBUG_LOCKS_WARN_ON(!irqs_disabled()))\n\t\treturn 0;\n\n\tfor (hlock = curr->held_locks + idx; idx < depth; idx++, hlock++) {\n\t\tswitch (__lock_acquire(hlock->instance,\n\t\t\t\t    hlock_class(hlock)->subclass,\n\t\t\t\t    hlock->trylock,\n\t\t\t\t    hlock->read, hlock->check,\n\t\t\t\t    hlock->hardirqs_off,\n\t\t\t\t    hlock->nest_lock, hlock->acquire_ip,\n\t\t\t\t    hlock->references, hlock->pin_count, 0)) {\n\t\tcase 0:\n\t\t\treturn 1;\n\t\tcase 1:\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\t*merged += (idx == first_idx);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tWARN_ON(1);\n\t\t\treturn 0;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int\n__lock_set_class(struct lockdep_map *lock, const char *name,\n\t\t struct lock_class_key *key, unsigned int subclass,\n\t\t unsigned long ip)\n{\n\tstruct task_struct *curr = current;\n\tunsigned int depth, merged = 0;\n\tstruct held_lock *hlock;\n\tstruct lock_class *class;\n\tint i;\n\n\tif (unlikely(!debug_locks))\n\t\treturn 0;\n\n\tdepth = curr->lockdep_depth;\n\t \n\tif (DEBUG_LOCKS_WARN_ON(!depth))\n\t\treturn 0;\n\n\thlock = find_held_lock(curr, lock, depth, &i);\n\tif (!hlock) {\n\t\tprint_unlock_imbalance_bug(curr, lock, ip);\n\t\treturn 0;\n\t}\n\n\tlockdep_init_map_type(lock, name, key, 0,\n\t\t\t      lock->wait_type_inner,\n\t\t\t      lock->wait_type_outer,\n\t\t\t      lock->lock_type);\n\tclass = register_lock_class(lock, subclass, 0);\n\thlock->class_idx = class - lock_classes;\n\n\tcurr->lockdep_depth = i;\n\tcurr->curr_chain_key = hlock->prev_chain_key;\n\n\tif (reacquire_held_locks(curr, depth, i, &merged))\n\t\treturn 0;\n\n\t \n\tif (DEBUG_LOCKS_WARN_ON(curr->lockdep_depth != depth - merged))\n\t\treturn 0;\n\treturn 1;\n}\n\nstatic int __lock_downgrade(struct lockdep_map *lock, unsigned long ip)\n{\n\tstruct task_struct *curr = current;\n\tunsigned int depth, merged = 0;\n\tstruct held_lock *hlock;\n\tint i;\n\n\tif (unlikely(!debug_locks))\n\t\treturn 0;\n\n\tdepth = curr->lockdep_depth;\n\t \n\tif (DEBUG_LOCKS_WARN_ON(!depth))\n\t\treturn 0;\n\n\thlock = find_held_lock(curr, lock, depth, &i);\n\tif (!hlock) {\n\t\tprint_unlock_imbalance_bug(curr, lock, ip);\n\t\treturn 0;\n\t}\n\n\tcurr->lockdep_depth = i;\n\tcurr->curr_chain_key = hlock->prev_chain_key;\n\n\tWARN(hlock->read, \"downgrading a read lock\");\n\thlock->read = 1;\n\thlock->acquire_ip = ip;\n\n\tif (reacquire_held_locks(curr, depth, i, &merged))\n\t\treturn 0;\n\n\t \n\tif (DEBUG_LOCKS_WARN_ON(merged))\n\t\treturn 0;\n\n\t \n\tif (DEBUG_LOCKS_WARN_ON(curr->lockdep_depth != depth))\n\t\treturn 0;\n\n\treturn 1;\n}\n\n \nstatic int\n__lock_release(struct lockdep_map *lock, unsigned long ip)\n{\n\tstruct task_struct *curr = current;\n\tunsigned int depth, merged = 1;\n\tstruct held_lock *hlock;\n\tint i;\n\n\tif (unlikely(!debug_locks))\n\t\treturn 0;\n\n\tdepth = curr->lockdep_depth;\n\t \n\tif (depth <= 0) {\n\t\tprint_unlock_imbalance_bug(curr, lock, ip);\n\t\treturn 0;\n\t}\n\n\t \n\thlock = find_held_lock(curr, lock, depth, &i);\n\tif (!hlock) {\n\t\tprint_unlock_imbalance_bug(curr, lock, ip);\n\t\treturn 0;\n\t}\n\n\tif (hlock->instance == lock)\n\t\tlock_release_holdtime(hlock);\n\n\tWARN(hlock->pin_count, \"releasing a pinned lock\\n\");\n\n\tif (hlock->references) {\n\t\thlock->references--;\n\t\tif (hlock->references) {\n\t\t\t \n\t\t\treturn 1;\n\t\t}\n\t}\n\n\t \n\n\tcurr->lockdep_depth = i;\n\tcurr->curr_chain_key = hlock->prev_chain_key;\n\n\t \n\tif (i == depth-1)\n\t\treturn 1;\n\n\tif (reacquire_held_locks(curr, depth, i + 1, &merged))\n\t\treturn 0;\n\n\t \n\tDEBUG_LOCKS_WARN_ON(curr->lockdep_depth != depth - merged);\n\n\t \n\treturn 0;\n}\n\nstatic __always_inline\nint __lock_is_held(const struct lockdep_map *lock, int read)\n{\n\tstruct task_struct *curr = current;\n\tint i;\n\n\tfor (i = 0; i < curr->lockdep_depth; i++) {\n\t\tstruct held_lock *hlock = curr->held_locks + i;\n\n\t\tif (match_held_lock(hlock, lock)) {\n\t\t\tif (read == -1 || !!hlock->read == read)\n\t\t\t\treturn LOCK_STATE_HELD;\n\n\t\t\treturn LOCK_STATE_NOT_HELD;\n\t\t}\n\t}\n\n\treturn LOCK_STATE_NOT_HELD;\n}\n\nstatic struct pin_cookie __lock_pin_lock(struct lockdep_map *lock)\n{\n\tstruct pin_cookie cookie = NIL_COOKIE;\n\tstruct task_struct *curr = current;\n\tint i;\n\n\tif (unlikely(!debug_locks))\n\t\treturn cookie;\n\n\tfor (i = 0; i < curr->lockdep_depth; i++) {\n\t\tstruct held_lock *hlock = curr->held_locks + i;\n\n\t\tif (match_held_lock(hlock, lock)) {\n\t\t\t \n\t\t\tcookie.val = 1 + (sched_clock() & 0xffff);\n\t\t\thlock->pin_count += cookie.val;\n\t\t\treturn cookie;\n\t\t}\n\t}\n\n\tWARN(1, \"pinning an unheld lock\\n\");\n\treturn cookie;\n}\n\nstatic void __lock_repin_lock(struct lockdep_map *lock, struct pin_cookie cookie)\n{\n\tstruct task_struct *curr = current;\n\tint i;\n\n\tif (unlikely(!debug_locks))\n\t\treturn;\n\n\tfor (i = 0; i < curr->lockdep_depth; i++) {\n\t\tstruct held_lock *hlock = curr->held_locks + i;\n\n\t\tif (match_held_lock(hlock, lock)) {\n\t\t\thlock->pin_count += cookie.val;\n\t\t\treturn;\n\t\t}\n\t}\n\n\tWARN(1, \"pinning an unheld lock\\n\");\n}\n\nstatic void __lock_unpin_lock(struct lockdep_map *lock, struct pin_cookie cookie)\n{\n\tstruct task_struct *curr = current;\n\tint i;\n\n\tif (unlikely(!debug_locks))\n\t\treturn;\n\n\tfor (i = 0; i < curr->lockdep_depth; i++) {\n\t\tstruct held_lock *hlock = curr->held_locks + i;\n\n\t\tif (match_held_lock(hlock, lock)) {\n\t\t\tif (WARN(!hlock->pin_count, \"unpinning an unpinned lock\\n\"))\n\t\t\t\treturn;\n\n\t\t\thlock->pin_count -= cookie.val;\n\n\t\t\tif (WARN((int)hlock->pin_count < 0, \"pin count corrupted\\n\"))\n\t\t\t\thlock->pin_count = 0;\n\n\t\t\treturn;\n\t\t}\n\t}\n\n\tWARN(1, \"unpinning an unheld lock\\n\");\n}\n\n \nstatic noinstr void check_flags(unsigned long flags)\n{\n#if defined(CONFIG_PROVE_LOCKING) && defined(CONFIG_DEBUG_LOCKDEP)\n\tif (!debug_locks)\n\t\treturn;\n\n\t \n\tinstrumentation_begin();\n\n\tif (irqs_disabled_flags(flags)) {\n\t\tif (DEBUG_LOCKS_WARN_ON(lockdep_hardirqs_enabled())) {\n\t\t\tprintk(\"possible reason: unannotated irqs-off.\\n\");\n\t\t}\n\t} else {\n\t\tif (DEBUG_LOCKS_WARN_ON(!lockdep_hardirqs_enabled())) {\n\t\t\tprintk(\"possible reason: unannotated irqs-on.\\n\");\n\t\t}\n\t}\n\n#ifndef CONFIG_PREEMPT_RT\n\t \n\tif (!hardirq_count()) {\n\t\tif (softirq_count()) {\n\t\t\t \n\t\t\tDEBUG_LOCKS_WARN_ON(current->softirqs_enabled);\n\t\t} else {\n\t\t\t \n\t\t\tDEBUG_LOCKS_WARN_ON(!current->softirqs_enabled);\n\t\t}\n\t}\n#endif\n\n\tif (!debug_locks)\n\t\tprint_irqtrace_events(current);\n\n\tinstrumentation_end();\n#endif\n}\n\nvoid lock_set_class(struct lockdep_map *lock, const char *name,\n\t\t    struct lock_class_key *key, unsigned int subclass,\n\t\t    unsigned long ip)\n{\n\tunsigned long flags;\n\n\tif (unlikely(!lockdep_enabled()))\n\t\treturn;\n\n\traw_local_irq_save(flags);\n\tlockdep_recursion_inc();\n\tcheck_flags(flags);\n\tif (__lock_set_class(lock, name, key, subclass, ip))\n\t\tcheck_chain_key(current);\n\tlockdep_recursion_finish();\n\traw_local_irq_restore(flags);\n}\nEXPORT_SYMBOL_GPL(lock_set_class);\n\nvoid lock_downgrade(struct lockdep_map *lock, unsigned long ip)\n{\n\tunsigned long flags;\n\n\tif (unlikely(!lockdep_enabled()))\n\t\treturn;\n\n\traw_local_irq_save(flags);\n\tlockdep_recursion_inc();\n\tcheck_flags(flags);\n\tif (__lock_downgrade(lock, ip))\n\t\tcheck_chain_key(current);\n\tlockdep_recursion_finish();\n\traw_local_irq_restore(flags);\n}\nEXPORT_SYMBOL_GPL(lock_downgrade);\n\n \nstatic void verify_lock_unused(struct lockdep_map *lock, struct held_lock *hlock, int subclass)\n{\n#ifdef CONFIG_PROVE_LOCKING\n\tstruct lock_class *class = look_up_lock_class(lock, subclass);\n\tunsigned long mask = LOCKF_USED;\n\n\t \n\tif (!class)\n\t\treturn;\n\n\t \n\tif (!hlock->read)\n\t\tmask |= LOCKF_USED_READ;\n\n\tif (!(class->usage_mask & mask))\n\t\treturn;\n\n\thlock->class_idx = class - lock_classes;\n\n\tprint_usage_bug(current, hlock, LOCK_USED, LOCK_USAGE_STATES);\n#endif\n}\n\nstatic bool lockdep_nmi(void)\n{\n\tif (raw_cpu_read(lockdep_recursion))\n\t\treturn false;\n\n\tif (!in_nmi())\n\t\treturn false;\n\n\treturn true;\n}\n\n \nbool read_lock_is_recursive(void)\n{\n\treturn force_read_lock_recursive ||\n\t       !IS_ENABLED(CONFIG_QUEUED_RWLOCKS) ||\n\t       in_interrupt();\n}\nEXPORT_SYMBOL_GPL(read_lock_is_recursive);\n\n \nvoid lock_acquire(struct lockdep_map *lock, unsigned int subclass,\n\t\t\t  int trylock, int read, int check,\n\t\t\t  struct lockdep_map *nest_lock, unsigned long ip)\n{\n\tunsigned long flags;\n\n\ttrace_lock_acquire(lock, subclass, trylock, read, check, nest_lock, ip);\n\n\tif (!debug_locks)\n\t\treturn;\n\n\tif (unlikely(!lockdep_enabled())) {\n\t\t \n\t\tif (lockdep_nmi() && !trylock) {\n\t\t\tstruct held_lock hlock;\n\n\t\t\thlock.acquire_ip = ip;\n\t\t\thlock.instance = lock;\n\t\t\thlock.nest_lock = nest_lock;\n\t\t\thlock.irq_context = 2; \n\t\t\thlock.trylock = trylock;\n\t\t\thlock.read = read;\n\t\t\thlock.check = check;\n\t\t\thlock.hardirqs_off = true;\n\t\t\thlock.references = 0;\n\n\t\t\tverify_lock_unused(lock, &hlock, subclass);\n\t\t}\n\t\treturn;\n\t}\n\n\traw_local_irq_save(flags);\n\tcheck_flags(flags);\n\n\tlockdep_recursion_inc();\n\t__lock_acquire(lock, subclass, trylock, read, check,\n\t\t       irqs_disabled_flags(flags), nest_lock, ip, 0, 0, 0);\n\tlockdep_recursion_finish();\n\traw_local_irq_restore(flags);\n}\nEXPORT_SYMBOL_GPL(lock_acquire);\n\nvoid lock_release(struct lockdep_map *lock, unsigned long ip)\n{\n\tunsigned long flags;\n\n\ttrace_lock_release(lock, ip);\n\n\tif (unlikely(!lockdep_enabled()))\n\t\treturn;\n\n\traw_local_irq_save(flags);\n\tcheck_flags(flags);\n\n\tlockdep_recursion_inc();\n\tif (__lock_release(lock, ip))\n\t\tcheck_chain_key(current);\n\tlockdep_recursion_finish();\n\traw_local_irq_restore(flags);\n}\nEXPORT_SYMBOL_GPL(lock_release);\n\n \nvoid lock_sync(struct lockdep_map *lock, unsigned subclass, int read,\n\t       int check, struct lockdep_map *nest_lock, unsigned long ip)\n{\n\tunsigned long flags;\n\n\tif (unlikely(!lockdep_enabled()))\n\t\treturn;\n\n\traw_local_irq_save(flags);\n\tcheck_flags(flags);\n\n\tlockdep_recursion_inc();\n\t__lock_acquire(lock, subclass, 0, read, check,\n\t\t       irqs_disabled_flags(flags), nest_lock, ip, 0, 0, 1);\n\tcheck_chain_key(current);\n\tlockdep_recursion_finish();\n\traw_local_irq_restore(flags);\n}\nEXPORT_SYMBOL_GPL(lock_sync);\n\nnoinstr int lock_is_held_type(const struct lockdep_map *lock, int read)\n{\n\tunsigned long flags;\n\tint ret = LOCK_STATE_NOT_HELD;\n\n\t \n\tif (unlikely(!lockdep_enabled()))\n\t\treturn LOCK_STATE_UNKNOWN;\n\n\traw_local_irq_save(flags);\n\tcheck_flags(flags);\n\n\tlockdep_recursion_inc();\n\tret = __lock_is_held(lock, read);\n\tlockdep_recursion_finish();\n\traw_local_irq_restore(flags);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(lock_is_held_type);\nNOKPROBE_SYMBOL(lock_is_held_type);\n\nstruct pin_cookie lock_pin_lock(struct lockdep_map *lock)\n{\n\tstruct pin_cookie cookie = NIL_COOKIE;\n\tunsigned long flags;\n\n\tif (unlikely(!lockdep_enabled()))\n\t\treturn cookie;\n\n\traw_local_irq_save(flags);\n\tcheck_flags(flags);\n\n\tlockdep_recursion_inc();\n\tcookie = __lock_pin_lock(lock);\n\tlockdep_recursion_finish();\n\traw_local_irq_restore(flags);\n\n\treturn cookie;\n}\nEXPORT_SYMBOL_GPL(lock_pin_lock);\n\nvoid lock_repin_lock(struct lockdep_map *lock, struct pin_cookie cookie)\n{\n\tunsigned long flags;\n\n\tif (unlikely(!lockdep_enabled()))\n\t\treturn;\n\n\traw_local_irq_save(flags);\n\tcheck_flags(flags);\n\n\tlockdep_recursion_inc();\n\t__lock_repin_lock(lock, cookie);\n\tlockdep_recursion_finish();\n\traw_local_irq_restore(flags);\n}\nEXPORT_SYMBOL_GPL(lock_repin_lock);\n\nvoid lock_unpin_lock(struct lockdep_map *lock, struct pin_cookie cookie)\n{\n\tunsigned long flags;\n\n\tif (unlikely(!lockdep_enabled()))\n\t\treturn;\n\n\traw_local_irq_save(flags);\n\tcheck_flags(flags);\n\n\tlockdep_recursion_inc();\n\t__lock_unpin_lock(lock, cookie);\n\tlockdep_recursion_finish();\n\traw_local_irq_restore(flags);\n}\nEXPORT_SYMBOL_GPL(lock_unpin_lock);\n\n#ifdef CONFIG_LOCK_STAT\nstatic void print_lock_contention_bug(struct task_struct *curr,\n\t\t\t\t      struct lockdep_map *lock,\n\t\t\t\t      unsigned long ip)\n{\n\tif (!debug_locks_off())\n\t\treturn;\n\tif (debug_locks_silent)\n\t\treturn;\n\n\tpr_warn(\"\\n\");\n\tpr_warn(\"=================================\\n\");\n\tpr_warn(\"WARNING: bad contention detected!\\n\");\n\tprint_kernel_ident();\n\tpr_warn(\"---------------------------------\\n\");\n\tpr_warn(\"%s/%d is trying to contend lock (\",\n\t\tcurr->comm, task_pid_nr(curr));\n\tprint_lockdep_cache(lock);\n\tpr_cont(\") at:\\n\");\n\tprint_ip_sym(KERN_WARNING, ip);\n\tpr_warn(\"but there are no locks held!\\n\");\n\tpr_warn(\"\\nother info that might help us debug this:\\n\");\n\tlockdep_print_held_locks(curr);\n\n\tpr_warn(\"\\nstack backtrace:\\n\");\n\tdump_stack();\n}\n\nstatic void\n__lock_contended(struct lockdep_map *lock, unsigned long ip)\n{\n\tstruct task_struct *curr = current;\n\tstruct held_lock *hlock;\n\tstruct lock_class_stats *stats;\n\tunsigned int depth;\n\tint i, contention_point, contending_point;\n\n\tdepth = curr->lockdep_depth;\n\t \n\tif (DEBUG_LOCKS_WARN_ON(!depth))\n\t\treturn;\n\n\thlock = find_held_lock(curr, lock, depth, &i);\n\tif (!hlock) {\n\t\tprint_lock_contention_bug(curr, lock, ip);\n\t\treturn;\n\t}\n\n\tif (hlock->instance != lock)\n\t\treturn;\n\n\thlock->waittime_stamp = lockstat_clock();\n\n\tcontention_point = lock_point(hlock_class(hlock)->contention_point, ip);\n\tcontending_point = lock_point(hlock_class(hlock)->contending_point,\n\t\t\t\t      lock->ip);\n\n\tstats = get_lock_stats(hlock_class(hlock));\n\tif (contention_point < LOCKSTAT_POINTS)\n\t\tstats->contention_point[contention_point]++;\n\tif (contending_point < LOCKSTAT_POINTS)\n\t\tstats->contending_point[contending_point]++;\n\tif (lock->cpu != smp_processor_id())\n\t\tstats->bounces[bounce_contended + !!hlock->read]++;\n}\n\nstatic void\n__lock_acquired(struct lockdep_map *lock, unsigned long ip)\n{\n\tstruct task_struct *curr = current;\n\tstruct held_lock *hlock;\n\tstruct lock_class_stats *stats;\n\tunsigned int depth;\n\tu64 now, waittime = 0;\n\tint i, cpu;\n\n\tdepth = curr->lockdep_depth;\n\t \n\tif (DEBUG_LOCKS_WARN_ON(!depth))\n\t\treturn;\n\n\thlock = find_held_lock(curr, lock, depth, &i);\n\tif (!hlock) {\n\t\tprint_lock_contention_bug(curr, lock, _RET_IP_);\n\t\treturn;\n\t}\n\n\tif (hlock->instance != lock)\n\t\treturn;\n\n\tcpu = smp_processor_id();\n\tif (hlock->waittime_stamp) {\n\t\tnow = lockstat_clock();\n\t\twaittime = now - hlock->waittime_stamp;\n\t\thlock->holdtime_stamp = now;\n\t}\n\n\tstats = get_lock_stats(hlock_class(hlock));\n\tif (waittime) {\n\t\tif (hlock->read)\n\t\t\tlock_time_inc(&stats->read_waittime, waittime);\n\t\telse\n\t\t\tlock_time_inc(&stats->write_waittime, waittime);\n\t}\n\tif (lock->cpu != cpu)\n\t\tstats->bounces[bounce_acquired + !!hlock->read]++;\n\n\tlock->cpu = cpu;\n\tlock->ip = ip;\n}\n\nvoid lock_contended(struct lockdep_map *lock, unsigned long ip)\n{\n\tunsigned long flags;\n\n\ttrace_lock_contended(lock, ip);\n\n\tif (unlikely(!lock_stat || !lockdep_enabled()))\n\t\treturn;\n\n\traw_local_irq_save(flags);\n\tcheck_flags(flags);\n\tlockdep_recursion_inc();\n\t__lock_contended(lock, ip);\n\tlockdep_recursion_finish();\n\traw_local_irq_restore(flags);\n}\nEXPORT_SYMBOL_GPL(lock_contended);\n\nvoid lock_acquired(struct lockdep_map *lock, unsigned long ip)\n{\n\tunsigned long flags;\n\n\ttrace_lock_acquired(lock, ip);\n\n\tif (unlikely(!lock_stat || !lockdep_enabled()))\n\t\treturn;\n\n\traw_local_irq_save(flags);\n\tcheck_flags(flags);\n\tlockdep_recursion_inc();\n\t__lock_acquired(lock, ip);\n\tlockdep_recursion_finish();\n\traw_local_irq_restore(flags);\n}\nEXPORT_SYMBOL_GPL(lock_acquired);\n#endif\n\n \n\nvoid lockdep_reset(void)\n{\n\tunsigned long flags;\n\tint i;\n\n\traw_local_irq_save(flags);\n\tlockdep_init_task(current);\n\tmemset(current->held_locks, 0, MAX_LOCK_DEPTH*sizeof(struct held_lock));\n\tnr_hardirq_chains = 0;\n\tnr_softirq_chains = 0;\n\tnr_process_chains = 0;\n\tdebug_locks = 1;\n\tfor (i = 0; i < CHAINHASH_SIZE; i++)\n\t\tINIT_HLIST_HEAD(chainhash_table + i);\n\traw_local_irq_restore(flags);\n}\n\n \nstatic void remove_class_from_lock_chain(struct pending_free *pf,\n\t\t\t\t\t struct lock_chain *chain,\n\t\t\t\t\t struct lock_class *class)\n{\n#ifdef CONFIG_PROVE_LOCKING\n\tint i;\n\n\tfor (i = chain->base; i < chain->base + chain->depth; i++) {\n\t\tif (chain_hlock_class_idx(chain_hlocks[i]) != class - lock_classes)\n\t\t\tcontinue;\n\t\t \n\t\tgoto free_lock_chain;\n\t}\n\t \n\treturn;\n\nfree_lock_chain:\n\tfree_chain_hlocks(chain->base, chain->depth);\n\t \n\tWRITE_ONCE(chain->chain_key, INITIAL_CHAIN_KEY);\n\tdec_chains(chain->irq_context);\n\n\t \n\thlist_del_rcu(&chain->entry);\n\t__set_bit(chain - lock_chains, pf->lock_chains_being_freed);\n\tnr_zapped_lock_chains++;\n#endif\n}\n\n \nstatic void remove_class_from_lock_chains(struct pending_free *pf,\n\t\t\t\t\t  struct lock_class *class)\n{\n\tstruct lock_chain *chain;\n\tstruct hlist_head *head;\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(chainhash_table); i++) {\n\t\thead = chainhash_table + i;\n\t\thlist_for_each_entry_rcu(chain, head, entry) {\n\t\t\tremove_class_from_lock_chain(pf, chain, class);\n\t\t}\n\t}\n}\n\n \nstatic void zap_class(struct pending_free *pf, struct lock_class *class)\n{\n\tstruct lock_list *entry;\n\tint i;\n\n\tWARN_ON_ONCE(!class->key);\n\n\t \n\tfor_each_set_bit(i, list_entries_in_use, ARRAY_SIZE(list_entries)) {\n\t\tentry = list_entries + i;\n\t\tif (entry->class != class && entry->links_to != class)\n\t\t\tcontinue;\n\t\t__clear_bit(i, list_entries_in_use);\n\t\tnr_list_entries--;\n\t\tlist_del_rcu(&entry->entry);\n\t}\n\tif (list_empty(&class->locks_after) &&\n\t    list_empty(&class->locks_before)) {\n\t\tlist_move_tail(&class->lock_entry, &pf->zapped);\n\t\thlist_del_rcu(&class->hash_entry);\n\t\tWRITE_ONCE(class->key, NULL);\n\t\tWRITE_ONCE(class->name, NULL);\n\t\tnr_lock_classes--;\n\t\t__clear_bit(class - lock_classes, lock_classes_in_use);\n\t\tif (class - lock_classes == max_lock_class_idx)\n\t\t\tmax_lock_class_idx--;\n\t} else {\n\t\tWARN_ONCE(true, \"%s() failed for class %s\\n\", __func__,\n\t\t\t  class->name);\n\t}\n\n\tremove_class_from_lock_chains(pf, class);\n\tnr_zapped_classes++;\n}\n\nstatic void reinit_class(struct lock_class *class)\n{\n\tWARN_ON_ONCE(!class->lock_entry.next);\n\tWARN_ON_ONCE(!list_empty(&class->locks_after));\n\tWARN_ON_ONCE(!list_empty(&class->locks_before));\n\tmemset_startat(class, 0, key);\n\tWARN_ON_ONCE(!class->lock_entry.next);\n\tWARN_ON_ONCE(!list_empty(&class->locks_after));\n\tWARN_ON_ONCE(!list_empty(&class->locks_before));\n}\n\nstatic inline int within(const void *addr, void *start, unsigned long size)\n{\n\treturn addr >= start && addr < start + size;\n}\n\nstatic bool inside_selftest(void)\n{\n\treturn current == lockdep_selftest_task_struct;\n}\n\n \nstatic struct pending_free *get_pending_free(void)\n{\n\treturn delayed_free.pf + delayed_free.index;\n}\n\nstatic void free_zapped_rcu(struct rcu_head *cb);\n\n \nstatic void call_rcu_zapped(struct pending_free *pf)\n{\n\tWARN_ON_ONCE(inside_selftest());\n\n\tif (list_empty(&pf->zapped))\n\t\treturn;\n\n\tif (delayed_free.scheduled)\n\t\treturn;\n\n\tdelayed_free.scheduled = true;\n\n\tWARN_ON_ONCE(delayed_free.pf + delayed_free.index != pf);\n\tdelayed_free.index ^= 1;\n\n\tcall_rcu(&delayed_free.rcu_head, free_zapped_rcu);\n}\n\n \nstatic void __free_zapped_classes(struct pending_free *pf)\n{\n\tstruct lock_class *class;\n\n\tcheck_data_structures();\n\n\tlist_for_each_entry(class, &pf->zapped, lock_entry)\n\t\treinit_class(class);\n\n\tlist_splice_init(&pf->zapped, &free_lock_classes);\n\n#ifdef CONFIG_PROVE_LOCKING\n\tbitmap_andnot(lock_chains_in_use, lock_chains_in_use,\n\t\t      pf->lock_chains_being_freed, ARRAY_SIZE(lock_chains));\n\tbitmap_clear(pf->lock_chains_being_freed, 0, ARRAY_SIZE(lock_chains));\n#endif\n}\n\nstatic void free_zapped_rcu(struct rcu_head *ch)\n{\n\tstruct pending_free *pf;\n\tunsigned long flags;\n\n\tif (WARN_ON_ONCE(ch != &delayed_free.rcu_head))\n\t\treturn;\n\n\traw_local_irq_save(flags);\n\tlockdep_lock();\n\n\t \n\tpf = delayed_free.pf + (delayed_free.index ^ 1);\n\t__free_zapped_classes(pf);\n\tdelayed_free.scheduled = false;\n\n\t \n\tcall_rcu_zapped(delayed_free.pf + delayed_free.index);\n\n\tlockdep_unlock();\n\traw_local_irq_restore(flags);\n}\n\n \nstatic void __lockdep_free_key_range(struct pending_free *pf, void *start,\n\t\t\t\t     unsigned long size)\n{\n\tstruct lock_class *class;\n\tstruct hlist_head *head;\n\tint i;\n\n\t \n\tfor (i = 0; i < CLASSHASH_SIZE; i++) {\n\t\thead = classhash_table + i;\n\t\thlist_for_each_entry_rcu(class, head, hash_entry) {\n\t\t\tif (!within(class->key, start, size) &&\n\t\t\t    !within(class->name, start, size))\n\t\t\t\tcontinue;\n\t\t\tzap_class(pf, class);\n\t\t}\n\t}\n}\n\n \nstatic void lockdep_free_key_range_reg(void *start, unsigned long size)\n{\n\tstruct pending_free *pf;\n\tunsigned long flags;\n\n\tinit_data_structures_once();\n\n\traw_local_irq_save(flags);\n\tlockdep_lock();\n\tpf = get_pending_free();\n\t__lockdep_free_key_range(pf, start, size);\n\tcall_rcu_zapped(pf);\n\tlockdep_unlock();\n\traw_local_irq_restore(flags);\n\n\t \n\tsynchronize_rcu();\n}\n\n \nstatic void lockdep_free_key_range_imm(void *start, unsigned long size)\n{\n\tstruct pending_free *pf = delayed_free.pf;\n\tunsigned long flags;\n\n\tinit_data_structures_once();\n\n\traw_local_irq_save(flags);\n\tlockdep_lock();\n\t__lockdep_free_key_range(pf, start, size);\n\t__free_zapped_classes(pf);\n\tlockdep_unlock();\n\traw_local_irq_restore(flags);\n}\n\nvoid lockdep_free_key_range(void *start, unsigned long size)\n{\n\tinit_data_structures_once();\n\n\tif (inside_selftest())\n\t\tlockdep_free_key_range_imm(start, size);\n\telse\n\t\tlockdep_free_key_range_reg(start, size);\n}\n\n \nstatic bool lock_class_cache_is_registered(struct lockdep_map *lock)\n{\n\tstruct lock_class *class;\n\tstruct hlist_head *head;\n\tint i, j;\n\n\tfor (i = 0; i < CLASSHASH_SIZE; i++) {\n\t\thead = classhash_table + i;\n\t\thlist_for_each_entry_rcu(class, head, hash_entry) {\n\t\t\tfor (j = 0; j < NR_LOCKDEP_CACHING_CLASSES; j++)\n\t\t\t\tif (lock->class_cache[j] == class)\n\t\t\t\t\treturn true;\n\t\t}\n\t}\n\treturn false;\n}\n\n \nstatic void __lockdep_reset_lock(struct pending_free *pf,\n\t\t\t\t struct lockdep_map *lock)\n{\n\tstruct lock_class *class;\n\tint j;\n\n\t \n\tfor (j = 0; j < MAX_LOCKDEP_SUBCLASSES; j++) {\n\t\t \n\t\tclass = look_up_lock_class(lock, j);\n\t\tif (class)\n\t\t\tzap_class(pf, class);\n\t}\n\t \n\tif (WARN_ON_ONCE(lock_class_cache_is_registered(lock)))\n\t\tdebug_locks_off();\n}\n\n \nstatic void lockdep_reset_lock_reg(struct lockdep_map *lock)\n{\n\tstruct pending_free *pf;\n\tunsigned long flags;\n\tint locked;\n\n\traw_local_irq_save(flags);\n\tlocked = graph_lock();\n\tif (!locked)\n\t\tgoto out_irq;\n\n\tpf = get_pending_free();\n\t__lockdep_reset_lock(pf, lock);\n\tcall_rcu_zapped(pf);\n\n\tgraph_unlock();\nout_irq:\n\traw_local_irq_restore(flags);\n}\n\n \nstatic void lockdep_reset_lock_imm(struct lockdep_map *lock)\n{\n\tstruct pending_free *pf = delayed_free.pf;\n\tunsigned long flags;\n\n\traw_local_irq_save(flags);\n\tlockdep_lock();\n\t__lockdep_reset_lock(pf, lock);\n\t__free_zapped_classes(pf);\n\tlockdep_unlock();\n\traw_local_irq_restore(flags);\n}\n\nvoid lockdep_reset_lock(struct lockdep_map *lock)\n{\n\tinit_data_structures_once();\n\n\tif (inside_selftest())\n\t\tlockdep_reset_lock_imm(lock);\n\telse\n\t\tlockdep_reset_lock_reg(lock);\n}\n\n \nvoid lockdep_unregister_key(struct lock_class_key *key)\n{\n\tstruct hlist_head *hash_head = keyhashentry(key);\n\tstruct lock_class_key *k;\n\tstruct pending_free *pf;\n\tunsigned long flags;\n\tbool found = false;\n\n\tmight_sleep();\n\n\tif (WARN_ON_ONCE(static_obj(key)))\n\t\treturn;\n\n\traw_local_irq_save(flags);\n\tlockdep_lock();\n\n\thlist_for_each_entry_rcu(k, hash_head, hash_entry) {\n\t\tif (k == key) {\n\t\t\thlist_del_rcu(&k->hash_entry);\n\t\t\tfound = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\tWARN_ON_ONCE(!found && debug_locks);\n\tif (found) {\n\t\tpf = get_pending_free();\n\t\t__lockdep_free_key_range(pf, key, 1);\n\t\tcall_rcu_zapped(pf);\n\t}\n\tlockdep_unlock();\n\traw_local_irq_restore(flags);\n\n\t \n\tsynchronize_rcu();\n}\nEXPORT_SYMBOL_GPL(lockdep_unregister_key);\n\nvoid __init lockdep_init(void)\n{\n\tprintk(\"Lock dependency validator: Copyright (c) 2006 Red Hat, Inc., Ingo Molnar\\n\");\n\n\tprintk(\"... MAX_LOCKDEP_SUBCLASSES:  %lu\\n\", MAX_LOCKDEP_SUBCLASSES);\n\tprintk(\"... MAX_LOCK_DEPTH:          %lu\\n\", MAX_LOCK_DEPTH);\n\tprintk(\"... MAX_LOCKDEP_KEYS:        %lu\\n\", MAX_LOCKDEP_KEYS);\n\tprintk(\"... CLASSHASH_SIZE:          %lu\\n\", CLASSHASH_SIZE);\n\tprintk(\"... MAX_LOCKDEP_ENTRIES:     %lu\\n\", MAX_LOCKDEP_ENTRIES);\n\tprintk(\"... MAX_LOCKDEP_CHAINS:      %lu\\n\", MAX_LOCKDEP_CHAINS);\n\tprintk(\"... CHAINHASH_SIZE:          %lu\\n\", CHAINHASH_SIZE);\n\n\tprintk(\" memory used by lock dependency info: %zu kB\\n\",\n\t       (sizeof(lock_classes) +\n\t\tsizeof(lock_classes_in_use) +\n\t\tsizeof(classhash_table) +\n\t\tsizeof(list_entries) +\n\t\tsizeof(list_entries_in_use) +\n\t\tsizeof(chainhash_table) +\n\t\tsizeof(delayed_free)\n#ifdef CONFIG_PROVE_LOCKING\n\t\t+ sizeof(lock_cq)\n\t\t+ sizeof(lock_chains)\n\t\t+ sizeof(lock_chains_in_use)\n\t\t+ sizeof(chain_hlocks)\n#endif\n\t\t) / 1024\n\t\t);\n\n#if defined(CONFIG_TRACE_IRQFLAGS) && defined(CONFIG_PROVE_LOCKING)\n\tprintk(\" memory used for stack traces: %zu kB\\n\",\n\t       (sizeof(stack_trace) + sizeof(stack_trace_hash)) / 1024\n\t       );\n#endif\n\n\tprintk(\" per task-struct memory footprint: %zu bytes\\n\",\n\t       sizeof(((struct task_struct *)NULL)->held_locks));\n}\n\nstatic void\nprint_freed_lock_bug(struct task_struct *curr, const void *mem_from,\n\t\t     const void *mem_to, struct held_lock *hlock)\n{\n\tif (!debug_locks_off())\n\t\treturn;\n\tif (debug_locks_silent)\n\t\treturn;\n\n\tpr_warn(\"\\n\");\n\tpr_warn(\"=========================\\n\");\n\tpr_warn(\"WARNING: held lock freed!\\n\");\n\tprint_kernel_ident();\n\tpr_warn(\"-------------------------\\n\");\n\tpr_warn(\"%s/%d is freeing memory %px-%px, with a lock still held there!\\n\",\n\t\tcurr->comm, task_pid_nr(curr), mem_from, mem_to-1);\n\tprint_lock(hlock);\n\tlockdep_print_held_locks(curr);\n\n\tpr_warn(\"\\nstack backtrace:\\n\");\n\tdump_stack();\n}\n\nstatic inline int not_in_range(const void* mem_from, unsigned long mem_len,\n\t\t\t\tconst void* lock_from, unsigned long lock_len)\n{\n\treturn lock_from + lock_len <= mem_from ||\n\t\tmem_from + mem_len <= lock_from;\n}\n\n \nvoid debug_check_no_locks_freed(const void *mem_from, unsigned long mem_len)\n{\n\tstruct task_struct *curr = current;\n\tstruct held_lock *hlock;\n\tunsigned long flags;\n\tint i;\n\n\tif (unlikely(!debug_locks))\n\t\treturn;\n\n\traw_local_irq_save(flags);\n\tfor (i = 0; i < curr->lockdep_depth; i++) {\n\t\thlock = curr->held_locks + i;\n\n\t\tif (not_in_range(mem_from, mem_len, hlock->instance,\n\t\t\t\t\tsizeof(*hlock->instance)))\n\t\t\tcontinue;\n\n\t\tprint_freed_lock_bug(curr, mem_from, mem_from + mem_len, hlock);\n\t\tbreak;\n\t}\n\traw_local_irq_restore(flags);\n}\nEXPORT_SYMBOL_GPL(debug_check_no_locks_freed);\n\nstatic void print_held_locks_bug(void)\n{\n\tif (!debug_locks_off())\n\t\treturn;\n\tif (debug_locks_silent)\n\t\treturn;\n\n\tpr_warn(\"\\n\");\n\tpr_warn(\"====================================\\n\");\n\tpr_warn(\"WARNING: %s/%d still has locks held!\\n\",\n\t       current->comm, task_pid_nr(current));\n\tprint_kernel_ident();\n\tpr_warn(\"------------------------------------\\n\");\n\tlockdep_print_held_locks(current);\n\tpr_warn(\"\\nstack backtrace:\\n\");\n\tdump_stack();\n}\n\nvoid debug_check_no_locks_held(void)\n{\n\tif (unlikely(current->lockdep_depth > 0))\n\t\tprint_held_locks_bug();\n}\nEXPORT_SYMBOL_GPL(debug_check_no_locks_held);\n\n#ifdef __KERNEL__\nvoid debug_show_all_locks(void)\n{\n\tstruct task_struct *g, *p;\n\n\tif (unlikely(!debug_locks)) {\n\t\tpr_warn(\"INFO: lockdep is turned off.\\n\");\n\t\treturn;\n\t}\n\tpr_warn(\"\\nShowing all locks held in the system:\\n\");\n\n\trcu_read_lock();\n\tfor_each_process_thread(g, p) {\n\t\tif (!p->lockdep_depth)\n\t\t\tcontinue;\n\t\tlockdep_print_held_locks(p);\n\t\ttouch_nmi_watchdog();\n\t\ttouch_all_softlockup_watchdogs();\n\t}\n\trcu_read_unlock();\n\n\tpr_warn(\"\\n\");\n\tpr_warn(\"=============================================\\n\\n\");\n}\nEXPORT_SYMBOL_GPL(debug_show_all_locks);\n#endif\n\n \nvoid debug_show_held_locks(struct task_struct *task)\n{\n\tif (unlikely(!debug_locks)) {\n\t\tprintk(\"INFO: lockdep is turned off.\\n\");\n\t\treturn;\n\t}\n\tlockdep_print_held_locks(task);\n}\nEXPORT_SYMBOL_GPL(debug_show_held_locks);\n\nasmlinkage __visible void lockdep_sys_exit(void)\n{\n\tstruct task_struct *curr = current;\n\n\tif (unlikely(curr->lockdep_depth)) {\n\t\tif (!debug_locks_off())\n\t\t\treturn;\n\t\tpr_warn(\"\\n\");\n\t\tpr_warn(\"================================================\\n\");\n\t\tpr_warn(\"WARNING: lock held when returning to user space!\\n\");\n\t\tprint_kernel_ident();\n\t\tpr_warn(\"------------------------------------------------\\n\");\n\t\tpr_warn(\"%s/%d is leaving the kernel with locks still held!\\n\",\n\t\t\t\tcurr->comm, curr->pid);\n\t\tlockdep_print_held_locks(curr);\n\t}\n\n\t \n\tlockdep_invariant_state(false);\n}\n\nvoid lockdep_rcu_suspicious(const char *file, const int line, const char *s)\n{\n\tstruct task_struct *curr = current;\n\tint dl = READ_ONCE(debug_locks);\n\tbool rcu = warn_rcu_enter();\n\n\t \n\tpr_warn(\"\\n\");\n\tpr_warn(\"=============================\\n\");\n\tpr_warn(\"WARNING: suspicious RCU usage\\n\");\n\tprint_kernel_ident();\n\tpr_warn(\"-----------------------------\\n\");\n\tpr_warn(\"%s:%d %s!\\n\", file, line, s);\n\tpr_warn(\"\\nother info that might help us debug this:\\n\\n\");\n\tpr_warn(\"\\n%srcu_scheduler_active = %d, debug_locks = %d\\n%s\",\n\t       !rcu_lockdep_current_cpu_online()\n\t\t\t? \"RCU used illegally from offline CPU!\\n\"\n\t\t\t: \"\",\n\t       rcu_scheduler_active, dl,\n\t       dl ? \"\" : \"Possible false positive due to lockdep disabling via debug_locks = 0\\n\");\n\n\t \n\tif (!rcu_is_watching())\n\t\tpr_warn(\"RCU used illegally from extended quiescent state!\\n\");\n\n\tlockdep_print_held_locks(curr);\n\tpr_warn(\"\\nstack backtrace:\\n\");\n\tdump_stack();\n\twarn_rcu_exit(rcu);\n}\nEXPORT_SYMBOL_GPL(lockdep_rcu_suspicious);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}