{
  "module_name": "qspinlock.c",
  "hash_id": "8e2bb6b9044e75ebe5898554a1865f77c5737d74d1fcc9ab0d1579f1229a6eda",
  "original_prompt": "Ingested from linux-6.6.14/kernel/locking/qspinlock.c",
  "human_readable_source": "\n \n\n#ifndef _GEN_PV_LOCK_SLOWPATH\n\n#include <linux/smp.h>\n#include <linux/bug.h>\n#include <linux/cpumask.h>\n#include <linux/percpu.h>\n#include <linux/hardirq.h>\n#include <linux/mutex.h>\n#include <linux/prefetch.h>\n#include <asm/byteorder.h>\n#include <asm/qspinlock.h>\n#include <trace/events/lock.h>\n\n \n#include \"qspinlock_stat.h\"\n\n \n\n#include \"mcs_spinlock.h\"\n#define MAX_NODES\t4\n\n \nstruct qnode {\n\tstruct mcs_spinlock mcs;\n#ifdef CONFIG_PARAVIRT_SPINLOCKS\n\tlong reserved[2];\n#endif\n};\n\n \n#ifndef _Q_PENDING_LOOPS\n#define _Q_PENDING_LOOPS\t1\n#endif\n\n \nstatic DEFINE_PER_CPU_ALIGNED(struct qnode, qnodes[MAX_NODES]);\n\n \n\nstatic inline __pure u32 encode_tail(int cpu, int idx)\n{\n\tu32 tail;\n\n\ttail  = (cpu + 1) << _Q_TAIL_CPU_OFFSET;\n\ttail |= idx << _Q_TAIL_IDX_OFFSET;  \n\n\treturn tail;\n}\n\nstatic inline __pure struct mcs_spinlock *decode_tail(u32 tail)\n{\n\tint cpu = (tail >> _Q_TAIL_CPU_OFFSET) - 1;\n\tint idx = (tail &  _Q_TAIL_IDX_MASK) >> _Q_TAIL_IDX_OFFSET;\n\n\treturn per_cpu_ptr(&qnodes[idx].mcs, cpu);\n}\n\nstatic inline __pure\nstruct mcs_spinlock *grab_mcs_node(struct mcs_spinlock *base, int idx)\n{\n\treturn &((struct qnode *)base + idx)->mcs;\n}\n\n#define _Q_LOCKED_PENDING_MASK (_Q_LOCKED_MASK | _Q_PENDING_MASK)\n\n#if _Q_PENDING_BITS == 8\n \nstatic __always_inline void clear_pending(struct qspinlock *lock)\n{\n\tWRITE_ONCE(lock->pending, 0);\n}\n\n \nstatic __always_inline void clear_pending_set_locked(struct qspinlock *lock)\n{\n\tWRITE_ONCE(lock->locked_pending, _Q_LOCKED_VAL);\n}\n\n \nstatic __always_inline u32 xchg_tail(struct qspinlock *lock, u32 tail)\n{\n\t \n\treturn (u32)xchg_relaxed(&lock->tail,\n\t\t\t\t tail >> _Q_TAIL_OFFSET) << _Q_TAIL_OFFSET;\n}\n\n#else  \n\n \nstatic __always_inline void clear_pending(struct qspinlock *lock)\n{\n\tatomic_andnot(_Q_PENDING_VAL, &lock->val);\n}\n\n \nstatic __always_inline void clear_pending_set_locked(struct qspinlock *lock)\n{\n\tatomic_add(-_Q_PENDING_VAL + _Q_LOCKED_VAL, &lock->val);\n}\n\n \nstatic __always_inline u32 xchg_tail(struct qspinlock *lock, u32 tail)\n{\n\tu32 old, new, val = atomic_read(&lock->val);\n\n\tfor (;;) {\n\t\tnew = (val & _Q_LOCKED_PENDING_MASK) | tail;\n\t\t \n\t\told = atomic_cmpxchg_relaxed(&lock->val, val, new);\n\t\tif (old == val)\n\t\t\tbreak;\n\n\t\tval = old;\n\t}\n\treturn old;\n}\n#endif  \n\n \n#ifndef queued_fetch_set_pending_acquire\nstatic __always_inline u32 queued_fetch_set_pending_acquire(struct qspinlock *lock)\n{\n\treturn atomic_fetch_or_acquire(_Q_PENDING_VAL, &lock->val);\n}\n#endif\n\n \nstatic __always_inline void set_locked(struct qspinlock *lock)\n{\n\tWRITE_ONCE(lock->locked, _Q_LOCKED_VAL);\n}\n\n\n \n\nstatic __always_inline void __pv_init_node(struct mcs_spinlock *node) { }\nstatic __always_inline void __pv_wait_node(struct mcs_spinlock *node,\n\t\t\t\t\t   struct mcs_spinlock *prev) { }\nstatic __always_inline void __pv_kick_node(struct qspinlock *lock,\n\t\t\t\t\t   struct mcs_spinlock *node) { }\nstatic __always_inline u32  __pv_wait_head_or_lock(struct qspinlock *lock,\n\t\t\t\t\t\t   struct mcs_spinlock *node)\n\t\t\t\t\t\t   { return 0; }\n\n#define pv_enabled()\t\tfalse\n\n#define pv_init_node\t\t__pv_init_node\n#define pv_wait_node\t\t__pv_wait_node\n#define pv_kick_node\t\t__pv_kick_node\n#define pv_wait_head_or_lock\t__pv_wait_head_or_lock\n\n#ifdef CONFIG_PARAVIRT_SPINLOCKS\n#define queued_spin_lock_slowpath\tnative_queued_spin_lock_slowpath\n#endif\n\n#endif  \n\n \nvoid __lockfunc queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)\n{\n\tstruct mcs_spinlock *prev, *next, *node;\n\tu32 old, tail;\n\tint idx;\n\n\tBUILD_BUG_ON(CONFIG_NR_CPUS >= (1U << _Q_TAIL_CPU_BITS));\n\n\tif (pv_enabled())\n\t\tgoto pv_queue;\n\n\tif (virt_spin_lock(lock))\n\t\treturn;\n\n\t \n\tif (val == _Q_PENDING_VAL) {\n\t\tint cnt = _Q_PENDING_LOOPS;\n\t\tval = atomic_cond_read_relaxed(&lock->val,\n\t\t\t\t\t       (VAL != _Q_PENDING_VAL) || !cnt--);\n\t}\n\n\t \n\tif (val & ~_Q_LOCKED_MASK)\n\t\tgoto queue;\n\n\t \n\tval = queued_fetch_set_pending_acquire(lock);\n\n\t \n\tif (unlikely(val & ~_Q_LOCKED_MASK)) {\n\n\t\t \n\t\tif (!(val & _Q_PENDING_MASK))\n\t\t\tclear_pending(lock);\n\n\t\tgoto queue;\n\t}\n\n\t \n\tif (val & _Q_LOCKED_MASK)\n\t\tsmp_cond_load_acquire(&lock->locked, !VAL);\n\n\t \n\tclear_pending_set_locked(lock);\n\tlockevent_inc(lock_pending);\n\treturn;\n\n\t \nqueue:\n\tlockevent_inc(lock_slowpath);\npv_queue:\n\tnode = this_cpu_ptr(&qnodes[0].mcs);\n\tidx = node->count++;\n\ttail = encode_tail(smp_processor_id(), idx);\n\n\ttrace_contention_begin(lock, LCB_F_SPIN);\n\n\t \n\tif (unlikely(idx >= MAX_NODES)) {\n\t\tlockevent_inc(lock_no_node);\n\t\twhile (!queued_spin_trylock(lock))\n\t\t\tcpu_relax();\n\t\tgoto release;\n\t}\n\n\tnode = grab_mcs_node(node, idx);\n\n\t \n\tlockevent_cond_inc(lock_use_node2 + idx - 1, idx);\n\n\t \n\tbarrier();\n\n\tnode->locked = 0;\n\tnode->next = NULL;\n\tpv_init_node(node);\n\n\t \n\tif (queued_spin_trylock(lock))\n\t\tgoto release;\n\n\t \n\tsmp_wmb();\n\n\t \n\told = xchg_tail(lock, tail);\n\tnext = NULL;\n\n\t \n\tif (old & _Q_TAIL_MASK) {\n\t\tprev = decode_tail(old);\n\n\t\t \n\t\tWRITE_ONCE(prev->next, node);\n\n\t\tpv_wait_node(node, prev);\n\t\tarch_mcs_spin_lock_contended(&node->locked);\n\n\t\t \n\t\tnext = READ_ONCE(node->next);\n\t\tif (next)\n\t\t\tprefetchw(next);\n\t}\n\n\t \n\tif ((val = pv_wait_head_or_lock(lock, node)))\n\t\tgoto locked;\n\n\tval = atomic_cond_read_acquire(&lock->val, !(VAL & _Q_LOCKED_PENDING_MASK));\n\nlocked:\n\t \n\n\t \n\tif ((val & _Q_TAIL_MASK) == tail) {\n\t\tif (atomic_try_cmpxchg_relaxed(&lock->val, &val, _Q_LOCKED_VAL))\n\t\t\tgoto release;  \n\t}\n\n\t \n\tset_locked(lock);\n\n\t \n\tif (!next)\n\t\tnext = smp_cond_load_relaxed(&node->next, (VAL));\n\n\tarch_mcs_spin_unlock_contended(&next->locked);\n\tpv_kick_node(lock, next);\n\nrelease:\n\ttrace_contention_end(lock, 0);\n\n\t \n\t__this_cpu_dec(qnodes[0].mcs.count);\n}\nEXPORT_SYMBOL(queued_spin_lock_slowpath);\n\n \n#if !defined(_GEN_PV_LOCK_SLOWPATH) && defined(CONFIG_PARAVIRT_SPINLOCKS)\n#define _GEN_PV_LOCK_SLOWPATH\n\n#undef  pv_enabled\n#define pv_enabled()\ttrue\n\n#undef pv_init_node\n#undef pv_wait_node\n#undef pv_kick_node\n#undef pv_wait_head_or_lock\n\n#undef  queued_spin_lock_slowpath\n#define queued_spin_lock_slowpath\t__pv_queued_spin_lock_slowpath\n\n#include \"qspinlock_paravirt.h\"\n#include \"qspinlock.c\"\n\nbool nopvspin __initdata;\nstatic __init int parse_nopvspin(char *arg)\n{\n\tnopvspin = true;\n\treturn 0;\n}\nearly_param(\"nopvspin\", parse_nopvspin);\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}