{
  "module_name": "qspinlock_paravirt.h",
  "hash_id": "c993283ab97b88e6f54c2b4a632acbd01e8f6823146b28bda40acea4fa7dd662",
  "original_prompt": "Ingested from linux-6.6.14/kernel/locking/qspinlock_paravirt.h",
  "human_readable_source": " \n#ifndef _GEN_PV_LOCK_SLOWPATH\n#error \"do not include this file\"\n#endif\n\n#include <linux/hash.h>\n#include <linux/memblock.h>\n#include <linux/debug_locks.h>\n\n \n\n#define _Q_SLOW_VAL\t(3U << _Q_LOCKED_OFFSET)\n\n \n#define PV_PREV_CHECK_MASK\t0xff\n\n \nenum vcpu_state {\n\tvcpu_running = 0,\n\tvcpu_halted,\t\t \n\tvcpu_hashed,\t\t \n};\n\nstruct pv_node {\n\tstruct mcs_spinlock\tmcs;\n\tint\t\t\tcpu;\n\tu8\t\t\tstate;\n};\n\n \n#define queued_spin_trylock(l)\tpv_hybrid_queued_unfair_trylock(l)\nstatic inline bool pv_hybrid_queued_unfair_trylock(struct qspinlock *lock)\n{\n\t \n\tfor (;;) {\n\t\tint val = atomic_read(&lock->val);\n\n\t\tif (!(val & _Q_LOCKED_PENDING_MASK) &&\n\t\t   (cmpxchg_acquire(&lock->locked, 0, _Q_LOCKED_VAL) == 0)) {\n\t\t\tlockevent_inc(pv_lock_stealing);\n\t\t\treturn true;\n\t\t}\n\t\tif (!(val & _Q_TAIL_MASK) || (val & _Q_PENDING_MASK))\n\t\t\tbreak;\n\n\t\tcpu_relax();\n\t}\n\n\treturn false;\n}\n\n \n#if _Q_PENDING_BITS == 8\nstatic __always_inline void set_pending(struct qspinlock *lock)\n{\n\tWRITE_ONCE(lock->pending, 1);\n}\n\n \nstatic __always_inline int trylock_clear_pending(struct qspinlock *lock)\n{\n\treturn !READ_ONCE(lock->locked) &&\n\t       (cmpxchg_acquire(&lock->locked_pending, _Q_PENDING_VAL,\n\t\t\t\t_Q_LOCKED_VAL) == _Q_PENDING_VAL);\n}\n#else  \nstatic __always_inline void set_pending(struct qspinlock *lock)\n{\n\tatomic_or(_Q_PENDING_VAL, &lock->val);\n}\n\nstatic __always_inline int trylock_clear_pending(struct qspinlock *lock)\n{\n\tint val = atomic_read(&lock->val);\n\n\tfor (;;) {\n\t\tint old, new;\n\n\t\tif (val  & _Q_LOCKED_MASK)\n\t\t\tbreak;\n\n\t\t \n\t\told = val;\n\t\tnew = (val & ~_Q_PENDING_MASK) | _Q_LOCKED_VAL;\n\t\tval = atomic_cmpxchg_acquire(&lock->val, old, new);\n\n\t\tif (val == old)\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}\n#endif  \n\n \nstruct pv_hash_entry {\n\tstruct qspinlock *lock;\n\tstruct pv_node   *node;\n};\n\n#define PV_HE_PER_LINE\t(SMP_CACHE_BYTES / sizeof(struct pv_hash_entry))\n#define PV_HE_MIN\t(PAGE_SIZE / sizeof(struct pv_hash_entry))\n\nstatic struct pv_hash_entry *pv_lock_hash;\nstatic unsigned int pv_lock_hash_bits __read_mostly;\n\n \nvoid __init __pv_init_lock_hash(void)\n{\n\tint pv_hash_size = ALIGN(4 * num_possible_cpus(), PV_HE_PER_LINE);\n\n\tif (pv_hash_size < PV_HE_MIN)\n\t\tpv_hash_size = PV_HE_MIN;\n\n\t \n\tpv_lock_hash = alloc_large_system_hash(\"PV qspinlock\",\n\t\t\t\t\t       sizeof(struct pv_hash_entry),\n\t\t\t\t\t       pv_hash_size, 0,\n\t\t\t\t\t       HASH_EARLY | HASH_ZERO,\n\t\t\t\t\t       &pv_lock_hash_bits, NULL,\n\t\t\t\t\t       pv_hash_size, pv_hash_size);\n}\n\n#define for_each_hash_entry(he, offset, hash)\t\t\t\t\t\t\\\n\tfor (hash &= ~(PV_HE_PER_LINE - 1), he = &pv_lock_hash[hash], offset = 0;\t\\\n\t     offset < (1 << pv_lock_hash_bits);\t\t\t\t\t\t\\\n\t     offset++, he = &pv_lock_hash[(hash + offset) & ((1 << pv_lock_hash_bits) - 1)])\n\nstatic struct qspinlock **pv_hash(struct qspinlock *lock, struct pv_node *node)\n{\n\tunsigned long offset, hash = hash_ptr(lock, pv_lock_hash_bits);\n\tstruct pv_hash_entry *he;\n\tint hopcnt = 0;\n\n\tfor_each_hash_entry(he, offset, hash) {\n\t\thopcnt++;\n\t\tif (!cmpxchg(&he->lock, NULL, lock)) {\n\t\t\tWRITE_ONCE(he->node, node);\n\t\t\tlockevent_pv_hop(hopcnt);\n\t\t\treturn &he->lock;\n\t\t}\n\t}\n\t \n\tBUG();\n}\n\nstatic struct pv_node *pv_unhash(struct qspinlock *lock)\n{\n\tunsigned long offset, hash = hash_ptr(lock, pv_lock_hash_bits);\n\tstruct pv_hash_entry *he;\n\tstruct pv_node *node;\n\n\tfor_each_hash_entry(he, offset, hash) {\n\t\tif (READ_ONCE(he->lock) == lock) {\n\t\t\tnode = READ_ONCE(he->node);\n\t\t\tWRITE_ONCE(he->lock, NULL);\n\t\t\treturn node;\n\t\t}\n\t}\n\t \n\tBUG();\n}\n\n \nstatic inline bool\npv_wait_early(struct pv_node *prev, int loop)\n{\n\tif ((loop & PV_PREV_CHECK_MASK) != 0)\n\t\treturn false;\n\n\treturn READ_ONCE(prev->state) != vcpu_running;\n}\n\n \nstatic void pv_init_node(struct mcs_spinlock *node)\n{\n\tstruct pv_node *pn = (struct pv_node *)node;\n\n\tBUILD_BUG_ON(sizeof(struct pv_node) > sizeof(struct qnode));\n\n\tpn->cpu = smp_processor_id();\n\tpn->state = vcpu_running;\n}\n\n \nstatic void pv_wait_node(struct mcs_spinlock *node, struct mcs_spinlock *prev)\n{\n\tstruct pv_node *pn = (struct pv_node *)node;\n\tstruct pv_node *pp = (struct pv_node *)prev;\n\tint loop;\n\tbool wait_early;\n\n\tfor (;;) {\n\t\tfor (wait_early = false, loop = SPIN_THRESHOLD; loop; loop--) {\n\t\t\tif (READ_ONCE(node->locked))\n\t\t\t\treturn;\n\t\t\tif (pv_wait_early(pp, loop)) {\n\t\t\t\twait_early = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcpu_relax();\n\t\t}\n\n\t\t \n\t\tsmp_store_mb(pn->state, vcpu_halted);\n\n\t\tif (!READ_ONCE(node->locked)) {\n\t\t\tlockevent_inc(pv_wait_node);\n\t\t\tlockevent_cond_inc(pv_wait_early, wait_early);\n\t\t\tpv_wait(&pn->state, vcpu_halted);\n\t\t}\n\n\t\t \n\t\tcmpxchg(&pn->state, vcpu_halted, vcpu_running);\n\n\t\t \n\t\tlockevent_cond_inc(pv_spurious_wakeup,\n\t\t\t\t  !READ_ONCE(node->locked));\n\t}\n\n\t \n}\n\n \nstatic void pv_kick_node(struct qspinlock *lock, struct mcs_spinlock *node)\n{\n\tstruct pv_node *pn = (struct pv_node *)node;\n\n\t \n\tsmp_mb__before_atomic();\n\tif (cmpxchg_relaxed(&pn->state, vcpu_halted, vcpu_hashed)\n\t    != vcpu_halted)\n\t\treturn;\n\n\t \n\tWRITE_ONCE(lock->locked, _Q_SLOW_VAL);\n\t(void)pv_hash(lock, pn);\n}\n\n \nstatic u32\npv_wait_head_or_lock(struct qspinlock *lock, struct mcs_spinlock *node)\n{\n\tstruct pv_node *pn = (struct pv_node *)node;\n\tstruct qspinlock **lp = NULL;\n\tint waitcnt = 0;\n\tint loop;\n\n\t \n\tif (READ_ONCE(pn->state) == vcpu_hashed)\n\t\tlp = (struct qspinlock **)1;\n\n\t \n\tlockevent_inc(lock_slowpath);\n\n\tfor (;; waitcnt++) {\n\t\t \n\t\tWRITE_ONCE(pn->state, vcpu_running);\n\n\t\t \n\t\tset_pending(lock);\n\t\tfor (loop = SPIN_THRESHOLD; loop; loop--) {\n\t\t\tif (trylock_clear_pending(lock))\n\t\t\t\tgoto gotlock;\n\t\t\tcpu_relax();\n\t\t}\n\t\tclear_pending(lock);\n\n\n\t\tif (!lp) {  \n\t\t\tlp = pv_hash(lock, pn);\n\n\t\t\t \n\t\t\tif (xchg(&lock->locked, _Q_SLOW_VAL) == 0) {\n\t\t\t\t \n\t\t\t\tWRITE_ONCE(lock->locked, _Q_LOCKED_VAL);\n\t\t\t\tWRITE_ONCE(*lp, NULL);\n\t\t\t\tgoto gotlock;\n\t\t\t}\n\t\t}\n\t\tWRITE_ONCE(pn->state, vcpu_hashed);\n\t\tlockevent_inc(pv_wait_head);\n\t\tlockevent_cond_inc(pv_wait_again, waitcnt);\n\t\tpv_wait(&lock->locked, _Q_SLOW_VAL);\n\n\t\t \n\t}\n\n\t \ngotlock:\n\treturn (u32)(atomic_read(&lock->val) | _Q_LOCKED_VAL);\n}\n\n \n#include <asm/qspinlock_paravirt.h>\n\n \n__visible __lockfunc void\n__pv_queued_spin_unlock_slowpath(struct qspinlock *lock, u8 locked)\n{\n\tstruct pv_node *node;\n\n\tif (unlikely(locked != _Q_SLOW_VAL)) {\n\t\tWARN(!debug_locks_silent,\n\t\t     \"pvqspinlock: lock 0x%lx has corrupted value 0x%x!\\n\",\n\t\t     (unsigned long)lock, atomic_read(&lock->val));\n\t\treturn;\n\t}\n\n\t \n\tsmp_rmb();\n\n\t \n\tnode = pv_unhash(lock);\n\n\t \n\tsmp_store_release(&lock->locked, 0);\n\n\t \n\tlockevent_inc(pv_kick_unlock);\n\tpv_kick(node->cpu);\n}\n\n#ifndef __pv_queued_spin_unlock\n__visible __lockfunc void __pv_queued_spin_unlock(struct qspinlock *lock)\n{\n\tu8 locked;\n\n\t \n\tlocked = cmpxchg_release(&lock->locked, _Q_LOCKED_VAL, 0);\n\tif (likely(locked == _Q_LOCKED_VAL))\n\t\treturn;\n\n\t__pv_queued_spin_unlock_slowpath(lock, locked);\n}\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}