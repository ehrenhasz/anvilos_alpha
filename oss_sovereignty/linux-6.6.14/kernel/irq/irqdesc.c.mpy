{
  "module_name": "irqdesc.c",
  "hash_id": "5417cd1bc12f51b5e6e12392be3e546b7daa7065dea29838f76c2633ec18811d",
  "original_prompt": "Ingested from linux-6.6.14/kernel/irq/irqdesc.c",
  "human_readable_source": "\n \n#include <linux/irq.h>\n#include <linux/slab.h>\n#include <linux/export.h>\n#include <linux/interrupt.h>\n#include <linux/kernel_stat.h>\n#include <linux/maple_tree.h>\n#include <linux/irqdomain.h>\n#include <linux/sysfs.h>\n\n#include \"internals.h\"\n\n \nstatic struct lock_class_key irq_desc_lock_class;\n\n#if defined(CONFIG_SMP)\nstatic int __init irq_affinity_setup(char *str)\n{\n\talloc_bootmem_cpumask_var(&irq_default_affinity);\n\tcpulist_parse(str, irq_default_affinity);\n\t \n\tcpumask_set_cpu(smp_processor_id(), irq_default_affinity);\n\treturn 1;\n}\n__setup(\"irqaffinity=\", irq_affinity_setup);\n\nstatic void __init init_irq_default_affinity(void)\n{\n\tif (!cpumask_available(irq_default_affinity))\n\t\tzalloc_cpumask_var(&irq_default_affinity, GFP_NOWAIT);\n\tif (cpumask_empty(irq_default_affinity))\n\t\tcpumask_setall(irq_default_affinity);\n}\n#else\nstatic void __init init_irq_default_affinity(void)\n{\n}\n#endif\n\n#ifdef CONFIG_SMP\nstatic int alloc_masks(struct irq_desc *desc, int node)\n{\n\tif (!zalloc_cpumask_var_node(&desc->irq_common_data.affinity,\n\t\t\t\t     GFP_KERNEL, node))\n\t\treturn -ENOMEM;\n\n#ifdef CONFIG_GENERIC_IRQ_EFFECTIVE_AFF_MASK\n\tif (!zalloc_cpumask_var_node(&desc->irq_common_data.effective_affinity,\n\t\t\t\t     GFP_KERNEL, node)) {\n\t\tfree_cpumask_var(desc->irq_common_data.affinity);\n\t\treturn -ENOMEM;\n\t}\n#endif\n\n#ifdef CONFIG_GENERIC_PENDING_IRQ\n\tif (!zalloc_cpumask_var_node(&desc->pending_mask, GFP_KERNEL, node)) {\n#ifdef CONFIG_GENERIC_IRQ_EFFECTIVE_AFF_MASK\n\t\tfree_cpumask_var(desc->irq_common_data.effective_affinity);\n#endif\n\t\tfree_cpumask_var(desc->irq_common_data.affinity);\n\t\treturn -ENOMEM;\n\t}\n#endif\n\treturn 0;\n}\n\nstatic void desc_smp_init(struct irq_desc *desc, int node,\n\t\t\t  const struct cpumask *affinity)\n{\n\tif (!affinity)\n\t\taffinity = irq_default_affinity;\n\tcpumask_copy(desc->irq_common_data.affinity, affinity);\n\n#ifdef CONFIG_GENERIC_PENDING_IRQ\n\tcpumask_clear(desc->pending_mask);\n#endif\n#ifdef CONFIG_NUMA\n\tdesc->irq_common_data.node = node;\n#endif\n}\n\n#else\nstatic inline int\nalloc_masks(struct irq_desc *desc, int node) { return 0; }\nstatic inline void\ndesc_smp_init(struct irq_desc *desc, int node, const struct cpumask *affinity) { }\n#endif\n\nstatic void desc_set_defaults(unsigned int irq, struct irq_desc *desc, int node,\n\t\t\t      const struct cpumask *affinity, struct module *owner)\n{\n\tint cpu;\n\n\tdesc->irq_common_data.handler_data = NULL;\n\tdesc->irq_common_data.msi_desc = NULL;\n\n\tdesc->irq_data.common = &desc->irq_common_data;\n\tdesc->irq_data.irq = irq;\n\tdesc->irq_data.chip = &no_irq_chip;\n\tdesc->irq_data.chip_data = NULL;\n\tirq_settings_clr_and_set(desc, ~0, _IRQ_DEFAULT_INIT_FLAGS);\n\tirqd_set(&desc->irq_data, IRQD_IRQ_DISABLED);\n\tirqd_set(&desc->irq_data, IRQD_IRQ_MASKED);\n\tdesc->handle_irq = handle_bad_irq;\n\tdesc->depth = 1;\n\tdesc->irq_count = 0;\n\tdesc->irqs_unhandled = 0;\n\tdesc->tot_count = 0;\n\tdesc->name = NULL;\n\tdesc->owner = owner;\n\tfor_each_possible_cpu(cpu)\n\t\t*per_cpu_ptr(desc->kstat_irqs, cpu) = 0;\n\tdesc_smp_init(desc, node, affinity);\n}\n\nint nr_irqs = NR_IRQS;\nEXPORT_SYMBOL_GPL(nr_irqs);\n\nstatic DEFINE_MUTEX(sparse_irq_lock);\nstatic struct maple_tree sparse_irqs = MTREE_INIT_EXT(sparse_irqs,\n\t\t\t\t\tMT_FLAGS_ALLOC_RANGE |\n\t\t\t\t\tMT_FLAGS_LOCK_EXTERN |\n\t\t\t\t\tMT_FLAGS_USE_RCU,\n\t\t\t\t\tsparse_irq_lock);\n\nstatic int irq_find_free_area(unsigned int from, unsigned int cnt)\n{\n\tMA_STATE(mas, &sparse_irqs, 0, 0);\n\n\tif (mas_empty_area(&mas, from, MAX_SPARSE_IRQS, cnt))\n\t\treturn -ENOSPC;\n\treturn mas.index;\n}\n\nstatic unsigned int irq_find_at_or_after(unsigned int offset)\n{\n\tunsigned long index = offset;\n\tstruct irq_desc *desc = mt_find(&sparse_irqs, &index, nr_irqs);\n\n\treturn desc ? irq_desc_get_irq(desc) : nr_irqs;\n}\n\nstatic void irq_insert_desc(unsigned int irq, struct irq_desc *desc)\n{\n\tMA_STATE(mas, &sparse_irqs, irq, irq);\n\tWARN_ON(mas_store_gfp(&mas, desc, GFP_KERNEL) != 0);\n}\n\nstatic void delete_irq_desc(unsigned int irq)\n{\n\tMA_STATE(mas, &sparse_irqs, irq, irq);\n\tmas_erase(&mas);\n}\n\n#ifdef CONFIG_SPARSE_IRQ\n\nstatic void irq_kobj_release(struct kobject *kobj);\n\n#ifdef CONFIG_SYSFS\nstatic struct kobject *irq_kobj_base;\n\n#define IRQ_ATTR_RO(_name) \\\nstatic struct kobj_attribute _name##_attr = __ATTR_RO(_name)\n\nstatic ssize_t per_cpu_count_show(struct kobject *kobj,\n\t\t\t\t  struct kobj_attribute *attr, char *buf)\n{\n\tstruct irq_desc *desc = container_of(kobj, struct irq_desc, kobj);\n\tssize_t ret = 0;\n\tchar *p = \"\";\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tunsigned int c = irq_desc_kstat_cpu(desc, cpu);\n\n\t\tret += scnprintf(buf + ret, PAGE_SIZE - ret, \"%s%u\", p, c);\n\t\tp = \",\";\n\t}\n\n\tret += scnprintf(buf + ret, PAGE_SIZE - ret, \"\\n\");\n\treturn ret;\n}\nIRQ_ATTR_RO(per_cpu_count);\n\nstatic ssize_t chip_name_show(struct kobject *kobj,\n\t\t\t      struct kobj_attribute *attr, char *buf)\n{\n\tstruct irq_desc *desc = container_of(kobj, struct irq_desc, kobj);\n\tssize_t ret = 0;\n\n\traw_spin_lock_irq(&desc->lock);\n\tif (desc->irq_data.chip && desc->irq_data.chip->name) {\n\t\tret = scnprintf(buf, PAGE_SIZE, \"%s\\n\",\n\t\t\t\tdesc->irq_data.chip->name);\n\t}\n\traw_spin_unlock_irq(&desc->lock);\n\n\treturn ret;\n}\nIRQ_ATTR_RO(chip_name);\n\nstatic ssize_t hwirq_show(struct kobject *kobj,\n\t\t\t  struct kobj_attribute *attr, char *buf)\n{\n\tstruct irq_desc *desc = container_of(kobj, struct irq_desc, kobj);\n\tssize_t ret = 0;\n\n\traw_spin_lock_irq(&desc->lock);\n\tif (desc->irq_data.domain)\n\t\tret = sprintf(buf, \"%lu\\n\", desc->irq_data.hwirq);\n\traw_spin_unlock_irq(&desc->lock);\n\n\treturn ret;\n}\nIRQ_ATTR_RO(hwirq);\n\nstatic ssize_t type_show(struct kobject *kobj,\n\t\t\t struct kobj_attribute *attr, char *buf)\n{\n\tstruct irq_desc *desc = container_of(kobj, struct irq_desc, kobj);\n\tssize_t ret = 0;\n\n\traw_spin_lock_irq(&desc->lock);\n\tret = sprintf(buf, \"%s\\n\",\n\t\t      irqd_is_level_type(&desc->irq_data) ? \"level\" : \"edge\");\n\traw_spin_unlock_irq(&desc->lock);\n\n\treturn ret;\n\n}\nIRQ_ATTR_RO(type);\n\nstatic ssize_t wakeup_show(struct kobject *kobj,\n\t\t\t   struct kobj_attribute *attr, char *buf)\n{\n\tstruct irq_desc *desc = container_of(kobj, struct irq_desc, kobj);\n\tssize_t ret = 0;\n\n\traw_spin_lock_irq(&desc->lock);\n\tret = sprintf(buf, \"%s\\n\",\n\t\t      irqd_is_wakeup_set(&desc->irq_data) ? \"enabled\" : \"disabled\");\n\traw_spin_unlock_irq(&desc->lock);\n\n\treturn ret;\n\n}\nIRQ_ATTR_RO(wakeup);\n\nstatic ssize_t name_show(struct kobject *kobj,\n\t\t\t struct kobj_attribute *attr, char *buf)\n{\n\tstruct irq_desc *desc = container_of(kobj, struct irq_desc, kobj);\n\tssize_t ret = 0;\n\n\traw_spin_lock_irq(&desc->lock);\n\tif (desc->name)\n\t\tret = scnprintf(buf, PAGE_SIZE, \"%s\\n\", desc->name);\n\traw_spin_unlock_irq(&desc->lock);\n\n\treturn ret;\n}\nIRQ_ATTR_RO(name);\n\nstatic ssize_t actions_show(struct kobject *kobj,\n\t\t\t    struct kobj_attribute *attr, char *buf)\n{\n\tstruct irq_desc *desc = container_of(kobj, struct irq_desc, kobj);\n\tstruct irqaction *action;\n\tssize_t ret = 0;\n\tchar *p = \"\";\n\n\traw_spin_lock_irq(&desc->lock);\n\tfor_each_action_of_desc(desc, action) {\n\t\tret += scnprintf(buf + ret, PAGE_SIZE - ret, \"%s%s\",\n\t\t\t\t p, action->name);\n\t\tp = \",\";\n\t}\n\traw_spin_unlock_irq(&desc->lock);\n\n\tif (ret)\n\t\tret += scnprintf(buf + ret, PAGE_SIZE - ret, \"\\n\");\n\n\treturn ret;\n}\nIRQ_ATTR_RO(actions);\n\nstatic struct attribute *irq_attrs[] = {\n\t&per_cpu_count_attr.attr,\n\t&chip_name_attr.attr,\n\t&hwirq_attr.attr,\n\t&type_attr.attr,\n\t&wakeup_attr.attr,\n\t&name_attr.attr,\n\t&actions_attr.attr,\n\tNULL\n};\nATTRIBUTE_GROUPS(irq);\n\nstatic const struct kobj_type irq_kobj_type = {\n\t.release\t= irq_kobj_release,\n\t.sysfs_ops\t= &kobj_sysfs_ops,\n\t.default_groups = irq_groups,\n};\n\nstatic void irq_sysfs_add(int irq, struct irq_desc *desc)\n{\n\tif (irq_kobj_base) {\n\t\t \n\t\tif (kobject_add(&desc->kobj, irq_kobj_base, \"%d\", irq))\n\t\t\tpr_warn(\"Failed to add kobject for irq %d\\n\", irq);\n\t\telse\n\t\t\tdesc->istate |= IRQS_SYSFS;\n\t}\n}\n\nstatic void irq_sysfs_del(struct irq_desc *desc)\n{\n\t \n\tif (desc->istate & IRQS_SYSFS)\n\t\tkobject_del(&desc->kobj);\n}\n\nstatic int __init irq_sysfs_init(void)\n{\n\tstruct irq_desc *desc;\n\tint irq;\n\n\t \n\tirq_lock_sparse();\n\n\tirq_kobj_base = kobject_create_and_add(\"irq\", kernel_kobj);\n\tif (!irq_kobj_base) {\n\t\tirq_unlock_sparse();\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tfor_each_irq_desc(irq, desc)\n\t\tirq_sysfs_add(irq, desc);\n\tirq_unlock_sparse();\n\n\treturn 0;\n}\npostcore_initcall(irq_sysfs_init);\n\n#else  \n\nstatic const struct kobj_type irq_kobj_type = {\n\t.release\t= irq_kobj_release,\n};\n\nstatic void irq_sysfs_add(int irq, struct irq_desc *desc) {}\nstatic void irq_sysfs_del(struct irq_desc *desc) {}\n\n#endif  \n\nstruct irq_desc *irq_to_desc(unsigned int irq)\n{\n\treturn mtree_load(&sparse_irqs, irq);\n}\n#ifdef CONFIG_KVM_BOOK3S_64_HV_MODULE\nEXPORT_SYMBOL_GPL(irq_to_desc);\n#endif\n\n#ifdef CONFIG_SMP\nstatic void free_masks(struct irq_desc *desc)\n{\n#ifdef CONFIG_GENERIC_PENDING_IRQ\n\tfree_cpumask_var(desc->pending_mask);\n#endif\n\tfree_cpumask_var(desc->irq_common_data.affinity);\n#ifdef CONFIG_GENERIC_IRQ_EFFECTIVE_AFF_MASK\n\tfree_cpumask_var(desc->irq_common_data.effective_affinity);\n#endif\n}\n#else\nstatic inline void free_masks(struct irq_desc *desc) { }\n#endif\n\nvoid irq_lock_sparse(void)\n{\n\tmutex_lock(&sparse_irq_lock);\n}\n\nvoid irq_unlock_sparse(void)\n{\n\tmutex_unlock(&sparse_irq_lock);\n}\n\nstatic struct irq_desc *alloc_desc(int irq, int node, unsigned int flags,\n\t\t\t\t   const struct cpumask *affinity,\n\t\t\t\t   struct module *owner)\n{\n\tstruct irq_desc *desc;\n\n\tdesc = kzalloc_node(sizeof(*desc), GFP_KERNEL, node);\n\tif (!desc)\n\t\treturn NULL;\n\t \n\tdesc->kstat_irqs = alloc_percpu(unsigned int);\n\tif (!desc->kstat_irqs)\n\t\tgoto err_desc;\n\n\tif (alloc_masks(desc, node))\n\t\tgoto err_kstat;\n\n\traw_spin_lock_init(&desc->lock);\n\tlockdep_set_class(&desc->lock, &irq_desc_lock_class);\n\tmutex_init(&desc->request_mutex);\n\tinit_rcu_head(&desc->rcu);\n\tinit_waitqueue_head(&desc->wait_for_threads);\n\n\tdesc_set_defaults(irq, desc, node, affinity, owner);\n\tirqd_set(&desc->irq_data, flags);\n\tkobject_init(&desc->kobj, &irq_kobj_type);\n\tirq_resend_init(desc);\n\n\treturn desc;\n\nerr_kstat:\n\tfree_percpu(desc->kstat_irqs);\nerr_desc:\n\tkfree(desc);\n\treturn NULL;\n}\n\nstatic void irq_kobj_release(struct kobject *kobj)\n{\n\tstruct irq_desc *desc = container_of(kobj, struct irq_desc, kobj);\n\n\tfree_masks(desc);\n\tfree_percpu(desc->kstat_irqs);\n\tkfree(desc);\n}\n\nstatic void delayed_free_desc(struct rcu_head *rhp)\n{\n\tstruct irq_desc *desc = container_of(rhp, struct irq_desc, rcu);\n\n\tkobject_put(&desc->kobj);\n}\n\nstatic void free_desc(unsigned int irq)\n{\n\tstruct irq_desc *desc = irq_to_desc(irq);\n\n\tirq_remove_debugfs_entry(desc);\n\tunregister_irq_proc(irq, desc);\n\n\t \n\tirq_sysfs_del(desc);\n\tdelete_irq_desc(irq);\n\n\t \n\tcall_rcu(&desc->rcu, delayed_free_desc);\n}\n\nstatic int alloc_descs(unsigned int start, unsigned int cnt, int node,\n\t\t       const struct irq_affinity_desc *affinity,\n\t\t       struct module *owner)\n{\n\tstruct irq_desc *desc;\n\tint i;\n\n\t \n\tif (affinity) {\n\t\tfor (i = 0; i < cnt; i++) {\n\t\t\tif (cpumask_empty(&affinity[i].mask))\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tfor (i = 0; i < cnt; i++) {\n\t\tconst struct cpumask *mask = NULL;\n\t\tunsigned int flags = 0;\n\n\t\tif (affinity) {\n\t\t\tif (affinity->is_managed) {\n\t\t\t\tflags = IRQD_AFFINITY_MANAGED |\n\t\t\t\t\tIRQD_MANAGED_SHUTDOWN;\n\t\t\t}\n\t\t\tmask = &affinity->mask;\n\t\t\tnode = cpu_to_node(cpumask_first(mask));\n\t\t\taffinity++;\n\t\t}\n\n\t\tdesc = alloc_desc(start + i, node, flags, mask, owner);\n\t\tif (!desc)\n\t\t\tgoto err;\n\t\tirq_insert_desc(start + i, desc);\n\t\tirq_sysfs_add(start + i, desc);\n\t\tirq_add_debugfs_entry(start + i, desc);\n\t}\n\treturn start;\n\nerr:\n\tfor (i--; i >= 0; i--)\n\t\tfree_desc(start + i);\n\treturn -ENOMEM;\n}\n\nstatic int irq_expand_nr_irqs(unsigned int nr)\n{\n\tif (nr > MAX_SPARSE_IRQS)\n\t\treturn -ENOMEM;\n\tnr_irqs = nr;\n\treturn 0;\n}\n\nint __init early_irq_init(void)\n{\n\tint i, initcnt, node = first_online_node;\n\tstruct irq_desc *desc;\n\n\tinit_irq_default_affinity();\n\n\t \n\tinitcnt = arch_probe_nr_irqs();\n\tprintk(KERN_INFO \"NR_IRQS: %d, nr_irqs: %d, preallocated irqs: %d\\n\",\n\t       NR_IRQS, nr_irqs, initcnt);\n\n\tif (WARN_ON(nr_irqs > MAX_SPARSE_IRQS))\n\t\tnr_irqs = MAX_SPARSE_IRQS;\n\n\tif (WARN_ON(initcnt > MAX_SPARSE_IRQS))\n\t\tinitcnt = MAX_SPARSE_IRQS;\n\n\tif (initcnt > nr_irqs)\n\t\tnr_irqs = initcnt;\n\n\tfor (i = 0; i < initcnt; i++) {\n\t\tdesc = alloc_desc(i, node, 0, NULL, NULL);\n\t\tirq_insert_desc(i, desc);\n\t}\n\treturn arch_early_irq_init();\n}\n\n#else  \n\nstruct irq_desc irq_desc[NR_IRQS] __cacheline_aligned_in_smp = {\n\t[0 ... NR_IRQS-1] = {\n\t\t.handle_irq\t= handle_bad_irq,\n\t\t.depth\t\t= 1,\n\t\t.lock\t\t= __RAW_SPIN_LOCK_UNLOCKED(irq_desc->lock),\n\t}\n};\n\nint __init early_irq_init(void)\n{\n\tint count, i, node = first_online_node;\n\tstruct irq_desc *desc;\n\n\tinit_irq_default_affinity();\n\n\tprintk(KERN_INFO \"NR_IRQS: %d\\n\", NR_IRQS);\n\n\tdesc = irq_desc;\n\tcount = ARRAY_SIZE(irq_desc);\n\n\tfor (i = 0; i < count; i++) {\n\t\tdesc[i].kstat_irqs = alloc_percpu(unsigned int);\n\t\talloc_masks(&desc[i], node);\n\t\traw_spin_lock_init(&desc[i].lock);\n\t\tlockdep_set_class(&desc[i].lock, &irq_desc_lock_class);\n\t\tmutex_init(&desc[i].request_mutex);\n\t\tinit_waitqueue_head(&desc[i].wait_for_threads);\n\t\tdesc_set_defaults(i, &desc[i], node, NULL, NULL);\n\t\tirq_resend_init(desc);\n\t}\n\treturn arch_early_irq_init();\n}\n\nstruct irq_desc *irq_to_desc(unsigned int irq)\n{\n\treturn (irq < NR_IRQS) ? irq_desc + irq : NULL;\n}\nEXPORT_SYMBOL(irq_to_desc);\n\nstatic void free_desc(unsigned int irq)\n{\n\tstruct irq_desc *desc = irq_to_desc(irq);\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&desc->lock, flags);\n\tdesc_set_defaults(irq, desc, irq_desc_get_node(desc), NULL, NULL);\n\traw_spin_unlock_irqrestore(&desc->lock, flags);\n\tdelete_irq_desc(irq);\n}\n\nstatic inline int alloc_descs(unsigned int start, unsigned int cnt, int node,\n\t\t\t      const struct irq_affinity_desc *affinity,\n\t\t\t      struct module *owner)\n{\n\tu32 i;\n\n\tfor (i = 0; i < cnt; i++) {\n\t\tstruct irq_desc *desc = irq_to_desc(start + i);\n\n\t\tdesc->owner = owner;\n\t\tirq_insert_desc(start + i, desc);\n\t}\n\treturn start;\n}\n\nstatic int irq_expand_nr_irqs(unsigned int nr)\n{\n\treturn -ENOMEM;\n}\n\nvoid irq_mark_irq(unsigned int irq)\n{\n\tmutex_lock(&sparse_irq_lock);\n\tirq_insert_desc(irq, irq_desc + irq);\n\tmutex_unlock(&sparse_irq_lock);\n}\n\n#ifdef CONFIG_GENERIC_IRQ_LEGACY\nvoid irq_init_desc(unsigned int irq)\n{\n\tfree_desc(irq);\n}\n#endif\n\n#endif  \n\nint handle_irq_desc(struct irq_desc *desc)\n{\n\tstruct irq_data *data;\n\n\tif (!desc)\n\t\treturn -EINVAL;\n\n\tdata = irq_desc_get_irq_data(desc);\n\tif (WARN_ON_ONCE(!in_hardirq() && handle_enforce_irqctx(data)))\n\t\treturn -EPERM;\n\n\tgeneric_handle_irq_desc(desc);\n\treturn 0;\n}\n\n \nint generic_handle_irq(unsigned int irq)\n{\n\treturn handle_irq_desc(irq_to_desc(irq));\n}\nEXPORT_SYMBOL_GPL(generic_handle_irq);\n\n \nint generic_handle_irq_safe(unsigned int irq)\n{\n\tunsigned long flags;\n\tint ret;\n\n\tlocal_irq_save(flags);\n\tret = handle_irq_desc(irq_to_desc(irq));\n\tlocal_irq_restore(flags);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(generic_handle_irq_safe);\n\n#ifdef CONFIG_IRQ_DOMAIN\n \nint generic_handle_domain_irq(struct irq_domain *domain, unsigned int hwirq)\n{\n\treturn handle_irq_desc(irq_resolve_mapping(domain, hwirq));\n}\nEXPORT_SYMBOL_GPL(generic_handle_domain_irq);\n\n  \nint generic_handle_domain_irq_safe(struct irq_domain *domain, unsigned int hwirq)\n{\n\tunsigned long flags;\n\tint ret;\n\n\tlocal_irq_save(flags);\n\tret = handle_irq_desc(irq_resolve_mapping(domain, hwirq));\n\tlocal_irq_restore(flags);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(generic_handle_domain_irq_safe);\n\n \nint generic_handle_domain_nmi(struct irq_domain *domain, unsigned int hwirq)\n{\n\tWARN_ON_ONCE(!in_nmi());\n\treturn handle_irq_desc(irq_resolve_mapping(domain, hwirq));\n}\n#endif\n\n \n\n \nvoid irq_free_descs(unsigned int from, unsigned int cnt)\n{\n\tint i;\n\n\tif (from >= nr_irqs || (from + cnt) > nr_irqs)\n\t\treturn;\n\n\tmutex_lock(&sparse_irq_lock);\n\tfor (i = 0; i < cnt; i++)\n\t\tfree_desc(from + i);\n\n\tmutex_unlock(&sparse_irq_lock);\n}\nEXPORT_SYMBOL_GPL(irq_free_descs);\n\n \nint __ref\n__irq_alloc_descs(int irq, unsigned int from, unsigned int cnt, int node,\n\t\t  struct module *owner, const struct irq_affinity_desc *affinity)\n{\n\tint start, ret;\n\n\tif (!cnt)\n\t\treturn -EINVAL;\n\n\tif (irq >= 0) {\n\t\tif (from > irq)\n\t\t\treturn -EINVAL;\n\t\tfrom = irq;\n\t} else {\n\t\t \n\t\tfrom = arch_dynirq_lower_bound(from);\n\t}\n\n\tmutex_lock(&sparse_irq_lock);\n\n\tstart = irq_find_free_area(from, cnt);\n\tret = -EEXIST;\n\tif (irq >=0 && start != irq)\n\t\tgoto unlock;\n\n\tif (start + cnt > nr_irqs) {\n\t\tret = irq_expand_nr_irqs(start + cnt);\n\t\tif (ret)\n\t\t\tgoto unlock;\n\t}\n\tret = alloc_descs(start, cnt, node, affinity, owner);\nunlock:\n\tmutex_unlock(&sparse_irq_lock);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(__irq_alloc_descs);\n\n \nunsigned int irq_get_next_irq(unsigned int offset)\n{\n\treturn irq_find_at_or_after(offset);\n}\n\nstruct irq_desc *\n__irq_get_desc_lock(unsigned int irq, unsigned long *flags, bool bus,\n\t\t    unsigned int check)\n{\n\tstruct irq_desc *desc = irq_to_desc(irq);\n\n\tif (desc) {\n\t\tif (check & _IRQ_DESC_CHECK) {\n\t\t\tif ((check & _IRQ_DESC_PERCPU) &&\n\t\t\t    !irq_settings_is_per_cpu_devid(desc))\n\t\t\t\treturn NULL;\n\n\t\t\tif (!(check & _IRQ_DESC_PERCPU) &&\n\t\t\t    irq_settings_is_per_cpu_devid(desc))\n\t\t\t\treturn NULL;\n\t\t}\n\n\t\tif (bus)\n\t\t\tchip_bus_lock(desc);\n\t\traw_spin_lock_irqsave(&desc->lock, *flags);\n\t}\n\treturn desc;\n}\n\nvoid __irq_put_desc_unlock(struct irq_desc *desc, unsigned long flags, bool bus)\n\t__releases(&desc->lock)\n{\n\traw_spin_unlock_irqrestore(&desc->lock, flags);\n\tif (bus)\n\t\tchip_bus_sync_unlock(desc);\n}\n\nint irq_set_percpu_devid_partition(unsigned int irq,\n\t\t\t\t   const struct cpumask *affinity)\n{\n\tstruct irq_desc *desc = irq_to_desc(irq);\n\n\tif (!desc)\n\t\treturn -EINVAL;\n\n\tif (desc->percpu_enabled)\n\t\treturn -EINVAL;\n\n\tdesc->percpu_enabled = kzalloc(sizeof(*desc->percpu_enabled), GFP_KERNEL);\n\n\tif (!desc->percpu_enabled)\n\t\treturn -ENOMEM;\n\n\tif (affinity)\n\t\tdesc->percpu_affinity = affinity;\n\telse\n\t\tdesc->percpu_affinity = cpu_possible_mask;\n\n\tirq_set_percpu_devid_flags(irq);\n\treturn 0;\n}\n\nint irq_set_percpu_devid(unsigned int irq)\n{\n\treturn irq_set_percpu_devid_partition(irq, NULL);\n}\n\nint irq_get_percpu_devid_partition(unsigned int irq, struct cpumask *affinity)\n{\n\tstruct irq_desc *desc = irq_to_desc(irq);\n\n\tif (!desc || !desc->percpu_enabled)\n\t\treturn -EINVAL;\n\n\tif (affinity)\n\t\tcpumask_copy(affinity, desc->percpu_affinity);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(irq_get_percpu_devid_partition);\n\nvoid kstat_incr_irq_this_cpu(unsigned int irq)\n{\n\tkstat_incr_irqs_this_cpu(irq_to_desc(irq));\n}\n\n \nunsigned int kstat_irqs_cpu(unsigned int irq, int cpu)\n{\n\tstruct irq_desc *desc = irq_to_desc(irq);\n\n\treturn desc && desc->kstat_irqs ?\n\t\t\t*per_cpu_ptr(desc->kstat_irqs, cpu) : 0;\n}\n\nstatic bool irq_is_nmi(struct irq_desc *desc)\n{\n\treturn desc->istate & IRQS_NMI;\n}\n\nstatic unsigned int kstat_irqs(unsigned int irq)\n{\n\tstruct irq_desc *desc = irq_to_desc(irq);\n\tunsigned int sum = 0;\n\tint cpu;\n\n\tif (!desc || !desc->kstat_irqs)\n\t\treturn 0;\n\tif (!irq_settings_is_per_cpu_devid(desc) &&\n\t    !irq_settings_is_per_cpu(desc) &&\n\t    !irq_is_nmi(desc))\n\t\treturn data_race(desc->tot_count);\n\n\tfor_each_possible_cpu(cpu)\n\t\tsum += data_race(*per_cpu_ptr(desc->kstat_irqs, cpu));\n\treturn sum;\n}\n\n \nunsigned int kstat_irqs_usr(unsigned int irq)\n{\n\tunsigned int sum;\n\n\trcu_read_lock();\n\tsum = kstat_irqs(irq);\n\trcu_read_unlock();\n\treturn sum;\n}\n\n#ifdef CONFIG_LOCKDEP\nvoid __irq_set_lockdep_class(unsigned int irq, struct lock_class_key *lock_class,\n\t\t\t     struct lock_class_key *request_class)\n{\n\tstruct irq_desc *desc = irq_to_desc(irq);\n\n\tif (desc) {\n\t\tlockdep_set_class(&desc->lock, lock_class);\n\t\tlockdep_set_class(&desc->request_mutex, request_class);\n\t}\n}\nEXPORT_SYMBOL_GPL(__irq_set_lockdep_class);\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}