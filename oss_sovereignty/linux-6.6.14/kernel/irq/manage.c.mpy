{
  "module_name": "manage.c",
  "hash_id": "efbbfb20db30ea95c9d3d4289d379554942601d5fcca6c0fdc6ed41c4edaa356",
  "original_prompt": "Ingested from linux-6.6.14/kernel/irq/manage.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) \"genirq: \" fmt\n\n#include <linux/irq.h>\n#include <linux/kthread.h>\n#include <linux/module.h>\n#include <linux/random.h>\n#include <linux/interrupt.h>\n#include <linux/irqdomain.h>\n#include <linux/slab.h>\n#include <linux/sched.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/task.h>\n#include <linux/sched/isolation.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/task_work.h>\n\n#include \"internals.h\"\n\n#if defined(CONFIG_IRQ_FORCED_THREADING) && !defined(CONFIG_PREEMPT_RT)\nDEFINE_STATIC_KEY_FALSE(force_irqthreads_key);\n\nstatic int __init setup_forced_irqthreads(char *arg)\n{\n\tstatic_branch_enable(&force_irqthreads_key);\n\treturn 0;\n}\nearly_param(\"threadirqs\", setup_forced_irqthreads);\n#endif\n\nstatic void __synchronize_hardirq(struct irq_desc *desc, bool sync_chip)\n{\n\tstruct irq_data *irqd = irq_desc_get_irq_data(desc);\n\tbool inprogress;\n\n\tdo {\n\t\tunsigned long flags;\n\n\t\t \n\t\twhile (irqd_irq_inprogress(&desc->irq_data))\n\t\t\tcpu_relax();\n\n\t\t \n\t\traw_spin_lock_irqsave(&desc->lock, flags);\n\t\tinprogress = irqd_irq_inprogress(&desc->irq_data);\n\n\t\t \n\t\tif (!inprogress && sync_chip) {\n\t\t\t \n\t\t\t__irq_get_irqchip_state(irqd, IRQCHIP_STATE_ACTIVE,\n\t\t\t\t\t\t&inprogress);\n\t\t}\n\t\traw_spin_unlock_irqrestore(&desc->lock, flags);\n\n\t\t \n\t} while (inprogress);\n}\n\n \nbool synchronize_hardirq(unsigned int irq)\n{\n\tstruct irq_desc *desc = irq_to_desc(irq);\n\n\tif (desc) {\n\t\t__synchronize_hardirq(desc, false);\n\t\treturn !atomic_read(&desc->threads_active);\n\t}\n\n\treturn true;\n}\nEXPORT_SYMBOL(synchronize_hardirq);\n\nstatic void __synchronize_irq(struct irq_desc *desc)\n{\n\t__synchronize_hardirq(desc, true);\n\t \n\twait_event(desc->wait_for_threads, !atomic_read(&desc->threads_active));\n}\n\n \nvoid synchronize_irq(unsigned int irq)\n{\n\tstruct irq_desc *desc = irq_to_desc(irq);\n\n\tif (desc)\n\t\t__synchronize_irq(desc);\n}\nEXPORT_SYMBOL(synchronize_irq);\n\n#ifdef CONFIG_SMP\ncpumask_var_t irq_default_affinity;\n\nstatic bool __irq_can_set_affinity(struct irq_desc *desc)\n{\n\tif (!desc || !irqd_can_balance(&desc->irq_data) ||\n\t    !desc->irq_data.chip || !desc->irq_data.chip->irq_set_affinity)\n\t\treturn false;\n\treturn true;\n}\n\n \nint irq_can_set_affinity(unsigned int irq)\n{\n\treturn __irq_can_set_affinity(irq_to_desc(irq));\n}\n\n \nbool irq_can_set_affinity_usr(unsigned int irq)\n{\n\tstruct irq_desc *desc = irq_to_desc(irq);\n\n\treturn __irq_can_set_affinity(desc) &&\n\t\t!irqd_affinity_is_managed(&desc->irq_data);\n}\n\n \nvoid irq_set_thread_affinity(struct irq_desc *desc)\n{\n\tstruct irqaction *action;\n\n\tfor_each_action_of_desc(desc, action) {\n\t\tif (action->thread)\n\t\t\tset_bit(IRQTF_AFFINITY, &action->thread_flags);\n\t\tif (action->secondary && action->secondary->thread)\n\t\t\tset_bit(IRQTF_AFFINITY, &action->secondary->thread_flags);\n\t}\n}\n\n#ifdef CONFIG_GENERIC_IRQ_EFFECTIVE_AFF_MASK\nstatic void irq_validate_effective_affinity(struct irq_data *data)\n{\n\tconst struct cpumask *m = irq_data_get_effective_affinity_mask(data);\n\tstruct irq_chip *chip = irq_data_get_irq_chip(data);\n\n\tif (!cpumask_empty(m))\n\t\treturn;\n\tpr_warn_once(\"irq_chip %s did not update eff. affinity mask of irq %u\\n\",\n\t\t     chip->name, data->irq);\n}\n#else\nstatic inline void irq_validate_effective_affinity(struct irq_data *data) { }\n#endif\n\nint irq_do_set_affinity(struct irq_data *data, const struct cpumask *mask,\n\t\t\tbool force)\n{\n\tstruct irq_desc *desc = irq_data_to_desc(data);\n\tstruct irq_chip *chip = irq_data_get_irq_chip(data);\n\tconst struct cpumask  *prog_mask;\n\tint ret;\n\n\tstatic DEFINE_RAW_SPINLOCK(tmp_mask_lock);\n\tstatic struct cpumask tmp_mask;\n\n\tif (!chip || !chip->irq_set_affinity)\n\t\treturn -EINVAL;\n\n\traw_spin_lock(&tmp_mask_lock);\n\t \n\tif (irqd_affinity_is_managed(data) &&\n\t    housekeeping_enabled(HK_TYPE_MANAGED_IRQ)) {\n\t\tconst struct cpumask *hk_mask;\n\n\t\thk_mask = housekeeping_cpumask(HK_TYPE_MANAGED_IRQ);\n\n\t\tcpumask_and(&tmp_mask, mask, hk_mask);\n\t\tif (!cpumask_intersects(&tmp_mask, cpu_online_mask))\n\t\t\tprog_mask = mask;\n\t\telse\n\t\t\tprog_mask = &tmp_mask;\n\t} else {\n\t\tprog_mask = mask;\n\t}\n\n\t \n\tcpumask_and(&tmp_mask, prog_mask, cpu_online_mask);\n\tif (!force && !cpumask_empty(&tmp_mask))\n\t\tret = chip->irq_set_affinity(data, &tmp_mask, force);\n\telse if (force)\n\t\tret = chip->irq_set_affinity(data, mask, force);\n\telse\n\t\tret = -EINVAL;\n\n\traw_spin_unlock(&tmp_mask_lock);\n\n\tswitch (ret) {\n\tcase IRQ_SET_MASK_OK:\n\tcase IRQ_SET_MASK_OK_DONE:\n\t\tcpumask_copy(desc->irq_common_data.affinity, mask);\n\t\tfallthrough;\n\tcase IRQ_SET_MASK_OK_NOCOPY:\n\t\tirq_validate_effective_affinity(data);\n\t\tirq_set_thread_affinity(desc);\n\t\tret = 0;\n\t}\n\n\treturn ret;\n}\n\n#ifdef CONFIG_GENERIC_PENDING_IRQ\nstatic inline int irq_set_affinity_pending(struct irq_data *data,\n\t\t\t\t\t   const struct cpumask *dest)\n{\n\tstruct irq_desc *desc = irq_data_to_desc(data);\n\n\tirqd_set_move_pending(data);\n\tirq_copy_pending(desc, dest);\n\treturn 0;\n}\n#else\nstatic inline int irq_set_affinity_pending(struct irq_data *data,\n\t\t\t\t\t   const struct cpumask *dest)\n{\n\treturn -EBUSY;\n}\n#endif\n\nstatic int irq_try_set_affinity(struct irq_data *data,\n\t\t\t\tconst struct cpumask *dest, bool force)\n{\n\tint ret = irq_do_set_affinity(data, dest, force);\n\n\t \n\tif (ret == -EBUSY && !force)\n\t\tret = irq_set_affinity_pending(data, dest);\n\treturn ret;\n}\n\nstatic bool irq_set_affinity_deactivated(struct irq_data *data,\n\t\t\t\t\t const struct cpumask *mask)\n{\n\tstruct irq_desc *desc = irq_data_to_desc(data);\n\n\t \n\tif (!IS_ENABLED(CONFIG_IRQ_DOMAIN_HIERARCHY) ||\n\t    irqd_is_activated(data) || !irqd_affinity_on_activate(data))\n\t\treturn false;\n\n\tcpumask_copy(desc->irq_common_data.affinity, mask);\n\tirq_data_update_effective_affinity(data, mask);\n\tirqd_set(data, IRQD_AFFINITY_SET);\n\treturn true;\n}\n\nint irq_set_affinity_locked(struct irq_data *data, const struct cpumask *mask,\n\t\t\t    bool force)\n{\n\tstruct irq_chip *chip = irq_data_get_irq_chip(data);\n\tstruct irq_desc *desc = irq_data_to_desc(data);\n\tint ret = 0;\n\n\tif (!chip || !chip->irq_set_affinity)\n\t\treturn -EINVAL;\n\n\tif (irq_set_affinity_deactivated(data, mask))\n\t\treturn 0;\n\n\tif (irq_can_move_pcntxt(data) && !irqd_is_setaffinity_pending(data)) {\n\t\tret = irq_try_set_affinity(data, mask, force);\n\t} else {\n\t\tirqd_set_move_pending(data);\n\t\tirq_copy_pending(desc, mask);\n\t}\n\n\tif (desc->affinity_notify) {\n\t\tkref_get(&desc->affinity_notify->kref);\n\t\tif (!schedule_work(&desc->affinity_notify->work)) {\n\t\t\t \n\t\t\tkref_put(&desc->affinity_notify->kref,\n\t\t\t\t desc->affinity_notify->release);\n\t\t}\n\t}\n\tirqd_set(data, IRQD_AFFINITY_SET);\n\n\treturn ret;\n}\n\n \nint irq_update_affinity_desc(unsigned int irq,\n\t\t\t     struct irq_affinity_desc *affinity)\n{\n\tstruct irq_desc *desc;\n\tunsigned long flags;\n\tbool activated;\n\tint ret = 0;\n\n\t \n\tif (IS_ENABLED(CONFIG_GENERIC_IRQ_RESERVATION_MODE))\n\t\treturn -EOPNOTSUPP;\n\n\tdesc = irq_get_desc_buslock(irq, &flags, 0);\n\tif (!desc)\n\t\treturn -EINVAL;\n\n\t \n\tif (irqd_is_started(&desc->irq_data)) {\n\t\tret = -EBUSY;\n\t\tgoto out_unlock;\n\t}\n\n\t \n\tif (irqd_affinity_is_managed(&desc->irq_data)) {\n\t\tret = -EBUSY;\n\t\tgoto out_unlock;\n\t}\n\n\t \n\tactivated = irqd_is_activated(&desc->irq_data);\n\tif (activated)\n\t\tirq_domain_deactivate_irq(&desc->irq_data);\n\n\tif (affinity->is_managed) {\n\t\tirqd_set(&desc->irq_data, IRQD_AFFINITY_MANAGED);\n\t\tirqd_set(&desc->irq_data, IRQD_MANAGED_SHUTDOWN);\n\t}\n\n\tcpumask_copy(desc->irq_common_data.affinity, &affinity->mask);\n\n\t \n\tif (activated)\n\t\tirq_domain_activate_irq(&desc->irq_data, false);\n\nout_unlock:\n\tirq_put_desc_busunlock(desc, flags);\n\treturn ret;\n}\n\nstatic int __irq_set_affinity(unsigned int irq, const struct cpumask *mask,\n\t\t\t      bool force)\n{\n\tstruct irq_desc *desc = irq_to_desc(irq);\n\tunsigned long flags;\n\tint ret;\n\n\tif (!desc)\n\t\treturn -EINVAL;\n\n\traw_spin_lock_irqsave(&desc->lock, flags);\n\tret = irq_set_affinity_locked(irq_desc_get_irq_data(desc), mask, force);\n\traw_spin_unlock_irqrestore(&desc->lock, flags);\n\treturn ret;\n}\n\n \nint irq_set_affinity(unsigned int irq, const struct cpumask *cpumask)\n{\n\treturn __irq_set_affinity(irq, cpumask, false);\n}\nEXPORT_SYMBOL_GPL(irq_set_affinity);\n\n \nint irq_force_affinity(unsigned int irq, const struct cpumask *cpumask)\n{\n\treturn __irq_set_affinity(irq, cpumask, true);\n}\nEXPORT_SYMBOL_GPL(irq_force_affinity);\n\nint __irq_apply_affinity_hint(unsigned int irq, const struct cpumask *m,\n\t\t\t      bool setaffinity)\n{\n\tunsigned long flags;\n\tstruct irq_desc *desc = irq_get_desc_lock(irq, &flags, IRQ_GET_DESC_CHECK_GLOBAL);\n\n\tif (!desc)\n\t\treturn -EINVAL;\n\tdesc->affinity_hint = m;\n\tirq_put_desc_unlock(desc, flags);\n\tif (m && setaffinity)\n\t\t__irq_set_affinity(irq, m, false);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(__irq_apply_affinity_hint);\n\nstatic void irq_affinity_notify(struct work_struct *work)\n{\n\tstruct irq_affinity_notify *notify =\n\t\tcontainer_of(work, struct irq_affinity_notify, work);\n\tstruct irq_desc *desc = irq_to_desc(notify->irq);\n\tcpumask_var_t cpumask;\n\tunsigned long flags;\n\n\tif (!desc || !alloc_cpumask_var(&cpumask, GFP_KERNEL))\n\t\tgoto out;\n\n\traw_spin_lock_irqsave(&desc->lock, flags);\n\tif (irq_move_pending(&desc->irq_data))\n\t\tirq_get_pending(cpumask, desc);\n\telse\n\t\tcpumask_copy(cpumask, desc->irq_common_data.affinity);\n\traw_spin_unlock_irqrestore(&desc->lock, flags);\n\n\tnotify->notify(notify, cpumask);\n\n\tfree_cpumask_var(cpumask);\nout:\n\tkref_put(&notify->kref, notify->release);\n}\n\n \nint\nirq_set_affinity_notifier(unsigned int irq, struct irq_affinity_notify *notify)\n{\n\tstruct irq_desc *desc = irq_to_desc(irq);\n\tstruct irq_affinity_notify *old_notify;\n\tunsigned long flags;\n\n\t \n\tmight_sleep();\n\n\tif (!desc || desc->istate & IRQS_NMI)\n\t\treturn -EINVAL;\n\n\t \n\tif (notify) {\n\t\tnotify->irq = irq;\n\t\tkref_init(&notify->kref);\n\t\tINIT_WORK(&notify->work, irq_affinity_notify);\n\t}\n\n\traw_spin_lock_irqsave(&desc->lock, flags);\n\told_notify = desc->affinity_notify;\n\tdesc->affinity_notify = notify;\n\traw_spin_unlock_irqrestore(&desc->lock, flags);\n\n\tif (old_notify) {\n\t\tif (cancel_work_sync(&old_notify->work)) {\n\t\t\t \n\t\t\tkref_put(&old_notify->kref, old_notify->release);\n\t\t}\n\t\tkref_put(&old_notify->kref, old_notify->release);\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(irq_set_affinity_notifier);\n\n#ifndef CONFIG_AUTO_IRQ_AFFINITY\n \nint irq_setup_affinity(struct irq_desc *desc)\n{\n\tstruct cpumask *set = irq_default_affinity;\n\tint ret, node = irq_desc_get_node(desc);\n\tstatic DEFINE_RAW_SPINLOCK(mask_lock);\n\tstatic struct cpumask mask;\n\n\t \n\tif (!__irq_can_set_affinity(desc))\n\t\treturn 0;\n\n\traw_spin_lock(&mask_lock);\n\t \n\tif (irqd_affinity_is_managed(&desc->irq_data) ||\n\t    irqd_has_set(&desc->irq_data, IRQD_AFFINITY_SET)) {\n\t\tif (cpumask_intersects(desc->irq_common_data.affinity,\n\t\t\t\t       cpu_online_mask))\n\t\t\tset = desc->irq_common_data.affinity;\n\t\telse\n\t\t\tirqd_clear(&desc->irq_data, IRQD_AFFINITY_SET);\n\t}\n\n\tcpumask_and(&mask, cpu_online_mask, set);\n\tif (cpumask_empty(&mask))\n\t\tcpumask_copy(&mask, cpu_online_mask);\n\n\tif (node != NUMA_NO_NODE) {\n\t\tconst struct cpumask *nodemask = cpumask_of_node(node);\n\n\t\t \n\t\tif (cpumask_intersects(&mask, nodemask))\n\t\t\tcpumask_and(&mask, &mask, nodemask);\n\t}\n\tret = irq_do_set_affinity(&desc->irq_data, &mask, false);\n\traw_spin_unlock(&mask_lock);\n\treturn ret;\n}\n#else\n \nint irq_setup_affinity(struct irq_desc *desc)\n{\n\treturn irq_select_affinity(irq_desc_get_irq(desc));\n}\n#endif  \n#endif  \n\n\n \nint irq_set_vcpu_affinity(unsigned int irq, void *vcpu_info)\n{\n\tunsigned long flags;\n\tstruct irq_desc *desc = irq_get_desc_lock(irq, &flags, 0);\n\tstruct irq_data *data;\n\tstruct irq_chip *chip;\n\tint ret = -ENOSYS;\n\n\tif (!desc)\n\t\treturn -EINVAL;\n\n\tdata = irq_desc_get_irq_data(desc);\n\tdo {\n\t\tchip = irq_data_get_irq_chip(data);\n\t\tif (chip && chip->irq_set_vcpu_affinity)\n\t\t\tbreak;\n#ifdef CONFIG_IRQ_DOMAIN_HIERARCHY\n\t\tdata = data->parent_data;\n#else\n\t\tdata = NULL;\n#endif\n\t} while (data);\n\n\tif (data)\n\t\tret = chip->irq_set_vcpu_affinity(data, vcpu_info);\n\tirq_put_desc_unlock(desc, flags);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(irq_set_vcpu_affinity);\n\nvoid __disable_irq(struct irq_desc *desc)\n{\n\tif (!desc->depth++)\n\t\tirq_disable(desc);\n}\n\nstatic int __disable_irq_nosync(unsigned int irq)\n{\n\tunsigned long flags;\n\tstruct irq_desc *desc = irq_get_desc_buslock(irq, &flags, IRQ_GET_DESC_CHECK_GLOBAL);\n\n\tif (!desc)\n\t\treturn -EINVAL;\n\t__disable_irq(desc);\n\tirq_put_desc_busunlock(desc, flags);\n\treturn 0;\n}\n\n \nvoid disable_irq_nosync(unsigned int irq)\n{\n\t__disable_irq_nosync(irq);\n}\nEXPORT_SYMBOL(disable_irq_nosync);\n\n \nvoid disable_irq(unsigned int irq)\n{\n\tmight_sleep();\n\tif (!__disable_irq_nosync(irq))\n\t\tsynchronize_irq(irq);\n}\nEXPORT_SYMBOL(disable_irq);\n\n \nbool disable_hardirq(unsigned int irq)\n{\n\tif (!__disable_irq_nosync(irq))\n\t\treturn synchronize_hardirq(irq);\n\n\treturn false;\n}\nEXPORT_SYMBOL_GPL(disable_hardirq);\n\n \nvoid disable_nmi_nosync(unsigned int irq)\n{\n\tdisable_irq_nosync(irq);\n}\n\nvoid __enable_irq(struct irq_desc *desc)\n{\n\tswitch (desc->depth) {\n\tcase 0:\n err_out:\n\t\tWARN(1, KERN_WARNING \"Unbalanced enable for IRQ %d\\n\",\n\t\t     irq_desc_get_irq(desc));\n\t\tbreak;\n\tcase 1: {\n\t\tif (desc->istate & IRQS_SUSPENDED)\n\t\t\tgoto err_out;\n\t\t \n\t\tirq_settings_set_noprobe(desc);\n\t\t \n\t\tirq_startup(desc, IRQ_RESEND, IRQ_START_FORCE);\n\t\tbreak;\n\t}\n\tdefault:\n\t\tdesc->depth--;\n\t}\n}\n\n \nvoid enable_irq(unsigned int irq)\n{\n\tunsigned long flags;\n\tstruct irq_desc *desc = irq_get_desc_buslock(irq, &flags, IRQ_GET_DESC_CHECK_GLOBAL);\n\n\tif (!desc)\n\t\treturn;\n\tif (WARN(!desc->irq_data.chip,\n\t\t KERN_ERR \"enable_irq before setup/request_irq: irq %u\\n\", irq))\n\t\tgoto out;\n\n\t__enable_irq(desc);\nout:\n\tirq_put_desc_busunlock(desc, flags);\n}\nEXPORT_SYMBOL(enable_irq);\n\n \nvoid enable_nmi(unsigned int irq)\n{\n\tenable_irq(irq);\n}\n\nstatic int set_irq_wake_real(unsigned int irq, unsigned int on)\n{\n\tstruct irq_desc *desc = irq_to_desc(irq);\n\tint ret = -ENXIO;\n\n\tif (irq_desc_get_chip(desc)->flags &  IRQCHIP_SKIP_SET_WAKE)\n\t\treturn 0;\n\n\tif (desc->irq_data.chip->irq_set_wake)\n\t\tret = desc->irq_data.chip->irq_set_wake(&desc->irq_data, on);\n\n\treturn ret;\n}\n\n \nint irq_set_irq_wake(unsigned int irq, unsigned int on)\n{\n\tunsigned long flags;\n\tstruct irq_desc *desc = irq_get_desc_buslock(irq, &flags, IRQ_GET_DESC_CHECK_GLOBAL);\n\tint ret = 0;\n\n\tif (!desc)\n\t\treturn -EINVAL;\n\n\t \n\tif (desc->istate & IRQS_NMI) {\n\t\tret = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\t \n\tif (on) {\n\t\tif (desc->wake_depth++ == 0) {\n\t\t\tret = set_irq_wake_real(irq, on);\n\t\t\tif (ret)\n\t\t\t\tdesc->wake_depth = 0;\n\t\t\telse\n\t\t\t\tirqd_set(&desc->irq_data, IRQD_WAKEUP_STATE);\n\t\t}\n\t} else {\n\t\tif (desc->wake_depth == 0) {\n\t\t\tWARN(1, \"Unbalanced IRQ %d wake disable\\n\", irq);\n\t\t} else if (--desc->wake_depth == 0) {\n\t\t\tret = set_irq_wake_real(irq, on);\n\t\t\tif (ret)\n\t\t\t\tdesc->wake_depth = 1;\n\t\t\telse\n\t\t\t\tirqd_clear(&desc->irq_data, IRQD_WAKEUP_STATE);\n\t\t}\n\t}\n\nout_unlock:\n\tirq_put_desc_busunlock(desc, flags);\n\treturn ret;\n}\nEXPORT_SYMBOL(irq_set_irq_wake);\n\n \nint can_request_irq(unsigned int irq, unsigned long irqflags)\n{\n\tunsigned long flags;\n\tstruct irq_desc *desc = irq_get_desc_lock(irq, &flags, 0);\n\tint canrequest = 0;\n\n\tif (!desc)\n\t\treturn 0;\n\n\tif (irq_settings_can_request(desc)) {\n\t\tif (!desc->action ||\n\t\t    irqflags & desc->action->flags & IRQF_SHARED)\n\t\t\tcanrequest = 1;\n\t}\n\tirq_put_desc_unlock(desc, flags);\n\treturn canrequest;\n}\n\nint __irq_set_trigger(struct irq_desc *desc, unsigned long flags)\n{\n\tstruct irq_chip *chip = desc->irq_data.chip;\n\tint ret, unmask = 0;\n\n\tif (!chip || !chip->irq_set_type) {\n\t\t \n\t\tpr_debug(\"No set_type function for IRQ %d (%s)\\n\",\n\t\t\t irq_desc_get_irq(desc),\n\t\t\t chip ? (chip->name ? : \"unknown\") : \"unknown\");\n\t\treturn 0;\n\t}\n\n\tif (chip->flags & IRQCHIP_SET_TYPE_MASKED) {\n\t\tif (!irqd_irq_masked(&desc->irq_data))\n\t\t\tmask_irq(desc);\n\t\tif (!irqd_irq_disabled(&desc->irq_data))\n\t\t\tunmask = 1;\n\t}\n\n\t \n\tflags &= IRQ_TYPE_SENSE_MASK;\n\tret = chip->irq_set_type(&desc->irq_data, flags);\n\n\tswitch (ret) {\n\tcase IRQ_SET_MASK_OK:\n\tcase IRQ_SET_MASK_OK_DONE:\n\t\tirqd_clear(&desc->irq_data, IRQD_TRIGGER_MASK);\n\t\tirqd_set(&desc->irq_data, flags);\n\t\tfallthrough;\n\n\tcase IRQ_SET_MASK_OK_NOCOPY:\n\t\tflags = irqd_get_trigger_type(&desc->irq_data);\n\t\tirq_settings_set_trigger_mask(desc, flags);\n\t\tirqd_clear(&desc->irq_data, IRQD_LEVEL);\n\t\tirq_settings_clr_level(desc);\n\t\tif (flags & IRQ_TYPE_LEVEL_MASK) {\n\t\t\tirq_settings_set_level(desc);\n\t\t\tirqd_set(&desc->irq_data, IRQD_LEVEL);\n\t\t}\n\n\t\tret = 0;\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"Setting trigger mode %lu for irq %u failed (%pS)\\n\",\n\t\t       flags, irq_desc_get_irq(desc), chip->irq_set_type);\n\t}\n\tif (unmask)\n\t\tunmask_irq(desc);\n\treturn ret;\n}\n\n#ifdef CONFIG_HARDIRQS_SW_RESEND\nint irq_set_parent(int irq, int parent_irq)\n{\n\tunsigned long flags;\n\tstruct irq_desc *desc = irq_get_desc_lock(irq, &flags, 0);\n\n\tif (!desc)\n\t\treturn -EINVAL;\n\n\tdesc->parent_irq = parent_irq;\n\n\tirq_put_desc_unlock(desc, flags);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(irq_set_parent);\n#endif\n\n \nstatic irqreturn_t irq_default_primary_handler(int irq, void *dev_id)\n{\n\treturn IRQ_WAKE_THREAD;\n}\n\n \nstatic irqreturn_t irq_nested_primary_handler(int irq, void *dev_id)\n{\n\tWARN(1, \"Primary handler called for nested irq %d\\n\", irq);\n\treturn IRQ_NONE;\n}\n\nstatic irqreturn_t irq_forced_secondary_handler(int irq, void *dev_id)\n{\n\tWARN(1, \"Secondary action handler called for irq %d\\n\", irq);\n\treturn IRQ_NONE;\n}\n\nstatic int irq_wait_for_interrupt(struct irqaction *action)\n{\n\tfor (;;) {\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\n\t\tif (kthread_should_stop()) {\n\t\t\t \n\t\t\tif (test_and_clear_bit(IRQTF_RUNTHREAD,\n\t\t\t\t\t       &action->thread_flags)) {\n\t\t\t\t__set_current_state(TASK_RUNNING);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t\t__set_current_state(TASK_RUNNING);\n\t\t\treturn -1;\n\t\t}\n\n\t\tif (test_and_clear_bit(IRQTF_RUNTHREAD,\n\t\t\t\t       &action->thread_flags)) {\n\t\t\t__set_current_state(TASK_RUNNING);\n\t\t\treturn 0;\n\t\t}\n\t\tschedule();\n\t}\n}\n\n \nstatic void irq_finalize_oneshot(struct irq_desc *desc,\n\t\t\t\t struct irqaction *action)\n{\n\tif (!(desc->istate & IRQS_ONESHOT) ||\n\t    action->handler == irq_forced_secondary_handler)\n\t\treturn;\nagain:\n\tchip_bus_lock(desc);\n\traw_spin_lock_irq(&desc->lock);\n\n\t \n\tif (unlikely(irqd_irq_inprogress(&desc->irq_data))) {\n\t\traw_spin_unlock_irq(&desc->lock);\n\t\tchip_bus_sync_unlock(desc);\n\t\tcpu_relax();\n\t\tgoto again;\n\t}\n\n\t \n\tif (test_bit(IRQTF_RUNTHREAD, &action->thread_flags))\n\t\tgoto out_unlock;\n\n\tdesc->threads_oneshot &= ~action->thread_mask;\n\n\tif (!desc->threads_oneshot && !irqd_irq_disabled(&desc->irq_data) &&\n\t    irqd_irq_masked(&desc->irq_data))\n\t\tunmask_threaded_irq(desc);\n\nout_unlock:\n\traw_spin_unlock_irq(&desc->lock);\n\tchip_bus_sync_unlock(desc);\n}\n\n#ifdef CONFIG_SMP\n \nstatic void\nirq_thread_check_affinity(struct irq_desc *desc, struct irqaction *action)\n{\n\tcpumask_var_t mask;\n\tbool valid = true;\n\n\tif (!test_and_clear_bit(IRQTF_AFFINITY, &action->thread_flags))\n\t\treturn;\n\n\t \n\tif (!alloc_cpumask_var(&mask, GFP_KERNEL)) {\n\t\tset_bit(IRQTF_AFFINITY, &action->thread_flags);\n\t\treturn;\n\t}\n\n\traw_spin_lock_irq(&desc->lock);\n\t \n\tif (cpumask_available(desc->irq_common_data.affinity)) {\n\t\tconst struct cpumask *m;\n\n\t\tm = irq_data_get_effective_affinity_mask(&desc->irq_data);\n\t\tcpumask_copy(mask, m);\n\t} else {\n\t\tvalid = false;\n\t}\n\traw_spin_unlock_irq(&desc->lock);\n\n\tif (valid)\n\t\tset_cpus_allowed_ptr(current, mask);\n\tfree_cpumask_var(mask);\n}\n#else\nstatic inline void\nirq_thread_check_affinity(struct irq_desc *desc, struct irqaction *action) { }\n#endif\n\n \nstatic irqreturn_t\nirq_forced_thread_fn(struct irq_desc *desc, struct irqaction *action)\n{\n\tirqreturn_t ret;\n\n\tlocal_bh_disable();\n\tif (!IS_ENABLED(CONFIG_PREEMPT_RT))\n\t\tlocal_irq_disable();\n\tret = action->thread_fn(action->irq, action->dev_id);\n\tif (ret == IRQ_HANDLED)\n\t\tatomic_inc(&desc->threads_handled);\n\n\tirq_finalize_oneshot(desc, action);\n\tif (!IS_ENABLED(CONFIG_PREEMPT_RT))\n\t\tlocal_irq_enable();\n\tlocal_bh_enable();\n\treturn ret;\n}\n\n \nstatic irqreturn_t irq_thread_fn(struct irq_desc *desc,\n\t\tstruct irqaction *action)\n{\n\tirqreturn_t ret;\n\n\tret = action->thread_fn(action->irq, action->dev_id);\n\tif (ret == IRQ_HANDLED)\n\t\tatomic_inc(&desc->threads_handled);\n\n\tirq_finalize_oneshot(desc, action);\n\treturn ret;\n}\n\nvoid wake_threads_waitq(struct irq_desc *desc)\n{\n\tif (atomic_dec_and_test(&desc->threads_active))\n\t\twake_up(&desc->wait_for_threads);\n}\n\nstatic void irq_thread_dtor(struct callback_head *unused)\n{\n\tstruct task_struct *tsk = current;\n\tstruct irq_desc *desc;\n\tstruct irqaction *action;\n\n\tif (WARN_ON_ONCE(!(current->flags & PF_EXITING)))\n\t\treturn;\n\n\taction = kthread_data(tsk);\n\n\tpr_err(\"exiting task \\\"%s\\\" (%d) is an active IRQ thread (irq %d)\\n\",\n\t       tsk->comm, tsk->pid, action->irq);\n\n\n\tdesc = irq_to_desc(action->irq);\n\t \n\tif (test_and_clear_bit(IRQTF_RUNTHREAD, &action->thread_flags))\n\t\twake_threads_waitq(desc);\n\n\t \n\tirq_finalize_oneshot(desc, action);\n}\n\nstatic void irq_wake_secondary(struct irq_desc *desc, struct irqaction *action)\n{\n\tstruct irqaction *secondary = action->secondary;\n\n\tif (WARN_ON_ONCE(!secondary))\n\t\treturn;\n\n\traw_spin_lock_irq(&desc->lock);\n\t__irq_wake_thread(desc, secondary);\n\traw_spin_unlock_irq(&desc->lock);\n}\n\n \nstatic void irq_thread_set_ready(struct irq_desc *desc,\n\t\t\t\t struct irqaction *action)\n{\n\tset_bit(IRQTF_READY, &action->thread_flags);\n\twake_up(&desc->wait_for_threads);\n}\n\n \nstatic void wake_up_and_wait_for_irq_thread_ready(struct irq_desc *desc,\n\t\t\t\t\t\t  struct irqaction *action)\n{\n\tif (!action || !action->thread)\n\t\treturn;\n\n\twake_up_process(action->thread);\n\twait_event(desc->wait_for_threads,\n\t\t   test_bit(IRQTF_READY, &action->thread_flags));\n}\n\n \nstatic int irq_thread(void *data)\n{\n\tstruct callback_head on_exit_work;\n\tstruct irqaction *action = data;\n\tstruct irq_desc *desc = irq_to_desc(action->irq);\n\tirqreturn_t (*handler_fn)(struct irq_desc *desc,\n\t\t\tstruct irqaction *action);\n\n\tirq_thread_set_ready(desc, action);\n\n\tsched_set_fifo(current);\n\n\tif (force_irqthreads() && test_bit(IRQTF_FORCED_THREAD,\n\t\t\t\t\t   &action->thread_flags))\n\t\thandler_fn = irq_forced_thread_fn;\n\telse\n\t\thandler_fn = irq_thread_fn;\n\n\tinit_task_work(&on_exit_work, irq_thread_dtor);\n\ttask_work_add(current, &on_exit_work, TWA_NONE);\n\n\tirq_thread_check_affinity(desc, action);\n\n\twhile (!irq_wait_for_interrupt(action)) {\n\t\tirqreturn_t action_ret;\n\n\t\tirq_thread_check_affinity(desc, action);\n\n\t\taction_ret = handler_fn(desc, action);\n\t\tif (action_ret == IRQ_WAKE_THREAD)\n\t\t\tirq_wake_secondary(desc, action);\n\n\t\twake_threads_waitq(desc);\n\t}\n\n\t \n\ttask_work_cancel(current, irq_thread_dtor);\n\treturn 0;\n}\n\n \nvoid irq_wake_thread(unsigned int irq, void *dev_id)\n{\n\tstruct irq_desc *desc = irq_to_desc(irq);\n\tstruct irqaction *action;\n\tunsigned long flags;\n\n\tif (!desc || WARN_ON(irq_settings_is_per_cpu_devid(desc)))\n\t\treturn;\n\n\traw_spin_lock_irqsave(&desc->lock, flags);\n\tfor_each_action_of_desc(desc, action) {\n\t\tif (action->dev_id == dev_id) {\n\t\t\tif (action->thread)\n\t\t\t\t__irq_wake_thread(desc, action);\n\t\t\tbreak;\n\t\t}\n\t}\n\traw_spin_unlock_irqrestore(&desc->lock, flags);\n}\nEXPORT_SYMBOL_GPL(irq_wake_thread);\n\nstatic int irq_setup_forced_threading(struct irqaction *new)\n{\n\tif (!force_irqthreads())\n\t\treturn 0;\n\tif (new->flags & (IRQF_NO_THREAD | IRQF_PERCPU | IRQF_ONESHOT))\n\t\treturn 0;\n\n\t \n\tif (new->handler == irq_default_primary_handler)\n\t\treturn 0;\n\n\tnew->flags |= IRQF_ONESHOT;\n\n\t \n\tif (new->handler && new->thread_fn) {\n\t\t \n\t\tnew->secondary = kzalloc(sizeof(struct irqaction), GFP_KERNEL);\n\t\tif (!new->secondary)\n\t\t\treturn -ENOMEM;\n\t\tnew->secondary->handler = irq_forced_secondary_handler;\n\t\tnew->secondary->thread_fn = new->thread_fn;\n\t\tnew->secondary->dev_id = new->dev_id;\n\t\tnew->secondary->irq = new->irq;\n\t\tnew->secondary->name = new->name;\n\t}\n\t \n\tset_bit(IRQTF_FORCED_THREAD, &new->thread_flags);\n\tnew->thread_fn = new->handler;\n\tnew->handler = irq_default_primary_handler;\n\treturn 0;\n}\n\nstatic int irq_request_resources(struct irq_desc *desc)\n{\n\tstruct irq_data *d = &desc->irq_data;\n\tstruct irq_chip *c = d->chip;\n\n\treturn c->irq_request_resources ? c->irq_request_resources(d) : 0;\n}\n\nstatic void irq_release_resources(struct irq_desc *desc)\n{\n\tstruct irq_data *d = &desc->irq_data;\n\tstruct irq_chip *c = d->chip;\n\n\tif (c->irq_release_resources)\n\t\tc->irq_release_resources(d);\n}\n\nstatic bool irq_supports_nmi(struct irq_desc *desc)\n{\n\tstruct irq_data *d = irq_desc_get_irq_data(desc);\n\n#ifdef CONFIG_IRQ_DOMAIN_HIERARCHY\n\t \n\tif (d->parent_data)\n\t\treturn false;\n#endif\n\t \n\tif (d->chip->irq_bus_lock || d->chip->irq_bus_sync_unlock)\n\t\treturn false;\n\n\treturn d->chip->flags & IRQCHIP_SUPPORTS_NMI;\n}\n\nstatic int irq_nmi_setup(struct irq_desc *desc)\n{\n\tstruct irq_data *d = irq_desc_get_irq_data(desc);\n\tstruct irq_chip *c = d->chip;\n\n\treturn c->irq_nmi_setup ? c->irq_nmi_setup(d) : -EINVAL;\n}\n\nstatic void irq_nmi_teardown(struct irq_desc *desc)\n{\n\tstruct irq_data *d = irq_desc_get_irq_data(desc);\n\tstruct irq_chip *c = d->chip;\n\n\tif (c->irq_nmi_teardown)\n\t\tc->irq_nmi_teardown(d);\n}\n\nstatic int\nsetup_irq_thread(struct irqaction *new, unsigned int irq, bool secondary)\n{\n\tstruct task_struct *t;\n\n\tif (!secondary) {\n\t\tt = kthread_create(irq_thread, new, \"irq/%d-%s\", irq,\n\t\t\t\t   new->name);\n\t} else {\n\t\tt = kthread_create(irq_thread, new, \"irq/%d-s-%s\", irq,\n\t\t\t\t   new->name);\n\t}\n\n\tif (IS_ERR(t))\n\t\treturn PTR_ERR(t);\n\n\t \n\tnew->thread = get_task_struct(t);\n\t \n\tset_bit(IRQTF_AFFINITY, &new->thread_flags);\n\treturn 0;\n}\n\n \nstatic int\n__setup_irq(unsigned int irq, struct irq_desc *desc, struct irqaction *new)\n{\n\tstruct irqaction *old, **old_ptr;\n\tunsigned long flags, thread_mask = 0;\n\tint ret, nested, shared = 0;\n\n\tif (!desc)\n\t\treturn -EINVAL;\n\n\tif (desc->irq_data.chip == &no_irq_chip)\n\t\treturn -ENOSYS;\n\tif (!try_module_get(desc->owner))\n\t\treturn -ENODEV;\n\n\tnew->irq = irq;\n\n\t \n\tif (!(new->flags & IRQF_TRIGGER_MASK))\n\t\tnew->flags |= irqd_get_trigger_type(&desc->irq_data);\n\n\t \n\tnested = irq_settings_is_nested_thread(desc);\n\tif (nested) {\n\t\tif (!new->thread_fn) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_mput;\n\t\t}\n\t\t \n\t\tnew->handler = irq_nested_primary_handler;\n\t} else {\n\t\tif (irq_settings_can_thread(desc)) {\n\t\t\tret = irq_setup_forced_threading(new);\n\t\t\tif (ret)\n\t\t\t\tgoto out_mput;\n\t\t}\n\t}\n\n\t \n\tif (new->thread_fn && !nested) {\n\t\tret = setup_irq_thread(new, irq, false);\n\t\tif (ret)\n\t\t\tgoto out_mput;\n\t\tif (new->secondary) {\n\t\t\tret = setup_irq_thread(new->secondary, irq, true);\n\t\t\tif (ret)\n\t\t\t\tgoto out_thread;\n\t\t}\n\t}\n\n\t \n\tif (desc->irq_data.chip->flags & IRQCHIP_ONESHOT_SAFE)\n\t\tnew->flags &= ~IRQF_ONESHOT;\n\n\t \n\tmutex_lock(&desc->request_mutex);\n\n\t \n\tchip_bus_lock(desc);\n\n\t \n\tif (!desc->action) {\n\t\tret = irq_request_resources(desc);\n\t\tif (ret) {\n\t\t\tpr_err(\"Failed to request resources for %s (irq %d) on irqchip %s\\n\",\n\t\t\t       new->name, irq, desc->irq_data.chip->name);\n\t\t\tgoto out_bus_unlock;\n\t\t}\n\t}\n\n\t \n\traw_spin_lock_irqsave(&desc->lock, flags);\n\told_ptr = &desc->action;\n\told = *old_ptr;\n\tif (old) {\n\t\t \n\t\tunsigned int oldtype;\n\n\t\tif (desc->istate & IRQS_NMI) {\n\t\t\tpr_err(\"Invalid attempt to share NMI for %s (irq %d) on irqchip %s.\\n\",\n\t\t\t\tnew->name, irq, desc->irq_data.chip->name);\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\t \n\t\tif (irqd_trigger_type_was_set(&desc->irq_data)) {\n\t\t\toldtype = irqd_get_trigger_type(&desc->irq_data);\n\t\t} else {\n\t\t\toldtype = new->flags & IRQF_TRIGGER_MASK;\n\t\t\tirqd_set_trigger_type(&desc->irq_data, oldtype);\n\t\t}\n\n\t\tif (!((old->flags & new->flags) & IRQF_SHARED) ||\n\t\t    (oldtype != (new->flags & IRQF_TRIGGER_MASK)) ||\n\t\t    ((old->flags ^ new->flags) & IRQF_ONESHOT))\n\t\t\tgoto mismatch;\n\n\t\t \n\t\tif ((old->flags & IRQF_PERCPU) !=\n\t\t    (new->flags & IRQF_PERCPU))\n\t\t\tgoto mismatch;\n\n\t\t \n\t\tdo {\n\t\t\t \n\t\t\tthread_mask |= old->thread_mask;\n\t\t\told_ptr = &old->next;\n\t\t\told = *old_ptr;\n\t\t} while (old);\n\t\tshared = 1;\n\t}\n\n\t \n\tif (new->flags & IRQF_ONESHOT) {\n\t\t \n\t\tif (thread_mask == ~0UL) {\n\t\t\tret = -EBUSY;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\t \n\t\tnew->thread_mask = 1UL << ffz(thread_mask);\n\n\t} else if (new->handler == irq_default_primary_handler &&\n\t\t   !(desc->irq_data.chip->flags & IRQCHIP_ONESHOT_SAFE)) {\n\t\t \n\t\tpr_err(\"Threaded irq requested with handler=NULL and !ONESHOT for %s (irq %d)\\n\",\n\t\t       new->name, irq);\n\t\tret = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\tif (!shared) {\n\t\t \n\t\tif (new->flags & IRQF_TRIGGER_MASK) {\n\t\t\tret = __irq_set_trigger(desc,\n\t\t\t\t\t\tnew->flags & IRQF_TRIGGER_MASK);\n\n\t\t\tif (ret)\n\t\t\t\tgoto out_unlock;\n\t\t}\n\n\t\t \n\t\tret = irq_activate(desc);\n\t\tif (ret)\n\t\t\tgoto out_unlock;\n\n\t\tdesc->istate &= ~(IRQS_AUTODETECT | IRQS_SPURIOUS_DISABLED | \\\n\t\t\t\t  IRQS_ONESHOT | IRQS_WAITING);\n\t\tirqd_clear(&desc->irq_data, IRQD_IRQ_INPROGRESS);\n\n\t\tif (new->flags & IRQF_PERCPU) {\n\t\t\tirqd_set(&desc->irq_data, IRQD_PER_CPU);\n\t\t\tirq_settings_set_per_cpu(desc);\n\t\t\tif (new->flags & IRQF_NO_DEBUG)\n\t\t\t\tirq_settings_set_no_debug(desc);\n\t\t}\n\n\t\tif (noirqdebug)\n\t\t\tirq_settings_set_no_debug(desc);\n\n\t\tif (new->flags & IRQF_ONESHOT)\n\t\t\tdesc->istate |= IRQS_ONESHOT;\n\n\t\t \n\t\tif (new->flags & IRQF_NOBALANCING) {\n\t\t\tirq_settings_set_no_balancing(desc);\n\t\t\tirqd_set(&desc->irq_data, IRQD_NO_BALANCING);\n\t\t}\n\n\t\tif (!(new->flags & IRQF_NO_AUTOEN) &&\n\t\t    irq_settings_can_autoenable(desc)) {\n\t\t\tirq_startup(desc, IRQ_RESEND, IRQ_START_COND);\n\t\t} else {\n\t\t\t \n\t\t\tWARN_ON_ONCE(new->flags & IRQF_SHARED);\n\t\t\t \n\t\t\tdesc->depth = 1;\n\t\t}\n\n\t} else if (new->flags & IRQF_TRIGGER_MASK) {\n\t\tunsigned int nmsk = new->flags & IRQF_TRIGGER_MASK;\n\t\tunsigned int omsk = irqd_get_trigger_type(&desc->irq_data);\n\n\t\tif (nmsk != omsk)\n\t\t\t \n\t\t\tpr_warn(\"irq %d uses trigger mode %u; requested %u\\n\",\n\t\t\t\tirq, omsk, nmsk);\n\t}\n\n\t*old_ptr = new;\n\n\tirq_pm_install_action(desc, new);\n\n\t \n\tdesc->irq_count = 0;\n\tdesc->irqs_unhandled = 0;\n\n\t \n\tif (shared && (desc->istate & IRQS_SPURIOUS_DISABLED)) {\n\t\tdesc->istate &= ~IRQS_SPURIOUS_DISABLED;\n\t\t__enable_irq(desc);\n\t}\n\n\traw_spin_unlock_irqrestore(&desc->lock, flags);\n\tchip_bus_sync_unlock(desc);\n\tmutex_unlock(&desc->request_mutex);\n\n\tirq_setup_timings(desc, new);\n\n\twake_up_and_wait_for_irq_thread_ready(desc, new);\n\twake_up_and_wait_for_irq_thread_ready(desc, new->secondary);\n\n\tregister_irq_proc(irq, desc);\n\tnew->dir = NULL;\n\tregister_handler_proc(irq, new);\n\treturn 0;\n\nmismatch:\n\tif (!(new->flags & IRQF_PROBE_SHARED)) {\n\t\tpr_err(\"Flags mismatch irq %d. %08x (%s) vs. %08x (%s)\\n\",\n\t\t       irq, new->flags, new->name, old->flags, old->name);\n#ifdef CONFIG_DEBUG_SHIRQ\n\t\tdump_stack();\n#endif\n\t}\n\tret = -EBUSY;\n\nout_unlock:\n\traw_spin_unlock_irqrestore(&desc->lock, flags);\n\n\tif (!desc->action)\n\t\tirq_release_resources(desc);\nout_bus_unlock:\n\tchip_bus_sync_unlock(desc);\n\tmutex_unlock(&desc->request_mutex);\n\nout_thread:\n\tif (new->thread) {\n\t\tstruct task_struct *t = new->thread;\n\n\t\tnew->thread = NULL;\n\t\tkthread_stop(t);\n\t\tput_task_struct(t);\n\t}\n\tif (new->secondary && new->secondary->thread) {\n\t\tstruct task_struct *t = new->secondary->thread;\n\n\t\tnew->secondary->thread = NULL;\n\t\tkthread_stop(t);\n\t\tput_task_struct(t);\n\t}\nout_mput:\n\tmodule_put(desc->owner);\n\treturn ret;\n}\n\n \nstatic struct irqaction *__free_irq(struct irq_desc *desc, void *dev_id)\n{\n\tunsigned irq = desc->irq_data.irq;\n\tstruct irqaction *action, **action_ptr;\n\tunsigned long flags;\n\n\tWARN(in_interrupt(), \"Trying to free IRQ %d from IRQ context!\\n\", irq);\n\n\tmutex_lock(&desc->request_mutex);\n\tchip_bus_lock(desc);\n\traw_spin_lock_irqsave(&desc->lock, flags);\n\n\t \n\taction_ptr = &desc->action;\n\tfor (;;) {\n\t\taction = *action_ptr;\n\n\t\tif (!action) {\n\t\t\tWARN(1, \"Trying to free already-free IRQ %d\\n\", irq);\n\t\t\traw_spin_unlock_irqrestore(&desc->lock, flags);\n\t\t\tchip_bus_sync_unlock(desc);\n\t\t\tmutex_unlock(&desc->request_mutex);\n\t\t\treturn NULL;\n\t\t}\n\n\t\tif (action->dev_id == dev_id)\n\t\t\tbreak;\n\t\taction_ptr = &action->next;\n\t}\n\n\t \n\t*action_ptr = action->next;\n\n\tirq_pm_remove_action(desc, action);\n\n\t \n\tif (!desc->action) {\n\t\tirq_settings_clr_disable_unlazy(desc);\n\t\t \n\t\tirq_shutdown(desc);\n\t}\n\n#ifdef CONFIG_SMP\n\t \n\tif (WARN_ON_ONCE(desc->affinity_hint))\n\t\tdesc->affinity_hint = NULL;\n#endif\n\n\traw_spin_unlock_irqrestore(&desc->lock, flags);\n\t \n\tchip_bus_sync_unlock(desc);\n\n\tunregister_handler_proc(irq, action);\n\n\t \n\t__synchronize_irq(desc);\n\n#ifdef CONFIG_DEBUG_SHIRQ\n\t \n\tif (action->flags & IRQF_SHARED) {\n\t\tlocal_irq_save(flags);\n\t\taction->handler(irq, dev_id);\n\t\tlocal_irq_restore(flags);\n\t}\n#endif\n\n\t \n\tif (action->thread) {\n\t\tkthread_stop(action->thread);\n\t\tput_task_struct(action->thread);\n\t\tif (action->secondary && action->secondary->thread) {\n\t\t\tkthread_stop(action->secondary->thread);\n\t\t\tput_task_struct(action->secondary->thread);\n\t\t}\n\t}\n\n\t \n\tif (!desc->action) {\n\t\t \n\t\tchip_bus_lock(desc);\n\t\t \n\t\traw_spin_lock_irqsave(&desc->lock, flags);\n\t\tirq_domain_deactivate_irq(&desc->irq_data);\n\t\traw_spin_unlock_irqrestore(&desc->lock, flags);\n\n\t\tirq_release_resources(desc);\n\t\tchip_bus_sync_unlock(desc);\n\t\tirq_remove_timings(desc);\n\t}\n\n\tmutex_unlock(&desc->request_mutex);\n\n\tirq_chip_pm_put(&desc->irq_data);\n\tmodule_put(desc->owner);\n\tkfree(action->secondary);\n\treturn action;\n}\n\n \nconst void *free_irq(unsigned int irq, void *dev_id)\n{\n\tstruct irq_desc *desc = irq_to_desc(irq);\n\tstruct irqaction *action;\n\tconst char *devname;\n\n\tif (!desc || WARN_ON(irq_settings_is_per_cpu_devid(desc)))\n\t\treturn NULL;\n\n#ifdef CONFIG_SMP\n\tif (WARN_ON(desc->affinity_notify))\n\t\tdesc->affinity_notify = NULL;\n#endif\n\n\taction = __free_irq(desc, dev_id);\n\n\tif (!action)\n\t\treturn NULL;\n\n\tdevname = action->name;\n\tkfree(action);\n\treturn devname;\n}\nEXPORT_SYMBOL(free_irq);\n\n \nstatic const void *__cleanup_nmi(unsigned int irq, struct irq_desc *desc)\n{\n\tconst char *devname = NULL;\n\n\tdesc->istate &= ~IRQS_NMI;\n\n\tif (!WARN_ON(desc->action == NULL)) {\n\t\tirq_pm_remove_action(desc, desc->action);\n\t\tdevname = desc->action->name;\n\t\tunregister_handler_proc(irq, desc->action);\n\n\t\tkfree(desc->action);\n\t\tdesc->action = NULL;\n\t}\n\n\tirq_settings_clr_disable_unlazy(desc);\n\tirq_shutdown_and_deactivate(desc);\n\n\tirq_release_resources(desc);\n\n\tirq_chip_pm_put(&desc->irq_data);\n\tmodule_put(desc->owner);\n\n\treturn devname;\n}\n\nconst void *free_nmi(unsigned int irq, void *dev_id)\n{\n\tstruct irq_desc *desc = irq_to_desc(irq);\n\tunsigned long flags;\n\tconst void *devname;\n\n\tif (!desc || WARN_ON(!(desc->istate & IRQS_NMI)))\n\t\treturn NULL;\n\n\tif (WARN_ON(irq_settings_is_per_cpu_devid(desc)))\n\t\treturn NULL;\n\n\t \n\tif (WARN_ON(desc->depth == 0))\n\t\tdisable_nmi_nosync(irq);\n\n\traw_spin_lock_irqsave(&desc->lock, flags);\n\n\tirq_nmi_teardown(desc);\n\tdevname = __cleanup_nmi(irq, desc);\n\n\traw_spin_unlock_irqrestore(&desc->lock, flags);\n\n\treturn devname;\n}\n\n \nint request_threaded_irq(unsigned int irq, irq_handler_t handler,\n\t\t\t irq_handler_t thread_fn, unsigned long irqflags,\n\t\t\t const char *devname, void *dev_id)\n{\n\tstruct irqaction *action;\n\tstruct irq_desc *desc;\n\tint retval;\n\n\tif (irq == IRQ_NOTCONNECTED)\n\t\treturn -ENOTCONN;\n\n\t \n\tif (((irqflags & IRQF_SHARED) && !dev_id) ||\n\t    ((irqflags & IRQF_SHARED) && (irqflags & IRQF_NO_AUTOEN)) ||\n\t    (!(irqflags & IRQF_SHARED) && (irqflags & IRQF_COND_SUSPEND)) ||\n\t    ((irqflags & IRQF_NO_SUSPEND) && (irqflags & IRQF_COND_SUSPEND)))\n\t\treturn -EINVAL;\n\n\tdesc = irq_to_desc(irq);\n\tif (!desc)\n\t\treturn -EINVAL;\n\n\tif (!irq_settings_can_request(desc) ||\n\t    WARN_ON(irq_settings_is_per_cpu_devid(desc)))\n\t\treturn -EINVAL;\n\n\tif (!handler) {\n\t\tif (!thread_fn)\n\t\t\treturn -EINVAL;\n\t\thandler = irq_default_primary_handler;\n\t}\n\n\taction = kzalloc(sizeof(struct irqaction), GFP_KERNEL);\n\tif (!action)\n\t\treturn -ENOMEM;\n\n\taction->handler = handler;\n\taction->thread_fn = thread_fn;\n\taction->flags = irqflags;\n\taction->name = devname;\n\taction->dev_id = dev_id;\n\n\tretval = irq_chip_pm_get(&desc->irq_data);\n\tif (retval < 0) {\n\t\tkfree(action);\n\t\treturn retval;\n\t}\n\n\tretval = __setup_irq(irq, desc, action);\n\n\tif (retval) {\n\t\tirq_chip_pm_put(&desc->irq_data);\n\t\tkfree(action->secondary);\n\t\tkfree(action);\n\t}\n\n#ifdef CONFIG_DEBUG_SHIRQ_FIXME\n\tif (!retval && (irqflags & IRQF_SHARED)) {\n\t\t \n\t\tunsigned long flags;\n\n\t\tdisable_irq(irq);\n\t\tlocal_irq_save(flags);\n\n\t\thandler(irq, dev_id);\n\n\t\tlocal_irq_restore(flags);\n\t\tenable_irq(irq);\n\t}\n#endif\n\treturn retval;\n}\nEXPORT_SYMBOL(request_threaded_irq);\n\n \nint request_any_context_irq(unsigned int irq, irq_handler_t handler,\n\t\t\t    unsigned long flags, const char *name, void *dev_id)\n{\n\tstruct irq_desc *desc;\n\tint ret;\n\n\tif (irq == IRQ_NOTCONNECTED)\n\t\treturn -ENOTCONN;\n\n\tdesc = irq_to_desc(irq);\n\tif (!desc)\n\t\treturn -EINVAL;\n\n\tif (irq_settings_is_nested_thread(desc)) {\n\t\tret = request_threaded_irq(irq, NULL, handler,\n\t\t\t\t\t   flags, name, dev_id);\n\t\treturn !ret ? IRQC_IS_NESTED : ret;\n\t}\n\n\tret = request_irq(irq, handler, flags, name, dev_id);\n\treturn !ret ? IRQC_IS_HARDIRQ : ret;\n}\nEXPORT_SYMBOL_GPL(request_any_context_irq);\n\n \nint request_nmi(unsigned int irq, irq_handler_t handler,\n\t\tunsigned long irqflags, const char *name, void *dev_id)\n{\n\tstruct irqaction *action;\n\tstruct irq_desc *desc;\n\tunsigned long flags;\n\tint retval;\n\n\tif (irq == IRQ_NOTCONNECTED)\n\t\treturn -ENOTCONN;\n\n\t \n\tif (irqflags & (IRQF_SHARED | IRQF_COND_SUSPEND | IRQF_IRQPOLL))\n\t\treturn -EINVAL;\n\n\tif (!(irqflags & IRQF_PERCPU))\n\t\treturn -EINVAL;\n\n\tif (!handler)\n\t\treturn -EINVAL;\n\n\tdesc = irq_to_desc(irq);\n\n\tif (!desc || (irq_settings_can_autoenable(desc) &&\n\t    !(irqflags & IRQF_NO_AUTOEN)) ||\n\t    !irq_settings_can_request(desc) ||\n\t    WARN_ON(irq_settings_is_per_cpu_devid(desc)) ||\n\t    !irq_supports_nmi(desc))\n\t\treturn -EINVAL;\n\n\taction = kzalloc(sizeof(struct irqaction), GFP_KERNEL);\n\tif (!action)\n\t\treturn -ENOMEM;\n\n\taction->handler = handler;\n\taction->flags = irqflags | IRQF_NO_THREAD | IRQF_NOBALANCING;\n\taction->name = name;\n\taction->dev_id = dev_id;\n\n\tretval = irq_chip_pm_get(&desc->irq_data);\n\tif (retval < 0)\n\t\tgoto err_out;\n\n\tretval = __setup_irq(irq, desc, action);\n\tif (retval)\n\t\tgoto err_irq_setup;\n\n\traw_spin_lock_irqsave(&desc->lock, flags);\n\n\t \n\tdesc->istate |= IRQS_NMI;\n\tretval = irq_nmi_setup(desc);\n\tif (retval) {\n\t\t__cleanup_nmi(irq, desc);\n\t\traw_spin_unlock_irqrestore(&desc->lock, flags);\n\t\treturn -EINVAL;\n\t}\n\n\traw_spin_unlock_irqrestore(&desc->lock, flags);\n\n\treturn 0;\n\nerr_irq_setup:\n\tirq_chip_pm_put(&desc->irq_data);\nerr_out:\n\tkfree(action);\n\n\treturn retval;\n}\n\nvoid enable_percpu_irq(unsigned int irq, unsigned int type)\n{\n\tunsigned int cpu = smp_processor_id();\n\tunsigned long flags;\n\tstruct irq_desc *desc = irq_get_desc_lock(irq, &flags, IRQ_GET_DESC_CHECK_PERCPU);\n\n\tif (!desc)\n\t\treturn;\n\n\t \n\ttype &= IRQ_TYPE_SENSE_MASK;\n\tif (type == IRQ_TYPE_NONE)\n\t\ttype = irqd_get_trigger_type(&desc->irq_data);\n\n\tif (type != IRQ_TYPE_NONE) {\n\t\tint ret;\n\n\t\tret = __irq_set_trigger(desc, type);\n\n\t\tif (ret) {\n\t\t\tWARN(1, \"failed to set type for IRQ%d\\n\", irq);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tirq_percpu_enable(desc, cpu);\nout:\n\tirq_put_desc_unlock(desc, flags);\n}\nEXPORT_SYMBOL_GPL(enable_percpu_irq);\n\nvoid enable_percpu_nmi(unsigned int irq, unsigned int type)\n{\n\tenable_percpu_irq(irq, type);\n}\n\n \nbool irq_percpu_is_enabled(unsigned int irq)\n{\n\tunsigned int cpu = smp_processor_id();\n\tstruct irq_desc *desc;\n\tunsigned long flags;\n\tbool is_enabled;\n\n\tdesc = irq_get_desc_lock(irq, &flags, IRQ_GET_DESC_CHECK_PERCPU);\n\tif (!desc)\n\t\treturn false;\n\n\tis_enabled = cpumask_test_cpu(cpu, desc->percpu_enabled);\n\tirq_put_desc_unlock(desc, flags);\n\n\treturn is_enabled;\n}\nEXPORT_SYMBOL_GPL(irq_percpu_is_enabled);\n\nvoid disable_percpu_irq(unsigned int irq)\n{\n\tunsigned int cpu = smp_processor_id();\n\tunsigned long flags;\n\tstruct irq_desc *desc = irq_get_desc_lock(irq, &flags, IRQ_GET_DESC_CHECK_PERCPU);\n\n\tif (!desc)\n\t\treturn;\n\n\tirq_percpu_disable(desc, cpu);\n\tirq_put_desc_unlock(desc, flags);\n}\nEXPORT_SYMBOL_GPL(disable_percpu_irq);\n\nvoid disable_percpu_nmi(unsigned int irq)\n{\n\tdisable_percpu_irq(irq);\n}\n\n \nstatic struct irqaction *__free_percpu_irq(unsigned int irq, void __percpu *dev_id)\n{\n\tstruct irq_desc *desc = irq_to_desc(irq);\n\tstruct irqaction *action;\n\tunsigned long flags;\n\n\tWARN(in_interrupt(), \"Trying to free IRQ %d from IRQ context!\\n\", irq);\n\n\tif (!desc)\n\t\treturn NULL;\n\n\traw_spin_lock_irqsave(&desc->lock, flags);\n\n\taction = desc->action;\n\tif (!action || action->percpu_dev_id != dev_id) {\n\t\tWARN(1, \"Trying to free already-free IRQ %d\\n\", irq);\n\t\tgoto bad;\n\t}\n\n\tif (!cpumask_empty(desc->percpu_enabled)) {\n\t\tWARN(1, \"percpu IRQ %d still enabled on CPU%d!\\n\",\n\t\t     irq, cpumask_first(desc->percpu_enabled));\n\t\tgoto bad;\n\t}\n\n\t \n\tdesc->action = NULL;\n\n\tdesc->istate &= ~IRQS_NMI;\n\n\traw_spin_unlock_irqrestore(&desc->lock, flags);\n\n\tunregister_handler_proc(irq, action);\n\n\tirq_chip_pm_put(&desc->irq_data);\n\tmodule_put(desc->owner);\n\treturn action;\n\nbad:\n\traw_spin_unlock_irqrestore(&desc->lock, flags);\n\treturn NULL;\n}\n\n \nvoid remove_percpu_irq(unsigned int irq, struct irqaction *act)\n{\n\tstruct irq_desc *desc = irq_to_desc(irq);\n\n\tif (desc && irq_settings_is_per_cpu_devid(desc))\n\t    __free_percpu_irq(irq, act->percpu_dev_id);\n}\n\n \nvoid free_percpu_irq(unsigned int irq, void __percpu *dev_id)\n{\n\tstruct irq_desc *desc = irq_to_desc(irq);\n\n\tif (!desc || !irq_settings_is_per_cpu_devid(desc))\n\t\treturn;\n\n\tchip_bus_lock(desc);\n\tkfree(__free_percpu_irq(irq, dev_id));\n\tchip_bus_sync_unlock(desc);\n}\nEXPORT_SYMBOL_GPL(free_percpu_irq);\n\nvoid free_percpu_nmi(unsigned int irq, void __percpu *dev_id)\n{\n\tstruct irq_desc *desc = irq_to_desc(irq);\n\n\tif (!desc || !irq_settings_is_per_cpu_devid(desc))\n\t\treturn;\n\n\tif (WARN_ON(!(desc->istate & IRQS_NMI)))\n\t\treturn;\n\n\tkfree(__free_percpu_irq(irq, dev_id));\n}\n\n \nint setup_percpu_irq(unsigned int irq, struct irqaction *act)\n{\n\tstruct irq_desc *desc = irq_to_desc(irq);\n\tint retval;\n\n\tif (!desc || !irq_settings_is_per_cpu_devid(desc))\n\t\treturn -EINVAL;\n\n\tretval = irq_chip_pm_get(&desc->irq_data);\n\tif (retval < 0)\n\t\treturn retval;\n\n\tretval = __setup_irq(irq, desc, act);\n\n\tif (retval)\n\t\tirq_chip_pm_put(&desc->irq_data);\n\n\treturn retval;\n}\n\n \nint __request_percpu_irq(unsigned int irq, irq_handler_t handler,\n\t\t\t unsigned long flags, const char *devname,\n\t\t\t void __percpu *dev_id)\n{\n\tstruct irqaction *action;\n\tstruct irq_desc *desc;\n\tint retval;\n\n\tif (!dev_id)\n\t\treturn -EINVAL;\n\n\tdesc = irq_to_desc(irq);\n\tif (!desc || !irq_settings_can_request(desc) ||\n\t    !irq_settings_is_per_cpu_devid(desc))\n\t\treturn -EINVAL;\n\n\tif (flags && flags != IRQF_TIMER)\n\t\treturn -EINVAL;\n\n\taction = kzalloc(sizeof(struct irqaction), GFP_KERNEL);\n\tif (!action)\n\t\treturn -ENOMEM;\n\n\taction->handler = handler;\n\taction->flags = flags | IRQF_PERCPU | IRQF_NO_SUSPEND;\n\taction->name = devname;\n\taction->percpu_dev_id = dev_id;\n\n\tretval = irq_chip_pm_get(&desc->irq_data);\n\tif (retval < 0) {\n\t\tkfree(action);\n\t\treturn retval;\n\t}\n\n\tretval = __setup_irq(irq, desc, action);\n\n\tif (retval) {\n\t\tirq_chip_pm_put(&desc->irq_data);\n\t\tkfree(action);\n\t}\n\n\treturn retval;\n}\nEXPORT_SYMBOL_GPL(__request_percpu_irq);\n\n \nint request_percpu_nmi(unsigned int irq, irq_handler_t handler,\n\t\t       const char *name, void __percpu *dev_id)\n{\n\tstruct irqaction *action;\n\tstruct irq_desc *desc;\n\tunsigned long flags;\n\tint retval;\n\n\tif (!handler)\n\t\treturn -EINVAL;\n\n\tdesc = irq_to_desc(irq);\n\n\tif (!desc || !irq_settings_can_request(desc) ||\n\t    !irq_settings_is_per_cpu_devid(desc) ||\n\t    irq_settings_can_autoenable(desc) ||\n\t    !irq_supports_nmi(desc))\n\t\treturn -EINVAL;\n\n\t \n\tif (desc->istate & IRQS_NMI)\n\t\treturn -EINVAL;\n\n\taction = kzalloc(sizeof(struct irqaction), GFP_KERNEL);\n\tif (!action)\n\t\treturn -ENOMEM;\n\n\taction->handler = handler;\n\taction->flags = IRQF_PERCPU | IRQF_NO_SUSPEND | IRQF_NO_THREAD\n\t\t| IRQF_NOBALANCING;\n\taction->name = name;\n\taction->percpu_dev_id = dev_id;\n\n\tretval = irq_chip_pm_get(&desc->irq_data);\n\tif (retval < 0)\n\t\tgoto err_out;\n\n\tretval = __setup_irq(irq, desc, action);\n\tif (retval)\n\t\tgoto err_irq_setup;\n\n\traw_spin_lock_irqsave(&desc->lock, flags);\n\tdesc->istate |= IRQS_NMI;\n\traw_spin_unlock_irqrestore(&desc->lock, flags);\n\n\treturn 0;\n\nerr_irq_setup:\n\tirq_chip_pm_put(&desc->irq_data);\nerr_out:\n\tkfree(action);\n\n\treturn retval;\n}\n\n \nint prepare_percpu_nmi(unsigned int irq)\n{\n\tunsigned long flags;\n\tstruct irq_desc *desc;\n\tint ret = 0;\n\n\tWARN_ON(preemptible());\n\n\tdesc = irq_get_desc_lock(irq, &flags,\n\t\t\t\t IRQ_GET_DESC_CHECK_PERCPU);\n\tif (!desc)\n\t\treturn -EINVAL;\n\n\tif (WARN(!(desc->istate & IRQS_NMI),\n\t\t KERN_ERR \"prepare_percpu_nmi called for a non-NMI interrupt: irq %u\\n\",\n\t\t irq)) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tret = irq_nmi_setup(desc);\n\tif (ret) {\n\t\tpr_err(\"Failed to setup NMI delivery: irq %u\\n\", irq);\n\t\tgoto out;\n\t}\n\nout:\n\tirq_put_desc_unlock(desc, flags);\n\treturn ret;\n}\n\n \nvoid teardown_percpu_nmi(unsigned int irq)\n{\n\tunsigned long flags;\n\tstruct irq_desc *desc;\n\n\tWARN_ON(preemptible());\n\n\tdesc = irq_get_desc_lock(irq, &flags,\n\t\t\t\t IRQ_GET_DESC_CHECK_PERCPU);\n\tif (!desc)\n\t\treturn;\n\n\tif (WARN_ON(!(desc->istate & IRQS_NMI)))\n\t\tgoto out;\n\n\tirq_nmi_teardown(desc);\nout:\n\tirq_put_desc_unlock(desc, flags);\n}\n\nint __irq_get_irqchip_state(struct irq_data *data, enum irqchip_irq_state which,\n\t\t\t    bool *state)\n{\n\tstruct irq_chip *chip;\n\tint err = -EINVAL;\n\n\tdo {\n\t\tchip = irq_data_get_irq_chip(data);\n\t\tif (WARN_ON_ONCE(!chip))\n\t\t\treturn -ENODEV;\n\t\tif (chip->irq_get_irqchip_state)\n\t\t\tbreak;\n#ifdef CONFIG_IRQ_DOMAIN_HIERARCHY\n\t\tdata = data->parent_data;\n#else\n\t\tdata = NULL;\n#endif\n\t} while (data);\n\n\tif (data)\n\t\terr = chip->irq_get_irqchip_state(data, which, state);\n\treturn err;\n}\n\n \nint irq_get_irqchip_state(unsigned int irq, enum irqchip_irq_state which,\n\t\t\t  bool *state)\n{\n\tstruct irq_desc *desc;\n\tstruct irq_data *data;\n\tunsigned long flags;\n\tint err = -EINVAL;\n\n\tdesc = irq_get_desc_buslock(irq, &flags, 0);\n\tif (!desc)\n\t\treturn err;\n\n\tdata = irq_desc_get_irq_data(desc);\n\n\terr = __irq_get_irqchip_state(data, which, state);\n\n\tirq_put_desc_busunlock(desc, flags);\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(irq_get_irqchip_state);\n\n \nint irq_set_irqchip_state(unsigned int irq, enum irqchip_irq_state which,\n\t\t\t  bool val)\n{\n\tstruct irq_desc *desc;\n\tstruct irq_data *data;\n\tstruct irq_chip *chip;\n\tunsigned long flags;\n\tint err = -EINVAL;\n\n\tdesc = irq_get_desc_buslock(irq, &flags, 0);\n\tif (!desc)\n\t\treturn err;\n\n\tdata = irq_desc_get_irq_data(desc);\n\n\tdo {\n\t\tchip = irq_data_get_irq_chip(data);\n\t\tif (WARN_ON_ONCE(!chip)) {\n\t\t\terr = -ENODEV;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tif (chip->irq_set_irqchip_state)\n\t\t\tbreak;\n#ifdef CONFIG_IRQ_DOMAIN_HIERARCHY\n\t\tdata = data->parent_data;\n#else\n\t\tdata = NULL;\n#endif\n\t} while (data);\n\n\tif (data)\n\t\terr = chip->irq_set_irqchip_state(data, which, val);\n\nout_unlock:\n\tirq_put_desc_busunlock(desc, flags);\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(irq_set_irqchip_state);\n\n \nbool irq_has_action(unsigned int irq)\n{\n\tbool res;\n\n\trcu_read_lock();\n\tres = irq_desc_has_action(irq_to_desc(irq));\n\trcu_read_unlock();\n\treturn res;\n}\nEXPORT_SYMBOL_GPL(irq_has_action);\n\n \nbool irq_check_status_bit(unsigned int irq, unsigned int bitmask)\n{\n\tstruct irq_desc *desc;\n\tbool res = false;\n\n\trcu_read_lock();\n\tdesc = irq_to_desc(irq);\n\tif (desc)\n\t\tres = !!(desc->status_use_accessors & bitmask);\n\trcu_read_unlock();\n\treturn res;\n}\nEXPORT_SYMBOL_GPL(irq_check_status_bit);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}