{
  "module_name": "transition.c",
  "hash_id": "d1ba965d9c6f883c7916dfcced02f82f1cf23c7e022bf9471f20a3974952d3c1",
  "original_prompt": "Ingested from linux-6.6.14/kernel/livepatch/transition.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/cpu.h>\n#include <linux/stacktrace.h>\n#include <linux/static_call.h>\n#include \"core.h\"\n#include \"patch.h\"\n#include \"transition.h\"\n\n#define MAX_STACK_ENTRIES  100\nstatic DEFINE_PER_CPU(unsigned long[MAX_STACK_ENTRIES], klp_stack_entries);\n\n#define STACK_ERR_BUF_SIZE 128\n\n#define SIGNALS_TIMEOUT 15\n\nstruct klp_patch *klp_transition_patch;\n\nstatic int klp_target_state = KLP_UNDEFINED;\n\nstatic unsigned int klp_signals_cnt;\n\n \n#if defined(CONFIG_PREEMPT_DYNAMIC) && defined(CONFIG_HAVE_PREEMPT_DYNAMIC_CALL)\n\n#define klp_cond_resched_enable() sched_dynamic_klp_enable()\n#define klp_cond_resched_disable() sched_dynamic_klp_disable()\n\n#else  \n\nDEFINE_STATIC_KEY_FALSE(klp_sched_try_switch_key);\nEXPORT_SYMBOL(klp_sched_try_switch_key);\n\n#define klp_cond_resched_enable() static_branch_enable(&klp_sched_try_switch_key)\n#define klp_cond_resched_disable() static_branch_disable(&klp_sched_try_switch_key)\n\n#endif  \n\n \nstatic void klp_transition_work_fn(struct work_struct *work)\n{\n\tmutex_lock(&klp_mutex);\n\n\tif (klp_transition_patch)\n\t\tklp_try_complete_transition();\n\n\tmutex_unlock(&klp_mutex);\n}\nstatic DECLARE_DELAYED_WORK(klp_transition_work, klp_transition_work_fn);\n\n \nstatic void klp_sync(struct work_struct *work)\n{\n}\n\n \nstatic void klp_synchronize_transition(void)\n{\n\tschedule_on_each_cpu(klp_sync);\n}\n\n \nstatic void klp_complete_transition(void)\n{\n\tstruct klp_object *obj;\n\tstruct klp_func *func;\n\tstruct task_struct *g, *task;\n\tunsigned int cpu;\n\n\tpr_debug(\"'%s': completing %s transition\\n\",\n\t\t klp_transition_patch->mod->name,\n\t\t klp_target_state == KLP_PATCHED ? \"patching\" : \"unpatching\");\n\n\tif (klp_transition_patch->replace && klp_target_state == KLP_PATCHED) {\n\t\tklp_unpatch_replaced_patches(klp_transition_patch);\n\t\tklp_discard_nops(klp_transition_patch);\n\t}\n\n\tif (klp_target_state == KLP_UNPATCHED) {\n\t\t \n\t\tklp_unpatch_objects(klp_transition_patch);\n\n\t\t \n\t\tklp_synchronize_transition();\n\t}\n\n\tklp_for_each_object(klp_transition_patch, obj)\n\t\tklp_for_each_func(obj, func)\n\t\t\tfunc->transition = false;\n\n\t \n\tif (klp_target_state == KLP_PATCHED)\n\t\tklp_synchronize_transition();\n\n\tread_lock(&tasklist_lock);\n\tfor_each_process_thread(g, task) {\n\t\tWARN_ON_ONCE(test_tsk_thread_flag(task, TIF_PATCH_PENDING));\n\t\ttask->patch_state = KLP_UNDEFINED;\n\t}\n\tread_unlock(&tasklist_lock);\n\n\tfor_each_possible_cpu(cpu) {\n\t\ttask = idle_task(cpu);\n\t\tWARN_ON_ONCE(test_tsk_thread_flag(task, TIF_PATCH_PENDING));\n\t\ttask->patch_state = KLP_UNDEFINED;\n\t}\n\n\tklp_for_each_object(klp_transition_patch, obj) {\n\t\tif (!klp_is_object_loaded(obj))\n\t\t\tcontinue;\n\t\tif (klp_target_state == KLP_PATCHED)\n\t\t\tklp_post_patch_callback(obj);\n\t\telse if (klp_target_state == KLP_UNPATCHED)\n\t\t\tklp_post_unpatch_callback(obj);\n\t}\n\n\tpr_notice(\"'%s': %s complete\\n\", klp_transition_patch->mod->name,\n\t\t  klp_target_state == KLP_PATCHED ? \"patching\" : \"unpatching\");\n\n\tklp_target_state = KLP_UNDEFINED;\n\tklp_transition_patch = NULL;\n}\n\n \nvoid klp_cancel_transition(void)\n{\n\tif (WARN_ON_ONCE(klp_target_state != KLP_PATCHED))\n\t\treturn;\n\n\tpr_debug(\"'%s': canceling patching transition, going to unpatch\\n\",\n\t\t klp_transition_patch->mod->name);\n\n\tklp_target_state = KLP_UNPATCHED;\n\tklp_complete_transition();\n}\n\n \nvoid klp_update_patch_state(struct task_struct *task)\n{\n\t \n\tpreempt_disable_notrace();\n\n\t \n\tif (test_and_clear_tsk_thread_flag(task, TIF_PATCH_PENDING))\n\t\ttask->patch_state = READ_ONCE(klp_target_state);\n\n\tpreempt_enable_notrace();\n}\n\n \nstatic int klp_check_stack_func(struct klp_func *func, unsigned long *entries,\n\t\t\t\tunsigned int nr_entries)\n{\n\tunsigned long func_addr, func_size, address;\n\tstruct klp_ops *ops;\n\tint i;\n\n\tif (klp_target_state == KLP_UNPATCHED) {\n\t\t  \n\t\tfunc_addr = (unsigned long)func->new_func;\n\t\tfunc_size = func->new_size;\n\t} else {\n\t\t \n\t\tops = klp_find_ops(func->old_func);\n\n\t\tif (list_is_singular(&ops->func_stack)) {\n\t\t\t \n\t\t\tfunc_addr = (unsigned long)func->old_func;\n\t\t\tfunc_size = func->old_size;\n\t\t} else {\n\t\t\t \n\t\t\tstruct klp_func *prev;\n\n\t\t\tprev = list_next_entry(func, stack_node);\n\t\t\tfunc_addr = (unsigned long)prev->new_func;\n\t\t\tfunc_size = prev->new_size;\n\t\t}\n\t}\n\n\tfor (i = 0; i < nr_entries; i++) {\n\t\taddress = entries[i];\n\n\t\tif (address >= func_addr && address < func_addr + func_size)\n\t\t\treturn -EAGAIN;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int klp_check_stack(struct task_struct *task, const char **oldname)\n{\n\tunsigned long *entries = this_cpu_ptr(klp_stack_entries);\n\tstruct klp_object *obj;\n\tstruct klp_func *func;\n\tint ret, nr_entries;\n\n\t \n\tlockdep_assert_preemption_disabled();\n\n\tret = stack_trace_save_tsk_reliable(task, entries, MAX_STACK_ENTRIES);\n\tif (ret < 0)\n\t\treturn -EINVAL;\n\tnr_entries = ret;\n\n\tklp_for_each_object(klp_transition_patch, obj) {\n\t\tif (!obj->patched)\n\t\t\tcontinue;\n\t\tklp_for_each_func(obj, func) {\n\t\t\tret = klp_check_stack_func(func, entries, nr_entries);\n\t\t\tif (ret) {\n\t\t\t\t*oldname = func->old_name;\n\t\t\t\treturn -EADDRINUSE;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int klp_check_and_switch_task(struct task_struct *task, void *arg)\n{\n\tint ret;\n\n\tif (task_curr(task) && task != current)\n\t\treturn -EBUSY;\n\n\tret = klp_check_stack(task, arg);\n\tif (ret)\n\t\treturn ret;\n\n\tclear_tsk_thread_flag(task, TIF_PATCH_PENDING);\n\ttask->patch_state = klp_target_state;\n\treturn 0;\n}\n\n \nstatic bool klp_try_switch_task(struct task_struct *task)\n{\n\tconst char *old_name;\n\tint ret;\n\n\t \n\tif (task->patch_state == klp_target_state)\n\t\treturn true;\n\n\t \n\tif (!klp_have_reliable_stack())\n\t\treturn false;\n\n\t \n\tif (task == current)\n\t\tret = klp_check_and_switch_task(current, &old_name);\n\telse\n\t\tret = task_call_func(task, klp_check_and_switch_task, &old_name);\n\n\tswitch (ret) {\n\tcase 0:\t\t \n\t\tbreak;\n\n\tcase -EBUSY:\t \n\t\tpr_debug(\"%s: %s:%d is running\\n\",\n\t\t\t __func__, task->comm, task->pid);\n\t\tbreak;\n\tcase -EINVAL:\t \n\t\tpr_debug(\"%s: %s:%d has an unreliable stack\\n\",\n\t\t\t __func__, task->comm, task->pid);\n\t\tbreak;\n\tcase -EADDRINUSE:  \n\t\tpr_debug(\"%s: %s:%d is sleeping on function %s\\n\",\n\t\t\t __func__, task->comm, task->pid, old_name);\n\t\tbreak;\n\n\tdefault:\n\t\tpr_debug(\"%s: Unknown error code (%d) when trying to switch %s:%d\\n\",\n\t\t\t __func__, ret, task->comm, task->pid);\n\t\tbreak;\n\t}\n\n\treturn !ret;\n}\n\nvoid __klp_sched_try_switch(void)\n{\n\tif (likely(!klp_patch_pending(current)))\n\t\treturn;\n\n\t \n\tpreempt_disable();\n\n\t \n\tif (unlikely(!klp_patch_pending(current)))\n\t\tgoto out;\n\n\t \n\tsmp_rmb();\n\n\tklp_try_switch_task(current);\n\nout:\n\tpreempt_enable();\n}\nEXPORT_SYMBOL(__klp_sched_try_switch);\n\n \nstatic void klp_send_signals(void)\n{\n\tstruct task_struct *g, *task;\n\n\tif (klp_signals_cnt == SIGNALS_TIMEOUT)\n\t\tpr_notice(\"signaling remaining tasks\\n\");\n\n\tread_lock(&tasklist_lock);\n\tfor_each_process_thread(g, task) {\n\t\tif (!klp_patch_pending(task))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (task->flags & PF_KTHREAD) {\n\t\t\t \n\t\t\twake_up_state(task, TASK_INTERRUPTIBLE);\n\t\t} else {\n\t\t\t \n\t\t\tset_notify_signal(task);\n\t\t}\n\t}\n\tread_unlock(&tasklist_lock);\n}\n\n \nvoid klp_try_complete_transition(void)\n{\n\tunsigned int cpu;\n\tstruct task_struct *g, *task;\n\tstruct klp_patch *patch;\n\tbool complete = true;\n\n\tWARN_ON_ONCE(klp_target_state == KLP_UNDEFINED);\n\n\t \n\tread_lock(&tasklist_lock);\n\tfor_each_process_thread(g, task)\n\t\tif (!klp_try_switch_task(task))\n\t\t\tcomplete = false;\n\tread_unlock(&tasklist_lock);\n\n\t \n\tcpus_read_lock();\n\tfor_each_possible_cpu(cpu) {\n\t\ttask = idle_task(cpu);\n\t\tif (cpu_online(cpu)) {\n\t\t\tif (!klp_try_switch_task(task)) {\n\t\t\t\tcomplete = false;\n\t\t\t\t \n\t\t\t\twake_up_if_idle(cpu);\n\t\t\t}\n\t\t} else if (task->patch_state != klp_target_state) {\n\t\t\t \n\t\t\tclear_tsk_thread_flag(task, TIF_PATCH_PENDING);\n\t\t\ttask->patch_state = klp_target_state;\n\t\t}\n\t}\n\tcpus_read_unlock();\n\n\tif (!complete) {\n\t\tif (klp_signals_cnt && !(klp_signals_cnt % SIGNALS_TIMEOUT))\n\t\t\tklp_send_signals();\n\t\tklp_signals_cnt++;\n\n\t\t \n\t\tschedule_delayed_work(&klp_transition_work,\n\t\t\t\t      round_jiffies_relative(HZ));\n\t\treturn;\n\t}\n\n\t \n\tklp_cond_resched_disable();\n\tpatch = klp_transition_patch;\n\tklp_complete_transition();\n\n\t \n\tif (!patch->enabled)\n\t\tklp_free_patch_async(patch);\n\telse if (patch->replace)\n\t\tklp_free_replaced_patches_async(patch);\n}\n\n \nvoid klp_start_transition(void)\n{\n\tstruct task_struct *g, *task;\n\tunsigned int cpu;\n\n\tWARN_ON_ONCE(klp_target_state == KLP_UNDEFINED);\n\n\tpr_notice(\"'%s': starting %s transition\\n\",\n\t\t  klp_transition_patch->mod->name,\n\t\t  klp_target_state == KLP_PATCHED ? \"patching\" : \"unpatching\");\n\n\t \n\tread_lock(&tasklist_lock);\n\tfor_each_process_thread(g, task)\n\t\tif (task->patch_state != klp_target_state)\n\t\t\tset_tsk_thread_flag(task, TIF_PATCH_PENDING);\n\tread_unlock(&tasklist_lock);\n\n\t \n\tfor_each_possible_cpu(cpu) {\n\t\ttask = idle_task(cpu);\n\t\tif (task->patch_state != klp_target_state)\n\t\t\tset_tsk_thread_flag(task, TIF_PATCH_PENDING);\n\t}\n\n\tklp_cond_resched_enable();\n\n\tklp_signals_cnt = 0;\n}\n\n \nvoid klp_init_transition(struct klp_patch *patch, int state)\n{\n\tstruct task_struct *g, *task;\n\tunsigned int cpu;\n\tstruct klp_object *obj;\n\tstruct klp_func *func;\n\tint initial_state = !state;\n\n\tWARN_ON_ONCE(klp_target_state != KLP_UNDEFINED);\n\n\tklp_transition_patch = patch;\n\n\t \n\tklp_target_state = state;\n\n\tpr_debug(\"'%s': initializing %s transition\\n\", patch->mod->name,\n\t\t klp_target_state == KLP_PATCHED ? \"patching\" : \"unpatching\");\n\n\t \n\tread_lock(&tasklist_lock);\n\tfor_each_process_thread(g, task) {\n\t\tWARN_ON_ONCE(task->patch_state != KLP_UNDEFINED);\n\t\ttask->patch_state = initial_state;\n\t}\n\tread_unlock(&tasklist_lock);\n\n\t \n\tfor_each_possible_cpu(cpu) {\n\t\ttask = idle_task(cpu);\n\t\tWARN_ON_ONCE(task->patch_state != KLP_UNDEFINED);\n\t\ttask->patch_state = initial_state;\n\t}\n\n\t \n\tsmp_wmb();\n\n\t \n\tklp_for_each_object(patch, obj)\n\t\tklp_for_each_func(obj, func)\n\t\t\tfunc->transition = true;\n}\n\n \nvoid klp_reverse_transition(void)\n{\n\tunsigned int cpu;\n\tstruct task_struct *g, *task;\n\n\tpr_debug(\"'%s': reversing transition from %s\\n\",\n\t\t klp_transition_patch->mod->name,\n\t\t klp_target_state == KLP_PATCHED ? \"patching to unpatching\" :\n\t\t\t\t\t\t   \"unpatching to patching\");\n\n\t \n\tread_lock(&tasklist_lock);\n\tfor_each_process_thread(g, task)\n\t\tclear_tsk_thread_flag(task, TIF_PATCH_PENDING);\n\tread_unlock(&tasklist_lock);\n\n\tfor_each_possible_cpu(cpu)\n\t\tclear_tsk_thread_flag(idle_task(cpu), TIF_PATCH_PENDING);\n\n\t \n\tklp_synchronize_transition();\n\n\t \n\tklp_transition_patch->enabled = !klp_transition_patch->enabled;\n\tklp_target_state = !klp_target_state;\n\n\t \n\tsmp_wmb();\n\n\tklp_start_transition();\n}\n\n \nvoid klp_copy_process(struct task_struct *child)\n{\n\n\t \n\tif (test_tsk_thread_flag(current, TIF_PATCH_PENDING))\n\t\tset_tsk_thread_flag(child, TIF_PATCH_PENDING);\n\telse\n\t\tclear_tsk_thread_flag(child, TIF_PATCH_PENDING);\n\n\tchild->patch_state = current->patch_state;\n}\n\n \nvoid klp_force_transition(void)\n{\n\tstruct klp_patch *patch;\n\tstruct task_struct *g, *task;\n\tunsigned int cpu;\n\n\tpr_warn(\"forcing remaining tasks to the patched state\\n\");\n\n\tread_lock(&tasklist_lock);\n\tfor_each_process_thread(g, task)\n\t\tklp_update_patch_state(task);\n\tread_unlock(&tasklist_lock);\n\n\tfor_each_possible_cpu(cpu)\n\t\tklp_update_patch_state(idle_task(cpu));\n\n\t \n\tif (klp_target_state == KLP_UNPATCHED)\n\t\tklp_transition_patch->forced = true;\n\telse if (klp_transition_patch->replace) {\n\t\tklp_for_each_patch(patch) {\n\t\t\tif (patch != klp_transition_patch)\n\t\t\t\tpatch->forced = true;\n\t\t}\n\t}\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}