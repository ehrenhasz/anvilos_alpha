{
  "module_name": "workqueue.c",
  "hash_id": "3b9d6544324cf32214383c33d50f3c60ad036f6c4201d936e2339a8f06f40fa7",
  "original_prompt": "Ingested from linux-6.6.14/kernel/workqueue.c",
  "human_readable_source": "\n \n\n#include <linux/export.h>\n#include <linux/kernel.h>\n#include <linux/sched.h>\n#include <linux/init.h>\n#include <linux/signal.h>\n#include <linux/completion.h>\n#include <linux/workqueue.h>\n#include <linux/slab.h>\n#include <linux/cpu.h>\n#include <linux/notifier.h>\n#include <linux/kthread.h>\n#include <linux/hardirq.h>\n#include <linux/mempolicy.h>\n#include <linux/freezer.h>\n#include <linux/debug_locks.h>\n#include <linux/lockdep.h>\n#include <linux/idr.h>\n#include <linux/jhash.h>\n#include <linux/hashtable.h>\n#include <linux/rculist.h>\n#include <linux/nodemask.h>\n#include <linux/moduleparam.h>\n#include <linux/uaccess.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/debug.h>\n#include <linux/nmi.h>\n#include <linux/kvm_para.h>\n#include <linux/delay.h>\n\n#include \"workqueue_internal.h\"\n\nenum {\n\t \n\tPOOL_MANAGER_ACTIVE\t= 1 << 0,\t \n\tPOOL_DISASSOCIATED\t= 1 << 2,\t \n\n\t \n\tWORKER_DIE\t\t= 1 << 1,\t \n\tWORKER_IDLE\t\t= 1 << 2,\t \n\tWORKER_PREP\t\t= 1 << 3,\t \n\tWORKER_CPU_INTENSIVE\t= 1 << 6,\t \n\tWORKER_UNBOUND\t\t= 1 << 7,\t \n\tWORKER_REBOUND\t\t= 1 << 8,\t \n\n\tWORKER_NOT_RUNNING\t= WORKER_PREP | WORKER_CPU_INTENSIVE |\n\t\t\t\t  WORKER_UNBOUND | WORKER_REBOUND,\n\n\tNR_STD_WORKER_POOLS\t= 2,\t\t \n\n\tUNBOUND_POOL_HASH_ORDER\t= 6,\t\t \n\tBUSY_WORKER_HASH_ORDER\t= 6,\t\t \n\n\tMAX_IDLE_WORKERS_RATIO\t= 4,\t\t \n\tIDLE_WORKER_TIMEOUT\t= 300 * HZ,\t \n\n\tMAYDAY_INITIAL_TIMEOUT  = HZ / 100 >= 2 ? HZ / 100 : 2,\n\t\t\t\t\t\t \n\tMAYDAY_INTERVAL\t\t= HZ / 10,\t \n\tCREATE_COOLDOWN\t\t= HZ,\t\t \n\n\t \n\tRESCUER_NICE_LEVEL\t= MIN_NICE,\n\tHIGHPRI_NICE_LEVEL\t= MIN_NICE,\n\n\tWQ_NAME_LEN\t\t= 24,\n};\n\n \n\n \n\nstruct worker_pool {\n\traw_spinlock_t\t\tlock;\t\t \n\tint\t\t\tcpu;\t\t \n\tint\t\t\tnode;\t\t \n\tint\t\t\tid;\t\t \n\tunsigned int\t\tflags;\t\t \n\n\tunsigned long\t\twatchdog_ts;\t \n\tbool\t\t\tcpu_stall;\t \n\n\t \n\tint\t\t\tnr_running;\n\n\tstruct list_head\tworklist;\t \n\n\tint\t\t\tnr_workers;\t \n\tint\t\t\tnr_idle;\t \n\n\tstruct list_head\tidle_list;\t \n\tstruct timer_list\tidle_timer;\t \n\tstruct work_struct      idle_cull_work;  \n\n\tstruct timer_list\tmayday_timer;\t   \n\n\t \n\tDECLARE_HASHTABLE(busy_hash, BUSY_WORKER_HASH_ORDER);\n\t\t\t\t\t\t \n\n\tstruct worker\t\t*manager;\t \n\tstruct list_head\tworkers;\t \n\tstruct list_head        dying_workers;   \n\tstruct completion\t*detach_completion;  \n\n\tstruct ida\t\tworker_ida;\t \n\n\tstruct workqueue_attrs\t*attrs;\t\t \n\tstruct hlist_node\thash_node;\t \n\tint\t\t\trefcnt;\t\t \n\n\t \n\tstruct rcu_head\t\trcu;\n};\n\n \nenum pool_workqueue_stats {\n\tPWQ_STAT_STARTED,\t \n\tPWQ_STAT_COMPLETED,\t \n\tPWQ_STAT_CPU_TIME,\t \n\tPWQ_STAT_CPU_INTENSIVE,\t \n\tPWQ_STAT_CM_WAKEUP,\t \n\tPWQ_STAT_REPATRIATED,\t \n\tPWQ_STAT_MAYDAY,\t \n\tPWQ_STAT_RESCUED,\t \n\n\tPWQ_NR_STATS,\n};\n\n \nstruct pool_workqueue {\n\tstruct worker_pool\t*pool;\t\t \n\tstruct workqueue_struct *wq;\t\t \n\tint\t\t\twork_color;\t \n\tint\t\t\tflush_color;\t \n\tint\t\t\trefcnt;\t\t \n\tint\t\t\tnr_in_flight[WORK_NR_COLORS];\n\t\t\t\t\t\t \n\n\t \n\tint\t\t\tnr_active;\t \n\tint\t\t\tmax_active;\t \n\tstruct list_head\tinactive_works;\t \n\tstruct list_head\tpwqs_node;\t \n\tstruct list_head\tmayday_node;\t \n\n\tu64\t\t\tstats[PWQ_NR_STATS];\n\n\t \n\tstruct kthread_work\trelease_work;\n\tstruct rcu_head\t\trcu;\n} __aligned(1 << WORK_STRUCT_FLAG_BITS);\n\n \nstruct wq_flusher {\n\tstruct list_head\tlist;\t\t \n\tint\t\t\tflush_color;\t \n\tstruct completion\tdone;\t\t \n};\n\nstruct wq_device;\n\n \nstruct workqueue_struct {\n\tstruct list_head\tpwqs;\t\t \n\tstruct list_head\tlist;\t\t \n\n\tstruct mutex\t\tmutex;\t\t \n\tint\t\t\twork_color;\t \n\tint\t\t\tflush_color;\t \n\tatomic_t\t\tnr_pwqs_to_flush;  \n\tstruct wq_flusher\t*first_flusher;\t \n\tstruct list_head\tflusher_queue;\t \n\tstruct list_head\tflusher_overflow;  \n\n\tstruct list_head\tmaydays;\t \n\tstruct worker\t\t*rescuer;\t \n\n\tint\t\t\tnr_drainers;\t \n\tint\t\t\tsaved_max_active;  \n\n\tstruct workqueue_attrs\t*unbound_attrs;\t \n\tstruct pool_workqueue\t*dfl_pwq;\t \n\n#ifdef CONFIG_SYSFS\n\tstruct wq_device\t*wq_dev;\t \n#endif\n#ifdef CONFIG_LOCKDEP\n\tchar\t\t\t*lock_name;\n\tstruct lock_class_key\tkey;\n\tstruct lockdep_map\tlockdep_map;\n#endif\n\tchar\t\t\tname[WQ_NAME_LEN];  \n\n\t \n\tstruct rcu_head\t\trcu;\n\n\t \n\tunsigned int\t\tflags ____cacheline_aligned;  \n\tstruct pool_workqueue __percpu __rcu **cpu_pwq;  \n};\n\nstatic struct kmem_cache *pwq_cache;\n\n \nstruct wq_pod_type {\n\tint\t\t\tnr_pods;\t \n\tcpumask_var_t\t\t*pod_cpus;\t \n\tint\t\t\t*pod_node;\t \n\tint\t\t\t*cpu_pod;\t \n};\n\nstatic struct wq_pod_type wq_pod_types[WQ_AFFN_NR_TYPES];\nstatic enum wq_affn_scope wq_affn_dfl = WQ_AFFN_CACHE;\n\nstatic const char *wq_affn_names[WQ_AFFN_NR_TYPES] = {\n\t[WQ_AFFN_DFL]\t\t\t= \"default\",\n\t[WQ_AFFN_CPU]\t\t\t= \"cpu\",\n\t[WQ_AFFN_SMT]\t\t\t= \"smt\",\n\t[WQ_AFFN_CACHE]\t\t\t= \"cache\",\n\t[WQ_AFFN_NUMA]\t\t\t= \"numa\",\n\t[WQ_AFFN_SYSTEM]\t\t= \"system\",\n};\n\n \nstatic unsigned long wq_cpu_intensive_thresh_us = ULONG_MAX;\nmodule_param_named(cpu_intensive_thresh_us, wq_cpu_intensive_thresh_us, ulong, 0644);\n\n \nstatic bool wq_power_efficient = IS_ENABLED(CONFIG_WQ_POWER_EFFICIENT_DEFAULT);\nmodule_param_named(power_efficient, wq_power_efficient, bool, 0444);\n\nstatic bool wq_online;\t\t\t \n\n \nstatic struct workqueue_attrs *wq_update_pod_attrs_buf;\n\nstatic DEFINE_MUTEX(wq_pool_mutex);\t \nstatic DEFINE_MUTEX(wq_pool_attach_mutex);  \nstatic DEFINE_RAW_SPINLOCK(wq_mayday_lock);\t \n \nstatic struct rcuwait manager_wait = __RCUWAIT_INITIALIZER(manager_wait);\n\nstatic LIST_HEAD(workqueues);\t\t \nstatic bool workqueue_freezing;\t\t \n\n \nstatic cpumask_var_t wq_unbound_cpumask;\n\n \nstatic struct cpumask wq_cmdline_cpumask __initdata;\n\n \nstatic DEFINE_PER_CPU(int, wq_rr_cpu_last);\n\n \n#ifdef CONFIG_DEBUG_WQ_FORCE_RR_CPU\nstatic bool wq_debug_force_rr_cpu = true;\n#else\nstatic bool wq_debug_force_rr_cpu = false;\n#endif\nmodule_param_named(debug_force_rr_cpu, wq_debug_force_rr_cpu, bool, 0644);\n\n \nstatic DEFINE_PER_CPU_SHARED_ALIGNED(struct worker_pool [NR_STD_WORKER_POOLS], cpu_worker_pools);\n\nstatic DEFINE_IDR(worker_pool_idr);\t \n\n \nstatic DEFINE_HASHTABLE(unbound_pool_hash, UNBOUND_POOL_HASH_ORDER);\n\n \nstatic struct workqueue_attrs *unbound_std_wq_attrs[NR_STD_WORKER_POOLS];\n\n \nstatic struct workqueue_attrs *ordered_wq_attrs[NR_STD_WORKER_POOLS];\n\n \nstatic struct kthread_worker *pwq_release_worker;\n\nstruct workqueue_struct *system_wq __read_mostly;\nEXPORT_SYMBOL(system_wq);\nstruct workqueue_struct *system_highpri_wq __read_mostly;\nEXPORT_SYMBOL_GPL(system_highpri_wq);\nstruct workqueue_struct *system_long_wq __read_mostly;\nEXPORT_SYMBOL_GPL(system_long_wq);\nstruct workqueue_struct *system_unbound_wq __read_mostly;\nEXPORT_SYMBOL_GPL(system_unbound_wq);\nstruct workqueue_struct *system_freezable_wq __read_mostly;\nEXPORT_SYMBOL_GPL(system_freezable_wq);\nstruct workqueue_struct *system_power_efficient_wq __read_mostly;\nEXPORT_SYMBOL_GPL(system_power_efficient_wq);\nstruct workqueue_struct *system_freezable_power_efficient_wq __read_mostly;\nEXPORT_SYMBOL_GPL(system_freezable_power_efficient_wq);\n\nstatic int worker_thread(void *__worker);\nstatic void workqueue_sysfs_unregister(struct workqueue_struct *wq);\nstatic void show_pwq(struct pool_workqueue *pwq);\nstatic void show_one_worker_pool(struct worker_pool *pool);\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/workqueue.h>\n\n#define assert_rcu_or_pool_mutex()\t\t\t\t\t\\\n\tRCU_LOCKDEP_WARN(!rcu_read_lock_held() &&\t\t\t\\\n\t\t\t !lockdep_is_held(&wq_pool_mutex),\t\t\\\n\t\t\t \"RCU or wq_pool_mutex should be held\")\n\n#define assert_rcu_or_wq_mutex_or_pool_mutex(wq)\t\t\t\\\n\tRCU_LOCKDEP_WARN(!rcu_read_lock_held() &&\t\t\t\\\n\t\t\t !lockdep_is_held(&wq->mutex) &&\t\t\\\n\t\t\t !lockdep_is_held(&wq_pool_mutex),\t\t\\\n\t\t\t \"RCU, wq->mutex or wq_pool_mutex should be held\")\n\n#define for_each_cpu_worker_pool(pool, cpu)\t\t\t\t\\\n\tfor ((pool) = &per_cpu(cpu_worker_pools, cpu)[0];\t\t\\\n\t     (pool) < &per_cpu(cpu_worker_pools, cpu)[NR_STD_WORKER_POOLS]; \\\n\t     (pool)++)\n\n \n#define for_each_pool(pool, pi)\t\t\t\t\t\t\\\n\tidr_for_each_entry(&worker_pool_idr, pool, pi)\t\t\t\\\n\t\tif (({ assert_rcu_or_pool_mutex(); false; })) { }\t\\\n\t\telse\n\n \n#define for_each_pool_worker(worker, pool)\t\t\t\t\\\n\tlist_for_each_entry((worker), &(pool)->workers, node)\t\t\\\n\t\tif (({ lockdep_assert_held(&wq_pool_attach_mutex); false; })) { } \\\n\t\telse\n\n \n#define for_each_pwq(pwq, wq)\t\t\t\t\t\t\\\n\tlist_for_each_entry_rcu((pwq), &(wq)->pwqs, pwqs_node,\t\t\\\n\t\t\t\t lockdep_is_held(&(wq->mutex)))\n\n#ifdef CONFIG_DEBUG_OBJECTS_WORK\n\nstatic const struct debug_obj_descr work_debug_descr;\n\nstatic void *work_debug_hint(void *addr)\n{\n\treturn ((struct work_struct *) addr)->func;\n}\n\nstatic bool work_is_static_object(void *addr)\n{\n\tstruct work_struct *work = addr;\n\n\treturn test_bit(WORK_STRUCT_STATIC_BIT, work_data_bits(work));\n}\n\n \nstatic bool work_fixup_init(void *addr, enum debug_obj_state state)\n{\n\tstruct work_struct *work = addr;\n\n\tswitch (state) {\n\tcase ODEBUG_STATE_ACTIVE:\n\t\tcancel_work_sync(work);\n\t\tdebug_object_init(work, &work_debug_descr);\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n \nstatic bool work_fixup_free(void *addr, enum debug_obj_state state)\n{\n\tstruct work_struct *work = addr;\n\n\tswitch (state) {\n\tcase ODEBUG_STATE_ACTIVE:\n\t\tcancel_work_sync(work);\n\t\tdebug_object_free(work, &work_debug_descr);\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic const struct debug_obj_descr work_debug_descr = {\n\t.name\t\t= \"work_struct\",\n\t.debug_hint\t= work_debug_hint,\n\t.is_static_object = work_is_static_object,\n\t.fixup_init\t= work_fixup_init,\n\t.fixup_free\t= work_fixup_free,\n};\n\nstatic inline void debug_work_activate(struct work_struct *work)\n{\n\tdebug_object_activate(work, &work_debug_descr);\n}\n\nstatic inline void debug_work_deactivate(struct work_struct *work)\n{\n\tdebug_object_deactivate(work, &work_debug_descr);\n}\n\nvoid __init_work(struct work_struct *work, int onstack)\n{\n\tif (onstack)\n\t\tdebug_object_init_on_stack(work, &work_debug_descr);\n\telse\n\t\tdebug_object_init(work, &work_debug_descr);\n}\nEXPORT_SYMBOL_GPL(__init_work);\n\nvoid destroy_work_on_stack(struct work_struct *work)\n{\n\tdebug_object_free(work, &work_debug_descr);\n}\nEXPORT_SYMBOL_GPL(destroy_work_on_stack);\n\nvoid destroy_delayed_work_on_stack(struct delayed_work *work)\n{\n\tdestroy_timer_on_stack(&work->timer);\n\tdebug_object_free(&work->work, &work_debug_descr);\n}\nEXPORT_SYMBOL_GPL(destroy_delayed_work_on_stack);\n\n#else\nstatic inline void debug_work_activate(struct work_struct *work) { }\nstatic inline void debug_work_deactivate(struct work_struct *work) { }\n#endif\n\n \nstatic int worker_pool_assign_id(struct worker_pool *pool)\n{\n\tint ret;\n\n\tlockdep_assert_held(&wq_pool_mutex);\n\n\tret = idr_alloc(&worker_pool_idr, pool, 0, WORK_OFFQ_POOL_NONE,\n\t\t\tGFP_KERNEL);\n\tif (ret >= 0) {\n\t\tpool->id = ret;\n\t\treturn 0;\n\t}\n\treturn ret;\n}\n\nstatic unsigned int work_color_to_flags(int color)\n{\n\treturn color << WORK_STRUCT_COLOR_SHIFT;\n}\n\nstatic int get_work_color(unsigned long work_data)\n{\n\treturn (work_data >> WORK_STRUCT_COLOR_SHIFT) &\n\t\t((1 << WORK_STRUCT_COLOR_BITS) - 1);\n}\n\nstatic int work_next_color(int color)\n{\n\treturn (color + 1) % WORK_NR_COLORS;\n}\n\n \nstatic inline void set_work_data(struct work_struct *work, unsigned long data,\n\t\t\t\t unsigned long flags)\n{\n\tWARN_ON_ONCE(!work_pending(work));\n\tatomic_long_set(&work->data, data | flags | work_static(work));\n}\n\nstatic void set_work_pwq(struct work_struct *work, struct pool_workqueue *pwq,\n\t\t\t unsigned long extra_flags)\n{\n\tset_work_data(work, (unsigned long)pwq,\n\t\t      WORK_STRUCT_PENDING | WORK_STRUCT_PWQ | extra_flags);\n}\n\nstatic void set_work_pool_and_keep_pending(struct work_struct *work,\n\t\t\t\t\t   int pool_id)\n{\n\tset_work_data(work, (unsigned long)pool_id << WORK_OFFQ_POOL_SHIFT,\n\t\t      WORK_STRUCT_PENDING);\n}\n\nstatic void set_work_pool_and_clear_pending(struct work_struct *work,\n\t\t\t\t\t    int pool_id)\n{\n\t \n\tsmp_wmb();\n\tset_work_data(work, (unsigned long)pool_id << WORK_OFFQ_POOL_SHIFT, 0);\n\t \n\tsmp_mb();\n}\n\nstatic void clear_work_data(struct work_struct *work)\n{\n\tsmp_wmb();\t \n\tset_work_data(work, WORK_STRUCT_NO_POOL, 0);\n}\n\nstatic inline struct pool_workqueue *work_struct_pwq(unsigned long data)\n{\n\treturn (struct pool_workqueue *)(data & WORK_STRUCT_WQ_DATA_MASK);\n}\n\nstatic struct pool_workqueue *get_work_pwq(struct work_struct *work)\n{\n\tunsigned long data = atomic_long_read(&work->data);\n\n\tif (data & WORK_STRUCT_PWQ)\n\t\treturn work_struct_pwq(data);\n\telse\n\t\treturn NULL;\n}\n\n \nstatic struct worker_pool *get_work_pool(struct work_struct *work)\n{\n\tunsigned long data = atomic_long_read(&work->data);\n\tint pool_id;\n\n\tassert_rcu_or_pool_mutex();\n\n\tif (data & WORK_STRUCT_PWQ)\n\t\treturn work_struct_pwq(data)->pool;\n\n\tpool_id = data >> WORK_OFFQ_POOL_SHIFT;\n\tif (pool_id == WORK_OFFQ_POOL_NONE)\n\t\treturn NULL;\n\n\treturn idr_find(&worker_pool_idr, pool_id);\n}\n\n \nstatic int get_work_pool_id(struct work_struct *work)\n{\n\tunsigned long data = atomic_long_read(&work->data);\n\n\tif (data & WORK_STRUCT_PWQ)\n\t\treturn work_struct_pwq(data)->pool->id;\n\n\treturn data >> WORK_OFFQ_POOL_SHIFT;\n}\n\nstatic void mark_work_canceling(struct work_struct *work)\n{\n\tunsigned long pool_id = get_work_pool_id(work);\n\n\tpool_id <<= WORK_OFFQ_POOL_SHIFT;\n\tset_work_data(work, pool_id | WORK_OFFQ_CANCELING, WORK_STRUCT_PENDING);\n}\n\nstatic bool work_is_canceling(struct work_struct *work)\n{\n\tunsigned long data = atomic_long_read(&work->data);\n\n\treturn !(data & WORK_STRUCT_PWQ) && (data & WORK_OFFQ_CANCELING);\n}\n\n \n\n \nstatic bool need_more_worker(struct worker_pool *pool)\n{\n\treturn !list_empty(&pool->worklist) && !pool->nr_running;\n}\n\n \nstatic bool may_start_working(struct worker_pool *pool)\n{\n\treturn pool->nr_idle;\n}\n\n \nstatic bool keep_working(struct worker_pool *pool)\n{\n\treturn !list_empty(&pool->worklist) && (pool->nr_running <= 1);\n}\n\n \nstatic bool need_to_create_worker(struct worker_pool *pool)\n{\n\treturn need_more_worker(pool) && !may_start_working(pool);\n}\n\n \nstatic bool too_many_workers(struct worker_pool *pool)\n{\n\tbool managing = pool->flags & POOL_MANAGER_ACTIVE;\n\tint nr_idle = pool->nr_idle + managing;  \n\tint nr_busy = pool->nr_workers - nr_idle;\n\n\treturn nr_idle > 2 && (nr_idle - 2) * MAX_IDLE_WORKERS_RATIO >= nr_busy;\n}\n\n \nstatic inline void worker_set_flags(struct worker *worker, unsigned int flags)\n{\n\tstruct worker_pool *pool = worker->pool;\n\n\tlockdep_assert_held(&pool->lock);\n\n\t \n\tif ((flags & WORKER_NOT_RUNNING) &&\n\t    !(worker->flags & WORKER_NOT_RUNNING)) {\n\t\tpool->nr_running--;\n\t}\n\n\tworker->flags |= flags;\n}\n\n \nstatic inline void worker_clr_flags(struct worker *worker, unsigned int flags)\n{\n\tstruct worker_pool *pool = worker->pool;\n\tunsigned int oflags = worker->flags;\n\n\tlockdep_assert_held(&pool->lock);\n\n\tworker->flags &= ~flags;\n\n\t \n\tif ((flags & WORKER_NOT_RUNNING) && (oflags & WORKER_NOT_RUNNING))\n\t\tif (!(worker->flags & WORKER_NOT_RUNNING))\n\t\t\tpool->nr_running++;\n}\n\n \nstatic struct worker *first_idle_worker(struct worker_pool *pool)\n{\n\tif (unlikely(list_empty(&pool->idle_list)))\n\t\treturn NULL;\n\n\treturn list_first_entry(&pool->idle_list, struct worker, entry);\n}\n\n \nstatic void worker_enter_idle(struct worker *worker)\n{\n\tstruct worker_pool *pool = worker->pool;\n\n\tif (WARN_ON_ONCE(worker->flags & WORKER_IDLE) ||\n\t    WARN_ON_ONCE(!list_empty(&worker->entry) &&\n\t\t\t (worker->hentry.next || worker->hentry.pprev)))\n\t\treturn;\n\n\t \n\tworker->flags |= WORKER_IDLE;\n\tpool->nr_idle++;\n\tworker->last_active = jiffies;\n\n\t \n\tlist_add(&worker->entry, &pool->idle_list);\n\n\tif (too_many_workers(pool) && !timer_pending(&pool->idle_timer))\n\t\tmod_timer(&pool->idle_timer, jiffies + IDLE_WORKER_TIMEOUT);\n\n\t \n\tWARN_ON_ONCE(pool->nr_workers == pool->nr_idle && pool->nr_running);\n}\n\n \nstatic void worker_leave_idle(struct worker *worker)\n{\n\tstruct worker_pool *pool = worker->pool;\n\n\tif (WARN_ON_ONCE(!(worker->flags & WORKER_IDLE)))\n\t\treturn;\n\tworker_clr_flags(worker, WORKER_IDLE);\n\tpool->nr_idle--;\n\tlist_del_init(&worker->entry);\n}\n\n \nstatic struct worker *find_worker_executing_work(struct worker_pool *pool,\n\t\t\t\t\t\t struct work_struct *work)\n{\n\tstruct worker *worker;\n\n\thash_for_each_possible(pool->busy_hash, worker, hentry,\n\t\t\t       (unsigned long)work)\n\t\tif (worker->current_work == work &&\n\t\t    worker->current_func == work->func)\n\t\t\treturn worker;\n\n\treturn NULL;\n}\n\n \nstatic void move_linked_works(struct work_struct *work, struct list_head *head,\n\t\t\t      struct work_struct **nextp)\n{\n\tstruct work_struct *n;\n\n\t \n\tlist_for_each_entry_safe_from(work, n, NULL, entry) {\n\t\tlist_move_tail(&work->entry, head);\n\t\tif (!(*work_data_bits(work) & WORK_STRUCT_LINKED))\n\t\t\tbreak;\n\t}\n\n\t \n\tif (nextp)\n\t\t*nextp = n;\n}\n\n \nstatic bool assign_work(struct work_struct *work, struct worker *worker,\n\t\t\tstruct work_struct **nextp)\n{\n\tstruct worker_pool *pool = worker->pool;\n\tstruct worker *collision;\n\n\tlockdep_assert_held(&pool->lock);\n\n\t \n\tcollision = find_worker_executing_work(pool, work);\n\tif (unlikely(collision)) {\n\t\tmove_linked_works(work, &collision->scheduled, nextp);\n\t\treturn false;\n\t}\n\n\tmove_linked_works(work, &worker->scheduled, nextp);\n\treturn true;\n}\n\n \nstatic bool kick_pool(struct worker_pool *pool)\n{\n\tstruct worker *worker = first_idle_worker(pool);\n\tstruct task_struct *p;\n\n\tlockdep_assert_held(&pool->lock);\n\n\tif (!need_more_worker(pool) || !worker)\n\t\treturn false;\n\n\tp = worker->task;\n\n#ifdef CONFIG_SMP\n\t \n\tif (!pool->attrs->affn_strict &&\n\t    !cpumask_test_cpu(p->wake_cpu, pool->attrs->__pod_cpumask)) {\n\t\tstruct work_struct *work = list_first_entry(&pool->worklist,\n\t\t\t\t\t\tstruct work_struct, entry);\n\t\tp->wake_cpu = cpumask_any_distribute(pool->attrs->__pod_cpumask);\n\t\tget_work_pwq(work)->stats[PWQ_STAT_REPATRIATED]++;\n\t}\n#endif\n\twake_up_process(p);\n\treturn true;\n}\n\n#ifdef CONFIG_WQ_CPU_INTENSIVE_REPORT\n\n \n#define WCI_MAX_ENTS 128\n\nstruct wci_ent {\n\twork_func_t\t\tfunc;\n\tatomic64_t\t\tcnt;\n\tstruct hlist_node\thash_node;\n};\n\nstatic struct wci_ent wci_ents[WCI_MAX_ENTS];\nstatic int wci_nr_ents;\nstatic DEFINE_RAW_SPINLOCK(wci_lock);\nstatic DEFINE_HASHTABLE(wci_hash, ilog2(WCI_MAX_ENTS));\n\nstatic struct wci_ent *wci_find_ent(work_func_t func)\n{\n\tstruct wci_ent *ent;\n\n\thash_for_each_possible_rcu(wci_hash, ent, hash_node,\n\t\t\t\t   (unsigned long)func) {\n\t\tif (ent->func == func)\n\t\t\treturn ent;\n\t}\n\treturn NULL;\n}\n\nstatic void wq_cpu_intensive_report(work_func_t func)\n{\n\tstruct wci_ent *ent;\n\nrestart:\n\tent = wci_find_ent(func);\n\tif (ent) {\n\t\tu64 cnt;\n\n\t\t \n\t\tcnt = atomic64_inc_return_relaxed(&ent->cnt);\n\t\tif (cnt >= 4 && is_power_of_2(cnt))\n\t\t\tprintk_deferred(KERN_WARNING \"workqueue: %ps hogged CPU for >%luus %llu times, consider switching to WQ_UNBOUND\\n\",\n\t\t\t\t\tent->func, wq_cpu_intensive_thresh_us,\n\t\t\t\t\tatomic64_read(&ent->cnt));\n\t\treturn;\n\t}\n\n\t \n\tif (wci_nr_ents >= WCI_MAX_ENTS)\n\t\treturn;\n\n\traw_spin_lock(&wci_lock);\n\n\tif (wci_nr_ents >= WCI_MAX_ENTS) {\n\t\traw_spin_unlock(&wci_lock);\n\t\treturn;\n\t}\n\n\tif (wci_find_ent(func)) {\n\t\traw_spin_unlock(&wci_lock);\n\t\tgoto restart;\n\t}\n\n\tent = &wci_ents[wci_nr_ents++];\n\tent->func = func;\n\tatomic64_set(&ent->cnt, 1);\n\thash_add_rcu(wci_hash, &ent->hash_node, (unsigned long)func);\n\n\traw_spin_unlock(&wci_lock);\n}\n\n#else\t \nstatic void wq_cpu_intensive_report(work_func_t func) {}\n#endif\t \n\n \nvoid wq_worker_running(struct task_struct *task)\n{\n\tstruct worker *worker = kthread_data(task);\n\n\tif (!READ_ONCE(worker->sleeping))\n\t\treturn;\n\n\t \n\tpreempt_disable();\n\tif (!(worker->flags & WORKER_NOT_RUNNING))\n\t\tworker->pool->nr_running++;\n\tpreempt_enable();\n\n\t \n\tworker->current_at = worker->task->se.sum_exec_runtime;\n\n\tWRITE_ONCE(worker->sleeping, 0);\n}\n\n \nvoid wq_worker_sleeping(struct task_struct *task)\n{\n\tstruct worker *worker = kthread_data(task);\n\tstruct worker_pool *pool;\n\n\t \n\tif (worker->flags & WORKER_NOT_RUNNING)\n\t\treturn;\n\n\tpool = worker->pool;\n\n\t \n\tif (READ_ONCE(worker->sleeping))\n\t\treturn;\n\n\tWRITE_ONCE(worker->sleeping, 1);\n\traw_spin_lock_irq(&pool->lock);\n\n\t \n\tif (worker->flags & WORKER_NOT_RUNNING) {\n\t\traw_spin_unlock_irq(&pool->lock);\n\t\treturn;\n\t}\n\n\tpool->nr_running--;\n\tif (kick_pool(pool))\n\t\tworker->current_pwq->stats[PWQ_STAT_CM_WAKEUP]++;\n\n\traw_spin_unlock_irq(&pool->lock);\n}\n\n \nvoid wq_worker_tick(struct task_struct *task)\n{\n\tstruct worker *worker = kthread_data(task);\n\tstruct pool_workqueue *pwq = worker->current_pwq;\n\tstruct worker_pool *pool = worker->pool;\n\n\tif (!pwq)\n\t\treturn;\n\n\tpwq->stats[PWQ_STAT_CPU_TIME] += TICK_USEC;\n\n\tif (!wq_cpu_intensive_thresh_us)\n\t\treturn;\n\n\t \n\tif ((worker->flags & WORKER_NOT_RUNNING) || READ_ONCE(worker->sleeping) ||\n\t    worker->task->se.sum_exec_runtime - worker->current_at <\n\t    wq_cpu_intensive_thresh_us * NSEC_PER_USEC)\n\t\treturn;\n\n\traw_spin_lock(&pool->lock);\n\n\tworker_set_flags(worker, WORKER_CPU_INTENSIVE);\n\twq_cpu_intensive_report(worker->current_func);\n\tpwq->stats[PWQ_STAT_CPU_INTENSIVE]++;\n\n\tif (kick_pool(pool))\n\t\tpwq->stats[PWQ_STAT_CM_WAKEUP]++;\n\n\traw_spin_unlock(&pool->lock);\n}\n\n \nwork_func_t wq_worker_last_func(struct task_struct *task)\n{\n\tstruct worker *worker = kthread_data(task);\n\n\treturn worker->last_func;\n}\n\n \nstatic void get_pwq(struct pool_workqueue *pwq)\n{\n\tlockdep_assert_held(&pwq->pool->lock);\n\tWARN_ON_ONCE(pwq->refcnt <= 0);\n\tpwq->refcnt++;\n}\n\n \nstatic void put_pwq(struct pool_workqueue *pwq)\n{\n\tlockdep_assert_held(&pwq->pool->lock);\n\tif (likely(--pwq->refcnt))\n\t\treturn;\n\t \n\tkthread_queue_work(pwq_release_worker, &pwq->release_work);\n}\n\n \nstatic void put_pwq_unlocked(struct pool_workqueue *pwq)\n{\n\tif (pwq) {\n\t\t \n\t\traw_spin_lock_irq(&pwq->pool->lock);\n\t\tput_pwq(pwq);\n\t\traw_spin_unlock_irq(&pwq->pool->lock);\n\t}\n}\n\nstatic void pwq_activate_inactive_work(struct work_struct *work)\n{\n\tstruct pool_workqueue *pwq = get_work_pwq(work);\n\n\ttrace_workqueue_activate_work(work);\n\tif (list_empty(&pwq->pool->worklist))\n\t\tpwq->pool->watchdog_ts = jiffies;\n\tmove_linked_works(work, &pwq->pool->worklist, NULL);\n\t__clear_bit(WORK_STRUCT_INACTIVE_BIT, work_data_bits(work));\n\tpwq->nr_active++;\n}\n\nstatic void pwq_activate_first_inactive(struct pool_workqueue *pwq)\n{\n\tstruct work_struct *work = list_first_entry(&pwq->inactive_works,\n\t\t\t\t\t\t    struct work_struct, entry);\n\n\tpwq_activate_inactive_work(work);\n}\n\n \nstatic void pwq_dec_nr_in_flight(struct pool_workqueue *pwq, unsigned long work_data)\n{\n\tint color = get_work_color(work_data);\n\n\tif (!(work_data & WORK_STRUCT_INACTIVE)) {\n\t\tpwq->nr_active--;\n\t\tif (!list_empty(&pwq->inactive_works)) {\n\t\t\t \n\t\t\tif (pwq->nr_active < pwq->max_active)\n\t\t\t\tpwq_activate_first_inactive(pwq);\n\t\t}\n\t}\n\n\tpwq->nr_in_flight[color]--;\n\n\t \n\tif (likely(pwq->flush_color != color))\n\t\tgoto out_put;\n\n\t \n\tif (pwq->nr_in_flight[color])\n\t\tgoto out_put;\n\n\t \n\tpwq->flush_color = -1;\n\n\t \n\tif (atomic_dec_and_test(&pwq->wq->nr_pwqs_to_flush))\n\t\tcomplete(&pwq->wq->first_flusher->done);\nout_put:\n\tput_pwq(pwq);\n}\n\n \nstatic int try_to_grab_pending(struct work_struct *work, bool is_dwork,\n\t\t\t       unsigned long *flags)\n{\n\tstruct worker_pool *pool;\n\tstruct pool_workqueue *pwq;\n\n\tlocal_irq_save(*flags);\n\n\t \n\tif (is_dwork) {\n\t\tstruct delayed_work *dwork = to_delayed_work(work);\n\n\t\t \n\t\tif (likely(del_timer(&dwork->timer)))\n\t\t\treturn 1;\n\t}\n\n\t \n\tif (!test_and_set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work)))\n\t\treturn 0;\n\n\trcu_read_lock();\n\t \n\tpool = get_work_pool(work);\n\tif (!pool)\n\t\tgoto fail;\n\n\traw_spin_lock(&pool->lock);\n\t \n\tpwq = get_work_pwq(work);\n\tif (pwq && pwq->pool == pool) {\n\t\tdebug_work_deactivate(work);\n\n\t\t \n\t\tif (*work_data_bits(work) & WORK_STRUCT_INACTIVE)\n\t\t\tpwq_activate_inactive_work(work);\n\n\t\tlist_del_init(&work->entry);\n\t\tpwq_dec_nr_in_flight(pwq, *work_data_bits(work));\n\n\t\t \n\t\tset_work_pool_and_keep_pending(work, pool->id);\n\n\t\traw_spin_unlock(&pool->lock);\n\t\trcu_read_unlock();\n\t\treturn 1;\n\t}\n\traw_spin_unlock(&pool->lock);\nfail:\n\trcu_read_unlock();\n\tlocal_irq_restore(*flags);\n\tif (work_is_canceling(work))\n\t\treturn -ENOENT;\n\tcpu_relax();\n\treturn -EAGAIN;\n}\n\n \nstatic void insert_work(struct pool_workqueue *pwq, struct work_struct *work,\n\t\t\tstruct list_head *head, unsigned int extra_flags)\n{\n\tdebug_work_activate(work);\n\n\t \n\tkasan_record_aux_stack_noalloc(work);\n\n\t \n\tset_work_pwq(work, pwq, extra_flags);\n\tlist_add_tail(&work->entry, head);\n\tget_pwq(pwq);\n}\n\n \nstatic bool is_chained_work(struct workqueue_struct *wq)\n{\n\tstruct worker *worker;\n\n\tworker = current_wq_worker();\n\t \n\treturn worker && worker->current_pwq->wq == wq;\n}\n\n \nstatic int wq_select_unbound_cpu(int cpu)\n{\n\tint new_cpu;\n\n\tif (likely(!wq_debug_force_rr_cpu)) {\n\t\tif (cpumask_test_cpu(cpu, wq_unbound_cpumask))\n\t\t\treturn cpu;\n\t} else {\n\t\tpr_warn_once(\"workqueue: round-robin CPU selection forced, expect performance impact\\n\");\n\t}\n\n\tnew_cpu = __this_cpu_read(wq_rr_cpu_last);\n\tnew_cpu = cpumask_next_and(new_cpu, wq_unbound_cpumask, cpu_online_mask);\n\tif (unlikely(new_cpu >= nr_cpu_ids)) {\n\t\tnew_cpu = cpumask_first_and(wq_unbound_cpumask, cpu_online_mask);\n\t\tif (unlikely(new_cpu >= nr_cpu_ids))\n\t\t\treturn cpu;\n\t}\n\t__this_cpu_write(wq_rr_cpu_last, new_cpu);\n\n\treturn new_cpu;\n}\n\nstatic void __queue_work(int cpu, struct workqueue_struct *wq,\n\t\t\t struct work_struct *work)\n{\n\tstruct pool_workqueue *pwq;\n\tstruct worker_pool *last_pool, *pool;\n\tunsigned int work_flags;\n\tunsigned int req_cpu = cpu;\n\n\t \n\tlockdep_assert_irqs_disabled();\n\n\n\t \n\tif (unlikely(wq->flags & (__WQ_DESTROYING | __WQ_DRAINING) &&\n\t\t     WARN_ON_ONCE(!is_chained_work(wq))))\n\t\treturn;\n\trcu_read_lock();\nretry:\n\t \n\tif (req_cpu == WORK_CPU_UNBOUND) {\n\t\tif (wq->flags & WQ_UNBOUND)\n\t\t\tcpu = wq_select_unbound_cpu(raw_smp_processor_id());\n\t\telse\n\t\t\tcpu = raw_smp_processor_id();\n\t}\n\n\tpwq = rcu_dereference(*per_cpu_ptr(wq->cpu_pwq, cpu));\n\tpool = pwq->pool;\n\n\t \n\tlast_pool = get_work_pool(work);\n\tif (last_pool && last_pool != pool) {\n\t\tstruct worker *worker;\n\n\t\traw_spin_lock(&last_pool->lock);\n\n\t\tworker = find_worker_executing_work(last_pool, work);\n\n\t\tif (worker && worker->current_pwq->wq == wq) {\n\t\t\tpwq = worker->current_pwq;\n\t\t\tpool = pwq->pool;\n\t\t\tWARN_ON_ONCE(pool != last_pool);\n\t\t} else {\n\t\t\t \n\t\t\traw_spin_unlock(&last_pool->lock);\n\t\t\traw_spin_lock(&pool->lock);\n\t\t}\n\t} else {\n\t\traw_spin_lock(&pool->lock);\n\t}\n\n\t \n\tif (unlikely(!pwq->refcnt)) {\n\t\tif (wq->flags & WQ_UNBOUND) {\n\t\t\traw_spin_unlock(&pool->lock);\n\t\t\tcpu_relax();\n\t\t\tgoto retry;\n\t\t}\n\t\t \n\t\tWARN_ONCE(true, \"workqueue: per-cpu pwq for %s on cpu%d has 0 refcnt\",\n\t\t\t  wq->name, cpu);\n\t}\n\n\t \n\ttrace_workqueue_queue_work(req_cpu, pwq, work);\n\n\tif (WARN_ON(!list_empty(&work->entry)))\n\t\tgoto out;\n\n\tpwq->nr_in_flight[pwq->work_color]++;\n\twork_flags = work_color_to_flags(pwq->work_color);\n\n\tif (likely(pwq->nr_active < pwq->max_active)) {\n\t\tif (list_empty(&pool->worklist))\n\t\t\tpool->watchdog_ts = jiffies;\n\n\t\ttrace_workqueue_activate_work(work);\n\t\tpwq->nr_active++;\n\t\tinsert_work(pwq, work, &pool->worklist, work_flags);\n\t\tkick_pool(pool);\n\t} else {\n\t\twork_flags |= WORK_STRUCT_INACTIVE;\n\t\tinsert_work(pwq, work, &pwq->inactive_works, work_flags);\n\t}\n\nout:\n\traw_spin_unlock(&pool->lock);\n\trcu_read_unlock();\n}\n\n \nbool queue_work_on(int cpu, struct workqueue_struct *wq,\n\t\t   struct work_struct *work)\n{\n\tbool ret = false;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\n\tif (!test_and_set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work))) {\n\t\t__queue_work(cpu, wq, work);\n\t\tret = true;\n\t}\n\n\tlocal_irq_restore(flags);\n\treturn ret;\n}\nEXPORT_SYMBOL(queue_work_on);\n\n \nstatic int select_numa_node_cpu(int node)\n{\n\tint cpu;\n\n\t \n\tif (node < 0 || node >= MAX_NUMNODES || !node_online(node))\n\t\treturn WORK_CPU_UNBOUND;\n\n\t \n\tcpu = raw_smp_processor_id();\n\tif (node == cpu_to_node(cpu))\n\t\treturn cpu;\n\n\t \n\tcpu = cpumask_any_and(cpumask_of_node(node), cpu_online_mask);\n\n\t \n\treturn cpu < nr_cpu_ids ? cpu : WORK_CPU_UNBOUND;\n}\n\n \nbool queue_work_node(int node, struct workqueue_struct *wq,\n\t\t     struct work_struct *work)\n{\n\tunsigned long flags;\n\tbool ret = false;\n\n\t \n\tWARN_ON_ONCE(!(wq->flags & WQ_UNBOUND));\n\n\tlocal_irq_save(flags);\n\n\tif (!test_and_set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work))) {\n\t\tint cpu = select_numa_node_cpu(node);\n\n\t\t__queue_work(cpu, wq, work);\n\t\tret = true;\n\t}\n\n\tlocal_irq_restore(flags);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(queue_work_node);\n\nvoid delayed_work_timer_fn(struct timer_list *t)\n{\n\tstruct delayed_work *dwork = from_timer(dwork, t, timer);\n\n\t \n\t__queue_work(dwork->cpu, dwork->wq, &dwork->work);\n}\nEXPORT_SYMBOL(delayed_work_timer_fn);\n\nstatic void __queue_delayed_work(int cpu, struct workqueue_struct *wq,\n\t\t\t\tstruct delayed_work *dwork, unsigned long delay)\n{\n\tstruct timer_list *timer = &dwork->timer;\n\tstruct work_struct *work = &dwork->work;\n\n\tWARN_ON_ONCE(!wq);\n\tWARN_ON_ONCE(timer->function != delayed_work_timer_fn);\n\tWARN_ON_ONCE(timer_pending(timer));\n\tWARN_ON_ONCE(!list_empty(&work->entry));\n\n\t \n\tif (!delay) {\n\t\t__queue_work(cpu, wq, &dwork->work);\n\t\treturn;\n\t}\n\n\tdwork->wq = wq;\n\tdwork->cpu = cpu;\n\ttimer->expires = jiffies + delay;\n\n\tif (unlikely(cpu != WORK_CPU_UNBOUND))\n\t\tadd_timer_on(timer, cpu);\n\telse\n\t\tadd_timer(timer);\n}\n\n \nbool queue_delayed_work_on(int cpu, struct workqueue_struct *wq,\n\t\t\t   struct delayed_work *dwork, unsigned long delay)\n{\n\tstruct work_struct *work = &dwork->work;\n\tbool ret = false;\n\tunsigned long flags;\n\n\t \n\tlocal_irq_save(flags);\n\n\tif (!test_and_set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work))) {\n\t\t__queue_delayed_work(cpu, wq, dwork, delay);\n\t\tret = true;\n\t}\n\n\tlocal_irq_restore(flags);\n\treturn ret;\n}\nEXPORT_SYMBOL(queue_delayed_work_on);\n\n \nbool mod_delayed_work_on(int cpu, struct workqueue_struct *wq,\n\t\t\t struct delayed_work *dwork, unsigned long delay)\n{\n\tunsigned long flags;\n\tint ret;\n\n\tdo {\n\t\tret = try_to_grab_pending(&dwork->work, true, &flags);\n\t} while (unlikely(ret == -EAGAIN));\n\n\tif (likely(ret >= 0)) {\n\t\t__queue_delayed_work(cpu, wq, dwork, delay);\n\t\tlocal_irq_restore(flags);\n\t}\n\n\t \n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(mod_delayed_work_on);\n\nstatic void rcu_work_rcufn(struct rcu_head *rcu)\n{\n\tstruct rcu_work *rwork = container_of(rcu, struct rcu_work, rcu);\n\n\t \n\tlocal_irq_disable();\n\t__queue_work(WORK_CPU_UNBOUND, rwork->wq, &rwork->work);\n\tlocal_irq_enable();\n}\n\n \nbool queue_rcu_work(struct workqueue_struct *wq, struct rcu_work *rwork)\n{\n\tstruct work_struct *work = &rwork->work;\n\n\tif (!test_and_set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work))) {\n\t\trwork->wq = wq;\n\t\tcall_rcu_hurry(&rwork->rcu, rcu_work_rcufn);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\nEXPORT_SYMBOL(queue_rcu_work);\n\nstatic struct worker *alloc_worker(int node)\n{\n\tstruct worker *worker;\n\n\tworker = kzalloc_node(sizeof(*worker), GFP_KERNEL, node);\n\tif (worker) {\n\t\tINIT_LIST_HEAD(&worker->entry);\n\t\tINIT_LIST_HEAD(&worker->scheduled);\n\t\tINIT_LIST_HEAD(&worker->node);\n\t\t \n\t\tworker->flags = WORKER_PREP;\n\t}\n\treturn worker;\n}\n\nstatic cpumask_t *pool_allowed_cpus(struct worker_pool *pool)\n{\n\tif (pool->cpu < 0 && pool->attrs->affn_strict)\n\t\treturn pool->attrs->__pod_cpumask;\n\telse\n\t\treturn pool->attrs->cpumask;\n}\n\n \nstatic void worker_attach_to_pool(struct worker *worker,\n\t\t\t\t   struct worker_pool *pool)\n{\n\tmutex_lock(&wq_pool_attach_mutex);\n\n\t \n\tif (pool->flags & POOL_DISASSOCIATED)\n\t\tworker->flags |= WORKER_UNBOUND;\n\telse\n\t\tkthread_set_per_cpu(worker->task, pool->cpu);\n\n\tif (worker->rescue_wq)\n\t\tset_cpus_allowed_ptr(worker->task, pool_allowed_cpus(pool));\n\n\tlist_add_tail(&worker->node, &pool->workers);\n\tworker->pool = pool;\n\n\tmutex_unlock(&wq_pool_attach_mutex);\n}\n\n \nstatic void worker_detach_from_pool(struct worker *worker)\n{\n\tstruct worker_pool *pool = worker->pool;\n\tstruct completion *detach_completion = NULL;\n\n\tmutex_lock(&wq_pool_attach_mutex);\n\n\tkthread_set_per_cpu(worker->task, -1);\n\tlist_del(&worker->node);\n\tworker->pool = NULL;\n\n\tif (list_empty(&pool->workers) && list_empty(&pool->dying_workers))\n\t\tdetach_completion = pool->detach_completion;\n\tmutex_unlock(&wq_pool_attach_mutex);\n\n\t \n\tworker->flags &= ~(WORKER_UNBOUND | WORKER_REBOUND);\n\n\tif (detach_completion)\n\t\tcomplete(detach_completion);\n}\n\n \nstatic struct worker *create_worker(struct worker_pool *pool)\n{\n\tstruct worker *worker;\n\tint id;\n\tchar id_buf[23];\n\n\t \n\tid = ida_alloc(&pool->worker_ida, GFP_KERNEL);\n\tif (id < 0) {\n\t\tpr_err_once(\"workqueue: Failed to allocate a worker ID: %pe\\n\",\n\t\t\t    ERR_PTR(id));\n\t\treturn NULL;\n\t}\n\n\tworker = alloc_worker(pool->node);\n\tif (!worker) {\n\t\tpr_err_once(\"workqueue: Failed to allocate a worker\\n\");\n\t\tgoto fail;\n\t}\n\n\tworker->id = id;\n\n\tif (pool->cpu >= 0)\n\t\tsnprintf(id_buf, sizeof(id_buf), \"%d:%d%s\", pool->cpu, id,\n\t\t\t pool->attrs->nice < 0  ? \"H\" : \"\");\n\telse\n\t\tsnprintf(id_buf, sizeof(id_buf), \"u%d:%d\", pool->id, id);\n\n\tworker->task = kthread_create_on_node(worker_thread, worker, pool->node,\n\t\t\t\t\t      \"kworker/%s\", id_buf);\n\tif (IS_ERR(worker->task)) {\n\t\tif (PTR_ERR(worker->task) == -EINTR) {\n\t\t\tpr_err(\"workqueue: Interrupted when creating a worker thread \\\"kworker/%s\\\"\\n\",\n\t\t\t       id_buf);\n\t\t} else {\n\t\t\tpr_err_once(\"workqueue: Failed to create a worker thread: %pe\",\n\t\t\t\t    worker->task);\n\t\t}\n\t\tgoto fail;\n\t}\n\n\tset_user_nice(worker->task, pool->attrs->nice);\n\tkthread_bind_mask(worker->task, pool_allowed_cpus(pool));\n\n\t \n\tworker_attach_to_pool(worker, pool);\n\n\t \n\traw_spin_lock_irq(&pool->lock);\n\n\tworker->pool->nr_workers++;\n\tworker_enter_idle(worker);\n\tkick_pool(pool);\n\n\t \n\twake_up_process(worker->task);\n\n\traw_spin_unlock_irq(&pool->lock);\n\n\treturn worker;\n\nfail:\n\tida_free(&pool->worker_ida, id);\n\tkfree(worker);\n\treturn NULL;\n}\n\nstatic void unbind_worker(struct worker *worker)\n{\n\tlockdep_assert_held(&wq_pool_attach_mutex);\n\n\tkthread_set_per_cpu(worker->task, -1);\n\tif (cpumask_intersects(wq_unbound_cpumask, cpu_active_mask))\n\t\tWARN_ON_ONCE(set_cpus_allowed_ptr(worker->task, wq_unbound_cpumask) < 0);\n\telse\n\t\tWARN_ON_ONCE(set_cpus_allowed_ptr(worker->task, cpu_possible_mask) < 0);\n}\n\nstatic void wake_dying_workers(struct list_head *cull_list)\n{\n\tstruct worker *worker, *tmp;\n\n\tlist_for_each_entry_safe(worker, tmp, cull_list, entry) {\n\t\tlist_del_init(&worker->entry);\n\t\tunbind_worker(worker);\n\t\t \n\t\twake_up_process(worker->task);\n\t}\n}\n\n \nstatic void set_worker_dying(struct worker *worker, struct list_head *list)\n{\n\tstruct worker_pool *pool = worker->pool;\n\n\tlockdep_assert_held(&pool->lock);\n\tlockdep_assert_held(&wq_pool_attach_mutex);\n\n\t \n\tif (WARN_ON(worker->current_work) ||\n\t    WARN_ON(!list_empty(&worker->scheduled)) ||\n\t    WARN_ON(!(worker->flags & WORKER_IDLE)))\n\t\treturn;\n\n\tpool->nr_workers--;\n\tpool->nr_idle--;\n\n\tworker->flags |= WORKER_DIE;\n\n\tlist_move(&worker->entry, list);\n\tlist_move(&worker->node, &pool->dying_workers);\n}\n\n \nstatic void idle_worker_timeout(struct timer_list *t)\n{\n\tstruct worker_pool *pool = from_timer(pool, t, idle_timer);\n\tbool do_cull = false;\n\n\tif (work_pending(&pool->idle_cull_work))\n\t\treturn;\n\n\traw_spin_lock_irq(&pool->lock);\n\n\tif (too_many_workers(pool)) {\n\t\tstruct worker *worker;\n\t\tunsigned long expires;\n\n\t\t \n\t\tworker = list_entry(pool->idle_list.prev, struct worker, entry);\n\t\texpires = worker->last_active + IDLE_WORKER_TIMEOUT;\n\t\tdo_cull = !time_before(jiffies, expires);\n\n\t\tif (!do_cull)\n\t\t\tmod_timer(&pool->idle_timer, expires);\n\t}\n\traw_spin_unlock_irq(&pool->lock);\n\n\tif (do_cull)\n\t\tqueue_work(system_unbound_wq, &pool->idle_cull_work);\n}\n\n \nstatic void idle_cull_fn(struct work_struct *work)\n{\n\tstruct worker_pool *pool = container_of(work, struct worker_pool, idle_cull_work);\n\tLIST_HEAD(cull_list);\n\n\t \n\tmutex_lock(&wq_pool_attach_mutex);\n\traw_spin_lock_irq(&pool->lock);\n\n\twhile (too_many_workers(pool)) {\n\t\tstruct worker *worker;\n\t\tunsigned long expires;\n\n\t\tworker = list_entry(pool->idle_list.prev, struct worker, entry);\n\t\texpires = worker->last_active + IDLE_WORKER_TIMEOUT;\n\n\t\tif (time_before(jiffies, expires)) {\n\t\t\tmod_timer(&pool->idle_timer, expires);\n\t\t\tbreak;\n\t\t}\n\n\t\tset_worker_dying(worker, &cull_list);\n\t}\n\n\traw_spin_unlock_irq(&pool->lock);\n\twake_dying_workers(&cull_list);\n\tmutex_unlock(&wq_pool_attach_mutex);\n}\n\nstatic void send_mayday(struct work_struct *work)\n{\n\tstruct pool_workqueue *pwq = get_work_pwq(work);\n\tstruct workqueue_struct *wq = pwq->wq;\n\n\tlockdep_assert_held(&wq_mayday_lock);\n\n\tif (!wq->rescuer)\n\t\treturn;\n\n\t \n\tif (list_empty(&pwq->mayday_node)) {\n\t\t \n\t\tget_pwq(pwq);\n\t\tlist_add_tail(&pwq->mayday_node, &wq->maydays);\n\t\twake_up_process(wq->rescuer->task);\n\t\tpwq->stats[PWQ_STAT_MAYDAY]++;\n\t}\n}\n\nstatic void pool_mayday_timeout(struct timer_list *t)\n{\n\tstruct worker_pool *pool = from_timer(pool, t, mayday_timer);\n\tstruct work_struct *work;\n\n\traw_spin_lock_irq(&pool->lock);\n\traw_spin_lock(&wq_mayday_lock);\t\t \n\n\tif (need_to_create_worker(pool)) {\n\t\t \n\t\tlist_for_each_entry(work, &pool->worklist, entry)\n\t\t\tsend_mayday(work);\n\t}\n\n\traw_spin_unlock(&wq_mayday_lock);\n\traw_spin_unlock_irq(&pool->lock);\n\n\tmod_timer(&pool->mayday_timer, jiffies + MAYDAY_INTERVAL);\n}\n\n \nstatic void maybe_create_worker(struct worker_pool *pool)\n__releases(&pool->lock)\n__acquires(&pool->lock)\n{\nrestart:\n\traw_spin_unlock_irq(&pool->lock);\n\n\t \n\tmod_timer(&pool->mayday_timer, jiffies + MAYDAY_INITIAL_TIMEOUT);\n\n\twhile (true) {\n\t\tif (create_worker(pool) || !need_to_create_worker(pool))\n\t\t\tbreak;\n\n\t\tschedule_timeout_interruptible(CREATE_COOLDOWN);\n\n\t\tif (!need_to_create_worker(pool))\n\t\t\tbreak;\n\t}\n\n\tdel_timer_sync(&pool->mayday_timer);\n\traw_spin_lock_irq(&pool->lock);\n\t \n\tif (need_to_create_worker(pool))\n\t\tgoto restart;\n}\n\n \nstatic bool manage_workers(struct worker *worker)\n{\n\tstruct worker_pool *pool = worker->pool;\n\n\tif (pool->flags & POOL_MANAGER_ACTIVE)\n\t\treturn false;\n\n\tpool->flags |= POOL_MANAGER_ACTIVE;\n\tpool->manager = worker;\n\n\tmaybe_create_worker(pool);\n\n\tpool->manager = NULL;\n\tpool->flags &= ~POOL_MANAGER_ACTIVE;\n\trcuwait_wake_up(&manager_wait);\n\treturn true;\n}\n\n \nstatic void process_one_work(struct worker *worker, struct work_struct *work)\n__releases(&pool->lock)\n__acquires(&pool->lock)\n{\n\tstruct pool_workqueue *pwq = get_work_pwq(work);\n\tstruct worker_pool *pool = worker->pool;\n\tunsigned long work_data;\n#ifdef CONFIG_LOCKDEP\n\t \n\tstruct lockdep_map lockdep_map;\n\n\tlockdep_copy_map(&lockdep_map, &work->lockdep_map);\n#endif\n\t \n\tWARN_ON_ONCE(!(pool->flags & POOL_DISASSOCIATED) &&\n\t\t     raw_smp_processor_id() != pool->cpu);\n\n\t \n\tdebug_work_deactivate(work);\n\thash_add(pool->busy_hash, &worker->hentry, (unsigned long)work);\n\tworker->current_work = work;\n\tworker->current_func = work->func;\n\tworker->current_pwq = pwq;\n\tworker->current_at = worker->task->se.sum_exec_runtime;\n\twork_data = *work_data_bits(work);\n\tworker->current_color = get_work_color(work_data);\n\n\t \n\tstrscpy(worker->desc, pwq->wq->name, WORKER_DESC_LEN);\n\n\tlist_del_init(&work->entry);\n\n\t \n\tif (unlikely(pwq->wq->flags & WQ_CPU_INTENSIVE))\n\t\tworker_set_flags(worker, WORKER_CPU_INTENSIVE);\n\n\t \n\tkick_pool(pool);\n\n\t \n\tset_work_pool_and_clear_pending(work, pool->id);\n\n\tpwq->stats[PWQ_STAT_STARTED]++;\n\traw_spin_unlock_irq(&pool->lock);\n\n\tlock_map_acquire(&pwq->wq->lockdep_map);\n\tlock_map_acquire(&lockdep_map);\n\t \n\tlockdep_invariant_state(true);\n\ttrace_workqueue_execute_start(work);\n\tworker->current_func(work);\n\t \n\ttrace_workqueue_execute_end(work, worker->current_func);\n\tpwq->stats[PWQ_STAT_COMPLETED]++;\n\tlock_map_release(&lockdep_map);\n\tlock_map_release(&pwq->wq->lockdep_map);\n\n\tif (unlikely(in_atomic() || lockdep_depth(current) > 0)) {\n\t\tpr_err(\"BUG: workqueue leaked lock or atomic: %s/0x%08x/%d\\n\"\n\t\t       \"     last function: %ps\\n\",\n\t\t       current->comm, preempt_count(), task_pid_nr(current),\n\t\t       worker->current_func);\n\t\tdebug_show_held_locks(current);\n\t\tdump_stack();\n\t}\n\n\t \n\tcond_resched();\n\n\traw_spin_lock_irq(&pool->lock);\n\n\t \n\tworker_clr_flags(worker, WORKER_CPU_INTENSIVE);\n\n\t \n\tworker->last_func = worker->current_func;\n\n\t \n\thash_del(&worker->hentry);\n\tworker->current_work = NULL;\n\tworker->current_func = NULL;\n\tworker->current_pwq = NULL;\n\tworker->current_color = INT_MAX;\n\tpwq_dec_nr_in_flight(pwq, work_data);\n}\n\n \nstatic void process_scheduled_works(struct worker *worker)\n{\n\tstruct work_struct *work;\n\tbool first = true;\n\n\twhile ((work = list_first_entry_or_null(&worker->scheduled,\n\t\t\t\t\t\tstruct work_struct, entry))) {\n\t\tif (first) {\n\t\t\tworker->pool->watchdog_ts = jiffies;\n\t\t\tfirst = false;\n\t\t}\n\t\tprocess_one_work(worker, work);\n\t}\n}\n\nstatic void set_pf_worker(bool val)\n{\n\tmutex_lock(&wq_pool_attach_mutex);\n\tif (val)\n\t\tcurrent->flags |= PF_WQ_WORKER;\n\telse\n\t\tcurrent->flags &= ~PF_WQ_WORKER;\n\tmutex_unlock(&wq_pool_attach_mutex);\n}\n\n \nstatic int worker_thread(void *__worker)\n{\n\tstruct worker *worker = __worker;\n\tstruct worker_pool *pool = worker->pool;\n\n\t \n\tset_pf_worker(true);\nwoke_up:\n\traw_spin_lock_irq(&pool->lock);\n\n\t \n\tif (unlikely(worker->flags & WORKER_DIE)) {\n\t\traw_spin_unlock_irq(&pool->lock);\n\t\tset_pf_worker(false);\n\n\t\tset_task_comm(worker->task, \"kworker/dying\");\n\t\tida_free(&pool->worker_ida, worker->id);\n\t\tworker_detach_from_pool(worker);\n\t\tWARN_ON_ONCE(!list_empty(&worker->entry));\n\t\tkfree(worker);\n\t\treturn 0;\n\t}\n\n\tworker_leave_idle(worker);\nrecheck:\n\t \n\tif (!need_more_worker(pool))\n\t\tgoto sleep;\n\n\t \n\tif (unlikely(!may_start_working(pool)) && manage_workers(worker))\n\t\tgoto recheck;\n\n\t \n\tWARN_ON_ONCE(!list_empty(&worker->scheduled));\n\n\t \n\tworker_clr_flags(worker, WORKER_PREP | WORKER_REBOUND);\n\n\tdo {\n\t\tstruct work_struct *work =\n\t\t\tlist_first_entry(&pool->worklist,\n\t\t\t\t\t struct work_struct, entry);\n\n\t\tif (assign_work(work, worker, NULL))\n\t\t\tprocess_scheduled_works(worker);\n\t} while (keep_working(pool));\n\n\tworker_set_flags(worker, WORKER_PREP);\nsleep:\n\t \n\tworker_enter_idle(worker);\n\t__set_current_state(TASK_IDLE);\n\traw_spin_unlock_irq(&pool->lock);\n\tschedule();\n\tgoto woke_up;\n}\n\n \nstatic int rescuer_thread(void *__rescuer)\n{\n\tstruct worker *rescuer = __rescuer;\n\tstruct workqueue_struct *wq = rescuer->rescue_wq;\n\tbool should_stop;\n\n\tset_user_nice(current, RESCUER_NICE_LEVEL);\n\n\t \n\tset_pf_worker(true);\nrepeat:\n\tset_current_state(TASK_IDLE);\n\n\t \n\tshould_stop = kthread_should_stop();\n\n\t \n\traw_spin_lock_irq(&wq_mayday_lock);\n\n\twhile (!list_empty(&wq->maydays)) {\n\t\tstruct pool_workqueue *pwq = list_first_entry(&wq->maydays,\n\t\t\t\t\tstruct pool_workqueue, mayday_node);\n\t\tstruct worker_pool *pool = pwq->pool;\n\t\tstruct work_struct *work, *n;\n\n\t\t__set_current_state(TASK_RUNNING);\n\t\tlist_del_init(&pwq->mayday_node);\n\n\t\traw_spin_unlock_irq(&wq_mayday_lock);\n\n\t\tworker_attach_to_pool(rescuer, pool);\n\n\t\traw_spin_lock_irq(&pool->lock);\n\n\t\t \n\t\tWARN_ON_ONCE(!list_empty(&rescuer->scheduled));\n\t\tlist_for_each_entry_safe(work, n, &pool->worklist, entry) {\n\t\t\tif (get_work_pwq(work) == pwq &&\n\t\t\t    assign_work(work, rescuer, &n))\n\t\t\t\tpwq->stats[PWQ_STAT_RESCUED]++;\n\t\t}\n\n\t\tif (!list_empty(&rescuer->scheduled)) {\n\t\t\tprocess_scheduled_works(rescuer);\n\n\t\t\t \n\t\t\tif (pwq->nr_active && need_to_create_worker(pool)) {\n\t\t\t\traw_spin_lock(&wq_mayday_lock);\n\t\t\t\t \n\t\t\t\tif (wq->rescuer && list_empty(&pwq->mayday_node)) {\n\t\t\t\t\tget_pwq(pwq);\n\t\t\t\t\tlist_add_tail(&pwq->mayday_node, &wq->maydays);\n\t\t\t\t}\n\t\t\t\traw_spin_unlock(&wq_mayday_lock);\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tput_pwq(pwq);\n\n\t\t \n\t\tkick_pool(pool);\n\n\t\traw_spin_unlock_irq(&pool->lock);\n\n\t\tworker_detach_from_pool(rescuer);\n\n\t\traw_spin_lock_irq(&wq_mayday_lock);\n\t}\n\n\traw_spin_unlock_irq(&wq_mayday_lock);\n\n\tif (should_stop) {\n\t\t__set_current_state(TASK_RUNNING);\n\t\tset_pf_worker(false);\n\t\treturn 0;\n\t}\n\n\t \n\tWARN_ON_ONCE(!(rescuer->flags & WORKER_NOT_RUNNING));\n\tschedule();\n\tgoto repeat;\n}\n\n \nstatic void check_flush_dependency(struct workqueue_struct *target_wq,\n\t\t\t\t   struct work_struct *target_work)\n{\n\twork_func_t target_func = target_work ? target_work->func : NULL;\n\tstruct worker *worker;\n\n\tif (target_wq->flags & WQ_MEM_RECLAIM)\n\t\treturn;\n\n\tworker = current_wq_worker();\n\n\tWARN_ONCE(current->flags & PF_MEMALLOC,\n\t\t  \"workqueue: PF_MEMALLOC task %d(%s) is flushing !WQ_MEM_RECLAIM %s:%ps\",\n\t\t  current->pid, current->comm, target_wq->name, target_func);\n\tWARN_ONCE(worker && ((worker->current_pwq->wq->flags &\n\t\t\t      (WQ_MEM_RECLAIM | __WQ_LEGACY)) == WQ_MEM_RECLAIM),\n\t\t  \"workqueue: WQ_MEM_RECLAIM %s:%ps is flushing !WQ_MEM_RECLAIM %s:%ps\",\n\t\t  worker->current_pwq->wq->name, worker->current_func,\n\t\t  target_wq->name, target_func);\n}\n\nstruct wq_barrier {\n\tstruct work_struct\twork;\n\tstruct completion\tdone;\n\tstruct task_struct\t*task;\t \n};\n\nstatic void wq_barrier_func(struct work_struct *work)\n{\n\tstruct wq_barrier *barr = container_of(work, struct wq_barrier, work);\n\tcomplete(&barr->done);\n}\n\n \nstatic void insert_wq_barrier(struct pool_workqueue *pwq,\n\t\t\t      struct wq_barrier *barr,\n\t\t\t      struct work_struct *target, struct worker *worker)\n{\n\tunsigned int work_flags = 0;\n\tunsigned int work_color;\n\tstruct list_head *head;\n\n\t \n\tINIT_WORK_ONSTACK(&barr->work, wq_barrier_func);\n\t__set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(&barr->work));\n\n\tinit_completion_map(&barr->done, &target->lockdep_map);\n\n\tbarr->task = current;\n\n\t \n\twork_flags |= WORK_STRUCT_INACTIVE;\n\n\t \n\tif (worker) {\n\t\thead = worker->scheduled.next;\n\t\twork_color = worker->current_color;\n\t} else {\n\t\tunsigned long *bits = work_data_bits(target);\n\n\t\thead = target->entry.next;\n\t\t \n\t\twork_flags |= *bits & WORK_STRUCT_LINKED;\n\t\twork_color = get_work_color(*bits);\n\t\t__set_bit(WORK_STRUCT_LINKED_BIT, bits);\n\t}\n\n\tpwq->nr_in_flight[work_color]++;\n\twork_flags |= work_color_to_flags(work_color);\n\n\tinsert_work(pwq, &barr->work, head, work_flags);\n}\n\n \nstatic bool flush_workqueue_prep_pwqs(struct workqueue_struct *wq,\n\t\t\t\t      int flush_color, int work_color)\n{\n\tbool wait = false;\n\tstruct pool_workqueue *pwq;\n\n\tif (flush_color >= 0) {\n\t\tWARN_ON_ONCE(atomic_read(&wq->nr_pwqs_to_flush));\n\t\tatomic_set(&wq->nr_pwqs_to_flush, 1);\n\t}\n\n\tfor_each_pwq(pwq, wq) {\n\t\tstruct worker_pool *pool = pwq->pool;\n\n\t\traw_spin_lock_irq(&pool->lock);\n\n\t\tif (flush_color >= 0) {\n\t\t\tWARN_ON_ONCE(pwq->flush_color != -1);\n\n\t\t\tif (pwq->nr_in_flight[flush_color]) {\n\t\t\t\tpwq->flush_color = flush_color;\n\t\t\t\tatomic_inc(&wq->nr_pwqs_to_flush);\n\t\t\t\twait = true;\n\t\t\t}\n\t\t}\n\n\t\tif (work_color >= 0) {\n\t\t\tWARN_ON_ONCE(work_color != work_next_color(pwq->work_color));\n\t\t\tpwq->work_color = work_color;\n\t\t}\n\n\t\traw_spin_unlock_irq(&pool->lock);\n\t}\n\n\tif (flush_color >= 0 && atomic_dec_and_test(&wq->nr_pwqs_to_flush))\n\t\tcomplete(&wq->first_flusher->done);\n\n\treturn wait;\n}\n\n \nvoid __flush_workqueue(struct workqueue_struct *wq)\n{\n\tstruct wq_flusher this_flusher = {\n\t\t.list = LIST_HEAD_INIT(this_flusher.list),\n\t\t.flush_color = -1,\n\t\t.done = COMPLETION_INITIALIZER_ONSTACK_MAP(this_flusher.done, wq->lockdep_map),\n\t};\n\tint next_color;\n\n\tif (WARN_ON(!wq_online))\n\t\treturn;\n\n\tlock_map_acquire(&wq->lockdep_map);\n\tlock_map_release(&wq->lockdep_map);\n\n\tmutex_lock(&wq->mutex);\n\n\t \n\tnext_color = work_next_color(wq->work_color);\n\n\tif (next_color != wq->flush_color) {\n\t\t \n\t\tWARN_ON_ONCE(!list_empty(&wq->flusher_overflow));\n\t\tthis_flusher.flush_color = wq->work_color;\n\t\twq->work_color = next_color;\n\n\t\tif (!wq->first_flusher) {\n\t\t\t \n\t\t\tWARN_ON_ONCE(wq->flush_color != this_flusher.flush_color);\n\n\t\t\twq->first_flusher = &this_flusher;\n\n\t\t\tif (!flush_workqueue_prep_pwqs(wq, wq->flush_color,\n\t\t\t\t\t\t       wq->work_color)) {\n\t\t\t\t \n\t\t\t\twq->flush_color = next_color;\n\t\t\t\twq->first_flusher = NULL;\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t} else {\n\t\t\t \n\t\t\tWARN_ON_ONCE(wq->flush_color == this_flusher.flush_color);\n\t\t\tlist_add_tail(&this_flusher.list, &wq->flusher_queue);\n\t\t\tflush_workqueue_prep_pwqs(wq, -1, wq->work_color);\n\t\t}\n\t} else {\n\t\t \n\t\tlist_add_tail(&this_flusher.list, &wq->flusher_overflow);\n\t}\n\n\tcheck_flush_dependency(wq, NULL);\n\n\tmutex_unlock(&wq->mutex);\n\n\twait_for_completion(&this_flusher.done);\n\n\t \n\tif (READ_ONCE(wq->first_flusher) != &this_flusher)\n\t\treturn;\n\n\tmutex_lock(&wq->mutex);\n\n\t \n\tif (wq->first_flusher != &this_flusher)\n\t\tgoto out_unlock;\n\n\tWRITE_ONCE(wq->first_flusher, NULL);\n\n\tWARN_ON_ONCE(!list_empty(&this_flusher.list));\n\tWARN_ON_ONCE(wq->flush_color != this_flusher.flush_color);\n\n\twhile (true) {\n\t\tstruct wq_flusher *next, *tmp;\n\n\t\t \n\t\tlist_for_each_entry_safe(next, tmp, &wq->flusher_queue, list) {\n\t\t\tif (next->flush_color != wq->flush_color)\n\t\t\t\tbreak;\n\t\t\tlist_del_init(&next->list);\n\t\t\tcomplete(&next->done);\n\t\t}\n\n\t\tWARN_ON_ONCE(!list_empty(&wq->flusher_overflow) &&\n\t\t\t     wq->flush_color != work_next_color(wq->work_color));\n\n\t\t \n\t\twq->flush_color = work_next_color(wq->flush_color);\n\n\t\t \n\t\tif (!list_empty(&wq->flusher_overflow)) {\n\t\t\t \n\t\t\tlist_for_each_entry(tmp, &wq->flusher_overflow, list)\n\t\t\t\ttmp->flush_color = wq->work_color;\n\n\t\t\twq->work_color = work_next_color(wq->work_color);\n\n\t\t\tlist_splice_tail_init(&wq->flusher_overflow,\n\t\t\t\t\t      &wq->flusher_queue);\n\t\t\tflush_workqueue_prep_pwqs(wq, -1, wq->work_color);\n\t\t}\n\n\t\tif (list_empty(&wq->flusher_queue)) {\n\t\t\tWARN_ON_ONCE(wq->flush_color != wq->work_color);\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tWARN_ON_ONCE(wq->flush_color == wq->work_color);\n\t\tWARN_ON_ONCE(wq->flush_color != next->flush_color);\n\n\t\tlist_del_init(&next->list);\n\t\twq->first_flusher = next;\n\n\t\tif (flush_workqueue_prep_pwqs(wq, wq->flush_color, -1))\n\t\t\tbreak;\n\n\t\t \n\t\twq->first_flusher = NULL;\n\t}\n\nout_unlock:\n\tmutex_unlock(&wq->mutex);\n}\nEXPORT_SYMBOL(__flush_workqueue);\n\n \nvoid drain_workqueue(struct workqueue_struct *wq)\n{\n\tunsigned int flush_cnt = 0;\n\tstruct pool_workqueue *pwq;\n\n\t \n\tmutex_lock(&wq->mutex);\n\tif (!wq->nr_drainers++)\n\t\twq->flags |= __WQ_DRAINING;\n\tmutex_unlock(&wq->mutex);\nreflush:\n\t__flush_workqueue(wq);\n\n\tmutex_lock(&wq->mutex);\n\n\tfor_each_pwq(pwq, wq) {\n\t\tbool drained;\n\n\t\traw_spin_lock_irq(&pwq->pool->lock);\n\t\tdrained = !pwq->nr_active && list_empty(&pwq->inactive_works);\n\t\traw_spin_unlock_irq(&pwq->pool->lock);\n\n\t\tif (drained)\n\t\t\tcontinue;\n\n\t\tif (++flush_cnt == 10 ||\n\t\t    (flush_cnt % 100 == 0 && flush_cnt <= 1000))\n\t\t\tpr_warn(\"workqueue %s: %s() isn't complete after %u tries\\n\",\n\t\t\t\twq->name, __func__, flush_cnt);\n\n\t\tmutex_unlock(&wq->mutex);\n\t\tgoto reflush;\n\t}\n\n\tif (!--wq->nr_drainers)\n\t\twq->flags &= ~__WQ_DRAINING;\n\tmutex_unlock(&wq->mutex);\n}\nEXPORT_SYMBOL_GPL(drain_workqueue);\n\nstatic bool start_flush_work(struct work_struct *work, struct wq_barrier *barr,\n\t\t\t     bool from_cancel)\n{\n\tstruct worker *worker = NULL;\n\tstruct worker_pool *pool;\n\tstruct pool_workqueue *pwq;\n\n\tmight_sleep();\n\n\trcu_read_lock();\n\tpool = get_work_pool(work);\n\tif (!pool) {\n\t\trcu_read_unlock();\n\t\treturn false;\n\t}\n\n\traw_spin_lock_irq(&pool->lock);\n\t \n\tpwq = get_work_pwq(work);\n\tif (pwq) {\n\t\tif (unlikely(pwq->pool != pool))\n\t\t\tgoto already_gone;\n\t} else {\n\t\tworker = find_worker_executing_work(pool, work);\n\t\tif (!worker)\n\t\t\tgoto already_gone;\n\t\tpwq = worker->current_pwq;\n\t}\n\n\tcheck_flush_dependency(pwq->wq, work);\n\n\tinsert_wq_barrier(pwq, barr, work, worker);\n\traw_spin_unlock_irq(&pool->lock);\n\n\t \n\tif (!from_cancel &&\n\t    (pwq->wq->saved_max_active == 1 || pwq->wq->rescuer)) {\n\t\tlock_map_acquire(&pwq->wq->lockdep_map);\n\t\tlock_map_release(&pwq->wq->lockdep_map);\n\t}\n\trcu_read_unlock();\n\treturn true;\nalready_gone:\n\traw_spin_unlock_irq(&pool->lock);\n\trcu_read_unlock();\n\treturn false;\n}\n\nstatic bool __flush_work(struct work_struct *work, bool from_cancel)\n{\n\tstruct wq_barrier barr;\n\n\tif (WARN_ON(!wq_online))\n\t\treturn false;\n\n\tif (WARN_ON(!work->func))\n\t\treturn false;\n\n\tlock_map_acquire(&work->lockdep_map);\n\tlock_map_release(&work->lockdep_map);\n\n\tif (start_flush_work(work, &barr, from_cancel)) {\n\t\twait_for_completion(&barr.done);\n\t\tdestroy_work_on_stack(&barr.work);\n\t\treturn true;\n\t} else {\n\t\treturn false;\n\t}\n}\n\n \nbool flush_work(struct work_struct *work)\n{\n\treturn __flush_work(work, false);\n}\nEXPORT_SYMBOL_GPL(flush_work);\n\nstruct cwt_wait {\n\twait_queue_entry_t\t\twait;\n\tstruct work_struct\t*work;\n};\n\nstatic int cwt_wakefn(wait_queue_entry_t *wait, unsigned mode, int sync, void *key)\n{\n\tstruct cwt_wait *cwait = container_of(wait, struct cwt_wait, wait);\n\n\tif (cwait->work != key)\n\t\treturn 0;\n\treturn autoremove_wake_function(wait, mode, sync, key);\n}\n\nstatic bool __cancel_work_timer(struct work_struct *work, bool is_dwork)\n{\n\tstatic DECLARE_WAIT_QUEUE_HEAD(cancel_waitq);\n\tunsigned long flags;\n\tint ret;\n\n\tdo {\n\t\tret = try_to_grab_pending(work, is_dwork, &flags);\n\t\t \n\t\tif (unlikely(ret == -ENOENT)) {\n\t\t\tstruct cwt_wait cwait;\n\n\t\t\tinit_wait(&cwait.wait);\n\t\t\tcwait.wait.func = cwt_wakefn;\n\t\t\tcwait.work = work;\n\n\t\t\tprepare_to_wait_exclusive(&cancel_waitq, &cwait.wait,\n\t\t\t\t\t\t  TASK_UNINTERRUPTIBLE);\n\t\t\tif (work_is_canceling(work))\n\t\t\t\tschedule();\n\t\t\tfinish_wait(&cancel_waitq, &cwait.wait);\n\t\t}\n\t} while (unlikely(ret < 0));\n\n\t \n\tmark_work_canceling(work);\n\tlocal_irq_restore(flags);\n\n\t \n\tif (wq_online)\n\t\t__flush_work(work, true);\n\n\tclear_work_data(work);\n\n\t \n\tsmp_mb();\n\tif (waitqueue_active(&cancel_waitq))\n\t\t__wake_up(&cancel_waitq, TASK_NORMAL, 1, work);\n\n\treturn ret;\n}\n\n \nbool cancel_work_sync(struct work_struct *work)\n{\n\treturn __cancel_work_timer(work, false);\n}\nEXPORT_SYMBOL_GPL(cancel_work_sync);\n\n \nbool flush_delayed_work(struct delayed_work *dwork)\n{\n\tlocal_irq_disable();\n\tif (del_timer_sync(&dwork->timer))\n\t\t__queue_work(dwork->cpu, dwork->wq, &dwork->work);\n\tlocal_irq_enable();\n\treturn flush_work(&dwork->work);\n}\nEXPORT_SYMBOL(flush_delayed_work);\n\n \nbool flush_rcu_work(struct rcu_work *rwork)\n{\n\tif (test_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(&rwork->work))) {\n\t\trcu_barrier();\n\t\tflush_work(&rwork->work);\n\t\treturn true;\n\t} else {\n\t\treturn flush_work(&rwork->work);\n\t}\n}\nEXPORT_SYMBOL(flush_rcu_work);\n\nstatic bool __cancel_work(struct work_struct *work, bool is_dwork)\n{\n\tunsigned long flags;\n\tint ret;\n\n\tdo {\n\t\tret = try_to_grab_pending(work, is_dwork, &flags);\n\t} while (unlikely(ret == -EAGAIN));\n\n\tif (unlikely(ret < 0))\n\t\treturn false;\n\n\tset_work_pool_and_clear_pending(work, get_work_pool_id(work));\n\tlocal_irq_restore(flags);\n\treturn ret;\n}\n\n \nbool cancel_work(struct work_struct *work)\n{\n\treturn __cancel_work(work, false);\n}\nEXPORT_SYMBOL(cancel_work);\n\n \nbool cancel_delayed_work(struct delayed_work *dwork)\n{\n\treturn __cancel_work(&dwork->work, true);\n}\nEXPORT_SYMBOL(cancel_delayed_work);\n\n \nbool cancel_delayed_work_sync(struct delayed_work *dwork)\n{\n\treturn __cancel_work_timer(&dwork->work, true);\n}\nEXPORT_SYMBOL(cancel_delayed_work_sync);\n\n \nint schedule_on_each_cpu(work_func_t func)\n{\n\tint cpu;\n\tstruct work_struct __percpu *works;\n\n\tworks = alloc_percpu(struct work_struct);\n\tif (!works)\n\t\treturn -ENOMEM;\n\n\tcpus_read_lock();\n\n\tfor_each_online_cpu(cpu) {\n\t\tstruct work_struct *work = per_cpu_ptr(works, cpu);\n\n\t\tINIT_WORK(work, func);\n\t\tschedule_work_on(cpu, work);\n\t}\n\n\tfor_each_online_cpu(cpu)\n\t\tflush_work(per_cpu_ptr(works, cpu));\n\n\tcpus_read_unlock();\n\tfree_percpu(works);\n\treturn 0;\n}\n\n \nint execute_in_process_context(work_func_t fn, struct execute_work *ew)\n{\n\tif (!in_interrupt()) {\n\t\tfn(&ew->work);\n\t\treturn 0;\n\t}\n\n\tINIT_WORK(&ew->work, fn);\n\tschedule_work(&ew->work);\n\n\treturn 1;\n}\nEXPORT_SYMBOL_GPL(execute_in_process_context);\n\n \nvoid free_workqueue_attrs(struct workqueue_attrs *attrs)\n{\n\tif (attrs) {\n\t\tfree_cpumask_var(attrs->cpumask);\n\t\tfree_cpumask_var(attrs->__pod_cpumask);\n\t\tkfree(attrs);\n\t}\n}\n\n \nstruct workqueue_attrs *alloc_workqueue_attrs(void)\n{\n\tstruct workqueue_attrs *attrs;\n\n\tattrs = kzalloc(sizeof(*attrs), GFP_KERNEL);\n\tif (!attrs)\n\t\tgoto fail;\n\tif (!alloc_cpumask_var(&attrs->cpumask, GFP_KERNEL))\n\t\tgoto fail;\n\tif (!alloc_cpumask_var(&attrs->__pod_cpumask, GFP_KERNEL))\n\t\tgoto fail;\n\n\tcpumask_copy(attrs->cpumask, cpu_possible_mask);\n\tattrs->affn_scope = WQ_AFFN_DFL;\n\treturn attrs;\nfail:\n\tfree_workqueue_attrs(attrs);\n\treturn NULL;\n}\n\nstatic void copy_workqueue_attrs(struct workqueue_attrs *to,\n\t\t\t\t const struct workqueue_attrs *from)\n{\n\tto->nice = from->nice;\n\tcpumask_copy(to->cpumask, from->cpumask);\n\tcpumask_copy(to->__pod_cpumask, from->__pod_cpumask);\n\tto->affn_strict = from->affn_strict;\n\n\t \n\tto->affn_scope = from->affn_scope;\n\tto->ordered = from->ordered;\n}\n\n \nstatic void wqattrs_clear_for_pool(struct workqueue_attrs *attrs)\n{\n\tattrs->affn_scope = WQ_AFFN_NR_TYPES;\n\tattrs->ordered = false;\n}\n\n \nstatic u32 wqattrs_hash(const struct workqueue_attrs *attrs)\n{\n\tu32 hash = 0;\n\n\thash = jhash_1word(attrs->nice, hash);\n\thash = jhash(cpumask_bits(attrs->cpumask),\n\t\t     BITS_TO_LONGS(nr_cpumask_bits) * sizeof(long), hash);\n\thash = jhash(cpumask_bits(attrs->__pod_cpumask),\n\t\t     BITS_TO_LONGS(nr_cpumask_bits) * sizeof(long), hash);\n\thash = jhash_1word(attrs->affn_strict, hash);\n\treturn hash;\n}\n\n \nstatic bool wqattrs_equal(const struct workqueue_attrs *a,\n\t\t\t  const struct workqueue_attrs *b)\n{\n\tif (a->nice != b->nice)\n\t\treturn false;\n\tif (!cpumask_equal(a->cpumask, b->cpumask))\n\t\treturn false;\n\tif (!cpumask_equal(a->__pod_cpumask, b->__pod_cpumask))\n\t\treturn false;\n\tif (a->affn_strict != b->affn_strict)\n\t\treturn false;\n\treturn true;\n}\n\n \nstatic void wqattrs_actualize_cpumask(struct workqueue_attrs *attrs,\n\t\t\t\t      const cpumask_t *unbound_cpumask)\n{\n\t \n\tcpumask_and(attrs->cpumask, attrs->cpumask, unbound_cpumask);\n\tif (unlikely(cpumask_empty(attrs->cpumask)))\n\t\tcpumask_copy(attrs->cpumask, unbound_cpumask);\n}\n\n \nstatic const struct wq_pod_type *\nwqattrs_pod_type(const struct workqueue_attrs *attrs)\n{\n\tenum wq_affn_scope scope;\n\tstruct wq_pod_type *pt;\n\n\t \n\tlockdep_assert_held(&wq_pool_mutex);\n\n\tif (attrs->affn_scope == WQ_AFFN_DFL)\n\t\tscope = wq_affn_dfl;\n\telse\n\t\tscope = attrs->affn_scope;\n\n\tpt = &wq_pod_types[scope];\n\n\tif (!WARN_ON_ONCE(attrs->affn_scope == WQ_AFFN_NR_TYPES) &&\n\t    likely(pt->nr_pods))\n\t\treturn pt;\n\n\t \n\tpt = &wq_pod_types[WQ_AFFN_SYSTEM];\n\tBUG_ON(!pt->nr_pods);\n\treturn pt;\n}\n\n \nstatic int init_worker_pool(struct worker_pool *pool)\n{\n\traw_spin_lock_init(&pool->lock);\n\tpool->id = -1;\n\tpool->cpu = -1;\n\tpool->node = NUMA_NO_NODE;\n\tpool->flags |= POOL_DISASSOCIATED;\n\tpool->watchdog_ts = jiffies;\n\tINIT_LIST_HEAD(&pool->worklist);\n\tINIT_LIST_HEAD(&pool->idle_list);\n\thash_init(pool->busy_hash);\n\n\ttimer_setup(&pool->idle_timer, idle_worker_timeout, TIMER_DEFERRABLE);\n\tINIT_WORK(&pool->idle_cull_work, idle_cull_fn);\n\n\ttimer_setup(&pool->mayday_timer, pool_mayday_timeout, 0);\n\n\tINIT_LIST_HEAD(&pool->workers);\n\tINIT_LIST_HEAD(&pool->dying_workers);\n\n\tida_init(&pool->worker_ida);\n\tINIT_HLIST_NODE(&pool->hash_node);\n\tpool->refcnt = 1;\n\n\t \n\tpool->attrs = alloc_workqueue_attrs();\n\tif (!pool->attrs)\n\t\treturn -ENOMEM;\n\n\twqattrs_clear_for_pool(pool->attrs);\n\n\treturn 0;\n}\n\n#ifdef CONFIG_LOCKDEP\nstatic void wq_init_lockdep(struct workqueue_struct *wq)\n{\n\tchar *lock_name;\n\n\tlockdep_register_key(&wq->key);\n\tlock_name = kasprintf(GFP_KERNEL, \"%s%s\", \"(wq_completion)\", wq->name);\n\tif (!lock_name)\n\t\tlock_name = wq->name;\n\n\twq->lock_name = lock_name;\n\tlockdep_init_map(&wq->lockdep_map, lock_name, &wq->key, 0);\n}\n\nstatic void wq_unregister_lockdep(struct workqueue_struct *wq)\n{\n\tlockdep_unregister_key(&wq->key);\n}\n\nstatic void wq_free_lockdep(struct workqueue_struct *wq)\n{\n\tif (wq->lock_name != wq->name)\n\t\tkfree(wq->lock_name);\n}\n#else\nstatic void wq_init_lockdep(struct workqueue_struct *wq)\n{\n}\n\nstatic void wq_unregister_lockdep(struct workqueue_struct *wq)\n{\n}\n\nstatic void wq_free_lockdep(struct workqueue_struct *wq)\n{\n}\n#endif\n\nstatic void rcu_free_wq(struct rcu_head *rcu)\n{\n\tstruct workqueue_struct *wq =\n\t\tcontainer_of(rcu, struct workqueue_struct, rcu);\n\n\twq_free_lockdep(wq);\n\tfree_percpu(wq->cpu_pwq);\n\tfree_workqueue_attrs(wq->unbound_attrs);\n\tkfree(wq);\n}\n\nstatic void rcu_free_pool(struct rcu_head *rcu)\n{\n\tstruct worker_pool *pool = container_of(rcu, struct worker_pool, rcu);\n\n\tida_destroy(&pool->worker_ida);\n\tfree_workqueue_attrs(pool->attrs);\n\tkfree(pool);\n}\n\n \nstatic void put_unbound_pool(struct worker_pool *pool)\n{\n\tDECLARE_COMPLETION_ONSTACK(detach_completion);\n\tstruct worker *worker;\n\tLIST_HEAD(cull_list);\n\n\tlockdep_assert_held(&wq_pool_mutex);\n\n\tif (--pool->refcnt)\n\t\treturn;\n\n\t \n\tif (WARN_ON(!(pool->cpu < 0)) ||\n\t    WARN_ON(!list_empty(&pool->worklist)))\n\t\treturn;\n\n\t \n\tif (pool->id >= 0)\n\t\tidr_remove(&worker_pool_idr, pool->id);\n\thash_del(&pool->hash_node);\n\n\t \n\twhile (true) {\n\t\trcuwait_wait_event(&manager_wait,\n\t\t\t\t   !(pool->flags & POOL_MANAGER_ACTIVE),\n\t\t\t\t   TASK_UNINTERRUPTIBLE);\n\n\t\tmutex_lock(&wq_pool_attach_mutex);\n\t\traw_spin_lock_irq(&pool->lock);\n\t\tif (!(pool->flags & POOL_MANAGER_ACTIVE)) {\n\t\t\tpool->flags |= POOL_MANAGER_ACTIVE;\n\t\t\tbreak;\n\t\t}\n\t\traw_spin_unlock_irq(&pool->lock);\n\t\tmutex_unlock(&wq_pool_attach_mutex);\n\t}\n\n\twhile ((worker = first_idle_worker(pool)))\n\t\tset_worker_dying(worker, &cull_list);\n\tWARN_ON(pool->nr_workers || pool->nr_idle);\n\traw_spin_unlock_irq(&pool->lock);\n\n\twake_dying_workers(&cull_list);\n\n\tif (!list_empty(&pool->workers) || !list_empty(&pool->dying_workers))\n\t\tpool->detach_completion = &detach_completion;\n\tmutex_unlock(&wq_pool_attach_mutex);\n\n\tif (pool->detach_completion)\n\t\twait_for_completion(pool->detach_completion);\n\n\t \n\tdel_timer_sync(&pool->idle_timer);\n\tcancel_work_sync(&pool->idle_cull_work);\n\tdel_timer_sync(&pool->mayday_timer);\n\n\t \n\tcall_rcu(&pool->rcu, rcu_free_pool);\n}\n\n \nstatic struct worker_pool *get_unbound_pool(const struct workqueue_attrs *attrs)\n{\n\tstruct wq_pod_type *pt = &wq_pod_types[WQ_AFFN_NUMA];\n\tu32 hash = wqattrs_hash(attrs);\n\tstruct worker_pool *pool;\n\tint pod, node = NUMA_NO_NODE;\n\n\tlockdep_assert_held(&wq_pool_mutex);\n\n\t \n\thash_for_each_possible(unbound_pool_hash, pool, hash_node, hash) {\n\t\tif (wqattrs_equal(pool->attrs, attrs)) {\n\t\t\tpool->refcnt++;\n\t\t\treturn pool;\n\t\t}\n\t}\n\n\t \n\tfor (pod = 0; pod < pt->nr_pods; pod++) {\n\t\tif (cpumask_subset(attrs->__pod_cpumask, pt->pod_cpus[pod])) {\n\t\t\tnode = pt->pod_node[pod];\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\tpool = kzalloc_node(sizeof(*pool), GFP_KERNEL, node);\n\tif (!pool || init_worker_pool(pool) < 0)\n\t\tgoto fail;\n\n\tpool->node = node;\n\tcopy_workqueue_attrs(pool->attrs, attrs);\n\twqattrs_clear_for_pool(pool->attrs);\n\n\tif (worker_pool_assign_id(pool) < 0)\n\t\tgoto fail;\n\n\t \n\tif (wq_online && !create_worker(pool))\n\t\tgoto fail;\n\n\t \n\thash_add(unbound_pool_hash, &pool->hash_node, hash);\n\n\treturn pool;\nfail:\n\tif (pool)\n\t\tput_unbound_pool(pool);\n\treturn NULL;\n}\n\nstatic void rcu_free_pwq(struct rcu_head *rcu)\n{\n\tkmem_cache_free(pwq_cache,\n\t\t\tcontainer_of(rcu, struct pool_workqueue, rcu));\n}\n\n \nstatic void pwq_release_workfn(struct kthread_work *work)\n{\n\tstruct pool_workqueue *pwq = container_of(work, struct pool_workqueue,\n\t\t\t\t\t\t  release_work);\n\tstruct workqueue_struct *wq = pwq->wq;\n\tstruct worker_pool *pool = pwq->pool;\n\tbool is_last = false;\n\n\t \n\tif (!list_empty(&pwq->pwqs_node)) {\n\t\tmutex_lock(&wq->mutex);\n\t\tlist_del_rcu(&pwq->pwqs_node);\n\t\tis_last = list_empty(&wq->pwqs);\n\t\tmutex_unlock(&wq->mutex);\n\t}\n\n\tif (wq->flags & WQ_UNBOUND) {\n\t\tmutex_lock(&wq_pool_mutex);\n\t\tput_unbound_pool(pool);\n\t\tmutex_unlock(&wq_pool_mutex);\n\t}\n\n\tcall_rcu(&pwq->rcu, rcu_free_pwq);\n\n\t \n\tif (is_last) {\n\t\twq_unregister_lockdep(wq);\n\t\tcall_rcu(&wq->rcu, rcu_free_wq);\n\t}\n}\n\n \nstatic void pwq_adjust_max_active(struct pool_workqueue *pwq)\n{\n\tstruct workqueue_struct *wq = pwq->wq;\n\tbool freezable = wq->flags & WQ_FREEZABLE;\n\tunsigned long flags;\n\n\t \n\tlockdep_assert_held(&wq->mutex);\n\n\t \n\tif (!freezable && pwq->max_active == wq->saved_max_active)\n\t\treturn;\n\n\t \n\traw_spin_lock_irqsave(&pwq->pool->lock, flags);\n\n\t \n\tif (!freezable || !workqueue_freezing) {\n\t\tpwq->max_active = wq->saved_max_active;\n\n\t\twhile (!list_empty(&pwq->inactive_works) &&\n\t\t       pwq->nr_active < pwq->max_active)\n\t\t\tpwq_activate_first_inactive(pwq);\n\n\t\tkick_pool(pwq->pool);\n\t} else {\n\t\tpwq->max_active = 0;\n\t}\n\n\traw_spin_unlock_irqrestore(&pwq->pool->lock, flags);\n}\n\n \nstatic void init_pwq(struct pool_workqueue *pwq, struct workqueue_struct *wq,\n\t\t     struct worker_pool *pool)\n{\n\tBUG_ON((unsigned long)pwq & WORK_STRUCT_FLAG_MASK);\n\n\tmemset(pwq, 0, sizeof(*pwq));\n\n\tpwq->pool = pool;\n\tpwq->wq = wq;\n\tpwq->flush_color = -1;\n\tpwq->refcnt = 1;\n\tINIT_LIST_HEAD(&pwq->inactive_works);\n\tINIT_LIST_HEAD(&pwq->pwqs_node);\n\tINIT_LIST_HEAD(&pwq->mayday_node);\n\tkthread_init_work(&pwq->release_work, pwq_release_workfn);\n}\n\n \nstatic void link_pwq(struct pool_workqueue *pwq)\n{\n\tstruct workqueue_struct *wq = pwq->wq;\n\n\tlockdep_assert_held(&wq->mutex);\n\n\t \n\tif (!list_empty(&pwq->pwqs_node))\n\t\treturn;\n\n\t \n\tpwq->work_color = wq->work_color;\n\n\t \n\tpwq_adjust_max_active(pwq);\n\n\t \n\tlist_add_rcu(&pwq->pwqs_node, &wq->pwqs);\n}\n\n \nstatic struct pool_workqueue *alloc_unbound_pwq(struct workqueue_struct *wq,\n\t\t\t\t\tconst struct workqueue_attrs *attrs)\n{\n\tstruct worker_pool *pool;\n\tstruct pool_workqueue *pwq;\n\n\tlockdep_assert_held(&wq_pool_mutex);\n\n\tpool = get_unbound_pool(attrs);\n\tif (!pool)\n\t\treturn NULL;\n\n\tpwq = kmem_cache_alloc_node(pwq_cache, GFP_KERNEL, pool->node);\n\tif (!pwq) {\n\t\tput_unbound_pool(pool);\n\t\treturn NULL;\n\t}\n\n\tinit_pwq(pwq, wq, pool);\n\treturn pwq;\n}\n\n \nstatic void wq_calc_pod_cpumask(struct workqueue_attrs *attrs, int cpu,\n\t\t\t\tint cpu_going_down)\n{\n\tconst struct wq_pod_type *pt = wqattrs_pod_type(attrs);\n\tint pod = pt->cpu_pod[cpu];\n\n\t \n\tcpumask_and(attrs->__pod_cpumask, pt->pod_cpus[pod], attrs->cpumask);\n\tcpumask_and(attrs->__pod_cpumask, attrs->__pod_cpumask, cpu_online_mask);\n\tif (cpu_going_down >= 0)\n\t\tcpumask_clear_cpu(cpu_going_down, attrs->__pod_cpumask);\n\n\tif (cpumask_empty(attrs->__pod_cpumask)) {\n\t\tcpumask_copy(attrs->__pod_cpumask, attrs->cpumask);\n\t\treturn;\n\t}\n\n\t \n\tcpumask_and(attrs->__pod_cpumask, attrs->cpumask, pt->pod_cpus[pod]);\n\n\tif (cpumask_empty(attrs->__pod_cpumask))\n\t\tpr_warn_once(\"WARNING: workqueue cpumask: online intersect > \"\n\t\t\t\t\"possible intersect\\n\");\n}\n\n \nstatic struct pool_workqueue *install_unbound_pwq(struct workqueue_struct *wq,\n\t\t\t\t\tint cpu, struct pool_workqueue *pwq)\n{\n\tstruct pool_workqueue *old_pwq;\n\n\tlockdep_assert_held(&wq_pool_mutex);\n\tlockdep_assert_held(&wq->mutex);\n\n\t \n\tlink_pwq(pwq);\n\n\told_pwq = rcu_access_pointer(*per_cpu_ptr(wq->cpu_pwq, cpu));\n\trcu_assign_pointer(*per_cpu_ptr(wq->cpu_pwq, cpu), pwq);\n\treturn old_pwq;\n}\n\n \nstruct apply_wqattrs_ctx {\n\tstruct workqueue_struct\t*wq;\t\t \n\tstruct workqueue_attrs\t*attrs;\t\t \n\tstruct list_head\tlist;\t\t \n\tstruct pool_workqueue\t*dfl_pwq;\n\tstruct pool_workqueue\t*pwq_tbl[];\n};\n\n \nstatic void apply_wqattrs_cleanup(struct apply_wqattrs_ctx *ctx)\n{\n\tif (ctx) {\n\t\tint cpu;\n\n\t\tfor_each_possible_cpu(cpu)\n\t\t\tput_pwq_unlocked(ctx->pwq_tbl[cpu]);\n\t\tput_pwq_unlocked(ctx->dfl_pwq);\n\n\t\tfree_workqueue_attrs(ctx->attrs);\n\n\t\tkfree(ctx);\n\t}\n}\n\n \nstatic struct apply_wqattrs_ctx *\napply_wqattrs_prepare(struct workqueue_struct *wq,\n\t\t      const struct workqueue_attrs *attrs,\n\t\t      const cpumask_var_t unbound_cpumask)\n{\n\tstruct apply_wqattrs_ctx *ctx;\n\tstruct workqueue_attrs *new_attrs;\n\tint cpu;\n\n\tlockdep_assert_held(&wq_pool_mutex);\n\n\tif (WARN_ON(attrs->affn_scope < 0 ||\n\t\t    attrs->affn_scope >= WQ_AFFN_NR_TYPES))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tctx = kzalloc(struct_size(ctx, pwq_tbl, nr_cpu_ids), GFP_KERNEL);\n\n\tnew_attrs = alloc_workqueue_attrs();\n\tif (!ctx || !new_attrs)\n\t\tgoto out_free;\n\n\t \n\tcopy_workqueue_attrs(new_attrs, attrs);\n\twqattrs_actualize_cpumask(new_attrs, unbound_cpumask);\n\tcpumask_copy(new_attrs->__pod_cpumask, new_attrs->cpumask);\n\tctx->dfl_pwq = alloc_unbound_pwq(wq, new_attrs);\n\tif (!ctx->dfl_pwq)\n\t\tgoto out_free;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tif (new_attrs->ordered) {\n\t\t\tctx->dfl_pwq->refcnt++;\n\t\t\tctx->pwq_tbl[cpu] = ctx->dfl_pwq;\n\t\t} else {\n\t\t\twq_calc_pod_cpumask(new_attrs, cpu, -1);\n\t\t\tctx->pwq_tbl[cpu] = alloc_unbound_pwq(wq, new_attrs);\n\t\t\tif (!ctx->pwq_tbl[cpu])\n\t\t\t\tgoto out_free;\n\t\t}\n\t}\n\n\t \n\tcopy_workqueue_attrs(new_attrs, attrs);\n\tcpumask_and(new_attrs->cpumask, new_attrs->cpumask, cpu_possible_mask);\n\tcpumask_copy(new_attrs->__pod_cpumask, new_attrs->cpumask);\n\tctx->attrs = new_attrs;\n\n\tctx->wq = wq;\n\treturn ctx;\n\nout_free:\n\tfree_workqueue_attrs(new_attrs);\n\tapply_wqattrs_cleanup(ctx);\n\treturn ERR_PTR(-ENOMEM);\n}\n\n \nstatic void apply_wqattrs_commit(struct apply_wqattrs_ctx *ctx)\n{\n\tint cpu;\n\n\t \n\tmutex_lock(&ctx->wq->mutex);\n\n\tcopy_workqueue_attrs(ctx->wq->unbound_attrs, ctx->attrs);\n\n\t \n\tfor_each_possible_cpu(cpu)\n\t\tctx->pwq_tbl[cpu] = install_unbound_pwq(ctx->wq, cpu,\n\t\t\t\t\t\t\tctx->pwq_tbl[cpu]);\n\n\t \n\tlink_pwq(ctx->dfl_pwq);\n\tswap(ctx->wq->dfl_pwq, ctx->dfl_pwq);\n\n\tmutex_unlock(&ctx->wq->mutex);\n}\n\nstatic void apply_wqattrs_lock(void)\n{\n\t \n\tcpus_read_lock();\n\tmutex_lock(&wq_pool_mutex);\n}\n\nstatic void apply_wqattrs_unlock(void)\n{\n\tmutex_unlock(&wq_pool_mutex);\n\tcpus_read_unlock();\n}\n\nstatic int apply_workqueue_attrs_locked(struct workqueue_struct *wq,\n\t\t\t\t\tconst struct workqueue_attrs *attrs)\n{\n\tstruct apply_wqattrs_ctx *ctx;\n\n\t \n\tif (WARN_ON(!(wq->flags & WQ_UNBOUND)))\n\t\treturn -EINVAL;\n\n\t \n\tif (!list_empty(&wq->pwqs)) {\n\t\tif (WARN_ON(wq->flags & __WQ_ORDERED_EXPLICIT))\n\t\t\treturn -EINVAL;\n\n\t\twq->flags &= ~__WQ_ORDERED;\n\t}\n\n\tctx = apply_wqattrs_prepare(wq, attrs, wq_unbound_cpumask);\n\tif (IS_ERR(ctx))\n\t\treturn PTR_ERR(ctx);\n\n\t \n\tapply_wqattrs_commit(ctx);\n\tapply_wqattrs_cleanup(ctx);\n\n\treturn 0;\n}\n\n \nint apply_workqueue_attrs(struct workqueue_struct *wq,\n\t\t\t  const struct workqueue_attrs *attrs)\n{\n\tint ret;\n\n\tlockdep_assert_cpus_held();\n\n\tmutex_lock(&wq_pool_mutex);\n\tret = apply_workqueue_attrs_locked(wq, attrs);\n\tmutex_unlock(&wq_pool_mutex);\n\n\treturn ret;\n}\n\n \nstatic void wq_update_pod(struct workqueue_struct *wq, int cpu,\n\t\t\t  int hotplug_cpu, bool online)\n{\n\tint off_cpu = online ? -1 : hotplug_cpu;\n\tstruct pool_workqueue *old_pwq = NULL, *pwq;\n\tstruct workqueue_attrs *target_attrs;\n\n\tlockdep_assert_held(&wq_pool_mutex);\n\n\tif (!(wq->flags & WQ_UNBOUND) || wq->unbound_attrs->ordered)\n\t\treturn;\n\n\t \n\ttarget_attrs = wq_update_pod_attrs_buf;\n\n\tcopy_workqueue_attrs(target_attrs, wq->unbound_attrs);\n\twqattrs_actualize_cpumask(target_attrs, wq_unbound_cpumask);\n\n\t \n\twq_calc_pod_cpumask(target_attrs, cpu, off_cpu);\n\tpwq = rcu_dereference_protected(*per_cpu_ptr(wq->cpu_pwq, cpu),\n\t\t\t\t\tlockdep_is_held(&wq_pool_mutex));\n\tif (wqattrs_equal(target_attrs, pwq->pool->attrs))\n\t\treturn;\n\n\t \n\tpwq = alloc_unbound_pwq(wq, target_attrs);\n\tif (!pwq) {\n\t\tpr_warn(\"workqueue: allocation failed while updating CPU pod affinity of \\\"%s\\\"\\n\",\n\t\t\twq->name);\n\t\tgoto use_dfl_pwq;\n\t}\n\n\t \n\tmutex_lock(&wq->mutex);\n\told_pwq = install_unbound_pwq(wq, cpu, pwq);\n\tgoto out_unlock;\n\nuse_dfl_pwq:\n\tmutex_lock(&wq->mutex);\n\traw_spin_lock_irq(&wq->dfl_pwq->pool->lock);\n\tget_pwq(wq->dfl_pwq);\n\traw_spin_unlock_irq(&wq->dfl_pwq->pool->lock);\n\told_pwq = install_unbound_pwq(wq, cpu, wq->dfl_pwq);\nout_unlock:\n\tmutex_unlock(&wq->mutex);\n\tput_pwq_unlocked(old_pwq);\n}\n\nstatic int alloc_and_link_pwqs(struct workqueue_struct *wq)\n{\n\tbool highpri = wq->flags & WQ_HIGHPRI;\n\tint cpu, ret;\n\n\twq->cpu_pwq = alloc_percpu(struct pool_workqueue *);\n\tif (!wq->cpu_pwq)\n\t\tgoto enomem;\n\n\tif (!(wq->flags & WQ_UNBOUND)) {\n\t\tfor_each_possible_cpu(cpu) {\n\t\t\tstruct pool_workqueue **pwq_p =\n\t\t\t\tper_cpu_ptr(wq->cpu_pwq, cpu);\n\t\t\tstruct worker_pool *pool =\n\t\t\t\t&(per_cpu_ptr(cpu_worker_pools, cpu)[highpri]);\n\n\t\t\t*pwq_p = kmem_cache_alloc_node(pwq_cache, GFP_KERNEL,\n\t\t\t\t\t\t       pool->node);\n\t\t\tif (!*pwq_p)\n\t\t\t\tgoto enomem;\n\n\t\t\tinit_pwq(*pwq_p, wq, pool);\n\n\t\t\tmutex_lock(&wq->mutex);\n\t\t\tlink_pwq(*pwq_p);\n\t\t\tmutex_unlock(&wq->mutex);\n\t\t}\n\t\treturn 0;\n\t}\n\n\tcpus_read_lock();\n\tif (wq->flags & __WQ_ORDERED) {\n\t\tret = apply_workqueue_attrs(wq, ordered_wq_attrs[highpri]);\n\t\t \n\t\tWARN(!ret && (wq->pwqs.next != &wq->dfl_pwq->pwqs_node ||\n\t\t\t      wq->pwqs.prev != &wq->dfl_pwq->pwqs_node),\n\t\t     \"ordering guarantee broken for workqueue %s\\n\", wq->name);\n\t} else {\n\t\tret = apply_workqueue_attrs(wq, unbound_std_wq_attrs[highpri]);\n\t}\n\tcpus_read_unlock();\n\n\t \n\tif (ret)\n\t\tkthread_flush_worker(pwq_release_worker);\n\n\treturn ret;\n\nenomem:\n\tif (wq->cpu_pwq) {\n\t\tfor_each_possible_cpu(cpu) {\n\t\t\tstruct pool_workqueue *pwq = *per_cpu_ptr(wq->cpu_pwq, cpu);\n\n\t\t\tif (pwq)\n\t\t\t\tkmem_cache_free(pwq_cache, pwq);\n\t\t}\n\t\tfree_percpu(wq->cpu_pwq);\n\t\twq->cpu_pwq = NULL;\n\t}\n\treturn -ENOMEM;\n}\n\nstatic int wq_clamp_max_active(int max_active, unsigned int flags,\n\t\t\t       const char *name)\n{\n\tif (max_active < 1 || max_active > WQ_MAX_ACTIVE)\n\t\tpr_warn(\"workqueue: max_active %d requested for %s is out of range, clamping between %d and %d\\n\",\n\t\t\tmax_active, name, 1, WQ_MAX_ACTIVE);\n\n\treturn clamp_val(max_active, 1, WQ_MAX_ACTIVE);\n}\n\n \nstatic int init_rescuer(struct workqueue_struct *wq)\n{\n\tstruct worker *rescuer;\n\tint ret;\n\n\tif (!(wq->flags & WQ_MEM_RECLAIM))\n\t\treturn 0;\n\n\trescuer = alloc_worker(NUMA_NO_NODE);\n\tif (!rescuer) {\n\t\tpr_err(\"workqueue: Failed to allocate a rescuer for wq \\\"%s\\\"\\n\",\n\t\t       wq->name);\n\t\treturn -ENOMEM;\n\t}\n\n\trescuer->rescue_wq = wq;\n\trescuer->task = kthread_create(rescuer_thread, rescuer, \"kworker/R-%s\", wq->name);\n\tif (IS_ERR(rescuer->task)) {\n\t\tret = PTR_ERR(rescuer->task);\n\t\tpr_err(\"workqueue: Failed to create a rescuer kthread for wq \\\"%s\\\": %pe\",\n\t\t       wq->name, ERR_PTR(ret));\n\t\tkfree(rescuer);\n\t\treturn ret;\n\t}\n\n\twq->rescuer = rescuer;\n\tkthread_bind_mask(rescuer->task, cpu_possible_mask);\n\twake_up_process(rescuer->task);\n\n\treturn 0;\n}\n\n__printf(1, 4)\nstruct workqueue_struct *alloc_workqueue(const char *fmt,\n\t\t\t\t\t unsigned int flags,\n\t\t\t\t\t int max_active, ...)\n{\n\tva_list args;\n\tstruct workqueue_struct *wq;\n\tstruct pool_workqueue *pwq;\n\n\t \n\tif ((flags & WQ_UNBOUND) && max_active == 1)\n\t\tflags |= __WQ_ORDERED;\n\n\t \n\tif ((flags & WQ_POWER_EFFICIENT) && wq_power_efficient)\n\t\tflags |= WQ_UNBOUND;\n\n\t \n\twq = kzalloc(sizeof(*wq), GFP_KERNEL);\n\tif (!wq)\n\t\treturn NULL;\n\n\tif (flags & WQ_UNBOUND) {\n\t\twq->unbound_attrs = alloc_workqueue_attrs();\n\t\tif (!wq->unbound_attrs)\n\t\t\tgoto err_free_wq;\n\t}\n\n\tva_start(args, max_active);\n\tvsnprintf(wq->name, sizeof(wq->name), fmt, args);\n\tva_end(args);\n\n\tmax_active = max_active ?: WQ_DFL_ACTIVE;\n\tmax_active = wq_clamp_max_active(max_active, flags, wq->name);\n\n\t \n\twq->flags = flags;\n\twq->saved_max_active = max_active;\n\tmutex_init(&wq->mutex);\n\tatomic_set(&wq->nr_pwqs_to_flush, 0);\n\tINIT_LIST_HEAD(&wq->pwqs);\n\tINIT_LIST_HEAD(&wq->flusher_queue);\n\tINIT_LIST_HEAD(&wq->flusher_overflow);\n\tINIT_LIST_HEAD(&wq->maydays);\n\n\twq_init_lockdep(wq);\n\tINIT_LIST_HEAD(&wq->list);\n\n\tif (alloc_and_link_pwqs(wq) < 0)\n\t\tgoto err_unreg_lockdep;\n\n\tif (wq_online && init_rescuer(wq) < 0)\n\t\tgoto err_destroy;\n\n\tif ((wq->flags & WQ_SYSFS) && workqueue_sysfs_register(wq))\n\t\tgoto err_destroy;\n\n\t \n\tmutex_lock(&wq_pool_mutex);\n\n\tmutex_lock(&wq->mutex);\n\tfor_each_pwq(pwq, wq)\n\t\tpwq_adjust_max_active(pwq);\n\tmutex_unlock(&wq->mutex);\n\n\tlist_add_tail_rcu(&wq->list, &workqueues);\n\n\tmutex_unlock(&wq_pool_mutex);\n\n\treturn wq;\n\nerr_unreg_lockdep:\n\twq_unregister_lockdep(wq);\n\twq_free_lockdep(wq);\nerr_free_wq:\n\tfree_workqueue_attrs(wq->unbound_attrs);\n\tkfree(wq);\n\treturn NULL;\nerr_destroy:\n\tdestroy_workqueue(wq);\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(alloc_workqueue);\n\nstatic bool pwq_busy(struct pool_workqueue *pwq)\n{\n\tint i;\n\n\tfor (i = 0; i < WORK_NR_COLORS; i++)\n\t\tif (pwq->nr_in_flight[i])\n\t\t\treturn true;\n\n\tif ((pwq != pwq->wq->dfl_pwq) && (pwq->refcnt > 1))\n\t\treturn true;\n\tif (pwq->nr_active || !list_empty(&pwq->inactive_works))\n\t\treturn true;\n\n\treturn false;\n}\n\n \nvoid destroy_workqueue(struct workqueue_struct *wq)\n{\n\tstruct pool_workqueue *pwq;\n\tint cpu;\n\n\t \n\tworkqueue_sysfs_unregister(wq);\n\n\t \n\tmutex_lock(&wq->mutex);\n\twq->flags |= __WQ_DESTROYING;\n\tmutex_unlock(&wq->mutex);\n\n\t \n\tdrain_workqueue(wq);\n\n\t \n\tif (wq->rescuer) {\n\t\tstruct worker *rescuer = wq->rescuer;\n\n\t\t \n\t\traw_spin_lock_irq(&wq_mayday_lock);\n\t\twq->rescuer = NULL;\n\t\traw_spin_unlock_irq(&wq_mayday_lock);\n\n\t\t \n\t\tkthread_stop(rescuer->task);\n\t\tkfree(rescuer);\n\t}\n\n\t \n\tmutex_lock(&wq_pool_mutex);\n\tmutex_lock(&wq->mutex);\n\tfor_each_pwq(pwq, wq) {\n\t\traw_spin_lock_irq(&pwq->pool->lock);\n\t\tif (WARN_ON(pwq_busy(pwq))) {\n\t\t\tpr_warn(\"%s: %s has the following busy pwq\\n\",\n\t\t\t\t__func__, wq->name);\n\t\t\tshow_pwq(pwq);\n\t\t\traw_spin_unlock_irq(&pwq->pool->lock);\n\t\t\tmutex_unlock(&wq->mutex);\n\t\t\tmutex_unlock(&wq_pool_mutex);\n\t\t\tshow_one_workqueue(wq);\n\t\t\treturn;\n\t\t}\n\t\traw_spin_unlock_irq(&pwq->pool->lock);\n\t}\n\tmutex_unlock(&wq->mutex);\n\n\t \n\tlist_del_rcu(&wq->list);\n\tmutex_unlock(&wq_pool_mutex);\n\n\t \n\trcu_read_lock();\n\n\tfor_each_possible_cpu(cpu) {\n\t\tpwq = rcu_access_pointer(*per_cpu_ptr(wq->cpu_pwq, cpu));\n\t\tRCU_INIT_POINTER(*per_cpu_ptr(wq->cpu_pwq, cpu), NULL);\n\t\tput_pwq_unlocked(pwq);\n\t}\n\n\tput_pwq_unlocked(wq->dfl_pwq);\n\twq->dfl_pwq = NULL;\n\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL_GPL(destroy_workqueue);\n\n \nvoid workqueue_set_max_active(struct workqueue_struct *wq, int max_active)\n{\n\tstruct pool_workqueue *pwq;\n\n\t \n\tif (WARN_ON(wq->flags & __WQ_ORDERED_EXPLICIT))\n\t\treturn;\n\n\tmax_active = wq_clamp_max_active(max_active, wq->flags, wq->name);\n\n\tmutex_lock(&wq->mutex);\n\n\twq->flags &= ~__WQ_ORDERED;\n\twq->saved_max_active = max_active;\n\n\tfor_each_pwq(pwq, wq)\n\t\tpwq_adjust_max_active(pwq);\n\n\tmutex_unlock(&wq->mutex);\n}\nEXPORT_SYMBOL_GPL(workqueue_set_max_active);\n\n \nstruct work_struct *current_work(void)\n{\n\tstruct worker *worker = current_wq_worker();\n\n\treturn worker ? worker->current_work : NULL;\n}\nEXPORT_SYMBOL(current_work);\n\n \nbool current_is_workqueue_rescuer(void)\n{\n\tstruct worker *worker = current_wq_worker();\n\n\treturn worker && worker->rescue_wq;\n}\n\n \nbool workqueue_congested(int cpu, struct workqueue_struct *wq)\n{\n\tstruct pool_workqueue *pwq;\n\tbool ret;\n\n\trcu_read_lock();\n\tpreempt_disable();\n\n\tif (cpu == WORK_CPU_UNBOUND)\n\t\tcpu = smp_processor_id();\n\n\tpwq = *per_cpu_ptr(wq->cpu_pwq, cpu);\n\tret = !list_empty(&pwq->inactive_works);\n\n\tpreempt_enable();\n\trcu_read_unlock();\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(workqueue_congested);\n\n \nunsigned int work_busy(struct work_struct *work)\n{\n\tstruct worker_pool *pool;\n\tunsigned long flags;\n\tunsigned int ret = 0;\n\n\tif (work_pending(work))\n\t\tret |= WORK_BUSY_PENDING;\n\n\trcu_read_lock();\n\tpool = get_work_pool(work);\n\tif (pool) {\n\t\traw_spin_lock_irqsave(&pool->lock, flags);\n\t\tif (find_worker_executing_work(pool, work))\n\t\t\tret |= WORK_BUSY_RUNNING;\n\t\traw_spin_unlock_irqrestore(&pool->lock, flags);\n\t}\n\trcu_read_unlock();\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(work_busy);\n\n \nvoid set_worker_desc(const char *fmt, ...)\n{\n\tstruct worker *worker = current_wq_worker();\n\tva_list args;\n\n\tif (worker) {\n\t\tva_start(args, fmt);\n\t\tvsnprintf(worker->desc, sizeof(worker->desc), fmt, args);\n\t\tva_end(args);\n\t}\n}\nEXPORT_SYMBOL_GPL(set_worker_desc);\n\n \nvoid print_worker_info(const char *log_lvl, struct task_struct *task)\n{\n\twork_func_t *fn = NULL;\n\tchar name[WQ_NAME_LEN] = { };\n\tchar desc[WORKER_DESC_LEN] = { };\n\tstruct pool_workqueue *pwq = NULL;\n\tstruct workqueue_struct *wq = NULL;\n\tstruct worker *worker;\n\n\tif (!(task->flags & PF_WQ_WORKER))\n\t\treturn;\n\n\t \n\tworker = kthread_probe_data(task);\n\n\t \n\tcopy_from_kernel_nofault(&fn, &worker->current_func, sizeof(fn));\n\tcopy_from_kernel_nofault(&pwq, &worker->current_pwq, sizeof(pwq));\n\tcopy_from_kernel_nofault(&wq, &pwq->wq, sizeof(wq));\n\tcopy_from_kernel_nofault(name, wq->name, sizeof(name) - 1);\n\tcopy_from_kernel_nofault(desc, worker->desc, sizeof(desc) - 1);\n\n\tif (fn || name[0] || desc[0]) {\n\t\tprintk(\"%sWorkqueue: %s %ps\", log_lvl, name, fn);\n\t\tif (strcmp(name, desc))\n\t\t\tpr_cont(\" (%s)\", desc);\n\t\tpr_cont(\"\\n\");\n\t}\n}\n\nstatic void pr_cont_pool_info(struct worker_pool *pool)\n{\n\tpr_cont(\" cpus=%*pbl\", nr_cpumask_bits, pool->attrs->cpumask);\n\tif (pool->node != NUMA_NO_NODE)\n\t\tpr_cont(\" node=%d\", pool->node);\n\tpr_cont(\" flags=0x%x nice=%d\", pool->flags, pool->attrs->nice);\n}\n\nstruct pr_cont_work_struct {\n\tbool comma;\n\twork_func_t func;\n\tlong ctr;\n};\n\nstatic void pr_cont_work_flush(bool comma, work_func_t func, struct pr_cont_work_struct *pcwsp)\n{\n\tif (!pcwsp->ctr)\n\t\tgoto out_record;\n\tif (func == pcwsp->func) {\n\t\tpcwsp->ctr++;\n\t\treturn;\n\t}\n\tif (pcwsp->ctr == 1)\n\t\tpr_cont(\"%s %ps\", pcwsp->comma ? \",\" : \"\", pcwsp->func);\n\telse\n\t\tpr_cont(\"%s %ld*%ps\", pcwsp->comma ? \",\" : \"\", pcwsp->ctr, pcwsp->func);\n\tpcwsp->ctr = 0;\nout_record:\n\tif ((long)func == -1L)\n\t\treturn;\n\tpcwsp->comma = comma;\n\tpcwsp->func = func;\n\tpcwsp->ctr = 1;\n}\n\nstatic void pr_cont_work(bool comma, struct work_struct *work, struct pr_cont_work_struct *pcwsp)\n{\n\tif (work->func == wq_barrier_func) {\n\t\tstruct wq_barrier *barr;\n\n\t\tbarr = container_of(work, struct wq_barrier, work);\n\n\t\tpr_cont_work_flush(comma, (work_func_t)-1, pcwsp);\n\t\tpr_cont(\"%s BAR(%d)\", comma ? \",\" : \"\",\n\t\t\ttask_pid_nr(barr->task));\n\t} else {\n\t\tif (!comma)\n\t\t\tpr_cont_work_flush(comma, (work_func_t)-1, pcwsp);\n\t\tpr_cont_work_flush(comma, work->func, pcwsp);\n\t}\n}\n\nstatic void show_pwq(struct pool_workqueue *pwq)\n{\n\tstruct pr_cont_work_struct pcws = { .ctr = 0, };\n\tstruct worker_pool *pool = pwq->pool;\n\tstruct work_struct *work;\n\tstruct worker *worker;\n\tbool has_in_flight = false, has_pending = false;\n\tint bkt;\n\n\tpr_info(\"  pwq %d:\", pool->id);\n\tpr_cont_pool_info(pool);\n\n\tpr_cont(\" active=%d/%d refcnt=%d%s\\n\",\n\t\tpwq->nr_active, pwq->max_active, pwq->refcnt,\n\t\t!list_empty(&pwq->mayday_node) ? \" MAYDAY\" : \"\");\n\n\thash_for_each(pool->busy_hash, bkt, worker, hentry) {\n\t\tif (worker->current_pwq == pwq) {\n\t\t\thas_in_flight = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (has_in_flight) {\n\t\tbool comma = false;\n\n\t\tpr_info(\"    in-flight:\");\n\t\thash_for_each(pool->busy_hash, bkt, worker, hentry) {\n\t\t\tif (worker->current_pwq != pwq)\n\t\t\t\tcontinue;\n\n\t\t\tpr_cont(\"%s %d%s:%ps\", comma ? \",\" : \"\",\n\t\t\t\ttask_pid_nr(worker->task),\n\t\t\t\tworker->rescue_wq ? \"(RESCUER)\" : \"\",\n\t\t\t\tworker->current_func);\n\t\t\tlist_for_each_entry(work, &worker->scheduled, entry)\n\t\t\t\tpr_cont_work(false, work, &pcws);\n\t\t\tpr_cont_work_flush(comma, (work_func_t)-1L, &pcws);\n\t\t\tcomma = true;\n\t\t}\n\t\tpr_cont(\"\\n\");\n\t}\n\n\tlist_for_each_entry(work, &pool->worklist, entry) {\n\t\tif (get_work_pwq(work) == pwq) {\n\t\t\thas_pending = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (has_pending) {\n\t\tbool comma = false;\n\n\t\tpr_info(\"    pending:\");\n\t\tlist_for_each_entry(work, &pool->worklist, entry) {\n\t\t\tif (get_work_pwq(work) != pwq)\n\t\t\t\tcontinue;\n\n\t\t\tpr_cont_work(comma, work, &pcws);\n\t\t\tcomma = !(*work_data_bits(work) & WORK_STRUCT_LINKED);\n\t\t}\n\t\tpr_cont_work_flush(comma, (work_func_t)-1L, &pcws);\n\t\tpr_cont(\"\\n\");\n\t}\n\n\tif (!list_empty(&pwq->inactive_works)) {\n\t\tbool comma = false;\n\n\t\tpr_info(\"    inactive:\");\n\t\tlist_for_each_entry(work, &pwq->inactive_works, entry) {\n\t\t\tpr_cont_work(comma, work, &pcws);\n\t\t\tcomma = !(*work_data_bits(work) & WORK_STRUCT_LINKED);\n\t\t}\n\t\tpr_cont_work_flush(comma, (work_func_t)-1L, &pcws);\n\t\tpr_cont(\"\\n\");\n\t}\n}\n\n \nvoid show_one_workqueue(struct workqueue_struct *wq)\n{\n\tstruct pool_workqueue *pwq;\n\tbool idle = true;\n\tunsigned long flags;\n\n\tfor_each_pwq(pwq, wq) {\n\t\tif (pwq->nr_active || !list_empty(&pwq->inactive_works)) {\n\t\t\tidle = false;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (idle)  \n\t\treturn;\n\n\tpr_info(\"workqueue %s: flags=0x%x\\n\", wq->name, wq->flags);\n\n\tfor_each_pwq(pwq, wq) {\n\t\traw_spin_lock_irqsave(&pwq->pool->lock, flags);\n\t\tif (pwq->nr_active || !list_empty(&pwq->inactive_works)) {\n\t\t\t \n\t\t\tprintk_deferred_enter();\n\t\t\tshow_pwq(pwq);\n\t\t\tprintk_deferred_exit();\n\t\t}\n\t\traw_spin_unlock_irqrestore(&pwq->pool->lock, flags);\n\t\t \n\t\ttouch_nmi_watchdog();\n\t}\n\n}\n\n \nstatic void show_one_worker_pool(struct worker_pool *pool)\n{\n\tstruct worker *worker;\n\tbool first = true;\n\tunsigned long flags;\n\tunsigned long hung = 0;\n\n\traw_spin_lock_irqsave(&pool->lock, flags);\n\tif (pool->nr_workers == pool->nr_idle)\n\t\tgoto next_pool;\n\n\t \n\tif (!list_empty(&pool->worklist))\n\t\thung = jiffies_to_msecs(jiffies - pool->watchdog_ts) / 1000;\n\n\t \n\tprintk_deferred_enter();\n\tpr_info(\"pool %d:\", pool->id);\n\tpr_cont_pool_info(pool);\n\tpr_cont(\" hung=%lus workers=%d\", hung, pool->nr_workers);\n\tif (pool->manager)\n\t\tpr_cont(\" manager: %d\",\n\t\t\ttask_pid_nr(pool->manager->task));\n\tlist_for_each_entry(worker, &pool->idle_list, entry) {\n\t\tpr_cont(\" %s%d\", first ? \"idle: \" : \"\",\n\t\t\ttask_pid_nr(worker->task));\n\t\tfirst = false;\n\t}\n\tpr_cont(\"\\n\");\n\tprintk_deferred_exit();\nnext_pool:\n\traw_spin_unlock_irqrestore(&pool->lock, flags);\n\t \n\ttouch_nmi_watchdog();\n\n}\n\n \nvoid show_all_workqueues(void)\n{\n\tstruct workqueue_struct *wq;\n\tstruct worker_pool *pool;\n\tint pi;\n\n\trcu_read_lock();\n\n\tpr_info(\"Showing busy workqueues and worker pools:\\n\");\n\n\tlist_for_each_entry_rcu(wq, &workqueues, list)\n\t\tshow_one_workqueue(wq);\n\n\tfor_each_pool(pool, pi)\n\t\tshow_one_worker_pool(pool);\n\n\trcu_read_unlock();\n}\n\n \nvoid show_freezable_workqueues(void)\n{\n\tstruct workqueue_struct *wq;\n\n\trcu_read_lock();\n\n\tpr_info(\"Showing freezable workqueues that are still busy:\\n\");\n\n\tlist_for_each_entry_rcu(wq, &workqueues, list) {\n\t\tif (!(wq->flags & WQ_FREEZABLE))\n\t\t\tcontinue;\n\t\tshow_one_workqueue(wq);\n\t}\n\n\trcu_read_unlock();\n}\n\n \nvoid wq_worker_comm(char *buf, size_t size, struct task_struct *task)\n{\n\tint off;\n\n\t \n\toff = strscpy(buf, task->comm, size);\n\tif (off < 0)\n\t\treturn;\n\n\t \n\tmutex_lock(&wq_pool_attach_mutex);\n\n\tif (task->flags & PF_WQ_WORKER) {\n\t\tstruct worker *worker = kthread_data(task);\n\t\tstruct worker_pool *pool = worker->pool;\n\n\t\tif (pool) {\n\t\t\traw_spin_lock_irq(&pool->lock);\n\t\t\t \n\t\t\tif (worker->desc[0] != '\\0') {\n\t\t\t\tif (worker->current_work)\n\t\t\t\t\tscnprintf(buf + off, size - off, \"+%s\",\n\t\t\t\t\t\t  worker->desc);\n\t\t\t\telse\n\t\t\t\t\tscnprintf(buf + off, size - off, \"-%s\",\n\t\t\t\t\t\t  worker->desc);\n\t\t\t}\n\t\t\traw_spin_unlock_irq(&pool->lock);\n\t\t}\n\t}\n\n\tmutex_unlock(&wq_pool_attach_mutex);\n}\n\n#ifdef CONFIG_SMP\n\n \n\nstatic void unbind_workers(int cpu)\n{\n\tstruct worker_pool *pool;\n\tstruct worker *worker;\n\n\tfor_each_cpu_worker_pool(pool, cpu) {\n\t\tmutex_lock(&wq_pool_attach_mutex);\n\t\traw_spin_lock_irq(&pool->lock);\n\n\t\t \n\t\tfor_each_pool_worker(worker, pool)\n\t\t\tworker->flags |= WORKER_UNBOUND;\n\n\t\tpool->flags |= POOL_DISASSOCIATED;\n\n\t\t \n\t\tpool->nr_running = 0;\n\n\t\t \n\t\tkick_pool(pool);\n\n\t\traw_spin_unlock_irq(&pool->lock);\n\n\t\tfor_each_pool_worker(worker, pool)\n\t\t\tunbind_worker(worker);\n\n\t\tmutex_unlock(&wq_pool_attach_mutex);\n\t}\n}\n\n \nstatic void rebind_workers(struct worker_pool *pool)\n{\n\tstruct worker *worker;\n\n\tlockdep_assert_held(&wq_pool_attach_mutex);\n\n\t \n\tfor_each_pool_worker(worker, pool) {\n\t\tkthread_set_per_cpu(worker->task, pool->cpu);\n\t\tWARN_ON_ONCE(set_cpus_allowed_ptr(worker->task,\n\t\t\t\t\t\t  pool_allowed_cpus(pool)) < 0);\n\t}\n\n\traw_spin_lock_irq(&pool->lock);\n\n\tpool->flags &= ~POOL_DISASSOCIATED;\n\n\tfor_each_pool_worker(worker, pool) {\n\t\tunsigned int worker_flags = worker->flags;\n\n\t\t \n\t\tWARN_ON_ONCE(!(worker_flags & WORKER_UNBOUND));\n\t\tworker_flags |= WORKER_REBOUND;\n\t\tworker_flags &= ~WORKER_UNBOUND;\n\t\tWRITE_ONCE(worker->flags, worker_flags);\n\t}\n\n\traw_spin_unlock_irq(&pool->lock);\n}\n\n \nstatic void restore_unbound_workers_cpumask(struct worker_pool *pool, int cpu)\n{\n\tstatic cpumask_t cpumask;\n\tstruct worker *worker;\n\n\tlockdep_assert_held(&wq_pool_attach_mutex);\n\n\t \n\tif (!cpumask_test_cpu(cpu, pool->attrs->cpumask))\n\t\treturn;\n\n\tcpumask_and(&cpumask, pool->attrs->cpumask, cpu_online_mask);\n\n\t \n\tfor_each_pool_worker(worker, pool)\n\t\tWARN_ON_ONCE(set_cpus_allowed_ptr(worker->task, &cpumask) < 0);\n}\n\nint workqueue_prepare_cpu(unsigned int cpu)\n{\n\tstruct worker_pool *pool;\n\n\tfor_each_cpu_worker_pool(pool, cpu) {\n\t\tif (pool->nr_workers)\n\t\t\tcontinue;\n\t\tif (!create_worker(pool))\n\t\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nint workqueue_online_cpu(unsigned int cpu)\n{\n\tstruct worker_pool *pool;\n\tstruct workqueue_struct *wq;\n\tint pi;\n\n\tmutex_lock(&wq_pool_mutex);\n\n\tfor_each_pool(pool, pi) {\n\t\tmutex_lock(&wq_pool_attach_mutex);\n\n\t\tif (pool->cpu == cpu)\n\t\t\trebind_workers(pool);\n\t\telse if (pool->cpu < 0)\n\t\t\trestore_unbound_workers_cpumask(pool, cpu);\n\n\t\tmutex_unlock(&wq_pool_attach_mutex);\n\t}\n\n\t \n\tlist_for_each_entry(wq, &workqueues, list) {\n\t\tstruct workqueue_attrs *attrs = wq->unbound_attrs;\n\n\t\tif (attrs) {\n\t\t\tconst struct wq_pod_type *pt = wqattrs_pod_type(attrs);\n\t\t\tint tcpu;\n\n\t\t\tfor_each_cpu(tcpu, pt->pod_cpus[pt->cpu_pod[cpu]])\n\t\t\t\twq_update_pod(wq, tcpu, cpu, true);\n\t\t}\n\t}\n\n\tmutex_unlock(&wq_pool_mutex);\n\treturn 0;\n}\n\nint workqueue_offline_cpu(unsigned int cpu)\n{\n\tstruct workqueue_struct *wq;\n\n\t \n\tif (WARN_ON(cpu != smp_processor_id()))\n\t\treturn -1;\n\n\tunbind_workers(cpu);\n\n\t \n\tmutex_lock(&wq_pool_mutex);\n\tlist_for_each_entry(wq, &workqueues, list) {\n\t\tstruct workqueue_attrs *attrs = wq->unbound_attrs;\n\n\t\tif (attrs) {\n\t\t\tconst struct wq_pod_type *pt = wqattrs_pod_type(attrs);\n\t\t\tint tcpu;\n\n\t\t\tfor_each_cpu(tcpu, pt->pod_cpus[pt->cpu_pod[cpu]])\n\t\t\t\twq_update_pod(wq, tcpu, cpu, false);\n\t\t}\n\t}\n\tmutex_unlock(&wq_pool_mutex);\n\n\treturn 0;\n}\n\nstruct work_for_cpu {\n\tstruct work_struct work;\n\tlong (*fn)(void *);\n\tvoid *arg;\n\tlong ret;\n};\n\nstatic void work_for_cpu_fn(struct work_struct *work)\n{\n\tstruct work_for_cpu *wfc = container_of(work, struct work_for_cpu, work);\n\n\twfc->ret = wfc->fn(wfc->arg);\n}\n\n \nlong work_on_cpu_key(int cpu, long (*fn)(void *),\n\t\t     void *arg, struct lock_class_key *key)\n{\n\tstruct work_for_cpu wfc = { .fn = fn, .arg = arg };\n\n\tINIT_WORK_ONSTACK_KEY(&wfc.work, work_for_cpu_fn, key);\n\tschedule_work_on(cpu, &wfc.work);\n\tflush_work(&wfc.work);\n\tdestroy_work_on_stack(&wfc.work);\n\treturn wfc.ret;\n}\nEXPORT_SYMBOL_GPL(work_on_cpu_key);\n\n \nlong work_on_cpu_safe_key(int cpu, long (*fn)(void *),\n\t\t\t  void *arg, struct lock_class_key *key)\n{\n\tlong ret = -ENODEV;\n\n\tcpus_read_lock();\n\tif (cpu_online(cpu))\n\t\tret = work_on_cpu_key(cpu, fn, arg, key);\n\tcpus_read_unlock();\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(work_on_cpu_safe_key);\n#endif  \n\n#ifdef CONFIG_FREEZER\n\n \nvoid freeze_workqueues_begin(void)\n{\n\tstruct workqueue_struct *wq;\n\tstruct pool_workqueue *pwq;\n\n\tmutex_lock(&wq_pool_mutex);\n\n\tWARN_ON_ONCE(workqueue_freezing);\n\tworkqueue_freezing = true;\n\n\tlist_for_each_entry(wq, &workqueues, list) {\n\t\tmutex_lock(&wq->mutex);\n\t\tfor_each_pwq(pwq, wq)\n\t\t\tpwq_adjust_max_active(pwq);\n\t\tmutex_unlock(&wq->mutex);\n\t}\n\n\tmutex_unlock(&wq_pool_mutex);\n}\n\n \nbool freeze_workqueues_busy(void)\n{\n\tbool busy = false;\n\tstruct workqueue_struct *wq;\n\tstruct pool_workqueue *pwq;\n\n\tmutex_lock(&wq_pool_mutex);\n\n\tWARN_ON_ONCE(!workqueue_freezing);\n\n\tlist_for_each_entry(wq, &workqueues, list) {\n\t\tif (!(wq->flags & WQ_FREEZABLE))\n\t\t\tcontinue;\n\t\t \n\t\trcu_read_lock();\n\t\tfor_each_pwq(pwq, wq) {\n\t\t\tWARN_ON_ONCE(pwq->nr_active < 0);\n\t\t\tif (pwq->nr_active) {\n\t\t\t\tbusy = true;\n\t\t\t\trcu_read_unlock();\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t}\n\t\trcu_read_unlock();\n\t}\nout_unlock:\n\tmutex_unlock(&wq_pool_mutex);\n\treturn busy;\n}\n\n \nvoid thaw_workqueues(void)\n{\n\tstruct workqueue_struct *wq;\n\tstruct pool_workqueue *pwq;\n\n\tmutex_lock(&wq_pool_mutex);\n\n\tif (!workqueue_freezing)\n\t\tgoto out_unlock;\n\n\tworkqueue_freezing = false;\n\n\t \n\tlist_for_each_entry(wq, &workqueues, list) {\n\t\tmutex_lock(&wq->mutex);\n\t\tfor_each_pwq(pwq, wq)\n\t\t\tpwq_adjust_max_active(pwq);\n\t\tmutex_unlock(&wq->mutex);\n\t}\n\nout_unlock:\n\tmutex_unlock(&wq_pool_mutex);\n}\n#endif  \n\nstatic int workqueue_apply_unbound_cpumask(const cpumask_var_t unbound_cpumask)\n{\n\tLIST_HEAD(ctxs);\n\tint ret = 0;\n\tstruct workqueue_struct *wq;\n\tstruct apply_wqattrs_ctx *ctx, *n;\n\n\tlockdep_assert_held(&wq_pool_mutex);\n\n\tlist_for_each_entry(wq, &workqueues, list) {\n\t\tif (!(wq->flags & WQ_UNBOUND))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (!list_empty(&wq->pwqs)) {\n\t\t\tif (wq->flags & __WQ_ORDERED_EXPLICIT)\n\t\t\t\tcontinue;\n\t\t\twq->flags &= ~__WQ_ORDERED;\n\t\t}\n\n\t\tctx = apply_wqattrs_prepare(wq, wq->unbound_attrs, unbound_cpumask);\n\t\tif (IS_ERR(ctx)) {\n\t\t\tret = PTR_ERR(ctx);\n\t\t\tbreak;\n\t\t}\n\n\t\tlist_add_tail(&ctx->list, &ctxs);\n\t}\n\n\tlist_for_each_entry_safe(ctx, n, &ctxs, list) {\n\t\tif (!ret)\n\t\t\tapply_wqattrs_commit(ctx);\n\t\tapply_wqattrs_cleanup(ctx);\n\t}\n\n\tif (!ret) {\n\t\tmutex_lock(&wq_pool_attach_mutex);\n\t\tcpumask_copy(wq_unbound_cpumask, unbound_cpumask);\n\t\tmutex_unlock(&wq_pool_attach_mutex);\n\t}\n\treturn ret;\n}\n\n \nint workqueue_set_unbound_cpumask(cpumask_var_t cpumask)\n{\n\tint ret = -EINVAL;\n\n\t \n\tcpumask_and(cpumask, cpumask, cpu_possible_mask);\n\tif (!cpumask_empty(cpumask)) {\n\t\tapply_wqattrs_lock();\n\t\tif (cpumask_equal(cpumask, wq_unbound_cpumask)) {\n\t\t\tret = 0;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tret = workqueue_apply_unbound_cpumask(cpumask);\n\nout_unlock:\n\t\tapply_wqattrs_unlock();\n\t}\n\n\treturn ret;\n}\n\nstatic int parse_affn_scope(const char *val)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(wq_affn_names); i++) {\n\t\tif (!strncasecmp(val, wq_affn_names[i], strlen(wq_affn_names[i])))\n\t\t\treturn i;\n\t}\n\treturn -EINVAL;\n}\n\nstatic int wq_affn_dfl_set(const char *val, const struct kernel_param *kp)\n{\n\tstruct workqueue_struct *wq;\n\tint affn, cpu;\n\n\taffn = parse_affn_scope(val);\n\tif (affn < 0)\n\t\treturn affn;\n\tif (affn == WQ_AFFN_DFL)\n\t\treturn -EINVAL;\n\n\tcpus_read_lock();\n\tmutex_lock(&wq_pool_mutex);\n\n\twq_affn_dfl = affn;\n\n\tlist_for_each_entry(wq, &workqueues, list) {\n\t\tfor_each_online_cpu(cpu) {\n\t\t\twq_update_pod(wq, cpu, cpu, true);\n\t\t}\n\t}\n\n\tmutex_unlock(&wq_pool_mutex);\n\tcpus_read_unlock();\n\n\treturn 0;\n}\n\nstatic int wq_affn_dfl_get(char *buffer, const struct kernel_param *kp)\n{\n\treturn scnprintf(buffer, PAGE_SIZE, \"%s\\n\", wq_affn_names[wq_affn_dfl]);\n}\n\nstatic const struct kernel_param_ops wq_affn_dfl_ops = {\n\t.set\t= wq_affn_dfl_set,\n\t.get\t= wq_affn_dfl_get,\n};\n\nmodule_param_cb(default_affinity_scope, &wq_affn_dfl_ops, NULL, 0644);\n\n#ifdef CONFIG_SYSFS\n \nstruct wq_device {\n\tstruct workqueue_struct\t\t*wq;\n\tstruct device\t\t\tdev;\n};\n\nstatic struct workqueue_struct *dev_to_wq(struct device *dev)\n{\n\tstruct wq_device *wq_dev = container_of(dev, struct wq_device, dev);\n\n\treturn wq_dev->wq;\n}\n\nstatic ssize_t per_cpu_show(struct device *dev, struct device_attribute *attr,\n\t\t\t    char *buf)\n{\n\tstruct workqueue_struct *wq = dev_to_wq(dev);\n\n\treturn scnprintf(buf, PAGE_SIZE, \"%d\\n\", (bool)!(wq->flags & WQ_UNBOUND));\n}\nstatic DEVICE_ATTR_RO(per_cpu);\n\nstatic ssize_t max_active_show(struct device *dev,\n\t\t\t       struct device_attribute *attr, char *buf)\n{\n\tstruct workqueue_struct *wq = dev_to_wq(dev);\n\n\treturn scnprintf(buf, PAGE_SIZE, \"%d\\n\", wq->saved_max_active);\n}\n\nstatic ssize_t max_active_store(struct device *dev,\n\t\t\t\tstruct device_attribute *attr, const char *buf,\n\t\t\t\tsize_t count)\n{\n\tstruct workqueue_struct *wq = dev_to_wq(dev);\n\tint val;\n\n\tif (sscanf(buf, \"%d\", &val) != 1 || val <= 0)\n\t\treturn -EINVAL;\n\n\tworkqueue_set_max_active(wq, val);\n\treturn count;\n}\nstatic DEVICE_ATTR_RW(max_active);\n\nstatic struct attribute *wq_sysfs_attrs[] = {\n\t&dev_attr_per_cpu.attr,\n\t&dev_attr_max_active.attr,\n\tNULL,\n};\nATTRIBUTE_GROUPS(wq_sysfs);\n\nstatic ssize_t wq_nice_show(struct device *dev, struct device_attribute *attr,\n\t\t\t    char *buf)\n{\n\tstruct workqueue_struct *wq = dev_to_wq(dev);\n\tint written;\n\n\tmutex_lock(&wq->mutex);\n\twritten = scnprintf(buf, PAGE_SIZE, \"%d\\n\", wq->unbound_attrs->nice);\n\tmutex_unlock(&wq->mutex);\n\n\treturn written;\n}\n\n \nstatic struct workqueue_attrs *wq_sysfs_prep_attrs(struct workqueue_struct *wq)\n{\n\tstruct workqueue_attrs *attrs;\n\n\tlockdep_assert_held(&wq_pool_mutex);\n\n\tattrs = alloc_workqueue_attrs();\n\tif (!attrs)\n\t\treturn NULL;\n\n\tcopy_workqueue_attrs(attrs, wq->unbound_attrs);\n\treturn attrs;\n}\n\nstatic ssize_t wq_nice_store(struct device *dev, struct device_attribute *attr,\n\t\t\t     const char *buf, size_t count)\n{\n\tstruct workqueue_struct *wq = dev_to_wq(dev);\n\tstruct workqueue_attrs *attrs;\n\tint ret = -ENOMEM;\n\n\tapply_wqattrs_lock();\n\n\tattrs = wq_sysfs_prep_attrs(wq);\n\tif (!attrs)\n\t\tgoto out_unlock;\n\n\tif (sscanf(buf, \"%d\", &attrs->nice) == 1 &&\n\t    attrs->nice >= MIN_NICE && attrs->nice <= MAX_NICE)\n\t\tret = apply_workqueue_attrs_locked(wq, attrs);\n\telse\n\t\tret = -EINVAL;\n\nout_unlock:\n\tapply_wqattrs_unlock();\n\tfree_workqueue_attrs(attrs);\n\treturn ret ?: count;\n}\n\nstatic ssize_t wq_cpumask_show(struct device *dev,\n\t\t\t       struct device_attribute *attr, char *buf)\n{\n\tstruct workqueue_struct *wq = dev_to_wq(dev);\n\tint written;\n\n\tmutex_lock(&wq->mutex);\n\twritten = scnprintf(buf, PAGE_SIZE, \"%*pb\\n\",\n\t\t\t    cpumask_pr_args(wq->unbound_attrs->cpumask));\n\tmutex_unlock(&wq->mutex);\n\treturn written;\n}\n\nstatic ssize_t wq_cpumask_store(struct device *dev,\n\t\t\t\tstruct device_attribute *attr,\n\t\t\t\tconst char *buf, size_t count)\n{\n\tstruct workqueue_struct *wq = dev_to_wq(dev);\n\tstruct workqueue_attrs *attrs;\n\tint ret = -ENOMEM;\n\n\tapply_wqattrs_lock();\n\n\tattrs = wq_sysfs_prep_attrs(wq);\n\tif (!attrs)\n\t\tgoto out_unlock;\n\n\tret = cpumask_parse(buf, attrs->cpumask);\n\tif (!ret)\n\t\tret = apply_workqueue_attrs_locked(wq, attrs);\n\nout_unlock:\n\tapply_wqattrs_unlock();\n\tfree_workqueue_attrs(attrs);\n\treturn ret ?: count;\n}\n\nstatic ssize_t wq_affn_scope_show(struct device *dev,\n\t\t\t\t  struct device_attribute *attr, char *buf)\n{\n\tstruct workqueue_struct *wq = dev_to_wq(dev);\n\tint written;\n\n\tmutex_lock(&wq->mutex);\n\tif (wq->unbound_attrs->affn_scope == WQ_AFFN_DFL)\n\t\twritten = scnprintf(buf, PAGE_SIZE, \"%s (%s)\\n\",\n\t\t\t\t    wq_affn_names[WQ_AFFN_DFL],\n\t\t\t\t    wq_affn_names[wq_affn_dfl]);\n\telse\n\t\twritten = scnprintf(buf, PAGE_SIZE, \"%s\\n\",\n\t\t\t\t    wq_affn_names[wq->unbound_attrs->affn_scope]);\n\tmutex_unlock(&wq->mutex);\n\n\treturn written;\n}\n\nstatic ssize_t wq_affn_scope_store(struct device *dev,\n\t\t\t\t   struct device_attribute *attr,\n\t\t\t\t   const char *buf, size_t count)\n{\n\tstruct workqueue_struct *wq = dev_to_wq(dev);\n\tstruct workqueue_attrs *attrs;\n\tint affn, ret = -ENOMEM;\n\n\taffn = parse_affn_scope(buf);\n\tif (affn < 0)\n\t\treturn affn;\n\n\tapply_wqattrs_lock();\n\tattrs = wq_sysfs_prep_attrs(wq);\n\tif (attrs) {\n\t\tattrs->affn_scope = affn;\n\t\tret = apply_workqueue_attrs_locked(wq, attrs);\n\t}\n\tapply_wqattrs_unlock();\n\tfree_workqueue_attrs(attrs);\n\treturn ret ?: count;\n}\n\nstatic ssize_t wq_affinity_strict_show(struct device *dev,\n\t\t\t\t       struct device_attribute *attr, char *buf)\n{\n\tstruct workqueue_struct *wq = dev_to_wq(dev);\n\n\treturn scnprintf(buf, PAGE_SIZE, \"%d\\n\",\n\t\t\t wq->unbound_attrs->affn_strict);\n}\n\nstatic ssize_t wq_affinity_strict_store(struct device *dev,\n\t\t\t\t\tstruct device_attribute *attr,\n\t\t\t\t\tconst char *buf, size_t count)\n{\n\tstruct workqueue_struct *wq = dev_to_wq(dev);\n\tstruct workqueue_attrs *attrs;\n\tint v, ret = -ENOMEM;\n\n\tif (sscanf(buf, \"%d\", &v) != 1)\n\t\treturn -EINVAL;\n\n\tapply_wqattrs_lock();\n\tattrs = wq_sysfs_prep_attrs(wq);\n\tif (attrs) {\n\t\tattrs->affn_strict = (bool)v;\n\t\tret = apply_workqueue_attrs_locked(wq, attrs);\n\t}\n\tapply_wqattrs_unlock();\n\tfree_workqueue_attrs(attrs);\n\treturn ret ?: count;\n}\n\nstatic struct device_attribute wq_sysfs_unbound_attrs[] = {\n\t__ATTR(nice, 0644, wq_nice_show, wq_nice_store),\n\t__ATTR(cpumask, 0644, wq_cpumask_show, wq_cpumask_store),\n\t__ATTR(affinity_scope, 0644, wq_affn_scope_show, wq_affn_scope_store),\n\t__ATTR(affinity_strict, 0644, wq_affinity_strict_show, wq_affinity_strict_store),\n\t__ATTR_NULL,\n};\n\nstatic struct bus_type wq_subsys = {\n\t.name\t\t\t\t= \"workqueue\",\n\t.dev_groups\t\t\t= wq_sysfs_groups,\n};\n\nstatic ssize_t wq_unbound_cpumask_show(struct device *dev,\n\t\tstruct device_attribute *attr, char *buf)\n{\n\tint written;\n\n\tmutex_lock(&wq_pool_mutex);\n\twritten = scnprintf(buf, PAGE_SIZE, \"%*pb\\n\",\n\t\t\t    cpumask_pr_args(wq_unbound_cpumask));\n\tmutex_unlock(&wq_pool_mutex);\n\n\treturn written;\n}\n\nstatic ssize_t wq_unbound_cpumask_store(struct device *dev,\n\t\tstruct device_attribute *attr, const char *buf, size_t count)\n{\n\tcpumask_var_t cpumask;\n\tint ret;\n\n\tif (!zalloc_cpumask_var(&cpumask, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\tret = cpumask_parse(buf, cpumask);\n\tif (!ret)\n\t\tret = workqueue_set_unbound_cpumask(cpumask);\n\n\tfree_cpumask_var(cpumask);\n\treturn ret ? ret : count;\n}\n\nstatic struct device_attribute wq_sysfs_cpumask_attr =\n\t__ATTR(cpumask, 0644, wq_unbound_cpumask_show,\n\t       wq_unbound_cpumask_store);\n\nstatic int __init wq_sysfs_init(void)\n{\n\tstruct device *dev_root;\n\tint err;\n\n\terr = subsys_virtual_register(&wq_subsys, NULL);\n\tif (err)\n\t\treturn err;\n\n\tdev_root = bus_get_dev_root(&wq_subsys);\n\tif (dev_root) {\n\t\terr = device_create_file(dev_root, &wq_sysfs_cpumask_attr);\n\t\tput_device(dev_root);\n\t}\n\treturn err;\n}\ncore_initcall(wq_sysfs_init);\n\nstatic void wq_device_release(struct device *dev)\n{\n\tstruct wq_device *wq_dev = container_of(dev, struct wq_device, dev);\n\n\tkfree(wq_dev);\n}\n\n \nint workqueue_sysfs_register(struct workqueue_struct *wq)\n{\n\tstruct wq_device *wq_dev;\n\tint ret;\n\n\t \n\tif (WARN_ON(wq->flags & __WQ_ORDERED_EXPLICIT))\n\t\treturn -EINVAL;\n\n\twq->wq_dev = wq_dev = kzalloc(sizeof(*wq_dev), GFP_KERNEL);\n\tif (!wq_dev)\n\t\treturn -ENOMEM;\n\n\twq_dev->wq = wq;\n\twq_dev->dev.bus = &wq_subsys;\n\twq_dev->dev.release = wq_device_release;\n\tdev_set_name(&wq_dev->dev, \"%s\", wq->name);\n\n\t \n\tdev_set_uevent_suppress(&wq_dev->dev, true);\n\n\tret = device_register(&wq_dev->dev);\n\tif (ret) {\n\t\tput_device(&wq_dev->dev);\n\t\twq->wq_dev = NULL;\n\t\treturn ret;\n\t}\n\n\tif (wq->flags & WQ_UNBOUND) {\n\t\tstruct device_attribute *attr;\n\n\t\tfor (attr = wq_sysfs_unbound_attrs; attr->attr.name; attr++) {\n\t\t\tret = device_create_file(&wq_dev->dev, attr);\n\t\t\tif (ret) {\n\t\t\t\tdevice_unregister(&wq_dev->dev);\n\t\t\t\twq->wq_dev = NULL;\n\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\t}\n\n\tdev_set_uevent_suppress(&wq_dev->dev, false);\n\tkobject_uevent(&wq_dev->dev.kobj, KOBJ_ADD);\n\treturn 0;\n}\n\n \nstatic void workqueue_sysfs_unregister(struct workqueue_struct *wq)\n{\n\tstruct wq_device *wq_dev = wq->wq_dev;\n\n\tif (!wq->wq_dev)\n\t\treturn;\n\n\twq->wq_dev = NULL;\n\tdevice_unregister(&wq_dev->dev);\n}\n#else\t \nstatic void workqueue_sysfs_unregister(struct workqueue_struct *wq)\t{ }\n#endif\t \n\n \n#ifdef CONFIG_WQ_WATCHDOG\n\nstatic unsigned long wq_watchdog_thresh = 30;\nstatic struct timer_list wq_watchdog_timer;\n\nstatic unsigned long wq_watchdog_touched = INITIAL_JIFFIES;\nstatic DEFINE_PER_CPU(unsigned long, wq_watchdog_touched_cpu) = INITIAL_JIFFIES;\n\n \nstatic void show_cpu_pool_hog(struct worker_pool *pool)\n{\n\tstruct worker *worker;\n\tunsigned long flags;\n\tint bkt;\n\n\traw_spin_lock_irqsave(&pool->lock, flags);\n\n\thash_for_each(pool->busy_hash, bkt, worker, hentry) {\n\t\tif (task_is_running(worker->task)) {\n\t\t\t \n\t\t\tprintk_deferred_enter();\n\n\t\t\tpr_info(\"pool %d:\\n\", pool->id);\n\t\t\tsched_show_task(worker->task);\n\n\t\t\tprintk_deferred_exit();\n\t\t}\n\t}\n\n\traw_spin_unlock_irqrestore(&pool->lock, flags);\n}\n\nstatic void show_cpu_pools_hogs(void)\n{\n\tstruct worker_pool *pool;\n\tint pi;\n\n\tpr_info(\"Showing backtraces of running workers in stalled CPU-bound worker pools:\\n\");\n\n\trcu_read_lock();\n\n\tfor_each_pool(pool, pi) {\n\t\tif (pool->cpu_stall)\n\t\t\tshow_cpu_pool_hog(pool);\n\n\t}\n\n\trcu_read_unlock();\n}\n\nstatic void wq_watchdog_reset_touched(void)\n{\n\tint cpu;\n\n\twq_watchdog_touched = jiffies;\n\tfor_each_possible_cpu(cpu)\n\t\tper_cpu(wq_watchdog_touched_cpu, cpu) = jiffies;\n}\n\nstatic void wq_watchdog_timer_fn(struct timer_list *unused)\n{\n\tunsigned long thresh = READ_ONCE(wq_watchdog_thresh) * HZ;\n\tbool lockup_detected = false;\n\tbool cpu_pool_stall = false;\n\tunsigned long now = jiffies;\n\tstruct worker_pool *pool;\n\tint pi;\n\n\tif (!thresh)\n\t\treturn;\n\n\trcu_read_lock();\n\n\tfor_each_pool(pool, pi) {\n\t\tunsigned long pool_ts, touched, ts;\n\n\t\tpool->cpu_stall = false;\n\t\tif (list_empty(&pool->worklist))\n\t\t\tcontinue;\n\n\t\t \n\t\tkvm_check_and_clear_guest_paused();\n\n\t\t \n\t\tif (pool->cpu >= 0)\n\t\t\ttouched = READ_ONCE(per_cpu(wq_watchdog_touched_cpu, pool->cpu));\n\t\telse\n\t\t\ttouched = READ_ONCE(wq_watchdog_touched);\n\t\tpool_ts = READ_ONCE(pool->watchdog_ts);\n\n\t\tif (time_after(pool_ts, touched))\n\t\t\tts = pool_ts;\n\t\telse\n\t\t\tts = touched;\n\n\t\t \n\t\tif (time_after(now, ts + thresh)) {\n\t\t\tlockup_detected = true;\n\t\t\tif (pool->cpu >= 0) {\n\t\t\t\tpool->cpu_stall = true;\n\t\t\t\tcpu_pool_stall = true;\n\t\t\t}\n\t\t\tpr_emerg(\"BUG: workqueue lockup - pool\");\n\t\t\tpr_cont_pool_info(pool);\n\t\t\tpr_cont(\" stuck for %us!\\n\",\n\t\t\t\tjiffies_to_msecs(now - pool_ts) / 1000);\n\t\t}\n\n\n\t}\n\n\trcu_read_unlock();\n\n\tif (lockup_detected)\n\t\tshow_all_workqueues();\n\n\tif (cpu_pool_stall)\n\t\tshow_cpu_pools_hogs();\n\n\twq_watchdog_reset_touched();\n\tmod_timer(&wq_watchdog_timer, jiffies + thresh);\n}\n\nnotrace void wq_watchdog_touch(int cpu)\n{\n\tif (cpu >= 0)\n\t\tper_cpu(wq_watchdog_touched_cpu, cpu) = jiffies;\n\n\twq_watchdog_touched = jiffies;\n}\n\nstatic void wq_watchdog_set_thresh(unsigned long thresh)\n{\n\twq_watchdog_thresh = 0;\n\tdel_timer_sync(&wq_watchdog_timer);\n\n\tif (thresh) {\n\t\twq_watchdog_thresh = thresh;\n\t\twq_watchdog_reset_touched();\n\t\tmod_timer(&wq_watchdog_timer, jiffies + thresh * HZ);\n\t}\n}\n\nstatic int wq_watchdog_param_set_thresh(const char *val,\n\t\t\t\t\tconst struct kernel_param *kp)\n{\n\tunsigned long thresh;\n\tint ret;\n\n\tret = kstrtoul(val, 0, &thresh);\n\tif (ret)\n\t\treturn ret;\n\n\tif (system_wq)\n\t\twq_watchdog_set_thresh(thresh);\n\telse\n\t\twq_watchdog_thresh = thresh;\n\n\treturn 0;\n}\n\nstatic const struct kernel_param_ops wq_watchdog_thresh_ops = {\n\t.set\t= wq_watchdog_param_set_thresh,\n\t.get\t= param_get_ulong,\n};\n\nmodule_param_cb(watchdog_thresh, &wq_watchdog_thresh_ops, &wq_watchdog_thresh,\n\t\t0644);\n\nstatic void wq_watchdog_init(void)\n{\n\ttimer_setup(&wq_watchdog_timer, wq_watchdog_timer_fn, TIMER_DEFERRABLE);\n\twq_watchdog_set_thresh(wq_watchdog_thresh);\n}\n\n#else\t \n\nstatic inline void wq_watchdog_init(void) { }\n\n#endif\t \n\nstatic void __init restrict_unbound_cpumask(const char *name, const struct cpumask *mask)\n{\n\tif (!cpumask_intersects(wq_unbound_cpumask, mask)) {\n\t\tpr_warn(\"workqueue: Restricting unbound_cpumask (%*pb) with %s (%*pb) leaves no CPU, ignoring\\n\",\n\t\t\tcpumask_pr_args(wq_unbound_cpumask), name, cpumask_pr_args(mask));\n\t\treturn;\n\t}\n\n\tcpumask_and(wq_unbound_cpumask, wq_unbound_cpumask, mask);\n}\n\n \nvoid __init workqueue_init_early(void)\n{\n\tstruct wq_pod_type *pt = &wq_pod_types[WQ_AFFN_SYSTEM];\n\tint std_nice[NR_STD_WORKER_POOLS] = { 0, HIGHPRI_NICE_LEVEL };\n\tint i, cpu;\n\n\tBUILD_BUG_ON(__alignof__(struct pool_workqueue) < __alignof__(long long));\n\n\tBUG_ON(!alloc_cpumask_var(&wq_unbound_cpumask, GFP_KERNEL));\n\tcpumask_copy(wq_unbound_cpumask, cpu_possible_mask);\n\trestrict_unbound_cpumask(\"HK_TYPE_WQ\", housekeeping_cpumask(HK_TYPE_WQ));\n\trestrict_unbound_cpumask(\"HK_TYPE_DOMAIN\", housekeeping_cpumask(HK_TYPE_DOMAIN));\n\tif (!cpumask_empty(&wq_cmdline_cpumask))\n\t\trestrict_unbound_cpumask(\"workqueue.unbound_cpus\", &wq_cmdline_cpumask);\n\n\tpwq_cache = KMEM_CACHE(pool_workqueue, SLAB_PANIC);\n\n\twq_update_pod_attrs_buf = alloc_workqueue_attrs();\n\tBUG_ON(!wq_update_pod_attrs_buf);\n\n\t \n\tpt->pod_cpus = kcalloc(1, sizeof(pt->pod_cpus[0]), GFP_KERNEL);\n\tpt->pod_node = kcalloc(1, sizeof(pt->pod_node[0]), GFP_KERNEL);\n\tpt->cpu_pod = kcalloc(nr_cpu_ids, sizeof(pt->cpu_pod[0]), GFP_KERNEL);\n\tBUG_ON(!pt->pod_cpus || !pt->pod_node || !pt->cpu_pod);\n\n\tBUG_ON(!zalloc_cpumask_var_node(&pt->pod_cpus[0], GFP_KERNEL, NUMA_NO_NODE));\n\n\tpt->nr_pods = 1;\n\tcpumask_copy(pt->pod_cpus[0], cpu_possible_mask);\n\tpt->pod_node[0] = NUMA_NO_NODE;\n\tpt->cpu_pod[0] = 0;\n\n\t \n\tfor_each_possible_cpu(cpu) {\n\t\tstruct worker_pool *pool;\n\n\t\ti = 0;\n\t\tfor_each_cpu_worker_pool(pool, cpu) {\n\t\t\tBUG_ON(init_worker_pool(pool));\n\t\t\tpool->cpu = cpu;\n\t\t\tcpumask_copy(pool->attrs->cpumask, cpumask_of(cpu));\n\t\t\tcpumask_copy(pool->attrs->__pod_cpumask, cpumask_of(cpu));\n\t\t\tpool->attrs->nice = std_nice[i++];\n\t\t\tpool->attrs->affn_strict = true;\n\t\t\tpool->node = cpu_to_node(cpu);\n\n\t\t\t \n\t\t\tmutex_lock(&wq_pool_mutex);\n\t\t\tBUG_ON(worker_pool_assign_id(pool));\n\t\t\tmutex_unlock(&wq_pool_mutex);\n\t\t}\n\t}\n\n\t \n\tfor (i = 0; i < NR_STD_WORKER_POOLS; i++) {\n\t\tstruct workqueue_attrs *attrs;\n\n\t\tBUG_ON(!(attrs = alloc_workqueue_attrs()));\n\t\tattrs->nice = std_nice[i];\n\t\tunbound_std_wq_attrs[i] = attrs;\n\n\t\t \n\t\tBUG_ON(!(attrs = alloc_workqueue_attrs()));\n\t\tattrs->nice = std_nice[i];\n\t\tattrs->ordered = true;\n\t\tordered_wq_attrs[i] = attrs;\n\t}\n\n\tsystem_wq = alloc_workqueue(\"events\", 0, 0);\n\tsystem_highpri_wq = alloc_workqueue(\"events_highpri\", WQ_HIGHPRI, 0);\n\tsystem_long_wq = alloc_workqueue(\"events_long\", 0, 0);\n\tsystem_unbound_wq = alloc_workqueue(\"events_unbound\", WQ_UNBOUND,\n\t\t\t\t\t    WQ_MAX_ACTIVE);\n\tsystem_freezable_wq = alloc_workqueue(\"events_freezable\",\n\t\t\t\t\t      WQ_FREEZABLE, 0);\n\tsystem_power_efficient_wq = alloc_workqueue(\"events_power_efficient\",\n\t\t\t\t\t      WQ_POWER_EFFICIENT, 0);\n\tsystem_freezable_power_efficient_wq = alloc_workqueue(\"events_freezable_power_efficient\",\n\t\t\t\t\t      WQ_FREEZABLE | WQ_POWER_EFFICIENT,\n\t\t\t\t\t      0);\n\tBUG_ON(!system_wq || !system_highpri_wq || !system_long_wq ||\n\t       !system_unbound_wq || !system_freezable_wq ||\n\t       !system_power_efficient_wq ||\n\t       !system_freezable_power_efficient_wq);\n}\n\nstatic void __init wq_cpu_intensive_thresh_init(void)\n{\n\tunsigned long thresh;\n\tunsigned long bogo;\n\n\tpwq_release_worker = kthread_create_worker(0, \"pool_workqueue_release\");\n\tBUG_ON(IS_ERR(pwq_release_worker));\n\n\t \n\tif (wq_cpu_intensive_thresh_us != ULONG_MAX)\n\t\treturn;\n\n\t \n\tthresh = 10 * USEC_PER_MSEC;\n\n\t \n\tbogo = max_t(unsigned long, loops_per_jiffy / 500000 * HZ, 1);\n\tif (bogo < 4000)\n\t\tthresh = min_t(unsigned long, thresh * 4000 / bogo, USEC_PER_SEC);\n\n\tpr_debug(\"wq_cpu_intensive_thresh: lpj=%lu BogoMIPS=%lu thresh_us=%lu\\n\",\n\t\t loops_per_jiffy, bogo, thresh);\n\n\twq_cpu_intensive_thresh_us = thresh;\n}\n\n \nvoid __init workqueue_init(void)\n{\n\tstruct workqueue_struct *wq;\n\tstruct worker_pool *pool;\n\tint cpu, bkt;\n\n\twq_cpu_intensive_thresh_init();\n\n\tmutex_lock(&wq_pool_mutex);\n\n\t \n\tfor_each_possible_cpu(cpu) {\n\t\tfor_each_cpu_worker_pool(pool, cpu) {\n\t\t\tpool->node = cpu_to_node(cpu);\n\t\t}\n\t}\n\n\tlist_for_each_entry(wq, &workqueues, list) {\n\t\tWARN(init_rescuer(wq),\n\t\t     \"workqueue: failed to create early rescuer for %s\",\n\t\t     wq->name);\n\t}\n\n\tmutex_unlock(&wq_pool_mutex);\n\n\t \n\tfor_each_online_cpu(cpu) {\n\t\tfor_each_cpu_worker_pool(pool, cpu) {\n\t\t\tpool->flags &= ~POOL_DISASSOCIATED;\n\t\t\tBUG_ON(!create_worker(pool));\n\t\t}\n\t}\n\n\thash_for_each(unbound_pool_hash, bkt, pool, hash_node)\n\t\tBUG_ON(!create_worker(pool));\n\n\twq_online = true;\n\twq_watchdog_init();\n}\n\n \nstatic void __init init_pod_type(struct wq_pod_type *pt,\n\t\t\t\t bool (*cpus_share_pod)(int, int))\n{\n\tint cur, pre, cpu, pod;\n\n\tpt->nr_pods = 0;\n\n\t \n\tpt->cpu_pod = kcalloc(nr_cpu_ids, sizeof(pt->cpu_pod[0]), GFP_KERNEL);\n\tBUG_ON(!pt->cpu_pod);\n\n\tfor_each_possible_cpu(cur) {\n\t\tfor_each_possible_cpu(pre) {\n\t\t\tif (pre >= cur) {\n\t\t\t\tpt->cpu_pod[cur] = pt->nr_pods++;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (cpus_share_pod(cur, pre)) {\n\t\t\t\tpt->cpu_pod[cur] = pt->cpu_pod[pre];\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tpt->pod_cpus = kcalloc(pt->nr_pods, sizeof(pt->pod_cpus[0]), GFP_KERNEL);\n\tpt->pod_node = kcalloc(pt->nr_pods, sizeof(pt->pod_node[0]), GFP_KERNEL);\n\tBUG_ON(!pt->pod_cpus || !pt->pod_node);\n\n\tfor (pod = 0; pod < pt->nr_pods; pod++)\n\t\tBUG_ON(!zalloc_cpumask_var(&pt->pod_cpus[pod], GFP_KERNEL));\n\n\tfor_each_possible_cpu(cpu) {\n\t\tcpumask_set_cpu(cpu, pt->pod_cpus[pt->cpu_pod[cpu]]);\n\t\tpt->pod_node[pt->cpu_pod[cpu]] = cpu_to_node(cpu);\n\t}\n}\n\nstatic bool __init cpus_dont_share(int cpu0, int cpu1)\n{\n\treturn false;\n}\n\nstatic bool __init cpus_share_smt(int cpu0, int cpu1)\n{\n#ifdef CONFIG_SCHED_SMT\n\treturn cpumask_test_cpu(cpu0, cpu_smt_mask(cpu1));\n#else\n\treturn false;\n#endif\n}\n\nstatic bool __init cpus_share_numa(int cpu0, int cpu1)\n{\n\treturn cpu_to_node(cpu0) == cpu_to_node(cpu1);\n}\n\n \nvoid __init workqueue_init_topology(void)\n{\n\tstruct workqueue_struct *wq;\n\tint cpu;\n\n\tinit_pod_type(&wq_pod_types[WQ_AFFN_CPU], cpus_dont_share);\n\tinit_pod_type(&wq_pod_types[WQ_AFFN_SMT], cpus_share_smt);\n\tinit_pod_type(&wq_pod_types[WQ_AFFN_CACHE], cpus_share_cache);\n\tinit_pod_type(&wq_pod_types[WQ_AFFN_NUMA], cpus_share_numa);\n\n\tmutex_lock(&wq_pool_mutex);\n\n\t \n\tlist_for_each_entry(wq, &workqueues, list) {\n\t\tfor_each_online_cpu(cpu) {\n\t\t\twq_update_pod(wq, cpu, cpu, true);\n\t\t}\n\t}\n\n\tmutex_unlock(&wq_pool_mutex);\n}\n\nvoid __warn_flushing_systemwide_wq(void)\n{\n\tpr_warn(\"WARNING: Flushing system-wide workqueues will be prohibited in near future.\\n\");\n\tdump_stack();\n}\nEXPORT_SYMBOL(__warn_flushing_systemwide_wq);\n\nstatic int __init workqueue_unbound_cpus_setup(char *str)\n{\n\tif (cpulist_parse(str, &wq_cmdline_cpumask) < 0) {\n\t\tcpumask_clear(&wq_cmdline_cpumask);\n\t\tpr_warn(\"workqueue.unbound_cpus: incorrect CPU range, using default\\n\");\n\t}\n\n\treturn 1;\n}\n__setup(\"workqueue.unbound_cpus=\", workqueue_unbound_cpus_setup);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}