{
  "module_name": "cgroup.c",
  "hash_id": "95673c8cb5cc6261ba406d0eeadd9516634ec064f120d3e161d5a3b6073634e6",
  "original_prompt": "Ingested from linux-6.6.14/kernel/cgroup/cgroup.c",
  "human_readable_source": " \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include \"cgroup-internal.h\"\n\n#include <linux/bpf-cgroup.h>\n#include <linux/cred.h>\n#include <linux/errno.h>\n#include <linux/init_task.h>\n#include <linux/kernel.h>\n#include <linux/magic.h>\n#include <linux/mutex.h>\n#include <linux/mount.h>\n#include <linux/pagemap.h>\n#include <linux/proc_fs.h>\n#include <linux/rcupdate.h>\n#include <linux/sched.h>\n#include <linux/sched/task.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/string.h>\n#include <linux/hashtable.h>\n#include <linux/idr.h>\n#include <linux/kthread.h>\n#include <linux/atomic.h>\n#include <linux/cpuset.h>\n#include <linux/proc_ns.h>\n#include <linux/nsproxy.h>\n#include <linux/file.h>\n#include <linux/fs_parser.h>\n#include <linux/sched/cputime.h>\n#include <linux/sched/deadline.h>\n#include <linux/psi.h>\n#include <net/sock.h>\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/cgroup.h>\n\n#define CGROUP_FILE_NAME_MAX\t\t(MAX_CGROUP_TYPE_NAMELEN +\t\\\n\t\t\t\t\t MAX_CFTYPE_NAME + 2)\n \n#define CGROUP_FILE_NOTIFY_MIN_INTV\tDIV_ROUND_UP(HZ, 100)\n\n \n#define CGROUP_HAS_SUBSYS_CONFIG\t(CGROUP_SUBSYS_COUNT > 0)\n\n \nDEFINE_MUTEX(cgroup_mutex);\nDEFINE_SPINLOCK(css_set_lock);\n\n#ifdef CONFIG_PROVE_RCU\nEXPORT_SYMBOL_GPL(cgroup_mutex);\nEXPORT_SYMBOL_GPL(css_set_lock);\n#endif\n\nDEFINE_SPINLOCK(trace_cgroup_path_lock);\nchar trace_cgroup_path[TRACE_CGROUP_PATH_LEN];\nstatic bool cgroup_debug __read_mostly;\n\n \nstatic DEFINE_SPINLOCK(cgroup_idr_lock);\n\n \nstatic DEFINE_SPINLOCK(cgroup_file_kn_lock);\n\nDEFINE_PERCPU_RWSEM(cgroup_threadgroup_rwsem);\n\n#define cgroup_assert_mutex_or_rcu_locked()\t\t\t\t\\\n\tRCU_LOCKDEP_WARN(!rcu_read_lock_held() &&\t\t\t\\\n\t\t\t   !lockdep_is_held(&cgroup_mutex),\t\t\\\n\t\t\t   \"cgroup_mutex or RCU read lock required\");\n\n \nstatic struct workqueue_struct *cgroup_destroy_wq;\n\n \n#define SUBSYS(_x) [_x ## _cgrp_id] = &_x ## _cgrp_subsys,\nstruct cgroup_subsys *cgroup_subsys[] = {\n#include <linux/cgroup_subsys.h>\n};\n#undef SUBSYS\n\n \n#define SUBSYS(_x) [_x ## _cgrp_id] = #_x,\nstatic const char *cgroup_subsys_name[] = {\n#include <linux/cgroup_subsys.h>\n};\n#undef SUBSYS\n\n \n#define SUBSYS(_x)\t\t\t\t\t\t\t\t\\\n\tDEFINE_STATIC_KEY_TRUE(_x ## _cgrp_subsys_enabled_key);\t\t\t\\\n\tDEFINE_STATIC_KEY_TRUE(_x ## _cgrp_subsys_on_dfl_key);\t\t\t\\\n\tEXPORT_SYMBOL_GPL(_x ## _cgrp_subsys_enabled_key);\t\t\t\\\n\tEXPORT_SYMBOL_GPL(_x ## _cgrp_subsys_on_dfl_key);\n#include <linux/cgroup_subsys.h>\n#undef SUBSYS\n\n#define SUBSYS(_x) [_x ## _cgrp_id] = &_x ## _cgrp_subsys_enabled_key,\nstatic struct static_key_true *cgroup_subsys_enabled_key[] = {\n#include <linux/cgroup_subsys.h>\n};\n#undef SUBSYS\n\n#define SUBSYS(_x) [_x ## _cgrp_id] = &_x ## _cgrp_subsys_on_dfl_key,\nstatic struct static_key_true *cgroup_subsys_on_dfl_key[] = {\n#include <linux/cgroup_subsys.h>\n};\n#undef SUBSYS\n\nstatic DEFINE_PER_CPU(struct cgroup_rstat_cpu, cgrp_dfl_root_rstat_cpu);\n\n \nstruct cgroup_root cgrp_dfl_root = { .cgrp.rstat_cpu = &cgrp_dfl_root_rstat_cpu };\nEXPORT_SYMBOL_GPL(cgrp_dfl_root);\n\n \nstatic bool cgrp_dfl_visible;\n\n \nstatic u16 cgrp_dfl_inhibit_ss_mask;\n\n \nstatic u16 cgrp_dfl_implicit_ss_mask;\n\n \nstatic u16 cgrp_dfl_threaded_ss_mask;\n\n \nLIST_HEAD(cgroup_roots);\nstatic int cgroup_root_count;\n\n \nstatic DEFINE_IDR(cgroup_hierarchy_idr);\n\n \nstatic u64 css_serial_nr_next = 1;\n\n \nstatic u16 have_fork_callback __read_mostly;\nstatic u16 have_exit_callback __read_mostly;\nstatic u16 have_release_callback __read_mostly;\nstatic u16 have_canfork_callback __read_mostly;\n\n \nstruct cgroup_namespace init_cgroup_ns = {\n\t.ns.count\t= REFCOUNT_INIT(2),\n\t.user_ns\t= &init_user_ns,\n\t.ns.ops\t\t= &cgroupns_operations,\n\t.ns.inum\t= PROC_CGROUP_INIT_INO,\n\t.root_cset\t= &init_css_set,\n};\n\nstatic struct file_system_type cgroup2_fs_type;\nstatic struct cftype cgroup_base_files[];\nstatic struct cftype cgroup_psi_files[];\n\n \nenum cgroup_opt_features {\n#ifdef CONFIG_PSI\n\tOPT_FEATURE_PRESSURE,\n#endif\n\tOPT_FEATURE_COUNT\n};\n\nstatic const char *cgroup_opt_feature_names[OPT_FEATURE_COUNT] = {\n#ifdef CONFIG_PSI\n\t\"pressure\",\n#endif\n};\n\nstatic u16 cgroup_feature_disable_mask __read_mostly;\n\nstatic int cgroup_apply_control(struct cgroup *cgrp);\nstatic void cgroup_finalize_control(struct cgroup *cgrp, int ret);\nstatic void css_task_iter_skip(struct css_task_iter *it,\n\t\t\t       struct task_struct *task);\nstatic int cgroup_destroy_locked(struct cgroup *cgrp);\nstatic struct cgroup_subsys_state *css_create(struct cgroup *cgrp,\n\t\t\t\t\t      struct cgroup_subsys *ss);\nstatic void css_release(struct percpu_ref *ref);\nstatic void kill_css(struct cgroup_subsys_state *css);\nstatic int cgroup_addrm_files(struct cgroup_subsys_state *css,\n\t\t\t      struct cgroup *cgrp, struct cftype cfts[],\n\t\t\t      bool is_add);\n\n#ifdef CONFIG_DEBUG_CGROUP_REF\n#define CGROUP_REF_FN_ATTRS\tnoinline\n#define CGROUP_REF_EXPORT(fn)\tEXPORT_SYMBOL_GPL(fn);\n#include <linux/cgroup_refcnt.h>\n#endif\n\n \nbool cgroup_ssid_enabled(int ssid)\n{\n\tif (!CGROUP_HAS_SUBSYS_CONFIG)\n\t\treturn false;\n\n\treturn static_key_enabled(cgroup_subsys_enabled_key[ssid]);\n}\n\n \nbool cgroup_on_dfl(const struct cgroup *cgrp)\n{\n\treturn cgrp->root == &cgrp_dfl_root;\n}\n\n \nstatic int cgroup_idr_alloc(struct idr *idr, void *ptr, int start, int end,\n\t\t\t    gfp_t gfp_mask)\n{\n\tint ret;\n\n\tidr_preload(gfp_mask);\n\tspin_lock_bh(&cgroup_idr_lock);\n\tret = idr_alloc(idr, ptr, start, end, gfp_mask & ~__GFP_DIRECT_RECLAIM);\n\tspin_unlock_bh(&cgroup_idr_lock);\n\tidr_preload_end();\n\treturn ret;\n}\n\nstatic void *cgroup_idr_replace(struct idr *idr, void *ptr, int id)\n{\n\tvoid *ret;\n\n\tspin_lock_bh(&cgroup_idr_lock);\n\tret = idr_replace(idr, ptr, id);\n\tspin_unlock_bh(&cgroup_idr_lock);\n\treturn ret;\n}\n\nstatic void cgroup_idr_remove(struct idr *idr, int id)\n{\n\tspin_lock_bh(&cgroup_idr_lock);\n\tidr_remove(idr, id);\n\tspin_unlock_bh(&cgroup_idr_lock);\n}\n\nstatic bool cgroup_has_tasks(struct cgroup *cgrp)\n{\n\treturn cgrp->nr_populated_csets;\n}\n\nstatic bool cgroup_is_threaded(struct cgroup *cgrp)\n{\n\treturn cgrp->dom_cgrp != cgrp;\n}\n\n \nstatic bool cgroup_is_mixable(struct cgroup *cgrp)\n{\n\t \n\treturn !cgroup_parent(cgrp);\n}\n\n \nstatic bool cgroup_can_be_thread_root(struct cgroup *cgrp)\n{\n\t \n\tif (cgroup_is_mixable(cgrp))\n\t\treturn true;\n\n\t \n\tif (cgroup_is_threaded(cgrp))\n\t\treturn false;\n\n\t \n\tif (cgrp->nr_populated_domain_children)\n\t\treturn false;\n\n\t \n\tif (cgrp->subtree_control & ~cgrp_dfl_threaded_ss_mask)\n\t\treturn false;\n\n\treturn true;\n}\n\n \nstatic bool cgroup_is_thread_root(struct cgroup *cgrp)\n{\n\t \n\tif (cgroup_is_threaded(cgrp))\n\t\treturn false;\n\n\t \n\tif (cgrp->nr_threaded_children)\n\t\treturn true;\n\n\t \n\tif (cgroup_has_tasks(cgrp) &&\n\t    (cgrp->subtree_control & cgrp_dfl_threaded_ss_mask))\n\t\treturn true;\n\n\treturn false;\n}\n\n \nstatic bool cgroup_is_valid_domain(struct cgroup *cgrp)\n{\n\t \n\tif (cgroup_is_threaded(cgrp))\n\t\treturn false;\n\n\t \n\twhile ((cgrp = cgroup_parent(cgrp))) {\n\t\tif (!cgroup_is_mixable(cgrp) && cgroup_is_thread_root(cgrp))\n\t\t\treturn false;\n\t\tif (cgroup_is_threaded(cgrp))\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\n \nstatic u16 cgroup_control(struct cgroup *cgrp)\n{\n\tstruct cgroup *parent = cgroup_parent(cgrp);\n\tu16 root_ss_mask = cgrp->root->subsys_mask;\n\n\tif (parent) {\n\t\tu16 ss_mask = parent->subtree_control;\n\n\t\t \n\t\tif (cgroup_is_threaded(cgrp))\n\t\t\tss_mask &= cgrp_dfl_threaded_ss_mask;\n\t\treturn ss_mask;\n\t}\n\n\tif (cgroup_on_dfl(cgrp))\n\t\troot_ss_mask &= ~(cgrp_dfl_inhibit_ss_mask |\n\t\t\t\t  cgrp_dfl_implicit_ss_mask);\n\treturn root_ss_mask;\n}\n\n \nstatic u16 cgroup_ss_mask(struct cgroup *cgrp)\n{\n\tstruct cgroup *parent = cgroup_parent(cgrp);\n\n\tif (parent) {\n\t\tu16 ss_mask = parent->subtree_ss_mask;\n\n\t\t \n\t\tif (cgroup_is_threaded(cgrp))\n\t\t\tss_mask &= cgrp_dfl_threaded_ss_mask;\n\t\treturn ss_mask;\n\t}\n\n\treturn cgrp->root->subsys_mask;\n}\n\n \nstatic struct cgroup_subsys_state *cgroup_css(struct cgroup *cgrp,\n\t\t\t\t\t      struct cgroup_subsys *ss)\n{\n\tif (CGROUP_HAS_SUBSYS_CONFIG && ss)\n\t\treturn rcu_dereference_check(cgrp->subsys[ss->id],\n\t\t\t\t\tlockdep_is_held(&cgroup_mutex));\n\telse\n\t\treturn &cgrp->self;\n}\n\n \nstatic struct cgroup_subsys_state *cgroup_e_css_by_mask(struct cgroup *cgrp,\n\t\t\t\t\t\t\tstruct cgroup_subsys *ss)\n{\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tif (!ss)\n\t\treturn &cgrp->self;\n\n\t \n\twhile (!(cgroup_ss_mask(cgrp) & (1 << ss->id))) {\n\t\tcgrp = cgroup_parent(cgrp);\n\t\tif (!cgrp)\n\t\t\treturn NULL;\n\t}\n\n\treturn cgroup_css(cgrp, ss);\n}\n\n \nstruct cgroup_subsys_state *cgroup_e_css(struct cgroup *cgrp,\n\t\t\t\t\t struct cgroup_subsys *ss)\n{\n\tstruct cgroup_subsys_state *css;\n\n\tif (!CGROUP_HAS_SUBSYS_CONFIG)\n\t\treturn NULL;\n\n\tdo {\n\t\tcss = cgroup_css(cgrp, ss);\n\n\t\tif (css)\n\t\t\treturn css;\n\t\tcgrp = cgroup_parent(cgrp);\n\t} while (cgrp);\n\n\treturn init_css_set.subsys[ss->id];\n}\n\n \nstruct cgroup_subsys_state *cgroup_get_e_css(struct cgroup *cgrp,\n\t\t\t\t\t     struct cgroup_subsys *ss)\n{\n\tstruct cgroup_subsys_state *css;\n\n\tif (!CGROUP_HAS_SUBSYS_CONFIG)\n\t\treturn NULL;\n\n\trcu_read_lock();\n\n\tdo {\n\t\tcss = cgroup_css(cgrp, ss);\n\n\t\tif (css && css_tryget_online(css))\n\t\t\tgoto out_unlock;\n\t\tcgrp = cgroup_parent(cgrp);\n\t} while (cgrp);\n\n\tcss = init_css_set.subsys[ss->id];\n\tcss_get(css);\nout_unlock:\n\trcu_read_unlock();\n\treturn css;\n}\nEXPORT_SYMBOL_GPL(cgroup_get_e_css);\n\nstatic void cgroup_get_live(struct cgroup *cgrp)\n{\n\tWARN_ON_ONCE(cgroup_is_dead(cgrp));\n\tcgroup_get(cgrp);\n}\n\n \nint __cgroup_task_count(const struct cgroup *cgrp)\n{\n\tint count = 0;\n\tstruct cgrp_cset_link *link;\n\n\tlockdep_assert_held(&css_set_lock);\n\n\tlist_for_each_entry(link, &cgrp->cset_links, cset_link)\n\t\tcount += link->cset->nr_tasks;\n\n\treturn count;\n}\n\n \nint cgroup_task_count(const struct cgroup *cgrp)\n{\n\tint count;\n\n\tspin_lock_irq(&css_set_lock);\n\tcount = __cgroup_task_count(cgrp);\n\tspin_unlock_irq(&css_set_lock);\n\n\treturn count;\n}\n\nstruct cgroup_subsys_state *of_css(struct kernfs_open_file *of)\n{\n\tstruct cgroup *cgrp = of->kn->parent->priv;\n\tstruct cftype *cft = of_cft(of);\n\n\t \n\tif (CGROUP_HAS_SUBSYS_CONFIG && cft->ss)\n\t\treturn rcu_dereference_raw(cgrp->subsys[cft->ss->id]);\n\telse\n\t\treturn &cgrp->self;\n}\nEXPORT_SYMBOL_GPL(of_css);\n\n \n#define for_each_css(css, ssid, cgrp)\t\t\t\t\t\\\n\tfor ((ssid) = 0; (ssid) < CGROUP_SUBSYS_COUNT; (ssid)++)\t\\\n\t\tif (!((css) = rcu_dereference_check(\t\t\t\\\n\t\t\t\t(cgrp)->subsys[(ssid)],\t\t\t\\\n\t\t\t\tlockdep_is_held(&cgroup_mutex)))) { }\t\\\n\t\telse\n\n \n#define do_each_subsys_mask(ss, ssid, ss_mask) do {\t\t\t\\\n\tunsigned long __ss_mask = (ss_mask);\t\t\t\t\\\n\tif (!CGROUP_HAS_SUBSYS_CONFIG) {\t\t\t\t\\\n\t\t(ssid) = 0;\t\t\t\t\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tfor_each_set_bit(ssid, &__ss_mask, CGROUP_SUBSYS_COUNT) {\t\\\n\t\t(ss) = cgroup_subsys[ssid];\t\t\t\t\\\n\t\t{\n\n#define while_each_subsys_mask()\t\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (false)\n\n \n#define cgroup_for_each_live_child(child, cgrp)\t\t\t\t\\\n\tlist_for_each_entry((child), &(cgrp)->self.children, self.sibling) \\\n\t\tif (({ lockdep_assert_held(&cgroup_mutex);\t\t\\\n\t\t       cgroup_is_dead(child); }))\t\t\t\\\n\t\t\t;\t\t\t\t\t\t\\\n\t\telse\n\n \n#define cgroup_for_each_live_descendant_pre(dsct, d_css, cgrp)\t\t\\\n\tcss_for_each_descendant_pre((d_css), cgroup_css((cgrp), NULL))\t\\\n\t\tif (({ lockdep_assert_held(&cgroup_mutex);\t\t\\\n\t\t       (dsct) = (d_css)->cgroup;\t\t\t\\\n\t\t       cgroup_is_dead(dsct); }))\t\t\t\\\n\t\t\t;\t\t\t\t\t\t\\\n\t\telse\n\n \n#define cgroup_for_each_live_descendant_post(dsct, d_css, cgrp)\t\t\\\n\tcss_for_each_descendant_post((d_css), cgroup_css((cgrp), NULL))\t\\\n\t\tif (({ lockdep_assert_held(&cgroup_mutex);\t\t\\\n\t\t       (dsct) = (d_css)->cgroup;\t\t\t\\\n\t\t       cgroup_is_dead(dsct); }))\t\t\t\\\n\t\t\t;\t\t\t\t\t\t\\\n\t\telse\n\n \nstruct css_set init_css_set = {\n\t.refcount\t\t= REFCOUNT_INIT(1),\n\t.dom_cset\t\t= &init_css_set,\n\t.tasks\t\t\t= LIST_HEAD_INIT(init_css_set.tasks),\n\t.mg_tasks\t\t= LIST_HEAD_INIT(init_css_set.mg_tasks),\n\t.dying_tasks\t\t= LIST_HEAD_INIT(init_css_set.dying_tasks),\n\t.task_iters\t\t= LIST_HEAD_INIT(init_css_set.task_iters),\n\t.threaded_csets\t\t= LIST_HEAD_INIT(init_css_set.threaded_csets),\n\t.cgrp_links\t\t= LIST_HEAD_INIT(init_css_set.cgrp_links),\n\t.mg_src_preload_node\t= LIST_HEAD_INIT(init_css_set.mg_src_preload_node),\n\t.mg_dst_preload_node\t= LIST_HEAD_INIT(init_css_set.mg_dst_preload_node),\n\t.mg_node\t\t= LIST_HEAD_INIT(init_css_set.mg_node),\n\n\t \n\t.dfl_cgrp\t\t= &cgrp_dfl_root.cgrp,\n};\n\nstatic int css_set_count\t= 1;\t \n\nstatic bool css_set_threaded(struct css_set *cset)\n{\n\treturn cset->dom_cset != cset;\n}\n\n \nstatic bool css_set_populated(struct css_set *cset)\n{\n\tlockdep_assert_held(&css_set_lock);\n\n\treturn !list_empty(&cset->tasks) || !list_empty(&cset->mg_tasks);\n}\n\n \nstatic void cgroup_update_populated(struct cgroup *cgrp, bool populated)\n{\n\tstruct cgroup *child = NULL;\n\tint adj = populated ? 1 : -1;\n\n\tlockdep_assert_held(&css_set_lock);\n\n\tdo {\n\t\tbool was_populated = cgroup_is_populated(cgrp);\n\n\t\tif (!child) {\n\t\t\tcgrp->nr_populated_csets += adj;\n\t\t} else {\n\t\t\tif (cgroup_is_threaded(child))\n\t\t\t\tcgrp->nr_populated_threaded_children += adj;\n\t\t\telse\n\t\t\t\tcgrp->nr_populated_domain_children += adj;\n\t\t}\n\n\t\tif (was_populated == cgroup_is_populated(cgrp))\n\t\t\tbreak;\n\n\t\tcgroup1_check_for_release(cgrp);\n\t\tTRACE_CGROUP_PATH(notify_populated, cgrp,\n\t\t\t\t  cgroup_is_populated(cgrp));\n\t\tcgroup_file_notify(&cgrp->events_file);\n\n\t\tchild = cgrp;\n\t\tcgrp = cgroup_parent(cgrp);\n\t} while (cgrp);\n}\n\n \nstatic void css_set_update_populated(struct css_set *cset, bool populated)\n{\n\tstruct cgrp_cset_link *link;\n\n\tlockdep_assert_held(&css_set_lock);\n\n\tlist_for_each_entry(link, &cset->cgrp_links, cgrp_link)\n\t\tcgroup_update_populated(link->cgrp, populated);\n}\n\n \nstatic void css_set_skip_task_iters(struct css_set *cset,\n\t\t\t\t    struct task_struct *task)\n{\n\tstruct css_task_iter *it, *pos;\n\n\tlist_for_each_entry_safe(it, pos, &cset->task_iters, iters_node)\n\t\tcss_task_iter_skip(it, task);\n}\n\n \nstatic void css_set_move_task(struct task_struct *task,\n\t\t\t      struct css_set *from_cset, struct css_set *to_cset,\n\t\t\t      bool use_mg_tasks)\n{\n\tlockdep_assert_held(&css_set_lock);\n\n\tif (to_cset && !css_set_populated(to_cset))\n\t\tcss_set_update_populated(to_cset, true);\n\n\tif (from_cset) {\n\t\tWARN_ON_ONCE(list_empty(&task->cg_list));\n\n\t\tcss_set_skip_task_iters(from_cset, task);\n\t\tlist_del_init(&task->cg_list);\n\t\tif (!css_set_populated(from_cset))\n\t\t\tcss_set_update_populated(from_cset, false);\n\t} else {\n\t\tWARN_ON_ONCE(!list_empty(&task->cg_list));\n\t}\n\n\tif (to_cset) {\n\t\t \n\t\tWARN_ON_ONCE(task->flags & PF_EXITING);\n\n\t\tcgroup_move_task(task, to_cset);\n\t\tlist_add_tail(&task->cg_list, use_mg_tasks ? &to_cset->mg_tasks :\n\t\t\t\t\t\t\t     &to_cset->tasks);\n\t}\n}\n\n \n#define CSS_SET_HASH_BITS\t7\nstatic DEFINE_HASHTABLE(css_set_table, CSS_SET_HASH_BITS);\n\nstatic unsigned long css_set_hash(struct cgroup_subsys_state **css)\n{\n\tunsigned long key = 0UL;\n\tstruct cgroup_subsys *ss;\n\tint i;\n\n\tfor_each_subsys(ss, i)\n\t\tkey += (unsigned long)css[i];\n\tkey = (key >> 16) ^ key;\n\n\treturn key;\n}\n\nvoid put_css_set_locked(struct css_set *cset)\n{\n\tstruct cgrp_cset_link *link, *tmp_link;\n\tstruct cgroup_subsys *ss;\n\tint ssid;\n\n\tlockdep_assert_held(&css_set_lock);\n\n\tif (!refcount_dec_and_test(&cset->refcount))\n\t\treturn;\n\n\tWARN_ON_ONCE(!list_empty(&cset->threaded_csets));\n\n\t \n\tfor_each_subsys(ss, ssid) {\n\t\tlist_del(&cset->e_cset_node[ssid]);\n\t\tcss_put(cset->subsys[ssid]);\n\t}\n\thash_del(&cset->hlist);\n\tcss_set_count--;\n\n\tlist_for_each_entry_safe(link, tmp_link, &cset->cgrp_links, cgrp_link) {\n\t\tlist_del(&link->cset_link);\n\t\tlist_del(&link->cgrp_link);\n\t\tif (cgroup_parent(link->cgrp))\n\t\t\tcgroup_put(link->cgrp);\n\t\tkfree(link);\n\t}\n\n\tif (css_set_threaded(cset)) {\n\t\tlist_del(&cset->threaded_csets_node);\n\t\tput_css_set_locked(cset->dom_cset);\n\t}\n\n\tkfree_rcu(cset, rcu_head);\n}\n\n \nstatic bool compare_css_sets(struct css_set *cset,\n\t\t\t     struct css_set *old_cset,\n\t\t\t     struct cgroup *new_cgrp,\n\t\t\t     struct cgroup_subsys_state *template[])\n{\n\tstruct cgroup *new_dfl_cgrp;\n\tstruct list_head *l1, *l2;\n\n\t \n\tif (memcmp(template, cset->subsys, sizeof(cset->subsys)))\n\t\treturn false;\n\n\n\t \n\tif (cgroup_on_dfl(new_cgrp))\n\t\tnew_dfl_cgrp = new_cgrp;\n\telse\n\t\tnew_dfl_cgrp = old_cset->dfl_cgrp;\n\n\tif (new_dfl_cgrp->dom_cgrp != cset->dom_cset->dfl_cgrp)\n\t\treturn false;\n\n\t \n\tl1 = &cset->cgrp_links;\n\tl2 = &old_cset->cgrp_links;\n\twhile (1) {\n\t\tstruct cgrp_cset_link *link1, *link2;\n\t\tstruct cgroup *cgrp1, *cgrp2;\n\n\t\tl1 = l1->next;\n\t\tl2 = l2->next;\n\t\t \n\t\tif (l1 == &cset->cgrp_links) {\n\t\t\tBUG_ON(l2 != &old_cset->cgrp_links);\n\t\t\tbreak;\n\t\t} else {\n\t\t\tBUG_ON(l2 == &old_cset->cgrp_links);\n\t\t}\n\t\t \n\t\tlink1 = list_entry(l1, struct cgrp_cset_link, cgrp_link);\n\t\tlink2 = list_entry(l2, struct cgrp_cset_link, cgrp_link);\n\t\tcgrp1 = link1->cgrp;\n\t\tcgrp2 = link2->cgrp;\n\t\t \n\t\tBUG_ON(cgrp1->root != cgrp2->root);\n\n\t\t \n\t\tif (cgrp1->root == new_cgrp->root) {\n\t\t\tif (cgrp1 != new_cgrp)\n\t\t\t\treturn false;\n\t\t} else {\n\t\t\tif (cgrp1 != cgrp2)\n\t\t\t\treturn false;\n\t\t}\n\t}\n\treturn true;\n}\n\n \nstatic struct css_set *find_existing_css_set(struct css_set *old_cset,\n\t\t\t\t\tstruct cgroup *cgrp,\n\t\t\t\t\tstruct cgroup_subsys_state **template)\n{\n\tstruct cgroup_root *root = cgrp->root;\n\tstruct cgroup_subsys *ss;\n\tstruct css_set *cset;\n\tunsigned long key;\n\tint i;\n\n\t \n\tfor_each_subsys(ss, i) {\n\t\tif (root->subsys_mask & (1UL << i)) {\n\t\t\t \n\t\t\ttemplate[i] = cgroup_e_css_by_mask(cgrp, ss);\n\t\t} else {\n\t\t\t \n\t\t\ttemplate[i] = old_cset->subsys[i];\n\t\t}\n\t}\n\n\tkey = css_set_hash(template);\n\thash_for_each_possible(css_set_table, cset, hlist, key) {\n\t\tif (!compare_css_sets(cset, old_cset, cgrp, template))\n\t\t\tcontinue;\n\n\t\t \n\t\treturn cset;\n\t}\n\n\t \n\treturn NULL;\n}\n\nstatic void free_cgrp_cset_links(struct list_head *links_to_free)\n{\n\tstruct cgrp_cset_link *link, *tmp_link;\n\n\tlist_for_each_entry_safe(link, tmp_link, links_to_free, cset_link) {\n\t\tlist_del(&link->cset_link);\n\t\tkfree(link);\n\t}\n}\n\n \nstatic int allocate_cgrp_cset_links(int count, struct list_head *tmp_links)\n{\n\tstruct cgrp_cset_link *link;\n\tint i;\n\n\tINIT_LIST_HEAD(tmp_links);\n\n\tfor (i = 0; i < count; i++) {\n\t\tlink = kzalloc(sizeof(*link), GFP_KERNEL);\n\t\tif (!link) {\n\t\t\tfree_cgrp_cset_links(tmp_links);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tlist_add(&link->cset_link, tmp_links);\n\t}\n\treturn 0;\n}\n\n \nstatic void link_css_set(struct list_head *tmp_links, struct css_set *cset,\n\t\t\t struct cgroup *cgrp)\n{\n\tstruct cgrp_cset_link *link;\n\n\tBUG_ON(list_empty(tmp_links));\n\n\tif (cgroup_on_dfl(cgrp))\n\t\tcset->dfl_cgrp = cgrp;\n\n\tlink = list_first_entry(tmp_links, struct cgrp_cset_link, cset_link);\n\tlink->cset = cset;\n\tlink->cgrp = cgrp;\n\n\t \n\tlist_move_tail(&link->cset_link, &cgrp->cset_links);\n\tlist_add_tail(&link->cgrp_link, &cset->cgrp_links);\n\n\tif (cgroup_parent(cgrp))\n\t\tcgroup_get_live(cgrp);\n}\n\n \nstatic struct css_set *find_css_set(struct css_set *old_cset,\n\t\t\t\t    struct cgroup *cgrp)\n{\n\tstruct cgroup_subsys_state *template[CGROUP_SUBSYS_COUNT] = { };\n\tstruct css_set *cset;\n\tstruct list_head tmp_links;\n\tstruct cgrp_cset_link *link;\n\tstruct cgroup_subsys *ss;\n\tunsigned long key;\n\tint ssid;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\t \n\tspin_lock_irq(&css_set_lock);\n\tcset = find_existing_css_set(old_cset, cgrp, template);\n\tif (cset)\n\t\tget_css_set(cset);\n\tspin_unlock_irq(&css_set_lock);\n\n\tif (cset)\n\t\treturn cset;\n\n\tcset = kzalloc(sizeof(*cset), GFP_KERNEL);\n\tif (!cset)\n\t\treturn NULL;\n\n\t \n\tif (allocate_cgrp_cset_links(cgroup_root_count, &tmp_links) < 0) {\n\t\tkfree(cset);\n\t\treturn NULL;\n\t}\n\n\trefcount_set(&cset->refcount, 1);\n\tcset->dom_cset = cset;\n\tINIT_LIST_HEAD(&cset->tasks);\n\tINIT_LIST_HEAD(&cset->mg_tasks);\n\tINIT_LIST_HEAD(&cset->dying_tasks);\n\tINIT_LIST_HEAD(&cset->task_iters);\n\tINIT_LIST_HEAD(&cset->threaded_csets);\n\tINIT_HLIST_NODE(&cset->hlist);\n\tINIT_LIST_HEAD(&cset->cgrp_links);\n\tINIT_LIST_HEAD(&cset->mg_src_preload_node);\n\tINIT_LIST_HEAD(&cset->mg_dst_preload_node);\n\tINIT_LIST_HEAD(&cset->mg_node);\n\n\t \n\tmemcpy(cset->subsys, template, sizeof(cset->subsys));\n\n\tspin_lock_irq(&css_set_lock);\n\t \n\tlist_for_each_entry(link, &old_cset->cgrp_links, cgrp_link) {\n\t\tstruct cgroup *c = link->cgrp;\n\n\t\tif (c->root == cgrp->root)\n\t\t\tc = cgrp;\n\t\tlink_css_set(&tmp_links, cset, c);\n\t}\n\n\tBUG_ON(!list_empty(&tmp_links));\n\n\tcss_set_count++;\n\n\t \n\tkey = css_set_hash(cset->subsys);\n\thash_add(css_set_table, &cset->hlist, key);\n\n\tfor_each_subsys(ss, ssid) {\n\t\tstruct cgroup_subsys_state *css = cset->subsys[ssid];\n\n\t\tlist_add_tail(&cset->e_cset_node[ssid],\n\t\t\t      &css->cgroup->e_csets[ssid]);\n\t\tcss_get(css);\n\t}\n\n\tspin_unlock_irq(&css_set_lock);\n\n\t \n\tif (cgroup_is_threaded(cset->dfl_cgrp)) {\n\t\tstruct css_set *dcset;\n\n\t\tdcset = find_css_set(cset, cset->dfl_cgrp->dom_cgrp);\n\t\tif (!dcset) {\n\t\t\tput_css_set(cset);\n\t\t\treturn NULL;\n\t\t}\n\n\t\tspin_lock_irq(&css_set_lock);\n\t\tcset->dom_cset = dcset;\n\t\tlist_add_tail(&cset->threaded_csets_node,\n\t\t\t      &dcset->threaded_csets);\n\t\tspin_unlock_irq(&css_set_lock);\n\t}\n\n\treturn cset;\n}\n\nstruct cgroup_root *cgroup_root_from_kf(struct kernfs_root *kf_root)\n{\n\tstruct cgroup *root_cgrp = kernfs_root_to_node(kf_root)->priv;\n\n\treturn root_cgrp->root;\n}\n\nvoid cgroup_favor_dynmods(struct cgroup_root *root, bool favor)\n{\n\tbool favoring = root->flags & CGRP_ROOT_FAVOR_DYNMODS;\n\n\t \n\tif (favor && !favoring) {\n\t\trcu_sync_enter(&cgroup_threadgroup_rwsem.rss);\n\t\troot->flags |= CGRP_ROOT_FAVOR_DYNMODS;\n\t} else if (!favor && favoring) {\n\t\trcu_sync_exit(&cgroup_threadgroup_rwsem.rss);\n\t\troot->flags &= ~CGRP_ROOT_FAVOR_DYNMODS;\n\t}\n}\n\nstatic int cgroup_init_root_id(struct cgroup_root *root)\n{\n\tint id;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tid = idr_alloc_cyclic(&cgroup_hierarchy_idr, root, 0, 0, GFP_KERNEL);\n\tif (id < 0)\n\t\treturn id;\n\n\troot->hierarchy_id = id;\n\treturn 0;\n}\n\nstatic void cgroup_exit_root_id(struct cgroup_root *root)\n{\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tidr_remove(&cgroup_hierarchy_idr, root->hierarchy_id);\n}\n\nvoid cgroup_free_root(struct cgroup_root *root)\n{\n\tkfree(root);\n}\n\nstatic void cgroup_destroy_root(struct cgroup_root *root)\n{\n\tstruct cgroup *cgrp = &root->cgrp;\n\tstruct cgrp_cset_link *link, *tmp_link;\n\n\ttrace_cgroup_destroy_root(root);\n\n\tcgroup_lock_and_drain_offline(&cgrp_dfl_root.cgrp);\n\n\tBUG_ON(atomic_read(&root->nr_cgrps));\n\tBUG_ON(!list_empty(&cgrp->self.children));\n\n\t \n\tWARN_ON(rebind_subsystems(&cgrp_dfl_root, root->subsys_mask));\n\n\t \n\tspin_lock_irq(&css_set_lock);\n\n\tlist_for_each_entry_safe(link, tmp_link, &cgrp->cset_links, cset_link) {\n\t\tlist_del(&link->cset_link);\n\t\tlist_del(&link->cgrp_link);\n\t\tkfree(link);\n\t}\n\n\tspin_unlock_irq(&css_set_lock);\n\n\tif (!list_empty(&root->root_list)) {\n\t\tlist_del(&root->root_list);\n\t\tcgroup_root_count--;\n\t}\n\n\tcgroup_favor_dynmods(root, false);\n\tcgroup_exit_root_id(root);\n\n\tcgroup_unlock();\n\n\tcgroup_rstat_exit(cgrp);\n\tkernfs_destroy_root(root->kf_root);\n\tcgroup_free_root(root);\n}\n\n \nstatic inline struct cgroup *__cset_cgroup_from_root(struct css_set *cset,\n\t\t\t\t\t    struct cgroup_root *root)\n{\n\tstruct cgroup *res_cgroup = NULL;\n\n\tif (cset == &init_css_set) {\n\t\tres_cgroup = &root->cgrp;\n\t} else if (root == &cgrp_dfl_root) {\n\t\tres_cgroup = cset->dfl_cgrp;\n\t} else {\n\t\tstruct cgrp_cset_link *link;\n\t\tlockdep_assert_held(&css_set_lock);\n\n\t\tlist_for_each_entry(link, &cset->cgrp_links, cgrp_link) {\n\t\t\tstruct cgroup *c = link->cgrp;\n\n\t\t\tif (c->root == root) {\n\t\t\t\tres_cgroup = c;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tBUG_ON(!res_cgroup);\n\treturn res_cgroup;\n}\n\n \nstatic struct cgroup *\ncurrent_cgns_cgroup_from_root(struct cgroup_root *root)\n{\n\tstruct cgroup *res = NULL;\n\tstruct css_set *cset;\n\n\tlockdep_assert_held(&css_set_lock);\n\n\trcu_read_lock();\n\n\tcset = current->nsproxy->cgroup_ns->root_cset;\n\tres = __cset_cgroup_from_root(cset, root);\n\n\trcu_read_unlock();\n\n\treturn res;\n}\n\n \nstatic struct cgroup *current_cgns_cgroup_dfl(void)\n{\n\tstruct css_set *cset;\n\n\tif (current->nsproxy) {\n\t\tcset = current->nsproxy->cgroup_ns->root_cset;\n\t\treturn __cset_cgroup_from_root(cset, &cgrp_dfl_root);\n\t} else {\n\t\t \n\t\treturn &cgrp_dfl_root.cgrp;\n\t}\n}\n\n \nstatic struct cgroup *cset_cgroup_from_root(struct css_set *cset,\n\t\t\t\t\t    struct cgroup_root *root)\n{\n\tlockdep_assert_held(&cgroup_mutex);\n\tlockdep_assert_held(&css_set_lock);\n\n\treturn __cset_cgroup_from_root(cset, root);\n}\n\n \nstruct cgroup *task_cgroup_from_root(struct task_struct *task,\n\t\t\t\t     struct cgroup_root *root)\n{\n\t \n\treturn cset_cgroup_from_root(task_css_set(task), root);\n}\n\n \n\nstatic struct kernfs_syscall_ops cgroup_kf_syscall_ops;\n\nstatic char *cgroup_file_name(struct cgroup *cgrp, const struct cftype *cft,\n\t\t\t      char *buf)\n{\n\tstruct cgroup_subsys *ss = cft->ss;\n\n\tif (cft->ss && !(cft->flags & CFTYPE_NO_PREFIX) &&\n\t    !(cgrp->root->flags & CGRP_ROOT_NOPREFIX)) {\n\t\tconst char *dbg = (cft->flags & CFTYPE_DEBUG) ? \".__DEBUG__.\" : \"\";\n\n\t\tsnprintf(buf, CGROUP_FILE_NAME_MAX, \"%s%s.%s\",\n\t\t\t dbg, cgroup_on_dfl(cgrp) ? ss->name : ss->legacy_name,\n\t\t\t cft->name);\n\t} else {\n\t\tstrscpy(buf, cft->name, CGROUP_FILE_NAME_MAX);\n\t}\n\treturn buf;\n}\n\n \nstatic umode_t cgroup_file_mode(const struct cftype *cft)\n{\n\tumode_t mode = 0;\n\n\tif (cft->read_u64 || cft->read_s64 || cft->seq_show)\n\t\tmode |= S_IRUGO;\n\n\tif (cft->write_u64 || cft->write_s64 || cft->write) {\n\t\tif (cft->flags & CFTYPE_WORLD_WRITABLE)\n\t\t\tmode |= S_IWUGO;\n\t\telse\n\t\t\tmode |= S_IWUSR;\n\t}\n\n\treturn mode;\n}\n\n \nstatic u16 cgroup_calc_subtree_ss_mask(u16 subtree_control, u16 this_ss_mask)\n{\n\tu16 cur_ss_mask = subtree_control;\n\tstruct cgroup_subsys *ss;\n\tint ssid;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tcur_ss_mask |= cgrp_dfl_implicit_ss_mask;\n\n\twhile (true) {\n\t\tu16 new_ss_mask = cur_ss_mask;\n\n\t\tdo_each_subsys_mask(ss, ssid, cur_ss_mask) {\n\t\t\tnew_ss_mask |= ss->depends_on;\n\t\t} while_each_subsys_mask();\n\n\t\t \n\t\tnew_ss_mask &= this_ss_mask;\n\n\t\tif (new_ss_mask == cur_ss_mask)\n\t\t\tbreak;\n\t\tcur_ss_mask = new_ss_mask;\n\t}\n\n\treturn cur_ss_mask;\n}\n\n \nvoid cgroup_kn_unlock(struct kernfs_node *kn)\n{\n\tstruct cgroup *cgrp;\n\n\tif (kernfs_type(kn) == KERNFS_DIR)\n\t\tcgrp = kn->priv;\n\telse\n\t\tcgrp = kn->parent->priv;\n\n\tcgroup_unlock();\n\n\tkernfs_unbreak_active_protection(kn);\n\tcgroup_put(cgrp);\n}\n\n \nstruct cgroup *cgroup_kn_lock_live(struct kernfs_node *kn, bool drain_offline)\n{\n\tstruct cgroup *cgrp;\n\n\tif (kernfs_type(kn) == KERNFS_DIR)\n\t\tcgrp = kn->priv;\n\telse\n\t\tcgrp = kn->parent->priv;\n\n\t \n\tif (!cgroup_tryget(cgrp))\n\t\treturn NULL;\n\tkernfs_break_active_protection(kn);\n\n\tif (drain_offline)\n\t\tcgroup_lock_and_drain_offline(cgrp);\n\telse\n\t\tcgroup_lock();\n\n\tif (!cgroup_is_dead(cgrp))\n\t\treturn cgrp;\n\n\tcgroup_kn_unlock(kn);\n\treturn NULL;\n}\n\nstatic void cgroup_rm_file(struct cgroup *cgrp, const struct cftype *cft)\n{\n\tchar name[CGROUP_FILE_NAME_MAX];\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tif (cft->file_offset) {\n\t\tstruct cgroup_subsys_state *css = cgroup_css(cgrp, cft->ss);\n\t\tstruct cgroup_file *cfile = (void *)css + cft->file_offset;\n\n\t\tspin_lock_irq(&cgroup_file_kn_lock);\n\t\tcfile->kn = NULL;\n\t\tspin_unlock_irq(&cgroup_file_kn_lock);\n\n\t\tdel_timer_sync(&cfile->notify_timer);\n\t}\n\n\tkernfs_remove_by_name(cgrp->kn, cgroup_file_name(cgrp, cft, name));\n}\n\n \nstatic void css_clear_dir(struct cgroup_subsys_state *css)\n{\n\tstruct cgroup *cgrp = css->cgroup;\n\tstruct cftype *cfts;\n\n\tif (!(css->flags & CSS_VISIBLE))\n\t\treturn;\n\n\tcss->flags &= ~CSS_VISIBLE;\n\n\tif (!css->ss) {\n\t\tif (cgroup_on_dfl(cgrp)) {\n\t\t\tcgroup_addrm_files(css, cgrp,\n\t\t\t\t\t   cgroup_base_files, false);\n\t\t\tif (cgroup_psi_enabled())\n\t\t\t\tcgroup_addrm_files(css, cgrp,\n\t\t\t\t\t\t   cgroup_psi_files, false);\n\t\t} else {\n\t\t\tcgroup_addrm_files(css, cgrp,\n\t\t\t\t\t   cgroup1_base_files, false);\n\t\t}\n\t} else {\n\t\tlist_for_each_entry(cfts, &css->ss->cfts, node)\n\t\t\tcgroup_addrm_files(css, cgrp, cfts, false);\n\t}\n}\n\n \nstatic int css_populate_dir(struct cgroup_subsys_state *css)\n{\n\tstruct cgroup *cgrp = css->cgroup;\n\tstruct cftype *cfts, *failed_cfts;\n\tint ret;\n\n\tif (css->flags & CSS_VISIBLE)\n\t\treturn 0;\n\n\tif (!css->ss) {\n\t\tif (cgroup_on_dfl(cgrp)) {\n\t\t\tret = cgroup_addrm_files(&cgrp->self, cgrp,\n\t\t\t\t\t\t cgroup_base_files, true);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\n\t\t\tif (cgroup_psi_enabled()) {\n\t\t\t\tret = cgroup_addrm_files(&cgrp->self, cgrp,\n\t\t\t\t\t\t\t cgroup_psi_files, true);\n\t\t\t\tif (ret < 0)\n\t\t\t\t\treturn ret;\n\t\t\t}\n\t\t} else {\n\t\t\tcgroup_addrm_files(css, cgrp,\n\t\t\t\t\t   cgroup1_base_files, true);\n\t\t}\n\t} else {\n\t\tlist_for_each_entry(cfts, &css->ss->cfts, node) {\n\t\t\tret = cgroup_addrm_files(css, cgrp, cfts, true);\n\t\t\tif (ret < 0) {\n\t\t\t\tfailed_cfts = cfts;\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t}\n\t}\n\n\tcss->flags |= CSS_VISIBLE;\n\n\treturn 0;\nerr:\n\tlist_for_each_entry(cfts, &css->ss->cfts, node) {\n\t\tif (cfts == failed_cfts)\n\t\t\tbreak;\n\t\tcgroup_addrm_files(css, cgrp, cfts, false);\n\t}\n\treturn ret;\n}\n\nint rebind_subsystems(struct cgroup_root *dst_root, u16 ss_mask)\n{\n\tstruct cgroup *dcgrp = &dst_root->cgrp;\n\tstruct cgroup_subsys *ss;\n\tint ssid, ret;\n\tu16 dfl_disable_ss_mask = 0;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tdo_each_subsys_mask(ss, ssid, ss_mask) {\n\t\t \n\t\tif (css_next_child(NULL, cgroup_css(&ss->root->cgrp, ss)) &&\n\t\t    !ss->implicit_on_dfl)\n\t\t\treturn -EBUSY;\n\n\t\t \n\t\tif (ss->root != &cgrp_dfl_root && dst_root != &cgrp_dfl_root)\n\t\t\treturn -EBUSY;\n\n\t\t \n\t\tif (ss->root == &cgrp_dfl_root)\n\t\t\tdfl_disable_ss_mask |= 1 << ssid;\n\n\t} while_each_subsys_mask();\n\n\tif (dfl_disable_ss_mask) {\n\t\tstruct cgroup *scgrp = &cgrp_dfl_root.cgrp;\n\n\t\t \n\t\tcgrp_dfl_root.subsys_mask &= ~dfl_disable_ss_mask;\n\t\tWARN_ON(cgroup_apply_control(scgrp));\n\t\tcgroup_finalize_control(scgrp, 0);\n\t}\n\n\tdo_each_subsys_mask(ss, ssid, ss_mask) {\n\t\tstruct cgroup_root *src_root = ss->root;\n\t\tstruct cgroup *scgrp = &src_root->cgrp;\n\t\tstruct cgroup_subsys_state *css = cgroup_css(scgrp, ss);\n\t\tstruct css_set *cset, *cset_pos;\n\t\tstruct css_task_iter *it;\n\n\t\tWARN_ON(!css || cgroup_css(dcgrp, ss));\n\n\t\tif (src_root != &cgrp_dfl_root) {\n\t\t\t \n\t\t\tsrc_root->subsys_mask &= ~(1 << ssid);\n\t\t\tWARN_ON(cgroup_apply_control(scgrp));\n\t\t\tcgroup_finalize_control(scgrp, 0);\n\t\t}\n\n\t\t \n\t\tRCU_INIT_POINTER(scgrp->subsys[ssid], NULL);\n\t\trcu_assign_pointer(dcgrp->subsys[ssid], css);\n\t\tss->root = dst_root;\n\t\tcss->cgroup = dcgrp;\n\n\t\tspin_lock_irq(&css_set_lock);\n\t\tWARN_ON(!list_empty(&dcgrp->e_csets[ss->id]));\n\t\tlist_for_each_entry_safe(cset, cset_pos, &scgrp->e_csets[ss->id],\n\t\t\t\t\t e_cset_node[ss->id]) {\n\t\t\tlist_move_tail(&cset->e_cset_node[ss->id],\n\t\t\t\t       &dcgrp->e_csets[ss->id]);\n\t\t\t \n\t\t\tlist_for_each_entry(it, &cset->task_iters, iters_node)\n\t\t\t\tif (it->cset_head == &scgrp->e_csets[ss->id])\n\t\t\t\t\tit->cset_head = &dcgrp->e_csets[ss->id];\n\t\t}\n\t\tspin_unlock_irq(&css_set_lock);\n\n\t\tif (ss->css_rstat_flush) {\n\t\t\tlist_del_rcu(&css->rstat_css_node);\n\t\t\tsynchronize_rcu();\n\t\t\tlist_add_rcu(&css->rstat_css_node,\n\t\t\t\t     &dcgrp->rstat_css_list);\n\t\t}\n\n\t\t \n\t\tdst_root->subsys_mask |= 1 << ssid;\n\t\tif (dst_root == &cgrp_dfl_root) {\n\t\t\tstatic_branch_enable(cgroup_subsys_on_dfl_key[ssid]);\n\t\t} else {\n\t\t\tdcgrp->subtree_control |= 1 << ssid;\n\t\t\tstatic_branch_disable(cgroup_subsys_on_dfl_key[ssid]);\n\t\t}\n\n\t\tret = cgroup_apply_control(dcgrp);\n\t\tif (ret)\n\t\t\tpr_warn(\"partial failure to rebind %s controller (err=%d)\\n\",\n\t\t\t\tss->name, ret);\n\n\t\tif (ss->bind)\n\t\t\tss->bind(css);\n\t} while_each_subsys_mask();\n\n\tkernfs_activate(dcgrp->kn);\n\treturn 0;\n}\n\nint cgroup_show_path(struct seq_file *sf, struct kernfs_node *kf_node,\n\t\t     struct kernfs_root *kf_root)\n{\n\tint len = 0;\n\tchar *buf = NULL;\n\tstruct cgroup_root *kf_cgroot = cgroup_root_from_kf(kf_root);\n\tstruct cgroup *ns_cgroup;\n\n\tbuf = kmalloc(PATH_MAX, GFP_KERNEL);\n\tif (!buf)\n\t\treturn -ENOMEM;\n\n\tspin_lock_irq(&css_set_lock);\n\tns_cgroup = current_cgns_cgroup_from_root(kf_cgroot);\n\tlen = kernfs_path_from_node(kf_node, ns_cgroup->kn, buf, PATH_MAX);\n\tspin_unlock_irq(&css_set_lock);\n\n\tif (len >= PATH_MAX)\n\t\tlen = -ERANGE;\n\telse if (len > 0) {\n\t\tseq_escape(sf, buf, \" \\t\\n\\\\\");\n\t\tlen = 0;\n\t}\n\tkfree(buf);\n\treturn len;\n}\n\nenum cgroup2_param {\n\tOpt_nsdelegate,\n\tOpt_favordynmods,\n\tOpt_memory_localevents,\n\tOpt_memory_recursiveprot,\n\tnr__cgroup2_params\n};\n\nstatic const struct fs_parameter_spec cgroup2_fs_parameters[] = {\n\tfsparam_flag(\"nsdelegate\",\t\tOpt_nsdelegate),\n\tfsparam_flag(\"favordynmods\",\t\tOpt_favordynmods),\n\tfsparam_flag(\"memory_localevents\",\tOpt_memory_localevents),\n\tfsparam_flag(\"memory_recursiveprot\",\tOpt_memory_recursiveprot),\n\t{}\n};\n\nstatic int cgroup2_parse_param(struct fs_context *fc, struct fs_parameter *param)\n{\n\tstruct cgroup_fs_context *ctx = cgroup_fc2context(fc);\n\tstruct fs_parse_result result;\n\tint opt;\n\n\topt = fs_parse(fc, cgroup2_fs_parameters, param, &result);\n\tif (opt < 0)\n\t\treturn opt;\n\n\tswitch (opt) {\n\tcase Opt_nsdelegate:\n\t\tctx->flags |= CGRP_ROOT_NS_DELEGATE;\n\t\treturn 0;\n\tcase Opt_favordynmods:\n\t\tctx->flags |= CGRP_ROOT_FAVOR_DYNMODS;\n\t\treturn 0;\n\tcase Opt_memory_localevents:\n\t\tctx->flags |= CGRP_ROOT_MEMORY_LOCAL_EVENTS;\n\t\treturn 0;\n\tcase Opt_memory_recursiveprot:\n\t\tctx->flags |= CGRP_ROOT_MEMORY_RECURSIVE_PROT;\n\t\treturn 0;\n\t}\n\treturn -EINVAL;\n}\n\nstatic void apply_cgroup_root_flags(unsigned int root_flags)\n{\n\tif (current->nsproxy->cgroup_ns == &init_cgroup_ns) {\n\t\tif (root_flags & CGRP_ROOT_NS_DELEGATE)\n\t\t\tcgrp_dfl_root.flags |= CGRP_ROOT_NS_DELEGATE;\n\t\telse\n\t\t\tcgrp_dfl_root.flags &= ~CGRP_ROOT_NS_DELEGATE;\n\n\t\tcgroup_favor_dynmods(&cgrp_dfl_root,\n\t\t\t\t     root_flags & CGRP_ROOT_FAVOR_DYNMODS);\n\n\t\tif (root_flags & CGRP_ROOT_MEMORY_LOCAL_EVENTS)\n\t\t\tcgrp_dfl_root.flags |= CGRP_ROOT_MEMORY_LOCAL_EVENTS;\n\t\telse\n\t\t\tcgrp_dfl_root.flags &= ~CGRP_ROOT_MEMORY_LOCAL_EVENTS;\n\n\t\tif (root_flags & CGRP_ROOT_MEMORY_RECURSIVE_PROT)\n\t\t\tcgrp_dfl_root.flags |= CGRP_ROOT_MEMORY_RECURSIVE_PROT;\n\t\telse\n\t\t\tcgrp_dfl_root.flags &= ~CGRP_ROOT_MEMORY_RECURSIVE_PROT;\n\t}\n}\n\nstatic int cgroup_show_options(struct seq_file *seq, struct kernfs_root *kf_root)\n{\n\tif (cgrp_dfl_root.flags & CGRP_ROOT_NS_DELEGATE)\n\t\tseq_puts(seq, \",nsdelegate\");\n\tif (cgrp_dfl_root.flags & CGRP_ROOT_FAVOR_DYNMODS)\n\t\tseq_puts(seq, \",favordynmods\");\n\tif (cgrp_dfl_root.flags & CGRP_ROOT_MEMORY_LOCAL_EVENTS)\n\t\tseq_puts(seq, \",memory_localevents\");\n\tif (cgrp_dfl_root.flags & CGRP_ROOT_MEMORY_RECURSIVE_PROT)\n\t\tseq_puts(seq, \",memory_recursiveprot\");\n\treturn 0;\n}\n\nstatic int cgroup_reconfigure(struct fs_context *fc)\n{\n\tstruct cgroup_fs_context *ctx = cgroup_fc2context(fc);\n\n\tapply_cgroup_root_flags(ctx->flags);\n\treturn 0;\n}\n\nstatic void init_cgroup_housekeeping(struct cgroup *cgrp)\n{\n\tstruct cgroup_subsys *ss;\n\tint ssid;\n\n\tINIT_LIST_HEAD(&cgrp->self.sibling);\n\tINIT_LIST_HEAD(&cgrp->self.children);\n\tINIT_LIST_HEAD(&cgrp->cset_links);\n\tINIT_LIST_HEAD(&cgrp->pidlists);\n\tmutex_init(&cgrp->pidlist_mutex);\n\tcgrp->self.cgroup = cgrp;\n\tcgrp->self.flags |= CSS_ONLINE;\n\tcgrp->dom_cgrp = cgrp;\n\tcgrp->max_descendants = INT_MAX;\n\tcgrp->max_depth = INT_MAX;\n\tINIT_LIST_HEAD(&cgrp->rstat_css_list);\n\tprev_cputime_init(&cgrp->prev_cputime);\n\n\tfor_each_subsys(ss, ssid)\n\t\tINIT_LIST_HEAD(&cgrp->e_csets[ssid]);\n\n\tinit_waitqueue_head(&cgrp->offline_waitq);\n\tINIT_WORK(&cgrp->release_agent_work, cgroup1_release_agent);\n}\n\nvoid init_cgroup_root(struct cgroup_fs_context *ctx)\n{\n\tstruct cgroup_root *root = ctx->root;\n\tstruct cgroup *cgrp = &root->cgrp;\n\n\tINIT_LIST_HEAD(&root->root_list);\n\tatomic_set(&root->nr_cgrps, 1);\n\tcgrp->root = root;\n\tinit_cgroup_housekeeping(cgrp);\n\n\t \n\troot->flags = ctx->flags & ~CGRP_ROOT_FAVOR_DYNMODS;\n\tif (ctx->release_agent)\n\t\tstrscpy(root->release_agent_path, ctx->release_agent, PATH_MAX);\n\tif (ctx->name)\n\t\tstrscpy(root->name, ctx->name, MAX_CGROUP_ROOT_NAMELEN);\n\tif (ctx->cpuset_clone_children)\n\t\tset_bit(CGRP_CPUSET_CLONE_CHILDREN, &root->cgrp.flags);\n}\n\nint cgroup_setup_root(struct cgroup_root *root, u16 ss_mask)\n{\n\tLIST_HEAD(tmp_links);\n\tstruct cgroup *root_cgrp = &root->cgrp;\n\tstruct kernfs_syscall_ops *kf_sops;\n\tstruct css_set *cset;\n\tint i, ret;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tret = percpu_ref_init(&root_cgrp->self.refcnt, css_release,\n\t\t\t      0, GFP_KERNEL);\n\tif (ret)\n\t\tgoto out;\n\n\t \n\tret = allocate_cgrp_cset_links(2 * css_set_count, &tmp_links);\n\tif (ret)\n\t\tgoto cancel_ref;\n\n\tret = cgroup_init_root_id(root);\n\tif (ret)\n\t\tgoto cancel_ref;\n\n\tkf_sops = root == &cgrp_dfl_root ?\n\t\t&cgroup_kf_syscall_ops : &cgroup1_kf_syscall_ops;\n\n\troot->kf_root = kernfs_create_root(kf_sops,\n\t\t\t\t\t   KERNFS_ROOT_CREATE_DEACTIVATED |\n\t\t\t\t\t   KERNFS_ROOT_SUPPORT_EXPORTOP |\n\t\t\t\t\t   KERNFS_ROOT_SUPPORT_USER_XATTR,\n\t\t\t\t\t   root_cgrp);\n\tif (IS_ERR(root->kf_root)) {\n\t\tret = PTR_ERR(root->kf_root);\n\t\tgoto exit_root_id;\n\t}\n\troot_cgrp->kn = kernfs_root_to_node(root->kf_root);\n\tWARN_ON_ONCE(cgroup_ino(root_cgrp) != 1);\n\troot_cgrp->ancestors[0] = root_cgrp;\n\n\tret = css_populate_dir(&root_cgrp->self);\n\tif (ret)\n\t\tgoto destroy_root;\n\n\tret = cgroup_rstat_init(root_cgrp);\n\tif (ret)\n\t\tgoto destroy_root;\n\n\tret = rebind_subsystems(root, ss_mask);\n\tif (ret)\n\t\tgoto exit_stats;\n\n\tret = cgroup_bpf_inherit(root_cgrp);\n\tWARN_ON_ONCE(ret);\n\n\ttrace_cgroup_setup_root(root);\n\n\t \n\tlist_add(&root->root_list, &cgroup_roots);\n\tcgroup_root_count++;\n\n\t \n\tspin_lock_irq(&css_set_lock);\n\thash_for_each(css_set_table, i, cset, hlist) {\n\t\tlink_css_set(&tmp_links, cset, root_cgrp);\n\t\tif (css_set_populated(cset))\n\t\t\tcgroup_update_populated(root_cgrp, true);\n\t}\n\tspin_unlock_irq(&css_set_lock);\n\n\tBUG_ON(!list_empty(&root_cgrp->self.children));\n\tBUG_ON(atomic_read(&root->nr_cgrps) != 1);\n\n\tret = 0;\n\tgoto out;\n\nexit_stats:\n\tcgroup_rstat_exit(root_cgrp);\ndestroy_root:\n\tkernfs_destroy_root(root->kf_root);\n\troot->kf_root = NULL;\nexit_root_id:\n\tcgroup_exit_root_id(root);\ncancel_ref:\n\tpercpu_ref_exit(&root_cgrp->self.refcnt);\nout:\n\tfree_cgrp_cset_links(&tmp_links);\n\treturn ret;\n}\n\nint cgroup_do_get_tree(struct fs_context *fc)\n{\n\tstruct cgroup_fs_context *ctx = cgroup_fc2context(fc);\n\tint ret;\n\n\tctx->kfc.root = ctx->root->kf_root;\n\tif (fc->fs_type == &cgroup2_fs_type)\n\t\tctx->kfc.magic = CGROUP2_SUPER_MAGIC;\n\telse\n\t\tctx->kfc.magic = CGROUP_SUPER_MAGIC;\n\tret = kernfs_get_tree(fc);\n\n\t \n\tif (!ret && ctx->ns != &init_cgroup_ns) {\n\t\tstruct dentry *nsdentry;\n\t\tstruct super_block *sb = fc->root->d_sb;\n\t\tstruct cgroup *cgrp;\n\n\t\tcgroup_lock();\n\t\tspin_lock_irq(&css_set_lock);\n\n\t\tcgrp = cset_cgroup_from_root(ctx->ns->root_cset, ctx->root);\n\n\t\tspin_unlock_irq(&css_set_lock);\n\t\tcgroup_unlock();\n\n\t\tnsdentry = kernfs_node_dentry(cgrp->kn, sb);\n\t\tdput(fc->root);\n\t\tif (IS_ERR(nsdentry)) {\n\t\t\tdeactivate_locked_super(sb);\n\t\t\tret = PTR_ERR(nsdentry);\n\t\t\tnsdentry = NULL;\n\t\t}\n\t\tfc->root = nsdentry;\n\t}\n\n\tif (!ctx->kfc.new_sb_created)\n\t\tcgroup_put(&ctx->root->cgrp);\n\n\treturn ret;\n}\n\n \nstatic void cgroup_fs_context_free(struct fs_context *fc)\n{\n\tstruct cgroup_fs_context *ctx = cgroup_fc2context(fc);\n\n\tkfree(ctx->name);\n\tkfree(ctx->release_agent);\n\tput_cgroup_ns(ctx->ns);\n\tkernfs_free_fs_context(fc);\n\tkfree(ctx);\n}\n\nstatic int cgroup_get_tree(struct fs_context *fc)\n{\n\tstruct cgroup_fs_context *ctx = cgroup_fc2context(fc);\n\tint ret;\n\n\tWRITE_ONCE(cgrp_dfl_visible, true);\n\tcgroup_get_live(&cgrp_dfl_root.cgrp);\n\tctx->root = &cgrp_dfl_root;\n\n\tret = cgroup_do_get_tree(fc);\n\tif (!ret)\n\t\tapply_cgroup_root_flags(ctx->flags);\n\treturn ret;\n}\n\nstatic const struct fs_context_operations cgroup_fs_context_ops = {\n\t.free\t\t= cgroup_fs_context_free,\n\t.parse_param\t= cgroup2_parse_param,\n\t.get_tree\t= cgroup_get_tree,\n\t.reconfigure\t= cgroup_reconfigure,\n};\n\nstatic const struct fs_context_operations cgroup1_fs_context_ops = {\n\t.free\t\t= cgroup_fs_context_free,\n\t.parse_param\t= cgroup1_parse_param,\n\t.get_tree\t= cgroup1_get_tree,\n\t.reconfigure\t= cgroup1_reconfigure,\n};\n\n \nstatic int cgroup_init_fs_context(struct fs_context *fc)\n{\n\tstruct cgroup_fs_context *ctx;\n\n\tctx = kzalloc(sizeof(struct cgroup_fs_context), GFP_KERNEL);\n\tif (!ctx)\n\t\treturn -ENOMEM;\n\n\tctx->ns = current->nsproxy->cgroup_ns;\n\tget_cgroup_ns(ctx->ns);\n\tfc->fs_private = &ctx->kfc;\n\tif (fc->fs_type == &cgroup2_fs_type)\n\t\tfc->ops = &cgroup_fs_context_ops;\n\telse\n\t\tfc->ops = &cgroup1_fs_context_ops;\n\tput_user_ns(fc->user_ns);\n\tfc->user_ns = get_user_ns(ctx->ns->user_ns);\n\tfc->global = true;\n\n#ifdef CONFIG_CGROUP_FAVOR_DYNMODS\n\tctx->flags |= CGRP_ROOT_FAVOR_DYNMODS;\n#endif\n\treturn 0;\n}\n\nstatic void cgroup_kill_sb(struct super_block *sb)\n{\n\tstruct kernfs_root *kf_root = kernfs_root_from_sb(sb);\n\tstruct cgroup_root *root = cgroup_root_from_kf(kf_root);\n\n\t \n\tif (list_empty(&root->cgrp.self.children) && root != &cgrp_dfl_root &&\n\t    !percpu_ref_is_dying(&root->cgrp.self.refcnt)) {\n\t\tcgroup_bpf_offline(&root->cgrp);\n\t\tpercpu_ref_kill(&root->cgrp.self.refcnt);\n\t}\n\tcgroup_put(&root->cgrp);\n\tkernfs_kill_sb(sb);\n}\n\nstruct file_system_type cgroup_fs_type = {\n\t.name\t\t\t= \"cgroup\",\n\t.init_fs_context\t= cgroup_init_fs_context,\n\t.parameters\t\t= cgroup1_fs_parameters,\n\t.kill_sb\t\t= cgroup_kill_sb,\n\t.fs_flags\t\t= FS_USERNS_MOUNT,\n};\n\nstatic struct file_system_type cgroup2_fs_type = {\n\t.name\t\t\t= \"cgroup2\",\n\t.init_fs_context\t= cgroup_init_fs_context,\n\t.parameters\t\t= cgroup2_fs_parameters,\n\t.kill_sb\t\t= cgroup_kill_sb,\n\t.fs_flags\t\t= FS_USERNS_MOUNT,\n};\n\n#ifdef CONFIG_CPUSETS\nstatic const struct fs_context_operations cpuset_fs_context_ops = {\n\t.get_tree\t= cgroup1_get_tree,\n\t.free\t\t= cgroup_fs_context_free,\n};\n\n \nstatic int cpuset_init_fs_context(struct fs_context *fc)\n{\n\tchar *agent = kstrdup(\"/sbin/cpuset_release_agent\", GFP_USER);\n\tstruct cgroup_fs_context *ctx;\n\tint err;\n\n\terr = cgroup_init_fs_context(fc);\n\tif (err) {\n\t\tkfree(agent);\n\t\treturn err;\n\t}\n\n\tfc->ops = &cpuset_fs_context_ops;\n\n\tctx = cgroup_fc2context(fc);\n\tctx->subsys_mask = 1 << cpuset_cgrp_id;\n\tctx->flags |= CGRP_ROOT_NOPREFIX;\n\tctx->release_agent = agent;\n\n\tget_filesystem(&cgroup_fs_type);\n\tput_filesystem(fc->fs_type);\n\tfc->fs_type = &cgroup_fs_type;\n\n\treturn 0;\n}\n\nstatic struct file_system_type cpuset_fs_type = {\n\t.name\t\t\t= \"cpuset\",\n\t.init_fs_context\t= cpuset_init_fs_context,\n\t.fs_flags\t\t= FS_USERNS_MOUNT,\n};\n#endif\n\nint cgroup_path_ns_locked(struct cgroup *cgrp, char *buf, size_t buflen,\n\t\t\t  struct cgroup_namespace *ns)\n{\n\tstruct cgroup *root = cset_cgroup_from_root(ns->root_cset, cgrp->root);\n\n\treturn kernfs_path_from_node(cgrp->kn, root->kn, buf, buflen);\n}\n\nint cgroup_path_ns(struct cgroup *cgrp, char *buf, size_t buflen,\n\t\t   struct cgroup_namespace *ns)\n{\n\tint ret;\n\n\tcgroup_lock();\n\tspin_lock_irq(&css_set_lock);\n\n\tret = cgroup_path_ns_locked(cgrp, buf, buflen, ns);\n\n\tspin_unlock_irq(&css_set_lock);\n\tcgroup_unlock();\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(cgroup_path_ns);\n\n \nvoid cgroup_attach_lock(bool lock_threadgroup)\n{\n\tcpus_read_lock();\n\tif (lock_threadgroup)\n\t\tpercpu_down_write(&cgroup_threadgroup_rwsem);\n}\n\n \nvoid cgroup_attach_unlock(bool lock_threadgroup)\n{\n\tif (lock_threadgroup)\n\t\tpercpu_up_write(&cgroup_threadgroup_rwsem);\n\tcpus_read_unlock();\n}\n\n \nstatic void cgroup_migrate_add_task(struct task_struct *task,\n\t\t\t\t    struct cgroup_mgctx *mgctx)\n{\n\tstruct css_set *cset;\n\n\tlockdep_assert_held(&css_set_lock);\n\n\t \n\tif (task->flags & PF_EXITING)\n\t\treturn;\n\n\t \n\tWARN_ON_ONCE(list_empty(&task->cg_list));\n\n\tcset = task_css_set(task);\n\tif (!cset->mg_src_cgrp)\n\t\treturn;\n\n\tmgctx->tset.nr_tasks++;\n\n\tlist_move_tail(&task->cg_list, &cset->mg_tasks);\n\tif (list_empty(&cset->mg_node))\n\t\tlist_add_tail(&cset->mg_node,\n\t\t\t      &mgctx->tset.src_csets);\n\tif (list_empty(&cset->mg_dst_cset->mg_node))\n\t\tlist_add_tail(&cset->mg_dst_cset->mg_node,\n\t\t\t      &mgctx->tset.dst_csets);\n}\n\n \nstruct task_struct *cgroup_taskset_first(struct cgroup_taskset *tset,\n\t\t\t\t\t struct cgroup_subsys_state **dst_cssp)\n{\n\ttset->cur_cset = list_first_entry(tset->csets, struct css_set, mg_node);\n\ttset->cur_task = NULL;\n\n\treturn cgroup_taskset_next(tset, dst_cssp);\n}\n\n \nstruct task_struct *cgroup_taskset_next(struct cgroup_taskset *tset,\n\t\t\t\t\tstruct cgroup_subsys_state **dst_cssp)\n{\n\tstruct css_set *cset = tset->cur_cset;\n\tstruct task_struct *task = tset->cur_task;\n\n\twhile (CGROUP_HAS_SUBSYS_CONFIG && &cset->mg_node != tset->csets) {\n\t\tif (!task)\n\t\t\ttask = list_first_entry(&cset->mg_tasks,\n\t\t\t\t\t\tstruct task_struct, cg_list);\n\t\telse\n\t\t\ttask = list_next_entry(task, cg_list);\n\n\t\tif (&task->cg_list != &cset->mg_tasks) {\n\t\t\ttset->cur_cset = cset;\n\t\t\ttset->cur_task = task;\n\n\t\t\t \n\t\t\tif (cset->mg_dst_cset)\n\t\t\t\t*dst_cssp = cset->mg_dst_cset->subsys[tset->ssid];\n\t\t\telse\n\t\t\t\t*dst_cssp = cset->subsys[tset->ssid];\n\n\t\t\treturn task;\n\t\t}\n\n\t\tcset = list_next_entry(cset, mg_node);\n\t\ttask = NULL;\n\t}\n\n\treturn NULL;\n}\n\n \nstatic int cgroup_migrate_execute(struct cgroup_mgctx *mgctx)\n{\n\tstruct cgroup_taskset *tset = &mgctx->tset;\n\tstruct cgroup_subsys *ss;\n\tstruct task_struct *task, *tmp_task;\n\tstruct css_set *cset, *tmp_cset;\n\tint ssid, failed_ssid, ret;\n\n\t \n\tif (tset->nr_tasks) {\n\t\tdo_each_subsys_mask(ss, ssid, mgctx->ss_mask) {\n\t\t\tif (ss->can_attach) {\n\t\t\t\ttset->ssid = ssid;\n\t\t\t\tret = ss->can_attach(tset);\n\t\t\t\tif (ret) {\n\t\t\t\t\tfailed_ssid = ssid;\n\t\t\t\t\tgoto out_cancel_attach;\n\t\t\t\t}\n\t\t\t}\n\t\t} while_each_subsys_mask();\n\t}\n\n\t \n\tspin_lock_irq(&css_set_lock);\n\tlist_for_each_entry(cset, &tset->src_csets, mg_node) {\n\t\tlist_for_each_entry_safe(task, tmp_task, &cset->mg_tasks, cg_list) {\n\t\t\tstruct css_set *from_cset = task_css_set(task);\n\t\t\tstruct css_set *to_cset = cset->mg_dst_cset;\n\n\t\t\tget_css_set(to_cset);\n\t\t\tto_cset->nr_tasks++;\n\t\t\tcss_set_move_task(task, from_cset, to_cset, true);\n\t\t\tfrom_cset->nr_tasks--;\n\t\t\t \n\t\t\tcgroup_freezer_migrate_task(task, from_cset->dfl_cgrp,\n\t\t\t\t\t\t    to_cset->dfl_cgrp);\n\t\t\tput_css_set_locked(from_cset);\n\n\t\t}\n\t}\n\tspin_unlock_irq(&css_set_lock);\n\n\t \n\ttset->csets = &tset->dst_csets;\n\n\tif (tset->nr_tasks) {\n\t\tdo_each_subsys_mask(ss, ssid, mgctx->ss_mask) {\n\t\t\tif (ss->attach) {\n\t\t\t\ttset->ssid = ssid;\n\t\t\t\tss->attach(tset);\n\t\t\t}\n\t\t} while_each_subsys_mask();\n\t}\n\n\tret = 0;\n\tgoto out_release_tset;\n\nout_cancel_attach:\n\tif (tset->nr_tasks) {\n\t\tdo_each_subsys_mask(ss, ssid, mgctx->ss_mask) {\n\t\t\tif (ssid == failed_ssid)\n\t\t\t\tbreak;\n\t\t\tif (ss->cancel_attach) {\n\t\t\t\ttset->ssid = ssid;\n\t\t\t\tss->cancel_attach(tset);\n\t\t\t}\n\t\t} while_each_subsys_mask();\n\t}\nout_release_tset:\n\tspin_lock_irq(&css_set_lock);\n\tlist_splice_init(&tset->dst_csets, &tset->src_csets);\n\tlist_for_each_entry_safe(cset, tmp_cset, &tset->src_csets, mg_node) {\n\t\tlist_splice_tail_init(&cset->mg_tasks, &cset->tasks);\n\t\tlist_del_init(&cset->mg_node);\n\t}\n\tspin_unlock_irq(&css_set_lock);\n\n\t \n\ttset->nr_tasks = 0;\n\ttset->csets    = &tset->src_csets;\n\treturn ret;\n}\n\n \nint cgroup_migrate_vet_dst(struct cgroup *dst_cgrp)\n{\n\t \n\tif (!cgroup_on_dfl(dst_cgrp))\n\t\treturn 0;\n\n\t \n\tif (!cgroup_is_valid_domain(dst_cgrp->dom_cgrp))\n\t\treturn -EOPNOTSUPP;\n\n\t \n\tif (cgroup_can_be_thread_root(dst_cgrp) || cgroup_is_threaded(dst_cgrp))\n\t\treturn 0;\n\n\t \n\tif (dst_cgrp->subtree_control)\n\t\treturn -EBUSY;\n\n\treturn 0;\n}\n\n \nvoid cgroup_migrate_finish(struct cgroup_mgctx *mgctx)\n{\n\tstruct css_set *cset, *tmp_cset;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tspin_lock_irq(&css_set_lock);\n\n\tlist_for_each_entry_safe(cset, tmp_cset, &mgctx->preloaded_src_csets,\n\t\t\t\t mg_src_preload_node) {\n\t\tcset->mg_src_cgrp = NULL;\n\t\tcset->mg_dst_cgrp = NULL;\n\t\tcset->mg_dst_cset = NULL;\n\t\tlist_del_init(&cset->mg_src_preload_node);\n\t\tput_css_set_locked(cset);\n\t}\n\n\tlist_for_each_entry_safe(cset, tmp_cset, &mgctx->preloaded_dst_csets,\n\t\t\t\t mg_dst_preload_node) {\n\t\tcset->mg_src_cgrp = NULL;\n\t\tcset->mg_dst_cgrp = NULL;\n\t\tcset->mg_dst_cset = NULL;\n\t\tlist_del_init(&cset->mg_dst_preload_node);\n\t\tput_css_set_locked(cset);\n\t}\n\n\tspin_unlock_irq(&css_set_lock);\n}\n\n \nvoid cgroup_migrate_add_src(struct css_set *src_cset,\n\t\t\t    struct cgroup *dst_cgrp,\n\t\t\t    struct cgroup_mgctx *mgctx)\n{\n\tstruct cgroup *src_cgrp;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\tlockdep_assert_held(&css_set_lock);\n\n\t \n\tif (src_cset->dead)\n\t\treturn;\n\n\tif (!list_empty(&src_cset->mg_src_preload_node))\n\t\treturn;\n\n\tsrc_cgrp = cset_cgroup_from_root(src_cset, dst_cgrp->root);\n\n\tWARN_ON(src_cset->mg_src_cgrp);\n\tWARN_ON(src_cset->mg_dst_cgrp);\n\tWARN_ON(!list_empty(&src_cset->mg_tasks));\n\tWARN_ON(!list_empty(&src_cset->mg_node));\n\n\tsrc_cset->mg_src_cgrp = src_cgrp;\n\tsrc_cset->mg_dst_cgrp = dst_cgrp;\n\tget_css_set(src_cset);\n\tlist_add_tail(&src_cset->mg_src_preload_node, &mgctx->preloaded_src_csets);\n}\n\n \nint cgroup_migrate_prepare_dst(struct cgroup_mgctx *mgctx)\n{\n\tstruct css_set *src_cset, *tmp_cset;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\t \n\tlist_for_each_entry_safe(src_cset, tmp_cset, &mgctx->preloaded_src_csets,\n\t\t\t\t mg_src_preload_node) {\n\t\tstruct css_set *dst_cset;\n\t\tstruct cgroup_subsys *ss;\n\t\tint ssid;\n\n\t\tdst_cset = find_css_set(src_cset, src_cset->mg_dst_cgrp);\n\t\tif (!dst_cset)\n\t\t\treturn -ENOMEM;\n\n\t\tWARN_ON_ONCE(src_cset->mg_dst_cset || dst_cset->mg_dst_cset);\n\n\t\t \n\t\tif (src_cset == dst_cset) {\n\t\t\tsrc_cset->mg_src_cgrp = NULL;\n\t\t\tsrc_cset->mg_dst_cgrp = NULL;\n\t\t\tlist_del_init(&src_cset->mg_src_preload_node);\n\t\t\tput_css_set(src_cset);\n\t\t\tput_css_set(dst_cset);\n\t\t\tcontinue;\n\t\t}\n\n\t\tsrc_cset->mg_dst_cset = dst_cset;\n\n\t\tif (list_empty(&dst_cset->mg_dst_preload_node))\n\t\t\tlist_add_tail(&dst_cset->mg_dst_preload_node,\n\t\t\t\t      &mgctx->preloaded_dst_csets);\n\t\telse\n\t\t\tput_css_set(dst_cset);\n\n\t\tfor_each_subsys(ss, ssid)\n\t\t\tif (src_cset->subsys[ssid] != dst_cset->subsys[ssid])\n\t\t\t\tmgctx->ss_mask |= 1 << ssid;\n\t}\n\n\treturn 0;\n}\n\n \nint cgroup_migrate(struct task_struct *leader, bool threadgroup,\n\t\t   struct cgroup_mgctx *mgctx)\n{\n\tstruct task_struct *task;\n\n\t \n\tspin_lock_irq(&css_set_lock);\n\ttask = leader;\n\tdo {\n\t\tcgroup_migrate_add_task(task, mgctx);\n\t\tif (!threadgroup)\n\t\t\tbreak;\n\t} while_each_thread(leader, task);\n\tspin_unlock_irq(&css_set_lock);\n\n\treturn cgroup_migrate_execute(mgctx);\n}\n\n \nint cgroup_attach_task(struct cgroup *dst_cgrp, struct task_struct *leader,\n\t\t       bool threadgroup)\n{\n\tDEFINE_CGROUP_MGCTX(mgctx);\n\tstruct task_struct *task;\n\tint ret = 0;\n\n\t \n\tspin_lock_irq(&css_set_lock);\n\trcu_read_lock();\n\ttask = leader;\n\tdo {\n\t\tcgroup_migrate_add_src(task_css_set(task), dst_cgrp, &mgctx);\n\t\tif (!threadgroup)\n\t\t\tbreak;\n\t} while_each_thread(leader, task);\n\trcu_read_unlock();\n\tspin_unlock_irq(&css_set_lock);\n\n\t \n\tret = cgroup_migrate_prepare_dst(&mgctx);\n\tif (!ret)\n\t\tret = cgroup_migrate(leader, threadgroup, &mgctx);\n\n\tcgroup_migrate_finish(&mgctx);\n\n\tif (!ret)\n\t\tTRACE_CGROUP_PATH(attach_task, dst_cgrp, leader, threadgroup);\n\n\treturn ret;\n}\n\nstruct task_struct *cgroup_procs_write_start(char *buf, bool threadgroup,\n\t\t\t\t\t     bool *threadgroup_locked)\n{\n\tstruct task_struct *tsk;\n\tpid_t pid;\n\n\tif (kstrtoint(strstrip(buf), 0, &pid) || pid < 0)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t \n\tlockdep_assert_held(&cgroup_mutex);\n\t*threadgroup_locked = pid || threadgroup;\n\tcgroup_attach_lock(*threadgroup_locked);\n\n\trcu_read_lock();\n\tif (pid) {\n\t\ttsk = find_task_by_vpid(pid);\n\t\tif (!tsk) {\n\t\t\ttsk = ERR_PTR(-ESRCH);\n\t\t\tgoto out_unlock_threadgroup;\n\t\t}\n\t} else {\n\t\ttsk = current;\n\t}\n\n\tif (threadgroup)\n\t\ttsk = tsk->group_leader;\n\n\t \n\tif (tsk->no_cgroup_migration || (tsk->flags & PF_NO_SETAFFINITY)) {\n\t\ttsk = ERR_PTR(-EINVAL);\n\t\tgoto out_unlock_threadgroup;\n\t}\n\n\tget_task_struct(tsk);\n\tgoto out_unlock_rcu;\n\nout_unlock_threadgroup:\n\tcgroup_attach_unlock(*threadgroup_locked);\n\t*threadgroup_locked = false;\nout_unlock_rcu:\n\trcu_read_unlock();\n\treturn tsk;\n}\n\nvoid cgroup_procs_write_finish(struct task_struct *task, bool threadgroup_locked)\n{\n\tstruct cgroup_subsys *ss;\n\tint ssid;\n\n\t \n\tput_task_struct(task);\n\n\tcgroup_attach_unlock(threadgroup_locked);\n\n\tfor_each_subsys(ss, ssid)\n\t\tif (ss->post_attach)\n\t\t\tss->post_attach();\n}\n\nstatic void cgroup_print_ss_mask(struct seq_file *seq, u16 ss_mask)\n{\n\tstruct cgroup_subsys *ss;\n\tbool printed = false;\n\tint ssid;\n\n\tdo_each_subsys_mask(ss, ssid, ss_mask) {\n\t\tif (printed)\n\t\t\tseq_putc(seq, ' ');\n\t\tseq_puts(seq, ss->name);\n\t\tprinted = true;\n\t} while_each_subsys_mask();\n\tif (printed)\n\t\tseq_putc(seq, '\\n');\n}\n\n \nstatic int cgroup_controllers_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup *cgrp = seq_css(seq)->cgroup;\n\n\tcgroup_print_ss_mask(seq, cgroup_control(cgrp));\n\treturn 0;\n}\n\n \nstatic int cgroup_subtree_control_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup *cgrp = seq_css(seq)->cgroup;\n\n\tcgroup_print_ss_mask(seq, cgrp->subtree_control);\n\treturn 0;\n}\n\n \nstatic int cgroup_update_dfl_csses(struct cgroup *cgrp)\n{\n\tDEFINE_CGROUP_MGCTX(mgctx);\n\tstruct cgroup_subsys_state *d_css;\n\tstruct cgroup *dsct;\n\tstruct css_set *src_cset;\n\tbool has_tasks;\n\tint ret;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\t \n\tspin_lock_irq(&css_set_lock);\n\tcgroup_for_each_live_descendant_pre(dsct, d_css, cgrp) {\n\t\tstruct cgrp_cset_link *link;\n\n\t\t \n\t\tif (dsct == cgrp)\n\t\t\tcontinue;\n\n\t\tlist_for_each_entry(link, &dsct->cset_links, cset_link)\n\t\t\tcgroup_migrate_add_src(link->cset, dsct, &mgctx);\n\t}\n\tspin_unlock_irq(&css_set_lock);\n\n\t \n\thas_tasks = !list_empty(&mgctx.preloaded_src_csets);\n\tcgroup_attach_lock(has_tasks);\n\n\t \n\tret = cgroup_migrate_prepare_dst(&mgctx);\n\tif (ret)\n\t\tgoto out_finish;\n\n\tspin_lock_irq(&css_set_lock);\n\tlist_for_each_entry(src_cset, &mgctx.preloaded_src_csets,\n\t\t\t    mg_src_preload_node) {\n\t\tstruct task_struct *task, *ntask;\n\n\t\t \n\t\tlist_for_each_entry_safe(task, ntask, &src_cset->tasks, cg_list)\n\t\t\tcgroup_migrate_add_task(task, &mgctx);\n\t}\n\tspin_unlock_irq(&css_set_lock);\n\n\tret = cgroup_migrate_execute(&mgctx);\nout_finish:\n\tcgroup_migrate_finish(&mgctx);\n\tcgroup_attach_unlock(has_tasks);\n\treturn ret;\n}\n\n \nvoid cgroup_lock_and_drain_offline(struct cgroup *cgrp)\n\t__acquires(&cgroup_mutex)\n{\n\tstruct cgroup *dsct;\n\tstruct cgroup_subsys_state *d_css;\n\tstruct cgroup_subsys *ss;\n\tint ssid;\n\nrestart:\n\tcgroup_lock();\n\n\tcgroup_for_each_live_descendant_post(dsct, d_css, cgrp) {\n\t\tfor_each_subsys(ss, ssid) {\n\t\t\tstruct cgroup_subsys_state *css = cgroup_css(dsct, ss);\n\t\t\tDEFINE_WAIT(wait);\n\n\t\t\tif (!css || !percpu_ref_is_dying(&css->refcnt))\n\t\t\t\tcontinue;\n\n\t\t\tcgroup_get_live(dsct);\n\t\t\tprepare_to_wait(&dsct->offline_waitq, &wait,\n\t\t\t\t\tTASK_UNINTERRUPTIBLE);\n\n\t\t\tcgroup_unlock();\n\t\t\tschedule();\n\t\t\tfinish_wait(&dsct->offline_waitq, &wait);\n\n\t\t\tcgroup_put(dsct);\n\t\t\tgoto restart;\n\t\t}\n\t}\n}\n\n \nstatic void cgroup_save_control(struct cgroup *cgrp)\n{\n\tstruct cgroup *dsct;\n\tstruct cgroup_subsys_state *d_css;\n\n\tcgroup_for_each_live_descendant_pre(dsct, d_css, cgrp) {\n\t\tdsct->old_subtree_control = dsct->subtree_control;\n\t\tdsct->old_subtree_ss_mask = dsct->subtree_ss_mask;\n\t\tdsct->old_dom_cgrp = dsct->dom_cgrp;\n\t}\n}\n\n \nstatic void cgroup_propagate_control(struct cgroup *cgrp)\n{\n\tstruct cgroup *dsct;\n\tstruct cgroup_subsys_state *d_css;\n\n\tcgroup_for_each_live_descendant_pre(dsct, d_css, cgrp) {\n\t\tdsct->subtree_control &= cgroup_control(dsct);\n\t\tdsct->subtree_ss_mask =\n\t\t\tcgroup_calc_subtree_ss_mask(dsct->subtree_control,\n\t\t\t\t\t\t    cgroup_ss_mask(dsct));\n\t}\n}\n\n \nstatic void cgroup_restore_control(struct cgroup *cgrp)\n{\n\tstruct cgroup *dsct;\n\tstruct cgroup_subsys_state *d_css;\n\n\tcgroup_for_each_live_descendant_post(dsct, d_css, cgrp) {\n\t\tdsct->subtree_control = dsct->old_subtree_control;\n\t\tdsct->subtree_ss_mask = dsct->old_subtree_ss_mask;\n\t\tdsct->dom_cgrp = dsct->old_dom_cgrp;\n\t}\n}\n\nstatic bool css_visible(struct cgroup_subsys_state *css)\n{\n\tstruct cgroup_subsys *ss = css->ss;\n\tstruct cgroup *cgrp = css->cgroup;\n\n\tif (cgroup_control(cgrp) & (1 << ss->id))\n\t\treturn true;\n\tif (!(cgroup_ss_mask(cgrp) & (1 << ss->id)))\n\t\treturn false;\n\treturn cgroup_on_dfl(cgrp) && ss->implicit_on_dfl;\n}\n\n \nstatic int cgroup_apply_control_enable(struct cgroup *cgrp)\n{\n\tstruct cgroup *dsct;\n\tstruct cgroup_subsys_state *d_css;\n\tstruct cgroup_subsys *ss;\n\tint ssid, ret;\n\n\tcgroup_for_each_live_descendant_pre(dsct, d_css, cgrp) {\n\t\tfor_each_subsys(ss, ssid) {\n\t\t\tstruct cgroup_subsys_state *css = cgroup_css(dsct, ss);\n\n\t\t\tif (!(cgroup_ss_mask(dsct) & (1 << ss->id)))\n\t\t\t\tcontinue;\n\n\t\t\tif (!css) {\n\t\t\t\tcss = css_create(dsct, ss);\n\t\t\t\tif (IS_ERR(css))\n\t\t\t\t\treturn PTR_ERR(css);\n\t\t\t}\n\n\t\t\tWARN_ON_ONCE(percpu_ref_is_dying(&css->refcnt));\n\n\t\t\tif (css_visible(css)) {\n\t\t\t\tret = css_populate_dir(css);\n\t\t\t\tif (ret)\n\t\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic void cgroup_apply_control_disable(struct cgroup *cgrp)\n{\n\tstruct cgroup *dsct;\n\tstruct cgroup_subsys_state *d_css;\n\tstruct cgroup_subsys *ss;\n\tint ssid;\n\n\tcgroup_for_each_live_descendant_post(dsct, d_css, cgrp) {\n\t\tfor_each_subsys(ss, ssid) {\n\t\t\tstruct cgroup_subsys_state *css = cgroup_css(dsct, ss);\n\n\t\t\tif (!css)\n\t\t\t\tcontinue;\n\n\t\t\tWARN_ON_ONCE(percpu_ref_is_dying(&css->refcnt));\n\n\t\t\tif (css->parent &&\n\t\t\t    !(cgroup_ss_mask(dsct) & (1 << ss->id))) {\n\t\t\t\tkill_css(css);\n\t\t\t} else if (!css_visible(css)) {\n\t\t\t\tcss_clear_dir(css);\n\t\t\t\tif (ss->css_reset)\n\t\t\t\t\tss->css_reset(css);\n\t\t\t}\n\t\t}\n\t}\n}\n\n \nstatic int cgroup_apply_control(struct cgroup *cgrp)\n{\n\tint ret;\n\n\tcgroup_propagate_control(cgrp);\n\n\tret = cgroup_apply_control_enable(cgrp);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\treturn cgroup_update_dfl_csses(cgrp);\n}\n\n \nstatic void cgroup_finalize_control(struct cgroup *cgrp, int ret)\n{\n\tif (ret) {\n\t\tcgroup_restore_control(cgrp);\n\t\tcgroup_propagate_control(cgrp);\n\t}\n\n\tcgroup_apply_control_disable(cgrp);\n}\n\nstatic int cgroup_vet_subtree_control_enable(struct cgroup *cgrp, u16 enable)\n{\n\tu16 domain_enable = enable & ~cgrp_dfl_threaded_ss_mask;\n\n\t \n\tif (!enable)\n\t\treturn 0;\n\n\t \n\tif (!cgroup_is_valid_domain(cgrp->dom_cgrp))\n\t\treturn -EOPNOTSUPP;\n\n\t \n\tif (cgroup_is_mixable(cgrp))\n\t\treturn 0;\n\n\tif (domain_enable) {\n\t\t \n\t\tif (cgroup_is_thread_root(cgrp) || cgroup_is_threaded(cgrp))\n\t\t\treturn -EOPNOTSUPP;\n\t} else {\n\t\t \n\t\tif (cgroup_can_be_thread_root(cgrp) || cgroup_is_threaded(cgrp))\n\t\t\treturn 0;\n\t}\n\n\t \n\tif (cgroup_has_tasks(cgrp))\n\t\treturn -EBUSY;\n\n\treturn 0;\n}\n\n \nstatic ssize_t cgroup_subtree_control_write(struct kernfs_open_file *of,\n\t\t\t\t\t    char *buf, size_t nbytes,\n\t\t\t\t\t    loff_t off)\n{\n\tu16 enable = 0, disable = 0;\n\tstruct cgroup *cgrp, *child;\n\tstruct cgroup_subsys *ss;\n\tchar *tok;\n\tint ssid, ret;\n\n\t \n\tbuf = strstrip(buf);\n\twhile ((tok = strsep(&buf, \" \"))) {\n\t\tif (tok[0] == '\\0')\n\t\t\tcontinue;\n\t\tdo_each_subsys_mask(ss, ssid, ~cgrp_dfl_inhibit_ss_mask) {\n\t\t\tif (!cgroup_ssid_enabled(ssid) ||\n\t\t\t    strcmp(tok + 1, ss->name))\n\t\t\t\tcontinue;\n\n\t\t\tif (*tok == '+') {\n\t\t\t\tenable |= 1 << ssid;\n\t\t\t\tdisable &= ~(1 << ssid);\n\t\t\t} else if (*tok == '-') {\n\t\t\t\tdisable |= 1 << ssid;\n\t\t\t\tenable &= ~(1 << ssid);\n\t\t\t} else {\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\t} while_each_subsys_mask();\n\t\tif (ssid == CGROUP_SUBSYS_COUNT)\n\t\t\treturn -EINVAL;\n\t}\n\n\tcgrp = cgroup_kn_lock_live(of->kn, true);\n\tif (!cgrp)\n\t\treturn -ENODEV;\n\n\tfor_each_subsys(ss, ssid) {\n\t\tif (enable & (1 << ssid)) {\n\t\t\tif (cgrp->subtree_control & (1 << ssid)) {\n\t\t\t\tenable &= ~(1 << ssid);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (!(cgroup_control(cgrp) & (1 << ssid))) {\n\t\t\t\tret = -ENOENT;\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t} else if (disable & (1 << ssid)) {\n\t\t\tif (!(cgrp->subtree_control & (1 << ssid))) {\n\t\t\t\tdisable &= ~(1 << ssid);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t \n\t\t\tcgroup_for_each_live_child(child, cgrp) {\n\t\t\t\tif (child->subtree_control & (1 << ssid)) {\n\t\t\t\t\tret = -EBUSY;\n\t\t\t\t\tgoto out_unlock;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif (!enable && !disable) {\n\t\tret = 0;\n\t\tgoto out_unlock;\n\t}\n\n\tret = cgroup_vet_subtree_control_enable(cgrp, enable);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\t \n\tcgroup_save_control(cgrp);\n\n\tcgrp->subtree_control |= enable;\n\tcgrp->subtree_control &= ~disable;\n\n\tret = cgroup_apply_control(cgrp);\n\tcgroup_finalize_control(cgrp, ret);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tkernfs_activate(cgrp->kn);\nout_unlock:\n\tcgroup_kn_unlock(of->kn);\n\treturn ret ?: nbytes;\n}\n\n \nstatic int cgroup_enable_threaded(struct cgroup *cgrp)\n{\n\tstruct cgroup *parent = cgroup_parent(cgrp);\n\tstruct cgroup *dom_cgrp = parent->dom_cgrp;\n\tstruct cgroup *dsct;\n\tstruct cgroup_subsys_state *d_css;\n\tint ret;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\t \n\tif (cgroup_is_threaded(cgrp))\n\t\treturn 0;\n\n\t \n\tif (cgroup_is_populated(cgrp) ||\n\t    cgrp->subtree_control & ~cgrp_dfl_threaded_ss_mask)\n\t\treturn -EOPNOTSUPP;\n\n\t \n\tif (!cgroup_is_valid_domain(dom_cgrp) ||\n\t    !cgroup_can_be_thread_root(dom_cgrp))\n\t\treturn -EOPNOTSUPP;\n\n\t \n\tcgroup_save_control(cgrp);\n\n\tcgroup_for_each_live_descendant_pre(dsct, d_css, cgrp)\n\t\tif (dsct == cgrp || cgroup_is_threaded(dsct))\n\t\t\tdsct->dom_cgrp = dom_cgrp;\n\n\tret = cgroup_apply_control(cgrp);\n\tif (!ret)\n\t\tparent->nr_threaded_children++;\n\n\tcgroup_finalize_control(cgrp, ret);\n\treturn ret;\n}\n\nstatic int cgroup_type_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup *cgrp = seq_css(seq)->cgroup;\n\n\tif (cgroup_is_threaded(cgrp))\n\t\tseq_puts(seq, \"threaded\\n\");\n\telse if (!cgroup_is_valid_domain(cgrp))\n\t\tseq_puts(seq, \"domain invalid\\n\");\n\telse if (cgroup_is_thread_root(cgrp))\n\t\tseq_puts(seq, \"domain threaded\\n\");\n\telse\n\t\tseq_puts(seq, \"domain\\n\");\n\n\treturn 0;\n}\n\nstatic ssize_t cgroup_type_write(struct kernfs_open_file *of, char *buf,\n\t\t\t\t size_t nbytes, loff_t off)\n{\n\tstruct cgroup *cgrp;\n\tint ret;\n\n\t \n\tif (strcmp(strstrip(buf), \"threaded\"))\n\t\treturn -EINVAL;\n\n\t \n\tcgrp = cgroup_kn_lock_live(of->kn, true);\n\tif (!cgrp)\n\t\treturn -ENOENT;\n\n\t \n\tret = cgroup_enable_threaded(cgrp);\n\n\tcgroup_kn_unlock(of->kn);\n\treturn ret ?: nbytes;\n}\n\nstatic int cgroup_max_descendants_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup *cgrp = seq_css(seq)->cgroup;\n\tint descendants = READ_ONCE(cgrp->max_descendants);\n\n\tif (descendants == INT_MAX)\n\t\tseq_puts(seq, \"max\\n\");\n\telse\n\t\tseq_printf(seq, \"%d\\n\", descendants);\n\n\treturn 0;\n}\n\nstatic ssize_t cgroup_max_descendants_write(struct kernfs_open_file *of,\n\t\t\t\t\t   char *buf, size_t nbytes, loff_t off)\n{\n\tstruct cgroup *cgrp;\n\tint descendants;\n\tssize_t ret;\n\n\tbuf = strstrip(buf);\n\tif (!strcmp(buf, \"max\")) {\n\t\tdescendants = INT_MAX;\n\t} else {\n\t\tret = kstrtoint(buf, 0, &descendants);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (descendants < 0)\n\t\treturn -ERANGE;\n\n\tcgrp = cgroup_kn_lock_live(of->kn, false);\n\tif (!cgrp)\n\t\treturn -ENOENT;\n\n\tcgrp->max_descendants = descendants;\n\n\tcgroup_kn_unlock(of->kn);\n\n\treturn nbytes;\n}\n\nstatic int cgroup_max_depth_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup *cgrp = seq_css(seq)->cgroup;\n\tint depth = READ_ONCE(cgrp->max_depth);\n\n\tif (depth == INT_MAX)\n\t\tseq_puts(seq, \"max\\n\");\n\telse\n\t\tseq_printf(seq, \"%d\\n\", depth);\n\n\treturn 0;\n}\n\nstatic ssize_t cgroup_max_depth_write(struct kernfs_open_file *of,\n\t\t\t\t      char *buf, size_t nbytes, loff_t off)\n{\n\tstruct cgroup *cgrp;\n\tssize_t ret;\n\tint depth;\n\n\tbuf = strstrip(buf);\n\tif (!strcmp(buf, \"max\")) {\n\t\tdepth = INT_MAX;\n\t} else {\n\t\tret = kstrtoint(buf, 0, &depth);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (depth < 0)\n\t\treturn -ERANGE;\n\n\tcgrp = cgroup_kn_lock_live(of->kn, false);\n\tif (!cgrp)\n\t\treturn -ENOENT;\n\n\tcgrp->max_depth = depth;\n\n\tcgroup_kn_unlock(of->kn);\n\n\treturn nbytes;\n}\n\nstatic int cgroup_events_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup *cgrp = seq_css(seq)->cgroup;\n\n\tseq_printf(seq, \"populated %d\\n\", cgroup_is_populated(cgrp));\n\tseq_printf(seq, \"frozen %d\\n\", test_bit(CGRP_FROZEN, &cgrp->flags));\n\n\treturn 0;\n}\n\nstatic int cgroup_stat_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup *cgroup = seq_css(seq)->cgroup;\n\n\tseq_printf(seq, \"nr_descendants %d\\n\",\n\t\t   cgroup->nr_descendants);\n\tseq_printf(seq, \"nr_dying_descendants %d\\n\",\n\t\t   cgroup->nr_dying_descendants);\n\n\treturn 0;\n}\n\n#ifdef CONFIG_CGROUP_SCHED\n \nstatic struct cgroup_subsys_state *cgroup_tryget_css(struct cgroup *cgrp,\n\t\t\t\t\t\t     struct cgroup_subsys *ss)\n{\n\tstruct cgroup_subsys_state *css;\n\n\trcu_read_lock();\n\tcss = cgroup_css(cgrp, ss);\n\tif (css && !css_tryget_online(css))\n\t\tcss = NULL;\n\trcu_read_unlock();\n\n\treturn css;\n}\n\nstatic int cgroup_extra_stat_show(struct seq_file *seq, int ssid)\n{\n\tstruct cgroup *cgrp = seq_css(seq)->cgroup;\n\tstruct cgroup_subsys *ss = cgroup_subsys[ssid];\n\tstruct cgroup_subsys_state *css;\n\tint ret;\n\n\tif (!ss->css_extra_stat_show)\n\t\treturn 0;\n\n\tcss = cgroup_tryget_css(cgrp, ss);\n\tif (!css)\n\t\treturn 0;\n\n\tret = ss->css_extra_stat_show(seq, css);\n\tcss_put(css);\n\treturn ret;\n}\n\nstatic int cgroup_local_stat_show(struct seq_file *seq,\n\t\t\t\t  struct cgroup *cgrp, int ssid)\n{\n\tstruct cgroup_subsys *ss = cgroup_subsys[ssid];\n\tstruct cgroup_subsys_state *css;\n\tint ret;\n\n\tif (!ss->css_local_stat_show)\n\t\treturn 0;\n\n\tcss = cgroup_tryget_css(cgrp, ss);\n\tif (!css)\n\t\treturn 0;\n\n\tret = ss->css_local_stat_show(seq, css);\n\tcss_put(css);\n\treturn ret;\n}\n#endif\n\nstatic int cpu_stat_show(struct seq_file *seq, void *v)\n{\n\tint ret = 0;\n\n\tcgroup_base_stat_cputime_show(seq);\n#ifdef CONFIG_CGROUP_SCHED\n\tret = cgroup_extra_stat_show(seq, cpu_cgrp_id);\n#endif\n\treturn ret;\n}\n\nstatic int cpu_local_stat_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup __maybe_unused *cgrp = seq_css(seq)->cgroup;\n\tint ret = 0;\n\n#ifdef CONFIG_CGROUP_SCHED\n\tret = cgroup_local_stat_show(seq, cgrp, cpu_cgrp_id);\n#endif\n\treturn ret;\n}\n\n#ifdef CONFIG_PSI\nstatic int cgroup_io_pressure_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup *cgrp = seq_css(seq)->cgroup;\n\tstruct psi_group *psi = cgroup_psi(cgrp);\n\n\treturn psi_show(seq, psi, PSI_IO);\n}\nstatic int cgroup_memory_pressure_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup *cgrp = seq_css(seq)->cgroup;\n\tstruct psi_group *psi = cgroup_psi(cgrp);\n\n\treturn psi_show(seq, psi, PSI_MEM);\n}\nstatic int cgroup_cpu_pressure_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup *cgrp = seq_css(seq)->cgroup;\n\tstruct psi_group *psi = cgroup_psi(cgrp);\n\n\treturn psi_show(seq, psi, PSI_CPU);\n}\n\nstatic ssize_t pressure_write(struct kernfs_open_file *of, char *buf,\n\t\t\t      size_t nbytes, enum psi_res res)\n{\n\tstruct cgroup_file_ctx *ctx = of->priv;\n\tstruct psi_trigger *new;\n\tstruct cgroup *cgrp;\n\tstruct psi_group *psi;\n\n\tcgrp = cgroup_kn_lock_live(of->kn, false);\n\tif (!cgrp)\n\t\treturn -ENODEV;\n\n\tcgroup_get(cgrp);\n\tcgroup_kn_unlock(of->kn);\n\n\t \n\tif (ctx->psi.trigger) {\n\t\tcgroup_put(cgrp);\n\t\treturn -EBUSY;\n\t}\n\n\tpsi = cgroup_psi(cgrp);\n\tnew = psi_trigger_create(psi, buf, res, of->file, of);\n\tif (IS_ERR(new)) {\n\t\tcgroup_put(cgrp);\n\t\treturn PTR_ERR(new);\n\t}\n\n\tsmp_store_release(&ctx->psi.trigger, new);\n\tcgroup_put(cgrp);\n\n\treturn nbytes;\n}\n\nstatic ssize_t cgroup_io_pressure_write(struct kernfs_open_file *of,\n\t\t\t\t\t  char *buf, size_t nbytes,\n\t\t\t\t\t  loff_t off)\n{\n\treturn pressure_write(of, buf, nbytes, PSI_IO);\n}\n\nstatic ssize_t cgroup_memory_pressure_write(struct kernfs_open_file *of,\n\t\t\t\t\t  char *buf, size_t nbytes,\n\t\t\t\t\t  loff_t off)\n{\n\treturn pressure_write(of, buf, nbytes, PSI_MEM);\n}\n\nstatic ssize_t cgroup_cpu_pressure_write(struct kernfs_open_file *of,\n\t\t\t\t\t  char *buf, size_t nbytes,\n\t\t\t\t\t  loff_t off)\n{\n\treturn pressure_write(of, buf, nbytes, PSI_CPU);\n}\n\n#ifdef CONFIG_IRQ_TIME_ACCOUNTING\nstatic int cgroup_irq_pressure_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup *cgrp = seq_css(seq)->cgroup;\n\tstruct psi_group *psi = cgroup_psi(cgrp);\n\n\treturn psi_show(seq, psi, PSI_IRQ);\n}\n\nstatic ssize_t cgroup_irq_pressure_write(struct kernfs_open_file *of,\n\t\t\t\t\t char *buf, size_t nbytes,\n\t\t\t\t\t loff_t off)\n{\n\treturn pressure_write(of, buf, nbytes, PSI_IRQ);\n}\n#endif\n\nstatic int cgroup_pressure_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup *cgrp = seq_css(seq)->cgroup;\n\tstruct psi_group *psi = cgroup_psi(cgrp);\n\n\tseq_printf(seq, \"%d\\n\", psi->enabled);\n\n\treturn 0;\n}\n\nstatic ssize_t cgroup_pressure_write(struct kernfs_open_file *of,\n\t\t\t\t     char *buf, size_t nbytes,\n\t\t\t\t     loff_t off)\n{\n\tssize_t ret;\n\tint enable;\n\tstruct cgroup *cgrp;\n\tstruct psi_group *psi;\n\n\tret = kstrtoint(strstrip(buf), 0, &enable);\n\tif (ret)\n\t\treturn ret;\n\n\tif (enable < 0 || enable > 1)\n\t\treturn -ERANGE;\n\n\tcgrp = cgroup_kn_lock_live(of->kn, false);\n\tif (!cgrp)\n\t\treturn -ENOENT;\n\n\tpsi = cgroup_psi(cgrp);\n\tif (psi->enabled != enable) {\n\t\tint i;\n\n\t\t \n\t\tfor (i = 0; i < NR_PSI_RESOURCES; i++)\n\t\t\tcgroup_file_show(&cgrp->psi_files[i], enable);\n\n\t\tpsi->enabled = enable;\n\t\tif (enable)\n\t\t\tpsi_cgroup_restart(psi);\n\t}\n\n\tcgroup_kn_unlock(of->kn);\n\n\treturn nbytes;\n}\n\nstatic __poll_t cgroup_pressure_poll(struct kernfs_open_file *of,\n\t\t\t\t\t  poll_table *pt)\n{\n\tstruct cgroup_file_ctx *ctx = of->priv;\n\n\treturn psi_trigger_poll(&ctx->psi.trigger, of->file, pt);\n}\n\nstatic void cgroup_pressure_release(struct kernfs_open_file *of)\n{\n\tstruct cgroup_file_ctx *ctx = of->priv;\n\n\tpsi_trigger_destroy(ctx->psi.trigger);\n}\n\nbool cgroup_psi_enabled(void)\n{\n\tif (static_branch_likely(&psi_disabled))\n\t\treturn false;\n\n\treturn (cgroup_feature_disable_mask & (1 << OPT_FEATURE_PRESSURE)) == 0;\n}\n\n#else  \nbool cgroup_psi_enabled(void)\n{\n\treturn false;\n}\n\n#endif  \n\nstatic int cgroup_freeze_show(struct seq_file *seq, void *v)\n{\n\tstruct cgroup *cgrp = seq_css(seq)->cgroup;\n\n\tseq_printf(seq, \"%d\\n\", cgrp->freezer.freeze);\n\n\treturn 0;\n}\n\nstatic ssize_t cgroup_freeze_write(struct kernfs_open_file *of,\n\t\t\t\t   char *buf, size_t nbytes, loff_t off)\n{\n\tstruct cgroup *cgrp;\n\tssize_t ret;\n\tint freeze;\n\n\tret = kstrtoint(strstrip(buf), 0, &freeze);\n\tif (ret)\n\t\treturn ret;\n\n\tif (freeze < 0 || freeze > 1)\n\t\treturn -ERANGE;\n\n\tcgrp = cgroup_kn_lock_live(of->kn, false);\n\tif (!cgrp)\n\t\treturn -ENOENT;\n\n\tcgroup_freeze(cgrp, freeze);\n\n\tcgroup_kn_unlock(of->kn);\n\n\treturn nbytes;\n}\n\nstatic void __cgroup_kill(struct cgroup *cgrp)\n{\n\tstruct css_task_iter it;\n\tstruct task_struct *task;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tspin_lock_irq(&css_set_lock);\n\tset_bit(CGRP_KILL, &cgrp->flags);\n\tspin_unlock_irq(&css_set_lock);\n\n\tcss_task_iter_start(&cgrp->self, CSS_TASK_ITER_PROCS | CSS_TASK_ITER_THREADED, &it);\n\twhile ((task = css_task_iter_next(&it))) {\n\t\t \n\t\tif (task->flags & PF_KTHREAD)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (__fatal_signal_pending(task))\n\t\t\tcontinue;\n\n\t\tsend_sig(SIGKILL, task, 0);\n\t}\n\tcss_task_iter_end(&it);\n\n\tspin_lock_irq(&css_set_lock);\n\tclear_bit(CGRP_KILL, &cgrp->flags);\n\tspin_unlock_irq(&css_set_lock);\n}\n\nstatic void cgroup_kill(struct cgroup *cgrp)\n{\n\tstruct cgroup_subsys_state *css;\n\tstruct cgroup *dsct;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tcgroup_for_each_live_descendant_pre(dsct, css, cgrp)\n\t\t__cgroup_kill(dsct);\n}\n\nstatic ssize_t cgroup_kill_write(struct kernfs_open_file *of, char *buf,\n\t\t\t\t size_t nbytes, loff_t off)\n{\n\tssize_t ret = 0;\n\tint kill;\n\tstruct cgroup *cgrp;\n\n\tret = kstrtoint(strstrip(buf), 0, &kill);\n\tif (ret)\n\t\treturn ret;\n\n\tif (kill != 1)\n\t\treturn -ERANGE;\n\n\tcgrp = cgroup_kn_lock_live(of->kn, false);\n\tif (!cgrp)\n\t\treturn -ENOENT;\n\n\t \n\tif (cgroup_is_threaded(cgrp))\n\t\tret = -EOPNOTSUPP;\n\telse\n\t\tcgroup_kill(cgrp);\n\n\tcgroup_kn_unlock(of->kn);\n\n\treturn ret ?: nbytes;\n}\n\nstatic int cgroup_file_open(struct kernfs_open_file *of)\n{\n\tstruct cftype *cft = of_cft(of);\n\tstruct cgroup_file_ctx *ctx;\n\tint ret;\n\n\tctx = kzalloc(sizeof(*ctx), GFP_KERNEL);\n\tif (!ctx)\n\t\treturn -ENOMEM;\n\n\tctx->ns = current->nsproxy->cgroup_ns;\n\tget_cgroup_ns(ctx->ns);\n\tof->priv = ctx;\n\n\tif (!cft->open)\n\t\treturn 0;\n\n\tret = cft->open(of);\n\tif (ret) {\n\t\tput_cgroup_ns(ctx->ns);\n\t\tkfree(ctx);\n\t}\n\treturn ret;\n}\n\nstatic void cgroup_file_release(struct kernfs_open_file *of)\n{\n\tstruct cftype *cft = of_cft(of);\n\tstruct cgroup_file_ctx *ctx = of->priv;\n\n\tif (cft->release)\n\t\tcft->release(of);\n\tput_cgroup_ns(ctx->ns);\n\tkfree(ctx);\n}\n\nstatic ssize_t cgroup_file_write(struct kernfs_open_file *of, char *buf,\n\t\t\t\t size_t nbytes, loff_t off)\n{\n\tstruct cgroup_file_ctx *ctx = of->priv;\n\tstruct cgroup *cgrp = of->kn->parent->priv;\n\tstruct cftype *cft = of_cft(of);\n\tstruct cgroup_subsys_state *css;\n\tint ret;\n\n\tif (!nbytes)\n\t\treturn 0;\n\n\t \n\tif ((cgrp->root->flags & CGRP_ROOT_NS_DELEGATE) &&\n\t    !(cft->flags & CFTYPE_NS_DELEGATABLE) &&\n\t    ctx->ns != &init_cgroup_ns && ctx->ns->root_cset->dfl_cgrp == cgrp)\n\t\treturn -EPERM;\n\n\tif (cft->write)\n\t\treturn cft->write(of, buf, nbytes, off);\n\n\t \n\trcu_read_lock();\n\tcss = cgroup_css(cgrp, cft->ss);\n\trcu_read_unlock();\n\n\tif (cft->write_u64) {\n\t\tunsigned long long v;\n\t\tret = kstrtoull(buf, 0, &v);\n\t\tif (!ret)\n\t\t\tret = cft->write_u64(css, cft, v);\n\t} else if (cft->write_s64) {\n\t\tlong long v;\n\t\tret = kstrtoll(buf, 0, &v);\n\t\tif (!ret)\n\t\t\tret = cft->write_s64(css, cft, v);\n\t} else {\n\t\tret = -EINVAL;\n\t}\n\n\treturn ret ?: nbytes;\n}\n\nstatic __poll_t cgroup_file_poll(struct kernfs_open_file *of, poll_table *pt)\n{\n\tstruct cftype *cft = of_cft(of);\n\n\tif (cft->poll)\n\t\treturn cft->poll(of, pt);\n\n\treturn kernfs_generic_poll(of, pt);\n}\n\nstatic void *cgroup_seqfile_start(struct seq_file *seq, loff_t *ppos)\n{\n\treturn seq_cft(seq)->seq_start(seq, ppos);\n}\n\nstatic void *cgroup_seqfile_next(struct seq_file *seq, void *v, loff_t *ppos)\n{\n\treturn seq_cft(seq)->seq_next(seq, v, ppos);\n}\n\nstatic void cgroup_seqfile_stop(struct seq_file *seq, void *v)\n{\n\tif (seq_cft(seq)->seq_stop)\n\t\tseq_cft(seq)->seq_stop(seq, v);\n}\n\nstatic int cgroup_seqfile_show(struct seq_file *m, void *arg)\n{\n\tstruct cftype *cft = seq_cft(m);\n\tstruct cgroup_subsys_state *css = seq_css(m);\n\n\tif (cft->seq_show)\n\t\treturn cft->seq_show(m, arg);\n\n\tif (cft->read_u64)\n\t\tseq_printf(m, \"%llu\\n\", cft->read_u64(css, cft));\n\telse if (cft->read_s64)\n\t\tseq_printf(m, \"%lld\\n\", cft->read_s64(css, cft));\n\telse\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\nstatic struct kernfs_ops cgroup_kf_single_ops = {\n\t.atomic_write_len\t= PAGE_SIZE,\n\t.open\t\t\t= cgroup_file_open,\n\t.release\t\t= cgroup_file_release,\n\t.write\t\t\t= cgroup_file_write,\n\t.poll\t\t\t= cgroup_file_poll,\n\t.seq_show\t\t= cgroup_seqfile_show,\n};\n\nstatic struct kernfs_ops cgroup_kf_ops = {\n\t.atomic_write_len\t= PAGE_SIZE,\n\t.open\t\t\t= cgroup_file_open,\n\t.release\t\t= cgroup_file_release,\n\t.write\t\t\t= cgroup_file_write,\n\t.poll\t\t\t= cgroup_file_poll,\n\t.seq_start\t\t= cgroup_seqfile_start,\n\t.seq_next\t\t= cgroup_seqfile_next,\n\t.seq_stop\t\t= cgroup_seqfile_stop,\n\t.seq_show\t\t= cgroup_seqfile_show,\n};\n\n \nstatic int cgroup_kn_set_ugid(struct kernfs_node *kn)\n{\n\tstruct iattr iattr = { .ia_valid = ATTR_UID | ATTR_GID,\n\t\t\t       .ia_uid = current_fsuid(),\n\t\t\t       .ia_gid = current_fsgid(), };\n\n\tif (uid_eq(iattr.ia_uid, GLOBAL_ROOT_UID) &&\n\t    gid_eq(iattr.ia_gid, GLOBAL_ROOT_GID))\n\t\treturn 0;\n\n\treturn kernfs_setattr(kn, &iattr);\n}\n\nstatic void cgroup_file_notify_timer(struct timer_list *timer)\n{\n\tcgroup_file_notify(container_of(timer, struct cgroup_file,\n\t\t\t\t\tnotify_timer));\n}\n\nstatic int cgroup_add_file(struct cgroup_subsys_state *css, struct cgroup *cgrp,\n\t\t\t   struct cftype *cft)\n{\n\tchar name[CGROUP_FILE_NAME_MAX];\n\tstruct kernfs_node *kn;\n\tstruct lock_class_key *key = NULL;\n\tint ret;\n\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n\tkey = &cft->lockdep_key;\n#endif\n\tkn = __kernfs_create_file(cgrp->kn, cgroup_file_name(cgrp, cft, name),\n\t\t\t\t  cgroup_file_mode(cft),\n\t\t\t\t  GLOBAL_ROOT_UID, GLOBAL_ROOT_GID,\n\t\t\t\t  0, cft->kf_ops, cft,\n\t\t\t\t  NULL, key);\n\tif (IS_ERR(kn))\n\t\treturn PTR_ERR(kn);\n\n\tret = cgroup_kn_set_ugid(kn);\n\tif (ret) {\n\t\tkernfs_remove(kn);\n\t\treturn ret;\n\t}\n\n\tif (cft->file_offset) {\n\t\tstruct cgroup_file *cfile = (void *)css + cft->file_offset;\n\n\t\ttimer_setup(&cfile->notify_timer, cgroup_file_notify_timer, 0);\n\n\t\tspin_lock_irq(&cgroup_file_kn_lock);\n\t\tcfile->kn = kn;\n\t\tspin_unlock_irq(&cgroup_file_kn_lock);\n\t}\n\n\treturn 0;\n}\n\n \nstatic int cgroup_addrm_files(struct cgroup_subsys_state *css,\n\t\t\t      struct cgroup *cgrp, struct cftype cfts[],\n\t\t\t      bool is_add)\n{\n\tstruct cftype *cft, *cft_end = NULL;\n\tint ret = 0;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\nrestart:\n\tfor (cft = cfts; cft != cft_end && cft->name[0] != '\\0'; cft++) {\n\t\t \n\t\tif ((cft->flags & __CFTYPE_ONLY_ON_DFL) && !cgroup_on_dfl(cgrp))\n\t\t\tcontinue;\n\t\tif ((cft->flags & __CFTYPE_NOT_ON_DFL) && cgroup_on_dfl(cgrp))\n\t\t\tcontinue;\n\t\tif ((cft->flags & CFTYPE_NOT_ON_ROOT) && !cgroup_parent(cgrp))\n\t\t\tcontinue;\n\t\tif ((cft->flags & CFTYPE_ONLY_ON_ROOT) && cgroup_parent(cgrp))\n\t\t\tcontinue;\n\t\tif ((cft->flags & CFTYPE_DEBUG) && !cgroup_debug)\n\t\t\tcontinue;\n\t\tif (is_add) {\n\t\t\tret = cgroup_add_file(css, cgrp, cft);\n\t\t\tif (ret) {\n\t\t\t\tpr_warn(\"%s: failed to add %s, err=%d\\n\",\n\t\t\t\t\t__func__, cft->name, ret);\n\t\t\t\tcft_end = cft;\n\t\t\t\tis_add = false;\n\t\t\t\tgoto restart;\n\t\t\t}\n\t\t} else {\n\t\t\tcgroup_rm_file(cgrp, cft);\n\t\t}\n\t}\n\treturn ret;\n}\n\nstatic int cgroup_apply_cftypes(struct cftype *cfts, bool is_add)\n{\n\tstruct cgroup_subsys *ss = cfts[0].ss;\n\tstruct cgroup *root = &ss->root->cgrp;\n\tstruct cgroup_subsys_state *css;\n\tint ret = 0;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\t \n\tcss_for_each_descendant_pre(css, cgroup_css(root, ss)) {\n\t\tstruct cgroup *cgrp = css->cgroup;\n\n\t\tif (!(css->flags & CSS_VISIBLE))\n\t\t\tcontinue;\n\n\t\tret = cgroup_addrm_files(css, cgrp, cfts, is_add);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\n\tif (is_add && !ret)\n\t\tkernfs_activate(root->kn);\n\treturn ret;\n}\n\nstatic void cgroup_exit_cftypes(struct cftype *cfts)\n{\n\tstruct cftype *cft;\n\n\tfor (cft = cfts; cft->name[0] != '\\0'; cft++) {\n\t\t \n\t\tif (cft->max_write_len && cft->max_write_len != PAGE_SIZE)\n\t\t\tkfree(cft->kf_ops);\n\t\tcft->kf_ops = NULL;\n\t\tcft->ss = NULL;\n\n\t\t \n\t\tcft->flags &= ~(__CFTYPE_ONLY_ON_DFL | __CFTYPE_NOT_ON_DFL |\n\t\t\t\t__CFTYPE_ADDED);\n\t}\n}\n\nstatic int cgroup_init_cftypes(struct cgroup_subsys *ss, struct cftype *cfts)\n{\n\tstruct cftype *cft;\n\tint ret = 0;\n\n\tfor (cft = cfts; cft->name[0] != '\\0'; cft++) {\n\t\tstruct kernfs_ops *kf_ops;\n\n\t\tWARN_ON(cft->ss || cft->kf_ops);\n\n\t\tif (cft->flags & __CFTYPE_ADDED) {\n\t\t\tret = -EBUSY;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (cft->seq_start)\n\t\t\tkf_ops = &cgroup_kf_ops;\n\t\telse\n\t\t\tkf_ops = &cgroup_kf_single_ops;\n\n\t\t \n\t\tif (cft->max_write_len && cft->max_write_len != PAGE_SIZE) {\n\t\t\tkf_ops = kmemdup(kf_ops, sizeof(*kf_ops), GFP_KERNEL);\n\t\t\tif (!kf_ops) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tkf_ops->atomic_write_len = cft->max_write_len;\n\t\t}\n\n\t\tcft->kf_ops = kf_ops;\n\t\tcft->ss = ss;\n\t\tcft->flags |= __CFTYPE_ADDED;\n\t}\n\n\tif (ret)\n\t\tcgroup_exit_cftypes(cfts);\n\treturn ret;\n}\n\nstatic void cgroup_rm_cftypes_locked(struct cftype *cfts)\n{\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tlist_del(&cfts->node);\n\tcgroup_apply_cftypes(cfts, false);\n\tcgroup_exit_cftypes(cfts);\n}\n\n \nint cgroup_rm_cftypes(struct cftype *cfts)\n{\n\tif (!cfts || cfts[0].name[0] == '\\0')\n\t\treturn 0;\n\n\tif (!(cfts[0].flags & __CFTYPE_ADDED))\n\t\treturn -ENOENT;\n\n\tcgroup_lock();\n\tcgroup_rm_cftypes_locked(cfts);\n\tcgroup_unlock();\n\treturn 0;\n}\n\n \nstatic int cgroup_add_cftypes(struct cgroup_subsys *ss, struct cftype *cfts)\n{\n\tint ret;\n\n\tif (!cgroup_ssid_enabled(ss->id))\n\t\treturn 0;\n\n\tif (!cfts || cfts[0].name[0] == '\\0')\n\t\treturn 0;\n\n\tret = cgroup_init_cftypes(ss, cfts);\n\tif (ret)\n\t\treturn ret;\n\n\tcgroup_lock();\n\n\tlist_add_tail(&cfts->node, &ss->cfts);\n\tret = cgroup_apply_cftypes(cfts, true);\n\tif (ret)\n\t\tcgroup_rm_cftypes_locked(cfts);\n\n\tcgroup_unlock();\n\treturn ret;\n}\n\n \nint cgroup_add_dfl_cftypes(struct cgroup_subsys *ss, struct cftype *cfts)\n{\n\tstruct cftype *cft;\n\n\tfor (cft = cfts; cft && cft->name[0] != '\\0'; cft++)\n\t\tcft->flags |= __CFTYPE_ONLY_ON_DFL;\n\treturn cgroup_add_cftypes(ss, cfts);\n}\n\n \nint cgroup_add_legacy_cftypes(struct cgroup_subsys *ss, struct cftype *cfts)\n{\n\tstruct cftype *cft;\n\n\tfor (cft = cfts; cft && cft->name[0] != '\\0'; cft++)\n\t\tcft->flags |= __CFTYPE_NOT_ON_DFL;\n\treturn cgroup_add_cftypes(ss, cfts);\n}\n\n \nvoid cgroup_file_notify(struct cgroup_file *cfile)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&cgroup_file_kn_lock, flags);\n\tif (cfile->kn) {\n\t\tunsigned long last = cfile->notified_at;\n\t\tunsigned long next = last + CGROUP_FILE_NOTIFY_MIN_INTV;\n\n\t\tif (time_in_range(jiffies, last, next)) {\n\t\t\ttimer_reduce(&cfile->notify_timer, next);\n\t\t} else {\n\t\t\tkernfs_notify(cfile->kn);\n\t\t\tcfile->notified_at = jiffies;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&cgroup_file_kn_lock, flags);\n}\n\n \nvoid cgroup_file_show(struct cgroup_file *cfile, bool show)\n{\n\tstruct kernfs_node *kn;\n\n\tspin_lock_irq(&cgroup_file_kn_lock);\n\tkn = cfile->kn;\n\tkernfs_get(kn);\n\tspin_unlock_irq(&cgroup_file_kn_lock);\n\n\tif (kn)\n\t\tkernfs_show(kn, show);\n\n\tkernfs_put(kn);\n}\n\n \nstruct cgroup_subsys_state *css_next_child(struct cgroup_subsys_state *pos,\n\t\t\t\t\t   struct cgroup_subsys_state *parent)\n{\n\tstruct cgroup_subsys_state *next;\n\n\tcgroup_assert_mutex_or_rcu_locked();\n\n\t \n\tif (!pos) {\n\t\tnext = list_entry_rcu(parent->children.next, struct cgroup_subsys_state, sibling);\n\t} else if (likely(!(pos->flags & CSS_RELEASED))) {\n\t\tnext = list_entry_rcu(pos->sibling.next, struct cgroup_subsys_state, sibling);\n\t} else {\n\t\tlist_for_each_entry_rcu(next, &parent->children, sibling,\n\t\t\t\t\tlockdep_is_held(&cgroup_mutex))\n\t\t\tif (next->serial_nr > pos->serial_nr)\n\t\t\t\tbreak;\n\t}\n\n\t \n\tif (&next->sibling != &parent->children)\n\t\treturn next;\n\treturn NULL;\n}\n\n \nstruct cgroup_subsys_state *\ncss_next_descendant_pre(struct cgroup_subsys_state *pos,\n\t\t\tstruct cgroup_subsys_state *root)\n{\n\tstruct cgroup_subsys_state *next;\n\n\tcgroup_assert_mutex_or_rcu_locked();\n\n\t \n\tif (!pos)\n\t\treturn root;\n\n\t \n\tnext = css_next_child(NULL, pos);\n\tif (next)\n\t\treturn next;\n\n\t \n\twhile (pos != root) {\n\t\tnext = css_next_child(pos, pos->parent);\n\t\tif (next)\n\t\t\treturn next;\n\t\tpos = pos->parent;\n\t}\n\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(css_next_descendant_pre);\n\n \nstruct cgroup_subsys_state *\ncss_rightmost_descendant(struct cgroup_subsys_state *pos)\n{\n\tstruct cgroup_subsys_state *last, *tmp;\n\n\tcgroup_assert_mutex_or_rcu_locked();\n\n\tdo {\n\t\tlast = pos;\n\t\t \n\t\tpos = NULL;\n\t\tcss_for_each_child(tmp, last)\n\t\t\tpos = tmp;\n\t} while (pos);\n\n\treturn last;\n}\n\nstatic struct cgroup_subsys_state *\ncss_leftmost_descendant(struct cgroup_subsys_state *pos)\n{\n\tstruct cgroup_subsys_state *last;\n\n\tdo {\n\t\tlast = pos;\n\t\tpos = css_next_child(NULL, pos);\n\t} while (pos);\n\n\treturn last;\n}\n\n \nstruct cgroup_subsys_state *\ncss_next_descendant_post(struct cgroup_subsys_state *pos,\n\t\t\t struct cgroup_subsys_state *root)\n{\n\tstruct cgroup_subsys_state *next;\n\n\tcgroup_assert_mutex_or_rcu_locked();\n\n\t \n\tif (!pos)\n\t\treturn css_leftmost_descendant(root);\n\n\t \n\tif (pos == root)\n\t\treturn NULL;\n\n\t \n\tnext = css_next_child(pos, pos->parent);\n\tif (next)\n\t\treturn css_leftmost_descendant(next);\n\n\t \n\treturn pos->parent;\n}\n\n \nbool css_has_online_children(struct cgroup_subsys_state *css)\n{\n\tstruct cgroup_subsys_state *child;\n\tbool ret = false;\n\n\trcu_read_lock();\n\tcss_for_each_child(child, css) {\n\t\tif (child->flags & CSS_ONLINE) {\n\t\t\tret = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn ret;\n}\n\nstatic struct css_set *css_task_iter_next_css_set(struct css_task_iter *it)\n{\n\tstruct list_head *l;\n\tstruct cgrp_cset_link *link;\n\tstruct css_set *cset;\n\n\tlockdep_assert_held(&css_set_lock);\n\n\t \n\tif (it->tcset_pos) {\n\t\tl = it->tcset_pos->next;\n\n\t\tif (l != it->tcset_head) {\n\t\t\tit->tcset_pos = l;\n\t\t\treturn container_of(l, struct css_set,\n\t\t\t\t\t    threaded_csets_node);\n\t\t}\n\n\t\tit->tcset_pos = NULL;\n\t}\n\n\t \n\tl = it->cset_pos;\n\tl = l->next;\n\tif (l == it->cset_head) {\n\t\tit->cset_pos = NULL;\n\t\treturn NULL;\n\t}\n\n\tif (it->ss) {\n\t\tcset = container_of(l, struct css_set, e_cset_node[it->ss->id]);\n\t} else {\n\t\tlink = list_entry(l, struct cgrp_cset_link, cset_link);\n\t\tcset = link->cset;\n\t}\n\n\tit->cset_pos = l;\n\n\t \n\tif (it->flags & CSS_TASK_ITER_THREADED) {\n\t\tif (it->cur_dcset)\n\t\t\tput_css_set_locked(it->cur_dcset);\n\t\tit->cur_dcset = cset;\n\t\tget_css_set(cset);\n\n\t\tit->tcset_head = &cset->threaded_csets;\n\t\tit->tcset_pos = &cset->threaded_csets;\n\t}\n\n\treturn cset;\n}\n\n \nstatic void css_task_iter_advance_css_set(struct css_task_iter *it)\n{\n\tstruct css_set *cset;\n\n\tlockdep_assert_held(&css_set_lock);\n\n\t \n\twhile ((cset = css_task_iter_next_css_set(it))) {\n\t\tif (!list_empty(&cset->tasks)) {\n\t\t\tit->cur_tasks_head = &cset->tasks;\n\t\t\tbreak;\n\t\t} else if (!list_empty(&cset->mg_tasks)) {\n\t\t\tit->cur_tasks_head = &cset->mg_tasks;\n\t\t\tbreak;\n\t\t} else if (!list_empty(&cset->dying_tasks)) {\n\t\t\tit->cur_tasks_head = &cset->dying_tasks;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!cset) {\n\t\tit->task_pos = NULL;\n\t\treturn;\n\t}\n\tit->task_pos = it->cur_tasks_head->next;\n\n\t \n\tif (it->cur_cset) {\n\t\tlist_del(&it->iters_node);\n\t\tput_css_set_locked(it->cur_cset);\n\t}\n\tget_css_set(cset);\n\tit->cur_cset = cset;\n\tlist_add(&it->iters_node, &cset->task_iters);\n}\n\nstatic void css_task_iter_skip(struct css_task_iter *it,\n\t\t\t       struct task_struct *task)\n{\n\tlockdep_assert_held(&css_set_lock);\n\n\tif (it->task_pos == &task->cg_list) {\n\t\tit->task_pos = it->task_pos->next;\n\t\tit->flags |= CSS_TASK_ITER_SKIPPED;\n\t}\n}\n\nstatic void css_task_iter_advance(struct css_task_iter *it)\n{\n\tstruct task_struct *task;\n\n\tlockdep_assert_held(&css_set_lock);\nrepeat:\n\tif (it->task_pos) {\n\t\t \n\t\tif (it->flags & CSS_TASK_ITER_SKIPPED)\n\t\t\tit->flags &= ~CSS_TASK_ITER_SKIPPED;\n\t\telse\n\t\t\tit->task_pos = it->task_pos->next;\n\n\t\tif (it->task_pos == &it->cur_cset->tasks) {\n\t\t\tit->cur_tasks_head = &it->cur_cset->mg_tasks;\n\t\t\tit->task_pos = it->cur_tasks_head->next;\n\t\t}\n\t\tif (it->task_pos == &it->cur_cset->mg_tasks) {\n\t\t\tit->cur_tasks_head = &it->cur_cset->dying_tasks;\n\t\t\tit->task_pos = it->cur_tasks_head->next;\n\t\t}\n\t\tif (it->task_pos == &it->cur_cset->dying_tasks)\n\t\t\tcss_task_iter_advance_css_set(it);\n\t} else {\n\t\t \n\t\tcss_task_iter_advance_css_set(it);\n\t}\n\n\tif (!it->task_pos)\n\t\treturn;\n\n\ttask = list_entry(it->task_pos, struct task_struct, cg_list);\n\n\tif (it->flags & CSS_TASK_ITER_PROCS) {\n\t\t \n\t\tif (!thread_group_leader(task))\n\t\t\tgoto repeat;\n\n\t\t \n\t\tif (it->cur_tasks_head == &it->cur_cset->dying_tasks &&\n\t\t    !atomic_read(&task->signal->live))\n\t\t\tgoto repeat;\n\t} else {\n\t\t \n\t\tif (it->cur_tasks_head == &it->cur_cset->dying_tasks)\n\t\t\tgoto repeat;\n\t}\n}\n\n \nvoid css_task_iter_start(struct cgroup_subsys_state *css, unsigned int flags,\n\t\t\t struct css_task_iter *it)\n{\n\tmemset(it, 0, sizeof(*it));\n\n\tspin_lock_irq(&css_set_lock);\n\n\tit->ss = css->ss;\n\tit->flags = flags;\n\n\tif (CGROUP_HAS_SUBSYS_CONFIG && it->ss)\n\t\tit->cset_pos = &css->cgroup->e_csets[css->ss->id];\n\telse\n\t\tit->cset_pos = &css->cgroup->cset_links;\n\n\tit->cset_head = it->cset_pos;\n\n\tcss_task_iter_advance(it);\n\n\tspin_unlock_irq(&css_set_lock);\n}\n\n \nstruct task_struct *css_task_iter_next(struct css_task_iter *it)\n{\n\tif (it->cur_task) {\n\t\tput_task_struct(it->cur_task);\n\t\tit->cur_task = NULL;\n\t}\n\n\tspin_lock_irq(&css_set_lock);\n\n\t \n\tif (it->flags & CSS_TASK_ITER_SKIPPED)\n\t\tcss_task_iter_advance(it);\n\n\tif (it->task_pos) {\n\t\tit->cur_task = list_entry(it->task_pos, struct task_struct,\n\t\t\t\t\t  cg_list);\n\t\tget_task_struct(it->cur_task);\n\t\tcss_task_iter_advance(it);\n\t}\n\n\tspin_unlock_irq(&css_set_lock);\n\n\treturn it->cur_task;\n}\n\n \nvoid css_task_iter_end(struct css_task_iter *it)\n{\n\tif (it->cur_cset) {\n\t\tspin_lock_irq(&css_set_lock);\n\t\tlist_del(&it->iters_node);\n\t\tput_css_set_locked(it->cur_cset);\n\t\tspin_unlock_irq(&css_set_lock);\n\t}\n\n\tif (it->cur_dcset)\n\t\tput_css_set(it->cur_dcset);\n\n\tif (it->cur_task)\n\t\tput_task_struct(it->cur_task);\n}\n\nstatic void cgroup_procs_release(struct kernfs_open_file *of)\n{\n\tstruct cgroup_file_ctx *ctx = of->priv;\n\n\tif (ctx->procs.started)\n\t\tcss_task_iter_end(&ctx->procs.iter);\n}\n\nstatic void *cgroup_procs_next(struct seq_file *s, void *v, loff_t *pos)\n{\n\tstruct kernfs_open_file *of = s->private;\n\tstruct cgroup_file_ctx *ctx = of->priv;\n\n\tif (pos)\n\t\t(*pos)++;\n\n\treturn css_task_iter_next(&ctx->procs.iter);\n}\n\nstatic void *__cgroup_procs_start(struct seq_file *s, loff_t *pos,\n\t\t\t\t  unsigned int iter_flags)\n{\n\tstruct kernfs_open_file *of = s->private;\n\tstruct cgroup *cgrp = seq_css(s)->cgroup;\n\tstruct cgroup_file_ctx *ctx = of->priv;\n\tstruct css_task_iter *it = &ctx->procs.iter;\n\n\t \n\tif (!ctx->procs.started) {\n\t\tif (WARN_ON_ONCE((*pos)))\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\tcss_task_iter_start(&cgrp->self, iter_flags, it);\n\t\tctx->procs.started = true;\n\t} else if (!(*pos)) {\n\t\tcss_task_iter_end(it);\n\t\tcss_task_iter_start(&cgrp->self, iter_flags, it);\n\t} else\n\t\treturn it->cur_task;\n\n\treturn cgroup_procs_next(s, NULL, NULL);\n}\n\nstatic void *cgroup_procs_start(struct seq_file *s, loff_t *pos)\n{\n\tstruct cgroup *cgrp = seq_css(s)->cgroup;\n\n\t \n\tif (cgroup_is_threaded(cgrp))\n\t\treturn ERR_PTR(-EOPNOTSUPP);\n\n\treturn __cgroup_procs_start(s, pos, CSS_TASK_ITER_PROCS |\n\t\t\t\t\t    CSS_TASK_ITER_THREADED);\n}\n\nstatic int cgroup_procs_show(struct seq_file *s, void *v)\n{\n\tseq_printf(s, \"%d\\n\", task_pid_vnr(v));\n\treturn 0;\n}\n\nstatic int cgroup_may_write(const struct cgroup *cgrp, struct super_block *sb)\n{\n\tint ret;\n\tstruct inode *inode;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tinode = kernfs_get_inode(sb, cgrp->procs_file.kn);\n\tif (!inode)\n\t\treturn -ENOMEM;\n\n\tret = inode_permission(&nop_mnt_idmap, inode, MAY_WRITE);\n\tiput(inode);\n\treturn ret;\n}\n\nstatic int cgroup_procs_write_permission(struct cgroup *src_cgrp,\n\t\t\t\t\t struct cgroup *dst_cgrp,\n\t\t\t\t\t struct super_block *sb,\n\t\t\t\t\t struct cgroup_namespace *ns)\n{\n\tstruct cgroup *com_cgrp = src_cgrp;\n\tint ret;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\t \n\twhile (!cgroup_is_descendant(dst_cgrp, com_cgrp))\n\t\tcom_cgrp = cgroup_parent(com_cgrp);\n\n\t \n\tret = cgroup_may_write(com_cgrp, sb);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tif ((cgrp_dfl_root.flags & CGRP_ROOT_NS_DELEGATE) &&\n\t    (!cgroup_is_descendant(src_cgrp, ns->root_cset->dfl_cgrp) ||\n\t     !cgroup_is_descendant(dst_cgrp, ns->root_cset->dfl_cgrp)))\n\t\treturn -ENOENT;\n\n\treturn 0;\n}\n\nstatic int cgroup_attach_permissions(struct cgroup *src_cgrp,\n\t\t\t\t     struct cgroup *dst_cgrp,\n\t\t\t\t     struct super_block *sb, bool threadgroup,\n\t\t\t\t     struct cgroup_namespace *ns)\n{\n\tint ret = 0;\n\n\tret = cgroup_procs_write_permission(src_cgrp, dst_cgrp, sb, ns);\n\tif (ret)\n\t\treturn ret;\n\n\tret = cgroup_migrate_vet_dst(dst_cgrp);\n\tif (ret)\n\t\treturn ret;\n\n\tif (!threadgroup && (src_cgrp->dom_cgrp != dst_cgrp->dom_cgrp))\n\t\tret = -EOPNOTSUPP;\n\n\treturn ret;\n}\n\nstatic ssize_t __cgroup_procs_write(struct kernfs_open_file *of, char *buf,\n\t\t\t\t    bool threadgroup)\n{\n\tstruct cgroup_file_ctx *ctx = of->priv;\n\tstruct cgroup *src_cgrp, *dst_cgrp;\n\tstruct task_struct *task;\n\tconst struct cred *saved_cred;\n\tssize_t ret;\n\tbool threadgroup_locked;\n\n\tdst_cgrp = cgroup_kn_lock_live(of->kn, false);\n\tif (!dst_cgrp)\n\t\treturn -ENODEV;\n\n\ttask = cgroup_procs_write_start(buf, threadgroup, &threadgroup_locked);\n\tret = PTR_ERR_OR_ZERO(task);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\t \n\tspin_lock_irq(&css_set_lock);\n\tsrc_cgrp = task_cgroup_from_root(task, &cgrp_dfl_root);\n\tspin_unlock_irq(&css_set_lock);\n\n\t \n\tsaved_cred = override_creds(of->file->f_cred);\n\tret = cgroup_attach_permissions(src_cgrp, dst_cgrp,\n\t\t\t\t\tof->file->f_path.dentry->d_sb,\n\t\t\t\t\tthreadgroup, ctx->ns);\n\trevert_creds(saved_cred);\n\tif (ret)\n\t\tgoto out_finish;\n\n\tret = cgroup_attach_task(dst_cgrp, task, threadgroup);\n\nout_finish:\n\tcgroup_procs_write_finish(task, threadgroup_locked);\nout_unlock:\n\tcgroup_kn_unlock(of->kn);\n\n\treturn ret;\n}\n\nstatic ssize_t cgroup_procs_write(struct kernfs_open_file *of,\n\t\t\t\t  char *buf, size_t nbytes, loff_t off)\n{\n\treturn __cgroup_procs_write(of, buf, true) ?: nbytes;\n}\n\nstatic void *cgroup_threads_start(struct seq_file *s, loff_t *pos)\n{\n\treturn __cgroup_procs_start(s, pos, 0);\n}\n\nstatic ssize_t cgroup_threads_write(struct kernfs_open_file *of,\n\t\t\t\t    char *buf, size_t nbytes, loff_t off)\n{\n\treturn __cgroup_procs_write(of, buf, false) ?: nbytes;\n}\n\n \nstatic struct cftype cgroup_base_files[] = {\n\t{\n\t\t.name = \"cgroup.type\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.seq_show = cgroup_type_show,\n\t\t.write = cgroup_type_write,\n\t},\n\t{\n\t\t.name = \"cgroup.procs\",\n\t\t.flags = CFTYPE_NS_DELEGATABLE,\n\t\t.file_offset = offsetof(struct cgroup, procs_file),\n\t\t.release = cgroup_procs_release,\n\t\t.seq_start = cgroup_procs_start,\n\t\t.seq_next = cgroup_procs_next,\n\t\t.seq_show = cgroup_procs_show,\n\t\t.write = cgroup_procs_write,\n\t},\n\t{\n\t\t.name = \"cgroup.threads\",\n\t\t.flags = CFTYPE_NS_DELEGATABLE,\n\t\t.release = cgroup_procs_release,\n\t\t.seq_start = cgroup_threads_start,\n\t\t.seq_next = cgroup_procs_next,\n\t\t.seq_show = cgroup_procs_show,\n\t\t.write = cgroup_threads_write,\n\t},\n\t{\n\t\t.name = \"cgroup.controllers\",\n\t\t.seq_show = cgroup_controllers_show,\n\t},\n\t{\n\t\t.name = \"cgroup.subtree_control\",\n\t\t.flags = CFTYPE_NS_DELEGATABLE,\n\t\t.seq_show = cgroup_subtree_control_show,\n\t\t.write = cgroup_subtree_control_write,\n\t},\n\t{\n\t\t.name = \"cgroup.events\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.file_offset = offsetof(struct cgroup, events_file),\n\t\t.seq_show = cgroup_events_show,\n\t},\n\t{\n\t\t.name = \"cgroup.max.descendants\",\n\t\t.seq_show = cgroup_max_descendants_show,\n\t\t.write = cgroup_max_descendants_write,\n\t},\n\t{\n\t\t.name = \"cgroup.max.depth\",\n\t\t.seq_show = cgroup_max_depth_show,\n\t\t.write = cgroup_max_depth_write,\n\t},\n\t{\n\t\t.name = \"cgroup.stat\",\n\t\t.seq_show = cgroup_stat_show,\n\t},\n\t{\n\t\t.name = \"cgroup.freeze\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.seq_show = cgroup_freeze_show,\n\t\t.write = cgroup_freeze_write,\n\t},\n\t{\n\t\t.name = \"cgroup.kill\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.write = cgroup_kill_write,\n\t},\n\t{\n\t\t.name = \"cpu.stat\",\n\t\t.seq_show = cpu_stat_show,\n\t},\n\t{\n\t\t.name = \"cpu.stat.local\",\n\t\t.seq_show = cpu_local_stat_show,\n\t},\n\t{ }\t \n};\n\nstatic struct cftype cgroup_psi_files[] = {\n#ifdef CONFIG_PSI\n\t{\n\t\t.name = \"io.pressure\",\n\t\t.file_offset = offsetof(struct cgroup, psi_files[PSI_IO]),\n\t\t.seq_show = cgroup_io_pressure_show,\n\t\t.write = cgroup_io_pressure_write,\n\t\t.poll = cgroup_pressure_poll,\n\t\t.release = cgroup_pressure_release,\n\t},\n\t{\n\t\t.name = \"memory.pressure\",\n\t\t.file_offset = offsetof(struct cgroup, psi_files[PSI_MEM]),\n\t\t.seq_show = cgroup_memory_pressure_show,\n\t\t.write = cgroup_memory_pressure_write,\n\t\t.poll = cgroup_pressure_poll,\n\t\t.release = cgroup_pressure_release,\n\t},\n\t{\n\t\t.name = \"cpu.pressure\",\n\t\t.file_offset = offsetof(struct cgroup, psi_files[PSI_CPU]),\n\t\t.seq_show = cgroup_cpu_pressure_show,\n\t\t.write = cgroup_cpu_pressure_write,\n\t\t.poll = cgroup_pressure_poll,\n\t\t.release = cgroup_pressure_release,\n\t},\n#ifdef CONFIG_IRQ_TIME_ACCOUNTING\n\t{\n\t\t.name = \"irq.pressure\",\n\t\t.file_offset = offsetof(struct cgroup, psi_files[PSI_IRQ]),\n\t\t.seq_show = cgroup_irq_pressure_show,\n\t\t.write = cgroup_irq_pressure_write,\n\t\t.poll = cgroup_pressure_poll,\n\t\t.release = cgroup_pressure_release,\n\t},\n#endif\n\t{\n\t\t.name = \"cgroup.pressure\",\n\t\t.seq_show = cgroup_pressure_show,\n\t\t.write = cgroup_pressure_write,\n\t},\n#endif  \n\t{ }\t \n};\n\n \nstatic void css_free_rwork_fn(struct work_struct *work)\n{\n\tstruct cgroup_subsys_state *css = container_of(to_rcu_work(work),\n\t\t\t\tstruct cgroup_subsys_state, destroy_rwork);\n\tstruct cgroup_subsys *ss = css->ss;\n\tstruct cgroup *cgrp = css->cgroup;\n\n\tpercpu_ref_exit(&css->refcnt);\n\n\tif (ss) {\n\t\t \n\t\tstruct cgroup_subsys_state *parent = css->parent;\n\t\tint id = css->id;\n\n\t\tss->css_free(css);\n\t\tcgroup_idr_remove(&ss->css_idr, id);\n\t\tcgroup_put(cgrp);\n\n\t\tif (parent)\n\t\t\tcss_put(parent);\n\t} else {\n\t\t \n\t\tatomic_dec(&cgrp->root->nr_cgrps);\n\t\tcgroup1_pidlist_destroy_all(cgrp);\n\t\tcancel_work_sync(&cgrp->release_agent_work);\n\t\tbpf_cgrp_storage_free(cgrp);\n\n\t\tif (cgroup_parent(cgrp)) {\n\t\t\t \n\t\t\tcgroup_put(cgroup_parent(cgrp));\n\t\t\tkernfs_put(cgrp->kn);\n\t\t\tpsi_cgroup_free(cgrp);\n\t\t\tcgroup_rstat_exit(cgrp);\n\t\t\tkfree(cgrp);\n\t\t} else {\n\t\t\t \n\t\t\tcgroup_destroy_root(cgrp->root);\n\t\t}\n\t}\n}\n\nstatic void css_release_work_fn(struct work_struct *work)\n{\n\tstruct cgroup_subsys_state *css =\n\t\tcontainer_of(work, struct cgroup_subsys_state, destroy_work);\n\tstruct cgroup_subsys *ss = css->ss;\n\tstruct cgroup *cgrp = css->cgroup;\n\n\tcgroup_lock();\n\n\tcss->flags |= CSS_RELEASED;\n\tlist_del_rcu(&css->sibling);\n\n\tif (ss) {\n\t\t \n\t\tif (!list_empty(&css->rstat_css_node)) {\n\t\t\tcgroup_rstat_flush(cgrp);\n\t\t\tlist_del_rcu(&css->rstat_css_node);\n\t\t}\n\n\t\tcgroup_idr_replace(&ss->css_idr, NULL, css->id);\n\t\tif (ss->css_released)\n\t\t\tss->css_released(css);\n\t} else {\n\t\tstruct cgroup *tcgrp;\n\n\t\t \n\t\tTRACE_CGROUP_PATH(release, cgrp);\n\n\t\tcgroup_rstat_flush(cgrp);\n\n\t\tspin_lock_irq(&css_set_lock);\n\t\tfor (tcgrp = cgroup_parent(cgrp); tcgrp;\n\t\t     tcgrp = cgroup_parent(tcgrp))\n\t\t\ttcgrp->nr_dying_descendants--;\n\t\tspin_unlock_irq(&css_set_lock);\n\n\t\t \n\t\tif (cgrp->kn)\n\t\t\tRCU_INIT_POINTER(*(void __rcu __force **)&cgrp->kn->priv,\n\t\t\t\t\t NULL);\n\t}\n\n\tcgroup_unlock();\n\n\tINIT_RCU_WORK(&css->destroy_rwork, css_free_rwork_fn);\n\tqueue_rcu_work(cgroup_destroy_wq, &css->destroy_rwork);\n}\n\nstatic void css_release(struct percpu_ref *ref)\n{\n\tstruct cgroup_subsys_state *css =\n\t\tcontainer_of(ref, struct cgroup_subsys_state, refcnt);\n\n\tINIT_WORK(&css->destroy_work, css_release_work_fn);\n\tqueue_work(cgroup_destroy_wq, &css->destroy_work);\n}\n\nstatic void init_and_link_css(struct cgroup_subsys_state *css,\n\t\t\t      struct cgroup_subsys *ss, struct cgroup *cgrp)\n{\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tcgroup_get_live(cgrp);\n\n\tmemset(css, 0, sizeof(*css));\n\tcss->cgroup = cgrp;\n\tcss->ss = ss;\n\tcss->id = -1;\n\tINIT_LIST_HEAD(&css->sibling);\n\tINIT_LIST_HEAD(&css->children);\n\tINIT_LIST_HEAD(&css->rstat_css_node);\n\tcss->serial_nr = css_serial_nr_next++;\n\tatomic_set(&css->online_cnt, 0);\n\n\tif (cgroup_parent(cgrp)) {\n\t\tcss->parent = cgroup_css(cgroup_parent(cgrp), ss);\n\t\tcss_get(css->parent);\n\t}\n\n\tif (ss->css_rstat_flush)\n\t\tlist_add_rcu(&css->rstat_css_node, &cgrp->rstat_css_list);\n\n\tBUG_ON(cgroup_css(cgrp, ss));\n}\n\n \nstatic int online_css(struct cgroup_subsys_state *css)\n{\n\tstruct cgroup_subsys *ss = css->ss;\n\tint ret = 0;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tif (ss->css_online)\n\t\tret = ss->css_online(css);\n\tif (!ret) {\n\t\tcss->flags |= CSS_ONLINE;\n\t\trcu_assign_pointer(css->cgroup->subsys[ss->id], css);\n\n\t\tatomic_inc(&css->online_cnt);\n\t\tif (css->parent)\n\t\t\tatomic_inc(&css->parent->online_cnt);\n\t}\n\treturn ret;\n}\n\n \nstatic void offline_css(struct cgroup_subsys_state *css)\n{\n\tstruct cgroup_subsys *ss = css->ss;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tif (!(css->flags & CSS_ONLINE))\n\t\treturn;\n\n\tif (ss->css_offline)\n\t\tss->css_offline(css);\n\n\tcss->flags &= ~CSS_ONLINE;\n\tRCU_INIT_POINTER(css->cgroup->subsys[ss->id], NULL);\n\n\twake_up_all(&css->cgroup->offline_waitq);\n}\n\n \nstatic struct cgroup_subsys_state *css_create(struct cgroup *cgrp,\n\t\t\t\t\t      struct cgroup_subsys *ss)\n{\n\tstruct cgroup *parent = cgroup_parent(cgrp);\n\tstruct cgroup_subsys_state *parent_css = cgroup_css(parent, ss);\n\tstruct cgroup_subsys_state *css;\n\tint err;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tcss = ss->css_alloc(parent_css);\n\tif (!css)\n\t\tcss = ERR_PTR(-ENOMEM);\n\tif (IS_ERR(css))\n\t\treturn css;\n\n\tinit_and_link_css(css, ss, cgrp);\n\n\terr = percpu_ref_init(&css->refcnt, css_release, 0, GFP_KERNEL);\n\tif (err)\n\t\tgoto err_free_css;\n\n\terr = cgroup_idr_alloc(&ss->css_idr, NULL, 2, 0, GFP_KERNEL);\n\tif (err < 0)\n\t\tgoto err_free_css;\n\tcss->id = err;\n\n\t \n\tlist_add_tail_rcu(&css->sibling, &parent_css->children);\n\tcgroup_idr_replace(&ss->css_idr, css, css->id);\n\n\terr = online_css(css);\n\tif (err)\n\t\tgoto err_list_del;\n\n\treturn css;\n\nerr_list_del:\n\tlist_del_rcu(&css->sibling);\nerr_free_css:\n\tlist_del_rcu(&css->rstat_css_node);\n\tINIT_RCU_WORK(&css->destroy_rwork, css_free_rwork_fn);\n\tqueue_rcu_work(cgroup_destroy_wq, &css->destroy_rwork);\n\treturn ERR_PTR(err);\n}\n\n \nstatic struct cgroup *cgroup_create(struct cgroup *parent, const char *name,\n\t\t\t\t    umode_t mode)\n{\n\tstruct cgroup_root *root = parent->root;\n\tstruct cgroup *cgrp, *tcgrp;\n\tstruct kernfs_node *kn;\n\tint level = parent->level + 1;\n\tint ret;\n\n\t \n\tcgrp = kzalloc(struct_size(cgrp, ancestors, (level + 1)), GFP_KERNEL);\n\tif (!cgrp)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tret = percpu_ref_init(&cgrp->self.refcnt, css_release, 0, GFP_KERNEL);\n\tif (ret)\n\t\tgoto out_free_cgrp;\n\n\tret = cgroup_rstat_init(cgrp);\n\tif (ret)\n\t\tgoto out_cancel_ref;\n\n\t \n\tkn = kernfs_create_dir(parent->kn, name, mode, cgrp);\n\tif (IS_ERR(kn)) {\n\t\tret = PTR_ERR(kn);\n\t\tgoto out_stat_exit;\n\t}\n\tcgrp->kn = kn;\n\n\tinit_cgroup_housekeeping(cgrp);\n\n\tcgrp->self.parent = &parent->self;\n\tcgrp->root = root;\n\tcgrp->level = level;\n\n\tret = psi_cgroup_alloc(cgrp);\n\tif (ret)\n\t\tgoto out_kernfs_remove;\n\n\tret = cgroup_bpf_inherit(cgrp);\n\tif (ret)\n\t\tgoto out_psi_free;\n\n\t \n\tcgrp->freezer.e_freeze = parent->freezer.e_freeze;\n\tif (cgrp->freezer.e_freeze) {\n\t\t \n\t\tset_bit(CGRP_FREEZE, &cgrp->flags);\n\t\tset_bit(CGRP_FROZEN, &cgrp->flags);\n\t}\n\n\tspin_lock_irq(&css_set_lock);\n\tfor (tcgrp = cgrp; tcgrp; tcgrp = cgroup_parent(tcgrp)) {\n\t\tcgrp->ancestors[tcgrp->level] = tcgrp;\n\n\t\tif (tcgrp != cgrp) {\n\t\t\ttcgrp->nr_descendants++;\n\n\t\t\t \n\t\t\tif (cgrp->freezer.e_freeze)\n\t\t\t\ttcgrp->freezer.nr_frozen_descendants++;\n\t\t}\n\t}\n\tspin_unlock_irq(&css_set_lock);\n\n\tif (notify_on_release(parent))\n\t\tset_bit(CGRP_NOTIFY_ON_RELEASE, &cgrp->flags);\n\n\tif (test_bit(CGRP_CPUSET_CLONE_CHILDREN, &parent->flags))\n\t\tset_bit(CGRP_CPUSET_CLONE_CHILDREN, &cgrp->flags);\n\n\tcgrp->self.serial_nr = css_serial_nr_next++;\n\n\t \n\tlist_add_tail_rcu(&cgrp->self.sibling, &cgroup_parent(cgrp)->self.children);\n\tatomic_inc(&root->nr_cgrps);\n\tcgroup_get_live(parent);\n\n\t \n\tif (!cgroup_on_dfl(cgrp))\n\t\tcgrp->subtree_control = cgroup_control(cgrp);\n\n\tcgroup_propagate_control(cgrp);\n\n\treturn cgrp;\n\nout_psi_free:\n\tpsi_cgroup_free(cgrp);\nout_kernfs_remove:\n\tkernfs_remove(cgrp->kn);\nout_stat_exit:\n\tcgroup_rstat_exit(cgrp);\nout_cancel_ref:\n\tpercpu_ref_exit(&cgrp->self.refcnt);\nout_free_cgrp:\n\tkfree(cgrp);\n\treturn ERR_PTR(ret);\n}\n\nstatic bool cgroup_check_hierarchy_limits(struct cgroup *parent)\n{\n\tstruct cgroup *cgroup;\n\tint ret = false;\n\tint level = 1;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tfor (cgroup = parent; cgroup; cgroup = cgroup_parent(cgroup)) {\n\t\tif (cgroup->nr_descendants >= cgroup->max_descendants)\n\t\t\tgoto fail;\n\n\t\tif (level > cgroup->max_depth)\n\t\t\tgoto fail;\n\n\t\tlevel++;\n\t}\n\n\tret = true;\nfail:\n\treturn ret;\n}\n\nint cgroup_mkdir(struct kernfs_node *parent_kn, const char *name, umode_t mode)\n{\n\tstruct cgroup *parent, *cgrp;\n\tint ret;\n\n\t \n\tif (strchr(name, '\\n'))\n\t\treturn -EINVAL;\n\n\tparent = cgroup_kn_lock_live(parent_kn, false);\n\tif (!parent)\n\t\treturn -ENODEV;\n\n\tif (!cgroup_check_hierarchy_limits(parent)) {\n\t\tret = -EAGAIN;\n\t\tgoto out_unlock;\n\t}\n\n\tcgrp = cgroup_create(parent, name, mode);\n\tif (IS_ERR(cgrp)) {\n\t\tret = PTR_ERR(cgrp);\n\t\tgoto out_unlock;\n\t}\n\n\t \n\tkernfs_get(cgrp->kn);\n\n\tret = cgroup_kn_set_ugid(cgrp->kn);\n\tif (ret)\n\t\tgoto out_destroy;\n\n\tret = css_populate_dir(&cgrp->self);\n\tif (ret)\n\t\tgoto out_destroy;\n\n\tret = cgroup_apply_control_enable(cgrp);\n\tif (ret)\n\t\tgoto out_destroy;\n\n\tTRACE_CGROUP_PATH(mkdir, cgrp);\n\n\t \n\tkernfs_activate(cgrp->kn);\n\n\tret = 0;\n\tgoto out_unlock;\n\nout_destroy:\n\tcgroup_destroy_locked(cgrp);\nout_unlock:\n\tcgroup_kn_unlock(parent_kn);\n\treturn ret;\n}\n\n \nstatic void css_killed_work_fn(struct work_struct *work)\n{\n\tstruct cgroup_subsys_state *css =\n\t\tcontainer_of(work, struct cgroup_subsys_state, destroy_work);\n\n\tcgroup_lock();\n\n\tdo {\n\t\toffline_css(css);\n\t\tcss_put(css);\n\t\t \n\t\tcss = css->parent;\n\t} while (css && atomic_dec_and_test(&css->online_cnt));\n\n\tcgroup_unlock();\n}\n\n \nstatic void css_killed_ref_fn(struct percpu_ref *ref)\n{\n\tstruct cgroup_subsys_state *css =\n\t\tcontainer_of(ref, struct cgroup_subsys_state, refcnt);\n\n\tif (atomic_dec_and_test(&css->online_cnt)) {\n\t\tINIT_WORK(&css->destroy_work, css_killed_work_fn);\n\t\tqueue_work(cgroup_destroy_wq, &css->destroy_work);\n\t}\n}\n\n \nstatic void kill_css(struct cgroup_subsys_state *css)\n{\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tif (css->flags & CSS_DYING)\n\t\treturn;\n\n\tcss->flags |= CSS_DYING;\n\n\t \n\tcss_clear_dir(css);\n\n\t \n\tcss_get(css);\n\n\t \n\tpercpu_ref_kill_and_confirm(&css->refcnt, css_killed_ref_fn);\n}\n\n \nstatic int cgroup_destroy_locked(struct cgroup *cgrp)\n\t__releases(&cgroup_mutex) __acquires(&cgroup_mutex)\n{\n\tstruct cgroup *tcgrp, *parent = cgroup_parent(cgrp);\n\tstruct cgroup_subsys_state *css;\n\tstruct cgrp_cset_link *link;\n\tint ssid;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\t \n\tif (cgroup_is_populated(cgrp))\n\t\treturn -EBUSY;\n\n\t \n\tif (css_has_online_children(&cgrp->self))\n\t\treturn -EBUSY;\n\n\t \n\tcgrp->self.flags &= ~CSS_ONLINE;\n\n\tspin_lock_irq(&css_set_lock);\n\tlist_for_each_entry(link, &cgrp->cset_links, cset_link)\n\t\tlink->cset->dead = true;\n\tspin_unlock_irq(&css_set_lock);\n\n\t \n\tfor_each_css(css, ssid, cgrp)\n\t\tkill_css(css);\n\n\t \n\tcss_clear_dir(&cgrp->self);\n\tkernfs_remove(cgrp->kn);\n\n\tif (cgroup_is_threaded(cgrp))\n\t\tparent->nr_threaded_children--;\n\n\tspin_lock_irq(&css_set_lock);\n\tfor (tcgrp = parent; tcgrp; tcgrp = cgroup_parent(tcgrp)) {\n\t\ttcgrp->nr_descendants--;\n\t\ttcgrp->nr_dying_descendants++;\n\t\t \n\t\tif (test_bit(CGRP_FROZEN, &cgrp->flags))\n\t\t\ttcgrp->freezer.nr_frozen_descendants--;\n\t}\n\tspin_unlock_irq(&css_set_lock);\n\n\tcgroup1_check_for_release(parent);\n\n\tcgroup_bpf_offline(cgrp);\n\n\t \n\tpercpu_ref_kill(&cgrp->self.refcnt);\n\n\treturn 0;\n};\n\nint cgroup_rmdir(struct kernfs_node *kn)\n{\n\tstruct cgroup *cgrp;\n\tint ret = 0;\n\n\tcgrp = cgroup_kn_lock_live(kn, false);\n\tif (!cgrp)\n\t\treturn 0;\n\n\tret = cgroup_destroy_locked(cgrp);\n\tif (!ret)\n\t\tTRACE_CGROUP_PATH(rmdir, cgrp);\n\n\tcgroup_kn_unlock(kn);\n\treturn ret;\n}\n\nstatic struct kernfs_syscall_ops cgroup_kf_syscall_ops = {\n\t.show_options\t\t= cgroup_show_options,\n\t.mkdir\t\t\t= cgroup_mkdir,\n\t.rmdir\t\t\t= cgroup_rmdir,\n\t.show_path\t\t= cgroup_show_path,\n};\n\nstatic void __init cgroup_init_subsys(struct cgroup_subsys *ss, bool early)\n{\n\tstruct cgroup_subsys_state *css;\n\n\tpr_debug(\"Initializing cgroup subsys %s\\n\", ss->name);\n\n\tcgroup_lock();\n\n\tidr_init(&ss->css_idr);\n\tINIT_LIST_HEAD(&ss->cfts);\n\n\t \n\tss->root = &cgrp_dfl_root;\n\tcss = ss->css_alloc(NULL);\n\t \n\tBUG_ON(IS_ERR(css));\n\tinit_and_link_css(css, ss, &cgrp_dfl_root.cgrp);\n\n\t \n\tcss->flags |= CSS_NO_REF;\n\n\tif (early) {\n\t\t \n\t\tcss->id = 1;\n\t} else {\n\t\tcss->id = cgroup_idr_alloc(&ss->css_idr, css, 1, 2, GFP_KERNEL);\n\t\tBUG_ON(css->id < 0);\n\t}\n\n\t \n\tinit_css_set.subsys[ss->id] = css;\n\n\thave_fork_callback |= (bool)ss->fork << ss->id;\n\thave_exit_callback |= (bool)ss->exit << ss->id;\n\thave_release_callback |= (bool)ss->release << ss->id;\n\thave_canfork_callback |= (bool)ss->can_fork << ss->id;\n\n\t \n\tBUG_ON(!list_empty(&init_task.tasks));\n\n\tBUG_ON(online_css(css));\n\n\tcgroup_unlock();\n}\n\n \nint __init cgroup_init_early(void)\n{\n\tstatic struct cgroup_fs_context __initdata ctx;\n\tstruct cgroup_subsys *ss;\n\tint i;\n\n\tctx.root = &cgrp_dfl_root;\n\tinit_cgroup_root(&ctx);\n\tcgrp_dfl_root.cgrp.self.flags |= CSS_NO_REF;\n\n\tRCU_INIT_POINTER(init_task.cgroups, &init_css_set);\n\n\tfor_each_subsys(ss, i) {\n\t\tWARN(!ss->css_alloc || !ss->css_free || ss->name || ss->id,\n\t\t     \"invalid cgroup_subsys %d:%s css_alloc=%p css_free=%p id:name=%d:%s\\n\",\n\t\t     i, cgroup_subsys_name[i], ss->css_alloc, ss->css_free,\n\t\t     ss->id, ss->name);\n\t\tWARN(strlen(cgroup_subsys_name[i]) > MAX_CGROUP_TYPE_NAMELEN,\n\t\t     \"cgroup_subsys_name %s too long\\n\", cgroup_subsys_name[i]);\n\n\t\tss->id = i;\n\t\tss->name = cgroup_subsys_name[i];\n\t\tif (!ss->legacy_name)\n\t\t\tss->legacy_name = cgroup_subsys_name[i];\n\n\t\tif (ss->early_init)\n\t\t\tcgroup_init_subsys(ss, true);\n\t}\n\treturn 0;\n}\n\n \nint __init cgroup_init(void)\n{\n\tstruct cgroup_subsys *ss;\n\tint ssid;\n\n\tBUILD_BUG_ON(CGROUP_SUBSYS_COUNT > 16);\n\tBUG_ON(cgroup_init_cftypes(NULL, cgroup_base_files));\n\tBUG_ON(cgroup_init_cftypes(NULL, cgroup_psi_files));\n\tBUG_ON(cgroup_init_cftypes(NULL, cgroup1_base_files));\n\n\tcgroup_rstat_boot();\n\n\tget_user_ns(init_cgroup_ns.user_ns);\n\n\tcgroup_lock();\n\n\t \n\thash_add(css_set_table, &init_css_set.hlist,\n\t\t css_set_hash(init_css_set.subsys));\n\n\tBUG_ON(cgroup_setup_root(&cgrp_dfl_root, 0));\n\n\tcgroup_unlock();\n\n\tfor_each_subsys(ss, ssid) {\n\t\tif (ss->early_init) {\n\t\t\tstruct cgroup_subsys_state *css =\n\t\t\t\tinit_css_set.subsys[ss->id];\n\n\t\t\tcss->id = cgroup_idr_alloc(&ss->css_idr, css, 1, 2,\n\t\t\t\t\t\t   GFP_KERNEL);\n\t\t\tBUG_ON(css->id < 0);\n\t\t} else {\n\t\t\tcgroup_init_subsys(ss, false);\n\t\t}\n\n\t\tlist_add_tail(&init_css_set.e_cset_node[ssid],\n\t\t\t      &cgrp_dfl_root.cgrp.e_csets[ssid]);\n\n\t\t \n\t\tif (!cgroup_ssid_enabled(ssid))\n\t\t\tcontinue;\n\n\t\tif (cgroup1_ssid_disabled(ssid))\n\t\t\tpr_info(\"Disabling %s control group subsystem in v1 mounts\\n\",\n\t\t\t\tss->name);\n\n\t\tcgrp_dfl_root.subsys_mask |= 1 << ss->id;\n\n\t\t \n\t\tWARN_ON(ss->implicit_on_dfl && !ss->threaded);\n\n\t\tif (ss->implicit_on_dfl)\n\t\t\tcgrp_dfl_implicit_ss_mask |= 1 << ss->id;\n\t\telse if (!ss->dfl_cftypes)\n\t\t\tcgrp_dfl_inhibit_ss_mask |= 1 << ss->id;\n\n\t\tif (ss->threaded)\n\t\t\tcgrp_dfl_threaded_ss_mask |= 1 << ss->id;\n\n\t\tif (ss->dfl_cftypes == ss->legacy_cftypes) {\n\t\t\tWARN_ON(cgroup_add_cftypes(ss, ss->dfl_cftypes));\n\t\t} else {\n\t\t\tWARN_ON(cgroup_add_dfl_cftypes(ss, ss->dfl_cftypes));\n\t\t\tWARN_ON(cgroup_add_legacy_cftypes(ss, ss->legacy_cftypes));\n\t\t}\n\n\t\tif (ss->bind)\n\t\t\tss->bind(init_css_set.subsys[ssid]);\n\n\t\tcgroup_lock();\n\t\tcss_populate_dir(init_css_set.subsys[ssid]);\n\t\tcgroup_unlock();\n\t}\n\n\t \n\thash_del(&init_css_set.hlist);\n\thash_add(css_set_table, &init_css_set.hlist,\n\t\t css_set_hash(init_css_set.subsys));\n\n\tWARN_ON(sysfs_create_mount_point(fs_kobj, \"cgroup\"));\n\tWARN_ON(register_filesystem(&cgroup_fs_type));\n\tWARN_ON(register_filesystem(&cgroup2_fs_type));\n\tWARN_ON(!proc_create_single(\"cgroups\", 0, NULL, proc_cgroupstats_show));\n#ifdef CONFIG_CPUSETS\n\tWARN_ON(register_filesystem(&cpuset_fs_type));\n#endif\n\n\treturn 0;\n}\n\nstatic int __init cgroup_wq_init(void)\n{\n\t \n\tcgroup_destroy_wq = alloc_workqueue(\"cgroup_destroy\", 0, 1);\n\tBUG_ON(!cgroup_destroy_wq);\n\treturn 0;\n}\ncore_initcall(cgroup_wq_init);\n\nvoid cgroup_path_from_kernfs_id(u64 id, char *buf, size_t buflen)\n{\n\tstruct kernfs_node *kn;\n\n\tkn = kernfs_find_and_get_node_by_id(cgrp_dfl_root.kf_root, id);\n\tif (!kn)\n\t\treturn;\n\tkernfs_path(kn, buf, buflen);\n\tkernfs_put(kn);\n}\n\n \nstruct cgroup *cgroup_get_from_id(u64 id)\n{\n\tstruct kernfs_node *kn;\n\tstruct cgroup *cgrp, *root_cgrp;\n\n\tkn = kernfs_find_and_get_node_by_id(cgrp_dfl_root.kf_root, id);\n\tif (!kn)\n\t\treturn ERR_PTR(-ENOENT);\n\n\tif (kernfs_type(kn) != KERNFS_DIR) {\n\t\tkernfs_put(kn);\n\t\treturn ERR_PTR(-ENOENT);\n\t}\n\n\trcu_read_lock();\n\n\tcgrp = rcu_dereference(*(void __rcu __force **)&kn->priv);\n\tif (cgrp && !cgroup_tryget(cgrp))\n\t\tcgrp = NULL;\n\n\trcu_read_unlock();\n\tkernfs_put(kn);\n\n\tif (!cgrp)\n\t\treturn ERR_PTR(-ENOENT);\n\n\troot_cgrp = current_cgns_cgroup_dfl();\n\tif (!cgroup_is_descendant(cgrp, root_cgrp)) {\n\t\tcgroup_put(cgrp);\n\t\treturn ERR_PTR(-ENOENT);\n\t}\n\n\treturn cgrp;\n}\nEXPORT_SYMBOL_GPL(cgroup_get_from_id);\n\n \nint proc_cgroup_show(struct seq_file *m, struct pid_namespace *ns,\n\t\t     struct pid *pid, struct task_struct *tsk)\n{\n\tchar *buf;\n\tint retval;\n\tstruct cgroup_root *root;\n\n\tretval = -ENOMEM;\n\tbuf = kmalloc(PATH_MAX, GFP_KERNEL);\n\tif (!buf)\n\t\tgoto out;\n\n\tcgroup_lock();\n\tspin_lock_irq(&css_set_lock);\n\n\tfor_each_root(root) {\n\t\tstruct cgroup_subsys *ss;\n\t\tstruct cgroup *cgrp;\n\t\tint ssid, count = 0;\n\n\t\tif (root == &cgrp_dfl_root && !READ_ONCE(cgrp_dfl_visible))\n\t\t\tcontinue;\n\n\t\tseq_printf(m, \"%d:\", root->hierarchy_id);\n\t\tif (root != &cgrp_dfl_root)\n\t\t\tfor_each_subsys(ss, ssid)\n\t\t\t\tif (root->subsys_mask & (1 << ssid))\n\t\t\t\t\tseq_printf(m, \"%s%s\", count++ ? \",\" : \"\",\n\t\t\t\t\t\t   ss->legacy_name);\n\t\tif (strlen(root->name))\n\t\t\tseq_printf(m, \"%sname=%s\", count ? \",\" : \"\",\n\t\t\t\t   root->name);\n\t\tseq_putc(m, ':');\n\n\t\tcgrp = task_cgroup_from_root(tsk, root);\n\n\t\t \n\t\tif (cgroup_on_dfl(cgrp) || !(tsk->flags & PF_EXITING)) {\n\t\t\tretval = cgroup_path_ns_locked(cgrp, buf, PATH_MAX,\n\t\t\t\t\t\tcurrent->nsproxy->cgroup_ns);\n\t\t\tif (retval >= PATH_MAX)\n\t\t\t\tretval = -ENAMETOOLONG;\n\t\t\tif (retval < 0)\n\t\t\t\tgoto out_unlock;\n\n\t\t\tseq_puts(m, buf);\n\t\t} else {\n\t\t\tseq_puts(m, \"/\");\n\t\t}\n\n\t\tif (cgroup_on_dfl(cgrp) && cgroup_is_dead(cgrp))\n\t\t\tseq_puts(m, \" (deleted)\\n\");\n\t\telse\n\t\t\tseq_putc(m, '\\n');\n\t}\n\n\tretval = 0;\nout_unlock:\n\tspin_unlock_irq(&css_set_lock);\n\tcgroup_unlock();\n\tkfree(buf);\nout:\n\treturn retval;\n}\n\n \nvoid cgroup_fork(struct task_struct *child)\n{\n\tRCU_INIT_POINTER(child->cgroups, &init_css_set);\n\tINIT_LIST_HEAD(&child->cg_list);\n}\n\n \nstatic struct cgroup *cgroup_v1v2_get_from_file(struct file *f)\n{\n\tstruct cgroup_subsys_state *css;\n\n\tcss = css_tryget_online_from_dir(f->f_path.dentry, NULL);\n\tif (IS_ERR(css))\n\t\treturn ERR_CAST(css);\n\n\treturn css->cgroup;\n}\n\n \nstatic struct cgroup *cgroup_get_from_file(struct file *f)\n{\n\tstruct cgroup *cgrp = cgroup_v1v2_get_from_file(f);\n\n\tif (IS_ERR(cgrp))\n\t\treturn ERR_CAST(cgrp);\n\n\tif (!cgroup_on_dfl(cgrp)) {\n\t\tcgroup_put(cgrp);\n\t\treturn ERR_PTR(-EBADF);\n\t}\n\n\treturn cgrp;\n}\n\n \nstatic int cgroup_css_set_fork(struct kernel_clone_args *kargs)\n\t__acquires(&cgroup_mutex) __acquires(&cgroup_threadgroup_rwsem)\n{\n\tint ret;\n\tstruct cgroup *dst_cgrp = NULL;\n\tstruct css_set *cset;\n\tstruct super_block *sb;\n\tstruct file *f;\n\n\tif (kargs->flags & CLONE_INTO_CGROUP)\n\t\tcgroup_lock();\n\n\tcgroup_threadgroup_change_begin(current);\n\n\tspin_lock_irq(&css_set_lock);\n\tcset = task_css_set(current);\n\tget_css_set(cset);\n\tspin_unlock_irq(&css_set_lock);\n\n\tif (!(kargs->flags & CLONE_INTO_CGROUP)) {\n\t\tkargs->cset = cset;\n\t\treturn 0;\n\t}\n\n\tf = fget_raw(kargs->cgroup);\n\tif (!f) {\n\t\tret = -EBADF;\n\t\tgoto err;\n\t}\n\tsb = f->f_path.dentry->d_sb;\n\n\tdst_cgrp = cgroup_get_from_file(f);\n\tif (IS_ERR(dst_cgrp)) {\n\t\tret = PTR_ERR(dst_cgrp);\n\t\tdst_cgrp = NULL;\n\t\tgoto err;\n\t}\n\n\tif (cgroup_is_dead(dst_cgrp)) {\n\t\tret = -ENODEV;\n\t\tgoto err;\n\t}\n\n\t \n\tret = cgroup_may_write(dst_cgrp, sb);\n\tif (ret)\n\t\tgoto err;\n\n\t \n\tret = cgroup_attach_permissions(cset->dfl_cgrp, dst_cgrp, sb,\n\t\t\t\t\t!(kargs->flags & CLONE_THREAD),\n\t\t\t\t\tcurrent->nsproxy->cgroup_ns);\n\tif (ret)\n\t\tgoto err;\n\n\tkargs->cset = find_css_set(cset, dst_cgrp);\n\tif (!kargs->cset) {\n\t\tret = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\tput_css_set(cset);\n\tfput(f);\n\tkargs->cgrp = dst_cgrp;\n\treturn ret;\n\nerr:\n\tcgroup_threadgroup_change_end(current);\n\tcgroup_unlock();\n\tif (f)\n\t\tfput(f);\n\tif (dst_cgrp)\n\t\tcgroup_put(dst_cgrp);\n\tput_css_set(cset);\n\tif (kargs->cset)\n\t\tput_css_set(kargs->cset);\n\treturn ret;\n}\n\n \nstatic void cgroup_css_set_put_fork(struct kernel_clone_args *kargs)\n\t__releases(&cgroup_threadgroup_rwsem) __releases(&cgroup_mutex)\n{\n\tstruct cgroup *cgrp = kargs->cgrp;\n\tstruct css_set *cset = kargs->cset;\n\n\tcgroup_threadgroup_change_end(current);\n\n\tif (cset) {\n\t\tput_css_set(cset);\n\t\tkargs->cset = NULL;\n\t}\n\n\tif (kargs->flags & CLONE_INTO_CGROUP) {\n\t\tcgroup_unlock();\n\t\tif (cgrp) {\n\t\t\tcgroup_put(cgrp);\n\t\t\tkargs->cgrp = NULL;\n\t\t}\n\t}\n}\n\n \nint cgroup_can_fork(struct task_struct *child, struct kernel_clone_args *kargs)\n{\n\tstruct cgroup_subsys *ss;\n\tint i, j, ret;\n\n\tret = cgroup_css_set_fork(kargs);\n\tif (ret)\n\t\treturn ret;\n\n\tdo_each_subsys_mask(ss, i, have_canfork_callback) {\n\t\tret = ss->can_fork(child, kargs->cset);\n\t\tif (ret)\n\t\t\tgoto out_revert;\n\t} while_each_subsys_mask();\n\n\treturn 0;\n\nout_revert:\n\tfor_each_subsys(ss, j) {\n\t\tif (j >= i)\n\t\t\tbreak;\n\t\tif (ss->cancel_fork)\n\t\t\tss->cancel_fork(child, kargs->cset);\n\t}\n\n\tcgroup_css_set_put_fork(kargs);\n\n\treturn ret;\n}\n\n \nvoid cgroup_cancel_fork(struct task_struct *child,\n\t\t\tstruct kernel_clone_args *kargs)\n{\n\tstruct cgroup_subsys *ss;\n\tint i;\n\n\tfor_each_subsys(ss, i)\n\t\tif (ss->cancel_fork)\n\t\t\tss->cancel_fork(child, kargs->cset);\n\n\tcgroup_css_set_put_fork(kargs);\n}\n\n \nvoid cgroup_post_fork(struct task_struct *child,\n\t\t      struct kernel_clone_args *kargs)\n\t__releases(&cgroup_threadgroup_rwsem) __releases(&cgroup_mutex)\n{\n\tunsigned long cgrp_flags = 0;\n\tbool kill = false;\n\tstruct cgroup_subsys *ss;\n\tstruct css_set *cset;\n\tint i;\n\n\tcset = kargs->cset;\n\tkargs->cset = NULL;\n\n\tspin_lock_irq(&css_set_lock);\n\n\t \n\tif (likely(child->pid)) {\n\t\tif (kargs->cgrp)\n\t\t\tcgrp_flags = kargs->cgrp->flags;\n\t\telse\n\t\t\tcgrp_flags = cset->dfl_cgrp->flags;\n\n\t\tWARN_ON_ONCE(!list_empty(&child->cg_list));\n\t\tcset->nr_tasks++;\n\t\tcss_set_move_task(child, NULL, cset, false);\n\t} else {\n\t\tput_css_set(cset);\n\t\tcset = NULL;\n\t}\n\n\tif (!(child->flags & PF_KTHREAD)) {\n\t\tif (unlikely(test_bit(CGRP_FREEZE, &cgrp_flags))) {\n\t\t\t \n\t\t\tspin_lock(&child->sighand->siglock);\n\t\t\tWARN_ON_ONCE(child->frozen);\n\t\t\tchild->jobctl |= JOBCTL_TRAP_FREEZE;\n\t\t\tspin_unlock(&child->sighand->siglock);\n\n\t\t\t \n\t\t}\n\n\t\t \n\t\tkill = test_bit(CGRP_KILL, &cgrp_flags);\n\t}\n\n\tspin_unlock_irq(&css_set_lock);\n\n\t \n\tdo_each_subsys_mask(ss, i, have_fork_callback) {\n\t\tss->fork(child);\n\t} while_each_subsys_mask();\n\n\t \n\tif (kargs->flags & CLONE_NEWCGROUP) {\n\t\tstruct css_set *rcset = child->nsproxy->cgroup_ns->root_cset;\n\n\t\tget_css_set(cset);\n\t\tchild->nsproxy->cgroup_ns->root_cset = cset;\n\t\tput_css_set(rcset);\n\t}\n\n\t \n\tif (unlikely(kill))\n\t\tdo_send_sig_info(SIGKILL, SEND_SIG_NOINFO, child, PIDTYPE_TGID);\n\n\tcgroup_css_set_put_fork(kargs);\n}\n\n \nvoid cgroup_exit(struct task_struct *tsk)\n{\n\tstruct cgroup_subsys *ss;\n\tstruct css_set *cset;\n\tint i;\n\n\tspin_lock_irq(&css_set_lock);\n\n\tWARN_ON_ONCE(list_empty(&tsk->cg_list));\n\tcset = task_css_set(tsk);\n\tcss_set_move_task(tsk, cset, NULL, false);\n\tlist_add_tail(&tsk->cg_list, &cset->dying_tasks);\n\tcset->nr_tasks--;\n\n\tif (dl_task(tsk))\n\t\tdec_dl_tasks_cs(tsk);\n\n\tWARN_ON_ONCE(cgroup_task_frozen(tsk));\n\tif (unlikely(!(tsk->flags & PF_KTHREAD) &&\n\t\t     test_bit(CGRP_FREEZE, &task_dfl_cgroup(tsk)->flags)))\n\t\tcgroup_update_frozen(task_dfl_cgroup(tsk));\n\n\tspin_unlock_irq(&css_set_lock);\n\n\t \n\tdo_each_subsys_mask(ss, i, have_exit_callback) {\n\t\tss->exit(tsk);\n\t} while_each_subsys_mask();\n}\n\nvoid cgroup_release(struct task_struct *task)\n{\n\tstruct cgroup_subsys *ss;\n\tint ssid;\n\n\tdo_each_subsys_mask(ss, ssid, have_release_callback) {\n\t\tss->release(task);\n\t} while_each_subsys_mask();\n\n\tspin_lock_irq(&css_set_lock);\n\tcss_set_skip_task_iters(task_css_set(task), task);\n\tlist_del_init(&task->cg_list);\n\tspin_unlock_irq(&css_set_lock);\n}\n\nvoid cgroup_free(struct task_struct *task)\n{\n\tstruct css_set *cset = task_css_set(task);\n\tput_css_set(cset);\n}\n\nstatic int __init cgroup_disable(char *str)\n{\n\tstruct cgroup_subsys *ss;\n\tchar *token;\n\tint i;\n\n\twhile ((token = strsep(&str, \",\")) != NULL) {\n\t\tif (!*token)\n\t\t\tcontinue;\n\n\t\tfor_each_subsys(ss, i) {\n\t\t\tif (strcmp(token, ss->name) &&\n\t\t\t    strcmp(token, ss->legacy_name))\n\t\t\t\tcontinue;\n\n\t\t\tstatic_branch_disable(cgroup_subsys_enabled_key[i]);\n\t\t\tpr_info(\"Disabling %s control group subsystem\\n\",\n\t\t\t\tss->name);\n\t\t}\n\n\t\tfor (i = 0; i < OPT_FEATURE_COUNT; i++) {\n\t\t\tif (strcmp(token, cgroup_opt_feature_names[i]))\n\t\t\t\tcontinue;\n\t\t\tcgroup_feature_disable_mask |= 1 << i;\n\t\t\tpr_info(\"Disabling %s control group feature\\n\",\n\t\t\t\tcgroup_opt_feature_names[i]);\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn 1;\n}\n__setup(\"cgroup_disable=\", cgroup_disable);\n\nvoid __init __weak enable_debug_cgroup(void) { }\n\nstatic int __init enable_cgroup_debug(char *str)\n{\n\tcgroup_debug = true;\n\tenable_debug_cgroup();\n\treturn 1;\n}\n__setup(\"cgroup_debug\", enable_cgroup_debug);\n\n \nstruct cgroup_subsys_state *css_tryget_online_from_dir(struct dentry *dentry,\n\t\t\t\t\t\t       struct cgroup_subsys *ss)\n{\n\tstruct kernfs_node *kn = kernfs_node_from_dentry(dentry);\n\tstruct file_system_type *s_type = dentry->d_sb->s_type;\n\tstruct cgroup_subsys_state *css = NULL;\n\tstruct cgroup *cgrp;\n\n\t \n\tif ((s_type != &cgroup_fs_type && s_type != &cgroup2_fs_type) ||\n\t    !kn || kernfs_type(kn) != KERNFS_DIR)\n\t\treturn ERR_PTR(-EBADF);\n\n\trcu_read_lock();\n\n\t \n\tcgrp = rcu_dereference(*(void __rcu __force **)&kn->priv);\n\tif (cgrp)\n\t\tcss = cgroup_css(cgrp, ss);\n\n\tif (!css || !css_tryget_online(css))\n\t\tcss = ERR_PTR(-ENOENT);\n\n\trcu_read_unlock();\n\treturn css;\n}\n\n \nstruct cgroup_subsys_state *css_from_id(int id, struct cgroup_subsys *ss)\n{\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\treturn idr_find(&ss->css_idr, id);\n}\n\n \nstruct cgroup *cgroup_get_from_path(const char *path)\n{\n\tstruct kernfs_node *kn;\n\tstruct cgroup *cgrp = ERR_PTR(-ENOENT);\n\tstruct cgroup *root_cgrp;\n\n\troot_cgrp = current_cgns_cgroup_dfl();\n\tkn = kernfs_walk_and_get(root_cgrp->kn, path);\n\tif (!kn)\n\t\tgoto out;\n\n\tif (kernfs_type(kn) != KERNFS_DIR) {\n\t\tcgrp = ERR_PTR(-ENOTDIR);\n\t\tgoto out_kernfs;\n\t}\n\n\trcu_read_lock();\n\n\tcgrp = rcu_dereference(*(void __rcu __force **)&kn->priv);\n\tif (!cgrp || !cgroup_tryget(cgrp))\n\t\tcgrp = ERR_PTR(-ENOENT);\n\n\trcu_read_unlock();\n\nout_kernfs:\n\tkernfs_put(kn);\nout:\n\treturn cgrp;\n}\nEXPORT_SYMBOL_GPL(cgroup_get_from_path);\n\n \nstruct cgroup *cgroup_v1v2_get_from_fd(int fd)\n{\n\tstruct cgroup *cgrp;\n\tstruct fd f = fdget_raw(fd);\n\tif (!f.file)\n\t\treturn ERR_PTR(-EBADF);\n\n\tcgrp = cgroup_v1v2_get_from_file(f.file);\n\tfdput(f);\n\treturn cgrp;\n}\n\n \nstruct cgroup *cgroup_get_from_fd(int fd)\n{\n\tstruct cgroup *cgrp = cgroup_v1v2_get_from_fd(fd);\n\n\tif (IS_ERR(cgrp))\n\t\treturn ERR_CAST(cgrp);\n\n\tif (!cgroup_on_dfl(cgrp)) {\n\t\tcgroup_put(cgrp);\n\t\treturn ERR_PTR(-EBADF);\n\t}\n\treturn cgrp;\n}\nEXPORT_SYMBOL_GPL(cgroup_get_from_fd);\n\nstatic u64 power_of_ten(int power)\n{\n\tu64 v = 1;\n\twhile (power--)\n\t\tv *= 10;\n\treturn v;\n}\n\n \nint cgroup_parse_float(const char *input, unsigned dec_shift, s64 *v)\n{\n\ts64 whole, frac = 0;\n\tint fstart = 0, fend = 0, flen;\n\n\tif (!sscanf(input, \"%lld.%n%lld%n\", &whole, &fstart, &frac, &fend))\n\t\treturn -EINVAL;\n\tif (frac < 0)\n\t\treturn -EINVAL;\n\n\tflen = fend > fstart ? fend - fstart : 0;\n\tif (flen < dec_shift)\n\t\tfrac *= power_of_ten(dec_shift - flen);\n\telse\n\t\tfrac = DIV_ROUND_CLOSEST_ULL(frac, power_of_ten(flen - dec_shift));\n\n\t*v = whole * power_of_ten(dec_shift) + frac;\n\treturn 0;\n}\n\n \n#ifdef CONFIG_SOCK_CGROUP_DATA\n\nvoid cgroup_sk_alloc(struct sock_cgroup_data *skcd)\n{\n\tstruct cgroup *cgroup;\n\n\trcu_read_lock();\n\t \n\tif (in_interrupt()) {\n\t\tcgroup = &cgrp_dfl_root.cgrp;\n\t\tcgroup_get(cgroup);\n\t\tgoto out;\n\t}\n\n\twhile (true) {\n\t\tstruct css_set *cset;\n\n\t\tcset = task_css_set(current);\n\t\tif (likely(cgroup_tryget(cset->dfl_cgrp))) {\n\t\t\tcgroup = cset->dfl_cgrp;\n\t\t\tbreak;\n\t\t}\n\t\tcpu_relax();\n\t}\nout:\n\tskcd->cgroup = cgroup;\n\tcgroup_bpf_get(cgroup);\n\trcu_read_unlock();\n}\n\nvoid cgroup_sk_clone(struct sock_cgroup_data *skcd)\n{\n\tstruct cgroup *cgrp = sock_cgroup_ptr(skcd);\n\n\t \n\tcgroup_get(cgrp);\n\tcgroup_bpf_get(cgrp);\n}\n\nvoid cgroup_sk_free(struct sock_cgroup_data *skcd)\n{\n\tstruct cgroup *cgrp = sock_cgroup_ptr(skcd);\n\n\tcgroup_bpf_put(cgrp);\n\tcgroup_put(cgrp);\n}\n\n#endif\t \n\n#ifdef CONFIG_SYSFS\nstatic ssize_t show_delegatable_files(struct cftype *files, char *buf,\n\t\t\t\t      ssize_t size, const char *prefix)\n{\n\tstruct cftype *cft;\n\tssize_t ret = 0;\n\n\tfor (cft = files; cft && cft->name[0] != '\\0'; cft++) {\n\t\tif (!(cft->flags & CFTYPE_NS_DELEGATABLE))\n\t\t\tcontinue;\n\n\t\tif (prefix)\n\t\t\tret += snprintf(buf + ret, size - ret, \"%s.\", prefix);\n\n\t\tret += snprintf(buf + ret, size - ret, \"%s\\n\", cft->name);\n\n\t\tif (WARN_ON(ret >= size))\n\t\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic ssize_t delegate_show(struct kobject *kobj, struct kobj_attribute *attr,\n\t\t\t      char *buf)\n{\n\tstruct cgroup_subsys *ss;\n\tint ssid;\n\tssize_t ret = 0;\n\n\tret = show_delegatable_files(cgroup_base_files, buf + ret,\n\t\t\t\t     PAGE_SIZE - ret, NULL);\n\tif (cgroup_psi_enabled())\n\t\tret += show_delegatable_files(cgroup_psi_files, buf + ret,\n\t\t\t\t\t      PAGE_SIZE - ret, NULL);\n\n\tfor_each_subsys(ss, ssid)\n\t\tret += show_delegatable_files(ss->dfl_cftypes, buf + ret,\n\t\t\t\t\t      PAGE_SIZE - ret,\n\t\t\t\t\t      cgroup_subsys_name[ssid]);\n\n\treturn ret;\n}\nstatic struct kobj_attribute cgroup_delegate_attr = __ATTR_RO(delegate);\n\nstatic ssize_t features_show(struct kobject *kobj, struct kobj_attribute *attr,\n\t\t\t     char *buf)\n{\n\treturn snprintf(buf, PAGE_SIZE,\n\t\t\t\"nsdelegate\\n\"\n\t\t\t\"favordynmods\\n\"\n\t\t\t\"memory_localevents\\n\"\n\t\t\t\"memory_recursiveprot\\n\");\n}\nstatic struct kobj_attribute cgroup_features_attr = __ATTR_RO(features);\n\nstatic struct attribute *cgroup_sysfs_attrs[] = {\n\t&cgroup_delegate_attr.attr,\n\t&cgroup_features_attr.attr,\n\tNULL,\n};\n\nstatic const struct attribute_group cgroup_sysfs_attr_group = {\n\t.attrs = cgroup_sysfs_attrs,\n\t.name = \"cgroup\",\n};\n\nstatic int __init cgroup_sysfs_init(void)\n{\n\treturn sysfs_create_group(kernel_kobj, &cgroup_sysfs_attr_group);\n}\nsubsys_initcall(cgroup_sysfs_init);\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}