{
  "module_name": "rstat.c",
  "hash_id": "67d011a0adbe987b965bd9f0f0cd8e4a1795e33936a26eeaa86037803450b851",
  "original_prompt": "Ingested from linux-6.6.14/kernel/cgroup/rstat.c",
  "human_readable_source": "\n#include \"cgroup-internal.h\"\n\n#include <linux/sched/cputime.h>\n\n#include <linux/bpf.h>\n#include <linux/btf.h>\n#include <linux/btf_ids.h>\n\nstatic DEFINE_SPINLOCK(cgroup_rstat_lock);\nstatic DEFINE_PER_CPU(raw_spinlock_t, cgroup_rstat_cpu_lock);\n\nstatic void cgroup_base_stat_flush(struct cgroup *cgrp, int cpu);\n\nstatic struct cgroup_rstat_cpu *cgroup_rstat_cpu(struct cgroup *cgrp, int cpu)\n{\n\treturn per_cpu_ptr(cgrp->rstat_cpu, cpu);\n}\n\n \n__bpf_kfunc void cgroup_rstat_updated(struct cgroup *cgrp, int cpu)\n{\n\traw_spinlock_t *cpu_lock = per_cpu_ptr(&cgroup_rstat_cpu_lock, cpu);\n\tunsigned long flags;\n\n\t \n\tif (data_race(cgroup_rstat_cpu(cgrp, cpu)->updated_next))\n\t\treturn;\n\n\traw_spin_lock_irqsave(cpu_lock, flags);\n\n\t \n\twhile (true) {\n\t\tstruct cgroup_rstat_cpu *rstatc = cgroup_rstat_cpu(cgrp, cpu);\n\t\tstruct cgroup *parent = cgroup_parent(cgrp);\n\t\tstruct cgroup_rstat_cpu *prstatc;\n\n\t\t \n\t\tif (rstatc->updated_next)\n\t\t\tbreak;\n\n\t\t \n\t\tif (!parent) {\n\t\t\trstatc->updated_next = cgrp;\n\t\t\tbreak;\n\t\t}\n\n\t\tprstatc = cgroup_rstat_cpu(parent, cpu);\n\t\trstatc->updated_next = prstatc->updated_children;\n\t\tprstatc->updated_children = cgrp;\n\n\t\tcgrp = parent;\n\t}\n\n\traw_spin_unlock_irqrestore(cpu_lock, flags);\n}\n\n \nstatic struct cgroup *cgroup_rstat_cpu_pop_updated(struct cgroup *pos,\n\t\t\t\t\t\t   struct cgroup *root, int cpu)\n{\n\tstruct cgroup_rstat_cpu *rstatc;\n\tstruct cgroup *parent;\n\n\tif (pos == root)\n\t\treturn NULL;\n\n\t \n\tif (!pos) {\n\t\tpos = root;\n\t\t \n\t\tif (!cgroup_rstat_cpu(pos, cpu)->updated_next)\n\t\t\treturn NULL;\n\t} else {\n\t\tpos = cgroup_parent(pos);\n\t}\n\n\t \n\twhile (true) {\n\t\trstatc = cgroup_rstat_cpu(pos, cpu);\n\t\tif (rstatc->updated_children == pos)\n\t\t\tbreak;\n\t\tpos = rstatc->updated_children;\n\t}\n\n\t \n\tparent = cgroup_parent(pos);\n\tif (parent) {\n\t\tstruct cgroup_rstat_cpu *prstatc;\n\t\tstruct cgroup **nextp;\n\n\t\tprstatc = cgroup_rstat_cpu(parent, cpu);\n\t\tnextp = &prstatc->updated_children;\n\t\twhile (*nextp != pos) {\n\t\t\tstruct cgroup_rstat_cpu *nrstatc;\n\n\t\t\tnrstatc = cgroup_rstat_cpu(*nextp, cpu);\n\t\t\tWARN_ON_ONCE(*nextp == parent);\n\t\t\tnextp = &nrstatc->updated_next;\n\t\t}\n\t\t*nextp = rstatc->updated_next;\n\t}\n\n\trstatc->updated_next = NULL;\n\treturn pos;\n}\n\n \n__diag_push();\n__diag_ignore_all(\"-Wmissing-prototypes\",\n\t\t  \"kfuncs which will be used in BPF programs\");\n\n__weak noinline void bpf_rstat_flush(struct cgroup *cgrp,\n\t\t\t\t     struct cgroup *parent, int cpu)\n{\n}\n\n__diag_pop();\n\n \nstatic void cgroup_rstat_flush_locked(struct cgroup *cgrp)\n\t__releases(&cgroup_rstat_lock) __acquires(&cgroup_rstat_lock)\n{\n\tint cpu;\n\n\tlockdep_assert_held(&cgroup_rstat_lock);\n\n\tfor_each_possible_cpu(cpu) {\n\t\traw_spinlock_t *cpu_lock = per_cpu_ptr(&cgroup_rstat_cpu_lock,\n\t\t\t\t\t\t       cpu);\n\t\tstruct cgroup *pos = NULL;\n\t\tunsigned long flags;\n\n\t\t \n\t\traw_spin_lock_irqsave(cpu_lock, flags);\n\t\twhile ((pos = cgroup_rstat_cpu_pop_updated(pos, cgrp, cpu))) {\n\t\t\tstruct cgroup_subsys_state *css;\n\n\t\t\tcgroup_base_stat_flush(pos, cpu);\n\t\t\tbpf_rstat_flush(pos, cgroup_parent(pos), cpu);\n\n\t\t\trcu_read_lock();\n\t\t\tlist_for_each_entry_rcu(css, &pos->rstat_css_list,\n\t\t\t\t\t\trstat_css_node)\n\t\t\t\tcss->ss->css_rstat_flush(css, cpu);\n\t\t\trcu_read_unlock();\n\t\t}\n\t\traw_spin_unlock_irqrestore(cpu_lock, flags);\n\n\t\t \n\t\tif (need_resched() || spin_needbreak(&cgroup_rstat_lock)) {\n\t\t\tspin_unlock_irq(&cgroup_rstat_lock);\n\t\t\tif (!cond_resched())\n\t\t\t\tcpu_relax();\n\t\t\tspin_lock_irq(&cgroup_rstat_lock);\n\t\t}\n\t}\n}\n\n \n__bpf_kfunc void cgroup_rstat_flush(struct cgroup *cgrp)\n{\n\tmight_sleep();\n\n\tspin_lock_irq(&cgroup_rstat_lock);\n\tcgroup_rstat_flush_locked(cgrp);\n\tspin_unlock_irq(&cgroup_rstat_lock);\n}\n\n \nvoid cgroup_rstat_flush_hold(struct cgroup *cgrp)\n\t__acquires(&cgroup_rstat_lock)\n{\n\tmight_sleep();\n\tspin_lock_irq(&cgroup_rstat_lock);\n\tcgroup_rstat_flush_locked(cgrp);\n}\n\n \nvoid cgroup_rstat_flush_release(void)\n\t__releases(&cgroup_rstat_lock)\n{\n\tspin_unlock_irq(&cgroup_rstat_lock);\n}\n\nint cgroup_rstat_init(struct cgroup *cgrp)\n{\n\tint cpu;\n\n\t \n\tif (!cgrp->rstat_cpu) {\n\t\tcgrp->rstat_cpu = alloc_percpu(struct cgroup_rstat_cpu);\n\t\tif (!cgrp->rstat_cpu)\n\t\t\treturn -ENOMEM;\n\t}\n\n\t \n\tfor_each_possible_cpu(cpu) {\n\t\tstruct cgroup_rstat_cpu *rstatc = cgroup_rstat_cpu(cgrp, cpu);\n\n\t\trstatc->updated_children = cgrp;\n\t\tu64_stats_init(&rstatc->bsync);\n\t}\n\n\treturn 0;\n}\n\nvoid cgroup_rstat_exit(struct cgroup *cgrp)\n{\n\tint cpu;\n\n\tcgroup_rstat_flush(cgrp);\n\n\t \n\tfor_each_possible_cpu(cpu) {\n\t\tstruct cgroup_rstat_cpu *rstatc = cgroup_rstat_cpu(cgrp, cpu);\n\n\t\tif (WARN_ON_ONCE(rstatc->updated_children != cgrp) ||\n\t\t    WARN_ON_ONCE(rstatc->updated_next))\n\t\t\treturn;\n\t}\n\n\tfree_percpu(cgrp->rstat_cpu);\n\tcgrp->rstat_cpu = NULL;\n}\n\nvoid __init cgroup_rstat_boot(void)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu)\n\t\traw_spin_lock_init(per_cpu_ptr(&cgroup_rstat_cpu_lock, cpu));\n}\n\n \nstatic void cgroup_base_stat_add(struct cgroup_base_stat *dst_bstat,\n\t\t\t\t struct cgroup_base_stat *src_bstat)\n{\n\tdst_bstat->cputime.utime += src_bstat->cputime.utime;\n\tdst_bstat->cputime.stime += src_bstat->cputime.stime;\n\tdst_bstat->cputime.sum_exec_runtime += src_bstat->cputime.sum_exec_runtime;\n#ifdef CONFIG_SCHED_CORE\n\tdst_bstat->forceidle_sum += src_bstat->forceidle_sum;\n#endif\n}\n\nstatic void cgroup_base_stat_sub(struct cgroup_base_stat *dst_bstat,\n\t\t\t\t struct cgroup_base_stat *src_bstat)\n{\n\tdst_bstat->cputime.utime -= src_bstat->cputime.utime;\n\tdst_bstat->cputime.stime -= src_bstat->cputime.stime;\n\tdst_bstat->cputime.sum_exec_runtime -= src_bstat->cputime.sum_exec_runtime;\n#ifdef CONFIG_SCHED_CORE\n\tdst_bstat->forceidle_sum -= src_bstat->forceidle_sum;\n#endif\n}\n\nstatic void cgroup_base_stat_flush(struct cgroup *cgrp, int cpu)\n{\n\tstruct cgroup_rstat_cpu *rstatc = cgroup_rstat_cpu(cgrp, cpu);\n\tstruct cgroup *parent = cgroup_parent(cgrp);\n\tstruct cgroup_rstat_cpu *prstatc;\n\tstruct cgroup_base_stat delta;\n\tunsigned seq;\n\n\t \n\tif (!parent)\n\t\treturn;\n\n\t \n\tdo {\n\t\tseq = __u64_stats_fetch_begin(&rstatc->bsync);\n\t\tdelta = rstatc->bstat;\n\t} while (__u64_stats_fetch_retry(&rstatc->bsync, seq));\n\n\t \n\tcgroup_base_stat_sub(&delta, &rstatc->last_bstat);\n\tcgroup_base_stat_add(&cgrp->bstat, &delta);\n\tcgroup_base_stat_add(&rstatc->last_bstat, &delta);\n\tcgroup_base_stat_add(&rstatc->subtree_bstat, &delta);\n\n\t \n\tif (cgroup_parent(parent)) {\n\t\tdelta = cgrp->bstat;\n\t\tcgroup_base_stat_sub(&delta, &cgrp->last_bstat);\n\t\tcgroup_base_stat_add(&parent->bstat, &delta);\n\t\tcgroup_base_stat_add(&cgrp->last_bstat, &delta);\n\n\t\tdelta = rstatc->subtree_bstat;\n\t\tprstatc = cgroup_rstat_cpu(parent, cpu);\n\t\tcgroup_base_stat_sub(&delta, &rstatc->last_subtree_bstat);\n\t\tcgroup_base_stat_add(&prstatc->subtree_bstat, &delta);\n\t\tcgroup_base_stat_add(&rstatc->last_subtree_bstat, &delta);\n\t}\n}\n\nstatic struct cgroup_rstat_cpu *\ncgroup_base_stat_cputime_account_begin(struct cgroup *cgrp, unsigned long *flags)\n{\n\tstruct cgroup_rstat_cpu *rstatc;\n\n\trstatc = get_cpu_ptr(cgrp->rstat_cpu);\n\t*flags = u64_stats_update_begin_irqsave(&rstatc->bsync);\n\treturn rstatc;\n}\n\nstatic void cgroup_base_stat_cputime_account_end(struct cgroup *cgrp,\n\t\t\t\t\t\t struct cgroup_rstat_cpu *rstatc,\n\t\t\t\t\t\t unsigned long flags)\n{\n\tu64_stats_update_end_irqrestore(&rstatc->bsync, flags);\n\tcgroup_rstat_updated(cgrp, smp_processor_id());\n\tput_cpu_ptr(rstatc);\n}\n\nvoid __cgroup_account_cputime(struct cgroup *cgrp, u64 delta_exec)\n{\n\tstruct cgroup_rstat_cpu *rstatc;\n\tunsigned long flags;\n\n\trstatc = cgroup_base_stat_cputime_account_begin(cgrp, &flags);\n\trstatc->bstat.cputime.sum_exec_runtime += delta_exec;\n\tcgroup_base_stat_cputime_account_end(cgrp, rstatc, flags);\n}\n\nvoid __cgroup_account_cputime_field(struct cgroup *cgrp,\n\t\t\t\t    enum cpu_usage_stat index, u64 delta_exec)\n{\n\tstruct cgroup_rstat_cpu *rstatc;\n\tunsigned long flags;\n\n\trstatc = cgroup_base_stat_cputime_account_begin(cgrp, &flags);\n\n\tswitch (index) {\n\tcase CPUTIME_USER:\n\tcase CPUTIME_NICE:\n\t\trstatc->bstat.cputime.utime += delta_exec;\n\t\tbreak;\n\tcase CPUTIME_SYSTEM:\n\tcase CPUTIME_IRQ:\n\tcase CPUTIME_SOFTIRQ:\n\t\trstatc->bstat.cputime.stime += delta_exec;\n\t\tbreak;\n#ifdef CONFIG_SCHED_CORE\n\tcase CPUTIME_FORCEIDLE:\n\t\trstatc->bstat.forceidle_sum += delta_exec;\n\t\tbreak;\n#endif\n\tdefault:\n\t\tbreak;\n\t}\n\n\tcgroup_base_stat_cputime_account_end(cgrp, rstatc, flags);\n}\n\n \nstatic void root_cgroup_cputime(struct cgroup_base_stat *bstat)\n{\n\tstruct task_cputime *cputime = &bstat->cputime;\n\tint i;\n\n\tmemset(bstat, 0, sizeof(*bstat));\n\tfor_each_possible_cpu(i) {\n\t\tstruct kernel_cpustat kcpustat;\n\t\tu64 *cpustat = kcpustat.cpustat;\n\t\tu64 user = 0;\n\t\tu64 sys = 0;\n\n\t\tkcpustat_cpu_fetch(&kcpustat, i);\n\n\t\tuser += cpustat[CPUTIME_USER];\n\t\tuser += cpustat[CPUTIME_NICE];\n\t\tcputime->utime += user;\n\n\t\tsys += cpustat[CPUTIME_SYSTEM];\n\t\tsys += cpustat[CPUTIME_IRQ];\n\t\tsys += cpustat[CPUTIME_SOFTIRQ];\n\t\tcputime->stime += sys;\n\n\t\tcputime->sum_exec_runtime += user;\n\t\tcputime->sum_exec_runtime += sys;\n\t\tcputime->sum_exec_runtime += cpustat[CPUTIME_STEAL];\n\n#ifdef CONFIG_SCHED_CORE\n\t\tbstat->forceidle_sum += cpustat[CPUTIME_FORCEIDLE];\n#endif\n\t}\n}\n\nvoid cgroup_base_stat_cputime_show(struct seq_file *seq)\n{\n\tstruct cgroup *cgrp = seq_css(seq)->cgroup;\n\tu64 usage, utime, stime;\n\tstruct cgroup_base_stat bstat;\n#ifdef CONFIG_SCHED_CORE\n\tu64 forceidle_time;\n#endif\n\n\tif (cgroup_parent(cgrp)) {\n\t\tcgroup_rstat_flush_hold(cgrp);\n\t\tusage = cgrp->bstat.cputime.sum_exec_runtime;\n\t\tcputime_adjust(&cgrp->bstat.cputime, &cgrp->prev_cputime,\n\t\t\t       &utime, &stime);\n#ifdef CONFIG_SCHED_CORE\n\t\tforceidle_time = cgrp->bstat.forceidle_sum;\n#endif\n\t\tcgroup_rstat_flush_release();\n\t} else {\n\t\troot_cgroup_cputime(&bstat);\n\t\tusage = bstat.cputime.sum_exec_runtime;\n\t\tutime = bstat.cputime.utime;\n\t\tstime = bstat.cputime.stime;\n#ifdef CONFIG_SCHED_CORE\n\t\tforceidle_time = bstat.forceidle_sum;\n#endif\n\t}\n\n\tdo_div(usage, NSEC_PER_USEC);\n\tdo_div(utime, NSEC_PER_USEC);\n\tdo_div(stime, NSEC_PER_USEC);\n#ifdef CONFIG_SCHED_CORE\n\tdo_div(forceidle_time, NSEC_PER_USEC);\n#endif\n\n\tseq_printf(seq, \"usage_usec %llu\\n\"\n\t\t   \"user_usec %llu\\n\"\n\t\t   \"system_usec %llu\\n\",\n\t\t   usage, utime, stime);\n\n#ifdef CONFIG_SCHED_CORE\n\tseq_printf(seq, \"core_sched.force_idle_usec %llu\\n\", forceidle_time);\n#endif\n}\n\n \nBTF_SET8_START(bpf_rstat_kfunc_ids)\nBTF_ID_FLAGS(func, cgroup_rstat_updated)\nBTF_ID_FLAGS(func, cgroup_rstat_flush, KF_SLEEPABLE)\nBTF_SET8_END(bpf_rstat_kfunc_ids)\n\nstatic const struct btf_kfunc_id_set bpf_rstat_kfunc_set = {\n\t.owner          = THIS_MODULE,\n\t.set            = &bpf_rstat_kfunc_ids,\n};\n\nstatic int __init bpf_rstat_kfunc_init(void)\n{\n\treturn register_btf_kfunc_id_set(BPF_PROG_TYPE_TRACING,\n\t\t\t\t\t &bpf_rstat_kfunc_set);\n}\nlate_initcall(bpf_rstat_kfunc_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}