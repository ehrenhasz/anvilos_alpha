{
  "module_name": "cpuset.c",
  "hash_id": "87ed8ae9a59b5ad363ba95c0520167b7a9e727ff80d41d21032cc688afe72e7b",
  "original_prompt": "Ingested from linux-6.6.14/kernel/cgroup/cpuset.c",
  "human_readable_source": " \n\n#include <linux/cpu.h>\n#include <linux/cpumask.h>\n#include <linux/cpuset.h>\n#include <linux/init.h>\n#include <linux/interrupt.h>\n#include <linux/kernel.h>\n#include <linux/mempolicy.h>\n#include <linux/mm.h>\n#include <linux/memory.h>\n#include <linux/export.h>\n#include <linux/rcupdate.h>\n#include <linux/sched.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/task.h>\n#include <linux/security.h>\n#include <linux/spinlock.h>\n#include <linux/oom.h>\n#include <linux/sched/isolation.h>\n#include <linux/cgroup.h>\n#include <linux/wait.h>\n\nDEFINE_STATIC_KEY_FALSE(cpusets_pre_enable_key);\nDEFINE_STATIC_KEY_FALSE(cpusets_enabled_key);\n\n \nDEFINE_STATIC_KEY_FALSE(cpusets_insane_config_key);\n\n \n\nstruct fmeter {\n\tint cnt;\t\t \n\tint val;\t\t \n\ttime64_t time;\t\t \n\tspinlock_t lock;\t \n};\n\n \nenum prs_errcode {\n\tPERR_NONE = 0,\n\tPERR_INVCPUS,\n\tPERR_INVPARENT,\n\tPERR_NOTPART,\n\tPERR_NOTEXCL,\n\tPERR_NOCPUS,\n\tPERR_HOTPLUG,\n\tPERR_CPUSEMPTY,\n};\n\nstatic const char * const perr_strings[] = {\n\t[PERR_INVCPUS]   = \"Invalid cpu list in cpuset.cpus\",\n\t[PERR_INVPARENT] = \"Parent is an invalid partition root\",\n\t[PERR_NOTPART]   = \"Parent is not a partition root\",\n\t[PERR_NOTEXCL]   = \"Cpu list in cpuset.cpus not exclusive\",\n\t[PERR_NOCPUS]    = \"Parent unable to distribute cpu downstream\",\n\t[PERR_HOTPLUG]   = \"No cpu available due to hotplug\",\n\t[PERR_CPUSEMPTY] = \"cpuset.cpus is empty\",\n};\n\nstruct cpuset {\n\tstruct cgroup_subsys_state css;\n\n\tunsigned long flags;\t\t \n\n\t \n\n\t \n\tcpumask_var_t cpus_allowed;\n\tnodemask_t mems_allowed;\n\n\t \n\tcpumask_var_t effective_cpus;\n\tnodemask_t effective_mems;\n\n\t \n\tcpumask_var_t subparts_cpus;\n\n\t \n\tnodemask_t old_mems_allowed;\n\n\tstruct fmeter fmeter;\t\t \n\n\t \n\tint attach_in_progress;\n\n\t \n\tint pn;\n\n\t \n\tint relax_domain_level;\n\n\t \n\tint nr_subparts_cpus;\n\n\t \n\tint partition_root_state;\n\n\t \n\tint use_parent_ecpus;\n\tint child_ecpus_count;\n\n\t \n\tint nr_deadline_tasks;\n\tint nr_migrate_dl_tasks;\n\tu64 sum_migrate_dl_bw;\n\n\t \n\tenum prs_errcode prs_err;\n\n\t \n\tstruct cgroup_file partition_file;\n};\n\n \n#define PRS_MEMBER\t\t0\n#define PRS_ROOT\t\t1\n#define PRS_ISOLATED\t\t2\n#define PRS_INVALID_ROOT\t-1\n#define PRS_INVALID_ISOLATED\t-2\n\nstatic inline bool is_prs_invalid(int prs_state)\n{\n\treturn prs_state < 0;\n}\n\n \nstruct tmpmasks {\n\tcpumask_var_t addmask, delmask;\t \n\tcpumask_var_t new_cpus;\t\t \n};\n\nstatic inline struct cpuset *css_cs(struct cgroup_subsys_state *css)\n{\n\treturn css ? container_of(css, struct cpuset, css) : NULL;\n}\n\n \nstatic inline struct cpuset *task_cs(struct task_struct *task)\n{\n\treturn css_cs(task_css(task, cpuset_cgrp_id));\n}\n\nstatic inline struct cpuset *parent_cs(struct cpuset *cs)\n{\n\treturn css_cs(cs->css.parent);\n}\n\nvoid inc_dl_tasks_cs(struct task_struct *p)\n{\n\tstruct cpuset *cs = task_cs(p);\n\n\tcs->nr_deadline_tasks++;\n}\n\nvoid dec_dl_tasks_cs(struct task_struct *p)\n{\n\tstruct cpuset *cs = task_cs(p);\n\n\tcs->nr_deadline_tasks--;\n}\n\n \ntypedef enum {\n\tCS_ONLINE,\n\tCS_CPU_EXCLUSIVE,\n\tCS_MEM_EXCLUSIVE,\n\tCS_MEM_HARDWALL,\n\tCS_MEMORY_MIGRATE,\n\tCS_SCHED_LOAD_BALANCE,\n\tCS_SPREAD_PAGE,\n\tCS_SPREAD_SLAB,\n} cpuset_flagbits_t;\n\n \nstatic inline bool is_cpuset_online(struct cpuset *cs)\n{\n\treturn test_bit(CS_ONLINE, &cs->flags) && !css_is_dying(&cs->css);\n}\n\nstatic inline int is_cpu_exclusive(const struct cpuset *cs)\n{\n\treturn test_bit(CS_CPU_EXCLUSIVE, &cs->flags);\n}\n\nstatic inline int is_mem_exclusive(const struct cpuset *cs)\n{\n\treturn test_bit(CS_MEM_EXCLUSIVE, &cs->flags);\n}\n\nstatic inline int is_mem_hardwall(const struct cpuset *cs)\n{\n\treturn test_bit(CS_MEM_HARDWALL, &cs->flags);\n}\n\nstatic inline int is_sched_load_balance(const struct cpuset *cs)\n{\n\treturn test_bit(CS_SCHED_LOAD_BALANCE, &cs->flags);\n}\n\nstatic inline int is_memory_migrate(const struct cpuset *cs)\n{\n\treturn test_bit(CS_MEMORY_MIGRATE, &cs->flags);\n}\n\nstatic inline int is_spread_page(const struct cpuset *cs)\n{\n\treturn test_bit(CS_SPREAD_PAGE, &cs->flags);\n}\n\nstatic inline int is_spread_slab(const struct cpuset *cs)\n{\n\treturn test_bit(CS_SPREAD_SLAB, &cs->flags);\n}\n\nstatic inline int is_partition_valid(const struct cpuset *cs)\n{\n\treturn cs->partition_root_state > 0;\n}\n\nstatic inline int is_partition_invalid(const struct cpuset *cs)\n{\n\treturn cs->partition_root_state < 0;\n}\n\n \nstatic inline void make_partition_invalid(struct cpuset *cs)\n{\n\tif (is_partition_valid(cs))\n\t\tcs->partition_root_state = -cs->partition_root_state;\n}\n\n \nstatic inline void notify_partition_change(struct cpuset *cs, int old_prs)\n{\n\tif (old_prs == cs->partition_root_state)\n\t\treturn;\n\tcgroup_file_notify(&cs->partition_file);\n\n\t \n\tif (is_partition_valid(cs))\n\t\tWRITE_ONCE(cs->prs_err, PERR_NONE);\n}\n\nstatic struct cpuset top_cpuset = {\n\t.flags = ((1 << CS_ONLINE) | (1 << CS_CPU_EXCLUSIVE) |\n\t\t  (1 << CS_MEM_EXCLUSIVE)),\n\t.partition_root_state = PRS_ROOT,\n};\n\n \n#define cpuset_for_each_child(child_cs, pos_css, parent_cs)\t\t\\\n\tcss_for_each_child((pos_css), &(parent_cs)->css)\t\t\\\n\t\tif (is_cpuset_online(((child_cs) = css_cs((pos_css)))))\n\n \n#define cpuset_for_each_descendant_pre(des_cs, pos_css, root_cs)\t\\\n\tcss_for_each_descendant_pre((pos_css), &(root_cs)->css)\t\t\\\n\t\tif (is_cpuset_online(((des_cs) = css_cs((pos_css)))))\n\n \n\nstatic DEFINE_MUTEX(cpuset_mutex);\n\nvoid cpuset_lock(void)\n{\n\tmutex_lock(&cpuset_mutex);\n}\n\nvoid cpuset_unlock(void)\n{\n\tmutex_unlock(&cpuset_mutex);\n}\n\nstatic DEFINE_SPINLOCK(callback_lock);\n\nstatic struct workqueue_struct *cpuset_migrate_mm_wq;\n\n \nstatic void cpuset_hotplug_workfn(struct work_struct *work);\nstatic DECLARE_WORK(cpuset_hotplug_work, cpuset_hotplug_workfn);\n\nstatic DECLARE_WAIT_QUEUE_HEAD(cpuset_attach_wq);\n\nstatic inline void check_insane_mems_config(nodemask_t *nodes)\n{\n\tif (!cpusets_insane_config() &&\n\t\tmovable_only_nodes(nodes)) {\n\t\tstatic_branch_enable(&cpusets_insane_config_key);\n\t\tpr_info(\"Unsupported (movable nodes only) cpuset configuration detected (nmask=%*pbl)!\\n\"\n\t\t\t\"Cpuset allocations might fail even with a lot of memory available.\\n\",\n\t\t\tnodemask_pr_args(nodes));\n\t}\n}\n\n \nstatic inline bool is_in_v2_mode(void)\n{\n\treturn cgroup_subsys_on_dfl(cpuset_cgrp_subsys) ||\n\t      (cpuset_cgrp_subsys.root->flags & CGRP_ROOT_CPUSET_V2_MODE);\n}\n\n \nstatic inline bool partition_is_populated(struct cpuset *cs,\n\t\t\t\t\t  struct cpuset *excluded_child)\n{\n\tstruct cgroup_subsys_state *css;\n\tstruct cpuset *child;\n\n\tif (cs->css.cgroup->nr_populated_csets)\n\t\treturn true;\n\tif (!excluded_child && !cs->nr_subparts_cpus)\n\t\treturn cgroup_is_populated(cs->css.cgroup);\n\n\trcu_read_lock();\n\tcpuset_for_each_child(child, css, cs) {\n\t\tif (child == excluded_child)\n\t\t\tcontinue;\n\t\tif (is_partition_valid(child))\n\t\t\tcontinue;\n\t\tif (cgroup_is_populated(child->css.cgroup)) {\n\t\t\trcu_read_unlock();\n\t\t\treturn true;\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn false;\n}\n\n \nstatic void guarantee_online_cpus(struct task_struct *tsk,\n\t\t\t\t  struct cpumask *pmask)\n{\n\tconst struct cpumask *possible_mask = task_cpu_possible_mask(tsk);\n\tstruct cpuset *cs;\n\n\tif (WARN_ON(!cpumask_and(pmask, possible_mask, cpu_online_mask)))\n\t\tcpumask_copy(pmask, cpu_online_mask);\n\n\trcu_read_lock();\n\tcs = task_cs(tsk);\n\n\twhile (!cpumask_intersects(cs->effective_cpus, pmask)) {\n\t\tcs = parent_cs(cs);\n\t\tif (unlikely(!cs)) {\n\t\t\t \n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\tcpumask_and(pmask, pmask, cs->effective_cpus);\n\nout_unlock:\n\trcu_read_unlock();\n}\n\n \nstatic void guarantee_online_mems(struct cpuset *cs, nodemask_t *pmask)\n{\n\twhile (!nodes_intersects(cs->effective_mems, node_states[N_MEMORY]))\n\t\tcs = parent_cs(cs);\n\tnodes_and(*pmask, cs->effective_mems, node_states[N_MEMORY]);\n}\n\n \nstatic void cpuset_update_task_spread_flags(struct cpuset *cs,\n\t\t\t\t\tstruct task_struct *tsk)\n{\n\tif (cgroup_subsys_on_dfl(cpuset_cgrp_subsys))\n\t\treturn;\n\n\tif (is_spread_page(cs))\n\t\ttask_set_spread_page(tsk);\n\telse\n\t\ttask_clear_spread_page(tsk);\n\n\tif (is_spread_slab(cs))\n\t\ttask_set_spread_slab(tsk);\n\telse\n\t\ttask_clear_spread_slab(tsk);\n}\n\n \n\nstatic int is_cpuset_subset(const struct cpuset *p, const struct cpuset *q)\n{\n\treturn\tcpumask_subset(p->cpus_allowed, q->cpus_allowed) &&\n\t\tnodes_subset(p->mems_allowed, q->mems_allowed) &&\n\t\tis_cpu_exclusive(p) <= is_cpu_exclusive(q) &&\n\t\tis_mem_exclusive(p) <= is_mem_exclusive(q);\n}\n\n \nstatic inline int alloc_cpumasks(struct cpuset *cs, struct tmpmasks *tmp)\n{\n\tcpumask_var_t *pmask1, *pmask2, *pmask3;\n\n\tif (cs) {\n\t\tpmask1 = &cs->cpus_allowed;\n\t\tpmask2 = &cs->effective_cpus;\n\t\tpmask3 = &cs->subparts_cpus;\n\t} else {\n\t\tpmask1 = &tmp->new_cpus;\n\t\tpmask2 = &tmp->addmask;\n\t\tpmask3 = &tmp->delmask;\n\t}\n\n\tif (!zalloc_cpumask_var(pmask1, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\tif (!zalloc_cpumask_var(pmask2, GFP_KERNEL))\n\t\tgoto free_one;\n\n\tif (!zalloc_cpumask_var(pmask3, GFP_KERNEL))\n\t\tgoto free_two;\n\n\treturn 0;\n\nfree_two:\n\tfree_cpumask_var(*pmask2);\nfree_one:\n\tfree_cpumask_var(*pmask1);\n\treturn -ENOMEM;\n}\n\n \nstatic inline void free_cpumasks(struct cpuset *cs, struct tmpmasks *tmp)\n{\n\tif (cs) {\n\t\tfree_cpumask_var(cs->cpus_allowed);\n\t\tfree_cpumask_var(cs->effective_cpus);\n\t\tfree_cpumask_var(cs->subparts_cpus);\n\t}\n\tif (tmp) {\n\t\tfree_cpumask_var(tmp->new_cpus);\n\t\tfree_cpumask_var(tmp->addmask);\n\t\tfree_cpumask_var(tmp->delmask);\n\t}\n}\n\n \nstatic struct cpuset *alloc_trial_cpuset(struct cpuset *cs)\n{\n\tstruct cpuset *trial;\n\n\ttrial = kmemdup(cs, sizeof(*cs), GFP_KERNEL);\n\tif (!trial)\n\t\treturn NULL;\n\n\tif (alloc_cpumasks(trial, NULL)) {\n\t\tkfree(trial);\n\t\treturn NULL;\n\t}\n\n\tcpumask_copy(trial->cpus_allowed, cs->cpus_allowed);\n\tcpumask_copy(trial->effective_cpus, cs->effective_cpus);\n\treturn trial;\n}\n\n \nstatic inline void free_cpuset(struct cpuset *cs)\n{\n\tfree_cpumasks(cs, NULL);\n\tkfree(cs);\n}\n\n \nstatic int validate_change_legacy(struct cpuset *cur, struct cpuset *trial)\n{\n\tstruct cgroup_subsys_state *css;\n\tstruct cpuset *c, *par;\n\tint ret;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\n\t \n\tret = -EBUSY;\n\tcpuset_for_each_child(c, css, cur)\n\t\tif (!is_cpuset_subset(c, trial))\n\t\t\tgoto out;\n\n\t \n\tret = -EACCES;\n\tpar = parent_cs(cur);\n\tif (par && !is_cpuset_subset(trial, par))\n\t\tgoto out;\n\n\tret = 0;\nout:\n\treturn ret;\n}\n\n \n\nstatic int validate_change(struct cpuset *cur, struct cpuset *trial)\n{\n\tstruct cgroup_subsys_state *css;\n\tstruct cpuset *c, *par;\n\tint ret = 0;\n\n\trcu_read_lock();\n\n\tif (!is_in_v2_mode())\n\t\tret = validate_change_legacy(cur, trial);\n\tif (ret)\n\t\tgoto out;\n\n\t \n\tif (cur == &top_cpuset)\n\t\tgoto out;\n\n\tpar = parent_cs(cur);\n\n\t \n\tret = -ENOSPC;\n\tif ((cgroup_is_populated(cur->css.cgroup) || cur->attach_in_progress)) {\n\t\tif (!cpumask_empty(cur->cpus_allowed) &&\n\t\t    cpumask_empty(trial->cpus_allowed))\n\t\t\tgoto out;\n\t\tif (!nodes_empty(cur->mems_allowed) &&\n\t\t    nodes_empty(trial->mems_allowed))\n\t\t\tgoto out;\n\t}\n\n\t \n\tret = -EBUSY;\n\tif (is_cpu_exclusive(cur) &&\n\t    !cpuset_cpumask_can_shrink(cur->cpus_allowed,\n\t\t\t\t       trial->cpus_allowed))\n\t\tgoto out;\n\n\t \n\tret = -EINVAL;\n\tcpuset_for_each_child(c, css, par) {\n\t\tif ((is_cpu_exclusive(trial) || is_cpu_exclusive(c)) &&\n\t\t    c != cur &&\n\t\t    cpumask_intersects(trial->cpus_allowed, c->cpus_allowed))\n\t\t\tgoto out;\n\t\tif ((is_mem_exclusive(trial) || is_mem_exclusive(c)) &&\n\t\t    c != cur &&\n\t\t    nodes_intersects(trial->mems_allowed, c->mems_allowed))\n\t\t\tgoto out;\n\t}\n\n\tret = 0;\nout:\n\trcu_read_unlock();\n\treturn ret;\n}\n\n#ifdef CONFIG_SMP\n \nstatic int cpusets_overlap(struct cpuset *a, struct cpuset *b)\n{\n\treturn cpumask_intersects(a->effective_cpus, b->effective_cpus);\n}\n\nstatic void\nupdate_domain_attr(struct sched_domain_attr *dattr, struct cpuset *c)\n{\n\tif (dattr->relax_domain_level < c->relax_domain_level)\n\t\tdattr->relax_domain_level = c->relax_domain_level;\n\treturn;\n}\n\nstatic void update_domain_attr_tree(struct sched_domain_attr *dattr,\n\t\t\t\t    struct cpuset *root_cs)\n{\n\tstruct cpuset *cp;\n\tstruct cgroup_subsys_state *pos_css;\n\n\trcu_read_lock();\n\tcpuset_for_each_descendant_pre(cp, pos_css, root_cs) {\n\t\t \n\t\tif (cpumask_empty(cp->cpus_allowed)) {\n\t\t\tpos_css = css_rightmost_descendant(pos_css);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (is_sched_load_balance(cp))\n\t\t\tupdate_domain_attr(dattr, cp);\n\t}\n\trcu_read_unlock();\n}\n\n \nstatic inline int nr_cpusets(void)\n{\n\t \n\treturn static_key_count(&cpusets_enabled_key.key) + 1;\n}\n\n \nstatic int generate_sched_domains(cpumask_var_t **domains,\n\t\t\tstruct sched_domain_attr **attributes)\n{\n\tstruct cpuset *cp;\t \n\tstruct cpuset **csa;\t \n\tint csn;\t\t \n\tint i, j, k;\t\t \n\tcpumask_var_t *doms;\t \n\tstruct sched_domain_attr *dattr;   \n\tint ndoms = 0;\t\t \n\tint nslot;\t\t \n\tstruct cgroup_subsys_state *pos_css;\n\tbool root_load_balance = is_sched_load_balance(&top_cpuset);\n\n\tdoms = NULL;\n\tdattr = NULL;\n\tcsa = NULL;\n\n\t \n\tif (root_load_balance && !top_cpuset.nr_subparts_cpus) {\n\t\tndoms = 1;\n\t\tdoms = alloc_sched_domains(ndoms);\n\t\tif (!doms)\n\t\t\tgoto done;\n\n\t\tdattr = kmalloc(sizeof(struct sched_domain_attr), GFP_KERNEL);\n\t\tif (dattr) {\n\t\t\t*dattr = SD_ATTR_INIT;\n\t\t\tupdate_domain_attr_tree(dattr, &top_cpuset);\n\t\t}\n\t\tcpumask_and(doms[0], top_cpuset.effective_cpus,\n\t\t\t    housekeeping_cpumask(HK_TYPE_DOMAIN));\n\n\t\tgoto done;\n\t}\n\n\tcsa = kmalloc_array(nr_cpusets(), sizeof(cp), GFP_KERNEL);\n\tif (!csa)\n\t\tgoto done;\n\tcsn = 0;\n\n\trcu_read_lock();\n\tif (root_load_balance)\n\t\tcsa[csn++] = &top_cpuset;\n\tcpuset_for_each_descendant_pre(cp, pos_css, &top_cpuset) {\n\t\tif (cp == &top_cpuset)\n\t\t\tcontinue;\n\t\t \n\t\tif (!cpumask_empty(cp->cpus_allowed) &&\n\t\t    !(is_sched_load_balance(cp) &&\n\t\t      cpumask_intersects(cp->cpus_allowed,\n\t\t\t\t\t housekeeping_cpumask(HK_TYPE_DOMAIN))))\n\t\t\tcontinue;\n\n\t\tif (root_load_balance &&\n\t\t    cpumask_subset(cp->cpus_allowed, top_cpuset.effective_cpus))\n\t\t\tcontinue;\n\n\t\tif (is_sched_load_balance(cp) &&\n\t\t    !cpumask_empty(cp->effective_cpus))\n\t\t\tcsa[csn++] = cp;\n\n\t\t \n\t\tif (!is_partition_valid(cp))\n\t\t\tpos_css = css_rightmost_descendant(pos_css);\n\t}\n\trcu_read_unlock();\n\n\tfor (i = 0; i < csn; i++)\n\t\tcsa[i]->pn = i;\n\tndoms = csn;\n\nrestart:\n\t \n\tfor (i = 0; i < csn; i++) {\n\t\tstruct cpuset *a = csa[i];\n\t\tint apn = a->pn;\n\n\t\tfor (j = 0; j < csn; j++) {\n\t\t\tstruct cpuset *b = csa[j];\n\t\t\tint bpn = b->pn;\n\n\t\t\tif (apn != bpn && cpusets_overlap(a, b)) {\n\t\t\t\tfor (k = 0; k < csn; k++) {\n\t\t\t\t\tstruct cpuset *c = csa[k];\n\n\t\t\t\t\tif (c->pn == bpn)\n\t\t\t\t\t\tc->pn = apn;\n\t\t\t\t}\n\t\t\t\tndoms--;\t \n\t\t\t\tgoto restart;\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tdoms = alloc_sched_domains(ndoms);\n\tif (!doms)\n\t\tgoto done;\n\n\t \n\tdattr = kmalloc_array(ndoms, sizeof(struct sched_domain_attr),\n\t\t\t      GFP_KERNEL);\n\n\tfor (nslot = 0, i = 0; i < csn; i++) {\n\t\tstruct cpuset *a = csa[i];\n\t\tstruct cpumask *dp;\n\t\tint apn = a->pn;\n\n\t\tif (apn < 0) {\n\t\t\t \n\t\t\tcontinue;\n\t\t}\n\n\t\tdp = doms[nslot];\n\n\t\tif (nslot == ndoms) {\n\t\t\tstatic int warnings = 10;\n\t\t\tif (warnings) {\n\t\t\t\tpr_warn(\"rebuild_sched_domains confused: nslot %d, ndoms %d, csn %d, i %d, apn %d\\n\",\n\t\t\t\t\tnslot, ndoms, csn, i, apn);\n\t\t\t\twarnings--;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\tcpumask_clear(dp);\n\t\tif (dattr)\n\t\t\t*(dattr + nslot) = SD_ATTR_INIT;\n\t\tfor (j = i; j < csn; j++) {\n\t\t\tstruct cpuset *b = csa[j];\n\n\t\t\tif (apn == b->pn) {\n\t\t\t\tcpumask_or(dp, dp, b->effective_cpus);\n\t\t\t\tcpumask_and(dp, dp, housekeeping_cpumask(HK_TYPE_DOMAIN));\n\t\t\t\tif (dattr)\n\t\t\t\t\tupdate_domain_attr_tree(dattr + nslot, b);\n\n\t\t\t\t \n\t\t\t\tb->pn = -1;\n\t\t\t}\n\t\t}\n\t\tnslot++;\n\t}\n\tBUG_ON(nslot != ndoms);\n\ndone:\n\tkfree(csa);\n\n\t \n\tif (doms == NULL)\n\t\tndoms = 1;\n\n\t*domains    = doms;\n\t*attributes = dattr;\n\treturn ndoms;\n}\n\nstatic void dl_update_tasks_root_domain(struct cpuset *cs)\n{\n\tstruct css_task_iter it;\n\tstruct task_struct *task;\n\n\tif (cs->nr_deadline_tasks == 0)\n\t\treturn;\n\n\tcss_task_iter_start(&cs->css, 0, &it);\n\n\twhile ((task = css_task_iter_next(&it)))\n\t\tdl_add_task_root_domain(task);\n\n\tcss_task_iter_end(&it);\n}\n\nstatic void dl_rebuild_rd_accounting(void)\n{\n\tstruct cpuset *cs = NULL;\n\tstruct cgroup_subsys_state *pos_css;\n\n\tlockdep_assert_held(&cpuset_mutex);\n\tlockdep_assert_cpus_held();\n\tlockdep_assert_held(&sched_domains_mutex);\n\n\trcu_read_lock();\n\n\t \n\tdl_clear_root_domain(&def_root_domain);\n\n\tcpuset_for_each_descendant_pre(cs, pos_css, &top_cpuset) {\n\n\t\tif (cpumask_empty(cs->effective_cpus)) {\n\t\t\tpos_css = css_rightmost_descendant(pos_css);\n\t\t\tcontinue;\n\t\t}\n\n\t\tcss_get(&cs->css);\n\n\t\trcu_read_unlock();\n\n\t\tdl_update_tasks_root_domain(cs);\n\n\t\trcu_read_lock();\n\t\tcss_put(&cs->css);\n\t}\n\trcu_read_unlock();\n}\n\nstatic void\npartition_and_rebuild_sched_domains(int ndoms_new, cpumask_var_t doms_new[],\n\t\t\t\t    struct sched_domain_attr *dattr_new)\n{\n\tmutex_lock(&sched_domains_mutex);\n\tpartition_sched_domains_locked(ndoms_new, doms_new, dattr_new);\n\tdl_rebuild_rd_accounting();\n\tmutex_unlock(&sched_domains_mutex);\n}\n\n \nstatic void rebuild_sched_domains_locked(void)\n{\n\tstruct cgroup_subsys_state *pos_css;\n\tstruct sched_domain_attr *attr;\n\tcpumask_var_t *doms;\n\tstruct cpuset *cs;\n\tint ndoms;\n\n\tlockdep_assert_cpus_held();\n\tlockdep_assert_held(&cpuset_mutex);\n\n\t \n\tif (!top_cpuset.nr_subparts_cpus &&\n\t    !cpumask_equal(top_cpuset.effective_cpus, cpu_active_mask))\n\t\treturn;\n\n\t \n\tif (top_cpuset.nr_subparts_cpus) {\n\t\trcu_read_lock();\n\t\tcpuset_for_each_descendant_pre(cs, pos_css, &top_cpuset) {\n\t\t\tif (!is_partition_valid(cs)) {\n\t\t\t\tpos_css = css_rightmost_descendant(pos_css);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (!cpumask_subset(cs->effective_cpus,\n\t\t\t\t\t    cpu_active_mask)) {\n\t\t\t\trcu_read_unlock();\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\n\t \n\tndoms = generate_sched_domains(&doms, &attr);\n\n\t \n\tpartition_and_rebuild_sched_domains(ndoms, doms, attr);\n}\n#else  \nstatic void rebuild_sched_domains_locked(void)\n{\n}\n#endif  \n\nvoid rebuild_sched_domains(void)\n{\n\tcpus_read_lock();\n\tmutex_lock(&cpuset_mutex);\n\trebuild_sched_domains_locked();\n\tmutex_unlock(&cpuset_mutex);\n\tcpus_read_unlock();\n}\n\n \nstatic void update_tasks_cpumask(struct cpuset *cs, struct cpumask *new_cpus)\n{\n\tstruct css_task_iter it;\n\tstruct task_struct *task;\n\tbool top_cs = cs == &top_cpuset;\n\n\tcss_task_iter_start(&cs->css, 0, &it);\n\twhile ((task = css_task_iter_next(&it))) {\n\t\tconst struct cpumask *possible_mask = task_cpu_possible_mask(task);\n\n\t\tif (top_cs) {\n\t\t\t \n\t\t\tif (kthread_is_per_cpu(task))\n\t\t\t\tcontinue;\n\t\t\tcpumask_andnot(new_cpus, possible_mask, cs->subparts_cpus);\n\t\t} else {\n\t\t\tcpumask_and(new_cpus, possible_mask, cs->effective_cpus);\n\t\t}\n\t\tset_cpus_allowed_ptr(task, new_cpus);\n\t}\n\tcss_task_iter_end(&it);\n}\n\n \nstatic void compute_effective_cpumask(struct cpumask *new_cpus,\n\t\t\t\t      struct cpuset *cs, struct cpuset *parent)\n{\n\tif (parent->nr_subparts_cpus && is_partition_valid(cs)) {\n\t\tcpumask_or(new_cpus, parent->effective_cpus,\n\t\t\t   parent->subparts_cpus);\n\t\tcpumask_and(new_cpus, new_cpus, cs->cpus_allowed);\n\t\tcpumask_and(new_cpus, new_cpus, cpu_active_mask);\n\t} else {\n\t\tcpumask_and(new_cpus, cs->cpus_allowed, parent->effective_cpus);\n\t}\n}\n\n \nenum subparts_cmd {\n\tpartcmd_enable,\t\t \n\tpartcmd_disable,\t \n\tpartcmd_update,\t\t \n\tpartcmd_invalidate,\t \n};\n\nstatic int update_flag(cpuset_flagbits_t bit, struct cpuset *cs,\n\t\t       int turning_on);\nstatic void update_sibling_cpumasks(struct cpuset *parent, struct cpuset *cs,\n\t\t\t\t    struct tmpmasks *tmp);\n\n \nstatic int update_partition_exclusive(struct cpuset *cs, int new_prs)\n{\n\tbool exclusive = (new_prs > 0);\n\n\tif (exclusive && !is_cpu_exclusive(cs)) {\n\t\tif (update_flag(CS_CPU_EXCLUSIVE, cs, 1))\n\t\t\treturn PERR_NOTEXCL;\n\t} else if (!exclusive && is_cpu_exclusive(cs)) {\n\t\t \n\t\tupdate_flag(CS_CPU_EXCLUSIVE, cs, 0);\n\t}\n\treturn 0;\n}\n\n \nstatic void update_partition_sd_lb(struct cpuset *cs, int old_prs)\n{\n\tint new_prs = cs->partition_root_state;\n\tbool rebuild_domains = (new_prs > 0) || (old_prs > 0);\n\tbool new_lb;\n\n\t \n\tif (new_prs > 0) {\n\t\tnew_lb = (new_prs != PRS_ISOLATED);\n\t} else {\n\t\tnew_lb = is_sched_load_balance(parent_cs(cs));\n\t}\n\tif (new_lb != !!is_sched_load_balance(cs)) {\n\t\trebuild_domains = true;\n\t\tif (new_lb)\n\t\t\tset_bit(CS_SCHED_LOAD_BALANCE, &cs->flags);\n\t\telse\n\t\t\tclear_bit(CS_SCHED_LOAD_BALANCE, &cs->flags);\n\t}\n\n\tif (rebuild_domains)\n\t\trebuild_sched_domains_locked();\n}\n\n \nstatic int update_parent_subparts_cpumask(struct cpuset *cs, int cmd,\n\t\t\t\t\t  struct cpumask *newmask,\n\t\t\t\t\t  struct tmpmasks *tmp)\n{\n\tstruct cpuset *parent = parent_cs(cs);\n\tint adding;\t \n\tint deleting;\t \n\tint old_prs, new_prs;\n\tint part_error = PERR_NONE;\t \n\n\tlockdep_assert_held(&cpuset_mutex);\n\n\t \n\tif (!is_partition_valid(parent)) {\n\t\treturn is_partition_invalid(parent)\n\t\t       ? PERR_INVPARENT : PERR_NOTPART;\n\t}\n\tif (!newmask && cpumask_empty(cs->cpus_allowed))\n\t\treturn PERR_CPUSEMPTY;\n\n\t \n\tadding = deleting = false;\n\told_prs = new_prs = cs->partition_root_state;\n\tif (cmd == partcmd_enable) {\n\t\t \n\t\tif (!cpumask_intersects(cs->cpus_allowed, parent->cpus_allowed))\n\t\t\treturn PERR_INVCPUS;\n\n\t\t \n\t\tif (cpumask_subset(parent->effective_cpus, cs->cpus_allowed) &&\n\t\t    partition_is_populated(parent, cs))\n\t\t\treturn PERR_NOCPUS;\n\n\t\tcpumask_copy(tmp->addmask, cs->cpus_allowed);\n\t\tadding = true;\n\t} else if (cmd == partcmd_disable) {\n\t\t \n\t\tdeleting = !is_prs_invalid(old_prs) &&\n\t\t\t   cpumask_and(tmp->delmask, cs->cpus_allowed,\n\t\t\t\t       parent->subparts_cpus);\n\t} else if (cmd == partcmd_invalidate) {\n\t\tif (is_prs_invalid(old_prs))\n\t\t\treturn 0;\n\n\t\t \n\t\tdeleting = cpumask_and(tmp->delmask, cs->cpus_allowed,\n\t\t\t\t       parent->subparts_cpus);\n\t\tif (old_prs > 0) {\n\t\t\tnew_prs = -old_prs;\n\t\t\tpart_error = PERR_NOTEXCL;\n\t\t}\n\t} else if (newmask) {\n\t\t \n\t\tcpumask_andnot(tmp->delmask, cs->cpus_allowed, newmask);\n\t\tdeleting = cpumask_and(tmp->delmask, tmp->delmask,\n\t\t\t\t       parent->subparts_cpus);\n\n\t\tcpumask_and(tmp->addmask, newmask, parent->cpus_allowed);\n\t\tadding = cpumask_andnot(tmp->addmask, tmp->addmask,\n\t\t\t\t\tparent->subparts_cpus);\n\t\t \n\t\tif (cpumask_empty(newmask)) {\n\t\t\tpart_error = PERR_CPUSEMPTY;\n\t\t \n\t\t} else if (adding &&\n\t\t    cpumask_subset(parent->effective_cpus, tmp->addmask) &&\n\t\t    !cpumask_intersects(tmp->delmask, cpu_active_mask) &&\n\t\t    partition_is_populated(parent, cs)) {\n\t\t\tpart_error = PERR_NOCPUS;\n\t\t\tadding = false;\n\t\t\tdeleting = cpumask_and(tmp->delmask, cs->cpus_allowed,\n\t\t\t\t\t       parent->subparts_cpus);\n\t\t}\n\t} else {\n\t\t \n\t\tcpumask_and(tmp->addmask, cs->cpus_allowed,\n\t\t\t\t\t  parent->cpus_allowed);\n\t\tadding = cpumask_andnot(tmp->addmask, tmp->addmask,\n\t\t\t\t\tparent->subparts_cpus);\n\n\t\tif ((is_partition_valid(cs) && !parent->nr_subparts_cpus) ||\n\t\t    (adding &&\n\t\t     cpumask_subset(parent->effective_cpus, tmp->addmask) &&\n\t\t     partition_is_populated(parent, cs))) {\n\t\t\tpart_error = PERR_NOCPUS;\n\t\t\tadding = false;\n\t\t}\n\n\t\tif (part_error && is_partition_valid(cs) &&\n\t\t    parent->nr_subparts_cpus)\n\t\t\tdeleting = cpumask_and(tmp->delmask, cs->cpus_allowed,\n\t\t\t\t\t       parent->subparts_cpus);\n\t}\n\tif (part_error)\n\t\tWRITE_ONCE(cs->prs_err, part_error);\n\n\tif (cmd == partcmd_update) {\n\t\t \n\t\tswitch (cs->partition_root_state) {\n\t\tcase PRS_ROOT:\n\t\tcase PRS_ISOLATED:\n\t\t\tif (part_error)\n\t\t\t\tnew_prs = -old_prs;\n\t\t\tbreak;\n\t\tcase PRS_INVALID_ROOT:\n\t\tcase PRS_INVALID_ISOLATED:\n\t\t\tif (!part_error)\n\t\t\t\tnew_prs = -old_prs;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!adding && !deleting && (new_prs == old_prs))\n\t\treturn 0;\n\n\t \n\tif (old_prs != new_prs) {\n\t\tint err = update_partition_exclusive(cs, new_prs);\n\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\t \n\tspin_lock_irq(&callback_lock);\n\tif (adding) {\n\t\tcpumask_or(parent->subparts_cpus,\n\t\t\t   parent->subparts_cpus, tmp->addmask);\n\t\tcpumask_andnot(parent->effective_cpus,\n\t\t\t       parent->effective_cpus, tmp->addmask);\n\t}\n\tif (deleting) {\n\t\tcpumask_andnot(parent->subparts_cpus,\n\t\t\t       parent->subparts_cpus, tmp->delmask);\n\t\t \n\t\tcpumask_and(tmp->delmask, tmp->delmask, cpu_active_mask);\n\t\tcpumask_or(parent->effective_cpus,\n\t\t\t   parent->effective_cpus, tmp->delmask);\n\t}\n\n\tparent->nr_subparts_cpus = cpumask_weight(parent->subparts_cpus);\n\n\tif (old_prs != new_prs)\n\t\tcs->partition_root_state = new_prs;\n\n\tspin_unlock_irq(&callback_lock);\n\n\tif (adding || deleting) {\n\t\tupdate_tasks_cpumask(parent, tmp->addmask);\n\t\tif (parent->child_ecpus_count)\n\t\t\tupdate_sibling_cpumasks(parent, cs, tmp);\n\t}\n\n\t \n\tif ((cmd == partcmd_update) && !newmask && cpus_read_trylock()) {\n\t\tupdate_partition_sd_lb(cs, old_prs);\n\t\tcpus_read_unlock();\n\t}\n\n\tnotify_partition_change(cs, old_prs);\n\treturn 0;\n}\n\n \n#define HIER_CHECKALL\t\t0x01\t \n#define HIER_NO_SD_REBUILD\t0x02\t \n\n \nstatic void update_cpumasks_hier(struct cpuset *cs, struct tmpmasks *tmp,\n\t\t\t\t int flags)\n{\n\tstruct cpuset *cp;\n\tstruct cgroup_subsys_state *pos_css;\n\tbool need_rebuild_sched_domains = false;\n\tint old_prs, new_prs;\n\n\trcu_read_lock();\n\tcpuset_for_each_descendant_pre(cp, pos_css, cs) {\n\t\tstruct cpuset *parent = parent_cs(cp);\n\t\tbool update_parent = false;\n\n\t\tcompute_effective_cpumask(tmp->new_cpus, cp, parent);\n\n\t\t \n\t\tif (is_in_v2_mode() && cpumask_empty(tmp->new_cpus)) {\n\t\t\tif (is_partition_valid(cp) &&\n\t\t\t    cpumask_equal(cp->cpus_allowed, cp->subparts_cpus))\n\t\t\t\tgoto update_parent_subparts;\n\n\t\t\tcpumask_copy(tmp->new_cpus, parent->effective_cpus);\n\t\t\tif (!cp->use_parent_ecpus) {\n\t\t\t\tcp->use_parent_ecpus = true;\n\t\t\t\tparent->child_ecpus_count++;\n\t\t\t}\n\t\t} else if (cp->use_parent_ecpus) {\n\t\t\tcp->use_parent_ecpus = false;\n\t\t\tWARN_ON_ONCE(!parent->child_ecpus_count);\n\t\t\tparent->child_ecpus_count--;\n\t\t}\n\n\t\t \n\t\tif (!cp->partition_root_state && !(flags & HIER_CHECKALL) &&\n\t\t    cpumask_equal(tmp->new_cpus, cp->effective_cpus) &&\n\t\t    (!cgroup_subsys_on_dfl(cpuset_cgrp_subsys) ||\n\t\t    (is_sched_load_balance(parent) == is_sched_load_balance(cp)))) {\n\t\t\tpos_css = css_rightmost_descendant(pos_css);\n\t\t\tcontinue;\n\t\t}\n\nupdate_parent_subparts:\n\t\t \n\t\told_prs = new_prs = cp->partition_root_state;\n\t\tif ((cp != cs) && old_prs) {\n\t\t\tswitch (parent->partition_root_state) {\n\t\t\tcase PRS_ROOT:\n\t\t\tcase PRS_ISOLATED:\n\t\t\t\tupdate_parent = true;\n\t\t\t\tbreak;\n\n\t\t\tdefault:\n\t\t\t\t \n\t\t\t\tif (is_partition_valid(cp))\n\t\t\t\t\tnew_prs = -cp->partition_root_state;\n\t\t\t\tWRITE_ONCE(cp->prs_err,\n\t\t\t\t\t   is_partition_invalid(parent)\n\t\t\t\t\t   ? PERR_INVPARENT : PERR_NOTPART);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (!css_tryget_online(&cp->css))\n\t\t\tcontinue;\n\t\trcu_read_unlock();\n\n\t\tif (update_parent) {\n\t\t\tupdate_parent_subparts_cpumask(cp, partcmd_update, NULL,\n\t\t\t\t\t\t       tmp);\n\t\t\t \n\t\t\tnew_prs = cp->partition_root_state;\n\t\t}\n\n\t\tspin_lock_irq(&callback_lock);\n\n\t\tif (cp->nr_subparts_cpus && !is_partition_valid(cp)) {\n\t\t\t \n\t\t\tcpumask_or(tmp->new_cpus, tmp->new_cpus,\n\t\t\t\t   cp->subparts_cpus);\n\t\t\tcpumask_and(tmp->new_cpus, tmp->new_cpus,\n\t\t\t\t   cpu_active_mask);\n\t\t\tcp->nr_subparts_cpus = 0;\n\t\t\tcpumask_clear(cp->subparts_cpus);\n\t\t}\n\n\t\tcpumask_copy(cp->effective_cpus, tmp->new_cpus);\n\t\tif (cp->nr_subparts_cpus) {\n\t\t\t \n\t\t\tcpumask_andnot(cp->effective_cpus, cp->effective_cpus,\n\t\t\t\t       cp->subparts_cpus);\n\t\t}\n\n\t\tcp->partition_root_state = new_prs;\n\t\tspin_unlock_irq(&callback_lock);\n\n\t\tnotify_partition_change(cp, old_prs);\n\n\t\tWARN_ON(!is_in_v2_mode() &&\n\t\t\t!cpumask_equal(cp->cpus_allowed, cp->effective_cpus));\n\n\t\tupdate_tasks_cpumask(cp, tmp->new_cpus);\n\n\t\t \n\t\tif (cgroup_subsys_on_dfl(cpuset_cgrp_subsys) &&\n\t\t    !is_partition_valid(cp) &&\n\t\t    (is_sched_load_balance(parent) != is_sched_load_balance(cp))) {\n\t\t\tif (is_sched_load_balance(parent))\n\t\t\t\tset_bit(CS_SCHED_LOAD_BALANCE, &cp->flags);\n\t\t\telse\n\t\t\t\tclear_bit(CS_SCHED_LOAD_BALANCE, &cp->flags);\n\t\t}\n\n\t\t \n\t\tif (!cpumask_empty(cp->cpus_allowed) &&\n\t\t    is_sched_load_balance(cp) &&\n\t\t   (!cgroup_subsys_on_dfl(cpuset_cgrp_subsys) ||\n\t\t    is_partition_valid(cp)))\n\t\t\tneed_rebuild_sched_domains = true;\n\n\t\trcu_read_lock();\n\t\tcss_put(&cp->css);\n\t}\n\trcu_read_unlock();\n\n\tif (need_rebuild_sched_domains && !(flags & HIER_NO_SD_REBUILD))\n\t\trebuild_sched_domains_locked();\n}\n\n \nstatic void update_sibling_cpumasks(struct cpuset *parent, struct cpuset *cs,\n\t\t\t\t    struct tmpmasks *tmp)\n{\n\tstruct cpuset *sibling;\n\tstruct cgroup_subsys_state *pos_css;\n\n\tlockdep_assert_held(&cpuset_mutex);\n\n\t \n\trcu_read_lock();\n\tcpuset_for_each_child(sibling, pos_css, parent) {\n\t\tif (sibling == cs)\n\t\t\tcontinue;\n\t\tif (!sibling->use_parent_ecpus)\n\t\t\tcontinue;\n\t\tif (!css_tryget_online(&sibling->css))\n\t\t\tcontinue;\n\n\t\trcu_read_unlock();\n\t\tupdate_cpumasks_hier(sibling, tmp, HIER_NO_SD_REBUILD);\n\t\trcu_read_lock();\n\t\tcss_put(&sibling->css);\n\t}\n\trcu_read_unlock();\n}\n\n \nstatic int update_cpumask(struct cpuset *cs, struct cpuset *trialcs,\n\t\t\t  const char *buf)\n{\n\tint retval;\n\tstruct tmpmasks tmp;\n\tbool invalidate = false;\n\tint old_prs = cs->partition_root_state;\n\n\t \n\tif (cs == &top_cpuset)\n\t\treturn -EACCES;\n\n\t \n\tif (!*buf) {\n\t\tcpumask_clear(trialcs->cpus_allowed);\n\t} else {\n\t\tretval = cpulist_parse(buf, trialcs->cpus_allowed);\n\t\tif (retval < 0)\n\t\t\treturn retval;\n\n\t\tif (!cpumask_subset(trialcs->cpus_allowed,\n\t\t\t\t    top_cpuset.cpus_allowed))\n\t\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (cpumask_equal(cs->cpus_allowed, trialcs->cpus_allowed))\n\t\treturn 0;\n\n\tif (alloc_cpumasks(NULL, &tmp))\n\t\treturn -ENOMEM;\n\n\tretval = validate_change(cs, trialcs);\n\n\tif ((retval == -EINVAL) && cgroup_subsys_on_dfl(cpuset_cgrp_subsys)) {\n\t\tstruct cpuset *cp, *parent;\n\t\tstruct cgroup_subsys_state *css;\n\n\t\t \n\t\tinvalidate = true;\n\t\trcu_read_lock();\n\t\tparent = parent_cs(cs);\n\t\tcpuset_for_each_child(cp, css, parent)\n\t\t\tif (is_partition_valid(cp) &&\n\t\t\t    cpumask_intersects(trialcs->cpus_allowed, cp->cpus_allowed)) {\n\t\t\t\trcu_read_unlock();\n\t\t\t\tupdate_parent_subparts_cpumask(cp, partcmd_invalidate, NULL, &tmp);\n\t\t\t\trcu_read_lock();\n\t\t\t}\n\t\trcu_read_unlock();\n\t\tretval = 0;\n\t}\n\tif (retval < 0)\n\t\tgoto out_free;\n\n\tif (cs->partition_root_state) {\n\t\tif (invalidate)\n\t\t\tupdate_parent_subparts_cpumask(cs, partcmd_invalidate,\n\t\t\t\t\t\t       NULL, &tmp);\n\t\telse\n\t\t\tupdate_parent_subparts_cpumask(cs, partcmd_update,\n\t\t\t\t\t\ttrialcs->cpus_allowed, &tmp);\n\t}\n\n\tcompute_effective_cpumask(trialcs->effective_cpus, trialcs,\n\t\t\t\t  parent_cs(cs));\n\tspin_lock_irq(&callback_lock);\n\tcpumask_copy(cs->cpus_allowed, trialcs->cpus_allowed);\n\n\t \n\tif (cs->nr_subparts_cpus) {\n\t\tif (!is_partition_valid(cs) ||\n\t\t   (cpumask_subset(trialcs->effective_cpus, cs->subparts_cpus) &&\n\t\t    partition_is_populated(cs, NULL))) {\n\t\t\tcs->nr_subparts_cpus = 0;\n\t\t\tcpumask_clear(cs->subparts_cpus);\n\t\t} else {\n\t\t\tcpumask_and(cs->subparts_cpus, cs->subparts_cpus,\n\t\t\t\t    cs->cpus_allowed);\n\t\t\tcs->nr_subparts_cpus = cpumask_weight(cs->subparts_cpus);\n\t\t}\n\t}\n\tspin_unlock_irq(&callback_lock);\n\n\t \n\tupdate_cpumasks_hier(cs, &tmp, 0);\n\n\tif (cs->partition_root_state) {\n\t\tstruct cpuset *parent = parent_cs(cs);\n\n\t\t \n\t\tif (parent->child_ecpus_count)\n\t\t\tupdate_sibling_cpumasks(parent, cs, &tmp);\n\n\t\t \n\t\tupdate_partition_sd_lb(cs, old_prs);\n\t}\nout_free:\n\tfree_cpumasks(NULL, &tmp);\n\treturn 0;\n}\n\n \n\nstruct cpuset_migrate_mm_work {\n\tstruct work_struct\twork;\n\tstruct mm_struct\t*mm;\n\tnodemask_t\t\tfrom;\n\tnodemask_t\t\tto;\n};\n\nstatic void cpuset_migrate_mm_workfn(struct work_struct *work)\n{\n\tstruct cpuset_migrate_mm_work *mwork =\n\t\tcontainer_of(work, struct cpuset_migrate_mm_work, work);\n\n\t \n\tdo_migrate_pages(mwork->mm, &mwork->from, &mwork->to, MPOL_MF_MOVE_ALL);\n\tmmput(mwork->mm);\n\tkfree(mwork);\n}\n\nstatic void cpuset_migrate_mm(struct mm_struct *mm, const nodemask_t *from,\n\t\t\t\t\t\t\tconst nodemask_t *to)\n{\n\tstruct cpuset_migrate_mm_work *mwork;\n\n\tif (nodes_equal(*from, *to)) {\n\t\tmmput(mm);\n\t\treturn;\n\t}\n\n\tmwork = kzalloc(sizeof(*mwork), GFP_KERNEL);\n\tif (mwork) {\n\t\tmwork->mm = mm;\n\t\tmwork->from = *from;\n\t\tmwork->to = *to;\n\t\tINIT_WORK(&mwork->work, cpuset_migrate_mm_workfn);\n\t\tqueue_work(cpuset_migrate_mm_wq, &mwork->work);\n\t} else {\n\t\tmmput(mm);\n\t}\n}\n\nstatic void cpuset_post_attach(void)\n{\n\tflush_workqueue(cpuset_migrate_mm_wq);\n}\n\n \nstatic void cpuset_change_task_nodemask(struct task_struct *tsk,\n\t\t\t\t\tnodemask_t *newmems)\n{\n\ttask_lock(tsk);\n\n\tlocal_irq_disable();\n\twrite_seqcount_begin(&tsk->mems_allowed_seq);\n\n\tnodes_or(tsk->mems_allowed, tsk->mems_allowed, *newmems);\n\tmpol_rebind_task(tsk, newmems);\n\ttsk->mems_allowed = *newmems;\n\n\twrite_seqcount_end(&tsk->mems_allowed_seq);\n\tlocal_irq_enable();\n\n\ttask_unlock(tsk);\n}\n\nstatic void *cpuset_being_rebound;\n\n \nstatic void update_tasks_nodemask(struct cpuset *cs)\n{\n\tstatic nodemask_t newmems;\t \n\tstruct css_task_iter it;\n\tstruct task_struct *task;\n\n\tcpuset_being_rebound = cs;\t\t \n\n\tguarantee_online_mems(cs, &newmems);\n\n\t \n\tcss_task_iter_start(&cs->css, 0, &it);\n\twhile ((task = css_task_iter_next(&it))) {\n\t\tstruct mm_struct *mm;\n\t\tbool migrate;\n\n\t\tcpuset_change_task_nodemask(task, &newmems);\n\n\t\tmm = get_task_mm(task);\n\t\tif (!mm)\n\t\t\tcontinue;\n\n\t\tmigrate = is_memory_migrate(cs);\n\n\t\tmpol_rebind_mm(mm, &cs->mems_allowed);\n\t\tif (migrate)\n\t\t\tcpuset_migrate_mm(mm, &cs->old_mems_allowed, &newmems);\n\t\telse\n\t\t\tmmput(mm);\n\t}\n\tcss_task_iter_end(&it);\n\n\t \n\tcs->old_mems_allowed = newmems;\n\n\t \n\tcpuset_being_rebound = NULL;\n}\n\n \nstatic void update_nodemasks_hier(struct cpuset *cs, nodemask_t *new_mems)\n{\n\tstruct cpuset *cp;\n\tstruct cgroup_subsys_state *pos_css;\n\n\trcu_read_lock();\n\tcpuset_for_each_descendant_pre(cp, pos_css, cs) {\n\t\tstruct cpuset *parent = parent_cs(cp);\n\n\t\tnodes_and(*new_mems, cp->mems_allowed, parent->effective_mems);\n\n\t\t \n\t\tif (is_in_v2_mode() && nodes_empty(*new_mems))\n\t\t\t*new_mems = parent->effective_mems;\n\n\t\t \n\t\tif (nodes_equal(*new_mems, cp->effective_mems)) {\n\t\t\tpos_css = css_rightmost_descendant(pos_css);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!css_tryget_online(&cp->css))\n\t\t\tcontinue;\n\t\trcu_read_unlock();\n\n\t\tspin_lock_irq(&callback_lock);\n\t\tcp->effective_mems = *new_mems;\n\t\tspin_unlock_irq(&callback_lock);\n\n\t\tWARN_ON(!is_in_v2_mode() &&\n\t\t\t!nodes_equal(cp->mems_allowed, cp->effective_mems));\n\n\t\tupdate_tasks_nodemask(cp);\n\n\t\trcu_read_lock();\n\t\tcss_put(&cp->css);\n\t}\n\trcu_read_unlock();\n}\n\n \nstatic int update_nodemask(struct cpuset *cs, struct cpuset *trialcs,\n\t\t\t   const char *buf)\n{\n\tint retval;\n\n\t \n\tif (cs == &top_cpuset) {\n\t\tretval = -EACCES;\n\t\tgoto done;\n\t}\n\n\t \n\tif (!*buf) {\n\t\tnodes_clear(trialcs->mems_allowed);\n\t} else {\n\t\tretval = nodelist_parse(buf, trialcs->mems_allowed);\n\t\tif (retval < 0)\n\t\t\tgoto done;\n\n\t\tif (!nodes_subset(trialcs->mems_allowed,\n\t\t\t\t  top_cpuset.mems_allowed)) {\n\t\t\tretval = -EINVAL;\n\t\t\tgoto done;\n\t\t}\n\t}\n\n\tif (nodes_equal(cs->mems_allowed, trialcs->mems_allowed)) {\n\t\tretval = 0;\t\t \n\t\tgoto done;\n\t}\n\tretval = validate_change(cs, trialcs);\n\tif (retval < 0)\n\t\tgoto done;\n\n\tcheck_insane_mems_config(&trialcs->mems_allowed);\n\n\tspin_lock_irq(&callback_lock);\n\tcs->mems_allowed = trialcs->mems_allowed;\n\tspin_unlock_irq(&callback_lock);\n\n\t \n\tupdate_nodemasks_hier(cs, &trialcs->mems_allowed);\ndone:\n\treturn retval;\n}\n\nbool current_cpuset_is_being_rebound(void)\n{\n\tbool ret;\n\n\trcu_read_lock();\n\tret = task_cs(current) == cpuset_being_rebound;\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\nstatic int update_relax_domain_level(struct cpuset *cs, s64 val)\n{\n#ifdef CONFIG_SMP\n\tif (val < -1 || val >= sched_domain_level_max)\n\t\treturn -EINVAL;\n#endif\n\n\tif (val != cs->relax_domain_level) {\n\t\tcs->relax_domain_level = val;\n\t\tif (!cpumask_empty(cs->cpus_allowed) &&\n\t\t    is_sched_load_balance(cs))\n\t\t\trebuild_sched_domains_locked();\n\t}\n\n\treturn 0;\n}\n\n \nstatic void update_tasks_flags(struct cpuset *cs)\n{\n\tstruct css_task_iter it;\n\tstruct task_struct *task;\n\n\tcss_task_iter_start(&cs->css, 0, &it);\n\twhile ((task = css_task_iter_next(&it)))\n\t\tcpuset_update_task_spread_flags(cs, task);\n\tcss_task_iter_end(&it);\n}\n\n \n\nstatic int update_flag(cpuset_flagbits_t bit, struct cpuset *cs,\n\t\t       int turning_on)\n{\n\tstruct cpuset *trialcs;\n\tint balance_flag_changed;\n\tint spread_flag_changed;\n\tint err;\n\n\ttrialcs = alloc_trial_cpuset(cs);\n\tif (!trialcs)\n\t\treturn -ENOMEM;\n\n\tif (turning_on)\n\t\tset_bit(bit, &trialcs->flags);\n\telse\n\t\tclear_bit(bit, &trialcs->flags);\n\n\terr = validate_change(cs, trialcs);\n\tif (err < 0)\n\t\tgoto out;\n\n\tbalance_flag_changed = (is_sched_load_balance(cs) !=\n\t\t\t\tis_sched_load_balance(trialcs));\n\n\tspread_flag_changed = ((is_spread_slab(cs) != is_spread_slab(trialcs))\n\t\t\t|| (is_spread_page(cs) != is_spread_page(trialcs)));\n\n\tspin_lock_irq(&callback_lock);\n\tcs->flags = trialcs->flags;\n\tspin_unlock_irq(&callback_lock);\n\n\tif (!cpumask_empty(trialcs->cpus_allowed) && balance_flag_changed)\n\t\trebuild_sched_domains_locked();\n\n\tif (spread_flag_changed)\n\t\tupdate_tasks_flags(cs);\nout:\n\tfree_cpuset(trialcs);\n\treturn err;\n}\n\n \nstatic int update_prstate(struct cpuset *cs, int new_prs)\n{\n\tint err = PERR_NONE, old_prs = cs->partition_root_state;\n\tstruct cpuset *parent = parent_cs(cs);\n\tstruct tmpmasks tmpmask;\n\n\tif (old_prs == new_prs)\n\t\treturn 0;\n\n\t \n\tif (new_prs && is_prs_invalid(old_prs)) {\n\t\tcs->partition_root_state = -new_prs;\n\t\treturn 0;\n\t}\n\n\tif (alloc_cpumasks(NULL, &tmpmask))\n\t\treturn -ENOMEM;\n\n\terr = update_partition_exclusive(cs, new_prs);\n\tif (err)\n\t\tgoto out;\n\n\tif (!old_prs) {\n\t\t \n\t\tif (cpumask_empty(cs->cpus_allowed)) {\n\t\t\terr = PERR_CPUSEMPTY;\n\t\t\tgoto out;\n\t\t}\n\n\t\terr = update_parent_subparts_cpumask(cs, partcmd_enable,\n\t\t\t\t\t\t     NULL, &tmpmask);\n\t} else if (old_prs && new_prs) {\n\t\t \n\t\t;\n\t} else {\n\t\t \n\t\tupdate_parent_subparts_cpumask(cs, partcmd_disable, NULL,\n\t\t\t\t\t       &tmpmask);\n\n\t\t \n\t\tif (unlikely(cs->nr_subparts_cpus)) {\n\t\t\tspin_lock_irq(&callback_lock);\n\t\t\tcs->nr_subparts_cpus = 0;\n\t\t\tcpumask_clear(cs->subparts_cpus);\n\t\t\tcompute_effective_cpumask(cs->effective_cpus, cs, parent);\n\t\t\tspin_unlock_irq(&callback_lock);\n\t\t}\n\t}\nout:\n\t \n\tif (err) {\n\t\tnew_prs = -new_prs;\n\t\tupdate_partition_exclusive(cs, new_prs);\n\t}\n\n\tspin_lock_irq(&callback_lock);\n\tcs->partition_root_state = new_prs;\n\tWRITE_ONCE(cs->prs_err, err);\n\tspin_unlock_irq(&callback_lock);\n\n\t \n\tif (!list_empty(&cs->css.children))\n\t\tupdate_cpumasks_hier(cs, &tmpmask, !new_prs ? HIER_CHECKALL : 0);\n\n\t \n\tupdate_partition_sd_lb(cs, old_prs);\n\n\tnotify_partition_change(cs, old_prs);\n\tfree_cpumasks(NULL, &tmpmask);\n\treturn 0;\n}\n\n \n\n#define FM_COEF 933\t\t \n#define FM_MAXTICKS ((u32)99)    \n#define FM_MAXCNT 1000000\t \n#define FM_SCALE 1000\t\t \n\n \nstatic void fmeter_init(struct fmeter *fmp)\n{\n\tfmp->cnt = 0;\n\tfmp->val = 0;\n\tfmp->time = 0;\n\tspin_lock_init(&fmp->lock);\n}\n\n \nstatic void fmeter_update(struct fmeter *fmp)\n{\n\ttime64_t now;\n\tu32 ticks;\n\n\tnow = ktime_get_seconds();\n\tticks = now - fmp->time;\n\n\tif (ticks == 0)\n\t\treturn;\n\n\tticks = min(FM_MAXTICKS, ticks);\n\twhile (ticks-- > 0)\n\t\tfmp->val = (FM_COEF * fmp->val) / FM_SCALE;\n\tfmp->time = now;\n\n\tfmp->val += ((FM_SCALE - FM_COEF) * fmp->cnt) / FM_SCALE;\n\tfmp->cnt = 0;\n}\n\n \nstatic void fmeter_markevent(struct fmeter *fmp)\n{\n\tspin_lock(&fmp->lock);\n\tfmeter_update(fmp);\n\tfmp->cnt = min(FM_MAXCNT, fmp->cnt + FM_SCALE);\n\tspin_unlock(&fmp->lock);\n}\n\n \nstatic int fmeter_getrate(struct fmeter *fmp)\n{\n\tint val;\n\n\tspin_lock(&fmp->lock);\n\tfmeter_update(fmp);\n\tval = fmp->val;\n\tspin_unlock(&fmp->lock);\n\treturn val;\n}\n\nstatic struct cpuset *cpuset_attach_old_cs;\n\n \nstatic int cpuset_can_attach_check(struct cpuset *cs)\n{\n\tif (cpumask_empty(cs->effective_cpus) ||\n\t   (!is_in_v2_mode() && nodes_empty(cs->mems_allowed)))\n\t\treturn -ENOSPC;\n\treturn 0;\n}\n\nstatic void reset_migrate_dl_data(struct cpuset *cs)\n{\n\tcs->nr_migrate_dl_tasks = 0;\n\tcs->sum_migrate_dl_bw = 0;\n}\n\n \nstatic int cpuset_can_attach(struct cgroup_taskset *tset)\n{\n\tstruct cgroup_subsys_state *css;\n\tstruct cpuset *cs, *oldcs;\n\tstruct task_struct *task;\n\tbool cpus_updated, mems_updated;\n\tint ret;\n\n\t \n\tcpuset_attach_old_cs = task_cs(cgroup_taskset_first(tset, &css));\n\toldcs = cpuset_attach_old_cs;\n\tcs = css_cs(css);\n\n\tmutex_lock(&cpuset_mutex);\n\n\t \n\tret = cpuset_can_attach_check(cs);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tcpus_updated = !cpumask_equal(cs->effective_cpus, oldcs->effective_cpus);\n\tmems_updated = !nodes_equal(cs->effective_mems, oldcs->effective_mems);\n\n\tcgroup_taskset_for_each(task, css, tset) {\n\t\tret = task_can_attach(task);\n\t\tif (ret)\n\t\t\tgoto out_unlock;\n\n\t\t \n\t\tif (!cgroup_subsys_on_dfl(cpuset_cgrp_subsys) ||\n\t\t    (cpus_updated || mems_updated)) {\n\t\t\tret = security_task_setscheduler(task);\n\t\t\tif (ret)\n\t\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tif (dl_task(task)) {\n\t\t\tcs->nr_migrate_dl_tasks++;\n\t\t\tcs->sum_migrate_dl_bw += task->dl.dl_bw;\n\t\t}\n\t}\n\n\tif (!cs->nr_migrate_dl_tasks)\n\t\tgoto out_success;\n\n\tif (!cpumask_intersects(oldcs->effective_cpus, cs->effective_cpus)) {\n\t\tint cpu = cpumask_any_and(cpu_active_mask, cs->effective_cpus);\n\n\t\tif (unlikely(cpu >= nr_cpu_ids)) {\n\t\t\treset_migrate_dl_data(cs);\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tret = dl_bw_alloc(cpu, cs->sum_migrate_dl_bw);\n\t\tif (ret) {\n\t\t\treset_migrate_dl_data(cs);\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\nout_success:\n\t \n\tcs->attach_in_progress++;\nout_unlock:\n\tmutex_unlock(&cpuset_mutex);\n\treturn ret;\n}\n\nstatic void cpuset_cancel_attach(struct cgroup_taskset *tset)\n{\n\tstruct cgroup_subsys_state *css;\n\tstruct cpuset *cs;\n\n\tcgroup_taskset_first(tset, &css);\n\tcs = css_cs(css);\n\n\tmutex_lock(&cpuset_mutex);\n\tcs->attach_in_progress--;\n\tif (!cs->attach_in_progress)\n\t\twake_up(&cpuset_attach_wq);\n\n\tif (cs->nr_migrate_dl_tasks) {\n\t\tint cpu = cpumask_any(cs->effective_cpus);\n\n\t\tdl_bw_free(cpu, cs->sum_migrate_dl_bw);\n\t\treset_migrate_dl_data(cs);\n\t}\n\n\tmutex_unlock(&cpuset_mutex);\n}\n\n \nstatic cpumask_var_t cpus_attach;\nstatic nodemask_t cpuset_attach_nodemask_to;\n\nstatic void cpuset_attach_task(struct cpuset *cs, struct task_struct *task)\n{\n\tlockdep_assert_held(&cpuset_mutex);\n\n\tif (cs != &top_cpuset)\n\t\tguarantee_online_cpus(task, cpus_attach);\n\telse\n\t\tcpumask_andnot(cpus_attach, task_cpu_possible_mask(task),\n\t\t\t       cs->subparts_cpus);\n\t \n\tWARN_ON_ONCE(set_cpus_allowed_ptr(task, cpus_attach));\n\n\tcpuset_change_task_nodemask(task, &cpuset_attach_nodemask_to);\n\tcpuset_update_task_spread_flags(cs, task);\n}\n\nstatic void cpuset_attach(struct cgroup_taskset *tset)\n{\n\tstruct task_struct *task;\n\tstruct task_struct *leader;\n\tstruct cgroup_subsys_state *css;\n\tstruct cpuset *cs;\n\tstruct cpuset *oldcs = cpuset_attach_old_cs;\n\tbool cpus_updated, mems_updated;\n\n\tcgroup_taskset_first(tset, &css);\n\tcs = css_cs(css);\n\n\tlockdep_assert_cpus_held();\t \n\tmutex_lock(&cpuset_mutex);\n\tcpus_updated = !cpumask_equal(cs->effective_cpus,\n\t\t\t\t      oldcs->effective_cpus);\n\tmems_updated = !nodes_equal(cs->effective_mems, oldcs->effective_mems);\n\n\t \n\tif (cgroup_subsys_on_dfl(cpuset_cgrp_subsys) &&\n\t    !cpus_updated && !mems_updated) {\n\t\tcpuset_attach_nodemask_to = cs->effective_mems;\n\t\tgoto out;\n\t}\n\n\tguarantee_online_mems(cs, &cpuset_attach_nodemask_to);\n\n\tcgroup_taskset_for_each(task, css, tset)\n\t\tcpuset_attach_task(cs, task);\n\n\t \n\tcpuset_attach_nodemask_to = cs->effective_mems;\n\tif (!is_memory_migrate(cs) && !mems_updated)\n\t\tgoto out;\n\n\tcgroup_taskset_for_each_leader(leader, css, tset) {\n\t\tstruct mm_struct *mm = get_task_mm(leader);\n\n\t\tif (mm) {\n\t\t\tmpol_rebind_mm(mm, &cpuset_attach_nodemask_to);\n\n\t\t\t \n\t\t\tif (is_memory_migrate(cs))\n\t\t\t\tcpuset_migrate_mm(mm, &oldcs->old_mems_allowed,\n\t\t\t\t\t\t  &cpuset_attach_nodemask_to);\n\t\t\telse\n\t\t\t\tmmput(mm);\n\t\t}\n\t}\n\nout:\n\tcs->old_mems_allowed = cpuset_attach_nodemask_to;\n\n\tif (cs->nr_migrate_dl_tasks) {\n\t\tcs->nr_deadline_tasks += cs->nr_migrate_dl_tasks;\n\t\toldcs->nr_deadline_tasks -= cs->nr_migrate_dl_tasks;\n\t\treset_migrate_dl_data(cs);\n\t}\n\n\tcs->attach_in_progress--;\n\tif (!cs->attach_in_progress)\n\t\twake_up(&cpuset_attach_wq);\n\n\tmutex_unlock(&cpuset_mutex);\n}\n\n \n\ntypedef enum {\n\tFILE_MEMORY_MIGRATE,\n\tFILE_CPULIST,\n\tFILE_MEMLIST,\n\tFILE_EFFECTIVE_CPULIST,\n\tFILE_EFFECTIVE_MEMLIST,\n\tFILE_SUBPARTS_CPULIST,\n\tFILE_CPU_EXCLUSIVE,\n\tFILE_MEM_EXCLUSIVE,\n\tFILE_MEM_HARDWALL,\n\tFILE_SCHED_LOAD_BALANCE,\n\tFILE_PARTITION_ROOT,\n\tFILE_SCHED_RELAX_DOMAIN_LEVEL,\n\tFILE_MEMORY_PRESSURE_ENABLED,\n\tFILE_MEMORY_PRESSURE,\n\tFILE_SPREAD_PAGE,\n\tFILE_SPREAD_SLAB,\n} cpuset_filetype_t;\n\nstatic int cpuset_write_u64(struct cgroup_subsys_state *css, struct cftype *cft,\n\t\t\t    u64 val)\n{\n\tstruct cpuset *cs = css_cs(css);\n\tcpuset_filetype_t type = cft->private;\n\tint retval = 0;\n\n\tcpus_read_lock();\n\tmutex_lock(&cpuset_mutex);\n\tif (!is_cpuset_online(cs)) {\n\t\tretval = -ENODEV;\n\t\tgoto out_unlock;\n\t}\n\n\tswitch (type) {\n\tcase FILE_CPU_EXCLUSIVE:\n\t\tretval = update_flag(CS_CPU_EXCLUSIVE, cs, val);\n\t\tbreak;\n\tcase FILE_MEM_EXCLUSIVE:\n\t\tretval = update_flag(CS_MEM_EXCLUSIVE, cs, val);\n\t\tbreak;\n\tcase FILE_MEM_HARDWALL:\n\t\tretval = update_flag(CS_MEM_HARDWALL, cs, val);\n\t\tbreak;\n\tcase FILE_SCHED_LOAD_BALANCE:\n\t\tretval = update_flag(CS_SCHED_LOAD_BALANCE, cs, val);\n\t\tbreak;\n\tcase FILE_MEMORY_MIGRATE:\n\t\tretval = update_flag(CS_MEMORY_MIGRATE, cs, val);\n\t\tbreak;\n\tcase FILE_MEMORY_PRESSURE_ENABLED:\n\t\tcpuset_memory_pressure_enabled = !!val;\n\t\tbreak;\n\tcase FILE_SPREAD_PAGE:\n\t\tretval = update_flag(CS_SPREAD_PAGE, cs, val);\n\t\tbreak;\n\tcase FILE_SPREAD_SLAB:\n\t\tretval = update_flag(CS_SPREAD_SLAB, cs, val);\n\t\tbreak;\n\tdefault:\n\t\tretval = -EINVAL;\n\t\tbreak;\n\t}\nout_unlock:\n\tmutex_unlock(&cpuset_mutex);\n\tcpus_read_unlock();\n\treturn retval;\n}\n\nstatic int cpuset_write_s64(struct cgroup_subsys_state *css, struct cftype *cft,\n\t\t\t    s64 val)\n{\n\tstruct cpuset *cs = css_cs(css);\n\tcpuset_filetype_t type = cft->private;\n\tint retval = -ENODEV;\n\n\tcpus_read_lock();\n\tmutex_lock(&cpuset_mutex);\n\tif (!is_cpuset_online(cs))\n\t\tgoto out_unlock;\n\n\tswitch (type) {\n\tcase FILE_SCHED_RELAX_DOMAIN_LEVEL:\n\t\tretval = update_relax_domain_level(cs, val);\n\t\tbreak;\n\tdefault:\n\t\tretval = -EINVAL;\n\t\tbreak;\n\t}\nout_unlock:\n\tmutex_unlock(&cpuset_mutex);\n\tcpus_read_unlock();\n\treturn retval;\n}\n\n \nstatic ssize_t cpuset_write_resmask(struct kernfs_open_file *of,\n\t\t\t\t    char *buf, size_t nbytes, loff_t off)\n{\n\tstruct cpuset *cs = css_cs(of_css(of));\n\tstruct cpuset *trialcs;\n\tint retval = -ENODEV;\n\n\tbuf = strstrip(buf);\n\n\t \n\tcss_get(&cs->css);\n\tkernfs_break_active_protection(of->kn);\n\tflush_work(&cpuset_hotplug_work);\n\n\tcpus_read_lock();\n\tmutex_lock(&cpuset_mutex);\n\tif (!is_cpuset_online(cs))\n\t\tgoto out_unlock;\n\n\ttrialcs = alloc_trial_cpuset(cs);\n\tif (!trialcs) {\n\t\tretval = -ENOMEM;\n\t\tgoto out_unlock;\n\t}\n\n\tswitch (of_cft(of)->private) {\n\tcase FILE_CPULIST:\n\t\tretval = update_cpumask(cs, trialcs, buf);\n\t\tbreak;\n\tcase FILE_MEMLIST:\n\t\tretval = update_nodemask(cs, trialcs, buf);\n\t\tbreak;\n\tdefault:\n\t\tretval = -EINVAL;\n\t\tbreak;\n\t}\n\n\tfree_cpuset(trialcs);\nout_unlock:\n\tmutex_unlock(&cpuset_mutex);\n\tcpus_read_unlock();\n\tkernfs_unbreak_active_protection(of->kn);\n\tcss_put(&cs->css);\n\tflush_workqueue(cpuset_migrate_mm_wq);\n\treturn retval ?: nbytes;\n}\n\n \nstatic int cpuset_common_seq_show(struct seq_file *sf, void *v)\n{\n\tstruct cpuset *cs = css_cs(seq_css(sf));\n\tcpuset_filetype_t type = seq_cft(sf)->private;\n\tint ret = 0;\n\n\tspin_lock_irq(&callback_lock);\n\n\tswitch (type) {\n\tcase FILE_CPULIST:\n\t\tseq_printf(sf, \"%*pbl\\n\", cpumask_pr_args(cs->cpus_allowed));\n\t\tbreak;\n\tcase FILE_MEMLIST:\n\t\tseq_printf(sf, \"%*pbl\\n\", nodemask_pr_args(&cs->mems_allowed));\n\t\tbreak;\n\tcase FILE_EFFECTIVE_CPULIST:\n\t\tseq_printf(sf, \"%*pbl\\n\", cpumask_pr_args(cs->effective_cpus));\n\t\tbreak;\n\tcase FILE_EFFECTIVE_MEMLIST:\n\t\tseq_printf(sf, \"%*pbl\\n\", nodemask_pr_args(&cs->effective_mems));\n\t\tbreak;\n\tcase FILE_SUBPARTS_CPULIST:\n\t\tseq_printf(sf, \"%*pbl\\n\", cpumask_pr_args(cs->subparts_cpus));\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t}\n\n\tspin_unlock_irq(&callback_lock);\n\treturn ret;\n}\n\nstatic u64 cpuset_read_u64(struct cgroup_subsys_state *css, struct cftype *cft)\n{\n\tstruct cpuset *cs = css_cs(css);\n\tcpuset_filetype_t type = cft->private;\n\tswitch (type) {\n\tcase FILE_CPU_EXCLUSIVE:\n\t\treturn is_cpu_exclusive(cs);\n\tcase FILE_MEM_EXCLUSIVE:\n\t\treturn is_mem_exclusive(cs);\n\tcase FILE_MEM_HARDWALL:\n\t\treturn is_mem_hardwall(cs);\n\tcase FILE_SCHED_LOAD_BALANCE:\n\t\treturn is_sched_load_balance(cs);\n\tcase FILE_MEMORY_MIGRATE:\n\t\treturn is_memory_migrate(cs);\n\tcase FILE_MEMORY_PRESSURE_ENABLED:\n\t\treturn cpuset_memory_pressure_enabled;\n\tcase FILE_MEMORY_PRESSURE:\n\t\treturn fmeter_getrate(&cs->fmeter);\n\tcase FILE_SPREAD_PAGE:\n\t\treturn is_spread_page(cs);\n\tcase FILE_SPREAD_SLAB:\n\t\treturn is_spread_slab(cs);\n\tdefault:\n\t\tBUG();\n\t}\n\n\t \n\treturn 0;\n}\n\nstatic s64 cpuset_read_s64(struct cgroup_subsys_state *css, struct cftype *cft)\n{\n\tstruct cpuset *cs = css_cs(css);\n\tcpuset_filetype_t type = cft->private;\n\tswitch (type) {\n\tcase FILE_SCHED_RELAX_DOMAIN_LEVEL:\n\t\treturn cs->relax_domain_level;\n\tdefault:\n\t\tBUG();\n\t}\n\n\t \n\treturn 0;\n}\n\nstatic int sched_partition_show(struct seq_file *seq, void *v)\n{\n\tstruct cpuset *cs = css_cs(seq_css(seq));\n\tconst char *err, *type = NULL;\n\n\tswitch (cs->partition_root_state) {\n\tcase PRS_ROOT:\n\t\tseq_puts(seq, \"root\\n\");\n\t\tbreak;\n\tcase PRS_ISOLATED:\n\t\tseq_puts(seq, \"isolated\\n\");\n\t\tbreak;\n\tcase PRS_MEMBER:\n\t\tseq_puts(seq, \"member\\n\");\n\t\tbreak;\n\tcase PRS_INVALID_ROOT:\n\t\ttype = \"root\";\n\t\tfallthrough;\n\tcase PRS_INVALID_ISOLATED:\n\t\tif (!type)\n\t\t\ttype = \"isolated\";\n\t\terr = perr_strings[READ_ONCE(cs->prs_err)];\n\t\tif (err)\n\t\t\tseq_printf(seq, \"%s invalid (%s)\\n\", type, err);\n\t\telse\n\t\t\tseq_printf(seq, \"%s invalid\\n\", type);\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic ssize_t sched_partition_write(struct kernfs_open_file *of, char *buf,\n\t\t\t\t     size_t nbytes, loff_t off)\n{\n\tstruct cpuset *cs = css_cs(of_css(of));\n\tint val;\n\tint retval = -ENODEV;\n\n\tbuf = strstrip(buf);\n\n\t \n\tif (!strcmp(buf, \"root\"))\n\t\tval = PRS_ROOT;\n\telse if (!strcmp(buf, \"member\"))\n\t\tval = PRS_MEMBER;\n\telse if (!strcmp(buf, \"isolated\"))\n\t\tval = PRS_ISOLATED;\n\telse\n\t\treturn -EINVAL;\n\n\tcss_get(&cs->css);\n\tcpus_read_lock();\n\tmutex_lock(&cpuset_mutex);\n\tif (!is_cpuset_online(cs))\n\t\tgoto out_unlock;\n\n\tretval = update_prstate(cs, val);\nout_unlock:\n\tmutex_unlock(&cpuset_mutex);\n\tcpus_read_unlock();\n\tcss_put(&cs->css);\n\treturn retval ?: nbytes;\n}\n\n \n\nstatic struct cftype legacy_files[] = {\n\t{\n\t\t.name = \"cpus\",\n\t\t.seq_show = cpuset_common_seq_show,\n\t\t.write = cpuset_write_resmask,\n\t\t.max_write_len = (100U + 6 * NR_CPUS),\n\t\t.private = FILE_CPULIST,\n\t},\n\n\t{\n\t\t.name = \"mems\",\n\t\t.seq_show = cpuset_common_seq_show,\n\t\t.write = cpuset_write_resmask,\n\t\t.max_write_len = (100U + 6 * MAX_NUMNODES),\n\t\t.private = FILE_MEMLIST,\n\t},\n\n\t{\n\t\t.name = \"effective_cpus\",\n\t\t.seq_show = cpuset_common_seq_show,\n\t\t.private = FILE_EFFECTIVE_CPULIST,\n\t},\n\n\t{\n\t\t.name = \"effective_mems\",\n\t\t.seq_show = cpuset_common_seq_show,\n\t\t.private = FILE_EFFECTIVE_MEMLIST,\n\t},\n\n\t{\n\t\t.name = \"cpu_exclusive\",\n\t\t.read_u64 = cpuset_read_u64,\n\t\t.write_u64 = cpuset_write_u64,\n\t\t.private = FILE_CPU_EXCLUSIVE,\n\t},\n\n\t{\n\t\t.name = \"mem_exclusive\",\n\t\t.read_u64 = cpuset_read_u64,\n\t\t.write_u64 = cpuset_write_u64,\n\t\t.private = FILE_MEM_EXCLUSIVE,\n\t},\n\n\t{\n\t\t.name = \"mem_hardwall\",\n\t\t.read_u64 = cpuset_read_u64,\n\t\t.write_u64 = cpuset_write_u64,\n\t\t.private = FILE_MEM_HARDWALL,\n\t},\n\n\t{\n\t\t.name = \"sched_load_balance\",\n\t\t.read_u64 = cpuset_read_u64,\n\t\t.write_u64 = cpuset_write_u64,\n\t\t.private = FILE_SCHED_LOAD_BALANCE,\n\t},\n\n\t{\n\t\t.name = \"sched_relax_domain_level\",\n\t\t.read_s64 = cpuset_read_s64,\n\t\t.write_s64 = cpuset_write_s64,\n\t\t.private = FILE_SCHED_RELAX_DOMAIN_LEVEL,\n\t},\n\n\t{\n\t\t.name = \"memory_migrate\",\n\t\t.read_u64 = cpuset_read_u64,\n\t\t.write_u64 = cpuset_write_u64,\n\t\t.private = FILE_MEMORY_MIGRATE,\n\t},\n\n\t{\n\t\t.name = \"memory_pressure\",\n\t\t.read_u64 = cpuset_read_u64,\n\t\t.private = FILE_MEMORY_PRESSURE,\n\t},\n\n\t{\n\t\t.name = \"memory_spread_page\",\n\t\t.read_u64 = cpuset_read_u64,\n\t\t.write_u64 = cpuset_write_u64,\n\t\t.private = FILE_SPREAD_PAGE,\n\t},\n\n\t{\n\t\t.name = \"memory_spread_slab\",\n\t\t.read_u64 = cpuset_read_u64,\n\t\t.write_u64 = cpuset_write_u64,\n\t\t.private = FILE_SPREAD_SLAB,\n\t},\n\n\t{\n\t\t.name = \"memory_pressure_enabled\",\n\t\t.flags = CFTYPE_ONLY_ON_ROOT,\n\t\t.read_u64 = cpuset_read_u64,\n\t\t.write_u64 = cpuset_write_u64,\n\t\t.private = FILE_MEMORY_PRESSURE_ENABLED,\n\t},\n\n\t{ }\t \n};\n\n \nstatic struct cftype dfl_files[] = {\n\t{\n\t\t.name = \"cpus\",\n\t\t.seq_show = cpuset_common_seq_show,\n\t\t.write = cpuset_write_resmask,\n\t\t.max_write_len = (100U + 6 * NR_CPUS),\n\t\t.private = FILE_CPULIST,\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t},\n\n\t{\n\t\t.name = \"mems\",\n\t\t.seq_show = cpuset_common_seq_show,\n\t\t.write = cpuset_write_resmask,\n\t\t.max_write_len = (100U + 6 * MAX_NUMNODES),\n\t\t.private = FILE_MEMLIST,\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t},\n\n\t{\n\t\t.name = \"cpus.effective\",\n\t\t.seq_show = cpuset_common_seq_show,\n\t\t.private = FILE_EFFECTIVE_CPULIST,\n\t},\n\n\t{\n\t\t.name = \"mems.effective\",\n\t\t.seq_show = cpuset_common_seq_show,\n\t\t.private = FILE_EFFECTIVE_MEMLIST,\n\t},\n\n\t{\n\t\t.name = \"cpus.partition\",\n\t\t.seq_show = sched_partition_show,\n\t\t.write = sched_partition_write,\n\t\t.private = FILE_PARTITION_ROOT,\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.file_offset = offsetof(struct cpuset, partition_file),\n\t},\n\n\t{\n\t\t.name = \"cpus.subpartitions\",\n\t\t.seq_show = cpuset_common_seq_show,\n\t\t.private = FILE_SUBPARTS_CPULIST,\n\t\t.flags = CFTYPE_DEBUG,\n\t},\n\n\t{ }\t \n};\n\n\n \nstatic struct cgroup_subsys_state *\ncpuset_css_alloc(struct cgroup_subsys_state *parent_css)\n{\n\tstruct cpuset *cs;\n\n\tif (!parent_css)\n\t\treturn &top_cpuset.css;\n\n\tcs = kzalloc(sizeof(*cs), GFP_KERNEL);\n\tif (!cs)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (alloc_cpumasks(cs, NULL)) {\n\t\tkfree(cs);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\t__set_bit(CS_SCHED_LOAD_BALANCE, &cs->flags);\n\tnodes_clear(cs->mems_allowed);\n\tnodes_clear(cs->effective_mems);\n\tfmeter_init(&cs->fmeter);\n\tcs->relax_domain_level = -1;\n\n\t \n\tif (cgroup_subsys_on_dfl(cpuset_cgrp_subsys))\n\t\t__set_bit(CS_MEMORY_MIGRATE, &cs->flags);\n\n\treturn &cs->css;\n}\n\nstatic int cpuset_css_online(struct cgroup_subsys_state *css)\n{\n\tstruct cpuset *cs = css_cs(css);\n\tstruct cpuset *parent = parent_cs(cs);\n\tstruct cpuset *tmp_cs;\n\tstruct cgroup_subsys_state *pos_css;\n\n\tif (!parent)\n\t\treturn 0;\n\n\tcpus_read_lock();\n\tmutex_lock(&cpuset_mutex);\n\n\tset_bit(CS_ONLINE, &cs->flags);\n\tif (is_spread_page(parent))\n\t\tset_bit(CS_SPREAD_PAGE, &cs->flags);\n\tif (is_spread_slab(parent))\n\t\tset_bit(CS_SPREAD_SLAB, &cs->flags);\n\n\tcpuset_inc();\n\n\tspin_lock_irq(&callback_lock);\n\tif (is_in_v2_mode()) {\n\t\tcpumask_copy(cs->effective_cpus, parent->effective_cpus);\n\t\tcs->effective_mems = parent->effective_mems;\n\t\tcs->use_parent_ecpus = true;\n\t\tparent->child_ecpus_count++;\n\t}\n\n\t \n\tif (cgroup_subsys_on_dfl(cpuset_cgrp_subsys) &&\n\t    !is_sched_load_balance(parent))\n\t\tclear_bit(CS_SCHED_LOAD_BALANCE, &cs->flags);\n\n\tspin_unlock_irq(&callback_lock);\n\n\tif (!test_bit(CGRP_CPUSET_CLONE_CHILDREN, &css->cgroup->flags))\n\t\tgoto out_unlock;\n\n\t \n\trcu_read_lock();\n\tcpuset_for_each_child(tmp_cs, pos_css, parent) {\n\t\tif (is_mem_exclusive(tmp_cs) || is_cpu_exclusive(tmp_cs)) {\n\t\t\trcu_read_unlock();\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\tspin_lock_irq(&callback_lock);\n\tcs->mems_allowed = parent->mems_allowed;\n\tcs->effective_mems = parent->mems_allowed;\n\tcpumask_copy(cs->cpus_allowed, parent->cpus_allowed);\n\tcpumask_copy(cs->effective_cpus, parent->cpus_allowed);\n\tspin_unlock_irq(&callback_lock);\nout_unlock:\n\tmutex_unlock(&cpuset_mutex);\n\tcpus_read_unlock();\n\treturn 0;\n}\n\n \n\nstatic void cpuset_css_offline(struct cgroup_subsys_state *css)\n{\n\tstruct cpuset *cs = css_cs(css);\n\n\tcpus_read_lock();\n\tmutex_lock(&cpuset_mutex);\n\n\tif (is_partition_valid(cs))\n\t\tupdate_prstate(cs, 0);\n\n\tif (!cgroup_subsys_on_dfl(cpuset_cgrp_subsys) &&\n\t    is_sched_load_balance(cs))\n\t\tupdate_flag(CS_SCHED_LOAD_BALANCE, cs, 0);\n\n\tif (cs->use_parent_ecpus) {\n\t\tstruct cpuset *parent = parent_cs(cs);\n\n\t\tcs->use_parent_ecpus = false;\n\t\tparent->child_ecpus_count--;\n\t}\n\n\tcpuset_dec();\n\tclear_bit(CS_ONLINE, &cs->flags);\n\n\tmutex_unlock(&cpuset_mutex);\n\tcpus_read_unlock();\n}\n\nstatic void cpuset_css_free(struct cgroup_subsys_state *css)\n{\n\tstruct cpuset *cs = css_cs(css);\n\n\tfree_cpuset(cs);\n}\n\nstatic void cpuset_bind(struct cgroup_subsys_state *root_css)\n{\n\tmutex_lock(&cpuset_mutex);\n\tspin_lock_irq(&callback_lock);\n\n\tif (is_in_v2_mode()) {\n\t\tcpumask_copy(top_cpuset.cpus_allowed, cpu_possible_mask);\n\t\ttop_cpuset.mems_allowed = node_possible_map;\n\t} else {\n\t\tcpumask_copy(top_cpuset.cpus_allowed,\n\t\t\t     top_cpuset.effective_cpus);\n\t\ttop_cpuset.mems_allowed = top_cpuset.effective_mems;\n\t}\n\n\tspin_unlock_irq(&callback_lock);\n\tmutex_unlock(&cpuset_mutex);\n}\n\n \nstatic int cpuset_can_fork(struct task_struct *task, struct css_set *cset)\n{\n\tstruct cpuset *cs = css_cs(cset->subsys[cpuset_cgrp_id]);\n\tbool same_cs;\n\tint ret;\n\n\trcu_read_lock();\n\tsame_cs = (cs == task_cs(current));\n\trcu_read_unlock();\n\n\tif (same_cs)\n\t\treturn 0;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\tmutex_lock(&cpuset_mutex);\n\n\t \n\tret = cpuset_can_attach_check(cs);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tret = task_can_attach(task);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tret = security_task_setscheduler(task);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\t \n\tcs->attach_in_progress++;\nout_unlock:\n\tmutex_unlock(&cpuset_mutex);\n\treturn ret;\n}\n\nstatic void cpuset_cancel_fork(struct task_struct *task, struct css_set *cset)\n{\n\tstruct cpuset *cs = css_cs(cset->subsys[cpuset_cgrp_id]);\n\tbool same_cs;\n\n\trcu_read_lock();\n\tsame_cs = (cs == task_cs(current));\n\trcu_read_unlock();\n\n\tif (same_cs)\n\t\treturn;\n\n\tmutex_lock(&cpuset_mutex);\n\tcs->attach_in_progress--;\n\tif (!cs->attach_in_progress)\n\t\twake_up(&cpuset_attach_wq);\n\tmutex_unlock(&cpuset_mutex);\n}\n\n \nstatic void cpuset_fork(struct task_struct *task)\n{\n\tstruct cpuset *cs;\n\tbool same_cs;\n\n\trcu_read_lock();\n\tcs = task_cs(task);\n\tsame_cs = (cs == task_cs(current));\n\trcu_read_unlock();\n\n\tif (same_cs) {\n\t\tif (cs == &top_cpuset)\n\t\t\treturn;\n\n\t\tset_cpus_allowed_ptr(task, current->cpus_ptr);\n\t\ttask->mems_allowed = current->mems_allowed;\n\t\treturn;\n\t}\n\n\t \n\tmutex_lock(&cpuset_mutex);\n\tguarantee_online_mems(cs, &cpuset_attach_nodemask_to);\n\tcpuset_attach_task(cs, task);\n\n\tcs->attach_in_progress--;\n\tif (!cs->attach_in_progress)\n\t\twake_up(&cpuset_attach_wq);\n\n\tmutex_unlock(&cpuset_mutex);\n}\n\nstruct cgroup_subsys cpuset_cgrp_subsys = {\n\t.css_alloc\t= cpuset_css_alloc,\n\t.css_online\t= cpuset_css_online,\n\t.css_offline\t= cpuset_css_offline,\n\t.css_free\t= cpuset_css_free,\n\t.can_attach\t= cpuset_can_attach,\n\t.cancel_attach\t= cpuset_cancel_attach,\n\t.attach\t\t= cpuset_attach,\n\t.post_attach\t= cpuset_post_attach,\n\t.bind\t\t= cpuset_bind,\n\t.can_fork\t= cpuset_can_fork,\n\t.cancel_fork\t= cpuset_cancel_fork,\n\t.fork\t\t= cpuset_fork,\n\t.legacy_cftypes\t= legacy_files,\n\t.dfl_cftypes\t= dfl_files,\n\t.early_init\t= true,\n\t.threaded\t= true,\n};\n\n \n\nint __init cpuset_init(void)\n{\n\tBUG_ON(!alloc_cpumask_var(&top_cpuset.cpus_allowed, GFP_KERNEL));\n\tBUG_ON(!alloc_cpumask_var(&top_cpuset.effective_cpus, GFP_KERNEL));\n\tBUG_ON(!zalloc_cpumask_var(&top_cpuset.subparts_cpus, GFP_KERNEL));\n\n\tcpumask_setall(top_cpuset.cpus_allowed);\n\tnodes_setall(top_cpuset.mems_allowed);\n\tcpumask_setall(top_cpuset.effective_cpus);\n\tnodes_setall(top_cpuset.effective_mems);\n\n\tfmeter_init(&top_cpuset.fmeter);\n\tset_bit(CS_SCHED_LOAD_BALANCE, &top_cpuset.flags);\n\ttop_cpuset.relax_domain_level = -1;\n\n\tBUG_ON(!alloc_cpumask_var(&cpus_attach, GFP_KERNEL));\n\n\treturn 0;\n}\n\n \nstatic void remove_tasks_in_empty_cpuset(struct cpuset *cs)\n{\n\tstruct cpuset *parent;\n\n\t \n\tparent = parent_cs(cs);\n\twhile (cpumask_empty(parent->cpus_allowed) ||\n\t\t\tnodes_empty(parent->mems_allowed))\n\t\tparent = parent_cs(parent);\n\n\tif (cgroup_transfer_tasks(parent->css.cgroup, cs->css.cgroup)) {\n\t\tpr_err(\"cpuset: failed to transfer tasks out of empty cpuset \");\n\t\tpr_cont_cgroup_name(cs->css.cgroup);\n\t\tpr_cont(\"\\n\");\n\t}\n}\n\nstatic void\nhotplug_update_tasks_legacy(struct cpuset *cs,\n\t\t\t    struct cpumask *new_cpus, nodemask_t *new_mems,\n\t\t\t    bool cpus_updated, bool mems_updated)\n{\n\tbool is_empty;\n\n\tspin_lock_irq(&callback_lock);\n\tcpumask_copy(cs->cpus_allowed, new_cpus);\n\tcpumask_copy(cs->effective_cpus, new_cpus);\n\tcs->mems_allowed = *new_mems;\n\tcs->effective_mems = *new_mems;\n\tspin_unlock_irq(&callback_lock);\n\n\t \n\tif (cpus_updated && !cpumask_empty(cs->cpus_allowed))\n\t\tupdate_tasks_cpumask(cs, new_cpus);\n\tif (mems_updated && !nodes_empty(cs->mems_allowed))\n\t\tupdate_tasks_nodemask(cs);\n\n\tis_empty = cpumask_empty(cs->cpus_allowed) ||\n\t\t   nodes_empty(cs->mems_allowed);\n\n\t \n\tif (is_empty) {\n\t\tmutex_unlock(&cpuset_mutex);\n\t\tremove_tasks_in_empty_cpuset(cs);\n\t\tmutex_lock(&cpuset_mutex);\n\t}\n}\n\nstatic void\nhotplug_update_tasks(struct cpuset *cs,\n\t\t     struct cpumask *new_cpus, nodemask_t *new_mems,\n\t\t     bool cpus_updated, bool mems_updated)\n{\n\t \n\tif (cpumask_empty(new_cpus) && !is_partition_valid(cs))\n\t\tcpumask_copy(new_cpus, parent_cs(cs)->effective_cpus);\n\tif (nodes_empty(*new_mems))\n\t\t*new_mems = parent_cs(cs)->effective_mems;\n\n\tspin_lock_irq(&callback_lock);\n\tcpumask_copy(cs->effective_cpus, new_cpus);\n\tcs->effective_mems = *new_mems;\n\tspin_unlock_irq(&callback_lock);\n\n\tif (cpus_updated)\n\t\tupdate_tasks_cpumask(cs, new_cpus);\n\tif (mems_updated)\n\t\tupdate_tasks_nodemask(cs);\n}\n\nstatic bool force_rebuild;\n\nvoid cpuset_force_rebuild(void)\n{\n\tforce_rebuild = true;\n}\n\n \nstatic void cpuset_hotplug_update_tasks(struct cpuset *cs, struct tmpmasks *tmp)\n{\n\tstatic cpumask_t new_cpus;\n\tstatic nodemask_t new_mems;\n\tbool cpus_updated;\n\tbool mems_updated;\n\tstruct cpuset *parent;\nretry:\n\twait_event(cpuset_attach_wq, cs->attach_in_progress == 0);\n\n\tmutex_lock(&cpuset_mutex);\n\n\t \n\tif (cs->attach_in_progress) {\n\t\tmutex_unlock(&cpuset_mutex);\n\t\tgoto retry;\n\t}\n\n\tparent = parent_cs(cs);\n\tcompute_effective_cpumask(&new_cpus, cs, parent);\n\tnodes_and(new_mems, cs->mems_allowed, parent->effective_mems);\n\n\tif (cs->nr_subparts_cpus)\n\t\t \n\t\tcpumask_andnot(&new_cpus, &new_cpus, cs->subparts_cpus);\n\n\tif (!tmp || !cs->partition_root_state)\n\t\tgoto update_tasks;\n\n\t \n\tif (cs->nr_subparts_cpus && is_partition_valid(cs) &&\n\t    cpumask_empty(&new_cpus) && partition_is_populated(cs, NULL)) {\n\t\tspin_lock_irq(&callback_lock);\n\t\tcs->nr_subparts_cpus = 0;\n\t\tcpumask_clear(cs->subparts_cpus);\n\t\tspin_unlock_irq(&callback_lock);\n\t\tcompute_effective_cpumask(&new_cpus, cs, parent);\n\t}\n\n\t \n\tif (is_partition_valid(cs) && (!parent->nr_subparts_cpus ||\n\t   (cpumask_empty(&new_cpus) && partition_is_populated(cs, NULL)))) {\n\t\tint old_prs, parent_prs;\n\n\t\tupdate_parent_subparts_cpumask(cs, partcmd_disable, NULL, tmp);\n\t\tif (cs->nr_subparts_cpus) {\n\t\t\tspin_lock_irq(&callback_lock);\n\t\t\tcs->nr_subparts_cpus = 0;\n\t\t\tcpumask_clear(cs->subparts_cpus);\n\t\t\tspin_unlock_irq(&callback_lock);\n\t\t\tcompute_effective_cpumask(&new_cpus, cs, parent);\n\t\t}\n\n\t\told_prs = cs->partition_root_state;\n\t\tparent_prs = parent->partition_root_state;\n\t\tif (is_partition_valid(cs)) {\n\t\t\tspin_lock_irq(&callback_lock);\n\t\t\tmake_partition_invalid(cs);\n\t\t\tspin_unlock_irq(&callback_lock);\n\t\t\tif (is_prs_invalid(parent_prs))\n\t\t\t\tWRITE_ONCE(cs->prs_err, PERR_INVPARENT);\n\t\t\telse if (!parent_prs)\n\t\t\t\tWRITE_ONCE(cs->prs_err, PERR_NOTPART);\n\t\t\telse\n\t\t\t\tWRITE_ONCE(cs->prs_err, PERR_HOTPLUG);\n\t\t\tnotify_partition_change(cs, old_prs);\n\t\t}\n\t\tcpuset_force_rebuild();\n\t}\n\n\t \n\telse if (is_partition_valid(parent) && is_partition_invalid(cs)) {\n\t\tupdate_parent_subparts_cpumask(cs, partcmd_update, NULL, tmp);\n\t\tif (is_partition_valid(cs))\n\t\t\tcpuset_force_rebuild();\n\t}\n\nupdate_tasks:\n\tcpus_updated = !cpumask_equal(&new_cpus, cs->effective_cpus);\n\tmems_updated = !nodes_equal(new_mems, cs->effective_mems);\n\tif (!cpus_updated && !mems_updated)\n\t\tgoto unlock;\t \n\n\tif (mems_updated)\n\t\tcheck_insane_mems_config(&new_mems);\n\n\tif (is_in_v2_mode())\n\t\thotplug_update_tasks(cs, &new_cpus, &new_mems,\n\t\t\t\t     cpus_updated, mems_updated);\n\telse\n\t\thotplug_update_tasks_legacy(cs, &new_cpus, &new_mems,\n\t\t\t\t\t    cpus_updated, mems_updated);\n\nunlock:\n\tmutex_unlock(&cpuset_mutex);\n}\n\n \nstatic void cpuset_hotplug_workfn(struct work_struct *work)\n{\n\tstatic cpumask_t new_cpus;\n\tstatic nodemask_t new_mems;\n\tbool cpus_updated, mems_updated;\n\tbool on_dfl = is_in_v2_mode();\n\tstruct tmpmasks tmp, *ptmp = NULL;\n\n\tif (on_dfl && !alloc_cpumasks(NULL, &tmp))\n\t\tptmp = &tmp;\n\n\tmutex_lock(&cpuset_mutex);\n\n\t \n\tcpumask_copy(&new_cpus, cpu_active_mask);\n\tnew_mems = node_states[N_MEMORY];\n\n\t \n\tcpus_updated = !cpumask_equal(top_cpuset.effective_cpus, &new_cpus);\n\tmems_updated = !nodes_equal(top_cpuset.effective_mems, new_mems);\n\n\t \n\tif (!cpus_updated && top_cpuset.nr_subparts_cpus)\n\t\tcpus_updated = true;\n\n\t \n\tif (cpus_updated) {\n\t\tspin_lock_irq(&callback_lock);\n\t\tif (!on_dfl)\n\t\t\tcpumask_copy(top_cpuset.cpus_allowed, &new_cpus);\n\t\t \n\t\tif (top_cpuset.nr_subparts_cpus) {\n\t\t\tif (cpumask_subset(&new_cpus,\n\t\t\t\t\t   top_cpuset.subparts_cpus)) {\n\t\t\t\ttop_cpuset.nr_subparts_cpus = 0;\n\t\t\t\tcpumask_clear(top_cpuset.subparts_cpus);\n\t\t\t} else {\n\t\t\t\tcpumask_andnot(&new_cpus, &new_cpus,\n\t\t\t\t\t       top_cpuset.subparts_cpus);\n\t\t\t}\n\t\t}\n\t\tcpumask_copy(top_cpuset.effective_cpus, &new_cpus);\n\t\tspin_unlock_irq(&callback_lock);\n\t\t \n\t}\n\n\t \n\tif (mems_updated) {\n\t\tspin_lock_irq(&callback_lock);\n\t\tif (!on_dfl)\n\t\t\ttop_cpuset.mems_allowed = new_mems;\n\t\ttop_cpuset.effective_mems = new_mems;\n\t\tspin_unlock_irq(&callback_lock);\n\t\tupdate_tasks_nodemask(&top_cpuset);\n\t}\n\n\tmutex_unlock(&cpuset_mutex);\n\n\t \n\tif (cpus_updated || mems_updated) {\n\t\tstruct cpuset *cs;\n\t\tstruct cgroup_subsys_state *pos_css;\n\n\t\trcu_read_lock();\n\t\tcpuset_for_each_descendant_pre(cs, pos_css, &top_cpuset) {\n\t\t\tif (cs == &top_cpuset || !css_tryget_online(&cs->css))\n\t\t\t\tcontinue;\n\t\t\trcu_read_unlock();\n\n\t\t\tcpuset_hotplug_update_tasks(cs, ptmp);\n\n\t\t\trcu_read_lock();\n\t\t\tcss_put(&cs->css);\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\n\t \n\tif (cpus_updated || force_rebuild) {\n\t\tforce_rebuild = false;\n\t\trebuild_sched_domains();\n\t}\n\n\tfree_cpumasks(NULL, ptmp);\n}\n\nvoid cpuset_update_active_cpus(void)\n{\n\t \n\tschedule_work(&cpuset_hotplug_work);\n}\n\nvoid cpuset_wait_for_hotplug(void)\n{\n\tflush_work(&cpuset_hotplug_work);\n}\n\n \nstatic int cpuset_track_online_nodes(struct notifier_block *self,\n\t\t\t\tunsigned long action, void *arg)\n{\n\tschedule_work(&cpuset_hotplug_work);\n\treturn NOTIFY_OK;\n}\n\n \nvoid __init cpuset_init_smp(void)\n{\n\t \n\ttop_cpuset.old_mems_allowed = top_cpuset.mems_allowed;\n\n\tcpumask_copy(top_cpuset.effective_cpus, cpu_active_mask);\n\ttop_cpuset.effective_mems = node_states[N_MEMORY];\n\n\thotplug_memory_notifier(cpuset_track_online_nodes, CPUSET_CALLBACK_PRI);\n\n\tcpuset_migrate_mm_wq = alloc_ordered_workqueue(\"cpuset_migrate_mm\", 0);\n\tBUG_ON(!cpuset_migrate_mm_wq);\n}\n\n \n\nvoid cpuset_cpus_allowed(struct task_struct *tsk, struct cpumask *pmask)\n{\n\tunsigned long flags;\n\tstruct cpuset *cs;\n\n\tspin_lock_irqsave(&callback_lock, flags);\n\trcu_read_lock();\n\n\tcs = task_cs(tsk);\n\tif (cs != &top_cpuset)\n\t\tguarantee_online_cpus(tsk, pmask);\n\t \n\tif ((cs == &top_cpuset) || cpumask_empty(pmask)) {\n\t\tconst struct cpumask *possible_mask = task_cpu_possible_mask(tsk);\n\n\t\t \n\t\tcpumask_andnot(pmask, possible_mask, top_cpuset.subparts_cpus);\n\t\tif (!cpumask_intersects(pmask, cpu_online_mask))\n\t\t\tcpumask_copy(pmask, possible_mask);\n\t}\n\n\trcu_read_unlock();\n\tspin_unlock_irqrestore(&callback_lock, flags);\n}\n\n \n\nbool cpuset_cpus_allowed_fallback(struct task_struct *tsk)\n{\n\tconst struct cpumask *possible_mask = task_cpu_possible_mask(tsk);\n\tconst struct cpumask *cs_mask;\n\tbool changed = false;\n\n\trcu_read_lock();\n\tcs_mask = task_cs(tsk)->cpus_allowed;\n\tif (is_in_v2_mode() && cpumask_subset(cs_mask, possible_mask)) {\n\t\tdo_set_cpus_allowed(tsk, cs_mask);\n\t\tchanged = true;\n\t}\n\trcu_read_unlock();\n\n\t \n\treturn changed;\n}\n\nvoid __init cpuset_init_current_mems_allowed(void)\n{\n\tnodes_setall(current->mems_allowed);\n}\n\n \n\nnodemask_t cpuset_mems_allowed(struct task_struct *tsk)\n{\n\tnodemask_t mask;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&callback_lock, flags);\n\trcu_read_lock();\n\tguarantee_online_mems(task_cs(tsk), &mask);\n\trcu_read_unlock();\n\tspin_unlock_irqrestore(&callback_lock, flags);\n\n\treturn mask;\n}\n\n \nint cpuset_nodemask_valid_mems_allowed(nodemask_t *nodemask)\n{\n\treturn nodes_intersects(*nodemask, current->mems_allowed);\n}\n\n \nstatic struct cpuset *nearest_hardwall_ancestor(struct cpuset *cs)\n{\n\twhile (!(is_mem_exclusive(cs) || is_mem_hardwall(cs)) && parent_cs(cs))\n\t\tcs = parent_cs(cs);\n\treturn cs;\n}\n\n \nbool cpuset_node_allowed(int node, gfp_t gfp_mask)\n{\n\tstruct cpuset *cs;\t\t \n\tbool allowed;\t\t\t \n\tunsigned long flags;\n\n\tif (in_interrupt())\n\t\treturn true;\n\tif (node_isset(node, current->mems_allowed))\n\t\treturn true;\n\t \n\tif (unlikely(tsk_is_oom_victim(current)))\n\t\treturn true;\n\tif (gfp_mask & __GFP_HARDWALL)\t \n\t\treturn false;\n\n\tif (current->flags & PF_EXITING)  \n\t\treturn true;\n\n\t \n\tspin_lock_irqsave(&callback_lock, flags);\n\n\trcu_read_lock();\n\tcs = nearest_hardwall_ancestor(task_cs(current));\n\tallowed = node_isset(node, cs->mems_allowed);\n\trcu_read_unlock();\n\n\tspin_unlock_irqrestore(&callback_lock, flags);\n\treturn allowed;\n}\n\n \nstatic int cpuset_spread_node(int *rotor)\n{\n\treturn *rotor = next_node_in(*rotor, current->mems_allowed);\n}\n\n \nint cpuset_mem_spread_node(void)\n{\n\tif (current->cpuset_mem_spread_rotor == NUMA_NO_NODE)\n\t\tcurrent->cpuset_mem_spread_rotor =\n\t\t\tnode_random(&current->mems_allowed);\n\n\treturn cpuset_spread_node(&current->cpuset_mem_spread_rotor);\n}\n\n \nint cpuset_slab_spread_node(void)\n{\n\tif (current->cpuset_slab_spread_rotor == NUMA_NO_NODE)\n\t\tcurrent->cpuset_slab_spread_rotor =\n\t\t\tnode_random(&current->mems_allowed);\n\n\treturn cpuset_spread_node(&current->cpuset_slab_spread_rotor);\n}\nEXPORT_SYMBOL_GPL(cpuset_mem_spread_node);\n\n \n\nint cpuset_mems_allowed_intersects(const struct task_struct *tsk1,\n\t\t\t\t   const struct task_struct *tsk2)\n{\n\treturn nodes_intersects(tsk1->mems_allowed, tsk2->mems_allowed);\n}\n\n \nvoid cpuset_print_current_mems_allowed(void)\n{\n\tstruct cgroup *cgrp;\n\n\trcu_read_lock();\n\n\tcgrp = task_cs(current)->css.cgroup;\n\tpr_cont(\",cpuset=\");\n\tpr_cont_cgroup_name(cgrp);\n\tpr_cont(\",mems_allowed=%*pbl\",\n\t\tnodemask_pr_args(&current->mems_allowed));\n\n\trcu_read_unlock();\n}\n\n \n\nint cpuset_memory_pressure_enabled __read_mostly;\n\n \n\nvoid __cpuset_memory_pressure_bump(void)\n{\n\trcu_read_lock();\n\tfmeter_markevent(&task_cs(current)->fmeter);\n\trcu_read_unlock();\n}\n\n#ifdef CONFIG_PROC_PID_CPUSET\n \nint proc_cpuset_show(struct seq_file *m, struct pid_namespace *ns,\n\t\t     struct pid *pid, struct task_struct *tsk)\n{\n\tchar *buf;\n\tstruct cgroup_subsys_state *css;\n\tint retval;\n\n\tretval = -ENOMEM;\n\tbuf = kmalloc(PATH_MAX, GFP_KERNEL);\n\tif (!buf)\n\t\tgoto out;\n\n\tcss = task_get_css(tsk, cpuset_cgrp_id);\n\tretval = cgroup_path_ns(css->cgroup, buf, PATH_MAX,\n\t\t\t\tcurrent->nsproxy->cgroup_ns);\n\tcss_put(css);\n\tif (retval >= PATH_MAX)\n\t\tretval = -ENAMETOOLONG;\n\tif (retval < 0)\n\t\tgoto out_free;\n\tseq_puts(m, buf);\n\tseq_putc(m, '\\n');\n\tretval = 0;\nout_free:\n\tkfree(buf);\nout:\n\treturn retval;\n}\n#endif  \n\n \nvoid cpuset_task_status_allowed(struct seq_file *m, struct task_struct *task)\n{\n\tseq_printf(m, \"Mems_allowed:\\t%*pb\\n\",\n\t\t   nodemask_pr_args(&task->mems_allowed));\n\tseq_printf(m, \"Mems_allowed_list:\\t%*pbl\\n\",\n\t\t   nodemask_pr_args(&task->mems_allowed));\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}