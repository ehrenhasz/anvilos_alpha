{
  "module_name": "rcu.h",
  "hash_id": "56f4cdedac9d34c242fe51d3c00ffdfbfe924c1b1702a9259471d40542519e7e",
  "original_prompt": "Ingested from linux-6.6.14/kernel/rcu/rcu.h",
  "human_readable_source": " \n \n\n#ifndef __LINUX_RCU_H\n#define __LINUX_RCU_H\n\n#include <trace/events/rcu.h>\n\n \n\n#define RCU_SEQ_CTR_SHIFT\t2\n#define RCU_SEQ_STATE_MASK\t((1 << RCU_SEQ_CTR_SHIFT) - 1)\n\n \n#define RCU_GET_STATE_COMPLETED\t0x1\n\nextern int sysctl_sched_rt_runtime;\n\n \nstatic inline unsigned long rcu_seq_ctr(unsigned long s)\n{\n\treturn s >> RCU_SEQ_CTR_SHIFT;\n}\n\n \nstatic inline int rcu_seq_state(unsigned long s)\n{\n\treturn s & RCU_SEQ_STATE_MASK;\n}\n\n \nstatic inline void rcu_seq_set_state(unsigned long *sp, int newstate)\n{\n\tWARN_ON_ONCE(newstate & ~RCU_SEQ_STATE_MASK);\n\tWRITE_ONCE(*sp, (*sp & ~RCU_SEQ_STATE_MASK) + newstate);\n}\n\n \nstatic inline void rcu_seq_start(unsigned long *sp)\n{\n\tWRITE_ONCE(*sp, *sp + 1);\n\tsmp_mb();  \n\tWARN_ON_ONCE(rcu_seq_state(*sp) != 1);\n}\n\n \nstatic inline unsigned long rcu_seq_endval(unsigned long *sp)\n{\n\treturn (*sp | RCU_SEQ_STATE_MASK) + 1;\n}\n\n \nstatic inline void rcu_seq_end(unsigned long *sp)\n{\n\tsmp_mb();  \n\tWARN_ON_ONCE(!rcu_seq_state(*sp));\n\tWRITE_ONCE(*sp, rcu_seq_endval(sp));\n}\n\n \nstatic inline unsigned long rcu_seq_snap(unsigned long *sp)\n{\n\tunsigned long s;\n\n\ts = (READ_ONCE(*sp) + 2 * RCU_SEQ_STATE_MASK + 1) & ~RCU_SEQ_STATE_MASK;\n\tsmp_mb();  \n\treturn s;\n}\n\n \nstatic inline unsigned long rcu_seq_current(unsigned long *sp)\n{\n\treturn READ_ONCE(*sp);\n}\n\n \nstatic inline bool rcu_seq_started(unsigned long *sp, unsigned long s)\n{\n\treturn ULONG_CMP_LT((s - 1) & ~RCU_SEQ_STATE_MASK, READ_ONCE(*sp));\n}\n\n \nstatic inline bool rcu_seq_done(unsigned long *sp, unsigned long s)\n{\n\treturn ULONG_CMP_GE(READ_ONCE(*sp), s);\n}\n\n \nstatic inline bool rcu_seq_done_exact(unsigned long *sp, unsigned long s)\n{\n\tunsigned long cur_s = READ_ONCE(*sp);\n\n\treturn ULONG_CMP_GE(cur_s, s) || ULONG_CMP_LT(cur_s, s - (2 * RCU_SEQ_STATE_MASK + 1));\n}\n\n \nstatic inline bool rcu_seq_completed_gp(unsigned long old, unsigned long new)\n{\n\treturn ULONG_CMP_LT(old, new & ~RCU_SEQ_STATE_MASK);\n}\n\n \nstatic inline bool rcu_seq_new_gp(unsigned long old, unsigned long new)\n{\n\treturn ULONG_CMP_LT((old + RCU_SEQ_STATE_MASK) & ~RCU_SEQ_STATE_MASK,\n\t\t\t    new);\n}\n\n \nstatic inline unsigned long rcu_seq_diff(unsigned long new, unsigned long old)\n{\n\tunsigned long rnd_diff;\n\n\tif (old == new)\n\t\treturn 0;\n\t \n\trnd_diff = (new & ~RCU_SEQ_STATE_MASK) -\n\t\t   ((old + RCU_SEQ_STATE_MASK) & ~RCU_SEQ_STATE_MASK) +\n\t\t   ((new & RCU_SEQ_STATE_MASK) || (old & RCU_SEQ_STATE_MASK));\n\tif (ULONG_CMP_GE(RCU_SEQ_STATE_MASK, rnd_diff))\n\t\treturn 1;  \n\treturn ((rnd_diff - RCU_SEQ_STATE_MASK - 1) >> RCU_SEQ_CTR_SHIFT) + 2;\n}\n\n \n\n#ifdef CONFIG_DEBUG_OBJECTS_RCU_HEAD\n# define STATE_RCU_HEAD_READY\t0\n# define STATE_RCU_HEAD_QUEUED\t1\n\nextern const struct debug_obj_descr rcuhead_debug_descr;\n\nstatic inline int debug_rcu_head_queue(struct rcu_head *head)\n{\n\tint r1;\n\n\tr1 = debug_object_activate(head, &rcuhead_debug_descr);\n\tdebug_object_active_state(head, &rcuhead_debug_descr,\n\t\t\t\t  STATE_RCU_HEAD_READY,\n\t\t\t\t  STATE_RCU_HEAD_QUEUED);\n\treturn r1;\n}\n\nstatic inline void debug_rcu_head_unqueue(struct rcu_head *head)\n{\n\tdebug_object_active_state(head, &rcuhead_debug_descr,\n\t\t\t\t  STATE_RCU_HEAD_QUEUED,\n\t\t\t\t  STATE_RCU_HEAD_READY);\n\tdebug_object_deactivate(head, &rcuhead_debug_descr);\n}\n#else\t \nstatic inline int debug_rcu_head_queue(struct rcu_head *head)\n{\n\treturn 0;\n}\n\nstatic inline void debug_rcu_head_unqueue(struct rcu_head *head)\n{\n}\n#endif\t \n\nextern int rcu_cpu_stall_suppress_at_boot;\n\nstatic inline bool rcu_stall_is_suppressed_at_boot(void)\n{\n\treturn rcu_cpu_stall_suppress_at_boot && !rcu_inkernel_boot_has_ended();\n}\n\n#ifdef CONFIG_RCU_STALL_COMMON\n\nextern int rcu_cpu_stall_ftrace_dump;\nextern int rcu_cpu_stall_suppress;\nextern int rcu_cpu_stall_timeout;\nextern int rcu_exp_cpu_stall_timeout;\nextern int rcu_cpu_stall_cputime;\nextern bool rcu_exp_stall_task_details __read_mostly;\nint rcu_jiffies_till_stall_check(void);\nint rcu_exp_jiffies_till_stall_check(void);\n\nstatic inline bool rcu_stall_is_suppressed(void)\n{\n\treturn rcu_stall_is_suppressed_at_boot() || rcu_cpu_stall_suppress;\n}\n\n#define rcu_ftrace_dump_stall_suppress() \\\ndo { \\\n\tif (!rcu_cpu_stall_suppress) \\\n\t\trcu_cpu_stall_suppress = 3; \\\n} while (0)\n\n#define rcu_ftrace_dump_stall_unsuppress() \\\ndo { \\\n\tif (rcu_cpu_stall_suppress == 3) \\\n\t\trcu_cpu_stall_suppress = 0; \\\n} while (0)\n\n#else  \n\nstatic inline bool rcu_stall_is_suppressed(void)\n{\n\treturn rcu_stall_is_suppressed_at_boot();\n}\n#define rcu_ftrace_dump_stall_suppress()\n#define rcu_ftrace_dump_stall_unsuppress()\n#endif  \n\n \n#define TPS(x)  tracepoint_string(x)\n\n \n#define rcu_ftrace_dump(oops_dump_mode) \\\ndo { \\\n\tstatic atomic_t ___rfd_beenhere = ATOMIC_INIT(0); \\\n\t\\\n\tif (!atomic_read(&___rfd_beenhere) && \\\n\t    !atomic_xchg(&___rfd_beenhere, 1)) { \\\n\t\ttracing_off(); \\\n\t\trcu_ftrace_dump_stall_suppress(); \\\n\t\tftrace_dump(oops_dump_mode); \\\n\t\trcu_ftrace_dump_stall_unsuppress(); \\\n\t} \\\n} while (0)\n\nvoid rcu_early_boot_tests(void);\nvoid rcu_test_sync_prims(void);\n\n \nextern void resched_cpu(int cpu);\n\n#if !defined(CONFIG_TINY_RCU)\n\n#include <linux/rcu_node_tree.h>\n\nextern int rcu_num_lvls;\nextern int num_rcu_lvl[];\nextern int rcu_num_nodes;\nstatic bool rcu_fanout_exact;\nstatic int rcu_fanout_leaf;\n\n \nstatic inline void rcu_init_levelspread(int *levelspread, const int *levelcnt)\n{\n\tint i;\n\n\tfor (i = 0; i < RCU_NUM_LVLS; i++)\n\t\tlevelspread[i] = INT_MIN;\n\tif (rcu_fanout_exact) {\n\t\tlevelspread[rcu_num_lvls - 1] = rcu_fanout_leaf;\n\t\tfor (i = rcu_num_lvls - 2; i >= 0; i--)\n\t\t\tlevelspread[i] = RCU_FANOUT;\n\t} else {\n\t\tint ccur;\n\t\tint cprv;\n\n\t\tcprv = nr_cpu_ids;\n\t\tfor (i = rcu_num_lvls - 1; i >= 0; i--) {\n\t\t\tccur = levelcnt[i];\n\t\t\tlevelspread[i] = (cprv + ccur - 1) / ccur;\n\t\t\tcprv = ccur;\n\t\t}\n\t}\n}\n\nextern void rcu_init_geometry(void);\n\n \n#define rcu_first_leaf_node() (rcu_state.level[rcu_num_lvls - 1])\n\n \n#define rcu_is_leaf_node(rnp) ((rnp)->level == rcu_num_lvls - 1)\n\n \n#define rcu_is_last_leaf_node(rnp) ((rnp) == &rcu_state.node[rcu_num_nodes - 1])\n\n \n#define _rcu_for_each_node_breadth_first(sp, rnp) \\\n\tfor ((rnp) = &(sp)->node[0]; \\\n\t     (rnp) < &(sp)->node[rcu_num_nodes]; (rnp)++)\n#define rcu_for_each_node_breadth_first(rnp) \\\n\t_rcu_for_each_node_breadth_first(&rcu_state, rnp)\n#define srcu_for_each_node_breadth_first(ssp, rnp) \\\n\t_rcu_for_each_node_breadth_first(ssp->srcu_sup, rnp)\n\n \n#define rcu_for_each_leaf_node(rnp) \\\n\tfor ((rnp) = rcu_first_leaf_node(); \\\n\t     (rnp) < &rcu_state.node[rcu_num_nodes]; (rnp)++)\n\n \n#define for_each_leaf_node_possible_cpu(rnp, cpu) \\\n\tfor (WARN_ON_ONCE(!rcu_is_leaf_node(rnp)), \\\n\t     (cpu) = cpumask_next((rnp)->grplo - 1, cpu_possible_mask); \\\n\t     (cpu) <= rnp->grphi; \\\n\t     (cpu) = cpumask_next((cpu), cpu_possible_mask))\n\n \n#define rcu_find_next_bit(rnp, cpu, mask) \\\n\t((rnp)->grplo + find_next_bit(&(mask), BITS_PER_LONG, (cpu)))\n#define for_each_leaf_node_cpu_mask(rnp, cpu, mask) \\\n\tfor (WARN_ON_ONCE(!rcu_is_leaf_node(rnp)), \\\n\t     (cpu) = rcu_find_next_bit((rnp), 0, (mask)); \\\n\t     (cpu) <= rnp->grphi; \\\n\t     (cpu) = rcu_find_next_bit((rnp), (cpu) + 1 - (rnp->grplo), (mask)))\n\n#endif  \n\n#if !defined(CONFIG_TINY_RCU) || defined(CONFIG_TASKS_RCU_GENERIC)\n\n \n#define raw_spin_lock_rcu_node(p)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\traw_spin_lock(&ACCESS_PRIVATE(p, lock));\t\t\t\\\n\tsmp_mb__after_unlock_lock();\t\t\t\t\t\\\n} while (0)\n\n#define raw_spin_unlock_rcu_node(p)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tlockdep_assert_irqs_disabled();\t\t\t\t\t\\\n\traw_spin_unlock(&ACCESS_PRIVATE(p, lock));\t\t\t\\\n} while (0)\n\n#define raw_spin_lock_irq_rcu_node(p)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\traw_spin_lock_irq(&ACCESS_PRIVATE(p, lock));\t\t\t\\\n\tsmp_mb__after_unlock_lock();\t\t\t\t\t\\\n} while (0)\n\n#define raw_spin_unlock_irq_rcu_node(p)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tlockdep_assert_irqs_disabled();\t\t\t\t\t\\\n\traw_spin_unlock_irq(&ACCESS_PRIVATE(p, lock));\t\t\t\\\n} while (0)\n\n#define raw_spin_lock_irqsave_rcu_node(p, flags)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\traw_spin_lock_irqsave(&ACCESS_PRIVATE(p, lock), flags);\t\\\n\tsmp_mb__after_unlock_lock();\t\t\t\t\t\\\n} while (0)\n\n#define raw_spin_unlock_irqrestore_rcu_node(p, flags)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tlockdep_assert_irqs_disabled();\t\t\t\t\t\\\n\traw_spin_unlock_irqrestore(&ACCESS_PRIVATE(p, lock), flags);\t\\\n} while (0)\n\n#define raw_spin_trylock_rcu_node(p)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tbool ___locked = raw_spin_trylock(&ACCESS_PRIVATE(p, lock));\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tif (___locked)\t\t\t\t\t\t\t\\\n\t\tsmp_mb__after_unlock_lock();\t\t\t\t\\\n\t___locked;\t\t\t\t\t\t\t\\\n})\n\n#define raw_lockdep_assert_held_rcu_node(p)\t\t\t\t\\\n\tlockdep_assert_held(&ACCESS_PRIVATE(p, lock))\n\n#endif \n\n#ifdef CONFIG_TINY_RCU\n \nstatic inline bool rcu_gp_is_normal(void) { return true; }\nstatic inline bool rcu_gp_is_expedited(void) { return false; }\nstatic inline bool rcu_async_should_hurry(void) { return false; }\nstatic inline void rcu_expedite_gp(void) { }\nstatic inline void rcu_unexpedite_gp(void) { }\nstatic inline void rcu_async_hurry(void) { }\nstatic inline void rcu_async_relax(void) { }\nstatic inline bool rcu_cpu_online(int cpu) { return true; }\n#else  \nbool rcu_gp_is_normal(void);      \nbool rcu_gp_is_expedited(void);   \nbool rcu_async_should_hurry(void);   \nvoid rcu_expedite_gp(void);\nvoid rcu_unexpedite_gp(void);\nvoid rcu_async_hurry(void);\nvoid rcu_async_relax(void);\nvoid rcupdate_announce_bootup_oddness(void);\nbool rcu_cpu_online(int cpu);\n#ifdef CONFIG_TASKS_RCU_GENERIC\nvoid show_rcu_tasks_gp_kthreads(void);\n#else  \nstatic inline void show_rcu_tasks_gp_kthreads(void) {}\n#endif  \n#endif  \n\n#ifdef CONFIG_TASKS_RCU\nstruct task_struct *get_rcu_tasks_gp_kthread(void);\n#endif \n\n#ifdef CONFIG_TASKS_RUDE_RCU\nstruct task_struct *get_rcu_tasks_rude_gp_kthread(void);\n#endif \n\n#define RCU_SCHEDULER_INACTIVE\t0\n#define RCU_SCHEDULER_INIT\t1\n#define RCU_SCHEDULER_RUNNING\t2\n\nenum rcutorture_type {\n\tRCU_FLAVOR,\n\tRCU_TASKS_FLAVOR,\n\tRCU_TASKS_RUDE_FLAVOR,\n\tRCU_TASKS_TRACING_FLAVOR,\n\tRCU_TRIVIAL_FLAVOR,\n\tSRCU_FLAVOR,\n\tINVALID_RCU_FLAVOR\n};\n\n#if defined(CONFIG_RCU_LAZY)\nunsigned long rcu_lazy_get_jiffies_till_flush(void);\nvoid rcu_lazy_set_jiffies_till_flush(unsigned long j);\n#else\nstatic inline unsigned long rcu_lazy_get_jiffies_till_flush(void) { return 0; }\nstatic inline void rcu_lazy_set_jiffies_till_flush(unsigned long j) { }\n#endif\n\n#if defined(CONFIG_TREE_RCU)\nvoid rcutorture_get_gp_data(enum rcutorture_type test_type, int *flags,\n\t\t\t    unsigned long *gp_seq);\nvoid do_trace_rcu_torture_read(const char *rcutorturename,\n\t\t\t       struct rcu_head *rhp,\n\t\t\t       unsigned long secs,\n\t\t\t       unsigned long c_old,\n\t\t\t       unsigned long c);\nvoid rcu_gp_set_torture_wait(int duration);\n#else\nstatic inline void rcutorture_get_gp_data(enum rcutorture_type test_type,\n\t\t\t\t\t  int *flags, unsigned long *gp_seq)\n{\n\t*flags = 0;\n\t*gp_seq = 0;\n}\n#ifdef CONFIG_RCU_TRACE\nvoid do_trace_rcu_torture_read(const char *rcutorturename,\n\t\t\t       struct rcu_head *rhp,\n\t\t\t       unsigned long secs,\n\t\t\t       unsigned long c_old,\n\t\t\t       unsigned long c);\n#else\n#define do_trace_rcu_torture_read(rcutorturename, rhp, secs, c_old, c) \\\n\tdo { } while (0)\n#endif\nstatic inline void rcu_gp_set_torture_wait(int duration) { }\n#endif\n\n#if IS_ENABLED(CONFIG_RCU_TORTURE_TEST) || IS_MODULE(CONFIG_RCU_TORTURE_TEST)\nlong rcutorture_sched_setaffinity(pid_t pid, const struct cpumask *in_mask);\n#endif\n\n#ifdef CONFIG_TINY_SRCU\n\nstatic inline void srcutorture_get_gp_data(enum rcutorture_type test_type,\n\t\t\t\t\t   struct srcu_struct *sp, int *flags,\n\t\t\t\t\t   unsigned long *gp_seq)\n{\n\tif (test_type != SRCU_FLAVOR)\n\t\treturn;\n\t*flags = 0;\n\t*gp_seq = sp->srcu_idx;\n}\n\n#elif defined(CONFIG_TREE_SRCU)\n\nvoid srcutorture_get_gp_data(enum rcutorture_type test_type,\n\t\t\t     struct srcu_struct *sp, int *flags,\n\t\t\t     unsigned long *gp_seq);\n\n#endif\n\n#ifdef CONFIG_TINY_RCU\nstatic inline bool rcu_dynticks_zero_in_eqs(int cpu, int *vp) { return false; }\nstatic inline unsigned long rcu_get_gp_seq(void) { return 0; }\nstatic inline unsigned long rcu_exp_batches_completed(void) { return 0; }\nstatic inline unsigned long\nsrcu_batches_completed(struct srcu_struct *sp) { return 0; }\nstatic inline void rcu_force_quiescent_state(void) { }\nstatic inline bool rcu_check_boost_fail(unsigned long gp_state, int *cpup) { return true; }\nstatic inline void show_rcu_gp_kthreads(void) { }\nstatic inline int rcu_get_gp_kthreads_prio(void) { return 0; }\nstatic inline void rcu_fwd_progress_check(unsigned long j) { }\nstatic inline void rcu_gp_slow_register(atomic_t *rgssp) { }\nstatic inline void rcu_gp_slow_unregister(atomic_t *rgssp) { }\n#else  \nbool rcu_dynticks_zero_in_eqs(int cpu, int *vp);\nunsigned long rcu_get_gp_seq(void);\nunsigned long rcu_exp_batches_completed(void);\nunsigned long srcu_batches_completed(struct srcu_struct *sp);\nbool rcu_check_boost_fail(unsigned long gp_state, int *cpup);\nvoid show_rcu_gp_kthreads(void);\nint rcu_get_gp_kthreads_prio(void);\nvoid rcu_fwd_progress_check(unsigned long j);\nvoid rcu_force_quiescent_state(void);\nextern struct workqueue_struct *rcu_gp_wq;\n#ifdef CONFIG_RCU_EXP_KTHREAD\nextern struct kthread_worker *rcu_exp_gp_kworker;\nextern struct kthread_worker *rcu_exp_par_gp_kworker;\n#else  \nextern struct workqueue_struct *rcu_par_gp_wq;\n#endif  \nvoid rcu_gp_slow_register(atomic_t *rgssp);\nvoid rcu_gp_slow_unregister(atomic_t *rgssp);\n#endif  \n\n#ifdef CONFIG_RCU_NOCB_CPU\nvoid rcu_bind_current_to_nocb(void);\n#else\nstatic inline void rcu_bind_current_to_nocb(void) { }\n#endif\n\n#if !defined(CONFIG_TINY_RCU) && defined(CONFIG_TASKS_RCU)\nvoid show_rcu_tasks_classic_gp_kthread(void);\n#else\nstatic inline void show_rcu_tasks_classic_gp_kthread(void) {}\n#endif\n#if !defined(CONFIG_TINY_RCU) && defined(CONFIG_TASKS_RUDE_RCU)\nvoid show_rcu_tasks_rude_gp_kthread(void);\n#else\nstatic inline void show_rcu_tasks_rude_gp_kthread(void) {}\n#endif\n#if !defined(CONFIG_TINY_RCU) && defined(CONFIG_TASKS_TRACE_RCU)\nvoid show_rcu_tasks_trace_gp_kthread(void);\n#else\nstatic inline void show_rcu_tasks_trace_gp_kthread(void) {}\n#endif\n\n#ifdef CONFIG_TINY_RCU\nstatic inline bool rcu_cpu_beenfullyonline(int cpu) { return true; }\n#else\nbool rcu_cpu_beenfullyonline(int cpu);\n#endif\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}