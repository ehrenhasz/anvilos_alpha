{
  "module_name": "tree_exp.h",
  "hash_id": "0a54563a1fc8162e3086ac02e88c3edd64344ef485e78d22495c87006e224f29",
  "original_prompt": "Ingested from linux-6.6.14/kernel/rcu/tree_exp.h",
  "human_readable_source": " \n \n\n#include <linux/lockdep.h>\n\nstatic void rcu_exp_handler(void *unused);\nstatic int rcu_print_task_exp_stall(struct rcu_node *rnp);\nstatic void rcu_exp_print_detail_task_stall_rnp(struct rcu_node *rnp);\n\n \nstatic void rcu_exp_gp_seq_start(void)\n{\n\trcu_seq_start(&rcu_state.expedited_sequence);\n\trcu_poll_gp_seq_start_unlocked(&rcu_state.gp_seq_polled_exp_snap);\n}\n\n \nstatic __maybe_unused unsigned long rcu_exp_gp_seq_endval(void)\n{\n\treturn rcu_seq_endval(&rcu_state.expedited_sequence);\n}\n\n \nstatic void rcu_exp_gp_seq_end(void)\n{\n\trcu_poll_gp_seq_end_unlocked(&rcu_state.gp_seq_polled_exp_snap);\n\trcu_seq_end(&rcu_state.expedited_sequence);\n\tsmp_mb();  \n}\n\n \nstatic unsigned long rcu_exp_gp_seq_snap(void)\n{\n\tunsigned long s;\n\n\tsmp_mb();  \n\ts = rcu_seq_snap(&rcu_state.expedited_sequence);\n\ttrace_rcu_exp_grace_period(rcu_state.name, s, TPS(\"snap\"));\n\treturn s;\n}\n\n \nstatic bool rcu_exp_gp_seq_done(unsigned long s)\n{\n\treturn rcu_seq_done(&rcu_state.expedited_sequence, s);\n}\n\n \nstatic void sync_exp_reset_tree_hotplug(void)\n{\n\tbool done;\n\tunsigned long flags;\n\tunsigned long mask;\n\tunsigned long oldmask;\n\tint ncpus = smp_load_acquire(&rcu_state.ncpus);  \n\tstruct rcu_node *rnp;\n\tstruct rcu_node *rnp_up;\n\n\t \n\tif (likely(ncpus == rcu_state.ncpus_snap))\n\t\treturn;\n\trcu_state.ncpus_snap = ncpus;\n\n\t \n\trcu_for_each_leaf_node(rnp) {\n\t\traw_spin_lock_irqsave_rcu_node(rnp, flags);\n\t\tif (rnp->expmaskinit == rnp->expmaskinitnext) {\n\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n\t\t\tcontinue;   \n\t\t}\n\n\t\t \n\t\toldmask = rnp->expmaskinit;\n\t\trnp->expmaskinit = rnp->expmaskinitnext;\n\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n\n\t\t \n\t\tif (oldmask)\n\t\t\tcontinue;\n\n\t\t \n\t\tmask = rnp->grpmask;\n\t\trnp_up = rnp->parent;\n\t\tdone = false;\n\t\twhile (rnp_up) {\n\t\t\traw_spin_lock_irqsave_rcu_node(rnp_up, flags);\n\t\t\tif (rnp_up->expmaskinit)\n\t\t\t\tdone = true;\n\t\t\trnp_up->expmaskinit |= mask;\n\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp_up, flags);\n\t\t\tif (done)\n\t\t\t\tbreak;\n\t\t\tmask = rnp_up->grpmask;\n\t\t\trnp_up = rnp_up->parent;\n\t\t}\n\t}\n}\n\n \nstatic void __maybe_unused sync_exp_reset_tree(void)\n{\n\tunsigned long flags;\n\tstruct rcu_node *rnp;\n\n\tsync_exp_reset_tree_hotplug();\n\trcu_for_each_node_breadth_first(rnp) {\n\t\traw_spin_lock_irqsave_rcu_node(rnp, flags);\n\t\tWARN_ON_ONCE(rnp->expmask);\n\t\tWRITE_ONCE(rnp->expmask, rnp->expmaskinit);\n\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n\t}\n}\n\n \nstatic bool sync_rcu_exp_done(struct rcu_node *rnp)\n{\n\traw_lockdep_assert_held_rcu_node(rnp);\n\treturn READ_ONCE(rnp->exp_tasks) == NULL &&\n\t       READ_ONCE(rnp->expmask) == 0;\n}\n\n \nstatic bool sync_rcu_exp_done_unlocked(struct rcu_node *rnp)\n{\n\tunsigned long flags;\n\tbool ret;\n\n\traw_spin_lock_irqsave_rcu_node(rnp, flags);\n\tret = sync_rcu_exp_done(rnp);\n\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n\n\treturn ret;\n}\n\n\n \nstatic void __rcu_report_exp_rnp(struct rcu_node *rnp,\n\t\t\t\t bool wake, unsigned long flags)\n\t__releases(rnp->lock)\n{\n\tunsigned long mask;\n\n\traw_lockdep_assert_held_rcu_node(rnp);\n\tfor (;;) {\n\t\tif (!sync_rcu_exp_done(rnp)) {\n\t\t\tif (!rnp->expmask)\n\t\t\t\trcu_initiate_boost(rnp, flags);\n\t\t\telse\n\t\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n\t\t\tbreak;\n\t\t}\n\t\tif (rnp->parent == NULL) {\n\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n\t\t\tif (wake) {\n\t\t\t\tsmp_mb();  \n\t\t\t\tswake_up_one(&rcu_state.expedited_wq);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\tmask = rnp->grpmask;\n\t\traw_spin_unlock_rcu_node(rnp);  \n\t\trnp = rnp->parent;\n\t\traw_spin_lock_rcu_node(rnp);  \n\t\tWARN_ON_ONCE(!(rnp->expmask & mask));\n\t\tWRITE_ONCE(rnp->expmask, rnp->expmask & ~mask);\n\t}\n}\n\n \nstatic void __maybe_unused rcu_report_exp_rnp(struct rcu_node *rnp, bool wake)\n{\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave_rcu_node(rnp, flags);\n\t__rcu_report_exp_rnp(rnp, wake, flags);\n}\n\n \nstatic void rcu_report_exp_cpu_mult(struct rcu_node *rnp,\n\t\t\t\t    unsigned long mask, bool wake)\n{\n\tint cpu;\n\tunsigned long flags;\n\tstruct rcu_data *rdp;\n\n\traw_spin_lock_irqsave_rcu_node(rnp, flags);\n\tif (!(rnp->expmask & mask)) {\n\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n\t\treturn;\n\t}\n\tWRITE_ONCE(rnp->expmask, rnp->expmask & ~mask);\n\tfor_each_leaf_node_cpu_mask(rnp, cpu, mask) {\n\t\trdp = per_cpu_ptr(&rcu_data, cpu);\n\t\tif (!IS_ENABLED(CONFIG_NO_HZ_FULL) || !rdp->rcu_forced_tick_exp)\n\t\t\tcontinue;\n\t\trdp->rcu_forced_tick_exp = false;\n\t\ttick_dep_clear_cpu(cpu, TICK_DEP_BIT_RCU_EXP);\n\t}\n\t__rcu_report_exp_rnp(rnp, wake, flags);  \n}\n\n \nstatic void rcu_report_exp_rdp(struct rcu_data *rdp)\n{\n\tWRITE_ONCE(rdp->cpu_no_qs.b.exp, false);\n\trcu_report_exp_cpu_mult(rdp->mynode, rdp->grpmask, true);\n}\n\n \nstatic bool sync_exp_work_done(unsigned long s)\n{\n\tif (rcu_exp_gp_seq_done(s)) {\n\t\ttrace_rcu_exp_grace_period(rcu_state.name, s, TPS(\"done\"));\n\t\tsmp_mb();  \n\t\treturn true;\n\t}\n\treturn false;\n}\n\n \nstatic bool exp_funnel_lock(unsigned long s)\n{\n\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, raw_smp_processor_id());\n\tstruct rcu_node *rnp = rdp->mynode;\n\tstruct rcu_node *rnp_root = rcu_get_root();\n\n\t \n\tif (ULONG_CMP_LT(READ_ONCE(rnp->exp_seq_rq), s) &&\n\t    (rnp == rnp_root ||\n\t     ULONG_CMP_LT(READ_ONCE(rnp_root->exp_seq_rq), s)) &&\n\t    mutex_trylock(&rcu_state.exp_mutex))\n\t\tgoto fastpath;\n\n\t \n\tfor (; rnp != NULL; rnp = rnp->parent) {\n\t\tif (sync_exp_work_done(s))\n\t\t\treturn true;\n\n\t\t \n\t\tspin_lock(&rnp->exp_lock);\n\t\tif (ULONG_CMP_GE(rnp->exp_seq_rq, s)) {\n\n\t\t\t \n\t\t\tspin_unlock(&rnp->exp_lock);\n\t\t\ttrace_rcu_exp_funnel_lock(rcu_state.name, rnp->level,\n\t\t\t\t\t\t  rnp->grplo, rnp->grphi,\n\t\t\t\t\t\t  TPS(\"wait\"));\n\t\t\twait_event(rnp->exp_wq[rcu_seq_ctr(s) & 0x3],\n\t\t\t\t   sync_exp_work_done(s));\n\t\t\treturn true;\n\t\t}\n\t\tWRITE_ONCE(rnp->exp_seq_rq, s);  \n\t\tspin_unlock(&rnp->exp_lock);\n\t\ttrace_rcu_exp_funnel_lock(rcu_state.name, rnp->level,\n\t\t\t\t\t  rnp->grplo, rnp->grphi, TPS(\"nxtlvl\"));\n\t}\n\tmutex_lock(&rcu_state.exp_mutex);\nfastpath:\n\tif (sync_exp_work_done(s)) {\n\t\tmutex_unlock(&rcu_state.exp_mutex);\n\t\treturn true;\n\t}\n\trcu_exp_gp_seq_start();\n\ttrace_rcu_exp_grace_period(rcu_state.name, s, TPS(\"start\"));\n\treturn false;\n}\n\n \nstatic void __sync_rcu_exp_select_node_cpus(struct rcu_exp_work *rewp)\n{\n\tint cpu;\n\tunsigned long flags;\n\tunsigned long mask_ofl_test;\n\tunsigned long mask_ofl_ipi;\n\tint ret;\n\tstruct rcu_node *rnp = container_of(rewp, struct rcu_node, rew);\n\n\traw_spin_lock_irqsave_rcu_node(rnp, flags);\n\n\t \n\tmask_ofl_test = 0;\n\tfor_each_leaf_node_cpu_mask(rnp, cpu, rnp->expmask) {\n\t\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);\n\t\tunsigned long mask = rdp->grpmask;\n\t\tint snap;\n\n\t\tif (raw_smp_processor_id() == cpu ||\n\t\t    !(rnp->qsmaskinitnext & mask)) {\n\t\t\tmask_ofl_test |= mask;\n\t\t} else {\n\t\t\tsnap = rcu_dynticks_snap(cpu);\n\t\t\tif (rcu_dynticks_in_eqs(snap))\n\t\t\t\tmask_ofl_test |= mask;\n\t\t\telse\n\t\t\t\trdp->exp_dynticks_snap = snap;\n\t\t}\n\t}\n\tmask_ofl_ipi = rnp->expmask & ~mask_ofl_test;\n\n\t \n\tif (rcu_preempt_has_tasks(rnp))\n\t\tWRITE_ONCE(rnp->exp_tasks, rnp->blkd_tasks.next);\n\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n\n\t \n\tfor_each_leaf_node_cpu_mask(rnp, cpu, mask_ofl_ipi) {\n\t\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);\n\t\tunsigned long mask = rdp->grpmask;\n\nretry_ipi:\n\t\tif (rcu_dynticks_in_eqs_since(rdp, rdp->exp_dynticks_snap)) {\n\t\t\tmask_ofl_test |= mask;\n\t\t\tcontinue;\n\t\t}\n\t\tif (get_cpu() == cpu) {\n\t\t\tmask_ofl_test |= mask;\n\t\t\tput_cpu();\n\t\t\tcontinue;\n\t\t}\n\t\tret = smp_call_function_single(cpu, rcu_exp_handler, NULL, 0);\n\t\tput_cpu();\n\t\t \n\t\tif (!ret)\n\t\t\tcontinue;\n\n\t\t \n\t\traw_spin_lock_irqsave_rcu_node(rnp, flags);\n\t\tif ((rnp->qsmaskinitnext & mask) &&\n\t\t    (rnp->expmask & mask)) {\n\t\t\t \n\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n\t\t\ttrace_rcu_exp_grace_period(rcu_state.name, rcu_exp_gp_seq_endval(), TPS(\"selectofl\"));\n\t\t\tschedule_timeout_idle(1);\n\t\t\tgoto retry_ipi;\n\t\t}\n\t\t \n\t\tif (rnp->expmask & mask)\n\t\t\tmask_ofl_test |= mask;\n\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n\t}\n\t \n\tif (mask_ofl_test)\n\t\trcu_report_exp_cpu_mult(rnp, mask_ofl_test, false);\n}\n\nstatic void rcu_exp_sel_wait_wake(unsigned long s);\n\n#ifdef CONFIG_RCU_EXP_KTHREAD\nstatic void sync_rcu_exp_select_node_cpus(struct kthread_work *wp)\n{\n\tstruct rcu_exp_work *rewp =\n\t\tcontainer_of(wp, struct rcu_exp_work, rew_work);\n\n\t__sync_rcu_exp_select_node_cpus(rewp);\n}\n\nstatic inline bool rcu_gp_par_worker_started(void)\n{\n\treturn !!READ_ONCE(rcu_exp_par_gp_kworker);\n}\n\nstatic inline void sync_rcu_exp_select_cpus_queue_work(struct rcu_node *rnp)\n{\n\tkthread_init_work(&rnp->rew.rew_work, sync_rcu_exp_select_node_cpus);\n\t \n\tkthread_queue_work(rcu_exp_par_gp_kworker, &rnp->rew.rew_work);\n}\n\nstatic inline void sync_rcu_exp_select_cpus_flush_work(struct rcu_node *rnp)\n{\n\tkthread_flush_work(&rnp->rew.rew_work);\n}\n\n \nstatic void wait_rcu_exp_gp(struct kthread_work *wp)\n{\n\tstruct rcu_exp_work *rewp;\n\n\trewp = container_of(wp, struct rcu_exp_work, rew_work);\n\trcu_exp_sel_wait_wake(rewp->rew_s);\n}\n\nstatic inline void synchronize_rcu_expedited_queue_work(struct rcu_exp_work *rew)\n{\n\tkthread_init_work(&rew->rew_work, wait_rcu_exp_gp);\n\tkthread_queue_work(rcu_exp_gp_kworker, &rew->rew_work);\n}\n\nstatic inline void synchronize_rcu_expedited_destroy_work(struct rcu_exp_work *rew)\n{\n}\n#else  \nstatic void sync_rcu_exp_select_node_cpus(struct work_struct *wp)\n{\n\tstruct rcu_exp_work *rewp =\n\t\tcontainer_of(wp, struct rcu_exp_work, rew_work);\n\n\t__sync_rcu_exp_select_node_cpus(rewp);\n}\n\nstatic inline bool rcu_gp_par_worker_started(void)\n{\n\treturn !!READ_ONCE(rcu_par_gp_wq);\n}\n\nstatic inline void sync_rcu_exp_select_cpus_queue_work(struct rcu_node *rnp)\n{\n\tint cpu = find_next_bit(&rnp->ffmask, BITS_PER_LONG, -1);\n\n\tINIT_WORK(&rnp->rew.rew_work, sync_rcu_exp_select_node_cpus);\n\t \n\tif (unlikely(cpu > rnp->grphi - rnp->grplo))\n\t\tcpu = WORK_CPU_UNBOUND;\n\telse\n\t\tcpu += rnp->grplo;\n\tqueue_work_on(cpu, rcu_par_gp_wq, &rnp->rew.rew_work);\n}\n\nstatic inline void sync_rcu_exp_select_cpus_flush_work(struct rcu_node *rnp)\n{\n\tflush_work(&rnp->rew.rew_work);\n}\n\n \nstatic void wait_rcu_exp_gp(struct work_struct *wp)\n{\n\tstruct rcu_exp_work *rewp;\n\n\trewp = container_of(wp, struct rcu_exp_work, rew_work);\n\trcu_exp_sel_wait_wake(rewp->rew_s);\n}\n\nstatic inline void synchronize_rcu_expedited_queue_work(struct rcu_exp_work *rew)\n{\n\tINIT_WORK_ONSTACK(&rew->rew_work, wait_rcu_exp_gp);\n\tqueue_work(rcu_gp_wq, &rew->rew_work);\n}\n\nstatic inline void synchronize_rcu_expedited_destroy_work(struct rcu_exp_work *rew)\n{\n\tdestroy_work_on_stack(&rew->rew_work);\n}\n#endif  \n\n \nstatic void sync_rcu_exp_select_cpus(void)\n{\n\tstruct rcu_node *rnp;\n\n\ttrace_rcu_exp_grace_period(rcu_state.name, rcu_exp_gp_seq_endval(), TPS(\"reset\"));\n\tsync_exp_reset_tree();\n\ttrace_rcu_exp_grace_period(rcu_state.name, rcu_exp_gp_seq_endval(), TPS(\"select\"));\n\n\t \n\trcu_for_each_leaf_node(rnp) {\n\t\trnp->exp_need_flush = false;\n\t\tif (!READ_ONCE(rnp->expmask))\n\t\t\tcontinue;  \n\t\tif (!rcu_gp_par_worker_started() ||\n\t\t    rcu_scheduler_active != RCU_SCHEDULER_RUNNING ||\n\t\t    rcu_is_last_leaf_node(rnp)) {\n\t\t\t \n\t\t\tsync_rcu_exp_select_node_cpus(&rnp->rew.rew_work);\n\t\t\tcontinue;\n\t\t}\n\t\tsync_rcu_exp_select_cpus_queue_work(rnp);\n\t\trnp->exp_need_flush = true;\n\t}\n\n\t \n\trcu_for_each_leaf_node(rnp)\n\t\tif (rnp->exp_need_flush)\n\t\t\tsync_rcu_exp_select_cpus_flush_work(rnp);\n}\n\n \nstatic bool synchronize_rcu_expedited_wait_once(long tlimit)\n{\n\tint t;\n\tstruct rcu_node *rnp_root = rcu_get_root();\n\n\tt = swait_event_timeout_exclusive(rcu_state.expedited_wq,\n\t\t\t\t\t  sync_rcu_exp_done_unlocked(rnp_root),\n\t\t\t\t\t  tlimit);\n\t\n\tif (t > 0 || sync_rcu_exp_done_unlocked(rnp_root))\n\t\treturn true;\n\tWARN_ON(t < 0);   \n\treturn false;\n}\n\n \nstatic void synchronize_rcu_expedited_wait(void)\n{\n\tint cpu;\n\tunsigned long j;\n\tunsigned long jiffies_stall;\n\tunsigned long jiffies_start;\n\tunsigned long mask;\n\tint ndetected;\n\tstruct rcu_data *rdp;\n\tstruct rcu_node *rnp;\n\tstruct rcu_node *rnp_root = rcu_get_root();\n\tunsigned long flags;\n\n\ttrace_rcu_exp_grace_period(rcu_state.name, rcu_exp_gp_seq_endval(), TPS(\"startwait\"));\n\tjiffies_stall = rcu_exp_jiffies_till_stall_check();\n\tjiffies_start = jiffies;\n\tif (tick_nohz_full_enabled() && rcu_inkernel_boot_has_ended()) {\n\t\tif (synchronize_rcu_expedited_wait_once(1))\n\t\t\treturn;\n\t\trcu_for_each_leaf_node(rnp) {\n\t\t\traw_spin_lock_irqsave_rcu_node(rnp, flags);\n\t\t\tmask = READ_ONCE(rnp->expmask);\n\t\t\tfor_each_leaf_node_cpu_mask(rnp, cpu, mask) {\n\t\t\t\trdp = per_cpu_ptr(&rcu_data, cpu);\n\t\t\t\tif (rdp->rcu_forced_tick_exp)\n\t\t\t\t\tcontinue;\n\t\t\t\trdp->rcu_forced_tick_exp = true;\n\t\t\t\tif (cpu_online(cpu))\n\t\t\t\t\ttick_dep_set_cpu(cpu, TICK_DEP_BIT_RCU_EXP);\n\t\t\t}\n\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n\t\t}\n\t\tj = READ_ONCE(jiffies_till_first_fqs);\n\t\tif (synchronize_rcu_expedited_wait_once(j + HZ))\n\t\t\treturn;\n\t}\n\n\tfor (;;) {\n\t\tif (synchronize_rcu_expedited_wait_once(jiffies_stall))\n\t\t\treturn;\n\t\tif (rcu_stall_is_suppressed())\n\t\t\tcontinue;\n\t\ttrace_rcu_stall_warning(rcu_state.name, TPS(\"ExpeditedStall\"));\n\t\tpr_err(\"INFO: %s detected expedited stalls on CPUs/tasks: {\",\n\t\t       rcu_state.name);\n\t\tndetected = 0;\n\t\trcu_for_each_leaf_node(rnp) {\n\t\t\tndetected += rcu_print_task_exp_stall(rnp);\n\t\t\tfor_each_leaf_node_possible_cpu(rnp, cpu) {\n\t\t\t\tstruct rcu_data *rdp;\n\n\t\t\t\tmask = leaf_node_cpu_bit(rnp, cpu);\n\t\t\t\tif (!(READ_ONCE(rnp->expmask) & mask))\n\t\t\t\t\tcontinue;\n\t\t\t\tndetected++;\n\t\t\t\trdp = per_cpu_ptr(&rcu_data, cpu);\n\t\t\t\tpr_cont(\" %d-%c%c%c%c\", cpu,\n\t\t\t\t\t\"O.\"[!!cpu_online(cpu)],\n\t\t\t\t\t\"o.\"[!!(rdp->grpmask & rnp->expmaskinit)],\n\t\t\t\t\t\"N.\"[!!(rdp->grpmask & rnp->expmaskinitnext)],\n\t\t\t\t\t\"D.\"[!!data_race(rdp->cpu_no_qs.b.exp)]);\n\t\t\t}\n\t\t}\n\t\tpr_cont(\" } %lu jiffies s: %lu root: %#lx/%c\\n\",\n\t\t\tjiffies - jiffies_start, rcu_state.expedited_sequence,\n\t\t\tdata_race(rnp_root->expmask),\n\t\t\t\".T\"[!!data_race(rnp_root->exp_tasks)]);\n\t\tif (ndetected) {\n\t\t\tpr_err(\"blocking rcu_node structures (internal RCU debug):\");\n\t\t\trcu_for_each_node_breadth_first(rnp) {\n\t\t\t\tif (rnp == rnp_root)\n\t\t\t\t\tcontinue;  \n\t\t\t\tif (sync_rcu_exp_done_unlocked(rnp))\n\t\t\t\t\tcontinue;\n\t\t\t\tpr_cont(\" l=%u:%d-%d:%#lx/%c\",\n\t\t\t\t\trnp->level, rnp->grplo, rnp->grphi,\n\t\t\t\t\tdata_race(rnp->expmask),\n\t\t\t\t\t\".T\"[!!data_race(rnp->exp_tasks)]);\n\t\t\t}\n\t\t\tpr_cont(\"\\n\");\n\t\t}\n\t\trcu_for_each_leaf_node(rnp) {\n\t\t\tfor_each_leaf_node_possible_cpu(rnp, cpu) {\n\t\t\t\tmask = leaf_node_cpu_bit(rnp, cpu);\n\t\t\t\tif (!(READ_ONCE(rnp->expmask) & mask))\n\t\t\t\t\tcontinue;\n\t\t\t\tpreempt_disable(); \n\t\t\t\tdump_cpu_task(cpu);\n\t\t\t\tpreempt_enable();\n\t\t\t}\n\t\t\trcu_exp_print_detail_task_stall_rnp(rnp);\n\t\t}\n\t\tjiffies_stall = 3 * rcu_exp_jiffies_till_stall_check() + 3;\n\t\tpanic_on_rcu_stall();\n\t}\n}\n\n \nstatic void rcu_exp_wait_wake(unsigned long s)\n{\n\tstruct rcu_node *rnp;\n\n\tsynchronize_rcu_expedited_wait();\n\n\t\n\t\n\t\n\tmutex_lock(&rcu_state.exp_wake_mutex);\n\trcu_exp_gp_seq_end();\n\ttrace_rcu_exp_grace_period(rcu_state.name, s, TPS(\"end\"));\n\n\trcu_for_each_node_breadth_first(rnp) {\n\t\tif (ULONG_CMP_LT(READ_ONCE(rnp->exp_seq_rq), s)) {\n\t\t\tspin_lock(&rnp->exp_lock);\n\t\t\t \n\t\t\tif (ULONG_CMP_LT(rnp->exp_seq_rq, s))\n\t\t\t\tWRITE_ONCE(rnp->exp_seq_rq, s);\n\t\t\tspin_unlock(&rnp->exp_lock);\n\t\t}\n\t\tsmp_mb();  \n\t\twake_up_all(&rnp->exp_wq[rcu_seq_ctr(s) & 0x3]);\n\t}\n\ttrace_rcu_exp_grace_period(rcu_state.name, s, TPS(\"endwake\"));\n\tmutex_unlock(&rcu_state.exp_wake_mutex);\n}\n\n \nstatic void rcu_exp_sel_wait_wake(unsigned long s)\n{\n\t \n\tsync_rcu_exp_select_cpus();\n\n\t \n\trcu_exp_wait_wake(s);\n}\n\n#ifdef CONFIG_PREEMPT_RCU\n\n \nstatic void rcu_exp_handler(void *unused)\n{\n\tint depth = rcu_preempt_depth();\n\tunsigned long flags;\n\tstruct rcu_data *rdp = this_cpu_ptr(&rcu_data);\n\tstruct rcu_node *rnp = rdp->mynode;\n\tstruct task_struct *t = current;\n\n\t \n\tif (!depth) {\n\t\tif (!(preempt_count() & (PREEMPT_MASK | SOFTIRQ_MASK)) ||\n\t\t    rcu_is_cpu_rrupt_from_idle()) {\n\t\t\trcu_report_exp_rdp(rdp);\n\t\t} else {\n\t\t\tWRITE_ONCE(rdp->cpu_no_qs.b.exp, true);\n\t\t\tset_tsk_need_resched(t);\n\t\t\tset_preempt_need_resched();\n\t\t}\n\t\treturn;\n\t}\n\n\t \n\tif (depth > 0) {\n\t\traw_spin_lock_irqsave_rcu_node(rnp, flags);\n\t\tif (rnp->expmask & rdp->grpmask) {\n\t\t\tWRITE_ONCE(rdp->cpu_no_qs.b.exp, true);\n\t\t\tt->rcu_read_unlock_special.b.exp_hint = true;\n\t\t}\n\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n\t\treturn;\n\t}\n\n\t\n\tWARN_ON_ONCE(1);\n}\n\n \nstatic void sync_sched_exp_online_cleanup(int cpu)\n{\n}\n\n \nstatic int rcu_print_task_exp_stall(struct rcu_node *rnp)\n{\n\tunsigned long flags;\n\tint ndetected = 0;\n\tstruct task_struct *t;\n\n\traw_spin_lock_irqsave_rcu_node(rnp, flags);\n\tif (!rnp->exp_tasks) {\n\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n\t\treturn 0;\n\t}\n\tt = list_entry(rnp->exp_tasks->prev,\n\t\t       struct task_struct, rcu_node_entry);\n\tlist_for_each_entry_continue(t, &rnp->blkd_tasks, rcu_node_entry) {\n\t\tpr_cont(\" P%d\", t->pid);\n\t\tndetected++;\n\t}\n\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n\treturn ndetected;\n}\n\n \nstatic void rcu_exp_print_detail_task_stall_rnp(struct rcu_node *rnp)\n{\n\tunsigned long flags;\n\tstruct task_struct *t;\n\n\tif (!rcu_exp_stall_task_details)\n\t\treturn;\n\traw_spin_lock_irqsave_rcu_node(rnp, flags);\n\tif (!READ_ONCE(rnp->exp_tasks)) {\n\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n\t\treturn;\n\t}\n\tt = list_entry(rnp->exp_tasks->prev,\n\t\t       struct task_struct, rcu_node_entry);\n\tlist_for_each_entry_continue(t, &rnp->blkd_tasks, rcu_node_entry) {\n\t\t \n\t\ttouch_nmi_watchdog();\n\t\tsched_show_task(t);\n\t}\n\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n}\n\n#else  \n\n \nstatic void rcu_exp_need_qs(void)\n{\n\t__this_cpu_write(rcu_data.cpu_no_qs.b.exp, true);\n\t \n\tsmp_store_release(this_cpu_ptr(&rcu_data.rcu_urgent_qs), true);\n\tset_tsk_need_resched(current);\n\tset_preempt_need_resched();\n}\n\n \nstatic void rcu_exp_handler(void *unused)\n{\n\tstruct rcu_data *rdp = this_cpu_ptr(&rcu_data);\n\tstruct rcu_node *rnp = rdp->mynode;\n\tbool preempt_bh_enabled = !(preempt_count() & (PREEMPT_MASK | SOFTIRQ_MASK));\n\n\tif (!(READ_ONCE(rnp->expmask) & rdp->grpmask) ||\n\t    __this_cpu_read(rcu_data.cpu_no_qs.b.exp))\n\t\treturn;\n\tif (rcu_is_cpu_rrupt_from_idle() ||\n\t    (IS_ENABLED(CONFIG_PREEMPT_COUNT) && preempt_bh_enabled)) {\n\t\trcu_report_exp_rdp(this_cpu_ptr(&rcu_data));\n\t\treturn;\n\t}\n\trcu_exp_need_qs();\n}\n\n \nstatic void sync_sched_exp_online_cleanup(int cpu)\n{\n\tunsigned long flags;\n\tint my_cpu;\n\tstruct rcu_data *rdp;\n\tint ret;\n\tstruct rcu_node *rnp;\n\n\trdp = per_cpu_ptr(&rcu_data, cpu);\n\trnp = rdp->mynode;\n\tmy_cpu = get_cpu();\n\t \n\tif (!(READ_ONCE(rnp->expmask) & rdp->grpmask) ||\n\t    READ_ONCE(rdp->cpu_no_qs.b.exp)) {\n\t\tput_cpu();\n\t\treturn;\n\t}\n\t \n\tif (my_cpu == cpu) {\n\t\tlocal_irq_save(flags);\n\t\trcu_exp_need_qs();\n\t\tlocal_irq_restore(flags);\n\t\tput_cpu();\n\t\treturn;\n\t}\n\t \n\tret = smp_call_function_single(cpu, rcu_exp_handler, NULL, 0);\n\tput_cpu();\n\tWARN_ON_ONCE(ret);\n}\n\n \nstatic int rcu_print_task_exp_stall(struct rcu_node *rnp)\n{\n\treturn 0;\n}\n\n \nstatic void rcu_exp_print_detail_task_stall_rnp(struct rcu_node *rnp)\n{\n}\n\n#endif  \n\n \nvoid synchronize_rcu_expedited(void)\n{\n\tbool boottime = (rcu_scheduler_active == RCU_SCHEDULER_INIT);\n\tunsigned long flags;\n\tstruct rcu_exp_work rew;\n\tstruct rcu_node *rnp;\n\tunsigned long s;\n\n\tRCU_LOCKDEP_WARN(lock_is_held(&rcu_bh_lock_map) ||\n\t\t\t lock_is_held(&rcu_lock_map) ||\n\t\t\t lock_is_held(&rcu_sched_lock_map),\n\t\t\t \"Illegal synchronize_rcu_expedited() in RCU read-side critical section\");\n\n\t \n\tif (rcu_blocking_is_gp()) {\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\trcu_poll_gp_seq_start_unlocked(&rcu_state.gp_seq_polled_exp_snap);\n\t\trcu_poll_gp_seq_end_unlocked(&rcu_state.gp_seq_polled_exp_snap);\n\n\t\tlocal_irq_save(flags);\n\t\tWARN_ON_ONCE(num_online_cpus() > 1);\n\t\trcu_state.expedited_sequence += (1 << RCU_SEQ_CTR_SHIFT);\n\t\tlocal_irq_restore(flags);\n\t\treturn;  \n\t}\n\n\t \n\tif (rcu_gp_is_normal()) {\n\t\twait_rcu_gp(call_rcu_hurry);\n\t\treturn;\n\t}\n\n\t \n\ts = rcu_exp_gp_seq_snap();\n\tif (exp_funnel_lock(s))\n\t\treturn;   \n\n\t \n\tif (unlikely(boottime)) {\n\t\t \n\t\trcu_exp_sel_wait_wake(s);\n\t} else {\n\t\t \n\t\trew.rew_s = s;\n\t\tsynchronize_rcu_expedited_queue_work(&rew);\n\t}\n\n\t \n\trnp = rcu_get_root();\n\twait_event(rnp->exp_wq[rcu_seq_ctr(s) & 0x3],\n\t\t   sync_exp_work_done(s));\n\tsmp_mb();  \n\n\t \n\tmutex_unlock(&rcu_state.exp_mutex);\n\n\tif (likely(!boottime))\n\t\tsynchronize_rcu_expedited_destroy_work(&rew);\n}\nEXPORT_SYMBOL_GPL(synchronize_rcu_expedited);\n\n \nstatic void sync_rcu_do_polled_gp(struct work_struct *wp)\n{\n\tunsigned long flags;\n\tint i = 0;\n\tstruct rcu_node *rnp = container_of(wp, struct rcu_node, exp_poll_wq);\n\tunsigned long s;\n\n\traw_spin_lock_irqsave(&rnp->exp_poll_lock, flags);\n\ts = rnp->exp_seq_poll_rq;\n\trnp->exp_seq_poll_rq = RCU_GET_STATE_COMPLETED;\n\traw_spin_unlock_irqrestore(&rnp->exp_poll_lock, flags);\n\tif (s == RCU_GET_STATE_COMPLETED)\n\t\treturn;\n\twhile (!poll_state_synchronize_rcu(s)) {\n\t\tsynchronize_rcu_expedited();\n\t\tif (i == 10 || i == 20)\n\t\t\tpr_info(\"%s: i = %d s = %lx gp_seq_polled = %lx\\n\", __func__, i, s, READ_ONCE(rcu_state.gp_seq_polled));\n\t\ti++;\n\t}\n\traw_spin_lock_irqsave(&rnp->exp_poll_lock, flags);\n\ts = rnp->exp_seq_poll_rq;\n\tif (poll_state_synchronize_rcu(s))\n\t\trnp->exp_seq_poll_rq = RCU_GET_STATE_COMPLETED;\n\traw_spin_unlock_irqrestore(&rnp->exp_poll_lock, flags);\n}\n\n \nunsigned long start_poll_synchronize_rcu_expedited(void)\n{\n\tunsigned long flags;\n\tstruct rcu_data *rdp;\n\tstruct rcu_node *rnp;\n\tunsigned long s;\n\n\ts = get_state_synchronize_rcu();\n\trdp = per_cpu_ptr(&rcu_data, raw_smp_processor_id());\n\trnp = rdp->mynode;\n\tif (rcu_init_invoked())\n\t\traw_spin_lock_irqsave(&rnp->exp_poll_lock, flags);\n\tif (!poll_state_synchronize_rcu(s)) {\n\t\tif (rcu_init_invoked()) {\n\t\t\trnp->exp_seq_poll_rq = s;\n\t\t\tqueue_work(rcu_gp_wq, &rnp->exp_poll_wq);\n\t\t}\n\t}\n\tif (rcu_init_invoked())\n\t\traw_spin_unlock_irqrestore(&rnp->exp_poll_lock, flags);\n\n\treturn s;\n}\nEXPORT_SYMBOL_GPL(start_poll_synchronize_rcu_expedited);\n\n \nvoid start_poll_synchronize_rcu_expedited_full(struct rcu_gp_oldstate *rgosp)\n{\n\tget_state_synchronize_rcu_full(rgosp);\n\t(void)start_poll_synchronize_rcu_expedited();\n}\nEXPORT_SYMBOL_GPL(start_poll_synchronize_rcu_expedited_full);\n\n \nvoid cond_synchronize_rcu_expedited(unsigned long oldstate)\n{\n\tif (!poll_state_synchronize_rcu(oldstate))\n\t\tsynchronize_rcu_expedited();\n}\nEXPORT_SYMBOL_GPL(cond_synchronize_rcu_expedited);\n\n \nvoid cond_synchronize_rcu_expedited_full(struct rcu_gp_oldstate *rgosp)\n{\n\tif (!poll_state_synchronize_rcu_full(rgosp))\n\t\tsynchronize_rcu_expedited();\n}\nEXPORT_SYMBOL_GPL(cond_synchronize_rcu_expedited_full);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}