{
  "module_name": "refscale.c",
  "hash_id": "aa753c0da8cbf3e2188e89d81e865070fabac0073a76cc7a0362b34d2f5c6a59",
  "original_prompt": "Ingested from linux-6.6.14/kernel/rcu/refscale.c",
  "human_readable_source": "\n\n\n\n\n\n\n\n\n#define pr_fmt(fmt) fmt\n\n#include <linux/atomic.h>\n#include <linux/bitops.h>\n#include <linux/completion.h>\n#include <linux/cpu.h>\n#include <linux/delay.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/interrupt.h>\n#include <linux/kthread.h>\n#include <linux/kernel.h>\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <linux/moduleparam.h>\n#include <linux/notifier.h>\n#include <linux/percpu.h>\n#include <linux/rcupdate.h>\n#include <linux/rcupdate_trace.h>\n#include <linux/reboot.h>\n#include <linux/sched.h>\n#include <linux/spinlock.h>\n#include <linux/smp.h>\n#include <linux/stat.h>\n#include <linux/srcu.h>\n#include <linux/slab.h>\n#include <linux/torture.h>\n#include <linux/types.h>\n\n#include \"rcu.h\"\n\n#define SCALE_FLAG \"-ref-scale: \"\n\n#define SCALEOUT(s, x...) \\\n\tpr_alert(\"%s\" SCALE_FLAG s, scale_type, ## x)\n\n#define VERBOSE_SCALEOUT(s, x...) \\\n\tdo { \\\n\t\tif (verbose) \\\n\t\t\tpr_alert(\"%s\" SCALE_FLAG s \"\\n\", scale_type, ## x); \\\n\t} while (0)\n\nstatic atomic_t verbose_batch_ctr;\n\n#define VERBOSE_SCALEOUT_BATCH(s, x...)\t\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\t\t\\\n\tif (verbose &&\t\t\t\t\t\t\t\t\t\\\n\t    (verbose_batched <= 0 ||\t\t\t\t\t\t\t\\\n\t     !(atomic_inc_return(&verbose_batch_ctr) % verbose_batched))) {\t\t\\\n\t\tschedule_timeout_uninterruptible(1);\t\t\t\t\t\\\n\t\tpr_alert(\"%s\" SCALE_FLAG s \"\\n\", scale_type, ## x);\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\t\t\\\n} while (0)\n\n#define SCALEOUT_ERRSTRING(s, x...) pr_alert(\"%s\" SCALE_FLAG \"!!! \" s \"\\n\", scale_type, ## x)\n\nMODULE_LICENSE(\"GPL\");\nMODULE_AUTHOR(\"Joel Fernandes (Google) <joel@joelfernandes.org>\");\n\nstatic char *scale_type = \"rcu\";\nmodule_param(scale_type, charp, 0444);\nMODULE_PARM_DESC(scale_type, \"Type of test (rcu, srcu, refcnt, rwsem, rwlock.\");\n\ntorture_param(int, verbose, 0, \"Enable verbose debugging printk()s\");\ntorture_param(int, verbose_batched, 0, \"Batch verbose debugging printk()s\");\n\n\ntorture_param(int, holdoff, IS_BUILTIN(CONFIG_RCU_REF_SCALE_TEST) ? 10 : 0,\n\t      \"Holdoff time before test start (s)\");\n\ntorture_param(long, lookup_instances, 0, \"Number of typesafe_lookup structures.\");\n\ntorture_param(long, loops, 10000, \"Number of loops per experiment.\");\n\ntorture_param(int, nreaders, -1, \"Number of readers, -1 for 75% of CPUs.\");\n\ntorture_param(int, nruns, 30, \"Number of experiments to run.\");\n\ntorture_param(int, readdelay, 0, \"Read-side delay in nanoseconds.\");\n\n#ifdef MODULE\n# define REFSCALE_SHUTDOWN 0\n#else\n# define REFSCALE_SHUTDOWN 1\n#endif\n\ntorture_param(bool, shutdown, REFSCALE_SHUTDOWN,\n\t      \"Shutdown at end of scalability tests.\");\n\nstruct reader_task {\n\tstruct task_struct *task;\n\tint start_reader;\n\twait_queue_head_t wq;\n\tu64 last_duration_ns;\n};\n\nstatic struct task_struct *shutdown_task;\nstatic wait_queue_head_t shutdown_wq;\n\nstatic struct task_struct *main_task;\nstatic wait_queue_head_t main_wq;\nstatic int shutdown_start;\n\nstatic struct reader_task *reader_tasks;\n\n\nstatic atomic_t nreaders_exp;\n\n\nstatic atomic_t n_init;\nstatic atomic_t n_started;\nstatic atomic_t n_warmedup;\nstatic atomic_t n_cooleddown;\n\n\nstatic int exp_idx;\n\n\nstruct ref_scale_ops {\n\tbool (*init)(void);\n\tvoid (*cleanup)(void);\n\tvoid (*readsection)(const int nloops);\n\tvoid (*delaysection)(const int nloops, const int udl, const int ndl);\n\tconst char *name;\n};\n\nstatic struct ref_scale_ops *cur_ops;\n\nstatic void un_delay(const int udl, const int ndl)\n{\n\tif (udl)\n\t\tudelay(udl);\n\tif (ndl)\n\t\tndelay(ndl);\n}\n\nstatic void ref_rcu_read_section(const int nloops)\n{\n\tint i;\n\n\tfor (i = nloops; i >= 0; i--) {\n\t\trcu_read_lock();\n\t\trcu_read_unlock();\n\t}\n}\n\nstatic void ref_rcu_delay_section(const int nloops, const int udl, const int ndl)\n{\n\tint i;\n\n\tfor (i = nloops; i >= 0; i--) {\n\t\trcu_read_lock();\n\t\tun_delay(udl, ndl);\n\t\trcu_read_unlock();\n\t}\n}\n\nstatic bool rcu_sync_scale_init(void)\n{\n\treturn true;\n}\n\nstatic struct ref_scale_ops rcu_ops = {\n\t.init\t\t= rcu_sync_scale_init,\n\t.readsection\t= ref_rcu_read_section,\n\t.delaysection\t= ref_rcu_delay_section,\n\t.name\t\t= \"rcu\"\n};\n\n\nDEFINE_STATIC_SRCU(srcu_refctl_scale);\nstatic struct srcu_struct *srcu_ctlp = &srcu_refctl_scale;\n\nstatic void srcu_ref_scale_read_section(const int nloops)\n{\n\tint i;\n\tint idx;\n\n\tfor (i = nloops; i >= 0; i--) {\n\t\tidx = srcu_read_lock(srcu_ctlp);\n\t\tsrcu_read_unlock(srcu_ctlp, idx);\n\t}\n}\n\nstatic void srcu_ref_scale_delay_section(const int nloops, const int udl, const int ndl)\n{\n\tint i;\n\tint idx;\n\n\tfor (i = nloops; i >= 0; i--) {\n\t\tidx = srcu_read_lock(srcu_ctlp);\n\t\tun_delay(udl, ndl);\n\t\tsrcu_read_unlock(srcu_ctlp, idx);\n\t}\n}\n\nstatic struct ref_scale_ops srcu_ops = {\n\t.init\t\t= rcu_sync_scale_init,\n\t.readsection\t= srcu_ref_scale_read_section,\n\t.delaysection\t= srcu_ref_scale_delay_section,\n\t.name\t\t= \"srcu\"\n};\n\n#ifdef CONFIG_TASKS_RCU\n\n\n\nstatic void rcu_tasks_ref_scale_read_section(const int nloops)\n{\n\tint i;\n\n\tfor (i = nloops; i >= 0; i--)\n\t\tcontinue;\n}\n\nstatic void rcu_tasks_ref_scale_delay_section(const int nloops, const int udl, const int ndl)\n{\n\tint i;\n\n\tfor (i = nloops; i >= 0; i--)\n\t\tun_delay(udl, ndl);\n}\n\nstatic struct ref_scale_ops rcu_tasks_ops = {\n\t.init\t\t= rcu_sync_scale_init,\n\t.readsection\t= rcu_tasks_ref_scale_read_section,\n\t.delaysection\t= rcu_tasks_ref_scale_delay_section,\n\t.name\t\t= \"rcu-tasks\"\n};\n\n#define RCU_TASKS_OPS &rcu_tasks_ops,\n\n#else \n\n#define RCU_TASKS_OPS\n\n#endif \n\n#ifdef CONFIG_TASKS_TRACE_RCU\n\n\nstatic void rcu_trace_ref_scale_read_section(const int nloops)\n{\n\tint i;\n\n\tfor (i = nloops; i >= 0; i--) {\n\t\trcu_read_lock_trace();\n\t\trcu_read_unlock_trace();\n\t}\n}\n\nstatic void rcu_trace_ref_scale_delay_section(const int nloops, const int udl, const int ndl)\n{\n\tint i;\n\n\tfor (i = nloops; i >= 0; i--) {\n\t\trcu_read_lock_trace();\n\t\tun_delay(udl, ndl);\n\t\trcu_read_unlock_trace();\n\t}\n}\n\nstatic struct ref_scale_ops rcu_trace_ops = {\n\t.init\t\t= rcu_sync_scale_init,\n\t.readsection\t= rcu_trace_ref_scale_read_section,\n\t.delaysection\t= rcu_trace_ref_scale_delay_section,\n\t.name\t\t= \"rcu-trace\"\n};\n\n#define RCU_TRACE_OPS &rcu_trace_ops,\n\n#else \n\n#define RCU_TRACE_OPS\n\n#endif \n\n\nstatic atomic_t refcnt;\n\nstatic void ref_refcnt_section(const int nloops)\n{\n\tint i;\n\n\tfor (i = nloops; i >= 0; i--) {\n\t\tatomic_inc(&refcnt);\n\t\tatomic_dec(&refcnt);\n\t}\n}\n\nstatic void ref_refcnt_delay_section(const int nloops, const int udl, const int ndl)\n{\n\tint i;\n\n\tfor (i = nloops; i >= 0; i--) {\n\t\tatomic_inc(&refcnt);\n\t\tun_delay(udl, ndl);\n\t\tatomic_dec(&refcnt);\n\t}\n}\n\nstatic struct ref_scale_ops refcnt_ops = {\n\t.init\t\t= rcu_sync_scale_init,\n\t.readsection\t= ref_refcnt_section,\n\t.delaysection\t= ref_refcnt_delay_section,\n\t.name\t\t= \"refcnt\"\n};\n\n\nstatic rwlock_t test_rwlock;\n\nstatic bool ref_rwlock_init(void)\n{\n\trwlock_init(&test_rwlock);\n\treturn true;\n}\n\nstatic void ref_rwlock_section(const int nloops)\n{\n\tint i;\n\n\tfor (i = nloops; i >= 0; i--) {\n\t\tread_lock(&test_rwlock);\n\t\tread_unlock(&test_rwlock);\n\t}\n}\n\nstatic void ref_rwlock_delay_section(const int nloops, const int udl, const int ndl)\n{\n\tint i;\n\n\tfor (i = nloops; i >= 0; i--) {\n\t\tread_lock(&test_rwlock);\n\t\tun_delay(udl, ndl);\n\t\tread_unlock(&test_rwlock);\n\t}\n}\n\nstatic struct ref_scale_ops rwlock_ops = {\n\t.init\t\t= ref_rwlock_init,\n\t.readsection\t= ref_rwlock_section,\n\t.delaysection\t= ref_rwlock_delay_section,\n\t.name\t\t= \"rwlock\"\n};\n\n\nstatic struct rw_semaphore test_rwsem;\n\nstatic bool ref_rwsem_init(void)\n{\n\tinit_rwsem(&test_rwsem);\n\treturn true;\n}\n\nstatic void ref_rwsem_section(const int nloops)\n{\n\tint i;\n\n\tfor (i = nloops; i >= 0; i--) {\n\t\tdown_read(&test_rwsem);\n\t\tup_read(&test_rwsem);\n\t}\n}\n\nstatic void ref_rwsem_delay_section(const int nloops, const int udl, const int ndl)\n{\n\tint i;\n\n\tfor (i = nloops; i >= 0; i--) {\n\t\tdown_read(&test_rwsem);\n\t\tun_delay(udl, ndl);\n\t\tup_read(&test_rwsem);\n\t}\n}\n\nstatic struct ref_scale_ops rwsem_ops = {\n\t.init\t\t= ref_rwsem_init,\n\t.readsection\t= ref_rwsem_section,\n\t.delaysection\t= ref_rwsem_delay_section,\n\t.name\t\t= \"rwsem\"\n};\n\n\nstatic DEFINE_RAW_SPINLOCK(test_lock);\n\nstatic void ref_lock_section(const int nloops)\n{\n\tint i;\n\n\tpreempt_disable();\n\tfor (i = nloops; i >= 0; i--) {\n\t\traw_spin_lock(&test_lock);\n\t\traw_spin_unlock(&test_lock);\n\t}\n\tpreempt_enable();\n}\n\nstatic void ref_lock_delay_section(const int nloops, const int udl, const int ndl)\n{\n\tint i;\n\n\tpreempt_disable();\n\tfor (i = nloops; i >= 0; i--) {\n\t\traw_spin_lock(&test_lock);\n\t\tun_delay(udl, ndl);\n\t\traw_spin_unlock(&test_lock);\n\t}\n\tpreempt_enable();\n}\n\nstatic struct ref_scale_ops lock_ops = {\n\t.readsection\t= ref_lock_section,\n\t.delaysection\t= ref_lock_delay_section,\n\t.name\t\t= \"lock\"\n};\n\n\n\nstatic void ref_lock_irq_section(const int nloops)\n{\n\tunsigned long flags;\n\tint i;\n\n\tpreempt_disable();\n\tfor (i = nloops; i >= 0; i--) {\n\t\traw_spin_lock_irqsave(&test_lock, flags);\n\t\traw_spin_unlock_irqrestore(&test_lock, flags);\n\t}\n\tpreempt_enable();\n}\n\nstatic void ref_lock_irq_delay_section(const int nloops, const int udl, const int ndl)\n{\n\tunsigned long flags;\n\tint i;\n\n\tpreempt_disable();\n\tfor (i = nloops; i >= 0; i--) {\n\t\traw_spin_lock_irqsave(&test_lock, flags);\n\t\tun_delay(udl, ndl);\n\t\traw_spin_unlock_irqrestore(&test_lock, flags);\n\t}\n\tpreempt_enable();\n}\n\nstatic struct ref_scale_ops lock_irq_ops = {\n\t.readsection\t= ref_lock_irq_section,\n\t.delaysection\t= ref_lock_irq_delay_section,\n\t.name\t\t= \"lock-irq\"\n};\n\n\nstatic DEFINE_PER_CPU(unsigned long, test_acqrel);\n\nstatic void ref_acqrel_section(const int nloops)\n{\n\tunsigned long x;\n\tint i;\n\n\tpreempt_disable();\n\tfor (i = nloops; i >= 0; i--) {\n\t\tx = smp_load_acquire(this_cpu_ptr(&test_acqrel));\n\t\tsmp_store_release(this_cpu_ptr(&test_acqrel), x + 1);\n\t}\n\tpreempt_enable();\n}\n\nstatic void ref_acqrel_delay_section(const int nloops, const int udl, const int ndl)\n{\n\tunsigned long x;\n\tint i;\n\n\tpreempt_disable();\n\tfor (i = nloops; i >= 0; i--) {\n\t\tx = smp_load_acquire(this_cpu_ptr(&test_acqrel));\n\t\tun_delay(udl, ndl);\n\t\tsmp_store_release(this_cpu_ptr(&test_acqrel), x + 1);\n\t}\n\tpreempt_enable();\n}\n\nstatic struct ref_scale_ops acqrel_ops = {\n\t.readsection\t= ref_acqrel_section,\n\t.delaysection\t= ref_acqrel_delay_section,\n\t.name\t\t= \"acqrel\"\n};\n\nstatic volatile u64 stopopts;\n\nstatic void ref_clock_section(const int nloops)\n{\n\tu64 x = 0;\n\tint i;\n\n\tpreempt_disable();\n\tfor (i = nloops; i >= 0; i--)\n\t\tx += ktime_get_real_fast_ns();\n\tpreempt_enable();\n\tstopopts = x;\n}\n\nstatic void ref_clock_delay_section(const int nloops, const int udl, const int ndl)\n{\n\tu64 x = 0;\n\tint i;\n\n\tpreempt_disable();\n\tfor (i = nloops; i >= 0; i--) {\n\t\tx += ktime_get_real_fast_ns();\n\t\tun_delay(udl, ndl);\n\t}\n\tpreempt_enable();\n\tstopopts = x;\n}\n\nstatic struct ref_scale_ops clock_ops = {\n\t.readsection\t= ref_clock_section,\n\t.delaysection\t= ref_clock_delay_section,\n\t.name\t\t= \"clock\"\n};\n\nstatic void ref_jiffies_section(const int nloops)\n{\n\tu64 x = 0;\n\tint i;\n\n\tpreempt_disable();\n\tfor (i = nloops; i >= 0; i--)\n\t\tx += jiffies;\n\tpreempt_enable();\n\tstopopts = x;\n}\n\nstatic void ref_jiffies_delay_section(const int nloops, const int udl, const int ndl)\n{\n\tu64 x = 0;\n\tint i;\n\n\tpreempt_disable();\n\tfor (i = nloops; i >= 0; i--) {\n\t\tx += jiffies;\n\t\tun_delay(udl, ndl);\n\t}\n\tpreempt_enable();\n\tstopopts = x;\n}\n\nstatic struct ref_scale_ops jiffies_ops = {\n\t.readsection\t= ref_jiffies_section,\n\t.delaysection\t= ref_jiffies_delay_section,\n\t.name\t\t= \"jiffies\"\n};\n\n\n\n\n\n\n\nstruct refscale_typesafe {\n\tatomic_t rts_refctr;  \n\tspinlock_t rts_lock;\n\tseqlock_t rts_seqlock;\n\tunsigned int a;\n\tunsigned int b;\n};\n\nstatic struct kmem_cache *typesafe_kmem_cachep;\nstatic struct refscale_typesafe **rtsarray;\nstatic long rtsarray_size;\nstatic DEFINE_TORTURE_RANDOM_PERCPU(refscale_rand);\nstatic bool (*rts_acquire)(struct refscale_typesafe *rtsp, unsigned int *start);\nstatic bool (*rts_release)(struct refscale_typesafe *rtsp, unsigned int start);\n\n\nstatic bool typesafe_ref_acquire(struct refscale_typesafe *rtsp, unsigned int *start)\n{\n\treturn atomic_inc_not_zero(&rtsp->rts_refctr);\n}\n\n\nstatic bool typesafe_ref_release(struct refscale_typesafe *rtsp, unsigned int start)\n{\n\tif (!atomic_dec_return(&rtsp->rts_refctr)) {\n\t\tWRITE_ONCE(rtsp->a, rtsp->a + 1);\n\t\tkmem_cache_free(typesafe_kmem_cachep, rtsp);\n\t}\n\treturn true;\n}\n\n\nstatic bool typesafe_lock_acquire(struct refscale_typesafe *rtsp, unsigned int *start)\n{\n\tspin_lock(&rtsp->rts_lock);\n\treturn true;\n}\n\n\nstatic bool typesafe_lock_release(struct refscale_typesafe *rtsp, unsigned int start)\n{\n\tspin_unlock(&rtsp->rts_lock);\n\treturn true;\n}\n\n\nstatic bool typesafe_seqlock_acquire(struct refscale_typesafe *rtsp, unsigned int *start)\n{\n\t*start = read_seqbegin(&rtsp->rts_seqlock);\n\treturn true;\n}\n\n\n\nstatic bool typesafe_seqlock_release(struct refscale_typesafe *rtsp, unsigned int start)\n{\n\treturn !read_seqretry(&rtsp->rts_seqlock, start);\n}\n\n\n\n\nstatic void typesafe_delay_section(const int nloops, const int udl, const int ndl)\n{\n\tunsigned int a;\n\tunsigned int b;\n\tint i;\n\tlong idx;\n\tstruct refscale_typesafe *rtsp;\n\tunsigned int start;\n\n\tfor (i = nloops; i >= 0; i--) {\n\t\tpreempt_disable();\n\t\tidx = torture_random(this_cpu_ptr(&refscale_rand)) % rtsarray_size;\n\t\tpreempt_enable();\nretry:\n\t\trcu_read_lock();\n\t\trtsp = rcu_dereference(rtsarray[idx]);\n\t\ta = READ_ONCE(rtsp->a);\n\t\tif (!rts_acquire(rtsp, &start)) {\n\t\t\trcu_read_unlock();\n\t\t\tgoto retry;\n\t\t}\n\t\tif (a != READ_ONCE(rtsp->a)) {\n\t\t\t(void)rts_release(rtsp, start);\n\t\t\trcu_read_unlock();\n\t\t\tgoto retry;\n\t\t}\n\t\tun_delay(udl, ndl);\n\t\t\n\t\tif (!rts_release(rtsp, start)) {\n\t\t\trcu_read_unlock();\n\t\t\tgoto retry;\n\t\t}\n\t\tb = READ_ONCE(rtsp->a);\n\t\tWARN_ONCE(a != b, \"Re-read of ->a changed from %u to %u.\\n\", a, b);\n\t\tb = rtsp->b;\n\t\trcu_read_unlock();\n\t\tWARN_ON_ONCE(a * a != b);\n\t}\n}\n\n\n\n\n\nstatic void typesafe_read_section(const int nloops)\n{\n\ttypesafe_delay_section(nloops, 0, 0);\n}\n\n\nstatic struct refscale_typesafe *typesafe_alloc_one(void)\n{\n\tstruct refscale_typesafe *rtsp;\n\n\trtsp = kmem_cache_alloc(typesafe_kmem_cachep, GFP_KERNEL);\n\tif (!rtsp)\n\t\treturn NULL;\n\tatomic_set(&rtsp->rts_refctr, 1);\n\tWRITE_ONCE(rtsp->a, rtsp->a + 1);\n\tWRITE_ONCE(rtsp->b, rtsp->a * rtsp->a);\n\treturn rtsp;\n}\n\n\n\nstatic void refscale_typesafe_ctor(void *rtsp_in)\n{\n\tstruct refscale_typesafe *rtsp = rtsp_in;\n\n\tspin_lock_init(&rtsp->rts_lock);\n\tseqlock_init(&rtsp->rts_seqlock);\n\tpreempt_disable();\n\trtsp->a = torture_random(this_cpu_ptr(&refscale_rand));\n\tpreempt_enable();\n}\n\nstatic struct ref_scale_ops typesafe_ref_ops;\nstatic struct ref_scale_ops typesafe_lock_ops;\nstatic struct ref_scale_ops typesafe_seqlock_ops;\n\n\nstatic bool typesafe_init(void)\n{\n\tlong idx;\n\tlong si = lookup_instances;\n\n\ttypesafe_kmem_cachep = kmem_cache_create(\"refscale_typesafe\",\n\t\t\t\t\t\t sizeof(struct refscale_typesafe), sizeof(void *),\n\t\t\t\t\t\t SLAB_TYPESAFE_BY_RCU, refscale_typesafe_ctor);\n\tif (!typesafe_kmem_cachep)\n\t\treturn false;\n\tif (si < 0)\n\t\tsi = -si * nr_cpu_ids;\n\telse if (si == 0)\n\t\tsi = nr_cpu_ids;\n\trtsarray_size = si;\n\trtsarray = kcalloc(si, sizeof(*rtsarray), GFP_KERNEL);\n\tif (!rtsarray)\n\t\treturn false;\n\tfor (idx = 0; idx < rtsarray_size; idx++) {\n\t\trtsarray[idx] = typesafe_alloc_one();\n\t\tif (!rtsarray[idx])\n\t\t\treturn false;\n\t}\n\tif (cur_ops == &typesafe_ref_ops) {\n\t\trts_acquire = typesafe_ref_acquire;\n\t\trts_release = typesafe_ref_release;\n\t} else if (cur_ops == &typesafe_lock_ops) {\n\t\trts_acquire = typesafe_lock_acquire;\n\t\trts_release = typesafe_lock_release;\n\t} else if (cur_ops == &typesafe_seqlock_ops) {\n\t\trts_acquire = typesafe_seqlock_acquire;\n\t\trts_release = typesafe_seqlock_release;\n\t} else {\n\t\tWARN_ON_ONCE(1);\n\t\treturn false;\n\t}\n\treturn true;\n}\n\n\nstatic void typesafe_cleanup(void)\n{\n\tlong idx;\n\n\tif (rtsarray) {\n\t\tfor (idx = 0; idx < rtsarray_size; idx++)\n\t\t\tkmem_cache_free(typesafe_kmem_cachep, rtsarray[idx]);\n\t\tkfree(rtsarray);\n\t\trtsarray = NULL;\n\t\trtsarray_size = 0;\n\t}\n\tkmem_cache_destroy(typesafe_kmem_cachep);\n\ttypesafe_kmem_cachep = NULL;\n\trts_acquire = NULL;\n\trts_release = NULL;\n}\n\n\nstatic struct ref_scale_ops typesafe_ref_ops = {\n\t.init\t\t= typesafe_init,\n\t.cleanup\t= typesafe_cleanup,\n\t.readsection\t= typesafe_read_section,\n\t.delaysection\t= typesafe_delay_section,\n\t.name\t\t= \"typesafe_ref\"\n};\n\nstatic struct ref_scale_ops typesafe_lock_ops = {\n\t.init\t\t= typesafe_init,\n\t.cleanup\t= typesafe_cleanup,\n\t.readsection\t= typesafe_read_section,\n\t.delaysection\t= typesafe_delay_section,\n\t.name\t\t= \"typesafe_lock\"\n};\n\nstatic struct ref_scale_ops typesafe_seqlock_ops = {\n\t.init\t\t= typesafe_init,\n\t.cleanup\t= typesafe_cleanup,\n\t.readsection\t= typesafe_read_section,\n\t.delaysection\t= typesafe_delay_section,\n\t.name\t\t= \"typesafe_seqlock\"\n};\n\nstatic void rcu_scale_one_reader(void)\n{\n\tif (readdelay <= 0)\n\t\tcur_ops->readsection(loops);\n\telse\n\t\tcur_ops->delaysection(loops, readdelay / 1000, readdelay % 1000);\n}\n\n\n\nstatic int\nref_scale_reader(void *arg)\n{\n\tunsigned long flags;\n\tlong me = (long)arg;\n\tstruct reader_task *rt = &(reader_tasks[me]);\n\tu64 start;\n\ts64 duration;\n\n\tVERBOSE_SCALEOUT_BATCH(\"ref_scale_reader %ld: task started\", me);\n\tWARN_ON_ONCE(set_cpus_allowed_ptr(current, cpumask_of(me % nr_cpu_ids)));\n\tset_user_nice(current, MAX_NICE);\n\tatomic_inc(&n_init);\n\tif (holdoff)\n\t\tschedule_timeout_interruptible(holdoff * HZ);\nrepeat:\n\tVERBOSE_SCALEOUT_BATCH(\"ref_scale_reader %ld: waiting to start next experiment on cpu %d\", me, raw_smp_processor_id());\n\n\t\n\twait_event(rt->wq, (atomic_read(&nreaders_exp) && smp_load_acquire(&rt->start_reader)) ||\n\t\t\t   torture_must_stop());\n\n\tif (torture_must_stop())\n\t\tgoto end;\n\n\t\n\tWARN_ON_ONCE(raw_smp_processor_id() != me);\n\n\tWRITE_ONCE(rt->start_reader, 0);\n\tif (!atomic_dec_return(&n_started))\n\t\twhile (atomic_read_acquire(&n_started))\n\t\t\tcpu_relax();\n\n\tVERBOSE_SCALEOUT_BATCH(\"ref_scale_reader %ld: experiment %d started\", me, exp_idx);\n\n\n\t\n\t\n\trcu_scale_one_reader();\n\tif (!atomic_dec_return(&n_warmedup))\n\t\twhile (atomic_read_acquire(&n_warmedup))\n\t\t\trcu_scale_one_reader();\n\t\n\t\n\tlocal_irq_save(flags);\n\tstart = ktime_get_mono_fast_ns();\n\n\trcu_scale_one_reader();\n\n\tduration = ktime_get_mono_fast_ns() - start;\n\tlocal_irq_restore(flags);\n\n\trt->last_duration_ns = WARN_ON_ONCE(duration < 0) ? 0 : duration;\n\t\n\t\n\tif (!atomic_dec_return(&n_cooleddown))\n\t\twhile (atomic_read_acquire(&n_cooleddown))\n\t\t\trcu_scale_one_reader();\n\n\tif (atomic_dec_and_test(&nreaders_exp))\n\t\twake_up(&main_wq);\n\n\tVERBOSE_SCALEOUT_BATCH(\"ref_scale_reader %ld: experiment %d ended, (readers remaining=%d)\",\n\t\t\t\tme, exp_idx, atomic_read(&nreaders_exp));\n\n\tif (!torture_must_stop())\n\t\tgoto repeat;\nend:\n\ttorture_kthread_stopping(\"ref_scale_reader\");\n\treturn 0;\n}\n\nstatic void reset_readers(void)\n{\n\tint i;\n\tstruct reader_task *rt;\n\n\tfor (i = 0; i < nreaders; i++) {\n\t\trt = &(reader_tasks[i]);\n\n\t\trt->last_duration_ns = 0;\n\t}\n}\n\n\nstatic u64 process_durations(int n)\n{\n\tint i;\n\tstruct reader_task *rt;\n\tchar buf1[64];\n\tchar *buf;\n\tu64 sum = 0;\n\n\tbuf = kmalloc(800 + 64, GFP_KERNEL);\n\tif (!buf)\n\t\treturn 0;\n\tbuf[0] = 0;\n\tsprintf(buf, \"Experiment #%d (Format: <THREAD-NUM>:<Total loop time in ns>)\",\n\t\texp_idx);\n\n\tfor (i = 0; i < n && !torture_must_stop(); i++) {\n\t\trt = &(reader_tasks[i]);\n\t\tsprintf(buf1, \"%d: %llu\\t\", i, rt->last_duration_ns);\n\n\t\tif (i % 5 == 0)\n\t\t\tstrcat(buf, \"\\n\");\n\t\tif (strlen(buf) >= 800) {\n\t\t\tpr_alert(\"%s\", buf);\n\t\t\tbuf[0] = 0;\n\t\t}\n\t\tstrcat(buf, buf1);\n\n\t\tsum += rt->last_duration_ns;\n\t}\n\tpr_alert(\"%s\\n\", buf);\n\n\tkfree(buf);\n\treturn sum;\n}\n\n\n\n\n\n\n\nstatic int main_func(void *arg)\n{\n\tint exp, r;\n\tchar buf1[64];\n\tchar *buf;\n\tu64 *result_avg;\n\n\tset_cpus_allowed_ptr(current, cpumask_of(nreaders % nr_cpu_ids));\n\tset_user_nice(current, MAX_NICE);\n\n\tVERBOSE_SCALEOUT(\"main_func task started\");\n\tresult_avg = kzalloc(nruns * sizeof(*result_avg), GFP_KERNEL);\n\tbuf = kzalloc(800 + 64, GFP_KERNEL);\n\tif (!result_avg || !buf) {\n\t\tSCALEOUT_ERRSTRING(\"out of memory\");\n\t\tgoto oom_exit;\n\t}\n\tif (holdoff)\n\t\tschedule_timeout_interruptible(holdoff * HZ);\n\n\t\n\tatomic_inc(&n_init);\n\twhile (atomic_read(&n_init) < nreaders + 1)\n\t\tschedule_timeout_uninterruptible(1);\n\n\t\n\tfor (exp = 0; exp < nruns && !torture_must_stop(); exp++) {\n\t\tif (torture_must_stop())\n\t\t\tgoto end;\n\n\t\treset_readers();\n\t\tatomic_set(&nreaders_exp, nreaders);\n\t\tatomic_set(&n_started, nreaders);\n\t\tatomic_set(&n_warmedup, nreaders);\n\t\tatomic_set(&n_cooleddown, nreaders);\n\n\t\texp_idx = exp;\n\n\t\tfor (r = 0; r < nreaders; r++) {\n\t\t\tsmp_store_release(&reader_tasks[r].start_reader, 1);\n\t\t\twake_up(&reader_tasks[r].wq);\n\t\t}\n\n\t\tVERBOSE_SCALEOUT(\"main_func: experiment started, waiting for %d readers\",\n\t\t\t\tnreaders);\n\n\t\twait_event(main_wq,\n\t\t\t   !atomic_read(&nreaders_exp) || torture_must_stop());\n\n\t\tVERBOSE_SCALEOUT(\"main_func: experiment ended\");\n\n\t\tif (torture_must_stop())\n\t\t\tgoto end;\n\n\t\tresult_avg[exp] = div_u64(1000 * process_durations(nreaders), nreaders * loops);\n\t}\n\n\t\n\tSCALEOUT(\"END OF TEST. Calculating average duration per loop (nanoseconds)...\\n\");\n\n\tpr_alert(\"Runs\\tTime(ns)\\n\");\n\tfor (exp = 0; exp < nruns; exp++) {\n\t\tu64 avg;\n\t\tu32 rem;\n\n\t\tavg = div_u64_rem(result_avg[exp], 1000, &rem);\n\t\tsprintf(buf1, \"%d\\t%llu.%03u\\n\", exp + 1, avg, rem);\n\t\tstrcat(buf, buf1);\n\t\tif (strlen(buf) >= 800) {\n\t\t\tpr_alert(\"%s\", buf);\n\t\t\tbuf[0] = 0;\n\t\t}\n\t}\n\n\tpr_alert(\"%s\", buf);\n\noom_exit:\n\t\n\tif (shutdown) {\n\t\tshutdown_start = 1;\n\t\twake_up(&shutdown_wq);\n\t}\n\n\t\n\twhile (!torture_must_stop())\n\t\tschedule_timeout_uninterruptible(1);\n\nend:\n\ttorture_kthread_stopping(\"main_func\");\n\tkfree(result_avg);\n\tkfree(buf);\n\treturn 0;\n}\n\nstatic void\nref_scale_print_module_parms(struct ref_scale_ops *cur_ops, const char *tag)\n{\n\tpr_alert(\"%s\" SCALE_FLAG\n\t\t \"--- %s:  verbose=%d shutdown=%d holdoff=%d loops=%ld nreaders=%d nruns=%d readdelay=%d\\n\", scale_type, tag,\n\t\t verbose, shutdown, holdoff, loops, nreaders, nruns, readdelay);\n}\n\nstatic void\nref_scale_cleanup(void)\n{\n\tint i;\n\n\tif (torture_cleanup_begin())\n\t\treturn;\n\n\tif (!cur_ops) {\n\t\ttorture_cleanup_end();\n\t\treturn;\n\t}\n\n\tif (reader_tasks) {\n\t\tfor (i = 0; i < nreaders; i++)\n\t\t\ttorture_stop_kthread(\"ref_scale_reader\",\n\t\t\t\t\t     reader_tasks[i].task);\n\t}\n\tkfree(reader_tasks);\n\n\ttorture_stop_kthread(\"main_task\", main_task);\n\tkfree(main_task);\n\n\t\n\tif (cur_ops->cleanup != NULL)\n\t\tcur_ops->cleanup();\n\n\ttorture_cleanup_end();\n}\n\n\nstatic int\nref_scale_shutdown(void *arg)\n{\n\twait_event_idle(shutdown_wq, shutdown_start);\n\n\tsmp_mb(); \n\tref_scale_cleanup();\n\tkernel_power_off();\n\n\treturn -EINVAL;\n}\n\nstatic int __init\nref_scale_init(void)\n{\n\tlong i;\n\tint firsterr = 0;\n\tstatic struct ref_scale_ops *scale_ops[] = {\n\t\t&rcu_ops, &srcu_ops, RCU_TRACE_OPS RCU_TASKS_OPS &refcnt_ops, &rwlock_ops,\n\t\t&rwsem_ops, &lock_ops, &lock_irq_ops, &acqrel_ops, &clock_ops, &jiffies_ops,\n\t\t&typesafe_ref_ops, &typesafe_lock_ops, &typesafe_seqlock_ops,\n\t};\n\n\tif (!torture_init_begin(scale_type, verbose))\n\t\treturn -EBUSY;\n\n\tfor (i = 0; i < ARRAY_SIZE(scale_ops); i++) {\n\t\tcur_ops = scale_ops[i];\n\t\tif (strcmp(scale_type, cur_ops->name) == 0)\n\t\t\tbreak;\n\t}\n\tif (i == ARRAY_SIZE(scale_ops)) {\n\t\tpr_alert(\"rcu-scale: invalid scale type: \\\"%s\\\"\\n\", scale_type);\n\t\tpr_alert(\"rcu-scale types:\");\n\t\tfor (i = 0; i < ARRAY_SIZE(scale_ops); i++)\n\t\t\tpr_cont(\" %s\", scale_ops[i]->name);\n\t\tpr_cont(\"\\n\");\n\t\tfirsterr = -EINVAL;\n\t\tcur_ops = NULL;\n\t\tgoto unwind;\n\t}\n\tif (cur_ops->init)\n\t\tif (!cur_ops->init()) {\n\t\t\tfirsterr = -EUCLEAN;\n\t\t\tgoto unwind;\n\t\t}\n\n\tref_scale_print_module_parms(cur_ops, \"Start of test\");\n\n\t\n\tif (shutdown) {\n\t\tinit_waitqueue_head(&shutdown_wq);\n\t\tfirsterr = torture_create_kthread(ref_scale_shutdown, NULL,\n\t\t\t\t\t\t  shutdown_task);\n\t\tif (torture_init_error(firsterr))\n\t\t\tgoto unwind;\n\t\tschedule_timeout_uninterruptible(1);\n\t}\n\n\t\n\tif (nreaders < 0)\n\t\tnreaders = (num_online_cpus() >> 1) + (num_online_cpus() >> 2);\n\tif (WARN_ONCE(loops <= 0, \"%s: loops = %ld, adjusted to 1\\n\", __func__, loops))\n\t\tloops = 1;\n\tif (WARN_ONCE(nreaders <= 0, \"%s: nreaders = %d, adjusted to 1\\n\", __func__, nreaders))\n\t\tnreaders = 1;\n\tif (WARN_ONCE(nruns <= 0, \"%s: nruns = %d, adjusted to 1\\n\", __func__, nruns))\n\t\tnruns = 1;\n\treader_tasks = kcalloc(nreaders, sizeof(reader_tasks[0]),\n\t\t\t       GFP_KERNEL);\n\tif (!reader_tasks) {\n\t\tSCALEOUT_ERRSTRING(\"out of memory\");\n\t\tfirsterr = -ENOMEM;\n\t\tgoto unwind;\n\t}\n\n\tVERBOSE_SCALEOUT(\"Starting %d reader threads\", nreaders);\n\n\tfor (i = 0; i < nreaders; i++) {\n\t\tinit_waitqueue_head(&reader_tasks[i].wq);\n\t\tfirsterr = torture_create_kthread(ref_scale_reader, (void *)i,\n\t\t\t\t\t\t  reader_tasks[i].task);\n\t\tif (torture_init_error(firsterr))\n\t\t\tgoto unwind;\n\t}\n\n\t\n\tinit_waitqueue_head(&main_wq);\n\tfirsterr = torture_create_kthread(main_func, NULL, main_task);\n\tif (torture_init_error(firsterr))\n\t\tgoto unwind;\n\n\ttorture_init_end();\n\treturn 0;\n\nunwind:\n\ttorture_init_end();\n\tref_scale_cleanup();\n\tif (shutdown) {\n\t\tWARN_ON(!IS_MODULE(CONFIG_RCU_REF_SCALE_TEST));\n\t\tkernel_power_off();\n\t}\n\treturn firsterr;\n}\n\nmodule_init(ref_scale_init);\nmodule_exit(ref_scale_cleanup);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}