{
  "module_name": "tree.c",
  "hash_id": "c15da7a3eedee60bc123c6c059faecd7ea444954acbddd2b08dd427e3f922f8d",
  "original_prompt": "Ingested from linux-6.6.14/kernel/rcu/tree.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) \"rcu: \" fmt\n\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/init.h>\n#include <linux/spinlock.h>\n#include <linux/smp.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/interrupt.h>\n#include <linux/sched.h>\n#include <linux/sched/debug.h>\n#include <linux/nmi.h>\n#include <linux/atomic.h>\n#include <linux/bitops.h>\n#include <linux/export.h>\n#include <linux/completion.h>\n#include <linux/kmemleak.h>\n#include <linux/moduleparam.h>\n#include <linux/panic.h>\n#include <linux/panic_notifier.h>\n#include <linux/percpu.h>\n#include <linux/notifier.h>\n#include <linux/cpu.h>\n#include <linux/mutex.h>\n#include <linux/time.h>\n#include <linux/kernel_stat.h>\n#include <linux/wait.h>\n#include <linux/kthread.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/prefetch.h>\n#include <linux/delay.h>\n#include <linux/random.h>\n#include <linux/trace_events.h>\n#include <linux/suspend.h>\n#include <linux/ftrace.h>\n#include <linux/tick.h>\n#include <linux/sysrq.h>\n#include <linux/kprobes.h>\n#include <linux/gfp.h>\n#include <linux/oom.h>\n#include <linux/smpboot.h>\n#include <linux/jiffies.h>\n#include <linux/slab.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/clock.h>\n#include <linux/vmalloc.h>\n#include <linux/mm.h>\n#include <linux/kasan.h>\n#include <linux/context_tracking.h>\n#include \"../time/tick-internal.h\"\n\n#include \"tree.h\"\n#include \"rcu.h\"\n\n#ifdef MODULE_PARAM_PREFIX\n#undef MODULE_PARAM_PREFIX\n#endif\n#define MODULE_PARAM_PREFIX \"rcutree.\"\n\n \n\nstatic DEFINE_PER_CPU_SHARED_ALIGNED(struct rcu_data, rcu_data) = {\n\t.gpwrap = true,\n#ifdef CONFIG_RCU_NOCB_CPU\n\t.cblist.flags = SEGCBLIST_RCU_CORE,\n#endif\n};\nstatic struct rcu_state rcu_state = {\n\t.level = { &rcu_state.node[0] },\n\t.gp_state = RCU_GP_IDLE,\n\t.gp_seq = (0UL - 300UL) << RCU_SEQ_CTR_SHIFT,\n\t.barrier_mutex = __MUTEX_INITIALIZER(rcu_state.barrier_mutex),\n\t.barrier_lock = __RAW_SPIN_LOCK_UNLOCKED(rcu_state.barrier_lock),\n\t.name = RCU_NAME,\n\t.abbr = RCU_ABBR,\n\t.exp_mutex = __MUTEX_INITIALIZER(rcu_state.exp_mutex),\n\t.exp_wake_mutex = __MUTEX_INITIALIZER(rcu_state.exp_wake_mutex),\n\t.ofl_lock = __ARCH_SPIN_LOCK_UNLOCKED,\n};\n\n \nstatic bool dump_tree;\nmodule_param(dump_tree, bool, 0444);\n \nstatic bool use_softirq = !IS_ENABLED(CONFIG_PREEMPT_RT);\n#ifndef CONFIG_PREEMPT_RT\nmodule_param(use_softirq, bool, 0444);\n#endif\n \nstatic bool rcu_fanout_exact;\nmodule_param(rcu_fanout_exact, bool, 0444);\n \nstatic int rcu_fanout_leaf = RCU_FANOUT_LEAF;\nmodule_param(rcu_fanout_leaf, int, 0444);\nint rcu_num_lvls __read_mostly = RCU_NUM_LVLS;\n \nint num_rcu_lvl[] = NUM_RCU_LVL_INIT;\nint rcu_num_nodes __read_mostly = NUM_RCU_NODES;  \n\n \nint rcu_scheduler_active __read_mostly;\nEXPORT_SYMBOL_GPL(rcu_scheduler_active);\n\n \nstatic int rcu_scheduler_fully_active __read_mostly;\n\nstatic void rcu_report_qs_rnp(unsigned long mask, struct rcu_node *rnp,\n\t\t\t      unsigned long gps, unsigned long flags);\nstatic void rcu_boost_kthread_setaffinity(struct rcu_node *rnp, int outgoingcpu);\nstatic void invoke_rcu_core(void);\nstatic void rcu_report_exp_rdp(struct rcu_data *rdp);\nstatic void sync_sched_exp_online_cleanup(int cpu);\nstatic void check_cb_ovld_locked(struct rcu_data *rdp, struct rcu_node *rnp);\nstatic bool rcu_rdp_is_offloaded(struct rcu_data *rdp);\nstatic bool rcu_rdp_cpu_online(struct rcu_data *rdp);\nstatic bool rcu_init_invoked(void);\nstatic void rcu_cleanup_dead_rnp(struct rcu_node *rnp_leaf);\nstatic void rcu_init_new_rnp(struct rcu_node *rnp_leaf);\n\n \nstatic int kthread_prio = IS_ENABLED(CONFIG_RCU_BOOST) ? 1 : 0;\nmodule_param(kthread_prio, int, 0444);\n\n \n\nstatic int gp_preinit_delay;\nmodule_param(gp_preinit_delay, int, 0444);\nstatic int gp_init_delay;\nmodule_param(gp_init_delay, int, 0444);\nstatic int gp_cleanup_delay;\nmodule_param(gp_cleanup_delay, int, 0444);\n\n\nstatic int rcu_unlock_delay;\n#ifdef CONFIG_RCU_STRICT_GRACE_PERIOD\nmodule_param(rcu_unlock_delay, int, 0444);\n#endif\n\n \nstatic int rcu_min_cached_objs = 5;\nmodule_param(rcu_min_cached_objs, int, 0444);\n\n\n\n\n\n\n\n\n\nstatic int rcu_delay_page_cache_fill_msec = 5000;\nmodule_param(rcu_delay_page_cache_fill_msec, int, 0444);\n\n \nint rcu_get_gp_kthreads_prio(void)\n{\n\treturn kthread_prio;\n}\nEXPORT_SYMBOL_GPL(rcu_get_gp_kthreads_prio);\n\n \n#define PER_RCU_NODE_PERIOD 3\t \n\n \nstatic int rcu_gp_in_progress(void)\n{\n\treturn rcu_seq_state(rcu_seq_current(&rcu_state.gp_seq));\n}\n\n \nstatic long rcu_get_n_cbs_cpu(int cpu)\n{\n\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);\n\n\tif (rcu_segcblist_is_enabled(&rdp->cblist))\n\t\treturn rcu_segcblist_n_cbs(&rdp->cblist);\n\treturn 0;\n}\n\nvoid rcu_softirq_qs(void)\n{\n\trcu_qs();\n\trcu_preempt_deferred_qs(current);\n\trcu_tasks_qs(current, false);\n}\n\n \nstatic void rcu_dynticks_eqs_online(void)\n{\n\tif (ct_dynticks() & RCU_DYNTICKS_IDX)\n\t\treturn;\n\tct_state_inc(RCU_DYNTICKS_IDX);\n}\n\n \nstatic int rcu_dynticks_snap(int cpu)\n{\n\tsmp_mb();  \n\treturn ct_dynticks_cpu_acquire(cpu);\n}\n\n \nstatic bool rcu_dynticks_in_eqs(int snap)\n{\n\treturn !(snap & RCU_DYNTICKS_IDX);\n}\n\n \nstatic bool rcu_dynticks_in_eqs_since(struct rcu_data *rdp, int snap)\n{\n\treturn snap != rcu_dynticks_snap(rdp->cpu);\n}\n\n \nbool rcu_dynticks_zero_in_eqs(int cpu, int *vp)\n{\n\tint snap;\n\n\t\n\tsnap = ct_dynticks_cpu(cpu) & ~RCU_DYNTICKS_IDX;\n\tsmp_rmb(); \n\tif (READ_ONCE(*vp))\n\t\treturn false;  \n\tsmp_rmb(); \n\n\t\n\treturn snap == ct_dynticks_cpu(cpu);\n}\n\n \nnotrace void rcu_momentary_dyntick_idle(void)\n{\n\tint seq;\n\n\traw_cpu_write(rcu_data.rcu_need_heavy_qs, false);\n\tseq = ct_state_inc(2 * RCU_DYNTICKS_IDX);\n\t \n\tWARN_ON_ONCE(!(seq & RCU_DYNTICKS_IDX));\n\trcu_preempt_deferred_qs(current);\n}\nEXPORT_SYMBOL_GPL(rcu_momentary_dyntick_idle);\n\n \nstatic int rcu_is_cpu_rrupt_from_idle(void)\n{\n\tlong nesting;\n\n\t \n\tlockdep_assert_irqs_disabled();\n\n\t \n\tRCU_LOCKDEP_WARN(ct_dynticks_nesting() < 0,\n\t\t\t \"RCU dynticks_nesting counter underflow!\");\n\tRCU_LOCKDEP_WARN(ct_dynticks_nmi_nesting() <= 0,\n\t\t\t \"RCU dynticks_nmi_nesting counter underflow/zero!\");\n\n\t \n\tnesting = ct_dynticks_nmi_nesting();\n\tif (nesting > 1)\n\t\treturn false;\n\n\t \n\tWARN_ON_ONCE(!nesting && !is_idle_task(current));\n\n\t \n\treturn ct_dynticks_nesting() == 0;\n}\n\n#define DEFAULT_RCU_BLIMIT (IS_ENABLED(CONFIG_RCU_STRICT_GRACE_PERIOD) ? 1000 : 10)\n\t\t\t\t \n#define DEFAULT_MAX_RCU_BLIMIT 10000  \nstatic long blimit = DEFAULT_RCU_BLIMIT;\n#define DEFAULT_RCU_QHIMARK 10000  \nstatic long qhimark = DEFAULT_RCU_QHIMARK;\n#define DEFAULT_RCU_QLOMARK 100    \nstatic long qlowmark = DEFAULT_RCU_QLOMARK;\n#define DEFAULT_RCU_QOVLD_MULT 2\n#define DEFAULT_RCU_QOVLD (DEFAULT_RCU_QOVLD_MULT * DEFAULT_RCU_QHIMARK)\nstatic long qovld = DEFAULT_RCU_QOVLD;  \nstatic long qovld_calc = -1;\t   \n\nmodule_param(blimit, long, 0444);\nmodule_param(qhimark, long, 0444);\nmodule_param(qlowmark, long, 0444);\nmodule_param(qovld, long, 0444);\n\nstatic ulong jiffies_till_first_fqs = IS_ENABLED(CONFIG_RCU_STRICT_GRACE_PERIOD) ? 0 : ULONG_MAX;\nstatic ulong jiffies_till_next_fqs = ULONG_MAX;\nstatic bool rcu_kick_kthreads;\nstatic int rcu_divisor = 7;\nmodule_param(rcu_divisor, int, 0644);\n\n \nstatic long rcu_resched_ns = 3 * NSEC_PER_MSEC;\nmodule_param(rcu_resched_ns, long, 0644);\n\n \nstatic ulong jiffies_till_sched_qs = ULONG_MAX;\nmodule_param(jiffies_till_sched_qs, ulong, 0444);\nstatic ulong jiffies_to_sched_qs;  \nmodule_param(jiffies_to_sched_qs, ulong, 0444);  \n\n \nstatic void adjust_jiffies_till_sched_qs(void)\n{\n\tunsigned long j;\n\n\t \n\tif (jiffies_till_sched_qs != ULONG_MAX) {\n\t\tWRITE_ONCE(jiffies_to_sched_qs, jiffies_till_sched_qs);\n\t\treturn;\n\t}\n\t \n\tj = READ_ONCE(jiffies_till_first_fqs) +\n\t\t      2 * READ_ONCE(jiffies_till_next_fqs);\n\tif (j < HZ / 10 + nr_cpu_ids / RCU_JIFFIES_FQS_DIV)\n\t\tj = HZ / 10 + nr_cpu_ids / RCU_JIFFIES_FQS_DIV;\n\tpr_info(\"RCU calculated value of scheduler-enlistment delay is %ld jiffies.\\n\", j);\n\tWRITE_ONCE(jiffies_to_sched_qs, j);\n}\n\nstatic int param_set_first_fqs_jiffies(const char *val, const struct kernel_param *kp)\n{\n\tulong j;\n\tint ret = kstrtoul(val, 0, &j);\n\n\tif (!ret) {\n\t\tWRITE_ONCE(*(ulong *)kp->arg, (j > HZ) ? HZ : j);\n\t\tadjust_jiffies_till_sched_qs();\n\t}\n\treturn ret;\n}\n\nstatic int param_set_next_fqs_jiffies(const char *val, const struct kernel_param *kp)\n{\n\tulong j;\n\tint ret = kstrtoul(val, 0, &j);\n\n\tif (!ret) {\n\t\tWRITE_ONCE(*(ulong *)kp->arg, (j > HZ) ? HZ : (j ?: 1));\n\t\tadjust_jiffies_till_sched_qs();\n\t}\n\treturn ret;\n}\n\nstatic const struct kernel_param_ops first_fqs_jiffies_ops = {\n\t.set = param_set_first_fqs_jiffies,\n\t.get = param_get_ulong,\n};\n\nstatic const struct kernel_param_ops next_fqs_jiffies_ops = {\n\t.set = param_set_next_fqs_jiffies,\n\t.get = param_get_ulong,\n};\n\nmodule_param_cb(jiffies_till_first_fqs, &first_fqs_jiffies_ops, &jiffies_till_first_fqs, 0644);\nmodule_param_cb(jiffies_till_next_fqs, &next_fqs_jiffies_ops, &jiffies_till_next_fqs, 0644);\nmodule_param(rcu_kick_kthreads, bool, 0644);\n\nstatic void force_qs_rnp(int (*f)(struct rcu_data *rdp));\nstatic int rcu_pending(int user);\n\n \nunsigned long rcu_get_gp_seq(void)\n{\n\treturn READ_ONCE(rcu_state.gp_seq);\n}\nEXPORT_SYMBOL_GPL(rcu_get_gp_seq);\n\n \nunsigned long rcu_exp_batches_completed(void)\n{\n\treturn rcu_state.expedited_sequence;\n}\nEXPORT_SYMBOL_GPL(rcu_exp_batches_completed);\n\n \nstatic struct rcu_node *rcu_get_root(void)\n{\n\treturn &rcu_state.node[0];\n}\n\n \nvoid rcutorture_get_gp_data(enum rcutorture_type test_type, int *flags,\n\t\t\t    unsigned long *gp_seq)\n{\n\tswitch (test_type) {\n\tcase RCU_FLAVOR:\n\t\t*flags = READ_ONCE(rcu_state.gp_flags);\n\t\t*gp_seq = rcu_seq_current(&rcu_state.gp_seq);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\nEXPORT_SYMBOL_GPL(rcutorture_get_gp_data);\n\n#if defined(CONFIG_NO_HZ_FULL) && (!defined(CONFIG_GENERIC_ENTRY) || !defined(CONFIG_KVM_XFER_TO_GUEST_WORK))\n \nstatic void late_wakeup_func(struct irq_work *work)\n{\n}\n\nstatic DEFINE_PER_CPU(struct irq_work, late_wakeup_work) =\n\tIRQ_WORK_INIT(late_wakeup_func);\n\n \nnoinstr void rcu_irq_work_resched(void)\n{\n\tstruct rcu_data *rdp = this_cpu_ptr(&rcu_data);\n\n\tif (IS_ENABLED(CONFIG_GENERIC_ENTRY) && !(current->flags & PF_VCPU))\n\t\treturn;\n\n\tif (IS_ENABLED(CONFIG_KVM_XFER_TO_GUEST_WORK) && (current->flags & PF_VCPU))\n\t\treturn;\n\n\tinstrumentation_begin();\n\tif (do_nocb_deferred_wakeup(rdp) && need_resched()) {\n\t\tirq_work_queue(this_cpu_ptr(&late_wakeup_work));\n\t}\n\tinstrumentation_end();\n}\n#endif  \n\n#ifdef CONFIG_PROVE_RCU\n \nvoid rcu_irq_exit_check_preempt(void)\n{\n\tlockdep_assert_irqs_disabled();\n\n\tRCU_LOCKDEP_WARN(ct_dynticks_nesting() <= 0,\n\t\t\t \"RCU dynticks_nesting counter underflow/zero!\");\n\tRCU_LOCKDEP_WARN(ct_dynticks_nmi_nesting() !=\n\t\t\t DYNTICK_IRQ_NONIDLE,\n\t\t\t \"Bad RCU  dynticks_nmi_nesting counter\\n\");\n\tRCU_LOCKDEP_WARN(rcu_dynticks_curr_cpu_in_eqs(),\n\t\t\t \"RCU in extended quiescent state!\");\n}\n#endif  \n\n#ifdef CONFIG_NO_HZ_FULL\n \nvoid __rcu_irq_enter_check_tick(void)\n{\n\tstruct rcu_data *rdp = this_cpu_ptr(&rcu_data);\n\n\t\n\tif (in_nmi())\n\t\treturn;\n\n\tRCU_LOCKDEP_WARN(rcu_dynticks_curr_cpu_in_eqs(),\n\t\t\t \"Illegal rcu_irq_enter_check_tick() from extended quiescent state\");\n\n\tif (!tick_nohz_full_cpu(rdp->cpu) ||\n\t    !READ_ONCE(rdp->rcu_urgent_qs) ||\n\t    READ_ONCE(rdp->rcu_forced_tick)) {\n\t\t\n\t\t\n\t\treturn;\n\t}\n\n\t\n\t\n\t\n\t\n\t\n\t\n\traw_spin_lock_rcu_node(rdp->mynode);\n\tif (READ_ONCE(rdp->rcu_urgent_qs) && !rdp->rcu_forced_tick) {\n\t\t\n\t\t\n\t\tWRITE_ONCE(rdp->rcu_forced_tick, true);\n\t\ttick_dep_set_cpu(rdp->cpu, TICK_DEP_BIT_RCU);\n\t}\n\traw_spin_unlock_rcu_node(rdp->mynode);\n}\nNOKPROBE_SYMBOL(__rcu_irq_enter_check_tick);\n#endif  \n\n \nint rcu_needs_cpu(void)\n{\n\treturn !rcu_segcblist_empty(&this_cpu_ptr(&rcu_data)->cblist) &&\n\t\t!rcu_rdp_is_offloaded(this_cpu_ptr(&rcu_data));\n}\n\n \nstatic void rcu_disable_urgency_upon_qs(struct rcu_data *rdp)\n{\n\traw_lockdep_assert_held_rcu_node(rdp->mynode);\n\tWRITE_ONCE(rdp->rcu_urgent_qs, false);\n\tWRITE_ONCE(rdp->rcu_need_heavy_qs, false);\n\tif (tick_nohz_full_cpu(rdp->cpu) && rdp->rcu_forced_tick) {\n\t\ttick_dep_clear_cpu(rdp->cpu, TICK_DEP_BIT_RCU);\n\t\tWRITE_ONCE(rdp->rcu_forced_tick, false);\n\t}\n}\n\n \nnotrace bool rcu_is_watching(void)\n{\n\tbool ret;\n\n\tpreempt_disable_notrace();\n\tret = !rcu_dynticks_curr_cpu_in_eqs();\n\tpreempt_enable_notrace();\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(rcu_is_watching);\n\n \nvoid rcu_request_urgent_qs_task(struct task_struct *t)\n{\n\tint cpu;\n\n\tbarrier();\n\tcpu = task_cpu(t);\n\tif (!task_curr(t))\n\t\treturn;  \n\tsmp_store_release(per_cpu_ptr(&rcu_data.rcu_urgent_qs, cpu), true);\n}\n\n \nstatic void rcu_gpnum_ovf(struct rcu_node *rnp, struct rcu_data *rdp)\n{\n\traw_lockdep_assert_held_rcu_node(rnp);\n\tif (ULONG_CMP_LT(rcu_seq_current(&rdp->gp_seq) + ULONG_MAX / 4,\n\t\t\t rnp->gp_seq))\n\t\tWRITE_ONCE(rdp->gpwrap, true);\n\tif (ULONG_CMP_LT(rdp->rcu_iw_gp_seq + ULONG_MAX / 4, rnp->gp_seq))\n\t\trdp->rcu_iw_gp_seq = rnp->gp_seq + ULONG_MAX / 4;\n}\n\n \nstatic int dyntick_save_progress_counter(struct rcu_data *rdp)\n{\n\trdp->dynticks_snap = rcu_dynticks_snap(rdp->cpu);\n\tif (rcu_dynticks_in_eqs(rdp->dynticks_snap)) {\n\t\ttrace_rcu_fqs(rcu_state.name, rdp->gp_seq, rdp->cpu, TPS(\"dti\"));\n\t\trcu_gpnum_ovf(rdp->mynode, rdp);\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\n \nstatic int rcu_implicit_dynticks_qs(struct rcu_data *rdp)\n{\n\tunsigned long jtsq;\n\tint ret = 0;\n\tstruct rcu_node *rnp = rdp->mynode;\n\n\t \n\tif (rcu_dynticks_in_eqs_since(rdp, rdp->dynticks_snap)) {\n\t\ttrace_rcu_fqs(rcu_state.name, rdp->gp_seq, rdp->cpu, TPS(\"dti\"));\n\t\trcu_gpnum_ovf(rnp, rdp);\n\t\treturn 1;\n\t}\n\n\t \n\tif (WARN_ON_ONCE(!rcu_rdp_cpu_online(rdp))) {\n\t\tstruct rcu_node *rnp1;\n\n\t\tpr_info(\"%s: grp: %d-%d level: %d ->gp_seq %ld ->completedqs %ld\\n\",\n\t\t\t__func__, rnp->grplo, rnp->grphi, rnp->level,\n\t\t\t(long)rnp->gp_seq, (long)rnp->completedqs);\n\t\tfor (rnp1 = rnp; rnp1; rnp1 = rnp1->parent)\n\t\t\tpr_info(\"%s: %d:%d ->qsmask %#lx ->qsmaskinit %#lx ->qsmaskinitnext %#lx ->rcu_gp_init_mask %#lx\\n\",\n\t\t\t\t__func__, rnp1->grplo, rnp1->grphi, rnp1->qsmask, rnp1->qsmaskinit, rnp1->qsmaskinitnext, rnp1->rcu_gp_init_mask);\n\t\tpr_info(\"%s %d: %c online: %ld(%d) offline: %ld(%d)\\n\",\n\t\t\t__func__, rdp->cpu, \".o\"[rcu_rdp_cpu_online(rdp)],\n\t\t\t(long)rdp->rcu_onl_gp_seq, rdp->rcu_onl_gp_flags,\n\t\t\t(long)rdp->rcu_ofl_gp_seq, rdp->rcu_ofl_gp_flags);\n\t\treturn 1;  \n\t}\n\n\t \n\tjtsq = READ_ONCE(jiffies_to_sched_qs);\n\tif (!READ_ONCE(rdp->rcu_need_heavy_qs) &&\n\t    (time_after(jiffies, rcu_state.gp_start + jtsq * 2) ||\n\t     time_after(jiffies, rcu_state.jiffies_resched) ||\n\t     rcu_state.cbovld)) {\n\t\tWRITE_ONCE(rdp->rcu_need_heavy_qs, true);\n\t\t \n\t\tsmp_store_release(&rdp->rcu_urgent_qs, true);\n\t} else if (time_after(jiffies, rcu_state.gp_start + jtsq)) {\n\t\tWRITE_ONCE(rdp->rcu_urgent_qs, true);\n\t}\n\n\t \n\tif (tick_nohz_full_cpu(rdp->cpu) &&\n\t    (time_after(jiffies, READ_ONCE(rdp->last_fqs_resched) + jtsq * 3) ||\n\t     rcu_state.cbovld)) {\n\t\tWRITE_ONCE(rdp->rcu_urgent_qs, true);\n\t\tWRITE_ONCE(rdp->last_fqs_resched, jiffies);\n\t\tret = -1;\n\t}\n\n\t \n\tif (time_after(jiffies, rcu_state.jiffies_resched)) {\n\t\tif (time_after(jiffies,\n\t\t\t       READ_ONCE(rdp->last_fqs_resched) + jtsq)) {\n\t\t\tWRITE_ONCE(rdp->last_fqs_resched, jiffies);\n\t\t\tret = -1;\n\t\t}\n\t\tif (IS_ENABLED(CONFIG_IRQ_WORK) &&\n\t\t    !rdp->rcu_iw_pending && rdp->rcu_iw_gp_seq != rnp->gp_seq &&\n\t\t    (rnp->ffmask & rdp->grpmask)) {\n\t\t\trdp->rcu_iw_pending = true;\n\t\t\trdp->rcu_iw_gp_seq = rnp->gp_seq;\n\t\t\tirq_work_queue_on(&rdp->rcu_iw, rdp->cpu);\n\t\t}\n\n\t\tif (rcu_cpu_stall_cputime && rdp->snap_record.gp_seq != rdp->gp_seq) {\n\t\t\tint cpu = rdp->cpu;\n\t\t\tstruct rcu_snap_record *rsrp;\n\t\t\tstruct kernel_cpustat *kcsp;\n\n\t\t\tkcsp = &kcpustat_cpu(cpu);\n\n\t\t\trsrp = &rdp->snap_record;\n\t\t\trsrp->cputime_irq     = kcpustat_field(kcsp, CPUTIME_IRQ, cpu);\n\t\t\trsrp->cputime_softirq = kcpustat_field(kcsp, CPUTIME_SOFTIRQ, cpu);\n\t\t\trsrp->cputime_system  = kcpustat_field(kcsp, CPUTIME_SYSTEM, cpu);\n\t\t\trsrp->nr_hardirqs = kstat_cpu_irqs_sum(rdp->cpu);\n\t\t\trsrp->nr_softirqs = kstat_cpu_softirqs_sum(rdp->cpu);\n\t\t\trsrp->nr_csw = nr_context_switches_cpu(rdp->cpu);\n\t\t\trsrp->jiffies = jiffies;\n\t\t\trsrp->gp_seq = rdp->gp_seq;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\n \nstatic void trace_rcu_this_gp(struct rcu_node *rnp, struct rcu_data *rdp,\n\t\t\t      unsigned long gp_seq_req, const char *s)\n{\n\ttrace_rcu_future_grace_period(rcu_state.name, READ_ONCE(rnp->gp_seq),\n\t\t\t\t      gp_seq_req, rnp->level,\n\t\t\t\t      rnp->grplo, rnp->grphi, s);\n}\n\n \nstatic bool rcu_start_this_gp(struct rcu_node *rnp_start, struct rcu_data *rdp,\n\t\t\t      unsigned long gp_seq_req)\n{\n\tbool ret = false;\n\tstruct rcu_node *rnp;\n\n\t \n\traw_lockdep_assert_held_rcu_node(rnp_start);\n\ttrace_rcu_this_gp(rnp_start, rdp, gp_seq_req, TPS(\"Startleaf\"));\n\tfor (rnp = rnp_start; 1; rnp = rnp->parent) {\n\t\tif (rnp != rnp_start)\n\t\t\traw_spin_lock_rcu_node(rnp);\n\t\tif (ULONG_CMP_GE(rnp->gp_seq_needed, gp_seq_req) ||\n\t\t    rcu_seq_started(&rnp->gp_seq, gp_seq_req) ||\n\t\t    (rnp != rnp_start &&\n\t\t     rcu_seq_state(rcu_seq_current(&rnp->gp_seq)))) {\n\t\t\ttrace_rcu_this_gp(rnp, rdp, gp_seq_req,\n\t\t\t\t\t  TPS(\"Prestarted\"));\n\t\t\tgoto unlock_out;\n\t\t}\n\t\tWRITE_ONCE(rnp->gp_seq_needed, gp_seq_req);\n\t\tif (rcu_seq_state(rcu_seq_current(&rnp->gp_seq))) {\n\t\t\t \n\t\t\ttrace_rcu_this_gp(rnp_start, rdp, gp_seq_req,\n\t\t\t\t\t  TPS(\"Startedleaf\"));\n\t\t\tgoto unlock_out;\n\t\t}\n\t\tif (rnp != rnp_start && rnp->parent != NULL)\n\t\t\traw_spin_unlock_rcu_node(rnp);\n\t\tif (!rnp->parent)\n\t\t\tbreak;   \n\t}\n\n\t \n\tif (rcu_gp_in_progress()) {\n\t\ttrace_rcu_this_gp(rnp, rdp, gp_seq_req, TPS(\"Startedleafroot\"));\n\t\tgoto unlock_out;\n\t}\n\ttrace_rcu_this_gp(rnp, rdp, gp_seq_req, TPS(\"Startedroot\"));\n\tWRITE_ONCE(rcu_state.gp_flags, rcu_state.gp_flags | RCU_GP_FLAG_INIT);\n\tWRITE_ONCE(rcu_state.gp_req_activity, jiffies);\n\tif (!READ_ONCE(rcu_state.gp_kthread)) {\n\t\ttrace_rcu_this_gp(rnp, rdp, gp_seq_req, TPS(\"NoGPkthread\"));\n\t\tgoto unlock_out;\n\t}\n\ttrace_rcu_grace_period(rcu_state.name, data_race(rcu_state.gp_seq), TPS(\"newreq\"));\n\tret = true;   \nunlock_out:\n\t \n\tif (ULONG_CMP_LT(gp_seq_req, rnp->gp_seq_needed)) {\n\t\tWRITE_ONCE(rnp_start->gp_seq_needed, rnp->gp_seq_needed);\n\t\tWRITE_ONCE(rdp->gp_seq_needed, rnp->gp_seq_needed);\n\t}\n\tif (rnp != rnp_start)\n\t\traw_spin_unlock_rcu_node(rnp);\n\treturn ret;\n}\n\n \nstatic bool rcu_future_gp_cleanup(struct rcu_node *rnp)\n{\n\tbool needmore;\n\tstruct rcu_data *rdp = this_cpu_ptr(&rcu_data);\n\n\tneedmore = ULONG_CMP_LT(rnp->gp_seq, rnp->gp_seq_needed);\n\tif (!needmore)\n\t\trnp->gp_seq_needed = rnp->gp_seq;  \n\ttrace_rcu_this_gp(rnp, rdp, rnp->gp_seq,\n\t\t\t  needmore ? TPS(\"CleanupMore\") : TPS(\"Cleanup\"));\n\treturn needmore;\n}\n\n \nstatic void rcu_gp_kthread_wake(void)\n{\n\tstruct task_struct *t = READ_ONCE(rcu_state.gp_kthread);\n\n\tif ((current == t && !in_hardirq() && !in_serving_softirq()) ||\n\t    !READ_ONCE(rcu_state.gp_flags) || !t)\n\t\treturn;\n\tWRITE_ONCE(rcu_state.gp_wake_time, jiffies);\n\tWRITE_ONCE(rcu_state.gp_wake_seq, READ_ONCE(rcu_state.gp_seq));\n\tswake_up_one(&rcu_state.gp_wq);\n}\n\n \nstatic bool rcu_accelerate_cbs(struct rcu_node *rnp, struct rcu_data *rdp)\n{\n\tunsigned long gp_seq_req;\n\tbool ret = false;\n\n\trcu_lockdep_assert_cblist_protected(rdp);\n\traw_lockdep_assert_held_rcu_node(rnp);\n\n\t \n\tif (!rcu_segcblist_pend_cbs(&rdp->cblist))\n\t\treturn false;\n\n\ttrace_rcu_segcb_stats(&rdp->cblist, TPS(\"SegCbPreAcc\"));\n\n\t \n\tgp_seq_req = rcu_seq_snap(&rcu_state.gp_seq);\n\tif (rcu_segcblist_accelerate(&rdp->cblist, gp_seq_req))\n\t\tret = rcu_start_this_gp(rnp, rdp, gp_seq_req);\n\n\t \n\tif (rcu_segcblist_restempty(&rdp->cblist, RCU_WAIT_TAIL))\n\t\ttrace_rcu_grace_period(rcu_state.name, gp_seq_req, TPS(\"AccWaitCB\"));\n\telse\n\t\ttrace_rcu_grace_period(rcu_state.name, gp_seq_req, TPS(\"AccReadyCB\"));\n\n\ttrace_rcu_segcb_stats(&rdp->cblist, TPS(\"SegCbPostAcc\"));\n\n\treturn ret;\n}\n\n \nstatic void rcu_accelerate_cbs_unlocked(struct rcu_node *rnp,\n\t\t\t\t\tstruct rcu_data *rdp)\n{\n\tunsigned long c;\n\tbool needwake;\n\n\trcu_lockdep_assert_cblist_protected(rdp);\n\tc = rcu_seq_snap(&rcu_state.gp_seq);\n\tif (!READ_ONCE(rdp->gpwrap) && ULONG_CMP_GE(rdp->gp_seq_needed, c)) {\n\t\t \n\t\t(void)rcu_segcblist_accelerate(&rdp->cblist, c);\n\t\treturn;\n\t}\n\traw_spin_lock_rcu_node(rnp);  \n\tneedwake = rcu_accelerate_cbs(rnp, rdp);\n\traw_spin_unlock_rcu_node(rnp);  \n\tif (needwake)\n\t\trcu_gp_kthread_wake();\n}\n\n \nstatic bool rcu_advance_cbs(struct rcu_node *rnp, struct rcu_data *rdp)\n{\n\trcu_lockdep_assert_cblist_protected(rdp);\n\traw_lockdep_assert_held_rcu_node(rnp);\n\n\t \n\tif (!rcu_segcblist_pend_cbs(&rdp->cblist))\n\t\treturn false;\n\n\t \n\trcu_segcblist_advance(&rdp->cblist, rnp->gp_seq);\n\n\t \n\treturn rcu_accelerate_cbs(rnp, rdp);\n}\n\n \nstatic void __maybe_unused rcu_advance_cbs_nowake(struct rcu_node *rnp,\n\t\t\t\t\t\t  struct rcu_data *rdp)\n{\n\trcu_lockdep_assert_cblist_protected(rdp);\n\tif (!rcu_seq_state(rcu_seq_current(&rnp->gp_seq)) || !raw_spin_trylock_rcu_node(rnp))\n\t\treturn;\n\t \n\tif (rcu_seq_state(rcu_seq_current(&rnp->gp_seq)))\n\t\tWARN_ON_ONCE(rcu_advance_cbs(rnp, rdp));\n\traw_spin_unlock_rcu_node(rnp);\n}\n\n \nstatic void rcu_strict_gp_check_qs(void)\n{\n\tif (IS_ENABLED(CONFIG_RCU_STRICT_GRACE_PERIOD)) {\n\t\trcu_read_lock();\n\t\trcu_read_unlock();\n\t}\n}\n\n \nstatic bool __note_gp_changes(struct rcu_node *rnp, struct rcu_data *rdp)\n{\n\tbool ret = false;\n\tbool need_qs;\n\tconst bool offloaded = rcu_rdp_is_offloaded(rdp);\n\n\traw_lockdep_assert_held_rcu_node(rnp);\n\n\tif (rdp->gp_seq == rnp->gp_seq)\n\t\treturn false;  \n\n\t \n\tif (rcu_seq_completed_gp(rdp->gp_seq, rnp->gp_seq) ||\n\t    unlikely(READ_ONCE(rdp->gpwrap))) {\n\t\tif (!offloaded)\n\t\t\tret = rcu_advance_cbs(rnp, rdp);  \n\t\trdp->core_needs_qs = false;\n\t\ttrace_rcu_grace_period(rcu_state.name, rdp->gp_seq, TPS(\"cpuend\"));\n\t} else {\n\t\tif (!offloaded)\n\t\t\tret = rcu_accelerate_cbs(rnp, rdp);  \n\t\tif (rdp->core_needs_qs)\n\t\t\trdp->core_needs_qs = !!(rnp->qsmask & rdp->grpmask);\n\t}\n\n\t \n\tif (rcu_seq_new_gp(rdp->gp_seq, rnp->gp_seq) ||\n\t    unlikely(READ_ONCE(rdp->gpwrap))) {\n\t\t \n\t\ttrace_rcu_grace_period(rcu_state.name, rnp->gp_seq, TPS(\"cpustart\"));\n\t\tneed_qs = !!(rnp->qsmask & rdp->grpmask);\n\t\trdp->cpu_no_qs.b.norm = need_qs;\n\t\trdp->core_needs_qs = need_qs;\n\t\tzero_cpu_stall_ticks(rdp);\n\t}\n\trdp->gp_seq = rnp->gp_seq;   \n\tif (ULONG_CMP_LT(rdp->gp_seq_needed, rnp->gp_seq_needed) || rdp->gpwrap)\n\t\tWRITE_ONCE(rdp->gp_seq_needed, rnp->gp_seq_needed);\n\tif (IS_ENABLED(CONFIG_PROVE_RCU) && READ_ONCE(rdp->gpwrap))\n\t\tWRITE_ONCE(rdp->last_sched_clock, jiffies);\n\tWRITE_ONCE(rdp->gpwrap, false);\n\trcu_gpnum_ovf(rnp, rdp);\n\treturn ret;\n}\n\nstatic void note_gp_changes(struct rcu_data *rdp)\n{\n\tunsigned long flags;\n\tbool needwake;\n\tstruct rcu_node *rnp;\n\n\tlocal_irq_save(flags);\n\trnp = rdp->mynode;\n\tif ((rdp->gp_seq == rcu_seq_current(&rnp->gp_seq) &&\n\t     !unlikely(READ_ONCE(rdp->gpwrap))) ||  \n\t    !raw_spin_trylock_rcu_node(rnp)) {  \n\t\tlocal_irq_restore(flags);\n\t\treturn;\n\t}\n\tneedwake = __note_gp_changes(rnp, rdp);\n\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n\trcu_strict_gp_check_qs();\n\tif (needwake)\n\t\trcu_gp_kthread_wake();\n}\n\nstatic atomic_t *rcu_gp_slow_suppress;\n\n \nvoid rcu_gp_slow_register(atomic_t *rgssp)\n{\n\tWARN_ON_ONCE(rcu_gp_slow_suppress);\n\n\tWRITE_ONCE(rcu_gp_slow_suppress, rgssp);\n}\nEXPORT_SYMBOL_GPL(rcu_gp_slow_register);\n\n \nvoid rcu_gp_slow_unregister(atomic_t *rgssp)\n{\n\tWARN_ON_ONCE(rgssp && rgssp != rcu_gp_slow_suppress);\n\n\tWRITE_ONCE(rcu_gp_slow_suppress, NULL);\n}\nEXPORT_SYMBOL_GPL(rcu_gp_slow_unregister);\n\nstatic bool rcu_gp_slow_is_suppressed(void)\n{\n\tatomic_t *rgssp = READ_ONCE(rcu_gp_slow_suppress);\n\n\treturn rgssp && atomic_read(rgssp);\n}\n\nstatic void rcu_gp_slow(int delay)\n{\n\tif (!rcu_gp_slow_is_suppressed() && delay > 0 &&\n\t    !(rcu_seq_ctr(rcu_state.gp_seq) % (rcu_num_nodes * PER_RCU_NODE_PERIOD * delay)))\n\t\tschedule_timeout_idle(delay);\n}\n\nstatic unsigned long sleep_duration;\n\n \nvoid rcu_gp_set_torture_wait(int duration)\n{\n\tif (IS_ENABLED(CONFIG_RCU_TORTURE_TEST) && duration > 0)\n\t\tWRITE_ONCE(sleep_duration, duration);\n}\nEXPORT_SYMBOL_GPL(rcu_gp_set_torture_wait);\n\n \nstatic void rcu_gp_torture_wait(void)\n{\n\tunsigned long duration;\n\n\tif (!IS_ENABLED(CONFIG_RCU_TORTURE_TEST))\n\t\treturn;\n\tduration = xchg(&sleep_duration, 0UL);\n\tif (duration > 0) {\n\t\tpr_alert(\"%s: Waiting %lu jiffies\\n\", __func__, duration);\n\t\tschedule_timeout_idle(duration);\n\t\tpr_alert(\"%s: Wait complete\\n\", __func__);\n\t}\n}\n\n \nstatic void rcu_strict_gp_boundary(void *unused)\n{\n\tinvoke_rcu_core();\n}\n\n \nstatic void rcu_poll_gp_seq_start(unsigned long *snap)\n{\n\tstruct rcu_node *rnp = rcu_get_root();\n\n\tif (rcu_scheduler_active != RCU_SCHEDULER_INACTIVE)\n\t\traw_lockdep_assert_held_rcu_node(rnp);\n\n\t \n\tif (!rcu_seq_state(rcu_state.gp_seq_polled))\n\t\trcu_seq_start(&rcu_state.gp_seq_polled);\n\n\t \n\t*snap = rcu_state.gp_seq_polled;\n}\n\n \nstatic void rcu_poll_gp_seq_end(unsigned long *snap)\n{\n\tstruct rcu_node *rnp = rcu_get_root();\n\n\tif (rcu_scheduler_active != RCU_SCHEDULER_INACTIVE)\n\t\traw_lockdep_assert_held_rcu_node(rnp);\n\n\t \n\t \n\t \n\tif (*snap && *snap == rcu_state.gp_seq_polled) {\n\t\trcu_seq_end(&rcu_state.gp_seq_polled);\n\t\trcu_state.gp_seq_polled_snap = 0;\n\t\trcu_state.gp_seq_polled_exp_snap = 0;\n\t} else {\n\t\t*snap = 0;\n\t}\n}\n\n \n \nstatic void rcu_poll_gp_seq_start_unlocked(unsigned long *snap)\n{\n\tunsigned long flags;\n\tstruct rcu_node *rnp = rcu_get_root();\n\n\tif (rcu_init_invoked()) {\n\t\tif (rcu_scheduler_active != RCU_SCHEDULER_INACTIVE)\n\t\t\tlockdep_assert_irqs_enabled();\n\t\traw_spin_lock_irqsave_rcu_node(rnp, flags);\n\t}\n\trcu_poll_gp_seq_start(snap);\n\tif (rcu_init_invoked())\n\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n}\n\n\n\nstatic void rcu_poll_gp_seq_end_unlocked(unsigned long *snap)\n{\n\tunsigned long flags;\n\tstruct rcu_node *rnp = rcu_get_root();\n\n\tif (rcu_init_invoked()) {\n\t\tif (rcu_scheduler_active != RCU_SCHEDULER_INACTIVE)\n\t\t\tlockdep_assert_irqs_enabled();\n\t\traw_spin_lock_irqsave_rcu_node(rnp, flags);\n\t}\n\trcu_poll_gp_seq_end(snap);\n\tif (rcu_init_invoked())\n\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n}\n\n \nstatic noinline_for_stack bool rcu_gp_init(void)\n{\n\tunsigned long flags;\n\tunsigned long oldmask;\n\tunsigned long mask;\n\tstruct rcu_data *rdp;\n\tstruct rcu_node *rnp = rcu_get_root();\n\n\tWRITE_ONCE(rcu_state.gp_activity, jiffies);\n\traw_spin_lock_irq_rcu_node(rnp);\n\tif (!READ_ONCE(rcu_state.gp_flags)) {\n\t\t \n\t\traw_spin_unlock_irq_rcu_node(rnp);\n\t\treturn false;\n\t}\n\tWRITE_ONCE(rcu_state.gp_flags, 0);  \n\n\tif (WARN_ON_ONCE(rcu_gp_in_progress())) {\n\t\t \n\t\traw_spin_unlock_irq_rcu_node(rnp);\n\t\treturn false;\n\t}\n\n\t \n\trecord_gp_stall_check_time();\n\t \n\trcu_seq_start(&rcu_state.gp_seq);\n\tASSERT_EXCLUSIVE_WRITER(rcu_state.gp_seq);\n\ttrace_rcu_grace_period(rcu_state.name, rcu_state.gp_seq, TPS(\"start\"));\n\trcu_poll_gp_seq_start(&rcu_state.gp_seq_polled_snap);\n\traw_spin_unlock_irq_rcu_node(rnp);\n\n\t \n\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_ONOFF);\n\t \n\trcu_for_each_leaf_node(rnp) {\n\t\tlocal_irq_save(flags);\n\t\tarch_spin_lock(&rcu_state.ofl_lock);\n\t\traw_spin_lock_rcu_node(rnp);\n\t\tif (rnp->qsmaskinit == rnp->qsmaskinitnext &&\n\t\t    !rnp->wait_blkd_tasks) {\n\t\t\t \n\t\t\traw_spin_unlock_rcu_node(rnp);\n\t\t\tarch_spin_unlock(&rcu_state.ofl_lock);\n\t\t\tlocal_irq_restore(flags);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\toldmask = rnp->qsmaskinit;\n\t\trnp->qsmaskinit = rnp->qsmaskinitnext;\n\n\t\t \n\t\tif (!oldmask != !rnp->qsmaskinit) {\n\t\t\tif (!oldmask) {  \n\t\t\t\tif (!rnp->wait_blkd_tasks)  \n\t\t\t\t\trcu_init_new_rnp(rnp);\n\t\t\t} else if (rcu_preempt_has_tasks(rnp)) {\n\t\t\t\trnp->wait_blkd_tasks = true;  \n\t\t\t} else {  \n\t\t\t\trcu_cleanup_dead_rnp(rnp);\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (rnp->wait_blkd_tasks &&\n\t\t    (!rcu_preempt_has_tasks(rnp) || rnp->qsmaskinit)) {\n\t\t\trnp->wait_blkd_tasks = false;\n\t\t\tif (!rnp->qsmaskinit)\n\t\t\t\trcu_cleanup_dead_rnp(rnp);\n\t\t}\n\n\t\traw_spin_unlock_rcu_node(rnp);\n\t\tarch_spin_unlock(&rcu_state.ofl_lock);\n\t\tlocal_irq_restore(flags);\n\t}\n\trcu_gp_slow(gp_preinit_delay);  \n\n\t \n\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_INIT);\n\trcu_for_each_node_breadth_first(rnp) {\n\t\trcu_gp_slow(gp_init_delay);\n\t\traw_spin_lock_irqsave_rcu_node(rnp, flags);\n\t\trdp = this_cpu_ptr(&rcu_data);\n\t\trcu_preempt_check_blocked_tasks(rnp);\n\t\trnp->qsmask = rnp->qsmaskinit;\n\t\tWRITE_ONCE(rnp->gp_seq, rcu_state.gp_seq);\n\t\tif (rnp == rdp->mynode)\n\t\t\t(void)__note_gp_changes(rnp, rdp);\n\t\trcu_preempt_boost_start_gp(rnp);\n\t\ttrace_rcu_grace_period_init(rcu_state.name, rnp->gp_seq,\n\t\t\t\t\t    rnp->level, rnp->grplo,\n\t\t\t\t\t    rnp->grphi, rnp->qsmask);\n\t\t \n\t\tmask = rnp->qsmask & ~rnp->qsmaskinitnext;\n\t\trnp->rcu_gp_init_mask = mask;\n\t\tif ((mask || rnp->wait_blkd_tasks) && rcu_is_leaf_node(rnp))\n\t\t\trcu_report_qs_rnp(mask, rnp, rnp->gp_seq, flags);\n\t\telse\n\t\t\traw_spin_unlock_irq_rcu_node(rnp);\n\t\tcond_resched_tasks_rcu_qs();\n\t\tWRITE_ONCE(rcu_state.gp_activity, jiffies);\n\t}\n\n\t\n\tif (IS_ENABLED(CONFIG_RCU_STRICT_GRACE_PERIOD))\n\t\ton_each_cpu(rcu_strict_gp_boundary, NULL, 0);\n\n\treturn true;\n}\n\n \nstatic bool rcu_gp_fqs_check_wake(int *gfp)\n{\n\tstruct rcu_node *rnp = rcu_get_root();\n\n\t\n\tif (*gfp & RCU_GP_FLAG_OVLD)\n\t\treturn true;\n\n\t\n\t*gfp = READ_ONCE(rcu_state.gp_flags);\n\tif (*gfp & RCU_GP_FLAG_FQS)\n\t\treturn true;\n\n\t\n\tif (!READ_ONCE(rnp->qsmask) && !rcu_preempt_blocked_readers_cgp(rnp))\n\t\treturn true;\n\n\treturn false;\n}\n\n \nstatic void rcu_gp_fqs(bool first_time)\n{\n\tint nr_fqs = READ_ONCE(rcu_state.nr_fqs_jiffies_stall);\n\tstruct rcu_node *rnp = rcu_get_root();\n\n\tWRITE_ONCE(rcu_state.gp_activity, jiffies);\n\tWRITE_ONCE(rcu_state.n_force_qs, rcu_state.n_force_qs + 1);\n\n\tWARN_ON_ONCE(nr_fqs > 3);\n\t \n\tif (nr_fqs) {\n\t\tif (nr_fqs == 1) {\n\t\t\tWRITE_ONCE(rcu_state.jiffies_stall,\n\t\t\t\t   jiffies + rcu_jiffies_till_stall_check());\n\t\t}\n\t\tWRITE_ONCE(rcu_state.nr_fqs_jiffies_stall, --nr_fqs);\n\t}\n\n\tif (first_time) {\n\t\t \n\t\tforce_qs_rnp(dyntick_save_progress_counter);\n\t} else {\n\t\t \n\t\tforce_qs_rnp(rcu_implicit_dynticks_qs);\n\t}\n\t \n\tif (READ_ONCE(rcu_state.gp_flags) & RCU_GP_FLAG_FQS) {\n\t\traw_spin_lock_irq_rcu_node(rnp);\n\t\tWRITE_ONCE(rcu_state.gp_flags,\n\t\t\t   READ_ONCE(rcu_state.gp_flags) & ~RCU_GP_FLAG_FQS);\n\t\traw_spin_unlock_irq_rcu_node(rnp);\n\t}\n}\n\n \nstatic noinline_for_stack void rcu_gp_fqs_loop(void)\n{\n\tbool first_gp_fqs = true;\n\tint gf = 0;\n\tunsigned long j;\n\tint ret;\n\tstruct rcu_node *rnp = rcu_get_root();\n\n\tj = READ_ONCE(jiffies_till_first_fqs);\n\tif (rcu_state.cbovld)\n\t\tgf = RCU_GP_FLAG_OVLD;\n\tret = 0;\n\tfor (;;) {\n\t\tif (rcu_state.cbovld) {\n\t\t\tj = (j + 2) / 3;\n\t\t\tif (j <= 0)\n\t\t\t\tj = 1;\n\t\t}\n\t\tif (!ret || time_before(jiffies + j, rcu_state.jiffies_force_qs)) {\n\t\t\tWRITE_ONCE(rcu_state.jiffies_force_qs, jiffies + j);\n\t\t\t \n\t\t\tsmp_wmb();\n\t\t\tWRITE_ONCE(rcu_state.jiffies_kick_kthreads,\n\t\t\t\t   jiffies + (j ? 3 * j : 2));\n\t\t}\n\t\ttrace_rcu_grace_period(rcu_state.name, rcu_state.gp_seq,\n\t\t\t\t       TPS(\"fqswait\"));\n\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_WAIT_FQS);\n\t\t(void)swait_event_idle_timeout_exclusive(rcu_state.gp_wq,\n\t\t\t\t rcu_gp_fqs_check_wake(&gf), j);\n\t\trcu_gp_torture_wait();\n\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_DOING_FQS);\n\t\t \n\t\t \n\t\tif (!READ_ONCE(rnp->qsmask) &&\n\t\t    !rcu_preempt_blocked_readers_cgp(rnp))\n\t\t\tbreak;\n\t\t \n\t\tif (!time_after(rcu_state.jiffies_force_qs, jiffies) ||\n\t\t    (gf & (RCU_GP_FLAG_FQS | RCU_GP_FLAG_OVLD))) {\n\t\t\ttrace_rcu_grace_period(rcu_state.name, rcu_state.gp_seq,\n\t\t\t\t\t       TPS(\"fqsstart\"));\n\t\t\trcu_gp_fqs(first_gp_fqs);\n\t\t\tgf = 0;\n\t\t\tif (first_gp_fqs) {\n\t\t\t\tfirst_gp_fqs = false;\n\t\t\t\tgf = rcu_state.cbovld ? RCU_GP_FLAG_OVLD : 0;\n\t\t\t}\n\t\t\ttrace_rcu_grace_period(rcu_state.name, rcu_state.gp_seq,\n\t\t\t\t\t       TPS(\"fqsend\"));\n\t\t\tcond_resched_tasks_rcu_qs();\n\t\t\tWRITE_ONCE(rcu_state.gp_activity, jiffies);\n\t\t\tret = 0;  \n\t\t\tj = READ_ONCE(jiffies_till_next_fqs);\n\t\t} else {\n\t\t\t \n\t\t\tcond_resched_tasks_rcu_qs();\n\t\t\tWRITE_ONCE(rcu_state.gp_activity, jiffies);\n\t\t\tWARN_ON(signal_pending(current));\n\t\t\ttrace_rcu_grace_period(rcu_state.name, rcu_state.gp_seq,\n\t\t\t\t\t       TPS(\"fqswaitsig\"));\n\t\t\tret = 1;  \n\t\t\tj = jiffies;\n\t\t\tif (time_after(jiffies, rcu_state.jiffies_force_qs))\n\t\t\t\tj = 1;\n\t\t\telse\n\t\t\t\tj = rcu_state.jiffies_force_qs - j;\n\t\t\tgf = 0;\n\t\t}\n\t}\n}\n\n \nstatic noinline void rcu_gp_cleanup(void)\n{\n\tint cpu;\n\tbool needgp = false;\n\tunsigned long gp_duration;\n\tunsigned long new_gp_seq;\n\tbool offloaded;\n\tstruct rcu_data *rdp;\n\tstruct rcu_node *rnp = rcu_get_root();\n\tstruct swait_queue_head *sq;\n\n\tWRITE_ONCE(rcu_state.gp_activity, jiffies);\n\traw_spin_lock_irq_rcu_node(rnp);\n\trcu_state.gp_end = jiffies;\n\tgp_duration = rcu_state.gp_end - rcu_state.gp_start;\n\tif (gp_duration > rcu_state.gp_max)\n\t\trcu_state.gp_max = gp_duration;\n\n\t \n\trcu_poll_gp_seq_end(&rcu_state.gp_seq_polled_snap);\n\traw_spin_unlock_irq_rcu_node(rnp);\n\n\t \n\tnew_gp_seq = rcu_state.gp_seq;\n\trcu_seq_end(&new_gp_seq);\n\trcu_for_each_node_breadth_first(rnp) {\n\t\traw_spin_lock_irq_rcu_node(rnp);\n\t\tif (WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp)))\n\t\t\tdump_blkd_tasks(rnp, 10);\n\t\tWARN_ON_ONCE(rnp->qsmask);\n\t\tWRITE_ONCE(rnp->gp_seq, new_gp_seq);\n\t\tif (!rnp->parent)\n\t\t\tsmp_mb();  \n\t\trdp = this_cpu_ptr(&rcu_data);\n\t\tif (rnp == rdp->mynode)\n\t\t\tneedgp = __note_gp_changes(rnp, rdp) || needgp;\n\t\t \n\t\tneedgp = rcu_future_gp_cleanup(rnp) || needgp;\n\t\t \n\t\tif (rcu_is_leaf_node(rnp))\n\t\t\tfor_each_leaf_node_cpu_mask(rnp, cpu, rnp->cbovldmask) {\n\t\t\t\trdp = per_cpu_ptr(&rcu_data, cpu);\n\t\t\t\tcheck_cb_ovld_locked(rdp, rnp);\n\t\t\t}\n\t\tsq = rcu_nocb_gp_get(rnp);\n\t\traw_spin_unlock_irq_rcu_node(rnp);\n\t\trcu_nocb_gp_cleanup(sq);\n\t\tcond_resched_tasks_rcu_qs();\n\t\tWRITE_ONCE(rcu_state.gp_activity, jiffies);\n\t\trcu_gp_slow(gp_cleanup_delay);\n\t}\n\trnp = rcu_get_root();\n\traw_spin_lock_irq_rcu_node(rnp);  \n\n\t \n\ttrace_rcu_grace_period(rcu_state.name, rcu_state.gp_seq, TPS(\"end\"));\n\trcu_seq_end(&rcu_state.gp_seq);\n\tASSERT_EXCLUSIVE_WRITER(rcu_state.gp_seq);\n\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_IDLE);\n\t \n\trdp = this_cpu_ptr(&rcu_data);\n\tif (!needgp && ULONG_CMP_LT(rnp->gp_seq, rnp->gp_seq_needed)) {\n\t\ttrace_rcu_this_gp(rnp, rdp, rnp->gp_seq_needed,\n\t\t\t\t  TPS(\"CleanupMore\"));\n\t\tneedgp = true;\n\t}\n\t \n\toffloaded = rcu_rdp_is_offloaded(rdp);\n\tif ((offloaded || !rcu_accelerate_cbs(rnp, rdp)) && needgp) {\n\n\t\t \n\t\t \n\t\t \n\t\t \n\t\t \n\t\t \n\t\t \n\t\t \n\t\t \n\n\t\tWRITE_ONCE(rcu_state.gp_flags, RCU_GP_FLAG_INIT);\n\t\tWRITE_ONCE(rcu_state.gp_req_activity, jiffies);\n\t\ttrace_rcu_grace_period(rcu_state.name, rcu_state.gp_seq, TPS(\"newreq\"));\n\t} else {\n\n\t\t \n\t\t \n\t\t \n\t\t \n\t\t \n\n\t\tWRITE_ONCE(rcu_state.gp_flags, rcu_state.gp_flags & RCU_GP_FLAG_INIT);\n\t}\n\traw_spin_unlock_irq_rcu_node(rnp);\n\n\t \n\tif (IS_ENABLED(CONFIG_RCU_STRICT_GRACE_PERIOD))\n\t\ton_each_cpu(rcu_strict_gp_boundary, NULL, 0);\n}\n\n \nstatic int __noreturn rcu_gp_kthread(void *unused)\n{\n\trcu_bind_gp_kthread();\n\tfor (;;) {\n\n\t\t \n\t\tfor (;;) {\n\t\t\ttrace_rcu_grace_period(rcu_state.name, rcu_state.gp_seq,\n\t\t\t\t\t       TPS(\"reqwait\"));\n\t\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_WAIT_GPS);\n\t\t\tswait_event_idle_exclusive(rcu_state.gp_wq,\n\t\t\t\t\t READ_ONCE(rcu_state.gp_flags) &\n\t\t\t\t\t RCU_GP_FLAG_INIT);\n\t\t\trcu_gp_torture_wait();\n\t\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_DONE_GPS);\n\t\t\t \n\t\t\tif (rcu_gp_init())\n\t\t\t\tbreak;\n\t\t\tcond_resched_tasks_rcu_qs();\n\t\t\tWRITE_ONCE(rcu_state.gp_activity, jiffies);\n\t\t\tWARN_ON(signal_pending(current));\n\t\t\ttrace_rcu_grace_period(rcu_state.name, rcu_state.gp_seq,\n\t\t\t\t\t       TPS(\"reqwaitsig\"));\n\t\t}\n\n\t\t \n\t\trcu_gp_fqs_loop();\n\n\t\t \n\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_CLEANUP);\n\t\trcu_gp_cleanup();\n\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_CLEANED);\n\t}\n}\n\n \nstatic void rcu_report_qs_rsp(unsigned long flags)\n\t__releases(rcu_get_root()->lock)\n{\n\traw_lockdep_assert_held_rcu_node(rcu_get_root());\n\tWARN_ON_ONCE(!rcu_gp_in_progress());\n\tWRITE_ONCE(rcu_state.gp_flags,\n\t\t   READ_ONCE(rcu_state.gp_flags) | RCU_GP_FLAG_FQS);\n\traw_spin_unlock_irqrestore_rcu_node(rcu_get_root(), flags);\n\trcu_gp_kthread_wake();\n}\n\n \nstatic void rcu_report_qs_rnp(unsigned long mask, struct rcu_node *rnp,\n\t\t\t      unsigned long gps, unsigned long flags)\n\t__releases(rnp->lock)\n{\n\tunsigned long oldmask = 0;\n\tstruct rcu_node *rnp_c;\n\n\traw_lockdep_assert_held_rcu_node(rnp);\n\n\t \n\tfor (;;) {\n\t\tif ((!(rnp->qsmask & mask) && mask) || rnp->gp_seq != gps) {\n\n\t\t\t \n\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n\t\t\treturn;\n\t\t}\n\t\tWARN_ON_ONCE(oldmask);  \n\t\tWARN_ON_ONCE(!rcu_is_leaf_node(rnp) &&\n\t\t\t     rcu_preempt_blocked_readers_cgp(rnp));\n\t\tWRITE_ONCE(rnp->qsmask, rnp->qsmask & ~mask);\n\t\ttrace_rcu_quiescent_state_report(rcu_state.name, rnp->gp_seq,\n\t\t\t\t\t\t mask, rnp->qsmask, rnp->level,\n\t\t\t\t\t\t rnp->grplo, rnp->grphi,\n\t\t\t\t\t\t !!rnp->gp_tasks);\n\t\tif (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {\n\n\t\t\t \n\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n\t\t\treturn;\n\t\t}\n\t\trnp->completedqs = rnp->gp_seq;\n\t\tmask = rnp->grpmask;\n\t\tif (rnp->parent == NULL) {\n\n\t\t\t \n\n\t\t\tbreak;\n\t\t}\n\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n\t\trnp_c = rnp;\n\t\trnp = rnp->parent;\n\t\traw_spin_lock_irqsave_rcu_node(rnp, flags);\n\t\toldmask = READ_ONCE(rnp_c->qsmask);\n\t}\n\n\t \n\trcu_report_qs_rsp(flags);  \n}\n\n \nstatic void __maybe_unused\nrcu_report_unblock_qs_rnp(struct rcu_node *rnp, unsigned long flags)\n\t__releases(rnp->lock)\n{\n\tunsigned long gps;\n\tunsigned long mask;\n\tstruct rcu_node *rnp_p;\n\n\traw_lockdep_assert_held_rcu_node(rnp);\n\tif (WARN_ON_ONCE(!IS_ENABLED(CONFIG_PREEMPT_RCU)) ||\n\t    WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp)) ||\n\t    rnp->qsmask != 0) {\n\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n\t\treturn;   \n\t}\n\n\trnp->completedqs = rnp->gp_seq;\n\trnp_p = rnp->parent;\n\tif (rnp_p == NULL) {\n\t\t \n\t\trcu_report_qs_rsp(flags);\n\t\treturn;\n\t}\n\n\t \n\tgps = rnp->gp_seq;\n\tmask = rnp->grpmask;\n\traw_spin_unlock_rcu_node(rnp);\t \n\traw_spin_lock_rcu_node(rnp_p);\t \n\trcu_report_qs_rnp(mask, rnp_p, gps, flags);\n}\n\n \nstatic void\nrcu_report_qs_rdp(struct rcu_data *rdp)\n{\n\tunsigned long flags;\n\tunsigned long mask;\n\tbool needacc = false;\n\tstruct rcu_node *rnp;\n\n\tWARN_ON_ONCE(rdp->cpu != smp_processor_id());\n\trnp = rdp->mynode;\n\traw_spin_lock_irqsave_rcu_node(rnp, flags);\n\tif (rdp->cpu_no_qs.b.norm || rdp->gp_seq != rnp->gp_seq ||\n\t    rdp->gpwrap) {\n\n\t\t \n\t\trdp->cpu_no_qs.b.norm = true;\t \n\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n\t\treturn;\n\t}\n\tmask = rdp->grpmask;\n\trdp->core_needs_qs = false;\n\tif ((rnp->qsmask & mask) == 0) {\n\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n\t} else {\n\t\t \n\t\tif (!rcu_rdp_is_offloaded(rdp)) {\n\t\t\t \n\t\t\tWARN_ON_ONCE(rcu_accelerate_cbs(rnp, rdp));\n\t\t} else if (!rcu_segcblist_completely_offloaded(&rdp->cblist)) {\n\t\t\t \n\t\t\tneedacc = true;\n\t\t}\n\n\t\trcu_disable_urgency_upon_qs(rdp);\n\t\trcu_report_qs_rnp(mask, rnp, rnp->gp_seq, flags);\n\t\t \n\n\t\tif (needacc) {\n\t\t\trcu_nocb_lock_irqsave(rdp, flags);\n\t\t\trcu_accelerate_cbs_unlocked(rnp, rdp);\n\t\t\trcu_nocb_unlock_irqrestore(rdp, flags);\n\t\t}\n\t}\n}\n\n \nstatic void\nrcu_check_quiescent_state(struct rcu_data *rdp)\n{\n\t \n\tnote_gp_changes(rdp);\n\n\t \n\tif (!rdp->core_needs_qs)\n\t\treturn;\n\n\t \n\tif (rdp->cpu_no_qs.b.norm)\n\t\treturn;\n\n\t \n\trcu_report_qs_rdp(rdp);\n}\n\n \nstatic bool rcu_do_batch_check_time(long count, long tlimit,\n\t\t\t\t    bool jlimit_check, unsigned long jlimit)\n{\n\t \n\treturn unlikely(tlimit) &&\n\t       (!likely(count & 31) ||\n\t\t(IS_ENABLED(CONFIG_RCU_DOUBLE_CHECK_CB_TIME) &&\n\t\t jlimit_check && time_after(jiffies, jlimit))) &&\n\t       local_clock() >= tlimit;\n}\n\n \nstatic void rcu_do_batch(struct rcu_data *rdp)\n{\n\tlong bl;\n\tlong count = 0;\n\tint div;\n\tbool __maybe_unused empty;\n\tunsigned long flags;\n\tunsigned long jlimit;\n\tbool jlimit_check = false;\n\tlong pending;\n\tstruct rcu_cblist rcl = RCU_CBLIST_INITIALIZER(rcl);\n\tstruct rcu_head *rhp;\n\tlong tlimit = 0;\n\n\t \n\tif (!rcu_segcblist_ready_cbs(&rdp->cblist)) {\n\t\ttrace_rcu_batch_start(rcu_state.name,\n\t\t\t\t      rcu_segcblist_n_cbs(&rdp->cblist), 0);\n\t\ttrace_rcu_batch_end(rcu_state.name, 0,\n\t\t\t\t    !rcu_segcblist_empty(&rdp->cblist),\n\t\t\t\t    need_resched(), is_idle_task(current),\n\t\t\t\t    rcu_is_callbacks_kthread(rdp));\n\t\treturn;\n\t}\n\n\t \n\trcu_nocb_lock_irqsave(rdp, flags);\n\tWARN_ON_ONCE(cpu_is_offline(smp_processor_id()));\n\tpending = rcu_segcblist_get_seglen(&rdp->cblist, RCU_DONE_TAIL);\n\tdiv = READ_ONCE(rcu_divisor);\n\tdiv = div < 0 ? 7 : div > sizeof(long) * 8 - 2 ? sizeof(long) * 8 - 2 : div;\n\tbl = max(rdp->blimit, pending >> div);\n\tif ((in_serving_softirq() || rdp->rcu_cpu_kthread_status == RCU_KTHREAD_RUNNING) &&\n\t    (IS_ENABLED(CONFIG_RCU_DOUBLE_CHECK_CB_TIME) || unlikely(bl > 100))) {\n\t\tconst long npj = NSEC_PER_SEC / HZ;\n\t\tlong rrn = READ_ONCE(rcu_resched_ns);\n\n\t\trrn = rrn < NSEC_PER_MSEC ? NSEC_PER_MSEC : rrn > NSEC_PER_SEC ? NSEC_PER_SEC : rrn;\n\t\ttlimit = local_clock() + rrn;\n\t\tjlimit = jiffies + (rrn + npj + 1) / npj;\n\t\tjlimit_check = true;\n\t}\n\ttrace_rcu_batch_start(rcu_state.name,\n\t\t\t      rcu_segcblist_n_cbs(&rdp->cblist), bl);\n\trcu_segcblist_extract_done_cbs(&rdp->cblist, &rcl);\n\tif (rcu_rdp_is_offloaded(rdp))\n\t\trdp->qlen_last_fqs_check = rcu_segcblist_n_cbs(&rdp->cblist);\n\n\ttrace_rcu_segcb_stats(&rdp->cblist, TPS(\"SegCbDequeued\"));\n\trcu_nocb_unlock_irqrestore(rdp, flags);\n\n\t \n\ttick_dep_set_task(current, TICK_DEP_BIT_RCU);\n\trhp = rcu_cblist_dequeue(&rcl);\n\n\tfor (; rhp; rhp = rcu_cblist_dequeue(&rcl)) {\n\t\trcu_callback_t f;\n\n\t\tcount++;\n\t\tdebug_rcu_head_unqueue(rhp);\n\n\t\trcu_lock_acquire(&rcu_callback_map);\n\t\ttrace_rcu_invoke_callback(rcu_state.name, rhp);\n\n\t\tf = rhp->func;\n\t\tWRITE_ONCE(rhp->func, (rcu_callback_t)0L);\n\t\tf(rhp);\n\n\t\trcu_lock_release(&rcu_callback_map);\n\n\t\t \n\t\tif (in_serving_softirq()) {\n\t\t\tif (count >= bl && (need_resched() || !is_idle_task(current)))\n\t\t\t\tbreak;\n\t\t\t \n\t\t\tif (rcu_do_batch_check_time(count, tlimit, jlimit_check, jlimit))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\t\n\t\t\t\n\t\t\tlocal_bh_enable();\n\t\t\tlockdep_assert_irqs_enabled();\n\t\t\tcond_resched_tasks_rcu_qs();\n\t\t\tlockdep_assert_irqs_enabled();\n\t\t\tlocal_bh_disable();\n\t\t\t\n\t\t\t\n\t\t\tif (rdp->rcu_cpu_kthread_status == RCU_KTHREAD_RUNNING &&\n\t\t\t    rcu_do_batch_check_time(count, tlimit, jlimit_check, jlimit)) {\n\t\t\t\trdp->rcu_cpu_has_work = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\trcu_nocb_lock_irqsave(rdp, flags);\n\trdp->n_cbs_invoked += count;\n\ttrace_rcu_batch_end(rcu_state.name, count, !!rcl.head, need_resched(),\n\t\t\t    is_idle_task(current), rcu_is_callbacks_kthread(rdp));\n\n\t \n\trcu_segcblist_insert_done_cbs(&rdp->cblist, &rcl);\n\trcu_segcblist_add_len(&rdp->cblist, -count);\n\n\t \n\tcount = rcu_segcblist_n_cbs(&rdp->cblist);\n\tif (rdp->blimit >= DEFAULT_MAX_RCU_BLIMIT && count <= qlowmark)\n\t\trdp->blimit = blimit;\n\n\t \n\tif (count == 0 && rdp->qlen_last_fqs_check != 0) {\n\t\trdp->qlen_last_fqs_check = 0;\n\t\trdp->n_force_qs_snap = READ_ONCE(rcu_state.n_force_qs);\n\t} else if (count < rdp->qlen_last_fqs_check - qhimark)\n\t\trdp->qlen_last_fqs_check = count;\n\n\t \n\tempty = rcu_segcblist_empty(&rdp->cblist);\n\tWARN_ON_ONCE(count == 0 && !empty);\n\tWARN_ON_ONCE(!IS_ENABLED(CONFIG_RCU_NOCB_CPU) &&\n\t\t     count != 0 && empty);\n\tWARN_ON_ONCE(count == 0 && rcu_segcblist_n_segment_cbs(&rdp->cblist) != 0);\n\tWARN_ON_ONCE(!empty && rcu_segcblist_n_segment_cbs(&rdp->cblist) == 0);\n\n\trcu_nocb_unlock_irqrestore(rdp, flags);\n\n\ttick_dep_clear_task(current, TICK_DEP_BIT_RCU);\n}\n\n \nvoid rcu_sched_clock_irq(int user)\n{\n\tunsigned long j;\n\n\tif (IS_ENABLED(CONFIG_PROVE_RCU)) {\n\t\tj = jiffies;\n\t\tWARN_ON_ONCE(time_before(j, __this_cpu_read(rcu_data.last_sched_clock)));\n\t\t__this_cpu_write(rcu_data.last_sched_clock, j);\n\t}\n\ttrace_rcu_utilization(TPS(\"Start scheduler-tick\"));\n\tlockdep_assert_irqs_disabled();\n\traw_cpu_inc(rcu_data.ticks_this_gp);\n\t \n\tif (smp_load_acquire(this_cpu_ptr(&rcu_data.rcu_urgent_qs))) {\n\t\t \n\t\tif (!rcu_is_cpu_rrupt_from_idle() && !user) {\n\t\t\tset_tsk_need_resched(current);\n\t\t\tset_preempt_need_resched();\n\t\t}\n\t\t__this_cpu_write(rcu_data.rcu_urgent_qs, false);\n\t}\n\trcu_flavor_sched_clock_irq(user);\n\tif (rcu_pending(user))\n\t\tinvoke_rcu_core();\n\tif (user || rcu_is_cpu_rrupt_from_idle())\n\t\trcu_note_voluntary_context_switch(current);\n\tlockdep_assert_irqs_disabled();\n\n\ttrace_rcu_utilization(TPS(\"End scheduler-tick\"));\n}\n\n \nstatic void force_qs_rnp(int (*f)(struct rcu_data *rdp))\n{\n\tint cpu;\n\tunsigned long flags;\n\tstruct rcu_node *rnp;\n\n\trcu_state.cbovld = rcu_state.cbovldnext;\n\trcu_state.cbovldnext = false;\n\trcu_for_each_leaf_node(rnp) {\n\t\tunsigned long mask = 0;\n\t\tunsigned long rsmask = 0;\n\n\t\tcond_resched_tasks_rcu_qs();\n\t\traw_spin_lock_irqsave_rcu_node(rnp, flags);\n\t\trcu_state.cbovldnext |= !!rnp->cbovldmask;\n\t\tif (rnp->qsmask == 0) {\n\t\t\tif (rcu_preempt_blocked_readers_cgp(rnp)) {\n\t\t\t\t \n\t\t\t\trcu_initiate_boost(rnp, flags);\n\t\t\t\t \n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n\t\t\tcontinue;\n\t\t}\n\t\tfor_each_leaf_node_cpu_mask(rnp, cpu, rnp->qsmask) {\n\t\t\tstruct rcu_data *rdp;\n\t\t\tint ret;\n\n\t\t\trdp = per_cpu_ptr(&rcu_data, cpu);\n\t\t\tret = f(rdp);\n\t\t\tif (ret > 0) {\n\t\t\t\tmask |= rdp->grpmask;\n\t\t\t\trcu_disable_urgency_upon_qs(rdp);\n\t\t\t}\n\t\t\tif (ret < 0)\n\t\t\t\trsmask |= rdp->grpmask;\n\t\t}\n\t\tif (mask != 0) {\n\t\t\t \n\t\t\trcu_report_qs_rnp(mask, rnp, rnp->gp_seq, flags);\n\t\t} else {\n\t\t\t \n\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n\t\t}\n\n\t\tfor_each_leaf_node_cpu_mask(rnp, cpu, rsmask)\n\t\t\tresched_cpu(cpu);\n\t}\n}\n\n \nvoid rcu_force_quiescent_state(void)\n{\n\tunsigned long flags;\n\tbool ret;\n\tstruct rcu_node *rnp;\n\tstruct rcu_node *rnp_old = NULL;\n\n\t \n\trnp = raw_cpu_read(rcu_data.mynode);\n\tfor (; rnp != NULL; rnp = rnp->parent) {\n\t\tret = (READ_ONCE(rcu_state.gp_flags) & RCU_GP_FLAG_FQS) ||\n\t\t       !raw_spin_trylock(&rnp->fqslock);\n\t\tif (rnp_old != NULL)\n\t\t\traw_spin_unlock(&rnp_old->fqslock);\n\t\tif (ret)\n\t\t\treturn;\n\t\trnp_old = rnp;\n\t}\n\t \n\n\t \n\traw_spin_lock_irqsave_rcu_node(rnp_old, flags);\n\traw_spin_unlock(&rnp_old->fqslock);\n\tif (READ_ONCE(rcu_state.gp_flags) & RCU_GP_FLAG_FQS) {\n\t\traw_spin_unlock_irqrestore_rcu_node(rnp_old, flags);\n\t\treturn;   \n\t}\n\tWRITE_ONCE(rcu_state.gp_flags,\n\t\t   READ_ONCE(rcu_state.gp_flags) | RCU_GP_FLAG_FQS);\n\traw_spin_unlock_irqrestore_rcu_node(rnp_old, flags);\n\trcu_gp_kthread_wake();\n}\nEXPORT_SYMBOL_GPL(rcu_force_quiescent_state);\n\n\n\nstatic void strict_work_handler(struct work_struct *work)\n{\n\trcu_read_lock();\n\trcu_read_unlock();\n}\n\n \nstatic __latent_entropy void rcu_core(void)\n{\n\tunsigned long flags;\n\tstruct rcu_data *rdp = raw_cpu_ptr(&rcu_data);\n\tstruct rcu_node *rnp = rdp->mynode;\n\t \n\tconst bool do_batch = !rcu_segcblist_completely_offloaded(&rdp->cblist);\n\n\tif (cpu_is_offline(smp_processor_id()))\n\t\treturn;\n\ttrace_rcu_utilization(TPS(\"Start RCU core\"));\n\tWARN_ON_ONCE(!rdp->beenonline);\n\n\t \n\tif (IS_ENABLED(CONFIG_PREEMPT_COUNT) && (!(preempt_count() & PREEMPT_MASK))) {\n\t\trcu_preempt_deferred_qs(current);\n\t} else if (rcu_preempt_need_deferred_qs(current)) {\n\t\tset_tsk_need_resched(current);\n\t\tset_preempt_need_resched();\n\t}\n\n\t \n\trcu_check_quiescent_state(rdp);\n\n\t \n\tif (!rcu_gp_in_progress() &&\n\t    rcu_segcblist_is_enabled(&rdp->cblist) && do_batch) {\n\t\trcu_nocb_lock_irqsave(rdp, flags);\n\t\tif (!rcu_segcblist_restempty(&rdp->cblist, RCU_NEXT_READY_TAIL))\n\t\t\trcu_accelerate_cbs_unlocked(rnp, rdp);\n\t\trcu_nocb_unlock_irqrestore(rdp, flags);\n\t}\n\n\trcu_check_gp_start_stall(rnp, rdp, rcu_jiffies_till_stall_check());\n\n\t \n\tif (do_batch && rcu_segcblist_ready_cbs(&rdp->cblist) &&\n\t    likely(READ_ONCE(rcu_scheduler_fully_active))) {\n\t\trcu_do_batch(rdp);\n\t\t \n\t\tif (rcu_segcblist_ready_cbs(&rdp->cblist))\n\t\t\tinvoke_rcu_core();\n\t}\n\n\t \n\tdo_nocb_deferred_wakeup(rdp);\n\ttrace_rcu_utilization(TPS(\"End RCU core\"));\n\n\t\n\tif (IS_ENABLED(CONFIG_RCU_STRICT_GRACE_PERIOD))\n\t\tqueue_work_on(rdp->cpu, rcu_gp_wq, &rdp->strict_work);\n}\n\nstatic void rcu_core_si(struct softirq_action *h)\n{\n\trcu_core();\n}\n\nstatic void rcu_wake_cond(struct task_struct *t, int status)\n{\n\t \n\tif (t && (status != RCU_KTHREAD_YIELDING || is_idle_task(current)))\n\t\twake_up_process(t);\n}\n\nstatic void invoke_rcu_core_kthread(void)\n{\n\tstruct task_struct *t;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\t__this_cpu_write(rcu_data.rcu_cpu_has_work, 1);\n\tt = __this_cpu_read(rcu_data.rcu_cpu_kthread_task);\n\tif (t != NULL && t != current)\n\t\trcu_wake_cond(t, __this_cpu_read(rcu_data.rcu_cpu_kthread_status));\n\tlocal_irq_restore(flags);\n}\n\n \nstatic void invoke_rcu_core(void)\n{\n\tif (!cpu_online(smp_processor_id()))\n\t\treturn;\n\tif (use_softirq)\n\t\traise_softirq(RCU_SOFTIRQ);\n\telse\n\t\tinvoke_rcu_core_kthread();\n}\n\nstatic void rcu_cpu_kthread_park(unsigned int cpu)\n{\n\tper_cpu(rcu_data.rcu_cpu_kthread_status, cpu) = RCU_KTHREAD_OFFCPU;\n}\n\nstatic int rcu_cpu_kthread_should_run(unsigned int cpu)\n{\n\treturn __this_cpu_read(rcu_data.rcu_cpu_has_work);\n}\n\n \nstatic void rcu_cpu_kthread(unsigned int cpu)\n{\n\tunsigned int *statusp = this_cpu_ptr(&rcu_data.rcu_cpu_kthread_status);\n\tchar work, *workp = this_cpu_ptr(&rcu_data.rcu_cpu_has_work);\n\tunsigned long *j = this_cpu_ptr(&rcu_data.rcuc_activity);\n\tint spincnt;\n\n\ttrace_rcu_utilization(TPS(\"Start CPU kthread@rcu_run\"));\n\tfor (spincnt = 0; spincnt < 10; spincnt++) {\n\t\tWRITE_ONCE(*j, jiffies);\n\t\tlocal_bh_disable();\n\t\t*statusp = RCU_KTHREAD_RUNNING;\n\t\tlocal_irq_disable();\n\t\twork = *workp;\n\t\tWRITE_ONCE(*workp, 0);\n\t\tlocal_irq_enable();\n\t\tif (work)\n\t\t\trcu_core();\n\t\tlocal_bh_enable();\n\t\tif (!READ_ONCE(*workp)) {\n\t\t\ttrace_rcu_utilization(TPS(\"End CPU kthread@rcu_wait\"));\n\t\t\t*statusp = RCU_KTHREAD_WAITING;\n\t\t\treturn;\n\t\t}\n\t}\n\t*statusp = RCU_KTHREAD_YIELDING;\n\ttrace_rcu_utilization(TPS(\"Start CPU kthread@rcu_yield\"));\n\tschedule_timeout_idle(2);\n\ttrace_rcu_utilization(TPS(\"End CPU kthread@rcu_yield\"));\n\t*statusp = RCU_KTHREAD_WAITING;\n\tWRITE_ONCE(*j, jiffies);\n}\n\nstatic struct smp_hotplug_thread rcu_cpu_thread_spec = {\n\t.store\t\t\t= &rcu_data.rcu_cpu_kthread_task,\n\t.thread_should_run\t= rcu_cpu_kthread_should_run,\n\t.thread_fn\t\t= rcu_cpu_kthread,\n\t.thread_comm\t\t= \"rcuc/%u\",\n\t.setup\t\t\t= rcu_cpu_kthread_setup,\n\t.park\t\t\t= rcu_cpu_kthread_park,\n};\n\n \nstatic int __init rcu_spawn_core_kthreads(void)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu)\n\t\tper_cpu(rcu_data.rcu_cpu_has_work, cpu) = 0;\n\tif (use_softirq)\n\t\treturn 0;\n\tWARN_ONCE(smpboot_register_percpu_thread(&rcu_cpu_thread_spec),\n\t\t  \"%s: Could not start rcuc kthread, OOM is now expected behavior\\n\", __func__);\n\treturn 0;\n}\n\n \nstatic void __call_rcu_core(struct rcu_data *rdp, struct rcu_head *head,\n\t\t\t    unsigned long flags)\n{\n\t \n\tif (!rcu_is_watching())\n\t\tinvoke_rcu_core();\n\n\t \n\tif (irqs_disabled_flags(flags) || cpu_is_offline(smp_processor_id()))\n\t\treturn;\n\n\t \n\tif (unlikely(rcu_segcblist_n_cbs(&rdp->cblist) >\n\t\t     rdp->qlen_last_fqs_check + qhimark)) {\n\n\t\t \n\t\tnote_gp_changes(rdp);\n\n\t\t \n\t\tif (!rcu_gp_in_progress()) {\n\t\t\trcu_accelerate_cbs_unlocked(rdp->mynode, rdp);\n\t\t} else {\n\t\t\t \n\t\t\trdp->blimit = DEFAULT_MAX_RCU_BLIMIT;\n\t\t\tif (READ_ONCE(rcu_state.n_force_qs) == rdp->n_force_qs_snap &&\n\t\t\t    rcu_segcblist_first_pend_cb(&rdp->cblist) != head)\n\t\t\t\trcu_force_quiescent_state();\n\t\t\trdp->n_force_qs_snap = READ_ONCE(rcu_state.n_force_qs);\n\t\t\trdp->qlen_last_fqs_check = rcu_segcblist_n_cbs(&rdp->cblist);\n\t\t}\n\t}\n}\n\n \nstatic void rcu_leak_callback(struct rcu_head *rhp)\n{\n}\n\n \nstatic void check_cb_ovld_locked(struct rcu_data *rdp, struct rcu_node *rnp)\n{\n\traw_lockdep_assert_held_rcu_node(rnp);\n\tif (qovld_calc <= 0)\n\t\treturn; \n\tif (rcu_segcblist_n_cbs(&rdp->cblist) >= qovld_calc)\n\t\tWRITE_ONCE(rnp->cbovldmask, rnp->cbovldmask | rdp->grpmask);\n\telse\n\t\tWRITE_ONCE(rnp->cbovldmask, rnp->cbovldmask & ~rdp->grpmask);\n}\n\n \nstatic void check_cb_ovld(struct rcu_data *rdp)\n{\n\tstruct rcu_node *const rnp = rdp->mynode;\n\n\tif (qovld_calc <= 0 ||\n\t    ((rcu_segcblist_n_cbs(&rdp->cblist) >= qovld_calc) ==\n\t     !!(READ_ONCE(rnp->cbovldmask) & rdp->grpmask)))\n\t\treturn; \n\traw_spin_lock_rcu_node(rnp);\n\tcheck_cb_ovld_locked(rdp, rnp);\n\traw_spin_unlock_rcu_node(rnp);\n}\n\nstatic void\n__call_rcu_common(struct rcu_head *head, rcu_callback_t func, bool lazy_in)\n{\n\tstatic atomic_t doublefrees;\n\tunsigned long flags;\n\tbool lazy;\n\tstruct rcu_data *rdp;\n\tbool was_alldone;\n\n\t \n\tWARN_ON_ONCE((unsigned long)head & (sizeof(void *) - 1));\n\n\tif (debug_rcu_head_queue(head)) {\n\t\t \n\t\tif (atomic_inc_return(&doublefrees) < 4) {\n\t\t\tpr_err(\"%s(): Double-freed CB %p->%pS()!!!  \", __func__, head, head->func);\n\t\t\tmem_dump_obj(head);\n\t\t}\n\t\tWRITE_ONCE(head->func, rcu_leak_callback);\n\t\treturn;\n\t}\n\thead->func = func;\n\thead->next = NULL;\n\tkasan_record_aux_stack_noalloc(head);\n\tlocal_irq_save(flags);\n\trdp = this_cpu_ptr(&rcu_data);\n\tlazy = lazy_in && !rcu_async_should_hurry();\n\n\t \n\tif (unlikely(!rcu_segcblist_is_enabled(&rdp->cblist))) {\n\t\t\n\t\tWARN_ON_ONCE(rcu_scheduler_active != RCU_SCHEDULER_INACTIVE);\n\t\tWARN_ON_ONCE(!rcu_is_watching());\n\t\t\n\t\t\n\t\tif (rcu_segcblist_empty(&rdp->cblist))\n\t\t\trcu_segcblist_init(&rdp->cblist);\n\t}\n\n\tcheck_cb_ovld(rdp);\n\tif (rcu_nocb_try_bypass(rdp, head, &was_alldone, flags, lazy))\n\t\treturn; \n\t\n\trcu_segcblist_enqueue(&rdp->cblist, head);\n\tif (__is_kvfree_rcu_offset((unsigned long)func))\n\t\ttrace_rcu_kvfree_callback(rcu_state.name, head,\n\t\t\t\t\t (unsigned long)func,\n\t\t\t\t\t rcu_segcblist_n_cbs(&rdp->cblist));\n\telse\n\t\ttrace_rcu_callback(rcu_state.name, head,\n\t\t\t\t   rcu_segcblist_n_cbs(&rdp->cblist));\n\n\ttrace_rcu_segcb_stats(&rdp->cblist, TPS(\"SegCBQueued\"));\n\n\t \n\tif (unlikely(rcu_rdp_is_offloaded(rdp))) {\n\t\t__call_rcu_nocb_wake(rdp, was_alldone, flags);  \n\t} else {\n\t\t__call_rcu_core(rdp, head, flags);\n\t\tlocal_irq_restore(flags);\n\t}\n}\n\n#ifdef CONFIG_RCU_LAZY\n \nvoid call_rcu_hurry(struct rcu_head *head, rcu_callback_t func)\n{\n\treturn __call_rcu_common(head, func, false);\n}\nEXPORT_SYMBOL_GPL(call_rcu_hurry);\n#endif\n\n \nvoid call_rcu(struct rcu_head *head, rcu_callback_t func)\n{\n\treturn __call_rcu_common(head, func, IS_ENABLED(CONFIG_RCU_LAZY));\n}\nEXPORT_SYMBOL_GPL(call_rcu);\n\n \n#define KFREE_DRAIN_JIFFIES (5 * HZ)\n#define KFREE_N_BATCHES 2\n#define FREE_N_CHANNELS 2\n\n \nstruct kvfree_rcu_bulk_data {\n\tstruct list_head list;\n\tstruct rcu_gp_oldstate gp_snap;\n\tunsigned long nr_records;\n\tvoid *records[];\n};\n\n \n#define KVFREE_BULK_MAX_ENTR \\\n\t((PAGE_SIZE - sizeof(struct kvfree_rcu_bulk_data)) / sizeof(void *))\n\n \n\nstruct kfree_rcu_cpu_work {\n\tstruct rcu_work rcu_work;\n\tstruct rcu_head *head_free;\n\tstruct rcu_gp_oldstate head_free_gp_snap;\n\tstruct list_head bulk_head_free[FREE_N_CHANNELS];\n\tstruct kfree_rcu_cpu *krcp;\n};\n\n \nstruct kfree_rcu_cpu {\n\t\n\t\n\tstruct rcu_head *head;\n\tunsigned long head_gp_snap;\n\tatomic_t head_count;\n\n\t\n\tstruct list_head bulk_head[FREE_N_CHANNELS];\n\tatomic_t bulk_count[FREE_N_CHANNELS];\n\n\tstruct kfree_rcu_cpu_work krw_arr[KFREE_N_BATCHES];\n\traw_spinlock_t lock;\n\tstruct delayed_work monitor_work;\n\tbool initialized;\n\n\tstruct delayed_work page_cache_work;\n\tatomic_t backoff_page_cache_fill;\n\tatomic_t work_in_progress;\n\tstruct hrtimer hrtimer;\n\n\tstruct llist_head bkvcache;\n\tint nr_bkv_objs;\n};\n\nstatic DEFINE_PER_CPU(struct kfree_rcu_cpu, krc) = {\n\t.lock = __RAW_SPIN_LOCK_UNLOCKED(krc.lock),\n};\n\nstatic __always_inline void\ndebug_rcu_bhead_unqueue(struct kvfree_rcu_bulk_data *bhead)\n{\n#ifdef CONFIG_DEBUG_OBJECTS_RCU_HEAD\n\tint i;\n\n\tfor (i = 0; i < bhead->nr_records; i++)\n\t\tdebug_rcu_head_unqueue((struct rcu_head *)(bhead->records[i]));\n#endif\n}\n\nstatic inline struct kfree_rcu_cpu *\nkrc_this_cpu_lock(unsigned long *flags)\n{\n\tstruct kfree_rcu_cpu *krcp;\n\n\tlocal_irq_save(*flags);\t\n\tkrcp = this_cpu_ptr(&krc);\n\traw_spin_lock(&krcp->lock);\n\n\treturn krcp;\n}\n\nstatic inline void\nkrc_this_cpu_unlock(struct kfree_rcu_cpu *krcp, unsigned long flags)\n{\n\traw_spin_unlock_irqrestore(&krcp->lock, flags);\n}\n\nstatic inline struct kvfree_rcu_bulk_data *\nget_cached_bnode(struct kfree_rcu_cpu *krcp)\n{\n\tif (!krcp->nr_bkv_objs)\n\t\treturn NULL;\n\n\tWRITE_ONCE(krcp->nr_bkv_objs, krcp->nr_bkv_objs - 1);\n\treturn (struct kvfree_rcu_bulk_data *)\n\t\tllist_del_first(&krcp->bkvcache);\n}\n\nstatic inline bool\nput_cached_bnode(struct kfree_rcu_cpu *krcp,\n\tstruct kvfree_rcu_bulk_data *bnode)\n{\n\t\n\tif (krcp->nr_bkv_objs >= rcu_min_cached_objs)\n\t\treturn false;\n\n\tllist_add((struct llist_node *) bnode, &krcp->bkvcache);\n\tWRITE_ONCE(krcp->nr_bkv_objs, krcp->nr_bkv_objs + 1);\n\treturn true;\n}\n\nstatic int\ndrain_page_cache(struct kfree_rcu_cpu *krcp)\n{\n\tunsigned long flags;\n\tstruct llist_node *page_list, *pos, *n;\n\tint freed = 0;\n\n\tif (!rcu_min_cached_objs)\n\t\treturn 0;\n\n\traw_spin_lock_irqsave(&krcp->lock, flags);\n\tpage_list = llist_del_all(&krcp->bkvcache);\n\tWRITE_ONCE(krcp->nr_bkv_objs, 0);\n\traw_spin_unlock_irqrestore(&krcp->lock, flags);\n\n\tllist_for_each_safe(pos, n, page_list) {\n\t\tfree_page((unsigned long)pos);\n\t\tfreed++;\n\t}\n\n\treturn freed;\n}\n\nstatic void\nkvfree_rcu_bulk(struct kfree_rcu_cpu *krcp,\n\tstruct kvfree_rcu_bulk_data *bnode, int idx)\n{\n\tunsigned long flags;\n\tint i;\n\n\tif (!WARN_ON_ONCE(!poll_state_synchronize_rcu_full(&bnode->gp_snap))) {\n\t\tdebug_rcu_bhead_unqueue(bnode);\n\t\trcu_lock_acquire(&rcu_callback_map);\n\t\tif (idx == 0) { \n\t\t\ttrace_rcu_invoke_kfree_bulk_callback(\n\t\t\t\trcu_state.name, bnode->nr_records,\n\t\t\t\tbnode->records);\n\n\t\t\tkfree_bulk(bnode->nr_records, bnode->records);\n\t\t} else { \n\t\t\tfor (i = 0; i < bnode->nr_records; i++) {\n\t\t\t\ttrace_rcu_invoke_kvfree_callback(\n\t\t\t\t\trcu_state.name, bnode->records[i], 0);\n\n\t\t\t\tvfree(bnode->records[i]);\n\t\t\t}\n\t\t}\n\t\trcu_lock_release(&rcu_callback_map);\n\t}\n\n\traw_spin_lock_irqsave(&krcp->lock, flags);\n\tif (put_cached_bnode(krcp, bnode))\n\t\tbnode = NULL;\n\traw_spin_unlock_irqrestore(&krcp->lock, flags);\n\n\tif (bnode)\n\t\tfree_page((unsigned long) bnode);\n\n\tcond_resched_tasks_rcu_qs();\n}\n\nstatic void\nkvfree_rcu_list(struct rcu_head *head)\n{\n\tstruct rcu_head *next;\n\n\tfor (; head; head = next) {\n\t\tvoid *ptr = (void *) head->func;\n\t\tunsigned long offset = (void *) head - ptr;\n\n\t\tnext = head->next;\n\t\tdebug_rcu_head_unqueue((struct rcu_head *)ptr);\n\t\trcu_lock_acquire(&rcu_callback_map);\n\t\ttrace_rcu_invoke_kvfree_callback(rcu_state.name, head, offset);\n\n\t\tif (!WARN_ON_ONCE(!__is_kvfree_rcu_offset(offset)))\n\t\t\tkvfree(ptr);\n\n\t\trcu_lock_release(&rcu_callback_map);\n\t\tcond_resched_tasks_rcu_qs();\n\t}\n}\n\n \nstatic void kfree_rcu_work(struct work_struct *work)\n{\n\tunsigned long flags;\n\tstruct kvfree_rcu_bulk_data *bnode, *n;\n\tstruct list_head bulk_head[FREE_N_CHANNELS];\n\tstruct rcu_head *head;\n\tstruct kfree_rcu_cpu *krcp;\n\tstruct kfree_rcu_cpu_work *krwp;\n\tstruct rcu_gp_oldstate head_gp_snap;\n\tint i;\n\n\tkrwp = container_of(to_rcu_work(work),\n\t\tstruct kfree_rcu_cpu_work, rcu_work);\n\tkrcp = krwp->krcp;\n\n\traw_spin_lock_irqsave(&krcp->lock, flags);\n\t\n\tfor (i = 0; i < FREE_N_CHANNELS; i++)\n\t\tlist_replace_init(&krwp->bulk_head_free[i], &bulk_head[i]);\n\n\t\n\thead = krwp->head_free;\n\tkrwp->head_free = NULL;\n\thead_gp_snap = krwp->head_free_gp_snap;\n\traw_spin_unlock_irqrestore(&krcp->lock, flags);\n\n\t\n\tfor (i = 0; i < FREE_N_CHANNELS; i++) {\n\t\t\n\t\tlist_for_each_entry_safe(bnode, n, &bulk_head[i], list)\n\t\t\tkvfree_rcu_bulk(krcp, bnode, i);\n\t}\n\n\t \n\tif (head && !WARN_ON_ONCE(!poll_state_synchronize_rcu_full(&head_gp_snap)))\n\t\tkvfree_rcu_list(head);\n}\n\nstatic bool\nneed_offload_krc(struct kfree_rcu_cpu *krcp)\n{\n\tint i;\n\n\tfor (i = 0; i < FREE_N_CHANNELS; i++)\n\t\tif (!list_empty(&krcp->bulk_head[i]))\n\t\t\treturn true;\n\n\treturn !!READ_ONCE(krcp->head);\n}\n\nstatic bool\nneed_wait_for_krwp_work(struct kfree_rcu_cpu_work *krwp)\n{\n\tint i;\n\n\tfor (i = 0; i < FREE_N_CHANNELS; i++)\n\t\tif (!list_empty(&krwp->bulk_head_free[i]))\n\t\t\treturn true;\n\n\treturn !!krwp->head_free;\n}\n\nstatic int krc_count(struct kfree_rcu_cpu *krcp)\n{\n\tint sum = atomic_read(&krcp->head_count);\n\tint i;\n\n\tfor (i = 0; i < FREE_N_CHANNELS; i++)\n\t\tsum += atomic_read(&krcp->bulk_count[i]);\n\n\treturn sum;\n}\n\nstatic void\nschedule_delayed_monitor_work(struct kfree_rcu_cpu *krcp)\n{\n\tlong delay, delay_left;\n\n\tdelay = krc_count(krcp) >= KVFREE_BULK_MAX_ENTR ? 1:KFREE_DRAIN_JIFFIES;\n\tif (delayed_work_pending(&krcp->monitor_work)) {\n\t\tdelay_left = krcp->monitor_work.timer.expires - jiffies;\n\t\tif (delay < delay_left)\n\t\t\tmod_delayed_work(system_wq, &krcp->monitor_work, delay);\n\t\treturn;\n\t}\n\tqueue_delayed_work(system_wq, &krcp->monitor_work, delay);\n}\n\nstatic void\nkvfree_rcu_drain_ready(struct kfree_rcu_cpu *krcp)\n{\n\tstruct list_head bulk_ready[FREE_N_CHANNELS];\n\tstruct kvfree_rcu_bulk_data *bnode, *n;\n\tstruct rcu_head *head_ready = NULL;\n\tunsigned long flags;\n\tint i;\n\n\traw_spin_lock_irqsave(&krcp->lock, flags);\n\tfor (i = 0; i < FREE_N_CHANNELS; i++) {\n\t\tINIT_LIST_HEAD(&bulk_ready[i]);\n\n\t\tlist_for_each_entry_safe_reverse(bnode, n, &krcp->bulk_head[i], list) {\n\t\t\tif (!poll_state_synchronize_rcu_full(&bnode->gp_snap))\n\t\t\t\tbreak;\n\n\t\t\tatomic_sub(bnode->nr_records, &krcp->bulk_count[i]);\n\t\t\tlist_move(&bnode->list, &bulk_ready[i]);\n\t\t}\n\t}\n\n\tif (krcp->head && poll_state_synchronize_rcu(krcp->head_gp_snap)) {\n\t\thead_ready = krcp->head;\n\t\tatomic_set(&krcp->head_count, 0);\n\t\tWRITE_ONCE(krcp->head, NULL);\n\t}\n\traw_spin_unlock_irqrestore(&krcp->lock, flags);\n\n\tfor (i = 0; i < FREE_N_CHANNELS; i++) {\n\t\tlist_for_each_entry_safe(bnode, n, &bulk_ready[i], list)\n\t\t\tkvfree_rcu_bulk(krcp, bnode, i);\n\t}\n\n\tif (head_ready)\n\t\tkvfree_rcu_list(head_ready);\n}\n\n \nstatic void kfree_rcu_monitor(struct work_struct *work)\n{\n\tstruct kfree_rcu_cpu *krcp = container_of(work,\n\t\tstruct kfree_rcu_cpu, monitor_work.work);\n\tunsigned long flags;\n\tint i, j;\n\n\t\n\tkvfree_rcu_drain_ready(krcp);\n\n\traw_spin_lock_irqsave(&krcp->lock, flags);\n\n\t\n\tfor (i = 0; i < KFREE_N_BATCHES; i++) {\n\t\tstruct kfree_rcu_cpu_work *krwp = &(krcp->krw_arr[i]);\n\n\t\t\n\t\t\n\t\t\n\t\tif (need_wait_for_krwp_work(krwp))\n\t\t\tcontinue;\n\n\t\t\n\t\tif (need_offload_krc(krcp)) {\n\t\t\t\n\t\t\t\n\t\t\tfor (j = 0; j < FREE_N_CHANNELS; j++) {\n\t\t\t\tif (list_empty(&krwp->bulk_head_free[j])) {\n\t\t\t\t\tatomic_set(&krcp->bulk_count[j], 0);\n\t\t\t\t\tlist_replace_init(&krcp->bulk_head[j],\n\t\t\t\t\t\t&krwp->bulk_head_free[j]);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t\n\t\t\t\n\t\t\tif (!krwp->head_free) {\n\t\t\t\tkrwp->head_free = krcp->head;\n\t\t\t\tget_state_synchronize_rcu_full(&krwp->head_free_gp_snap);\n\t\t\t\tatomic_set(&krcp->head_count, 0);\n\t\t\t\tWRITE_ONCE(krcp->head, NULL);\n\t\t\t}\n\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\tqueue_rcu_work(system_wq, &krwp->rcu_work);\n\t\t}\n\t}\n\n\traw_spin_unlock_irqrestore(&krcp->lock, flags);\n\n\t\n\t\n\t\n\t\n\t\n\tif (need_offload_krc(krcp))\n\t\tschedule_delayed_monitor_work(krcp);\n}\n\nstatic enum hrtimer_restart\nschedule_page_work_fn(struct hrtimer *t)\n{\n\tstruct kfree_rcu_cpu *krcp =\n\t\tcontainer_of(t, struct kfree_rcu_cpu, hrtimer);\n\n\tqueue_delayed_work(system_highpri_wq, &krcp->page_cache_work, 0);\n\treturn HRTIMER_NORESTART;\n}\n\nstatic void fill_page_cache_func(struct work_struct *work)\n{\n\tstruct kvfree_rcu_bulk_data *bnode;\n\tstruct kfree_rcu_cpu *krcp =\n\t\tcontainer_of(work, struct kfree_rcu_cpu,\n\t\t\tpage_cache_work.work);\n\tunsigned long flags;\n\tint nr_pages;\n\tbool pushed;\n\tint i;\n\n\tnr_pages = atomic_read(&krcp->backoff_page_cache_fill) ?\n\t\t1 : rcu_min_cached_objs;\n\n\tfor (i = READ_ONCE(krcp->nr_bkv_objs); i < nr_pages; i++) {\n\t\tbnode = (struct kvfree_rcu_bulk_data *)\n\t\t\t__get_free_page(GFP_KERNEL | __GFP_NORETRY | __GFP_NOMEMALLOC | __GFP_NOWARN);\n\n\t\tif (!bnode)\n\t\t\tbreak;\n\n\t\traw_spin_lock_irqsave(&krcp->lock, flags);\n\t\tpushed = put_cached_bnode(krcp, bnode);\n\t\traw_spin_unlock_irqrestore(&krcp->lock, flags);\n\n\t\tif (!pushed) {\n\t\t\tfree_page((unsigned long) bnode);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tatomic_set(&krcp->work_in_progress, 0);\n\tatomic_set(&krcp->backoff_page_cache_fill, 0);\n}\n\nstatic void\nrun_page_cache_worker(struct kfree_rcu_cpu *krcp)\n{\n\t\n\tif (!rcu_min_cached_objs)\n\t\treturn;\n\n\tif (rcu_scheduler_active == RCU_SCHEDULER_RUNNING &&\n\t\t\t!atomic_xchg(&krcp->work_in_progress, 1)) {\n\t\tif (atomic_read(&krcp->backoff_page_cache_fill)) {\n\t\t\tqueue_delayed_work(system_wq,\n\t\t\t\t&krcp->page_cache_work,\n\t\t\t\t\tmsecs_to_jiffies(rcu_delay_page_cache_fill_msec));\n\t\t} else {\n\t\t\thrtimer_init(&krcp->hrtimer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);\n\t\t\tkrcp->hrtimer.function = schedule_page_work_fn;\n\t\t\thrtimer_start(&krcp->hrtimer, 0, HRTIMER_MODE_REL);\n\t\t}\n\t}\n}\n\n\n\n\n\n\n\nstatic inline bool\nadd_ptr_to_bulk_krc_lock(struct kfree_rcu_cpu **krcp,\n\tunsigned long *flags, void *ptr, bool can_alloc)\n{\n\tstruct kvfree_rcu_bulk_data *bnode;\n\tint idx;\n\n\t*krcp = krc_this_cpu_lock(flags);\n\tif (unlikely(!(*krcp)->initialized))\n\t\treturn false;\n\n\tidx = !!is_vmalloc_addr(ptr);\n\tbnode = list_first_entry_or_null(&(*krcp)->bulk_head[idx],\n\t\tstruct kvfree_rcu_bulk_data, list);\n\n\t \n\tif (!bnode || bnode->nr_records == KVFREE_BULK_MAX_ENTR) {\n\t\tbnode = get_cached_bnode(*krcp);\n\t\tif (!bnode && can_alloc) {\n\t\t\tkrc_this_cpu_unlock(*krcp, *flags);\n\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\tbnode = (struct kvfree_rcu_bulk_data *)\n\t\t\t\t__get_free_page(GFP_KERNEL | __GFP_NORETRY | __GFP_NOMEMALLOC | __GFP_NOWARN);\n\t\t\traw_spin_lock_irqsave(&(*krcp)->lock, *flags);\n\t\t}\n\n\t\tif (!bnode)\n\t\t\treturn false;\n\n\t\t\n\t\tbnode->nr_records = 0;\n\t\tlist_add(&bnode->list, &(*krcp)->bulk_head[idx]);\n\t}\n\n\t\n\tbnode->records[bnode->nr_records++] = ptr;\n\tget_state_synchronize_rcu_full(&bnode->gp_snap);\n\tatomic_inc(&(*krcp)->bulk_count[idx]);\n\n\treturn true;\n}\n\n \nvoid kvfree_call_rcu(struct rcu_head *head, void *ptr)\n{\n\tunsigned long flags;\n\tstruct kfree_rcu_cpu *krcp;\n\tbool success;\n\n\t \n\tif (!head)\n\t\tmight_sleep();\n\n\t \n\tif (debug_rcu_head_queue(ptr)) {\n\t\t\n\t\tWARN_ONCE(1, \"%s(): Double-freed call. rcu_head %p\\n\",\n\t\t\t  __func__, head);\n\n\t\t\n\t\treturn;\n\t}\n\n\tkasan_record_aux_stack_noalloc(ptr);\n\tsuccess = add_ptr_to_bulk_krc_lock(&krcp, &flags, ptr, !head);\n\tif (!success) {\n\t\trun_page_cache_worker(krcp);\n\n\t\tif (head == NULL)\n\t\t\t\n\t\t\tgoto unlock_return;\n\n\t\thead->func = ptr;\n\t\thead->next = krcp->head;\n\t\tWRITE_ONCE(krcp->head, head);\n\t\tatomic_inc(&krcp->head_count);\n\n\t\t\n\t\tkrcp->head_gp_snap = get_state_synchronize_rcu();\n\t\tsuccess = true;\n\t}\n\n\t \n\tkmemleak_ignore(ptr);\n\n\t\n\tif (rcu_scheduler_active == RCU_SCHEDULER_RUNNING)\n\t\tschedule_delayed_monitor_work(krcp);\n\nunlock_return:\n\tkrc_this_cpu_unlock(krcp, flags);\n\n\t \n\tif (!success) {\n\t\tdebug_rcu_head_unqueue((struct rcu_head *) ptr);\n\t\tsynchronize_rcu();\n\t\tkvfree(ptr);\n\t}\n}\nEXPORT_SYMBOL_GPL(kvfree_call_rcu);\n\nstatic unsigned long\nkfree_rcu_shrink_count(struct shrinker *shrink, struct shrink_control *sc)\n{\n\tint cpu;\n\tunsigned long count = 0;\n\n\t \n\tfor_each_possible_cpu(cpu) {\n\t\tstruct kfree_rcu_cpu *krcp = per_cpu_ptr(&krc, cpu);\n\n\t\tcount += krc_count(krcp);\n\t\tcount += READ_ONCE(krcp->nr_bkv_objs);\n\t\tatomic_set(&krcp->backoff_page_cache_fill, 1);\n\t}\n\n\treturn count == 0 ? SHRINK_EMPTY : count;\n}\n\nstatic unsigned long\nkfree_rcu_shrink_scan(struct shrinker *shrink, struct shrink_control *sc)\n{\n\tint cpu, freed = 0;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tint count;\n\t\tstruct kfree_rcu_cpu *krcp = per_cpu_ptr(&krc, cpu);\n\n\t\tcount = krc_count(krcp);\n\t\tcount += drain_page_cache(krcp);\n\t\tkfree_rcu_monitor(&krcp->monitor_work.work);\n\n\t\tsc->nr_to_scan -= count;\n\t\tfreed += count;\n\n\t\tif (sc->nr_to_scan <= 0)\n\t\t\tbreak;\n\t}\n\n\treturn freed == 0 ? SHRINK_STOP : freed;\n}\n\nstatic struct shrinker kfree_rcu_shrinker = {\n\t.count_objects = kfree_rcu_shrink_count,\n\t.scan_objects = kfree_rcu_shrink_scan,\n\t.batch = 0,\n\t.seeks = DEFAULT_SEEKS,\n};\n\nvoid __init kfree_rcu_scheduler_running(void)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct kfree_rcu_cpu *krcp = per_cpu_ptr(&krc, cpu);\n\n\t\tif (need_offload_krc(krcp))\n\t\t\tschedule_delayed_monitor_work(krcp);\n\t}\n}\n\n \nstatic int rcu_blocking_is_gp(void)\n{\n\tif (rcu_scheduler_active != RCU_SCHEDULER_INACTIVE) {\n\t\tmight_sleep();\n\t\treturn false;\n\t}\n\treturn true;\n}\n\n \nvoid synchronize_rcu(void)\n{\n\tunsigned long flags;\n\tstruct rcu_node *rnp;\n\n\tRCU_LOCKDEP_WARN(lock_is_held(&rcu_bh_lock_map) ||\n\t\t\t lock_is_held(&rcu_lock_map) ||\n\t\t\t lock_is_held(&rcu_sched_lock_map),\n\t\t\t \"Illegal synchronize_rcu() in RCU read-side critical section\");\n\tif (!rcu_blocking_is_gp()) {\n\t\tif (rcu_gp_is_expedited())\n\t\t\tsynchronize_rcu_expedited();\n\t\telse\n\t\t\twait_rcu_gp(call_rcu_hurry);\n\t\treturn;\n\t}\n\n\t\n\t\n\t\n\t\n\t\n\t\n\trcu_poll_gp_seq_start_unlocked(&rcu_state.gp_seq_polled_snap);\n\trcu_poll_gp_seq_end_unlocked(&rcu_state.gp_seq_polled_snap);\n\n\t\n\t\n\t\n\t\n\tlocal_irq_save(flags);\n\tWARN_ON_ONCE(num_online_cpus() > 1);\n\trcu_state.gp_seq += (1 << RCU_SEQ_CTR_SHIFT);\n\tfor (rnp = this_cpu_ptr(&rcu_data)->mynode; rnp; rnp = rnp->parent)\n\t\trnp->gp_seq_needed = rnp->gp_seq = rcu_state.gp_seq;\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL_GPL(synchronize_rcu);\n\n \nvoid get_completed_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp)\n{\n\trgosp->rgos_norm = RCU_GET_STATE_COMPLETED;\n\trgosp->rgos_exp = RCU_GET_STATE_COMPLETED;\n}\nEXPORT_SYMBOL_GPL(get_completed_synchronize_rcu_full);\n\n \nunsigned long get_state_synchronize_rcu(void)\n{\n\t \n\tsmp_mb();   \n\treturn rcu_seq_snap(&rcu_state.gp_seq_polled);\n}\nEXPORT_SYMBOL_GPL(get_state_synchronize_rcu);\n\n \nvoid get_state_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp)\n{\n\tstruct rcu_node *rnp = rcu_get_root();\n\n\t \n\tsmp_mb();   \n\trgosp->rgos_norm = rcu_seq_snap(&rnp->gp_seq);\n\trgosp->rgos_exp = rcu_seq_snap(&rcu_state.expedited_sequence);\n}\nEXPORT_SYMBOL_GPL(get_state_synchronize_rcu_full);\n\n \nstatic void start_poll_synchronize_rcu_common(void)\n{\n\tunsigned long flags;\n\tbool needwake;\n\tstruct rcu_data *rdp;\n\tstruct rcu_node *rnp;\n\n\tlockdep_assert_irqs_enabled();\n\tlocal_irq_save(flags);\n\trdp = this_cpu_ptr(&rcu_data);\n\trnp = rdp->mynode;\n\traw_spin_lock_rcu_node(rnp); \n\t\n\t\n\t\n\t\n\t\n\t\n\tneedwake = rcu_start_this_gp(rnp, rdp, rcu_seq_snap(&rcu_state.gp_seq));\n\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n\tif (needwake)\n\t\trcu_gp_kthread_wake();\n}\n\n \nunsigned long start_poll_synchronize_rcu(void)\n{\n\tunsigned long gp_seq = get_state_synchronize_rcu();\n\n\tstart_poll_synchronize_rcu_common();\n\treturn gp_seq;\n}\nEXPORT_SYMBOL_GPL(start_poll_synchronize_rcu);\n\n \nvoid start_poll_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp)\n{\n\tget_state_synchronize_rcu_full(rgosp);\n\n\tstart_poll_synchronize_rcu_common();\n}\nEXPORT_SYMBOL_GPL(start_poll_synchronize_rcu_full);\n\n \nbool poll_state_synchronize_rcu(unsigned long oldstate)\n{\n\tif (oldstate == RCU_GET_STATE_COMPLETED ||\n\t    rcu_seq_done_exact(&rcu_state.gp_seq_polled, oldstate)) {\n\t\tsmp_mb();  \n\t\treturn true;\n\t}\n\treturn false;\n}\nEXPORT_SYMBOL_GPL(poll_state_synchronize_rcu);\n\n \nbool poll_state_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp)\n{\n\tstruct rcu_node *rnp = rcu_get_root();\n\n\tsmp_mb(); \n\tif (rgosp->rgos_norm == RCU_GET_STATE_COMPLETED ||\n\t    rcu_seq_done_exact(&rnp->gp_seq, rgosp->rgos_norm) ||\n\t    rgosp->rgos_exp == RCU_GET_STATE_COMPLETED ||\n\t    rcu_seq_done_exact(&rcu_state.expedited_sequence, rgosp->rgos_exp)) {\n\t\tsmp_mb();  \n\t\treturn true;\n\t}\n\treturn false;\n}\nEXPORT_SYMBOL_GPL(poll_state_synchronize_rcu_full);\n\n \nvoid cond_synchronize_rcu(unsigned long oldstate)\n{\n\tif (!poll_state_synchronize_rcu(oldstate))\n\t\tsynchronize_rcu();\n}\nEXPORT_SYMBOL_GPL(cond_synchronize_rcu);\n\n \nvoid cond_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp)\n{\n\tif (!poll_state_synchronize_rcu_full(rgosp))\n\t\tsynchronize_rcu();\n}\nEXPORT_SYMBOL_GPL(cond_synchronize_rcu_full);\n\n \nstatic int rcu_pending(int user)\n{\n\tbool gp_in_progress;\n\tstruct rcu_data *rdp = this_cpu_ptr(&rcu_data);\n\tstruct rcu_node *rnp = rdp->mynode;\n\n\tlockdep_assert_irqs_disabled();\n\n\t \n\tcheck_cpu_stall(rdp);\n\n\t \n\tif (rcu_nocb_need_deferred_wakeup(rdp, RCU_NOCB_WAKE))\n\t\treturn 1;\n\n\t \n\tif ((user || rcu_is_cpu_rrupt_from_idle()) && rcu_nohz_full_cpu())\n\t\treturn 0;\n\n\t \n\tgp_in_progress = rcu_gp_in_progress();\n\tif (rdp->core_needs_qs && !rdp->cpu_no_qs.b.norm && gp_in_progress)\n\t\treturn 1;\n\n\t \n\tif (!rcu_rdp_is_offloaded(rdp) &&\n\t    rcu_segcblist_ready_cbs(&rdp->cblist))\n\t\treturn 1;\n\n\t \n\tif (!gp_in_progress && rcu_segcblist_is_enabled(&rdp->cblist) &&\n\t    !rcu_rdp_is_offloaded(rdp) &&\n\t    !rcu_segcblist_restempty(&rdp->cblist, RCU_NEXT_READY_TAIL))\n\t\treturn 1;\n\n\t \n\tif (rcu_seq_current(&rnp->gp_seq) != rdp->gp_seq ||\n\t    unlikely(READ_ONCE(rdp->gpwrap)))  \n\t\treturn 1;\n\n\t \n\treturn 0;\n}\n\n \nstatic void rcu_barrier_trace(const char *s, int cpu, unsigned long done)\n{\n\ttrace_rcu_barrier(rcu_state.name, s, cpu,\n\t\t\t  atomic_read(&rcu_state.barrier_cpu_count), done);\n}\n\n \nstatic void rcu_barrier_callback(struct rcu_head *rhp)\n{\n\tunsigned long __maybe_unused s = rcu_state.barrier_sequence;\n\n\tif (atomic_dec_and_test(&rcu_state.barrier_cpu_count)) {\n\t\trcu_barrier_trace(TPS(\"LastCB\"), -1, s);\n\t\tcomplete(&rcu_state.barrier_completion);\n\t} else {\n\t\trcu_barrier_trace(TPS(\"CB\"), -1, s);\n\t}\n}\n\n \nstatic void rcu_barrier_entrain(struct rcu_data *rdp)\n{\n\tunsigned long gseq = READ_ONCE(rcu_state.barrier_sequence);\n\tunsigned long lseq = READ_ONCE(rdp->barrier_seq_snap);\n\tbool wake_nocb = false;\n\tbool was_alldone = false;\n\n\tlockdep_assert_held(&rcu_state.barrier_lock);\n\tif (rcu_seq_state(lseq) || !rcu_seq_state(gseq) || rcu_seq_ctr(lseq) != rcu_seq_ctr(gseq))\n\t\treturn;\n\trcu_barrier_trace(TPS(\"IRQ\"), -1, rcu_state.barrier_sequence);\n\trdp->barrier_head.func = rcu_barrier_callback;\n\tdebug_rcu_head_queue(&rdp->barrier_head);\n\trcu_nocb_lock(rdp);\n\t \n\twas_alldone = rcu_rdp_is_offloaded(rdp) && !rcu_segcblist_pend_cbs(&rdp->cblist);\n\tWARN_ON_ONCE(!rcu_nocb_flush_bypass(rdp, NULL, jiffies, false));\n\twake_nocb = was_alldone && rcu_segcblist_pend_cbs(&rdp->cblist);\n\tif (rcu_segcblist_entrain(&rdp->cblist, &rdp->barrier_head)) {\n\t\tatomic_inc(&rcu_state.barrier_cpu_count);\n\t} else {\n\t\tdebug_rcu_head_unqueue(&rdp->barrier_head);\n\t\trcu_barrier_trace(TPS(\"IRQNQ\"), -1, rcu_state.barrier_sequence);\n\t}\n\trcu_nocb_unlock(rdp);\n\tif (wake_nocb)\n\t\twake_nocb_gp(rdp, false);\n\tsmp_store_release(&rdp->barrier_seq_snap, gseq);\n}\n\n \nstatic void rcu_barrier_handler(void *cpu_in)\n{\n\tuintptr_t cpu = (uintptr_t)cpu_in;\n\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);\n\n\tlockdep_assert_irqs_disabled();\n\tWARN_ON_ONCE(cpu != rdp->cpu);\n\tWARN_ON_ONCE(cpu != smp_processor_id());\n\traw_spin_lock(&rcu_state.barrier_lock);\n\trcu_barrier_entrain(rdp);\n\traw_spin_unlock(&rcu_state.barrier_lock);\n}\n\n \nvoid rcu_barrier(void)\n{\n\tuintptr_t cpu;\n\tunsigned long flags;\n\tunsigned long gseq;\n\tstruct rcu_data *rdp;\n\tunsigned long s = rcu_seq_snap(&rcu_state.barrier_sequence);\n\n\trcu_barrier_trace(TPS(\"Begin\"), -1, s);\n\n\t \n\tmutex_lock(&rcu_state.barrier_mutex);\n\n\t \n\tif (rcu_seq_done(&rcu_state.barrier_sequence, s)) {\n\t\trcu_barrier_trace(TPS(\"EarlyExit\"), -1, rcu_state.barrier_sequence);\n\t\tsmp_mb();  \n\t\tmutex_unlock(&rcu_state.barrier_mutex);\n\t\treturn;\n\t}\n\n\t \n\traw_spin_lock_irqsave(&rcu_state.barrier_lock, flags);\n\trcu_seq_start(&rcu_state.barrier_sequence);\n\tgseq = rcu_state.barrier_sequence;\n\trcu_barrier_trace(TPS(\"Inc1\"), -1, rcu_state.barrier_sequence);\n\n\t \n\tinit_completion(&rcu_state.barrier_completion);\n\tatomic_set(&rcu_state.barrier_cpu_count, 2);\n\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);\n\n\t \n\tfor_each_possible_cpu(cpu) {\n\t\trdp = per_cpu_ptr(&rcu_data, cpu);\nretry:\n\t\tif (smp_load_acquire(&rdp->barrier_seq_snap) == gseq)\n\t\t\tcontinue;\n\t\traw_spin_lock_irqsave(&rcu_state.barrier_lock, flags);\n\t\tif (!rcu_segcblist_n_cbs(&rdp->cblist)) {\n\t\t\tWRITE_ONCE(rdp->barrier_seq_snap, gseq);\n\t\t\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);\n\t\t\trcu_barrier_trace(TPS(\"NQ\"), cpu, rcu_state.barrier_sequence);\n\t\t\tcontinue;\n\t\t}\n\t\tif (!rcu_rdp_cpu_online(rdp)) {\n\t\t\trcu_barrier_entrain(rdp);\n\t\t\tWARN_ON_ONCE(READ_ONCE(rdp->barrier_seq_snap) != gseq);\n\t\t\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);\n\t\t\trcu_barrier_trace(TPS(\"OfflineNoCBQ\"), cpu, rcu_state.barrier_sequence);\n\t\t\tcontinue;\n\t\t}\n\t\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);\n\t\tif (smp_call_function_single(cpu, rcu_barrier_handler, (void *)cpu, 1)) {\n\t\t\tschedule_timeout_uninterruptible(1);\n\t\t\tgoto retry;\n\t\t}\n\t\tWARN_ON_ONCE(READ_ONCE(rdp->barrier_seq_snap) != gseq);\n\t\trcu_barrier_trace(TPS(\"OnlineQ\"), cpu, rcu_state.barrier_sequence);\n\t}\n\n\t \n\tif (atomic_sub_and_test(2, &rcu_state.barrier_cpu_count))\n\t\tcomplete(&rcu_state.barrier_completion);\n\n\t \n\twait_for_completion(&rcu_state.barrier_completion);\n\n\t \n\trcu_barrier_trace(TPS(\"Inc2\"), -1, rcu_state.barrier_sequence);\n\trcu_seq_end(&rcu_state.barrier_sequence);\n\tgseq = rcu_state.barrier_sequence;\n\tfor_each_possible_cpu(cpu) {\n\t\trdp = per_cpu_ptr(&rcu_data, cpu);\n\n\t\tWRITE_ONCE(rdp->barrier_seq_snap, gseq);\n\t}\n\n\t \n\tmutex_unlock(&rcu_state.barrier_mutex);\n}\nEXPORT_SYMBOL_GPL(rcu_barrier);\n\n \nstatic unsigned long rcu_rnp_online_cpus(struct rcu_node *rnp)\n{\n\treturn READ_ONCE(rnp->qsmaskinitnext);\n}\n\n \nstatic bool rcu_rdp_cpu_online(struct rcu_data *rdp)\n{\n\treturn !!(rdp->grpmask & rcu_rnp_online_cpus(rdp->mynode));\n}\n\nbool rcu_cpu_online(int cpu)\n{\n\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);\n\n\treturn rcu_rdp_cpu_online(rdp);\n}\n\n#if defined(CONFIG_PROVE_RCU) && defined(CONFIG_HOTPLUG_CPU)\n\n \nbool rcu_lockdep_current_cpu_online(void)\n{\n\tstruct rcu_data *rdp;\n\tbool ret = false;\n\n\tif (in_nmi() || !rcu_scheduler_fully_active)\n\t\treturn true;\n\tpreempt_disable_notrace();\n\trdp = this_cpu_ptr(&rcu_data);\n\t \n\tif (rcu_rdp_cpu_online(rdp) || arch_spin_is_locked(&rcu_state.ofl_lock))\n\t\tret = true;\n\tpreempt_enable_notrace();\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(rcu_lockdep_current_cpu_online);\n\n#endif  \n\n \n \nstatic bool rcu_init_invoked(void)\n{\n\treturn !!rcu_state.n_online_cpus;\n}\n\n \nint rcutree_dying_cpu(unsigned int cpu)\n{\n\tbool blkd;\n\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);\n\tstruct rcu_node *rnp = rdp->mynode;\n\n\tif (!IS_ENABLED(CONFIG_HOTPLUG_CPU))\n\t\treturn 0;\n\n\tblkd = !!(READ_ONCE(rnp->qsmask) & rdp->grpmask);\n\ttrace_rcu_grace_period(rcu_state.name, READ_ONCE(rnp->gp_seq),\n\t\t\t       blkd ? TPS(\"cpuofl-bgp\") : TPS(\"cpuofl\"));\n\treturn 0;\n}\n\n \nstatic void rcu_cleanup_dead_rnp(struct rcu_node *rnp_leaf)\n{\n\tlong mask;\n\tstruct rcu_node *rnp = rnp_leaf;\n\n\traw_lockdep_assert_held_rcu_node(rnp_leaf);\n\tif (!IS_ENABLED(CONFIG_HOTPLUG_CPU) ||\n\t    WARN_ON_ONCE(rnp_leaf->qsmaskinit) ||\n\t    WARN_ON_ONCE(rcu_preempt_has_tasks(rnp_leaf)))\n\t\treturn;\n\tfor (;;) {\n\t\tmask = rnp->grpmask;\n\t\trnp = rnp->parent;\n\t\tif (!rnp)\n\t\t\tbreak;\n\t\traw_spin_lock_rcu_node(rnp);  \n\t\trnp->qsmaskinit &= ~mask;\n\t\t \n\t\tWARN_ON_ONCE(rnp->qsmask);\n\t\tif (rnp->qsmaskinit) {\n\t\t\traw_spin_unlock_rcu_node(rnp);\n\t\t\t \n\t\t\treturn;\n\t\t}\n\t\traw_spin_unlock_rcu_node(rnp);  \n\t}\n}\n\n \nint rcutree_dead_cpu(unsigned int cpu)\n{\n\tif (!IS_ENABLED(CONFIG_HOTPLUG_CPU))\n\t\treturn 0;\n\n\tWRITE_ONCE(rcu_state.n_online_cpus, rcu_state.n_online_cpus - 1);\n\t\n\ttick_dep_clear(TICK_DEP_BIT_RCU);\n\treturn 0;\n}\n\n \nstatic void rcu_init_new_rnp(struct rcu_node *rnp_leaf)\n{\n\tlong mask;\n\tlong oldmask;\n\tstruct rcu_node *rnp = rnp_leaf;\n\n\traw_lockdep_assert_held_rcu_node(rnp_leaf);\n\tWARN_ON_ONCE(rnp->wait_blkd_tasks);\n\tfor (;;) {\n\t\tmask = rnp->grpmask;\n\t\trnp = rnp->parent;\n\t\tif (rnp == NULL)\n\t\t\treturn;\n\t\traw_spin_lock_rcu_node(rnp);  \n\t\toldmask = rnp->qsmaskinit;\n\t\trnp->qsmaskinit |= mask;\n\t\traw_spin_unlock_rcu_node(rnp);  \n\t\tif (oldmask)\n\t\t\treturn;\n\t}\n}\n\n \nstatic void __init\nrcu_boot_init_percpu_data(int cpu)\n{\n\tstruct context_tracking *ct = this_cpu_ptr(&context_tracking);\n\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);\n\n\t \n\trdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);\n\tINIT_WORK(&rdp->strict_work, strict_work_handler);\n\tWARN_ON_ONCE(ct->dynticks_nesting != 1);\n\tWARN_ON_ONCE(rcu_dynticks_in_eqs(rcu_dynticks_snap(cpu)));\n\trdp->barrier_seq_snap = rcu_state.barrier_sequence;\n\trdp->rcu_ofl_gp_seq = rcu_state.gp_seq;\n\trdp->rcu_ofl_gp_flags = RCU_GP_CLEANED;\n\trdp->rcu_onl_gp_seq = rcu_state.gp_seq;\n\trdp->rcu_onl_gp_flags = RCU_GP_CLEANED;\n\trdp->last_sched_clock = jiffies;\n\trdp->cpu = cpu;\n\trcu_boot_init_nocb_percpu_data(rdp);\n}\n\n \nint rcutree_prepare_cpu(unsigned int cpu)\n{\n\tunsigned long flags;\n\tstruct context_tracking *ct = per_cpu_ptr(&context_tracking, cpu);\n\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);\n\tstruct rcu_node *rnp = rcu_get_root();\n\n\t \n\traw_spin_lock_irqsave_rcu_node(rnp, flags);\n\trdp->qlen_last_fqs_check = 0;\n\trdp->n_force_qs_snap = READ_ONCE(rcu_state.n_force_qs);\n\trdp->blimit = blimit;\n\tct->dynticks_nesting = 1;\t \n\traw_spin_unlock_rcu_node(rnp);\t\t \n\n\t \n\tif (!rcu_segcblist_is_enabled(&rdp->cblist))\n\t\trcu_segcblist_init(&rdp->cblist);   \n\n\t \n\trnp = rdp->mynode;\n\traw_spin_lock_rcu_node(rnp);\t\t \n\trdp->gp_seq = READ_ONCE(rnp->gp_seq);\n\trdp->gp_seq_needed = rdp->gp_seq;\n\trdp->cpu_no_qs.b.norm = true;\n\trdp->core_needs_qs = false;\n\trdp->rcu_iw_pending = false;\n\trdp->rcu_iw = IRQ_WORK_INIT_HARD(rcu_iw_handler);\n\trdp->rcu_iw_gp_seq = rdp->gp_seq - 1;\n\ttrace_rcu_grace_period(rcu_state.name, rdp->gp_seq, TPS(\"cpuonl\"));\n\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n\trcu_spawn_one_boost_kthread(rnp);\n\trcu_spawn_cpu_nocb_kthread(cpu);\n\tWRITE_ONCE(rcu_state.n_online_cpus, rcu_state.n_online_cpus + 1);\n\n\treturn 0;\n}\n\n \nstatic void rcutree_affinity_setting(unsigned int cpu, int outgoing)\n{\n\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);\n\n\trcu_boost_kthread_setaffinity(rdp->mynode, outgoing);\n}\n\n \nbool rcu_cpu_beenfullyonline(int cpu)\n{\n\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);\n\n\treturn smp_load_acquire(&rdp->beenonline);\n}\n\n \nint rcutree_online_cpu(unsigned int cpu)\n{\n\tunsigned long flags;\n\tstruct rcu_data *rdp;\n\tstruct rcu_node *rnp;\n\n\trdp = per_cpu_ptr(&rcu_data, cpu);\n\trnp = rdp->mynode;\n\traw_spin_lock_irqsave_rcu_node(rnp, flags);\n\trnp->ffmask |= rdp->grpmask;\n\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n\tif (rcu_scheduler_active == RCU_SCHEDULER_INACTIVE)\n\t\treturn 0;  \n\tsync_sched_exp_online_cleanup(cpu);\n\trcutree_affinity_setting(cpu, -1);\n\n\t \n\ttick_dep_clear(TICK_DEP_BIT_RCU);\n\treturn 0;\n}\n\n \nint rcutree_offline_cpu(unsigned int cpu)\n{\n\tunsigned long flags;\n\tstruct rcu_data *rdp;\n\tstruct rcu_node *rnp;\n\n\trdp = per_cpu_ptr(&rcu_data, cpu);\n\trnp = rdp->mynode;\n\traw_spin_lock_irqsave_rcu_node(rnp, flags);\n\trnp->ffmask &= ~rdp->grpmask;\n\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n\n\trcutree_affinity_setting(cpu, cpu);\n\n\t \n\ttick_dep_set(TICK_DEP_BIT_RCU);\n\treturn 0;\n}\n\n \nvoid rcu_cpu_starting(unsigned int cpu)\n{\n\tunsigned long mask;\n\tstruct rcu_data *rdp;\n\tstruct rcu_node *rnp;\n\tbool newcpu;\n\n\tlockdep_assert_irqs_disabled();\n\trdp = per_cpu_ptr(&rcu_data, cpu);\n\tif (rdp->cpu_started)\n\t\treturn;\n\trdp->cpu_started = true;\n\n\trnp = rdp->mynode;\n\tmask = rdp->grpmask;\n\tarch_spin_lock(&rcu_state.ofl_lock);\n\trcu_dynticks_eqs_online();\n\traw_spin_lock(&rcu_state.barrier_lock);\n\traw_spin_lock_rcu_node(rnp);\n\tWRITE_ONCE(rnp->qsmaskinitnext, rnp->qsmaskinitnext | mask);\n\traw_spin_unlock(&rcu_state.barrier_lock);\n\tnewcpu = !(rnp->expmaskinitnext & mask);\n\trnp->expmaskinitnext |= mask;\n\t \n\tsmp_store_release(&rcu_state.ncpus, rcu_state.ncpus + newcpu);  \n\tASSERT_EXCLUSIVE_WRITER(rcu_state.ncpus);\n\trcu_gpnum_ovf(rnp, rdp);  \n\trdp->rcu_onl_gp_seq = READ_ONCE(rcu_state.gp_seq);\n\trdp->rcu_onl_gp_flags = READ_ONCE(rcu_state.gp_flags);\n\n\t \n\tif (WARN_ON_ONCE(rnp->qsmask & mask)) {  \n\t\t \n\t\tunsigned long flags;\n\n\t\tlocal_irq_save(flags);\n\t\trcu_disable_urgency_upon_qs(rdp);\n\t\t \n\t\trcu_report_qs_rnp(mask, rnp, rnp->gp_seq, flags);\n\t} else {\n\t\traw_spin_unlock_rcu_node(rnp);\n\t}\n\tarch_spin_unlock(&rcu_state.ofl_lock);\n\tsmp_store_release(&rdp->beenonline, true);\n\tsmp_mb();  \n}\n\n \nvoid rcu_report_dead(unsigned int cpu)\n{\n\tunsigned long flags, seq_flags;\n\tunsigned long mask;\n\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);\n\tstruct rcu_node *rnp = rdp->mynode;   \n\n\t \n\tdo_nocb_deferred_wakeup(rdp);\n\n\trcu_preempt_deferred_qs(current);\n\n\t \n\tmask = rdp->grpmask;\n\tlocal_irq_save(seq_flags);\n\tarch_spin_lock(&rcu_state.ofl_lock);\n\traw_spin_lock_irqsave_rcu_node(rnp, flags);  \n\trdp->rcu_ofl_gp_seq = READ_ONCE(rcu_state.gp_seq);\n\trdp->rcu_ofl_gp_flags = READ_ONCE(rcu_state.gp_flags);\n\tif (rnp->qsmask & mask) {  \n\t\t \n\t\trcu_disable_urgency_upon_qs(rdp);\n\t\trcu_report_qs_rnp(mask, rnp, rnp->gp_seq, flags);\n\t\traw_spin_lock_irqsave_rcu_node(rnp, flags);\n\t}\n\tWRITE_ONCE(rnp->qsmaskinitnext, rnp->qsmaskinitnext & ~mask);\n\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n\tarch_spin_unlock(&rcu_state.ofl_lock);\n\tlocal_irq_restore(seq_flags);\n\n\trdp->cpu_started = false;\n}\n\n#ifdef CONFIG_HOTPLUG_CPU\n \nvoid rcutree_migrate_callbacks(int cpu)\n{\n\tunsigned long flags;\n\tstruct rcu_data *my_rdp;\n\tstruct rcu_node *my_rnp;\n\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);\n\tbool needwake;\n\n\tif (rcu_rdp_is_offloaded(rdp) ||\n\t    rcu_segcblist_empty(&rdp->cblist))\n\t\treturn;   \n\n\traw_spin_lock_irqsave(&rcu_state.barrier_lock, flags);\n\tWARN_ON_ONCE(rcu_rdp_cpu_online(rdp));\n\trcu_barrier_entrain(rdp);\n\tmy_rdp = this_cpu_ptr(&rcu_data);\n\tmy_rnp = my_rdp->mynode;\n\trcu_nocb_lock(my_rdp);  \n\tWARN_ON_ONCE(!rcu_nocb_flush_bypass(my_rdp, NULL, jiffies, false));\n\traw_spin_lock_rcu_node(my_rnp);  \n\t \n\tneedwake = rcu_advance_cbs(my_rnp, rdp) ||\n\t\t   rcu_advance_cbs(my_rnp, my_rdp);\n\trcu_segcblist_merge(&my_rdp->cblist, &rdp->cblist);\n\traw_spin_unlock(&rcu_state.barrier_lock);  \n\tneedwake = needwake || rcu_advance_cbs(my_rnp, my_rdp);\n\trcu_segcblist_disable(&rdp->cblist);\n\tWARN_ON_ONCE(rcu_segcblist_empty(&my_rdp->cblist) != !rcu_segcblist_n_cbs(&my_rdp->cblist));\n\tcheck_cb_ovld_locked(my_rdp, my_rnp);\n\tif (rcu_rdp_is_offloaded(my_rdp)) {\n\t\traw_spin_unlock_rcu_node(my_rnp);  \n\t\t__call_rcu_nocb_wake(my_rdp, true, flags);\n\t} else {\n\t\trcu_nocb_unlock(my_rdp);  \n\t\traw_spin_unlock_irqrestore_rcu_node(my_rnp, flags);\n\t}\n\tif (needwake)\n\t\trcu_gp_kthread_wake();\n\tlockdep_assert_irqs_enabled();\n\tWARN_ONCE(rcu_segcblist_n_cbs(&rdp->cblist) != 0 ||\n\t\t  !rcu_segcblist_empty(&rdp->cblist),\n\t\t  \"rcu_cleanup_dead_cpu: Callbacks on offline CPU %d: qlen=%lu, 1stCB=%p\\n\",\n\t\t  cpu, rcu_segcblist_n_cbs(&rdp->cblist),\n\t\t  rcu_segcblist_first_cb(&rdp->cblist));\n}\n#endif\n\n \nstatic int rcu_pm_notify(struct notifier_block *self,\n\t\t\t unsigned long action, void *hcpu)\n{\n\tswitch (action) {\n\tcase PM_HIBERNATION_PREPARE:\n\tcase PM_SUSPEND_PREPARE:\n\t\trcu_async_hurry();\n\t\trcu_expedite_gp();\n\t\tbreak;\n\tcase PM_POST_HIBERNATION:\n\tcase PM_POST_SUSPEND:\n\t\trcu_unexpedite_gp();\n\t\trcu_async_relax();\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn NOTIFY_OK;\n}\n\n#ifdef CONFIG_RCU_EXP_KTHREAD\nstruct kthread_worker *rcu_exp_gp_kworker;\nstruct kthread_worker *rcu_exp_par_gp_kworker;\n\nstatic void __init rcu_start_exp_gp_kworkers(void)\n{\n\tconst char *par_gp_kworker_name = \"rcu_exp_par_gp_kthread_worker\";\n\tconst char *gp_kworker_name = \"rcu_exp_gp_kthread_worker\";\n\tstruct sched_param param = { .sched_priority = kthread_prio };\n\n\trcu_exp_gp_kworker = kthread_create_worker(0, gp_kworker_name);\n\tif (IS_ERR_OR_NULL(rcu_exp_gp_kworker)) {\n\t\tpr_err(\"Failed to create %s!\\n\", gp_kworker_name);\n\t\treturn;\n\t}\n\n\trcu_exp_par_gp_kworker = kthread_create_worker(0, par_gp_kworker_name);\n\tif (IS_ERR_OR_NULL(rcu_exp_par_gp_kworker)) {\n\t\tpr_err(\"Failed to create %s!\\n\", par_gp_kworker_name);\n\t\tkthread_destroy_worker(rcu_exp_gp_kworker);\n\t\treturn;\n\t}\n\n\tsched_setscheduler_nocheck(rcu_exp_gp_kworker->task, SCHED_FIFO, &param);\n\tsched_setscheduler_nocheck(rcu_exp_par_gp_kworker->task, SCHED_FIFO,\n\t\t\t\t   &param);\n}\n\nstatic inline void rcu_alloc_par_gp_wq(void)\n{\n}\n#else  \nstruct workqueue_struct *rcu_par_gp_wq;\n\nstatic void __init rcu_start_exp_gp_kworkers(void)\n{\n}\n\nstatic inline void rcu_alloc_par_gp_wq(void)\n{\n\trcu_par_gp_wq = alloc_workqueue(\"rcu_par_gp\", WQ_MEM_RECLAIM, 0);\n\tWARN_ON(!rcu_par_gp_wq);\n}\n#endif  \n\n \nstatic int __init rcu_spawn_gp_kthread(void)\n{\n\tunsigned long flags;\n\tstruct rcu_node *rnp;\n\tstruct sched_param sp;\n\tstruct task_struct *t;\n\tstruct rcu_data *rdp = this_cpu_ptr(&rcu_data);\n\n\trcu_scheduler_fully_active = 1;\n\tt = kthread_create(rcu_gp_kthread, NULL, \"%s\", rcu_state.name);\n\tif (WARN_ONCE(IS_ERR(t), \"%s: Could not start grace-period kthread, OOM is now expected behavior\\n\", __func__))\n\t\treturn 0;\n\tif (kthread_prio) {\n\t\tsp.sched_priority = kthread_prio;\n\t\tsched_setscheduler_nocheck(t, SCHED_FIFO, &sp);\n\t}\n\trnp = rcu_get_root();\n\traw_spin_lock_irqsave_rcu_node(rnp, flags);\n\tWRITE_ONCE(rcu_state.gp_activity, jiffies);\n\tWRITE_ONCE(rcu_state.gp_req_activity, jiffies);\n\t \n\tsmp_store_release(&rcu_state.gp_kthread, t);   \n\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n\twake_up_process(t);\n\t \n\tWARN_ON(num_online_cpus() > 1);\n\t \n\trcu_spawn_cpu_nocb_kthread(smp_processor_id());\n\trcu_spawn_one_boost_kthread(rdp->mynode);\n\trcu_spawn_core_kthreads();\n\t \n\trcu_start_exp_gp_kworkers();\n\treturn 0;\n}\nearly_initcall(rcu_spawn_gp_kthread);\n\n \nvoid rcu_scheduler_starting(void)\n{\n\tunsigned long flags;\n\tstruct rcu_node *rnp;\n\n\tWARN_ON(num_online_cpus() != 1);\n\tWARN_ON(nr_context_switches() > 0);\n\trcu_test_sync_prims();\n\n\t \n\tlocal_irq_save(flags);\n\trcu_for_each_node_breadth_first(rnp)\n\t\trnp->gp_seq_needed = rnp->gp_seq = rcu_state.gp_seq;\n\tlocal_irq_restore(flags);\n\n\t \n\trcu_scheduler_active = RCU_SCHEDULER_INIT;\n\trcu_test_sync_prims();\n}\n\n \nstatic void __init rcu_init_one(void)\n{\n\tstatic const char * const buf[] = RCU_NODE_NAME_INIT;\n\tstatic const char * const fqs[] = RCU_FQS_NAME_INIT;\n\tstatic struct lock_class_key rcu_node_class[RCU_NUM_LVLS];\n\tstatic struct lock_class_key rcu_fqs_class[RCU_NUM_LVLS];\n\n\tint levelspread[RCU_NUM_LVLS];\t\t \n\tint cpustride = 1;\n\tint i;\n\tint j;\n\tstruct rcu_node *rnp;\n\n\tBUILD_BUG_ON(RCU_NUM_LVLS > ARRAY_SIZE(buf));   \n\n\t \n\tif (rcu_num_lvls <= 0 || rcu_num_lvls > RCU_NUM_LVLS)\n\t\tpanic(\"rcu_init_one: rcu_num_lvls out of range\");\n\n\t \n\n\tfor (i = 1; i < rcu_num_lvls; i++)\n\t\trcu_state.level[i] =\n\t\t\trcu_state.level[i - 1] + num_rcu_lvl[i - 1];\n\trcu_init_levelspread(levelspread, num_rcu_lvl);\n\n\t \n\n\tfor (i = rcu_num_lvls - 1; i >= 0; i--) {\n\t\tcpustride *= levelspread[i];\n\t\trnp = rcu_state.level[i];\n\t\tfor (j = 0; j < num_rcu_lvl[i]; j++, rnp++) {\n\t\t\traw_spin_lock_init(&ACCESS_PRIVATE(rnp, lock));\n\t\t\tlockdep_set_class_and_name(&ACCESS_PRIVATE(rnp, lock),\n\t\t\t\t\t\t   &rcu_node_class[i], buf[i]);\n\t\t\traw_spin_lock_init(&rnp->fqslock);\n\t\t\tlockdep_set_class_and_name(&rnp->fqslock,\n\t\t\t\t\t\t   &rcu_fqs_class[i], fqs[i]);\n\t\t\trnp->gp_seq = rcu_state.gp_seq;\n\t\t\trnp->gp_seq_needed = rcu_state.gp_seq;\n\t\t\trnp->completedqs = rcu_state.gp_seq;\n\t\t\trnp->qsmask = 0;\n\t\t\trnp->qsmaskinit = 0;\n\t\t\trnp->grplo = j * cpustride;\n\t\t\trnp->grphi = (j + 1) * cpustride - 1;\n\t\t\tif (rnp->grphi >= nr_cpu_ids)\n\t\t\t\trnp->grphi = nr_cpu_ids - 1;\n\t\t\tif (i == 0) {\n\t\t\t\trnp->grpnum = 0;\n\t\t\t\trnp->grpmask = 0;\n\t\t\t\trnp->parent = NULL;\n\t\t\t} else {\n\t\t\t\trnp->grpnum = j % levelspread[i - 1];\n\t\t\t\trnp->grpmask = BIT(rnp->grpnum);\n\t\t\t\trnp->parent = rcu_state.level[i - 1] +\n\t\t\t\t\t      j / levelspread[i - 1];\n\t\t\t}\n\t\t\trnp->level = i;\n\t\t\tINIT_LIST_HEAD(&rnp->blkd_tasks);\n\t\t\trcu_init_one_nocb(rnp);\n\t\t\tinit_waitqueue_head(&rnp->exp_wq[0]);\n\t\t\tinit_waitqueue_head(&rnp->exp_wq[1]);\n\t\t\tinit_waitqueue_head(&rnp->exp_wq[2]);\n\t\t\tinit_waitqueue_head(&rnp->exp_wq[3]);\n\t\t\tspin_lock_init(&rnp->exp_lock);\n\t\t\tmutex_init(&rnp->boost_kthread_mutex);\n\t\t\traw_spin_lock_init(&rnp->exp_poll_lock);\n\t\t\trnp->exp_seq_poll_rq = RCU_GET_STATE_COMPLETED;\n\t\t\tINIT_WORK(&rnp->exp_poll_wq, sync_rcu_do_polled_gp);\n\t\t}\n\t}\n\n\tinit_swait_queue_head(&rcu_state.gp_wq);\n\tinit_swait_queue_head(&rcu_state.expedited_wq);\n\trnp = rcu_first_leaf_node();\n\tfor_each_possible_cpu(i) {\n\t\twhile (i > rnp->grphi)\n\t\t\trnp++;\n\t\tper_cpu_ptr(&rcu_data, i)->mynode = rnp;\n\t\trcu_boot_init_percpu_data(i);\n\t}\n}\n\n \nstatic void __init sanitize_kthread_prio(void)\n{\n\tint kthread_prio_in = kthread_prio;\n\n\tif (IS_ENABLED(CONFIG_RCU_BOOST) && kthread_prio < 2\n\t    && IS_BUILTIN(CONFIG_RCU_TORTURE_TEST))\n\t\tkthread_prio = 2;\n\telse if (IS_ENABLED(CONFIG_RCU_BOOST) && kthread_prio < 1)\n\t\tkthread_prio = 1;\n\telse if (kthread_prio < 0)\n\t\tkthread_prio = 0;\n\telse if (kthread_prio > 99)\n\t\tkthread_prio = 99;\n\n\tif (kthread_prio != kthread_prio_in)\n\t\tpr_alert(\"%s: Limited prio to %d from %d\\n\",\n\t\t\t __func__, kthread_prio, kthread_prio_in);\n}\n\n \nvoid rcu_init_geometry(void)\n{\n\tulong d;\n\tint i;\n\tstatic unsigned long old_nr_cpu_ids;\n\tint rcu_capacity[RCU_NUM_LVLS];\n\tstatic bool initialized;\n\n\tif (initialized) {\n\t\t \n\t\tWARN_ON_ONCE(old_nr_cpu_ids != nr_cpu_ids);\n\t\treturn;\n\t}\n\n\told_nr_cpu_ids = nr_cpu_ids;\n\tinitialized = true;\n\n\t \n\td = RCU_JIFFIES_TILL_FORCE_QS + nr_cpu_ids / RCU_JIFFIES_FQS_DIV;\n\tif (jiffies_till_first_fqs == ULONG_MAX)\n\t\tjiffies_till_first_fqs = d;\n\tif (jiffies_till_next_fqs == ULONG_MAX)\n\t\tjiffies_till_next_fqs = d;\n\tadjust_jiffies_till_sched_qs();\n\n\t \n\tif (rcu_fanout_leaf == RCU_FANOUT_LEAF &&\n\t    nr_cpu_ids == NR_CPUS)\n\t\treturn;\n\tpr_info(\"Adjusting geometry for rcu_fanout_leaf=%d, nr_cpu_ids=%u\\n\",\n\t\trcu_fanout_leaf, nr_cpu_ids);\n\n\t \n\tif (rcu_fanout_leaf < 2 ||\n\t    rcu_fanout_leaf > sizeof(unsigned long) * 8) {\n\t\trcu_fanout_leaf = RCU_FANOUT_LEAF;\n\t\tWARN_ON(1);\n\t\treturn;\n\t}\n\n\t \n\trcu_capacity[0] = rcu_fanout_leaf;\n\tfor (i = 1; i < RCU_NUM_LVLS; i++)\n\t\trcu_capacity[i] = rcu_capacity[i - 1] * RCU_FANOUT;\n\n\t \n\tif (nr_cpu_ids > rcu_capacity[RCU_NUM_LVLS - 1]) {\n\t\trcu_fanout_leaf = RCU_FANOUT_LEAF;\n\t\tWARN_ON(1);\n\t\treturn;\n\t}\n\n\t \n\tfor (i = 0; nr_cpu_ids > rcu_capacity[i]; i++) {\n\t}\n\trcu_num_lvls = i + 1;\n\n\t \n\tfor (i = 0; i < rcu_num_lvls; i++) {\n\t\tint cap = rcu_capacity[(rcu_num_lvls - 1) - i];\n\t\tnum_rcu_lvl[i] = DIV_ROUND_UP(nr_cpu_ids, cap);\n\t}\n\n\t \n\trcu_num_nodes = 0;\n\tfor (i = 0; i < rcu_num_lvls; i++)\n\t\trcu_num_nodes += num_rcu_lvl[i];\n}\n\n \nstatic void __init rcu_dump_rcu_node_tree(void)\n{\n\tint level = 0;\n\tstruct rcu_node *rnp;\n\n\tpr_info(\"rcu_node tree layout dump\\n\");\n\tpr_info(\" \");\n\trcu_for_each_node_breadth_first(rnp) {\n\t\tif (rnp->level != level) {\n\t\t\tpr_cont(\"\\n\");\n\t\t\tpr_info(\" \");\n\t\t\tlevel = rnp->level;\n\t\t}\n\t\tpr_cont(\"%d:%d ^%d  \", rnp->grplo, rnp->grphi, rnp->grpnum);\n\t}\n\tpr_cont(\"\\n\");\n}\n\nstruct workqueue_struct *rcu_gp_wq;\n\nstatic void __init kfree_rcu_batch_init(void)\n{\n\tint cpu;\n\tint i, j;\n\n\t \n\tif (rcu_delay_page_cache_fill_msec < 0 ||\n\t\trcu_delay_page_cache_fill_msec > 100 * MSEC_PER_SEC) {\n\n\t\trcu_delay_page_cache_fill_msec =\n\t\t\tclamp(rcu_delay_page_cache_fill_msec, 0,\n\t\t\t\t(int) (100 * MSEC_PER_SEC));\n\n\t\tpr_info(\"Adjusting rcutree.rcu_delay_page_cache_fill_msec to %d ms.\\n\",\n\t\t\trcu_delay_page_cache_fill_msec);\n\t}\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct kfree_rcu_cpu *krcp = per_cpu_ptr(&krc, cpu);\n\n\t\tfor (i = 0; i < KFREE_N_BATCHES; i++) {\n\t\t\tINIT_RCU_WORK(&krcp->krw_arr[i].rcu_work, kfree_rcu_work);\n\t\t\tkrcp->krw_arr[i].krcp = krcp;\n\n\t\t\tfor (j = 0; j < FREE_N_CHANNELS; j++)\n\t\t\t\tINIT_LIST_HEAD(&krcp->krw_arr[i].bulk_head_free[j]);\n\t\t}\n\n\t\tfor (i = 0; i < FREE_N_CHANNELS; i++)\n\t\t\tINIT_LIST_HEAD(&krcp->bulk_head[i]);\n\n\t\tINIT_DELAYED_WORK(&krcp->monitor_work, kfree_rcu_monitor);\n\t\tINIT_DELAYED_WORK(&krcp->page_cache_work, fill_page_cache_func);\n\t\tkrcp->initialized = true;\n\t}\n\tif (register_shrinker(&kfree_rcu_shrinker, \"rcu-kfree\"))\n\t\tpr_err(\"Failed to register kfree_rcu() shrinker!\\n\");\n}\n\nvoid __init rcu_init(void)\n{\n\tint cpu = smp_processor_id();\n\n\trcu_early_boot_tests();\n\n\tkfree_rcu_batch_init();\n\trcu_bootup_announce();\n\tsanitize_kthread_prio();\n\trcu_init_geometry();\n\trcu_init_one();\n\tif (dump_tree)\n\t\trcu_dump_rcu_node_tree();\n\tif (use_softirq)\n\t\topen_softirq(RCU_SOFTIRQ, rcu_core_si);\n\n\t \n\tpm_notifier(rcu_pm_notify, 0);\n\tWARN_ON(num_online_cpus() > 1); \n\trcutree_prepare_cpu(cpu);\n\trcu_cpu_starting(cpu);\n\trcutree_online_cpu(cpu);\n\n\t \n\trcu_gp_wq = alloc_workqueue(\"rcu_gp\", WQ_MEM_RECLAIM, 0);\n\tWARN_ON(!rcu_gp_wq);\n\trcu_alloc_par_gp_wq();\n\n\t \n\t \n\tif (qovld < 0)\n\t\tqovld_calc = DEFAULT_RCU_QOVLD_MULT * qhimark;\n\telse\n\t\tqovld_calc = qovld;\n\n\t\n\t(void)start_poll_synchronize_rcu_expedited();\n\n\trcu_test_sync_prims();\n}\n\n#include \"tree_stall.h\"\n#include \"tree_exp.h\"\n#include \"tree_nocb.h\"\n#include \"tree_plugin.h\"\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}